# Conformal Classification with Equalized Coverage for Adaptively Selected Groups

Yanfei Zhou

Department of Data Sciences and Operations

University of Southern California

Los Angeles, California, USA

yanfei.zhou@marshall.usc.edu

&Matteo Sesia

Department of Data Sciences and Operations

University of Southern California

Los Angeles, California, USA

sesia@marshall.usc.edu

###### Abstract

This paper introduces a conformal inference method to evaluate uncertainty in classification by generating prediction sets with valid coverage conditional on adaptively chosen features. These features are carefully selected to reflect potential model limitations or biases. This can be useful to find a practical compromise between efficiency--by providing informative predictions--and algorithmic fairness--by ensuring equalized coverage for the most sensitive groups. We demonstrate the validity and effectiveness of this method on simulated and real data sets.

## 1 Introduction

### Uncertainty, Fairness, and Efficiency in Machine Learning

Increasingly sophisticated machine learning (ML) models, like deep neural networks, are revolutionizing decision-making in many high-stakes domains, including medical diagnostics [1], job screening [2], and recidivism prediction [3, 4]. However, serious concerns related to _uncertainty quantification_[5, 6] and _algorithmic fairness_[7, 8, 9, 10] underscore the need for novel methods that can provide reliable and unbiased measures of confidence, applicable to any model.

Uncertainty quantification is crucial because ML models, although effective on average, can make errors while displaying overconfidence [11]. Consequently, in some situations users may lack sufficient warning about the potential unreliability of a prediction, raising trust and safety concerns. A promising solution is _conformal inference_[12, 13, 14], which enables converting the output of any model into prediction sets with precise coverage guarantees. These sets reflect the model's confidence on a case-by-case basis, with smaller sets indicating higher confidence in a specific prediction.

Algorithmic fairness focuses on the challenges of prediction inaccuracies that disproportionately impact specific groups, often identified by sensitive attributes like race, sex, and age. Among the many sources of algorithmic bias are training data that do not adequately represent the population's heterogeneity and a focus on maximizing average performance. However, fairness is partly subjective and lacks a universally accepted definition [15], leading to sometimes conflicting interpretations [16].

This complexity makes conformal inference with _equalized coverage_[17] an appealing approach. Equalized coverage aims to ensure that the prediction sets attain their coverage not only on average for the whole population (e.g., above 90%) but also at the same level within each group of interest. While this does not necessarily imply that the prediction sets will have equal size on average across different groups--since it is possible the predictive model may be more or less accurate for different groups--it objectively communicates the possible limitations of a model. This transparency helps decision-makers recognize when predictions may be less reliable for specific subgroups, allowing them to either avoid unnecessary actions or adopt more cautious strategies in cases of higher uncertainty, thereby minimizing the potential harm from inaccurate predictions.

A limitation of the method developed in [17] for conformal inference with equalized coverage is that it does not scale well to situations involving diverse populations with multiple sensitive attributes. In such cases, it necessitates splitting the data into exponentially many subsets, significantly reducing the effective sample size and leading to less informative predictions. Balancing this trade-off [18] between _efficiency_--aiming for highly informative predictions with small set sizes--and _fairness_-ensuring unbiased treatment--is challenging and requires novel approaches. This paper introduces a method to address this by providing equalized coverage conditional on carefully chosen features, informed by the model and data. While it cannot guarantee equalized coverage for _all_ sensitive groups, it seeks a _reasonable compromise_ with finite data sets, mitigating significant biases while retaining predictive power.

### Background on Conformal Inference for Classification

Consider a data set comprising \(n\) exchangeable (e.g., i.i.d.) observations \(Z_{i}\) for \(i\in\mathcal{D}:=[n]:=\{1,\ldots,n\}\), sampled from an arbitrary and unknown distribution \(P_{Z}\). In classification, one can write \(Z_{i}=(X_{i},Y_{i})\), where \(Y_{i}\in[L]:=\{1,\ldots,L\}\) is a categorical label and \(X_{i}\in\mathcal{X}\) represents the individual's features, taking values in some space \(\mathcal{X}\). As explained below, we will assume these features include some sensitive attributes. Further, we consider a test point \(Z_{n+1}=(X_{n+1},Y_{n+1})\), also sampled exchangeably from \(P_{Z}\), and whose label \(Y_{n+1}\in[L]\) has not yet been observed.

A standard goal for split conformal prediction methods is to quantify the predictive uncertainty of a given "black-box" ML model (e.g., pre-trained on an independent data set) by constructing a prediction set \(\hat{C}(X_{n+1})\) for \(Y_{n+1}\), guaranteeing _marginal coverage_ at some desired level \(\alpha\in(0,1)\):

\[\mathbb{P}[Y_{n+1}\in\hat{C}(X_{n+1})]\geq 1-\alpha. \tag{1}\]

This probability is taken over the randomness in \(Y_{n+1}\) and \(X_{n+1}\), as well as in the data indexed by \(\mathcal{D}\). Intuitively, marginal coverage means the prediction sets are expected to cover the correct outcomes for a fraction \(1-\alpha\) of the population. However, this is not always satisfactory, especially if the miscoverage errors may disproportionately affect individuals characterized by well-defined features.

To address these concerns, one might consider _feature-conditional coverage_, \(\mathbb{P}[Y_{n+1}\in\hat{C}(X_{n+1})\mid X_{n+1}=x]\geq 1-\alpha\) for all \(x\in\mathcal{X}\). This would ensure consistent coverage for all possible test features \(X_{n+1}\). However, it is impossible to achieve without additional assumptions, such as modeling the distribution \(P_{Z}\)[19] or significantly restricting the feature space \(\mathcal{X}\)[20]. Given that such assumptions may be unrealistic in real-world settings, exact feature-conditional coverage is typically unachievable.

Equalized coverage [17] seeks a practical middle ground between the two extremes of marginal and feature-conditional coverage, focusing on accounting for specific _discrete_ attributes encapsulated by \(X_{n+1}\). To facilitate the subsequent exposition of our method, it is useful to recall the definition of equalized coverage with the following notation.

Let \(K\) denote the number of sensitive attributes, and for each \(k\in[K]\) let \(M_{k}\in\mathbb{N}\) count the possible values of the \(k\)-th attribute. Consider a function \(\phi:\mathcal{X}\times\{0,1\}^{K}\rightarrow\mathbb{N}^{d}\) for any subset \(A\subseteq[K]\) with \(|A|=d\) elements, so that \(\phi(x,A)\) is a vector of length \(|A|\) representing the values of all attributes indexed by \(A\) for an individual with features \(x\). In the special case where \(A\) is an empty set, \(\phi\) returns a constant. If \(A\) is a singleton, e.g., \(A=\{k\}\) for some \(k\in[K]\), then \(\phi(x,\{k\})\in[M_{k}]\) denotes the value of the \(k\)-th attribute; e.g., someone's academic degree. More generally, \(\phi(x,\{k,l\})\in[M_{k}]\times[M_{l}]\), for any distinct \(k,l\in[K]\), denotes the joint values of two attributes, characterizing a smaller group, such as "males with a bachelor's degree.".

When multiple sensitive attributes are involved, i.e., \(K>1\), the concept of equalized coverage introduced by [17] can be naturally extended to _exhaustive equalized coverage_, defined as:

\[\mathbb{P}[Y_{n+1}\in\hat{C}(X_{n+1})\mid\phi(X_{n+1},[K])]\geq 1-\alpha. \tag{2}\]

In words, this says \(\hat{C}(X_{n+1})\) has valid coverage conditional on all \(K\) sensitive attributes. Prediction sets satisfying (2) can be obtained by applying the standard conformal calibration method separately within each of the \(M=\prod_{k=1}^{K}M_{k}\) groups characterized by a specific combination of the protected attributes represented by \(\phi(X_{i},[K])\); see Appendix A1 for details. However, a downside of this approach is that the calibration subsets may be too small if \(M\) is large, leading to uninformative predictions for even moderate values of \(K\). This limitation forms the starting point of our work.

### Preview of Our Contributions: Adaptive Equalized Coverage

In practice, for a given model and data set, different groups may not exhibit the same need for rigorous equalized coverage guarantees (2), as conformal predictions may be able to approximately achieve the desired coverage even without explicit constraints. Algorithmic bias typically affects only a minority of the population, so standard prediction sets with marginal coverage (1) may approximately satisfy (2) for most groups. Therefore, we propose Adaptively Fair Conformal Prediction (AFCP), a new method that efficiently identifies and addresses groups suffering from algorithmic bias in a data-driven way, adjusting their prediction sets to equalize coverage without sacrificing informativeness.

AFCP involves two main steps. First, as sketched in Figure 1, it carefully selects a sensitive attribute \(\hat{A}(X_{n+1})\in\{\emptyset,\{1\},\ldots,\{K\}\}\), based on \(X_{n+1}\) and the data in \(\mathcal{D}\). Although AFCP can be extended to select multiple attributes, we begin by focusing on this simpler version for clarity. Intuitively, AFCP searches for the attribute corresponding to the group most negatively affected by algorithmic bias. It may also opt to select no attribute (\(\hat{A}(X_{n+1})=\emptyset\)) in the absence of significant biases.

Next, AFCP constructs a prediction set \(\hat{C}(X_{n+1})\) for \(Y_{n+1}\) that guarantees the following notion of _adaptive equalized coverage_ at the desired level \(\alpha\in(0,1)\):

\[\mathbb{P}[Y_{n+1}\in\hat{C}(X_{n+1})\ |\ \phi(X_{n+1},\hat{A}(X_{n+1}))]\geq 1 -\alpha. \tag{3}\]

In words, this tells us \(\hat{C}(X_{n+1})\) is well-calibrated for the groups defined by the selected attribute \(\hat{A}(X_{n+1})\). It is worth highlighting the key distinctions between (3) and the existing notions of coverage reviewed above. On the one hand, if AFCP identifies no significant bias, selecting \(\hat{A}(X_{n+1})=\emptyset\), then (3) reduces to marginal coverage (1), following the convention that \(\phi(X_{n+1},\emptyset)\) is a constant. On the other hand, exhaustive equalized coverage (2) would correspond to simultaneously selecting all possible sensitive attributes instead of only that identified by \(\hat{A}(X_{n+1})\). To clarify the terminology, in this paper we will say that an attribute is _sensitive_ if it may identify a group affected by algorithmic bias. By contrast, a _protected_ attribute is one for which equalized coverage is explicitly sought.

Figure 2 illustrates this intuition through a simulated example. In this scenario, we generate synthetic medical diagnosis data, considering six possible diagnosis labels, and designate race, sex, and age group as potentially sensitive attributes alongside other demographic factors. Notably, the female group, identified by sex, is characterized by fewer samples and higher algorithmic bias, resulting in marginal prediction sets with low group-conditional coverage. By contrast, the model leads to no significant disparities across races and age groups in this dataset.

For two example patients from the critical group, the standard marginal prediction sets fail to cover the true label. Conversely, sets calibrated for exhaustive equalized coverage are too conservative to be informative. By contrast, AFCP generates prediction sets that are both efficient and fair.

Without additional sample splitting, which would be inefficient, constructing informative prediction sets that satisfy (3) is challenging due to potential selection bias from using the same data for attribute selection and conformal calibration. This paper presents a novel solution to address this challenge.

Figure 1: Schematic visualization of the automatic sensitive attribute selection carried out by our Adaptively Fair Conformal Prediction (AFCP) method. This method is designed to find the attribute corresponding to the group most negatively affected by algorithmic bias, on a case-by-case basis.

### Table of Contents

Section 2 presents our AFCP method, focusing on the special case in which at most one sensitive attribute may be selected. Section 3 demonstrates the empirical performance of AFCP on synthetic and real data. Section 4 discusses some limitations and suggests ideas for future work.

Additional content is presented in the Appendices. Appendix A1 reviews relevant details of existing approaches. Appendices A2 and A3 present two extensions of our method, respectively enabling the selection of more than one sensitive attribute and providing valid coverage also conditional on the true test label; both extensions involve distinct technical challenges. Additionally, a variation of AFCP designed for outlier detection tasks is detailed in Appendix A4. Appendix A5 contains all mathematical proofs. Appendix A6 explains how to implement our method efficiently and studies its computational cost. Appendix A7 describes the results of numerous additional experiments.

### Related Works

Conformal inference is a very active research area, with numerous methods addressing diverse tasks, including outlier detection [21, 22, 23], classification [24, 25, 26, 27, 28], and regression [29, 30, 31]. Overcoming the limitations of the standard marginal coverage guarantees (1) is a main interest in this field.

Some works have proposed _conformity scores_ designed to seek high feature-conditional coverage while calibrating prediction sets for marginal coverage [27, 30]. Others attempt to mitigate overconfidence while training the ML model [32, 33], and several have developed calibration methods for non-exchangeable data, accounting for possible distribution shifts [34, 35, 36, 37, 38]. These works are complementary to ours, as we focus on guaranteeing a new adaptive notion of equalized coverage.

In addition to [17], several other works have considered constructing prediction sets adhering to various notions of equalized coverage and have empirically investigated the performance of conformal predictors in this regard [39]. In the context of regression, [40] and [41] proposed strategies to enhance conditional coverage given several protected attributes, but they targeted a different notion of equalized coverage designed for continuous outcomes. In classification, a classical approach to move beyond marginal coverage is label-conditional coverage, where the "protected" groups are defined not based on the features \(X_{n+1}\) but by the label itself, \(Y_{n+1}\)[42, 43, 44]. As explained in Appendix A3, the method proposed in this paper can also be extended to provide label-conditional coverage.

More closely related to the notion of equalized coverage [17] are the works of [45, 46], which differ from ours as they do not consider the automatic selection of the sensitive groups. To tackle a related challenge due to unknown biased attributes, [47] studied how to identify unfairly treated groups by establishing a simultaneously valid confidence bound on group-wise disparities. In principle, their approach can be integrated within the selection component of our method. Very recently, [48] proposed an elegant method to obtain valid conformal prediction sets for adaptively selected subsets of test cases. While their perspective aligns more closely with ours, their approach and focus differ as they study different selection rules not specifically aimed at mitigating algorithmic bias.

Figure 2: Prediction sets constructed with different methods for patients in groups negatively affected by algorithm bias. Our method (AFCP) is designed to provide informative prediction sets that are well-calibrated conditional on the automatically identified critical sensitive attribute.

## 2 Method

### Automatic Attribute Selection

Given a pre-trained classifier, an independent calibration data set \(\mathcal{D}\), and a test point \(Z_{n+1}=(X_{n+1},Y_{n+1})\) with an unknown label \(Y_{n+1}\), we will select (at most) one sensitive attribute, \(\hat{A}(X_{n+1})\in\{\emptyset,\{1\},\ldots,\{K\}\}\), according to the following _leave-one-out_ procedure.

For each \(y\in[L]\), imagine \(Y_{n+1}\) is equal to \(y\), and define an _augmented_ calibration set \(\mathcal{D}^{\prime}_{y}:=\mathcal{D}\cup\{(X_{n+1},y)\}\). For each \(i\in[n+1]\), define also the leave-one-out set \(\mathcal{D}^{\prime}_{y,i}:=\mathcal{D}^{\prime}_{y}\setminus\{(X_{i},Y_{i})\}\), with \(y\) acting as a placeholder for \(Y_{n+1}\). Then, for each \(i\in[n+1]\), we construct a conformal prediction set \(\hat{C}^{\text{loo}}_{y}(X_{i})\) for \(Y_{i}\) given \(X_{i}\) by calibrating the classifier using the data in \(\mathcal{D}^{\prime}_{y,i}\). Any method can be applied for this purpose, although it may be helpful for concreteness to focus on employing the standard approach seeking marginal coverage (1) using the adaptive conformity scores proposed by [27]. Let \(E_{y,i}\) denote the binary indicator of whether \(\hat{C}^{\text{loo}}_{y}(X_{i})\) fails to cover \(Y_{i}\):

\[E_{y,i}:=\mathbf{1}\{Y_{i}\notin\hat{C}^{\text{loo}}_{y}(X_{i})\}. \tag{4}\]

After evaluating \(E_{y,i}\) for all \(i\in[n+1]\), we will assess the leave-one-out miscoverage rate for the worst-off group identified by each sensitive attribute \(k\in[K]\). That is, we evaluate

\[\delta_{y,k}:=\max_{m\in[M_{k}]}\frac{\sum_{i=1}^{n+1}E_{y,i}\cdot\mathbf{1}\{ \phi(X_{i},\{k\})=m\}}{\sum_{i=1}^{n+1}\mathbf{1}\{\phi(X_{i},\{k\})=m\}}.\]

Intuitively, \(\delta_{y,k}\) denotes the maximum miscoverage rate across all groups identified by the \(k\)-th attribute. Large values of \(\delta_{y,k}\) suggest that the \(k\)-th attribute may be a sensitive attribute corresponding to at least one group suffering from algorithmic bias.

To assess whether there is evidence of significant algorithmic bias, we can perform a statistical test for the null hypothesis that no algorithmic bias exists. Note that this test can be heuristic since it does not need to be exact for our method to rigorously guarantee (3). Therefore, we do not need to carefully consider the assumptions underlying this test. As a useful heuristic, we define:

\[\hat{q}_{y}:=\max_{k\in[K]}\delta_{y,k}, \tag{5}\]

and carry out a one-sided t-test for the null hypothesis \(H_{0}:\hat{q}_{y}\leq\alpha\) against \(H_{1}:\hat{q}_{y}>\alpha\).

If \(H_{0}\) is rejected (at any desired level, like 5%), we conclude there exists a group suffering from significant algorithmic bias, and we identify the corresponding attribute through

\[\hat{A}(X_{n+1},y)=\{\operatorname*{arg\,max}_{k\in[K]}\delta_{y,k}\}. \tag{6}\]

Otherwise, we set \(\hat{A}(X_{n+1},y)=\emptyset\), which corresponds to selecting no attribute. See Algorithm 1 for an outline of this procedure, as a function of the placeholder label \(y\).

After repeating this procedure for each \(y\in[L]\), the final selected attribute \(\hat{A}(X_{n+1})\) is:

\[\hat{A}(X_{n+1})=\cap_{y\in[L]}\hat{A}(X_{n+1},y). \tag{7}\]

Therefore, an attribute is selected if and only if it is consistently flagged by our leave-one-out procedure for all values of the placeholder label \(y\in[L]\). This approach minimizes the potential arbitrariness due to the use of a placeholder label and is necessary to guarantee that our method constructs prediction sets achieving (3), as discussed in the next section.

Before explaining how our method utilizes the selected sensitive attribute obtained in (7) to construct prediction sets satisfying (3), we pause to make two remarks. First, as long as \(n\) is large enough, \(\hat{A}(X_{n+1},y)\) is quite stable with respect to both \(X_{n+1}\) and \(y\), as each of these variables plays a relatively small role in determining the leave-one-out miscoverage rates. Therefore, the selected attribute \(\hat{A}(X_{n+1})\) given by (7) is also quite stable for different values of \(X_{n+1}\). This stability will be demonstrated empirically in Section 3. Second, despite its iterative nature, our method can be implemented efficiently; see Appendix A6. Further, if \(n\) is very large, our method could be streamlined using cross-validation instead of a leave-one-out approach.

```
1:Input: calibration data \(\mathcal{D}\); test point with features \(X_{n+1}\); list of \(K\) sensitive attributes;
2: pre-trained classifier \(\hat{f}\); fixed rule for computing nonconformity scores; level \(\alpha\in(0,1)\);
3: placeholder label \(y\in[L]\).
4: Assume \(Y_{n+1}=y\) and define the augmented data set \(\mathcal{D}^{\prime}_{y}:=\mathcal{D}\cup\{(X_{n+1},y)\}\).
5:for\(i\in[n+1]\)do
6: Pretend that \((X_{i},Y_{i})\) is the test point and \(\mathcal{D}^{\prime}_{y}\setminus\{(X_{i},Y_{i})\}\) is the calibration set.
7: Construct a conformal prediction set \(\hat{C}^{\text{loo}}_{y}(X_{i})\) for \(Y_{i}\).
8: Evaluate the miscoverage indicator \(E_{y,i}\) using (4).
9:endfor
10: Perform a one-sided test for \(H_{0}:\hat{q}_{y}\leq\alpha\) vs. \(H_{1}:\hat{q}_{y}>\alpha\), with \(\hat{q}_{y}\) defined as in (5).
11: Select the attribute \(\hat{A}(X_{n+1},y)\) using (6) if \(H_{0}\) is rejected, else set \(\hat{A}(X_{n+1},y)=\emptyset\).
12:Output: \(\hat{A}(X_{n+1},y)\), either a selected sensitive attribute or an empty set.
```

**Algorithm 1** Automatic attribute selection using a placeholder test label.

### Constructing the Adaptive Prediction Sets

After evaluating \(\hat{A}(X_{n+1},y)\) by applying Algorithm 1 with placeholder label \(y\) for \(Y_{n+1}\) for all \(y\in[L]\), and selecting either an empty set or a single attribute \(\hat{A}(X_{n+1})\) using (7), AFCP constructs an adaptive prediction set for \(Y_{n+1}\) that satisfies (3) as follows.

First, it constructs a _marginal_ conformal prediction set \(\hat{C}^{\text{m}}(X_{n+1})\) targeting (1), by applying the standard approach reviewed in Appendix A1. Then, for each \(y\in[L]\), it constructs a conformal prediction set \(\hat{C}(X_{n+1},\hat{A}(X_{n+1},y))\) with equalized coverage for the group identified by attribute \(\hat{A}(X_{n+1},y)\), as if it had been fixed. This is achieved by applying the standard marginal method based on a restricted calibration sample indexed by \(\{i\in[n]:\phi(X_{i},\hat{A}(X_{n+1},y))=\phi(X_{n+1},\hat{A}(X_{n+1},y))\}\); see Algorithm A1 in Appendix A1 for further details. Therefore, note that \(\phi(X_{i},\hat{A}(X_{n+1},y))\) becomes equivalent to \(\hat{C}^{\text{m}}(X_{n+1})\) if \(\hat{A}(X_{n+1},y)=\emptyset\). Finally, the AFCP prediction set for \(Y_{n+1}\) is given by:

\[\hat{C}(X_{n+1})=\hat{C}^{\text{m}}(X_{n+1})\cup\left\{\cup_{y=1}^{L}\hat{C}(X _{n+1},\hat{A}(X_{n+1},y))\right\}. \tag{8}\]

See Algorithm 2 for an outline of this procedure.

Note that the AFCP set \(\hat{C}(X_{n+1})\) given by (8) always contains the marginal set \(\hat{C}^{\text{m}}(X_{n+1})\); this is essential to prove the validity of our approach. Second, in practice the selection \(\hat{A}(X_{n+1},y)\) tends to be very consistent for different values of the placeholder label \(y\), as long as the sample size \(n\) is large enough; therefore, the union in (8) will typically not lead to a very large prediction set.

```
1:Input: calibration data \(\mathcal{D}\); test point with features \(X_{n+1}\); list of \(K\) sensitive attributes;
2: pre-trained classifier \(\hat{f}\); fixed rule for computing nonconformity scores; level \(\alpha\in(0,1)\).
3:for\(y\in[L]\)do
4: Select an attribute \(\hat{A}(X_{n+1},y)\) by applying Algorithm 1 with placeholder label \(y\).
5: Construct \(\hat{C}(X_{n+1},A)\) by applying Algorithm A1 with the attribute \(A=\hat{A}(X_{n+1},y)\).
6:endfor
7: Construct \(\hat{C}^{\text{m}}(X_{n+1})\) by applying Algorithm A1 without protected attributes.
8:Output: selected attribute \(\hat{A}(X_{n+1})\) given by (7) and prediction set \(\hat{C}(X_{n+1})\) given by (8).
```

**Algorithm 2** Adaptively Fair Conformal Prediction (AFCP).

The following result, proved in Appendix A5, establishes that the prediction sets \(\hat{C}(X_{n+1})\) output by AFCP guarantee adaptive equalized coverage (3) with respect to the adaptively selected attribute \(\hat{A}(X_{n+1})\). It is worth emphasizing this result is not straightforward and involves an innovative proof technique to address the lack of exchangeability introduced by the adaptive selection step.

**Theorem 1**.: _If \(\{(X_{i},Y_{i})\}_{i=1}^{n+1}\) are exchangeable, the prediction set \(\hat{C}(X_{n+1})\) and the selected attribute \(\hat{A}(X_{n+1})\) output by Algorithm 2 satisfy the adaptive equalized coverage defined in (3)._Numerical Experiments

### Setup and Benchmarks

This section demonstrates the empirical performance of AFCP, focusing on the implementation described in Section 2, which selects at most one sensitive attribute. Our method is compared with three existing approaches, which utilize the same data, ML model, and conformity scores but produce prediction sets with different guarantees. The first is the _marginal_ benchmark, which constructs prediction sets guaranteeing (1) by applying Algorithm A1 without protected attributes. The second is the _exhaustive_ equalized benchmark, which constructs prediction sets guaranteeing (2) by applying Algorithm A1 with all \(K\) sensitive attributes simultaneously protected. The third is a _partial_ equalized benchmark that separately applies Algorithm A1 with each possible protected attribute \(k\in[K]\), and then takes the union of all such prediction sets. This is an intuitive approach that can be easily verified to provide a coverage guarantee intermediate between (2) and (3), namely:

\[\mathbb{P}[Y_{n+1}\in\hat{C}(X_{n+1})\mid\phi(X_{n+1},\{k\})]\geq 1-\alpha, \quad\forall k\in[K]. \tag{9}\]

However, we will see that these prediction sets are often still too conservative in practice.

In addition, we apply a variation of AFCP that always selects one sensitive attribute, regardless of the outcome of the significance test. This method is denoted as AFCP1 in our experiments.

For all methods considered, the classifier is based on a five-layer neural network with linear layers interconnected via a ReLU activation function. The output layer uses a softmax function to estimate the conditional label probabilities. The Adam optimizer and cross-entropy loss function are used in the training process, with a learning rate set at 0.0001. The loss values demonstrate convergence after 100 epochs of training. For all methods, the miscoverage target level is set at \(\alpha=0.1\).

### Synthetic Data

We generate synthetic classification data to mimic a medical diagnosis task with six possible labels: Skin cancer, Diabetes, Asthma, Stroke, Flu, and Epilepsy. The available features include three sensitive attributes--Age Group, Region, and Color--and six additional non-sensitive covariates. Color is categorized as Blue or Grey, with 10% and 90% marginal frequencies, respectively. The Age Group is cyclically repeated as \(<18,18-24,25-40,41-65,>65\), and Region is sampled from an i.i.d. multinomial distribution across {West, East, North, South} with equal probabilities. The six non-sensitive features are i.i.d. random samples from a uniform distribution on \([0,1]\). For simplicity, Color is denoted as \(X_{0}\) and the first non-sensitive feature as \(X_{1}\). Conditional on \(X\), the label \(Y\) is generated based on a decision tree model that depends only on \(X_{0}\) and \(X_{1}\), as detailed in Appendix A7. This model is designed so that the diagnosis label for individuals with Color equal to Blue is intrinsically harder to predict, mimicking the presence of algorithmic bias.

Figure 3 shows the performance of all methods as a function of the total sample size, ranging from 200 to 2000. In each case, \(50\%\) of the samples are used for training and the remaining \(50\%\) for calibration. Results are averaged over 500 test points and 100 independent experiments.

While the marginal benchmark produces the smallest prediction sets on average, it leads to significant empirical undercoverage within the Blue group. In contrast, the exhaustive benchmark, which achieves the highest coverage overall, tends to lead to overly conservative and thus uninformative prediction sets, especially for the Blue group. The partial benchmark, though less conservative than the exhaustive method, still generates prediction sets that are too large when the sample size is small.

Our AFCP method and its simpler variation, AFCP1, not only achieve valid coverage for the Blue group but do so with prediction sets that, on average, are not much larger than the marginal ones. AFCP1 is slightly more robust than AFCP when the sample size is very small, as it never fails to select a sensitive attribute. This is advantageous in scenarios where we know there is a sensitive attribute worth equalizing coverage for, though this may not always be the case in practice. See Figure A1 and Table A1 for detailed results with standard errors.

Figure 4 provides additional insight into our method's performance by plotting the selection frequencies of each sensitive attribute as a function of sample size, within the same experiments described in Figure 3. These results show that our method behaves as anticipated. When the sample size and algorithmic bias are both small, AFCP shows more variability in selecting the sensitive attribute,

[MISSING_PAGE_EMPTY:8]

[MISSING_PAGE_FAIL:9]

### Comparison between AFCP and AFCP1

Both AFCP and AFCP1 outperform the benchmark approaches when applied to datasets with small sample sizes, each excelling in different scenarios. AFCP is better suited to situations where there is uncertainty regarding the presence of significant algorithmic bias, while AFCP1 is more effective when prior knowledge suggests that at least one attribute may be biased. For example, in Figure 3, which illustrates a case where one group (Color-Blue) is consistently biased, AFCP1 achieves slightly higher conditional coverage than AFCP. While AFCP exhibits slight undercoverage for the blue group with small sample sizes, it still outperforms the Marginal approach. The occasional inability of AFCP to select a sensitive attribute in small samples reflects the inherent challenges posed by limited datasets. When the method does not select an attribute, it often signifies a lack of sufficient evidence of algorithmic bias, making it reasonable to calibrate the prediction sets solely for marginal coverage.

## 4 Discussion

This paper presents a practical and statistically principled method to construct informative conformal prediction sets with valid coverage conditional on adaptively selected features. This approach balances efficiency and equalized coverage, which may be particularly useful in applications involving multiple sensitive attributes. While we believe it offers substantial benefits, a potential limitation of this method is that it does not always identify the most relevant sensitive attribute, particularly when working with limited sample sizes. Nevertheless, our empirical results are quite encouraging, demonstrating that AFCP effectively mitigates significant instances of algorithmic bias when the sample size is adequate. Moreover, our method is flexible, allowing for the integration of prior knowledge about which sensitive attributes might require protection against algorithmic bias.

This paper creates several opportunities for further work. Future research could focus on theoretically studying the conditions under which our method can be guaranteed to select the correct sensitive attribute with high probability. Additionally, future extensions might explore implementing different attribute selection procedures, such as those inspired by [47] - within our flexible AFCP framework to delve into the subtle trade-offs associated with different selection algorithms. Moreover, adapting our approach to accommodate different fairness criteria by adaptively adjusting the coverage rate target for each subgroup is another promising area of study. Extending our method to more efficiently handle scenarios with an extremely high number of possible classes is also worthwhile, potentially drawing inspiration from [43]. Furthermore, investigating extensions for classification tasks where the target variable is ordered could be both intriguing and practically useful. In such cases, a naive modification of our method would involve utilizing the discrete convex hull of all components instead of unions of subintervals on the right-hand side of Equation (8). However, developing a more refined approach would be a valuable contribution for future work. Future extensions of our work could focus on adapting to distributional shifts or enhancing the robustness and efficiency of our method under adversarial attacks or contaminated data, potentially drawing connections with [52, 53, 54, 55, 56]. Finally, extending our method to accommodate regression tasks with continuous outcomes presents additional computational challenges, but potential solutions could be inspired by [33].

The numerical experiments described in this paper were carried out on a computing cluster. Individual experiments, involving 1000 calibration samples and 500 test samples, required less than 25 minutes and 5GB of memory on a single CPU. The entire project took approximately 100 hours of computing time, and did not involve preliminary or failed experiments.

Software implementing the algorithms and data experiments are available online at [https://github.com/FionaZ3696/Adaptively-Fair-Conformal-Prediction](https://github.com/FionaZ3696/Adaptively-Fair-Conformal-Prediction).

## Acknowledgements

The authors thank anonymous reviewers for helpful comments, and the Center for Advanced Research Computing at the University of Southern California for providing computing resources. M. S. and Y. Z. were partly supported by NSF grant DMS 2210637. M. S. was also partly supported by an Amazon Research Award.

## References

* [1] A. Jones, B. Smith, and C. Johnson. Deep learning models for medical diagnosis. _Journal of Medical Imaging_, 7(2):021008, 2020.
* [2] J. K. Brown et al. Using machine learning for job application screening: A comparative analysis. In _International Conference on Artificial Intelligence in HR_, pages 12-21, 2021.
* [3] Jiaming Zeng, Berk Ustun, and Cynthia Rudin. Interpretable classification models for recidivism prediction. _J. R. Stat. Soc. (A)_, 180(3):689-722, 2017.
* [4] Arun K Kuchibhotla and Richard A Berk. Nested conformal prediction sets for classification with applications to probation data. _Ann. Appl. Stat._, 17(1):761-785, 2023.
* [5] Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In _IEEE Conference on Computer Vision and Pattern Recognition_, pages 427-436, 2015.
* [6] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In _Int. Conf. Mach. Learn._, pages 1321-1330. PMLR, 2017.
* [7] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In _Innovations in Theoretical Computer Science Conference_, page 214-226. Association for Computing Machinery, 2012.
* [8] Richard Zemel, Yu (Ledell) Wu, Kevin Swersky, Toniann Pitassi, and Cyntia Dwork. Learning fair representations. In _Int. Conf. Mach. Learn._, 2013.
* [9] Moritz Hardt, Eric Price, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In _Adv. Neural. Inf. Process. Syst._, volume 29, 2016.
* [10] Richard A Berk, Arun Kumar Kuchibhotla, and Eric Tchetgen Tchetgen. Improving fairness in criminal justice algorithmic risk assessments using optimal transport and conformal prediction sets. _Sociological Methods & Research_, 2021.
* [11] Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift. _Adv. Neural Inf. Process. Syst._, 32, 2019.
* [12] Harris Papadopoulos, Kostas Proedrou, Vladimir Vovk, and Alex Gammerman. Inductive confidence machines for regression. In _Eur. Conf. Mach. Learn._, pages 345-356, 2002.
* [13] Vladimir Vovk, Alex Gammerman, and Glenn Shafer. _Algorithmic learning in a random world_. Springer, 2005.
* [14] Jing Lei, Max G'Sell, Alessandro Rinaldo, Ryan J. Tibshirani, and Larry Wasserman. Distribution-free predictive inference for regression. _J. Am. Stat. Assoc._, 113(523):1094-1111, 2018.
* [15] Deborah Hellman. Measuring algorithmic fairness. _Virginia Law Review_, 106(4):811-866, 2020.
* [16] Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. _Big Data_, 5(2):153-163, 2017.
* [17] Yaniv Romano, Rina Foygel Barber, Chiara Sabatti, and Emmanuel Candes. With malice toward none: Assessing uncertainty via equalized coverage. _Harvard Data Science Review_, 2020.
* [18] Jianqing Fan, Xin Tong, Yanhui Wu, and Shunan Yao. Neyman-Pearson and equal opportunity: when efficiency meets fairness in classification. _arXiv preprint arXiv:2310.01009_, 2023.
* [19] Rina Foygel Barber, Emmanuel J Candes, Aaditya Ramdas, and Ryan J Tibshirani. The limits of distribution-free conditional predictive inference. _Information and Inference_, 10(2):455-482, 2021.

* [20] Yonghoon Lee and Rina Barber. Distribution-free inference for regression: discrete, continuous, and in between. _Adv. Neural Inf. Process. Syst._, 34:7448-7459, 2021.
* 178, 2023.
* [22] Ariane Marandon, Lihua Lei, David Mary, and Etienne Roquain. Adaptive novelty detection with false discovery rate guarantee. _Ann. Stat._, 52(1):157-183, 2024.
* [23] Ziyi Liang, Matteo Sesia, and Wenguang Sun. Integrative conformal p-values for out-of-distribution testing with labelled outliers. _J. R. Stat. Soc. (B)_, page qkad138, 2024.
* [24] Jing Lei, James Robins, and Larry Wasserman. Distribution-free prediction sets. _J. Am. Stat. Assoc._, 108(501):278-287, 2013.
* [25] Mauricio Sadinle, Jing Lei, and Larry Wasserman. Least ambiguous set-valued classifiers with bounded error levels. _J. Am. Stat. Assoc._, 114(525):223-234, 2019.
* [26] Aleksandr Podkopaev and Aaditya Ramdas. Distribution-free uncertainty quantification for classification under label shift. In _Uncertainty in Artificial Intelligence_, pages 844-853. PMLR, 2021.
* [27] Yaniv Romano, Matteo Sesia, and Emmanuel J. Candes. Classification with valid and adaptive coverage. _Adv. Neural Inf. Process. Syst._, 33, 2020.
* [28] Anastasios Nikolas Angelopoulos, Stephen Bates, Michael Jordan, and Jitendra Malik. Uncertainty sets for image classifiers using conformal prediction. In _Int. Conf. Learn. Represent._, 2021.
* [29] Jing Lei and Larry Wasserman. Distribution-free prediction bands for non-parametric regression. _J. R. Stat. Soc. (B)_, 76(1):71-96, 2014.
* [30] Yaniv Romano, Evan Patterson, and Emmanuel J Candes. Conformalized quantile regression. In _Adv. Neural Inf. Process. Syst._, pages 3538-3548, 2019.
* [31] Matteo Sesia and Yaniv Romano. Conformal prediction using conditional histograms. _Adv. Neural Inf. Process. Syst._, 34, 2021.
* [32] Bat-Sheva Einbinder, Yaniv Romano, Matteo Sesia, and Yanfei Zhou. Training uncertainty-aware classifiers with conformalized deep learning. In _Adv. Neural Inf. Process. Syst._, volume 35, 2022.
* [33] Ziyi Liang, Yanfei Zhou, and Matteo Sesia. Conformal inference is (almost) free for neural networks trained with early stopping. In _Int. Conf. Mach. Learn._, 2023.
* [34] Rina Foygel Barber, Emmanuel J Candes, Aaditya Ramdas, and Ryan J Tibshirani. Conformal prediction beyond exchangeability. _Ann. Stat._, 51(2):816-845, 2023.
* [35] Ryan J Tibshirani, Rina Foygel Barber, Emmanuel Candes, and Aaditya Ramdas. Conformal prediction under covariate shift. _Adv. Neural Inf. Process. Syst._, 32, 2019.
* [36] Isaac Gibbs and Emmanuel Candes. Conformal inference for online prediction with arbitrary distribution shifts. _arXiv preprint arXiv:2208.08401_, 2022.
* [37] Yachong Yang, Arun Kumar Kuchibhotla, and Eric Tchetgen Tchetgen. Doubly robust calibration of prediction sets under covariate shift. _J. R. Stat. Soc. (B)_, page qkae009, 2024.
* [38] Ziyi Liang, Tianmin Xie, Xin Tong, and Matteo Sesia. Structured conformal inference for matrix completion with applications to group recommender systems. _arXiv preprint arXiv:2404.17561_, 2024.
* [39] Charles Lu, Andreanne Lemay, Ken Chang, Katharina Hoebel, and Jayashree Kalpathy-Cramer. Fair conformal predictors for applications in medical imaging. _AAAI Conference on Artificial Intelligence_, 36:12008-12016, 06 2022.

* Wang et al. [2023] Fangxin Wang, Lu Cheng, Ruocheng Guo, Kay Liu, and Philip S. Yu. Equal opportunity of coverage in fair regression. In _Adv. Neural Inf. Process. Syst_, 2023.
* Liu et al. [2022] Meichen Liu, Lei Ding, Dengdeng Yu, Wulong Liu, Linglong Kong, and Bei Jiang. Conformalized fairness via quantile regression. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Adv. Neural Inf. Process. Syst._, 2022.
* Vovk et al. [2003] Vladimir Vovk, David Lindsay, Ilia Noureddinov, and Alex Gammerman. Mondrian confidence machine. Technical report, Royal Holloway, University of London, 2003. On-line Compression Modelling project.
* Ding et al. [2023] Tiffany Ding, Anastasios Nikolas Angelopoulos, Stephen Bates, Michael Jordan, and Ryan Tibshirani. Class-conditional conformal prediction with many classes. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023. URL [https://openreview.net/forum?id=mYz6ApeU4J](https://openreview.net/forum?id=mYz6ApeU4J).
* Lofstrom et al. [2015] Tuve Lofstrom, Henrik Bostrom, Henrik Linusson, and Ulf Johansson. Bias reduction through conditional conformal prediction. _Intell. Data Anal._, 19(6):1355-1375, nov 2015. ISSN 1088-467X.
* Jung et al. [2023] Christopher Jung, Georgy Noarov, Ramya Ramalingam, and Aaron Roth. Batch multivalid conformal prediction. In _Int. Conf. Learn. Represent._, 2023.
* Gibbs et al. [2023] Isaac Gibbs, John J Cherian, and Emmanuel J Candes. Conformal prediction with conditional guarantees. _arXiv preprint arXiv:2305.12616_, 2023.
* Cherian and Candes [2023] John J. Cherian and Emmanuel J. Candes. Statistical inference for fairness auditing. _arXiv preprint arXiv:2305.03712_, 2023.
* Jin and Ren [2024] Ying Jin and Zhimei Ren. Confidence on the focal: Conformal prediction with selection-conditional coverage. _arXiv preprint arXiv:2403.03868_, 2024.
* Rajkovic [1997] Vladislav Rajkovic. Nursery. UCI Machine Learning Repository, 1997. DOI: [https://doi.org/10.24432/C5P88W](https://doi.org/10.24432/C5P88W), License: CC-BY 4.0.
* Larson et al. [2016] Jeff Larson, Surya Mattu, Lauren Kirchner, and Julia Angwin. How we analyzed the compas recidivism algorithm. [https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm), 2016.
* Becker and Kohavi [1996] Barry Becker and Ronny Kohavi. Adult. UCI Machine Learning Repository, 1996. DOI: [https://doi.org/10.24432/C5XW20](https://doi.org/10.24432/C5XW20), License: CC-BY 4.0.
* Einbinder et al. [2022] Bat-Sheva Einbinder, Stephen Bates, Anastasios N Angelopoulos, Asaf Gendler, and Yaniv Romano. Conformal prediction is robust to label noise. _arXiv preprint arXiv:2209.14295_, 2022.
* Yan et al. [2024] Ge Yan, Yaniv Romano, and Tsui-Wei Weng. Provably robust conformal prediction with improved efficiency. In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=BWAhEjXjeG](https://openreview.net/forum?id=BWAhEjXjeG).
* Gendler et al. [2022] Asaf Gendler, Tsui-Wei Weng, Luca Daniel, and Yaniv Romano. Adversarially robust conformal prediction. In _International Conference on Learning Representations_, 2022. URL [https://openreview.net/forum?id=9L1BsI4wP1H](https://openreview.net/forum?id=9L1BsI4wP1H).
* Feldman et al. [2023] Shai Feldman, Bat-Sheva Einbinder, Stephen Bates, Anastasios N. Angelopoulos, Asaf Gendler, and Yaniv Romano. Conformal prediction is robust to dispersive label noise. In Harris Papadopoulos, Khuong An Nguyen, Henrik Bostrom, and Lars Carlsson, editors, _Proceedings of the Twelfth Symposium on Conformal and Probabilistic Prediction with Applications_, volume 204 of _Proceedings of Machine Learning Research_, pages 624-626. PMLR, 13-15 Sep 2023. URL [https://proceedings.mlr.press/v204/feldman23a.html](https://proceedings.mlr.press/v204/feldman23a.html).
* Sesia et al. [2024] Matteo Sesia, Y. X. Rachel Wang, and Xin Tong. Adaptive conformal classification with noisy labels, 2024. URL [https://arxiv.org/abs/2309.05092](https://arxiv.org/abs/2309.05092).
* Shafer and Vovk [2008] Glenn Shafer and Vladimir Vovk. A tutorial on conformal inference. _J. Mach. Learn. Res._, 9(3), 2008.

* [58] Tuve Lofstrom, Henrik Bostrom, Henrik Linusson, and Ulf Johansson. Bias reduction through conditional conformal prediction. _Intell. Data Anal._, 19(6):1355-1375, 11 2015.
* [59] COMPAS Data Set. [https://github.com/propublica/compas-analysis](https://github.com/propublica/compas-analysis). Accessed: July, 2024.

Review of Existing Conformal Classification Methods

Algorithm A1 outlines the standard approach for constructing conformal prediction sets with equalized coverage with respect to a fixed list of protected attributes [17]. In the special case where the list of protected attributes is empty, this method reduces to the standard approach for constructing prediction sets with marginal coverage.

```
1:Input: calibration data \(\mathcal{D}\); test point with features \(X_{n+1}\); list of protected attributes \(A\);
2: pre-trained classifier \(\hat{f}\); pre-defined rule for computing nonconformity scores;
3: nominal level \(\alpha\in(0,1)\).
4: Define the calibration subset \[\mathcal{I}(X_{n+1},A)=\left\{i\in[n]:\phi(X_{i},A)=\phi(X_{n+1},A)\right\}.\]
5:for\(y\in[L]\)do
6: Compute the nonconformity scores \(\hat{S}^{y}_{i}\) for \(i\in\mathcal{I}(X_{n+1},A)\cup\left\{(X_{n+1},y)\right\}\) using \(\hat{f}\).
7: Compute the conformal p-value: \[\hat{u}^{y}(X_{n+1})=\frac{1+|i\in\mathcal{I}(X_{n+1},A):\hat{S}^{y}_{i}\leq \hat{S}^{y}_{n+1}|}{1+|\mathcal{I}(X_{n+1},A)|}.\]
8:endfor
9: Construct a prediction set using \(\hat{C}(X_{n+1})=\left\{y\in[L]:\hat{u}^{y}(X_{n+1})\geq\alpha\right\}\).
10:Output: a prediction set \(\hat{C}(X_{n+1})\).
```

**Algorithm A2** Conformal p-value with equalized FPR for fixed protected attributes.

In the context of outlier detection, Algorithm A2 reviews the standard approach for computing conformal p-values achieving valid false positive rate (FPR) control conditional a fixed list of protected attributes.

```
1:Input: calibration data \(\mathcal{D}\); test point \(Z_{n+1}\); list of protected attributes \(A\);
2: pre-trained one-class classifier \(\hat{f}\); pre-defined rule for computing nonconformity scores;
3: nominal level \(\alpha\in(0,1)\).
4: Define the calibration subset \[\mathcal{I}(Z_{n+1},A)=\left\{i\in[n]:\phi(Z_{i},A)=\phi(Z_{n+1},A)\right\}.\]
5: Compute the nonconformity scores \(\hat{S}_{i}\) for \(i\in\mathcal{I}(Z_{n+1},A)\cup\left\{Z_{n+1}\right\}\) using \(\hat{f}\).
6: Compute the conformal p-value: \[\hat{u}(Z_{n+1})=\frac{1+|i\in\mathcal{I}(Z_{n+1},A):\hat{S}_{i}\leq\hat{S}_{n +1}|}{1+|\mathcal{I}(Z_{n+1},A)|}.\]
7:Output: a conformal p-value \(\hat{u}(Z_{n+1})\).
```

**Algorithm A1** Conformal classification with equalized coverage for fixed protected attributes.

## Appendix B2 Methodology Extension: AFCP with Multiple Selected Attributes

This section introduces an extension of AFCP that enables the selection of more than one sensitive attribute. For simplicity, we focus on the selection of up to two attributes. The methodology for selecting more than two attributes can be extended in a similar manner, as explained later.

### Automatic Multiple Attribute Selections

Given a pre-trained classification model, an independent calibration data set \(\mathcal{D}\) with size \(n\), and a test point \(Z_{n+1}=(X_{n+1},Y_{n+1})\) with an unknown label \(Y_{n+1}\), Algorithm 1 in Section 2.1 introduces

[MISSING_PAGE_FAIL:16]

```
1:Input: calibration data \(\mathcal{D}\); test point with features \(X_{n+1}\); list of \(K\) sensitive attributes;
2: pre-trained classifier \(\hat{f}\); pre-defined rule for computing nonconformity scores;
3: nominal level \(\alpha\in(0,1)\).
4:for\(y\in[L]\)do
5: Select attribute(s) \(\hat{A}(X_{n+1},y)\) by applying Algorithm A3 with placeholder label \(y\).
6: Construct \(\hat{C}(X_{n+1},A)\) by applying Algorithm A1 with protected attribute(s) \(A=\hat{A}(X_{n+1},y)\).
7:endfor
8:for\(\ell\in\cup_{y\in[L]}\hat{A}(X_{n+1},y)\)do
9: Construct \(\hat{C}^{\text{eq}}(X_{n+1},\ell)\) by applying Algorithm A1 with protected attribute \(\{\ell\}\).
10:endfor
11: Construct \(\hat{C}^{\text{m}}(X_{n+1})\) by applying Algorithm A1 without protected attributes.
12: Define the final selected attribute(s) \(\hat{A}(X_{n+1})\) using Equation (A10).
13: Define the final prediction set \(\hat{C}(X_{n+1})\) using Equation (A11).
14:Output: \(\hat{A}(X_{n+1})\) and \(\hat{C}(X_{n+1})\).
```

**Algorithm A4** AFCP with two selected attributes.

## Appendix A3 Methodology Extension: AFCP with Label Conditional Coverage

This section extends the AFCP method with adaptive equalized coverage that is also conditional on the true test label. We focus on the main implementation of AFCP, where it can select up to one sensitive attribute.

First, the label conditional counterparts of the marginal coverage, the exhaustive equalized coverage, and the adaptive equalized coverage are defined. The label-conditional counterpart of marginal coverage is defined as:

\[\mathbb{P}[Y_{n+1}\in\hat{C}(X_{n+1})\mid Y_{n+1}=y]\geq 1-\alpha,\quad \forall y\in[L].\] (A12)

Intuitively, this coverage ensures that the prediction sets constructed are valid for each group with the test label \(y\) for all possible values of \(y\in[L]\). This coverage offers a stronger assurance than marginal coverage in classification contexts. However, it overlooks scenarios where groups, identified by feature attributes, may suffer adverse effects from prediction biases. To address these concerns, one can aim for label-conditional exhaustive equalized coverage [17], defined as:

\[\mathbb{P}[Y_{n+1}\in\hat{C}(X_{n+1})\mid Y_{n+1}=y,\phi(X_{n+1},[K])]\geq 1 -\alpha,\quad\forall y\in[L].\] (A13)

Achieving this label conditional exhaustive equalized coverage involves applying the standard conformal classification method (outlined in Algorithm A1) separately within each of the groups characterized by every possible combination of sensitive attributes and test labels. Hence, this approach can become overly conservative when a large number of sensitive attributes or response labels are presented.

Our AFCP method strikes a balance between the two approaches to achieve label-conditional adaptive equalized coverage:

\[\mathbb{P}[Y_{n+1}\in\hat{C}(X_{n+1})\mid Y_{n+1}=y,\phi(X_{n+1},\hat{A}(X_{n+ 1}))]\geq 1-\alpha,\quad\forall y\in[L],\] (A14)

which guarantees that the true test label is contained within the prediction sets with high probability for the groups defined by the selected attribute \(\hat{A}(X_{n+1})\) and the test label \(y\) for every \(y\in[L]\).

#### a3.1 Automatic Attribute Selection

Given a pre-trained classification model and an independent calibration data set \(\mathcal{D}\) with size \(n\), for each placeholder label \(y\in[L]\) for \(Y_{n+1}\), the AFCP method with label-conditional adaptive equalized coverage (A14) selects the sensitive attribute \(\hat{A}(X_{n+1},y)\) by simply applying Algorithm 1 using the _label-restricted calibration data_\(\mathcal{D}_{y}=\{i\in[n]:Y_{i}=y\}\). After repeating this process for each \(y\in[L]\), the final selected attribute \(\hat{A}(X_{n+1})\) is again given by

\[\hat{A}(X_{n+1})=\cap_{y\in[L]}\hat{A}(X_{n+1},y).\] (A15)

#### a3.2 Constructing the Adaptive Prediction Sets

After selecting \(\hat{A}(X_{n+1},y)\) by applying Algorithm 1 with placeholder label \(y\) based on the label-restricted calibration data \(\mathcal{D}_{y}=\{i\in[n]:Y_{i}=y\}\), and selecting a sensitive attribute \(\hat{A}(X_{n+1})\), AFCP constructs an adaptive prediction set for \(Y_{n+1}\) that satisfies (A14) as follows.

For each \(y\in[L]\), it firstly construct a conformal prediction set \(\hat{C}^{\text{tc}}(X_{n+1},y)\) by applying Algorithm A1 using the label-restricted calibration set \(\mathcal{D}_{y}\) without considering protected attributes. Then, it constructs another conformal prediction set \(\hat{C}(X_{n+1},\hat{A}(X_{n+1},y))\) with equalized coverage for the group identified by both the selected attribute \(\hat{A}(X_{n+1},y)\)_and label_\(y\). This can be achieved by applying Algorithm A1 based on a subset of the calibration samples indexed by \(\mathcal{I}(X_{n+1},y)=\{i\in\mathcal{D}_{y}:\phi(X_{i},\hat{A}(X_{n+1},y))= \phi(X_{n+1},\hat{A}(X_{n+1},y))\}\). Lemma A1 shows that, for any given placeholder label, the prediction set constructed in this step satisfies the label conditional adaptive equalized coverage as long as the selected variable using that placeholder label is fixed.

**Lemma A1**.: _If \(\{(X_{i},Y_{i})\}_{i=1}^{n+1}\) are exchangeable and the selected attribute \(\hat{A}(X_{n+1},y)\) is fixed for some placeholder label \(y\), then, the prediction set \(\hat{C}(X_{n+1},\hat{A}(X_{n+1},y))\) constructed by calibrating on \(\mathcal{I}(X_{n+1},y)\) satisfies_

\[\mathbb{P}[Y_{n+1}\in\hat{C}(X_{n+1},\hat{A}(X_{n+1},y))\mid Y_{n+1}=\tilde{y},\phi(X_{n+1},\hat{A}(X_{n+1},y))]\geq 1-\alpha,\]

_for any \(\tilde{y}\in[L]\)._

Lastly, the final AFCP prediction set is obtained by:

\[\hat{C}(X_{n+1})=\left\{\cup_{y=1}^{L}\hat{C}^{\text{tc}}(X_{n+1},y)\right\} \cup\left\{\cup_{y=1}^{L}\hat{C}(X_{n+1},\hat{A}(X_{n+1},y))\right\}.\] (A16)

The procedures to form AFCP prediction sets with the label-conditional adaptive equalized coverage (A14) is summarized in Algorithm A5.

```
1:Input: calibration data \(\mathcal{D}\); test point with features \(X_{n+1}\); list of \(K\) sensitive attributes;
2: pre-trained classifier \(\hat{f}\); pre-defined rule for computing nonconformity scores;
3: nominal level \(\alpha\in(0,1)\).
4:for\(y\in[L]\)do
5: Define the label-restricted calibration set \(\mathcal{D}_{y}=\{i\in[n]:Y_{i}=y\}\).
6: Select an attribute \(\hat{A}(X_{n+1},y)\) by applying Algorithm 1 with placeholder label \(y\) on \(\mathcal{D}_{y}\).
7: Construct \(\hat{C}(X_{n+1},\hat{A}(X_{n+1},y))\) by applying Algorithm A1 with protected attributes \(\hat{A}(X_{n+1},y)\) on \(\mathcal{D}_{y}\).
8: Construct \(\hat{C}^{\text{tc}}(X_{n+1},y)\) by applying Algorithm A1 on \(\mathcal{D}_{y}\).
9:endfor
10: Define the final selected attribute \(\hat{A}(X_{n+1})\) using Equation (A15).
11: Define the final prediction set \(\hat{C}(X_{n+1})\) using Equation (A16).
12:Output: \(\hat{A}(X_{n+1})\) and \(\hat{C}(X_{n+1})\).
```

**Algorithm A5** AFCP with label-conditional adaptive equalized coverage (A14).

**Theorem A2**.: _If \(\{(X_{i},Y_{i})\}_{i=1}^{n+1}\) are exchangeable random samples, then the conformal prediction set \(\hat{C}(X_{n+1})\) and the selected attribute \(\hat{A}(X_{n+1})\) output by Algorithm A5 satisfy the label-conditional adaptive equalized coverage defined in (A14)._

### Methodology Extension: AFCP for Outlier Detection

Consider a dataset \(\mathcal{D}=\{Z_{i}\}_{i=1}^{n}\) containing \(n\) sample points drawn exchangeably from an unknown distribution \(P_{Z}\). Consider an additional test point \(Z_{n+1}\). In the outlier detection problems, our AFCP method aims to study whether \(Z_{n+1}\sim P_{Z}\) by constructing a valid conformal \(\mathfrak{p}\)-value \(\hat{u}(Z_{n+1})\) conditional on the group identified by the selected attribute \(\hat{A}(Z_{n+1})\), that is:

\[\mathbb{P}[\hat{u}(Z_{n+1})\leq\alpha\mid\phi(Z_{n+1},\hat{A}(Z_{n+1}))]\leq\alpha,\] (A17)for any \(\alpha\in(0,1)\). Intuitively, this guarantees that, on groups defined by the selected attribute \(\hat{A}(Z_{n+1})\), the conformal p-value is super-uniform, therefore controlling the FPR (the probability of rejecting the null hypothesis that \(Z_{n+1}\) is an inlier when it is true) below \(\alpha\).

#### a4.1 Automatic Attribute Selection

Given a pre-trained one-class classifier \(\hat{f}\), an independent calibration dataset \(\mathcal{D}\), and a test point \(Z_{n+1}\), our method selects a sensitive attribute \(\hat{A}(Z_{n+1})\in\{\emptyset,\{1\},\ldots,\{K\}\}\) according to the following _leave-one-out_ procedure. Sometimes, no attribute may be selected, as denoted by \(\hat{A}(Z_{n+1})=\emptyset\).

Define an _augmented_ calibration set \(\mathcal{D}^{\prime}:=\mathcal{D}\cup\{Z_{n+1}\}\). For each \(i\in[n+1]\), define also the leave-one-out set \(\mathcal{D}^{\prime}_{i}:=\mathcal{D}^{\prime}\setminus\{Z_{i}\}\). Then, for each \(i\in[n+1]\), we compute a conformal p-value \(\hat{u}^{\text{loo}}(Z_{i})\) for \(Z_{i}\) using the data in \(\mathcal{D}^{\prime}_{i}\) to test if \(Z_{i}\) is an outlier. This can be accomplished by running Algorithm A2, using \(\mathcal{D}^{\prime}_{i}\) as the calibration data and \(Z_{i}\) as the test point, and with the convention that smaller nonconformity scores suggest \(Z_{i}\) is more likely to be an outlier. Any nonconformity scores can be utilized here. For concreteness, we focus on using the adaptive conformity scores proposed by [27]. Small \(\hat{u}^{\text{loo}}(Z_{i})\) provides stronger evidence to reject the null hypothesis \(H_{0,i}:Z_{i}\) is an inlier. Let \(E_{i}\) denote the binary indicator of whether \(H_{0,i}\) is rejected:

\[E_{i}:=\mathbf{1}\{\hat{u}^{\text{loo}}(Z_{i})\leq\alpha\}. \tag{18}\]

After evaluating \(E_{i}\) for all \(i\in[n+1]\), we will assess the leave-one-out FPR for the worst-off group identified by each sensitive attribute \(k\in[K]\). That is, we evaluate

\[\delta_{k}:=\max_{m\in[M_{k}]}\frac{\sum_{i=1}^{n+1}E_{i}\cdot\mathbf{1}\{ \phi(Z_{i},\{k\})=m\}}{\sum_{i=1}^{n+1}\mathbf{1}\{\phi(Z_{i},\{k\})=m\}}. \tag{19}\]

Intuitively, \(\delta_{k}\) denotes the maximum FPR across all groups identified by the \(k\)-th attribute, as estimated by the leave-one-out simulation carried out under the assumption that \(Z_{n+1}\) is an inlier. Large values of \(\delta_{k}\) suggest that the \(k\)-th attribute may be a sensitive attribute corresponding to at least one group suffering from algorithmic bias.

To assess whether there is evidence of significant algorithmic bias, we can perform a statistical test for the null hypothesis that no algorithmic bias exists. We define:

\[\hat{q}:=\max_{k\in[K]}\delta_{k}, \tag{20}\]

and carry out a one-sided t-test for the null hypothesis \(H_{0}:\hat{q}\leq\alpha\) against \(H_{1}:\hat{q}>\alpha\).

If \(H_{0}\) is rejected (at any desired level, such as 5%), we conclude there exists a group suffering from significant algorithmic bias, and we identify the corresponding attribute through

\[\hat{A}(Z_{n+1})=\{\operatorname*{arg\,max}_{k\in[K]}\delta_{k}\}. \tag{21}\]

Otherwise, we set \(\hat{A}(Z_{n+1})=\emptyset\), which corresponds to selecting no attribute. See Algorithm A6 for an outline of this procedure.

#### a4.2 Evaluating the Adaptive Conformal P-Value

After selecting either a single attribute or an empty set \(\hat{A}(X_{n+1})\) that corresponds to at least one group suffering from algorithmic bias, the next step of our AFCP method is to compute an adaptive conformal p-value for testing whether \(Z_{n+1}\) is an outlier that satisfies (A17). This can be simply achieved by applying the standard conformal method outlined in Algorithm A2 based on a restricted calibration sample indexed by \(\mathcal{I}(\hat{A}(Z_{n+1}))=\{i\in[n]:\phi(Z_{i},\hat{A}(Z_{n+1}))=\phi(Z_{ n+1},\hat{A}(Z_{n+1}))\}\), See Algorithm A7 for a summary of AFCP for outlier detection tasks.

**Theorem A3**.: _If \(\{Z_{i}\}_{i=1}^{n+1}\) are exchangeable random samples, the conformal p-value \(\hat{u}(Z_{n+1})\) and the selected attribute \(\hat{A}(Z_{n+1})\) output by Algorithm A7 satisfy (A17)._```
1:Input: calibration data \(\mathcal{D}\); test point \(Z_{n+1}\); list of \(K\) sensitive attributes;
2: pre-trained one-class classifier \(\hat{f}\); pre-defined rule for computing nonconformity scores;
3: nominal level \(\alpha\in(0,1)\).
4: Define the augmented data set \(\mathcal{D}^{\prime}=\mathcal{D}\cup\{Z_{n+1}\}\).
5:for\(i\in[n+1]\)do
6: Pretend that \(Z_{i}\) is the test point and \(\mathcal{D}^{\prime}\setminus\{Z_{i}\}\) is the calibration set.
7: Compute a conformal p-value \(\hat{u}^{\text{loo}}(Z_{i})\).
8: Evaluate the false positive indicator \(E_{i}\) using (A18).
9:endfor
10: Compute \(\hat{q}\) using (A20).
11: Perform a one-sided test for \(H_{0}:\hat{q}\leq\alpha\) vs. \(H_{1}:\hat{q}>\alpha\).
12: Select the attribute \(\hat{A}(Z_{n+1})\) using (A21) if \(H_{0}\) is rejected, else set \(\hat{A}(Z_{n+1})=\emptyset\).
13:Output: \(\hat{A}(Z_{n+1})\), either a selected sensitive attribute or an empty set.
```

**Algorithm A6** Automatic attribute selection for outlier detection.

#### a4.3 AFCP for Outlier Detection with Multiple Selected Attributes

AFCP for outlier detection problems can be readily extended to select \(J\) sensitive attributes where \(J>1\). This is achieved by repeatedly applying the single attribute selection procedure described in Algorithm A6\(J\) times. Each time, the algorithm selects a (possibly empty) subset of attributes \(\hat{A}(Z_{n+1})^{j}\) from the list of sensitive attributes excluding the previously selected attribute \(\hat{A}(Z_{n+1})^{j-1}\). The final set of selected attributes is given by \(\hat{A}(Z_{n+1})=\cup_{j=1}^{J}\hat{A}(Z_{n+1})^{j}\). See Algorithm A8 for an outline of this procedure.

```
1:Input: calibration data \(\mathcal{D}\); test point \(Z_{n+1}\); list of \(K\) sensitive attributes;
2: pre-trained one-class classifier \(\hat{f}\); pre-defined rule for computing nonconformity scores;
3: nominal level \(\alpha\in(0,1)\).
4: Define the augmented data set \(\mathcal{D}^{\prime}=\mathcal{D}\cup\{Z_{n+1}\}\).
5:for\(i\in[n+1]\)do
6: Pretend that \(Z_{i}\) is the test point and \(\mathcal{D}^{\prime}\setminus\{Z_{i}\}\) is the calibration set.
7: Compute a conformal p-value \(\hat{u}^{\text{loo}}(Z_{i})\).
8: Evaluate the false positive indicator \(E_{i}\) using (A18).
9:endfor
10: Compute \(\hat{q}\) using (A20).
11: Perform a one-sided test for \(H_{0}:\hat{q}\leq\alpha\) vs. \(H_{1}:\hat{q}>\alpha\).
12: Select the attribute \(\hat{A}(Z_{n+1})\) using (A21) if \(H_{0}\) is rejected, else set \(\hat{A}(Z_{n+1})=\emptyset\).
13:Output: \(\hat{A}(Z_{n+1})\), either a selected sensitive attribute or an empty set.
```

**Algorithm A7** AFCP for outlier detection.

After selecting a set of attributes \(\hat{A}(Z_{n+1})\), which might be empty or include one or more sensitive attributes, AFCP constructs an adaptive conformal p-value satisfying (A17). This can be easily achieved by applying Algorithm A2 with protected attributes \(\hat{A}(Z_{n+1})\). Algorithm A9 summarizes the AFCP implementation for outlier detection that allows selecting multiple protected attributes.

**Theorem A4**.: _If \(\{Z_{i}\}_{i=1}^{n+1}\) are exchangeable random samples, the conformal p-value \(\hat{u}(Z_{n+1})\) and selected attributes \(\hat{A}(Z_{n+1})\) output by Algorithm A9 satisfy (A17)._```
1:Input: calibration data \(\mathcal{D}\); test point \(Z_{n+1}\); list of \(K\) sensitive attributes;
2: pre-trained one-class classifier \(\hat{f}\); pre-defined rule for computing nonconformity scores;
3: nominal level \(\alpha\in(0,1)\); number of selected attributes \(J\).
4: Select up to \(J\) sensitive attributes \(\hat{A}(Z_{n+1})\) by applying Algorithm A8.
5: Evaluate \(\hat{u}(Z_{n+1})\) by applying Algorithm A2 with protected attribute \(\hat{A}(Z_{n+1})\).
6:Output: \(\hat{u}(Z_{n+1})\).
```

**Algorithm A9** AFCP for outlier detection with multiple selected attributes.

## Appendix A5 Mathematical Proofs

Proof of Theorem 1.: Consider an imaginary oracle that has access to the true value of \(Y_{n+1}\). Denote \(\hat{A}^{\text{o}}(X_{n+1},Y_{n+1})\) as the sensitive attribute selected by this oracle by applying Algorithm 1 with the true \(Y_{n+1}\) instead of a placeholder label. Let \(\hat{C}^{\text{o}}(X_{n+1},\hat{A}^{\text{o}}(X_{n+1},Y_{n+1}))\) represent the corresponding output prediction set by applying Algorithm A1 with the protected attribute \(\hat{A}^{\text{o}}(X_{n+1},Y_{n+1})\). Consider also \(\hat{C}^{\text{m}}(X_{n+1})\), the standard prediction set with marginal coverage (1).

The main idea of our proof is to connect the output prediction set \(\hat{C}(X_{n+1})\) and selected attribute \(\hat{A}(X_{n+1})\) from Algorithm 2 to those of the imaginary oracle described above. Throughout this proof, we adopt the convention that \(\phi(X_{n+1},\emptyset)=0\).

To establish this connection, note that the attribute \(\hat{A}(X_{n+1})\) selected by Algorithm 2 is either empty, \(\hat{A}(X_{n+1})=\emptyset\), or a singleton, \(\hat{A}(X_{n+1})=\{k\}\) for some \(k\in[K]\). In the latter case, \(\hat{A}(X_{n+1})=\hat{A}(X_{n+1},\tilde{y})\), \(\forall\tilde{y}\in[L]\), and thus \(\hat{A}(X_{n+1})=\hat{A}^{\text{o}}(X_{n+1},Y_{n+1})\) almost-surely. Therefore,

\[\mathbb{P}[Y_{n+1}\in\hat{C}(X_{n+1})\mid\phi(X_{n+1},\hat{A}(X_{n +1}))]\] (A22) \[\geq\min\Biggl{\{}\mathbb{P}[Y_{n+1}\in\hat{C}(X_{n+1})\mid\phi(X _{n+1},\emptyset)],\] \[\qquad\qquad\mathbb{P}[Y_{n+1}\in\hat{C}(X_{n+1})\mid\phi(X_{n+1},\hat{A}^{\text{o}}(X_{n+1},Y_{n+1

[MISSING_PAGE_FAIL:22]

and the second element respectively, \(A(X_{n+1})=\ell_{1}\) or \(\hat{A}(X_{n+1})=\ell_{2}\) with probability \(1\). Then,

\[\begin{split}&\mathbb{P}[Y_{n+1}\in\cup_{\ell\in\hat{A}^{\text{o}}(X_ {n+1},Y_{n+1})}\hat{C}^{\text{eq}}(X_{n+1},\ell)\mid\phi(X_{n+1},\dot{k}(X_{n+1 }))]\\ &\qquad\geq\min\Biggl{\{}\mathbb{P}[Y_{n+1}\in\cup_{\ell\in\hat{A}^ {\text{o}}(X_{n+1},Y_{n+1})}\hat{C}^{\text{eq}}(X_{n+1},\ell)\mid\phi(X_{n+1}, \ell_{1})],\\ &\qquad\qquad\qquad\mathbb{P}[Y_{n+1}\in\cup_{\ell\in\hat{A}^{ \text{o}}(X_{n+1},Y_{n+1})}\hat{C}^{\text{eq}}(X_{n+1},\ell)\mid\phi(X_{n+1}, \ell_{2})]\Biggr{\}}\\ &\qquad\geq\min\Biggl{\{}\mathbb{P}[Y_{n+1}\in\hat{C}^{\text{eq}} (X_{n+1},\ell_{1})\mid\phi(X_{n+1},\ell_{1})],\\ &\qquad\qquad\qquad\mathbb{P}[Y_{n+1}\in\hat{C}^{\text{eq}}(X_{n+ 1},\ell_{2})\mid\phi(X_{n+1},\ell_{2})]\Biggr{\}}\\ &\qquad\geq\min\{1-\alpha,1-\alpha\},\end{split}\] (A24)

where the inequality of the last line of (A24) is proved in [17] with fixed attribute \(\ell_{1}\) and \(\ell_{2}\) respectively. Lastly, realize that \(\cup_{\ell\in\hat{A}^{\text{o}}(X_{n+1},Y_{n+1})}\hat{C}^{\text{eq}}(X_{n+1}, \ell)\subseteq\cup_{\ell\in\cup_{y=1}^{L}\hat{A}(X_{n+1},y)}\hat{C}^{\text{ eq}}(X_{n+1},\ell)\subseteq\hat{C}(X_{n+1})\) almost-surely, the proof is completed. 

Proof of Theorem a.: Similar to the proof of Theorem 1, consider an imaginary oracle that has access to the true value of \(Y_{n+1}\). Denote \(\hat{A}^{\text{o}}(X_{n+1},Y_{n+1})\) as the sensitive attribute selected by this oracle by applying Algorithm 1 based on a subset of the calibration data indexed by \(\mathcal{D}_{Y_{n+1}}=\{i\in[n]:Y_{i}=Y_{n+1}\}\). Let \(\hat{C}^{\text{o}}(X_{n+1},\hat{A}^{\text{o}}(X_{n+1},Y_{n+1}))\) represent the output prediction set by applying Algorithm A1 with the protected attribute \(\hat{A}^{\text{o}}(X_{n+1},Y_{n+1})\). Consider also \(\hat{C}^{\text{tc}}(X_{n+1},Y_{n+1})\), the prediction set with label-conditional coverage obtained by running Algorithm A1 using \(\mathcal{D}_{Y_{n+1}}\) without any protected attributes.

To establish this connection between the output prediction set \(\hat{C}(X_{n+1})\) and the selected attribute \(\hat{A}(X_{n+1})\) of Algorithm A5 and these of the imaginary oracle, note that the selected attribute \(\hat{A}(X_{n+1})\) must be \(\hat{A}(X_{n+1})=\emptyset\) or \(\hat{A}(X_{n+1})=\hat{A}^{\text{o}}(X_{n+1},Y_{n+1})\).

Therefore, for any \(y\in[L]\),

\[\mathbb{P}[Y_{n+1}\in\hat{C}(X_{n+1})\mid Y_{n+1}=y,\phi(X_{n+1},\hat{A}(X_{n +1}))]\]

\[\geq\min\Biggl{\{}\mathbb{P}(Y_{n+1}\in\hat{C}(X_{n+1})\mid Y_{n+1}=y,\phi(X_{ n+1},\emptyset)),\]

\[\mathbb{P}(Y_{n+1}\in\hat{C}(X_{n+1})\mid Y_{n+1}=y,\phi(X_{n+1},\hat{A}^{ \text{o}}(X_{n+1},Y_{n+1})))\Biggr{\}}\]

\[=\min\Biggl{\{}\mathbb{P}(Y_{n+1}\in\hat{C}(X_{n+1})\mid Y_{n+1}=y),\]

\[\mathbb{P}(Y_{n+1}\in\hat{C}(X_{n+1})\mid Y_{n+1}=y,\phi(X_{n+1},\hat{A}^{ \text{o}}(X_{n+1},Y_{n+1})))\Biggr{\}}\]

\[\geq\min\Biggl{\{}\mathbb{P}(Y_{n+1}\in\hat{C}^{\text{tc}}(X_{n+1},Y_{n+1}) \mid Y_{n+1}=y),\]

\[\mathbb{P}(Y_{n+1}\in\hat{C}^{\text{o}}(X_{n+1},\hat{A}^{\text{o}}(X_{n+1},Y_{n +1}))\mid Y_{n+1}=y,\phi(X_{n+1},\hat{A}^{\text{o}}(X_{n+1},Y_{n+1})))\Biggr{\}},\] (A25)

where the last inequality follows from the facts that \(\hat{C}^{\text{tc}}(X_{n+1},Y_{n+1})\subseteq\hat{C}(X_{n+1})\) and \(\hat{C}^{\text{o}}(X_{n+1},\hat{A}^{\text{o}}(X_{n+1},Y_{n+1}))\subseteq\hat{C}( X_{n+1})\) almost-surely.

Next, we only need to separately lower-bound by \(1-\alpha\) the two terms on the right-hand-side of (A25). The first part of the remaining task is trivial. It is already well-known that \(\mathbb{P}(Y_{n+1}\in\hat{C}^{\text{tc}}(X_{n+1},Y_{n+1})\mid Y_{n+1}=y)\geq 1-\alpha\); see [13, 47, 58].

Next, we prove the lower bound for the second term. Fix any \(y\in[L]\), and assume \(Y_{n+1}=y\). First, note that the oracle-selected attribute \(\hat{A}^{\text{o}}(X_{n+1},Y_{n+1})\) is invariant to any permutations of the exchangeable data indexed by \(\mathcal{D}_{y}\cup\{X_{n+1},y\}\). Therefore, the data points are exchangeable conditional on the group identified by the oracle-selected attribute \(\hat{A}^{\text{o}}(X_{n+1},Y_{n+1})\). This means that we can imagine the protected attribute \(\hat{A}^{\text{o}}(X_{n+1},Y_{n+1})\) is fixed, and the oracle prediction set \(\hat{C}^{\text{o}}(X_{n+1},\hat{A}^{\text{o}}(X_{n+1},Y_{n+1}))\) is simply obtained by applying Algorithm A1 to exchangeable data using a fixed protected attribute. This procedure has guaranteed coverage above \(1-\alpha\); see Lemma A1. 

Proof of Theorem a.3.: The strategy of this proof is very standard in the conformal inference literature. We add this proof for completeness.

Recall that \(\mathcal{I}(\hat{A}(Z_{n+1}))\) denotes a subset of the calibration data \(\mathcal{D}\) that has the same value of the selected attribute as the test point. To prove (A17), it suffices to show that the nonconformity scores \(\{\hat{S}_{i}:i\in\mathcal{I}(\hat{A}(Z_{n+1}))\cup\{Z_{n+1}\}\}\) are exchangeable. Indeed, if the scores are exchangeable and almost surely distinct (which can be easily achieved by adding continuous random noises), then the rank of \(\hat{S}_{n+1}\) is uniformly distributed over the discrete values \(\{1,2,\ldots,\mathcal{I}(\hat{A}(Z_{n+1}))+1\}\). Consequently, the conformal p-value \(\hat{u}(Z_{n+1})\) constructed by Algorithm A7 follows a uniform distribution \(\text{Unif}(\{\frac{1}{|\mathcal{I}(\hat{A}(Z_{n+1}))|+1},\frac{1}{|\mathcal{ I}(\hat{A}(Z_{n+1}))|+1},\ldots,1\})\). This implies that \(\mathbb{P}(\hat{u}(Z_{n+1})\leq\alpha\mid\phi(Z_{n+1},\hat{A}(Z_{n+1})))=\alpha\). Even if the nonconformity scores are not almost surely distinct, one can still verify that the distribution of \(\hat{u}(Z_{n+1})\) is super-uniform. Combining both cases, we have \(\mathbb{P}(\hat{u}(Z_{n+1})\leq\alpha|\phi(Z_{n+1},\hat{A}(Z_{n+1})))\leq\alpha\) for any \(\alpha\in(0,1)\).

We complete the proof by showing that the nonconformity scores \(\{\hat{S}_{i}:i\in\mathcal{I}(\hat{A}(Z_{n+1}))\cup\{Z_{n+1}\}\}\) are exchangeable. Define \(\sigma\) as an arbitrary permutation function applied on \(\mathcal{D}\cup\{Z_{n+1}\}\) and denote the permuted dataset as \(\sigma(\mathcal{D})\). We first run Algorithm A7 based on \(\mathcal{D}\) to select the sensitive attribute \(\hat{A}(Z_{n+1})\). Next, assume in a parallel world, we repeat Algorithm A7 with the same parameters and seed settings but based on the permuted data \(\sigma(\mathcal{D})\). Denote the selected attribute in the parallel world as \(\hat{A}^{{}^{\prime}}(Z_{n+1})\). Essentially, \(\hat{A}^{{}^{\prime}}(Z_{n+1})=\hat{A}(Z_{n+1})\). This is because the computation of conformal p-values and the procedure of selecting the attribute with the worst FPR in Algorithm A6 are not affected by the order of the calibration and test data. Therefore, the attribute selection process is invariant to the order of \(\mathcal{D}\cup\{Z_{n+1}\}\). This implies that the attribute selected by Algorithm A6\(\hat{A}(Z_{n+1})\) can be treated as fixed, and the nonconformity scores computed on the subset \(\mathcal{I}(\hat{A}(Z_{n+1}))\cup\{Z_{n+1}\}\) are simply reordered in the parallel world, i.e.,

\[\{\hat{S}_{\sigma(i)}^{{}^{\prime}}:i\in\mathcal{I}(\hat{A}(Z_{n+1}))\cup\{Z_{ n+1}\}\}=\bar{\sigma}(\{\hat{S}_{i}:i\in\mathcal{I}(\hat{A}(Z_{n+1}))\cup\{Z_{n+1} \}\}),\]

where \(\bar{\sigma}\) is the permutation obtained by restricting \(\sigma\) on \(\mathcal{I}(\hat{A}(Z_{n+1}))\cup\{Z_{n+1}\}\). Hence, we have

\[\{\hat{S}_{i}:i\in\mathcal{I}(\hat{A}(Z_{n+1}))\cup\{Z_{n+1}\}\} \stackrel{{ d}}{{=}}\{\hat{S}_{\sigma(i)}^{{}^{\prime}}:i \in\mathcal{I}(\hat{A}(Z_{n+1}))\cup\{Z_{n+1}\}\}\] \[=\bar{\sigma}(\{\hat{S}_{i}:i\in\mathcal{I}(\hat{A}(Z_{n+1}))\cup \{Z_{n+1}\}\}),\]

where the equality in distribution is implied by \(\mathcal{D}\stackrel{{ d}}{{=}}\sigma(\mathcal{D})\). 

Proof of Theorem a.4.: This proof is the same as the proof of Theorem A3 since the multiple selected attributes \(\hat{A}(Z_{n+1})\) are invariant to any permutation of the calibration and test data. 

Proof of Lemma a.1.: This proof is a minor extension of the proof of Theorem 1 in [17], with the difference that we additionally condition on the true test label.

Fix any \(\tilde{y}\in[L]\), and suppose \(Y_{n+1}=\tilde{y}\). For a placeholder label \(y\), consider a subset of the calibration data \(\mathcal{D}\) indexed by \(\mathcal{I}(X_{n+1},y)=\{i\in\mathcal{D}:Y_{i}=\tilde{y},\phi(X_{i},\hat{A}(X _{n+1},y))=\phi(X_{n+1},\hat{A}(X_{n+1},y))\}\).

Since \(\{(X_{i},Y_{i})\}_{i=1}^{n+1}\) are exchangeable and the protected attribute \(\hat{A}(X_{n+1},y)\) is fixed, the nonconformity scores \(\hat{S}_{i}\) evaluated on the subset \(\mathcal{I}(X_{n+1},y)\cup\{(X_{n+1},y)\}\) are also exchangeable.

Further, denote \(\hat{Q}(X_{n+1},y)\) as the \(\lceil(1-\alpha)\cdot|1+\mathcal{I}(X_{n+1},y)|\rceil\)-th smallest value of \(\{\hat{S}_{i}\}_{i\in\mathcal{I}(X_{n+1},y)}\). By the quantile lemma [13, 17, 57], for any \(\alpha\in(0,1)\),

\[\mathbb{P}[Y_{n+1} \in\hat{C}(X_{n+1},\hat{A}(X_{n+1},y))\mid Y_{n+1}=\tilde{y}, \phi(X_{n+1},\hat{A}(X_{n+1},y))]\] \[=\mathbb{P}[\hat{S}_{n+1}\leq\hat{Q}(X_{n+1},y)\mid Y_{n+1}= \tilde{y},\phi(X_{n+1},\hat{A}(X_{n+1},y))]\] (A26) \[\geq 1-\alpha.\]

### Computational Shortcuts and Efficient Implementation

#### a6.1 Outlier Detection

Given a pre-trained one-class classifier, consider a calibration dataset \(\mathcal{D}\) of size \(n\). Let \(K\) denote the number of sensitive attributes, and for each attribute \(k\in[K]\), let \(M_{k}\in\mathbb{N}\) denote the count of its possible values. Denote \(M=\max_{k}M_{k}\) as the maximum count across all attributes.

AFCP for outlier detection outlined in Algorithm A7 has the following computational cost.

**Analysis for a single test point**

* The cost of computing the false positive indicators for every sample in \(\mathcal{D}\cup\{Z_{n+1}\}\) takes \(\mathcal{O}(n\cdot\log n)\). Breaking into steps, for every data in \(\mathcal{D}\cup\{Z_{n+1}\}\), compute their nonconformity scores takes \(\mathcal{O}(n)\). Their associated conformal p-value can be computed at once by sorting all scores and keeping track of their ranks, which takes \(\mathcal{O}(n\cdot\log n)\).
* Then, selecting the sensitive attributes requires \(\mathcal{O}(n\cdot K\cdot M)\).
* Once the attribute is selected, the cost of applying conformal prediction conditional on the group identified by the selected attribute is \(\mathcal{O}(1)\) because the subset of the group has been found in the last step.

Hence, the total cost of running Algorithm A7 for a single test point is \(\mathcal{O}(n\log n+nKM)\).

**Analysis for \(m\) test points**

* The cost of computing the false positive indicators for every sample in \(\mathcal{D}\cup\{Z_{n+t}\}_{t=1}^{m}\) takes \(\mathcal{O}(n\cdot(m+\log n))\). This can be derived by rewriting the conformal p-values for all \(j\in\mathcal{D}\cup\{Z_{n+t}\}\) and for all \(t\in[m]\) as follows: \[\hat{u}_{j,t} =\frac{1}{1+n}\Big{(}\sum_{i\in\mathcal{D}\cup\{Z_{n+t}\}}\mathbf{ 1}(\hat{S}_{i}\leq\hat{S}_{j})\Big{)}\] \[=\frac{1}{1+n}\Big{(}\text{rank}(\hat{S}_{j})\text{ among }\{\hat{S}_{i}\}_{i\in\mathcal{D}}+\mathbf{1}(\hat{S}_{n+t}\leq\hat{S}_{j}) \Big{)}.\] Computing the nonconformity scores for all samples in \(\mathcal{D}\cup\{Z_{n+t}\}_{t=1}^{m}\) costs \(\mathcal{O}(n+m)\), evaluating the ranks takes \(\mathcal{O}(n\log n)\), and comparing \(\hat{S}_{n+t}\) and \(\hat{S}_{j}\) costs \(\mathcal{O}(n\cdot m)\).
* Selecting the sensitive attribute costs \(\mathcal{O}(n\cdot m+M\cdot K\cdot(n+m))\). Breaking in steps, for each test sample \(Z_{n+t}\), the worst FPR for attribute \(k\), as defined in (A20), can be rewritten as follows: \[\delta_{k,t} :=\max_{m\in[M_{k}]}\frac{\sum_{i\in\mathcal{D}\cup\{Z_{n+t}\}}E_{i,t} \cdot\mathbf{1}\{\phi(Z_{i},\{k\})=m\}}{\sum_{i\in\mathcal{D}\cup\{Z_{n+t}\}} \mathbf{1}\{\phi(Z_{i},\{k\})=m\}}\] \[=\max_{m\in[M_{k}]}\frac{\sum_{i\in\mathcal{D}}E_{i,t}\cdot \mathbf{1}\{\phi(Z_{i},\{k\})=m\}+E_{t,t}\cdot\mathbf{1}\{\phi(Z_{i},\{k\})=m\}}{ \sum_{i\in\mathcal{D}}\mathbf{1}\{\phi(Z_{i},\{k\})=m\}+\mathbf{1}\{\phi(Z_{i}, \{k\})=m\}}\] \[=\max_{m\in[M_{k}]}\frac{\sum_{i\in\mathcal{D}(k,m)}E_{i,t}+E_{t,t }\cdot\mathbf{1}\{\phi(Z_{i},\{k\})=m\}}{|\mathcal{D}(k,m)|+\mathbf{1}\{\phi(Z_ {i},\{k\})=m\}},\]

[MISSING_PAGE_FAIL:26]

rate for every test sample \(\{(X_{n+t},y)\}_{t=1}^{m}\). In specific, identifying \(\mathcal{D}(k,m),\forall k\in[K],m\in[M_{k}]\) takes \(\mathcal{O}(n\cdot M\cdot K)\). For each \(\mathcal{D}(k,m)\), computing \(\sum_{i\in\mathcal{D}(k,m)}E_{y,i,t},\forall t\in[m],\forall y\in[L]\) takes \(\mathcal{O}(L\cdot|\mathcal{D}(k,m)|\cdot m)\), and computing \(E_{y,t,t}\mathbf{1}\{\phi(Z_{i},\{k\})=m\}\) takes \(\mathcal{O}(L\cdot m)\). Therefore, repeating this process for all \(\mathcal{D}(k,m),k\in[K],m\in[M_{k}]\) and for all \(y\in[L]\) in total takes \(\mathcal{O}(L\cdot n\cdot m)\). Lastly, finding the maximum miscoverage across all attributes takes \(\mathcal{O}(L\cdot m\cdot M\cdot K)\).

Hence, the total cost of running Algorithm 2 for \(m\) test samples is \(\mathcal{O}(n\log n+Lnm+MK(n+Lm))\).

## Appendix A7 Additional Results from Numerical Experiments

### AFCP for Multiclass Classification

#### a7.1.1 Synthetic Data

Recall from Section 3.2 that Color is denoted as \(X_{0}\) and the first one of the non-sensitive features as \(X_{1}\). The distribution of \(Y\) conditional on \(X\) is modeled by a simple decision tree, where \(X_{0}\) and \(X_{1}\) are the only useful predictors for \(Y\), formulated as the following:

\[\mathbb{P}[Y\mid X]=\begin{cases}\left(\frac{1}{3},\frac{1}{3},\frac{1}{3}, 0,0,0\right),&\text{if }X_{0}\text{=Blue and }X_{1}<0.5,\\ \left(0,0,0,\frac{1}{3},\frac{1}{3},\frac{1}{3}\right),&\text{ifFigure A1: Coverage and size of prediction sets constructed with different methods for groups formed by Color. For the Blue group, the Marginal method (dashed orange lines) fails to detect and correct for its undercoverage, and the Exhaustive method produces prediction sets that are too conservative to be helpful. In contrast, our AFCP and AFCP1 methods correct the undercoverage and maintain small prediction sets. See Table A2 for numerical details and standard errors.

Figure A2: Coverage and size of prediction sets constructed with different methods for groups formed by Age group. By design, all groups have similar performance, and none of them are subject to unfairness/undercoverage. See Table A3 for numerical details and standard errors.

[MISSING_PAGE_FAIL:29]

[MISSING_PAGE_FAIL:30]

[MISSING_PAGE_EMPTY:31]

Figure A6: Coverage and size of prediction sets constructed with different methods for groups formed by Age group. By design, all groups have similar performance, and none of them are subject to unfairness/undercoverage. See Table A7 for numerical details and standard errors.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c} \hline \hline  & & \multicolumn{2}{c}{AFCP} & \multicolumn{2}{c}{AFCP1} & \multicolumn{2}{c}{Marginal} & \multicolumn{2}{c}{Partial} & \multicolumn{2}{c}{Exhaustive} \\ \cline{3-11} Group & 
\begin{tabular}{c} Sample \\ size \\ \end{tabular} & \multicolumn{1}{c}{Coverage} & Size & Coverage & Size & Coverage & Size & Coverage & Size & Coverage & Size \\ \hline
**Grey** & & & & & & & & & & & & \\  & & & & & & & & & & & & \\  & & & & & & & & & & & & \\  & & & & & & & & & & & & \\  & & & & & & & & & & & & \\  & & & & & & & & & & & \\ \hline \multirow{4}{*}{Grey} & & & & & & & & & & & & & \\  & & & & & & & & & & & & \\  & & & & & & & & & & & & \\  & & & & & & & & & & & & \\  & & &

[MISSING_PAGE_FAIL:33]

\begin{table}
\begin{tabular}{c c c c c c c c c c c c} \hline \hline  & & \multicolumn{2}{c}{AFCP} & \multicolumn{2}{c}{AFCP1} & \multicolumn{2}{c}{Marginal} & \multicolumn{2}{c}{Partial} & \multicolumn{2}{c}{Ehaustive} \\ \cline{3-14} Group & Sample size & Coverage & Size & Coverage & Size & Coverage & Size & Coverage & Size & Coverage & Size \\ \hline
**West** & & & & & & & & & & & & \\  & & & & & & & & & & & & \\   West & 200 & 0.927 & 3.165 & 0.999 & 5.979 & 0.923 & 3.164 & 1.000 & 6.000 & 1.000 & 6.000 \\   West & 200 & 0.004 & 0.047 & (0.000) & (0.009) & (0.004) & (0.048) & (0.000) & (0.000) & (0.000) & (0.000) \\   West & 500 & 0.910 & 2.018 & 0.955 & 3.594 & 0.894 & 1.872 & 0.995 & 5.303 & 1.000 & 5.999 \\   West & 500 & 0.003 & (0.028) & (0.003) & (0.055) & (0.003) & (0.016) & (0.001) & (0.023) & (0.000) & (0.001) \\   West & 1000 & 0.930 & 1.869 & 0.940 & 1.972 & 0.887 & 1.575 & 0.981 & 2.718 & 0.999 & 5.976 \\   West & 0.003 & (0.015) & (0.003) & (0.018) & (0.003) & (0.012) & (0.001) & (0.024) & (0.000) & (0.005) \\   West & 2000 & 0.936 & 1.632 & 0.938 & 1.640 & 0.894 & 1.465 & 0.968 & 1.978 & 0.983 & 5.102 \\  & & & & & & & & & & \\
**East** & & & & & & & & & & & \\   East & 200 & 0.942 & 3.174 & 0.999 & 5.952 & 0.940 & 3.172 & 1.000 & 6.000 & 1.000 & 6.000 \\   East & 200 & 0.003 & (0.048) & (0.000) & (0.014) & (0.003) & (0.049) & (0.000) & (0.000) & (0.000)

[MISSING_PAGE_EMPTY:35]

Figure A11: Coverage and size of prediction sets constructed with different methods for groups formed by Social. All groups have similar performance, and none of them are subject to unfairness/undercoverage. See Table A13 for numerical details and standard errors.

Figure A12: Coverage and size of prediction sets constructed with different methods for groups formed by Health. All groups have similar performance, and none of them are subject to unfairness/undercoverage. See Table A14 for numerical details and standard errors.

[MISSING_PAGE_FAIL:37]

[MISSING_PAGE_FAIL:38]

[MISSING_PAGE_FAIL:39]

[MISSING_PAGE_EMPTY:40]

Figure A16: Coverage and size of prediction sets constructed with different methods for groups formed by Finance. All groups have similar performance, and none of them are subject to unfairness/undercoverage. See Table A18 for numerical details and standard errors.

Figure A17: Coverage and size of prediction sets constructed with different methods for groups formed by Social. All groups have similar performance, and none of them are subject to unfairness/undercoverage. See Table A19 for numerical details and standard errors.

Figure A18: Coverage and size of prediction sets constructed with different methods for groups formed by Health. All groups have similar performances, and none of them are subject to unfairness/undercoverage. See Table A20 for numerical details and standard errors.

[MISSING_PAGE_FAIL:42]

\begin{table}
\begin{tabular}{l c c c c c c c c c c c} \hline \hline  & & \multicolumn{2}{c}{AFCP} & \multicolumn{2}{c}{AFCP1} & \multicolumn{2}{c}{Marginal} & \multicolumn{2}{c}{Partial} & \multicolumn{2}{c}{Ehaustive} \\ \cline{3-14} Group & Sample size & Coverage & Size & Coverage & Size & Coverage & Size & Coverage & Size & Coverage & Size \\ \hline
**Nosprob** & & 0.931 & 2.768 & 0.964 & 3.846 & 0.924 & 2.601 & 0.996 & 3.971 & 0.999 & 3.980 \\ Nooprob & & 0.004 & (0.031) & (0.003) & (0.017) & (0.004) & (0.025) & (0.001) & (0.014) & (0.001) & (0.014) \\ Nooprob & & 0.926 & 2.993 & 0.945 & 3.674 & 0.912 & 2.238 & 0.979 & 3.815 & 1.000 & 4.000 \\ Nooprob & & 0.003 & (0.035) & (0.003) & (0.009) & (0.003) & (0.020) & (0.001) & (0.008) & (0.000) & (0.000) \\ Nooprob & & 0.924 & 3.016 & 0.935 & 3.151 & 0.909 & 1.980 & 0.968 & 3.568 & 1.000 & 4.000 \\ Nooprob & & 0.921 & 2.934 & 0.926 & 3.335 & 0.903 & 1.711 & 0.954 & 3.301 & 0.996 & 3.990 \\ Nooprob & & 0.002 & (0.035) & (0.002) & (0.002) & (0.001) & (0.002) & (0.001) & (0.002) & (0.005) & (0.001) \\ Nooprob & & 0.915 & 1.904 & 0.916 & 2.437 & 0.900 & 1.317 & 0.994 & 2.585 & 0.957 & 3.875 \\ Nooprob & & 0.002 & (0.024) & (0.002) & (0.002) & (0.002) & (0.007) & (0.002) & (0.002) & (0.002) & (0.003) \\
**Problematic** & & 0.919 & 2.802 & 0.923 & 3.335 & 0.890 & 1.666 & 0.950 & 3.483 & 0.997 & 3.996 \\ Problematic & & 0.002 & (0.026) & (0.020) & (0.011) & (0.002) & (0.020) & (0.001) & (0.004) & (0.000) & (0.001) \\ Nooprob & & 0.923 & 2.941 & 0.944 & 3.674 & 0.911 & 2.213 & 0.978 & 3.310 & 1.000 & 4.000 \\ Problematic & & 0.003 & (0.034) & (0.002) & (0.009) & (0.003) & (0.019) & (0.002) & (0.007) & (0.000) & (0.000) \\ Nooprob & & 0.923 & 2.999 & 0.935 & 3.508 & 0.904 & 1.554 & 0.963 & 3.644 & 1.000 & 4.000 \\ Problematic & & 0.002 & (0.029) & (0.002) & (0.007) & (0.003) & (0.016) & (0.020) & (0.007) & (0.000) & (0.000) \\ Problematic & & 0.924 & 2.949 & 0.939 & 3.155 & 0.904 & 1.681 & 0.953 & 3.486 & 0.997 & 3.996 \\ Problematic & & 0.002 & (0.026) & (0.002) & (0.001) & (0.003) & (0.021) & (0.002) & (0.008) & (0.001) & (0.001) \\ Problematic & & 0.912 & 1.863 & 0.914 & 2.394 & 0.903 & 1.106 & 0.935 & 2.589 & 0.952 & 3.871 \\ Nooprob & & 0.002 & (0.022) & (0.002) & (0.002) & (0.002) & (0.002) & (0.005) & (0.002) & (0.002) & (0.002) & (0.003) \\
**Slightly_prob** & & & & & & & & & & & & \\ \hline \end{tabular}
\end{table}
Table A18: Coverage and size of prediction sets constructed with different methods for groups formed by Finance. All groups have similar performance, and none of them are subject to unfairness/undercoverage. See corresponding plots in Figure A16.

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline  & & \multicolumn{2}{c}{AFCP} & \multicolumn{2}{c}{AFCP1} & \multicolumn{2}{c}{Marginal} & \multicolumn{2}{c}{Partial} & \multicolumn{2}{c}{Ehaustive} \\ \cline{3-14} Group & Sample size & Coverage & Size & Coverage & Size & Coverage & Size & Coverage & Size & Coverage & Size \\ \hline
**Convenient** & & & & & & & & & & & \\  & & & & & & & & & & & & \\  & & & & & & & & & & & & \\  & & & & & & & & & & & & \\  & & & & & &

[MISSING_PAGE_EMPTY:44]

#### a7.1.3 COMPAS data

We extend our experiments to investigate the effectiveness of AFCP using the Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) dataset [50]. The COMPAS dataset is widely studied in the fairness literature for multi-class classification tasks, predicting the risk of recidivism across three categories: 'High', 'Medium', and 'Low'. Following the data preprocessing steps outlined in [59], we exclude rows with missing or low-quality data and compute the length of stay in jail. Additionally, we merge the race groups Asian and Native American, both of which have few occurrences, into the 'Others' category. After preprocessing, the dataset comprises 6,172 instances with five categorical features: charge degree of defendants (2 levels), race (4 levels), age category (3 levels), sex (2 levels), and score category of defendants (3 levels). We regard the first four features as potentially sensitive attributes.

Similar to the Nursery dataset case, we utilize the LabelEncoder function to numerically encode categorical features and outcome labels. To increase prediction difficulty and emphasize algorithmic bias, we introduce independent, uniformly distributed noise to the labels of samples identified as African-American. Additionally, we undersample the African-American group to 200 samples, while the Caucasian group contains 2,103 samples, Hispanic 509, and Others 385.

Figure A19 summarizes the performance of all methods as a function of the total number of training and calibration data points, which range from 200 to 1000. Figure A20 narrows the focus by analyzing the performance relative to the number of restricted calibration data points, as defined in Section 2.2. The results are averaged over 500 randomly selected test points and 100 repeated experiments. In each experiment, 50% of the samples are randomly assigned for training and the remaining 50% for calibration. Once again, we conclude that our AFCP methods outperform the other benchmarks considered, resulting in more informative prediction sets with higher conditional coverage for the African-American subgroup.

Figure A21 shows the selection frequencies of the protected attribute Race as a function of sample size within the same experiment described in Figure A19. This plot demonstrates that both AFCP and AFCP 1 consistently select Race as the most sensitive attribute as the number of samples increases. Also, we report the prediction accuracy in Table A21. The results confirm that the African-American group is disproportionately affected by algorithmic bias, not only in terms of uncertainty estimates but also in prediction accuracy, as they experience significantly lower-than-average test accuracy.

Figure A19: Performance of prediction sets constructed by different methods on the COMPAS data, as a function of the sample size. AFCP leads to more informative predictions with higher coverage conditional on the sensitive attribute, Race (shown for level African-American).

### AFCP for Outlier Detection

#### a7.2.1 Setup and Benchmarks

This section demonstrates the empirical performance of our AFCP extension for outlier detection. Firstly, we focus on the implementation described in Algorithm A7, which selects at most one sensitive attribute. Similar to the multi-class classification cases, Our method is compared with three existing approaches, which utilize the same data, ML model, and conformity scores but compute conformal p-values with different guarantees. The first is the _marginal_ benchmark, which constructs conformal p-value guaranteeing \(\mathbb{P}(\hat{u}^{\text{marginal}}(Z_{n+1})\leq\alpha)\leq\alpha\) by applying Algorithm A2 without protected attributes. The second is the _exhaustive_ equalized benchmark, which evaluates conformal p-values guaranteeing \(\mathbb{P}(\hat{u}^{\text{exhaustive}}(Z_{n+1})\leq\alpha\mid\phi(Z_{n+1},[K]))\leq\alpha\) by applying Algorithm A2 with all \(K\) sensitive attributes simultaneously protected. The third is a _partial_ equalized benchmark that separately applies Algorithm A2 with each possible protected attribute \(k\in[K]\), and then takes the maximum of all such p values. This is an intuitive approach that can be easily verified to provide a coverage guarantee \(\mathbb{P}(\hat{u}^{\text{partial}}(Z_{n+1})\leq\alpha\mid\phi(Z_{n+1},\{k\})) \leq\alpha\quad\forall k\in[K]\).

In addition, similar to the multiclass classification experiments, we consider AFCP1 - the AFCP implementation that always selects the most critical protected attribute without conducting the significance test.

For all methods considered, the outlier detection model is based on a three-layer neural network, and the outputs from each layer are batch-normalized. The Adam optimizer and the BCEWithLogitsLoss loss function are used in the training process, with a learning rate set at 0.0001. The loss values demonstrate convergence after 100 epochs of training. For all methods, the miscoverage target level is set at \(\alpha=0.1\). Note that the training and testing data contain both inliers and outliers, while the calibration data only contains inliers.

We assess the performance of the methods based on the False Positive Rate (FPR) and the True Positive Rate (TPR). Ideally, the objective is to achieve a higher conditional FPR for groups experiencing unfairness, thereby maintaining the average FPR below the target threshold of 0.1 while simultaneously achieving a higher TPR. The results presented are averaged over 500 independent test points and 30 experiments.

#### a7.2.2 Synthetic Data

We employ the same data settings as in the multi-class classification example, designating Color as the sensitive attribute associated with the Blue group, which suffers from undercoverage. While Age Group and Region are also sensitive attributes, they are not subject to biases in this context. The outcome labels \(Y\) have two possible values: \(Y=0\) if the unit is properly treated and \(Y=1\)

[MISSING_PAGE_FAIL:47]

[MISSING_PAGE_FAIL:48]

[MISSING_PAGE_FAIL:49]

[MISSING_PAGE_FAIL:50]

[MISSING_PAGE_FAIL:51]

[MISSING_PAGE_FAIL:52]

[MISSING_PAGE_FAIL:53]

[MISSING_PAGE_FAIL:54]

[MISSING_PAGE_FAIL:55]

[MISSING_PAGE_FAIL:56]

[MISSING_PAGE_FAIL:57]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims made in the abstract and introduction accurately reflect the paper's contributions and scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The paper discusses the limitations of this work in the Discussion section. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: All assumptions are stated in the paper and all proofs are provided in the Appendix. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The paper fully discloses all the information needed to reproduce the main experimental results. Additionally, code is provided. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The paper provides open access to the code needed to reproduce the main experimental results, and precise information about how the open-domain data were obtained. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The paper specify all the training and test details necessary to understand the results. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The paper reports error bars or standard deviations where appropriate, along with suitable explanations. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The paper provides information on the computer resources in the Discussion section. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: This work is motivated by the need to address algorithmic bias issues, which is a topic with potentially broad impacts, as discussed in the paper. However, this paper focuses on foundational research that is not tied to a particular application.

Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper describes foundational research and does not release new data or models. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: This paper uses open-domain data, properly crediting the license and creators. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The new code accompanying this paper is well documented. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.