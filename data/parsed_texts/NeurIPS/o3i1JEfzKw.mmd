# Provable Partially Observable Reinforcement Learning with Privileged Information

 Yang Cai\({}^{1}\) Xiangyu Liu\({}^{2}\) Argyris Oikonomou\({}^{1}\) Kaiqing Zhang\({}^{2}\)

\({}^{1}\) Yale University \({}^{2}\)University of Maryland, College Park

yang.cai@yale.edu, xyliu999@umd.edu

argyris.oikonomou@yale.edu, kaiqing@umd.edu

###### Abstract

Partial observability of the underlying states generally presents significant challenges for reinforcement learning (RL). In practice, certain _privileged information_, e.g., the access to states from simulators, has been exploited in training and has achieved prominent empirical successes. To better understand the benefits of privileged information, we revisit and examine several simple and practically used paradigms in this setting. Specifically, we first formalize the empirical paradigm of _expert distillation_ (also known as _teacher-student_ learning), demonstrating its pitfall in finding near-optimal policies. We then identify a condition of the partially observable environment, the _deterministic filter condition_, under which expert distillation achieves sample and computational complexities that are _both_ polynomial. Furthermore, we investigate another successful empirical paradigm of _asymmetric actor-critic_, and focus on the more challenging setting of observable partially observable Markov decision processes. We develop a belief-weighted asymmetric actor-critic algorithm with polynomial sample and quasi-polynomial computational complexities, in which one key component is a new provable oracle for learning belief states that preserves _filter stability_ under a misspecified model, which may be of independent interest. Finally, we also investigate the provable efficiency of partially observable multi-agent RL (MARL) with privileged information. We develop algorithms featuring _centralized-training-with-decentralized-execution_, a popular framework in empirical MARL, with polynomial sample and (quasi-)polynomial computational complexities in both paradigms above. Compared with a few recent related theoretical studies, our focus is on understanding practically inspired algorithmic paradigms, without computationally intractable oracles.

## 1 Introduction

In most real-world applications of reinforcement learning (RL), e.g., perception-based robot learning [46, 3], autonomous driving [73, 41], dialogue systems [88], and clinical trials [77], only _partial observations_ of the environment state are available for sequential decision-making. Such partial observability presents significant challenges for efficient decision-making and learning, with known computational [66] and statistical [43, 36] barriers under the general model of partially observable Markov decision processes (POMDPs). The curse of partial observability becomes severer when _multiple_ RL agents interact, where not only the environment state, but also other agents' information, are not fully-observable in decision-making [85, 80].

On the other hand, a flurry of empirical paradigms has made partially observable (multi-agent) RL promising in practice. One notable example is to exploit the _privileged information_ that may be available (only) during training. The privileged information usually includes direct access to the underlying states, as well as access to other agents' observations/actions in multi-agent RL (MARL), due to the use of simulators and/or high-precision sensors for training. The latter is also known asthe _centralized-training-with-decentralized-execution_ (CTDE) framework in deep MARL, and has become prevalent in practice [53; 70; 22; 82]. These approaches can be mainly categorized into two types: i) privileged _policy_ learning, where an expert/teacher policy is trained with privileged information, and then _distilled_ into a student partially observable policy. This _expert distillation_, also known as _teacher-student learning_, approach has been the key to some empirical successes in robotic locomotion [45; 59] and autonomous driving [14]; ii) privileged _value_ learning, where a value function is trained conditioned on privileged information, and used to improve a partially observable policy. It is typically instantiated as the _asymmetric actor-critic_ algorithm [68], and serves as the backbone of some high-profiled successes in robotic manipulation [46; 3] and MARL [53; 82].

Despite the remarkable empirical successes, theoretical understandings of partially observable RL with privileged information have been rather limited, except for a few recent prominent advances in RL with _hindsight observability_[44; 30] (see Appendix B for a detailed discussion). However, most of these theoretically sound algorithms are different from those used in practice, and require computationally intractable oracles to achieve provable sample efficiency. The soundness and efficiency of the aforementioned paradigms used in practice remain elusive. In this work, we examine both paradigms of expert distillation and asymmetric actor-critic, with foresight privileged information as in these empirical works. In contrast to [44; 30], which purely focused on sample efficiency, we aim to understand the benefits of privileged information by examining these practically inspired paradigms under several POMDP models, without computationally intractable oracles. We defer a detailed literature review to Appendix B, and summarize our contribution as follows.

Contributions.We first formalize the empirical paradigm of _expert distillation_, and demonstrate its pitfall in distilling near-optimal policies even in observable POMDPs, a model class that was recently shown to allow provable partially observable RL without computationally intractable oracles [25]. We then identify a new condition for POMDPs, the _deterministic filter_ condition, and establish sample and computational complexities that are _both_ polynomial for expert distillation. The new condition is weaker and thus encompasses several known (statistically) tractable POMDP models (see Figure 1 for a summary). Further, we revisit the _asymmetric actor-critic_ paradigm and analyze its efficiency under the more challenging setting of observable POMDPs above (where expert distillation fails). Identifying the inefficiency of vanilla asymmetric actor-critic, and inspired by the empirical success in _belief-state-learning_, we develop a new _belief-weighted_ version of asymmetric actor-critic, with polynomial-sample and quasi-polynomial-time complexities. Key to the results is a new belief-state learning oracle that preserves _filter stability_ under a misspecified model, which may be of independent interest. Finally, we also investigate the provable efficiency of partially observable multi-agent RL with privileged information, by studying algorithms under the CTDE framework, with polynomial-sample and (quasi-)polynomial-time complexities in both paradigms above.

## 2 Preliminaries

### Partially Observable RL (with Privileged Information)

Model.Consider a POMDP characterized by a tuple \(\mathcal{P}=(H,\mathcal{S},\mathcal{A},\mathcal{O},\mathbb{T},\mathbb{O},\mu_{ 1},r)\), where \(H\) denotes the length of each episode, \(\mathcal{S}\) is the state space with \(|\mathcal{S}|=S\), \(\mathcal{A}\) denotes the action space with \(|\mathcal{A}|=A\). We use \(\mathbb{T}=\{\mathbb{T}_{h}\}_{h\in[H]}\) to denote the collection of transition matrices, so that \(\mathbb{T}_{h}(\cdot\mid s,a)\in\Delta(\mathcal{S})\) gives the probability of the next state if action \(a\) is taken at state \(s\) and step \(h\). In the following discussions, for any given \(a\), we treat \(\mathbb{T}_{h}(a)\in\mathbb{R}^{|\mathcal{S}|\times|\mathcal{S}|}\) as a matrix, where each row gives the probability for reaching each next state from different current states. We use \(\mu_{1}\) to denote the distribution of the initial state \(s_{1}\), and \(\mathcal{O}\) to denote the observation space with \(|\mathcal{O}|=O\). We use \(\mathbb{O}=\{\mathbb{O}_{h}\}_{h\in[H]}\) to denote the collection of emission matrices, so that \(\mathbb{O}_{h}(\cdot\mid s)\in\Delta(\mathcal{O})\) gives the emission distribution over the observation space \(\mathcal{O}\) at state \(s\) and step \(h\). For notational convenience, we will at times adopt the matrix convention, where \(\mathbb{O}_{h}\) is a matrix with rows \(\mathbb{O}_{h}(\cdot\mid s)\) for each \(s\in\mathcal{S}\). Finally, \(r=\{r_{h}\}_{h\in[H]}\) is a collection of reward functions, so that \(r_{h}(s,a)\in[0,1]\) is the reward given the state \(s\) and action \(a\) at step \(h\). When privileged information is available, the agent can observe the underlying state \(s\in\mathcal{S}\) directly _during training_ (only). We thus denote the trajectory until step \(h\)_with states_ as \(\overline{\tau}_{h}=(s_{1:h},o_{1:h},a_{1:h-1})\), the one _without states_ as \(\tau_{h}=(o_{1:h},a_{1:h-1})\), and its space as \(\mathcal{T}_{h}\). Finally, we use \(\bm{b}_{h}(\tau_{h})\) to denote the posterior distribution over the underlying state at step \(h\) given history \(\tau_{h}\), which is known as the _belief state_ (c.f. Appendix C.1 for more details).

Policy and value function.We define a stochastic policy at step \(h\) as:

\[\pi_{h}:\mathcal{O}^{h}\times\mathcal{A}^{h-1}\rightarrow\Delta(\mathcal{A}),\] (2.1)where the agent bases on the entire (partially observable) history for decision-making. The corresponding policy class is denoted as \(\Pi_{h}\). We further denote \(\Pi=\times_{h\in[H]}\Pi_{h}\). We also define \(\Pi^{\text{gen}}:=\{\pi_{1:H}\,|\,\pi_{h}:\mathcal{S}^{h}\times\mathcal{O}^{h} \times\mathcal{A}^{h-1}\rightarrow\Delta(\mathcal{A})\text{ for }h\in[H]\}\) to be the most general policy space in partially observable RL with privileged state information, which can potentially depend on all historical states, observations, and actions. It can be seen that \(\Pi\subseteq\Pi^{\text{gen}}\). We may also use policies that only receive a _finite memory_ instead of the whole history as inputs: fix an integer \(L>0\), we define the policy space \(\Pi^{L}\) to be the space of all possible policies \(\pi=\pi_{1:H}:=(\pi_{h})_{h\in[H]}\) such that \(\pi_{h}:\mathcal{Z}_{h}\rightarrow\Delta(\mathcal{A})\) with \(\mathcal{Z}_{h}:=\mathcal{O}^{\min\{L,h\}}\times\mathcal{A}^{\min\{L,h\}}\) for each \(h\in[H]\). Finally, we define the space of state-based policies as \(\Pi_{\mathcal{S}}\), i.e., for any \(\pi=\pi_{1:H}\in\Pi_{\mathcal{S}}\), \(\pi_{h}:\mathcal{S}\rightarrow\Delta(\mathcal{A})\) for all \(h\in[H]\).

Given the POMDP model \(\mathcal{P}\), we write \(\mathbb{P}_{s_{1:H+1,a_{1:H}},o_{1:H}\sim\mathcal{E}}^{\mathcal{P}}(\mathcal{ E})\) to denote the event \(\mathcal{E}\) when \((s_{1:H+1},a_{1:H},o_{1:H})\) is drawn as a trajectory following the policy \(\pi\) in the model \(\mathcal{P}\). We will also use the shorthand notation \(\mathbb{P}^{\pi,\mathcal{P}}(\cdot)\) if \((s_{1:H+1},a_{1:H},o_{1:H})\) is evident. We write \(\mathbb{E}_{\pi}^{\mathcal{P}}[\cdot]\) to denote the expectation similarly. We define the value function at step \(h\) as \(V_{h}^{\pi,\mathcal{P}}(y_{h}):=\mathbb{E}_{\pi}^{\mathcal{P}}[\sum_{h=h}^{H}r _{t}(s_{t},a_{t})\,\left|\,y_{h}\right|]\), denoting the expected accumulated rewards from step \(h\), where \(y_{h}\subseteq(s_{1:h},o_{1:h},a_{1:h-1})\), and we slightly abuse the notation by treating as a set the sequence of states \(s_{1:h}\), the sequence of observations \(o_{1:h}\), and the sequence of actions \(a_{1:h-1}\) up to time \(h\), which are the available information to the agent at step \(h\). We say \(y_{h}\) is _reachable_ if there exists some policy \(\pi\in\Pi^{\text{gen}}\) such that \(\mathbb{P}^{\pi,\mathcal{P}}(y_{h})>0\). For \(h=1\), we adopt the simplified notation \(v^{\mathcal{P}}(\pi)=\mathbb{E}_{\pi}^{\mathcal{P}}[\sum_{h=1}^{H}r_{h}(s_{h}, a_{h})]\). Meanwhile, we also define \(Q_{h}^{\pi,\mathcal{P}}(y_{h},a_{h}):=\mathbb{E}_{\pi}^{\mathcal{P}}[\sum_{t=h }^{H}r_{t}(s_{t},a_{t})\,|\,y_{h},a_{h}]\). We denote the occupancy measure on the state space as \(d_{h}^{\pi,\mathcal{P}}(s_{h})=\mathbb{P}^{\pi,\mathcal{P}}(s_{h})\). The goal of learning in POMDPs is to find the optimal policy that _maximizes_ the expected accumulated reward over the policies that take \(\tau_{h}\) as input at each step \(h\in[H]\), i.e., those \(\pi\in\Pi\). Formally, we define:

**Definition 2.1** (\(\epsilon\)-optimal policy).: Given \(\epsilon>0\), a policy \(\pi^{\star}\in\Pi\) is \(\epsilon\)-optimal, if \(v^{\mathcal{P}}(\pi^{\star})\geq\max_{\pi\in\Pi}v^{\mathcal{P}}(\pi)-\epsilon\).

Learning with privileged information.Common RL algorithms for POMDPs deal with the scenario where during _both_ the training and test time, the agent can only observe its historical observations and actions \(\tau_{h}\) at step \(h\), while the states are not accessible. In other words, the agent can only utilize policies from \(\Pi\) to interact with the environment. In contrast, in settings with _privileged information_, e.g., training in simulators and/or using sensors with higher precision, the underlying state can be used in training. Thus, the agent is allowed to utilize policies from the class \(\Pi^{\text{gen}}\) during training. Meanwhile, the objective is still to find the optimal history-dependent policy in the space of \(\Pi\), since at test time, the agent cannot access the state information anymore, and it is the performance for such policies that matters eventually. For simplicity, we assume the reward function is known since under our privileged information setting, learning the reward function is much easier than learning the transition and emission, and the sample/computational complexity for the former is dominated by that for the latter. This assumption has also been made for learning in POMDPs without privileged information [36; 47; 48].

### Partially Observable Multi-agent RL with Information Sharing

Partially observable stochastic games (POSGs) are a natural generalization of POMDPs with multiple agents of potentially independent interests. We define a POSG with \(n\) agents by a tuple \(\mathcal{G}=(H,\mathcal{S},\{\mathcal{A}_{i}\}_{i=1}^{n},\{\mathcal{O}_{i}\}_{ i=1}^{n},\mathbb{T},\mathbb{O},\mu_{1},\{r_{i}\}_{i=1}^{n})\), where each agent \(i\) has its individual action space \(\mathcal{A}_{i}\), observation space \(\mathcal{O}_{i}\), and reward function \(r_{i}=\{r_{i,h}\}_{h\in[H]}\) with \(r_{i,h}(s,a)\in[0,1]\) denoting the reward given state \(s\) and joint action \(a\) for agent \(i\) at step \(h\). An episode of POSG proceeds as follows: at each step \(h\) and state \(s_{h}\), a joint observation is drawn from \((o_{i,h})_{i\in[n]}\sim\mathbb{O}_{h}(\cdot\,|\,s_{h})\), and each agent receives its own observation \(o_{i,h}\), takes the corresponding action \(a_{i,h}\), obtains the reward \(r_{i,h}(s_{h},a_{h})\), where \(a_{h}:=(a_{i,h})_{i\in[n]}\), and then the system transitions to the next state as \(s_{h+1}\sim\mathbb{T}_{h}(\cdot\,|\,s_{h},a_{h})\). Notably, each agent \(i\) may not only know its local information \((o_{i,1:h},a_{i,1:h-1})\), but also information from some other agents. Therefore, we denote the information available to each agent \(i\) at step \(h\) also as \(\tau_{i,h}\subseteq(o_{1:h},a_{1:h-1})\) and define the _common information_ as \(c_{h}=\cap_{i\in[n]}\tau_{i,h}\) and _private information_ as \(p_{i,h}=\tau_{i,h}\setminus c_{h}\). We denote the space for common information and private information as \(\mathcal{C}_{h}\) and \(\mathcal{P}_{i,h}\) for each agent \(i\) and step \(h\). The joint private information at step \(h\) is denoted as \(p_{h}=(p_{i,h})_{i\in[n]}\), where the collection of the joint private information is given by \(\mathcal{P}_{h}=\mathcal{P}_{1,h}\times\cdots\times\mathcal{P}_{n,h}\). We refer more examples of this setting of POSG with information-sharing to Appendix C.2 (and also [62; 63; 51]). Correspondingly, the policy each agent \(i\) deploys at test time takes the form of \(\pi_{i,h}:\Omega_{h}\times\mathcal{C}_{h}\times\mathcal{P}_{i,h}\rightarrow\Delta (\mathcal{A}_{i})\), where \(\Omega_{h}\) is the space of random seeds. We denote the policy space for agent \(i\) as \(\Pi_{i}\). If \(\pi_{i,h}\) takes the state \(s_{h}\) instead of \((c_{h},p_{i,h})\) as input, we denote its policy space as \(\Pi_{\mathcal{S},i}\), e.g., for each agent \(i\), and policy \(\pi_{1:H}\in\Pi_{\mathcal{S},i}\), we have \(\pi_{i,h}:\mathcal{S}\rightarrow\Delta(\mathcal{A}_{i})\) for each step \(h\in[H]\). Similar to the POMDP setting, we define \(\Pi^{\text{gen}}\) to be the most general policy space, i.e., \(\Pi^{\text{gen}}:=\{\pi_{1:H}\,|\,\pi_{h}:\mathcal{S}^{h}\times\mathcal{O}^{h }\times\mathcal{A}^{h-1}\rightarrow\Delta(\mathcal{A})\text{ for }h\in[H]\}\). Note that this model covers several recent POSG models studied for partially observable MARL, e.g., [49; 27]. For example, at each step \(h\), if there is no shared information, then \(c_{h}=\emptyset\), and if all history information is shared, then \(p_{i,h}=\emptyset\) for all \(i\in[n]\). In _privileged-information_-based learning, the training algorithm may exploit not only the underlying state information, but also the observations and actions of other agents.

Solution concepts.The solution concepts for POSGs are usually the _equilibria_, particularly Nash equilibrium (NE) for two-player zero-sum games (i.e., when \(n=2\) and \(r_{1,h}+r_{2,h}=1\)),1 and correlated equilibrium (CE) or coarse correlated equilibrium (CCE) for general-sum games. We defer the formal definitions of these standard solution concepts to Appendix C.2.

Footnote 1: Note that we require \(r_{1,h}+r_{2,h}\) to be 1 instead of \(0\) to be consistent with our assumption that \(r_{i,h}\in[0,1]\) for each \(i\in[0,1]\), and this requirement does not lose optimality as one can always subtract the constant-sum offset to attain a zero-sum reward structure.

### Technical Assumptions for Computational Tractability

A key technical assumption is that the POMDPs/POSGs we consider satisfy an _observability_ assumption, as outlined below. This observability assumption allows us to use short memory policies to approximate the optimal policy, and yields quasi-polynomial-time complexity for both planning and learning in POMDPs/POSGs [26; 25; 51]. Meanwhile, we defer an additional assumption to ensure the traceability for solving POSGs to Appendix C.3.

**Assumption 2.2** (\(\gamma\)-observability [20; 26; 25]).: Let \(\gamma>0\). For \(h\in[H]\), we say that the matrix \(\mathbb{O}_{h}\) satisfies the \(\gamma\)-observability assumption if for each \(h\in[H]\), for any \(b,b^{\prime}\in\Delta(\mathcal{S})\), \(\left\|\mathbb{O}_{h}^{\top}b-\mathbb{O}_{h}^{\top}b^{\prime}\right\|_{1} \geq\gamma\left\|b-b^{\prime}\right\|_{1}\). A POMDP/POSG satisfies \(\gamma\)-observability if all its \(\mathbb{O}_{h}\) for \(h\in[H]\) do so.

## 3 Revisiting Empirical Paradigms of RL with Privileged Information

Most empirical paradigms of RL with privileged information can be categorized into two types: i) privileged _policy_ learning, where the policy in training is conditioned on the privileged information, and the trained policy is then _distilled_ to a policy that does not take the privileged information as input. This is usually referred to as either _expert distillation_[14; 64; 58] or _teacher-student learning_[45; 59; 75] in the literature; ii) privileged _value_ learning, where the value function is conditioned on the privileged information, and is then used to directly output a policy that takes partial observation (history) as input. One prominent example of ii) is _asymmetric-actor-critic_[68; 3]. It is worth noting that asymmetric-actor-critic is also closely related to one of the most successful paradigms for multi-agent RL, _centralized-training-with-decentralized-execution_[53; 86; 21], which is usually instantiated under the actor-critic framework, with the critic taking privileged information as input

\begin{table}
\begin{tabular}{|l|c|c|} \hline  & Without PI & With PI (Ours) \\ \hline \multirow{3}{*}{Block MDP} & With STD & \multirow{3}{*}{
\begin{tabular}{} \end{tabular} } \\  & Oracle-efficient & \\  & & \\  & Without additional assumption : & Poly sample \\  & Computationally harder than SL, [28] & + time \\ \hline \(\mathcal{A}\)-decidable & Expectation-in [9] & & \\ POMDP & sample + time [9] & FA : \\  & Without WSE : & Poly sample + \\  & Statistically hard [47] & Classification \\  & With WSE : & (SL), oracle \\  & Poly sample + time [35] & \\ \hline POSG with & & \\  & dt. filter & N/A & Poly sample + time \\ \hline Observative & Quasi-poly & Poly sample + \\  & simple + time & Quasi-poly time \\  & POSG & [25][51] & \\ \hline \end{tabular}
\end{table}
Table 1: Comparison of the theoretical guarantees with and without privileged information. PI: privileged information; STD: structural assumptions on transition dynamics, e.g., deterministic transition or reachability of all states; SL: supervised learning; FA: function approximation; WSE: well-separated emission.

Figure 1: A landscape of POMDP models that partially observable RL with privileged information can/cannot address. The axes denote the “restrictiveness” of the assumptions, on the emission channels and transition dynamics, respectively.

in training. Here we formalize and revisit the potential pitfalls of these two paradigms, and further develop theoretical guarantees under certain additional conditions and/or algorithm variants.

### Privileged Policy Learning: Expert Policy Distillation

The motivation behind expert policy distillation is that learning an optimal _fully observable_ policy in MDPs is a much easier and better-studied problem with many known efficient algorithms. The (expected) distillation objective can be formalized as follows:

\[\widehat{\pi}^{\star}\in\arg\min_{\pi\in\Pi}\ \ \mathbb{E}_{\pi^{\prime}}^{ \mathcal{P}}\left[\sum_{h=1}^{H}D_{f}\left(\pi_{h}^{\star}(\cdot\,|\,s_{h})\,| \,\pi_{h}(\cdot\,|\,\tau_{h})\right)\right],\] (3.1)

where \(\pi^{\prime}\in\Pi^{\text{sen}}\) is some given behavior policy to collect exploratory trajectories, \(\pi^{\star}\in\arg\max_{\pi\in\Pi_{S}}v^{\mathcal{P}}(\pi)\) denotes the optimal fully observable policy, and \(D_{f}\) denotes the general \(f\)-divergence to measure the discrepancy between \(\pi^{\star}\) and \(\pi\).

Such a formulation looks promising since it essentially circumvents the challenging issue of _exploration in partially observable environments_, by directly mimicking an expert policy that can be obtained from any off-the-shelf MDP learning algorithm. However, we point out in the following proposition that even if the POMDP satisfies Assumption 2.2, the distilled policy can still be strictly suboptimal even with infinite data, i.e., by solving the expected objective Equation (3.1) completely. We postpone the proof of Proposition 3.1 to Appendix E.

**Proposition 3.1** (Pitfall of expert policy distillation).: For any \(\epsilon,\gamma\in(0,1)\), there exists a \(\gamma\)-observable POMDP \(\mathcal{P}^{\epsilon}\) with \(H=1\), \(S=O=A=2\) such that for any behavior policy \(\pi^{\prime}\in\Pi^{\text{sen}}\) and choice of \(D_{f}\) in Equation (3.1), it holds that \(v^{\mathcal{P}^{\epsilon}}(\widehat{\pi}^{\star})\leq\max_{\pi\in\Pi}v^{ \mathcal{P}^{\epsilon}}(\pi)-\frac{(1-\epsilon)(1-\gamma)}{4}\).

The key reason Equation (3.1) fails is that in general, the underlying state can remain highly uncertain even given the full history. Thus, the distilled policy may not be able to mimic the state-based expert policy well at different states \(s_{h}\) if the associated \(\pi_{h}^{\star}(\cdot\,|\,s_{h})\) differs significantly across \(s_{h}\). To see how we may rule out such an issue, notice that if \(\gamma=1\) (note that according to Assumption 2.2, we have \(\gamma\) is at most \(1\) since \(\gamma\leq\|\mathbb{O}_{h}\|_{\infty}\leq 1\)), implying that the observation can decode the underlying state, the bound in Proposition 3.1 becomes vacuous. Inspired by this, we propose the following condition that incorporates this case of \(\gamma=1\), and will be shown to suffice to make expert distillation effective.

**Definition 3.2** (Deterministic filter condition).: We say a POMDP \(\mathcal{P}\) satisfies the _deterministic filter_ condition if for each \(h\geq 2\), the belief update operator under \(\mathcal{P}\) satisfies that there exists an _unknown function_\(\psi_{h}:\mathcal{S}\times\mathcal{A}\times\mathcal{O}\to\mathcal{S}\) such that for any reachable \(s_{h-1}\in\mathcal{S}\), \(o_{h}\in\mathcal{O}\), \(a_{h-1}\in\mathcal{A}\), \(U_{h}(b^{s_{h-1}};a_{h-1},o_{h})=b^{\psi_{h}(s_{h-1},a_{h-1},o_{h})}\), where we define for any \(s\in\mathcal{S}\), \(b^{s}\in\Delta(\mathcal{S})\) and \(b^{s}(s)=1\) is a one-hot vector. In addition, for \(h=1\), there exists a function \(\psi_{1}:\mathcal{O}\to\mathcal{S}\) such that for any reachable \(o_{1}\), \(B_{1}(\mu_{1};o_{1})=b^{\psi_{1}(o_{1})}\), where \(B_{h}(b;o_{h}):=\mathbb{P}_{s_{h}\sim b}^{\mathcal{P}}(\cdot\,|\,o_{h})\in \Delta(\mathcal{S})\), \(U_{h}(b;a_{h-1},o_{h}):=\mathbb{P}_{s_{h-1}\sim b}^{\mathcal{P}}(\cdot\,|\,a_ {h-1},o_{h-1})\in\Delta(\mathcal{S})\) are the belief update operators under the Bayes rule for any \(b\in\Delta(\mathcal{S})\), for which we defer the formal introduction to Appendix C.1.

Notably, this condition is weaker than and thus covers several known tractable classes of POMDPs with sample and computation efficiency guarantees including Block MDP, deterministic POMDP, \(k\)-decodable POMDP as well as a new setting we have identified and existing literature cannot handle. We refer the formal introduction to Appendix E and Figure 1 for an illustration.

In light of the pitfall in Proposition 3.1, we will analyze _both_ the computational and statistical efficiencies of expert distillation in Section 4, under the condition in Definition 3.2.

### Privileged Value Learning: Asymmetric Actor-Critic

Asymmetric actor-critic [68] iterates between two procedures as in standard actor-critic algorithms [42]: policy _improvement_ and policy _evaluation_. As the name suggests, its key difference from the standard actor-critic is that the algorithm maintains \(Q\)-value functions (the critic) based on the _state/privileged information_, while the policy receives only the (partially observable) _history_ as input.

Policy evaluation.At iteration \(t-1\), given the policy \(\pi^{t-1}\), the algorithm estimates \(Q\)-functions in the form of \(\{Q_{h}^{t-1}(\tau_{h},s_{h},a_{h})\}_{h\in[H]}\), where we adopt the "unbiased" version [6] such that functions are conditioned on _both_ the _history_ and the _states_. 2 One key to achieving sample efficiency is by adding some bonus terms in policy evaluation to encourage exploration, i.e., obtaining some _optimistic_\(\bar{Q}\)-function estimates, similarly as in the fully-observable MDP setting, see e.g., [11, 74], for which we defer the detailed introduction to Section 4.

Footnote 2: As pointed out in [6], the original asymmetric actor-critic [68], where the value function was only conditioned on the _state_, is a _biased_ estimate of the actual history-conditioned value function that appears in the policy gradient in the infinite-horizon discounted setting. We verify that such a state-based value function is indeed also biased under our finite-horizon setting, see Remark E.1 for an example. We will thus use the unbiased value function estimate conditioned on _both_ the state and the history throughout.

Policy improvementAt each iteration \(t\), given the critic \(\{Q_{h}^{t-1}(\tau_{h},s_{h},a_{h})\}_{h\in[H]}\) for \(\pi^{t-1}\), the vanilla asymmetric actor-critic algorithm updates the policy according to the _sample-based_ gradient estimation via \(K\) trajectories \(\{s_{1:H+1}^{k},o_{1:H}^{k},a_{1:H}^{k}\}_{k\in[K]}\) sampled from \(\pi^{t-1}\)

\[\pi^{t}\leftarrow\textsc{Proj}_{\Pi}\left(\pi^{t-1}+\frac{\lambda_{t}}{K} \sum_{k\in[K]}\sum_{h\in[H]}\nabla_{\pi}\log\pi_{h}^{t-1}(a_{h}^{k}\,|\,\tau_{ h}^{k})Q_{h}^{t-1}(\tau_{h}^{k},s_{h}^{k},a_{h}^{k})\right),\] (3.2)

where \(\lambda_{t}\) is the step-size and \(\textsc{Proj}_{\Pi}\) is the projection operator onto the space of \(\Pi\), which corresponds to projecting onto the simplex of \(\Delta(\mathcal{A})\) for each \(h\in[H]\). Here we point out the potential drawback of the vanilla algorithm as in [68, 6], where the key insight is that for each iteration of policy evaluation and improvement, one roughly _only_ performs the computation of order \(\mathcal{O}(KH)\), while needing to collect \(K\) new episodes of samples. Thus, the _sample complexity_ will scale in the same order as the _computational complexity_ when the algorithm converges after some iterations to an \(\epsilon\)-optimal solution, which will be super-polynomial even for \(\gamma\)-observable POMDPs [27]. Proof of the result is deferred to Appendix E.

**Proposition 3.3** (Inefficiency of vanilla asymmetric actor-critic).: Under the tabular parameterization for both the policy and the value function, the vanilla asymmetric actor-critic algorithm (Equation (3.2)) suffers from super-polynomial sample complexity for \(\gamma\)-observable POMDPs under standard hardness assumptions.

To address such an issue, one may need to perform _more_ computation _per iteration_, so that although the _total_ computational complexity (iteration number \(\times\) per-iteration computational complexity) is super-polynomial, the total iteration number can be lower such that the total sample complexity may be lower as well. This desideratum is hard to achieve if one _computes_ policy update only on the _sampled_ trajectories \(\tau_{h}\) per iteration, i.e., update _asynchronously_, since this will couple the scales of computational and sample complexities similarly as Equation (3.2). In contrast, we first propose to update _all_ trajectories per iteration in a _synchronous_ way, with the following proximal-policy optimization-type [72] policy improvement update with the state-history-dependent \(Q\)-functions \(\{Q_{h}^{t-1}(\tau_{h},s_{h},a_{h})\}_{h\in[H]}\):

\[\pi_{h}^{t}(\cdot\,|\,\tau_{h})\propto\pi_{h}^{t-1}(\cdot\,|\,\tau_{h})\exp \left(\eta\mathbb{E}_{s_{h}\sim\bm{b}_{h}(\tau_{h})}\left[Q_{h}^{t-1}(\tau_{h},s_{h},\cdot)\right]\right),\forall h\in[H],\tau_{h}\in\mathcal{T}_{h},\] (3.3)

where we recall \(\bm{b}_{h}(\tau_{h})\in\Delta(\mathcal{S})\) denotes the belief state and \(\eta>0\) is the learning rate. This update rule also reduces to the natural policy gradient (NPG) [40] update under the softmax policy parameterization in the fully-observable case [1], when updated for each state \(s_{h}\) separately [74]. We defer the detailed derivation of Equation (3.3) to Appendix E.

However, such an update presents two challenges: (1) It requires enumerating all possible \(\tau_{h}\), whose number scales exponentially with the horizon, making it still computationally intractable; (2) An explicit belief function \(\bm{b}_{h}\) is needed. Motivated by these two challenges, we propose to consider _finite-memory_-based policy and assume access to an approximate belief function \(\{\bm{b}_{h}^{\text{\tiny{spx}}}:\mathcal{Z}_{h}\to\Delta(\mathcal{S})\}_{h \in[H]}\) (the learning for which will be made clear later). Correspondingly, the policy update is modified as:

\[\pi_{h}^{t}(\cdot\,|\,z_{h})\propto\pi_{h}^{t-1}(\cdot\,|\,z_{h})\exp\left( \eta\mathbb{E}_{s_{h}\sim\bm{b}_{h}^{\text{\tiny{spx}}}(z_{h})}\left[Q_{h}^{t- 1}(z_{h},s_{h},\cdot)\right]\right),\forall h\in[H],z_{h}\in\mathcal{Z}_{h}.\]

Then we develop and analyze one possible approach to learning such an approximate belief efficiently (c.f. Section 5). It is worth noting that the policy optimization algorithm we aim to develop and analyze does not depend on the specific algorithm approximate belief learning. Such a decoupling enables a more modular algorithm design framework, and can potentially incorporate the rich literature on learning approximate beliefs in practice, see e.g., [24, 65, 16, 87, 83], which has mostly not been theoretically analyzed before. We will thus analyze such an oracle in Section 5.

Provably Efficient Expert Policy Distillation

We now focus on the provable correctness and efficiency of expert policy distillation, under the deterministic filter condition in Definition 3.2. We will defer all the proofs in this section to Appendix F. Definition 3.2 motivates us to consider only succinct policies that incorporate an auxiliary parameter representing the most recent state, as well as the most recent observations and actions. We consider policies that are the composition of two functions: at step \(h\) a function \(g_{h}:\mathcal{S}\times\mathcal{A}\times\mathcal{O}\to\mathcal{S}\) that decodes the state based on the previous state, the most recent action, and the most recent observation, and a policy \(\pi^{E}\in\Pi_{\mathcal{S}}\) that takes as input the current (decoded) underlying state and outputs a distribution over actions.

**Definition 4.1**.: We define a policy class \(\Pi^{D}\) as:

\[\Pi^{D}=\left\{\pi^{E}_{h}\left(g_{h}(s_{h-1},a_{h-1},o_{h})\right):g_{h}: \mathcal{S}\times\mathcal{A}\times\mathcal{O}\to\mathcal{S},\pi^{E}_{h}: \mathcal{S}\to\Delta(\mathcal{A})\right\}_{h\in[H]},\]

where \(\pi^{E}\) stands for an arbitrary expert policy, and \(\Pi^{D}\) stands for the distilled policy class, and for \(h=1\), \(a_{0}\), \(s_{0}\) are some fixed dummy action and state. Intuitively, the distilled policy \(\pi\in\Pi^{D}\) executes as follows: it firstly _decodes_ the underlying states by applying \(\{g_{h}\}_{h\in[H]}\)_recursively_ along the history, and then takes actions using \(\pi^{E}\) based on the decoded states.

Our goal is to learn the two functions independently, that is, we want to learn an approximately optimal policy \(\pi^{E}\in\Pi_{\mathcal{S}}\) with respect to the MDP \(\mathcal{M}\) derived from POMDP \(\mathcal{P}\) by omitting the observations and observing the underlying state (see Definition 4.2 for a formal definition), and for each step \(h\in[H]\), a decoding function \(g_{h}(s_{h-1},a_{h-1},o_{h})\) such that the probability of _incorrectly_ decoding a state-action-observation triplet over the trajectories induced by the policy \(\pi^{E}\) is low.

**Definition 4.2** (POMDP-induced MDP).: Given a POMDP \(\mathcal{P}=(H,\mathcal{S},\mathcal{A},\mathcal{O},\mathbb{T},\mathbb{O},\mu_ {1},r)\), we define its associated Markov Decision Process (MDP) \(\mathcal{M}\) as \(\mathcal{M}=(H,\mathcal{S},\mathcal{A},\mathbb{T},\mu_{1},r)\) without observations.

**Definition 4.3**.: Consider a POMDP \(\mathcal{P}\) that satisfies Definition 3.2, and let \(\psi=\{\psi_{h}\}_{h\in[H]}\) be the promised set of functions that always correctly decode a state-action-observation triplet into an underlying state. Consider policy \(\widetilde{\pi^{E}}=\{\pi^{E}_{h}(\psi(\cdot)):\mathcal{S}\times\mathcal{A} \times\mathcal{O}\to\Delta(\mathcal{A})\}_{h\in[H]}\in\Pi^{D}\). We slightly abuse the notation and simply denote by \(v^{\mathcal{P}}(\pi^{E})=v^{\mathcal{P}}(\widetilde{\pi^{E}})\).

**Lemma 4.4**.: Let \(\mathcal{P}=(H,\mathcal{S},\mathcal{A},\mathcal{O},\mathbb{T},\mathbb{O},\mu_ {1},r)\) be a POMDP that satisfies Definition 3.2, and consider a policy \(\pi^{E}\in\Pi_{\mathcal{S}}\). Consider a set of decoding functions \(\{g_{h}\}_{h\in[H]}\) such that, \(\mathbb{P}^{\pi^{E},\mathcal{P}}\)\([\exists h\in[H]:g_{h}\left(s_{h-1},a_{h-1},o_{h}\right)\neq s_{h}]\leq\epsilon\). Consider the policy \(\pi=\left\{\pi^{E}_{h}\left(g_{h}(\cdot)\right):\mathcal{S}\times\mathcal{A} \times\mathcal{O}\to\Delta(\mathcal{A})\right\}_{h\in[H]}\) on the POMDP \(\mathcal{P}\), then: \(v^{\mathcal{P}}(\pi)\geq v^{\mathcal{P}}(\widetilde{\pi^{E}})-H\epsilon\).

We can use any off-the-shelf algorithm to learn an approximate optimal policy \(\pi^{E}\) for the associated MDP \(\mathcal{M}\) (see Definition 4.2). Thus, in the rest of the section, we focus on learning the decoding function \(\left\{g_{h}\right\}_{h\in[H]}\). To efficiently learn the decoding function, we model the access to the underlying state by keeping track of the most recent pair of the action and the observation, as well as the two most recent states. We summarize the algorithm of decoding-function learning in Algorithm 1.

**Theorem 4.5**.: Consider a POMDP \(\mathcal{P}\) that satisfies Definition 3.2, a policy \(\pi^{E}\in\Pi_{\mathcal{S}}\), and let \(\{g_{h}\}_{h\in[H]}\) be the output of Algorithm 1 with \(M=\frac{AOS+\log(H/\delta)}{\epsilon^{2}}\). Then, with probability at least \(1-\delta\), for each step \(h\in[H]\): \(\mathbb{P}^{\pi^{E},\mathcal{P}}\)\([\exists h\in[H]:g_{h}\left(s_{h-1},a_{h-1},o_{h}\right)\neq s_{h}]\leq\epsilon\), using \(\textsc{poly}(H,A,O,S,\frac{1}{\epsilon},\log\left(\frac{1}{\delta}\right))\) episodes in time \(\textsc{poly}(H,A,O,S,\frac{1}{\epsilon},\log\left(\frac{1}{\delta}\right))\).

The following is an immediate consequence of Lemma 4.4 and Theorem 4.5. Note that _both_ the sample and computation complexities are _polynomial_, which is in stark contrast to the \(k\)-decodable POMDP case [19] (a special one covered by our Definition 3.2), for which the sample complexity is necessarily _exponential in \(k\)_ when there is no privileged information [19]. In fact, thanks to privileged information, the complexities are only polynomial in horizon \(H\) even when the decodable length is _unknown_. For the benefits of using privileged information in several other subclasses of problems, we refer to Table 1 for more details.

**Theorem 4.6**.: Let \(\mathcal{P}\) satisfy Definition 3.2 and consider any policy \(\pi^{E}\in\Pi_{\mathcal{S}}\). Using \(\textsc{poly}(H,A,O,S,\frac{1}{\epsilon},\log\left(\frac{1}{\delta}\right))\) episodes and in time \(\textsc{poly}(H,A,O,S,\frac{1}{\epsilon},\log\left(\frac{1}{\delta}\right))\), we can compute a policy \(\pi\in\Pi^{D}\) (see Definition 4.1) such that with probability at least \(1-\delta\), \(v^{\mathcal{P}}(\pi)\geq v^{\mathcal{P}}(\pi^{E})-\epsilon\).

Extension to the case with general function approximation.Due to the modularity of our algorithmic framework and its compatibility with supervised learning oracles, it can be readily generalized to the function approximation setting to handle large observation spaces. We defer the corresponding results to Appendix G.

Provable Asymmetric Actor-Critic with Approximate Belief Learning

Unlike most existing theoretical studies on provably sample-efficient partially observable RL [36; 25; 47], which directly learn an approximate _POMDP model_ for planning near-optimal policies, we consider a general framework with two steps: firstly learning an approximate _belief function_, followed by adopting a _fully observable RL_ subroutine on the belief state space.

### Belief-Weighted Optimistic Asymmetric Actor-Critic

We now introduce our main algorithmic contribution to the privileged policy learning setting. Our algorithm is conceptually similar to the natural policy gradient methods [40; 1; 74] in the fully-observable setting, with additional weighting over the states \(s_{h}\) using some learned belief states, to handle the additional _state_-dependence in the asymmetric critic. The overall algorithm is presented in Algorithm 2. The algorithm requires a belief-learning subroutine that takes the stored memory as input and outputs a belief about the underlying state (c.f. \(\{\bm{b}_{h}^{\text{apx}}\}_{h\in[H]}\)). Additionally, similar to the fully observable setting, we include a subroutine to estimate the \(Q\)-function, which introduces additional challenges due to partial observability (see Appendix H). We establish the performance guarantee of Algorithm 2 in the following theorem. We defer the proof to Appendix H.

**Theorem 5.1** (Near-optimal policy).: Fix \(\epsilon,\delta\in(0,1)\). Given a POMDP \(\mathcal{P}\) and an approximate belief \(\{\bm{b}_{h}^{\text{apx}}:\mathcal{Z}_{h}\to\Delta(\mathcal{S})\}_{h\in[H]}\), with probability at least \(1-\delta\), Algorithm 2 can learn an approximate optimal policy \(\pi^{\star}\) of \(\mathcal{P}\) in the space of \(\Pi^{L}\) such that \(v^{\mathcal{P}}(\pi^{\star})\geq\max_{\pi\in\Pi^{L}}v^{\mathcal{P}}(\pi)- \mathcal{O}(\epsilon+H^{2}\epsilon_{\text{belief}})\), with sample complexity \(\textsc{poly}(S,A,O,H,\frac{1}{\epsilon},\log\frac{1}{\delta})\) and time complexity \(\textsc{poly}(S,A,O,H,Z,\frac{1}{\epsilon},\log\frac{1}{\delta})\), where \(\epsilon_{\text{belief}}\) is the belief-learning error defined as \(\epsilon_{\text{belief}}:=\max_{h\in[H]}\max_{\pi\in\Pi^{L}}\mathbb{E}_{\pi} ^{\mathcal{P}}\|\bm{b}_{h}(\tau_{h})-\bm{b}_{h}^{\text{apx}}(z_{h})\|_{1}\) and \(Z:=\max_{h\in[H]}|\mathcal{Z}_{h}|\). Furthermore, if \(\mathcal{P}\) is additionally \(\gamma\)-observable (c.f. Assumption 2.2), then \(\pi^{\star}\) is also an approximate optimal policy in the space of \(\Pi\) such that \(v^{\mathcal{P}}(\pi^{\star})\geq\max_{\pi\in\Pi}v^{\mathcal{P}}(\pi)-\mathcal{ O}(\epsilon+H^{2}\epsilon_{\text{belief}})\), as long as \(L\geq\widetilde{\Omega}(\gamma^{-4}\log(SH/\epsilon))\).

### Approximate Belief Learning

At a high level, our belief-learning algorithm first learns an approximate POMDP model \(\widehat{\mathcal{P}}\) by explicitly exploring the state space. The main technical challenge here is that there may exist states that are reachable with very low probability, making it infeasible to collect enough samples to sufficiently explore them, thus potentially breaking the \(\gamma\)-observability property of the ground-truth model \(\mathcal{P}\). To circumvent this issue, we ignore such hard-to-visit states and _redirect_ probabilities flowing to them to certain other states. Thus, in our truncated POMDP, where each state is sufficiently explored, we can approximate the transition and emission matrices to a desired accuracy _uniformly across all the states_ and preserve the \(\gamma\)-observability property. This ensures that the learned approximate belief function in the truncated POMDP is sufficiently close to the actual belief function of the original POMDP \(\mathcal{P}\). Note that the key to achieving belief learning with _both_ polynomial sample and time complexities is our explicit exploration in the state space, which relies on executing _fully observable_ policies from an _MDP learning_ subroutine. We remark that the belief function may also be learned even if the state space is only explored by partially observable policies, thus utilizing only hindsight observability may be sufficient for this purpose [44]. However, for such exploration to be _computationally tractable_, one requires to avoid using computationally intractable oracles for _POMDP learning_, which is in fact our final goal. We present the guarantees in the next theorem and postpone the proof to Appendix H.

**Theorem 5.2**.: Consider a \(\gamma\)-observable POMDP \(\mathcal{P}\) (c.f. Assumption 2.2) and assume that \(L\geq\widetilde{\Omega}(\gamma^{-4}\log(SH/\epsilon))\) for an \(\epsilon>0\). Then, we can learn an approximate belief \(\{\bm{b}_{h}^{\text{apx}}\}_{h\in[H]}\) from Algorithm 4 using \(\widetilde{\mathcal{O}}(\frac{S^{2}AH^{2}O+S^{3}AH^{2}}{e^{2}}+\frac{S^{4}A^{2 }H^{6}\epsilon Q}{e\gamma^{2}})\) episodes in time \(\textsc{poly}\left(S,H,A,O,\frac{1}{\gamma},\frac{1}{\epsilon},\log\left(\frac {1}{\delta}\right)\right)\) such that with probability at least \(1-\delta\), for any \(\pi\in\Pi^{L}\) and \(h\in[H]\), \(\mathbb{E}_{\pi}^{\mathcal{P}}\|\bm{b}_{h}(\tau_{h})-\bm{b}_{h}^{\text{apx}}(z_{h })\|_{1}\leq\epsilon\).

Theorem 5.2 shows that an approximate belief can be learned with both polynomial samples and time, which, combined with Theorem 5.1, yields the final polynomial sample and quasi-polynomial time guarantee below. In contrast to the case without privileged information [25; 27], the sample complexity is reduced from quasi-polynomial to polynomial for \(\gamma\)-observable POMDPs. Note that the computational complexity remains quasi-polynomial, which is known to be unimprovable even for planning [27]. The key to such an improvement, as pointed out in Section 3.2, is the more practical update rule of actor-critic (in conjunction with our belief-weighted idea), which allows _more computation_ at each iteration (instead of only performing computation at the _sampled_ finite-memory). This allows the total computation to remain quasi-polynomial, while the overall sample complexity becomes polynomial. A detailed comparison can be found in Table 1.

**Theorem 5.3**.: Let \(\mathcal{P}\) be a \(\gamma\)-observable POMDP (c.f. Assumption 2.2), and consider \(L\geq\widetilde{\Omega}(\gamma^{-4}\log(SH/\epsilon))\) for an \(\epsilon>0\). With probability at least \(1-\delta\), Algorithm 2 can learn a policy \(\pi\in\Pi^{L}\) such that \(v^{\mathcal{P}}(\pi)\geq\max_{\pi^{\prime}\in\Pi}v^{\mathcal{P}}(\pi^{\prime} )-\epsilon\), using \(\textsc{poly}(S,H,1/\epsilon,1/\gamma,\log(1/\delta),O,A)\) episodes and in time \(\textsc{poly}(S,H,1/\epsilon,\log(1/\delta),O^{L},A^{L})\).

## 6 Numerical Validation

We now provide some numerical results for both of our principled algorithms. Here we mainly compare with two baselines, the vanilla asymmetric actor-critic [68], and asymmetric \(Q\)-learning [7], on two settings, POMDP under the deterministic filter condition (c.f. Definition 3.2) and general POMDPs. We report the results in Table 2 and Figure 2, where our algorithms converge faster to higher rewards. We defer the implementation details and discussions to Appendix I.

## 7 Extension to Partially Observable MARL with Privileged Information

### Privileged Policy Learning: Equilibrium Distillation

To understand how the deterministic filter condition may be extended for POSGs, we first note the following equivalent characterization of Definition 3.2, the proof of which is deferred to Appendix J.

**Proposition 7.1**.: Definition 3.2 is equivalent to the following: for each \(h\in[H]\), there exists an _unknown_ function \(\phi_{h}:\mathcal{T}_{h}\to\mathcal{S}\) such that \(\mathbb{P}^{\mathcal{P}}(s_{h}=\phi_{h}(\tau_{h})\,|\,\tau_{h})=1\) for any reachable \(\tau_{h}\in\mathcal{T}_{h}\).

Proposition 7.1 implies that at each step \(h\), given the _entire_ history information, the agent can uniquely decode the current underlying state \(s_{h}\). Thus, we generalize this condition to POSGs by requiring each agent to uniquely decode the current state \(s_{h}\) given the information it has collected so far.

**Definition 7.2** (Deterministic filter condition for POSGs).: We say a POSG \(\mathcal{G}\) satisfies the _deterministic filter condition_ if for each \(i\in[n]\), \(h\in[H]\), there exists _an unknown_ function \(\phi_{i,h}:\mathcal{C}_{h}\times\mathcal{P}_{i,h}\to\mathcal{S}\) such that \(\mathbb{P}^{\mathcal{G}}(s_{h}=\phi_{i,h}(c_{h},p_{i,h})\,|\,c_{h},p_{i,h})=1\) for any reachable \((c_{h},p_{i,h})\).

Here we have required that each agent can decode the underlying state through their own information _individually_. Naturally, one may wonder whether one can relax it so that only the _joint_ history information of all the agents can decode the underlying state. However, we point out in the following that it does not circumvent the computational hardness of POSG, the proof of which is deferred to Appendix J. Note that the computational hardness result can not be mitigated even with privileged state information, as the hardness we state here holds even for the planning problem with model knowledge, with which one can simulate the RL problems with privileged information.

**Proposition 7.3**.: Computing CCE in POSGs that satisfy that for each step \(h\in[H]\), there exists a function \(\phi_{h}:\mathcal{C}_{h}\times\mathcal{P}_{h}\to\mathcal{S}\) such that \(\mathbb{P}^{\mathcal{G}}(s_{h}=\phi_{h}(c_{h},p_{h})\,|\,c_{h},p_{h})=1\) for any reachable \((c_{h},p_{h})\) is still PSPACE-hard.

Learning multi-agent individual decoding functions with unilateral exploration.Similar to our framework for POMDPs, our framework for POSGs is also decoupled into two steps: i) learning an _expert_ equilibrium policy that is fully observable, ii) learning the _decoding function_, where the first step can be instantiated by any provable off-the-shelf algorithm of learning in Markov games. The major difference from the framework for POMDPs lies in how to learn the decoding function. In Theorem J.1, we prove that the difference of the NE/CE/CCE-gap between the expert policy and the distilled student policy is bounded by the decoding errors under policies from the _unilateral deviation_ of the expert policy. Hence, given the expert policy \(\pi\), the key algorithmic principle is to perform _unilateral exploration_ for each agent \(i\) to make sure the decoding function is accurate under policies

Figure 2: Results for POMDPs of different sizes, where our methods achieve the best performance with the lowest sample complexity (VI: value iteration; AAC: asymmetric actor-critic).

\((\pi^{\prime}_{i},\pi_{-i})\) for any \(\pi^{\prime}_{i}\), keeping \(\pi_{-i}\) fixed. We refer the detailed algorithm to Algorithm 5, and present below the guarantees for learning the decoding functions and the corresponding distilled policy for learning NE/CCE, while we defer the results for learning CE to Theorem J.6.

**Theorem 7.4** (Equilibria learning; Combining Theorem J.1 and Theorem J.4).: Under Assumption C.8 and conditions of Definition 7.2, given a \(\frac{\epsilon}{2}\)-NE/CCE \(\pi^{E}\) for the associated Markov game of \(\mathcal{G}\), Algorithm 5 can learn decoding function \(\{\widehat{g}_{i,h}\}_{i\in[n],h\in[H]}\) such that with probability at least \(1-\delta\), it is guaranteed that \(\max_{u_{i}\in\Pi,j\in[n]}\mathbb{P}^{u_{i}\times\pi_{-i},\mathcal{G}}(s_{h} \neq\widehat{g}_{j,h}(c_{h},p_{j,h}))\leq\frac{\epsilon}{4nH^{2}},\) for any \(i\in[n],h\in[H]\) with both sample and computational complexities \(\textsc{poly}(S,A,H,O,\frac{1}{\epsilon},\log\frac{1}{\delta})\). Consequently, policy \(\pi\) distilled from \(\pi^{E}\) (c.f. Theorem J.1 for the formal distillation procedures) is an \(\epsilon\)-NE/CCE of \(\mathcal{G}\).

### Privileged Value Learning: Asymmetric MARL with Approximate Belief Learning

For POMDPs, we have used _finite-memory_ policies for computational efficiency. We generalize to POSGs with information sharing by defining the _compression_ of the common information.

**Definition 7.5** (Compressed approximate common information [57, 79, 51]).: For each \(h\in[H]\), given a set \(\widehat{\mathcal{C}}_{h}\), we say \(\textsc{Compress}_{h}\) is a compression function if \(\textsc{Compress}_{h}\in\{f:\mathcal{C}_{h}\to\widehat{\mathcal{C}}_{h}\}\). For each \(c_{h}\in\mathcal{C}_{h}\), we denote \(\widehat{c}_{h}:=\textsc{Compress}_{h}(c_{h})\). We also require the compression function to satisfy the regularity condition that for each \(h\in[H]\), there exists a function \(\widehat{\Lambda}_{h+1}\) such that \(\widehat{c}_{h+1}=\widehat{\Lambda}_{h+1}(\widehat{c}_{h},\varpi_{h+1})\), for any \(c_{h}\in\mathcal{C}_{h}\), \(\varpi_{h+1}\in\Upsilon_{h+1}\), where we recall \(c_{h+1}:=c_{h}\cup\varpi_{h+1}\) and the definition of \(\Upsilon_{h+1}\) in Assumption C.7.

Similar to the framework we developed for POMDPs in Section 5, we firstly develop the multi-agent RL algorithm based on some approximate belief, and then instantiate it with one provable approach for learning such an approximate belief.

Optimistic value iteration of POSGs with approximate belief.For POMDPs, the sufficient statistics for optimal decision-making is the posterior distribution over the state given history. However, for POSGs with information-sharing, as shown in [63, 62, 51], the sufficient statistics become the posterior distribution over the state _and the private information_ given the common information, instead of only the state. Therefore, we consider the approximate belief in the form of \(\widehat{P}_{h}:\widehat{\mathcal{C}}_{h}\to\Delta(\mathcal{P}_{h}\times \mathcal{S})\) for each \(h\in[H]\), where we define the error compared with the ground-truth belief to be \(\epsilon_{\text{belief}}:=\max_{h\in[H]}\max_{\pi\in\Pi}\mathbb{E}_{\pi}^{ \mathcal{G}}\sum_{s_{h},p_{h}}|\mathbb{P}^{\mathcal{G}}(s_{h},p_{h}\,|\,c_{h} )-\widehat{P}_{h}(s_{h},p_{h}\,|\,\widehat{c}_{h})|\), i.e., the _expected_ total variation distance from the true one. Note that both \(\widehat{P}_{h}\) and thus \(\epsilon_{\text{belief}}\) have implicit dependencies on \(\textsc{Compress}_{h}\), as \(\widehat{c}_{h}:=\textsc{Compress}_{h}(c_{h})\). We outline our algorithm in Algorithm 7, which is conceptually similar to the algorithm for POMDPs (Algorithm 2), maintaining the asymmetric critic (i.e., value function), and performing the actor update (i.e., policy update) using the _belief-weighted_ value function.

**Theorem 7.6** (Equilibria learning; Combining Theorem J.15 and Theorem J.16).: Fix \(\epsilon,\delta\in(0,1)\). Under Assumption C.8, with probability at least \(1-\delta\), Algorithm 7 can learn an \((\epsilon+H^{2}\epsilon_{\text{belief}})\)-NE if \(\mathcal{G}\) is zero-sum and \((\epsilon+H^{2}\epsilon_{\text{belief}})\)-CEE if \(\mathcal{G}\) is general-sum with sample complexity \(\mathcal{O}(\frac{H^{4}SAO\log(SAHO/\delta)}{\epsilon^{2}})\) and computational complexity \(\textsc{poly}(S,(AO)^{\mathcal{O}(\gamma^{-4}\log(SH/\epsilon))},H,\frac{1}{ \epsilon},\log\frac{1}{\delta})\).

Learning approximate belief with model truncation.The belief learning algorithm we design for POSGs is conceptually similar to that we designed for POMDPs, where the key to achieving _both_ polynomial sample and computational complexity is still to firstly learn approximate models, i.e., transitions and emissions, and then carefully _truncate_ (as in Section 5.2) its transition and emission to build the approximate belief, where we defer the detailed algorithm to Algorithm 8. Next, we provide its provable guarantees, which leads to a final polynomial-sample and quasi-polynomial-time complexity result when combined with Theorem 7.6.

**Theorem 7.7**.: For any \(\epsilon>0\), under Assumption 2.2, it holds that one can learn the approximate belief \(\{\widehat{P}_{h}:\widehat{\mathcal{C}}_{h}\to\Delta(\mathcal{S}\times\mathcal{P }_{h})\}_{h\in[H]}\) such that \(\epsilon_{\text{belief}}\leq\frac{\epsilon}{H^{2}}\) with both polynomial sample complexity and computational complexity \(\textsc{poly}(S,A,O,H,\frac{1}{\gamma},\frac{1}{\epsilon},\log\frac{1}{\delta})\) for all the examples in Appendix C.3. As a consequence, Algorithm 7 can learn an \(\epsilon\)-NE if \(\mathcal{G}\) is zero-sum and \(\epsilon\)-CEE/CCE if \(\mathcal{G}\) is general-sum with sample complexity \(\mathcal{O}(\frac{H^{4}SAO\log(SAHO/\delta)}{\epsilon^{2}})\) and computational complexity \(\textsc{poly}(S,(AO)^{\mathcal{O}(\gamma^{-4}\log(SH/\epsilon))},H,\frac{1}{ \epsilon},\log\frac{1}{\delta})\).

## Acknowledgement

The authors would like to thank the anonymous reviewers and area chair from NeurIPS 2024 for the valuable feedback. Y.C. acknowledges the support from the NSF Awards CCF-1942583 (CAREER) and CCF-2342642. X.L. and K.Z. acknowledge the support from Army Research Laboratory (ARL) Grant W911NF-24-1-0085. K.Z. also acknowledges the support from Simons-Berkeley Research Fellowship. A.O. acknowledges financial support from a Meta PhD fellowship, a Sloan Foundation Research Fellowship and the NSF Award CCF-1942583 (CAREER). Part of this work was done while the authors were visiting the Simons Institute for the Theory of Computing.

## References

* [1] A. Agarwal, S. M. Kakade, J. D. Lee, and G. Mahajan. Optimality and approximation with policy gradient methods in Markov decision processes. In _Conference on Learning Theory_, pages 64-66, 2020.
* [2] E. Altman, V. Kambley, and A. Silva. Stochastic games with one step delay sharing information pattern with application to power control. In _2009 International Conference on Game Theory for Networks_, pages 124-129. IEEE, 2009.
* [3] O. M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. McGrew, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray, et al. Learning dexterous in-hand manipulation. _The International Journal of Robotics Research_, 39(1):3-20, 2020.
* [4] R. J. Aumann, M. Maschler, and R. E. Stearns. _Repeated games with incomplete information_. MIT press, 1995.
* [5] R. Avalos, F. Delgrange, A. Nowe, G. Perez, and D. M. Roijers. The wasserstein believer: Learning belief updates for partially observable environments through reliable latent space models. In _The Twelfth International Conference on Learning Representations_, 2023.
* [6] A. Baisero and C. Amato. Unbiased asymmetric reinforcement learning under partial observability. In _Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems_, 2022.
* [7] A. Baisero, B. Daley, and C. Amato. Asymmetric DQN for partially observable reinforcement learning. In _Uncertainty in Artificial Intelligence_, pages 107-117. PMLR, 2022.
* [8] N. Brukhim, D. Carmon, I. Dinur, S. Moran, and A. Yehudayoff. A characterization of multiclass learnability, 2022.
* [9] N. Brukhim, D. Carmon, I. Dinur, S. Moran, and A. Yehudayoff. A characterization of multiclass learnability. In _2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS)_, pages 943-955. IEEE, 2022.
* [10] S. Bubeck. Convex optimization: Algorithms and complexity. _Found. Trends Mach. Learn._, 8(3-4):231-357, 2015.
* [11] Q. Cai, Z. Yang, C. Jin, and Z. Wang. Provably efficient exploration in policy optimization. In _International Conference on Machine Learning_, pages 1283-1294. PMLR, 2020.
* [12] Q. Cai, Z. Yang, and Z. Wang. Reinforcement learning from partial observation: Linear function approximation with provable sample efficiency. In _International Conference on Machine Learning_, pages 2485-2522. PMLR, 2022.
* [13] C. L. Canonne. A short note on learning discrete distributions. _arXiv preprint arXiv:2002.11457_, 2020.
* [14] D. Chen, B. Zhou, V. Koltun, and P. Krahenbuhl. Learning by cheating. In _Conference on Robot Learning_, pages 66-75. PMLR, 2020.
* [15] F. Chen, Y. Bai, and S. Mei. Partially observable RL with b-stability: Unified structural condition and sharp sample-efficient algorithms. In _The Eleventh International Conference on Learning Representations_, 2023.

* [16] X. Chen, Y. M. Mu, P. Luo, S. Li, and J. Chen. Flow-based recurrent belief state learning for pomdps. In _International Conference on Machine Learning_, pages 3444-3468. PMLR, 2022.
* [17] C. Dann, N. Jiang, A. Krishnamurthy, A. Agarwal, J. Langford, and R. E. Schapire. On oracle-efficient pac rl with rich observations. _Advances in neural information processing systems_, 31, 2018.
* [18] S. Du, A. Krishnamurthy, N. Jiang, A. Agarwal, M. Dudik, and J. Langford. Provably efficient rl with rich observations via latent state decoding. In _International Conference on Machine Learning_, pages 1665-1674. PMLR, 2019.
* [19] Y. Efroni, C. Jin, A. Krishnamurthy, and S. Miryoosefi. Provable reinforcement learning with a short-term memory. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 5832-5850. PMLR, 2022.
* [20] E. Even-Dar, S. M. Kakade, and Y. Mansour. The value of observation for monitoring dynamic systems. In M. M. Veloso, editor, _IJCAI 2007, Proceedings of the 20th International Joint Conference on Artificial Intelligence, Hyderabad, India, January 6-12, 2007_, pages 2474-2479, 2007.
* [21] J. Foerster, I. A. Assael, N. De Freitas, and S. Whiteson. Learning to communicate with deep multi-agent reinforcement learning. _Advances in Neural Information Processing Systems_, 29, 2016.
* [22] J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson. Counterfactual multi-agent policy gradients. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 32, 2018.
* [23] K. Fujii. Bayes correlated equilibria and no-regret dynamics. _arXiv preprint arXiv:2304.05005_, 2023.
* [24] T. Gangwani, J. Lehman, Q. Liu, and J. Peng. Learning belief representations for imitation learning in pomdps. In _uncertainty in artificial intelligence_, pages 1061-1071. PMLR, 2020.
* [25] N. Golowich, A. Moitra, and D. Rohatgi. Learning in observable POMDPs, without computationally intractable oracles. In _Advances in Neural Information Processing Systems_, 2022.
* [26] N. Golowich, A. Moitra, and D. Rohatgi. Planning in observable pomdps in quasipolynomial time. _arXiv preprint arXiv:2201.04735_, 2022.
* [27] N. Golowich, A. Moitra, and D. Rohatgi. Planning and learning in partially observable systems via filter stability. In _Proceedings of the 55th Annual ACM Symposium on Theory of Computing_, pages 349-362, 2023.
* [28] N. Golowich, A. Moitra, and D. Rohatgi. Exploration is harder than prediction: Cryptographically separating reinforcement learning from supervised learning. _arXiv preprint arXiv:2404.03774_, 2024.
* [29] G. J. Gordon, A. Greenwald, and C. Marks. No-regret learning in convex games. In _Proceedings of the 25th international conference on Machine learning_, pages 360-367, 2008.
* [30] J. Guo, M. Chen, H. Wang, C. Xiong, M. Wang, and Y. Bai. Sample-efficient learning of pomdps with multiple observations in hindsight. In _The Twelfth International Conference on Learning Representations_, 2023.
* [31] A. Gupta, A. Nayyar, C. Langbort, and T. Basar. Common information based markov perfect equilibria for linear-gaussian games with asymmetric information. _SIAM Journal on Control and Optimization_, 52(5):3228-3260, 2014.
* [32] J. Hartline, V. Syrgkanis, and E. Tardos. No-regret learning in bayesian games. _Advances in Neural Information Processing Systems_, 28, 2015.

* [33] H. Hu, A. Lerer, N. Brown, and J. Foerster. Learned belief search: Efficiently improving policies in partially observable settings. _arXiv preprint arXiv:2106.09086_, 2021.
* [34] N. Jiang. On value functions and the agent-environment boundary. _arXiv preprint arXiv:1905.13341_, 2019.
* [35] N. Jiang, A. Krishnamurthy, A. Agarwal, J. Langford, and R. E. Schapire. Contextual decision processes with low bellman rank are pac-learnable. In D. Precup and Y. W. Teh, editors, _Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017_, volume 70 of _Proceedings of Machine Learning Research_, pages 1704-1713. PMLR, 2017.
* [36] C. Jin, S. Kakade, A. Krishnamurthy, and Q. Liu. Sample-efficient reinforcement learning of undercomplete POMDPs. _Advances in Neural Information Processing Systems_, 33:18530-18539, 2020.
* [37] C. Jin, A. Krishnamurthy, M. Simchowitz, and T. Yu. Reward-free exploration for reinforcement learning. In _International Conference on Machine Learning_, pages 4870-4879. PMLR, 2020.
* [38] C. Jin, Q. Liu, Y. Wang, and T. Yu. V-learning-a simple, efficient, decentralized algorithm for multiagent rl. _arXiv preprint arXiv:2110.14555_, 2021.
* [39] S. Kakade and J. Langford. Approximately optimal approximate reinforcement learning. In _International Conference on Machine Learning_, volume 2, pages 267-274, 2002.
* [40] S. M. Kakade. A natural policy gradient. In _Advances in Neural Information Processing Systems_, pages 1531-1538, 2002.
* [41] B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. Al Sallab, S. Yogamani, and P. Perez. Deep reinforcement learning for autonomous driving: A survey. _IEEE Transactions on Intelligent Transportation Systems_, 23(6):4909-4926, 2021.
* [42] V. R. Konda and J. N. Tsitsiklis. Actor-critic algorithms. In _Advances in Neural Information Processing Systems_, pages 1008-1014, 2000.
* [43] A. Krishnamurthy, A. Agarwal, and J. Langford. Pac reinforcement learning with rich observations. _Advances in Neural Information Processing Systems_, 29, 2016.
* [44] J. Lee, A. Agarwal, C. Dann, and T. Zhang. Learning in pomdps is sample-efficient with hindsight observability. In _International Conference on Machine Learning_, pages 18733-18773. PMLR, 2023.
* [45] J. Lee, J. Hwangbo, L. Wellhausen, V. Koltun, and M. Hutter. Learning quadrupedal locomotion over challenging terrain. _Science robotics_, 5(47):eabc5986, 2020.
* [46] S. Levine, C. Finn, T. Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies. _The Journal of Machine Learning Research_, 17(1):1334-1373, 2016.
* [47] Q. Liu, A. Chung, C. Szepesvari, and C. Jin. When is partially observable reinforcement learning not scary? In _Conference on Learning Theory_, pages 5175-5220, 2022.
* [48] Q. Liu, P. Netrapalli, C. Szepesvari, and C. Jin. Optimistic MLE: A generic model-based algorithm for partially observable sequential decision making. In _Proceedings of the 55th Annual ACM Symposium on Theory of Computing_, pages 363-376, 2023.
* [49] Q. Liu, C. Szepesvari, and C. Jin. Sample-efficient reinforcement learning of partially observable Markov games. In _Advances in Neural Information Processing Systems_, 2022.
* [50] Q. Liu, T. Yu, Y. Bai, and C. Jin. A sharp analysis of model-based reinforcement learning with self-play. In _International Conference on Machine Learning_, pages 7001-7010. PMLR, 2021.
* [51] X. Liu and K. Zhang. Partially observable multi-agent RL with (quasi-)efficiency: the blessing of information sharing. In _International Conference on Machine Learning_, pages 22370-22419. PMLR, 2023.

* [52] X. Liu and K. Zhang. Partially observable multi-agent reinforcement learning with information sharing, 2024.
* [53] R. Lowe, Y. I. Wu, A. Tamar, J. Harb, O. Pieter Abbeel, and I. Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. _Advances in Neural Information Processing Systems_, 30, 2017.
* [54] M. Lu, Y. Min, Z. Wang, and Z. Yang. Pessimism in the face of confounders: Provably efficient offline reinforcement learning in partially observable markov decision processes. In _The Eleventh International Conference on Learning Representations_, 2023.
* [55] X. Lyu, A. Baisero, Y. Xiao, and C. Amato. A deeper understanding of state-based critics in multi-agent reinforcement learning. In _Proceedings of the AAAI conference on artificial intelligence_, volume 36, pages 9396-9404, 2022.
* [56] X. Lyu, A. Baisero, Y. Xiao, B. Daley, and C. Amato. On centralized critics in multi-agent reinforcement learning. _Journal of Artificial Intelligence Research_, 77:295-354, 2023.
* [57] W. Mao, K. Zhang, E. Miehling, and T. Basar. Information state embedding in partially observable cooperative multi-agent reinforcement learning. In _2020 59th IEEE Conference on Decision and Control (CDC)_, pages 6124-6131. IEEE, 2020.
* [58] G. B. Margolis, T. Chen, K. Paigwar, X. Fu, D. Kim, S. bae Kim, and P. Agrawal. Learning to jump from pixels. In _5th Annual Conference on Robot Learning_, 2021.
* [59] T. Miki, J. Lee, J. Hwangbo, L. Wellhausen, V. Koltun, and M. Hutter. Learning robust perceptive locomotion for quadrupedal robots in the wild. _Science Robotics_, 7(62):eabb2822, 2022.
* [60] D. Misra, M. Henaff, A. Krishnamurthy, and J. Langford. Kinematic state abstraction and provably efficient rich-observation reinforcement learning. In _International conference on machine learning_, pages 6961-6971. PMLR, 2020.
* [61] P. Moreno, J. Humplik, G. Papamakarios, B. A. Pires, L. Buesing, N. Heess, and T. Weber. Neural belief states for partially observed domains. In _NeurIPS 2018 Workshop on Reinforcement Learning under Partial Observability_, 2018.
* [62] A. Nayyar, A. Gupta, C. Langbort, and T. Basar. Common information based markov perfect equilibria for stochastic games with asymmetric information: Finite games. _IEEE Transactions on Automatic Control_, 59(3):555-570, 2013.
* [63] A. Nayyar, A. Mahajan, and D. Teneketzis. Decentralized stochastic control with partial history sharing: A common information approach. _IEEE Transactions on Automatic Control_, 58(7):1644-1658, 2013.
* [64] H. Nguyen, A. Baisero, D. Wang, C. Amato, and R. Platt. Leveraging fully observable policies for learning under partial observability. In _Conference on Robot Learning_, 2022.
* [65] H. Nguyen, B. Daley, X. Song, C. Amato, and R. Platt. Belief-grounded networks for accelerated robot learning under partial observability. In _Conference on Robot Learning_, pages 1640-1653. PMLR, 2021.
* [66] C. H. Papadimitriou and J. N. Tsitsiklis. The complexity of markov decision processes. _Mathematics of operations research_, 12(3):441-450, 1987.
* [67] A. Pathak, H. Pucha, Y. Zhang, Y. C. Hu, and Z. M. Mao. A measurement study of internet delay asymmetry. In _Passive and Active Network Measurement: 9th International Conference, PAM 2008, Cleveland, OH, USA, April 29-30, 2008. Proceedings 9_, pages 182-191. Springer, 2008.
* [68] L. Pinto, M. Andrychowicz, P. Welinder, W. Zaremba, and P. Abbeel. Asymmetric actor critic for image-based robot learning. _Robotics: Science and Systems XIV_, 2018.

* [69] S. Qiu, Z. Dai, H. Zhong, Z. Wang, Z. Yang, and T. Zhang. Posterior sampling for competitive rl: Function approximation and partial observation. _Advances in Neural Information Processing Systems_, 36, 2024.
* [70] T. Rashid, M. Samvelyan, C. S. De Witt, G. Farquhar, J. Foerster, and S. Whiteson. QMIX: Monotonic value function factorisation for deep multi-agent reinforcement learning. In _International Conference on Machine learning_, pages 681-689, 2018.
* [71] T. Roughgarden. Algorithmic game theory. _Communications of the ACM_, 53(7):78-86, 2010.
* [72] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.
* [73] S. Shalev-Shwartz, S. Shammah, and A. Shashua. Safe, multi-agent, reinforcement learning for autonomous driving. _arXiv preprint arXiv:1610.03295_, 2016.
* [74] L. Shani, Y. Efroni, A. Rosenberg, and S. Mannor. Optimistic policy optimization with bandit feedback. In _International Conference on Machine Learning_, pages 8604-8613. PMLR, 2020.
* [75] I. Shenfeld, Z.-W. Hong, A. Tamar, and P. Agrawal. Tgrl: An algorithm for teacher guided reinforcement learning. In _International Conference on Machine Learning_, pages 31077-31093. PMLR, 2023.
* [76] M. Shi, Y. Liang, and N. Shroff. Theoretical hardness and tractability of pomdps in rl with partial online state information, 2024.
* [77] S. M. Shortreed, E. Laber, D. J. Lizotte, T. S. Stroup, J. Pineau, and S. A. Murphy. Informing sequential clinical decision-making through reinforcement learning: an empirical study. _Machine learning_, 84:109-136, 2011.
* [78] Z. Song, S. Mei, and Y. Bai. When can we learn general-sum Markov games with a large number of players sample-efficiently? _arXiv preprint arXiv:2110.04184_, 2021.
* [79] J. Subramanian, A. Sinha, R. Seraj, and A. Mahajan. Approximate information state for approximate planning and reinforcement learning in partially observed systems. _J. Mach. Learn. Res._, 23:12-1, 2022.
* [80] J. Tsitsiklis and M. Athans. On the complexity of decentralized decision making and detection problems. _IEEE Transactions on Automatic Control_, 30(5):440-446, 1985.
* [81] M. Uehara, A. Sekhari, J. D. Lee, N. Kallus, and W. Sun. Computationally efficient pac rl in pomdps with latent determinism and conditional embeddings. In _International Conference on Machine Learning_, pages 34615-34641. PMLR, 2023.
* [82] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev, et al. Grandmaster level in StarCraft II using multi-agent reinforcement learning. _Nature_, 575(7782):350-354, 2019.
* [83] A. Wang, A. C. Li, T. Q. Klassen, R. T. Icarte, and S. A. McIlraith. Learning belief representations for partially observable deep rl. In _International Conference on Machine Learning_, pages 35970-35988. PMLR, 2023.
* [84] L. Wang, Q. Cai, Z. Yang, and Z. Wang. Represent to control partially observed systems: Representation learning with provable sample efficiency. In _The Eleventh International Conference on Learning Representations_, 2022.
* [85] H. S. Witsenhausen. A counterexample in stochastic optimum control. _SIAM Journal on Control_, 6(1):131-147, 1968.
* [86] G. Yang, M. Liu, W. Hong, W. Zhang, F. Fang, G. Zeng, and Y. Lin. Perfectdou: Dominatingoudizhu with perfect information distillation. _Advances in Neural Information Processing Systems_, 35:34954-34965, 2022.

* [87] Y. Yang, Y. Jiang, J. Chen, S. E. Li, Z. Gu, Y. Yin, Q. Zhang, and K. Yu. Belief state actor-critic algorithm from separation principle for POMDP. In _2023 American Control Conference (ACC)_, pages 2560-2567. IEEE, 2023.
* [88] S. Young, M. Gasic, B. Thomson, and J. D. Williams. Pomdp-based statistical spoken dialog systems: A review. _Proceedings of the IEEE_, 101(5):1160-1179, 2013.
* [89] A. Zanette and E. Brunskill. Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds. In _International Conference on Machine Learning_, pages 7304-7312. PMLR, 2019.
* [90] W. Zhan, M. Uehara, W. Sun, and J. D. Lee. PAC reinforcement learning for predictive state representations. In _The Eleventh International Conference on Learning Representations_, 2023.

**Supplementary Materials for**

**"Provable Partially Observable Reinforcement Learning with Privileged Information"**

###### Contents

* 1 Introduction
* 2 Preliminaries
	* 2.1 Partially Observable RL (with Privileged Information)
	* 2.2 Partially Observable Multi-agent RL with Information Sharing
	* 2.3 Technical Assumptions for Computational Tractability
* 3 Revisiting Empirical Paradigms of RL with Privileged Information
	* 3.1 Privileged Policy Learning: Expert Policy Distillation
	* 3.2 Privileged Value Learning: Asymmetric Actor-Critic
* 4 Provably Efficient Expert Policy Distillation
* 5 Provable Asymmetric Actor-Critic with Approximate Belief Learning
	* 5.1 Belief-Weighted Optimistic Asymmetric Actor-Critic
	* 5.2 Approximate Belief Learning
* 6 Numerical Validation
* 7 Extension to Partially Observable MARL with Privileged Information
	* 7.1 Privileged Policy Learning: Equilibrium Distillation
	* 7.2 Privileged Value Learning: Asymmetric MARL with Approximate Belief Learning
* A Societal Impact
* B Related Work
* C Additional Preliminaries
* C.1 Additional Preliminaries on POMDPs
* C.2 Additional Preliminaries for POSGs
* C.2.1 Evolution of the Common and Private Information
* C.3 Strategy Independence of Belief and Examples
* D Collection of Algorithms
* E Missing Details in Section 3
* F Missing Details in Section 4* G Provably Efficient Expert Policy Distillation with Function Approximation
* H Missing Details in Section 5 H.1 Supporting Technical Lemmas
* I Missing Details in Section 6
* J Missing Details in Section 7  J.1 Background on Bayesian Games
* K Concluding Remarks and Limitations
Societal Impact

Our work is theoretical by nature, and aimed at better understanding reinforcement learning under partial observability with privileged information. As such, we do not anticipate any direct positive or negative societal impact from this research.

## Appendix B Related Work

Provable partial observable RL.While POMDPs are generally known to be both statistically hard [43] and computationally intractable [66], a productive line of research has identified several structured subclasses of POMDPs that can be efficiently solved. [43] introduced the class of POMDPs in the rich-observation setting, where the observation space can be large and fully reveal the underlying state, where sample-efficient RL becomes possible [35, 60]. [19] introduced \(k\)-step decodable POMDPs, where the last \(k\) observation-action pairs can uniquely determine the state, and proposed polynomial-sample complexity algorithms (assuming \(k\) is a small constant). Beyond settings where the underlying state can be _exactly_ recovered, [36, 47] proposed weakly revealing POMDPs, where the observations are assumed to be informative enough. Under the weakly revealing condition (and its variant), there has been a fast-growing line of recent works developing sample-efficient RL algorithms for various settings, see e.g., [84, 15, 12, 54, 48, 90]. Notably, these algorithms are typically computationally inefficient, requiring access to an optimistic planning oracle for POMDPs. On a promising note, [26] showed that in observable POMDPs (see Assumption 2.2), one can have quasi-polynomial-time algorithms for planning the near-optimal policy, which further leads to provable RL algorithms [25, 27] with _both quasi-polynomial_ samples and time.

RL under hindsight observability.The closest line of research to ours are the recent theoretical studies for Hindsight Observable Markov Decision Processes (HOMDPs) [44], where the underlying state is revealed at the end of the episode; see also subsequent related works in [30, 76] with different observation feedback models. These works focused purely on _sample efficiency_, and showed that polynomial sample complexity can be achieved without (or by further relaxing) aforementioned structural assumptions of the model (e.g., observability or decodability), in both tabular and/or function approximation settings. However, the algorithms (also) require an oracle for planning or even optimistic planning in a learned approximate POMDP, which are not computationally tractable in general. Indeed, without any structural assumption, learning the optimal policy in HOMDPs is computationally no easier than the planning problem, which thus remains PSPACE-hard. [66]. Meanwhile, even under the additional assumption of observability on the _underlying_ POMDP model, it is still not clear if these algorithms can avoid computationally intractable oracles, since the approximate POMDP that [44] needs to do planning on at every iteration during learning can be quite different from the underlying model. For example, at the beginning of exploration when not enough samples are collected, or when there exist certain states that remain less explored during the entire learning process, the potentially _misspecified emission (and transition)_ may break the observability (or other structural) assumptions made for the underlying POMDP. This makes that single iteration even computationally intractable. In contrast, our focus is on better understanding practically inspired algorithmic paradigms, without computationally intractable oracles, which in practice often do have and use the privileged state information _during_ each episode (instead of only at the end) [68, 45, 14].

Most related empirical works.Privileged information has been widely used in empirical partially observable RL, with two main types of approaches based on privileged _policy_ and privileged _value_ learning, respectively. For the former, one prominent example is expert distillation [14, 64, 58], also known as _teacher-student_ learning [45, 59, 75], as we analyze in Section 4. For the latter, asymmetric actor-critic [68] represents one of the well-known examples, with other studies in [7, 3]. Learning privileged value functions (to improve the policies) has also been widely used in multi-agent RL, featured in centralized-training-decentralized-execution, see e.g., [53, 22, 70, 86, 82]. Intriguingly, it was shown that if the privileged value function depends _only on_ the state, the associated actor will cause bias [6, 55, 56]. This has thus necessitated the use of history/belief in asymmetric actor-critic, as in our Section 5. Notably, the empirical framework in [83] exactly matches ours, where they exploited the privileged state information in training for _belief learning_, followed by policy optimization over the learned belief states. Indeed, many empirical works explicitly separate the procedures of explicit belief-state learning and planning [24, 65, 33, 16, 87] as we study in Section 5, oftentimes with privileged state information to supervise the belief learning procedure [61, 5].

## Appendix C Additional Preliminaries

### Additional Preliminaries on POMDPs

Belief and approximate belief.Although in a POMDP, the agent cannot see the underlying state directly, it can still form a _belief_ over the underlying state by the historical observations and actions.

**Definition C.1** (Belief state update).For each \(h\in[H+1]\), the Bayes operator \(B_{h}:\Delta(\mathcal{S})\times\mathcal{O}\rightarrow\Delta(\mathcal{S})\) is defined for \(b\in\Delta(\mathcal{S})\), and \(o\in\mathcal{O}\) by:

\[B_{h}(b;o)(x)=\frac{\mathbb{O}_{h}(o\,|\,x)b(x)}{\sum_{z\in\mathcal{S}} \mathbb{O}_{h}(o\,|\,z)b(z)},\]

for each \(x\in\mathcal{S}\). For each \(h\in[H+1]\), the belief update operator \(U_{h}:\Delta(\mathcal{S})\times\mathcal{A}\times\mathcal{O}\rightarrow\Delta (\mathcal{S})\), is defined by

\[U_{h}(b;a,o)=B_{h+1}\left(\mathbb{T}_{h}(a)\cdot b;o\right),\]

where \(\mathbb{T}_{h}(a)\cdot b\) represents the matrix multiplication. We use the notation \(b_{h}\) to denote the belief update function, which receives a sequence of actions and observations and outputs a distribution over states at the step \(h\): the belief state at step \(h=1\) is defined as \(\bm{b}_{1}(\emptyset)=\mu_{1}\). For any \(2\leq h\leq H\) and any action-observation sequence \((a_{1:h},a_{1:h})\), we inductively define the belief state:

\[\bm{b}_{h+1}(a_{1:h},o_{1:h}) =\mathbb{T}_{h}(a_{h})\cdot\bm{b}_{h}(a_{1:h-1},o_{1:h}),\] \[\bm{b}_{h}(a_{1:h-1},o_{1:h}) =B_{h}(\bm{b}_{h}(a_{1:h-1},o_{1:h-1});o_{h}).\]

We also define the approximate belief update using the most recent \(L\)-step history. For \(1\leq h\leq H\), we follow the notation of [26] and define

\[\bm{b}_{h}^{\text{apx},\mathcal{P}}(\emptyset;D)=\begin{cases}\mu_{1}&\text{ if }h=1\\ D&\text{ otherwise },\end{cases}\]

where \(D\in\Delta(\mathcal{S})\) is the prior for the approximate belief update. Then for any \(1\leq h-L<h\leq H\) and any action-observation sequence \((a_{h-L:h},o_{h-L+1:h})\), we inductively define

\[\bm{b}_{h+1}^{\text{apx},\mathcal{P}}(a_{h-L:h},o_{h-L+1:h};D) =\mathbb{T}_{h}(a_{h})\cdot\bm{b}_{h}^{\text{apx},\mathcal{P}}(a_ {h-L:h-1},o_{h-L+1:h};D),\] \[\bm{b}_{h}^{\text{apx},\mathcal{P}}(a_{h-L:h-1},o_{h-L+1:h};D) =B_{h}(\bm{b}_{h}^{\text{apx},\mathcal{P}}(a_{h-L:h-1},o_{h-L+1:h- 1};D);o_{h}).\]

For the remainder of our paper, we will use an important initialization for the approximate belief, which are defined as \(\bm{b}_{h}^{\prime}(\cdot):=\bm{b}_{h}^{\text{apx},\mathcal{P}}(\cdot;\text{ Unif}(\mathcal{S}))\).

### Additional Preliminaries for POSGs

Model.We use a general framework of partially observable stochastic games (POSGs) as the model for partially observable MARL. Formally, we define a POSG with \(n\) agents by a tuple \(\mathcal{G}=(H,\mathcal{S},\{\mathcal{A}_{i}\}_{i=1}^{n},\{\mathcal{O}_{i}\}_ {i=1}^{n},\mathbb{T},\mathbb{O},\mu_{1},\{r_{i}\}_{i=1}^{n})\), where \(H\) denotes the length of each episode, \(\mathcal{S}\) is the state space with \(|\mathcal{S}|=S\), \(\mathcal{A}_{i}\) denotes the action space for agent \(i\) with \(|\mathcal{A}_{i}|=A_{i}\). We denote by \(a:=(a_{1},\cdots,a_{n})\) the joint action of all the \(n\) agents, and by \(\mathcal{A}=\mathcal{A}_{1}\times\cdots\times\mathcal{A}_{n}\) the joint action space with \(|\mathcal{A}|=A=\prod_{i=1}^{n}A_{i}\). We use \(\mathbb{T}=\{\mathbb{T}_{h}\}_{h\in[H]}\) to denote the collection of transition matrices, so that \(\mathbb{T}_{h}(\cdot\,|\,s,a)\in\Delta(\mathcal{S})\) gives the probability of the next state if joint action \(a\in\mathcal{A}\) is taken at state \(s\in\mathcal{S}\) and step \(h\). In the following discussions, for any given \(a\), we treat \(\mathbb{T}_{h}(a)\in\mathbb{R}^{|\mathcal{S}|\times|\mathcal{S}|}\) as a matrix, where each row gives the probability for the next state from different current states. We use \(\mu_{1}\) to denote the distribution of the initial state \(s_{1}\), and \(\mathcal{O}_{i}\) to denote the observation space for agent \(i\) with \(|\mathcal{O}_{i}|=O_{i}\). We denote by \(o:=(o_{1},\ldots,o_{n})\) the joint observation of all \(n\) agents, and by \(\mathcal{O}:=\mathcal{O}_{1}\times\cdots\times\mathcal{O}_{n}\) with \(|\mathcal{O}|=O=\prod_{i=1}^{n}O_{i}\). We use \(\mathbb{O}=\{\mathbb{O}_{h}\}_{h\in[H]}\) to denote the collection of the joint emission matrices, so that \(\mathbb{O}_{h}(\cdot\,|\,s)\in\Delta(\mathcal{O})\) gives the emission distribution over the joint observation space \(\mathcal{O}\) at state \(s\) and step \(h\). For notational convenience, we will at times adopt the matrix convention, where \(\mathbb{O}_{h}\) is a matrix with rows \(\mathbb{O}_{h}(\cdot\,|\,s_{h})\). We also denote \(\mathbb{O}_{i,h}(\cdot\,|\,s)\in\Delta(\mathcal{O}_{i})\) as the marginalized emission for agent \(i\) agent. Finally, \(r_{i}=\{r_{i,h}\}_{h\in[H]}\) is a collection of reward functions, so that \(r_{i,h}(s_{h},a_{h})\) is the reward of agent \(i\) agent given the state \(s_{h}\) and joint action \(a_{h}\) at step \(h\).

Similar to a POMDP, in a POSG, the states are not observable to the agents, and each agent can only access its own individual observations. The game proceeds as follows. At the beginning of each episode, the environment samples \(s_{1}\) from \(\mu_{1}\). At each step \(h\), each agent \(i\) observes its own observation \(o_{i,h}\), where \(o_{h}:=(o_{1,h},\ldots,o_{n,h})\) is sampled jointly from \(\mathbb{O}_{h}(\cdot\,|\,s_{h})\). Then each agent \(i\) takes the action \(a_{i,h}\) and receives the reward \(r_{i,h}(s_{h},a_{h})\). After that the environment transitions to the next state \(s_{h+1}\sim\mathbb{T}_{h}(\cdot\,|\,s_{h},a_{h})\). The current episode terminates once \(s_{H+1}\) is reached.

Information sharing, common and private information.Each agent \(i\) in the POSG maintains its own information, \(\tau_{i,h}\), a collection of historical observations and actions at step \(h\), namely, \(\tau_{i,h}\subseteq\{o_{1},a_{1},o_{2},\cdots,a_{h-1},o_{h}\}\), and the collection of such history at step \(h\) is denoted by \(\mathcal{T}_{i,h}\).

In many practical examples, agents may share part of the history with each other, which may introduce some _information structures_ of the game that may lead to both sample and computation efficiencies. The information sharing splits the history into _common/shared_ and _private_ information for each agent. The _common information_ at step \(h\) is a subset of the joint history \(\tau_{h}=(\tau_{i,h})_{i\in[n]}\): \(c_{h}\subseteq\{o_{1},a_{1},o_{2},\cdots,a_{h-1},o_{h}\}\), which is available to _all the agents_ in the system, and the collection of the common information is denoted as \(\mathcal{C}_{h}\) and we define \(C_{h}=|\mathcal{C}_{h}|\). Given the common information \(c_{h}\), each agent also has her private information \(p_{i,h}=\tau_{i,h}\setminus c_{h}\), where the collection of the private information for agent \(i\) is denoted as \(\mathcal{P}_{i,h}\) and its cardinality as \(P_{i,h}\). The cardinality of the joint private information is \(P_{h}=\prod_{i=1}^{n}P_{i,h}\). We allow \(c_{h}\) or \(p_{i,h}\) to take the special value \(\emptyset\) when there is _no_ common or private information. In particular, when \(\mathcal{C}_{h}=\{\emptyset\}\), the problem reduces to the general POSG without any favorable information structure; when \(\mathcal{P}_{i,h}=\{\emptyset\}\), every agent holds the same history, and it reduces to a POMDP when the agents share a common reward function, where the goal is usually to find the team-optimal solution.

Policies and value functions.We define a stochastic policy for agent \(i\) at step \(h\) as:

\[\pi_{i,h}:\Omega_{h}\times\mathcal{P}_{i,h}\times\mathcal{C}_{h}\to\Delta( \mathcal{A}_{i}),\] (C.1)

where \(\Omega_{h}\) is the random seed space, which is shared among agents and \(\omega_{i,h}\in\Omega_{h}\) is the random seed for agent \(i\). The corresponding policy class is denoted as \(\Pi_{i,h}\). Hereafter, unless otherwise noted, when referring to _policies_, we mean the policies given in the form of (C.1), which maps the available information of agent \(i\), i.e., the private information together with the common information, to the distribution over her actions.

We define \(\pi_{i}\) as a sequence of policies for agent \(i\) at all steps \(h\in[H]\), i.e., \(\pi_{i}=(\pi_{i,1},\cdots,\pi_{i,H})\).

We further denote \(\Pi_{i}=\times_{h\in[H]}\Pi_{i,h}\) as the policy space for agent \(i\) and \(\Pi=\prod_{i\in[n]}\Pi_{i}\) as the joint policy space. As a special case, we define the space of _deterministic_ policy as \(\widetilde{\Pi}_{i}\), where \(\widetilde{\pi}_{i}\in\widetilde{\Pi}_{i}\) maps the private information and common information to a _deterministic_ action for agent \(i\) and the joint space as \(\widetilde{\Pi}=\prod_{i\in[n]}\widetilde{\Pi}_{i}\).

A _product_ policy is denoted as \(\pi=\pi_{1}\times\pi_{2}\cdots\times\pi_{n}\in\Pi\) if the distributions of drawing each seed \(\omega_{i,h}\) for different agents are independent, and a (potentially correlated) joint policy is denoted as \(\pi=\pi_{1}\odot\pi_{2}\cdots\odot\pi_{n}\in\Pi\).

We are now ready to define the _value function_ conditioned on the common information under our model of POSG with information sharing:

**Definition C.2** (Value function with information sharing).: For each agent \(i\in[n]\) and step \(h\in[H]\), given common information \(c_{h}\) and joint policy \(\pi=(\pi_{i})_{i=1}^{n}\in\Pi\), the _value function conditioned on the common information_ of agent \(i\) is defined as: \(V_{i,h}^{\pi,\mathcal{G}}(c_{h}):=\mathbb{E}_{\pi}^{\mathcal{G}}\left[\sum_{h^{ \prime}=h}^{H}r_{i,h^{\prime}}(s_{h^{\prime}},a_{h^{\prime}})\,|\,c_{h}\right]\), where the expectation is taken over the randomness from the model \(\mathcal{G}\), policy \(\pi\), and the random seeds. For any \(c_{H+1}\in\mathcal{C}_{H+1}:V_{i,H+1}^{\pi,\mathcal{G}}(c_{H+1}):=0\). From now on, we will refer to it as _value function_ for short.

Another key concept in our analysis is the belief about the state _and_ the private information conditioned on the common information among agents. Formally, at step \(h\), given policies from \(1\) to \(h-1\)we consider the common-information-based conditional belief \(\mathbb{P}_{h}^{\pi_{i:h-1},\mathcal{G}}\left(s_{h},p_{h}\,|\,c_{h}\right)\). This belief not only infers the current underlying state \(s_{h}\), but also all agents' private information \(p_{h}\). With the common-information-based conditional belief, the value function given in Definition C.2 has the following recursive structure:

\[V_{i,h}^{\pi,\mathcal{G}}(c_{h})=\mathbb{E}_{\pi}^{\mathcal{G}}[r_{i,h}(s_{h}, a_{h})+V_{i,h+1}^{\pi,\mathcal{G}}(c_{h+1})\,|\,c_{h}],\] (C.2)

where the expectation is taken over the randomness of \((s_{h},p_{h},a_{h},o_{h+1})\). With this relationship, we can define the _prescription-value_ function correspondingly, a generalization of the _action-value_ function in (fully observable) stochastic games and MDPs to POSGs with information sharing, as follows.

**Definition C.3** (Prescription-value function with information sharing).: At step \(h\), given the common information \(c_{h}\), joint policies \(\pi=(\pi_{i})_{i=1}^{n}\in\Pi\), and prescriptions \((\gamma_{i,h})_{i=1}^{n}\in\Gamma_{h}\), the _prescription-value function conditioned on the common information and joint prescription_ of agent \(i\) is defined as:

\[Q_{i,h}^{\pi,\mathcal{G}}(c_{h},(\gamma_{j,h})_{j\in[n]}):=\mathbb{E}_{\pi}^ {\mathcal{G}}\big{[}r_{i,h}(s_{h},a_{h})+V_{i,h+1}^{\pi,\mathcal{G}}(c_{h+1}) \,\big{|}\,c_{h},(\gamma_{j,h})_{j\in[n]}\big{]},\]

where prescription \(\gamma_{i,h}\in\Delta(\mathcal{A}_{i})^{P_{i,h}}\) replaces the partial function \(\pi_{i,h}(\cdot\,|\,\omega_{i,h},c_{h},\cdot\,)\) in the value function. From now on, we will refer to it as _prescription-value function_ for short. With such a prescription-value function, agents can take actions purely based on their local private information [63, 62, 51].

This prescription-value function indicates the expected return for agent \(i\) when all the agents firstly adopt the prescriptions \((\gamma_{j,h})_{j\in[n]}\) and then follow the policy \(\pi\).

Equilibrium notions.With the definition of value functions, we can accordingly define the solution concepts. Here we define the notions of \(\epsilon\)-NE, \(\epsilon\)-CCE, \(\epsilon\)-CE, and \(\epsilon\)-team optimum under the information-sharing framework as follows. For a joint policy \((\pi_{i})_{i=1}^{n}\in\Pi\) we denote the expected reward of agent \(i\) by \(v_{i}^{\mathcal{G}}(\pi)=\mathbb{E}_{\pi}^{\mathcal{P}}[\sum_{h=1}^{H}r_{i,h} (s_{h},a_{h})]\).

**Definition C.4** (\(\epsilon\)-approximate Nash equilibrium with information sharing).: For any \(\epsilon\geq 0\), a product policy \(\pi^{\star}\in\Pi\) is an \(\epsilon\)-approximate Nash equilibrium of the POSG \(\mathcal{G}\) with information sharing if:

\[\mathrm{NE\text{-}gap}(\pi^{\star}):=\max_{i}\left(\max_{\pi_{i}^{\prime}\in \Pi_{i}}v_{i}^{\mathcal{G}}(\pi_{i}^{\prime}\times\pi_{-i}^{\star})-v_{i}^{ \mathcal{G}}(\pi^{\star})\right)\leq\epsilon.\]

**Definition C.5** (\(\epsilon\)-approximate coarse correlated equilibrium with information sharing).: For any \(\epsilon\geq 0\), a joint policy \(\pi^{\star}\in\Pi\) is an \(\epsilon\)-approximate coarse correlated equilibrium of the POSG \(\mathcal{G}\) with information sharing if:

\[\mathrm{CCE\text{-}gap}(\pi^{\star}):=\max_{i}\left(\max_{\pi_{i}^{\prime}\in \Pi_{i}}v_{i}^{\mathcal{G}}(\pi_{i}^{\prime}\times\pi_{-i}^{\star})-v_{i}^{ \mathcal{G}}(\pi^{\star})\right)\leq\epsilon.\]

**Definition C.6** (\(\epsilon\)-approximate correlated equilibrium with information sharing).: For any \(\epsilon\geq 0\), a joint policy \(\pi^{\star}\in\Pi\) is an \(\epsilon\)-approximate correlated equilibrium of the POSG \(\mathcal{G}\) with information sharing if:

\[\mathrm{CE\text{-}gap}(\pi^{\star}):=\max_{i}\left(\max_{\phi_{i}}v_{i}^{ \mathcal{G}}((m_{i}\circ\pi_{i}^{\star})\odot\pi_{-i}^{\star})-v_{i}^{ \mathcal{G}}(\pi^{\star})\right)\leq\epsilon,\]

where \(m_{i}\) is called _strategy modification_ for agent \(i\), and \(m_{i}=\{m_{i,h,c_{h},p_{i,h}}\}_{h,c_{h},p_{i,h}}\), with each \(m_{i,h,c_{h},p_{i,h}}:\mathcal{A}_{i}\rightarrow\mathcal{A}_{i}\) being a mapping from the action set to itself. The space of \(m_{i}\) is denoted as \(\mathcal{M}_{i}\). The composition \(m_{i}\circ\pi_{i}\) is defined as follows: at step \(h\), when agent \(i\) is given \(c_{h}\) and \(p_{i,h}\), the joint action chosen to be \((a_{1,h},\cdots,a_{i,h},\cdots,a_{n,h})\) will be modified to \((a_{1,h},\cdots,m_{i,h,c_{h},p_{i,h}}(a_{i,h}),\cdots,a_{n,h})\). Note that this definition follows from that in [78, 50, 38, 49, 51] when there exists certain common information, and is a natural generalization of the definition in the normal-form game case [71]. We denote by \(\mathcal{M}_{i}^{\text{gen}}\) the space of all possible strategy modifications \(m_{i}\) if it conditions on any history information instead of only \((c_{h},p_{i,h})\). Similarly, we use \(\mathcal{M}_{\mathcal{S},i}\) to denote the space of all possible strategy modifications \(m_{i}\) if it only conditions on the current state e.g., a modification \(m_{i,h,s}:\mathcal{A}_{i}\rightarrow\mathcal{A}_{i}\).

#### c.2.1 Evolution of the Common and Private Information

**Assumption C.7** (Evolution of common and private information).: We assume that common information and private information evolve over time as follows:

* Common information \(c_{h}\) is non-decreasing over time, that is, \(c_{h}\subseteq c_{h+1}\) for all \(h\). Let \(\varpi_{h+1}=c_{h+1}\setminus c_{h}\). Thus, \(c_{h+1}=\{c_{h},\varpi_{h+1}\}\). Further, we have \[\varpi_{h+1}=\chi_{h+1}(p_{h},a_{h},o_{h+1}),\] (C.3) where \(\chi_{h+1}\) is a fixed transformation. We use \(\Upsilon_{h+1}\) to denote the collection of \(\varpi_{h+1}\) at step \(h\).
* Private information evolves according to: \[p_{i,h+1}=\xi_{i,h+1}(p_{i,h},a_{i,h},o_{i,h+1}),\] (C.4) where \(\xi_{i,h+1}\) is a fixed transformation.

Equation (C.3) states that the increment in the common information depends on the "new" information \((a_{h},o_{h+1})\) generated between steps \(h\) and \(h+1\) and part of the old information \(p_{h}\). The incremental common information can be obtained by certain sharing and communication protocols among the agents. Equation (C.4) implies that the evolution of private information only depends on the newly generated private information \(a_{i,h}\) and \(o_{i,h+1}\). These evolution rules are standard in the literature [62; 63], specifying the source of common information and private information. Based on such evolution rules, we define \(\{f_{h}\}_{h\in[H]}\) and \(\{g_{h}\}_{h\in[H]}\), where \(f_{h}:\mathcal{A}^{h}\times\mathcal{O}^{h}\to\mathcal{C}_{h}\) and \(g_{h}:\mathcal{A}^{h}\times\mathcal{O}^{h}\to\mathcal{P}_{h}\) for \(h\in[H]\), as the mappings that map the joint history to common information and joint private information, respectively.

### Strategy Independence of Belief and Examples

To solve a POSG without computationally intractable oracles, certain information-sharing is needed even under the observability assumption [51]. We thus make the following assumption as in [51].

**Assumption C.8** (Strategy independence of beliefs [62; 31; 51]).: Consider any step \(h\in[H]\), any policy \(\pi\in\Pi\), and any realization of common information \(c_{h}\) that has a non-zero probability under the trajectories generated by \(\pi_{1:h-1}\). Consider any other policies \(\pi^{\prime}_{1:h-1}\), which also give a non-zero probability to \(c_{h}\). Then, we assume that: for any such \(c_{h}\in\mathcal{C}_{h}\), and any \(p_{h}\in\mathcal{P}_{h},s_{h}\in\mathcal{S}\), \(\mathbb{P}_{h}^{\pi_{1:h-1},\mathcal{G}}\left(s_{h},p_{h}\,|\,c_{h}\right)= \mathbb{P}_{h}^{\pi_{1:h-1},\mathcal{G}}\left(s_{h},p_{h}\,|\,c_{h}\right).\)

We provide examples satisfying this assumption in Appendix C.2, which include the fully-sharing structure as in [69; 27] as a special case. Finally, we also assume that common information and private information evolve over time properly in Assumption C.7, as standard in [62; 63; 51], which covers the models considered in [27; 69; 49].

Here we take the examples from [52; 51] to illustrate the generality of the information-sharing framework.

**Example C.9** (One-step delayed sharing).: At any step \(h\in[H]\), the common and private information are given as \(c_{h}=\{o_{2:h-1},a_{1:h-1}\}\) and \(p_{i,h}=\{o_{i,h}\}\), respectively. In other words, the players share all the action-observation history until the previous step \(h-1\), with only the new observation being the private information. This model has been shown useful for power control [2].

**Example C.10** (State controlled by one controller with asymmetric delay sharing).: We assume there are \(2\) players for convenience. It extends naturally to \(n\)-player settings. Consider the case where the state dynamics are controlled by player \(1\), i.e., \(\mathbb{T}_{h}(\cdot\,|\,s_{h},a_{1,h},a_{2,h})=\mathbb{T}_{h}(\cdot\,|\,s_{h}, a_{1,h},a_{2,h}^{\prime})\) for all \((s_{h},a_{1,h},a_{2,h},a_{2,h}^{\prime},h)\). There are two kinds of delay-sharing structures we could consider: **Case A:** the information structure is given as \(c_{h}=\{o_{1,2:h},o_{2,2:h-d},a_{1,1:h-1}\}\), \(p_{1,h}=\emptyset\), \(p_{2,h}=\{o_{2,h-d+1:h}\}\), i.e., player \(1\)'s observations are available to player \(2\) instantly, while player \(2\)'s observations are available to player \(1\) with a delay of \(d\geq 1\) time steps. **Case B:** similar to **Case A** but player \(1\)'s observation is available to player \(2\) with a delay of \(1\) step. The information structure is given as \(c_{h}=\{o_{1,2:h-1},o_{2,2:h-d},a_{1,1:h-1}\}\), \(p_{1,h}=\{o_{1,h}\}\), \(p_{2,h}=\{o_{2,h-d+1:h}\}\), where \(d\geq 1\). This kind of asymmetric sharing is common in network routing [67], where packages arrive at different hosts with different delays, leading to asymmetric delay sharing among hosts.

**Example C.11** (Symmetric information game).: Consider the case when all observations and actions are available for all the agents, and there is no private information. Essentially, we have \(c_{h}=\{o_{2:h},a_{1:h-1}\}\) and \(p_{i,h}=\emptyset\). We will also denote this structure as _fully sharing_ hereafter.

**Example C.12** (Information sharing with one-directional-one-step delay).: Similar to the previous cases, we also assume there are \(2\) players for ease of exposition, and the case can be generalized to multi-player cases straightforwardly. Similar to the one-step delay case, we consider the situation where all observations of player \(1\) are available to player \(2\), while the observations of player \(2\) are available to player \(1\) with one-step delay. All the past actions are available to both players. That is, in this case, \(c_{h}=\{o_{1,2:h},o_{2,2:h-1},a_{1:h-1}\}\), and player \(1\) has no private information, i.e., \(p_{1,h}=\emptyset\), and player \(2\) has private information \(p_{2,h}=\{o_{2,h}\}\).

**Example C.13** (Uncontrolled state process).: Consider the case where the state transition does not depend on the actions, that is, \(\mathbb{T}_{h}(\cdot\mid s_{h},a_{h})=\mathbb{T}_{h}(\cdot\mid s_{h},a_{h}^{ \prime})\) for any \(s_{h},a_{h},a_{h}^{\prime},h\). Note that the agents are still coupled through the joint reward. An example of this case is the information structure where controllers share their observations with a delay of \(d\geq 1\) time steps. In this case, the common information is \(c_{h}=\{o_{2:h-d}\}\) and the private information is \(p_{i,h}=\{o_{i,h-d+1:h}\}\). Such information structures can be used to model repeated games with incomplete information [4].

## Appendix D Collection of Algorithms

```
0:
* POMDP \(\mathcal{P}\),
* Expert policy \(\pi^{E}\in\Pi_{\mathcal{S}}\),
* Number of sampled episodes per step \(M\).

**Ensure:**: A decoding function for each step \(\{g_{h}\}_{h\in[H]}\) (see Theorem 4.6)

For the \(h=1\) step: Collect \(M\) state-observation pairs from the first-step \(\widehat{D}_{1}=\left\{\left(s_{1}^{(i)},o_{1}^{(i)}\right)\right\}_{i\in[M]}\) on POMDP \(\mathcal{P}\) and define the decoding function \(g_{1}\) for the first step as:

\[g_{1}(o_{1})=\left\{s_{1}:(s_{1},o_{1})\in\widehat{D}_{1}\right\}\]

for each step \(h\in[2,H]\)do  Collect \(M\) episodes \(\left\{\left(s_{1:H+1}^{(i)},o_{1:H}^{(i)},a_{1:H}^{(i)}\right)\right\}_{i\in[M]}\) on POMDP \(\mathcal{P}\) using policy \(\pi^{E}\) and let:

\[\widehat{D}_{h}:=\left\{\left(s_{h-1}^{(i)},a_{h-1}^{(i)},o_{h}^{(i)},s_{h}^{(i )}\right)\right\}_{i\in[M]}\]  Define the decoding function \(g_{h}\) for step \(h\) as:

\[g_{h}(s_{h-1},a_{h-1},o_{h})=\left\{s_{h}:(s_{h-1},a_{h-1},o_{h},s_{h})\in \widehat{D}_{h}\right\}\]

endfor return\(\{g_{h}\}_{h\in[H]}\) ```

**Algorithm 1** Learning Decoding Function with Privileged Information

**Require:**

* POMDP \(\mathcal{P}\), policy \(\pi_{1:H}\in\Pi^{L}\) such that \(\pi_{h}:\mathcal{S}\rightarrow\Delta(\mathcal{A})\),
* Number of episodes \(M\) per step.

**Ensure:** Approximate \(Q\)-functions \(\{\widetilde{Q}_{h}\}_{h\in[H]}\) (see Lemma H.3)

**Initialize:**: \(\forall z_{H+1}\in\mathcal{Z}_{H+1},s_{H+1}\in\mathcal{S},a_{H+1}\in\mathcal{A}\)

\[\widetilde{Q}_{H+1}(z_{H+1},s_{H+1},a_{H+1})\gets 0,\qquad\pi_{H+1}(a_{H+1} \,|\,z_{H+1})\leftarrow\frac{1}{A}\]

**for** step \(h=H,\ldots,1\)**do**

Collect \(M\) trajectories using policy \(\pi\) and let \(D_{h}=\{\overline{\tau}^{(i)}\}_{i\in[M]}\) be the collected trajectories.

Compute empirical counts and define empirical distributions:

\[\widehat{\mathbb{T}}_{h}(s_{h+1}\mid s_{h},a_{h}) =\frac{|\{\overline{\tau}\in D_{h}:(s^{\prime}_{h},a^{\prime}_{h},s^{\prime}_{h+1})=(s_{h},a_{h},s_{h+1})\}|}{|\{\overline{\tau}\in D_{h}:(s^{ \prime}_{h},a^{\prime}_{h})=(s_{h},a_{h})\}|}\] \[\widehat{\mathbb{O}}_{h}(o_{h}\mid s_{h}) =\frac{|\{\overline{\tau}\in D_{h}:(s^{\prime}_{h},o^{\prime}_{h} )=(s_{h},o_{h})\}|}{|\{\overline{\tau}\in D_{h}:s^{\prime}_{h}=s_{h}\}|}\]
**for** each memory-state pair \((z_{h},s_{h})\in\mathcal{Z}_{h}\times\mathcal{S}\)**do**

\[\widetilde{Q}(z_{h},s_{h},a_{h}) =\min\Bigg{(}H-h+1,\mathbb{E}_{\begin{subarray}{c}s_{h+1}\sim \widehat{\mathbb{T}}_{h}(\cdot\,|\,s_{h},a_{h}),\\ o_{h+1}\sim\widehat{\mathbb{O}}_{h+1}(\cdot\,|\,s_{h+1})\end{subarray}}[ \widetilde{V}_{h+1}(z_{h+1},s_{h+1})]\] \[\qquad+r(s_{h},a_{h})+H\cdot\min\Bigg{(}2,C\cdot\sqrt{\frac{S \log(1/\delta_{1})}{\max(N_{h}(s_{h},a_{h}),1)}}\Bigg{)}\] \[\qquad+\mathbb{E}_{s_{h+1}\sim\widehat{\mathbb{T}}_{h}(\cdot\,| \,s_{h},a_{h})}H\cdot\min\Bigg{(}2,C\cdot\sqrt{\frac{O\log(1/\delta_{1})}{ \max(N_{h+1}(s_{h+1}),1)}}\Bigg{)}\Bigg{)},\]

where \(\widetilde{V}_{h+1}(z_{h+1},s_{h+1})=\mathbb{E}_{a_{h+1}\sim\pi_{h+1}(\cdot\,| \,z_{h+1})}[\widetilde{Q}_{h+1}(z_{h+1},s_{h+1},a_{h+1})]\)
**end for**

**end for**

**return**\(\{\widetilde{Q}_{h}\}_{h\in[H]}\)```
0:
* POMDP \(\mathcal{P}=(H,\mathcal{S},\mathcal{A},\mathcal{O},\{\mathbb{T}_{h}\}_{h\in[H]},\{ \mathbb{O}_{h}\}_{h\in[H]},\mu_{1},\{r_{h}\}_{h\in[H]})\),
* An MDP learning oracle MDP_Learning that efficiently learns an approximate optimal policy of an MDP,
* Number of trajectories \(N\),
* The threshold \(\epsilon\).
0: Approximate belief \(\{\bm{b}_{h}^{\text{apx}}\}_{h\in[H]}\) (See Theorem H.5) for\(h\in[H],s_{h}\in\mathcal{S}\)do \(\widehat{r}_{h^{\prime}}(s_{h}^{\prime},a_{h}^{\prime})\leftarrow\mathbb{1}[h^ {\prime}=h,s_{h}^{\prime}=s_{h}]\) for any \((h^{\prime},s_{h}^{\prime},a_{h}^{\prime})\in[H]\times\mathcal{S}\times \mathcal{A}\) \(\mathcal{M}\leftarrow(H,\mathcal{S},\mathcal{A},\{\mathbb{T}_{h}\}_{h\in[H]}, \mu_{1},\{\widehat{r}_{h}\}_{h\in[H]})\) to be the MDP associated with \(\mathcal{P}\) \(\Psi(h,s_{h})\leftarrow\text{MDP\_Learning}(\mathcal{M})\)  Collect \(N\) trajectories by executing policy \(\Psi(h,s_{h})\) for the first \(h-1\) steps then take action \(a_{h}\) for each \(a_{h}\in\mathcal{A}\) deterministically and denote the dataset \(\{(s_{h}^{i},o_{h}^{i},a_{h}^{i},s_{h+1}^{i})\}_{i\in[NA]}\) for\((o_{h},a_{h},s_{h+1})\in\mathcal{O}\times\mathcal{A}\times\mathcal{S}\)do \(N_{h}(s_{h})\leftarrow\sum_{i\in[NA]}\mathbb{1}[s_{h}^{i}=s_{h}]\) \(N_{h}(s_{h},a_{h})\leftarrow\sum_{i\in[NA]}\mathbb{1}[s_{h}^{i}=s_{h},a_{h}^{ i}=a_{h}]\) \(N_{h}(s_{h},a_{h},s_{h+1})\leftarrow\sum_{i\in[NA]}\mathbb{1}[s_{h}^{i}=s_{h},a_{h}^{ i}=a_{h},s_{h+1}^{i}=s_{h+1}]\) \(N_{h}(s_{h},o_{h})\leftarrow\sum_{i\in[NA]}\mathbb{1}[s_{h}^{i}=s_{h},o_{h}^{ i}=o_{h}]\) \(\widehat{\mathbb{T}}_{h}(s_{h+1}\,|\,s_{h},a_{h})\leftarrow\frac{N_{h}(s_{h},a_{h},s_{h+1})}{N _{h}(s_{h},a_{h})}\) \(\widehat{\mathbb{O}}_{h}(o_{h}\,|\,s_{h})\leftarrow\frac{N_{h}(s_{h},o_{h})}{N _{h}(s_{h})}\) endfor endfor for\(h\in[H]\)do \(\mathcal{S}_{h}^{\text{low}}\leftarrow\left\{s_{h}\in\mathcal{S}\,|\,\frac{N_{h}( s_{h})}{NA}\leq\epsilon\right\}\) \(\mathcal{S}_{h}^{\text{high}}\leftarrow\left\{s_{h}\in\mathcal{S}\,|\,\frac{N_{ h}(s_{h})}{NA}>\epsilon\right\}\) endfor for\((h,s_{h},o_{h},a_{h},s_{h+1})\in[H]\times\mathcal{S}_{h}^{\text{high}}\times\mathcal{O} \times\mathcal{A}\times\mathcal{S}_{h}^{\text{high}}\)do \(\widehat{\mathbb{T}}_{h}^{\text{trunc}}(s_{h+1}\,|\,s_{h},a_{h})\leftarrow \widehat{\mathbb{T}}_{h}(s_{h+1}\,|\,s_{h},a_{h})+\frac{\sum_{s_{h+1}^{\prime} \in\mathcal{S}_{h+1}^{\text{low}}}\widehat{\mathbb{T}}_{h}(s_{h+1}^{\prime}\,| \,s_{h},a_{h})}{|\mathcal{S}_{h+1}^{\text{high}}|}\) \(\widehat{\mathbb{O}}_{h}^{\text{trunc}}(o_{h}\,|\,s_{h})\leftarrow\widehat{ \mathbb{O}}_{h}(o_{h}\,|\,s_{h})\) \(\widehat{\mu}_{1}^{\text{trunc}}(s_{1}):=\widehat{\mu}_{1}(s_{1})+\frac{\sum_{s _{1}^{\prime}\in\mathcal{S}^{\text{low}}}\widehat{\mu}_{1}(s_{1}^{\prime})}{| \mathcal{S}_{1}^{\text{high}}|},\forall s_{1}\in\mathcal{S}_{1}^{\text{high}}\) endfor  Let \(\widehat{\mathcal{P}}^{\text{sub}}:=(H,\{\mathcal{S}_{h}^{\text{high}}\}_{h\in[H] },\mathcal{A},\mathcal{O},\{\widehat{\mathbb{T}}_{h}^{\text{trunc}}\}_{h\in[H] },\{\widehat{\mathbb{O}}_{h}^{\text{trunc}}\}_{h\in[H]},\widehat{\mu}_{1}^{ \text{trunc}},\{r_{h}\}_{h\in[H]})\)  Define \(\{\widehat{\mathbb{O}}_{h}^{\text{sub}}:\mathcal{Z}_{h}\rightarrow\Delta( \mathcal{S}_{h}^{\text{high}})\}_{h\in[H]}\) to be the approximate belief w.r.t. \(\widehat{\mathcal{P}}^{\text{sub}}\)  Define \(\{\bm{b}_{h}^{\text{apx}}:\mathcal{Z}_{h}\rightarrow\Delta(\mathcal{S})\}_{h \in[H]}\) such that \(\bm{b}_{h}^{\text{apx}}(z_{h})(s_{h})=\widehat{\mathbb{O}}_{h}^{\prime,\text{ sub}}(z_{h})(s_{h})\) for \(s_{h}\in\mathcal{S}_{h}^{\text{high}}\) and \(0\) otherwise. return\(\{\bm{b}_{h}^{\text{apx}}\}_{h\in[H]}\) ```

**Algorithm 4** Approximate Belief Learning with Privileged Information via Model Truncation

**Algorithm 5** Learning Multi-Agent (Individual) Decoding Functions with Privileged Information (NE/CCE Version)

**Require:**

* POSG \(\mathcal{G}=(H,\mathcal{S},\mathcal{A},\mathcal{O},\{\mathbb{T}_{h}\}_{h\in[H]}, \{\mathbb{O}_{h}\}_{h\in[H]},\mu_{1},\{r_{i}\}_{i\in[n]})\),
* \(\pi\in\Pi_{\mathcal{S}}\), controller set \(\{\mathcal{I}_{h}\subseteq[n]\}_{h\in[H]}\),
* Procedure MDP_Learning\((\cdot,\cdot)\) that takes as input an MDP and a reward function and returns an approximate optimal policy,
* Number of trajectories \(N\).

**Ensure:**: A decoding function for each agent and step \(\{\widehat{g}_{j,h}\}_{j\in[n],h\in[H]}\) (see Theorem J.4)

**for**\(h\in[H]\), \(s_{h}\in\mathcal{S}\)**do**

**for**\(i\in[n]\)**do**

\(\widehat{r}_{i,h^{\prime}}(s^{\prime}_{h},a^{\prime}_{h})\leftarrow\mathbb{1} [h^{\prime}=h,s^{\prime}_{h}=s_{h}]\) for any \((h^{\prime},s^{\prime}_{h},a^{\prime}_{h})\in[H]\times\mathcal{S}\times \mathcal{A}\)

Define the Markov game \(\mathcal{M}\) associated with \(\mathcal{G}\) as \(\mathcal{M}=(H,S,A,\{\mathbb{T}_{h}\}_{h\in[H]},\mu_{1})\), where we omit the specification for the reward functions and one can specify them arbitrarily

Define \(\mathcal{M}(\pi_{-i})\) to be the MDP marginalized by \(\pi_{-i}\)

\(\Psi_{i}(h,s_{h})\leftarrow\) MDP_Learning\((\mathcal{M}(\pi_{-i}),\widehat{r}_{i})\)

**end for**

For each \(i\in[n]\), \(a_{h}\in\mathcal{A}\), collect \(N\) trajectories by executing policy \(\Psi_{i}(h,s_{h})\times\pi_{-i}\) for the first \(h-1\) steps then take action \(a_{h}\) deterministically and denote the dataset \(\{(s^{k,i}_{h},o^{k,i}_{h},a^{k,i}_{h},s^{k,i}_{h+1})\}_{k\in[NA]}\)

**for**\((o_{h},a_{h},s_{h+1})\in\mathcal{O}\times\mathcal{A}\times\mathcal{S}\)**do**

\(N_{h}(s_{h})\leftarrow\sum_{k\in[N],i\in[n]}\mathbb{1}[s^{k,i}_{h}=s_{h}]\)

\(N_{h}(s_{h},a_{\mathcal{T}_{h},h},s_{h+1})\leftarrow\sum_{k\in[N],i\in[n]} \mathbb{1}[s^{k,i}_{h}=s_{h},a^{k,i}_{\mathcal{T}_{h},h}=a_{\mathcal{T}_{h},h},s^{k,i}_{h+1}=s_{h+1}]\)

\(N_{h}(s_{h},o_{h})\leftarrow\sum_{k\in[N],i\in[n]}\mathbb{1}[s^{k,i}_{h}=s_{h },o^{k,i}_{h}=o_{h}]\)

\(\widehat{\mathbb{T}}_{h}(s_{h+1}\,|\,s_{h},a_{\mathcal{T}_{h},h})\leftarrow \frac{N_{h}(s_{h},a_{\mathcal{T}_{h},h},s_{h+1})}{N_{h}(s_{h},a_{\mathcal{T}_{ h},h})}\)

\(\widehat{\mathbb{O}}_{h}(o_{h}\,|\,s_{h})\leftarrow\frac{N_{h}(s_{h},o_{h})}{N_{h}(s_{h})}\)

**end for**

**end for**

Define \(\widehat{\mathcal{G}}:=(H,\mathcal{S},\mathcal{A},\mathcal{O},\{\widehat{ \mathbb{T}}_{h}\}_{h\in[H]},\{\widehat{\mathbb{O}}_{h}\}_{h\in[H]},\mu_{1}, \{r_{i}\}_{i\in[n]})\)

Define \(\widehat{g}_{j,h}(s_{h}\,|\,c_{h},p_{j,h}):=\mathbb{P}^{\widehat{\mathcal{G}}} (s_{h}\,|\,c_{h},p_{j,h})\) for each \(j\in[n]\), \(h\in[H]\), \(c_{h}\in\mathcal{C}_{h}\), \(p_{j,h}\in\mathcal{P}_{j,h}\)

**return**\(\{\widehat{g}_{j,h}\}_{j\in[n],h\in[H]}\)```
0:
* POSG \(\mathcal{G}=(H,\mathcal{S},\mathcal{A},\mathcal{O},\{\mathbb{T}_{h}\}_{h\in[H]}, \{\mathbb{O}_{h}\}_{h\in[H]},\mu_{1},\{r_{i}\}_{i\in[n]})\),
* \(\pi\in\Pi_{\mathcal{S}}\), controller set \(\{\mathcal{I}_{h}\subseteq[n]\}\),
* Procedure MDP_Learning\((\cdot,\cdot)\) that takes as input an MDP and a reward function and returns an approximate optimal policy,
* Number of trajectories \(N\).
* A decoding function for each agent and step \(\{\widehat{g}_{j,h}\}_{j\in[n],h\in[H]}\) (see Theorem J.6) for\(h\in[H]\), \(s_{h}\in\mathcal{S}\)do for\(i\in[n]\)do \(\widehat{r}_{i,h^{\prime}}(s^{\prime}_{h},a^{\prime}_{h})\leftarrow\mathbbm{1} [h^{\prime}=h,s^{\prime}_{h}=s_{h}]\) for any \((h^{\prime},s^{\prime}_{h},a^{\prime}_{h})\in[H]\times\mathcal{S}\times \mathcal{A}\).  Define \(\mathcal{M}^{\text{extended}}_{i}(\pi)\) to be the _extended_ MDP, which is defined in Definition J.5. \(\Psi_{i}(h,s_{h})\leftarrow\text{MDP\_Learning}(\mathcal{M}^{\text{extended}}_{i}(\pi),\widehat{r}_{i})\) endfor  For each \(i\in[n]\), \(a_{h}\in\mathcal{A}\), collect \(N\) trajectories by executing policy \(\Psi_{i}(h,s_{h})\times\pi_{-i}\) for the first \(h-1\) steps then take action \(a_{h}\) deterministically and denote the dataset \(\{(s^{k,i}_{h},o^{k,i}_{h},a^{k,i}_{h},s^{k,i}_{h+1})\}_{k\in[NA]}\) for\((o_{h},a_{h},s_{h+1})\in\mathcal{O}\times\mathcal{A}\times\mathcal{S}\)do \(N_{h}(s_{h})\leftarrow\sum_{k\in[N],i\in[n]}\mathbbm{1}[s^{k,i}_{h}=s_{h}]\) \(N_{h}(s_{h},a_{\mathcal{T}_{h},h},s_{h+1})\leftarrow\sum_{k\in[N],i\in[n]} \mathbbm{1}[s^{k,i}_{h}=s_{h},a^{k,i}_{\mathcal{T}_{h},h}=a_{\mathcal{T}_{h},h },s^{k,i}_{h+1}=s_{h+1}]\) \(N_{h}(s_{h},o_{h})\leftarrow\sum_{k\in[N],i\in[n]}\mathbbm{1}[s^{k,i}_{h}=s_{h},o^{k,i}_{h}=o_{h}]\) \(\widehat{\mathbb{T}}_{h}(s_{h+1}\,|\,s_{h},a_{\mathcal{T}_{h},h})\leftarrow \frac{N_{h}(s_{h},a_{\mathcal{T}_{h},h},s_{h+1})}{N_{h}(s_{h},a_{\mathcal{T}_{ h},h})}\) \(\widehat{\mathbb{O}}_{h}(o_{h}\,|\,s_{h})\leftarrow\frac{N_{h}(s_{h},o_{h})}{N_{h}(s_{h})}\) endfor endfor  Define \(\widehat{g}_{j,h}(s_{h}\,|\,c_{h},p_{j,h}):=\mathbb{P}^{\widehat{\mathcal{G}} }(s_{h}\,|\,c_{h},p_{j,h})\) for each \(j\in[n]\), \(h\in[H]\), \(c_{h}\in\mathcal{C}_{h},p_{j,h}\in\mathcal{P}_{j,h}\) return\(\{\widehat{g}_{j,h}\}_{j\in[n],h\in[H]}\) ```

**Algorithm 6** Learning Multi-Agent (Individual) Decoding Functions with Privileged Information (CE Version)

**Require:**

* POSG \(\mathcal{G}=(H,\mathcal{S},\mathcal{A},\mathcal{O},\{\mathbb{T}_{h}\}_{h\in[H]}, \{\mathbb{O}_{h}\}_{h\in[H]},\mu_{1},\{r_{i}\}_{i\in[n]})\),
* An approximate belief \(\{\widehat{P}_{h}:\widehat{\mathcal{C}}_{h}\to\Delta(\mathcal{S}\times \mathcal{P}_{h})\}_{h\in[H]}\),
* Number of iterations \(K\).

**Ensure:** An approximate equilibrium policy

**Initialize:**

\[N_{h}^{0}(s_{h},a_{h})\gets 0,\ \ \ \ N_{h}^{0}(s_{h},a_{h},o_{h})\gets 0,\ \ \ \ \widehat{\mathbb{J}}^{0}(o_{h+1}\,|\,s_{h},a_{h})\leftarrow\frac{1}{O}\]

**for \(k\in[K]\) do**

**for \(h\gets H,H-1,\cdots,1\) do**

**for \(\widehat{c}_{h}\in\widehat{\mathcal{C}}_{h}\) do**

\[Q_{i,h}^{\text{high},k}(\widehat{c}_{h},p_{h},s_{h},a_{h})\] \[\leftarrow\min\left\{r_{i,h}(s_{h},a_{h})+b_{h}^{k-1}(s_{h},a_{h })+\mathbb{E}_{o_{h+1}\sim\widehat{\mathbb{J}}_{h}^{k-1}(\,\cdot\,|\,s_{h},a_{ h})}\left[V_{i,h+1}^{\text{high},k}(\widehat{c}_{h+1})\right],H-h+1\right\}\text{ for }i\in[n]\]

\[Q_{i,h}^{\text{low},k}(\widehat{c}_{h},p_{h},s_{h},a_{h})\] \[\leftarrow\max\left\{r_{i,h}(s_{h},a_{h})-b_{h}^{k-1}(s_{h},a_{h })+\mathbb{E}_{o_{h+1}\sim\widehat{\mathbb{J}}_{h}^{k-1}(\,\cdot\,|\,s_{h},a_{ h})}\left[V_{i,h+1}^{\text{low},k}(\widehat{c}_{h+1})\right],0\right\}\text{ for }i\in[n]\]

Define

\[Q_{i,h}^{\text{high},k}(\widehat{c}_{h},\gamma_{h}):=\mathbb{E}_{s_{h},p_{h} \sim\widehat{P}_{h}(\cdot,\,|\,\widehat{c}_{h})}\mathbb{E}_{\{a_{j,h}\sim \gamma_{j,h}(\,\cdot\,|\,p_{j,h})\}_{j\in[n]}}\left[Q_{i,h}^{\text{high},k}( \widehat{c}_{h},p_{h},s_{h},a_{h})\right]\text{ for }i\in[n]\]

Define

\[Q_{i,h}^{\text{low},k}(\widehat{c}_{h},\gamma_{h}):=\mathbb{E}_{s_{h},p_{h} \sim\widehat{P}_{h}(\cdot,\,|\,\widehat{c}_{h})}\mathbb{E}_{\{a_{j,h}\sim \gamma_{j,h}(\,\cdot\,|\,p_{j,h})\}_{j\in[n]}}\left[Q_{i,h}^{\text{low},k}( \widehat{c}_{h},p_{h},s_{h},a_{h})\right]\text{ for }i\in[n]\]

\[\{\pi_{j,h}^{k}(\,\cdot\,|\cdot,\widehat{c}_{h},\cdot\,)\}_{j\in[n]}\leftarrow \text{Bayesian-CE/CCE}(\{Q_{j,h}^{\text{high},k}(\widehat{c}_{h},\cdot\,)\}_{j \in[n]})\text{ (c.f.\ Appendix~{}\ref{eq:c-exp})}\]

\[V_{i,h}^{\text{high},k}(\widehat{c}_{h})\leftarrow\mathbb{E}_{\omega_{h}}\left[ Q_{i,h}^{\text{high},k}(\widehat{c}_{h},\{\pi_{j,h}^{k}(\,\cdot\,|\,\omega_{j,h}, \widehat{c}_{h},\cdot\})_{j\in[n]})\right]\text{ for }i\in[n]\]

\[V_{i,h}^{\text{low},k}(\widehat{c}_{h})\leftarrow\mathbb{E}_{\omega_{h}}\left[ Q_{i,h}^{\text{low},k}(\widehat{c}_{h},\{\pi_{j,h}^{k}(\,\cdot\,|\,\omega_{j,h}, \widehat{c}_{h},\cdot\})_{j\in[n]})\right]\text{ for }i\in[n]\]

**end for**

**end for**

**Algorithm 7** Optimistic Common-Information-Based Value Iteration with Privileged Information

[MISSING_PAGE_EMPTY:30]

Missing Details in Section 3

**Proof of Proposition 3.1:** We recall that \(\bm{b}_{h}(\cdot)\) is the belief of the agent about the underlying state, see Appendix C for a detailed introduction. Note that Equation (3.1) can be written as

\[\widehat{\pi}^{\star}\in\arg\min_{\pi\in\Pi}\ \sum_{h=1}^{H}\mathbb{E}_{r_{h} \sim\pi^{\prime}}^{\mathcal{P}}\mathbb{E}_{s_{h}\sim\bm{b}_{h}(\tau_{h})}\left[ D_{f}(\pi_{h}^{\star}(\cdot\,|\,s_{h})\,|\,\pi_{h}(\cdot\,|\,\tau_{h}))\right].\]

Therefore, for any \(h\in[H]\) and \(\tau_{h}\) such that \(\mathbb{P}^{\pi^{\prime},\mathcal{P}}(\tau_{h})>0\), we can optimize \(\pi\) separately for each \(h\in[H]\) and \(\tau_{h}\) as:

\[\widehat{\pi}_{h}^{\star}(\cdot\,|\,\tau_{h})\in\operatorname*{ argmin}_{q\in\Delta(\mathcal{A})}\ \mathbb{E}_{s_{h}\sim\bm{b}_{h}(\tau_{h})}\left[D_{f}(\pi_{h}^{\star}(\cdot\,| \,s_{h})\,|\,q)\right].\]

Now we are ready to construct the counter-example of \(\gamma\)-observable POMDP \(\mathcal{P}^{\epsilon}\) for some \(\epsilon\in(0,1)\) with \(H=1\), \(\mathcal{S}=\left\{s^{1},s^{2}\right\}\), \(\mathcal{A}=\left\{a^{1},a^{2}\right\}\), and \(\mathcal{O}=\left\{o^{1},o^{2}\right\}\). We let \(\mu_{1}=(\frac{1-\gamma}{2-\gamma},\frac{1}{2-\gamma})\), \(\mathbb{O}_{1}(o^{1}\,|\,s^{1})=1\), and \(\mathbb{O}_{1}(o^{1}\,|\,s^{2})=1-\gamma\), \(\mathbb{O}_{1}(o^{2}\,|\,s^{2})=\gamma\). Therefore, it is direct to see that \(\mathbb{O}_{1}\) is exactly \(\gamma\)-observable. Most importantly, we choose \(r_{1}(s^{1},a^{1})=1\), \(r_{1}(s^{1},a^{2})=0\), and \(r_{1}(s^{2},a^{1})=0\), \(r_{1}(s^{2},a^{2})=\epsilon\).

Therefore, given such a reward function, the fully observable expert policy is given by \(\pi_{1}^{\star}(a^{1}\,|\,s^{1})=1\) and \(\pi_{1}^{\star}(a^{2}\,|\,s^{2})=1\), i.e., choosing \(a^{1}\) at state \(s^{1}\) and \(a^{2}\) at state \(s^{2}\) deterministically. Meanwhile, by our construction, one can compute that the belief given observation \(o^{1}\) ensures \(\bm{b}_{1}(o^{1})=\operatorname{Unif}(\mathcal{S})\). Hence, the corresponding "distilled" partially observable policy under observation \(o^{1}\) is given by

\[\widehat{\pi}_{1}^{\star}(\cdot\,|\,o^{1}) =\operatorname*{argmin}_{q\in\Delta(\mathcal{A})}\mathbb{E}_{s_{ 1}\sim\bm{b}_{1}(o^{1})}\left[D_{f}(\pi_{1}^{\star}(\cdot\,|\,s_{1})\,|\,q)\right]\] \[=\operatorname*{argmin}_{q\in\Delta(\mathcal{A})}\frac{D_{f}(\pi_ {1}^{\star}(\cdot\,|\,s^{1})\,|\,q)+D_{f}(\pi_{1}^{\star}(\cdot\,|\,s^{2})\,| \,q)}{2}\] \[=\operatorname*{argmin}_{q\in\Delta(\mathcal{A})}\frac{f(1/q(a^ {1}))q(a^{1})+f(0)q(a^{2})+f(0)q(a^{1})+f(1/q(a^{2}))q(a^{2})}{2}\] \[=\operatorname*{argmin}_{q\in\Delta(\mathcal{A})}\frac{f(0)+f(1/q (a^{1}))q(a^{1})+f(1/q(a^{2}))q(a^{2})}{2},\]

where the last step is due to \(q\in\Delta(\mathcal{A})\). Now consider the function \(g(x)=xf(1/x)\) for \(x>0\). It is direct to compute that \(g^{\prime}(x)=f(1/x)-\frac{f^{\prime}(1/x)}{x}\), and \(g^{\prime\prime}(x)=\frac{f^{\prime\prime}(1/x)}{x^{3}}\geq 0\) due to the convexity of the function \(f\). Thus, we conclude that \(g\) is also convex. By Jensen's inequality, we have

\[\frac{f(1/q(a^{1}))q(a^{1})+f(1/q(a^{2}))q(a^{2})}{2}\geq f(2/(q(a^{1})+q(a^{ 2})))(q(a^{1})+q(a^{2}))/2=f(2)/2,\]

where the equality holds when \(q(a^{1})=q(a^{2})=\frac{1}{2}\). This indicates that \(\widehat{\pi}_{1}^{\star}(\cdot\,|\,o_{1})=\operatorname{Unif}(\mathcal{A})\). On the other hand, combining the fact that \(\bm{b}_{1}(o^{1})=\operatorname{Unif}(\mathcal{S})\) with \(\epsilon<1\), it is direct to see that the optimal partially observable policy \(\widetilde{\pi}\in\arg\max_{\pi\in\Pi}v^{\mathcal{P}}(\pi)\) satisfies \(\widetilde{\pi}_{1}(a^{1}\,|\,o^{1})=1\). Now we are ready to evaluate the optimality gap between \(\widetilde{\pi}\) and \(\widetilde{\pi}^{\star}\) as follows

\[v^{\mathcal{P}^{\epsilon}}(\widetilde{\pi})-v^{\mathcal{P}^{ \epsilon}}(\widehat{\pi}^{\star}) =\mathbb{P}^{\mathcal{P}^{\epsilon}}(o^{1})(V_{1}^{\widetilde{\pi},\mathcal{P}^{\epsilon}}(o^{1})-V_{1}^{\widetilde{\pi}^{\star},\mathcal{P}^{ \epsilon}}(o^{1}))+\mathbb{P}^{\mathcal{P}^{\epsilon}}(o^{2})(V_{1}^{ \widetilde{\pi},\mathcal{P}^{\epsilon}}(o^{2})-V_{1}^{\widetilde{\pi}^{\star}, \mathcal{P}^{\epsilon}}(o^{2}))\] \[\geq\mathbb{P}^{\mathcal{P}^{\epsilon}}(o^{1})(V_{1}^{\widetilde{ \pi},\mathcal{P}^{\epsilon}}(o^{1})-V_{1}^{\widetilde{\pi}^{\star},\mathcal{P}^{ \epsilon}}(o^{1})),\]

where the last step is due to the fact that \(\widetilde{\pi}\) is the optimal policy, leading to the fact that \(V_{1}^{\widetilde{\pi},\mathcal{P}^{\epsilon}}(o^{2})-V_{1}^{\widetilde{\pi}^{ \star},\mathcal{P}^{\epsilon}}(o^{2})\geq 0\). Now it is not hard to compute that

\[\mathbb{P}^{\mathcal{P}^{\epsilon}}(o^{1})\geq 1-\gamma.\]

Meanwhile, we can evaluate that

\[V_{1}^{\widetilde{\pi},\mathcal{P}^{\epsilon}}(o^{1})=\frac{1}{2},\quad V_{1}^{ \widehat{\pi}^{\star},\mathcal{P}^{\epsilon}}(o^{1})=\frac{1+\epsilon}{4}\]

and correspondingly \(V_{1}^{\widetilde{\pi},\mathcal{P}^{\epsilon}}(o^{1})-V_{1}^{\widetilde{\pi}^{ \star},\mathcal{P}^{\epsilon}}(o^{1})=\frac{1-\epsilon}{4}\), implying that \(v^{\mathcal{P}^{\epsilon}}(\widetilde{\pi})-v^{\mathcal{P}^{\epsilon}}( \widehat{\pi}^{\star})\geq\frac{(1-\gamma)(1-\epsilon)}{4}\). This concludes our proof.

Note that another counterexample in a similar spirit was also constructed in [34], demonstrating that the expert policy for a poorly chosen agent-environment boundary can be useless in imitation learning, although the \(\gamma\)-observability property is not satisfied for the construction therein.

**Remark E.1**.: The counter-example \(\mathcal{P}^{\epsilon}\) constructed above can be also used to demonstrate the _bias_ of the state-only-based value function as an estimate of the history-based value function that appeared in the policy gradient formula for POMDPs, in the finite-horizon setting, i.e. \(\mathbb{E}_{s_{h}\sim\bm{b}_{1}(\tau_{h})}[V_{h}^{\pi,\mathcal{P}^{\epsilon}} (s_{h})]\neq V_{h}^{\pi,\mathcal{P}^{\epsilon}}(\tau_{h})\) (mirroring Theorem 4.2 of [6]). Specifically, in the counter-example above, we consider the policy \(\pi\) such that \(\pi_{1}(a_{1}\,|\,o_{1})=1\) and \(\pi_{1}(a_{2}\,|\,o_{2})=1\). The state-only-based value function can be evaluated as

\[V_{1}^{\pi,\mathcal{P}^{\epsilon}}(s_{1})=\frac{1-\gamma}{2- \gamma},\qquad\qquad V_{1}^{\pi,\mathcal{P}^{\epsilon}}(s_{2})=\gamma\epsilon,\]

which implies that \(\mathbb{E}_{s_{1}\sim\bm{b}_{1}(o_{1})}[V_{1}^{\pi,\mathcal{P}^{\epsilon}}(s_ {1})]=\frac{V_{1}^{\pi,\mathcal{P}^{\epsilon}}(s_{1})+V_{1}^{\pi,\mathcal{P}^ {\epsilon}}(s_{2})}{2}=\frac{\frac{1-\gamma}{2-\gamma}+\gamma\epsilon}{2}\). On the other hand, it holds that \(V_{1}^{\pi,\mathcal{P}^{\epsilon}}(o_{1})=\frac{1+\epsilon}{2}\), which is not equal to \(\mathbb{E}_{s_{1}\sim\bm{b}_{1}(o_{1})}[V_{1}^{\pi,\mathcal{P}^{\epsilon}}(s_ {1})]\), showing the bias of such a state-only value function.

**Example E.2** (Deterministic POMDP [36, 81]).: We say a POMDP \(\mathcal{P}\) is of deterministic transition if entries of matrices \(\{\mathbb{T}_{h}\}_{h\in[H]}\) and the vector \(\mu_{1}\) are either \(0\) or \(1\). Note that we do not make any assumptions on the emission matrices.

**Example E.3** (Block MDP [43, 18]).: We say a POMDP \(\mathcal{P}\) is a block MDP if for any \(h\in[H]\), \(s_{h},s_{h}^{\prime}\in\mathcal{S}\), it holds that \(\operatorname{supp}(\mathbb{O}_{h}(\cdot\,|\,s_{h}))\cap\operatorname{supp}( \mathbb{O}_{h}(\cdot\,|\,s_{h}^{\prime}))=\emptyset\) when \(s_{h}\neq s_{h}^{\prime}\).

**Example E.4** (\(k\)-step decodable POMDP [19]).: We say a POMDP \(\mathcal{P}\) is a \(k\)-step decodable POMDP if there exists an unknown decoder \(\phi^{\star}=\{\phi^{\star}_{h}:\mathcal{Z}_{h}\to\mathcal{S}\}_{h\in[H]}\) such that for any \(h\in[H]\) and reachable trajectory \(\tau_{h}\), \(\mathbb{P}^{\mathcal{P}}(s_{h}=\phi^{\star}_{h}(z_{h})\,|\,\tau_{h})=1\), where \(\mathcal{Z}_{h}=(\mathcal{O}\times\mathcal{A})^{\max\{h-1,k-1\}}\times\mathcal{O}\), \(z_{h}=((o,a)_{k(h):h-1},o_{h})\), and \(k(h)=\max\{h-k+1,1\}\).

Finally, to understand how our condition can extend beyond known examples in the literature, we show that one can indeed allow the decoding length of Example E.4 to be unknown and arbitrary (instead of being a small known constant as in [19] to have provably efficient algorithms).

**Example E.5** (POMDP with arbitrary, unknown decodable length).: This example is similar to Example E.4, but the decoding length \(m\) is unknown and not necessarily a small constant.

In light of the pitfall in Proposition 3.1, we will analyze _both_ the computational and statistical efficiencies of expert distillation in Section 4, under the condition in Definition 3.2.

Proof of Example E.2 & Example E.3 & Example E.4 & Example E.5: To see why those examples follow our Definition 3.2, it is indeed an immediate result of Proposition 7.1. 

Proof of Proposition 3.3:

Here we evaluate the computational complexity and sample complexity of each iteration \(t\) as follows.

Sample complexity:The algorithm executes the policy \(\pi^{t-1}\) and collect \(K\) episodes, denoted as \(\{s_{1:H+1}^{k},o_{1:H}^{k},a_{1:H}^{k}\}_{k\in[K]}\). Thus, the sample complexity of each iteration is \(\Theta(K)\).

Computational complexity for policy evaluation:The policy evaluation of the vanilla asymmetric actor-critic is done by minimizing the Bellman error. In the finite-horizon setting with tabular parameterization, it is equivalent to performing the following update for each \(h\in[H]\) in a backward way and each \(k\in[K]\):

\[Q_{h}^{t}(\tau_{h}^{k},s_{h}^{k},a_{h}^{k}) \leftarrow(1-\alpha)Q_{h}^{t-1}(\tau_{h}^{k},s_{h}^{k},a_{h}^{k})\] \[+\alpha\left(r_{h}(s_{h}^{k},a_{h}^{k})+\frac{1}{|\mathcal{J}( \tau_{h}^{k},s_{h}^{k},a_{h}^{k})|}\sum_{j\in\mathcal{J}(\tau_{h}^{k},s_{h}^{k},a_{h}^{k})}Q_{h+1}^{t}(\tau_{h+1}^{j},s_{h+1}^{j},a_{h+1}^{j})\right),\]

for some stepsize \(\alpha\in(0,1)\), where \(\mathcal{J}(\tau_{h}^{k},s_{h}^{k},a_{h}^{k}):=\{j\in[K]|\,(\tau_{h}^{j},s_{h}^{j },a_{h}^{j})=(\tau_{h}^{k},s_{h}^{k},a_{h}^{k})\}\). Therefore, the computational complexity for this procedure is of \(\textsc{poly}(H,K)\).

Computational complexity for policy improvement:For tabular parameterization, computing \(\nabla\log\pi_{h}^{t-1}(a_{h}^{k}\,|\,\tau_{h}^{k})\) takes \(\mathcal{O}(1)\) computation. Hence the policy update in Equation (3.2) performs \(\textsc{poly}(H,K)\) computation.

Meanwhile, under the exponential time hypothesis, there is no polynomial time algorithm for even planning an \(\epsilon\)-approximate optimal policy in \(\gamma\)-observable POMDPs [26]. This implies that the vanilla asymmetric actor-critic needs to take super-polynomial time to find an approximately optimal policy. This implies the corresponding sample complexity has to be super-polynomial.

Finally, we remark that even if we let the policy and the \(Q\)-function not depend on the entire history \(\tau_{h}\) but only the finite-memory \(z_{h}\), the proof still holds. The key is that whenever one only _computes_ at the _sampled_ history/finite-memories, i.e., updates the policy in an _asynchronous_ way (in contrast to the _synchronous_ one where the policies at _all_ histories/finite-memories are updated), the sample and computational complexities will be coupled with the same order per iteration, which implies a super-polynomial sample complexity due to the super-polynomial computational complexity. This completes the proof. 

Derivation for the closed-form update Equation (3.3).Note that the proximal policy optimization [72] update has the policy improvement as follows

\[\pi^{t}\leftarrow\arg\max_{\pi}\left\{L^{t-1}(\pi)-\eta^{-1}\mathbb{E}_{\pi^{ t-1}}^{\mathcal{P}}\left[\sum_{h\in[H]}\text{KL}(\pi_{h}(\cdot\,|\,\tau_{h})\,|\, \pi_{h}^{t-1}(\cdot\,|\,\tau_{h}))\right]\right\},\] (E.1)

where \(\eta\) is some learning rate and \(L^{t-1}(\pi)\) is a first-order approximation of the expected accumulated rewards at \(\pi^{t-1}\):

\[L^{t-1}(\pi):=v^{\mathcal{P}}(\pi^{t-1})+\mathbb{E}_{\pi^{t-1}}^{\mathcal{P}} \left[\sum_{h\in[H]}\left\langle Q_{h}^{t-1}(\tau_{h},s_{h},\cdot),\pi_{h}( \cdot\,|\,\tau_{h})-\pi_{h}^{t-1}(\cdot\,|\,\tau_{h})\right\rangle\right].\]

By plugging \(L^{t-1}(\pi)\) into Equation (E.1), with simple algebric manipulations, we prove that:

\[\pi_{h}^{t}(\cdot\,|\,\tau_{h})\propto\pi_{h}^{t-1}(\cdot\,|\,\tau_{h})\exp \left(\eta\mathbb{E}_{s_{h}\sim\bm{b}_{h}(\tau_{h})}\left[Q_{h}^{t-1}(\tau_{h },s_{h},\cdot)\right]\right).\]

## Appendix F Missing Details in Section 4

**Proof of Lemma 4.4:** The proof follows by the assumption that the total cumulative reward is at most \(H\),

\[v^{\mathcal{P}}(\pi)\geq\mathbb{E}_{\pi}^{\mathcal{P}}\left[ \left(\sum_{h\in[H]}r_{h}\right)\mathbbm{1}[\forall h:\in[H]:g_{h}\left(s_{h-1 },a_{h-1},o_{h}\right)=s_{h}]\right]\] \[\qquad\qquad+\mathbb{E}_{\pi^{E}}^{\mathcal{P}}\left[\left(\sum_ {h\in[H]}r_{h}\right)\mathbbm{1}[\exists h:\in[H]:g_{h}\left(s_{h-1},a_{h-1},o _{h}\right)\neq s_{h}]\right]\] \[\qquad\qquad-\mathbb{E}_{\pi^{E}}^{\mathcal{P}}\left[\left(\sum_ {h\in[H]}r_{h}\right)\mathbbm{1}[\exists h:\in[H]:g_{h}\left(s_{h-1},a_{h-1},o _{h}\right)\neq s_{h}]\right]\] \[\qquad\qquad\geq v^{\mathcal{P}}(\pi^{E})-H\mathbb{P}^{\pi^{E}, \mathcal{P}}\ [\exists h:\in[H]:g_{h}\left(s_{h-1},a_{h-1},o_{h}\right)\neq s_{h}]\] \[\qquad\qquad\geq v^{\mathcal{P}}(\pi^{E})-H\epsilon,\]

which completes the proof.

**Proof of Theorem 4.5:** For each step \(h\in[H]\) we define \(D_{h}\) to be the distribution over the underlying state \(s_{h-1}\) at step \(h-1\), taken action \(a_{h-1}\in\mathcal{A}\) based on \(\pi^{E}\), the underlying state transitioned to \(s_{h}\in\mathcal{S}\), and the observation \(o_{h}\sim\mathcal{O}_{h}(\cdot\mid s_{h})\). We remind that we include at step zero, a dummy state-observation pair \((s_{0},o_{0})\), for notational convenience. Formally, \(D_{h}\) is defined as \(D_{h}(s_{h-1},a_{h-1},o_{h},s_{h}):=\mathbb{P}^{\pi^{E},\mathcal{P}}\left[s_{h -1},a_{h-1},o_{h},s_{h}\right].\)

We first use union bound to decompose the probability that we incorrectly decode,

\[\mathbb{P}^{\pi^{E},\mathcal{P}}\ \left[\exists h\in[H]:g_{h}\left(s_{h-1},a_{h-1},o_ {h}\right)\neq s_{h}\right] \leq\sum_{h\in[H]}\mathbb{P}^{\pi^{E},\mathcal{P}}\ \left[g_{h}\left(s_{h-1},a_{h-1},o_{h}\right)\neq s_{h}\right]\] \[=\sum_{h\in[H]}\mathbb{P}_{\left(s_{h-1},a_{h-1},o_{h},s_{h} \right)\sim D_{h}}\ \left[g_{h}\left(s_{h-1},a_{h-1},o_{h}\right)\neq s_{h}\right].\] (F.1)

For each \(h\in[H]\), we can use \(M\) episodes to collect \(M\) samples from the distribution \(D_{h}\). In addition, since state \(s_{H+1}\) is dummy, we need not to collect episodes for \(D_{H+1}\). Denote the set of collected samples by \(\widehat{D}_{h}^{M}\). We define the decoding \(g_{h}\) for step \(h\in[H]\) as follows:

\[g_{h}(s_{h-1},a_{h-1},o_{h})=\left\{s_{h}\mid(s_{h-1},a_{h-1},o_{h},s_{h})\in \widehat{D}_{h}^{M}\right\}.\]

Observe that by Definition 3.2, \(\{s_{h}\mid(s_{h-1},a_{h-1},o_{h},s_{h})\in\widehat{D}_{h}^{M}\}\) is either the empty set or contains only a single elements, in which case, it is true that \(g_{h}(s_{h-1},a_{h-1},o_{h})=\psi_{h}(s_{h-1},a_{h-1},o_{h})\) (\(\psi\) is the real decoding function, see Definition 3.2). Moreover, we slightly abuse the notation and let \(\widehat{D}_{h}^{M}\) denote the empirical distribution induced by the samples in \(\widehat{D}_{h}^{M}\). Thus, with probability at least \(1-\frac{\delta}{H}\) and setting \(M=\Theta\left(\frac{A\cdot O\cdot S+\log(H/\delta)}{\epsilon^{2}}\right)\) for each step \(h\in[H]\), we obtain the following by the result in [13]:

\[\mathbb{P}^{\pi^{E},\mathcal{P}}\ \left[g_{h}\left(s_{h-1},a_{h-1},o_{h} \right)\neq s_{h}\right]= \mathbb{P}^{\pi^{E},\mathcal{P}}\ \left[g_{h}\left(s_{h-1},a_{h-1},o_{h} \right)=\emptyset\right]\] \[= \mathbb{P}_{\left(s_{h-1},a_{h-1},o_{h},s_{h}\right)\sim D_{h}} \ \left[(s_{h-1},a_{h-1},o_{h},s_{h})\notin\widehat{D}_{h}^{M}\right]\] \[= \sum_{u\in\operatorname{supp}(D_{h})}\mathbb{P}_{u^{\prime}\sim D _{h}}[u=u^{\prime}]\mathbbm{1}[u\notin\operatorname{supp}(\widehat{D}_{h}^{M })]\] \[\leq d_{TV}(D_{h},\widetilde{D}_{h}^{M})\] \[\leq \epsilon.\]

Thus, by union bound, with probability at least \(1-\delta\), we have that for each step \(h\in[H]\),

\[\mathbb{P}_{\left(s_{h-1},a_{h-1},o_{h},s_{h}\right)\sim D_{h}}\ \left[g_{h}\left(s_{h-1},a_{h-1},o_{h} \right)\neq s_{h}\right]\leq\epsilon,\]

which in combination with Equation (F.1) concludes the proof. Finally, we note that we used a total of \(\Theta\left(H\cdot\frac{A\cdot O\cdot S+\log(H/\delta)}{\epsilon^{2}}\right)\) episodes from the POMDP, and the computational time was \(\operatorname{\textsc{poly}}(H,A,O,S,\frac{1}{\epsilon},\log\left(\frac{1}{ \delta}\right))\). 

## Appendix G Provably Efficient Expert Policy Distillation with Function Approximation

We now turn our attention to the rich-observation setting under our deterministic filter condition. Definition 3.2 motivates us to consider only succinct policies that incorporate an auxiliary parameter representing the most recent state, as well as the most recent observations and actions. To handle the large observation space, we further assume that for each step \(h\in[H]\), the agent selects a decoding function \(g_{h}\) from a family of _multi-class classifiers_\(\mathcal{F}_{h}\subset\{\mathcal{S}\times\mathcal{A}\times\mathcal{O}\to \mathcal{S}\}\). For the function class \(\mathcal{F}_{h}\), we make the standard realizability assumption. We formally summarize our assumptions in Assumption G.1.

**Assumption G.1**.: We consider a POMDP that satisfies Definition 3.2. In addition, to derive learning algorithms that do not dependent on \(O\), for each step \(h\in[H]\), we assume that we have access to a class of functions \(\mathcal{F}_{h}:\mathcal{S}\times\mathcal{A}\times\mathcal{O}\to\mathcal{S}\) such that the perfect decoding function \(\psi_{h}\in\mathcal{F}_{h}\).

We aim for our final bounds to depend on a complexity measure of the function class \(\mathcal{F}=\{\mathcal{F}_{h}\}_{h\in[H]}\) rather than the cardinality of the observation space \(\mathbb{O}\). We utilize the Daniely and Shalev-Shwartz-Dimension (DS Dimension) (Theorem G.2), which characterizes the PAC learnability for multi-class classification [8]. Defining the DS dimension is beyond the scope of our paper; we direct interested readers to [8] for further details. For intuition, readers can think of the DS Dimension as a certificate of PAC learnability without loss of intuition.

**Theorem G.2** (Theorem 1 in [8]).: Consider a family of multi-class classifiers \(\mathcal{F}\) that map features in space \(x\in\mathcal{X}\) to labels in space \(y\in\mathcal{Y}\). Moreover, assume there is a joint probability distribution \(D\) over features in \(\mathcal{X}\) and labels in \(\mathcal{Y}\), and that there exists \(g^{*}\in\mathcal{F}\) such that for each \((x,y)\in\mathrm{supp}(D)\), \(g^{*}(x)=y\). Given \(n\) samples from \(D\), there exists an algorithm that with probability at least \(1-\delta\) outputs \(\widetilde{g}\in\mathcal{F}\) such that

\[\mathbb{P}_{(x,y)\sim D}[\widetilde{g}(x)\neq y]\leq\widetilde{\mathcal{O}} \left(\frac{d_{DS}^{3/2}(\mathcal{F})+\log\left(\frac{1}{\delta}\right)}{n} \right),\]

where \(d_{DS}(\mathcal{F})\) denotes the Daniely and Shalev-Shwartz-Dimension of the function class \(\mathcal{F}\).

We are now ready to present the main theorem of this section.

**Theorem G.3**.: Consider a POMDP \(\mathcal{P}\) that satisfies Definition 3.2, a policy \(\pi^{E}\in\Pi_{\mathcal{S}}\), and let \(\{\mathcal{F}_{h}\subseteq\{\mathcal{S}\times\mathcal{A}\times\mathcal{O} \rightarrow\mathcal{S}\}\}_{h\in[H]}\) be the decoding function class, and \(\psi_{h}\in\mathcal{F}_{h}\) for each \(h\in[H]\), i.e., \(\{\mathcal{F}_{h}\}_{h\in[H]}\) is realizable. Then given access to the classification oracle of [9], there exists an algorithm learning the decoding function \(\{g_{h}\}_{h\in[H]}\) such that with probability at least \(1-\delta\), for each step \(h\in[H]\):

\[\mathbb{P}^{\pi^{E},\mathcal{P}}\ \left[\exists h\in[H]:g_{h}\left(s_{h-1},a_{h-1}, o_{h}\right)\neq s_{h}\right]\leq\epsilon,\]

using \(\mathcal{O}\left(\frac{H^{2}\left(\max_{h\in[H]}d_{DS}^{3/2}(\mathcal{F}_{h}) +\log\left(\frac{1}{\delta}\right)\right)}{\epsilon}\right)\) episodes, where \(d_{DS}(\mathcal{F}_{h})\) is the Daniely and Shalev-Shwartz-Dimension of \(\mathcal{F}_{h}\)[8].

Combining Theorem G.3 and Lemma 4.4, we obtain the final polynomial sample complexity in this function approximation setting, using classification (supervised learning) oracles (c.f. Table 1).

**Proof of Theorem G.3:**

For each step \(h\in[H]\), we define \(D_{h}\) to be the distribution over the underlying state \(s_{h-1}\) at step \(h-1\), taken action \(a_{h-1}\in\mathcal{A}\) from \(\pi^{E}\), the underlying state transitioned to \(s_{h}\in\mathcal{S}\), and the hallucinated observation \(o_{h}\sim\mathbb{O}_{h}(\cdot\mid s_{h})\) (we remind readers that for step \(0\), we use dummy state \(s_{0}\) and action \(a_{0}\)). Formally, the probability that the sequence \(\left(s_{h-1},a_{h-1},o_{h},s_{h}\right)\) is sampled from \(D_{h}\) equals to

\[D_{h}(s_{h-1},a_{h-1},o_{h},s_{h}):=\mathbb{P}^{\pi^{E},\mathcal{P}}\left[s_{h- 1},a_{h-1},o_{h},s_{h}\right].\]

We first use union bound to decompose the misclassification error,

\[\mathbb{P}^{\pi^{E},\mathcal{P}}\ \left[\exists h\in[H]: \widetilde{g}_{h}\left(s_{h-1},a_{h-1},o_{h}\right)\neq s_{h}\right] \leq\sum_{h\in[H]}\mathbb{P}^{\pi^{E},\mathcal{P}}\ \left[\widetilde{g}_{h}\left(s_{h-1},a_{h-1},o_{h}\right)\neq s_{h}\right]\] \[=\sum_{h\in[H]}\mathbb{P}_{\left(s_{h-1},a_{h-1},o_{h},s_{h} \right)\sim D_{h}}\ \left[\widetilde{g}_{h}\left(s_{h-1},a_{h-1},o_{h}\right)\neq s_{h}\right].\] (G.1)

For each \(h\in[H]\), we can use \(\widetilde{\mathcal{O}}\left(\frac{H}{\epsilon}\cdot\left(\max_{h\in[H]}d_{DS} ^{3/2}(\mathcal{F}_{h})+\log\left(\frac{1}{\delta}\right)\right)\right)\) episodes to collect \(\widetilde{\mathcal{O}}\left(\frac{H}{\epsilon}\cdot\left(\max_{h\in[H]}d_{ DS}^{3/2}(\mathcal{F}_{h})+\log\left(\frac{1}{\delta}\right)\right)\right)\) samples from distribution \(D_{h}\). Hence, by Theorem G.2, with probability at least \(1-\frac{\delta}{H}\), we have that

\[\mathbb{P}_{\left(s_{h-1},a_{h-1},o_{h},s_{h}\right)\sim D_{h}}\ \left[ \widetilde{g}_{h}\left(s_{h-1},a_{h-1},o_{h}\right)\neq s_{h}\right]\leq\frac{ \epsilon}{H}.\]

Thus, by union bound, with probability at least \(1-\delta\), using a total of \(\widetilde{\mathcal{O}}\left(\frac{H^{2}}{\epsilon}\cdot\left(\max_{h\in[H]}d_{ DS}^{3/2}(\mathcal{F}_{h})+\log\left(\frac{1}{\delta}\right)\right)\right)\) episodes we have that,

\[\sum_{h\in[H]}\mathbb{P}_{\left(s_{h-1},a_{h-1},o_{h},s_{h}\right)\sim D_{h}} \ \left[\widetilde{g}_{h}\left(s_{h-1},a_{h-1},o_{h}\right)\neq s_{h}\right]\leq\epsilon,\]which in combination with Equation (G.1) concludes the proof. 

## Appendix H Missing Details in Section 5

**Proof of Theorem 5.1:** Let \(\pi^{*}\in\operatorname*{argmax}_{\pi\in\Pi^{L}}\mathbb{E}_{s_{1}\sim\mu_{1}} \left[V_{1}^{\pi}(s_{1})\right]\), where we define \(V_{1}^{\pi}(s_{1}):=\mathbb{E}_{o_{1}\sim\mathcal{O}_{1}(\cdot\mid s_{1}),a_{1 }\sim\pi_{1}(\cdot\mid o_{1})}[Q_{h}^{\pi}(z_{1}=(o_{1}),s_{1},a_{1})]\). We first note the following equation

\[\frac{1}{T}\sum_{t\in[T]}\mathbb{E}_{s_{1}\sim\mu_{1}}[V_{1}^{\pi^ {t}}(s_{1})]\] \[\quad=\mathbb{E}_{s_{1}\sim\mu_{1}}[V_{1}^{\pi^{*}}(s_{1})]+\frac {1}{T}\sum_{t\in[T]}\mathbb{E}_{s_{1}\sim\mu_{1}}\left(\widetilde{V}_{1}^{\pi ^{t}}(s_{1})-V_{1}^{\pi^{*}}(s_{1})\right)+\frac{1}{T}\sum_{t\in[T]}\mathbb{E} _{s_{1}\sim\mu_{1}}\left(V_{1}^{\pi^{t}}(s_{1})-\widetilde{V}_{1}^{\pi^{t}}(s _{1})\right).\] (H.1)

Next, we make use of the performance difference lemma [39, 1, 74] on the extended space \(\prod_{h\in[H]}(\mathcal{Z}_{h}\times\mathcal{S})\).

**Definition H.1**.: Consider the class of policies \(\Pi^{PL}\) such that at step \(h\in[H]\), the policies in \(\Pi^{PL}\) take an action based on finite memory up to this step and the use of the underlying state, e.g., for each policy \(\pi_{1:H}\in\Pi^{PL}\), \(\pi_{h}:\mathcal{Z}_{h}\times\mathcal{S}\to\Delta(\mathcal{A})\).

**Observation 1**.: Note that \(\Pi^{L}\subseteq\Pi^{PL}\).

**Lemma H.2** (Performance difference Lemma [39, 1, 74]; see e.g., Lemma 1 in [74]).: For any pair of policies \(\pi=\{\pi_{h}\}_{h\in[H]},\pi^{\prime}=\{\pi^{\prime}_{h}\}_{h\in[H]}\in\Pi^{ PL}\), and approximation of the \(Q\)-function of policy \(\pi\), we have that for each state \(s_{1}\in\operatorname*{supp}(\mu_{1})\):

\[\widetilde{V}_{1}^{\pi}(z_{1},s_{1})-V_{1}^{\pi^{\prime}}(z_{1}, s_{1})\] \[\quad\quad+\sum_{h\in[H]}\mathbb{E}_{\pi_{h}\sim\pi^{\prime}\mid z _{1}}\left[\widetilde{Q}_{h}^{\pi}(z_{h},s_{h},a_{h})-\mathbb{E}_{\begin{subarray} {c}s_{h+1}\sim\mathbb{T}_{h}(\cdot\mid s_{h},a_{h}),\\ o_{h+1}\sim\mathcal{O}_{h+1}(\cdot\mid s_{h+1})\end{subarray}}\left[r_{h}(s_{ h},a_{h})+\widetilde{V}_{h+1}^{\pi}(z_{h+1},s_{h+1})\right]\right],\]

where \(\widetilde{V}_{h}^{\pi}(z_{h},s_{h})=\mathbb{E}_{a_{h}\sim\pi_{h}(\cdot\mid z _{h})}[\widetilde{Q}_{h}^{\pi}(z_{h},s_{h},a_{h})]\).

Setting \(\pi=\pi^{t}\in\Pi^{L}\subseteq\Pi^{LP}\), and \(\pi^{\prime}=\pi^{*}\in\Pi^{L}\subseteq\Pi^{LP}\), where we remind that \(\pi^{*}\in\operatorname*{argmax}_{\pi\in\Pi^{L}}V_{1}^{\pi}(s_{1})\), and for each \(z_{h}\in\mathcal{Z}_{h}\) and \(h\in[H]\), we abuse the notation by letting \(\pi^{t}_{h}(\cdot\mid z_{h},s_{h})=\pi^{t}_{h}(\cdot\mid z_{h})\) and \(\pi^{*}_{h}(\cdot\mid z_{h},s_{h})=\pi^{*}_{h}(\cdot\mid z_{h})\) for all \(s_{h}\in\mathcal{S}\). The above formulation is thus simplified to

\[\mathbb{E}_{s_{1}\sim\mu_{1}}[\widetilde{V}_{1}^{\pi^{t}}(s_{1})- V_{1}^{\pi^{*}}(s_{1})]\] \[\quad\quad+\sum_{h\in[H]}\mathbb{E}_{\pi_{h}\sim\pi^{*}}\left[ \widetilde{Q}_{h}^{\pi}(z_{h},s_{h},a_{h})-\mathbb{E}_{\begin{subarray}{c}s_{h+ 1}\sim\mathbb{T}_{h}(\cdot\mid s_{h},a_{h}),\\ o_{h+1}\sim\mathcal{O}_{h+1}(\cdot\mid s_{h+1})\end{subarray}}\left[r_{h}(s_{ h},a_{h})+\widetilde{V}_{h+1}^{\pi}(z_{h+1},s_{h+1})\right]\right]\] \[\quad\quad\geq\sum_{h\in[H]}\mathbb{E}_{\pi_{h}\sim\pi^{*}}\left[ \left\langle\widetilde{Q}_{h}^{\pi^{t}}(z_{h},s_{h},.),\pi^{t}_{h}(\cdot\mid z_{h},s_{h})-\pi^{*}_{h}(\cdot\mid z_{h},s_{h})\right\rangle\right],\]where in the inequality above we used Lemma H.3. Since our policy does not depend on the realized underlying state \(s_{h}\),

\[\mathbb{E}_{s_{1}\sim\mu_{1}}[\widetilde{V}_{1}^{\pi^{\prime}}(s_{1 })-V_{1}^{\pi^{*}}(s_{1})]\] \[\geq\sum_{h\in[H]}\mathbb{E}_{\tau_{h}\sim\pi^{*}}\left[\left\langle \widetilde{Q}_{h}^{\pi^{\prime}}(z_{h},s_{h},\cdot),\pi_{h}^{t}(\cdot\mid z_{h },s_{h})-\pi_{h}^{*}(\cdot\mid z_{h},s_{h})\right\rangle\right]\] \[=\sum_{h\in[H]}\mathbb{E}_{\tau_{h}\sim\pi^{*}}\left[\left\langle \mathbb{E}_{s_{h}\sim\bm{b}\bm{\tau}_{h}}\left[\widetilde{Q}_{h}^{\pi^{\prime }}(z_{h},s_{h},\cdot)\right],\pi_{h}^{t}(\cdot\mid z_{h})-\pi_{h}^{*}(\cdot\mid z _{h})\right\rangle\right]\] \[=\sum_{h\in[H]}\mathbb{E}_{\tau_{h}\sim\pi^{*}}\left[\left\langle \mathbb{E}_{s_{h}\sim\bm{b}\bm{\tau}^{\text{ex}}(z_{h})}\left[\widetilde{Q}_{ h}^{\pi^{\prime}}(z_{h},s_{h},\cdot)\right],\pi_{h}^{t}(\cdot\mid z_{h})-\pi_{h}^{*}( \cdot\mid z_{h})\right\rangle\right]\] \[\qquad\qquad+\sum_{h\in[H]}\mathbb{E}_{\tau_{h}\sim\pi^{*}}\left[ \left\langle\mathbb{E}_{s_{h}\sim\bm{b}(\tau_{h})}\left[\widetilde{Q}_{h}^{\pi^ {\prime}}(z_{h},s_{h},\cdot)\right]-\mathbb{E}_{s_{h}\sim\bm{b}\bm{\tau}^{ \text{ex}}(z_{h})}\left[\widetilde{Q}_{h}^{\pi^{\prime}}(z_{h},s_{h},\cdot) \right],\pi_{h}^{t}(\cdot\mid z_{h})-\pi_{h}^{*}(\cdot\mid z_{h})\right\rangle\right]\] \[\geq\sum_{h\in[H]}\mathbb{E}_{\tau_{h}\sim\pi^{*}}\left[\left\langle \mathbb{E}_{s_{h}\sim\bm{b}\bm{\tau}^{\text{ex}}(z_{h})}\left[\widetilde{Q}_{ h}^{\pi^{\prime}}(z_{h},s_{h},\cdot)\right],\pi_{h}^{t}(\cdot\mid z_{h})-\pi_{h}^{*}( \cdot\mid z_{h})\right\rangle\right]\] \[\qquad\qquad-\sum_{h\in[H]}\mathbb{E}_{\tau_{h}\sim\pi^{*}}\left[ \left\|\mathbb{E}_{s_{h}\sim\bm{b}(\tau_{h})}\left[\widetilde{Q}_{h}^{\pi^{ \prime}}(z_{h},s_{h},\cdot)\right]-\mathbb{E}_{s_{h}\sim\bm{b}\bm{\tau}^{ \text{ex}}(z_{h})}\left[\widetilde{Q}_{h}^{\pi^{\prime}}(z_{h},s_{h},\cdot) \right]\right\|_{1}\right]\] \[\geq\sum_{h\in[H]}\mathbb{E}_{\tau_{h}\sim\pi^{*}}\left[\left\langle \mathbb{E}_{s_{h}\sim\bm{b}\bm{\tau}^{\text{ex}}(z_{h})}\left[\widetilde{Q}_{ h}^{\pi^{\prime}}(z_{h},s_{h},\cdot)\right],\pi_{h}^{t}(\cdot\mid z_{h})-\pi_{h}^{*}( \cdot\mid z_{h})\right\rangle\right]-2\cdot H\cdot\sum_{h\in[H]}\mathbb{E}_{ \tau_{h}\sim\pi^{*}}\left[d_{TV}(\bm{b}_{h}(\tau_{h}),\bm{b}_{h}^{\text{apx}}( z_{h}))\right].\]

The last inequality follows by Lemma H.3. By averaging we get,

\[\frac{1}{T}\sum_{t\in[T]}\mathbb{E}_{s_{1}\sim\mu_{1}}[\widetilde {V}_{1}^{\pi^{\prime}}(s_{1})]\] \[\geq\mathbb{E}_{s_{1}\sim\mu_{1}}[V_{1}^{\pi^{*}}(s_{1})]+\frac{1} {T}\sum_{h\in[H]}\mathbb{E}_{\tau_{h}\sim\pi^{*}}\left[\sum_{t\in[T]}\left\langle \mathbb{E}_{s_{h}\sim\bm{b}\bm{\tau}^{\text{ex}}(z_{h})}\left[\widetilde{Q}_{ h}^{\pi^{\prime}}(z_{h},s_{h},\cdot)\right],\pi_{h}^{t}(\cdot\mid z_{h})-\pi_{h}^{*}( \cdot\mid z_{h})\right\rangle\right]\] \[\geq\mathbb{E}_{s_{1}\sim\mu_{1}}[V_{1}^{\pi^{*}}(s_{1})]+\frac{H} {T}\max_{h\in[H]}\mathbb{E}_{\tau_{h}\sim\pi^{*}}\left[\sum_{t\in[T]}\left\langle \mathbb{E}_{s_{h}\sim\bm{b}\bm{\tau}^{\text{ex}}(z_{h})}\left[\widetilde{Q}_{ h}^{\pi^{\prime}}(z_{h},s_{h},\cdot)\right],\pi_{h}^{t}(\cdot\mid z_{h})-\pi_{h}^{*}( \cdot\mid z_{h})\right\rangle\right]\] \[\qquad-2\cdot H^{2}\cdot\max_{h\in[H]}\mathbb{E}_{\tau_{h}\sim\pi^ {*}}\left[d_{TV}(\bm{b}_{h}(\tau_{h}),\bm{b}_{h}^{\text{apx}}(z_{h}))\right]\] \[\geq\mathbb{E}_{s_{1}\sim\mu_{1}}[V_{1}^{\pi^{*}}(s_{1})]-\frac{2 H\sqrt{H\log\left(|\mathcal{A}|\right)}}{\sqrt{T}}-2\cdot H^{2}\cdot\max_{h\in[H]} \mathbb{E}_{\tau_{h}\sim\pi^{*}}\left[d_{TV}(\bm{b}_{h}(\tau_{h}),\bm{b}_{h}^{ \text{apx}}(z_{h}))\right],\]

where the last inequality follows since for fixed \(h\in[H]\) and \(z_{h}\in\mathcal{Z}_{h}\), the agent updates her policy on memory \(z_{h}\) according to MWU on feedback \(\left\{\mathbb{E}_{s_{h}\sim\bm{b}\bm{\tau}^{\text{ex}}(z_{h})}\left[\widetilde{Q}_{ h}^{\pi^{\prime}}(z_{h},s_{h},a)\right]\right\}_{a\in\mathcal{A}}\), and thus, the accumulate regret is bounded by (Section 4.3 in [10]):

\[\sum_{t\in[T]}\left\langle\mathbb{E}_{s_{h}\sim\bm{b}\bm{\tau}^{ \text{ex}}(z_{h})}\left[\widetilde{Q}_{h}^{\pi^{t}}(z_{h},s_{h},\cdot)\right], \pi_{h}^{*}(\cdot\mid z_{h})-\pi_{h}^{t}(\cdot\mid z_{h})\right\rangle\] \[\qquad\leq\frac{\log\left(|\mathcal{A}|\right)}{\eta}+\eta\cdot T \cdot\left\|Q_{h}^{\pi^{t}}\left(z_{h},s_{h},\cdot\right)\right\|_{+\infty}\leq \frac{\log\left(|\mathcal{A}|\right)}{\eta}+\eta\cdot T\cdot H=2\sqrt{T\cdot H \log\left(|\mathcal{A}|\right)}.\]

The proof follows by combining Equation (H.1) and the inequality above. Finally, to achieve the near optimality in the class of \(\Pi^{L}\), we bound the optimistic estimate using Equation (H.2) in Lemma H.3, and its global near-optimality for a large enough \(L\) under \(\gamma\)-observability is a direct consequence of Theorem 4.1 in [26].

**Lemma H.3** (Optimistic \(Q\)-function - adapted from [48]).: Given a policy \(\pi\in\Pi^{L}\), and a parameter \(M\in\mathbb{N}\), let \(\{\widetilde{Q}_{h}^{\pi}:\mathcal{Z}_{h}\times\mathcal{S}\times\mathcal{A} \rightarrow[0,H]\}_{h\in[H]}\) be the output of Algorithm 3. Then, with probability at least \(1-\delta\): \(\forall z_{h}\in\mathcal{Z}_{h},s_{h}\in\mathcal{S},a_{h}\in\mathcal{A}\)

\[H-h+1\geq\widetilde{Q}_{h}^{\pi}(z_{h},s_{h},a_{h})\geq\mathbb{E }_{\begin{subarray}{c}s_{h+1}\sim\mathbb{T}_{h}(\cdot\mid s_{h},a_{h}),\\ o_{h+1}\sim\partial_{h+1}(\cdot\mid s_{h+1})\end{subarray}}\left[r_{h}(s_{h}, a_{h})+\widetilde{V}_{h+1}^{\pi}(z_{h+1},s_{h+1})\right],\] \[\mathbb{E}_{s_{1}\sim\mu_{1}}[\widetilde{V}_{1}^{\pi}(s_{1})]- \mathbb{E}_{s_{1}\sim\mu_{1}}[V_{1}^{\pi}(s_{1})]\leq O\left(H^{2}\cdot\sqrt{ \frac{\max(O,S)\cdot S\cdot A}{M}}\cdot\log\left(\frac{S\cdot A}{\delta} \right)\log\left(\frac{M\cdot S\cdot A\cdot H}{\delta}\right)\right),\] (H.2)

where \(V_{1}^{\pi}(s_{1})=\mathbb{E}_{o_{1}\sim\mathbb{O}_{1}(\cdot\mid s_{1}),a_{1} \sim\pi_{1}(\cdot\mid o_{1})}[Q_{h}^{\pi}(z_{1}=(o_{1}),s_{1},a_{1})]\), \(\widetilde{V}_{1}^{\pi}(s_{1})=\mathbb{E}_{o_{1}\sim\mathbb{O}_{1}(\cdot\mid s _{1}),a_{1}\sim\pi_{1}(\cdot\mid o_{1})}[\widetilde{Q}_{h}^{\pi}(z_{1}=(o_{1} ),s_{1},a_{1})]\) and \(\widetilde{V}_{h}^{\pi}(z_{h},s_{h})=\mathbb{E}_{a_{h}\sim\pi_{h}(\cdot\mid z _{h})}[\widetilde{Q}_{h}^{\pi}(z_{h},s_{h},a_{h})]\). Moreover, Algorithm 3 needs a total of \(H\cdot M\) episodes from POMDP \(\mathcal{P}\) and runs in time \(\textsc{poly}(H,M,S,A^{L},O^{L})\).

Proof.: For each step \(h\in[H]\), collect \(M\) trajectories with states using policy \(\pi\) on POMDP \(\mathcal{P}\) and let \(D_{h}=\{\overline{\tau}^{(i)}\}_{i\in[M]}\) be those collected trajectories. Define the empirical transition, observation and reward distribution as follows:

\[N_{h}(s_{h},a_{h},s_{h+1})= \big{|}\{\overline{\tau}=(s_{1}^{\prime},o_{1}^{\prime},a_{1}^{ \prime},r_{1}^{\prime}\ldots,s_{h}^{\prime},o_{h}^{\prime},a_{h}^{\prime},r_{h }^{\prime})\in D_{h}\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad:(s_{h},a_ {h},s_{h+1})=(s_{h}^{\prime},a_{h}^{\prime},s_{h+1}^{\prime})\}\big{|},\] \[N_{h}(s_{h},a_{h})= \sum_{s_{h+1}\in\mathcal{S}}N_{h}(s_{h},a_{h},s_{h+1}),\] \[N_{h}(s_{h})= \sum_{a_{h}\in\mathcal{A}}N_{h}(s_{h},a_{h}),\] \[N_{h}(s_{h},a_{h})= \big{|}\{\overline{\tau}=(s_{1}^{\prime},o_{1}^{\prime},a_{1}^{ \prime},r_{1}^{\prime}\ldots,s_{h}^{\prime},o_{h}^{\prime},a_{h}^{\prime},r_{h }^{\prime})\in D_{h}:(s_{h},o_{h})=(s_{h}^{\prime},o_{h}^{\prime})\}\big{|},\] \[\widehat{\mathbb{T}}_{h}(s_{h+1}\mid s_{h},a_{h})= \frac{N_{h}(s_{h},a_{h},s_{h+1})}{N_{h}(s_{h},a_{h})},\] \[\widehat{\mathbb{O}}_{h}(o_{h}\mid s_{h})= \frac{N_{h}(s_{h},o_{h})}{N_{h}(s_{h})}.\]

Set \(\delta_{1}=\frac{\delta}{2\cdot S\cdot(A+1)}\). By [13], there exists a constant \(C>0\) such that for each step \(h\in[H]\), state \(s\in\mathcal{S}\) and action \(a\in\mathcal{A}\) with probability at least \(1-\delta_{1}\):

\[\|\mathbb{T}_{h}(\cdot\mid s_{h},a_{h})-\widehat{\mathbb{T}}_{h}( \cdot\mid s_{h},a_{h})\|_{1}\leq \min\left(2,C\cdot\sqrt{\frac{S\log(1/\delta_{1})}{\max(N_{h}(s_{h},a_{h}),1)}}\right),\] \[\|\mathbb{O}_{h}(\cdot\mid s_{h})-\widehat{\mathbb{O}}_{h}(\cdot \mid s_{h})\|_{1}\leq \min\left(2,C\cdot\sqrt{\frac{O\log(1/\delta_{1})}{\max(N_{h}(s_{h} ),1)}}\right).\]

For the rest of the proof, we condition on this event. By union bound, this happens with probability at least \(1-\frac{\delta}{2}\). We define the optimistic \(Q\)-function recursively as follows for a memory-state pair \((z_{h},s_{h})\in\mathcal{Z}_{h}\times\mathcal{S}\):

\[\widetilde{Q}_{H+1}^{\pi}(z_{H+1},s_{H+1},\cdot)=0,\qquad\qquad \forall z_{H+1}\in\mathcal{Z}_{H+1},s_{H+1}\in\mathcal{S}\] \[\widetilde{Q}_{h}^{\pi}(z_{h},s_{h},a_{h})= \min\left(H-h+1,\mathbb{E}_{\begin{subarray}{c}s_{h+1}\sim\widehat{ \mathbb{T}}_{h}(\cdot\mid s_{h},a_{h}),\\ o_{h+1}\sim\widehat{\mathbb{O}}_{h+1}(\cdot\mid s_{h+1})\end{subarray}}[ \widetilde{V}_{h+1}^{\pi}(z_{h+1},s_{h+1})]+r(s_{h},a_{h})\right.\] \[+H\cdot\min\left(2,C\cdot\sqrt{\frac{S\log(1/\delta_{1})}{\max(N_{h }(s_{h},a_{h}),1)}}\right)\] \[+\mathbb{E}_{s_{h+1}\sim\widehat{\mathbb{T}}_{h}(\cdot\mid s_{h},a _{h})}H\cdot\min\left(2,C\cdot\sqrt{\frac{O\log(1/\delta_{1})}{\max(N_{h+1}(s_{h+ 1}),1)}}\right)\Bigg{)},\]where \(\widetilde{V}_{h+1}^{\pi}(z_{h+1},s_{h+1})=\mathbb{E}_{a_{h+1}\sim\mathbb{T}_{h}( \cdot\mid s_{h},a_{h})}[\widetilde{V}_{h+1}^{\pi}(z_{h+1},s_{h+1})]\widetilde{Q} _{h+1}^{\pi}(z_{h+1},s_{h+1},a_{h+1})]\). Hence the time complexity of our algorithm is \(\textsc{poly}(H,M,S,A^{L},O^{L})\). To prove the first condition, we fix step \(h\in[H],z_{h}\in\mathcal{Z}_{h},a_{h}\in\mathcal{A}\) and state \(s_{h}\in\mathcal{S}\) and consider the case where \(\widetilde{Q}_{h}^{\pi}(z_{h},s_{h},a_{h})=H-h+1\). In this case, since by assumption on the POMDP \(\mathcal{P}\), \(r_{h}(s_{h},a_{h})\leq 1\), and by definition of \(\widetilde{Q}_{h+1}^{\pi}(\cdot,\cdot,\cdot)\leq H-h\), we have:

\[\widetilde{Q}_{h}^{\pi}(z_{h},s_{h},a_{h})=1+H-h\geq\mathbb{E}_{ \begin{subarray}{c}s_{h+1}\sim\mathbb{T}_{h}(\cdot\mid s_{h},a_{h}),\\ o_{h+1}\sim\mathbb{O}_{h+1}(\cdot\mid s_{h+1})\end{subarray}}[r_{h}(s_{h},a_ {h})+\widetilde{V}_{h+1}^{\pi}(z_{h+1},s_{h+1})].\]

If \(\widetilde{Q}_{h}^{\pi}(z_{h},s_{h},a_{h})\neq H-h+1\), observe that:

\[\widetilde{Q}_{h}^{\pi}(z_{h},s_{h},a_{h})= \mathbb{E}_{\begin{subarray}{c}s_{h+1}\sim\widetilde{\mathbb{T}}_ {h}(\cdot\mid s_{h},a_{h}),\\ o_{h+1}\sim\widetilde{\mathbb{O}}_{h+1}(\cdot\mid s_{h+1})\end{subarray}}[ \widetilde{V}_{h+1}^{\pi}(z_{h+1},s_{h+1})]+r(s_{h},a_{h})+H\cdot\min\left(2,C \cdot\sqrt{\frac{S\log(1/\delta_{1})}{\max(N_{h}(s_{h},a_{h}),1)}}\right)\] \[\quad+\mathbb{E}_{\begin{subarray}{c}s_{h+1}\sim\mathbb{T}_{h}( \cdot\mid s_{h},a_{h})\\ o_{h+1}\sim\mathbb{O}_{h+1}(\cdot\mid s_{h+1})\end{subarray}}H\cdot\min\left(2, C\cdot\sqrt{\frac{O\log(1/\delta_{1})}{\max(N_{h+1}(s_{h+1}),1)}}\right)\] \[\geq \mathbb{E}_{\begin{subarray}{c}s_{h+1}\sim\mathbb{T}_{h}(\cdot \mid s_{h},a_{h}),\\ o_{h+1}\sim\mathbb{O}_{h+1}(\cdot\mid s_{h+1})\end{subarray}}[r_{h}(s_{h},a_ {h})+\widetilde{V}_{h+1}^{\pi}(z_{h+1},s_{h+1})],\]

and hence, \(\{\widetilde{Q}_{h}^{\pi}\}_{h\in[H]}\) satisfies the first condition. Moreover, it holds that:

\[\widetilde{Q}_{h}^{\pi}(z_{h},s_{h},a_{h})\leq \mathbb{E}_{\begin{subarray}{c}s_{h+1}\sim\mathbb{T}_{h}(\cdot \mid s_{h},a_{h}),\\ o_{h+1}\sim\mathbb{O}_{h+1}(\cdot\mid s_{h+1})\end{subarray}}[r_{h}(s_{h},a_ {h})+\widetilde{V}_{h+1}^{\pi}(z_{h+1},s_{h+1})]+2H\cdot\min\left(2,C\cdot\sqrt{ \frac{S\log(1/\delta_{1})}{\max(N_{h}(s_{h},a_{h}),1)}}\right)\] \[\quad+2\cdot\mathbb{E}_{\begin{subarray}{c}s_{h+1}\sim\mathbb{T}_ {h}(\cdot\mid s_{h},a_{h}),\\ o_{h+1}\sim\mathbb{O}_{h+1}(\cdot\mid s_{h+1})\end{subarray}}H\cdot\min\left(2, C\cdot\sqrt{\frac{O\log(1/\delta_{1})}{\max(N_{h+1}(s_{h+1}),1)}}\right)\] \[\leq \mathbb{E}_{\begin{subarray}{c}s_{h+1}\sim\mathbb{T}_{h}(\cdot \mid s_{h},a_{h}),\\ o_{h+1}\sim\mathbb{O}_{h+1}(\cdot\mid s_{h+1})\end{subarray}}[r_{h}(s_{h},a_ {h})+\widetilde{V}_{h+1}^{\pi}(z_{h+1},s_{h+1})]+6H\cdot\min\left(2,C\cdot\sqrt{ \frac{S\log(1/\delta_{1})}{\max(N_{h}(s_{h},a_{h}),1)}}\right)\] \[\quad+2\cdot\mathbb{E}_{s_{h+1}\sim\mathbb{T}_{h}(\cdot\mid s_{h},a_{h})}H\cdot\min\left(2,C\cdot\sqrt{\frac{O\log(1/\delta_{1})}{\max(N_{h+1}(s _{h+1}),1)}}\right).\]

Thus, it holds that:

\[\leq \mathbb{E}_{\begin{subarray}{c}a_{h}\sim\pi_{h}(\cdot\mid z_{h} ),\\ s_{h+1}\sim\mathbb{T}_{h}(\cdot\mid s_{h},a_{h}),\\ o_{h+1}\sim\mathbb{O}_{h+1}(\cdot\mid s_{h},a_{h})\end{subarray}}[\widetilde{V }_{h+1}^{\pi}(z_{h+1},s_{h+1})-V_{h+1}^{\pi}(z_{h+1},s_{h+1})]+6\cdot C\cdot H \cdot\mathbb{E}_{a_{h}\sim\pi_{h}(\cdot\mid z_{h})}\sqrt{\frac{S\log(1/\delta_{1 })}{\max(N_{h}(s_{h},a_{h}),1)}}\] \[\quad+2\cdot C\cdot H\cdot\mathbb{E}_{\begin{subarray}{c}a_{h}\sim \pi_{h}(\cdot\mid z_{h}),\\ s_{h+1}\sim\mathbb{T}_{h}(\cdot\mid s_{h},a_{h})\end{subarray}}\sqrt{\frac{O\log(1/ \delta_{1})}{\max(N_{h+1}(s_{h+1}),1)}}.\]

Thus, we conclude that

\[\mathbb{E}_{s_{1}\sim\mu_{1}}[\widetilde{V}_{1}^{\pi}(s_{1})]-\mathbb{E}_{s_{1} \sim\mu_{1}}[V_{1}^{\pi}(s_{1})]\leq\mathbb{E}_{\tau=(s_{1},a_{1},\ldots,s_{H+1}) \sim\pi}\left[\sum_{h\in[H]}8\cdot H\cdot C\cdot\sqrt{\frac{\max(S,O)\log(1/ \delta_{1})}{\max(N_{h}(s_{h},a_{h}),1)}}\right]\]

\[=8\cdot H\sqrt{\max(O,S)\cdot\log(1/\delta_{1})}\cdot C\cdot\sum_{h\in[H]}\mathbb{E}_ {\tau=(s_{1},a_{1},\ldots,s_{H+1})\sim\pi}\left[\sqrt{\frac{1}{\max(N_{h}(s_{h},a_ {h}),1)}}\right].\]

To finish the proof, we make use of the following lemma.

**Lemma H.4** (Lemma 6 in [48]).: For each step \(h\in[H]\), and state-action pair \((s_{h},a_{h})\in\mathcal{S}\times\mathcal{A}\), with probability at least \(1-\delta_{2}\):

\[\sqrt{\frac{1}{\max(N_{h}(s_{h},a_{h}),1)}}=O\left(\sqrt{\frac{S\cdot A\log(M/ \delta_{2})}{M}}\right).\]By setting \(\delta_{2}=\frac{\delta}{2\cdot S\cdot A\cdot H}\), and taking union bound we have that with probability at least \(1-\delta\):

\[\mathbb{E}_{s_{1}\sim\mu_{1}}[\widehat{V}_{1}^{\pi}(s_{1})]- \mathbb{E}_{s_{1}\sim\mu_{1}}[V_{1}^{\pi}(s_{1})]\] \[\quad=8\sqrt{\max(O,S)\cdot\log(1/\delta_{1})}\cdot C\cdot\sum_{h \in[H]}\mathbb{E}_{\pi=(s_{1},a_{1},\ldots,s_{H+1})\sim\pi}\left[\sqrt{\frac{1 }{\max(N_{h}(s_{h},a_{h}),1)}}\right]\] \[\quad\leq O\left(H^{2}\cdot\sqrt{\frac{\max(O,S)\cdot S\cdot A}{M }\cdot\log\left(\frac{S\cdot A}{\delta}\right)\log\left(\frac{M\cdot S\cdot A \cdot H}{\delta}\right)}\right).\]

**Proof of Theorem 5.2:** The proof of Theorem 5.2 follows by combining Theorem H.5 and Theorem H.6 below. Theorem H.5 proves that we can approximately learn a POMDP model \(\mathcal{P}\) computationally and sample efficiently, thanks to the privileged information.

**Theorem H.5**.: Fix any \(\epsilon,\delta\in(0,1)\). Algorithm 4 can learn the approximate POMDP model with transition \(\widehat{\mathbb{T}}_{1:H}\) and emission \(\widehat{\mathbb{O}}_{1:H}\) such that with probability at least \(1-\delta\), for any policy \(\pi\in\Pi^{\text{gen}}\) and \(h\in[H]\)

\[\mathbb{E}_{\pi}^{\mathcal{P}}\left[\|\mathbb{T}_{h}(\cdot\,|\,s_{h},a_{h})- \widehat{\mathbb{T}}_{h}(\cdot\,|\,s_{h},a_{h})\|_{1}+\|\mathcal{O}_{h}(\cdot \,|\,s_{h})-\widehat{\mathbb{O}}_{h}(\cdot\,|\,s_{h})\|_{1}\right]\leq O( \epsilon),\]

using \(\textsc{poly}(S,A,H,O,\frac{1}{\epsilon},\log(\frac{1}{\delta}))\) episodes in time \(\textsc{poly}(S,A,H,O,\frac{1}{\epsilon},\log(\frac{1}{\delta}))\).

Proof.: Note that by Lemma H.11, it suffices to consider only \(\pi\in\Pi_{\mathcal{S}}\) as the optimal value for policies in \(\Pi^{\text{gen}}\) can be achieved by those in \(\Pi_{\mathcal{S}}\) (by considering \(r_{h}(s_{h},a_{h}):=\|\mathbb{T}_{h}(\cdot\,|\,s_{h},a_{h})-\widehat{\mathbb{T }}_{h}(\cdot\,|\,s_{h},a_{h})\|_{1}+\|\mathcal{O}_{h}(\cdot\,|\,s_{h})- \widehat{\mathbb{O}}_{h}(\cdot\,|\,s_{h})\|_{1}\)). For each \(h\in[H]\) and \(s_{h}\in\mathcal{S}\), we define

\[p_{h}(s_{h})=\max_{\pi\in\Pi_{\mathcal{S}}}d_{h}^{\pi}(s_{h}).\]

Fix \(\epsilon_{1}>0\), we define \(\mathcal{U}(h,\epsilon_{1})=\{s_{h}\in\mathcal{S}\,|\,p_{h}(s_{h})\geq\epsilon _{1}\}\). By the guarantee of the EULER algorithm from [89, 37], one can learn a policy \(\Psi(h,s_{h})\) with sample complexity \(\widetilde{\mathcal{O}}(\frac{S^{2}AH^{4}}{\epsilon_{1}})\) such that \(d_{h}^{\Psi(h,s_{h})}(s_{h})\geq\frac{p_{h}(s_{h})}{2}\) for each \(s_{h}\in\mathcal{U}(h,\epsilon_{1})\) with probability \(1-\delta_{1}\). Now we assume this event holds for any \(h\in[H]\) and \(s_{h}\in\mathcal{U}(h,\epsilon_{1})\). For each \(s_{h}\in\mathcal{S}\) and \(a_{h}\in\mathcal{A}\), we have executed in Algorithm 4 the policy \(\Psi(h,s_{h})\) followed by an action \(a_{h}\in\mathcal{A}\) for \(N\) episodes, and denote the number of episodes that \(s_{h}\) and \(a_{h}\) are visited as \(N_{h}(s_{h},a_{h})\). Then with probability \(1-e^{N\epsilon_{1}/8}\), \(N_{h}(s_{h},a_{h})\geq\frac{Np_{h}(s_{h})}{2}\) by Chernoff bound. Now conditioned on this event, we are ready to evaluate the following: for any \(\pi\in\Pi_{\mathcal{S}}\)

\[\frac{1}{2}\cdot\mathbb{E}_{\pi}^{\mathcal{P}}\|\mathbb{T}_{h}( \cdot\,|\,s_{h},a_{h})-\widehat{\mathbb{T}}_{h}(\cdot\,|\,s_{h},a_{h})\|_{1}= \frac{1}{2}\sum_{s_{h},a_{h}}d_{h}^{\pi}(s_{h})\pi_{h}(a_{h}\,|\,s_{h})\| \mathbb{T}_{h}(\cdot\,|\,s_{h},a_{h})-\widehat{\mathbb{T}}_{h}(\cdot\,|\,s_{h},a_{h})\|_{1}\] \[\quad\leq S\epsilon_{1}+\frac{1}{2}\sum_{s_{h}\in\mathcal{U}(h, \epsilon_{1}),a_{h}}d_{h}^{\pi}(s_{h})\pi_{h}(a_{h}\,|\,s_{h})\sqrt{\frac{2S \log(1/\delta_{2})}{Np_{h}(s_{h})}}\] \[\quad\leq S\epsilon_{1}+\sum_{s_{h}\in\mathcal{U}(h,\epsilon_{1} ),a_{h}}d_{h}^{\Psi(h,s_{h})}(s_{h})\pi_{h}(a_{h}\,|\,s_{h})\sqrt{\frac{2S\log( 1/\delta_{2})}{Np_{h}(s_{h})}}\] \[\quad\leq S\epsilon_{1}+\sum_{s_{h}}\sqrt{d_{h}^{\Psi(h,s_{h})}(s _{h})}\sqrt{\frac{2S\log(1/\delta_{2})}{N}}\] \[\quad\leq S\epsilon_{1}+S\sqrt{\frac{2\log(1/\delta_{2})}{N}},\]where the first inequality follows by [13] with probability at least \(1-\delta_{2}\), and the second inequality uses \(d_{h}^{\Psi(h,s_{h})}(s_{h})\geq\frac{p_{h}(s_{h})}{2}\) for all \(s_{h}\in\mathcal{U}(h,\epsilon_{1})\). Similarly, for any \(\pi\in\Pi_{\mathcal{S}}\)

\[\frac{1}{2}\cdot \mathbb{E}_{\pi}^{\mathcal{P}}\|\mathbb{O}_{h}(\cdot\,|\,s_{h})- \widehat{\mathbb{O}}_{h}(\cdot\,|\,s_{h})\|_{1}\leq S\epsilon_{1}+\frac{1}{2} \sum_{s_{h}\in\mathcal{U}(h,\epsilon_{1})}d_{h}^{\pi}(s_{h})\sqrt{\frac{2O\log (1/\delta_{2})}{Np_{h}(s_{h})}}\] \[\leq S\epsilon_{1}+\sum_{s_{h}\in\mathcal{U}(h,\epsilon_{1})}d_{h }^{\Psi(h,s_{h})}(s_{h})\sqrt{\frac{2O\log(1/\delta_{2})}{Np_{h}(s_{h})}}\leq S \epsilon_{1}+\sum_{s_{h}\in\mathcal{U}(h,\epsilon_{1})}\sqrt{d_{h}^{\Psi(h,s _{h})}(s_{h})}\sqrt{\frac{2O\log(1/\delta_{2})}{N}}\] \[\leq S\epsilon_{1}+\sqrt{\frac{2SO\log(1/\delta_{2})}{N}},\]

where similarly the first inequality follows by [13] with probability at least \(1-\delta_{2}\), and the second inequality uses \(d_{h}^{\Psi(h,s_{h})}(s_{h})\geq\frac{p_{h}(s_{h})}{2}\) for all \(s_{h}\in\mathcal{U}(h,\epsilon_{1})\). Therefore, by a union bound, all the high probability events above hold with probability

\[1-SH\delta_{1}-SHAe^{-N\epsilon_{1}/8}-2SAH\delta_{2}.\]

Therefore, we can choose \(N=\widetilde{\Theta}(\frac{S^{2}+SO}{\epsilon^{2}})\) and \(\epsilon_{1}=\Theta(\frac{\epsilon}{5})\), leading to the total sample complexity

\[SHA\left(N+\widetilde{\Theta}\left(\frac{S^{3}AH^{4}}{\epsilon} \right)\right)=\widetilde{\Theta}\left(\frac{S^{2}AHO+S^{3}AH}{\epsilon^{2}}+ \frac{S^{4}A^{2}H^{5}}{\epsilon}\right),\] (H.3)

which completes the proof. 

Now with such a model learned in a reward-free way, we are ready to present our main result for approximate belief learning.

**Theorem H.6**.: Consider a \(\gamma\)-observable POMDP \(\mathcal{P}\) (c.f. Assumption 2.2), an \(\epsilon>0\), approximate transition and emission \(\{\widehat{\mathbb{T}}_{h}\}_{h\in[H]}\) and \(\{\widehat{\mathbb{O}}_{h}\}_{h\in[H]}\) learned from Algorithm 4 that ensure that for any \(\pi\in\Pi^{\text{gen}}\) and \(h\in[H]\):

\[\mathbb{E}_{\pi}^{\mathcal{P}}\left[\|\mathbb{T}_{h}(\cdot\,|\,s_ {h},a_{h})-\widehat{\mathbb{T}}_{h}(\cdot\,|\,s_{h},a_{h})\|_{1}+\|\mathbb{O} _{h}(\cdot\,|\,s_{h})-\widehat{\mathbb{O}}_{h}(\cdot\,|\,s_{h})\|_{1}\right] \leq\mathcal{O}\left(\frac{\epsilon}{H}\right).\]

Then we can construct in time \(\textsc{poly}(H,A,S,O,\frac{1}{\epsilon},\log\frac{1}{\delta})\) a belief \(\{\bm{b}_{h}^{\text{apx}}:\mathcal{Z}_{h}\to\Delta(\mathcal{S})\}_{h\in[H]}\) with no further samples. In addition, if the parameter in Algorithm 4 satisfies \(N=\widetilde{\Theta}(\frac{O\log(SH/\delta)}{\gamma^{2}\epsilon_{1}})\) and our class of finite memory policies \(\Pi^{L}\) satisfies \(L\geq\widetilde{\Omega}(\gamma^{-4}\log(S/\epsilon)\), then for any \(\pi\in\Pi^{\text{gen}}\) and \(h\in[H]\):

\[\mathbb{E}_{\pi}^{\mathcal{P}}\|\bm{b}_{h}(\tau_{h})-\bm{b}_{h}^ {\text{apx}}(z_{h})\|_{1}\leq\mathcal{O}(\epsilon).\]

Proof.: We firstly consider the following simple while important fact: for the estimated emission \(\widehat{\mathbb{O}}_{h}\), its observability can be evaluated as

\[\|\widehat{\mathbb{O}}_{h}^{\top}(b-b^{\prime})\|_{1}\geq\|\mathbb{O}_{h}^{ \top}(b-b^{\prime})\|_{1}-\|(\mathbb{O}_{h}^{\top}-\widehat{\mathbb{O}}_{h}^{ \top})(b-b^{\prime})\|_{1}\geq(\gamma-\|\widehat{\mathbb{O}}_{h}-\mathbb{O}_{h }\|_{\infty})\|b-b^{\prime}\|_{1},\]

for any \(b,b^{\prime}\in\Delta(\mathcal{S})\) and \(\|\widehat{\mathbb{O}}_{h}-\mathbb{O}_{h}\|_{\infty}:=\max_{s_{h}\in\mathcal{S }}\|\mathbb{O}_{h}(\cdot\,|\,s_{h})-\widehat{\mathbb{O}}_{h}(\cdot\,|\,s_{h}) \|_{1}\). Therefore, if one can ensure that the emission at _any_ state \(s_{h}\) is learned accurately in the sense that \(\|\mathbb{O}_{h}(\cdot\,|\,s_{h})-\widehat{\mathbb{O}}_{h}(\cdot\,|\,s_{h})\|_{1} \leq\frac{\gamma}{2}\), we can conclude that \(\widehat{\mathbb{O}}_{h}\) is also \(\gamma/2\)-observable. However, the key challenge here is that there could exist some states \(s_{h}\) that can only be visited _with a low probability no matter what exploration policy is used_. Therefore, emissions at such states may not be learned accurately. To address this issue, our key technique is to _redirect_ the transition probability into states that cannot be explored sufficiently to some highly visited states, in a new POMDP that is close to the original one in generating the beliefs.

Specifically, first, for any \(\epsilon_{1}>0\), we define

\[\mathcal{S}_{h}^{\text{low}}:=\left\{s_{h}\in\mathcal{S}\left|\, \frac{N_{h}(s_{h})}{N}<\frac{\epsilon_{1}}{2}\right.\right\},\quad\mathcal{S}_{ h}^{\text{high}}:=\mathcal{S}\setminus\mathcal{S}_{h}^{\text{low}}.\]By Chernoff bound, with probability at least \(1-Se^{-Ne_{1}/8}\), it holds that

\[\mathcal{S}_{h}^{\text{low}}\subseteq\{s_{h}\in\mathcal{S}\,|\,p_{h}(s_{h})< \epsilon_{1}\},\]

where \(p_{h}(s_{h}):=\max_{\pi\in\mathbb{I}_{S}}d_{h}^{\pi}(s_{h})\). To see the reason, we notice that for any \(s_{h}\) such that \(p_{h}(s_{h})\geq\epsilon_{1}\), with probability \(1-e^{-Ne_{1}/8}\), it holds that \(\frac{N_{h}(s_{h})}{N}\geq\frac{\epsilon_{1}}{2}\). Therefore, the last step is by taking a union bound for all \(s_{h}\). Now with \(\mathcal{S}_{h}^{\text{low}}\) defined, we are ready to construct a truncated POMDP \(\mathcal{P}^{\text{trunc}}\) such that for each \(h\in[H]\), we define the transition as

\[\mathbb{T}_{h}^{\text{trunc}}(s_{h+1}\,|\,s_{h},a_{h}) :=\mathbb{T}_{h}(s_{h+1}\,|\,s_{h},a_{h})+\frac{\sum_{s^{\prime}_ {h+1}\in\mathcal{S}_{h+1}^{\text{low}}}\mathbb{T}_{h}(s^{\prime}_{h+1}\,|\,s_ {h},a_{h})}{|\mathcal{S}_{h+1}^{\text{high}}|}, \forall s_{h}\in\mathcal{S},s_{h+1}\in\mathcal{S}_{h+1}^{\text{high}},a_{h} \in\mathcal{A},\] \[\mathbb{T}_{h}^{\text{trunc}}(s_{h+1}\,|\,s_{h},a_{h}) :=0, \forall s_{h}\in\mathcal{S},s_{h+1}\in\mathcal{S}_{h+1}^{\text{ low}},a_{h}\in\mathcal{A}.\]

Meanwhile, for the initial state distribution, we define

\[\mu_{1}^{\text{trunc}}(s_{1}) :=\mu_{1}(s_{1})+\frac{\sum_{s^{\prime}_{1}\in\mathcal{S}_{1}^{ \text{low}}}\mu_{1}(s^{\prime}_{1})}{|\mathcal{S}_{1}^{\text{high}}|}, \forall s_{1}\in\mathcal{S}_{1}^{\text{high}},\] \[\mu_{1}^{\text{trunc}}(s_{1}) :=0, \forall s_{1}\in\mathcal{S}_{1}^{\text{low}}.\]

For emission, we simply define

\[\mathbb{Q}_{h}^{\text{trunc}}(o_{h}\,|\,s_{h}):=\mathbb{O}_{h}(o_{h}\,|\,s_{h} ), \forall h\in[H],s_{h}\in\mathcal{S},o_{h}\in\mathcal{O}_{h}.\]

Finally, we define the rewards for \(\mathcal{P}^{\text{trunc}}\) arbitrarily. Now we examine the total variation distance between the trajectory distributions in \(\mathcal{P}\) and \(\mathcal{P}^{\text{trunc}}\). For any policy \(\pi\in\Pi^{\text{gen}}\), it is easy to see that

\[\mathbb{P}^{\pi,\mathcal{P}}(\overline{\tau}_{h})\leq\mathbb{P}^{\pi,\mathcal{ P}^{\text{trunc}}}(\overline{\tau}_{h}),\]

for any \(\overline{\tau}_{h}\in\overline{\mathcal{T}}_{h}^{\text{high}}:=\{\left(s_{1: h},o_{1:h},a_{1:h-1}\right)|s^{\prime}_{h}\in\mathcal{S}_{h}^{\text{high}}, \forall h^{\prime}\in[h]\}\), since some rarely visited states' probability has been redirected to the highly visited ones in \(\mathcal{P}^{\text{trunc}}\). Meanwhile, it holds by a union bound that for any \(h\in[H]\)

\[\mathbb{P}^{\pi,\mathcal{P}}(\overline{\tau}_{h}\not\in\overline{\mathcal{T} }_{h}^{\text{high}})\leq\sum_{h^{\prime}\in[h]}\mathbb{P}^{\pi,\mathcal{P}}(s _{h^{\prime}}\in\mathcal{S}_{h^{\prime}}^{\text{low}})\leq HS\epsilon_{1}.\]

Therefore, by noticing that \(\mathbb{P}^{\pi,\mathcal{P}^{\text{trunc}}}(\overline{\tau}_{h})=0\) for any \(\overline{\tau}_{h}\not\in\overline{\mathcal{T}}_{h}^{\text{high}}\) and \(h\in[H]\), the total variation distance between the trajectory distributions in \(\mathcal{P}\) and \(\mathcal{P}^{\text{trunc}}\) can be bounded by

\[\sum_{\overline{\tau}_{h}}|\mathbb{P}^{\pi,\mathcal{P}}(\overline{\tau}_{h})- \mathbb{P}^{\pi,\mathcal{P}^{\text{trunc}}}(\overline{\tau}_{h})|\leq 2HS \epsilon_{1}.\] (H.4)

On the other hand, by Equation (H.9) of Lemma H.9, we have

\[\mathbb{E}_{\pi}^{\mathcal{P}}[\|\bm{b}_{h}(\tau_{h})-\bm{b}_{h}^{\text{trunc}} (\tau_{h})\|_{1}]\leq 2\sum_{\overline{\tau}_{h}}|\mathbb{P}^{\pi,\mathcal{P}}( \overline{\tau}_{h})-\mathbb{P}^{\pi,\mathcal{P}^{\text{trunc}}}(\overline{\tau} _{h})|\leq 4HS\epsilon_{1}.\] (H.5)

With such an intermediate quantity \(\mathcal{P}^{\text{trunc}}\), we define the transition of its approximate version \(\widehat{\mathcal{P}}^{\text{trunc}}\) as follows: we define the transition as

\[\widehat{\mathbb{T}}_{h}^{\text{trunc}}(s_{h+1}\,|\,s_{h},a_{h}) :=\widehat{\mathbb{T}}_{h}(s_{h+1}\,|\,s_{h},a_{h})+\frac{\sum_{s^{ \prime}_{h+1}\in\mathcal{S}_{h+1}^{\text{low}}}\widehat{\mathbb{T}}_{h}(s^{ \prime}_{h+1}\,|\,s_{h},a_{h})}{|\mathcal{S}_{h+1}^{\text{high}}|}, \forall s_{h}\in\mathcal{S},s_{h+1}\in\mathcal{S}_{h+1}^{\text{high}},a_{h}\in \mathcal{A},\] \[\widehat{\mathbb{T}}_{h}^{\text{trunc}}(s_{h+1}\,|\,s_{h},a_{h}) :=0, \forall s_{h}\in\mathcal{S},s_{h+1}\in\mathcal{S}_{h+1}^{\text{low}},a_{h}\in\mathcal{A}.\]

Meanwhile, for the initial state distribution, we define

\[\widehat{\mu}_{1}^{\text{trunc}}(s_{1}) :=\widehat{\mu}_{1}(s_{1})+\frac{\sum_{s^{\prime}_{1}\in\mathcal{ S}_{1}^{\text{low}}}\widehat{\mu}_{1}(s^{\prime}_{1})}{|\mathcal{S}_{1}^{\text{high}}|}, \forall s_{1}\in\mathcal{S}_{1}^{\text{high}},\] \[\widehat{\mu}_{1}^{\text{trunc}}(s_{1}) :=0, \forall s_{1}\in\mathcal{S}_{1}^{\text{low}}.\]For emission, we define

\[\widehat{\mathbb{O}}_{h}^{\text{trunc}}(o_{h}\,|\,s_{h}):=\widehat{\mathbb{O}}_{h }(o_{h}\,|\,s_{h}),\quad\forall h\in[H],s_{h}\in\mathcal{S},o_{h}\in\mathcal{O} _{h}.\]

Now we define \(\widehat{\mathbb{O}}_{h}^{\text{sub}}\in\mathbb{R}^{|\mathcal{S}_{h}^{\text{ sub}}|\times O}\) to be the sub-matrix of \(\widehat{\mathbb{O}}_{h}^{\text{trunc}}\), where we only keep those rows of \(\widehat{\mathbb{O}}_{h}^{\text{trunc}}\) that correspond to the states in \(\mathcal{S}_{h}^{\text{high}}\). Similarly, we define \(\mathbb{O}_{h}^{\text{sub}}\in\mathbb{R}^{|\mathcal{S}_{h}^{\text{high}}| \times O}\) to be the sub-matrix of \(\mathbb{O}_{h}\), where we only keep those rows of \(\mathbb{O}_{h}\) that correspond to the states in \(\mathcal{S}_{h}^{\text{high}}\). It is direct to see that \(\mathbb{O}^{\text{sub}}\) is still \(\gamma\)-observable. Meanwhile, we notice that

\[\|\widehat{\mathbb{O}}_{h}^{\text{sub}}-\mathbb{O}_{h}^{\text{sub}}\|_{\infty }=\max_{s_{h}\in\mathcal{S}_{h}^{\text{sub}}}\|\mathbb{O}_{h}(\cdot\,|\,s_{h} )-\widehat{\mathbb{O}}_{h}(\cdot\,|\,s_{h})\|_{1}\leq\max_{s_{h}\in\mathcal{S }_{h}^{\text{sub}}}\sqrt{\frac{O\log(SH/\delta)}{N_{h}(s_{h})}}\leq\max_{s_{h }\in\mathcal{S}_{h}^{\text{sub}}}\sqrt{\frac{2O\log(SH/\delta)}{N\epsilon_{1} }},\]

where the first inequality is by Lemma I.9, and the second inequality is by the definition of \(\mathcal{S}_{h}^{\text{high}}\). Therefore, if we take

\[N\geq\frac{8O\log(SH/\delta)}{\gamma^{2}\epsilon_{1}},\]

it is guaranteed that \(\|\widehat{\mathbb{O}}_{h}^{\text{sub}}-\mathbb{O}_{h}^{\text{sub}}\|_{\infty }\leq\frac{\gamma}{2}\). Therefore, we conclude that \(\widehat{\mathbb{O}}_{h}^{\text{sub}}\) is also \(\gamma/2\)-observable.

Now we are ready to examine \(\widehat{\boldsymbol{b}}_{h}^{\text{trunc}}\). We firstly define the following POMDP \(\widehat{\mathcal{P}}^{\text{sub}}\), which essentially deletes all states in \(\mathcal{S}_{h}^{\text{low}}\) from the state space of \(\widehat{\mathcal{P}}^{\text{trunc}}\) at each step \(h\), which does not affect the trajectory distribution as they were not reachable in \(\widehat{\mathcal{P}}^{\text{trunc}}\). Notice that the emission of \(\widehat{\mathcal{P}}^{\text{sub}}\) is exactly \(\widehat{\mathbb{O}}_{h}^{\text{sub}}\), implying that \(\widehat{\mathcal{P}}^{\text{sub}}\) is a \(\gamma/2\)-observable POMDP. Therefore, for policy class with finite memory \(\Pi^{L}\) with \(L\geq\widetilde{\Omega}(\gamma^{-4}\log(S/\epsilon)\), by Theorem 4.1 in [25], it is guaranteed that for any \(\pi\in\Pi\),

\[\mathbb{E}_{\pi}^{\widehat{\mathcal{P}}^{\text{sub}}}\|\widehat{\boldsymbol{ b}}_{h}^{\text{sub}}(\tau_{h})-\widehat{\boldsymbol{b}}_{h}^{\text{\prime,sub}}(z_{h}) \|_{1}\leq\epsilon,\]

where \(\widehat{\boldsymbol{b}}_{h}^{\text{sub}}(\tau_{h}),\widehat{\boldsymbol{b}}_ {h}^{\text{\prime,sub}}(z_{h})\in\Delta(\mathcal{S}_{h}^{\text{high}})\). Now we claim that

\[\mathbb{E}_{\pi}^{\widehat{\mathcal{P}}^{\text{trunc}}}\|\widehat{\boldsymbol{ b}}_{h}^{\text{trunc}}(\tau_{h})-\boldsymbol{b}_{h}^{\text{apx}}(z_{h})\|_{1}\leq\epsilon,\] (H.6)

where we define \(\boldsymbol{b}_{h}^{\text{apx}}(z_{h})\in\Delta(\mathcal{S})\) by _augmenting_\(\widehat{\boldsymbol{b}}_{h}^{\text{\prime,sub}}(z_{h})\) with \(0\) for states from \(\mathcal{S}_{h}^{\text{low}}\). To see the reason, we notice that simulating \(\widehat{\mathcal{P}}^{\text{trunc}}\) is exactly equivalent to simulating \(\widehat{\mathcal{P}}^{\text{sub}}\), and that \(\widehat{\boldsymbol{b}}_{h}^{\text{trunc}}(\tau_{h})(s_{h})=0\) for \(s_{h}\in\mathcal{S}_{h}^{\text{low}}\), \(\widehat{\boldsymbol{b}}_{h}^{\text{trunc}}(\tau_{h})(s_{h})=\widehat{\boldsymbol {b}}_{h}^{\text{sub}}(\tau_{h})(s_{h})\) for \(s_{h}\in\mathcal{S}_{h}^{\text{high}}\).

For the total variation distance between the trajectory distributions in \(\mathcal{P}^{\text{trunc}}\) and \(\widehat{\mathcal{P}}^{\text{trunc}}\), it holds that by Lemma H.9

\[\sum_{\overline{\tau}_{H}}|\mathbb{P}^{\pi,\mathcal{P}^{\text{ trunc}}}(\overline{\tau}_{H})-\mathbb{P}^{\pi,\widehat{\mathcal{P}}^{\text{ trunc}}}(\overline{\tau}_{H})|\] \[\quad\leq\mathbb{E}_{\pi}^{\widehat{\mathcal{P}}^{\text{trunc}}} \left[\sum_{h\in[H]}\|\mathbb{T}_{h}^{\text{trunc}}(\cdot\,|\,s_{h},a_{h})- \widehat{\mathbb{T}}_{h}^{\text{trunc}}(\cdot\,|\,s_{h},a_{h})\|_{1}+\|\mathbb{ O}_{h}^{\text{trunc}}(\cdot\,|\,s_{h})-\widehat{\mathbb{O}}_{h}^{\text{ trunc}}(\cdot\,|\,s_{h})\|_{1}\right]\] \[\quad\leq\sum_{h\in[H]}\mathbb{E}_{\pi}^{\widehat{\mathcal{P}}^{ \text{trunc}}}\left[\|\mathbb{T}_{h}(\cdot\,|\,s_{h},a_{h})-\widehat{\mathbb{T}}_ {h}(\cdot\,|\,s_{h},a_{h})\|_{1}+\|\mathbb{O}_{h}(\cdot\,|\,s_{h})-\widehat{ \mathbb{O}}_{h}(\cdot\,|\,s_{h})\|_{1}\right],\]

where the last step is by Lemma H.7.

Now by Equation (H.4), we have

\[\sum_{h}\mathbb{E}_{\pi}^{\widehat{\mathcal{P}}^{\text{trunc}}} \left[\|\mathbb{T}_{h}(\cdot\,|\,s_{h},a_{h})-\widehat{\mathbb{T}}_{h}(\cdot\,| \,s_{h},a_{h})\|_{1}+\|\mathbb{O}_{h}(\cdot\,|\,s_{h})-\widehat{\mathbb{O}}_{h}( \cdot\,|\,s_{h})\|_{1}\right]\] \[=\sum_{h}8HS\epsilon_{1}+\mathbb{E}_{\pi}^{\mathcal{P}}\left[\| \mathbb{T}_{h}(\cdot\,|\,s_{h},a_{h})-\widehat{\mathbb{T}}_{h}(\cdot\,|\,s_{h},a_ {h})\|_{1}+\|\mathbb{O}_{h}(\cdot\,|\,s_{h})-\widehat{\mathbb{O}}_{h}(\cdot\,|\,s_{h })\|_{1}\right]\] \[\leq\frac{\epsilon}{3}+8H^{2}S\epsilon_{1},\]where the last step is by Theorem H.5. Hence, by Lemma H.9, it holds that

\[\|\mathbb{P}^{\pi,\mathcal{P}^{\text{trunc}}}-\mathbb{P}^{\pi, \widehat{\mathcal{P}}^{\text{trunc}}}\|_{1}\leq\frac{\epsilon}{3}+8H^{2}S \epsilon_{1},\] (H.7) \[\mathbb{E}_{\pi}^{\mathcal{P}^{\text{trunc}}}\|\bm{b}_{h}^{\text{ trunc}}(\tau_{h})-\widehat{\bm{b}}_{h}^{\text{trunc}}(\tau_{h})\|_{1}\leq\frac{2 \epsilon}{3}+16H^{2}S\epsilon_{1}.\] (H.8)

Finally, we are ready to prove

\[\mathbb{E}_{\pi}^{\mathcal{P}}\|\bm{b}_{h}(\tau_{h})-\bm{b}_{h}^{ \text{apx}}(z_{h})\|_{1}\] \[\leq\mathbb{E}_{\pi}^{\mathcal{P}}\|\bm{b}_{h}(\tau_{h})-\bm{b}_ {h}^{\text{trunc}}(\tau_{h})\|_{1}+\mathbb{E}_{\pi}^{\mathcal{P}}\|\bm{b}_{h} ^{\text{trunc}}(\tau_{h})-\widehat{\bm{b}}_{h}^{\text{trunc}}(\tau_{h})\|_{1} +\mathbb{E}_{\pi}^{\mathcal{P}}\|\widehat{\bm{b}}_{h}^{\text{trunc}}(\tau_{h} )-\bm{b}_{h}^{\text{apx}}(z_{h})\|_{1}\] \[\leq\mathbb{E}_{\pi}^{\mathcal{P}}\|\bm{b}_{h}(\tau_{h})-\bm{b}_ {h}^{\text{trunc}}(\tau_{h})\|_{1}+\mathbb{E}_{\pi}^{\mathcal{P}^{\text{trunc} }}\|\bm{b}_{h}^{\text{trunc}}(\tau_{h})-\widehat{\bm{b}}_{h}^{\text{trunc}}( \tau_{h})\|_{1}+\mathbb{E}_{\pi}^{\widehat{\mathcal{P}}^{\text{trunc}}}\| \widehat{\bm{b}}_{h}^{\text{trunc}}(\tau_{h})-\bm{b}_{h}^{\text{apx}}(z_{h})\| _{1}\] \[\qquad+4\|\mathbb{P}^{\pi,\mathcal{P}}-\mathbb{P}^{\text{trunc}} \|_{1}+2\|\mathbb{P}^{\pi,\mathcal{P}^{\text{trunc}}}-\mathbb{P}^{\pi, \widehat{\mathcal{P}}^{\text{trunc}}}\|_{1}\] \[\leq\mathcal{O}(H^{2}S\epsilon_{1})+\mathcal{O}(\epsilon).\]

Therefore, by setting \(\epsilon_{1}=\Theta\left(\frac{\epsilon}{H^{2}S}\right)\), we prove our lemma. Observe that our algorithm needed no further samples. The computational complexity follows by computing belief \(\bm{b}_{h}^{\text{apx}}\) on POMDP \(\widehat{\mathcal{P}}^{\text{sub}}\) using finite-memory policies of size \(\widetilde{\Theta}(\gamma^{-4}\log(S/\epsilon))\). For the final sample complexity, we only need to ensure \(N=\widetilde{\Theta}(\frac{O\log(SH/\delta)}{\gamma^{2}\epsilon_{1}})\) in Equation (H.3), thus concluding our proof. 

We conclude the proof of Theorem 5.2 by combining Theorem H.5 and Theorem H.6. 

### Supporting Technical Lemmas

In the following, we provide some technical lemmas and their proofs.

**Lemma H.7**.: Fix \(n>0\) and an index set \(S\subseteq[n]\). For two sequences \(x_{1:n}\) and \(y_{1:n}\) such that \(x_{i},y_{i}\in[0,1]\) for \(i\in[n]\) and \(\sum_{i}x_{i}=1,\sum_{i}y_{i}=1\), we define

\[\widehat{x}_{i}=x_{i}+\frac{\sum_{j\in S}x_{j}}{n-|S|},\quad\forall i\not\in S ;\qquad\qquad\widehat{x}_{i}=0,\quad\forall i\in S.\]

We define \(\widehat{y}_{1:n}\) similarly. Then, it holds that

\[\sum_{i}|x_{i}-y_{i}|\geq\sum_{i}|\widehat{x}_{i}-\widehat{y}_{i}|.\]

Proof.: Note that

\[\sum_{i}|\widehat{x}_{i}-\widehat{y}_{i}| =\sum_{i\not\in S}|\widehat{x}_{i}-\widehat{y}_{i}|=\sum_{i\not\in S }\left|x_{i}+\frac{\sum_{j\in S}x_{j}}{n-|S|}-y_{i}-\frac{\sum_{j\in S}y_{j}}{ n-|S|}\right|\] \[\leq(n-|S|)\left|\frac{\sum_{j\in S}x_{j}}{n-|S|}-\frac{\sum_{j \in S}y_{j}}{n-|S|}\right|+\sum_{i\not\in S}|x_{i}-y_{i}|\] \[\leq\sum_{i}|x_{i}-y_{i}|,\]

which completes the proof. 

**Lemma H.8**.: Fix two finite sets \(\mathcal{X}\), \(\mathcal{Y}\) and two joint distributions \(P_{1},P_{2}\in\Delta(\mathcal{X}\times\mathcal{Y})\). It holds that

\[-\mathbb{E}_{P_{1}(x)}\sum_{y}|P_{1}(y\,|\,x)-P_{2}(y\,|\,x)| \leq\sum_{x,y}|P_{1}(x,y)-P_{2}(x,y)|-\sum_{x}|P_{1}(x)-P_{2}(x)|\] \[\leq\mathbb{E}_{P_{1}(x)}\sum_{y}|P_{1}(y\,|\,x)-P_{2}(y\,|\,x)|.\]Proof.: For the second inequality, it holds that

\[\sum_{x,y}|P_{1}(x,y)-P_{2}(x,y)| =\sum_{x,y}|P_{1}(x,y)-P_{1}(x)P_{2}(y\,|\,x)+P_{1}(x)P_{2}(y\,|\,x)- P_{2}(x,y)|\] \[\leq\sum_{x,y}|P_{1}(x,y)-P_{1}(x)P_{2}(y\,|\,x)|+|P_{1}(x)P_{2}(y \,|\,x)-P_{2}(x,y)|\] \[=\sum_{x,y}|P_{1}(x)(P_{1}(y\,|\,x)-P_{2}(y\,|\,x))|+|P_{2}(y\,|\,x )(P_{1}(x)-P_{2}(x))|\] \[=\mathbb{E}_{P_{1}(x)}\sum_{y}|P_{1}(y\,|\,x)-P_{2}(y\,|\,x)|+\sum _{x}|P_{1}(x)-P_{2}(x)|.\]

Meanwhile, for the first inequality, it holds that

\[\sum_{x,y}|P_{1}(x,y)-P_{2}(x,y)| =\sum_{x,y}|P_{1}(x,y)-P_{1}(x)P_{2}(y\,|\,x)+P_{1}(x)P_{2}(y\,|\,x )-P_{2}(x,y)|\] \[\geq\sum_{x,y}-|P_{1}(x,y)-P_{1}(x)P_{2}(y\,|\,x)|+|P_{1}(x)P_{2} (y\,|\,x)-P_{2}(x,y)|\] \[=\sum_{x,y}-|P_{1}(x)(P_{1}(y\,|\,x)-P_{2}(y\,|\,x))|+|P_{2}(y\,| \,x)(P_{1}(x)-P_{2}(x))|\] \[=\mathbb{E}_{P_{1}(x)}-\sum_{y}|P_{1}(y\,|\,x)-P_{2}(y\,|\,x)|+ \sum_{x}|P_{1}(x)-P_{2}(x)|,\]

concluding our lemma. 

**Lemma H.9**.: Consider any two POMDP instances \(\mathcal{P}\) and \(\widehat{\mathcal{P}}\) and define the belief functions as \(\bm{b}_{1:H}\) and \(\widehat{\bm{b}}_{1:H}\), respectively (see Appendix C for the definition of belief functions). It holds that for any \(\pi\in\Pi^{\text{gen}}\)

\[\left\|\mathbb{P}^{\pi,\mathcal{P}}-\mathbb{P}^{\pi,\widehat{ \mathcal{P}}}\right\|_{1} =\sum_{\overline{\tau}_{H}}|\mathbb{P}^{\pi,\mathcal{P}}(\overline {\tau}_{H})-\mathbb{P}^{\pi,\widehat{\mathcal{P}}}(\overline{\tau}_{H})|\] \[\leq\|\mu_{1}-\widehat{\mu}_{1}\|_{1}+\mathbb{E}_{\pi}^{ \mathcal{P}}\sum_{h\in[H-1]}\|\mathbb{T}_{h}(\cdot\,|\,s_{h},a_{h})-\widehat{ \mathbb{T}}_{h}(\cdot\,|\,s_{h},a_{h})\|_{1}\] \[\qquad+\mathbb{E}_{\pi}^{\mathcal{P}}\sum_{h\in[H]}\|\mathbb{O}_{ h}(\cdot\,|\,s_{h})-\widehat{\mathbb{O}}_{h}(\cdot\,|\,s_{h})\|_{1},\] \[\mathbb{E}_{\pi}^{\mathcal{P}}\|\bm{b}_{h}(\tau_{h})-\widehat{ \bm{b}}_{h}(\tau_{h})\|_{1} \leq 2\|\mu_{1}-\widehat{\mu}_{1}\|_{1}+2\mathbb{E}_{\pi}^{\mathcal{P }}\sum_{h\in[H-1]}\|\mathbb{T}_{h}(\cdot\,|\,s_{h},a_{h})-\widehat{\mathbb{T }}_{h}(\cdot\,|\,s_{h},a_{h})\|_{1}\] \[\qquad+2\mathbb{E}_{\pi}^{\mathcal{P}}\sum_{h\in[H]}\|\mathbb{O}_{ h}(\cdot\,|\,s_{h})-\widehat{\mathbb{O}}_{h}(\cdot\,|\,s_{h})\|_{1},\]

where we remind readers that we denote by \(\overline{\tau}_{H}=(s_{1:H},o_{1:H},a_{1:H-1})\) the trajectory with states from an episode of the POMDP.

Proof.: The first inequality also appears in [44] and we provide a simplified proof here for completeness. By Lemma H.8, it holds that

\[\sum_{\overline{\tau}_{H}}|\mathbb{P}^{\pi,\mathcal{P}}(\overline {\tau}_{H})-\mathbb{P}^{\pi,\widehat{\mathcal{P}}}(\overline{\tau}_{H})|\] \[\quad\leq\sum_{\overline{\tau}_{H-1}}|\mathbb{P}^{\pi,\mathcal{P }}(\overline{\tau}_{H-1})-\mathbb{P}^{\pi,\widehat{\mathcal{P}}}(\overline{ \tau}_{H-1})|+\mathbb{E}_{\pi}^{\mathcal{P}}\sum_{a_{H-1},s_{H},o_{H}}\left| \overline{\tau}_{H-1}(a_{H-1}\,|\,\overline{\tau}_{H-1})\mathbb{T}_{H-1}(s_{H} \,|\,s_{H-1},a_{H-1})\mathbb{O}_{H}(o_{H}\,|\,s_{H})\right.\] \[\qquad\qquad-\left.\pi_{H-1}(a_{H-1}\,|\,\overline{\tau}_{H-1}) \widehat{\mathbb{T}}_{H-1}(s_{H}\,|\,s_{H-1},a_{H-1})\widehat{\mathbb{O}}_{H}( o_{H}\,|\,s_{H})\right|\] \[\leq\sum_{\overline{\tau}_{H-1}}|\mathbb{P}^{\pi,\mathcal{P}}( \overline{\tau}_{H-1})-\mathbb{P}^{\pi,\widehat{\mathcal{P}}}(\overline{\tau}_{H -1})|+\mathbb{E}_{\pi}^{\mathcal{P}}\left[\|\mathbb{T}_{H-1}(\cdot\,|\,s_{H-1},a_{H-1})-\widehat{\mathbb{T}}_{H-1}(\cdot\,|\,s_{H-1},a_{H-1})\|_{1}+\| \mathbb{O}_{H}(\cdot\,|\,s_{H})-\widehat{\mathbb{O}}_{H}(\cdot\,|\,s_{H})\|_{1} \right],\]where the last step is again from Lemma H.8. Therefore, by repeatedly unrolling the inequality, we proved the first result.

For the second result, we notice that by Lemma H.8, it holds that

\[\sum_{\tau_{h},s_{h}}|\mathbb{P}_{h}^{\pi,\mathcal{P}}(\tau_{h},s_{h})-\mathbb{ P}_{h}^{\pi,\widehat{\mathcal{P}}}(\tau_{h},s_{h})|\geq-\sum_{\tau_{h}}|\mathbb{P}_{h}^ {\pi,\mathcal{P}}(\tau_{h})-\mathbb{P}_{h}^{\pi,\widehat{\mathcal{P}}}(\tau_{h} )|+\mathbb{E}_{\pi}^{\mathcal{P}}\sum_{s_{h}}|\mathbb{P}_{h}^{\pi,\mathcal{P} }(s_{h}\,|\,\tau_{h})-\mathbb{P}_{h}^{\pi,\widehat{\mathcal{P}}}(s_{h}\,|\, \tau_{h})|.\]

Notice the fact that \(\mathbb{P}_{h}^{\pi,\mathcal{P}}(s_{h}\,|\,\tau_{h})\) does not depend on \(\pi\) since it is exactly the belief \(\bm{b}_{h}(\tau_{h})(s_{h})\), we conclude that

\[\mathbb{E}_{\pi}^{\mathcal{P}}\|\bm{b}_{h}(\tau_{h})-\widehat{ \bm{b}}_{h}(\tau_{h})\|_{1}\leq \sum_{\tau_{h},s_{h}}|\mathbb{P}_{h}^{\pi,\mathcal{P}}(\tau_{h},s _{h})-\mathbb{P}_{h}^{\pi,\widehat{\mathcal{P}}}(\tau_{h},s_{h})|+\sum_{\tau_ {h}}|\mathbb{P}_{h}^{\pi,\mathcal{P}}(\tau_{h})-\mathbb{P}_{h}^{\pi,\widehat{ \mathcal{P}}}(\tau_{h})|\] \[\leq 2\sum_{\tau_{H}}|\mathbb{P}^{\pi,\mathcal{P}}(\overline{\tau}_{ H})-\mathbb{P}^{\pi,\widehat{\mathcal{P}}}(\overline{\tau}_{H})|,\] (H.9)

where the last step comes from the fact that after marginalization, the total variation distance will not increase. By combining it with the first result, we proved the second result. 

**Corollary H.10**.: Consider any two POSG instances \(\mathcal{G}\), \(\widehat{\mathcal{G}}\) that satisfy Assumption C.8 and the corresponding belief functions in the form of \(\mathbb{P}^{\mathcal{G}}\), \(\mathbb{P}^{\widehat{\mathcal{G}}}:\mathcal{C}_{h}\to\Delta(\mathcal{P}_{h} \times\mathcal{S})\) for any \(h\in[H]\), respectively. It holds that for any \(\pi\in\Pi^{\text{gen}}\)

\[\mathbb{E}_{\pi}^{\mathcal{G}}\|\mathbb{P}^{\mathcal{G}}(\cdot, \cdot\,|\,c_{h})-\mathbb{P}^{\widehat{\mathcal{G}}}(\cdot,\cdot\,|\,c_{h})\|_{1}\] \[\quad\leq 2\|\mu_{1}-\widehat{\mu}_{1}\|_{1}+2\mathbb{E}_{\pi}^{ \mathcal{G}}\sum_{h\in[H-1]}\|\mathbb{T}_{h}(\cdot\,|\,s_{h},a_{h})-\widehat{ \mathbb{T}}_{h}(\cdot\,|\,s_{h},a_{h})\|_{1}+2\mathbb{E}_{\pi}^{\mathcal{G}} \sum_{h\in[H]}\|\mathbb{O}_{h}(\cdot\,|\,s_{h})-\widehat{\mathbb{O}}_{h}( \cdot\,|\,s_{h})\|_{1}.\]

Proof.: For the second result, we notice that by Lemma H.8, it holds that

\[\sum_{c_{h},p_{h},s_{h}}|\mathbb{P}_{h}^{\pi,\mathcal{G}}(c_{h},p _{h},s_{h})-\mathbb{P}_{h}^{\pi,\widehat{\mathcal{G}}}(c_{h},p_{h},s_{h})| \geq-\sum_{c_{h}}|\mathbb{P}_{h}^{\pi,\mathcal{P}}(c_{h})-\mathbb{ P}_{h}^{\pi,\widehat{\mathcal{G}}}(c_{h})|\] \[+\mathbb{E}_{\pi}^{\mathcal{G}}\sum_{s_{h},p_{h}}|\mathbb{P}_{h}^ {\pi,\mathcal{G}}(s_{h},p_{h}\,|\,c_{h})-\mathbb{P}_{h}^{\pi,\widehat{\mathcal{ G}}}(s_{h},p_{h}\,|\,c_{h})|.\]

Notice the fact that \(\mathbb{P}_{h}^{\pi,\mathcal{G}}(s_{h},p_{h}\,|\,c_{h}),\mathbb{P}_{h}^{\pi, \widehat{\mathcal{G}}}(s_{h},p_{h}\,|\,c_{h})\) do not depend on \(\pi\) due to Assumption C.8, we conclude that

\[\mathbb{E}_{\pi}^{\mathcal{P}}\|\mathbb{P}^{\mathcal{G}}(\cdot, \cdot\,|\,c_{h})-\mathbb{P}^{\widehat{\mathcal{G}}}(\cdot,\cdot\,|\,c_{h})\|_{1} \leq \sum_{c_{h},p_{h}s_{h}}|\mathbb{P}_{h}^{\pi,\mathcal{G}}(c_{h},p _{h},s_{h})-\mathbb{P}_{h}^{\pi,\widehat{\mathcal{G}}}(c_{h},p_{h},s_{h})|+ \sum_{c_{h}}|\mathbb{P}_{h}^{\pi,\mathcal{G}}(c_{h})-\mathbb{P}_{h}^{\pi, \widehat{\mathcal{G}}}(c_{h})|\] \[\leq 2\sum_{\overline{\tau}_{H}}|\mathbb{P}^{\pi,\mathcal{G}}( \overline{\tau}_{H})-\mathbb{P}^{\pi,\widehat{\mathcal{G}}}(\overline{\tau}_{H})|,\] (H.10)

where the last step again comes from the fact that after marginalization, the total variation distance will not increase. By combining it with Lemma H.9, we proved the second result. 

**Lemma H.11**.: For any reward function \(r_{1:H}\) of \(\mathcal{P}\) with \(r_{h}:\mathcal{S}\times\mathcal{A}\to\mathbb{R}\) for any \(h\in[H]\), it holds that

\[\max_{\pi\in\Pi^{\text{gen}}}v^{\mathcal{P}}(\pi)\leq\max_{\pi\in\Pi_{ \mathcal{S}}}v^{\mathcal{P}}(\pi).\]

Proof.: Denote \(\pi^{\star}\in\Pi_{\mathcal{S}}\) to be the optimal policy obtained by running value iteration only on the state space \(\mathcal{S}\) for \(\mathcal{P}\). Now we are ready to prove the following argument for any \(\pi\in\Pi^{\text{gen}}\) inductively:

\[Q_{h}^{\pi^{\star},\mathcal{P}}(s_{h},a_{h})\geq Q_{h}^{\pi, \mathcal{P}}(s_{1:h},o_{1:h},a_{1:h}).\]It is easy to see the argument holds for \(h=H\). Fix state-action pair \((s_{h},a_{h})\in\mathcal{S}\times\mathcal{A}\) and trajectory \((s_{1:h-1},o_{1:h},a_{1:h-1})\), we note that:

\[Q_{h}^{\pi^{*},\mathcal{P}}(s_{h},a_{h}) =r_{h}(s_{h},a_{h})+\mathbb{E}_{s_{h+1}\sim\mathbb{T}_{h}(\,\cdot \,|\,s_{h},a_{h})}\left[\max_{a_{h+1}}Q_{h+1}^{\pi^{*},\mathcal{P}}(s_{h+1},a_{ h+1})\right]\] \[\geq r_{h}(s_{h},a_{h})+\mathbb{E}_{\begin{subarray}{c}s_{h+1} \sim\mathbb{T}_{h}(\,\cdot\,|\,s_{h},a_{h}),\\ o_{h+1}\sim O_{h+1}(\,|s_{h+1})\end{subarray}}\left[\max_{a_{h+1}}Q_{h+1}^{\pi,\mathcal{P}}(s_{1:h+1},o_{1:h+1},a_{1:h+1})\right]\] \[\geq r_{h}(s_{h},a_{h})+\mathbb{E}_{\begin{subarray}{c}s_{h+1} \sim\mathbb{T}_{h}(\,\cdot\,|\,s_{h},a_{h}),\\ o_{h+1}\sim O_{h+1}(\,|s_{h+1})\end{subarray}}\left[\mathbb{E}_{a_{h+1}\sim \pi_{h+1}(\,\cdot\,|\,s_{1:h+1},o_{1:h+1},a_{1:h})}Q_{h+1}^{\pi,\mathcal{P}}(s _{1:h+1},o_{1:h+1},a_{1:h+1})\right]\] \[=Q_{h}^{\pi,\mathcal{P}}(s_{1:h},o_{1:h},a_{1:h}),\]

where the first inequality comes from the inductive hypothesis. 

## Appendix I Missing Details in Section 6

POMDP under deterministic filter condition.We first evaluate our algorithms on POMDPs with certain structures, i.e., the deterministic conditions. In particular, we generate POMDPs, where either the transition dynamics are deterministic or the emission ensures decodability. We test three of our approaches, expert policy distillation, asymmetric optimistic natural policy gradient. We summarize our results in Table 2, where the four cases corresponds to POMDPs with \((S=A=2,O=3,H=5),(S=A=2,O=3,H=10),(S=3,A=2,O=4,H=5),\)\((S=3,A=2,O=4,H=5),\) and we can see that our approach based on expert distillation outperforms all the other methods, which is consistent with the fact that such methods have exploited the special structures of the POMDPs achieving both polynomial sample and computational complexity.

General POMDPs.Here we also evaluate our methods for general randomly generated POMDPs without any structures. Hence, we compare the baselines with asymmetric optimistic natural policy gradient and asymmetric optimistic value iteration (i.e., the single-agent version of Algorithm 5). In Figure 2, we show the performance of different algorithms in POMDPs of different size, where the four cases corresponds to POMDPs with \((S=A=O=2,H=5)\), \((S=A=O=2,H=10)\), \((S=O=3,A=2,H=10)\), \((S=A=3,O=2,H=10)\), and our approaches achieves the highest rewards with small number of episodes.

Implementation details.For each problem setting, we generated \(20\) POMDPs randomly and report the average performance and its standard deviation for each algorithms. For our algorithms based on privileged value learning methods, we find that using a finite memory of \(3\) already provides strong performance. For our algorithms based privileged policy learning, we instantiate the MDP learning algorithm with the fully observable optimistic natural policy gradient algorithm [74]. Meanwhile, for both the decoder learning and belief learning, we find that just utilizing all the historic trajectories gives us reasonable performance without additional samples. For baselines, the hyperparameters \(\alpha\) for \(Q\)-value update and step size for the policy update are tuned by grid search, where \(\alpha\) controls the update of temporal difference learning (recall the update rule of temporal difference learning as

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|} \hline  & & Asymmetric & Expert policy & Asymmetric & Vanilla AAC \\ \cline{3-6}  & & optimistic NPG & distillation & Q-learning & \\ \hline \multirow{3}{*}{Deterministic} & Case 1 & \(3.32\pm 0.66\) & \(\mathbf{3.33\pm 0.62}\) & \(3.04\pm 0.58\) & \(3.25\pm 0.65\) \\ \cline{2-6}  & Case 2 & \(7.1\pm 0.48\) & \(\mathbf{7.26\pm 0.68}\) & \(6.15\pm 0.85\) & \(6.41\pm 0.91\) \\ \cline{2-6}  & Case 3 & \(3.04\pm 0.23\) & \(\mathbf{3.25\pm 0.33}\) & \(3.09\pm 0.39\) & \(3.1\pm 0.38\) \\ \cline{2-6}  & Case 4 & \(6.51\pm 0.6\) & \(\mathbf{6.54\pm 0.58}\) & \(6.16\pm 0.48\) & \(5.87\pm 0.58\) \\ \hline \multirow{3}{*}{Block MDP} & Case 1 & \(3.31\pm 0.46\) & \(\mathbf{3.37\pm 0.41}\) & \(3.03\pm 0.4\) & \(3.08\pm 0.43\) \\ \cline{2-6}  & Case 2 & \(6.36\pm 0.52\) & \(\mathbf{6.67\pm 0.54}\) & \(5.74\pm 0.43\) & \(5.7\pm 0.46\) \\ \cline{1-1} \cline{2-6}  & Case 3 & \(3.2\pm 0.26\) & \(\mathbf{3.37\pm 0.22}\) & \(3.14\pm 0.31\) & \(2.97\pm 0.32\) \\ \cline{1-1} \cline{2-6}  & Case 4 & \(6.01\pm 0.32\) & \(\mathbf{6.44\pm 0.36}\) & \(5.58\pm 0.32\) & \(5.33\pm 0.19\) \\ \hline \end{tabular}
\end{table}
Table 2: Rewards of different approaches for POMDPs under the deterministic filter condition.

\(Q\leftarrow(1-\alpha)Q+\alpha Q^{\text{target}}\)). For asymmetric \(Q\)-learning, we use \(\epsilon\)-greedy exploration, where we use the seminal decreasing rate \(\epsilon_{t}=\frac{H+1}{H+t}\). Finally, all simulations are conducted on a personal laptop with Apple M1 CPU and 16 GB memory.

Empirical insights and interpretation of the experimental results.To understand intuitively why our approach outperforms those baseline algorithms, we notice the key difference in the value and policy update style between our approaches and vanilla asymmetric actor critic and asymmetric \(Q\)-learning. The baselines often roll-out the polices, collect trajectories, and only update the value and the policies on the states/history the trajectories have visited. Therefore, ideally, to learn a good policy for baselines, the number of trajectories to collect is at least as large as the history size, which is indeed exponential in the horizon \(H\). This is known as curse of history for partially observable RL. In contrast, in our algorithms, we explicitly estimate the empirical transition and emissions, which is indeed of polynomial size. Thus, the sample complexity avoids suffering from the potential exponential dependency of horizon or the length of the finite memory. Finally, we acknowledge that the baselines are developed to handle complex, high-dimensional deep RL problems, while scaling our methods to deep RL benchmarks requires non-trivial engineering efforts.

## Appendix J Missing Details in Section 7

Proof of Proposition 7.1:

Given the condition of Definition 3.2, the function \(\phi_{h}\) that satisfies the condition of Proposition 7.1 can be constructed recursively as follows for any reachable \(\tau_{h}\)

\[\phi_{h}(\tau_{h}):=\psi_{h}(\phi_{h-1}(\tau_{h-1}),a_{h-1},o_{h}),\]

and \(\phi_{1}(o_{1}):=\psi_{1}(o_{1})\). Therefore, we can prove by induction that by belief update rule

\[\mathbb{P}^{\mathcal{P}}(s_{h}\,|\,\tau_{h})=U_{h}(b^{\phi_{h-1}(\tau_{h-1})}; a_{h-1},o_{h})=b^{\psi_{h}(\phi_{h-1}(\tau_{h-1}),a_{h-1},o_{h})},\]

where the last step is by the definition of our \(\phi_{h}\). Therefore, we have \(\mathbb{P}^{\mathcal{P}}(s_{h}=\psi_{h}(\phi_{h-1}(\tau_{h-1}),a_{h-1},o_{h}) \,|\,\tau_{h})=1\).

For the other direction, it is similar to the proof of Lemma C.1 in [19] for \(m\)-step decodable POMDP. Here we prove it for completeness. For any reachable trajectory \(\tau_{h}\in\mathcal{T}_{h}\), it holds by the belief update and induction that

\[\mathbb{P}^{\mathcal{P}}(s_{h}\,|\,\tau_{h})=U_{h}(b^{\phi_{h-1}(\tau_{h-1})}, a_{h-1},o_{h})=\mathbb{P}^{\mathcal{P}}(s_{h}\,|\,s_{h-1}=\phi_{h-1}(\tau_{h-1}),a _{h-1},o_{h}).\]

Meanwhile, since we know \(\mathbb{P}^{\mathcal{P}}(\cdot\,|\,\tau_{h})\) is a one-hot vector, we can construct \(\psi_{h}\) such that \(\psi_{h}(s_{h-1},a_{h-1},o_{h})\) is the unique \(s_{h}\) that makes \(\mathbb{P}^{\mathcal{P}}(s_{h}\,|\,\tau_{h})>0\) with \(s_{h-1}=\phi_{h-1}(\tau_{h-1})\). If this procedure does not complete the definition of \(\psi\) for some \((s_{h-1},a_{h-1},o_{h})\), it implies that either \(s_{h-1}\) is not reachable or \(o_{h}\) is not reachable given \(s_{h-1}\), i.e., \(\mathbb{P}^{\mathcal{P}}(o_{h}\,|\,s_{h-1},o^{\prime}_{h-1})=0\) for any \(a^{\prime}_{h-1}\in\mathcal{A}\), thus recovering the conditions of Definition 3.2. 

Proof of Proposition 7.3:

Note that for any given problem instance of a POMDP, we can construct a POSG by adding a dummy agent that has the observation being the exact state at each time step, and has only one dummy action that does not affect the transition or reward. Therefore, even the local private information \(p_{i,h}\) of the dummy agent can decode the underlying state, and hence \((c_{h},p_{h})\) reveals the underlying state. Therefore, the corresponding POSG with the dummy agent satisfies the condition in Proposition 7.3. Meanwhile, it is direct to see that any CCE of the POSG is an optimal policy for the original POMDP. Now by the PSPACE-hardness of planning for POMDPs [66], we proved our proposition. 

**Theorem J.1**.: Suppose the POSG \(\mathcal{G}\) satisfies Definition 7.2. For any \(\pi\in\Pi_{\mathcal{S}}\) and (potentially stochastic) decoding functions \(\widehat{g}=\{\widehat{g}_{i,h}\}_{i\in[n],h\in[H]}\) with \(\widehat{g}_{i,h}:\mathcal{C}_{h}\times\mathcal{P}_{i,h}\rightarrow\Delta( \mathcal{S})\) for each \(i\in[n],h\in[H]\), it holds that

\[\text{NE}/\text{CCE-gap}(\pi^{\widehat{g}})-\text{NE}/\text{CCE- gap}(\pi)\leq 2nH^{2}\max_{i\in[n]}\max_{u_{i}\in\Pi_{i}}\max_{j\in[n],h\in[H]}\mathbb{P}^{u_ {i}\times\pi_{-i},\mathcal{G}}(s_{h}\neq\widehat{g}_{j,h}(c_{h},p_{j,h}))\] \[\text{CE-gap}(\pi^{\widehat{g}})-\text{CE-gap}(\pi)\leq 2nH^{2}\max_{i\in[n]} \max_{m_{i}\in\mathcal{M}_{i}}\max_{j\in[n],h\in[H]}\mathbb{P}^{(m_{i}\circ \pi_{i})\odot\pi_{-i},\mathcal{G}}(s_{h}\neq\widehat{g}_{j,h}(c_{h},p_{j,h}))\]where \(\pi^{\widehat{g}}\) is the distilled policy of \(\pi\) through the decoding functions \(\widehat{g}\), where at step \(h\), each agent \(i\) firstly individually decodes the state by sampling \(s_{h}\sim\widehat{g}_{i,h}(\cdot\,|\,c_{h},p_{i,h})\), and then acts according to the expert \(\pi_{i,h}\), where in the following discussions, we slightly abuse the notation and regard \(\widehat{g}_{i,h}(c_{h},p_{i,h})\) as a random variable following the distribution of \(\widehat{g}_{i,h}(\cdot\,|\,c_{h},p_{i,h})\). In other words, the decoding process does not need correlations among the agents.

Proof.: For notational simplicity, we write \(v_{i}\) instead of \(v_{i}^{\mathcal{G}}\) when the underlying model is clear from the context. Firstly, we consider any deterministic decoding function \(\widehat{\phi}=\{\widehat{\phi}_{i,h}\}_{i\in[n],h\in[H]}\) with \(\widehat{\phi}_{i,h}:\mathcal{C}_{h}\times\mathcal{P}_{i,h}\to\mathcal{S}\) for each \(i\in[n],h\in[H]\), and note the following that for any \(i\in[n]\),

\[v_{i}(\pi)-v_{i}(\pi^{\widehat{\phi}})=\mathbb{E}_{\pi}^{ \mathcal{G}}[R]-\mathbb{E}_{\pi^{\widehat{\phi}}}^{\mathcal{G}}[R]\] \[=\mathbb{E}_{\pi}^{\mathcal{G}}[R1[\forall j\in[n],h\in[H]:s_{h} =\widehat{\phi}_{j,h}(c_{h},p_{j,h})]]-\mathbb{E}_{\pi^{\widehat{\phi}}}^{ \mathcal{G}}[R1\!\!1[\forall j\in[n],h\in[H]:s_{h}=\widehat{\phi}_{j,h}(c_{h},p _{j,h})]]\] \[\qquad+\mathbb{E}_{\pi}^{\mathcal{G}}[R1\!\!1[\exists j\in[n],h\in [H]:s_{h}\neq\widehat{\phi}_{j,h}(c_{h},p_{j,h})]]-\mathbb{E}_{\pi^{\widehat{ \phi}}}^{\mathcal{G}}[R1\!\!1[\exists j\in[n],h\in[H]:s_{h}\neq\widehat{\phi}_ {j,h}(c_{h},p_{j,h})]]\] \[=\mathbb{E}_{\pi}^{\mathcal{G}}[R1\!\!1[\exists j\in[n],h\in[H]:s_ {h}\neq\widehat{\phi}_{j,h}(c_{h},p_{j,h})]]-\mathbb{E}_{\pi^{\widehat{\phi}}} ^{\mathcal{G}}[R1\!\!1[\exists j\in[n],h\in[H]:s_{h}\neq\widehat{\phi}_{j,h}(c_ {h},p_{j,h})]],\]

where we define \(R:=\sum_{h}r_{i,h}(s_{h},a_{h})\), and the last step is by the definition of \(\pi^{\widehat{\phi}}\)

\[v_{i}(\pi)-v_{i}(\pi^{\widehat{\phi}})\leq H\mathbb{P}^{\pi, \mathcal{G}}(\exists j\in[n],h\in[H]:s_{h}\neq\widehat{\phi}_{j,h}(c_{h},p_{j,h })).\]

Therefore, by noticing the fact that \(\widehat{g}\) is equivalent to a mixture of deterministic decoding functions, where at the beginning of each episode, one can firstly independently sample the outcome for each \((c_{h},p_{j,h})\) for \(j\in[n]\) and \(h\in[H]\), we conclude that

\[v_{i}(\pi)-v_{i}(\pi^{\widehat{g}}) =v_{i}(\pi)-\mathbb{E}_{\widehat{\phi}\sim\widehat{g}}v_{i}(\pi^{ \widehat{\phi}})\leq H\mathbb{E}_{\widehat{\phi}\sim\widehat{g}}\mathbb{P}^{ \pi,\mathcal{G}}(\exists j\in[n],h\in[H]:s_{h}\neq\widehat{\phi}_{j,h}(c_{h},p _{j,h}))\] \[=H\mathbb{P}^{\pi,\mathcal{G}}(\exists j\in[n],h\in[H]:s_{h}\neq \widehat{g}_{j,h}(c_{h},p_{j,h})).\]

Now we prove our result for NE and CCE first. Due to similar arguments for evaluating \(v_{i}(\pi)-v_{i}(\pi^{\widehat{\phi}})\), for any \(u_{i}\in\Pi_{i}\cup\Pi_{\mathcal{S},i}\), it holds that

\[v_{i}(u_{i}\times\pi^{\widehat{\phi}}_{-i})-v_{i}(u_{i}\times\pi _{-i}) \leq\mathbb{E}_{u_{i}\times\pi^{\widehat{\phi}}_{-i}}^{\mathcal{G }}[R1\!\!1[\exists j\in[n]\setminus\{i\},h\in[H]:s_{h}\neq\widehat{\phi}_{j,h}( c_{h},p_{j,h})]]\] \[\leq H\mathbb{P}^{u_{i}\times\pi^{\widehat{\phi}}_{-i},\mathcal{G }}(\exists j\in[n]\setminus\{i\},h\in[H]:s_{h}\neq\widehat{\phi}_{j,h}(c_{h},p _{j,h})).\]

We notice the following fact that

\[\mathbb{P}^{u_{i}\times\pi_{-i},\mathcal{G}}(\forall j\in[n]\setminus\{i\},h \in[H]:s_{h}=\widehat{\phi}_{j,h}(c_{h},p_{j,h}))=\sum_{\bar{\tau}_{H}\in \overline{\mathcal{F}}_{H}(\widehat{\phi})}\mathbb{P}^{u_{i}\times\pi_{-i}, \mathcal{G}}(\bar{\tau}_{H}),\]

where we define \(\overline{\mathcal{F}}_{H}(\widehat{\phi}):=\{\overline{\tau}_{H}\in \overline{\mathcal{F}}_{H}\,|\,\forall j\in[n]\setminus\{i\},h\in[H]:s_{h}= \widehat{\phi}_{j,h}(c_{h},p_{j,h})\}\). By definition of \(\pi^{\widehat{\phi}}\), it holds that

\[\forall\overline{\tau}_{H}\in\overline{\mathcal{F}}_{H}(\widehat{\phi}): \mathbb{P}^{u_{i}\times\pi_{-i},\mathcal{G}}(\overline{\tau}_{H})=\mathbb{P}^ {u_{i}\times\pi^{\widehat{\phi}}_{-i},\mathcal{G}}(\overline{\tau}_{H}).\]

Therefore, we have

\[\mathbb{P}^{u_{i}\times\pi_{-i},\mathcal{G}}(\forall j\in[n]\setminus\{i\},h \in[H]:s_{h}=\widehat{\phi}_{j,h}(c_{h},p_{j,h}))=\mathbb{P}^{u_{i}\times\pi^{ \widehat{\phi}}_{-i},\mathcal{G}}(\forall j\in[n]\setminus\{i\},h\in[H]:s_{h} =\widehat{\phi}_{j,h}(c_{h},p_{j,h})).\]

Correspondingly, it holds that

\[\mathbb{P}^{u_{i}\times\pi_{-i},\mathcal{G}}(\exists j\in[n]\setminus\{i\},h \in[H]:s_{h}\neq\widehat{\phi}_{j,h}(c_{h},p_{j,h}))=\mathbb{P}^{u_{i}\times \pi^{\widehat{\phi}}_{-i},\mathcal{G}}(\exists j\in[n]\setminus\{i\},h\in[H]:s_ {h}\neq\widehat{\phi}_{j,h}(c_{h},p_{j,h})),\]

which implies that

\[v_{i}(u_{i}\times\pi^{\widehat{\phi}}_{-i})-v_{i}(u_{i}\times\pi_{-i}) \leq H\mathbb{P}^{u_{i}\times\pi^{\widehat{\phi}}_{-i},\mathcal{G} }(\exists j\in[n]\setminus\{i\},h\in[H]:s_{h}\neq\widehat{\phi}_{j,h}(c_{h},p_{j,h }))\] \[=H\mathbb{P}^{u_{i}\times\pi_{-i},\mathcal{G}}(\exists j\in[n] \setminus\{i\},h\in[H]:s_{h}\neq\widehat{\phi}_{j,h}(c_{h},p_{j,h})).\]Again by the fact that \(\widehat{g}\) is equivalent to a mixture of deterministic decoding functions, it holds

\[v_{i}(u_{i}\times\pi_{-i}^{\widehat{g}})-v_{i}(u_{i}\times\pi_{-i}) =\mathbb{E}_{\widehat{g}\sim\widehat{g}}v_{i}(u_{i}\times\pi_{-i}^{ \widehat{\phi}})-v_{i}(u_{i}\times\pi_{-i})\] \[\leq H\mathbb{E}_{\widehat{g}\sim\widehat{g}}\mathbb{P}^{u_{i} \times\pi_{-i},\mathcal{G}}(\exists j\in[n]\setminus\{i\},h\in[H]:s_{h}\neq \widehat{\phi}_{j,h}(c_{h},p_{j,h}))\] \[=H\mathbb{P}^{u_{i}\times\pi_{-i},\mathcal{G}}(\exists j\in[n] \setminus\{i\},h\in[H]:s_{h}\neq\widehat{g}_{j,h}(c_{h},p_{j,h})).\] (J.1)

Now we are ready to evaluate the NE/CCE-gap of policy \(\pi^{\widehat{g}}\) as follows:

\[\mathrm{NE/CCE-gap}(\pi^{\widehat{g}})-\mathrm{NE/CCE-gap}(\pi)\] \[\leq\max_{i\in[n]}\left(\max_{u_{i}\in\Pi_{i}}v_{i}(u_{i}\times \pi_{-i}^{\widehat{g}})-\max_{u_{i}\in\Pi_{S,i}}v_{i}(u_{i}\times\pi_{-i}) \right)+H\mathbb{P}^{\pi,\mathcal{G}}(\exists j\in[n],h\in[H]:s_{h}\neq \widehat{g}_{j,h}(c_{h},p_{j,h})).\]

Now we notice that \(\max_{u_{i}\in\Pi_{S,i}}v_{i}(u_{i}\times\pi_{-i})=\max_{u_{i}\in\Pi_{i}}v_{i} (u_{i}\times\pi_{-i})\) since \(\Pi_{\mathcal{S},i}\subseteq\Pi_{i}\) by the deterministic filter condition Definition 3.2 and \(\pi_{-i}\) is a Markov policy. Therefore, we conclude that

\[\mathrm{NE/CCE-gap}(\pi^{\widehat{g}})-\mathrm{NE/CCE-gap}(\pi)\] \[\leq\max_{i\in[n]}\left(\max_{u_{i}\in\Pi_{i}}v_{i}(u_{i}\times \pi_{-i}^{\widehat{g}})-\max_{u_{i}\in\Pi_{i}}v_{i}(u_{i}\times\pi_{-i}) \right)+H\mathbb{P}^{\pi,\mathcal{G}}(\exists j\in[n],h\in[H]:s_{h}\neq \widehat{g}_{j,h}(c_{h},p_{j,h}))\] \[\leq\max_{i\in[n]}\left(\max_{u_{i}\in\Pi_{i}}\left(v_{i}(u_{i} \times\pi_{-i}^{\widehat{g}})-v_{i}(u_{i}\times\pi_{-i})\right)\right)+H \mathbb{P}^{\pi,\mathcal{G}}(\exists j\in[n],h\in[H]:s_{h}\neq\widehat{g}_{j,h} (c_{h},p_{j,h}))\] \[\leq\max_{i\in[n]}\left(\max_{u_{i}\in\Pi_{i}}H\mathbb{P}^{u_{i} \times\pi_{-i},\mathcal{G}}(\exists j\in[n]\setminus\{i\},h\in[H]:s_{h}\neq \widehat{g}_{j,h}(c_{h},p_{j,h}))\right)+H\mathbb{P}^{\pi,\mathcal{G}}(\exists j \in[n],h\in[H]:s_{h}\neq\widehat{g}_{j,h}(c_{h},p_{j,h}))\] \[\leq 2H\max_{i\in[n],u_{i}\in\Pi_{i}}\sum_{j\in[n]}\sum_{h} \mathbb{P}^{u_{i}\times\pi_{-i},\mathcal{G}}(s_{h}\neq\widehat{g}_{j,h}(c_{h},p _{j,h}))\] \[\leq 2nH^{2}\max_{i\in[n],u_{i}\in\Pi_{i},j\in[n],h\in[H]} \mathbb{P}^{u_{i}\times\pi_{-i},\mathcal{G}}(s_{h}\neq\widehat{g}_{j,h}(c_{h},p _{j,h})),\]

where the second last step is by a union bound, thus proving our result for NE and CCE.

For CE, consider any strategy modification \(m_{i}\in\mathcal{M}_{i}\cup\mathcal{M}_{\mathcal{S},i}\), it holds that

\[\mathrm{CE-gap}(\pi^{\widehat{g}})-\mathrm{CE-gap}(\pi)\] \[\leq\max_{m_{i}\in\mathcal{M}_{i}}v_{i}((m_{i}\circ\pi_{i}^{ \widehat{g}})\odot\pi_{-i}^{\widehat{g}})-\max_{m_{i}\in\mathcal{M}_{ \mathcal{S},i}}v_{i}((m_{i}\circ\pi_{i})\odot\pi_{-i})+H\mathbb{P}^{\pi, \mathcal{G}}(\exists j\in[n],h\in[H]:s_{h}\neq\widehat{g}_{j,h}(c_{h},p_{j,h})).\]

Now we notice that \(\max_{m_{i}\in\mathcal{M}_{\mathcal{S},i}}v_{i}((m_{i}\circ\pi_{i})\odot\pi_{ -i})=\max_{m_{i}\in\mathcal{M}_{\mathcal{S},i}}v_{i}((m_{i}\circ\pi_{i})\odot \pi_{-i})\) since \(\mathcal{M}_{\mathcal{S},i}\subseteq\mathcal{M}_{i}\) by the deterministic filter condition Definition 7.2 and Lemma J.2. Therefore, we conclude that

\[\mathrm{CE-gap}(\pi^{\widehat{g}})-\mathrm{CE-gap}(\pi)\] \[\leq\max_{i\in[n]}\max_{m_{i}\in\mathcal{M}_{i}}v_{i}((m_{i}\circ \pi_{i}^{\widehat{g}})\odot\pi_{-i}^{\widehat{g}})-\max_{m_{i}\in\mathcal{M}_{ i}}v_{i}((m_{i}\circ\pi_{i})\odot\pi_{-i})+H\mathbb{P}^{\pi,\mathcal{G}}(\exists j \in[n],h\in[H]:s_{h}\neq\widehat{g}_{j,h}(c_{h},p_{j,h}))\] \[\leq\max_{i\in[n]}\max_{m_{i}\in\mathcal{M}_{i}}\left(v_{i}((m_{i }\circ\pi_{i}^{\widehat{g}})\odot\pi_{-i}^{\widehat{g}})-v_{i}((m_{i}\circ\pi_ {i})\odot\pi_{-i})\right)+H\mathbb{P}^{\pi,\mathcal{G}}(\exists j\in[n],h\in[H]: s_{h}\neq\widehat{g}_{j,h}(c_{h},p_{j,h}))\] \[\leq\max_{i\in[n]}\max_{m_{i}\in\mathcal{M}_{i}}H\mathbb{P}^{(m_{i} \circ\pi_{i})\odot\pi_{-i},\mathcal{G}}(\exists j\in[n]\setminus\{i\},h\in[H]: s_{h}\neq\widehat{g}_{j,h}(c_{h},p_{j,h}))\] \[\qquad\qquad+H\mathbb{P}^{\pi,\mathcal{G}}(\exists j\in[n],h\in[ H]:s_{h}\neq\widehat{g}_{j,h}(c_{h},p_{j,h}))\] \[\leq 2H\max_{i\in[n],m_{i}\in\mathcal{M}_{i}}\sum_{j\in[n]}\sum_{h} \mathbb{P}^{(m_{i}\circ\pi_{j,i})\odot\pi_{-i},\mathcal{G}}(s_{h}\neq\widehat{ g}_{j,h}(c_{h},p_{j,h}))\] \[\leq 2nH^{2}\max_{i\in[n],m_{i}\in\mathcal{M}_{i},h\in[H],j\in[n]} \mathbb{P}^{(m_{i}\circ\pi_{i})\odot\pi_{-i},\mathcal{G}}(s_{h}\neq\widehat{g}_{j,h}(c _{h},p_{j,h}))\]

where the third step is due to the same reason as Equation (J.1). 

**Lemma J.2**.: For any \(\pi\in\Pi_{\mathcal{S}}\), it holds for any reward function and \(i\in[n]\) that

\[\max_{m_{i}\in\mathcal{M}_{i}^{\mathrm{con}}}v_{i}((m_{i}\circ\pi_{i})\odot\pi_{- i})=\max_{m_{i}\in\mathcal{M}_{\mathcal{S},i}}v_{i}((m_{i}\circ\pi_{i})\odot\pi_{-i}).\]Proof.: Denote \(m_{i}^{\star}\in\operatorname*{argmax}_{m_{i}\in\mathcal{M}_{i}^{\mathsf{reg}}}v_{i} ((m_{i}\diamond\pi_{i})\odot\pi_{-i})\) and \(\widehat{m}_{i}^{\star}\in\operatorname*{argmax}_{m_{i}\in\mathcal{M}_{S,i}}v_ {i}((m_{i}\diamond\pi_{i})\odot\pi_{-i})\). Now we shall prove that \(V_{i,h}^{(m_{i}^{\star}\diamond\pi_{i})\odot\pi_{-i},\mathcal{G}}(s_{1:h},o_{ 1:h},a_{1:h-1})\leq V_{i,h}^{(\widehat{m}_{i}^{\star}\diamond\pi_{i})\odot\pi_ {-i},\mathcal{G}}(s_{h})\) inductively for each \(h\in[H]\). Note that it holds for \(h=H+1\). Now we consider the following

\[V_{i,h}^{(m_{i}^{\star}\diamond\pi_{i})\odot\pi_{-i},\mathcal{G}} (s_{1:h},o_{1:h},a_{1:h-1})\] \[=\mathbb{E}_{\begin{subarray}{c}a_{h}\sim\pi_{h}(\,\cdot\,|\,s_{h })\\ s_{h+1}\sim T_{h}(\,\cdot\,|\,s_{h},m_{i,h}^{\star}(s_{1:h},o_{1:h},a_{2:h-1},a_ {i,h}),a_{-i,h})\\ o_{h+1}\sim O_{h+1}(\,\cdot\,|\,s_{h+1})\end{subarray}}\Bigg{\{}r_{h}(s_{h},m _{i,h}^{\star}(s_{1:h},o_{1:h},a_{1:h-1},a_{i,h}),a_{-i,h})\] \[\qquad\qquad+V_{i,h+1}^{(m_{i}^{\star}\diamond\pi_{i})\odot\pi_ {-i},\mathcal{G}}(s_{1:h+1},o_{1:h+1},a_{1:h-1},m_{i,h}^{\star}(s_{1:h},o_{1:h },a_{1:h-1},a_{i,h}),a_{-i,h})\Bigg{\}}\] \[\leq\mathbb{E}_{\begin{subarray}{c}a_{h}\sim\pi_{h}(\,\cdot\,|\,s _{h})\\ s_{h+1}\sim T_{h}(\,\cdot\,|\,s_{h},m_{i,h}^{\star}(s_{1:h},o_{1:h},a_{2:h-1},a_ {i,h}),a_{-i,h})\end{subarray}}\Bigg{\{}r_{h}(s_{h},m_{i,h}^{\star}(s_{1:h},o_ {1:h},a_{1:h-1},a_{i,h}),a_{-i,h})+V_{i,h+1}^{(\widehat{m}_{i}^{\star} \diamond\pi_{i})\odot\pi_{-i},\mathcal{G}}(s_{h+1})\Big{\}}\] \[\leq\mathbb{E}_{\begin{subarray}{c}a_{h}\sim\pi_{h}(\,\cdot\,|\,s _{h})\\ s_{h+1}\sim T_{h}(\,\cdot\,|\,s_{h},\widehat{m}_{i,h}^{\star}(s_{h},a_{i,h}),a _{-i,h})\\ o_{h+1}\sim O_{h+1}(\,\cdot\,|\,s_{h+1})\end{subarray}}\Big{[}r_{h}(s_{h}, \widehat{m}_{i,h}^{\star}(s_{h},a_{i,h}),a_{-i,h})+V_{i,h+1}^{(\widehat{m}_{i }^{\star}\diamond\pi_{i})\odot\pi_{-i},\mathcal{G}}(s_{h+1})\Big{]}\] \[=V_{i,h}^{(\widehat{m}_{i}^{\star}\diamond\pi_{i})\odot\pi_{-i}, \mathcal{G}}(s_{h}),\]

where the second inequality follows from the inductive hypothesis and the third step is by the definition of \(\widehat{m}_{i}^{\star}\in\operatorname*{argmax}_{m_{i}\in\mathcal{M}_{S,i}}v_ {i}((m_{i}\diamond\pi_{i})\odot\pi_{-i})\). 

**Lemma J.3**.: Given an approximate POSG \(\widehat{\mathcal{G}}\) that satisfies Assumption C.8 with approximate transitions and emissions being \(\{\widehat{\mathbb{T}}_{h},\widehat{\mathbb{O}}_{h}\}_{h\in[H]}\), we define the approximate decoding function \(\widehat{g}\) to be

\[\widehat{g}_{i,h}(s_{h}\,|\,c_{h},p_{i,h}):=\mathbb{P}^{\widehat{\mathcal{G}}} (s_{h}\,|\,c_{h},p_{i,h}),\]

for each \(h\in[H]\), \(s_{h}\in\mathcal{S}\), \(c_{h}\in\mathcal{C}_{h}\), \(p_{i,h}\in\mathcal{P}_{i,h}\). Then it holds that for any \(\pi\in\Pi_{\mathcal{S}}\),

\[\max_{i\in[n],u_{i}\in\Pi_{i},j\in[n],h\in[H]}\mathbb{P}^{u_{i} \times\pi_{-i},\mathcal{G}}(s_{h}\neq\widehat{g}_{j,h}(c_{h},p_{j,h}))\] \[\qquad\leq\max_{i\in[n],u_{i}\in\Pi_{S,i}}\mathbb{E}_{u_{i}\times \pi_{-i}}^{\mathcal{G}}\left[\sum_{h\in[H-1]}\|\mathbb{T}_{h}(\cdot\,|\,s_{h},a _{h})-\widehat{\mathbb{T}}_{h}(\cdot\,|\,s_{h},a_{h})\|_{1}+\sum_{h\in[H]}\| \mathbb{O}_{h}(\cdot\,|\,s_{h})-\widehat{\mathbb{O}}_{h}(\cdot\,|\,s_{h})\|_{1 }\right],\]

and

\[\max_{i\in[n],m_{i}\in\mathcal{M}_{i},j\in[n],h\in[H]}\mathbb{P} ^{(m_{i}\diamond\pi_{i})\odot\pi_{-i},\mathcal{G}}(s_{h}\neq\widehat{g}_{j,h}(c_ {h},p_{j,h}))\] \[\qquad\leq\max_{i\in[n],m_{i}\in\mathcal{M}_{S,i}}\mathbb{E}_{(m _{i}\diamond\pi_{i})\odot\pi_{-i}}^{\mathcal{G}}\left[\sum_{h\in[H]}\|\mathbb{T }_{h}(\cdot\,|\,s_{h},a_{h})-\widehat{\mathbb{T}}_{h}(\cdot\,|\,s_{h},a_{h})\|_ {1}+\sum_{h\in[H-1]}\|\mathbb{O}_{h}(\cdot\,|\,s_{h})-\widehat{\mathbb{O}}_{h}( \cdot\,|\,s_{h})\|_{1}\right].\]

Proof.: Note for any \(i\in[n]\), \(u_{i}\in\Pi_{i}\), \(j\in[n]\), \(h\in[H]\), it holds

\[\mathbb{P}^{u_{i}\times\pi_{-i},\mathcal{G}}(s_{h}\neq\widehat{g}_{j,h}(c_{h},p_{j,h}))=\frac{1}{2}\mathbb{E}_{u_{i}\times\pi_{-i}}^{\mathcal{G}}\sum_{s_{h}} \left|\mathbb{P}^{\mathcal{G}}(s_{h}\,|\,c_{h},p_{j,h})-\mathbb{P}^{\widehat{ \mathcal{G}}}(s_{h}\,|\,c_{h},p_{j,h})\right|,\]due to the condition in Definition 7.2. Meanwhile,

\[\frac{1}{2}\mathbb{E}^{\mathcal{G}}_{u_{i}\times\pi_{-i}}\sum_{s_{h} }\left|\mathbb{P}^{\mathcal{G}}(s_{h}\,|\,c_{h},p_{j,h})-\mathbb{P}^{\widehat{ \mathcal{G}}}(s_{h}\,|\,c_{h},p_{j,h})\right|\] \[\quad\leq\sum_{s_{h},c_{h},p_{j,h}}\left|\mathbb{P}^{u_{i}\times \pi_{-i},\mathcal{G}}(s_{h},c_{h},p_{j,h})-\mathbb{P}^{u_{i}\times\pi_{-i}, \widehat{\mathcal{G}}}(s_{h},c_{h},p_{j,h})\right|\] \[\quad\leq\sum_{\overline{r}_{h}}\left|\mathbb{P}^{u_{i}\times\pi_ {-i},\mathcal{G}}(\overline{r}_{h})-\mathbb{P}^{u_{i}\times\pi_{-i},\widehat{ \mathcal{G}}}(\overline{r}_{h})\right|\] \[\quad\leq\mathbb{E}^{\mathcal{G}}_{u_{i}\times\pi_{-i}}\left[ \sum_{h\in[H-1]}\|\mathbb{T}_{h}(\cdot\,|\,s_{h},a_{h})-\widehat{\mathbb{T}}_{ h}(\cdot\,|\,s_{h},a_{h})\|_{1}+\sum_{h\in[H]}\|\mathbb{O}_{h}(\cdot\,|\,s_{h})- \widehat{\mathbb{O}}_{h}(\cdot\,|\,s_{h})\|_{1}\right],\]

where the first inequality is by the first inequality in Lemma J.7, the second and third inequalities are due to the fact that TV distance does not increase after marginalization, and the last inequality is by Lemma H.9. Since \(\pi_{-i}\) is a fixed and fully-observable Markov policy, by Lemma H.11, we have

\[\mathbb{E}^{\mathcal{G}}_{u_{i}\times\pi_{-i}}\left[\sum_{h\in[H- 1]}\|\mathbb{T}_{h}(\cdot\,|\,s_{h},a_{h})-\widehat{\mathbb{T}}_{h}(\cdot\,| \,s_{h},a_{h})\|_{1}+\sum_{h\in[H]}\|\mathbb{O}_{h}(\cdot\,|\,s_{h})-\widehat{ \mathbb{O}}_{h}(\cdot\,|\,s_{h})\|_{1}\right]\] \[\quad\leq\max_{u_{i}\in\Pi_{\mathcal{S},i}}\mathbb{E}^{\mathcal{G }}_{u_{i}\times\pi_{-i}}\left[\sum_{h\in[H-1]}\|\mathbb{T}_{h}(\cdot\,|\,s_{h},a_{h})-\widehat{\mathbb{T}}_{h}(\cdot\,|\,s_{h},a_{h})\|_{1}+\sum_{h\in[H]} \|\mathbb{O}_{h}(\cdot\,|\,s_{h})-\widehat{\mathbb{O}}_{h}(\cdot\,|\,s_{h})\| _{1}\right],\]

thus proving the first result of our lemma.

For the second result of our lemma, it can be proved similarly that for any \(i\in[n]\), \(m_{i}\in\mathcal{M}_{i}\), \(j\in[n]\), \(h\in[H]\),

\[\mathbb{P}^{(m_{i}\circ\pi_{i})\odot\pi_{-i},\mathcal{G}}(s_{h} \neq\widehat{g}_{j,h}(c_{h},p_{j,h}))\] \[\leq\mathbb{E}^{\mathcal{G}}_{(m_{i}\circ\pi_{i})\odot\pi_{-i}} \left[\sum_{h\in[H-1]}\|\mathbb{T}_{h}(\cdot\,|\,s_{h},a_{h})-\widehat{\mathbb{ T}}_{h}(\cdot\,|\,s_{h},a_{h})\|_{1}+\sum_{h\in[H]}\|\mathbb{O}_{h}(\cdot\,|\,s_{h})- \widehat{\mathbb{O}}_{h}(\cdot\,|\,s_{h})\|_{1}\right].\]

By Lemma J.2, we proved the second result. 

**Theorem J.4**.: Fix any \(\epsilon,\delta\in(0,1)\) and \(\pi\in\Pi_{\mathcal{S}}\). Algorithm 5 can learn a decoding function \(\widehat{g}\) such that with probability \(1-\delta\)

\[\max_{i\in[n],u_{i}\in\Pi_{i,j}\in[n],h\in[H]}\mathbb{P}^{u_{i}\times\pi_{-i}, \mathcal{G}}(s_{h}\neq\widehat{g}_{j,h}(c_{h},p_{j,h}))\leq\epsilon,\]

with total sample complexity \(\widehat{\mathcal{O}}(\frac{nS^{2}AHO+nS^{3}AH}{\epsilon^{2}}+\frac{S^{4}A^{2} H^{5}}{\epsilon})\) and computational complexity \(\textsc{poly}(S,A,H,O,\frac{1}{\epsilon}),\log\frac{1}{\delta}\).

Proof.: With the help of Lemma J.3, it suffices to prove

\[\max_{i\in[n],u_{i}\in\Pi_{\mathcal{G},i}}\mathbb{E}^{\mathcal{G}}_{u_{i} \times\pi_{-i}}\left[\sum_{h\in[H]}\|\mathbb{T}_{h}(\cdot\,|\,s_{h},a_{h})- \widehat{\mathbb{T}}_{h}(\cdot\,|\,s_{h},a_{h})\|_{1}+\|\mathbb{O}_{h}(\cdot \,|\,s_{h})-\widehat{\mathbb{O}}_{h}(\cdot\,|\,s_{h})\|_{1}\right]\leq\epsilon.\]

The following proof procedure follows similarly to that of Theorem H.5. For each \(h\in[H]\) and \(s_{h}\in\mathcal{S}\), we define

\[p_{h}(s_{h})=\max_{i\in[n],u_{i}\in\Pi_{\mathcal{S},i}}d^{u_{i}\times\pi_{-i}} _{h}(s_{h}).\]

Fix \(\epsilon_{1},\delta_{1}>0\), we define \(\mathcal{U}(h,\epsilon_{1})=\{s_{h}\in\mathcal{S}\,|\,p_{h}(s_{h})\geq\epsilon_{1}\}\). By [37], one can learn a policy \(\{\Psi_{i}(h,s_{h})\}_{i\in[n]}\) with sample complexity \(\widehat{\mathcal{O}}(\frac{S^{2}A_{i}H^{4}}{\epsilon_{1}})\) such that \(\max_{i\in[n]}d^{\Psi_{i}(h,s_{h})\times\pi_{-i}}_{h}(s_{h})\geq\epsilon_{1}\).

\(\frac{p_{h}(s_{h})}{2}\) for each \(s_{h}\in\mathcal{U}(h,\epsilon_{1})\) with probability \(1-n\cdot\delta_{1}\). Now we assume this event holds for any \(h\in[H]\) and \(s_{h}\in\mathcal{U}(h,\epsilon_{1})\). For each \(s_{h}\in\mathcal{S}\) and \(a_{h}\in\mathcal{A}\), we have executed each policy \(\{\Psi_{i}(h,s_{h})\times\pi_{-i}\}_{i\in[n]}\) for the first \(h-1\) steps followed by an action \(a_{h}\in\mathcal{A}\) for \(N\) episodes and denote the total number of episodes that \(s_{h}\) and \(a_{h}\) are visited as \(N_{h}(s_{h},a_{h})\), and \(N_{h}(s_{h})=\sum_{a\in\mathcal{A}}N_{h}(s_{h},a)\). Then, with probability \(1-e^{-N\epsilon_{1}/8}\), we have \(N_{h}(s_{h},a_{h})\geq\frac{Np_{h}(s_{h})}{2}\) by Chernoff bound. Now conditioned on this event, we are ready to evaluate the following for any \(i\in[n]\), and \(u_{i}\in\Pi_{\mathcal{S},i}\):

\[\mathbb{E}_{u_{i}\times\pi_{-i}}^{\mathcal{G}}\|\mathbb{T}_{h}( \cdot\,|\,s_{h},a_{h})-\widehat{\mathbb{T}}_{h}(\cdot\,|\,s_{h},a_{h})\|_{1}= \sum_{s_{h},a_{h}}d_{h}^{u_{i}\times\pi_{-i}}(s_{h})(u_{i}\times\pi_{-i})_{h}( a_{h}\,|\,s_{h})\|\mathbb{T}_{h}(\cdot\,|\,s_{h},a_{h})-\widehat{\mathbb{T}}_{h}( \cdot\,|\,s_{h},a_{h})\|_{1}\] \[\quad\leq 2\cdot S\epsilon_{1}+\sum_{s_{h}\in\mathcal{U}(h, \epsilon_{1}),a_{h}}d_{h}^{u_{i}\times\pi_{-i}}(s_{h})[u_{i}\times\pi_{-i}]_{h }(a_{h}\,|\,s_{h})\sqrt{\frac{S\log(1/\delta_{2})}{N_{h}(s_{h},a_{h})}}\] \[\quad\leq 2\cdot S\epsilon_{1}+\sum_{s_{h}\in\mathcal{U}(h, \epsilon_{1}),a_{h}}d_{h}^{u_{i}\times\pi_{-i}}(s_{h})[u_{i}\times\pi_{-i}]_{h }(a_{h}\,|\,s_{h})\sqrt{\frac{2S\log(1/\delta_{2})}{Np_{h}(s_{h})}}\] \[\quad\leq 2\cdot S\epsilon_{1}+\sum_{s_{h}}\sqrt{d_{h}^{u_{i} \times\pi_{-i}}(s_{h})}\sqrt{\frac{2S\log(1/\delta_{2})}{N}}\] \[\quad\leq 2\cdot S\epsilon_{1}+S\sqrt{\frac{2\log(1/\delta_{2})}{ N}},\]

where the second step is by Lemma J.8, and the last step is by Cauchy-Schwarz inequality. Similarly,

\[\mathbb{E}_{u_{i}\times\pi_{-i}}^{\mathcal{G}}\|\mathbb{O}_{h}( \cdot\,|\,s_{h})-\widehat{\mathbb{O}}_{h}(\cdot\,|\,s_{h})\|_{1}=\sum_{s_{h}}d _{h}^{u_{i}\times\pi_{-i}}(s_{h})\|\mathbb{O}_{h}(\cdot\,|\,s_{h})-\widehat{ \mathbb{O}}_{h}(\cdot\,|\,s_{h})\|_{1}\] \[\quad\leq 2\cdot S\epsilon_{1}+\sum_{s_{h}\in\mathcal{U}(h, \epsilon_{1})}d_{h}^{u_{i}\times\pi_{-i}}(s_{h})\sqrt{\frac{O\log(1/\delta_{2 })}{N_{h}(s_{h})}}\] \[\quad\leq 2\cdot S\epsilon_{1}+\sum_{s_{h}\in\mathcal{U}(h, \epsilon_{1})}d_{h}^{u_{i}\times\pi_{-i}}(s_{h})\sqrt{\frac{2O\log(1/\delta_{2 })}{N}}\] \[\quad\leq 2\cdot S\epsilon_{1}+\sqrt{\frac{2SO\log(1/\delta_{2})}{ N}},\]

where the second step is by Lemma J.9, and the last step is by Cauchy-Schwarz inequality. Therefore, by a union bound, all high probability events hold with probability

\[1-SHn\delta_{1}-SHAe^{-N\epsilon_{1}/8}-2SAH\delta_{2}.\]

Therefore, we can choose \(N=\widetilde{\Theta}(\frac{S^{2}+SO}{\epsilon^{2}})\) and \(\epsilon_{1}=\Theta(\frac{\epsilon}{S})\), leading to the total sample complexity

\[SHA\left(nN+\widetilde{\Theta}\left(\frac{S^{3}AH^{4}}{\epsilon}\right) \right)=\widetilde{\Theta}\left(\frac{nS^{2}AHO+nS^{3}AH}{\epsilon^{2}}+\frac{ S^{4}A^{2}H^{5}}{\epsilon}\right),\]

which completes the proof. 

Note that although our Algorithm 5 and Theorem J.4 are stated for NE/CCE, it can also handle CE with simple modifications, where the key observation is that the strategy modification \(m_{i}\in\mathcal{M}_{\mathcal{S},i}\) can also be regarded as a Markov policy in an _extended_ MDP marginalized by \(\pi_{-i}\) as defined below.

**Definition J.5**.: Fix \(\pi\in\Pi_{\mathcal{S}}\). We define \(\mathcal{M}_{i}^{\text{extended}}(\pi)\) to be an MDP for agent \(i\), where for each \(h\in[H]\), the state is \((s_{h},a_{i,h})\), the action is some modified action \(a_{i,h}^{\prime}\), the transition is defined as \(\mathbb{E}_{a_{-i,h}\sim\pi_{h}(\cdot\,|\,s_{h,a_{i,h}})}[\mathbb{T}_{h}(s_{h+1}\,| \,s_{h},a_{i,h}^{\prime},a_{-i,h})\pi_{h+1}(a_{i,h+1}\,|\,s_{h+1})]\), where we slightly abuse the notation of \(\pi_{h}(a_{-i,h}\,|\,s_{h},a_{i,h})\) and \(\pi_{h}(a_{i,h}\,|\,s_{h})\) by defining them as the posterior and marginal distributions induced by the joint distribution \(\pi_{h}(a_{h}\,|\,s_{h})\). Similarly, the reward is given by \(r_{h}^{\text{extended}}(s_{h},a_{i,h},a_{i,h}^{\prime}):=\mathbb{E}_{a_{-i,h} \sim\pi_{h}(\cdot\,|\,s_{h,a_{i,h}})}[r_{h}(s_{h},a_{i,h}^{\prime},a_{-i,h})]\).

With the help of such an extended MDP, we can develop Algorithm 6, which is a CE version of Algorithm 5 with the following guarantees.

**Theorem J.6**.: Fix any \(\epsilon,\delta\in(0,1)\) and \(\pi\in\Pi_{\mathcal{S}}\). Algorithm 6 can learn a decoding function \(\widehat{g}\) such that

\[\max_{i\in[n],m_{i}\in\mathcal{M}_{i},j\in[n],h\in[H]}\hskip-10.0pt\mathbb{P} ^{(m_{i}\circ\pi_{i})\odot\pi_{-i},\mathcal{G}}(s_{h}\neq\widehat{g}_{j,h}(c_{ h},p_{j,h}))\leq\epsilon,\]

with total sample complexity \(\widetilde{\mathcal{O}}(\frac{nS^{2}A^{3}HO+nS^{3}A^{4}H}{\epsilon^{2}}+ \frac{S^{4}A^{6}H^{5}}{\epsilon})\) and computational complexity \(\textsc{poly}(S,A,H,O,\frac{1}{\epsilon})\).

Proof.: Due to the construction of \(\mathcal{M}_{i}^{\text{extended}}(\pi)\), the proof of Theorem J.4 readily applies, where the only difference is that the state space of \(\mathcal{M}_{i}^{\text{extended}}(\pi)\) is now \(SA_{i}\), larger than that of \(\mathcal{M}(\pi_{-i})\) by a factor of \(A_{i}\) thus proving our theorem. 

We next introduce and prove several supporting lemmas used before.

**Lemma J.7**.: Suppose we can sample from a joint distribution \(P\in\Delta(\mathcal{X}\times\mathcal{Y})\) for some finite \(\mathcal{X}\), \(\mathcal{Y}\) i.i.d. Then we can learn an approximate distribution \(Q\in\Delta(\mathcal{X}\times\mathcal{Y})\) with sample complexity \(\Theta\left(\frac{|\mathcal{X}||\mathcal{Y}|+\log 1/\delta}{\epsilon^{2}}\right)\) such that

\[\mathbb{E}_{x\sim P}\sum_{y\in\mathcal{Y}}|P(y\,|\,x)-Q(y\,|\,x)|\leq 2\sum_{x \in\mathcal{X},y\in\mathcal{Y}}|P(x,y)-Q(x,y)|\leq\epsilon,\]

with probability \(1-\delta\).

Proof.: Note the following holds

\[\sum_{x\in\mathcal{X},y\in\mathcal{Y}}|P(x,y)-Q(x,y)| =\sum_{x\in\mathcal{X},y\in\mathcal{Y}}|P(x,y)-P(x)Q(y\,|\,x)+P(x )Q(y\,|\,x)-Q(x,y)|\] \[\geq\sum_{x\in\mathcal{X},y\in\mathcal{Y}}|P(x,y)-P(x)Q(y\,|\,x)|-| P(x)Q(y\,|\,x)-Q(x,y)|.\]

Therefore, we have

\[\mathbb{E}_{x\sim P}\sum_{y\in\mathcal{Y}}|P(y\,|\,x)-Q(y\,|\,x)| \leq\sum_{x\in\mathcal{X},y\in\mathcal{Y}}|P(x,y)-Q(x,y)|+\sum_{x \in\mathcal{X},y\in\mathcal{Y}}Q(y\,|\,x)|P(x)-Q(x)|\] \[\leq 2\sum_{x\in\mathcal{X},y\in\mathcal{Y}}|P(x,y)-Q(x,y)|.\] (J.2)

By the sample complexity of learning discrete distributions [13], we can learn \(Q\) such that \(\sum_{x\in\mathcal{X},y\in\mathcal{Y}}|P(x,y)-Q(x,y)|\leq\epsilon\) in sample complexity \(\Theta\left(\frac{|\mathcal{X}||\mathcal{Y}|+\log 1/\delta}{\epsilon^{2}}\right)\) with probability \(1-\delta\). Thus, we proved our lemma. 

**Lemma J.8** (Concentration on transition).: Fix \(\delta>0\) and dataset \(\{\overline{\tau}_{H}^{k}\}_{k\in[N]}\) sampled from \(\mathcal{P}\) (or \(\mathcal{G}\) in the multi-agent setting) under policy \(\pi\in\Pi^{\text{gen}}\). We define for each \(h\in[H]\), \((s_{h},a_{h},s_{h+1})\in\mathcal{S}\times\mathcal{A}\times\mathcal{S}\)

\[N_{h}(s_{h},a_{h})=\sum_{k\in[N]}\mathbbm{1}[s_{h}^{k}=s_{h},a_{h}^{k}=a_{h}], \qquad N_{h}(s_{h},a_{h},s_{h+1})=\sum_{k\in[N]}\mathbbm{1}[s_{h}^{k}=s_{h},a_ {h}^{k}=a_{h},s_{h+1}^{k}=s_{h}].\]

Then, with probability at least \(1-\delta\), it holds that for any \(k\in[K],h\in[H],s_{h}\in\mathcal{S},a_{h}\in\mathcal{A}\):

\[\|\mathbb{T}_{h}(\cdot\,|\,s_{h},a_{h})-\widehat{\mathbb{T}}_{h}(\cdot\,|\,s_{h },a_{h})\|_{1}\leq C_{1}\sqrt{\frac{S\log(SAHK/\delta)}{\max\{N_{h}(s_{h},a_{h}), 1\}}},\]

for some absolute constant \(C_{1}>0\), where we define \(\widehat{\mathbb{T}}_{h}(s_{h+1}\,|\,s_{h},a_{h})=\frac{N_{h}(s_{h},a_{h},s_{h+1 })}{\max\{N_{h}(s_{h},a_{h}),1\}}\).

Proof.: This is done by firstly bounding \(\|\mathbb{T}_{h}(\cdot\,|\,s_{h},a_{h})-\widehat{\mathbb{T}}_{h}(\cdot\,|\,s_{h},a _{h})\|_{1}\) for specific \(k,h,s_{h},a_{h}\) according to [13], and then taking union bound for all \(k\in[K],h\in[H],s_{h}\in\mathcal{S},a_{h}\in\mathcal{A}\). 

**Lemma J.9** (Concentration on emission).: Fix \(\delta>0\) and dataset \(\{\overline{\pi}_{H}^{k}\}_{k\in[N]}\) sampled from \(\mathcal{P}\) (or \(\mathcal{G}\) in the multi-agent setting) under some policy \(\pi\in\Pi^{\text{sen}}\). We define for each \(h\in[H]\), \((s_{h},o_{h})\in\mathcal{S}\times\mathcal{O}\)

\[N_{h}(s_{h},o_{h})=\sum_{k\in[N]}\mathbbm{1}[s_{h}^{k}=s_{h},o_{h}^{k}=o_{h}], \qquad N_{h}(s_{h})=\sum_{k\in[N]}\mathbbm{1}[s_{h}^{k}=s_{h}].\]

Then, with probability at least \(1-\delta\), it holds that

\[\|\mathbb{O}_{h}(\cdot\,|\,s_{h})-\widehat{\mathbb{O}}_{h}(\cdot\,|\,s_{h})\| _{1}\leq C_{2}\sqrt{\frac{O\log(SHK/\delta)}{\max\{N_{h}^{k}(s_{h}),1\}}},\]

for some absolute constant \(C_{2}>0\), where we define \(\widehat{\mathbb{O}}_{h}(o_{h}\,|\,s_{h})=\frac{N_{h}(s_{h},o_{h})}{\max\{N_{ h}(s_{h}),1\}}\).

Proof.: This is done by firstly bounding \(\|\mathbb{O}_{h}(\cdot\,|\,s_{h})-\widehat{\mathbb{O}}_{h}(\cdot\,|\,s_{h})\| _{1}\) for specific \(k,h,s_{h}\) according to [13], and then taking union bound for all \(k\in[K],h\in[H],s_{h}\in\mathcal{S}\). 

Now we switch to proving the guarantees for Algorithm 7.

**Lemma J.10**.: Fix \(\delta>0\). With probability \(1-\delta\), it holds that for any \(k\in[K],h\in[H],s_{h}\in\mathcal{S}\):

\[\sum_{o_{h+1}}\Big{|}\mathbbm{P}^{\mathcal{G}}(o_{h+1}\,|\,s_{h},a_{h})- \widehat{\mathbb{J}}_{h}^{k}(o_{h+1}\,|\,s_{h},a_{h})\Big{|}\leq C_{3}\sqrt{ \frac{O\log(SHKA/\delta)}{N_{h}^{k}(s_{h},a_{h})}},\]

where \(\widehat{\mathbb{J}}_{h}^{k}\) is defined in Algorithm 7.

Proof.: This is done by firstly bounding \(\sum_{o_{h+1}}\Big{|}\mathbbm{P}^{\mathcal{G}}(o_{h+1}\,|\,s_{h},a_{h})- \widehat{\mathbb{J}}_{h}^{k}(o_{h+1}\,|\,s_{h},a_{h})\Big{|}\) for specific \(k,h,s_{h},a_{h}\) according to [13], and then taking union bound for all \(k\in[K],h\in[H],s_{h}\in\mathcal{S},a_{h}\in\mathcal{A}\). 

From now on, we shall use the bonus

\[b_{h}^{k}(s_{h},a_{h})=\min\left\{C_{3}(H-h)\sqrt{\frac{O\log(SAHK/\delta)}{ \max\{N_{h}^{k}(s_{h},a_{h}),1\}}},2(H-h)\right\}\] (J.3)

in Algorithm 7, for some absolute constant \(C_{3}>0\).

Before presenting our technical analysis, we define the following notation for the ease of presentation. We define the following approximate value functions for any policy \(\pi\in\Pi\) in a backward way for \(h\in[H]\) when given some approximate belief in the form of \(\{\widehat{P}_{h}:\widehat{\mathbb{C}}_{h}\to\Delta(\mathcal{P}_{h}\times \mathcal{S})\}_{h\in[H]}\) as discussed in Section 7.2:

\[\widehat{V}_{i,h}^{\pi,\mathcal{G}}(c_{h}) :=\mathbb{E}_{s_{h},p_{h}\sim\widehat{P}_{h}(\cdot\,|\,\widehat{c} _{h})}\mathbb{E}_{\omega_{h},\{a_{j,h}\sim\pi_{j,h}(\cdot\,|\,\omega_{j,h},c_{h },p_{j,h})\}_{j\in[n]}}\] \[\mathbb{E}_{s_{h+1}\sim\tau_{h}(\cdot\,|\,s_{h},a_{h}),o_{h+1} \sim o_{h+1}(\cdot\,|\,s_{h+1})}\left[r_{i,h}(s_{h},a_{h})+V_{i,h+1}^{\pi, \mathcal{G}}(c_{h+1})\right],\] \[\widehat{\mathbb{G}}_{i,h}^{\pi,\mathcal{G}}(c_{h},\gamma_{h}) :=\mathbb{E}_{s_{h},p_{h}\sim\widehat{P}_{h}(\cdot\,|\,\widehat{c} _{h})}\mathbb{E}_{\{a_{j,h}\sim\gamma_{j,h}(\cdot\,|\,p_{j,h})\}_{j\in[n]}}\] \[\mathbb{E}_{s_{h+1}\sim\tau_{h}(\cdot\,|\,s_{h},a_{h}),o_{h+1} \sim o_{h+1}(\cdot\,|\,s_{h+1})}\left[r_{i,h}(s_{h},a_{h})+V_{i,h+1}^{\pi, \mathcal{G}}(c_{h+1})\right],\]

for each \((i,c_{h})\in[n]\times\mathcal{C}_{h}\) and \(\gamma_{h}\in\Gamma_{h}\), where we define \(\widehat{V}_{i,H+1}^{\pi,\mathcal{G}}(c_{H+1})=0\).

Intuitively, this definition of \(\widehat{V}_{i,h}^{\pi,\mathcal{G}}(c_{h})\) mimics the Bellman equation of the ground-truth value function \(V_{i,h}^{\pi,\mathcal{G}}(c_{h})\) by replacing the ground-truth belief \(\mathbbm{P}^{\mathcal{G}}(s_{h},p_{h}\,|\,c_{h})\) by \(\widehat{P}_{h}(s_{h},p_{h}\,|\,\widehat{c}_{h})\). Next, we point out the following quantitative bound when using \(\widehat{V}_{i,h}^{\pi,\mathcal{G}}(c_{h})\) to approximate \(V_{i,h}^{\pi,\mathcal{G}}(c_{h})\).

**Lemma J.11**.: For any \(\pi^{\prime},\pi\in\Pi\), it holds that

\[\mathbb{E}_{\pi^{\prime}}^{\mathcal{G}}|V_{i,h}^{\pi,\mathcal{G}}(c_{h})-\widehat {V}_{i,h}^{\pi,\mathcal{G}}(c_{h})\big{|}\leq(H-h+1)^{2}\epsilon_{\text{belief}},\]

where

\[\epsilon_{\text{belief}}:=\max_{h\in[H]}\max_{\pi\in\Pi}\mathbb{E}_{\pi}^{ \mathcal{G}}\left\|\mathbb{P}^{\mathcal{G}}(\cdot,\cdot|\,c_{h})-\widehat{P}_ {h}(\cdot,\cdot|\,\widehat{c}_{h})\right\|_{1}.\]

Proof.: It follows directly by combining Lemma 4 and Lemma 8 of [51]. 

Meanwhile, note that although in Algorithm 7, the value functions we maintain have input \(\widehat{c}_{h}\) instead of \(c_{h}\) for computational efficiency, we extend the definitions of those value functions to also accept \(c_{h}\) as inputs as follows (with a slight abuse of notation):

\[Q_{i,h}^{\text{high},k}(c_{h},\gamma_{h}) :=Q_{i,h}^{\text{high},k}(\widehat{c}_{h},\gamma_{h}), Q_{i,h}^{\text{high},k}(c_{h},p_{h},s_{h},a_{h}):=Q_{i,h}^{\text{ high},k}(\widehat{c}_{h},p_{h},s_{h},a_{h}), V_{i,h}^{\text{high},k}(c_{h}):=V_{i,h}^{\text{high},k}(\widehat{c}_{h})\] \[Q_{i,h}^{\text{low},k}(c_{h},\gamma_{h}) :=Q_{i,h}^{\text{low},k}(\widehat{c}_{h},\gamma_{h}), Q_{i,h}^{\text{low},k}(c_{h},p_{h},s_{h},a_{h}):=Q_{i,h}^{\text{low},k}( \widehat{c}_{h},p_{h},s_{h},a_{h}), V_{i,h}^{\text{low},k}(c_{h}):=V_{i,h}^{\text{low},k}(\widehat{c}_{h}),\]

where we recall that \(\widehat{c}_{h}=\text{Compress}_{h}(c_{h})\).

**Lemma J.12** (Optimism 1 for NE/CCE).: With probability \(1-\delta\), for any \(k\in[K]\), for Algorithm 7, it holds that for any \(i\in[n]\), \(\pi^{\prime}_{i}\in\Pi_{i}\), \(h\in[H]\)

\[Q_{i,h}^{\text{high},k}(\widehat{c}_{h},\gamma_{h})\geq\widehat{Q}_{i,h}^{ \pi^{\prime}_{i}\times\pi^{k}_{-i},\mathcal{G}}(c_{h},\gamma_{h}), V_{i,h}^{\text{high},k}(\widehat{c}_{h})\geq\widehat{V}_{i,h}^{\pi^{\prime}_{i} \times\pi^{k}_{-i},\mathcal{G}}(c_{h}),\]

where we recall that \(\widehat{c}_{h}=\text{Compress}_{h}(c_{h})\).

Proof.: We will prove by backward induction. Obviously, it holds for \(h=H+1\). Now we assume

\[Q_{i,h}^{\text{high},k}(c_{h},\gamma_{h})=\mathbb{E}_{s_{h},p_{h }\sim\widehat{P}_{h}(\cdot,\cdot|\,\widehat{c}_{h})}\mathbb{E}_{\{a_{j,h} \sim\gamma_{j,h}(\cdot|\,p_{j,h})\}_{j\in[n]}}\left[Q_{i,h}^{\text{high},k}(c_ {h},p_{h},s_{h},a_{h})\right]\] \[=\mathbb{E}_{s_{h},p_{h}\sim\widehat{P}_{h}(\cdot,\cdot|\, \widehat{c}_{h})}\mathbb{E}_{\{a_{j,h}\sim\gamma_{j,h}(\cdot|\,p_{j,h})\}_{j \in[n]}}\min\{r_{i,h}(s_{h},a_{h})+b_{h}^{k-1}(s_{h},a_{h})\] \[\qquad\qquad\qquad\qquad+\mathbb{E}_{o_{h+1}\sim\widehat{J}_{h}^ {k-1}(\cdot|\,s_{h},a_{h})}\left[V_{i,h+1}^{\text{high},k}(c_{h+1})\right],H-h+1\}\] \[\geq\mathbb{E}_{s_{h},p_{h}\sim\widehat{P}_{h}(\cdot,\cdot|\,s_{h })}\mathbb{E}_{\{a_{j,h}\sim\gamma_{j,h}(\cdot|\,p_{j,h})\}_{j\in[n]}}\] \[\qquad\qquad\min\left\{r_{i,h}(s_{h},a_{h})+b_{h}^{k-1}(s_{h},a_{h })+\mathbb{E}_{o_{h+1}\sim\widehat{J}_{h}^{k-1}(\cdot|\,s_{h},a_{h})}\left[ \widehat{V}_{i,h+1}^{\pi^{\prime}_{i}\times\pi^{k}_{-i},\mathcal{G}}(c_{h+1}) \right],H-h+1\right\},\]

where the last step is by inductive hypothesis. Now note that for any \((s_{h},p_{h},a_{h})\), we have

\[b_{h}^{k-1}(s_{h},a_{h})+\mathbb{E}_{o_{h+1}\sim\widehat{J}_{h}^ {k-1}(\cdot|\,s_{h},a_{h})}\left[\widehat{V}_{i,h+1}^{\pi^{\prime}_{i}\times \pi^{k}_{-i},\mathcal{G}}(c_{h+1})\right]\] \[\geq b_{h}^{k-1}(s_{h},a_{h})-(H-h)\|\widehat{J}_{h}^{k-1}(\cdot| \,s_{h},a_{h})-\mathbb{P}^{\mathcal{G}}(\cdot|\,s_{h},a_{h})\|_{1}+\mathbb{E}_{s _{h+1}\sim\mathbb{T}_{h}(\cdot|\,s_{h},a_{h}),o_{h+1}\sim\mathbb{O}_{h+1}( \cdot|\,s_{h+1})}\left[\widehat{V}_{i,h+1}^{\pi^{\prime}_{i}\times\pi^{k}_{-i}, \mathcal{G}}(c_{h+1})\right]\] \[\geq\mathbb{E}_{s_{h+1}\sim\mathbb{T}_{h}(\cdot|\,s_{h},a_{h}),o _{h+1}\sim\mathbb{O}_{h+1}(\cdot|\,s_{h+1})}\left[\widehat{V}_{i,h+1}^{\pi^{ \prime}_{i}\times\pi^{k}_{-i},\mathcal{G}}(c_{h+1})\right],\]

where we notice \(\mathbb{P}^{\mathcal{G}}(o_{h+1}|\,s_{h},a_{h})=\sum_{s_{h+1}}\mathbb{O}_{h+1}( o_{h+1}\,|\,s_{h+1})\mathbb{T}_{h}(s_{h+1}\,|\,s_{h},a_{h})\) for the first inequality, and the second inequality comes from the construction of our bonus \(b_{h}^{k-1}(s_{h},a_{h})\) in Equation (J.3) and Lemma J.10. Meanwhile, by the definition of value functions, it holds that \(\mathbb{E}_{s_{h+1}\sim\mathbb{T}_{h}(\cdot|\,s_{h},a_{h}),o_{h+1}\sim\mathbb{O} _{h+1}(\cdot|\,s_{h+1})}\left[\widehat{V}_{i,h+1}^{\pi^{\prime}_{i}\times\pi^{ k}_{-i},\mathcal{G}}(c_{h+1})\right]\leq H-h\). Therefore, we have

\[\min \left\{r_{i,h}(s_{h},a_{h})+b_{h}^{k-1}(s_{h},a_{h})+\mathbb{E}_{ o_{h+1}\sim\widehat{J}_{h}^{k-1}(\cdot|\,s_{h},a_{h})}\left[V_{i,h+1}^{\pi^{ \prime}_{i}\times\pi^{k}_{-i},\mathcal{G}}(c_{h+1})\right],H-h+1\right\}\] \[\geq r_{i,h}(s_{h},a_{h})+\mathbb{E}_{s_{h+1}\sim\mathbb{T}_{h}( \cdot|\,s_{h},a_{h}),o_{h+1}\sim\mathbb{O}_{h+1}(\cdot|\,s_{h+1})}\left[ \widehat{V}_{i,h+1}^{\pi^{\prime}_{i}\times\pi^{k}_{-i},\mathcal{G}}(c_{h+1}) \right].\]

[MISSING_PAGE_EMPTY:57]

[MISSING_PAGE_EMPTY:58]

[MISSING_PAGE_FAIL:59]

Since \(|Z_{k,h}^{1}|\leq H\), \(|Z_{k,h}^{2}|\leq H\), and \(\epsilon_{h}(c_{h}^{k})\leq 2\), by Azuma-Hoeffding bound, we conclude with probability \(1-3\delta\), the following holds

\[\sum_{k,h}Z_{k,h}^{1} \leq\mathcal{O}(H\sqrt{HK\log\frac{1}{\delta}}), \sum_{k,h}Z_{k,h}^{2}\leq\mathcal{O}(H\sqrt{HK\log\frac{1}{\delta}}),\] \[\sum_{k,h}\epsilon_{h}(c_{h}^{k}) \leq\sum_{k}\mathbb{E}_{\pi^{k}}^{\mathcal{G}}\left[\sum_{h} \epsilon_{h}(c_{h})\right]+\mathcal{O}(\sqrt{HK\log\frac{1}{\delta}})\leq KH \epsilon_{\text{belief}}+\mathcal{O}(\sqrt{HK\log\frac{1}{\delta}}).\]

Meanwhile, by the pigeonhole principle, it holds that

\[\sum_{k,h}b_{h}^{k-1}(s_{h}^{k},a_{h}^{k})\leq H\sqrt{O\log(SAHK/ \delta)}\sum_{k,h}\frac{1}{\sqrt{\max\{1,N_{h}^{k-1}(s_{h}^{k},a_{h}^{k})\}}}\] \[\leq\mathcal{O}\left(H\sqrt{O\log(SAHK/\delta)}(H\sqrt{SAK}+HSA) \right).\]

Now by Lemma J.12 and Lemma J.14 and putting everything together, we conclude

\[\sum_{k\in[K]}\max_{i\in[n]}\left(\max_{\pi_{i}^{\prime}\in\Pi_{ i}}\widehat{V}_{i,1}^{\pi_{i}^{\prime}\times\pi_{-i},\mathcal{G}}(c_{1}^{k})- \widehat{V}_{i,1}^{\pi,\mathcal{G}}(c_{1}^{k})\right)\leq KH^{2}\epsilon_{ \text{belief}}+\mathcal{O}(H^{2}\sqrt{SAOK\log(SAHK/\delta)}+H^{2}SA\sqrt{O \log(SAHK/\delta)}).\]

Now by Lemma J.11, we proved the regret guarantees as follows

\[\sum_{k\in[K]}\max_{i\in[n]}\left(\max_{\pi_{i}^{\prime}\in\Pi_{ i}}V_{i,1}^{\pi_{i}^{\prime}\times\pi_{-i}^{k},\mathcal{G}}(c_{1}^{k})-V_{i,1}^{ \pi_{i}^{k},\mathcal{G}}(c_{1}^{k})\right)\] \[\leq\mathcal{O}(KH^{2}\epsilon_{\text{belief}}+H^{2}\sqrt{SAOK \log(SAHK/\delta)}+H^{2}SA\sqrt{O\log(SAHK/\delta)}).\]

For the PAC guarantees, since we define \(k^{*}\in\arg\min_{i\in[n],k\in[K]}V_{i,1}^{\text{high},k}(c_{1}^{k})-V_{i,1}^{ \text{low},k^{*}}(c_{1}^{k})\), we have

\[\text{CCE-gap}(\pi^{k^{*}}) \leq\mathcal{O}(H^{2}\epsilon_{\text{belief}})+\max_{i\in[n]} \left(V_{i,h}^{\text{high},k^{*}}(c_{1}^{k^{*}})-V_{i,h}^{\text{low},k^{*}}(c_{ 1}^{k^{*}})\right)\] \[\leq\mathcal{O}(H^{2}\epsilon_{\text{belief}})+\left(\frac{1}{K }\sum_{k\in[K]}V_{i,h}^{\text{high},k}(c_{1}^{k})-V_{i,h}^{\text{low},k}(c_{1} ^{k})\right)\] \[\leq\mathcal{O}(H^{2}\epsilon_{\text{belief}}+H^{2}\sqrt{SAO \log(SAHK/\delta)/K}+\frac{H^{2}SA}{K}\sqrt{O\log(SAHK/\delta)}).\]

Finally, for two-player zero-sum games, we denote \(\widehat{\pi}^{k^{*}}\) to be the marginalized policy of \(\pi^{k^{*}}\). Then we have

\[\text{NE-gap}(\widehat{\pi}^{k^{*}})\leq\text{CCE-gap}(\pi^{k^{*}}),\]

thus concluding our theorem. 

**Theorem J.16** (CE version).: With probability \(1-\delta\), Algorithm 7 enjoys the regret guarantee of

\[\sum_{k\in[K]}\max_{i\in[n]}\left(\max_{m_{i}^{\prime}\in \mathcal{M}_{i}}V_{i,1}^{(m_{i}^{\prime}\circ\pi_{i}^{k})\odot\pi_{-i}^{k}, \mathcal{G}}(c_{1}^{k})-V_{i,1}^{\pi^{k},\mathcal{G}}(c_{1}^{k})\right)\] \[\leq\mathcal{O}(KH^{2}\epsilon_{\text{belief}}+H^{2}\sqrt{SAOK \log(SAHK/\delta)}+H^{2}SA\sqrt{O\log(SAHK/\delta)}).\]

Correspondingly, this implies that one can learn an \((\epsilon+H^{2}\epsilon_{\text{belief}})\)-CE with sample complexity \(\mathcal{O}(\frac{H^{4}SAO\log(SAHO/\delta)}{\epsilon^{2}})\) and computation complexity \(\text{poly}(S,\max_{h\in[H]}|\widehat{\mathcal{C}}_{h}|,\max_{h\in[H]}| \mathcal{P}_{h}|,H,\frac{1}{\epsilon},\log\frac{1}{\delta})\)

Proof.: Then proof follows as that of Theorem J.15, where we only need to change the first step of the proof as

\[\widehat{V}_{i,h}^{\pi_{i}^{\prime}\times\pi_{-i}^{k},\mathcal{G}}(c_{h}^{k})- \widehat{V}_{i,h}^{\pi^{k},\mathcal{G}}(c_{h}^{k})\leq V_{i,h}^{\text{high},k}(c_ {h}^{k})-V_{i,h}^{\text{low},k}(c_{h}^{k}),\]

by Lemma J.13 and Lemma J.14, and the remaining steps are exactly the same.

**Lemma J.17** (Adapted from Theorem H.5).: Algorithm 8 can learn the approximate POMDP with transition \(\widehat{\mathbb{T}}_{1:H}\) and emission \(\widehat{\mathbb{O}}_{1:H}\) such that for any policy \(\pi\in\Pi^{\text{gen}}\) and \(h\in[H]\)

\[\mathbb{E}_{\pi}^{\mathcal{G}}\left[\|\mathbb{T}_{h}(\cdot\,|\,s_{h},a_{h})- \widehat{\mathbb{T}}_{h}(\cdot\,|\,s_{h},a_{h})\|_{1}+\|\mathbb{O}_{h}(\cdot \,|\,s_{h})-\widehat{\mathbb{O}}_{h}(\cdot\,|\,s_{h})\|_{1}\right]\leq \mathcal{O}(\epsilon),\]

using sample complexity \(\widetilde{\mathcal{O}}(\frac{S^{2}AHO+S^{3}AH}{\epsilon^{2}}+\frac{S^{4}A^{2 }H^{5}}{\epsilon})\) with probability \(1-\delta\).

Proof.: Note that Algorithm 8 is essentially treating the POSG \(\mathcal{G}\) as a centralized MDP and running Algorithm 4, where the only modifications we make in Algorithm 8 is that we take the controller set (see some examples of the controller set in Appendix C.3) into considerations when learning the models. Specifically, for the transition \(\widehat{\mathbb{T}}_{h}\), what we estimate is only \(\widehat{\mathbb{T}}_{h}(s_{h+1}\,|\,s_{h},a_{\mathcal{I}_{h},h})\) instead of \(\widehat{\mathbb{T}}_{h}(s_{h+1}\,|\,s_{h},a_{h})\), where \(\mathcal{T}_{h}\subseteq[n]\) is the controller set. Therefore, the sample complexity of Algorithm 8 will not be worse than that of Algorithm 4. 

**Proof of Theorem 7.7:**

Note that the proof idea essentially resembles that of Theorem H.6, where we construct the model \(\mathcal{G}^{\text{trunc}}\) for \(\mathcal{G}\) in exactly the same way as constructing \(\mathcal{P}^{\text{trunc}}\) for \(\mathcal{P}\). Therefore, by Corollary H.10, we have

\[\mathbb{E}_{\pi}^{\mathcal{G}}[\|\mathbb{P}^{\mathcal{G}}(\cdot,\cdot\,|\,c_{h })-\mathbb{P}^{\mathcal{G}^{\text{trunc}}}(\cdot,\cdot\,|\,c_{h})\|_{1}]\leq 2 \sum_{\tau_{H}}|\mathbb{P}^{\pi,\mathcal{G}}(\overline{\tau}_{H})-\mathbb{P}^{ \pi,\mathcal{G}^{\text{trunc}}}(\overline{\tau}_{H})|\leq 4HS\epsilon_{1}.\]

Meanwhile, we can construct \(\widehat{\mathcal{G}}^{\text{trunc}}\) and \(\widehat{\mathcal{G}}^{\text{sub}}\) using exactly the same way as for \(\widehat{\mathcal{P}}^{\text{trunc}}\) and \(\widehat{\mathcal{P}}^{\text{sub}}\), where \(\widehat{\mathcal{G}}^{\text{sub}}\) is a \(\gamma/2\)-observable POSG.

Now, according to [51], for all the examples in Appendix C.3, there exists a compression function that maps \(c_{h}\) to \(\widehat{c}_{h}\) such that the size of the compressed common information is quasi-polynomial, i.e., \(\widehat{C}_{h}\leq(AO)^{C_{\gamma^{-4}}\log\frac{\beta H}{\epsilon_{2}}}\) for some absolute constant \(C\) and \(\epsilon_{2}\in(0,1)\), and the corresponding approximate belief \(\{\widehat{P}_{h}:\widehat{\mathcal{C}}_{h}\to\Delta(\mathcal{S}\times \mathcal{P}_{h})\}_{h\in[H]}\) satisfies that

\[\mathbb{E}_{\pi}^{\widehat{\mathcal{G}}^{\text{sub}}}\|\mathbb{P}^{\widehat {\mathcal{G}}^{\text{sub}}}(\cdot,\cdot\,|\,c_{h})-\widetilde{P}_{h}(\cdot, \cdot\,|\,\widehat{c}_{h})\|_{1}\leq\epsilon_{2}.\]

Therefore, we can do the same augmentation for \(\widetilde{P}_{h}\) on states from \(\mathcal{S}_{h}^{\text{low}}\) to construct the approximate belief \(\widehat{P}_{h}\) as in the proof of Theorem H.6, and the remaining steps follow from the proof of Theorem H.6. This will lead to a total of polynomial-time and polynomial-sample complexities. 

### Background on Bayesian Games

The Bayesian game is a generalization of normal-form games in partially observable settings. Specifically, a Bayesian game is specified as \((n,\{\mathcal{A}_{i}\}_{i\in[n]},\{\Theta_{i}\}_{i\in[n]},\{r_{i}\}_{i\in[n]},\mu)\), where \(n\) is the number of players, \(\mathcal{A}_{i}\) is the actor space, \(\Theta_{i}\) is the type space, \(r_{i}:\times_{i\in[n]}(\Theta_{i}\times\mathcal{A}_{i})\to[0,1]\) is the reward function, and \(\mu\) is the prior distribution of the joint type. At the beginning of the game, a type \(\theta=(\theta_{i})_{i\in[n]}\) is drawn from the prior distribution \(\mu\in\Delta(\Theta)\). Then each agent \(i\) gets its own type \(\theta_{i}\) and takes the action \(a_{i}\). We define a pure strategy of an agent as \(s_{i}\in\text{ST}_{i}:=\{\Theta_{i}\to\mathcal{A}_{i}\}\). We define \(J_{i}(s_{i},s_{-i})\) to be the expected rewards for agent \(i\), given the pure joint strategy \((s_{i},s_{-i})\).

By definition, \(J_{i}(s_{i},s_{-i})\) can be evaluated as

\[J_{i}(s_{i},s_{-i}):=\mathbb{E}_{\theta\sim\mu}r_{i}(\theta,s_{i}(\theta_{i}), s_{-i}(\theta_{-i})).\]

Bayesian NE.We define \(\gamma^{\star}\in\times_{i\in[n]}\Delta(\text{ST}_{i})\) is an \(\epsilon\)-NE is it satisfies that

\[\mathbb{E}_{s\sim\gamma^{\star}}J_{i}(s)\geq\mathbb{E}_{s\sim\gamma^{\star}}J_ {i}(s^{\prime}_{i},s_{-i})-\epsilon,\quad\forall i\in[n],s^{\prime}_{i}\in \text{ST}_{i}.\]

Bayesian CCE.We say a distribution of joint strategies \(\gamma^{\star}\in\Delta(\times_{i\in[n]}\text{ST}_{i})\) to be a \(\epsilon\)-Bayesian CCE if it satisfies

\[\mathbb{E}_{s\sim\gamma^{\star}}J_{i}(s)\geq\mathbb{E}_{s\sim\gamma^{\star}}J_ {i}(s^{\prime}_{i},s_{-i})-\epsilon,\quad\forall i\in[n],s^{\prime}_{i}\in\text{ ST}_{i}.\](Agent-form) Bayesian CE.We say a distribution of joint strategies \(\gamma^{\star}\in\Delta(\times_{i\in[n]}\text{ST}_{i})\) to be an \(\epsilon\)-agent-form Bayesian CE if it satisfies

\[\mathbb{E}_{s\sim\gamma^{\star}}\mathcal{J}_{i}(s)\geq\mathbb{E}_{s\sim\gamma^{ \star}}\mathcal{J}_{i}(m_{i}\diamond s_{i},s_{-i})-\epsilon,\quad\forall i\in[n ],m_{i}^{\prime}\in\mathcal{M}_{i},\]

where \(\mathcal{M}_{i}=\{\Theta_{i}\times\mathcal{A}_{i}\rightarrow\mathcal{A}_{i}\}\) is the space for strategy modification, where \(m_{i}\) modifies \(s_{i}\) as follows: given current type \(\theta_{i}\) and the recommended action \(a_{i}\), the strategy modification changes the action to the another action \(m_{i}(\theta_{i},a_{i})\).

Note that Bayesian NE for zero-sum games, and (agent-form) Bayesian CE/CCE are all tractable solution concepts and can be computed with polynomial computational complexity, e.g., [29, 32, 23].

## Appendix K Concluding Remarks and Limitations

In this paper, we aim to understand the provable benefits of privileged information for partially observable RL problems under two empirically successful paradigms, _expert distillation_[14, 64, 58] and _asymmetric actor-critic_[68, 7, 3], which represent privileged _policy_ and privileged _value_ learning, respectively, with an emphasis on studying _both_ the computational and sample efficiencies of the algorithms. Our results (as summarized in Table 1) showed that privileged information does improve learning efficiency in a series of known POMDP subclasses. One potential limitation of our work is that we only focused on the case with _exact_ state information. It remains to explore whether such an assumption can be further relaxed, e.g., when privileged state information may be biased, partially observable, or delayed, as usually happens in practice, and how our theoretical results may be affected. Meanwhile, as an initial theoretical study, we have been primarily focusing on the tabular settings (except Appendix G), and it would be interesting to extend the results to function-approximation settings to handle massively large state, action, and observation spaces in practice.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our abstract indeed accurately summarizes out paper's contribution and scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We clearly outline all of our assumption needed to derive our theoretical results. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: We prove every theorem or lemma in this paper.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Yes, detailed are introduced in Appendix I Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?

[MISSING_PAGE_FAIL:65]

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We have provides all details in Appendix I. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: It does. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We address the societal impacts of our paper in Appendix A. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: There is no such risk. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: No external codes, data, models are used. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We do not release any new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: No such experiments were involved. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: No such approval was needed. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.