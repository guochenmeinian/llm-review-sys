# Fast Trac:

A Parameter-Free Optimizer for

Lifelong Reinforcement Learning

 Aneesh Muppidi

Harvard College

aneeshmuppidi@college.harvard.edu

&Zhiyu Zhang

Harvard University

zhiyuz@seas.harvard.edu

&Heng Yang

Harvard University

hankyang@seas.harvard.edu

###### Abstract

A key challenge in lifelong reinforcement learning (RL) is the loss of plasticity, where previous learning progress hinders an agent's adaptation to new tasks. While regularization and resetting can help, they require precise hyperparameter selection at the outset and environment-dependent adjustments. Building on the principled theory of online convex optimization, we present a parameter-free optimizer for lifelong RL, called Trac, which requires no tuning or prior knowledge about the distribution shifts. Extensive experiments on Procgen, Atari, and Gym Control environments show that Trac works surprisingly well--mitigating loss of plasticity and rapidly adapting to challenging distribution shifts--despite the underlying optimization problem being nonconvex and nonstationary. Project website and code is available here.

## 1 Introduction

Spot, the agile robot dog, has been learning to walk confidently across soft, lush grass. But when Spot moves from the grassy field to a gravel surface, the small stones shift beneath her feet, causing her to stumble. When Spot tries to walk across a sandy beach or on ice, the challenges multiply, and her once-steady walk becomes erratic. Spot wants to adjust quickly to these new terrains, but the patterns she learned on grass are not suited to gravel, sand, or ice. Furthermore, she never knows when the terrain will change again and how different it will be, therefore must continually plan for the unknown while avoiding reliance on outdated experiences.

Spot's struggle exemplifies a well-known and extensively studied challenge in real-world decision making: _lifelong reinforcement learning_ (lifelong RL) Abel et al. (2024); Nath et al. (2023); Mendez et al. (2020); Xie and Finn (2022). In lifelong RL, the learning agent must continually acquire new knowledge to adapt to the nonstationarity of the environment. At first glance, there appears to be

Figure 1: Severe loss of plasticity in Procgen (Starpilot). There is a steady decline in reward with each distribution shift.

an obvious solution: given a policy gradient oracle, the agent could just keep running gradient descent nonstop. However, recent experiments have demonstrated an intriguing behavior called _loss of plasticity_(Dohare et al., 2021; Lyle et al., 2022; Abbas et al., 2023; Sokar et al., 2023): despite persistent gradient steps, such an agent can gradually lose its responsiveness to incoming observations. There are even extreme cases of loss of plasticity (known as _negative transfer_ or _primacy bias_), where prior learning can significantly hamper the performance in new tasks (Nikishin et al., 2022; Ahn et al., 2024); see Figure 1 for an example. All these suggest that the problem is more involved than one might think.

From the optimization perspective, the above issues might be attributed to the _lack of stability_ under gradient descent. That is, the weights of the agent's parameterized policy can drift far away from the origin (or a good initialization), leading to a variety of undesirable behaviors.1 Fitting this narrative, it has been shown that simply adding a \(L_{2}\) regularizer to the optimization objective (Kumar et al., 2023) or periodically resetting the weights (Dohare et al., 2021; Asadi et al., 2024; Sokar et al., 2023; Ahn et al., 2024) can help mitigate the problem. However, a particularly important limitation is their use of _hyperparameters_, such as the magnitude of the regularizer and the resetting frequency2. Good performance hinges on the suitable environment-dependent hyperparameter, but how can one confidently choose that _before_ interacting with the environment? The classical cross-validation approach would violate the one-shot nature of lifelong RL (and online learning in general; see Chapter 1 of Orabona, 2023), since it is impossible to experience the same environment multiple times. This leads to the contributions of the present work.

Footnote 1: Such as the inactivation of many neurons, due to the ReLU activation function (Abbas et al., 2023; Sokar et al., 2023).

Footnote 2: Indeed, hyperparameter selection, in general, is a well-known problem in lifelong as well as continual learning settings (De Lange et al., 2021).

ContributionThe present work addresses the key challenges in lifelong RL using the principled theory of _Online Convex Optimization_ (OCO). Specifically, our contributions are two fold.

* **Algorithm:**TracBuilding on a series of results in OCO (Cutkosky and Orabona, 2018; Cutkosky, 2019; Cutkosky et al., 2023; Zhang et al., 2024), we propose a (hyper)-_parameter-free_ optimizer for lifelong RL, called Trac (AdapTive RegularAtion in Continual environments). Intuitively, the idea is a refinement of regularization: instead of manually selecting the magnitude of regularization beforehand, Trac chooses that in an online, data-dependent manner. From the perspective of OCO theory, Trac is insensitive to its own hyperparameter, which means that no hyperparameter tuning is necessary in practice. Furthermore, as an optimization approach to lifelong RL, Trac is compatible with any policy parameterization method.
* **ExperimentUsing _Proximal Policy Optimization_ (PPO) (Schulman et al., 2017), we conduct comprehensive experiments on the instantiation of Trac called Trac PPO. A diverse range of lifelong RL environments are tested (based on Procgen, Atari, and Gym Control), with considerably larger scale than prior works. In settings where existing approaches (Abbas et al., 2023; Kumar et al., 2023; Nath et al., 2023) struggle, we find that Trac PPO
* mitigates mild and extreme loss of plasticity;
* and rapidly adapts to new tasks when distribution shifts are introduced. Such findings might be surprising: the theoretical advantage of Trac is motivated by the convexity in OCO, but lifelong RL is _both nonconvex and nonstationary_ in terms of optimization.

OrganizationSection 2 surveys the basics of lifelong RL. Section 3 introduces our parameter-free algorithm Trac, and experiments are presented in Section 4. We defer the discussion of related works and results to Section 5. Finally, Section 6 concludes the paper.

## 2 Lifelong RL

As a sequential decision making framework, _reinforcement learning_ (RL) is commonly framed as a _Markov Decision Process_ (MDP) defined by the state space \(\mathcal{S}\), the action space \(\mathcal{A}\), the transition dynamics \(P(s_{t+1}|s_{t},a_{t})\), and the reward function \(R(s_{t},a_{t},s_{t+1})\). In the \(t\)-th round, starting from a state \(s_{t}\in\mathcal{S}\), the learning agent needs to choose an action \(a_{t}\in\mathcal{A}\) without knowing \(P\) and \(R\). Then, the environment samples a new state \(s_{t+1}\sim P(\cdot|s_{t},a_{t})\), and the agent receives a _reward_\(r_{t}=R(s_{t},a_{t},s_{t+1})\). There are standard MDP objectives driven by theoretical tractability, but from a practical perspective, we measure the agent's performance by its cumulative reward \(\sum_{t=1}^{T}r_{t}\).

The standard setting above concerns a _stationary_ MDP. Motivated by the prevalence of distribution shifts in practice, the present work studies a nonstationary variant called _lifelong_ RL, where the transition dynamics \(P_{t}\) and the reward function \(R_{t}\) can vary over time. Certainly, one should not expect any meaningful "learning" against _arbitrary_ unstructured nonstationarity. Therefore, we implicitly assume \(P_{t}\) and \(R_{t}\) to be _piecewise constant_ over time, and each piece is called a _task_ - just like our example of Spot in the introduction. The main challenge here is to transfer previous learning progress to new tasks. This is reasonable when tasks are similar, but we also want to reduce the degradation when tasks turn out to be very different.

Lifelong RL as online optimizationDeep RL approaches, including PPO (Schulman et al., 2017) and others, crucially utilize the idea of _policy parameterization_. Specifically, a policy refers to the distribution of the agent's action \(a_{t}\) (conditioned on the historical observations), and we use \(\theta_{t}\in\mathbb{R}^{d}\) to denote the parameterizing _weight vector_. After sampling \(a_{t}\) and receiving new observations, the agent could define a _loss function_\(J_{t}(\theta)\) that characterizes the "hypothetical performance" of each weight \(\theta\in\mathbb{R}^{d}\). Then, by computing the _policy gradient_\(g_{t}=\nabla J_{t}(\theta_{t})\), one could apply a _first order optimization algorithm3_ OPT to obtain the updated weight, \(\theta_{t+1}=\text{OPT}(\theta_{t},g_{t})\).

Footnote 3: Formally, a dynamical system that given its state \(\theta_{t}\) and input \(g_{t}\) outputs the new state \(\text{OPT}(\theta_{t},g_{t})\).

For the rest of this paper, we will work with such an abstraction. The feedback of the environment is treated as a _policy gradient oracle_\(\mathcal{G}\), which maps the time \(t\) and the current weight \(\theta_{t}\) into a policy gradient \(g_{t}=\mathcal{G}(t,\theta_{t})\). Our goal is to design an optimizer OPT well suited for lifelong RL.

Lifelong vs. ContinualIn the RL literature, the use of "lifelong" and "continual" varies significantly across studies, which may lead to confusion. Abel et al. (2024) characterized _continual reinforcement learning_ (CRL) as a never-ending learning process. However, much of the literature cited under CRL, such as (Abbas et al., 2023; Ahn et al., 2024), primarily focuses on the problem of _backward transfer_ (avoiding catastrophic forgetting). Various policy-based architectures, such as those proposed by Rolnick et al. (2019); Schwarz et al. (2018); Nath et al. (2023), focus on tackling this issue. Conversely, the present work addresses the problem of _forward transfer_, which refers to the rapid adaptation to new tasks. Because of this we use "lifelong" rather than "continual" in our exposition, similar to (Thrun, 1996; Abel et al., 2018; Julian et al., 2020).

## 3 Method

Inspired by (Cutkosky et al., 2023), we study lifelong RL by exploiting its connection to _Online Convex Optimization_ (OCO; Zinkevich, 2003). The latter is a classical theoretical problem in online learning, and much effort has been devoted to designing _parameter-free_ algorithms that require minimum tuning or prior knowledge (Streeter & Mcmahan, 2012; McMahan & Orabona, 2014; Orabona & Pal, 2016; Foster et al., 2017; Cutkosky & Orabona, 2018; Mhammedi & Koolen, 2020; Chen et al., 2021; Jacobsen & Cutkosky, 2022). The surprising observation of Cutkosky et al. (2023) is that several algorithmic ideas closely tied to the convexity of OCO can actually improve the nonconvex deep learning training, suggesting certain notions of "near convexity" on its loss landscape. We find that lifelong RL (which is _both nonconvex and nonstationary_ in terms of optimization) exhibits a similar behavior, therefore a particularly strong algorithm (named Trac) can be obtained from principled results in parameter-free OCO. Let us start from the background.

Basics of (parameter-free) OcoAs a standalone theoretical topic, OCO concerns a sequential optimization problem where the convex loss function \(l_{t}\) can vary arbitrarily over time. In the \(t\)-th

Figure 2: Visualization of Trac’s key idea.

iteration, the optimization algorithm picks an iterate \(x_{t}\) and then observes a gradient \(g_{t}=\nabla l_{t}(x_{t})\). Motivated by the pursuit of "convergence" in optimization, the standard objective is to guarantee low (i.e., sublinear in \(T\)) _static regret_, defined as

\[\operatorname{Regret}_{T}(l_{1:T},u):=\sum_{t=1}^{T}l_{t}(x_{t})-\sum_{t=1}^{T }l_{t}(u),\]

where \(T\) is the total number of rounds, and \(u\) is a _comparator_ that the algorithm does not know beforehand. In other words, the goal is to make \(\operatorname{Regret}_{T}(l_{1:T},u)\) small for _all_ possible loss sequence \(l_{1:T}\) and comparator \(u\). Note that for _nonstationary_ OCO problems analogous to lifelong RL, it is better to consider a different objective called the _discounted regret_. Algorithms there mostly follow the same principle as in the stationary setting, just wrapped by _loss rescaling_(Zhang et al., 2024).

For minimizing static regret, classical _minimax_ algorithms like gradient descent (Zinkevich, 2003) would assume a small _uncertainty set_\(\mathcal{U}\) at the beginning. Then, by setting the hyperparameter (such as the learning rate) according to \(\mathcal{U}\), it is possible to guarantee sublinear _worst case regret_,

\[\max_{(l_{1:T},u)\in\mathcal{U}}\operatorname{Regret}_{T}(l_{1:T},u)=o(T).\] (1)

In contrast, parameter-free algorithms use very different strategies4 to bound \(\operatorname{Regret}_{T}(l_{1:T},u)\) directly (without taking the maximum) by a function of both \(l_{1:T}\) and \(u\). The resulting bound is more refined than Eq.(1) (Orabona, 2023, Chapter 9), and crucially, since there is no need to pick an uncertainty set \(\mathcal{U}\), much less hyperparameter tuning is needed. This is where its name comes from.

Footnote 4: The key difference with gradient descent is the use of intricate (non-\(L_{2}\)) regularizers. See (Fang et al., 2022; Jacobsen and Cutkosky, 2022) for a theoretical justification of their importance.

**TRAC for Lifelong RL:** In lifelong RL, a key issue is the excessive drifting of weights \(\theta_{t}\), which can detrimantly affect adapting to new tasks. To address this, TRAC enforces proximity to a well-chosen reference point \(\theta_{\mathrm{ref}}\), providing a principled solution derived from a decade of research in parameter-free OCO. Unlike traditional methods such as \(L_{2}\) regularization or resetting, TRAC avoids hyperparameter tuning, utilizing the properties of OCO to maintain weight stability and manage the drift effectively.

The core of TRAC, similar to other parameter-free optimizers, incorporates three techniques:

* **Direction-Magnitude Decomposition**: Inspired by Cutkosky and Orabona (2018), this technique employs a carefully designed one-dimensional algorithm, the "parameter-free tuner," atop a base optimizer. This setup acts as a data-dependent regularizer, controlling the extent to which the iterates deviate from their initialization, thereby minimizing loss of plasticity, which is crucial given the high plasticity at the initial policy parameterization (Abbas et al., 2023).
* **Erfi Potential Function**: Building on the previous concept, the tuner utilizes the Erfi potential function, as developed by Zhang et al. (2024). This function is crafted to effectively balance the distance of the iterates from both the origin and the empirical optimum. It manages the update magnitude by focusing on the gradient projection along the direction \(\theta_{t}-\theta_{\mathrm{ref}}\).
* **Additive Aggregation:** The tuner above necessitates discounting. Thus, we employ Additive Aggregation by Cutkosky (2019). This approach enables the combination of multiple parameter-free OCO algorithms, each with different discount factors, to approximate the performance of the best-performing algorithm. Importantly, it facilitates the automatic selection of the optimal discount factor during training.

These three components crucially work together to guarantee good regret bounds in the convex setting and are the minimum requirement for any reasonable parameter-free optimizer.

Without going deep into the theory, here is an overview of the important ideas (also see Figure 2 for a visualization).

* First, Trac is a meta-algorithm that operates on top of a "default" optimizer Base. It can simply be gradient descent with a constant learning rate, or Adam(Kingma and Ba, 2014) as in our experiments. Applying Base alone would be equivalent to enforcing the scaling parameter \(S_{t+1}\equiv 1\) in Trac, but this would suffer from the drifting of \(\theta_{t+1}^{\mathrm{Base}}\) (and thus, the weight \(\theta_{t+1}\)).

* To fix this issue, Trac uses the tuner (Algorithm 2) to select the scaling parameter \(S_{t+1}\), making it _data-dependent_. Typically \(S_{t+1}\) is within \([0,1]\) (see Figure 17 to 19), therefore essentially, we define the updated weight \(\theta_{t+1}\) as a _convex combination_ of the Base's weight \(\theta_{t}^{\text{Base}}\) and the reference point \(\theta_{\mathrm{ref}}\), \[\theta_{t+1}=S_{t+1}\cdot\theta_{t+1}^{\text{Base}}+(1-S_{t+1})\theta_{ \mathrm{ref}}.\] This brings the weight closer to \(\theta_{\mathrm{ref}}\), which is known to be "safe" (i.e., not overfitting any particular lifelong RL task), although possibly conservative.
* To inject the right amount of conservatism without hyperparameter tuning, the tuner (Algorithm 2) applies an unusual decision rule based on the \(\mathrm{erfi}\) function. Theoretically, this is known to be optimal in an idealized variant of OCO (Zhang et al., 2022, 2024b), but removing the idealized assumptions requires a tiny bit of extra conservatism, which is challenging (and not necessarily practical). Focusing on the lifelong RL problem that considerably deviates from OCO, we simply apply the \(\mathrm{erfi}\) decision rule as is. This is loosely motivated by deep learning training dynamics, e.g., (Cohen et al., 2020; Ahn et al., 2023; Andriushchenko et al., 2023), where an aggressive optimizer is often observed to be better.
* Finally, the tuner requires a discount factor \(\beta\). This crucially controls the strength of regularization (elaborated next), but also introduces a hyperparameter tuning problem. Following (Cutkosky, 2019), we aggregate tuners with different \(\beta\) (on a log-scaled grid) by simply summing up their outputs. This is justified by the _adaptivity_ of the tuner itself: in OCO, if we add a parameter-free algorithm \(\mathcal{A}_{1}\) to any other algorithm \(\mathcal{A}_{2}\) that already works well, then \(\mathcal{A}_{1}\) can automatically identify this and "tune down" its aggressiveness, such that \(\mathcal{A}_{1}+\mathcal{A}_{2}\) still performs as well as \(\mathcal{A}_{2}\).

Connection to regularizationDespite its nested structure, Trac can actually be seen as a parameter-free refinement of \(L_{2}\) regularization (Kumar et al., 2023). To concretely explain this intuition, let us consider the following two optimization dynamics.

* First, suppose we run gradient descent with learning rate \(\eta\), on the policy gradient sequence \(\{g_{t}\}\) with the \(L_{2}\) regularizer \(\frac{\lambda}{2}\left\|\theta-\theta_{\mathrm{ref}}\right\|^{2}\). Quantitatively, it means that starting from the \(t\)-th weight \(\theta_{t}\), \[\theta_{t+1}=\theta_{t}-\eta\left[g_{t}+\lambda\left(\theta_{t}-\theta_{ \mathrm{ref}}\right)\right],\quad\Longrightarrow\quad\theta_{t+1}-\theta_{ \mathrm{ref}}=\left(1-\lambda\eta\right)\left(\theta_{t}-\theta_{\mathrm{ref} }\right)-\eta g_{t}.\] (2) That is, the updated weight \(\theta_{t+1}\) is determined by a (\(1-\lambda\eta\))-discounting with respect to the reference point \(\theta_{\mathrm{ref}}\), followed by a gradient step \(-\eta g_{t}\).
* Alternatively, consider applying the following simplification of Trac on the same policy gradient sequence \(\{g_{t}\}\): (\(i\)) Base is still gradient descent with learning rate \(\eta\); (\(ii\)) there is just one discount factor \(\beta\); and (\(iii\)) the one-dimensional tuner (Algorithm 2) is replaced by the \(\beta\)-discounted gradient descent with learning rate \(\alpha\), i.e., \(S_{t+1}=\beta S_{t}-\alpha h_{t}\). In this case, we have \[\theta_{t+1}-\theta_{\mathrm{ref}} =S_{t+1}\left(\theta_{t+1}^{\mathrm{Base}}-\theta_{\mathrm{ref} }\right)\] \[=\left(\beta S_{t}-\alpha h_{t}\right)\left(\theta_{t}^{\mathrm{ Base}}-\theta_{\mathrm{ref}}-\eta g_{t}\right)\] \[=\left(\beta-\alpha S_{t}^{-1}h_{t}\right)\left(\theta_{t}- \theta_{\mathrm{ref}}\right)-\eta S_{t+1}g_{t}.\qquad\qquad\text{( mildly assuming $S_{t}\neq 0$)}\] Notice that \(S_{t}\) is a \(\beta\)-discounted sum of \(\alpha h_{1},\ldots,\alpha h_{t-1}\), thus in the typical situation of \(\beta\approx 1\) one might expect \(\alpha h_{t}\ll|S_{t}|\). Then, the resulting update of \(\theta_{t+1}\) is similar to Eq.(2), with quantitative changes on the "effective discounting" \(1-\lambda\eta\rightarrow\beta\), and the "effective learning rate" \(\eta\rightarrow\eta S_{t+1}\).

The main message here is that under a simplified setting, Trac is almost equivalent to \(L_{2}\) regularization. The latter requires choosing the hyperparameters \(\lambda\) and \(\eta\), and similarly, the above _simplified_ Trac requires choosing \(\beta\) and \(\eta\). Going beyond this simplification, the actual Trac removes the tuning of \(\beta\) using aggregation, and the tuning of \(\eta\) using the \(\mathrm{erfi}\) decision rule.

On the hyperparametersAlthough Trac is called "parameter-free", it still needs the \(\beta\)-grid, the constant \(\varepsilon\) and the algorithm Base as inputs. The idea is that Trac is particularly insensitive to such choices, as supported by the OCO theory. As the result, the generic default values recommended by Cutkosky et al. (2023) are sufficient in practice. We note that those are proposed for training supervised deep learning models, thus should be agnostic to the lifelong RL applications we consider.

## 4 Experiment

Does Trac experience the common pitfalls of loss of plasticity? Does it rapidly adapt to distribution shifts? To answer these questions, we test Trac in empirical RL benchmarks such as vision-based games and physics-based control environments in lifelong settings (Figure 3). Specifically, we instantiate PPO with two different optimizers: Adam with constant learning rate for baseline comparison, and Trac for our proposed method (with exactly the same Adam as the input Base). We also test Adam PPO with _concatenated ReLU activations_ (CReLU; Shang et al., 2016), previously shown to mitigate loss of plasticity in certain deep RL settings (Abbas et al., 2023). Our numerical results are summarized in Table 1. Across every lifelong RL setting, we observe substantial improvements in the cumulative episode reward by using Trac PPO compared to Adam PPO or CReLU. Below are the details, with more in the Appendix.

ProcgenWe first evaluate on OpenAI Procgen, a suite of 16 procedurally generated game environments (Cobbe et al., 2020). We introduce distribution shifts by sampling a new procedurally generated level of the current game every 2 million time steps, treating each level as a distinct task.

Figure 3: Experimental setup for lifelong RL.

[MISSING_PAGE_FAIL:7]

Gym ControlWe use the CartPole-v1 and Acrobot-v1 environments from the Gym Classic Control suite, along with LunarLander-v2 from Box2d Control. To introduce distribution shifts, Mendez et al. (2020) periodically alters the environment dynamics. Although such distribution shifts pose only mild challenges for robust methods like PPO with Adam (Appendix D). We instead implement a more challenging form of distribution shift. Every 200 steps we perturb each observation dimension with random noise within a range of \(\pm 2\), treating each perturbation phase as a distinct task.

Here (Figure 6), we notice a peculiar behavior after introducing the first distribution shift in both Adam PPO and CReLU: policy collapse. We describe this as an _extreme_ form of loss of plasticity. Surprisingly, Trac PPO remains resistant to these extreme distribution shifts. As we see in the Acrobot experiment, Trac PPO shows minimal to no policy damage after the first few distribution shifts, whereas Adam PPO and CReLU are unable to recover a policy at all. We investigate if Trac's behavior here indicates positive transfer in Appendix A. Across the three Gym Control environments,

\begin{table}
\begin{tabular}{l c c c} \hline
**Environment** & **Adam PPO** & **CReLU** & **Trac PPO (Ours)** \\ \hline Starpilot & \(3.4\) & \(3.6\) & \(\mathbf{12.5}\) \\ Dodgeball & \(1.9\) & \(2.3\) & \(\mathbf{5.2}\) \\ Chaser & \(1.4\) & \(1.7\) & \(\mathbf{2.2}\) \\ Fruitbot & \(0.1\) & \(1.0\) & \(\mathbf{1.8}\) \\ CartPole & \(5.1\) & \(1.2\) & \(\mathbf{39.6}\) \\ Acrobot & \(-14.3\) & \(-13.9\) & \(\mathbf{-12.9}\) \\ LunarLander & \(-21.7\) & \(-19.4\) & \(\mathbf{-8.6}\) \\ Atari 6 & \(3.1\) & \(4.8\) & \(\mathbf{10.5}\) \\ Atari 9 & \(3.9\) & \(17.0\) & \(\mathbf{20.2}\) \\ \hline \end{tabular}
\end{table}
Table 1: Cumulative sum of mean episode reward for Trac PPO, Adam PPO, and CReLU on Procgen, Atari, and Gym Control environments. Rewards are scaled by \(10^{5}\); higher is better.

Figure 5: Reward in the lifelong Atari environments, across games with action spaces of 6 and 9. Trac PPO rapidly adapts to new tasks, in contrast to the Adam PPO and CReLU which struggle to achieve high reward, indicating mild loss of plasticity.

Figure 6: Reward performance across CartPole, Acrobot, and LunarLander Gym Control tasks. Both Adam PPO and CReLU experience extreme plasticity loss, failing to recover after the initial distribution shift. Conversely, Trac PPO successfully avoids such plasticity loss, rapidly adapting when facing extreme distribution shifts.

Trac PPO shows an average normalized improvement of 204.18% over Adam PPO and 1044.24% over CReLU (Table 1).

## 5 Discussion

Related workCombating loss of plasticity has been studied extensively in lifelong RL. A typical challenge for existing solutions is the tuning of their hyperparameters, which requires prior knowledge on the nature of the distribution shift, e.g., (Asadi et al., 2024; Nath et al., 2023; Nikishin et al., 2024; Sokar et al., 2023; Mesbahi et al., 2024). An architectural modification called CReLU is studied in (Abbas et al., 2023), but our experiments suggest that its benefit might be specific to the Atari setup. Besides, Abel et al. (2018, 2018) presented a theoretical analysis of skill transfer in lifelong RL, based on value iteration. Moreover, related contributions in nonstationary RL, where reward and state transition functions also change unpredictably, are limited to theoretical sequential decision-making settings with a focus on establishing complexity bounds (Roy et al., 2019; Cheung et al., 2020; Wei Luo, 2021; Mao et al., 2020).

Our algorithm Trac builds on a long line of works on parameter-free OCO (see Section 3). To our knowledge, the only existing work applying parameter-free OCO to RL is (Jacobsen and Chan, 2021), which focuses on estimating the value function (i.e., policy evaluation). Our scope is different, focusing on empirical RL in lifelong problems by exploring the key connection between parameter-free OCO and regularization.

Particularly, we are inspired by the Mechanic algorithm from (Cutkosky et al., 2023), which goes beyond the traditional convex setting of parameter-free OCO to handle stationary deep learning optimization tasks. Lifelong reinforcement learning, however, introduces a layer of complexity with its inherent nonstationarity. Furthermore, compared to Mechanic, Trac improves the scale tuner there (which is based on the _coin-betting_ framework; Orabona and Pal, 2016) by the erfi algorithm that enjoys a better OCO performance guarantee. As an ablation study, we empirically compare Trac and Mechanic in the Appendix G (Table 3). We find that Trac is slightly better, but both algorithms can mitigate the loss of plasticity, suggesting the effectiveness of the general "parameter-free" principle in lifelong RL.

Trac encourages positive transferIn our experiments, we observe that Trac's reward decline due to distribution shifts is less severe than that of baseline methods. These results may suggest Trac facilitates positive transfer between related tasks. To investigate this further, we compared Trac to a privileged weight-resetting approach, where the network's parameters are reset for each new task, in the Gym Control environments (see Appendix A). Our results show that Trac maintains higher rewards during tasks than privileged weight-resetting and avoids declining to the same low reward levels as privileged weight-resetting at the start of a new task (Figure 8).

On the choice of \(\theta_{\mathrm{ref}}\)In general, the reference point \(\theta_{\mathrm{ref}}\) should be good or "safe" for Trac to perform effectively. One might presume that achieving this requires "warmstarting", or pre-training using the underlying Base optimizer. While our experiments validate that such warmstarting is indeed beneficial (Appendix B), our main experiments show that even a random initialization of the policy's weight serves as a _good enough_\(\theta_{\mathrm{ref}}\), even when tasks are similar (Figure 4).

This observation aligns with discussions by Lyle et al. (2023), Sokar et al. (2023), and Abbas et al. (2023), who suggested that persistent gradient steps away from a random initialization can deactivate ReLU activations, leading to activation collapse and loss of plasticity in neural networks. Our results also support Kumar et al. (2023)'s argument that maintaining some weights close to their initial values not only prevents dead ReLU units but also allows quick adaptation to new distribution shifts.

Tuning \(L_{2}\) regularizationThe success of Trac suggests that an adaptive form of regularization--anchoring to the reference point \(\theta_{\mathrm{ref}}\)--may suffice to counteract both mild and extreme forms of loss of plasticity. From this angle, we further elaborate the limitation of the \(L_{2}\) regularization approach considered in (Kumar et al., 2023). It requires selecting a regularization strength parameter \(\lambda\) through cross-validation, which is incompatible with the one-shot nature of lifelong learning settings. Furthermore, it is nontrivial to select the search grid: for example, we tried the \(\lambda\)-grid suggested by (Kumar et al., 2023), and there is no effective \(\lambda\) value within the grid for the lifelong RL environments we consider. All the values are too small.

Continuing this reasoning, we conduct a hyperparameter search for \(\lambda\), over various larger values \([0.2,0.8,1,5,10,15,20,25,30,35,40,45,50]\). Given the expense of such experiments, only the more sample-efficient control environments are considered. We discover that each environment and task responds uniquely to these regularization strengths (see bar plot of \(\lambda\) values in Figure 7). This highlights the challenges of tuning \(\lambda\) in a lifelong learning context, where adjusting for each environment, let alone each distribution shift, would require extensive pre-experimental analysis.

In contrast, Trac offers a parameter-free solution that adapts dynamically with the data in an online manner. The scaling output of Trac adjusts autonomously to the ongoing conditions, consistently competing with well-tuned \(\lambda\) values in the various environments, as demonstrated in the reward plots for CartPole, Acrobot, and LunarLander (Figure 7).

Trac compared to other plasticity methodsBoth layer normalization and plasticity injection Nikishin et al. (2024); Lyle et al. (2023) have been shown to combat plasticity loss. For instance, Appendix E Figure 15 demonstrates that both layer normalization and plasticity injection are effective at reducing plasticity loss when applied to the CartPole environment using Adam as a baseline optimizer. We implemented plasticity injection following the methodology laid out by Nikishin et al. (2024), where plasticity is injected at the start of every distribution shift. While this approach does help in reducing the decline in performance due to plasticity loss, our results indicate that it is consistently outperformed by TRAC across all three control environments--CartPole, Acrobot, and LunarLander. Moreover, while layer normalization improves Adam's performance, it too is outperformed by TRAC across the same control settings (Figure 15). Notably, combining layer normalization with TRAC resulted in the best performance gains.

Near convexity of lifelong RLOur results demonstrate the rapid adaptation of Trac, in lifelong RL problems with complicated function approximation. From the perspective of optimization, the latter requires tackling both nonconvexity and nonstationarity, which is typically regarded intractable in theory. Perhaps surprisingly, when approaching this complex problem using the theoretical insights from OCO, we observe compelling results. This suggests a certain "hidden convexity" in this problem, which could be an exciting direction for both theoretical and empirical research (e.g., policy gradient methods provably converge to global optimizers in linear quadratic control (Hu et al., 2023)).

LimitationsWhile Trac offers robust adaptability in nonstationary environments, it can exhibit suboptimal performance at the outset. In the early stages of deployment, Trac might underperform compared to the baseline optimizer. We address this by proposing a warmstarting solution detailed in Appendix B, which helps increase the initial performance gap.

## 6 Conclusion

In this work, we introduced Trac, a parameter-free optimizer for lifelong RL that leverages the principles of OCO. Our approach dynamically refines regularization in a data-dependent manner, eliminating the need for hyperparameter tuning. Through extensive experimentation in Procgen, Atari, and Gym Control environments, we demonstrated that Trac effectively mitigates loss of plasticity and rapidly adapts to new distribution shifts, where baseline methods fail. Trac's success leads to a compelling takeaway: empirical lifelong RL scenarios may exhibit more convex properties than previously appreciated, and might inherently benefit from parameter-free OCO approaches.

Figure 7: For each Gym Control environment and the initial ten tasks, we identified the best \(\lambda\), which is the regularization strength that maximizes reward for each task’s specific distribution shift. We also determined the best overall (well-tuned) \(\lambda\) for each environment. The results demonstrate that each environment and each task’s distribution shift is sensitive to different \(\lambda\) and that Trac PPO performs competitively with each environment’s well-tuned \(\lambda\).

Acknowledgments

We thank Ashok Cutkosky for insightful discussions on online optimization in nonstationary settings. We are grateful to David Abel for his thoughtful insights on loss of plasticity in relation to lifelong reinforcement learning. We appreciate Kaiqing Zhang and Yang Hu for their comments on theoretical and nonstationary RL. This project is partially funded by Harvard University Dean's Competitive Fund for Promising Scholarship.

## References

* Abbas et al. (2023) Zaheer Abbas, Rosie Zhao, Joseph Modayil, Adam White, and Marlos C Machado. Loss of plasticity in continual deep reinforcement learning. In _Conference on Lifelong Learning Agents_, pp. 620-636. PMLR, 2023.
* Abel et al. (2018a) David Abel, Dilip Arumugam, Lucas Lehnert, and Michael Littman. State abstractions for lifelong reinforcement learning. In _International Conference on Machine Learning_, pp. 10-19. PMLR, 2018a.
* Abel et al. (2018b) David Abel, Yuu Jinnai, Sophie Yue Guo, George Konidaris, and Michael Littman. Policy and value transfer in lifelong reinforcement learning. In _International Conference on Machine Learning_, pp. 20-29. PMLR, 2018b.
* Abel et al. (2024) David Abel, Andre Barreto, Benjamin Van Roy, Doina Precup, Hado P van Hasselt, and Satinder Singh. A definition of continual reinforcement learning. _Advances in Neural Information Processing Systems_, 36, 2024.
* Ahn et al. (2024) Hongjoon Ahn, Jinu Hyeon, Youngmin Oh, Bosun Hwang, and Taesup Moon. Catastrophic negative transfer: An overlooked problem in continual reinforcement learning, 2024. URL https://openreview.net/forum?id=o7BuUyXz1f.
* Ahn et al. (2023) Kwangjun Ahn, Sebastien Bubeck, Sinho Chewi, Yin Tat Lee, Felipe Suarez, and Yi Zhang. Learning threshold neurons via edge of stability. _Advances in Neural Information Processing Systems_, 36, 2023.
* Andriushchenko et al. (2023) Maksym Andriushchenko, Aditya Vardhan Varre, Loucas Pillaud-Vivien, and Nicolas Flammarion. Sgd with large step sizes learns sparse features. In _International Conference on Machine Learning_, pp. 903-925. PMLR, 2023.
* Asadi et al. (2024) Kavosh Asadi, Rasool Fakoor, and Shoham Sabach. Resetting the optimizer in deep rl: An empirical study. _Advances in Neural Information Processing Systems_, 36, 2024.
* Bellemare et al. (2013) Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. _Journal of Artificial Intelligence Research_, 47:253-279, 2013.
* Chen et al. (2021) Liyu Chen, Haipeng Luo, and Chen-Yu Wei. Impossible tuning made possible: A new expert algorithm and its applications. In _Conference on Learning Theory_, pp. 1216-1259. PMLR, 2021.
* Cheung et al. (2020) Wang Chi Cheung, David Simchi-Levi, and Ruihao Zhu. Reinforcement learning for non-stationary markov decision processes: The blessing of (more) optimism. In _International conference on machine learning_, pp. 1843-1854. PMLR, 2020.
* Cobbe et al. (2020) Karl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generation to benchmark reinforcement learning. In _International conference on machine learning_, pp. 2048-2056. PMLR, 2020.
* Cohen et al. (2020) Jeremy Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar. Gradient descent on neural networks typically occurs at the edge of stability. In _International Conference on Learning Representations_, 2020.
* Cutkosky (2019) Ashok Cutkosky. Combining online learning guarantees. In _Conference on Learning Theory_, pp. 895-913. PMLR, 2019.
* Cutkosky and Orabona (2018) Ashok Cutkosky and Francesco Orabona. Black-box reductions for parameter-free online learning in banach spaces. In _Conference On Learning Theory_, pp. 1493-1529. PMLR, 2018.
* Cutkosky et al. (2023) Ashok Cutkosky, Aaron Defazio, and Harsh Mehta. Mechanic: A learning rate tuner. _Advances in Neural Information Processing Systems_, 36, 2023.
* De Lange et al. (2021) Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. _IEEE transactions on pattern analysis and machine intelligence_, 44(7):3366-3385, 2021.
* De et al. (2020)Shibhansh Dohare, Richard S Sutton, and A Rupam Mahmood. Continual backprop: Stochastic gradient descent with persistent randomness. _arXiv preprint arXiv:2108.06325_, 2021.
* Espeholt et al. [2018] Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. In _International conference on machine learning_, pp. 1407-1416. PMLR, 2018.
* Fang et al. [2022] Huang Fang, Nicholas JA Harvey, Victor S Portella, and Michael P Friedlander. Online mirror descent and dual averaging: keeping pace in the dynamic case. _Journal of Machine Learning Research_, 23(1):5271-5308, 2022.
* Foster et al. [2017] Dylan J Foster, Satyen Kale, Mehryar Mohri, and Karthik Sridharan. Parameter-free online learning via model selection. _Advances in Neural Information Processing Systems_, 30, 2017.
* Hu et al. [2023] Bin Hu, Kaiqing Zhang, Na Li, Mehran Mesbahi, Maryam Fazel, and Tamer Basar. Toward a theoretical foundation of policy optimization for learning control policies. _Annual Review of Control, Robotics, and Autonomous Systems_, 6:123-158, 2023.
* Jacobsen and Chan [2021] Andrew Jacobsen and Alan Chan. Parameter-free gradient temporal difference learning. _arXiv preprint arXiv:2105.04129_, 2021.
* Jacobsen and Cutkosky [2022] Andrew Jacobsen and Ashok Cutkosky. Parameter-free mirror descent. In _Conference on Learning Theory_, pp. 4160-4211. PMLR, 2022.
* Julian et al. [2020] Ryan Julian, Benjamin Swanson, Gaurav S Sukhatme, Sergey Levine, Chelsea Finn, and Karol Hausman. Efficient adaptation for end-to-end vision-based robotic manipulation. In _4th Lifelong Machine Learning Workshop at ICML 2020_, 2020.
* Kingma and Ba [2014] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* Kumar et al. [2023] Saurabh Kumar, Henrik Marklund, and Benjamin Van Roy. Maintaining plasticity via regenerative regularization. _arXiv preprint arXiv:2308.11958_, 2023.
* Lyle et al. [2022] Clare Lyle, Mark Rowland, and Will Dabney. Understanding and preventing capacity loss in reinforcement learning. In _International Conference on Learning Representations_, 2022.
* Lyle et al. [2023] Clare Lyle, Zeyu Zheng, Evgenii Nikishin, Bernardo Avila Pires, Razvan Pascanu, and Will Dabney. Understanding plasticity in neural networks. In _International Conference on Machine Learning_, pp. 23190-23211. PMLR, 2023.
* Lyle et al. [2024] Clare Lyle, Zeyu Zheng, Khimya Khetarpal, Hado van Hasselt, Razvan Pascanu, James Martens, and Will Dabney. Disentangling the causes of plasticity loss in neural networks, 2024. URL https://arxiv.org/abs/2402.18762.
* Mao et al. [2020] Weichao Mao, Kaiqing Zhang, Ruihao Zhu, David Simchi-Levi, and Tamer Basar. Model-free non-stationary rl: Near-optimal regret and applications in multi-agent rl and inventory control. _arXiv preprint arXiv:2010.03161_, 2020.
* McMahan and Orabona [2014] H Brendan McMahan and Francesco Orabona. Unconstrained online linear learning in hilbert spaces: Minimax algorithms and normal approximations. In _Conference on Learning Theory_, pp. 1020-1039. PMLR, 2014.
* Mendez et al. [2020] Jorge Mendez, Boyu Wang, and Eric Eaton. Lifelong policy gradient learning of factored policies for faster training without forgetting. _Advances in Neural Information Processing Systems_, 33:14398-14409, 2020.
* Mesbahi et al. [2024] Golnaz Mesbahi, Olya Mastikhina, Parham Mohammad Panahi, Martha White, and Adam White. Tuning for the unknown: Revisiting evaluation strategies for lifelong rl. _arXiv preprint arXiv:2404.02113_, 2024.
* Mhammedi and Koolen [2020] Zakaria Mhammedi and Wouter M Koolen. Lipschitz and comparator-norm adaptivity in online learning. In _Conference on Learning Theory_, pp. 2858-2887. PMLR, 2020.
* Nath et al. [2023] Saptarshi Nath, Christos Peridis, Esegbene Ben-Iwhiwhu, Xinran Liu, Shirin Dora, Cong Liu, Soheil Kolouri, and Andrea Soltoggio. Sharing lifelong reinforcement learning knowledge via modulating masks. In Sarath Chandar, Razvan Pascanu, Hanie Sedghi, and Doina Precup (eds.), _Proceedings of The 2nd Conference on Lifelong Learning Agents_, Proceedings of Machine Learning Research. PMLR, 2023.
* Nikishin et al. [2022] Evgenii Nikishin, Max Schwarzer, Pierluca D'Oro, Pierre-Luc Bacon, and Aaron Courville. The primacy bias in deep reinforcement learning. In _International Conference on Machine Learning_, pp. 16828-16847. PMLR, 2022.
* Nester et al. [2020]Evgenii Nikishin, Junhyuk Oh, Georg Ostrovski, Clare Lyle, Razvan Pascanu, Will Dabney, and Andre Barreto. Deep reinforcement learning with plasticity injection. _Advances in Neural Information Processing Systems_, 36, 2024.
* Orabona [2023] Francesco Orabona. A modern introduction to online learning. _arXiv preprint arXiv:1912.13213_, 2023.
* Orabona and Pal [2016] Francesco Orabona and David Pal. Coin betting and parameter-free online learning. _Advances in Neural Information Processing Systems_, 29, 2016.
* Rolnick et al. [2019] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. Experience replay for continual learning. In _Advances in Neural Information Processing Systems_, pp. 348-358, 2019. URL https://arxiv.org/abs/1811.11682.
* Roy et al. [2019] Abhishek Roy, Krishnakumar Balasubramanian, Saeed Ghadimi, and Prasant Mohapatra. Multi-point bandit algorithms for nonstationary online nonconvex optimization. _arXiv preprint arXiv:1907.13616_, 2019.
* Schulman et al. [2017] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.
* Schwarz et al. [2018] Jonathan Schwarz, Jelena Luketina, Wojciech M Czarnecki, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable framework for continual learning. In _ICML_, 2018. URL https://arxiv.org/abs/1805.06370.
* Shang et al. [2016] Wenling Shang, Kihyuk Sohn, Diogo Almeida, and Honglak Lee. Understanding and improving convolutional neural networks via concatenated rectified linear units. In _international conference on machine learning_, pp. 2217-2225. PMLR, 2016.
* Sokar et al. [2023] Ghada Sokar, Rishabh Agarwal, Pablo Samuel Castro, and Utku Evci. The dormant neuron phenomenon in deep reinforcement learning. In _International Conference on Machine Learning_, pp. 32145-32168. PMLR, 2023.
* Streeter and Mcmahan [2012] Matthew Streeter and Brendan Mcmahan. No-regret algorithms for unconstrained online convex optimization. _Advances in Neural Information Processing Systems_, 25, 2012.
* Thrun [1996] S. Thrun. _Explanation-Based Neural Network Learning: A Lifelong Learning Approach_. Kluwer Academic Publishers, Boston, MA, 1996.
* Wei and Luo [2021] Chen-Yu Wei and Haipeng Luo. Non-stationary reinforcement learning without prior knowledge: An optimal black-box approach. In _Conference on learning theory_, pp. 4300-4354. PMLR, 2021.
* Xie and Finn [2022] Annie Xie and Chelsea Finn. Lifelong robotic reinforcement learning by retaining experiences, 2022.
* Zhang et al. [2022] Zhiyu Zhang, Ashok Cutkosky, and Ioannis Paschalidis. Pde-based optimal strategy for unconstrained online learning. In _International Conference on Machine Learning_, pp. 26085-26115. PMLR, 2022.
* Zhang et al. [2024a] Zhiyu Zhang, David Bombara, and Heng Yang. Discounted adaptive online learning: Towards better regularization. In _International Conference on Machine Learning_, 2024a.
* Zhang et al. [2024b] Zhiyu Zhang, Heng Yang, Ashok Cutkosky, and Ioannis C Paschalidis. Improving adaptive online learning using refined discretization. In _International Conference on Algorithmic Learning Theory_, pp. 1208-1233. PMLR, 2024b.
* Zinkevich [2003] Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In _International Conference on Machine Learning_, pp. 928-936, 2003.

## Appendix A Trac Encourages Positive Transfer

To explore whether Trac encourages positive transfer, we introduce a privileged weight-reset baseline. This baseline is "privileged" in the sense that it knows when a distribution shift is introduced and resets the parameters to a random initialization at the start of each new task. We applied this baseline to three Gym control tasks: CartPole-v1, Acrobot-v1, and LunarLander-v2, and compared it to Trac PPO and Adam PPO, as shown in Figure 8.

We observe that the privileged weight-reset baseline exhibits spikes in reward at the beginning of each new task. Surprisingly, Trac maintains even higher rewards than the privileged weight-reset baseline, even at its peak learning phases. Additionally, Trac's reward does not decline to the reward seen at the start of new tasks with privileged weight-resetting (Trac does not have to "start over" with each task), suggesting that Trac successfully transfers skills positively between tasks.

## Appendix B Warmstarting

In our theoretical framework, we hypothesize that a robust parameter initialization, denoted as \(\theta_{\mathrm{ref}}\), could enhance the performance of our models, suggesting that empirical implementations might benefit from initializing parameters using a base optimizer such as Adam prior to deploying Trac. Contrary to this assumption, our experimental results detailed in Section 4 reveal that warmstarting is not essential for Trac's success. Below, we examine the performance of Adam PPO and Trac PPO when warmstarted.

Both Trac PPO and Adam PPO were warmstarted using Adam for the initial 150,000 steps in all games for the Atari and Procgen environments, and for the first 30 steps in the Gym Control experiments. As seen in Figure 9, in games like Starpilot, Fruitbot, and Dodgeball, Trac PPO surpasses Adam PPO in the first level/task of the online setup, with its performance closely matching that of Adam PPO in Chaser. Importantly, Trac PPO continues to avoid the loss of plasticity encountered by Adam PPO, even when both are warmstarted. This makes sense since all of the distributions share some foundational game dynamics; the initial learning phases likely explore these dynamics, so leveraging a good parameter initialization to regularize in this early region can be beneficial for Trac--we observe that forward transfer occurs somewhat in later level distribution shifts as the reward does not drop back to zero where it initially started from.

Our findings indicate that warmstarting does not confer a significant advantage in the Atari games. This makes sense because a parameter initialization that is good in one game setting is likely a random parameterization for another setting, which is equivalent to the setup without warmstarting where Trac regularizes towards a random parameter initialization. In the Gym Control experiments although warmstarted Trac PPO manages to avoid the extreme plasticity loss and policy collapse seen in warmstarted Adam PPO, it does not perform as well as non-warmstarted Trac PPO. This result underscores that the efficacy of warmstarting is environment-specific and highlights the

Figure 8: Reward comparison of Trac PPO, Adam PPO, and privileged weight-resetting on Cartpole-v1, Acrobot-v1, and LunarLander-v2. Trac PPO encourages positive transfer between tasks.

challenge in predicting when Adam PPO may achieve a parameter initialization that is advantageous for Trac PPO to regularize towards.

From an overall perspective, warmstarting Trac PPO in every setting still shows substantial improvement over Adam PPO (Table 2).

## Appendix C Other RL Baselines

While PPO is a widely used policy gradient method in reinforcement learning, it is not the only approach applicable to lifelong RL. Other continual RL methods, such as IMPALA (Espeholt et al., 2018), Online EWC (Schwarz et al., 2018), CLEAR (Rolnick et al., 2019), and Modulating Masks (Nath et al., 2023), are designed to address challenges like catastrophic forgetting in dynamic, nonstationary environments. We incorporated these algorithm implementations adapted from the code from Nath et al. (2023) into our experiments to offer a more comprehensive evaluation. These methods vary in their mechanisms for maintaining task performance over time but may still suffer from plasticity loss in later stages of training.

Figure 10: Reward in the lifelong Atari environments with warmstarted Trac PPO and warmstarted Adam PPO. No significant benefit is found by warmstarting Trac PPO compared to not warmstarting it.

Figure 9: Reward in the lifelong Procgen environments for StarPilot, Dodgeball, Fruitbot, and Chaser with warmstarted Trac PPO and warmstarted Adam PPO. Inital performance of Trac PPO is improved with warmstarting and continues to avoid loss of plasticity.

[MISSING_PAGE_FAIL:16]

Our observations suggest that Adam PPO is robust to such dynamics-based distribution shifts, as shown in Figure 14. This indicates that while Adam PPO implicitly models the dynamics of the environment well--where changes in dynamics minimally impact performance--it struggles more with adapting to out-of-distribution observations such as seen in the main experiments (Figure 6) and in the warmstarting experiments (Figure 11).

## Appendix E LayerNorm, Plasticity Injection, and Weight Decay

To evaluate Trac alongside other methods that aim to mitigate plasticity loss, we compare it against LayerNorm (Lyle et al., 2023), Plasticity Injection (Nikishin et al., 2024), and tuning weight decay (Lyle et al., 2024).

As discussed in Section 5, we confirm that both layer normalization and plasticity injection (applied at the start of every distribution shift) (Nikishin et al., 2024; Lyle et al., 2023) are effective in reducing plasticity loss (Figure 15). While these methods help slow the decline in performance due to plasticity loss, Trac consistently outperforms them across the three Gym Control environments. Importantly, because Trac is an optimizer, it can be combined with layer normalization, and doing so resulted in the best performance gains in our Control setups.

Figure 12: Performance comparison between Adam-based and Trac-based continual RL methods (IMPALA, Online EWC, CLEAR, Modulating Masks) in Starpilot. While Adam suffers from plasticity loss in later levels, Trac effectively mitigates this and maintains better performance over distribution shifts. For clarity, standard deviation fills are omitted here but included in the bar plot.

Figure 13: Average normalized rewards over five seeds and 120M timesteps for Dodeball, Chaser, and Fruitbot. Each method (IMPALA, Online EWC, CLEAR, and Modulating Masks) is evaluated using both Adam and Trac. Trac consistently outperforms Adam across all methods and environments, with improvements ranging from 10% to 21%.

Tuning weight decay:In addition to LayerNorm and Plasticity Injection, we also evaluated the effects of tuning weight decay using PyTorch's AdamW optimizer. We conducted a hyperparameter sweep across three control environments with 15 seeds for each of the following weight decay values: 0.0001, 0.001, 0.01, 0.1, 1.0, 5.0, 10.0, 15.0, and 50.0. Figure 16 presents the average normalized reward for each weight decay value over 15 seeds and 3000 timesteps, compared to Trac.

The results indicate that while tuning weight decay with Adam does provide some benefit, these values consistently underperform in comparison to Trac across all three control environments. Figure 16 plots the performance of the best-performing weight decay value with Adam over 10 distribution shifts in the control environments. We observe that weight decay values are highly sensitive to the specific environment and the nature of the distribution shift.

Interestingly, in our initial experiments, we set the weight decay to zero, yet Trac still outperformed Adam with various weight decay values. This suggests that while weight decay can mitigate plasticity loss to some extent, it does not match the overall effectiveness of Trac.

## Appendix F Scaling-Value Convergence

As discussed in the algorithm section (see Section 3), Trac operates as a meta-algorithm on top of a standard optimizer, denoted as Base. The crucial component of Trac involves the dynamic adjustment of the scaling parameter \(S_{t+1}\), managed by the tuner algorithm (Algorithm 2). This parameter is data-dependent and typically ranges between \([0,1]\). The weight update \(\theta_{t+1}\) is consequently defined as a convex combination of the current optimizer's weight \(\theta_{t}^{\text{Base}}\) and a predetermined reference point \(\theta_{\text{ref}}\).

This section presents the convergence behavior of the scaling parameter \(S_{t+1}\) across different environments, analyzed through the mean values over multiple seeds.

Figure 14: Mean Episode Reward for Adam PPO on CartPole-v1 with varying gravity. Adam PPO demonstrates robust policy recovery across most gravity-based distribution shifts.

Figure 15: Performance comparison of plasticity loss mitigation techniques across Gym Control environments. Both layer normalization and plasticity injection reduce plasticity loss when applied with Adam. Trac outperforms both layer norm Adam and plasticity injection Adam, with the combination of layer norm and Trac achieving the highest performance.

The convergence of the scaling parameter \(S_{t+1}\) observed across the Procgen and Gym Control environments, as depicted in Figures 17 and 19, reflects a good scaling value that effectively determines the strength of regularization towards the initialization points, yielding robust empirical outcomes in lifelong RL settings. Interestingly, in Procgen environments, this converged scaling value exhibits consistency across various games, typically hovering between 0.02 and 0.03, as shown in Figure 17. In contrast, in the Gym Control environments, the scaling values are lower, ranging between 0.005 and 0.01, as illustrated in Figure 19.

## Appendix G Comparison to Mechanic

In our analysis, we extend the examination to other OCO-based optimizers within the lifelong RL setup. Table 3 presents a comparative assessment of Trac PPO and Mechanic PPO (Cutkosky et al., 2023) for the lifelong Gym Control tasks (with 300 seed runs). The p-values were calculated using two-sample t-tests to test the hypothesis that the means between Trac and Mechanic are the same (Null Hypothesis, \(H_{0}\)) against the alternative hypothesis that they are different (Alternative Hypothesis, \(H_{1}\)). The results indicate that while Mechanic effectively mitigates plasticity loss and adapts quickly to new distribution shifts, it slightly underperforms in comparison to Trac.

Figure 16: Effect of weight decay on performance in the three Gym Control environments. Bar plots show the average normalized rewards over 25 seeds for different weight decay values using Adam across 3000 timesteps, compared to Trac with no weight decay.

Figure 17: Convergence of the scaling parameter \(S_{t+1}\) in the Procgen environments.

## Appendix H Experimental Setup

Procgen and Atari Vision backboneFor both the Atari and Procgen experiments, the Impala architecture was used as the vision backbone. The Impala model had 3 Impala blocks, each containing a convolutional layer followed by 2 residual blocks. The output of this is flattened and connected to a fully connected layer. The impala model parameters are initialized using Xavier uniform initialization.

Policy and Value NetworksAcross all experiments--including Gym Control, Atari, and Procgen--the policy and value functions are implemented using a multi-layer perceptron (MLP) architecture. This architecture processes the input features into action probabilities and state value estimates. The MLP comprises several fully connected layers activated by ReLU. The output from the final layer uses a softmax activation.

TracTrac, for all experiments, was implemented using the same experiment-specific baseline architectures and baseline optimizer. For the Procgen and Atari experiments, the base Adam optimizer was configured as the same as baseline, with a learning rate of 0.001, and for the Gym Control experiments, a learning rate of 0.01 was used. Both learning rates were tested for all experiments and found to have negligible differences in performance outcomes. Other than the learning rate, we use the default Adam parameters, including weight decay and betas, followed by the specifications outlined in the PyTorch Documentation.5

Footnote 5: https://pytorch.org/docs/stable/generated/torch.optim.Adam.html

The setup for Trac included \(\beta\) values for adaptive gradient adjustments: 0.9, 0.99, 0.999, 0.9999, 0.99999, and 0.999999. Both \(S_{t}\) and \(\varepsilon\) were initially set to (\(1\times 10^{-8}\)). Modifications were made to a PyTorch error function library, which accepts complex inputs to accommodate the necessary computations for the imaginary error function. This library can be found at Torch Erf GitHub.6

Footnote 6: https://github.com/rednsic/torch_erf

Distribution ShiftsIn the Atari experiments, game environments were switched every 4 million steps. The sequence for games with an action space of 6 included "BasicMath", "Qbert", "SpaceInvaders", "UpNDown", "Galaxian", "Bowling", "Demonat attack", "NameThisGame", while games with an action space of 9 included "LostLuggage", "VideoPinball", "BeamRider", "Asterix", "Enduro", "CrazyClimber", "MsPacman", "Koolaid".

Figure 19: Convergence of the scaling parameter \(S_{t+1}\) in the Gym Control environments.

Figure 18: Evolution of the scaling parameter \(S_{t+1}\) in the Atari environments. Here we don’t see a meaningful convergence of \(S_{t+1}\).

[MISSING_PAGE_FAIL:21]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: As said in our abstract, we offer a parameter-free optimizer that performs well in empirical lifelong RL environments. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have a limitations paragraph in our discussion section (5). Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: We do not provide a novel theoretical result. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We include a comprehensive experimental setup section in our Appendix (H). We also provide a code submission. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: We provide a python jupyter notebook in our supplementary material to walk through how to setup and reproduce the results for the computationally less expensive experiments (which have a similar setup to the computationally expensive experiments). Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The full experimental details can be found in H and in the code submission in our supplementary materials. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Our reward plots include standard deviation error bars around the curves. We detail this in the experimental setup section of our Appendix (H). Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer:[Yes] Justification: Full details of our copmute for each experiment can be found in our H. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We follow the NeurIPS Code of Ethics in every manner. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA]. Justification: Not applicable. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA]. Justification: Not applicable. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA]. Justification: We dot use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Not applicable. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Not applicable. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Not applicable. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.