# Activation Map Compression through Tensor Decomposition for Deep Learning

 Le-Trung Nguyen  Ael Quelennec  Enzo Tartaglione

Samuel Tardieu  Van-Tam Nguyen

LTCI, Telecom Paris, Institut Polytechnique de Paris

{name.surname}@telecom-paris.fr

###### Abstract

Internet of Things and Deep Learning are synergetically and exponentially growing industrial fields with a massive call for their unification into a common framework called Edge AI. While on-device inference is a well-explored topic in recent research, backpropagation remains an open challenge due to its prohibitive computational and memory costs compared to the extreme resource constraints of embedded devices. Drawing on tensor decomposition research, we tackle the main bottleneck of backpropagation, namely the memory footprint of activation map storage. We investigate and compare the effects of activation compression using Singular Value Decomposition and its tensor variant, High-Order Singular Value Decomposition. The application of low-order decomposition results in considerable memory savings while preserving the features essential for learning, and also offers theoretical guarantees to convergence. Experimental results obtained on main-stream architectures and tasks demonstrate Pareto-superiority over other state-of-the-art solutions, in terms of the trade-off between generalization and memory footprint.1

Footnote 1: Code: https://github.com/Le-TrungNguyen/NeurIPS2024-ActivationCompression.git

## 1 Introduction

Recent advances in Deep Learning have enabled Deep Neural networks to be used as an efficient solution for a wide variety of use cases, including computer vision [21, 39, 30], speech recognition [8, 31] and natural language processing [45, 42]. Much of this performance improvement is linked to the exponential increase in the number of parameters in the neural architectures. According to Sevilla _et al._, the release of AlphaGo [47] in late 2015 marks the advent of a new era, which they call the "Large Scale Era", in reference to the computational cost of training doubling every 8 to 17 months [38]. At the root of exponential growth in neural network size is the improvement in hardware capabilities, particularly those designed for large-scale parallel computing, such as GPUs and TPUs [1]. While this trend demonstrates the strength of neural networks as a powerful generalization tool in many fields, it goes in the opposite direction when it comes to environmental concerns [41], making the deployment of newer architectures increasingly difficult. This is particularly true for edge devices such as mobile phones and embedded sys

Figure 1: We compress the activations that will be later employed for backpropagation.

tems, which cannot afford the high computing or memory costs. To address these challenges, three interconnected factors must be taken into account: power consumption, memory usage, and latency.

When considering larger neural networks with more layers and nodes, reducing their storage and computational cost becomes essential, especially for certain real-time applications such as edge computing. In addition, recent years have seen significant advances in the fields of intelligent edge and embedded intelligence, creating unprecedented opportunities for researchers to address the fundamental challenges of deploying deep learning systems on edge devices with limited resources (e.g. memory, CPU, power, bandwidth). Efficient deep learning methods can have a significant impact on distributed systems and embedded devices for artificial intelligence. Since training is supposed to take place in the cloud, most research on model compression and acceleration is specifically focused on inference [7]. There is, however, an emerging area of research concerning on-device training, which represents a decisive advance in the field of artificial intelligence, with considerable implications for a variety of practical situations [17]. Models trained offline on a dataset built at one point in time tend to fall victim to data drift when deployed "in the wild" [36]. Its combination with online learning strategies has the potential to enable continuous model improvement after deployment [14], thus adapting the model predictions to observed evolutions in the data distribution. We can illustrate this with the example of sensors in autonomous vehicles, where deep learning models must correctly classify vehicles with new designs never seen in the training set. Other advantages of on-device learning include security and privacy. By processing data locally, sensitive information remains more secure and less susceptible to data breach, a major concern in applications such as healthcare.

The main challenge limiting the feasibility of on-device learning lies in the computational demands of the backward pass, as gradient computation and parameter updates are significantly more resource-intensive than the forward pass (Appendix A.1). On embedded devices, memory and computation limitations act as strict budgets that must not be exceeded. Some approaches address the memory constraints by exploring alternatives to traditional backpropagation, including unsupervised learning for image segmentation [51], the Forward-Forward algorithm [16], and PEPITA [34]. While promising, these methods typically fall short of backpropagation-based techniques in terms of performance. A pioneering effort by Lin _et al._, demonstrated that fine-tuning a deep neural network within a 256 kB of memory is feasible by selectively updating a sub-network, achieving a good results [25]. In a complementary approach, Yang _et al._ proposed reducing the number of unique elements in the gradient map through patch-based compression of the input and gradients of a given layer with respect to the output, thereby lowering memory costs and speeding up the learning process [53].

Inspired by tensor decomposition methods, we propose a method that compresses activation maps, reducing the memory demands for backpropagation while maintaining the deep neural network's generalization capability(Fig. 1). Our approach adaptively captures the majority of tensor variance, offering guaranteed accuracy in the gradient estimation. The key contributions of our work are as follows:

* We propose to exploit powerful low-rank approximation algorithms to compress activation maps, enabling efficient on-device learning with controlled information loss (Sec. 3.2 and Sec. 3.3).
* We provide a theoretical foundation for our method, along with an error analysis demonstrating that high compression ratios are achievable with limited performance degradation (Sec. 3.4).
* We extensively explore a diverse experimental landscape, demonstrating the generalization capacity of our proposed algorithm (Sec. 4).

## 2 Related Works

**Tensor Decomposition.** Model compression and acceleration is a well-developed and ongoing area of research in the deep learning community. It can be divided into two major areas, namely hardware and algorithm design. In the case of algorithmic compression, the five main components that stand out are efficient and compact model design, data quantization, network sparsification, knowledge distillation, and tensor decomposition [4; 7]. All these approaches aim to reduce the space occupied by network parameters. Also known as a low-rank approximation, _tensor decomposition_ has emerged as a robust solution due to its combination of grounded theoretical findings and its practically in terms of hardware implementation [49]. Originating in the field of systems theory and signal processing [28], low-rank approximation has attracted growing interest in the deep learning community. Early examples were limited to the application of singular value decomposition (SVD) used to compress fully connected layers [50]. More recent work includes the application of generalized Kronecker product decomposition (GKPD) to both linear and convolutional layers [13], the introduction of semi-tensor product (STP) to improve compression ratios with reduced loss [56] or even more recently the acceleration of inference and backpropagation in vision transformers [52]. Evolutions in this domain can be summed up as improvements in compression ratios, latency, and power consumption, with limited performance loss and diversification of architectures considered.

**Activation Map Compression.** The term "activations" here refers to the outputs of each layer after the non-linearity has been applied, which are then fed to the next layer. While model compression generally aims at reducing the storage space occupied by parameters (which typically translates into network acceleration during inference), a key observation is that activations occupy much more space than parameters in memory during backward pass, as they are required to compute weight derivatives [2]. This state of facts motivates a new line of research, whose main objective is to compress activation maps using methods inspired by the literature on weight compression. For example, we mention the use of quantization [9], sparsification [22], a combination of both with the addition of entropy encoding [12], or even the application of wavelet transform in combination with quantization [10]. With the exception of Eliassen _et al._'s work which accelerates training runtime, most of these works focus on accelerating inference, in a similar way to traditional model compression.

**On-device Learning.** Today's typical pipeline for on-device AI consists of designing resource-efficient deep neural networks [24], training them offline on target data, compressing them, and deploying them for inference on the resource-constrained environment. Alongside the rapid commercial expansion of the IoT field and the ubiquitous presence of embedded devices in our daily lives, AI at the edge has naturally attracted a great deal of interest. However, recent research has shown that real-world data often deviated from training data, resulting in poor predictive performance [40]. In such a context, continual learning proved to be a strong candidate for ensuring ongoing adaptation after deployment. Similar to the human learning process, it provides models with the ability to adapt to new incoming tasks, ideally without falling into the trap of catastrophic forgetting (i.e., degrading performance on older tasks in favor of new ones) [11; 33; 19]. In this respect, the very low memory and computing resources of extreme edge devices are an obstacle to the high cost of backpropagation. Lin _et al._ have shown that this challenge can be overcome by selectively fine-tuning a sub-graph of pre-trained networks on a general dataset [25]. Their work started a line of research showing the possibility of improvement through careful selection of channels to be updated at training time [23; 35]. However, none of these works addresses the computational cost of training a neuron, which is related to both the cost of loading the necessary weights and activations into RAM.

**Gradient Filtering.** In their work, Yang _et al._ demonstrate that it is possible to significantly reduce the memory and computational cost of full layer learning using a method called gradient filtering, which reduces the number of unique elements in the gradient map [53]. In such a frame and with the same specific objective, we propose to compress the activation maps by using tensor decomposition to minimize the memory cost. We perform this while providing a strong guarantee on the signal loss: unlike gradient filtering, we adaptively size the decomposed tensor to guarantee minimal information loss. In the next section, we will illustrate our proposed approach.

## 3 Method

In this section, we motivate our compression proposal by exposing the major memory bottleneck of backpropagation (Sec. 3.1). We then present our contribution which is the compression of activation maps through two well-known low-rank decomposition strategies, namely Singular Values Decomposition (SVD) and its tensor counterpart Higher Order SVD (HOSVD) (Sec. 3.2). In Sec. 3.3 we explore the induced changes to backpropagation and in Sec. 3.4 we study the resulting memory and computational complexity, alongside an evaluation of the error introduced. Our ultimate goal here is to reduce the memory footprint of backpropagation (BP).

### The Memory Bottleneck of Backpropagation

Following the formalism introduced in [2], we consider a simple convolutional neural network (CNN) represented as a sequence of \(n\) convolutional layers (excluding the bias for simplicity):

\[\mathcal{F}(\mathcal{X})=(\mathcal{C}_{\mathcal{W}_{n}}\circ\mathcal{C}_{ \mathcal{W}_{n-1}}\circ\cdots\circ\mathcal{C}_{\mathcal{W}_{2}}\circ\mathcal{C }_{\mathcal{W}_{1}})(\mathcal{X}),\] (1)

where \(\mathcal{W}_{i}\in\mathbb{R}^{C^{\prime}\times C\times D\times D}\) represents the filter parameters of the \(i^{th}\) layer, with a kernel size of \(D\times D\). This layer receives a \(C\)-channel input and produces an output with \(C^{\prime}\) channels. For this layer, we denote the input and output activation tensors as \(\mathcal{A}_{i}\in\mathbb{R}^{B\times C\times H\times W}\) and \(\mathcal{A}_{i+1}\in\mathbb{R}^{B\times C^{\prime}\times H^{\prime}\times W^{ \prime}}\), respectively. Note that \(H\) and \(W\) denote the height and width of each element of the input, while \(H^{\prime}\) and \(W^{\prime}\) denote the height and width of each element of the output, with \(B\) as the minibatch size. The loss \(\mathcal{L}\) is computed at the output of the network and backpropagated until the \(i^{th}\) layer as \(\frac{\partial\mathcal{L}}{\partial\mathcal{A}_{i+1}}\). At this stage, the filter parameters are updated thanks to the computation of \(\frac{\partial\mathcal{L}}{\partial\mathcal{W}_{i}}\) and the loss is propagated to the previous layer as \(\frac{\partial\mathcal{L}}{\partial\mathcal{A}_{i}}\). The computation of these terms follows the chain rule:

\[\frac{\partial\mathcal{L}}{\partial\mathcal{W}_{i}} =\frac{\partial\mathcal{L}}{\partial\mathcal{A}_{i+1}}\cdot\frac{ \partial\mathcal{A}_{i+1}}{\partial\mathcal{W}_{i}}=\operatorname{conv}\left( \mathcal{A}_{i},\frac{\partial\mathcal{L}}{\partial\mathcal{A}_{i+1}}\right),\] (2) \[\frac{\partial\mathcal{L}}{\partial\mathcal{A}_{i}} =\frac{\partial\mathcal{L}}{\partial\mathcal{A}_{i+1}}\cdot\frac{ \partial\mathcal{A}_{i+1}}{\partial\mathcal{A}_{i}}=\operatorname{conv}_{ \text{full}}\left[\frac{\partial\mathcal{L}}{\partial\mathcal{A}_{i+1}}, \operatorname{rot}(\mathcal{W}_{i})\right],\] (3)

where \(\operatorname{conv}(\cdot)\) is the traditional convolution operation, convolving the kernel \(\frac{\partial\mathcal{L}}{\partial\mathcal{A}_{i+1}}\) with the input \(\mathcal{A}_{i}\); while \(\operatorname{conv}_{\text{full}}(\cdot)\) is the convolutional operation that naturally maps the input \(\frac{\partial\mathcal{L}}{\partial\mathcal{A}_{i+1}}\) to an output with the same dimensions as \(\mathcal{A}_{i}\) by using the \(180^{\circ}\) rotated kernel \(\mathcal{W}_{i}\).

From (2), it is clear that computing the weight derivatives requires to load input activation \(\mathcal{A}_{i}\) and (3) shows that the weights \(\mathcal{W}_{i}\) must be loaded into memory to calculate activation derivatives. We obtain the same conclusions for linear layers and provide the demonstration in Appendix A.2.

To save memory, two possibilities naturally emerge: either compressing the weights or compressing the activation. Weight compression is an extensively explored matter for network acceleration and we do not intend to further this area of research in this work. This leaves us with activation compression, which is still a new domain of exploration, but shows great potential for prospectively enabling on-device backpropagation. In such a regard, thanks to its strong theoretical grounding, tensor decomposition stands as a promising approach.

### Tensor Decomposition

We will present here first the general Singular Value Decomposition (SVD) approach, which is then extended to a multidimensional variant (HOSVD), instrumental for our purposes.

**SVD.** Given a matrix \(A\in\mathbb{R}^{M\times N}\), applying SVD to \(A\) consists in a factorization of the form:

\[A=U\Sigma V^{T},\qquad U\in\mathbb{R}^{M\times M},\quad\Sigma\in\mathbb{R}^{ M\times N},\quad V\in\mathbb{R}^{N\times N},\] (4)

where \(\Sigma\) is a rectangular diagonal matrix composed of \(r\) singular values \(s_{i\in[1,r]}\) with \(r\) the rank of \(A\). From this, we can deduce the amount of overall variance \(\sigma_{i}^{2}\) explained by the \(i^{th}\) pair of SVD vectors as \(\sigma_{i}^{2}=s_{i}^{2}/\sum_{j}s_{j}^{2}\).

Let us assume the singular values in \(\Sigma\) are ordered in descending order, \(s_{i}\geq s_{j},\forall i\leq j\). Given a desired threshold of cumulated explained variance \(\varepsilon\in[0,1]\), it becomes easy to find the "truncation threshold" which is the minimal \(K\in[1,r]\) such that \(\sum_{i=1}^{K}\sigma_{i}^{2}\geq\varepsilon\). We can then approximate \(A\) by only selecting the \(K\) first columns of \(U\) and \(V\) and the \(K\) first singular values from \(\Sigma\), according to:

\[\tilde{A}=U_{(K)}\Sigma_{(K)}V_{(K)}^{T},\qquad U_{(K)}\in\mathbb{R}^{M\times K },\quad\Sigma_{(K)}\in\mathbb{R}^{K\times K},\quad V_{(K)}\in\mathbb{R}^{N\times K}.\] (5)

Historically, SVD was the first example of tensor decomposition applied to neural network compression and acceleration [55] but applied to the model's parameters [49]. We take the SVD decomposition to the activation maps as one possible baseline to compare with more complex low-rank compression solutions.

In the case of convolutional layer \(i\), since SVD is designed for matrix decomposition, we will simply reshape the activation \(\mathcal{A}_{i}\) as matrix \(A_{i}\) of dimensions \(B\times(CHW)\). Given a desired level of explained variance \(\varepsilon\), we obtain the decomposition described in (5). We then store in memory the terms\(U_{(K)}\times\Sigma_{(K)}\) and \(V_{(K)}^{T}\), meaning that instead of storing \(\Theta_{\text{space}}(BCHW)\) unique elements in memory, we are only storing \(\Theta_{\text{space}}[K(B+CHW)]\) unique elements. Regarding linear layers, activations are \(M\times N\) matrices, and applying SVD is much more straightforward, leading to the storage cost of \(\Theta_{\text{space}}[K(M+N)]\) instead of \(\Theta_{\text{space}}(MN)\) elements. The larger the explained variance, the closer \(\tilde{A}\) will be to \(A\), intuitively allowing for better estimation when performing backpropagation. However, this also means a larger \(K\) which results in a larger memory occupation. The goal is then to find an efficient trade-off between the desired explained variance and compression rates.

**HOSVD.** By construction, SVD is designed for matrix decomposition. It was demonstrated that the reshaping operation on tensors introduced structure information distortion, leading to sub-optimal performance [49]. Other methods more suited for tensor decomposition such as Canonical-Polydic (CP) decomposition [20] or Tucker decomposition [43] were naturally introduced to tackle this issue. Given a \(n^{th}\)-order tensor \(\mathcal{T}\in\mathbb{R}^{M_{1}\times M_{2}\times\cdots\times M_{n}}\), its Tucker decomposition corresponds to:

\[\mathcal{T}=\mathcal{S}\times_{1}U^{(1)}\times_{2}U^{(2)}\times_{3}\cdots \times_{n}U^{(n)},\] (6)

where \(\mathcal{S}\in\mathbb{R}^{L_{1}\times L_{2}\times\cdots\times L_{n}}\) is the core tensor which can be viewed as a compressed version of \(\mathcal{T}\), \(U^{(j)}\in\mathbb{R}^{M_{j}\times L_{j}}\) are the factor matrices and their columns correspond to the principal components over the \(j^{th}\) mode. The \(i\)-mode product "\(\times_{i}\)" of a \(n^{th}\)-order tensor \(\mathcal{G}\in\mathbb{R}^{P_{1}\times P_{2}\times\cdots\times P_{n}}\) and a matrix \(B\in\mathbb{R}^{Q\times P_{i}}\) is a \(n^{th}\)-order tensor \(\mathcal{R}\in\mathbb{R}^{P_{1}\times\cdots\times P_{i-1}\times Q\times P_{i+ 1}\times\cdots\times P_{n}}\) which can be expressed as:

\[\mathcal{R}_{p_{1},\ldots,p_{i-1},q,p_{i+1},\ldots,p_{n}}=\mathcal{G}\times_{ i}Q=\sum_{p_{i}=1}^{P_{i}}g_{p_{1},p_{2},\ldots,p_{n}}b_{q,p_{i}}.\] (7)

In such a setup, Higher-Order SVD is a specific case of Tucker decomposition where the factor matrices \(U^{(j)}\) are orthogonal [44]. As a follow-up to SVD, we propose to compress activation tensors through HOSVD, with the intuition that each dimension encodes a different variance, potentially providing enhanced compression rates for equivalent performance and vice-versa. Similarly to traditional SVD, we can truncate \(\mathcal{S}\) and each \(U^{(j)}\) along each mode given a desired level of explained variance resulting in:

\[\tilde{\mathcal{T}}=\hat{\mathcal{S}}\times_{1}U^{(1)}_{(K_{1})}\times_{2} \cdots\times_{n}U^{(n)}_{(K_{n})}\approx\mathcal{T},\] (8)

where \(U^{(j)}_{(K_{j})}\in\mathbb{R}^{M_{j}\times K_{j}}\) corresponds to the \(K_{j}\) first columns of \(U^{(j)}\) and \(\hat{\mathcal{S}}\in\mathbb{R}^{K_{1}\times\cdots\times K_{n}}\) is the truncated version of \(\mathcal{S}\). In the following section, we focus on HOSVD decomposition, more precisely on the alterations induced to the backpropagation graph, the resulting compression and speedup ratio, and the error bound. An equivalent analysis can be made for SVD.

### Backpropagation with Compressed Activations

Prior works [13; 18] already demonstrated the possibility of performing backpropagation operations in the decomposed space without relying on the recomposition of compressed tensors. In their work, Kim _et al_. compress a \(4^{th}\)-order kernel tensor \(\mathcal{W}\in\mathbb{R}^{C^{\prime}\times C\times D\times D}\) through Tucker decomposition limited to mode \(1\) and \(2\):

\[\mathcal{W}=\mathcal{W}^{\prime}\times_{1}U^{(1)}\times_{2}U^{(2)},\qquad \mathcal{W}^{\prime}\in\mathbb{R}^{L_{1}\times L_{2}\times D\times D},\quad U ^{(1)}\in\mathbb{R}^{C^{\prime}\times L_{1}},\quad U^{(2)}\in\mathbb{R}^{C \times L_{2}}.\] (9)

Then, it is shown that the output is calculated through

\[\mathcal{Y}=\mathrm{conv}_{1\times 1}\{U^{(2)},\mathrm{conv}_{D\times D}[ \mathcal{W}^{\prime},\mathrm{conv}_{1\times 1}(U^{(1)},\mathcal{X})]\}.\] (10)

Our problem is similar although with some fundamental differences linked to the need to reconstruct _gradients_. A convolution operator is still required to compute the weight derivatives as introduced in (2): we apply a specific case of Tucker to one of the two components, namely the activation \(\mathcal{A}_{i}\). Following the same reasoning, we derive that the computation of \(\frac{\partial\mathcal{L}}{\partial\mathcal{W}_{i}}\) in (2) becomes:

\[\frac{\partial\mathcal{L}}{\partial\mathcal{W}_{i}}=\mathrm{conv}_{1\times 1} \left\{\mathrm{conv}_{*}\left[\mathrm{conv}_{1\times 1}\left(\mathrm{conv}_{1 \times 1}\left(\hat{\mathcal{S}},\underline{U}^{(3)}_{(K_{3})}\right), \underline{U}^{(4)}_{(K_{4})}\right),\mathrm{conv}_{1\times 1}\left( \frac{\partial\mathcal{L}}{\partial\mathcal{A}_{i+1}},U^{(1)}_{(K_{1})} \right)\right],U^{(2)}_{(K_{2})}\right\},\] (11)

where \(\mathrm{conv}_{*}\) is a 2D convolution with a specific kernel size as defined in (23), and \(\underline{U}^{(j)}_{(K_{j})}\) is the vertically padded version of \(U^{(j)}_{(K_{j})}\) as defined in (19). Demonstration details are available in Appendix A.3. This way, we can compute the approximated weight derivative without reconstructing the activation, through the successive computation of simpler convolutions.

Fig. 1 illustrates our method. During training, the forward pass proceeds as usual, with one key modification to memory management. Instead of keeping complete activation maps in memory, we store only their principal components, which are derived from the decomposition process. During the backward pass, the principal components are retrieved from memory and used for calculations as described in (11).

### Complexity and Error Analysis

**Computational Speedup and Space Complexity.** For simplicity we assume here that \(\mathcal{A}_{i+1}\) and \(\mathcal{A}_{i}\) have the same shape and that the truncation thresholds across all modes are equal for HOSVD, i.e., \(K_{1}=K_{2}=K_{3}=K_{4}\). Considering different thresholding values, we can predict the relative improvement of HOSVD to vanilla training in both space complexity (30) and computational speedup (29), for a backward pass. As illustrated in Fig. 1(a) and Fig. 1(b), our method is more effective for both space complexity and latency for the backward pass when the truncation threshold decreases and the activation size increases. More details are provided in Appendix A.4.

We hypothesize that the first components along each dimension are enough to encode most of the variance, implying that with relatively low values of \(K_{j}\), we can achieve good training performance. We later empirically confirm this assumption in a variety of experimental setups (Sec. 4.2).

**Signal to Noise Ratio.** For simplicity, we rewrite in this paragraph \(\frac{\partial\mathcal{L}}{\partial\mathcal{W}_{i}}\) as \(\Delta\mathcal{W}\), \(\mathcal{A}_{i}\) as \(\mathcal{I}\) and \(\frac{\partial\mathcal{C}}{\partial\mathcal{A}_{i+1}}\) as \(\Delta\mathcal{Y}\). Regarding the error introduced by truncating \(\mathcal{I}\) in order to retain the components corresponding to a given level of explained variance \(\varepsilon\in[0,1]\), we demonstrate that the energy contained in the resulting gradient is equal to the energy contained in \(\tilde{\mathcal{I}}\). We note \(I,\Delta W\) and \(\Delta Y\) as the input activation, weight derivative, and output activation derivative in the frequency domain; \(I[u,v]\), the spectrum value at frequency \((u,v)\). Applying the discrete Fourier transformation to \(\tilde{\mathcal{I}}\) gives us \(\tilde{I}=\varepsilon I\), which we use to compute the signal to noise ratio \(SNR_{\tilde{I}}\):

\[SNR_{\tilde{I}}=\frac{\sum_{(u,v)}I[u,v]^{2}}{\sum(I[u,v]- \varepsilon I[u,v])^{2}}=\frac{1}{(1-\varepsilon)^{2}}.\] (12)

Similarly, in the frequency domain, as the convolutional operation becomes a regular multiplication, \(\Delta\tilde{\mathcal{V}}\) becomes \(\Delta\tilde{W}=\tilde{I}\Delta Y=\varepsilon I\Delta Y\). As for (12), we obtain \(SNR_{\Delta\tilde{W}}=(1-\varepsilon)^{-2}=SNR_{\tilde{I}}\). A visual representation is provided in Fig. 1(c). Analytically, this confirms the idea that the closer \(\varepsilon\) is to \(1\), the larger the energy is transferred from the compressed input to the weight derivatives (for \(\varepsilon=0.8\) we have \(SNR_{\Delta\tilde{W}}=25\)).

Additionally, in our setup since we are only compressing the activations and given (2) and (3), the introduced error only transfers to the weight derivative for each layer. The activation derivatives that are propagated from one layer to another are exact computations as they do not involve the

Figure 2: For a single convolutional layer with minibatch size \(B\), **(a)** and **(b)** illustrate the predicted changes in compression rate \(R_{C}\) and speedup ratios \(R_{S}\) as functions of \(K_{j}\), when comparing HOSVD with vanilla training, respectively. **(c)** shows the evolution of the SNR with retained variance \(\varepsilon\).

activations. This means that the error introduced when compressing the activations at each layer does not accumulate through the network.

## 4 Experiments

In this section, we describe the experiments conducted to support the claims presented in Sec. 3. First, we introduce the setups used for our experiments (Sec. 4.1); then, we analyze the energy distribution in the different dimensions of HOSVD, providing an overview of the typical values of \(K\) (Sec. 4.2); finally, we test our algorithm in different setups, state-of-the-art architectures and datasets to evaluate the tradeoff between accuracy and memory footprint (Sec. 4.3). Experiments were performed using a NVIDIA RTX 3090Ti and the source code uses PyTorch 1.13.1.

### Experimental setup

To validate our approach, we perform a variety of computer vision experiments split across two tasks, classification and segmentation.

**Classification.** We explore two types of fine-tuning policies:

* _Full fine-tuning_ (referred to as "setup A"): Following conventional fine-tuning trends, we load models pre-trained on ImageNet [21] and we fine-tune them on a variety of downstream datasets (CIFAR-10, CIFAR-100, CUB [46], Flowers [32] and Pets [54]).
* _Half fine-tuning_ (referred to as "setup B"): Following Yang _et al._ approach, each classification dataset (ImageNet, CIFAR-10/100) is split into two non-i.i.d. partitions of equal size using the FedAvg [29] method. The partitions are then split as follows: \(80\%\) for training and \(20\%\) for validation. The first partition is used for pretraining, and the second partition is used for finetuning.

**Semantic Segmentation.** Similarly to the half fine-tuning method, we reproduce Yang _et al._ segmentation setup: we fine-tune models pretrained on Cityscapes [6] by MMSegmentation [5]. Here there is only one downstream dataset which is Pascal-VOC12 [3]. Experimental details for both tasks (hyper-parameters, policy, etc.) are provided in Appendix B.2.

**Memory Logging.** For HOSVD and SVD, instead of focusing on compressing the tensor based on rank, we control compression through the desired amount of retained information. As a consequence, we cannot explicitly control the memory usage of the principal components. In the results presented below, we will always include two columns displaying peak and average memory, along with their standard deviations.

### Explained Variance Evolution

In this section, we conduct experiments to fine-tune the last four convolutional layers of MCUNet [24] following Setup A, and CIFAR-10 as the downstream dataset. Using HOSVD with \(\varepsilon\) set to \(0.8\), Fig. 3 shows the explained variance retained across dimension \(j\) as a function of \(K_{j}\), where \(j=\{1,2\}\) corresponds to the two largest dimensions of the activation map. We define \(K_{j}^{(\varepsilon=x)}\) as the number of principal components required to retain at least a fraction \(x\) of the explained variance.

Figure 3: Explained variance \(\varepsilon\) for the first two dimensions of the activation map in the \(4^{th}\) last layer when fine-tuning the last four layers of MCUNet using HOSVD on CIFAR-10, following setup A.

We observe that along the largest dimensions (batch size and number of channels), less than \(20\%\) of the components capture more than \(80\%\) of the explained variance, confirming the assumption proposed in Sec. 3.4. Additionally, the curves observed present a logarithmic behavior, hinting at the possibility of reaching high explained variance with little loss regarding the compression ratio. This is especially important as the \(SNR\) transmitted to the weight derivatives increases quadratically to the explained variance (Fig. 2c). The results for the other layers are presented in Appendix B.3.

Fig. 4 illustrates the results of performing HOSVD with different explained variance thresholds \(\varepsilon\). The results are averaged over three different random seeds. For \(\varepsilon\) smaller than \(0.8\), we observe that as it increases, we achieve significant gains in accuracy, along with impressive compression ratios. However, when \(\varepsilon\) exceeds \(0.8\), the accuracy growth starts slowing down. Above the \(0.9\) threshold, the exponential growth of peak memory results in a worsened tradeoff between accuracy and compression ratio. Therefore, in subsequent experiments presented in this paper, we will use \(\varepsilon\) values of \(0.8\) and \(0.9\).

### Main Results

**MCUNet on classification with setup A.** In this experiment, we fine-tune on CIFAR-10 an MCUNet pre-trained on ImageNet, with the number of finetuned layers increasing from \(1\) to \(42\) (all layers). We compare vanilla training, and gradient filtering with patch sizes of \(2\), \(4\), and \(7\), SVD, and HOSVD with an explained variance threshold of \(0.8\). For each method, we compare the effect of fine-tuning different model depths. Fig. 5 presents the performance curves for our experiments, with the X-axis representing activation memory in kiloBytes (kB) on a logarithmic scale and the Y-axis representing the highest validation accuracy. Each marker indicates the number of convolutional layers finetuned. For example, on the yellow curve representing the HOSVD method, the marker closest to the Y-axis shows the result when finetuning the last convolutional layer, the next marker represents finetuning the last two convolutional layers, and so on. The most effective method is the one whose performance curve trends towards the upper-left corner of the plot.

Figure 4: Behavior of top1 validation accuracy and peak memory when applying HOSVD with different explained variance thresholds \(\varepsilon\) when finetuning the last four convolutional layers of an MCUNet model using the CIFAR-10 dataset on setup A.

Figure 5: Performance curves of an MCUNet pre-trained on ImageNet and finetuned on CIFAR-10 with different activation compression strategies.

We observe that as the number of finetuned layers increases, the gradient filtering accuracy increases up to a certain point, and then deteriorates, whereas the accuracy for SVD, HOSVD, and vanilla training keeps improving with additional layers, along a similar trend. Intuitively, gradient filtering might propagate errors through the layers during training, while our HOSVD method keeps the error introduced on each individual layer confined to that specific layer as demonstrated in Sec. 3.4. Moreover, we observe that for identical depths, SVD accuracies consistently exceed HOSVD ones. We hypothesize that this is due to HOSVD performing SVD across all modes of the tensor: it potentially loses information in all modes, whereas SVD only loses information in one mode.

Compared to SVD, HOSVD significantly reduces memory, given equivalent accuracy levels (up to \(18.87\) times). Similarly, for equivalent memory usage, HOSVD yields greatly improved accuracy compared to SVD (up to \(19.11\%\)). When compared to methods such as gradient filtering and vanilla training, given an equivalent memory budget, HOSVD yields significantly higher accuracies. Notably, fine-tuning all layers with HOSVD requires much less memory than fine-tuning only the last layer with vanilla training, which is consistent with the theoretical compression ratio shown in Fig. 1(a). Additional experiments with setup A can be found in Appendix B.5.

**ImageNet Classification with setup B.** Table 1 presents the classification performance and memory consumption for MobileNetV2 [37], ResNet18, and ResNet34 [15] models using various methods, including vanilla training, gradient filtering, HOSVD, and SVD on the ImageNet dataset. We observe in most cases that, for the same depth, SVD and HOSVD are competitive with the gradient filtering method in terms of performance while reaching much lower activation memory with HOSVD.

**Segmentation.** Table 2 reports the segmentation performance and memory consumption for a variety of architectures as presented in Yang _et al._. In general, we observe that increasing the level of explained variance from \(0.8\) to \(0.9\) substantially increases the performance with little trade-off on retained activation memory. This further confirms that most of the explained variance is contained in the first few components, allowing for efficient generalization with high compression rates.

## 5 Conclusion

In this work, we have addressed one of the main obstacles to on-device training. Inspired by traditional low-rank optimization approaches, we propose to compress activation maps by tensor decomposition, using HOSVD as a supporting example (Sec. 3.2). We demonstrate that the compression error introduced is bounded and confined to each individual layer considered, validating our approach

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline \multirow{2}{*}{**Method**} & \multicolumn{4}{c}{**MobileNetV2**} & \multirow{2}{*}{**Method**} & \multicolumn{4}{c}{**ResNet18**} \\ \cline{2-2} \cline{6-9}  & \#Layers & Acc \(\uparrow\) & Peak Mem (MB) \(\downarrow\) & Mean Mem (MB) \(\downarrow\) & \#Layers & Acc \(\uparrow\) & Peak Mem (MB) \(\downarrow\) & Mean Mem (MB) \(\downarrow\) \\ \hline Vanilla & \(\Delta\)I & 74.0 & 1651.84 & 1651.84 \(\pm\) 0.00 & \multirow{2}{*}{Vanilla} & All & 72.8 & 52.88 & 532.88 \(\pm\) 0.00 \\ training & 2 & 62.6 & 15.31 & 15.37 \(\pm\) 0.00 & \multirow{2}{*}{training} & \(\mathcal{Z}\) & 69.9 & 12.52 & 12.25 \(\pm\) 0.00 \\  & 4 & 65.8 & 28.71 & 28.71 \(\pm\) 0.00 & & & 71.5 & 30.63 & 30.63 \(\pm\) 0.00 \\ \hline Gradient & 2 & 62.6 & 5.00 & 5.00 \(\pm\) 0.00 & Gradient & 2 & 68.7 & 4.00 & 4.00 \(\pm\) 0.00 \\ \hline Filter R2 & 4 & 65.2 & 9.38 & 9.38 \(\pm\) 0.00 & Filter R2 & 4 & 69.3 & 7.00 & 7.00 \(\pm\) 0.00 \\ \hline SVD & 2 & 61.7 & 4.97 & 4.92 \(\pm\) 0.08 & SVD & 2 & 69.5 & 7.88 & 7.71 \(\pm\) 0.21 \\ \((e=0.8)\) & 4 & 65.2 & 14.76 & 14.61 \(\pm\) 0.09 & \((e=0.8)\) & 4 & 71.1 & 19.98 & 19.72 \(\pm\) 0.28 \\ \hline SVD & 2 & 62.3 & 8.97 & 8.91 \(\pm\) 0.08 & SVD & 2 & 69.7 & 9.86 & 9.77 \(\pm\) 0.13 \\ \((e=0.9)\) & 4 & 65.5 & 20.35 & 20.20 \(\pm\) 0.07 & \((e=0.9)\) & 4 & 71.3 & 24.81 & 24.66 \(\pm\) 0.17 \\ \hline HOSVD & 2 & 61.1 & 0.15 & 0.15 \(\pm\) 0.00 & HOSVD & 2 & 69.2 & 0.97 & 0.91 \(\pm\) 0.05 \\ \((e=0.8)\) & 4 & 63.9 & 0.73 & 0.68 \(\pm\) 0.03 & \((e=0.8)\) & 4 & 70.5 & 2.89 & 2.74 \(\pm\) 0.12 \\ \hline HOSVD & 2 & 61.8 & 0.43 & 0.43 \(\pm\) 0.01 & HOSVD & 2 & 69.5 & 2.73 & 2.63 \(\pm\) 0.10 \\ \((e=0.9)\) & 4 & 64.8 & 1.92 & 1.76 \(\pm\) 0.08 & \((e=0.9)\) & 4 & 71.1 & 7.96 & 7.66 \(\pm\) 0.21 \\ \hline \hline \multicolumn{9}{c}{**MUCNet**} & \multicolumn{4}{c}{**ResNet34**} \\ \cline{2-2} \cline{6-9}  & \#Layers & Acc \(\uparrow\) & Peak Mem (MB) \(\downarrow\) & Mean Mem (MB) \(\downarrow\) & \#Layers & Acc \(\uparrow\) & Peak Mem (MB) \(\downarrow\) & Mean Mem (MB) \(\downarrow\) \\ \hline Vanilla & All & 67.4 & 62.98 & 63.98 \(\pm\) 0.00 & Vanilla & All & 75.6 & 89.04 \(\pm\) 0.00 \\ training & 2 & 62.71 & 13.78 & 13.78 \(\pm\) 0.00 & \multirow{2}{*}{training} & \(\mathcal{Z}\) & 69.6 & 12.55 & 12.25 \(\pm\) 0.00 \\ \cline{2-2} \cline{6-9}  & 4 & 64.7 & 19.52 & 19.52 \(\pm\) 0.00 & & 4 & 72.2 & 24.50 \(\pm\) 0.00 \\ \hline Gradient & 2 & 61.8 & 4.50 & 4.50 \(\pm\) 0.000 & Gradient & 2 & 68.8 & 4.00 & 4.00 \(\pm\) 0.00 \\ Filter R2 & 4 & 64.4 & 6.38 & 6.38 \(\pm\) 0.00 & Filter R2 & 4 & 70.9 & 8.00 & 8.00 \(\pm\) 0.00 \\ \hline SVD & 2 & 62.0 & 7.62 & 7.51 \(\pm\) 0.12 & SVD & 2 & 69.2 & 6.70 & 6.49 \(\pm\) 0.29 \\ \((e=0.8)\) & 4 & 64.5 & 10.59 & 10.37 \(\pm\) 0.20 & \((e=0.8)\) & 4 & 71.8 & 14.68 & 14.24 \(\pm\) 0.50 \\ \hline SVD & 2 & 62.1 & 10.32 & 10.26 \(\pm\) 0.08 & SVD & 2 & 69.4 & 9.10 & 8.96 \(\pm\) 0.23 \\ \((e=0.9)\) & 4 & 64.6 & 14.39 & 14.26 \(\pm\) 0.13 & \((e=0.9)\) & 4 & 72.0 & 19.11 & 18.83 \(\pm\) 0.37 \\ \hline HOSVD & 2 & 61.7 & 0.48 & 0.43 \(\pm\) 0.04 & HOSVD & 2 & 68.7 & 0.30 & 0.27 \(\pm\) 0.02 \\ \((e=0.8)\) & 4 & 63.9 & 0.88 & 0.78 \(\pm\) 0.07 & \((e=0.8)\) & 4 & 71.1 & 1.11 & 1.02 \(\pm\) 0.07 \\ \hline HOSVD & 2 & 62.0 & 13.2 & 12.7 \(\pm\) 0.06 & HOSVD & 2 & 69.2 & 0.71 & 0.65 \(\pm\) 0.05 \\ \((e=0.9)\) & 4 & 64.4 & 2.52 & \(2.36\pm\) 0.15 & \((e=0.9)\) & 4 & 71.9 & 3.24 & 3.09 \(\pm\) 0.13 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Experimental results on ImageNet-1k. “#Layers” refers to the number of fine-tuned convolutional layers (counted from the end of the model). Activation memory consumption is shown in MegaBytes (MB).

[MISSING_PAGE_FAIL:10]

## References

* [1] Toru Baji. Evolution of the gpu device widely used in ai and massive parallel processing. In _2018 IEEE 2nd Electron devices technology and manufacturing conference (EDTM)_, pages 7-9. IEEE, 2018.
* [2] Han Cai, Chuang Gan, Ligeng Zhu, and Song Han. Tinytl: Reduce memory, not parameters for efficient on-device learning. _Advances in Neural Information Processing Systems_, 33:11285-11297, 2020.
* [3] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. _arXiv preprint arXiv:1706.05587_, 2017.
* [4] Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A survey of model compression and acceleration for deep neural networks. _arXiv preprint arXiv:1710.09282_, 2017.
* [5] MMSegmentation Contributors. Mmsegmentation: Openmmlab semantic segmentation toolbox and benchmark, 2020.
* [6] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3213-3223, 2016.
* [7] Lei Deng, Guoqi Li, Song Han, Luping Shi, and Yuan Xie. Model compression and hardware acceleration for neural networks: A comprehensive survey. _Proceedings of the IEEE_, 108(4):485-532, 2020.
* [8] Li Deng, Geoffrey Hinton, and Brian Kingsbury. New types of deep neural network learning for speech recognition and related applications: An overview. In _2013 IEEE international conference on acoustics, speech and signal processing_, pages 8599-8603. IEEE, 2013.
* 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 7430-7434, 2024.
* [10] Shahaf E. Finder, Yair Zohav, Maor Ashkenazi, and Eran Treister. Wavelet feature maps compression for image-to-image cnns. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 20592-20606. Curran Associates, Inc., 2022.
* [11] Robert M. French. Catastrophic interference in connectionist networks: Can it be predicted, can it be prevented? In _Proceedings of the 6th International Conference on Neural Information Processing Systems_. Morgan Kaufmann Publishers Inc., 1993.
* [12] Georgios Georgiadis. Accelerating convolutional neural networks via activation map compression. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7085-7095, 2019.
* [13] Marawan Gamal Abdel Hameed, Marzieh S Tahaei, Ali Mosleh, and Vahid Partovi Nia. Convolutional neural network compression through generalized kronecker product decomposition. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 771-779, 2022.
* [14] Tyler L Hayes and Christopher Kanan. Online continual learning for embedded devices. _arXiv preprint arXiv:2203.10681_, 2022.
* [15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [16] Geoffrey Hinton. The forward-forward algorithm: Some preliminary investigations. _arXiv preprint arXiv:2212.13345_, 2022.

* [17] Ozlem Durmaz Incel and Sevda Ozge Bursa. On-device deep learning for mobile and wearable sensing applications: A review. _IEEE Sensors Journal_, 2023.
* [18] Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu Yang, and Dongjun Shin. Compression of deep convolutional neural networks for fast and low power mobile applications. _arXiv preprint arXiv:1511.06530_, 2015.
* [19] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks. _Proceedings of the National Academy of Sciences_, 114, 2017.
* [20] Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. _SIAM review_, 51(3):455-500, 2009.
* [21] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. _Advances in neural information processing systems_, 25, 2012.
* [22] Mark Kurtz, Justin Kopinsky, Rati Gelashvili, Alexander Matveev, John Carr, Michael Goin, William Leiserson, Sage Moore, Nir Shavit, and Dan Alistarh. Inducing and exploiting activation sparsity for fast inference on deep neural networks. In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 5533-5543. PMLR, 13-18 Jul 2020.
* [23] Young D Kwon, Rui Li, Stylianos I Venieris, Jagmohan Chauhan, Nicholas D Lane, and Cecilia Mascolo. Tinytrain: Deep neural network training at the extreme edge. _arXiv preprint arXiv:2307.09988_, 2023.
* [24] Ji Lin, Wei-Ming Chen, Yujun Lin, johnohn, Chuang Gan, and Song Han. Mcunet: Tiny deep learning on iot devices. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 11711-11722. Curran Associates, Inc., 2020.
* [25] Ji Lin, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, and Song Han. On-device training under 256kb memory. _Advances in Neural Information Processing Systems_, 35, 2022.
* [26] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 10012-10022, 2021.
* [27] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3431-3440, 2015.
* [28] Ivan Markovsky. Structured low-rank approximation and its applications. _Automatica_, 44(4):891-909, 2008.
* [29] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In _Artificial intelligence and statistics_, pages 1273-1282. PMLR, 2017.
* [30] Shervin Minaee, Yuri Boykov, Fati Porikli, Antonio Plaza, Nasser Kehtarnavaz, and Demetri Terzopoulos. Image segmentation using deep learning: A survey. _IEEE transactions on pattern analysis and machine intelligence_, 44(7):3523-3542, 2021.
* [31] Ali Bou Nassif, Ismail Shahin, Imtinan Attili, Mohammad Azzeh, and Khaled Shaalan. Speech recognition using deep neural networks: A systematic review. _IEEE access_, 7:19143-19165, 2019.
* [32] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In _Indian Conference on Computer Vision, Graphics and Image Processing_, 2008.

* [33] German I. Parisi, Ronald Kemker, Jose L. Part, Christopher Kanan, and Stefan Wermter. Continual lifelong learning with neural networks: A review. _Neural Netw._, 113, 2019.
* [34] Danilo Pietro Pau and Fabrizio Maria Aymone. Suitability of forward-forward and pepita learning to mlcommons-tiny benchmarks. In _2023 IEEE International Conference on Omnilayer Intelligent Systems (COINS)_, 2023.
* [35] Ael Quelennec, Enzo Tartaglione, Pavlo Mozharovskyi, and Van-Tam Nguyen. Towards on-device learning on the edge: Ways to select neurons to update under a budget constraint. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 685-694, 2024.
* [36] Berkman Sahiner, Weijie Chen, Ravi K Samala, and Nicholas Petrick. Data drift in medical machine learning: implications and potential remedies. _The British Journal of Radiology_, 96(1150):20220878, 2023.
* [37] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4510-4520, 2018.
* [38] Jaime Sevilla, Lennart Heim, Anson Ho, Tamay Besiroglu, Marius Hobbhahn, and Pablo Villalobos. Compute trends across three eras of machine learning. In _2022 International Joint Conference on Neural Networks (IJCNN)_, pages 1-8. IEEE, 2022.
* [39] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. _arXiv preprint arXiv:1409.1556_, 2014.
* [40] Gabriele Spadaro, Riccardo Renzulli, Andrea Bragagnolo, Jhony H Giraldo, Attilio Fiandrotti, Marco Grangetto, and Enzo Tartaglione. Shannon strikes again! entropy-based pruning in deep neural networks for transfer learning under extreme memory and computation budgets. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2023.
* [41] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in nlp. _arXiv preprint arXiv:1906.02243_, 2019.
* [42] Ian Tenney, Dipanjan Das, and Ellie Pavlick. Bert rediscovers the classical nlp pipeline. _arXiv preprint arXiv:1905.05950_, 2019.
* [43] Ledyard R Tucker. Some mathematical notes on three-mode factor analysis. _Psychometrika_, 31(3):279-311, 1966.
* [44] M Alex O Vasilescu and Demetri Terzopoulos. Multilinear subspace analysis of image ensembles. In _2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings._, volume 2, pages II-93. IEEE, 2003.
* [45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [46] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset, 2011.
* [47] Fei-Yue Wang, Jun Jason Zhang, Xinhu Zheng, Xiao Wang, Yong Yuan, Xiaoxiao Dai, Jie Zhang, and Liuqing Yang. Where does alphago go: From church-turing thesis to alphago thesis and beyond. _IEEE/CAA Journal of Automatica Sinica_, 3(2):113-120, 2016.
* [48] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In _Proceedings of the European conference on computer vision (ECCV)_, pages 418-434, 2018.
* [49] Ou Xinwei, Chen Zhangxin, Zhu Ce, and Liu Yipeng. Low rank optimization for efficient deep learning: Making a balance between compact architecture and fast training. _Journal of Systems Engineering and Electronics_, 2023.

* [50] Jian Xue, Jinyu Li, and Yifan Gong. Restructuring of deep neural network acoustic models with singular value decomposition. In _Interspeech_, pages 2365-2369, 2013.
* [51] Junhuan Yang, Yi Sheng, Yuzhou Zhang, Weiwen Jiang, and Lei Yang. On-device unsupervised image segmentation. _arXiv preprint arXiv:2303.12753_, 2023.
* [52] Yuedong Yang, Hung-Yueh Chiang, Guihong Li, Diana Marculescu, and Radu Marculescu. Efficient low-rank backpropagation for vision transformer adaptation. In _Proceedings of the 37th International Conference on Neural Information Processing Systems_, pages 14725-14736, 2023.
* [53] Yuedong Yang, Guihong Li, and Radu Marculescu. Efficient on-device training via gradient filtering. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3811-3820, 2023.
* [54] Hui Zhang, Shenglong Zhou, Geoffrey Ye Li, and Naihua Xiu. 0/1 deep neural networks via block coordinate descent. _arXiv preprint arXiv:2206.09379_, 2022.
* [55] Xiangyu Zhang, Jianhua Zou, Kaiming He, and Jian Sun. Accelerating very deep convolutional networks for classification and detection. _IEEE transactions on pattern analysis and machine intelligence_, 38(10):1943-1955, 2015.
* [56] Hengling Zhao, Yipeng Liu, Xiaolin Huang, and Ce Zhu. Semi-tensor product-based tensor-composition for neural network compression. _arXiv preprint arXiv:2109.15200_, 2021.
* [57] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2881-2890, 2017.

Additional Theoretical Details

### Forward pass Vs. Backward pass

We consider the same notations introduced in Sec. 3.1. Regarding the training of a convolutional layer:

* There is one convolution operation in each forward pass with a computational complexity of \(\Theta_{\text{FLOPS}}(D^{2}CC^{\prime}BH^{\prime}W^{\prime})\)
* In each backward pass, there are two convolution operations, corresponding to formulas (2) and (3), and one weight update operation, with computational complexities of \(\Theta_{\text{FLOPS}}(D^{2}CC^{\prime}BH^{\prime}W^{\prime})\), \(\Theta_{\text{FLOPS}}(D^{2}CC^{\prime}BHW)\) and \(\Theta_{\text{FLOPS}}(D^{2}CC^{\prime})\), respectively.

From this, we can calculate the ratio of computational complexity between forward and backward pass \(R_{\text{FLOPS}}\) as follows:

\[R_{\text{FLOPs}}=\frac{D^{2}CC^{\prime}BH^{\prime}W^{\prime}}{D^{2}CC^{\prime} BH^{\prime}W^{\prime}+D^{2}CC^{\prime}BHW+D^{2}CC^{\prime}}.\] (13)

It is evident that \(R_{\text{FLOPs}}\) is always smaller than one, indicating that the backward pass is always more computationally intensive than the forward pass.

### Backpropagation Derivatives in Linear Layers

With the same notations introduced in Sec. 3.1 but in the case of a linear neural network represented as a sequence of \(n\) linear layers:

\[\mathcal{F}(X)=(\mathcal{F}_{W_{n}}\circ\mathcal{F}_{W_{n-1}}\circ\cdots\circ \mathcal{F}_{W_{2}}\circ\mathcal{F}_{W_{1}})(X),\] (14)

where \(W_{i}\in\mathbb{R}^{O\times I}\) corresponds to parameters matrix of the \(i^{th}\) layer. This layer receives an \(I\)-feature input \(A_{i}\in\mathbb{R}^{B\times I}\) with minibatch size \(B\) to produce an \(O\)-feature output \(A_{i+1}\in\mathbb{R}^{B\times O}\). The derivatives in that setup become:

\[\frac{\partial\mathcal{L}}{\partial W_{i}} =\frac{\partial\mathcal{L}}{\partial A_{i+1}}\cdot\frac{\partial A _{i+1}}{\partial W_{i}}=A_{i}^{T}\cdot\frac{\partial\mathcal{L}}{\partial A_ {i+1}},\] (15) \[\frac{\partial\mathcal{L}}{\partial A_{i}} =\frac{\partial\mathcal{L}}{\partial A_{i+1}}\cdot\frac{\partial A _{i+1}}{\partial A_{i}}=W_{i}^{T}\cdot\frac{\partial\mathcal{L}}{\partial A_ {i+1}}.\] (16)

The same conclusions as for convolutional layers can naturally be derived from these equations for linear layers, namely that activations and weights occupy most of the space in memory when performing backpropagation.

### Details of Backpropagation with Decomposed Activation Tensors

Since the convolution operation involves other variables such as stride, dilation, and groups, we provide a more precise description by rewriting in (2), \(\frac{\partial\mathcal{L}}{\partial\mathcal{W}_{i}}\) as \(\Delta\mathcal{W}\in\mathbb{R}^{N_{out}\times\mathcal{C}\times D\times D}\), \(\mathcal{A}_{i}\) as \(\mathcal{I}\in\mathbb{R}^{B\times C\times H\times W}\) and \(\frac{\partial\mathcal{L}}{\partial A_{i+1}}\) as \(\Delta\mathcal{Y}\in\mathbb{R}^{B\times C^{\prime}\times H^{\prime}\times W^{ \prime}}\) which gives us:\[\Delta\mathcal{W}_{c^{\prime}_{g},c,k,l} =\sum_{b=1}^{B}\sum_{h^{\prime}=1}^{H^{\prime}}\sum_{w^{\prime}=1}^ {W^{\prime}}\mathcal{I}_{b,c_{g},h,w}\Delta\mathcal{Y}_{b,c^{\prime}_{g},h^{ \prime},w^{\prime}},\] (17) where: \[h=h^{\prime}\times\text{Stride}+k\times\text{Dilation},\] \[w=w^{\prime}\times\text{Stride}+l\times\text{Dilation},\] \[N_{out}=\left\lfloor\frac{\text{Groups}}{C^{\prime}}\right\rfloor,\] \[N_{in}=\left\lfloor\frac{\text{Groups}}{C}\right\rfloor,\] \[c^{\prime}_{g}=g\times N_{out}+c^{\prime},\] \[c_{g}=g\times N_{in}+c,\] \[c^{\prime}\in[1,N_{out}],\] \[c\in[1,N_{in}],\] \[g\in[1,\text{Groups}],\] \[k\text{ and }l\in[1,D],\] \[\underline{\mathcal{I}}\text{ is the padded input.}\]

In our case, before being saved in memory, \(\mathcal{I}\) is decomposed and compressed through truncated HOSVD as presented in (8):

\[\tilde{\mathcal{I}}_{b,c_{g},h,w}=\sum_{k_{1}=1}^{K_{1}}\sum_{k_{2}=1}^{K_{2}} \sum_{k_{3}=1}^{K_{3}}\sum_{k_{4}=1}^{K_{4}}\hat{\mathcal{S}}_{k_{1},k_{2},k_{ 3},k_{4}}\times U_{(K_{1})_{b,k_{1}}}^{(1)}\times U_{(K_{2})_{c_{g},k_{2}}}^{( 2)}\times U_{(K_{3})_{h,k_{3}}}^{(3)}\times U_{(K_{4})_{w,k_{4}}}^{(4)}.\] (18)

Therefore, in the backward pass, its padded version can be restored using the following formula:

\[\tilde{\mathcal{I}}_{b,c_{g},h,w}=\sum_{k_{1}=1}^{K_{1}}\sum_{k_{2}=1}^{K_{2}} \sum_{k_{3}=1}^{K_{3}}\sum_{k_{4}=1}^{K_{4}}\hat{\mathcal{S}}_{k_{1},k_{2},k_{ 3},k_{4}}\times U_{(K_{1})_{b,k_{1}}}^{(1)}\times U_{(K_{2})_{c_{g},k_{2}}}^{( 2)}\times\underline{U}_{(K_{3})_{h,k_{3}}}^{(3)}\times\underline{U}_{(K_{4})_ {w,k_{4}}}^{(4)},\] (19)

where, \(\underline{U}_{(K_{3})}^{(3)}\) and \(\underline{U}_{(K_{4})}^{(4)}\) are \(U_{(K_{3})}^{(3)}\) and \(U_{(K_{4})}^{(4)}\) with vertical padding (top and bottom edges only), respectively. Then, we substitute (19) into (17) which gives us, after reordering and grouping:

\[\mathcal{Z}_{k_{1},c^{\prime}_{g},h^{\prime},w^{\prime}}^{(1)} =\sum_{b=1}^{B}\Delta\mathcal{Y}_{b,c^{\prime}_{g},h^{\prime},w^{ \prime}}U_{(K_{1})_{b,k_{1}}}^{(1)},\] (20) \[\mathcal{Z}_{k_{1},k_{2},k,k_{4}}^{(2)} =\sum_{k_{3}=1}^{K_{3}}\hat{\mathcal{S}}_{k_{1},k_{2},k_{3},k_{4} }\underline{U}_{(K_{3})_{h,k_{3}}}^{(3)},\] (21) \[\mathcal{Z}_{k_{1},k_{2},h,w}^{(3)} =\sum_{k_{4}=1}^{K_{4}}Z_{k_{1},k_{2},h,k_{4}}^{(2)}\underline{U} _{(K_{4})_{w,k_{4}}}^{(4)},\] (22) \[\mathcal{Z}_{c^{\prime}_{g},k_{2},k,l}^{(4)} =\sum_{h^{\prime}=1}^{H^{\prime}}\sum_{w^{\prime}=1}^{W^{\prime} }\sum_{k_{1}=1}^{K_{1}}Z_{k_{1},k_{2},h,w}^{(3)}Z_{k_{1},c^{\prime}_{g},h^{ \prime},w^{\prime}}^{(1)},\] (23) \[\Delta\mathcal{W}_{c^{\prime}_{g},c,k,l} =\sum_{k_{2}=1}^{K_{2}}Z_{c^{\prime}_{g},k_{2},k,l}^{(4)}U_{(K_{2} )_{c_{g},k_{2}}}^{(2)}.\] (24)

Computing (20), (21), (22) and (24) corresponds to performing \(1\times 1\) convolutions while (23) is a \(H^{\prime}\times W^{\prime}\) convolution, resulting in (11).

### Details of Overhead, Computational Speedup and Space Complexity

Following the notations introduced in the main paper, we propose in this section to analitically study the overhead introduced in the forward pass when performing HOSVD compression, the speedup of efficient weight derivative computation and the required space to store the compressed activations.

**Overhead.** During the forward pass, the overhead of activation compression is the computational complexity of performing the tensor decomposition, which can be calculated as follows:

The computational complexity of SVD for a matrix of size \(m\times n\) with \(m\geq n\) is \(\Theta_{\text{FLOPS}}(m^{2}n)\). HOSVD essentially performs SVD on each mode of the tensor. During the forward pass, for each convolution layer, given an activation map of size \(B\times C\times H\times W\), HOSVD involves performing SVD on four matrices of sizes \(B\times CHW\), \(C\times BHW\), \(H\times BCW\), and \(W\times BCH\). Therefore, the computational complexity for decomposition in the forward pass is:

\[\text{FLOPs}_{\text{overhead}}=\Theta_{\text{FLOPS}}\Big{(} \max(B,CHW)^{2}\times\min(B,CHW)+\max(C,BHW)^{2}\times\min(C, BHW)+\] (25) \[\max(H,BCW)^{2}\times\min(H,BCW)+\max(W,BCH)^{2}\times\min(W,BCH) \Big{)}.\]

Additionally, the amount of FLOPs that vanilla training requires to perform the forward pass is:

\[\text{FLOPs}_{\text{Vanilla}}=D^{2}CC^{\prime}BHW.\] (26)

From this, the total FLOPs that HOSVD requires is:

\[\text{FLOPs}_{\text{HOSVD}}=\Theta_{\text{FLOPS}}\Big{(} \max(B,CHW)^{2}\times\min(B,CHW)+\max(C,BHW)^{2}\times\min(C, BHW)+\] (27) \[\max(H,BCW)^{2}\times\min(H,BCW)+\max(W,BCH)^{2}\times\min(W,BCH) \Big{)}+D^{2}CC^{\prime}BHW.\]

Based on these calculations, we represent in Fig. 6 the evolution of FLOPs in the forward pass for both vanilla training and HOSVD, showing that our method results in an increased latency in the forward pass.

**Speedup.** The key difference between HOSVD and Vanilla BP lies in computing \(\Delta\mathcal{W}\).

For Vanilla BP, the cost of computing \(\Delta\mathcal{W}\) is \(\Theta_{\text{FLOPS}}(D^{2}CC^{\prime}BH^{\prime}W^{\prime})\).

Whereas our method involves:

\[\text{Computing }Z^{(1)}:\Theta_{\text{FLOPS}}(K_{1}C^{\prime}H^{ \prime}W^{\prime}B),\] \[\text{Computing }Z^{(2)}:\Theta_{\text{FLOPS}}(K_{1}K_{2}HK_{4}K_{3}),\] \[\text{Computing }Z^{(3)}:\Theta_{\text{FLOPS}}(K_{1}K_{2}HK_{4}K_{3}+K_{1 }K_{2}HWK_{4}),\] \[\text{Computing }Z^{(4)}:\Theta_{\text{FLOPS}}(K_{1}C^{\prime}H^{ \prime}W^{\prime}B+K_{1}K_{2}HK_{4}K_{3}+K_{1}K_{2}HWK_{4}+C^{\prime}K_{2}D^{2 }H^{\prime}W^{\prime}K_{1}).\]

Therefore, the complexity for computing \(\Delta\mathcal{W}\) with our efficient reconstruction is:

\[\Theta_{\text{FLOPS}}(K_{1}C^{\prime}H^{\prime}W^{\prime}B+K_{1}K_{2}HK_{4}K_ {3}+K_{1}K_{2}HWK_{4}+C^{\prime}K_{2}D^{2}H^{\prime}W^{\prime}K_{1}+C^{\prime} CD^{2}K_{2}).\] (28)

We thus deduce the speedup ratio \(R_{S}\) for the backward pass, between vanilla training and HOSVD (results represented in Fig. 2b):

\[R_{S}=\frac{D^{2}CC^{\prime}BH^{\prime}W^{\prime}}{K_{1}C^{\prime}H^{\prime} W^{\prime}B+K_{1}K_{2}HK_{4}K_{3}+K_{1}K_{2}HWK_{4}+C^{\prime}K_{2}D^{2}H^{ \prime}W^{\prime}K_{1}+C^{\prime}CD^{2}K_{2}}.\] (29)

**Space Complexity.** For vanilla training, storing \(\mathcal{A}_{i}\) correspond to the storage of \(\Theta_{\text{space}}(BCHW)\) elements.

Figure 6: Predicted evolution of FLOPs for the forward pass of both vanilla training and HOSVD.

For HOSVD, instead of storing the entire tensor, we store its truncated principal components: \(\tilde{\mathcal{S}}\), \(U_{(K_{1})}^{(1)}\), \(U_{(K_{2})}^{(2)}\), \(U_{(K_{3})}^{(3)}\), and \(U_{(K_{4})}^{(4)}\), which corresponds to a total of \(\Theta_{\text{space}}(K_{1}K_{2}K_{3}K_{4}+BK_{1}+CK_{2}+HK_{3}+WK_{4})\) elements.

Thus, we obtain the storage complexity ratio \(R_{C}\) as follows (results represented in Fig. 1(a)):

\[R_{C}=\frac{BCHW}{K_{1}K_{2}K_{3}K_{4}+BK_{1}+CK_{2}+HK_{3}+WK_{4}}.\] (30)

## Appendix B Additional Experimental Details

### Variance of Different Runs

We conducted classification experiments using MCUNet with setup A, fine-tuning on CIFAR-10, and segmentation experiments using the DeepLabV3 checkpoint provided by [53], fine-tuning on the augmented Pascal-VOC12 dataset. Both experiments employed three different random seeds (\(233\), \(234\), and \(235\)). The results, presented in Table 3 and Table 4, indicate no significant variation in performance across the different random seeds. Therefore, for subsequent experiments, we used a single random seed, \(233\).

### Detailed Experimental Setup

**ImageNet Classification.** In this setup, we use a similar finetuning policy to [53]. Specifically, we finetune the checkpoints for \(90\) epochs with L2 gradient clipping with a threshold of \(2.0\). We use SGD with a weight decay of \(1\times 10^{-4}\) and a momentum of \(0.9\). The data is randomly resized, randomly flipped, normalized, and divided into batches of \(64\) elements. We use cross-entropy as the loss function. The difference from their setup lies in the initial learning rate value after the warm-up epochs: in our case, the learning rate increases linearly over \(4\) warm-up epochs up to \(0.005\) (while in [53] it is \(0.05\)). After this, similarly to their approach, the learning rate decays according to the cosine annealing method in the following epochs.

**Other dataset finetuning.** We use the same set of hyperparameters as described in [53] for both setup A and setup B. For training, we use cross-entropy loss with the SGD optimizer. The learning rate starts at \(0.05\) and decays according to the cosine annealing method. Momentum is set to \(0\), and weight decay is set to \(1\times 10^{-4}\). We apply L2 gradient clipping with a threshold of \(2.0\).

For setup B, batch normalization layers are fused with convolutional layers before training, a common technique for accelerating inference. We set the batch size to \(128\) and normalized the data before feeding it to the model.

**Semantic Segmentation.** Following the policy outlined in [53], we utilized the pretrained and calibrated checkpoints provided by the authors for our experiments. The models were pretrained on Cityscapes using MMSegmentation, then we finetuned them on the augmented Pascal-VOC12 dataset. The optimizer's learning rate starts at \(0.01\) and decays according to the cosine annealing schedule during training. Additionally, we set the weight decay to \(5\times 10^{-4}\) and momentum to \(0.9\). The batch size is set to \(8\). For data augmentation, we randomly crop, flip, apply photometric distortions, and normalize the images. Cross-entropy is used as the loss function. Models are finetuned for \(20,000\) steps.

\begin{table}
\begin{tabular}{c|c c c} \hline \hline
**Method** & **\#Layers** & **mIoU** & **mAcc** \\ \hline HOSVD (\(\varepsilon=0.8\)) & 5 & 38.62\(\pm\)0.01 & 50.41\(\pm\)0.06 \\  & 10 & 50.33\(\pm\)0.05 & 63.39\(\pm\)0.05 \\ \hline SVD (\(\varepsilon=0.8\)) & 5 & 39.71\(\pm\)0.00 & 51.39\(\pm\)0.07 \\  & 10 & 52.47\(\pm\)0.04 & 66.09\(\pm\)0.01 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Segmentation with DeepLabV3 on different random seeds.

\begin{table}
\begin{tabular}{c|c c c c c c} \hline \hline
**\#Layers** & **Vanilla training** & **Gradient Filter R2** & **Gradient Filter R4** & **Gradient Filter R7** & **SVD (\(\varepsilon=0.8\))** & **HOSVD (\(\varepsilon=0.8\))** \\ \hline
2 & 71.82\(\pm\)0.36 & 70.75\(\pm\)0.19 & 87.68\(\pm\)0.01 & 70.42\(\pm\)0.18 & 66.84\(\pm\)0.18 & 65.46\(\pm\)0.09 \\
4 & 83.15\(\pm\)0.19 & 82.19\(\pm\)0.11 & 89.54\(\pm\)0.03 & 80.98\(\pm\)0.06 & 80.79\(\pm\)0.04 & 77.22\(\pm\)0.03 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Classification with MCUNet on different random seeds.

### Additional Explained Variance Evolution Results

As anticipated in Sec. 4.2, Fig. 7 presents the explained variance as a function of \(K_{j}\) at the \(j^{th}\) dimension of the four fine-tuned layers of MCUNet. We observe that in the first two dimensions, which are also the largest, more than \(80\%\) of the tensor's energy is concentrated within the first \(30\%\) of the principal components.

With the same experiment, Fig. 8 shows that \(K_{j}\) tends to gradually increase after each training epoch, then peaks and begins to decline. Intuitively, at the beginning, the activation map energy is mainly concentrated in the few first components. As training progresses, this energy starts to spread out to other components, leading to an increase in the value of \(K_{j}\). Over time, the model learns how to efficiently condense the energy, leading to a gradual decrease in \(K_{j}\). This instability in \(K_{j}\) throughout the training process is why we had to log memory as described in Section 4.1.

Figure 7: \(K_{j}\) and variance for each of the last four layers of an MCUNet when fine-tuning them using HOSVD on CIFAR-10 following setup A.

### Processing Time Results

This section presents latency comparisons between vanilla training and HOSVD. We conducted the experiments using MCUNet on a single batch of CIFAR-10 dataset with setup A and for one epoch only.

Fig. 9 compares the execution time of the forward, backward, and total training processes between vanilla training and HOSVD. It can be observed that the backward processing time of HOSVD is tens of times lower than vanilla training, but the forward time is up to a thousand times higher. As a result, when considering the total training time (the sum of forward and backward), HOSVD is on average \(4.29\) times slower than vanilla training. This outcome is entirely expected, as the decomposition process introduces an overhead, as predicted and described in Fig. 6 and Appendix A.4.

Figure 8: Behavior of \(K_{j}\) during training for each of the last four layers of an MCUNet when fine-tuning them using HOSVD on CIFAR-10 dataset following setup A with two different values of \(\varepsilon\).

### Additional Classification Results

In addition to the main experiments, we also performed many classification experiments with setup A on various datasets, the results are shown in Table 5. Especially, when fine-tuning SwinT on CUB200, Pets, and CIFAR-100, applying HOSVD to the last four layers (maintaining the rest of the architecture frozen) consumes less memory than fully fine-tuning the last two layers. This happens because the variance of the activation maps is concentrated in few components: for the same \(\varepsilon\), \(K\) will be smaller.

## Appendix C Limitation

While HOSVD is effective in compressing the forward signal in DNNs for gradient estimation during backpropagation, it is just one among possible choices when dealing with tensor decomposition. Indeed, more variants of tensor decomposition methods such as GKPD or STP could have been used and evaluated and could have led to larger memory and computational gains. Our work opens the door to further explorations in such direction.

Figure 9: **(a), (b), and (c) represent the time in seconds taken by the algorithm to respectively perform Forward, Backward, and the total training process, when fine-tuning an MCUNet on one batch of CIFAR-10 with Setup A across \(1\) to \(10\) layers.**

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We develop the theoretical aspects of tensor decomposition applied to activations in Sec. 3 (construction, practical implementation, complexity, and error bounding). In Sec. 4 we provide experimental results on a wide variety of popular tasks, architectures, and datasets, while also exploring the combined effect of various hyper-parameters.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Limitations of the work are discussed in the dedicated section of the paper(Sec. C).
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: In Sec. 3.1 we develop the formulas necessary to perform backpropagation in a convolutional network using the chain rule. In Sec. 3.2, we provide explicit definition of the tools used in this papers. The complete proof for Sec. 3.3 is provided in the supplemental materials(Sec. A.3). In Sec. 3.4 we evaluate the resulting memory required, the computational complexity of our method, and the error introduced in the gradient computation.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Algorithmic details are developped throughout 3. Code to reproduce results is included in the supplemental materials, alongside instructions on how to use it. The experimental setup is described briefly in Sec. 4 and more exhaustive explicit details are provided in the supplemental materials B.2.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: In footnote 1 on the first page of the paper, the exact code used to collect the experimental results is provided in an open GitHub repository, along with instructions on how to run the code.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyper-parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Such details are discussed in Sec. 4 and developed further in the corresponding section of the supplementary material (Sec. B.2).
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?Answer: [Yes] Justification: Given the computational cost of the experiments, we have performed all our experiments (even reproducing those from the literature) under the same conditions, but we did not provide confidence intervals for all of them. In the supplementary material we show some results providing confidence intervals, and observing a very small variance between the results obtained from different seeds.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We detail the material used in Sec. 4 and the time of execution in the supplementary materials.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Research conducted in this paper is conform with NeurIPS Code of Ethics.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Our method is meant to enable on-device learning and we discuss in the introduction about possible impacts of such a solution in our lives (increased privacy and security, reduced energy consumption and network overload,...) (Sec. 1). Nevertheless, a specific section is not devoted given that this work is very general and not specific to some application.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The proposed approach is a generalist compression algorithm tailored for on-device learning, thus, it does not fall in that category.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All the existing datasets, models, and code used are presented in Sec. 4, and credit is given to the respective creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: All the details about the new assets introduced are provided in the paper. At submission time the source code is not provided, but we commit to release it (alongside a detailed documentation) upon acceptance of the paper.
14. **Crowdsourcing and Research with Human Subjects**Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA]
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA]