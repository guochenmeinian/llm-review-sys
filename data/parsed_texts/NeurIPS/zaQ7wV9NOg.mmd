# Optimistic Natural Policy Gradient: a Simple Efficient Policy Optimization Framework for Online RL

 Qinghua Liu

Princeton University

qinghual@princeton.edu &Gellert Weisz

Google DeepMind

gellert@deepmind.com &Andras Gyorgy

Google DeepMind

agyorgy@deepmind.com &Chi Jin

Princeton University

chij@princeton.edu &Csaba Szepesvari

Google DeepMind and University of Alberta

szepesva@ualberta.ca

This work was done when QL interned at DeepMind.

###### Abstract

While policy optimization algorithms have played an important role in recent empirical success of Reinforcement Learning (RL), the existing theoretical understanding of policy optimization remains rather limited--they are either restricted to tabular MDPs or suffer from highly suboptimal sample complexity, especially in online RL where exploration is necessary. This paper proposes a simple efficient policy optimization framework--Optimistic NPG for online RL. Optimistic NPG can be viewed as a simple combination of the classic natural policy gradient (NPG) algorithm [14] and an optimistic policy evaluation subroutine to encourage exploration. For \(d\)-dimensional linear MDPs, Optimistic NPG is computationally efficient, and learns an \(\epsilon\)-optimal policy within \(\tilde{\mathcal{O}}(d^{2}/\epsilon^{3})\) samples, which is the first computationally efficient algorithm whose sample complexity has the optimal dimension dependence \(\tilde{\Theta}(d^{2})\). It also improves over state-of-the-art results of policy optimization algorithms [17] by a factor of \(d\). For general function approximation that subsumes linear MDPs, Optimistic NPG, to our best knowledge, is also the first policy optimization algorithm that achieves the polynomial sample complexity for learning near-optimal policies.

## 1 Introduction

Policy optimization algorithms [16, 15] with neural network function approximation have played an important role in recent empirical success of reinforcement learning (RL), such as robotics [12], games [1] and large language models [2]. Motivated by the empirical success, the theory community made a large effort to design provably efficient policy optimization algorithms that work in the presence of linear function approximation [1, 1, 13, 14, 15, 16, 17, 18, 19, 20, 21]. Early works focused on proving that policy optimization algorithms are capable to learn near-optimal policies using a polynomial number of samples under certain reachability (coverage) assumptions. [e.g., 1, 17, 16, 18, 19, 19]. While this was good for laying down the foundations for future work, the reachability assumptions basically imply that the state space is already well-explored or rather easy to explore, which avoids the challenge of performing strategic exploration -- one of the central problems in both empirical and theoretical RL.

To address this limitation, later works (Agarwal et al., 2020; Zanette et al., 2021) proposed policy optimization algorithms that enjoy polynomial sample-complexity guarantee without making any reachability assumption, but at the cost of either complicating the algorithm design (and analysis) with various tricks or getting highly suboptimal sample-complexity guarantees. For example, the PC-PG algorithm (Agarwal et al., 2020), which, to our knowledge was the first policy optimization algorithm for learning linear MDPs without reachability assumptions, requires \(\tilde{\mathcal{O}}(\mathrm{poly}(d)/\epsilon^{11})\) samples to learn an \(\epsilon\)-optimal policy for \(d\)-dimensional linear MDPs. That \(\tilde{\mathcal{O}}(1/\epsilon^{11})\) samples were necessary for this task is highly unlikely. Indeed, Zanette et al. (2021) greatly improved this sample complexity to \(\tilde{\mathcal{O}}(d^{3}/\epsilon^{3})\) at the cost of considerably complicating the algorithm design and the analysis. Moreover, we are not aware of efficient guarantees of policy optimization for generic function approximation, which is rich enough to subsume linear MDPs. This motivates us to ask the following question:

Can we design a **simple, general** policy optimization algorithm

with **sharp** sample-complexity guarantees in the **exploration** setting2?

Footnote 2: By “exploration setting”, we mean a setup where there is no simulator nor reachability assumption and a learner can only influence the evolution of environmental states by taking actions.

In particular, we aim to achieve sharper sample complexity than Zanette et al. (2021) in the linear MDP setting, and achieve low-degree polynomial sample complexity in the general function approximation setting. This paper answers the highlighted question affirmatively by making the following contributions:

* **Sharper rate.** We propose a computationally efficient policy optimization algorithm--Optimistic NPG with sample complexity \[\tilde{\mathcal{O}}(d^{2}/\epsilon^{3})\] for learning an \(\epsilon\)-optimal policy in an online fashion while interacting with a \(d\)-dimensional linear MDP. This result improves over the best previous one (Zanette et al., 2021) in policy optimization by a factor of \(d\). Moreover, to our knowledge, this is the first computationally efficient algorithm to achieve the optimal quadratic dimension dependence. Before moving on, we remark that previous FQI-style algorithms (Zanette et al., 2020; Jin et al., 2021) achieve the optimal sample complexity \(\tilde{\Theta}(d^{2}/\epsilon^{2})\)(Zanette et al., 2020) but they are computationally _inefficient_ due to the mechanism of global optimism in their algorithm design. Several very recent works (He et al., 2022; Agarwal et al., 2022; Wagenmaker et al., 2022) achieve the sample complexity \(\tilde{\mathcal{O}}(d^{2}/\epsilon^{2}+d^{\geq 4}/\epsilon)\) using computationally efficient algorithms. Compared to our work, the aforementioned rate has worse dependence on \(d\) and better dependence on \(\epsilon\). Nonetheless, our result is better in the practically important regime where the feature dimension \(d\) is typically very large and the target accuracy \(\epsilon\) is not too small. To summarize, the sample complexity of Optimistic NPG strictly improves over the best existing policy optimization algorithms and is not dominated by that of any existing computationally efficient algorithm.

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline
**Algorithms** & **Sample Complexity** & **Computationally Efficient** & **Policy Optimization** \\ \hline \hline Zanette et al. (2020) & \(\tilde{\Theta}(d^{2}/\epsilon^{2})\) & \(\times\) & \(\times\) \\ Jin et al. (2021) & \(\tilde{\mathcal{O}}(d^{2}/\epsilon^{2}+d^{\geq 4}/\epsilon)\) & ✓ & \(\times\) \\ \hline Agarwal et al. (2020) & \(\tilde{\mathcal{O}}(\mathrm{poly}(d)/\epsilon^{11})\) & ✓ & ✓ \\ \hline Zanette et al. (2021) & \(\tilde{\mathcal{O}}(d^{3}/\epsilon^{3})\) & ✓ & ✓ \\ \hline
**Optimistic NPG (this work)** & \(\tilde{\mathcal{O}}(d^{2}/\epsilon^{3})\) & ✓ & ✓ \\ \hline \end{tabular}
\end{table}
Table 1: A comparison of sample-complexity results for linear MDPs.

To our best knowledge, this paper also achieves the first polynomial sample-complexity guarantee for policy optimization under general function approximation.
* **Simple on-policy algorithm.** At a high level, Optimistic NPG is almost an _on-policy_ version of natural policy gradient (NPG) (Kakade, 2001) with Boltzmann policies that use linear function approximation for optimistic action-value estimates. By "on-policy", we mean the transition-reward data used to improve the current policy are obtained exactly by executing the current policy. However, the algorithm has a tuneable parameter (denoted by \(m\) in Algorithm 1) that allows it to reuse data sampled from earlier policies. By using such data, the algorithm becomes _off-policy_. The optimal choice of the tuneable parameter is dictated by theoretical arguments. Interestingly, our analysis shows that even the purely on-policy version of the algorithm (i.e., set \(m=1\) in Algorithm 1) enjoys a well-controlled sample complexity of \(\tilde{\mathcal{O}}(d^{2}/\epsilon^{4})\). To the best of our knowledge, this is the first time an on-policy method is shown to have polynomial sample complexity in the exploration setting, given that all previous policy optimization or Q-learning style or model-based algorithms (e.g., Jin et al., 2020; Zanette et al., 2020, 2021; Agarwal et al., 2020; Cai et al., 2020; Shani et al., 2020, etc) are off-policy.
* **New proof techniques.** To achieve the improved rate, we introduce several new ideas in the proof, including but not limited to (a) exploiting the softmax parameterization of the policies to reuse data and improve sample efficiency (Lemma 3), (b) controlling _on-policy_ uncertainty (Lemma 4) instead of cumulative uncertainty as in previous off-policy works, and (c) using a bonus term that is smaller than those in previous works by a factor of \(\sqrt{d}\) (see Lemma 2 and the discussions that follow).

### Related works

Since this paper studies policy optimization algorithms in the setting of linear MDPs, below we restrict our focus to previous theoretical works on either policy optimization or linear MDPs.

Policy optimization.This work is inspired by and builds upon two recent works (Shani et al., 2020; Cai et al., 2020) that combine NPG with bonus-based optimism to handle exploration. In terms of algorithmic design, this paper (Optimistic NPG) utilizes on-policy fresh data for value estimation while Shani et al. (2020); Cai et al. (2020) are off-policy and reuse _all_ the historical data. In terms of theoretical guarantees, Shani et al. (2020) and Cai et al. (2020) only study tabular MDPs and linear mixture MDPs (Zhou et al., 2021) respectively, while this paper considers the more challenging setting of linear MDPs (Jin et al., 2020) (in our view, linear MDPs are more challenging as there the number of model parameters scale with the number of states). We remark that due to some subtle technical challenge, so far it still remains unknown whether it is possible to generalize the analysis of Cai et al. (2020) to handle linear MDPs. Agarwal et al. (2020); Zanette et al. (2021) derive the first line of policy optimization results for RL in linear MDPs without any reachability- (or coverage-) style assumption. Compared to Agarwal et al. (2020); Zanette et al. (2021), our work considers the same setting but designs a simpler algorithm with cleaner analysis and sharper sample-complexity guarantee. Nevertheless, we remark that in certain model-misspecification settings, the algorithms of Agarwal et al. (2020); Zanette et al. (2021) can potentially achieve stronger guarantees. Since we focus on the well-specified setting (that is, the environment is perfectly modeled by a linear MDP), we refer interested readers to Agarwal et al. (2020) for more details. Finally, there have been a long line of works (e.g., Agarwal et al., 2021; Bhandari and Russo, 2019; Liu et al., 2019; Neu et al., 2017; Abbasi-Yadkori et al., 2019, etc) that study policy optimization under reachability (or coverage) assumptions, which eliminates the need for performing strategic exploration. Throughout this paper we do not make any such assumption and directly tackle the exploration challenge.

Linear MDPs.For linear MDPs, Jin et al. (2020) proposed a computationally efficient algorithm (LSVI-UCB) with \(\tilde{\mathcal{O}}(d^{3}/\epsilon^{2})\) sample complexity. Later on, Zanette et al. (2020) utilized the idea of global optimism to obtain the optimal sample complexity \(\tilde{\Theta}(d^{2}/\epsilon^{2})\) at the cost of sacrificing computational efficiency. Recently, He et al. (2022); Agarwal et al. (2022); Wagenmaker et al. (2022) designed new computationally efficient algorithms that can achieve \(\tilde{\mathcal{O}}(d^{2}/\epsilon^{2}+d^{\geq 4}/\epsilon)\) sample complexity. Compared to the above works, the sample complexity of Optimistic NPG is not strictly worse than that of any known computationally efficient algorithm. In fact, it is the only computationally efficient method to achieve the optimal dimension dependence. Nonetheless, for learning a near-optimal policy with a vanishing suboptimality \(\epsilon\), the current sample complexity of Optimistic NPG is loose by a factor of \(\epsilon^{-1}\).

## 2 Preliminaries

MDP.We consider the model of episodic Markov Decision Processes (MDPs). Formally, an MDP is defined by a tuple \((\mathcal{S},\mathcal{A},\mathbb{P},R,H)\) where \(\mathcal{S}\) denotes the set of states, \(\mathcal{A}\) denotes the set of actions, both of which are assumed to be finite, \(\mathbb{P}=\{\mathbb{P}_{h}\}_{h\in[H]}\) denotes the set of transition probability functions so that \(\mathbb{P}_{h}(s^{\prime}\mid s,a)\) is equal to the probability of transitioning to state \(s^{\prime}\) given that action \(a\) is taken at state \(s\) and step \(h\), \(R=\{R_{h}\}_{h\in[H]}\) denotes the collection of expected reward functions so that \(R_{h}(s,a)\) is equal to the expected reward to be received if action \(a\) is taken at state \(s\) and step \(h\), and \(H\) denotes the length of each episode. An agent interacts an MDP in the form of episodes. Formally, we assume without loss of generality that each episode always starts from a _fixed_ initial state \(s_{1}\). At the \(h^{\mathrm{th}}\) step of this episode, the agent first observes the current state \(s_{h}\), then takes action \(a_{h}\) and receives reward \(r_{h}(s_{h},a_{h})\) satisfying

\[\mathbb{E}[r_{h}(s_{h},a_{h})\mid s_{h},a_{h}]=R_{h}(s_{h},a_{h})\,.\]

After that, the environment transitions to

\[s_{h+1}\sim\mathbb{P}_{h}(\cdot\mid s_{h},a_{h})\,.\]

The current episode terminates immediately once \(r_{H}\) is received. Throughout this paper, we assume \(\mathbb{P}\) and \(R\) are unknown to the learner.

Linear MDP.A \(d\)-dimensional linear MDP (Jin et al., 2020) is defined by two sets of feature mappings, \(\{\phi_{h}\}_{h\in[H]}\subseteq(\mathcal{S}\times\mathcal{A}\to\mathbb{R}^{d})\) and \(\{\psi_{h}\}_{h\in[H]}\subseteq(\mathcal{S}\to\mathbb{R}^{d})\), and a set of vectors \(\{w^{\star}_{h}\}_{h\in[H]}\subseteq\mathbb{R}^{d}\) so that the transition probability functions can be represented as bilinear functions of feature mappings \(\{\phi_{h}\}_{h\in[H]}\) and \(\{\psi_{h}\}_{h\in[H]}\), and the reward functions can be represented as linear functions of \(\{\phi_{h}\}_{h\in[H]}\). Formally, we have that for all \((s,a,s^{\prime})\in\mathcal{S}\times\mathcal{A}\times\mathcal{S}\):

\[\mathbb{P}_{h}(s^{\prime}\mid s,a) =\langle\phi_{h}(s,a),\psi_{h}(s^{\prime})\rangle,\] \[R_{h}(s,a) =\langle\phi_{h}(s,a),w^{\star}_{h}\rangle.\]

For the purpose the regularity, linear MDPs also require that

\[\max_{h,s,a}\|\phi_{h}(s,a)\|_{2}\leq 1,\quad\max_{h}\|w^{\star}_{h}\|_{2}\leq \sqrt{d},\]

and for any function \(V_{h+1}:\mathcal{S}\to[0,1]\),

\[\left\|\int_{s\in\mathcal{S}}V_{h+1}(s)\psi_{h+1}(s)ds\right\|_{2}\leq\sqrt{d}.\]

Throughout this paper, we assume only \(\{\phi_{h}\}_{h\in[H]}\) is available to the learner while \(\{\psi_{h}\}_{h\in[H]}\) and \(\{w^{\star}_{h}\}_{h\in[H]}\) are not.

Policy and value.A (Markov) policy is a set of conditional probability functions \(\pi=\{\pi_{h}\}_{h\in[H]}\) so that \(\pi_{h}(\cdot\mid s)\in\Delta_{\mathcal{A}}\) gives the distribution over the action set conditioned on the current state \(s\) at step \(h\). We define the V-value functions of policy \(\pi\) by \(\{V^{\pi}_{h}\}_{h\in[H]}\subseteq(\mathcal{S}\to\mathbb{R})\) so that \(V^{\pi}_{h}(s)\) is equal to the expected cumulative reward an agent will receive if she follows policy \(\pi\) starting from state \(s\) and step \(h\). Formally,

\[V^{\pi}_{h}(s)=\mathbb{E}\left[\sum_{h^{\prime}=h}^{H}r_{h^{\prime}}(s_{h^{ \prime}},a_{h^{\prime}})\mid s_{h}=s,a_{h^{\prime}}\sim\pi_{h^{\prime}}(s_{h^{ \prime}}),s^{\prime}_{h+1}\sim\mathbb{P}_{h}(\cdot\mid s^{\prime}_{h},a^{ \prime}_{h})\right],\]

where the expectation is with respect to the randomness of the transition, the reward and the policy. Similarly, we can define the Q-value functions of policy \(\pi\) by \(\{Q^{\pi}_{h}\}_{h\in[H]}\subseteq(\mathcal{S}\times\mathcal{A}\to\mathbb{R})\) so that \(Q^{\pi}_{h}(s,a)\) is equal to the expected cumulative reward an agent will receive if she follows policy \(\pi\) starting from taking action \(a\) at state \(s\) and step \(h\). Formally,

\[Q^{\pi}_{h}(s,a)=\mathbb{E}\Bigg{[}\sum_{h^{\prime}=h}^{H}r_{h^{\prime}}(s_{h^{ \prime}},a_{h^{\prime}})\mid(s_{h},a_{h})=(s,a),a_{h^{\prime}}\sim\pi_{h^{ \prime}}(s_{h^{\prime}}),s^{\prime}_{h+1}\sim\mathbb{P}_{h}(\cdot\mid s^{ \prime}_{h},a^{\prime}_{h})\Bigg{]}.\]We denote by \(\pi^{\star}=\{\pi_{h}^{\star}\}_{h\in[H]}\) the optimal policy such that \(\pi^{\star}\in\operatorname*{argmax}_{\pi}V_{h}^{\pi}(s)\) for all \((s,h)\in\mathcal{S}\times[H]\). By backward induction, one can easily prove that there always exists an optimal Markov policy. For simplicity of notations, we denote by \(V_{h}^{\star}=V_{h}^{\pi^{\star}}\) and \(Q_{h}^{\star}=Q_{h}^{\pi^{\star}}\) the optimal value functions. Note that given an MDP, the optimal value functions are unique despite that there may exist multiple optimal policies.

Learning objective.The objective of this paper is to design an efficient policy optimization algorithm to learn an \(\epsilon\)-optimal policy \(\pi\) such that \(V_{1}^{\pi}(s_{1})\geq V_{1}^{\star}(s_{1})-\epsilon\). Here the optimality is only measured by the value at the initial state \(s_{1}\), because (a) each episode always starts from \(s_{1}\), and (b) this paper studies the online exploration setting without access to a simulator, which means some states might be unreachable and learning optimal policies starting from those states is in general impossible.

## 3 Optimistic Natural Policy Gradient

In this section, we present the algorithm Optimistic NPG (Optimistic Natural Policy Gradient) and its theoretical guarantees.

### Algorithm

The pseudocode of Optimistic NPG is provided in Algorithm 1. At a high level, the algorithm consists of the following three key modules.

* **Periodic on-policy data collection** (Lines 4-6): Similarly to the empirical PPO algorithm (Schulman et al., 2017), Optimistic NPG discards all the old data, and executes the _current_ policy \(\pi^{k}\) to collect a batch of fresh data \(\mathcal{D}^{k}\) after every \(m\) steps of policy update. These data will later be used to evaluate and improve the policies in the next \(m\) steps. Noticeably, this _on-policy_ data mechanism is very different from most existing works in theoretical RL, where they either need to keep the historical data or have to rerun historical policies to refresh the dataset for the technical purpose of elliptical potential arguments in proofs. In comparison, Optimistic NPG only uses fresh data collected by the current (or very recent) policy, which resembles practical policy optimization algorithms such as PPO (Schulman et al., 2017) and TRPO (Schulman et al., 2015).
* **Optimistic policy evaluation** (Line 8): Given the above collected dataset \(\mathcal{D}^{k}\), we estimate the value functions of the current policy \(\pi^{k}\) by invoking Subroutine OPE (optimistic policy evaluation). In Section 4, we show how to implement Subroutine OPE for tabular MDPs, linear MDPs and RL problems of low eluder dimension.
* **Policy update** (Line 9): Given the optimistic value estimates \(\{\overline{Q}_{h}^{k}\}_{h\in[H]}\) of \(\pi^{k}\), Optimistic NPG performs one-step mirror ascent from \(\pi_{h}^{k}(\cdot\mid s)\) with gradient \(\overline{Q}_{h}^{k}(s,\cdot)\) to obtain \(\pi_{h}^{k+1}(\cdot\mid s)\) at each \((h,s)\). Importantly, this step can be implemented in a computationally efficient way, because by the update rule of mirror ascent \(\pi_{h}^{k}(\cdot\mid s)\propto\exp(\eta\sum_{t=1}^{k-1}\overline{Q}_{h}^{t}(s,\cdot))\), we only need to store \(\{\{\overline{Q}_{h}^{t}\}_{h\in[H]}\}_{t\in[K]}\), from which any \(\pi_{h}^{k}(\cdot\mid s)\) can be computed on the fly.

### Theoretical guarantee

Optimistic NPG is a generic policy optimization meta-algorithm, and we can show it provably learns near-optimal policies as long as subroutine OPE satisfies the following condition.

**Condition 1** (Requirements for OPE).: _Suppose parameter \(m\) and \(\eta\) in Optimistic NPG satisfy \(m\leq(\eta H^{2})^{-1}\). Then with probability at least \(1-\delta\), \(\textsc{OPE}(\pi^{k},\mathcal{D}^{k})\) returns \(\{\overline{Q}_{h}^{k}\}_{h\in[H]}\subseteq(\mathcal{S}\times\mathcal{A}\to[0, H])\) that satisfy the following properties for all \(k\in[K]\)._

1. **Optimism:** _For all \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times[H]\)_ \[\overline{Q}_{h}^{k}(s,a)\geq(\mathcal{T}_{h}^{\pi^{k}}\overline{Q}_{h+1}^{k})( s,a)\] \[\text{where }(\mathcal{T}_{h}^{\pi}f)(s,a)=R_{h}(s,a)+\mathbb{E}\left[f (s^{\prime},a^{\prime})\mid s^{\prime}\sim\mathbb{P}_{h}(\cdot\mid s,a),a^{ \prime}\sim\pi_{h+1}(s^{\prime})\right].\]1. **Consistency:** _There exists an absolute complexity measure_ \(L\in\mathbb{R}^{+}\) _such that_ \[\overline{V}_{1}^{k}(s_{1})-V_{1}^{\pi^{k}}(s_{1})\leq\sqrt{(L/N) \times\log^{2}(NKL/\delta)},\] _where_ \(\overline{V}_{1}^{k}(s_{1})=\mathbb{E}\left[\overline{Q}_{1}^{k}(s_{1},a_{1}) \mid a_{1}\sim\pi^{k}(s_{1})\right].\)__

Condition (1A) requires that the Q-value estimate returned by OPE satisfies the Bellman equation under policy \(\pi^{k}\) optimistically. Requiring optimism is very common in analyzing RL algorithms, as most algorithms rely on the principle of optimism in face of uncertainty to handle exploration. Condition (1B) requires that the degree of over-optimism at initial state \(s_{1}\) is roughly of order \(\sqrt{1/N}\) with respect to the number of samples \(N\). This is intuitive since as more data are collected from policy \(\pi^{k}\) or some policy similar to \(\pi^{k}\) (as is enforced by the precondition \(m\leq(\eta H^{2})^{-1}\)), the value estimate at the initial state should be more accurate. In Section 4, we will provide concrete instantiations of Subroutine OPE for tabular MDPs, linear MDPs and RL problems of low eluder dimension, all of which satisfy Condition 1 with mild complexity measure \(L\).

Under Condition 1, we have the following theoretical guarantees for Optimistic NPG, which we prove in Appendix A.

**Theorem 1**.: _Suppose Condition 1 holds. In Algorithm 1, if we choose_

\[K=\Theta\left(\frac{H^{4}\log|\mathcal{A}|}{\epsilon^{2}}\right),\quad N= \Theta\left(\frac{L\log^{2}(LK/\delta)}{\epsilon^{2}}\right),\quad\eta=\Theta \left(\frac{\epsilon}{H^{3}}\right),\qquad m\leq\Theta\left(\frac{H}{\epsilon }\right),\]

_then with probability at least \(1/2\), \(\pi^{\mathrm{out}}\) is \(\mathcal{O}(\epsilon)\)-optimal._

Below we emphasize two special choices of \(m\) (the period of sampling fresh data) in Theorem 1:

* When choosing \(m=1\), Optimistic NPG is purely _on-policy_ as it only uses data sampled from the current policy to perform policy optimization. In this case, the total sample complexity is \[\frac{\text{\# iteration}}{\text{period of sampling}}\times\text{ batch size}=\frac{K}{m}\times N=\tilde{\Theta}\left(\frac{LH^{4}}{\epsilon^{4}}\right),\] which, to our knowledge, is the first polynomial sample complexity guarantee for purely on-policy algorithms.
* When choosing \(m=[H/\epsilon]\), we obtain sample complexity \[\frac{\text{\# iteration}}{\text{period of sampling}}\times\text{ batch size}=\frac{K}{m}\times N=\tilde{\Theta}\left(\frac{LH^{3}}{\epsilon^{3}}\right),\] which improves over the above purely on-policy version by a factor of \(H/\epsilon\). In Section 4.2, we will specialize this result to linear MDPs to derive a sample complexity that improves the best existing one in policy optimization (Zanette et al., 2021) by a factor of \(d\).

Finally we remark that for cleaner presentation, we state Theorem 1 for a fixed, constant failure probability. However, the result is easy to extend to the case when the failure probability is some arbitrary value of \(\delta\in(0,1)\) at the expense of increasing the sample complexity by a factor of \(\log(1/\delta)\): one simply need to run Algorithm 1 for \(\log(1/\delta)\) times, estimate the values of every output policy to accuracy \(\epsilon\), and then pick the one with the highest estimate.

## 4 Examples

In this section, we implement subroutine OPE for tabular MDPs, linear MDPs and RL problems of low eluder dimension, and derive the respective sample complexity guarantees of Optimistic NPG.

### Tabular MDPs

The implementation of tabular OPE (Subroutine 1) is rather standard: first estimate the transition and reward from dataset \(\mathcal{D}\), then plug the estimated transition-reward into the Bellman equation under policy \(\pi\) to compute the Q-value estimate backwards. And to guarantee optimism, we additionally add standard counter-based UCB bonus to compensate the error in model estimation. Formally, we have the following guarantee for tabular OPE.

**Proposition 1** (tabular MDPs).: _Suppose we choose \(\alpha=\Theta\left(H\sqrt{\log(KH|\mathcal{S}||\mathcal{A}|)}\right)\) in Subroutine 1, then Condition 1 holds with \(L=SAH^{3}\)._

By combining Proposition 1 with Theorem 1, we prove that Optimistic NPG with Subroutine 1 learns an \(\epsilon\)-optimal policy within \((KN/m)=\tilde{\mathcal{O}}(H^{6}|\mathcal{S}||\mathcal{A}|/\epsilon^{3})\) episodes for any tabular MDP. This rate is strictly worse than the best existing result \(\tilde{\mathcal{O}}(H^{3}|\mathcal{S}||\mathcal{A}|/\epsilon^{2})\) in tabular policy optimization [20]. Nonetheless, the proof techniques in [20] are specially tailored to tabular MDPs and it is highly unclear how to generalize them to linear MDPs due to certain technical difficulty that arises from covering a prohibitively large policy space. In comparison, our policy optimization framework easily extends to linear MDPs and provides improved sample complexity over the best existing one, as is shown in the following section.

### Linear MDPs

We provide the instantiation of OPE for linear MDPs in Subroutine 2. At a high level, linear OPE computes an upper bound \(\overline{Q}\) for \(Q^{\pi}\) by using the Bellman equation under policy \(\pi\) backwards from step \(H\) to step \(1\) while adding a bonus to compensate the uncertainty of parameter estimation. Specifically, the step of ridge regression (Line 5) utilizes the linear completeness property of linear MDPs in computing \(\overline{Q}_{h}\) from \(\overline{V}_{h+1}\), which states that for any function \(\overline{V}_{h+1}:\mathcal{S}\rightarrow\mathbb{R}\), there exists \(\overline{\theta}_{h}\in\mathbb{R}^{d}\) so that \(\langle\phi_{h}(s,a),\overline{\theta}_{h}\rangle=R_{h}(s,a)+\mathbb{E}_{s^{ \prime}\sim P_{h}(\cdot|s,a)}[\overline{V}_{h+1}(s^{\prime})]\) for all \((s,a)\in\mathcal{S}\times\mathcal{A}\). And in Line 6, we add an elliptical bonus \(b_{h}(s,a)\) to compensate the error between our estimate \(\hat{\theta}_{h}\) and the groundtruth \(\overline{\theta}_{h}\) so that we can guarantee \(\overline{Q}_{h}(s,a)\geq(\mathcal{T}_{h}^{\ast}\overline{Q}_{h+1})(s,a)\) for all \((s,a,h)\) with high probability.

\[\hat{\theta}_{h}=\operatorname*{argmin}_{\theta}\sum_{(s_{h},a_{h},r_{h},s_{ h+1})\in\mathcal{D}_{h}}\left(\phi_{h}(s_{h},a_{h})^{\mathrm{T}}\theta-r_{h}- \overline{V}_{h+1}(s_{h+1})\right)^{2}+\lambda\|\theta\|_{2}^{2}.\] (1)

**Proposition 2** (linear MDPs).: _Suppose we choose \(\lambda=1\) and \(\alpha=\Theta\left(H\sqrt{d\log(KN)}\right)\) in Subroutine 2, then Condition 1 holds with \(L=d^{2}H^{3}\)._

By combining Proposition 2 with Theorem 1, we obtain that \((KN/m)=\tilde{\mathcal{O}}(d^{2}H^{6}/\epsilon^{3})\) episodes are sufficient for Optimistic NPG to learn an \(\epsilon\)-optimal policy, improving upon the state-of-the-art policy optimization results (Zanette et al., 2021) by a factor of \(d\). Notably, this is also the first computationally efficient algorithm to achieve optimal quadratic dimension dependence for learning linear MDPs. The key factor behind this improvement is Optimistic NPG's periodic collection of fresh on-policy data, which eliminates the undesired correlation and avoids the union bound over certain nonlinear function class that are commonly observed in previous works. Consequently, Optimistic NPG uses a bonus function that is \(\sqrt{d}\) times smaller than in previous works (e.g., Jin et al., 2020). For further details on how we achieve this improvement, we refer interested readers to Appendix A, specifically Lemma 2.

### General function approximation

Now we instantiate OPE for RL with general function approximation. In this setting, the learner is provided with a function class \(\mathcal{F}=\mathcal{F}_{1}\times\cdots\times\mathcal{F}_{H}\) for approximating the Q values of polices, where \(\mathcal{F}_{h}\subseteq(\mathcal{S}\times\mathcal{A}\to[0,H])\).

General OPE.The pseudocode is provided in Subroutine 3. At each step \(h\in[H]\), general OPE first constructs a confidence set \(\mathcal{B}_{h}\) which contains candidate estimates for \(\overline{\mathcal{T}_{h}^{n}\mathcal{Q}_{h+1}}\) (Line 6). Specifically, \(\mathcal{B}_{h}\) consists of all the value candidates \(f_{h}\in\mathcal{F}_{h}\) whose square temporal difference (TD) error on dataset \(\mathcal{D}_{h}\) is no larger than the smallest one by an additive factor \(\beta\). Such construction can be viewed as a relaxation of the classic fitted Q-iteration (FQI) algorithm which only keeps the value candidate with the smallest TD error. In particular, if we pick \(\beta=0\), \(\mathcal{B}_{h}\) collapses to the solution of FQI. Equipped with confidence set \(\mathcal{B}_{h}\), general OPE then perform optimistic planning at every \((s,a)\in\mathcal{S}\times\mathcal{A}\) by computing \(\overline{Q}_{h}(s,a)\) to be the largest possible value that can be achieved by any candidate in confidence set \(\mathcal{B}_{h}\) (Line 7). We remark that general OPE shares a similar algorithmic spirit to the GOLF algorithm (Jin et al., 2021). The key difference is that GOLF is a purely value-based algorithm and only performs optimistic planning at the initial state while general OPE performs optimistic planning at every state and is part of a policy gradient algorithm.

Theoretical guarantee.To state the main theorem, we need to first introduce two standard concepts: value closeness and eluder dimension, which have been widely used in previous works that study RL with general function approximation.

**Assumption 1** (value closeness (Wang et al., 2020)).: _For all \(h\in[H]\) and \(V_{h+1}:\mathcal{S}\rightarrow[0,H]\), \(\mathcal{T}_{h}V_{h+1}\in\mathcal{F}_{h}\) where \([\mathcal{T}_{h}V_{h+1}](s,a)=R_{h}(s,a)+\mathbb{E}[V_{h+1}(s^{\prime})\mid s ^{\prime}\sim\mathbb{P}_{h}(\cdot\mid s,a)]\)._

Intuitively, Assumption 1 can be understood as requiring that the application of the Bellman operator \(\mathcal{T}_{h}\) to any V-value function \(V_{h+1}\) results in a function that belongs to the value function class \(\mathcal{F}_{h}\). This assumption holds in various settings, including tabular MDPs and linear MDPs. Furthermore, it is reasonable to assume that value closeness holds whenever the function class \(\mathcal{F}\) has sufficient expressive power, such as the class of neural networks.

**Definition 1** (eluder dimension (Russo and Van Roy, 2013)).: _Let \(\mathcal{G}\) be a function class from \(\mathcal{X}\) to \(\mathbb{R}\) and \(\epsilon>0\). We define the \(\epsilon\)-eluder dimension of \(\mathcal{G}\), denoted as \(d_{E}(\mathcal{G},\epsilon)\), to be the largest \(L\in\mathbb{N}^{+}\) such that there exists \(x_{1},\ldots,x_{L}\in\mathcal{X}\) and \(g_{1},g_{1}^{\prime},\ldots,g_{L},g_{L}^{\prime}\in\mathcal{G}\) satisfying: for all \(l\in[L]\), \(\sum_{i<l}(g_{l}(x_{i})-g_{l}^{\prime}(x_{i}))^{2}\leq\epsilon\) but \(g_{l}(x_{l})-g_{l}^{\prime}(x_{i})\geq\epsilon\)._

At a high level, eluder dimension \(d_{E}(\mathcal{G},\epsilon)\) measures how many mistakes we have to make in order to identify an unknown function from function class \(\mathcal{G}\) to accuracy \(\epsilon\), in the worst case. It has been widely used as a sufficient condition for proving sample-efficiency guarantee for optimistic algorithms in RL with general function approximation, e.g., Wang et al. (2020); Jin et al. (2021).

Now we state the theoretical guarantee for general OPE.

**Proposition 3** (general function approximation).: _Suppose Assumption 1 holds and we choose \(\beta=\Theta\left(H^{2}\log(|\mathcal{F}|NK/\delta)\right)\) in Subroutine 3, then Condition 1 holds with \(L=H^{3}\log(|\mathcal{F}|)\max_{h}d_{E}(\mathcal{F}_{h},1/N)\)._

Plugging Proposition 3 back into Theorem 1, we obtain that \((KN/m)=\tilde{\mathcal{O}}(d_{E}\log(|\mathcal{F}|)H^{6}/\epsilon^{3})\) episodes are sufficient for Optimistic NPG to learn an \(\epsilon\)-optimal policy, which to our knowledge is the first polynomial sample complexity guarantee for policy optimization with general function approximation.

## 5 Conclusions

We proposed a model-free, policy optimization algorithm--Optimistic NPG for online RL and analyzed its behavior in the episodic setting. In terms of algorithmic design, it is not only considerably simpler but also more closely resembles the empirical policy optimization algorithms (e.g., PPO, TRPO) than the previous theoretical algorithms. In terms of sample efficiency, for \(d\)-dimensional linear MDPs, it improves over state-of-the-art policy optimization algorithm by a factor of \(d\), and is the first computationally efficient algorithm to achieve the optimal dimension dependence. To our best knowledge, Optimistic NPG is also the first sample-efficient policy optimization algorithm under general function approximation.

For future research, we believe that it is an important direction to investigate the optimal complexity for using policy optimization algorithms to solve linear MDPs. Despite the optimal dependence on dimension \(d\), the current sample complexity of Optimistic NPG is worse than the optimal rate by a factor of \(1/\epsilon\). It remains unclear to us how to shave off this \(1/\epsilon\) factor to achieve the optimal rate.

## Acknowledgement

Qinghua Liu would like to thank Yu Bai for valuable discussions. This work was partially supported by National Science Foundation Grant NSF-IIS-2107304.

## References

* Abbasi-Yadkori et al. (2011) Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. _Advances in neural information processing systems_, 24, 2011.
* Abbasi-Yadkori et al. (2019) Yasin Abbasi-Yadkori, Peter Bartlett, Kush Bhatia, Nevena Lazic, Csaba Szepesvari, and Gellert Weisz. Politex: Regret bounds for policy iteration using expert prediction. In _International Conference on Machine Learning_, pages 3692-3702. PMLR, 2019.
* Agarwal et al. (2020) Alekh Agarwal, Mikael Henaff, Sham Kakade, and Wen Sun. Pc-pg: Policy cover directed exploration for provable policy gradient learning. _Advances in neural information processing systems_, 33:13399-13412, 2020.
* Agarwal et al. (2021) Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. _J. Mach. Learn. Res._, 22(98):1-76, 2021.
* Agarwal et al. (2022) Alekh Agarwal, Yujia Jin, and Tong Zhang. VO\(q\)L: Towards optimal regret in model-free RL with nonlinear function approximation. _arXiv preprint arXiv:2212.06069_, 2022.
* Berner et al. (2019) Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Christopher Hesse, Rafal Jozefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique Ponde de Oliveira Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, and Susan Zhang. Dota 2 with large scale deep reinforcement learning. _arXiv preprint arXiv:1912.06680_, 2019.
* Bhandari and Russo (2019) Jalaj Bhandari and Daniel Russo. Global optimality guarantees for policy gradient methods. _arXiv preprint arXiv:1906.01786_, 2019.
* Cai et al. (2020) Qi Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang. Provably efficient exploration in policy optimization. In _International Conference on Machine Learning_, pages 1283-1294. PMLR, 2020.
* Finn et al. (2016) Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control via policy optimization. In _International conference on machine learning_, pages 49-58. PMLR, 2016.
* He et al. (2022) Jiafan He, Heyang Zhao, Dongruo Zhou, and Quanquan Gu. Nearly minimax optimal reinforcement learning for linear Markov decision processes. _arXiv preprint arXiv:2212.06132_, 2022.
* Jin et al. (2020) Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In _Conference on Learning Theory_, pages 2137-2143. PMLR, 2020.
* Jin et al. (2021) Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes of RL problems, and sample-efficient algorithms. _Advances in neural information processing systems_, 34:13406-13418, 2021.
* Kakade (2001) Sham M Kakade. A natural policy gradient. _Advances in neural information processing systems_, 14, 2001.
* Liu et al. (2019) Boyi Liu, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural proximal/trust region policy optimization attains globally optimal policy. _arXiv preprint arXiv:1906.10306_, 2019.
* Neu et al. (2017) Gergely Neu, Anders Jonsson, and Vicenc Gomez. A unified view of entropy-regularized Markov decision processes. _arXiv preprint arXiv:1705.07798_, 2017.
* OpenAI (2022) OpenAI. ChatGPT: Optimizing language models for dialogue, 2022. URL https://openai.com/blog/chatgpt/.

Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic exploration. _Advances in Neural Information Processing Systems_, 26, 2013.
* Schulman et al. (2015) John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In _International conference on machine learning_, pages 1889-1897. PMLR, 2015.
* Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.
* Shani et al. (2020) Lior Shani, Yonathan Efroni, Aviv Rosenberg, and Shie Mannor. Optimistic policy optimization with bandit feedback. In _International Conference on Machine Learning_, pages 8604-8613. PMLR, 2020.
* Wagenmaker et al. (2022) Andrew J Wagenmaker, Yifang Chen, Max Simchowitz, Simon Du, and Kevin Jamieson. Reward-free RL is no harder than reward-aware RL in linear Markov decision processes. In _International Conference on Machine Learning_, pages 22430-22456. PMLR, 2022.
* Wang et al. (2020) Ruosong Wang, Russ R Salakhutdinov, and Lin Yang. Reinforcement learning with general value function approximation: Provably efficient approach via bounded eluder dimension. _Advances in Neural Information Processing Systems_, 33:6123-6135, 2020.
* Wu et al. (2022) Tianhao Wu, Yunchang Yang, Han Zhong, Liwei Wang, Simon Du, and Jiantao Jiao. Nearly optimal policy optimization with stable at any time guarantee. In _International Conference on Machine Learning_, pages 24243-24265. PMLR, 2022.
* Zanette et al. (2020) Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill. Learning near optimal policies with low inherent Bellman error. In _International Conference on Machine Learning_, pages 10978-10989. PMLR, 2020.
* Zanette et al. (2021) Andrea Zanette, Ching-An Cheng, and Alekh Agarwal. Cautiously optimistic policy optimization and exploration with linear function approximation. In _Conference on Learning Theory_, pages 4473-4525. PMLR, 2021.
* Zhou et al. (2021) Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari. Nearly minimax optimal reinforcement learning for linear mixture Markov decision processes. In _Conference on Learning Theory_, pages 4532-4576. PMLR, 2021.

Proof of Theorem 1

Recall in Algorithm 1, \(\pi^{\mathrm{out}}\) is sampled uniformly at random from \(\{\pi^{k}\}_{k\in[K]}\), so we have

\[\mathbb{E}\left[V_{1}^{\star}(s_{1})-V_{1}^{\pi^{\mathrm{out}}}(s_{1 })\right]\] \[=\frac{1}{K}\sum_{k=1}^{K}\left[V_{1}^{\star}(s_{1})-V_{1}^{\pi^{ k}}(s_{1})\right]\] \[=\underbrace{\frac{1}{K}\sum_{k=1}^{K}\left[V_{1}^{\star}(s_{1}) -\overline{V}_{1}^{k}(s_{1})\right]}_{\text{Term (I)}}+\underbrace{\frac{1}{K}\sum_{k=1}^{K}\left[\overline{V}_{1}^{k}(s_{1}) -V_{1}^{\pi^{k}}(s_{1})\right]}_{\text{Term (II)}}.\]

To control Term (I), we import the following generalized policy difference lemma from (Shani et al., 2020; Cai et al., 2020).

**Lemma 1** (generalized policy-difference lemma).: _For any policy \(\pi\) and \(k\in[K]\),_

\[V_{1}^{\pi}(s_{1})-\overline{V}_{1}^{k}(s_{1})\] \[= \sum_{h=1}^{H}\mathbb{E}_{s_{h}\sim\pi}\left[\langle\pi_{h}( \cdot\mid s_{h})-\pi_{h}^{k}(\cdot\mid s_{h}),\overline{Q}_{h}^{k}(s_{h}, \cdot)\rangle\right]\] \[-\sum_{h=1}^{H}\mathbb{E}_{(s_{h},a_{h})\sim\pi}\Bigg{[}\overline {Q}_{h}^{k}(s_{h},a_{h})-(\mathcal{T}_{h}^{\pi^{k}}\overline{Q}_{h+1}^{k})(s_{ h},a_{h})\Bigg{]}.\]

By invoking Lemma 1 with \(\pi=\pi^{\star}\), we have

\[\mathrm{Term(I)}=\frac{1}{K}\sum_{k=1}^{K}\left[V_{1}^{\star}(s_ {1})-\overline{V}_{1}^{k}(s_{1})\right]\] \[\leq H\max_{h,s}\Bigg{[}\frac{1}{K}\sum_{k=1}^{K}\left\langle\pi _{h}^{\star}(\cdot\mid s)-\pi_{h}^{k}(\cdot\mid s),\overline{Q}_{h}^{k}(s, \cdot)\right\rangle\Bigg{]}+H\max_{h,s,a}\Bigg{[}(\mathcal{T}_{h}^{\pi^{k}} \overline{Q}_{h+1}^{k}-\overline{Q}_{h}^{k})(s,a)\Bigg{]}\] (2) \[\leq\mathcal{O}\left(\frac{H\log|\mathcal{A}|}{\eta K}+\eta H^{3} \right)+H\max_{h,s,a}\Bigg{[}(\mathcal{T}_{h}^{\pi^{k}}\overline{Q}_{h+1}^{k} -\overline{Q}_{h}^{k})(s,a)\Bigg{]},\]

where the second inequality follows from the standard regret bound of online mirror ascent. By Condition (1A), the second term on the RHS of Equation (2) is upper bounded by zero. So we have

\[\mathrm{Term(I)}\leq\mathcal{O}\left(\frac{H\log|\mathcal{A}|}{\eta K}+\eta H^ {3}\right).\]

As for Term (II), we can directly upper bound it by Condition (1B) which gives:

\[\mathrm{Term(II)}\leq H\sqrt{(L/N)\times\log(NKL/\delta)}.\]

We complete the proof by plugging in the choice of \(K\), \(N\), \(\eta\).

## Appendix B Proofs for Examples

In this section, we prove the three instantiations of OPE in Section 4 satisfy Condition 1 with mild complexity measure \(L\). The proofs of the lemmas used in this section can be found in Appendix C.

### Proof of Proposition 2 (linear OPE)

We start with the following lemma showing that \(\overline{Q}_{h}^{k}\) is an optimistic estimate of \(\mathcal{T}_{h}^{\pi^{k}}\overline{Q}_{h+1}^{k}\) and the degree of over-optimism can be bounded by the bonus function.

[MISSING_PAGE_FAIL:13]

### Proof of Proposition 1 (tabular OPE)

The proof follows basically the same as that of Proposition 2. The only modification needed is to replace Lemma 2 and 4 by their tabular counterparts, which we state below.

**Lemma 5** (optimism of value estimates: tabular).: _Under the same choice of \(\alpha\) as in Proposition 1, with probability at least \(1-\delta\): for all \((k,h,s,a)\in[K]\times[H]\times\mathcal{S}\times\mathcal{A}\):_

\[0\leq(\overline{Q}_{h}^{k}-\mathcal{T}_{h}^{\pi^{k}}\overline{Q}_{h+1}^{k})(s,a)\leq 2b_{h}^{k}(s,a),\]

_where \(\{b_{h}^{k}\}_{h\in[H]}\) are the bonus functions defined in Line 6, Subroutine 1 tabular \(\operatorname{OPE}(\pi^{k},\mathcal{D}^{k})\)._

**Lemma 6** (on-policy uncertainty: tabular).: _Let \(\pi\) be an arbitrary policy. Suppose we sample \(\{(s_{h}^{n},a_{h}^{n})\}_{n=1}^{N}\) i.i.d. from \(\pi\). Denote \(J_{h}(s,a)=\sum_{i=1}^{N}\mathbf{1}((s_{h}^{i},a_{h}^{i})=(s,a))\). Then with probability at least \(1-\delta\):_

\[\mathbb{E}_{(s_{h},a_{h})\sim\pi}\left[\sqrt{\frac{1}{J_{h}(s,a)+1}}\right]= \mathcal{O}\left(\sqrt{\frac{SA\log(N/\delta)}{N}}\right).\]

### Proof of Proposition 3 (general OPE)

The proof follows basically the same as that of Proposition 2. The only modification needed is to replace Lemma 2 and 4 by their general counterparts, which we state below.

**Lemma 7** (optimism of value estimates: general).: _Suppose Assumption 1 holds. There exists an absolute constant \(c\) such that with probability at least \(1-\delta\): for all \((k,h,s,a)\in[K]\times[H]\times\mathcal{S}\times\mathcal{A}\):_

\[0\leq(\overline{Q}_{h}^{k}-\mathcal{T}_{h}^{\pi^{k}}\overline{Q}_{h+1}^{k})(s,a)\leq b_{h}(s,a,\mathcal{D}_{h}^{k}):=\left(\begin{array}{c}\sup_{f_{h},f _{h}^{\prime}\in\mathcal{F}_{h}}f_{h}(s,a)-f_{h}^{\prime}(s,a)\\ \text{s.t.}\sum_{(s_{h},a_{h})\in\mathcal{D}_{h}^{k}}(f_{h}(s_{h},a_{h})-f_{h} ^{\prime}(s_{h},a_{h}))^{2}\leq c\beta\end{array}\right).\]

**Lemma 8** (on-policy uncertainty: general).: _Let \(\pi\) be an arbitrary policy. Suppose we sample \(\mathcal{D}_{h}=\{(s_{h}^{n},a_{h}^{n})\}_{n=1}^{N}\) i.i.d. from \(\pi\). Then with probability at least \(1-\delta\):_

\[\mathbb{E}_{(s_{h},a_{h})\sim\pi}\left[b_{h}(s,a,\mathcal{D}_{h})\right]= \mathcal{O}\left(H\sqrt{\frac{d_{E}(1/N,\mathcal{F}_{h})\beta}{N}}+H\sqrt{ \frac{\log(1/\delta)}{N}}\right).\]

## Appendix C Proofs of Lemmas

### Proof of Lemma 1

Lemma 1 is an immediate consequence of Lemma 4.2 in (Cai et al., 2020) or Lemma 1 in (Shani et al., 2020).

### Proof of Lemma 2

Let us consider a fixed pair \((k,h)\in[K]\times[H]\). Recall \(\overline{Q}_{h}^{k}\) is defined as

\[\overline{Q}_{h}^{k}(s,a)=\operatorname{Truncate}_{[0,H-h+1]}\bigl{(}\langle \hat{\theta}_{h}^{k},\phi_{h}(s,a)\rangle+b_{h}^{k}(s,a)\bigr{)},\]

where

\[\hat{\theta}_{h}^{k}=\operatorname*{argmin}_{(s_{h},a_{h},r_{h},s_{h+1})\in \mathcal{D}_{h}^{k}}\left(\phi_{h}(s_{h},a_{h})^{\mathrm{T}}\theta-r_{h}- \overline{V}_{h+1}^{k}(s_{h+1})\right)^{2}+\lambda\|\theta\|_{2}^{2},\]

\[b_{h}^{k}(s,a)=\alpha\times\|\phi_{h}(s,a)\|_{(\Sigma_{h}^{k}+\lambda\mathbf{I} _{d\times d})^{-1}},\quad\text{and}\quad\Sigma_{h}^{k}=\sum_{(s_{h},a_{h})\in \mathcal{D}_{h}^{k}}\phi(s_{h},a_{h})(\phi(s_{h},a_{h}))^{\mathrm{T}}.\]

By the standard linear completeness property of linear MDPs Jin et al. (2020), there exists \(\theta_{h}^{k}\) such that \(\|\theta_{h}^{k}\|_{2}\leq H\sqrt{d}\) and

\[\langle\phi_{h}(s,a),\theta_{h}^{k}\rangle=R_{h}(s,a)+\mathbb{E}_{s^{\prime} \sim\Gamma(\cdot|s,a)}\left[\overline{V}_{h+1}^{k}(s^{\prime})\right]\quad \text{for all }(s,a)\in\mathcal{S}\times\mathcal{A}.\]Therefore, to prove Lemma 2, it suffices to show that for all \((s,a)\)

\[\left|\langle\phi_{h}(s,a),\theta_{h}^{k}-\hat{\theta}_{h}^{k}\rangle\right|\leq b _{h}^{k}(s,a).\]

To condense notations, denote by \(\{(s_{h}^{i},a_{h}^{i},r_{h}^{i},s_{h+1}^{i})\}_{i=1}^{M}\) the tuples in \(\mathcal{D}_{h}^{k}\). By the definition of ridge regression,

\[\left|\langle\phi_{h}(s,a),\hat{\theta}_{h}^{k}-\theta_{h}^{k} \rangle\right|\] \[= \left|\left\langle\phi_{h}(s,a)\,\ (\Sigma_{h}^{k}+\lambda\mathbf{I}_ {d\times d})^{-1}\times\right.\right.\] \[\left.\left.\left(\sum_{i=1}^{M}\phi_{h}(s_{h}^{i},a_{h}^{i}) \left(r_{h}^{i}+\overline{V}_{h+1}^{k}(s_{h+1}^{i})-\phi_{h}(s_{h}^{i},a_{h}^{ i})^{\mathrm{T}}\theta_{h}^{k}\right)-\lambda\theta_{h}^{k}\right)\right\rangle\right|\] \[\leq \|\phi_{h}(s,a)\|_{(\Sigma_{h}^{k}+\lambda\mathbf{I}_{d\times d} )^{-1}}\times\] \[\left.\left(\left\|\sum_{i=1}^{M}\phi_{h}(s_{h}^{i},a_{h}^{i}) \left(r_{h}^{i}+\overline{V}_{h+1}^{k}(s_{h+1}^{i})-\phi_{h}(s_{h}^{i},a_{h}^{ i})^{\mathrm{T}}\theta_{h}^{k}\right)\right\|_{(\Sigma_{h}^{k}+\mathbf{\lambda }\mathbf{I}_{d\times d})^{-1}}+H\sqrt{d}\right),\]

where the inequality follows from \(\|\lambda\theta_{h}^{k}\|_{(\Sigma_{h}^{k}+\mathbf{\lambda}\mathbf{I}_{d \times d})^{-1}}\leq\sqrt{\lambda}\|\theta_{h}^{k}\|_{2}\leq H\sqrt{d}\). It remains to bound the first term in the bracket. Now here comes the key observation that helps shave off the \(\sqrt{d}\) factor from the bonus.

**Observation 1**.: \(\mathcal{D}_{h}^{k}\) _and \(\overline{V}_{h+1}^{k}\) are independent conditioning on \(\pi^{t_{k}}\) where \(t_{k}\) denotes the index of the last iteration of collecting fresh data before the \(k^{\mathrm{th}}\) iteration._

To see why, recall that after obtaining \(\mathcal{D}^{t_{k}}\) Algorithm 2 splits it evenly into \(H\) disjoint subsets \(\{\mathcal{D}_{h}^{t_{h}}\}_{h=1}^{H}\) and then only uses \(\{\mathcal{D}_{h^{\prime}}^{t_{h}}\}_{h^{\prime}=h+1}^{H}\) for evaluating and improving \(\{\pi_{h^{\prime}}^{l}\}_{h^{\prime}=h+1}^{H}\) for \(l\in[t_{k},t_{k}+m-1]\). As a result, \(\{\overline{V}_{h+1}^{t_{h}+m-1}\}_{t_{k}}\) (including \(\overline{V}_{h+1}^{k}\)) is independent of \(\mathcal{D}_{h}^{t_{k}}=\mathcal{D}_{h}^{k}\) conditioning on \(\pi^{t_{k}}\). By the concentration of self-normalized processes [Abbasi-Yadkori et al., 2011], we conclude that with probability at least \(1-\delta\): for all \((k,h)\in[K]\times[H]\)

\[\left\|\sum_{i=1}^{M}\phi_{h}(s_{h}^{i},a_{h}^{i})\left(r_{h}^{i}+\overline{ V}_{h+1}^{k}(s_{h+1}^{i})-\phi_{h}(s_{h}^{i},a_{h}^{i})^{\mathrm{T}}\theta_{h}^{ k}\right)\right\|_{(\Sigma_{h}^{k}+\mathbf{\lambda}\mathbf{I}_{d\times d})^{-1}} \leq\mathcal{O}\left(H\sqrt{d\log(MKH/\delta)}\right).\]

### Proof of Lemma 3

We first introduce the following two auxiliary lemmas about the lipschitzness continuity of softmax parameterization.

**Lemma 9**.: _There exists an absolute constant \(c>0\) such that for any policy \(\pi,\hat{\pi}\) satisfying \(\hat{\pi}_{h}(a\mid s)\propto\pi_{h}(a\mid s)\times\exp(L_{h}(s,a))\) where \(\{L_{h}(s,a)\}_{h\in[H]}\) is set of functions from \(\mathcal{S}\times\mathcal{A}\) to \([-1/H,1/H]\), we have that for any \(\tau_{H}:=(s_{1},a_{1},\ldots,s_{H},a_{H})\in(\mathcal{S}\times\mathcal{A})^{H}\):_

\[\mathbb{P}^{\hat{\pi}}(\tau_{H})\leq c\times\mathbb{P}^{\pi}(\tau_{H}).\]

Proof of Lemma 9.: By using the relation between \(\pi\) and \(\hat{\pi}\) and the normalization property of policy \(\pi\), we have that for any \((h,s,a)\):

\[\hat{\pi}_{h}(a\mid s) =\frac{\pi_{h}(a\mid s)\times\exp(L_{h}(s,a))}{\sum_{a^{\prime}} \pi_{h}(a^{\prime}\mid s)\times\exp(L_{h}(s,a^{\prime}))}\] \[\leq\frac{\pi_{h}(a\mid s)\times\exp(1/H)}{\sum_{a^{\prime}}\pi_{h }(a^{\prime}\mid s)\exp(-1/H)}\] \[=\pi_{h}(a\mid s)\times\exp(2/H)\leq\pi_{h}(a\mid s)\times\left(1+ \frac{\mathcal{O}(1)}{H}\right).\]Therefore, for any \(\tau_{H}:=(s_{1},a_{1},\ldots,s_{H},a_{H})\)

\[\mathbb{P}^{\sharp}(\tau_{H})= \left(\prod_{h=1}^{H-1}\mathbb{P}_{h}(s_{h+1}\mid s_{h},a_{h}) \right)\times\left(\prod_{h=1}^{H}\hat{\pi}_{h}(a_{h}\mid s_{h})\right)\] \[\leq \left(\prod_{h=1}^{H-1}\mathbb{P}_{h}(s_{h+1}\mid s_{h},a_{h}) \right)\times\left(\prod_{h=1}^{H}\pi_{h}(a_{h}\mid s_{h})\right)\times\left(1 +\frac{\mathcal{O}(1)}{H}\right)^{H}\] \[= \mathcal{O}(1)\times\left(\prod_{h=1}^{H-1}\mathbb{P}_{h}(s_{h+1 }\mid s_{h},a_{h})\right)\times\left(\prod_{h=1}^{H}\pi_{h}(a_{h}\mid s_{h}) \right)=\mathcal{O}(1)\times\mathbb{P}^{\pi}(\tau_{H}).\]

**Lemma 10**.: _Let \(\mu,\nu\) be two probability densities defined over \(\mathcal{X}\) such that \(\|\mu/\nu\|_{\infty}\leq\alpha\), then we have that for any function \(f:\mathcal{X}\to\mathbb{R}^{+}\), \(\mathbb{E}_{\mu}[f(x)]\leq\alpha\mathbb{E}_{\nu}[f(x)]\)._

Proof of Lemma 10.: By definition, \(\mathbb{E}_{\mu}[f(x)]=\int_{x}f(x)\mu(x)dx\leq\int_{x}f(x)(\alpha\nu(x))dx= \alpha\mathbb{E}_{\nu}[f(x)]\). 

By the update rule of NPG, we have

\[\pi_{h}^{k}(\cdot\mid s)\propto\pi_{h}^{t_{k}}(\cdot\mid s)\times\exp\left( \eta\sum_{i=t_{k}}^{k-1}\overline{Q}_{h}^{i}(s,\cdot)\right).\]

Since we choose \(\eta\) and \(m\) such that \(\eta m\leq 1/H^{2}\),

\[\left|\eta\sum_{i=t_{k}}^{k-1}\overline{Q}_{h}^{i}(s,\cdot)\right|\leq\eta(k- t_{k})H\leq\eta mH\leq 1/H.\]

Therefore, by invoking Lemma 9 with \(\hat{\pi}=\pi^{k}\) and \(\pi=\pi^{t_{k}}\), we have that for any \(\tau_{H}\in(\mathcal{S}\times\mathcal{A})^{H}\):

\[\mathbb{P}^{\pi^{k}}(\tau_{H})\leq c\times\mathbb{P}^{\pi^{t_{k}}}(\tau_{H}).\]

By further invoking Lemma 10 with \(\mathcal{X}=(\mathcal{S}\times\mathcal{A})^{H}\), \(\mu=\mathbb{P}^{\pi^{k}}\) and \(\nu=\mathbb{P}^{\pi^{t_{k}}}\), we obtain that for any function \(f:\mathcal{S}\times\mathcal{A}\to\mathbb{R}^{+}\):

\[\mathbb{E}_{\pi^{t_{k}}}[f(s_{h},a_{h})]=\mathcal{O}\left(\mathbb{E}_{\pi^{t_{ k}}}[f(s_{h},a_{h})]\right).\]

Similarly, we can show \(\mathbb{E}_{\pi^{t_{k}}}[f(s_{h},a_{h})]=\mathcal{O}\left(\mathbb{E}_{\pi^{k}}[ f(s_{h},a_{h})]\right)\), which completes the proof of Lemma 3.

### Proof of Lemma 4

To simplify notations, denote \(\phi_{h}^{i}:=\phi_{h}(s_{h}^{i},a_{h}^{i})\). For the technical purpose of applying martingale concentration, we additionally define the intermediate empirical covariance matrices: \(\Sigma_{h}^{n}=\sum_{i=1}^{n-1}\phi_{h}^{i}(\phi_{h}^{i})^{\mathrm{T}}\), for \(n\in[N]\). We have that with probability at least \(1-\delta\),

\[\mathbb{E}_{(s_{h},a_{h})\sim\pi}\big{[}\|\phi_{h}(s_{h},a_{h})\| _{(\Sigma_{h}+\lambda\mathbf{I}_{d\times d})^{-1}}\big{]}\] (4) \[\leq\frac{1}{N}\sum_{n=1}^{N}\mathbb{E}_{(s_{h},a_{h})\sim\pi} \big{[}\|\phi_{h}(s_{h},a_{h})\|_{(\Sigma_{h}^{n}+\lambda\mathbf{I}_{d\times d })^{-1}}\big{]}\] \[\leq\frac{1}{N}\sum_{n=1}^{N}\|\phi_{h}^{n}\|_{(\Sigma_{h}^{n}+ \lambda\mathbf{I}_{d\times d})^{-1}}+\mathcal{O}\left(\sqrt{\frac{\log(1/ \delta)}{N}}\right)\] \[\leq\mathcal{O}\left(\sqrt{\frac{d\log N}{N}}\right)+\mathcal{O} \left(\sqrt{\frac{\log(1/\delta)}{N}}\right),\]

where the first inequality uses \(\Sigma_{h}^{n}\preceq\Sigma_{h}\), the second one uses Azuma-Hoeffding inequality with \(\lambda=1\), and the last one uses Cauchy-Schwarz inequality and the the standard elliptical potential argument.

### Proof of Lemma 5

Let us consider a fixed pair \((k,h)\in[K]\times[H]\). Recall \(\overline{Q}_{h}^{k}\) is defined as

\[\overline{Q}_{h}^{k}(s,a)=\min\left\{H-h+1,\ \mathbb{E}_{s^{\prime}\sim \mathbb{\hat{b}}_{h}^{k}(s,a)}[\overline{V}_{h+1}^{k}(s^{\prime})]+\hat{R}_{h}^ {k}(s,a)+b_{h}^{k}(s,a)\right\}\]

where \(\hat{\mathbb{P}}_{h}^{k}\) and \(\hat{\mathbb{R}}_{h}^{k}\) are the empirical estimates of transition and reward at step \(h\) from \(\mathcal{D}_{h}^{k}\), and \(b_{h}^{k}(s,a)=\alpha(J_{h}^{k}(s,a)+1)^{-1/2}\) with \(J_{h}^{k}(s,a)=\sum_{(s_{h},a_{h})\in\mathcal{D}_{h}^{k}}\mathbf{1}((s_{h},a_{ h})=(s,a))\). Therefore, to prove Lemma 5, it suffices to show that for all \((s,a)\)

\[\left|\left(\mathbb{E}_{s^{\prime}\sim\mathbb{\hat{b}}_{h}^{k}(s,a)}[\overline {V}_{h+1}^{k}(s^{\prime})]-\mathbb{E}_{s^{\prime}\sim\mathbb{\hat{p}}_{h}(s,a )}[\overline{V}_{h+1}^{k}(s^{\prime})]\right)+\left(\hat{R}_{h}^{k}(s,a)-R_{h} (s,a)\right)\right|\leq b_{h}^{k}(s,a).\]

By observation 1, we know \(\{\overline{V}_{h+1}^{l}\}_{l=t_{h}}^{t_{h}+m-1}\) is independent of \(\mathcal{D}_{h}^{t_{k}}=\mathcal{D}_{h}^{k}\) conditioning on \(\pi^{t_{k}}\), which implies \(\overline{V}_{h+1}^{k}\) is independent of \(\hat{\mathbb{P}}_{h}^{k}\). Therefore, by Azuma-Hoeffding inequality and standard union bound, we conclude that with probability at least \(1-\delta\): for all \((k,h,s,a)\in[K]\times[H]\times\mathcal{S}\times\mathcal{A}\),

\[\left|\left(\mathbb{E}_{s^{\prime}\sim\hat{\mathbb{P}}_{h}^{k}(s, a)}[\overline{V}_{h+1}^{k}(s^{\prime})]-\mathbb{E}_{s^{\prime}\sim\mathbb{ \hat{p}}_{h}(s,a)}[\overline{V}_{h+1}^{k}(s^{\prime})]\right)+\left(\hat{R}_{h} ^{k}(s,a)-R_{h}(s,a)\right)\right|\] \[\leq\mathcal{O}\left(H\sqrt{\frac{\log(KNHSA/\delta)}{J_{h}^{k}(s,a)+1}}\right).\]

### Proof of Lemma 6

For the technical purpose of performing martingale concentration, we introduce the notion of intermediate counters: \(J_{h}^{n}(s,a)=\sum_{i=1}^{n-1}\mathbf{1}((s_{h}^{i},a_{h}^{i})=(s,a))\), for \(n\in[N]\). We have that with probability at least \(1-\delta\),

\[\mathbb{E}_{(s_{h},a_{h})\sim\pi}\left[\sqrt{\frac{1}{J_{h}(s,a)+ 1}}\right]\] (5) \[\leq\frac{1}{N}\sum_{n=1}^{N}\mathbb{E}_{(s_{h},a_{h})\sim\pi} \left[\sqrt{\frac{1}{J_{h}^{n}(s,a)+1}}\right]\] \[\leq\frac{1}{N}\sum_{n=1}^{N}\sqrt{\frac{1}{J_{h}^{n}(s_{h}^{n}, a_{h}^{n})+1}}+\mathcal{O}\left(\sqrt{\frac{\log(1/\delta)}{N}}\right)\] \[\leq\mathcal{O}\left(\sqrt{\frac{SA\log N}{N}}\right)+\mathcal{O }\left(\sqrt{\frac{\log(1/\delta)}{N}}\right),\]

where the first inequality uses \(J_{h}^{n}(s,a)\leq J_{h}(s,a)\) for all \((s,a)\in\mathcal{S}\times\mathcal{A}\), the second one uses Azuma-Hoeffding inequality, and the last one follows the standard pigeon-hole argument.

### Proof of Lemma 7

By observation 1, we know \(\mathcal{D}_{h}^{k}\) is independent of \(\overline{V}_{h+1}^{k}\) conditioning on \(\pi^{t_{k}}\) where \(t_{k}\) is the index of the last iteration when Optimistic NPG collects fresh data before iteration \(k+1\). With this independence relation in mind, we can easily prove that confidence set \(\mathcal{B}_{h}^{k}\) satisfy the following properties by standard concentration inequality and union bounds.

**Lemma 11**.: _Suppose Assumption 1 holds. There exists an absolute constant \(c\) such that with probability at least \(1-\delta\), for all \(k\in[K]\) and \(h\in[H]\),_

* \(\mathcal{T}_{h}^{\pi^{k}}\overline{Q}_{h+1}^{k}\in\mathcal{B}_{h}^{k}\)_,_
* _for any_ \(f_{h}\in\mathcal{B}_{h^{\prime}}^{k}\sum_{(s_{h},a_{h})\in\mathcal{D}_{h}^{k} }(f_{h}(s_{h},a_{h})-\mathcal{T}_{h}^{\pi^{k}}\overline{Q}_{h+1}^{k}(s_{h},a_{ h}))^{2}\leq c\beta\)_._

The proof of Lemma 11 follows trivially from modifying the proofs of Lemma 39 and 40 in Jin et al. (2021), which we omit here.

Define

\[\bar{\mathcal{B}}_{h}^{k}=\{f_{h}\in\mathcal{F}_{h}:\ \sum_{(s_{h},a_{h})\in \mathcal{D}_{h}^{k}}(f_{h}(s_{h},a_{h})-(\mathcal{T}_{h}^{\pi^{k}}\overline{Q}_{ h+1}^{k})(s_{h},a_{h}))^{2}\leq c\beta\}.\]

By using the first relation in Lemma 11 and the definition of \(\overline{Q}_{h}^{k}\), we immediately obtain that

\[(\mathcal{T}_{h}^{\pi^{k}}\overline{Q}_{h+1}^{k})(s,a)\leq\sup_{f_{h}\in \mathcal{B}_{h}^{k}}f_{h}(s,a)=\overline{Q}_{h}^{k}(s,a)\]

and

\[\overline{Q}_{h}^{k}(s,a)-(\mathcal{T}_{h}^{\pi^{k}}\overline{Q}_{h+1}^{k})(s,a)\leq\sup_{f_{h},f_{h}^{\prime}\in\mathcal{B}_{h}^{k}}(f_{h}(s,a)-f_{h}^{ \prime}(s,a))\leq\sup_{f_{h},f_{h}^{\prime}\in\mathcal{B}_{h}^{k}}(f_{h}(s,a )-f_{h}^{\prime}(s,a)),\]

where the last inequality uses \(\bar{\mathcal{B}}_{h}^{k}\subset\mathcal{B}_{h}^{k}\) by Lemma 11.

### Proof of Lemma 8

Recall we define

\[b_{h}(s,a,\mathcal{D}_{h}):=\left(\begin{array}{c}\sup_{f_{h},f_{h}^{\prime }\in\mathcal{F}_{h}}f_{h}(s,a)-f_{h}^{\prime}(s,a)\\ \text{s.t.}\ \sum_{(s_{h},a_{h})\in\mathcal{D}_{h}}(f_{h}(s_{h},a_{h})-f_{h}^{ \prime}(s_{h},a_{h}))^{2}\leq c\beta\end{array}\right),\]

which implies that \(b_{h}(s,a,\mathcal{D}_{h})\leq b_{h}(s,a,\underline{\mathcal{D}}_{h})\) for any \(\underline{\mathcal{D}}_{h}\subseteq\mathcal{D}_{h}\). Let \(\mathcal{D}_{h}^{(n)}=\{(s_{h}^{i},a_{h}^{i})\}_{i=1}^{n}\). We have that with probability at least \(1-\delta\),

\[\mathbb{E}_{(s_{h},a_{h})\sim\pi}\left[b_{h}(s,a,\mathcal{D}_{h})\right] \leq\frac{1}{N}\sum_{n=1}^{N}\mathbb{E}_{(s_{h},a_{h})\sim\pi} \left[b_{h}(s,a,\mathcal{D}_{h}^{(n-1)})\right]\] \[\leq\frac{1}{N}\sum_{n=1}^{N}b_{h}(s_{h}^{n},a_{h}^{n},\mathcal{ D}_{h}^{(n-1)})+\mathcal{O}\left(H\sqrt{\frac{\log(1/\delta)}{N}}\right)\] \[\leq\mathcal{O}\left(H\sqrt{\frac{d_{E}(1/N,\mathcal{F}_{h}) \beta}{N}}+H\sqrt{\frac{\log(1/\delta)}{N}}\right),\]

where the second inequality uses Azuma-Hoeffding inequality and the last one uses the standard regret guarantee for eluder dimension (e.g., Lemma 2 in Russo and Van Roy (2013)).