# An Efficient High-Dimensional Gradient Estimator

for Stochastic Differential Equations

 Shengbo Wang

MS&E

Stanford University

Stanford, CA 94305

shengbo.wang@stanford.edu

&Jose Blanchet

MS&E

Stanford University

Stanford, CA 94305

jose.blanchet@stanford.edu

&Peter Glynn

MS&E

Stanford University

Stanford, CA 94305

glynn@stanford.edu

###### Abstract

Overparameterized stochastic differential equation (SDE) models have achieved remarkable success in various complex environments, such as PDE-constrained optimization, stochastic control and reinforcement learning, financial engineering, and neural SDEs. These models often feature system evolution coefficients that are parameterized by a high-dimensional vector \(\theta\in\mathbb{R}^{n}\), aiming to optimize expectations of the SDE, such as a value function, through stochastic gradient ascent. Consequently, designing efficient gradient estimators for which the computational complexity scales well with \(n\) is of significant interest. This paper introduces a novel unbiased stochastic gradient estimator-the generator gradient estimator-for which the computation time remains stable in \(n\). In addition to establishing the validity of our methodology for general SDEs with jumps, we also perform numerical experiments that test our estimator in linear-quadratic control problems parameterized by high-dimensional neural networks. The results show a significant improvement in efficiency compared to the widely used pathwise differentiation method: Our estimator achieves near-constant computation times, increasingly outperforms its counterpart as \(n\) increases, and does so without compromising estimation variance. These empirical findings highlight the potential of our proposed methodology for optimizing SDEs in contemporary applications.

## 1 Introduction

We consider a family of jump diffusions \(\left\{X_{\theta}^{x}(t,s)\in\mathbb{R}^{d}:s\in[t,T]\right\}\) that are generated by stochastic differential equations (SDEs) and indexed by the initial condition \(x\in\mathbb{R}^{d}\) at time \(s\) and a parameter \(\theta\in\Theta\subset\mathbb{R}^{n}\). In modern applications, the parameter \(\theta\), encoding characteristics of an engineering model, often represents the weights of a deep neural network. This paper focuses particularly on scenarios where the dimension \(n\) of \(\theta\) is significantly greater than the dimension \(d\) of the space. This setting naturally arises in the implementation of large AI architectures in modern applications.

Concretely, for each \(1\leq i\leq d\), the \(i\)'th entry of \(X^{x}_{\theta}(t,\cdot)\), denoted by \(X^{x}_{\theta,i}(t,\cdot)\), satisfies the Ito SDE:

\[\begin{split} X^{x}_{\theta,i}(t,s)&=x_{i}+\int_{t} ^{s}\mu_{\theta,i}(r,X^{x}_{\theta}(t,r))d\mbox{$r$}\\ &\quad+\int_{t}^{s}\sum_{k=1}^{d^{\prime}}\sigma_{\theta,i,k}(r,X^ {x}_{\theta}(t,r-))dB_{k}(r)+\int_{t}^{s}dJ_{\theta,i}(r)\end{split} \tag{1.1}\]

Here, \(\{\mu_{\theta,i}:1\leq i\leq d\}\) and \(\{\sigma_{\theta,i,k}:1\leq i,k\leq d\}\) are the drift and volatility, respectively, satisfying suitable regularity conditions (to be discussed). For simplicity in our introductory explanations, we will assume that the jump term \(J_{\theta}\) is zero. However, incorporating this jump feature is valuable in many applied settings, and arises in various fields such as financial engineering [17], stochastic control [6], and neural SDE models [7]. Accordingly, we will fully integrate and discuss the jump components in our main results in Section 3.

The primary objective of this paper is to develop an efficient gradient estimator, with respect to \(\theta\), for a large class of path-dependent expectations derived from an SDE. Concretely, we consider

\[v_{\theta}(t,x)=E\left[\int_{t}^{T}\rho_{\theta}(s,X^{x}_{\theta}(t,s))ds+g_{ \theta}(X^{x}_{\theta}(t,T))\right]. \tag{1.2}\]

The value \(v_{\theta}(t,x)\) represents the expected cumulative reward running \(X^{x}_{\theta}\) from time \(t\) to \(T\). Here, \(\rho_{\theta}\) and \(g_{\theta}\) represents the reward rate and the terminal reward, respectively. This formulation encompasses a wide range of science and engineering problems including PDE-constrained optimization [22, 20], stochastic control and reinforcement learning [8], and neural SDE models [23].

The gradient \(\nabla_{\theta}v_{\theta}(t,x)=(\partial_{\theta_{t}}v_{\theta}(t,x),\ldots, \partial_{\theta_{t}}v_{\theta}(t,x))\in\mathbb{R}^{n}\) is of significant interest in the sensitivity analysis, learning, and optimization of these models. In particular, finding an efficient unbiased estimator for \(\nabla_{\theta}v_{\theta}(t,x)\) with low variance is essential if one is to apply stochastic gradient descent to find near optimal policies or model parameters within the parametric class \(\theta\in\Theta\).

Under reasonable smoothness and integrability conditions, it is natural to consider the pathwise differentiation estimator obtained by applying infinitesimal perturbation analysis (IPA) to the sample path of \(X^{x}_{\theta}\) w.r.t. the \(i\)th coordinate of \(\theta\). For instance, if \(\rho_{\theta}(\cdot)=\rho(\cdot)\) independent of \(\theta\) and \(g=0\), then we have a representation

\[\partial_{\theta_{i}}v_{\theta}(0,x)=E\left[\int_{0}^{T}\sum_{j=1}^{d}\partial _{x_{j}}\rho_{\theta}(t,X^{x}_{\theta}(t))\partial_{\theta_{i}}X^{x}_{\theta,j }(t)dt\right]. \tag{1.3}\]

where \(\partial_{\theta_{i}}X^{x}_{\theta}(t)\) is the pathwise derivative of the process \(X^{x}_{\theta}\) w.r.t. \(\theta_{i}\). The processes \(\{X^{x}_{\theta,j},\partial_{\theta_{i}}X^{x}_{\theta,j}:i=1,\ldots,n;j=1, \ldots,d\}\) satisfy a system of \(d+d\cdot n\) SDEs [11, Equation (3.31)], which must be jointly simulated. Therefore, to estimate the gradient, the pathwise differentiation method requires simulating this \(d+d\cdot n\) dimensional SDE. Note that the dimension is linear in \(n\), the dimension of the parameter space. Contemporary applications of SDEs in physics-informed and data-driven environments such as deep neural SDEs and deep RL where overparameterization excel, necessitate a model with exceptionally large \(n\) that is often many orders of magnitude larger than \(d\). Hence, simulating the SDE of dimension \(d+d\cdot n\) becomes extremely resource-intensive. Motivated by these applications, we ask the following question:

_Can we device an efficient, unbiased, and finite variance estimator for \(\nabla_{\theta}v_{\theta}(t,x)\) with a computation time insensitive to \(n\)?_

The answer is affirmative. Precisely, our main contribution is designing the unbiased _generator gradient_ estimator of \(\nabla_{\theta}v_{\theta}(t,x)\) that requires only simulating \(O(d^{2})\) SDEs when the volatility parameters \(\sigma_{\theta}\) do not depend on \(\theta\) and \(O(d^{3})\) SDEs in the general setting, as summarized in Table 1.

We remark that in addition to pathwise differentiation, likelihood ratio-based estimators are also popular for sensitivity analysis in SDEs; see e.g. Yang and Kushner [26]. However, typically they are only applicable if \(\sigma_{\theta}\) is independent of \(\theta\) and under more restrictive jump structures. When applicable, likelihood ratio-based estimators could be appealing alternatives as they introduce a change of measure that represents the derivatives as a functional of the \(d\)-dimensional processes \(X^{x}_{\theta}\). Nevertheless, these estimators typically have significantly higher variance.

Finally, we apply our estimator to linear-quadratic control problems and test its performance in optimizing neural-network-parameterized controls. As we increase the number of network parameters \(n\), the results in Figure 0(a) and Table 2 highlight a substantial improvement in computational efficiency, as compared to the pathwise differentiation method, while still maintaining competitive variance levels. Furthermore, Figure 0(a) confirms that the computation time of our estimator is robust to increases in \(n\), even in extremely high-dimensional scenarios with \(n\) approaching \(10^{8}\).

### Literature Review

**Gradient Estimation:** Gradient estimation, particularly likelihood ratios and IPA methods, is crucial in sensitivity analysis. Foundational works in the late 20th century by Glynn [5; 4] and further adaptations to the SDE setting [26; 3] highlight these developments. IPA has evolved to apply stochastic flow techniques to SDEs, both with and without reflecting boundaries [23; 12; 16; 24; 14].

**Applications of Gradient Estimators:** Gradient estimators are widely used in stochastic control and reinforcement learning (RL) models. Policy gradient methods in discrete-time RL, including REINFORCE and deep policy gradient approaches, are notable applications [25; 13; 21]. Continuous-time RL have been explored using policy gradients in settings with continuous diffusion dynamics [8]. Jump diffusions are important models in financial engineering and stochastic control [17; 15; 9; 6]. Gradient estimators can also be used for optimizing these models. Neural SDE models are modern computational frameworks that model the dynamics of stochastic systems using a neural-network-parameterized SDE. Chen et al. [1], Tzen and Raginsky [23], Kidger [10] focus on the continuous case, while Jia and Benson [7] consider ODEs modulated by compound Poisson jumps. Efficient gradient estimators in high-dimensional settings are crucial for fitting these SDE models.

**Diffusion with Jumps and Stochastic Flow:** The main technical tools for this paper are SDEs with jumps and stochastic flows. Our references are Protter [19], Kunita [11], Oksendal and Sulem [18].

### Remarks on Paper Organizations

The paper is structured as follows: Section 2 outlines the core concepts of our estimator in a zero-jump setting, focusing on intuitive understanding over technical detail. In Section 3, we introduce the SDE model with jumps and provide a set of sufficient conditions that rigorously support the earlier insights. While more general and complex assumptions exist that lead to similar conclusions, these are presented in Appendix A to align with the concise format of the conference proceedings. The paper concludes with Section 4, where we conduct numerical experiments on neural-network-parameterized linear-quadratic control problems, demonstrating the effectiveness of our methodology.

## 2 Key Methodological Insights

In this section, we motivate our proposed generator gradient estimator by first providing a non-rigorous derivation. We assume the SDE model (1.1) where the jumps \(J_{\theta}\equiv 0\) and \(\Theta\subset\mathbb{R}^{n}\) is a bounded open neighbourhood of the origin. W.l.o.g, we are interested in estimating the gradient at \(\theta=0\in\Theta\) and \(t=0\); i.e. \(\nabla_{\theta}v_{0}(0,x)=\nabla_{\theta}v_{\theta}(0,x)|_{\theta=0}\).

To simplify notation, we denote \(X_{\theta}^{x}(t):=X_{\theta}^{x}(0,t)\) and \(X_{\theta}^{x}(t-):=X_{\theta}^{x}(0,t-)\), and the function

\[a_{\theta,i,j}(t,x):=\frac{1}{2}\sum_{k=1}^{d^{\prime}}\sigma_{ \theta,i,k}(t,x)\sigma_{\theta,j,k}(t,x). \tag{2.1}\]

\begin{table}
\begin{tabular}{c c c} \hline \hline \multirow{2}{*}{Estimator} & \multicolumn{2}{c}{If the volatility depends on \(\theta\)} \\  & Yes & No \\ \hline Pathwise Differentiation & \(d+d\cdot n\) & \(d+d\cdot n\) \\ Generator Gradient & \(d+d^{2}+\frac{1}{2}d^{3}\) & \(d+d^{2}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of the dimensions of SDEs needed to be simulated.

Also, for function \(v_{\theta}(t,x)\), we use \(\partial_{i}v_{\theta}(t,x)\) to denote the space derivative \(\frac{\partial_{i}v}{\partial x}\big{|}_{\theta,t,x}\) and \(\nabla\) the space gradient. Similarly, \(\partial_{\theta_{i}}\) and \(\nabla_{\theta}\) denotes the \(\theta\) partials.

Under sufficient regularity conditions, by the Feynman-Kac formula, \(v_{\theta}\) in (1.2) is the solution to the partial differential equation (PDE)

\[\partial_{t}v_{\theta}+\mathcal{L}_{\theta}v_{\theta}+\rho_{\theta}=0,\quad v_ {\theta}(T,\cdot)=g_{\theta} \tag{2.2}\]

for all \(\theta\in\Theta\), where \(\mathcal{L}_{\theta}\) is the _generator_ of \(X^{x}_{\theta}\) given by

\[\mathcal{L}_{\theta}f(t,x):=\sum_{i=1}^{d}\mu_{\theta,i}(t,x)\partial_{i}f(t, x)+\sum_{i,j=1}^{d}a_{\theta,i,j}(t,x)\partial_{i}\partial_{j}f(t,x)\]

for \(f\) that is twice differentiable in \(x\). Assuming enough smoothness, we formally differentiate the PDE (2.2) w.r.t. \(\theta_{i}\) and then set \(\theta=0\) to obtain

\[\partial_{t}\partial_{\theta_{i}}v_{0}+\mathcal{L}_{0}\partial_{\theta_{i}}v_ {0}+(\partial_{\theta_{i}}\mathcal{L}_{0}v_{0}+\partial_{\theta_{i}}\rho_{0}) =0,\quad\partial_{\theta_{i}}v_{0}(T,\cdot)=\partial_{\theta_{i}}g_{0}. \tag{2.3}\]

Here, the operator \(\partial_{\theta_{i}}\mathcal{L}_{0}\) is defined as

\[\partial_{\theta_{i}}\mathcal{L}_{0}f(t,x):=\sum_{j=1}^{d}\partial_{\theta_{i }}\mu_{0,j}(t,x)\partial_{j}f(t,x)+\sum_{j,l=1}^{d}\partial_{\theta_{i}}a_{0,j,l}(t,x)\partial_{j}\partial_{l}f(t,x). \tag{2.4}\]

Interpreted as the derivative of \(\mathcal{L}_{\theta}\) w.r.t. \(\theta\) at \(0\), this inspires the name "generator gradient" method.

Next, define \(u_{0}=\partial_{\theta_{i}}v_{0}\). Treating \(\partial_{\theta_{i}}\mathcal{L}_{0}v_{0}\) as fixed, we observe that \(u_{0}\) solves the PDE (2.3) which is of the form (2.2). Hence, applying the Feynman-Kac formula again to \(\partial_{\theta_{i}}v_{0}(0,x)=u_{0}(0,x)\) yields the following expectation representation

\[\partial_{\theta_{i}}v_{0}(0,x)=E\left[\int_{0}^{T}\partial_{\theta_{i}} \mathcal{L}_{0}v_{0}(t,X^{x}_{0}(t))+\partial_{\theta_{i}}\rho_{0}(t,X^{x}_{0} (t))dt+\partial_{\theta_{i}}g_{0}(X^{x}_{0}(T))\right]. \tag{2.5}\]

Note that the expression inside the expectation contains only space derivatives (due to \(\partial_{\theta_{i}}\mathcal{L}_{0}\)) of the value function \(v_{0}\) but not the \(\theta\) derivatives. In particular, if we can estimate the gradient \(\nabla v_{0}(t,x)\) and the Hessian matrix \(H[v_{0}](t,x):=\{\partial_{i}\partial_{j}v_{0}(t,x):1\leq i,j\leq d\}\) efficiently, then the representation in (2.5) will lead to a natural estimator of \(\partial_{\theta_{i}}v_{0}(0,x)\).

To estimate \(\nabla v_{0}(t,x)\) and \(H[v_{0}](t,x)\), we employ the pathwise differentiation estimator from (1.3). Specifically, under enough regularity conditions, we can interchange the derivatives and integration

\[\nabla v_{0}(t,x)^{\top} =EZ(t,x)^{\top}:=E\left[\int_{t}^{T}\nabla\rho_{0}^{\top}\nabla X ^{x}_{0}(t,r)dr+\nabla g_{0}^{\top}\nabla X^{x}_{0}(t,T)\right], \tag{2.6}\] \[H[v_{0}](t,x) =EH(t,x):=E\left[\nabla X^{x}_{0}(t,T)^{\top}H[g_{0}]\nabla X^{x }_{0}(t,T)+\left\langle\nabla g_{0},H[X^{x}_{0,\cdot}](t,T)\right\rangle\right]\] \[\quad+E\left[\int_{t}^{T}\nabla X^{x}_{0}(t,r)^{\top}H[\rho_{0}] \nabla X^{x}_{0}(t,r)+\left\langle\nabla\rho_{0},H[X^{x}_{0,\cdot}](t,r) \right\rangle dr\right].\]

Here, we write \(\nabla X^{x}_{0}:=\big{\{}\partial_{a}X^{x}_{0,\cdot}:i,a=1,\ldots d\big{\}}\) and \(H[X^{x}_{0}]:=\big{\{}\partial_{b}\partial_{a}X^{x}_{0,\cdot}:i,a,b=1,\ldots d \big{\}}\). The notation \(\left\langle\nabla h,H[X^{x}_{0,\cdot}]\right\rangle:=\sum_{a=1}^{d}\partial_ {a}hH[X^{x}_{0,a}]\in\mathbb{R}^{d\times d}\) for \(h=\rho_{0},g_{0}\). The dependence of \(\rho_{0},g_{0}\) on time and the state process is hidden.

We estimate these expectations by simulating the SDEs for \(\{X^{x}_{0},\nabla X^{x}_{0},H[X^{x}_{0}]\}\) given by (1.1) and

\[\partial_{a}X^{x}_{0,i} =\delta_{i,a}+\int_{t}^{s}\sum_{l=1}^{d}\partial_{l}\mu_{0,i} \partial_{a}X^{x}_{0,l}dr+\int_{t}^{s}\sum_{l=1}^{d}\sum_{k=1}^{d^{\prime}} \partial_{l}\sigma_{0,i,k}\partial_{a}X^{x}_{0,l}dB_{k}(r)\] \[\partial_{b}\partial_{a}X^{x}_{0,i} =\int_{t}^{s}\sum_{l=1}^{d}\left[\partial_{l}\mu_{0,i}\partial_ {b}\partial_{a}X^{x}_{0,l}+\sum_{m=1}^{d}\partial_{m}\partial_{l}\mu_{0,i} \partial_{a}X^{x}_{0,l}\partial_{b}X^{x}_{0,m}\right]dr \tag{2.7}\] \[\quad+\int_{t}^{s}\sum_{k=1}^{d^{\prime}}\sum_{l=1}^{d}\left[ \partial_{l}\sigma_{0,i,k}\partial_{b}\partial_{a}X^{x}_{0,l}+\sum_{m=1}^{d} \partial_{m}\partial_{l}\sigma_{0,i,k}\partial_{a}X^{x}_{0,l}\partial_{b}X^{x}_{0,m}\right]dB_{k}(r)\]where the dependence of the coefficients on \(r\), \(X^{x}_{0}(t,r-)\), and \(z\), as well as the dependence of \(X^{x}_{0},\partial_{a}X^{x}_{0},\partial_{a}\partial_{b}X^{x}_{0}\) on \((t,s),(t,r-)\) are suppressed.

The dimension of these SDEs is \(d+d^{2}+\frac{1}{2}d^{3}\), where the \(\frac{1}{2}\) comes from the Hessian being symmetric. Moreover, when the volatility \(\sigma\) is independent of \(\theta\), our method only necessitates estimating \(\nabla v_{0}\). This reduction leads to simulating the SDEs for \(\{X^{x}_{0},\nabla X^{x}_{0}\}\) of dimension only \(d+d^{2}\).

Assuming sufficient integrability, the unbiasedness of \(Z\) implies

\[E\left[\int_{0}^{T}\partial_{\theta_{k}}\mu_{0}^{\top}Z(t,X^{x}_{0}(0,t))dt \right]=E\left[\int_{0}^{T}\partial_{\theta_{k}}\mu_{0}^{\top}\nabla v_{0}(t, X^{x}_{0}(0,t))dt\right] \tag{2.8}\]

which we will elaborate upon in (A.1). The same holds for the \(H(t,x)\) process as well. Therefore, we can replace the derivatives \(\nabla v_{0}\) with \(Z\) and \(H[v_{0}]\) with \(H\) in (2.5) without changing the expectation.

Also note that producing a sample of \(Z(t,x)\) requires simulating the solution to SDEs (1.1) and (2.7) within time \([t,T]\) starting from \(x,I,0\). So, it is not very efficient to compute \(Z(t,X^{x}_{0}(0,t))\) for every \(t\); a similar issue exists for \(H\) as well. This can be addressed by randomizing the integral.

With these considerations, we proceed to define the generator gradient estimator. First, let \(\nabla_{\theta}L_{0}V_{0}(t,x)\) be defined by replacing \(\partial_{i}v(t,x)\) with \(Z_{i}(t,x)\) and \(\partial_{j}\partial_{i}v\) with \(H_{i,j}(t,x)\) in the definition (2.4) of \(\partial_{\theta_{i}}\mathcal{L}_{0}v_{0}(t,x)\). Then, define the generator gradient estimator as

\[D(x):=T\nabla_{\theta}L_{0}V_{0}(\tau,X^{x}_{0}(0,\tau))+\int_{0}^{T}\nabla_{ \theta}\rho_{0}(t,X^{x}_{0}(t))dt+\nabla_{\theta}g_{0}(X^{x}_{0}(T)). \tag{2.9}\]

where \(\tau\sim\mathrm{Unif}[0,T]\) is sampled independently. We can also randomize the integral of \(\nabla_{\theta}\rho_{0}(t,X^{x}_{\theta}(t))\) if the gradient is hard to compute. With the derivation in (2.8), it is easy to see that \(ED(x)=\nabla_{\theta}v_{0}(0,x)\) is unbiased.

In summary, due to the observation in (2.5), we are able to "move" the estimation of \(\nabla_{\theta}v_{0}\) onto that of \(\nabla v_{0}\) and \(H[v_{0}]\). This results in a significant reduction in the dimension of the SDEs we need to simulate, underlying the remarkable efficiency of our methodology, especially when the dimension \(n\) of \(\theta\) significantly exceeds \(d\).

## 3 Jump Diffusions and the Generator Gradient Estimator

In this section, we rigorously formulate a jump diffusion process driven by an SDE. We extend the generator gradient estimator to this context by first rigorously establishing an expectation representation of the derivative as in (2.5). Then, we also validate the representation (2.6) using the jump version of (2.7). These lead to our generator gradient estimator in the jump diffusion context. To improve the clarity of the paper (at a cost of generalizability), we will state a set of sufficient assumptions that are easy to verify. However, we will state and prove our theorems using a set of more general assumptions in the Appendix A.

We consider jump diffusions on the canonical probability space of cadlag functions \([0,T]\rightarrow\mathbb{R}^{d}\) generated by SDEs of the form (1.1) where the jump term is given by

\[\begin{split} X^{x}_{\theta,i}(t,s)&=x_{i}+\int_{t }^{s}\mu_{\theta,i}(r,X^{x}_{\theta}(t,r))dr+\int_{t}^{s}\sum_{k=1}^{d^{\prime }}\sigma_{\theta,i,k}(r,X^{x}_{\theta}(t,r-))dB_{k}(r)\\ &\quad+\int_{t}^{s}\int_{\mathbb{R}^{d^{\prime}}_{0}}\chi_{\theta, i}(t,X^{x}_{\theta}(s,r-),z)d\widetilde{N}(dr,dz).\end{split} \tag{3.1}\]

In this expression, \(B\) is a standard Brownian motion in \(\mathbb{R}^{d^{\prime}}\); \(\widetilde{N}\) is a compensated Poisson random measure with intensity measure \(dt\times\nu(dz)\) with \(\nu\) a Levy measure on \((\mathbb{R}^{d^{\prime}}_{0}:=\mathbb{R}^{d^{\prime}}\setminus\{0\}\,,\mathcal{ B}(\mathbb{R}^{d^{\prime}}_{0}))\), i.e. \(\int_{\mathbb{R}^{d^{\prime}}_{0}}1\wedge|z|^{2}\nu(dz)<\infty\); the \(-r\) notation in \(X^{x}_{\theta}(t,r-)\) denotes the left limit; and the stochastic integrations are Ito integrals. Here, for a vector/matrix/tensor \(v\in\mathbb{R}^{d_{1}\times d_{2}\times d_{3}}\), we denote \(|v|^{2}:=\sum_{i,j,k}|v_{i,j,k}|^{2}\). We further define \(\gamma(z)=|z|\wedge 1\) and \(\mu(dz)=\gamma(z)^{2}\nu(dz)\). Then \(\mu\) is a finite measure on \((\mathbb{R}^{d^{\prime}}_{0}\,,\mathcal{B}(\mathbb{R}^{d^{\prime}}_{0}))\). Also, since we are interested in the gradient at \(\theta=0\), we can assume w.l.o.g. that \(\Theta\) is a bounded open neighbourhood of \(0\).

The generator of this system of SDEs is \(\mathcal{L}_{\theta}:=\mathcal{L}_{\theta}^{C}+\mathcal{L}_{\theta}^{J}\), where

\[\mathcal{L}_{\theta}^{C}f(t,x) =\sum_{i=1}^{d}\mu_{\theta,i}(t,x)\partial_{i}f(t,x)+\sum_{i,j=1}^{ d}a_{\theta,i,j}(t,x)\partial_{i}\partial_{j}f(t,x) \tag{3.2}\] \[\mathcal{L}_{\theta}^{J}f(t,x) =\int_{\mathbb{R}_{0}^{d^{\prime}}}\left[f(t,x+\chi_{\theta}(t,x, z))-f(t,x)-\sum_{i=1}^{d}\chi_{\theta,i}(t,x,z)\partial_{i}f(t,x)\right]\nu(dz).\]

for \(f\in C^{1,2}([0,T],\mathbb{R}^{d})\). We remark that for open subsets \(\mathbb{W},\mathbb{X}\), the space \(C^{i,j,k}([0,T],\mathbb{W},\mathbb{X})\) represents the set of functions \(f\) on \([0,T]\times\mathbb{W}\times\mathbb{X}\) that has continuous mixed partial derivatives \(\partial_{t}^{a}\partial_{w}^{b}\partial_{c}^{c}f\) on \((0,T)\times\mathbb{W}\times\mathbb{X}\) for every \(a\leq i,b\leq j,c\leq k\). Moreover, these mixed partial derivatives have continuous extensions on \([0,T]\times\mathbb{W}\times\mathbb{X}\).

### Probabilistic Representation of the Gradient

In this section, we rigorously establish the probabilistic representation of the gradient \(\nabla_{\theta}v_{0}(0,x)\) as outlined in equation (2.5). Our approach leverages the continuous dependence of \(\theta\to X_{\theta}^{x}\) of the solutions to (3.1) in a neighbourhood of \(0\), given sufficient regularity conditions. This behavior extends the properties associated with stochastic flows, as explained in the work by Kunita [11].

Recall that \(\Theta\) is a bounded neighbourhood of \(0\in\mathbb{R}^{n}\). To clarify the assumptions, we enlarge \(\Theta\) and consider \(\Theta_{\epsilon}=\{\theta+v:\theta\in\Theta,v\in B^{n}(0,\epsilon)\}\) where \(B^{n}(0,\epsilon)\) is the open ball in \(\mathbb{R}^{n}\) at \(0\) of radius \(\epsilon\).

**Assumption 1**.: _For some \(\epsilon>0\), the following regularity conditions hold_

1. _The mappings_ \((s,\theta,x)\to\mu_{\theta}(s,x),\sigma_{\theta}(s,x),\rho_{\theta}(s,x),g_{ \theta}(s,x)\) _are_ \(C^{0,1,1}([0,T],\Theta_{\epsilon},\mathbb{R}^{d})\)_. For each_ \(z\in\mathbb{R}_{0}^{d^{\prime}}\)_,_ \((s,\theta,x)\to\chi_{\theta}(s,x,z)/\gamma(z)\) _is_ \(C^{0,1,1}([0,T],\Theta_{\epsilon},\mathbb{R}^{d})\)_. Moreover,_ \(|\chi_{\theta}(s,0,z)/\gamma(z)|\) _is uniformly bounded in_ \(s\in[0,T]\) _and_ \(z\in\mathbb{R}_{0}^{d^{\prime}}\)_._
2. _The spacial derivatives_ \(|\nabla\mu_{\theta}|\)_,_ \(|\nabla\sigma_{\theta}|\)_, and_ \(|\nabla\chi_{\theta}|\) _are uniformly bounded. The_ \(\theta\) _derivatives satisfy linear growth_ \[|\nabla_{\theta}\mu_{\theta}(s,x)|+|\nabla_{\theta}\sigma_{\theta}(s,x)|+\left| \frac{\nabla_{\theta}\chi_{\theta}(s,x,z)}{\gamma(z)}\right|\leq\ell(|x|+1)\] _for all_ \(s\in[0,T]\)_,_ \(x\in\mathbb{R}^{d}\)_,_ \(z\in\mathbb{R}^{d^{\prime}}\)_, and_ \(\theta\in\Theta\)_._
3. _The_ \(\theta\) _derivatives of the rewards satisfy polynomial growth: for some_ \(m\geq 1\)_,_ \[|\nabla_{\theta}\rho_{\theta}(s,x)|+|\nabla_{\theta}g_{\theta}(x)|\leq\ell(|x| +1)^{m}\] _for all_ \(s\in[0,T]\)_,_ \(x\in\mathbb{R}^{d}\)_, and_ \(\theta\in\Theta\)_._

_Remark_.: Requirement 1 implies that for each fixed \(x\), the \(\theta\) derivatives of the coefficients are uniformly bounded in \([0,T]\times\Theta\), as \(\Theta\) is assumed to be bounded. So, the seemingly strong requirements of the \(\theta\) derivative satisfying the growth condition in items 2 and 3 are not very restrictive. The boundedness of \(\chi_{\theta}(s,x,z)/\gamma(z)\) in \(z\) is relaxed in Assumption 5 in the appendix, allowing unbounded jumps. The strong condition is the uniform boundedness of \(|\nabla\mu_{\theta}|\), \(|\nabla\sigma_{\theta}|\), and \(|\nabla\chi_{\theta}|\). However, this is typically necessary for the existence and uniqueness of strong solutions to the SDE (3.1).

**Assumption 2**.: _Assume that \(\big{\{}v_{\theta}\in C^{1,2}([0,T],\mathbb{R}^{d}):\theta\in\Theta\big{\}}\) are classical solutions to the partial-integro-differential equations (PIDE)_

\[\partial_{t}v_{\theta}+\mathcal{L}_{\theta}v_{\theta}+\rho_{\theta}=0,\qquad v _{\theta}(T,\cdot)=g_{\theta}\]

_where \(\mathcal{L}_{\theta}=\mathcal{L}_{\theta}^{C}+\mathcal{L}_{\theta}^{J}\) are defined in (3.2). Moreover, \(v_{\theta}\) and its space derivatives satisfy polynomial growth: for each \(\theta\in\Theta\), there exists \(0<c_{\theta}<\infty\) and \(m\geq 1\) s.t._

\[\sup_{x\in\mathbb{R}^{d},t\in[0,T]}\frac{|v_{\theta}(t,x)|}{(|x|+1)^{m}}\leq c _{\theta},\quad\sup_{x\in\mathbb{R}^{d},t\in[0,T]}\frac{|\nabla v_{\theta}(t, x)|}{(|x|+1)^{m}}\leq c_{\theta},\quad\sup_{x\in\mathbb{R}^{d},t\in[0,T]} \frac{|H[v_{\theta}](t,x)|}{(|x|+1)^{m}}\leq c_{\theta}.\]

_Remark_.: By classical solution, we mean that \(v_{\theta}\) satisfies \(\partial_{t}v_{\theta}+\mathcal{L}_{\theta}v_{\theta}+\rho_{\theta}=0\) on \((0,T)\times\mathbb{R}^{d}\) with its continuous extensions of satisfying \(v_{\theta}(T,\cdot)=g_{\theta}\). This is possible, for example, in settings with \(C^{2}\) terminal rewards. Note that is a stronger requirement compared to the definition in Evans [2].

As we have motivated in Section 2, Assumption 2 follows from a generalized version of the Feynman-Kac formula, under additional technical assumptions. Moreover, the growth of \(v_{\theta}\) and its space derivatives can be derived from assumptions on the growth of the rewards. However, in order to not obscure the main message of the paper and to streamline the proof, we directly assume these properties. We refer interested readers to Kunita (Kunita, Chapter 4) where stochastic flow techniques similar to the proofs in the paper are employed to establish the PIDE and validate the growth rates.

**Theorem 1** (Probabilistic Representation of the Gradient).: _If Assumptions 1 and 2 are in force, then \(\theta\to v_{\theta}(0,x)\) is differentiable at \(0\). Moreover, the gradient_

\[\nabla_{\theta}v_{0}(0,x)=E\left[\int_{0}^{T}\nabla_{\theta}\mathcal{L}_{0}v_{ 0}(s,X_{0}^{x}(s))+\nabla_{\theta}\rho_{\theta}(X_{0}^{x}(s))ds+\nabla_{\theta }g_{\theta}(X_{0}^{x}(T))\right],\]

_where \(\nabla_{\theta}\mathcal{L}_{0}:=\nabla_{\theta}\mathcal{L}_{0}^{C}+\nabla_{ \theta}\mathcal{L}_{0}^{J}\) s.t. for \(f(t,x)\in C^{1,2}\),_

\[\nabla_{\theta}\mathcal{L}_{\theta}^{C}f(t,x) =\sum_{i=1}^{d}\nabla_{\theta}\mu_{\theta,i}(t,x)\partial_{i}f(t, x)+\sum_{i,j=1}^{d}\nabla_{\theta}a_{\theta,i,j}(t,x)\partial_{i}\partial_{j }f(t,x), \tag{3.3}\] \[\nabla_{\theta}\mathcal{L}_{\theta}^{J}f(t,x) =\int_{\mathbb{R}_{0}^{d^{\prime}}}\left[\sum_{i=1}^{d}\nabla_{ \theta}\chi_{\theta,i}(t,x,z)\left(\partial_{i}f(t,x+\chi_{\theta}(t,x,z))- \partial_{i}f(t,x)\right)\right]\nu(dz). \tag{3.4}\]

In Theorem 1, we have successfully established an expectation representation of the gradient \(\nabla_{\theta}v_{0}(0,x)\) of the form (2.5). This naturally leads to the consideration of using Monte Carlo to estimate \(\nabla_{\theta}v_{0}(0,x)\). However, one observes that the representation in Theorem 1 involves the space derivatives \(\partial_{i}v_{0}(t,x)\) and \(\partial_{i}\partial_{j}v_{0}(t,x)\), which are usually hard to compute exactly.

In the next section, following the heuristics in (2.6) we establish conditions on the model primitives so that the space derivatives \(\partial_{i}v_{0}(t,x)\) and \(\partial_{i}\partial_{j}v_{0}(t,x)\) admit probabilistic representations as expectations of random processes \(\{X_{0}^{\alpha},\nabla X_{0}^{\alpha},H[X_{0}^{x}]\}\) that can be easily simulated.

### Probabilistic Representation of the Space Derivatives

We proceed with introducing assumptions that guarantee Theorem 2, providing representations of \(\partial_{i}v_{0}(t,x)\) and \(\partial_{i}\partial_{j}v_{0}(t,x)\) as illustrated in (2.6). To achieve this, we first need to ensure that the derivative of the mapping \(x\to X_{0}^{x}\) is well defined. This is formally established in Proposition A.1.

**Assumption 3**.: _For each \(z\in\mathbb{R}_{0}^{d^{\prime}}\), the SDE coefficients \((s,x)\rightarrow(\mu_{0}(s,x),\sigma_{0}(s,x),\chi_{0}(s,x,z))\) are \(C^{0,2}([0,T],\mathbb{R}^{d})\). For each \(i,j=1,\ldots,d\), the coefficients and derivatives, seen as functions \((s,x)\rightarrow(\alpha(s,x),\beta(s,x),\zeta(s,x,\cdot))\) where \((\alpha,\beta,\zeta)=(\mu_{0},\sigma_{0},\chi_{0}/\gamma)\), \((\partial_{i}\mu_{0},\partial_{i}\sigma_{0},\partial_{i}\chi_{0}/\gamma)\), and \((\partial_{j}\partial_{i}\mu_{0},\partial_{j}\partial_{i}\sigma_{0},\partial_{j }\partial_{i}\chi_{0}/\gamma)\) are uniformly Lipschitz; i.e. there exists \(0\leq\ell<\infty\) s.t. for all \(s\in[0,T],z\in\mathbb{R}^{d^{\prime}}\)_

\[|\alpha(s,x)-\alpha(s,x^{\prime})|+|\beta(s,x)-\beta(s,x^{\prime})|+|\zeta(s,x,z)-\zeta(s,x^{\prime},z)|\leq\ell\left|x-x^{\prime}\right|.\]

_Moreover, \(|\zeta(s,0,z)|\) is uniformly bounded for \(s\in[0,T]\) and \(z\in\mathbb{R}_{0}^{d^{\prime}}\)._

In view of this assumption, we consider the following SDEs, as jump versions of (2.7), for which the strong solutions should be the space derivatives of \(X_{0}^{x}\). Again, the dependence of the coefficients on \(r\), \(X_{0}^{x}(t,r-)\), and \(z\), as well as the dependence of \(X_{0}^{x},\partial_{a}X_{0}^{x},\partial_{a}\partial_{b}X_{0}^{x}\) on \((t,s),(t,r-)\) has been suppressed.

\[\partial_{a}X^{x}_{0,i} =\delta_{i,a}+\int_{t}^{s}\sum_{l=1}^{d}\partial_{l}\mu_{0,i} \partial_{a}X^{x}_{0,l}dr+\int_{t}^{s}\sum_{l=1}^{d}\sum_{k=1}^{d^{\prime}} \partial_{l}\sigma_{0,i,k}\partial_{a}X^{x}_{0,l}dB_{k}(r)\] \[\quad+\int_{t}^{s}\sum_{l=1}^{d}\partial_{l}\chi_{0,i}\partial_{a} X^{x}_{0,l}d\widetilde{N}(dr,dz)\] \[\partial_{b}\partial_{a}X^{x}_{0,i} =\int_{t}^{s}\sum_{l=1}^{d}\left[\partial_{l}\mu_{0,i}\partial_{ b}\partial_{a}X^{x}_{0,l}+\sum_{m=1}^{d}\partial_{m}\partial_{l}\mu_{0,i} \partial_{a}X^{x}_{0,l}\partial_{b}X^{x}_{0,m}\right] \tag{3.5}\] \[\quad+\int_{t}^{s}\sum_{k=1}^{d^{\prime}}\sum_{l=1}^{d}\left[ \partial_{l}\sigma_{0,i,k}\partial_{b}\partial_{a}X^{x}_{0,l}+\sum_{m=1}^{d} \partial_{m}\partial_{l}\sigma_{0,i,k}\partial_{a}X^{x}_{0,l}\partial_{b}X^{x} _{0,m}\right]dB_{k}(r)\] \[\quad+\int_{t}^{s}\sum_{l=1}^{d}\left[\partial_{l}\chi_{0,i} \partial_{b}\partial_{a}X^{x}_{0,l}+\sum_{m=1}^{d}\partial_{m}\partial_{l}\chi _{0,i}\partial_{a}X^{x}_{0,l}\partial_{b}X^{x}_{0,m}\right]d\widetilde{N}(dr,dz).\]

As we will show in Proposition A.1, under Assumption 3 the process \(X^{x}_{0}(t,s)\) has a version that is twice continuously differentiable in \(x\) for every \(0\leq t<s\leq T\). The processes \(\{\nabla X^{x}_{0},H[X^{x}_{0}]\}\), as defined in (3.5), will then correspond to the derivatives. Moreover, these processes, as well as \(X^{x}_{0}\), will possess desirable integrability properties.

To guarantee sufficient integrability and to provide a variance bound for our estimator, we also need to assume growth conditions on the rewards.

**Assumption 4**.: _Assume that the mapping \(x\rightarrow\rho_{0}(t,x),g_{0}(x)\) is \(C^{2}\) for all \(t\in[0,T]\). Moreover, for \(h(t,x)=\rho_{0}(t,x)\) and \(g_{0}(x)\) there exists \(c_{h}\) s.t._

\[\sup_{x\in\mathbb{R}^{d},t\in[0,T]}\frac{|h(t,x)|}{(|x|+1)^{m}}\leq c_{h},\quad \sup_{x\in\mathbb{R}^{d},t\in[0,T]}\frac{|\nabla h(t,x)|}{(|x|+1)^{m}}\leq c_{ h},\quad\sup_{x\in\mathbb{R}^{d},t\in[0,T]}\frac{|H[h](t,x)|}{(|x|+1)^{m}}\leq c_{ h}.\]

With these assumptions, we validate the representations in (2.6) using the following theorem.

**Theorem 2** (Probabilistic Representation of the Space Derivatives).: _Under Assumptions 3 and 4, the representations in (2.6) hold with the jump version of \(\{X^{x}_{0},\nabla X^{x}_{0},H[X^{x}_{0}]\}\) in (3.1) and (3.5)._

### The Generator Gradient Estimator

With Theorems 1 and 2, we construct our generator gradient estimator and show that it is unbiased with a variance that grows polynomially in \(x\). Recall the estimators \(Z(t,x)\) and \(H(t,x)\) in (2.6).

By Theorem 2 and the integrability in Proposition A.1 under Assumption 3, the equality (2.8) holds. Then, following the notation in (2.9), we define

\[\nabla_{\theta}L_{0}V_{0}(t,x):=\nabla_{\theta}L_{0}^{C}V_{0}(t,x)+\nabla_{ \theta}L_{0}^{J}V_{0}(t,x)\]

where \(\nabla_{\theta}L_{0}^{C}V_{0}(t,x)\) and \(\nabla_{\theta}L_{0}^{J}V_{0}(t,x)\) are defined by replacing \(\partial_{i}v(t,x)\) with \(Z_{i}(t,x)\) and \(\partial_{j}\partial_{i}v\) with \(H_{i,j}(t,x)\) in (3.3) and (3.4), respectively. Then, our estimator \(D(x)\) is given by (2.9).

**Theorem 3**.: _Suppose Assumptions 1-4 are in force. Then, the generator gradient estimator \(D(x)\) is unbiased; i.e. \(ED(x)=\nabla_{\theta}v_{0}(0,x)\). Moreover, the variance \(\operatorname{Var}(D(x))\leq C(|x|+1)^{2m+4}\) has at most polynomial growth in \(x\), where the constant \(C\) can be dependent on other parameters of the problem but not \(x\)._

_Remark_.: The \(m\) signifies the growth rate of the rewards and their derivatives. The extra additive factor \(2\) in the variance is from the growth of the \(\theta\) derivative of \(a_{0}\), the volatility squared.

## 4 Example: Linear System with Quadratic Loss

In this section, we illustrate some analytical properties and the effectiveness of our estimator by considering a linear quadratic control problem.

Let \(X\in\mathbb{R}^{d}\) be the controlled process, given by the solution to the SDE

\[X^{x}(t)=x+\int_{0}^{t}AX^{x}(s)+BU(t)ds+\int_{0}^{t}CdB(s),\]

where \(B(t)\in\mathbb{R}^{d^{\prime}}\) is a standard Brownian motion, \(U(t)\in\mathbb{R}^{m}\) is the control process that is adapted to the filtration generated by \(X\), \(A\in\mathbb{R}^{d\times d},B\in\mathbb{R}^{d\times m},C\in\mathbb{R}^{d\times d ^{\prime}}\) are non-random matrices. The objective is to choose an admissible control \(U(t)\) that minimizes the quadratic loss

\[E\left[\int_{0}^{T}X^{x}(t)^{\top}QX^{x}(t)+U(t)^{\top}RU(t)dt+X^{x}(T)^{\top} Q_{T}X^{x}(T)\right]\]

where \(Q,Q_{T}\in\mathbb{R}^{d\times d}\) and \(R\in\mathbb{R}^{m\times m}\) are non-random matrices.

In various applications of interests, the admissible control \(U(t)\) is a parameterized function of time and state \(U(t)=u_{\theta}(t,X^{x}_{\theta}(t))\) where the state process under control \(u_{\theta}\) is denoted by \(X^{x}_{\theta}\). The dimension of \(\theta\) could potentially be very high--e.g. when \(u_{\theta}\) is a neural network. To achieve an optimized loss in this over-parameterized setting, one common approach is to run gradient descent. Hence, an efficient gradient estimator that scales well with the dimension \(n\) of \(\theta\) is highly desirable.

We compare the performance of the proposed generator gradient estimator and the pathwise differentiation estimator. In this context, these estimators take the following form. The detailed derivations are presented in Appendix F.1.

**The Generator Gradient Estimator:** In this setting, our generator gradient estimator in (2.9) is

\[D_{i}(x)=T\partial_{\theta_{i}}u_{\theta}(\tau,X^{x}_{\theta}(\tau))^{\top}B^{ \top}Z(\tau,X^{x}_{\theta}(\tau))+Tu_{\theta}(\tau,X^{x}_{\theta}(\tau))^{\top} (R+R^{\top})\partial_{\theta_{i}}u_{\theta}(\tau,X^{x}_{\theta}(\tau))\]

where the definition of \(Z\) follows from (2.6), and is given by (F.1) in Appendix F.1. As explained in (2.9), we also randomize the integral corresponding to the gradient of the reward rate \(\nabla_{\theta}\rho_{0}\).

**The Pathwise Differentiation Estimator:** From (1.3), we find the following IPA estimator that randomizes the time integral

\[\widetilde{D}_{i}(x) =Tu_{\theta}(\tau,X^{x}_{\theta}(\tau))(R+R^{\top})\nabla u_{ \theta}(\tau,X^{x}_{\theta}(\tau))\partial_{\theta_{i}}X^{x}_{\theta}(\tau)+ TX^{x}_{\theta}(\tau)^{\top}(Q+Q^{\top})\partial_{\theta_{i}}X^{x}_{\theta}(\tau)\] \[\quad+Tu_{\theta}(\tau,X^{x}_{\theta}(\tau))^{\top}(R+R^{\top}) \partial_{\theta_{i}}u_{\theta}(\tau,X^{x}_{\theta}(\tau))+X^{x}_{\theta}(T) ^{\top}(Q_{T}+Q^{\top}_{T})\partial_{\theta_{i}}X^{x}_{\theta}(T).\]

Here, the pathwise derivatives \(\partial_{\theta_{i}}X^{x}_{\theta}(t)\) is the solution to (F.3).

We deploy these estimators in an environment where the state variable \(x\in\mathbb{R}^{4}\) represents the x-y positions and velocities of a point mass on a 2D plane. The controller applies a force to this mass. The cost function is designed to encourage the controller to swiftly move the point mass to the origin with minimal force. The force is state-time-dependent and parameterized through a 4-layer fully connected neural network with variable width. All computation times are recorded from a Tesla V100 GPU. Further details about the setup of our numerical experiments can be found in Appendix F.2.

Figure 1: Comparisons of 100-sample estimation statistics and averaged runtime.

In Figure 0(a), we present a comparison of the average runtime for computing a single sample of the generator gradient and the pathwise differentiation estimators \(D(x),\widetilde{D}(x)\in\mathbb{R}^{n}\), across increasing values of \(n\) the dimension of \(\theta\). Our findings indicate that the generator gradient estimator not only outperforms the widely used pathwise differentiation method across all tested values of \(n\) but also surpasses it by more than an order of magnitude for larger values of \(n\). Additionally, the computation time for our estimator shows remarkable stability with respect to increases in \(n\), displaying only a slight uptrend when \(n\gtrsim 10^{7}\).

Figure 0(b) confirms that, at \(n=102\), the estimated values by the two estimators are very similar with high confidence. This confirms that our estimator is consistently estimating the gradient \(\nabla_{\theta}v_{\theta}(0,x)\).

Finally, Table 2 presents the standard errors (SE) (F.4) from 400 replications of both estimators, averaged over the gradient coordinates. It also displayed the averaged ratios of the standard errors (F.5). We observe averaged SE ratios that are consistently less than 1 for all \(n\), suggesting that our generator gradient estimator not only provides significantly faster computations as shown in Figure 0(a) but also achieves lower estimation variances. Further analysis of the SEs for each gradient coordinate is conducted and displayed in Figure 2 in Appendix F.2, highlighting similar histogram shapes and observable reduction in large values of SEs of our estimator.

## 5 Concluding Remarks

The theoretical results in this paper have the limitation of requiring second-order continuous differentiability and uniform boundedness of the space derivatives of the parameters of the underlying jump diffusion. These strong conditions, which are standard in the literature of stochastic flows (cf. [19, 11]) to guarantee global existence and uniqueness of the derivative processes in (3.5), are necessary to achieve the generality of the results presented in this paper.

However, our generator gradient estimator often works even when coefficients are not continuously differentiable. This is true if the generator and rewards gradients are defined almost everywhere, and the derivative processes in (3.5), with almost everywhere derivatives of the SDE parameters, exist for every \(t\in[0,T]\) and satisfy some integrability conditions. Examples include neural networks parameterized stochastic control with ReLU activation functions, heavy-traffic limits of controlled multi-server queues, and the Cox-Ingersoll-Ross (CIR) model. For these models, the existence and integrability of the derivative processes can be checked on a case-by-case basis, allowing the consistency and unbiasedness of the generator gradient estimator to be established. We confirm this by numerically investigating the CIR process and an SDE with ReLU drift in Appendix G.

## Acknowledgments and Disclosure of Funding

The material in this paper is based upon work supported by the Air Force Office of Scientific Research under award number FA9550-20-1-0397. Additional support is gratefully acknowledged from NSF 2118199, 2229012, 2312204, and ONR 13983111.

## References

* [1] R. T. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud. Neural ordinary differential equations. _Advances in Neural Information Processing Systems_, 31, 2018.
* [2] L. C. Evans. _Partial Differential Equations_, volume 19. American Mathematical Society, 2022.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline \(n\) (dimension of \(\theta\)) & 102 & 1002 & 5502 & 21002 & 3.24e5 & 1.29e6 & 5.14e6 & 1.15e7 \\ \hline Avg SE of GG & 5.253 & 5.785 & 3.533 & 1.205 & 0.965 & 0.729 & 0.600 & 0.407 \\ Avg SE of PD & 6.424 & 5.710 & 4.453 & 1.191 & 1.110 & 0.935 & 0.786 & 0.466 \\ Avg SE ratios & 0.971 & 0.932 & 0.946 & 0.903 & 0.902 & 0.914 & 0.926 & 0.961 \\ \hline \hline \end{tabular}
\end{table}
Table 2: 400-sample standard error (SE) comparison between generator gradient (GG) and pathwise differentiation (PD) estimators.

* [3] W. Fang and M. B. Giles. Importance sampling for pathwise sensitivity of stochastic chaotic systems. _SIAM/ASA Journal on Uncertainty Quantification_, 9(3):1217-1241, 2021.
* [4] P. W. Glynn. Optimization of stochastic systems via simulation. In _Proceedings of the 21st Winter Simulation Conference_, pages 90-105, 1989.
* [5] P. W. Glynn. Likelihood ratio gradient estimation for stochastic systems. _Communications of the ACM_, 33(10):75-84, 1990.
* [6] X. Guo, A. Hu, and Y. Zhang. Reinforcement learning for linear-convex models with jumps via stability analysis of feedback controls. _SIAM Journal on Control and Optimization_, 61(2):755-787, 2023.
* [7] J. Jia and A. R. Benson. Neural jump stochastic differential equations. _Advances in Neural Information Processing Systems_, 32, 2019.
* [8] Y. Jia and X. Y. Zhou. Policy gradient and actor-critic learning in continuous time and space: Theory and algorithms. _Journal of Machine Learning Research_, 23(275):1-50, 2022.
* [9] I. Karatzas, S. E. Shreve, I. Karatzas, and S. E. Shreve. _Methods of Mathematical Finance_, volume 39. Springer, 1998.
* [10] P. Kidger. On neural differential equations. _arXiv preprint arXiv:2202.02435_, 2022.
* [11] H. Kunita. _Stochastic Flows and Jump-Diffusions_. Springer, 2019.
* [12] X. Li, T.-K. L. Wong, R. T. Q. Chen, and D. Duvenaud. Scalable gradients for stochastic differential equations. In S. Chiappa and R. Calandra, editors, _Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics_, volume 108 of _Proceedings of Machine Learning Research_, pages 3870-3882. PMLR, 26-28 Aug 2020.
* [13] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep reinforcement learning. _arXiv preprint arXiv:1509.02971_, 2015.
* [14] D. Lipshutz and K. Ramanan. A Monte Carlo method for estimating sensitivities of reflected diffusions in convex polyhedral domains. _Stochastic Systems_, 9(2):101-140, 2019.
* [15] D. B. Madan, P. P. Carr, and E. C. Chang. The variance gamma process and option pricing. _Review of Finance_, 2(1):79-105, 1998.
* [16] S. Massaroli, M. Poli, S. Peluchetti, J. Park, A. Yamashita, and H. Asama. Learning stochastic optimal policies via gradient descent. _IEEE Control Systems Letters_, 6:1094-1099, 2021.
* [17] R. C. Merton. Option pricing when underlying stock returns are discontinuous. _Journal of Financial Economics_, 3(1):125-144, 1976. ISSN 0304-405X.
* [18] B. Oksendal and A. Sulem. _Applied Stochastic Control of Jump Diffusions_. Springer, 2019.
* [19] P. Protter. _Stochastic Integration and Differential Equations_. Springer-Verlag, Berlin, Heidelberg, second edition, 1992.
* [20] J. Sirignano, J. MacArt, and K. Spiliopoulos. Pde-constrained models with neural network terms: Optimization and global convergence. _Journal of Computational Physics_, 481:112016, 2023.
* [21] R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. _Advances in Neural Information Processing Systems_, 12, 1999.
* [22] F. Troltzsch. _Optimal Control of Partial Differential Equations: Theory, Methods, and Applications_, volume 112. American Mathematical Soc., 2010.
* [23] B. Tzen and M. Raginsky. Neural stochastic differential equations: deep latent Gaussian models in the diffusion limit. _arXiv preprint arXiv:1905.09883_, 2019.

* [24] Z. Wang and J. Sirignano. A forward propagation algorithm for online optimization of nonlinear stochastic differential equations. _arXiv preprint arXiv:2207.04496_, 2022.
* [25] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. _Machine Learning_, 8:229-256, 1992.
* [26] J. Yang and H. J. Kushner. A Monte Carlo method for sensitivity analysis and parametric optimization of nonlinear stochastic systems. _SIAM Journal on Control and Optimization_, 29(5):1216-1249, 1991.

## Appendix A Generalizations of the Assumptions

### Probabilistic Representation of the Gradient

In this section, we develop a generalized version of Theorem 1, weakening Assumptions 1 and 2. In particular, we allow discontinuities in time of the SDE coefficients. This flexibility is especially relevant in applications in data-driven decision-making environments where non-homogeneous SDE models with estimated drift, volatility, and jump parameters could be piece-wise constant. Moreover, we also relax the differentiability of the coefficients in the space variable to Lipschitz continuity. We will state the new set of assumptions, and establish a generalized version of Theorem 1 as in Theorem 1'

We proceed by presenting a critical theorem, along with the necessary assumptions, that forms the foundation of our probabilistic representation in Theorem 1'.

**Assumption 5**.: _Assume that for each \(\theta\) the coefficients \(\mu_{\theta}(\cdot,\cdot)\), \(\sigma_{\theta}(\cdot,\cdot)\), and \(\chi_{\theta}(\cdot,\cdot,\cdot)\) are jointly Borel measurable. Moreover, assume the following holds true:_

1. _At_ \(x=0\)_, the coefficients are bounded: for all_ \(p\geq 2\)_,_ \[\sup_{\theta\in\Theta,s\in[0,T]}\left[\left|\mu_{\theta}(s,0)\right|+\left| \sigma_{\theta}(s,0)\right|+\int_{\mathbb{R}^{d^{\prime}}_{0}}\left|\frac{ \chi_{\theta}(s,0,z)}{\gamma(z)}\right|^{p}\mu(dz)\right]<\infty\]
2. _The coefficients are uniform Lipschitz in_ \(x\)_, uniformly in_ \(s,\theta\) _in the following sense: there exists constants_ \(c\) _and_ \(\{c_{p}:p\geq 2\}\) _s.t._ \[\left|\mu_{\theta}(s,x)-\mu_{\theta}(s,x^{\prime})\right|\leq c\left|x-x^{ \prime}\right|,\quad\left|\sigma_{\theta}(s,x)-\sigma_{\theta}(s,x^{\prime}) \right|\leq c\left|x-x^{\prime}\right|,\] _and for any_ \(p\geq 2\)__ \[\left(\int_{\mathbb{R}^{d^{\prime}}_{0}}\left|\frac{\chi_{\theta}(s,x,z)}{ \gamma(z)}-\frac{\chi_{\theta}(s,x^{\prime},z)}{\gamma(z)}\right|^{p}\mu(dz) \right)^{\frac{1}{p}}\leq c_{p}|x-x^{\prime}|\] _for all_ \(s\in[0,T]\)_,_ \(x,x^{\prime}\in\mathbb{R}^{d}\)_,_ \(\theta\in\Theta\)_._
3. _The coefficients are weakly Lipschitz in_ \(\theta\) _the following sense: for each_ \(p\geq 2\)_, there exists a time-dependent positive field_ \(\left\{\kappa^{p}_{\theta,\theta^{\prime}}(s)\in\mathbb{R}_{>0}:s\in[0,T], \theta,\theta^{\prime}\in\Theta\right\}\) _s.t. for some constant_ \(\ell_{p}\)_,_ \[\left(\int_{0}^{T}\kappa^{p}_{\theta,\theta^{\prime}}(s)ds\right)^{\frac{1}{p }}\leq\ell_{p}\left|\theta-\theta^{\prime}\right|\] _for all_ \(\theta,\theta^{\prime}\in\Theta\)_, and the coefficients satisfy_ \[\left|\mu_{\theta}(s,x)-\mu_{\theta^{\prime}}(s,x)\right|^{p}\leq\kappa^{p}_{ \theta,\theta^{\prime}}(s)(|x|+1)^{p},\quad\left|\sigma_{\theta}(s,x)-\sigma_{ \theta^{\prime}}(s,x)\right|^{p}\leq\kappa^{p}_{\theta,\theta^{\prime}}(s)(|x |+1)^{p},\] _and_ \[\int_{\mathbb{R}^{d^{\prime}}_{0}}\left|\frac{\chi_{\theta}(s,x,z)}{\gamma(z )}-\frac{\chi_{\theta^{\prime}}(s,x,z)}{\gamma(z)}\right|^{p}\mu(dz)\leq\kappa ^{p}_{\theta,\theta^{\prime}}(s)(|x|+1)^{p}\] _for all_ \(s\in[0,T]\)_,_ \(\theta,\theta^{\prime}\in\Theta\)_,_ \(x\in\mathbb{R}^{d}\)_._

**Theorem K** (Theorem 3.3.1 of Kunita [11]).: _If Assumption 5 is in force, then the family of solutions \(X^{x}:=\left\{X^{x}_{\theta}(t):t\in[0,T],\theta\in\Theta\right\}\) has a version \(\dot{X}^{x}\) (i.e. \(\left\{\exists t\in[0,T],\theta\in\Theta:X^{x}_{\theta}(t)\neq\dot{X}^{x}_{ \theta}(t)\right\}\subset N\) with \(P(N)=0\)) that is \(\mathcal{B}([0,T])\times\mathcal{B}(\Theta)\times\mathcal{F}\) measurable. Moreover, w.p.1, for each \(\theta\in\Theta\), \(X^{x}_{\theta}(\omega,t)\) is cadlag in \(t\), and \(\theta\to\dot{X}^{x}_{\theta}(\omega,\cdot)\) seen as a mapping \(\Theta\to(D[0,T],\|\cdot\|_{\infty})\) is uniformly continuous on compacts. Furthermore, for any \(p\geq 2\) there exists \(b_{p}\in(0,\infty)\) s.t._

\[\sup_{\theta\in\Theta}E\sup_{t\in[0,T]}|X^{x}_{\theta}(t)|^{p}\leq b^{p}_{p}( |x|+1)^{p}.\]_Remark_.: Theorem K is an extension of Kunita [11, Theorem 3.3.1] using the a.s. version of Kolmogorov's continuity criterion; see Corollary 1 of Protter [19, Theorem 73].

To guarantee that Theorem 1' holds, the requirement that Assumption 5 holds for all \(p\geq 2\) can be relaxed to all \(p\leq n+\epsilon\) where \(n\) is the dimension of \(\theta\). However, the intended application of our theory focuses on a regime where \(n\gg d\). So, we adopted this stronger version of Assumption 5. This also clarifies the presentations of the following assumptions: To guarantee the main results of this paper, a weaker version of this assumption requires that the assumption holds for all \(p\leq 4m+\epsilon\), where \(m\) is the growth rate of \(v,r,g\), and their derivatives as in Assumption 8 and 7.

Next, we present additional regularities that implies the probabilistic representation in Theorem 1'.

**Assumption 6**.: _For each \(s\in[0,T]\) and \(z\in\mathbb{R}_{0}^{d^{\prime}}\), the mappings \((\theta,x)\rightarrow\mu_{\theta}(s,x),\sigma_{\theta}(s,x),\chi_{\theta}(s,x,z),\rho_{\theta}(s,x),g_{\theta}(s,x)\) are \(C^{1,0}(\Theta,\mathbb{R}^{d})\)._

**Assumption 7**.: _The measurable reward rate \(\rho_{\theta}\) and terminal reward \(g_{\theta}\) functions are Lipschitz in \(\theta\) in the following sense:_

1. _There exist_ \(m\geq 1\)_,_ \(\alpha>1\)_, and_ \(\left\{\kappa_{\theta,\theta^{\prime}}^{\alpha}(s)\in\mathbb{R}_{>0}:s\in[0,T],\theta,\theta^{\prime}\in\Theta\right\}\) _s.t._ \[\left(\int_{0}^{T}\kappa_{\theta,\theta^{\prime}}^{\alpha}(s)ds\right)^{\frac{ 1}{\alpha}}\leq\ell_{\alpha}\left|\theta-\theta^{\prime}\right|\] _for all_ \(\theta,\theta^{\prime}\in\Theta\)_, and the reward rate satisfies_ \[|\rho_{\theta}(s,x)-\rho_{\theta^{\prime}}(s,x)|^{\alpha}\leq\kappa_{\theta, \theta^{\prime}}^{\alpha}(s)(|x|+1)^{m\alpha}\] _for all_ \(s\in[0,T]\)_,_ \(\theta,\theta^{\prime}\in\Theta\)_,_ \(x\in\mathbb{R}^{d}\)_._
2. _For some_ \(\ell\geq 0\)_, the terminal reward satisfies_ \[|g_{\theta}(x)-g_{\theta^{\prime}}(x)|\leq\ell|\theta-\theta^{\prime}|(|x|+1)^ {m}.\] _for all_ \(s\in[0,T]\)_,_ \(\theta,\theta^{\prime}\in\Theta\)_,_ \(x\in\mathbb{R}^{d}\)_._

Note that for notation simplicity, w.l.o.g. we use the same \(\kappa_{\theta,\theta^{\prime}}^{\alpha}\) and \(\ell_{\alpha}\) as in part 3 of Assumption 5 to denote the Lipschitz coefficient, and the same \(m\) as in Assumption 9.

_Remark_.: It is not hard to see that Assumptions 6 along with 5 and 7 are generalization of Assumption 1; i.e. if Assumption 1 holds then so will Assumptions 6, 5, and 7.

Next, we slightly generalize the growth part in Assumption 2 as in Assumption 9.

**Assumption 8**.: _Assume that \(\left\{v_{\theta}\in C^{1,2}([0,T],\mathbb{R}^{d}):\theta\in\Theta\right\}\) is a family of classical solution to the PIDEs_

\[\partial_{t}v_{\theta}+\mathcal{L}_{\theta}v_{\theta}+\rho_{\theta} =0\] \[v_{\theta}(T,\cdot) =g_{\theta}\]

_where \(\mathcal{L}_{\theta}=\mathcal{L}_{\theta}^{C}+\mathcal{L}_{\theta}^{J}\) as defined in (3.2)._

**Assumption 9**.: _There exists \(0<c_{v}<\infty\) and \(m\geq 1\) s.t._

\[\sup_{x\in\mathbb{R}^{d},t\in[0,T]}\frac{|v_{0}(t,x)|}{(|x|+1)^{m}}\leq c_{v},\quad\sup_{x\in\mathbb{R}^{d},t\in[0,T]}\frac{|\nabla v_{0}(t,x)|}{(|x|+1)^{m }}\leq c_{v},\quad\text{and}\sup_{x\in\mathbb{R}^{d},t\in[0,T]}\frac{|H[v_{0} ](t,x)|}{(|x|+1)^{m}}\leq c_{v}.\]

_Moreover, for each \(\theta\), there exists \(c_{\theta,v}\) s.t._

\[\sup_{x\in\mathbb{R}^{d},t\in[0,T]}\frac{|\nabla v_{\theta}(t,x)|}{(|x|+1)^{m }}\leq c_{\theta,v}\]

**Theorem 1'**.: _If Assumptions 5, 6, 7, 8, and 9 are in force, then the statement in Theorem 1 hold; i.e._

\[\nabla_{\theta}v_{0}(0,x)=E\left[\int_{0}^{T}\nabla_{\theta}\mathcal{L}_{0}v_{ 0}(s,X_{0}^{x}(s))+\nabla_{\theta}\rho_{\theta}(X_{0}^{x}(s))ds+\nabla_{\theta }g_{\theta}(X_{0}^{x}(T))\right]\]

_where \(\nabla_{\theta}\mathcal{L}_{0}:=\nabla_{\theta}\mathcal{L}_{0}^{C}+\nabla_{ \theta}\mathcal{L}_{0}^{J}\) are define in (3.3) and (3.4), respectively._

### Probabilistic Representation of the Space Derivatives

Following the same spirit, in this section, we develop a generalized version of Theorem 2, weakening the Assumption 3 to the following Assumption:

**Assumption 10**.: _For each \(s\in[0,T],z\in\mathbb{R}_{0}^{d^{\prime}}\), the coefficients \(x\to(\mu_{0}(s,x),\sigma_{0}(s,x),\chi_{0}(s,x,z))\) is second continuously differentiable. Moreover, for each \(i,j=1,\ldots,d\), the coefficients and derivatives, seen as functions \((s,x)\to(\alpha(s,x),\beta(s,x),\zeta(s,x,\cdot))\) where \((\alpha,\beta,\zeta)=(\mu_{0},\sigma_{0},\chi_{0})\), \((\partial_{i}\mu_{0},\partial_{i}\sigma_{0},\partial_{i}\chi_{0})\), and \((\partial_{j}\partial_{i}\mu_{0},\partial_{j}\partial_{i}\sigma_{0},\partial_{ j}\partial_{i}\chi_{0})\) satisfies the following conditions:_

1. _At_ \(x=0\)_, the coefficients are uniformly bounded in time: for all_ \(p\geq 2\)_,_ \[\sup_{s\in[0,T]}\left[\left|\alpha(s,0)\right|+\left|\beta(s,0)\right|+\int_{ \mathbb{R}_{0}^{d^{\prime}}}\left|\frac{\zeta(s,0,z)}{\gamma(z)}\right|^{p} \mu(dz)\right]<\infty\]
2. _The coefficients are uniform Lipschitz in_ \(x\)_: There exists constants_ \(c\) _and_ \(\{c_{p}:p\geq 2\}\) _s.t._ \[\left|\alpha(s,x)-\alpha(s,x^{\prime})\right|\leq c\left|x-x^{\prime}\right|, \quad\left|\beta(s,x)-\beta(s,x^{\prime})\right|\leq c\left|x-x^{\prime}\right|,\] _and for any_ \(p\geq 2\)__ \[\left(\int_{\mathbb{R}_{0}^{d^{\prime}}}\left|\frac{\zeta(s,x,z)}{\gamma(z)}- \frac{\zeta(s,x^{\prime},z)}{\gamma(z)}\right|^{p}\mu(dz)\right)^{\frac{1}{p}} \leq c_{p}|x-x^{\prime}|\] _for all_ \(s\in[0,T],\,x,x^{\prime}\in\mathbb{R}^{d}\)_._

**Proposition A.1**.: _Suppose that Assumption 10 is in force. Then, the family of stochastic flow solutions \(\left\{X_{0}^{x}(s,t),0\leq s\leq t\leq T:x\in\mathbb{R}^{d}\right\}\) of the SDEs (3.1) has a version \(\hat{X}_{0}\) that is second differentiable in \(x\) at any time. Moreover, \(\left\{\hat{X}_{0}^{x},\nabla\hat{X}_{0}^{x},H[\hat{X}_{0}^{x}]:x\in\mathbb{R} ^{d}\right\}\) is a version of the solutions of the systems of SDEs (3.1) and (3.5). Further, the family of solutions of (3.1) and (3.5) satisfies the following properties:_

1. _For each_ \(p\geq 1\)_, there is_ \(0<b_{p}<\infty\) _s.t._ \[E\sup_{t\in[0,T]}|X_{0}^{x}(t)|^{p}\leq b_{p}^{p}(|x|+1)^{p}\] _and the derivatives_ \[\sup_{x\in\mathbb{R}^{d}}E\sup_{0\leq s\leq t\leq T}|\nabla X_{0}^{x}(s,t)|^{ p}\leq b_{p}^{p},\qquad\sup_{x\in\mathbb{R}^{d}}E\sup_{0\leq s\leq t\leq T}|H[X _{0}^{x}](s,t)|^{p}\leq b_{p}^{p}.\]
2. _For any_ \(p\geq 1\)_, there exists_ \(0<l_{p}<\infty\) _s.t. for all_ \(x,x^{\prime}\in\mathbb{R}^{d}\)_,_ \[E\sup_{0\leq s\leq t\leq T}|X_{0}^{x}(s,t)-X_{0}^{x^{\prime}}(s,t)|^{p}\leq l _{p}^{p}|x-x^{\prime}|^{p},\] \[E\sup_{0\leq s\leq t\leq T}|\nabla X_{0}^{x}(s,t)-\nabla X_{0}^{x^ {\prime}}(s,t)|^{p}\leq l_{p}^{p}|x-x^{\prime}|^{p}.\]

A proof of Proposition A.1 is provided in Appendix C.

_Remark_.: We observe that part 2 of Assumption 10 will imply the space derivatives of the coefficients are bounded, which is used to get the \(L^{p}\) boundedness and Lipschitzness of the derivative processes. Assumption 10 also ensures that the second derivative is uniformly Lipschitz as well. This is not used in the proof for the upcoming results.

We establish the following Theorem 2' generalizing 2. The proof is deferred to Appendix D.

**Theorem 2'**.: _Under Assumptions 10 and 4, then (2.6) holds; i.e._

\[\nabla v_{0}(t,x)^{\top} =E\left[\int_{t}^{T}\nabla\rho_{0}^{\top}\nabla X_{0}^{x}(t,r)dr+ \nabla g_{0}^{\top}\nabla X_{0}^{x}(t,T)\right],\] \[H[v_{0}](t,x) =E\left[\int_{t}^{T}\nabla X_{0}^{x}(t,r)^{\top}H[\rho_{0}] \nabla X_{0}^{x}(t,r)+\left\langle\nabla\rho_{0},H[X_{0,\cdot}^{x}](t,r) \right\rangle dr\right]\] \[\qquad+E\left[\nabla X_{0}^{x}(t,T)^{\top}H[g_{0}]\nabla X_{0}^ {x}(t,T)+\left\langle\nabla g_{0},H[X_{0,\cdot}^{x}](t,T)\right\rangle\right].\]

_where \(\{X_{0}^{x},\nabla X_{0}^{x},H[X_{0}^{x}]\}\) are the strong solutions to (3.1) and (3.5)._

### The Estimator

With Proposition A.1 and Theorem 2', we are ready to define our generator gradient estimator for \(\nabla_{\theta}v_{0}(0,x)\) and show that it is unbiased and has a variance that grows polynomially in \(x\).

First, recall the definition of \(Z\) and \(H\) in (2.6)

\[Z(t,x)^{\top} :=\int_{t}^{T}\nabla\rho_{0}^{\top}\nabla X_{0}^{x}(t,r)dr+\nabla g _{0}^{\top}\nabla X_{0}^{x}(t,T),\] (B.1) \[H(t,x) :=\int_{t}^{T}\nabla X_{0}^{x}(t,r)^{\top}H[\rho_{0}]\nabla X_{0} ^{x}(t,r)+\left\langle\nabla\rho_{0},H[X_{0,\cdot}^{x}](t,r)\right\rangle dr\] \[\quad+\nabla X_{0}^{x}(t,T)^{\top}H[g_{0}]\nabla X_{0}^{x}(t,T)+ \left\langle\nabla g_{0},H[X_{0,\cdot}^{x}](t,T)\right\rangle.\]

Observe that by Theorem 2', the integrability in Proposition A.1,

\[E \int_{0}^{T}\partial_{\theta_{k}}\mu_{0}(t,X_{0}^{x}(0,t))^{\top }Z(t,X_{0}^{x}(0,t))dt\] (A.1) \[=\int_{0}^{T}dtE\left[\partial_{\theta_{k}}\mu_{0}(t,X_{0}^{x}(0, t))^{\top}E\left[\int_{t}^{T}\nabla\rho_{0}^{\top}\nabla X_{0}^{X_{0}^{x}(0,t)} (t,r)dr+\nabla g_{0}^{\top}\nabla X_{0}^{X_{0}^{x}(0,t)}(t,T)\Bigg{|}X_{0}^{x} (0,t)\right]\right]\] \[=E\int_{0}^{T}\partial_{\theta_{k}}\mu_{0}(t,X_{0}^{x}(0,t))^{ \top}\nabla v_{0}(t,X_{0}^{x}(0,t))dt;\]

a similar property hold for the \(H(t,x)\) process as well. Therefore, we can replace the derivatives \(\nabla v_{0}\) with \(Z\) and \(H[v_{0}]\) with \(H\) in Theorem 1' without changing the expectation, showing the validity of (2.8). In particular, this implies that the generator gradient estimator defined in (2.9) is unbiased.

Finally, we establish a generalized version Theorem 3 using the assumptions in this section. The additional proof of this theorem is presented in Appendix E.

**Theorem 3'**.: _Suppose Assumptions 4-10 are in force. Then, the generator gradient estimator (2.9) is unbiased; i.e. \(ED(x)=\nabla_{\theta}v_{0}(0,x)\). Moreover, if the \(\alpha>1\) in item 1 of Assumption 7 is replaced by \(\alpha>2\), then the variance \(\operatorname{Var}(D(x))\leq C(|x|+1)^{2m+4}\) has at most polynomial growth in \(x\), where \(C\) can be dependent on other parameters of the problem but not \(x\)._

_Remark._ The \(m\) signifies the growth rate of the rewards and their derivatives. The extra additive factor \(2\) in the variance is from the growth of the \(\theta\) derivative of \(a_{0}\), the volatility squared.

## Appendix B Proof of Theorem 1'

In this section, we prove Theorem 1' and hence the simplified Theorem 1.

Proof of Theorem 1'.: First, we recall Ito's formula. For \(f\in C^{1,2}([0,T]\times\mathbb{R}^{d})\),

\[df(t,X_{\theta}^{x}(t)) =\left[\partial_{t}f(t,X_{\theta}^{x}(t))+(\mathcal{L}_{\theta}^ {C}f)(t,X_{\theta}^{x}(t))\right]dt+\sum_{i=1}^{d}\sum_{k=1}^{d^{\prime}} \partial_{i}f(t,X_{\theta}^{x}(t-))\sigma_{\theta,i,k}(t,X_{\theta}^{x}(t))dB _{k}(t)\] (B.2) \[\quad+(\mathcal{L}_{\theta}^{J}f)(t,X_{\theta}^{x}(t))dt+\int_{ \mathbb{R}_{0}^{d^{\prime}}}\left[f(t,X_{\theta}^{x}(t-)+\chi_{\theta}(t,X_{ \theta}^{x}(t-),z))-f(t,X_{\theta}^{x}(t-))\right]d\widetilde{N}(dt,dz),\]

where the operators \(\mathcal{L}_{\theta}^{C}\) and \(\mathcal{L}_{\theta}^{J}\) are defined in (3.2).

Then, an application of Ito's formula (B.1) under Assumption 8 and 9 yields the following result for which the proof is presented in Section B.1.

**Lemma 1**.: _For any \(\theta\in\Theta\),_

\[M_{\theta,\theta}(t) =v_{\theta}(t,X_{\theta}^{x}(t))-v_{\theta}(0,x)+\int_{0}^{t} \rho_{\theta}(s,X_{\theta}^{x}(s))ds\] (B.3) \[M_{0,\theta}(t) =v_{0}(t,X_{\theta}^{x}(t))-v_{0}(0,x)-\int_{0}^{t}\partial_{s}v _{0}(s,X_{\theta}^{x}(s))+\mathcal{L}_{\theta}v_{0}(s,X_{\theta}^{x}(s))ds\] (B.4)

_are martingales for \(0\leq t\leq T\)._Therefore,

\[0=E[M_{\theta,\theta}(T)-M_{0,\theta}(T)].\]

Then, rearranging terms, one gets

\[v_{\theta}(0,x)-v_{0}(0,x)\] (B.2) \[=Ev_{\theta}(T,X_{\theta}^{x}(T))-v_{0}(T,X_{\theta}^{x}(T))+\int_ {0}^{T}\rho_{\theta}(s,X_{\theta}^{x}(s))\pm\rho_{0}(s,X_{\theta}^{x}(s))+ \partial_{s}v_{0}(s,X_{\theta}^{x}(s))+\mathcal{L}_{\theta}v_{0}(s,X_{\theta}^ {x}(s))ds\] \[\stackrel{{(i)}}{{=}}Eg_{\theta}(X_{\theta}^{x}(T))-g _{0}(X_{\theta}^{x}(T))+\int_{0}^{T}\rho_{\theta}(s,X_{\theta}^{x}(s))-\rho_{0} (s,X_{\theta}^{x}(s))+\mathcal{L}_{\theta}v_{0}(s,X_{\theta}^{x}(s))-\mathcal{ L}_{0}v_{0}(s,X_{\theta}^{x}(s))ds\] \[\stackrel{{(ii)}}{{=}}E\int_{0}^{T}\left(\mathcal{L} _{\theta}^{C}-\mathcal{L}_{0}^{C}\right)v_{0}(s,X_{\theta}^{x}(s))ds+\int_{0 }^{T}\left(\mathcal{L}_{\theta}^{J}-\mathcal{L}_{0}^{J}\right)v_{0}(s,X_{ \theta}^{x}(s))ds\] \[\quad+Eg_{\theta}(X_{\theta}^{x}(T))-g_{0}(X_{\theta}^{x}(T))+ \int_{0}^{T}\rho_{\theta}(s,X_{\theta}^{x}(s))-\rho_{0}(s,X_{\theta}^{x}(s))ds\]

where \((i)\) follows from Assumption 8 that \(v_{\theta}(T,\cdot)=g_{\theta}(\cdot)\) and \(\rho_{0}(s,x)=-\partial_{s}v_{0}(s,x)-\mathcal{L}_{0}v_{0}(s,x)\) for all \(x\in\mathbb{R}^{d}\) and \(0\leq s<T\), and \((ii)\) recalls the definition that \(\mathcal{L}_{\theta}=\mathcal{L}_{\theta}^{C}+\mathcal{L}_{\theta}^{J}\).

To conclude Theorem 1', we analyze the finite difference approximations of the above three expectations, where they correspond to the **continuous**, the **jump**, and the **rewards** part, respectively. The results are summarized by the following Proposition B.1, whose proof is deferred to Appendix B.2.

**Proposition B.1**.: _Under the assumptions of Theorem 1', for \(K=C,J\),_

\[\lim_{\theta\to 0}\frac{1}{|\theta|}\left|E\left[\int_{0}^{T}\left(\mathcal{L} _{\theta}^{K}-\mathcal{L}_{0}^{K}\right)v_{0}(s,X_{\theta}^{x}(s))ds\right]- \theta^{T}E\left[\int_{0}^{T}\nabla_{\theta}\mathcal{L}_{0}^{K}v_{0}(s,X_{0}^{ x}(s))ds\right]\right|=0.\]

_where \(\nabla_{\theta}\mathcal{L}_{0}^{C}\) and \(\nabla_{\theta}\mathcal{L}_{0}^{J}\) are defined in (3.3) and (3.4) respectively. Moreover,_

\[\lim_{\theta\to 0}\frac{1}{|\theta|}\left|E\left[\int_{0}^{T}\rho_{\theta}(s,X_{ \theta}^{x}(s))-\rho_{0}(s,X_{\theta}^{x}(s))ds\right]-\theta^{T}E\left[\int_ {0}^{T}\nabla_{\theta}\rho_{0}(s,X_{0}^{x}(s))ds\right]\right|=0\]

_and_

\[\lim_{\theta\to 0}\frac{1}{|\theta|}\left|E\left[g_{\theta}(X_{\theta}^{x}(T))-g _{0}(X_{\theta}^{x}(T))\right]-\theta^{T}E\nabla_{\theta}g_{0}(X_{0}^{x}(T)) \right|=0.\]

With Proposition B.1 handling each term in (B.2), we conclude that

\[\lim_{\theta\to 0}\frac{1}{|\theta|}\left|v_{\theta}(0,x)-v_{0}(0,x)-\theta^{T}E \left[\int_{0}^{T}\nabla_{\theta}\mathcal{L}_{0}v_{0}(s,X_{0}^{x}(s))+\nabla_{ \theta}\rho_{0}(X_{0}^{x}(s))ds+\nabla_{\theta}g_{0}(X_{0}^{x}(T))\right]\right|=0\]

### Proof of Lemma 1

Apply Ito's formula (B.1) to \(v_{\theta}(t,X_{\theta}^{x}(t))\) yield

\[0 =v_{\theta}(t,X_{\theta}^{x}(t))-v_{\theta}(0,x)-\int_{0}^{t} \partial_{s}v_{\theta}(t,X_{\theta}^{x}(s))-\mathcal{L}_{\theta}v_{\theta}(t,X _{\theta}^{x}(s))dt\] \[\quad-\sum_{i=1}^{d}\sum_{k=1}^{d^{\prime}}\int_{0}^{t}\partial_{ i}v_{\theta}(s,X_{\theta}^{x}(s-))\sigma_{\theta,i,k}(s,X_{\theta}^{x}(s-))dB_{k}(t)\] \[\quad-\int_{0}^{t}\int_{\mathbb{R}_{0}^{d^{\prime}}}\left[v_{ \theta}(s,X_{\theta}^{x}(s-)+\chi_{\theta}(s,X_{\theta}^{x}(s-),z))-v_{\theta }(s,X_{\theta}^{x}(s-))\right]d\widetilde{N}(ds,dz)\] \[=:M_{\theta,\theta}(t)-I_{1}(t)-I_{2}(t)\]by Assumption 8, where \(I_{1}(t)\) and \(I_{2}(t)\) denotes the Ito stochastic integrals on the previous lines, respectively. Since the integrands are a.s. finite, \(I_{1}(t)\) and \(I_{2}(t)\) are local martingales. We show that they are true martingales. First, for \(I_{1}\), apply the Burkholder-Davis-Gundy inequality

\[E|I_{1}(t)|^{2} \leq E\sup_{t\leq T}|I_{1}(t)|^{2}\] \[\leq C\sum_{i=1}^{d}\sum_{k=1}^{d^{\prime}}E\int_{0}^{T}\left| \partial_{i}v_{\theta}(s,X_{\theta}^{x}(s))\sigma_{\theta,i,k}(s,X_{\theta}^ {x}(s))\right|^{2}ds\] \[\leq CE\int_{0}^{T}\left|\nabla v_{\theta}(s,X_{\theta}^{x}(s)) \right|^{2}\left|\sigma_{\theta}(s,X_{\theta}^{x}(s))\right|^{2}ds\] \[\leq CE\int_{0}^{T}\left|\nabla v_{\theta}(s,X_{\theta}^{x}(s)) \right|^{4}dsE\int_{0}^{T}\left|\sigma_{\theta}(s,X_{\theta}^{x}(s))-\sigma_{ \theta}(s,0)+\sigma_{\theta}(s,0)\right|^{4}ds\]

where \(C\) is some constant that could change line by line. Notice that by Assumption 5,

\[\sup_{\theta\in\Theta,t\in[0,T]}\left|\sigma_{\theta}(t,0)\right|=:\sigma_{ \vee}<\infty.\] (B.3)

Therefore, by Assumption 5 item 2, Assumption 9, and Theorem K

\[E|I_{1}(t)|^{2} \leq CE\int_{0}^{T}(|X_{\theta}^{x}(s)|+1)^{4m}dsE\int_{0}^{T}(|X _{\theta}^{x}(s)|+\sigma_{\vee})^{4}ds<\infty.\]

Therefore, \(I_{1}\) is a martingale. For \(I_{2}\), by Kunita [11, Proposition 2.6.1]

\[E|I_{2}(t)|^{2} \leq CE\int_{0}^{t}\int_{\mathbb{R}_{0}^{d^{\prime}}}\left(\frac{v _{\theta}(s,X_{\theta}^{x}(s)+\chi_{\theta}(s,X_{\theta}^{x}(s),z))-v_{\theta} (s,X_{\theta}^{x}(s-))}{\gamma(z)}\right)^{2}\mu(dz)ds\] \[\overset{(i)}{=} CE\int_{0}^{t}\int_{\mathbb{R}_{0}^{d^{\prime}}}\left| \frac{\chi_{\theta}(s,X_{\theta}^{x}(s),z)}{\gamma(z)}\right|^{4}\mu(dz)ds\cdot E \int_{0}^{t}\int_{\mathbb{R}_{0}^{d^{\prime}}}\left|\nabla v_{\theta}(s,X_{ \theta}^{x}(s)+\xi\chi_{\theta}(s,X_{\theta}^{x}(s),z))\right|^{4}\mu(dz)ds\]

where \((i)\) follows from the mean value theorem with \(\xi:=\xi_{\theta}(s,X_{\theta}^{x}(s),z)\in[0,1]\), and \((ii)\) follows from the Cauchy-Schwartz inequality applied to the integral w.r.t. the finite measure \(P\times\mathrm{Leb}\times\mu\). By Assumption 5,

\[\chi_{\theta,\vee}^{4}:=\sup_{s\in[0,T]}\int_{\mathbb{R}_{0}^{d^{ \prime}}}\frac{\left|\chi_{\theta}(s,0,z)\right|^{4}}{\gamma(z)^{4}}\mu(dz)<\infty\]

Again, by Assumption 5 item 2 and Theorem K,

\[E\int_{0}^{t}\int_{\mathbb{R}_{0}^{d^{\prime}}}\left|\frac{ \chi_{\theta}(s,X_{\theta}^{x}(s),z)}{\gamma(z)}\right|^{4}\mu(dz)ds \leq CE\int_{0}^{t}\int_{\mathbb{R}_{0}^{d^{\prime}}}\left|\frac {\chi_{\theta}(s,X_{\theta}^{x}(s),z)-\chi_{\theta}(s,0,z)}{\gamma(z)}\right|^ {4}\mu(dz)ds+C\chi_{\theta,\vee}^{4}\] \[\leq C\left(\chi_{\theta,\vee}^{4}+E\int_{0}^{t}|X_{\theta}^{x}( s)|^{4}ds\right)\] \[<\infty.\]

Also, by Assumption 9 and \(\xi\in[0,1]\)

\[E\int_{0}^{t}\int_{\mathbb{R}_{0}^{d^{\prime}}}\left|\nabla v_{ \theta}(s,X_{\theta}^{x}(s)+\xi\chi_{\theta}(s,X_{\theta}^{x}(s),z))\right|^{ 4}\mu(dz)ds\] \[\leq c_{\theta,v}E\int_{0}^{t}\int_{\mathbb{R}_{0}^{d^{\prime}}} \left(|X_{\theta}^{x}(s)|+|\chi_{\theta}(s,X_{\theta}^{x}(s),z))\right|+1)^{4 m}\mu(dz)ds\] \[\overset{(i)}{\leq}C\left(E\int_{0}^{t}|X_{\theta}^{x}(s)|^{4m} ds+E\int_{0}^{t}\int_{\mathbb{R}_{0}^{d^{\prime}}}\frac{\left|\chi_{\theta}(s,X_{ \theta}^{x}(s),z))\right|^{4m}}{\gamma(z)^{4m}}\mu(dz)ds+1\right)\] \[<\infty\]where \((i)\) follows from \(\mu\) being a finite measure and \(\gamma(z)=|z|\wedge 1\leq 1\). This shows that \(I_{2}\), hence \(M_{\theta,\theta}(t)\) is a martingale.

To show that \(M_{0,\theta}(t)\) is a martingale, we employ the same derivation with \(v_{\theta}\) replaced by \(v_{0}\). This completes the proof of Lemma 1.

### Proof of Proposition b.1

Since \(X^{x}\) is indistinguishable from \(\hat{X}^{x}\), we can use \(X^{x}\) and \(\hat{X}^{x}\) interchangeably when evaluating expectations. Therefore, it is understood that we use \(\hat{X}^{x}\) when we need continuity in \(\theta\), while we keep the notation \(X^{x}\).

In this proof, for notation simplicity, the letter \(C\) will denote a constant that could change from line to line. \(C\) can be dependent on the dimensions \(d,d^{\prime}\), the growth rate \(m\), the horizon \(T\), the Levy measure \(\nu\), and polynomial power \(p\) or \(\alpha\). But it doesn't depend on \(\theta\) (or sometimes \(\delta\)) and \(x\).

**The Continuous Part:** We prove the claim that the derivative for the continuous part should be

\[E\int_{0}^{T}\nabla_{\theta}\mathcal{L}_{0}^{C}v_{0}(s,X_{0}^{x}(s))ds,\] (B.4)

where \(\nabla_{\theta}\mathcal{L}_{0}\) is defined in (3.3).

To proceed, we also claim that

\[E\left|\int_{0}^{T}\left(\mathcal{L}_{\theta}^{C}-\mathcal{L}_{0}^{C}\right)v _{0}(s,X_{\theta}^{x}(s))ds\right|<\infty,\quad E\left|\int_{0}^{T}\nabla_{ \theta}\mathcal{L}_{0}^{C}v_{0}(s,X_{0}^{x}(s))ds\right|<\infty\] (B.5)

so that the derivative ratio and the derivative are well-defined. The finiteness of these expectations is shown below.

To prove the claimed expression (B.4) is indeed the derivative, we consider the limit

\[\begin{split}&\lim_{\theta\to 0}\frac{1}{|\theta|}\left|E\left[ \int_{0}^{T}\left(\mathcal{L}_{\theta}^{C}-\mathcal{L}_{0}^{C}\right)v_{0}(s, X_{\theta}^{x}(s))ds\right]-\theta^{T}E\left[\int_{0}^{T}\nabla_{\theta} \mathcal{L}_{0}^{C}v_{0}(s,X_{0}^{x}(s))ds\right]\right|\\ &\leq T\lim_{\theta\to 0}E\frac{1}{T}\int_{0}^{T}\frac{1}{| \theta|}\left|\left(\mathcal{L}_{\theta}^{C}-\mathcal{L}_{0}^{C}\right)v_{0}(s,X_{\theta}^{x}(s))-\theta^{T}\nabla_{\theta}\mathcal{L}_{0}^{C}v_{0}(s,X_{0}^ {x}(s))\right|ds\end{split}\] (B.6)

To show the r.h.s. go to 0, we first show what's inside the two integrals is U.I. Consider for \(\alpha>1\),

\[\begin{split}& E\frac{1}{T}\int_{0}^{T}\frac{1}{|\theta|^{ \alpha}}\left|\left(\mathcal{L}_{\theta}^{C}-\mathcal{L}_{0}^{C}\right)v_{0}(s,X_{\theta}^{x}(s))-\theta^{T}\nabla_{\theta}\mathcal{L}_{0}^{C}v_{0}(s,X_{0} ^{x}(s))\right|^{\alpha}ds\\ &\leq 2^{\alpha-1}E\frac{1}{T}\int_{0}^{T}\frac{1}{|\theta|^{ \alpha}}\left|\left(\mathcal{L}_{\theta}^{C}-\mathcal{L}_{0}^{C}\right)v_{0}(s,X_{\theta}^{x}(s))\right|^{\alpha}+2^{\alpha-1}E\frac{1}{T}\int_{0}^{T}\left| \nabla_{\theta}\mathcal{L}_{0}^{C}v_{0}(s,X_{0}^{x}(s))\right|^{\alpha}ds\end{split}\] (B.7)

For the first term, consider

\[\begin{split}&\left|\left(\mathcal{L}_{\theta}^{C}-\mathcal{L}_{0 }^{C}\right)v_{0}(t,x)\right|\\ &=(\mu_{\theta}(t,x)-\mu_{0}(t,x))^{\top}\nabla_{x}v_{0}(t,x)+ \sum_{i,j=1}^{d}(a_{\theta,i,j}(t,x)-a_{0,i,j}(t,x)\partial_{i}\partial_{j}v_ {0}(t,x).\\ &\leq\left|\mu_{\theta}(t,x)-\mu_{0}(t,x)\right|\left|\nabla_{x}v_ {0}(t,x)\right|+\left|a_{\theta}(t,x)-a_{0}(t,x)\right|\left|H[v_{0}](t,x) \right|\\ &\overset{(i)}{\leq}c_{v}(\left|x\right|+1)^{m}\left(\left|\mu_{ \theta}(t,x)-\mu_{0}(t,x)\right|+\left|a_{\theta}(t,x)-a_{0}(t,x)\right|\right) \end{split}\] (B.8)where \((i)\) follows from Assumption 9. For the second term, recall the definition in (2.1):

\[\left|a_{\theta}(t,x)-a_{0}(t,x)\right|\] \[\leq\sum_{i,j=1}^{d}\left|a_{\theta,i,j}(t,x)-a_{0,i,j}(t,x)\right|\] \[=\frac{1}{2}\sum_{i,j=1}^{d}\left|\sum_{k=1}^{d^{\prime}}\sigma_{ \theta,i,k}(t,x)\sigma_{\theta,j,k}(t,x)-\sigma_{0,i,k}(t,x)\sigma_{0,j,k}(t,x )\right|\] \[=\frac{1}{2}\sum_{i,j=1}^{d}\left|\sum_{k=1}^{d^{\prime}}\left[ \sigma_{\theta,i,k}(t,x)-\sigma_{0,i,k}(t,x)\right]\sigma_{\theta,j,k}(t,x)+ \sigma_{0,i,k}(t,x)\left[\sigma_{\theta,j,k}(t,x)-\sigma_{0,j,k}(t,x)\right]\right|\] \[\leq\frac{1}{2}\sum_{i,j=1}^{d}\sum_{k=1}^{d^{\prime}}\left|\sigma _{\theta,i,k}(t,x)-\sigma_{0,i,k}(t,x)\right|\left|\sigma_{\theta,j,k}(t,x) \right|+\left|\sigma_{0,i,k}(t,x)\right||\sigma_{\theta,j,k}(t,x)-\sigma_{0,j,k}(t,x)\right|\]

The two terms can be handled in the same way as follows:

\[\frac{1}{2}\sum_{i,j=1}^{d}\sum_{k=1}^{d^{\prime}}\left|\sigma_{ \theta,i,k}(t,x)-\sigma_{0,i,k}(t,x)\right|\left|\sigma_{\theta,j,k}(t,x)\right|\] \[\leq\frac{1}{2}\sum_{j=1}^{d}\left|\sigma_{\theta,j,\cdot}(t,x) \right|\sum_{i=1}^{d}\left|\sigma_{\theta,i,\cdot}(t,x)-\sigma_{0,i,k}(t,x)\right|\] \[\leq\frac{1}{2}\left(d\sum_{j=1}^{d}\frac{1}{d}\sqrt{\sum_{k=1}^ {d^{\prime}}\left|\sigma_{\theta,j,\cdot}(t,x)\right|^{2}}\right)\left(d\sum_{ i=1}^{d}\frac{1}{d}\sqrt{\sum_{k=1}^{d^{\prime}}\left|\sigma_{\theta,i,k}(t,x)- \sigma_{0,i,k}(t,x)\right|^{2}}\right)\] \[\leq\frac{d}{2}\sqrt{\sum_{j=1}^{d}\sum_{k=1}^{d^{\prime}}\left| \sigma_{\theta,j,k}(t,x)\right|^{2}}\sqrt{\sum_{i=1}^{d}\sum_{k=1}^{d^{\prime }}\left|\sigma_{\theta,i,k}(t,x)-\sigma_{0,i,k}(t,x)\right|^{2}}\] \[=\frac{d}{2}\left|\sigma_{\theta}(t,x)\right||\sigma_{\theta}(t,x )-\sigma_{0}(t,x)\right|\] \[\leq\frac{d}{2}\left(\left|\sigma_{\theta}(t,x)-\sigma_{\theta}(t,0)\right|+\left|\sigma_{\theta}(t,0)\right|\right)\left|\sigma_{\theta}(t,x)- \sigma_{0}(t,x)\right|\] \[\stackrel{{(i)}}{{\leq}}\frac{d}{2}\left(\left|x \right|+\sigma_{\vee}\right)\left|\sigma_{\theta}(t,x)-\sigma_{0}(t,x)\right|.\]

where \((i)\) follows from Assumption 5 item 2 and the constant bound for \(\sigma_{\theta}(t,0)\) in (B.3). Going back to inequality (B.7), these bounds implies that

\[\left|\left(\mathcal{L}_{\theta}^{C}-\mathcal{L}_{0}^{C}\right) v_{0}(t,x)\right|^{\alpha} \leq 2^{\alpha-1}c_{v}(\left|x\right|+1)^{m\alpha}\left(\left| \mu_{\theta}(t,x)-\mu_{0}(t,x)\right|^{\alpha}+\left|a_{\theta}(t,x)-a_{0}(t,x)\right|^{\alpha}\right)\] \[\leq C(\left|x\right|+1)^{m\alpha}\left(\left|\mu_{\theta}(t,x)- \mu_{0}(t,x)\right|^{\alpha}+\left(c\left|x\right|+\sigma_{\vee}\right)^{ \alpha}\left|\sigma_{\theta}(t,x)-\sigma_{0}(t,x)\right|^{\alpha}\right)\] \[\stackrel{{(i)}}{{\leq}}C(\left|x\right|+1)^{m \alpha}\left(\kappa_{\theta,0}^{\alpha}(t)(\left|x\right|+1)^{\alpha}+\kappa_{ \theta,0}^{\alpha}(t)(\left|x\right|+1)^{2\alpha}\right)\] \[\leq C\kappa_{\theta,0}^{\alpha}(t)(\left|x\right|+1)^{(m+2)\alpha}\]for some \(C\) that doesn't depend on \(\theta\), where \((i)\) follows from item 3 of Assumption 5. Therefore,

\[\frac{1}{|\theta|^{\alpha}}E\frac{1}{T}\int_{0}^{T}\left|\left( \mathcal{L}_{\theta}^{G}-\mathcal{L}_{0}^{C}\right)v_{0}(s,X_{\theta}^{x}(s)) \right|^{\alpha}ds\] (B.9) \[\leq\frac{C}{|\theta|^{\alpha}}E\frac{1}{T}\int_{0}^{T}\kappa_{ \theta,0}^{\alpha}(s)(|X_{\theta}^{x}(s)|+1)^{(m+2)\alpha}ds\] \[\stackrel{{(i)}}{{=}}\frac{C}{|\theta|^{\alpha}} \frac{1}{T}\int_{0}^{T}\kappa_{\theta,0}^{\alpha}(s)E(|X_{\theta}^{x}(s)|+1)^{ (m+2)\alpha}ds\] \[\leq\frac{C}{|\theta|^{\alpha}}\int_{0}^{T}\kappa_{\theta,0}^{ \alpha}(s)ds\sup_{\theta\in\Theta,s\in[0,T]}E(|X_{\theta}^{x}(s)|+1)^{(m+2)\alpha}\] \[\leq Cl_{\alpha}^{\alpha}\sup_{\theta\in\Theta}2^{(m+2)\alpha-1} \left(E\sup_{s\in[0,T]}|X_{\theta}^{x}(s)|^{(m+2)\alpha}+1\right)\] \[\stackrel{{(ii)}}{{\leq}}C\left(b_{(m+2)\alpha}^{(m+ 2)\alpha}(|x|+1)^{(m+2)\alpha}+1\right)\] \[\leq C(|x|+1)^{(m+2)\alpha}\]

where \((i)\) applies Fubini's theorem due to the positivity of \(\kappa_{\theta,0}^{\alpha}\), and \((ii)\) follows from Theorem K. We have shown that this expectation above is finite and independent of \(\theta\). Note that, in particular, this implies that the first expectation in (B.5) is finite as well.

For the second term in the last line of (B.7), we first consider for matrix \(M\in\mathbb{R}^{n\times d}\) and vector \(v\in\mathbb{R}^{d}\),

\[|Mv|^{\alpha} =\left(\sum_{l=1}^{n}\left|\sum_{i=1}^{d}M_{l,i}v_{i}\right|^{2} \right)^{\alpha/2}\] \[\leq\left|v\right|^{\alpha}_{\infty}\left(\sum_{l=1}^{n}\left( \sum_{i=1}^{d}|M_{l,i}|\right)^{2}\right)^{\alpha/2}\] \[\leq\left|v\right|^{\alpha}\left(\sum_{l=1}^{n}\sum_{i=1}^{d}|M_ {l,i}|\right)^{\alpha}\] \[\leq\left(nd\right)^{\alpha-1}\left|v\right|^{\alpha}\sum_{l=1}^ {n}\sum_{i=1}^{d}|M_{l,i}|^{\alpha}\]

Apply this inequality, we obtain

\[E\frac{1}{T}\int_{0}^{T}\left|\nabla_{\theta}\mathcal{L}_{0}^{C }v_{0}(s,X_{0}^{x}(s))\right|^{\alpha}ds\] (B.10) \[\leq 2^{\alpha-1}E\frac{1}{T}\int_{0}^{T}\left|\sum_{i=1}^{d} \nabla_{\theta}\mu_{0,i}(s,X_{0}^{x}(s))\partial_{i}v_{0}(s,X_{0}^{x}(s)) \right|^{\alpha}+\left|\sum_{i,j=1}^{d}\nabla_{\theta}a_{0,i,j}(s,X_{0}^{x}( s))\partial_{i}\partial_{j}v_{0}(s,X_{0}^{x}(s))\right|^{\alpha}ds\] \[\stackrel{{(i)}}{{\leq}}CE\frac{1}{T}\int_{0}^{T}(|X _{0}^{x}(s)|+1)^{(m+1)\alpha}\left(\sum_{l=1}^{n}\sum_{i=1}^{d}\left|\partial _{\theta_{l}}\mu_{0,i}(s,X_{0}^{x}(s))\right|^{\alpha}+\sum_{l=1}^{n}\sum_{i,j=1}^{d}\left|\partial_{\theta_{l}}a_{0,i,j}(s,X_{0}^{x}(s))\right|^{\alpha }\right)ds\] \[\stackrel{{(ii)}}{{\leq}}C\left[E\frac{1}{T}\int_{ 0}^{T}\sum_{l=1}^{n}\sum_{i=1}^{d}\left|\partial_{\theta_{l}}\mu_{0,i}(s,X_{0} ^{x}(s))\right|^{2\alpha}+\sum_{l=1}^{n}\sum_{i,j=1}^{d}\left|\partial_{ \theta_{l}}a_{0,i,j}(s,X_{0}^{x}(s))\right|^{2\alpha}ds\right]^{1/2}\]

where \((i)\) uses Assumption 9 and the previous matrix norm inequality, and \((ii)\) uses Cauchy-Schwartz inequality. Let \(e_{l}\in\mathbb{R}^{n}\) be the unit vector with the \(l\)'th coordinate equal to 1. Now by Assumption 5,

[MISSING_PAGE_FAIL:22]

Therefore, in view of (B.7), (B.9), and (B.14), we conclude that

\[\frac{1}{|\theta|}\left|\left(\mathcal{L}_{\theta}^{C}-\mathcal{L}_{0}^{C}\right)v _{0}(s,X_{\theta}^{x}(s))-\theta^{T}\nabla_{\theta}\mathcal{L}_{0}^{C}v_{0}(s, X_{0}^{x}(s))\right|\]

is U.I. on \((\Omega\times[0,T],\mathcal{F}\times\mathcal{B}([0,T]),P\times\frac{1}{T} \mathrm{Leb})\). Hence,

\[\lim_{\theta\to 0}E\frac{1}{T}\int_{0}^{T}\frac{1}{|\theta|} \left|\left(\mathcal{L}_{\theta}^{C}-\mathcal{L}_{0}^{C}\right)v_{0}(s,X_{ \theta}^{x}(s))-\theta^{T}\nabla_{\theta}\mathcal{L}_{0}^{C}v_{0}(s,X_{0}^{x}( s))\right|ds\] \[=E\frac{1}{T}\int_{0}^{T}\lim_{\theta\to 0}\frac{1}{|\theta|} \left|\left(\mathcal{L}_{\theta}^{C}-\mathcal{L}_{0}^{C}\right)v_{0}(s,X_{ \theta}^{x}(s))-\theta^{T}\nabla_{\theta}\mathcal{L}_{0}^{C}v_{0}(s,X_{0}^{x} (s))\right|ds\]

We use the mean value theorem to get that for some \(C>0\) and \(\xi_{i}=\xi_{\theta,i}(s,X_{\theta}^{x}(s))\in(0,1),\eta_{i,j}=\eta_{\theta,i, j}(s,X_{\theta}^{x}(s))\in(0,1)\),

\[\lim_{\theta\to 0}\frac{1}{|\theta|}\left|\left(\mathcal{L}_{ \theta}^{C}-\mathcal{L}_{0}^{C}\right)v_{0}(s,X_{\theta}^{x}(s))-\theta^{T} \nabla_{\theta}\mathcal{L}_{0}^{C}v_{0}(s,X_{0}^{x}(s))\right|\] \[\leq C\lim_{\theta\to 0}\left|\sum_{i=1}^{d}\nabla_{\theta}\mu_{ \xi_{i}\theta,i}(s,X_{\theta}^{x}(s))\partial_{i}v_{0}(s,X_{\theta}^{x}(s))- \nabla_{\theta}\mu_{0,i}(s,X_{0}^{x}(s))\partial_{i}v_{0}(s,X_{0}^{x}(s))\right|\] \[\quad+C\lim_{\theta\to 0}\sum_{i,j=1}^{d}\left|\nabla_{\theta}a_{ \eta_{i,j}\theta,i,j}(s,X_{\theta}^{x}(s))\partial_{i}\partial_{j}v_{0}(s,X_{ \theta}^{x}(s))-\nabla_{\theta}a_{0,i,j}(s,X_{0}^{x}(s))\partial_{i}\partial_{ j}v_{0}(s,X_{0}^{x}(s))\right|\] \[=0\]

where the last equality follows from the continuity of \((\theta,x)\to\nabla_{\theta}\mu_{\theta,i}(s,x)\) and \(\nabla_{\theta}a_{\theta,i,j}(s,x)\), \(x\to\partial_{i}v_{0}(s,x)\) (Assumption 6) and \(\partial_{i}\partial_{j}v_{0}(s,x)\) (Assumption 8), and \(\theta\to X_{\theta}^{x}(s)\) (Theorem K).

Therefore, going back to the limit ratio in (B.6), we have shown that

\[\lim_{\theta\to 0}\frac{1}{|\theta|}\left|E\left[\int_{0}^{T}\left(\mathcal{L} _{\theta}^{C}-\mathcal{L}_{0}^{C}\right)v_{0}(s,X_{\theta}^{x}(s))ds\right]- \theta^{T}E\left[\int_{0}^{T}\nabla_{\theta}\mathcal{L}_{0}^{C}v_{0}(s,X_{0}^{ x}(s))ds\right]\right|=0\]

**The Jump Part:** Similar to the continuous part, we claim that the derivative should be

\[E\int_{0}^{T}\nabla_{\theta}\mathcal{L}_{0}^{J}v_{0}(s,X_{0}^{x}(s))ds\] (B.15)

where \(\nabla_{\theta}\mathcal{L}_{0}^{J}\) is defined in (3.4).

To simplify notation, write

\[\left(\mathcal{L}_{\theta}^{J}-\mathcal{L}_{0}^{J}\right)v_{0}(s,X_{\theta}^{ x}(s))=\int_{\mathbb{R}_{0}^{d^{\prime}}}D_{1}-D_{2}\nu(dz)\]

where

\[D_{1} :=v_{0}(s,X_{\theta}^{x}(s)+\chi_{\theta}(s,X_{\theta}^{x}(s),z))- v_{0}(s,X_{\theta}^{x}(s)+\chi_{0}(s,X_{\theta}^{x}(s),z))\] \[D_{2} :=\sum_{i=1}^{d}\left[\chi_{\theta,i}(s,X_{\theta}^{x}(s),z)- \chi_{0,i}(s,X_{\theta}^{x}(s),z)\right]\partial_{i}v_{0}(s,X_{\theta}^{x}(s)).\]

Further, we write \(\chi_{\theta}:=\chi_{\theta}(s,X_{\theta}^{x}(s),z)\) and \(\chi_{\theta,i}:=\chi_{\theta,i}(s,X_{\theta}^{x}(s),z)\) when there is no ambiguity in the dependence on \(s,X_{\theta}^{x}(s),z\). Then, apply the mean value theorem to \(\rho\to v_{0}(s,X_{\theta}^{x}(s)+\rho\chi_{\theta}+(1-\rho)\chi_{0})\), there exists \(\xi=\xi_{\theta}(s,X_{\theta}^{x}(s),z)\in(0,1)\) s.t.

\[D_{1}=\sum_{i=1}^{d}\left[\chi_{\theta,i}-\chi_{0,i}\right]\partial_{i}v_{0}(s, X_{\theta}^{x}(s)+\xi\chi_{\theta}+(1-\xi)\chi_{0}),\]

Therefore,

\[D_{1}-D_{2}=\sum_{i=1}^{d}\left[\chi_{\theta,i}-\chi_{0,i}\right]\left[\partial _{i}v_{0}(s,X_{\theta}^{x}(s)+\xi\chi_{\theta}+(1-\xi)\chi_{0})-\partial_{i}v_ {0}(s,X_{\theta}^{x}(s))\right].\] (B.16)Again, we consider the limit

\[\lim_{\theta\to 0}\frac{1}{|\theta|}\left|E\left[\int_{0}^{T}\left( \mathcal{L}_{\theta}^{J}-\mathcal{L}_{0}^{J}\right)v_{0}(s,X_{\theta}^{x}(s))ds \right]-\theta^{T}E\left[\int_{0}^{T}\nabla_{\theta}\mathcal{L}_{0}^{J}v_{0}(s,X_{0}^{x}(s))ds\right]\right|\] \[\leq\lim_{\theta\to 0}E\int_{0}^{T}\frac{1}{|\theta|}\left|\left( \mathcal{L}_{\theta}^{J}-\mathcal{L}_{0}^{J}\right)v_{0}(s,X_{\theta}^{x}(s))- \theta^{T}\nabla_{\theta}\mathcal{L}_{0}^{J}v_{0}(s,X_{0}^{x}(s))\right|ds\] \[\leq\lim_{\theta\to 0}E\int_{0}^{T}\int_{\mathbb{R}_{0}^{d}} \frac{1}{|\theta|\gamma(z)^{2}}\left|D_{1}-D_{2}-\sum_{i=1}^{d}\theta^{T} \nabla_{\theta}\chi_{0,i}\left(\partial_{i}v_{0}(t,x+\chi_{0})-\partial_{i}v_{ 0}(t,x)\right)\right|\mu(dz)ds\] (B.17)

where, as we will show below, the two pre-limit expectations in the first line are finite.

As before, we proceed show that the limit in \(\theta\) can be exchanged into the triple integration by showing U.I. of

\[\frac{1}{|\theta|\gamma(z)^{2}}\left|D_{1}-D_{2}-\sum_{i=1}^{d}\theta^{T} \nabla_{\theta}\chi_{0,i}(s,X_{0}^{s}(s),z)\left(\partial_{i}v_{0}(s,X_{0}^{s} (s)+\chi_{0})-\partial_{i}v_{0}(s,X_{0}^{s}(s))\right)\right|.\] (B.18)

on \(\Omega\times[0,T]\times\mathbb{R}_{0}^{d^{\prime}}\) w.r.t. the probability measure \(P\times\frac{1}{T}\mathrm{Leb}\times\frac{1}{\mu(\mathbb{R}_{0}^{d^{\prime}})}\mu\). We consider

\[E\frac{1}{T}\int_{0}^{T}\frac{1}{\mu(\mathbb{R}_{0}^{d^{\prime}}) }\int_{\mathbb{R}_{0}^{d^{\prime}}}\frac{1}{|\theta|^{\alpha}\gamma(z)^{2 \alpha}}\left|D_{1}-D_{2}-\sum_{i=1}^{d}\theta^{T}\nabla_{\theta}\chi_{0,i}(s, X_{0}^{s}(s),z)\left(\partial_{i}v_{0}(s,X_{0}^{s}(s)+\chi_{0})-\partial_{i}v_{ 0}(s,X_{0}^{s}(s))\right)\right|^{\alpha}\mu(dz)ds\] \[\leq E\frac{1}{T}\int_{0}^{T}\frac{1}{\mu(\mathbb{R}_{0}^{d^{ \prime}})}\int_{\mathbb{R}_{0}^{d^{\prime}}}\frac{|D_{1}-D_{2}|^{\alpha}}{| \theta|^{\alpha}\gamma(z)^{2\alpha}}\mu(dz)ds\] \[\quad+E\frac{1}{T}\int_{0}^{T}\frac{1}{\mu(\mathbb{R}_{0}^{d^{ \prime}})}\int_{\mathbb{R}_{0}^{d^{\prime}}}\frac{1}{\gamma(z)^{2\alpha}}\left| \sum_{i=1}^{d}\nabla_{\theta}\chi_{0,i}(s,X_{0}^{s}(s),z)\left(\partial_{i}v_ {0}(s,X_{0}^{s}(s)+\chi_{0})-\partial_{i}v_{0}(s,X_{0}^{s}(s))\right)\right|^{ \alpha}\mu(dz)ds\] \[=:E_{1}+E_{2}\] (B.19)

We consider the two terms separately. For \(E_{1}\), applying the mean value theorem again to (B.16), there exists \(\eta_{i}=\eta_{\theta,i}(s,X_{\theta}^{x}(s),z,\xi)\) s.t.

\[D_{1}-D_{2}=\sum_{i,j=1}^{d}\left[\chi_{\theta,i}-\chi_{0,i}\right]\left[\xi \chi_{\theta,j}+(1-\xi)\chi_{0,j}\right]\partial_{j}\partial_{i}v_{0}(s,X_{ \theta}^{x}(s)+\eta_{i}\xi\chi_{\theta}+\eta_{i}(1-\xi)\chi_{0})\]

Therefore,

\[\int_{\mathbb{R}_{0}^{d^{\prime}}}\left|\frac{D_{1}-D_{2}}{\gamma (z)^{2}}\right|^{\alpha}\mu(dz)\] \[\leq d^{2(\alpha-1)}\int_{\mathbb{R}_{0}^{d^{\prime}}}\left(\sum _{i,j=1}^{d}\left|\partial_{j}\partial_{i}v_{0}(s,X_{\theta}^{x}(s)+\eta_{i} \xi\chi_{\theta}+\eta_{i}(1-\xi)\chi_{0})\right|^{2}\right)^{\frac{\alpha}{2 }}\frac{1}{\gamma(z)^{2\alpha}}\sqrt{\sum_{i,j=1}^{d}\left|\chi_{\theta,i}- \chi_{0,i}\right|^{2\alpha}\left|\xi\chi_{\theta,j}+(1-\xi)\chi_{0,j}\right|^{ 2\alpha}}\mu(dz)\] \[\leq C\left(\int_{\mathbb{R}_{0}^{d^{\prime}}}\left(\sum_{i,j=1} ^{d}\left|\partial_{j}\partial_{i}v_{0}(s,X_{\theta}^{x}(s)+\eta_{i}\xi\chi_{ \theta}+\eta_{i}(1-\xi)\chi_{0})\right|^{2}\right)^{\alpha}\mu(dz)\sum_{i,j=1 }^{d}\int_{\mathbb{R}_{0}^{d^{\prime}}}\frac{\left|\chi_{\theta,i}-\chi_{0,i} \right|^{2\alpha}\left|\xi\chi_{\theta,j}+(1-\xi)\chi_{0,j}\right|^{2\alpha}} {\gamma(z)^{4\alpha}}\mu(dz)\right)^{\frac{1}{2}}\] \[=:C(I_{1}\cdot I_{2})^{1/2}\]We look at \(I_{1}\) and \(I_{2}\) separately. By Assumption 9,

\[\sum_{i=1}^{d}\sum_{j=1}^{d}|\partial_{j}\partial_{i}v_{0}(s,X_{ \theta}^{x}(s)+\eta_{i}\xi\chi_{\theta}+\eta_{i}(1-\xi)\chi_{0})|^{2}\] \[\leq\sum_{i=1}^{d}|H[v_{0}](s,X_{\theta}^{x}(s)+\eta_{i}\xi\chi_{ \theta}+\eta_{i}(1-\xi)\chi_{0})|^{2}\] \[\leq\sum_{i=1}^{d}c_{v}^{2}\left(|X_{\theta}^{x}(s)+\eta_{i}\xi \chi_{\theta}+\eta_{i}(1-\xi)\chi_{0})|+1\right)^{2m}\] \[\leq dc_{v}^{2}\left(|X_{\theta}^{x}(s)|+|\chi_{\theta}|+|\chi_{0 }|+1\right)^{2m}\] \[\leq C\left(|X_{\theta}^{x}(s)|^{2m}+\frac{|\chi_{\theta}|^{2m}}{ \gamma(z)^{2m}}+\frac{|\chi_{0}|^{2m}}{\gamma(z)^{2m}}+1\right).\]

where we recall that \(\gamma(z)=|z|\wedge 1\leq 1\). Then, we consider, by Assumption 5, for \(p\geq 2\)

\[\chi_{p,\vee}^{p}:=\sup_{\theta^{\prime}\in\Theta,s\in[0,T]}E\int_{\mathbb{R}_ {0}^{d^{\prime}}}\frac{|\chi_{\theta^{\prime}}(s,0,z)|^{p}}{\gamma(z)^{p}}\mu (dz)<\infty\] (B.20)

and for all \(\theta,\theta^{\prime}\in\Theta\)

\[\int_{\mathbb{R}_{0}^{d^{\prime}}}\frac{|\chi_{\theta^{\prime}}(s,X_{\theta}^{ x}(s),z)-\chi_{\theta^{\prime}}(s,0,z)|^{p}}{\gamma(z)^{p}}\mu(dz)\leq c_{p}^{p}|X_{ \theta}^{x}(s)|^{p}.\]

So, for \(p\geq 2\)

\[\int_{\mathbb{R}_{0}^{d^{\prime}}}\frac{|\chi_{\theta^{\prime}}(s,X_{\theta}^{x}(s),z)|^{p}}{\gamma(z)^{p}}\mu(dz) \leq C|X_{\theta}^{x}(s)|^{p}+\int_{\mathbb{R}_{0}^{d^{\prime}}} \frac{|\chi_{\theta^{\prime}}(s,0,z)|^{p}}{\gamma(z)^{p}}\mu(dz)\] (B.21) \[\leq C|X_{\theta}^{x}(s)|^{p}+\chi_{p,\vee}^{p}.\]

As \(\mu\) is a finite measure, we have

\[I_{1} =\int_{\mathbb{R}_{0}^{d^{\prime}}}\left(C\left(|X_{\theta}^{x} (s)|^{2m}+\frac{|\chi_{\theta}|^{2m}}{\gamma(z)^{2m}}+\frac{|\chi_{0}|^{2m}}{ \gamma(z)^{2m}}+1\right)\right)^{\alpha}\mu(dz)\] \[\leq C(|X_{\theta}^{x}(s)|+1)^{2\alpha m}.\]

For \(I_{2}\), we bound

\[I_{2} =\sum_{i,j=1}^{d}\int_{\mathbb{R}_{0}^{d^{\prime}}}\frac{|\chi_{ \theta,i}-\chi_{0,i}|^{2\alpha}\left|\xi\chi_{\theta,j}+(1-\xi)\chi_{0,j} \right|^{2\alpha}}{\gamma(z)^{4\alpha}}\mu(dz)\] \[\leq\sum_{i,j=1}^{d}\left(\int_{\mathbb{R}_{0}^{d^{\prime}}}\frac {|\xi\chi_{\theta,j}+(1-\xi)\chi_{0,j}|^{4\alpha}}{\gamma(z)^{4\alpha}}\mu(dz) \int_{\mathbb{R}_{0}^{d^{\prime}}}\frac{|\chi_{\theta,i}-\chi_{0,i}|^{4\alpha }}{\gamma(z)^{4\alpha}}\mu(dz)\right)^{1/2}\] \[\leq d\left(\sum_{j=1}^{d}\int_{\mathbb{R}_{0}^{d^{\prime}}}\frac {|\xi\chi_{\theta,j}+(1-\xi)\chi_{0,j}|^{4\alpha}}{\gamma(z)^{4\alpha}}\mu(dz) \sum_{i=1}^{d}\int_{\mathbb{R}_{0}^{d^{\prime}}}\frac{|\chi_{\theta,i}-\chi_{0,i}|^{4\alpha}}{\gamma(z)^{4\alpha}}\mu(dz)\right)^{1/2}\] \[\leq d2^{4\alpha-1}\ \left(\sum_{j=1}^{d}\int_{\mathbb{R}_{0}^{d^{ \prime}}}\frac{|\chi_{\theta,j}|^{4\alpha}+|\chi_{0,j}|^{4\alpha}}{\gamma(z)^{4 \alpha}}\mu(dz)\sum_{i=1}^{d}\int_{\mathbb{R}_{0}^{d^{\prime}}}\frac{|\chi_{ \theta,i}-\chi_{0,i}|^{4\alpha}}{\gamma(z)^{4\alpha}}\mu(dz)\right)^{1/2}\] \[\leq C\left(\int_{\mathbb{R}_{0}^{d^{\prime}}}\frac{|\chi_{\theta }|^{4\alpha}+|\chi_{0}|^{4\alpha}}{\gamma(z)^{4\alpha}}\mu(dz)\int_{\mathbb{R }_{0}^{d^{\prime}}}\frac{|\chi_{\theta}-\chi_{0}|^{4\alpha}}{\gamma(z)^{4 \alpha}}\mu(dz)\right)^{1/2}\]By Assumption 5,

\[\int_{\mathbb{R}_{0}^{d^{\prime}}}\frac{\left|\chi_{\theta}-\chi_{0}\right|^{4 \alpha}}{\gamma(z)^{4\alpha}}\mu(dz)\leq\kappa_{\theta,0}^{4\alpha}(s)(|X_{ \theta}^{x}(s)|+1)^{4\alpha}.\]

Use this and inequality (B.21), we obtain

\[I_{2} =\sum_{i,j=1}^{d}\int_{\mathbb{R}_{0}^{d^{\prime}}}\frac{\left| \chi_{\theta,i}-\chi_{0,i}\right|^{2\alpha}\left|\xi\chi_{\theta,j}+(1-\xi) \chi_{0,j}\right|^{2\alpha}}{\gamma(z)^{4\alpha}}\mu(dz)\] \[\leq C\left[2\left(C|X_{\theta}^{x}(s)|^{4\alpha}+\chi_{4\alpha, \vee}^{4\alpha}\right)\kappa_{\theta,0}^{4\alpha}(s)(|X_{\theta}^{x}(s)|+1)^{ 4\alpha}\right]^{1/2}\] \[\leq C\kappa_{\theta,0}^{4\alpha}(s)^{1/2}(|X_{\theta}^{x}(s)|+1 )^{4\alpha}\]

In summary, we have

\[\int_{\mathbb{R}_{0}^{d^{\prime}}}\left|\frac{D_{1}-D_{2}}{\gamma (z)^{2}}\right|^{\alpha}\mu(dz) \leq C(I_{1}\cdot I_{2})^{1/2}\] \[\leq C\kappa_{\theta,0}^{4\alpha}(s)^{1/4}(|X_{\theta}^{x}(s)|+1 )^{(m+2)\alpha}.\]

Therefore, by the same argument as in the derivation (B.9), we conclude that

\[\sup_{\theta\in\Theta}E_{1} =\sup_{\theta\in\Theta}E\frac{1}{T}\int_{0}^{T}\frac{1}{\mu( \mathbb{R}_{0}^{d^{\prime}})}\int_{\mathbb{R}_{0}^{d^{\prime}}}\frac{\left|D_ {1}-D_{2}\right|^{\alpha}}{|\theta|^{\alpha}\gamma(z)^{2\alpha}}\mu(dz)ds\] (B.22) \[\leq C\sup_{\theta\in\Theta}\frac{1}{|\theta|^{\alpha}}E\int_{0}^ {T}\kappa_{\theta,0}^{4\alpha}(s)^{1/4}(|X_{\theta}^{x}(s)|+1)^{(m+2)\alpha}ds\] \[\leq C\sup_{\theta\in\Theta}\frac{1}{|\theta|^{\alpha}}\int_{0}^ {T}\kappa_{\theta,0}^{4\alpha}(s)^{1/4}ds\cdot\sup_{\theta\in\Theta,s\in[0,T]} E(|X_{\theta}^{x}(s)|+1)^{(m+2)\alpha}\] \[\stackrel{{(i)}}{{\leq}}C\sup_{\theta\in\Theta} \frac{1}{|\theta|^{\alpha}}\left(\int_{0}^{T}\kappa_{\theta,0}^{4\alpha}(s)ds \right)^{1/4}\left(b_{(m+2)\alpha}^{(m+2)\alpha}(|x|+1)^{(m+2)\alpha}+1\right)\] \[\leq C(|x|+1)^{(m+2)\alpha}.\]

where \((i)\) uses Jensen's inequality and Theorem K. Note that, with \(\alpha=1\), this also implies the finiteness of the first expectation in (B.17).

For the second term in (B.19), we use the same technique as in the derivation for that of the continuous part. First, we consider

\[\int_{\mathbb{R}_{0}^{d^{\prime}}}\frac{1}{\gamma(z)^{2\alpha}} \left|\sum_{i=1}^{d}\nabla_{\theta}\chi_{0,i}(s,x,z)\left(\partial_{i}v_{0}(s,x +\chi_{0}(s,x,z))-\partial_{i}v_{0}(s,x)\right)\right|^{\alpha}\mu(dz)\] \[\leq C\int_{\mathbb{R}_{0}^{d^{\prime}}}\frac{1}{\gamma(z)^{2\alpha }}\sum_{i=1}^{d}\left|\nabla_{\theta}\chi_{0,i}(s,x,z)\left(\partial_{i}v_{0}(s, x+\chi_{0}(s,x,z))-\partial_{i}v_{0}(s,x)\right)\right|^{\alpha}\mu(dz)\] \[\leq C\int_{\mathbb{R}_{0}^{d^{\prime}}}\frac{1}{\gamma(z)^{2 \alpha}}\sum_{i=1}^{d}\left|\partial_{i}v_{0}(s,x+\chi_{0}(s,x,z))-\partial_{i }v_{0}(s,x)\right|^{\alpha}\left|\nabla_{\theta}\chi_{0,i}(s,x,z)\right|^{ \alpha}\mu(dz)\] \[\stackrel{{(i)}}{{=}}C\int_{\mathbb{R}_{0}^{d^{\prime }}}\frac{1}{\gamma(z)^{2\alpha}}\sum_{i=1}^{d}\left|\sum_{j=1}^{d}\partial_{j }\partial_{i}v_{0}(s,x+\xi_{i}\chi_{0}(s,x,z))\chi_{0,j}(s,x,z)\right|^{\alpha }\sum_{l=1}^{n}|\partial_{\theta_{l}}\chi_{0,i}(s,x,z)|^{\alpha}\mu(dz)\] \[\leq C\int_{\mathbb{R}_{0}^{d^{\prime}}}\frac{1}{\gamma(z)^{2 \alpha}}\sum_{i=1}^{d}\left|\chi_{0}(s,x,z)\right|^{\alpha}\left|\sum_{j=1}^{d }\left|\partial_{j}\partial_{i}v_{0}(s,x+\xi_{i}\chi_{0}(s,x,z))\right|^{2} \right|^{\frac{\alpha}{2}}\sum_{l=1}^{n}|\partial_{\theta_{l}}\chi_{0,i}(s,x,z )|^{\alpha}\mu(dz)\] \[\leq C\int_{\mathbb{R}_{0}^{d^{\prime}}}\frac{1}{\gamma(z)^{2 \alpha}}\sum_{i=1}^{d}\left|\chi_{0}(s,x,z)\right|^{\alpha}\left|H[v_{0}](s,x+ \xi_{i}\chi_{0}(s,x,z))\right|^{\alpha}\sum_{l=1}^{n}|\partial_{\theta_{l}} \chi_{0,i}(s,x,z)|^{\alpha}\mu(dz)\] \[\stackrel{{(ii)}}{{\leq}}C\int_{\mathbb{R}_{0}^{d^{ \prime}}}\frac{\left|\chi_{0}(s,x,z)\right|^{\alpha}}{\gamma(z)^{2\alpha}}\sum _{i=1}^{d}(\left|x+\xi_{i}\chi_{0}(s,x,z)\right|+1)^{m\alpha}\sum_{l=1}^{n}| \partial_{\theta_{l}}\chi_{0,i}(s,x,z)|^{\alpha}\mu(dz)\] \[\stackrel{{(iii)}}{{\leq}}C\int_{\mathbb{R}_{0}^{d^{ \prime}}}\frac{\left|\chi_{0}(s,x,z)\right|^{\alpha}}{\gamma(z)^{2\alpha}} \left[(\left|x\right|+1)^{m\alpha}+\frac{\left|\chi_{0}(s,x,z)\right|^{m \alpha}}{\gamma(z)^{m\alpha}}\right]\sum_{i=1}^{d}\sum_{l=1}^{n}|\partial_{ \theta_{l}}\chi_{0,i}(s,x,z)|^{\alpha}\mu(dz)\] \[\leq C(\left|x\right|+1)^{m\alpha}\sum_{i=1}^{d}\sum_{l=1}^{n} \int_{\mathbb{R}_{0}^{d^{\prime}}}\frac{\left|\chi_{0}(s,x,z)\right|^{2\alpha} }{\gamma(z)^{\alpha}}\frac{|\partial_{\theta_{l}}\chi_{0,i}(s,x,z)|^{\alpha}} {\gamma(z)^{\alpha}}\mu(dz)\] \[\leq C(\left|x\right|+1)^{m\alpha}\sum_{i=1}^{d}\sum_{l=1}^{n} \left(\int_{\mathbb{R}_{0}^{d^{\prime}}}\frac{\left|\chi_{0}(s,x,z)\right|^{2 \alpha}}{\gamma(z)^{2\alpha}}\mu(dz)\int_{\mathbb{R}_{0}^{d^{\prime}}}\frac{ |\partial_{\theta_{l}}\chi_{0,i}(s,x,z)|^{2\alpha}}{\gamma(z)^{2\alpha}}\mu( dz)\right)^{\frac{1}{2}}\] \[\stackrel{{(iv)}}{{\leq}}C(\left|x\right|+1)^{(m+1 )\alpha}\sum_{i=1}^{d}\sum_{l=1}^{n}\left(\int_{\mathbb{R}_{0}^{d^{\prime}}} \frac{\left|\partial_{\theta_{l}}\chi_{0,i}(s,x,z)\right|^{2\alpha}}{\gamma(z) ^{2\alpha}}\mu(dz)\right)^{\frac{1}{2}}\]

where \((i)\) applies the mean value theorem to \(\rho\rightarrow\partial_{i}v_{0}(s,x+\rho\chi_{0})\) to yield the existence of such \(\xi_{i}:=\xi_{i}(s,x,z)\), \((ii)\) follows from Assumption 9, and \((iii)\) uses Holder's inequality \(\|fg\|_{1}\leq\|f\|_{\infty}\|g\|_{1}\) as well as \(\gamma(z)\leq 1\), and \((iv)\) follows from (B.21) where we have that for \(p\geq 2\)

\[\int_{\mathbb{R}_{0}^{d^{\prime}}}\frac{\left|\chi_{0}(s,x,z)\right|^{p}}{\gamma (z)^{p}}\mu(dz)\leq C(\left|x\right|+1)^{p}.\]Therefore, by Theorem K and Cauchy-Schwarz inequality,

\[E_{2} =E\frac{1}{T}\int_{0}^{T}\frac{1}{\mu(\mathbb{R}_{0}^{d^{\prime}})} \int_{\mathbb{R}_{0}^{d^{\prime}}}\frac{1}{\gamma(z)^{2\alpha}}\left|\sum_{i=1} ^{d}\nabla_{\theta}\chi_{0,i}(s,X_{0}^{s}(s),z)\left(\partial_{i}v_{0}(s,X_{0} ^{s}(s)+\chi_{0})-\partial_{i}v_{0}(s,X_{0}^{s}(s))\right)\right|^{\alpha}\mu(dz)\] \[\leq C\sum_{i=1}^{d}\sum_{l=1}^{n}E\frac{1}{T}\int_{0}^{T}(|X_{0} ^{x}(s)|+1)^{(m+1)\alpha}\left(\int_{\mathbb{R}_{0}^{d^{\prime}}}\frac{| \partial_{\theta_{l}}\chi_{0,i}(s,X_{0}^{x}(s),z)|^{2\alpha}}{\gamma(z)^{2 \alpha}}\mu(dz)\right)^{1/2}ds\] \[\leq Cb_{2(m+1)\alpha}^{(m+1)\alpha}(|x|+1)^{(m+1)\alpha}\sum_{i= 1}^{d}\sum_{l=1}^{n}\left(E\frac{1}{T}\int_{0}^{T}\int_{\mathbb{R}_{0}^{d^{ \prime}}}\frac{|\partial_{\theta_{l}}\chi_{0,i}(s,X_{0}^{x}(s),z)|^{2\alpha}}{ \gamma(z)^{2\alpha}}\mu(dz)ds\right)^{1/2}\]

To bound this, as in the continuous part, we check the uniform integrability on \(\Omega\times[0,T]\times\mathbb{R}_{0}^{d^{\prime}}\) w.r.t. the probability measure \(P\times\frac{1}{T}\mathrm{Leb}\times\frac{1}{\mu(\mathbb{R}_{0}^{d^{\prime}})}\mu\) when \(\delta\) is in a sufficiently small neighbourhood of \(0\) of the derivative ratio

\[\frac{1}{\gamma(z)^{2\alpha}}\left(\frac{|\chi_{\delta e_{l},i}(s,X_{0}^{x}(s),z)-\chi_{0,i}(s,X_{0}^{x}(s),z)|}{\delta}\right)^{2\alpha}.\] (B.23)

To simplify notation, we again denote \(\chi_{\theta,i}:=\chi_{\theta,i}(s,X_{0}^{x}(s),z)\). To check this, we consider for \(\epsilon\geq 0\)

\[E\frac{1}{T}\int_{0}^{T}\frac{1}{\mu(\mathbb{R}_{0}^{d^{\prime}}) }\int_{\mathbb{R}_{0}^{d^{\prime}}}\frac{1}{\gamma(z)^{2\alpha+\epsilon}} \left(\frac{|\chi_{\delta e_{l},i}-\chi_{0,i}|}{\delta}\right)^{2\alpha+ \epsilon}\mu(dz)ds\] \[=\frac{1}{\mu(\mathbb{R}_{0}^{d^{\prime}})}E\frac{1}{T}\int_{0}^{ T}\frac{1}{\delta^{2\alpha+\epsilon}}\int_{\mathbb{R}_{0}^{d^{\prime}}}\left( \frac{|\chi_{\delta e_{l},i}-\chi_{0,i}|}{\gamma(z)}\right)^{2\alpha+\epsilon} \mu(dz)ds\] \[\leq\frac{1}{\mu(\mathbb{R}_{0}^{d^{\prime}})}\frac{1}{\delta^{2 \alpha+\epsilon}}E\frac{1}{T}\int_{0}^{T}\kappa_{\delta e_{l},0}^{2\alpha+ \epsilon}(s)(|X_{0}^{x}(s)|+1)^{2\alpha+\epsilon}ds\] \[\leq C\frac{1}{\delta^{2\alpha+\epsilon}}\int_{0}^{T}\kappa_{ \delta e_{l},0}^{2\alpha+\epsilon}(s)ds\cdot E\sup_{s\in[0,T]}(|X_{0}^{x}(s)| +1)^{2\alpha+\epsilon}\] \[\leq Cl_{2\alpha+\epsilon}^{2\alpha+\epsilon}t_{2\alpha+\epsilon} ^{2\alpha+\epsilon}((|x|+1)^{2\alpha+\epsilon}+1)\]

independent of \(\delta\). Choose \(\epsilon>0\) will show the U.I. of (B.23). Therefore, we

\[E_{2} \leq C(|x|+1)^{(m+1)\alpha}\sum_{i=1}^{d}\sum_{l=1}^{n}\left(E \frac{1}{T}\int_{0}^{T}\frac{1}{\mu(\mathbb{R}_{0}^{d^{\prime}})}\int_{ \mathbb{R}_{0}^{d^{\prime}}}\frac{|\partial_{\theta_{l}}\chi_{0,i}(s,X_{0}^{x }(s),z)|^{2\alpha}}{\gamma(z)^{2\alpha}}\mu(dz)ds\right)^{1/2}\] \[=C(|x|+1)^{(m+1)\alpha}\sum_{i=1}^{d}\sum_{l=1}^{n}\left(\lim_{ \delta\downarrow 0}E\frac{1}{T}\int_{0}^{T}\frac{1}{\mu(\mathbb{R}_{0}^{d^{\prime}}) }\int_{\mathbb{R}_{0}^{d^{\prime}}}\frac{1}{\gamma(z)^{2\alpha}}\left(\frac{ |\chi_{\delta e_{l},i}-\chi_{0,i}|}{\delta}\right)^{2\alpha}\mu(dz)ds\right)^ {1/2}\] \[\leq C(|x|+1)^{(m+2)\alpha}\]

where the last inequality follows from previous derivation with \(\epsilon=0\). In particular, recalling the definition of \(E_{2}\) in (B.19), this shows that

\[E\left|\int_{0}^{T}\nabla_{\theta}\mathcal{L}_{0}^{J}v_{0}(s,X_{0}^{x}(s))ds \right|<\infty\]

as claimed above.

Therefore, by bounding the two terms in (B.19), we conclude the uniform integrability of (B.18). So, going back to (B.17), U.I. implies that

\[\lim_{\theta\to 0}\frac{1}{|\theta|}\left|E\left[\int_{0}^{T}\left( \mathcal{L}_{\theta}^{J}-\mathcal{L}_{0}^{J}\right)v_{0}(s,X_{\theta}^{x}(s)) ds\right]-\theta^{T}E\left[\int_{0}^{T}\nabla_{\theta}\mathcal{L}_{0}^{J}v_{0}(s,X_{0 }^{x}(s))ds\right]\right|\] \[\leq\lim_{\theta\to 0}E\int_{0}^{T}\int_{\mathbb{R}_{0}^{d^{\prime}}} \frac{1}{|\theta|\gamma(z)^{2}}\left|D_{1}-D_{2}-\sum_{i=1}^{d}\theta^{T} \nabla_{\theta}\chi_{0,i}\left(\partial_{i}v_{0}(s,X_{0}^{x}(s)+\chi_{0})- \partial_{i}v_{0}(s,X_{0}^{x}(s))\right)\right|\mu(dz)ds\] \[=E\int_{0}^{T}\int_{\mathbb{R}_{0}^{d^{\prime}}}\frac{1}{\gamma(z )^{2}}\lim_{\theta\to 0}\frac{1}{|\theta|}\left|D_{1}-D_{2}-\sum_{i=1}^{d} \theta^{T}\nabla_{\theta}\chi_{0,i}\left(\partial_{i}v_{0}(s,X_{0}^{x}(s)+\chi _{0})-\partial_{i}v_{0}(s,X_{0}^{x}(s))\right)\right|\mu(dz)ds\] \[=0,\]

where the last step follows from

\[\lim_{\theta\to 0}\frac{1}{|\theta|}\left[D_{1}-D_{2}-\sum_{i=1}^{d} \theta^{T}|\theta|\nabla_{\theta}\chi_{0,i}(s,X_{0}^{x}(s),z)\left(\partial_{i} v_{0}(s,X_{0}^{x}(s)+\chi_{0})-\partial_{i}v_{0}(s,X_{0}^{x}(s))\right)\right]\] \[=\sum_{i=1}^{d}\left(\partial_{i}v_{0}(s,X_{0}^{x}(s)+\chi_{0})- \partial_{i}v_{0}(s,X_{0}^{x}(s))\right)\lim_{\theta\to 0}\frac{1}{|\theta|} \left(\chi_{\theta,i}(s,X_{\theta}^{x}(s),z)-\chi_{0,i}(s,X_{\theta}^{x}(s),z)- \theta^{T}\nabla_{\theta}\chi_{0,i}(s,X_{0}^{x}(s),z)\right)\] \[\stackrel{{(i)}}{{=}}\sum_{i=1}^{d}\left(\partial_{i} v_{0}(s,X_{0}^{x}(s)+\chi_{0})-\partial_{i}v_{0}(s,X_{0}^{x}(s))\right)\lim_{ \theta\to 0}\frac{1}{|\theta|}\theta^{T}\left(\nabla_{\theta}\chi_{\xi, \theta,i}(s,X_{\theta}^{x}(s),z)-\nabla_{\theta}\chi_{0,i}(s,X_{0}^{x}(s),z)\right)\] \[\stackrel{{(ii)}}{{=}}0.\]

Here, \((i)\) applies the mean value theorem and \((ii)\) use the continuity of \(\theta\to\nabla_{\theta}\chi_{\xi,\theta,i}(s,X_{\theta}^{x}(s),z)\) as in Assumption 6.

**The Rewards Part:** We first consider the reward rate \(r\). As in the previous proof, we show the U.I. of

\[I_{1}(\theta):=\frac{1}{|\theta|^{\alpha}}E\frac{1}{T}\int_{0}^{T}|\rho_{ \theta}(s,X_{\theta}^{x}(s))-\rho_{0}(s,X_{\theta}^{x}(s))|^{\alpha}\,ds\]

and the finiteness of

\[I_{2}:=\frac{1}{|\theta|^{\alpha}}E\frac{1}{T}\int_{0}^{T}|\nabla_{\theta}\rho _{0}(s,X_{\theta}^{x}(s))|^{\alpha}\,ds\]

for some \(\alpha>1\). By Assumption 7 item 1 and the same derivation as in (B.9),

\[I_{1}(\theta) \leq\frac{C}{|\theta|^{\alpha}}E\frac{1}{T}\int_{0}^{T}\kappa_{ \theta,0}^{\alpha}(s)(|X_{\theta}^{x}(s)|+1)^{m\alpha}ds\] \[\leq\frac{C}{|\theta|^{\alpha}}\int_{0}^{T}\kappa_{\theta,0}^{ \alpha}(s)ds\sup_{\theta\in\Theta,s\in[0,T]}E(|X_{\theta}^{x}(s)|+1)^{m\alpha}\] \[\leq C(|x|+1)^{m\alpha}\]

uniformly in \(\theta\). Moreover,

\[E\frac{1}{T}\int_{0}^{T}|\partial_{\theta_{i}}\rho_{0}(s,X_{0}^ {x}(s))|^{\alpha}ds =E\frac{1}{T}\int_{0}^{T}\lim_{\delta\downarrow 0}\left|\frac{r_{ \delta\varepsilon_{i},i}(s,X_{0}^{x}(s))-r_{0,i}(s,X_{0}^{x}(s))}{\delta} \right|^{\alpha}ds\] \[=\lim_{\delta\downarrow 0}E\frac{1}{T}\int_{0}^{T}\left|\frac{r_{ \delta\varepsilon_{i},i}(s,X_{0}^{x}(s))-r_{0,i}(s,X_{0}^{x}(s))}{\delta} \right|^{\alpha}ds\] \[\leq\sup_{\theta\in\Theta}\frac{1}{|\theta|^{\alpha}}E\frac{1}{ T}\int_{0}^{T}\kappa_{\theta,0}^{\alpha}(s)(1+|X_{0}^{x}(s)|)^{\alpha}ds\] \[<\infty.\]These results and the continuity of \((\theta,x)\to\nabla_{\theta}r(s,x)\) and \(\theta\to X^{x}_{\theta}(s)\) implies that

\[\lim_{\theta\to 0}\frac{1}{|\theta|}\left|E\left[\int_{0}^{T}\rho_{ \theta}(s,X^{x}_{\theta}(s))-\rho_{0}(s,X^{x}_{\theta}(s))ds\right]-\theta^{T} E\left[\int_{0}^{T}\nabla_{\theta}\rho_{0}(s,X^{x}_{0}(s))ds\right]\right|\] \[\leq E\left[\int_{0}^{T}\lim_{\theta\to 0}\frac{1}{|\theta|}\left|\rho_{ \theta}(s,X^{x}_{\theta}(s))-\rho_{0}(s,X^{x}_{\theta}(s))-\theta^{T}\nabla_{ \theta}\rho_{0}(s,X^{x}_{0}(s))\right|ds\right]\] \[=0.\]

For the terminal reward \(g\) term, the same proof with the integral removed and \(s\) replaced by \(T\) will yield the desired conclusion.

## Appendix C Proof of Proposition a.1

We note that the statement for \(X^{x}_{0}\) and its first derivative holds from directly applying Kunita [11, Theorem 3.3.2] and the a.s. version of Kolmogorov's continuity criterion as in Corollary 1 of Protter [19, Theorem 73].

To show the statement for the second derivative, we apply the proof of Kunita [11, Theorem 3.4.2]. From display (3.43), we look at the random drift:

\[M^{x}_{a,b,i}(r,H_{a,b}):=\sum_{l=1}^{d}\left[\partial_{l}\mu_{0,i}(r,X^{x}_{0 }(s,r))H_{a,b,l}+\sum_{m=1}^{d}\partial_{m}\partial_{l}\mu_{0,i}(r,X^{x}_{0}(s,r))\partial_{a}X^{x}_{0,l}(s,r)\partial_{b}X^{x}_{0,m}(s,r)\right]\]

seen as a function of \(r\in[0,T],H\in\mathbb{R}^{d\times d\times d}\), and show that it satisfies the conditions for Kunita [11, Theorem 3.3.2] with \(\lambda=x\); i.e. the conditions in Assumption 10.

We note that as \(\mu_{0}\) and \(\partial_{m}\mu_{0}(r,x)\) satisfying item 2 of Assumption 10 for any \(l,m=1,\ldots,d\),

\[\sup_{r\in[0,T]}|\partial_{m}\mu_{0}(r,x)| =\sup_{r\in[0,T]}\left|\lim_{\delta\downarrow 0}\frac{\mu_{0}(r, x+\delta e_{m})-\mu_{0}(r,x)}{\delta}\right|\] \[\leq\sup_{r\in[0,T]}\lim_{\delta\downarrow 0}\frac{|\mu_{0}(r, x+\delta e_{m})-\mu_{0}(r,x)|}{\delta}\] \[\leq c\]

is bounded. Same for \(\partial_{m}\partial_{l}\mu_{0}\).

First, at \(H_{a,b}=0\).

\[\sup_{r\in[0,T],x\in\mathbb{R}^{d}}E|M^{x}_{a,b,.}(r,0)|^{p}\leq C\sup_{r\in[ 0,T],x\in\mathbb{R}}\sum_{m,l=1}^{d}E|\partial_{a}X^{x}_{0,l}(s,r)||\partial_ {b}X^{x}_{0,m}(s,r)|<\infty.\]

Second, \(M^{x}_{a,b,i}(r,H_{a,b})\) is clearly uniformly Lipschitz in \(H_{a,b}\) as \(\partial_{l}\mu_{0}\) is bounded.

Third, using the boundedness of \(\partial_{m}\mu_{0}\) and \(\partial_{m}\partial_{l}\mu_{0}\), we have

\[\sum_{l=1}^{d}|\partial_{l}\mu_{0,i}(r,X^{x}_{0}(s,r))H_{a,b,l}-\partial_{l} \mu_{0,i}(r,X^{y}_{0}(s,r))H_{a,b,l}|\leq C|X^{x}_{0}(s,r)-X^{y}_{0}(s,r)||H_{ a,b}|\]and

\[\sum_{m,l=1}^{d}\left|\partial_{m}\partial_{l}\mu_{0,i}(r,X_{0}^{x}(s,r ))\partial_{a}X_{0,l}^{x}(s,r)\partial_{b}X_{0,m}^{x}(s,r)-\partial_{m}\partial_ {l}\mu_{0,i}(r,X_{0}^{y}(s,r))\partial_{a}X_{0,l}^{y}(s,r)\partial_{b}X_{0,m}^{ y}(s,r)\right|\] \[\leq\sum_{m,l=1}^{d}\left|\partial_{m}\partial_{l}\mu_{0,i}(r,X_{0 }^{x}(s,r))-\partial_{m}\partial_{l}\mu_{0,i}(r,X_{0}^{y}(s,r))\right|\left| \partial_{a}X_{0,l}^{y}(s,r)\partial_{b}X_{0,m}^{y}(s,r)\right|\] \[\quad+\left|\partial_{m}\partial_{l}\mu_{0,i}(r,X_{0}^{x}(s,r)) \right|\left|\partial_{a}X_{0,l}^{x}(s,r)\partial_{b}X_{0,m}^{x}(s,r)-\partial _{a}X_{0,l}^{y}(s,r)\partial_{b}X_{0,m}^{y}(s,r)\right|\] \[\leq\sum_{m,l=1}^{d}C|X_{0}^{x}(s,r)-X_{0}^{y}(s,r)|\left|\partial _{a}X_{0,l}^{y}(s,r)\partial_{b}X_{0,m}^{y}(s,r)\right|+C\left|\partial_{a}X_ {0,l}^{x}(s,r)-\partial_{a}X_{0,l}^{y}(s,r)\right|\left|\partial_{b}X_{0,m}^{x }(s,r)\right|\] \[\quad+C\left|\partial_{b}X_{0,l}^{x}(s,r)-\partial_{b}X_{0,l}^{y} (s,r)\right|\left|\partial_{a}X_{0,m}^{x}(s,r)\right|.\]

Therefore, defining \(K_{x,y}(r,H)\) to be the sum of the two, we see that

\[E\int_{0}^{T}K_{x,y}^{(a,b)}(r,H)^{p}dr\leq C|H_{a,b}|^{p}|x-y|^{p}+C|x-y|^{p} \leq C|x-y|^{p}(|H_{a,b}|+1)^{p}.\]

Here the first inequality follows from the first derivative satisfying the Proposition A.1, which follows from a direct application of Kunita [11, Theorem 3.3.2].

Similar results can be established for the volatility and the jump coefficients. Therefore, we conclude the proof by applying Kunita [11, Theorem 3.3.2] to the derivative and the second derivatives.

## Appendix D Proof of Theorem 2'

Our proof of Theorem 2' hinges on the ability to exchange the derivative with the expectation and time integral. To achieve this, first, we use similar techniques as in the proof of Theorem 1' to prove the following lemma.

**Lemma 2**.: _Under the assumptions of Theorem 2', for \(h(t,x)=\rho_{0}(t,x)\) and \(g_{0}(x)\), we have_

\[\partial_{x_{i}}Eh(s,X_{0}^{x}(t,s))=E\partial_{x_{i}}h(s,X_{0}^{x}(t,s))=E \nabla h(s,X_{0}^{x}(t,s))^{\top}\partial_{i}X_{0}^{x}(t,s)\] (D.1)

_and_

\[\partial_{x_{j}}\partial_{x_{i}}Eh(s,X_{0}^{x}(t,s)) =E\partial_{x_{j}}\partial_{x_{i}}h(s,X_{0}^{x}(t,s))\] \[=E\partial_{i}X_{0}^{x}(t,s)^{\top}H[h](s,X_{0}^{x}(t,s))\partial _{j}X_{0}^{x}(t,s)+\nabla h(s,X_{0}^{x}(t,s))^{\top}\partial_{j}\partial_{i}X_ {0}^{x}(t,s)).\] (D.2)

_Moreover, there exists a constant \(C\) independent of \(t,s\) s.t._

\[E|\partial_{x_{i}}h(s,X_{0}^{x}(t,s))|\leq C(|x|+1)^{m},\quad\text{and}\quad E| \partial_{x_{j}}\partial_{x_{i}}h(s,X_{0}^{x}(t,s))|\leq C(|x|+1)^{m}.\]

Lemma 2 directly implies that the derivatives of the expected terminal rewards in (2.6) satisfy

\[\nabla_{x}Eg_{0}^{\top}(t,X_{0}^{x}(t,T)) =E\nabla g_{0}^{\top}\nabla X_{0}^{x}(t,T),\] (D.3) \[H_{x}[Eg_{0}^{\top}(t,X_{0}^{x}(t,T))] =E\left[\nabla X_{0}^{x}(t,T)^{\top}H[g_{0}]\nabla X_{0}^{x}(t,T )+\left\langle\nabla g_{0},H[X_{0,}^{x}](t,T)\right\rangle\right].\]

By the same argument, to prove Theorem 2', it suffices to show that for the cumulative reward parts in (2.6), the time integral and space derivatives can be interchanged. First, by Lemma 2, we see that

\[\int_{t}^{T}E|\partial_{x_{i}}\rho_{0}(s,X_{0}^{x}(t,s))|ds<\infty,\quad\text{ and}\quad\int_{t}^{T}E|\partial_{x_{j}}\partial_{x_{i}}\rho_{0}(s,X_{0}^{x}(t,s))|ds<\infty.\]So, by Fubini's theorem and Lemma 2

\[E\int_{t}^{T}\partial_{x_{i}}\rho_{0}(s,X_{0}^{x}(t,s))ds =\int_{t}^{T}E\partial_{x_{i}}\rho_{0}(s,X_{0}^{x}(t,s))ds\] \[=\int_{t}^{T}\partial_{x_{i}}E\rho_{0}(s,X_{0}^{x}(t,s))ds\] \[\stackrel{{(i)}}{{=}}\partial_{x_{i}}\int_{t}^{T}E \rho_{0}(s,X_{0}^{x}(t,s))ds\] \[=\partial_{x_{i}}E\int_{t}^{T}\rho_{0}(s,X_{0}^{x}(t,s))ds\]

where \((i)\) follows from dominated convergence that for \(y\) in a \(\epsilon\) neighbourhood of \(x\),

\[|\partial_{y_{i}}E\rho_{0}(s,X_{0}^{y}(t,s))|\leq E|\partial_{y_{i}}\rho_{0}(s, X_{0}^{y}(t,s))|\leq C(|x|+\epsilon+1)^{m}\]

independent of \(s\). Similarly,

\[E\int_{t}^{T}\partial_{x_{j}}\partial_{x_{i}}\rho_{0}(s,X_{0}^{x}(t,s))ds= \partial_{x_{j}}\partial_{x_{i}}E\int_{t}^{T}\rho_{0}(s,X_{0}^{x}(t,s))ds.\]

This and (D.3) implies (2.6), completing the proof.

### Proof of Lemma 2

**First Space Derivatives:** We first show equality (D.1). Consider

\[\partial_{x_{i}}Eh(s,X_{0}^{x}(t,s))=\lim_{\delta\to 0}\frac{1}{\delta}E\left[h(s,X_{0}^{x+ \delta e_{i}}(t,s))-h(s,X_{0}^{x}(t,s))\right].\] (D.4)

We exchange the limit and the expectation by considering

\[E\delta^{-\alpha}\left|h(s,X_{0}^{x+\delta e_{i}}(t,s))-h(s,X_{0 }^{x}(t,s))\right|^{\alpha}\] \[=E\delta^{-\alpha}\left|\nabla h(s,\xi X_{0}^{x+\delta e_{j}}(t,s )+(1-\xi)X_{0}^{x}(t,s))^{\top}\left(X_{0}^{x+\delta e_{j}}(t,s)-X_{0}^{x}(t,s )\right)\right|^{\alpha}\] \[\leq\left(E\left|\frac{X_{0}^{x+\delta e_{j}}(t,s)-X_{0}^{x}(t,s) }{\delta}\right|^{2\alpha}E|\nabla h(s,\xi X_{0}^{x+\delta e_{j}}(t,s)+(1-\xi )X_{0}^{x}(t,s))|^{2\alpha}\right)^{1/2}\]

where the mean value theorem implies the existence of such r.v. \(\xi\in[0,1]\). For the first term, Proposition A.1 implies that

\[E\left|\frac{X_{0}^{x+\delta e_{j}}(t,s)-X_{0}^{x}(t,s)}{\delta}\right|^{2 \alpha}\leq l_{2\alpha}^{2\alpha}.\]

For the second term, by Assumption 4

\[E|\nabla h(s,\xi X_{0}^{x+\delta e_{j}}(t,s)+(1-\xi)X_{0}^{x}(t, s))|^{2\alpha}\] \[\leq c_{h}^{2\alpha}E(|X_{0}^{x+\delta e_{i}}(t,s)|+|X_{0}^{x}(t, s)|+1)^{2\alpha m}\] \[\leq c_{h}^{2\alpha}E(|X_{0}^{x+\delta e_{i}}(t,s)-X_{0}^{x}(t, s)|+2|X_{0}^{x}(t,s)|+1)^{2\alpha m}\] (D.5) \[\leq C(|x|+1)^{2\alpha m}+Cl_{2\alpha m}^{2\alpha m}|\delta|^{2 \alpha m}\] \[\leq C(|x|+1)^{2\alpha m}.\]

where the last inequality considers \(|\delta|\leq 1\) and \(C\) can be chosen so that it doesn't depend on \(\delta\), \(s\), and \(t\). Therefore, the limit in the r.h.s. of (D.4) can be interchanged with the expectation and we have that

\[\partial_{x_{i}}Eh(s,X_{0}^{x}(t,s)) =E\partial_{x_{i}}h(s,X_{0}^{x}(t,s))\] \[=E\nabla h(s,X_{0}^{x}(t,s))^{\top}\partial_{i}X_{0}^{x}(t,s).\]Also, the previous derivation with \(\alpha=1\) and taking the limit \(\delta\to 0\) implies that

\[E|\partial_{x_{i}}h(s,X_{0}^{x}(t,s))|\leq C(|x|+1)^{m}\]

where \(C\) doesn't depend on \(s\) and \(t\).

**Second Space Derivatives:** Then, we show equality (D.2). Previous proof implies that

\[\partial_{x_{j}}\partial_{x_{i}}Eh(s,X_{0}(t,s,x))=\partial_{x_{j}}E\nabla h(s, X_{0}^{x}(t,s))^{\top}\partial_{i}X_{0}^{x}(t,s).\]

Hence we employ the same strategy to exchange the limit and expectations for the following expression

\[\begin{split}&\lim_{\delta\to 0}E\frac{1}{\delta}\left[\nabla h (s,X_{0}^{x+\delta e_{j}}(t,s))^{\top}\partial_{i}X_{0}^{x+\delta e_{j}}(t,s)- \nabla h(s,X_{0}^{x}(t,s))^{\top}\partial_{i}X_{0}^{x}(t,s)\right]\\ &=\lim_{\delta\to 0}E\frac{1}{\delta}\partial_{i}X_{0}^{x}(t,s)^{ \top}(\nabla h(s,X_{0}^{x+\delta e_{j}}(t,s))-\nabla h(s,X_{0}^{x}(t,s)))\\ &+\lim_{\delta\to 0}E\frac{1}{\delta}\nabla h(s,X_{0}^{x+\delta e _{j}}(t,s))^{\top}(\partial_{i}X_{0}^{x+\delta e_{j}}(t,s)-\partial_{i}X_{0}^{ x}(t,s))\end{split}\] (D.6)

We show U.I. for the two terms in (D.6) separately. For the first term, consider

\[\begin{split}& E\left|\frac{1}{\delta}\partial_{i}X_{0}^{x}(t,s)^{ \top}(\nabla h(s,X_{0}^{x+\delta e_{j}}(t,s))-\nabla h(s,X_{0}^{x}(t,s)))\right| ^{\alpha}\\ &\leq\left(E\left|\partial_{i}X_{0}^{x}(t,s)\right|^{2\alpha}E \frac{1}{\delta^{2\alpha}}\left|\nabla h(s,X_{0}^{x+\delta e_{j}}(t,s))-\nabla h (s,X_{0}^{x}(t,s))\right|^{2\alpha}\right)^{1/2}\end{split}\]

By Proposition A.1, the first expectation is bounded uniformly in \(s\) and \(t\). For the second term, consider

\[\begin{split}&\frac{1}{\delta^{2\alpha}}\left|\nabla h(s,X_{0}^{x+ \delta e_{j}}(t,s))-\nabla h(s,X_{0}^{x}(t,s)))\right|^{2\alpha}\\ &=\frac{1}{\delta^{2\alpha}}\left(\sum_{i=1}^{d}\left|\partial_{i }h(s,X_{0}^{x+\delta e_{j}}(t,s))-\partial_{i}h(s,X_{0}^{x}(t,s))\right|^{2} \right)^{\alpha}\\ &\stackrel{{(i)}}{{\leq}}\frac{1}{\delta^{2\alpha}} \left(\left|X_{0}^{x+\delta e_{j}}(t,s)-X_{0}^{x}(t,s)\right|^{2}\sum_{i=1}^{ d}\left|\nabla\partial_{i}h(s,\xi_{i}X_{0}^{x+\delta e_{j}}(t,s)+(1-\xi_{i})X_{0}^{x}(t,s)) \right|^{2}\right)^{\alpha}\\ &=\frac{1}{\delta^{2\alpha}}\left|X_{0}^{x+\delta e_{j}}(t,s)-X_{ 0}^{x}(t,s)\right|^{2\alpha}\left|H[h](s,\xi_{i}X_{0}^{x+\delta e_{j}}(t,s)+( 1-\xi_{i})X_{0}^{x}(t,s))\right|^{2\alpha}\\ &\stackrel{{(ii)}}{{\leq}}\frac{c_{h}^{2\alpha}}{ \delta^{2\alpha}}\left|X_{0}^{x+\delta e_{j}}(t,s)-X_{0}^{x}(t,s)\right|^{2 \alpha}(|X_{0}^{x+\delta e_{j}}(t,s)-X_{0}^{x}(t,s)|+|X_{0}^{x}(t,s)|+1)^{2 \alpha m}\end{split}\]

where \((i)\) follows from the mean value theorem with r.v. \(\xi_{i}\in[0,1]\), and \((ii)\) applies Assumption 4. Therefore, we have that

\[\begin{split}& E\frac{1}{\delta^{2\alpha}}\left|\nabla h(s,X_{0}^{x+ \delta e_{j}}(t,s))-\nabla h(s,X_{0}^{x}(t,s)))\right|^{2\alpha}\\ &\leq CE\frac{1}{\delta^{2\alpha}}\left|X_{0}^{x+\delta e_{j}}(t, s)-X_{0}^{x}(t,s)\right|^{2\alpha(m+1)}\\ &\quad+C\left(E\left[(|X_{0}^{x}(t,s)|+1)^{4\alpha m}\right]E \frac{1}{\delta^{4\alpha}}\left|X_{0}^{x+\delta e_{j}}(t,s)-X_{0}^{x}(t,s) \right|^{4\alpha}\right)^{1/2}\\ &\leq C\left[\delta^{2\alpha m}l_{2\alpha(m+1)}^{2\alpha(m+1)}+C( 1+|x|)^{2\alpha m}\right]\end{split}\]

where the last inequality follows from Proposition A.1. This is uniformly bounded in \(\delta\) as \(\delta\to 0\), showing U.I. for the first term in (D.6).

For the second term in (D.6), we consider

\[E\left|\frac{1}{\delta}\nabla h(s,X_{0}^{x+\delta e_{j}}(t,s))^{ \top}(\partial_{i}X_{0}^{x+\delta e_{j}}(t,s)-\partial_{i}X_{0}^{x}(t,s))\right| ^{\alpha}\] \[\leq\left(E\left|\nabla h(s,X_{0}^{x+\delta e_{j}}(t,s))\right|^{2 \alpha}\cdot E\frac{1}{\delta^{2\alpha}}\left|(\partial_{i}X_{0}^{x+\delta e_{j }}(t,s)-\partial_{i}X_{0}^{x}(t,s))\right|^{2\alpha}\right)^{1/2}\] \[\leq l_{2\alpha}^{\alpha}c_{h}^{\alpha}\left(E(|X_{0}^{x+\delta e_ {j}}(t,s)|+1)^{2\alpha m}\right)^{1/2}\] \[\leq C(b_{2\alpha m}^{2\alpha m}(|x+\delta e_{j}|+1)^{2\alpha m}+ 1)^{1/2}\]

which is also uniformly bounded in \(\delta\) as \(\delta\to 0\).

Therefore, exchanging the limits in (D.6), we obtain

\[\partial_{x_{j}}\partial_{x_{i}}Eh(s,X_{0}(t,s,x))=E\partial_{i}X_{0}^{x}(t,s )^{\top}H[h](s,X_{0}^{x}(t,s))\partial_{j}X_{0}^{x}(t,s)+\nabla h(s,X_{0}^{x}( t,s))^{\top}\partial_{j}\partial_{i}X_{0}^{x}(t,s)).\]

Moreover, by setting \(\alpha=1\) and taking the limit as \(\delta\to 0\) in the preceding derivations, we see that

\[E|\partial_{x_{j}}\partial_{x_{i}}h(s,X_{0}(t,s,x))|\leq C(|x|+1)^{m}.\]

where the constant \(C\) is uniform in \(s\) and \(t\).

## Appendix E Proof of Theorem 3'

From (A.1), we see that

\[E\int_{0}^{T}\nabla_{\theta}L_{0}V_{0}(t,X_{0}^{x}(0,t))dt=E\int_{0}^{T}\nabla _{\theta}\mathcal{L}_{0}v_{0}(t,X_{0}^{x}(0,t))dt.\]

Moreover, since \(\tau\) is independent of \(\mathcal{F}\),

\[E\int_{0}^{T}\nabla_{\theta}L_{0}V_{0}(t,X_{0}^{x}(0,t))dt =T\int_{0}^{T}E[\nabla_{\theta}L_{0}V_{0}(\tau,X_{0}^{x}(0,\tau)) |\tau=t]\frac{1}{T}dt\] \[=TE[\nabla_{\theta}L_{0}V_{0}(\tau,X_{0}^{x}(0,\tau))|\tau]\] \[=ET\nabla_{\theta}L_{0}V_{0}(\tau,X_{0}^{x}(0,\tau))\]

Therefore, by Theorem 1', \(ED(x)=\nabla_{\theta}v_{0}(0,x)\).

For the variance, we consider

\[E|T\nabla_{\theta}L_{0}V_{0}(\tau,X_{0}^{x}(0,\tau))|^{2} =\int_{0}^{T}E[|T\nabla_{\theta}L_{0}V_{0}(t,X_{0}^{x}(0,t))|^{2}| \tau=t]\frac{1}{T}dt\] \[=TE\int_{0}^{T}|\nabla_{\theta}L_{0}V_{0}(t,X_{0}^{x}(0,t))|^{2}dt\] \[\leq C\int_{0}^{T}E|\nabla_{\theta}\mu_{0}|^{2}|Z(t,X_{0}^{x}(0,t ))|^{2}+|\nabla_{\theta}a_{0}|^{2}|H(t,X_{0}^{x}(0,t))|^{2}dt\] \[\leq C\frac{1}{T}\int_{0}^{T}\left(E|\nabla_{\theta}\mu_{0}|^{4}E |Z(t,X_{0}^{x}(0,t))|^{4}\right)^{1/2}+\left(E|\nabla_{\theta}a_{0}|^{4}|H(t,X _{0}^{x}(0,t))|^{4}\right)^{1/2}dt\] \[\leq C\left(\frac{1}{T}\int_{0}^{T}E|\nabla_{\theta}\mu_{0}|^{4} dt\cdot\frac{1}{T}\int_{0}^{T}E|Z(t,X_{0}^{x}(0,t))|^{4}dt\right)^{1/2}\] \[\quad+C\left(\frac{1}{T}\int_{0}^{T}E|\nabla_{\theta}a_{0}|^{4} dt\cdot\frac{1}{T}\int_{0}^{T}E|H(t,X_{0}^{x}(0,t))|^{4}dt\right)^{1/2}\]

By (B.11) and (B.13) with \(\alpha=2\),

\[\frac{1}{T}\int_{0}^{T}E|\nabla_{\theta}\mu_{0}|^{4}dt\leq\frac{1}{T}\sup_{ \theta\in\Theta}\frac{1}{|\theta|^{4}}\int_{0}^{T}\kappa_{\theta,0}^{4}(s)ds \sup_{s\in[0,T]}E(|X_{0}^{x}(s)|+1)^{4}\leq C(|x|+1)^{4}\]and similarly

\[\frac{1}{T}\int_{0}^{T}E|\nabla_{\theta}a_{0}|^{4}dt\leq C(|x|+1)^{8}.\]

By definition and Proposition A.1, we have that

\[E|Z(t,X_{0}^{x}(0,t))|^{4} \leq CE\int_{t}^{T}|\nabla\rho_{0}|^{4}|\nabla X_{0}^{x}(t,r)|^{4} dr+|\nabla g_{0}|^{4}|\nabla X_{0}^{x}(t,T)|^{4}\] \[\leq CE\int_{t}^{T}(|X_{0}^{x}(t,r)|+1)^{4m}|\nabla X_{0}^{x}(t,r) |^{4}dr+(|X_{0}^{x}(t,r)|+1)^{4m}|\nabla X_{0}^{x}(t,T)|^{4}\] \[\leq 2CT\sup_{r\in[0,T]}E(|X_{0}^{x}(t,r)|+1)^{4m}|\nabla X_{0}^{x }(t,r)|^{4}\] \[\leq C(|x|+1)^{4m}.\]

Similarly, \(E|H(t,X_{0}^{x}(0,t))|^{4}\leq C(|x|+1)^{4m}\). These calculations imply that

\[E|T\nabla_{\theta}L_{0}V_{0}(\tau,X_{0}^{x}(0,\tau))|^{2}\leq C(|x|+1)^{2m+4}.\]

For the reward rate and terminal reward terms, we recall Assumption 10 with the additional Assumption that \(\alpha>2\). Note that since \(\alpha>2\), for

\[|\rho_{\theta}(t,x)-\rho_{0}(t,x)|^{2} =|\rho_{\theta}(t,x)-\rho_{0}(t,x)|^{\alpha\cdot\frac{2}{\alpha}}\] \[\leq\kappa_{\theta,0}^{\alpha}(s)^{2/\alpha}(|x|+1)^{\alpha}.\]

So, we have that

\[E\int_{0}^{T}|\nabla_{\theta}\rho_{0}|^{2}dt \stackrel{{(i)}}{{\leq}}\lim_{\theta\to 0}E\int_{0}^{T} \frac{1}{|\theta|^{2}}|\rho_{\theta}-\rho_{0}|^{2}dt\] \[\leq\sup_{\theta\in 0}\int_{0}^{T}E\frac{1}{|\theta|^{2}}|\rho_{ \theta}-\rho_{0}|^{2}dt\] \[\leq\sup_{\theta\in 0}\int_{0}^{T}\frac{1}{|\theta|^{2}}\kappa_{ \theta,0}^{\alpha}(s)^{2/\alpha}dt\sup_{t\in[0,T]}E(|X_{0}^{x}(0,t)|+1)^{2m}\] \[\stackrel{{(ii)}}{{\leq}}\left(\sup_{\theta\in 0}\int_{0}^{T}\frac{1}{|\theta|^{2}}\kappa_{ \theta,0}^{\alpha}(s)dt\right)^{2/\alpha}C(|x|+1)^{2m}\] \[\leq C(|x|+1)^{2m}\]

where \((i)\) uses \(\alpha>0\) so that the integrand is U.I. in \(\theta\in\Theta\) (see (B.11) for a similar proof), and \((ii)\) uses Jensen's inequality with \(2/\alpha<1\). The same holds for the terminal reward term, with \(\kappa_{\theta,\theta^{\prime}}^{\alpha}=\ell^{\alpha}|\theta-\theta^{\prime} |^{\alpha}\) integrable.

Therefore, we conclude that \(\text{Var}(|D(x)|)\leq E|D(x)|^{2}\leq C(|x|+1)^{2m+4},\) where \(C\) can be dependent on other parameters but not \(x\).

## Appendix F Supplementary Materials for Section 4

### Calculating the Estimators

**The Generator Gradient Estimator:** We compute

\[\partial_{i}v(t,x) =E\int_{t}^{T}\partial_{x_{i}}\rho_{\theta}(s,X_{\theta}^{x}(t,s) )ds+\partial_{x_{i}}g(X_{\theta}^{x}(t,T))\] \[=E\int_{t}^{T}u_{\theta}(s,X_{\theta}^{x}(t,s))^{\top}(R+R^{\top} )\nabla u_{\theta}(s,X_{\theta}^{x}(t,s))\partial_{i}X_{\theta}^{x}(t,s)ds\] \[\quad+\int_{t}^{T}X_{\theta}^{x}(t,s)^{\top}(Q+Q^{\top})\partial_ {i}X_{\theta}^{x}(t,s)ds+X_{\theta}^{x}(t,T)^{\top}(Q_{T}+Q_{T}^{\top})\partial _{i}X_{\theta}^{x}(t,T).\]Here \(\partial_{i}X^{x}_{\theta}(t,s)\) is a column vector. So, replacing it by the Jacobian will yield a row vector. Following the definition of \(Z\) in (2.6), we define

\[Z(t,x)^{\top} :=\int_{t}^{T}u_{\theta}(s,X^{x}_{\theta}(t,s))(R+R^{\top})\nabla u _{\theta}(s,X^{x}_{\theta}(t,s))\nabla X^{x}_{\theta}(t,s)ds\] (F.1) \[\quad+\int_{t}^{T}X^{x}_{\theta}(t,s)^{\top}(Q+Q^{\top})\nabla X^ {x}_{\theta}(t,s)ds+X^{x}_{\theta}(t,T)^{\top}(Q_{T}+Q_{T}^{\top})\nabla X^{x} _{\theta}(t,T).\]

Here, the derivative process \(\nabla X^{x}_{\theta}\) satisfies the following ODE with random coefficients:

\[\partial_{i}X^{x}_{\theta}(t,s)=e_{i}+\int_{t}^{s}(A+B\nabla u_{\theta}(r,X^{x }_{\theta}(t,r)))\partial_{i}X^{x}_{\theta}(t,r)dr;\]

or in matrix form:

\[\nabla X^{x}_{\theta}(t,s)=I+\int_{t}^{s}(A+B\nabla u_{\theta}(r,X^{x}_{\theta }(t,r)))\nabla X^{x}_{\theta}(t,r)dr.\]

Therefore, in this setting, our generator gradient estimator in (2.9) is

\[D_{i}(x)=T\partial_{\theta_{i}}u_{\theta}(\tau,X^{x}_{\theta}(\tau))^{\top}B^ {\top}Z(\tau,X^{x}_{\theta}(\tau))+Tu_{\theta}(\tau,X^{x}_{\theta}(\tau))^{\top }(R+R^{\top})\partial_{\theta_{i}}u_{\theta}(\tau,X^{x}_{\theta}(\tau))\] (F.2)

where \(Z\) is given by (F.1). As explained in (2.9), we also randomize the integral corresponding to the gradient of the reward rate \(\nabla_{\theta}\rho_{0}\).

**The Pathwise Differentiation Estimator:** From (1.3), we construct the following IPA estimator that randomizes the time integral

\[\widetilde{D}_{i}(x) =Tu_{\theta}(\tau,X^{x}_{\theta}(\tau))(R+R^{\top})\nabla u_{ \theta}(\tau,X^{x}_{\theta}(\tau))\partial_{\theta_{i}}X^{x}_{\theta}(\tau)+TX ^{x}_{\theta}(\tau)^{\top}(Q+Q^{\top})\partial_{\theta_{i}}X^{x}_{\theta}(\tau)\] \[\quad+Tu_{\theta}(\tau,X^{x}_{\theta}(\tau))^{\top}(R+R^{\top}) \partial_{\theta_{i}}u_{\theta}(\tau,X^{x}_{\theta}(\tau))+X^{x}_{\theta}(T)^ {\top}(Q_{T}+Q_{T}^{\top})\partial_{\theta_{i}}X^{x}_{\theta}(T).\]

Here, the pathwise derivatives \(\partial_{\theta_{i}}X^{x}_{\theta}(t)\) is the solution the following ODE with random coefficient:

\[\partial_{\theta_{i}}X^{x}_{\theta}(t)=\int_{0}^{t}(A+B\nabla u_{\theta}(s,X^{ x}_{\theta}(s)))\partial_{\theta_{i}}X^{x}_{\theta}(s)+B\partial_{\theta_{i}}u_{ \theta}(s,X^{x}_{\theta}(s))ds.\] (F.3)

### Numerical Experimentation Details

We conducted the computation time and variance comparison for both estimators using PyTorch. The computation time data was generated on a system equipped with a PCIE version of Nvidia Tesla V100 GPU, featuring 32GB of VRAM. Additionally, the system includes a 2-core CPU and 16GB of RAM, which are used to format and store data. The primary computational tasks are handled by the GPU.

The data for Table 2 is produced as follows. For each \(n\), we produce 400 i.i.d. GG and PD estimators \(\left\{D^{(j)}(x_{0}),\widetilde{D}^{(j)}(x_{0})\in\mathbb{R}^{n}:j=1,\dots,4 00\right\}\). Let

\[\hat{\sigma}_{\text{GG},i} :=\frac{1}{20}\sum_{j=1}^{400}\left(D^{(j)}_{i}(x_{0})-\frac{1}{4 00}\sum_{j=1}^{400}D^{(j)}_{i}(x_{0})\right)^{2},\] \[\hat{\sigma}_{\text{PD},i} :=\frac{1}{20}\sum_{j=1}^{400}\left(\widetilde{D}^{(j)}_{i}(x_{0} )-\frac{1}{400}\sum_{j=1}^{400}\widetilde{D}^{(j)}_{i}(x_{0})\right)^{2}.\]

The "Avg SE of GG" and "Avg SE of PD" entries record

\[\frac{1}{n}\sum_{i=1}^{n}\hat{\sigma}_{\text{GG},i}\quad\text{and}\quad\frac{1 }{n}\sum_{i=1}^{n}\hat{\sigma}_{\text{GG},i},\] (F.4)

respectively. The "Avg SE ratios" compute

\[\frac{1}{n}\sum_{k=1}^{n}\frac{\hat{\sigma}_{\text{GG},i}}{\hat{\sigma}_{\text{ PD},i}}.\] (F.5)The numerical values used for the matrices, initial conditions, and network initializations for the SDE models can be found in the supplied code.

We further analyze variance by plotting histograms of the distribution formed by the standard errors of the coordinates of the estimators, as shown in Figure 2. The standard error distribution of the pathwise differentiation method exhibits a heavier tail compared to our proposed generator gradient estimator. This aligns with the superior variance performance of our estimator demonstrated in Table 2. Figure 2 also provides insights into the confidence intervals in Figure 0(b), which are barely visible due to high confidence levels. In particular, the generator gradient estimator has tighter confidence intervals in Figure 0(b).

Figure 2: Histograms comparison of the distribution formed by the standard errors of coordinates of the estimators. These plots use the same data as that produces Table 2.

Experiments on SDEs with Non-Differentiable Parameters

### CIR Model

In this section, we use the Cox-Ingersoll-Ross (CIR) diffusion as an example to test the validity of the proposed generator gradient estimator when the differentiability assumptions of the coefficients are violated. Specifically, consider the one-dimensional process:

\[X_{\theta}^{x}(t,s)=x+\int_{t}^{s}(\theta-X_{\theta}^{x}(t,r))dr+\int_{t}^{s} \sqrt{X_{\theta}^{x}(t,r)}dB(r)\] (G.1)

for \(t,s\in[0,2]\), where \(x,\theta>0\). Note that the volatility \(\sigma(t,x)=\sqrt{x}\) is not differentiable at 0, though it is \(C^{\infty}\) for \(x>0\). A unique strong solution to (G.1) always exists. Moreover, if \(\theta\geq 1/2\), then \(X_{\theta}(t)>0\) for all \(t\in[0,2]\) almost surely.

We consider the following value function:

\[v_{\theta}(t,x):=E\left[\int_{t}^{T}X_{\theta}^{x}(t,s)ds\right].\] (G.2)

We aim to estimate the gradient \(\nabla_{\theta}v_{\theta}(0,x)\) evaluated at \(x=0.1\) for multiple values of \(\theta\).

Since the pathwise differentiation estimator also suffers from non-differentiability issues, we validate the generator gradient (GG) estimator by comparing it with the finite difference (FD) estimator. Specifically, the FD estimator computes

\[\frac{1}{h}\Delta_{h}(\theta):=\frac{1}{h}\left[v_{\theta+\frac{h}{2}}(0,x)-v_ {\theta-\frac{h}{2}}(0,x)\right]\] (G.3)

using Monte Carlo simulation of the SDE (G.1) and the value function. In this context, the FD estimator should consistently estimate the gradient evaluated at any \(\theta>0\). The GG estimator is produced from (2.9). We use the Euler scheme to simulate the SDEs and the derivative processes. To avoid numerical issues when the Euler discretization of the CIR process crosses 0, we take the absolute value of the discretized process at each time step.

Table 3 summarizes the estimated value and confidence interval for both the GG and FD estimators. We note that a bias of \(O(h^{2})\) is present in the FD case. When \(\theta\geq 1/2\), we observe that even though Assumptions 1 and 3 are violated, the GG estimator produces results consistent with the FD estimator. This suggests the validity of the GG estimator even when Assumptions 1 and 3 don't hold. This consistency occurs because, in this case, the derivative processes are still well-defined up to the first time \(X_{\theta}^{x}(t)\) hits 0, which does not happen when \(\theta>1/2\).

However, when \(\theta<1/2\) (cases highlighted in blue in Table 3), the sample paths of the SDE (G.1) can reach 0. Although the statistics in Table 3 appear consistent, we observe a significant increase in the variance of the GG estimator as \(\theta\) decreases. This increase may indicate that the GG estimator is not consistently estimating the gradient in these cases.

\begin{table}
\begin{tabular}{l l l l} \hline \hline Value of \(\theta\) & 4 & 2 & 0.55 \\ \hline GG \(\pm\)\(95\%\) CI & \(1.135\pm 0.0011\) & \(1.135\pm 0.0012\) & \(1.134\pm 0.0019\) \\ FD \(\pm\)\(95\%\) CI & \(1.095\pm 0.064\) & \(1.097\pm 0.046\) & \(1.116\pm 0.026\) \\ \hline \

### SDE with ReLU Drift

In this section, we use the following SDE with ReLU drift as an example to test the validity of the proposed generator gradient estimator:

\[X_{\theta}^{x}(t,s)=x+\int_{t}^{s}(\mathrm{ReLU}(\theta X_{\theta}^{x}(t,r))+1) dr+\int_{t}^{s}dB(r)\] (G.4)

for \(t,s\in[0,2]\), where \(\theta>0\) and we choose \(x=-0.1\). Note that the \(+1\) in the drift makes it always positive. So, starting from \(-0.1\), the process should cross 0 (where the drift is non-differentiable) before time \(2\) with high probability.

With \(v_{\theta}\) defined in (G.2), we aim to estimate the gradient \(\nabla_{\theta}v_{\theta}(0,x)\) evaluated at \(x=0.1\) for multiple values of \(\theta\) using the GG and FD (defined in (G.3)) estimators.

Table 4 summarizes the estimated values and confidence intervals for both the GG and FD estimators. Note that a bias of \(O(h^{2})\) is present in the FD case. Despite violations of Assumptions 1 and 3, the GG estimator still produces consistent results compared to the FD estimator. We note that in this context, it should be possible to establish the existence and integrability of the derivative processes for any value of \(\theta\).

\begin{table}
\begin{tabular}{l l l l} \hline \hline Value of \(\theta\) & 2 & 1 & 0.5 \\ \hline GG \(\pm\)\(95\%\) CI & \(14.91\pm 0.031\) & \(4.087\pm 0.008\) & \(2.300\pm 0.005\) \\ FD \(\pm\)\(95\%\) CI & \(14.78\pm 0.570\) & \(4.131\pm 0.192\) & \(2.394\pm 0.127\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Statistics for \(10^{6}\)-sample averaged GG and FD estimators. For the FD estimator, we choose \(h=0.05\) in (G.3), resulting in a bias of \(O(h^{2})\).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The context and contributions of this paper are clearly and accurately stated in the abstract and the introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We acknowledge the specific parts of the assumptions that could potentially be stronger than what is necessary. See, for example, the remarks and discussion following Assumption 1 and 2. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes]

Justification: All theorems, propositions, and lemmas are either proved within the paper or cited from other works. The assumptions are clearly labeled and discussed. We also provided intuitive justification for our theoretical results in Section 2.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes]

Justification: The code attached to the submission, if run properly with system configurations similar to that indicated in the appendix, will produce very similar qualitative results as that presented in the paper. However, as the neural networks are randomly initialized, the quantitative outcome might differ. Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The code is submitted with the paper. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer:[Yes] Justification: The parameters of the control system in Section 4 are presented in the code. There is no hyperparameter that needs fine-tuning. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: Although we do report data involving error bars (see 1b), these are barely visible, and have no qualitative significance to the numerical results. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: This is fully specified in Appendix F. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: We conform to the NeurIPS Code of Ethics. Anonymity is preserved in this submission. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This work makes methodological contributions to the SDE gradient estimation problem. There is no direct social impact associated with this work. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer:[NA] Justification: There is no high-risk data in this paper. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA]. Justification: The paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.

* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.