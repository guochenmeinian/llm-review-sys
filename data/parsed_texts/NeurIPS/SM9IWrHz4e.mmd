# Achieving Tractable Minimax Optimal Regret in Average Reward MDPs

 Victor Boone

victor.boone@univ-grenoble-alpes.fr

Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LIG, 38000 Grenoble, France

&Zihan Zhang

zz5478@princeton.edu

Princeton University

###### Abstract

In recent years, significant attention has been directed towards learning average-reward Markov Decision Processes (MDPs). However, existing algorithms either suffer from sub-optimal regret guarantees or computational inefficiencies. In this paper, we present the first _tractable_ algorithm with minimax optimal regret of \(\widetilde{\mathsf{O}}\left(\sqrt{\mathsf{sp}\left(h^{*}\right)SAT}\right)\),1 where \(\mathsf{sp}\left(h^{*}\right)\) is the span of the optimal bias function \(h^{*}\), \(S\times A\) is the size of the state-action space and \(T\) the number of learning steps. Remarkably, our algorithm does not require prior information on \(\mathsf{sp}\left(h^{*}\right)\).

Footnote 1: \(\widetilde{\mathsf{O}}(\cdot)\) hides logarithmic factors of \((S,A,T)\).

Our algorithm relies on a novel subroutine, **P**rojected **M**itigated **E**x**tended **V**alue **I**teration (**P**MEVI), to compute bias-constrained optimal policies efficiently. This subroutine can be applied to various previous algorithms to improve regret bounds.

## 1 Introduction

Reinforcement learning (RL) Burnetas and Katehakis (1997); Sutton and Barto (2018) has become a popular approach for solving complex sequential decision-making tasks and has recently achieved notable advancements in diverse fields of application. RL problems are generally formulated with Markov Decision Processes (MDPs) Puterman (1994), where a learning agent seeks to maximize the rewards that are gathered by interacting with an unknown environment.

This paper focuses on average reward MDPs where the learning agent must maximize the sum of rewards in the long run without any reset mechanism. In this setting, the proper balancing between exploration (i.e., playing sub-optimally to learn the unknown environment) and exploitation (i.e., planning optimally according to the current knowledge), usually known as the _exploration-exploitation trade-off_, is key to learn efficiently. The measure of learning performance that we adopt throughout is the _regret_, that compares the aggregate rewards collected by the learning agent during the learning process to the expected performance of an omniscient agent that knows everything in advance. The seminal work of Auer et al. (2009) provides a _minimax_ regret lower bound of \(\Omega\left(\sqrt{DSAT}\right)\), where \(D\) is the diameter (the maximal distance between two different states), \(S\) the number of states, \(A\) the number of actions and \(T\) the learning horizon. They also provide an algorithm achieving regret \(\widetilde{\mathsf{O}}\left(\sqrt{D^{2}S^{2}AT}\right)\), where \(\widetilde{\mathsf{O}}(-)\). Ever since Auer et al. (2009), many works have been devoted to close the gap between the regret lower and upper bounds in the average reward setting Auer et al. (2009); Bartlett and Tewari (2009); Filippi et al. (2010); Talebi and Maillard (2018); Fruit et al. (2018, 2020); Bourel et al. (2020); Zhang and Ji (2019); Ouyang et al. (2017); Agrawal and Jia (2023);Abbasi-Yadkori et al. (2019); Wei et al. (2020) and more. Subsequent works Fruit et al. (2018); Zhang and Ji (2019) refined the minimax regret lower bound to \(\Omega\left(\sqrt{\mathrm{sp}\left(h^{*}\right)SAT}\right)\) where \(\mathrm{sp}\left(h^{*}\right)\) is the span of the bias function, which is the maximal gap of the long-term accumulative rewards starting from two different states. The difference is significant, since \(\mathrm{sp}\left(h^{*}\right)\leq D\) and the gap between the two can be arbitrarily large. However, no existing work achieves the following three requirements simultaneously:

1. The method achieves minimax optimal regret guarantees \(\widetilde{\mathrm{O}}\left(\sqrt{\mathrm{sp}\left(h^{*}\right)SAT}\right)\);
2. The proposed method is tractable;
3. No prior knowledge on the model is required.

Most algorithms simply fail to achieve minimax optimal regret, and the only method achieving it Zhang and Ji (2019) is intractable because it relies an oracle to solve difficult optimization problems along the learning process. Naturally, we raise the question of whether these three requirements can be met all at once:

_Is there a tractable algorithm with \(\widetilde{\mathrm{O}}\left(\sqrt{\mathrm{sp}\left(h^{*}\right)SAT}\right)\) minimax regret without prior knowledge?_

Contributions.In this paper, we answer the above question affirmatively, by proposing a polynomial time algorithm with regret guarantees \(\widetilde{\mathrm{O}}\left(\sqrt{\mathrm{sp}\left(h^{*}\right)SAT}\right)\) for average-reward MDPs. Our method can further incorporate almost arbitrary prior bias information \(\mathcal{H}_{*}\subseteq\mathbf{R}^{S}\) to further improve its regret.

**Theorem 1** (Informal).: _Provided that the confidence region used by \(\mathtt{PMEVI-DT}\) satisfy mild regularity conditions (see Assumption 1-3), then for every weakly communicating model \(M\) with \(\mathrm{sp}(h^{*})\leq T^{1/5}\) and \(\mathrm{sp}(h^{*})\in\mathcal{H}_{*}\), \(\mathtt{PMEVI-DT}(\mathcal{H}_{*},\delta,T)\) achieves regret:_

\[\mathrm{O}\left(\sqrt{\mathrm{sp}(h^{*})SAT}\log\left(\tfrac{SAT}{\delta} \right)\right)+\mathrm{O}\left(\mathrm{sp}(h^{*})S^{\frac{5}{2}}A^{\frac{1}{2 }}T^{\frac{9}{50}}\log^{2}\left(\tfrac{SAT}{\delta}\right)\right)\]

_with probability \(1-26\delta\). Moreover, if \(\mathtt{PMEVI-DT}\) runs with the same confidence regions that \(\mathtt{UCRL2}\)Auer et al. (2009) on a communicating environment, it has a time complexity \(\mathrm{O}(\mathit{DS}^{3}\mathit{AT})\)._

Taking \(\delta=\sqrt{1/T}\), we also obtain a \(\widetilde{\mathrm{O}}\left(\sqrt{\mathrm{sp}\left(h^{*}\right)SAT}\right)\) regret bound in expectation. The geometry of the prior bias region \(\mathcal{H}_{*}\) that \(\mathtt{PMEVI-DT}\) can support is discussed later (see Assumption 4). It can be taken trivial with \(\mathcal{H}_{*}=\mathbf{R}^{S}\) to obtain a completely prior-less algorithm.

To the best of our knowledge, \(\mathtt{PMEVI-DT}\) is the first tractable algorithm with minimax optimal regret bounds (up to logarithmic factors). The algorithm does not necessitate any prior knowledge of \(\mathrm{sp}\left(h^{*}\right)\), thus circumventing the potentially high cost associated with learning \(\mathrm{sp}\left(h^{*}\right)\). On the technical side, a key novelty of our method is the subroutine named \(\mathtt{PMEVI}\) (see Algorithm 2) that improves and can replace \(\mathtt{EVI}\)Auer et al. (2009) in any algorithm that relies on it Auer et al. (2009); Fruit et al. (2018); Filippi et al. (2010); Fruit et al. (2020); Bourel et al. (2020) to boost its performance and achieve minimax optimal regret.

Related works on average reward MDPs.For communicating MDPs, the notable work of Auer et al. (2009) proposes the famous \(\mathtt{UCRL2}\) algorithm, a mature version of their prior \(\mathtt{UCRL}\)Auer and Ortner (2006), achieving a regret bound of \(\widetilde{\mathrm{O}}(\mathit{DS}\sqrt{AT})\). This paper pioneered the use _optimistic_ methods to learn MDPs efficiently. A line of papers Filippi et al. (2010); Fruit et al. (2020); Bourel et al. (2020) developed this direction by tightening the confidence region that \(\mathtt{UCRL2}\) relies on, and sharpened the analysis through the use of local properties of MDPs, such as local diameters and local bias variances. However, none of these works went beyond regret guarantees of order \(\mathit{S}\sqrt{DAT}\) and suffer from an \(\mathrm{extra}\sqrt{S}\). A parallel direction was initiated by Bartlett and Tewari (2009) with \(\mathtt{REGAL}\), obtaining regret bounds scaling with \(\mathrm{sp}(h^{*})\) instead of \(D\), and extending the regret bounds to weakly-communicating MDPs in the mean time. The computational intractability of \(\mathtt{REGAL}\) is addressed by Fruit et al. (2018) with \(\mathtt{SCAL}\), and regret guarantees are further improved by Zhang and Ji (2019) with \(\mathtt{EBF}\), eventually reaching optimal minimax regret but loosing tractability.

Another successful design approach is Bayesian-flavored sampling, derived from Thompson Sampling Thompson (1933), that usually replaces optimism. The regret guarantees of these algorithms usually stick to the Bayesian setting however Ouyang et al. (2017); Theocharous et al. (2017), although Agrawal and Jia (2023) also enjoys \(\widetilde{\mathsf{O}}(S\sqrt{DAT})\) high probability regret by coupling posterior sampling with optimism. Another line of research focuses on the study of ergodic MDPs, where the environment is such that all states are visited infinitely often under every policy. To name a few, the model-free algorithm Politex Abbasi-Yadkori et al. (2019) attains a regret of \(\widetilde{\mathsf{O}}(t_{\max})^{3}t_{\text{hit}}\sqrt{SAT}^{\frac{3}{2}}\)) where \(t_{\max}\) and \(t_{\text{hit}}\) are respectively the mixing and the hitting times of the ergodic environment. By leveraging an optimistic mirror descent algorithm, Wei et al. (2020) achieve an enhanced regret of \(\widetilde{\mathsf{O}}(\sqrt{t_{\max})^{2}t_{\text{hit}}AT}\).

We refer the readers to Table 1 for a (non-exhaustive) list of existing algorithms.

## 2 Preliminaries

We fix a finite state-action space structure \(\mathcal{X}:=\bigcup_{s\in\mathcal{S}}\{s\}\times\mathcal{A}(s)\), and denote \(\mathcal{M}\) the collection of all MDPs with state-action space \(\mathcal{X}\) and rewards supported in \([0,1]\).

Infinite-horizon MDP.An element \(M\in\mathcal{M}\) is a tuple \((\mathcal{S},\mathcal{A},p,r)\) where \(p\) is the transition kernel and \(r\) the reward function. The random state-action pair played by the agent at time \(t\) is denoted \(X_{t}\equiv(S_{t},A_{t})\), and the achieved reward is \(R_{t}\). A policy is a _deterministic_ rule \(\pi:\mathcal{S}\to\mathcal{A}\) and we write \(\Pi\) the space of policies. When coupled with a MDP \(M\in\mathcal{M}\), a policy properly defines the distribution of \((X_{t},R_{t})\) whose associated probability probability and expectation operators are denoted \(\mathbf{P}_{s}^{\pi},\mathbf{E}_{s}^{\pi}\), where \(s\in\mathcal{S}\) is the initial state. Under \(M\), a fixed policy has a reward function \(r^{\pi}(s):=r(s,\pi(s))\), a transition matrix \(P^{\pi}\), a gain \(g^{\pi}(s):=\lim\frac{1}{T}\mathbf{E}_{s}^{\pi}[R_{0}+\ldots+R_{T-1}]\) and a bias \(h^{\pi}(s):=\text{Cesaro-}\lim\mathbf{E}_{s}^{\pi}[\sum_{j=0}^{T-1}(R_{t}-g(S_ {t}))]\), that all together satisfy the Poisson equation \(h^{\pi}+g^{\pi}=r^{\pi}+P^{\pi}h^{\pi}\), see Puterman (1994). The _Bellman operator_ of the MDP is:

\[Lu(s):=\max_{a\in\mathcal{A}(s)}\left\{r(s,a)+p(s,a)u\right\}\] (1)

The _optimal gain_ is \(g^{*}(s):=\max_{\pi}g^{\pi}(s)\) and the _optimal bias_ is \(h^{*}(s):=\max\left\{h^{\pi}(s):\pi\text{ s.t. }g^{\pi}=g^{*}\right\}\).

Weakly-communicating MDPs.\(M\) is weakly-communicating Puterman (1994); Bartlett and Tewari (2009) if the state space can be divided into two sets: (1) the transient set, consisting in states that are transient under all policies; (2) the non-transient set, where every state is reachable starting from any other non-transient state. In this case, \(h^{*}\) is a span-fixpoint of \(L\) (see Puterman (1994)), i.e., \(Lh^{*}-h^{*}\in\mathbf{R}e\) where \(e\) is the vector full of ones. We write \(h^{*}\in\operatorname{Fix}(L)\). Then \(g^{*}=Lh^{*}-h^{*}\) and every policy \(\pi\) satisfies \(r^{\pi}+P^{\pi}h^{*}\leq g^{*}+h^{*}\). We accordingly define the _Bellman gaps_:

\[\Delta^{*}(s,a):=h^{*}(s)+g^{*}(s)-r(s,a)-p(s,a)h^{*}\geq 0.\] (2)

Another important concept is the _diameter_, that describes the maximal distance from one state to another state. It is given by \(D:=\sup_{s\neq s^{\prime}}\inf_{\pi}\mathbf{E}_{s}^{\pi}[\inf\{t\geq 1:S_{t}=s^{ \prime}\}]\). An MDP is said _communicating

\begin{table}
\begin{tabular}{c c c c} \hline
**Algorithm** & **Regret in \(\widetilde{\mathsf{O}}(-)\)** & **Tractable** & **Comment/Requirements** \\ \hline REGAL Bartlett and Tewari (2009) & sp (\(h^{*}\))\(S\sqrt{AT}\) & \(\times\) & knowledge of sp (\(h^{*}\)) \\ UCRL2 Auer et al. (2009) & \(DS\sqrt{AT}\) & \(\check{\sim}\) & - \\ PSRLA Agrawal and Jia (2023) & \(DS\sqrt{AT}\) & \(\check{\sim}\) & Bayesian regret \\ SCAL Fruit et al. (2018) & sp (\(h^{*}\))\(S\sqrt{AT}\) & \(\check{\sim}\) & knowledge of sp (\(h^{*}\)) \\ UCRL2B Fruit et al. (2020) & \(S\sqrt{DAT}\) & \(\check{\sim}\) & extra \(\sqrt{\log(T)}\) in upper-bound \\ UCRL3 Bouret et al. (2020) & \(D+\sqrt{T}\sum_{L,a}D_{a}^{2}L_{\text{in}}\) & \(\check{\sim}\) & \(L_{\text{in}}:=\sum_{r}\sqrt{p(s^{\prime}|s,a)(1-p(s^{\prime}|s,a))}\) \\ KL-UCRL Filippi et al. (2010); Taleb and Maillard (2018) & \(S\sqrt{DAT}\) & \(\check{\sim}\) & - \\ ERF Zhang and Ji (2019) & \(\sqrt{p}(h^{*})S\frac{\pi}{2}\) & \(\times\) & optimal, knowledge of sp (\(h^{*}\)) \\ Optimistic-Q Wei et al. (2020) & sp (\(h^{*}\))\(S\frac{\pi}{2}\) & \(\check{\sim}\) & model-free \\ UCR-AVG Zhang and Xie (2023) & \(S^{A}\text{A}^{\text{sep}}(h^{*}\sqrt{T}\) & \(\check{\sim}\) & model-free, knowledge of sp (\(h^{*}\)) \\ MDP-OMD Wei et al. (2020) & \(\sqrt{(t_{\max})^{2}t_{\text{hit}}AT}\) & \(\check{\sim}\) & ergodic \\ Politex Abbasi-Yadkori et al. (2019) & \((t_{\max})^{3}t_{\text{hit}}\sqrt{SAT}^{1}\) & \(\check{\sim}\) & model-free, ergodic \\ PMEVI-DT (**this work**) & \(\sqrt{\log\left(h^{*}\right)SAT}\) & \(\check{\sim}\) & - \\ \hline
**Lower bound** & \(\Omega\left(\sqrt{\log\left(h^{*}\right)SAT}\right)\) & - & - \\ \hline \end{tabular}
\end{table}
Table 1: Comparison of related works on RL algorithms for average-reward MDP, where \(S\times A\) is the size of state-action space, \(T\) is the total number of steps, \(D\) (\(D_{s}\)) is the (local) diameter, sp (\(h^{*}\)) \(\leq\)\(D\) is the span of the bias vector, \(t_{\max}\) is the worst-case mixing time, \(t_{\text{hit}}\) is the hitting time (i.e., the expected time cost to visit some certain state under any policy).

if its diameter \(D\) is finite, in which case \(\operatorname{sp}(h^{*})\leq\operatorname{sp}(r)D\), see Bartlett and Tewari (2009); Fruit (2019), where \(\operatorname{sp}(-)\) is the _span_ function given by \(\operatorname{sp}\left(u\right):=\max(u)-\min(u)\).

Reinforcement learning.The learner is only aware that \(M\in\mathcal{M}\) but doesn't have a clue about what \(M\) further looks like. From the past observations and the current state \(S_{t}\), the agent picks an available action \(\mathcal{A}(S_{t})\), receives a reward \(R_{t}\) and observe the new state \(S_{t+1}\). The _regret_ of the agent is:

\[\operatorname{Reg}(T):=Tg^{*}-\sum_{t=0}^{T-1}R_{t}.\] (3)

Its expected value satisfies \(\mathbf{E}[\operatorname{Reg}(T)]=\mathbf{E}[\sum_{t=0}^{T-1}\Delta^{*}(X_{t} )]+\mathbf{E}[h^{*}(S_{0})-h^{*}(S_{T})]\) and the quantity \(\sum_{t=0}^{T-1}\Delta^{*}(X_{t})\) will be referred to as the _pseudo-regret_. This paper focuses on _minimax regret guarantees_. Specifically, for \(c\geq 1\), denote \(\mathcal{M}_{c}:=\left\{M\in\mathcal{M}:\exists t^{*}\in\operatorname{Fix}( L(M)),\operatorname{sp}\left(h^{*}\right)\leq c\right\}\) the set of weakly-communicating MDPs that admit a bias function with span at most \(c\). Following Auer et al. (2009), every algorithm \(\mathbf{A}\), for all \(c>0\), we have

\[\max_{M\in\mathcal{M}_{c}}\mathbf{E}^{M,\mathbf{A}}[\operatorname{Reg}(T)]= \Omega\left(\sqrt{cSAT}\right).\] (4)

The goal of this work is to reach this lower bound with a tractable algorithm.

## 3 Algorithm Pmevi-dt

The algorithm PMEVI-DT that we present in this work is actually a general method can be applied to improve various existing algorithms Auer et al. (2009); Filippi et al. (2010); Fruit et al. (2018); Bourel et al. (2020); Tewari and Bartlett (2007). All these algorithms work episodically, by maintaining a policy \(\pi_{k}\) that drives play during a time-window \(\{t_{k},\ldots,t_{k+1}-1\}\) called an episode. An episode rule determines when \(\pi_{k}\) should be considered obsolete and defines the time \(t_{k+1}\) at which the policy is renewed. To compute \(\pi_{k}\), these algorithms follow the _optimism-in-face-of-certainty_ (OFU) design principle, by choosing \(\pi_{k}\) that achieves the largest possible gain that is plausible under their current information. This is done by building a confidence region \(\mathcal{M}_{t}\subseteq\mathcal{M}\) for the hidden model \(M\), then searching for a policy \(\pi\) solving the optimization problem:

\[g^{*}(\mathcal{M}_{t}):=\sup\left\{g^{\pi}(\mathcal{M}_{t}):\pi\in\Pi, \operatorname{sp}\left(g^{\pi}(\mathcal{M}_{t})\right)=0\right\}\text{ with }g^{\pi}(\mathcal{M}_{t}):=\sup\left\{g^{\pi}(\widetilde{M}): \widetilde{M}\in\mathcal{M}_{t}\right\}.\] (5)

The design of the confidence region \(\mathcal{M}_{t}\) varies from a work to another. Given a confidence region \((\mathcal{M}_{t})_{t\geq 0}\), OFU-algorithms work as follows: At the start of episode \(k\), the optimization problem (5) is solved, and its solution \(\pi_{k}\) is played until the end of episode. The duration of episodes can be managed in various ways, although the most popular is arguably the _doubling trick_ (DT), that essentially waits until a state-action pair is about to double the visit count it had at the beginning of the current episode (see Algorithm 1).

Notations.In the rest of this section, we use \(\hat{p}_{t}(s,a)\) (and \(\hat{r}_{t}(s,a)\)) to denote the empirical transition (and reward) of the latest doubling update before the \(t\)-th step, and further denote \(\hat{M}_{t}:=(\hat{r}_{t},\hat{p}_{t})\).

Extended Bellman operators and EVI.To solve (5) efficiently, the celebrated work Auer et al. (2009) introduce the _extended value iteration_ algorithm (EVI), that can be run whenever \(\mathcal{M}_{t}\) is a \((s,a)\)-rectangular confidence region, meaning that \(\mathcal{M}_{t}\equiv\prod_{s,a}(\mathcal{R}_{t}(s,a)\times\mathcal{P}_{t}(s,a))\) where \(\mathcal{R}_{t}(s,a)\) and \(\mathcal{P}_{t}(s,a)\) are respectively the confidence region for \(r(s,a)\) and \(p(s,a)\) after \(t\) learning steps. EVI is the algorithm computing the sequence defined by:

\[v_{i+1}(s)\equiv\mathcal{L}_{t}v_{i}(s):=\max_{a\in\mathcal{M}_{t}(s)}\max_{ \tilde{r}(s,a)\in\mathcal{R}_{t}(s,a)}\max_{\tilde{r}(s,a)\in\mathcal{P}_{t}( s,a)}(\tilde{r}(s,a)+\tilde{p}(s,a)\cdot v_{i})\] (6)

until \(\operatorname{sp}\left(v_{i+1}-v_{i}\right)<\epsilon\) where \(\epsilon>0\) is the numerical precision. When the process stops, it is known that any policy \(\pi\) such that \(\pi(s)\) achieves \(\mathcal{L}_{t}v_{i}\) in (6) satisfies \(g^{\pi}(\mathcal{M}_{t})\geq g^{*}(\mathcal{M})-\epsilon\), hence is nearly optimistically optimal. This process gets its name from the observation that \(\mathcal{L}_{t}\) is the Bellman operator of \(\mathcal{M}_{t}\) seen as a MDP, hence EVI is just the Value Iteration algorithm Puterman (1994) ran in \(\mathcal{M}_{t}\). A choice of action from \(s\in\mathcal{S}\) in \(\mathcal{M}_{t}\) consists in (1) a choice of action \(a\in\mathcal{A}(s)\), (2) a choice of reward \(\tilde{r}(s,a)\in\mathcal{R}_{t}(s,a)\) and (3) a choice of transition \(\tilde{p}(s,a)\in\mathcal{P}_{t}(s,a)\); It is an _extended_ version of \(\mathcal{A}(s)\).

Towards Projected Mitigated Evi.Obviously, the regret of an OFU-algorithm is directly related to the quality of the confidence region \(\mathcal{M}_{t}\). That is why most previous works tried to approach the regret lower bound \(\sqrt{DSAT}\) of Auer et al. (2009) by refining \(\mathcal{M}_{t}\). The older works of Auer et al. (2009), Bartlett and Tewari (2009), Filippi et al. (2010) have been improved with a variance aware analysis Talebi and Maillard (2018); Fruit et al. (2018, 2020), Bourel et al. (2020) that essentially make use of tightened kernel confidence regions \(\mathcal{P}_{t}\). While all these algorithms successively reduce the gap between the regret upper and lower bounds, they fail to achieve optimal regret \(\sqrt{DSAT}\). Meanwhile, the EBF algorithm of Zhang and Ji (2019) is minimax optimal but (1) the algorithm is intractable because it relies on an oracle to retrieve optimistically optimal policies and (2) needs prior information on the bias function. Nonetheless, the method of Zhang and Ji (2019) strongly suggests that inferring bias information from the available data is key to achieve minimax optimal regret.

Rather surprisingly and in opposition to this previous line of work, our work suggests that the choice of the confidence region \(\mathcal{M}_{t}\) has little importance. Instead, our algorithm takes an arbitrary (well-behaved) confidence region in, infer bias information similarly to Zhang and Ji (2019) and makes use of it to refine the extended Bellman operator (6) associated to the input confidence region. Our algorithm can further take arbitrary prior information (possibly none) on the bias vector to tighten its bias confidence region. The pseudo-code given in Algorithm 1 is the high level structure our algorithm \(\mathtt{PMEVI-DT}\). In the next Section 3.1, we explain how (6) is refined using bias information.

``` Parameters: Bias prior \(\mathcal{H}_{t}\), horizon \(T\), a system of confidence regions \(t\mapsto\mathcal{M}_{t}\)
1:for\(k=1,2,\ldots\)do
2: Set \(t_{k}\gets t\), update confidence region \(\mathcal{M}_{t}\);
3:\(\mathcal{H}^{\prime}_{t}\leftarrow\mathtt{BiasEstimation}(\mathcal{F}_{t}, \mathcal{M}_{t},\delta)\):
4:\(\mathcal{H}_{t}\leftarrow\mathcal{H}_{t}\cap\{u:\operatorname{sp}(u)\leq T^{1 /5}\}\cap\mathcal{H}^{\prime}_{t}\);
5:\(\Gamma_{t}\leftarrow\mathtt{BiasProjection}(\mathcal{H}_{t},-)\);
6:\(\beta_{t}\leftarrow\mathtt{VarianceApprox}(\mathcal{H}^{\prime}_{t},\mathcal{ F}_{t})\);
7:\(\mathfrak{h}_{k}\leftarrow\mathtt{PMEVI}(\mathcal{M}_{t},\beta_{t},\Gamma_{t}, \sqrt{\log(t)}/t)\) ;
8:\(\mathfrak{g}_{k}\leftarrow\mathfrak{L}_{0}\mathfrak{h}_{k}-\mathfrak{h}_{k}\) ;
9: Update policy \(\pi_{k}\leftarrow\mathtt{Greedy}(\mathcal{M}_{t},\mathfrak{h}_{k},\beta_{t})\);
10:repeat
11: Play \(A_{t}\leftarrow\pi_{k}(S_{t})\), observe \(R_{t},S_{t+1}\);
12: Increment \(t\gets t+1\);
13:until (DT) \(N_{t}(S_{t},\pi_{k}(S_{t}))\geq 1\lor 2N_{t_{k}}(X_{t})\).
14:endfor ```

**Algorithm 1**\(\mathtt{PMEVI-DT}(\mathcal{H}_{t},T,t\mapsto\mathcal{M}_{t})\)

### Projected mitigated extended value iteration (\(\mathtt{PMEVI}\))

Assume that an external mechanism provides a confidence region \(\mathcal{H}_{t}\) for the bias function \(h^{*}\). Provided that \(\mathcal{M}_{t}\) is correct (\(M\in\mathcal{M}_{t}\)) and that \(\mathcal{H}_{t}\) is correct (\(h^{*}\in\mathcal{H}_{t}\)), we want to find a policy-model pair \((\pi,\widetilde{M})\) that maximizes the gain among pairs with \(h^{\pi}(\widetilde{M})\in\mathcal{H}_{t}\). This is done with an improved version of (6) combining two ideas, that are both necessary to achieve minimax optimal regret in the analysis.

1. **Projection (Section 3.2).** Whenever it is correct, the bias confidence region \(\mathcal{H}_{t}\) informs the learner that the search of an optimistic model can be constrained to those with bias within \(\mathcal{H}_{t}\). This is done by projecting \(\mathcal{L}^{\beta}_{t}\) (see _mitigation_) using an operator \(\Gamma_{t}:\mathbf{R}^{S}\rightarrow\mathcal{H}_{t}\), that has to satisfy a few non-trivial regularity conditions that are specified in Proposition 2.
2. **Mitigation (Section 3.3).** When one is aware that \(h^{*}\in\mathcal{H}_{t}\), the _dynamical bias update_\(\tilde{p}(s,a)v_{i}\) in (6) can be controlled better, by trying to restrict (6) to the \(\tilde{p}(s,a)\) such that \(\tilde{p}(s,a)v_{i}\leq\hat{p}_{t}(s,a)v_{i}+(p(s,a)-\hat{p}_{t}(s,a))v_{i}\) with the knowledge that \(v_{i}\in\mathcal{H}_{t}\). However, controlling the error \((p(s,a)-\hat{p}_{t}(s,a))v_{i}\) by doing a union-bound on all possible values of \(v_{i}\) is equivalent to building a confidence region for \(p(s,a)\), which produces an extra \(S^{1/2}\) in the error term that cannot be afforded by a minimax optimal algorithm.

We take a different approach instead. For a fixed \(u\in\mathbf{R}^{S}\), the empirical Bernstein inequality (Lemma 38) provides a variance bound of the form \((\hat{p}_{t}(s,a)-p(s,a))u\leq\beta_{t}(s,a,u)\). Byestimating \(\beta_{i}(s,a):=\max_{a\in\mathcal{H}_{i}}\beta_{i}(s,a,u)\), the search makes sure that \((\hat{p}_{i}(s,a)-p(s,a))u\leq\beta_{i}(s,a)\) holds with high probability for \(u=h^{*}\), even though \(h^{*}\) is unknown. For \(\beta\in\mathbf{R}_{+}^{X}\), we introduce the \(\beta\)_-mitigated_ extended Bellman operator:

\[\mathcal{L}_{t}^{\beta}u(s):=\max_{a\in\mathcal{H}_{i}}\sup_{\gamma(s,a)\in \mathcal{H}_{i}(s,a)}\sup_{\hat{p}(s,a)\in\mathcal{P}(s,a)}\left\{\tilde{r}(s, a)+\min\left\{\tilde{p}(s,a)u_{i},\hat{p}_{i}(s,a)u_{i}+\beta(s,a)\right\}\right\}\] (7)

The mitigation \(\beta(s,a)\) is independent of \(u\), which is crucial for \(\mathcal{L}_{t}^{\beta}\) to be well-behaved.

The proposition below shows how well-behaved the composition \(\mathfrak{L}_{t}:=\Gamma_{t}\circ\mathcal{L}_{t}^{\beta}\) is. Its proof requires to build a complete analysis of projected mitigated Bellman operators. This is deferred to the appendix.

**Proposition 2**.: _Fix \(\beta\in\mathbf{R}_{+}^{X}\) and assume that there exists a projection operator \(\Gamma_{t}:\mathbf{R}^{X}\to\mathcal{H}_{t}\) which is **(O1)** monotone: \(u\leq v\Rightarrow\Gamma u\leq\Gamma v\); **(O2)** non span-expansive: \(\operatorname{sp}\left(\Gamma u-\Gamma v\right)\leq\operatorname{sp}\left(u-v\right)\); **(O3)** linear: \(\Gamma(u+\lambda e)=\Gamma u+\lambda e\) and **(O4)**\(\Gamma u\leq u\). Then, the projected mitigated extended Bellman operator \(\mathfrak{L}_{t}:=\Gamma_{t}\circ\mathcal{L}_{t}^{\beta}\) has the following properties:_

1. _There exists a unique_ \(\mathfrak{q}_{t}\in\mathbf{R}e\) _such that_ \(\exists\mathfrak{h}_{t}\in\mathcal{H}_{t},\mathfrak{L}_{t}\mathfrak{h}_{t}= \mathfrak{h}_{t}+\mathfrak{q}_{t}\)_;_
2. _If_ \(M\in\mathcal{M}_{t}\)_,_ \(h^{*}\in\mathcal{H}_{t}\) _and_ \((\hat{p}_{t}(s,a)-p(s,a))h^{*}\leq\beta(s,a)\)_, then_ \(\mathfrak{q}_{t}\geq g^{*}(M)\)_;_
3. _If_ \(\mathcal{M}_{t}\) _is convex, then for all_ \(u\in\mathbf{R}^{S}\)_, the policy_ \(\pi:\mathbf{Greedy}(\mathcal{M}_{t},u,\beta)\) _picking the actions achieving_ \(\mathcal{L}_{t}^{\beta}u\) _satisfies_ \(\mathfrak{L}_{t}u=\tilde{r}^{\pi}+\tilde{P}^{\pi}u\) _for_ \(\tilde{r}^{\pi}(s)\leq\sup\mathcal{R}_{t}(s,\pi(s))\) _and_ \(\tilde{P}^{\pi}(s)\in\mathcal{P}_{t}(s,\pi(s))\)_;_
4. _For all_ \(u\in\mathbf{R}^{S}\) _and_ \(n\geq 0\)_,_ \(\operatorname{sp}\left(\mathcal{L}_{t}^{n+1}u-\mathfrak{L}_{t}^{n}u\right) \leq\operatorname{sp}\left((\mathcal{L}_{t})^{n+1}u-(\mathcal{L}_{t})^{n}u\right)\)_._

The property (1) guarantees that \(\mathfrak{L}_{t}\) has a fix-point while (2) states that this fix-point corresponds to an optimistic gain \(\mathfrak{q}_{t}\) if the model and the bias confidence region are correct and the mitigation isn't too aggressive. Combined with (3), the Poisson equation of a policy corresponds to this fix-point, i.e., \(\tilde{r}^{\pi}+\tilde{P}^{\pi}\mathfrak{h}_{t}=\mathfrak{h}_{t}+\mathfrak{q}_ {t}\), so that \(\mathfrak{q}_{t}\) is the gain and \(\mathfrak{h}_{t}\in\mathcal{H}_{t}\) is a legal bias for \(\pi\) under the model \((\tilde{r}^{\pi},\tilde{P}^{\pi})\). Lastly, the property (4) guarantees that the iterates \(\mathfrak{L}_{t}^{\pi}u\) converge to a fix-point of \(\mathfrak{L}\) at least as quickly as \(\mathcal{L}_{t}^{n}u\) goes to a fix-point of \(\mathcal{L}_{t}\); The convergence of \((\mathcal{L}_{t})^{n}u\) is already guaranteed by existing studies and is discussed in the appendix.

Provided that the bias confidence region is constructed, Proposition 2 foreshadows how powerful the construction is: The algorithm \(\mathtt{PMEVI}\), obtained by iterating \(\mathfrak{L}_{t}\) instead of \(\mathcal{L}_{t}\) in \(\mathtt{EVI}\), can replace the well-known \(\mathtt{EVI}\) within any algorithm of the literature that relies on it (UCRL2 Auer et al. (2009); UCRL2B Fruit et al. (2020) or KL-UCRL Filippi et al. (2010)) for an immediate improvement of its theoretical guarantees.

### Building the bias confidence region and its projection operator

The bias confidence region used by \(\mathtt{PMEVI}\)-\(\mathtt{DT}\) is obtained as a collection of constraints of the form:

\[\forall s\neq s^{\prime},\quad\mathfrak{b}(s)-\mathfrak{b}(s^{\prime})-c(s,s^{ \prime})\leq d(s,s^{\prime}).\] (8)

Such constraints include (1) prior bias constraints (if any) of the form of \(\mathfrak{b}(s)-\mathfrak{b}(s^{\prime})\leq c_{*}(s,s^{\prime})\); (2) span constraints of the form \(\mathfrak{b}(s)-\mathfrak{b}(s^{\prime})\leq c_{0}:=T^{1/5}\) spawning the span semi-ball \(\{u:\operatorname{sp}\left(u\right)\leq T^{1/5}\}\); and (3) pair-wise constraints obtained by estimating bias differences in the style of Zhang and Ji (2019); Zhang and Xie (2023) that we further improve. We start by defining a bias difference estimator.

**Definition 1** (Bias difference estimator).: Given a pair of states \(s\neq s^{\prime}\), their sequence of _commute times_\((\tau_{i}^{s\leftrightarrow s^{\prime}})_{i\geq 0}\) is defined by \(\tau_{2i}^{s\leftrightarrow s^{\prime}}:=\inf\{t>\tau_{2i-1}^{s \leftrightarrow s^{\prime}}:S_{t}=s\}\) and \(\tau_{2i+1}^{s\leftrightarrow s^{\prime}}:=\inf\{t>\tau_{2i}^{s \leftrightarrow s^{\prime}}:S_{t}=s^{\prime}\}\) with the convention that \(\tau_{2i-1}^{s\leftrightarrow s^{\prime}}=-\infty\). The number of commutations up to time \(t\) is \(N_{t}(s\leftrightarrow s^{\prime}):=\inf\{i:\tau_{2i}^{s\leftrightarrow s^{ \prime}}\leq t\}\), and \(\tilde{g}(t):=\frac{1}{t}\sum_{i=0}^{t-1}R_{i}\) is the empirical gain. The _bias difference estimator_ at time \(T\) is any quantity \(c_{T}(s,s^{\prime})\in\mathbf{R}\) such that:

\[N_{t}(s\leftrightarrow s^{\prime})c_{T}(s,s^{\prime})=\sum\nolimits_{t=0}^{N_{T}(s \leftrightarrow s^{\prime})-1}(-1)^{i}\sum\nolimits_{t=\tau_{2i}^{s \leftrightarrow s^{\prime}}}^{\tau_{2i+1}^{s\leftrightarrow s^{\prime}}-1}(\tilde{g}( T)-R_{t}).\] (9)

**Lemma 3**.: _With probability \(1-2\delta\), for all \(T^{\prime}\leq T\), we have_

\[N_{T^{\prime}}(s\leftrightarrow s^{\prime})\left|h^{*}(s)-h^{*}(s^{\prime})-c_{T^{ \prime}}(s,s^{\prime})\right|\leq 3\operatorname{sp}\left(h^{*}\right)+(1+ \operatorname{sp}\left(h^{*}\right))\sqrt{8T\log(\frac{2}{\delta})}+2\sum \nolimits_{t=0}^{T^{-1}}(g^{*}-R_{t}).\] (10)Lemma 3 says that the quality of the estimator \(c_{T}(s,s^{\prime})\) is directly linked to the number of observed commutes between \(s\) and \(s^{\prime}\) as well as the regret. The idea is that if the algorithm makes many commutes between \(s\) and \(s^{\prime}\) and if its regret is small, then the algorithm mostly takes optimal paths from \(s\) to \(s^{\prime}\). The bound provided by Lemma 3 is not accessible to the learner however, because \(\operatorname{sp}\left(h^{*}\right)\) is unknown in general. To overcome this issue, \(\operatorname{sp}\left(h^{*}\right)\) is upper-bounded by \(c_{0}:=T^{1/5}\). Overall, this leads to the design of the algorithm estimating the bias confidence region as specified in Algorithm 3.

```
0: History \(\mathcal{F}_{t}\), model region \(\mathcal{M}_{t}\), confidence \(\delta>0\)
1: Estimate bias differences \(c_{t}\) via (9);
2: Estimate optimistic gain \(\tilde{g}\leftarrow\min_{k<K(t)}\tilde{g}_{k}\);
3: Inner regret estimation \(B_{0}\gets t\tilde{g}-\sum_{i=0}^{t-1}R_{i}\);
4:\(\ell\leftarrow\sqrt{8T\log\left(\frac{2}{\delta}\right)}\), \(c_{0}\gets T^{\frac{1}{5}}\);
5: Estimate the bias difference errors as: \(d_{t}(s,s^{\prime})\equiv\operatorname{error}(c_{t},s,s^{\prime}):=\frac{3c_ {0}+(1+c_{0})(1+\ell)+2B_{0}}{N_{t}(s,\leftrightarrow s^{\prime})}\)
6:return\((c_{t},\operatorname{error}(c_{t},-,-))\), (8) defines \(\mathcal{H}^{\prime}_{t}\). ```

**Algorithm 3**\(\texttt{BiasEstimation}(\mathcal{F}_{t},\mathcal{M}_{t},\delta)\)

Coupled with prior information and span constraints, the bias confidence region \(\mathcal{H}_{t}\) is a polyhedron of the same kind as the one encountered in Zhang and Xie (2023). When generated by constraints of the form (8), following Zhang and Xie (2023), Proposition 3, one can project onto \(\mathcal{H}_{t}\) in polynomial time with Algorithm 4. Moreover, the resulting projection operator satisfies the prerequisites (**O1-4**) of Proposition 2, making \(\mathtt{PMEVI}\) (Algorithm 2) well-behaved. See Appendix B.2 for proofs.

**Lemma 4**.: _Assume that \(\mathcal{H}\) is a set of \(b\in\mathbf{R}^{S}\) satisfying a system of equations of the form of (8). If \(\mathcal{H}\) is non empty, then the operator \(\Gamma u:=\texttt{BiasProjection}(\mathcal{H},u)\) (see Algorithm 4) is a projection on \(\mathcal{H}\) and satisfies the properties (**O1-4**) defined in Proposition 2._

### Mitigation using finer bias dynamical error

The fact that \(h^{*}\in\mathcal{H}_{t}\) with high probability is used in \(\mathtt{PMEVI}\)-\(\mathtt{DT}\) to restrict the search of \(\mathtt{EVI}\) by reducing the dynamical bias error. This reduction is based on a empirical Bernstein inequality (see Lemma 38) applied to \((\hat{p}(s,a)-p(s,a))u\). Here, it gives that with probability \(1-\delta\), we have:

\[\left(\hat{p}_{t}(s,a)-p(s,a)\right)u\leq\sqrt{\frac{2\mathbf{V}(\hat{p}_{t}(s,a),u)\log\left(\frac{3T}{\delta}\right)}{\max\left\{1,N_{t}(s,a)\right\}}}+ \frac{3\operatorname{sp}\left(u\right)\log\left(\frac{3T}{\delta}\right)}{\max \left\{1,N_{t}(s,a)\right\}}=:\beta_{t}(s,a,u)\] (11)

where \(\mathbf{V}(\hat{p}_{t}(s,a),u)\) is the variance of \(u\) under the probability vector \(\hat{p}_{t}(s,a)\). More specifically, if \(q\) is a probability on \(\mathcal{S}\) and \(q\in\mathbf{R}^{S}\), we set \(\mathbf{V}(q,u):=\sum_{s}q(s)u(s)-q\cdot u)^{2}\). In (11), \(u\in\mathbf{R}^{S}\), \((s,a)\in\mathcal{X}\) and \(T\geq 1\) are fixed. Once is tempted to use (11) directly to mitigate the extended Bellman operator, but the resulting operator is ill-behaved because it loses monotony. This issue is avoided by changing \(\beta_{t}(s,a,u)\) to \(\max_{u\in\mathcal{H}_{t}}\beta_{t}(s,a,u)\) in (11). The resulting inequality is _not_ guaranteed to hold simultaneously for all \(u\in\mathcal{H}_{t}\) and with high probability; However, it is guaranteed to hold with high probability for \(u=h^{*}\), which will be enough.

The variance maximization problem \(\max_{u\in\mathcal{H}_{t}}\beta_{t}(s,a,u)\) is a _convex maximization problem_ with linear constraints. Even in very simple settings, such optimization problems are NP-hard Pardalos and Schnitger (1988) hence computing \(\max_{u\in\mathcal{H}_{t}}\beta_{t}(s,a,u)\) is not reasonable in general. Thankfully, this value can be upper-bounded by a tractable quantity that is enough in the regret analysis. The mitigation \(\beta_{t}\) used by \(\mathtt{PMEVI}\)-\(\mathtt{DT}\) is provided by \(\mathtt{Algorithm~{}\ref{eq:BiasProjection}}\). See Lemma 12 and Appendix A.2.2 for details.

## 4 Regret guarantees

Theorem 5 thereafter shows that \(\mathtt{PMEVI}\)-\(\mathtt{DT}\) has minimax optimal regret under regularity assumptions on the used confidence region \(\mathcal{M}_{t}\). Assumption 1 asserts that the confidence region holds uniformly with high probability. Assumption 2 asserts that the reward confidence region is sub-Weissman (seeLemma 35) and Assumption 3 assumes that the model confidence region makes sure that \(\mathtt{EVI}\) (6) converges in the first place. Assumption 4 asserts that the prior bias region is correct.

**Assumption 1**.: With probability \(1-\delta\), we have \(M\in\bigcap_{k=1}^{K(T)}\mathcal{M}_{\mathfrak{h}_{k}}\).

**Assumption 2**.: There exists a constant \(C>0\) such that for all \((s,a)\in\mathcal{S}\), for all \(t\leq T\), we have:

\[\mathcal{R}_{t}(s,a)\subseteq\left\{\tilde{r}(s,a)\in\mathcal{R}(s,a):N_{t}(s, a)\left\|\tilde{r}_{t}(s,a)-\tilde{r}(s,a)\right\|_{1}^{2}\leq C\log\left(\frac{2S _{t}(1+N_{t}(s,a))}{\delta}\right)\right\}.\]

**Assumption 3**.: For \(t\geq 0\), \(\mathcal{M}_{t}\) is a \((s,a)\)-rectangular convex region and \(\mathcal{L}_{t}^{n}u\) converges a fix-point.

**Assumption 4**.: The prior bias region \(\mathcal{H}_{t}\) contains \(h^{*}(M)\) and is generated by constraints of the form:

\[\forall s\neq s^{\prime},\quad\mathfrak{h}(s)-\mathfrak{h}(s^{\prime})\leq c _{*}(s,s^{\prime})\]

with \(c_{*}(s,s^{\prime})\in[-\infty,\infty]\) (possibly infinite).

Refer to Appendix A.2 for the feasibility of Assumption 1, Appendix A.2.3 for Assumption 2, and Appendix A.3 for Assumption 3.

**Theorem 5** (Main result).: _Let \(c>0\). Assume that \(\mathtt{PHEVI}\)-\(\mathtt{DT}\) runs with a confidence region system \(t\mapsto\mathcal{M}_{t}\) that guarantees Assumptions 1-3. If \(T\geq c^{5}\), then for every weakly communicating model with \(\mathrm{sp}\left(h^{*}\right)\leq c\) and such that Assumption 4 is satisfied \((h^{*}\in\mathcal{H}_{*})\), \(\mathtt{PHEVI}\)-\(\mathtt{DT}\) achieves regret:_

\[\mathrm{O}\left(\sqrt{cSAT\log\left(\frac{SAT}{\delta}\right)}\right)+ \mathrm{O}\left(c^{5\frac{5}{4}}\dot{A}^{2}\dot{T}^{\frac{5}{2}}\log^{2}\left( \frac{SAT}{\delta}\right)\right)\]

_with probability \(1-26\delta\), and in expectation if \(\delta<\sqrt{1/T}\). Moreover, if \(\mathtt{PHEVI}\)-\(\mathtt{DT}\) runs with the same confidence regions that \(\mathtt{UCRL2}\)Auer et al. (2009), then it enjoys a time complexity \(\mathrm{O}(DS^{3}AT)\)._

To have a completely prior-less algorithm, pick \(\mathcal{H}_{*}=\mathbf{R}^{\mathcal{S}}\). The proof of Theorem 5 is tedious and its details are deferred to appendix. We will focus here on the main ideas.

Notations.At episode \(k\), the played policy is denoted \(\pi_{k}\). As a greedy response to \(\mathfrak{h}_{k}\), by Proposition 2 (3), there exists \(\tilde{r}_{k}(s)\leq\sup\mathcal{R}_{\mathfrak{h}_{k}}(s,\pi_{k}(s))\) and \(\tilde{P}_{k}(s)\in\mathcal{P}_{\mathfrak{h}_{k}}(s,\pi(x))\) such that \(\mathfrak{h}_{k}+\mathfrak{a}_{k}=\tilde{r}_{k}+\tilde{P}_{k}\mathfrak{h}_{k}\). The reward-kernel pair \(\tilde{M}_{k}=(\tilde{r}_{k},\tilde{P}_{k})\) is referred to as the _optimistic model_ of \(\pi_{k}\). We write \(P_{k}:=P_{\pi_{k}}(M)\) the true kernel and \(\tilde{P}_{k}:=P_{\pi_{k}}(\tilde{M}_{k})\) the empirical kernel. Likewise, we define the reward functions \(r_{k}\) and \(\tilde{r}_{k}\). The optimistic gain and bias satisfy \(\mathfrak{g}_{k}=g(\pi_{k},\widetilde{M}_{k})\) and \(\mathfrak{h}_{k}=h(\pi_{k},\widetilde{M}_{k})\). We further denote \(c_{0}=T^{\frac{1}{3}}\).

The regret is first decomposed episodically with \(\mathrm{Reg}(T)=\sum_{k}\sum_{t=\mathfrak{g}_{k}}^{t_{k+1}-1}(g^{*}-R_{t})\). The first step goes back to the analysis of \(\mathtt{UCRL2}\)Auer et al. (2009), and consists in upper-bounding the regret of the episode \(k\) with optimistic quantities that are exclusive to that episode.

**Lemma 6** (Reward optimism).: _With probability \(1-6\delta\), we have:_

\[\mathrm{Reg}(T)\leq\sum_{k}\sum_{t=\mathfrak{t}_{k}}^{\mathfrak{h}_{k}-1}( \mathfrak{g}_{k}-R_{t})\leq\sum_{k}\sum_{t=\mathfrak{t}_{k}}^{\mathfrak{h}_{k }-1}(\mathfrak{g}_{k}-\tilde{r}_{k}(X_{t}))+\mathrm{O}\left(\sqrt{SAT\log\left( \frac{T}{\delta}\right)}\right).\] (12)

We introduce the two optimistic regrets \(B(T):=\sum_{k}\sum_{t=\mathfrak{t}_{k}}^{\mathfrak{h}_{k}-1}(\mathfrak{g}_{k} -R_{t})\) and \(\tilde{B}(T):=\sum_{k}\sum_{t=\mathfrak{h}_{k}}^{\mathfrak{h}_{k}-1}(\mathfrak{g }_{k}-\tilde{r}_{k}(X_{t}))\). Rewriting the summand \(\mathfrak{g}_{k}-\tilde{r}_{k}(X_{t})\) using the Poisson equation \(\mathfrak{h}_{k}+\mathfrak{a}_{k}=\tilde{r}_{k}+\tilde{P}_{k}\mathfrak{h}_{k}\), we get:

\[\tilde{B}(T)=\sum_{k}\sum_{t=\mathfrak{t}_{k}}^{\mathfrak{h}_{k}-1}\left( \tilde{p}_{k}(S_{t})-e_{S_{t}}\right)\mathfrak{h}_{k}.\]The analysis proceeds by decomposing the above expression of \(\hat{B}(T)\) in the style of Zhang and Ji (2019). We write \(\sum_{t=t_{k}}^{t_{k+1}-1}(\tilde{p}_{k}(S_{t})-e_{S_{t}})\mathfrak{b}_{k}\) as:

\[\sum_{t=t_{k}}^{t_{k+1}-1}\left(\underbrace{(p_{k}(S_{t})-e_{S_{t}})\,\mathfrak{ b}_{k}}_{\text{navigation error (1k)}}+\underbrace{(\hat{p}_{k}(S_{t})-p_{k}(S_{t}))\,\mathfrak{b}_{k}}_{\text{ empirical bias error (2k)}}+\underbrace{(\hat{p}_{k}(S_{t})-\hat{p}_{k}(S_{t}))\,\mathfrak{b}_{k}}_{\text{ optimistic overshoot (3k)}}+\underbrace{(\hat{p}_{k}(S_{t})-p_{k}(S_{t}))\,(\mathfrak{b}_{k}-h^{*})}_{\text{ second order error (4k)}}\right)\]

Each error term is bounded separately. Below, we denote \(\mathbf{V}(q,u):=\sum_{s}q(s)(u(s)-q\cdot u)^{2}\).

**Lemma 7** (Navigation error).: _With probability \(1-7\delta\), the navigation error is bounded by:_

\[\sum_{k}\sum_{t=t_{k}}^{t_{k+1}-1}(p_{k}(S_{t})-e_{S_{t}})\mathfrak{b}_{k} \leq\sqrt{2\sum_{t=0}^{T-1}\mathbf{V}(p(X_{t}),h^{*})\log\left(\frac{T}{ \delta}\right)}+2SA^{\frac{1}{2}}\sqrt{3B(T)}\log\left(\frac{T}{\delta} \right)+\widetilde{\mathrm{O}}\left(T^{\frac{j}{3}}\right).\]

**Lemma 8** (Empirical bias error).: _With probability \(1-\delta\), the empirical bias error is bounded by:_

\[\sum_{k}\sum_{t=t_{k}}^{t_{k+1}-1}\left(\hat{p}_{k}(S_{t})-p_{k}(S_{t})\,h^{ *}\leq 4\sqrt{SA\sum_{t=0}^{T-1}\mathbf{V}(p(X_{t}),h^{*})\log\left(\frac{SAT}{ \delta}\right)}+\mathrm{O}\left(\log^{2}(T)\right).\]

**Lemma 9** (Optimistic overshoot).: _With probability \(1-6\delta\), the optimistic overshoot is bounded by:_

\[\sum_{k}\sum_{t=t_{k}}^{t_{k+1}-1}\left(\tilde{p}_{k}(S_{t})-\hat{p}_{k}(S_{t} )\,\right)\mathfrak{b}_{k}\leq\left\{\begin{matrix}4\sqrt{2SA\sum_{t=0}^{T-1} \mathbf{V}(p(X_{t}),h^{*})\log\left(\frac{SAT}{\delta}\right)}\\ +8(1+c_{0})S^{\frac{3}{2}}A\log^{\frac{3}{2}}\left(\frac{SAT}{\delta}\right) \sqrt{B(T)}+\widetilde{\mathrm{O}}\left(T^{\frac{1}{4}}\right)\end{matrix} \right\}.\]

**Lemma 10** (Second order error).: _With probability \(1-6\delta\), the second order error is bounded by:_

\[\sum_{k}\sum_{t=t_{k}}^{t_{k+1}-1}\left(\hat{p}_{k}(S_{t})-p_{k}(S_{t})\, \right)(\mathfrak{b}_{k}-h^{*})\leq 16S^{2}A(1+c_{0})\log^{\frac{1}{2}}\left( \frac{S^{2}AT}{\delta}\right)\sqrt{2B(T)}+\widetilde{\mathrm{O}}\left(T^{ \frac{1}{4}}\right).\]

We see that the empirical bias error (Lemma 8) and the optimistic overshoot (Lemma 9) both involve the sum of variances \(\sum_{t=0}^{T-1}\mathbf{V}(p(X_{t}),h^{*})\), which is shown in Lemma 29 to be of order \(\mathrm{sp}\left(h^{*}\right)\mathrm{sp}\left(r\right)T+\sum_{t=0}^{T-1}\Delta ^{*}(X_{t})\). The pseudo-regret term \(\sum_{t=0}^{T-1}\Delta^{*}(X_{t})\) is bounded with the regret using Corollary 31, then by \(B(T)\). With high probability, we obtain an equation of the form:

\[B(T)\leq C\sqrt{(1+\mathrm{sp}\left(h^{*}\right))SAT\log\left(\frac{T}{\delta} \right)}+CS^{2}A(1+c_{0})\log^{2}(T)\sqrt{B(T)}+\tilde{\mathrm{O}}\left(T^{ \frac{1}{4}}\right)\]

where \(C\) is a constant. Setting \(\alpha:=CS^{2}A(1+c_{0})\log^{2}(T)\) and \(\beta:=C\sqrt{(1+\mathrm{sp}\left(h^{*}\right))SAT\log(T/\delta)}+\tilde{ \mathrm{O}}(T^{1/4})\), the above equation is of the form \(B(T)\leq\beta+\alpha\sqrt{B(T)}\). Solving in \(B(T)\), we find \(B(T)\leq\beta+2\sqrt{\alpha\beta}+\alpha^{2}\). The dominant term is \(\beta\), hence we readily obtain:

\[B(T)\leq C\sqrt{(1+\mathrm{sp}\left(h^{*}\right))\mathrm{sp}\left(r\right)SAT \log\left(\frac{T}{\delta}\right)}+\widetilde{\mathrm{O}}\left(\mathrm{sp} \left(h^{*}\right)\mathrm{sp}\left(r\right)S^{\frac{5}{4}}A^{\frac{1}{2}}(1+c_{0 })T^{\frac{1}{4}}\right).\] (13)

Since \(c_{0}=\mathrm{o}(T^{\frac{1}{4}})\), we conclude that \(B(T)=\mathrm{O}\left(\sqrt{\mathrm{sp}\left(h^{*}\right)SAT\log(T/\delta)}\right)\), ending the proof.

Figure 1: An overview of PMEVI-DT and its regret analysis. In the above, \(\mathfrak{a}_{k}\) and \(\mathfrak{b}_{k}\) are the optimistic gain and bias functions produced by PMEVI (see Algorithm 2) at episode \(k\), and \(\hat{p}_{k}\) and \(\hat{p}_{k}\) are respectively the empirical and optimistic kernel models at episode \(k\).

Experimental illustrations

To get a grasp of how PMEVI-DT behaves in practice, we provide in Fig. 2 a first round of illustrative experiments. In both, the environment is a river-swim which is a model known to be hard to learn despite its size, with high diameter and bias span, see Appendix D for the model's description.

On the first experiment, we observe that PMEVI can exploit prior bias knowledge effectively and drastically improve the regret performance, depending on the quality of the prior region.

On the second experiment however, we observe that without prior knowledge, PMEVI has nearly the same regret performance that its EVI counterparts, meaning that the bias confidence region is too large to effectively improve the regret performance. This observation is first to be taken with caution. Indeed, the regret that is being estimated above is model specific, hence is not an estimate of the minimax regret -- This being said, it undoubtedly shows that the bias confidence region is ineffective and this can be explained as follows. On experiments, we see that most of the regret is due to the early phase of the learning process, where proper bias information is nearly impossible to get. Indeed, the regret is still growing linearly, so no bias information can be inferred. But in addition, this "bad" early data pollutes the bias estimator for a long duration. In other words, while the theoretical regret guarantees of PMEVI-DT are better than its EVI analogues, there is room to improve the bias estimation mechanism and the practical performance.

## 6 Conclusion

In this work, we have shown that regret guarantees of order \(\sqrt{\text{sp}(h^{*})SAT\log(T)}\) can be achieved for weakly communicating MDPs without prior knowledge, nor exponential computational cost. In particular, regret guarantees can scale with the bias span rather than the diameter without prior knowledge. This is in opposition to the recent results that the sample complexity cannot be bounded in term of bias span without prior knowledge for average reward MDPs Tuynman et al. (2024); Wang et al. (2024); Zurek and Chen (2024a,b). This difference lies in the fact \((\epsilon,\delta)\)-PAC algorithm must produce a policy \(\pi_{\tau}\) after \(\tau\) learning steps where \(\tau\) is a stopping time, with \(\mathbf{P}(g^{\pi_{\tau}}\leq g^{*}-\epsilon)\leq\delta\). Implicitly, these algorithms must hereby _certify_ that the output policy is approximately optimal. In opposition, regret robust algorithms have no need to assess that deployed policies are indeed optimal.

In the end, the regret advantages of PMEVI-DT over pure EVI-based methods remain theoretical, and the experimental shortcomings displayed in Section 5 leave a few opportunities for future work. Can bias information be inferred more efficiently? Or, do the experiments indicate that the regret analysis of EVI-based methods may be drastically improved?

Figure 2: (**To the left**) Running UCRL2 and PMEVI-DT with the same confidence region than UCRL2 on a 3-state river-swim. PMEVI-DT is run with prior knowledge \(h^{*}(s_{1})\leq h^{*}(s_{2})-c\leq h^{*}(s_{3})-2c\) for \(c\in\{0,0.5,1,1.5,2\}\). (**To the right**) Running a few algorithms of the literature on 5-state river-swim and comparing their average regret against their PMEVI variants, obtained by changing calls to the EVI sub-routine to calls to PMEVI.

## References

* Abbasi-Yadkori et al. (2019) Yasin Abbasi-Yadkori, Nevena Lazic, Csaba Szepesvari, and Gellert Weisz. Exploration-enhanced POLITEX. 2019. arXiv:1908.10479.
* Agrawal and Jia (2023) Shipra Agrawal and Randy Jia. Optimistic Posterior Sampling for Reinforcement Learning: Worst-Case Regret Bounds. _Mathematics of Operations Research_, 48(1):363-392, 2023. Publisher: INFORMS.
* Audibert et al. (2009) Jean-Yves Audibert, Remi Munos, and Csaba Szepesvari. Exploration-exploitation tradeoff using variance estimates in multi-armed bandits. _Theoretical Computer Science_, 410(19):1876-1902, 2009. Publisher: Elsevier.
* Auer and Ortner (2006) Peter Auer and Ronald Ortner. Logarithmic Online Regret Bounds for Undiscounted Reinforcement Learning. _Proceedings of the 19th International Conference on Neural Information Processing Systems_, December 2006.
* Auer et al. (2002) Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-Time Analysis of the Multiarmed Bandit Problem. _Mach. Learn._, 47(2-3):235-256, May 2002.
* Auer et al. (2009) Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal Regret Bounds for Reinforcement Learning. In _Advances in Neural Information Processing Systems_, volume 21. Curran Associates, Inc., 2009.
* 367, 1967. Publisher: Tohoku University, Mathematical Institute.
* Bartlett and Tewari (2009) Peter L. Bartlett and Ambuj Tewari. REGAL: a regularization based algorithm for reinforcement learning in weakly communicating MDPs. In _Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence_, UAI '09, pages 35-42, Arlington, Virginia, USA, June 2009. AUAI Press.
* Bourel et al. (2020) Hippolyte Bourel, Odalric Maillard, and Mohammad Sadegh Talebi. Tightening Exploration in Upper Confidence Reinforcement Learning. In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 1056-1066. PMLR, July 2020.
* MOR_, 22:222-255, February 1997.
* Cohen et al. (2020) Michael B. Cohen, Yin Tat Lee, and Zhao Song. Solving linear programs in the current matrix multiplication time, 2020.
* Filippi et al. (2010) Sarah Filippi, Olivier Cappe, and Aurelien Garivier. Optimism in Reinforcement Learning and Kullback-Leibler Divergence. _2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton)_, pages 115-122, September 2010. arXiv: 1004.5229.
* Fruit (2019) Ronan Fruit. _Exploration-exploitation dilemma in Reinforcement Learning under various form of prior knowledge_. PhD Thesis, Universite de Lille 1, Sciences et Technologies; CRIStAL UMR 9189, 2019.
* Fruit et al. (2018) Ronan Fruit, Matteo Pirotta, Alessandro Lazaric, and Ronald Ortner. Efficient Bias-Span-Constrained Exploration-Exploitation in Reinforcement Learning. _Proceedings of the 35 th International Conference on Machine Learning_, 2018.
* Fruit et al. (2020) Ronan Fruit, Matteo Pirotta, and Alessandro Lazaric. Improved Analysis of UCRL2 with Empirical Bernstein Inequality. 2020. arXiv:2007.05456.
* Jonsson et al. (2020) Anders Jonsson, Emilie Kaufmann, Pierre Menard, Omar Darwiche Domingues, Edouard Leurent, and Michal Valko. Planning in markov decision processes with gap-dependent sample complexity. _Advances in Neural Information Processing Systems_, 33:1253-1263, 2020.
* Ouyang et al. (2017) Yi Ouyang, Mukul Gagrani, Ashutosh Nayyar, and Rahul Jain. Learning Unknown Markov Decision Processes: A Thompson Sampling Approach. September 2017. arXiv: 1709.04570.
* Ouyang et al. (2018)Panos M. Pardalos and Georg Schnitger. Checking local optimality in constrained quadratic programming is NP-hard. _Operations Research Letters_, 7:33-35, 1988.
* Puterman (1994) Martin L. Puterman. _Markov Decision Processes: Discrete Stochastic Dynamic Programming_. Wiley Series in Probability and Statistics. Wiley, 1 edition, April 1994. ISBN 978-0-471-61977-2 978-0-470-31688-7.
* Sutton and Barto (2018) Richard S Sutton and Andrew G Barto. _Reinforcement learning: An introduction_. MIT press, 2018.
* Talebi and Maillard (2018) Mohammad Sadegh Talebi and Odalric-Ambrym Maillard. Variance-Aware Regret Bounds for Undiscounted Reinforcement Learning in MDPs. _Journal of Machine Learning Research_, pages 1-36, April 2018. Publisher: Microtome Publishing.
* Tewari and Bartlett (2007) Ambuj Tewari and P. Bartlett. Optimistic Linear Programming gives Logarithmic Regret for Irreducible MDPs. In _NIPS_, 2007.
* Theocharous et al. (2017) Georgios Theocharous, Zheng Wen, Yasin Abbasi-Yadkori, and Nikos Vlassis. Posterior sampling for large scale reinforcement learning. 2017. arXiv:1711.07979.
* Thompson (1933) William R Thompson. On the Likelihood that One Probability Exceeds Another in View of the Evidence of Two Samples. _Biometrika_, 25(3-4):285-294, December 1933. ISSN 0006-3444.
* Tuynman et al. (2024) Adrienne Tuynman, Remy Degenne, and Emilie Kaufmann. Finding good policies in average-reward Markov Decision Processes without prior knowledge, May 2024.
* Wang et al. (2024) Shengbo Wang, Jose Blanchet, and Peter Glynn. Optimal Sample Complexity for Average Reward Markov Decision Processes, February 2024.
* Wei et al. (2020) Chen-Yu Wei, Mehdi Jafarnia Jahromi, Haipeng Luo, Hiteshi Sharma, and Rahul Jain. Model-free Reinforcement Learning in Infinite-horizon Average-reward Markov Decision Processes. In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 10170-10180. PMLR, July 2020.
* Zhang and Ji (2019) Zihan Zhang and Xiangyang Ji. Regret Minimization for Reinforcement Learning by Evaluating the Optimal Bias Function. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d' Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* Zhang and Xie (2023) Zihan Zhang and Qiaomin Xie. Sharper Model-free Reinforcement Learning for Average-reward Markov Decision Processes. In _The Thirty Sixth Annual Conference on Learning Theory_, pages 5476-5477. PMLR, 2023.
* Zhang et al. (2020) Zihan Zhang, Yuan Zhou, and Xiangyang Ji. Almost Optimal Model-Free Reinforcement Learning via Reference-Advantage Decomposition. June 2020. arXiv: 2004.10019 [cs, stat].
* Zurek and Chen (2024a) Matthew Zurek and Yudong Chen. The Plug-in Approach for Average-Reward and Discounted MDPs: Optimal Sample Complexity Analysis, 2024a. arXiv:2410.07616 [cs.LG].
* Zurek and Chen (2024b) Matthew Zurek and Yudong Chen. Span-Based Optimal Sample Complexity for Weakly Communicating and General Average Reward MDPs, 2024b. arXiv:2403.11477 [cs.LG].

Construction of Pmevi-dt

This section provides the technical details required to understand the design of PMEVI-DT in Section3. We further discuss the assumptions 1-4 appearing in Theorem5 and provide sufficient conditions so that they are met.

### Proof of Lemma3, estimation of the bias error

Fix \(s,s^{\prime}\in\mathcal{S}\). We denote \(\alpha_{T}:=N_{T}(s\leftrightarrow s^{\prime})(h^{*}(s)-h^{*}(s^{\prime})-c_{ T}(s,s^{\prime}))\). We will start by considering the better estimator \(c^{\prime}_{T}(s,s^{\prime})\) that satisfies the same equation (9) than \(c_{T}(s,s^{\prime})\) but with \(\hat{g}(T)\) changed to \(g^{*}\), readily:

\[N_{t}(s\leftrightarrow s^{\prime})c^{\prime}_{T}(s,s^{\prime})=\sum\nolimits _{t=0}^{N_{T}(s\leftrightarrow s^{\prime})-1}(-1)^{j}\sum\nolimits_{t=\tau_{t =0}^{s^{\prime}}}^{\tau_{t+1}^{s+\tau}-1}(g^{*}-R_{t}).\]

To avoid a typographical clutter, we write \(\tau_{i}\) instead of \(\tau_{i}^{s\leftrightarrow s^{\prime}}\) in the remaining of the proof and we write \(\alpha^{\prime}_{T}:=N_{T}(s\leftrightarrow s^{\prime})(h^{*}(s)-h^{*}(s^{ \prime})-c^{\prime}_{T}(s,s^{\prime})\).

(**STEP 1**) We start by relating the two estimators. Intuitively, \(\hat{g}(T)\) is a good estimator for \(g^{*}\) when the regret is small. Recall that \(\hat{g}(T):=\frac{1}{T}\sum_{t=0}^{T-1}R_{t}\), hence:

\[\sum\nolimits_{t=0}^{T-1}\left|\hat{g}(T)-g^{*}\right|=\left|\sum\nolimits_{t =0}^{T-1}(R_{t}-g^{*})\right|=\left|\text{Reg}(T)\right|.\]

Therefore,

\[\left|\alpha_{T}\right|\leq\left|\alpha^{\prime}_{T}\right|+\left|\alpha_{T}- \alpha^{\prime}_{T}\right|\leq\left|\alpha^{\prime}_{T}\right|+\sum\nolimits_{ t=0}^{T-1}\left|\hat{g}(T)-g^{*}\right|\leq\left|\alpha^{\prime}_{T}\right|+ \left|\text{Reg}(T)\right|.\]

We are left with upper-bounding \(\left|\alpha^{\prime}_{T}\right|\).

(**STEP 2**) If \(i\) is even, then \(S_{\tau_{i}}\) and \(S_{\tau_{i+1}}=s^{\prime}\); otherwise \(S_{\tau_{i}}=s^{\prime}\) and \(S_{\tau_{i+1}}=s\). In both cases, we have \(h^{*}(S_{\tau_{i+1}})-h^{*}(S_{\tau_{i}})=(-1)^{i}(h^{*}(s^{\prime})-h^{*}(s))\). Therefore, using Bellman's equation, the quantity \(\text{A}:=\sum\nolimits_{t=\tau_{i}}^{\tau_{t+1}-1}(g^{*}-R_{t})\) satisfies:

\[\text{A} =\sum\nolimits_{t=\tau_{i}}^{\tau_{t+1}-1}\left(p(X_{t})-e_{S_{t }}\right)h^{*}+\sum\nolimits_{t=\tau_{i}}^{\tau_{t+1}-1}(r(X_{t})-R_{t})+\sum \nolimits_{t=\tau_{i}}^{\tau_{t+1}-1}\Delta^{*}(X_{t})\] \[=\sum\nolimits_{t=\tau_{i}}^{\tau_{t+1}-1}\left(e_{S_{\tau_{i+1} }}-e_{S_{t}}\right)h^{*}+\sum\nolimits_{t=\tau_{i}}^{\tau_{t+1}-1}\left(p(X_{ t})-e_{S_{t+1}}\right)h^{*}+\sum\nolimits_{t=\tau_{i}}^{\tau_{t+1}-1}(r(X_{t})-R_{t})+ \sum\nolimits_{t=\tau_{i}}^{\tau_{t+1}-1}\Delta^{*}(X_{t})\] \[=(-1)^{i}(h^{*}(s^{\prime})-h^{*}(s))+\sum\nolimits_{t=\tau_{i}} ^{\tau_{t+1}-1}\left(p(X_{t})-e_{S_{t+1}}\right)h^{*}+\sum\nolimits_{t=\tau_{ i}}^{\tau_{t+1}-1}(r(X_{t})-R_{t})+\sum\nolimits_{t=\tau_{i}}^{\tau_{t+1}-1} \Delta^{*}(X_{t}).\]

Multiplying by \((-1)^{i}\) and rearranging, \(h^{*}(s^{\prime})-h^{*}(s)+(-1)^{i+1}\sum\nolimits_{t=\tau_{i}}^{\tau_{t+1}-1} (g^{*}-R_{t})\) appears to be equal to:

\[(-1)^{i+1}\left(\sum\nolimits_{t=\tau_{i}}^{\tau_{t+1}-1}\left(\left(p(X_{t}) -e_{S_{t+1}}\right)h^{*}+r(X_{t})-R_{t}\right)+\sum\nolimits_{t=\tau_{i}}^{ \tau_{t+1}-1}\Delta^{*}(X_{t})\right).\]

Proceed by summing over \(i\). By triangular inequality, we obtain:

\[\left|\alpha^{\prime}_{T}\right|\leq\left|\sum\nolimits_{i=0}^{N_{T}(s \leftrightarrow s^{\prime})-1}\sum\nolimits_{t=\tau_{i}}^{\tau_{t+1}-1}(-1)^{i+ 1}\left(\left(p(X_{t})-e_{S_{t+1}}\right)h^{*}+r(X_{t})-R_{t}\right)\right|+ \sum\nolimits_{i=0}^{N_{T}(s\leftrightarrow s^{\prime})-1}\sum\nolimits_{t= \tau_{i}}^{\tau_{t+1}-1}\Delta^{*}(X_{t}).\]

Because all Bellman gaps \(\Delta^{*}\) are non-negative, the second term is upper-bounded by the pseudo-regret \(\sum\nolimits_{t=0}^{T-1}\Delta^{*}(X_{t})\). The first term is a martingale, and the martingale difference sequence \((-1)^{i+1}((p(X_{t})-e_{S_{t+1}})h^{*}+r(X_{t})-R_{t})\) has span at most \(\operatorname{sp}\left(h^{*}\right)+1\) since rewards are supported in \([0,1]\). Although the number of involved terms is random, it is upper-bounded by \(T\), hence by the maximal version of Azuma-Hoeffding's inequality (Lemma32), we have that with probability at least \(1-\delta\) and uniformly for \(T^{\prime}\leq T\),

\[\left|\sum\nolimits_{i=0}^{N_{T}(s\leftrightarrow s^{\prime})-1}\sum\nolimits_{t =\tau_{i}}^{\tau_{t+1}-1}(-1)^{i+1}\left(\left(p(X_{t})-e_{S_{t+1}}\right)h^{*} +r(X_{t})-R_{t}\right)\right|\leq(1+\operatorname{sp}\left(h^{*}\right))\sqrt{ \tfrac{1}{2}T\log\left(\tfrac{2}{\delta}\right)}.\]

(**STEP 3**) We conclude that with probability \(1-\delta\), for all \(T^{\prime}\leq T\),

\[\alpha_{T^{\prime}}\leq(1+\operatorname{sp}\left(h^{*}\right))\sqrt{\tfrac{1}{2}T \log\left(\tfrac{2}{\delta}\right)}+\sum\nolimits_{t=0}^{T^{\prime}-1}\Delta^ {*}(X_{t})+\left|\text{Reg}(T^{\prime})\right|.\]We are left with relating both \(\sum_{t=0}^{T^{\prime}-1}\Delta^{*}(X_{t})\) and \(\left|\operatorname{Reg}(T^{\prime})\right|\) to \(\sum_{t=0}^{T^{\prime}-1}(\tilde{g}-R_{t})\). Using the Bellman equation again, we find that:

\[\left|\sum\nolimits_{t=0}^{T^{\prime}-1}(g^{*}-R_{t}-\Delta^{*}(X _{t}))\right| \leq\left|h^{*}(S_{0})-h^{*}(S_{T^{\prime}})\right|+\left|\sum \nolimits_{t=0}^{T^{\prime}-1}\left(\left(p(X_{t})-e_{S_{t^{\prime}}}\right)h^ {*}+(r(X_{t})-R_{t})\right)\right|\] \[\leq\operatorname{sp}\left(h^{*}\right)+(1+\operatorname{sp} \left(h^{*}\right))\sqrt{\tfrac{1}{2}T\log\left(\tfrac{2}{\delta}\right)}\]

where the last inequality holds with probability \(1-\delta\) uniformly over \(T^{\prime}\leq T\) by Azuma-Hoeffding's inequality again (Lemma32). Remark that if \(y-z\leq x\leq y+z\), then \(\left|x\right|\leq\left|y\right|+\left|z\right|\), hence we conclude that with probability \(1-\delta\), for all \(T^{\prime}\leq T\):

\[\sum\nolimits_{t=0}^{T^{\prime}-1}\Delta^{*}(X_{t})+\left| \operatorname{Reg}(T^{\prime})\right| \leq 2\sum\nolimits_{t=0}^{T^{\prime}-1}\Delta^{*}(X_{t})+(1+ \operatorname{sp}\left(h^{*}\right))\sqrt{\tfrac{1}{2}T\log\left(\tfrac{2}{ \delta}\right)}+\operatorname{sp}\left(h^{*}\right)\] \[\leq 2\sum\nolimits_{t=0}^{T^{\prime}-1}(g^{*}-R_{t})+3(1+ \operatorname{sp}\left(h^{*}\right))\sqrt{\tfrac{1}{2}T\log\left(\tfrac{2}{ \delta}\right)}+3\operatorname{sp}\left(h^{*}\right)\] \[\leq 2\sum\nolimits_{t=0}^{T^{\prime}-1}(\tilde{g}-R_{t})+3(1+ \operatorname{sp}\left(h^{*}\right))\sqrt{\tfrac{1}{2}T\log\left(\tfrac{2}{ \delta}\right)}+3\operatorname{sp}\left(h^{*}\right)\]

where the last inequality invokes \(\tilde{g}\geq g^{*}\). We conclude that, with probability \(1-2\delta\), for all \(T^{\prime}\leq T\), we have:

\[N_{T^{\prime}}(s\leftrightarrow s^{\prime})(h^{*}(s)-h^{*}(s^{\prime})-c_{T^{ \prime}}(s,s^{\prime}))\leq 3\operatorname{sp}\left(h^{*}\right)+(1+ \operatorname{sp}\left(h^{*}\right))\sqrt{8T\log\left(\tfrac{2}{\delta} \right)}+\sum\nolimits_{t=0}^{T^{\prime}-1}(\tilde{g}-R_{t}).\]

This concludes the proof. 

### The confidence region of Pmevi-dt

The algorithm PMEVI-DT can be instantiated with a large panel of possibilities, depending on the type of confidence region one is willing to use for rewards and kernels. In this work, we allow for four types of confidence regions, described below. For conciseness, \(q\in\{r,p\}\) is a symbolic letter that can be a reward or a kernel and we denote \(\mathcal{Q}_{l}(s,a)\) the confidence region for \(q(s,a)\) at time \(t\). If \(q=r\), then \(\dim(q)=2\) (Bernoulli rewards) with \(\mathcal{Q}(s,a)=[0,1]\); and if \(q=p\), then \(\dim(q)=S\) with \(\mathcal{Q}(s,a)=\mathcal{P}(\mathcal{S})\).

* _Azuma-Hoeffding_ or _Weissman_ type confidence regions, with \(\mathcal{Q}_{l}(s,a)\) taken as: \[\left\{\tilde{q}(s,a)\in\mathcal{Q}(s,a):N_{l}(s,a)\left\|\tilde{q}_{l}(s,a)- \tilde{q}(s,a)\right\|_{1}^{2}\leq\dim(q)\log\left(\tfrac{2SA(1+N_{l}(s,a))} {\delta}\right)\right\}.\]
* _Empirical Bernstein_ type confidence regions, with \(\mathcal{Q}_{l}(s,a)\) taken as: \[\left\{\tilde{q}(s,a)\in\mathcal{Q}(s,a):\forall i,|\tilde{q}_{l}(\tilde{u}|s,a )-\tilde{q}(\tilde{u}|s,a)|\leq\sqrt{\tfrac{2\operatorname{VI}(\tilde{q}_{l}( \tilde{u}|s,a))\log\left(\tfrac{2\dim(q)SAT}{\delta}\right)}{N_{l}(s,a)}}+ \tfrac{3\log\left(\tfrac{2\dim(q)SAT}{\delta}\right)}{N_{l}(s,a)}\right\}.\] with the convention that \(x/0=+\infty\) for \(x>0\).
* _Empirical likelihood_ type confidence regions, with \(\mathcal{Q}_{l}(s,a)\) taken as: \[\left\{\tilde{q}(s,a)\in\mathcal{Q}(s,a):N_{l}(s,a)\operatorname{KL}(\hat{q}_{ l}(s,a)\|\tilde{q}(s,a))\leq\log\left(\tfrac{2SA}{\delta}\right)+(\dim(q)-1) \log\left(e\left(1+\tfrac{N_{l}(s,a)}{\dim q-1}\right)\right)\right\}.\]
* _Trivial_ confidence region with \(\mathcal{Q}_{l}(s,a)=\mathcal{Q}(s,a)\).

A few remarks are in order. When rewards are not Bernoulli, only the confidence regions (**C1**) and (**C4**) are eligible among the above. Then, Weissman's inequality must be changed to Azuma's inequality for \(\sigma\)-sub-Gaussian random variables, see Lemma34. Since rewards are supported in \([0,1]\), Hoeffding's Lemma guarantees that reward distributions are \(\sigma\)-sub-Gaussian with \(\sigma=\tfrac{1}{2}\).

#### a.2.1 Correctness of the model confidence region \(\mathcal{M}_{t}\) and Assumption 1

The confidence regions \(\mathcal{Q}_{t}(s,a)\) described with (**C1-4**) are tuned so that the following result holds:

**Lemma 11**.: _Assume that, for all \(q\in\{r,p\}\) and \((s,a)\in\mathcal{X}\), we choose \(\mathcal{Q}_{t}(s,a)\) among (**C1-4**). Then Assumption 1 holds. More specifically, the region of models \(\mathcal{M}_{t}:=\prod_{s,a}(\mathcal{R}_{t}(s,a)\times\mathcal{P}_{t}(s,a))\) satisfies \(\mathbf{P}(\exists t\leq T:M\notin\mathcal{M}_{t})\leq\delta\)._

Proof.: We show that, for all \(q\in\{r,q\}\) and \((s,a)\in\mathcal{X}\), if \(\mathcal{Q}_{t}(s,a)\) is chosen amoung (**C1-4**), then

\[\mathbf{P}\left(\exists t\leq T:q(s,a)\notin\mathcal{Q}_{t}(s,a)\right)\leq\delta.\]

If \(\mathcal{Q}_{t}(s,a)\) is chosen with (**C1**), this is a direct application of Lemma 35; with (**C2**), this is Lemma 36; with (**C3**), this is Lemma 37; and with (**C4**) this is by definition. 

a.2.2 Simultaneous correctness of bias confidence region \(\mathcal{H}_{t}\), mitigation \(\beta_{t}\) and optimism

In this section, we show that if Assumption 1 holds, then the bias confidence region constructed by \(\mathtt{PMEVI}\)-DT is correct with high probability, and that the mitigation is not too strong. Recall that \((\mathfrak{a}_{k},\mathfrak{b}_{k})\) are the optimistic gain and bias of the policy deployed in episode \(k\) (see Algorithm 1). In particular, we have \(\mathfrak{a}_{k}=\mathfrak{a}_{t_{k}}\mathfrak{b}_{k}-\mathfrak{b}_{k}\) with \(\mathfrak{b}_{k}\in\mathcal{H}_{t_{k}}\). We start by a result on the deviation of the variance, which is what the variance approximation Algorithm 5 is based on. Recall that the bias confidence region \(\mathcal{H}_{t}\) is obtained as the collection of constraints:

1. prior constraints (if any) \(\mathfrak{b}(s)-\mathfrak{b}(s^{\prime})\leq c_{*}(s,s^{\prime})\);
2. span constraints \(\mathfrak{b}(s)-\mathfrak{b}(s^{\prime})\leq c_{0}:=T^{1/5}\);
3. dynamically inferred constraints \(\left|\mathfrak{b}(s)-\mathfrak{b}(s^{\prime})-c_{t}(s^{\prime},s)\right|\leq \operatorname{error}(c_{t},s^{\prime},s)\) (see Algorithm 3).

We start with the technical result that is behind the variance approximation Algorithm 5.

**Lemma 12**.: _Let \(u,v\in\mathcal{H}_{t}\) and fix \(p\) a probability distribution on \(\mathcal{S}\). Then for all \(s\in\mathcal{S}\),_

\[\mathbf{V}(p,u)\leq\mathbf{V}(p,v)+8c_{0}\sum\nolimits_{s^{\prime}\in \mathcal{S}}p(s^{\prime})\operatorname{error}(c_{t},s^{\prime},s).\]

Proof.: We start by establishing the following result: If \(p\) is a probability distribution on \(\mathcal{S}\) and \(u,v\in\mathbf{R}^{\mathcal{S}}\), we have:

\[\mathbf{V}(p,u)\leq\mathbf{V}(p,v)+2\left(p\cdot\left|u-v\right|\right)\max(u +v)\] (14)

where \(\cdot\) is the dot product, \(u^{2}\) the Hadamard product \(uu\) and \(\left|u\right|\) the vector whose entry \(s\) is \(\left|u(s)\right|\). (14) is obtained with a straight forward computation:

\[\mathbf{V}(p,u)-\mathbf{V}(p,v) =p\cdot(u^{2}-v^{2})+(p\cdot v)^{2}-(p\cdot u)^{2}\] \[=p\cdot((u-v)(u+v))+(p\cdot(u-v))(p\cdot(u+v))\] \[\leq p\cdot(\left|u-v\right|(u+v))+(p\cdot\left|u-v\right|)(p \cdot\left|u+v\right|)\] \[\leq 2(p\cdot\left|u-v\right|)\max(u+v).\]

Observe that \(v\) can be changed to \(v+\lambda e\), where \(e\) is the vector full of ones, without changing the result. The same goes for \(u\). We now move to the proof of the main statement. First, translate \(u\) and \(v\) such that \(u(s)=v(s)=0\). Then, we have:

\[p\cdot(u-v) =\sum\nolimits_{s^{\prime}\in\mathcal{S}}p(s^{\prime})\left|u(s^{ \prime})-u(s)-c_{t}(s^{\prime},s)+v(s)-v(s^{\prime})+c_{t}(s^{\prime},s)\right|\] \[\leq\sum\nolimits_{s^{\prime}\in\mathcal{S}}p(s^{\prime})\left( \left|u(s^{\prime})-u(s)-c_{t}(s^{\prime},s)\right|+\left|v(s^{\prime})-v(s)-c _{t}(s^{\prime},s)\right|\right)\] \[\leq 2\sum\nolimits_{s^{\prime}\in\mathcal{S}}p(s^{\prime}) \operatorname{error}(c_{t},s^{\prime},s).\]

Conclude using that \(\max(u+v)\leq\max(u)+\max(v)+2c_{0}\) for \(u,v\in\mathcal{H}\) such that \(u(s)=v(s)=0\). 

**Lemma 13**.: _Assume that Assumption 1 holds and that \(c_{0}\geq\operatorname{sp}\left(h^{*}\right)\). Then, with probability \(1-4\delta\), for all \(k\leq K(T)\), (1) \(\mathfrak{a}_{k}\geq g^{*}\) and (2) \(h^{*}\in\mathcal{H}_{t_{k}}\) and (3) for all \((s,a)\), \((\hat{p}_{t_{k}}(s,a)-p(s,a))h^{*}\leq\beta_{t_{k}}(s,a)\)._Proof.: Let \(E_{1}\) be the event \((\forall k\leq K(T),M\in\mathcal{M}_{\mathfrak{h}_{k}})\). Let \(E_{2}\) the event stating that, for all \(T^{\prime}\leq T\),

\[N_{T^{\prime}}(s\leftrightarrow s^{\prime})\left|h^{*}(s)-h^{*}(s^{\prime})-c_{ T^{\prime}}(s,s^{\prime})\right|\leq 3\mathrm{sp}\left(h^{*}\right)+(1+\mathrm{sp} \left(h^{*}\right))\sqrt{8T\log(\frac{2}{\delta})}+2\sum_{t=0}^{T^{\prime}-1 }(g^{*}-R_{t}),\]

and let \(E_{3}\) the event stating that, for all \(T^{\prime}\leq T\) and for all \((s,a)\in\mathcal{X}\), we have:

\[\left(\hat{p}_{T^{\prime}}(s,a)-p(s,a)\right)h^{*}\leq\sqrt{\frac{2\mathbf{V} \left(\hat{p}_{T^{\prime}}(s,a)\right)h^{*}\log\left(\frac{s\mathcal{X}}{ \delta}\right)}{N_{T^{\prime}}(s,a)}}+\frac{3\mathrm{sp}\left(h^{*}\right) \log\left(\frac{s\mathcal{X}}{\delta}\right)}{N_{T^{\prime}}(s,a)}.\]

By Assumption 1, we have \(\mathbf{P}(E_{1})\geq 1-\delta\). By Lemma 3, we have \(\mathbf{P}(E_{2})\geq 1-2\delta\) and by Lemma 36, we have \(\mathbf{P}(E_{3})\geq 1-\delta\), so \(\mathbf{P}(E_{1}\cap E_{2}\cap E_{3})\geq 1-4\delta\). We prove by induction on \(k\leq K(T)\) that, on \(E_{1}\cap E_{2}\cap E_{3}\), (1) \(\mathfrak{g}_{k}\geq g^{*}\), (2) \(h^{*}\in\mathcal{H}_{\mathfrak{t}_{k}}\) (3) and for all \((s,a)\), \((\hat{p}_{\mathfrak{t}_{k}}(s,a)-p(s,a))h^{*}\leq\beta_{\mathfrak{t}_{k}}(s,a)\), where \(\mathfrak{g}_{k}\) is the optimistic gain of the policy deployed at episode \(k\).

It is obvious for \(k=0\). Indeed, \(N_{0}(s\leftrightarrow s^{\prime})=0\) for all \(s,s^{\prime}\) hence \(c_{0}(s,s^{\prime})=c_{0}\geq\mathrm{sp}\left(h^{*}\right)\). Therefore,

\[\mathcal{H}_{0}\supseteq\left\{\mathfrak{b}\in\mathbf{R}^{S}:\mathrm{sp}\left( b\right)\leq c_{0}\right\}\supseteq\left\{\mathfrak{b}\in\mathbf{R}^{S}: \mathrm{sp}\left(b\right)\leq\mathrm{sp}\left(h^{*}\right)\right\}\]

so contains \(h^{*}\), proving (2). Moreover, since \(N_{0}(s,a)=0\), we have \(\beta_{0}(s,a)=+\infty\), proving (3). Finally, since \(M\in\mathcal{M}_{0}\) on \(E_{1}\), by the statement (2) of Proposition 2, we have \(\mathfrak{g}_{k}\geq g^{*}\), hence proving (1).

Now assume that \(k\geq 1\). By induction \(\mathfrak{g}_{\ell}\geq g^{*}\) for all \(\ell<k\), so on \(E_{2}\) we have:

\[N_{h}(s\leftrightarrow s^{\prime})\left|h^{*}(s)-h^{*}(s^{\prime})-c_{h}(s,s^ {\prime})\right|\leq 3\mathrm{sp}\left(h^{*}\right)+(1+\mathrm{sp}\left(h^{*} \right))\sqrt{8T\log(\frac{2}{\delta})}+2\sum_{\ell=1}^{k-1}\sum_{t=t_{\ell} }^{t_{\ell+1}-1}(\mathfrak{g}_{\ell}-R_{t}).\]

By design of \(\mathcal{H}_{\mathfrak{t}_{k}}\) (see Algorithm 3), we deduce that (2) \(h^{*}\in\mathcal{H}_{\mathfrak{t}_{k}}\). Denote \(h_{0}\in\mathcal{H}_{\mathfrak{t}_{k}}\) the reference point used by Algorithm 5. We have, for all \((s,a)\in\mathcal{X}\), on \(E_{1}\cap E_{2}\cap E_{3}\), we have:

\[\left(\hat{p}_{\mathfrak{t}_{k}}(s,a)-p(s,a)\right)h^{*} \leq\sqrt{\frac{2\mathbf{V}\left(\hat{p}_{\mathfrak{t}_{k}}(s,a) \right)\cdot\log\left(\frac{s\mathcal{X}}{\delta}\right)}{N_{\mathfrak{t}_{k} }(s,a)}}+\frac{3\mathrm{sp}\left(h^{*}\right)\log\left(\frac{s\mathcal{X}}{ \delta}\right)}{N_{\mathfrak{t}_{k}}(s,a)}\] \[(h^{*}\in\mathcal{H}_{\mathfrak{t}_{k}}+\text{Lemma \ref{lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemmalemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemmalemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemmalemma:lemma:lemma:lemma:lemmalemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemmalemma:lemma:lemma:lemma:lemma:lemmalemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemmalemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemmalemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemmalemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemmalemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemmalemma:lemma:lemmalemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemma:lemmalemma:lemma:lemma:lemma:lemma:lemma:lemmalemma:lemma:lemma:lemma:lemmalemma:lemma:lemma:lemmalemma:lemmalemma:lemma:lemma:lemma:lemmalemma:lemma:lemmalemma:lemma:lemmalemma:lemma:lemmalemma:lemmalemma:lemmalemma:lemmalemma:lemmalemma:lemmalemmalemma:lemmalemma:lemma

**Lemma 15**.: _Let \(M\) a weakly-communicating MDP with finite state space \(\mathbf{R}^{S}\) and compact action space, and let \(L\) its Bellman operator. Assume that there exists \(\gamma>0\) such that, \(\forall u\in\mathbf{R}^{S}\),_

\[\forall s\in\mathcal{S},\exists a\in\mathcal{A}(s),\quad Lu(s)=r(s,a)+p(s,a)u=r (s,a)+\gamma\max(u)+(1-\gamma)q_{s}^{u}u\]

_with \(q_{s}^{u}\in\mathcal{P}(\mathcal{S})\). Then, for all \(u\in\mathbf{R}^{S}\) and all \(\epsilon>0\), if \(\operatorname{sp}\left(L^{n+1}u-L^{n}u\right)\geq\epsilon\), then:_

\[n\leq 2+\frac{4\operatorname{sp}\left(w_{0}\right)}{\gamma\epsilon}+\frac{2}{ \gamma}\log\left(\frac{2\operatorname{sp}\left(w_{0}\right)}{\epsilon}\right).\]

Proof.: Since \(M\) is weakly communicating, has finitely many states and compact action space, it has well-defined gain \(g^{*}\) and bias \(h^{*}\) functions. Denote \(u_{n+1}:=L^{n}u\).

\[w_{n} :=\max_{\pi\in\Pi}\left\{r_{\pi}+P_{\pi}u_{n-1}\right\}-ng^{*}-h^ {*}\] \[=\max_{\pi\in\Pi}\left\{r_{\pi}-g^{*}+(P_{\pi}-I)h^{*}+P_{\pi} \left(u_{n-1}-h^{*}-(n-1)g^{*}\right)\right\}=:\max_{\pi\in\Pi}\left\{r_{\pi} ^{\prime}+P_{\pi}w_{n-1}\right\}.\]

Observe that the policy achieving the maximum is the one achieving \(u_{n}=r_{\pi}+P_{\pi}u_{n-1}\). Remark that \(r_{\pi}^{\prime}(s)=-\Delta^{*}(s,\pi(s))\leq 0\) is the Bellman gap of the pair \((s,\pi(s))\), that we more simply write \(\Delta_{\pi}\). For all \(n\), there exists \(\pi_{n}\in\Pi\) such that \(w_{n+1}=-\Delta_{\pi_{n}}+P_{\pi_{n}}w_{n}\). Moreover, by assumption, we have \(P_{\pi_{n}}=\gamma\cdot e_{\pi_{n}}^{\top}e+(1-\gamma)Q_{n}\) where \(Q_{n}\) is a stochastic matrix. Moreover,

\[\left(\min(-\Delta_{\pi_{n}})+\gamma w_{n}(s_{n})\right)e+(1-\gamma)Q_{n}w_{n }\leq w_{n+1}\leq\left(\max(-\Delta_{\pi_{n}})+\gamma w_{n}(s_{n})\right)e+(1- \gamma)Q_{n}w_{n}.\]

Hence, \(\operatorname{sp}\left(w_{n+1}\right)\leq(1-\gamma)\operatorname{sp}\left(w_ {n}\right)+\operatorname{sp}\left(\Delta_{\pi_{n}}\right)\). In addition, \(w_{n}=L^{n}u-L^{n}h^{*}\), so by non-expansiveness of \(L\) in span semi-norm, \(\operatorname{sp}\left(w_{n+1}\right)\leq\operatorname{sp}\left(w_{n}\right)\). Overall,

\[\operatorname{sp}\left(w_{n+1}\right)\leq\min\left((1-\gamma)\operatorname{sp }\left(w_{n}\right)+\operatorname{sp}\left(\Delta_{\pi_{n}}\right)\text{,} \operatorname{sp}\left(w_{n}\right)\right).\] (15)

Fix \(\epsilon>0\), and let \(n_{\epsilon}:=\inf\left\{n:\operatorname{sp}\left(w_{n}\right)<\epsilon\right\}\).

Let \(\pi^{*}\) an optimal policy. We have \(w_{n+1}\geq P_{\pi^{*}}w_{n}\) so by induction, \(w_{n+1}\geq P_{\pi^{*}}^{n+1}w_{0}\geq\min(w_{0})e\). Meanwhile, we see that \(\left\|w_{n}\right\|_{1}\geq\sum_{k=0}^{n-1}\left\|\Delta_{\pi_{k}}\right\|_{1 }+S\min(w_{0})\), so \(\sum_{k=0}^{n-1}\left\|\Delta_{\pi_{k}}\right\|_{1}\leq\operatorname{sp}\left( w_{0}\right)\). Since \(\Delta_{\pi_{k}}\leq 0\) for all \(k\), we have \(\operatorname{sp}\left(\Delta_{\pi_{k}}\right)\leq\left\|\Delta_{\pi_{k}}\right\|_ {1}\) so \(\sum_{k=0}^{n-1}\operatorname{sp}\left(\Delta_{\pi_{k}}\right)\leq\operatorname{ sp}\left(w_{0}\right)\).

By (15), either \(\operatorname{sp}\left(w_{n+1}\right)\leq(1-\frac{1}{2}\gamma)\max(\epsilon, \operatorname{sp}\left(w_{n}\right))\) or \(\operatorname{sp}\left(\Delta_{\pi_{k}}\right)\geq\frac{1}{2}\gamma\epsilon\), but because \(\sum_{k=0}^{+\infty}\operatorname{sp}\left(\Delta_{\pi_{k}}\right)\leq \operatorname{sp}\left(w_{0}\right)\), the second case can happen at most \(\frac{2\operatorname{sp}\left(w_{0}\right)}{\gamma\epsilon}\) times. We deduce that, for all \(n\leq n_{\epsilon}\),

\[\operatorname{sp}\left(w_{n+1}\right)\leq\left(1-\frac{1}{2}\gamma\right)^{n- \frac{2\operatorname{sp}\left(w_{0}\right)}{\gamma\epsilon}}\operatorname{ sp}\left(w_{0}\right).\]

In particular, for \(n=n_{\epsilon}-1\), we get:

\[\epsilon\leq\left(1-\frac{1}{2}\gamma\right)^{n_{\epsilon}-2-\frac{2\operatorname {sp}\left(w_{0}\right)}{\gamma\epsilon}}\operatorname{sp}\left(w_{0}\right).\]

We obtain:

\[n_{\epsilon}\leq 2+\frac{2\operatorname{sp}\left(w_{0}\right)}{\gamma\epsilon}+ \frac{2}{\gamma}\log\left(\frac{\operatorname{sp}\left(w_{0}\right)}{\epsilon} \right).\]

To conclude, check that \(\operatorname{sp}\left(L^{n+1}u-L^{n}u\right)=\operatorname{sp}\left(w_{n+1}-w_{ n}\right)\leq 2\operatorname{sp}\left(w_{n}\right)\). 

Before moving to the application of interest, remark that this result can be greatly improved if the supremum \(\sup\left\{\Delta^{*}(s,a):\Delta^{*}(s,a)<0\right\}\) is not zero, to change the dominant term \(\frac{4\operatorname{sp}\left(w_{0}\right)}{\gamma\epsilon}\) for a constant independent of \(\epsilon\).

**Corollary 16**.: _Assume that the \(\mathcal{M}_{t}\) has non-empty interior, and that its Bellman operator satisfies the requirement of Lemma 15, i.e., there exists \(\gamma>0\) such that, \(\forall u\in\mathbf{R}^{S},\forall s\in\mathcal{S},\exists a\in\mathcal{A}(s), \exists\overline{r}_{t}(s,a)\in\mathcal{R}_{t}(s,a),\exists\overline{p}_{t}(s,a) \in\mathcal{P}_{t}(s,a)\):_

\[\mathcal{L}_{t}u(s)=\tilde{r}_{t}(s,a)+\tilde{p}_{t}(s,a)u=\tilde{r}_{t}(s,a)+ \gamma\max(u)+(1-\gamma)q_{s}^{u}u\]

_for some \(q_{s}^{u}\in\mathcal{P}(\mathcal{S})\). Then Assumption 3 is satisfied, and span fix-points \(\tilde{h}_{t}\) of \(\mathcal{L}_{t}\) are such that \(g^{*}(\mathcal{M}_{t})=\mathcal{L}_{t}\tilde{h}_{t}-\tilde{h}_{t}\)._Proof.: If \(\mathcal{M}_{t}\) is has non-empty interior, it means that for all \((s,a)\), \(\mathcal{P}_{t}(s,a)\) has non-empty interior. Therefore, for all state-action pair, there exists \(\tilde{p}_{t}(s,a)\in\mathcal{P}_{t}(s,a)\) that is fully supported. It follows that \(\mathcal{M}_{t}\) is communicating, and it follows from standard results Puterman (1994) that its span fix-points \(\tilde{h}\) do exist and that \(\tilde{g}_{t}:=\tilde{\mathcal{L}}\tilde{h}_{t}-\tilde{h}_{t}\in\mathbf{R}e\) does not depend on the initial state.

Moreover, if \(\widetilde{M}\in\mathcal{M}_{t}\) and \(\pi\in\Pi\) with \(\tilde{g}_{\pi}\equiv g(\pi,\mathcal{M}_{t})\in\mathbf{R}e\), letting \(\tilde{r}_{\pi}:=r_{\pi}(\tilde{M})\) and \(\tilde{P}_{\pi}:=P_{\pi}(\tilde{M})\), we have:

\[\tilde{r}_{\pi}+\tilde{p}_{\pi}\tilde{h}_{t}\leq\mathcal{L}_{t}\tilde{h}_{t} \leq\tilde{g}_{t}e+\tilde{h}_{t}.\]

So by induction and since \(\mathcal{L}_{t}\) is obviously monotone and linear, we show that:

\[\sum_{k=0}^{n}\tilde{P}_{k}^{k}\tilde{r}_{\pi}\leq n\tilde{g}_{t}e+(I-\tilde{ P}_{\pi}^{n})\tilde{h}_{\pi}.\]

Dividing by \(n\) and letting it go to infinity, we obtain \(g(\pi,\mathcal{M}_{t})\leq\tilde{g}_{t}\). Observe that we have equility by taking the policy achieving \((\tilde{g}_{t},\tilde{h}_{t})\).

To see that EVI converges indeed, simply observe that Lemma15 provides a finite bound on how much time is required until the \(\operatorname{sp}\left(\mathcal{L}_{t}^{n+1}u-\mathcal{L}_{t}^{n}u\right)\leq\epsilon\). Hence \(\operatorname{sp}\left(\mathcal{L}_{t}^{n+1}u-\mathcal{L}_{t}^{n}u\right)\) vanishes to \(0\). 

About Assumption3.The assumptions made by Corollary16 are met if the kernel confidence regions are:

* Built out of Weissman's inequality (**C1**) (see the next section, also Auer et al. (2009));
* Built out of Bernstein's inequality (**C2**) (because the maximization algorithm to compute \(\tilde{p}_{t}(s,a)u_{i}\) in EVI has the same greedy properties than with Weissman's inequality);
* Trivial (**C4**).

For confidence regions build with empirical likelihood estimates (**C3**), there is no guarantee of convergence (although we conjecture that one could be established), although the gain is still well-defined because \(\mathcal{M}_{t}\) remains communicating. However, just like the original work of Filippi et al. (2010), the convergence is always met numerically.

### Proof of Theorem5: Complexity of PMEVI with Weissman confidence regions

In this section, we show that when one is using Weissman confidence regions for kernels (**C1**), then the iterates of \(\mathcal{L}_{t}\) converge to an \(\epsilon\) span-fix-point quickly.

**Proposition 17**.: _Assume that_ PMEVI-DT _uses kernel confidence regions of Weissman type (**C1**) satisfying Assumption1. Then with probability \(1-\delta\), the number of iterations of_ PMEVI _(see Algorithm2) is_ O\(\left(D/\overline{S}AT\right)\)_, hence the algorithm has polynomial per-step amortized complexity._

Proof.: With Weissman type confidence regions for kernels, for all \(t\leq T\) and \((s,a)\in\mathcal{X}\), we have

\[\mathcal{P}_{t}(s,a)\geq\left\{\tilde{p}(s,a)\in\mathcal{P}(s,a):\|\tilde{p} (s,a)-\hat{p}_{t}(s,a)\|_{1}\leq\sqrt{\frac{S\,\log(2SAT)}{T}}\right\}\]

It follows that, for all \(t\leq T\), the extended Bellman operator \(\mathcal{L}_{t}\) satisfies the prerequisite (\(\ast\)) of Lemma15 with

\[\gamma=\frac{1}{2}\sqrt{\frac{S\,\log(2SAT/\delta)}{T}}=\Omega\left(\sqrt{ \frac{S\,\log(T/\delta)}{T}}\right).\]

Under Assumption1, we have \(M\in\mathcal{M}_{t}\) with probability \(1-\delta\). Under this event, \(\mathcal{M}_{t}\) is weakly communicating and \(\operatorname{sp}\left(h^{*}(\mathcal{M}_{t})\right)\leq D(M)\), we can apply Lemma15 and conclude that every calls to PMEVI (Algorithm2) takes

\[\text{O}\left(\frac{\operatorname{sp}\left(w_{0}\right)\sqrt{T}}{\epsilon\sqrt{ \frac{S\,\log(T/\delta)}{T}}}\right)=\text{O}\left(\frac{DT}{\sqrt{S\,\log(T)}}\right)\]

where we use that \(\epsilon=\sqrt{\frac{\log(SAT/\delta)}{T}}\), that \(\operatorname{sp}\left(w_{0}\right)=\text{O}\left(\operatorname{sp}\left(h^{* }(\mathcal{M}_{t})\right)\right)=\text{O}(D(M))\) and that \(\delta\geq\frac{1}{T}\). Since the number of episodes under the doubling trick (DT) is O\((SA\log(T))\), we conclude accordingly.

Every call to the projection operator solves a linear program. Although in theory, this time is polynomial (relying on recent work on the complexity of LP such as Cohen et al. (2020), it is the current matrix multiplication time \(\mathrm{O}(S^{2.38})\)), in practice, reducing the number of calls to the projection operator is key to run PMEVI-DT in reasonable time.

Analysis of the projected mitigated Bellman operator

In this section, we fix the model region \(\mathcal{M}\), the bias region \(\mathcal{H}\) and the mitigation vector \(\beta\), dropping the sub-script \(t\) for conciseness. We denote \(\hat{p}\), \(\hat{p}\) the respective empirical reward and kernel. Further assume that \(\mathcal{H}=\mathcal{H}_{0}+\mathbf{R}e\) with \(\mathcal{H}_{0}\) a compact convex set. The associated projection operation (see B.2) is denoted \(\Gamma\). The (vanilla) extended Bellman operator \(\mathcal{L}\) associated to \(\mathcal{M}\) is given by \(\mathcal{L}u(s):=\max_{a\in\mathcal{H}(s)}\left\{\sup\mathcal{R}(s,a)+\sup \mathcal{P}(s,a)u\right\}\). The _\(\beta\)-mitigated extended Bellman operator_ associated to \(\mathcal{M}\) is:

\[\mathcal{L}^{\beta}u(s):=\max_{a\in\mathcal{H}(s)}\sup_{\tilde{p}(s,a)\in \mathcal{R}(s,a)}\sup_{\tilde{p}(s,a)\in\mathcal{P}(s,a)}\Big{\{}\tilde{p}(s,a )+\min\left\{\tilde{p}(s,a)u_{i},\tilde{p}(s,a)u_{i}+\beta(s,a)\right\}\Big{\}}.\] (16)

The function \(\text{Greedy}(\mathcal{M},u,\beta)\) returns a stationary deterministic policy that picks its actions among the one reaching the maximum above. The projection of \(\mathcal{L}^{\beta}\) to \(\mathcal{H}\) is

\[\mathfrak{L}\equiv\mathfrak{L}^{\beta,\mathcal{H}}:=\Gamma\circ\mathcal{L}^{ \beta}.\] (17)

The goal of this section is to establish Proposition2 and

* Proposition2 statement (1) is a consequence of Lemma22;
* Proposition2 statement (2) follows from Theorem25;
* Proposition2 statement (3) follows from Corollary27;
* Proposition2 statement (4) follows from Corollary21;
* Proposition2 prerequisites on the projection operator and Lemma4 follows from Lemma19.

### Finding an optimistic policy under bias constraints

The main goal is to find and optimistic policy under _bias constraints_ (projection) and _bias error constraints_ (mitigation). The bias constraints imply that we search for a policy \(\pi\) together with a model \(\widetilde{M}\) such that \(h^{\pi}(\widetilde{M})\in\mathcal{H}\). The bias error means that, for \(\tilde{h}\equiv h^{\pi}(\widetilde{M})\), we want in addition \(\tilde{p}(s,\pi(s))\tilde{h}\leq\tilde{p}(s,\pi(s))\tilde{h}+\beta(s,\pi(s))\) where \(\tilde{p}\) is the transition kernel of \(\widetilde{M}\). In the end, our goal is to track the solution of the following optimization problem:

\[g^{*}(\mathcal{H},\beta,\mathcal{M}):=\sup\left\{\begin{array}{cc}\pi\in \Pi,\widetilde{M}\in\mathcal{M},\\ g^{*}(\widetilde{M}):&\forall s\in\mathcal{S},\ \tilde{p}(s,\pi(s))\tilde{h}\leq\tilde{p}(s,\pi(s)) \tilde{h}+\beta(s,\pi(s)),\\ \tilde{h}\equiv h^{\pi}(\widetilde{M})\in\mathcal{H},\ \text{sp}(g^{*}( \widetilde{M}))=0\end{array}\right\}\] (18)

where the supremum is taken with respect to the product order \(\mathbf{R}^{S}\). In particular, if \(\mathcal{U}\subseteq\mathcal{R}^{S}\), check that \(u^{*}=\sup\mathcal{U}\) is obtained as \(u^{*}(s):=\sup\left\{v(s):v\in\mathcal{U}\right\}\). The constraint \(\text{sp}(g^{\pi}(\widetilde{M}))=0\) is suggested by the work of Fruit et al. (2018); Fruit (2019) and is key for the problem to be solvable.

The bias constraint and the constraint involving \(\beta\) make the problem impossible to handle with a "pure" extended MDP solution, which is why the extended Bellman operators are mitigated (with \(\beta\)) then projected (with \(\Gamma\)). The mitigation operation guarantees that the constraint involving \(\beta\) is satisfied, while the projection on \(\mathcal{H}\) makes sure that the bias constraint is satisfied. It is important for both operations to be compatible, i.e., that the constraint involving \(\beta\) that \(\mathcal{L}^{\beta}\) forces is not lost when applying \(\Gamma\). As a matter of fact, projecting then mitigating would not work.

We now explain why \(\mathfrak{L}\) can be used to solve (18).

### Projection operation and definition of \(\mathfrak{L}\)

We start by discussing why \(\mathfrak{L}\) is well-defined at all. The well-definition of \(\mathcal{L}^{\beta}\) is obvious. The point is to explain why the projection onto \(\mathcal{H}\) is possible while preserving mandatory structural properties such as monotony, non-expansivity, linearity and more. For general \(\mathcal{H}\), such properties are impossible to meet. But the bias confidence region constructed with Algorithm3 has a specific shape that makes the projection possible. The central property is the one below:

* _The downward closure_ \(\{v\leq u:v\in\mathcal{H}\}\) _of every \(u\in\mathbf{R}^{S}\) has a maximum in \(\mathcal{H}\)._

The only order that we will be considering is the product order on \(\mathbf{R}^{S}\). Recall that a set \(\mathcal{U}\subseteq\mathbf{R}^{S}\) has a _maximum_ if there exists \(u\in\mathcal{U}\) such that \(v\leq u\) for all \(u\in\mathcal{U}\). A _supremum_ of \(\mathcal{U}\) is a minimal upper-bound of \(\mathcal{U}\), i.e., \(u\) such that (1) \(v\leq u\) for all \(v\in\mathcal{U}\) and (2) no \(w\) satisfying (1) can be smaller than \(u\). For the product order, the supremum of a subset \(\mathcal{U}\) is unique and of the form \(u(s)=\sup\left\{v(s):v\in\mathcal{U}\right\}\).

Define the projection \(\Gamma:\mathbf{R}^{S}\to\mathcal{H}\) as such:

\[\Gamma u:=\max\left\{v\leq u:v\in\mathcal{H}\right\}.\] (19)

In general, Assumption (**A1**) is satisfied when \(\mathcal{H}\) admits a join, i.e., is stable by finite supremum: \(u,v\in\mathcal{H}\Rightarrow\sup(u,v)\in\mathcal{H}\).

**Lemma 18**.: _If \(\mathcal{H}\) is generated by constraints of the form \(\mathfrak{h}(s)-\mathfrak{h}(s^{\prime})-c(s,s^{\prime})\leq d(s,s^{\prime})\), then it has a join and **(A1)** is satisfied. Moreover, \(\Gamma\) is then correctly computed with Algorithm 4._

Proof.: The first half of the result is well-known, see Zhang and Xie (2023), but we recall a proof for self-containedness. Let \(v_{1},v_{2}\in\mathcal{H}\) and define \(v_{3}:=\sup(v_{1},v_{2})\). Observe that \(v_{3}(s)-v_{3}(s^{\prime})\leq\max(v_{1}(s)-v_{1}(s^{\prime}),v_{2}(s)-v_{2}( s^{\prime}))\leq c(s,s^{\prime})+d(s,s^{\prime})\). So \(v_{3}\in\mathcal{H}\).

We continue by showing that if \(\mathcal{H}\) has a join, then (19) is well-defined. For \(s\in\mathcal{S}\), take a sequence \(v_{n}^{s}\) such that \(v_{n}^{s}(s)\to\alpha(s):=\sup\left\{v(s):v\leq u,v\in\mathcal{H}\right\}\). Because the span of every element of \(\mathcal{H}\) is upper-bounded by \(c:=\sup\left\{\operatorname{sp}\left(v\right):v\in\mathcal{H}\right\}\), it follows that \(v_{n}^{s}\) evolves in the compact region \(\left\{v\leq u:v\in\mathcal{H}\right\}\cap\left\{v:\left\|v-\alpha se\right\|_ {\infty}=1+c\right\}\). We can therefore extract a convergent sequence of \(v_{n}^{s}\), converging \(v_{n}^{s}\) that belongs to \(\mathcal{H}\) since the latter is closed. By construction, \(v_{n}^{s}(s)=\alpha(s)\). Because \(\mathcal{H}\) has a join, \(v_{*}:=\sup\left\{v_{*}^{s}:s\in\mathcal{S}\right\}\in\mathcal{H}\). 

**Lemma 19**.: _Under assumption **(A1)**, the operator \(\Gamma u:=\max\left\{v\leq u:v\in\mathcal{H}\right\}\) is well-defined, and is:_

1. _monotone:_ \(u\leq v\Rightarrow\Gamma u\leq\Gamma v\)_;_
2. _non span-expansive:_ \(\operatorname{sp}\left(\Gamma u-\Gamma v\right)\leq\operatorname{sp}\left(u-v\right)\)_;_
3. _linear:_ \(\Gamma(u+\lambda e)=\Gamma u+\lambda e\)_;_
4. \(\Gamma u\leq u\)_._

Proof.: The well-definition of \(\Gamma\) is obvious from (**A1**). For (2), if \(u\leq v\) then \(w\leq u\Rightarrow w\leq v\). Hence \(\Gamma u:=\max\left\{w\leq u:w\in\mathcal{H}\right\}\leq\max\left\{w\leq v:w \in\mathcal{H}\right\}=:\Gamma v\). For (3), check that it follows from \(\mathcal{H}=\mathcal{H}+\mathbf{R}e\). For (4), we obviously have \(\Gamma u:=\max\left\{v\leq u:v\in\mathcal{H}\right\}\leq u\).

The more difficult point is (2) span non-expansivity. Pick \(u,v\in\mathbf{R}^{S}\). By linearity, it suffices to show the result for \(\sum_{s}u(s)=\sum_{s}v(s)\). In that case, we have \(\operatorname{sp}\left(v-u\right)=\max(v-u)+\max(u-v)\). Observe that for all \(w\leq u\), we have \(w+\min(v-u)e\leq v\). Since \(\mathcal{H}=\mathcal{H}+\mathbf{R}e\), it follows that:

\[\max\left\{w\leq u:u\in\mathcal{H}\right\}\leq\max\left\{w\leq v:w\in \mathcal{H}\right\}+\max(u-v)e.\]

Similarly, we have \(\max\left\{w\leq u:w\in\mathcal{H}\right\}\geq\max\left\{w\leq v:w\in \mathcal{H}\right\}+\min(v-u)e\). Using them both at once, we find \(\operatorname{sp}\left(\Gamma u-\Gamma v\right)\leq\operatorname{sp}\left(v-u\right)\). 

The properties (1), (3) and (4) are essential for \(\mathfrak{L}\) to properly address the optimization problem (18). The property (2) is just as important, because it plays a central part in the convergence of value iteration. The next result shows similar properties for the \(\beta\)-mitigated extended Bellman operator \(\mathcal{L}^{\beta}\). From now on, we will assume (A1), because it is almost-surely satisfied by the bias confidence region generated by Algorithm 3.

**Lemma 20**.: _The \(\beta\)-mitigated extended Bellman operator \(\mathcal{L}^{\beta}\) is (1) monotone, (2) non-span-expansive and (3) linear._

Proof.: The properties (1) and (3) directly follow from the definition. We focus on (2). Fix \(u,u^{\prime}\in\mathbf{R}^{S}\). By Lemma 26, we can write \(\mathcal{L}^{\beta}u=\tilde{r}_{\pi}+\tilde{P}_{\pi}u\) and \(\mathcal{L}^{\beta}u^{\prime}=\tilde{r}_{\pi^{\prime}}+\tilde{P}_{\pi^{\prime}}u ^{\prime}\). In the following, we write \(\beta_{\pi}(s):=\beta(s,\pi(s))\). Check that:

\[\mathcal{L}^{\beta}u-\mathcal{L}^{\beta}u^{\prime}=\tilde{r}_{\pi}+\tilde{P}_{ \pi}u-\left(\tilde{r}_{\pi^{\prime}}+\tilde{P}_{\pi^{\prime}}u^{\prime}\right) \leq\tilde{r}_{\pi}+\tilde{P}_{\pi}u-\left(\tilde{r}_{\pi}+\min\left\{\tilde{P}_ {\pi}u^{\prime},\tilde{P}_{\pi}u^{\prime}+\beta_{\pi}\right\}\right).\]

If the minimum is reached with \(\tilde{P}_{\pi}u^{\prime}\), then:

\[\mathcal{L}^{\beta}u-\mathcal{L}^{\beta}u^{\prime}\leq\tilde{P}_{\pi}(u-u^{ \prime}).\]If the minimum is reached with \(\dot{P}_{\pi}u^{\prime}+\beta_{\pi}\), then upper-bound \(\dot{P}_{\pi}u\) by \(\dot{P}_{\pi}u+\beta_{\pi}\) to obtain:

\[\mathcal{L}^{\beta}u-\mathcal{L}^{\beta}u^{\prime}\leq\dot{P}_{\pi}(u-u^{\prime}).\]

Overall, we find that there exists \(Q_{\pi}\in\mathcal{P}_{\pi}\) such that \(\mathcal{L}^{\beta}u-\mathcal{L}^{\beta}u^{\prime}\leq Q_{\pi}(u-u^{\prime})\). Similarly, we find \(Q_{\pi^{\prime}}\in\mathcal{P}_{\pi^{\prime}}\) such that \(\mathcal{L}^{\beta}u-\mathcal{L}^{\beta}u^{\prime}\geq\mathcal{Q}_{\pi^{ \prime}}(u-u^{\prime})\). We conclude that:

\[\operatorname{sp}\left(\mathcal{L}^{\beta}u-\mathcal{L}^{\beta}u^{\prime} \right)\leq\operatorname{sp}\left((Q_{\pi}-Q_{\pi^{\prime}})(u-u^{\prime}) \right)\leq\operatorname{sp}\left(u-u^{\prime}\right).\]

This concludes the proof. 

By composition, we obtain the following result.

**Corollary 21**.: \(\mathfrak{L}\) _is (1) monotone, (2) non-span-expansive and (3) linear. Moreover, \(\operatorname{sp}\left(\mathfrak{L}u-\mathfrak{L}v\right)\leq\operatorname{ sp}\left(\mathcal{L}u-\mathcal{L}v\right)\) for all \(u,v\in\mathbf{R}^{\mathcal{S}}\)._

### Fix-points of \(\mathfrak{L}\) and (weak) optimism

**Lemma 22**.: \(\mathfrak{L}\) _has a fix-point in span semi-norm, i.e., \(\exists u\in\mathcal{H},\operatorname{sp}\left(\mathfrak{L}u-u\right)=0\)._

Proof.: The idea is to apply Brouwer's fix-point theorem in \(\mathbf{R}^{\mathcal{S}}\) quotiented by the equivalence relation \(u\sim v\Leftrightarrow\operatorname{sp}\left(u-v\right)=0\), where \(\operatorname{sp}\left(-\right)\) becomes a norm. By linearity (Corollary 21), \(\mathfrak{L}\) is well-defined in this quotient space, and if \(\mathfrak{L}\) is shown continuous on \(\mathbf{R}^{\mathcal{S}}\), so will it be on the quotient.

We show that \(\mathfrak{L}\) is sequentially continuous on \(\mathcal{H}\). Consider a sequence \(u_{n}\in\mathcal{H}^{\mathbf{N}}\) converging to \(u\in\mathcal{H}\) and fix \(\epsilon>0\). Provided that \(n>N_{e}\) for \(N_{e}\) large enough, we have \(\|u_{n}-u\|_{\infty}<\epsilon\), i.e., \(u_{n}-\epsilon e\leq u_{n}\leq u+\epsilon e\). Therefore, in the one hand, for all \(v\leq u_{n}\), we have \(v-\epsilon e\leq u\) so \(\max\left\{v\leq u_{n}:v\in\mathcal{H}\right\}\leq\max\left\{v\leq u:v\in \mathcal{H}\right\}+\epsilon e\); And on the other hand, for all \(v\leq u\), \(v+\epsilon e\leq u_{n}\) so \(\max\left\{v\leq u:v\in\mathcal{H}\right\}\leq\max\left\{v\leq u_{n}:v\in \mathcal{H}\right\}+\epsilon e\). Hence:

\[\|\max\left\{v\leq u:v\in\mathcal{H}\right\}-\max\left\{v\leq u_{n}:v\in \mathcal{H}\right\}\|\leq\epsilon.\]

It shows that \(\Gamma\) is continuous. The operator \(\mathcal{L}^{\beta}\) is obviously continuous as well, so \(\mathfrak{L}=\Gamma\circ\mathcal{L}^{\beta}\) is continuous by composition. Since \(\mathcal{H}=\mathcal{H}_{0}+\mathbf{R}e\) with \(\mathcal{H}_{0}\) compact and convex, the quotient \(\mathcal{H}/\sim\) is compact and convex, and is preserved by \(\mathfrak{L}/\sim\). By Brouwer's fix-point theorem, \(\mathfrak{L}/\sim\) has a fix-point in \(\mathcal{H}\)/\(\sim\). So \(\mathfrak{L}\) has a span fix-point in \(\mathcal{H}\). 

We write \(\operatorname{Fix}(\mathfrak{L})\) the span fix-points of \(\mathfrak{L}\).

**Lemma 23**.: \(\mathfrak{L}\) _has well-defined growth. Specifically, if \(\mathfrak{L}u=u+\mathfrak{L}e\), then:_

1. _There exists_ \(c>0\)_, s.t., for all_ \(v\in\mathcal{H}_{0}\)_,_ \((n-c)e+u\leq\mathfrak{L}^{n}v\leq(n\mathfrak{L}+c)e+u\)_;_
2. _If_ \(u^{\prime}\in\operatorname{Fix}(\mathfrak{L})\)_, then_ \(\mathfrak{L}u^{\prime}-u^{\prime}=\mathfrak{L}e\)_._

Proof.: Setting \(c:=\max_{v\in\mathcal{H}_{0}}\|v-u\|_{\infty}<\infty\), one can check that \(u-ce\leq v\leq u+ce\) for all \(v\in\mathcal{H}_{0}\). this proves (1) for \(n=0\) and we then proceed by induction on \(n\geq 0\). By induction, \(\mathfrak{L}^{n}v\leq u+(n\mathfrak{L}+c)e\) and by Corollary 21, \(\mathfrak{L}\) is monotone, so we have:

\[\mathfrak{L}^{n+1}v\leq\mathfrak{L}^{\mathfrak{L}^{n}}v\leq\mathfrak{L}(u+(n \mathfrak{L}+c)e)=u+((n+1)\mathfrak{L}+c)e\]

where the last inequality use the linearity of \(\mathfrak{L}\) together with \(\mathfrak{L}u=u+\mathfrak{L}e\). The lower bound of \(\mathfrak{L}^{n}v\) is shown similarly, establishing (1).

For (2), pick \(u^{\prime}\in\operatorname{Fix}(\mathfrak{L})\) with \(\mathfrak{L}u^{\prime}=u^{\prime}+\mathfrak{q}^{\prime}e\). Up to translating \(u^{\prime}\), we can assume that \(u^{\prime}\in\mathcal{H}_{0}\) and apply (1). We get:

\[(n\mathfrak{L}-c)e+u\leq n\mathfrak{L}^{\prime}e+u^{\prime}\leq(n\mathfrak{L}+c )e+u.\]

Divided by \(n\) and let it go to infinity. We conclude that \(\mathfrak{L}=\mathfrak{q}^{\prime}\). 

We finally have everything in hand to claim that \(\mathfrak{L}\) solves (18).

**Corollary 24**.: _The growth of \(\mathfrak{L}\) given by \(\mathfrak{L}=\mathfrak{L}u-u\) for \(u\in\operatorname{Fix}(\mathfrak{L})\) is well-defined, and:_

\[\forall u\in\mathcal{H},\quad\mathfrak{L}e=\liminf_{n\to\infty}\frac{ \mathfrak{L}^{n}u}{n}=\limsup_{n\to\infty}\frac{\mathfrak{L}^{n}u}{n}.\]

_Moreover, \(\mathfrak{L}\geq g^{*}(\mathcal{H},\beta,\mathcal{M})\)._Proof.: The growth property is a direct consequence of Lemma 23. We show \(\mathfrak{g}\geq g^{*}(\mathcal{H},\beta,\mathcal{M})\) which is defined in (18). Pick \(\pi\in\Pi,\widetilde{M}\in\mathcal{M}\) its model with \(\tilde{h}\equiv h(\pi,\widetilde{M})\) and \(\tilde{P}_{\pi}\tilde{h}\leq\tilde{P}_{\pi}\tilde{h}+\beta_{\pi}\) where \(\beta_{\pi}(s):=\beta(s,\pi(s))\). Up to translation, we can assume that \(\tilde{h}\in\mathcal{H}_{0}\).

We have \(g(\pi,\widetilde{M})=\tilde{g}e\) for \(\tilde{g}\in\mathbf{R}\), so

\[\tilde{h}+\tilde{g}e=\tilde{r}_{\pi}+\tilde{P}_{\pi}\tilde{h}\leq\mathfrak{G}\tilde {h}\]

by definition. By monotony of \(\mathfrak{G}\), see Corollary 21, \(n\tilde{g}e+\tilde{h}\leq\mathfrak{G}^{n}\tilde{h}\) follows by induction on \(n\geq 0\). By Lemma 23, we further have \(\mathfrak{G}^{n}\tilde{h}\leq n(\mathfrak{g}+c)e+u\) where \(u\in\operatorname{Fix}(\mathfrak{G})\). In tandem,

\[\tilde{g}e\leq\mathfrak{g}e+\frac{ce+u-\tilde{h}}{n}.\]

Letting \(n\to\infty\), we deduce that \(\tilde{g}\leq\mathfrak{g}\). Conclude by taking the best \(\pi\) and \(\widetilde{M}\). 

The next theorem follows directly with the same proof technique, and guarantees optimism.

**Theorem 25**.: _Assume that \(g^{*}+h^{*}\leq\mathfrak{G}h^{*}\). Then \(\mathfrak{g}\geq g^{*}\)._

The condition "\(g^{*}+h^{*}\leq\mathfrak{G}h^{*}\)" can be referred to as a _weak_ form of optimism. We qualify this version of optimism as _weak_ because it is much weaker than optimism property suggested by Fruit [2019]\(\mathcal{L}\geq L\) where \(L\) is the Bellman operator of the true MDP. Here, we only ask for \(\mathfrak{G}h^{*}\geq Lh^{*}\), i.e., optimism at a span fix-point of \(L\). This condition is met as soon as \(M\in\mathcal{M}\), \(h^{*}\in\mathcal{H}\) and \(\beta\) large enough.

### Modelization of the projected mitigated Bellman operator \(\mathfrak{G}\)

The aim of this paragraph is to establish Corollary 27, stating that \(\mathfrak{G}u\) can be viewed as a policy produced by \(\mathsf{Greedy}(\mathcal{M},u,\beta)\).

**Lemma 26** (Modelization).: _For \(\pi\in\Pi\), denote \(\beta_{\pi}(s):=\beta(s,\pi(s))\), \(\mathcal{R}_{\pi}:=\prod_{s}\mathcal{R}(s,\pi(s))\) and \(\mathcal{P}_{\pi}:=\prod_{s}\mathcal{P}(s,\pi(s))\). Fix \(u\in\mathbf{R}^{S}\) and let \(\pi:=\mathsf{Greedy}(\mathcal{M},u,\beta)\)._

1. _If_ \(\mathcal{P}\) _is convex, then there exists_ \((\tilde{r}_{\pi},\tilde{P}_{\pi})\in\mathcal{R}_{\pi}\times\mathcal{P}_{\pi}\) _such that_ \(\mathcal{L}_{\beta}u=\tilde{r}_{\pi}+\tilde{P}_{\pi}u\)_._
2. _Assume that_ \(\mathcal{L}_{\beta}u=\tilde{r}_{\pi}+\tilde{P}_{\pi}u\)_. There exists_ \(r_{\pi}^{\prime}\leq\tilde{r}_{\pi}\) _such that_ \(\mathfrak{G}u=r_{\pi}^{\prime}+\tilde{P}_{\pi}u\)_._

The convexity requirement of (1) is always true if the kernel confidence region is chosen via (**C1-4**).

Proof.: For (1), fix a state \(s\in\mathcal{S}\), let \(a:=\pi(s)\) and \(\rho:=\min(\sup\mathcal{P}(s,a)u,\,\hat{\rho}(s,a)u+\beta(s,a))\). If \(\rho=\sup\mathcal{P}(s,a)u\), then there is nothing to say because \(\mathcal{P}\) is compact, hence the sup is a max and \(\rho\) is of the form \(\tilde{p}(s,a)u\). Otherwise, let \(\tilde{p}(s,a)u>\hat{p}(s,a)u+\beta(s,a)\) with \(\tilde{p}(s,a)\in\mathcal{P}(s,a)\). Introduce, for \(\lambda\in[0,1]\),

\[\tilde{p}_{\lambda}(s,a):=\lambda\tilde{p}(s,a)+(1-\lambda)\hat{p}(s,a).\]

By continuity, there exists \(\lambda\in(0,1)\) such that \(\tilde{p}_{\lambda}(s,a)u=\hat{p}(s,a)u+\beta(s,a)\) and by convexity of \(\mathcal{P}(s,a)\), \(\tilde{p}_{\lambda}(s,a)\in\mathcal{P}(s,a)\). This proves (1).

For (2), recall that \(\mathfrak{G}u=\Gamma\mathcal{L}^{\beta}u=\Gamma(\tilde{r}_{\pi}+\tilde{P}_{\pi}u)\). Since \(\Gamma v\leq v\), for \(v\in\mathbf{R}^{S}\), we have:

\[\Gamma(\tilde{r}_{\pi}+\tilde{P}_{\pi}u)\leq\tilde{r}_{\pi}+\tilde{P}_{\pi}u.\]

Set \(r_{\pi}^{\prime}:=\Gamma(\tilde{r}_{\pi}+\tilde{P}_{\pi}u)-\tilde{P}_{\pi}u\). Check that \(r_{\pi}^{\prime}\) satisfies \(r_{\pi}^{\prime}\leq\tilde{r}_{\pi}\) and \(\mathfrak{G}u=r_{\pi}^{\prime}+\tilde{P}_{\pi}u\). 

The last corollary bellow is crucial to claim that greedy policies are good choices in \(\mathtt{PMEVI-DT}\).

**Corollary 27** (Greedy modelization).: _Let \(u\in\mathbf{R}^{S}\) and fix \(\pi:=\mathsf{Greedy}(\mathcal{M},u,\beta)\). If \(\mathcal{P}\) is convex, then with the notations of Lemma 26, there exists \(\tilde{r}_{\pi}\leq\sup\mathcal{R}_{\pi}\) and \(\tilde{P}_{\pi}\in\mathcal{P}_{\pi}\) such that \(\mathfrak{G}u=\tilde{r}_{\pi}+\tilde{P}_{\pi}u\)._

[MISSING_PAGE_FAIL:24]

Up to the constant \(\mathrm{sp}\left(h^{*}\right)\), the two error terms are respectively a navigation and a reward error. The second is bounded using Azuma's inequality (Lemma 32), showing that with probability \(1-2\delta\), we have:

\[\left|\sum\nolimits_{t=0}^{T-1}(R_{t}-r(X_{t}))\right|\leq\sqrt{\tfrac{1}{2}T \log\left(\tfrac{1}{\delta}\right)}.\]

We continue by using Freedman's inequality, instantiated in the form of Lemma 33. With probability \(1-\delta\), we have:

\[\left|\sum\nolimits_{t=0}^{T-1}\left(p(X_{t})-e_{S_{t+1}}\right)h^{*}\right| \leq\sqrt{2\sum\nolimits_{t=0}^{T-1}\mathbf{V}(p(X_{t}),h^{*})\log\left(\tfrac {T}{\delta}\right)}+4\mathrm{sp}\left(h^{*}\right)\log\left(\tfrac{T}{\delta} \right).\]

The quantity \(\sum\nolimits_{t=0}^{T-1}\mathbf{V}(p(X_{t}),h^{*})\) is a classical one that appears at several places throughout the analysis. Using Lemma 29, we bound it explicitely. Further simplifying the bound with \(\sqrt{a+b}\leq\sqrt{a}+\sqrt{b}\), we get that with probability \(1-4\delta\), we have:

\[\mathrm{A}\leq\left\{\begin{array}{c}\sqrt{2\mathrm{sp}\left(h^{*}\right) \mathrm{sp}\left(r\right)T\log\left(\tfrac{T}{\delta}\right)}+\sqrt{\tfrac{1}{ 2}T\log\left(\tfrac{1}{\delta}\right)}+\sqrt{2\mathrm{sp}\left(h^{*}\right) \log\left(\tfrac{T}{\delta}\right)\sum\nolimits_{t=0}^{T-1}\Delta^{*}(X_{t})} \\ +\mathrm{sp}\left(h^{*}\right)\left(\tfrac{1}{2}T\right)^{\frac{1}{4}}\log \tfrac{1}{\delta}\left(\tfrac{T}{\delta}\right)+4\mathrm{sp}\left(h^{*}\right) \log\left(\tfrac{T}{\delta}\right)+2\mathrm{sp}\left(h^{*}\right)\end{array} \right\}.\]

Bound \(\log(\tfrac{1}{\delta})\) by \(\log(\tfrac{T}{\delta})\) and use \(\sqrt{a}+\sqrt{b}\leq 2\sqrt{a+b}\) to merge the terms in \(\sqrt{T\log(\tfrac{T}{\delta})}\) under a single square-root. 

Overall, Lemma 30 states that the regret \(\sum\nolimits_{t=0}^{T-1}(g^{*}-R_{t})\) and the pseudo-regret \(\sum\nolimits_{t=0}^{T-1}\Delta^{*}(X_{t})\) differ by about \((\mathrm{sp}\left(h^{*}\right)T\log(\tfrac{T}{\delta}))^{1/2}\) in probability (up to asymptotically negligible additional terms). In general, the precise form of Lemma 30 is not convenient to use because it is of form form \(x\leq y+\alpha\sqrt{y}+\beta\) that is not linear in \(y\). Corollary 31 factorizes the result into one which will be more convenient in proofs.

**Corollary 31**.: _Denote \(x:=\sum\nolimits_{t=0}^{T-1}(g^{*}-R_{t})\) and \(y:=\sum\nolimits_{t=0}^{T-1}\Delta^{*}(X_{t})\). Further introduce:_

\[\alpha :=\sqrt{2\mathrm{sp}\left(h^{*}\right)\log\left(\tfrac{T}{\delta} \right)}\] \[\beta :=2\sqrt{\left(2\mathrm{sp}\left(h^{*}\right)\mathrm{sp}\left(r \right)+\tfrac{1}{2}\right)T\log\left(\tfrac{T}{\delta}\right)}+\mathrm{sp} \left(h^{*}\right)\left(\tfrac{1}{2}T\right)^{\frac{1}{4}}\log^{\frac{3}{4}} \left(\tfrac{T}{\delta}\right)+2\mathrm{sp}\left(h^{*}\right)\left(2\log \left(\tfrac{T}{\delta}\right)+1\right).\]

_Then, with probability \(1-4\delta\), we have \(\sqrt{x}\leq\sqrt{y}+\tfrac{1}{2}\alpha+\sqrt{\beta}\) and \(\sqrt{y}\leq\sqrt{x}+\alpha+\sqrt{\beta}\)._

Proof.: This is straight forward algebra from the result of Lemma 30. 

### Proof of Lemma 6, reward optimism

We start by getting rid of the reward noise. We have:

\[\mathrm{Reg}(T) :=\sum\nolimits_{t=0}^{T-1}(g^{*}-R_{t})=\sum\nolimits_{t=0}^{T-1 }(g^{*}-r(X_{t}))+\sum\nolimits_{t=0}^{T-1}(r(X_{t})-R_{t})\] \[\leq\sum\nolimits_{t=0}^{T-1}(g^{*}-r(X_{t}))+\sqrt{\tfrac{1}{2}T \log\left(\tfrac{1}{\delta}\right)}\]

with probability \(1-\delta\) by Azuma's inequality (Lemma 32). We are left with \(\sum\nolimits_{t=0}^{T-1}(g^{*}-r(X_{t}))\). We continue by splitting the regret episodically and invoking optimism. By Lemma 13, with probability \(1-4\delta\), we have \(\sum\nolimits_{t=0}^{T-1}(g^{*}-r(X_{t}))\leq\sum\nolimits_{k}\sum\nolimits_{t =t_{k}}^{h_{k+1}-1}(\mathfrak{g}_{k}-r(X_{t}))\). Introduce

\[B_{0}(T):=\sum\limits_{k}\sum\limits_{t=t_{k}}^{h_{k+1}-1}(\mathfrak{g}_{k}-r(X _{t})).\] (20)

We focus on bounding \(B_{0}(T)\). By Assumption 2, \(\tilde{r}_{k}(s,a)\) is of the form \(\hat{r}_{k}(s,a)+\sqrt{C\log(2SAT/\delta)/N_{t_{k}}(s,a)}-\eta_{k}(s,a)\) with \(\eta_{k}(s,a)\in\mathbf{R}\). By the statement (3) of Proposition 2, \(\eta_{k}(s,a)\geq 0\). Therefore,

\[B_{0}(T)=\sum\limits_{k}\sum\limits_{t=t_{k}}^{h_{k+1}-1}(\mathfrak{g}_{k}- \tilde{r}_{k}(X_{t}))+\sum\limits_{k}\sum\limits_{t=t_{k}}^{h_{k+1}-1}(\tilde{r }_{k}(X_{t})-r(X_{t}))\]\[\leq\sum_{k}\sum_{t=t_{k}}^{h_{k1}-1}\left(\mathfrak{g}_{k}-\tilde{r}_{ k}(X_{t})\right)+SA+\sum_{k}\sum_{t=t_{k}}^{h_{k1}-1}\mathbf{1}\left(N_{t_{k}}(X_{t}) \geq 1\right)\left(\hat{r_{k}}(X_{t})-r(X_{t})+\sqrt{\frac{C\log\left(\frac{2 SAT}{\delta}\right)}{N_{t_{k}}(X_{t})}}\right)\] \[\stackrel{{(*)}}{{\leq}}\sum_{k}\sum_{t=t_{k}}^{h_{k1 }-1}\left(\mathfrak{g}_{k}-\tilde{r}_{k}(X_{t})\right)+SA+\sum_{k}\sum_{t=t_{k} }^{h_{k1}-1}\mathbf{1}\left(N_{t_{k}}(X_{t})\geq 1\right)\left(\sqrt{\frac{2 \log\left(\frac{2SAT}{\delta}\right)}{N_{t_{k}}(s,a)}}+\sqrt{\frac{C\log\left( \frac{2SAT}{\delta}\right)}{N_{t_{k}}(s,a)}}\right)\]

where \((*)\) holds with probability \(1-\delta\) following Lemma 35. By the doubling trick rule (DT), we have \(N_{t}(X_{t})\leq 2N_{h}(X_{t})\) for \(t<t_{k+1}\), so, with probability \(1-\delta\),

\[B_{0}(T) \leq\sum_{k}\sum_{t=t_{k}}^{h_{k1}-1}\left(\mathfrak{g}_{k}- \tilde{r}_{k}(X_{t})\right)+SA+2\sum_{k}\sum_{t=t_{k}}^{h_{k1}-1}\mathbf{1} \left(N_{t_{k}}(X_{t})\geq 1\right)\sqrt{\frac{(2+C)\log\left(\frac{2SAT}{ \delta}\right)}{N_{t_{k}}(s,a)}}\] \[\leq\sum_{k}\sum_{t=t_{k}}^{h_{k1}-1}\left(\mathfrak{g}_{k}- \tilde{r}_{k}(X_{t})\right)+SA+2\sqrt{(2+C)\log\left(\frac{2SAT}{\delta} \right)}\sum_{s,a}\sum_{n=1}^{N_{T}(s,a)-1}\sqrt{\frac{1}{n}}\] \[\leq\sum_{k}\sum_{t=t_{k}}^{h_{k1}-1}\left(\mathfrak{g}_{k}- \tilde{r}_{k}(X_{t})\right)+SA+4\sqrt{(2+C)\log\left(\frac{2SAT}{\delta} \right)}\sum_{s,a}\sqrt{N_{T}(s,a)}\] \[\text{(Jensen)} \leq\sum_{k}\sum_{t=t_{k}}^{h_{k1}-1}\left(\mathfrak{g}_{k}- \tilde{r}_{k}(X_{t})\right)+SA+4\sqrt{(2+C)SAT\log\left(\frac{2SAT}{\delta} \right)}.\]

We conclude that with probability \(1-6\delta\), we have:

\[\text{Reg}(T)\leq\sum_{k}\sum_{t=t_{k}}^{h_{k1}-1}\left(\mathfrak{g}_{k}- \tilde{r}_{k}(X_{t})\right)+4\sqrt{(2+C)SAT\log\left(\frac{2SAT}{\delta} \right)}+\sqrt{\frac{1}{2}T\log\left(\frac{2SAT}{\delta}\right)}+SA.\] (21)

This concludes the proof. 

### Proof of Lemma 7, navigation error

We have:

\[\sum_{k}\sum_{t=t_{k}}^{h_{k1}-1}(p_{k}(S_{t})-e_{S_{t}})b_{k} \leq\sum_{k}\sum_{t=t_{k}}^{h_{k1}-1}(p_{k}(S_{t})-e_{S_{t+1}}) \mathfrak{h}_{k}+\sum_{k}\text{sp}\left(\mathfrak{h}_{k}\right)\] \[\leq\underbrace{\sum_{k}\sum_{t=t_{k}}^{h_{k1}-1}(p_{k}(S_{t})-e _{S_{t+1}})(\mathfrak{h}_{k}-h^{*})}_{\text{A}_{1}}+\underbrace{\sum_{k}\sum_{t =t_{k}}^{h_{k1}-1}(p_{k}(S_{t})-e_{S_{t+1}})h^{*}}_{\text{A}_{2}}+\sum_{k} \text{sp}\left(\mathfrak{h}_{k}\right).\]

The last term is \(\text{O}(c_{0}SA\log(T))\) by Lemma 28, hence is \(\text{O}(T^{1/5}\log(T))\).

**(STEP 1)** We start by bounding \(\text{A}_{1}\). By Lemma 13, with probability \(1-4\delta\), we have \(h^{*}\in\mathcal{H}_{t_{k}}\) for all \(k\leq K(T)\). So \(\text{sp}\left(\mathfrak{h}_{k}-h^{*}\right)\leq\text{sp}\left(\mathfrak{h}_{k} \right)+\text{sp}\left(h^{*}\right)\leq 2c_{0}\). By Freedman's inequality invoked in the form of Lemma 33, we have with probability \(1-5\delta\),

\[\text{A}_{1}\leq\sqrt{2\sum_{k}\sum_{t=t_{k}}^{h_{k1}-1}\mathbf{V}\left(p(X_{t} ),\mathfrak{h}_{k}-h^{*}\right)\log\left(\frac{T}{\delta}\right)}+8c_{0}\log \left(\frac{T}{\delta}\right)\]

It suffices to bound the first term. Recall that \(e\) is the vector full of ones. We have:

\[\sum_{k}\sum_{t=t_{k}}^{h_{k1}-1}\mathbf{V}(p(X_{t}),\mathfrak{h}_ {k}-h^{*})=\sum_{k}\sum_{t=t_{k}}^{h_{k1}-1}\mathbf{V}\left(p(X_{t}),\mathfrak{h }_{k}-h^{*}-\left(\mathfrak{h}_{k}(S_{t})-h^{*}(S_{t})\right)\cdot e\right)\] \[\leq\sum_{k}\sum_{t=t_{k}}^{h_{k1}-1}\sum_{s^{\prime}\in\mathcal{ S}}p(s^{\prime}|X_{t})\left(\mathfrak{h}_{k}(s^{\prime})-h^{*}(s^{\prime})-\left( \mathfrak{h}_{k}(S_{t})-h^{*}(S_{t})\right)\right)^{2}\]\[\stackrel{{(*)}}{{\leq}}3\sum_{k}\sum_{t=t_{k}}^{t_{k+1}-1} \mathbf{E}\left[\sum_{s^{\prime}\in S}p(s^{\prime}|X_{t})\left(b_{k}(s^{\prime})- h^{*}(s^{\prime})-\left(b_{k}(S_{t})-h^{*}(S_{t})\right)\right)^{2}\left|\mathcal{F}_{t} \right|+16c_{0}^{2}\log\left(\frac{1}{\delta}\right)\right.\] \[=3\sum_{k}\sum_{t=t_{k}}^{t_{k+1}-1}\left(b_{k}(S_{t+1})-h^{*}(S_ {t+1})-\left(b_{k}(S_{t})-h^{*}(S_{t})\right)\right)^{2}+16c_{0}^{2}\log\left( \frac{1}{\delta}\right).\]

Here the inequality \((*)\) holds with probability \(1-\delta\) following Lemma40. We will bound the summand with the bias estimation error error\((c_{k},s,s^{\prime})\) that spawns the inner regret estimation \(B_{0}(t_{k})=\sum_{\ell=1}^{k-1}\sum_{\ell=t_{\ell}}^{t_{k-1}-1}(g_{\ell}-R_{t})\). This inner estimation is linked to \(B(T):=\sum_{k,t}(\mathfrak{g}_{k}-R_{t})\) the overall optimistic regret by:

\[B_{0}(t_{k}) \leq\sum_{\ell=1}^{K(T)}\sum_{t=t_{\ell}}^{t_{k+1}-1}\left( \mathfrak{g}_{k}-R_{t}\right)-\sum_{\ell=k}^{K(T)}\sum_{t=t_{\ell}}^{t_{k+1}- 1}\left(\mathfrak{g}_{k}-R_{t}\right)\] \[\stackrel{{(*)}}{{\leq}}\sum_{\ell=1}^{K(T)}\sum_{t= t_{\ell}}^{t_{k+1}-1}\left(\mathfrak{g}_{k}-R_{t}\right)-\sum_{\ell=k}^{K(T)} \sum_{t=t_{\ell}}^{t_{\ell+1}-1}\left(g^{*}-R_{t}\right)\] \[\leq\sum_{\ell=1}^{K(T)}\sum_{t=t_{\ell}}^{t_{k+1}-1}\left( \mathfrak{g}_{k}-R_{t}\right)-\sum_{\ell=k}^{K(T)}\sum_{t=t_{k}}^{T-1}\left( \left(p(X_{t})-e_{S_{s^{\prime}}}\right)h^{*}+r(X_{t})-R_{t}\right)\] \[\stackrel{{(\dagger)}}{{\leq}}\sum_{\ell=1}^{K(T)} \sum_{t=t_{\ell}}^{t_{k+1}-1}\left(\mathfrak{g}_{k}-R_{t}\right)+\operatorname {sp}\left(h^{*}\right)+\left(1+\operatorname{sp}\left(h^{*}\right)\right)\sqrt {\frac{1}{2}T\log\left(\frac{1}{\delta}\right)}\] \[=:B(T)+\operatorname{sp}\left(h^{*}\right)+\left(1+\operatorname{ sp}\left(h^{*}\right)\right)\sqrt{\frac{1}{2}T\log\left(\frac{1}{\delta}\right)}.\]

In the above, \((*)\) holds with probability \(1-4\delta\) uniformly on \(k\) following Lemma13 and \((\dagger)\) holds, also uniformly on \(k\), with probability \(1-\delta\) by applying Azuma-Hoeffding's inequality (Lemma32). Continuing, still on the event specified by Lemma13, we have with probability \(1-6\delta\):

\[\sum_{k}\sum_{t=t_{k}}^{t_{k+1}-1}\mathbf{V}(p(X_{t}),\mathfrak{b }_{k}-h^{*}) \leq 3\sum_{k}\sum_{t=t_{k}}^{t_{k+1}-1}\frac{3c_{0}+\left(1+c_{0} \right)\sqrt{8t_{k}\log\left(\frac{2}{\delta}\right)}+2B_{0}(t_{k})}{N_{k}(S_{ t+1}\leftrightarrow S_{t})}+16c_{0}^{2}\log\left(\frac{1}{\delta}\right)\] \[\leq 3\sum_{k}\sum_{t=t_{k}}^{t_{k+1}-1}\frac{4c_{0}+\left(1+c_{0} \right)\sqrt{32T\log\left(\frac{2}{\delta}\right)}+2B(T)}{N_{t_{k}}(S_{t},A_{t },S_{t+1})}+16c_{0}^{2}\log\left(\frac{1}{\delta}\right)\] \[(\text{DT}) \leq 12c_{0}^{2}S^{2}A+3\left(4c_{0}+\left(1+c_{0}\right)\sqrt{32T \log\left(\frac{2}{\delta}\right)}+2B(T)\right)S^{2}A\log(T)\] \[+16c_{0}^{2}\log\left(\frac{1}{\delta}\right).\]

**(STEP 2)** For \(\mathrm{A}_{2}\), by Freedman's inequality invoked in the form of Lemma33 again, we have with probability \(1-\delta\),

\[\mathrm{A}_{2} \leq\sqrt{2\sum_{k}\sum_{t=t_{k}}^{t_{k+1}-1}\mathbf{V}(p_{k}(S_{ t}),h^{*})\log\left(\frac{T}{\delta}\right)}+8c_{0}\log\left(\frac{T}{\delta}\right)\] \[\leq\sqrt{2\sum_{k=0}^{T-1}\mathbf{V}(p(X_{t}),h^{*})\log\left( \frac{T}{\delta}\right)}+8c_{0}\log\left(\frac{T}{\delta}\right).\]

We recognize the sum of variance \(\sum_{t=0}^{T-1}\mathbf{V}(p(X_{t}),h^{*})\) that we leave as is.

**(STEP 3)** As a result, with probability \(1-7\delta\), we have:

\[\sum_{k}\sum_{t=t_{k}}^{t_{k+1}-1}(p_{k}(S_{t})-e_{S_{s}})\mathfrak{b}_{k}\leq \sqrt{2\sum_{t=0}^{T-1}\mathbf{V}(p(X_{t}),h^{*})\log\left(\frac{T}{\delta} \right)}+2SA^{\frac{1}{2}}\sqrt{3B(T)}\log\left(\frac{T}{\delta}\right)+ \operatorname{O}\left(SA^{\frac{1}{2}}T^{\frac{2}{\delta}}\log^{\frac{1}{2}} \left(\frac{T}{\delta}\right)\right)\]

when \(c_{0}=T^{\frac{1}{2}}\). \(\Box\)

[MISSING_PAGE_EMPTY:28]

\[\leq\frac{\sqrt{\left|\mathbf{V}(\hat{p}_{k}(S_{t}),h^{\prime}_{k})- \mathbf{V}(\hat{p}_{k}(X_{t}),h^{\prime})\right|}\sqrt{\left|\mathbf{V}(\hat{p}_ {k}(S_{t}),h^{*})-\mathbf{V}(p(X_{t}),h^{*})\right|}+\sqrt{\mathbf{V}(p(X_{t}),h ^{*})}\] \[\stackrel{{(*)}}{{\leq}}\sqrt{8c_{0}\sum_{s^{\prime} \in\mathcal{S}}\hat{p}_{k}(s^{\prime}|S_{t})d_{k}(s^{\prime},S_{t})}+\mathrm{ sp}\left(h^{*}\right)\sqrt{\left|\hat{p}_{k}(S_{t})-p_{k}(S_{t})\right|_{1}}+\sqrt{ \mathbf{V}(p(X_{t}),h^{*})}\] \[\stackrel{{(\dagger)}}{{\leq}}\sqrt{8c_{0}\sum_{s^{ \prime}\in\mathcal{S}}\hat{p}_{k}(s^{\prime}|S_{t})d_{k}(s^{\prime},S_{t})}+ \mathrm{sp}\left(h^{*}\right)\left(\frac{S\log\left(\frac{SAT}{\delta}\right)} {N_{t_{k}}(X_{t})}\right)^{\frac{1}{2}}+\sqrt{\mathbf{V}(p(X_{t}),h^{*})}\] \[\leq\frac{\mathrm{A}_{2}}{\sqrt{2N_{t_{k}}(X_{t})}}+\mathrm{sp} \left(h^{*}\right)\left(\frac{S\log\left(\frac{SAT}{\delta}\right)}{N_{t_{k} }(X_{t})}\right)^{\frac{1}{4}}+\sqrt{\mathbf{V}(p(X_{t}),h^{*})}\]

where \((*)\) is obtained by applying Lemma12 and \((\dagger)\) holds with probability \(1-\delta\) by applying Weissman's inequality, see Lemma35. All together, with probability \(1-6\delta\), \(\mathrm{A}\) is upper-bounded by:

\[\mathrm{A}\leq\sqrt{\frac{2\mathbf{V}(p(X_{t}),h^{*})\log\left(\frac{SAT}{ \delta}\right)}{N_{t_{k}}(X_{t})}}+2\mathrm{A}_{2}+\underbrace{\mathrm{sp} \left(h^{*}\right)\sqrt{\frac{2\log\left(\frac{SAT}{\delta}\right)\sqrt{S \log\frac{SAT}{\delta}}}{N_{t_{k}}(X_{t})\sqrt{N_{t_{k}}(X_{t})}}}+\frac{3c_{0} \log\left(\frac{SAT}{\delta}\right)}{N_{t_{k}}(X_{t})}}_{\mathrm{A}_{\gamma _{t}}(\hat{x}_{t})}.\]

**(STEP 2)** The number of visits \(N_{k}(X_{t})\) is lower-bounded by \(\frac{1}{2}N_{t}(X_{t})\) when \(N_{k}(X_{t})\geq 1\) by doubling trick (DT). By summing over \(t\) and \(k\), we find that with probability \(1-6\delta\),

\[\sum_{k}(3k) \leq SAc_{0}+\sum_{k}\sum_{t=t_{k}}^{t_{k+1}-1}\mathbf{1}_{N_{t_{k }}(X_{t})\geq 1}\sqrt{\frac{2\mathbf{V}(p(X_{t}),h^{*})\log\left(\frac{SAT}{ \delta}\right)}{N_{t}(X_{t})}}+\sum_{k}\sum_{t=t_{k}}^{t_{k+1}-1}\mathbf{1}_{N_ {t_{k}}(X_{t})\geq 1}(2\mathrm{A}_{2}(k,t)+\mathrm{A}_{3}(k,t))\] (DT) \[\leq SAc_{0}+2\sum_{k}\sum_{t=t_{k}}^{t_{k+1}-1}\mathbf{1}_{N_{t_{k }}(X_{t})\geq 1}\sqrt{\frac{2\mathbf{V}(p(X_{t}),h^{*})\log\left(\frac{ SAT}{\delta}\right)}{N_{t}(X_{t})}}+\sum_{k}\sum_{t=t_{k}}^{t_{k+1}-1}\mathbf{1}_{N_{t_{k }}(X_{t})\geq 1}(2\mathrm{A}_{2}(k,t)+\mathrm{A}_{3}(k,t))\] \[\leq SAc_{0}+4\sqrt{2S\,A\sum_{t=0}^{T-1}\mathbf{V}(p(X_{t}),h^{*}) \log\left(\frac{SAT}{\delta}\right)}+\sum_{k}\sum_{t=t_{k}}^{t_{k+1}-1} \mathbf{1}_{N_{t_{k}}(X_{t})\geq 1}(2\mathrm{A}_{2}(k,t)+\mathrm{A}_{3}(k,t))\]where the last inequality is obtained with computations that are similar to those detailed in the proof of Lemma 8. We recognize the variance that we will leave as is. We finish the proof by bounding the lower order terms \(\mathrm{A}_{2}\) and \(\mathrm{A}_{3}\).

**(STEP 3)** We start with \(\mathrm{A}_{2}\). We have:

\[\sum_{k}\sum_{l=l_{k}}^{h_{k1}-1}\mathbf{1}_{N_{k}(X_{l})\geq 1} \mathrm{A}_{2}(k,t) :=\sum_{k}\sum_{t=l_{k}}^{h_{k1}-1}\mathbf{1}_{N_{k}(X_{l})\geq 1} \frac{\sqrt{16c_{0}\log\left(\frac{SAT}{\delta}\right)\mathcal{S}\left(\left(1+c _{0}\right)\left(3+2\sqrt{8T\log\left(\frac{2}{\delta}\right)}+2B(T)\right) \right)}}{N_{k}(X_{l})}\] \[(\mathrm{DT}) \leq 2\sqrt{16c_{0}S\,\log\left(\frac{SAT}{\delta}\right)\left( \left(1+c_{0}\right)\left(3+2\sqrt{8T\log\left(\frac{2}{\delta}\right)}+2B(T) \right)\right)}\,SA\log(T)\] \[\leq 8(1+c_{0})S^{\frac{1}{2}}A\log^{\frac{1}{2}}\left(\frac{ SAT}{\delta}\right)\left(2+4T^{\frac{1}{2}}\log^{\frac{1}{2}}\left(\frac{SAT}{ \delta}\right)+\sqrt{2B(T)}\right).\]

**(STEP 4)** We are left with \(\mathrm{A}_{3}\). We have:

\[\sum_{k}\sum_{l=l_{k}}^{l_{k1}-1}\mathbf{1}_{N_{k}(X_{l})\geq 1} \mathrm{A}_{3}(k,t) :=\sum_{k}\sum_{t=l_{k}}^{h_{k1}-1}\mathbf{1}_{N_{k}(X_{l})\geq 1 }\left(\mathrm{sp}\left(h^{*}\right)\sqrt{\frac{2\log\left(\frac{SAT}{\delta} \right)\sqrt{S\,\log\frac{SAT}{\delta}}}{N_{l_{k}}(X_{l})\sqrt{N_{l_{k}}(X_{l })}}}+\frac{3c_{0}\log\left(\frac{SAT}{\delta}\right)}{N_{l_{k}}(X_{l})}\right)\] \[(\mathrm{DT}) \leq\sum_{k}\sum_{l=l_{k}}^{h_{k1}-1}\mathbf{1}_{N_{k}(X_{l}) \geq 1}\left(\mathrm{sp}\left(h^{*}\right)\sqrt{\frac{2\log\left(\frac{ SAT}{\delta}\right)\sqrt{S\,\log\frac{SAT}{\delta}}}{N_{h}(X_{l})\sqrt{N_{l_{k}}(X_{l })}}}+\frac{3c_{0}\log\left(\frac{SAT}{\delta}\right)}{N_{l_{k}}(X_{l})}\right)\] \[\leq C\mathrm{sp}\left(h^{*}\right)S^{\frac{1}{4}}AT^{\frac{1}{4} }\log^{\frac{1}{2}}\left(\frac{SAT}{\delta}\right)+6c_{0}SA\log\left(\frac{ SAT}{\delta}\right)\] \[=\mathrm{O}\left(\mathrm{sp}\left(h^{*}\right)S^{\frac{1}{4}}AT^{ \frac{1}{4}}\log\left(\frac{SAT}{\delta}\right)\right).\]

This concludes the proof. 

### Proof of Lemma 10, second order error

Recall that by Lemma 13, with probability \(1-4\delta\), \(h^{*}\in\mathcal{H}_{h}\) for all \(k\), hence \(\mathrm{sp}\left(\mathrm{b}_{k}-h^{*}\right)\leq 2c_{0}\) for all \(k\) on the same event. Therefore, with probability \(1-4\delta\),

\[\sum_{k}(4k) :=2c_{0}SA+\sum_{k}\sum_{t=l_{k}}^{h_{k1}-1}\mathbf{1}_{N_{k}(X_ {l})\geq 1}\left(\hat{p}_{k}(S_{t})-p_{k}(S_{t})\right)\left(\mathrm{b}_{k}-h ^{*}\right)\] \[=2c_{0}SA+\sum_{k}\sum_{t=l_{k}}^{h_{k1}-1}\sum_{s^{\prime}\in \mathcal{S}}\mathbf{1}_{N_{k}(X_{l})\geq 1}(\hat{p}_{k}(s^{\prime}|S_{t})-p_{k}(s^{ \prime}|S_{t}))(\mathrm{b}_{k}-h^{*}(s^{\prime}))\] \[\stackrel{{(*)}}{{\leq}}2c_{0}SA+2\sum_{k}\sum_{t=l_{k }}^{h_{k1}-1}\sum_{s^{\prime}\in\mathcal{S}}\mathbf{1}_{N_{k}(X_{l})\geq 1}(\hat{p}_{k}(s ^{\prime}|S_{t})-p_{k}(s^{\prime}|S_{t}))d_{k}(s^{\prime},S_{t})\] \[\stackrel{{(*)}}{{\leq}}2c_{0}SA+2\sum_{k}\sum_{t=l_{ k}}^{h_{k1}-1}\sum_{s^{\prime}\in\mathcal{S}}\mathbf{1}_{N_{k}(X_{l})\geq 1} \left(d_{k}(s^{\prime},S_{t})\sqrt{\frac{2\hat{p}_{k}(s^{\prime}|S_{t})\log \left(\frac{S^{2}AT}{\delta}\right)}{N_{l_{k}}(X_{t})}}+3d_{k}(s^{\prime}|S_{t })\frac{\log\left(\frac{S^{2}AT}{\delta}\right)}{N_{l_{k}}(X_{t})}\right)\] \[\leq 2c_{0}SA+2\sum_{k}\sum_{t=l_{k}}^{h_{k1}-1}\sum_{s^{\prime}\in \mathcal{S}}\mathbf{1}_{N_{k}(X_{l})\geq 1}\left(\sqrt{c_{0}}\sqrt{\frac{2\hat{p}_{k}(s ^{\prime}|S_{t})d_{k}(s^{\prime},S_{t})\log\left(\frac{S^{2}AT}{\delta} \right)}{N_{l_{k}}(X_{t})}}+\frac{3c_{0}\log\left(\frac{S^{2}AT}{\delta} \right)}{N_{l_{k}}(X_{t})}\right)\] \[\leq 2c_{0}SA+4\sum_{k}\sum_{t=l_{k}}^{h_{k1}-1}\sum_{s^{\prime}\in \mathcal{S}}\mathbf{1}_{N_{k}(X_{l})\geq 1}\left(\sqrt{c_{0}}\sqrt{\frac{2\hat{p}_{k}(s ^{\prime}|S_{t})d_{k}(s^{\prime},S_{t})\log\left(\frac{S^{2}AT}{\delta}\right)} {N_{l}(X_{t})}}+\frac{3c_{0}\log\left(\frac{S^{2}AT}{\delta}\right)}{N_{l}(X_{ t})}\right)\]

where \((*)\) uses that \(h^{*}\in\mathcal{H}_{h_{k}}\), and \((\dagger)\) is obtained by applying the empirical Bernstein's inequality, see Lemma 36, to \(\hat{p}_{k}(s^{\prime}|S_{t})-p_{k}(s^{\prime}|S_{t})\), and holds with probability \(1-\delta\). The rightmost term's sum is upper-bounded by:

\[4\sum_{k}\sum_{t=n_{k}}^{n_{k}-1}\sum_{s^{\prime}\in S}\frac{3c_{0}\log\left(\frac {S^{2}A\Gamma}{\delta}\right)}{N_{t}(X_{t})}\leq 12S^{2}A\log(T)\log\left(\frac{S^{2} A\Gamma}{\delta}\right).\]

For the other term, follow the line of the proof of Lemma 9 (term \(\mathrm{A}_{2}\)). We have with probability \(1-5\delta\) (\(4\delta\) of which is by invoking Lemma 13):

\[\hat{p}_{k}(s^{\prime}|S_{t})d_{k}(s^{\prime},S_{t}) =\frac{N_{t_{k}}(S_{t},A_{t},s^{\prime})\left((1+c_{0})\left(1+ \sqrt{8t_{k}\log\left(\frac{2}{\delta}\right)}\right)+2B_{0}(t_{k})\right)}{N_ {t_{k}}(S_{t}\leftrightarrow s^{\prime})N_{t_{k}}(X_{t})}\] \[\leq\frac{\left((1+c_{0})\left(3+2\sqrt{8T\log\left(\frac{2}{ \delta}\right)}+2B(T)\right)\right)}{N_{t_{k}}(X_{t})}.\]

Therefore,

\[\sqrt{c_{0}}\sqrt{\frac{2\hat{p}_{k}(s^{\prime}|S_{t})d_{k}(s^{\prime},S_{t}) \log\left(\frac{S^{2}A\Gamma}{\delta}\right)}{N_{t}(X_{t})}}\leq\frac{4(1+c_{ 0})\sqrt{\left(3+2\sqrt{8T\log\left(\frac{2}{\delta}\right)}+2B(T)\right)\log \left(\frac{S^{2}A\Gamma}{\delta}\right)}}{N_{t}(X_{t})}.\]

Summing over \(k\), \(t\), \(s^{\prime}\), with probability \(1-6\delta\), we have:

\[\sum_{k}(4k)\leq\begin{pmatrix}16S^{2}A(1+c_{0})\log^{\frac{1}{2}\left(\frac {S^{2}A\Gamma}{\delta}\right)}\left(\sqrt{2B(T)}+2\left(8T\log\left(\frac{2} {\delta}\right)\right)^{\frac{1}{2}}\right)\\ +32S^{2}A\left(\log(T)\log\left(\frac{S^{2}A\Gamma}{\delta}\right)+(1+c_{0}) \log^{\frac{1}{2}\left(\frac{S^{2}A\Gamma}{\delta}\right)}\right)\end{pmatrix}\]

This concludes the proof.

More details on experiments

### River swim as a hard communicating environment

Experiments of Fig.2 are run on \(n\)-states river-swim. Such MDPs are, despite their size, known to be hard to learn. They consists in \(n\) states aligned in a straight line with two playable actions right and left whose dynamics are given in the figure below. Rewards are Bernoulli and null everywhere excepted for \(r(s_{n},\textsc{right})=0.95\) and \(r(s_{0},\textsc{left})=0.05\).

3-state river-swim.The gain is \(g^{*}\approx 0.82\) and \(h^{*}\approx(-4.28,-2.24,0.4)\).

5-state river-swim.The gain is \(g^{*}\approx 0.82\) and \(h^{*}\approx(-9.62,-7.58,-4.96,-2.27,0.45)\).

### Experiments in weakly-communicating environments

Beyond communicating models, PMEVI-DT is superior to EVI-based methods. On Fig.4, we see that PMEVI-DT can learn in environments of infinite diameter while UCRL2 cannot. The gain is due to the truncation operation, that makes sure that the optimistic bias vector has span less than \(T^{1/5}\).

Without the truncation operation, EVI-based methods can never reject the plausibility that the reward at \(s_{0}\) is maximal (equal to one) and that there is a positive probability \(\epsilon\) to switch from \(s_{1}\) to \(s_{0}\) when taking the sub-optimal action from \(s_{1}\). More precisely, denote the actions \(a,b\) where \(r(s_{0},a)=1\), \(r(s_{1},a)=0.5\) and \(r(s_{1},b)=0.1\), so that \(a\) is the optimal action and \(b\) is sub-optimal. There are two policies \(\pi_{a}\) and \(\pi_{b}\) choosing action \(a\) and \(b\) from \(s_{1}\) respectively. Because \(s_{0}\) is only visited once, the best reward that can be achieved from \(s_{0}\) is \(\tilde{r}_{t}(s_{0},a)=1\) at all times. If one runs UCRL2, the confidence region for transition kernels is roughly of the form \(\mathcal{P}_{r}(x)=\{\tilde{p}(x):N_{t}(x)\,\|\tilde{p}(x)-\tilde{p}_{t}(x)\| _{1}^{2}\leq C_{p}\log(t)\}\) and the plausible transition kernel \(\tilde{p}(x)\in\mathcal{P}_{r}(x)\) that goes the quickest from \(s_{1}\) to \(s_{0}\) is of the form:

\[\tilde{p}_{t}(s_{0}|s_{1},a)=\sqrt{\frac{C_{p}\log(t)}{N_{t}(s_{1},a)}}\quad \text{and}\quad\tilde{p}_{t}(s_{0}|s_{1},b)=\sqrt{\frac{C_{p}\log(t)}{N_{t}(s _{1},b)}}.\]

Figure 4: Bernoulli bandit with dandling state (weakly-communicating model). Each arrow is a choice of action whose label is the mean reward of the associated state-action pair. The learner starts in state \(s_{0}\) and transitions to the absorbing state \(s_{1}\) as soon as an action is played.

Figure 3: The kernel of a \(n\)-state river-swim.

After running EVI for \(i\) steps, the current vector \(v_{i}\) (see (6)) is \(V_{i}^{n}(\mathcal{M}_{t})\), the maximal amount of reward that one can collect in \(i\) steps on \(\mathcal{M}_{t}\) seen as an extended Markov decision process, see Auer et al. (2009). If the data is well concentrated, the optimal reward from \(s_{1}\) are respectively \(\tilde{r}_{i}(s_{1},a)\approx 0.5+\sqrt{C_{r}\log(t)/N_{t}(s_{1},a)}\) and \(\tilde{r}_{i}(s_{1},b)=0.1+\sqrt{C_{r}\log(t)/N_{t}(s_{1},b)}\). From all this, one can argue that when \(i\) is large enough, the optimistic scores of \(\pi_{a}\) and \(\pi_{b}\) over \(i\) steps are roughly equal to:

\[V_{i}^{\pi_{a}}(s_{1};\mathcal{M}_{t}) \approx i+\left(-0.5+\sqrt{\frac{C_{r}\log(t)}{N_{t}(s_{1},a)}} \right)\sqrt{\frac{N_{t}(s_{1},a)}{C_{p}\log(t)}}=i-0.5\sqrt{\frac{N_{t}(s_{1},a)}{C_{p}\log(t)}}+\sqrt{\frac{C_{r}}{C_{p}}}\] \[V_{i}^{\pi_{b}}(s_{1};\mathcal{M}_{t}) \approx i+\left(-0.9+\sqrt{\frac{C_{r}\log(t)}{N_{t}(s_{1},b)}} \right)\sqrt{\frac{N_{t}(s_{1},b)}{C_{p}\log(t)}}=i-0.9\sqrt{\frac{N_{t}(s_{1},b)}{C_{p}\log(t)}}+\sqrt{\frac{C_{r}}{C_{p}}}.\]

So \(V_{i}^{\pi_{a}}(s_{1};\mathcal{M}_{t})\leq V_{i}^{\pi_{b}}(s_{1};\mathcal{M}_ {t})\) if \(N_{t}(s,b)\ll\frac{25}{81}N_{t}(s,a)\).

This means that EVI will output \(\pi_{b}\) if \(N_{t}(s,b)\ll\frac{25}{81}N_{t}(s,a)\), leading to \(N_{t}(s,a)\asymp N_{t}(s,b)\) so both growing linearly with \(t\). This informal argument can be generalized EVI-based algorithms with other types of confidence regions: EVI-based methods such as UCRL Auer et al. (2002), UCRL2B Fruit et al. (2020) and KLUCRL Filippi et al. (2010) will suffer from \(N_{t}(s,b)=\Theta(t)\) and their regret will grow linearly.

In opposition, PMEVI-based methods use truncation, making sure that \(|v_{i}(s_{1})-v_{i}(s_{0})|\leq T^{1/5}\) at all times. Intuitively, it makes PMEVI-based method "think" that \(\tilde{p}_{r}(s_{0}|s_{1},b)\) cannot be as small as \(\sqrt{C_{p}\log(t)/N_{t}(s_{1},b)}\), because the optimistic bias of \(\pi_{b}\) would be too large otherwise; Or, equivalently, that \(\tilde{p}_{r}(s_{0}|s_{1},b)=\sqrt{C_{p}\log(t)/N_{t}(s_{1},b)}\) but with \(\tilde{r}_{r}(s_{0},a)\ll 1\), hence killing the optimistic reward at \((s_{0},a)\) to meet the bias constraints.

Overall, these features of PMEVI-based methods are shared with algorithms such as REGAL Bartlett and Tewari (2009) and SCAL Fruit et al. (2018). The difference is that these methods require precise prior information on \(\operatorname{sp}(h^{*})\) that PMEVI-DT does not need.

## Appendix E Standard concentration inequalities

**Lemma 32** (Azuma's inequality, Azuma (1967)).: _Let \((U_{t})_{t\geq 0}\) a martingale difference sequence such that \(\operatorname{sp}\left(U_{t}\right)\leq c\) a.s., i.e., there exists \(a_{t}\in\mathbf{R}\) such that \(a_{t}\leq U_{t}\leq a_{t}+c\) a.s. Then, for all \(\delta>0\),_

\[\mathbf{P}\left(\sum\nolimits_{t=0}^{T-1}U_{t}\geq c\sqrt{\frac{1}{2}T \log\left(\frac{1}{\delta}\right)}\right)\leq\delta.\]

**Lemma 33** (Freedman's inequality, Zhang et al. (2020)).: _Let \((U_{t})_{t\geq 0}\) a martingale difference sequence such that \(|U_{t}|\leq c\) a.s., and denote its conditional variance \(V_{t}:=\mathbf{E}[U_{t}^{2}|\mathcal{F}_{t-1}]\). Then, for all \(\delta>0\),_

\[\mathbf{P}\left(\exists T^{\prime}\leq T:\sum\nolimits_{t=0}^{T^{\prime}-1}U_ {t}\geq\sqrt{2\sum\nolimits_{t=0}^{T^{\prime}-1}V_{t}\log\left(\frac{T}{ \delta}\right)}+4c\log\left(\frac{T}{\delta}\right)\right)\leq\delta.\]

**Lemma 34** (Time-uniform Azuma, Bourel et al. (2020)).: _Let \((U_{t})\) a martingale difference sequence such that, for all \(\lambda\in\mathbf{R}\), \(\mathbf{E}[\exp(\lambda U_{t})|U_{1},\ldots,U_{t-1}]\leq\exp(\frac{\lambda^{ \prime}\sigma^{2}}{2})\). Then:_

\[\forall\delta>0,\quad\mathbf{P}\left(\exists n\geq 1,\quad\left(\sum \nolimits_{k=1}^{n}U_{k}\right)^{2}\geq n\sigma^{2}\left(1+\frac{1}{n}\right) \log\left(\frac{\sqrt{1+n}}{\delta}\right)\right)\leq\delta.\]

**Lemma 35** (Time-uniform Weissman).: _Let \(q\) a distribution over \(\{1,\ldots,d\}\). Let \((U_{t})\) a sequence of i.i.d. random variables of distribution \(q\). Then:_

\[\forall\delta>0,\quad\mathbf{P}\left(\exists n\geq 1,\left\|\sum\nolimits_{i=1} ^{n}\left(e_{U_{i}}-q\right)\right\|_{1}^{2}\geq nd\log\left(\frac{2\sqrt{1+n }}{\delta}\right)\right)\leq\delta.\]

Proof.: Remark that \(\left\|\sum_{k=1}^{n}(e_{U_{k}}-q)\right\|_{1}=\max_{v\in[-1,1]^{d}}\sum_{k=1 }^{n}\left\langle e_{U_{k}}-q,v\right\rangle\). Let \(W_{k}^{v}:=\left\langle e_{U_{k}}-q,v\right\rangle\). Remark that for each \(v\in\{-1,1\}^{d}\), \((W_{k}^{v})\) is a family of i.i.d. random variables with \(-\left\langle q,v\right\rangle\leq W_{k}^{v}\leq 1-\left\langle q,v\right\rangle\), so \(\mathbf{E}[\exp(\lambda W_{k}^{v})]\leq\exp(\frac{\lambda^{\prime}}{8})\) by Hoeffding's Lemma. By Lemma 34, we have:

\[\mathbf{P}\left(\exists n\geq 1,\left\|\sum_{k=1}^{n}(e_{U_{k}}-q) \right\|_{1}\geq\sqrt{nd\log\left(\frac{2\sqrt{1+n}}{\delta}\right)}\right) =\mathbf{P}\left(\exists v\in\{-1,1\}^{d}\,,\exists n,\sum_{k=1} ^{n}W_{k}^{v}\geq\sqrt{nd\log\left(\frac{2\sqrt{1+n}}{\delta}\right)}\right)\] \[\leq\sum_{v\in\{-1,1\}^{d}}\mathbf{P}\left(\exists n,\sum_{k=1}^{ n}W_{k}^{v}\geq\sqrt{nd\log\left(\frac{2\sqrt{1+n}}{\delta}\right)}\right)\] \[\leq\sum_{v\in\{-1,1\}^{d}}\mathbf{P}\left(\exists n,\sum_{k=1}^{ n}W_{k}^{v}\geq\sqrt{\frac{1}{2}n\left(1+\frac{1}{n}\right)\log\left(\frac{ \sqrt{1+n}}{2^{d}-\delta}\right)}\right)\] \[\leq 2^{d}\cdot 2^{d}\delta=\delta.\]

This concludes the proof. 

**Lemma 36** (Time-uniform Empirical Bernstein).: _Let \((U_{k})_{k\geq 1}\) a martingale difference sequence such that \(\operatorname{sp}\left(U_{n}\right)\leq c\) a.s., let \(\hat{U}_{n}:=\frac{1}{n}\sum_{k=1}^{n}U_{k}\) the empirical mean and \(\hat{V}_{n}:=\frac{1}{n}\sum_{k=1}^{n}(U_{k}-\hat{U}_{n})^{2}\) the population variance. Then,_

\[\forall\delta>0,\forall T>0,\quad\mathbf{P}\left(\exists t\leq T,\sum \nolimits_{i=1}^{t}U_{i}\geq\sqrt{2t\hat{V}_{t}\log\left(\frac{3T}{\delta} \right)}+3c\log\left(\frac{3T}{\delta}\right)\right)\leq\delta.\]

Proof.: This is obtained with a union bound on the values of \(n\leq T\), then applying Lemma 38. 

**Lemma 37** (Time-uniform Empirical Likelihoods, Jonsson et al. (2020)).: _Let \(q\) a distribution on \(\{1,\ldots,d\}\). Let \((U_{t})\) a sequence of i.i.d. random variables of distribution \(q\). Then:_

\[\forall\delta>0,\quad\mathbf{P}\left(\exists n\geq 1,n\operatorname{KL}(\hat{q}_{n} \|q)>\log\left(\frac{1}{\delta}\right)+(d-1)\log\left(e\left(1+\frac{n}{d-1} \right)\right)\right)\leq\delta.\]

**Lemma 38** (Empirical Bernstein inequality, Audibert et al. (2009)).: _Let \((U_{k})_{k\geq 1}\) a martingale difference sequence such that \(\operatorname{sp}\left(U_{n}\right)\leq c\) a.s., let \(\hat{U}_{n}:=\frac{1}{n}\sum_{k=1}^{n}U_{k}\) the empirical mean and \(\hat{V}_{n}:=\frac{1}{n}\sum_{k=1}^{n}(U_{k}-\hat{U}_{n})^{2}\) the population variance. Then,_

\[\forall\delta>0,\forall n\geq 1,\quad\mathbf{P}\left(\sum\nolimits_{k=1}^{n}U_{k} \geq\sqrt{2n\hat{V}_{n}\log\left(\frac{3}{\delta}\right)}+3c\log\left(\frac{3} {\delta}\right)\right)\leq\delta.\]

**Lemma 39** (Bennett's inequality, Audibert et al. (2009)).: _Let \((U_{t})_{t\geq 0}\) a martingale difference sequence such that \(|U_{t}|\leq c\) a.s., and denote its conditional variance \(V_{t}:=\mathbb{E}[U_{t}^{2}|\mathcal{F}_{t-1}]\). Then,_

\[\forall\delta>0,\forall n\geq 1,\quad\mathbb{P}\left(\exists k\leq n,\sum_{i=1} ^{k}U_{i}\geq\sqrt{2\sum_{i=1}^{n}V_{i}\log\left(\frac{1}{\delta}\right)}+ \tfrac{1}{3}c\log\left(\tfrac{1}{\delta}\right)\right)\leq\delta.\]

**Lemma 40** (Lemma 3 of Zhang and Xie (2023)).: _Let \((U_{t})\) be a sequence of random variables such that \(0\leq U_{t}\leq c\) a.s., and let \(\mathcal{F}_{t}:=\sigma(U_{0},U_{1},\ldots,U_{t-1})\). Then:_

\[\forall\delta>0,\quad\mathbb{P}\left(\exists T\geq 0,\sum_{t=0} ^{T-1}U_{t}\geq 3\sum_{t=0}^{T-1}\mathbb{E}[U_{t}|\mathcal{F}_{t-1}]+c\log \left(\tfrac{1}{\delta}\right)\right)\leq\delta;\] \[\forall\delta>0,\quad\mathbb{P}\left(\exists T\geq 0,\sum_{t=0} ^{T-1}\mathbb{E}[U_{t}|\mathcal{F}_{t-1}]\geq 3\sum_{t=0}^{T-1}U_{t}+c\log \left(\tfrac{1}{\delta}\right)\right)\leq\delta.\]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We claim that the algorithm reaches minimax optimal regret, and it does. We claim that our algorithm can effectively use bias information, and the experimental illustration supports the claim. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [NA] Justification: The paper is a theoretical one that covers all weakly communicating MDPs, and although it may help the design of algorithms intended to be applicable in real-life scenarios, our method is not meant to be directly applied. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: To every result is attached a proof or a reference. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The algorithmic content in the main body is detailed enough to implement the algorithm, the exception being the confidence intervals and the implementation of EVI (because those come from already existing works). The confidence intervals are discussed in a dedicated section in the Appendix, while the works of Auer et al. (2009); Filippi et al. (2010); Fruit et al. (2020) etc. provide detailed ready-to-implement pseudo-code for EVI. Our code is mostly written in Python. Experiments took a few dozen minutes on a low end laptop. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).

4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The code is provided in the supplementary material, together with the scripts to reproduce the exact figures of the paper. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: People that are unfamiliar with Markov decision processes may not know the river-swim setting in which experiments are driven, but it is not required to understand the figures and the discussion. Moreover, a description of the environment (river-swim) is provided in the appendix in addition to be referenced. See also **Experimental Result Reproducibility** for more. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No]Justification: We did a hundred runs for every algorithm, which is enough to get statistical significance for the small sized problems that we experiment on. Also, experiments are mostly illustrative and do not account for an extensive numerical validation of the theoretical results. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No] Justification: This is mostly a theoretical paper. Most of the computational cost of the work is processing LaTeX and rendering a PDF. Like said earlier, experiments all together took less than a hour on a low end laptop. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: - Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.

* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: It's mainly a theoretical work on small sized Markov decision processes. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: - Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA]Justification: - Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: - Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: - Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?Answer: [NA]

Justification: -

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.