# Thinking Forward: Memory-Efficient Federated Finetuning of Language Models

 Kunjal Panchal

University of Massachusetts

Amherst, MA 01003-9264

kpanchal@umass.edu

&Nisarg Parikh

University of Massachusetts

Amherst, MA 01003-9264

nkparikh@umass.edu

&Sunav Choudhary

Adobe Research

Bangalore, India 560103

schoudha@adobe.com

Lijun Zhang

University of Massachusetts

Amherst, MA 01003-9264

lijunzhang@cs.umass.edu

&Yuriy Brun

University of Massachusetts

Amherst, MA 01003-9264

brun@cs.umass.edu

&Hui Guan

University of Massachusetts

Amherst, MA 01003-9264

huiguan@cs.umass.edu

###### Abstract

Finetuning large language models (LLMs) in federated learning (FL) settings has become increasingly important as it allows resource-constrained devices to finetune a model using private data. However, finetuning LLMs using backpropagation requires excessive memory (especially from intermediate activations) for resource-constrained devices. While Forward-mode Auto-Differentiation (AD) can significantly reduce memory footprint from activations, we observe that directly applying it to LLM finetuning results in slow convergence and poor accuracy. In this paper, we introduce Spry, an FL algorithm that splits trainable weights of an LLM among participating clients, such that each client computes gradients using Forward-mode AD that are closer estimations of the true gradients. Spry achieves a low memory footprint, high accuracy, and fast convergence. We formally prove that the global gradients in Spry are unbiased estimators of true global gradients for homogeneous data distributions across clients, while heterogeneity increases bias of the estimates. We also derive Spry's convergence rate, showing that the gradients decrease inversely proportional to the number of FL rounds, indicating the convergence up to the limits of heterogeneity. Empirically, Spry reduces the memory footprint during training by 1.4-7.1\(\times\) in contrast to backpropagation, while reaching comparable accuracy, across a wide range of language tasks, models, and FL settings. Spry reduces the convergence time by 1.2-20.3\(\times\) and achieves 5.2-13.5% higher accuracy against state-of-the-art zero-order methods. When finetuning LLama2-7B with LoRA, compared to the peak memory consumption of 33.9GB of backpropagation, Spry only consumes 6.2GB of peak memory. For OPT13B, the reduction is from 76.5GB to 10.8GB. Spry makes feasible previously impossible FL deployments on commodity mobile and edge devices. Our source code is available for replication at https://github.com/Astuary/Spy.

## 1 Introduction

In cross-device federated learning (FL), thousands of edge devices (called _clients_) collaborate through an orchestrator (called _server_) to jointly train a machine learning (ML) model [1, 2]. In each round of FL, the server sends an ML model to participating clients, who then update the model weights for several epochs on their individual data and send the new weights back to the server. The server aggregates the weights to update the model and initiates the next round of FL training. Due to theinherent privacy-preserving nature of FL, it has been adopted in many privacy-sensitive domains, such as healthcare [3], IoT [4; 5], and e-commerce [6].

In parallel, large language models (LLMs) have demonstrated impressive performance on natural language processing tasks [7; 8], creating a surge of interest in finetuning LLMs in FL settings [9; 10]. However, the problem is challenging in practice because of the memory requirements in finetuning LLMs. LLMs can have billions of weights, and finetuning them with backpropagation requires dozens of GBs of memory. These requirements easily overwhelm edge devices, particularly mobile phones with limited memory. The memory footprint of finetuning LLMs mainly comes from model weights and their gradients, optimizer states, and intermediate activations.

There are three categories of existing algorithms that reduce the memory footprint of finetuning LLMs: parameter-efficient finetuning (PEFT) [11; 12; 13; 14; 15; 16], quantization [17], and zero-order gradient estimation methods [18; 19; 20]. Although PEFT and quantization can reduce the memory consumption from parameters and optimizer states, the memory consumed by intermediate activations remains a significant bottleneck because these methods still use backpropagation to finetune LLMs. Backpropagation requires storing all intermediate activations during the forward pass to estimate gradients in the backward pass. For example, finetuning a 4-bit quantized Llama2-7B model [21] with LoRA techniques [12] requires \(\sim\)33.9GB of RAM, with 83.8% used for intermediate activations. Zero-order methods leverage finite difference [22] to estimate gradients and thus reduce the memory consumption from intermediate activations [19; 20; 23]. However, these methods suffer from slow convergence and poor model quality because the accumulation of truncation and round-off errors [24], a fundamental issue of finite difference, leads to noisy estimation of weight gradients.

_Forward-mode Auto-Differentiation (AD)_[24; 23] has the potential to address the memory consumption problems of backpropagation without introducing the round-off errors of finite difference. It estimates gradients by computing a Jacobian-vector product (jvp) based on random perturbations of weights during the forward pass, alleviating the need to store all intermediate activations, similar to zero-order methods. jvp represents how much changing the weights in the direction of a random perturbation affects the outputs. However, simply replacing backpropagation with Forward-mode AD in FL settings does not produce convergence speed and accuracy performance comparable to established federated optimizers based on backpropagation, such as FedAvg[1], FedYogi[25], FedSgD[1]. Forward gradients are _computationally inefficient_ and _inaccurate_ in estimating true gradients when the number of trainable weights is large. We empirically observed that, for LLMs finetuned with the LoRA technique, Forward-mode AD suffers from 8.3-57.2% accuracy loss and is 1.4-3.9\(\times\) slower to converge for models whose number of trainable weights exceed approximately 1.15M (see Appendix G).

In this paper, we propose Spry1, an FL algorithm for finetuning LLMs using Forward-mode AD while achieving low memory footprint, high accuracy, and fast convergence. Spry tackles the shortcomings of Forward-mode AD by splitting the trainable weights among participating clients per FL round. Spry improves computation efficiency as each client only needs to perturb a small fraction of trainable weights to derive their gradients, reducing the number of computations in each forward pass. Spry achieves higher accuracy and faster convergence because the smaller number of trainable weights for each participating client allows computing gradients that are closer estimations of the true gradients. In contrast to zero-order methods where one training iteration requires 20-100 forward passes, each with a different perturbation [20; 19] to estimate weight gradients well, Spry allows computing weight gradients from only one forward pass per iteration for each client. Unlike split learning [26], Spry does not need to transfer intermediate activations among clients. The union of the partial weights trained from each participating client in an FL round updates all the trainable weights of the language model. Since only a subset of weights is finetuned per client, Spry also saves client-to-server communication bandwidth.

Footnote 1: Named after its light memory consumption and speed.

We formally prove that the global gradients aggregated on the server side in Spry are unbiased estimators of the true global gradients in case of homogeneous data distributions across clients, while the heterogeneity increases the bias of the estimations. We also derive the convergence rate of Spry, showing that the norm of global gradients decreases linearly with the inverse of the number of FL rounds. We further discuss how configurations in Spry affect the convergence behavior of the algorithm and empirically validate the theoretical analysis.

We empirically evaluate Spry's memory efficiency, accuracy, computation efficiency, and communication efficiency through experiments on a wide range of language tasks, models, and FL settings. Spry achieves within 0.6-6.2% of the accuracy of the best-performing FL backpropagation, with 1.4-7.1\(\times\) less memory consumption for each client and comparable time to convergence. Spry also outperforms zero-order-based baselines with 5.2-13.5% higher accuracy, an average of 1.5-28.6\(\times\) faster per-round computation time, and 1.2-20.3\(\times\) faster convergence. We also compare Spry's communication efficiency to that of FedAvg (per-epoch communication) and FedSgd (per-iteration communication). For communication frequency of per-epoch, Spry reduces the number of model weights sent from a client to the server by \(M\) times, where \(M\) is the number of participating clients per round. For per-iteration communication frequency, each client of Spry only needs to send back a scalar to the server, fixing the client-to-server total communication cost to \(M\) scalar values.

We make the following contributions:

1. Spry, the first work that demonstrate the potential of Forward-mode AD for finetuning language models (with 18M to 13B parameters) in FL settings with low memory footprint, high accuracy, and fast convergence.
2. A federated optimization strategy that only requires a single forward pass per batch on each client to finetune a language model.
3. A theoretical analysis of how Spry's global gradients estimate true gradients based on the heterogeneity of FL clients, and a proof that Spry's convergence is linearly dependent on the number of FL rounds when a client's learning rate is inversely proportional to the size of perturbations and client data heterogeneity.
4. An empirical evaluation shows that Spry consumes 1.4-7.1\(\times\) less memory than its backpropagation-based counterparts, and converges 1.2-20.3\(\times\) faster with 5.2-13.5% higher accuracy compared to its zero-order counterparts.

## 2 Forward-mode Automatic Differentiation

This section presents the background on Forward-mode Auto-Differentiation (AD) necessary to follow the work. Related works on zero-order optimization methods, Forward-mode AD, and FL for LLMs are discussed in detail in Appendix A.

Forward-mode AD computes gradients by measuring how changes in model weights, in the direction of a random perturbation, affect the loss. Since these gradients are derived from a forward pass, they are referred to as _forward gradients_[23]. In contrast, backpropagation (also Reverse-mode AD) calculates a direction to adjust weights in, to decrease the loss. Formally, for each training iteration, given the trainable weights \(\bm{w}\), Forward-mode AD generates a random perturbation \(\bm{v}\) whose size is the same as \(\bm{w}\). In one forward pass, given training data \(\mathcal{D}\), Forward-mode AD computes the value of the objective function \(f(\bm{w};\mathcal{D})\) and the Jacobian-vector product (jvp) as follows:

\[\mathbf{J}_{f}\bm{v}=\nabla\bm{f}_{\bm{v}}(\bm{w};\mathcal{D})=\left[\frac{ \partial f(\bm{w};\mathcal{D})}{\partial w_{1}}\quad\ldots\quad\frac{\partial f (\bm{w};\mathcal{D})}{\partial w_{d}}\right]\left[v_{1}\quad\ldots\quad v_{d} \right]^{T}.\] (1)

This jvp is a scalar for neural networks since the output of the objective function \(f\) is a scalar. Multiplying jvp with the perturbation \(\bm{v}\) gives us the unbiased estimate of true gradients [23],

\[\nabla F(\bm{w}) =\mathbb{E}_{\bm{v}}\left[\mathbf{J}_{f}\bm{v}\cdot\bm{v}\right] =\mathbb{E}_{\bm{w},\mathcal{D}}\left[\left(\frac{\partial f(\bm{w}; \mathcal{D})}{\partial w_{1}}\quad\ldots\quad\frac{\partial f(\bm{w}; \mathcal{D})}{\partial w_{d}}\right]\bm{v}^{T}\right)\bm{v}\right]\] (2) \[=\mathbb{E}_{\mathcal{D}}\left[\frac{\partial f(\bm{w};\mathcal{ D})}{\partial w_{1}}\quad\ldots\quad\frac{\partial f(\bm{w};\mathcal{D})}{ \partial w_{d}}\right]=\mathbb{E}_{\mathcal{D}}\left[\nabla f(\bm{w};\mathcal{ D})\right].\] (3)

The partial derivative \(\partial f(\bm{w};\mathcal{D})/\partial\bm{w}\) is computed by chain rule on intermediate activations in the forward pass [24]. Unlike backpropagation where all the intermediate activations need to be stored during the forward pass, Forward-mode AD only stores the previous layer's activations in the forward pass for the chain rule derivatives. Hence, the memory overhead of deriving gradients would be the size of the largest activation in the forward pass.

## 3 Spry: Memory-Efficient Federated Finetuning of Language Models

While Forward-mode AD can decrease memory usage during model finetuning, merely substituting backpropagation with Forward-mode AD in FL scenarios often results in poor accuracy and computational inefficiency. To address this challenge, Spry recognizes that Forward-mode AD operates more efficiently and yields better gradient estimations when the trainable weight count is minimized. Therefore, Spry optimizes performance by distributing trainable weights among participating clients, assigning each client a responsibility to compute gradients for only a subset of trainable weights. Spry is compatible with PEFT methods such as IA3 [15], Adapter-based methods [27], BitFit [28], and LoRA [12], which mitigate the memory consumption from gradients and optimizer states. In this work, we focus on LoRA due to its demonstrated superiority, as highlighted in Appendix G.

Figure 1 gives an overview of Spry. It includes 5 main steps: **(1)** At the beginning of each FL round, the server assigns a few trainable layers to each of the participating clients of that round. **(2)** Each client is sent (i) the trainable weights of the layers assigned to it, (ii) frozen weights of the remaining layers if not previously received, and (iii) a scalar seed value. **(3)** On the client side, weight perturbations for the allocated layers are generated based on the received seed. These perturbations are utilized to update the assigned weights through computation of the forward gradients. **(4)** Clients only transmit the updated weights back to the server. **(5)** The server aggregates all the trained layer weights and updates the language model for the subsequent round.

Next, we discuss the two key steps of Spry in detail: Step (1), where the server assigns trainable weights to the participating clients and Step (3), where each client finetunes the assigned trainable weights using Forward-mode AD.

### Assigning Trainable Layers to Clients at the Server-side

To enable closer gradient estimations through forward gradients, the server reduces the number of trainable weights per client by selecting a layer and assigning it to a client in a cyclic manner. With LoRA, the server selects a LoRA layer, which consists of a pair of weights (\(w_{A}\) and \(w_{B}\) matrices) for each client. When # trainable layers \(>\) # participating clients, each client will be assigned more than one layer. Otherwise, each layer will be assigned to more than one client. The server aggregates the trained weights from each client and updates the model using adaptive optimizers such as FedYogi. Adaptive optimizers are shown to be less prone to noisy updates compared to FedAvg in the literature [25; 29]. The server keeps a mapping of layer names to client IDs, hence it can gather updated layer weights from all clients and update the model.

FL often faces the data heterogeneity issue, where the data distribution of one client differs from another, leading to poor model accuracy. While the primary aim of Spry does not directly tackle this issue, it seamlessly integrates with existing finetuning-based personalization techniques [30] to mitigate it. Spry distributes trainable classifier layers to all participating clients, enabling each client to finetune these layers to personalize the jointly trained model.

### Finetuning Weights with Forward Gradients on the Client-side

Clients update the assigned trainable weights with gradients estimated through Forward-mode AD. Specifically, each participating client will get a copy of the trainable weights assigned to it and a

Figure 1: Overview of Spry, a federated learning framework to finetune language models with low memory footprint. The term “PEFT” stands for parameter-efficient fine-tuning.

[MISSING_PAGE_FAIL:5]

_(Asmp I.2), and bound on gradient magnitude \(G\) (Asmp I.3); and the following conditions on the local learning rate \(\eta_{\ell}\),_

\[\eta_{\ell}=\min\left\{\mathcal{O}\left(\frac{\tau^{2}}{\sqrt{\beta _{2}\eta GL}}\right)^{\frac{1}{2}},\mathcal{O}\left(\frac{1}{\sqrt{\beta_{2}}G }\right),\mathcal{O}\left(\frac{\tau^{3}}{\sqrt{\beta_{2}(1-\beta_{2})G^{2}}} \right)^{\frac{1}{2}},\right.\] \[\left.\mathcal{O}\left(\frac{\widetilde{M}K}{\beta_{2}G(3d+K-1) \sum_{m\in[M]}\sum_{c\in[C]}\alpha_{m,c}^{2}}\right)\right\};\] (5)

_The global true gradients of Spry satisfies the following bound,_

\[\min_{0\leq r\leq R}\mathbb{E}_{r}||\nabla f(w^{(r)})||^{2}\leq \frac{f(w^{(0)})-\mathbb{E}_{R}[f(w^{(R)})]}{\eta R}\] \[\qquad+\left(2+\frac{\eta\eta_{\ell}L}{2\tau^{2}}+\frac{\sqrt{1- \beta_{2}}G\eta_{\ell}}{\tau^{3}}\right)\left(\frac{\sigma_{g}^{2}(1-s)(3d+K- 1)}{\widetilde{M}K}\right)\sum_{m\in\mathcal{M}}\sum_{c\in[C]}\alpha_{m,c}^{2},\] (6)

_where \(R\) is the total number of FL rounds, \(w\in\mathbb{R}^{d}\) are trainable weights, \(v\in\mathbb{R}^{d}\) are random perturbation, \(K\) is count of random perturbations per iteration, \(\eta\) is global learning rate, \(\tau\) is adaptability constant, and \(s\) is client sampling rate. Rest of the symbols are defined in Theorem 4.1._

**Discussion.** We focus on analyzing how different configurations in Spry affect its convergence. **(a)**_The number of FL rounds (\(R\)):_ The upper bounds of the norm of the global forward gradient in Eq. 6 decrease in proportion to the inverse of \(R\), indicating the convergence of the algorithm up to the limits of data heterogeneity. **(b)**_Dimension of perturbations (\(d\)):_\(\eta_{\ell}\propto\frac{1}{d}\) shows that as the number of weights to be perturbed increases, the learning rate must decrease. A lower learning rate can make convergence slower, or worse, as our empirical experiments in Appendix G will show, not converge at all. **(c)**_The number of perturbations per iteration (\(K\)):_ We observe \(K\) both in nominator and denominator, which indicates that increasing \(K\) brings little advantage in convergence speed. Results in Appendix G confirm the above statement. **(d)**_Data heterogeneity:_\(\eta_{\ell}\propto\frac{1}{\alpha_{m,c}^{2}}\) shows that more homogeneous data distributions across clients allow higher learning rate, and hence faster error reduction. This observation is corroborated by the comparison of convergence speeds between homogeneous and heterogeneous clients in Appendix H. **(e)**_The number of clients training same subset of weights: \(\eta_{\ell}\propto\widetilde{M}\)_ shows that more clients training the same subset of weights is beneficial for faster convergence, similar observation is shown in Appendix G.

The theorem also sheds light on the accuracy performance gap between Forward-mode AD and backpropagation in FL settings. The upper bounds on the global forward gradient norm includes a second term that increases with \(\alpha_{m,c}^{2}\) and variance \(\sigma_{g}^{2}\), but does not decrease with \(R\). This results in a gap between estimation errors of backpropagation-based methods like FedAvg and Spry.

## 5 Empirical Evaluation

We empirically evaluate Spry on 8 language tasks, 5 medium, and 3 large language models, under various FL settings. Our evaluation measures Spry's prediction performance, peak memory consumption, and time-to-convergence. We also ablate Spry's components to study the impact of communication frequency, the number of trainable parameters, the number of perturbations per iteration, the number of participating clients, and the importance of splitting layers on performance.

**Datasets, Tasks, and Models.** Our evaluation uses 8 datasets: **AG News**[31] (4-class classification), **SST2**[32] (2-class classification), **Yelp**[31] (2-class classification), **Yahoo**[31] (10-class classification), **SNLI**[33] (3-class classification), **MNLI**[34] (3-class classification), **SquADv2**[35] (Closed-book question answering), and **MultiRC**[36] (2-class classification). We chose these datasets because they allow us to generate heterogeneous splits in FL settings using Dirichlet distribution [37]. The default dataset split is across 1,000 clients, except the smallest datasets **SST2** and **MultiRC**, where there are 100 clients. **SquADv2** has 500 total clients. Each dataset has two versions: **(i)** Dirichlet \(\alpha=1.0\) (Homogeneous split), and **(ii)** Dirichlet \(\alpha=0.1\) (Heterogeneous split).

Our evaluation uses the following language models: **OPT13B**[38], **Llama2-7B**[21], **OPT6.7B**[38], **RoBERTa Large** (355M) [39], **BERT Large** (336M) [40], **BERT Base** (110M) [40], **DistilBERT Base** (67M) [41], and **Albert Large V2** (17.9M) [42]. For the billion-sized models, we use 4-bit quantization. For all the models, we use LoRA as the PEFT method. Appendix B describes the datasets and hyperparameters in more detail.

**Comparison Counterparts and Metrics.** We compare Spry to (a) Backpropagation-based federated optimizers FedAvg[1], FedYogi[25], FedSgd (Variant of FedAvg with per-iteration communication) [1], (b) Zero-order federated methods FedMeZO (federated version of MeZO [18]), Baffle[20], FwdLLM[19], all based on finite difference. MeZO uses prompt-based finetuning to improve the performance of finite differences. FwdLLM generates a random perturbation that has a high cosine similarity to the global gradients of the previous rounds. Baffle generates \(\sim\)100-500 perturbations per iteration. More details of these methods are in Appendix A. The original implementations of FwdLLM and Baffle had excessive memory usage in their implementations. We improve their codebase to be memory-efficient by perturbing only the trainable weights, similar to Spry. We refer to our implementation as FwdLLM+ and Baffle+.

Evaluation of Spry and its counterparts for classification tasks is on **generalized accuracy**\(Acc_{g}\) and **personalized accuracy**\(Acc_{p}\), which are metrics measured on server-side aggregated model and client-side locally updated model, respectively. Similarly, for question-answering tasks, we measure **Exact Matches** and **F1 Score**. We also measure **time to convergence** and **peak memory consumption** during training. Our convergence criterion is the absence of change in the variance of a performance metric, assessed at intervals of 50 rounds.

Spry is implemented in Flower [43] library. Quantization is done using AutoGPTQ [44]. For the zero-order methods, we used their respective client-side implementations with the server simulation structure of Flower. We utilized two Nvidia 1080ti to conduct all experiments of sub-billion sized models and billion-sized models for Spry and its zero-order methods. We used two RTX8000s and two A100s for Llama2-7B and OPT models on backpropagation-based methods respectively. Each experiment was run thrice with 0, 1, and 2 as seeds.

### Accuracy Performance Comparison

Table 1 reports the accuracy performance of Spry and its backpropagation- and zero-order-based counterparts on heterogeneous datasets for million-sized RoBERTa Large, and billion-sized Llama2-7B, OPT6.7B, and OPT13B. Similar results on personalized performance is shown in Appendix H, Figure 5. Results on MultiRC for FedMeZO are absent since the prompt-based finetuning variant of Llama2-7B was unavailable. Results on more model architectures and dataset combinations are available in Appendix G. Details on the learning curves, homogeneous dataset splits, and variance across 3 runs are in Appendix H.

Overall, Spry achieves 5.15-13.50% higher generalized accuracy and 4.87-12.79% higher personalized accuracy over the best-performing zero-order-based methods across all datasets. FwdLLM+, the best-performing zero-order counterpart, attempts to reduce the effect of numeric instability of finite differences by (a) Sampling \(K\) perturbations (default \(K=10\)) per batch for each client and picking 1 perturbation per batch that has the highest cosine similarity with the previous round's aggregated gradients and (b) Only picking trained weights from clients whose computed gradients have variance lower than a set threshold. However, we posit that this strategy leads to some clients getting excluded due to a low variance threshold or outlying clients getting included due to a high

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline  & \multicolumn{2}{c}{Backpropagation-based} & \multicolumn{2}{c}{Zero-order-based} & \multicolumn{2}{c}{First-order} & \multicolumn{2}{c}{Difference between} \\  & Methods \(\uparrow\) & \multicolumn{2}{c}{Methods \(\uparrow\)} & \multicolumn{2}{c}{Forward Mode AD \(\uparrow\)} & \multicolumn{2}{c}{performances of Spry and} \\ \cline{2-9}  & FedAvg & FedYogi & FwdLLM+ & FedMeZO & Baffle+ & Spry & best-performing best-performing backpropagation zero-order method \(\uparrow\) & method \(\uparrow\) \\ \hline AG News & 93.07\% & 92.77\% & 76.94\% & 70.56\% & 57.69\% & 87.89\% & \(-\)5.18\% & 10.95\% \\ SST2 & 88.00\% & 92.14\% & 84.41\% & 72.17\% & 61.57\% & 91.54\% & \(-\)0.60\% & 7.13\% \\ SNLI & 86.45\% & 79.31\% & 74.30\% & 69.57\% & 62.10\% & 82.66\% & \(-\)3.79\% & 8.36\% \\ MNLI & 84.29\% & 84.98\% & 72.66\% & 66.66\% & 62.85\% & 80.32\% & \(-\)4.66\% & 7.66\% \\ Yahoo & 67.37\% & 63.08\% & 56.06\% & 44.69\% & 37.81\% & 61.21\% & \(-\)6.16\% & 5.15\% \\ Yelp & 90.48\% & 79.10\% & 71.83\% & 65.10\% & 55.99\% & 85.33\% & \(-\)5.15\% & 13.50\% \\ MultiRC \(\diamond\) & 47.56\% & 72.53\% & 64.58\% & N/A & 85.12\% & 68.65\% & \(-\)3.88\% & 4.07\% \\ SQuADv2 \(\star\) & 19.06 & 19.91 & 13.46 & 13.09 & 11.09 & 16.75 & \(-\)3.16 & 3.29 \\ SQuADv2 \(\square\) & 11.88 & 11.30 & 7.85 & 8.07 & 6.92 & 8.84 & \(-\)3.04 & 0.77 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Generalized accuracy for Spry and its backpropagation- and zero-order-based counterparts on RoBERTa Large and LLMs. SQuADv2 uses F1 score. \(\uparrow\) shows that higher values are better. The datasets are split with Dir \(\alpha=0.1\). \(\diamond\) = Llama2-7B. \(\star\) = OPT6.7B. \(\square\) = OPT13B. Spry outperforms the best-performing zero-order-based methods by 5.15-13.50% and approaches the performance of backpropagation-based methods, with a difference of 0.60–6.16%.

variance threshold. Besides, picking new perturbations based on the previous round's aggregated gradients in the initial rounds can damage the learning trajectory. While Baffle+ samples more perturbation for each batch to make zero-order finite differences more tractable, the scale of language models demands perturbations on the scale of 500-1000 per batch, which becomes computationally infeasible. FedMeZO manages to outperform Baffle+ due to its prompt-based finetuning trick but still falls short due to only using 1 perturbation per batch for finite differences on each client. In contrast, Forward-mode AD used in Spry avoids the numerical instability from finite differences and improves accuracy by reducing trainable weights assigned to each client.

Compared to backpropagation-based methods, FedAvg and FedYogi, Spry manages to come as close as 0.60-6.16% of generalized accuracy and 2.50-14.12% of personalized accuracy. The performance gap between backpropagation and Forward-mode AD arises because in backpropagation, weight updates are more accurate as all gradients are computed directly using the error signal of the objective function. In contrast, Forward-mode AD relies on random perturbations, which is relatively less accurate for gradient estimation. Nonetheless, the advantages of Spry become evident when we see the peak memory consumption of Forward-more AD compared to backpropagation, which we will discuss next.

### Peak Memory Consumption Comparison

Figure 2 shows peak memory consumption of backpropagation (used in FedAvg, FedYogi), zero-order finite differences (used in FwdLLM+, Baffle+, FedMeZO), and first-order forward mode AD (used in Spry). The methods are profiled for a single client.

Compared to backpropagation, Forward-mode AD reduces peak memory usage of RoBERTa Large by 27.90%, Llama2-7B by 81.73%, OPT6.7B by 86.26% and OPT13B by 85.93%. The sizes of trainable parameter (colored ) and gradient + optimizer state (colored ) are consistent across the 3 modes of computing gradients for all 4 models. Hence the savings come from the reduced memory footprint related to activations (colored ) in Forward-mode AD. Compared to backpropagation-based methods, the memory cost related to activations is decreased by 12.12-49.25\(\times\) in Spry. Unlike storing all the intermediate activations in backpropagation, Forward-mode AD only has to store the previous layer's activation in a forward pass. The activation footprint of Forward-mode AD is equal to the size of the largest activation.

Against zero-order methods, Forward-mode AD activations cost 1.96\(\times\), 1.95\(\times\), 1.83\(\times\), and 1.54\(\times\) more for RoBERTa Large, Llama2-7B, OPT6.7B, and OPT13B respectively. The increasing cost comes from parallel evaluations of (a) the objective function on the original weights and (b) jvp computation on the perturbations in a single forward pass. However, as discussed in SS 5.1, the increased memory cost is offset by a boost of up to 13.50% in accuracy performance. And as SS 5.3 will discuss, Forward-mode AD reaches convergence faster than zero-order methods since it takes fewer steps to compute a closer gradient estimation.

### Time to Convergence Comparison

Figure 3 shows wallclock time-to-convergence for Spry and its counterparts. We observe that Spry is 1.15-1.59\(\times\), 6.16-20.28\(\times\), and 1.34-2.98\(\times\) faster than the zero-order methods FwdLLM+, Baffle+, and FedMeZO respectively. For a client in each round, Spry achieves a faster per-round computation time of 1.46\(\times\), 28.57\(\times\), and 1.80\(\times\) on average, against FwdLLM+, Baffle+, and FedMeZO. Forward-mode AD achieves faster convergence and faster per-round computation by providing a more accurate gradient estimation through a single perturbation per batch, leading to fewer steps needed to reach convergence. Since each client in Spry only trains partial weights,

Figure 2: Peak memory consumption of Spry’s Forward-mode AD versus backpropagation- and zero-order-based methods. RoBERTa Large, Llama2-7B, and OPT6.7B are profiled with a batch size of 8, and OPT13B with a batch size of 4. Spry reduces total memory usage by 27.90–86.26% compared to backpropagation-based methods. The 1.54–1.96\(\times\) additional memory Spry uses, compared to zero-order-based methods, is offset by the accuracy gains (§ 5.1).

it gains a speedup of 1.14\(\times\) over backpropagation-based FedAvg, FedYogi, and FedSGD for RoBERTa Large. However, compared to the backpropagation-based methods, Spry slows down for billion-sized LMs. We attribute this loss of speedup to the way jvp is computed in Forward-mode AD. jvp is computed column-wise, while its counterpart vjp in backpropagation are computed row-wise. The column-wise computation incurs time overhead.

### Ablation Studies

We summarize the ablation experiments on various components of Spry. Further discussions are in Appendix G.

**Spry can generalize to other language model architectures.** Similar to the observations from Table 1, we see a trend of Spry outperforming the best performing zero-order method FwdLLM+ by 3.15-10.25% for generalized accuracy, demonstrating that Spry can generalize to other language model architectures.

**Spry is compatible with other PEFT methods.** We integrate Spry with different PEFT methods like IA3, BitFit, and Classifier-Only Finetuning. Results shows that LoRA with Spry performs the best, with accuracy improvements of 10.60-16.53%.

**Effects of the number of trainable weights.** We change the number of trainable weights by controlling the rank \(r\) and scale \(\alpha\) hyperparameters in LoRA. Results show that Spry achieves the highest accuracy with (\(r\)=1, \(\alpha\)=1) setting, which has the smallest trainable weight count. The result is consistent with our theoretical analysis in SS4.

**Effects of communication frequency.** Per-iteration communication in Spry has been shown to boost accuracy by 4.47% compared to per-epoch communication. This improvement brings the accuracy within 0.92% and 0.96% of FedAvg and FedSGD, respectively.

**Effects of perturbation count per batch.** We observe that increasing the number of perturbations per batch (\(K\)) for Forward-mode AD has little to no impact on the end prediction performance of Spry, with \(K=100\) improving the generalized test accuracy by 1.1% over \(K=1\). The benefits of increasing \(K\) are however seen in the convergence speed. Setting \(K=10\) achieves a steady state (of accuracy \(\sim\)86%) around the 200th round, while the setting with \(K=1\) takes 500 rounds.

**Effects of participating client count.** Increasing the client count increases the prediction performance of Spry. For the SST2 dataset, with the total client count fixed to 100, the three settings \(C=10\), \(C=50\), and \(C=100\) produce accuracies of \(85.14\%\), \(86.56\%\), and \(88.08\%\), respectively. We also see an improvement in the convergence speed as the participating client count increases. To achieve an accuracy of \(\sim\)85%; \(C=10\), \(C=50\), \(C=100\) require 500, 450, and 150 rounds, respectively.

**Importance of splitting weights.** To understand the effects of splitting, we conduct the following two experiments: (a) With FedAvgSplit, we apply the strategy of splitting trainable layers across clients (see SS 3.1) to backpropagation-based FedAvg, and (b) With FedGD, we omit the splitting strategy of Spry for FL with Forward-mode AD. We observe that FedAvgSplit fails to achieve similar accuracy to FedAvg with a drop of 2.60-10.00%. FedFed fails to converge as the size of trainable weights increases, e.g., with RoBERTa Large with LoRA, that has 1.15M trainable weights. This proves the necessity of splitting trainable weights for Forward-mode AD in Spry.

### Communication and Computation Costs

Tables 2 and 3 in Appendix F shows the communication and computation overhead of Spry against all its baselines. We summarize the main observations here:

**Spry has lower communication cost due to the splitting strategy and scalar** jvp.** Suppose \(w_{g}\) is the total trainable parameter count of a model to be trained in federated setting. The

Figure 3: Time to convergence for Spry and its counterparts. Spry achieves faster convergence than zero-order methods due to more accurate gradient estimations in a single perturbation.

backpropagation-based baselines; FedAvg (per-epoch communication), FedSgd (per-iteration communication), and FedYogi (per-epoch communication) transmit the entire set of trainable parameters to each participating client, and receives the same from each participating client. That results in "client to server" communication cost of \(w_{g}\) and "server to client" communication cost of \(w_{g}\times M\). The per-epoch versions of the zero-order baselines (FedMeZO, Baffle, FwdLLM) follow a similar logic due to all the parameters needing to be transmitted to each client, with "client to server" communication costing \(w_{g}\), and "server to client" communication taking the cost of \(w_{g}\times M\). However, the per-iteration versions of the zero-order baselines fare better, with "client to server" communication only requiring each client sending a scalar finite difference (cost of 1), and "server to client" communication accruing \((w_{g}+1)\times M\) cost, due to the server also needing to send a scalar seed to each client now.

Meanwhile, Spry only needs to send \(\max\left(\frac{L}{M},1\right)\) layers (where \(L\) is the total layer count of a model, and \(M\) is the participating count of clients for a round), each layer of size \(w_{\ell}\). Hence, for per-epoch "client to server" communication accrues \(w_{\ell}\max\left(\frac{L}{M},1\right)\), which is smaller than \(w_{g}\). Similarly, per-epoch "server to client" communication costs \(w_{\ell}M\max\left(\frac{L}{M},1\right)\), which is also smaller than the cost of \(w_{g}M\) related to the baselines. For per-iteration communication, clients only need to send a scalar jvp (cost of 1) to the server; this matches the communication cost of per-iteration zero-order methods. Server needs to send a total of \(w_{\ell}M\max(L,M)\) (derivation given in Table 2), which is a smaller cost than the costs of backpropagation and zero-order methods.

Spry accrues lower computation cost due to lower trainable parameter count.Spry's client-side computation cost is traded off by a faster convergence to higher accuracy through better gradient approximations compared to finite difference-based methods. And Spry is the least computationally expensive on the server side due to needing to aggregate fewer parameters from the clients.

Let's assume that matrix multiplication costs \(c\) for each layer, resulting in a forward pass cost of \(c\). The cost of backpropagation is \(2c\) because the computation of the current layer's weight gradient is \(c\), and the cost of computing the previous layer's activation gradient is another \(c\). jvp computation in Spry takes additional cost of \(c\) for each layer. Moreover, since jvp calculation happens through column-by-column vector multiplications, the related overhead is quantified by \(v\).

Hence backpropagation-based methods FedAvg, FedSgd, and FedYogi computationally costs \(3Lc\) at client, and costs \(w_{g}(M-1)\) at server (due to additions). Note that \(L\) amounts to all layers in the model here. FedMeZO costs \(L(2c+3w_{\ell})\) at client through two forward passes, and generating perturbations three times. FwdLLM and Baffle costs \(KL(2c+w_{\ell})\) due to \(K\) perturbations for all \(L\) layers, with two forward passes and generation perturbations once. Against that, Spry costs \(2\max(\frac{L}{M},1)(c+v)+w_{\ell}L\) for a smaller count of \(L\), traded-off by the jvp computation cost of \(v\).

On the server side, Spry is the least computationally demanding. Spry needs to aggregate a subset of layer weights from only the clients that were assigned to those layers. Computation cost on the server-side changes based on the communication frequency per-iteration communication incurs an additional overhead of \(w_{\ell}L(\frac{M}{L}+1)\) and \(w_{\ell}L(M+1)\) (generation of perturbations at the server-side, and multiplying those perturbations with aggregate of the jvp values received from the clients) for Spry and its zero-order counterparts respectively.

## 6 Conclusion

Spry enables finetuning medium and large language models in cross-device FL. It introduces a training strategy where trainable weights are split across federated clients, so each client only applies Forward-mode AD to a fraction of the weights. This approach significantly reduces the memory footprint compared to backpropagation and achieves better gradient estimation, resulting in higher accuracy and faster convergence than zero-order methods. Experiments on various language tasks and models demonstrate Spry's effectiveness in reducing memory usage while maintaining accuracy comparable to backpropagation. We formally prove that the estimation bias of the global forward gradients in Spry depends on data heterogeneity across clients. We also analyzed how the convergence rate of Spry relates to the configurations of Spry and FL settings including properties of weight perturbations, data heterogeneity, and the number of clients and FL rounds.

## Acknowledgments and Disclosure of Funding

This material is based upon work supported by the National Science Foundation under grant no. CCF-2210243, DMS-2220211, CNS-2224054, CNS-2312396, and CNS-2338512, and by Adobe.

## References

* [1] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-Efficient Learning of Deep Networks from Decentralized Data. In _Proceedings of the 20th International Conference on Artificial Intelligence and Statistics_, Proceedings of Machine Learning Research. PMLR, 2017.
* [2] Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aurelien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista A. Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G. L. D'Oliveira, Hubert Eichner, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adria Gascon, Badih Ghazi, Phillip B. Gibbons, Marco Grutester, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub Konecny, Aleksandra Korolova, Farinaz Koushanfar, Sannmi Koyejo, Tancrede Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer Ozgur, Rasmus Pagh, Hang Qi, Daniel Ramage, Ramesh Raskar, Mariana Raykova, Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tramer, Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Advances and open problems in federated learning. _Foundations and Trends in Machine Learning_, 2021.
* [3] Tyler J Loftus, Matthew M Ruppert, Benjamin Shickel, Tezcan Ozrazgat-Baslanti, Jeremy A Balch, Philip A Efron, Jr. Gilbert R Upchurch, Parisa Rashidi, Christopher Tignanelli, Jiang Bian, and Azra Bihorac. Federated learning for preserving data privacy in collaborative healthcare research. _Digital Health_, 2022.
* [4] Li Li, Xi Yu, Xuliang Cai, Xin He, and Yanhong Liu. Contract theory based incentive mechanism for federated learning in health crowdsensing. _IEEE Internet of Things Journal_, 2022.
* [5] Suresh Dara, Ambedkar Kanapala, A. Ramesh Babu, Swetha Dhamercherala, Ankit Vidyarthi, and Ruchi Agarwal. Scalable federated-learning and internet-of-things enabled architecture for chest computer tomography image classification. _Computers and Electrical Engineering_, 2022.
* [6] Fabio Pinelli, Gabriele Tolomei, and Giovanni Trappolini. Flirt: Federated learning for information retrieval. In _Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval_, 2023.
* [7] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadet-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Cheang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arak Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Yhjei Elouandou, David Farhi, Liam Fedus, Niko Reilx, Simon Posada Fishman, Juston Forte, Isabella Fufford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Gou, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhu Jin, Deny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Lukasz Kaiser, Ali Kamali, Ingram Kantscheider, Nitish Shirish Keshkar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kircher, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Lukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makajun, Kim Maltzchi, Sam Manning, Todor Markov, Yaniv Markovi, Biana Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mely, Ashvin Nair, Reitichov Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O'Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascando, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Prito, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, GirishSastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Siemens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natale Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Ceron Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinhoa, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wieffhoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report. _arXiv 2303.08774_, 2024.
* [8] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Eric Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clement Cerepy, Shachi Dave, Mostafa Dehghani, Sunjing Dev, Jacob Devlin, Mark Diaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyan Feng, Vlad Fienber, Markus Fretiag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongta Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pollat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saea, Rajkumar Samuel, Reine Shely, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report. _arXiv 2305.10403_, 2023.
* [9] Bill Yuchen Lin, Chaoyang He, Zihang Ze, Hulin Wang, Yufen Hua, Christophe Dupuy, Rahul Gupta, Mahdi Soltanolkotabi, Xiang Ren, and Salman Avestimehr. FedNLP: Benchmarking federated learning methods for natural language processing tasks. In _Findings of the Association for Computational Linguistics: NAACL 2022_. Association for Computational Linguistics, 2022.
* [10] Yuanyishu Tian, Yao Wan, Lingjuan Lyu, Dezhong Yao, Hai Jin, and Lichao Sun. Fedbert: When federated learning meets pre-training. _ACM Transactions on Intelligent Systems and Technology (TIST)_, 2022.
* [11] Alexander Borzunov, Max Ryabinin, Artem Chumachenko, Dmitry Baranchuk, Tim Dettmers, Younes Belkada, Pavel Samygin, and Colin A Raffel. Distributed inference and fine-tuning of large language models over the internet. _Advances in Neural Information Processing Systems_, 2024.
* [12] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In _International Conference on Learning Representations_, 2022.
* China 2023_. Association for Computing Machinery, 2023.
* [14] Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian Weller, and Bernhard Scholkopf. Controlling text-to-image diffusion by orthogonal finetuning. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [15] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. In _Advances in Neural Information Processing Systems_, 2022.
* [16] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_. Association for Computational Linguistics, 2021.

* [17] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2018.
* [18] Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D. Lee, Danqi Chen, and Sanjeev Arora. Fine-tuning language models with just forward passes. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [19] Mengwei Xu, Dongqi Cai, Yaozong Wu, Xiang Li, and Shangguang Wang. Fwdllm: Efficient fedllm using forward gradient. _arXiv 2308.13894_, 2024.
* [20] Haozhe Feng, Tianyu Pang, Chao Du, Wei Chen, Shuicheng Yan, and Min Lin. Does federated learning really need backpropagation? _arXiv 2301.12195_, 2023.
* [21] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajijwal Bhagava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yunning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. _arXiv 2307.09288_, 2023.
* [22] C. H. Richardson. An introduction to the calculus of finite differences. by c.h. richardson pp. vi, 142. 28s. 1954. (van nostrand, new york; macmillan, london). _The Mathematical Gazette_, 39(330), 1955. doi: 10.2307/3608616.
* [23] Atilm Gunes Baydin, Barak A. Pearlmutter, Don Syme, Frank Wood, and Philip Torr. Gradients without backpropagation. _arXiv 2202.08587_, 2022.
* [24] Atilm Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Automatic differentiation in machine learning: a survey. _The Journal of Machine Learning Research_, 2017.
* [25] Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecny, Sanjiv Kumar, and Hugh Brendan McMahan. Adaptive federated optimization. In _International Conference on Learning Representations_, 2021.
* [26] Chandra Thapa, Pathum Chamikara Mahawaga Arachchige, Seyit Camtepe, and Lichao Sun. Splitfed: When federated learning meets split learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, 2022.
* [27] Renrui Zhang, Jiaming Han, Chris Liu, Aojun Zhou, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. LLaMA-adapter: Efficient fine-tuning of large language models with zero-initialized attention. In _The Twelfth International Conference on Learning Representations_, 2024.
* [28] Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)_. Association for Computational Linguistics, 2022.
* [29] Kunjal Panchal, Sunav Choudhary, Subrata Mitra, Koyel Mukherjee, Somdeb Sarkhel, Saayan Mitra, and Hui Guan. Flash: concept drift adaptation in federated learning. In _International Conference on Machine Learning_, pages 26931-26962. PMLR, 2023.
* [30] Shanshan Wu, Tian Li, Zachary Charles, Yu Xiao, Ziyu Liu, Zheng Xu, and Virginia Smith. Motley: Benchmarking heterogeneity and personalization in federated learning. _arXiv 2206.09262_, 2022.
* [31] Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In _Neural Information Processing Systems_, 2015. Available at https://huggingface.co/datasets/ag_news, https://huggingface.co/datasets/yelp_polarity, https://huggingface.co/datasets/yahoo_answers_topics, Accessed on 15 May, 2024.

* [32] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In _Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing_. Association for Computational Linguistics, 2013. Available at https://huggingface.co/datasets/stanfordnlp/sst2, Accessed on 15 May, 2024.
* [33] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. In _Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing_. Association for Computational Linguistics, 2015. Available at https://huggingface.co/datasets/stanfordnlp/snl, Accessed on 15 May, 2024.
* [34] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)_. Association for Computational Linguistics, 2018. Available at https://huggingface.co/datasets/SetFit/mnli, Accessed on 15 May, 2024.
* [35] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don't know: Unanswerable questions for SQuAD. In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)_. Association for Computational Linguistics, 2018. Available at https://huggingface.co/datasets/rajpurkar/squad_v2, Accessed on 15 May, 2024.
* [36] Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)_. Association for Computational Linguistics, 2018. Available at https://huggingface.co/datasets/mtc/multirc, Accessed on 15 May, 2024.
* [37] Kunjal Panchal, Sunav Choudhary, Nisarg Parikh, Lijun Zhang, and Hui Guan. Flow: Per-instance personalized federated learning. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [38] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models. _arXiv 2205.01068_, 2022.
* [39] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. _arxiv 1907.11692_, 2019.
* [40] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. _arXiv 1810.04805_, 2018.
* [41] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. _arXiv 1910.01108_, 2019.
* [42] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. ALBERT: A lite BERT for self-supervised learning of language representations. _arXiv 1909.11942_, 2019.
* [43] Daniel J Beutel, Taner Topal, Akhil Mathur, Xinchi Qiu, Titouan Parcollet, and Nicholas D Lane. Flower: A friendly federated learning research framework. _arXiv preprint 2007.14390_, 2020.
* [44] AutoGPTQ, 2024. URL https://github.com/AutoGPTQ/AutoGPTQ.
* [45] Richard L. Burden and J. Douglas. Faires. _Numerical analysis / Richard L. Burden, J. Douglas Faires. Thomson Brooks/Cole, 8th ed. edition_, 2005. ISBN 0534392008.
* [46] Karoly Jordan. _Calculus of finite differences_. American Mathematical Soc., 1965.
* [47] Wenzhi Fang, Ziyi Yu, Yuning Jiang, Yuanming Shi, Colin N. Jones, and Yong Zhou. Communication-efficient stochastic zeroth-order optimization for federated learning. _IEEE Transactions on Signal Processing_, 2022.
* [48] Rui Ye, Wenhao Wang, Jingyi Chai, Dihan Li, Zexi Li, Yinda Xu, Yaxin Du, Yanfeng Wang, and Siheng Chen. Openfedllm: Training large language models on decentralized private data via federated learning. _arXiv 2402.06954_, 2024.

* [49] Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, and Qiang Yang. Fate-Ilm: A industrial grade federated learning framework for large language models. _arXiv 2310.10049_, 2023.
* [50] Fan Lai, Yinwei Dai, Sanjay S. Singapuram, Jiachen Liu, Xiangfeng Zhu, Harsha V. Madhyastha, and Mosharaf Chowdhury. Fedscale: Benchmarking model and system performance of federated learning at scale. _arXiv 2105.11367_, 2022.
* [51] Jae Ro, Theresa Breiner, Lara McConnaughey, Mingqing Chen, Ananda Suresh, Shankar Kumar, and Rajiv Mathews. Scaling language model size in cross-device federated learning. In _Proceedings of the First Workshop on Federated Learning for Natural Language Processing (FL4NLP 2022)_. Association for Computational Linguistics, 2022.
* [52] Shubham Malaviya, Manish Shukla, and Sachin Lodha. Reducing communication overhead in federated learning for pre-trained language models using parameter-efficient finetuning. In _Conference on Lifelong Learning Agents_. PMLR, 2023.
* [53] Zhuo Zhang, Yuanhang Yang, Yong Dai, Qifan Wang, Yue Yu, Lizhen Qu, and Zenglin Xu. FedPETuning: When federated learning meets the parameter-efficient tuning methods of pre-trained language models. In _Findings of the Association for Computational Linguistics: ACL 2023_. Association for Computational Linguistics, 2023.
* [54] Seonghwan Park, Dahun Shin, Jinseok Chung, and Namhoon Lee. Fedfwd: Federated learning without backpropagation. _arXiv 2309.01150_, 2023.
* [55] Geoffrey Hinton. The forward-forward algorithm: Some preliminary investigations. _arXiv 2212.13345_, 2022.
* [56] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. Parameter-efficient fine-tuning of large-scale pre-trained language models. _Nature Machine Intelligence_, 2023.
* [57] Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, and Sai Qian Zhang. Parameter-efficient fine-tuning for large models: A comprehensive survey. _arXiv 2403.14608_, 2024.
* [58] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient finetuning of quantized LLMs. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.

Related Work

Spry uses first-order forward gradients for federated finetuning of language models. Hence, we review the literature on estimating gradients with low memory consumption and show how Spry represents a significant advancement in finetuning language models in FL.

Zero-order Gradients.Gradients derived from finite difference [45, 22, 46] methods are called _zero-order gradients_ since they don't involve Taylor expansion of the objective function \(f\). MeZO [18] has shown that finite difference with one perturbation per batch does not reach convergence on its own without additional tricks like prompt-based finetuning, which are highly specific to the tasks. For a more accurate gradient approximation, an average of zero-order gradients derived from multiple (\(\sim\)10 to 100) random perturbations on the same input batch is required [24], leading to slow convergence. Baffle[20] and FedZO [47] utilize zero-order gradients in federated settings. To train vision models (of parameter count \(\leq\) 13M), Baffle requires (a) \(\sim\)100-500 perturbations per batch for each client respectively, and (b) per-iteration communication among clients like FedSGD[1]. FedZO also requires \(\sim\)20 perturbations per batch for a vision model of size \(\leq\) 25M. Besides, numerical errors associated with finite difference make MeZO, Baffle, and FedZO suffer from sub-optimal predictions compared to backpropagation-based counterparts. Spry, using Forward-mode AD, computes gradients more accurately in a single forward pass compared to averaged gradients obtained through finite difference methods. This higher accuracy is achieved without needing modifications to model architectures or task structures, while also maintaining a memory footprint similar to that of finite difference methods.

First-order Forward Gradients.Gradients derived from Forward-mode Auto-Differentiation (AD) are considered first-order since it involves computing partial derivatives of the intermediate activations with respect to the input. We guide interested readers to the survey on different modes of automatic differentiation [24]. Fgd[23] shows preliminary results on the speedup and comparable accuracy achieved by Forward-mode AD against backpropagation. The challenge that makes Forward-mode AD less popular than backpropagation is that gradients derived from forward mode require more jvp column-by-column evaluations per input batch as the number of trainable weights increases. Moreover, evaluation of Fgd is limited to a multi-layer perceptron of size \(d\approx\)1.8M. Direct use of Fgd to finetune language models leads to slow or no convergence. Spry splits the trainable layers of a large language model across multiple clients in FL, letting each client finetune only a small subset of weights through forward gradients.

Training or Finetuning Language Models in Federated Learning.In recent years, several frameworks have been proposed to train or finetune LLMs in FL [48, 49, 50]. The backpropagation-based methods [10, 51], even with parameter efficient finetuning (PEFT) and quantization [52, 13, 53], have large memory footprints due to the overhead related to activations, gradients, and gradient history storage for adaptive optimizers [18].

FedFwd[54] applies FwdFwd[55] (which measures "goodness" of forward pass activations to judge which perturbations are useful) in FL, but FwdFwd struggles as model size scales up. FwdLLM uses zero-order gradients to finetune language models. It samples \(\sim\)10 perturbations per batch. For each batch, it picks 1 perturbation that has the highest cosine similarity with the previous round's gradients. Sampling new perturbations based on aggregated gradients from previous rounds during the initial stages can disrupt the learning trajectory. Spry requires 1 perturbation (without resampling) per batch to reach a higher prediction performance faster than FwdLLM.

DTABLE]

## Appendix B Datasets and Hyperparameters

Here we provide details of the datasets and their corresponding training hyperparameters used in this work.

Simulating Heterogeneity through Dirichlet Distribution.For each of the tasks, the class distribution each client gets depends on the Dirichlet distribution, where a parameter \(\alpha\) regulates the concentration of samples of a particular class for a specific client. Dir \(\alpha=1.0\) means all clients have homogeneous datasets where each class is equally likely to be on each client. With Dir \(\alpha\to 0\), the datasets of each client get more heterogeneous, where the sample distribution of each class is more likely to be concentrated on only a subset of clients.

Default Hyperparameters.Unless otherwise mentioned in dataset-specific paragraphs, the default hyperparameters for each method and for all datasets are stated here. For the backpropagation-based methods FedAvg, FedYogi, and FedSgd; we will fix the number of **epochs** to 1 since the goal of this work is to inch closer to backpropagation-like prediction performance while reducing the memory footprint. All the experiments have been run for 1500 **FL rounds**, except the experiments on OPT models, which are run for 600 FL rounds. Our observation from hyperparameter-tuning shows that the learning rate that gives the best performance is the same for all studied methods. Baffle and its memory-efficient improvement Baffle+ made by us, can perform better as the number of perturbations per batch increases, but due to the scale of experiments with 10-100 per round and up to 1500 rounds in the FL setting, we limit the total perturbations per batch of Baffle+ to 20 perturbations per batch and fixed finite difference step size \(\sigma=1\)e-4. FwdLLM+ samples 10 perturbations for each batch, finite difference step size of \(\sigma=1\)e-2. FedMeZO samples 1 perturbation for each batch, with finite difference step size of \(\sigma=1\)e-3. FedMeZO also requires 3-5 epochs for each client. Spry has 1 perturbation per batch for each client. For Spry and its zero-order counterparts (Baffle+, FwdLLM+, and FedMeZO), perturbations are **sampled for a normal distribution** with 0 mean and 1 variance. Default LoRA \(r\) and \(\alpha\) are 1 and 1, respectively. All methods use AdamW as client-side optimizer. Besides FedAvg, all methods use FedYogi as server-side optimizer.

AG News.AG News dataset [31] has been derived from a corpus of 496,835 categorized news articles. A subset of the corpus is used that has 120,000 total training samples and 7,600 total testing samples spread equally across 4 classes. The news articles are classified into 4 classes: World, Sports, Business, and Sci/Tech. We split this data across 1000 clients. Each client gets an equal number of samples for train and test datasets. This dataset is under Creative Commons CCZero(CC0) public domain dedication.

Learning rate for backpropagation-based (FedAvg, FedYogi, and FedSgd), zero-order-based (FwdLLM, Baffle, and FedMeZO), and first-order-based Spry is {1e-3, 5e-4, **1e-4**, 1e-5}. The batch size is set to 8. The max sequence length is 128. All methods use AdamW as a client-side optimizer, while Spry performs better with Sgd. Variance threshold of FwdLLM+ is 1e+1.

Sst2.Stanford Sentiment Treebank Binary[32] (or SST2) dataset is for a binary sentiment classification task. The dataset has 11,855 sentences derived from a set of movie reviews. The corpus was parsed using the Stanford Parser into 215,154 discrete phrases annotated by 3 human judges. This dataset contains 67,349 training samples, 872 validation samples, and 1821 testing samples. This sentiment classification dataset has the following sentiments as classes: Positive, and Negative. These samples are equally split between 100 clients, depending on the Dirichlet distribution. This dataset is under Creative Commons CCZero public domain dedication.

Learning rate for backpropagation-based (FedAvg, FedYogi, and FedSgd), zero-order-based (FwdLLM, Baffle, and FedMeZO), and first-order-based Spry is {1e-3, 5e-4, **1e-4**, 1e-5}. The batch size is set to \(8\). The max sequence length is 64. Variance threshold of FwdLLM+ is 5e+0.

Yelp.The Yelp reviews dataset[31] is a binary classification dataset gathered during the 2015 Yelp Dataset Challenge. The full dataset has 1,569,264 total samples, and it defines 2 classification tasks. We use the polarity classification task, which is a binary classification problem. It considers 1-2 stars negative polarity and 3-4 stars positive polarity. This dataset has 280,000 training samples and 19,000test samples in each polarity. We split this data into 1000 clients. This dataset is under the Apache License, Version 2.0.

Learning rate for backpropagation-based (FedAvg, FedYogi, and FedSgd), zero-order-based (FwdLLM, Baffle, and FedMeZO), and first-order-based Spry is {1e-3, 5e-4, 1e-4, **5e-5**, 1e-5}. The batch size is set to \(8\). The max sequence length is 128. Variance threshold of FwdLLM+ is 5e+1.

Yahoo.Yahoo! Answers Comprehensive Questions and Answers dataset [31] was gathered via the Yahoo! webscope program. The corpus itself contains 4,483,032 question/answer pairs, which were then formulated as a 10-class classification task. Each class in this dataset has 140,000 training and 5,000 testing samples.are The question/answer pairs are split into the following classes: 1. Society & Culture, 2. Science & Mathematics, 3. Health, 4. Education & Reference, 5. Computers & Internet, 6. Sports, 7. Business & Finance, 8. Entertainment & Music, 9. Family & Relationships, 10. Politics & Government. We split this dataset between 1000 clients, where each client gets an equal amount of data samples, where the data distribution is set by changing the Dirichlet distribution \(\alpha\) to range from most homogeneous (\(\alpha=1\)) to least homogeneous (\(\alpha=0\)). This dataset is under the Apache License, Version 2.0.

Learning rate for backpropagation-based (FedAvg, FedYogi, and FedSgd), zero-order-based (FwdLLM, Baffle, and FedMeZO), and first-order-based Spry is {1e-3, 5e-4, **1e-4**, 1e-5}. The batch size is set to \(8\). The max sequence length is 128. Variance threshold of FwdLLM+ is 5e+1.

SnliThe Stanford Natural Language Inference corpus [33] has 570,152 total sentence pairs. It is a natural language inference dataset, where the task is identifying if one sentence infers another. It has 550,152 training samples, 10,000 testing samples, and 10,000 evaluation samples. This dataset is split among 1,000 clients. The dataset has 3 classes: 1) The first sentence entails the second sentence, 2) The first sentence is neutral to the second sentence and 3) The first sentence contradicts the second sentence. This dataset is under CC BY-SA 4.0.

Learning rate for backpropagation-based (FedAvg, FedYogi, and FedSgd), zero-order-based (FwdLLM, Baffle, and FedMeZO), and first-order-based Spry is {1e-3, 5e-4, 1e-4, **5e-5**, 1e-5}. The batch size is set to \(8\). The max sequence length is 80. Variance threshold of FwdLLM+ is 1e+2.

Mnli.The Multi-Genre Natural Language Inference (MNLI) [34] corpus contains 432,702 sentence pairs which were crowd-sourced and then annotated with textual entailment information. This dataset is also a Natural Language Inference dataset as with SNLI. This dataset has 392,702 training, 20,000 evaluation, and 20,000 testing samples. These samples are split among 1,000 clients. This dataset draws from multiple sources, most of which are under the Open American National Corpus (OANC) license. The rest are under either the CC BY 3.0 Unported licenses or the CC BY-SA 3.0 licenses.

Learning rate for backpropagation-based (FedAvg, FedYogi, and FedSgd), zero-order-based (FwdLLM, Baffle, and FedMeZO), and first-order-based Spry is {1e-3, 5e-4, 1e-4, **5e-5**, 1e-5}. The batch size is set to \(8\). The max sequence length is 80. Variance threshold of FwdLLM+ is 1e+2.

SQuADv2.The Stanford Question Answering Dataset (SQuADv2) [35] consists of crowd-sourced questions about a set of Wikipedia articles. It is a reading comprehension dataset, where the answer to a question is a section (or a span) from the passage. It is also possible for the question to be unanswerable. The dataset contains 100,000 answerable and 50,000 unanswerable questions. This dataset was split into 500 clients. The heterogeneity is generated based on the topic labels (or "titles") associated with each question in the dataset. There are 35 titles available. This dataset is under the CC BY-SA 4.0 license.

Learning rate for backpropagation-based (FedAvg, FedYogi, and FedSgd), zero-order-based (FwdLLM, Baffle, and FedMeZO), and first-order-based Spry is {1e-3, 5e-4, 1e-4, **1e-5**}. The batch size is set to \(8\) for Forward-mode AD and zero-order differentiation-based methods. Backpropagation-based methods use a batch size of \(4\). The max sequence length is 400. Variance threshold of FwdLLM+ is 5e+1.

MultiRC.Multi-sentence Reading Comprehension (MultiRC) corpus is a dataset that contains short paragraphs with questions, whose answers can be found in the paragraph itself. We consider a task where we classify if the input question given as input is correct or false. It has 6k multi-sentence questions about 800+ paragraphs. Due to the scale of the dataset, we split it into 100 clients. This dataset is under the MIT license.

Learning rate for backpropagation-based (FedAvg, FedYogi, and FedSgd), zero-order-based (FwdLLM, Baffle, and FedMeZO), and first-order-based Spry is {1e-3, 5e-4, **1e-4**, 1e-5}. The batch size is set to \(8\). The max sequence length is 256. Variance threshold of FwdLLM+ is 1e+2.

## Appendix C Limitations and Future Work

Spry achieves a remarkable drop in memory consumption due to Forward-mode AD not having to store activations during the forward pass. However, the current implementation of Forward-mode AD by PyTorch is still suboptimal in terms of computation time. The high computation time is attributed to the column-by-column computation of the intermediate results of jvp. Improving the computation of jvp such that it takes significantly less time (almost half) than backpropagation remains an open and interesting problem. Moreover, there's room for improvement in reducing the memory consumption to that of zero-order methods. In zero-order methods, the weights are perturbed and a forward pass is computed on the perturbed weights. However, with the current implementation of Forward-mode AD, perturbations create a separate copy from the original weights, which accrues additional overhead. To further utilize the computation capacity of clients in FL, device-heterogeneity-aware strategies on splitting and mapping layers to clients can be explored for Spry, e.g., layer selection could happen on the client-side according to their data distributions.

## Appendix D Broader Impact

Through Spry, we aspire to bring impact in terms of data privacy and accessible finetuning of large language models (LLMs) on edge devices.

For small and medium organizations and individual users, the cost to finetune these language models can be prohibitively expensive as the size of the trainable weights increases. With Spry, we enable finetuning LLMs on resource-constrained edge devices with low memory consumption, making it feasible for a wider range of users. Moreover, the federated setting also provides benefits of data privacy. This can be ideal for use cases where LLMs can bring improved performance for a plethora of personalized downstream tasks, but the finetuning data containing sensitive and confidential information is never shared with a third party. With Spry, such data remains on the user's device, ensuring privacy while still allowing for effective finetuning of the LLM.

However, making LLM finetuning accessible to a broader range of organizations and individuals comes with its own challenges like a spread of biases or misinformation. Without preprocessing and filtering client data on their devices, the LLMs can be fed harmful and misleading information. Hence, it is necessary to develop guardrails on what kind of data should be filtered in, to finetune LLMs with crowd-sourced compute resources.

Spry Pseudocode

Algorithm 1 shows the workflow of Spry.

``` Input:\(R\): Total number of rounds, \(r\in[R]\): Round index, \(M\): Number of clients per round, \(m\in[M]\): Client index, \(\mathcal{M}\): Set of available clients, \(s\): Client sampling rate, \(\mathcal{D}_{m}\): Dataset of client \(m\), \(\eta_{\ell}\): Local learning rate, \(\bm{w}^{(r)}\): Model weights at \(r^{th}\) round, \(\overline{\bm{w}}_{m}^{(r)}\): Subset of assigned trainable weights for client \(m\) at \(r^{th}\) round, \(f\): Objective function. Output:\(\bm{w}^{(R+1)}\): Model at the end of the training
1Main Spry
2 Server loads pre-trained LM \(\bm{w}^{(1)}\) with PEFT for\(r\in[R]\)rounddo
3 Sample \(M\) clients from \(\mathcal{M}\) at rate of \(s\)
4\(\mathcal{L}\leftarrow\) list of trainable PEFT param names client_layer_mapping\(\leftarrow\)MapLayersToClients(\(\mathcal{L},M\)) for\(m\in[M]\)inparalleldo
5\(\overline{\bm{w}}_{m}^{(r)}\leftarrow\)ClientTrain(\(\bm{w}^{(r)}\), client_layer_mapping[\(m\)])
6 end for
7 Build \(\bm{w}^{\prime(r)}\) with \(\overline{\bm{w}}_{m}^{(r)}\) using client_layer_mapping[\(m\)], \(\forall m\in[M]\) Use adaptive optimizer like FedYogi to build \(\bm{w}^{(r+1)}\) based on the aggregated \(\bm{w}^{\prime(r)}\)
8 end for
9
10 end for
11FunctionMapLayersToClients(\(\mathcal{L},M\)) client_idx\(\leftarrow\) 0; mapping\(\leftarrow\) {} forname\(\in\)\(\mathcal{L}\)do
12 rollover_idx\(\leftarrow\) client_idx % \(M\) mapping[rollover_idx].update(name) client_idx \(\leftarrow\) client_idx + 1
13 end for
14return mapping
15 end for ```

**Algorithm 1**Spry

The function MapLayersToClients on Line 14 in Algorithm 1 shows how we have assigned only a few trainable layers to each client in FL, to make Forward-mode AD more effective at generating better estimation of the gradients. In function ClientTrain on Line 23 of Algorithm 1; each client \(m\) gets a copy of the entire language model \(\bm{w}_{m}^{(r)}\) and a list trainable_layers of parameter names the client \(m\) has to train. A client \(m\) freezes parameters that are not included in its trainable_layers. And for each of the parameter \(\overline{w}_{m}^{(r)}\) which need to be trained, Spry generates a corresponding random perturbation \(v\) using a normal distribution \(\mathcal{N}(0,\mathbf{I}_{\overline{w}_{m}^{(r)},\text{shape}})\).

Once a client \(m\) has obtained forward gradients of all the trainable parameters \(\overline{\bm{w}}_{m}^{(r)}\), those parameters are locally updated with optimizers like SGD or Adam. The updated trainable parameters \(\overline{\bm{w}}_{m}^{(r)}\) are sent back to the server. The server has a mapping of parameter names to client IDs, and hence it builds \(\bm{w}^{\prime(r)}\) by using \(\overline{\bm{w}}_{m}^{(r)}\)\(\forall m\in[M]\). If there are multiple clients mapped to the same parameter, then we take a weighted average (similar to FedAvg) of all the parameters to build \(\bm{w}^{\prime(r)}\). Spry uses adaptive optimizers like FedYogi at server-side on effective gradients \(\Delta=\bm{w}^{\prime(r)}-\bm{w}^{(r)}\) to reduce the noise of forward gradients.

Communication and Computation Costs

### Communication Costs

Table 2 illustrates communication costs of Spry and its backpropagation- and finite difference- based baselines. A discussion on communication modes of Spry is also given in Section 3.2, "Per-Epoch Communication" and "Per-Iteration Communication".

Here we discuss the costs related to those communication modes:

Per-epoch Communication.Spry's client-to-server communication cost does not scale linearly with clients like in its backpropagation and finite-difference counterparts, but instead decreases or stays constant for \(L\) as more clients are present. Server-to-client communication cost is lower in Spry due to only sending one layer per client when \(M>L\) or \(\frac{L}{M}\) layers per client otherwise. This result follows from the below observation:

Backpropagation-based and finite-difference-based methods have a communication cost of \(w_{g}\), where \(w_{g}\) represents the global model size. Each client in \([M]\) (set of participating clients) receives all trainable parameters from the server, requiring the server to send a total of \(w_{g}\times M\) parameters each round.

Spry's communication cost per epoch is \(w_{\ell}\max(\frac{L}{M},1)\), where \(L\) is the layer count and \(w_{\ell}\) is the count of parameters for each layer. Each client sends a subset of trainable parameters, incurring a communication cost of \(\frac{w_{\ell}L}{M}\) parameters for \(L>M\), and \(w_{\ell}\) for \(L\leq M\). When \(L\leq M\), each client gets 1 layer, hence the communication for each client is \(w_{\ell}\).

Per-iteration Communication.Spry accrues lower communication cost than the finite difference and backpropagation counterparts due to the layer splitting strategy, and the server's ability to compute gradients based on the jvp value. This is because:

The communication cost from client to server for forward-mode AD and finite differences is 1. This is due to an FL round that involves (1) server selecting a random seed, (2) server sending it with trainable parameters to clients, (3) clients generating perturbations based on the seed, (4) deriving and

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & & Communication cost & Communication cost & Communication cost \\ Gradient computation & Method & (in parameter count) & (in parameter count) \\  & (Comm. frequency) & **from each client** & **from server to all** \\  & & **to server for each** & **clients** for each \\  & & communication round & communication round \\ \hline \multirow{3}{*}{Backpropagation (First-order gradients)} & FedAvg / FedYogi & \multirow{3}{*}{\(w_{g}\)} & \multirow{3}{*}{\(w_{g}\times M\)} \\  & (Per-epoch) & & \\ \cline{2-3}  & FedSgd & & \\ \cline{2-3}  & (Per-iteration) & & \\ \hline \multirow{3}{*}{Finite differences (Zero-order gradients)} & FedMeZO / FwdLLM / & \multirow{3}{*}{\(w_{g}\)} & \multirow{3}{*}{\(w_{g}\times M\)} \\  & Bafle (Per-epoch) & & \\ \cline{2-3}  & FedMeZO / FwdLLM & 1 (of finite & \((w_{g}+1)\times M\) \\  & Bafle (Per-iteration) & difference scalar) & (“1” is for \\  & & & perturbation seed) \\ \hline \multirow{3}{*}{Forward-mode AD (First-order gradients)} & Spry & \multirow{3}{*}{\(w_{\ell}\times\max(L/M,1)\) (Assuming \(L\%M=0\) for each of exposition)} & \multirow{3}{*}{\(w_{\ell}\times\max(L/M,1)\times M\)} \\  & (Per-epoch) & & \\ \cline{1-1}  & Spry & \multirow{3}{*}{1 (of jvp scalar)} & \multirow{3}{*}{\(w_{\ell}\times\max(L/M,1)\times M\)} \\  & (Per-iteration) & & \\ \cline{1-1}  & & & \(=w_{\ell}\times\max(L,M)+M\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Communication cost of Spry and all its baselines. \(M\) is the count of participation clients. Total count of trainable parameters of a global model is \(w_{g}=w_{\ell}L\) (for simplicity, we assume that each layer has \(w_{\ell}\) parameters).

sending back a scalar or finite difference scalar to the server, and then (5) server computing gradients by multiplying the derived perturbations with the seed.

The server to client communication is \((w_{g}+1)\times M\), where the "+1" is due the randomness seed.

### Computation Costs

Table 3 shows the computation costs of Spry and its baselines, where the client-side cost is for each iteration, and the server-side cost is for each round.

Briefly, Spry's client-side computation cost is traded off by a faster convergence to higher accuracy through better gradient approximations compared to finite difference-based methods. Furthermore, Spry is the least computationally expensive on the server side due to needing to aggregate fewer parameters from the clients.

Table 3 assumes that matrix multiplication costs \(c\) for each layer, resulting in a forward pass cost of \(c\). The cost of backpropagation is \(2c\) because the computation of the current layer's weight gradient is \(c\), and the cost of computing the previous layer's activation gradient is another \(c\). \(\mathsf{jvp}\) computation in Spry takes additional cost of \(c\) for each layer. Moreover, since \(\mathsf{jvp}\) calculation happens through column-by-column vector multiplications (Sec 3.1 of [24]), the related overhead is quantified by \(v\).

Client-side per-iteration computation cost.Backpropagation needs 3 matrix multiplication operations per layer. For zero-order methods, there are 2 matrix multiplications (incurred due to two forward passes) per layer, and per perturbation within a training iteration; and additional overhead \(w_{\ell}KL\) for perturbation generation. MeZO requires generation of perturbations thrice for the same seed (Algorithm 1 in MeZO [18]).

\begin{table}
\begin{tabular}{c c c c} \hline \hline \multirow{2}{*}{Gradient computation} & Method & Computation cost of & Computation cost of \\  & (Comm. frequency) & **each client for** & **the server for** \\  & & **each iteration** & **each round** \\ \hline \multirow{3}{*}{Backpropagation (First-order gradients)} & FedAvg / FedYogi & \multirow{3}{*}{\(3Lc\)} & (Aggregating \(L\) layer \\  & (Per-epoch) & & weights from \(M\) clients) \\  & & & \((M-1)\times w_{\ell}L\) \\ \cline{2-4}  & FedSgd & \multirow{3}{*}{\(3Lc\)} & \multirow{3}{*}{\((M-1)\times w_{\ell}L\)} \\  & (Per-iteration) & & \\ \hline \multirow{3}{*}{Finite differences (Zero-order gradients)} & FedMeZO (Per-epoch) & \multirow{3}{*}{\(L(2c+3w_{\ell})\)} & \multirow{3}{*}{\((M-1)\times w_{\ell}L\)} \\  & & & \\ \cline{2-4}  & FwdLLM / Baffle (Per-epoch) & & \\ \cline{2-4}  & & & \((M-1)\times w_{\ell}L\) \\ \cline{2-4}  & FedMeZO (Per-iteration) & & \\ \cline{2-4}  & FwdLLM / Baffle (Per-epoch) & & \\ \hline \multirow{3}{*}{Forward-mode AD (First-order gradients)} & Spry (Per-epoch) & \multirow{3}{*}{\(2\times\max(\frac{L}{M},1)\)\(\times(c+v)+w_{\ell}L\)} & \multirow{3}{*}{\(\sum_{\mathcal{M}\subset[M]}\left((|\mathcal{M}|-1)w_{\ell}\max\left( \frac{L}{M},1\right)\right)\)} \\  & & & (usually, \(|\mathcal{M}|=\max\left(\frac{M}{L},1\right)\)) \\ \cline{1-1} \cline{2-4}  & Spry (Per-iteration) & \multirow{3}{*}{\(2\times\max(\frac{L}{M},1)\)\(\times(c+v)+w_{\ell}L\)} & \multirow{3}{*}{\(\sum_{\mathcal{M}\subset[M]}\left(2|\mathcal{M}|w_{\ell}\max\left( \frac{L}{M},1\right)\right)\)} \\ \cline{1-1} \cline{2-4}  & & & \\ \hline \hline \end{tabular}
\end{table}
Table 3: Computation cost of Spry and all its baselines. The client-side cost is for each iteration, and the server-side cost is for each round. \(L\) is the layer count, \(M\) is the participating client count, \(c\) is the cost of matrix multiplication for each layer. \(v\) is the overhead related to column-by-column vector multiplications of \(\mathsf{jvp}\). \(w_{\ell}\) is the size of each layer and hence size of each layer’s perturbation too. \(K\) is the perturbation count per iteration. \(K=1\) for Spry and FedMeZO, and \(\sim 20\) for Baffle and FwdLLM.

Spry's computation cost is \(2\times\max(\frac{L}{M},1)\times(c+v)+w_{\ell}L\). Since Spry allocates at most \(\frac{L}{M}\) layers to each client, the computation cost only scales with \(\max(\frac{L}{M},1)\), against its counterparts scaling with \(L\). However, forward-mode AD computes jvp column-wise, while its counterpart vjp in backpropagation is computed row-wise. This results in time overhead ( ) if the number of trainable parameters exceeds the output size (1 as loss is scalar), which is the case for neural networks. Therefore, Spry's per-iteration computation cost is higher compared to other approaches.

Note that the per-iteration computation cost of Spry is not the whole picture. It takes fewer communication rounds to reach higher accuracy due to better gradient approximation of forward-mode AD than finite difference methods. This is why "Time to Convergence" (Section 5.3) discusses a fair comparison of Spry's runtime and prediction performance.

Server-side per-round computation cost.On the server side, Spry is the least computationally demanding. Spry needs to aggregate a subset of layer weights from only the clients that were assigned to those layers, while its counterparts need to aggregate all layers from all clients.

Computation cost on the server-side changes based on the communication frequency per-iteration communication incurs an additional overhead of \(w_{\ell}L(\frac{M}{L}+1)\) and \(w_{\ell}L(M+1)\) (generation of perturbations at the server-side, and multiplying those perturbations with aggregate of the jvp values received from the clients) for Spry and its zero-order counterparts respectively.

## Appendix G Ablation Studies

Here we will dive into various components of Spry and see their impact on the performance of Spry.

Spy can Generalize to Different Language Model Architectures.Table 4 shows generalized and personalized accuracies \(Acc_{g}\) and \(Acc_{p}\) for homogeneous data splits for language model architectures other than RoBERTa Large.

For zero-order gradients, we show the results of the best-performing method, FwdLLM+. We see a similar trend of Spry outperforming FwdLLM+ by 3.15-10.25% for generalized accuracy, and by 2.75-12.37% for personalized accuracy, exhibiting how Spry is independent of model architectures. Spry also comes as close as 4.44-9.71% to the best-performing backpropagation-based method.

Spy supports Other PEFT Methods.Figure 3(a) shows the generalized accuracy of Spry using three different PEFT methods: LoRA, IA3, and Bitfit. We also experiment with finetuning only classifier layers, calling it Classifier-Only. LoRA (with 0.3241% of the total parameters of RoBERTa Large) outperforms IA3 with 0.3449% of the total parameters by 10.60%, while BitFit fails to converge for all datasets. Only training classifier layers is worse than finetuning LoRA weights by 16.53%. The observation on comparison of LoRA with IA3 is consistent with the work benchmarking various PEFT methods [56]. BitFit has been observed to fail on LLMs [57]. Furthermore, unlike IA3, LoRA has been shown to be successful at finetuning quantized billion-sized models in QLoRA [58], making it a strong candidate for our work.

Effects of Communication Frequency.One way to reduce the noise introduced by the random perturbations in gradient computation is, to communicate the gradients back to the server every iteration instead of every few epochs. Figure 3(b) shows results of per-epoch and per-iteration communication variants of Spry and FedAvg.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline  & \multicolumn{3}{c}{Backpropagation-based} & \multicolumn{2}{c}{Zero-order} & \multicolumn{2}{c}{First-order} \\  & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{based Method} & \multicolumn{2}{c}{Forward-mode AD} \\ \cline{2-7}  & FedAvg & FedYogi & FwdLLM+ & \multicolumn{2}{c}{Spry} \\  & \(Acc_{g}\) & \(Acc_{p}\) & \(Acc_{g}\) & \(Acc_{p}\) & \(Acc_{g}\) & \(Acc_{p}\) & \(Acc_{g}\) & \(Acc_{p}\) \\ \hline AG News on BERT Base & 93.00\% & 93.34\% & 93.31\% & 93.88\% & 83.41\% & 83.42\% & 86.74\% & 92.42\% \\ SST2 on DistilBERT Base & 91.47\% & 95.28\% & 87.95\% & 92.97\% & 79.09\% & 80.94\% & 84.90\% & 87.12\% \\ SNLI on BERT Large & 85.79\% & 89.36\% & 86.72\% & 90.33\% & 66.76\% & 64.84\% & 77.01\% & 77.21\% \\ Yahoo on DistilBERT Base & 69.13\% & 74.75\% & 63.84\% & 71.25\% & 54.29\% & 55.87\% & 61.17\% & 61.47\% \\ Yelp on AlbertV2 Large & 90.17\% & 92.78\% & 90.24\% & 94.00\% & 82.65\% & 83.25\% & 85.80\% & 86.00\% \\ \hline \hline \end{tabular}
\end{table}
Table 4: Generalized (\(Acc_{g}\)) and personalized (\(Acc_{p}\)) accuracies (the higher, the better) for Spry and its backpropagation and zero-order based counterparts on various language model architectures. The datasets are split with Dir \(\alpha=0.1\).

Figure 4: Ablation studies on PEFT methods, communication frequency, and LoRA hyperparameters.

By communicating each iteration, we see a boost of 4.47% in the accuracy of Spry, only 0.92% and 0.96% away from the accuracy of FedAvg and FedSgd respectively. Furthermore, as shown in Figure 3(b), the convergence speed of Spry also improves by communicating each iteration.

As discussed in Section 3, to reduce the trade-off between performance gains and communication cost per iteration, each client can send only the jvp scalar to the server instead of transmitting all the assigned trainable weights [20]. With the seed value of the randomness, the server can then generate the random perturbation vector \(\bm{v}\), which was used by all the clients to generate their respective jvp values. Then the random perturbation is multiplied with the received jvp values of all clients, to compute gradients.

Effects of the Number of Trainable Weights.Figure 3(c) displays the effect of changing trainable LoRA parameter count on the prediction performance of Spry. For DistilBERT Base (total parameter count of 66M), LoRA reduces the trainable parameter count to 0.61M (0.91%), 0.74M (1.11%), 0.89M (1.33%), and 1.18M (1.77%) with LoRA hyperparameter settings of (\(r\)=1, \(\alpha\)=1), (\(r\)=8, \(\alpha\)=16), (\(r\)=16, \(\alpha\)=16), and (\(r\)=32, \(\alpha\)=32).

Spry achieves the highest accuracy of 84.90% with (\(r\)=1, \(\alpha\)=1) setting, which has the smallest trainable parameter count. The accuracy increases as the layer size decreases since fewer perturbed weights provide less noisy gradients.

Effects of the Number of Perturbations per Batch.The effect of increasing the number of perturbations per batch and hence the number of jvp evaluations for a batch is shown in Figure 4(a) for the SST2 dataset. Here, gradients generated from each random perturbation and their corresponding jvp values are averaged to update the model. We observe that increasing \(K\) (perturbations per batch) for Forward-mode AD has little to no impact on the end prediction performance of Spry, with \(K=100\) improving the generalized test accuracy by 1.1% over \(K=1\). However, the benefits of increasing the perturbation count per batch are seen in the convergence speed. Setting \(K=10\) achieves a steady state (of accuracy \(\sim\)86%) around 200th round, while the setting with \(K=1\) takes 500 rounds. The improvements in convergence speed are saturated for \(K>10\). This shows that more perturbations reduce the gradient estimation noise only to an extent.

Effects of the Number of Participating Clients per Round.Figure 4(b) shows how changing the per-round number of participating clients \(C\) influences Spry on the SST2 dataset. Increasing client count increases the prediction performance of Spry. With the total client count fixed to 100, the three settings \(C=10\), \(C=50\), and \(C=100\) produce accuracies of \(85.14\%\), \(86.56\%\), and \(88.08\%\), respectively. Similar to the findings of Section G, we also see an improvement in the convergence speed as the participating client count increases. To achieve an accuracy of \(\sim\)85%; \(C=10\), \(C=50\), \(C=100\) require 500, 450, and 150 rounds respectively. The performance gains and faster convergence are due to more clients training the weights of the same layers.

The Importance of Splitting Layers.To understand the effects of splitting, we compare the results of the following two experiments: (a) With FedAvgSplit, we apply the strategy of splitting trainable layers across clients (Section 3.1) to backpropagation-based FedAvg, and (b) With FedFgd, we omit the splitting strategy of Spry.

Figure 5: Ablation studies on perturbation counts, participating client counts, and layer splitting strategy.

Figure 5c shows the performance of FedAvg and FedAvgSplit against FedFGd and Spry for two LMs: RoBERTa Large (355M) and BERT Base (110M). We observe that FedAvgSplit fails to achieve similar accuracy for both models with a drop of 2.60% and 10.00%. This is because in FedAvgSplit, fewer clients are training each subset of weights. Moreover, we see a similar accuracy with an absolute difference of 2.70-3.61% between FedAvgSplit and Spry, since the trainable weight count per client is low. FedYogiSplit follows the same observation of not achieving similar accuracy to FedYogi if the trainable weights are split across clients. On the contrary, FedFed converges for the smaller model BERT base, albeit 150 rounds slower than Spry, and with 2.87% accuracy drop. But as the size of trainable weights increases, e.g., for RoBERTa Large, FedFed fails to converge. This proves the necessity of splitting layers for Forward-mode AD so that each client has fewer trainable weights to perturb.

[MISSING_PAGE_FAIL:27]

Figure 7: Generalized accuracy /F1 score / Exact matches for homogeneous clients (Dirichlet \(\alpha=1.0\)) setting for a variety of language models

Figure 8: Generalized accuracy / F1 score / Exact matches for heterogeneous clients (Dirichlet \(\alpha=0.1\)) setting

[MISSING_PAGE_FAIL:30]

Figure 10: Personalized accuracy / F1 score / Exact matches for homogeneous clients (Dirichlet \(\alpha=1.0\)) setting for a variety of language models

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline  & \multicolumn{2}{c}{Backpropagation-based} & \multicolumn{2}{c}{Zero-order-based} & \multicolumn{2}{c}{First-order} & \multicolumn{2}{c}{Difference between} \\  & \multicolumn{2}{c}{Methods \(\uparrow\)} & \multicolumn{2}{c}{Methods \(\uparrow\)} & \multicolumn{2}{c}{Forward Mode AD \(\uparrow\)} & \multicolumn{2}{c}{performances of Spry and} \\ \cline{2-9}  & FedAvg & FedYogi & FwdLLM+ & FedMEZO & Baffe+ & Spry & \begin{tabular}{c} best-performing \\ backpropagation \\ method \(\uparrow\) \\ \end{tabular} & 
\begin{tabular}{c} best-performing \\ zero-order \\ method \(\uparrow\) \\ \end{tabular} \\ \hline AG News & 97.76\% & 97.71\% & 79.94\% & 72.69\% & 60.89\% & 89.91\% & -7.85\% & 9.97\% \\ SST2 & 95.84\% & 95.90\% & 85.51\% & 73.26\% & 64.55\% & 93.40\% & -2.50\% & 7.89\% \\ SNLI & 97.41\% & 97.57\% & 74.53\% & 71.54\% & 68.55\% & 83.45\% & -14.12\% & 8.92\% \\ MNLI & 90.38\% & 90.03\% & 72.71\% & 67.53\% & 63.58\% & 80.63\% & -9.75\% & 7.92\% \\ Yahoo & 89.76\% & 89.64\% & 77.93\% & 67.64\% & 59.40\% & 82.80\% & -6.96\% & 4.87\% \\ Yelp & 93.44\% & 97.81\% & 73.04\% & 68.77\% & 57.78\% & 85.83\% & -11.98\% & 12.79\% \\ MultiRC \(\diamond\) & 50.28\% & 75.21\% & 65.91\% & N/A & 61.41\% & 71.20\% & -4.01\% & 5.29\% \\ SQaADv2 \(\star\) & 20.49 & 21.25 & 14.43 & 14.04 & 12.27 & 17.73 & -3.52 & 3.30 \\ SQaADv2 \(\square\) & 13.06 & 12.49 & 8.96 & 9.10 & 8.17 & 9.88 & -3.18 & 0.78 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Personalized accuracy (\(Acc_{p}\)) for Spry and its backpropagation- and zero-order-based counterparts on RoBERTa Large and LLMs. SQuADv2 uses F1 score. \(\uparrow\) shows that higher values are better. The datasets are split with Dir \(\alpha=0.1\). \(\diamond\) = Llama2-7B. \(\star\) = OPT6.7B. \(\square\) = OPT13B.

Spry significantly outperforms the best-performing zero-order-based methods.

Figure 11: Personalized accuracy / F1 score / Exact matches for heterogeneous clients (Dirichlet \(\alpha=0.1\)) setting

[MISSING_PAGE_EMPTY:33]

[MISSING_PAGE_FAIL:34]

**Assumption I.2** (Bounded Global Variance (Assumption 2 in AFO [25])).: The variance of the global objective function \(f\) is bounded by \(\sigma_{g}\) as

\[\frac{1}{M}\sum_{m=1}^{M}||\left[\nabla F_{m}(w_{\overline{m}})\right]_{j}- \left[\nabla f(w_{\overline{m}})\right]_{j}||^{2}\leq\sigma_{g,j}^{2};\;\forall m \in[M],w\in\mathbb{R}^{d}\text{ and }\forall j\in[d],\]

where \(\nabla F_{m}\) is the true gradient of client \(m\). As defined earlier, \(\overline{m}\in\left[\frac{(m-1)d}{M}+1,\frac{md}{M}\right]\).

**Assumption I.3** (Bounded Gradients (Assumption 3 in AFO [25])).: The function \(f_{m}(w;(x,y))\) has \(G\)-bounded gradients such that for any client \(m\in[M]\), weights \(w\in\mathbb{R}^{d}\), and sampled data \((x,y)\sim\mathcal{D}_{m}\); we have

\[|\left[\nabla f_{m}(w;(x,y))\right]_{j}|<G,\;\forall j\in[d]\]

### First and Second Moments of Forward Gradients

We first prove that in Spry, estimation of \(\nabla f\) by the accumulated forward mode gradients \(\nabla\hat{f}\) across all clients of an arbitrary round \(r\), depends on the heterogeneity across client datasets. Statements on homogeneous data have been proven for FGd[23] and MeZO[18] in single-client or centralized settings. Here we focus on the specific federated setting of Spry, where gradients are accumulated differently than in traditional FedAvg[1]. In Spry, we have

\[\nabla f(w)=\mathbb{E}_{v}\left[\nabla\hat{f}(w,v)\right]=\left[\frac{1}{ \widetilde{M}}\sum_{m\in\widetilde{\mathcal{M}}}\mathbb{E}\left[\nabla\hat{f} _{m}(w_{\overline{m}},v_{\overline{m}};\mathcal{D}_{m})\right]\Big{|}\;\forall \widetilde{\mathcal{M}}\subset\mathcal{M};\overline{m}\in\left[\frac{(m-1)d}{ M}+1,\frac{md}{M}\right]\right],\] (12)

where \(w\) represents weights, \(v\) are their corresponding perturbations, and \(\mathcal{D}_{m}\) is the dataset of client \(m\).

We omit the round index of the model weights \(w\) and their perturbations \(v\) for this section since the same relationship will hold for any arbitrary round \(r\).

**Theorem I.4** (Estimation of the Global Gradient).: _In Spry, global forward gradient \(\nabla\hat{f}\) of the trainable weights \(w\in\mathbb{R}^{d}\), with the corresponding weight perturbations \(v\in\mathbb{R}^{d}\), computed by \(M\) participating clients is estimated in terms of true global gradient \(\nabla f\) as,_

\[\mathbb{E}_{v,\mathcal{D}}[\nabla\hat{f}(w,v;\mathcal{D})]=\nabla f (w)\] \[\qquad\qquad+\frac{1}{\widetilde{M}}\left[\sum_{m\in\widetilde{ \mathcal{M}}_{1}}\sum_{c=1}^{C}\alpha_{m,c}\mathbb{E}_{(x,y_{c})\in\mathcal{D} }\left[\nabla\hat{f}_{m}(w_{[1,\frac{d}{M}]},v_{[1,\frac{d}{M}]};(x,y_{c})) \right]\right]^{T}\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\vdots\]

_where \(C\) is total number of classes and \(\alpha_{m,c}=\left(\frac{n_{c}}{|\mathcal{D}|}-\frac{n_{m,c}n_{c}}{|\mathcal{D }_{m}|}\right)\). For a class \(c\), \(n_{c}\) is its sample count, \(\alpha_{c}\) is its Dirichlet concentration parameter. For a client \(m\); \(n_{m,c}\) is the sample count of the \(c^{th}\) class, and \(\mathcal{D}_{m}\) is the size of the data of client \(m\). The global data is \(\mathcal{D}=\sum_{m\in\mathcal{M}}\mathcal{D}_{m}\). \(\widetilde{\mathcal{M}}\) is the set of clients training an arbitrary subset of weights, \(\widetilde{M}=|\widetilde{\mathcal{M}}_{i}|;\;\forall i\in[M/d]\)._

Proof.: Suppose the global dataset \(\mathcal{D}\) is defined as a combination of all \(\mathcal{D}_{m}\). Hence, \(\mathcal{D}=\cup_{m\in[M]}\mathcal{D}_{m}\). In Spry, the global forward gradient is defined as

\[\mathbb{E}_{v,\mathcal{D}}[\nabla\hat{f}(w,v;\mathcal{D})]=\mathbb{E}\left[ \begin{array}{c}\frac{1}{M}\sum_{m\in\widetilde{\mathcal{M}}_{1}}\nabla\hat{ f}_{m}(w_{[1,\frac{d}{M}]},v_{[1,\frac{d}{M}]};\mathcal{D})\\ \frac{1}{M}\sum_{m\in\widetilde{\mathcal{M}}_{2}}\nabla\hat{f}_{m}(w_{[\frac{d} {M}+1,\frac{d}{M}]},v_{[\frac{d}{M}+1,\frac{d}{M}]};\mathcal{D})\\ \vdots\\ \frac{1}{M}\sum_{m\in\widetilde{\mathcal{M}}_{M/d}}\nabla\hat{f}_{m}(w_{[\frac {(M-1)d}{M}+1,d]},v_{[\frac{(M-1)d}{M}+1,d]};\mathcal{D})\end{array}\right]^ {T}\] (13)To combine the gradient estimates from the above equation for all clients \(m\in\mathcal{M}\), we first consider the _bias_\(b_{m}\) between the gradient estimate under **(a)** Globally combined data \(\mathcal{D}\) and **(b)** Data \(\mathcal{D}_{m}\) of an arbitrary client \(m\).

**Measuring the dataset bias.** Note that we consider samples \((x,y)\) of \(\mathcal{D}_{m}\) to be sampled from \(\mathcal{D}\), since \(\mathcal{D}_{m}\subset\mathcal{D}\).

\[\mathbb{E}_{v_{\overline{m}},\mathcal{D}}\left[\nabla\hat{f}_{m}(w_{\overline {m}},v_{\overline{m}};\mathcal{D})\right]=\mathbb{E}_{v_{\overline{m}}, \mathcal{D}_{m}}\left[\nabla\hat{f}_{m}(w_{\overline{m}},v_{\overline{m}}; \mathcal{D}_{m})\right]+b_{m}\] (14)

Computing the _bias_ term \(b\) requires information on dataset distribution. In FL settings, Dirichlet distribution to spread the data into heterogeneous splits is a popular way to simulate heterogeneity [37]. The biased dataset \(\mathcal{D}_{m}\) is sampled from the combined dataset \(\mathcal{D}\) using the Dirichlet distribution. This allows us to utilize the properties of the distribution to derive the relationship between forward gradients on global data and on individual local data:

For classification tasks, let's say \(\mathcal{D}\) has \(C\) total classes: \(y_{1},\ldots,y_{C}\). We have \(\alpha_{1},\ldots,\alpha_{C}\) as concentration parameters of the Dirichlet distribution. The expected forward gradient for the combined dataset \(\mathcal{D}\) can be expressed as a weighted sum of the expected forward gradients for each class \(C\):

\[\mathbb{E}_{v_{\overline{m}},\mathcal{D}}\left[\nabla\hat{f}_{m}(w_{\overline {m}},v_{\overline{m}};\mathcal{D})\right]=\sum_{c=1}^{C}\frac{n_{c}}{| \mathcal{D}|}\mathbb{E}_{v_{\overline{m}},(x,y_{c})\in\mathcal{D}}\left[\nabla \hat{f}_{m}(w_{\overline{m}},v_{\overline{m}};(x,y_{c}))\right],\] (15)

where \(n_{c}\) is the sample count of class \(c\). And for the biased dataset \(\mathcal{D}_{m}\) sampled with Dirichlet concentration parameters \(\alpha_{1},\ldots,\alpha_{C}\), the expected forward gradient can be expressed as,

\[\mathbb{E}_{v_{\overline{m}},\mathcal{D}_{m}}\left[\nabla\hat{f}_{m}(w_{ \overline{m}},v_{\overline{m}};\mathcal{D}_{m})\right]=\sum_{c=1}^{C}\frac{n_{ m,c}\alpha_{c}}{|\mathcal{D}_{m}|}\mathbb{E}_{v_{\overline{m}},(x,y_{c})\in \mathcal{D}}\left[\nabla\hat{f}_{m}(w_{\overline{m}},v_{\overline{m}};(x,y_{c }))\right],\] (16)

where \(n_{m,c}\) is the sample count of class \(c\) for client \(m\).

Subtracting Equation 16 from Equation 15,

\[b_{m} =\mathbb{E}_{v_{\overline{m}},\mathcal{D}}\left[\nabla\hat{f}_{m} (w_{\overline{m}},v_{\overline{m}};\mathcal{D})\right]-\mathbb{E}_{v_{ \overline{m}},\mathcal{D}_{m}}\left[\nabla\hat{f}_{m}(w_{\overline{m}},v_{ \overline{m}};\mathcal{D}_{m})\right]\] (17) \[=\sum_{c=1}^{C}\left(\frac{n_{c}}{|\mathcal{D}|}-\frac{n_{m,c} \alpha_{c}}{|\mathcal{D}_{m}|}\right)\mathbb{E}_{v_{\overline{m}},(x,y_{c}) \in\mathcal{D}}\left[\nabla\hat{f}_{m}(w_{\overline{m}},v_{\overline{m}};(x,y_ {c}))\right]\] (18)

For any \(\overline{m}\in\left[\frac{(m-1)d}{M}+1,\frac{md}{M}\right]\), we have \(\mathbb{E}_{v_{\overline{m}},\mathcal{D}_{m}}\left[\nabla\hat{f}_{m}(w_{ \overline{m}},v_{\overline{m}};\mathcal{D}_{m})\right]=F_{m}(w)\) from Lemma 1 in FGD [23].

Plugging in the above result and Equation 18 in Equation 14,

\[\mathbb{E}_{v_{\overline{m}},\mathcal{D}}\left[\nabla\hat{f}_{m}( w_{\overline{m}},v_{\overline{m}};\mathcal{D})\right]=\mathbb{E}_{v_{ \overline{m}},\mathcal{D}_{m}}\left[\nabla\hat{f}_{m}(w_{\overline{m}},v_{ \overline{m}};\mathcal{D}_{m})\right]+b_{m}\] \[=\nabla F_{m}(w_{\overline{m}})+\sum_{c=1}^{C}\left(\frac{n_{c}}{| \mathcal{D}|}-\frac{n_{m,c}\alpha_{c}}{|\mathcal{D}_{m}|}\right)\mathbb{E}_{v_ {\overline{m}},(x,y_{c})\in\mathcal{D}}\left[\nabla\hat{f}_{m}(w_{\overline{m}},v_{\overline{m}};(x,y_{c}))\right]\] (19)

For \(i^{th}\) row in Equation 13,

\[\mathbb{E}_{v_{\overline{m}},\mathcal{D}}\left[\nabla\hat{f}_{i}( w_{\overline{m}},v_{\overline{m}};\mathcal{D})\right]=\frac{1}{\widetilde{M}}\sum_{m\in \widetilde{\mathcal{M}}_{1}}\mathbb{E}_{v_{\overline{m}},\mathcal{D}}\left[ \nabla\hat{f}_{m}(w_{\overline{m}},v_{\overline{m}};\mathcal{D})\right]\] (20) \[=\frac{1}{\widetilde{M}}\sum_{m\in\widetilde{\mathcal{M}}_{1}} \left(\nabla F_{m}(w_{\overline{m}})+\sum_{c=1}^{C}\left(\frac{n_{c}}{| \mathcal{D}|}-\frac{n_{m,c}\alpha_{c}}{|\mathcal{D}_{m}|}\right)\mathbb{E}_{v_ {\overline{m}},(x,y_{c})\in\mathcal{D}}\left[\nabla\hat{f}_{m}(w_{\overline{m} },v_{\overline{m}};(x,y_{c}))\right]\right)\] (21) \[=\nabla f(w_{\overline{m}})+\frac{1}{\widetilde{M}}\sum_{m\in \widetilde{\mathcal{M}}_{1}}\sum_{c=1}^{C}\left(\frac{n_{c}}{|\mathcal{D}|}- \frac{n_{m,c}\alpha_{c}}{|\mathcal{D}_{m}|}\right)\mathbb{E}_{v_{\overline{m}},(x,y_{c})\in\mathcal{D}}\left[\nabla\hat{f}_{m}(w_{\overline{m}},v_{\overline{m}};(x,y_{c}))\right]\] (22)\[\mathbb{E}_{v,\mathcal{D}}[\nabla\hat{f}(w,v;\mathcal{D})]\] (23) \[=\begin{bmatrix}\nabla f(w_{\left[1,\frac{d}{M}\right]})+\frac{1}{M} \sum_{m\in\widetilde{\mathcal{M}}_{1}}\sum_{c=1}^{C}\left(\frac{n_{c}}{| \mathcal{D}|}-\frac{n_{m,c}\alpha_{c}}{|\mathcal{D}_{m}|}\right)\mathbb{E}_{( x,y_{c})\in\mathcal{D}}\left[\nabla\hat{f}_{m}(w_{\left[1,\frac{d}{M}\right]},v_{ \left[1,\frac{d}{M}\right]};(x,y_{c}))\right]\\ \nabla f(w_{\left[\frac{d}{M}+1,\frac{2d}{M}\right]})+\frac{1}{M}\sum_{m\in \widetilde{\mathcal{M}}_{2}}\sum_{c=1}^{C}\left(\frac{n_{c}}{|\mathcal{D}|}- \frac{n_{m,c}\alpha_{c}}{|\mathcal{D}_{m}|}\right)\mathbb{E}_{(x,y_{c})\in \mathcal{D}}\left[\nabla\hat{f}_{m}(w_{\left[\frac{d}{M}+1,\frac{2d}{M}\right]} ;v_{\left[\frac{d}{M}+1,\frac{2d}{M}\right]};(x,y_{c}))\right]\\ \vdots\end{bmatrix}^{T}\] (24) \[=\begin{bmatrix}\nabla f(w_{\left[1,\frac{d}{M}\right]})\\ \nabla f(w_{\left[\frac{d}{M}+1,\frac{2d}{M}\right]})\\ \vdots\end{bmatrix}^{T}+\frac{1}{\widetilde{M}}\begin{bmatrix}\sum_{m\in \widetilde{\mathcal{M}}_{1}}\sum_{c=1}^{C}\left(\frac{n_{c}}{|\mathcal{D}|}- \frac{n_{m,c}\alpha_{c}}{|\mathcal{D}_{m}|}\right)\mathbb{E}_{(x,y_{c})\in \mathcal{D}}\left[\nabla\hat{f}_{m}(w_{\left[\frac{d}{M}+1,\frac{2d}{M}\right]} ;v_{\left[\frac{d}{M}+1,\frac{2d}{M}\right]};(x,y_{c}))\right]\\ \vdots\end{bmatrix}^{T}\] (25) \[=\nabla f(w)+\frac{1}{\widetilde{M}}\begin{bmatrix}\sum_{m\in \widetilde{\mathcal{M}}_{1}}\sum_{c=1}^{C}\left(\frac{n_{c}}{|\mathcal{D}|}- \frac{n_{m,c}\alpha_{c}}{|\mathcal{D}_{m}|}\right)\mathbb{E}_{(x,y_{c})\in \mathcal{D}}\left[\nabla\hat{f}_{m}(w_{\left[1,\frac{d}{M}\right]},v_{\left[1,\frac{d}{M}\right]};(x,y_{c}))\right]\\ \sum_{m\in\widetilde{\mathcal{M}}_{2}}\sum_{c=1}^{C}\left(\frac{n_{c}}{| \mathcal{D}|}-\frac{n_{m,c}\alpha_{c}}{|\mathcal{D}_{m}|}\right)\mathbb{E}_{( x,y_{c})\in\mathcal{D}}\left[\nabla\hat{f}_{m}(w_{\left[\frac{d}{M}+1,\frac{2d}{M} \right]};v_{\left[\frac{d}{M}+1,\frac{2d}{M}\right]};(x,y_{c}))\right]\\ \vdots\end{bmatrix}^{T}\] (26)

Next, we formulate the norm of the forward gradient \(\nabla\hat{f}\).

**Lemma I.5** (Norm of the Forward Gradient).: _Under Assumption I.2, and at the participation rate of \(s\), \(M\) participating clients training weights \(w\in\mathbb{R}^{d}\) through random perturbations \(v\in\mathbb{R}^{d}\) in Spry derives the accumulated forward gradient \(\nabla\hat{f}(w,v;\mathcal{D})\) such that,_

\[\mathbb{E}_{v,\mathcal{D}}||\nabla\hat{f}(w,v;\mathcal{D})||^{2} =\mathbb{E}||\nabla f(w)||^{2}\left(1+\left(\frac{2(3d+K-1)}{ \widetilde{MK}}\right)\sum_{m\in\mathcal{M}}\sum_{c\in[C]}\left(\frac{n_{c}}{| \mathcal{D}|}-\frac{n_{m,c}\alpha_{c}}{|\mathcal{D}_{m}|}\right)^{2}\right)\\ +\left(\frac{2\sigma_{g}^{2}(1-s)(3d+K-1)}{\widetilde{MK}}\right) \sum_{m\in\mathcal{M}}\sum_{c\in[C]}\left(\frac{n_{c}}{|\mathcal{D}|}-\frac{n_{ m,c}\alpha_{c}}{|\mathcal{D}_{m}|}\right)^{2},\] (27)

_where \(\mathcal{D}\) is the combination of datasets of all \(M\) clients, \(K\) is the number of perturbations per batch, \(\widetilde{M}\) is the count of clients training a particular weight subset, \(\sigma_{g}^{2}\) is the upper bound of the global gradient variance. For a total of \(C\) classes in \(\mathcal{D}\), we define \(n_{c}\) as the sample count of the \(c^{th}\) class, \(\alpha_{c}\) is Dirichlet concentration parameter for the \(c^{th}\) class, \(\mathcal{D}\) is the size of the global data. For a client \(m\); \(n_{m,c}\) is the sample count of the \(c^{th}\) class for client \(m\), and \(\mathcal{D}_{m}\) is the size of the data of client \(m\)._

Proof.: The proof follows a similar style of Lemma 2 in MeZO[18]. The difference in our setting is that we have \(\nabla\hat{f}(w,v;\mathcal{D})\) which is an aggregate of \(\nabla\hat{f}_{m}(w,v;\mathcal{D})\) derived from the federated clients, while MeZO has results under the setting of a single client.

From Theorem I.4 we have,

\[\mathbb{E}_{v,\mathcal{D}}\left[\nabla\hat{f}(w,v;\mathcal{D})\right]\] (28) \[=\begin{bmatrix}\nabla f(w_{\left[1,\frac{d}{M}\right]})+\frac{1}{ M}\sum_{m\in\widetilde{\mathcal{M}}_{1}}\sum_{c=1}^{C}\left(\frac{n_{c}}{| \mathcal{D}|}-\frac{n_{m,c}\alpha_{c}}{|\mathcal{D}_{m}|}\right)\mathbb{E}_{( x,y_{c})\in\mathcal{D}}\left[\nabla\hat{f}_{m}(w_{\left[1,\frac{d}{M}\right]},v_{ \left[1,\frac{d}{M}\right]};(x,y_{c}))\right]\\ \nabla f(w_{\left[\frac{d}{M}+1,\frac{2d}{M}\right]})+\frac{1}{\widetilde{M}} \sum_{m\in\widetilde{\mathcal{M}}_{2}}\sum_{c=1}^{C}\left(\frac{n_{c}}{| \mathcal{D}|}-\frac{n_{m,c}\alpha_{c}}{|\mathcal{D}_{m}|}\right)\mathbb{E}_{( x,y_{c})\in\mathcal{D}}\left[\nabla\hat{f}_{m}(w_{\left[\frac{d}{M}+1,\frac{2d}{M}\right]},v_{ \left[\frac{d}{M}+1,\frac{2d}{M}\right]};(x,y_{c}))\right]\\ \vdots\end{bmatrix}^{T}\]where

\[\mathbb{E}_{v_{\overline{m}},(x,y_{c})\sim\mathcal{D}}\left[\nabla\hat{f}_{m}(w_{ \overline{m}},v_{\overline{m}};(x,y_{c}))\right]=\frac{1}{|\mathcal{D}|K}\sum_{ (x,y_{c})\sim\mathcal{D}}\sum_{i\in[K]}\mathbb{E}\left[(\nabla f_{m}\left(w_{ \overline{m}};(x,y_{c})\right)\cdot v_{i,\overline{m}})\,v_{i,\overline{m}}^{ T}\right],\]

using Equation 10. The right hand side expectation is also under the randomness of partial perturbations. Here, \(K\) is perturbation count, and \(|\mathcal{D}|\) is the size of the combined dataset \(\mathcal{D}=\cup_{m\in[M]}\mathcal{D}_{m}\).

To compute the second moment for a client \(m\),

\[\mathbb{E}_{v_{\overline{m}},(x,y_{c})\sim\mathcal{D}} \left[\nabla\hat{f}_{m}(w_{\overline{m}},v_{\overline{m}};(x,y_{c }))\cdot\nabla\hat{f}_{m}(w_{\overline{m}},v_{\overline{m}};(x,y_{c}))^{T}\right]\] \[=\frac{1}{|\mathcal{D}|^{2}K^{2}}\sum_{\begin{subarray}{c}(x,y_ {c}\sim\mathcal{D})\\ (\overline{x},\overline{y}_{c}\sim\mathcal{D})\end{subarray}}\sum_{ \begin{subarray}{c}i\in[K]\\ j\in[K]\end{subarray}}\mathbb{E}\Bigg{[}\left(\nabla f_{m}(w_{\overline{m}};(x,y_{c}))v_{i,\overline{m}}v_{i,\overline{m}}^{T}\right)\cdot\left(\nabla f_{m} (w_{\overline{m}};(x,y_{c}))^{T}v_{j,\overline{m}}v_{j,\overline{m}}^{T} \right)\Bigg{]}.\] (29)

For simplicity, let \(a\) and \(b\) be two arbitrary vectors representing \(\mathbb{E}\left[\nabla f_{m}(w_{\overline{m}};(x,y_{c}))\right]\) and \(\mathbb{E}\left[\nabla f_{m}(w_{\overline{m}};(\overline{x},\overline{y}_{c}) )^{T}\right]\), respectively.

For the sum over all perturbations, we have two cases (all the expectations are under the randomness of partial \(v\)),

1. If \(i\neq j\). (Occurs \(K(K-1)\) times) \[\mathbb{E}\left[v_{i,\overline{m}}v_{i,\overline{m}}^{T}\ ab^{T} \ v_{j,\overline{m}}v_{j,\overline{m}}^{T}\right]=\mathbb{E}\left[v_{i, \overline{m}}v_{i,\overline{m}}^{T}\right]\cdot ab^{T}\cdot\mathbb{E}\left[v_ {j,\overline{m}}v_{j,\overline{m}}^{T}\right]\] (30) \[\text{(since $v_{i}$ and $v_{j}$ are independent of each other and $a,b$ don't depend on $v$)}\] \[=I\cdot ab^{T}\cdot I=ab^{T}\] (31)
2. If \(i=j\). (Occurs \(K\) times) \[\mathbb{E}\left[v_{i,\overline{m}}v_{i,\overline{m}}^{T}\ ab^{T} \ v_{j,\overline{m}}v_{j,\overline{m}}^{T}\right] =\mathbb{E}\left[v_{i,\overline{m}}v_{i,\overline{m}}^{T}\ ab^{T} \ v_{i,\overline{m}}v_{i,\overline{m}}^{T}\right]\] (32) \[=\mathbb{E}_{v_{i}}\left[v_{i}^{4}\right]\left\langle a,b\right\rangle\] (33) For all such \(v_{i}\) with \(i\in[K]\), \[\mathbb{E}_{v}[v^{\otimes 4}]\langle a,b\rangle =3d\text{ Sym}(I^{\otimes 4})\langle a,b\rangle\] (34) \[=2d\cdot ab^{T}+d\cdot I\cdot a^{T}b\] (35)

Plugging in the results of the above two cases in Equation 29,

\[\therefore\mathbb{E}\left[\nabla\hat{f}_{m}(w_{\overline{m}},v_{ \overline{m}};(x,y_{c}))\cdot\nabla\hat{f}_{m}(w_{\overline{m}},v_{\overline{ m}};(x,y_{c}))^{T}\right]\] \[=\frac{1}{|\mathcal{D}|^{2}K^{2}}\sum_{\begin{subarray}{c}(x,y_{c }\sim\mathcal{D})\\ (\overline{x},\overline{y}_{c}\sim\mathcal{D})\end{subarray}}\left[K(K-1)ab^{ T}+2dKab^{T}+dK\cdot I\cdot a^{T}b\right]\] (36) \[=\frac{1}{|\mathcal{D}|^{2}K}\sum_{\begin{subarray}{c}(x,y_{c} \sim\mathcal{D})\\ (\overline{x},\overline{y}_{c}\sim\mathcal{D})\end{subarray}}\Bigg{[}(2d+K-1) \mathbb{E}\left[\nabla f_{m}(w_{\overline{m}};(x,y_{c}))\nabla f_{m}(w_{ \overline{m}};(\overline{x},\overline{y}_{c}))^{T}\right]\] \[\qquad\qquad\qquad\qquad\qquad+d\cdot I\cdot\mathbb{E}\Big{[} \nabla f_{m}(w_{\overline{m}};(x,y_{c}))\nabla f_{m}(w_{\overline{m}};(\overline {x},\overline{y}_{c}))^{T}\Big{]}\Bigg{]}\] (37)

Here, the randomness is under the sampled data.

For the sum over samples of \(\mathcal{D}\), we have two cases,

1. If \((x,y_{c})\neq(\overline{x},\overline{y}_{c})\). (Occurs \(|\mathcal{D}|(|\mathcal{D}|-1)\) times) \[\mathbb{E}[\nabla f_{m}(w_{\overline{m}};(x,y_{c}))\nabla f_{m}(w_{\overline{m}} ;(\overline{x},\overline{y}_{c}))^{T}]=\nabla F_{m}(w_{\overline{m}})\nabla F_ {m}(w_{\overline{m}})^{T}\] (38)2. If \((x,y_{c})=(\overline{x},\overline{y}_{c})\). (Occurs \(|\mathcal{D}|\) times) \[\mathbb{E}[\nabla f_{m}(w_{\overline{m}};(x,y_{c}))\nabla f_{m}(w_{\overline{m} };(\overline{x},\overline{y}_{c}))^{T}]=\nabla F_{m}(w_{\overline{m}})\nabla F _{m}(w_{\overline{m}})^{T}+\Sigma(w_{\overline{m}})\] (39)

Combining both the cases, we get

\[\mathbb{E}_{(x,y_{c})\sim\mathcal{D}}[\nabla f_{m}(w_{\overline{m}};(x,y_{c})) \nabla f_{m}(w_{\overline{m}};(\overline{x},\overline{y}_{c}))^{T}]=|\mathcal{ D}|^{2}\nabla F_{m}(w_{\overline{m}})\nabla F_{m}(w_{\overline{m}})^{T}+| \mathcal{D}|\cdot\Sigma(w_{\overline{m}})\] (40)

Plugging in Equation 40 in Equation 37,

\[\mathbb{E}_{v_{\overline{m}},(x,y_{c})\sim\mathcal{D}}[\nabla\hat {f}_{m}(w_{\overline{m}},v_{\overline{m}};(x,y_{c}))\nabla\hat{f}_{m}(w_{ \overline{m}},v_{\overline{m}};(x,y_{c}))^{T}]\] \[=\frac{1}{|\mathcal{D}|^{2}K}\Bigg{[}(2d+K-1)|\mathcal{D}|(| \mathcal{D}|\nabla F_{m}(w_{\overline{m}})\nabla F_{m}(w_{\overline{m}})^{T}+ \Sigma(w_{\overline{m}}))\] \[\qquad\qquad+d\cdot|\mathcal{D}|\left(|\mathcal{D}|\cdot|| \nabla F_{m}(w_{\overline{m}})||^{2}+\text{tr}\left(\Sigma(w_{\overline{m}}) \right)\right)\Bigg{]}\] (41) \[=\frac{(2d+K-1)}{K}\left(\nabla F_{m}(w_{\overline{m}})\nabla F_{ m}(w_{\overline{m}})^{T}+\frac{1}{|\mathcal{D}|}\Sigma(w_{\overline{m}})\right)\] \[\qquad+\frac{d}{K}\left(||\nabla F_{m}(w_{\overline{m}})||^{2}+ \frac{1}{|\mathcal{D}|}\text{tr}\left(\Sigma(w_{\overline{m}})\right)\right)\] (42) \[=\frac{3d+K-1}{K}\mathbb{E}_{(x,y_{c})\sim\mathcal{D}}||\nabla f_ {m}(w_{\overline{m}};(x,y_{c}))||^{2}\] (43)

Hence for client \(m\), the expected norm of forward gradients under randomness of perturbations \(v\) is,

\[\mathbb{E}_{v_{\overline{m}},(x,y_{c})\sim\mathcal{D}}||\nabla \hat{f}_{m}(w_{\overline{m}},v_{\overline{m}};(x,y_{c}))||^{2} =\frac{3d+K-1}{K}\mathbb{E}_{(x,y_{c})\sim\mathcal{D}}||\nabla f _{m}(w_{\overline{m}};(x,y_{c}))||^{2}\] (44) \[=\frac{3d+K-1}{K}||\nabla F_{m}(w_{\overline{m}})||^{2}\] (45)

For \(i^{th}\) row of \(\mathbb{E}\left[\nabla\hat{f}(w,v;\mathcal{D})\right]\) of Equation 28,

\[\mathbb{E}\left||\nabla\hat{f}_{i}(w_{\left[\frac{(i-1)d}{M}+1, \frac{id}{M}\right]},v_{\left[\frac{(i-1)d}{M}+1,\frac{id}{M}\right]};\mathcal{ D})\right||^{2}=\mathbb{E}\left||\nabla f_{i}(w_{\left[\frac{(i-1)d}{M}+1, \frac{id}{M}\right]})||^{2}\] \[\quad+\frac{1}{(\widetilde{M})^{2}}\sum_{m\in\widetilde{\mathcal{ M}}_{i}}\sum_{c\in[C]}\left(\frac{n_{c}}{|\mathcal{D}|}-\frac{n_{m,c}\alpha_{c}}{| \mathcal{D}_{m}|}\right)^{2}\mathbb{E}_{(x,y_{c})\sim\mathcal{D}}\left||\nabla \hat{f}_{m}(w_{\left[\frac{(i-1)d}{M}+1,\frac{id}{M}\right]},v_{\left[\frac{( i-1)d}{M}+1,\frac{id}{M}\right]};(x,y_{c}))\right||^{2}\] (46)

The left-hand side expectation under the randomness of sampled data \(\mathcal{D}\) and subset of random perturbations \(v\).

Plugging in Equation 45 in the above equation and using \(\overline{i}=\left[\frac{(i-1)d}{M}+1,\frac{id}{M}\right]\),

\[\mathbb{E}_{v_{\overline{i}};\mathcal{D}}\left||\nabla\hat{f}_{i}(w_{\overline{ i}},v_{\overline{i}};\mathcal{D})\right||^{2}=\mathbb{E}\left||\nabla f_{i}(w_{ \overline{i}})||^{2}+\frac{(3d+K-1)}{(\widetilde{M})^{2}K}\sum_{m\in\widetilde{ \mathcal{M}}_{i}}||\nabla F_{m}(w_{\overline{i}})||^{2}\sum_{c\in[C]}\left( \frac{n_{c}}{|\mathcal{D}|}-\frac{n_{m,c}\alpha_{c}}{|\mathcal{D}_{m}|}\right)^{2}\] (47)Since \(\nabla\hat{f}_{i},\ \forall\widetilde{\mathcal{M}}\subset\mathcal{M}\) are independent of each other, we can compute the norm of \(\nabla\hat{f}\) as follows,

\[\mathbb{E}_{v,\mathcal{D}}||\nabla\hat{f}(w,v;\mathcal{D})||^{2}= \mathbb{E}||\nabla f(w)||^{2}+\left(\frac{3d+K-1}{(\widetilde{M})^{2}K}\right) \sum_{m\in\mathcal{M}}\left||\nabla F_{m}(w_{\overline{m}})\right||^{2}\sum_{c \in[C]}\left(\frac{n_{c}}{|\mathcal{D}|}-\frac{n_{m,c}\alpha_{c}}{|\mathcal{D} _{m}|}\right)^{2}\] (48) \[=\mathbb{E}||\nabla f(w)||^{2}+\left(\frac{3d+K-1}{(\widetilde{M} )^{2}K}\right)\sum_{m\in\mathcal{M}}\left||\nabla F_{m}(w_{\overline{m}})- \nabla f(w_{\overline{m}})+\nabla f(w_{\overline{m}})\right||^{2}\sum_{c\in[C ]}\left(\frac{n_{c}}{|\mathcal{D}|}-\frac{n_{m,c}\alpha_{c}}{|\mathcal{D}_{m} |}\right)^{2}\] (49) \[=\mathbb{E}||\nabla f(w)||^{2}+\left(\frac{2(3d+K-1)}{(\widetilde {M})^{2}K}\right)\sum_{m\in\mathcal{M}}\left||\nabla F_{m}(w_{\overline{m}})- \nabla f(w_{\overline{m}})\right||^{2}\sum_{c\in[C]}\left(\frac{n_{c}}{| \mathcal{D}|}-\frac{n_{m,c}\alpha_{c}}{|\mathcal{D}_{m}|}\right)^{2}\] \[\qquad+\left(\frac{2(3d+K-1)}{(\widetilde{M})^{2}K}\right)\sum_{ m\in\mathcal{M}}\left||\nabla f(w_{\overline{m}})\right||^{2}\sum_{c\in[C]} \left(\frac{n_{c}}{|\mathcal{D}|}-\frac{n_{m,c}\alpha_{c}}{|\mathcal{D}_{m}|} \right)^{2}\] (50)

Using Assumption I.2 and limited participation rate of \(s\),

\[\mathbb{E}_{v,\mathcal{D}}||\nabla\hat{f}(w,v;\mathcal{D})||^{2}= \mathbb{E}||\nabla f(w)||^{2}+\left(\frac{2\sigma_{g}^{2}(1-s)(3d+K-1)}{ \widetilde{M}K}\right)\sum_{m\in\mathcal{M}}\sum_{c\in[C]}\left(\frac{n_{c}}{| \mathcal{D}|}-\frac{n_{m,c}\alpha_{c}}{|\mathcal{D}_{m}|}\right)^{2}\] \[\qquad+\left(\frac{2(3d+K-1)}{\widetilde{M}K}\right)\mathbb{E} \left||\nabla f(w)\right||^{2}\sum_{m\in\mathcal{M}}\sum_{c\in[C]}\left(\frac{ n_{c}}{|\mathcal{D}|}-\frac{n_{m,c}\alpha_{c}}{|\mathcal{D}_{m}|}\right)^{2}\] (51)

Rearranging the terms, we get,

\[\mathbb{E}_{v,\mathcal{D}}||\nabla\hat{f}(w,v;\mathcal{D})||^{2}= \mathbb{E}||\nabla f(w)||^{2}\left(1+\left(\frac{2(3d+K-1)}{ \widetilde{M}K}\right)\sum_{m\in\mathcal{M}}\sum_{c\in[C]}\left(\frac{n_{c}}{| \mathcal{D}|}-\frac{n_{m,c}\alpha_{c}}{|\mathcal{D}_{m}|}\right)^{2}\right)\] \[\qquad+\left(\frac{2\sigma_{g}^{2}(1-s)(3d+K-1)}{\widetilde{M}K} \right)\sum_{m\in\mathcal{M}}\sum_{c\in[C]}\left(\frac{n_{c}}{|\mathcal{D}|}- \frac{n_{m,c}\alpha_{c}}{|\mathcal{D}_{m}|}\right)^{2}\] (52)

### Convergence Rate of Spry

The general template for the convergence analysis of Spry is similar to FedAdam, hence we will follow Theorem 2 of AFO [25]. Our aim here is to highlight the differences in treatment of our gradient estimator \(\nabla\hat{f}_{m}\ \forall m\in[M]\), and the aggregate global gradient \(\nabla f\) as shown in Equation 9.

**Theorem I.6**.: _Under the assumptions on \(L\)-smoothness (Asmp I.1), bounded global variance \(\sigma_{g}^{2}\) of accumulated gradients (Asmp I.2), and bound on gradient magnitude \(G\) (Asmp I.3) and the following conditions on the local learning rate \(\eta_{\ell}\),_

\[\eta_{\ell}=\min\left\{\mathcal{O}\left(\frac{\tau^{2}}{\sqrt{ \beta_{2}\eta GL}}\right)^{\frac{1}{2}},\mathcal{O}\left(\frac{1}{\sqrt{\beta_ {2}}G}\right),\mathcal{O}\left(\frac{\tau^{3}}{\sqrt{\beta_{2}}\sqrt{1-\beta_{2 }}G^{2}}\right)^{\frac{1}{2}},\right.\right.\] \[\left.\left.\mathcal{O}\left(\frac{\widetilde{M}K}{\beta_{2}G(3d+ K-1)\sum_{m\in[M]}\sum_{c\in[C]}\alpha_{m,c}^{2}}\right)\frac{}{}\right\};\] (53)

Spry _satisfies the following bound,_

\[\min_{0\leq r\leq R}\mathbb{E}_{r}||\nabla f(w^{(r)})||^{2}\leq \frac{f(w^{(0)})-\mathbb{E}_{R}[f(w^{(R)})]}{\eta R}\] \[\qquad+\left(2+\frac{\eta\eta_{\ell}L}{2\tau^{2}}+\frac{\sqrt{1- \beta_{2}}G\eta_{\ell}}{\tau^{3}}\right)\left(\frac{\sigma_{g}^{2}(1-s)(3d+K-1)} {\widetilde{M}K}\right)\sum_{m\in\mathcal{M}}\sum_{c\in[C]}\alpha_{m,c}^{2},\] (54)_where \(R\) is the total round count, \(w\in\mathbb{R}^{d}\) are the trainable weights, \(v\in\mathbb{R}^{d}\) are the random perturbations, \(K\) is the count of random perturbations per batch, \(\eta\) is the global learning rate, \(\tau\) is adaptability hyperparameter, \(s\) is client sampling rate. The rest of the symbols are defined in Theorem I.4._

Proof.: As shown in Equation 7, an update of the model weights \(w\) at server-side in Spry looks like,

\[w^{(r+1)}=w^{(r)}+\eta\frac{\Delta^{(r)}}{\sqrt{\Lambda^{(r)}}+\tau}\] (55)

Using Assumption I.1 and then the server-side update rule, we have

\[f(w^{(r+1)}) \leq f(w^{(r)})+\left\langle\nabla f(w^{(r)}),w^{(r+1)}-w^{(r)} \right\rangle+\frac{L}{2}||w^{(r+1)}-w^{(r)}||^{2}\] (56) \[=f(w^{(r)})+\eta\left\langle\nabla f(w^{(r)}),\frac{\Delta^{(r)} }{\sqrt{\Lambda^{(r)}}+\tau}\right\rangle+\frac{\eta^{2}L}{2}\sum_{i\in[d]} \frac{(\Delta^{(r)}_{i})^{2}}{\left(\sqrt{\Lambda^{(r)}_{i}}+\tau\right)^{2}}\] (57)

Taking expectation over randomness of round \(r\) and simplifying the terms,

\[\mathbb{E}_{r}[f(w^{(r+1)})]\] \[\qquad+\eta\underbrace{\left\langle\nabla f(w^{(r)}),\mathbb{E}_ {r}\left[\frac{\Delta^{(r)}}{\sqrt{\Lambda^{(r)}}+\tau}-\frac{\Delta^{(r)}}{ \sqrt{\beta\Lambda^{(r-1)}}+\tau}\right]\right\rangle}_{R_{2}}\] (58)

Bounds for \(R_{2}\) are derived in the exactly same manner as "Bounding \(R_{2}\)" in Theorem 2 of FedAdam[25],

\[R_{2}\leq\sqrt{1-\beta_{2}}\mathbb{E}_{r}\sum_{j=1}^{d}\frac{G}{\tau}\times \left[\frac{(\Delta^{(r)}_{j})^{2}}{\sqrt{\Lambda^{(r)}_{j}}+\tau}\right]\] (59)

**Bounding \(R_{1}\)** has a different treatment due to the distinct aggregation strategy of Spry:

\[R_{1}=\left\langle\nabla f(w^{(r)}),\mathbb{E}_{r}\left[\frac{\Delta^{(r)}}{ \sqrt{\beta\Lambda^{(r-1)}}+\tau}\right]\right\rangle\] (60)

Since \(\Delta^{(r)}\) is piece-wise made of aggregations of forward gradients for several parts of the model weights, as shown in Equation 12, we first center each gradient piece for the computation of the squared norm,

\[\therefore R_{1}=\left\langle\nabla f(w^{(r)}),\mathbb{E}_{r}\left[\frac{-\eta_{ \ell}\nabla\hat{f}(w^{(r)},v^{(r)},\mathcal{D})}{\sqrt{\beta\Lambda^{(r-1)}}+ \tau}\right]\right\rangle\] (61)

Using \(ab\leq(a^{2}+b^{2})/2\),

\[R_{1}\leq-\frac{\eta_{\ell}}{2}\sum_{j\in[d]}\frac{[\nabla f(w^{(r)})]^{2}}{ \sqrt{\beta\Lambda^{(r-1)}_{j}}+\tau}+\frac{\eta_{\ell}}{2}\mathbb{E}_{r}|| \nabla\hat{f}(w^{(r)},v^{r},\mathcal{D})||^{2}\] (62)

Using the result of Lemma I.5,

\[\therefore R_{1}\leq-\frac{\eta_{\ell}}{2}\sum_{j\in[d]}\frac{[ \nabla f(w^{(r)})]^{2}}{\sqrt{\beta\Lambda^{(r-1)}_{j}}+\tau}+\frac{\eta_{ \ell}}{2}\left(\frac{2\sigma_{g}^{2}(1-s)(3d+K-1)}{\widetilde{M}K}\right)\sum _{m\in\mathcal{M}}\sum_{c\in[C]}\left(\frac{n_{c}}{|\mathcal{D}|}-\frac{n_{m, c}\alpha_{c}}{|\mathcal{D}_{m}|}\right)^{2}\] (63)Putting \(R_{1}\) and \(R_{2}\) bounds in Equation 58,

\[\mathbb{E}_{r}[f(w^{(r+1)})]\leq f(w^{(r)})-\frac{\eta\eta_{\ell}}{2}\sum_{j\in[d]}\frac{[ \nabla f(w^{(r)})]^{2}}{\sqrt{\beta\Lambda_{j}^{(r-1)}}+\tau}\] \[\qquad+\frac{\eta\eta_{\ell}}{2}\left(\frac{2\sigma_{g}^{2}(1-s) (3d+K-1)}{\widetilde{M}K}\right)\sum_{m\in\mathcal{M}}\sum_{c\in[C]}\left( \frac{n_{c}}{|\mathcal{D}|}-\frac{n_{m,c}\alpha_{c}}{|\mathcal{D}_{m}|}\right) ^{2}\] \[\qquad+\frac{\eta\eta_{\ell}}{2}\left(1+\left(\frac{2(3d+K-1)}{ \widetilde{M}K}\right)\sum_{m\in\mathcal{M}}\sum_{c\in[C]}\left(\frac{n_{c}}{| \mathcal{D}|}-\frac{n_{m,c}\alpha_{c}}{|\mathcal{D}_{m}|}\right)^{2}\right) \mathbb{E}_{r}||\nabla f(w^{(r)})||^{2}\] \[\qquad+\frac{\eta^{2}L}{2}\sum_{i\in[d]}\mathbb{E}_{r}\left[ \frac{(\Delta_{i}^{(r)})^{2}}{\Lambda_{i}^{(r)}+\tau^{2}}\right]+\frac{\eta \sqrt{1-\beta_{2}}G}{\tau}\sum_{i\in[d]}\mathbb{E}_{r}\left[\frac{(\Delta_{i} ^{(r)})^{2}}{\sqrt{\Lambda_{i}^{(r)}}+\tau}\right]\] (64)

Summing over \(r=0\) to \(R-1\) and using telescoping sum, we get

\[\mathbb{E}_{R}[f(w^{(R)})]\leq f(w^{(0)})-\frac{\eta\eta_{\ell}}{2}\sum_{r=0}^{R-1} \sum_{j\in[d]}\frac{[\nabla f(w^{(r)})]^{2}}{\sqrt{\beta\Lambda_{j}^{(r-1)}}+\tau}\] \[\qquad+\frac{\eta\eta_{\ell}R}{2}\left(\frac{2\sigma_{g}^{2}(1-s) (3d+K-1)}{\widetilde{M}K}\right)\sum_{m\in\mathcal{M}}\sum_{c\in[C]}\left( \frac{n_{c}}{|\mathcal{D}|}-\frac{n_{m,c}\alpha_{c}}{|\mathcal{D}_{m}|}\right) ^{2}\] \[\qquad+\frac{\eta\eta_{\ell}}{2}\left(1+\left(\frac{2(3d+K-1)}{ \widetilde{M}K}\right)\sum_{m\in\mathcal{M}}\sum_{c\in[C]}\left(\frac{n_{c}}{| \mathcal{D}|}-\frac{n_{m,c}\alpha_{c}}{|\mathcal{D}_{m}|}\right)^{2}\right) \sum_{r=0}^{R-1}\mathbb{E}_{r}||\nabla f(w^{(r)})||^{2}\] \[\qquad+\left(\frac{\eta^{2}L}{2}+\frac{\eta\sqrt{1-\beta_{2}}G}{ \tau}\right)\underbrace{\sum_{r=0}^{R-1}\sum_{i\in[d]}\mathbb{E}_{r}\left[ \frac{(\Delta_{i}^{(r)})^{2}}{\Lambda_{i}^{(r)}+\tau^{2}}\right]}_{R_{4}}\] (65)

**Bounding \(R_{4}\)** follows a similar derivation as "Bounding \(R_{1}\)",

\[R_{4} =\mathbb{E}\sum_{r=0}^{R-1}\sum_{i\in[d]}\frac{(\Delta_{i}^{(r)}) ^{2}}{\Lambda_{i}^{(r)}+\tau^{2}}=\mathbb{E}\sum_{r=0}^{R-1}\sum_{i\in[d]} \frac{[-\eta_{\ell}\nabla\hat{f}(w^{(r)},v^{(r)},\mathcal{D})]_{i}^{2}}{ \Lambda_{i}^{(r)}+\tau^{2}}\] (66) \[\leq\eta_{\ell}^{2}\mathbb{E}\sum_{r=0}^{R-1}\left|\left|\frac{ \nabla\hat{f}(w^{(r)},v^{(r)},\mathcal{D})}{\tau^{2}}\right|\right|^{2}\] (67)

Using Lemma I.5 once again,

\[\therefore R_{4}\leq\frac{\eta_{\ell}^{2}}{\tau^{2}}\left(1+\left( \frac{2(3d+K-1)}{\widetilde{M}K}\right)\sum_{m\in\mathcal{M}}\sum_{c\in[C]} \left(\frac{n_{c}}{|\mathcal{D}|}-\frac{n_{m,c}\alpha_{c}}{|\mathcal{D}_{m}|} \right)^{2}\right)\mathbb{E}\sum_{r=0}^{R-1}||\nabla f(w^{(r)})||^{2}\] \[\qquad\qquad+\frac{\eta_{\ell}^{2}}{\tau^{2}}\left(\frac{2\sigma _{g}^{2}(1-s)R(3d+K-1)}{\widetilde{M}K}\right)\sum_{m\in\mathcal{M}}\sum_{c\in[ C]}\left(\frac{n_{c}}{|\mathcal{D}|}-\frac{n_{m,c}\alpha_{c}}{|\mathcal{D}_{m}|} \right)^{2}\] (68)Updating Equation 65 with the bounds of \(R_{4}\),

\[\mathbb{E}_{R}[f(w^{(R)})]\leq f(w^{(0)})-\frac{\eta\eta_{\ell}}{2} \sum_{r=0}^{R-1}\sum_{j\in[d]}\frac{[\nabla f(w^{(r)})]^{2}}{\sqrt{\beta\Lambda_ {j}^{(r-1)}+\tau}}\] \[\qquad+\frac{\eta\eta_{\ell}R}{2}\left(\frac{2\sigma_{g}^{2}(1-s)( 3d+K-1)}{\widetilde{M}K}\right)\sum_{m\in\mathcal{M}}\sum_{c\in[C]}\left(\frac{ n_{c}}{|\mathcal{D}|}-\frac{n_{m,c}\alpha_{c}}{|\mathcal{D}_{m}|}\right)^{2}\] \[\qquad+\frac{\eta\eta_{\ell}}{2}\left(1+\left(\frac{2(3d+K-1)}{ \widetilde{M}K}\right)\sum_{m\in\mathcal{M}}\sum_{c\in[C]}\left(\frac{n_{c}}{| \mathcal{D}|}-\frac{n_{m,c}\alpha_{c}}{|\mathcal{D}_{m}|}\right)^{2}\right) \sum_{r=0}^{R-1}\mathbb{E}_{r}||\nabla f(w^{(r)})||^{2}\] \[\qquad+\left(\frac{\eta^{2}L}{2}+\frac{\eta\sqrt{1-\beta_{2}}G}{ \tau}\right)\frac{\eta_{\ell}^{2}}{\tau^{2}}\left(1+\left(\frac{2(3d+K-1)}{ \widetilde{M}K}\right)\sum_{m\in\mathcal{M}}\sum_{c\in[C]}\left(\frac{n_{c}}{| \mathcal{D}|}-\frac{n_{m,c}\alpha_{c}}{|\mathcal{D}_{m}|}\right)^{2}\right) \sum_{r=0}^{R-1}\mathbb{E}_{r}||\nabla f(w^{(r)})||^{2}\] \[\qquad+\left(\frac{\eta^{2}L}{2}+\frac{\eta\sqrt{1-\beta_{2}}G}{ \tau}\right)\frac{\eta_{\ell}^{2}}{\tau^{2}}\left(\frac{2\sigma_{g}^{2}(1-s)R( 3d+K-1)}{\widetilde{M}K}\right)\sum_{m\in\mathcal{M}}\sum_{c\in[C]}\left(\frac{ n_{c}}{|\mathcal{D}|}-\frac{n_{m,c}\alpha_{c}}{|\mathcal{D}_{m}|}\right)^{2}\] (69)

Rearranging the terms,

\[\sum_{r=0}^{R-1}\sum_{j\in[d]}\frac{[\nabla f(w^{(r)})]^{2}}{ \sqrt{\beta\Lambda_{j}^{(r-1)}+\tau}}\leq\frac{f(w^{(0)})-\mathbb{E}_{R}[f(w^{ (R)})]}{\eta}\] \[\qquad+\left(\frac{2R\sigma_{g}^{2}(1-s)(3d+K-1)}{\widetilde{M}K} \right)\sum_{m\in\mathcal{M}}\sum_{c\in[C]}\left(\frac{n_{c}}{|\mathcal{D}|}- \frac{n_{m,c}\alpha_{c}}{|\mathcal{D}_{m}|}\right)^{2}\] \[\qquad+\left(1+\left(\frac{2(3d+K-1)}{\widetilde{M}K}\right)\sum_ {m\in\mathcal{M}}\sum_{c\in[C]}\left(\frac{n_{c}}{|\mathcal{D}|}-\frac{n_{m,c }\alpha_{c}}{|\mathcal{D}_{m}|}\right)^{2}\right)\sum_{r=0}^{R-1}\mathbb{E}_{r }||\nabla f(w^{(r)})||^{2}\] \[\qquad+\left(\eta L+\frac{2\sqrt{1-\beta_{2}}G}{\tau}\right)\frac {\eta_{\ell}}{\tau^{2}}\left(1+\left(\frac{2(3d+K-1)}{\widetilde{M}K}\right) \sum_{m\in\mathcal{M}}\sum_{c\in[C]}\left(\frac{n_{c}}{|\mathcal{D}|}-\frac{n_ {m,c}\alpha_{c}}{|\mathcal{D}_{m}|}\right)^{2}\right)\sum_{r=0}^{R-1}\mathbb{ E}_{r}||\nabla f(w^{(r)})||^{2}\] \[\qquad+\left(\frac{\eta L}{2}+\frac{\sqrt{1-\beta_{2}}G}{\tau} \right)\frac{\eta_{\ell}}{\tau^{2}}\left(\frac{R\sigma_{g}^{2}(1-s)(3d+K-1)}{ \widetilde{M}K}\right)\sum_{m\in\mathcal{M}}\sum_{c\in[C]}\left(\frac{n_{c}}{| \mathcal{D}|}-\frac{n_{m,c}\alpha_{c}}{|\mathcal{D}_{m}|}\right)^{2}\] (70)

Getting a lower bound for the left hand side term through using the fact \(\sqrt{\Lambda^{(r-1)}}\leq\eta_{\ell}KG\) from Theorem 2 of AFO [25],

\[\sum_{r=0}^{R-1}\sum_{j=1}^{d}\frac{\mathbb{E}_{r}[\nabla f(w^{(r)})]_{j}^{2}}{ \sqrt{\beta_{2}\Lambda_{j}^{(r-1)}+\tau}}\geq\sum_{r=0}^{R-1}\sum_{j=1}^{d} \frac{\mathbb{E}_{r}[\nabla f(w^{(r)})]_{j}^{2}}{\sqrt{\beta_{2}}\eta_{\ell}KG+ \tau}\geq\frac{R}{\sqrt{\beta_{2}}\eta_{\ell}KG+\tau}\min_{0\leq r\leq R} \mathbb{E}_{r}||\nabla f(w^{(r)})||^{2}\] (71)\[\cdot\frac{R}{\sqrt{\beta_{2}}\eta_{\ell}KG+\tau}\min_{0\leq r\leq R} \mathbb{E}_{r}||\nabla f(w^{(r)})||^{2}\leq\frac{f(w^{(0)})-\mathbb{E}_{R}[f(w^{ (R)})]}{\eta}\] \[\qquad+\left(2+\frac{\eta\eta_{\ell}L}{2\tau^{2}}+\frac{\sqrt{1- \beta_{2}}G\eta_{\ell}}{\tau^{3}}\right)\left(\frac{R\sigma_{g}^{2}(1-s)(3d+K- 1)}{\widetilde{MK}}\right)\sum_{m\in\mathcal{M}}\sum_{c\in[C]}\left(\frac{n_{c }}{|\mathcal{D}|}-\frac{n_{m,c}\alpha_{c}}{|\mathcal{D}_{m}|}\right)^{2}\] \[\qquad+\left(1+\left(\frac{2(3d+K-1)}{\widetilde{MK}}\right)\sum _{m\in\mathcal{M}}\sum_{c\in[C]}\left(\frac{n_{c}}{|\mathcal{D}|}-\frac{n_{m,c }\alpha_{c}}{|\mathcal{D}_{m}|}\right)^{2}\right)\mathbb{E}\sum_{r=0}^{R-1}|| \nabla f(w^{(r)})||^{2}\] (72) \[\qquad+\left(\eta L+\frac{2\sqrt{1-\beta_{2}}G}{\tau}\right)\frac {\eta_{\ell}}{\tau^{2}}\left(1+\left(\frac{2(3d+K-1)}{\widetilde{MK}}\right) \sum_{m\in\mathcal{M}}\sum_{c\in[C]}\left(\frac{n_{c}}{|\mathcal{D}|}-\frac{n_ {m,c}\alpha_{c}}{|\mathcal{D}_{m}|}\right)^{2}\right)\mathbb{E}\sum_{r=0}^{R- 1}||\nabla f(w^{(r)})||^{2}\]

Considering the coefficients of \(\mathbb{E}||\nabla f(w^{(r)})||^{2}\) terms, conditioning on the following inequality,

\[\frac{R}{\sqrt{\beta_{2}}\eta_{\ell}KG+\tau}\geq\left(\frac{\eta\eta_{\ell}L} {\tau^{2}}+\frac{2\eta_{\ell}\sqrt{1-\beta_{2}}G}{\tau^{3}}+1\right)\left(1+ \frac{2(3d+K-1)}{\widetilde{MK}}\sum_{m\in[M]}\sum_{c\in[C]}\left(\frac{n_{c}} {|\mathcal{D}|}-\frac{n_{m,c}\alpha_{c}}{|\mathcal{D}_{m}|}\right)^{2}\right)\] (73)

We get the following condition on the local learning rate,

\[\eta_{\ell}=\min\left\{\mathcal{O}\left(\frac{\tau^{2}}{\sqrt{ \beta_{2}}\eta GL}\right)^{\frac{1}{2}},\mathcal{O}\left(\frac{1}{\sqrt{\beta _{2}}G}\right),\mathcal{O}\left(\frac{\tau^{3}}{\sqrt{\beta_{2}}\sqrt{1-\beta_ {2}}G^{2}}\right)^{\frac{1}{2}},\right.\] \[\left.\mathcal{O}\left(\frac{\widetilde{MK}}{\beta_{2}G(3d+K-1) \sum_{m\in[M]}\sum_{c\in[C]}\left(\frac{n_{c}}{|\mathcal{D}|}-\frac{n_{m,c} \alpha_{c}}{|\mathcal{D}_{m}|}\right)^{2}}\right)\right\}\] (74)

for the following bound on the gradient norm,

\[\min_{0\leq r\leq R}\mathbb{E}_{r}||\nabla f(w^{(r)})||^{2}\leq \frac{f(w^{(0)})-\mathbb{E}_{R}[f(w^{(R)})]}{\eta R}\] \[\qquad+\left(2+\frac{\eta\eta_{\ell}L}{2\tau^{2}}+\frac{\sqrt{1- \beta_{2}}G\eta_{\ell}}{\tau^{3}}\right)\left(\frac{\sigma_{g}^{2}(1-s)(3d+K- 1)}{\widetilde{MK}}\right)\sum_{m\in\mathcal{M}}\sum_{c\in[C]}\left(\frac{n_{c }}{|\mathcal{D}|}-\frac{n_{m,c}\alpha_{c}}{|\mathcal{D}_{m}|}\right)^{2}\] (75)

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The claims on (a) Spry consuming significantly less memory than its backpropagation-based counterparts, (b) Spry achieving higher accuracy than its zero-order counterparts, and comparable accuracy to the backpropagation-based counterparts, (c) Spry running faster in terms of wallclock time to convergence than its zero-order counterparts are proved in Section 5.2, 5.1, and 5.3 respectively. The theoretical claims are discussed and proved in Section 4 and Appendix I. The claims on direct application of Forward-mode AD in an FL setup being inefficient and inaccurate are supported by experiments shown in Appendix G. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitations and potential paths for future work are discussed in Appendix C. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The assumptions and proofs are listed in detail in Appendix I. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The details about implementation and experimental setup are given at the onset of Section 5. The pseudocode is in Appendix E, and each part of Spry is explained in Section 3. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.

In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The source code is currently available as supplementary material accompanying this work and an anonymized version is also available here. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The experimental setting is described in Section 5. Full details about datasets, their splits, and hyperparameters are given in Appendix B. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Experimental variance related to the main results in the paper, across 3 differently seeded runs is given in Appendix H.3. Guidelines: * The answer NA means that the paper does not include experiments.

* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Compute resources are discussed at the onset of Section 5. Details on memory consumption are given in Section 5.2. Time to convergence is discussed in Section 5.3. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our work does not involve human subjects. All the experiments are done on open-source datasets, which are widely used in the natural language community. Moreover, the goal of Spry is to enable crowd-sourcing compute resources to finetune LLMs, without making private data accessible to a third party. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?Answer: [Yes] Justification: The positive and negative broader impact is discussed in Appendix D. Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We have not created any new models or new data. Our main goal is to finetune the existing pre-trained language models. The datasets we have used are open-source, and free-to-use. The license information of datasets is given in Appendix B. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Licenses for all the datasets are stated in Appendix B. Our work is within the usage bounds of each of the datasets. Dataset URLs are also provided under the references section. Guidelines:* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The source code (also available as a supplemental material) of Spry comes with a detailed README.md, explaining how to run the presented experiments of our algorithm and its counterpart baselines. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our work does not involve crowdsourcing and research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?Answer: [NA]

Justification: Our work does not involve research with or on human subjects.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.