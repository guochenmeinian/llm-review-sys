# Unsupervised Anomaly Detection with Rejection

 Lorenzo Perini

DTAI lab & Leuven.AI,

KU Leuven, Belgium

lorenzo.perini@kuleuven.be &Jesse Davis,

DTAI lab & Leuven.AI,

KU Leuven, Belgium

jesse.davis@kuleuven.be

###### Abstract

Anomaly detection goal is to detect unexpected behaviours in the data. Because anomaly detection is usually an unsupervised task, traditional anomaly detectors learn a decision boundary by employing heuristics based on intuitions, which are hard to verify in practice. This introduces some uncertainty, especially close to the decision boundary, which may reduce the user trust in the detector's predictions. A way to combat this is by allowing the detector to reject examples with high uncertainty (Learning to Reject). This requires employing a confidence metric that captures the distance to the decision boundary and setting a rejection threshold to reject low-confidence predictions. However, selecting a proper metric and setting the rejection threshold without labels are challenging tasks. In this paper, we solve these challenges by setting a constant rejection threshold on the stability metric computed by ExCeeD. Our insight relies on a theoretical analysis of this metric. Moreover, setting a constant threshold results in strong guarantees: we estimate the test rejection rate, and derive a theoretical upper bound for both the rejection rate and the expected prediction cost. Experimentally, we show that our method outperforms some metric-based methods.

## 1 Introduction

Anomaly detection is the task of detecting unexpected behaviors in the data [6]. Often, these anomalies are critical adverse events such as the destruction or alteration of proprietary user data [29], water leaks in stores [50], breakdowns in gas [65] and wind [64] turbines, or failures in petroleum extraction [45]. Usually, anomalies are associated with a cost such as a monetary cost (e.g., maintenance, paying for fraudulent purchases) or a societal cost such as environmental damages (e.g., dispersion of petroleum or gas). Hence, detecting anomalies in a timely manner is an important problem.

When using an anomaly detector for decision-making, it is crucial that the user trusts the system. However, it is often hard or impossible to acquire labels for anomalies. Moreover, anomalies may not follow a pattern. Therefore anomaly detection is typically treated as an unsupervised learning problem where traditional algorithms learn a decision boundary by employing heuristics based on intuitions [22, 55, 61, 41], such as that anomalies are far away from normal examples [3]. Because these intuitions are hard to verify and may not hold in some cases, some predictions may have high uncertainty, especially for examples close to the decision boundary [32, 28]. As a result, the detector's predictions should be treated with some circumspection.

One way to increase user trust is to consider Learning to Reject [12]. In this setting, the model does not always make a prediction. Instead, it can abstain when it is at a heightened risk of making a mistake thereby improving its performance when it does offer a prediction. Abstention has the drawback that no prediction is made, which means that a person must intervene to make a decision. In the literature, two types of rejection have been identified [25]: novelty rejection allows the model to abstain when given an out-of-distribution (OOD) example, while ambiguity rejection enables abstention for a test example that is too close to the model's decision boundary. Because anomaliesoften are OOD examples, novelty rejection does not align well with our setting as the model would reject all OOD anomalies (i.e., a full class) [10; 63; 30]. On the other hand, current approaches for ambiguity rejection threshold what constitutes being too close to the decision boundary by evaluating the model's predictive performance on the examples for which it makes a prediction (i.e., accepted), and those where it abstains from making a prediction (i.e., rejected) [9; 44; 16]. Intuitively, the idea is to find a threshold where the model's predictive performance is (1) significantly lower on rejected examples than on accepted examples and (2) higher on accepted examples than on all examples (i.e., if it always makes a prediction). Unfortunately, existing learning to reject approaches that set a threshold in this manner require labeled data, which is not available in anomaly detection.

This paper fills this gap by proposing an approach to perform ambiguity rejection for anomaly detection in a completely unsupervised manner. Specifically, we make three major contributions. First, we conduct a thorough novel theoretical analysis of a stability metric for anomaly detection [49] and show that it has several previously unknown properties that are of great importance in the context of learning to reject. Namely, it captures the uncertainty close to the detector's decision boundary, and only limited number of examples get a stability value strictly lower than \(1\). Second, these enables us to design an ambiguity rejection mechanism without _any labeled data_ that offers strong guarantees which are often sought in Learning to Reject [12; 60; 7] We can derive an accurate estimate of the rejected examples proportion, as well as a theoretical upper bound that is satisfied with high probability. Moreover, given a cost function for different types of errors, we provide an estimated upper bound on the expected cost at the prediction time. Third, we evaluate our approach on an extensive set of unsupervised detectors and benchmark datasets and conclude that (1) it performs better than several adapted baselines based on other unsupervised metrics, and (2) our theoretical results hold in practice.

## 2 Preliminaries and notation

We will introduce the relevant background on anomaly detection, learning to reject, and the ExCeeD's metric that this paper builds upon.

**Anomaly Detection.** Let \(\mathcal{X}\) be a \(d\) dimensional input space and \(D=\{x_{1},\ldots,x_{n}\}\) be a training set, where each \(x_{i}\in\mathcal{X}\). The goal in anomaly detection is to train a detector \(f\colon\mathcal{X}\to\mathbb{R}\) that maps examples to a real-valued anomaly score, denoted by \(s\). In practice, it is necessary to convert these soft scores to a hard prediction, which requires setting a threshold \(\lambda\). Assuming that higher scores equate to being more anomalous, a predicted label \(\hat{y}\) can be made for an example \(x\) as follows: \(\hat{y}=1\) (anomaly) if \(s=f(x)\geq\lambda\), while \(\hat{y}=0\) (normal) if \(s=f(x)<\lambda\). We let \(\hat{Y}\) be the random variable that denotes the predicted label. Because of the absence of labels, one usually sets the threshold such that \(\gamma\times n\) scores are \(\geq\lambda\), where \(\gamma\) is the dataset's contamination factor (i.e., expected proportion of anomalies) [51; 50].

**Learning to Reject.** Learning to reject extends the output space of the model to include the symbol \(\circledotimes\), which means that the model abstains from making a prediction. This entails learning a second model \(r\) (the rejector) to determine when the model abstains. A canonical example of ambiguity rejection is when \(r\) consists of a pair [confidence \(\mathcal{M}_{s}\), rejection threshold \(\tau\)] such that an example is rejected if the detector's confidence is lower than the threshold. The model output becomes

\[\hat{y}_{\circledotimes}=\begin{cases}\hat{y}&\text{if }\mathcal{M}_{s}>\tau;\\ \circledotimes&\text{if }\mathcal{M}_{s}\leq\tau;\end{cases}\qquad\hat{y}_{ \circledotimes}\in\{0,1,\circledotimes\}.\]

A standard approach is to evaluate different values for \(\tau\) to find a balance between making too many incorrect predictions because \(\tau\) is too low (i.e., \(\hat{y}\neq y\) but \(\mathcal{M}_{s}>\tau\)) and rejecting correct predictions because \(\tau\) is too high (i.e., \(\hat{y}=y\) but \(\mathcal{M}_{s}\leq\tau\)) [9; 44; 16]. Unfortunately, in an unsupervised setting, it is impossible to evaluate the threshold because it relies on having access to labeled data.

**ExCeeD's metric.** Traditional confidence metrics (such as calibrated class probabilities) quantify how likely a prediction is to be correct, This obviously requires labels [9] which are unavailable in an unsupervised setting. Thus, one option is to move the focus towards the **concept of stability**: given a fixed test example \(x\) with anomaly score \(s\), _perturbing the training data alters the model learning, which, in turn, affects the label prediction_. Intuitively, the more stable a detector's output is for a test example, the less sensitive its predicted label is to changes in the training data. On the other hand, when \(\mathbb{P}(\hat{Y}=1|s)\approx\mathbb{P}(\hat{Y}=0|s)\approx 0.5\) the prediction for \(x\) is highly unstable, as training the detector with slightly different examples would flip its prediction for the same test score \(s\). Thus, a stability-based confidence metric \(\mathscr{M}_{s}\) can be expressed as the margin between the two classes' probabilities:

\[\mathscr{M}_{s}=|\mathbb{P}(\hat{Y}=1|s)-\mathbb{P}(\hat{Y}=0|s)|=|2\mathbb{P}( \hat{Y}=1|s)-1|,\]

where the lower \(\mathscr{M}_{s}\) the more unstable the prediction.

Recently, Perini et al. introduced ExCeeD to estimate the detector's stability \(\mathbb{P}(\hat{Y}=1|s)\). Roughly speaking, ExCeeD uses a Bayesian formulation that simulates bootstrapping the training set as a form of perturbation. Formally, it measures such stability for a test score \(s\) in two steps.

**First**, it computes the _training frequency_\(\psi_{n}=\frac{|\{i\leq n:s\leq s\}|}{n}\!\in\![0,\!1]\), i.e. the proportion of training scores lower than \(s\). This expresses how extreme the score \(s\) ranks with respect to the training scores.

**Second**, it computes the probability that the score \(s\) will be predicted as an anomaly when randomly drawing a training set of \(n\) scores from the population of scores. In practice, this is the probability that the chosen threshold \(\lambda\) will be less than or equal to the score \(s\). The stability is therefore estimated as

\[\mathbb{P}(\hat{Y}=1|s)=\sum_{i=n(1-\gamma)+1}^{n}\binom{n}{i}\left(\frac{1+n \psi_{n}}{2+n}\right)^{i}\left(\frac{n(1-\psi_{n})+1}{2+n}\right)^{n-i}.\] (1)

_Assumption._ExCeeD's Bayesian formulation requires assuming that \(Y|s\) follows a Bernoulli distribution with parameter \(p_{s}=\mathbb{P}(S\leq s)\), where \(S\) is the detector's population of scores. Note that the stability metric is a detector property and, therefore, is tied to the specific choice of the unsupervised detector \(f\).

## 3 Methodology

This paper addresses the following problem:

_Given:_ An unlabeled dataset \(D\) with contamination \(\gamma\), an unsupervised detector \(f\), a cost function \(c\);

_Do:_ Introduce a reject option to \(f\), i.e. find a pair (confidence, threshold) that minimizes the cost.

We propose an anomaly detector-agnostic approach for performing learning to reject that requires _no labels_. Our key contribution is a novel theoretical analysis of the ExCeeD confidence metric that proves that _only a limited number of examples have confidence lower than \(1-\varepsilon\)_ (Sec. 3.1). Intuitively, the detector's predictions for most examples would not be affected by slight perturbations of the training set: it is easy to identify the majority of normal examples and anomalies because they will strongly adhere to the data-driven heuristics that unsupervised anomaly detectors use. For example, using the data density as a measure of anomalousness [5] tends to identify all densely clustered normals and isolated anomalies, which constitute the majority of all examples. In contrast, only relatively few cases would be ambiguous and hence receive low confidence (e.g., small clusters of anomalies and normals at the edges of dense clusters).

Our approach is called **RejEx** (Rejecting via ExCeeD) and simply computes the stability-based confidence metric \(\mathscr{M}_{s}\) and rejects any example with confidence that falls below threshold \(\tau=1-\varepsilon\). Theoretically, this constant reject threshold provides several relevant guarantees. First, one often needs to control the proportion of rejections (namely, the _rejection rate_) to estimate the number of decisions left to the user. Thus, we propose _an estimator that only uses training instances to estimate the rejection rate at test time_. Second, because in some applications avoiding the risk of rejecting all the examples is a strict constraint, we provided _an upper bound for the rejection rate_ (Sec. 3.2). Finally, we compute a _theoretical upper bound for a given cost function_ that guarantees that using RejEx keeps the expected cost per example at test time low (Sec. 3.3).

### Setting the Rejection Threshold through a Novel Theoretical Analysis of ExCeeD

Our novel theoretical analysis proves (1) that the stability metric by ExCeeD is lower than \(1-\varepsilon\) for a limited number of examples (Theorem 3.1), and (2) that such examples with low confidence are the ones close to the decision boundary (Corollay 3.2). Thus, we propose to reject all these uncertain examples by setting a rejection threshold

\[\tau=1-\varepsilon=1-2e^{-T}\quad\text{for }T\geq 4,\]where \(2e^{-T}\) is the tolerance that excludes unlikely scenarios, and \(T\geq 4\) is required for Theorem 3.1.

We motivate our approach as follows. Given an example \(x\) with score \(s\) and the proportion of lower training scores \(\psi_{n}\), Theorem 3.1 shows that the confidence \(\mathscr{M}_{s}\) is lower than \(1-2e^{-T}\) (for \(T\geq 4\)) if \(\psi_{n}\) belongs to the interval \([t_{1},t_{2}]\). By analyzing \([t_{1},t_{2}]\), Corollary 3.2 proves that the closer an example is to the decision boundary, the lower the confidence \(\mathscr{M}_{s}\), and that a score \(s=\lambda\) (decision threshold) has confidence \(\mathscr{M}_{s}=0\).

_Remark_.: Perini et al. performed an asymptotic analysis of ExCeeD that investigates the metric's behavior when the training set's size \(n\to+\infty\). In contrast, our novel analysis is finite-sample and hence provides more practical insights, as real-world scenarios involve having a finite dataset with size \(n\in\mathbb{N}\).

**Theorem 3.1** (Analysis of ExCeeD).: _Let \(s\) be an anomaly score, and \(\psi_{n}\in[0,1]\) its training frequency. For \(T\!\geq\!4\), there exist \(t_{1}=t_{1}(n,\gamma,T)\!\in\![0,1]\), \(t_{2}=t_{2}(n,\gamma,T)\!\in\![0,1]\) such that_

\[\psi_{n}\in[t_{1},t_{2}]\implies\mathscr{M}_{s}\leq 1-2e^{-T}.\]

Proof.: See the Supplement for the formal proof. 

The interval \([t_{1},t_{2}]\) has two relevant properties. First, it becomes _narrower when increasing \(n\)_ (P1) and _larger when increasing \(T\)_ (P2). This means that collecting more training data results in smaller rejection regions while decreasing the tolerance \(\varepsilon=2e^{-T}\) has the opposite effect. Second, it is centered (not symmetrically) on \(1-\gamma\) (P3-P4), which means that _examples with anomaly scores close to the decision threshold \(\lambda\) are the ones with a low confidence score_ (P5). The next Corollary lists these properties.

**Corollary 3.2**.: _Given \(t_{1},t_{2}\) as in Theorem 3.1, the following properties hold for any \(s\), \(n\), \(\gamma\), \(T\geq 4\):_

1. \(\lim_{n\to+\infty}t_{1}=\lim_{n\to+\infty}t_{2}=1-\gamma\)_;_
2. \(t_{1}\) _and_ \(t_{2}\) _are, respectively, monotonic decreasing and increasing as functions of_ \(T\)_;_
3. _the interval always contains_ \(1-\gamma\)_, i.e._ \(t_{1}\leq 1-\gamma\leq t_{2}\)_;_
4. _for_ \(n\to\infty\)_, there exists_ \(s^{*}\) _with_ \(\psi_{n}=t^{*}\in[t_{1},t_{2}]\) _such that_ \(t^{*}\to 1-\gamma\) _and_ \(\mathscr{M}_{s}\to 0\)_._
5. \(\psi_{n}\in[t_{1},t_{2}]\)__iff__\(s\in[\lambda-u_{1},\lambda+u_{2}]\)_, where_ \(u_{1}(n,\gamma,T),u_{2}(n,\gamma,T)\) _are positive functions._

Proof sketch.: For P1, it is enough to observe that \(t_{1},t_{2}\to 1-\gamma\) for \(n\to+\infty\). For P2 and P3, the result comes from simple algebraic steps. P4 follows from the surjectivity of \(\mathscr{M}_{s}\) when \(n\to+\infty\), the monotonicity of \(\mathbb{P}(\hat{Y}=1|s)\), from P1 with the squeeze theorem. Finally, P5 follows from \(\psi_{n}\in[t_{1},t_{2}]\implies s\in\left[\psi_{n}^{-1}(t_{1}),\psi_{n}^{-1} (t_{2})\right]\), as \(\psi_{n}\) is monotonic increasing, where \(\psi_{n}^{-1}\) is the inverse-image of \(\psi_{n}\). Because for P3 \(1-\gamma\in[t_{1},t_{2}]\), it holds that \(\psi_{n}^{-1}(t_{1})\leq\psi_{n}^{-1}(1-\gamma)=\lambda\leq\psi_{n}^{-1}(t_{2})\). This implies that \(s\in[\lambda-u_{1},\lambda+u_{2}]\), where \(u_{1}=\lambda-\psi_{n}^{-1}(t_{1})\), \(u_{2}=\lambda-\psi_{n}^{-1}(t_{2})\). 

### Estimating and Bounding the Rejection Rate

It is important to have an estimate of the rejection rate, which is the proportion of examples for which the model will abstain from making a prediction. This is an important performance characteristic for differentiating among candidate models. Moreover, it is important that not all examples are rejected because such a model is useless in practice. We propose a way to estimate the rejection rate and Theorem 3.5 shows that our estimate approaches the true rate for large training sets. We strengthen our analysis and introduce an upper bound for the rejection rate, which guarantees that, with arbitrarily high probability, the rejection rate is kept lower than a constant (Theorem 3.6).

**Definition 3.3** (Rejection rate).: Given the confidence metric \(\mathscr{M}_{s}\) and the rejection threshold \(\tau\), the _rejection rate_\(\mathcal{R}=\mathbb{P}(\mathscr{M}_{s}\leq\tau)\) is the probability that a test example with score \(s\) gets rejected.

We propose the following estimator for the reject rate:

**Definition 3.4** (Rejection rate estimator).: Given anomaly scores \(s\) with training frequencies \(\psi_{n}\), let \(g\colon[0,1]\to[0,1]\) be the function such that \(\mathbb{P}(\hat{Y}=1|s)=g(\psi_{n})\) (see Eq. 1). We define the _rejection rate estimator_\(\hat{\mathcal{R}}\) as

\[\hat{\mathcal{R}}=\hat{F}_{\psi_{n}}\left(g^{-1}\left(1-e^{-T}\right)\right)- \hat{F}_{\psi_{n}}\left(g^{-1}\left(e^{-T}\right)\right)\] (2)where \(g^{-1}\) is the inverse-image through \(g\), and, for \(u\in[0,1]\), \(\hat{F}_{\psi_{n}}(u)=\frac{\|i\leq n\cdot\,\psi_{n}(s_{i})\leq u\|}{n}\) is the empirical cumulative distribution of \(\psi_{n}\).

Note that \(\hat{\mathcal{R}}\) can be computed in practice, as the \(\psi_{n}\) has a distribution that is arbitrarily close to uniform, as stated by Theorem A.1 and A.2 in the Supplement.

**Theorem 3.5** (Rejection rate estimate).: _Let \(g\) be as in Def. 3.4. Then, for high values of \(n\), \(\hat{\mathcal{R}}\approx\mathcal{R}\)._

Proof.: From the definition of rejection rate 3.3, it follows

\[\mathcal{R} =\mathbb{P}\left(\mathscr{M}_{s}\leq 1-2e^{-T}\right)=\mathbb{P} \left(\mathbb{P}(\hat{Y}=1|s)\in\left[e^{-T},1-e^{-T}\right]\right)=\mathbb{P }\left(g(\psi_{n})\in\left[e^{-T},1-e^{-T}\right]\right)\] \[=\mathbb{P}\left(\psi_{n}\in\left[g^{-1}\left(e^{-T}\right),g^{- 1}\left(1-e^{-T}\right)\right]\right)=F_{\psi_{n}}\left(g^{-1}\left(1-e^{-T} \right)\right)-F_{\psi_{n}}\left(g^{-1}\left(e^{-T}\right)\right).\]

where \(F_{\psi_{n}}(\cdot)=\mathbb{P}(\psi_{n}\leq\cdot)\) is the theoretical cumulative distribution of \(\psi_{n}\). Because the true distribution of \(\psi_{n}\) for test examples is unknown, the estimator approximates \(F_{\psi_{n}}\) using the training scores \(s_{i}\) and computes the empirical \(\hat{F}_{\psi_{n}}\). As a result,

\[\mathcal{R}\approx\hat{F}_{\psi_{n}}\left(g^{-1}\left(1-e^{-T}\right)\right)- \hat{F}_{\psi_{n}}\left(g^{-1}\left(e^{-T}\right)\right)=\hat{\mathcal{R}}.\]

**Theorem 3.6** (Rejection rate upper bound).: _Let \(s\) be an anomaly score, \(\mathscr{M}_{s}\) be its confidence value, and \(\tau=1-2e^{-T}\) be the rejection threshold. For \(n\in\mathbb{N}\), \(\gamma\in[0,0.5)\), and small \(\delta>0\), there exists a positive real function \(h(n,\gamma,T,\delta)\) such that \(\mathcal{R}\leq h(n,\gamma,T,\delta)\) with probability at least \(1-\delta\), i.e. the rejection rate is bounded._

Proof.: Theorem 3.1 states that there exists two functions \(t_{1}=t_{1}(n,\gamma,T),t_{2}=t_{2}(n,\gamma,T)\in[0,1]\) such that the confidence is lower than \(\tau\) if \(\psi_{n}\in[t_{1},t_{2}]\). Moreover, Theorems A.1 and A.2 claim that \(\psi_{n}\) has a distribution that is close to uniform with high probability (see the theorems and proofs in the Supplement). As a result, with probability at least \(1-\delta\), we find \(h(n,\gamma,T,\delta)\) as follows:

\[\mathcal{R} =\mathbb{P}(\mathscr{M}_{s}\leq 1-2e^{-T}\overbrace{\leq}^{ \mathrm{T3.1}}\mathbb{P}\left(\psi_{n}\in[t_{1},t_{2}]\right)=F_{\psi_{n}}(t_ {2})-F_{\psi_{n}}(t_{1})\] \[\overbrace{\leq}^{\mathrm{T4.2}}F_{\psi}(t_{2})-F_{\psi}(t_{1})+2 \sqrt{\frac{\ln\frac{2}{\delta}TA.1}{2n}}t_{2}(n,\gamma,T)-t_{1}(n,\gamma,T)+2 \sqrt{\frac{\ln\frac{2}{\delta}}{2n}}=h(n,\gamma,T,\delta).\]

### Upper Bounding the Expected Test Time Cost

In a learning with reject scenario, there are costs associated with three outcomes: false positives (\(c_{fp}>0\)), false negatives (\(c_{fn}>0\)), and rejection (\(c_{r}\)) because abstaining typically involves having a person intervene. Estimating an expected per example prediction cost at test time can help with model selection and give a sense of performance. Theorem 3.8 provides an upper bound on the expected per example cost when (1) using our estimated rejection rate (Theorem 3.5), and (2) setting the decision threshold \(\lambda\) as in Sec. 2.

**Definition 3.7** (Cost function).: Let \(Y\) be the true label random variable. Given the costs \(c_{fp}>0\), \(c_{fn}>0\), and \(c_{r}\), the **cost function** is a function \(c\colon\{0,1\}\times\{0,1,\emptyset\}\to\mathbb{R}\) such that

\[c(Y,\hat{Y})\!=\!c_{r}\mathbb{P}(\hat{Y}\!\!=\!\emptyset)+c_{fp}\mathbb{P}(\hat {Y}\!\!=\!1|Y\!\!=\!0)+c_{fn}\mathbb{P}(\hat{Y}\!\!=\!0|Y\!\!=\!1)\]

Note that defining a specific cost function requires domain knowledge. Following the learning to reject literature, we set an additive cost function. Moreover, the rejection cost needs to satisfy the inequality \(c_{r}\leq\min\{(1-\gamma)c_{fp},\gamma c_{fn}\}\). This avoids the possibility of predicting always anomaly for an expected cost of \((1-\gamma)c_{fp}\), or always normal with an expected cost of \(\gamma c_{fn}\)[52].

**Theorem 3.8**.: _Let \(c\) be a cost function as defined in Def. 3.7, and \(g\) be as in Def. 3.4. Given a (test) example \(x\) with score \(s\), the expected example-wise cost is bounded by_

\[\mathbb{E}_{x}[c]\leq\min\{\gamma,A\}c_{fn}+(1-B)c_{fp}+(B-A)c_{r},\] (3)

_where \(A=\hat{F}_{\psi_{n}}(g^{-1}\left(e^{-T}\right))\) and \(B=\hat{F}_{\psi_{n}}(g^{-1}\left(1-e^{-T}\right))\) are as in Theorem 3.5._Proof.: We indicate the true label random variable as \(Y\), and the non-rejected false positives and false negatives as, respectively,

\[FP=\mathbb{P}\left(\hat{Y}=1|Y=0,\mathscr{M}_{s}>1-2e^{-T}\right)\quad FN= \mathbb{P}\left(\hat{Y}=0|Y=1,\mathscr{M}_{s}>1-2e^{-T}\right)\]

Using Theorem 3.5 results in

\[\mathbb{E}_{x}[c]=\mathbb{E}_{x}[c_{fn}FN+c_{fp}FP+c_{r}\mathcal{R}]=\mathbb{E }_{x}[c_{fn}FN]+\mathbb{E}_{x}[c_{fp}FP]+c_{r}(B-A)\]

where \(A=\hat{F}_{\psi_{n}}(g^{-1}\left(e^{-T}\right))\), \(B=\hat{F}_{\psi_{n}}(g^{-1}\left(1-e^{-T}\right))\) come from Theorem 3.5. Now we observe that setting a decision threshold \(\lambda\) such that \(n\times\gamma\) scores are higher implies that, on expectation, the detector predicts a proportion of positives equal to \(\gamma=\mathbb{P}(Y=1)\). Moreover, for \(\varepsilon=2e^{-T}\),

* \(FP\leq\mathbb{P}\left(\hat{Y}=1|\mathscr{M}_{s}>1-\varepsilon\right)=1-B\) as false positives must be less than total accepted positive predictions;
* \(FN\leq\gamma\) and \(FN\leq\mathbb{P}\left(\hat{Y}=0|\mathscr{M}_{s}>1-\varepsilon\right)=A\), as you cannot have more false negatives than positives (\(\gamma\)), nor than accepted negative predictions (\(A\)).

From these observations, we conclude that \(\mathbb{E}_{x}[c]\leq\min\{\gamma,A\}c_{fn}+(1-B)c_{fp}+(B-A)c_{r}\). 

## 4 Related work

There is no research on learning to reject in unsupervised anomaly detection. However, **three** main research lines are connected to this work.

1) Supervised methods.If some labels are available, one can use traditional supervised approaches to add the reject option into the detector [11; 38]. Commonly, labels can be used to find the optimal rejection threshold in two ways: 1) by trading off the model performance (e.g., AUC) on the accepted examples with its rejection rate [24; 1], or 2) by minimizing a cost function [46; 7], a risk function [18; 27], or an error function [35; 33]. Alternatively, one can include the reject option in the model and directly optimize it during the learning phase [60; 12; 31].

2) Self-Supervised methods.If labels are not available, one can leverage self-supervised approaches to generate pseudo-labels in order to apply traditional supervised learning to reject methods [26; 59; 19; 37]. For example, one can employ any unsupervised anomaly detector to assign training labels, fit a (semi-)supervised detector (such as DeepSAD [57] or Repen[47]) on the pseudo labels, compute a confidence metric [14], and find the optimal rejection threshold by minimizing the cost function treating the pseudo-labels as the ground truth.

3) Optimizing unsupervised metrics.There exist several unsupervised metrics (i.e., they can be computed without labels) for quantifying detector quality [43]. Because they do not need labels, one can find the rejection threshold by maximizing the margin between the detector's quality (computed using such metric) on the accepted and on the rejected examples [54]. This allows us to obtain a model that performs well on the accepted examples and poorly on the rejected ones, which is exactly the same intuition that underlies the supervised approaches. Some examples of existing unsupervised metrics (see [43]) are the following. Em and Mv[20] quantify the clusterness of inlier scores, where more compact scores indicate better models. Stability[48] measures the robustness of anomaly detectors' predictions by looking at how consistently they rank examples by anomalousness. Udr[15] is a model-selection metric that selects the model with a hyperparameter setting that yields consistent results across various seeds, which can be used to set the rejection threshold through the analogy [hyperparameter, seed] and [rejection threshold, detectors]. Finally, Ens[56; 67] measures the detector trustworthiness as the ranking-based similarity (e.g., correlation) of a detector's output to the "pseudo ground truth", computed via aggregating the output of an ensemble of detectors, which allows one to set the rejection threshold that maximizes the correlation between the detector's and the ensemble's outputs.

Experiments

We experimentally address the following research questions:

1. How does ReJEx's cost compare to the baselines?
2. How does varying the cost function affect the results?
3. How does ReJEx's CPU time compare to the baselines?
4. Do the theoretical results hold in practice?
5. Would ReJEx's performance significantly improve if it had access to training labels?

### Experimental Setup

Methods.We compare **ReJEx1** against \(7\) baselines for setting the rejection threshold. These can be divided into three categories: no rejection, self-supervised, and unsupervised metric based.

Footnote 1: Code available at: https://github.com/Lorenzo-Perini/RejEx.

We use one method **NoReject** that always makes predictions and never rejects (no reject option).

We consider one self-supervised approach **SS-Repen**[47]. This uses (any) unsupervised detector to obtain pseudo labels for the training set. It then sets the rejection threshold as follows: 1) it creates a held-out validation set (\(20\%\)), 2) it fits Repen, a state-of-the-art (semi-)supervised anomaly detector on the training set with the pseudo labels, 3) it computes on the validation set the confidence values as the margin between Repen's predicted class probabilities \(|\mathbb{P}(Y=1|s)-\mathbb{P}(Y=0|s)|\), 4) it finds the optimal threshold \(\tau\) by minimizing the total cost obtained on the validation set.

We consider \(5\) approaches that employ an existing unsupervised metric to set the rejection threshold and hence do not require having access to labels. **MV**[20], **EM**[20], and **Stability**[48] are unsupervised metric-based methods based on stand-alone internal evaluations that use a single anomaly detector to measure its quality, **Udr[15]** and **Ens**[56] are unsupervised consensus-based metrics that an ensemble of detectors (all 12 considered in our experiments) to measure a detector's quality.2 We apply each of these \(5\) baselines as follows. 1) We apply the unsupervised detector to assign an anomaly score to each train set example. 2) We convert these scores into class probabilities using [34]. 3) We compute the confidence scores on the training set as difference between these probabilities: \(|\mathbb{P}(Y=1|s)-\mathbb{P}(Y=0|s)|\). 4) We evaluate possible thresholds on this confidence by computing the considered unsupervised metric on the accepted and on the rejected examples and select the threshold that maximizes the difference in the metric's value on these two sets of examples. This aligns with the common learning to reject criteria for picking a threshold [9; 54] such that the model performs well on the accepted examples and poorly on the rejected ones.

Footnote 2: Sec. 4 describes these approaches.

Data.We carry out our study on \(34\) publicly available benchmark datasets, widely used in the literature [23]. These datasets cover many application domains, including healthcare (e.g., disease diagnosis), audio and language processing (e.g., speech recognition), image processing (e.g., object identification), and finance (e.g., fraud detection). To limit the computational time, we randomly sub-sample \(20,000\) examples from all large datasets. Table 3 in the Supplement provides further details.

Anomaly Detectors and Hyperparameters.We set our tolerance \(\varepsilon=2e^{-T}\) with \(T=32\). Note that the exponential smooths out the effect of \(T\geq 4\), which makes setting a different \(T\) have little impact. We use a set of \(12\) unsupervised anomaly detectors implemented in PyOD[66] with default hyperparameters [62] because the unsupervised setting does not allow us to tune them: Knn[3], IForest[42], Lof[5], Ocsvm[58], Ae[8], Hbos[21], Loda[53], Copod[39], Gmm[2], Ecod[40], Kde[36], Inne[4]. We set all the baselines' rejection threshold via Bayesian Optimization with \(50\) calls [17].

Setup.For each [dataset, detector] pair, we proceed as follows: (1) we split the dataset into training and test sets (80-20) using \(5\) fold cross-validation; (2) we use the detector to assign the anomaly scores on the training set; (3) we use either ReJEx or a baseline to set the rejection threshold;(4) we measure the total cost on the test set using the given cost function. We carry out a total of \(34\times 12\times 5=2040\) experiments. All experiments were run on an Intel(R) Xeon(R) Silver 4214 CPU.

### Experimental Results

Q1: RejEx against the baselines.Figure 1 shows the comparison between our method and the baselines, grouped by detector, when setting the costs \(c_{fp}=c_{fn}=1\) and \(c_{r}=\gamma\) (see the Supplement for further details). RejEx achieves the lowest (best) cost per example for \(9\) out of \(12\) detectors (left-hand side) and similar values to SS-Repen when using Loda, Lof and Kde. Averaging over the detectors, RejEx reduces the relative cost by more than \(5\%\) vs SS-Repen, \(11\%\) vs Ens, \(13\%\) vs Mv and Udr, \(17\%\) vs Em, \(19\%\) vs NoReject. Table 4 (Supplement) shows a detailed breakdown.

For each experiment, we rank all the methods from \(1\) to \(8\), where position \(1\) indicates the lowest (best) cost. The right-hand side of Figure 1 shows that RejEx always obtains the lowest average ranking. We run a statistical analysis separately for each detector: the Friedman test rejects the null-hypothesis that all methods perform similarly (p-value \(<e^{-16}\)) for all the detectors. The ranking-based post-hoc Bonferroni-Dunn statistical test [13] with \(\alpha=0.05\) finds that RejEx is significantly better than the baselines for \(6\) detectors (Inne, IForest, Hbos, Knn, Econd, Ocsvm).

Q2. Varying the costs \(c_{fp}\), \(c_{fn}\), \(c_{r}\).The three costs \(c_{fp}\), \(c_{fn}\), and \(c_{r}\) are usually set based on domain knowledge: whether to penalize the false positives or the false negatives more depends on the application domain. Moreover, the rejection cost needs to satisfy the constraint

Figure 1: Average cost per example (left) and rank (right) aggregated per detector (x-axis) over all the datasets. Our method obtains the lowest (best) cost for \(9\) out of \(12\) detectors and it always has the lowest (best) ranking position for \(c_{fp}=c_{fn}=1\), \(c_{r}=\gamma\).

Figure 2: Average cost per example aggregated by detector over the \(34\) datasets when varying the three costs on three representative cases: (left) false positives are penalized more, (center) false negatives are penalized more, (right) rejection has a lower cost than FPs and FNs.

\(\gamma\))\(c_{fp}\), \(\gamma\)\(c_{fn}\)) [52]. Therefore, we study their impact on three representative cases: (case 1) high false positive cost (\(c_{fp}=10\), \(c_{fn}=1\), \(c_{r}=\min\{10(1-\gamma),\gamma\}\), (case 2) high false negative cost (\(c_{fp}=1\), \(c_{fn}=10\), \(c_{r}=\min\{(1-\gamma),10\gamma\}\), and (case 3) same cost for both mispredictions but low rejection cost (\(c_{fp}=5\), \(c_{fn}=5\), \(c_{r}=\gamma\)). Note that scaling all the costs has no effect on the relative comparison between the methods, so the last case is equivalent to \(c_{fp}=1\), \(c_{fn}=1\), and \(c_{r}=\gamma/5\).

Figure 2 shows results for the three scenarios. Compared to the unsupervised metric-based methods, the left plot shows that our method is clearly the best for high false positives cost: for \(11\) out of \(12\) detectors, ReJEx obtains both the lowest (or similar for Gmm) average cost and the lowest average ranking position. This indicates that using ReJEx is suitable when false alarms are expensive. Similarly, the right plot illustrates that ReJEx outperforms all the baselines for all the detectors when the rejection cost is low (w.r.t. the false positive and false negative costs). Even when the false negative cost is high (central plot), ReJEx obtains the lowest average cost for \(11\) detectors and has always the lowest average rank per detector. See the Supplement (Table 6 and 7) for more details.

Q3. Comparing the CPU time.Table 1 reports CPU time in milliseconds per training example aggregated over the \(34\) datasets needed for each method to set the rejection threshold on three unsupervised anomaly detectors (IForest, Hbos, Copod). NoReject has CPU time equal to \(0\) because it does not use any reject option. ReJEx takes just a little more time than NoReject because computing ExCeeD has linear time while setting a constant threshold has constant time. In contrast, all other methods take \(1000\times\) longer because they evaluate multiple thresholds. For some of these (e.g., Stability), this involves an expensive internal procedure.

Q4. Checking on the theoretical results.Section 3 introduces three theoretical results: the rejection rate estimate (Theorem 3.5), and the upper bound for the rejection rate (Theorem 3.6) and for the cost (Theorem 3.8). We run experiments to verify whether they hold in practice. Figure 3 shows the results aggregated over the detectors. The left-hand side confirms that the prediction cost per example (blue circle) is always \(\leq\) than the upper bound (black line). Note that the upper bound is sufficiently strict, as in some cases it equals the empirical cost (e.g., Census, Wilt, Optdigits).

\begin{table}
\begin{tabular}{l|c c c c c c c c} \hline \hline  & & \multicolumn{6}{c}{CPU time in ms (mean \(\pm\) std.)} \\ Detector & NoReject & **ReJEx** & SS-Repen & My & Em & Udr & Ens & Stability \\ \hline IForest & 0.0\(\pm\)0.0 & **0.06\(\pm\)0.22** & 90\(\pm\)68 & 89\(\pm\)128 & 155\(\pm\)161 & 120\(\pm\)132 & 122\(\pm\)135 & 916\(\pm\)900 \\ Hbos & 0.0\(\pm\)0.0 & **0.13\(\pm\)0.93** & 89\(\pm\)53 & 39\(\pm\)81 & 80\(\pm\)129 & 200\(\pm\)338 & 210\(\pm\)358 & 142\(\pm\)242 \\ Copod & 0.0\(\pm\)0.0 & **0.04\(\pm\)0.04** & 84\(\pm\)53 & 21\(\pm\)28 & 81\(\pm\)60 & 119\(\pm\)131 & 123\(\pm\)138 & 140\(\pm\)248 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Average CPU time (in ms) per training example (\(\pm\) std) to set the rejection threshold aggregated over all the datasets when using IForest, Hbos, and Copod as unsupervised anomaly detector. ReJEx has a lower time than all the methods but NoReject, which uses no reject option.

Figure 3: Average cost per example (left) and average rejection rate (right) at test time aggregated by dataset over the \(12\) detectors. In both plots, the empirical value (circle) is always lower than the predicted upper bound (continuous black line), which makes it consistent with the theory. On the right, the expected rejection rates (stars) are almost identical to the empirical values.

The right-hand side shows that our rejection rate estimate (orange star) is almost identical to the empirical rejection rate (orange circle) for most of the datasets, especially the large ones. On the other hand, small datasets have the largest gap, e.g., Wine (\(n=129\)), Lymphography (\(n=148\)), WPBC (\(n=198\)), Vertebral (\(n=240\)). Finally, the empirical rejection rate is always lower than the theoretical upper bound (black line), which we compute by using the empirical frequencies \(\psi_{n}\).

Q5. Impact of training labels on RejEx.We simulate having access to the training labels and include an extra baseline: Oracle uses ExCeeD as a confidence metric and sets the (optimal) rejection threshold by minimizing the cost function using the training labels. Table 2 shows the average cost and rejection rates at test time obtained by the two methods. Overall, RejEx obtains an average cost that is only \(0.6\%\) higher than Oracle's cost. On a per-detector basis, RejEx obtains a \(2.5\%\) higher cost in the worst case (with Loda), while getting only a \(0.08\%\) increase in the best case (with Kde). Comparing the rejection rates, RejEx rejects on average only \(\approx 1.5\) percentage points more examples than Oracle (\(12.9\%\) vs \(11.4\%\)). The supplement provides further details.

## 6 Conclusion and Limitations

This paper addressed learning to reject in the context of unsupervised anomaly detection. The key challenge was how to set the rejection threshold without access to labels which are required by all existing approaches We proposed an approach RejEx that exploits our novel theoretical analysis of the ExCeeD confidence metric. Our new analysis shows that it is possible to set a constant rejection threshold and that doing so offers strong theoretical guarantees. First, we can estimate the proportion of rejected test examples and provide an upper bound for our estimate. Second, we can provide a theoretical upper bound on the expected test-time prediction cost per example. Experimentally, we compared RejEx against several (unsupervised) metric-based methods and showed that, for the majority of anomaly detectors, it obtained lower (better) cost. Moreover, we proved that our theoretical results hold in practice and that our rejection rate estimate is almost identical to the true value in the majority of cases.

**Limitations.** Because RejEx does not rely on labels, it can only give a coarse-grained view of performance. For example, in many applications anomalies will have varying costs (i.e., there are instance-specific costs) which we cannot account for. Moreover, RejEx has a strictly positive rejection rate, which may increase the cost of a highly accurate detector. However, this happens only in \(\approx 5\%\) of our experiments.

\begin{table}
\begin{tabular}{l|c|c||c|c} \hline \hline  & \multicolumn{2}{c||}{**Cost per example** (mean \(\pm\) std.)} & \multicolumn{2}{c}{**Rejection Rate** (mean \(\pm\) std.)} \\ Detector & RejEx & Oracle & RejEx & Oracle \\ \hline Ae & 0.126 \(\pm\) 0.139 & 0.126 \(\pm\) 0.139 & 0.131 \(\pm\) 0.132 & 0.118 \(\pm\) 0.125 \\ Copod & 0.123 \(\pm\) 0.140 & 0.121 \(\pm\) 0.140 & 0.123 \(\pm\) 0.131 & 0.101 \(\pm\) 0.114 \\ Ecod & 0.119 \(\pm\) 0.138 & 0.118 \(\pm\) 0.138 & 0.125 \(\pm\) 0.130 & 0.107 \(\pm\) 0.114 \\ Gmm & 0.123 \(\pm\) 0.135 & 0.122 \(\pm\) 0.134 & 0.139 \(\pm\) 0.143 & 0.132 \(\pm\) 0.136 \\ Hbos & 0.118 \(\pm\) 0.129 & 0.118 \(\pm\) 0.129 & 0.139 \(\pm\) 0.148 & 0.114 \(\pm\) 0.128 \\ IForest & 0.118 \(\pm\) 0.129 & 0.118 \(\pm\) 0.128 & 0.127 \(\pm\) 0.131 & 0.118 \(\pm\) 0.130 \\ Inne & 0.115 \(\pm\) 0.129 & 0.115 \(\pm\) 0.128 & 0.132 \(\pm\) 0.132 & 0.122 \(\pm\) 0.125 \\ Kde & 0.129 \(\pm\) 0.140 & 0.129 \(\pm\) 0.139 & 0.121 \(\pm\) 0.129 & 0.105 \(\pm\) 0.120 \\ Knn & 0.119 \(\pm\) 0.123 & 0.118 \(\pm\) 0.123 & 0.127 \(\pm\) 0.129 & 0.112 \(\pm\) 0.117 \\ Loda & 0.125 \(\pm\) 0.133 & 0.122 \(\pm\) 0.130 & 0.126 \(\pm\) 0.124 & 0.110 \(\pm\) 0.114 \\ Lof & 0.126 \(\pm\) 0.131 & 0.125 \(\pm\) 0.131 & 0.129 \(\pm\) 0.126 & 0.118 \(\pm\) 0.115 \\ Occym & 0.120 \(\pm\) 0.131 & 0.120 \(\pm\) 0.131 & 0.126 \(\pm\) 0.128 & 0.107 \(\pm\) 0.115 \\ \hline Avg. & 0.122 \(\pm\) 0.133 & 0.121 \(\pm\) 0.133 & 0.129 \(\pm\) 0.132 & 0.114 \(\pm\) 0.121 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Mean \(\pm\) std. for the **cost per example** (on the left) and the **rejection rate** (on the right) at test time on a per detector basis and aggregated over the datasets.

## Acknowledgements

This research is supported by an FB Ph.D. fellowship by FWO-Vlaanderen (grant 1166222N) [LP], the Flemish Government under the "Onderzoeksprogramma Artificiele Intelligentie (AI) Vlaanderen" programme [LP,JD], and KUL Research Fund iBOF/21/075 [JD].

## References

* [1] M. R. Abbas, M. S. A. Nadeem, A. Shaheen, A. A. Alshdadi, R. Alharbey, S.-O. Shim, and W. Aziz. Accuracy rejection normalized-cost curves (arnccs): A novel 3-dimensional framework for robust classification. _IEEE Access_, 7:160125-160143, 2019.
* [2] C. C. Aggarwal. An introduction to outlier analysis. In _Outlier analysis_, pages 1-34. Springer, 2017.
* [3] F. Angiulli and C. Pizzuti. Fast outlier detection in high dimensional spaces. In _European conference on principles of data mining and knowledge discovery_, pages 15-27. Springer, 2002.
* [4] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells. Isolation-based anomaly detection using nearest-neighbor ensembles. _Computational Intelligence_, 34(4):968-998, 2018.
* [5] M. M. Breunig, H.-P. Kriegel, R. T. Ng, and J. Sander. Lof: identifying density-based local outliers. In _Proceedings of the 2000 ACM SIGMOD international conference on Management of data_, pages 93-104, 2000.
* [6] V. Chandola, A. Banerjee, and V. Kumar. Anomaly detection: A survey. _ACM computing surveys (CSUR)_, 41(3):1-58, 2009.
* [7] N. Charoenphakdee, Z. Cui, Y. Zhang, and M. Sugiyama. Classification with rejection based on cost-sensitive classification. In _International Conference on Machine Learning_, pages 1507-1517. PMLR, 2021.
* [8] Z. Chen, C. K. Yeo, B. S. Lee, and C. T. Lau. Autoencoder-based network anomaly detection. In _2018 Wireless telecommunications symposium (WTS)_, pages 1-5. IEEE, 2018.
* [9] C. Chow. On optimum recognition error and reject tradeoff. _IEEE Transactions on information theory_, 16(1):41-46, 1970.
* [10] L. Coenen, A. K. Abdullah, and T. Guns. Probability of default estimation, with a reject option. In _2020 IEEE 7th International Conference on Data Science and Advanced Analytics (DSAA)_, pages 439-448. IEEE, 2020.
* [11] D. Conte, P. Foggia, G. Percannella, A. Saggese, and M. Vento. An ensemble of rejecting classifiers for anomaly detection of audio events. In _2012 IEEE Ninth International Conference on Advanced Video and Signal-Based Surveillance_, pages 76-81. IEEE, 2012.
* [12] C. Cortes, G. DeSalvo, and M. Mohri. Learning with rejection. In _International Conference on Algorithmic Learning Theory_. Springer, 2016.
* [13] J. Demsar. Statistical comparisons of classifiers over multiple data sets. _The Journal of Machine learning research_, 7:1-30, 2006.
* [14] C. Denis and M. Hebiri. Consistency of plug-in confidence sets for classification in semi-supervised learning. _Journal of Nonparametric Statistics_, 32(1):42-72, 2020.
* [15] S. Duan, L. Matthey, A. Saraiva, N. Watters, C. P. Burgess, A. Lerchner, and I. Higgins. Unsupervised model selection for variational disentangled representation learning. _arXiv preprint arXiv:1905.12614_, 2019.
* [16] R. El-Yaniv et al. On the foundations of noise-free selective classification. _Journal of Machine Learning Research_, 11(5), 2010.
* [17] P. I. Frazier. A tutorial on bayesian optimization. _arXiv preprint arXiv:1807.02811_, 2018.

* [18] Y. Geifman and R. El-Yaniv. Selectivenet: A deep neural network with an integrated reject option. In _International conference on machine learning_, pages 2151-2159. PMLR, 2019.
* [19] M.-I. Georgescu, A. Barbalau, R. T. Ionescu, F. S. Khan, M. Popescu, and M. Shah. Anomaly detection in video via self-supervised and multi-task learning. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 12742-12752, 2021.
* [20] N. Goix. How to evaluate the quality of unsupervised anomaly detection algorithms? _arXiv preprint arXiv:1607.01152_, 2016.
* [21] M. Goldstein and A. Dengel. Histogram-based outlier score (hbos): A fast unsupervised anomaly detection algorithm. _KI-2012: poster and demo track_, 9, 2012.
* [22] D. Guthrie, L. Guthrie, B. Allison, and Y. Wilks. Unsupervised anomaly detection. In _IJCAI_, pages 1624-1628, 2007.
* [23] S. Han, X. Hu, H. Huang, M. Jiang, and Y. Zhao. Adbench: Anomaly detection benchmark. In _Neural Information Processing Systems (NeurIPS)_, 2022.
* [24] B. Hanczar. Performance visualization spaces for classification with rejection option. _Pattern Recognition_, 96:106984, 2019.
* [25] K. Hendrickx, L. Perini, D. Van der Plas, W. Meert, and J. Davis. Machine learning with a reject option: A survey. _arXiv preprint arXiv:2107.11277_, 2021.
* [26] H. Hojjati, T. K. K. Ho, and N. Armanfard. Self-supervised anomaly detection: A survey and outlook. _arXiv preprint arXiv:2205.05173_, 2022.
* [27] L. Huang, C. Zhang, and H. Zhang. Self-adaptive training: beyond empirical risk minimization. _Advances in neural information processing systems_, 33:19365-19376, 2020.
* [28] E. Hullermeier and W. Waegeman. Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods. _Machine Learning_, 110(3):457-506, 2021.
* [29] H. Jmila and M. I. Khedher. Adversarial machine learning for network intrusion detection: A comparative study. _Computer Networks_, page 109073, 2022.
* [30] M. U. K. Khan, H.-S. Park, and C.-M. Kyung. Rejecting motion outliers for efficient crowd anomaly detection. _IEEE Transactions on Information Forensics and Security_, 14(2):541-556, 2018.
* [31] M. A. Kocak, D. Ramirez, E. Erkip, and D. E. Shasha. Safepredict: A meta-algorithm for machine learning that uses refusals to guarantee correctness. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 43(2):663-678, 2019.
* [32] B. Kompa, J. Snoek, and A. L. Beam. Second opinion needed: communicating uncertainty in medical machine learning. _NPJ Digital Medicine_, 4(1):4, 2021.
* [33] L. Korycki, A. Cano, and B. Krawczyk. Active learning with abstaining classifiers for imbalanced drifting data streams. In _2019 IEEE international conference on big data (big data)_, pages 2334-2343. IEEE, 2019.
* [34] H.-P. Kriegel, P. Kroger, E. Schubert, and A. Zimek. Interpreting and unifying outlier scores. In _Proceedings of the 2011 SIAM International Conference on Data Mining_, pages 13-24. SIAM, 2011.
* [35] S. Laroui, X. Descombes, A. Vernay, F. Villiers, F. Villalba, and E. Debreuve. How to define a rejection class based on model learning? In _2020 25th International Conference on Pattern Recognition (ICPR)_, pages 569-576. IEEE, 2021.
* [36] L. J. Latecki, A. Lazarevic, and D. Pokrajac. Outlier detection with kernel density functions. In _International Workshop on Machine Learning and Data Mining in Pattern Recognition_, pages 61-75. Springer, 2007.

* [37] C.-L. Li, K. Sohn, J. Yoon, and T. Pfister. Cutpaste: Self-supervised learning for anomaly detection and localization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9664-9674, 2021.
* [38] S. Li, X. Ji, E. Dobriban, O. Sokolsky, and I. Lee. Pac-wrap: Semi-supervised pac anomaly detection. _arXiv preprint arXiv:2205.10798_, 2022.
* [39] Z. Li, Y. Zhao, N. Botta, C. Ionescu, and X. Hu. Copod: copula-based outlier detection. In _2020 IEEE International Conference on Data Mining (ICDM)_, pages 1118-1123. IEEE, 2020.
* [40] Z. Li, Y. Zhao, X. Hu, N. Botta, C. Ionescu, and G. Chen. Ecod: Unsupervised outlier detection using empirical cumulative distribution functions. _IEEE Transactions on Knowledge and Data Engineering_, 2022.
* [41] O. Lindenbaum, Y. Aizenbud, and Y. Kluger. Probabilistic robust autoencoders for outlier detection. _arXiv preprint arXiv:2110.00494_, 2021.
* [42] F. T. Liu, K. M. Ting, and Z.-H. Zhou. Isolation-based anomaly detection. _ACM Transactions on Knowledge Discovery from Data (TKDD)_, 6(1):1-39, 2012.
* [43] M. Q. Ma, Y. Zhao, X. Zhang, and L. Akoglu. A large-scale study on unsupervised outlier model selection: Do internal strategies suffice? _arXiv preprint arXiv:2104.01422_, 2021.
* [44] C. Marrocco, M. Molinara, and F. Tortorella. An empirical comparison of ideal and empirical roc-based reject rules. In _International Workshop on Machine Learning and Data Mining in Pattern Recognition_, pages 47-60. Springer, 2007.
* [45] L. Marti, N. Sanchez-Pi, J. M. Molina, and A. C. B. Garcia. Anomaly detection based on sensor data in petroleum industry applications. _Sensors_, 2015.
* [46] V.-L. Nguyen and E. Hullermeier. Reliable multilabel classification: Prediction with partial abstention. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 5264-5271, 2020.
* [47] G. Pang, L. Cao, L. Chen, and H. Liu. Learning representations of ultrahigh-dimensional data for random distance-based outlier detection. In _Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining_, pages 2041-2050, 2018.
* [48] L. Perini, C. Galvin, and V. Vercruyssen. A ranking stability measure for quantifying the robustness of anomaly detection methods. In _Joint European Conference on Machine Learning and Knowledge Discovery in Databases_, pages 397-408. Springer, 2020.
* [49] L. Perini, V. Vercruyssen, and J. Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In _Joint European Conference on Machine Learning and Knowledge Discovery in Databases_, pages 227-243. Springer, 2020.
* [50] L. Perini, V. Vercruyssen, and J. Davis. Transferring the contamination factor between anomaly detection domains by shape similarity. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 4128-4136, 2022.
* [51] L. Perini, P.-C. Burkner, and A. Klami. Estimating the contamination factor's distribution in unsupervised anomaly detection. In _International Conference on Machine Learning_, pages 27668-27679. PMLR, 2023.
* [52] L. Perini, D. Giannuzzi, and J. Davis. How to allocate your label budget? choosing between active learning and learning to reject in anomaly detection. _arXiv preprint arXiv:2301.02909_, 2023.
* [53] T. Pevny. Loda: Lightweight on-line detector of anomalies. _Machine Learning_, 102(2):275-304, 2016.
* [54] A. Pugnana and S. Ruggieri. Auc-based selective classification. In _International Conference on Artificial Intelligence and Statistics_, pages 2494-2514. PMLR, 2023.

* Qiu et al. [2021] C. Qiu, T. Pfrommer, M. Kloft, S. Mandt, and M. Rudolph. Neural transformation learning for deep anomaly detection beyond images. In _International Conference on Machine Learning_, pages 8703-8714. PMLR, 2021.
* Rayana and Akoglu [2016] S. Rayana and L. Akoglu. Less is more: Building selective anomaly ensembles. _Acm transactions on knowledge discovery from data (tkdd)_, 10(4):1-33, 2016.
* Ruff et al. [2019] L. Ruff, R. A. Vandermeulen, N. Gornitz, A. Binder, E. Muller, K.-R. Muller, and M. Kloft. Deep semi-supervised anomaly detection. _arXiv preprint arXiv:1906.02694_, 2019.
* Scholkopf et al. [2001] B. Scholkopf, J. C. Platt, J. Shawe-Taylor, A. J. Smola, and R. C. Williamson. Estimating the support of a high-dimensional distribution. _Neural computation_, 13(7):1443-1471, 2001.
* Sehwag et al. [2021] V. Sehwag, M. Chiang, and P. Mittal. Ssd: A unified framework for self-supervised outlier detection. _arXiv preprint arXiv:2103.12051_, 2021.
* Shekhar et al. [2019] S. Shekhar, M. Ghavamzadeh, and T. Javidi. Binary classification with bounded abstention rate. _arXiv preprint arXiv:1905.09561_, 2019.
* Shenkar and Wolf [2021] T. Shenkar and L. Wolf. Anomaly detection for tabular data with internal contrastive learning. In _International Conference on Learning Representations_, 2021.
* Soenen et al. [2021] J. Soenen, E. Van Wolputte, L. Perini, V. Vercruyssen, W. Meert, J. Davis, and H. Blockeel. The effect of hyperparameter tuning on the comparative evaluation of unsupervised anomaly detection methods. In _Proceedings of the KDD_, volume 21, pages 1-9, 2021.
* Van der Plas et al. [2021] D. Van der Plas, W. Meert, J. Verbraecken, and J. Davis. A reject option for automated sleep stage scoring. In _Workshop on Interpretable ML in Healthcare at International Conference on Machine Learning (ICML)_, 2021.
* Xiang et al. [2022] L. Xiang, X. Yang, A. Hu, H. Su, and P. Wang. Condition monitoring and anomaly detection of wind turbine based on cascaded and bidirectional deep learning networks. _Applied Energy_, 305:117925, 2022.
* Zhao et al. [2016] N. Zhao, X. Wen, and S. Li. A review on gas turbine anomaly detection for implementing health management. _Turbo Expo: Power for Land, Sea, and Air_, 2016.
* Zhao et al. [2019] Y. Zhao, Z. Nasrullah, and Z. Li. Pyod: A python toolbox for scalable outlier detection. _Journal of Machine Learning Research_, 20(96):1-7, 2019. URL http://jmlr.org/papers/v20/19-011.html.
* Zimek et al. [2014] A. Zimek, R. J. Campello, and J. Sander. Ensembles for unsupervised outlier detection: challenges and research questions a position paper. _Acm Sigkdd Explorations Newsletter_, 15(1):11-22, 2014.

## Supplement

In this supplementary material we (1) provide additional theorems and proofs for Section 3, and (2) further describe the experimental results.

## Appendix A Theoretical Results

Firstly, we provide the proof for Theorem 3.1.

**Theorem 3.1** (Analysis of ExCeeD) Let \(s\) be an anomaly score, and \(\psi_{n}\in[0,1]\) the proportion of training scores \(\leq s\). For \(T\!\geq\!4\), there exist \(t_{1}=t_{1}(n,\gamma,T)\!\in\![0,1]\), \(t_{2}=t_{2}(n,\gamma,T)\!\in\![0,1]\) such that

\[\psi_{n}\in[t_{1},t_{2}]\implies\mathscr{M}_{s}\leq 1-2e^{-T}.\]

Proof.: We split this proof into two parts: we show that the reverse inequalities, i.e. that **(a)** if \(\psi_{n}\leq t_{1}\), then \(\mathscr{M}_{s}\geq 1-2e^{-T}\), and **(b)** if \(\psi_{n}\geq t_{2}\), then \(\mathscr{M}_{s}\geq 1-2e^{-T}\), hold and prove the final statement because \(\mathbb{P}(\hat{Y}=1|s)\) is monotonic increasing on \(s\).

**(a)** The probability \(\mathbb{P}(\hat{Y}=1|s)\) (as in Eq. 1) can be seen as the cumulative distribution \(F\) of a binomial random variable \(\mathcal{B}(q_{s},n)\) with at most \(n\gamma-1\) successes out of \(n\) trials, with \(q_{s}=\frac{n(1-\psi_{n})+1}{2+n}\) as the success probability. By applying Hoeffding's inequality, we obtain the upper bound

\[\mathbb{P}(\hat{Y}=1|s)\leq\exp\!\left(\!-2n\left(\frac{n(1-\psi_{n})+1}{2+n} -\frac{n\gamma-1}{n}\right)^{2}\right)\]

that holds for the constraint \(\psi_{n}\leq\frac{2+n}{n^{2}}+\frac{1-2\gamma}{n}+(1-\gamma)\). Because \(\mathbb{P}(\hat{Y}=1|s)\leq e^{-T}\) implies that \(\mathscr{M}_{s}\geq 1-2e^{-T}\), we search for the values of \(\psi_{n}\) such that the upper bound is \(\leq e^{-T}\). Forcing the upper bound \(\leq e^{-T}\) results in

\[2n\left(\frac{n(1-\psi_{n})+1}{2+n}-\frac{n\gamma-1}{n}\right)^{2}-T\geq 0,\]

which is satisfied for (\(I_{1}\)) \(0\leq\psi_{n}\leq A_{1}-\sqrt{B_{1}}\) and (\(I_{2}\)) \(A_{1}+\sqrt{B_{1}}\leq\psi_{n}\leq 1\), where

\[A_{1}=\frac{2+n(n+1)(1-\gamma)}{n^{2}}\qquad B_{1}=\frac{2n\left(-3\gamma^{2} \!-\!2n(1-\gamma)^{2}\!+4\gamma-3\right)\!+T(n+2)^{2}\!-\!8}{2n^{3}}.\]

However, for \(T\geq 4\), no values of \(n\), \(\gamma\), and \(T\) that satisfy the constraint on \(\psi_{n}\) also satisfy \(I_{2}\). Moving to \(I_{1}\), we find out that if \(\psi_{n}\) satisfies \(I_{1}\), then it also satisfies the constraint on \(\psi_{n}\) for any \(n\), \(\gamma\), and \(T\). Therefore, we we set \(t_{1}(n,\gamma,T)=A_{1}-\sqrt{B_{1}}\). As a result,

\[\psi_{n}\leq t_{1}\implies\mathbb{P}(\hat{Y}=1|s)\leq e^{-T}\implies\mathscr{ M}_{s}\geq 1-2e^{-T}.\]

**(b)** Similarly, \(\mathbb{P}(\hat{Y}=0|s)\) can be seen as the cumulative distribution \(F\) of \(\mathcal{B}(p_{s},n)\), with \(n(1-\gamma)\) successes and \(p_{s}=\frac{1+n\psi_{n}(s)}{2+n}\). By seeing the binomial as a sum of Bernoulli random variables, and using the property of its cumulative distribution \(F(n(1-\gamma),n,p_{s})+F(n\gamma-1,n,1-p_{s})=1\), we apply the Hoeffding's inequality and compare such upper bound to the \(e^{-T}\). We obtain

\[2n\left(\frac{1+\psi_{n}n}{2+n}-(1-\gamma)\right)^{2}-T\geq 0\]

that holds with the constraint \(\psi_{n}\geq\frac{(2+n)(1-\gamma)-1}{n}\). The quadratic inequality in \(\psi_{n}\) has solutions for (\(I_{1}\)) \(0\leq\psi_{n}\leq A_{2}-\sqrt{B_{2}}\) and (\(I_{2}\)) \(A_{2}+\sqrt{B_{2}}\leq\psi_{n}\leq 1\), where \(A_{2}=\frac{(2+n)(1-\gamma)-1}{n}\), and \(B_{2}=\frac{T(n+2)^{2}}{2n^{3}}\). However, the constraint limits the solutions to \(I_{2}\), i.e. for \(\psi_{n}\geq A_{2}+\sqrt{B_{2}}\). Thus, we set \(t_{2}(n,\gamma,T)=A_{2}+\sqrt{B_{2}}\) and conclude that

\[\psi_{n}\geq t_{2}\implies\mathbb{P}(\hat{Y}=1|s)\geq 1-e^{-T}\Longrightarrow \mathscr{M}_{s}\geq 1-2e^{-T}.\]Secondly, Theorem 3.6 relies on two important results: given \(S\) the anomaly score random variable, (1) if \(\psi_{n}\) was the _theoretical_ cumulative of \(S\), it would have a uniform distribution (Theorem A.1), but because in practice (2) \(\psi_{n}\) is the _empirical_ cumulative of \(S\), its distribution is close to uniform with high probability (Theorem A.2). We prove these results in the following theorems.

**Theorem A.1**.: _Let \(S\) be the anomaly score random variable, and \(\psi=F_{S}(S)\) be the cumulative distribution of \(S\) applied to \(S\) itself. Then \(\psi\sim Unif(0,1)\)._

Proof.: We prove that, if \(\psi\sim Unif(0,1)\), then \(F_{\psi}(t)=t\) for any \(t\in[0,1]\):

\[F_{\psi}(t)=\mathbb{P}(\psi\leq t)=\mathbb{P}(F_{S}(S)\leq t)=\mathbb{P}(S \leq F_{S}^{-1}(t))=F_{S}(F_{S}^{-1}(t))=t\ \implies\ \psi\sim Unif(0,1).\]

**Theorem A.2**.: _Let \(\psi\) be as in Theorem A.1, and \(F_{\psi_{n}}\) be its empirical distribution obtained from a sample of size \(n\). For any small \(\delta>0\) and \(t\in[0,1]\), with probability \(>1-\delta\)_

\[F_{\psi_{n}}(t)\in\left[F_{\psi}(t)-\sqrt{\frac{\ln\frac{2}{\delta}}{2n}},F_{ \psi}(t)+\sqrt{\frac{\ln\frac{2}{\delta}}{2n}}\right].\]

Proof.: For any \(\varepsilon>0\), the DKW inequality implies

\[\mathbb{P}\left(\sup_{t\in[0,1]}|F_{\psi_{n}}(t)-F_{\psi}(t)|>\varepsilon \right)\leq 2\exp\left(-2n\varepsilon^{2}\right).\]

By setting \(\delta=2\exp\left(-2n\varepsilon^{2}\right)\), i.e. \(\varepsilon=\sqrt{\frac{\ln\frac{2}{\delta}}{2n}}\), and using the complementary probability we conclude that

\[\mathbb{P}\left(\sup_{t\in[0,1]}|F_{\psi_{n}}(t)-F_{\psi}(t)|\leq\sqrt{\frac{ \ln\frac{2}{\delta}}{2n}}\right)>1-\delta.\]

## Appendix B Experiments

Data.Table 3 shows the properties of the \(34\) datasets used for the experimental comparison, in terms of number of examples, features, and contamination factor \(\gamma\). The datasets can be downloaded in the following link: https://github.com/Minqi824/ADBench/tree/main/datasets/Classical.

Q1. ReJEx against the baselines.Table 4 and Table 5 show the results (mean \(\pm\) std) aggregated by detectors in terms of, respectively, cost per example and ranking position. Results confirm that ReJEx obtains an average cost per example lower than all the baselines for \(9\) out of \(12\) detectors, which is similar to the runner-up SS-Repen for the remaining \(3\) detectors. Moreover, ReJEx has always the best (lowest) average ranking position.

Q2. Varying the costs \(c_{fp},c_{fn},c_{r}\).Table 6 and Table 7 show the average cost per example and the ranking position (mean \(\pm\) std) aggregated by detectors for three representative cost functions, as discussed in the paper. Results are similar in all three cases. For high false positives cost (\(c_{fp}=10\)), ReJEx obtains an average cost per example lower than all the baselines for \(11\) out of \(12\) detectors and always the best average ranking position. For high false negative cost (\(c_{fn}=10\)) as well as for low rejection cost (\(c_{fp}=5,c_{fn}=5,c_{r}=\gamma\)), it has the lowest average cost for all detectors and always the best average ranking. Moreover, when rejection is highly valuable (low cost), ReJEx's cost has a large gap with respect to the baselines, which means that it is particularly useful when rejection is less expensive.

\begin{table}
\begin{tabular}{l|r r|r r|r|r|r|r|r} \hline \hline  & \multicolumn{6}{c}{**Cost per example (mean \(\pm\) std.)**} \\ \cline{2-9} Det. & \multicolumn{1}{c|}{RejEx} & \multicolumn{1}{c|}{SS-Repen} & \multicolumn{1}{c|}{Mv} & \multicolumn{1}{c|}{Ens} & \multicolumn{1}{c|}{Udr} & \multicolumn{1}{c|}{Em} & \multicolumn{1}{c}{Stability} & \multicolumn{1}{c}{NoReject} \\ \hline Ae & **0.122 \(\pm\) 0.139** & 0.124 \(\pm\) 0.133 & 0.136 \(\pm\) 0.143 & 0.134 \(\pm\) 0.151 & 0.138 \(\pm\) 0.148 & 0.143 \(\pm\) 0.150 & 0.143 \(\pm\) 0.149 & 0.148 \(\pm\) 0.152 \\ Coprod & **0.123 \(\pm\) 0.138** & 0.125 \(\pm\) 0.134 & 0.135 \(\pm\) 0.142 & 0.133 \(\pm\) 0.140 & 0.140 \(\pm\) 0.144 & 0.143 \(\pm\) 0.148 & 0.145 \(\pm\) 0.147 & 0.146 \(\pm\) 0.148 \\ Eco & **0.120 \(\pm\) 0.136** & 0.124 \(\pm\) 0.135 & 0.129 \(\pm\) 0.136 & 0.133 \(\pm\) 0.143 & 0.139 \(\pm\) 0.142 & 0.140 \(\pm\) 0.145 & 0.143 \(\pm\) 0.144 & 0.145 \(\pm\) 0.145 \\ GMM & **0.122 \(\pm\) 0.135** & 0.124 \(\pm\) 0.137 & 0.144 \(\pm\) 0.141 & 0.113 \(\pm\) 0.146 & 0.142 \(\pm\) 0.145 & 0.156 \(\pm\) 0.148 & 0.151 \(\pm\) 0.147 & 0.145 \(\pm\) 0.149 \\ Hbos & **0.116 \(\pm\) 0.129** & 0.123 \(\pm\) 0.138 & 0.131 \(\pm\) 0.132 & 0.134 \(\pm\) 0.136 & 0.135 \(\pm\) 0.137 & 0.139 \(\pm\) 0.141 & 0.140 \(\pm\) 0.139 & 0.142 \(\pm\) 0.142 \\ Igo & **0.115 \(\pm\) 0.128** & 0.123 \(\pm\) 0.135 & 0.130 \(\pm\) 0.136 & 0.129 \(\pm\) 0.136 & 0.134 \(\pm\) 0.139 & 0.140 \(\pm\) 0.143 & 0.139 \(\pm\) 0.141 & 0.143 \(\pm\) 0.144 \\ Inse & **0.113 \(\pm\) 0.129** & 0.122 \(\pm\) 0.133 & 0.145 \(\pm\) 0.134 & 0.146 \(\pm\) 0.140 & 0.145 \(\pm\) 0.138 & 0.147 \(\pm\) 0.140 & 0.146 \(\pm\) 0.139 & 0.145 \(\pm\) 0.140 \\ Kde & **0.127 \(\pm\) 0.140** & **0.127 \(\pm\) 0.134** & 0.143 \(\pm\) 0.145 & 0.138 \(\pm\) 0.145 & 0.144 \(\pm\) 0.145 & 0.150 \(\pm\) 0.148 & 0.145 \(\pm\) 0.143 & 0.152 \(\pm\) 0.148 \\ Kun & **0.119 \(\pm\) 0.123** & 0.125 \(\pm\) 0.135 & 0.140 \(\pm\) 0.131 & 0.135 \(\pm\) 0.131 & 0.130 \(\pm\) 0.130 & 0.144 \(\pm\) 0.132 & 0.141 \(\pm\) 0.131 & 0.146 \(\pm\) 0.133 \\ Loda & **0.125 \(\pm\) 0.133** & **0.125 \(\pm\) 0.134** & 0.131 \(\pm\) 0.130 & 0.139 \(\pm\) 0.137 & 0.140 \(\pm\) 0.136 & 0.146 \(\pm\) 0.141 & 0.141 \(\pm\) 0.131 & 0.151 \(\pm\) 0.142 \\ Lof & **0.126 \(\pm\) 0.131** & **0.126 \(\pm\) 0.136** & 0.155 \(\pm\) 0.140 & 0.140 \(\pm\) 0.139 & 0.142 \(\pm\) 0.138 & 0.157 \(\pm\) 0.140 & 0.151 \(\pm\) 0.139 & 0.158 \(\pm\) 0.140 \\ Ocesym & **0.120 \(\pm\) 0.131** & 0.125 \(\pm\) 0.133 & 0.138 \(\pm\) 0.138 & 0.132 \(\pm\) 0.140 & 0.138 \(\pm\) 0.140 & 0.141 \(\pm\) 0.140 & 0.137 \(\pm\) 0.136 & 0.147 \(\pm\) 0.143 \\ \hline Avg. & **0.121 \(\pm\) 0.133** & 0.125 \(\pm\) 0.135 & 0.138 \(\pm\) 0.137 & 0.136 \(\pm\) 0.140 & 0.139 \(\pm\) 0.140 & 0.146 \(\pm\) 0.143 & 0.144 \(\pm\) 0.140 & 0.148 \(\pm\) 0.144 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Cost per example (mean \(\pm\) std) per detector aggregated over the datasets. Results show that RejEx obtains a lower average cost for 9 out of \(12\) detectors and similar average cost as the runner-up SS-Repen for the remaining \(3\) detectors. Moreover, RejEx has the best overall average (last row).

\begin{table}
\begin{tabular}{l r r} \hline \hline Dataset & \#Examples & \#Features & \(\gamma\) \\ \hline Aloi & 20000 & 27 & 0.0315 \\ Annthyroid & 7062 & 6 & 0.0756 \\ Campaign & 20000 & 62 & 0.1127 \\ Cardio & 1822 & 21 & 0.0960 \\ Cardiotocography & 2110 & 21 & 0.2204 \\ Census & 20000 & 500 & 0.0854 \\ Donors & 20000 & 10 & 0.2146 \\ Fault & 1941 & 27 & 0.3467 \\ Fraud & 20000 & 29 & 0.0021 \\ Glass & 213 & 7 & 0.0423 \\ Http & 20000 & 3 & 0.0004 \\ Internetads & 1966 & 1555 & 0.1872 \\ Landsat & 6435 & 36 & 0.2071 \\ Letter & 1598 & 32 & 0.0626 \\ Lymphography & 148 & 18 & 0.0405 \\ Mammography & 7848 & 6 & 0.0322 \\ Musk & 3062 & 166 & 0.0317 \\ Optdigits & 5198 & 64 & 0.0254 \\ PageBloocks & 5393 & 10 & 0.0946 \\ Pendigits & 6870 & 16 & 0.0227 \\ Pima & 768 & 8 & 0.3490 \\ Satellite & 6435 & 36 & 0.3164 \\ Satimage & 5801 & 36 & 0.0119 \\ Shuttle & 20000 & 9 & 0.0725 \\ Thyroid & 3656 & 6 & 0.0254 \\ Vertebral & 240 & 6 & 0.1250 \\ Vowels & 1452 & 12 & 0.0317 \\ Waveform & 3443 & 21 & 0.0290 \\ WBC & 223 & 9 & 0.0448 \\ WDBC & 367 & 30 & 0.0272 \\ Wilt & 4819 & 5 & 0.0533 \\ Wine & 129 & 13 & 0.0775 \\ WPBC & 198 & 33 & 0.2374 \\ Yeast & 1453 & 8 & 0.3310 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Properties (number of examples, features, and contamination factor) of the \(34\) benchmark datasets used for the experiments.

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c|c|c|c} \hline \hline  & \multicolumn{10}{c}{**CoST per Example for Three Cost Functions (mean \(\pm\) std)**} \\ \cline{2-10} Det. & ReJEx & SS-Repen & Mv & Ens & Udr & Em & Stability & NoReject \\ \hline \multicolumn{10}{c}{**False Positive Cost \(=10\), False Negative Cost \(=1\), Rejection Cost \(=\min\{10(1-\gamma,\gamma)\}\)**} \\ \hline \multicolumn{10}{l}{**Ae**} & **0.504 \(\pm\) 0.626** & 0.584 \(\pm\) 0.723 & 0.697 \(\pm\) 0.763 & 0.661 \(\pm\) 0.830 & 0.703 \(\pm\) 0.829 & 0.766 \(\pm\) 0.841 & 0.768 \(\pm\) 0.826 & 0.825 \(\pm\) 0.873 \\ \multicolumn{10}{l}{**Copod**} & **0.491 \(\pm\) 0.637** & 0.593 \(\pm\) 0.706 & 0.686 \(\pm\) 0.746 & 0.618 \(\pm\) 0.726 & 0.707 \(\pm\) 0.788 & 0.778 \(\pm\) 0.825 & 0.785 \(\pm\) 0.801 & 0.781 \(\pm\) 0.833 \\ \multicolumn{10}{l}{**Ecood**} & **0.479 \(\pm\) 0.628** & 0.584 \(\pm\) 0.727 & 0.625 \(\pm\) 0.705 & 0.642 \(\pm\) 0.755 & 0.711 \(\pm\) 0.774 & 0.748 \(\pm\) 0.803 & 0.770 \(\pm\) 0.783 & 0.717 \(\pm\) 0.817 \\ \multicolumn{10}{l}{**Gam**} & **0.568 \(\pm\) 0.713** & 0.589 \(\pm\) 0.752 & 0.823 \(\pm\) 0.878 & 0.715 \(\pm\) 0.929 & 0.790 \(\pm\) 0.925 & 0.941 \(\pm\) 0.948 & 0.889 \(\pm\) 0.929 & 0.950 \(\pm\) 0.967 \\ \multicolumn{10}{l}{**Hos**} & **0.475 \(\pm\) 0.595** & 0.569 \(\pm\) 0.758 & 0.666 \(\pm\) 0.693 & 0.697 \(\pm\) 0.732 & 0.709 \(\pm\) 0.764 & 0.776 \(\pm\) 0.803 & 0.771 \(\pm\) 0.770 & 0.809 \(\pm\) 0.816 \\ \multicolumn{10}{l}{**Iron**} & **0.477 \(\pm\) 0.602** & 0.575 \(\pm\) 0.712 & 0.665 \(\pm\) 0.718 & 0.634 \(\pm\) 0.731 & 0.683 \(\pm\) 0.786 & 0.776 \(\pm\) 0.818 & 0.763 \(\pm\) 0.783 & 0.808 \(\pm\) 0.831 \\ \multicolumn{10}{l}{**Inse**} & **0.479 \(\pm\) 0.592** & 0.567 \(\pm\) 0.698 & 0.752 \(\pm\) 0.724 & 0.820 \(\pm\) 0.795 & 0.815 \(\pm\) 0.787 & 0.819 \(\pm\) 0.793 & 0.818 \(\pm\) 0.792 & 0.823 \(\pm\) 0.799 \\ \multicolumn{10}{l}{**Kde**} & 0.602 \(\pm\) 0.827 & **0.589 \(\pm\) 0.704** & 0.819 \(\pm\) 0.947 & 0.740 \(\pm\) 0.913 & 0.793 \(\pm\) 0.939 & 0.897 \(\pm\) 0.945 & 0.774 \(\pm\) 0.906 & 0.914 \(\pm\) 0.945 \\ \multicolumn{10}{l}{**Kun**} & **0.498 \(\pm\) 0.571** & 0.596 \(\pm\) 0.726 & 0.741 \(\pm\) 0.734 & 0.669 \(\pm\) 0.720 & 0.669 \(\pm\) 0.736 & 0.777 \(\pm\) 0.747 & 0.739 \(\pm\) 0.735 & 0.800 \(\pm\) 0.749 \\ \multicolumn{10}{l}{**Lola**} & **0.518 \(\pm\) 0.619** & 0.574 \(\pm\) 0.709 & 0.574 \(\pm\) 0.647 & 0.689 \(\pm\) 0.729 & 0.701 \(\pm\) 0.748 & 0.762 \(\pm\) 0.774 & 0.697 \(\pm\) 0.682 & 0.827 \(\pm\) 0.797 \\ \multicolumn{10}{l}{**Lof**} & **0.539 \(\pm\) 0.623** & 0.603 \(\pm\) 0.742 & 0.898 \(\pm\) 0.840 & 0.685 \(\pm\) 0.773 & 0.715 \(\pm\) 0.790 & 0.891 \(\pm\) 0.813 & 0.831 \(\pm\) 0.821 & 0.887 \(\pm\) 0.808 \\ \multicolumn{10}{l}{**Oscvm**} & **0.479 \(\pm\) 0.599** & 0.589 \(\pm\) 0.705 & 0.745 \(\pm\) 0.790 & 0.632 \(\pm\) 0.752 & 0.694 \(\pm\) 0.782 & 0.760 \(\pm\) 0.775 & 0.695 \(\pm\) 0.737 & 0.818 \(\pm\) 0.806 \\ \hline \multicolumn{10}{c}{**False Positive Cost \(=1\), False Negative Cost \(=10\), Rejection Cost \(=\min\{1-\gamma,\,10\gamma\}\)**} \\ \hline \multicolumn{10}{l}{**Ae**} & **0.730 \(\pm\) 0.747** & 0.761 \(\pm\) 0.756 & 0.909 \(\pm\) 0.882 & 0.784 \(\pm\) 0.825 & 0.780 \(\pm\) 0.805 & 0.819 \(\pm\) 0.843 & 0.789 \(\pm\) 0.825 & 0.797 \(\pm\) 0.821 \\ \multicolumn{10}{l}{**Copod**} & **0.761 \(\pm\) 0.767** & 0.765 \(\pm\) 0.770 & 0.930 \(\pm\) 0.888 & 0.794 \(\pm\) 0.805 & 0.800 \(\pm\) 0.801 & 0.804 \(\pm\) 0.844 & 0.802 \(\pm\) 0.815 & 0.827 \(\pm\) 0.832 \\ \multicolumn{10}{l}{**Ecood**} & **0.739 \(\pm\) 0.759** & 0.767 \(\pm\) 0.766 & 0.900 \(\pm\) 0.858 & 0.789 \(\pm\) 0.891 & 0.788 \(\pm\) 0.787 & 0.870 \(\pm\) 0.839 & 0.791 \(\pm\) 0.803 & 0.821 \(\pm\) 0.819 \\ \multicolumn{10}{l}{**Gam**} & **0.670 \(\pm\) 0.676** & 0.765 \(\pm\) 0.767 & 0.845 \(\pm\) 0.782 & 0.754 \(\pm\) 0.755 & 0.739 \(\pm\) 0.736 & 0.736 \(\pm\) 0.785 & 0.757 \(\pm\) 0.760 & 0.753 \(\pm\) 0.766 \\ \multicolumn{10}{l}{**Hos**} & **0.687 \(\pm\) 0.684** & 0.776 \(\pm\) 0.782 & 0.824 \(\pm\) 0.808 & 0.750 \(\pm\) 0.768 & 0.744 \(\pm\) 0.747 & 0.745 \(\pm\) 0.787 & 0.787 \(\pm\) 0.787 & 0.749 \(\pm\) 0.765 & 0.753 \(\pm\) 0.766 \\ \multicolumn{10}{l}{**For**} & **0.679 \(\pm\

[MISSING_PAGE_EMPTY:19]