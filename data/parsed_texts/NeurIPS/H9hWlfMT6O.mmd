# Training Transformers with 4-bit Integers

Haocheng Xi\({}^{2}\), Changhao Li\({}^{1}\), Jianfei Chen\({}^{1}\), and Jun Zhu\({}^{1}\)

\({}^{1}\)Dept. of Comp. Sci. and Tech., Institute for AI, BNRist Center, THBI Lab,

Tsinghua-Bosch Joint ML Center, Tsinghua University

\({}^{2}\)Institute for Interdisciplinary Information Sciences, Tsinghua University

{xihc20,lichangh20}@mails.tsinghua.edu.cn, {jianfeic,dcszj}@tsinghua.edu.cn

This work was done during an internship in the Department of Computer Science and Technology, Tsinghua University

###### Abstract

Quantizing the activation, weight, and gradient to 4-bit is promising to accelerate neural network training. However, existing 4-bit training methods require custom numerical formats which are not supported by contemporary hardware. In this work, we propose a training method for transformers with all matrix multiplications implemented with the INT4 arithmetic. Training with an ultra-low INT4 precision is challenging. To achieve this, we carefully analyze the specific structures of activation and gradients in transformers to propose dedicated quantizers for them. For forward propagation, we identify the challenge of outliers and propose a Hadamard quantizer to suppress the outliers. For backpropagation, we leverage the structural sparsity of gradients by proposing bit splitting and leverage score sampling techniques to quantize gradients accurately. Our algorithm achieves competitive accuracy on a wide range of tasks including natural language understanding, machine translation, and image classification. Unlike previous 4-bit training methods, our algorithm can be implemented on the current generation of GPUs. Our prototypical linear operator implementation is up to 2.2 times faster than the FP16 counterparts and speeds up the training by 17.8% on average for sufficiently large models. Our code is available at https://github.com/xijiu9/Train_Transformers_with_INT4.

## 1 Introduction

Training neural networks is computationally demanding. Training with low-precision arithmetic (a.k.a., fully quantized training or FQT) is promising to improve computational and memory efficiency. FQT methods add some quantizers and dequantizers in the original full-precision computational graph, and replace expensive floating-point operations with cheap low-precision ones.

Research in FQT aims to reduce the training numerical precision, without sacrificing much convergence speed or accuracy. The required numerical precision has been reduced from FP16 [33] to FP8 [54, 46], INT32+INT8 [3] and INT8+INT5 [7]. FP8 training is implemented in Nvidia's H100 GPU with Transformer Engine [35], achieving impressive speedup for the training of large-scale transformers. Recently, the training numerical precision has been pushed down to 4 bits. Sun et al. [47] successfully trained several modern networks with INT4 activation/weights and FP4 gradients; and Chmeli et al. [8] propose a custom 4-bit logarithmic numerical format to further improve the accuracy. However, these 4-bit training methods cannot be directly utilized for acceleration as they require custom numerical formats that are not supported on contemporary hardware.

There are significant optimization challenges to train neural networks at an extremely low 4-bit level. First, the non-differentiable quantizers in forward propagation make the loss landscape rugged, where gradient-based optimizers can easily stuck at local optima [31]. Second, gradients are only computedapproximately in low-precision. Such imprecise gradients slow down the training process and even cause the training to be unstable or diverge.

In this work, we propose a novel INT4 training algorithm for a class of popular neural networks, transformers [52]. All the costly linear operations for training transformers can be written in a matrix multiplication (MM) form. This MM form allows us to design more flexible quantizers, which better approximate FP32 matrix multiplications by utilizing specific structures of the activations, weights, and gradients in transformers. Our quantizers leverage advances in the field of randomized numerical linear algebra (RandNLA) [14].

For forward propagation, we find that outliers in the activation are the main reason for accuracy degradation. To suppress outliers, we propose a _Hadamard quantizer_, which quantizes a _transformed version_ of the activation matrix. The transformation is a block diagonal Hadamard matrix, which spreads the information carried in outliers to its nearby entries of the matrix and thus reduces the numerical range of the outliers.

For backpropagation, we exploit the _structural sparsity_ of activation gradients. We find that the gradients of a few tokens are extremely large. Meanwhile, the gradients for the rest majority of the tokens are very small, even smaller than the quantization residuals of larger gradients. Rather than computing these small gradients, it is better to save the computational resources for calculating the residuals of the larger gradients. To utilize such sparsity, we propose _bit splitting_, which splits the gradient of each token into higher 4 bits and lower 4 bits. Then, we choose the most informative gradients by _leverage score sampling_, which is an importance sampling technique for RandNLA.

Combining quantization techniques for forward and backward propagation, we propose an algorithm that uses INT4 MMs for all linear operations in transformers. We evaluate our algorithm for training transformers on a wide variety of tasks, including natural language understanding, question answering, machine translation, and image classification. Our algorithm achieves competitive or superior accuracy compared with existing works on 4-bit training [47; 8]. Moreover, our algorithm _is compatible with contemporary hardware_ like GPUs, since it does not require custom numerical formats like FP4 or logarithm formats. Our prototypical quantization + INT4 MM operator implementation is up to 2.2 times faster than the FP16 MM baseline, and it speeds up the training by up to 35.1%.

Finally, we would like to point out that utilizing ultra-low 4-bit numerical formats for training neural networks is still an open problem, and the main purpose of this research is to study whether it is possible to design an INT4 training algorithm that can achieve reasonable accuracy on practical tasks. The current research state of INT4 training is not yet mature enough to provide significant speedup for most tasks in a generic and plug-and-play manner. See Sec. 6 for further discussions.

## 2 Related Work

Fully Quantized TrainingFully quantized training (FQT) [33; 54; 46; 3; 15; 57; 65; 29; 30; 59; 68] methods accelerate training by quantizing the activations, weights, and gradients to low-precision, so linear and nonlinear operators during training can be implemented with low-precision arithmetic. Research on FQT design novel numerical formats and quantization algorithms that better approximate full-precision tensors. The current research frontier is 4-bit FQT. FQT is challenging due to the vast numerical range of the gradient and the optimization issues of training quantized networks from scratch. Due to these challenges, existing 4-bit FQT algorithms [47; 8] still have \(\sim\)1-2.5% accuracy drop on several tasks, and they cannot support contemporary hardware.

Other Efficient Training MethodsMixture-of-experts [43] improves the model capacity without increasing the training budget. Structural dropout [21; 17] exploits computationally efficient ways to regularize the model. Efficient attention [27; 10] reduces the quadratic time complexity for computing attention. Distributed training systems [39; 22] reduce training time by leveraging more computational resources. Our work on reducing numerical precision is orthogonal with these directions.

## 3 Forward Propagation

Neural network training is an iterative optimization procedure with stochastic gradients computed by forward and backpropagation. We accelerate forward and back propagation with 4-bit integer (INT4) arithmetic. We first describe the forward propagation of our training procedure. The forward propagation can be formulated as a composition of linear and non-linear (GeLU, normalization, softmax, etc.) operators. In our training procedure, we accelerate all the linear operators with INT4 arithmetic and leave all the less-computationally-intensive non-linear operators in the 16-bit floating-point (FP16) format. All linear operations in transformers can be written in a matrix multiplication (MM) form. For ease of presentation, we consider the acceleration of the following simple matrix multiplication throughout this paper:

\[\mathbf{Z}=\mathbf{X}\mathbf{W}^{\top},\text{where }\mathbf{Z}\in\mathbb{R}^{N \times C},\mathbf{X}\in\mathbb{R}^{N\times D}\text{and }\mathbf{W}\in\mathbb{R}^{C\times D}.\] (1)

The most predominant use case of such MM is the fully-connected layer. Consider a transformer with an input shape of _(batch size \(S\), sequence length \(T\)_, dimensionality \(D\)). The fully-connected layer can be written as Eq. (1) where \(\mathbf{X}\) is the activation for \(N=ST\) tokens, and \(\mathbf{W}\) is the weight matrix. For attention layers, batch matrix multiplications (BMMs) might be required. Our proposed techniques can be applied to BMMs, and we leave the discussion of BMMs in Appendix. A.1.

### Learned Step Size Quantization

To accelerate training, the forward propagation must be computed with integer arithmetic. We leverage the _learned step size quantizer_ (LSQ) [16] for this purpose. LSQ is a static quantization method whose quantization scale does not depend on the input, and is thus cheaper than dynamic quantization methods [23], which need to compute the quantization scale dynamically per iteration.

Given a FP matrix \(\mathbf{X}\), LSQ _quantizes_\(\mathbf{X}\) to integer with

\[\text{int}_{s_{X}}\left(\mathbf{X}\right):=\lfloor\text{clamp}(\mathbf{X}/s _{X},-Q_{N},Q_{P})\rceil\;,\] (2)

where \(s_{X}\) is a learnable scalar parameter, clamp restricts its input to the range \([-Q_{N},Q_{P}]\), \(\lfloor\cdot\rceil\) is a rounding operation, and \(\mathbf{X}/s_{X}\) is computed elementwise. The resultant matrix takes values from \(\{-Q_{N},-Q_{N}+1,\ldots,Q_{P}\}\). Since we aim to perform INT4 MMs, we set \(Q_{N}=Q_{P}=7\). The integer matrix can be _dequantized_ back to FP through float \(\left(\text{int}_{s_{X}}\left(\mathbf{X}\right)\right)=s_{X}\text{int}_{s_{X} }\left(\mathbf{X}\right)\approx\mathbf{X}\).

With LSQ, Eq. (1) can be computed approximately as \(\mathbf{Y}=\mathbf{X}\mathbf{W}^{\top}\approx s_{X}s_{W}\text{int}_{s_{X}} \left(\mathbf{X}\right)\text{int}_{s_{W}}\left(\mathbf{W}\right)^{\top},\) where the INT4 MM \(\text{int}_{s_{X}}\left(\mathbf{X}\right)\text{int}_{s_{W}}\left(\mathbf{W} \right)^{\top}\) can be implemented efficiently on hardware.

**Remark:** Quantization-aware training (QAT) [63; 67; 23; 12; 11; 44; 60; 45; 49; 64; 2; 18; 55] is an _inference acceleration_ technique which trains networks with quantizers inserted in the forward propagation graph, so the trained network can perform efficiently during inference. QAT can compress activation/weights to extremely low precision (e.g. 1-2 bits). It is tempting to think that directly applying a quantizer for QAT to FQT can lead to similar low activation/weights bit-width. However, even only quantizing the forward propagation for FQT is much more challenging than QAT because: (1) QAT requires a converged full-precision model as initialization [16] and/or as a teacher model for knowledge distillation [2]; (2) QAT can adopt expensive multi-stage training pipelines without worrying about the convergence speed [32], while FQT algorithm must converge as fast as full-precision training algorithms to be useful; (3) QAT may approximate the discrete quantizer with continuous functions during training [19], which cannot be implemented with integer arithmetic. Due to these challenges, it is still an open problem to do FQT with 4-bit activations/weights.

### Activation Outliers

Simply applying LSQ for FQT with 4-bit activation/weights leads to accuracy degradation due to _activation outliers_[58]. As shown in Fig. 1(a), activations have some outlier entries, which are much larger in magnitude than other entries. In this case, the step size \(s_{X}\) poses a trade-off between quantization granularity and representable numerical range. If \(s_{X}\) is large, we can represent the outliers well at the expense of representing most other entries in a very coarse manner. On the other hand, if \(s_{X}\) is small, we have to truncate the entries outside the range \([-Q_{N}s_{X},Q_{PS}s_{X}]\). Unfortunately, the transformers tend to store information in these outliers, and such truncation would seriously harm accuracy (see Sec. 5.2 for details). The outlier problem is particularly significant when the training task is to fine-tune a pre-trained model on some new downstream tasks, since the pre-train model contains more outliers [58] than random initialization.

There exists some works to handle activation outliers for post-training quantization (PTQ). Outlier Suppression [56] discover that LayerNorms amplify outliers, and propose Gamma Migration and Token-Wise Clipping to solve this issue and achieves 6-bit BERT PTQ without too much degradation. SmoothQuant [58] migrates the quantization difficulty of activation outliers to weights and achieves 8-bit PTQ for large language models, such as OPT-175B. Outlier Channel Splitting [66] duplicates channels containing outliers with small overhead on the size of the network. However, these methods mainly focus on PTQ or QAT, and seldom successfully deal with ultra-low 4-bit training.

### Hadamard Quantization

We propose a _Hadamard quantizer_ (HQ) to solve the outlier problem. Its main idea is to quantize the matrices _in another linear space_ which has fewer outliers.

The outliers in activation matrices form a feature-wise structure [58]. They are typically concentrated on a few dimensions, i.e., only a few columns of \(\mathbf{X}\) are significantly larger than others. Hadamard transform [48] is a linear transformation, which can amortize the outliers into other entries. Specifically, the Hadamard transform \(\mathbf{H}_{k}\) is a \(2^{k}\times 2^{k}\) matrix, where

\[\mathbf{H}_{0}=\left[1\right],\quad\mathbf{H}_{k}=\tfrac{1}{\sqrt{2}}\left[ \mathbf{H}_{k-1}\quad\mathbf{H}_{k-1};\mathbf{H}_{k-1}\quad-\mathbf{H}_{k-1} \right].\]

Hadamard matrices are orthogonal and symmetric: \(\mathbf{H}_{k}=\mathbf{H}_{k}^{\top}=\mathbf{H}_{k}^{-1}\), so \(\mathbf{H}_{k}\mathbf{H}_{k}=\mathbf{I},\forall k\geq 0\). Consider any coordinate row vector*\(\mathbf{e}_{i}^{\top}\in\mathbb{R}^{2^{k}}\), we have \(\mathbf{e}_{i}^{\top}\mathbf{H}_{k}=2^{-k/2}\mathbf{1}_{2^{k}},\forall i\), where \(\mathbf{1}_{2^{k}}=(\pm 1,\pm 1,\dots,\pm 1)\) is a \(2^{k}\)-dimensional vector with all of its values being \(1\) or \(-1\). This demonstrates the extreme case when a single outlier dominates all the rest dimensions. In this case, Hadamard transformation effectively turns the vector into a quantization-friendly all-one-vector. The practical effect of the Hadamard transform on suppressing activation outliers is demonstrated in Fig. 2(b).

Footnote *: A vector which \(i\)-th dimension is 1, and all other dimensions are 0.

HQ uses a block-diagonal transformation matrix \(\mathbf{H}\in\mathbb{R}^{D\times D}\): \(\mathbf{H}=\text{BlockDiag}(\mathbf{H}_{k},\dots,\mathbf{H}_{k}),\) where \(D\) is a multiple of \(2^{k}\). To suppress outliers, we quantize a transformed version of \(\mathbf{X}\) and \(\mathbf{W}\):

\[\mathbf{X}=(\mathbf{X}\mathbf{H})\mathbf{H}^{\top}\approx s_{X}\text{int}_{s _{X}}\left(\mathbf{X}\mathbf{H}\right)\mathbf{H}^{\top},\quad\mathbf{W}=( \mathbf{W}\mathbf{H})\mathbf{H}^{\top}\approx s_{W}\text{int}_{s_{W}}\left( \mathbf{W}\mathbf{H}\right)\mathbf{H}^{\top}.\]

Combining the quantized matrices, we get

\[\mathbf{Y}=\mathbf{X}\mathbf{W}^{\top}\approx s_{X}s_{W}\text{int}_{s_{X}} \left(\mathbf{X}\mathbf{H}\right)\mathbf{H}^{\top}\text{Hint}_{s_{W}}\left( \mathbf{H}^{\top}\mathbf{W}^{\top}\right) =s_{X}s_{W}\text{int}_{s_{X}}\left(\mathbf{X}\mathbf{H}\right) \text{int}_{s_{W}}\left(\mathbf{H}^{\top}\mathbf{W}^{\top}\right),\] (3)

where the inverse transformations cancel with each other, and the MM can be implemented as:

**Procedure** HQ-MM

Compute \(\mathbf{X}\mathbf{H}\) and \(\mathbf{H}^{\top}\mathbf{W}^{\top}\) in FP16.

Quantize the resultant matrices to INT4 by LSQ.

Multiply the two INT4 matrices.

Dequantize the resultant INT32 matrix to FP16 by multiplying \(s_{X}s_{W}\).

For time complexity, Step 1 takes \(O(2^{k}N(D+C))\) FP16 multiply-accumulates (MACs); Step 2and Step 4 takes \(O(N(D+C))\) FP16 MACs in total; and Step 3 takes \(O(NDC)\) INT4 MACs. Comparing with the plain LSQ Eq. (2), the amount of FP16 MACs increases by \(2^{k}\) times, from \(O(N(D+C))\) to \(O(2^{k}N(D+C))\). However, our HQ-MM is still much cheaper than an FP16 MM given \(2^{k}\ll D\) and \(2^{k}\ll C\). The number \(k\) shows a tradeoff between the ability to suppress outliers and computation complexity. Larger \(k\) allows for amortizing the outlier within a larger horizon, at the cost of being more expensive. We propose an adaptive algorithm to choose \(k\) for each activation depending on the outlier scale, as discussed in Appendix A.5. The typical value is \(k=5\), while the dimensionality \(C\) and \(D\) ranges from 768 to 4096.

## 4 Backpropagation

We now consider accelerating the backpropagation of the linear layer with INT4 operations. The linear operator HQ-MM defined in Eq. (3) has four inputs: activation \(\hat{\mathbf{X}}\), weight \(\mathbf{W}\), and step sizes \(s_{X}\), \(s_{W}\). Given the output gradient \(\nabla_{\mathbf{Y}}\mathcal{L}\) w.r.t. some loss function \(\mathcal{L}\), we need to compute the gradient of all four inputs. We discuss the computation of activation/weight gradients in this section, and left the discussion of step size gradients to Appendix A.3. For simplicity, we omit \(\mathcal{L}\) and simply use \(\nabla_{\mathbf{Y}}\) to denote the gradient in the following text.

By the straight-through estimator \(\left\lfloor x\right\rceil^{\prime}=1\)[5] and the chain rule, we have

\[\nabla_{\mathbf{W}}=s_{X}\left(\nabla_{\mathbf{Y}}^{\top}\hat{\mathbf{X}} \circ\mathbb{I}_{W}\right)\mathbf{H}^{\top},\quad\nabla_{\mathbf{X}}=s_{W} \mathbb{I}_{X}\circ\nabla_{\mathbf{Y}}\hat{\mathbf{W}}\mathbf{H}^{\top},\] (4)

where we define \(\hat{\mathbf{X}}=\text{int}_{s_{X}}\left(\mathbf{X}\mathbf{H}\right)\), \(\hat{\mathbf{W}}=\text{int}_{s_{W}}\left(\mathbf{W}\mathbf{H}\right)\), \(\mathbb{I}_{X}=\mathbb{I}(-Q_{N}\leq\mathbf{X}/s_{X}\leq Q_{P})\), and \(\mathbb{I}_{W}=\mathbb{I}(-Q_{N}\leq\mathbf{W}/s_{W}\leq Q_{P})\). For computing the gradients, three types of matrix multiplications are required:

1. The element-wise multiplication \(\circ\) of a \(0/1\) matrix \(\mathbb{I}_{X}\) (or \(\mathbb{I}_{W}\)) with another INT4 (or INT32) matrix. This operation has low time complexity.
2. The multiplication of an INT32 matrix with an FP16 block-wise Hadamard matrix \(s_{W}\mathbf{H}^{\top}\), which also has low-time complexity, as discussed in Sec. 3.3.
3. The multiplication of the FP16 gradient \(\nabla_{\mathbf{Y}}\) with an INT4 matrix \(\hat{\mathbf{X}}\) or \(\hat{\mathbf{W}}\), which we will accelerate by quantizing \(\nabla_{\mathbf{Y}}\) to INT4.

In the rest of this section, we will discuss quantization methods to compute the "type 3" MMs \(\nabla_{\mathbf{Y}}^{\top}\hat{\mathbf{X}}\) and \(\nabla_{\mathbf{Y}}\hat{\mathbf{W}}\). We quantize \(\nabla_{\mathbf{Y}}\) dynamically for each MM, while \(\hat{\mathbf{X}}\) and \(\hat{\mathbf{W}}\) have been already calculated in forward propagation in Section. 3. We start by discussing the structure of the gradient.

### Structural Sparsity of Gradients

We note that the gradient matrix \(\nabla_{\mathbf{Y}}\) tends to be very sparse along the training process. Furthermore, the sparsity has a structure: few rows (i.e., tokens) of \(\nabla_{\mathbf{Y}}\) have large entries, while most other rows are close to an all-zero vector. We illustrate this by plotting the histogram of per-row norm \(\|(\nabla_{\mathbf{Y}})_{i,:}\|\) for all the rows \(i\) in Fig. 2.

Such a structural sparsity arises from the heavy overparameterization [62] of modern neural networks. During almost the entire training process, the network operates in the overparameterized scheme [34], where it can fit most training data well, except for a few hard examples. Therefore, the (activation) gradient will be close to zero for well-fitted data points. We find that for pretraining tasks, such structural sparsity quickly emerges after only a few training epochs. For fine-tuning tasks, the gradient is always sparse during the whole training process.

### Bit Splitting and Leverage Score Sampling

Here, we discuss how to design gradient quantizers to accurately compute the MMs during backpropagation by leveraging structural sparsity. The high-level idea is that many rows of the gradient are so small that they have little impact on the parameter gradient, yet they waste abundant computation. On the other hand, the large rows cannot be accurately represented with INT4. We drop some small rows and use the saved computation to represent large rows more accurately.

First, we propose _bit splitting_ (BS), which splits a full-precision matrix as higher and lower 4 bits:

\[\nabla_{\mathbf{Y}}\approx s_{\uparrow}\nabla_{\mathbf{Y}}^{\dagger}+s_{ \downarrow}\nabla_{\mathbf{Y}}^{\downarrow},\] (5)

where \(s_{\uparrow},s_{\downarrow}\) are two floating-point scalars, and \(\nabla_{\mathbf{Y}}^{\dagger}\), \(\nabla_{\mathbf{Y}}^{\dagger}\) are INT4 matrices representing the higher and lower 4 bits, respectively. BS can be implemented by first quantizing \(\nabla_{\mathbf{Y}}\) to INT4 as \(\nabla_{\mathbf{Y}}\approx s_{\uparrow}\nabla_{\mathbf{Y}}^{\uparrow}\) and then quantize the residual to INT4 as \(\nabla_{\mathbf{Y}}-s_{\uparrow}\nabla_{\mathbf{Y}}^{\uparrow}\approx s_{ \downarrow}\nabla_{\mathbf{Y}}^{\downarrow}\). BS can be viewed as an INT8 representation of a matrix, where \(\nabla_{\mathbf{Y}}^{\uparrow}\) and \(\nabla_{\mathbf{Y}}^{\downarrow}\) are the higher and lower 4 bits of the INT8 representation. Next, we discuss how to compute the weight and activation gradient.

Weight GradientAs discussed earlier, weight gradient involves the matrix multiplication \(\nabla_{\mathbf{Y}}^{\top}\hat{\mathbf{X}}\), where \(\nabla_{\mathbf{Y}}\in\mathbf{R}^{N\times C}\) and \(\hat{\mathbf{X}}\) is an \(N\times D\) INT4 matrix. By Eq. (5):

\[\nabla_{\mathbf{Y}}^{\top}\hat{\mathbf{X}}\approx(s_{\uparrow}\nabla_{ \mathbf{Y}}^{\uparrow}{}^{\top}+s_{\downarrow}\nabla_{\mathbf{Y}}^{\downarrow }{}^{\top})\hat{\mathbf{X}}=\nabla_{\mathbf{Y}}^{\uparrow}{}^{\top}\mathbf{X }^{\ddagger},\] (6)

where we define \(\nabla_{\mathbf{Y}}^{\ddagger}=[s_{\uparrow}\nabla_{\mathbf{Y}}^{\dagger};s_{ \downarrow}\nabla_{\mathbf{Y}}^{\downarrow}]^{\top}\in\mathbb{R}^{2N\times C}\) and \(\hat{\mathbf{X}}^{\ddagger}=[\hat{\mathbf{X}};\hat{\mathbf{X}}]\) to be a \(2N\times D\) INT4 matrix. Eq. (6) represents the product of an INT8 \(\nabla_{\mathbf{Y}}^{\top}\) and an INT4 \(\hat{\mathbf{W}}\), and can be implemented by two INT4 MMs \(\nabla_{\mathbf{Y}}^{\uparrow}{}^{\top}\hat{\mathbf{X}}\) and \(\nabla_{\mathbf{Y}}^{\downarrow}{}^{\top}\hat{\mathbf{X}}\). Such MM is rather accurate since \(\nabla_{\mathbf{Y}}\) is represented with 8 bits.

However, comparing to a naive quantization of \(\nabla_{\mathbf{Y}}\) to INT4, BS doubles the amount of INT4 operations for MM. We propose _leverage score sampling_ (LSS) to cut the operations of Eq. (5) by half, to the same amount as the naive MM \(s_{\uparrow}\nabla_{\mathbf{Y}}^{\ddagger}\hat{\mathbf{X}}\). Noticing that the MM Eq. (6) can be written as the sum of \(2N\) rank-1 matrices:

\[\nabla_{\mathbf{Y}}^{\ddagger}{}^{\top}\mathbf{X}^{\ddagger}=\sum_{i=1}^{2N} \nabla_{\mathbf{Y};i}^{\ddagger}{}^{\top}\mathbf{X}_{i}^{\ddagger}=\sum_{i=1} ^{2N}\nabla_{\mathbf{W}_{i}},\] (7)

where \(\nabla_{\mathbf{W}_{i}}=\nabla_{\mathbf{Y};i}^{\ddagger}{}^{\top}\mathbf{X}_ {i}^{\ddagger}\). Due to the sparsity of \(\nabla_{\mathbf{Y}}\), the matrices \(\nabla_{\mathbf{W}_{i}}\) differ in magnitude and small matrices can be discarded without having a big influence on the result.

Our proposed LSS assigns each \(\nabla_{\mathbf{W}_{i}}\) a probability \(p_{i}\in[0,1],i=1,\cdots,2N\), that satisfies \(\sum_{i=1}^{2N}p_{i}=N\). We define random masks \(m_{i}\sim\text{Bern}(p_{i})\) and mask matrix \(\tilde{\mathbf{M}}\), and approximate it as

\[\nabla_{\mathbf{Y}}^{\ddagger}{}^{\top}\mathbf{X}^{\ddagger}\approx\nabla_{ \mathbf{Y}}^{\ddagger}{}^{\top}\tilde{\mathbf{M}}\mathbf{X}^{\ddagger}=\sum_{ i=1}^{2N}\frac{m_{i}}{p_{i}}\nabla_{\mathbf{Y};i}^{\ddagger}{}^{\top}\mathbf{X}_ {i}^{\ddagger},\text{where }\tilde{\mathbf{M}}=\text{diag}\left(\frac{m_{1}}{p_{1}}, \ldots,\frac{m_{2N}}{p_{2N}}\right),\]

which is an unbiased approximation since \(\mathbb{E}\left[\nabla_{\mathbf{Y}}^{\ddagger}{}^{\top}\tilde{\mathbf{M}} \mathbf{X}^{\ddagger}\right]=\nabla_{\mathbf{Y}}^{\ddagger}{}^{\top}\mathbb{E }\left[\tilde{\mathbf{M}}\right]\mathbf{X}^{\ddagger}=\nabla_{\mathbf{Y}}^{ \ddagger}{}^{\top}\mathbf{X}^{\ddagger}\).

In expectation, there are only \(N\) nonzero \(m_{i}\)s. Therefore, LSS reduces the cost of MM by half. For LSS to be accurate, we minimize its variance. We have:

**Proposition 4.1**.: _(LSS variance for weight gradient)_

\[\mathrm{Var}\left[\sum_{i=1}^{2N}\frac{m_{i}}{p_{i}}\nabla_{\mathbf{Y};i}^{\ddagger }{}^{\top}\mathbf{X}_{i}^{\ddagger}\right]=\sum_{i=1}^{2N}\frac{1-p_{i}}{p_{i}} \|\nabla_{\mathbf{Y}_{i};i}^{\ddagger}\|^{2}\|\mathbf{X}_{i,:}^{\ddagger}\|^{ 2},\text{where }\mathrm{Var}\left[\mathbf{X}\right]:=\mathbb{E}\left[\|\mathbf{X}-\mathbb{E} \mathbf{X}\|\right]_{F}^{2}.\]

The coefficient \(c_{i}:=\|\nabla_{\mathbf{Y}_{i};i}^{\ddagger}\|\|\mathbf{X}_{i,:}^{\ddagger}\|\) is called the _leverage score_, which can be easily computed in low time complexity. When \(p_{i}\propto c_{i}\), the variance attends its minimum due to Cauchy inequality:

\[\sum_{i=1}^{2N}\frac{1}{p_{i}}\|\nabla_{\mathbf{Y}_{i};i}^{\ddagger}\|^{2}\| \mathbf{X}_{i,:}^{\ddagger}\|^{2}=\sum_{i=1}^{2N}\frac{c_{i}^{2}}{p_{i}}=\sum_{ i=1}^{2N}\frac{c_{i}^{2}}{p_{i}}\sum_{i=1}^{2N}p_{i}\geq(\sum_{i=1}^{2N}c_{i})^{2},\]

where the equality holds when \(p_{i}\propto c_{i}\). Intuitively, LSS can approximate the MM Eq. (7) well with significantly lower computational cost when the leverage scores \(\{c_{i}\}\) are diverse, which is indeed the case as shown in Fig. 2.

Define \(\mathbf{M}^{\ddagger}\) to be the top-left \(N\times N\) submatrix of \(\tilde{\mathbf{M}}\) and \(\mathbf{M}^{\ddagger}\) to be the bottom-right one, we have

\[\nabla_{\mathbf{Y}}^{\ddagger}{}^{\top}\tilde{\mathbf{M}}\mathbf{X}^{\ddagger} =s_{\uparrow}\nabla_{\mathbf{Y}}^{\uparrow}{}^{\top}\tilde{\mathbf{M}}^{\ddagger} \hat{\mathbf{X}}+s_{\downarrow}\nabla_{\mathbf{Y}}^{\downarrow}{}^{\top}\tilde{ \mathbf{M}}^{\ddagger}\hat{\mathbf{X}},\]which can be implemented by two INT4 MMs with sampled rows/columns. Putting everything together, we propose the following MM procedure to compute the weight gradient:

**Procedure** LSS-MM

1. Quantize \(\nabla_{\mathbf{Y}}\) with BS to obtain \(\nabla_{\mathbf{Y}}^{\dagger}\) and \(\nabla_{\mathbf{Y}}^{\dagger}\) in INT4.
2. Compute the leverage score \(\|\nabla_{\mathbf{Y}_{i,:}}^{\dagger}\|\|\mathbf{X}_{i,:}^{\ddagger}\|\) in FP16.
3. Sample the masks \(\{m_{i}\}\).
4. Sample rows of \(\nabla_{\mathbf{Y}}\) and \(\hat{\mathbf{X}}\) given the masks \(\{m_{i}\}\).
5. Compute INT4 MMs \(\nabla_{\mathbf{Y}}^{\dagger}\,\tilde{\mathbf{M}}^{\dagger}\hat{\mathbf{X}}\) and \(\nabla_{\mathbf{Y}}^{\dagger}\,\tilde{\mathbf{M}}^{\dagger}\hat{\mathbf{X}}\),
6. Dequantize and sum up the resultant INT32 matrices to obtain the FP16 result \(\nabla_{\mathbf{Y}}^{\ddagger\,\top}\,\tilde{\mathbf{M}}\mathbf{X}^{\ddagger}\).

As \(\tilde{\mathbf{M}}\) only has \(N\) non-zero elements in expectation, the two matrix multiplications in Step 5 take about \(2NCD\) INT4 MACs, which aligns with the cost of the naive MM \(s_{\uparrow}\nabla_{\mathbf{Y}}^{\dagger}\hat{\mathbf{X}}\). The overhead of all the other steps is \(O(NC+ND)\) in total.

Activation GradientSimilar to the previous discussion, the gradient of input can be written as

\[\nabla_{\mathbf{Y}}\,\hat{\mathbf{W}}\approx(s_{\uparrow}\nabla_{\mathbf{Y}}^ {\dagger}+s_{\downarrow}\nabla_{\mathbf{Y}}^{\downarrow})\hat{\mathbf{W}}=s_{ \uparrow}\nabla_{\mathbf{Y}}^{\dagger}\,\hat{\mathbf{W}}+s_{\downarrow}\nabla_ {\mathbf{Y}}^{\downarrow}\,\hat{\mathbf{W}}=\left(\hat{\mathbf{I}}^{\ddagger} \nabla_{\mathbf{Y}}^{\ddagger}\right)\hat{\mathbf{W}},\] (8)

where we define \(\nabla_{\mathbf{Y}}^{\ddagger}=[s_{\uparrow}\nabla_{\mathbf{Y}}^{\dagger};s_{ \downarrow}\nabla_{\mathbf{Y}}^{\downarrow}]\in\mathbb{R}^{2N\times C}\) and \(\hat{\mathbf{I}}^{\ddagger}=[\mathbf{I}\quad\mathbf{I}]\) to be a \(N\times N\) INT4 matrix, \(\mathbf{I}\) is a \(N\times N\) identity matrix. The original product can also be implemented by two INT4 MMs \(\nabla_{\mathbf{Y}}^{\ddagger}\hat{\mathbf{W}}\) and \(\nabla_{\mathbf{Y}}^{\ddagger}\hat{\mathbf{W}}\). But different from weight gradients, we now focus on \(\hat{\mathbf{I}}^{\ddagger}\nabla_{\mathbf{Y}}^{\ddagger}\) in Eq. (8) and do leverage score sampling on this MM. A detailed discussion can be found in B.2, and we only present the leverage score here. Similarly, we write the MM as the sum of \(2N\) smaller multiplications:

\[\hat{\mathbf{I}}^{\ddagger}\nabla_{\mathbf{Y}}^{\ddagger}=\sum_{i=1}^{2N}\hat {\mathbf{I}}_{:,i}^{\ddagger}\nabla_{\mathbf{Y}i}^{\ddagger}\approx\frac{m_{ i}}{p_{i}}\sum_{i=1}^{2N}\nabla_{\mathbf{Y}_{i}},\]

where we define \(\nabla_{\mathbf{Y}_{i}}=\hat{\mathbf{I}}_{:,i}^{\ddagger}\nabla_{\mathbf{Y} }^{\ddagger}\) and associate the probability \(p_{i}\) and Bernoulli mask \(m_{i}\sim\mathsf{Bern}(p_{i})\) with the \(i\) multiplication. The leverage score for activation gradient is \(c_{i}:=\|\nabla_{\mathbf{Y}_{i}}^{\ddagger}\|\), and the variance attains minimum when \(p_{i}\propto c_{i}\). More details about the algorithm can be found at Appendix. A.3 On the implementation side, once the mask \(\{m_{i}\}\) is known, we can decompose the MM Eq. (8) as two INT4 MMs: \(\left(\hat{\mathbf{I}}^{\ddagger}\tilde{\mathbf{M}}\nabla_{\mathbf{Y}}^{ \ddagger}\right)\hat{\mathbf{W}}=s_{\uparrow}\tilde{\mathbf{M}}^{\dagger} \nabla_{\mathbf{Y}}^{\dagger}\,\hat{\mathbf{W}}+s_{\downarrow}\tilde{\mathbf{M }}^{\dagger}\nabla_{\mathbf{Y}}^{\ddagger}\hat{\mathbf{W}}\).

## 5 Experiments

We evaluate our INT4 training algorithm on a wide variety of tasks including language model fine-tuning, machine translation, and image classification. We implement our proposed HQ-MM and

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline \multirow{2}{*}{Dataset} & \multirow{2}{*}{Train type} & \multirow{2}{*}{Model} & \multirow{2}{*}{Metric same} & \multicolumn{2}{c}{Baselines} & \multicolumn{2}{c}{4-HT training methods} \\ \cline{5-8}  & & & & FP & INT8 & LSO+LUO & HQ+LSS \\ \hline GLUE-dev & FT & Bert-base & Avg & \(82.67_{0.24}\) & \(81.45_{0.13}\) & \(75.29_{0.23}\) & \(\mathbf{80.81_{0.21}}\) \\ \hline SQUAD v1 & FT & Bert-base & Avg & \(84.57_{0.42}\) & \(82.74_{0.24}\) & \(55.93_{2.47}\) & \(\mathbf{82.25_{0.58}}\) \\ \hline SQUAD v2 & FT & Bert-base & F1 & \(88.32_{0.30}\) & \(88.42_{0.20}\) & \(85.75_{0.31}\) & \(\mathbf{87.60_{0.23}}\) \\ \hline SQUAD v2 & FT & Bert-base & F1 & \(76.04_{0.68}\) & \(75.63_{0.07}\) & \(71.02_{0.41}\) & \(\mathbf{74.63_{0.18}}\) \\ \hline Adversarial QA & FT & Bert-base & F1 & \(40.99_{0.38}\) & \(40.17_{0.58}\) & \(31.85_{0.30}\) & \(\mathbf{38.70_{0.77}}\) \\ \hline SWAG & FT & Bert-base & Acc & \(79.84_{0.10}\) & \(79.18_{0.19}\) & \(70.79_{1.20}\) & \(\mathbf{77.49_{0.16}}\) \\ \hline CONLL & FT & Bert-base & Acc & \(53.38_{0.68}\) & \(93.13_{0.14}\) & \(87.63_{0.39}\) & \(\mathbf{91.00_{0.48}}\) \\ \hline WMT & PT & Transformer-base & BLEU & \(27.5\) & \(25.4\)(UNTA Low) & \(27.17\) & \(-\) \\ \hline \multirow{2}{*}{CIFAR10} & FT & ViT-B/32 & \multirow{2}{*}{Top1 Acc} & \multirow{2}{*}{\(98.77_{0.98}\)} & \multirow{2}{*}{\(98.59_{0.02}\)} & \multirow{2}{*}{\(97.76_{0.10}\)} & \multirow{2}{*}{\(98.36_{0.05}\)} \\  & & ViT-B/32 & & & & & \\ \hline \multirow{2}{*}{CIFAR100} & FT & ViT-B/32 & \multirow{2}{*}{Top1 Acc} & \multirow{2}{*}{\(99.98_{0.98}\)} & \multirow{2}{*}{\(98.76\)} & \multirow{2}{*}{\(88.63_{0.05}\)} & \multirow{2}{*}{\(\mathbf{98.47}\)} \\  & & ViT-B/32 & & & & & \\ \hline \multirow{2}{*}{CIFAR100} & FT & ViT-B/32 & \multirow{2}{*}{Top1 Acc} & \multirow{2}{*}{\(91.94_{0.11}\)} & \multirow{2}{*}{\(99.99_{0.97}\)} & \multirow{2}{*}{\(88.63_{0.05}\)} & \multirow{2}{*}{\(\mathbf{99.78_{0.09}}\)} \\  & & ViT-L/32 & & & & & \\ \hline \multirow{2}{*}{ImageNet1k} & FT & ViT-B/32 & \multirow{2}{*}{Top1 Acc} & \multirow{2}{*}{\(81.82\)} & \multirow{2}{*}{\(70.725\)} & \multirow{2}{*}{\(\mathbf{79.18}\)} \\  & & ViT-L/32 & & & & & \\ \hline \multirow{2}{*}{ImageNet1k} & FT & ViT-L/32 & \multirow{2}{*}{Top1 Acc} & \multirow{2}{*}{\(81.62\)} & \multirow{2}{*}{\(81.3\)} & \multirow{2}{*}{\(77.41\)} & \multirow{2}{*}{\(\mathbf{80.06}\)} \\  & & ViT-L/16 & & & & & & \\ \hline \multirow{2}{*}{PT} & Dett-Small & \multirow{2}{*}{Top1 Acc} & \multirow{2}{*}{\(73.1\)} & \multirow{2}{LSS-MM algorithms with CUDA and cutlass2, and the implementation details can be found in Appendix A. We replace all the floating-point linear operators with our INT4 implementation except simply using LSQ for embedding layers, and leaving the last classifier layer in full precision. We adopt default architectures, optimizers, schedulers, and hyper-parameters for all the evaluated models.

Footnote 2: https://github.com/ NVIDIA/cutlass

### Converged Model Accuracy

We compare the accuracy of the converged model on various tasks in Table 1. The compared methods include full-precision training (FP), INT8 training [3](INT8), FP4 training [47] ("Ultra-low"), 4-bit logarithm quantization [8] with LSQ for activations and weights (LSQ+LUQ), and our algorithm which utilizes HQ for forward and LSS for backpropagation (HQ+LSS). Ultra-low does not have a public implementation, so we only report its performance from its original paper on the machine translation task. Except for the large machine translation task and the task of large vision transformers, we repeat each run by three times and report the standard deviation as subscripts in tables. We do not include any kind of knowledge distillation or data augmentation.

Language model fine-tuning: We use the pretrained BERT-base-uncased and BERT-large-uncased [24] model, and evaluate the performance of our method on GLUE dev-set [53], SQUAD [41], SQUADv2 [40], Adversarial QA [4], CoNLL-2003 [42] and SWAG [61] datasets. We present the average result of the Bert-base-uncased and bert-large-uncased models on the GLUE dataset. The full results are listed in Appendix C.2. Compared with LSQ+LUQ, our method achieves \(5.5\%\) improvement of accuracy on average for the Bert-base model and achieves \(>25\%\) improvement of accuracy on average for the Bert-large model. We further show the result on the SQUAD, SQUAD 2.0, Adversarial QA, CoNLL-2003, and SWAG datasets. On all of the tasks, compared with LSQ+LUQ, our method achieves better performance. We improve by \(1.8\%\) and \(3.6\%\) on SQUAD and SQUAD 2.0 compared to LSQ+LUQ, respectively. On the more difficult Adversarial QA, we improve by \(6.8\%\) on F1 score. On SWAG we improve by \(6.7\%\) and on CoNLL-2003 we improve by \(4.2\%\) accuracy.

Machine translation: We also apply our method for pretraining. We train a Transformer-base [52] model on WMT 14 En-De dataset [6] for machine translation. Note that we reproduce this experiment with Fairseq's recipe 3, which reports the SacreBleu score (26.5 for FP) [37], while Ultra-low and LUQ report the more optimistic original BLEU score (27.5 for FP) [36]. Our HQ+LSS has about \(1.0\%\) BLEU degradation, which is smaller than \(2.1\%\) of Ultra-low and higher than \(0.3\%\) reported in the LUQ paper. Nevertheless, HQ+LSS still performs comparably with existing methods for this pretraining task, and it supports contemporary hardware.

Footnote 3: https://github.com/facebookresearch/fairseq

Image Classification: We load ViT checkpoints pretrained on ImageNet21k [13], and fine-tune it on CIFAR-10, CIFAR-100 [28], and ImageNet1k. We use ViT-B/32 and ViT-L/32 for CIFAR datasets and use ViT-B/32, ViT-L/32 and ViT-L/16 for ImageNet1k. On CIFAR10 we achieve \(<0.5\%\) accuracy degradation, while LSQ+LUQ has \(1\%\) degradation for ViT-B/32 and \(0.6\%\) degradation for ViT-L/32. On CIFAR100, INT8 already has \(\sim 1\%\) accuracy degradation, which shows its difficulty. We improve by \(1.1\%\) accuracy for ViT-B/32 and \(0.2\%\) accuracy for ViT-L/32 compared with LSQ+LUQ. On ImageNet1k, we improve by \(2\%\) accuracy for ViT-B/32, \(2.6\%\) accuracy for ViT-L/32 and \(0.2\%\) for ViT-L/32 compared with LSQ+LUQ. We further test the effectiveness of our algorithm for pretraining a DeiT-Small model [51] on ImageNet1K, where HQ+LSS can still converge to similar accuracy level compared to LSQ+LUQ, while being more hardware friendly.

### Ablation Study

Here, we conduct ablation studies to show the effectiveness of our forward and backward methods independently on the challenging CoLA dataset. To study the effectiveness of different quantizers for forward propagation, we leave backpropagation in FP16. The result is shown in Fig. 3(a). We first validate the claim in Sec. 3.2 that outliers are the main cause of accuracy degradation in quantized forward propagation. We test an "outlier" method which maintains \(1\%\) largest activation entries in FP. The "outlier" method achieves good performance, which proves that outliers are indeed the most significant challenge of the transformer's forward quantization. The hardware-unfriendly "outlier" method serves as an upper bound of methods to handle outliers. Our HQ outperforms LSQ by better handling the outliers and achieves comparable results to maintaining the outliers.

We also investigated whether more granular quantizers, such as per-token quantization or per-channel quantization could be used to quantify outliers, or whether existing methods like SmoothQuant [58] could be used for INT4 FQT. The results are listed in C.3, and we find that without HQ, none of these methods achieve good accuracy under 4-bit quantization, and the result of HQ is not strongly affected when more granular quantization methods are applied.

For backpropagation, we compare a simple minimax quantizer [3], LUQ [8] and our LSS, and leave forward propagation in FP16. The minimax quantizer divides the numerical range from the minimum to the maximum into equally large quantization bins. The result is shown in Fig. 3(b). While the bit-width is higher than 2, our LSS achieves results that are comparable and even slightly higher than LUQ. Meanwhile, LSS is more hardware friendly as it requires only INT4 arithmetic.

### Computational and Memory Efficiency

Finally, we demonstrate the potential of our method to accelerate neural network training by evaluating our prototypical implementation discussed in Appendix A.6. We emphasize that our implementation is not fully optimized. For example, the backward computation requires an INT4 MM in the form of \(\mathbf{Y}=\mathbf{A}\mathbf{B}\), while cutlass only supports \(\mathbf{Y}=\mathbf{A}\mathbf{B}^{\top}\), so explicit transpose is required. We also do not fuse the linear operators with nonlinearities and normalizations. Therefore, the results cannot fully reflect the potential of INT4 training algorithms. A fully optimized implementation requires heavy engineering, which exceeds the scope of our paper.

Operator Speed:We compare the throughput of our proposed HQ-MM (HQ), LSS for computing weight gradient (LSSWeight), LSS for computing activation gradient (LSSAct), and their average throughput (INT4) with a baseline tensor-core FP16 GEMM implementation (FP16) provided by cutlass in Fig. 4 on an Nvidia RTX 3090 GPU which has a peak throughput at 142 FP16 TFLOPs and 568 INT4 TFLOPs. As the matrix size grows, the overhead of quantization diminishes and our INT4 operators can be up to 2.2 times faster compared with FP16 MM. We further analyze the quantization overhead for each operator in Appendix C.5.

Training Throughput:We compare the training throughput of the FP16 PyTorch AMP and our INT4 training algorithm for training BERT [24] and GPT [38]-style language models on a system of 8 Nvidia A100 GPUs. We vary the hidden layer size, intermediate fully-connected layer size, and batch size, and plot the speedup of INT4 training in Fig. 5. Our INT4 training algorithm can achieve up to 35.1% speedup and an average of 15.8 % for BERT-style models and up to 26.5% speedup and an average of 19.2 % for GPT-style models. The training time can be found in Appendix C.4.

Inference Speed:We compare the inference speed of our algorithm with I-BERT [25] by comparing the speedup numbers reported in its original paper. Following the I-BERT paper, we compare the speedup of integer-based inference algorithms relative to a FP32 baseline on an Nvidia T4 GPU on the BERT-base and BERT-large models, and test with sequence lengths of 128 and 256. While I-BERT only reported speedup numbers for batch sizes 1, 2, 4, 8, we test the speedup for batch sizes ranging from 1 to 1024 to better reflect the performance of throughput-oriented scenarios (such as a cloud language model service provider).

In Table 2 we report the speed up result. While I-BERT's speedup numbers seems to be insensitive to the batch size and sequence length, our speedup increases with the batch size. I-BERT shows up to 3.98x speedup for smaller batch size, while our algorithm can achieve higher speedup for batch sizes higher than 64, and eventually gives a speedup of 6.48x for a sequence length of 128 and a batch size of 1024.

For the BERT-base model, our method shows an inference speed improvement of 3.57-4.92 times compared to FP32 for batch sizes larger than 64. In comparison, I-BERT achieved an inference speed improvement of 2.42-3.39 times compared to FP32 when the batch size was small. For BERT-large, our method shows an inference speed improvement of 4.81-6.48 times compared to FP32 for batch sizes larger than 64. In comparison, I-BERT achieved an inference speed improvement of 3.20-4.00 times compared to FP32 when the batch size is small. Therefore, our algorithm can potentially achieve higher throughput than I-BERT.

## 6 Conclusions

We propose a hardware-friendly INT4 training method for transformers. By analyzing the properties of MMs in transformers, we propose HQ and LSS methods to quantize activations and gradients while preserving accuracy. On several important tasks, our method performs comparably or better than existing INT4 methods. Our work can be potentially extended beyond transformers to other MM-only architectures, such as MLP-Mixer [50], graph neural networks [26], and recurrent neural networks [20]. We leave it as a future direction.

**Broader Impacts:** Our algorithm can improve efficiency and reduce the energy consumption of training neural networks, which helps reduce the carbon footprint caused by deep learning. However, our efficient training algorithm might also facilitate the development of large language models with safety concerns for human beings; and malicious AI applications such as fake content generation.

**Limitations:** Our INT4 training algorithm does not support convolutional layers. Moreover, our algorithm cannot yet work well for those extremely large models such as OPT-175B. LSS utilizes gradient sparsity, and may not work well for certainty pretraining tasks. Finally, our algorithm may incur more memory accesses than FP16 training algorithms, which might affect the speedup.

## Acknowledgements

The authors would like to thank Weilin Zhao, Sixuan Ma, Xiaoxuan Liu, Ziteng Wang, Bingrui Li, Cheng Lu, and Zhiyuan Liu for valuable discussions and help on the training of large-scale language models. This work was supported by the National Key Research and Development Program of China (No. 2021ZD0110502), NSFC Projects (Nos. 62061136001, 62106123, 62076147, U19A2081, 61972224, 62106120), Tsinghua Institute for Guo Qiang, and the High Performance Computing Center, Tsinghua University. J.Z is also supported by the XPlorer Prize.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c} \hline \hline  & & & & & & & & Batch Size & & & & & \\ \cline{3-16} SL & Model & Method & 1 & 2 & 4 & 8 & 16 & 32 & 64 & 128 & 256 & 512 & 1024 \\ \hline \multirow{3}{*}{128} & bert-base & I-BERT & 2.42 & 3.36 & 3.39 & 3.31 & - & - & - & - & - & - & - \\  & Ours & 0.17 & 0.25 & 0.46 & 0.87 & 1.22 & 2.07 & **3.61** & **3.57** & **4.5** & **4.54** & **4.92** \\ \cline{1-1}  & bert-large & I-BERT & 3.2 & 4 & 3.98 & 3.81 & - & - & - & - & - & - & - \\  & Ours & 0.22 & 0.43 & 0.81 & 1.46 & 2.13 & 3.34 & **4.28** & **4.81** & **5.4** & **6.08** & **6.48** \\ \hline \multirow{3}{*}{256} & bert-base & 1-BERT & 3.11 & 2.96 & 2.94 & 3.15 & - & - & - & - & - & - & - \\  & Ours & 0.25 & 0.54 & 0.95 & 1.53 & 1.94 & **3.2** & **3.76** & **4** & **4.15** & **4.14** \\ \cline{1-1}  & bert-large & I-BERT & 3.19 & 3.51 & 3.37 & 3.4 & - & - & - & - & - & - & - \\ \cline{1-1}  & Ours & 0.45 & 0.86 & 1.47 & 2.44 & 2.67 & **3.99** & **4.87** & **5.24** & **5.11** & **5.41** & **OOM** \\ \hline \hline \end{tabular}
\end{table}
Table 2: The inference speed up compared with I-BERT. Since the open-sourced versions of I-BERT are based on fake quantization, we compare the inference speed of our algorithm with I-BERT by comparing the speedup numbers reported in its original paper with respect to FP32.

## References

* [1] Menachem Adelman and Mark Silberstein. Faster neural network training with approximate tensor operations. _arXiv preprint arXiv:1805.08079_, 2018.
* [2] Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael Lyu, and Irwin King. Binarybert: Pushing the limit of bert quantization. _arXiv preprint arXiv:2012.15701_, 2020.
* [3] Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry. Scalable methods for 8-bit training of neural networks. In _Advances in Neural Information Processing Systems_, pages 5145-5153, 2018.
* [4] Max Bartolo, Alastair Roberts, Johannes Welbl, Sebastian Riedel, and Pontus Stenetorp. Beat the ai: Investigating adversarial human annotation for reading comprehension. _Transactions of the Association for Computational Linguistics_, 8:662-678, 2020.
* [5] Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. _arXiv preprint arXiv:1308.3432_, 2013.
* [6] Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, et al. Findings of the 2014 workshop on statistical machine translation. In _Proceedings of the ninth workshop on statistical machine translation_, pages 12-58, 2014.
* [7] Jianfei Chen, Yu Gai, Zhewei Yao, Michael W Mahoney, and Joseph E Gonzalez. A statistical framework for low-bitwidth training of deep neural networks. In _Advances in neural information processing systems_, 2020.
* [8] Brian Chmiel, Ron Banner, Elad Hoffer, Hilla Ben Yaacov, and Daniel Soudry. Logarithmic unbiased quantization: Practical 4-bit training in deep learning. _arXiv preprint arXiv:2112.10769_, 2021.
* [9] Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan. Pact: Parameterized clipping activation for quantized neural networks. _arXiv preprint arXiv:1805.06085_, 2018.
* [10] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. In _International Conference on Learning Representations_, 2020.
* [11] Zhen Dong, Zhewei Yao, Yaohui Cai, Daiyaan Arfeen, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Hawq-v2: Hessian aware trace-weighted quantization of neural networks. _arXiv preprint arXiv:1911.03852_, 2019.
* [12] Zhen Dong, Zhewei Yao, Amir Gholami, Michael Mahoney, and Kurt Keutzer. Hawq: Hessian aware quantization of neural networks with mixed-precision. _ICCV_, 2019.
* [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.
* [14] Petros Drineas and Michael W Mahoney. Randnla: randomized numerical linear algebra. _Communications of the ACM_, 59(6):80-90, 2016.
* [15] Mario Drumond, LIN Tao, Martin Jaggi, and Babak Falsafi. Training dnns with hybrid block floating point. In _Advances in Neural Information Processing Systems_, pages 453-463, 2018.
* [16] Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S Modha. Learned step size quantization. _arXiv preprint arXiv:1902.08153_, 2019.
* [17] Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with structured dropout. In _International Conference on Learning Representations_, 2019.

* [18] Pierre Foret, Ariel Kleiner, Hossein Moahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. _arXiv preprint arXiv:2010.01412_, 2020.
* [19] Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin, Fengwei Yu, and Junjie Yan. Differentiable soft quantization: Bridging full-precision and low-bit neural networks. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4852-4861, 2019.
* [20] Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. _Neural computation_, 9(8):1735-1780, 1997.
* [21] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth. In _European conference on computer vision_, pages 646-661. Springer, 2016.
* [22] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Efficient training of giant neural networks using pipeline parallelism. _Advances in neural information processing systems_, 32, 2019.
* [23] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 2704-2713, 2018.
* [24] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In _Proceedings of NAACL-HLT_, pages 4171-4186, 2019.
* [25] Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. I-bert: Integer-only bert quantization. In _International conference on machine learning_, pages 5506-5518. PMLR, 2021.
* [26] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. _arXiv preprint arXiv:1609.02907_, 2016.
* [27] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In _International Conference on Learning Representations_, 2019.
* [28] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. _Technical report_, 2009.
* [29] Hamed F Langroudi, Zachariah Carmichael, and Dhireesha Kudithipudi. Deep learning training on the edge with low-precision posits. _arXiv preprint arXiv:1907.13216_, 2019.
* [30] Hamed F Langroudi, Zachariah Carmichael, David Pastuch, and Dhireesha Kudithipudi. Cheetah: Mixed low-precision hardware & software co-design framework for dnns on the edge. _arXiv preprint arXiv:1908.02386_, 2019.
* [31] Zechun Liu, Zhiqiang Shen, Shichao Li, Koen Helwegen, Dong Huang, and Kwang-Ting Cheng. How do adam and training strategies help bnnns optimization. In _International Conference on Machine Learning_, pages 6936-6946. PMLR, 2021.
* [32] Zechun Liu, Zhiqiang Shen, Marios Savvides, and Kwang-Ting Cheng. Reactnet: Towards precise binary neural network with generalized activation functions. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XIV 16_, pages 143-159. Springer, 2020.
* [33] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. In _International Conference on Learning Representations_, 2018.
* [34] Preetum Nakkiran, Gal Kaplan, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep double descent: Where bigger models and more data hurt. _Journal of Statistical Mechanics: Theory and Experiment_, 2021(12):124003, 2021.

* [35] Nvidia. Transformer Engine. https://github.com/NVIDIA/TransformerEngine, 2023. Online; accessed 23 January 2023.
* [36] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In _Proceedings of the 40th annual meeting of the Association for Computational Linguistics_, pages 311-318, 2002.
* [37] Matt Post. A call for clarity in reporting bleu scores. _arXiv preprint arXiv:1804.08771_, 2018.
* [38] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. _OpenAI Blog_, 1(8):9, 2019.
* [39] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In _SC20: International Conference for High Performance Computing, Networking, Storage and Analysis_, pages 1-16. IEEE, 2020.
* [40] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don't know: Unanswerable questions for squad. _arXiv preprint arXiv:1806.03822_, 2018.
* [41] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. _arXiv preprint arXiv:1606.05250_, 2016.
* [42] Erik F Sang and Fien De Meulder. Introduction to the conll-2003 shared task: Language-independent named entity recognition. _arXiv preprint cs/0306050_, 2003.
* [43] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. _arXiv preprint arXiv:1701.06538_, 2017.
* [44] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Q-bert: Hessian based ultra low precision quantization of bert. _arXiv preprint arXiv:1909.05840_, 2019.
* [45] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Q-bert: Hessian based ultra low precision quantization of bert. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 8815-8821, 2020.
* [46] Xiao Sun, Jungwook Choi, Chia-Yu Chen, Naigang Wang, Swagath Venkataramani, Vijayalakshmi Viji Srinivasan, Xiaodong Cui, Wei Zhang, and Kailash Gopalakrishnan. Hybrid 8-bit floating point (hfp8) training and inference for deep neural networks. In _Advances in Neural Information Processing Systems_, pages 4901-4910, 2019.
* [47] Xiao Sun, Naigang Wang, Chia-Yu Chen, Jiamin Ni, Ankur Agrawal, Xiaodong Cui, Swagath Venkataramani, Kaoutar El Maghraoui, Vijayalakshmi Viji Srinivasan, and Kailash Gopalakrishnan. Ultra-low precision 4-bit training of deep neural networks. In _Advances in Neural Information Processing Systems_, volume 33, 2020.
* [48] James Joseph Sylvester. Lx. thoughts on inverse orthogonal matrices, simultaneous signsuccessions, and tessellated pavements in two or more colours, with applications to newton's rule, ornamental tile-work, and the theory of numbers. _The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science_, 34(232):461-475, 1867.
* [49] Hanlin Tang, Xipeng Zhang, Kai Liu, Jianchen Zhu, and Zhanhui Kang. Mkq-bert: Quantized bert with 4-bits weights and activations. _arXiv preprint arXiv:2203.13483_, 2022.
* [50] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlpmixer: An all-mlp architecture for vision. _Advances in neural information processing systems_, 34:24261-24272, 2021.
* [51] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In _International conference on machine learning_, pages 10347-10357. PMLR, 2021.

* [52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [53] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. _arXiv preprint arXiv:1804.07461_, 2018.
* [54] Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. Training deep neural networks with 8-bit floating point numbers. In _Advances in Neural Information Processing Systems_, pages 7675-7684, 2018.
* [55] Zheng Wang, Juncheng B Li, Shuhui Qu, Florian Metze, and Emma Strubell. Squat: Sharpness-and quantization-aware training for bert. _arXiv preprint arXiv:2210.07171_, 2022.
* [56] Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression: Pushing the limit of low-bit transformer language models. _arXiv preprint arXiv:2209.13325_, 2022.
* [57] Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi. Training and inference with integers in deep neural networks. In _International Conference on Learning Representations_, 2018.
* [58] Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. _arXiv preprint arXiv:2211.10438_, 2022.
* [59] Yukuan Yang, Lei Deng, Shuang Wu, Tianyi Yan, Yuan Xie, and Guoqi Li. Training high-performance and large-scale deep neural networks with full 8-bit integers. _Neural Networks_, 125:70-82, 2020.
* [60] Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. QSbert: Quantized 8bit bert. In _2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS)_, pages 36-39. IEEE, 2019.
* [61] Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. Swag: A large-scale adversarial dataset for grounded commonsense inference. _arXiv preprint arXiv:1808.05326_, 2018.
* [62] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. _Communications of the ACM_, 64(3):107-115, 2021.
* [63] Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua. LQ-Nets: Learned quantization for highly accurate and compact deep neural networks. In _The European Conference on Computer Vision (ECCV)_, September 2018.
* [64] Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, and Qun Liu. Ternarybert: Distillation-aware ultra-low bit bert. _arXiv preprint arXiv:2009.12812_, 2020.
* [65] Xishan Zhang, Shaoli Liu, Rui Zhang, Chang Liu, Di Huang, Shiyi Zhou, Jiaming Guo, Yu Kang, Qi Guo, Zidong Du, et al. Adaptive precision training: Quantify back propagation in neural networks with fixed-point numbers. _arXiv preprint arXiv:1911.00361_, 2019.
* [66] Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and Zhiru Zhang. Improving neural network quantization without retraining using outlier channel splitting. In _International conference on machine learning_, pages 7543-7552. PMLR, 2019.
* [67] Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. Incremental network quantization: Towards lossless cnns with low-precision weights. _International Conference on Learning Representations_, 2017.
* [68] Feng Zhu, Ruihao Gong, Fengwei Yu, Xianglong Liu, Yanfei Wang, Zhelong Li, Xiuqi Yang, and Junjie Yan. Towards unified int8 training for convolutional neural network. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1969-1979, 2020.

Implementation Details

In this section, we present some works that need to be done to actually accelerate the training process on hardware.

### BMM in Attention

In attention, there are batch matrix multiplications (BMMs) that need to be dealt with. We now show that our method for MMs can be extended to BMMs.

Consider the following BMM product:

\[\mathbf{T}=\text{BMM}(\mathbf{Q},\mathbf{K}^{\top}),\]

where we define \(\mathbf{T}\in\mathbb{R}^{B\times N\times P},\mathbf{Q}\in\mathbb{R}^{B\times N \times M},\mathbf{K}\in\mathbb{R}^{B\times P\times M}.\) The Hadamard matrix is defined as :

\[\hat{\mathbf{H}}=\text{Repeat}_{B}(\mathbf{H})=\text{Repeat}_{B}(\text{BlockDiag}( \mathbf{H}_{k},\dots,\mathbf{H}_{k})),\]

where \(\hat{\mathbf{H}}\in\mathbb{R}^{B\times M\times M},\mathbf{H}\in\mathbb{R}^{M \times M},\mathbf{H}_{k}\in\mathbb{R}^{2^{k}\times 2^{k}}.\) In this case,

\[\mathbf{T}\approx\text{BMM}\big{(}\text{BMM}(\mathbf{Q},\hat{\mathbf{H}}), \text{BMM}(\mathbf{K},\hat{\mathbf{H}})^{\top}\big{)},\]

which verifies that our HQ can be applied to BMMs.

For backward, the gradient of weight and activation can be calculated by the straight-through estimator \(\lfloor x\rceil^{\prime}=1\) and the chain rule:

\[\nabla_{\mathbf{Q}} =s_{Q}\left(\text{BMM}(\nabla_{\mathbf{T}}^{\top},\hat{\mathbf{K }})\circ\mathbb{I}_{Q}\right)\mathbf{H}^{\top},\] \[\nabla_{\mathbf{K}} =s_{K}\mathbb{I}_{K}\circ\text{BMM}(\nabla_{\mathbf{T}},\hat{ \mathbf{Q}})\mathbf{H}^{\top}=s_{K}\text{BMM}(\mathbb{I}_{K}\circ\nabla_{ \mathbf{T}},\hat{\mathbf{Q}})\mathbf{H}^{\top},\]

where we define \(s_{Q}\in\mathbb{R}^{B},s_{k}\in\mathbb{R}^{B}\) being the batch step size, \(\hat{\mathbf{K}}=\text{int}_{s_{K}}\left(\text{BMM}(\mathbf{K},\hat{\mathbf{ H}})\right)\), \(\hat{\mathbf{Q}}=\text{int}_{s_{Q}}\left(\text{BMM}(\mathbf{Q},\hat{\mathbf{H}})\right)\), \(\mathbb{I}_{Q}=\mathbb{I}(-Q_{N}\leq\mathbf{Q}/s_{Q}\leq Q_{P})\), and \(\mathbb{I}_{K}=\mathbb{I}(-Q_{N}\leq\mathbf{K}/s_{K}\leq Q_{P}).\)

Similar to Sec. 4.2, we only focus on \(\text{BMM}(\nabla_{\mathbf{T}}^{\top},\hat{\mathbf{K}})\) and \(\nabla_{\mathbf{T}}\), since we do leverage sampling on them.

For \(\text{BMM}(\nabla_{\mathbf{T}}^{\top},\hat{\mathbf{K}})\), we define the sample probability \(p_{i}\) and sample the \(\tilde{\mathbf{M}}\) in the same way as MMs. The matrix can be computed as \(\text{BMM}(\text{BMM}(\nabla_{\mathbf{T}}^{\downarrow\top},\hat{\hat{\mathbf{ H}}}),\hat{\mathbf{K}}^{\hat{\mathbf{T}}})\), where \(\hat{\hat{\mathbf{H}}}\) is defined as \(\text{CONCAT}(\tilde{\mathbf{H}}_{1},\cdots,\tilde{\mathbf{H}}_{B})\), \(\nabla_{\mathbf{T}}^{\downarrow\top}\) and \(\hat{\mathbf{K}}^{\hat{\mathbf{T}}}\) follows the same definition of Eq. 6and the leverage score is \(c_{b,i}:=\|\nabla_{\mathbf{T}b,i,\cdot}^{\ddagger}\|\mathbf{K}_{b,i,\cdot}^{ \ddagger}\|\) for \(0\leq b\leq B,0\leq i\leq 2M.\)

For \(\nabla_{\mathbf{T}}\), similarly, can be viewed as \(\nabla_{\mathbf{T}}=\text{BMM}(\hat{\mathbf{T}}^{\dagger},\nabla_{\mathbf{T}}^ {\ddagger}),\)where we define \(\nabla_{\mathbf{Y}}^{\ddagger}=\text{CONCAT}([s_{\uparrow b}\nabla_{\mathbf{T }b}^{\uparrow},s_{\downarrow b}\nabla_{\mathbf{T}b}^{\downarrow}])\in\mathbb{ R}^{B\times 2N\times P}\), \(\hat{\mathbf{I}}^{\ddagger}=\text{CONCAT}([\mathbf{I}\quad\mathbf{I}])\in \mathbb{R}^{B\times N\times 2N}\), \(s_{\uparrow b},\nabla_{\mathbf{T}b}^{\uparrow},s_{\downarrow b},\nabla_{ \mathbf{T}b}^{\downarrow}\) follows the definition of Eq.5. So it can be computed as \(\text{BMM}(\text{BMM}(\hat{\mathbf{T}}^{\ddagger},\tilde{\mathbf{H}}),\nabla_ {\mathbf{T}}^{\ddagger})\), where \(\hat{\hat{\mathbf{H}}}\) is defined as \(\text{CONCAT}(\tilde{\mathbf{H}}_{1},\cdots,\tilde{\mathbf{H}}_{B})\), and the leverage score is \(c_{b,i}:=\|\nabla_{\mathbf{T}b,i,\cdot}^{\ddagger}\|\) for \(0\leq b\leq B,0\leq i\leq 2M,\) which verifies that our LSS can be applied to BMM.

### Computing Leverage Score

In the previous discussion, we find the optimal sample probability \(p_{i}\) that can minimize the variance of the gradient. However, it is likely for the proportional \(p_{i}\) is larger than one, which is invalid for the Bernoulli distribution. Accordingly, we propose an algorithm to solve this issue.

Define the probability array as

\[P=[p_{1}^{0},\cdots,p_{2N}^{0}],\sum_{i=1}^{2N}p_{i}^{0}=N,\]we first clamp the array to \(p_{i}^{1}\in[0,1]\). In this case, \(\sum_{i=1}^{2N}p_{i}^{1}\leq N\), so we scale the \(p_{i}\) which is smaller than 1 to make sure their sum is again \(N\). However, this will probably introduce some more elements larger than 1, so we cycle through the above operations until all the \(p_{i}\in[0,1]\). This process will certainly stop, since if after the scaling operation, no element is larger than 1, then we get a valid distribution. Otherwise, the number larger than 1 is reduced by at least one, thus the process will halt after at most \(O(N)\) times.

### Learning Quantizer Parameters

In this section, we discuss the detail of how to calculate the gradient of activation and quantization step size.

For gradient of activation, the coefficient \(c_{i}:=\|\nabla_{\mathbf{Y}}^{\ddagger}\|\) is the _leverage score_ for activation gradient, and the variance achieves its minimum When \(p_{i}\propto c_{i}\) by the Cauchy Inequality.

Putting everything together, we propose the following MM procedure to compute activation gradient:

**Procedure** LSS-MM

1. Quantize \(\nabla_{\mathbf{Y}}\) with BS to obtain \(\nabla_{\mathbf{Y}}^{\dagger}\) and \(\nabla_{\mathbf{Y}}^{\downarrow}\) in INT4.
2. Compute the leverage score \(\|\nabla_{\mathbf{Y}}^{\ddagger}\|\) in FP16.
3. Sample the masks \(\{m_{i}\}\).
4. Sample rows of \(\nabla_{\mathbf{Y}}\) given the masks \(\{m_{i}\}\).
5. Compute \(\mathbf{I}\hat{\mathbf{M}}^{\dagger}\nabla_{\mathbf{Y}}^{\dagger}\) and \(\mathbf{I}\hat{\mathbf{M}}^{\dagger}\nabla_{\mathbf{Y}}^{\dagger}\) by discard some of its rows.
6. Compute INT4 MMs \(\mathbf{I}\hat{\mathbf{M}}^{\dagger}\nabla_{\mathbf{Y}}^{\dagger}\hat{ \mathbf{W}}\) and \(\mathbf{I}\hat{\mathbf{M}}^{\dagger}\nabla_{\mathbf{Y}}^{\dagger}\hat{ \mathbf{W}}\).
7. Depa quantize and sum up the resultant INT32 matrices to obtain the FP16 result \(\hat{\mathbf{I}}^{\dagger}\nabla_{\mathbf{Y}}^{\ddagger}\hat{\mathbf{W}}\).

The two matrix multiplications in Step 5 take about \(2NCD\) INT4 MACs in expectation.

For the quantization step sizes. Following the chain rule, we have

\[\nabla_{s_{W}}=g(s_{W})\nabla_{\mathbf{Y}}^{\top}\hat{\mathbf{X}}\circ\delta_ {\mathbf{W}}(s_{W}),\;\nabla_{s_{X}}=g(s_{X})\nabla_{\mathbf{Y}}\hat{\mathbf{ W}}\circ\delta_{\mathbf{X}}(s_{X}),\]

where we define \(g(s_{W})=1/\sqrt{Q_{p}N_{W}}\), \(g(s_{X})=1/\sqrt{Q_{p}N_{X}}\), \(N_{W}\) and \(N_{X}\) being the number of elements of weight and activation, \(\delta_{\mathbf{X}}(s_{X})=\text{int}_{s_{X}}\left(\mathbf{X}\right)-\mathbb{ I}_{X}\circ(\mathbf{X}/s_{X})\), and \(\delta_{\mathbf{W}}(s_{W})=\text{int}_{s_{W}}\left(\mathbf{W}\right)-\mathbb{ I}_{W}\circ(\mathbf{W}/s_{W})\).

Notice that for computing \(\nabla_{s_{W}}\) and \(\nabla_{s_{X}}\), the most expensive MMs are \(\nabla_{\mathbf{Y}}^{\top}\hat{\mathbf{X}}\) and \(\nabla_{\mathbf{Y}}\hat{\mathbf{W}}\), which are already calculated through Eq. (7) and Eq. (8) during previous calculations, so it does not require extra computation. The elementwise multiplication with \(\delta_{\mathbf{X}}(s_{X})\) and \(\delta_{\mathbf{W}}(s_{W})\) requires minor computation.

### Cold Start Problem

There is a _cold start problem_. When the model is trained from scratch (i.e., from a random initialization), distributions of weights and activations can change rapidly in the early stage of optimization. In this case, jointly optimizing the quantization step size and the weights would cause the training to be unstable. As a remedy, we do not learn the step size in the first few iterations, and use a heuristic rule to dynamically set the step size for each tensor \(\mathbf{X}\) to \(2\text{mean}(\mathbf{X})/\sqrt{Q_{p}}\) in each iteration. For the WMT experiment, we stop to re-initialize at the second epoch, since the BLEU score is relatively high at the second epoch (about 21, while the final BLEU score after 30 epochs is 25.5). However, for the deit-small pretraining experiment, we re-initialize it at the end of the 10th epoch (we train it for 90 epochs in total), since at the end of the second epoch, the accuracy is not high, but at the end of the 10th epoch, the accuracy is relatively high.

### Choose hadamard matrix size

For the hadamard matrix, let the hadamard matrix to be \(\mathbf{H}\in\mathbb{R}^{D\times D}\): \(\mathbf{H}=\text{BlockDiag}(\mathbf{H}_{k},\dots,\mathbf{H}_{k}),\) where \(D\) is a multiple of \(2^{k}\). We first define

\[\bar{\mathbf{X}}_{k}=s_{X}\text{int}_{s_{X}}\left(\mathbf{X}\mathbf{H}\right) \mathbf{H}^{\top},\quad\bar{\mathbf{W}}=s_{W}\text{int}_{s_{W}}\left(\mathbf{W }\mathbf{H}\right)\mathbf{H}^{\top},\]where \(\bar{\mathbf{X}}\) and \(\bar{\mathbf{W}}\) can be viewed as an approximation of \(\mathbf{X}\) and \(\mathbf{W}\). Then, we define the quantization error to be \(\text{MSE}(\bar{\mathbf{X}},\mathbf{X})\times\text{MSE}(\bar{\mathbf{W}}, \mathbf{W})\). We search for the optimal \(k\) that can minimize this quantization error. For fine-tuning tasks, once the hadamard matrix size has been calculated, we fix it through the training process. For the pre-training task, since the distribution shifts greatly as we train the model, we empirically define a time when we re-initialize the hadamard matrix size and the LSQ step size. Usually, we do this when the first 2 epochs finish.

### GPU Implementation

In the previous discussion, we get to know HQ-MM  and LSS-MM  from an algorithm level, nevertheless it is not enough to actually implement it on hardware. In this section, we will delve deeper into hardware implementation details as well as extra limitations.

HQ-MM  can be divided into 5 parts: Hadamard matrix multiplication, Quantize, Data Pack, INT4 GEMM, and Dequantize.

For the Hadamard matrix multiplication process, since it can be interpreted as a half float matrix multiplication process where the two matrices involved in the operation are input/weight matrix and hadamard matrix, respectively, we implement it in Python, because PyTorch MM uses CublassGemm and is more efficient then CutlassGemm.

In the quantize process, we quantize input/weight into INT4 data respectively, and also preserve a corresponding FP16 version for the LSQ Back Propagation process to use.

In the previous discussion, we assume the quantize part of HQ-MM  is quantizing the resultant matrices to INT4, however, the smallest representation unit of data is INT8. As a result, we actually use INT8 data type to represent quantized data and pack two adjacent data into one data using \((data[1]<<4)|(data[0]\&15)\) in the data packing process, which means we use one INT8 data to represent two adjacent INT4 data. With both input matrices' data packed in this way, we then use cutlass tensor-core INT4 GEMM to do the matrix multiplication.

For the GEMM process, we choose Nvidia CutlassGemm because it's the most efficient open-source operator library we can find. We use INT4 Tensor Core Gemm for our implementation and it requires the two input matrices A&B to be RowMajor and ColMajor, respectively. Since the default Pytorch tensor is RowMajor, we have to use Transpose+Contiguous operations to make it ColMajor, which is very time-consuming and needs further optimization in the future.

Finally, we dequantize the INT GEMM result back into FP16 output using a dequantize kernel, which is the final output of the forward kernel.

As compared, LSS-MM  is more complicated, and can be divided into 7 parts: Quantization of higher lower 4-bit, Leverage Score Calculating, Sampling, Data Pack, INT4 GEMM, Dequantize, and LSQ Back Propagation.

In the Quantize process, we fuse the quantize operation of higher 4-bit and lower 4-bit into a single kernel for acceleration. In the Leverage Score Calculating process, we use the quantized INT8 data to calculate the score and scale up it in the final because integer arithmetic is far more efficient than float arithmetic.

In the sampling process, we sample out rows/columns given the previously calculated leverage score. Note that in Section. A.2, we repeat our proposed algorithm for several loops to sample out specific elements, which is effective but not efficient. According to experiments, however, we notice that simply selecting elements whose leverage score is bigger than 0 can also work well, even better than our proposed algorithm in some cases. So in real quantization implementation, we just sample out rows/ columns whose Euclidean norm is bigger than 0 to accelerate our training process.

Pack, Gemm, and Dequantize processes are as similar as before. It's worth noting that for Int4 Tensor Core Gemm, suppose two input matrices have shape \(M\times K\) and \(K\times N\), \(K\) needs to be a multiple of 32 so that the Tensor core Gemm address can be aligned. We do not need to consider this in the Forward Propagation process because the input data shape always satisfies. However, in the Back Propagation process, the matrix shape may not meet the requirement after sampling. As a result, we need zero_padding the sampled matrix so that \(K\) can be a multiple of 32.

Finally, we utilize the dequantized data to do the LSQ Back Propagation. We also fuse all operations into a single Cuda kernel for acceleration, and the metric remains.

Besides the component of HQ-MM and LSS-MM, there is still something that needs to be mentioned.

1. We omit the Quantization and Leverage Score Calculating process in LSSinput, and use the same value as LSSWeight to accelerate the training process.
2. For Element-Wise kernel, we set block size as 256, grid size as input.numel()/256. For Reduction kernels like sum and min/max, we set block size as 32, grid size as RowNum, reducing elements in each row to the first 32 elements. We find this setting to be most efficient through experiments.

## Appendix B Proofs.

In this section, we present the proofs of the leverage score.

### Proof of Proposition.4.1

**Proposition B.1**.: _(LSS variance for weight gradient)_

\[\mathrm{Var}\left[\sum_{i=1}^{2N}\frac{m_{i}}{p_{i}}\nabla_{\mathbf{Y}_{:,i}}^ {\top}\mathbf{X}_{i}^{\ddagger}\right]=\sum_{i=1}^{2N}\frac{1-p_{i}}{p_{i}} \|\nabla_{\mathbf{Y}_{i,i}}^{\ddagger}\|^{2}\|\mathbf{X}_{i,:}^{\ddagger}\|^{2}.\]

Proof.: \[Var(\nabla_{\mathbf{W}}) =Var\Big{(}\sum_{i=1}^{2N}\frac{1}{p_{i}}(m_{i}{\nabla_{\mathbf{Z} _{:,i}}^{\ddagger}}\mathbf{X}_{i}^{\ddagger})\Big{)}\] \[=Var\Big{(}\sum_{i=1}^{2N}\frac{1}{p_{i}}(\sum_{j=1}^{C}\sum_{k=1} ^{D}m_{i}{\nabla_{\mathbf{Z}_{j,i}}^{\ddagger}}\mathbf{X}_{i,k}^{\ddagger}) \Big{)}\] \[=\sum_{i=1}^{2N}\frac{p_{i}(1-p_{i})}{p_{i}^{2}}Var\Big{(}(\sum_{j =1}^{C}\sum_{k=1}^{D}{\nabla_{\mathbf{Z}_{j,i}}^{\ddagger}}\mathbf{X}_{i,k}^{ \ddagger})\Big{)}\] \[=\sum_{i=1}^{2N}\frac{1-p_{i}}{p_{i}}(\sum_{j=1}^{C}\sum_{k=1}^{D }{\nabla_{\mathbf{Z}_{j,i}}^{\ddagger}}^{2}\mathbf{X}_{i,k}^{\ddagger}).\]

So that

\[Var(\nabla_{\mathbf{W}}) =\sum_{i=1}^{2N}(\frac{1}{p_{i}}-1)(\sum_{j=1}^{C}{\nabla_{ \mathbf{Z}_{j,i}}^{\ddagger}}^{2})(\sum_{k=1}^{D}{\mathbf{X}_{i,k}^{\ddagger} }^{2})\] (9) \[=\sum_{i=1}^{2N}(\frac{1}{p_{i}}-1)\|{\nabla_{\mathbf{Z}_{:,i}}^{ \ddagger}}^{\top}\|^{2}\|\mathbf{X}_{i,:}^{\ddagger}\|^{2},\] (10)

which proves.

### Proof of Activation Leverage Score in Sec.4.2

we divide the matrix multiplication into the sum of \(2N\) smaller multiplications:

\[\hat{\mathbf{I}}^{\ddagger}\nabla_{\mathbf{Y}}^{\ddagger}=\sum_{i=1}^{2N} \hat{\mathbf{I}}_{:,i}^{\ddagger}\nabla_{\mathbf{Y}i}^{\ddagger}=\sum_{i=1} ^{2N}\hat{\nabla}_{\mathbf{Y}_{i}},\] (11)

where we define \(\hat{\nabla}_{\mathbf{Y}_{i}}=\hat{\mathbf{I}}_{:,i}^{\ddagger}\nabla_{ \mathbf{Y}i}^{\ddagger}\).

We assigns each \(\nabla_{\mathbf{Y}_{i}}\) a probability \(p_{i}\in[0,1],i=1,\cdots,2N\), that satisfies \(\sum_{i=1}^{2N}p_{i}=N\). We define random masks \(m_{i}\sim\text{Bern}(p_{i})\), and define \(\tilde{\mathbf{M}}=\text{diag}\left(\frac{m_{1}}{p_{1}},\dots,\frac{m_{2N}}{p_{ 2N}}\right)\), and make an unbiased estimation:

\[\hat{\mathbf{I}}^{\updownarrow}\nabla_{\mathbf{Y}}^{\updownarrow}\approx\hat {\mathbf{I}}^{\updownarrow}\tilde{\mathbf{M}}\nabla_{\mathbf{Y}}^{\updownarrow}= \sum_{i=1}^{2N}\frac{m_{i}}{p_{i}}\nabla_{\mathbf{Y}}^{\updownarrow}.\]

Define \(\mathbf{M}^{\upuparrow}\) to be the top-left \(N\times N\) submatrix of \(\mathbf{M}\) and \(\mathbf{M}^{\updownarrow}\) to be the bottom-right one, we have

\[\hat{\mathbf{I}}^{\updownarrow}\tilde{\mathbf{M}}\nabla_{\mathbf{Y}}^{\updownarrow} =s_{\upuparrow}\tilde{\mathbf{M}}^{\updownarrow}\nabla_{\mathbf{Y}}^{\updownarrow }+s_{\updownarrow}\tilde{\mathbf{M}}^{\updownarrow}\nabla_{\mathbf{Y}}^{ \updownarrow},\]

In this case, \(\tilde{\mathbf{M}}^{\upuparrow}\nabla_{\mathbf{Y}}^{\updownarrow}\) and \(\tilde{\mathbf{M}}^{\updownarrow}\nabla_{\mathbf{Y}}^{\updownarrow}\) both only have parts of its rows being non zero, and the rest rows are zeros since they are discarded. Then, when we multiply it by \(\hat{\mathbf{W}}\), there are half of rows being zeros in \(\tilde{\mathbf{M}}^{\upuparrow}\nabla_{\mathbf{Y}}^{\updownarrow}\hat{ \mathbf{W}}\) and \(\tilde{\mathbf{M}}^{\updownarrow}\nabla_{\mathbf{Y}}^{\updownarrow}\hat{ \mathbf{W}}\). So there's no need to calculate them, and we successfully cut off half of the computation in this case.

Now focus on the variance that

**Proposition B.2**.: _(LSS variance for activation gradient)_

\[\mathrm{Var}\left[\sum_{i=1}^{2N}\hat{\mathbf{I}}^{\updownarrow}_{:,i}\nabla _{\mathbf{Y}^{\updownarrow}}^{\updownarrow}\right]=\sum_{i=1}^{2N}\frac{1-p_{ i}}{p_{i}}\|\nabla_{\mathbf{Y}^{\updownarrow}}^{\updownarrow}\|^{2}.\]

Proof.: \[Var(\nabla\mathbf{x}) =Var\bigg{(}\sum_{i=1}^{2N}\frac{1}{p_{i}}(m_{i}\hat{\mathbf{I}}^ {\updownarrow}_{:,i}\mathbf{X}^{\updownarrow}_{i})\bigg{)}\] \[=Var\bigg{(}\sum_{i=1}^{2N}\frac{1}{p_{i}}(\sum_{j=1}^{C}\sum_{k= 1}^{D}m_{i}\hat{\mathbf{I}}^{\updownarrow}_{j,i}\nabla_{\mathbf{Y}^{\updownarrow },k}^{\updownarrow})\bigg{)}\] \[=\sum_{i=1}^{2N}\frac{p_{i}(1-p_{i})}{p_{i}^{2}}Var\bigg{(}(\sum_ {j=1}^{C}\sum_{k=1}^{D}\hat{\mathbf{I}}^{\updownarrow}_{j,i}\nabla_{\mathbf{Y }^{\updownarrow},k}^{\updownarrow})\bigg{)}\] \[=\sum_{i=1}^{2N}\frac{1-p_{i}}{p_{i}}(\sum_{j=1}^{C}\sum_{k=1}^{D }(\hat{\mathbf{I}}^{\updownarrow}_{j,i})^{2}(\nabla_{\mathbf{Y}^{\updownarrow },k}^{\updownarrow})^{2})\] \[=\sum_{i=1}^{2N}(\frac{1}{p_{i}}-1)\big{(}\sum_{j=1}^{C}(\hat{ \mathbf{I}}^{\updownarrow}_{j,i})^{2})(\sum_{k=1}^{D}(\nabla_{\mathbf{Y}^{ \updownarrow},k}^{\updownarrow})^{2})\] \[=\sum_{i=1}^{2N}(\frac{1}{p_{i}}-1)\|\hat{\mathbf{I}}^{\updownarrow }_{:,i}\|^{2}\|\nabla_{\mathbf{Y}^{\updownarrow}_{i}}^{\updownarrow}\|^{2}\] \[=\sum_{i=1}^{2N}(\frac{1}{p_{i}}-1)\|\nabla_{\mathbf{Y}^{\updownarrow }_{i}}^{\updownarrow}\|^{2}.\]

In this way, the coefficient \(c_{i}:=\|\nabla_{\mathbf{Y}^{\updownarrow}}^{\updownarrow}\|\) is the _leverage score_.

## Appendix C Experiments.

In this section, we present more details for experiments in Sec. 5.

### Experiments setup

For the GLUE, QA, SWAG, and CONLL tasks, we implement our algorithm based on https://github.com/huggingface/transformers. For the machine translation task, we implement our algorithm based on https://github.com/facebookresearch/fairseq. For the ViT fine-tuning task, we implement our algorithm based on https://github.com/jeonsworld/ViT-pytorch. For the deit pretraining task, we implement our algorithm based on https://github.com/facebookresearch/deit.

We employed NVIDIA GeForce RTX 3090 for running most of the experiments, while the NVIDIA A40 was utilized to evaluate the performance of BERT-Large and ViT-L. Furthermore, we conducted runtime measurements using the NVIDIA T4, 3090, and A100 GPUs.

### GLUE results

In this section, we present the detailed result of fine-tuning the GLUE dataset on BERT-base-uncased and BERT-large-uncased.

On BERT-base, on STSB, SST2, QNLI, and QQP, HQ+LSS only has \(<0.5\%\) accuracy degradation. On the most challenging tasks CoLA and RTE, our accuracy degradation is much smaller compared to LSQ+LUQ. On QQP and MNLI, our method achieves \(<1.3\%\) degradation, while LSQ + LUQ has \(\geq 1.8\%\) degradation. The trend is that the more difficult the task is, the more significant our advantage over LSQ+LUQ.

On BERT-large, the improvement is significant. On CoLA, QNLI, and MNLI, the accuracy improvement compared with LSQ+LUQ \(>30\%\). On other datasets like SST2 and QQP, the accuracy improvement is \(>10\%\). On RTE the accuracy improvement is \(>5\%\), and on STSB and MRPC the improvement is \(>3\%\).

We suspect that for those challenging tasks, there is more information stored in the outliers, which results in a larger gap between our method and LSQ+LUQ.

### More Granular Quantization Methods

In this section, in Table 5, we show that the more granular quantization methods, such as per-token quantization and per-channel quantization, or smoothing techniques, such as SmoothQuant, do not work under the 4-bit FQT setting. Meanwhile, combining these methods with HQ will not bring significant improvement.

We find that LSQ is beneficial for all of these more granular quantization methods under low-bit settings, which highlights the importance of LSQ. Meanwhile, we also notice that the smoothquant will even harm the result of LSQ when the bit-width is low. Our explanation is that the motivation of LSQ is to learn a trade-off between outliers and inliers, while smoothquant aims to sacrifice the

Figure 6: Time proportion for each part in HQ-MM and LSS-MM operator.

precision of inliers in order to exactly maintain the information of outliers. When the bitwidth is high, this is not a problem, since there are still enough bits to quantize the inliers. But when the bitwidth is low, such sacrifice will cause severe problems since the inlier information is discarded.

### Large Language Model Operator Speed

In this section, we show that our hardware-friendly INT4 training method can really accelerate the training process on Large Language Models. We run distributed training on a system of 8 A100 cards

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline  & & \multicolumn{4}{c}{Training Methods} \\ \cline{3-6} Model & (hidden\_size, intermediate\_size, batch\_size) & FP16 & HQ+LSS & SpeedUp \\ \hline \multirow{8}{*}{Bert-large} & (2560, 10240, 2048) & 15.094s & 18.949s & \(\mathbf{-25.5\%}\) \\  & (4096, 16384, 1280) & 32.016s & 30.594s & \(\mathbf{4.4\%}\) \\  & (5120, 20480, 960) & 47.418s & 39.482s & \(\mathbf{16.7\%}\) \\  & (7680, 30720, 600) & 95.832s & 67.253s & \(\mathbf{29.8\%}\) \\  & (8960, 35840, 480) & 128.441s & 83.388s & \(\mathbf{35.1\%}\) \\  & (9600, 38400, 160) & 161.114s & 141.325s & \(\mathbf{29.0\%}\) \\  & (12800, 51200, 100) & 326.265s & 255.966s & \(\mathbf{21.5\%}\) \\  & (14400, 57600, 96) & 409.291s & 346.354s & \(\mathbf{15.3\%}\) \\ \hline \multirow{8}{*}{GPT2-base} & (2560, 10240, 1536) & 17.253s & 22.037s & \(\mathbf{-27.7\%}\) \\  & (4096, 16384, 960) & 35.937s & 35.694s & \(\mathbf{-}\) \\ \cline{1-1}  & (5120, 20480, 768) & 52.723s & 46.548s & \(\mathbf{11.7\%}\) \\ \cline{1-1}  & (7680, 30720, 260) & 113.855s & 92.548s & \(\mathbf{18.7\%}\) \\ \cline{1-1}  & (8960, 35840, 200) & 150.680s & 114.881s & \(\mathbf{23.8\%}\) \\ \cline{1-1}  & (9600, 38400, 180) & 172.182s & 126.540s & \(\mathbf{26.5\%}\) \\ \cline{1-1}  & (12800, 51200, 112) & 320.757s & 236.433s & \(\mathbf{26.3\%}\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Experiments on GPT2-base and Bert-large. Total time spent for epoch 1-5 are reported.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline  & & \multicolumn{4}{c}{Quantization Methods} \\ \cline{3-6} Model & Dataset & FP & INT8 & LSQ+LUQ & HQ+LSS \\ \hline \multirow{8}{*}{Bert-base} & CoLA & \(56.89_{0.64}\) & \(56.15_{0.94}\) & \(18.76_{3.58}\) & \(\mathbf{52.46_{1.46}}\) \\  & STSB & \(88.14_{0.73}\) & \(87.05_{0.38}\) & \(84.31_{0.29}\) & \(\mathbf{87.77_{0.30}}\) \\  & RTE & \(64.80_{1.26}\) & \(62.27_{1.26}\) & \(56.80_{0.92}\) & \(\mathbf{62.45_{1.08}}\) \\  & MRPC & \(88.61_{0.66}\) & \(86.85_{0.76}\) & \(86.23_{0.67}\) & \(\mathbf{86.54_{0.83}}\) \\  & SST2 & \(92.72_{0.06}\) & \(92.37_{0.17}\) & \(90.37_{0.46}\) & \(\mathbf{92.49_{0.29}}\) \\  & QNLI & \(91.52_{0.22}\) & \(90.92_{0.24}\) & \(87.33_{0.48}\) & \(\mathbf{90.53_{0.23}}\) \\  & QQP & \(91.09_{0.11}\) & \(90.57_{0.05}\) & \(89.26_{0.03}\) & \(\mathbf{89.80_{0.05}}\) \\  & MNLI & \(84.52_{0.22}\) & \(84.10_{0.08}\) & \(81.79_{0.18}\) & \(\mathbf{83.59_{0.12}}\) \\  & MNLI-MM & \(84.68_{0.20}\) & \(84.49_{0.31}\) & \(82.22_{0.33}\) & \(\mathbf{83.75_{0.28}}\) \\ \hline \multirow{8}{*}{Bert-large} & CoLA & \(60.33_{0.49}\) & \(58.80_{1.52}\) & \(0.00_{0.00}\) & \(\mathbf{53.46_{1.17}}\) \\  & STSB & \(87.59_{2.39}\) & \(86.53_{0.20}\) & \(83.08_{0.41}\) & \(\mathbf{87.57_{0.78}}\) \\  & RTE & \(71.12_{1.80}\) & \(63.71_{1.26}\) & \(53.06_{0.72}\) & \(\mathbf{64.62_{0.78}}\) \\  & MRPC & \(91.06_{0.28}\) & \(87.57_{1.47}\) & \(82.56_{0.59}\) & \(\mathbf{87.62_{0.51}}\) \\  & SST2 & \(93.98_{0.17}\) & \(93.75_{0.63}\) & \(83.94_{0.69}\) & \(\mathbf{93.52_{0.40}}\) \\  & QNLI & \(92.26_{0.05}\) & \(92.29_{0.29}\) & \(63.18_{13.10}\) & \(\mathbf{91.53_{0.38}}\) \\  & QQP & \(91.04_{0.63}\) & \(90.71_{0.00}\) & \(75.62_{12.44}\) & \(\mathbf{90.77_{0.02}}\) \\  & MNLI & \(86.71_{0.19}\) & \(85.82_{0.08}\) & \(33.42_{1.38}\) & \(\mathbf{85.86_{0.10}}\) \\  & MNLI-MM & \(86.41_{0.35}\) & \(85.87_{0.14}\) & \(33.54_{1.55}\) & \(\mathbf{85.82_{0.07}}\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: GLUE results on BERT-base-uncased and BERT-large uncased. FP refers to full precision training, INT8 refers to INT8 training, LSQ + LUQ refers to learned step size quantization for forward and logarithmic unbiased quantization for backward, and HQ + LSS refers to Hadamard quantization for forward and leverage score sampling for backward.

and our implementation uses distributed data parallel training with zero-3, gradient checkpointing, and optimizer offloading.

We experimented with two architectures: BERT-Large and GPT2-base. We vary the network width and batch size to make full utilization of the GPU memory and show the end-to-end performance for fine-tuning these models on the SuperGLUE RTE dataset in Table 4.

Figure 8: Real quantization performance on Nvidia A100.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline  & \multicolumn{8}{c}{Quantize Bits} \\ \cline{2-9} quantization methods & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\ \hline Per-tensor & 0 & 0 & 0 & 0 & 0 & 50.2 & 54.6 \\ Per-token & 0 & 0 & 0 & 0 & 31.4 & 52.8 & 56 \\ Per-channel & 0 & 0 & 0 & 0 & 0 & 51.9 & 56.7 \\ smoothquant & 0 & 0 & 0 & 0 & 0 & 49.4 & 57.7 \\ Per-token + Per-channel + smoothquant & 0 & 0 & 0 & 0 & 40.7 & 55.7 & 56.7 \\ \hline LSQ & 0 & 9.16 & 24.2 & 37.3 & 39.6 & 45.3 & 51.4 \\ Per-token + LSQ & 0 & 15.3 & 27.8 & 31.6 & 42.9 & 46 & 54.4 \\ Per-channel + LSQ & 0 & 8 & 23.9 & 29.3 & 40 & 45.5 & 50.7 \\ smoothquant + LSQ & 0 & 0 & 0 & 0 & 49.6 & 54.9 & 57 \\ Per-token + Per-channel + smoothquant + LSQ & 0 & 0 & 0 & 0 & 28.8 & 52.4 & 55.2 \\ \hline HQ & 0 & 45.2 & 54.6 & 54.2 & 56.5 & 57.4 & 58.4 \\ HQ + Per-token + Per-channel & 0 & 48.4 & 54.1 & 54.9 & 55 & 56 & 56 \\ HQ + Per-token + Per-channel + smoothquant & 0 & 0 & 46.6 & 54.9 & 55.9 & 55.8 & 56.5 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Comparison of different quantization methods, quantize the activation and weight into the same bit-width from 2 to 8. Per-token refers to quantize activation per-token, while Per-channel refers to quantize weight per-channel.

Figure 7: Real quantization performance on Nvidia T4.

### More experiments on Operator Speed

Time proportionWe examine the proportion of time for each part of computation in HQ-MM and LSS-MM operator in Fig. 6 when the shapes of input matrices vary. In HQ, hadamard means multiplying the input matrix with the Hadamard matrix, pack means packing input data into INT4 data, gemm means the matrix multiplication of two INT4 matrices. In LSSWeight, quantize corresponds to the quantization of higher and lower 4-bit, leverage means computing leverage score, sample means sample out rows/columns given the leverage score, dequantize is the process of dequantizing INT data back into FP16 data, and LSQ is the backpropagation process of LSQ method. In LSSAct, we ignore quantize and leverage process, using the same value as LSSWeight for saving time, other processes share the same meaning with LSSWeight. Note that our implementation is not fully optimized, and optimizations like operator fusion can further improve the performance.

Operator Speed on more GPUsOn an Nvidia RTX 3090 GPU with a Cuda capability of sm_86., we show the comparison of FP16 MM, HQ, and LSS operators in Section 5.3 as well as time proportion in each operator in Figure. 6. We also adjust our hardware implementation and test its performance on Nvidia T4 GPU and Nvidia A100 GPU, which have Cuda capability of sm_75 and sm_80, respectively. The result is shown in Fig. 7 and Fig. 8.