# Sample Complexity Reduction via Policy Difference Estimation in Tabular Reinforcement Learning

 Adhyyan Narang

University of Washington

adhyyan@uw.edu

&Andrew Wagenmaker

University of California, Berkeley

ajwagen@berkeley.edu

&Lillian J. Ratliff

University of Washington

ratliffl@uw.edu

&Kevin Jamieson

University of Washington

jamieson@cs.washington.edu

###### Abstract

In this paper, we study the non-asymptotic sample complexity for the pure exploration problem in contextual bandits and tabular reinforcement learning (RL): identifying an \(\epsilon\)-optimal policy from a set of policies \(\Pi\) with high probability. Existing work in bandits has shown that it is possible to identify the best policy by estimating only the _difference_ between the behaviors of individual policies- which can be substantially cheaper than estimating the behavior of each policy directly --yet the best-known complexities in RL fail to take advantage of this, and instead estimate the behavior of each policy directly. Does it suffice to estimate only the differences in the behaviors of policies in RL? We answer this question positively for contextual bandits, but in the negative for tabular RL, showing a separation between contextual bandits and RL. However, inspired by this, we show that it _almost_ suffices to estimate only the differences in RL: if we can estimate the behavior of a _single_ reference policy, it suffices to only estimate how any other policy deviates from this reference policy. We develop an algorithm which instantiates this principle and obtains, to the best of our knowledge, the tightest known bound on the sample complexity of tabular RL.

## 1 Introduction

Online platforms, such as AirBnB, often try to improve their services by A/B testing different marketing strategies. Based on the inventory, their strategy could include emphasizing local listings versus tourist destinations, providing discounts for longer stays, or de-prioritizing homes that have low ratings. In order to choose the best strategy, the standard approach would be to apply each strategy sequentially and measure outcomes. However, recognize that the choice of strategy (policy) affects the future inventory (state) of the platform. This complex interaction between different strategies makes it difficult to estimate the impact of any strategy, if it were to be applied independently. To address this, we can model the platform as an Markov Decision Process (MDP) with an observed state [17; 15] and a finite set of policies \(\Pi\) corresponding to possible strategies. We wish to collect data by playing _exploratory_ actions which will enable us to estimate the true value of each policy \(\pi\in\Pi\), and identify the best policy from \(\Pi\) as quickly as possible.

In addition to A/B testing, similar challenges arise in complex medical trials, learning robot policies to pack totes, and autonomous navigation in unfamiliar environments. All of these problems can be formally modeled as the PAC (Probably Approximately Correct) policy identification problem in reinforcement learning (RL). An algorithm is said to be \((\epsilon,\delta)\)-PAC if, given a set of policies \(\Pi\), it returns a policy \(\pi\in\Pi\) that performs within \(\epsilon\) of the optimal policy in \(\Pi\), with probability \(1-\delta\). Thegoal is to satisfy this condition whilst minimizing the number of interactions with the environment (the _sample complexity_).

Traditionally, prior work has aimed to obtain _minimax_ or _worst-case_ guarantees for this problem--guarantees that hold across _all_ environments within a problem class. Such worst-case guarantees typically scale with the "size" of the environment, for example, scaling as \(\mathcal{O}(\mathrm{poly}(S,A,H)/\epsilon^{2})\), for environments with \(S\) states, \(A\) actions, horizon \(H\). While guarantees of this form quantify which classes of problems are efficiently learnable, they fail to characterize the difficulty of particular problem instances--producing the same complexity on both "easy" and "hard" problems that share the same "size". This is not simply a failure of analysis--recent work has shown that algorithms that achieve the minimax-optimal rate could be very suboptimal on particular problem instances [46]. Motivated by this, a variety of recent work has sought to obtain _instance-dependent_ complexity measures that capture the hardness of learning each particular problem instance. However, despite progress in this direction, the question of the _optimal_ instance-dependent complexity has remained elusive, even in tabular settings.

Towards achieving instance-optimality in RL, the key question is: _what aspects_ of a given environment must be learned, in order to choose a near-optimal policy? In the simpler bandit setting, this question has been settled by showing that it is sufficient to learn the _differences_ between values of actions rather than learning the value of each individual action: it is only important whether a given action's value is greater or lesser than that of other actions. This observation can yield significant improvements in sample efficiency [37, 16, 13, 30]. Precisely, the best-known complexity measures in the bandit setting scale as:

\[\inf_{\pi_{\mathrm{exp}}}\max_{\pi\in\Pi}\frac{\|\phi^{\pi}-\phi^{\star}\|_{ \Lambda(\pi_{\mathrm{exp}})^{-1}}^{2}}{\Delta(\pi)^{2}},\] (1.1)

where \(\phi^{\pi}\) is the feature vector of action \(\pi\), \(\phi^{\star}\) the feature vector of the optimal action, \(\Delta(\pi)\) is the suboptimality of action \(\pi\). Here, \(\Lambda(\pi_{\mathrm{exp}})\) are the covariates induced by \(\pi_{\mathrm{exp}}\), our distribution of exploratory actions. The denominator of this expression measures the performance gap between action \(\pi\) and the optimal action. The numerator measures the variance of the estimated (from data collected by \(\pi_{\mathrm{exp}}\)) difference in values between \((\pi,\pi^{\star})\). The max over actions follows because to choose the best action, we have to rule out every sub-optimal action from the set of candidates \(\Pi\); the infimum optimizes over data collection strategies.

In contrast, in RL, instead of estimating the difference between policy values _directly_, the best known algorithms simply estimate the value of each individual policy _separately_ and then take the difference. This obtains instance-dependent complexities which scale as follows [42]:

\[\sum_{h=1}^{H}\inf_{\pi_{\mathrm{exp}}}\max_{\pi\in\Pi}\frac{\|\phi_{h}^{\pi}\|_ {\Lambda_{h}(\pi_{\mathrm{exp}})^{-1}}^{2}+\|\phi_{h}^{\star}\|_{\Lambda_{h}( \pi_{\mathrm{exp}})^{-1}}^{2}}{\Delta(\pi)^{2}}\] (1.2)

where \(\phi_{h}^{\pi}\) is the state-action visitation of policy \(\pi\) at step \(h\). Since now the difference is calculated _after_ estimation, the variance of the difference is the sum of the individual variances of the estimates of each policy, captured in the numerator of (1.2). Comparing the numerator of (1.2) to that of (1.1) begs the question: in RL can we estimate the _difference_ of policies directly to reduce the sample complexity of RL?

To motivate why this distinction is important, consider the tabular MDP example of Figure 1. In this example, the agent starts in state \(s_{1}\), takes one of three actions, and then transitions to one of states \(s_{2},s_{3},s_{4}\). Consider the policy set \(\Pi=\{\pi_{1},\pi_{2}\}\), where \(\pi_{1}\) always plays action \(a_{1}\), and \(\pi_{2}\) is identical, except plays actions \(a_{2}\) in the red states. If \(\phi_{h}^{\pi_{i}}\in\Delta_{S\times\mathcal{A}}\) denotes the state-action visitations of policy \(\pi_{i}\) at time \(h=1,2\), then we see that \(\phi_{1}^{\pi_{1}}=\phi_{1}^{\pi_{2}}\) since \(\pi_{1}\) and \(\pi_{2}\) agree on the action in \(s_{1}\). But \(\phi_{2}^{\pi_{1}}\neq\phi_{2}^{\pi_{2}}\) as their actions differ on the red states.

Since these red states will be reached with probability at most \(3\epsilon\), the norm of the _difference_

\[\|\phi_{2}^{\pi}-\phi_{2}^{\star}\|_{\Lambda_{2}(\pi_{\mathrm{exp}})^{-1}}^{2} =\sum_{s,a}\frac{(\phi_{2}^{\pi}(s,a)-\phi_{2}^{\star}(s,a))^{2}}{\phi_{2}^{ \pi_{\mathrm{exp}}}(s,a)}\]

is significantly less than the sum of the individual norms

\[\|\phi_{2}^{\pi}\|_{\Lambda_{2}(\pi_{\mathrm{exp}})^{-1}}^{2}+\|\phi_{2}^{\star }\|_{\Lambda_{2}(\pi_{\mathrm{exp}})^{-1}}^{2}=\sum_{s,a}\frac{\phi_{2}^{\pi}( s,a)^{2}+\phi^{\star}(s,a)^{2}}{\phi_{2}^{\pi_{\mathrm{exp}}}(s,a)}.\]Intuitively, to minimize differences \(\pi_{\exp}\) can explore just states \(s_{3},s_{4}\) where the policies differ, whereas minimizing the individual norms requires wasting lots of energy in state \(s_{2}\) where the two policies and the difference is zero. Formally:

**Proposition 1**.: _On the MDP and policy set \(\Pi\) from Figure 1, we have that_

\[\inf_{\pi_{\exp}}\max_{\pi\in\Pi}\|\phi_{2}^{\pi}\|_{\Lambda_{2}(\pi_{\exp})^{ -1}}^{2}\geq 1\quad\text{and}\quad\inf_{\pi_{\exp}}\max_{\pi\in\Pi}\|\phi_{2}^{ \star}-\phi_{2}^{\pi}\|_{\Lambda_{2}(\pi_{\exp})^{-1}}^{2}\leq 15\epsilon^{2}.\]

Proposition 1 shows that indeed, the complexity of the form Equation (1.1) (generalized to RL) in terms of differences could be significantly tighter than Equation (1.2); in this case, it is a factor of \(\epsilon^{2}\) better. But achieving a sample complexity that depends on the differences requires more than just a better analysis: it requires a new estimator and an algorithm to exploit it.

Contributions.In this work, we aim to understand whether such a complexity is achievable in RL. Letting \(\rho_{\Pi}\) denote the generalization of (1.1) to the RL case--that is, (1.2) but with \(\|\phi_{h}^{\pi}\|_{\Lambda_{h}(\pi_{\exp})^{-1}}^{2}\) replaced by \(\|\phi_{h}^{\pi}-\phi_{h}^{\pi}\|_{\Lambda_{h}(\pi_{\exp})^{-1}}^{2}\), our contributions are as follows:

1. In the Tabular RL case, [2] recently showed that \(\rho_{\Pi}\) is a lower bound on the sample complexity of RL by characterizing the difficulty of learning the unknown reward function; however, they did not resolve whether it is achievable when the state-transitions are unknown as well. We provide a lower bound which demonstrates that \(\mathcal{O}(\rho_{\Pi})\) is _not_ sufficient for learning with state transitions.
2. We provide an algorithm Perp, which first learns the behavior a particular reference policy \(\bar{\pi}\), and then estimates the difference in behavior between \(\bar{\pi}\) and every other policy \(\pi\), rather than estimating the behavior of each \(\pi\) directly.
3. In the case of tabular RL, we show that Perp obtains a complexity that scales with \(\mathcal{O}(\rho_{\Pi})\), in addition to an extra term which measures the cost of learning the behavior of the reference policy \(\bar{\pi}\). We argue that this additional term is critical to achieving instance-optimal guarantees in RL, and that Perp leads to improved complexities over existing work.
4. In the contextual bandit setting, we provide an upper bound that scales (up to lower order terms) as \(\mathcal{O}(\rho_{\Pi})\) for the _unknown-context_ distribution case. This matches the lower bound from [30] for the known context distribution case, thus showing that \(\rho_{\Pi}\) is necessary and sufficient in contextual bandits even when the context distribution is unknown. Hence, we observe a qualitative information-theoretic separation between contextual bandits and RL.

The key insight from our work is that it does not suffice to _only_ learn the differences between policy values in RL, but it _almost_ suffices to--if we can learn how a single policy behaves, it suffices to learn the difference between this policy and every other policy.

## 2 Related Work

The reinforcement learning literature is vast, and here we focus on results in tabular RL and instance-dependent guarantees in RL.

Figure 1: A motivating example for differences. The rewards for all actions other than the ones specified in the figure are \(0\). Define policy set \(\Pi=\{\pi_{1},\pi_{2}\}\) so that \(\pi_{1}\) always plays \(a_{1}\), whereas \(\pi_{2}\) plays \(a_{1}\) on green states but \(a_{2}\) on red states. The difference of their state-action visitation probabilities is only non-zero in states \(s_{3},s_{4}\) and are just \(O(\epsilon)\) apart.

**Minimax Guarantees Tabular RL.** Finite-time minimax-style results on policy identification in tabular MDPs go back to at least the late 90s and early 2000s [24; 26; 25; 8; 21]. This early work was built upon and refined by a variety of other works over the following decade [38; 4; 34; 39], leading up to works such as [28; 9], which establish sample complexity bounds of \(\mathcal{O}(S^{2}A\cdot\mathrm{poly}(H)/\epsilon^{2})\). More recently, [10; 33; 11] have proposed algorithms which achieve the optimal dependence of \(\mathcal{O}(SA\cdot\mathrm{poly}(H)/\epsilon^{2})\), with [11; 33] also achieving the optimal \(H\) dependence. The question of regret minimization is intimately related to that of policy identification--any low-regret algorithm can be used to obtain a near-optimal policy via an online-to-batch conversion [19]. Early examples of low-regret algorithms in tabular MDPs are [3; 4; 5; 48], with more recent works removing the horizon dependence or achieving the optimal lower-order terms as well [50; 51]. Recently, [6; 7] provide minimax guarantees in the multi-task RL setting as well.

**Instance-Dependence in RL.** While the problem of obtaining worst-case optimal guarantees in tabular RL is nearly closed, we are only beginning to understand what types of instance-dependent guarantees are possible. In the setting of regret minimization, [35; 14] achieve instance-optimal regret for tabular RL asymptotically. Simchowitz and Jamieson [36] show that standard optimistic algorithms achieve regret bounded as \(\mathcal{O}(\sum_{s,a,h}\frac{\log K}{\Delta_{s}(s,a)})\), a result later refined by [47; 12]. In settings of RL with linear function approximation, several works achieve instance-dependent regret guarantees [18; 44]. Recently, Wagenmaker and Foster [45] achieved finite-time guarantees on instance-optimal regret in general decision-making settings, a setting encompassing much of RL.

On the policy identification side, early works obtaining instance-dependent guarantees for tabular MDPs include [49; 20; 31; 32], but they all exhibit shortcomings such as requiring access to a generative model or lacking finite-time results. The work of Wagenmaker et al. [46] achieves a finite-time instance-dependent guarantee for tabular RL, introducing a new notion of complexity, the _gap-visitation complexity_. In the special case of deterministic, tabular MDPs, Tirinzoni et al. [41] show matching finite-time instance-dependent upper and lower bounds. For RL with linear function approximation, [42; 43] achieve instance-dependent guarantees on policy identification, in particular, the complexity given in (1.2), and propose an algorithm, Pedel, which directly inspires our algorithmic approach. On the lower bound side, Al-Marjani et al. [2] show that \(\rho_{\Pi}\) is necessary for tabular RL, but fail to close the aforementioned gap between \(\rho_{\Pi}\) and (1.2). We will show instead that this gap is real and both the lower bound of Al-Marjani et al. [2] and upper bound of Wagenmaker and Jamieson [42] are loose.

Several works on linear and contextual bandits are also relevant. In the seminal work, [37] posed the best-arm identification problem for linear bandits and beautifully argued--without proof--that estimating differences were crucial and that (1.1) ought to be the true sample complexity of the problem. Over time, this conjecture was affirmed and generalized [16; 13; 22]. This improved understanding of pure-exploration directly led to instance-dependent optimal linear bandit algorithms for regret [29; 27]. More recently, contextual bandits have also been given a similar treatment [40; 30].

## 3 Preliminaries and Problem Setting

Let \(\|x\|_{\Lambda}^{2}=x^{\top}\Lambda x\) for any \((x,\Lambda)\). We let \(\mathbb{E}_{\pi}\) denote the probability measure induced by playing policy \(\pi\) in our MDP.

Tabular Markov Decision Processes.We study episodic, finite-horizon, time inhomogenous and tabular Markov Decision Processes (MDPs), denoted by the tuple \((\mathcal{S},\mathcal{A},H,\{P_{h}\}_{h=1}^{H},\{\nu_{h}\}_{h=1}^{H})\) where the state space \(\mathcal{S}\) and action space \(\mathcal{A}\) are finite, \(H\) is the horizon, \(P_{h}\in\mathbb{R}^{S\times SA}\) denote the transition matrix at stage \(h\) where \([P_{h}]_{s^{\prime},sa}=\mathbb{P}(s_{h+1}=s^{\prime}|s_{h}=s,a_{h}=a)\), and \(\nu_{h}(s,a)\in\triangle_{[0,1]}\) denote the distribution over reward at stage \(h\) when the state of the system is \(s\) and action \(a\) is chosen. Let \(r_{h}(s,a)\) be the expectation of a reward drawn from \(\nu_{h}(s,a)\). We assume that every episode starts in state \(s_{1}\), and that \(\nu_{h}\) and \(P_{h}\) are initially unknown and must be estimated over time.

Let \(\pi=\{\pi_{h}\}_{h=1}^{H}\) denote a policy mapping states to actions, so that \(\pi_{h}(s)\in\triangle_{\mathcal{A}}\) denotes the distribution over actions for the policy at \((s,h)\); when the policy is deterministic, \(\pi_{h}(s)\in\mathcal{A}\) outputs a single action. An episode begins in state \(s_{1}\), the agent takes action \(a_{1}\sim\pi_{1}(s_{1})\) and receives reward \(R_{1}\sim\nu_{1}(s_{1},a_{1})\) with expectation \(r_{1}(s_{1},a_{1})\); the environment transitions to state \(s_{2}\sim P_{h}(s_{1},a_{1})\). The process repeats until timestep \(H\), at which point the episode ends and the agent returns to state \(s_{1}\). Let \(V_{h}^{\pi}(s)=\mathbb{E}_{\pi}[\sum_{h^{\prime}=h}^{H}r_{h^{\prime}}(s_{h^{ \prime}},a_{h^{\prime}})|s_{h}=s]\), \(V_{0}^{\pi}\) the total expected reward, \(V_{0}^{\pi}:=V_{1}^{\pi}(s_{0})\)and \(Q_{h}^{\pi}(s,a)=\mathbb{E}_{\pi}[\sum_{h^{\prime}=h}^{H}r_{h^{\prime}}(s_{h^{ \prime}},a_{h^{\prime}})|s_{h}=s,a_{h}=a]\) the amount of reward we expect to collect if we are in state \(s\) at step \(h\), play action \(a\) and then play policy \(\pi\) for the remainder of the episode. Note that we can understand these functions as \(S\) and \(SA\)-dimensional vectors respectively. We use \(V^{\pi}=V_{0}^{\pi}\) when clear from context.

We call \(w_{h}^{\pi}\in\triangle_{S}\) the _state visitation vector_ at step \(h\) for policy \(\pi\), so that \(w_{h}^{\pi}(s)\) captures the probability that policy \(\pi\) would land in state \(s\) at step \(h\) during an episode. Let \(\bm{\pi}_{h}\in\mathbb{R}^{SA\times S}\) denote the policy matrix for policy \(\pi\), that maps states to state-actions as follows

\[[\bm{\pi}_{h}]_{(s,a),s^{\prime}}=\mathbb{I}(s=s^{\prime})[\pi_{h}(s)]_{a}.\]

Denote \(\phi_{h}^{\pi}\in\triangle_{SA}\) as \(\phi_{h}^{\pi}:=\bm{\pi}_{h}w_{h}^{\pi}\) as the _state-action visitation vector:_\(\phi_{h}^{\pi}(s,a)\) measures the the probability that policy \(\pi\) would land in state \(s\) and play action \(a\) at step \(h\) during an episode. From these definitions, it follows that \([P_{h}\phi_{h}^{\pi}]_{s}=[P_{h}\bm{\pi}_{h}w_{h}^{\pi}]_{s}=w_{h+1}^{\pi}(s)\). For policy \(\pi\), denote the covariance matrix at timestep \(h\) as \(\Lambda_{h}(\pi)=\sum_{s,a}\phi_{h}^{\pi}(s,a)\mathbf{e}_{(s,a)}\mathbf{e}_{(s,a)}^{\top}\).

\((\epsilon,\delta)\)-PAC Best Policy Identification.For a collection of policies \(\Pi\), define \(\pi^{\star}:=\arg\max_{\pi\in\Pi}V^{\pi}\) as the optimal policy, \(V^{\star}\) its value, and \(\phi_{h}^{\star}\) as its state-action visitation vector. Let \(\Delta_{\min}:=\min_{\pi\in\Pi\setminus\{\pi^{\star}\}}V^{\star}-V^{\pi}\) in the case when \(\pi^{\star}\) is unique, and otherwise \(\Delta_{\min}:=0\). Define \(\Delta(\pi):=\max\{V^{\star}-V^{\pi},\Delta_{\min}\}\). Given \(\epsilon\geq 0\), \(\delta\in(0,1)\) an algorithm is said to be \((\epsilon,\delta)\)-PAC if at a stopping time \(\tau\) of its choosing, it returns a policy \(\widehat{\pi}\) which satisfies \(\Delta(\pi)\leq\epsilon\) with probability \(1-\delta\). Our goal is to obtain an \((\epsilon,\delta)\)-PAC algorithm that minimizes \(\tau\). A fundamental complexity measure used throughout this work is defined as

\[\rho_{\Pi}:=\sum_{h=1}^{H}\inf_{\pi_{\mathrm{exp}}}\max_{\pi\in\Pi}\frac{\| \phi_{h}^{\star}-\phi_{h}^{\pi}\|^{2}_{\Lambda_{h}(\pi_{\mathrm{exp}})^{-1}}} {\max\{\epsilon^{2},\Delta(\pi)^{2}\}}\quad\text{for}\quad\|\phi_{h}^{\star}- \phi_{h}^{\pi}\|^{2}_{\Lambda_{h}(\pi_{\mathrm{exp}})^{-1}}:=\sum_{s,a}\frac{( \phi_{h}^{\star}(s,a)-\phi_{h}^{\star}(s,a))^{2}}{\phi_{h}^{\pi_{\mathrm{exp}} }(s,a)}\]

where the infimum is over all exploration policies \(\pi_{\mathrm{exp}}\) (not necessarily just those in \(\Pi\)). Recall that for \(\epsilon=0\), [2] showed any \((\epsilon,\delta)\)-PAC algorithm satisfies \(\mathbb{E}[\tau]\geq\rho_{\Pi}\log(\frac{1}{2.4\delta})\).

## 4 What is the Sample Complexity of Tabular RL?

In this section, we seek to understand the complexity of tabular RL. We start by showing that \(\rho_{\Pi}\) is not sufficient. We have the following result.

**Lemma 1**.: _For the MDP \(\mathcal{M}\) and policy set \(\Pi\) from Figure 1,_

1. \(\sum_{h=1}^{H}\inf_{\pi_{\mathrm{exp}}}\max_{\pi\in\Pi}\frac{\|\phi_{h}^{\star} -\phi_{h}^{\pi}\|^{2}_{\Lambda_{h}(\pi_{\mathrm{exp}})^{-1}}}{\max\{\epsilon^{2},\Delta(\pi)^{2}\}}\leq 15,\)__
2. _Any_ \((\epsilon,\delta)\)_-PAC algorithm must collect at least_ \(\mathbb{E}^{\mathcal{M}}[\tau]\geq\frac{1}{\epsilon}\cdot\log\frac{1}{2.4\delta}\)_. samples._

Where does the additional complexity arise on the instance of Figure 1? As described in the introduction, \(\pi_{1}\) and \(\pi_{2}\) differ only on the red states, and a complexity scaling as \(\rho_{\Pi}\) quantifies only the difficulty of distinguishing \(\{\pi_{1},\pi_{2}\}\) on these states. Note that on this example \(\pi_{1}\) plays the optimal action in state \(s_{3}\) and a suboptimal action in state \(s_{4}\), and \(\pi_{2}\) plays a suboptimal action in \(s_{3}\) and the optimal action in \(s_{4}\). The total reward of policy \(\pi_{1}\) is therefore equal to the reward achieved at state \(s_{3}\) times the probability it reaches state \(s_{3}\), and the total reward of policy \(\pi_{2}\) is the reward achieved at state \(s_{4}\) times the probability it reaches state \(s_{4}\). Here, \(\rho_{\Pi}\) would quantify the difficulty of learning the reward achieved at each state. However, it fails to quantify _the probability of reaching each state_, since this depends on the behavior at step 1, not step 2.

Thus, on this example, to determine whether \(\pi_{1}\) or \(\pi_{2}\) is optimal, we must pay some additional complexity to learn the outgoing transitions from the initial state, giving rise to the lower bound in Lemma 1. Inspecting the lower bound of [2], one realizes that the construction of this lower bound only quantifies the cost of learning the reward distributions \(\{\nu_{h}\}_{h}\) and _not_ the state transition matrices \(\{P_{h}\}_{h}\). On examples such as Figure 1, this lower bound then does not quantify the cost of learning the probability of visiting each state, which we've argued is necessary. We therefore conclude that, while \(\rho_{\Pi}\) may be enough for learning the rewards, it is _not_ sufficient for solving the full tabular RL problem. Our main algorithm builds on this intuition, and, in addition to estimating the rewards, aims to estimate where policies visit as efficiently as possible.

### Main Result

If \(\rho_{\Pi}\) is not achievable as the sample complexity for Tabular RL, what is the best that we can do? In this section, we answer this question with our sample complexity bound; we later describe the algorithmic insights that enable us to achieve this result in the following section. First, for any \(\pi,\bar{\pi}\in\Pi\), we define

\[U(\pi,\bar{\pi}):=\sum_{h=1}^{H}\!\mathbb{E}_{s_{h}\sim w_{h}^{\pi}}[(Q_{h}^{\pi }(s_{h},\pi_{h}(s_{h}))-Q_{h}^{\pi}(s_{h},\bar{\pi}_{h}(s_{h})))^{2}].\] (4.1)

Now, we state our main result.

**Theorem 1**.: _There exists an algorithm (Algorithm 1) which, with probability at least \(1-2\delta\), finds an \(\epsilon\)-optimal policy and terminates after collecting at most_

\[\sum_{h=1}^{H}\inf_{\pi_{\mathrm{exp}}}\max_{\pi\in\Pi}\frac{H^{4}\|\phi_{h}^{ \pi}-\phi_{h}^{\pi}\|^{2}_{\Lambda_{h}(\pi_{\mathrm{exp}})^{-1}}}{\max\{ \epsilon^{2},\Delta(\pi)^{2}\}}\cdot\iota\beta^{2}+\max_{\pi\in\Pi}\frac{HU( \pi,\pi^{\star})}{\max\{\epsilon^{2},\Delta(\pi)^{2}\}}\log\frac{H\|\Pi\|_{ \epsilon}}{\delta}+\frac{C_{\mathrm{poly}}}{\max\{\epsilon^{\frac{5}{3}},\Delta _{\min}^{\frac{5}{3}}\}}\]

_episodes, for \(C_{\mathrm{poly}}:=\mathrm{poly}(S,A,H,\log 1/\delta,\iota,\log|\Pi|),\beta:=C \sqrt{\log(\frac{SH\|\Pi|}{\delta}\cdot\frac{1}{\Delta_{\min}\nu\epsilon})}\) and \(\iota:=\log\frac{1}{\Delta_{\min}\nu\epsilon}\)._

Theorem 1 shows that, up to terms lower-order in \(\epsilon\) and \(\Delta_{\min}\), \(\rho_{\Pi}\) is almost sufficient, if we are willing to pay for an additional term scaling as \(U(\pi,\pi^{\star})/\Delta(\pi)^{2}\). Recognize the similarity of this term to the that from the performance difference lemma: if there were no square inside the expectation, the quantity \(U(\pi,\pi^{\star})\) would be equal to \(\Delta(\pi)\). However, the square may change the scaling in some instances. Below, Lemma 2 shows that there exist settings where the complexity of Theorem 1 could be significantly tighter than Equation (1.2), the complexity achieved by the Pedel algorithm of [42]. We revisit the instance from Figure 1 to show this; recall from Lemma 1 that the first term from Theorem 1 is a universal constant for this instance.

**Lemma 2**.: _On MDP \(\mathcal{M}\) and policy set \(\Pi\) from Figure 1, we have:_

1. \(\max_{\pi\in\Pi}\frac{HU(\pi,\pi^{\star})}{\max\{\epsilon^{2},\Delta(\pi)^{2} \}}=\frac{3H}{\epsilon},\)__
2. \(\sum_{h=1}^{H}\inf_{\pi_{\mathrm{exp}}}\max_{\pi\in\Pi}\frac{\|\phi_{h}^{\pi} \|^{2}_{\Lambda_{h}(\pi_{\mathrm{exp}})^{-1}}}{\max\{\epsilon^{2},\Delta(\pi)^ {2}\}}\geq\frac{H}{\epsilon^{2}}.\)__

Furthermore, the complexity of Theorem 1 is never worse than Equation (1.2).

**Lemma 3**.: _For any MDP instance and policy set \(\Pi\), we have that_

\[\max\bigg{\{}\sum_{h=1}^{H}\inf_{\pi_{\mathrm{exp}}}\max_{\pi\in\Pi}\frac{\| \phi_{h}^{\star}-\phi_{h}^{\pi}\|^{2}_{\Lambda_{h}(\pi_{\mathrm{exp}})^{-1}}}{ \max\{\epsilon^{2},\Delta(\pi)^{2}\}},\frac{HU(\pi,\pi^{\star})}{\max\{\epsilon ^{2},\Delta(\pi)^{2}\}}\bigg{\}}\leq\sum_{h=1}^{H}\inf_{\pi_{\mathrm{exp}}}\max_ {\pi\in\Pi}\frac{\|\phi_{h}^{\pi}\|^{2}_{\Lambda_{h}(\pi_{\mathrm{exp}})^{-1}} }{\max\{\epsilon^{2},\Delta(\pi)^{2}\}}.\]

We briefly remark on the lower-order term for Theorem 1, \(\frac{C_{\mathrm{poly}}}{\max\{\epsilon^{5/3},\Delta_{\min}^{5/3}\}}\). Note that for small \(\epsilon\) or \(\Delta_{\min}\), this term will be dominated by the leading-order terms, which scale with \(\min\{\epsilon^{-2},\Delta_{\min}^{-2}\}\). While we make no claims on the tightness of this term, we note that recent work has shown that some lower-order terms are necessary for achieving instance-optimality [45].

### The Main Algorithmic Insight: The Reduced-Variance Difference Estimator

In this section, we describe how we can estimate the difference between the values of policies directly, and provide intuition for why this results in the two main terms in Theorem 1. Fix any reference policy \(\bar{\pi}\) and logging policy \(\mu\) (neither are necessarily in \(\Pi\)). Here \(\mu\) can be thought of as playing the role of \(\pi_{\mathrm{exp}}\). Or, we can consider the A/B testing scenario from the introduction, where a policy \(\mu\) is taking random actions and one wishes to perform off-policy estimation over some set of policies \(\Pi\)[17; 15]. For any \(s\in\mathcal{S}\), we define

\[\delta_{h}^{\pi}(s):=w_{h}^{\pi}(s)-w_{h}^{\bar{\pi}}(s)\]

as the difference in state-visitations of policy \(\pi\) from reference policy \(\bar{\pi}\), and \(\delta_{h}^{\pi}\in\mathbb{R}^{S}\) as the vectorization of \(\delta_{h}^{\pi}(s^{\prime})\).

Policy selection rule.First, we describe our procedure of data collection and estimation. We collect \(K_{\bar{\pi}}\) trajectories from \(\bar{\pi}\) and \(K_{\mu}\) trajectories from \(\mu\), and let \(\{\widehat{w}_{h}^{\pi}(s)\}_{s,h}\) denote the empirical state visitations from playing \(\bar{\pi}\). From the data collected by playing \(\mu\), we construct estimates \(\{\widehat{P}_{h}(s^{\prime}|s,a)\}_{s,a,s^{\prime},h}\) of the transition matrices. Note that \(\widehat{w}_{h}^{\pi}(s)\) simply counts visitations, so that \(\mathbb{E}[(\widehat{w}_{h}^{\pi}(s)-w_{h}^{\pi}(s))^{2}]\leq\frac{w_{h}^{\pi} (s)}{K_{\pi}}\) for all \(h,s\). Define estimated state visitations for policy \(\pi\) in terms of deviations from \(\bar{\pi}\) as \(\widehat{w}_{h}^{\pi}:=\widehat{w}_{h}^{\pi}+\widehat{\delta}_{h}^{\pi}\). Here, \(\widehat{\delta}_{h}^{\pi}\) is defined recursively as:

\[\widehat{\delta}_{h+1}^{\pi}:=\widehat{P}_{h}\bm{\pi}_{h}\widehat{\delta}_{h }^{\pi}+\widehat{P}_{h}(\bm{\pi}_{h}-\bar{\bm{\pi}}_{h})\widehat{w}_{h}^{\pi}\]

Then, assuming, for simplicity, that rewards are known, we recommend the following policy:

\[\widehat{\pi}=\arg\max_{\pi\in\Pi}\widehat{D}^{\pi}\qquad\text{ where }\qquad\widehat{D}^{\pi}:=\sum_{h=1}^{H}\langle r_{h},\bm{\pi}_{h}\widehat{ \delta}_{h}^{\pi}\rangle-\langle r_{h},(\bar{\bm{\pi}}_{h}-\bm{\pi}_{h}) \widehat{w}_{h}^{\pi}\rangle\]

Sufficient condition for \(\epsilon\)-optimality.Here, we show that if

\[\forall\pi\in\Pi,\qquad|\widehat{D}^{\pi}-D^{\pi}|\leq\frac{1}{3}\max\{\epsilon,\Delta(\pi)\}\] (4.2)

then \(\widehat{\pi}\) is \(\epsilon\)-optimal. First, write the difference between values of policies \(\pi\) and \(\bar{\pi}\) as:

\[\begin{split} D^{\pi}:=V_{0}^{\pi}-V_{0}^{\bar{\pi}}& =\sum_{h=1}^{H}\langle r_{h},\bm{\pi}_{h}w_{h}^{\pi}\rangle-\sum_ {h=1}^{H}\langle r_{h},\bar{\bm{\pi}}_{h}w_{h}^{\bar{\pi}}\rangle\\ &=\sum_{h=1}^{H}\langle r_{h},\bm{\pi}_{h}\delta_{h}^{\pi} \rangle-\langle r_{h},(\bar{\bm{\pi}}_{h}-\bm{\pi}_{h})w_{h}^{\bar{\pi}}\rangle.\end{split}\] (4.3)

Then, it is easy to verify that if \(|\widehat{D}^{\pi}-D^{\pi}|\leq 1/3\ \Delta(\pi)\), then \(\widehat{D}^{\pi^{*}}-\widehat{D}^{\pi}\geq 0\); hence, \(\widehat{\pi}\neq\pi\). Hence, under Condition (4.2), either \(\widehat{\pi}=\pi^{\star}\) or or \(|\widehat{D}^{\pi}-D^{\pi}|\leq\epsilon\). In the first case, clearly \(\widehat{\pi}\) is \(\epsilon\)-optimal. In the second case, we can add and subtract terms to write

\[V^{\star}-V^{\widehat{\pi}}\leq|D^{\pi^{*}}-\widehat{D}^{\pi^{*}}|+\widehat {D}^{\pi^{*}}-\widehat{D}^{\widehat{\pi}}+|\widehat{D}^{\widehat{\pi}}-D^{ \widehat{\pi}}|\leq\frac{2\epsilon}{3}+\hat{D}^{\pi^{*}}-\hat{D}^{\widehat{ \pi}}\leq\frac{2\epsilon}{3}.\]

The last inequality follows since \(\widehat{\pi}\) maximizes \(\widehat{D}^{\pi}\). Hence, \(\widehat{\pi}\) would be \(\epsilon\)-optimal in this case as well.

Sample complexity.Now, we characterize how many samples must be collected from \(\mu\) and \(\bar{\pi}\) in order to meet Condition (4.2). After dropping some lower-order terms and unrolling the recursion (see Section A for details), we observe that

\[\begin{split}\widehat{\delta}_{h+1}^{\pi}-\delta_{h+1}^{\pi}& \approx(\widehat{P}_{h}-P_{h})(\phi_{h}^{\pi}-\phi_{h}^{\bar{\pi}})+P_{h}( \bm{\pi}_{h}-\bar{\bm{\pi}}_{h})(\widehat{w}_{h}^{\bar{\pi}}-w_{h}^{\bar{\pi} })+P_{h}\bm{\pi}_{h}(\widehat{\delta}_{h}^{\pi}-\delta_{h}^{\pi})\\ &=\sum_{k=0}^{h}\bigl{(}\prod_{j=k+1}^{H}P_{j}\bm{\pi}_{j}\bigr{)} \bigl{(}(\widehat{P}_{k}-P_{k})(\phi_{k}^{\pi}-\phi_{k}^{\bar{\pi}})+P_{k}( \bm{\pi}_{k}-\bar{\bm{\pi}}_{k})(\widehat{w}_{k}^{\bar{\pi}}-w_{h}^{\bar{\pi} })\bigr{)}.\end{split}\]

After manipulating this expression a bit more, we observe that

\[\sum_{h=1}^{H}\langle r_{h},\bm{\pi}_{h}(\widehat{\delta}_{h}^{\pi}-\delta_{h}^ {\pi})\rangle=\sum_{k=0}^{H-1}\langle V_{k+1}^{\pi},(\widehat{P}_{k}-P_{k})( \phi_{k}^{\pi}-\phi_{k}^{\bar{\pi}})+P_{k}(\bm{\pi}_{k}-\bar{\bm{\pi}}_{k})( \widehat{w}_{k}^{\bar{\pi}}-w_{k}^{\bar{\pi}})\rangle\]

Recognizing \(Q_{h}^{\pi}=r_{h}+P_{h}^{\top}V_{h+1}^{\pi}\),

\[\begin{split}|\widehat{D}^{\pi}&-D^{\pi}|=\left|\sum_ {h=1}^{H}\langle r_{h},\bm{\pi}_{h}(\widehat{\delta}_{h}^{\pi}-\delta_{h}^{ \pi})\rangle+\langle r_{h},(\bm{\pi}_{h}-\bar{\bm{\pi}}_{h})(\widehat{w}_{h}^{ \bar{\pi}}-w_{h}^{\bar{\pi}})\rangle\right|\\ &=\left|\sum_{h=0}^{H-1}\langle V_{h+1}^{\pi},(\widehat{P}_{h}-P_{h })(\phi_{h}^{\pi}-\phi_{h}^{\bar{\pi}})\rangle+\langle r_{h}+P_{h}^{\top}V_{h+1} ^{\pi},(\bm{\pi}_{h}-\bar{\bm{\pi}}_{h})(\widehat{w}_{h}^{\bar{\pi}}-w_{h}^{ \bar{\pi}})\rangle\right|\end{split}\]

We can bound this as:

\[\begin{split}&\lesssim\sqrt{H^{2}\sum_{h=0}^{H-1}\sum_{s,a}\frac{( \phi_{h}^{\pi}(s,a)-\phi_{h}^{\bar{\pi}}(s,a))^{2}}{K_{\mu}\mu_{h}(s,a)}}+\sqrt {\sum_{h=0}^{H-1}\sum_{s}\big{(}Q_{h}^{\pi}(s,\pi_{h}(s))-Q_{h}^{\pi}(s,\bar{ \pi}_{h}(s))\big{)}^{2}\frac{w_{h}^{\bar{\pi}}(s)}{K_{\bar{\pi}}}}\\ &=\sqrt{H^{2}\sum_{h=0}^{H-1}\frac{\|\phi_{h}^{\pi}-\phi_{h}^{ \bar{\pi}}\|_{\lambda_{h}(\mu)^{-1}}^{2}}{K_{\mu}}}+\sqrt{\frac{U(\pi,\bar{\pi} )}{K_{\bar{\pi}}}}.\end{split}\]Here, we applied Bernstein's inequality and observed that \(\sum_{s^{\prime}}V_{h+1}^{\pi}(s^{\prime})^{2}P_{h}(s^{\prime}|s,a)\leq H^{2}\). Now, we have that if

\[K_{\mu}\gtrsim\max_{\pi\in\Pi}\sum_{h=0}^{H-1}\frac{H^{2}\|\phi_{h}^{\pi}-\phi_ {h}^{\bar{\pi}}\|_{\Lambda_{h}(\mu)^{-1}}^{2}}{\max\{\epsilon^{2},\Delta(\pi )^{2}\}}\qquad\text{and}\qquad K_{\bar{\pi}}\gtrsim\max_{\pi\in\Pi}\frac{U(\pi,\bar{\pi})}{\max\{\epsilon^{2},\Delta(\pi)^{2}\}}\] (4.4)

then Condition (4.2) holds. Notice that up to \(H\) and \(\log(\cdot)\) factors, this is precisely the sample complexity of Theorem 1 if we set \(\bar{\pi}=\pi^{\star}\) and minimize over all logging/exploration policies \(\mu/\pi_{\exp}\). Note that, if \(\bar{V}\) denotes the average reward collected from rolling out \(\bar{\pi}\)\(K_{\bar{\pi}}\) times, then \(|\bar{V}-V_{0}^{\bar{\pi}}|\leq\sqrt{\frac{H^{2}}{K_{\pi}}}\) by Hoeffding's inequality. Thus, one could use \(\widehat{V}^{\pi}=\widehat{D}^{\pi}+\bar{V}\) as an effective off-policy estimator. Likewise, \(\widehat{D}^{\pi}-\widehat{D}^{\pi^{\prime}}\) is an effective estimator for \(V_{0}^{\pi}-V_{0}^{\pi^{\prime}}\).

This calculation (elaborated on in Appendix A) suggests that our analysis is tight, and clearly illustrates that the \(U(\pi,\bar{\pi})\) term arises due to estimating the behavior of the reference policy \(w_{h}^{\bar{\pi}}\). The \(U(\pi,\bar{\pi})\) term is, to the best of our knowledge, novel in the literature. More precisely, this term corresponds to the cost of estimating where \(\bar{\pi}\) visits, if our goal is to estimate the difference in value between policy \(\pi\) and \(\bar{\pi}\). If, for a given state, the actions taken by \(\pi\) and \(\bar{\pi}\) achieve the same long-term reward, then it is not critical that the frequency with which \(\bar{\pi}\) visits this state is estimated, as it does not affect the difference in values between \(\pi\) and \(\bar{\pi}\); if the actions take by \(\pi\) and \(\bar{\pi}\) do achieve different long-term reward at \(s\), then we must estimate the behavior of each policy at this state. This is reflected by the term inside the expectation of \(U(\pi,\bar{\pi})\); this will be \(0\) in the former case, and scale with the difference between long-term action reward in the latter case.

Additionally, note that if we had offline data from _some_ policy \(\bar{\pi}\), that had been played for a long time, so that \(K_{\bar{\pi}}\approx\infty\), then we would only incur the \(K_{\mu}\) term; this is precisely \(\rho_{\Pi}\), but with \(\pi^{\star}\) replaced with our reference policy \(\bar{\pi}\) in the numerator.

## 5 Achieving Theorem 1: Perp Algorithm

While the above section provides intuition for where the terms in Theorem 1 come from, it does not lead to a practical algorithm. This is because the desired number of samples in Equation (4.4) are in terms of unknown quantities: \(\{\|\phi_{h}^{\pi}-\phi_{h}^{\bar{\pi}}\|_{\Lambda_{h}(\mu)^{-1}}^{2},\Delta( \pi),U(\pi,\bar{\pi})\}\), which depend on our unknown environment variables \(\nu_{h},P_{h}\); hence, we would not know how many samples to collect. In this section, we propose an algorithm that will proceed in rounds, successively improving our estimates of these quantities. Define

\[\widehat{U}_{\ell,h}(\pi,\pi^{\prime}):=\widehat{\mathbb{E}}_{\pi^{\prime}, \ell}[(\widehat{Q}_{\ell,h}^{\pi}(s_{h},\pi_{h}(s))-\widehat{Q}_{\ell,h}^{\pi }(s_{h},\pi^{\prime}_{h}(s)))^{2}],\] (5.1)

where \(\widehat{\mathbb{E}}_{\pi^{\prime},\ell}\) denotes the expectation induced playing policy \(\pi^{\prime}\) on the MDP with transitions \(\widehat{P}_{\ell,h}\), and \(\widehat{Q}_{\ell,h}^{\pi}\) denotes the \(Q\)-function of policy \(\pi\) on this same MDP. To compute \(\widehat{P}_{\ell,h}\), we use the standard estimator: \(\widehat{P}_{\ell,h}(s^{\prime}\mid s,a)=\frac{N_{\ell,h}(s,a,s^{\prime})}{N_ {\ell,h}(s,a)}\) for \(N_{\ell,h}(s,a)\) and \(N_{\ell,h}(s,a,s^{\prime})\) the visitation counts in \(\mathfrak{D}_{\ell,h}^{\mathrm{ED}}\). We set \(\widehat{P}_{\ell,h}(s^{\prime}\mid s,a)=\mathrm{unif}(\mathcal{S})\) if \(N_{\ell,h}(s,a)=0\). The analogous estimator is used to estimate \(\widehat{r}_{\ell,h}\). The quantity \(\phi_{h}^{\bar{\pi}}-\phi_{h}^{\bar{\pi}}\) is estimated as in the previous section: \((\bm{\pi}_{h}-\bar{\bm{\pi}}_{\ell,h})\widehat{w}_{\ell,h}^{\bar{\pi}}+\bm{ \pi}_{h}\widehat{\delta}_{\ell,h}^{\pi}\).

Algorithm 1 proceeds in epochs. It begins with a policy set \(\Pi_{1}\), which contains all policies of interest, \(\Pi\). It then gradually begins to refine this policy set, seeking to estimate the _difference_ in values between policies in the set up to tolerance \(\epsilon_{\ell}=2^{-\ell}\). To achieve this, it instantiates the intuition above. First, it chooses a reference policy \(\bar{\pi}_{\ell}\), then running this estimate a sufficient number of times to estimate \(w_{h}^{\bar{\pi}_{\ell}}\). Given this estimate, it then seeks to estimate \(\delta_{h}^{\pi}\) for each \(\pi\) in the active set of policies, \(\Pi_{\ell}\), by collecting data covering the directions \((\bm{\pi}_{h}-\bar{\bm{\pi}}_{\ell,h})\widehat{w}_{\ell,h}^{\bar{\pi}}+\bm{\pi}_ {h}\widehat{\delta}_{\ell,h}^{\pi}\) for all \(\pi\in\Pi_{\ell}\). To efficiently collect this covering data, on line 1, we run a data collection procedure first developed in [42]. Finally, after estimating each \(\delta_{h}^{\pi}\), it estimates the differences between policy values as in (4.3), and eliminates suboptimal policies.

The computational complexity of Perp is poly \((S,A,H,1/\epsilon,|\Pi|,\log(1/\delta))\). The primary contributor to the computational complexity is the the use of the Franke-Wolfe algorithm for experiment design in the OptCov subroutine. Lemma 37 from Wagenmaker and Pacchiano [43] shows that the number of iterations of the Franke-Wolfe algorithm is bounded polynomially in the problem parameters,and from the definition of this procedure given in Wagenmaker and Pacchiano [43], we see that each iteration of Franke-Wolfe has computational complexity polynomial in problem parameters. We omit several technical details from Algorithm 1 for simplicity, but present the full definition in Algorithm 2.

```
1:tolerance \(\epsilon\), confidence \(\delta\), policies \(\Pi\)
2:\(\Pi_{1}\leftarrow\Pi\), \(\widehat{P}_{0}\leftarrow\) arbitrary transition matrix
3:for\(\ell=1,2,3,\ldots,\lceil\log_{2}\frac{16}{\epsilon}\rceil\)do
4: Set \(\epsilon_{\ell}\gets 2^{-\ell}\)
5: // Compute new reference policy
6: Compute \(\widehat{U}_{\ell-1,h}(\pi,\pi^{\prime})\) as in (5.1) for all \((\pi,\pi^{\prime})\in\Pi_{\ell}\)
7: Choose \(\bar{\pi}_{\ell}\leftarrow\min_{\pi\in\Pi_{\ell}}\max_{\pi\in\Pi_{\ell}}\sum_{ h=1}^{H}\widehat{U}_{\ell-1,h}(\pi,\bar{\pi})\)
8: Collect the following number of episodes from \(\bar{\pi}_{\ell}\) and store in dataset \(\mathfrak{D}_{\ell}^{\mathrm{ref}}\) \[\bar{n}_{\ell}=\mathcal{O}\Big{(}\max_{\pi\in\Pi_{\ell}}c\cdot\frac{H\hat{U}_ {\ell-1}(\pi,\bar{\pi}_{\ell})}{\epsilon_{\ell}^{2}}\cdot\log\tfrac{H\hat{ \epsilon}^{2}\|\Pi_{\ell}\|}{\delta}\Big{)}\]
9: Compute \(\{\widehat{w}_{\ell,h}^{\bar{\pi}}(s)\}_{h=1}^{H}\) using empirical state visitation frequencies in \(\mathfrak{D}_{\ell}^{\mathrm{ref}}\)
10: // Estimate Policy Differences
11: Initialize \(\widehat{\delta}_{1}^{\pi}\gets 0\)
12:for\(h=1,\ldots,H\)do
13: Run OptCov (Algorithm 3) to collect dataset \(\mathfrak{D}_{\ell,h}^{\mathrm{ED}}\) such that: \[\sup_{\pi\in\Pi_{\ell}}\|(\bm{\pi}_{h}-\bm{\bar{\pi}}_{\ell,h})\widehat{w}_{ \ell,h}^{\bar{\pi}}+\bm{\pi}_{h}\widehat{\delta}_{\ell,h}^{\pi}\|_{\Lambda_{ \ell,h}^{-1}}^{2}\leq\epsilon_{\ell}^{2}/H^{4}\beta_{\ell}^{2}\quad\text{for} \quad\Lambda_{\ell,h}=\sum_{(s,a)\in\mathfrak{D}_{\ell,h}^{\mathrm{ED}}}e_{sa} e_{sa}^{\top}\] and \(\beta_{\ell}\leftarrow\mathcal{O}(\sqrt{\log SH\ell^{2}|\Pi_{\ell}|/\delta})\)
14: Use \(\mathfrak{D}_{\ell,h}^{\mathrm{ED}}\) to compute \(\widehat{P}_{\ell,h}(s^{\prime}|s,a)\) and \(\widehat{r}_{\ell,h}\)
15: Compute \(\widehat{\delta}_{\ell,h+1}^{\pi}\leftarrow\widehat{P}_{\ell,h}(\bm{\pi}_{h}- \bm{\bar{\pi}}_{\ell,h})\widehat{w}_{\ell,h}^{\bar{\pi}}+\widehat{P}_{\ell,h} \bm{\pi}_{h}\widehat{\delta}_{\ell,h}^{\pi})\)
16:endfor
17:// Eliminate suboptimal policies
18: Compute \(\widehat{D}_{\bar{\pi}_{\ell}}(\pi)\leftarrow\sum_{h}\langle\widehat{r}_{\ell, h},\bm{\pi}_{h}\widehat{\delta}_{\ell,h}\rangle+\sum_{h}\langle\widehat{r}_{ \ell,h},(\bm{\pi}_{h}-\bm{\bar{\pi}}_{\ell,h})\widehat{w}_{\ell,h}^{\pi}\rangle\)
19: Update \(\Pi_{\ell+1}=\Pi_{\ell}\backslash\{\pi\in\Pi_{\ell}:\max_{\pi^{\prime}}\widehat {D}_{\bar{\pi}_{\ell}}(\pi^{\prime})-\widehat{D}_{\bar{\pi}_{\ell}}(\pi)>8 \epsilon_{\ell}\ \}\)
20:if\(|\Pi_{\ell+1}|=1\)thenreturn\(\pi\in\Pi_{\ell+1}\)
21:endfor
22:return any \(\pi\in\Pi_{\ell+1}\) ```

**Algorithm 1**Perp: Policy Elimination with Reference Policy (informal)

## 6 When is Sufficient?

Our results so far show that \(\rho_{\Pi}\) is not in general sufficient for tabular RL. In this section, we consider several special cases where it _is_ sufficient.

Tabular Contextual Bandits.The tabular contextual bandit setting is the special case of the RL setting with \(H=1\) and where the initial action does not affect the next-state transition. Theorem 2.2 of Li et al. [30] show that if the rewards distributions \(\nu(s,a)\) are Gaussian for each \((s,a)\), where here \(s\) denotes the context, any \((0,\delta)\)-PAC algorithm requires at least \(\rho_{\Pi}\) samples. Crucially, however, they assume that the context distribution--in this case corresponding to the initial transition \(P_{1}\)--is known. Their algorithm makes explicit use of this fact, using this to estimate the value of \(\phi^{\pi}\). The following result shows that knowing the context distribution is not critical--we can achieve a complexity of \(\mathcal{O}(\rho_{\Pi})\) without this prior knowledge.

**Corollary 1**.: _For the setting of tabular contextual bandits, there exists an algorithm such that with probability at least \(1-2\delta\), as long as \(\Pi\) contains only deterministic policies, it finds an \(\epsilon\)-optimalpolicy and terminates after collecting at most the following number of samples:_

\[\inf_{\pi_{\mathrm{exp}}}\max_{\pi\in\Pi}\frac{\|\phi^{\star}-\phi^{\pi}\|^{2}_{ \Lambda_{(\pi_{\mathrm{exp}})^{-1}}}}{\max\{\epsilon^{2},\Delta(\pi)^{2}\}} \cdot\beta^{2}\log\frac{1}{\Delta_{\min}\vee\epsilon}+\frac{C_{\mathrm{poly}} }{\max\{\epsilon^{5/3},\Delta_{\min}^{5/3}\}},\]

_for \(C_{\mathrm{poly}}=\mathrm{poly}(|\mathcal{S}|,A,\log 1/\delta,\log 1/(\Delta_{ \min}\vee\epsilon),\log|\Pi|)\) and \(\beta=C\sqrt{\log(\frac{S|\Pi|}{\delta}\cdot\frac{1}{\Delta_{\min}\vee \epsilon})}\)._

The theorem is proved in Appendix D, and follows from the application of our algorithm Perp to the contextual bandit problem. The key intuition behind this result is that, in the contextual case:

\[U(\pi,\bar{\pi})=\mathbb{E}_{s\sim P_{1}}[(r_{1}(s,\pi_{1}(s))-r_{1}(s,\bar{ \pi}_{1}(s))^{2}]\leq\mathbb{E}_{s\sim P_{1}}[\mathbb{I}\{\pi_{1}(s)\neq\bar{ \pi}_{1}(s)\}].\]

It is then possible to show that, since \(\pi_{\mathrm{exp}}\) only has choices of which actions are taken (and cannot affect the context distribution), this can be further bounded by \(\inf_{\pi_{\mathrm{exp}}}\|\phi^{\pi}-\phi^{\bar{\pi}}\|^{2}_{\Lambda(\pi_{ \mathrm{exp}})^{-1}}\). This is not true in the full MDP case, where our choice of exploration policy in \(\pi_{\mathrm{exp}}\) could make \(\inf_{\pi_{\mathrm{exp}}}\|\phi^{\pi}-\phi^{\bar{\pi}}\|^{2}_{\Lambda(\pi_{ \mathrm{exp}})^{-1}}\) significantly smaller than \(U(\pi,\bar{\pi})\) (as is the case in Lemma 2). Hence, we observe that the cost of learning the contexts is dominated by that of learning the rewards in the case of contextual bandits. This is the opposite of tabular RL, where our complexity from Theorem 1 is unchanged (as seen in Section 4.2) even if we knew the reward distribution. This shows that there is a distinct separation between instance-optimal learning in tabular RL vs contextual bandits.

MDPs with Action-Independent Transitions.In the special case of MDPs where the transitions do not depend on the actions selected, the complexity simplifies to \(\mathcal{O}(\rho_{\Pi})\). Note that this exactly matches (up to lower order terms) the lower bound from [2].

**Corollary 2**.: _Assume that all \(P_{h}\) are such that \(P_{h}(s^{\prime}|s,a)=P_{h}(s^{\prime}|s,a^{\prime})\) for all \((a,a^{\prime})\in\mathcal{A}\). Then, with probability at least \(1-2\delta\), Perp (Algorithm 2) finds an \(\epsilon\)-optimal policy and terminates after collecting at most the following number of episodes:_

\[\sum_{h=1}^{H}\inf_{\pi_{\mathrm{exp}}}\max_{\pi\in\Pi}\frac{\|\phi^{\star}_{ h}-\phi^{\pi}_{h}\|^{2}_{\Lambda_{h}(\pi_{\mathrm{exp}})^{-1}}}{\max\{\epsilon^{2}, \Delta(\pi)^{2}\}}\cdot\iota H^{4}\beta^{2}+\frac{C_{\mathrm{poly}}}{\max\{ \epsilon^{5/3},\Delta_{\min}^{5/3}\}}\]

_for \(C_{\mathrm{poly}},\beta\) as defined in Theorem 1._

The intuition for Corollary 2 is similar to that of Corollary 1, and proved in Appendix E.

## 7 Discussion

In this paper, we performed a fine-grained study of the instance-dependent complexity of tabular RL. We proposed a new off-policy estimator that estimates the value relative to a reference policy. We leveraged this insight to close the instance-dependent contextual bandits problem and obtained the tightest known upper bound for tabular MDPs.

**Limitations and Future work** One limitation of the present work is that Perp, in it's current form, would be too computationally expensive to run for most practical applications; enumerating the policy set \(\Pi\) is often intractable, but works in contextual bandits have avoided this issue by only relying on argmax oracles over this set [1, 30]; an interesting direction of future work would be to extend this technique to tabular RL. Extending the results from this paper to obtain refined instance-dependent bounds for linear MDPs and general function approximation is an exciting direction as well.

The new estimator and its improved sample complexity raise additional theoretical questions. Our upper bound has unfortunate low order terms; can these be removed? Can one show that \(\frac{U(\pi,\bar{\pi})}{\max(\Delta(\pi)^{2},\epsilon^{2})}\) is unavoidable for all MDPs in general, thereby matching our upper bound? As discussed above, a few works have proven gap-dependent regret upper bounds, but we are unaware of any matching lower bounds besides over restricted classes of MDPs; can our estimator involving the differences result in even tighter instance-dependent regret bounds for MDPs?

## Acknowledgments

AN and LJR are supported in part by ONR YIP N000142012571 and NSF CAREER 1844729. AN was supported, in part by the Amazon Hub Fellowship at the University of Washington. KJ and AW were funded in part by NSF CAREER 2141511 and NSF TRIPODS 202329.

## References

* Agarwal et al. [2014] Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming the monster: A fast and simple algorithm for contextual bandits. In _International Conference on Machine Learning_, pages 1638-1646. PMLR, 2014.
* Al-Marjani et al. [2023] Aymen Al-Marjani, Andrea Tirinzoni, and Emilie Kaufmann. Towards instance-optimality in online pac reinforcement learning. _arXiv preprint arXiv:2311.05638_, 2023.
* Auer and Ortner [2006] Peter Auer and Ronald Ortner. Logarithmic online regret bounds for undiscounted reinforcement learning. _Advances in neural information processing systems_, 19, 2006.
* Auer et al. [2008] Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for reinforcement learning. _Advances in neural information processing systems_, 21, 2008.
* Azar et al. [2017] Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforcement learning. In _International Conference on Machine Learning_, pages 263-272. PMLR, 2017.
* Bose et al. [2023] Avinandan Bose, Mihaela Curmei, Daniel L. Jiang, Jamie Morgenstern, Sarah Dean, Lillian J. Ratliff, and Maryam Fazel. Initializing Services in Interactive ML Systems for Diverse Users. _arXiv preprint arXiv:2312.11846_, 2023.
* Bose et al. [2024] Avinandan Bose, Simon Shaolei Du, and Maryam Fazel. Offline Multi-task Transfer RL with Representational Penalization. _arXiv preprint arXiv:2402.12570_, 2024.
* Brafman and Tennenholtz [2002] Ronen I Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for near-optimal reinforcement learning. _Journal of Machine Learning Research_, 3(Oct):213-231, 2002.
* Dann and Brunskill [2015] Christoph Dann and Emma Brunskill. Sample complexity of episodic fixed-horizon reinforcement learning. _Advances in Neural Information Processing Systems_, 2015.
* Dann et al. [2017] Christoph Dann, Tor Lattimore, and Emma Brunskill. Unifying pac and regret: Uniform pac bounds for episodic reinforcement learning. _Advances in Neural Information Processing Systems_, 2017.
* Dann et al. [2019] Christoph Dann, Lihong Li, Wei Wei, and Emma Brunskill. Policy certificates: Towards accountable reinforcement learning. In _International Conference on Machine Learning_, pages 1507-1516. PMLR, 2019.
* Dann et al. [2021] Christoph Dann, Teodor Vanislavov Marinov, Mehryar Mohri, and Julian Zimmert. Beyond value-function gaps: Improved instance-dependent regret bounds for episodic reinforcement learning. _Advances in Neural Information Processing Systems_, 34, 2021.
* Degenne et al. [2020] Remy Degenne, Pierre Menard, Xuedong Shang, and Michal Valko. Gamification of pure exploration for linear bandits. In _International Conference on Machine Learning_, pages 2432-2442. PMLR, 2020.
* Dong and Ma [2022] Kefan Dong and Tengyu Ma. Asymptotic instance-optimal algorithms for interactive decision making. _arXiv preprint arXiv:2206.02326_, 2022.
* Farias et al. [2022] Vivek Farias, Andrew Li, Tianyi Peng, and Andrew Zheng. Markovian interference in experiments. _Advances in Neural Information Processing Systems_, 35:535-549, 2022.
* Fiez et al. [2019] Tanner Fiez, Lalit Jain, Kevin G Jamieson, and Lillian Ratliff. Sequential experimental design for transductive linear bandits. _Advances in neural information processing systems_, 32, 2019.
* Glynn et al. [2020] Peter W Glynn, Ramesh Johari, and Mohammad Rasouli. Adaptive experimental design with temporal interference: A maximum likelihood approach. _Advances in Neural Information Processing Systems_, 33:15054-15064, 2020.
* He et al. [2021] Jiafan He, Dongruo Zhou, and Quanquan Gu. Logarithmic regret for reinforcement learning with linear function approximation. _International Conference on Machine Learning_, 2021.

* [19] Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is q-learning provably efficient? In _Proceedings of the 32nd International Conference on Neural Information Processing Systems_, pages 4868-4878, 2018.
* [20] Anders Jonsson, Emilie Kaufmann, Pierre Menard, Omar Darwiche Domingues, Edouard Leurent, and Michal Valko. Planning in markov decision processes with gap-dependent sample complexity. _Advances in Neural Information Processing Systems_, 2020.
* [21] Sham Machandranath Kakade. _On the sample complexity of reinforcement learning_. PhD thesis, UCL (University College London), 2003.
* [22] Julian Katz-Samuels, Lalit Jain, and Kevin G Jamieson. An empirical process approach to the union bound: Practical algorithms for combinatorial and linear bandits. _Advances in Neural Information Processing Systems_, 33:10371-10382, 2020.
* [23] Emilie Kaufmann, Olivier Cappe, and Aurelien Garivier. On the complexity of best-arm identification in multi-armed bandit models. _The Journal of Machine Learning Research_, 17(1):1-42, 2016.
* [24] Michael Kearns and Satinder Singh. Finite-sample convergence rates for q-learning and indirect algorithms. _Advances in neural information processing systems_, 11, 1998.
* [25] Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. _Machine learning_, 49(2):209-232, 2002.
* [26] Michael Kearns, Yishay Mansour, and Andrew Ng. Approximate planning in large pomdps via reusable trajectories. _Advances in Neural Information Processing Systems_, 12, 1999.
* [27] Johannes Kirschner, Tor Lattimore, Claire Vernade, and Csaba Szepesvari. Asymptotically optimal information-directed sampling. In _Conference on Learning Theory_, pages 2777-2821. PMLR, 2021.
* [28] Tor Lattimore and Marcus Hutter. Pac bounds for discounted mdps. In _International Conference on Algorithmic Learning Theory_, pages 320-334. Springer, 2012.
* [29] Tor Lattimore and Csaba Szepesvari. The end of optimism? an asymptotic analysis of finite-armed linear bandits. In _Artificial Intelligence and Statistics_, pages 728-737. PMLR, 2017.
* [30] Zhaoqi Li, Lillian Ratliff, Houssam Nassif, Kevin Jamieson, and Lalit Jain. Instance-optimal PAC algorithms for contextual bandits. _Advances in Neural Information Processing Systems_, 2022.
* [31] Aymen Al Marjani and Alexandre Proutiere. Adaptive sampling for best policy identification in markov decision processes. _International Conference on Machine Learning_, 2021.
* [32] Aymen Al Marjani, Aurelien Garivier, and Alexandre Proutiere. Navigating to the best policy in markov decision processes. _Neural Information Processing Systems_, 2021.
* [33] Pierre Menard, Omar Darwiche Domingues, Anders Jonsson, Emilie Kaufmann, Edouard Leurent, and Michal Valko. Fast active learning for pure exploration in reinforcement learning. _International Conference on Machine Learning_, 2021.
* [34] Remi Munos and Csaba Szepesvari. Finite-time bounds for fitted value iteration. _Journal of Machine Learning Research_, 9(5), 2008.
* [35] Jungseul Ok, Alexandre Proutiere, and Damianos Tranos. Exploration in structured reinforcement learning. _arXiv preprint arXiv:1806.00775_, 2018.
* [36] Max Simchowitz and Kevin G Jamieson. Non-asymptotic gap-dependent regret bounds for tabular mdps. _Advances in Neural Information Processing Systems_, 32, 2019.
* [37] Marta Soare, Alessandro Lazaric, and Remi Munos. Best-arm identification in linear bandits. _Advances in Neural Information Processing Systems_, 27, 2014.

* [38] Alexander L Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael L Littman. Pac model-free reinforcement learning. In _Proceedings of the 23rd international conference on Machine learning_, pages 881-888, 2006.
* [39] Alexander L Strehl, Lihong Li, and Michael L Littman. Reinforcement learning in finite mdps: Pac analysis. _Journal of Machine Learning Research_, 10(11), 2009.
* [40] Andrea Tirinzoni, Matteo Pirotta, Marcello Restelli, and Alessandro Lazaric. An asymptotically optimal primal-dual incremental algorithm for contextual linear bandits. _Neural Information Processing Systems_, 2020.
* [41] Andrea Tirinzoni, Aymen Al-Marjani, and Emilie Kaufmann. Near instance-optimal pac reinforcement learning for deterministic mdps. _Neural Information Processing Systems_, 2022.
* [42] Andrew Wagenmaker and Kevin G Jamieson. Instance-dependent near-optimal policy identification in linear mdps via online experiment design. _Advances in Neural Information Processing Systems_, 35:5968-5981, 2022.
* [43] Andrew Wagenmaker and Aldo Pacchiano. Leveraging offline data in online reinforcement learning. _International Conference of Machine Learning_, 2023.
* [44] Andrew Wagenmaker, Yifang Chen, Max Simchowitz, Simon S Du, and Kevin Jamieson. First-order regret in reinforcement learning with linear function approximation: A robust estimation approach. _International Conference of Machine Learning_, 2022.
* [45] Andrew J Wagenmaker and Dylan J Foster. Instance-optimality in interactive decision making: Toward a non-asymptotic theory. In _The Thirty Sixth Annual Conference on Learning Theory_, pages 1322-1472. PMLR, 2023.
* [46] Andrew J Wagenmaker, Max Simchowitz, and Kevin Jamieson. Beyond no regret: Instance-dependent pac reinforcement learning. In _Conference on Learning Theory_, pages 358-418. PMLR, 2022.
* [47] Haike Xu, Tengyu Ma, and Simon S Du. Fine-grained gap-dependent bounds for tabular mdps via adaptive multi-step bootstrap. _Conference on Learning Theory_, 2021.
* [48] Andrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds. In _International Conference on Machine Learning_, pages 7304-7312. PMLR, 2019.
* [49] Andrea Zanette, Mykel J Kochenderfer, and Emma Brunskill. Almost horizon-free structure-aware best policy identification with a generative model. In _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* [50] Zihan Zhang, Xiangyang Ji, and Simon S Du. Is reinforcement learning more difficult than bandits? a near-optimal algorithm escaping the curse of horizon. _Conference on Learning Theory_, 2021.
* [51] Zihan Zhang, Yuxin Chen, Jason D Lee, and Simon S Du. Settling the sample complexity of online reinforcement learning. _arXiv preprint arXiv:2307.13586_, 2023.

###### Contents

* 1 Introduction
* 2 Related Work
* 3 Preliminaries and Problem Setting
* 4 What is the Sample Complexity of Tabular RL?
	* 4.1 Main Result
	* 4.2 The Main Algorithmic Insight: The Reduced-Variance Difference Estimator
* 5 Achieving Theorem 1: Perp Algorithm
* 6 When is Sufficient?
* 7 Discussion
* A Understanding the origins of \(U(\pi,\bar{\pi})\)
* B Tabular MDPs: Comparison with Prior Work and Lower Bounds
* B.1 Comparison with complexities from prior work
* B.2 Lower bound
* C Tabular MDP Upper Bound
* C.1 Notation
* C.2 Technical Results
* C.3 Concentration Arguments and Good Events
* C.4 Estimation of Reference Policy and Values
* C.5 Correctness and Sample Complexity
* D Tabular Contextual Bandits: Upper Bound
* E MDPs with Action-Independent Transitions
* F Tabular Franke Wolfe
* F.1 Data Conditioning
* F.2 Online Frank-Wolfe
* F.3 Pruning Hard-to-Reach States

## Appendix A Understanding the origins of \(U(\pi,\bar{\pi})\)

This section is inspired by the exposition of Soare et al. [37] for justifying the sample complexity of linear bandits. Fix a reference policy \(\bar{\pi}\) and some (stochastic) logging policy \(\mu\). For \(K\in\mathbb{N}\) to be determined later, roll out \(\bar{\pi}\)\(K\) times and compute the empirical state visitations \(\widehat{w}_{h}^{\bar{\pi}}(s)=\frac{1}{K}\sum_{k=1}^{K}\sum_{s,h}\mathbf{1} \{s_{h}^{k}=s\}\). Also roll out \(\mu\)\(K\) times and compute the empirical transition probabilities \(\widehat{P}_{h}(s^{\prime}|s,a)=\frac{\sum_{k=1}^{K}\mathbf{1}\{(s_{h}^{s},a_{ h}^{s},s_{h+1}^{s})=(s,a,s^{\prime})\}}{\sum_{k=1}^{K}\mathbf{1}\{(s_{h}^{s},a_{ h}^{s})=(s,a)\}}\). For any \(\pi\neq\bar{\pi}\), use \(\{\widehat{P}_{h}(s^{\prime}|s,a)\}_{s,a,s^{\prime},h}\) to compute \(\widehat{w}_{h}^{\pi}(s)\). With \(\delta_{h+1}^{\pi}:=w_{h+1}^{\pi}-w_{h+1}^{\bar{\pi}}=P_{h}\boldsymbol{\pi}_ {h}w_{h}^{\pi}-P_{h}\boldsymbol{\pi}_{h}w_{h}^{\bar{\pi}}=P_{h}\boldsymbol{\pi }_{h}\delta_{h}^{\pi}+P_{h}(\boldsymbol{\pi}_{h}-\boldsymbol{\bar{\pi}}_{h})w_ {h}^{\bar{\pi}}\) set

\[D(\pi)=V_{0}^{\pi}-V_{0}^{\bar{\pi}}=\sum_{h=1}^{H}\langle r_{h},\boldsymbol{ \pi}_{h}w_{h}^{\pi}-\boldsymbol{\bar{\pi}}_{h}w_{h}^{\bar{\pi}}\rangle=\sum_{h =1}^{H}\langle r_{h},\boldsymbol{\pi}_{h}\delta_{h}^{\pi}\rangle+\langle r_{h},(\boldsymbol{\pi}_{h}-\boldsymbol{\bar{\pi}}_{h})w_{h}^{\bar{\pi}}\rangle\]

and also define the empirical counterparts \(\widehat{\delta}_{h+1}^{\pi}:=\widehat{P}_{h}\boldsymbol{\pi}_{h}\widehat{ \delta}_{h}^{\pi}+\widehat{P}_{h}(\boldsymbol{\pi}_{h}-\boldsymbol{\bar{\pi}} _{h})\widehat{w}_{h}^{\bar{\pi}}\) with

\[\widehat{D}(\pi)=\sum_{h=1}^{H}\langle r_{h},\boldsymbol{\pi}_{h}\widehat{ \delta}_{h}^{\pi}\rangle+\langle r_{h},(\boldsymbol{\pi}_{h}-\boldsymbol{ \bar{\pi}}_{h})\widehat{w}_{h}^{\bar{\pi}}\rangle.\]

\begin{table}
\begin{tabular}{l l} \hline Notation & Description \\ \hline \(\mathcal{S}\) & State space \\ \(\mathcal{A}\) & Action space \\ \(H\) & Horizon \\ \(P_{h}\) & Transition matrix at stage \(h\) \\ \(v_{h}\) & Distribution over reward at stage \(h\) \\ \(r_{h}(s,a)\) & Expected reward at stage \(h\) for state \(s\) and action \(a\) \\ \(\pi\) & Policy \\ \(\Pi\) & Set of candidate policies \\ \(\pi_{h}(s)\) & Distribution over actions for policy \(\pi\) at state \(s\) and stage \(h\) \\ \(w_{h}^{\pi}\) & State visitation vector at step \(h\) for policy \(\pi\) \\ \(\boldsymbol{\pi}_{h}\) & Policy matrix for policy \(\pi\) at step \(h\) \\ \(\phi_{h}^{\pi}\) & State-action visitation vector for policy \(\pi\) at step \(h\) \\ \(\Lambda_{h}(\pi)\) & Expected covariance matrix at timestep \(h\) for policy \(\pi\) \\ \(Q_{h}^{\pi}(s,a)\) & Q-value function for policy \(\pi\) at state \(s\), action \(a\), and step \(h\) \\ \(V_{h}^{\pi}(s)\) & Value function for policy \(\pi\) at state \(s\) and step \(h\) \\ \(V^{\pi}\) & Value of policy \(\pi\) \\ \(\pi^{\star}\) & Optimal policy within \(\Pi\) \\ \(\Delta(\pi)\) & Suboptimality of policy \(\pi\) \\ \(W_{h}^{\star}(s)\) & Maximum probability of reaching state \(s\) at step \(h\) over all policies \\ \(\mathcal{C}\) & Context space (for contextual bandits) \\ \(\mu^{\star}\) & Context distribution (for contextual bandits) \\ \(\theta^{\star}\) & Reward parameters (for contextual bandits) \\ \(\rho_{\Pi}\) & Complexity measure based on feature differences \\ \(\bar{\pi}_{\ell}\) & Reference policy \\ \(\bar{\delta}_{h}^{\pi}\) & Difference in state visitation between policy \(\pi\) and reference policy at step \(h\) \\ \(D_{\bar{\pi}_{\ell}}(\pi)\) & Difference in value between policy \(\pi\) and reference policy \\ \(U_{h}(\pi,\pi^{\prime})\) & Expected squared difference in Q-values between policies \(\pi\) and \(\pi^{\prime}\) at step \(h\) \\ \(\mathcal{S}_{\ell}^{\text{keep}}\) & Set of reachable states at epoch \(\ell\) \\ \(\epsilon_{\inf}^{\ell}\) & Minimum reachability threshold at epoch \(\ell\) \\ \(\epsilon_{\exp}^{\ell}\) & Tolerance for experiment design at epoch \(\ell\) \\ \(\beta_{\ell}\) & Confidence parameter at epoch \(\ell\) \\ \(n_{\ell},K_{\,\,\,\,\text{unif}}^{\ell}\) & Number of samples and minimum exploration at epoch \(\ell\) \\ \(\mathfrak{D}_{\ell,h}^{\text{ED}}\) & Dataset collected during exploration in PERP \\ \(\mathfrak{D}_{\ell}^{\text{ref}}\) & Dataset collected from reference policy \\ \hline \end{tabular}
\end{table}
Table 1: Table of notation used in the paperIf \(\widehat{\pi}=\arg\max_{\pi\in\Pi}\widehat{D}(\pi)\), how large must \(K\) be to ensure that \(\widehat{\pi}=\pi^{\star}:=\arg\max_{\pi\in\Pi}D(\pi)=\arg\max_{\pi\in\Pi}V_{0}^{\pi}\)?

Assume at time \(h=0\) all policies are initialized arbitrarily in some state \(s_{0}\) so that \(\widehat{P}_{0}(s^{\prime}|s_{0},a)\) simply defines the initial empirical state distribution at time \(h=1\). Let \(\widehat{w}_{0}^{\pi}(s_{0})=w_{0}^{\pi}(s_{0})=1\) We can then unroll the recursion for \(h=0,\ldots,H-1\)

\[\widehat{\delta}_{h+1}^{\pi}-\delta_{h+1}^{\pi}=\widehat{P}_{h} \boldsymbol{\pi}_{h}\widehat{\delta}_{h}^{\pi}+\widehat{P}_{h}(\boldsymbol{ \pi}_{h}-\bar{\boldsymbol{\pi}}_{h})\widehat{w}_{h}^{\pi}-\delta_{h+1}^{\pi}\] \[=(\widehat{P}_{h}-P_{h})\boldsymbol{\pi}_{h}\delta_{h}^{\pi}+( \widehat{P}_{h}-P_{h})(\boldsymbol{\pi}_{h}-\bar{\boldsymbol{\pi}}_{h})w_{h}^ {\pi}+P_{h}(\boldsymbol{\pi}_{h}-\bar{\boldsymbol{\pi}}_{h})(\widehat{w}_{h}^ {\pi}-w_{h}^{\bar{\pi}})+P_{h}\boldsymbol{\pi}_{h}(\widehat{\delta}_{h}^{\pi} -\delta_{h}^{\pi})\] \[\qquad+(\underbrace{\widehat{P}_{h}-P_{h}}_{h})\boldsymbol{\pi}_ {h}(\widehat{\delta}_{h}^{\pi}-\delta_{h}^{\pi})+(\widehat{P}_{h}-P_{h})( \boldsymbol{\pi}_{h}-\bar{\boldsymbol{\pi}}_{h})(\widehat{w}_{h}^{\pi}-w_{h}^ {\bar{\pi}})\] \[\approx(\widehat{P}_{h}-P_{h})(\phi_{k}^{\pi}-\phi_{k}^{\pi})+P_{ h}(\boldsymbol{\pi}_{h}-\bar{\boldsymbol{\pi}}_{h})(\widehat{w}_{h}^{\pi}-w_{h}^ {\bar{\pi}})+P_{h}\boldsymbol{\pi}_{h}(\widehat{\delta}_{h}^{\pi}-\delta_{h}^ {\pi})\] \[\approx\sum_{i=0}^{h}\big{(}\prod_{j=h-i+1}^{h}P_{j}\boldsymbol{ \pi}_{j}\big{)}\big{(}(\widehat{P}_{h-i}-P_{h-i})(\phi_{h-i}^{\pi}-\phi_{h-i}^ {\bar{\pi}})+P_{h-i}(\boldsymbol{\pi}_{h-i}-\bar{\boldsymbol{\pi}}_{h-i})( \widehat{w}_{h-i}^{\bar{\pi}}-w_{h-i}^{\bar{\pi}})\big{)}\] \[=\sum_{k=0}^{h}\big{(}\prod_{j=k+1}^{h}P_{j}\boldsymbol{\pi}_{j} \big{)}\big{(}(\widehat{P}_{k}-P_{k})(\phi_{k}^{\pi}-\phi_{k}^{\bar{\pi}})+P_{ k}(\boldsymbol{\pi}_{k}-\bar{\boldsymbol{\pi}}_{k})(\widehat{w}_{k}^{\bar{\pi}}-w_{k}^ {\bar{\pi}})\big{)}\]

where we recall \(\phi_{k}^{\pi}=\boldsymbol{\pi}_{k}w_{k}^{\pi}\). If \(\epsilon_{k+1}:=(\widehat{P}_{h}-P_{k})(\boldsymbol{\pi}_{h}w_{k}^{\pi}-\bar{ \boldsymbol{\pi}}w_{k}^{\bar{\pi}})+P_{k}(\boldsymbol{\pi}_{k}-\bar{ \boldsymbol{\pi}}_{k})(\widehat{w}_{k}^{\bar{\pi}}-w_{k}^{\bar{\pi}})\) then

\[\sum_{h=1}^{H}\langle r_{h},\boldsymbol{\pi}_{h}(\widehat{ \delta}_{h}^{\pi}-\delta_{h}^{\pi})\rangle =\sum_{h=1}^{H}\sum_{k=0}^{h-1}\langle r_{h},\boldsymbol{\pi}_{ h}\Big{(}\prod_{j=k+1}^{h-1}P_{j}\boldsymbol{\pi}_{j}\Big{)}\epsilon_{k+1}\rangle\] \[=\sum_{k=0}^{H-1}\sum_{h=k+1}^{H}\langle r_{h},\boldsymbol{\pi}_{ h}\Big{(}\prod_{j=k+1}^{h-1}P_{j}\boldsymbol{\pi}_{j}\Big{)}\epsilon_{k+1} \rangle=\sum_{k=0}^{H-1}\langle V_{k+1}^{\pi},\epsilon_{k+1}\rangle\] \[=\sum_{k=0}^{H-1}\langle V_{k+1}^{\pi},(\widehat{P}_{k}-P_{k})( \phi_{k}^{\pi}-\phi_{k}^{\bar{\pi}})+P_{k}(\boldsymbol{\pi}_{k}-\bar{ \boldsymbol{\pi}}_{k})(\widehat{w}_{k}^{\bar{\pi}}-w_{k}^{\bar{\pi}})\rangle.\]

Finally, we use these calculations to compute the deviation

\[\widehat{D}(\pi)-D(\pi) =\sum_{h=1}^{H}\langle r_{h},\boldsymbol{\pi}_{h}(\widehat{ \delta}_{h}^{\pi}-\delta_{h}^{\pi})\rangle+\langle r_{h},(\boldsymbol{\pi}_{h}- \bar{\boldsymbol{\pi}}_{h})(\widehat{w}_{h}^{\bar{\pi}}-w_{h}^{\bar{\pi}})\rangle\] \[=\sum_{h=0}^{H-1}\langle V_{h+1}^{\pi},(\widehat{P}_{h}-P_{h})( \phi_{h}^{\pi}-\phi_{h}^{\bar{\pi}})\rangle+\langle r_{h}+P_{h}^{\top}V_{h+1}^{ \pi},(\boldsymbol{\pi}_{h}-\bar{\boldsymbol{\pi}}_{h})(\widehat{w}_{h}^{\bar{ \pi}}-w_{h}^{\bar{\pi}})\rangle\] \[=\sum_{h=0}^{H-1}\langle V_{h+1}^{\pi},(\widehat{P}_{h}-P_{h})( \phi_{h}^{\pi}-\phi_{h}^{\bar{\pi}})\rangle+\langle Q_{h}^{\pi},(\boldsymbol{ \pi}_{h}-\bar{\boldsymbol{\pi}}_{h})(\widehat{w}_{h}^{\bar{\pi}}-w_{h}^{\bar{\pi}})\rangle\] \[=\sum_{h=0}^{H-1}\sum_{s,a,s^{\prime}}V_{h+1}^{\pi}(s^{\prime})( \widehat{P}_{h}(s^{\prime}|s,a)-P_{h}(s^{\prime}|s,a))(\phi_{h}^{\pi}(s,a)- \phi_{h}^{\bar{\pi}}(s,a))\] \[\qquad+\sum_{h=0}^{H-1}\sum_{s}\big{(}Q_{h}^{\pi}(s,\pi_{h}(s))-Q_ {h}^{\pi}(s,\bar{\pi}_{h}(s))\big{)}(\widehat{w}_{h}^{\bar{\pi}}(s)-w_{h}^{\bar{ \pi}}(s))\] \[\lesssim\sqrt{\sum_{h=0}^{H-1}\sum_{s,a,s^{\prime}}V_{h+1}^{\pi}(s^ {\prime})^{2}\frac{P_{h}(s^{\prime}|s,a)}{K\mu_{h}(s,a)}(\phi_{h}^{\pi}(s,a)-\phi_ {h}^{\bar{\pi}}(s,a))^{2}}\] \[\qquad+\sqrt{\sum_{h=0}^{H-1}\sum_{s}\big{(}Q_{h}^{\pi}(s,\pi_{h}(s) )-Q_{h}^{\pi}(s,\bar{\pi}_{h}(s))\big{)}^{2}\frac{w_{h}^{\bar{\pi}}(s)}{K}}.\]Applying \(\sum_{s^{\prime}}V_{h+1}^{\pi}(s^{\prime})^{2}P_{h}(s^{\prime}|s,a)\leq H^{2}\), we observe that if

\[K \geq\min_{\mu,\pi}\max_{\pi}H^{2}\sum_{h=1}^{H-1}\frac{\sum_{s,a}( \phi_{h}^{\pi}(s,a)-\phi_{h}^{\bar{\pi}}(s,a))^{2}/\mu_{h}(s,a)}{\Delta(\pi)^{2}}\] \[\quad+\sum_{h=1}^{H-1}\frac{\sum_{s}\big{(}Q_{h}^{\pi}(s,\pi_{h}( s))-Q_{h}^{\pi}(s,\bar{\pi}_{h}(s))\big{)}^{2}w_{h}^{\bar{\pi}}(s)}{\Delta(\pi)^{2}}\]

and we employ the minimizers \(\mu,\bar{\pi}\) to collect data, then \(\widehat{D}(\pi)-D(\pi)<\Delta(\pi)\) and \(\widehat{\pi}=\arg\max_{\pi\in\Pi}\widehat{D}(\pi)=\arg\max_{\pi\in\Pi}D(\pi)\). Notice that up to \(H\) and \(\log\) factors, this is precisely the sample complexity of our algorithm. A natural candidate for \(\bar{\pi}\) is \(\pi^{\star}\) so that the first term matches the lower bound of [2].

On the other hand, suppose we used the data from the logging policy \(\mu\) to compute the empirical state visitations \(\widehat{w}_{h}^{\pi}\) for all \(\pi\in\Pi\) and set \(\widehat{\pi}=\arg\max_{\pi\in\Pi}\sum_{h=1}^{H}\langle r_{h},\bm{\pi}\widehat {w}_{h}^{\pi}\rangle=:\widehat{V}_{0}^{\pi}\). Using the same techniques as above, it is straightforward to show that if

\[\widehat{w}_{h+1}^{\pi}-w_{h+1}^{\pi} =\widehat{P}_{h}\bm{\pi}_{h}\widehat{w}_{h}^{\pi}-P_{h}\bm{\pi}_ {h}w_{h}^{\pi}\] \[=(\widehat{P}_{h}-P_{h}+P_{h})\bm{\pi}_{h}(\widehat{w}_{h}^{\pi} -w_{h}^{\pi}+w_{h}^{\pi})-P_{h}\bm{\pi}_{h}w_{h}^{\pi}\] \[=(\widehat{P}_{h}-P_{h})\bm{\pi}_{h}w_{h}^{\pi}+P_{h}\bm{\pi}_{h} (\widehat{w}_{h}^{\pi}-w_{h}^{\pi})+\underbrace{(\widehat{P}_{h}-P_{h})\bm{ \pi}_{h}(\widehat{w}_{h}^{\pi}-w_{h}^{\pi})}_{\text{Low order terms}\approx 0}\] \[\approx\sum_{i=0}^{h}\big{(}\prod_{j=h-i+1}^{h}P_{j}\bm{\pi}_{j} \big{)}(\widehat{P}_{h-i}-P_{h-i})\bm{\pi}_{h-i}w_{h-i}^{\pi}\] \[=\sum_{k=0}^{h}\big{(}\prod_{j=k+1}^{h}P_{j}\bm{\pi}_{j}\big{)}( \widehat{P}_{k}-P_{k})\bm{\pi}_{k}w_{k}^{\pi}\]

and we employ the minimizer \(\mu\) to collect data, then \(\widehat{V}_{0}^{\pi}-V_{0}^{\pi}\leq\Delta(\pi)\) and \(\widehat{\pi}=\arg\max_{\pi\in\Pi}\widehat{V}_{0}^{\pi}=\arg\max_{\pi\in\Pi}V_ {0}^{\pi}\).

## Appendix B Tabular MDPs: Comparison with Prior Work and Lower Bounds

Illustrative Family of MDP InstancesRecall the family of MDP instances in the introduction (visualized in Figure 2 for ease of reference). The family of MDPs is parameterized by \(\epsilon,\epsilon_{1},\epsilon_{2}>0\), with \(H=2\), \(\mathcal{S}=\{s_{1},s_{2},s_{3},s_{4}\}\), and \(\mathcal{A}=\{a_{1},a_{2},a_{3}\}\), which start in state \(s_{0}\) and are defined as:

\[P_{1}(s_{2}\mid s_{1},a_{1})=1-3\epsilon,\quad P_{1}(s_{3}\mid s _{1},a_{1})=\epsilon_{1},\quad P_{1}(s_{4}\mid s_{1},a_{1})=\epsilon_{2}\] \[P_{1}(s_{3}\mid s_{1},a_{2})=P_{1}(s_{4}\mid s_{1},a_{3})=1.\]

We define the reward function so that all rewards are 0 except \(r_{1}(s_{1},a_{1})=r_{2}(s_{3},a_{1})=r_{2}(s_{4},a_{2})=1\) for all \(a\).

Let \(\mathcal{M}\) denote the MDP above with \(\epsilon_{1}=2\epsilon,\epsilon_{2}=\epsilon\), and \(\mathcal{M}^{\prime}\) the MDP above with \(\epsilon_{1}=\epsilon,\epsilon_{2}=2\epsilon\).

Figure 2: A motivating example for differences. All rewards other than the ones specified in the figure are \(0\).

Let \(\Pi=\{\pi_{1},\pi_{2}\}\) denote some set of policies. Let \(\pi_{1}\) denote the policy which always plays \(a_{1}\), and \(\pi_{2}\) the policy which plays \(a_{1}\) at green states and \(a_{2}\) at red states i.e \(\pi_{2}(s_{1})=\pi_{2}(s_{2})=a_{1}\) and \(\pi_{2}(s_{3})=\pi_{2}(s_{4})=a_{2}\).

Now note that \(V_{0}^{\mathcal{M},\pi_{1}}=1+2\epsilon\), \(V_{0}^{\mathcal{M},\pi_{2}}=1+\epsilon\), \(V_{0}^{\mathcal{M}^{\prime},\pi_{1}}=1+\epsilon\), and \(V_{0}^{\mathcal{M}^{\prime},\pi_{2}}=1+2\epsilon\).

### Comparison with complexities from prior work

The lemma below shows that the upper bound presented in Theorem 1 is smaller than that of Pedel from Theorem 1 of [42] for all MDP instances.

**Lemma 4**.: _For any MDP instance and policy set \(\Pi\), we have that_

1. \(\inf_{\pi_{\mathrm{exp}}}\max_{\pi\in\Pi}\frac{\|\phi_{h}^{\pi}\|_{\Lambda_{h} (\pi_{\mathrm{exp}})^{-1}}^{2}}{\max\{\epsilon^{2},\Delta(\pi)^{2},\Delta_{ \mathrm{min}}^{2}\}}\geq\frac{1}{\max\{\epsilon^{2},\Delta(\pi)^{2},\Delta_{ \mathrm{min}}^{2}\}}\)__
2. \[H^{4}\sum_{h=1}^{H}\inf_{\pi_{\mathrm{exp}}}\max_{\pi\in\Pi}\frac{\|\phi_{h}^{ \star}-\phi_{h}^{\pi}\|_{\Lambda_{h}(\pi_{\mathrm{exp}})^{-1}}^{2}}{\max\{ \epsilon^{2},\Delta(\pi)^{2},\Delta_{\mathrm{min}}^{2}\}}\leq 4H^{4}\sum_{h=1}^{H} \inf_{\pi_{\mathrm{exp}}}\max_{\pi\in\Pi}\frac{\|\phi_{h}^{\pi}\|_{\Lambda_{h }(\pi_{\mathrm{exp}})^{-1}}^{2}}{\max\{\epsilon^{2},\Delta(\pi)^{2},\Delta_{ \mathrm{min}}^{2}\}}\]
3. \(\frac{HU(\pi,\pi^{\star})}{\max\{\epsilon^{2},\Delta(\pi)^{2},\Delta_{ \mathrm{min}}^{2}\}}\leq H^{4}\sum_{h=1}^{H}\inf_{\pi_{\mathrm{exp}}}\max_{\pi \in\Pi}\frac{\|\phi_{h}^{\pi}\|_{\Lambda_{h}(\pi_{\mathrm{exp}})^{-1}}^{2}}{ \max\{\epsilon^{2},\Delta(\pi)^{2},\Delta_{\mathrm{min}}^{2}\}}\)__

Proof.: **Proof of Claim 1.** Note that

\[\|\phi_{h}^{\pi}\|_{\Lambda_{h}(\pi_{\mathrm{exp}})^{-1}}^{2}=\sum_{s,a}\frac {\phi_{h}^{\pi}(s,a)^{2}}{\phi_{h}^{\pi_{\mathrm{exp}}}(s,a)}\geq\inf_{\lambda \in\Delta_{SA}}\sum_{s,a}\frac{\phi_{h}^{\pi}(s,a)^{2}}{\lambda_{s,a}}\]

In order to solve this optimization problem, we can consider the KKT conditions. We can verify from stationarity that at optimality, \(\lambda_{s,a}=\frac{\phi_{h}^{\pi}(s,a)}{\sqrt{\beta}}\) for some constant \(\beta>0\). But since \(\lambda_{s,a}\) must live in the simplex \(\Delta_{SA}\), and since \(\phi_{h}^{\pi}(s,a)\) is itself a distribution over \(\mathcal{S}\times\mathcal{A}\), it follows that \(\beta=1\) must be true. Plugging this optimal value into the above, we obtain that

\[\|\phi_{h}^{\pi}\|_{\Lambda_{h}(\pi_{\mathrm{exp}})^{-1}}^{2}\geq\inf_{\lambda \in\Delta_{SA}}\sum_{s,a}\frac{\phi_{h}^{\pi}(s,a)^{2}}{\lambda_{s,a}}=1\]

Then,

\[\inf_{\pi_{\mathrm{exp}}}\max_{\pi\in\Pi}\frac{\|\phi_{h}^{\pi}\|_{\Lambda_{h} (\pi_{\mathrm{exp}})^{-1}}^{2}}{\max\{\epsilon^{2},\Delta(\pi)^{2},\Delta_{ \mathrm{min}}^{2}\}}\geq\frac{1}{\max\{\epsilon^{2},\Delta(\pi)^{2},\Delta_{ \mathrm{min}}^{2}\}}\]

directly follows from the above.

Proof of Claim 2.From the triangle inequality,

\[\inf_{\pi_{\mathrm{exp}}}\max_{\pi\in\Pi}\frac{\|\phi_{h}^{\star}- \phi_{h}^{\pi}\|_{\Lambda_{h}(\pi_{\mathrm{exp}})^{-1}}^{2}}{\max\{\epsilon^{2}, \Delta(\pi)^{2},\Delta_{\mathrm{min}}^{2}\}}\] \[\leq 2\inf_{\pi_{\mathrm{exp}}}\max_{\pi\in\Pi}\left(\frac{\|\phi_ {h}^{\star}\|_{\Lambda_{h}(\pi_{\mathrm{exp}})^{-1}}^{2}}{\max\{\epsilon^{2}, \Delta(\pi)^{2},\Delta_{\mathrm{min}}^{2}\}}+\frac{\|\phi_{h}^{\pi}\|_{ \Lambda_{h}(\pi_{\mathrm{exp}})^{-1}}^{2}}{\max\{\epsilon^{2},\Delta(\pi)^{2}, \Delta_{\mathrm{min}}^{2}\}}\right)\] \[\leq 2\inf_{\pi_{\mathrm{exp}}}\max_{\pi\in\Pi}\left(\frac{\|\phi_ {h}^{\star}\|_{\Lambda_{h}(\pi_{\mathrm{exp}})^{-1}}^{2}}{\max\{\epsilon^{2}, \Delta(\pi^{2})^{2},\Delta_{\mathrm{min}}^{2}\}}+\frac{\|\phi_{h}^{\pi}\|_{ \Lambda_{h}(\pi_{\mathrm{exp}})^{-1}}^{2}}{\max\{\epsilon^{2},\Delta(\pi)^{2}, \Delta_{\mathrm{min}}^{2}\}}\right)\] \[\leq 4\inf_{\pi_{\mathrm{exp}}}\max_{\pi\in\Pi}\frac{\|\phi_{h}^{ \pi}\|_{\Lambda_{h}(\pi_{\mathrm{exp}})^{-1}}^{2}}{\max\{\epsilon^{2},\Delta( \pi)^{2},\Delta_{\mathrm{min}}^{2}\}}\]

where we have used that \(\Delta(\pi)\geq\Delta(\pi^{\star})\) for all \(\pi\). Plugging this bound into the expression from (2) from the Lemma statement completes the proof.

Proof of Claim 3.We have that

\[HU(\pi,\pi^{\star})=H\sum_{h=1}^{H}\mathbb{E}_{s_{h}\sim w_{h}^{\star}}[(Q_{h}^{ \pi}(s_{h},\pi_{h}(s))-Q_{h}^{\pi}(s_{h},\pi_{h}^{\star}(s)))^{2}]\leq H\sum_{h= 1}^{H}H^{2}\leq H^{4}\]

Then,

\[\frac{HU(\pi,\pi^{\star})}{\max\{\epsilon^{2},\Delta(\pi)^{2}, \Delta_{\min}^{2}\}} \leq\frac{H^{4}}{\max\{\epsilon^{2},\Delta(\pi)^{2},\Delta_{\min} ^{2}\}}\] \[\leq H^{4}\sum_{h=1}^{H}\inf_{\pi_{\mathrm{exp}}}\max_{\pi\in \Pi}\frac{\|\phi_{h}^{\pi}\|_{\Lambda_{h}(\pi_{\mathrm{exp}})^{-1}}^{2}}{\max \{\epsilon^{2},\Delta(\pi)^{2},\Delta_{\min}^{2}\}}\]

Where the final inequality follows from Claim 1 above. 

The lemma below shows that there are some instances where the complexity from Theorem 1 is strictly smaller in terms of \(\epsilon\) dependence than that from Theorem 1 from [42] for Pedel.

**Lemma 5**.: _On MDP \(\mathcal{M}\) defined above, we have:_

1. \(\sum_{h=1}^{H}\inf_{\pi_{\mathrm{exp}}}\max_{\pi\in\Pi}\frac{\|\phi_{h}^{\star }-\phi_{h}^{\pi 2}\|_{\Lambda_{h}(\pi_{\mathrm{exp}})^{-1}}}{\max\{\epsilon^{2}, \Delta(\pi)^{2},\Delta_{\min}^{2}\}}\leq 15\)__
2. \(\max_{\pi\in\Pi}\frac{HU(\pi,\pi^{\star})}{\max\{\epsilon^{2},\Delta(\pi)^{2},\Delta_{\min}^{2}\}}=\frac{3H}{\epsilon}\)__
3. \(\sum_{h=1}^{H}\inf_{\pi_{\mathrm{exp}}}\max_{\pi\in\Pi}\frac{\|\phi_{h}^{\star }\|_{\Lambda_{h}(\pi_{\mathrm{exp}})^{-1}}^{2}}{\max\{\epsilon^{2},\Delta(\pi) ^{2},\Delta_{\min}^{2}\}}\geq\frac{H}{\epsilon^{2}}\)__

Proof of 1.: In this case we have that \(\pi^{\star}=\pi_{1}\), and the only other \(\pi\) of interest is \(\pi_{2}\). Note that \(\pi_{1}\) and \(\pi_{2}\) differ only at state \(s_{3}\) and \(s_{4}\) at \(h=2\). Let \(\pi_{\mathrm{exp}}\) be the policy that plays actions uniformly at random. Then, we have

\[\sum_{h=1}^{H}\inf_{\pi_{\mathrm{exp}}}\max_{\pi\in\Pi}\frac{\| \phi_{h}^{\star}-\phi_{h}^{\pi}\|_{\Lambda_{h}(\pi_{\mathrm{exp}})^{-1}}^{2}}{ \max\{\epsilon^{2},\Delta(\pi)^{2},\Delta_{\min}^{2}\}} \leq\inf_{\pi_{\mathrm{exp}}}\frac{\|\phi_{2}^{\pi_{1}}-\phi_{2} ^{\pi_{2}}\|_{\Lambda_{h}(\pi_{\mathrm{exp}})^{-1}}^{2}}{\epsilon^{2}}\] \[=\frac{1}{\epsilon^{2}}\left(\frac{w_{2}^{\pi_{1}}(s_{3})^{2}}{w _{2}^{\pi_{\mathrm{exp}}}(s_{3})}+\frac{w_{2}^{\pi_{1}}(s_{4})^{2}}{w_{2}^{ \pi_{\mathrm{exp}}}(s_{4})}\right)\] \[\leq\frac{1}{\epsilon^{2}}\left(\frac{4\epsilon^{2}}{1/3}+\frac{ \epsilon^{2}}{1/3}\right)\] \[=15.\]

Proof of 2.Note that

\[\max_{\pi\in\Pi}\frac{HU(\pi,\pi^{\star})}{\max\{\epsilon^{2},\Delta(\pi)^{2 },\Delta_{\min}^{2}\}}=\frac{HU(\pi_{2},\pi_{1})}{\epsilon^{2}}.\]

Then,

\[U(\pi_{2},\pi_{1})=\sum_{h=1}^{H}\mathbb{E}_{s\sim w_{h}^{\pi_{1} }}[(Q_{h}^{\pi_{1}}(s,\pi_{1,h}(s))-Q_{h}^{\pi_{1}}(s,\pi_{2,h}(s)))^{2}]\] \[=\mathbb{E}_{s\sim w_{2}^{\pi_{1}}}[(Q_{2}^{\pi_{1}}(s,\pi_{1,2}( s))-Q_{2}^{\pi_{1}}(s,\pi_{2,2}(s)))^{2}]\] \[=2\epsilon+\epsilon=3\epsilon.\]

Combining these proves the result.

Proof of 3.By Claim 1 in Lemma 4, the stated result then follows by recognizing that \(\max\{\epsilon^{2},\Delta(\pi)^{2},\Delta_{\min}^{2}\}\leq\epsilon^{2}\).

### Lower bound

**Lemma 6**.: _On MDP \(\mathcal{M}\) defined above, any \((\epsilon,\delta)\)-PAC algorithm must collect_

\[\mathbb{E}^{\mathcal{M}}[\tau]\geq\frac{1}{\epsilon}\cdot\log\frac{1}{2.4\delta}.\]

_samples._

Proof.: Consider \(\Pi,\mathcal{M}\), and \(\mathcal{M}^{\prime}\) defined above. Let \(\mathcal{E}\) denote the event \(\{\widehat{\pi}=\pi_{1}\}\). By the above observations, we have that \(\pi_{1}\) is \(\epsilon\)-optimal on \(\mathcal{M}\) while \(\pi_{2}\) is not, and that \(\pi_{2}\) is \(\epsilon\)-optimal on \(\mathcal{M}^{\prime}\) while \(\pi_{1}\) is not. Then by the definition of an \((\epsilon,\delta)\)-PAC algorithm, \(\mathbb{P}^{\mathcal{M}}[\mathcal{E}]\geq 1-\delta\) and \(\mathbb{P}^{\mathcal{M}^{\prime}}[\mathcal{E}]\leq\delta\).

Let \(\gamma_{h}(s,a)\) denote the distribution of \((r_{h},s_{h+1})\) given \((s,a,h)\) on \(\mathcal{M}\), and \(\gamma^{\prime}_{h}(s,a)\) is the same on \(\mathcal{M}^{\prime}\). Then, letting \(\nu_{h}\leftarrow\gamma_{h},\nu^{\prime}_{h}\leftarrow\gamma^{\prime}_{h}\) and otherwise adopting the same notation as in Lemma F.1 of [46], we have from Lemma F.1 of [46] that:

\[\sum_{s,a,h}\mathbb{E}^{\mathcal{M}}[N^{\tau}_{h}(s,a)]\mathrm{ KL}(\gamma_{h}(s,a),\gamma^{\prime}_{h}(s,a)) \geq\sup_{\mathcal{E}^{\prime}\in\mathcal{F}_{\tau}}d(\mathbb{P}^ {\mathcal{M}}[\mathcal{E}^{\prime}],\mathbb{P}^{\mathcal{M}^{\prime}}[ \mathcal{E}^{\prime}])\] \[\geq d(\mathbb{P}^{\mathcal{M}}[\mathcal{E}],\mathbb{P}^{ \mathcal{M}^{\prime}}[\mathcal{E}])\] \[\geq\log\frac{1}{2.4\delta}\]

where the last inequality follows from [23].

Note that \(\mathcal{M}\) and \(\mathcal{M}^{\prime}\) differ only at \((s_{1},a_{1})\), so

\[\sum_{s,a,h}\mathbb{E}^{\mathcal{M}}[N^{\tau}_{h}(s,a)]\mathrm{ KL}(\gamma_{h}(s,a),\gamma^{\prime}_{h}(s,a))=\mathbb{E}^{\mathcal{M}}[N^{\tau}_{1} (s_{1},a_{1})]\mathrm{KL}(\gamma_{1}(s_{1},a_{1}),\gamma^{\prime}_{1}(s_{1},a _{1})).\]

Furthermore, we see that

\[\mathrm{KL}(\gamma_{1}(s_{1},a_{1}),\gamma^{\prime}_{1}(s_{1},a_{1}))=2 \epsilon\log\frac{2\epsilon}{\epsilon}+\epsilon\log\frac{\epsilon}{2\epsilon}\leq\epsilon.\]

So it follows that we must have

\[\mathbb{E}^{\mathcal{M}}[N^{\tau}_{1}(s_{1},a_{1})]\geq\frac{1}{\epsilon} \cdot\log\frac{1}{2.4\delta}.\]

Noting that \(\mathbb{E}^{\mathcal{M}}[N^{\tau}_{1}(s_{1},a_{1})]\leq\mathbb{E}^{\mathcal{M }}[\tau]\) completes the proof. 

## Appendix C Tabular MDP Upper Bound

### Notation

Covariance matrices.We use

\[\Lambda_{h}(\pi_{\mathrm{exp}})=\mathbb{E}_{\pi_{\mathrm{exp}}}[e_{s_{h}a_{h} }e^{\top}_{s_{h}a_{h}}]\]

to denote the expected covariance matrix and \(\widehat{\Lambda}_{\ell,h}\) to denote the empirical covariance matrix collected from \(\mathfrak{D}_{\ell,h}^{\mathrm{ED}}\).

State visitations.Let \(\delta^{\pi}_{\ell,h}(s^{\prime}):=w^{\pi}_{h}(s^{\prime})-w^{\bar{\pi}_{\ell }}_{h}(s^{\prime})\), for \(\bar{\pi}_{\ell}\) the reference policy, \(\delta^{\pi}_{\ell,h}\) the vectorization of \(\delta^{\pi}_{\ell,h}(s^{\prime})\), and \(w^{\pi}_{h}(s)=\mathbb{P}_{\pi}[s_{h}=s]\) the visitation probability, and \(W^{\star}_{h}(s)=\sup_{\pi}w^{\pi}_{h}(s)\). Then, we can recursively define

\[\delta^{\pi}_{\ell,h+1}=P_{h}(\bm{\pi}_{h}-\bar{\bm{\pi}}_{\ell,h})w^{\bar{\pi} }_{\ell,h}+P_{h}\bm{\pi}_{h}\delta^{\pi}_{\ell,h}.\] (C.1)

Similarly,

\[\widetilde{\delta}^{\pi}_{\ell,h+1}=M_{h}\left(P_{h}(\bm{\pi}_{h}-\bar{\bm{\pi }}_{\ell,h})\widehat{w}^{\bar{\pi}}_{\ell,h}+P_{h}\bm{\pi}_{h}\widetilde{\delta }^{\pi}_{\ell,h}\right).\] (C.2)

And

\[\widehat{\delta}^{\pi}_{\ell,h+1}=M_{h}\left(\widehat{P}_{\ell,h}(\bm{\pi}_{h}- \bar{\bm{\pi}}_{\ell,h})\widehat{w}^{\bar{\pi}}_{\ell,h}+\widehat{P}_{\ell,h} \bm{\pi}_{h}\widehat{\delta}^{\pi}_{\ell,h}\right).\] (C.3)Value functions.Note that we can express the value function as:

\[V_{h}^{\pi}=\sum_{k=h}^{H}\left(\prod_{j=h+1}^{k}P_{j}\bm{\pi}_{j} \right)^{\top}\bm{\pi}_{k}^{\top}r_{k}\]

On the "pruned" MDP, define

\[\widetilde{r}_{\ell,h}=M_{\ell,h}r_{h},\]

and

\[\widetilde{V}_{\ell,h}:=\sum_{k=h}^{H}\left(\prod_{j=h+1}^{k}M_{ \ell,j+1}P_{j}\bm{\pi}_{j}\right)^{\top}\bm{\pi}_{k}^{\top}\widetilde{r}_{\ell,k}.\]

Reward difference term.Define

\[U_{h}(\pi,\pi^{\prime}):=\mathbb{E}_{\pi^{\prime}}[(Q_{h}^{\pi}(s_ {h},\pi_{h}(s))-Q_{h}^{\pi}(s_{h},\pi_{h}^{\prime}(s)))^{2}]\]and \(U(\pi,\pi^{\prime}):=\sum_{h=1}^{H}U_{h}(\pi,\pi^{\prime})\). Additionally, define

\[\widehat{U}_{\ell,h}(\pi,\pi^{\prime}):=\mathbb{E}_{\pi^{\prime},\ell}[(\widehat{ Q}_{\ell,h}^{\pi}(s_{h},\pi_{h}(s))-\widehat{Q}_{\ell,h}^{\pi}(s_{h},\pi^{\prime}_{ h}(s)))^{2}]\]

where \(\mathbb{E}_{\pi^{\prime},\ell}\) denotes the expectation induced playing \(\pi^{\prime}\) on the MDP with transitions \(\widehat{P}_{\ell}\), and \(\widehat{Q}_{\ell,h}^{\pi}\) denotes the \(Q\)-function for policy \(\pi\) on this same MDP. Let \(\widehat{U}_{\ell}(\pi,\pi^{\prime}):=\sum_{h=1}^{H}\widehat{U}_{\ell,h}(\pi, \pi^{\prime})\).

### Technical Results

**Lemma 7**.: _Let \(\mathfrak{D}=\{(s_{1},a_{1},s_{1}^{\prime}),\ldots(s_{T},a_{T},s_{T}^{\prime})\}\) be any dataset of transitions collected from level \(h\). Let \(\widehat{P}\in\mathbb{R}^{S\times SA}\) denote the empirical transition matrix with \([\widehat{P}]_{s^{\prime},sa}=\frac{N(s^{\prime}|s,a)}{N(s,a)}\) if \(N(s,a)>0\), and 0 otherwise, for \(N(s^{\prime}\mid s,a)=\sum_{t}\mathbb{I}\{(s_{t},a_{t},s_{t}^{\prime})=(s,a,s^ {\prime})\}\) and \(N(s,a)=\sum_{t}\mathbb{I}\{(s_{t},a_{t})=(s,a)\}\). Consider any \(v\in[0,1]^{S}\) and \(u\in\mathbb{R}^{SA}\) and assume that \(N(s,a)>\underline{\lambda}>0\) for all \(\langle s,a\rangle\in\operatorname{support}(u)\). Then, for \(P\) the true transition matrix, we have that with probability at least \(1-\delta\):_

\[\left|v^{\top}(P-\widehat{P})u\right|\leq\sqrt{\sum_{s,a}\frac{[u] _{s,a}^{2}}{N(s,a)}}\cdot\left(\sqrt{2\log\left(\frac{1}{\delta}\right)}+ \frac{4}{3\sqrt{\underline{\lambda}}}\log\left(\frac{1}{\delta}\right)\right).\]

Proof.: First write

\[v^{\top}(P-\widehat{P})u =\sum_{s^{\prime}}\sum_{s,a}v_{s^{\prime}}\left(P(s^{\prime}\mid s,a)-\frac{N(s^{\prime}\mid s,a)}{N(s,a)}\right)u_{sa}\] \[=\sum_{t}\sum_{s^{\prime}}\frac{v_{s^{\prime}}\left(P(s^{\prime} \mid s_{t},a_{t})-\mathbb{I}\{s_{t}^{\prime}=s^{\prime}\}\right)u_{s_{t}a_{t}}} {N(s_{t},a_{t})}\]

where the second equality follows from some simple manipulations. Note that, for any \(t\), we have

\[\mathbb{E}\left[\frac{v_{s^{\prime}}\left(P(s^{\prime}\mid s_{t},a_{t})- \mathbb{I}\{s_{t}^{\prime}=s^{\prime}\}\right)u_{s_{t}a_{t}}}{N(s_{t},a_{t})} \mid s_{t},a_{t}\right]=0\]

and can bound

\[\left|\sum_{s^{\prime}}\frac{v_{s^{\prime}}\left(P(s^{\prime}\mid s _{t},a_{t})-\mathbb{I}\{s_{t}^{\prime}=s^{\prime}\}\right)u_{s_{t}a_{t}}}{N(s _{t},a_{t})}\right| \leq\frac{2u_{s_{t}a_{t}}}{N(s_{t},a_{t})}\leq\frac{2}{\sqrt{ \underline{\lambda}}}\cdot\frac{u_{s_{t}a_{t}}}{\sqrt{N(s_{t},a_{t})}}\] \[\leq\frac{2}{\sqrt{\underline{\lambda}}}\cdot\sqrt{\sum_{s,a} \frac{u_{sa}^{2}}{N(s,a)}}\]

where we have used the fact that \(N(s,a)\geq\underline{\lambda}\) for \((s,a)\in\operatorname{support}(u)\), and since \(v\) has entries in \([0,1]\) and \(P(s^{\prime}\mid s_{t},a_{t})\) and \(\mathbb{I}\{s_{t}^{\prime}=s^{\prime}\}\) are valid distributions, so \(\sum_{s^{\prime}}v_{s^{\prime}}(P(s^{\prime}\mid s_{t},a_{t})-\mathbb{I}\{s_ {t}^{\prime}=s^{\prime}\})\in[-1,1]\). Furthermore, we have that

\[\mathbb{E}_{s_{t}^{\prime}}\left[\left(\sum_{s^{\prime}}\frac{v_{s^{\prime}} \left(P(s^{\prime}\mid s_{t},a_{t})-\mathbb{I}\{s_{t}^{\prime}=s^{\prime}\} \right)u_{s_{t}a_{t}}}{N(s_{t},a_{t})}\right)^{2}\right]\leq\mathbb{E}_{s_{t}^{ \prime}}\left[\left(\frac{u_{s_{t}a_{t}}}{N(s_{t},a_{t})}\right)^{2}\right]= \left(\frac{u_{s_{t}a_{t}}}{N(s_{t},a_{t})}\right)^{2}\]

where we have again used that \(\sum_{s^{\prime}}v_{s^{\prime}}(P(s^{\prime}\mid s_{t},a_{t})-\mathbb{I}\{s_ {t}^{\prime}=s^{\prime}\})\in[-1,1]\).

By Bernstein's inequality, we therefore have that with probability at least \(1-\delta\):

\[\left|v^{\top}(P-\widehat{P})u\right| \leq\sqrt{2\sum_{t}\left(\frac{u_{s_{t}a_{t}}}{N(s_{t},a_{t})} \right)^{2}\cdot\log\frac{2}{\delta}}+\frac{4}{3\sqrt{\underline{\lambda}}} \cdot\sqrt{\sum_{t}\frac{u_{s_{t}a_{t}}^{2}}{N(s_{t},a_{t})}}\cdot\log\frac{2}{ \delta}\] \[=\left(\sqrt{2\log\frac{2}{\delta}}+\frac{4}{3\sqrt{\underline{ \lambda}}}\log\frac{2}{\delta}\right)\cdot\sqrt{\sum_{s,a}\frac{u_{sa}^{2}}{N(s,a)}}.\]

**Lemma 8**.: _Let \(\mathfrak{D}=\{(s_{1},a_{1},r_{1}),\ldots(s_{T},a_{T},r_{T})\}\) be any dataset of state-action-reward tuples collected from level \(h\). Let \(\widehat{r}\in\mathbb{R}^{SA}\) denote the empirical reward estimation with \([\widehat{r}]_{sa}=\frac{1}{N(s,a)}\cdot\sum_{t=1}^{T}r_{t}\cdot\mathbb{I}\{(s _{t},a_{t})=(s,a)\}\) if \(N(s,a)>0\), and 0 otherwise, for \(N(s,a)=\sum_{t}\mathbb{I}\{(s_{t},a_{t})=(s,a)\}\). Consider any \(u\in\mathbb{R}^{SA}\) and assume that \(N(s,a)>\underline{\lambda}>0\) for all \((s,a)\in\operatorname{support}(u)\). Then, for \(r\) the true reward mean, we have that with probability at least \(1-\delta\):_

\[\big{|}(r-\widehat{r})^{\top}u\big{|}\leq\sqrt{\sum_{s,a}\frac{[u]_{s,a}^{2}} {N(s,a)}}\cdot\left(\sqrt{2\log\left(\frac{1}{\delta}\right)}+\frac{4}{3 \sqrt{\underline{\lambda}}}\log\left(\frac{1}{\delta}\right)\right).\]

Proof.: First write

\[(r-\widehat{r})^{\top}u=\sum_{t}\frac{\left(r(s_{t},a_{t})-r_{t} \right)u_{s_{t}a_{t}}}{N(s_{t},a_{t})}.\]

Note that, for any \(t\), we have

\[\mathbb{E}\left[\frac{\left(r(s_{t},a_{t})-r_{t}\right)u_{s_{t}a_{t}}}{N(s_{t },a_{t})}\mid s_{t},a_{t}\right]=0\]

and can bound

\[\left|\frac{\left(r(s_{t},a_{t})-r_{t}\right)u_{s_{t}a_{t}}}{N(s_{t},a_{t})} \right|\leq\frac{u_{s_{t}a_{t}}}{N(s_{t},a_{t})}\leq\frac{1}{\sqrt{\underline{ \lambda}}}\cdot\frac{u_{s_{t}a_{t}}}{\sqrt{N(s_{t},a_{t})}}\leq\frac{1}{\sqrt {\underline{\lambda}}}\cdot\sqrt{\sum_{s,a}\frac{u_{sa}^{2}}{N(s,a)}}\]

where we have used the fact that \(N(s,a)\geq\underline{\lambda}\) for \((s,a)\in\operatorname{support}(u)\), and since we assume our rewards are in \([0,1]\). Furthermore, we have that

\[\mathbb{E}_{r_{t}}\left[\left(\frac{\left(r(s_{t},a_{t})-r_{t} \right)u_{s_{t}a_{t}}}{N(s_{t},a_{t})}\right)^{2}\right]\leq\mathbb{E}_{r_{t} }\left[\left(\frac{u_{s_{t}a_{t}}}{N(s_{t},a_{t})}\right)^{2}\right]=\left( \frac{u_{s_{t}a_{t}}}{N(s_{t},a_{t})}\right)^{2}.\]

By Bernstein's inequality, we therefore have that with probability at least \(1-\delta\):

\[\big{|}(r-\widehat{r})^{\top}u\big{|} \leq\sqrt{2\sum_{t}\left(\frac{u_{s_{t}a_{t}}}{N(s_{t},a_{t})} \right)^{2}\cdot\log\frac{2}{\delta}}+\frac{4}{3\sqrt{\underline{\lambda}}} \cdot\sqrt{\sum_{t}\frac{u_{s_{t}a_{t}}^{2}}{N(s_{t},a_{t})}}\cdot\log\frac{ 2}{\delta}\] \[=\left(\sqrt{2\log\frac{2}{\delta}}+\frac{4}{3\sqrt{\underline{ \lambda}}}\log\frac{2}{\delta}\right)\cdot\sqrt{\sum_{s,a}\frac{u_{sa}^{2}}{N (s,a)}}.\]

**Lemma 9**.: _Let \(u\in\mathbb{R}^{S}\) be any vector such that \(\forall s,|u_{s}|\leq M\). Then, for any \((\ell,h)\), the following holds with probability \((1-\delta)\):_

\[\Big{|}\mathbb{E}_{s\sim w_{\ell,h}^{\overline{\eta}}}[u_{s}]- \mathbb{E}_{s\sim\tilde{\omega}_{\ell,h}^{\overline{\eta}}}[u_{s}]\Big{|}\leq \sqrt{\frac{2\mathbb{E}_{s\sim w_{\ell,h}^{\overline{\eta}}}[u_{s}^{2}]}{\bar {n}_{\ell}}\log\left(\frac{2}{\delta}\right)}+\frac{2M}{3\bar{n}_{\ell}}\log \left(\frac{2}{\delta}\right)\]

Proof.: The left side of the inequality above takes the form of the deviation between an empirical and true mean of the random variable \(u_{s}\). Hence, the result follows directly from Bernstein's inequality since we know \(|u_{s}|\leq M\) is bounded. 

**Lemma 10**.: _Assume that \(A\) and \(B\) are matrices with entries in \([0,1]\) and whose rows sum to a value \(\leq 1\). Then \(AB\) also satisfies this._

Proof.: To see this, consider the \(i\)th row of \(AB\), and note that the sum of the elements in this row can be written as, for \(a_{i}^{\top}\) the \(i\)th row of \(A\), and \(b_{j}\) the \(j\)th column of \(B\):

\[\sum_{j}a_{i}^{\top}b_{j}=\sum_{k}\sum_{j}a_{ik}b_{jk}=\sum_{k}a_{ik}(\sum_{j}b_ {jk}).\]

Now note that \(\sum_{j}b_{jk}\) is the sum across the \(k\)th row of \(B\), so this is \(\leq 1\) by assumption. Furthermore, \(\sum_{k}a_{ik}\leq 1\) for the same reason. Thus, the \(i\)th row of \(AB\) sums to a value \(\leq 1\). Furthermore, it is easy to see \(a_{i}^{\top}b_{j}\leq 1\) for each \(j\). Thus, \(AB\) has values in \([0,1]\) and rows that sum to a value \(\leq 1\)

**Lemma 11**.: _We have that \(\|\Pi_{h=i}^{j}M_{h+1}P_{h}\bm{\pi}_{h}\|_{2},\|\Pi_{h=i}^{j}P_{h}\bm{\pi}_{h}\|_{ 2}\leq\sqrt{S}\) for any \(i,j,h\)._

Proof.: By definition \(P_{h}\bm{\pi}_{h}\) is a transition matrix--each row has values in \([0,1]\) and sums to 1--and \(M_{h+1}\) is diagonal with diagonal elements either 0 or 1. Thus, each matrix \(M_{h}P_{h}\bm{\pi}_{h}\) has values in \([0,1]\) and rows that sum to a value \(\leq 1\), so Lemma 10 implies that \(\Pi_{h=i}^{j}M_{h+1}P_{h}\bm{\pi}_{h}\) does as well. Denote \(A:=\|\Pi_{h=i}^{j}M_{h}P_{h}\bm{\pi}_{h}\|_{2}\). We can then bound

\[\|\Pi_{h=i}^{j}M_{h+1}P_{h}\bm{\pi}_{h}\|_{2}^{2}=\|A\|_{2}^{2}\leq\|A\|_{ \mathrm{F}}^{2}=\sum_{i}\sum_{j}A_{ij}^{2}\leq\sum_{i}1\leq S,\]

which proves the result. The bound on \(\|\Pi_{h=i}^{j}P_{h}\bm{\pi}_{h}\|_{2}\) follows from the same argument. 

**Lemma 12**.: _We have_

\[\widetilde{\delta}_{\ell,h+1}^{\pi}-\widehat{\delta}_{\ell,h+1}^{ \widetilde{\tau}}\] \[=\sum_{i=0}^{h-2}\left(\prod_{j=h-i+1}^{h}M_{\ell,j+1}P_{j}\bm{ \pi}_{j}\right)(P_{h-i}-\widehat{P}_{\ell,h-i})M_{\ell,h-i}\Big{[}(\bm{\pi}_{h -i}-\bar{\bm{\pi}}_{\ell,h-i})\widehat{w}_{\ell,h-i}^{\widetilde{\tau}}+\bm{ \pi}_{h-i}\widehat{\delta}_{\ell,h-i}^{\widetilde{\tau}}\Big{]}.\]

Proof.: This follows immediately from the definition of \(\widetilde{\delta}_{\ell,h+1}^{\pi}\), \(\widehat{\delta}_{\ell,h+1}^{\widetilde{\tau}}\), and simple manipulations. 

### Concentration Arguments and Good Events

**Lemma 13**.: _Let \(\mathcal{E}_{\mathrm{prune}}^{\ell}\) be the event for which the call to Prune in epoch \(\ell\) in Algorithm 2 will terminate after running for at most_

\[\mathrm{poly}(S,A,H,\log\frac{SAH\ell}{\delta\epsilon_{\ell}})\cdot\frac{1}{ \epsilon_{\mathrm{unif}}^{\ell}}\]

_episodes and will return a set \(\mathcal{S}_{\ell}^{\mathrm{keep}}\) such that, for every \((s,h)\in\mathcal{S}_{\ell}^{\mathrm{keep}}\), we have \(W_{h}^{\star}(s)\geq\epsilon_{\mathrm{unif}}^{\ell}\), and, if \((s,h)\not\in\mathcal{S}_{\ell}^{\mathrm{keep}}\), then \(W_{h}^{\star}(s)\leq 32\epsilon_{\mathrm{unif}}^{\ell}\). Then \(\mathbb{P}(\mathcal{E}_{\mathrm{prune}}^{\ell})\geq 1-\frac{\delta}{3\ell^{2}}\)._

Proof.: From Lemma 38, this event follows directly with probability \((1-\frac{\delta}{3\ell^{2}})\). 

**Lemma 14**.: _Let \(\mathcal{E}_{\mathrm{exp}}^{\ell,h}\) be the event for which:_

1. _The exploration procedure in Algorithm_ 3 _will produce_ \(\mathfrak{D}_{\ell,h}^{\mathrm{ED}}\) _such that_ \[\max_{\pi\in\Pi_{\ell}}\|M_{\ell,h}((\bm{\pi}_{h}-\bar{\bm{\pi}}_{\ell,h}) \widehat{w}_{\ell,h}^{\widetilde{\tau}}+\bm{\pi}_{h}\widehat{\delta}_{\ell,h}^ {\pi})\|_{\widehat{\Lambda}_{\ell,h}^{-1}}^{2}\leq\epsilon_{\mathrm{exp}}^{ \ell}\quad\text{for}\quad\widehat{\Lambda}_{\ell,h}=\sum_{(s,a)\in\mathfrak{D }_{\ell,h}^{\mathrm{ED}}}e_{sa}e_{sa}^{\top},\] (C.4) _and will collect at most_ \[C\cdot\frac{\inf_{\pi_{\mathrm{exp}}}\max_{\pi\in\Pi_{\ell}}\|M _{\ell,h}((\bm{\pi}_{h}-\bar{\bm{\pi}}_{\ell,h})\widehat{w}_{\ell,h}^{ \widetilde{\sigma}}+\bm{\pi}_{h}\widehat{\delta}_{\ell,h}^{\pi})\|_{\Lambda_{h }(\pi_{\mathrm{exp}})^{-1}}^{2}}{\epsilon_{\mathrm{exp}}^{\ell}}+\frac{C_{ \mathrm{fw}}^{\ell}}{(\epsilon_{\mathrm{exp}}^{\ell})^{4/5}}\] \[+\frac{C_{\mathrm{fw}}^{\ell}}{\epsilon_{\mathrm{unif}}^{\ell}} +\log(C_{\mathrm{fw}}^{\ell})\cdot K_{\mathrm{unif}}^{\ell}\] _episodes._
2. _For each_ \(s\in\mathcal{S}_{\ell}^{\mathrm{keep}}\)_, we have that_ \(\sum_{(s^{\prime},a^{\prime})\in\mathfrak{D}_{\ell,h}^{\mathrm{ED}}}\mathbb{I} \{(s^{\prime},a^{\prime})=(s,a)\}\geq\frac{K_{\mathrm{unif}}^{\ell}\epsilon_{ \mathrm{unif}}^{\ell}}{SA}\) _for any_ \(a\in\mathcal{A}\)_._

_Above,_ \(C\) _is a universal constant and_ \(C_{\mathrm{fw}}^{\ell}=\mathrm{poly}(S,A,H,\log\ell/\delta,\log 1/\epsilon,\log|\Pi|)\)_. Then_ \(\mathbb{P}[(\mathcal{E}_{\mathrm{exp}}^{\ell,h})^{c}\cap\mathcal{E}_{ \mathrm{prune}}^{\ell}\cap\mathcal{E}_{\mathrm{est}}^{\ell}\cap(\cap_{h^{ \prime}\leq h-1}\mathcal{E}_{\mathrm{est}}^{\ell,h^{\prime}})\cap(\cap_{(h^{ \prime}\leq h-1}\mathcal{E}_{\mathrm{exp}}^{\ell,h^{\prime}})]\leq\frac{ \delta}{6H\ell^{2}}\)_._Proof.: Since the event \(\mathcal{E}^{\ell}_{\mathrm{prune}}\) holds, for each \(s\in\mathcal{S}^{\mathrm{keep}}_{\ell}\) we have \(W^{\star}_{h}(s)\geq\epsilon^{\ell}_{\mathrm{unif}}\). Now, observe that, for \(s\in\mathcal{S}^{\mathrm{keep}}_{\ell}\) and any \(a\):

\[|[(\bm{\pi}_{h}-\bar{\bm{\pi}}_{\ell,h^{\prime}})\hat{\alpha}^{ \#}_{\ell,h}+\bm{\pi}_{h}\widehat{\delta}^{\pi}_{\ell,h}]_{(s,a)}|\] \[\leq[\widehat{w}^{\#}_{\ell,h}]_{s}+[|\widehat{\delta}^{\pi}_{ \ell,h}]_{s}|\leq[w^{\#}_{\ell,h}]_{s}+|[|\widehat{\delta}^{\pi}_{\ell,h}]_{s} |+|[\widehat{w}^{\#}_{\ell,h}-w^{\#}_{\ell,h}]_{(s)}|+|[\delta^{\pi}_{\ell,h}]_ {s}-|[\widehat{\delta}^{\pi}_{\ell,h}]_{s}||.\]

By construction, we have \([w^{\#}_{\ell,h}]_{s},|[\delta^{\pi}_{\ell,h}]_{s}|\leq W^{\star}_{h}(s)\). By Lemma 19, on \(\mathcal{E}^{\ell}_{\mathrm{est}}\), we can bound \(|[\widehat{\delta}^{\pi}_{\ell,h}-w^{\#}_{\ell,h}]_{(s)}|\leq\sqrt{8S\epsilon ^{5/3}_{\ell}}\). By Lemma 18, on \(\mathcal{E}^{\ell}_{\mathrm{prune}}\cap(\cap_{h^{\prime}\leq h-1}\mathcal{E} ^{\ell,h^{\prime}}_{\mathrm{est}})\cap(\cap_{h^{\prime}\leq h-1}\mathcal{E}^{ \ell,h^{\prime}}_{\mathrm{exp}})\), we can bound

\[|[\delta^{\pi}_{\ell,h}]_{s}-|[\widehat{\delta}^{\pi}_{\ell,h}]_{s}|\leq\sqrt{ SH\beta_{\ell}\epsilon^{\ell}_{\mathrm{exp}}}+SH(\sqrt{8\epsilon^{5/3}_{\ell}}+32 \epsilon^{\ell}_{\mathrm{unif}}).\]

Altogether then, we have

\[|[(\bm{\pi}_{h}-\bar{\bm{\pi}}_{\ell,h^{\prime}})\widehat{w}^{ \#}_{\ell,h}+\bm{\pi}_{h}\widehat{\delta}^{\pi}_{\ell,h}]_{(s,a)}|\] \[\leq 2W^{\star}_{h}(s)+\sqrt{SH\beta_{\ell}\epsilon^{\ell}_{ \mathrm{exp}}}+SH(\sqrt{8\epsilon^{5/3}_{\ell}}+32\epsilon^{\ell}_{\mathrm{ unif}})+\sqrt{8S\epsilon^{5/3}_{\ell}}.\]

By our choice of \(\epsilon^{\ell}_{\mathrm{exp}}\) and \(\epsilon^{\ell}_{\mathrm{unif}}\), we can bound all of this as

\[\leq C_{\bm{\phi}}\cdot(W^{\star}_{h}(s)+\sqrt{K^{\ell}_{\mathrm{unif}} \epsilon^{\ell}_{\mathrm{unif}}}\epsilon^{\ell}_{\mathrm{exp}})\]

for \(C_{\bm{\phi}}=cSH\beta_{\ell}\). This is the condition required by Theorem 2, so the result follows from Theorem 2. 

**Lemma 15**.: _Let \(\mathcal{E}^{\ell,h}_{\mathrm{est}}\) be the event at epoch \(\ell\) for step \(h\) on which:_

1. _For all_ \(\pi\in\Pi_{\ell},\ h^{\prime}\leq h\)_:_ \[\left|\left\langle\bm{\pi}^{\top}_{h}\widetilde{\tau}_{\ell,h}, \left(\prod_{i=h^{\prime}+1}^{h}M_{\ell,i+1}P_{i}\bm{\pi}_{i}\right)(P_{h^{ \prime}}-\widehat{P}_{\ell,h^{\prime}})M_{\ell,h^{\prime}}\Big{[}(\bm{\pi}_{h ^{\prime}}-\bar{\bm{\pi}}_{\ell,h^{\prime}})\widehat{w}^{\#}_{\ell,h^{\prime }}+\bm{\pi}_{h^{\prime}}\widehat{\delta}^{\pi}_{\ell,h^{\prime}}\Big{]}\right\rangle\right|\] \[\leq\beta_{\ell}\sqrt{\sum_{s,a}\frac{\left[M_{\ell,h^{\prime}} \left((\bm{\pi}_{h^{\prime}}-\bar{\bm{\pi}}_{\ell,h^{\prime}})\widehat{w}^{ \#}_{\ell,h^{\prime}}+\bm{\pi}_{h^{\prime}}\widehat{\delta}^{\pi}_{\ell,h^{ \prime}}\right)\right]_{(s,a)}^{2}}{N_{\ell,h^{\prime}}(s,a)}}.\]
2. _For all canonical vectors_ \(e_{s^{\prime}}\) _in_ \(\mathbb{R}^{S},\pi\in\Pi_{\ell}\)_, and_ \(h^{\prime}\leq h\)_,_ \[\left|\left\langle e_{s^{\prime}},\left(\prod_{i=h^{\prime}+1}^{ h}M_{\ell,i+1}P_{i}\bm{\pi}_{i}\right)(P_{h^{\prime}}-\widehat{P}_{\ell,h^{ \prime}})M_{\ell,h^{\prime}}\Big{[}(\bm{\pi}_{h^{\prime}}-\bar{\bm{\pi}}_{\ell,h^{\prime}})\widehat{w}^{\#}_{\ell,h^{\prime}}+\bm{\pi}_{h^{\prime}}\widehat{ \delta}^{\pi}_{\ell,h^{\prime}}\Big{]}\right\rangle\right|\] \[\leq\beta_{\ell}\sqrt{\sum_{s,a}\frac{[M_{\ell,h^{\prime}}((\bm{ \pi}_{h^{\prime}}-\bar{\bm{\pi}}_{\ell,h^{\prime}})\widehat{w}^{\#}_{\ell,h^{ \prime}}+\bm{\pi}_{h^{\prime}}\widehat{\delta}^{\pi}_{\ell,h^{\prime}})]_{s,a}^ {2}}{N_{\ell,h^{\prime}}(s,a)}}.\]
3. _For each_ \((s,a)\)_, we have_ \[\sum_{s^{\prime}}|\widehat{P}_{\ell,h}(s^{\prime}\mid s,a)-P_{h}(s^{\prime} \mid s,a)|\leq S\sqrt{\frac{\log\frac{48S^{2}AH\ell^{2}}{\delta}}{N_{\ell,h}(s,a)}}.\]
4. _For each_ \(\pi\in\Pi_{\ell}\)_,_ \[|\langle\widehat{r}_{\ell,h}-\widetilde{r}_{\ell,h},\bm{\pi}_{h} \widehat{\delta}^{\pi}_{\ell,h}+(\bm{\pi}_{h}-\bar{\bm{\pi}}_{\ell,h})\widehat{w} ^{\#}_{\ell,h}\rangle|\] \[\leq\beta_{\ell}\sqrt{\sum_{s,a}\frac{\left[M_{\ell,h}\left((\bm{ \pi}_{h}-\bar{\bm{\pi}}_{\ell,h^{\prime}})\widehat{w}^{\#}_{\ell,h}+\bm{\pi}_{h} \widehat{\delta}^{\pi}_{\ell,h}\right)\right]_{(s,a)}^{2}}{N_{\ell,h}(s,a)}}.\]

_Then \(\mathbb{P}[(\mathcal{E}^{\ell,h}_{\mathrm{est}})^{c}\cap\mathcal{E}^{\ell}_{ \mathrm{prune}}\cap(\cap_{h^{\prime}\leq h}\mathcal{E}^{\ell,h}_{\mathrm{exp}})] \leq\frac{\delta}{6H\ell^{2}}\)._

Proof.: We prove each of the events sequentially.

Proof of Event (1).Consider any fixed choice of \((\pi,h^{\prime})\). By Lemma 10 and since our rewards are in \([0,1]\), we have that \(\left(\prod_{i=h^{\prime}+1}^{h}M_{\ell,i+1}P_{i}\bm{\pi}_{i}\right)^{\top}\bm{ \pi}_{h}^{\top}\widetilde{r}_{\ell,h}\) is a vector in \([0,1]\). Let \(v\leftarrow\left(\prod_{i=h^{\prime}+1}^{h}M_{\ell,i+1}P_{i}\bm{\pi}_{i} \right)^{\top}\bm{\pi}_{h}^{\top}\widetilde{r}_{\ell,h}\) and \(u\gets M_{\ell,h^{\prime}}\Big{[}(\bm{\pi}_{h^{\prime}}-\bar{\bm{\pi}}_{ \ell,h^{\prime}})\widehat{w}_{\ell,h^{\prime}}^{\bar{\pi}}+\bm{\pi}_{h^{\prime }}\widehat{\delta}_{\ell,h^{\prime}}^{\bar{\pi}}\Big{]}\). Note that by construction we have that \(u_{sa}=0\) for \(s\not\in\mathcal{E}_{\ell,h^{\prime}}^{\mathrm{keep}}\), and so on \(\mathcal{E}_{\mathrm{exp}}^{\ell,h^{\prime}}\), we have \(N_{\ell,h^{\prime}}(s,a)\geq\frac{K_{\ell}^{s}\epsilon_{\mathrm{unit}}^{\ell} }{2S\bm{A}}\) for all \((s,a)\in\mathrm{support}(u)\). On \(\mathcal{E}_{\mathrm{prune}}^{\ell}\cap\mathcal{E}_{\mathrm{exp}}^{\ell,h^{ \prime}}\), we can then apply Lemma 7 with \(u\) and \(v\) as defined above to get that the bound fails with probability at most \(\frac{\delta}{30H^{2}\ell^{2}|\Pi_{\ell}|}\). Union bounding over \(h^{\prime}\) and \(\pi\) we get that the stated result fails with probability at most \(\frac{\delta}{30H^{2}}\).

Proof of Event (2).Choose

\[v=e_{i}^{\top}\left(\prod_{i=h^{\prime}+1}^{h}M_{\ell,i}P_{i}\bm{\pi}_{i} \right)\quad\text{and}\quad u=M_{h^{\prime},\ell}\left((\bm{\pi}_{h^{\prime}}- \bar{\bm{\pi}}_{\ell,h^{\prime}})w_{\ell,h^{\prime}}^{\bar{\pi}}+\bm{\pi}_{h^{ \prime}}\widehat{\delta}_{\ell,h^{\prime}}^{\bar{\pi}}\right).\]

Note that by construction of \(w_{\ell,h^{\prime}}^{\bar{\pi}}\) and \(\widehat{\delta}_{\ell,h^{\prime}}^{\bar{\pi}}\) we have that \(u_{sa}=0\) for \(s\not\in\mathcal{E}_{\ell,h^{\prime}}^{\mathrm{keep}}\), and so on \(\mathcal{E}_{\mathrm{exp}}^{\ell,h^{\prime}}\), we have \(N_{\ell,h^{\prime}}(s,a)\geq\frac{K_{\mathrm{unit}}^{\ell}\epsilon_{\mathrm{unit }}^{\ell}}{2S\bm{A}}\) for all \((s,a)\in\mathrm{support}(u)\). Furthermore, we have that \(v\in[0,1]^{S}\) by Lemma 10. Then, the event follows by invoking Lemma 7.

Proof of Event (3).By Hoeffding's inequality, for any \((s,a)\), we have, with probability at least \(1-\frac{\delta}{24S^{2}AH\ell^{2}}\):

\[|\widehat{P}_{\ell,h}(s^{\prime}\mid s,a)-P_{h}(s^{\prime}\mid s,a)|\leq\sqrt {\frac{\log\frac{24S^{2}AH\ell^{2}}{\delta}}{N_{\ell,h}(s,a)}}.\]

Thus, we have that with probability at least \(1-\frac{\delta}{24SAH\ell^{2}}\):

\[\sum_{s^{\prime}}|\widehat{P}_{\ell,h}(s^{\prime}\mid s,a)-P_{h}(s^{\prime} \mid s,a)|\leq S\sqrt{\frac{\log\frac{24S^{2}AH\ell^{2}}{\delta}}{N_{\ell,h}( s,a)}}.\]

Union bounding over all \((s,a)\), we obtain that this holds with probability at least \(1-\frac{\delta}{24H\ell^{2}}\).

Proof of Event (4).Note first that \(\langle\widehat{r}_{\ell,h}-\widetilde{r}_{\ell,h},\bm{\pi}_{h}\widehat{\delta }_{\ell,h}^{\bar{\pi}}+(\bm{\pi}_{h}-\bar{\bm{\pi}}_{\ell,h})\widehat{w}_{ \ell,h}^{\bar{\pi}}\rangle=\langle\widehat{r}_{\ell,h}-\widetilde{r}_{\ell,h },M_{\ell,h}(\bm{\pi}_{h}\widehat{\delta}_{\ell,h}^{\bar{\pi}}+(\bm{\pi}_{h}- \bar{\bm{\pi}}_{\ell,h})\widehat{w}_{\ell,h}^{\bar{\pi}})\rangle\). The result then follows on \(\mathcal{E}_{\mathrm{prune}}^{\ell}\) by a direct application of Lemma 8.

The final result then holds by a union bound. 

**Lemma 16**.: _Let \(\widetilde{\mathcal{E}}_{\mathrm{est}}^{\ell}\) denote the event that at epoch \(\ell\) and for each \(h\):_

1. _For all_ \(\pi\in\Pi_{\ell}\) _and_ \(h\in[H]\)_, we have_ \[\left|\langle P_{h}^{\top}M_{\ell,h+1}\widetilde{V}_{\ell,h+1}+r_ {h},(\bm{\pi}_{h}-\bar{\bm{\pi}}_{\ell,h})(w_{\ell,h}^{\bar{\pi}}-\widehat{w}_{ \ell,h}^{\bar{\pi}})\rangle\right|\leq\frac{2H}{3\bar{n}_{\ell}}\log\frac{60H \ell^{2}|\Pi_{\ell}|}{\delta}\] \[+\sqrt{\frac{2\mathbb{E}_{s\sim w_{\ell,h}^{\pi}}[\langle P_{h}^{ \top}M_{\ell,h+1}\widetilde{V}_{\ell,h+1}^{\pi}+r_{h},(\bm{\pi}_{h}-\bar{\bm{ \pi}}_{\ell,h})e_{s}\rangle^{2}]}{\bar{n}_{\ell}}}\cdot\log\frac{60H\ell^{2}| \Pi_{\ell}|}{\delta}.\]
2. _For all canonical vectors_ \(e_{s}\in\mathbb{R}^{S}\)_,_ \[|\langle e_{s},\widehat{w}_{\ell,h}^{\bar{\pi}}-w_{\ell,h}^{\bar{\pi}}\rangle| \leq\sqrt{\frac{2\log\left(\frac{30H\ell^{2}S}{\delta}\right)}{\bar{n}_{\ell}} }+\frac{2\log\left(\frac{30H\ell^{2}S}{\delta}\right)}{\bar{n}_{\ell}}.\]

_Then \(\mathbb{P}[(\widetilde{\mathcal{E}}_{\mathrm{est}}^{\ell})^{c}]\leq\frac{\delta}{15 \ell^{2}}\)._Proof of Event (1).: Consider a fixed choice of \(\pi\), and let \(u_{s}^{\pi}=\left\langle P_{h}^{\top}\widetilde{V}_{\ell,h+1}^{\pi}+r_{h},(\bm{ \pi}_{h}-\bm{\bar{\pi}}_{\ell,h})e_{s}\right\rangle\), and note that \(|u_{s}^{\pi}|\leq H\) for all \(s\). Lemma 9 then gives that with probability at least \(1-\frac{\delta}{30H\ell^{2}|\Pi_{\ell}|}\) we have

\[\left|\langle P_{h}^{\top}M_{\ell,h+1}\widetilde{V}_{\ell,h+1}+r_ {h},(\bm{\pi}_{h}-\bm{\bar{\pi}}_{\ell,h})(w_{\ell,h}^{\bar{\pi}}-\widehat{w}_{ \ell,h}^{\bar{\pi}})\rangle\right|\] \[\leq\sqrt{\frac{2\mathbb{E}_{s\sim w_{\ell,h}^{\bar{\pi}}}\left( \langle P_{h}^{\top}M_{\ell,h+1}\widetilde{V}_{\ell,h+1}^{\pi}+r_{h},(\bm{\pi }_{h}-\bm{\bar{\pi}}_{\ell,h})e_{s}\rangle^{2}\right)}{\bar{n}_{\ell}}\cdot \log\frac{60H\ell^{2}|\Pi_{\ell}|}{\delta}}+\frac{2H}{3\bar{n}_{\ell}}\log \frac{60H\ell^{2}|\Pi_{\ell}|}{\delta}.\]

Proof of Event (2).: For a fixed choice of \(s\in[S]\), the event follows from Lemma 9 with \(u=e_{s}\) with probability \(1-\delta\), where \(\delta=\frac{\delta}{30H\ell^{2}S}\). Once we take the union bound over all \(s\in[S]\), then the event follows with probability \(1-\frac{\delta}{30H\ell^{2}}\).

The result then holds by union bounding over each of these for all \(h\). 

**Lemma 17**.: _On \(\mathcal{E}_{\mathrm{prune}}^{\ell}\), for all \(h\) and \(\pi\) we have_

\[\delta_{\ell,h+1}^{\pi}-\widetilde{\delta}_{\ell,h+1}^{\pi}\] \[=\sum_{i=0}^{h-2}\left(\prod_{j=h-i+1}^{h}M_{\ell,j+1}P_{j}\bm{ \pi}_{j}\right)M_{\ell,h-i+1}P_{h-i}(\bm{\pi}_{h-i}-\bm{\bar{\pi}}_{h-i})(w_{ \ell,h-i}^{\bar{\pi}_{\ell}}-\widehat{w}_{\ell,h-i}^{\bar{\pi}_{\ell}})+ \Delta_{\overline{\ell},h+1}^{\pi}\]

_for some \(\Delta_{\ell,h}^{\pi}\in\mathbb{R}^{S}\) with \(\|\Delta_{\ell,h}^{\pi}\|_{2}\leq 32SH\epsilon_{\mathrm{unif}}^{\ell}\). Furthermore, for any \(\pi\) and any \(i,k\) satisfying \(0\leq i\leq k\leq H\), we have_

\[\left\|\left(\prod_{j=i}^{k}M_{\ell,j+1}P_{j}\bm{\pi}_{j}-\prod_{j=i}^{k}P_{j} \bm{\pi}_{j}\right)w_{i}^{\pi}\right\|_{2}\leq 32SH\epsilon_{\mathrm{unif}}^{\ell}.\]

Proof.: By definition, we have that

\[\delta_{\ell,h+1}^{\pi}-\widetilde{\delta}_{\ell,h+1}^{\pi}\] \[=P_{h}(\bm{\pi}_{h}-\bm{\bar{\pi}}_{\ell,h})w_{\ell,h}^{\bar{\pi} _{\ell}}+P_{h}\bm{\pi}_{h}\delta_{\ell,h}^{\pi}-M_{\ell,h+1}P_{h}(\bm{\pi}_{h}- \bm{\bar{\pi}}_{\ell,h})\widehat{w}_{\ell,h}^{\bar{\pi}}-M_{\ell,h+1}P_{h}\bm {\pi}_{h}\widetilde{\delta}_{\ell,h}^{\pi}\] \[=(I-M_{\ell,h+1})P_{h}(\bm{\pi}_{h}-\bm{\bar{\pi}}_{\ell,h})w_{ \ell,h}^{\bar{\pi}_{\ell}}+M_{\ell,h+1}P_{h}(\bm{\pi}_{h}-\bm{\bar{\pi}}_{\ell, h})(w_{\ell,h}^{\bar{\pi}_{\ell}}-\widehat{w}_{\ell,h}^{\bar{\pi}})\] \[\qquad+(I-M_{\ell,h+1})P_{h}\bm{\pi}_{h}\delta_{\ell,h}^{\pi}+M_{ \ell,h+1}P_{h}\bm{\pi}_{h}(\delta_{\ell,h}^{\pi}-\widetilde{\delta}_{\ell,h}^{ \pi})\] \[\vdots\] \[=\sum_{i=0}^{h-2}\left(\prod_{j=h-i+1}^{h}M_{\ell,j+1}P_{j}\bm{ \pi}_{j}\right)\bigg{[}(I-M_{\ell,h-i+1})P_{h-i}(\bm{\pi}_{h-i}-\bm{\bar{\pi}}_{h -i})w_{\ell,h-i}^{\pi_{\ell}}\] \[\qquad+M_{\ell,h-i+1}P_{h-i}(\bm{\pi}_{h-i}-\bm{\bar{\pi}}_{h-i})( w_{\ell,h-i}^{\bar{\pi}_{\ell}}-\widehat{w}_{\ell,h-i}^{\bar{\pi}_{\ell}})+(I-M_{ \ell,h-i+1})P_{h-i}\bm{\pi}_{h-i}\delta_{\ell,h-i}^{\pi}\bigg{]}.\]

Note that \([P_{h-i}(\bm{\pi}_{h-i}-\bm{\bar{\pi}}_{h-i})w_{\ell,h}^{\bar{\pi}_{\ell}}]_{s} \leq W_{h-i+1}^{*}(s)\), and similarly \([P_{h-i}\bm{\pi}_{h-i}\delta_{\ell,h-i}^{\pi}]_{s}\leq W_{h-i+1}^{*}(s)\). On the event \(\mathcal{E}_{\mathrm{prune}}^{\ell}\), we have that if \([M_{\ell,h-i+1}]_{s,s}=0\), then \(W_{h-i+1}^{*}(s)\leq 32\epsilon_{\mathrm{unif}}^{\ell}\). It follows from this that every non-zero element in \((I-M_{\ell,h-i+1})P_{h-i}(\bm{\pi}_{h-i}-\bm{\bar{\pi}}_{h-i})w_{\ell,h-i}^{ \bar{\pi}_{\ell}}\) and \((I-M_{\ell,h-i+1})P_{h-i}\bm{\pi}_{h-i}\delta_{\ell,h-i}^{\pi}\) is bounded by \(32\epsilon_{\mathrm{unif}}^{\ell}\), so:

\[\|(I-M_{\ell,h-i+1})P_{h-i}(\bm{\pi}_{h-i}-\bm{\bar{\pi}}_{h-i})w_{\ell,h-i}^{ \bar{\pi}_{\ell}}\|_{2}\leq 32\sqrt{S}\epsilon_{\mathrm{unif}}^{\ell}\]

\[\|(I-M_{\ell,h-i+1})P_{h-i}\bm{\pi}_{h-i}\delta_{\ell,h-i}^{\pi}\|_{2}\leq 32 \sqrt{S}\epsilon_{\mathrm{unif}}^{\ell}.\]

By Lemma 11, we can bound

\[\|\prod_{j=h-i+1}^{h}M_{\ell,j+1}P_{j}\bm{\pi}_{j}\|_{2}\leq\sqrt{S}.\]Combining these gives the result.

We now prove the second part of the result. Denote \(A_{j}:=M_{\ell,j+1}P_{j}\bm{\pi}_{j}\) and \(B_{j}:=P_{j}\bm{\pi}_{j}\). Then

\[\prod_{j=i}^{k}M_{\ell,j+1}P_{j}\bm{\pi}_{j}-\prod_{j=i}^{k}P_{j} \bm{\pi}_{j} =\prod_{j=i}^{k}A_{j}-\prod_{j=i}^{k}B_{j}\] \[=A_{k}\left(\prod_{j=i}^{k-1}A_{j}-\prod_{j=i}^{k-1}B_{j}\right)+ (A_{k}-B_{k})\prod_{j=i}^{k-1}B_{j}\] \[\vdots\] \[=\sum_{s=i}^{k}\left(\prod_{j=s+1}^{k}A_{j}\right)(A_{s}-B_{s}) \left(\prod_{j^{\prime}=i}^{s-1}B_{j^{\prime}}\right).\]

By Lemma 11 we have \(\|\prod_{j=s+1}^{k}A_{j}\|_{2}\leq\sqrt{S}\). Furthermore, note that \(\prod_{j^{\prime}=i}^{s-1}B_{j^{\prime}}w_{i}^{\pi}=w_{s}^{\pi}\). So it follows that

\[\left\|(\prod_{j=i}^{k}M_{\ell,j+1}P_{j}\bm{\pi}_{j}-\prod_{j=i}^{k}P_{j}\bm{ \pi}_{j})w_{i}^{\pi}\right\|_{2}\leq\sum_{s=i}^{k}\sqrt{S}\|(A_{s}-B_{s})w_{s }^{\pi}\|_{2}.\]

By the same argument as above, we can bound \(\|(A_{s}-B_{s})w_{s}^{\pi}\|_{2}\leq 32\sqrt{S}\epsilon_{\mathrm{unif}}^{\ell}\). 

**Lemma 18**.: _On the event \(\mathcal{E}_{\mathrm{prune}}^{\ell}\cap(\cap_{h^{\prime}\leq h}\mathcal{E}_{ \mathrm{est}}^{\ell,h^{\prime}})\cap(\cap_{h^{\prime}\leq h}\mathcal{E}_{ \mathrm{exp}}^{\ell,h^{\prime}})\), we have, for all \(\pi\in\Pi_{\ell}\):_

\[\|\widehat{\delta}_{\ell,h+1}^{\pi}-\delta_{\ell,h+1}^{\pi}\|_{2}\leq\sqrt{SH \beta_{\ell}\epsilon_{\mathrm{exp}}^{\ell}}+SH(\sqrt{8\epsilon_{\ell}^{5/3}}+ 32\epsilon_{\mathrm{unif}}^{\ell}).\]

Proof.: We can write

\[\|\widehat{\delta}_{\ell,h+1}^{\pi}-\delta_{\ell,h+1}^{\pi}\|_{2}\leq\| \widehat{\delta}_{\ell,h+1}^{\pi}-\widetilde{\delta}_{\ell,h+1}^{\pi}\|_{2}+ \|\widehat{\delta}_{\ell,h+1}^{\pi}-\delta_{\ell,h+1}^{\pi}\|_{2}.\]

From Lemma 12 we have

\[\widetilde{\delta}_{\ell,h+1}^{\pi}-\widehat{\delta}_{\ell,h+1}^{\pi}\] \[=\sum_{i=0}^{h-2}\left(\prod_{j=h-i+1}^{h}M_{\ell,j+1}P_{j}\bm{ \pi}_{j}\right)(P_{h-i}-\widehat{P}_{\ell,h-i})M_{\ell,h-i}\Big{[}(\bm{\pi}_{h -i}-\bar{\bm{\pi}}_{\ell,h-i})\widehat{w}_{\ell,h-i}^{\pi}+\bm{\pi}_{h-i} \widehat{\delta}_{\ell,h-i}^{\pi}\Big{]}.\]

From Event (2) of \(\mathcal{E}_{\mathrm{est}}^{\ell,h}\) in Lemma 15, we have that for all canonical vectors \(e_{s}\) and \(\pi\in\Pi_{\ell}\):

\[\left\langle e_{s},\left(\prod_{j=h-i+1}^{h}M_{\ell,j+1}P_{j}\bm{ \pi}_{j}\right)(P_{h-i}-\widehat{P}_{\ell,h-i})M_{\ell,h-i}\Big{[}(\bm{\pi}_{h -i}-\bar{\bm{\pi}}_{h-i})\widehat{w}_{\ell,h-i}^{\pi}+\bm{\pi}_{h-i}\widehat{ \delta}_{\ell,h-i}^{\pi}\Big{]}\right\rangle\] \[\leq\beta_{\ell}\sqrt{\sum_{s,a}\frac{[M_{\ell,h-i}((\bm{\pi}_{h -i}-\bar{\bm{\pi}}_{\ell,h-i})\widehat{w}_{\ell,h-i}^{\bar{\pi}}+\bm{\pi}_{h-i} \widehat{\delta}_{\ell,h-i}^{\pi})]_{s,a}^{2}}{N_{\ell,h-i}(s,a)}}.\]

Now, summing over the bound above for all canonical vectors, and applying this for each \(i\), it follows that

\[\|\widehat{\delta}_{\ell,h+1}^{\pi}-\widetilde{\delta}_{\ell,h+1}^{\pi}\|_{2}^ {2}\leq S\beta_{\ell}^{2}\sum_{h^{\prime}=1}^{h}\sum_{s,a}\frac{[M_{\ell,h^{ \prime}}((\bm{\pi}_{h^{\prime}}-\bar{\bm{\pi}}_{\ell,h^{\prime}})\widehat{w}_ {\ell,h^{\prime}}^{\pi}+\bm{\pi}_{h^{\prime}}\widehat{\delta}_{\ell,h^{\prime} }^{\pi})]_{s,a}^{2}}{N_{\ell,h}(s,a)}\leq SH\beta_{\ell}\epsilon_{\mathrm{exp}}^{\ell}\]

where the last inequality holds on \(\cap_{h^{\prime}\leq h}\mathcal{E}_{\mathrm{exp}}^{\ell,h^{\prime}}\).

We now turn to bounding \(\|\widetilde{\delta}^{\pi}_{\ell,h+1}-\delta^{\pi}_{\ell,h+1}\|_{2}\). By Lemma 17 we have

\[\delta^{\pi}_{\ell,h+1}-\widetilde{\delta}^{\pi}_{\ell,h+1}\] \[=\sum_{i=0}^{h-2}\left(\prod_{j=h-i+1}^{h}M_{\ell,j+1}P_{j}\bm{ \pi}_{j}\right)M_{\ell,h-i+1}P_{h-i}(\bm{\pi}_{h-i}-\bm{\bar{\pi}}_{h-i})(w^{ \bar{\pi}_{\ell}}_{\ell,h-i}-\widehat{w}^{\bar{\pi}_{\ell}}_{\ell,h-i})+\Delta ^{\pi}_{\ell,h+1}\]

for some \(\Delta^{\pi}_{\ell,h}\in\mathbb{R}^{S}\) with \(\|\Delta^{\pi}_{\ell,h}\|_{2}\leq 32SH\epsilon^{\ell}_{\mathrm{unit}}\). Furthermore, on \(\mathcal{E}^{\ell,h-i}_{\mathrm{est}}\), by Lemma 19 we can bound

\[\|w^{\bar{\pi}_{\ell}}_{\ell,h-i}-\widehat{w}^{\bar{\pi}_{\ell}}_{\ell,h-i}\|_ {2}\leq\sqrt{8S\epsilon^{5/3}_{\ell}}.\]

Combining this with Lemma 11 gives the result.

**Lemma 19**.: _On event \(\bar{\mathcal{E}}^{\ell}_{\mathrm{est}}\) we have:_

\[\|\widehat{w}^{\bar{\pi}}_{\ell,h}-w^{\bar{\pi}}_{\ell,h}\|_{2}^{2}\leq 8S \epsilon^{5/3}_{\ell}.\]

Proof.: From Event (2) of Lemma 16, we have that for all canonical vectors \(e_{i}\in\mathbb{R}^{S}\):

\[|\langle e_{i},\widehat{w}^{\bar{\pi}}_{\ell,h}-w^{\bar{\pi}}_{\ell,h}\rangle| \leq\sqrt{\frac{2\log\left(\frac{30H\ell^{2}S}{\delta}\right)}{\bar{n}_{\ell} }}+\frac{2\log\left(\frac{30H\ell^{2}S}{\delta}\right)}{\bar{n}_{\ell}}.\]

Then, combining these bounds together for all \(s\):

\[\|\widehat{w}^{\bar{\pi}}_{\ell,h}-w^{\bar{\pi}}_{\ell,h}\|_{2}^{2}\leq\frac{ 4S\log\left(\frac{30H\ell^{2}S}{\delta}\right)}{\bar{n}_{\ell}}+\frac{4S\log^{ 2}\left(\frac{30H\ell^{2}S}{\delta}\right)}{\bar{n}_{\ell}^{2}}\leq 4S \epsilon^{5/3}_{\ell}+4S\epsilon^{10/3}_{\ell}\leq 8S\epsilon^{5/3}_{\ell},\]

where the last inequality follows from our choice of \(\bar{n}_{\ell}\) in Algorithm 2.

**Lemma 20**.: _Let \(\mathcal{E}_{\mathrm{good}}:=(\cap_{\ell=1}^{\infty}\mathcal{E}^{\ell}_{ \mathrm{prune}})\cap(\cap_{\ell=1}^{\infty}\bar{\mathcal{E}}^{\ell}_{\mathrm{ est}})\cap(\cap_{\ell=1}^{\infty}\cap_{h\in[H]}\mathcal{E}^{\ell,h}_{\mathrm{est}})\cap( \cap_{\ell=1}^{\infty}\cap_{h\in[H]}\mathcal{E}^{\ell,h}_{\mathrm{exp}})\). Then \(\mathbb{P}[\mathcal{E}_{\mathrm{good}}]\geq 1-2\delta\)._

Proof.: By a union bound and basic set manipulations, we have:

\[\mathbb{P}[\mathcal{E}^{c}_{\mathrm{good}}] \leq\sum_{\ell=1}^{\infty}\mathbb{P}[(\mathcal{E}^{\ell}_{\mathrm{ prune}})^{c}]+\sum_{\ell=1}^{\infty}\mathbb{P}[(\bar{\mathcal{E}}^{\ell}_{ \mathrm{est}})^{c}]\] \[\quad+\sum_{\ell=1}^{\infty}\sum_{h=1}^{H}\mathbb{P}[(\mathcal{E}^ {\ell,h}_{\mathrm{exp}})^{c}\cap\mathcal{E}^{\ell}_{\mathrm{prune}}\cap\bar{ \mathcal{E}}^{\ell}_{\mathrm{est}}\cap(\cap_{h^{\prime}\leq h-1}\mathcal{E}^{ \ell,h^{\prime}}_{\mathrm{est}})\cap(\cap_{h^{\prime}\leq h-1}\mathcal{E}^{ \ell,h^{\prime}}_{\mathrm{exp}})]\] \[\quad+\sum_{\ell=1}^{\infty}\sum_{h=1}^{H}\mathbb{P}[(\mathcal{E}^ {\ell,h}_{\mathrm{est}})^{c}\cap\mathcal{E}^{\ell}_{\mathrm{prune}}\cap(\cap _{h^{\prime}\leq h}\mathcal{E}^{\ell,h}_{\mathrm{exp}})].\]

By Lemma 13, we have \(\mathbb{P}[(\mathcal{E}^{\ell}_{\mathrm{prune}})^{c}]\leq\delta/3\ell^{2}\). By Lemma 16, we have \(\mathbb{P}[(\bar{\mathcal{E}}^{\ell}_{\mathrm{est}})^{c}]\leq\frac{\delta}{15 \ell^{2}}\). By Lemma 14, we have \(\mathbb{P}[(\mathcal{E}^{\ell,h}_{\mathrm{re}})^{c}\cap\mathcal{E}^{\ell}_{ \mathrm{prune}}\cap\bar{\mathcal{E}}^{\ell}_{\mathrm{est}}\cap(\cap_{h^{ \prime}\leq h-1}\mathcal{E}^{\ell,h^{\prime}}_{\mathrm{est}})\cap(\cap_{h^{ \prime}\leq h-1}\mathcal{E}^{\ell,h^{\prime}}_{\mathrm{exp}})]\leq\frac{\delta}{6H \ell^{2}}\). By Lemma 15 we have \(\mathbb{P}[(\mathcal{E}^{\ell,h}_{\mathrm{est}})^{c}\cap\mathcal{E}^{\ell}_{ \mathrm{prune}}\cap(\cap_{h^{\prime}\leq h}\mathcal{E}^{\ell,h}_{\mathrm{exp}})] \leq\frac{\delta}{6H\ell^{2}}\). Putting this together we can bound the above as

\[\leq\sum_{\ell=1}^{\infty}(\frac{\delta}{3\ell^{2}}+\frac{\delta}{15\ell^{2}})+ \sum_{\ell=1}^{\infty}\sum_{h=1}^{H}\frac{2\delta}{6H\ell^{2}}\leq 2\delta.\]

### Estimation of Reference Policy and Values

**Lemma 21**.: _On \(\mathcal{E}_{\mathrm{good}}\) we have that:_

\[\left|\sum_{h=1}^{H}\langle\widetilde{r}_{\ell,h},\bm{\pi}_{h}( \widetilde{\delta}_{\ell,h}^{\pi}-\widetilde{\delta}_{\ell,h}^{\pi})\rangle \right|\leq\epsilon_{\ell}\quad\text{and}\quad\sum_{h=1}^{H}|\langle\widehat{r }_{\ell,h}-\widetilde{r}_{\ell,h},\bm{\pi}_{h}\widehat{\delta}_{\ell,h}^{ \pi}+(\bm{\pi}_{h}-\bm{\bar{\pi}}_{\ell,h})\widehat{w}_{\ell,h}^{\bar{\pi}} \rangle|\leq\epsilon_{\ell}.\] (C.5)

Proof.: From Lemma 12 we have:

\[\widetilde{\delta}_{\ell,h+1}^{\pi}-\widehat{\delta}_{\ell,h+1}^{ \pi}\] \[=\sum_{i=0}^{h-2}\left(\prod_{j=h-i+1}^{h}M_{\ell,j+1}P_{j}\bm{ \pi}_{j}\right)(P_{h-i}-\widehat{P}_{\ell,h-i})M_{\ell,h-i}\Big{[}(\bm{\pi}_{ h-i}-\bm{\bar{\pi}}_{\ell,h-i})\widehat{w}_{\ell,h-i}^{\pi}+\bm{\pi}_{h-i} \widehat{\delta}_{\ell,h-i}^{\pi}\Big{]}.\]

A sufficient condition for (C.5) is that, for each \(i\):

\[\left|\left\langle\bm{\pi}_{h}^{\top}\widetilde{r}_{\ell,h}, \left(\prod_{j=h-i+1}^{h}M_{\ell,j+1}P_{j}\bm{\pi}_{j}\right)(P_{h-i}-\widehat{ P}_{\ell,h-i})\right.\right.\] \[\left.\left.M_{\ell,h-i}\Big{[}(\bm{\pi}_{h-i}-\bm{\bar{\pi}}_{ \ell,h-i})\widehat{w}_{\ell,h-i}^{\bar{\pi}}+\bm{\pi}_{h-i}\widehat{\delta}_{ \ell,h-i}^{\pi}\Big{]}\right\rangle\right|\leq\epsilon_{\ell}.\]

On \(\mathcal{E}_{\mathrm{good}}\), and in particular \(\mathcal{E}_{\mathrm{est}}^{\ell,h}\) (Lemma 15), we can bound the left-hand side of this as:

\[\leq\beta_{\ell}\sqrt{\sum_{s,a}\frac{\left[M_{\ell,h-i}\left(( \bm{\pi}_{h-i}-\bm{\bar{\pi}}_{\ell,h-i})\widehat{w}_{\ell,h-i}^{\bar{\pi}}+ \bm{\pi}_{h-i}\widehat{\delta}_{\ell,h-i}^{\bar{\tau}}\right)\right]_{(s,a)}^ {2}}{N_{\ell,h-i}(s,a)}}\] \[\leq\beta_{\ell}\sqrt{\epsilon_{\ell}^{2}/H^{4}\beta_{\ell}^{2}}\] \[\leq\epsilon_{\ell}/H^{2}\]

where the second inequality holds on \(\mathcal{E}_{\mathrm{good}}\) (in particular \(\mathcal{E}_{\mathrm{exp}}^{\ell,h-i}\)). This proves the first inequality.

On \(\mathcal{E}_{\mathrm{est}}^{\ell,h}\) we can also bound

\[|\langle\widehat{r}_{\ell,h}-\widetilde{r}_{\ell,h},\bm{\pi}_{h} \widehat{\delta}_{\ell,h}^{\pi}+(\bm{\pi}_{h}-\bm{\bar{\pi}}_{\ell,h})\widehat {w}_{\ell,h}^{\bar{\pi}}\rangle|\] \[\leq\beta_{\ell}\sqrt{\sum_{s,a}\frac{\left[M_{\ell,h}\left((\bm {\pi}_{h}-\bm{\bar{\pi}}_{\ell,h^{\prime}})\widehat{w}_{\ell,h}^{\bar{\pi}}+ \bm{\pi}_{h}\widehat{\delta}_{\ell,h}^{\pi}\right)\right]_{(s,a)}^{2}}{N_{\ell, h}(s,a)}}\] \[\leq\epsilon_{\ell}/H^{2}.\]

This proves the second inequality.

**Lemma 22**.: _On event \(\mathcal{E}_{\mathrm{good}}\), for any timestep \(h\), policies \(\pi,\pi^{\prime}\), and action \(a\), we have:_

\[\mathbb{E}_{\pi^{\prime}}||\widehat{Q}_{\ell,h}^{\pi}(s_{h},a)-Q_{h}^{\pi}(s_{ h},a)|]\leq H^{2}S^{3/2}\sqrt{A\log\frac{24S^{2}AH\ell^{2}}{\delta}}\cdot\epsilon_{ \ell}^{1/3}+64H^{2}S\epsilon_{\mathrm{unif}}^{\ell}.\] (C.6)

Proof.: By Lemma E.15 of [10], we have that:

\[\widehat{Q}_{\ell,h}^{\pi}(s,a)-Q_{h}^{\pi}(s,a)\] \[=\mathbb{E}_{\pi}\left[\sum_{h^{\prime}=h}^{H}\sum_{s^{\prime}}( \widehat{P}_{\ell,h^{\prime}}(s^{\prime}\mid s_{h^{\prime}},a_{h^{\prime}})-P_ {h}(s^{\prime}\mid s_{h^{\prime}},a_{h^{\prime}}))\widehat{V}_{\ell,h^{\prime}+ 1}^{\pi}(s_{h^{\prime}})\mid s_{h}=s,a_{h}=a\right].\]On \(\mathcal{E}_{\mathrm{good}}\), in particular \(\mathcal{E}_{\mathrm{est}}^{\ell,h^{\prime}}\), we can bound, for \(s\in\mathcal{S}_{\ell,h^{\prime}}^{\mathrm{keep}}\) and any \(a\):

\[\left|\sum_{s^{\prime}}(\widehat{P}_{\ell,h^{\prime}}(s^{\prime} \mid s,a)-P_{h}(s^{\prime}\mid s,a))\widehat{V}_{\ell,h^{\prime}+1}^{\tau}(s^ {\prime})\right|\] \[\leq SH\sqrt{\frac{\log\frac{24S^{2}AH\ell^{2}}{\delta}}{N_{\ell, h^{\prime}}(s,a)}}\leq SH\sqrt{\frac{SA\log\frac{24S^{2}AH\ell^{2}}{ \delta}}{K_{\mathrm{unif}}^{\ell}\epsilon_{\mathrm{unif}}^{\ell}}}\]

and where the last inequality follows on \(\mathcal{E}_{\mathrm{exp}}^{\ell,h^{\prime}}\). By our choice of \(K_{\mathrm{unif}}^{\ell}\) and \(\epsilon_{\mathrm{unif}}^{\ell}\), we can further bound this as

\[\leq SH\sqrt{SA\log\frac{24S^{2}AH\ell^{2}}{\delta}}\cdot\epsilon_{\ell}^{1/3}.\]

For \(s\not\in\mathcal{S}_{\ell,h^{\prime}}^{\mathrm{keep}}\), we can bound \(|\sum_{s^{\prime}}(\widehat{P}_{\ell,h^{\prime}}(s^{\prime}\mid s,a)-P_{h}(s ^{\prime}\mid s,a))\widehat{V}_{\ell,h^{\prime}}^{\pi}(s_{h^{\prime}})|\leq 2H\). We therefore have that

\[\mathbb{E}_{\pi^{\prime}}[|\widehat{Q}_{\ell,h}^{\pi}(s_{h},a)-Q _{h}^{\pi}(s_{h},a)|]\] \[\leq\mathbb{E}_{\pi^{\prime}}\bigg{[}\mathbb{E}_{\pi}\bigg{[} \sum_{h^{\prime}=h}^{H}SH\sqrt{SA\log\frac{24S^{2}AH\ell^{2}}{\delta}}\cdot \epsilon_{\ell}^{1/3}\cdot\mathbb{I}\{s_{h^{\prime}}\in\mathcal{S}_{\ell,h^{ \prime}}^{\mathrm{keep}}\}\] \[\qquad\qquad+2H\mathbb{I}\{s_{h^{\prime}}\not\in\mathcal{S}_{\ell,h^{\prime}}^{\mathrm{keep}}\}\mid s_{h}=s,a_{h}=a\bigg{]}\bigg{]}\] \[=\sum_{h^{\prime}=h}^{H}\mathbb{E}_{\tilde{\pi}}\left[SH\sqrt{ SA\log\frac{24S^{2}AH\ell^{2}}{\delta}}\cdot\epsilon_{\ell}^{1/3}\cdot \mathbb{I}\{s_{h^{\prime}}\in\mathcal{S}_{\ell,h^{\prime}}^{\mathrm{keep}}\}+ 2H\mathbb{I}\{s_{h^{\prime}}\not\in\mathcal{S}_{\ell,h^{\prime}}^{\mathrm{keep }}\}\right]\] \[\leq H^{2}S^{3/2}\sqrt{A\log\frac{24S^{2}AH\ell^{2}}{\delta}} \cdot\epsilon_{\ell}^{1/3}+64H^{2}S\epsilon_{\mathrm{unif}}^{\ell},\]

where the last inequality follows by definition of \(\mathcal{S}_{\ell,h^{\prime}}^{\mathrm{keep}}\), and \(\pi^{\prime}\) is the policy which plays \(\bar{\pi}_{\ell}\) for the first \(h\) steps and then plays \(\pi\). This proves the result. 

**Lemma 23**.: _On event \(\mathcal{E}_{\mathrm{good}}\), for all \(h\) and any \(\pi\) and \(\pi^{\prime}\), we have that_

\[|\widehat{U}_{\ell,h}(\pi,\pi^{\prime})-U_{h}(\pi,\pi^{\prime})|\leq 9H^{3}S^{ 3/2}\sqrt{A\log\frac{24S^{2}AH\ell^{2}}{\delta}}\cdot\epsilon_{\ell}^{1/3}+576 H^{3}S\epsilon_{\mathrm{unif}}^{\ell}.\]

Proof.: We have

\[\widehat{U}_{\ell,h}(\pi,\pi^{\prime})=\mathbb{E}_{\pi^{\prime},\ell}\left[ \left(\widehat{Q}_{\ell,h}^{\pi}(s_{h},\pi_{h}(s_{h}))-\widehat{Q}_{\ell,h}^{ \pi}(s_{h},\pi_{h}^{\prime}(s_{h}))\right)^{2}\right]\]

where \(\mathbb{E}_{\pi^{\prime},\ell}\) denotes the expectation induced playing policy \(\pi^{\prime}\) on the MDP with transition \(\widehat{P}_{\ell}\). We can think of this as simply a value function for policy \(\pi\) on the reward \(\check{r}_{h}(s,a)=\left(\widehat{Q}_{\ell,h}^{\pi}(s,\pi_{h}(s))-\widehat{Q} _{\ell,h}^{\pi}(s,a)\right)^{2}\). Let \(\check{V}\) denote the value function on this reward on \(\widehat{P}_{\ell}\), and note that \(\check{V}_{h}(s)\in[0,H^{2}]\) for all \((s,h)\). By Lemma E.15 of [10], we then have that

\[\left|\widehat{U}_{\ell,h}(\pi,\pi^{\prime})-\mathbb{E}_{\pi^{\prime }}\left[\left(\widehat{Q}_{\ell,h}^{\pi}(s_{h},\pi_{h}(s_{h}))-\widehat{Q}_{ \ell,h}^{\pi}(s_{h},\pi_{h}^{\prime}(s_{h}))\right)^{2}\right]\right|\] \[=\mathbb{E}_{\pi^{\prime}}\left[\sum_{h=1}^{H}\sum_{s^{\prime}}( \widehat{P}_{\ell,h}(s^{\prime}\mid s_{h},a_{h})-P_{h}(s^{\prime}\mid s_{h},a_{h }))\check{V}_{h+1}(s^{\prime})\right]\] \[\leq H^{2}\sum_{h=1}^{H}\mathbb{E}_{\pi^{\prime}}\left[\sum_{s^{ \prime}}|\widehat{P}_{\ell,h}(s^{\prime}\mid s_{h},a_{h})-P_{h}(s^{\prime}\mid s _{h},a_{h})|\right].\]Note that we always have \(\sum_{s^{\prime}}|\widehat{P}_{\ell,h}(s^{\prime}\mid s_{h},a_{h})-P_{h}(s^{ \prime}\mid s_{h},a_{h})|\leq 2\). Furthermore, on \(\mathcal{E}_{\mathrm{good}}\) we also have \(\sum_{s^{\prime}}|\widehat{P}_{\ell,h}(s^{\prime}\mid s_{h},a_{h})-P_{h}(s^{ \prime}\mid s_{h},a_{h})|\leq S\sqrt{\frac{\log\frac{24S^{2}AH\ell^{2}}{\delta }}{N_{\ell,h}(s_{h},a_{h})}}\). We can therefore bound the above as

\[\leq H^{2}\sum_{h=1}^{H}\mathbb{E}_{\pi^{\prime}}\left[\min\left\{ 2,S\sqrt{\frac{\log\frac{24S^{2}AH\ell^{2}}{\delta}}{N_{\ell,h}(s_{h},a_{h})} }\right\}\right]\] \[\leq H^{2}\sum_{h=1}^{H}\mathbb{E}_{\pi^{\prime}}\left[2\cdot \mathbb{I}\{s_{h}\not\in\mathcal{E}_{\ell,h}^{\mathrm{keep}}\}+S\sqrt{\frac{ \log\frac{24S^{2}AH\ell^{2}}{\delta}}{N_{\ell,h}(s_{h},a_{h})}}\cdot\mathbb{I} \{s_{h}\in\mathcal{S}_{\ell,h}^{\mathrm{keep}}\}\right].\]

For \(s\in\mathcal{S}_{\ell,h}^{\mathrm{keep}}\), on \(\mathcal{E}_{\mathrm{good}}\) we have \(N_{\ell,h}(s_{h},a_{h})\geq\frac{K_{\mathrm{unif}}^{\ell}\epsilon_{\mathrm{ unif}}^{\ell}}{SA}=\epsilon_{\ell}^{2/3}/SA\), and we also have for \(s_{h}\not\in\mathcal{S}_{\ell,h}^{\mathrm{keep}}\) that \(W_{h}^{\star}(s)\leq 32\epsilon_{\mathrm{unif}}^{\ell}\). Putting this together we can bound the above as

\[\leq H^{2}\sum_{h=1}^{H}\left[64S\epsilon_{\mathrm{unif}}^{\ell}+ S\sqrt{SA\log\frac{24S^{2}AH\ell^{2}}{\delta}}\cdot\epsilon_{\ell}^{1/3}\right]\] \[\leq 64SH^{3}\epsilon_{\mathrm{unif}}^{\ell}+H^{3}S^{3/2}\sqrt{A \log\frac{24S^{2}AH\ell^{2}}{\delta}}\cdot\epsilon_{\ell}^{1/3}.\]

Furthermore,

\[\left|\mathbb{E}_{\pi^{\prime}}\left[\left(\widehat{Q}_{\ell,h}^ {\pi}(s,\pi_{h}(s))-\widehat{Q}_{\ell,h}^{\pi}(s,\pi_{h}^{\prime}(s))\right)^ {2}\right]-\mathbb{E}_{\pi^{\prime}}\left[(Q_{h}^{\pi}(s,\pi_{h}(s))-Q_{h}^{ \pi}(s,\pi_{h}^{\prime}(s)))^{2}\right]\right|\] \[=\left|\mathbb{E}_{\pi^{\prime}}\left[\left(\widehat{Q}_{\ell,h}^ {\pi}(s,\pi_{h}(s))-Q_{h}^{\pi}(s,\pi_{h}(s))+Q_{h}^{\pi}(s,\pi_{h}^{\prime}(s ))-\widehat{Q}_{\ell,h}^{\pi}(s,\pi_{h}^{\prime}(s))\right)^{2}\right]\] \[\qquad\qquad+\mathbb{E}_{\pi^{\prime}}\bigg{[}\left(\widehat{Q} _{\ell,h}^{\pi}(s,\pi_{h}(s))-Q_{h}^{\pi}(s,\pi_{h}(s))+Q_{h}^{\pi}(s,\pi_{h}^ {\prime}(s))-\widehat{Q}_{\ell,h}^{\pi}(s,\pi_{h}^{\prime}(s))\right)\] \[\qquad\qquad\qquad\qquad(Q_{h}^{\pi}(s,\pi_{h}(s))-Q_{h}^{\pi}( s,\pi_{h}^{\prime}(s)))\bigg{]}\bigg{|}\] \[\leq 4H\mathbb{E}_{\pi^{\prime}}[|\widehat{Q}_{\ell,h}^{\pi}(s, \pi_{h}(s))-Q_{h}^{\pi}(s,\pi_{h}(s))|]+4H\mathbb{E}_{\pi^{\prime}}[|Q_{h}^{ \pi}(s,\pi_{h}^{\prime}(s))-\widehat{Q}_{\ell,h}^{\pi}(s,\pi_{h}^{\prime}(s))|]\] \[\leq 8H^{3}S^{3/2}\sqrt{A\log\frac{24S^{2}AH\ell^{2}}{\delta}} \cdot\epsilon_{\ell}^{1/3}+512H^{3}S\epsilon_{\mathrm{unif}}^{\ell}\]

where the final inequality follows from Lemma 22. Combining this with the above bound completes the argument. 

**Lemma 24**.: _On event \(\mathcal{E}_{\mathrm{good}}\), for all epochs \(\ell\), we have that_

\[\left|\sum_{h=1}^{H}\langle\widetilde{r}_{\ell,h},\bm{\pi}_{h}(\delta_{\ell,h}^ {\pi}-\widetilde{\delta}_{\ell,h}^{\pi})\rangle+\langle\widetilde{r}_{\ell,h}, (\bm{\pi}_{h}-\bar{\bm{\pi}}_{\ell,h})(w_{\ell,h}^{\bar{\pi}}-\widehat{w}_{ \ell,h}^{\bar{\pi}})\rangle\right|\leq\epsilon_{\ell}.\] (C.7)

Proof.: We first bound \(|\langle M_{\ell,h}r_{h},\bm{\pi}_{h}(\delta_{\ell,h}^{\pi}-\widetilde{\delta}_ {\ell,h}^{\pi})\rangle|\). By Lemma 17 we have that

\[\delta_{\ell,h+1}^{\pi}-\widetilde{\delta}_{\ell,h+1}^{\pi}\] \[=\sum_{i=0}^{h-2}\left(\prod_{j=h-i+1}^{h}M_{\ell,j+1}P_{j}\bm{ \pi}_{j}\right)M_{\ell,h-i+1}P_{h-i}(\bm{\pi}_{h-i}-\bar{\bm{\pi}}_{\ell,h-i})( w_{h-i}^{\pi}-\widehat{w}_{\ell,h-i}^{\bar{\pi}})+\Delta_{\ell,h+1}^{\pi}\]for some \(\Delta_{\ell,h}^{\pi}\in\mathbb{R}^{S}\) with \(\|\Delta_{\ell,h}^{\pi}\|_{2}\leq 325H\epsilon_{\rm unif}^{\ell}\). Furthermore, note that

\[\sum_{h=1}^{H}\sum_{i=0}^{h-2}\left\langle\widetilde{r}_{\ell,h}, \boldsymbol{\pi}_{h}\left(\prod_{j=h-i+1}^{h}M_{\ell,j+1}P_{j}\boldsymbol{\pi} _{j}\right)M_{\ell,h-i+1}P_{h-i}(\boldsymbol{\pi}_{h-i}-\bar{\boldsymbol{\pi}} _{\ell,h-i})(w_{h-i}^{\bar{\pi}}-\widehat{w}_{\ell,h-i}^{\bar{\pi}})\right\rangle\] \[=\sum_{h=1}^{H}\sum_{k=2}^{h}\left\langle\widetilde{r}_{\ell,h}, \boldsymbol{\pi}_{h}\left(\prod_{j=k+1}^{h}M_{\ell,j+1}P_{j}\boldsymbol{\pi} _{j}\right)M_{\ell,k+1}P_{k}(\boldsymbol{\pi}_{k}-\bar{\boldsymbol{\pi}}_{ \ell,k})(w_{k}^{\pi}-\widehat{w}_{\ell,k}^{\bar{\pi}})\right\rangle\] \[=\sum_{k=2}^{H}\sum_{h=k}^{H}\left\langle\widetilde{r}_{\ell,h}, \boldsymbol{\pi}_{h}\left(\prod_{j=k+1}^{h}M_{\ell,j+1}P_{j}\boldsymbol{\pi} _{j}\right)M_{\ell,k+1}P_{k}(\boldsymbol{\pi}_{k}-\bar{\boldsymbol{\pi}}_{ \ell,k})(w_{k}^{\bar{\pi}}-\widehat{w}_{\ell,k}^{\bar{\pi}})\right\rangle\] \[=\sum_{k=2}^{H}\langle P_{k}^{\top}M_{\ell,k+1}\widetilde{V}_{ \ell,k+1},(\boldsymbol{\pi}_{k}-\bar{\boldsymbol{\pi}}_{\ell,k})(w_{k}^{\pi}- \widehat{w}_{\ell,k}^{\bar{\pi}})\rangle.\]

It follows that

\[\sum_{h=1}^{H}\langle\widetilde{r}_{\ell,h},\boldsymbol{\pi}_{h} (\delta_{\ell,h}^{\pi}-\widetilde{\delta}_{\ell,h}^{\pi})\rangle+\langle \widetilde{r}_{\ell,h},(\boldsymbol{\pi}_{h}-\bar{\boldsymbol{\pi}}_{\ell,h}) (w_{h}^{\bar{\pi}}-\widehat{w}_{\ell,h}^{\bar{\pi}})\rangle\] \[=\sum_{h=2}^{H}\langle P_{h}^{\top}M_{\ell,h+1}\widetilde{V}_{ \ell,h+1}+\widetilde{r}_{\ell,h},(\boldsymbol{\pi}_{h}-\bar{\boldsymbol{\pi}} _{\ell,h})(w_{\ell,h}^{\bar{\pi}}-\widehat{w}_{\ell,h}^{\bar{\pi}})\rangle+\Delta\]

for some \(\Delta\) satisfying \(|\Delta|\leq 32S^{3/2}H^{2}\epsilon_{\rm unif}^{\ell}\). On \(\mathcal{E}_{\rm good}\) (specifically \(\bar{\mathcal{E}}_{\rm est}^{\ell}\)), we can bound

\[\sum_{h=2}^{H}|\langle P_{h}^{\top}M_{\ell,h+1}\widetilde{V}_{ \ell,h+1}+\widetilde{r}_{\ell,h},(\boldsymbol{\pi}_{h}-\bar{\boldsymbol{\pi}}_{ \ell,h})(w_{\ell,h}^{\bar{\pi}}-\widehat{w}_{\ell,h}^{\bar{\pi}})\rangle|\] \[\leq\sum_{h=2}^{H}\sqrt{\frac{2\mathbb{E}_{s\sim w_{\ell,h}^{\pi} [\langle P_{h}^{\top}M_{\ell,h+1}\widetilde{V}_{\ell,h+1}^{\pi}+\widetilde{r}_{ \ell,h},(\boldsymbol{\pi}_{h}-\bar{\boldsymbol{\pi}}_{\ell,h})e_{s}\rangle^{2 }]}{\bar{n}_{\ell}}}\cdot\log\frac{60H\ell^{2}|\Pi_{\ell}|}{\delta}\] \[\qquad+\frac{2H}{3\bar{n}_{\ell}}\log\frac{60H^{2}\ell^{2}|\Pi_{ \ell}|}{\delta}\]

We can also bound

\[\mathbb{E}_{s\sim w_{\ell,h}^{\pi}}[\langle P_{h}^{\top}M_{\ell,h +1}\widetilde{V}_{\ell,h+1}^{\pi}+\widetilde{r}_{\ell,h},(\boldsymbol{\pi}_{h}- \bar{\boldsymbol{\pi}}_{\ell,h})e_{s}\rangle^{2}]\] \[\leq 2\mathbb{E}_{s\sim w_{\ell,h}^{\pi}}[\langle P_{h}^{\top}V_{h +1}^{\pi}+r_{h},(\boldsymbol{\pi}_{h}-\bar{\boldsymbol{\pi}}_{\ell,h})e_{s} \rangle^{2}]+2H\mathbb{E}_{s\sim w_{\ell,h}^{\pi}}[[|\boldsymbol{\pi}_{h}^{\top }P_{h}^{\top}(M_{\ell,h+1}\widetilde{V}_{\ell,h+1}^{\pi}-V_{h+1}^{\pi})]_{s}|]\] \[\qquad+2H\mathbb{E}_{s\sim w_{\ell,h}^{\pi}}[[|\boldsymbol{\bar {\pi}}_{\ell,h}^{\top}P_{h}^{\top}(M_{\ell,h+1}\widetilde{V}_{\ell,h+1}^{\pi}-V _{h+1}^{\pi})]_{s}|]+4\mathbb{E}_{s\sim w_{\ell,h}^{\pi}}[\sup_{a}|r_{h}(s,a)- \widetilde{r}_{\ell,h}(s,a)|]\]

Furthermore,

\[\mathbb{E}_{s\sim w_{\ell,h}^{\pi}}[[|\boldsymbol{\pi}_{h}^{\top }P_{h}^{\top}(M_{\ell,h+1}\widetilde{V}_{\ell,h+1}^{\pi}-V_{h+1}^{\pi})]_{s}|]\] \[=\sum_{s}[|\boldsymbol{\pi}_{h}^{\top}P_{h}^{\top}(M_{\ell,h+1} \widetilde{V}_{\ell,h+1}^{\pi}-V_{h+1}^{\pi})]_{s}|w_{\ell,h}^{\bar{\pi}}(s)\] \[\leq\sqrt{S}\|(M_{\ell,h+1}\widetilde{V}_{\ell,h+1}^{\pi}-V_{h+1 }^{\pi})^{\top}P_{h}\boldsymbol{\pi}_{h}w_{\ell,h}^{\bar{\pi}}\|_{2}\] \[\leq\sqrt{S}\|(\widetilde{V}_{\ell,h+1}^{\pi}-V_{h+1}^{\pi})^{ \top}P_{h}\boldsymbol{\pi}_{h}w_{\ell,h}^{\bar{\pi}}\|_{2}+\sqrt{S}\|(M_{\ell,h +1}\widetilde{V}_{\ell,h+1}^{\pi}-\widetilde{V}_{\ell,h+1}^{\pi})^{\top}P_{h} \boldsymbol{\pi}_{h}w_{\ell,h}^{\bar{\pi}}\|_{2}\] \[\leq 64S^{2}H^{2}\epsilon_{\rm unif}^{\ell}\]

where the last inequality follows from the definition of \(\widetilde{V}\) and Lemma 17. A similar bound can be shown for \(\mathbb{E}_{s\sim w_{\ell,h}^{\pi}}[[|\boldsymbol{\bar{\pi}}_{\ell,h}^{\top}P_{h} ^{\top}(M_{\ell,h+1}\widetilde{V}_{\ell,h+1}^{\pi}-V_{h+1}^{\pi})]_{s}|]\). In addition, by definition of \(\widetilde{r}_{\ell,h}\) we have

\[\mathbb{E}_{s\sim w_{\ell,h}^{\pi}}[\sup_{a}|r_{h}(s,a)-\widetilde{r}_{\ell,h}(s,a)| ]\leq\mathbb{E}_{s\sim w_{\ell,h}^{\pi}}[\mathbb{I}\{s\not\in\mathcal{S}_{ \ell,h}^{\rm keep}\}]\leq 32S\epsilon_{\rm unif}^{\ell}.\]Thus, we have

\[\sum_{h=2}^{H}|\langle P_{h}^{\top}M_{\ell,h+1}\widetilde{V}_{\ell,h+ 1}+r_{h},(\bm{\pi}_{h}-\bar{\bm{\pi}}_{\ell,h})(w_{\ell,h}^{\bar{\pi}}-\widehat{ w}_{\ell,h}^{\bar{\pi}})\rangle|\] \[\leq\sum_{h=2}^{H}\sqrt{\frac{2\mathbb{E}_{s\sim w_{\ell,h}^{*}} [\langle P_{h}^{\top}M_{\ell,h+1}\widetilde{V}_{\ell,h+1}^{\pi}+r_{h},(\bm{\pi }_{h}-\bar{\bm{\pi}}_{\ell,h})e_{s}\rangle^{2}]}{\bar{n}_{\ell}}\cdot\log\frac{60 H\ell^{2}|\Pi_{\ell}|}{\delta}}\] \[\qquad\quad+\frac{2H}{3\bar{n}_{\ell}}\log\frac{60H^{2}\ell^{2}| \Pi_{\ell}|}{\delta}\] \[\leq\sum_{h=2}^{H}\sqrt{\frac{4U_{h}(\pi,\bar{\pi}_{\ell})+384S^{2 }H^{3}\epsilon_{\rm unif}^{\ell}}{\bar{n}_{\ell}}\cdot\log\frac{60H\ell^{2}| \Pi_{\ell}|}{\delta}}+\frac{2H}{3\bar{n}_{\ell}}\log\frac{60H^{2}\ell^{2}|\Pi _{\ell}|}{\delta}.\]

By Lemma 23 and Jensen's inequality, this can be further bounded as

\[\leq\sum_{h=2}^{H}c\sqrt{\frac{\widehat{U}_{\ell-1,h}(\pi,\bar{ \pi}_{\ell})+S^{3/2}H^{3}\sqrt{A\log\frac{24S^{2}AH\ell^{2}}{\delta}}\cdot \epsilon_{\ell}^{1/3}+S^{2}H^{3}\epsilon_{\rm unif}^{\ell}}{\bar{n}_{\ell}} \cdot\log\frac{60H\ell^{2}|\Pi_{\ell}|}{\delta}}\] \[\qquad+\frac{2H}{3\bar{n}_{\ell}}\log\frac{60H^{2}\ell^{2}|\Pi_{ \ell}|}{\delta}\] \[\leq c\sqrt{\frac{H\widehat{U}_{\ell-1}(\pi,\bar{\pi}_{\ell})+S^{3/2} H^{4}\sqrt{A\log\frac{24S^{2}AH\ell^{2}}{\delta}}\cdot\epsilon_{\ell}^{1/3}+S^{2} H^{4}\epsilon_{\rm unif}^{\ell}}{\bar{n}_{\ell}}\cdot\log\frac{60H\ell^{2}|\Pi_{ \ell}|}{\delta}}\] \[\qquad+\frac{2H}{3\bar{n}_{\ell}}\log\frac{60H^{2}\ell^{2}|\Pi_{ \ell}|}{\delta}.\]

The result then follows from this, our choice of \(\bar{n}_{\ell}\) and \(\epsilon_{\rm unif}^{\ell}\), and the bound on \(\Delta\) above.

**Lemma 25**.: _On \(\mathcal{E}_{\rm good}\), we can bound_

\[\frac{\inf_{\pi_{\rm exp}}\max_{\pi\in\Pi_{\ell}}\|M_{\ell,h}(( \bm{\pi}_{h}-\bar{\bm{\pi}}_{\ell,h})\widehat{w}_{\ell,h}^{\bar{\pi}}+\bm{\pi} _{h}\widehat{\delta}_{\ell,h}^{\bar{\pi}})\|_{\Lambda_{h}(\pi_{\rm exp})^{-1}} ^{2}}{\epsilon_{\rm exp}^{\ell}}\] \[\leq\frac{\inf_{\pi_{\rm exp}}\max_{\pi\in\Pi_{\ell}}4\|\bar{\bm{ \pi}}_{\ell,h}w_{\ell,h}^{\bar{\pi}}-\bm{\pi}_{h}w_{h}^{\pi}\|_{\Lambda_{h}( \pi_{\rm exp})^{-1}}^{2}}{\epsilon_{\rm exp}^{\ell}}\] \[\quad+\frac{(8S^{2}A+32S^{3}AH^{2})\epsilon_{\ell}^{5/3}+2S^{2}AH \beta\epsilon_{\rm exp}^{\ell}+4096S^{3}AH^{2}(\epsilon_{\rm unif}^{\ell})^{2 }}{\epsilon_{\rm unif}^{\ell}\epsilon_{\rm exp}^{\ell}}.\]

Proof.: We can bound:

\[\inf_{\pi_{\rm exp}}\max_{\pi\in\Pi_{\ell}}\|M_{\ell,h}((\bm{\pi }_{h}-\bar{\bm{\pi}}_{\ell,h})\widehat{w}_{\ell,h}^{\bar{\pi}}+\bm{\pi}_{h} \widehat{\delta}_{\ell,h}^{\bar{\pi}})\|_{\Lambda_{h}(\pi_{\rm exp})^{-1}}^{2}\] \[\leq\inf_{\pi_{\rm exp}}\max_{\pi\in\Pi_{\ell}}4\|M_{\ell,h}((\bm{ \pi}_{h}-\bar{\bm{\pi}}_{\ell,h})w_{\ell,h}^{\bar{\pi}}+\bm{\pi}_{h}\delta_{ \ell,h}^{\pi})\|_{\Lambda_{h}(\pi_{\rm exp})^{-1}}^{2}\] \[\qquad+\inf_{\pi_{\rm exp}}\max_{\pi\in\Pi_{\ell}}\Big{[}8\|M_{\ell,h}(\bm{\pi}_{h}-\bar{\bm{\pi}}_{\ell,h})(w_{\ell,h}^{\bar{\pi}}-\widehat{w}_{ \ell,h}^{\bar{\pi}})\|_{\Lambda_{h}(\pi_{\rm exp}^{\prime})^{-1}}^{2}\] \[\qquad+8\|M_{\ell,h}\bm{\pi}_{h}(\delta_{\ell,h}^{\bar{\pi}}- \widehat{\delta}_{\ell,h}^{\bar{\pi}})\|_{\Lambda_{h}(\pi_{\rm exp}^{\prime})^{-1} }^{2}\Big{]}.\]We can write

\[\|M_{\ell,h}(\bm{\pi}_{h}-\bar{\bm{\pi}}_{\ell,h})(w_{\ell,h}^{\bar{ \pi}}-\widehat{w}_{\ell,h}^{\bar{\pi}})\|_{\Lambda_{h}(\pi_{\mathrm{exp}}^{\prime })^{-1}}^{2}\] \[=\sum_{s,a}\frac{(\bm{\pi}_{h}(a\mid s)-\bar{\bm{\pi}}_{\ell,h}(a \mid s))^{2}(w_{\ell,h}^{\bm{\pi}}(s)-\widehat{w}_{\ell,h}^{\bar{\pi}}(s))^{2} }{[\Lambda_{h}(\pi_{\mathrm{exp}}^{\prime})]_{sa,sa}}\cdot\mathbb{I}\{(s,a)\in \mathcal{S}_{\ell,h}^{\mathrm{keep}}\}\] \[\leq\sum_{s,a}\frac{(w_{\ell,h}^{\bar{\pi}}(s)-\widehat{w}_{\ell,h}^{\bar{\pi}}(s))^{2}}{[\Lambda_{h}(\pi_{\mathrm{exp}}^{\prime})]_{sa,sa}} \cdot\mathbb{I}\{(s,a)\in\mathcal{S}_{\ell,h}^{\mathrm{keep}}\}.\]

On \(\mathcal{E}_{\mathrm{good}}\), for each \((s,a)\in\mathcal{S}_{\ell,h}^{\mathrm{keep}}\) we have \(W_{h}^{\star}(s)\geq\epsilon_{\mathrm{unif}}^{\ell}\). Let \(\pi^{sh}\) denote the policy which achieves \(w_{h}^{\star^{sh}}(s)=W_{h}^{\star}(s)\), and then plays actions uniformly at random at \((s,h)\). Let \(\pi_{\mathrm{exp}}^{\prime}=\mathrm{unif}(\{\pi^{sh}\}_{s})\). Then we have \([\Lambda_{h}(\pi_{\mathrm{exp}}^{\prime})]_{sa,sa}\geq W_{h}^{\star}(s)/SA\geq \epsilon_{\mathrm{unif}}^{\ell}/SA\) for each \((s,a)\in\mathcal{S}_{\ell,h}^{\mathrm{keep}}\), so we can bound the above as

\[\leq\frac{SA}{\epsilon_{\mathrm{unif}}^{\ell}}\sum_{s,a}(w_{\ell,h}^{\bar{\pi} }(s)-\widehat{w}_{\ell,h}^{\bar{\pi}}(s))^{2}=\frac{SA}{\epsilon_{\mathrm{ unif}}^{\ell}}\|w_{\ell,h}^{\bar{\pi}}-\widehat{w}_{\ell,h}^{\bar{\pi}}\|_{2}^{2} \leq\frac{8S^{2}A\epsilon_{\ell}^{5/3}}{\epsilon_{\mathrm{unif}}^{\ell}},\]

where the last inequality follows from Lemma 19.

We can obtain a bound on \(\|M_{\ell,h}\bm{\pi}_{h}(\delta_{\ell,h}^{\pi}-\widehat{\delta}_{\ell,h}^{\pi })\|_{\Lambda_{h}(\pi_{\mathrm{exp}}^{\prime})^{-1}}^{2}\) using a similar argument but now applying Lemma 18 to get that:

\[\|M_{\ell,h}\bm{\pi}_{h}(\delta_{\ell,h}^{\pi}-\widehat{\delta}_{\ell,h}^{\pi })\|_{\Lambda_{h}(\pi_{\mathrm{exp}}^{\prime})^{-1}}^{2}\leq\frac{2S^{2}AH \beta_{\ell}\epsilon_{\mathrm{exp}}^{\ell}}{\epsilon_{\mathrm{unif}}^{\ell}}+ \frac{32S^{3}AH^{2}\epsilon_{\ell}^{5/3}}{\epsilon_{\mathrm{unif}}^{\ell}}+4096 S^{3}AH^{2}\epsilon_{\mathrm{unif}}^{\ell}.\]

Finally, note that

\[\|M_{\ell,h}((\bm{\pi}_{h}-\bar{\bm{\pi}}_{\ell,h})w_{\ell,h}^{ \bar{\pi}}+\bm{\pi}_{h}\delta_{\ell,h}^{\pi})\|_{\Lambda_{h}(\pi_{\mathrm{exp} })^{-1}}^{2} =\|M_{\ell,h}(\bar{\bm{\pi}}_{\ell,h}w_{\ell,h}^{\bar{\pi}}+\bm{ \pi}_{h}w_{h}^{\pi})\|_{\Lambda_{h}(\pi_{\mathrm{exp}})^{-1}}^{2}\] \[\leq\|\bar{\bm{\pi}}_{\ell,h}w_{\ell,h}^{\bar{\pi}}-\bm{\pi}_{h}w_ {h}^{\pi}\|_{\Lambda_{h}(\pi_{\mathrm{exp}})^{-1}}^{2}\]

where the equality holds by definition, and the inequality by simply manipulations. Combining these bounds gives the result. 

### Correctness and Sample Complexity

**Lemma 26**.: _On the event \(\mathcal{E}_{\mathrm{good}}\), for all \(\pi\in\Pi_{\ell+1}\), we have \(V_{0}^{\star}(\Pi)-V_{0}^{\pi}\leq 16\epsilon_{\ell}\), and \(\pi^{\star}\in\Pi_{\ell}\)._

Proof.: Recall \(D_{\bar{\pi}_{\ell}}(\pi)=V_{0}^{\pi}-V_{0}^{\pi_{\ell}}\). For \(\pi\in\Pi_{\ell}\), we have

\[|\widehat{D}_{\bar{\pi}_{\ell}}(\pi)-D_{\bar{\pi}_{\ell}}(\pi)|\] \[=\left|\sum_{h=1}^{H}\left[\langle\widehat{r}_{\ell,h},\bm{\pi}_{h }\widehat{\delta}_{\overline{\ell},h}^{\overline{\pi}}+(\bm{\pi}_{h}-\bar{\bm {\pi}}_{\ell,h})\widehat{w}_{\overline{\ell},h}^{\overline{\pi}}\rangle- \langle r_{h},\bm{\pi}_{h}\delta_{\overline{\ell},h}^{\overline{\pi}}+(\bm{\pi} _{h}-\bar{\bm{\pi}}_{\ell,h})w_{\overline{\ell},h}^{\overline{\pi}}\rangle \right]\right|\] \[\leq\underbrace{\sum_{h=1}^{H}|\langle\widehat{r}_{\ell,h}-\widetilde {r}_{\ell,h},\bm{\pi}_{h}\widehat{\delta}_{\overline{\ell},h}^{\overline{\pi}}+( \bm{\pi}_{h}-\bar{\bm{\pi}}_{\ell,h})\widehat{w}_{\overline{\ell},h}^{\overline{ \pi}}\rangle|}_{(a)}+\underbrace{\sum_{h=1}^{H}|\langle\widetilde{r}_{\ell,h}, \bm{\pi}_{h}(\widetilde{\delta}_{\overline{\ell},h}^{\overline{\pi}}-\widehat{ \delta}_{\overline{\ell},h}^{\overline{\pi}})\rangle|}_{(b)}\] \[\quad\quad+\underbrace{\left|\sum_{h=1}^{H}\langle\widetilde{r}_{ \ell,h},\bm{\pi}_{h}(\delta_{\overline{\ell},h}^{\overline{\pi}}-\widetilde{ \delta}_{\overline{\ell},h}^{\overline{\pi}})\rangle+\langle r_{h},(\bm{\pi}_{h}- \bar{\bm{\pi}}_{\ell,h})(w_{\overline{\ell},h}^{\overline{\pi}}-\widehat{w}_{ \overline{\ell},h}^{\overline{\pi}})\rangle\right|}_{(c)}\] \[\quad\quad+\underbrace{\sum_{h=1}^{H}|\langle\widetilde{r}_{\ell,h}- r_{h},\bm{\pi}_{h}\delta_{\ell,h}^{\overline{\pi}}+(\bm{\pi}_{h}-\bar{\bm{\pi}}_{\ell,h})w_{ \ell,h}^{\overline{\pi}}\rangle|}_{(d)}.\]By Lemma 21, on \(\mathcal{E}_{\mathrm{good}}\) we have \((a)\leq\epsilon_{\ell}\) and \((b)\leq\epsilon_{\ell}\), and by Lemma 24, \((c)\leq\epsilon_{\ell}\). To bound \((d)\), we note that \(\bm{\pi}_{h}\delta_{\ell,h}^{\pi}+(\bm{\pi}_{h}-\bar{\bm{\pi}}_{\ell,h})w_{\ell,h}^{\bar{\pi}}=\bm{\pi}_{h}w_{h}^{\pi}-\bar{\bm{\pi}}_{\ell,h}w_{\ell,h}^{\bar{ \pi}}\), and so, on \(\mathcal{E}_{\mathrm{good}}\) and by definition of \(\widetilde{r}_{\ell,h}\),

\[(d)\leq\sum_{h=1}^{H}\sum_{s\not\in\mathcal{E}_{\ell,h}^{\mathrm{ sheep}}}(w_{h}^{\pi}(s)+w_{\ell,h}^{\bar{\pi}}(s))\leq 64HS\epsilon_{\mathrm{ unif}}^{\ell}\leq\epsilon_{\ell}.\]

Note that we only eliminate policy \(\pi\in\Pi_{\ell}\) at round \(\ell\) if \(\max_{\pi^{\prime}}\widehat{D}_{\bar{\pi}_{\ell}}(\pi^{\prime})-\widehat{D}_{ \bar{\pi}_{\ell}}(\pi)>8\epsilon_{\ell}\). Assume that \(\pi^{\star}\in\Pi_{\ell}\). By what we have just shown, if policy \(\pi\) is eliminated, we then have

\[8\epsilon_{\ell}<\max_{\pi^{\prime}\in\Pi_{\ell}}D_{\bar{\pi}_{\ell}}(\pi^{ \prime})-D_{\bar{\pi}_{\ell}}(\pi)+8\epsilon_{\ell}=V_{0}^{\star}-V_{0}^{\pi} +8\epsilon_{\ell}\implies V_{0}^{\pi}<V_{0}^{\star}.\]

It follows that \(\pi^{\star}\) will not be eliminated at round \(\ell\), as long as \(\pi^{\star}\in\Pi_{\ell}\). By a simple inductive argument, since \(\pi^{\star}\in\Pi_{0}\), it follows that on \(\mathcal{E}_{\mathrm{good}}\), \(\pi^{\star}\in\Pi_{\ell}\) for all \(\ell\).

Furthermore, for each \(\pi\in\Pi_{\ell+1}\), we have \(\max_{\pi^{\prime}}\widehat{D}_{\bar{\pi}_{\ell}}(\pi^{\prime})-\widehat{D}_{ \bar{\pi}_{\ell}}(\pi)\leq 8\epsilon_{\ell}\). Which, again by what we have just shown, implies that

\[8\epsilon_{\ell}\geq\max_{\pi^{\prime}\in\Pi_{\ell}}D_{\bar{\pi}_{\ell}}(\pi^ {\prime})-D_{\bar{\pi}_{\ell}}(\pi)-8\epsilon_{\ell}=V_{0}^{\star}-V_{0}^{\pi} -8\epsilon_{\ell}\implies V_{0}^{\star}-V_{0}^{\pi}\leq 16\epsilon_{ \ell}.\]

**Theorem 1**.: _There exists an algorithm (Algorithm 1) which, with probability at least \(1-2\delta\), finds an \(\epsilon\)-optimal policy and terminates after collecting at most_

\[\sum_{h=1}^{H}\inf_{\kappa_{\mathrm{exp}}}\max_{\pi\in\Pi}\frac{H^{4}\|\phi_{h }^{\star}-\phi_{h}^{\pi}\|_{\Lambda_{h}(\pi_{\mathrm{exp}})^{-1}}^{2}}{\max\{ \epsilon^{2},\Delta(\pi)^{2}\}}\cdot\iota\beta^{2}+\max_{\pi\in\Pi}\frac{HU(\pi,\pi^{\star})}{\max\{\epsilon^{2},\Delta(\pi)^{2}\}}\log\frac{H|\Pi|\iota}{ \delta}+\frac{C_{\mathrm{poly}}}{\max\{\epsilon^{\frac{5}{3}},\Delta_{\min}^{ \frac{5}{3}}\}}\]

_episodes, for \(C_{\mathrm{poly}}:=\mathrm{poly}(S,A,H,\log 1/\delta,\iota,\log|\Pi|),\beta:=C\sqrt{\log( \frac{SH|\Pi|}{\delta}\cdot\frac{1}{\Delta_{\min}\vee\epsilon})}\) and \(\iota:=\log\frac{1}{\Delta_{\min}\vee\epsilon}\)._

Proof.: First, by Lemma 20, we have that \(\mathbb{P}[\mathcal{E}_{\mathrm{good}}]\geq 1-2\delta\). For the remainder of the proof we assume we are on \(\mathcal{E}_{\mathrm{good}}\).

By Lemma 26, we have that on \(\mathcal{E}_{\mathrm{good}}\), for every \(\pi\in\Pi_{\ell+1}\), \(V_{0}^{\star}-V_{0}^{\pi}\leq 16\epsilon_{\ell}\), and that \(\pi^{\star}\in\Pi_{\ell}\) for all \(\ell\). It follows that, since we run for \(\ell_{\epsilon}=\lceil\log_{2}16/\epsilon\rceil\) epochs, when we terminate each policy \(\pi\in\Pi_{\ell_{\epsilon}}\) satisfies \(V_{0}^{\star}-V_{0}^{\pi}\leq 16\epsilon_{\ell_{\epsilon}}=16\cdot 2^{-\ell_{ \epsilon}}\leq\epsilon\). Furthermore, if we terminate early on Line 20, then we know that \(|\Pi_{\ell+1}|=1\), and since \(\pi^{\star}\in\Pi_{\ell+1}\), we have that the algorithm returns \(\pi^{\star}\). Thus, the policy returned by Algorithm 2 is always \(\epsilon\)-optimal.

It therefore remains to bound the sample complexity of Algorithm 2. At round \(\ell\) of Algorithm 2, we collect \(\bar{n}_{\ell}\) samples plus the number of samples collected from OptCov. On \(\mathcal{E}_{\mathrm{good}}\), we have that the number of samples collected by OptCov at round \(\ell\) step \(h\) is bounded by

\[C\cdot\frac{\inf_{\pi_{\exp}}\max_{\pi\in\Pi_{\ell}}\|M_{h}^{\ell}(( \boldsymbol{\pi}_{h}-\boldsymbol{\bar{\pi}}_{\ell,h})\widehat{w}_{\ell,h}^{ \pm}+\boldsymbol{\pi}_{h}\widehat{\delta}_{\ell,h}^{\tau})\|_{\Lambda_{h}(\pi_{ \exp})^{-1}}^{2}}{\epsilon_{\exp}^{\ell}}\] \[\qquad+\frac{C_{\text{fw}}^{\ell}}{(\epsilon_{\exp}^{\ell})^{4/5} }+\frac{C_{\text{fw}}^{\ell}}{\epsilon_{\text{unif}}^{\ell}}+\log(C_{\text{fw} }^{\ell})\cdot K_{\text{unif}}^{\ell}\] \[\overset{(a)}{\leq}C\cdot\frac{\inf_{\pi_{\exp}}\max_{\pi\in\Pi_{ \ell}}\|\boldsymbol{\bar{\pi}}_{\ell,h}w_{\ell,h}^{\mp}-\boldsymbol{\pi}_{h}w _{h}^{\mp}\|_{\Lambda_{h}(\pi_{\exp})^{-1}}^{2}}{\epsilon_{\exp}^{\ell}}+ \frac{C_{\text{fw}}^{\ell}}{(\epsilon_{\exp}^{\ell})^{4/5}}+\frac{C_{\text{fw }}^{\ell}}{\epsilon_{\text{unif}}^{\ell}}+\log(C_{\text{fw}}^{\ell})\cdot K_{ \text{unif}}^{\ell}\] \[\qquad\qquad\qquad\qquad+\frac{(8S^{2}A+32S^{3}AH^{2})\epsilon_{ \ell}^{5/3}+2S^{2}AH\beta_{\ell}\epsilon_{\exp}^{\ell}+4096S^{3}AH^{2}(\epsilon _{\text{unif}}^{\ell})^{2}}{\epsilon_{\text{unif}}^{\ell}\epsilon_{\exp}^{ \ell}}\] \[\overset{(b)}{\leq}C\cdot\frac{\inf_{\pi_{\exp}}\max_{\pi\in\Pi_{ \ell}}\|\boldsymbol{\bar{\pi}}_{\ell,h}w_{\ell,h}^{\mp}-\boldsymbol{\pi}_{h}w _{h}^{\mp}\|_{\Lambda_{h}(\pi_{\exp})^{-1}}^{2}}{\epsilon_{\ell}^{2}}\cdot H^{ 4}\beta_{\ell}^{2}+\frac{C_{\text{poly}}^{\ell}}{\epsilon_{\ell}^{5/3}}\] \[\overset{(c)}{\leq}C\cdot\frac{\inf_{\pi_{\exp}}\max_{\pi\in\Pi_{ \ell}}\|\boldsymbol{\pi}_{h}^{\star}w_{h}^{\pi^{\star}}-\boldsymbol{\pi}_{h}w _{h}^{\mp}\|_{\Lambda_{h}(\pi_{\exp})^{-1}}^{2}}{\epsilon_{\ell}^{2}}\cdot H^{ 4}\beta_{\ell}^{2}+\frac{C_{\text{poly}}^{\ell}}{\epsilon_{\ell}^{5/3}}\] \[\overset{(d)}{\leq}C\cdot\inf_{\pi_{\exp}}\max_{\pi\in\Pi}\frac{ \|\boldsymbol{\pi}_{h}^{\star}w_{h}^{\pi^{\star}}-\boldsymbol{\pi}_{h}w_{h}^{ \pi}\|_{\Lambda_{h}(\pi_{\exp})^{-1}}^{2}}{\max\{\epsilon_{\ell}^{2},\Delta( \pi)^{2}\}}\cdot H^{4}\beta_{\ell}^{2}+\frac{C_{\text{poly}}^{\ell}}{\epsilon_ {\ell}^{5/3}}\]

where the initial bound holds from Lemma 14, the \((a)\) follows from Lemma 25, and \((b)\) follows plugging in our choice of \(\epsilon_{\text{unif}}^{\ell}\) and \(\epsilon_{\exp}^{\ell}\), and with \(C_{\text{poly}}^{\ell}=\text{poly}(S,A,H,\log\ell/\delta,\log 1/\epsilon,\log| \Pi|)\), \((c)\) holds by the triangle inequality and since \(\bar{\pi}_{\ell}\in\Pi_{\ell}\), and \((d)\) holds because, for all \(\pi\in\Pi_{\ell}\), we have \(\Delta(\pi)\leq 32\epsilon_{\ell}\). Furthermore, we can bound \(\bar{n}_{\ell}\) as

\[\bar{n}_{\ell} =\min_{\pi\in\Pi_{\ell}}\max_{\pi\in\Pi_{\ell}}c\cdot\frac{H\widehat {U}_{\ell-1}(\pi,\bar{\pi})+H^{4}S^{3/2}\sqrt{A}\log\frac{SAH\ell^{2}}{\delta} \cdot\epsilon_{\ell}^{1/3}+S^{2}H^{4}\epsilon_{\text{unif}}^{\ell}}{\epsilon_{ \ell}^{2}}\cdot\log\frac{60H\ell^{2}|\Pi_{\ell}|}{\delta}\] \[\overset{(a)}{\leq}\min_{\pi\in\Pi_{\ell}}\max_{\pi\in\Pi_{\ell}}c \cdot\frac{HU(\pi,\bar{\pi})+H^{4}S^{3/2}\sqrt{A}\log\frac{SAH\ell^{2}}{\delta} \cdot\epsilon_{\ell}^{1/3}+S^{2}H^{4}\epsilon_{\text{unif}}^{\ell}}{\epsilon_{ \ell}^{2}}\cdot\log\frac{60H\ell^{2}|\Pi_{\ell}|}{\delta}\] \[\overset{(b)}{\leq}\max_{\pi\in\Pi}c\cdot\frac{HU(\pi,\pi^{ \star})}{\max\{\epsilon_{\ell}^{2},\Delta(\pi)^{2}\}}\cdot\log\frac{60H\ell^{2 }|\Pi_{\ell}|}{\delta}+\frac{C_{\text{poly}}^{\ell}}{\epsilon_{\ell}^{5/3}}\]

where \((a)\) follows from Lemma 23, and \((b)\) since \(\pi^{\star}\in\Pi_{\ell}\), and by a similar argument as above.

Thus, if we run for a total of \(L\) rounds, the sample complexity is bounded as

\[\sum_{\ell=1}^{L}\left(C\cdot\sum_{h=1}^{H}\inf_{\pi_{\exp}}\max_{ \pi\in\Pi}\frac{\|\boldsymbol{\pi}_{h}^{\star}w_{h}^{\star}-\boldsymbol{\pi}_{h}w _{h}^{\mp}\|_{\Lambda_{h}(\pi_{\exp})^{-1}}^{2}}{\max\{\epsilon_{\ell}^{2},\Delta( \pi)^{2}\}}\cdot H^{4}\beta_{\ell}^{2}\right.\] \[\qquad+\max_{\pi\in\Pi}c\cdot\frac{HU(\pi,\pi^{\star})}{\max\{ \epsilon_{\ell}^{2},\Delta(\pi)^{2}\}}\cdot\log\frac{60H\ell^{2}|\Pi_{\ell}|}{ \delta}\right)+\frac{LC_{\text{poly}}^{\ell}}{\epsilon_{L}^{5/3}}.\]

By construction, we have that \(L\leq\lceil\log_{2}16/\epsilon\rceil\). However, we terminate early if \(|\Pi_{\ell+1}|=1\), and since each \(\pi\in\Pi_{\ell+1}\) satisfies \(\Delta(\pi)\leq\epsilon_{\ell}\), it follows that we will have \(|\Pi_{\ell+1}|=1\) once \(\epsilon_{\ell}<\Delta_{\min}\), which will occur for \(\ell\geq\lceil\log_{2}\frac{1}{\Delta_{\min}}\rceil+1\). Thus, we can bound

\[L\leq\min\{\lceil\log_{2}16/\epsilon\rceil,\lceil\log_{2}1/\Delta_{\min}\rceil+1\},\]

and so for all \(\epsilon_{\ell},\ell\leq L\), we have \(\epsilon_{\ell}\geq c\cdot\max\{\epsilon,\Delta_{\min}\}\). Plugging this into the above gives the final complexity.

## Appendix D Tabular Contextual Bandits: Upper Bound

Setting and notation.We study stochastic tabular contextual bandits, denoted by the tuple \((\mathcal{C},\mathcal{A},\mu^{\star},\nu)\). At each episode, a context \(c\sim\mu^{\star}\) arrives, the agent chooses an action \(a\in\mathcal{A}\)and receives reward \(r(c,a)\sim\nu(c,a)\) in \(\mathbb{R}\). Note that this is a special case of the Tabular MDP when \(H=1\). In this setting, we use the terminology "contexts" instead of "states" to highlight that the agent has no impact on these. The vector \(\mu^{\star}\) plays the same role as the state visitation vectors \(w_{h}^{\pi}\) previously, except this is now policy-independent. The notation for policy matrix \(\bm{\pi}\), values \(V^{\pi}\), features \(\phi^{\pi}(c,a)\) are inherited directly from the general case.

Define \(\theta^{\star}\in R^{|\mathcal{C}|A}\) as the vector of reward means, so that \([\theta^{\star}]_{(c,a)}=\mathbb{E}_{\nu}[r(c,a)]\). Then, we can write the value of \(\pi\) as:

\[\mathbb{E}_{\nu,\mu^{\star}}[r(c,\pi(c))]=\sum_{c,a}\theta^{\star}_{c,a}[\mu^{ \star}]_{c}[\pi(c)]_{a}=(\theta^{\star})^{\top}\bm{\pi}\mu^{\star}\]

For any \((\theta,\mu)\) define \(\mathsf{OPT}(\theta,\mu):=\arg\max_{\pi\in\Pi}\theta^{\top}\bm{\pi}\mu\), where \(\theta\) is any hypothetical vector of reward-means and \(\mu\in\Delta_{|\mathcal{C}|}\) is a hypothetical context distribution.

Recall that we use \(\bm{\pi}\in\mathbb{R}^{|\mathcal{C}|A\times|\mathcal{C}|}\) to refer to the policy matrix. The vector \(\bm{\pi}\mu\in\mathbb{R}^{|\mathcal{C}|A}\) contains context-action visitations for policy \(\pi\) under context distribution \(\mu\). Define function \(G(\mu,\pi)=\mathbb{E}_{\mu,\pi}[(\bm{\pi}\mu)(\bm{\pi}\mu)^{\top}]\) which returns the expected covariance matrix of policy \(\pi\) under context distribution \(\mu\). For shorthand, we refer to \(\hat{A}(\pi)=G(\widehat{\mu}_{\ell},\pi_{\exp})\) and \(A(\pi)=G(\mu^{\star},\pi_{\exp})\) for any \(\pi\).

**Lemma 27**.: _Define the experimental design objective_

\[F(\pi_{\exp},\mu,\pi,\pi^{\prime})=\|(\bm{\pi}^{\prime}-\bm{\pi})\mu\|_{G(\mu, \pi_{\exp})^{-1}}^{2}.\]

_Then, for any \(\mu\in\Delta_{\mathcal{C}}\),_

\[\min_{\pi_{\exp}}\max_{\pi,\pi^{\prime}\in\Pi_{\ell}}F(\pi_{\exp},\mu,\pi,\pi^ {\prime})=\max_{\pi,\pi^{\prime}\in\Pi_{\ell}}\min_{\pi_{\exp}}F(\pi_{\exp}, \mu,\pi,\pi^{\prime})\]

Proof.: We can rewrite the maximization problem to be over the simplex \(\Delta_{\Pi_{\ell}\times\Pi_{\ell}}\) instead:

\[\min_{\pi_{\exp}}\max_{\lambda\in\Delta_{\Pi_{\ell}\times\Pi_{\ell}}}\sum_{\pi,\pi^{\prime}\in\Pi_{\ell}\times\Pi_{\ell}}\lambda_{\pi,\pi^{\prime}}F(\pi_{ \exp},\mu,\pi,\pi^{\prime})\] (D.1)

This does not change the objective value. To see this, note that for any selection \((\pi_{1},\pi_{2})\) in the original problem, the same objective value can be obtained by setting \(\lambda=e_{\pi_{1},\pi_{2}}\); hence, the modification to the optimization cannot reduce the value. Further if \(F(\pi_{\exp},\mu,\pi,\pi^{\prime})\) is maximized by \((\pi_{1},\pi_{2})\), setting \(\lambda\) as anything other than \(e_{\pi_{1},\pi_{2}}\) cannot increase the objective value.

Now, note that both the minimization and maximization problems are over simplices, which are compact and convex sets. The objective is linear in the maximization variable, and hence concave. The objective can be rewritten as

\[\sum_{c\in\mathcal{S}}\sum_{a\in\mathcal{A}}\frac{(\bm{\pi}-\bm{\pi}^{\prime} )^{\top}e_{a,c}e_{a,c}^{\top}(\bm{\pi}-\bm{\pi}^{\prime})}{p_{c,a}}.\]

Here, \(p_{c,a}\) as the probability that \(\pi_{\exp}\) plays action \(a\), given that we are in context \(c\). From this representation, we can clearly see that the objective is convex in each \(p_{c,a}\). Hence, since we are optimizing over finite-dimensional spaces (\(|\mathcal{A}|\) and \(|\mathcal{C}|\) are finite), Von Neumann's minimax theorem applies and the proof is complete. 

**Lemma 28**.: _For the contextual bandit problem, define the experimental design objective_

\[F(\pi_{\exp},\mu,\pi,\pi^{\prime})=\|(\bm{\pi}^{\prime}-\bm{\pi})\mu\|_{G(\mu, \pi_{\exp})^{-1}}^{2}.\]

_Then, for any \(\mu\) and assuming that all policies in \(\Pi_{\ell}\) are deterministic, we have:_

\[\min_{\pi_{\exp}}\max_{\pi,\pi^{\prime}\in\Pi_{\ell}}F(\pi_{\exp},\mu,\pi,\pi^ {\prime})=\max_{\pi,\pi^{\prime}\in\Pi_{\ell}}\mathbb{E}_{c\sim\mu}[4\mathbb{I} [\pi(c)\neq\pi^{\prime}(c)]],\] (D.2)Proof.: Below, we refer to \(p_{c,a}\) as the probability that \(\pi_{\mathrm{exp}}\) plays action \(a\), given that we are in context \(c\). We have:

\[\min_{\pi_{\mathrm{exp}}}\max_{\pi,\pi^{\prime}\in\Pi_{\ell}}\|( \boldsymbol{\pi}^{\prime}-\boldsymbol{\pi})\mu\|_{G(\mu,\pi_{\mathrm{exp}})^{- 1}}^{2}\] \[=\max_{\pi,\pi^{\prime}\in\Pi_{\ell}}\min_{\pi_{\mathrm{exp}}}\|( \boldsymbol{\pi}^{\prime}-\boldsymbol{\pi})\mu\|_{G(\mu,\pi_{\mathrm{exp}})^{- 1}}^{2}\] \[=\max_{\pi,\pi^{\prime}\in\Pi_{\ell}}\min_{p_{1}\dots p_{c}\in \mathcal{A}_{\Delta}}\sum_{a,c}\mu_{c}^{2}\frac{(\boldsymbol{\pi}-\boldsymbol{ \pi}^{\prime})^{\top}e_{a,c}e_{a,c}^{\top}(\boldsymbol{\pi}-\boldsymbol{\pi}^ {\prime})}{\mu_{c}p_{c,a}}\] \[=\max_{\pi,\pi^{\prime}\in\Pi_{\ell}}\sum_{c}\mu_{c}\min_{p_{c}} \sum_{a\in\mathcal{A}}\frac{(\boldsymbol{\pi}-\boldsymbol{\pi}^{\prime})^{ \top}e_{a,c}e_{a,c}^{\top}(\boldsymbol{\pi}-\boldsymbol{\pi}^{\prime})}{p_{c,a }}\] \[=\max_{\pi,\pi^{\prime}\in\Pi_{\ell}}\sum_{c}\mu_{c}\left(\sum_{a \in\mathcal{A}}\sqrt{(\boldsymbol{\pi}-\boldsymbol{\pi}^{\prime})^{\top}e_{a, c}e_{a,c}^{\top}(\boldsymbol{\pi}-\boldsymbol{\pi}^{\prime})}\right)^{2}.\]

Here the first equality follows from Lemma 27, and the last from Lemma D.6 of [30].

We have assumed that the policies in \(\Pi_{\ell}\) are deterministic. Hence, the only two actions in the summation over \(\mathcal{A}\) above that are relevant are \(\pi(c)\) and \(\pi^{\prime}(c)\). For all other \(a\in\mathcal{A}\), the term in the square root evaluates to \(0\). If \(\pi(c)=\pi^{\prime}(c)\), then the entire summation over \(\mathcal{A}\) evaluates to \(0\); else, the terms indexed by \(\pi(c)\) and \(\pi^{\prime}(c)\) are both 1, and the summation evalutes to \(2\). Hence, we can simplify the expression to exactly the form of Equation (D.2) from the lemma statement, and the proof is complete. 

**Lemma 29**.: _For the contextual bandits problem, we have that_

\[\max_{\pi\in\Pi}\mathbb{E}_{c\sim\mu^{\star}}[\mathbb{E}_{\nu^{\star}}[(r(c, \pi(c))-r(c,\pi^{\star}(c)))^{2}[c]]\leq\inf_{\pi_{\mathrm{exp}}}\max_{\pi\in \Pi}\|\phi^{\star}-\phi^{\pi}\|_{\Lambda(\pi_{\mathrm{exp}})^{-1}}^{2}\]

Proof.: Observe that \(r(c,\pi(c))-r(c,\pi^{\star}(c))=0\) if \(\pi(c)=\pi^{\star}(c)\); else, \(|r(c,\pi(c))-r(c,\pi^{\star}(c))|\leq 2\). Then, it follows that

\[\max_{\pi\in\Pi}\mathbb{E}_{c\sim\mu^{\star}}[\mathbb{E}_{\nu^{ \star}}[(r(c,\pi(c))-r(c,\pi^{\star}(c)))^{2}[c]]\] \[\leq\max_{\pi\in\Pi}4\mathbb{E}_{c\sim\mu^{\star}}\mathbb{I}[\pi(c )\neq\pi^{\star}(c))\] \[=\inf_{\pi_{\mathrm{exp}}}\max_{\pi\in\Pi}\|\phi^{\star}-\phi^{ \pi}\|_{\Lambda(\pi_{\mathrm{exp}})^{-1}}^{2},\]

where the equality follows from Lemma 28. 

Now, we state our main upper bound for contextual bandits.

**Corollary 1**.: _For the setting of tabular contextual bandits, there exists an algorithm such that with probability at least \(1-2\delta\), as long as \(\Pi\) contains only deterministic policies, it finds an \(\epsilon\)-optimal policy and terminates after collecting at most the following number of samples:_

\[\inf_{\pi_{\mathrm{exp}}}\max_{\pi\in\Pi}\frac{\|\phi^{\star}-\phi^{\pi}\|_{ \Lambda(\pi_{\mathrm{exp}})^{-1}}^{2}}{\max\{\epsilon^{2},\Delta(\pi)^{2}\}} \cdot\beta^{2}\log\frac{1}{\Delta_{\min}\vee\epsilon}+\frac{C_{\mathrm{poly}} }{\max\{\epsilon^{5/3},\Delta_{\min}^{5/3}\}},\]

_for \(C_{\mathrm{poly}}=\mathrm{poly}(|\mathcal{S}|,A,\log 1/\delta,\log 1/(\Delta_{\min} \vee\epsilon),\log|\Pi|)\) and \(\beta=C\sqrt{\log(\frac{S|\Pi|}{\delta}\cdot\frac{1}{\Delta_{\min}\vee \epsilon})}\)._

Proof.: In the special case of contextual bandits, \(U(\pi,\pi^{\star})\) defined in Theorem 1 can be written more simply as \(\mathbb{E}_{c\sim\mu^{\star}}[\mathbb{E}_{\nu^{\star}}[(r(c,\pi(c))-r(c,\pi^{ \star}(c)))^{2}[c]]\). Then, by Lemma 29, we have that:

\[\frac{U(\pi,\pi^{\star})}{\max\{\epsilon^{2},\Delta(\pi)^{2},\Delta_{\min}^{2} \}}\leq\inf_{\pi_{\mathrm{exp}}}\max_{\pi\in\Pi}\frac{\|\phi^{\star}-\phi^{\pi} \|_{\Lambda(\pi_{\mathrm{exp}})^{-1}}^{2}}{\max\{\epsilon^{2},\Delta(\pi)^{2}, \Delta_{\min}^{2}\}}\]

Plugging this into Theorem 1 completes the proof.

MDPs with Action-Independent Transitions

We consider here a special class of MDPs where the transitions only depend on the states and are independent of the actions selected i.e all \(P_{h}\) are such that \(P_{h}(s,a)=P_{h}(s,a^{\prime})\) for all \((a,a^{\prime})\in\mathcal{A}\). In this special case, we prove in this subsection that the (leading order) complexity of Perp reduces to \(O(\rho_{\Pi})\).

**Lemma 30**.: _For the ergodic MDP problem,_

\[\min_{\pi_{\mathrm{exp}}}\max_{\pi\in\Pi}\|\phi_{h}^{\pi}-\phi_{h}^{\star}\|_{ \Lambda_{h}(\pi_{\mathrm{exp}})-1}^{2}=\max_{\pi\in\Pi}\min_{\pi_{\mathrm{exp} }}\|\phi_{h}^{\pi}-\phi_{h}^{\star}\|_{\Lambda_{h}(\pi_{\mathrm{exp}})-1}^{2}\]

Proof.: We can rewrite the maximization problem to be over the simplex \(\Delta_{\Pi}\) instead:

\[\min_{\pi_{\mathrm{exp}}}\max_{\lambda\in\Delta_{\Pi}}\sum_{\pi\in\Pi}\lambda_ {\pi}\|\phi_{h}^{\pi}-\phi_{h}^{\star}\|_{\Lambda_{h}(\pi_{\mathrm{exp}})-1}^{2}\] (E.1)

This does not change the objective value. To see this, note that for any selection \(\pi\in\Pi\) in the original problem, the same objective value can be obtained by setting \(\lambda=e_{\pi}\) in Equation (E.1); hence, the modification to the optimization cannot reduce the value. Further if \(\|\phi_{h}^{\pi}-\phi_{h}^{\star}\|_{\Lambda_{h}(\pi_{\mathrm{exp}})-1}^{2}\) is maximized by \(\pi\) for any fixed \(\pi_{\mathrm{exp}}\), setting \(\lambda\) as anything other than \(e_{\pi}\) cannot increase the objective value.

Now, note that both the minimization and maximization problems are over simplices, which are compact and convex sets. The objective is linear in the maximization variable, and hence concave. The objective can be rewritten as

\[\sum_{a}\frac{(\bm{\pi}_{h}-\bm{\pi}_{h}^{\star})^{\top}e_{s,a}e_{s,a}^{\top} (\bm{\pi}_{h}-\bm{\pi}_{h}^{\star})}{p_{s,a}}\]

Here, \(p_{s,a}\) is the probability that \(\pi_{\mathrm{exp}}\) plays action \(a\), given that it is in context \(s\). From this representation, we can clearly see that the objective is convex in each \(p_{s,a}\). Hence, Von Neumann's minimax theorem applies and the proof is complete. 

**Lemma 31**.: _For the setting of ergodic MDPs,_

\[\min_{\pi_{\mathrm{exp}}}\max_{\pi\in\Pi}\|\phi_{h}^{\pi}-\phi_{h}^{\star}\|_{ \Lambda_{h}(\pi_{\mathrm{exp}})-1}^{2}=\max_{\pi\in\Pi}2\mathbb{E}_{s\sim w_{ h}^{\star}}\mathbb{I}[\pi_{h}(s)\neq\pi_{h}^{\prime}(s)],\] (E.2)

Proof.: Below, we refer to \(p_{s,a}\) as the probability that \(\pi_{\mathrm{exp}}\) plays action \(a\), given that it is in context \(s\). The second equality follows from Lemma 30.

\[\min_{\pi_{\mathrm{exp}}}\max_{\pi\in\Pi}\|\phi_{h}^{\pi}-\phi_{h }^{\star}\|_{\Lambda_{h}(\pi_{\mathrm{exp}})-1}^{2}\] \[=\min_{\pi_{\mathrm{exp}}}\max_{\pi\in\Pi}\|(\bm{\pi}_{h}-\bm{\pi }_{h}^{\star})w_{h}^{\star}\|_{\Lambda_{h}(\pi_{\mathrm{exp}})-1}^{2}\] \[=\max_{\pi\in\Pi}\min_{\pi_{\mathrm{exp}}}\|(\bm{\pi}_{h}-\bm{\pi }_{h}^{\star})w_{h}^{\star}\|_{\Lambda_{h}(\pi_{\mathrm{exp}})-1}^{2}\] \[=\max_{\pi\in\Pi}\min_{p_{1}\dots p_{S}\in\Delta_{\mathcal{A}}} \sum_{s,a}(w_{h}^{\star}(s))^{2}\frac{(\bm{\pi}_{h}-\bm{\pi}_{h}^{\star})^{ \top}e_{s,a}e_{s,a}^{\top}(\bm{\pi}_{h}-\bm{\pi}_{h}^{\star})}{w_{h}^{\star}(s) p_{s,a}}\] \[=\max_{\pi\in\Pi}\sum_{s}w_{h}^{\star}(s)\min_{p_{s}\in\Delta_{ \mathcal{A}}}\sum_{a}\frac{(\bm{\pi}_{h}-\bm{\pi}_{h}^{\star})^{\top}e_{s,a}e_ {s,a}^{\top}(\bm{\pi}_{h}-\bm{\pi}_{h}^{\star})}{p_{s,a}}\] \[=\max_{\pi\in\Pi}\sum_{s}w_{h}^{\star}(s)\left(\sum_{a}\sqrt{(\bm {\pi}_{h}-\bm{\pi}_{h}^{\star})^{\top}e_{s,a}e_{s,a}^{\top}(\bm{\pi}_{h}-\bm{ \pi}_{h}^{\star})}\right)^{2}\]

The optimization problems in the final line were solved using KKT conditions. We assume that the two policies are deterministic. Hence, the only two actions in the summation over \(\mathcal{A}\) above that are relevant are \(\pi_{h}(s)\) and \(\pi_{h}^{\prime}(s)\). For all other \(a\in\mathcal{A}\), the term in the square root evaluates to \(0\). If \(\pi_{h}(s)=\pi_{h}^{\prime}(s)\), then the entire summation over \(\mathcal{A}\) evaluates to \(0\); else, the terms indexed by \(\pi(c)\) and \(\pi^{\prime}(c)\) are both 1, and the summation evalutes to \(2\). Hence, we can simplify the expression to exactly the form of Equation (E.2) from the lemma statement, and the proof is complete.

**Lemma 32**.: _For the ergodic MDP problem, we have that_

\[\max_{\pi\in\Pi}\frac{HU(\pi,\pi^{\star})}{\max\{\epsilon^{2},\Delta(\pi)^{2}, \Delta_{\min}^{2}\}}\leq 2H^{4}\sum_{h=1}^{H}\inf_{\pi_{\mathrm{exp}}}\max_{\pi \in\Pi}\frac{\|\phi_{h}^{\star}-\phi_{h}^{\pi}\|_{\Lambda_{h}(\pi_{\mathrm{exp} })^{-1}}^{2}}{\max\{\epsilon^{2},\Delta(\pi)^{2},\Delta_{\min}^{2}\}}\]

Proof.: Recall the definition of \(U(\pi,\pi^{\star})\)

\[U(\pi,\pi^{\star})=\sum_{h=1}^{H}\mathbb{E}_{s_{h}\sim w_{h}^{\pi^{\star}}}[(Q_ {h}^{\pi}(s_{h},\pi_{h}(s))-Q_{h}^{\pi}(s_{h},\pi_{h}^{\star}(s)))^{2}].\]

Then, we have that

\[\max_{\pi\in\Pi}\frac{HU(\pi,\pi^{\star})}{\max\{\epsilon^{2}, \Delta(\pi)^{2},\Delta_{\min}^{2}\}}\] \[=\max_{\pi\in\Pi}\frac{H\sum_{h=1}^{H}\mathbb{E}_{s_{h}\sim w_{h} ^{\pi^{\star}}}[(Q_{h}^{\pi}(s_{h},\pi_{h}(s))-Q_{h}^{\pi}(s_{h},\pi_{h}^{ \star}(s)))^{2}]}{\max\{\epsilon^{2},\Delta(\pi)^{2},\Delta_{\min}^{2}\}}\] \[\leq H\sum_{h=1}^{H}\max_{\pi\in\Pi}\frac{\mathbb{E}_{s_{h}\sim w _{h}^{\pi^{\star}}}[(Q_{h}^{\pi}(s_{h},\pi_{h}(s))-Q_{h}^{\pi}(s_{h},\pi_{h}^{ \star}(s)))^{2}]}{\max\{\epsilon^{2},\Delta(\pi)^{2},\Delta_{\min}^{2}\}}\] \[\leq H\sum_{h=1}^{H}\max_{\pi\in\Pi}\frac{2H^{2}\mathbb{E}_{s\sim w _{h}^{\pi}}\mathbb{I}[\pi_{h}(s)\neq\pi_{h}^{\prime}(s)]}{\max\{\epsilon^{2}, \Delta(\pi)^{2},\Delta_{\min}^{2}\}}\] \[=H^{4}\sum_{h=1}^{H}\inf_{\pi_{\mathrm{exp}}}\max_{\pi\in\Pi} \frac{\|\phi_{h}^{\star}-\phi_{h}^{\pi}\|_{\Lambda_{h}(\pi_{\mathrm{exp}})^{- 1}}^{2}}{\max\{\epsilon^{2},\Delta(\pi)^{2},\Delta_{\min}^{2}\}}.\]

The final equality follows from Lemma 31. 

**Corollary 2**.: _Assume that all \(P_{h}\) are such that \(P_{h}(s^{\prime}|s,a)=P_{h}(s^{\prime}|s,a^{\prime})\) for all \((a,a^{\prime})\in\mathcal{A}\). Then, with probability at least \(1-2\delta\), Perp (Algorithm 2) finds an \(\epsilon\)-optimal policy and terminates after collecting at most the following number of episodes:_

\[\sum_{h=1}^{H}\inf_{\pi_{\mathrm{exp}}}\max_{\pi\in\Pi}\frac{\|\phi_{h}^{\star }-\phi_{h}^{\pi}\|_{\Lambda_{h}(\pi_{\mathrm{exp}})^{-1}}^{2}}{\max\{\epsilon^ {2},\Delta(\pi)^{2}\}}\cdot\iota H^{4}\beta^{2}+\frac{C_{\mathrm{poly}}}{\max\{ \epsilon^{5/3},\Delta_{\min}^{5/3}\}}\]

_for \(C_{\mathrm{poly}},\beta\) as defined in Theorem 1._

Proof.: The proof follows directly from Theorem 1 and Lemma 32. 

## Appendix F Tabular Franke Wolfe

**Theorem 2**.: _Fix parameters \(K_{\mathrm{unif}}>0\), \(\epsilon_{\mathrm{exp}}>0\), and consider some \(\Phi\subseteq\mathbb{R}^{SA}\) and set \(\mathcal{S}_{0}\subseteq\mathcal{S}\). Let \(\epsilon_{\mathrm{unif}}>0\) be some value satisfying_

\[W_{h}^{\star}(s)>\epsilon_{\mathrm{unif}},\forall s\in\mathcal{S}_{0},\quad \text{and}\quad K_{\mathrm{unif}}\geq\epsilon_{\mathrm{unif}}^{-1}.\]

_Assume that \(|[\boldsymbol{\phi}]_{(s,a)}|\leq C_{\boldsymbol{\phi}}\cdot(W_{h}^{\star}(s)+ \sqrt{\epsilon_{\phi}})\) for all \(s\in\mathcal{S}_{0}\), \(\boldsymbol{\phi}\in\Phi\), and some \(C_{\boldsymbol{\phi}}>0\), and that \([\boldsymbol{\phi}]_{(s,a)}=0\) for \(s\not\in\mathcal{S}_{0}\). Additionally, let the parameters be such that \(\epsilon_{\phi}/(K_{\mathrm{unif}}\epsilon_{\mathrm{unif}})\leq\epsilon_{ \mathrm{exp}}\). Then with probability at least \(1-\delta\), algorithm Algorithm 3 run with these parameters will collect at most_

\[\min\left\{C\cdot\frac{\inf_{\mathbf{A}\in\boldsymbol{\Omega}_{h}}\max_{ \boldsymbol{\phi}\in\Phi}\|\boldsymbol{\phi}\|_{\mathbf{A}^{-1}}^{2}}{\epsilon _{\mathrm{exp}}}+\frac{C_{\mathrm{fw}}}{\epsilon_{\mathrm{exp}}^{4/5}},C_{ \mathrm{fw}}(\frac{1}{\epsilon_{\mathrm{exp}}}+K_{\mathrm{unif}})\right\}+ \frac{C_{\mathrm{fw}}}{\epsilon_{\mathrm{unif}}}+\log(C_{\mathrm{fw}})\cdot K_ {\mathrm{unif}}\]

_episodes, for \(C\) a universal constant and \(C_{\mathrm{fw}}=\mathrm{poly}(S,A,H,C_{\boldsymbol{\phi}},\log 1/\delta,\log 1/\epsilon_{ \mathrm{exp}},\log|\Phi|)\), and will produce covariates \(\widehat{\mathbf{\Sigma}}\) such that_

\[\max_{\boldsymbol{\phi}\in\Phi}\|\boldsymbol{\phi}\|_{\widehat{\mathbf{ \Sigma}}^{-1}}^{2}\leq\epsilon_{\mathrm{exp}}\] (F.1)

_and, for all \(s\in\mathcal{S}_{0}\),_

\[[\widehat{\mathbf{\Sigma}}]_{(s,a)}\geq\frac{\epsilon_{\mathrm{unif}}}{2SA} \cdot K_{\mathrm{unif}}.\] (F.2)Proof.: To prove this result, we apply Lemma 37 combined with Lemma 36.

Let \(\mathcal{E}^{i}_{\exp}\) denote the success event of running Algorithm 4 at epoch \(i\), as defined in Lemma 36. On this event, and under the assumption that \(W^{\star}_{h}(s)>\epsilon_{\mathrm{unif}}\) for each \(s\in\mathcal{S}_{0}\), we have that \([\bm{\Sigma}_{i}]_{(s,a)}\geq\frac{W^{\star}_{h}(s)}{2SA}\cdot(T_{i}K_{i}+K_{ \mathrm{unif}})\) for each \((s,a)\) with \(s\in\mathcal{S}_{0}\) and \(\bm{\Sigma}_{i}\) the covariates induced by \(\bm{\mathfrak{D}}^{i}_{\mathrm{unif}}\), which implies that

\[[\bm{\Lambda}^{i}_{0}]_{(s,a)}\geq\frac{1}{T_{i}K_{i}}\frac{W^{\star}_{h}(s)} {2SA}\cdot(T_{i}K_{i}+K_{\mathrm{unif}})\geq\frac{W^{\star}_{h}(s)}{2SA}\]

for each \((s,a)\) with \(s\in\mathcal{S}_{0}\), and, furthermore, Algorithm 4 collects at most

\[T_{i}K_{i}+K_{\mathrm{unif}}+\mathrm{poly}(S,A,H,\log\frac{T_{i}K_{i}{}^{2}}{ \delta\epsilon_{\mathrm{unif}}})\cdot\frac{1}{\epsilon_{\mathrm{unif}}}\] (F.3)

episodes. Furthermore, by Lemma 36, we have \(\mathbb{P}[\mathcal{E}^{i}_{\exp}]\geq\delta/2i^{2}\), so it follows that

\[\mathbb{P}[\cup_{i\geq 1}(\mathcal{E}^{i}_{\exp})^{c}]\leq\sum_{i=1}^{ \infty}\frac{\delta}{8i^{2}}\leq\delta/4.\]

Henceforth, we therefore assume that \(\mathcal{E}^{i}_{\exp}\) holds for each \(i\). This immediately implies that (F.2) holds.

It remains to show that (F.1) is satisfied, and that our sample complexity guarantee is met. To this end we apply Lemma 37 with \(\bm{\Lambda}_{0}\) a diagonal matrix, with \([\bm{\Lambda}_{0}]_{(s,a)}=\frac{W^{\star}_{h}(s)}{2SA}\) for \(s\in\mathcal{S}_{0}\), and otherwise \([\bm{\Lambda}_{0}]_{(s,a)}=1\). Note that with this choice of \(\bm{\Lambda}_{0}\), by what we just showed above, we have \(\bm{\Lambda}^{i}_{0}\succeq\bm{\Lambda}_{0}\), as required by Lemma 37.

We next turn to bounding the smoothness constants, \(\beta\) and \(M\). First, note that by Lemma 34, at epoch \(i\) we have that all iterates of FWRegret live in the set \(\widehat{\bm{\Omega}}_{h,T_{i}K_{i}}(\delta/8i^{2})\) with probability \(1-\delta/8i^{2}\). Union bounding over this event for all \(i\), with probability at least \(1-\delta/4\), we have that for each \(i\) all iterates of FWRegret live in the set \(\widehat{\bm{\Omega}}_{h,T_{i}K_{i}}(\delta/8i^{2})\). By Lemma 35, since we have assumed that \(|[\bm{\phi}]_{(s,a)}|\leq C_{\bm{\phi}}\cdot(W^{\star}_{h}(s)+\sqrt{\epsilon_ {\phi}})\) for all \((s,a)\) with \(s\in\mathcal{S}_{0}\) and otherwise \([\bm{\phi}]_{(s,a)}=0\) for all \(\bm{\phi}\in\Phi\), we can then bound

\[M_{i} \leq\max_{s\in\mathcal{S}_{0}}\left(\frac{2SAC_{\bm{\phi}}^{2}}{C^{ \prime}}+\frac{2SAC_{\bm{\phi}}^{2}\epsilon_{\phi}}{C^{\prime}\cdot W_{h}^{*}(s) }\right)\cdot\left(\frac{2}{C^{\prime}}+\frac{2}{C^{\prime}T_{i}K_{i}W_{h}^{ \star}(s)}\cdot\log\frac{SAH}{\delta}\right)\] \[\beta_{i} \leq\max_{s\in\mathcal{S}_{0}}(2\eta_{i}+2)\left(\frac{2SAC_{\bm{ \phi}}^{2}}{C^{\prime}}+\frac{2SAC_{\bm{\phi}}^{2}\epsilon_{\phi}}{C^{\prime} \cdot W_{h}^{*}(s)}\right)^{2}\cdot\left(\frac{2}{C^{\prime}}+\frac{2}{C^{ \prime}T_{i}K_{i}W_{h}^{\star}(s)}\cdot\log\frac{SAH}{\delta}\right)^{2}\]

On the event \(\mathcal{E}_{\exp}^{i}\), as noted above we have \([\bm{\Lambda}_{0}^{i}]_{(s,a)}\geq\frac{W_{h}^{*}(s)}{2SA}(1+\frac{K_{\rm unif }}{T_{i}K_{i}})\) for \(s\in\mathcal{S}_{0}\), so we can take \(C^{\prime}=\frac{1}{2SA}(1+\frac{K_{\rm unif}}{T_{i}K_{i}})\). We can then bound

\[\max_{s\in\mathcal{S}_{0}}\left(\frac{2SAC_{\bm{\phi}}^{2}}{C^{ \prime}}+\frac{2SAC_{\bm{\phi}}^{2}\epsilon_{\phi}}{C^{\prime}\cdot W_{h}^{*}( s)}\right)\cdot\left(\frac{2}{C^{\prime}}+\frac{2}{C^{\prime}T_{i}K_{i}W_{h}^{*}( s)}\cdot\log\frac{SAH}{\delta}\right)\] \[\leq\left(4S^{2}A^{2}C_{\bm{\phi}}+\frac{4S^{2}A^{2}C_{\bm{\phi}} ^{2}\epsilon_{\phi}\cdot T_{i}K_{i}}{K_{\rm unif}\epsilon_{\rm unif}}\right) \cdot\left(4SA+\frac{4SA}{K_{\rm unif}\epsilon_{\rm unif}}\log\frac{SAH}{ \delta}\right)\]

where we have used that \(W_{h}^{\star}(s)\geq\epsilon_{\rm unif}\) for all \(s\in\mathcal{S}_{0}\), by assumption. By assumption we have \(\frac{\epsilon_{\phi}}{K_{\rm unif}\epsilon_{\rm unif}}\leq\epsilon_{\exp}\). Note that by construction, the while statement on Line 3 will ensure that we always have \(T_{i}K_{i}\leq\operatorname{poly}(S,A,H,C_{\bm{\phi}},\log 1/\delta,\log 1/ \epsilon_{\exp},\log|\Phi|)\cdot\epsilon_{\exp}^{-1}\), so we can bound

\[\epsilon_{\exp}\cdot T_{i}K_{i}\leq\operatorname{poly}(S,A,H,C_{\bm{\phi}}, \log 1/\delta,\log 1/\epsilon_{\exp},\log|\Phi|).\]

It follows that it suffices to take

\[\beta,M\leq\operatorname{poly}(S,A,H,C_{\bm{\phi}},\log 1/\delta,\log 1/ \epsilon_{\exp},\log|\Phi|).\]

We now consider two cases. In the first case, when the termination criteria on Line 7 is met, we can apply Lemma 37, to get that with probability at least \(1-\delta/4\) we have that the procedure terminates after running for at most

\[\max\left\{\begin{array}{ll}\min_{N}&16N&\text{s.t.}\quad\inf _{\bm{\Lambda}\in\bm{\Omega}}\max_{\bm{\phi}\in\Phi}\bm{\phi}^{\top}(N\bm{ \Lambda}+\bm{\Lambda}_{0})^{-1}\bm{\phi}\leq\frac{\epsilon_{\exp}}{6},\\ &\frac{\operatorname{poly}(\beta,R,d,H,M,\log 1/\delta,\log 1/\epsilon_{\exp}, \log|\Phi|)}{\epsilon_{\exp}^{4/5}}\end{array}\right\}\] \[\leq\max\left\{\begin{array}{ll}\min_{N}&16N&\text{s.t.}\quad \inf_{\bm{\Lambda}\in\bm{\Omega}}\max_{\bm{\phi}\in\Phi}\bm{\phi}^{\top}(N\bm{ \Lambda}+\bm{\Lambda}_{0})^{-1}\bm{\phi}\leq\frac{\epsilon_{\exp}}{6},\\ &\frac{\operatorname{poly}(S,A,H,C_{\bm{\phi}},\log 1/\delta,\log 1/\epsilon_{\exp}, \log|\Phi|)}{\epsilon_{\exp}^{4/5}}\end{array}\right\}\]

episodes, and returns data \(\widehat{\bm{\Sigma}}_{N}\) such that

\[f_{\widehat{i}}(N^{-1}\widehat{\bm{\Sigma}}_{N})\leq N\epsilon_{\exp},\]

where \(\widehat{i}\) is the index of the epoch on which it terminates. By Lemma D.1 of [42], we have

\[\max_{\bm{\phi}\in\Phi}\|\bm{\phi}\|_{\bm{\Lambda}(N^{-1}\widehat{\bm{\Sigma}} _{N})^{-1}}^{2}\leq f_{\widehat{i}}(N^{-1}\widehat{\bm{\Sigma}}_{N})\leq N \epsilon_{\exp}\]

which implies

\[\max_{\bm{\phi}\in\Phi}\|\bm{\phi}\|_{(\widehat{\bm{\Sigma}}_{N}+\bm{\Sigma}_{ \widehat{i}})^{-1}}^{2}\leq\epsilon_{\exp},\]

which proves (F.1). Furthermore, (F.2) holds since as noted \([\bm{\Sigma}_{i}]_{(s,a)}\geq\frac{W_{h}^{*}(s)}{2SA}\cdot(T_{i}K_{i}+K_{\rm unif})\) for each \((s,a)\) with \(s\in\mathcal{S}_{0}\), and since \(W_{h}^{*}(s)\geq\epsilon_{\rm unif}\) for all \(s\in\mathcal{S}_{0}\).

In the second case, when the while loop on Line 3 terminates since \(T_{i}K_{i}\leq\operatorname{poly}(S,A,H,C_{\bm{\phi}},\log 1/\delta,\log 1/ \epsilon_{\exp},\log|\Phi|)\cdot\epsilon_{\exp}^{-1}\), we can bound the total number of episodes collected within the calls to Algorithm 4 of [43] within the while loop by \(\mathrm{poly}(S,A,H,C_{\bm{\phi}},\log 1/\delta,\log 1/\epsilon_{\mathrm{exp}},\log| \bm{\Phi}|)\cdot\epsilon_{\mathrm{exp}}^{-1}\). Furthermore, by Lemma 36, with probability at least \(1-\delta/4\), we have that the call to UnifExp on Line 12 terminates after running for at most

\[\frac{8S^{2}A^{2}C_{\bm{\phi}}^{2}}{\epsilon_{\mathrm{exp}}}+(8S^{2}A^{2}C_{ \bm{\phi}}^{2}+1)K_{\mathrm{unif}}+\mathrm{poly}(S,A,H,\log\frac{T_{i}K_{i}i^{ 2}}{\delta\epsilon_{\mathrm{unif}}})\cdot\frac{1}{\epsilon_{\mathrm{unif}}}\]

episodes, and that the returned data satisfies \(N_{h}(s,a)\geq\frac{W_{h}^{\star}(s)}{2SA}\cdot(\frac{8S^{2}A^{2}C_{\bm{\phi}}^ {2}}{\epsilon_{\mathrm{exp}}}+8S^{2}A^{2}C_{\bm{\phi}}^{2}K_{\mathrm{unif}}+K_ {\mathrm{unif}})\). Since \(|[\bm{\phi}]_{(s,a)}|\leq C_{\bm{\phi}}\cdot(W_{h}^{\star}(s)+\sqrt{\epsilon_ {\phi}})\) and \(\epsilon_{\phi}/(K_{\mathrm{unif}}\epsilon_{\mathrm{unif}})\leq\epsilon_{ \mathrm{exp}}\) by assumption, some manipulation shows that

\[\frac{[\bm{\phi}]_{(s,a)}^{2}}{N_{h}(s,a)}\leq\frac{C_{\bm{\phi}}^{2}\cdot(W_ {h}^{\star}(s)+\sqrt{\epsilon_{\phi}})^{2}}{\frac{W_{h}^{\star}(s)}{2SA}\cdot( \frac{8S^{2}A^{2}C_{\bm{\phi}}^{2}}{\epsilon_{\mathrm{exp}}}+8S^{2}A^{2}C_{ \bm{\phi}}^{2}K_{\mathrm{unif}}+K_{\mathrm{unif}})}\leq\frac{\epsilon_{\mathrm{ exp}}}{SA}.\]

It follows then that, letting \(\widehat{\bm{\Sigma}}\) denote the covariance obtained by the call to UnifExp on Line 12,

\[\max_{\bm{\phi}\in\Phi}\|\bm{\phi}\|_{\widehat{\bm{\Sigma}}^{-1}}^{2}\leq \epsilon_{\mathrm{exp}}\]

as desired. Furthermore, it is straightforward to see that \([\widehat{\bm{\Sigma}}]_{(s,a)}\geq\frac{\epsilon_{\mathrm{unif}}}{2SA}\cdot K _{\mathrm{unif}}\) for \(s\in\mathcal{S}_{0}\) as well.

To complete the proof, we union bound over these events holding, and take the minimum of the sample complexity bounds from either case.

### Data Conditioning

**Lemma 33**.: _Consider running any algorithm for \(K\) episodes. Let \(K_{h}(s,a)\) denote the number of visits to \((s,a,h)\). Then with probability at least \(1-\delta\), for all \((s,a,h)\) simultaneously, we have_

\[K_{h}(s,a)\leq W_{h}^{\star}(s)K+\sqrt{2W_{h}^{\star}(s)K\cdot\log\frac{SAH}{ \delta}}+\log\frac{SAH}{\delta}.\]

Proof.: By definition, we have

\[\sup_{\pi}w_{h}^{\pi}(s)=W_{h}^{\star}(s).\]

This implies that any policy will reach \((s,h)\) with probability at most \(W_{h}^{\star}(s)\). We can therefore think of this as the sum of Bernoullis with parameter at most \(W_{h}^{\star}(s)\), so the bound follows by applying Bernstein's inequality and a union bound. 

**Lemma 34**.: _Consider the set_

\[\widehat{\bm{\Omega}}_{h,K}(\delta):=\left\{\mathrm{diag}(\bm{v})\ :\ \bm{v}\in\mathbb{R}_{+}^{SA},[\bm{v}]_{(s,a)}\leq W_{h}^{\star}(s)+\sqrt{\frac{2 W_{h}^{\star}(s)}{K}\cdot\log\frac{SAH}{\delta}}+\frac{1}{K}\log\frac{SAH}{ \delta}\right\}.\]

_Consider running some set of policies for \(K\) episodes, and let \(\widehat{\bm{\Lambda}}\) be defined as_

\[\widehat{\bm{\Lambda}}_{h}=\mathrm{diag}(\widehat{\bm{v}}),\quad[\bm{v}]_{(s,a )}=\frac{K_{h}(s,a)}{K}.\]

_Then with probability at least \(1-\delta\), we have that \(\widehat{\bm{\Lambda}}_{h}\in\widehat{\bm{\Omega}}_{h,K}(\delta)\) for all \(h\in[H]\) simultaneously._

Proof.: This is an immediate consequence of Lemma 33. 

We will denote \(\widehat{\bm{\Omega}}_{h,K}:=\widehat{\bm{\Omega}}_{h,K}(\delta)\) when the choice of \(\delta\) is clear from context.

**Lemma 35**.: _Consider the function_

\[f(\bm{\Lambda})=\frac{1}{\eta}\log\left(\sum_{\bm{\phi}\in\Phi}e^{\eta\|\bm{ \phi}\|_{\bm{\Lambda}(\bm{\Lambda})^{-1}}^{2}}\right)\quad\text{for}\quad\bm{ \Lambda}(\bm{\Lambda})=\bm{\Lambda}+\bm{\Lambda}_{0}\]_Assume that for all \(\bm{\phi}\in\Phi\) we have_

\[\max_{\bm{\phi}\in\Phi}|[\bm{\phi}]_{(s,a)}|\leq C_{\bm{\phi}}\cdot(W_{h}^{\star} (s)+\epsilon),\quad\forall s\in\mathcal{S}_{0}\]

_for some \(\mathcal{S}_{0}\) and some \(C_{\bm{\phi}},\epsilon>0\), and otherwise \([\bm{\phi}]_{(s,a)}=0\). Assume that \(\bm{\Lambda}_{0}=\mathrm{diag}(\bm{v})\) for some \(\bm{v}\) satisfying_

\[[\bm{v}]_{(s,a)}\geq C^{\prime}\cdot W_{h}^{\star}(s),\quad\forall s\in \mathcal{S}_{0}\]

_and otherwise \([\bm{v}]_{(s,a)}\geq\lambda\), for some \(C^{\prime},\lambda>0\). Then we can bound_

\[\sup_{\widehat{\bm{\Lambda}},\widehat{\bm{\Lambda}}^{\prime}\in \widehat{\bm{\Omega}}_{h,K}}|\nabla_{\bm{\Lambda}}f(\bm{\Lambda})|_{\bm{ \Lambda}=\widehat{\bm{\Lambda}}}[\widehat{\bm{\Lambda}}^{\prime}]|\] \[\leq \max_{s\in\mathcal{S}_{0}}\left(\frac{2SAC_{\bm{\phi}}^{2}}{C^{ \prime}}+\frac{2SAC_{\bm{\phi}}^{2}\epsilon^{2}}{C^{\prime}\cdot W_{h}^{\star} (s)}\right)\cdot\left(\frac{2}{C^{\prime}}+\frac{2}{C^{\prime}KW_{h}^{\star} (s)}\cdot\log\frac{SAH}{\delta}\right)\]

_and_

\[\sup_{\widehat{\bm{\Lambda}},\widehat{\bm{\Lambda}}^{\prime}, \widehat{\bm{\Lambda}}^{\prime\prime}\in\widehat{\bm{\Omega}}_{h,K}}|\nabla_{ \bm{\Lambda}}^{2}f(\bm{\Lambda})|_{\bm{\Lambda}=\widehat{\bm{\Lambda}}}[ \widehat{\bm{\Lambda}}^{\prime},\widehat{\bm{\Lambda}}^{\prime\prime}]|\] \[\leq \max_{s\in\mathcal{S}_{0}}(2+2\eta)\left(\frac{2SAC_{\bm{\phi}}^ {2}}{C^{\prime}}+\frac{2SAC_{\bm{\phi}}^{2}\epsilon^{2}}{C^{\prime}\cdot W_{h} ^{\star}(s)}\right)^{2}\cdot\left(\frac{2}{C^{\prime}}+\frac{2}{C^{\prime}KW_{h }^{\star}(s)}\cdot\log\frac{SAH}{\delta}\right)^{2}.\]

Proof.: By Lemma D.5 of [42], we have that

\[\nabla_{\bm{\Lambda}}f(\bm{\Lambda})|_{\bm{\Lambda}=\widehat{\bm{\Lambda}}}[ \widehat{\bm{\Lambda}}^{\prime}]=-\left(\sum_{\bm{\phi}\in\Phi}e^{\eta\|\bm{ \phi}\|^{2}_{\bm{\Lambda}(\widehat{\bm{\Lambda}})^{-1}}}\right)\cdot\sum_{\bm {\phi}\in\Phi}e^{\eta\|\bm{\phi}\|^{2}_{\bm{\Lambda}(\widehat{\bm{\Lambda}})^{- 1}}}\bm{\phi}^{\top}\bm{\Lambda}(\widehat{\bm{\Lambda}})^{-1}\widehat{\bm{ \Lambda}}^{\prime}\bm{\Lambda}(\widehat{\bm{\Lambda}})^{-1}\bm{\phi}.\]

We have

\[\bm{\phi}^{\top}\bm{\Lambda}(\widehat{\bm{\Lambda}})^{-1}\widehat{\bm{\Lambda} }^{\prime}\bm{\Lambda}(\widehat{\bm{\Lambda}})^{-1}\bm{\phi}=\sum_{s,a}\frac{ [\bm{\phi}]_{(s,a)}^{2}\cdot[\widehat{\bm{\Lambda}}^{\prime}]_{(s,a)}}{[\bm{ \Lambda}(\widehat{\bm{\Lambda}})]_{(s,a)}^{2}}=\sum_{s\in\mathcal{S}_{0}}\sum_ {a}\frac{[\bm{\phi}]_{(s,a)}^{2}\cdot[\widehat{\bm{\Lambda}}^{\prime}]_{(s,a) }}{[\bm{\Lambda}(\widehat{\bm{\Lambda}})]_{(s,a)}^{2}}\]

where the last equality follows since, for \(s\not\in\mathcal{S}_{0}\), we have assumed \([\bm{\phi}]_{(s,a)}=0\).

Now consider some \(s\in\mathcal{S}_{0}\). By assumption we have \([\bm{\phi}]_{(s,a)}^{2}\leq 2C_{\bm{\phi}}^{2}\cdot(W_{h}^{\star}(s)^{2}+\epsilon^{2})\) and by our assumption on \(\bm{\Lambda}_{0}\) we can lower bound \([\bm{\Lambda}(\widehat{\bm{\Lambda}})]_{(s,a)}\geq C^{\prime}\cdot W_{h}^{\star} (s)\). Furthermore, since \(\widehat{\bm{\Lambda}}^{\prime}\in\widehat{\bm{\Omega}}_{h,K}\), we have

\[[\widehat{\bm{\Lambda}}^{\prime}]_{(s,a)} \leq W_{h}^{\star}(s)+\sqrt{\frac{2W_{h}^{\star}(s)}{K}\cdot\log \frac{SAH}{\delta}}+\frac{1}{K}\log\frac{SAH}{\delta}\] \[\leq 2W_{h}^{\star}(s)+\frac{2}{K}\log\frac{SAH}{\delta}.\]

Putting this together, we have

\[\frac{[\bm{\phi}]_{(s,a)}^{2}\cdot[\widehat{\bm{\Lambda}}^{\prime }]_{(s,a)}}{[\bm{\Lambda}(\widehat{\bm{\Lambda}})]_{(s,a)}^{2}} \leq\frac{4C_{\bm{\phi}}^{2}\cdot(W_{h}^{\star}(s)^{2}+\epsilon^ {2})\cdot(W_{h}^{\star}(s)+\frac{1}{K}\log\frac{SAH}{\delta})}{(C^{\prime} \cdot W_{h}^{\star}(s))^{2}}\] \[\leq\left(\frac{2C_{\bm{\phi}}^{2}}{C^{\prime}}+\frac{2C_{\bm{ \phi}}^{2}\epsilon^{2}}{C^{\prime}W_{h}^{\star}(s)}\right)\cdot\left(\frac{2}{C ^{\prime}}+\frac{2}{C^{\prime}KW_{h}^{\star}(s)}\log\frac{SAH}{\delta}\right).\]

It follows that

\[\sum_{s\in\mathcal{S}_{0}}\sum_{a}\frac{[\bm{\phi}]_{(s,a)}^{2}\cdot[\widehat{ \bm{\Lambda}}^{\prime}]_{(s,a)}}{[\bm{\Lambda}(\widehat{\bm{\Lambda}})]_{(s,a)}^ {2}}\leq\max_{s\in\mathcal{S}_{0}}\left(\frac{2SAC_{\bm{\phi}}^{2}}{C^{\prime}}+ \frac{2SAC_{\bm{\phi}}^{2}\epsilon^{2}}{C^{\prime}W_{h}^{\star}(s)}\right) \cdot\left(\frac{2}{C^{\prime}}+\frac{2}{C^{\prime}KW_{h}^{\star}(s)}\log\frac{ SAH}{\delta}\right).\]

The second bound follows in an analogous fashion, using the expression for the second derivative given in Lemma D.5 of [42].

**Lemma 36**.: _With probability at least \(1-\delta\), Algorithm 4 will terminate after running for at most_

\[K+\mathrm{poly}(S,A,H,\log\frac{K}{\delta\epsilon_{\mathrm{unif}}})\cdot\frac{1} {\epsilon_{\mathrm{unif}}}\]

_episodes and will collect at least \(\frac{W_{h}^{\star}(s)K}{2SA}\) samples from each \((s,a)\) such that \(W_{h}^{\star}(s)>\epsilon_{\mathrm{unif}}\)._

Proof.: By Theorem 13 of [46], with probability at least \(1-\delta/2SA\), for any \((s,a)\):

* Learn2Explore will run for at most \(\mathrm{poly}(S,A,H,\log\frac{K}{\delta\epsilon_{\mathrm{unif}}})\cdot\frac{1} {\epsilon_{\mathrm{unif}}}\) episodes.
* Rerunning every policy in \(\Pi_{j_{sa}}\) once, with probability at least \(1-\delta/K\) we will collect \(N=2^{-j_{sa}}|\Pi_{j_{sa}}|\) samples from \((s,a)\), for \(|\Pi_{j_{sa}}|=\mathcal{O}(2^{j_{sa}}\cdot S^{3}A^{2}H^{4}\log^{3}1/\delta)\).
* We have that \(W_{h}^{\star}(s)\leq 2^{-j_{sa}+1}\).
* IF \((s,a)\not\in\mathcal{X}_{j}\) for all \(j=1,2,\ldots,\lceil\log 1/\epsilon_{\mathrm{unif}}\rceil\), then \(W_{h}^{\star}(s)\leq\epsilon_{\mathrm{unif}}\).

By the above conclusions, rerunning policies in \(\Pi_{j_{sa}}\) on Line 7, with probability at least \(1-\delta/2SA\) we will collect

\[N\cdot K_{sa}\geq N\cdot\frac{K}{SA|\Pi_{j_{sa}}|}=\frac{2^{-j_{sa}}K}{SA}\]

samples from \((s,a)\). As noted, \(W_{h}^{\star}(s)\leq 2^{-j_{sa}+1}\), so this implies that we will collect at least \(\frac{W_{h}^{\star}(s)K}{2SA}\) samples from \((s,a)\). Union bounding over this holding for all \((s,a)\), and noting that we only fail to collect this many samples if \(W_{h}^{\star}(s)\leq\epsilon_{\mathrm{unif}}\) gives the collection guarantee.

To bound the total number of episodes, we note that the procedure on Line 7 will, in total collect at most

\[\sum_{s,a;j_{sa}\,\mathrm{exists}}|\Pi_{j_{sa}}|\lceil K_{sa}\rceil\leq\sum_ {s,a;j_{sa}\,\mathrm{exists}}|\Pi_{j_{sa}}|+\sum_{s,a}\frac{K}{SA}=\sum_{s,a} |\Pi_{j_{sa}}|+K\]

episodes. IF \(j_{sa}\) exists, this implies that \(|\Pi_{j_{sa}}|\leq\mathcal{O}(2^{j_{sa}}\cdot S^{3}A^{2}H^{4}\log^{3}1/\delta)\), and since \(j_{sa}\in\{1,2,\ldots,\lceil\log 1/\epsilon_{\mathrm{unif}}\rceil\}\), this implies that the above is bounded by

\[K+\mathcal{O}(\epsilon_{\mathrm{unif}}^{-1}\cdot S^{3}A^{2}H^{4}\log^{3}1/ \delta).\]

Combining this with our bound on the total number of episodes collected by Learn2Explore, we have that the number of episodes collected by Algorithm 4 is bounded by

\[K+\mathrm{poly}(S,A,H,\log\frac{K}{\delta\epsilon_{\mathrm{unif}}})\cdot\frac{ 1}{\epsilon_{\mathrm{unif}}}.\]

### Online Frank-Wolfe

**Lemma 37**.: _Let_

\[f_{i}(\bm{\Lambda})=\frac{1}{\eta_{i}}\log\left(\sum_{\bm{\phi}\in\Phi}e^{\eta_{i} \|\bm{\phi}\|^{2}_{\bm{\Lambda}_{i}(\bm{\Lambda})^{-1}}}\right),\quad\bm{ \Lambda}_{i}(\bm{\Lambda})=\bm{\Lambda}+\frac{1}{T_{i}K_{i}}\bm{\Lambda}_{0,i}\]

_for some \(\bm{\Lambda}_{0,i}\) satisfying \(\bm{\Lambda}_{0,i}\succeq\bm{\Lambda}_{0}\) for all \(i\), and \(\eta_{i}=2^{2i/5}\). Let \((\beta_{i},M_{i})\) denote the smoothness and magnitude constants for \(f_{i}\). Let \((\beta,M)\) be some values such that \(\beta_{i}\leq\eta_{i}\beta,M_{i}\leq M\) for all \(i\), and \(R\) the diameter of the domain of possible values of \(\bm{\Lambda}\)._

_Then, if we run Algorithm 4 of [43] on \((f_{i})_{i}\) with constraint tolerance \(\epsilon\) and confidence \(\delta\) and \(K_{i}=T_{i}=2^{i}\), we have that with probability at least \(1-\delta\), it will run for at most_

\[\max\bigg{\{}\min_{N}16N\text{ s.t. }\inf_{\bm{\Lambda}\in\bm{\Omega}}\max_{ \bm{\phi}\in\Phi}\bm{\phi}^{\top}(N\bm{\Lambda}+\bm{\Lambda}_{0})^{-1}\bm{ \phi}\leq\frac{\epsilon}{6},\frac{\operatorname{poly}(\beta,R,d,H,M,\log 1/ \delta,\log|\Phi|)}{\epsilon^{4/5}}\bigg{\}}.\]

_episodes, and will return data \(\{\bm{\phi}_{\tau}\}_{\tau=1}^{N}\) with covariance \(\widehat{\bm{\Sigma}}_{N}=\sum_{\tau=1}^{N}\bm{\phi}_{\tau}\bm{\phi}_{\tau}^{\top}\) such that_

\[f_{\widehat{i}}(N^{-1}\widehat{\bm{\Sigma}}_{N})\leq N\epsilon,\]

_where \(\widehat{i}\) is the iteration on which OptCov terminates._

Proof.: Our goal is to simply find a setting of \(i\) that is sufficiently large to guarantee the condition \(f_{i}(\widehat{\bm{\Lambda}}_{i})\leq K_{i}T_{i}\epsilon\) is met. By Lemma C.1 of [43], we have with probability at least \(1-\delta/(2i^{2})\):

\[f_{i}(\widehat{\bm{\Lambda}}_{i}) \leq\inf_{\bm{\Lambda}\in\bm{\Omega}}f_{i}(\bm{\Lambda})+\frac{ \beta_{i}R^{2}(\log T_{i}+3)}{2T_{i}}+\sqrt{\frac{4M^{2}\log(8i^{2}T_{i}/ \delta)}{K_{i}}}\] \[\qquad+\sqrt{\frac{c_{1}M^{2}d^{4}H^{4}\log^{3}(8i^{2}HK_{i}T_{ i}/\delta)}{K_{i}}}+\frac{c_{2}Md^{4}H^{3}\log^{7/2}(4i^{2}HK_{i}T_{i}/ \delta)}{K_{i}}\] \[\leq 3\max\Bigg{\{}\inf_{\bm{\Lambda}\in\bm{\Omega}}f_{i}(\bm{ \Lambda}),\frac{\beta_{i}R^{2}(\log T_{i}+3)}{2T_{i}},\sqrt{\frac{4M^{2}\log( 8i^{2}T_{i}/\delta)}{K_{i}}}\] \[\qquad\qquad+\sqrt{\frac{c_{1}M^{2}d^{4}H^{4}\log^{3}(8i^{2}HK_{ i}T_{i}/\delta)}{K_{i}}}+\frac{c_{2}Md^{4}H^{3}\log^{7/2}(4i^{2}HK_{i}T_{i}/ \delta)}{K_{i}}\Bigg{\}}.\]

So a sufficient condition for \(f_{i}(\widehat{\bm{\Lambda}}_{i})\leq K_{i}T_{i}\epsilon\) is that

\[K_{i}T_{i}\geq \frac{3}{\epsilon}\max\Bigg{\{}\inf_{\bm{\Lambda}\in\bm{\Omega}} f_{i}(\bm{\Lambda}),\frac{\beta_{i}R^{2}(\log T_{i}+3)}{2T_{i}},\sqrt{\frac{4M^{2} \log(8i^{2}T_{i}/\delta)}{K_{i}}}\] (F.4) \[\qquad+\sqrt{\frac{c_{1}M^{2}d^{4}H^{4}\log^{3}(8i^{2}HK_{i}T_{ i}/\delta)}{K_{i}}}+\frac{c_{2}Md^{4}H^{3}\log^{7/2}(4i^{2}HK_{i}T_{i}/ \delta)}{K_{i}}\Bigg{\}}.\]

Recall that

\[f_{i}(\bm{\Lambda})=\frac{1}{\eta_{i}}\log\left(\sum_{\bm{\phi}\in\Phi}e^{\eta_ {i}\|\bm{\phi}\|^{2}_{\bm{\Lambda}_{i}(\bm{\Lambda})^{-1}}}\right),\quad\bm{ \Lambda}_{i}(\bm{\Lambda})=\bm{\Lambda}+\frac{1}{T_{i}K_{i}}\bm{\Lambda}_{0,i}.\]

By Lemma D.1 of [42], we can bound

\[\max_{\bm{\phi}\in\Phi}\|\bm{\phi}\|^{2}_{\bm{\Lambda}_{i}(\bm{\Lambda})^{-1}} \leq f_{i}(\bm{\Lambda})\leq\max_{\bm{\phi}\in\Phi}\|\bm{\phi}\|^{2}_{\bm{ \Lambda}_{i}(\bm{\Lambda})^{-1}}+\frac{\log|\Phi|}{\eta_{i}}.\]

Thus,

\[\inf_{\bm{\Lambda}\in\bm{\Omega}}f_{i}(\bm{\Lambda}) \leq\inf_{\bm{\Lambda}\in\bm{\By our choice of \(\eta_{i}=2^{2i/5}\), and \(K_{i}=2^{i}\), \(T_{i}=2^{i}\), we can ensure that

\[K_{i}T_{i}\geq\frac{6}{\epsilon}\frac{\log|\Phi|}{\eta_{i}}\]

as long as \(i\geq\frac{2}{5}\log_{2}[\frac{6\log|\Phi|}{\epsilon}]\). To ensure that

\[T_{i}K_{i}\geq\frac{6}{\epsilon}\inf_{\mathbf{A}\in\mathbf{\Omega}}\max_{ \boldsymbol{\phi}\in\Phi}T_{i}K_{i}\boldsymbol{\phi}^{\top}(T_{i}K_{i} \boldsymbol{\Lambda}+\boldsymbol{\Lambda}_{0,i})^{-1}\boldsymbol{\phi}\]

it suffices to take

\[i\geq\operatorname*{arg\,min}_{i}i\quad\text{s.t.}\quad\inf_{\mathbf{A}\in \mathbf{\Omega}}\max_{\boldsymbol{\phi}\in\Phi}\boldsymbol{\phi}^{\top}(2^{3i} \mathbf{A}+\boldsymbol{\Lambda}_{0,i})^{-1}\boldsymbol{\phi}\leq\frac{ \epsilon}{6}.\]

Since we assume that we can lower bound \(\boldsymbol{\Lambda}_{0,i}\succeq\boldsymbol{\Lambda}_{0}\) for each \(i\), so this can be further simplified to

\[i\geq\operatorname*{arg\,min}_{i}i\quad\text{s.t.}\quad\inf_{\mathbf{A}\in \mathbf{\Omega}}\max_{\boldsymbol{\phi}\in\Phi}\boldsymbol{\phi}^{\top}(2^{3i} \mathbf{A}+\boldsymbol{\Lambda}_{0})^{-1}\boldsymbol{\phi}\leq\frac{\epsilon }{6}.\] (F.5)

We next want to show that

\[T_{i}K_{i}\geq\frac{3}{\epsilon}\cdot\frac{\beta_{i}R^{2}(\log T_{i}+3)}{2T_{i }}.\]

Bounding \(\beta_{i}\leq\eta_{i}\beta\), a sufficient condition for this is that

\[i\geq\frac{2}{5}\left(\log_{2}(12\beta R^{2}i)+\log_{2}\frac{1}{\epsilon} \right).\]

By Lemma A.1 of [43], it suffices to take

\[i\geq\frac{6}{5}\log_{2}(9\beta R^{2}\log_{2}\frac{1}{\epsilon})+\frac{2}{5} \log_{2}\frac{1}{\epsilon}\] (F.6)

to meet this condition (this assumes that \(12\beta R^{2}\geq 1\) and \(\frac{2}{5}\log_{2}\frac{1}{\epsilon}\geq 1\)--if either of these is not the case we can just replace them with 1 without changing the validity of the final result).

Finally, we want to ensure that

\[T_{i}K_{i}\geq\frac{3}{\epsilon}\bigg{(} \sqrt{\frac{4M^{2}\log(8i^{2}T_{i}/\delta)}{K_{i}}}\] \[+\sqrt{\frac{c_{1}M^{2}d^{4}H^{4}\log^{3}(8i^{2}HK_{i}T_{i}/ \delta)}{K_{i}}}+\frac{c_{2}Md^{4}H^{3}\log^{7/2}(4i^{2}HK_{i}T_{i}/\delta)}{ K_{i}}\bigg{)}.\]

To guarantee this, it suffices that

\[2^{5i/2}\geq\frac{c}{\epsilon}\sqrt{M^{2}d^{4}H^{4}i^{3}\log^{3}(iH/\delta)},\quad 2^{3i}\geq\frac{c}{\epsilon}\cdot Md^{4}H^{3}i^{7/2}\log^{7/2}(iH/\delta).\]

or

\[i\geq\frac{4}{5}\log_{2}(cMdHi\log(H/\delta))+\frac{2}{5}\log_{2}\frac{1}{ \epsilon},\quad i\geq\frac{4}{3}\log_{2}(cMdH\log(H/\delta))+\frac{1}{3}\log _{2}\frac{1}{\epsilon}.\]

By Lemma A.1 of [43], it then suffices to take

\[i\geq\frac{12}{5}\log(cMdH\log(H/\delta)\log_{2}1/\epsilon)+ \frac{2}{5}\log_{2}\frac{1}{\epsilon},\] (F.7) \[i\geq 4\log_{2}(cMdH\log(H/\delta)\log_{2}1/\epsilon)+\frac{1}{3} \log_{2}\frac{1}{\epsilon}\]

Thus, a sufficient condition to guarantee (F.4) is that \(i\) is large enough to satisfy (F.5), (F.6), and (F.7) and \(i\geq\frac{2}{5}\log_{2}[\frac{6\log|\Phi|}{\epsilon}]\).

If \(\widehat{i}\) is the final round, the total complexity scales as

\[\sum_{i=1}^{\widehat{i}}T_{i}K_{i}=\sum_{i=1}^{\widehat{i}}2^{2i}\leq 2\cdot 2^{ 2\widehat{i}}.\]

Using the sufficient condition on \(i\) given above, we can bound the total complexity as

\[\max\bigg{\{}\min_{N}16N\text{ s.t. }\inf_{\mathbf{\Lambda}\in\mathbf{\Omega}}\max_{ \boldsymbol{\phi}\in\Phi}\boldsymbol{\phi}^{\top}(N\boldsymbol{\Lambda}+ \boldsymbol{\Lambda}_{0})^{-1}\boldsymbol{\phi}\leq\frac{\epsilon}{6},\frac{ \operatorname*{poly}(\beta,R,d,H,M,\log 1/\delta,\log|\Phi|)}{\epsilon^{4/5}}\bigg{\}}.\]

### Pruning Hard-to-Reach States

``` input: tolerance \(\epsilon_{\mathrm{unif}}\), confidence \(\delta\) \(\mathcal{S}^{\mathrm{keep}}\leftarrow\emptyset\) for\(h\in[H]\)do for\(s\in\mathcal{S}\)do // Learn2Explore is as defined in [46] \(\{(\mathcal{X}_{j},\Pi_{j},N_{j})\}_{j=1}^{\lceil\log_{2}\frac{1}{32\epsilon_{ \mathrm{unif}}}\rceil}\leftarrow\textsc{Learn2Explore}(\{(s,a)\},h,\frac{\delta}{ SH},\frac{1}{2},32\epsilon_{\mathrm{unif}})\) for any \(a\in\mathcal{A}\) if\(\exists j_{s}\) such that \((s,a)\in\mathcal{X}_{j_{s}}\)then \(\mathcal{S}^{\mathrm{keep}}=\mathcal{S}^{\mathrm{keep}}\cup\{(s,h)\}\) endif endfor endfor return\(\mathcal{S}^{\mathrm{keep}}\) ```

**Algorithm 5**Prune: Prune Hard-to-Reach States

**Lemma 38**.: _With probability at least \(1-\delta\), Algorithm 5 will terminate after running for at most_

\[\mathrm{poly}(S,A,H,\log\frac{1}{\delta\epsilon_{\mathrm{unif}}})\cdot\frac{1} {\epsilon_{\mathrm{unif}}}\]

_episodes and will return a set \(\mathcal{S}^{\mathrm{keep}}\) such that, for every \((s,h)\in\mathcal{S}^{\mathrm{keep}}\), we have \(W_{h}^{\star}(s)\geq\epsilon_{\mathrm{unif}}\), and, if \((s,h)\not\in\mathcal{S}^{\mathrm{keep}}\), then \(W_{h}^{\star}(s)\leq 32\epsilon_{\mathrm{unif}}\)._

Proof.: As in Lemma 36, by Theorem 13 of [46], with probability at least \(1-\delta/SH\), for any \((s,h)\):

* Learn2Explore will run for at most \(\mathrm{poly}(S,A,H,\log\frac{1}{\delta\epsilon_{\mathrm{unif}}})\cdot\frac{1} {\epsilon_{\mathrm{unif}}}\) episodes.
* Rerunning every policy in \(\Pi_{j_{s}}\) once, with probability at least \(1/2\) we will collect \(N=2^{-j_{s}}|\Pi_{j_{s}}|\) samples from \((s,a,h)\).
* If \((s,a)\not\in\mathcal{X}_{j}\) for all \(j=1,2,\dots,\lceil\log 1/\epsilon_{\mathrm{unif}}\rceil\), then \(W_{h}^{\star}(s)\leq 32\epsilon_{\mathrm{unif}}\).

We union bound over this event holding for all \((s,h)\), which occurs with probability at least \(1-\delta\).

It is immediate by the last property that, if \((s,h)\not\in\mathcal{S}^{\mathrm{keep}}\) then \(W_{h}^{\star}(s)\leq 32\epsilon_{\mathrm{unif}}\).

We next show that if \((s,h)\in\mathcal{S}^{\mathrm{keep}}\), then this implies that \(W_{h}^{\star}(s)\geq\epsilon_{\mathrm{unif}}\). Let \(X\) be a random variable denoting the total number of samples we collect from \((s,a,h)\) when rerunning all policies in \(\Pi_{j_{s}}\). Then by Markov's Inequality, by the above properties we have

\[\frac{1}{2}\leq\mathbb{P}[X\geq N_{j_{s}}/2]\leq\frac{2\mathbb{E}[X]}{N_{j_{s }}}\leq\frac{2|\Pi_{j_{s}}|W_{h}^{\star}(s)}{N_{j_{s}}}=8\cdot 2^{j_{s}}W_{h}^{ \star}(s).\]

It follows that

\[W_{h}^{\star}(s)\geq\frac{1}{16\cdot 2^{j_{s}}}\geq\frac{1}{16\cdot 2^{\lceil \log_{2}\frac{1}{32\epsilon_{\mathrm{unif}}}\rceil}}\geq\frac{1}{32\cdot 2^{ \log_{2}\frac{1}{32\epsilon_{\mathrm{unif}}}}}=\epsilon_{\mathrm{unif}}.\]

This completes the proof.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes]Justification: These can be found in the abstract and the contributions section of the introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: These can be found in the Discussion section of the main body of the paper. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: All proofs are found in the Appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * The authors should use the same notation as the proof of the paper.

* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: The contributions of this paper are entirely theoretical. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: The contributions of this paper are entirely theoretical. Guidelines: * The answer NA means that paper does not include experiments requiring code.

* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: The contributions of this paper are entirely theoretical. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: The contributions of this paper are entirely theoretical. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.

* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: The contributions of this paper are entirely theoretical. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: There are no human subjects and we discuss the ethical consequences in the "Broader Impact" section of the discussion. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: This theoretical paper poses minimal public concerns but holds significant potential to inspire advancements in algorithm development, contributing positively to the field of machine learning. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The contributions are theoretical. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The contributions are theoretical, and we do not use any such assets. For prior theoretical work, we have credited the authors appropriately. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets**Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The contributions are theoretical. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: No human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: No human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.