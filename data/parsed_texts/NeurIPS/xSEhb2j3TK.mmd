# Act As You Wish: Fine-Grained Control of Motion Diffusion Model with Hierarchical Semantic Graphs

 Peng Jin\({}^{1,4}\) Yang Wu\({}^{3}\) Yanbo Fan\({}^{3}\) Zhongqian Sun\({}^{3}\) Yang Wei\({}^{3}\) Li Yuan\({}^{1,2,4}\)

\({}^{1}\) School of Electronic and Computer Engineering, Peking University, Shenzhen, China

\({}^{2}\) Peng Cheng Laboratory, Shenzhen, China \({}^{3}\) Tencent AI Lab, China

\({}^{4}\)AI for Science (AI4S)-Preferred Program, Peking University Shenzhen Graduate School, China

jp21@stu.pku.edu.cn dylan.yangwu@qq.com yuanli-ece@pku.edu.cn

Corresponding author: Yang Wu, Li Yuan.

###### Abstract

Most text-driven human motion generation methods employ sequential modeling approaches, e.g., transformer, to extract sentence-level text representations automatically and implicitly for human motion synthesis. However, these compact text representations may overemphasize the action names at the expense of other important properties and lack fine-grained details to guide the synthesis of subtly distinct motion. In this paper, we propose hierarchical semantic graphs for fine-grained control over motion generation. Specifically, we disentangle motion descriptions into hierarchical semantic graphs including three levels of motions, actions, and specifics. Such global-to-local structures facilitate a comprehensive understanding of motion description and fine-grained control of motion generation. Correspondingly, to leverage the coarse-to-fine topology of hierarchical semantic graphs, we decompose the text-to-motion diffusion process into three semantic levels, which correspond to capturing the overall motion, local actions, and action specifics. Extensive experiments on two benchmark human motion datasets, including HumanML3D and KIT, with superior performances, justify the efficacy of our method. More encouragingly, by modifying the edge weights of hierarchical semantic graphs, our method can continuously refine the generated motion, which may have a far-reaching impact on the community. Code and pre-trained weights are available at https://github.com/jpthu17/GraphMotion.

## 1 Introduction

Human motion generation is a fundamental task in computer animation [54; 4] and has many practical applications across various industries including gaming, film production, virtual reality, and robotics [61; 8]. With the progress made in recent years, text-driven human motion generation has allowed for the synthesis of a variety of human motion sequences based on natural language descriptions. Therefore, there is a growing interest in generating manipulable, plausible, diverse, and realistic sequences of human motion from flexible natural language descriptions.

Existing text-to-motion generation methods [42; 61; 8; 1; 63] mainly rely on sentence-level representations of texts and directly learn the mapping from the high-level language space to the motion sequences. Recently, some works [54; 8; 62] propose conditional diffusion models for human motion synthesis and further improve the synthesized quality and diversity. Although these methods have made encouraging progress, they are still deficient in the following two aspects. (i) **Imbalance.** The model, which directly uses the transformers [55] to extract text features automatically and implicitly, may overemphasize the action names at the expense of other important properties like direction andintensity. As a typical consequence of this unbalanced learning, the network is insensitive to the subtle changes in the input text and lacks fine-grained controllability. (ii) **Coarseness.** On the one hand, motion descriptions frequently refer to multiple actions and attributes. However, the compact sentence-level representations extracted by current works usually fail to convey the clarity and detail needed to fully understand the text, leading to a lack of fine-grained details to guide the synthesis of subtly distinct motion. On the other hand, mapping directly of existing works from the high-level language space to motion sequences further hinders the generation of fine-grained details. Therefore, we argue that it is time to seek a more precise and detailed text-driven human motion generation method to ensure an accurate synthesis of complex human motions.

To this end, we propose a more fine-grained control signal, hierarchical semantic graphs, to represent different intentions for controllable motion generation and design a coarse-to-fine motion diffusion model, called GraphMotion. As shown in Fig. 1, motion descriptions inherently possess hierarchical structures and can be represented as hierarchical graphs composed of three types of abstract nodes, namely motions, actions, and specifics. Concretely, the overall sentence describes the global motion involving multiple actions, e.g., "walk", "pick", and "stand" in Fig. 1, which occur in sequential order. Each action consists of different specifics that act as its attributes, such as the agent and patient of the action. Such global-to-local structures contribute to a reliable and comprehensive understanding of motion descriptions. Correspondingly, to take full advantage of this fine-grained control signal, we decompose the text-to-motion diffusion process into three semantic levels from coarse to fine, which are responsible for capturing the overall motion, local actions, and action specifics, respectively.

The proposed GraphMotion has three compelling advantages: **First**, the explicit factorization of the language embedding space enables us to build a fine-grained correspondence between textual data and motion sequences, which avoids the imbalanced learning of different textual components and coarse-grained control signal representation. **Second**, the hierarchical refinement property of GraphMotion allows the model to progressively enhance the generated results from coarse to fine, which avoids the coarse-grained generated results. **Third**, to further fine-tune the generated results for more fine-grained control, our method can continuously refine the generated motion by modifying the edge weights of the hierarchical semantic graph. Experimental results on two benchmark datasets for text-to-motion generation, including HumanML3D [14] and KIT [43], demonstrate the advantages of GraphMotion. The main contributions of this work are as follows:

* To the best of our knowledge, we are the first to propose hierarchical semantic graphs, a fine-grained control signal, for text-to-motion generation. It decomposes motion descriptions into global-to-local three types of abstract nodes, namely motions, actions, and specifics.
* Correspondingly, we decompose the text-to-motion diffusion process into three semantic levels. This allows the model to gradually refine results from coarse to fine. Experiments show that our method achieves new state-of-the-art results on two text-to-motion datasets.
* More encouragingly, by modifying the edge weights of hierarchical semantic graphs, our method can continuously refine the generated motion, which may have a far-reaching impact.

Figure 1: **We propose hierarchical semantic graphs, a fine-grained control signal, for text-to-motion generation and factorize text-to-motion generation into hierarchical levels including motions, actions, and specifics to form a coarse-to-fine structure. This approach enhances the fine-grained correspondence between textual data and motion sequences and achieves better controllability conditioning on hierarchical semantic graphs than carefully designed baselines.**

Related Work

Text-driven Human Motion Generation.Text-driven human motion generation aims to generate 3D human motion based on text descriptions. Due to the user-friendliness and convenience of natural language [22], text-driven human motion generation is gaining significant attention and has many applications. Recently, two categories of motion generation methods have emerged: joint-latent models [2; 42] and diffusion models [8; 62; 47]. Joint-latent models, e.g., TEMOS [42], typically learn a motion variational autoencoder and a text variational autoencoder. These models then constrain the text and motion encoders into a shared latent space using the Kullback-Leibler divergences [29] loss. The latter category of methods, e.g., MDM [54], proposes a conditional diffusion model for human motion generation to learn a powerful probabilistic mapping from the textual descriptors to human motion sequences. Although these methods have made encouraging progress, they still suffer from two major deficiencies: unbalanced text learning and coarse-grained generated results. In this paper, we propose to leverage the inherent structure of language [6; 58] for motion generation. Specifically, we introduce hierarchical semantic graphs as a more effective control signal for representing different intentions and design a coarse-to-fine motion diffusion model using this signal.

Diffusion Generative Models.Diffusion generative models [49; 18; 11; 21; 50] are a type of neural generative model that uses the stochastic diffusion process, which is based on thermodynamics. The process involves gradually adding noise to a sample from the data distribution, and then training a neural network to reverse this process by gradually removing the noise. In recent years, diffusion models have shown promise in a variety of tasks such as image generation [18; 50; 11; 19; 57], natural language generation [3; 36; 13], as well as visual tasks [9]. Some other works [25; 32] have attempted to adapt diffusion models for cross-modal retrieval [33; 34; 35; 23; 24]. Inspired by the success of diffusion generative models, some works [62; 54; 8] have applied diffusion models to human motion generation. However, these methods typically learn a one-stage mapping from the high-level language space to motion sequences, which hinders the generation of fine-grained details. In this paper, we decompose the text-to-motion diffusion process into three semantic levels from coarse to fine. The resultant levels are responsible for capturing overall motion, local actions, and action specifics, which enhances the generated results progressively from coarse to fine.

Graph-based Reasoning.The graph convolutional network [28] is originally proposed to recognize graph data. It uses convolution on the neighborhoods of each node to produce outputs. Graph attention networks [56] further enhance graph-based reasoning by dynamically attending to the features of neighborhoods. The graph-based reasoning has shown great potential in many tasks, such as scene graph generation [60], visual question answering [20; 31; 30], natural language generation [6], and cross-modal retrieval [7]. In this paper, we focus on reasoning over hierarchical semantic graphs on motion descriptions for fine-grained control of human motion generation.

## 3 Methodology

In this paper, we tackle the tasks of text-driven human motion generation. Concretely, given an arbitrary motion description, our goal is to synthesize a human motion \(x^{1:L}=\{x^{i}\}_{i=1}^{L}\) of length \(L\). The overview of the proposed GraphMotion is shown in Fig. 2.

### Hierarchical Semantic Graph Modeling

Existing works for text-driven human motion generation typically directly use the transformer [55] to extract text features automatically and implicitly. However, motion descriptions inherently possess hierarchical structures that can be divided into three sorts of abstract nodes, including motions, actions, and specifics. Compared with the sequential structure, such global-to-local structure contributes to a reliable and comprehensive understanding of the semantic meaning of motion descriptions and is a promising fine-grained control signal for text-to-motion generation.

Semantic Role Parsing.To obtain actions, attributes of action as well as the semantic role of each attribute to the corresponding action, we implement a semantic parser of motion descriptions based on a semantic role parsing toolkit [48; 7]. We extract three types (motions, actions, and specifics) of nodes and twelve types of edges to represent various associations among the nodes. For details about the types of nodes and edges, please refer to our supplementary material.

Specifically, given the motion description, the parser extracts verbs that appeared in the sentence and attribute phrases corresponding verb, and the semantic role of each attribute phrase. The overall sentence is treated as the global motion node in the hierarchical graph. The verbs are considered as action nodes and connected to the motion node with direct edges, allowing for implicit learning of the temporal relationships among various actions during graph reasoning. The attribute phrases are specific nodes that are connected with action nodes. The edge type between action and specific nodes is determined by the semantic role of the specifics in relation to the action.

Graph Node Representation.Given the motion description, we follow previous works [53, 54, 61, 8] and leverage the text encoder of CLIP [45] to extract the text embedding. For the global motion node \(v^{m}\), we utilize the [CLS] token to summarize the salient event described in the sentence. For the action node \(v^{a}\), we use the token of the corresponding verb as the action node representation. For the specific node \(v^{s}\), we apply mean-pooling over tokens of each word in the attribute phrase.

Action-aware Graph Reasoning.The interactions across different levels in the constructed graph not only explain the properties of local actions and how local actions compose the global motion, but also reduce ambiguity at each node. For example, the verb "pick" in Fig. 1 can represent different actions without context, but the context "with both hands" constrains its semantics, so that it represents the action of "pick up with both hands" rather than "pick up with one hand." Therefore, we propose to reason over interactions in the graph to obtain hierarchical textual representations.

We utilize graph attention networks [56] (GAT) to model interactions in a graph. Specifically, given the initialized node embeddings \(v=\{v^{m},v^{a},v^{s}\}\), we first transform the input node embeddings into higher-level embeddings \(h=\{h^{m},h^{a},h^{s}\}\) by:

\[h^{m}=\bm{W}v^{m},\quad h^{a}=\bm{W}v^{a},\quad h^{s}=\bm{W}v^{s},\] (1)

where \(\bm{W}\in\mathbb{R}^{D\times D}\) is a shared linear transformation and \(D\) is the dimension of node representation. For each pair \(\{h_{i},h_{j}\}\) of connected nodes, we concatenate the node \(h_{i}\in\mathbb{R}^{D}\) with its neighbor node \(h_{j}\in\mathbb{R}^{D}\), generating the input data \(\tilde{h}_{ij}=[h_{i},h_{j}]\in\mathbb{R}^{2D}\) of the graph attention module.

However, in a graph with multiple types of edges, the vanilla graph networks need to learn separate transformation matrices for each edge type. This can be inefficient when learning from limited motion data, and prone to over-fitting on rare edge types.

To this end, we propose to factorize multi-relational weights into two parts: a common transformation matrix \(\bm{M}\in\mathbb{R}^{2D\times 1}\) that is shared for all edge types and a relationship embedding matrix \(\bm{M}_{\bm{r}}\in\mathbb{R}^{2D\times N}\) that is specific for different edges, where \(N\) is the number of edge types. Following GAT [56], we apply LeakyReLU [39] in the calculation of attention coefficients and set the negative input slope to 0.2. The attention coefficient \(\tilde{e}_{ij}\) is formulated as:

\[e_{ij}=\text{LeakyReLU}(\bm{M}^{\top}\tilde{h}_{ij})+\text{ LeakyReLU}(R_{ij}\bm{M}_{\bm{r}}^{\top}\tilde{h}_{ij}),\quad\tilde{e}_{ij}= \frac{\text{exp}(e_{ij})}{\sum_{k\in N_{i}}\text{exp}(e_{ik})},\] (2)

Figure 2: **The overview of the proposed GraphMotion for text-driven human motion generation.** We factorize motion descriptions into hierarchical semantic graphs including three levels of motions, actions, and specifics. Correspondingly, we decompose the text-to-motion diffusion process into three semantic levels, which correspond to capturing the overall motion, local actions, and action specifics.

where \(R_{ij}\in\mathbb{R}^{1\times N}\) is a one-hot vector denoting the type of edge between node \(i\) and \(j\). \(\mathbb{N}_{i}\) is the set of neighborhood nodes of node \(i\). To alleviate over-smoothing [59] in graph networks, we apply skip connection when calculating output embeddings. The output embeddings \(\mathcal{V}\) are formulated as:

\[\mathcal{V}_{i}=\sigma\big{(}\sum_{j\in\mathbb{N}_{i}}\tilde{e}_{ij}h_{j} \big{)}+v_{i},\] (3)

where \(\sigma\) is a nonlinear function. Following GAT [56], we use ELU [10] as the nonlinear function \(\sigma\).

### Coarse-to-Fine Motion Diffusion Model for Graphs

To leverage the coarse-to-fine topology of hierarchical semantic graphs during generation, we also decompose the text-to-motion diffusion process into three semantic levels, which correspond to capturing the overall motion, local actions, and action specifics. During the reverse denoising process, the fine-grained semantic layer generates results based on the results from the coarse-grained semantic layer. This allows for a detailed and plausible representation of the intended motion.

**Motion Representation.** Following previous works [42; 8; 61], we first encode the motion into the latent space with a motion variational autoencoder [27] and then use diffusion models to learn the mapping from hierarchical semantic graphs to the motion latent space.

Specifically, we build the motion encoder \(\mathcal{E}\) and decoder \(\mathcal{D}\) based on the transformer [55; 41]. For the motion encoder \(\mathcal{E}\), we take \(C\) learnable query tokens and motion sequence \(x^{1:L}=\{x^{i}\}_{i=1}^{L}\) as inputs to generate motion latent embeddings \(z\in\mathbb{R}^{C\times D^{\prime}}\), where \(D^{\prime}\) is the dimension of latent representation. For the motion decoder \(\mathcal{D}\), we take the latent embeddings \(z\in\mathbb{R}^{C\times D^{\prime}}\) and the motion query tokens as the input to generate a human motion sequence \(x^{1:L}=\{x^{i}\}_{i=1}^{L}\) with \(L\) frames.

The loss \(\mathcal{L}_{\text{VAE}}\) of the motion variational autoencoder can be divided into two parts. First, we use the mean squared error (MSE) to reconstruct the original input. Second, we use the Kullback-Leibler divergences (KL) loss [29] to narrow the distance between the distribution of latent space \(q(z|x)\) and the standard Gaussian distribution \(\mathcal{N}(0,\text{I})\). The full loss \(\mathcal{L}_{\text{VAE}}\) is formulated as:

\[\mathcal{L}_{\text{VAE}}=\mathbb{E}_{x\sim q(x)}\Big{[}\underbrace{\|x- \mathcal{D}(\mathcal{E}(x))\|_{2}^{2}}_{\text{MSE}}+\lambda\underbrace{ \text{KL}(\mathcal{N}(0,\text{I})\|q(z|x))}_{\text{KL}}\Big{]},\] (4)

where \(\lambda\) is the trade-off hyper-parameter. \(q(z|x)=\mathcal{N}(\mu_{z},\Sigma_{z})\) is obtained by sampling based on the mean \(\mu_{z}\) and variance \(\Sigma_{z}\) estimated by the model. To generate motion from coarse to fine step by step, we encode motion independently into three latent representation spaces \(z^{m}\in\mathbb{R}^{C^{m}\times D^{\prime}}\), \(z^{a}\in\mathbb{R}^{C^{a}\times D^{\prime}}\) and \(z^{s}\in\mathbb{R}^{C^{s}\times D^{\prime}}\), where the number of tokens gradually increases, i.e., \(C^{m}\leq C^{a}\leq C^{s}\).

**Hierarchical Graph-to-Motion Diffusion.** Corresponding to the three-level structure of the hierarchical semantic graphs, we decompose the diffusion process into three semantic levels and build three transformer-based denoising models, which correspond to motions, actions, and specifics.

For the motion level model \(\phi_{m}\), our goal is to learn the diffusion process from global motion node \(\mathcal{V}^{m}\) to motion latent representation \(z^{m}\). In a forward diffusion process \(q(z_{t}^{m}|z_{t-1}^{m})\), noised sampled from Gaussian distribution is added to a ground truth data distribution \(z_{0}^{m}\) at every noise level \(t\):

\[q(z_{t}^{m}|z_{t-1}^{m})=\mathcal{N}(z_{t}^{m};\sqrt{1-\beta_{t}}z_{t-1}^{m}, \beta_{t}\text{I}),\quad q(z_{1:T}^{m}|z_{0}^{m})=\prod_{t=1}^{T}q(z_{t}^{m}|z _{t-1}^{m}),\] (5)

where \(\beta_{t}\) is the step size which gradually increases. \(T\) is the length of the Markov chain. We sample \(z_{t}^{m}\) by \(z_{t}^{m}=\sqrt{\bar{\alpha}_{t}}z_{0}^{m}+\sqrt{1-\bar{\alpha}_{t}}\epsilon^{m}\), where \(\bar{\alpha}_{t}=\prod_{i=1}^{t}(1-\beta_{i})\). \(\epsilon^{m}\) is a noise sampled from \(\mathcal{N}(0,1)\). We follow previous works [18; 8] and predict the noise component \(\epsilon^{m}\), i.e., \(\widehat{\epsilon^{m}}=\phi_{m}(z^{m},t^{m},\mathcal{V}^{m})\).

For the action level model \(\phi_{a}\), to leverage the results generated by the motion level, we concatenate the action node \(\mathcal{V}^{a}\), the motion node \(\mathcal{V}^{m}\), and the result \(z^{m}\) generated by the motion level together as the input of the action level denoising network, i.e., \(\widehat{\epsilon}^{a}=\phi_{a}(z^{a},t^{a},[\mathcal{V}^{m},\mathcal{V}^{a},z ^{m}])\).

For the specific level model \(\phi_{s}\), we leverage the results generated by the action level and nodes at all semantic levels to predict the noise component, i.e., \(\widehat{\epsilon}^{s}=\phi_{s}(z^{s},t^{s},[\mathcal{V}^{m},\mathcal{V}^{a}, \mathcal{V}^{s},z^{a}])\).

[MISSING_PAGE_FAIL:6]

**HumanML3D**[14] is currently the largest 3D human motion dataset that originates from and textually reannotates the HumanAct12 [16] and AMASS [40] datasets. This dataset comprises 14,616 human motions and 44,970 text descriptions, with each motion accompanied by at least three precise descriptions. The lengths of these descriptions are around 12 words. **KIT**[43] contains 3,911 human motion sequences and 6,278 textual annotations. Each motion sequence is accompanied by one to four sentences, with an average description length of 8 words.

_Metrics._ Following previous works, we use the following five metrics to measure the performance of the model. (1) **R-Precision.** Under the feature space of the pre-trained network in [14], given one motion sequence and 32 text descriptions (1 ground-truth and 31 randomly selected mismatched descriptions), motion-retrieval precision calculates the text and motion Top 1/2/3 matching accuracy. (2) **Frechet Inception Distance (FID).** We measure the distribution distance between the generated and real motion using FID [17] on the extracted motion features [14]. (3) **Multimodal Distance (MM-Dist).** We calculate the average Euclidean distances between each text feature and the generated motion feature from that text. (4) **Diversity.** All generated motions are randomly sampled to two subsets of the same size. Then, we extract motion features [14] and compute the average Euclidean distances between the two subsets. (5) **Multimodality (MModality).** For each text description, we generate 20 motion sequences, forming 10 pairs of motions. We extract motion features and calculate the average Euclidean distance between each pair. We report the average of all text descriptions.

_Implementation Details._ For the motion variational autoencoder, motion encoder \(\mathcal{E}\) and decoder \(\mathcal{D}\) all consist of 9 layers and 4 heads with skip connection [46]. Following MLD [8], we utilize a frozen text encoder of the CLIP-ViT-L-14 [45] model for text representation. The dimension of node representation \(D\) is set to 768. The dimension of latent embedding \(D^{\prime}\) is set to 256. We set the token sizes \(C^{m}\) to 2, \(C^{a}\) to 4, and \(C^{s}\) to 8. We set \(\lambda\) to 1e-4. All our models are trained with the AdamW [26, 38] optimizer using a fixed learning rate of 1e-4. We use 4 Tesla V100 GPUs for the training, and there are 128 samples on each GPU, so the total batch size is 512. For the HumanML3D dataset, the model is trained for 6,000 epochs during the motion variational autoencoder stage and 3,000 epochs during the diffusion stage. The number of diffusion steps of each level is 1,000 during training, and the step sizes \(\beta_{i}\) are scaled linearly from \(8.5\times 1\)e-4 to 0.012. For runtime, training tasks 16 hours for motion variational autoencoder and 24 hours for denoiser on 4 Tesla V100 GPUs.

**Comparisons to State-of-the-art.** We compare the proposed GraphMotion with other methods on two benchmarks. In Tab. 1, we show the results on the HumanML3D test set. Tab. 2 shows the results

Figure 3: **Qualitative comparison of the existing methods. We provide the motion results from three text prompts. The darker colors indicate the later in time. The generated results of our method better match the descriptions, while others have downgraded motions or improper semantics, demonstrating that our method achieves superior controllability compared to well-designed baseline models.**

[MISSING_PAGE_EMPTY:8]

Analysis of the coarse-to-fine motion diffusion model.In Tab. 4, we provide the ablation study of the coarse-to-fine motion diffusion model on the HumanML3D test set. These results prove that coarse-to-fine generation is beneficial to motion generation. In addition, we show the performance at each level in Fig. 4. Among the three levels, the performance of the specific level is the best, which confirms the effectiveness of the coarse-to-fine motion diffusion model.

Effect of the diffusion steps.In Tab. 5, we show the ablation study of the total number of diffusion steps on the HumanML3D test set. Following MLD [8], we adopt the denoising diffusion implicit models [50] (DDIM) during interference. As shown in Tab. 5, our method consistently outperforms the existing state-of-the-art methods with the same total number of diffusion steps, which demonstrates the efficiency of our method. With the increase of the total diffusion steps, the performance of our method is further improved, while the performance of MLD saturates. We find that the number of diffusion steps at the higher level (e.g., specific level) has a greater impact on the result. Therefore, in scenarios requiring high efficiency, we recommend allocating more diffusion steps to the higher level.

Quantitative and Qualitative Discussion._Quantitative experiment of the imbalance problem._ In this experiment, we mask the verbs and action names in the motion description to force the model to generate motion only from action specifics. For example, given the motion description "a person walks several steps forward in a straight line.", we would mask "walks". Transformer extracts text features automatically and implicitly. However, it may encourage the model to take shortcuts, such as overemphasizing the action name "walks" at the expense of other important properties. Therefore, when the verbs and action names are masked, the other models, which directly use the transformer

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Methods & FID \(\downarrow\) & MM-Dist \(\downarrow\) & Diversity \(\rightarrow\) & MModality \(\uparrow\) \\ \hline MDM & 5.622 & 7.163 & 8.713 & 3.578 \\ MLD & 3.492 & 5.632 & 8.874 & 3.596 \\ GraphMotion & \(\mathbf{1.826}\) & \(\mathbf{5.530}\) & \(\mathbf{9.284}\) & \(\mathbf{3.699}\) \\ \hline \hline \end{tabular}
\end{table}
Table 6: **Quantitative experiment of the imbalance problem on the HumanML3D test set.** “\(\uparrow\)” denotes that higher is better. “\(\downarrow\)” denotes that lower is better.

\begin{table}
\begin{tabular}{l c c} \hline \hline Methods Compared & Preference Rate \\ \hline GraphMotion vs. MotionDiffuse & 64.10\% \\ GraphMotion vs. MLD & 56.41\% \\ GraphMotion vs. Ground Truth & 48.72\% \\ \hline \hline \end{tabular}
\end{table}
Table 7: **User studies for quantitative comparison. We show the preference rate of GraphMotion over the compared model.**

Figure 5: **Qualitative analysis of refining motion results. The darker colors indicate the later in time. The trajectory of the human body is indicated by an arrow. The trajectory associated with the modified edge is highlighted in red, and other parts are identified in blue.**

to extract text features, fail to generate motion well. By contrast, the hierarchical semantic graph explicitly extracts the action specifics. The explicit factorization of the language embedding space facilitates a comprehensive understanding of motion description. It allows the model to infer from action specifics such as "several steps forward" and "in a straight line" that the overall motion is "walking forward". As shown in Tab. 6, our method can synthesize motion by relying only on action specifics, while other methods fail to generate motion well. These results indicate that our method avoids the imbalance problem of other methods.

_Human evaluation._ In our evaluation, we randomly selected 39 motion descriptions for the user study. As shown in Tab. 7, GraphMotion is preferred over the other models most of the time.

_Qualitative analysis of refining motion results._ To fine-tune the generated results for more fine-grained control, our method can continuously refine the generated motion by modifying the edge weights of the hierarchical semantic graph. As illustrated in Fig. 5, we can alter the action attributes by manipulating the weights of the edges of the action node and the specific node. For example, by increasing the weights of the edges of "zags" and "to left", the human body will move farther to the left. Moreover, by fine-tuning the weights of the edges of the global motion node and the action node, we can fine-tune the duration of the corresponding action in the whole motion. For example, by enhancing the weights of the edges of the global motion node and "walking", the length of the walk will be increased. In Fig. 6, we provide additional qualitative analysis of refining motion results. Specifically, we perform the additional operations on the hierarchical semantic graphs: (1) masking the node by replacing it with the MASK token; (2) modifying the node; (3) deleting nodes; (4) adding a new node. The qualitative results demonstrate that our approach provides a novel method of refining generated motions, which may have a far-reaching impact on the community.

## 5 Conclusion

In this paper, we focus on improving the controllability of text-driven human motion generation. To provide fine-grained control over motion details, we propose a novel control signal called the hierarchical semantic graph, which consists of three kinds of abstract nodes, namely motions, actions, and specifics. Correspondingly, to leverage the coarse-to-fine topology of hierarchical semantic graphs, we decompose the text-to-motion diffusion process into three semantic levels, which correspond to capturing the overall motion, local actions, and action specifics. Extensive experiments demonstrate that our method achieves better controllability than the existing state-of-the-art methods. More encouragingly, our method can continuously refine the generated motion by modifying the edge weights of hierarchical semantic graphs, which may have a far-reaching impact on the community.

**Acknowledgements.** This work was supported by the National Key R&D Program of China (2022ZD0118101), Nature Science Foundation of China (No.62202014), and Shenzhen Basic Research Program (No.JCYJ20220813151736001).

Figure 6: **Additional qualitative analysis of refining motion results. The qualitative results demonstrate that our approach provides a novel method of refining generated motions.**

## References

* [1] Hyemin Ahn, Timothy Ha, Yunho Choi, Huviyeon Yoo, and Songhwai Oh. Text2action: Generative adversarial synthesis from language to action. In _ICRA_, pages 5915-5920, 2018.
* [2] Chaitanya Ahuja and Louis-Philippe Morency. Language2pose: Natural language grounded pose forecasting. In _3DV_, pages 719-728, 2019.
* [3] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. In _NeurIPS_, pages 17981-17993, 2021.
* [4] Norman I Badler, Cary B Phillips, and Bonnie Lynn Webber. _Simulating humans: computer graphics animation and control_. Oxford University Press, 1993.
* [5] Uttaran Bhattacharya, Nicholas Rewkowski, Abhishek Banerjee, Pooja Guhan, Aniket Bera, and Dinesh Manocha. Text2gestures: A transformer-based network for generating emotive body gestures for virtual agents. In _VR_, pages 1-10, 2021.
* [6] Shizhe Chen, Qin Jin, Peng Wang, and Qi Wu. Say as you wish: Fine-grained control of image caption generation with abstract scene graphs. In _CVPR_, pages 9962-9971, 2020.
* [7] Shizhe Chen, Yida Zhao, Qin Jin, and Qi Wu. Fine-grained video-text retrieval with hierarchical graph reasoning. In _CVPR_, pages 10638-10647, 2020.
* [8] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, Jingyi Yu, and Gang Yu. Executing your commands via motion diffusion in latent space. In _CVPR_, pages 18000-18010, 2023.
* [9] Zesen Cheng, Kehan Li, Peng Jin, Xiangyang Ji, Li Yuan, Chang Liu, and Jie Chen. Parallel vertex diffusion for unified visual grounding. _arXiv preprint arXiv:2303.07216_, 2023.
* [10] Djork-Arne Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). _arXiv preprint arXiv:1511.07289_, 2015.
* [11] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In _NeurIPS_, pages 8780-8794, 2021.
* [12] Anindita Ghosh, Noshaba Cheema, Cennet Oguz, Christian Theobalt, and Philipp Slusallek. Synthesis of compositional animations from textual descriptions. In _ICCV_, pages 1396-1406, 2021.
* [13] Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, and LingPeng Kong. Diffuseq: Sequence to sequence text generation with diffusion models. _arXiv preprint arXiv:2210.08933_, 2022.
* [14] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and natural 3d human motions from text. In _CVPR_, pages 5152-5161, 2022.
* [15] Chuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng. Tm2t: Stochastic and tokenized modeling for the reciprocal generation of 3d human motions and texts. In _ECCV_, pages 580-597, 2022.
* [16] Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao Sun, Annan Deng, Minglun Gong, and Li Cheng. Action2motion: Conditioned generation of 3d human motions. In _ACM MM_, pages 2021-2029, 2020.
* [17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In _NeurIPS_, 2017.
* [18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In _NeurIPS_, pages 6840-6851, 2020.
* [19] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022.

* [20] Ronghang Hu, Anna Rohrbach, Trevor Darrell, and Kate Saenko. Language-conditioned graph networks for relational reasoning. In _ICCV_, pages 10294-10303, 2019.
* [21] Hyeonho Jeong, Gihyun Kwon, and Jong Chul Ye. Zero-shot generation of coherent storybook from plain text story using diffusion models. _arXiv preprint arXiv:2302.03900_, 2023.
* [22] Peng Jin, Jinfa Huang, Fenglin Liu, Xian Wu, Shen Ge, Guoli Song, David Clifton, and Jie Chen. Expectation-maximization contrastive learning for compact video-and-language representations. In _NeurIPS_, pages 30291-30306, 2022.
* [23] Peng Jin, Jinfa Huang, Pengfei Xiong, Shangxuan Tian, Chang Liu, Xiangyang Ji, Li Yuan, and Jie Chen. Video-text as game players: Hierarchical banzhaf interaction for cross-modal representation learning. In _CVPR_, pages 2472-2482, 2023.
* [24] Peng Jin, Hao Li, Zesen Cheng, Jinfa Huang, Zhennan Wang, Li Yuan, Chang Liu, and Jie Chen. Text-video retrieval with disentangled conceptualization and set-to-set alignment. In _IJCAI_, pages 938-946, 8 2023.
* [25] Peng Jin, Hao Li, Zesen Cheng, Kehan Li, Xiangyang Ji, Chang Liu, Li Yuan, and Jie Chen. Diffusionnet: Generative text-video retrieval with diffusion model. In _ICCV_, pages 2470-2481, 2023.
* [26] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [27] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_, 2013.
* [28] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In _ICLR_, 2017.
* [29] Solomon Kullback. _Information theory and statistics_. Courier Corporation, 1997.
* [30] Hao Li, Xu Li, Belhal Karimi, Jie Chen, and Mingming Sun. Joint learning of object graph and relation graph for visual question answering. In _ICME_, pages 01-06, 2022.
* [31] Linjie Li, Zhe Gan, Yu Cheng, and Jingjing Liu. Relation-aware graph attention network for visual question answering. In _ICCV_, pages 10313-10322, 2019.
* [32] Pandeng Li, Chen-Wei Xie, Hongtao Xie, Liming Zhao, Lei Zhang, Yun Zheng, Deli Zhao, and Yongdong Zhang. Momentdiff: Generative video moment retrieval from random to real. _arXiv preprint arXiv:2307.02869_, 2023.
* [33] Pandeng Li, Chen-Wei Xie, Liming Zhao, Hongtao Xie, Jiannan Ge, Yun Zheng, Deli Zhao, and Yongdong Zhang. Progressive spatio-temporal prototype matching for text-video retrieval. In _ICCV_, pages 4100-4110, 2023.
* [34] Pandeng Li, Hongtao Xie, Jiannan Ge, Lei Zhang, Shaobo Min, and Yongdong Zhang. Dual-stream knowledge-preserving hashing for unsupervised video retrieval. In _ECCV_, pages 181-197, 2022.
* [35] Pandeng Li, Hongtao Xie, Shaobo Min, Jiannan Ge, Xun Chen, and Yongdong Zhang. Deep fourier ranking quantization for semi-supervised image retrieval. _TIP_, 31:5909-5922, 2022.
* [36] Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori B Hashimoto. Diffusion-lm improves controllable text generation. _arXiv preprint arXiv:2205.14217_, 2022.
* [37] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J Black. Smpl: A skinned multi-person linear model. _TOG_, 34(6):1-16, 2015.
* [38] Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. 2017.
* [39] Andrew L Maas, Awni Y Hannun, Andrew Y Ng, et al. Rectifier nonlinearities improve neural network acoustic models. In _ICML_, 2013.

* Mahmood et al. [2019] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Gerard Pons-Moll, and Michael J Black. Amass: Archive of motion capture as surface shapes. In _ICCV_, pages 5442-5451, 2019.
* Petrovich et al. [2021] Mathis Petrovich, Michael J Black, and Gul Varol. Action-conditioned 3d human motion synthesis with transformer vae. In _ICCV_, pages 10985-10995, 2021.
* Petrovich et al. [2022] Mathis Petrovich, Michael J Black, and Gul Varol. Temos: Generating diverse human motions from textual descriptions. In _ECCV_, pages 480-497, 2022.
* Plappert et al. [2016] Matthias Plappert, Christian Mandery, and Tamim Asfour. The kit motion-language dataset. _Big data_, 4(4):236-252, 2016.
* Plappert et al. [2018] Matthias Plappert, Christian Mandery, and Tamim Asfour. Learning a bidirectional mapping between human whole-body motion and natural language using deep recurrent neural networks. _Robotics and Autonomous Systems_, 109:13-26, 2018.
* Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In _ICML_, pages 8748-8763, 2021.
* Ronneberger et al. [2015] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In _MICCAI_, pages 234-241, 2015.
* Shafir et al. [2023] Yonatan Shafir, Guy Tevet, Roy Kapon, and Amit H Bermano. Human motion diffusion as a generative prior. _arXiv preprint arXiv:2303.01418_, 2023.
* Shi and Lin [2019] Peng Shi and Jimmy Lin. Simple bert models for relation extraction and semantic role labeling. _arXiv preprint arXiv:1904.05255_, 2019.
* Sohl-Dickstein et al. [2015] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _ICML_, pages 2256-2265, 2015.
* Song et al. [2020] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. _arXiv preprint arXiv:2010.02502_, 2020.
* Srivastava et al. [2014] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. _JMLR_, 15(1):1929-1958, 2014.
* Terlemez et al. [2014] Omer Terlemez, Stefan Ulbrich, Christian Mandery, Martin Do, Nikolaus Vahrenkamp, and Tamim Asfour. Master motor map (mmm)--framework and toolkit for capturing, representing, and reproducing human motion on humanoid robots. In _2014 IEEE-RAS International Conference on Humanoid Robots_, pages 894-901, 2014.
* Tevet et al. [2022] Guy Tevet, Brian Gordon, Amir Hertz, Amit H Bermano, and Daniel Cohen-Or. Motionclip: Exposing human motion generation to clip space. In _ECCV_, pages 358-374, 2022.
* Tevet et al. [2023] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit H Bermano. Human motion diffusion model. In _ICLR_, 2023.
* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _NeurIPS_, 2017.
* Velickovic et al. [2018] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. In _ICLR_, 2018.
* Wang et al. [2022] Yinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot image restoration using denoising diffusion null-space model. _arXiv preprint arXiv:2212.00490_, 2022.
* Wu et al. [2023] Yanmin Wu, Xinhua Cheng, Renrui Zhang, Zesen Cheng, and Jian Zhang. Eda: Explicit text-decoupling and dense alignment for 3d visual and language learning. In _CVPR_, 2023.
* Yang et al. [2020] Chaoqi Yang, Ruijie Wang, Shuochao Yao, Shengzhong Liu, and Tarek Abdelzaher. Revisiting over-smoothing in deep gcns. _arXiv preprint arXiv:2003.13663_, 2020.

* [60] Jianwei Yang, Jiasen Lu, Stefan Lee, Dhruv Batra, and Devi Parikh. Graph r-cnn for scene graph generation. In _ECCV_, pages 670-685, 2018.
* [61] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli Huang, Yong Zhang, Hongwei Zhao, Hongtao Lu, and Xi Shen. T2m-gpt: Generating human motion from textual descriptions with discrete representations. In _CVPR_, 2023.
* [62] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-driven human motion generation with diffusion model. _arXiv preprint arXiv:2208.15001_, 2022.
* [63] Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai, Fangzhou Hong, Huirong Li, Lei Yang, and Ziwei Liu. Remodiffuse: Retrieval-augmented motion diffusion model. _arXiv preprint arXiv:2304.01116_, 2023.

AbstractThis appendix provides additional discussions (Sec. A), implementation details (Sec. B), more qualitative results (Sec. C), several additional experiments (Sec. D), details of motion representations and metric definitions (Sec. E).

CodeCode is available at https://github.com/jpthu17/GraphMotion. In this code, we provide the process of the training and evaluation of the proposed method, and the pre-trained weights.

## Appendix A Additional Discussions

### Potential Negative Societal Impacts

While our work effectively enhances the quality of human motion synthesis, there is a potential risk that it may be used for generating fake content, such as generating fake news, which can pose a threat to information security. Moreover, when factoring in energy consumption, there is a possibility that the widespread use of generative models for synthesizing human motions may contribute to increased carbon emissions and exacerbate global warming.

### Limitations of our Work

Although our method makes some progress, there are still many limitations worth further study. (1) The proposed GraphMotion inherits the randomness of diffusion models. This property benefits diversity but may yield undesirable results sometimes. (2) The human motion synthesis capabilities of GraphMotion are limited by the performance of the pre-trained motion variational autoencoders, which we will discuss in experiments (Tab. D and Tab. E). This defect also exists in the existing state-of-the-art methods, such as MLD [8] and T2M-GPT [61], which also use motion variational autoencoder. (3) Though the proposed GraphMotion brings negligible extra cost on computations, it is still limited by the slow inference speed of existing diffusion models. We will discuss the inference time in experiments (Tab. C). This defect also exists in the existing state-of-the-art methods, such as MDM [54] and MLD [8], which also use diffusion models.

### Future Work

In this paper, we focus on improving the controllability of text-driven human motion generation. Recently, large language models have made remarkable progress, making large language models a promising text extractor for human motion generation. However, despite their strengths in general reasoning and broad applicability, large language models may not be optimized for extracting subtle motion nuances. In future research, we will incorporate the features of large-scale languages into our model, using hierarchical semantic graphs to give large language models the ability to extract fine-grained motion description structures.

## Appendix B Implementation Details

### Details of Hierarchical Semantic Graphs

To obtain actions, attributes of action as well as the semantic role of each attribute to the corresponding action, we implement a semantic parser of motion descriptions based on a semantic role parsing toolkit [48, 7]. Specifically, given the motion description, the parser extracts verbs that appeared in the sentence and attribute phrases corresponding verb, and the semantic role of each attribute phrase. The overall sentence is treated as the global motion node in the hierarchical graph. The verbs are considered as action nodes and connected to the motion node with direct edges, allowing for implicit learning of the temporal relationships among various actions during graph reasoning. The attribute phrases are specific nodes that are connected with action nodes. The edge type between action and specific nodes is determined by the semantic role of the specifics in relation to the action.

As shown in Tab. A, we extract three types (motions, actions, and specifics) of nodes and twelve types of edges to represent various associations among the nodes.

### Classifier-free Diffusion Guidance

Following MLD [8], our denoiser network is learned with classifier-free diffusion guidance [19]. The classifier-free diffusion guidance improves the quality of samples by reducing diversity in conditional diffusion models. Concretely, it learns both the conditioned and the unconditioned distribution (10% dropout [51]) of the samples. Finally, we perform a linear combination in the following manner, which is formulated as:

\[\begin{split}\widehat{\epsilon^{m}}_{scale}&= \alpha^{{}^{\prime}}\phi_{m}(z^{m},t^{m},\mathcal{V}^{m})+(1-\alpha^{{}^{ \prime}})\phi_{m}(z^{m},t^{m},\varnothing),\\ \widehat{\epsilon^{a}}_{scale}&=\alpha^{{}^{\prime }}\phi_{a}(z^{a},t^{a},[\mathcal{V}^{m},\mathcal{V}^{a},z^{m}])+(1-\alpha^{{} ^{\prime}})\phi_{a}(z^{a},t^{a},\varnothing),\\ \widehat{\epsilon^{b}}_{scale}&=\alpha^{{}^{\prime }}\phi_{s}(z^{s},t^{s},[\mathcal{V}^{m},\mathcal{V}^{a},\mathcal{V}^{s},z^{a} ])+(1-\alpha^{{}^{\prime}})\phi_{s}(z^{s},t^{s},\varnothing),\end{split}\] (A)

Figure A: **Qualitative comparison of the existing methods.** The darker colors indicate the later in time. The generated results of our method better match the descriptions, while others have downgraded motions or improper semantics, demonstrating that our method achieves superior controllability compared to well-designed baseline models. We have provided a supplemental video in our supplementary material. In the supplemental video, we show comparisons of text-driven motion generation. We suggest the reader watch this video for dynamic motion results.

Where \(\alpha^{{}^{\prime}}\) is the guidance scale and \(\alpha^{{}^{\prime}}>1\) can strengthen the effect of guidance [8]. We set \(\alpha^{{}^{\prime}}\) to 7.5 in practice following MLD. Please refer to our code for more details.

### Implementation Details for Different Datasets

Following MLD [8], we utilize a frozen text encoder of the CLIP-ViT-L-14 [45] model for text representation. The dimension of node representation \(D\) is set to 768. The dimension of latent embedding \(D^{\prime}\) is set to 256. For the motion variational autoencoder, motion encoder \(\mathcal{E}\) and decoder \(\mathcal{D}\) all consist of 9 layers and 4 heads with skip connection [46]. We set the token sizes \(C^{m}\) to 2, \(C^{a}\)

Figure B: **Additional qualitative motion results are generated with text prompts of the HumanML3D test set. The darker colors indicate the later in time. These results demonstrate that our method can generate diverse and accurate motion sequences.**

to 4, and \(C^{s}\) to 8. We set \(\lambda\) to 1e-4. All our models are trained with the AdamW [26; 38] optimizer using a fixed learning rate of 1e-4. We use 4 Tesla V100 GPUs for the training, and there are 128 samples on each GPU, so the total batch size is 512. The number of diffusion steps of each level is 1,000 during training, and the step sizes \(\beta_{t}\) are scaled linearly from 8.5\(\times\)1e-4 to 0.012. We keep running a similar number of iterations on different data sets. For the HumanML3D dataset, the model is trained for 6,000 epochs during the motion variational autoencoder stage and 3,000 epochs during the diffusion stage. For the KIT dataset, the model is trained for 30,000 epochs during the motion variational autoencoder stage and 15,000 epochs during the diffusion stage. Code is available at https://github.com/jpthu17/GraphMotion. In this code, we provide the process of the training and evaluation of the proposed method, and the pre-trained model.

Figure 10: **Qualitative analysis on the imbalance problem.** The darker colors indicate the later in time. When the verbs and action names are masked, existing models tend to generate motion randomly. In contrast, our method can generate motion based solely on the action specifics. These results show that our method is not overly focused on the verbs and action names.

## Appendix C Additional Qualitative Analysis

### Qualitative Comparison of the Existing Methods

We provide additional qualitative motion results in Fig. A. Compared to other methods, our method generates motions that match the text descriptions better, indicating that our method is more sensitive to subtle differences in texts. The generated results demonstrate that our method achieves superior controllability compared to well-designed baseline models.

### Additional Visualization Results

In Fig. B, we provide additional qualitative motion results which are generated with text prompts of the HumanML3D test set. These results demonstrate that our method can generate diverse and accurate motion sequences from a variety of motion descriptions.

### Qualitative Analysis on the Imbalance Problem

To demonstrate the imbalance problem of other methods and prove that our method does not have this problem, we mask the verbs and action names in the motion description to force the model to generate motion only from action specifics. As shown in Fig. C, when the verbs and action names are masked, existing models tend to generate motion randomly. In contrast, our method can generate motion that matches the description based solely on the action specifics. These results show that our method is not overly focused on the verbs and action names.

### Qualitative Comparison of Different Hierarchies

We provide different levels of qualitative comparison in Fig. D. The results show that the output at the higher level (e.g., specific level) has more action details. Specifically, the motion level generates only coarse-grained overall motion. The action level generates local actions better than the motion level but lacks action specifics. The specific level generates more action specifics than the action level.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline \multirow{2}{*}{Methods} & \multicolumn{3}{c}{Diffusion Steps} & \multicolumn{3}{c}{R-Precision \(\uparrow\)} & \multirow{2}{*}{FD \(\downarrow\)} \\ \cline{2-2} \cline{5-6}  & Motion \(T^{m}\) & Action \(T^{a}\) & Specific \(T^{a}\) & & Top-1 & Top-2 & Top-3 \\ \hline \multicolumn{6}{l}{_The total number of diffusion steps is 1000 with DDPM [18]_} \\ MDM [54] & 1000 & ✗ & ✗ & \(0.320\pm 0.005\) & \(0.498\pm 0.004\) & \(0.611\pm 0.007\) & \(0.544\pm 0.044\) \\ MotionDiffuse [62] & 1000 & ✗ & ✗ & \(0.491\pm 0.001\) & \(0.681\pm 0.001\) & \(0.782\pm 0.001\) & \(0.630\pm 0.001\) \\ \hline \multicolumn{6}{l}{_The total number of diffusion steps is 50 with DDPM [50]_} \\ MLD [8] & 50 & ✗ & ✗ & \(0.481\pm 0.003\) & \(0.673\pm 0.003\) & \(0.772\pm 0.002\) & \(0.473\pm 0.013\) \\ GraphMotion (Ours) & 20 & 15 & 15 & \(0.489\pm 0.003\) & \(0.676\pm 0.002\) & \(0.771\pm 0.002\) & \(0.131\pm 0.007\) \\ GraphMotion (Ours) & 15 & 15 & 20 & \(\mathbf{0.496\pm 0.003}\) & \(\mathbf{0.686\pm 0.003}\) & \(\mathbf{0.778\pm 0.002}\) & \(\mathbf{0.118\pm 0.008}\) \\ \hline \multicolumn{6}{l}{_The total number of diffusion steps is 150 with DDPM [50]_} \\ MLD [8] & 150 & ✗ & ✗ & \(0.461\pm 0.002\) & \(0.649\pm 0.003\) & \(0.797\pm 0.002\) & \(0.457\pm 0.011\) \\ GraphMotion (Ours) & 50 & 50 & 50 & \(\mathbf{0.504\pm 0.003}\) & \(\mathbf{0.699\pm 0.002}\) & \(\mathbf{0.785\pm 0.002}\) & \(\mathbf{0.116\pm 0.007}\) \\ \hline \multicolumn{6}{l}{_The total number of diffusion steps is 300 with DDPM [50]_} \\ MLD [8] & 300 & ✗ & \(\mathbf{x}\) & \(0.473\pm 0.002\) & \(0.664\pm 0.003\) & \(0.765\pm 0.002\) & \(0.403\pm 0.011\) \\ GraphMotion (Ours) & 100 & 100 & 100 & \(\mathbf{0.486\pm 0.003}\) & \(\mathbf{0.671\pm 0.004}\) & \(\mathbf{0.767\pm 0.003}\) & \(\mathbf{0.096\pm 0.008}\) \\ \hline \multicolumn{6}{l}{_The total number of diffusion steps is 1000 with DDPM [50]_} \\ MLD [8] & 1000 & ✗ & \(0.452\pm 0.002\) & \(0.639\pm 0.003\) & \(0.751\pm 0.002\) & \(0.460\pm 0.013\) \\ GraphMotion (Ours) & 400 & 300 & 300 & \(0.475\pm 0.003\) & \(0.659\pm 0.003\) & \(0.756\pm 0.003\) & \(0.136\pm 0.007\) \\ GraphMotion (Ours) & 300 & 300 & 400 & \(\mathbf{0.484\pm 0.003}\) & \(\mathbf{0.694\pm 0.003}\) & \(\mathbf{0.787\pm 0.003}\) & \(\mathbf{0.132\pm 0.008}\) \\ \hline \hline \end{tabular}
\end{table}
Table B: **Ablation study about the total number of diffusion steps on the HumanML3D test set. “\(\uparrow\)” denotes that higher is better. “\(\downarrow\)” denotes that lower is better. We repeat all the evaluations 20 times and report the average with a 95% confidence interval. “\(\chi\)” denotes that this method does not apply this parameter. To speed up the sampling process, we use DDIM in practice following MLD.**

## Appendix D Additional Experiments

### Analysis of the Diffusion Steps

In Tab. 1, we show the ablation study of the total number of diffusion steps on the HumanML3D test set. Following MLD [8], we adopt the denoising diffusion implicit models [50] (DDIM) during interference. As shown in Tab. 1, our method consistently outperforms the existing state-of-the-art methods with the same total number of diffusion steps, which demonstrates the efficiency of our method. We find that the number of diffusion steps at the higher level (e.g., specific level) has a greater impact on the result. Therefore, in scenarios requiring high efficiency, we recommend allocating more diffusion steps to the higher level. Moreover, with the increase of the total diffusion steps, the performance of our method is further improved, while the performance of MLD saturates. These results further prove the superiority of our design.

### Analysis of the Inference Time

In Tab. 2, we provide the evaluation of inference time costs. Our method is as efficient as the one-stage diffusion methods during the inference stage, even though we decompose the diffusion process into three parts. This is because we can control the total number \(T^{m}+T^{a}+T^{s}\) of iterations by restricting it to be the same as those of the one-stage diffusion methods. As shown in Tab. 2, the inference speed of our method is comparable to that of the existing state-of-the-art methods with the same total number of diffusion steps, which demonstrates the efficiency of our method.

### Analysis of the motion VAE models

We provide the evaluation of the motion VAE models. In Tab. 3, we show the results on the HumanML3D test set. Tab. 4 shows the results on the KIT test set. Among the three levels, the performance of the specific level is the best, which indicates that increasing the token size can improve the reconstruction ability of the motion VAE models.

## Appendix E Motion Representations and Metric Definitions

### Motion Representations

Motion representation can be summarized into the following four categories, and we follow the previous work of representing motion in latent space.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline \multirow{2}{*}{Methods} & \multirow{2}{*}{Reference} & \multicolumn{4}{c}{Diffusion Steps} & \multirow{2}{*}{Average time per sample (s) \(\downarrow\)} & \multirow{2}{*}{FID \(\downarrow\)} \\ \cline{3-3} \cline{5-6}  & & Motion & \(T^{m}\) & Action \(T^{a}\) & & Specific \(T^{s}\) \\ \hline \multicolumn{6}{l}{_The total number of diffusion steps is 1000 with DDPIM [18]_} \\ MDM [54] & ICLR 2023 & 1000 & ✘ & ✘ & \(178.7699\) & \(\mathbf{0.544}\) \\ MLD [8] & CVPR 2023 & 1000 & ✘ & ✘ & \(5.5045\) & \(0.568\) \\ \hline \multicolumn{6}{l}{_The total number of diffusion steps is 50 with DDIM [50]_} \\ MDM [54] & ICLR 2023 & 50 & ✘ & ✘ & \(20.5678\) & \(7.334\) \\ MLP [8] & CVPR 2023 & 50 & ✘ & ✘ & \(0.9349\) & \(0.473\) \\ GraphMotion & Ours & 20 & 15 & 15 & \(0.9094\) & \(0.131\) \\ GraphMotion & Ours & 15 & 15 & 20 & \(0.7758\) & \(\mathbf{0.118}\) \\ \hline \multicolumn{6}{l}{_The total number of diffusion steps is 150 with DDIM [50]_} \\ MLD [8] & CVPR 2023 & 150 & ✘ & ✘ & \(2.4998\) & \(0.457\) \\ GraphMotion & Ours & 50 & 50 & 50 & \(2.5518\) & \(\mathbf{0.116}\) \\ \hline \multicolumn{6}{l}{_The total number of diffusion steps is 100 with DDIM [50]_} \\ MLD [8] & CVPR 2023 & 1000 & ✘ & ✘ & \(16.6654\) & \(0.460\) \\ GraphMotion & Ours & 400 & 300 & 300 & \(22.1238\) & \(0.136\) \\ GraphMotion & Ours & 300 & 300 & 400 & \(17.0912\) & \(\mathbf{0.132}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Evaluation of Inference time costs on the HumanML3D test set.** We evaluate the average time per sample with different diffusion schedules and FID “\(\downarrow\)” denotes that lower is better. Please note the bad FID of MDM with DDIM is mentioned in their GitHub issues #76. “✘” denotes that this method does not apply this parameter. We use DDIM in practice following MLD.

Latent Format.Following previous works [42; 8; 61], we encode the motion into the latent space with a motion variational autoencoder [27]. The latent representation is formulated as:

\[\hat{x}^{1:L}=\mathcal{D}(z),\quad z=\mathcal{E}(x^{1:L}).\] (B)

HumanML3D Format.HumanML3D [14] proposes a motion representation \(x^{1:L}\) inspired by motion features in character control. This motion representation is well-suited for neural networks. To be specific, the \(i_{th}\) pose \(x^{i}\) is defined by a tuple consisting of the root angular velocity \(r^{a}\in\mathbb{R}\) along the Y-axis, root linear velocities (\(r^{x},r^{z}\in\mathbb{R}\)) on the XZ-plane, root height \(r^{y}\in\mathbb{R}\), local joints positions \(\bm{j^{P}}\in\mathbb{R}^{3N_{j}}\), velocities \(\bm{j^{v}}\in\mathbb{R}^{3N_{j}}\), and rotations \(\bm{j^{r}}\in\mathbb{R}^{6N_{j}}\) in root space, and binary foot-ground contact features \(\bm{c^{f}}\in\mathbb{R}^{4}\) obtained by thresholding the heel and toe joint velocities. Here, \(N_{j}\) denotes the joint number. Finally, the HumanML3D format can be defined as:

\[x^{i}=\{r^{a},r^{x},r^{z},r^{y},\bm{j^{P}},\bm{j^{v}},\bm{j^{r}},\bm{c^{f}}\}.\] (C)

SMPL-based Format.SMPL [37] is one of the most widely used parametric human models. SMPL and its variants propose motion parameters \(\theta\) and shape parameters \(\beta\). \(\theta\in\mathbb{R}^{3\times 23+3}\) is rotation vectors for 23 joints and a root, while \(\beta\) represents the weights for linear blended shapes. The global translation \(r\) is also incorporated to formulate the representation as follows:

\[x^{i}=\{r,\theta,\beta\}.\] (D)

MMM Format.Master Motor Map [52] (MMM) representations propose joint angle parameters based on a uniform skeleton structure with 50 degrees of freedom (DoFs). In text-to-motion tasks, recent methods [2; 12; 42] converts joint rotation angles into \(J=21\) joint XYZ coordinates. Given the global trajectory \(t_{root}\) and \(p_{m}\in\mathbb{R}^{3J}\), the preprocessed representation is formulated as:

\[x^{i}=\{p_{m},t_{root}\}.\] (E)

### Metric Definitions

Following previous works, we use the following five metrics to measure the performance of the model. Note that global representations of motion and text descriptions are first extracted with the pre-trained network in [14].

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline \multirow{2}{*}{Methods} & \multirow{2}{*}{Token Size} & \multicolumn{3}{c}{R-Precision \(\uparrow\)} & \multirow{2}{*}{FID \(\downarrow\)} & \multirow{2}{*}{Diversity \(\rightarrow\)} \\ \cline{3-3} \cline{5-6}  & & Top-1 & & & & Top-3 \\ \hline Real motion & - & \(0.511\) & \(0.703\) & \(0.797\) & \(0.002\) & \(9.503\) \\ \hline Motion Level & 2 & \(0.498\) & \(0.692\) & \(0.791\) & \(1.906\) & \(9.675\) \\ Action Level & 4 & \(0.514\) & \(0.703\) & \(0.793\) & \(0.068\) & \(\bm{9.610}\) \\ Specific Level & 8 & \(\bm{0.525}\) & \(\bm{0.708}\) & \(\bm{0.800}\) & \(\bm{0.019}\) & \(9.863\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: **Evaluation of the VAE models on the motion part of the HumanML3D test set.** “\(\uparrow\)” denotes that higher is better. “\(\downarrow\)” denotes that lower is better. “\(\rightarrow\)” denotes that results are better if the metric is closer to the real motion. The performance of the specific level is the best.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline \multirow{2}{*}{Methods} & \multirow{2}{*}{Token Size} & \multicolumn{3}{c}{R-Precision \(\uparrow\)} & \multirow{2}{*}{FID \(\downarrow\)} & \multirow{2}{*}{Diversity \(\rightarrow\)} \\ \cline{3-3} \cline{5-6}  & & Top-1 & & & Top-2 & & Top-3 \\ \hline Real motion & - & \(0.424\) & \(0.649\) & \(0.779\) & \(0.031\) & \(11.08\) \\ \hline Motion Level & 2 & \(\bm{0.431}\) & \(0.623\) & \(0.745\) & \(1.196\) & \(10.66\) \\ Action Level & 4 & \(0.413\) & \(\bm{0.644}\) & \(\bm{0.770}\) & \(0.396\) & \(10.85\) \\ Specific Level & 8 & \(0.414\) & \(0.640\) & \(0.760\) & \(\bm{0.361}\) & \(\bm{10.86}\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: **Evaluation of the VAE models on the motion part of the KIT test set.** “\(\uparrow\)” denotes that higher is better. “\(\downarrow\)” denotes that lower is better. “\(\rightarrow\)” denotes that results are better if the metric is closer to the real motion. The performance of the specific level is the best.

R-Precision.Under the feature space of the pre-trained network in [14], given one motion sequence and 32 text descriptions (1 ground-truth and 31 randomly selected mismatched descriptions), motion-retrieval precision calculates the text and motion Top 1/2/3 matching accuracy.

Frechet Inception Distance (FID).We measure the distribution distance between the generated and real motion using FID [17] on the extracted motion features [14]. The FID is calculated as:

\[\text{FID}=\|\mu_{gt}-\mu_{pred}\|^{2}-\text{Tr}(\Sigma_{gt}+\Sigma_{pred}-2( \Sigma_{gt}\Sigma_{pred})^{\frac{1}{2}}),\] (F)

where \(\Sigma\) is the covariance matrix. Tr denotes the trace of a matrix. \(\mu_{gt}\) and \(\mu_{pred}\) are the mean of ground-truth motion features and generated motion features.

Multimodal Distance (MM-Dist).Given \(N\) randomly generated samples, we calculate the average Euclidean distances between each text feature \(f_{t}\) and the generated motion feature \(f_{m}\) from that text. The multimodal distance is calculated as:

\[\text{MM-Dist}=\frac{1}{N}\sum_{i=1}^{N}\|f_{t,i}-f_{m,i}\|,\] (G)

where \(f_{t,i}\) and \(f_{m,i}\) are the features of the \(i_{th}\) text-motion pair.

Diversity.All generated motions are randomly sampled to two subsets (\(\{x_{1},x_{2},...,x_{X_{d}}\}\) and \(\{x^{{}^{\prime}}_{1},x^{{}^{\prime}}_{2},...,x^{{}^{\prime}}_{X_{d}}\}\)) of the same size \(X_{d}\). Then, we extract motion features [14] and compute the average Euclidean distances between the two subsets:

\[\text{Diversity}=\frac{1}{X_{d}}\sum_{i=1}^{X_{d}}\|x_{i}-x^{{}^{\prime}}_{i}\|.\] (H)

Multimodality (MModality).We randomly sample a set of text descriptions with size \(J_{m}\) from all descriptions. For each text description, we generate \(2\times X_{m}\) motion sequences, forming \(X_{m}\) pairs of motions. We extract motion features and calculate the average Euclidean distance between each pair. We report the average of all text descriptions. We define features of the \(j_{th}\) pair of the \(i_{th}\) text description as (\(x_{j,i}\), \(x^{{}^{\prime}}_{j,i}\)). The multimodality is calculated as:

\[\text{MModality}=\frac{1}{J_{m}\times X_{m}}\sum_{j=1}^{J_{m}}\sum_{i=1}^{X_{m} }\|x_{j,i}-x^{{}^{\prime}}_{j,i}\|.\] (I)