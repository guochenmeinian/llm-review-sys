# GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI

 Pengcheng Chen1,2 Jin Ye1,3,* Guoan Wang1,4 Yanjun Li1,4

**Zhongying Deng5 Wei Li1,6 Tianbin Li1 Haodong Duan1 Ziyan Huang1,6 Yanzhou Su1 Benyou Wang7,8 Shaoting Zhang1 Bin Fu9 Jianfei Cai3 Bohan Zhuang3 Eric J Seibel2 Yu Qiao1,9 Junjun He1\({}^{\ddagger}\)**

1Shanghai AI Laboratory 2 University of Washington 3Monash University 4East China Normal University 5University of Cambridge 6Shanghai Jiao Tong University 7The Chinese University of Hong Kong, Shenzhen 8Shenzhen Research Institute of Big Data

###### Abstract

Large Vision-Language Models (LVLMs) are capable of handling diverse data types such as imaging, text, and physiological signals, and can be applied in various fields. In the medical field, LVLMs have a high potential to offer substantial assistance for diagnosis and treatment. Before that, it is crucial to develop benchmarks to evaluate LVLMs' effectiveness in various medical applications. Current benchmarks are often built upon specific academic literature, mainly focusing on a single domain, and lacking varying perceptual granularities. Thus, they face specific challenges, including limited clinical relevance, incomplete evaluations,

Figure 1: Overview of the GMAI-MMBench. The benchmark is meticulously designed for testing LVLMs’ abilities in real-world clinical scenarios with three key features: (1) Comprehensive medical knowledge: It consists of 284 diverse clinical-related datasets from worldwide sources, covering 38 modalities. (2) Well-categorized data structure: It features 18 clinical VQA tasks and 18 clinical departments, meticulously organized into a lexical tree. (3) Multi-perceptual granularity: Interactive methods span from image to region level, offering varying degrees of perceptual details.

and insufficient guidance for interactive LVLMs. To address these limitations, we developed the GMAI-MMBench, the most comprehensive general medical AI benchmark with well-categorized data structure and multi-perceptual granularity to date. It is constructed from 284 datasets across 38 medical image modalities, 18 clinical-related tasks, 18 departments, and 4 perceptual granularities in a Visual Question Answering (VQA) format. Additionally, we implemented a lexical tree structure that allows users to customize evaluation tasks, accommodating various assessment needs and substantially supporting medical AI research and applications. We evaluated 50 LVLMs, and the results show that even the advanced GPT-4o only achieves an accuracy of 53.96%, indicating significant room for improvement. Moreover, we identified five key insufficiencies in current cutting-edge LVLMs that need to be addressed to advance the development of better medical applications. We believe that GMAI-MMBench will stimulate the community to build the next generation of LVLMs toward GMAI.

## Introduction

In clinical practice, diverse demands may be proposed by different medical institutions for disease diagnosis and treatment. These demands can be potentially fulfilled by general medical AI which provides general-purpose medical models to tackle a wide range of medical tasks. Such models are typically Large Vision-Language Models (LVLMs) trained on diverse data types, including imaging and clinical texts, to tackle diverse tasks, e.g., disease diagnosis and severity grading. Noticeably, the state-of-the-art LVLMs, including general-purpose ones (e.g., DeepSeek-VL [155], GPT-4V [5] and Claude3-Opus [13]) and medical purposes (like MedDr [95], LLaVA-Med [138], and Med-Flamingo [181]), have both demonstrated promising performance in some medical visual-textual tasks. However, it remains unclear to what extent these LVLMs can accommodate the diverse demands in real clinical scenarios. To validate their effectiveness and promote their application in clinical practice, it is crucial to establish a comprehensive benchmark to address diverse real-world demands. Therefore, an ideal benchmark should achieve three specific aims:

**Aim 1. Comprehensive medical knowledge.** Medical knowledge is embedded in medical data, so comprehensive medical knowledge requires diverse medical data of different modalities from various data sources. In clinical scenarios, various types of imaging modalities, including X-rays, Computed Tomography (CT), Magnetic Resonance Image (MRI), Ultrasound Imaging, Positron Emission Tomography (PET), etc, are employed for diagnostic and therapeutic purposes, reflecting different aspects of medical knowledge [267]. Besides, to encompass the diverse medical knowledge from different clinical facilities, the data used in a comprehensive benchmark should cover a range of different clinical institutions and hospitals which are preferably distributed across the world [205]. These demands favor benchmarks collected from diverse sources. **Aim 2. Comprehensive evaluation across all clinical aspects.** A comprehensive benchmark should be easily customized to evaluate any specific abilities of LVLMs for each clinical professional. This property is necessary because there are an excessive amount of clinical institutions, departments, and practitioners, each having their own specific demand. Their potential demands can be concluded in two sides: 1) _Evaluation across diverse tasks_. Some clinical practitioners may require MRI data for disease diagnosis while others may need to deal with surgical workflow recognition for computer-assisted or robot-assisted surgery systems. Therefore, a comprehensive benchmark should cover all clinical demands by encompassing a sufficient number of diseases and tasks. 2) _Evaluation for diverse clinical departments_. Some departments may be interested in LVLMs' performance on oncology-related tasks only while others may only focus on urology-related ones. As such, a comprehensive benchmark should be easily used for customized evaluation to accommodate the diverse demands of different clinical departments. These demands further require the benchmark to be well-categorized to facilitate ease of use. **Aim 3. Interactive ability in multi-perceptual granularity.** Given a specific medical image, doctors need to look through the whole image (image level) for an overview while also requiring comprehensive explanations in a specific position (mask level) or region (box level). This demand requires LVLMs to perceive the granularity range from a specific position to the entire image. Thus, a comprehensive benchmark should also evaluate LVLMs' perceptual granularity.

As shown in Table 1, there are some medical benchmarks, such as Medical-Diff-VQA [105], PathVQA [96], Cholec80-VQA [222], and Cholec80 [243], dedicated to evaluating specific abilities of LVLMs. These benchmarks effectively assess the performance of LVLMs within a particular modality or task, thereby facilitating the optimization of models for specific applications. Nonetheless, their limited modalities and tasks cannot meet the requirement of modal and task diversity. Other benchmarks including VQA-RAD [136], RadBench [254], and MMMU (Health & Medicine) [262] address this issue by providing multiple modalities and tasks for evaluation, with data consisting of natural image-text pairs sourced from academic papers, textbooks, and specific databases. Though these benchmarks significantly enhance the breadth and depth of medical assessment, they may not accurately reflect actual clinical requirements, as their sources are distant from clinic practice and prone to data leakage [44, 72]. More importantly, _none of these benchmarks can be customized to evaluate various abilities of LVLMs to accommodate highly diverse clinical demands_ because their data are not well categorized. For instance, it is hard to obtain the dimension, modality, and task information of a specific data point in these datasets, which prevents a clinical professional from evaluating LVLMs using the CT (modality) of 2D (dimension) images for blood vessel recognition (task). Due to this, they can hardly be used for customized evaluation. In summary, though existing medical multimodal benchmarks provide valuable evaluation frameworks, they present challenges in fully addressing clinical needs. Future developments necessitate more refined and customized benchmarks that are closely aligned with real-world clinical applications.

To address these challenges, we introduce the General Medical AI MultiModal Benchmark (GMAIMBench), a comprehensive multimodal benchmark that is well-categorized for medical image understanding and reasoning in real-world clinical scenarios. As shown in Figure 1, its comprehensiveness can be concluded in three aspects: 1) **comprehensive medical knowledge from diverse modalities, tasks, and data sources**, 2) **well-categorized in lexical tree structures**, and 3) **multiple perceptual granularity**.

First, GMAI-MMBench has diverse modalities and data sources because it is built upon 284 high-quality datasets collected across the world. These 284 datasets cover various medical image tasks, including 2D detection, 2D classification, and 2D/3D segmentation, to ensure the diversity of tasks. Using these foundational visual-based tasks has two advantages over using off-the-shelf image-text pair data. 1) It minimizes the risk of data leakage since the data in our benchmark are mostly image-label pairs rather than image-text pairs. The image-label pairs are not directly convertible to LVLMs training samples (usually image-text pairs), thus less likely to be used to train LVLMs; 2) It ensures high clinical relevance, as the images are sourced from hospitals and annotated by professional doctors. We then carefully selected approximately 26K cases with 38 different modalities to construct the GMAI-MMBench, thus meeting the modal diversity goal.

Second, GMAI-MMBench is a well-categorized medical benchmark that can comprehensively evaluate the pros and cons of various aspects of LVLMs, benefiting both model developers and users with specific needs. Specifically, we develop a categorization system, called lexical tree structure, which categorizes all cases into 18 clinical VQA tasks, 18 departments, 38 modalities, etc. The 'clinical VQA tasks' / 'departments' /'modalities' are the lexicons that can be used to retrieve desired

\begin{table}
\begin{tabular}{l|c c c c c c} \hline
**Benchmark** & **Modality** & **Size** & **Task** & **Dept** & **PG** & **Source** \\ \hline Medical-Diff-VQA* [105] & 1 & 70K & 7 & ✘ & I & MMIRC-CXR [120] \\ PathVQA* [96] & 1 & 6K & 7 & ✘ & I & Textbook, PEIR [1] \\ Cholec80-VQA* [222] & 1 & 9K & 2 & ✘ & I & Cholec80 [243] \\ VQA-RAD [136] & 3 & 3K & 11 & ✘ & I & Teaching cases from Medpix [2] \\ RadBench [254] & 6 & 137K & 5 & ✘ & I & 13 image-text paired datasets \\ MMMU (H \& M) [262] & 6 & 2K & 5 & ✘ & I, B & Exam, Quiz, Textbook \\ SLAKE* [145] & 3 & 2K & 10 & ✘ & I & MSD [227], Chest-ray8 [250], CHAOS [127] \\ OmnidKQA [106] & 12 & 128K & 5 & ✘ & 73 classification datasets & \\ \hline GMAI-MMBench & 38 & 26K & 18 & ✘ & I, B, M, C & 284 datasets from both public and hospital \\ \hline \end{tabular}
\end{table}
Table 1: Comparison between GMAI-MMBench and other existing benchmarks in the biomedical field. GMAI-MMBench is sourced from extensive data sources worldwide, offering comprehensive medical knowledge detailed in modalities, clinical tasks, departments, and perceptual granularities. Dept and PG indicate department and perceptual granularity, respectively. In the perceptual granularity types, I, B, M, and C denote image, box, mask, and contour, respectively. * indicates the test set.

cases for evaluation. For instance, the oncology department can select cases related to oncology to evaluate LVLMs' performance for oncology tasks, thus greatly enhancing flexibility and usability for specific demands.

Third, GMAI-MMBench can evaluate LVLMs' abilities to perceive different granularity, such as understanding the local image content in a mask or bounding box as well as recognizing the entire image content. This ability is important for detection, segmentation, and classification tasks as these tasks need different perceptual granularity for better performance. Furthermore, the perception of bounding boxes or masks is vital for interactive LVLMs [132], so the perceptual granularity evaluation in our benchmark can possibly be used to improve interactive LVLMs.

We assess 44 publicly available LVLMs (38 general purpose and 6 medical-specific models) as well as advanced proprietary LVLMs such as GPT-4o, GPT-4V, Claude3-Opus, Gemini 1.0, Gemini 1.5, and Qwen-VL-Max on our GMAI-MMBench. We summarize the key findings as follows:

(1) GMAI-MMBench presents significant challenges in clinical practice. Even the best proprietary GPT-4o only achieves an accuracy of 53.96%, which demonstrates the deficiencies of cutting-edge LVLMs in tackling medical professional issues, thus they can hardly fulfill diverse clinical demands.

(2) Open-source LVLMs, such as MedDr and DeepSeek-VL-7B, achieve approximately 44% accuracy, making them very competitive compared to proprietary models. For instance, they surpass Claude3-Opus and Qwen-VL-Max and achieve comparable performance to Gemini 1.5 and GPT-4V. However, they still exhibit a clear performance disparity compared to the top-performing GPT-4o.

(3) Most medical-specific models have difficulty reaching a general performance level (approximately 30% accuracy) achieved by general LVLMs, except MedDr with 43.69% accuracy.

(4) Most LVLMs exhibit unbalanced performance across different clinical VQA tasks, departments, and perceptual granularity. Notably, in the experiments on different perceptual granularity, box-level annotation consistently results in the worst accuracy, even worse than image-level annotation.

Figure 2: Examples of GMAI-MMBench. The benchmark covers a variety of clinical tasks, departments, and perceptual granularities from worldwide data sources.

(5) The major factors leading to performance bottlenecks include perceptual errors (e.g., misrecognition of image content), lack of medical domain knowledge, irrelevant responses, and rejection of answering questions due to safety protocols.

In summary, our contributions are three-fold. (a) We introduce a comprehensive benchmark, GMAI-MMBench, to evaluate existing LVLMs in clinical practice. GMAI-MMBench covers 38 modalities, 18 clinical VQA tasks, 18 departments, and 4 different perceptual granularity from 284 medical-related datasets, thereby offering a diverse range of modalities, tasks, and data sources. (b) GMAI-MMBench organizes each data point in lexical tree structures, with lexicons used to select desired data points to evaluate various aspects of LVLMs' abilities. Thus, GMAI-MMBench facilitates customized evaluation to meet highly diverse demands in clinical practice. **See Supplementary C.2**. (c) We evaluate 44 representative general-purpose LVLMs, including both open-source and proprietary models, as well as 6 medical-specific LVLMs on GMAI-MMBench. The comprehensive evaluation reveals the pros and cons of different LVLMs from diverse perspectives, providing insights to improve these models to accommodate real-world clinical applications.

## GMAI-MMBench

### Overview

We propose GMAI-MMBench, an innovative benchmark meticulously designed for the medical field, capable of providing comprehensive evaluations of LVLMs across various aspects of healthcare. (shown in the Figure 2) We collect 284 datasets from public sources and hospitals, covering medical imaging tasks of detection, classification, and segmentation, to form the data fuel for establishing such a benchmark. The detailed datasets are listed in the supplementary. Based on the data foundation, we design a reliable pipeline to generate question-answering pairs and organize them from different perspectives with manual validation. Finally, we carefully select approximately 26K questions with

Figure 3: Overall illustration of GMAI-MMBench. The data collection can be divided into three main steps: 1) We search hundreds of datasets from both the public and hospitals, then keep 284 datasets with highly qualified labels after dataset filtering, uniforming image format, and standardizing label expression. 2) We categorize all labels into 18 clinical VQA tasks and 18 clinical departments, then export a lexical tree for easily customized evaluation. 3) We generate QA pairs for each label from its corresponding question and option pool. Each question must include information about image modality, task cue, and corresponding annotation granularity. The final benchmark is obtained through additional validation and manual selection.

varying levels of perceptual granularity from the manually validated cases to construct the final GMAI-MMBench.

### Benchmark Construction

The detailed steps of constructing our GMAI-MMBench can be divided into three main steps as shown in Figure 3.

**Dataset collection and standardization.** As our aim is to build a large-scale benchmark for the comprehensive evaluation of LVLMs, the first and most important step is data collection. In contrast to benchmarks that directly use multimodal paired datasets, we source the datasets in two ways to minimize the data leakage problem and ensure the diversity and clinical property: First, we conduct thorough Internet searches to collect as many 2D/3D medical-related datasets as possible, retaining those that involve classification, detection, and segmentation tasks. Second, we collaborate with several hospitals that have agreed to share their ethically approved data. This process has enabled us to curate 284 datasets with highly qualified labels. Following data collection, we standardize both images and labels. For images, we adhere to the SA-Med2D-20M [258] protocol, transforming all 2D/3D medical images into 2D RGB images for further evaluation. For labels, we refer to the Medical Subject Headings (MeSH)4 to ensure every label is unique, clear, and free from conflict or ambiguity within each task. Specifically, we focus on three main situations: (1) expanding all abbreviations, such as changing "AMD" to "Age-related macular degeneration"; (2) unifying different expressions for the same target, such as standardizing both "lung nodule" and " pulmonary nodule"; (3) merging labels with left and right distinctions, such as combining "left kidney" and "right kidney" into "kidney", since our goal is to evaluate the abilities of understanding and reasoning rather than directional judgment.

Footnote 4: https://www.ncbi.nlm.nih.gov/mesh/1000048

**Label categorization and lexical tree construction.** We construct a well-categorized lexical tree to ensure GMAI-MMBench can be easily customized to evaluate the specific abilities of LVLMs for each clinical professional. The overview of the tree is shown in Figure 3, and the complete version is in supplementary. First, we integrate data properties and real applications to propose three subjects tailored for the biomedical fields: clinical VQA tasks, departments, and perceptual granularities. Specialized options are generated for each subject individually: For clinical VQA tasks, we extract keywords according to the original dataset descriptions and then lead to 18 categories. For departments, we refer to the Mayo Clinic5 and assign all labels to 18 departments. For perceptual granularity, we construct 4 types based on annotation methods (see the rightmost panel in Figure 1). We then recruit several biomedical engineering university students (including coauthors) to tag labels from the constructed options in these subjects. Specifically, each label is randomly assigned to 3 people, and their tagging results are merged by voting. After label categorization, the lexical tree can be directly exported for customized evaluation. An example of customized evaluation is presented in Supplementary C.2.

Footnote 5: https://www.mayoclinic.org/departments-centers

**QA generation and selection.** Following the label categorization, all labels are assigned to specific modalities, clinical VQA tasks, departments, and perceptual granularities. Based on the well-organized structure, we generate the VQA pairs for every label with three steps. First, questions and options generation. For question generation, a question must include three key pieces of information in GMAI-MMBench: modality, clinical task hint, and perceptual granularity information. For each combination of the three elements, we randomly pick 10 labels and generate 10 candidate questions with GPT-4o for each selected label. These questions are then manually reviewed to meet the following criteria: (1) they must include necessary information on modality, clinical task, and perceptual granularity; (2) they do not include any hints that would allow the question to be answered without viewing the image. After manual review, the modality is replaced with a placeholder for standardization. For example, a valid question template for Disease Diagnosis in segmentation task is: _"This is a <modality> image. Which of the following options is the most appropriate to demonstrate symptoms in the marked area?_" Once the question pool is generated, each category has its question pool based on its tags of modality, clinical VQA task, and perceptual granularity. For options generation, the global view (image level) and local view (mask level, bounding box level, and contour level) of perceptual granularity are handled separately. For the global view, the option pool for each answer is sourced from the remaining categories within the answer's dataset to avoid introducingmultiple correct answers. For instance, a fundus image dataset may focus solely on pathological myopia, but the images might also contain other diseases like diabetic retinopathy. Including other categories could render the question invalid. For the local view, we construct a shared option pool for the answers with the combination of modality, clinical VQA task, and perceptual granularity. Second, as each answer with corresponding images has its own question and option pool, we generate all QA pairs for all images. For each image, we randomly select a question from its question pool and replace the placeholder with its modality. Along with the correct answer, we randomly select \(n\) options (where \(n=\mathbf{randn}(\mathbf{max}(1,\mathbf{len(option\ pool)}),\mathbf{min}(4, \mathbf{len(option\ pool)})\) from the corresponding option pool to create the set of options. Third, to ensure data quality and balanced distribution, we perform additional manual validation and selection. In the validation stage, we assess the QA pairs based on the following criteria: (1) We drop cases whose questions do not contain the three key components and can be answered without the image. (2) We filter out cases with incorrect answers. (3) We drop cases where images have unclear targets or poor image quality. In the selection stage, we choose 30 cases per answer to ensure balance across all tasks (all cases are included if the number is less than 30). The selection rule is based on the consideration of diversity: Selecting images with large differences in appearance, data source, age, gender, etc. As a result, we finalize 25831 QA pairs for the GMAI-MMBench (4550 in the validation set and 21281 in the test set).

## Experiments

### Experiment setup

In this study, we evaluated various LVLMs, including medical-specific, open-source, and proprietary API general models. We selected versions with approximately 7 billion parameters for testing, and the model weights were sourced from their respective official Hugging Face repositories. Our evaluation was conducted using the VLMEvalKit6 framework and Multi-Modality-Arena7.

Footnote 6: https://github.com/open-compass/VLMEvalKit

Footnote 7: https://github.com/OpenGVLab/Multi-Modality-Arena/tree/main/MedicalEval/Question-answering_Score

The assessment was performed in a "zero-shot" setting. Specifically, our evaluation prompts did not include any example cues, and the models were required to perform inference on tasks without prior training or examples related to those tasks. This approach better tests the models' generalization capabilities and comprehension, examining their performance when confronted with novel problems. All tests were executed using NVIDIA A100 GPUs with 80GB of memory.

### Models

For completeness, we conducted evaluations using several state-of-the-art LVLMs to benchmark their performance on GMAI-MMBench, including both general models that have extended capabilities in the biomedical domain and medical-specific models that are meticulously trained for clinical medicine. By default, we use the latest, largest, and best-performing available checkpoint for each model family to ensure optimal performance. We picked 29 out of 50 models for demonstration in the main text, additional results are provided in the supplementary material. For medical-specific models, we include 5 latest powerful LVLMs: MedDr [95], LLaVA-Med [138], Med-Flamingo [181], RadFM [254], and Qilin-Med-VL-Chat [149]. For general models, we test 18 representative LVLMs: TransCore-M [3], VisualGLM-6B [61], mPLUG-Owl2 [259], OmniLMM-12B [261], Mini-Gemini-7B [141], Emu2-Chat [237], MMAlaya [154], CogVLM-Chat [249], InstructBLIP-7B [56], DeepSeek-VL-7B [155], Idefics-9B-Instruct [137], XComposer2 [62], Yi-VL-6B [7], InternVL-Chat-V1.5 [46], LLAVA-V1.5-7B [148], LLAVA-InternLM2-7b [54], MiniCPM-V2 [257], and Qwen-VL-Chat [18]. In addition, we also evaluate 6 proprietary LVLMs via API: Qwen-VL-Max [18], Claude3-Opus [13], GPT-4V [5], GPT-4o [5], Gemini 1.0 [240], and Gemini 1.5 [211].

### Metrics

To evaluate the model's performance, we use macro-averaged accuracy (ACC) as the evaluation metric for single-choice questions. For multiple-choice questions, we first count the number of correct predictions for each case, then calculate accuracy (\(\mathrm{ACC_{mcq}}\)) and recall (\(\mathrm{Recall_{mcq}}\)) based on 

[MISSING_PAGE_FAIL:8]

VL-7B, although not as accurate as GPT-4o, have surpassed Claude3 Opus and Qwen-VL-Max, approaching the performance of GPT-4V. This suggests that open-source models in the medical field are gradually catching up to the top-performing commercialized models.

**Insufficiency 1. Performance on different clinical VQA tasks needs improvement:** Table 2 shows that the best-performing clinical VQA tasks are Disease Diagnosis (DD) and Nervous Tissue (NT), with models exceeding the random baseline by an average of over 10%. However, in clinical VQA tasks such as Severity Grading (SG) and Attribute Recognition (AR), most LVLMs face challenges, and most of them perform worse than the random baseline. Overall, despite the advanced models like GPT-4o and Gemini 1.5 significantly outperforming the random baseline, there remains a substantial gap between their performance and the requirements of real-world applications, indicating that all the models still need more specialized medical knowledge for training.

**Insufficiency 2. The performance across different departments needs further balancing:** In examining performance across different medical departments, as shown in Table 3, we found that the Infectious Diseases (ID) and Neurosurgery (N) departments performed the best. In contrast, departments such as General Surgery (GS) and Obstetrics and Gynecology (OG) showed a need for improvement, as the performance of all models in these areas did not significantly exceed the random baseline compared to other departments. This indicates that current large models exhibit specialization biases, suggesting that future development of LVLMs aiming to achieve general medical AI should focus on balancing capabilities across all departments.

**Insufficiency 3. The LVLMs are not robust among different perceptual types:** As shown in Figure 4, models perform slightly better with contour-level perception compared to mask-level perception, and both outperform image-level perception (without annotation) significantly. However, bounding box-level perception shows the worst performance among all perceptual types, indicating that models are sensitive to this perceptual type. This evaluation underscores the need for LVLMs to address robustness issues across different perceptual types, which is crucial for their effectiveness in interactive applications.

\begin{table}
\begin{tabular}{c|c c c c c c c c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Method name} & Overall & Omil & CS & O & E6 & G5 & G5 & H & D-G & UMP & MN & N & OO & OM & O & OS & METHODS & PM & SM & U \\ \hline \multirow{2}{*}{Ranking} & Text20 & 58.4 & 22.8 & 20.9 & 25.9 & 27.2 & 23.4 & 24.6 & 37.1 & 28.9 & 28.6 & 24.0 & 28.3 & 27.7 & 30.6 & 29.2 & 27.3 & 27.4 & 22.7 & 28.7 & 29.1 \\ \cline{2-19}  & Med-Frameps.(181) & 12.4 & 16.4 & 17.6 & 12.9 & 10.0 & 10.8 & 9.3 & 5.2 & 10.6 & 12.0 & 10.0 & 19.1 & 12.8 & 14.9 & 18.7 & 12.0 & 13.3 & 12.9 & 10.4 \\ \cline{2-19}  & Max-Action(181) & 22.4 & 16.4 & 17.6 & 12.9 & 10.0 & 10.8 & 9.3 & 5.2 & 10.0 & 10.0 & 19.1 & 12.8 & 14.9 & 18.9 & 15.7 & 12.0 & 13.3 & 14.3 & 12.9 & 10.4 \\ \cline{2-19}  & Max-Action(181) & 22.4 & 22.6 & 22.4 & 22.8 & 10.0 & 10.0 & 10.8 & 18.3 & 17.3 & 22.6 & 10.0 & 10.0 & 19.1 & 12.8 & 14.9 & 18.7 & 12.0 & 14.7 & 12.3 \\ \cline{2-19}  & Max-Action(181) & 22.4 & 22.6 & 22.4 & 22.8 & 10.0 & 10.0 & 10.8 & 18.3 & 17.3 & 22.6 & 10.0 & 10.0 & 18.3 & 18.0 & 18.3 & 18.0 & 18.4 & 18.7 & 12.0 & 12.7 & 12.3 \\ \cline{2-19}  & Max-Action(181) & 22.9 & 22.9 & 22.4 & 22.8 & 10.0 & 20.0 & 19.9 & 18.9 & 18.3 & 22.8 & 22.4 & 10.0 & 10.0 & 18.0 & 18.0 & 18.3 & 18.0 & 18.4 & 18.7 & 12.1 & 12.3 & 22.0 \\ \cline{2-19}  & Med-Frameps.(181) & 22.9 & 22.9 & 22.4 & 22.7 & 10.0 & 20.0 & 20.0 & 19.9 & 18.9 & 22.8 & 22.4 & 10.0 & 18.0 & 18.0 & 18.0 & 18.0 & 18.0 & 18.0 & 18.7 & 18.0 & 18.7 & 18.7 & 18.7 & 18.7 & 18.7 & 18.7 \\ \cline{2-19}  & Max 

**Insufficiency 4. Medical-specific models need to enhance their instruction tuning:** Interestingly, medical-specific models significantly underperform compared to general models, despite being trained and fine-tuned directly on relevant medical data. Specifically, LLaVA-Med is fine-tuned from the LLaVA model series in the medical field, but its performance is even worse than LLAVA-V1.5-7B. The primary reason for the poor performance of these medical-specific models is their inability to follow instructions correctly and their failure to understand or answer medical-related questions accurately. Detailed analysis can be found in the case study and supplementary materials sections on medical model analysis. Among these, the best-performing medical-specific model is MedDr, which is fine-tuned from the InternVL series and successfully surpasses the InternVL-Chat-V1.5. Unlike other medical-specific models that derive instruction-tuning data from papers, online sources, and books, MedDr builds its dataset based on high-quality medical image classification datasets. This result suggests that the quality of currently available medical instruction tuning datasets on the internet needs improvement and highlights the effectiveness of MedDr's dataset construction strategy, serving as a valuable reference for future medical-specific models.

**Insufficiency 5. The performance of most LVLMs on multiple-choice questions needs improvement:** Based on our tests, none of the models can totally match the correct answers (they always miss or over-select), so we adopt a relatively loose evaluation method for multiple-choice questions: using multi-choice hit rate (\(\mathrm{ACC}_{\mathrm{mcq}}\)) and recall rate (\(\mathrm{Recall}_{\mathrm{mcq}}\)). The experimental results are shown in Figure 5. Using this method, we found that most models have an accuracy rate of around 40%-50% and a recall rate of around 40%-60%. Surprisingly, InternVL-Chat-V1.5 and Qwen-VL-Max performed well in single-choice questions but showed very poor recall and accuracy rates in multiple-choice questions. In contrast, Qwen-VL-Chat and CogVLM-Chat, which performed relatively poorly in single-choice questions, achieved very high recall rates and moderate accuracy rates in multiple-choice questions, especially CogVLM-Chat with over 90% recall rate. Nonetheless, even with this less strict evaluation method, all models had accuracy rates below 55%, indicating that there is still significant room for improvement in answering multiple-choice questions.

#### Case Study

We further analyze the results by requiring the models to output content beyond the provided options and explain their reasoning process. This approach helps us better understand the causes of errors. Through detailed testing and analysis, we identify 5 typical errors present in the LVLMs:

**Question misunderstanding:** This occurs when the model incorrectly understands the purpose of the question, leading to an inability to provide a correct response. As shown in Figure 6A, the model is asked to answer a multiple-choice question, but it describes the problem or repeats the options rather than choosing an option.

**Perceptual Error:** These errors occur when there is a mislocation or misrecognition of image content. This means that the model's understanding or interpretation of the visual content is incorrect, leading to an inaccurate response. As shown in Figure 6B, the model mistakenly identifies the esophagus as the spine, suggesting that while the model can locate the target on the image (The annotated esophagus is very close to the spine), it makes an error in perceiving the masked content.

Figure 6: Three examples of error cases. **A:** Question misunderstanding. **B:** Perceptual Errors. **C:** Lack of Knowledge. More studies can be found in the appendix.

**Lack of knowledge:** While the model can recognize text and images, it makes errors in specific areas that require specific knowledge, indicating a deficiency in relevant training or fine-tuning in those areas. For example, in Figure 6C, the model incorrectly identifies the staining method as Ziehl-Neelsen and misrecognizes the blue-stained structure as Mycobacterium tuberculosis, where it is actually a white blood cell stained with Giemsa or Wright stain. This error indicates the model's lack of knowledge in experimental medicine.

**Irrelevant Responses:** This error indicates the model fails to generate a readable answer, which is easily found in medical-specific models like RadFM. Examples are listed in the appendix.

**Reject to Answer:** Some models, especially proprietary LVLMs like GPT-4V, GPT-4o, Gemini 1.0, and Gemini 1.5, commonly refuse to provide an answer due to policy reasons, because safety is crucial according to the commercial rules and regulations. Many potentially risky responses are declined to ensure compliance with guidelines. Those models' strict adherence to safety protocols and ethical standards limits response capabilities in certain domains.

## Conclusion

The development of GMAI-MMBench as a benchmark for evaluating LVLMs' capabilities represents a significant advancement in the pursuit of general medical AI. GMAI-MMBench epitomizes the expertise of skilled medical professionals, serving as a pivotal guide for advancing large models toward GMAI by testing the limits of current LVLMs. Owing to the extensive and diverse source of GMAI-MMBench, which comprises medical datasets annotated by professional healthcare providers worldwide, this benchmark can comprehensively evaluate the model's capability across various specific aspects. In this way, GMAI-MMBench can guide the model development at a more fine-grained level, accelerating the development of robust and reliable GMAI systems. Moreover, this benchmark supports the advancement of interactive multimodal medical models by providing more perceptual modes and annotations that are commonly used by physicians in clinical practice, thereby creating a framework for their evaluation and improvement.

However, GMAI-MMBench, like all benchmarks, has its limitations. The manual curation process, despite being thorough, might introduce biases, and focusing solely on medical subjects may not fully meet the criteria for general medical AI as defined. Nevertheless, we assert that high performance on GMAI-MMBench is essential for demonstrating the extensive subject knowledge and expert-level reasoning skills required for general medical AI. Looking ahead, we intend to integrate human evaluations into GMAI-MMBench. This addition will offer a more grounded comparison between model capabilities and expert performance, providing insights into how close current AI systems are achieving general medical AI in the medical field.

## Acknowledgement

This work was supported by the National Key Research and Development Program of China (Grant No. 2022ZD0160101). Additionally, the authors acknowledge the funding provided by the National Natural Science Foundation of China (Grant No. 62272450), which has been instrumental in facilitating the research and development efforts presented in this study.

## References

* [1]https://peir.path.uab.edu/library/index.php?/category/2.
* [2]https://medpix.nlm.nih.gov/home.
* [3]https://github.com/PCIRresearch/TransCore-M.
* [4] Michael D Abramoff, James C Folk, Dennis P Han, Jonathan D Walker, David F Williams, Stephen R Russell, Pascale Massin, Beatrice Cochener, Philippe Gain, Li Tang, et al. Automated analysis of retinal images for detection of referable diabetic retinopathy. _JAMA ophthalmology_, 131(3):351-357, 2013.
* [5] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.

* [6] Hugo JWL Aerts, Emmanuel Rios Velazquez, Ralph TH Leijenaar, Chintan Parmar, Patrick Grossmann, Sara Carvalho, Johan Bussink, Rene Monshouwer, Benjamin Haibe-Kains, Derek Rietveld, et al. Decoding tumour phenotype by noninvasive imaging using a quantitative radiomics approach. _Nature communications_, 5(1):4006, 2014.
* [7] 01. AI, ; Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open foundation models by 01.ai, 2024.
* [8] Walid Al-Dhabyani, Mohammed Gomaa, Hussien Khaled, and Aly Fahmy. Dataset of breast ultrasound images. _Data in brief_, 28:104863, 2020.
* [9] Sharib Ali, Noha Ghatwary, Barbara Braden, Dominique Lamarque, Adam Bailey, Stefano Realdon, Renato Cannizzaro, Jens Rittscher, Christian Daul, and James East. Endoscopy disease detection challenge 2020. _arXiv preprint arXiv:2003.03376_, 2020.
* [10] Max Allan, Satoshi Kondo, Sebastian Bodenstedt, Stefan Leger, Rahim Kadkhodamohammadi, Imanol Luengo, Felix Fuentes, Evangello Flouty, Ahmed Mohammed, Marius Pedersen, et al. 2018 robotic scene segmentation challenge. _arXiv preprint arXiv:2001.11190_, 2020.
* [11] Max Allan, Alex Shvets, Thomas Kurmann, Zichen Zhang, Rahul Duggal, Yun-Hsuan Su, Nicola Rieke, Iro Laina, Niveditha Kalavakonda, Sebastian Bodenstedt, et al. 2017 robotic instrument segmentation challenge. _arXiv preprint arXiv:1902.06426_, 2019.
* [12] Mohamed Amgad, Habiba Elfandy, Hagar Hussein, Lamees A Atteya, Mai AT Elsebaie, Lamia S Abo Elnasr, Rokia A Sakr, Hazem SE Salem, Ahmed F Ismail, Anas M Saad, et al. Structured crowdsourcing enables convolutional segmentation of histology images. _Bioinformatics_, 35(18):3461-3467, 2019.
* [13] AI Anthropic. The claude 3 model family: Opus, sonnet, haiku. _Claude-3 Model Card_, 2024.
* [14] Michela Antonelli, Annika Reinke, Spyridon Bakas, Keyvan Farahani, Annette Kopp-Schneider, Bennett A Landman, Geert Litjens, Bjoern Menze, Olaf Ronneberger, Ronald M Summers, et al. The medical segmentation decathlon. _Nature communications_, 13(1):4128, 2022.
* [15] Guilherme Aresta, Teresa Araujo, Scotty Kwok, Sai Saketh Chennamsetty, Mohammed Safwan, Varghese Alex, Bahram Marami, Marcel Prastawa, Monica Chan, Michael Donovan, et al. Bach: Grand challenge on breast cancer histology images. _Medical image analysis_, 56:122-139, 2019.
* [16] Amanullah Asraf and Zabirul Islam. Covid19, pneumonia and normal chest x-ray pa dataset. _Mendeley Data_, 1:2, 2021.
* [17] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo: An open-source framework for training large autoregressive vision-language models. _arXiv preprint arXiv:2308.01390_, 2023.
* [18] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. _arXiv preprint arXiv:2308.12966_, 2023.
* [19] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond, 2023.
* [20] Ujjwal Baid, Satyam Ghodasara, Suyash Mohan, Michel Bilello, Evan Calabrese, Errol Colak, Keyvan Farahani, Jayashree Kalpathy-Cramer, Felipe C Kitamura, Sarthak Pati, et al. The rsna-asnr-mincai brats 2021 benchmark on brain tumor segmentation and radiogenomic classification. _arXiv preprint arXiv:2107.02314_, 2021.

* [21] Spyridon Bakas, Hamed Akbari, Aristeidis Sotiras, Michel Bilello, Martin Rozycki, Justin Kirby, John Freymann, Keyvan Farahani, and Christos Davatzikos. Segmentation labels and radiomic features for the pre-operative scans of the tcga-lgg collection. _The cancer imaging archive_, 286, 2017.
* [22] Spyridon Bakas, Hamed Akbari, Aristeidis Sotiras, Michel Bilello, Martin Rozycki, Justin S Kirby, John B Freymann, Keyvan Farahani, and Christos Davatzikos. Advancing the cancer genome atlas glioma mri collections with expert segmentation labels and radiomic features. _Scientific data_, 4(1):1-13, 2017.
* [23] Spyridon Bakas, Mauricio Reyes, Andras Jakab, Stefan Bauer, Markus Rempfler, Alessandro Crimi, Russell Takeshi Shinohara, Christoph Berger, Sung Min Ha, Martin Rozycki, et al. Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the brats challenge. _arXiv preprint arXiv:1811.02629_, 2018.
* [24] Vivek Singh Bawa, Gurkirt Singh, Francis KapingA, Inna Skarga-Bandurova, Alice Leporini, Carmela Landolfo, Armando Stabile, Francesco Setti, Riccardo Muradore, Elettra Oleari, et al. Esad: Endoscopic surgeon action detection dataset. _arXiv preprint arXiv:2006.07164_, 2020.
* [25] Vivek Singh Bawa, Gurkirt Singh, Francis KapingA, Inna Skarga-Bandurova, Elettra Oleari, Alice Leporini, Carmela Landolfo, Pengfei Zhao, Xi Xiang, Gongning Luo, et al. The saras endoscopic surgeon action detection (esad) dataset: Challenges and methods. _arXiv preprint arXiv:2104.03178_, 2021.
* [26] Veronica Elisa Castillo Benitez, Ingrid Castro Matto, Julio Cesar Mello Roman, Jose Luis Vazquez Noguera, Miguel Garcia-Torres, Jordan Ayala, Diego P Pinto-Roa, Pedro E Gardel-Sotomayor, Jacques Facon, and Sebastian Alberto Grillo. Dataset from fundus images for the study of diabetic retinopathy. _Data in brief_, 36:107068, 2021.
* [27] jjones BenO, Kumar H, Meg Risdal, Vadim Sherman MRao, Wendy Kan Vipul, and Yau Ben-Or. Intel & mobiledot cervical cancer screening, 2017.
* [28] Jorge Bernal, F Javier Sanchez, Gloria Fernandez-Esparrach, Debora Gil, Cristina Rodriguez, and Fernando Vilarino. Wm-dova maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians. _Computerized medical imaging and graphics_, 43:99-111, 2015.
* [29] Jorge Bernal, Nima Tajkbaksh, Francisco Javier Sanchez, Bogdan J Matuszewski, Hao Chen, Lequan Yu, Quentin Angermann, Olivier Romain, Bjorn Rustad, Ilangko Balasingham, et al. Comparative validation of polyp detection methods in video colonoscopy: results from the miccai 2015 endoscopic vision challenge. _IEEE transactions on medical imaging_, 36(6):1231-1249, 2017.
* [30] Olivier Bernard, Alain Lalande, Clement Zotti, Frederick Cervenansky, Xin Yang, Pheng-Ann Heng, Irem Cetin, Karim Lekadir, Oscar Camara, Miguel Angel Gonzalez Ballester, et al. Deep learning techniques for automatic mri cardiac multi-structures segmentation and diagnosis: is the problem solved? _IEEE transactions on medical imaging_, 37(11):2514-2525, 2018.
* [31] Aditya Bharatha, Masanori Hirose, Nobuhiko Hata, Simon K Warfield, Matthieu Ferrant, Kelly H Zou, Eduardo Suarez-Santana, Juan Ruiz-Alzola, Anthony D'amico, Robert A Cormack, et al. Evaluation of three-dimensional finite element-based deformable registration of pre-and intraoperative prostate imaging. _Medical physics_, 28(12):2551-2560, 2001.
* [32] Patrick Bilic, Patrick Christ, Hongwei Bran Li, Eugene Vorontsov, Avi Ben-Cohen, Georgios Kaissis, Adi Szeskin, Colin Jacobs, Gabriel Efrain Humpire Mamani, Gabriel Chartrand, et al. The liver tumor segmentation benchmark (lits). _Medical Image Analysis_, 84:102680, 2023.
* [33] H Menze Bjoern, Jakab Andras, Bauer Stefan, Kalpathy-Cramer Jayashree, Farahani Keyvan, Kirby Justin, et al. The multimodal brain tumor image segmentation benchmark (brats). _IEEE Trans. Med. Imaging_, 34(10):1993-2024, 2015.

* [34] Andrew A Borkowski, Marilyn M Bui, L Brannon Thomas, Catherine P Wilson, Lauren A DeLand, and Stephen M Mastorides. Lung and colon cancer histopathological image dataset (lc25000). _arXiv preprint arXiv:1912.12142_, 2019.
* [35] Attila Budai, R\({}^{\text{u}}\)udiger Bock, Andreas Maier, Joachim Hornegger, and Georg Michelson. Robust vessel segmentation in fundus images. _International journal of biomedical imaging_, 2013(1):154860, 2013.
* [36] Wouter Bulten, Kimmo Kartasalo, Po-Hsuan Cameron Chen, Peter Str\({}^{\text{u}}\)om, Hans Pinckaers, Kunal Nagpal, Yuannan Cai, David F Steiner, Hester Van Boven, Robert Vink, et al. Artificial intelligence for diagnosis and gleason grading of prostate cancer: the panda challenge. _Nature medicine_, 28(1):154-163, 2022.
* [37] Evan Calabrese, Javier E Villanueva-Meyer, Jeffrey D Rudie, Andreas M Rauschecker, Ujjwal Baid, Spyridon Bakas, Soonmee Cha, John T Mongan, and Christopher P Hess. The university of california san francisco preoperative diffuse glioma mri dataset. _Radiology: Artificial Intelligence_, 4(6):e220058, 2022.
* [38] Gabriele Campanella, Matthew G Hanna, Luke Geneslaw, Allen Miraflor, Vitor Werneck Krauss Silva, Klaus J Busam, Edi Brogi, Victor E Reuter, David S Klimstra, and Thomas J Fuchs. Clinical-grade computational pathology using weakly supervised deep learning on whole slide images. _Nature medicine_, 25(8):1301-1309, 2019.
* [39] Aaron Carass, Snehashis Roy, Amod Jog, Jennifer L Cuzzocreo, Elizabeth Magrath, Adrian Gherman, Julia Button, James Nguyen, Ferran Prados, Carole H Sudre, et al. Longitudinal multiple sclerosis lesion segmentation: resource and challenge. _NeuroImage_, 148:77-102, 2017.
* [40] C. Cardenas, A. Mohamed, G. Sharp, M. Gooding, H. Veeraraghavan, and J. & Yang. Data from aapm rt-mac grand challenge 2019. _The Cancer Imaging Archive_, 2019.
* [41] Ling-Ping Cen, Jie Ji, Jian-Wei Lin, Si-Tong Ju, Hong-Jie Lin, Tai-Ping Li, Yun Wang, Jian-Feng Yang, Yu-Fen Liu, Shaoying Tan, et al. Automatic detection of 39 fundus diseases and conditions in retinal photographs using deep neural networks. _Nature communications_, 12(1):4828, 2021.
* [42] Santiago Cepeda, Sergio Garcia-Garcia, Ignacio Arrese, Francisco Herrero, Trinidad Escudero, Tomas Zamora, and Rosario Sarabia. The rio hortega university hospital glioblastoma dataset: A comprehensive collection of preoperative, early postoperative and recurrence mri scans (rhuh-gbm). _Data in Brief_, 50:109617, 2023.
* [43] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions, 2023.
* [44] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? _arXiv preprint arXiv:2403.20330_, 2024.
* [45] Pingjun Chen. Knee osteoarthritis severity grading dataset. _Mendeley Data_, 1(10.17632), 2018.
* [46] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. _arXiv preprint arXiv:2404.16821_, 2024.
* [47] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. _arXiv preprint arXiv:2312.14238_, 2023.
* [48] R Chitalia et al. Expert tumor annotations and radiomic features for the ispy1/acrin 6657 trial data collection. _The Cancer Imaging Archive_, 2022.

* [49] Stephanie J Chiu, Yuliya Lokhnygina, Adam M Dubis, Alfredo Dubra, Joseph Carroll, Joseph A Izatt, and Sina Farsiu. Automatic cone photoreceptor segmentation using graph theory and dynamic programming. _Biomedical optics express_, 4(6):924-937, 2013.
* [50] Stephanie J Chiu, Cynthia A Toth, Catherine Bowes Rickman, Joseph A Izatt, and Sina Farsiu. Automatic segmentation of closed-contour features in ophthalmic images using graph theory and dynamic programming. _Biomedical optics express_, 3(5):1127-1140, 2012.
* [51] Muhammad EH Chowdhury, Tawsifur Rahman, Amith Khandakar, Rashid Mazhar, Muhammad Abdul Kadir, Zaid Bin Mahbub, Khandakar Reajul Islam, Muhammad Salman Khan, Atif Iqbal, Nasser Al Emadi, et al. Can ai help in screening viral and covid-19 pneumonia? _Ieee Access_, 8:132665-132676, 2020.
* [52] Joseph Paul Cohen, Paul Morrison, Lan Dao, Karsten Roth, Tim Q Duong, and Marzyeh Ghassemi. Covid-19 image data collection: Prospective predictions are the future. _arXiv preprint arXiv:2006.11988_, 2020.
* [53] Olivier Commowick, Micha"el Kain, Romain Casey, Roxana Ameli, Jean-Christophe Ferre, Anne Kerbrat, Thomas Tourdias, Frederic Cervenansky, Sorina Camarasu-Pop, Tristan Glatard, et al. Multiple sclerosis lesions segmentation from multiple experts: The miccai 2016 challenge dataset. _Neuroimage_, 244:118589, 2021.
* [54] XTuner Contributors. Xtuner: A toolkit for efficiently fine-tuning llm. https://github.com/InternLM/xtuner, 2023.
* [55] Qian Da, Xiaodi Huang, Zhongyu Li, Yanfei Zuo, Chenbin Zhang, Jingxin Liu, Wen Chen, Jiahui Li, Dou Xu, Zhiqiang Hu, et al. Digestpath: A benchmark dataset with challenge review for the pathological detection and segmentation of digestive-system. _Medical Image Analysis_, 80:102485, 2022.
* [56] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. _Advances in Neural Information Processing Systems_, 36, 2024.
* [57] Parisa Karimi Darabi. Diagnosis of diabetic retinopathy.
* [58] Coen De Vente, Koenraad A Vermeer, Nicolas Jaccard, He Wang, Hongyi Sun, Firas Khader, Daniel Truhn, Temirgali Aimyshev, Yerkebulan Zhanibekuly, Tien-Dung Le, et al. Airogs: artificial intelligence for robust glaucoma screening challenge. _IEEE transactions on medical imaging_, 2023.
* [59] Yang Deng, Ce Wang, Yuan Hui, Qian Li, Jun Li, Shiwei Luo, Mengke Sun, Quan Quan, Shuxin Yang, You Hao, et al. Ctspine1k: A large-scale dataset for spinal vertebrae segmentation in computed tomography. _arXiv preprint arXiv:2105.14711_, 2021.
* [60] Andres Diaz-Pinto, Sandra Morales, Valery Naranjo, Thomas K\({}^{\text{\textregistered}}\)ohler, Jose M Mossi, and Amparo Navea. Cnns for automatic glaucoma assessment using fundus images: an extensive validation. _Biomedical engineering online_, 18:1-19, 2019.
* [61] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. _Advances in Neural Information Processing Systems_, 34:19822-19835, 2021.
* [62] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. _arXiv preprint arXiv:2401.16420_, 2024.

* [63] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang, Haodong Duan, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Zhe Chen, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Kai Chen, Conghui He, Xingcheng Zhang, Jifeng Dai, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2-4khd: A pioneering large vision-language model handling resolutions from 336 pixels to 4k hd. _arXiv preprint arXiv:2404.06512_, 2024.
* [64] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, Dahua Lin, and Kai Chen. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models, 2024.
* [65] Emma Dugas, Jared, Jorge, and Will Cukierski. Diabetic retinopathy detection, 2015.
* [66] DungNB, Ha Q. Nguyen, Julia Elliott, KeepLearning, NguyenThanhNhan, and Phil Culliton. Vnbigdata chest x-ray abnormalities detection, 2020.
* [67] Rolando Estrada, Carlo Tomasi, Scott C Schmidler, and Sina Farsiu. Tree topology estimation. _IEEE transactions on pattern analysis and machine intelligence_, 37(8):1688-1701, 2014.
* [68] Huihui Fang, Fei Li, Huazhu Fu, Xu Sun, Xingxing Cao, Fengbin Lin, Jaemin Son, Sunho Kim, Gwenole Quellec, Sarah Matta, et al. Adam challenge: Detecting age-related macular degeneration from fundus images. _IEEE transactions on medical imaging_, 41(10):2828-2847, 2022.
* [69] Huihui Fang, Fei Li, Huazhu Fu, Junde Wu, Xiulan Zhang, and Yanwu Xu. Dataset and evaluation algorithm design for goals challenge. In _International Workshop on Ophthalmic Medical Image Analysis_, pages 135-142. Springer, 2022.
* [70] A Fedorov, M Schwier, D Clunie, C Herz, S Pieper, R Kikinis, C Tempany, and F Fennessy. Data from qin-prostate-repeatability. _The Cancer Imaging Archive_, 2018.
* [71] Adam E Flanders, Luciano M Prevedello, George Shih, Safwan S Halabi, Jayashree Kalpathy-Cramer, Robyn Ball, John T Mongan, Anouk Stein, Felipe C Kitamura, Matthew P Lungren, et al. Construction of a machine learning dataset through collaboration: the rsna 2019 brain ct hemorrhage challenge. _Radiology: Artificial Intelligence_, 2(3):e190211, 2020.
* [72] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: A comprehensive evaluation benchmark for multimodal large language models, 2024.
* [73] Huazhu Fu, Jun Cheng, Yanwu Xu, Damon Wing Kee Wong, Jiang Liu, and Xiaochun Cao. Joint optic disc and cup segmentation based on multi-label deep network and polar transformation. _IEEE transactions on medical imaging_, 37(7):1597-1605, 2018.
* [74] Huazhu Fu, Fei Li, Xu Sun, Xingxing Cao, Jingan Liao, Jose Ignacio Orlando, Xing Tao, Yuexiang Li, Shihao Zhang, Mingkui Tan, et al. Age challenge: angle closure glaucoma evaluation in anterior segment optical coherence tomography. _Medical Image Analysis_, 66:101798, 2020.
* [75] Francisco Fumero, Silvia Alayon, Jose L Sanchez, Jose Sigut, and M Gonzalez-Hernandez. Rim-one: An open retinal image database for optic nerve evaluation. In _2011 24th international symposium on computer-based medical systems (CBMS)_, pages 1-6. IEEE, 2011.
* [76] Radovan Fusek. Pupil localization using geodesic distance. In _Advances in Visual Computing: 13th International Symposium, ISVC 2018, Las Vegas, NV, USA, November 19-21, 2018, Proceedings 13_, pages 433-444. Springer, 2018.
* [77] Jevgenij Gamper, Navid Alemi Koohbanani, Ksenija Benet, Ali Khuram, and Nasir Rajpoot. Pannuke: an open pan-cancer histology dataset for nuclei instance segmentation and classification. In _European Congress on Digital Pathology_, pages 11-19. Springer, 2019.
* [78] Jevgenij Gamper, Navid Alemi Koohbanani, Simon Graham, Mostafa Jahanifar, Syed Ali Khurram, Ayesha Azam, Katherine Hewitt, and Nasir Rajpoot. Pannuke dataset extension, insights and baselines. _arXiv preprint arXiv:2003.10778_, 2020.

* [79] Elisa Drelie Gelasca, Jiyun Byun, Boguslaw Obara, and BS Manjunath. Evaluation and benchmark for biological image segmentation. In _2008 15th IEEE international conference on image processing_, pages 1816-1819. IEEE, 2008.
* [80] Ioannis Giotis, Nynke Molders, Sander Land, Michael Biehl, Marcel F Jonkman, and Nicolai Petkov. Med-node: A computer-assisted melanoma diagnosis system using non-dermoscopic images. _Expert systems with applications_, 42(19):6578-6585, 2015.
* [81] Y Glick. Viewing playlist: Covid-19 pneumonia. _Radiopaedia. org_, 2020.
* [82] HL Goldgof Dmitry, Hawkins Samuel, Schabath Matthew, Stringfield Olya, Garcia Alberto, Balagurunathan Yoganand, Kim Jongphil, Eschrich Steven, Berglund Anders, Gatenby Robert, et al. Long and short survival in adenocarcinoma lung cts. _The Cancer Imaging Archive_, 2017.
* [83] German Gonzalez, Daniel Jimenez-Carretero, Sara Rodriguez-Lopez, Carlos Cano-Espinosa, Miguel Cazorla, Tanya Agarwal, Vinit Agarwal, Nima Tajbakhsh, Michael B Gotway, Jianming Liang, et al. Computer aided detection for pulmonary embolism challenge (cad-pe). _arXiv preprint arXiv:2003.13440_, 2020.
* [84] Simon Graham, Hao Chen, Jevgenij Gamper, Qi Dou, Pheng-Ann Heng, David Snead, Yee Wah Tsang, and Nasir Rajpoot. Mild-net: Minimal information loss dilated network for gland instance segmentation in colon histology images. _Medical image analysis_, 52:199-211, 2019.
* [85] Matthew Groh, Caleb Harris, Luis Soenksen, Felix Lau, Rachel Han, Aerin Kim, Arash Koochek, and Omar Badri. Evaluating deep neural networks trained on clinical images in dermatology with the fitzpatrick 17k dataset. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1820-1828, 2021.
* micai 2018 challenge intervertebral disc localization and segmentation from 3d multi-modality mr (m3) images (2018). 2019.
* [87] Anubha Gupta and Ritu Gupta. Isbi 2019 c-nmc challenge: Classification in cancer cell imaging. _Select Proceedings_, 2019.
* [88] David Gutman, Noel CF Codella, Emre Celebi, Brian Helba, Michael Marchetti, Nabin Mishra, and Allan Halpern. Skin lesion analysis toward melanoma detection: A challenge at the international symposium on biomedical imaging (isbi) 2016, hosted by the international skin imaging collaboration (isic). _arXiv preprint arXiv:1605.01397_, 2016.
* [89] Safwan S Halabi, Luciano M Prevedello, Jayashree Kalpathy-Cramer, Artem B Mamonov, Alexander Bilbily, Mark Cicero, Ian Pan, Lucas Araujo Pereira, Rafael Teixeira Sousa, Nitamar Abdala, et al. The rsna pediatric bone age machine learning challenge. _Radiology_, 290(2):498-503, 2019.
* [90] Mohamed Hany. Chest ct-scan images dataset.
* [91] happyharrycn, Maggie, Phil Culliton, Poonam Yadav, and Sangjune Laurence Lee. Uvmadison gi tract image segmentation, 2022.
* [92] Khaled Harrar. Texture characterization of bone radiograph images. application to osteoporosis diagnosis. 2014.
* [93] Ali Hatamizadeh, Hamid Hosseini, Niraj Patel, Jinseo Choi, Cameron C Pole, Cory M Hoeferlin, Steven D Schwartz, and Demetri Terzopoulos. Ravir: A dataset and methodology for the semantic segmentation and quantitative analysis of retinal arteries and veins in infrared reflectance imaging. _IEEE Journal of Biomedical and Health Informatics_, 26(7):3272-3283, 2022.
* [94] Georges Hattab, Marvin Arnold, Leon Strenger, Max Allan, Darja Arsentejeva, Oliver Gold, Tobias Simpfend"orfer, Lena Maier-Hein, and Stefanie Speidel. Kidney edge detection in laparoscopic image data for computer-assisted surgery: Kidney edge detection. _International journal of computer assisted radiology and surgery_, 15:379-387, 2020.

* [95] Sunan He, Yuxiang Nie, Zhixuan Chen, Zhiyuan Cai, Hongmei Wang, Shu Yang, and Hao Chen. Meddr: Diagnosis-guided bootstrapping for large-scale medical vision-language learning. _arXiv preprint arXiv:2404.15127_, 2024.
* [96] Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. Pathvqa: 30000+ questions for medical visual question answering. _arXiv preprint arXiv:2003.10286_, 2020.
* [97] Yuting He, Guanyu Yang, Jian Yang, Yang Chen, Youyong Kong, Jiasong Wu, Lijun Tang, Xiaomei Zhu, Jean-Louis Dillenseger, Pengfei Shao, et al. Dense biased networks with deep priori anatomy and hard region adaptation: Semi-supervised learning for fine renal artery segmentation. _Medical image analysis_, 63:101722, 2020.
* [98] Tobias Heimann, Bram Van Ginneken, Martin A Styner, Yulia Arzhaeva, Volker Aurich, Christian Bauer, Andreas Beck, Christoph Becker, Reinhard Beichel, Gy\({}^{\rm i}\)orgy Bekes, et al. Comparison and evaluation of methods for liver segmentation from ct datasets. _IEEE transactions on medical imaging_, 28(8):1251-1265, 2009.
* [99] Nicholas Heller, Niranjan Sathianathen, Arveen Kalapara, Edward Walczak, Keenan Moore, Heather Kaluzniak, Joel Rosenberg, Paul Blake, Zachary Rengel, Makinna Oestreich, et al. The kits19 challenge data: 300 kidney tumor cases with clinical context, ct semantic segmentations, and surgical outcomes. _arXiv preprint arXiv:1904.00445_, 2019.
* [100] Carlos Hernandez-Matas, Xenophon Zabulis, Areti Triantafyllou, Panagiota Anyfanti, Stella Douma, and Antonis A Argyros. Fire: fundus image registration dataset. _Modeling and Artificial Intelligence in Ophthalmology_, 1(4):16-28, 2017.
* [101] Laurens Hogeweg, Clara I Sanchez, Pim A de Jong, Pragnya Maduskar, and Bram van Ginneken. Clavicle segmentation in chest radiographs. _Medical image analysis_, 16(8):1490-1502, 2012.
* [102] Murtadha Hssayeni, M Croock, A Salman, H Al-khafaji, Z Yahya, and B Ghoraani. Computed tomography images for intracranial hemorrhage detection and segmentation. _Intracranial hemorrhage segmentation using a deep convolutional model. Data_, 5(1):14, 2020.
* [103] Jinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu Pan, Qianyu Chen, Tianyu Yu, Hanghao Wu, Yue Zhao, Haoye Zhang, Xu Han, Yankai Lin, Jiao Xue, Dahai Li, Zhiyuan Liu, and Maosong Sun. Large multilingual models pivot zero-shot multimodal learning across languages, 2024.
* [104] Qiao Hu, Michael D Abramoff, and Mona K Garvin. Automated separation of binary overlapping trees in low-contrast color retinal images. In _Medical Image Computing and Computer-Assisted Intervention-MICCAI 2013: 16th International Conference, Nagoya, Japan, September 22-26, 2013, Proceedings, Part II 16_, pages 436-443. Springer, 2013.
* [105] Xinyue Hu, Lin Gu, Qiyuan An, Mengliang Zhang, Liangchen Liu, Kazuma Kobayashi, Tatsuya Harada, Ronald M Summers, and Yingying Zhu. Expert knowledge-aware image difference graph representation learning for difference-aware medical visual question answering. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 4156-4165, 2023.
* [106] Yutao Hu, Tianbin Li, Quanfeng Lu, Wenqi Shao, Junjun He, Yu Qiao, and Ping Luo. Omnimedvqa: A new large-scale comprehensive evaluation benchmark for medical lvlm. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22170-22183, 2024.
* [107] F Huazhu, L Fei, and IO Jose. Palm: pathologic myopia challenge. comput. vis. med. _Imaging_, 12, 2019.
* [108] Towhidul Islam, Mohammad Arafat Hussain, Forhad Uddin Hasan Chowdhury, and BM Riazul Islam. A web-scraped skin image database of monkeypx, chickenpox, smallpox, cowpox, and measles. _biorxiv_, pages 2022-08, 2022.

* [109] Stefan Jaeger, Sema Candemir, Sameer Antani, Yi-Xiang J Wang, Pu-Xuan Lu, and George Thoma. Two public chest x-ray datasets for computer-aided screening of pulmonary diseases. _Quantitative imaging in medicine and surgery_, 4(6):475, 2014.
* [110] Jan Jantzen, Jonas Norup, Georgios Dounias, and Beth Bjerregaard. Pap-smear benchmark data for pattern classification. _Nature inspired smart information systems (NiSIS 2005)_, pages 1-9, 2005.
* [111] Guillaume Jaume, Pushpak Pati, Valentin Anklin, Antonio Foncubierta, and Maria Gabrani. Histocartography: A toolkit for graph analytics in digital pathology. In _MICCAI Workshop on Computational Pathology_, pages 117-128, 2021.
* [112] Soroush Javadi and Seyed Abolghasem Mirroshandel. A novel deep learning method for automatic assessment of human sperm images. _Computers in biology and medicine_, 109:182-194, 2019.
* [113] K Jayashree and N Sandy. Multi-site collection of lung ct data with nodule segmentations. _J. Digit. Imaging_, pages 1-9, 2015.
* [114] Debesh Jha, Pia H Smedsrud, Michael A Riegler, Pal Halvorsen, Thomas De Lange, Dag Johansen, and Havard D Johansen. Kvasir-seg: A segmented polyp dataset. In _MultiMedia Modeling: 26th International Conference, MMM 2020, Daejeon, South Korea, January 5-8, 2020, Proceedings, Part II 26_, pages 451-462. Springer, 2020.
* [115] Debesh Jha, Nikhil Kumar Tomar, Sharib Ali, Michael A Riegler, Havard D Johansen, Dag Johansen, Thomas de Lange, and Pal Halvorsen. Nanonet: Real-time polyp segmentation in video capsule endoscopy and colonoscopy. In _2021 IEEE 34th International Symposium on Computer-Based Medical Systems (CBMS)_, pages 37-43. IEEE, 2021.
* [116] Yuanfeng Ji, Haotian Bai, Chongjian Ge, Jie Yang, Ye Zhu, Ruimao Zhang, Zhen Li, Lingyan Zhang, Wanling Ma, Xiang Wan, et al. Amos: A large-scale abdominal multi-organ benchmark for versatile medical image segmentation. _Advances in Neural Information Processing Systems_, 35:36722-36732, 2022.
* [117] Amy Jin, Serena Yeung, Jeffrey Jopling, Jonathan Krause, Dan Azagury, Arnold Milstein, and Li Fei-Fei. Tool detection and operative skill assessment in surgical videos using region-based convolutional neural networks. In _2018 IEEE winter conference on applications of computer vision (WACV)_, pages 691-699. IEEE, 2018.
* [118] Liang Jin, Jiancheng Yang, Kaiming Kuang, Bingbing Ni, Yiyi Gao, Yingli Sun, Pan Gao, Weiling Ma, Mingyu Tan, Hui Kang, Jiajun Chen, and Ming Li. Deep-learning-assisted detection and segmentation of rib fractures from ct scans: Development and validation of fracnet. _eBioMedicine_, 2020.
* [119] Yuan Jin, Antonio Pepe, Jianning Li, Christina Gsaxner, Fen-hua Zhao, Kelsey L Pomykala, Jens Kleesiek, Alejandro F Frangi, and Jan Egger. Ai-based aortic vessel tree segmentation for cardiovascular diseases treatment: status quo. _arXiv preprint arXiv:2108.02998_, 2021.
* [120] Alistair EW Johnson, Tom J Pollard, Nathaniel R Greenbaum, Matthew P Lungren, Chihying Deng, Yifan Peng, Zhiyong Lu, Roger G Mark, Seth J Berkowitz, and Steven Horng. Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs. _arXiv preprint arXiv:1901.07042_, 2019.
* [121] JR2NGB. Cataract image dataset.
* [122] Ma Jun, Ge Cheng, Wang Yixin, An Xingle, Gao Jiantao, Yu Ziqi, Zhang Minqing, Liu Xin, Deng Xueyuan, Cao Shucheng, et al. Covid-19 ct lung and infection segmentation dataset. 2020.
* [123] Aasheesh Kanwar, Brandon Merz, Cheryl Claunch, Shushan Rana, Arthur Hung, and Reid F Thompson. Stress-testing pelvic autosegmentation algorithms using anatomical edge cases. _Physics and Imaging in Radiation Oncology_, 25:100413, 2023.

* [124] Rashed Karim, Lauren-Emma Blake, Jiro Inoue, Qian Tao, Shuman Jia, R James Housden, Pranav Bhagirath, Jean-Luc Duval, Marta Varela, Jonathan M Behar, et al. Algorithms for left atrial wall segmentation and thickness-evaluation on an open-source ct and mri image database. _Medical image analysis_, 50:36-53, 2018.
* [125] Sohier Dane Karthik, Maggie. Aptos 2019 blindness detection, 2019.
* [126] Jakob Nikolas Kather, Niels Halama, and Alexander Marx. 100,000 histological images of human colorectal cancer and healthy tissue. _Zenodo10_, 5281:6, 2018.
* [127] A Emre Kavur, N Sinem Gezer, Mustafa Baris, Sinem Aslan, Pierre-Henri Conze, Vladimir Groza, Duc Duy Pham, Soumick Chatterjee, Philipp Ernst, Savas Ozkan, et al. Chaos challenge-combined (ct-mr) healthy abdominal organ segmentation. _Medical Image Analysis_, 69:101950, 2021.
* [128] A Emre Kavur, Naciye Sinem Gezer, Mustafa Baris, Yusuf Sahin, Savas Ozkan, Bora Baydar, Ulas Yuksel, Caglar Kilkcier, Sahin Olut, Gozde Bozdag Akar, et al. Comparison of semi-automatic and deep learning-based automatic methods for liver segmentation in living liver transplant donors. _Diagnostic and Interventional Radiology_, 26(1):11, 2020.
* [129] Jeremy Kawahara, Sara Daneshvar, Giuseppe Argenziano, and Ghassan Hamarneh. Seven-point checklist and skin lesion classification using multitask multimodal neural nets. _IEEE journal of biomedical and health informatics_, 23(2):538-546, 2018.
* [130] Andrew Kemp, Anna Zawacki, Chris Carr, George Shih, John Mongan, Julia Elliott, Kaiwen, ParasLakhani, and Phil Culliton. Siim-fisabio-rsna covid-19 detection, 2021.
* [131] Daniel Kermany, Kang Zhang, Michael Goldbaum, et al. Labeled optical coherence tomography (oct) and chest x-ray images for classification. _Mendeley data_, 2(2):651, 2018.
* [132] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4015-4026, 2023.
* [133] Kendall J Kiser, Sara Ahmed, Sonja Stieb, Abdallah SR Mohamed, Hesham Elhalawani, Peter YS Park, Nathan S Doyle, Brandon J Wang, Arko Barman, Zhao Li, et al. Plethora: Pleural effusion and thoracic cavity segmentations in diseased lungs for benchmarking chest ct processing pipelines. _Medical physics_, 47(11):5941-5952, 2020.
* [134] Alain Lalande, Zhihao Chen, Thibaut Pommier, Thomas Decourselle, Abdul Qayyum, Michel Salomon, Dominique Ginhac, Youssef Skandarani, Arnaud Boucher, Khawla Brahim, et al. Deep learning methods for automatic evaluation of delayed enhancement-mri. the results of the emidec challenge. _Medical Image Analysis_, 79:102428, 2022.
* [135] Bennett Landman, Zhoubing Xu, J Igelsias, Martin Styner, T Langerak, and Arno Klein. Miccai multi-atlas labeling beyond the cranial vault-workshop and challenge. In _Proc. MICCAI Multi-Atlas Labeling Beyond Cranial Vault--Workshop Challenge_, volume 5, page 12, 2015.
* [136] Jason J Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. A dataset of clinically generated visual questions and answers about radiology images. _Scientific data_, 5(1):1-10, 2018.
* [137] Hugo Laurencon, Lucile Saulnier, Leo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh. Obelics: An open web-scale filtered dataset of interleaved image-text documents, 2023.
* [138] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training a large language-and-vision assistant for biomedicine in one day. _Advances in Neural Information Processing Systems_, 36, 2024.

* [139] Fei Li, Diping Song, Han Chen, Jian Xiong, Xingyi Li, Hua Zhong, Guangxian Tang, Sujie Fan, Dennis SC Lam, Weihua Pan, et al. Development and clinical deployment of a smartphone-based visual field deep learning system for glaucoma detection. _NPJ digital medicine_, 3(1):123, 2020.
* [140] Lei Li, Veronika A Zimmer, Julia A Schnabel, and Xiahai Zhuang. Atrialgeneral: domain generalization for left atrial segmentation of multi-center lge mris. In _Medical Image Computing and Computer Assisted Intervention-MICCAI 2021: 24th International Conference, Strasbourg, France, September 27-October 1, 2021, Proceedings, Part VI 24_, pages 557-566. Springer, 2021.
* [141] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. _arXiv preprint arXiv:2403.18814_, 2024.
* [142] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important things for large multi-modal models, 2024.
* [143] Jie Lian, Jingyu Liu, Shu Zhang, Kai Gao, Xiaoqing Liu, Dingwen Zhang, and Yizhou Yu. A structure-aware relation network for thoracic diseases detection and segmentation. _IEEE Transactions on Medical Imaging_, 40(8):2042-2052, 2021.
* [144] Geert Litjens, Robert Toth, Wendy Van De Ven, Caroline Hoeks, Sjoerd Kerkstra, Bram Van Ginneken, Graham Vincent, Gwenael Guillard, Neil Birbeck, Jindang Zhang, et al. Evaluation of prostate segmentation algorithms for mri: the promise12 challenge. _Medical image analysis_, 18(2):359-373, 2014.
* [145] Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu. Slake: A semantically-labeled knowledge-enhanced dataset for medical visual question answering. In _2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)_, pages 1650-1654. IEEE, 2021.
* [146] Chi Liu, Xiaotong Han, Zhixi Li, Jason Ha, Guankai Peng, Wei Meng, and Mingguang He. A self-adaptive deep learning method for automated eye laterality detection based on color fundus photography. _PloS one_, 14(9):e0222025, 2019.
* [147] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024.
* [148] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.
* [149] Junling Liu, Ziming Wang, Qichen Ye, Dading Chong, Peilin Zhou, and Yining Hua. Qilin-med-vl: Towards chinese large vision-language model for general healthcare. _arXiv preprint arXiv:2310.17956_, 2023.
* [150] Pengbo Liu, Hu Han, Yuanqi Du, Heqin Zhu, Yinhao Li, Feng Gu, Honghu Xiao, Jun Li, Chunpeng Zhao, Li Xiao, et al. Deep learning to segment pelvic bones: large-scale ct datasets and baseline models. _International Journal of Computer Assisted Radiology and Surgery_, 16:749-756, 2021.
* [151] Quande Liu, Qi Dou, and Pheng-Ann Heng. Shape-aware meta-learning for generalizing prostate mri segmentation to unseen domains. In _Medical Image Computing and Computer Assisted Intervention-MICCAI 2020: 23rd International Conference, Lima, Peru, October 4-8, 2020, Proceedings, Part II 23_, pages 475-485. Springer, 2020.
* [152] Ruhan Liu, Xiangning Wang, Qiang Wu, Ling Dai, Xi Fang, Tao Yan, Jaemin Son, Shiqi Tang, Jiang Li, Zijian Gao, Adrian Galdran, J.M. Poorneshwaran, Hao Liu, Jie Wang, Yerui Chen, Prasanna Porwal, Gavin Siew Wei Tan, Xiaokang Yang, Chao Dai, Haitao Song, Mingang Chen, Huating Li, Weiping Jia, Dinggang Shen, Bin Sheng, and Ping Zhang. Deeprid: Diabetic retinopathy--grading and image quality estimation challenge. _Patterns_, page 100512, 2022.

* [153] Maximilian T L"offler, Anjany Sekuboyina, Alina Jacob, Anna-Lena Grau, Andreas Scharr, Malek El Husseini, Mareike Kallweit, Claus Zimmer, Thomas Baum, and Jan S Kirschke. A vertebral segmentation dataset with fracture grading. _Radiology: Artificial Intelligence_, 2(4):e190138, 2020.
* [154] DataCanvas Ltd. mmalaya. https://github.com/DataCanvasIO/MMAlaya, 2024.
* [155] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, et al. Deepseek-vl: towards real-world vision-language understanding. _arXiv preprint arXiv:2403.05525_, 2024.
* [156] Zhi Lu, Gustavo Carneiro, and Andrew P Bradley. An improved joint optimization of multiple level set functions for the segmentation of overlapping cervical cells. _IEEE transactions on image processing_, 24(4):1261-1272, 2015.
* [157] Zhi Lu, Gustavo Carneiro, Andrew P Bradley, Daniela Ushizima, Masoud S Nosrati, Andrea GC Bianchi, Claudia M Carneiro, and Ghassan Hamarneh. Evaluation of three algorithms for the segmentation of overlapping cervical cells. _IEEE journal of biomedical and health informatics_, 21(2):441-450, 2016.
* [158] Gongning Luo, Kuanquan Wang, Jun Liu, Shuo Li, Xinjie Liang, Xiangyu Li, Shaowei Gan, Wei Wang, Suyu Dong, Wenyi Wang, et al. Efficient automatic segmentation for multi-level pulmonary arteries: The parse challenge. _arXiv preprint arXiv:2304.03708_, 2023.
* [159] Xiangde Luo, Wenjun Liao, Jianghong Xiao, Jieneng Chen, Tao Song, Xiaofan Zhang, Kang Li, Dimitris N Metaxas, Guotai Wang, and Shaoting Zhang. Word: A large scale dataset, benchmark and clinical applicable study for abdominal organ segmentation from ct image. _arXiv preprint arXiv:2111.02403_, 2021.
* [160] Xinzhe Luo and Xiahai Zhuang. X-metric: An n-dimensional information-theoretic framework for groupwise registration and deep combined computing. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2022.
* [161] Yan Luo, Min Shi, Yu Tian, Tobias Elze, and Mengyu Wang. Harvard glaucoma detection and progression: A multimodal multitask dataset and generalization-reinforced semi-supervised learning, 2023.
* [162] Jun Ma, Yao Zhang, Song Gu, Xingle An, Zhihe Wang, Cheng Ge, Congcong Wang, Fan Zhang, Yu Wang, Yinan Xu, et al. Fast and low-gpu-memory abdomen ct organ segmentation: the flare challenge. _Medical Image Analysis_, 82:102616, 2022.
* [163] Jun Ma, Yao Zhang, Song Gu, Cheng Ge, Shihao Ma, Adamo Young, Cheng Zhu, Kangkang Meng, Xin Yang, Ziyan Huang, et al. Unleashing the strengths of unlabeled data in pan-cancer abdominal organ quantification: the flare22 challenge. _arXiv preprint arXiv:2308.05862_, 2023.
* [164] Jun Ma, Yao Zhang, Song Gu, Cheng Zhu, Cheng Ge, Yichi Zhang, Xingle An, Congcong Wang, Qiyuan Wang, Xin Liu, et al. Abdomenct-1k: Is abdominal organ segmentation a solved problem? _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 44(10):6695-6714, 2021.
* [165] Yuhui Ma, Huaying Hao, Jianyang Xie, Huazhu Fu, Jiong Zhang, Jianlong Yang, Zhen Wang, Jiang Liu, Yalin Zheng, and Yitian Zhao. Rose: a retinal oct-angiography vessel segmentation dataset and new model. _IEEE transactions on medical imaging_, 40(3):928-939, 2020.
* [166] K Scott Mader. Finding and measuring lungs in ct data.
* [167] Tahereh Mahmudi, Rahele Kafieh, Hossein Rabbani, Mohammadreza Akhlagi, et al. Comparison of macular octs in right and left eyes of normal people. In _Medical Imaging 2014: Biomedical Applications in Molecular, Structural, and Functional Imaging_, volume 9038, pages 472-477. SPIE, 2014.
* [168] Lena Maier-Hein, Annika Reinke, Michal Kozubek, Anne L Martel, Tal Arbel, Matthias Eisenmann, Allan Hanbury, Pierre Jannin, Henning M"uller, Sinan Onogur, et al. Bias: Transparent reporting of biomedical image analysis challenges. _Medical image analysis_, 66:101796, 2020.

* [169] Salman Maqbool. m2caiseg, 2020.
* [170] Carlos Martin-Isla, Victor M Campello, Cristian Izquierdo, Kaisar Kushibar, Carla Sendra-Balcells, Polyxeni Gkontra, Alireza Sojoudi, Mitchell J Fulton, Tewodros Weldebirhan Arega, Kumaradevan Punithakumar, et al. Deep learning segmentation of the right ventricle in cardiac mri: The m&ms challenge. _IEEE Journal of Biomedical and Health Informatics_, 27(7):3302-3313, 2023.
* [171] Mojtaba Masoudi, Hamid-Reza Pourreza, Mahdi Saadatmand-Tarzjan, Noushin Eftekhari, Fateme Shafiee Zargar, and Masoud Pezeshki Rad. A new dataset of computed-tomography angiography images for computer-aided detection of pulmonary embolism. _Scientific data_, 5(1):1-9, 2018.
* [172] C Matek, S Krappe, C M"unzenmayer, T Haferlach, and C Marr. An expert-annotated dataset of bone marrow cytology in hematologic malignancies. _The Cancer Imaging Archive_, 2021.
* [173] McNitt-Gray, Kim M., Zhao H., Schwartz B., Clunie L. H., Cohen D., PETRICK K., Fenimore N., Lu C., Z. Q. J., and A Buckler. Qiba volt group 1b round 2 no change size measurements (qiba-volct-1b) [data set]. _The Cancer Imaging Archive_, 2020.
* [174] Xueyan Mei, Zelong Liu, Philip M Robson, Brett Marinelli, Mingqian Huang, Amish Doshi, Adam Jacobi, Chendi Cao, Katherine E Link, Thomas Yang, et al. Radimagenet: an open radiologic deep learning research dataset for effective transfer learning. _Radiology: Artificial Intelligence_, 4(5):e210315, 2022.
* [175] Teresa Mendonca, Pedro M Ferreira, Jorge S Marques, Andre RS Marcal, and Jorge Rozeira. Ph 2-a dermoscopic image database for research and benchmarking. In _2013 35th annual international conference of the IEEE engineering in medicine and biology society (EMBC)_, pages 5437-5440. IEEE, 2013.
* [176] Masashi Misawa, Shin-ei Kudo, Yuichi Mori, Kinichi Hotta, Kazuo Ohtsuka, Takahisa Matsuda, Shoichi Saito, Toyoki Kudo, Toshiyuki Baba, Fumio Ishida, et al. Development of a computer-aided detection system for colonoscopy and a publicly accessible large colonoscopy video database (with video). _Gastrointestinal endoscopy_, 93(4):960-967, 2021.
* [177] AW Moawad, AA Ahmed, et al. Voxel-level segmentation of pathologically-proven adrenocortical carcinoma with ki-67 expression (adrenal-acc-ki67-seg)[data set]. _The Cancer Imaging Archive_, 2023.
* [178] AW Moawad, D Fuentes, A Morshid, AM Khalaf, MM Elmohr, A Abusaif, JD Hazle, AO Kaseb, M Hassan, A Mahvash, et al. Multimodality annotated hcc cases with and without advanced imaging segmentation. _The Cancer Imaging Archive (TCIA)_, 2021.
* [179] Anna Montoya, Hasnin, kaggle446, shirzad, Will Cukierski, and yffud. Ultrasound nerve segmentation, 2016.
* [180] Paul Mooney. Blood cell images.
* [181] Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Yash Dalmia, Jure Leskovec, Cyril Zakka, Eduardo Pontes Reis, and Pranav Rajpurkar. Med-flamingo: a multimodal medical few-shot learner. In _Machine Learning for Health (ML4H)_, pages 353-367. PMLR, 2023.
* [182] S. Mourya, S. Kant, P. Kumar, A. Gupta, and R Gupta. C_nmc_2019 dataset: All challenge dataset of isbi 2019. _The Cancer Imaging Archive_, 2019.
* [183] Yang Nan, Javier Del Ser, Zeyu Tang, Peng Tang, Xiaodan Xing, Yingying Fang, Francisco Herrera, Witold Pedrycz, Simon Walsh, and Guang Yang. Fuzzy attention neural network to tackle discontinuity in airway segmentation. _IEEE Transactions on Neural Networks and Learning Systems_, 2023.
* [184] Loris Nanni, Michelangelo Paci, Florentino Luciano Caetano dos Santos, Heli Skottman, Kati Juuti-Uusitalo, and Jari Hyttinen. Texture descriptors ensembles enable image-based classification of maturation of human stem cell-derived retinal pigmented epithelium. _PLoS One_, 11(2):e0149399, 2016.

* [185] Aman Neo. Diabetic retinopathy arranged, retina images with class labels for classification.
* [186] Uyen TV Nguyen, Alauddin Bhuiyan, Laurence AF Park, Ryo Kawasaki, Tien Y Wong, Jie Jin Wang, Paul Mitchell, and Kotagiri Ramamohanarao. An automated method for retinal arteriovenous nicking quantification from color fundus images. _IEEE Transactions on Biomedical Engineering_, 60(11):3194-3203, 2013.
* [187] National Institutes of Health et al. Nih clinical center provides one of the largest publicly available chest x-ray datasets to scientific community, 2017.
* [188] Jose Ignacio Orlando, Huazhu Fu, Joao Barbosa Breda, Karel Van Keer, Deepti R Bathula, Andres Diaz-Pinto, Ruogu Fang, Pheng-Ann Heng, Jeyoung Kim, JoonHo Lee, et al. Refuge challenge: A unified framework for evaluating automated methods for glaucoma assessment from fundus photographs. _Medical image analysis_, 59:101570, 2020.
* [189] Nikita V Orlov, Wayne W Chen, David Mark Eckley, Tomasz J Macura, Lior Shamir, Elaine S Jaffe, and Ilya G Goldberg. Automatic classification of lymphoma images with transform-based global features. _IEEE Transactions on Information Technology in Biomedicine_, 14(4):1003-1013, 2010.
* [190] Danielle F Pace, Adrian V Dalca, Tal Geva, Andrew J Powell, Mehdi H Moghari, and Polina Golland. Interactive whole-heart segmentation in congenital heart disease. In _Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18_, pages 80-88. Springer, 2015.
* [191] Andre GC Pacheco, Gustavo R Lima, Amanda S Salomao, Breno Krohling, Igor P Biral, Gabriel G de Angelo, Fabio CR Alves Jr, Jose GM Esgairo, Alana C Simora, Pedro BC Castro, et al. Pad-ufes-20: A skin lesion dataset composed of patient data and clinical images collected from smartphones. _Data in brief_, 32:106221, 2020.
* [192] O Paiva. Helping radiologists to help people in more than 100 countries. _Coronavirus Cases, CORONACASES. ORG_, 2020.
* [193] Pushpak Pati, Guillaume Jaume, Antonio Foncubierta, Florinda Feronce, Anna Maria Anniciello, Giosue Scognamiglio, Nadia Brancati, Maryse Fiche, Estelle Dubruc, Daniel Riccio, Maurizio Di Bonito, Giuseppe De Pietro, Gerardo Botti, Jean-Philippe Thiran, Maria Frucci, Orcun Goksel, and Maria Gabrani. Hierarchical graph representations for digital pathology. In _Medical Image Analysis (MedIA)_, volume 75, page 102264, 2021.
* [194] S Pati, R Verma, H Akbari, et al. Multi-institutional paired expert segmentations and radiomic features of the ivy gap dataset. _The Cancer Imaging Archive_, 10, 2020.
* [195] Lina Pedraza, Carlos Vargas, Fabian Narvaez, Oscar Duran, Emma Munoz, and Eduardo Romero. An open access thyroid ultrasound image database. In _10th International symposium on medical information processing and analysis_, volume 9287, pages 188-193. SPIE, 2015.
* [196] Joao Pedrosa, Guilherme Aresta, Carlos Ferreira, Marcio Rodrigues, Patricia Leitao, Andre Silva Carvalho, Joao Rebelo, Eduardo Negrao, Isabel Ramos, Antonio Cunha, et al. Lndb: a lung nodule database on computed tomography. _arXiv preprint arXiv:1911.08434_, 2019.
* [197] Antonio Pepe, Jianning Li, Malte Rolf-Pissarczyk, Christina Gsaxner, Xiaojun Chen, Gerhard A Holzapfel, and Jan Egger. Detection, segmentation, simulation and visualization of aortic dissections: a review. _Medical image analysis_, 65:101773, 2020.
* [198] Hady Ahmady Phoulady and Peter R. Mouton. A new cervical cytology dataset for nucleus detection and image classification (cervix93) and methods for cervical nucleus detection, 2018.
* [199] Gasper Podobnik, Primoz Strojan, Primoz Peterlin, Bulat Ibragimov, and Tomaz Vrtovec. Han-seg: The head and neck organ-at-risk ct and mr segmentation dataset. _Medical physics_, 50(3):1917-1927, 2023.

* [200] Konstantin Pogorelov, Kristin Ranheim Randel, Carsten Griwodz, Sigrun Losada Eskeland, Thomas de Lange, Dag Johansen, Concetto Spampinato, Duc-Tien Dang-Nguyen, Mathias Lux, Peter Thelin Schmidt, et al. Kvasir: A multi-class image dataset for computer aided gastrointestinal disease detection. In _Proceedings of the 8th ACM on Multimedia Systems Conference_, pages 164-169, 2017.
* [201] Long Pollehn. Bacteria detection with darkfield microscopy -- dataset for spirochaeta segmentation with image and manually annotated masks, 2020.
* [202] Prasanna Porwal, Samiksha Pachade, Ravi Kamble, Manesh Kokare, Girish Deshmukh, Vivek Sahasrabuddhe, and Fabrice Meriaudeau. Indian diabetic retinopathy image dataset (idrid): a database for diabetic retinopathy screening research. _Data_, 3(3):25, 2018.
* [203] Mohit Prabhushankar, Kiran Kokilepersaud, Yash-yee Logan, Stephanie Trejo Corona, Ghassan AlRegib, and Charles Wykoff. Olives dataset: Ophthalmic labels for investigating visual eye semantics. _Advances in Neural Information Processing Systems_, 35:9201-9216, 2022.
* [204] Bo Qian, Hao Chen, Xiangning Wang, Zhouyu Guan, Tingyao Li, Yixiao Jin, Yilan Wu, Yang Wen, Haoxuan Che, Gitaek Kwon, et al. Drac 2022: A public benchmark for diabetic retinopathy analysis on ultra-wide optical coherence tomography angiography images. _Patterns_, 2024.
* [205] Chongyu Qu, Tiezheng Zhang, Hualin Qiao, Yucheng Tang, Alan L Yuille, Zongwei Zhou, et al. Abdomenatlas-8k: Annotating 8,000 ct volumes for multi-organ segmentation in three weeks. _Advances in Neural Information Processing Systems_, 36, 2024.
* [206] Felix Quinton, Romain Popoff, Benoit Presles, Sarah Leclerc, Fabrice Meriaudeau, Guillaume Nodari, Olivier Lopez, Julie Pellegrinelli, Olivier Chevallier, Dominique Ginhac, et al. A tumour and liver automatic segmentation (atlas) dataset on contrast-enhanced magnetic resonance imaging for hepatocellular carcinoma. _Data_, 8(5):79, 2023.
* [207] Hossein Rabbani, Michael J Allingham, Priyatham S Mettu, Scott W Cousins, and Sina Farsiu. Fully automatic segmentation of fluorescein leakage in subjects with diabetic macular edema. _Investigative ophthalmology & visual science_, 56(3):1482-1492, 2015.
* [208] Lukas Radl, Yuan Jin, Antonio Pepe, Jianning Li, Christina Gsaxner, Fen-hua Zhao, and Jan Egger. Avt: Multicenter aortic vessel tree cta dataset collection with ground truth segmentation masks. _Data in brief_, 40:107801, 2022.
* covid-19, viral pneumonia, normal.
* [210] Patrik F Raudaschl, Paolo Zaffino, Gregory C Sharp, Maria Francesca Spadea, Antong Chen, Benoit M Dawant, Thomas Albrecht, Tobias Gass, Christoph Langguth, Marcel L'uthi, et al. Evaluation of segmentation methods on head and neck ct: auto-segmentation challenge 2015. _Medical physics_, 44(5):2020-2036, 2017.
* [211] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. _arXiv preprint arXiv:2403.05530_, 2024.
* [212] Blaine Rister, Darvin Yi, Kaushik Shivakumar, Tomomi Nobashi, and Daniel L Rubin. Ct-org, a new dataset for multiple organ segmentation in computed tomography. _Scientific Data_, 7(1):381, 2020.
* [213] Veronica Rotemberg, Nicholas Kurtansky, Brigid Betz-Stablein, Liam Caffery, Emmanouil Chousakos, Noel Codella, Marc Combalia, Stephen Dusza, Pascale Guitera, David Gutman, et al. A patient-centric dataset of images and metadata for identifying melanomas using clinical context. _Scientific data_, 8(1):34, 2021.

* [214] Holger R Roth, Ziyue Xu, Carlos Tor-Diez, Ramon Sanchez Jacob, Jonathan Zember, Jose Molto, Wenqi Li, Sheng Xu, Baris Turkbey, Evrim Turkbey, et al. Rapid artificial intelligence solutions in a pandemic--the covid-19-20 lung ct lesion segmentation challenge. _Medical image analysis_, 82:102605, 2022.
* [215] Darshan D Ruikar, KC Santosh, Ravindra S Hegadi, Lakhan Rupnar, and Vivek A Choudhary. 5k+ ct images on fractured limbs: a dataset for medical imaging research. _Journal of Medical Systems_, 45(4):51, 2021.
* [216] Bakas S, Akbari H, Sotiras A, Bilello M, Rozycki M, Kirby J, Freymann J, Farahani K, and Davatzikos C. Segmentation labels for the pre-operative scans of the tcga-gbm collection [data set]. _The Cancer Imaging Archive_, 2017.
* [217] Anindo Saha, Joeran S Bosma, Jasper J Twilt, Bram van Ginneken, Anders Bjartell, Anwar R Padhani, David Bonekamp, Geert Villeirs, Georg Salomon, Gianluca Giannarini, Jayashree Kalpathy-Cramer, Jelle Barentsz, Klaus H Maier-Hein, Mirabela Rusu, Olivier Rouviere, Roderick van den Bergh, Valeria Panebianco, Veeru Kasivisvanathan, Nancy A Obuchowski, Derya Yakar, Mattijs Elschot, Jeroen Veltman, Jurgen J Futerer, Maarten de Rooij, Henkjan Huisman, Anindo Saha, Joeran S. Bosma, Jasper J. Twilt, Bram van Ginneken, Constant R. Nordman, Ivan Slootweg, Christian Roest, Stefan J. Fransen, Mohammed R.S. Sunoort, Tone F. Bathen, Dennis Rouw, Jos Immerzelen, Jeroen Geerdink, Chris van Run, Miriam Groeneveld, James Meakin, Ahmet Karagoz, Alexandre Bone, Alexandre Routier, Arnaud Marcoux, Clement Abi-Nader, Cynthia Xinran Li, Dagan Feng, Deniz Alis, Ercan Karaarslan, Euijoon Ahn, Francois Nicolas, Geoffrey A. Sonn, Indrani Bhattacharya, Jinman Kim, Jun Shi, Hassan Jahanandish, Hong An, Hongyu Kan, Ilkay Oksuz, Liang Qiao, Marc-Michel Rohe, Mert Yergin, Mohamed Khadra, Mustafa E. Seker, Mustafa S. Kartal, Noelie Debs, Richard E. Fan, Sara Saunders, Simon J.C. Soerensen, Stefania Moroianu, Sulaiman Vesal, Yuan Yuan, Afsoun Malakoti-Fard, Agne Macuinien, Akira Kawashima, Ana M.M. de M.G. de Sousa Machadov, Ana Sofia L. Moreira, Andrea Ponsiglione, Annelies Rappaport, Arnaldo Stanzione, Arturas Ciusavosaya, Baris Turkbey, Bart de Keyzer, Bodil G. Pedersen, Bram Eijlers, Christine Chen, Ciabattori Riccardo, Deniz Alis, Ewout F.W. Courrech Staal, Fredrik Jaderling, Fredrik Langkilde, Giacomo Aringhieri, Giorgio Brembilla, Hannah Son, Hans Vanderelij, Henricus P.J. Raat, Ingrida Pikuniene, Iva Macova, Ivo Schoots, Iztok Caglic, Jeries P. Zawaideh, Jonas Wallstrom, Leonardo K. Bittencourt, Misbah Khurram, Moon H. Choi, Naoki Takahashi, Nelly Tan, Paolo N. Franco, Patricia A. Gutierrez, Per Erik Thimansson, Pieter Hanus, Philippe Puech, Philipp R. Rau, Pieter de Visschere, Ramette Guillaume, Renato Cuocolo, Ricardo O. Falcao, Rogier S.A. van Stiphout, Rossano Girometti, Ruta Briediene, Ruta Grigiene, Samuel Gitau, Samuel Withey, Sangeet Ghai, Tobias Penzkofer, Tristan Barrett, Varaha S. Tammisetti, Vibeke B. Logager, Vladimir Cerny, Wulphert Venderink, Yan M. Law, Young J. Lee, Anders Bjartell, Anwar R. Padhani, David Bonekamp, Geert Villeirs, Georg Salomon, Gianluca Giannarini, Jayashree Kalpathy-Cramer, Jelle Barentsz, Klaus H. Maier-Hein, Mirabela Rusu, Nancy A. Obuchowski, Olivier Rouviere, Roderick van den Bergh, Valeria Panebianco, Veeru Kasivisvanathan, Derya Yakar, Mattijs Elschot, Jeroen Veltman, Jurgen J. Futerer, Maarten de Rooij, and Henkjan Huisman. Artificial intelligence and radiologists in prostate cancer detection on mri (pi-cai): an international, paired, non-inferiority, confirmatory study. _The Lancet Oncology_, 2024.
* [218] Fabio Scarpa, Enrico Grisan, and Alfredo Ruggeri. Automatic recognition of corneal nerve structures in images from confocal microscopy. _Investigative ophthalmology & visual science_, 49(11):4801-4807, 2008.
* [219] Fabio Scarpa, Xiaodong Zheng, Yuichi Ohashi, and Alfredo Ruggeri. Automatic evaluation of corneal nerve tortuosity in images from in vivo confocal microscopy. _Investigative ophthalmology & visual science_, 52(9):6404-6408, 2011.
* [220] D Schindele, A Meyer, DF von Reibnitz, V Kiesswetter, M Schostak, M Rak, and C Hansen. High resolution prostate segmentations for the prostatex-challenge [dataset]. _The Cancer Imaging Archive_, page 131, 2020.
* [221] Prah M Schmainda KM. Data from brain-tumor-progression. _The Cancer Imaging Archive_, 2018.

* [222] Lalithkumar Seenivasan, Mobarakol Islam, Adithya K Krishna, and Hongliang Ren. Surgical-vqa: Visual question answering in surgical scenes using transformer. In _International Conference on Medical Image Computing and Computer-Assisted Intervention_, pages 33-43. Springer, 2022.
* [223] Anjany Sekuboyina, Malek E Husseini, Amirhossein Bayat, Maximilian L"offler, Hans Liebl, Hongwei Li, Giles Tetteh, Jan Kukacka, Christian Payer, Darko Stern, et al. Verse: A vertebrae labelling and segmentation benchmark for multi-detector ct images. _Medical image analysis_, 73:102166, 2021.
* [224] Arnaud Arindra Adiyoso Setio, Alberto Traverso, Thomas De Bel, Moira SN Berens, Cas Van Den Bogaard, Piergiorgio Cerello, Hao Chen, Qi Dou, Maria Evelina Fantacci, Bram Geurts, et al. Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: the luna16 challenge. _Medical image analysis_, 42:1-13, 2017.
* [225] Ugur Sevik, Cemal K"ose, Tolga Berber, and Hidayet Erd"ol. Identification of suitable fundus images using automated quality assessment methods. _Journal of biomedical optics_, 19(4):046006-046006, 2014.
* [226] Junji Shiraishi, Shigehiko Katsuragawa, Junpei Ikezoe, Tsuneo Matsumoto, Takeshi Kobayashi, Ken-ichi Komatsu, Mitate Matsui, Hiroshi Fujita, Yoshie Kodera, and Kunio Doi. Development of a digital image database for chest radiographs with and without a lung nodule: receiver operating characteristic analysis of radiologists' detection of pulmonary nodules. _American journal of roentgenology_, 174(1):71-74, 2000.
* [227] Amber L Simpson, Michela Antonelli, Spyridon Bakas, Michel Bilello, Keyvan Farahani, Bram Van Ginneken, Annette Kopp-Schneider, Bennett A Landman, Geert Litjens, Bjoern Menze, et al. A large annotated medical image dataset for the development and evaluation of segmentation algorithms. _arXiv preprint arXiv:1902.09063_, 2019.
* [228] Amber L Simpson, Jacob Peoples, John M Creasy, Gabor Fichtinger, Natalie Gangai, Krishna N Keshavamurthy, Andras Lasso, Jinru Shia, Michael I D'Angelica, and Richard KG Do. Preoperative ct and survival data for patients undergoing resection of colorectal liver metastases. _Scientific Data_, 11(1):172, 2024.
* [229] Korsuk Sirinukunwattana, Josien PW Pluim, Hao Chen, Xiaojuan Qi, Pheng-Ann Heng, Yun Bo Guo, Li Yang Wang, Bogdan J Matuszewski, Elia Bruni, Urko Sanchez, et al. Gland segmentation in colon histology images: The glas challenge contest. _Medical image analysis_, 35:489-502, 2017.
* [230] K Smith and T Nolan. Osteosarcoma data from ut southwestern/ut dallas for viable and necrotic tumor assessment (osteosarcoma-tumor-assessment). 2019.
* [231] Ecem Sogancioglu, Bram van Ginneken, Finn Behrendt, Marcel Bengs, Alexander Schlaefer, Miron Radu, Di Xu, Ke Sheng, Fabien Scalzo, Eric Marcus, et al. Nodule detection and generation on chest x-rays: Node21 challenge. _arXiv preprint arXiv:2401.02192_, 2024.
* [232] Fabio A Spanhol, Luiz S Oliveira, Caroline Petitjean, and Laurent Heutte. A dataset for breast cancer histopathological image classification. _Ieee transactions on biomedical engineering_, 63(7):1455-1462, 2015.
* [233] Joes Staal, Michael D Abramoff, Meindert Niemeijer, Max A Viergever, and Bram Van Ginneken. Ridge-based vessel segmentation in color images of the retina. _IEEE transactions on medical imaging_, 23(4):501-509, 2004.
* [234] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt: One model to instruction-follow them all, 2023.
* [235] John Suckling. The mammographic images analysis society digital mammogram database. In _Exerpta Medica. International Congress Series, 1994_, volume 1069, pages 375-378, 1994.
* [236] R Summers. Nih chest x-ray dataset of 14 common thorax disease categories. _NIH Clinical Center: Bethesda, MD, USA_, 2019.

* [237] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, et al. Generative multimodal models are in-context learners. _arXiv preprint arXiv:2312.13286_, 2023.
* [238] Yuxuan Sun, Hao Wu, Chenglu Zhu, Sunyi Zheng, Qizi Chen, Kai Zhang, Yunlong Zhang, Xiaoxiao Lan, Mengyue Zheng, Jingxiong Li, et al. Pathmmu: A massive multimodal expert-level benchmark for understanding and reasoning in pathology. _arXiv preprint arXiv:2401.16355_, 2024.
* [239] Siham Tabik, Anabel Gomez-Rios, Jose Luis Martin-Rodriguez, Ivan Sevillano-Garcia, Manuel Rey-Area, David Charte, Emilio Guirado, Juan-Luis Suarez, Julian Luengo, MA Valero-Gonzalez, et al. Covidgr dataset and covid-sdnet methodology for predicting covid-19 based on chest x-ray images. _IEEE journal of biomedical and health informatics_, 24(12):3595-3605, 2020.
* [240] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.
* [241] Catalina Tobon-Gomez, Arjan J Geers, Jochen Peters, J\({}^{\text{i}}\)urgen Weese, Karen Pinto, Rashed Karim, Mohammed Ammar, Abdelaziz Daoudi, Jan Margeta, Zulma Sandoval, et al. Benchmark for algorithms segmenting the left atrium from 3d ct and mri datasets. _IEEE transactions on medical imaging_, 34(7):1460-1473, 2015.
* [242] Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. _Scientific data_, 5(1):1-9, 2018.
* [243] Andru P Twinanda, Sherif Shehata, Didier Mutter, Jacques Marescaux, Michel De Mathelin, and Nicolas Padoy. Endonet: a deep architecture for recognition tasks on laparoscopic videos. _IEEE transactions on medical imaging_, 36(1):86-97, 2016.
* [244] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 7167-7176, 2017.
* [245] Mart van Rijthoven, Zaneta Swiderska-Chadaj, Katja Seeliger, Jeroen van der Laak, and Francesco Ciompi. You only look on lymphocytes once. 2018.
* [246] Chuanbo Wang, Amirreza Mahbod, Isabella Ellinger, Adrian Galdran, Sandeep Gopalakrishnan, Jeffrey Niezgoda, and Zeyun Yu. Fuseg: The foot ulcer segmentation challenge. _Information_, 15(3):140, 2024.
* [247] Linda Wang, Zhong Qiu Lin, and Alexander Wong. Covid-net: A tailored deep convolutional neural network design for detection of covid-19 cases from chest x-ray images. _Scientific reports_, 10(1):19549, 2020.
* [248] Shuo Wang, Chen Qin, Chengyan Wang, Kang Wang, Haoran Wang, Chen Chen, Cheng Ouyang, Xutong Kuang, Chengliang Dai, Yuanhan Mo, et al. The extreme cardiac mri analysis challenge under respiratory motion (cmrxmotion). _arXiv preprint arXiv:2210.06385_, 2022.
* [249] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. Cogvlm: Visual expert for pretrained language models, 2023.
* [250] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M Summers. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2097-2106, 2017.
* [251] Jakob Wasserthal, Hanns-Christian Breit, Manfred T Meyer, Maurice Pradella, Daniel Hinck, Alexander W Sauter, Tobias Heye, Daniel T Boll, Joshy Cyriac, Shan Yang, et al. Totalsegmentator: Robust segmentation of 104 anatomic structures in ct images. _Radiology: Artificial Intelligence_, 5(5), 2023.

- single cell classification, 2021.
* [253] Chris Wright and Pauline Reeves. Radbench: benchmarking image interpretation skills. _Radiography_, 22(2):e131-e136, 2016.
* [254] Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Towards generalist foundation model for radiology by leveraging web-scale 2d&3d medical data, 2023.
* [255] Yiming Xiao, Hassan Rivaz, Matthieu Chabanas, Maryse Fortin, Ines Machado, Yangming Ou, Mattias P Heinrich, Julia A Schnabel, Xia Zhong, Andreas Maier, et al. Evaluation of mri to ultrasound registration methods for brain shift correction: the curious2018 challenge. _IEEE transactions on medical imaging_, 39(3):777-786, 2019.
* [256] Feng Xu, Chuang Zhu, Wenqi Tang, Ying Wang, Yu Zhang, Jie Li, Hongchuan Jiang, Zhongyue Shi, Jun Liu, and Mulan Jin. Predicting axillary lymph node metastasis in early breast cancer using deep learning on primary tumor biopsy slides. _Frontiers in oncology_, 11:759007, 2021.
* [257] Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, and Gao Huang. LLAVA-UHD: an lmm perceiving any aspect ratio and high-resolution images. _arXiv preprint arXiv:2403.11703_, 2024.
* [258] Jin Ye, Junlong Cheng, Jianpin Chen, Zhongying Deng, Tianbin Li, Haoyu Wang, Yanzhou Su, Ziyan Huang, Jilong Chen, Lei Jiang, et al. Sa-med2d-20m dataset: Segment anything in 2d medical imaging with 20 million masks. _arXiv preprint arXiv:2311.11969_, 2023.
* [259] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. _arXiv preprint arXiv:2311.04257_, 2023.
* [260] Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, et al. Mmt-bench: A comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi. _arXiv preprint arXiv:2404.16006_, 2024.
* [261] Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Da Chen, Xiaoman Lu, Ganqu Cui, Taiwan He, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. Rlaif-v: Aligning mlms through open-source ai feedback for super gpt-4v trustworthiness. _arXiv preprint arXiv:2405.17220_, 2024.
* [262] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. _arXiv preprint arXiv:2311.16502_, 2023.
* [263] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9556-9567, 2024.
* [264] Anna Zawacki, Carol Wu, George Shih, Julia Elliott, Mikhail Fomittev, Mohannad Hussain, ParasLakhani, Phil Culliton, and Shunxing Bao. Siim-acr pneumothorax segmentation, 2019.
* [265] Minghui Zhang, Yangqian Wu, Hanxiao Zhang, Yulei Qin, Hao Zheng, Wen Tang, Corey Arnold, Chenhao Pei, Pengxin Yu, Yang Nan, et al. Multi-site, multi-domain airway tree modeling. _Medical Image Analysis_, 90:102957, 2023.
* [266] Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition. _arXiv preprint arXiv:2309.15112_, 2023.

* [267] Shaoting Zhang and Dimitris Metaxas. On the challenges and perspectives of foundation models for medical image analysis. _Medical Image Analysis_, page 102996, 2023.
* [268] Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-vqa: Visual instruction tuning for medical visual question answering, 2023.
* [269] Zhongchen Zhao, Huai Chen, and Lisheng Wang. A coarse-to-fine framework for the 2021 kidney and kidney tumor segmentation challenge. In _International Challenge on Kidney and Kidney Tumor Segmentation_, pages 53-58. Springer, 2021.

**GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI**

**Supplementary Materials**

###### Contents

* Supplementary Materials
* A Related work
* A.1 Large Vision-Language Model(LVLMs)
* A.2 Benchmarks
* B Dataset Details
* C Details of Well-categorized Data Structure
* C.1 Data Statistics
* C.2 Lexical Tree
* D Evaluation
* D.1 Evaluation Metric for Single-choice Questions
* D.2 Evaluation Metric for Multiple-choice Questions
* D.3 Evaluated Models
* E Results
* E.1 Quantitative Results
* E.2 Case Study

## Appendix A Related work

### Large Vision-Language Model(LVLMs)

In contrast to traditional deep learning models, Large Vision-Language Models (LVLMs) offer a broader spectrum of possibilities for AI-assisted healthcare. Their user-friendly and intuitive interaction mechanisms make them one of the most promising paradigms for future AI applications. Among the multitude of LVLMs, prominent proprietary models such as GPT-4o [5], Claude3-opus [13], and Qwen-max [18] exemplify the pinnacle of contemporary general-purpose large models. Additionally, numerous open-source general-purpose models have emerged, including the InternVL series [47, 46], LLVA series [147, 148, 43], DeepSeek series [155], CogVLM series [249], InstructBLIP series [56], Idefics series [137], XComoser series [43, 266, 62, 63], Yi-VL series [7], Xtuner series [54], and MiniCPM series [103, 257]. These open-source models are rapidly evolving due to their accessibility and collaborative development.

To address specialized medical tasks, researchers have trained and fine-tuned these large models using domain-specific medical data, resulting in specialized large models. Noteworthy examples include LLaVA-Med [138] derived from the LLAVA series, and MedDr [95] based on the InternLM framework. The advent of these specialized medical models has laid a solid foundation for the application of LVLMs in the healthcare sector, highlighting their transformative potential and accelerating their development within the medical domain.

### Benchmarks

In the swiftly emerging and burgeoning domain of LVLMs, the significance of rigorous evaluation cannot be overstated. Benchmarking serves as a crucial metric for guiding model enhancement, identifying deficiencies, and steering the trajectory of model development. Within the medical domain, benchmarks are typically categorized into specialized and general-purpose benchmarks.

Specialized benchmarks are often concentrated on a particular modality or medical discipline. For instance, VQA-RAD [136], SLAKE [145], and RadBench [253] focus on radiology, while PathVQA [96] and PathMMU [238] are dedicated to pathology. These benchmarks provide a wealth of evaluation data for specific modalities or disciplines, enabling comprehensive assessment of capabilities within targeted fields. However, their limited generalizability constrains their broader applicability.

In addition to these specialized benchmarks, there exist general-purpose medical benchmarks that span multiple medical domains. Prominent examples include MMMU [263], OmnimedVQA [106], and MMT-Bench [260]. These comprehensive benchmarks facilitate a more holistic evaluation of a model's overall competence in the medical field. Nonetheless, these general-purpose benchmarks often exhibit shortcomings in various aspects such as the volume of tasks, number of modalities, data distribution, and granularity of data. Addressing these limitations presents a significant challenge that necessitates prompt resolution.

The development and refinement of benchmarks are indispensable for the progress of LVLMs in healthcare. By elucidating the capabilities and limitations of specialized and general-purpose benchmarks, it becomes evident that while specialized benchmarks excel in evaluating domain-specific performance, their lack of generalizability is a notable drawback. Conversely, general-purpose benchmarks offer a broader assessment across multiple medical fields but often fall short in task diversity, modality coverage, and data granularity. Therefore, there is an urgent need for more comprehensive and robust benchmarks to bridge these gaps and better support the advancement of LVLMs in healthcare.

## Appendix B Dataset Details

In this section, we provide the detailed datasets used in GMAI-MMBench, including the name of the dataset or challenge, the number of sub-datasets in it, the modality, the dimension of data, the task type, and the number of cases. As shown in Table 4, GMAI-MMBench is constructed from 284 datasets across 38 medical image modalities. These datasets are derived from the public (268) and several hospitals (16) that have agreed to share their ethically approved data.

\begin{table}
\begin{tabular}{l|c|c|c|c|c} \hline Challenge / Dataset & N & Modality & Dim & Task & Count \\ \hline
5K+ CT Images on Fractured & 1 & CT & 2D & Cls & 60 \\ Limbs [215] & & & & & \\ AAPM RT-MAC 2019 [40] & 1 & T2 weighted MRI & 3D & Seg & 68 \\ Abdomentals 1.0 [205] & 1 & CT & 3D & Seg & 52 \\ Abdomement-T-IK [164] & 1 & CT & 3D & Seg & 28 \\ ACDC 2017 [30] & 1 & MRI & 3D & Seg & 10 \\ ACRIMA [60] & 1 & Fundus Photography & 2D & Cls & 1 \\ ADAM 2020 [68] & 1 & Fundus Photography & 2D & Cls & 1 \\ Adrenal-ACC-Ki67-Seg [177] & 1 & CT & 3D & Seg & 60 \\ AGE 2019 [74] & 1 & OCT & 2D & MCls & 20 \\ AIDA-E 2016 & 3 & Endoscopy & 2D & Cls & 187 \\ AIIB23 [183] & 1 & CT & 3D & Seg & 34 \\ AIROGS [58] & 1 & Fundus Photography & 2D & Cls & 57 \\ AMOS 2022 [116] & 1 & MRI, CT & 3D & Seg & 148 \\ APTOS 2019 [125] & 1 & Fundus Photography & 2D & Cls & 14 \\ \hline \end{tabular}
\end{table}
Table 4: Detailed datasets information in GMAI-MMBench. As one challenge/dataset may contain several sub-tasks or sub-challenges in the medical area, we count them in the “N” (second column). In the dimension (Dim) column, 2D and 3D denote the dimensions of the original data, respectively. In the task type (Task) column, Cls, MCLs, Seg, and Det indicate classification, multi-label classification, segmentation, and detection, respectively. The count represents the number of cases used in GMAI-MMBench.

[MISSING_PAGE_FAIL:33]

[MISSING_PAGE_FAIL:35]

PI-CAI [217] & 1 & T2 weighted MRI, MRI & 3D & Seg & 28 \\ PitVis & 1 & Endoscopy & 2D & Cls & 360 \\ PleThora [133] & 1 & CT & 3D & Seg & 120 \\ PROMISE 2009 [31] & 1 & T2 weighted MRI & 3D & Seg & 8 \\ PROMISE 2012 [144] & 1 & MRI & 3D & Seg & 8 \\ Prostate-Anatomical-Edge- & 1 & CT & 3D & Seg & 18 \\ Cases [123] & & T2 weighted MRI & 3D & Seg & 6 \\ PROSTATEx-Seg-HiRes [220] & 1 & X-ray & 2D & Cls & 12 \\ Abnormalities [109] & & X-ray & 2D & Cls & 13 \\ Pulmonary Chest X-Ray & 1 & CT & 3D & Seg & 14 \\ Abnormalities [244] & & CT & 3D & Seg & 60 \\ Pulmonary Embolism in CT & 1 & CT & 3D & Seg & 6 \\ images [171] & 1 & CT & 3D & Seg & 60 \\ QIBA-VolCT-1B [173] & 1 & CT & 3D & Seg & 6 \\ QIN-LungCT-Seg [113] & 1 & CT & 3D & Seg & 80 \\ QIN-PROSTATE-Repeatability [70] & 1 & T2 weighted MRI, DCE MRI, & 3D & Seg & 80 \\ \multicolumn{5}{c}{ADC MRI} & \multicolumn{5}{c}{ADC MRI} & \multicolumn{5}{c}{2D} & \multicolumn{5}{c}{Cls} & \multicolumn{5}{c}{4608} \\ RadImageNet [174] & 1 & UltraSound, MRI, CT & 2D & Cls & 4608 \\ RAVIR [93] & 1 & Infrared Reflectance (IR) & 2D & Seg & 92 \\ \multicolumn{5}{c}{imaging} & \multicolumn{5}{c}{2D} & \multicolumn{5}{c}{Seg} & \multicolumn{5}{c}{20} \\ REFUGE2 [139, 188] & 1 & Fundus Photography & 2D & Seg & 20 \\ Retina Fundus Image & 1 & OCT & 2D & Cls & 135 \\ Registration [100] & & & & & & \\ Retinal OCT Images [131] & 1 & OCT & 2D & Cls & 14 \\ RHUH-GBM [42] & 1 & T1ce MRI, T2 MRI, ADC MRI & 3D & Seg & 10 \\ RibFrac2020 [118] & 1 & CT & 3D & Seg & 60 \\ RIDER-LungCT-Seg [6] & 1 & CT & 3D & Seg & 26 \\ RIM-ONE [75] & 1 & Fundus Photography & 2D & Seg & 60 \\ RITE [104] & 1 & Fundus Photography & 2D & Seg & 16 \\ Robotic Instrument & 1 & Endoscopy & 2D & Seg & 74 \\ Segmentation [11] & & Fundus Photography & 2D & Seg & 30 \\ ROSE [165] & 1 & CT & 2D & MCIs & 289 \\ RSNA Intracranial Hemorrh & 1 & & & & & \\ Detection [71] & & & & & & \\ RSNA Pediatric Bone Age & 1 & X-ray & 2D & Cls & 1 \\ Challenge [89] & 1 & X-ray & 2D & Cls & 265 \\ RUS-CHN SAML [151] & 1 & T2 weighted MRI & 3D & Seg & 6 \\ SARAS-MESAD [25, 24] & 1 & Endoscopy & 2D & Det & 635 \\ SEG.A. 2023 [119, 197, 208, 168] & 1 & CT & 3D & Seg & 2 \\ SegPC-2021 [15, 32] & 1 & Histopathology & 2D & Seg & 30 \\ SegTHOR [98] & 1 & CT & 3D & Seg & 48 \\ SIIM-ACR Pneumothorax & 1 & X-ray & 2D & Seg & 16 \\ Segmentation [264] & & & & & & \\ SIIM-ACR Pneumothorax & 1 & X-ray & 2D & Cls & 58 \\ Segmentation [264] & & & & & & \\ SIIM-FISABIO-RSNA COVID-19 & 1 & X-ray & 2D & Cls & 90 \\ Detection [130] & & & & & & \\ SinaFarsiu-008-Chiu BOE 2012 [50] & 1 & OCT & 2D & Seg & 46 \\ SinaFarsiu-009-Chiu BOE 2013 [49] & 1 & OCT & 2D & Seg & 8 \\ SinaFarsiu-010-Rabbani IOVS & 1 & OCT & 2D & Seg & 48 \\ SinaFarsiu-013-Estrada PAMI & 1 & OCT & 2D & Cls & 30 \\

\begin{tabular}{l|c|c|c|c|c} \hline \multicolumn{6}{l|}{Surgical Instrument Multi-Domain} & 1 & Endoscopy & 2D & Seg & 2 \\ \multicolumn{6}{l|}{Segmentation Challenge} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & Endoscopy & 2D & Seg & 58 \\ \multicolumn{6}{l|}{Syn-ISS} \\ TCB Challenge [92] & 1 & Texture Characterization of Bone & 2D & Cls & 60 \\ \multicolumn{6}{l|}{Radiograph} \\ TotalSegmentator [251] & 1 & CT & 3D & Seg & 1218 \\ UCSF-PDGM [37] & 1 & ASL MRI, DWI MRI, T1 & 3D & Seg & 22 \\ \multicolumn{6}{l|}{weighted MRI, SWI MRI, DTI} \\ \multicolumn{6}{l|}{MRI, MRI, FLAIR MRI} \\ Ultrasound Nerve Segmentation [179] & 1 & UltraSound & 2D & Seg & 60 \\ UW-Madison GI Tract Image & 1 & MRI & 2D & Seg & 150 \\ Segmentation [91] & & & & & \\ VerSe 2019 [223, 153] & 1 & CT & 3D & Seg & 94 \\ VerSe 2020 [223, 153] & 1 & CT & 3D & Seg & 14 \\ VinBigData Chest X-ray & 1 & X-ray & 2D & Det & 107 \\ Abnormalities Detection [66] & & & & & \\ WORD [159] & 1 & CT & 3D & Seg & 72 \\ Yangxi Dataset [146] & 1 & Fundus Photography & 2D & Cls & 60 \\ \hline In-House Dataset & 1 & Fundus Photography & 2D & Cls & 23 \\ In-House Dataset & 1 & CT & 3D & Seg & 40 \\ In-House Dataset & 1 & CT & 3D & Seg & 12 \\ In-House Dataset & 1 & CT & 3D & Seg & 80 \\ In-House Dataset & 1 & CTA & 3D & Seg & 10 \\ In-House Dataset & 1 & CT & 3D & Seg & 18 \\ In-House Dataset & 1 & CT & 3D & Seg & 34 \\ In-House Dataset & 1 & CT & 3D & Seg & 60 \\ In-House Dataset & 1 & CT & 3D & Seg & 76 \\ In-House Dataset & 1 & CT & 3D & Seg & 60 \\ In-House Dataset & 1 & CT & 3D & Seg & 18 \\ In-House Dataset & 1 & CT & 3D & Seg & 96 \\ In-House Dataset & 1 & CT & 3D & Seg & 150 \\ In-House Dataset & 1 & CT & 3D & Seg & 40 \\ In-House Dataset & 1 & CT & 3D & Seg & 14 \\ In-House Dataset & 1 & CT & 3D & Seg & 82 \\ \hline \end{tabular}

[MISSING_PAGE_FAIL:38]

### Lexical Tree

To make the GMAI-MMBench more intuitive and user-friendly, we have systematized our labels and structured the entire dataset into a lexical tree, which is presented in HTML format as shown in Figure 7. Users can freely select the test contents based on this lexical tree. We believe that this customizable benchmark will effectively guide the improvement of models in specific areas. For instance, as mentioned in the main text, most models perform poorly at the bounding box level perception. Users can then update their models and test the accuracy at the bounding box level using this lexical tree, thereby achieving targeted improvements in model performance.

\begin{table}
\begin{tabular}{l|c|c c c|c c c} \hline Full Name & Abbreviation & \multicolumn{3}{c|}{Single Choice} & \multicolumn{3}{c}{Multiple Choice} \\  & & Modalities & Labels & Cases & Modalities & Labels & Cases \\ \hline Cardiovascular Surgery & CS & 9 & 9 & 270 & 1 & 1 & 424 \\ Dermatology & D & 1 & 30 & 894 & - & - & - \\ Endocrinology & E & 3 & 7 & 210 & - & - & - \\ Gastroenterology and Hepatology & GH & 7 & 60 & 1774 & - & - & - \\ General Surgery & GS & 6 & 68 & 2009 & - & - & - \\ Hematology & H & 6 & 80 & 2112 & - & - & - \\ Infectious Diseases & ID & 2 & 7 & 180 & - & - & - \\ Laboratory Medicine and Pathology & LMP & 2 & 45 & 1259 & 1 & 18 & 7614 \\ Nephrology and Hypertension & NH & 4 & 9 & 270 & - & - & - \\ Neurosurgery & N & 8 & 9 & 270 & - & - & - \\ None (Artitudes that do not belong to any department) & N/A & 2 & 15 & 450 & - & - & - \\ Obsteries and Gynecology & OG & 5 & 14 & 389 & - & - & - \\ Oncology (Medical) & OM & 20 & 51 & 1399 & - & - & - \\ Ophthalmology & O & 6 & 97 & 2232 & 2 & 11 & 218 \\ Orthopedic Surgery & OS & 8 & 54 & 1611 & - & - & - \\ Otalaryngology (ENT)/Head and Neck Surgery & ENT/HNS & 5 & 14 & 420 & 1 & 6 & 1015 \\ Pulmonary Medicine & PM & 2 & 55 & 1643 & 1 & 12 & 6420 \\ Sports Medicine & SM & 3 & 64 & 1919 & - & - & - \\ Urology & U & 8 & 33 & 933 & - & - & - \\ \hline \end{tabular}
\end{table}
Table 6: Statistics of the departments and their sub-task abbreviations mentioned in the paper with their corresponding full terms.

\begin{table}
\begin{tabular}{l|c c c} \hline Full Name & Modalities & Labels & Cases \\ \hline Mask Level & 36 & 188 & 5587 \\ Contour Level & 36 & 188 & 5587 \\ Box Level & 3 & 59 & 1715 \\ Image Level\({}^{*}\) & 13 & 474 & 12942 \\ Image Level\({}^{\#}\) & 5 & 48 & 15691 \\ \hline \end{tabular}
\end{table}
Table 7: Statistics of the perceptual granularities. \({}^{*}\) and \({}^{\#}\) denote the case for single choice and multiple choice, respectively.

\begin{table}
\begin{tabular}{l|c|c c|c c c c} \hline Full Name & Abbreviation & \multicolumn{3}{c|}{Single Choice} & \multicolumn{3}{c}{Multiple Choice} \\  & & Modalities & Labels & Cases & Modalities & Labels & Cases \\ \hline Attribute Recognition & AR & 5 & 26 & 780 & 1 & 4 & 40 \\ Blood Vessels Recognition & BVR & 7 & 15 & 436 & - & - & - \\ Bone & B & 6 & 22 & 655 & - & - & - \\ Cell Recognition & CR & 4 & 13 & 383 & 1 & 18 & 7614 \\ Counting & C & 1 & 38 & 853 & - & - & - \\ Disease Diagnosis & DD & 29 & 364 & 10167 & 3 & 26 & 8037 \\ Image Quality Grading & IQG & 2 & 10 & 300 & - & - & - \\ Microorganism Recognition & MR & 3 & 26 & 779 & - & - & - \\ Muscle & M & 1 & 5 & 150 & - & - & - \\ Nervous Tissue & NT & 2 & 4 & 120 & - & - \\ Organ Recognition - Abdomen & OR-A & 7 & 28 & 838 & - & - & - \\ Organ Recognition - Head and Neck & OR-HN & 5 & 16 & 480 & - & - & - \\ Organ Recognition - Pelvic & OR-P & 6 & 9 & 270 & - & - & - \\ Organ Recognition - Thorax & OR-T & 9 & 17 & 510 & - & - & - \\ Severity Grading & SG & 5 & 64 & 1678 & - & - & - \\ Surgeon Action Recognition & SAR & 1 & 23 & 635 & - & - & - \\ Surgical Instrument Recognition & SIR & 1 & 27 & 790 & - & - & - \\ Surgical Workflow Recognition & SWR & 1 & 14 & 420 & - & - & - \\ \hline \end{tabular}
\end{table}
Table 5: Statistics of the clinical VQA tasks and their sub-task abbreviations mentioned in the paper with their corresponding full terms.

Here, we specifically demonstrate how to customize the use of the lexical tree. First, select the data we need to test based on the users' requirements. In this example, we will focus on **ophthalmology** department and only **fundus photography** modality.

**Step-by-Step Process:**

1. **Select the Department:** First, navigate to the Lexical Tree interface and select the department relevant to our testing. In our case, we choose the "Ophthalmology" department from the available clinical tasks, as shown in Figure 8.
2. **Choose the Modality:** Within the ophthalmology department, several modalities related to eye conditions are listed. We specifically select the "Fundus Photography" modality. This selection allows us to access all the keywords associated with fundus images, which are crucial for the next step.
3. **Keyword Filtering:** After selecting the fundus photography modality, a comprehensive list of keywords appears. These keywords are critical as they will be used to filter the relevant questions for the evaluation. Examples of keywords include "advanced glaucoma", "age-related macular degeneration", and "diabetic retinopathy" among others.
4. **Retrieve Question List:** The system filters and retrieves questions from the pre-prepared question list using the selected keywords. Each question includes multiple options, and the correct answer corresponds to the keyword used for filtering. However, the correct answers are hidden from the users during the evaluation process. For instance, a question may ask about identifying a condition shown in an image, with options like "A. advanced glaucoma", "B. early glaucoma", "C. non glaucoma", etc. The correct answer, such as "advanced glaucoma" is derived from the keyword used for filtering.
5. **Model Evaluation:** The filtered question list is then used to evaluate various models. In this example, models such as GPT-4, Claude3-Opus, Qwen-Max, and others are assessed for their accuracy in answering the questions. The results are compiled and displayed in a tabular format, showcasing each model's performance.

Figure 7: Overview of the lexical tree. The whole tree is provided in the attached HTML file named “Lexical tree.html”.

In addition to the provided example, this method allows for the independent testing of **any other departments, modalities, clinical tasks, and their combinations.** For instance, if the objective is to evaluate only ophthalmology, fundus photographs, and disease diagnosis tasks, further refinement of the keywords can be achieved following the initial selection. By accessing the disease diagnosis task and selecting the fundus photography modality, we can intersect the keywords from the department-funds photography section with those from the clinical tasks-disease diagnosis section. The resulting keywords will represent those relevant exclusively to disease diagnosis tasks within the context of fundus photographs in ophthalmology.

In summary, the lexical tree provides a versatile framework for customizing evaluation processes across various medical domains, ensuring a comprehensive and focused assessment of model performance.

Figure 8: Example of how to use the Lexical Tree for customizing evaluations for the **ophthalmology** department and **funds photography** modality. The process involves selecting the department (ophthalmology), choosing the modality (funds photography), filtering questions using relevant keywords, and evaluating different models based on their accuracy in answering the filtered questions.

Evaluation

In this section, we will describe the evaluation process in detail. We evaluated various LVLMs, including medical-specific models, open-source general models, and closed-source API general models. We selected versions with approximately 7 billion parameters for testing, and the model weights were sourced from their respective official Hugging Face repositories. Our evaluation was conducted using the VLMEvalKit8 framework. For medical-specific models, we utilized the Multi-Modality-Arena9 repository for testing. Specifically, we input the prompt shown in Table 8 into the tested model to for evaluation, the option-only answers are expected. However, it's hard for some models to follow the instructions, if a model neither outputs a clear answer tagged by the letter options nor provides instructions to select an answer, we use ChatGPT-3.5-turbo-0613 to extract the answer from the model's outputs. If the answer cannot be extracted, we treat the outputs as errors. Otherwise, the extracted answers will be considered as the model's predicted answer for that question.

Footnote 8: https://github.com/open-compass/VLMEvalKit

Footnote 9: https://github.com/OpenGVLab/Multi-Modality-Arena/tree/main/MedicalEval/Question-answering_Score

### Evaluation Metric for Single-choice Questions

For all single-choice questions, we denote \(n_{\text{correct}}\) as the number of questions for which the model offered the correct answer, and \(n_{\text{questions}}\) as the total number of questions. The ACC can be calculated as follows:

\[\text{ACC}=\frac{n_{\text{correct}}}{n_{\text{questions}}}.\] (1)

### Evaluation Metric for Multiple-choice Questions

For all multiple-choice questions, we first count the number of correct predictions by the model within the groundtruth for each case, denoted as \(n_{\text{match}}\). The length of the prediction is denoted as \(l_{\text{prediction}}\), and the length of the groundtruth options are denoted as \(l_{\text{truth}}\). The evaluation metrics for multiple-choice questions is calculated as follows:

\[\text{ACC}_{mcls}=\frac{n_{\text{match}}}{l_{\text{prediction}}},\] (2)

\[\text{Recall}_{mcls}=\frac{n_{\text{match}}}{l_{\text{truth}}}.\] (3)

\begin{table}
\begin{tabular}{l} \hline \multicolumn{2}{c}{**Prompt example for single-choice questions**} \\ \hline _Question_: Observe the image. What is the most likely abnormality shown in the picture? \\ _Options_: \\ A.osteoporotic bone \\ B.healthy bone \\ Please select the correct answer from the options above. \\ \textless{}image\textgreater{} \\ \hline \multicolumn{2}{c}{**Prompt example for multiple-choice questions**} \\ \hline _Question_: Determine which part(s) is illustrated in the image. \\ _Options_: \\ A. cytosol \\ B. actin filaments \\ C. vesicles and punctate cytosolic patterns \\ D. microtubules \\ E. plasma membrane \\ F. endoplasmic reticulum \\ Please select all correct answers from the options above. Note that there is more than one correct answer. \\ Please output the answer options directly, separated by commas. For example: A,B \\ \textless{}image\textgreater{} \\ \hline \end{tabular}
\end{table}
Table 8: Examples of single-choice and multiple-choice question prompts.

### Evaluated Models

In this paper, we evaluate \(50\) models on our GMAI-MMBench, and we list them in Table 9.

## Appendix E Results

In this section, we first provide the complete quantitative results in our experiments, and then perform the case study by analyzing \(53\) representative examples of models' outputs.

### Quantitative Results

The complete test results are shown in the table below. Table 10 shows the results in different clinical VQA tasks; Table 11 shows the results across different departments; Table 12 shows the results in different perceptual granularities.

\begin{table}
\begin{tabular}{c|c c c c} \hline Series & Models & \#Params & Vision Encoder & LLM \\ \hline \multirow{4}{*}{Med model series} & MedVInT [268] & - & - & - \\  & Med-Flamingo [181] & 8.3B & CLIP ViT/L-14 & LLaMA-7B \\  & LLaVA-Med [138] & - & CLIP ViT/L-14 & Mistral-7B \\  & RadFM [254] & 14B & 3D ViT & MedLLaM-13B \\  & Qilin-Med-VL-Chat [149] & - & Clip ViT/L-14 & Chinese-LLaMA2-Chat-13B \\  & ModP [95] & 40B & InterViT-6B & Nous-Hermes-2-Yi-34B \\ \hline \multirow{4}{*}{Ungroupupped series} & TransCoreM [3] & 13.4B & CLIP ViT/L-14 & PCITransGP-T13B \\  & VisualGLM-6B [61] & 7.8B & EVA-CLIP & ChatGLM-6B \\  & mPLUG-Owl2 [259] & 8.2B & CLIP ViT-L/14 & LLaMA2-7B \\  & OmniLM-IM2B [261] & 12B & EVOA-5B & Zephy-7B-3 \\  & PandGLP [324] & 13B & Imagebird ViT-H/14 & Vicuna-v0-13B \\  & Mini-Gemini-7B [141] & 7B & CLIP-L & Vicuna-v1.5-7B \\  & Emu2-Chat [237] & 37B & EVA-02-CLIP-E-plus & LLaMA-33B \\  & Flamingo [217] & 9B & CLIP ViT-L/14 & MPTP-7B \\  & MMAlaya [154] & 7.8B & EVA-G & Alaya-7B-Chat \\ \hline \multirow{2}{*}{CogVLM series} & CogVLM-Chat [249] & 17B & EVA-CLIP-E & Vicuna-v1.5-7B \\  & CogVLM-grounding-generalist [249] & 17B & EVA-CLIP-E & Vicuna-v1.5-7B \\ \hline InstructBLIP series & InstructBLIP-7B [56] & 8B & EVA-G & Vicuna-7B \\ \hline DeepSeek series & DeepSeek-VL-13B [155] & 1.3B & SAM-B \& SigLIP-L & DeeSeek-1B \\  & DeepSeek-VL-7B [155] & 7.3B & SAM-B \& SigLIP-L & DeeSeek-7B \\ \hline \multirow{2}{*}{Idefics series} & Idefics-9BInstruct [137] & 9B & CLIP ViT/L14 & LLaMA 7B \\ \hline \multirow{4}{*}{XComposer series} & ShareCaptioner [43] & 8B & EVA-G & InternalLM-7B \\  & XComposer [266] & 8B & EVA-CLIP-G & InternalLM-7B \\  & XComposer2 [62] & 7B & CLIP ViT/L4 & InternalLM2-7B \\  & XComposer2-4KHD [63] & 7B & CLIP ViT-L/14 & InernLM2-7B \\ \hline \multirow{4}{*}{Yi-VL series} & Yi-VL-6B [7] & 6.6B & CLIP ViT-H/14 & Yi-6B \\ \hline \multirow{4}{*}{InterVL series} & InterViT-Chat-VL-1 [47] & 19B & InterViT-6B & LLAMA2-13B \\  & InterViT-Chat-VL-12 [47] & 40B & InterViT-6B & Nous-Hermes-2-Yi-34B \\  & InterViT-Chat-VL-12.9 [47] & 40B & InterViT-6B & Nous-Hermes-2-Yi-34B \\  & InterViT-Chat-VL-15.5 [46] & 25.5B & InterViT-6B & InterViT-6B \\ \hline \multirow{4}{*}{LIaVA series} & LLaVA-NexT-mistral-7B [147] & 7.6B & CLIP ViT-L/14 & Mistral-7B \\  & LLaVA-NexT-vicuna-7B [147] & 7.1B & CLIP ViT-L/14 & Vicuna-v1.5-7B \\  & LLaVA-NexT-5.7B [148] & 7.2B & CLIP ViT-L/14 & Vicuna-v1.5-7B \\  & ShareGPIVT-7B [43] & 7.2B & CLIP ViT-L/14 & Vicuna-v1.5-7B \\ \hline \multirow{4}{*}{Xtuner series} & LLaVA-InterLM-7B [54] & 7.6B & CLIP ViT-L/14 & InternalLM-7B \\  & LAVA-InternalM2-7B [54] & 8.1B & CLIP ViT-14 & IntermixT-7B \\  & LLaVA-V1.5-7B-Mexner [54] & 7.2B & CLIP ViT-L/14 & Vicuna-v1.5-7B \\  & LLaVA-V1.5-13b-ktuner [54] & 13.4B & CLIP ViT-L/14 & Vicuna-v1.5-3B \\ \hline MiniCPM series & MiniCPMV [103] & 2.8B & SigLIP-400M & MiniCPM-2.4B \\  & MiniCPM-V2 [257] & 2.8B & SigLIP-400M & MiniCPM-2.4B \\ \hline \multirow{4}{*}{Qwen series} & Monkey [142] & 9.8B & CLIP-ViT-BigHuge & Owen-7B \\  & Monkey-Chat [142] & 9.8B & ViT-BiHuge & Owen-7B \\  & Owen-VL [149] & 9.6B & CLIP ViT-G/16 & Owen-7B \\  & Owen-VL-Chat [149] & 9.6B & CLIP ViT-G/16 & Owen-7B \\ \hline \multirow{4}{*}{API series} & Qwen-VL-Max [18] & - & - & QwenLM \\  & Claude3-Opus [13] & - & - & - \\  & GPT-40 [5] & - & - & - \\  & GPT-4V [5] & - & - & - \\  & Gemini 1.0 [240] & - & - & - \\  & Gemini 1.5 [21] & - & - & - \\ \hline \end{tabular}
\end{table}
Table 9: The model architecture of 50 LVLMs evaluated on GMAIMMBench.

[MISSING_PAGE_FAIL:44]

\begin{table}
\begin{tabular}{l|c c c c c c c c c c c c c c c c c c c c c c c c} \hline \multirow{2}{*}{Model name} & \multicolumn{3}{c|}{Overall Overall} & \multirow{2}{*}{CS} & \multirow{2}{*}{D} & \multirow{2}{*}{E} & \multirow{2}{*}{GH} & \multirow{2}{*}{GS} & \multirow{2}{*}{H} & \multirow{2}{*}{ID} & \multirow{2}{*}{LMP} & \multirow{2}{*}{NH} & \multirow{2}{*}{N} & \multirow{2}{*}{OG} & \multirow{2}{*}{OM} & \multirow{2}{*}{OS} & \multirow{2}{*}{ENT/INS} & \multirow{2}{*}{PM} & \multirow{2}{*}{SM} & \multirow{2}{*}{U} \\  & (val) & (test) & & & & & & & & & & & & & & & & & & & & & & & & \\ \hline Random & 25.70 & 25.94 & 22.82 & 25.19 & 21.10 & 20.89 & 27.97 & 22.34 & 24.45 & 31.71 & 28.59 & 22.38 & 23.40 & 20.79 & 21.75 & 27.77 & 30.36 & 25.92 & 22.53 & 27.42 & 22.87 & 28.19 \\ \hline \multicolumn{11}{c}{MedVLT [268]} & \multicolumn{1}{c|}{22.29} & \multicolumn{1}{c|}{1.96} & \multicolumn{1}{c|}{0.24} & \multicolumn{1}{c|}{2.50} & \multicolumn{1}{c|}{1.00} & \multicolumn{1}{c|}{1.94} & \multicolumn{1}{c|}{1.09} & \multicolumn{1}{c|}{0.88} & \multicolumn{1}{c}{3.31} & \multicolumn{1}{c}{5.23} & \multicolumn{1}{c}{1.4} & \multicolumn{1}{c}{0.73} & \multicolumn{1}{c}{0.60} & \multicolumn{1}{c}{1.40} & \multicolumn{1}{c}{4.44} & \multicolumn{1}{c}{0.56} & \multicolumn{1}{c}{0.0} & \multicolumn{1}{c}{2.24} & \multicolumn{1}{c}{0.64} & \multicolumn{1}{c}{0.86} \\ Med-Planting [181] & 12.74 & 11.64 & 11.76 & 12.24 & 100.00 & 10.86 & 9.33 & 4.22 & 7.26 & 100.00 & 10.91 & 12.88 & 14.89 & 15.77 & 12.40 & 15.43 & 12.89 & 15.42 & 10.47 \\ LiLA/Va-Med [181] & 20.54 & 9.61 & 22.06 & 22.02 & 20.20 & 20.31 & 16.30 & 18.64 & 18.52 & 21.84 & 21.86 & 12.61 & 12.49 & 12.93 & 22.18 & 18.38 & 20.99 & 16.87 & 24.09 & 21.55 \\ \hline \multicolumn{11}{c}{Qin-ModVLT-Cat [149]} & \multicolumn{1}{c|}{22.44} & \multicolumn{1}{c|}{22.26} & \multicolumn{1}{c|}{12.94} & \multicolumn{1}{c|}{22.16} & \multicolumn{1}{c|}{15.50} & \multicolumn{1}{c}{22.20} & \multicolumn{1}{c}{18.93} & \multicolumn{1}{c}{17.38} & \multicolumn{1}{c}{17.88} & \multicolumn{1}{c}{17.32} & \multicolumn{1}{c}{23.14} & \multicolumn{1}{c}{28.20} & \multicolumn{1}{c}{20.20} & \multicolumn{1}{c}{28.53} & \multicolumn{1}{c}{25.55} & \multicolumn{1}{c}{19.07} & \multicolumn{1}{c}{18.41} & \multicolumn{1}{c}{29.42} & \multicolumn{1}{c}{22.27} & \multicolumn{1}{c}{22.29} \\ RadFM [244] & 22.95 & 22.93 & 22.34 & 23.20 & 20.59 & 20.93 & 19.92 & 28.48 & 24.48 & 21.80 & 20.30 & 16.95 & 26.50 & 26.52 & 18.26 & 26.54 & 25.19 & 23.70 & 22.00 \\ MedI Dev & 41.95 & 43.69 & 53.18 & 45.28 & 33.00 & 44.78 & 28.03 & 29.91 & 47.68 & 35.22 & 38.29 & 78.55 & 25.08 & 49.53 & 45.31 & 52.09 & 48.61 & 25.36 & 54.21 & 39.90 \\  & \multicolumn{11}{c}{Open

\begin{table}
\begin{tabular}{c c c c c c c c c c c} \hline Model name & Size & Overall(val) & Overall(test) & Seg C & Seg M & 2D Cls update & 2D Det & 2D Mcls\_acc & 2D Mcls\_recall \\ \hline Random & - & 25.70 & 25.88 & 22.19 & 22.91 & 28.93 & 24.55 & 45.85 & 57.02 \\ \hline \multicolumn{1}{c}{MedMed Special Model} & \multicolumn{1}{c}{} & & & & & & \\ MedVInT [268] & - & 2.29 & 1.98 & 0.82 & 0.25 & 3.48 & 0.12 & 0.05 & 0.02 \\ Med-Flaming [181] & 8.3B & 12.74 & 11.75 & 11.95 & 11.94 & 11.92 & 9.15 & 46.10 & 50.19 \\ LiLAVA-Med [138] & - & 20.54 & 19.83 & 18.45 & 18.97 & 21.15 & 17.14 & 45.84 & 41.19 \\ Qinin-Med-VL-Clat [149] & - & 22.34 & 22.06 & 19.84 & 20.30 & 23.80 & 21.87 & 44.50 & 33.90 \\ RadFM [254] & 14B & 22.95 & 22.93 & 20.43 & 20.27 & 25.71 & 18.83 & 40.98 & 57.45 \\ MedDv [95] & 40B & 41.95 & 43.18 & 42.55 & 44.03 & 45.08 & 28.10 & 48.09 & 23.38 \\ \hline \multicolumn{10}{c}{Open-Source LVLMs} & & & & & & & \\ CogVLM-grounding-generalist [249] & 17B & 5.20 & 5.39 & 6.80 & 5.51 & 5.11 & 2.57 & 46.24 & 49.82 \\ XComposer [266] & 8B & 8.92 & 7.71 & 8.87 & 6.24 & 8.02 & 6.30 & 31.45 & 23.68 \\ PandaPFP138 [234] & 13B & 16.69 & 15.94 & 19.25 & 18.88 & 13.74 & 12.24 & 41.22 & 49.95 \\ Flamingo v2 [17] & 9B & 25.58 & 26.23 & 22.52 & 22.48 & 30.12 & 21.17 & 41.80 & 19.17 \\ VisualGLM-Gh [61] & 7.8B & 29.58 & 30.20 & 27.30 & 27.31 & 33.75 & 22.16 & 43.08 & 35.22 \\ Icdrics-99-Instruct [137] & 9B & 29.74 & 30.81 & 25.50 & 25.21 & 36.45 & 23.85 & 43.47 & 46.02 \\ InstructBLIP-7B [56] & 8B & 31.80 & 31.00 & 29.12 & 21.77 & 36.71 & 24.08 & 39.43 & 23.79 \\ Mini-Gemin-7B [141] & 7B & 32.17 & 31.22 & 32.13 & 32.92 & 30.72 & 26.53 & 45.38 & 57.99 \\ MMAlaya [154] & 7.8B & 32.19 & 32.02 & 29.33 & 30.22 & 35.02 & 24.02 & 48.43 & 20.93 \\ Qwen-VL [19] & 9.6B & 34.80 & 35.55 & 33.20 & 33.43 & 38.95 & 24.49 & 44.95 & 56.97 \\ Yi-VL-6B [7] & 6.6B & 34.82 & 34.00 & 31.42 & 32.26 & 37.15 & 24.31 & 50.25 & 44.32 \\ LLLAVA-XX-Viciuna-7B [147] & 7.1B & 34.86 & 35.59 & 33.06 & 32.95 & 38.96 & 27.06 & 44.75 & 42.45 \\ Qwen-VL-Clat [19] & 9.6B & 35.07 & 36.35 & 34.45 & 35.20 & 39.55 & 22.04 & 42.88 & 81.23 \\ CogVLM-Chat [249] & 17B & 35.23 & 35.83 & 34.13 & 34.49 & 38.55 & 25.25 & 47.09 & **90.26** \\ Mooley [142] & 9.8B & 35.48 & 35.92 & 33.18 & 34.01 & 39.32 & 25.42 & 44.57 & 42.35 \\ mPLLO-GwL [259] & 8.2B & 35.62 & 35.89 & 33.68 & 34.74 & 38.80 & 24.90 & 42.59 & 41.84 \\ ShareCapioneer [43] & 8B & 36.37 & 36.07 & 34.74 & 35.93 & 38.25 & 24.37 & 40.00 & 16.95 \\ EmuZ-Dat [237] & 37B & 36.50 & 35.54 & 36.54 & 36.54 & 37.62 & 39.57 & 27.76 & 44.29 & 37.65 \\ XComposer-24KHD [63] & 7B & 36.66 & 37.93 & 36.84 & 38.02 & 39.84 & 26.65 & 48.83 & 44.08 \\ ShareGP4V-7B [43] & 7.2B & 36.71 & 36.52 & 34.74 & 35.15 & 39.24 & 26.18 & 46.11 &

### Case Study

In this section, we present a case study analysis of several LVLMs on various cases in GMAI-MMBench. The entire logical process of our study is illustrated in Figure 9. Other than **Correct**, we classify the error types from input to output into five major categories:

**Correct:** LVLMs offer the correct answer. This indicates that the model accurately understands both the image and the question, and provides an appropriate and relevant response.

**Question misunderstanding:** LVLMs fail to correctly understand the question and generate erroneous answers. For example: LLAVA-Med may not understand the purpose of identifying the surgical process from the question, instead, it describes the image content in detail as shown in Figure 27.

**Perceptual error:** LVLMs fail to locate, detect, or recognize the content or objects in images, which are necessary for answering the questions. This includes scenarios where the model misses critical details or misinterprets the image's content. For example: GPT-4o may ignore the important tool in the lower left corner that is clearing the debris in Figure 32. Claude3-Opus chooses the wrong answer as it cannot correctly identify the content in the mask in Figure 38.

**Lack of knowledge:** LVLMs can recognize both the image and the question but still make errors in specific cases, suggesting a lack of domain-specific knowledge required to answer specialized questions. For example: Models directly show their insufficient knowledge to answer or fail to respond without additional information as shown in Figure 52, Figure 54, Figure 52, etc. Another case in Figure 51 shows that GPT-4o correctly describes the image and understands the question but still chooses a wrong answer, suggesting it may lack the ability to distinguish between carcinoma in situ and invasive carcinoma.

**Irrelevant response:** LVLMs do not address the question directly and produce unreadable or unrelated responses. This problem is especially noticeable in open-source models. For example: RadFM only generates a reference paper without any additional outputs in Figure 57.

**Refuse to answer:** LVLMs decline to answer certain questions to keep the system safe for all users, such as those involving sensitive or ethical issues, and refuse to provide medical advice when they determine that human professional assistance is required. This issue only occurs in proprietary models like GPTs and Claudes.

In our test, we randomly select 53 VQA pairs from different clinical VQA tasks, departments, and perceptual granularities. All cases are listed in Table 13. Based on our observations of the evaluation results, we find that proprietary models like GPT-4o and Claude3-Opus rarely encounter difficulties in question understanding. The majority of errors for these models stem from perceptual error and lack of knowledge. In contrast, specialized medical models such as RadFM and LLAVA-Med frequently exhibit language understanding errors, making it difficult to effectively evaluate visual perceptual abilities. As a result, the case study indicates that general models need to enhance their performance

Figure 9: The illustration of the entire logical process from input to output in our case study.

on specialized medical images, which may require more medical data for training. Meanwhile, specialized medical models need further training or fine-tuning in language aspects.

\begin{table}
\begin{tabular}{c c c c c} \hline Figure & Clinical VQA task & Department & Perceptual granularity & Category \\ \hline
10 & MR & H & Image Level & Correct \\
11 & C & H & Image Level & Correct \\
12 & SWR & ENT & Image Level & Correct \\
13 & DD & GH & Image Level & Correct \\
14 & ASR & NH & Image Level & Correct \\
15 & SAR & U & Box Level & Correct \\
16 & DD & PM & Box Level & Correct \\
17 & OR-NH & E & Mask Level & Correct \\
18 & OR-P & U & Contour Level & Correct \\
19 & SIR & GS & Box Level & Correct \\
20 & BVR & H & Mask Level & Correct \\
21 & CR & H & Box Level & Correct \\
22 & DD & CS & Mask Level & Correct \\
23 & DD & OS & Contour Level & Correct \\
24 & NT & O & Mask Level & Correct \\
25 & OR-T & PM & Mask Level & Correct \\
26 & SIR & GS & Mask Level & Correct \\
27 & SWR & GS & Image Level & Question misunderstanding \\
28 & BVR & O & Mask Level & Question misunderstanding \\
29 & ACR & OS & Mask Level & Question misunderstanding \\
30 & MR & GH & Image Level & Question misunderstanding \\
31 & C & H & Image Level & Perceptual error \\
32 & SWR & GS & Image Level & Perceptual error \\
33 & OR-T & PM & Mask Level & Perceptual error \\
34 & AR & LMP & Image Level & Perceptual error \\
35 & NT & N & Mask Level & Perceptual error \\
36 & DD & CS & Box Level & Perceptual error \\
37 & DD & D & Mask Level & Perceptual error \\
38 & DD & GH & Contour Level & Perceptual error \\

[MISSING_PAGE_POST]

D & PM & Image Level & Refuse to answer \\
62 & BVR & O & Mask Level & Refuse to answer \\ \hline \end{tabular}
\end{table}
Table 13: Table index of our case study figures.

Figure 10: An example of correct case. Green highlights the right answer. Back to Table Index.

Figure 11: An example of correct case. Green highlights the right answer. Back to Table Index.

## Appendix A

Figure 12: An example of correct case. Green highlights the right answer. Back to Table Index.

## Appendix A

Figure 13: An example of correct case. Green highlights the right answer. Back to Table Index.

[MISSING_PAGE_EMPTY:53]

Figure 15: An example of correct case. Green highlights the right answer. Back to Table Index.

Figure 16: An example of correct case. Green highlights the right answer. Back to Table Index.

## Appendix A

Figure 17: An example of correct case. Green highlights the right answer. Back to Table Index.

Figure 18: An example of correct case. Green highlights the right answer. Back to Table Index.

Figure 19: An example of correct case. Green highlights the right answer. Back to Table Index.

Figure 20: An example of correct case. Green highlights the right answer. Back to Table Index.

## Appendix A

Figure 21: An example of correct case. Green highlights the right answer. Back to Table Index.

## Appendix A

Figure 22: An example of correct case. Green highlights the right answer. Back to Table Index.

## Appendix A

Figure 23: An example of correct case. Green highlights the right answer. Back to Table Index.

## Appendix A

Figure 24: An example of correct case. Green highlights the right answer. Back to Table Index.

Figure 25: An example of correct case. Green highlights the right answer. Back to Table Index.

[MISSING_PAGE_EMPTY:65]

[MISSING_PAGE_EMPTY:66]

[MISSING_PAGE_EMPTY:67]

[MISSING_PAGE_EMPTY:68]

[MISSING_PAGE_EMPTY:69]

## Appendix A

Figure 31: An example of perceptual error - detail missing case. Green highlights the right answer. Yellow highlights the wrong answer. Back to Table Index.

## Appendix A Appendix

Figure 32: An example of perceptual error - detail missing case. Green highlights the right answer. Yellow highlights the wrong answer. Back to Table Index.

## Appendix A Appendix

Figure 33: An example of perceptual error - detail missing case. Green highlights the right answer. Yellow highlights the wrong answer. Back to Table Index.

[MISSING_PAGE_EMPTY:73]

## Appendix A Appendix

Figure 35: An example of perceptual error - misinterpretation case. Green highlights the right answer. Yellow highlights the wrong answer. Back to Table Index.

## Appendix A Appendix

Figure 36: An example of perceptual error - misinterpretation case. Green highlights the right answer. Yellow highlights the wrong answer. Back to Table Index.

## Appendix A

Figure 37: An example of perceptual error - misinterpretation case. Green highlights the right answer. Yellow highlights the wrong answer. Back to Table Index.

[MISSING_PAGE_EMPTY:77]

Figure 39: An example of perceptual error - misinterpretation case. Green highlights the right answer. Yellow highlights the wrong answer. Back to Table Index.

[MISSING_PAGE_EMPTY:79]

## Chapter 4 Experimental Error

Figure 41: An example of perceptual error - misinterpretation case. Green highlights the right answer. Yellow highlights the wrong answer. Back to Table Index.

## Appendix A

Figure 42: A lack of knowledge example. Green highlights the right answer. Yellow highlights the wrong answer. Back to Table Index.

[MISSING_PAGE_EMPTY:82]

Figure 44: A lack of knowledge example. Green highlights the right answer. Yellow highlights the wrong answer. Back to Table Index.

Figure 45: A lack of knowledge example. Green highlights the right answer. Yellow highlights the wrong answer. Back to Table Index.

[MISSING_PAGE_EMPTY:85]

[MISSING_PAGE_EMPTY:86]

[MISSING_PAGE_EMPTY:87]

## Appendix A

Figure 49: A lack of knowledge example. Green highlights the right answer. Yellow highlights the wrong answer. Back to Table Index.

Figure 50: A lack of knowledge example. Green highlights the right answer. Yellow highlights the wrong answer. Back to Table Index.

## Chapter 5 Conclusion

Figure 51: A lack of knowledge example. Green highlights the right answer. Yellow highlights the wrong answer. Back to Table Index.

[MISSING_PAGE_EMPTY:91]

Figure 53: An example of unable to determine. Green highlights the right answer. Yellow highlights the wrong answer. Back to Table Index.

[MISSING_PAGE_EMPTY:93]

## Appendix A

Figure 55: An example of unable to determine. Green highlights the right answer. Yellow highlights the wrong answer. Back to Table Index.

Figure 56: An example of unable to determine. Green highlights the right answer. Yellow highlights the wrong answer. Back to Table Index.

Figure 57: An example of irrelevant response. Green highlights the right answer. Yellow highlights the wrong answer. Back to Table Index.

[MISSING_PAGE_EMPTY:97]

[MISSING_PAGE_FAIL:98]

[MISSING_PAGE_EMPTY:99]

Figure 61: An example of refuse to answer. Green highlights the right answer. Yellow highlights the wrong answer. Back to Table Index.

Figure 62: An example of refuse to answer. Green highlights the right answer. Yellow highlights the wrong answer. Back to Table Index.