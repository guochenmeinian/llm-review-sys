# Provably Robust Temporal Difference Learning for Heavy-Tailed Rewards

 Semih Cayci

Department of Mathematics

RWTH Aachen University

Aachen, Germany

cayci@mathc.rwth-aachen.de &Atilla Eryilmaz

Department of Electrical and Computer Engineering

The Ohio State University

Columbus, OH 43210

eryilmaz.2@osu.edu

###### Abstract

In a broad class of reinforcement learning applications, stochastic rewards have heavy-tailed distributions, which lead to infinite second-order moments for stochastic (semi)gradients in policy evaluation and direct policy optimization. In such instances, the existing RL methods may fail miserably due to frequent statistical outliers. In this work, we establish that temporal difference (TD) learning with a dynamic gradient clipping mechanism, and correspondingly operated natural actor-critic (NAC), can be provably robustified against heavy-tailed reward distributions. It is shown in the framework of linear function approximation that a favorable tradeoff between bias and variability of the stochastic gradients can be achieved with this dynamic gradient clipping mechanism. In particular, we prove that robust versions of TD learning achieve sample complexities of order \(\mathcal{O}(\varepsilon^{-\frac{1}{p}})\) and \(\mathcal{O}(\varepsilon^{-1-\frac{1}{p}})\) with and without the full-rank assumption on the feature matrix, respectively, under heavy-tailed rewards with finite moments of order \((1+p)\) for some \(p\in(0,1]\), both in expectation and with high probability. We show that a robust variant of NAC based on Robust TD learning achieves \(\tilde{\mathcal{O}}(\varepsilon^{-4-\frac{2}{p}})\) sample complexity. We corroborate our theoretical results with numerical experiments.

## 1 Introduction

In this paper, we develop a framework for robust reinforcement learning in the presence of rewards with heavy-tailed distributions. Heavy-tailed phenomena, stemming from frequently observed statistical outliers, have been ubiquitous in decision-making applications under uncertainty. To name a few examples, waiting times in wireless communication networks [44; 24; 58], completion times of SAT solvers [14], numerous payoff quantities (e.g., stock prices, consumer signals) in economics and finance [22; 35; 38; 37] exhibit heavy-tailed behavior. An important characteristic of heavy-tailed random variables is the infinite order of higher moments, which stems from the frequently occurring outliers.

In reinforcement learning (RL), the goal is to maximize expected total reward in a Markov decision process (MDP) by continual interactions with the unknown and dynamic environment. Among policy optimization methods, Natural actor-critic (NAC) method and its variants [51; 30; 46; 17; 27; 5; 29] have become particularly prevalent due to their desirable stability and versatility characteristics, emanating from the use of temporal difference (TD) learning as the critic for the policy evaluation component of the NAC operation. The existing theoretical analyses for temporal difference learning [4; 54] and natural policy gradient/actor-critic methods [62; 1; 57] assume that the stochastic gradients have finite second-order moments, or even they are bounded. In particular, it is unknown whether natural actor-critic with function approximation is robust for stochastic rewards of heavy-tailed distributions with potentially infinite second-order moments. Furthermore, in practice, these methodsare prone to non-vanishing and even increasing error under heavy-tailed reward distributions (see Example 1). This motivates us for the following fundamental question in this paper:

_Can temporal difference learning with function approximation be modified to provably achieve global optimality under stochastic rewards with heavy tails?_

We provide an affirmative answer to the above question by proposing a simple modification to the TD learning algorithm, which yields robustness against heavy tails. In particular, we show that incorporating a dynamic gradient clipping mechanism with a carefully-chosen sequence of clipping radii can provably robustify TD learning and NAC with linear function approximation, leading to global near-optimality even under stochastic rewards of infinite variance.

**Example 1** (Failure of TD learning under heavy-tailed reward).: In this example, we consider a randomly-generated discounted-reward Markov reward process1\((X_{t},R_{t})_{t}\) on a state space \(\mathbb{X}\) with \(|\mathbb{X}|=64\) states, with the discount factor \(\gamma=0.9\) and the reward \(R_{t}(X_{t})=r(X_{t})+N_{t}-\mathbb{E}[N_{t}]\) with \(N_{t}\stackrel{{ iid}}{{\sim}}\mathsf{Pareto}(1,1.4)\) for any \(t\). In order to predict the value function, we use (projected) TD learning (see [4]) with linear function approximation based on Gaussian features of dimension \(d=4\) and projection radius \(\rho=30\).

Footnote 1: The details of the setup, along with other numerical examples can be found in Section 4.

The performance results are shown in Figure 1. Since \(R_{t}\) is heavy-tailed with infinite variance, the existing convergence results for traditional TD learning, which assume that \(R_{t}\) has finite variance, do not hold. Furthermore, Figure 1 reveals that TD learning is prone to non-varnishing and even increasing error in practice despite the projection step, iterate averaging and small learning rate, due to the statistical outliers that cause extremely large error often as indicated by a non-negligible fraction of green lines in Figure 0(a). On the other hand, with the same learning rate, projection radius and state-reward realizations, our robust variant of TD learning provides resilience against outliers (see Figure 0(b)), and leads to convergence in the expected behavior as in Figure 0(c).

Stochastic rewards with heavy-tailed distributions appear in many important applications. Below, we briefly provide two motivating applications that necessitate robust RL methods to handle heavy tails.

**Application (1): Algorithm portfolios.** In solving complicated problems such as Boolean satisfiability (SAT) and complete search problems, which appear in numerous applications [19; 43], multiple algorithmic solutions with different characteristics are available. The algorithm selection problem is concerned about the minimization of total execution times to solve these problems [32; 49; 31], where different data distributions and machine characteristics, caused by recursive algorithms, are modeled as states, algorithm choices are modeled as actions, and the execution time of a selected algorithm is modeled as the cost (i.e., negative reward). It is well-known that the execution times, i.e., rewards, in the algorithm selection problem have _heavy-tailed distributions_ with infinite-variance (e.g., \(\mathsf{Pareto}(1,1+p)\) with \(0<p<1\) as in [13]) similar to the case in Example 1[13; 15; 49]. Thus, algorithm selection problem requires robust techniques that we consider here.

**Application (2): Scheduling for wireless networks.** The scheduling problem considers matching the users with random service demands to fading wireless channels (e.g., Gilbert-Elliot model) with stochastic transmission times so as to minimize the expected delay. A widely-adopted approach to

Figure 1: Non-convergent behavior of TD learning under heavy-tailed noise with tail index 1.4. Each faded green line is the MSE for an individual trial, and the solid lines with markers indicate the average mean squared error for TD learning and Robust TD learning.

study the scheduling problem is to use MDPs (see, e.g., [40; 21; 10; 2]). It has been observed that the transmission times follow heavy-tailed distributions of infinite variance, due to various factors including the MAC protocol used, packet size, and channel fading [18; 58; 24; 23]. As such, solving this by using RL approach necessitates robust methods to handle heavy-tailed execution times.

**Main contributions.** Our main contributions in this paper contain the following.

\(\bullet\)_Robust TD learning with dynamic clipping for heavy-tailed rewards._ We propose Robust TD learning with a dynamic gradient clipping mechanism, and prove that this TD learning variant with linear function approximation can achieve arbitrarily small estimation error that vanishes at rates \(\mathcal{O}(T^{-\frac{p}{1+p}})\) and \(\widetilde{\mathcal{O}}(T^{-p})\) without and with full-rank assumption on the feature matrix, respectively, even for heavy-tailed rewards with moments of order \(1+p\) for \(p\in(0,1]\). Our proof techniques make use of Lyapunov analysis coupled with martingale techniques for robust statistical estimation in dynamical systems, and can be of independent interest in the analysis of first-order methods.

\(\bullet\)_Robust NAC under heavy-tailed rewards._ Based on Robust TD learning and the compatible function approximation result in [27], we propose a robust NAC variant, and show that \(\mathcal{O}(\varepsilon^{-4-\frac{2}{p}})\) samples suffice to achieve \(\varepsilon>0\) error under standard concentrability coefficient assumptions.

\(\bullet\)_High-probability error bounds._ We provide high-probability (sub-Gaussian) error bounds for the robust NAC and TD learning methods in addition to the traditional expectation bounds.

From a statistical viewpoint, our analysis in this work indicates a favorable bias-variance tradeoff: by introducing a vanishing bias to the semi-gradient via particular choices of dynamic gradient clipping, one can achieve _robustness_ by eliminating the destructive impacts of statistical outliers even if the semi-gradient has infinite variance, leading to near optimality.

### Related Work

**Temporal difference learning.** Temporal difference (TD) learning was proposed in [50], and has been the prominent policy evaluation method. The existing theoretical analyses of TD learning consider MDPs with bounded rewards [4; 7], or rewards with finite variance [54], while we consider heavy-tailed rewards. Our analysis utilizes the Lyapunov techniques in [4].

**Policy gradient methods.** Policy gradient (PG), and its variant natural policy gradient (NPG) have attracted significant attention in RL [59; 52; 27]. Recent theoretical works investigate the local and global convergence of these methods in the exact case, or with stochastic and bounded rewards [1; 57; 34; 39; 61]. As such, heavy-tailed rewards have not been considered in these works.

**Bandits with heavy-tailed rewards.** Stochastic bandit variants with heavy-tailed payoffs were studied in multiple works [6; 48; 33; 8]. The stochastic bandit setting can be interpreted as a very simple single-state (i.e., stateless or memoryless) model-based and tabular RL problem. The model we consider in this paper is a model-free RL setting on an MDP with a large state space, which is considerably more complicated than the bandit setting.

**Stochastic gradient descent with heavy-tailed noise.** There has been an increasing interest in the analysis of SGD with heavy-tailed gradient noise recently [56; 11; 16], following the seminal work of [45]. In our work, we consider the RL problem, which has significantly different dynamics than stochastic convex optimization.

**Robust mean and covariance estimation.** In basic statistical problems of mean and covariance estimation [42; 41; 36] and regression [20], the traditional methods do not yield the optimal convergence rates for heavy-tailed random variables, which led to the development of robust mean and covariance estimation techniques (for reference, see [36; 28]). Our paper utilizes tools from robust mean estimation literature (particularly, truncated mean estimator analysis in [6]), but considers the more complicated problem of TD learning and policy optimization in a dynamic environment rather than a static mean or covariance estimation problem with iid observations.

### Notation

For a symmetric matrix \(A\in\mathbb{R}^{d\times d}\), \(\lambda_{\text{min}}(A)\) denotes its minimum eigenvalue. \(B_{2}(x,\rho)=\{y\in\mathbb{R}^{d}:\|x-y\|_{2}\leq\rho\}\) and \(\Pi_{\mathcal{C}}\{x\}=\arg\min_{y\in\mathcal{C}}\|x-y\|_{2}^{2}\) for any convex \(\mathcal{C}\subset\mathbb{R}^{d}\).

Robust TD Learning for Value Prediction under Heavy Tails

First, we consider the problem of predicting the value function for a given discounted-reward Markov reward process with heavy-tailed rewards.

### Value Prediction Problem

For a finite but arbitrarily large state space \(\mathbb{X}\), let \((X_{t})_{t\in\mathbb{N}}\) be an \(\mathbb{X}\)-valued Markov chain with the transition kernel \(\mathcal{P}:\mathbb{X}\times\mathbb{X}\to[0,1]\). We consider a Markov reward process \((X_{t},R_{t})_{t\in\mathbb{N}}\) such that at state \(X_{t}\), a stochastic reward \(R_{t}=R_{t}(X_{t})\) is obtained for all \(t\geq 0\). For a discount factor \(\gamma\in[0,1)\), the value function for the MRP \((X_{t},R_{t})_{t\in\mathbb{N}}\) is the following:

\[\mathcal{V}(x)=\mathbb{E}\Big{[}\sum_{t=1}^{\infty}\gamma^{t-1}R_{t}(X_{t}) \Big{|}X_{1}=x\Big{]},\;x\in\mathbb{X}.\] (1)

**Objective.** The goal is to learn \(\mathcal{V}\) without knowing the transition kernel \(\mathcal{P}\) by using samples from the system. In particular, for a parameterized class of functions \(\{f_{\Theta}:\mathbb{X}\to\mathbb{R}:\Theta\in\mathbb{R}^{d}\}\), the goal is to solve the following stochastic optimization problem with mean squared error:

\[\min_{\Theta\in\mathbb{R}^{d}}\mathop{\mathbb{E}}_{x\sim\mu}|f_{\Theta}(x)- \mathcal{V}(x)|^{2}.\] (2)

In order to solve (2) under Assumption 1, next we propose a robust variant of temporal difference (TD) learning with linear function approximation [50; 54].

### Robust TD Learning Algorithm

For a given set of feature vectors \(\{\Phi(x)\in\mathbb{R}^{d}:x\in\mathbb{X}\}\) with \(\sup_{x\in\mathbb{X}}\|\Phi(x)\|_{2}\leq 1\), we use \(f_{\Theta}(\cdot)=\langle\Theta,\Phi(\cdot)\rangle\) as the approximation architecture. For a given dataset \(\mathcal{D}=\{(X_{t},R_{t},X^{\prime}_{t})\in\mathbb{X}\times\mathbb{R}\times \mathbb{X}:t\in\mathbb{N}\}\) with \(X^{\prime}_{t}\sim\mathcal{P}(X_{t},\cdot)\), let the stochastic semi-gradient at \(\Theta\in\mathbb{R}^{d}\) be defined as

\[g_{t}(\Theta)=\Big{(}R_{t}(X_{t})+\gamma f_{\Theta}(X^{\prime}_{t})-f_{\Theta} (X_{t})\Big{)}\nabla_{\Theta}f_{\Theta}(X_{t}).\]

Robust TD learning is summarized in Algorithm 1.

``` Inputs: number of steps \(T\geq 1\), clipping radii \((b_{t})_{t\in[T]}\), projection radius \(\rho>0\), step-size \(\eta>0\)  Set \(\Theta(1)\in B_{2}(0,\rho)\)\(\backslash\)\(\backslash\)initialization for\(t=1,2,\dots,T\)do \(\Theta(t+1)=\Pi_{B_{2}(0,\rho)}\Big{\{}\Theta(t)+\eta_{t}\cdot g_{t}\big{(}\Theta(t )\big{)}\cdot 1\{\|g_{t}\big{(}\Theta(t)\big{)}\|_{2}\leq b_{t}\}\Big{\}}\) endfor Output:\(f_{\Theta(T)}(\cdot)=\big{\langle}\bar{\Theta}(T),\Phi(\cdot)\big{\rangle}\) where \(\bar{\Theta}(T)=\frac{1}{T}\sum_{t=1}^{T}\Theta(t)\) ```

**Algorithm 1**Robust TD learning

In the following, we will establish finite-time bounds for Robust TD learning by specifying the sequence of dynamic gradient clipping radii \((b_{t})_{t\geq 1}\), projection radius \(\rho\) and step-size \(\eta\).

### Finite-Time Bounds for Robust TD Learning

We make the following assumptions on the Markov reward process.

**Assumption 1**.: The stochastic process \((X_{t},R_{t})_{t\in\mathbb{N}}\) satisfies the following:

1. Ergodicity: \((X_{t})_{t\in\mathbb{N}}\) is an irreducible and aperiodic Markov chain with stationary distribution \(\mu=\mu\mathcal{P}\). Also, we assume that there are constants \(m>0,\zeta\in(0,1)\) such that \[\max_{x\in\mathbb{X}}\|\mathcal{P}^{t}(x,\cdot)-\mu\|_{\mathsf{TV}}\leq m\zeta^ {t},\;\forall t\in\mathbb{Z}_{+}.\] (3)
2. Heavy-tailed reward: For some \(p\in(0,1]\) and constant \(u_{0}\in(0,\infty)\), \[\mathbb{E}[|R_{t}(X_{t})|^{1+p}|X_{t}]\leq u_{0}<\infty,\;a.s.,\forall t\in \mathbb{N}.\] (4)
3. Mean reward: For any \(t\in\mathbb{N}\), \(\mathbb{E}[R_{t}(X_{t})|X_{t}]=r(X_{t})\in[-1,1]\) a.s.

We note that the uniform ergodicity and bounded mean reward assumptions are standard in TD learning literature [3, 54, 4].

**Assumption 2** (Sampling).: We consider two types of sampling strategies in this work:

_2a. IID sampling:_\(X_{t}\stackrel{{ iid}}{{\sim}}\mu\) and \(X^{\prime}_{t}\sim\mathcal{P}(X_{t},\cdot)\) for all \(t\geq 1\).

_2b. Markovian sampling:_\(X_{1}\sim\mu\) and \(X^{\prime}_{t}=X_{t+1}\sim\mathcal{P}(X_{t},\cdot)\) for all \(t\geq 1\).

**Assumption 3** (Realizability).: There exists \(\Theta^{\star}\in B_{2}(0,\rho)\) such that \(\mathcal{V}(\cdot)=\langle\Theta^{\star},\Phi(\cdot)\rangle\).

**Remark 1**.: We note that Assumption 3 holds directly in interesting realizable problem classes, e.g., linear MDPs [25], and allows us to obtain results on the statistical error performance of our design. In cases when it does not hold, our results will continue to hold with an additional function approximation error proportional to \(\inf_{\Theta\in B_{2}(0,\rho)}\sqrt{\mathbb{E}[|\mathcal{V}(x)-\langle\Theta, \Phi(x)\rangle|^{2}]}\), which is unavoidable due to the limitation of the linear function approximation architecture.

The following lemma is important in bounding the moments of the gradient norm under Robust TD learning in terms of the projection radius \(\rho>0\) and the upper bound \(u_{0}\) on \(\mathbb{E}\big{[}|R_{t}(X_{t})|^{1+p}\big{|}X_{t}\big{]}\).

**Lemma 1** (Tail bounds for \(\|g_{t}\big{(}\Theta(t)\big{)}\|_{2}\)).: Let \(\mathcal{F}^{+}_{t}=\sigma\big{(}\Theta(1),\Theta(2),\ldots,\Theta(t),X_{t} \big{)}\) for \(t\in\mathbb{Z}_{+}\). Then, under Assumption 1, we have:

\[\mathbb{E}[\|g_{t}(\Theta(t))\|_{2}^{1+p}|\mathcal{F}^{+}_{t}]\leq u<\infty, \ a.s.,\] (5)

for any \(t\in\mathbb{Z}_{+}\), where \(u=\min\{(u_{0}^{\frac{1}{1+p}}+2\rho)^{1+p},u_{0}+2^{2p+3}\rho^{1+p}\}\).

Proof.: Note that we have \(\mathbb{E}[\|g_{t}(\Theta(t))\|_{2}^{1+p}|\mathcal{F}^{+}_{t}]\leq\mathbb{E}[ |R_{t}(X_{t})+\gamma f_{\Theta(t)}(X^{\prime}_{t})-f_{\Theta(t)}(X_{t})|^{1+p }|\mathcal{F}^{+}_{t}]\) since \(\sup_{x\in\mathbb{X}}\|\Phi(x)\|_{2}\leq 1\). The upper bounds then follow by applying Minkowski's inequality and the triangle inequality for \(L^{p}\) spaces, respectively, to this inequality. 

This lemma will be useful in the analysis of both the expected (Theorems 1-2), and the high-probability bounds (Theorem 3) on the performance of Robust TD learning.

Next, we provide the main theoretical results in this paper: finite-time bounds for Robust TD learning. The proofs are mainly deferred to the appendix, while we provide a proof sketch for Theorem 3. In the following, we provide convergence bounds for the expected mean squared error under Robust TD learning with various choices of \(b_{t}\).

**Theorem 1** (Expected error under Robust TD learning - iid sampling).: Under Assumptions 1, 2a, 3, we have the following bounds for Robust TD learning:

**a)** For \(b_{t}=(ut)^{\frac{1}{1+p}}\) for any \(t\in\mathbb{Z}_{+}\) and \(\eta_{t}=\eta=\frac{2\rho(1-\gamma)}{(uT)^{\frac{1}{1+p}}}\), we have:

\[\mathop{\mathbb{E}}_{\begin{subarray}{c}\Theta(1),\Theta(2),\ldots,\Theta(T) \\ x\sim\mu\end{subarray}}\hskip-1.422638pt\Big{[}\Big{(}\mathcal{V}(x)-\langle \bar{\Theta}(T),\Phi(x)\rangle\Big{)}^{2}\Big{]}\leq\frac{6\rho u^{\frac{1}{1+p }}}{(1-\gamma)T^{\frac{p}{1+p}}},\ \forall T>1.\] (6)

**b)** Let \(\Lambda=\sum_{x\in\mathbb{X}}\mu(x)\Phi(x)\Phi^{\top}(x)\), and \(\mathfrak{C}_{p}(u,\lambda_{\text{min}},\gamma,\rho)=\frac{u}{1-\gamma}\Big{(} 4\rho+\frac{1}{(1-\gamma)\lambda_{\text{min}}}\Big{)}\). If \(\lambda_{\text{min}}(\Lambda)=\lambda_{\text{min}}>0\), then with the diminishing step-size \(\eta_{t}=\frac{1}{(1-\gamma)t\lambda_{\text{min}}}\) and \(b_{t}=t\) for \(t\in\mathbb{Z}_{+}\), for the average iterate \(\bar{\Theta}(T)\), we have2:

Footnote 2: The upper bound is \(\mathfrak{C}_{p}(u,\lambda_{\text{min}},\gamma,\rho)\frac{1}{T}\sum_{t=1}^{T} t^{-p}=\widetilde{\mathcal{O}}(T^{-p})\), which is further upper bounded as (6) and (7) by using integral bounds.

\[\mathop{\mathbb{E}}_{\begin{subarray}{c}\Theta(1),\Theta(2),\ldots,\Theta(T) \\ x\sim\mu\end{subarray}}\hskip-1.422638pt\Big{(}\mathcal{V}(x)-\langle\bar{ \Theta}(T),\Phi(x)\rangle\Big{)}^{2}\leq\mathfrak{C}_{p}(u,\lambda_{\text{min}},\gamma,\rho)\Bigg{[}\frac{\mathbf{1}_{p,1}T^{-p}}{1-p}+\frac{(1-\mathbf{1}_{p,1})\log(eT)}{T}\Bigg{]},\] (7)

and for the last iterate \(\Theta(T+1)\), we have:

\[\mathop{\mathbb{E}}_{\Theta(1),\ldots,\Theta(T)}\max_{x\in\mathbb{X}}|\mathcal{ V}(x)-\langle\Phi(x),\Theta(T+1)\rangle|^{2}\leq\frac{\mathfrak{C}_{p}(u, \lambda_{\text{min}},\gamma,\rho)}{\lambda_{\text{min}}}\Bigg{[}\frac{\log(eT) (1-\mathbf{1}_{p,1})}{T}+\frac{\mathbf{1}_{p,1}T^{-p}}{1-p}\Bigg{]},\] (8)

for any \(T>1\), where \(\mathbf{1}_{x,y}=1\) if \(x\neq y\) and 0 otherwise.

**Remark 2**.: The convergence rates in Theorem 1 are \(\mathcal{O}(T^{-\frac{p}{1+p}})\) and \(\tilde{\mathcal{O}}(T^{-p})\) without and with the full-rank assumption \(\lambda_{\text{min}}>0\), respectively. For \(p=1\), the convergence rates stated in Theorem 1 both match the existing results for TD learning with bounded rewards [4], up to a larger scaling factor of raw second-order moment rather than variance, due to clipping centered around 0.

**Remark 3** (Finite-time bounds in the unrealizable case).: We note that our results hold without Assumption 3 as well. In this general case, there will be an additional function approximation error proportional to \(\inf_{\Theta\in B_{2}(0,\rho)}\sqrt{\mathbb{E}[|\mathcal{V}(x)-\langle\Theta, \Phi(x)\rangle|^{2}]}\) (see Remark 1). One would use a richer function approximation scheme (e.g., larger projection radius \(\rho\) or dimension \(d\)) to reduce this function approximation error, which will lead to an increase in the statistical error as we characterize in our bounds. For the extension of our analysis to the general case without Assumption 3, please see Appendix A.1.

In the following, we provide convergence bounds for Robust TD learning under Markovian sampling.

**Theorem 2** (Expected error under Robust TD learning - Markovian sampling).: Under Assumptions 1,2b and 3, let \(T>1\), \(\rho>0\) be given, and define the mixing time \(\tau=\min\{t\in\mathbb{Z}_{+}:m\zeta^{t}\leq\sqrt{2}\rho(uT)^{-\frac{1}{1+p}}\}\). Then, with \(\eta_{t}=\eta=\sqrt{2}\rho(uT)^{-\frac{1}{1+p}}\), Robust TD learning yields the following:

\[\underset{\begin{subarray}{c}\Theta(1),\Theta(2),\cdots,\Theta(T)\\ x\sim\mu\end{subarray}}{\mathbb{E}}\Big{(}\mathcal{V}(x)-\langle\bar{\Theta}( T),\Phi(x)\rangle\Big{)}^{2}\leq\frac{7\rho u^{\frac{1}{1+p}}}{(1-\gamma)T^{\frac{p}{1+p}}}+ \frac{2\sqrt{2}\rho(1+2\rho)(4\rho+\tau(1+6\rho))}{(1-\gamma)T^{\frac{1}{1+p}}}.\] (9)

The proof of Theorem 2 is based on a similar Lyapunov technique as Theorem 1 in conjunction with the mixing time analysis in [4] for Markovian sampling, and can be found in Appendix A.

The bounds in Theorem 1 involve expectation over the parameters \(\Theta(t),t\in[T]\). In the following, we provide a high-probability error bound on the mean squared error under Robust TD learning.

**Theorem 3** (High-probability bound for Robust TD learning).: _For any \(\delta\in(0,1)\), let \(L_{\delta}=\log(4/\delta)\). Under Assumptions 1, 2a, 3, with step-size \(\eta=\frac{\sqrt{2}(1-\gamma)\rho L_{\delta}^{\frac{1}{1+p}}}{(uT)^{\frac{1}{1+ p}}}\) and clipping radius \(b_{t}=\Big{(}\frac{u}{L_{\delta}}\Big{)}^{\frac{1}{1+p}}\),_

\[\sum_{x\in\mathbb{X}}\mu(x)\Big{(}\mathcal{V}(x)-\big{\langle}\bar{\Theta}(T ),\Phi(x)\big{\rangle}\Big{)}^{2}\leq\frac{\rho u^{\frac{1}{1+p}}}{(1-\gamma) T^{\frac{p}{1+p}}}\Big{(}3L_{\delta}^{-\frac{1-p}{2(1+p)}}+7L_{\delta}^{\frac{ 1}{1+p}}\Big{)},\] (10)

_holds with probability at least \(1-\delta\)._

In the following, we give a proof sketch for Theorem 3. The full proof can be found in Appendix A.

Proof sketch.: Let \(\mathcal{L}(\Theta)=\|\Theta-\Theta^{\star}\|_{2}^{2}\) be the Lyapunov function, and \(\chi_{t}=1-\bar{\chi}_{t}=\mathbbm{1}\{\|g_{t}\|_{2}\leq b_{t}\}\). Then, the Lyapunov drift can be decomposed as follows:

\[\mathcal{L}(\Theta(t+1))-\mathcal{L}(\Theta(t))\leq 2\eta\mathbb{E}_{t}[g_{t}^{ \top}(\Theta(t)-\Theta^{\star})]+\eta^{2}\mathbb{E}_{t}[\|g_{t}\|_{2}^{2}\chi _{t}]+2\eta B(t)+\eta^{2}Z(t),\] (11)

where

\[B(t)=g_{t}^{\top}(\Theta(t)-\Theta^{\star})\chi_{t}-\mathbb{E}_{t}[g_{t}^{ \top}(\Theta(t)-\Theta^{\star})\chi_{t}]-\mathbb{E}_{t}[g_{t}^{\top}(\Theta(t)- \Theta^{\star})\bar{\chi}_{t}],\]

is the bias in the stochastic semi-gradient, and

\[Z(t)=\|g_{t}\|_{2}^{2}\chi_{t}-\mathbb{E}_{t}[\|g_{t}\|_{2}^{2}\chi_{t}].\]

We can decompose \(B(t)\) further into a martingale difference sequence

\[B_{0}(t)=g_{t}^{\top}(\Theta(t)-\Theta^{\star})\chi_{t}-\mathbb{E}_{t}[g_{t}^ {\top}(\Theta(t)-\Theta^{\star})\chi_{t}],\]

and a bias term

\[B_{\perp}(t)=-\mathbb{E}_{t}[g_{t}^{\top}(\Theta(t)-\Theta^{\star})\bar{\chi}_ {t}].\]

By Freedman's inequality for martingales [12, 53], we have \(\frac{1}{T}\sum_{t=1}^{T}B_{0}(t)\leq\frac{7\rho u^{\frac{1}{1+p}}L_{\delta}^{ \frac{p}{1+p}}}{T^{\frac{1}{1+p}}}\), and by Azuma inequality, we have \(\frac{1}{T}\sum_{t=1}^{t}Z(t)\leq\frac{u^{\frac{2}{1+p}}T^{\frac{1-p}{1+p}}}{L_{ \delta}^{\frac{1+p}{1+p}}}\), each holding with probability at least \(1-\delta/2\).

By Holder's inequality and Lemma 1, we can bound \(B_{\perp}(t)\leq ub_{t}^{-p}\) and \(\mathbb{E}_{t}[\|g_{t}\|_{2}^{2}\bar{\chi}_{t}]\leq ub_{t}^{1-p}\), both with probability 1. Finally, by Lemma 2 in [54], we have the negative drift term

\[\mathbb{E}_{t}[g_{t}^{\top}(\Theta(t)-\Theta^{*})]\leq-(1-\gamma)\sum_{x}\mu(x) (f_{\Theta(t)}(x)-\mathcal{V}(x))^{2}.\]

By telescoping sum of (11) and rearranging the terms, we have:

\[\frac{1}{T}\sum_{t=1}^{T}\|f_{\Theta(t)}-\mathcal{V}\|_{\mu}^{2}\leq\frac{ \mathcal{L}(\Theta(1))}{2\eta(1-\gamma)T}+\frac{1}{(1-\gamma)T}\sum_{t=1}^{T} B(t)+\frac{\eta}{2(1-\gamma)T}\sum_{t=1}^{T}\Big{(}Z(t)+\mathbb{E}_{t}[\|g_{t}\|_{2}^{2} \bar{\chi}_{t}]\Big{)}.\]

The proof is concluded by substituting the above high probability bounds on the sample means of \(B(t)\), \(Z(t)\) and \(\mathbb{E}_{t}[\|g_{t}\|_{2}^{2}\bar{\chi}_{t}]\) (via union bound and integral upper bounds), and using Jensen's inequality on the left side of the above inequality. 

Most notably, this important theorem establishes that, by appropriately controlling the bias term of dynamic gradient clipping to yield a vanishing sample mean with high probability as the number of iterations increases, one can limit the variance of the semi-gradient, thereby resulting in the provided global near-optimality guarantee.

## 3 Robust Natural Actor-Critic for Policy Optimization under Heavy Tails

In this section, we will study a two-timescale robust natural actor-critic algorithm (Robust NAC, in short) based on Robust TD learning, and provide finite-time bounds.

### Policy Optimization Problem

We consider a discounted-reward Markov decision process (MDP) with a finite but arbitrarily large state space \(\mathbb{S}\), finite action space \(\mathbb{A}\), transition kernel \(\mathcal{P}\) and discount factor \(\gamma\in(0,1)\). The controlled Markov chain \(\{(S_{t},A_{t})\in\mathbb{S}\times\mathbb{A}:t\in\mathbb{N}\}\) has the probability transition dynamics \(\mathbb{P}(S_{t+1}\in s|S_{1}^{t},A_{1}^{t})=\mathcal{P}_{A_{t}}(S_{t},s),\) for any \(s\in\mathbb{S}\). Taking the action \(A_{t}\in\mathbb{A}\) at state \(S_{t}\in\mathbb{S}\) yields a random reward of \(R_{t}(S_{t},A_{t})\) at any \(t\in\mathbb{Z}_{+}\). For a given stationary randomized policy \(\pi=(\pi(a|s))_{(s,a)\in\mathbb{S}\times\mathbb{A}}\), the value function \(\mathcal{V}^{\pi}\) and the state-action value function (also known as Q-function) \(\mathcal{Q}^{\pi}\) are defined as:

\[\mathcal{V}^{\pi}(s) =\mathbb{E}^{\pi}\Big{[}\sum_{t=1}^{\infty}\gamma^{t-1}R_{t}(S_{ t},A_{t})\Big{|}S_{1}=s\Big{]},\;s\in\mathbb{S}\] (12) \[\mathcal{Q}^{\pi}(s,a) =\mathbb{E}^{\pi}\Big{[}\sum_{t=1}^{\infty}\gamma^{t-1}R_{t}(S_{ t},A_{t})\Big{|}S_{1}=s,A_{1}=a\Big{]},\;(s,a)\in\mathbb{S}\times\mathbb{A}.\] (13)

**Remark 4** (From MDP to MRP).: Under any stationary randomized policy \(\pi\), the process \((S_{t},A_{t})_{t>0}=:(X_{t})_{t>0}\) is a Markov chain over the state-space \(\mathbb{X}=\mathbb{S}\times\mathbb{A}\), thus \((X_{t},R_{t})\) with \(R_{t}(X_{t})=R_{t}(S_{t},A_{t})\) is a Markov reward process of the kind that we analyzed in Section 2. As such, we can use Robust TD learning to evaluate \(\mathcal{V}(x)=\mathcal{Q}^{\pi}(x)\) for any \(x=(s,a)\in\mathbb{S}\times\mathbb{A}\).

**Heavy-tailed reward.** We assume that the process \((X_{t},R_{t})_{t>0}\) with the Markov chain \(X_{t}=(S_{t},A_{t})\) and the reward \(R_{t}=R_{t}(X_{t})\) satisfies Assumption 1. We denote the stationary distribution of \(X_{t}=(S_{t},A_{t})\) under \(\pi\) as \(\mu^{\pi}\).

**Objective.** For an initial state distribution \(\lambda\), the objective in this work is to find the following:

\[\pi^{*}\in\arg\max_{\pi}\ \int_{\mathbb{S}}\mathcal{V}^{\pi}(s)\lambda(ds)=: \mathcal{V}^{\pi}(\lambda),\] (14)

over the class of stationary randomized policies.

**Policy parameterization.** In this work, we consider a finite but arbitrarily large state space \(\mathbb{S}\), and for such problems, the tabular methods do not scale [51; 3]. In order to address this scalability issue, we consider widely-used softmax parameterization with linear function approximation: for a given set of feature vectors \(\{\Phi(s,a)\in\mathbb{R}^{d}:s\in\mathbb{S},a\in\mathbb{A}\}\) and policy parameter \(W\in\mathbb{R}^{d}\),

\[\pi_{W}(a|s)=\frac{\exp(W^{\top}\Phi(s,a))}{\sum_{a^{\prime}\in\mathbb{A}}\exp (W^{\top}\Phi(s,a^{\prime}))},\;(s,a)\in\mathbb{S}\times\mathbb{A}.\] (15)

In the following subsection, we will describe the robust natural actor-critic algorithm.

### Robust Natural Actor-Critic Algorithm

For any iteration \(k\geq 1\), we denote \(\pi_{k}:=\pi_{W(k)}\) throughout the policy optimization iterations.

For samples \(\mathcal{D}^{(k)}=\{(S_{t,k},A_{t,k},R_{t,k},S^{\prime}_{t,k},A^{\prime}_{t,k}):t \geq 1\}\), given \((b_{t,k})_{t,k\in\mathbb{Z}_{+}}\) and \(\rho>0\), Robust NAC Algorithm is summarized in Algorithm 2.

``` Inputs: clipping radii \((b_{t})_{t\geq 1}\), projection radius \(\rho>0\), learning rate \(\alpha>0\), \(L_{\delta}>0\) for\(k=1,2,\ldots,K\)do  Set \(\Theta_{k}(1)=0\)//initialization: max-entropy policy for\(t=1,2,\ldots,T\)do  Set \(g^{(k)}_{t}(\Theta_{k}(t))=\Big{(}R_{t,k}+\gamma f_{\Theta_{k}(t)}(S^{\prime}_{ t,k},A^{\prime}_{t,k})-f_{\Theta_{k}(t)}(S_{t,k},A_{t,k})\Big{)}\Phi(S_{t,k},A_{t,k})\). \(\Theta_{k}(t+1)=\Pi_{B_{2}(0,\rho)}\Big{\{}\Theta_{k}(t)+\eta_{t}\cdot g^{(k) }_{t}\big{(}\Theta_{k}(t)\big{)}\cdot\mathbf{1}\{\|g^{(k)}_{t}\big{(}\Theta_{ k}(t)\big{)}\|_{2}\leq b_{t}\}\Big{\}}\) endfor \(W(k+1)=W(k)+\alpha\cdot\frac{1}{T}\sum_{t=1}^{T}\Theta_{k}(t)\) endfor ```

**Algorithm 2**Robust Natural Actor-Critic Algorithm

**Remark 5**.: The optimal solution \(\Theta_{k}^{*}\in\operatorname*{arg\,min}_{\Theta\in\mathbb{R}^{d}}\ \ \mathbb{E}_{x=(s,a)}\Big{|} \langle\Theta,\Phi(x)\rangle-\mathcal{Q}^{\pi_{k}}(x)\Big{|}^{2}\) is a good approximation of the natural policy gradient:

\[u_{k}=[G(\pi_{k})]^{-1}\nabla_{W}\mathcal{V}^{\pi_{k}}(\lambda)\in \operatorname*{arg\,min}_{w\in\mathbb{R}^{d}}\ \ \mathbb{E}_{(s,a)\sim\mathbf{d}_{\lambda}^{\pi_{k}}\otimes\pi_{k}(\cdot|s)} \Big{|}\langle w,\nabla_{W}\log\pi_{k}(a|s)\rangle-\mathcal{A}^{\pi_{k}}(s,a) \Big{|}^{2},\]

which follows from Jensen's inequality and leads to the Q-NPG [1]. For a detailed discussion, refer to Appendix B.

### Finite-Time Bounds for Robust Natural Actor-Critic

In this subsection, we will provide finite-time bounds for Robust NAC.

We assume that the resulting Markov reward process under \(\pi_{k}\) for each \(k\) satisfies Assumptions 1-3 with stationary distribution \(\mu^{\pi_{k}}\) and \(u_{k}\geq\mathbb{E}[|R_{t,k}(S_{t,k},A_{t,k})|^{1+p}|S_{t,k},A_{t,k}]\). We assume that the dataset \(\mathcal{D}^{(k)}\) is obtained independently at each iteration \(k\geq 1\) for simplicity, with \((S_{t,k},A_{t,k})\overset{iid}{\sim}\mu^{\pi_{k}}\) and \(S^{\prime}_{t,k}\sim\mathcal{P}_{A_{t,k}}(S_{t,k},\cdot)\) and \(A_{t,k}\sim\pi_{k}(\cdot|S_{t,k})\) according to Assumption 2a under the stationary distribution \(\mu^{\pi_{k}}=[\mu^{\pi_{k}}(s,a)]_{s\in S,a\in\mathbb{A}}\) under \(\pi_{k}\). We make the following standard assumption for policy optimization, which is common in the policy gradient literature [1, 57, 34].

**Assumption 4** (Concentrability).: _For any \(k\geq 1\), we assume that there exists \(C_{\mathsf{conc}}<\infty\) such that:_

\[\max_{(s,a)\in\mathbb{S}\times\mathbb{A}}\frac{\mathbf{d}_{\lambda}^{\pi^{*}}( s)\pi^{*}(a|s)}{\mu^{\pi_{k}}(s,a)}\leq C_{\mathsf{conc}},\] (16)

_where \(\mu^{\pi_{k}}\) is the stationary distribution of \((S_{t,k},A_{t,k})_{t\geq 1}\) under \(\pi_{k}\)._

**Theorem 4** (Finite-time bounds for Robust NAC).: Under Assumptions 1-4 for any \(k\geq 1\), for any \(\delta\in(0,1)\) and \(T,K>1\), Robust NAC with \(\rho\geq\max_{k}\|\Theta_{k}^{*}\|_{2}\), \(b_{t,k}=\Big{(}\frac{u_{k}T}{\log(4T/\delta)}\Big{)}^{\frac{1}{1+p}}\), learning rates \(\eta=\frac{\sqrt{2}(1-\gamma)\rho L_{\delta}^{\frac{1-p}{2(1+p)}}}{(\max_{1 \leq k\leq K}u_{k}T)^{\frac{1}{1+p}}}\) and \(\alpha=\frac{\sqrt{\log|A|}}{\rho\sqrt{K}}\) achieves the following with probability at least \(1-\delta\):

\[\min_{1\leq k\leq K}\{\mathcal{V}^{\pi^{*}}(\lambda)-\mathcal{V}^{\pi_{k}}( \lambda)\}\leq\frac{2\rho\sqrt{\log|\mathbb{A}|}}{(1-\gamma)\sqrt{K}}+\sqrt{ \frac{(\max_{1\leq k\leq K}u_{k})^{\frac{1}{1+p}}C_{\mathsf{conc}}\rho}{(1- \gamma)^{3}T^{\frac{p}{1+p}}}}\Big{(}3L_{\delta}^{-\frac{1-p}{2(1+p)}}+7L_{ \delta}^{\frac{p}{1+p}}\Big{)},\]

where \(L_{\delta}=\log(4T/\delta)\).

[MISSING_PAGE_FAIL:9]

## 5 Conclusion

In this paper, we considered RL problem with heavy-tailed rewards, and proposed robust TD learning and NAC variants with a dynamic gradient clipping mechanism with provable performance guarantees, both in expectation and with high probability. Motivated by the results in this work, it would be interesting to explore single-timescale robust NAC and off-policy NAC for future work.

## Acknowledgments and Disclosure of Funding

Atilla Eryilmaz's research was supported in part by NSF AI Institute (AI-EDGE) 2112471, CNS-NeTS-2106679, CNS-NeTS-2007231; and the ONR Grant N00014-19-1-2621.

## References

* Agarwal et al. [2020] Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. Optimality and approximation with policy gradient methods in markov decision processes. In _Conference on Learning Theory_, pages 64-66. PMLR, 2020.
* Agarwal et al. [2008] Mukul Agarwal, Vivek S Borkar, and Abhay Karandikar. Structural properties of optimal transmission policies over a randomly varying channel. _IEEE Transactions on Automatic Control_, 53(6):1476-1491, 2008.
* Bertsekas and Tsitsiklis [1996] Dimitri P Bertsekas and John N Tsitsiklis. _Neuro-dynamic programming_. Athena Scientific, 1996.
* Bhandari et al. [2018] Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference learning with linear function approximation. _arXiv preprint arXiv:1806.02450_, 2018.
* Bhatnagar et al. [2009] Shalabh Bhatnagar, Richard S Sutton, Mohammad Ghavamzadeh, and Mark Lee. Natural actor-critic algorithms. _Automatica_, 45(11):2471-2482, 2009.
* Bubeck et al. [2013] Sebastien Bubeck, Nicolo Cesa-Bianchi, and Gabor Lugosi. Bandits with heavy tail. _IEEE Transactions on Information Theory_, 59(11):7711-7717, 2013.
* Cai et al. [2019] Qi Cai, Zhuoran Yang, Jason D Lee, and Zhaoran Wang. Neural temporal-difference learning converges to global optima. _Advances in Neural Information Processing Systems_, 32, 2019.
* Cayci et al. [2020] Semih Cayci, Atilla Eryilmaz, and Rayadurgam Srikant. Budget-constrained bandits over general cost and reward distributions. In _International Conference on Artificial Intelligence and Statistics_, pages 4388-4398. PMLR, 2020.
* Cayci et al. [2021] Semih Cayci, Niao He, and Rayadurgam Srikant. Linear convergence of entropy-regularized natural policy gradient with linear function approximation. _arXiv preprint arXiv:2106.04096_, 2021.

Figure 3: Performances of Robust TD learning and TD learning for the circular random walk under heavy-tailed reward with tail index 1.2. Each faded green line is the error trajectory for an individual trial, and the solid lines indicate the expected errors for TD learning and Robust TD learning.

* [10] Rong-Rong Chen and Sean Meyn. Value iteration and optimization of multiclass queueing networks. _Queueing Systems_, 32:65-97, 1999.
* [11] Ashok Cutkosky and Harsh Mehta. High-probability bounds for non-convex stochastic optimization with heavy tails. _Advances in Neural Information Processing Systems_, 34:4883-4895, 2021.
* [12] David A Freedman. On tail probabilities for martingales. _the Annals of Probability_, pages 100-118, 1975.
* [13] Carla P Gomes and Bart Selman. Algorithm portfolios. _Artificial Intelligence_, 126(1-2):43-62, 2001.
* [14] Carla P Gomes, Bart Selman, Nuno Crato, and Henry Kautz. Heavy-tailed phenomena in satisfiability and constraint satisfaction problems. _Journal of automated reasoning_, 24(1-2):67, 2000.
* [15] Carla P Gomes, Bart Selman, Henry Kautz, et al. Boosting combinatorial search through randomization. _AAAI/IAAI_, 98:431-437, 1998.
* [16] Mert Gurbuzbalaban, Umut Simsekli, and Lingjiong Zhu. The heavy-tail phenomenon in sgd. In _International Conference on Machine Learning_, pages 3964-3975. PMLR, 2021.
* [17] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In _International Conference on Machine Learning_, pages 1861-1870. PMLR, 2018.
* [18] Mor Harchol-Balter. The effect of heavy-tailed job size distributions on computer system design. In _Proc. of ASA-IMS Conf. on Applications of Heavy Tailed Distributions in Economics, Engineering and Statistics_, 1999.
* [19] Holger H Hoos and Thomas Stutzle. _Stochastic local search: Foundations and applications_. Elsevier, 2004.
* [20] Daniel Hsu and Sivan Sabato. Heavy-tailed regression with a generalized median-of-means. In _International Conference on Machine Learning_, pages 37-45. PMLR, 2014.
* [21] Yu-Pin Hsu, Eytan Modiano, and Lingjie Duan. Scheduling algorithms for minimizing age of information in wireless broadcast networks with random arrivals. _IEEE Transactions on Mobile Computing_, 19(12):2903-2915, 2019.
* [22] Marat Ibragimov, Rustam Ibragimov, and Johan Walden. _Heavy-tailed distributions and robustness in economics and finance_, volume 214. Springer, 2015.
* [23] Predrag R Jelenkovic and Jian Tan. Is aloha causing power law delays? In _Managing Traffic Performance in Converged Networks: 20th International Teletraffic Congress, ITC20 2007, Ottawa, Canada, June 17-21, 2007. Proceedings_, pages 1149-1160. Springer, 2007.
* [24] Predrag R Jelenkovic and Jian Tan. Characterizing heavy-tailed distributions induced by retransmissions. _Advances in Applied Probability_, 45(1):106-138, 2013.
* [25] Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In _Conference on Learning Theory_, pages 2137-2143. PMLR, 2020.
* [26] Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In _In Proc. 19th International Conference on Machine Learning_. Citeseer, 2002.
* [27] Sham M Kakade. A natural policy gradient. _Advances in neural information processing systems_, 14, 2001.
* [28] Yuan Ke, Stanislav Minsker, Zhao Ren, Qiang Sun, and Wen-Xin Zhou. User-friendly covariance estimation for heavy-tailed distributions. _Statistical Science_, 34(3):454-471, 2019.

* [29] Byungchan Kim, Jooyoung Park, Shinsuk Park, and Sungchul Kang. Impedance learning for robotic contact tasks using natural actor-critic algorithm. _IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)_, 40(2):433-443, 2009.
* [30] Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In _Advances in neural information processing systems_, pages 1008-1014. Citeseer, 2000.
* [31] Lars Kotthoff. Algorithm selection for combinatorial search problems: A survey. _Data mining and constraint programming: Foundations of a cross-disciplinary approach_, pages 149-190, 2016.
* [32] Michail G Lagoudakis, Michael L Littman, et al. Algorithm selection using reinforcement learning. In _ICML_, pages 511-518, 2000.
* [33] Kyungjae Lee, Hongjun Yang, Sungbin Lim, and Songhwai Oh. Optimal algorithms for stochastic multi-armed bandits with heavy tailed rewards. _Advances in Neural Information Processing Systems_, 33:8452-8462, 2020.
* [34] Boyi Liu, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural proximal/trust region policy optimization attains globally optimal policy. _arXiv preprint arXiv:1906.10306_, 2019.
* [35] Mico Loretan and Peter CB Phillips. Testing the covariance stationarity of heavy-tailed time series: An overview of the theory with applications to several financial datasets. _Journal of empirical finance_, 1(2):211-248, 1994.
* [36] Gabor Lugosi and Shahar Mendelson. Mean estimation and regression under heavy-tailed distributions: A survey. _Foundations of Computational Mathematics_, 19(5):1145-1190, 2019.
* [37] Benoit Mandelbrot. New methods in statistical economics. _Journal of political economy_, 71(5):421-440, 1963.
* [38] Mark M Meerschaert and Hans-Peter Scheffler. Moving averages of random vectors with regularly varying tails. _Journal of Time Series Analysis_, 21(3):297-328, 2000.
* [39] Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. On the global convergence rates of softmax policy gradient methods. In _International Conference on Machine Learning_, pages 6820-6829. PMLR, 2020.
* [40] Sean Meyn. _Control techniques for complex networks_. Cambridge University Press, 2008.
* [41] Stanislav Minsker. Geometric median and robust estimation in banach spaces. _Bernoulli_, pages 2308-2335, 2015.
* [42] Stanislav Minsker. Sub-gaussian estimators of the mean of a random matrix with heavy-tailed entries. _The Annals of Statistics_, 46(6A):2871-2903, 2018.
* [43] Rajeev Motwani and Prabhakar Raghavan. _Randomized algorithms_. Cambridge university press, 1995.
* [44] Jayakrishnan Nair, Adam Wierman, and Bert Zwart. The fundamentals of heavy-tails: Properties, emergence, and identification. In _Proceedings of the ACM SIGMETRICS/international conference on Measurement and modeling of computer systems_, pages 387-388, 2013.
* [45] Arkadij Semenovic Nemirovskij and David Borisovich Yudin. _Problem complexity and method efficiency in optimization_. Wiley-Interscience, 1983.
* [46] Jan Peters and Stefan Schaal. Natural actor-critic. _Neurocomputing_, 71(7-9):1180-1190, 2008.
* [47] Lior Shani, Yonathan Efroni, and Shie Mannor. Adaptive trust region policy optimization: Global convergence and faster rates for regularized mdps. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 5668-5675, 2020.
* [48] Han Shao, Xiaotian Yu, Irwin King, and Michael R Lyu. Almost optimal algorithms for linear stochastic bandits with heavy-tailed payoffs. _Advances in Neural Information Processing Systems_, 31, 2018.

* Streeter and Golovin [2008] Matthew Streeter and Daniel Golovin. An online algorithm for maximizing submodular functions. _Advances in Neural Information Processing Systems_, 21, 2008.
* Sutton [1988] Richard S Sutton. Learning to predict by the methods of temporal differences. _Machine learning_, 3(1):9-44, 1988.
* Sutton and Barto [2018] Richard S Sutton and Andrew G Barto. _Reinforcement learning: An introduction_. MIT press, 2018.
* Sutton et al. [1999] Richard S Sutton, David A McAllester, Satinder P Singh, Yishay Mansour, et al. Policy gradient methods for reinforcement learning with function approximation. In _NIPs_, volume 99, pages 1057-1063. Citeseer, 1999.
* Tropp [2011] Joel A Tropp. Freedman's inequality for matrix martingales. _Electronic Communications in Probability_, 16(25):262-270, 2011.
* Tsitsiklis and Van Roy [1997] John N Tsitsiklis and Benjamin Van Roy. An analysis of temporal-difference learning with function approximation. _IEEE transactions on automatic control_, 42(5):674-690, 1997.
* Wainwright [2019] Martin J Wainwright. _High-dimensional statistics: A non-asymptotic viewpoint_, volume 48. Cambridge university press, 2019.
* Wang et al. [2021] Hongjian Wang, Mert Gurbuzbalaban, Lingjiong Zhu, Umut Simsekli, and Murat A Erdogdu. Convergence rates of stochastic gradient descent under infinite noise variance. _Advances in Neural Information Processing Systems_, 34:18866-18877, 2021.
* Wang et al. [2019] Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural policy gradient methods: Global optimality and rates of convergence. _arXiv preprint arXiv:1909.01150_, 2019.
* Whitt [2000] Ward Whitt. The impact of a heavy-tailed service-time distribution upon the m/gi/s waiting-time distribution. _Queueing Systems_, 36(1-3):71-87, 2000.
* Williams [1992] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. _Machine learning_, 8(3-4):229-256, 1992.
* Xia and Wainwright [2023] Eric Xia and Martin Wainwright. Krylov-bellman boosting: Super-linear policy evaluation in general state spaces. In _International Conference on Artificial Intelligence and Statistics_, pages 9137-9166. PMLR, 2023.
* Xu et al. [2020] Tengyu Xu, Zhe Wang, and Yingbin Liang. Improving sample complexity bounds for (natural) actor-critic algorithms. _arXiv preprint arXiv:2004.12956_, 2020.
* Xu et al. [2020] Tengyu Xu, Zhe Wang, and Yingbin Liang. Non-asymptotic convergence analysis of two time-scale (natural) actor-critic algorithms. _arXiv preprint arXiv:2005.03557_, 2020.
* Yuan et al. [2022] Rui Yuan, Simon S Du, Robert M Gower, Alessandro Lazaric, and Lin Xiao. Linear convergence of natural policy gradient methods with log-linear policies. _arXiv preprint arXiv:2210.01400_, 2022.

Proofs for Robust TD Learning

The following lemma will be critical in our proofs.

**Lemma 2** (Lemma 4 in [54]).: _For any two vectors \(\widehat{V},V\in\mathbb{R}^{[\mathbb{X}]}\),_

\[\|\mathcal{T}\widehat{V}-\mathcal{T}V\|_{\mu}\leq\gamma\cdot\|\widehat{V}-V\|_{ \mu},\]

_where_

\[(\mathcal{T}V)(x)=r(x)+\gamma\sum_{x^{\prime}\in\mathbb{X}}\mathcal{P}(x,x^{ \prime})V(x^{\prime}),\] (17)

_is the Bellman operator._

Proof of Theorem.: _1._ The proof follows the Lyapunov approach in [4]. Let \(\mathcal{L}(\Theta)=\|\Theta-\Theta^{*}\|_{2}^{2}\) be the Lyapunov function for any \(\Theta\in\mathbb{R}^{d}\). Then, by the non-expansivity of \(\Pi_{\mathcal{B}_{2}(0,\rho)}\), we have:

\[\mathcal{L}(\Theta(t+1))\leq\mathcal{L}(\Theta(t))+\eta^{2}\|g_{ t}(\Theta(t))\|_{2}^{2}\mathds{1}\{\|g_{t}(\Theta(t))\|_{2}\leq b_{t}\}\\ -2\eta g_{t}(\Theta(t))^{\top}(\Theta(t)-\Theta^{*})\mathds{1}\{ \|g_{t}(\Theta(t))\|_{2}\leq b_{t}\}.\] (18)

Taking conditional expectation given \(\mathcal{F}_{t}\) and using the fact that \(\mathds{1}\{\|g_{t}(\Theta(t))\|_{2}>b_{t}\}=1-\mathds{1}\{\|g_{t}(\Theta(t)) \|_{2}\leq b_{t}\}\), we get:

\[\mathbb{E}[\mathcal{L}(\Theta(t+1))|\mathcal{F}_{t}]\leq\mathcal{ L}(\Theta(t))+2\eta\mathbb{E}_{t}[g_{t}^{\top}(\Theta(t))(\Theta(t)-\Theta^{ \star})]\\ -2\eta\mathbb{E}[g_{t}(\Theta(t))^{\top}(\Theta(t)-\Theta^{*}) \mathds{1}\{\|g_{t}(\Theta(t))\|_{2}>b_{t}\}|\mathcal{F}_{t}]+\eta^{2}ub_{t}^ {1-p},\] (19)

where we used

\[\mathbb{E}[\|g_{t}(\Theta(t))\|_{2}^{2}\mathds{1}\{\|g_{t}(\Theta (t))\|_{2}\leq b_{t}\}|\mathcal{F}_{t}] \leq\mathbb{E}[\|g_{t}(\Theta(t))\|_{2}^{1+p}b_{t}^{1-p}|\mathcal{ F}_{t}],\] (20) \[\leq ub_{t}^{1-p},\]

in the last term. Now, for \(\mathbb{E}_{t}[g_{t}^{\top}(\Theta(t))(\Theta(t)-\Theta^{\star})]\), we have the following inequality:

\[\mathbb{E}_{t}[g_{t}^{\top}(\Theta(t))(\Theta(t)-\Theta^{\star})] =\mathbb{E}_{t}[(R_{t}+\gamma f_{\Theta(t)}(X_{t}^{\prime})-f_{ \Theta(t)}(X_{t}))(f_{\Theta(t)}(X_{t})-\mathcal{V}(X_{t}))],\] \[=\mathbb{E}_{t}\Big{[}\Big{(}(\mathcal{T}f_{\Theta(t)})(X_{t})-f_ {\Theta(t)}(X_{t})\Big{)}\Big{(}f_{\Theta(t)}(X_{t})-\mathcal{V}(X_{t})\Big{)} \Big{]},\]

where \(\mathcal{T}\) is the Bellman operator (17). By using the fact that the value function \(\mathcal{V}\) is the fixed point of the Bellman operator \(\mathcal{T}\), we have the following:

\[\mathbb{E}_{t}[\Big{(}(\mathcal{T}f_{\Theta(t)})(X_{t})-f_{\Theta (t)}(X_{t})\Big{)}\Big{(}f_{\Theta(t)}(X_{t})-\mathcal{V}(X_{t})\Big{)}]\\ =\mathbb{E}_{t}\Big{[}\Big{(}\mathcal{T}f_{\Theta(t)}(X_{t})- \mathcal{T}\mathcal{V}(X_{t})\Big{)}\Big{(}f_{\Theta(t)}(X_{t})-\mathcal{V}(X _{t})\Big{)}\Big{]}-\mathbb{E}_{t}\Big{[}\Big{(}f_{\Theta(t)}(X_{t})-\mathcal{ V}(X_{t})\Big{)}^{2}\Big{]}.\] (21)

By using Lemma 2, we conclude that:

\[\mathbb{E}[g_{t}^{\top}(\Theta(t))(\Theta(t)-\Theta^{\star})]\leq-(1-\gamma) \sum_{x\in\mathbb{X}}\mu(x)\Big{(}f_{\Theta(t)}(x)-\mathcal{V}(x)\Big{)}^{2}=-( 1-\gamma)\|f_{\Theta(t)}-\mathcal{V}\|_{\mu}^{2}.\] (22)

Then, we can rewrite (19) as follows:

\[\mathbb{E}[\mathcal{L}(\Theta(t+1))|\mathcal{F}_{t}]\leq\mathcal{ L}(\Theta(t))-2(1-\gamma)\eta\|f_{\Theta(t)}-\mathcal{V}\|_{\mu}^{2}\\ -2\eta\mathbb{E}[g_{t}(\Theta(t))^{\top}(\Theta(t)-\Theta^{*}) \mathds{1}\{\|g_{t}(\Theta(t))\|_{2}>b_{t}\}|\mathcal{F}_{t}]+\eta^{2}ub_{t}^ {1-p},\] (23)

The bias introduced by using the gradient clipping can be bounded as follows:

\[\mathbb{E}[g_{t}(\Theta(t))^{\top}(\Theta(t)-\Theta^{*})\mathds{1 }\{\|g_{t}(\Theta(t))\|_{2}>b_{t}\}|\mathcal{F}_{t}]\\ \leq 2\rho\mathbb{E}[\|g_{t}(\Theta(t))\|_{2}\mathds{1}\{\|g_{t}( \Theta(t))\|_{2}>b_{t}\}|\mathcal{F}_{t}],\] (24)

which follows from Cauchy-Schwarz inequality, triangle inequality and the fact that \(\max\{\|\Theta(t)\|_{2},\|\Theta^{*}\|_{2}\}\leq\rho\) due to projection. Using Holder's inequality on the RHS of (24), we obtain:

\[\mathbb{E}[g_{t}(\Theta(t))^{\top}(\Theta(t)-\Theta^{*})\mathds{1}\{\|g_{t}( \Theta(t))\|_{2}>b_{t}\}|\mathcal{F}_{t}]\leq 2\rho u^{\frac{1}{1+p}}[\mathbb{P}(\|g_{t}( \Theta(t))\|_{2}>b_{t}|\mathcal{F}_{t})]^{\frac{p}{1+p}}.\]Using Markov's inequality, we bound the bias due to using the clipped stochastic gradient as:

\[\mathbb{E}[g_{t}(\Theta(t))^{\top}(\Theta(t)-\Theta^{*})\mathbbm{1}\{\|g_{t}( \Theta(t))\|_{2}>b_{t}\}|\mathcal{F}_{t}]\leq 2\rho ub_{t}^{-p}.\] (25)

Substituting (25) into (19), and taking expectation over the trajectory \(\mathcal{F}_{t}\), we obtain:

\[\mathbb{E}[\mathcal{L}(\Theta(t+1))-\mathcal{L}(\Theta(t))]\leq-2\eta(1-\gamma )\|f_{\Theta(t)}-\mathcal{V}\|_{\mu}^{2}+4\eta\rho ub_{t}^{-p}+\eta^{2}ub_{t}^ {1-p}.\]

Telescoping sum over \(t=1,2,\ldots,T\) yields:

\[\mathbb{E}\mathcal{L}(\Theta(T+1))-\mathcal{L}(\Theta(1))\leq-2 \eta(1-\gamma)\sum_{t=1}^{T}\Big{(}\mathbb{E}\|f_{\Theta(t)}-\mathcal{V}\|_{ \mu}^{2}\Big{)}\\ +4\eta\rho u\int_{0}^{T}b_{s}^{-p}ds+\eta^{2}u\int_{0}^{T}b_{s}^{ 1-p}ds.\] (26)

Rearranging the terms, using Jensen's inequality and \(\mathcal{L}(\Theta(1))\leq 4\rho^{2}\), and substituting the step-size \(\eta\) yields the result.

(b) For the full-rank case, note that

\[\|f_{\Theta}-\mathcal{V}\|_{\mu}^{2} =(\Theta-\Theta^{*})^{\top}\Big{(}\sum_{x\in\mathbb{X}}\mu(x) \Phi(x)\Phi^{\top}(x)\Big{)}(\Theta-\Theta^{*}),\] \[\geq\lambda_{\textsf{min}}\|\Theta-\Theta^{*}\|_{2}^{2},\]

which implies (together with (23)) that:

\[\mathbb{E}\|\Theta(t+1)-\Theta^{*}\|_{2}^{2}\leq(1-\eta_{t}\lambda(1-\gamma)) \|\Theta(t)-\Theta^{*}\|_{2}^{2}-\eta_{t}(1-\gamma)\mathbb{E}\|f_{\Theta(t)}- \mathcal{V}\|_{\mu}^{2}+4\eta_{t}\rho ub_{t}^{-p}+\eta_{t}^{2}b_{t}^{1-p}u.\]

With the step-size choice \(\eta_{t}=\frac{1}{(1-\gamma)\lambda t}\), we obtain by induction:

\[\mathbb{E}\|\Theta(t+1)-\Theta^{*}\|_{2}^{2}\leq-\frac{1}{\lambda t}\sum_{k=1 }^{t}\mathbb{E}\|f_{\Theta(k)}-\mathcal{V}\|_{\mu}^{2}+\frac{4\rho u}{\lambda_ {\textsf{min}}t}\sum_{k=1}^{t}b_{k}^{-p}+\frac{u}{\lambda_{\textsf{min}}^{2}t }\sum_{k=1}^{t}\frac{b_{k}^{1-p}}{k}.\]

By rearranging the terms and using the integral bound for the summations above, and using the Jensen's inequality for the \(\mu\)-norm, we obtain the result. 

Proof of Theorem 2.: Let \(\mathcal{F}_{t}^{++}=\sigma(\Theta(1),\ldots,\Theta(t),X_{t},X_{t+1})\) and \(\mathbb{E}_{t}^{++}[\cdot]=\mathbb{E}[\cdot|\mathcal{F}_{t}^{++}]\). Also, let

\[\hat{g}(\Theta) =\mathbb{E}_{t}^{++}g_{t}(\Theta),\] \[\bar{g}(\Theta) =\sum_{x,x^{\prime}\in\mathbb{X}}\mu(x)\mathcal{P}(x,x^{\prime})(r (x)+\gamma f_{\Theta}(x^{\prime})-f_{\Theta}(x))\Phi(x).\]

The bias due to Markovian sampling is:

\[Z_{t}(\Theta)=\Big{(}\hat{g}_{t}(\Theta)-\bar{g}(\Theta)\Big{)}^{\top}(\Theta -\Theta^{*}).\]

With the above definitions, the Lyapunov drift at time \(t\geq 1\) can be bounded as follows:

\[\mathbb{E}_{t}^{++}\|\Theta(t+1)-\Theta^{*}\|_{2}^{2}\leq\|\Theta (t)-\Theta^{*}\|_{2}^{2}-2\eta(1-\gamma)\|\mathcal{V}-f_{\Theta(t)}\|_{\mu}^{2 }+\eta^{2}\mathbb{E}_{t}^{++}[\|g_{t}(\Theta(t))\|_{2}^{2}\chi_{t}]\\ +2\eta\hat{g}^{\top}(\Theta(t)-\Theta^{*})\bar{\chi}_{t}+2\eta Z _{t}(\Theta(t)),\]

where \(\chi_{t}=1-\bar{\chi}_{t}=1\{\|g_{t}(\Theta(t))\|_{2}\leq b_{t}\}\). Compared to the case of iid sampling in Theorem 1, the difference is \(Z_{t}(\Theta(t))\). In the following, we bound \(\mathbb{E}Z_{t}(\Theta(t))\) by using the mixing time analysis in [4]. First, we provide two essential properties of \(Z_{t}(\Theta)\) to verify the conditions in Lemma 10 in [4].

**Lemma 3**.: _Under Assumption 1, we have:_

\[|Z_{t}(\Theta)| \leq(1+2\rho)^{2},\ \Theta\in B_{2}(0,\rho),\] (27) \[|Z_{t}(\Theta)-Z_{t}(\Theta^{\prime})| \leq 6(1+2\rho)^{2}\|\Theta-\Theta^{\prime}\|_{2}^{2},\ \Theta,\Theta^{\prime}\in B_{2}(0,\rho).\] (28)Thus, we have:

\[\mathbb{E}Z_{t}(\Theta(t))\leq\mathbb{E}[Z_{t}(\Theta(t-\tau))]+6(1+2\rho)^{2} \mathbb{E}\|\Theta(t)-\Theta(t-\tau)\|_{2}.\] (29)

We have the following inequality:

\[\|\Theta(t)-\Theta(t-\tau)\|_{2}\leq\sum_{k=t-\tau}^{t-1}\|\Theta(k+1)-\Theta( k)\|_{2}\leq\eta\sum_{k=t-\tau}^{t-1}\|g_{t}(\Theta(t))\|_{2}\chi_{t}.\]

Taking the expectation above, and using Holder's inequality:

\[\mathbb{E}\|\Theta(t)-\Theta(t-\tau)\|_{2}\leq\sum_{k=t-\tau}^{t-1}\Big{(} \mathbb{E}\big{[}\|g_{k}(\Theta(k))\|_{2}^{1+p}\big{]}\Big{)}^{\frac{1}{1+p}} \leq\eta\tau u^{\frac{1}{1+p}}.\]

By using the information theoretic bound in Lemma 9 in [4], we obtain

\[\mathbb{E}Z_{t}(\Theta(t-\tau))\leq 2(1+2\rho)^{2}\eta,\]

under the uniform ergodicity assumption in Assumption 1. Using the last two inequalities in (29), we obtain:

\[\mathbb{E}Z_{t}(\Theta(t))\leq 2(1+2\rho)^{2}\Big{(}1+6\tau u^{\frac{1}{1+p}} \Big{)}\eta.\] (30)

By using the above result, we obtain the ultimate inequality for the Lyapunov drift as follows:

\[\mathbb{E}\|\Theta(t+1)-\Theta^{*}\|_{2}^{2}\leq\mathbb{E}\|\Theta (t)-\Theta^{*}\|_{2}^{2}-2\eta(1-\gamma)\mathbb{E}\|\mathcal{V}-f_{\Theta(t) }\|_{\mu}^{2}+\eta^{2}\mathbb{E}\big{[}\|g_{t}\|_{2}^{2}\chi_{t}\big{]}+4\eta \rho\mathbb{E}\big{[}\|g_{t}\|_{2}\chi_{t}\big{]}\\ +4\eta^{2}(1+2\rho)^{2}\Big{(}1+6\tau u^{\frac{1}{1+p}}\Big{)}.\]

The proof follows from identical steps as Theorem 1. 

Proof of Theorem 3.: The main idea in the proof is to establish a centering argument for both the bias (due to using clipped stochastic gradients) and the variability (controlled by \(b_{t}\)), and to use martingale concentration arguments based on Freedman's inequality and Azuma-Hoeffding inequality to bound the sample mean for the bias and variability, respectively. This strategy extends the approach in [6] for robust mean estimation to reinforcement learning, which has a dynamic behavior unlike the mean estimation problem. Namely, for any \(t\in\{1,2,\ldots,T\}\), we have:

\[\mathcal{L}(\Theta(t+1)) \leq\mathcal{L}(\Theta(t))-2\eta(1-\gamma)\|f_{\Theta(t)}- \mathcal{V}\|_{\mu}^{2}\] \[+\eta^{2}\mathbb{E}\|g_{t}(\Theta(t))\|_{2}^{2}\mathds{1}\{\|g_{ t}(\Theta(t))\|_{2}\leq b_{t}\}|\mathcal{F}_{t}]\] \[+2\eta B(t)+\eta^{2}V(t),\]

where the first line follows from Lemma 2, and

\[B(t)=-\mathbb{E}[g_{t}(\Theta(t))^{\top}(\Theta(t)-\Theta^{*})|\mathcal{F}_{t }]+g_{t}(\Theta(t))^{\top}(\Theta(t)-\Theta^{*})\mathbb{1}\{\|g_{t}(\Theta(t) )\|_{2}\leq b_{t}\},\] (31)

is the bias term, and

\[Z(t)=\|g_{t}(\Theta(t))\|_{2}^{2}\mathds{1}\{\|g_{t}(\Theta(t))\|_{2}\leq b_{ t}\}-\mathbb{E}[\|g_{t}(\Theta(t))\|_{2}^{2}\mathds{1}\{\|g_{t}(\Theta(t))\|_{2} \leq b_{t}\}|\mathcal{F}_{t}]\]

is the variability. By telescoping sum over \(t=1,2,\ldots,T\) and some algebraic manipulations, we have:

\[\begin{split}\frac{\mathcal{L}(\Theta(T+1))}{T}-\frac{\mathcal{L }(\Theta(1))}{T}\leq-2\eta(\frac{1}{T}\sum_{t=1}^{T}f(\Theta(t))-f(\Theta^{*}) )\\ +\eta^{2}\frac{u}{T}\sum_{t=1}^{T}b_{t}^{1-p}+\frac{2\eta}{T} \sum_{t=1}^{T}B(t)+\frac{\eta^{2}}{T}\sum_{t=1}^{T}Z(t),\end{split}\] (32)

where we used (20) in the second line. In the following, we will bound the empirical processes \(\frac{1}{T}\sum_{t=1}Z(t)\) and \(\frac{1}{T}\sum_{t=1}B(t)\).

Note that \(\{Z(t):t\in\mathbb{N}\}\) is a martingale difference sequence (MDS) adapted to the filtration \(\{\mathcal{F}_{t}:t\in\mathbb{N}\}\). Furthermore, note that

\[|Z(t)|\leq 2b_{t}^{2}\leq 2b_{T}^{2},\]almost surely for any \(t\leq T\). Thus, \(\sum_{t=1}^{n}V(t)\mathbbm{1}\{n\leq T\}\) forms a martingale with bounded differences, and by using Azuma-Hoeffding inequality [55], we have:

\[\frac{1}{T}\sum_{t=1}^{T}Z(t)\leq b_{T}\sqrt{\frac{L_{\delta}}{T}}=\frac{u^{ \frac{1}{1+p}}T^{\frac{1-p}{2(1+p)}}}{L_{\delta}^{\frac{1-p}{2(1+p)}}}\leq\frac {u^{\frac{2}{1+p}}T^{\frac{1-p}{1+p}}}{L_{\delta}^{\frac{1-p}{1+p}}},\] (33)

with probability at least \(1-\delta/2\) where the last inequality holds since \(T>L_{\delta}u^{-\frac{2}{1-p}}\).

We decompose \(B(t)\) into predictable and non-predictable components as follows:

\[B(t)=\mathbb{E}[g_{t}(\Theta(t))^{\top}(\Theta(t)-\Theta^{*})\mathbbm{1}\{ \|g_{t}(\Theta(t))\|_{2}>b_{t}\}|\mathcal{F}_{t}]+B_{0}(t),\] (34)

where the martingale difference sequence \(B_{0}(t)\) is defined as follows:

\[B_{0}(t)=-\mathbb{E}[g_{t}(\Theta(t))^{\top}(\Theta(t)-\Theta^{ *})\mathbbm{1}\{\|g_{t}(\Theta(t))\|_{2}\leq b_{t}\}|\mathcal{F}_{t}]\\ +g_{t}(\Theta(t))^{\top}(\Theta(t)-\Theta^{*})\mathbbm{1}\{\|g_{ t}(\Theta(t))\|_{2}\leq b_{t}\}.\] (35)

By (25) (with \(b_{t}\) replaced by \(b_{t}\)), the first term on the RHS of (34) is bounded by \(2\rhouc_{t,\delta}^{-p}\). Thus, we have:

\[\frac{1}{T}\sum_{t=1}^{T}B(t)\leq\frac{2\rho u^{\frac{1}{1+p}}L_{\delta}^{ \frac{p}{1+p}}}{T^{\frac{1}{1+p}}}+\frac{1}{T}\sum_{t=1}^{T}B_{0}(t).\] (36)

In order to upper bound \(\frac{1}{T}\sum_{t=1}^{T}B_{0}(t)\), we use Freedman's inequality for martingales [12]. To use Freedman's inequality, we verify the following conditions.

1. For any \(t\leq T\), we have: \[|g_{t}(\Theta(t))^{\top}(\Theta(t)-\Theta^{*})\mathbbm{1}\{\|g_{t}(\Theta(t) )\|_{2}\leq b_{t}\}|\leq 2\rho b_{t}\leq 2\rho b_{t},\] almost surely.
2. The normalized quadratic variation process satisfies: \[\frac{1}{T}\sum_{t=1}^{T}|B_{0}(t)|^{2}\\ \leq\frac{1}{T}\sum_{t=1}^{T}\mathbb{E}[\|g_{t}(\Theta(t))^{\top }(\Theta(t)-\Theta^{*})|^{2}\mathbbm{1}\{\|g_{t}(\Theta(t))\|_{2}\leq b_{t}\}| \mathcal{F}_{t}],\\ \leq\frac{4\rho^{2}}{T}\sum_{t=1}^{T}\mathbb{E}[\|g_{t}(\Theta(t ))\|_{2}^{2}\mathbbm{1}\{\|g_{t}(\Theta(t))\|_{2}\leq b_{t}\}|\mathcal{F}_{t}], \\ \leq\frac{4\rho^{2}}{T}\sum_{t=1}^{T}wb_{t}^{1-p}\leq 4\rho^{2} \frac{u^{\frac{1}{1+p}}T^{\frac{1-p}{1+p}}}{L_{\delta}^{\frac{1-p}{1+p}}},\] where the first inequality is due to \(Var(Z)\leq\mathbb{E}[Z^{2}]\) for any random variable \(Z\) with a finite variance, the second inequality follows from Cauchy-Schwarz inequality and triangle inequality with \(\max\{\|\Theta(t)\|_{2},\|\Theta^{*}\|_{2}\}\leq\rho\) due to projection, the third inequality follows from (20) with \(b_{t}\) replaced by \(b_{t}\).

Thus, by Freedman's inequality, we have:

\[\frac{1}{T}\sum_{t=1}^{T}B_{0}(t) \leq\frac{2\sqrt{2}\rho u^{\frac{1}{1+p}}L_{\delta}^{\frac{p}{1+p }}}{T^{\frac{1}{1+p}}}+\frac{4\rho L_{\delta}b_{t}}{3T},\] \[\leq(2\sqrt{2}+4/3)\rho\frac{u^{\frac{1}{1+p}}L_{\delta}^{\frac{ p}{1+p}}}{T^{\frac{1}{1+p}}},\]with probability at least \(1-\delta/2\). Therefore, from (36) and the above inequality, with probability at least \(1-\delta/2\), we have:

\[\frac{1}{T}\sum_{t=1}^{T}B(t)\leq\frac{7\rho u^{\frac{1}{1+p}}L_{\xi}^{\frac{T}{1 +p}}}{T^{\frac{p}{1+p}}}\] (37)

Hence, by substituting (33) and (37) into (32) with union bound, and using the specified step-size together with the facts that \(\mathcal{L}(\Theta(1))\leq 4\rho^{2}\) and \(\mathcal{L}(\Theta(T+1))\geq 0\), we conclude the proof. 

### Finite-Time Analysis of Robust TD Learning without Realizability

In the following, we release the realizability assumption, and show that an additional function approximation error appears in the bounds for the general case.

**Theorem 5** (Performance of Robust TD learning - without realizability).: Let

\[\epsilon_{\mathsf{app}}=\inf_{\Theta\in B_{2}(0,\rho)}\sqrt{\mathbb{E}| \mathcal{V}(x)-\langle\Theta,\Phi(x)\rangle|^{2}}.\]

Then, under Assumptions 1 and 2a we have the following bounds for Robust TD learning:

For \(b_{t}=(ut)^{\frac{1}{1+p}}\) for any \(t\in\mathbb{Z}_{+}\) and \(\eta_{t}=\eta=\frac{2\rho(1-\gamma)}{(uT)^{\frac{1}{1+p}}}\), we have:

\[\mathbb{E}_{\Theta(1),\Theta^{(2)}_{\mathcal{V}\cdot\mu},\Theta(T)}\Big{[} \Big{(}\mathcal{V}(x)-\langle\bar{\Theta}(T),\Phi(x)\rangle\Big{)}^{2}\Big{]} \leq\frac{6\rho u^{\frac{1}{1+p}}}{(1-\gamma)T^{\frac{1}{1+p}}}+(\rho+\frac{1 }{1-\gamma})\frac{2\epsilon_{\mathsf{app}}}{1-\gamma},\] (38)

for any \(T>1\).

Proof of Theorem 5.: Note that we define

\[\Theta^{\star}\in\arg\min_{\Theta\in B_{2}(0,\rho)}\mathbb{E}[|\mathcal{V}(x) -\langle\Theta,\Phi(x)\rangle|^{2}].\]

By (19), we have:

\[\mathbb{E}[\mathcal{L}(\Theta(t+1))|\mathcal{F}_{t}]\leq\mathcal{ L}(\Theta(t))+2\eta\mathbb{E}_{t}[g_{t}^{\top}(\Theta(t))(\Theta(t)-\Theta^{ \star})]\\ -2\eta\mathbb{E}[g_{t}(\Theta(t))^{\top}(\Theta(t)-\Theta^{\star })\mathds{1}\{\|g_{t}(\Theta(t))\|_{2}>b_{t}\}|\mathcal{F}_{t}]+\eta^{2}ub_{t} ^{1-p},\] (39)

To eliminate the realizability assumption (Assumption 3), we make the following decomposition:

\[\mathbb{E}_{t}[g_{t}^{\top}(\Theta(t))(\Theta(t)-\Theta^{\star})] =\mathbb{E}_{t}[(R_{t}+\gamma f_{\Theta(t)}(X_{t}^{\prime})-f_{ \Theta(t)}(X_{t}))(f_{\Theta(t)}(X_{t})-\langle\Theta^{\star},\Phi(X_{t}) \rangle)],\] \[=\mathbb{E}_{t}\Big{[}\Big{(}(\mathcal{T}f_{\Theta(t)})(X_{t})-f _{\Theta(t)}(X_{t})\Big{)}\Big{(}f_{\Theta(t)}(X_{t})-\langle\Theta^{\star}, \Phi(X_{t})\rangle\Big{)}\Big{]},\] \[=\mathbb{E}_{t}\Big{[}\Big{(}(\mathcal{T}f_{\Theta(t)}(X_{t})-f _{\Theta(t)}(X_{t})\Big{)}\Big{(}f_{\Theta(t)}(X_{t})-\mathcal{V}(X_{t}) \Big{)}\Big{]}\] \[\qquad+\mathbb{E}_{t}\Big{[}\Big{(}(\mathcal{T}f_{\Theta(t)}(X_{t })-f_{\Theta(t)}(X_{t})\Big{)}\Big{(}\mathcal{V}(X_{t})-\langle\Theta^{\star},\Phi(X_{t})\rangle\Big{)}\Big{]}.\]

Note that the last term corresponds to the approximation error, and we can bound its expectation as:

\[\mathbb{E}\Big{[}\Big{(}(\mathcal{T}f_{\Theta(t)})(X_{t})-f_{\Theta(t)}(X_{t} )\Big{)}\Big{(}\mathcal{V}(X_{t})-\langle\Theta^{\star},\Phi(X_{t})\rangle \Big{)}\Big{]}\leq(1-\gamma+1)\|\mathcal{V}-f_{\Theta(t)}\|_{\mu}\cdot \epsilon_{\mathsf{app}}.\]

Since \(|\mathcal{V}(x)|\leq\frac{1}{1-\gamma}\) and \(|f_{\Theta(t)}(x)|\leq\rho\) for all \(x\), and \(\gamma\in(0,1)\), we have:

\[\mathbb{E}\Big{[}\Big{(}(\mathcal{T}f_{\Theta(t)})(X_{t})-f_{\Theta(t)}(X_{t} )\Big{)}\Big{(}\mathcal{V}(X_{t})-\langle\Theta^{\star},\Phi(X_{t})\rangle \Big{)}\Big{]}\leq 2(\rho+\frac{1}{1-\gamma})\epsilon_{\mathsf{app}},\]

for any \(t\). Using this result, and following identical steps as Theorem 1 with the same step-size yields the result.

[MISSING_PAGE_FAIL:19]