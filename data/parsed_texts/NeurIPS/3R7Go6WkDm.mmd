# Post-Hoc Reversal: Are We Selecting Models Prematurely?

Rishabh Ranjan\({}^{1}\)

Saurabh Garg\({}^{2}\)

Mrigank Raman\({}^{2}\)

Carlos Guestrin\({}^{1,3}\)

Zachary Lipton\({}^{2}\)

\({}^{1}\)Stanford University, \({}^{2}\)Carnegie Mellon University, \({}^{3}\)Chan Zuckerberg Biohub

{ranjanr,guestrin}@stanford.edu, {sgarg2,mrigankr,zlipton}@cmu.edu

###### Abstract

Trained models are often composed with post-hoc transforms such as temperature scaling (TS), ensembling and stochastic weight averaging (SWA) to improve performance, robustness, uncertainty estimation, etc. However, such transforms are typically applied only after the base models have already been finalized by standard means. In this paper, we challenge this practice with an extensive empirical study. In particular, we demonstrate a phenomenon that we call _post-hoc reversal_, where performance trends are reversed after applying post-hoc transforms. This phenomenon is especially prominent in high-noise settings. For example, while base models overfit badly early in training, both ensembling and SWA favor base models trained for more epochs. Post-hoc reversal can also prevent the appearance of double descent and mitigate mismatches between test loss and test error seen in base models. Preliminary analyses suggest that these transforms induce reversal by suppressing the influence of mislabeled examples, exploiting differences in their learning dynamics from those of clean examples. Based on our findings, we propose _post-hoc selection_, a simple technique whereby post-hoc metrics inform model development decisions such as early stopping, checkpointing, and broader hyperparameter choices. Our experiments span real-world vision, language, tabular and graph datasets. On an LLM instruction tuning dataset, post-hoc selection results in \(>1.5\times\) MMLU improvement compared to naive selection.2

Footnote 2: Code is available at https://github.com/rishabh-ranjan/post-hoc-reversal.

## 1 Introduction

Many widely used techniques in deep learning operate on trained models; we refer to these as _post-hoc transforms_. Examples include temperature scaling (TS) [19], stochastic weight averaging (SWA) [28] and ensembling [39]. These techniques have shown promise for improving predictive performance, robustness, uncertainty estimation, out-of-distribution generalization, and few-shot performance [4, 6, 39, 56, 84]. Typically, the pre-training and post-hoc stages are isolated. The workflow is: (1) pick model architecture, training recipe, hyperparameters, etc. to optimize for individual model performance; (2) train one or more models; (3) pick best-performing checkpoints; (4) apply post-hoc transforms. We refer to this procedure as _naive selection_.

In this paper, we demonstrate interesting drawbacks of naive selection. In a large-scale empirical study, we uncover _post-hoc reversal_--a phenomenon whereby post-hoc transforms reverse performance trends between models (Fig. 1). We demonstrate post-hoc reversal with respect to training epochs, model sizes, and other hyperparameters like learning rate schedules. We further establish that post-hoc reversal is a robust phenomenon by experimenting on real-world datasets across domains and modalities, with diverse model classes and training setups.

Post-hoc reversal is most prominent on noisy datasets (Fig. 2). Other phenomena exacerbated by noise include catastrophic overfitting [50],double descent [55], and loss-error mismatch [19]. While these phenomena pose challenges to model development, post-hoc reversal suggests a path to alleviate them. Noise can arise not only from labeling errors, but also from inherent uncertainty in the prediction task, such as in next token prediction [60]. Indeed, severe performance degradation has limited multi-epoch training of large language models (LLMs) [81]. Here too, post-hoc reversal reveals a promising path for sustained performance improvements over longer training.

The core intuition for post-hoc reversal is that models continue to learn generalizable patterns from clean examples, even when spurious patterns learnt from mislabeled examples worsen the overall performance. Post-hoc transforms exploit differences in the learning dynamics of clean and mislabeled examples [42] to reinforce the influence of the former, while suppressing that of the latter. When strong enough, this effect leads to reversal. We show evidence for these intuitions in SS 5.

Based on our findings, we propose _post-hoc selection_--a simple technique whereby base models are selected based on post-transform performance. The technique is practical as the transforms of interest can be cheaply incorporated into the validation phase of the training loop. Post-hoc selection significantly improves the performance of the transformed models, with \(>2\times\) improvements over naive selection in some cases (Fig. 2). In terms of absolute performance, post-hoc selection leads to \(>3\)-point reduction in test error over naive selection on a satellite imaging dataset (Fig. 1). The reduction is even higher (\(>5\) points) when using out-of-distribution (OOD) val/test splits for the same dataset. On an LLM instruction tuning dataset, under our procedure a composed transform of SWA, ensemble and TS gives \(>1.5\times\) MMLU improvement over a naive application of the same transform on prematurely selected models.

Figure 1: An illustration of the _phenomenon_ of _post-hoc reversal_ on the FMoW dataset: base performance at epoch \(t_{2}\) is worse than at epoch \(t_{1}\) (\(b_{2}>b_{1}\)), but post-hoc performance is better (\(p_{2}<p_{1}\)). The current practice of _naive selection_ considers base metrics to pick models at epoch \(t_{1}\). Our proposed _technique_ of _post-hoc selection_ instead uses post-hoc metrics to pick models at epoch \(t_{2}\), resulting in \(>2\times\) improvement over naive selection in both test loss and error. SWA+Ens+TS refers to the post-hoc transform obtained by composing SWA, ensemble (Ens) and temperature scaling (TS). Base curves show mean of \(8\) runs, models from which constitute the ensembles. Individual runs are shown in lighter colors. See Fig. 5 for more detailed curves on this dataset.

Figure 2: A comparison of naive and post-hoc selection on label sets from CIFAR-10/100-N (abbr. C-10/100-N) for the SWA+TS transform. On noisy label sets, post-hoc selection is often \(>2\times\) better.

Related Work

A slew of empirical works [10; 17; 31; 55; 57; 58] have revealed both challenges and opportunities for improving the understanding and practice of deep learning. Our work expands this list with a novel phenomenon tying together noisy data learning and post-hoc transforms. Orthogonal to our work, a number of training-stage strategies for noisy data have been proposed (see [69] for a survey).

TS belongs to a family of calibration techniques [2; 19] proposed with the goal of producing well-calibrated probabilities. Ensembling is a foundational technique in machine learning, with simple variants routinely used in deep learning [3; 39]. SWA [28] is the culmination of a line of work [18; 25] seeking to cheaply approximate ensembling. Despite their prevalence, a thorough understanding of best practices for wielding these techniques is lacking, especially in the context of noisy data. Our work fills this gap. For a more detailed discussion on related work, see App. A.

## 3 Preliminaries and Background

We describe our learning setup in SS 3.1, with emphasis on noisy data, a key focus of this work. In SS 3.2, we introduce the post-hoc transforms we study.

### Learning on Noisy Data

**Setup.** We consider multi-class classification with \(C\) classes, input \(\mathbf{x}\in\mathcal{X}\) and label \(y\in\mathcal{Y}=\{1,\ldots,C\}\). Training, validation and test sets are drawn i.i.d. from the data distribution \(\mathcal{D}\). A _classifier_\(f\colon\Theta\times\mathcal{X}\to\mathbb{R}^{C}\) outputs the logit vector \(\mathbf{z}=f(\mathbf{x};\mathbf{\theta})\), given parameter vector \(\mathbf{\theta}\in\Theta\). Predicted probability of class \(k\) is \(\mathbb{P}_{f}[y=k\mid\mathbf{x}]=\sigma(\mathbf{z})_{k}\), where \(\sigma\) is the softmax function.

**Noise.** Data \(\mathcal{D}\) is said to be _clean_ if \(\mathbb{P}_{\mathcal{D}}[y\mid\mathbf{x}]\) is one-hot for all \(\mathbf{x}\), _i.e._, \(\mathbb{P}_{\mathcal{D}}[y\mid\mathbf{x}]=\mathbf{1}\{y=y^{*}(\mathbf{x})\}\) for some labeling function \(y^{*}\colon\mathcal{X}\to\mathcal{Y}\). Then, for any example input \(\mathbf{x}^{(i)}\) in the dataset, the observed label is \(y^{(i)}=y^{*}(\mathbf{x}^{(i)})\). When \(\mathbb{P}_{\mathcal{D}}[y\mid\mathbf{x}]\) is not one-hot, \(\mathcal{D}\) is said to be _noisy_ and the observed label is only a stochastic sample \(y^{(i)}\sim\mathbb{P}_{\mathcal{D}}[y\mid\mathbf{x}=\mathbf{x}^{(i)}]\) from the underlying conditional distribution. Noise can arise due to (1) non-determinism in the prediction target (2) insufficient information in the input context, and (3) annotation errors. See App. B.1 for illustrated examples.

**Metrics.** A metric \(\mathcal{M}\colon\mathbb{R}^{C}\times\mathcal{Y}\to\mathbb{R}\) compares the predicted logits \(\mathbf{z}\) with the observed label \(y\). \(\mathcal{M}_{f}(\mathbf{\theta})=\mathcal{M}[f(\,\cdot\,;\mathbf{\theta})]= \mathbb{E}_{(\mathbf{x},y)\sim\mathcal{D}}[\mathcal{M}(f(\mathbf{x};\mathbf{ \theta}),y)]\) denotes the metric computed over \(\mathcal{D}\) given \(f\) and \(\mathbf{\theta}\). We use two metrics (1) _classification error_, or simply _error_, with \(\mathcal{M}^{\text{error}}(\mathbf{z},y)=\mathbf{1}\{\arg\max_{k}\mathbf{z}_{k }\neq y\}\) and (2) _cross-entropy loss_, or simply _loss_, with \(\mathcal{M}^{\text{loss}}(\mathbf{z},y)=-\log\sigma(\mathbf{z})_{y}\). The exponentiated loss, also called _perplexity_, is common in language modeling, where it is computed on a per-token basis. A standard result states that loss is minimized if and only if the ground truth conditional probability is recovered [20]. See App. B.1 for additional background.

### Post-Hoc Transforms in Machine Learning

**Definition 1** (Post-Hoc Transform): _A post-hoc transform \(\mathcal{T}\) maps a classifier \(f\colon\Theta\times\mathcal{X}\to\mathcal{Y}\) to another classifier \(\mathcal{T}\circ f\colon\Theta^{K}\times\mathcal{X}\to\mathcal{Y}\), for some \(K\)._

**Temperature Scaling (TS).** TS [19] involves scaling the logits with a _temperature_\(\tau\in\mathbb{R}\) obtained by optimizing the cross-entropy loss over the validation set, with model parameters fixed (Eqn. 1). Temperature scaling preserves error as it does not affect the predicted class. We use the torchcal[63] implementation, which optimizes the temperature on GPU with Newton's method [15].

\[(\mathcal{T}_{\text{TS}}\circ f)(\mathbf{x};\mathbf{\theta})=\frac{1}{\tau}f( \mathbf{x};\mathbf{\theta}),\text{ with }\tau=\arg\min_{\tau}\mathcal{M}^{\text{loss}}_{ \text{val}}\left[\frac{1}{\tau}f(\,\cdot\,;\mathbf{\theta})\right]\] (1)

**Ensembling.** In this method, predictions from an ensemble of classifiers are combined. In deep learning, simply averaging the temperature-scaled logits is effective (Eqn. 2). \(\mathbf{\theta}_{1},\ldots,\mathbf{\theta}_{K}\) are obtained from multiple training runs with the same architecture and dataset, with stochasticity from mini-batch sampling and random initialization, if applicable.

\[\left(\mathcal{T}_{\text{Ens}}\circ f\right)\left(\mathbf{x};\uptheta_{1},\dots, \uptheta_{K}\right)=\frac{1}{K}\sum_{k=1}^{K}\frac{1}{\tau_{k}}f(\mathbf{x}; \uptheta_{k}),\text{ with }\tau_{k}=\arg\min_{\tau}\mathcal{M}_{\text{val}}^{\text{ loss}}\left[\frac{1}{\tau}f(\,\cdot\,;\uptheta_{k})\right]\] (2)

**Stochastic Weight Averaging (SWA).** SWA [28] involves averaging weights \(\uptheta_{1},\dots,\uptheta_{K}\) from the same training run (Eqn. 3). BatchNorm statistics are recomputed after averaging, if required. We pick checkpoints at epoch boundaries. Unlike Izmailov et al. [28], we do not skip the initial epochs (warmup) or modify the learning rate schedule3.

Footnote 3: Thus, our variant of SWA is hyperparameter-free.

\[\left(\mathcal{T}_{\text{SWA}}\circ f\right)\left(\mathbf{x};\uptheta_{1}, \dots,\uptheta_{K}\right)=f\left(\mathbf{x};\frac{1}{K}\sum_{i=1}^{K}\uptheta _{i}\right)\] (3)

**Compositions.** TS, ensembling and SWA can be readily composed. In particular, we consider _SWA+TS_ and _SWA+Ens+TS_, for single- and multi-model settings respectively. We denote them with \(\mathcal{T}_{\text{S+T}}=\mathcal{T}_{\text{TS}}\circ\mathcal{T}_{\text{ SWA}}\) and \(\mathcal{T}_{\text{S+E+T}}=\mathcal{T}_{\text{TS}}\circ\mathcal{T}_{\text{Ens }}\circ\mathcal{T}_{\text{SWA}}\) (explicit forms in App. B.2).

## 4 Post-Hoc Reversal: Formalization and Empirical Study

To use post-hoc transforms, one must first select models to apply them to. Current practice is to select the best-performing model independent of post-hoc transforms, rationalized by an implicit _monotonicity_ assumption - "better-performing models result in better performance after transformation". As we shall see, this assumption is often violated in practice. We call such violations _post-hoc reversal_. In SS 4.1, we formalize post-hoc reversal and discuss ways to detect it. In SS 4.2, we empirically study various kinds of post-hoc reversal with special practical relevance.

### Definitions

First, we give a general definition of post-hoc reversal (Def. 2). If Def. 2 holds with \(\upvarphi_{k}\)'s which are optimal for the base metric \(\mathcal{M}_{f}\), then naive selection becomes suboptimal as it picks \(\upvarphi_{k}\)'s, but \(\uptheta_{k}\)'s are better under the post-hoc metric \(\mathcal{M}_{\mathcal{T}\circ f}\). Since the entire space of parameter tuples \(\Theta^{K}\) can be large, we study post-hoc reversal restricted to indexed parameters (Def. 3). Indices can be, for example, training epochs (SS 4.2.1), model sizes (SS 4.2.2) or hyperparameter configurations (SS 4.2.3).

**Definition 2** (Post-hoc reversal): _Let a post-hoc transform \(\mathcal{T}\) map a classifier \(f\colon\Theta\times\mathcal{X}\to\mathcal{Y}\) to \(\mathcal{T}\circ f\colon\Theta^{K}\times\mathcal{X}\to\mathcal{Y}\). \(\mathcal{T}\) applied to \(f\) exhibits post-hoc reversal for a metric \(\mathcal{M}\) if there exist \((\uptheta_{1},\dots,\uptheta_{K}),(\upvarphi_{1},\dots,\upvarphi_{K})\in \Theta^{K}\) such that \(\mathcal{M}_{f}(\uptheta_{k})\geq\mathcal{M}_{f}(\upvarphi_{k})\) for all \(k=1,\dots,K\) but \(\mathcal{M}_{\mathcal{T}\circ f}(\uptheta_{1},\dots,\uptheta_{K})<\mathcal{M}_ {\mathcal{T}\circ f}(\upvarphi_{1},\dots,\upvarphi_{K})\)._

**Definition 3** (Index-wise post-hoc reversal): _Let \(\mathcal{I}\) be a set of indices and \(\mathcal{P}\colon\mathcal{I}\to\Theta^{K}\) map indices to parameter tuples. When Def. 2 holds with \((\uptheta_{1},\dots,\uptheta_{K})=\mathcal{P}(s),(\upvarphi_{1},\dots,\upvarphi _{K})=\mathcal{P}(t)\) for some \(s,t\in\mathcal{I}\), we call it index-wise post-hoc reversal._

**Diagnosis.** To enable a visual diagnosis of post-hoc reversal, we define base and post-hoc curves (Def. 4) and a relaxed notion of post-hoc reversal for them (Def. 5). Post-hoc reversal is characterized by non-monotonicity between the base and post-hoc curves, i.e., there exist regions where one improves while the other worsens. This happens, for instance, when one curve exhibits double descent but the other doesn't. Different optimal indices for the two curves is another indicator of post-hoc reversal.

**Definition 4** (Base and post-hoc curves): _The base and post-hoc curves \(\mathcal{M}^{\text{base}},\mathcal{M}^{\text{post}}\colon\mathcal{I}\to\mathbb{R}\) are given by \(\mathcal{M}^{\text{base}}(t)=\frac{1}{K}\sum_{k=1}^{K}\mathcal{M}_{f}(\uptheta_ {k})\) and \(\mathcal{M}^{\text{post}}(t)=\mathcal{M}_{\mathcal{T}\circ f}(\uptheta_{1},\dots, \uptheta_{K})\), where \((\uptheta_{1},\dots,\uptheta_{K})=\mathcal{P}(t)\)._

**Definition 5** (Post-hoc reversal for curves): _Base and post-hoc curves \(\mathcal{M}^{\text{base}},\mathcal{M}^{\text{post}}\colon\mathcal{I}\to\mathbb{R}\) exhibit post-hoc reversal when there exist \(s,t\in\mathcal{I}\) such that \(\mathcal{M}^{\text{base}}(s)\geq\mathcal{M}^{\text{base}}(t)\) but \(\mathcal{M}^{\text{post}}(s)<\mathcal{M}^{\text{post}}(t)\)._

### Experiments

#### 4.2.1 Epoch-Wise Post-Hoc Reversal

When the indices in Def. 3 are training epochs, we call it _epoch-wise post-hoc reversal_. We use \(\Theta_{t}\) to denote the model at the end of epoch \(t\). For ensembles, a superscript \(j\) denotes the \(j\)-th training run (out of \(N\) runs). \(t\in\mathcal{I}\) maps to parameters \(\mathcal{P}(t)\in\Theta^{K}\) (\(K=1\) for TS; \(N\) for ensemble; and \(t\) for SWA) as follows: \(\mathcal{P}_{\text{TS}}(t)=(\Theta_{t});\mathcal{P}_{\text{Ens}}(t)=(\Theta_{t} ^{1},\dots,\Theta_{t}^{N})\)4; \(\mathcal{P}_{\text{SWA}}(t)=(\Theta_{1},\dots,\Theta_{t})\).

Footnote 4: Ensembling models from possibly unequal epochs is covered in § 6

**Experimental setup.** We focus on the CIFAR-N dataset [74]. CIFAR-10-N uses the same images as CIFAR-10 but provides multiple human-annotated label sets, allowing the study of realistic noise patterns of varying levels in a controlled manner. Clean is the original label set; Rand1,2,3 are 3 sets of human labels; Aggre combines Rand1,2,3 by majority vote; and Worst combines them by picking an incorrect label, if possible. Similarly CIFAR-100-N has two label sets, Clean and Noisy, with the latter being human-labeled. We train ResNet18 [21] models for 100 epochs with a cosine annealed learning rate. Additional details on datasets and training setup are in App. C. Fig. 3 shows test curves on CIFAR-10-N Clean, Rand1 and Worst. Other label sets and CIFAR-100-N are in App. E. For clarity, we omit the SWA base curve \(\mathcal{M}_{\text{SWA}}^{\text{base}}(t)=(\mathcal{M}_{f}(\Theta_{1})+\dots+ \mathcal{M}_{f}(\Theta_{t}))/t\) in the plots, and simply re-use the curve \(\mathcal{M}^{\text{base}}(t)=\mathcal{M}_{f}(\Theta_{t})\) to compare with the post-hoc SWA curve. While deviating from Def. 4, this better reflects the current practice of early stopping on the latest epoch's base metric.

**Observations.** First, we focus on the base curves: _(1) Overfitting:_ As noise increases, test curves go from a single descent to a double descent to a U-shaped curve with increased overfitting. _(2) Double descent:_ Noise amplifies double descent, and the second descent worsens with increasing noise (as compared to the first). _(3) Loss-error mismatch:_ Loss overfits more drastically than error, leading to a mismatch with higher noise. Optimal models for loss and error can be different.

Figure 3: Loss and error for CIFAR-10-N Clean (approx. \(0\%\) noise), Rand1 (approx. \(17\%\) noise) and Worst (approx. \(40\%\) noise). Except for ensemble curves, mean of \(8\) runs is shown; individual runs are in lighter shades. Ensembles comprise models from these \(8\) runs. For example, observe post-hoc reversal for C-10-N Worst: (1) error plot: from epoch \(5\) to \(50\), solid red (base) curve worsens but solid orange (SWA) curve improves; (2) error plot: solid red (base) curve has a double descent but dashed red (ensemble) curve does not; (3) loss plots: solid red (base) curve has a double descent pre-TS but not post-TS; (4) error plot: best error is at approx. epoch \(5\) for solid red (base) curve but at approx. epoch \(60\) for dashed orange (SWA ensemble) curve.

Next, we consider the general impact of post-hoc transforms: _(4) Performance improvements:_ TS, SWA and ensemble always improve performace, both individually and in composition with larger gaps for noisy label sets. _(5) Post-hoc reversal:_ Post-hoc reversal manifests as non-monotonicity between the base and post-hoc curves, especially for noisy label sets. _(6) SWA vs Ensemble:_ SWA can recover much of the ensemble gain, but the optimal epoch often differs a lot from the base curve. _(7) Smoother curves:_ Base curves fluctuate wildly, but SWA and ensemble curves are smooth, making them more reliable for early stopping.

Finally, we discuss some benefits from post-hoc reversal: _(8) Overfitting:_ All transforms reduce overfitting, often reverting performance degradation. _(9) Double descent:_ SWA, ensemble and compositions flatten the double descent peak. TS, on the other hand, leads to a double descent for some cases where there was none before. _(10) Loss-error mismatch:_ TS aligns the loss and error curves, enabling simultaneously good loss and error.

#### 4.2.2 Model-Wise Post-Hoc Reversal

Here, indices represent model sizes. Models of all sizes are trained for \(T\) epochs, large enough for convergence. Following [55], we avoid early stopping. Notation-wise, we add a subscript to \(\Theta\) to indicate the model size \(s\). Parameters are indexed as follows: \(\mathcal{P}_{\text{TS}}(s)=(\Theta_{T,s})\); \(\mathcal{P}_{\text{Ens}}(s)=(\Theta^{1}_{T,s},\dots,\Theta^{N}_{T,s})\); \(\mathcal{P}_{\text{SWA}}(s)=(\Theta_{1,s},\dots,\Theta_{T,s})\).

**Experimental setup.** We parameterize a family of ResNet18s by scaling the number of filters in the convolutional layers. Specifically, we use \([k,2k,4k,8k]\) filters for width \(k\). The standard ResNet18 corresponds to \(k=64\). Otherwise the training setup is same as before. Fig. 4 shows the curves. Concretely, the index set \(\mathcal{I}=\{2,4,\dots,64\}\) is the set of ResNet widths \(k\) described above.

**Observations.** Post-hoc transforms improve performance (up to \(\approx 10\) points for error) and mitigate double descent. Further, we see yet another way in which higher-capacity models are better: they give better results under post-hoc transforms even when lower-capacity base models perform better.

Figure 4: C-10-N Worst test curves against model size. Best width for solid blue curves is \(\sim 10\) but for dashed orange curves, it is \(\sim 50\) for error and \(\sim 25\) for post-TS loss.

Figure 5: FMoW test curves for \(3\) LR schedules. Note that the pre-TS loss is significantly higher than the post-TS loss. For example, observe post-hoc reversal w.r.t. cosine and constant LRs at epoch \(50\) between: (1) solid blue (base) and dashed blue (ensemble) error curves; (2) solid blue (base) and solid orange (SWA) post-TS loss curves; (3) solid blue (base) curves for pre-TS and post-TS loss.

#### 4.2.3 Hyperparameter-Wise Post-Hoc Reversal

In general, the index set \(\mathcal{I}\) can contain any hyperparameter configurations. Here, we consider two hyperparamters: learning rate schedule and training epochs. To avoid repeating CIFAR-N epoch-wise curves, we experiment on a fresh dataset, FMoW.

**Experimental setup.** We experiment on learning rates (LRs) and training epochs, with index set \(\mathcal{I}=\{\mathtt{const},\mathtt{exp},\mathtt{cos}\}\times\{1,\dots,T\}\). Here, \(\mathtt{const}\), \(\mathtt{exp}\) and \(\mathtt{cos}\) refer to constant, exponentially decaying and cosine annealed LRs respectively, and \(T\) is the total number of epochs. We train DenseNet121 [26] models on the FMoW dataset [9] which constitutes a \(62\)-way classification of land use from satellite images. For more details, see App. C. Fig. 5 shows the curves.

**LR-wise observations.** We see some interesting instances of post-hoc reversal: (1) constant LR has the worst base performance but the best post-hoc performance; (2) under SWA and TS (composed), the curves continue to improve at the later epochs for constant LR, but not for the decaying LRs5.

Footnote 5: Possibly due to higher model variance with constant LR, beneficial for both ensembling and SWA.

**Epoch-wise observations.** Epoch-wise post-hoc reversal occurs for all LR schedules. SWA and ensembling convert the double descent into a strong single descent, with approx. \(10\)-point improvement in error for the latter. For constant LR, this also changes the optimal epoch. SWA only recovers about half of the ensemble gain, and perhaps surprisingly, ensembling SWA models is not better than ensembling alone. Pre-TS loss curves show a strong mismatch with the error curves, but TS enables simultaneously good loss and error with the last epoch models. Overall, these observations reinforce the trends gleaned from the CIFAR-N experiments.

## 5 Intuitions for Post-Hoc Reversal

In this section, we give hypotheses for post-hoc reversal, backed by experimental evidence.

**Ensembling and SWA delay catastrophic overfitting.** Models learn generalizable patterns from clean examples, and spurious patterns from mislabeled ones. The latter causes overfitting. When noise is low, the former dominates and overfitting is benign. Otherwise, overfitting is catastrophic. Ensembling and SWA improve fitting of clean examples, and reduce memorization of mislabeled ones. When this overturns the dominance of spurious patterns, we observe reversal.

Fig. 6 validates this intuition for SWA on CIFAR-10-N Worst. Fig. 7 further suggests the underlying mechanism -- predictions on the mislabeled train subset fluctuate much more during training, allow

Figure 6: Evolution of the fit/memorization of clean and mislabeled examples during training, for base and SWA models on C-10-N Worst. Train error drops earlier for the clean subset. In the regime of post-hoc reversal (shaded), SWA further lowers the train error on the clean subset, while raising it on the mislabeled subset.

Figure 7: Flipping of predicted class between consecutive epochs, for clean and mislabeled train subsets of C-10-N Worst. % of examples flipped is about twice as high for the mislabeled subset, suggesting an unstable influence on the decision boundary.

ing SWA to easily revert their memorization. In App. G, we extend this analysis to ensembling and solidify the intuition further by visualizing decision boundaries on a synthetic dataset. This explanation also applies to flattening of the double descent peak, which is a manifestation of catastrophic overfitting.

**TS mitigates loss-error mismatch.** Once a neural net has fit a train example, the cross-entropy loss on it can be lowered by simply upscaling the weights of the linear output layer. This makes the model overconfident later in training, as shown in [19]. For a mislabeled example, this leads to worse loss on similar test instances. The test error is not affected as it is independent of the scale of the logits. In high-noise settings, test loss can worsen due to memorization of mislabeled examples, even as the test error improves from continued learning on clean examples, leading to loss-error mismatch. TS fixes this by downscaling the logits. Indeed, one finds that the temperature (as obtained with a held-out set) increases with epochs (Fig. 8).

**Post-hoc reversal can occur against epochs, model sizes or other hyperparameters.** Different variants of post-hoc reversal can be unified via _effective model complexity_ (EMC), introduced in [55] to unify epoch- and model-wise double descent. EMC measures memorization capacity, which plays a key role in post-hoc reversal. EMC increases with epochs and model size. Further, EMC increases with epochs more rapidly for constant LR than annealed LR, explaining our observations in SS 4.2.3.

## 6 Post-Hoc Selection: Leveraging Post-Hoc Reversal in Practice

Our findings from SS4 motivate the principle of _post-hoc selection_, where model development decisions take post-hoc transforms into account. For concreteness, we discuss the choice of checkpoints from training runs under the SWA+TS and SWA+Ens+TS transforms. Checkpoint selection reduces to the selection of the final epoch \(\widehat{T}\), as SWA uses all checkpoints up to that epoch. \(\mathcal{M}_{\text{val}}\) denotes a metric of choice computed on the validation set.

**SWA+TS.** Naive selection picks epoch \(\widehat{T}=\arg\min_{T}\mathcal{M}_{f}^{\text{val}}(\boldsymbol{\uptheta}_{T})\). In contrast, post-hoc selection picks \(\widehat{T}=\arg\min_{T}\mathcal{M}_{\mathcal{T}_{\text{S+T}}\circ f}^{\text{ val}}((\boldsymbol{\uptheta}_{t})_{t=1}^{T})\).

**SWA+Ens+TS.** Here we have \(N\) different training runs to pick epochs for. Naive selection picks \(\widehat{T}_{j}=\arg\min_{T}\mathcal{M}_{f}^{\text{val}}(\boldsymbol{\uptheta} _{T}^{j})\) for each run independently. In contrast, post-hoc selection would ideally pick \(\widehat{T}_{1},\dots,\widehat{T}_{N}=\arg\min_{T_{1},\dots,T_{N}}\mathcal{M} _{\mathcal{T}_{\text{S+T}}\circ f}^{\text{val}}((\boldsymbol{\uptheta}_{t}^{1 })_{t=1}^{T_{1}},\dots,(\boldsymbol{\uptheta}_{t}^{N})_{t=1}^{T_{N}})\) which jointly minimizes the ensemble performance. This being computationally expensive, we instead minimize under the constraint \(\widehat{T}_{1}=\dots=\widehat{T}_{N}\)6

Footnote 6: Alternatively, one can select \(\widehat{T}_{j}=\arg\min_{T}\mathcal{M}_{\mathcal{T}_{\text{S+T}}\circ f}^{ \text{val}}(\boldsymbol{\uptheta}_{1}^{j},\dots,\boldsymbol{\uptheta}_{T}^{j })\) as a hybrid between post-hoc selection (within runs) and naive selection (across runs).

**Results.** Tab. 1 compares naive and post-hoc selection strategies for CIFAR-N and FMoW. Except for some clean label sets, post-hoc selection is always better than naive selection, often with \(>2\times\) improvement from post-hoc selection as compared to naive selection. It remains effective with out

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline \multirow{2}{*}{Metric \(\rightarrow\)} & \multicolumn{6}{c}{Test Loss} & \multicolumn{6}{c}{Test Error (\%)} \\ \cline{2-10}  & None & \multicolumn{2}{c}{SWA+TS} & \multicolumn{2}{c}{SWA+Ens+TS} & \multicolumn{2}{c}{None} & \multicolumn{2}{c}{SWA+TS} & \multicolumn{2}{c}{SWA+Ens+TS} \\ \cline{2-10}  & & Naive & Ours & Naive & Ours & & Naive & Ours & Naive & Ours \\ \hline C-10-N Clean & 0.435 & **0.269** & 0.270 & 0.234 & **0.233** & 9.75 & **9.09** & 9.10 & 8.30 & **8.24** \\ C-10-N Aggre & 0.722 & 0.663 & **0.585** & 0.608 & **0.543** & 19.20 & 17.08 & **16.95** & 15.88 & **15.74** \\ C-10-N Randl & 1.009 & 0.968 & **0.907** & 0.916 & **0.859** & 28.63 & 27.13 & **24.84** & 24.80 & **25.50** \\ C-10-N Worst & 1.511 & 1.483 & **1.443** & 1.437 & **1.399** & 46.84 & 46.12 & **44.14** & 44.30 & **42.88** \\ \hline C-100-N Clean & 1.508 & 1.215 & **1.205** & 1.065 & **1.063** & 33.83 & **32.67** & 32.69 & **29.90** & 29.94 \\ C-100-N Noisy & 2.416 & 2.289 & **2.136** & 2.129 & **1.994** & 58.68 & 54.94 & **53.18** & 51.34 & **50.26** \\ \hline FMoW (ID) & 1.583 & 1.627 & **1.554** & 1.494 & **1.305** & 43.20 & 42.69 & **39.92** & 37.95 & **34.93** \\ FMoW (ODD) & 1.831 & 1.840 & **1.788** & 1.700 & **1.571** & 49.32 & 49.70 & **46.75** & 46.74 & **41.56** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Naive vs post-hoc (ours) selection for SWA+TS and SWA+Ens+TS transforms. Better values are in bold. Except some clean cases, post-hoc selection is always better, often more than doubling the improvement over no transform. See Tabs. 6 and 8 in App. E for standard deviations.

of-distribution (OOD) val/test sets, as seen for FMoW (we use ID and OOD splits from WILDS [34]). For some datasets, like C-100-N Noisy, post-hoc selection is only marginally better on test error. Often, in such cases, the error floor is already quite high (e.g., C-100-N Noisy has \(\sim 40\%\) noise and ResNet-18 has \(\sim 10\%\) error on clean C-100, so a test error of \(\sim 50\%\) is already impressive), and test loss is a more appropriate metric.

**Early stopping.** We advocate monitoring post-hoc metrics for early stopping. Only a running average needs to be updated for SWA, and TS involves a quick single-parameter optimization. Further, while the base curves can fluctuate wildly between consecutive runs, SWA+TS curves are considerably smoother (see Figs. 3, 11 and 10), making them more reliable for automated early stopping. One can similarly monitor metrics for SWA+Ens+TS under parallel training runs.

## 7 Experiments Across Domains and Modalities

In SS 4 and SS 6, we introduced post-hoc reversal and selection with experiments on the CIFAR-N and FMoW datasets. In this section, we supplement our experimental analysis with additional experiments across diverse domains and modalities to demonstrate the generality of our findings.

### LLM Instruction Tuning

Language models are pre-trained or fine-tuned with a self-supervised objective of predicting the next token in a text corpus. There might be many acceptable tokens following a given prefix, albeit with different probabilities. Thus next token prediction is noisy and one might reasonably expect to see post-hoc reversal. In this section, we test this hypothesis for the task of fine-tuning LLMs to follow instructions (instruction tuning [72]). Instruction tuning datasets are naturally small [85] and amenable to multi-epoch training where catastrophic overfitting becomes an important concern. Recent works [53; 81] have argued for data repetitions for LLM pre-training as well, but such experiments are beyond the scope of this paper.

**Experimental setup.** We fine-tune LLaMA-2-7B [70] on the Guanaco dataset [12] of chat completions. We evaluate perplexity and causal language modeling (CLM) error on the test set, and also the MMLU accuracy [24] to better contextualize model improvements. Fig. 9 shows the curves. Tab. 7 in App. E gives exact numbers, and App. F explores sub-epoch checkpointing. For TS, we use a shared temperature parameter to scale the logits of all tokens and leave more involved strategies like _long-horizon temperature scaling_[66] to future work.

**Observations.** We observe post-hoc reversal between epochs 1 and 2 for perplexity and error, and between epochs 2 and 3 for MMLU. Both SWA+TS and SWA+Ens+TS transforms show significant improvements, much of which is only realized under post-hoc selection.

### Other Text, Tabular and Graph Datasets

In this section, we further expand our experimental coverage to text, tabular and graph classification datasets from real-world applications.

Figure 9: Perplexity and causal language modeling (CLM) error on the Guanaco test set, and MMLU accuracy (higher is better) for instruction tuning LLaMA-2-7B. Shading indicates post-hoc reversal. Base and SWA+TS curves are mean of \(8\) runs; SWA+Ens+TS ensembles models from these runs. Individual runs are not shown as they have high variance (see Tab. 7 in App. E).

**Experimental setup.** We consider the following tasks: (1) sentiment classification on the Yelp reviews dataset [5] (text) with a pre-trained transformer BERT [13], (2) prediction tasks on census data from Folktables [14] (tabular) with MLPs and (3) community detection on the Reddit and Collab datasets [82] (graph) with graph neural networks (GNNs). Folktables has 5 prediction tasks: Income, PublicCoverage, Mobility, Employment and TravelTime. Reddit has 2 versions: Reddit-5k and Reddit-12k. For more details, see App. C. Figure 10 shows curves for Yelp, Income and Reddit-12k. Tab. 5 in App. D compares naive and post-hoc selection on all datasets.

**Observations.** Post-hoc reversal is a recurring feature across datasets, transforms and metrics. The 3 datasets show different patterns between the base and post-hoc curves, showing that post-hoc reversal can take a variety of forms.

## 8 Conclusion

We empirically studied temperature scaling (TS), ensembling, stochastic weight averaging (SWA) and their compositions, and found that these transforms can reverse model peformance trends (post-hoc reversal). Based on our findings, we presented the simple technique of post-hoc selection, and showed that it outperforms naive selection. We validated our findings and proposals over diverse settings.

Our work has broad implications for the field of deep learning. It shows that current practices surrounding the use of post-hoc transforms leave much room for improvement. This is especially true for noisy data, which is pervasive in real-world applications. Future directions include better strategies for checkpoint selection, developing a theoretical understanding, investigating impacts on scaling laws, and characterizing other instances of post-hoc reversal.

**Summary of practical recommendations.** We advocate for the use of TS, ensembling and SWA across deep learning applications. Further, such transforms should be tightly integrated into the model development pipeline, following the methodology outlined in the paper. In particular: (1) apply SWA+TS and SWA+Ens+TS transforms for better results in the single- and multi-model settings respectively; (2) track temperature-scaled loss to overcome loss-error mismatch; (3) monitor post-hoc metrics to avoid premature early stopping; (4) make hyperparameter decisions informed by post-transform performance; (5) use post-hoc selection to pick model checkpoints.

Figure 10: Test curves for 3 real-world noisy datasets. Note that the pre-TS loss is significantly higher than the post-TS loss. Examples of post-hoc reversal between the base curves given by the solid blue lines and the post-hoc curves given by the dashed orange lines (SWA ensemble): (1) optimal epoch is different for base and post-hoc curves for error and post-TS loss on all datasets; (2) for error on Yelp, base curve shows double descent but post-hoc curve does not; (3) for error on Income, base curve overfits catastrophically at approx. epoch \(5\) but post-hoc curve continues improving till approx. epoch \(20\); (4) for error on Reddit-12k, base curve does not show double descent but post-hoc curve does.

## Acknowledgements

ZL acknowledges Amazon AI, Salesforce Research, Facebook, UPMC, Abridge, the PwC Center, the Block Center, the Center for Machine Learning and Health, and the CMU Software Engineering Institute (SEI) via Department of Defense contract FA8702-15-D-0002, for their generous support of ACMI Lab's research on machine learning under distribution shift.

## References

* Abe et al. [2023] Taiga Abe, E. Kelly Buchanan, Geoff Pleiss, and John P Cunningham. Pathologies of predictive diversity in deep ensembles. _ArXiv_, abs/2302.00704, 2023.
* Alexandari et al. [2020] Amr Alexandari, Anshul Kundaje, and Avanti Shrikumar. Maximum likelihood with bias-corrected calibration is hard-to-beat at label shift adaptation. In _International Conference on Machine Learning_, pages 222-232. PMLR, 2020.
* Allen-Zhu and Li [2020] Zeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. _arXiv preprint arXiv:2012.09816_, 2020.
* Arpit et al. [2022] Devansh Arpit, Huan Wang, Yingbo Zhou, and Caiming Xiong. Ensemble of averages: Improving model selection and boosting performance in domain generalization. _Advances in Neural Information Processing Systems_, 35:8265-8277, 2022.
* Asghar [2016] Nabiha Asghar. Yelp dataset challenge: Review rating prediction. _arXiv preprint arXiv:1605.05362_, 2016.
* Cha et al. [2021] Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee, and Sungrae Park. Swad: Domain generalization by seeking flat minima. _Advances in Neural Information Processing Systems_, 34:22405-22418, 2021.
* Chen et al. [2021] John Chen, Qihan Wang, and Anastasios Kyrillidis. Mitigating deep double descent by concatenating inputs. _Proceedings of the 30th ACM International Conference on Information & Knowledge Management_, 2021.
* Chen et al. [2023] Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al. Alpagasus: Training a better alpaca with fewer data. _arXiv preprint arXiv:2307.08701_, 2023.
* Christie et al. [2018] Gordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee. Functional map of the world. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 6172-6180, 2018.
* Cohen et al. [2021] Jeremy M. Cohen, Simran Kaur, Yuanzhi Li, J. Zico Kolter, and Ameet Talwalkar. Gradient descent on neural networks typically occurs at the edge of stability. _ArXiv_, abs/2103.00065, 2021.
* Dehghani et al. [2023] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In _International Conference on Machine Learning_, pages 7480-7512. PMLR, 2023.
* Dettmers et al. [2023] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient finetuning of quantized LLMs. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* Devlin et al. [2018] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* Ding et al. [2021] Frances Ding, Moritz Hardt, John Miller, and Ludwig Schmidt. Retiring adult: New datasets for fair machine learning. _Advances in Neural Information Processing Systems_, 34, 2021.
* Feinman [2021] Reuben Feinman. Pytorch-minimize: a library for numerical optimization with autograd, 2021. URL https://github.com/rfeinman/pytorch-minimize.

* [16] Stanislav Fort, Huiyi Hu, and Balaji Lakshminarayanan. Deep ensembles: A loss landscape perspective. _arXiv preprint arXiv:1912.02757_, 2019.
* [17] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. _arXiv: Learning_, 2018.
* [18] Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P Vetrov, and Andrew G Wilson. Loss surfaces, mode connectivity, and fast ensembling of dnns. _Advances in neural information processing systems_, 31, 2018.
* [19] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In _International conference on machine learning_, pages 1321-1330. PMLR, 2017.
* [20] Trevor J. Hastie, Robert Tibshirani, and Jerome H. Friedman. The elements of statistical learning. 2001.
* [21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [22] Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. Bag of tricks for image classification with convolutional neural networks. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 558-567, 2019.
* [23] Reinhard Heckel and Fatih Yilmaz. Early stopping in deep networks: Double descent and how to eliminate it. _ArXiv_, abs/2007.10099, 2020.
* [24] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. _arXiv preprint arXiv:2009.03300_, 2020.
* [25] Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E Hopcroft, and Kilian Q Weinberger. Snapshot ensembles: Train 1, get m for free. _arXiv preprint arXiv:1704.00109_, 2017.
* [26] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4700-4708, 2017.
* [27] Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali Farhadi, and Ludwig Schmidt. Patching open-vocabulary models by interpolating weights. _Advances in Neural Information Processing Systems_, 35:29262-29277, 2022.
* [28] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. _arXiv preprint arXiv:1803.05407_, 2018.
* [29] Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender: Ensembling large language models with pairwise ranking and generative fusion. _arXiv preprint arXiv:2306.02561_, 2023.
* [30] Lu Jiang, Di Huang, Mason Liu, and Weilong Yang. Beyond synthetic noise: Deep learning on controlled noisy labels. In _International Conference on Machine Learning_, 2019.
* [31] Jared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu, and Dario Amodei. Scaling laws for neural language models. _ArXiv_, abs/2001.08361, 2020.
* [32] Amr Khalifa, Michael C Mozer, Hanie Sedghi, Behnam Neyshabur, and Ibrahim Alabdulmohsin. Layer-stack temperature scaling. _arXiv preprint arXiv:2211.10193_, 2022.
* [33] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. _arXiv preprint arXiv:1609.02907_, 2016.

* [34] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In _International Conference on Machine Learning_, pages 5637-5664. PMLR, 2021.
* [35] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big transfer (bit): General visual representation learning. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part V 16_, pages 491-507. Springer, 2020.
* [36] Dan Kondratyuk, Mingxing Tan, Matthew Brown, and Boqing Gong. When ensembling smaller models is more efficient than single large models. _arXiv preprint arXiv:2005.00570_, 2020.
* [37] Andreas Kopf, Yannic Kilcher, Dimitri von Rutte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richard Nagyfi, et al. Openassistant conversations-democratizing large language model alignment. _arXiv preprint arXiv:2304.07327_, 2023.
* [38] Ludmila I. Kuncheva and Christopher J. Whitaker. Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy. _Machine Learning_, 51:181-207, 2003.
* [39] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In _Neural Information Processing Systems_, 2016.
* [40] Kuang-Huei Lee, Xiaodong He, Lei Zhang, and Linjun Yang. Cleannet: Transfer learning for scalable image classifier training with label noise. _2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5447-5456, 2017.
* [41] Weishi Li, Yong Peng, Miao Zhang, Liang Ding, Han Hu, and Li Shen. Deep model fusion: A survey. _arXiv preprint arXiv:2309.15698_, 2023.
* [42] Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Early-learning regularization prevents memorization of noisy labels. _Advances in neural information processing systems_, 33:20331-20342, 2020.
* [43] Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Early-learning regularization prevents memorization of noisy labels. _ArXiv_, abs/2007.00151, 2020.
* [44] Sheng Liu, Zhihui Zhu, Qing Qu, and Chong You. Robust training under label noise by over-parameterization. _ArXiv_, abs/2202.14026, 2022.
* [45] Sheng Liu, Zhihui Zhu, Qing Qu, and Chong You. Robust training under label noise by over-parameterization. In _International Conference on Machine Learning_, pages 14153-14172. PMLR, 2022.
* [46] Shikun Liu, Linxi Fan, Edward Johns, Zhiding Yu, Chaowei Xiao, and Anima Anandkumar. Prismer: A vision-language model with an ensemble of experts. _arXiv preprint arXiv:2303.02506_, 2023.
* [47] Raphael Gontijo Lopes, Yann Dauphin, and Ekin Dogus Cubuk. No one representation to rule them all: Overlapping features of training methods. _ArXiv_, abs/2110.12899, 2021.
* [48] Keming Lu, Hongyi Yuan, Runji Lin, Junyang Lin, Zheng Yuan, Chang Zhou, and Jingren Zhou. Routing to the expert: Efficient reward-guided ensemble of large language models. _arXiv preprint arXiv:2311.08692_, 2023.
* [49] Xiaoding Lu, Adian Liusie, Vyas Raina, Yuwen Zhang, and William Beauchamp. Blending is all you need: Cheaper, better alternative to trillion-parameters llm. _arXiv preprint arXiv:2401.02994_, 2024.
* [50] Neil Rohit Mallinar, James B. Simon, Amirhesam Abedsoltan, Parthe Pandit, Mikhail Belkin, and Preetum Nakkiran. Benign, tempered, or catastrophic: A taxonomy of overfitting. _ArXiv_, abs/2207.06569, 2022.

* Melville and Mooney [2003] Prem Melville and Raymond J. Mooney. Constructing diverse classifier ensembles using artificial training examples. In _International Joint Conference on Artificial Intelligence_, 2003.
* Morris et al. [2020] Christopher Morris, Nils M Kriegre, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. _arXiv preprint arXiv:2007.08663_, 2020.
* Muennighoff et al. [2023] Niklas Muennighoff, Alexander M. Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. _ArXiv_, abs/2305.16264, 2023.
* Nakkiran et al. [2020] Preetum Nakkiran, Prayaag Venkat, Sham M. Kakade, and Tengyu Ma. Optimal regularization can mitigate double descent. _ArXiv_, abs/2003.01897, 2020.
* Nakkiran et al. [2021] Preetum Nakkiran, Gal Kaplan, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep double descent: Where bigger models and more data hurt. _Journal of Statistical Mechanics: Theory and Experiment_, 2021(12):124003, 2021.
* Ovadia et al. [2019] Yaniv Ovadia, Emily Fertig, Jie Jessie Ren, Zachary Nado, D. Sculley, Sebastian Nowozin, Joshua V. Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift. In _Neural Information Processing Systems_, 2019.
* 24663, 2020.
* Power et al. [2022] Alethea Power, Yuri Burda, Harrison Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. _ArXiv_, abs/2201.02177, 2022.
* Qu'etu and Tartaglione [2023] V Qu'etu and E Tartaglione. Can we avoid double descent in deep neural networks. _arXiv preprint arXiv:2302.13259_, 2023.
* Radford and Narasimhan [2018] Alec Radford and Karthik Narasimhan. Improving language understanding by generative pre-training. 2018.
* Rame et al. [2022] Alexandre Rame, Matthieu Kirchmeyer, Thibaud Rahier, Alain Rakotomamonjy, Patrick Gallinari, and Matthieu Cord. Diverse weight averaging for out-of-distribution generalization. _Advances in Neural Information Processing Systems_, 35:10821-10836, 2022.
* Rame et al. [2024] Alexandre Rame, Nino Vieillard, Leonard Hussenot, Robert Dadashi, Geoffrey Cideron, Olivier Bachem, and Johan Ferret. Warm: On the benefits of weight averaged reward models. _arXiv preprint arXiv:2401.12187_, 2024.
* Ranjan [2023] Rishabh Ranjan. torchcal: post-hoc calibration on GPU, 2023. URL https://github.com/rishabh-ranjan/torchcal.
* Sanyal et al. [2023] Sunny Sanyal, Atula Tejaswi Neerkaje, Jean Kaddour, Abhishek Kumar, et al. Early weight averaging meets high learning rates for llm pre-training. In _Workshop on Advancing Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization (WANT@ NeurIPS 2023)_, 2023.
* Schaeffer et al. [2023] Rylan Schaeffer, Mikail Khona, Zachary Robertson, Akhilan Boopathy, Kateryna Pistunova, Jason W. Rocks, Ila Rani Fiete, and Oluwasanmi Koyejo. Double descent demystified: Identifying, interpreting & ablating the sources of a deep learning puzzle. _ArXiv_, abs/2303.14151, 2023.
* Shih et al. [2023] Andy Shih, Dorsa Sadigh, and Stefano Ermon. Long horizon temperature scaling. In _International Conference on Machine Learning_, pages 31422-31434. PMLR, 2023.
* Siththaranjan et al. [2023] Anand Siththaranjan, Cassidy Laidlaw, and Dylan Hadfield-Menell. Distributional preference learning: Understanding and accounting for hidden context in rlhf. _arXiv preprint arXiv:2312.08358_, 2023.

* [68] Hwanjun Song, Minseok Kim, and Jae-Gil Lee. Selfie: Refurbishing unclean samples for robust deep learning. In _International Conference on Machine Learning_, 2019.
* [69] Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. Learning from noisy labels with deep neural networks: A survey. _IEEE Transactions on Neural Networks and Learning Systems_, 2022.
* [70] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* [71] Dongdong Wang, Boqing Gong, and Liqiang Wang. On calibrating semantic segmentation models: Analyses and an algorithm. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 23652-23662, 2023.
* [72] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. _ArXiv_, abs/2109.01652, 2021.
* [73] Jiaheng Wei, Zhaowei Zhu, Hao Cheng, Tongliang Liu, Gang Niu, and Yang Liu. Learning with noisy labels revisited: A study using real-world human annotations. _ArXiv_, abs/2110.12088, 2021.
* [74] Jiaheng Wei, Zhaowei Zhu, Hao Cheng, Tongliang Liu, Gang Niu, and Yang Liu. Learning with noisy labels revisited: A study using real-world human annotations. _arXiv preprint arXiv:2110.12088_, 2021.
* [75] Ross Wightman, Hugo Touvron, and Herve Jegou. Resnet strikes back: An improved training procedure in timm. _arXiv preprint arXiv:2110.00476_, 2021.
* [76] Andrew G Wilson and Pavel Izmailov. Bayesian deep learning and a probabilistic perspective of generalization. _Advances in neural information processing systems_, 33:4697-4708, 2020.
* [77] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In _International Conference on Machine Learning_, pages 23965-23998. PMLR, 2022.
* [78] Ruixuan Xiao, Yiwen Dong, Haobo Wang, Lei Feng, Runze Wu, Gang Chen, and Junbo Zhao. Promix: Combating label noise via maximizing clean sample utility. In Edith Elkind, editor, _Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-23_, pages 4442-4450. International Joint Conferences on Artificial Intelligence Organization, 8 2023. doi: 10.24963/ijcai.2023/494. Main Track.
* [79] Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning from massive noisy labeled data for image classification. _2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 2691-2699, 2015.
* [80] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? _arXiv preprint arXiv:1810.00826_, 2018.
* [81] Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, and Yang You. To repeat or not to repeat: Insights from scaling llm under token-crisis. _ArXiv_, abs/2305.13230, 2023.
* [82] Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In _Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining_, pages 1365-1374, 2015.
* [83] Mert Yukekgonul, Linjun Zhang, James Y. Zou, and Carlos Guestrin. Beyond confidence: Reliable models should also consider atypicality. _ArXiv_, abs/2305.18262, 2023.
* [84] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In _International Conference on Machine Learning_, pages 12697-12706. PMLR, 2021.

* [85] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. _arXiv preprint arXiv:2305.11206_, 2023.

Expanded Related Work

**Phenomena.** Empirical works like double descent [55], grokking [58], scaling laws [31], neural-collapse [57], edge-of-stability [10], lottery-ticket-hypothesis [17] have revealed both challenges and opportunities for improving the understanding and practices of deep neural network training. Post-hoc reversal expands this list as a novel phenomenon regarding learning dynamics under the lens of post-hoc transforms. It is most intimately connected with double descent, offering a way to mitigate it. Some works [7, 23, 54, 59, 65, 76] show other mitigations, such as regularization and data augmentation.

**Temperature Scaling (TS).** TS belongs to a family of post-hoc calibration techniques [2, 19, 32, 66, 83], with the unique property of preserving classification error. Recently, calibration has been applied to large vision and language models [11, 71, 84]. While loss-error mismatch has been reported before [11, 19], to the best of our knowledge, we are the first to report post-hoc reversal with TS.

**Ensembling.** Ensembling is a foundational technique in machine learning, encompassing bagging, boosting, etc. In deep learning, a uniform ensemble is most popular [3, 39], although recent work on ensembling LLMs has explored more efficient routing-based ensembles [29, 46, 48, 49]. Various works have explored strategies to form optimal ensembles [36, 47, 51, 77], generally based on model diversity [38], but recently Abe et al. [1] have warned against this. In contrast, our recommendation for forming ensembles relies directly on the validation performance of the ensemble, introducing no proxies, and still being computationally cheap.

**Stochastic Weight Averaging (SWA).** SWA [28] is the culmination of a line of work [18, 25] which seek to cheaply approximate ensembling. It has inspired numerous works which average weights in some form [4, 6, 27, 41, 61, 77] often in combination with ensembling. Recently, weight averaging has shown up in the LLM space [62, 64]. While these works generally apply SWA with a fixed training time determined independently, we present SWA in the role of early stopping and model selection. In practice, SWA has often been found to be unreliable7, and is often skipped from training recipes even when considered [35, 75]. Our work sheds some light on this, offering a rather counter-intuitive choice of models to include in the weight average for best results.

Footnote 7: See, for example, discussion at https://discuss.huggingface.co/t/improvements-with-swa/858.

**Noise.** Many training strategies have been introduced to deal with noisy data (see [69] for a survey). However, the efficacy of simple post-hoc transforms has been left unexplored. Further, most of these works are motivated by labeling errors, which leaves some of the core practical considerations for dealing with general noisy data unaddressed. For instance, access to a clean validation set is assumed and test loss is overlooked as an important metric [43, 44]. We also entirely avoid experiments on synthetic noise, informed by recent work which questions the transferability of findings to realistic noise patterns [30, 73]. Some recent datasets [30, 40, 68, 73, 79] make it possible to study realistic noise along with known noise estimates. Noise due to insufficient information in the input context (Fig. 11) has also been studied under different settings, such as for RLHF [67].

**Multi-epoch training of LLMs.** Multi-epoch training of LLMs runs into severe catastrophic overfitting. Xue et al. [81] examine the contributing factors and explore possible solutions. They find that regularization is not helpful, except for dropout. Muennighoff et al. [53] study scaling laws considering data repetitions. Complementarily, we put forward post-hoc transforms as an effective solution with our post-hoc selection methodology. This is especially important for fine-tuning LLMs, e.g. in instruction tuning [72], where [85] and [8] advocate for fine-tuning with a smaller amount of higher quality samples for more epochs.

## Appendix B Expanded Preliminaries and Background

### Learning on Noisy Data

Figures 12, 11 and 13 illustrate various sources of noise: aleatoric uncertainty, epistemic uncertainty and annotation errors. Below we provide some background on Bayes-optimal classifier and use it to introduce the clean error metric and Bayes loss/error as measures of noise level.

**Bayes-optimal classifier.**\(f_{\mathcal{D}}\), given by \(f_{\mathcal{D}}(\mathbf{x})_{k}=\log\mathcal{P}_{\mathcal{D}}[y=k\mid\mathbf{x}]\) minimizes both \(\mathcal{M}_{\mathcal{D}}^{\text{error}}\) and \(\mathcal{M}_{\mathcal{D}}^{\text{loss}}\), and is called the _Bayes-optimal classifier_ for \(\mathcal{D}\). The _Bayes error_\(\mathcal{M}_{\mathcal{D}}^{\text{error}}[f_{\mathcal{D}}]\) and _Bayes loss_\(\mathcal{M}_{\mathcal{D}}^{\text{loss}}[f_{\mathcal{D}}]\) are measures of the noise level. \(y^{*}(\mathbf{x})=\arg\max_{k}f_{\mathcal{D}}(\mathbf{x})_{k}\) is sometimes called the _clean label_. Using \(y^{*}\), one may define the _clean data distribution_\(\widetilde{\mathcal{D}}\) with \(\mathcal{P}_{\widetilde{\mathcal{D}}}[\mathbf{x}]=\mathcal{P}_{\mathcal{D}}[ \mathbf{x}]\) and \(\mathcal{P}_{\widetilde{\mathcal{D}}}[y\mid\mathbf{x}]=\mathbf{1}\{y=y^{*}( \mathbf{x})\}\). The _clean error_\(\mathcal{M}_{\mathcal{D}}^{\text{error}}\) is a common metric in the label noise literature but not a focus of our work as \(y^{*}\) is typically inaccessible in more general noisy settings.

### Post-Hoc Transforms in Machine Learning

The explicit forms of the composed transforms SWA+TS and SWA+Ens+TS (denoted as \(\mathcal{T}_{\text{S+T}}\) and \(\mathcal{T}_{\text{S+E+T}}\)) are given by Equations 4 and 5 respectively. For \(\mathcal{T}_{\text{S+E+T}}\), parameters \(\uptheta_{1}^{l},\ldots,\uptheta_{K_{l}}^{l}\) are weight-averaged and the \(L\) resulting models are ensembled, followed by temperature scaling. \(\tau_{l}\) is the temperature for weight-averaged models, and \(\tau_{\text{Ens}}\) is the temperature for the ensemble. As before, they are obtained by optimizing the cross-entropy loss over the validation set, with model parameters fixed.

\[\left(\mathcal{T}_{\text{S+T}}\circ f\right)\left(\mathbf{x};\uptheta_{1}, \ldots,\uptheta_{K}\right)=\frac{1}{\tau}f\left(\mathbf{x};\frac{1}{K}\sum_{i= 1}^{K}\uptheta_{i}\right),\text{ with }\tau=\arg\min_{\tau}\mathcal{M}_{\text{val}}^{\text{ loss}}\left[\frac{1}{\tau}f\left(\cdot;\frac{1}{K}\sum_{i=1}^{K}\uptheta_{i} \right)\right]\] (4)

\[\left(\mathcal{T}_{\text{S+E+T}}\circ f\right)\left(\mathbf{x};\uptheta_{1}^{ 1},\ldots,\uptheta_{K_{1}}^{1},\ldots,\uptheta_{1}^{L},\ldots,\uptheta_{K_{L }}^{L}\right)=\frac{1}{\tau_{\text{Ens}}}\frac{1}{L}\sum_{l=1}^{L}\frac{1}{ \tau_{l}}f\left(\mathbf{x};\frac{1}{K_{l}}\sum_{k=1}^{K_{l}}\uptheta_{k}^{l}\right)\] (5)

Figure 11: Data can be noisy due to insufficient information in the input context (epistemic uncertainty). Figures 10(a) and 10(b) show satellite images from the FMoW dataset. The labels are correct, as corroborated by external map data. However, they cannot be determined with full certainty from the images alone.

Figure 12: Data can be noisy due to non-determinism in the prediction target (aleatoric uncertainty). Figure shows a message tree from the OpenAssistant Conversations (OASST1) Dataset. A chatbot can continue a conversation satisfactorily in many different ways, making next token prediction noisy.

[MISSING_PAGE_EMPTY:19]

human labels for the original 50k CIFAR-10/100 train images, we split these into 40k/5k/5k images for train/val/test sets.

**FMoW [9, 34].** This is the version of the original FMoW dataset [9] as used in the WILDS benchmark [34]. For FMoW (ID) we use the in-distribution val and test sets, and for FMoW (OOD), we use the out-of-distribution val and test sets, where the val set is shifted with respect to the train set, and the test set is shifted with respect to both the train and val sets. All splits are as provided by WILDS. The input is an RGB satellite image (rescaled to 224 x 224 pixels) and the label is one of 62 building or land use categories. The labels were obtained by a combination of human annotation and cross-referenced geographical information. The original dataset provides additional metadata about location, time, sun angles, physical sizes, etc. which is ignored in the WILDS dataset (and hence in ours). While the labels have low noise compared to the ground-truth, this dataset is noisy because of insufficient information. It is hard to disambiguate the building or land use category with full certainty by looking at the satellite image alone. See Figure 11. Models and training setup are as used in [9, 34], except for the LR schedule, where we experiment with multiple alternatives.

**Guanaco [12].** This is a subset of the OASST1 dataset [37] containing only the highest-rated paths in the conversation tree. We follow the fine-tuning setup from [12], except that we use vanilla fine-tuning without any quantization or low-rank adapters.

**Yelp [5].** This is a subset of the Yelp Dataset Challenge 2015 dataset with 25k reviews in the train set and 5k reviews each in the validation and test sets. The input is a review text and the label is one of 5 classes (1 to 5 stars). Assigning a rating to a review is intrinsically non-deterministic as different reviewers might have different thresholds for the star ratings. This introduces noise in the data.

**Folktables [14].** Folktables consists of 5 classification tasks based on the US Census: Income, Employment, Health, TravelTime and PublicCoverage. The data is tabular. The available feature columns do not contain sufficient information to predict the targets with full certainty, even if the Census recorded the ground-truth labels with high accuracy. This results in noise.

**Collab and Reddit [52, 82].** These datasets are from TUDataset [52], and were originally introduced by Yanardag and Vishwanathan [82]. Collab is a scientific collaboration dataset. The input is an ego-network of a researcher and the label is the field of the researcher (one of High Energy Physics, Condensed Matter Physics and Astro Physics). The Reddit-5k and Reddit-12k datasets (originally called REDDIT-MULTI-5K and REDDIT-MULTI-12K) are balanced datasets where the input is a graph which corresponds to an online discussion thread from the social network site Reddit. Nodes correspond to users and there is an edge if one user responded to another's comment. The task is to predict which subreddit a discussion graph belongs to. Reddit-5k is smaller with 5k examples and 5 classes. Reddit-12k is bigger with 12k examples and 11 classes.

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline Metric \(\rightarrow\) & \multicolumn{6}{c}{Test Loss} & \multicolumn{6}{c}{Test Error (\%)} \\ \cline{2-10} Transform \(\rightarrow\) & None & SWA+TS & SWA+Ens+TS & None & SWA+TS & SWA+Ens+TS \\ Dataset \(\downarrow\) & & Naive & Ours & Naive & Ours & Naive & Ours & Naive & Ours \\ \hline Yelp & 0.908 & 0.890 & **0.854** & 0.841 & **0.824** & 39.41 & 38.02 & **37.33** & 36.18 & **36.14** \\ \hline Income & 0.393 & 0.390 & **0.387** & 0.388 & **0.385** & 17.84 & 17.69 & **17.54** & 17.62 & **17.40** \\ PublicCoverage & 0.544 & 0.540 & **0.539** & 0.538 & **0.538** & 27.52 & 27.31 & **27.25** & 27.25 & **27.02** \\ Mobility & 0.474 & 0.472 & **0.471** & 0.471 & 0.468 & 21.43 & **21.38** & 21.42 & **21.17** & 21.24 \\ Employment & 0.380 & 0.379 & **0.378** & 0.378 & **0.377** & 17.94 & **17.77** & 17.80 & **17.72** & 17.83 \\ TravelTime & 0.597 & 0.597 & **0.593** & 0.596 & **0.591** & 35.77 & 35.46 & **35.35** & 35.44 & **35.23** \\ \hline Collab & 0.492 & 0.475 & **0.460** & 0.439 & **0.404** & 20.65 & 21.58 & **20.27** & 20.40 & **18.80** \\ Reddit-5k & 1.154 & 1.112 & **1.100** & 1.101 & **1.085** & 47.42 & 48.35 & **47.04** & 47.09 & **45.49** \\ Reddit-12k & 1.405 & 1.381 & **1.366** & 1.367 & **1.346** & 51.78 & **51.08** & 51.11 & **50.34** & 51.26 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Naive vs post-hoc (ours) selection for SWA+TS and SWA+Ens+TS transforms on some real-world datasets. Better values are in bold.

[MISSING_PAGE_EMPTY:21]

Figs. 13(a) and 13(b) show the results. We find that a checkpointing interval of 0.7 epochs gives the best results, with higher and lower intervals performing slightly worse. This makes sense--higher intervals include too few checkpoints for SWA, lower ones include too many weaker checkpoints from earlier in training.

Also, we find that the optimal epoch is shifted further at smaller checkpointing intervals (by about 2 epochs when the checkpointing interval is 0.1 epochs), showing that **post-hoc reversal is even more important** in this setting. This is likely because with more checkpoints being averaged, even more overfitted checkpoints can be accomodated while still increasing the overall performance.

## Appendix G Visualizing Post-Hoc Reversal on a Synthetic Dataset

Here, we replicate post-hoc reversal on a synthetic dataset with 2 input features, with the aim of visualizing learnt decision surfaces to solidify our intuitions.

\begin{table}
\begin{tabular}{l l r r r r} \hline \hline \multicolumn{2}{l}{\multirow{2}{*}{
\begin{tabular}{} \end{tabular} }} & \multicolumn{4}{c}{Test Loss} & \multicolumn{4}{c}{Test Error (\%)} \\ \hline Dataset \(\downarrow\) & Select \(\downarrow\) & Epochs & Base & Final & Gain & Epochs & Base & Final & Gain \\ \hline \multirow{3}{*}{FMoW (ID)} & Naive & \(2_{\pm 0}\) & \(1.583_{\pm 0.014}\) & \(1.494\) & \(0.089_{\pm 0.014}\) & \(15_{\pm 19}\) & \(43.20_{\pm 0.46}\) & \(37.95\) & \(5.24_{\pm 0.46}\) \\  & Post-hoc & \(50\) & \(2.831_{\pm 0.053}\) & \(1.305\) & \(1.526_{\pm 0.053}\) & \(48\) & \(43.18_{\pm 0.55}\) & \(34.93\) & \(8.24_{\pm 0.55}\) \\  & \(\Delta\) & \(\uparrow 48_{\pm 0}\) & \(\uparrow 1.248_{\pm 0.062}\) & \(\downarrow 0.189\) & – & \(\uparrow 33_{\pm 10}\) & \(\downarrow 0.02_{\pm 0.80}\) & \(\downarrow 3.02\) & – \\ \hline \multirow{3}{*}{FMoW (OOD)} & Naive & \(2_{\pm 0}\) & \(1.831_{\pm 0.018}\) & \(1.700\) & \(0.131_{\pm 0.018}\) & \(3_{\pm 1}\) & \(49.32_{\pm 0.38}\) & \(46.74\) & \(2.58_{\pm 0.38}\) \\  & Post-hoc & \(50\) & \(3.399_{\pm 0.050}\) & \(1.571\) & \(1.828_{\pm 0.050}\) & \(50\) & \(500_{\pm 0.38}\) & \(41.56\) & \(8.52_{\pm 0.38}\) \\  & \(\Delta\) & \(\uparrow 48_{\pm 0}\) & \(\uparrow 1.567_{\pm 0.054}\) & \(\downarrow 0.129\) & – & \(\uparrow 47_{\pm 1}\) & \(0.75_{\pm 0.66}\) & \(\downarrow 5.19\) & – \\ \hline \hline \multirow{3}{*}{Yelp} & Naive & \(2_{\pm 1}\) & \(0.908_{\pm 0.008}\) & \(0.841\) & \(0.067_{\pm 0.008}\) & \(9_{\pm 8}\) & \(39.41_{\pm 0.76}\) & \(36.18\) & \(3.23_{\pm 0.76}\) \\  & Post-hoc & \(3\) & \(0.990_{\pm 0.044}\) & \(0.824\) & \(0.166_{\pm 0.044}\) & \(3\) & \(40.28_{\pm 1.29}\) & \(36.14\) & \(4.14_{\pm 1.29}\) \\  & \(\Delta\) & \(\uparrow 1_{\pm 1}\) & \(\uparrow 0.082_{\pm 0.040}\) & \(\downarrow 0.017\) & – & \(\downarrow 6_{\pm 8}\) & \(\uparrow 0.87_{\pm 1.16}\) & \(\downarrow 0.04\) & – \\ \hline \hline \multirow{3}{*}{Income} & Naive & \(5_{\pm 1}\) & \(0.393_{\pm 0.001}\) & \(0.388\) & \(0.005_{\pm 0.001}\) & \(7_{\pm 2}\) & \(17.84_{\pm 0.15}\) & \(17.62\) & \(0.22_{\pm 0.15}\) \\  & Post-hoc & \(11\) & \(0.421_{\pm 0.007}\) & \(0.385\) & \(0.036_{\pm 0.007}\) & \(19\) & \(19.21_{\pm 0.14}\) & \(17.40\) & \(1.81_{\pm 0.14}\) \\  & \(\Delta\) & \(\uparrow 6_{\pm 1}\) & \(\uparrow 0.028_{\pm 0.006}\) & \(\downarrow 0.003\) & – & \(\uparrow 1_{\pm 2}\) & \(\uparrow 1.37_{\pm 0.22}\) & \(\downarrow 0.22\) & \(\downarrow 0.22\) \\ \hline \multirow{3}{*}{Public Coverage} & Naive & \(10_{\pm 2}\) & \(0.544_{\pm 0.001}\) & \(0.538\) & \(0.006_{\pm 0.001}\) & \(12_{\pm 3}\) & \(27.52_{\pm 0.24}\) & \(27.25\) & \(0.28_{\pm 0.24}\) \\  & Post-hoc & \(18\) & \(0.554_{\pm 0.002}\) & \(0.538\) & \(0.016_{\pm 0.002}\) & \(22\) & \(27.96_{\pm 0.21}\) & \(27.02\) & \(0.94_{\pm 0.21}\) \\  & \(\Delta\) & \(\uparrow 8_{\pm 2}\) & \(\uparrow 0.010_{\pm 0.002}\) & \(\downarrow 0.000\) & – & \(\uparrow 10_{\pm 3}\) & \(\uparrow 0.44_{\pm 0.25}\) & \(\downarrow 0.22\) & – \\ \hline \multirow{3}{*}{Mobility} & Naive & \(6_{\pm 2}\) & \(0.474_{\pm 0.002}\) & \(0.471\) & \(0.003_{\pm 0.002}\) & \(13_{\pm 5}\) & \(21.43_{\pm 0.18}\) & \(21.17\) & \(0.26_{\pm 0.18}\) \\  & Post-hoc & \(14\) & \(0.476_{\pm 0.003}\) & \(0.468\) & \(0.008_{\pm 0.003}\) & \(11\) & \(21.40_{\pm 0.17}\) & \(21.24_{\pm 0.16}\) & \(0.16_{\pm 0.17}\) \\  & \(\Delta\) & \(\uparrow 8_{\pm 2}\) & \(\uparrow 0.002_{\pm 0.003}\) & \(\downarrow 0.003\) & – & \(\downarrow 2_{\pm 5}\) & \(\downarrow 0.03_{\pm 0.22}\) & \(\uparrow 0.07\) & – \\ \hline \multirow{3}{*}{Employment} & Naive & \(8_{\pm 1}\) & \(0.380_{\pm 0.000}\) & \(0.378\) & \(0.003_{\pm 0.000}\) & \(14_{\pm 4}\) & \(17.94_{\pm 0.08}\) & \(17.72\) & \(0.22_{\pm 0.08}\) \\  & Post-hoc & \(15\) & \(0.383_{\pm 0.001}\) & \(0.377\) & \(0.006_{\pm 0.001}\) & \(30\) & \(18.27_{\pm 0.12}\) & \(17.83\) & \(0.43_{\pm 0.12}\) \\  & \(\Delta\) & \(\uparrow 7_{\pm 1}\) & \(\uparrow 0.003_{\pm 0.001}\) & \(\downarrow 0.000\) & – & \(\uparrow 16_{\pm 4}\) & \(\uparrow 0.33_{\pm 0.16}\) & \(\uparrow 0.11\) & – \\ \hline \multirow{3}{*}{Travel Time} & Naive & \(6_{\pm 2}\) & \(0.597_{\pm 0.002}\) & \(0.596\) & \(0.001_{\pm 0.002}\) & \(9_{\pm 1}\) & \(35.77_{\pm 0We train 4-layer MLPs with 512 ReLU units per hidden layer on a 2-class spirals dataset of 1000 training examples, with 20% of the labels flipped at random. We train 16 MLPs and track the mean test error across epochs, as well as the test error of the ensemble (Fig. 15).

As per [3, 16] ensembling and SWA help when the data has a "multi-view" structure, or equivalently, the loss landscape has multiple modes. This is hard to achieve for 2D datasets, so instead we simulate the effect by training each MLP on a random 50% subsample of the training data.

Fig. 16 shows decision surfaces at epochs 440 and 1000 for 2 MLPs and the ensemble. Decision boundaries are spiky around noisy examples and smoother around clean ones. While the generalizable parts of the spiral are retained in the ensemble, the effects of noisy examples are diminished. Between epochs 440 and 1000, individual models spike around noisy examples more prominently than they learn new parts of the spiral, but the ensemble surface is relatively unchanged, except for small improvements to learning the spiral.

This reinforces our intuitions from SS 5 that mislabeled examples have a more unstable influence on the decision boundary, and post-hoc transforms exploit this to reduce their impact, while amplifying generalizable patterns learnt from clean examples.

Figure 16: Decision surfaces of 2 models and the ensemble (of 16 models) on a synthetic 2D dataset of spirals, at epochs 440 and 1000, between which post-hoc reversal occurs (Fig. 15).

Figure 14: Best MMLU and epoch at which it is achieved vs checkpointing interval, for the LLM instruction tuning setup of § 7.1. Checkpointing every 0.7 epochs gives the best results. Best epoch is shifted further at smaller checkpointing intervals, i.e. post-hoc reversal is more prominent in this setting.

Figure 15: The synthetic dataset setup in § G exhibits post-hoc reversal between epochs \(440\) and \(1000\).

## Appendix H Noise-Aware Training

While our experiments in the main paper use the standard cross-entropy (CE) loss, here we consider two leading training objectives from the label noise literature: (1) SOP [45] and (2) ELR [42]. Tables 9, 10 and 11 compare naive and post-hoc selection strategies for CIFAR-N datasets under CE, SOP and ELR losses respectively. Here again we find that post-hoc selection is superior to naive selection in general. We also note that the differences between CE, SOP and ELR are minimal. This is likely because we use i.i.d. (and therefore noisy) validation and test sets, unlike the original papers which use clean validation and test sets.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline Metric \(\rightarrow\) & \multicolumn{6}{c}{Test Loss} & \multicolumn{6}{c}{Test Error (\%)} \\ \cline{2-10} Transform \(\rightarrow\) & None & SWA+TS & SWA+Ens+TS & None & SWA+TS & SWA+Ens+TS \\ \cline{2-10} Dataset \(\downarrow\) & & Naive & Ours & Naive & Ours & Naive & Ours & Naive & Ours \\ \hline C-10-N Clean & 0.425 & 0.270 & **0.269** & 0.236 & **0.235** & 9.65 & 8.82 & **8.81** & **7.96** & 8.00 \\ C-10-N Agree & 0.728 & 0.693 & **0.573** & 0.634 & **0.541** & 18.03 & **16.55** & 16.56 & 15.85 & **15.56** \\ C-10-N Rand1 & 1.025 & 0.980 & **0.888** & 0.925 & **0.851** & 26.91 & 24.53 & **24.50** & 23.20 & **23.14** \\ C-10-N Rand2 & 1.045 & 1.015 & **0.920** & 0.957 & **0.883** & 27.39 & 25.34 & **25.25** & **24.12** & 24.16 \\ C-10-N Rand3 & 1.016 & 0.975 & **0.889** & 0.921 & **0.851** & 26.64 & **24.23** & 24.23 & 23.02 & **22.96** \\ C-10-N Worst & 1.514 & 1.492 & **1.451** & 1.447 & **1.413** & 46.78 & 46.26 & **44.29** & 44.50 & **42.78** \\ \hline Clean & 1.018 & 0.742 & **0.686** & 0.623 & **0.608** & 23.07 & **21.43** & 21.47 & **19.18** & 19.78 \\ Noisy & 1.427 & 1.347 & **1.229** & 1.247 & **1.145** & 41.39 & 38.01 & **35.85** & 34.32 & **33.94** \\ \hline C-100-N Clean & 1.513 & 1.213 & **1.203** & 1.063 & **1.061** & 33.79 & **32.66** & 32.68 & **29.46** & 29.56 \\ C-100-N Noisy & 2.415 & 2.268 & **2.137** & 2.118 & **1.997** & 58.34 & 54.76 & **53.48** & 51.06 & **50.54** \\ \hline \hline \end{tabular}
\end{table}
Table 10: Naive vs post-hoc (ours) selection for CIFAR-N trained with **SOP** loss. Better values are in bold.

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline Metric \(\rightarrow\) & \multicolumn{6}{c}{Test Loss} & \multicolumn{6}{c}{Test Error (\%)} \\ \cline{2-10} Transform \(\rightarrow\) & None & SWA+TS & SWA+Ens+TS & None & SWA+TS & SWA+Ens+TS \\ \cline{2-10} Dataset \(\downarrow\) & & Naive & Ours & Naive & Ours & Naive & Ours & Naive & Ours \\ \hline C-10-N Clean & 0.435 & **0.269** & 0.270 & 0.234 & **0.233** & 9.75 & **9.09** & 9.10 & 8.30 & **8.24** \\ C-10-N Agree & 0.722 & 0.663 & **0.585** & 0.608 & **0.543** & 19.20 & 17.08 & **16.95** & 15.88 & **15.74** \\ C-10-N Rand1 & 1.009 & 0.968 & **0.907** & 0.916 & **0.850** & 28.63 & 27.13 & **24.84** & 24.80 & **23.50** \\ C-10-N Rand2 & 1.040 & 0.983 & **0.935** & 0.931 & **0.888** & 29.91 & 27.60 & **25.69** & 25.44 & **24.12** \\ C-10-N Rand3 & 1.005 & 0.963 & **0.911** & 0.910 & **0.864** & 28.96 & 26.91 & **25.09** & 24.86 & **23.44** \\ C-10-N Worst & 1.511 & 1.483 & **1.443** & 1.437 & **1.399** & 46.84 & 46.12 & **44.14** & 44.30 & **42.88** \\ \hline Clean & 1.011 & 0.786 & **0.686** & 0.669 & **0.606** & 23.12 & **21.30** & 21.38 & **19.36** & 19.52 \\ Noisy & 1.431 & 1.330 & **1.235** & 1.234 & **1.150** & 41.42 & 38.08 & **35.87** & 34.42 & **33.54** \\ \hline C-100-N Clean & 1.508 & 1.215 & **1.205** & 1.065 & **1.063** & 33.83 & **32.67** & 32.69 & **29.90** & 29.94 \\ C-100-N Noisy & 2.416 & 2.289 & **2.136** & 2.129 & **1.994** & 58.68 & 54.94 & **53.18** & 51.34 & **50.26** \\ \hline \hline \end{tabular}
\end{table}
Table 9: Naive vs post-hoc (ours) selection for CIFAR-N trained with **cross-entropy (CE)** loss. Better values are in bold.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline Metric \(\rightarrow\) & \multicolumn{6}{c}{Test Loss} & \multicolumn{6}{c}{Test Error (\%)} \\ \cline{2-10} Transform \(\rightarrow\) & None & SWA+TS & SWA+Ens+TS & None & SWA+TS & SWA+Ens+TS \\ \cline{2-10} Dataset \(\downarrow\) & & Naive & Ours & Naive & Ours & Naive & Ours & Naive & Ours \\ \hline C-10-N Clean & 0.435 & **0.269** & 0.270 & 0.234 & **0.233** & 9.75 & **9.09** & 9.10 & 8.30 & **8.24** \\ C-10-N Agree & 0.722 & 0.663 & **0.585** & 0.608 & **0.543** & 19.20 & 17.08 & **16.95** & 15.88 & **15.74** \\ C-10-N Rand1 & 1.009 & 0.968 & **0.907** & 0.916 & **0.850** & 28.63 & 27.13 & **24.84** & 24.80 & **23.50** \\ C-10-N Rand2 & 1.040 & 0.983 & **0.935** & 0.931 & **0.888** & 29.91 & 27.60 & **25.69** & 25.44 & **24.12** \\ C-10-N Rand3 & 1.005 & 0.963 & **0.911** & 0.910 & **0.864** & 28.96 & 26.91 & **25.09** & 24.86 & **23.44** \\ C-10-N Worst & 1.511 & 1.483 & **1.443** & 1.437 & **1.399** & 46.84 & 46.12 & **44.14** & 44.30 & **42.88** \\ \hline \hline \end{tabular}
\end{table}
Table 9: Naive vs post-hoc (ours) selection for CIFAR-N trained with **cross-entropy (CE)** loss. Better values are in bold.

Limitations

We find post-hoc reversal to be an important phenomenon when the base curve exhibits performance degradation due to overfitting. However, under some scenarios, the base curve shows a monotonic improvement in performance with additional training (or increasing model size). Examples include: (1) the data has low noise, (2) the training is heavily regularized, and (3) there is an abundance of data, so that a single data point is not repeated enough to cause overfitting. In such cases, post-hoc selection outcomes are similar to naive selection. Since our suggested approach only ensembles models trained for the same number of epochs during post-hoc selection, it does not subsume the naive selection search space, leading to marginally worse performance sometimes, although this can be easily overcome in practice.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims made in the abstract and introduction accurately reflect the paper's contributions and scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Please see App. I. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: [NA] Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Code to reproduce experiments is available at https://anonymous.4open.science/r/post-hoc-reversal. Comprehensive dataset and training details are provided in App. C. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Please see code (including instructions to download data and reproduce the main results) at https://anonymous.4open.science/r/post-hoc-reversal. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Please see App. C. In particular, see Tab. 3 for training details. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Please see Tabs. 6 and 8 for detailed results with standard deviations. We don't report error bars for ensembles as it is computationally prohibitive to train many independent ensembles. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Please see App. C for training details, including compute resources used. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss the broader impact of our work in Sec. 8. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All datasets and models used in this paper are under permissive licenses allowing for research use. We have credited their authors by exhaustively citing sources. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.

* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.