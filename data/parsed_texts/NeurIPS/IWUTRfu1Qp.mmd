## 1 Introduction

Numerous large language models (LLMs), both closed-source and open-source (OpenAI, 2023; Touvron et al., 2023), are now available to the community. Evaluating their alignment with human preferences is crucial for selecting appropriate models in downstream applications (Ouyang et al., 2022). To meet this need, Chatbot Arena (Chiang et al., 2024) provides an open platform for evaluating LLMs based on human preferences. However, it typically takes weeks or even months for a newly released LLM to collect statistically enough human votes.

To reduce reliance on human annotations, automatic LLM benchmarks such as _AlpacaEval 2.0_(Dubois et al., 2024), _Arena-Hard-Auto_(Li et al., 2024b), and _MT-Bench_(Zheng et al., 2023) use LLM-based auto-annotators to evaluate language models. These automatic benchmarks are cheap, scalable, and have high Spearman correlations with Chatbot Arena (Li et al., 2023c). These advantages make them popular choices for providing timely assessments of newly released LLMs (Meng et al., 2024; Chen et al., 2024a), where high win rates can lead to significant _promotional benefits_.

While automatic benchmarks offer a valuable way for comparing LLMs, recent studies have revealed that auto-annotated win rates can be affected by biases related to output length and style (Dubois et al., 2024; Chen et al., 2024b; Zhang et al., 2024). In most cases, these biases are unintentional, stemming from the training data distribution; however, they can still game win rates, causing leaderboard results to deviate from actual human preferences. To mitigate this issue, several strategies have been introduced to control for output length and disentangle style from content, thereby reducing the potential for gameability (Dubois et al., 2024; Li et al., 2024a).

But, what if an adversary _intentionally_ cheats auto-annotators to achieve high win rates and capitalize on the resulting promotional benefits? In this study, we conduct stress tests on these benchmarks by submitting **"null models"** that, instead of responding to input instructions, generate **constant** outputs. Our initial experiments use ChatGPT to craft dozens of _persuasive_ responses (Zeng et al., 2024) expecting auto-annotators to favor them and gain high win rates. Note that persuasive responses do not respond to input instructions, so human annotators will assign them zero win rates.

We submit these persuasive responses to AlpacaEval 2.0 after wrapping them as null models. For instance, a null model NullModel("Pick me:") always returns the same output "Pick me!" for all the \(805\) input instructions in AlpacaEval 2.0, without providing any informative response. As seen in Figure 1(b), the AlpacaEval 2.0 auto-annotator (GPT-4-1106-preview) is robust to these persuasive responses, assigning win rates of less than \(1\%\).

Nevertheless, we find that **structured cheating responses** can cheat the auto-annotator by exploiting a weakness in LLMs, which may become confused during syntactic analysis when processing the evaluation templates, such as those used in AlpacaEval 2.0. A manually crafted cheating response that is structured can already achieve a \(76.8\%\) LC win rate, as seen in Figure 1(c).

We further modify this structured response by adding a prefix and optimizing it through random search based on querying results from GPT-4 (Andriushchenko et al., 2024; Zheng et al., 2024). To simulate more challenging scenarios, we assume that all input instructions of the automatic benchmarks are _private_. Thus, we craft a **transferable** prefix using a public set of instructions from UltraFeedback (Cui et al., 2023). We then evaluate this optimized prefix, concatenated with the structured cheating responses, by testing it on AlpacaEval 2.0, Arena-Hard-Auto, and MT-Bench as reported in Table 2. Additionally, we use open-source LLMs like Llama-3-Instruct (Meta, 2024; Touvron et al., 2023) as auto-annotators and conduct further ablation studies to verify our findings.

Anti-cheating has long been a critical consideration when designing the rules for leaderboards (Blum and Hardt, 2015), but this remains unexplored in the context of LLM benchmarks. While our experiments in this paper are primarily proof-of-concept, a determined adversary could leverage LLMs to generate more subtle and imperceptible cheating responses (Liu et al., 2023; Chao et al., 2023), unethically gaining high win rates and promotional advantages. Our findings highlight the urgent need to develop robust anti-cheating mechanisms to ensure reliable automatic LLM benchmarks.

## 2 Preliminaries

**LLM-based auto-annotators.** We focus on the problem of evaluating outputs from LLMs using auto-annotators. Formally, we define a model \(\mathtt{LLM}:\mathcal{X}^{*}\rightarrow\mathcal{X}^{*}\) as a function that transforms an input sequence of tokens into an output sequence of tokens, where \(\mathcal{X}\) is the vocabulary. Given an instruction \(I\in\mathcal{X}^{*}\), the LLM generates a response \(\mathtt{LLM}(I)\in\mathcal{X}^{*}\). To evaluate these responses, we introduce an auto-annotator function \(\mathtt{JUDGE}:\mathcal{X}^{*}\rightarrow\mathcal{P}(\mathcal{Y})\), where \(\mathcal{Y}\) represents the evaluation output space, and \(\mathcal{P}(\mathcal{Y})\) denotes the space of probability distributions over \(\mathcal{Y}\). For instance, in _MT-Bench_, there is \(\mathcal{Y}=\{1,2,...,10\}\), representing a score range; while in _AlpacaEval 2.0_, there is \(\mathcal{Y}=\{\mathrm{m},\mathrm{M}\}\), indicating binary judgments. The auto-annotator assesses the instruction \(I\), the response from the target model \(\mathtt{LLM}_{\text{tar}}(I)\), and optionally, the response from a reference model \(\mathtt{LLM}_{\text{ref}}(I)\). The output of the auto-annotator is either \(\mathtt{JUDGE}(I||\mathtt{LLM}_{\text{tar}}(I))\), evaluating the target model alone, or \(\mathtt{JUDGE}(I||\mathtt{LLM}_{\text{ref}}(I)||\mathtt{LLM}_{\text{tar}}(I))\), comparing the target and reference models to compute win rates.

**Threat model of cheating.** The cheater is assumed to have no direct access to the auto-annotator's parameters but can query the auto-annotator through an API provided by a service provider. Additionally, the cheater has no access to the test input instructions. The cheater's goal is to craft a _null model_ and manipulate the auto-annotator's evaluation to favor the **constant, non-informative** response outputs from the null model, rather than preferring the responses from the reference model.

**Experimental setup.** Our experiments utilize the official evaluation templates associated with different LLM-based evaluations unless stated otherwise. We evaluate our cheating method on AlpacaEval 2.0 (Li et al., 2023; Dubois et al., 2024), Arena-Hard-Auto (Li et al., 2024), and MT-Bench (Zheng et al., 2023) as detailed in Table 1. These benchmarks assess the models' ability to handle a wide range of conversational tasks across diverse query sets and have gained widespread adoption within the research community. We adhere to each benchmark's evaluation criteria when reporting our results. For AlpacaEval 2.0, we present both the raw win rate and the length-controlled (LC) win rate, with the LC one designed to mitigate bias from model verbosity. For Arena-Hard-Auto, we report the win rate against a reference model. Additionally, we provide the first-turn score for MT-Bench, using GPT-4-Preview-1106 as the auto-annotator model. The targeted auto-annotators include both open-source and closed-source LLMs: Llama-3-8B-Instruct,Llama-3-70B-Instruct(Meta, 2024; Touvron et al., 2023), and GPT-4-1106-Preview(OpenAI, 2023). Each LLM uses its default generation configuration with a temperature setting of \(0.0\). For Llama-3 auto-annotators, we use 4-bit quantized versions to reduce GPU memory usage.1 All experiments were conducted on \(8\times\) NVIDIA A100 (40G) GPUs within a few hours using vLLM as the inference engine, and the tokenization template was sourced from Hugging Face tokenizers.

Footnote 1: The quantized models are Meta-Llama-3-8B-Instruct-AWQ and Meta-Llama-3-70B-Instruct-AWQ.

## 3 Cheating strategies

Our initial experiments in Figure 2 indicate that using only an optimized adversarial suffix (without informative responses to input instructions) is ineffective on AlpacaEval 2.0 when GPT-4 acts as the auto-annotator. To address this limitation, our cheating strategies include: (1) constructing structured cheating responses to confuse widely used LLM auto-annotators, and (2) conducting token-level random search to craft the adversarial prefix, as outlined below:

**Structured cheating responses**. As shown in Figure 1, our cheating strategy involves replacing the original comparison with a misleading one, which disrupts the auto-annotator's syntactic analysis of the evaluation template and steers its judgment away from the intended outcomes. The response is carefully structured to be _resilient against swap operations_. For instance, on AlpacaEval 2.0, when

Figure 1: **Auto-annotator’s template of AlpacaEval 2.0**, which is fed into GPT-4-Preview-1106 to implement JUDGE. The placeholders {instruction} is filled in by each of the \(805\) input instructions \(I\), while in the _default_ setting, {output.1} is the reference model’s response \(\text{LLM}_{\text{ref}}(I)\) and {output.2} is the target model’s response \(\text{LLM}_{\text{ref}}(I)\). The _swap_ setting changes the order of outputs. In our experiments, the target model is instantiated by null models as NullModel(const_str), where const_str is either a **persuasive response** (**baseline**) or a **structed cheating response** (**ours**).

the submitted response is positioned last, the annotator predicts "M". Conversely, when it appears in the first position, the annotator predicts "m". The optimized response exhibits the following key properties: (1) It overrides the original instruction-output triplet with a fabricated one; (2) When positioned by default, it exploits the annotator's general preference for the last output, guiding it to predict "M"; (3) When swapped, it takes advantage of overwriting the output from model "M", causing the annotator to predict "m". The full template and final submission files are presented in Figures 7, 8 and 9. This structured response alone achieves a \(76.8\%\)_LC win rate on AlpacaEval 2.0_. Moreover, the response can be concatenated with an adversarial prefix to enhance the cheating effectiveness.

**Crafting adversarial prefix by random search (RS)**. To further improve the structured response, we incorporate an adversarial prefix and optimize it using an RS strategy based on GPT-4 query results. To emulate a more challenging scenario, we assume that the input instructions from the automatic benchmarks remain private. Therefore, we develop a transferable prefix, crafted using a publicly available instruction set. Our approach optimizes a single adversarial prefix by aggregating the losses over various instructions, ensuring that the prefix's impact is universal across different input instructions and positions. We utilize an RS algorithm to optimize the adversarial prefix (Zou et al., 2023; Andriushchenko et al., 2024; Zheng et al., 2024). The algorithm refines the prefix by sampling modifications and selecting the variant that minimizes the aggregated loss across multiple instructions. This process is detailed in Algorithm 1.

## 4 Cheating GPT-4 based automatic LLM benchmarks

GPT-4 models are the most widely used state-of-the-art auto-annotators, valued for their powerful evaluation capabilities. To assess the generality of our cheat, we applied it to a range of automatic LLM benchmarks, using the GPT-4-1106-Preview model as the auto-annotator. For RS, we set the number of training instructions \(N\) as \(10\), \(8\), and \(4\), the number of optimization steps \(T\) as \(384\), \(96\) and \(64\) for AlpacaEval 2.0, Arena-Hard-Auto and MT-Bench, respectively. The full templates and structured responses for Arena-Hard-Auto and MT-Bench are presented in Figures 10 and 11.

**The effectiveness of our structured response.** As mentioned in Section 3, we employ a structured response to facilitate the cheating, which provides a good initial point and could reduce the optimization cost. To further demonstrate the effectiveness of our structured cheating response, we evaluate \(\log p(\texttt{winner}=\texttt{NullModel})\) on a sampled subset of the AlpacaEval 2.0 test instructions using different null responses. We compare our structured response with the other 16 persuasive responses, as shown in Figure 3. The results highlight the superiority of our structured response (marked as "Ours") because it achieves the lowest log probabilities. This demonstrates the effectiveness of our structured response in cheating the auto-annotator to favor our null model. Additionally, Figure 3 shows that the default configuration, where the baseline is placed second and the target model the last, tends to have lower losses, suggesting a preference for the second-position response. This highlights the position bias of the GPT-4-based auto-annotator, which often favors the last response.

**Empirical results.** The results of our experiments, summarized in Table 2, underscore the effectiveness of our method across various benchmarks. On AlpacaEval 2.0, our structured responses achieved a LC win rate of \(76.8\%\) and a raw win rate of \(59.5\%\). After integrating RS optimization, the LC win rate increased to \(86.5\%\), and the raw win rate improved to \(76.9\%\). These results represent significant improvements compared to the verified SOTA model, which achieves only \(57.5\%\)LC and \(51.3\%\) raw win rates. Our structured approach with random search outperforms the verified SOTA \(29.0\) percentage points in LC win rate and \(25.6\) in raw win rate. Compared to the community SOTA, our method achieves better performance in LC (\(86.5\%\) vs. \(78.5\%\)) and is comparable in raw win rates (\(76.9\%\) vs. \(77.6\%\)). Additionally, the LC win rates of our cheats are generally higher than the raw win rates because of their short length, which highlights that AlpacEval 2.0 is also not robust to length cheat. On the Arena-Hard-Auto, our structured approach achieves a win rate of \(67.2\%\), which increases to \(83.0\%\) after the random search. This is particularly notable because our final win rate matches the performance of the verified SOTA model, which stands at \(82.6\%\). For the MT-Bench, our structured responses initially achieve an average score of \(7.75\), which increases to \(9.55\) with random search optimization. This brings the score greatly outperforming the verified SOTA score of \(8.96\). In summary, our method achieves substantial gains over the state-of-the-art approaches, demonstrating its effectiveness across various benchmarks, and reinforcing the need for more robust automatic LLM benchmarks.

## 5 Ablation studies on open-source auto-annotators

To better understand the mechanism behind our method, we conduct extensive ablation studies on auto-annotators based on open-source LLMs. We focus on open-source LLma-3-instruct (8B, 70B

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline \multirow{2}{*}{**Target model**} & \multicolumn{3}{c}{**AlpacEval 2.0\({}^{\star}\)**} & \multicolumn{3}{c}{**Arena-Hard-Auto\({}^{\alpha}\)**} & **MT-Bench\({}^{\dagger}\)** \\ \cline{2-7}  & LC & Win Rate & Discrete & Win Rate & 95\% CI & avg \#tokens & Score \\ \hline Verified SOTA & 57.5 & 51.3 & 53.8 & 82.6 & (-1.9, +2.0) & 662 & 8.96 \\ Community SOTA & 78.5 & 77.6 & 79.5 & - & - & - \\ \hline Structured (**Ours**) & 76.8 & 59.5 & 64.2 & 67.2 & (-1.7, 1.2) & 198 & 7.75 \\ Structured+RS (**Ours**) & **86.5** & **76.9** & **84.0** & **83.0** & (-1.1, 1.5) & 205 & **9.55** \\ \hline \hline \end{tabular}

* https://tatsu-lab.github.io/alpac_eval
* https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard
* https://lmsys.org/blog/2023-06-22-leaderboard
* https://lmsys.org/blog/2023-06-22-leaderboard
* https://lmsys.org/blog/2023-06-22-leaderboard
* https://lmsys.org/blog/2023-06-22-leaderboard

\end{table}
Table 2: **Summary of our results**. We present win rates and scores of our cheat, comparing them to the state-of-the-art models (_recorded before October 1st, 2024_). The evaluation is conducted using GPT-4-1106-Preview as the auto-annotator. For pairwise comparison benchmarks, including AlpacEval 2.0 and Arena-Hard-Auto, the reference models are GPT-4-1106-Preview and GPT-4-0314, respectively. We report the LC win rates, raw win rates, discrete win rates, and rating scores. Our structured response combined with random search (Structured+RS) significantly improves performance across all benchmarks, achieving the highest win rates and scores.

Figure 3: **Boxplot of the \(\log p(\texttt{winner}=\texttt{NullModel})\) using different null responses**. The response of each index can be found in Table 6. The target model’s responses are positioned in the second slot by “Default” and swapped to the first slot in “Swap”. Our structured response (marked as “Ours”) achieves the lowest log probabilities compared to the other 16 persuasive responses.

parameters) (Meta, 2024; Touvron et al., 2023). These models have been well-aligned by pair-wise preference data and show the ability to evaluate other LLMs.2 For RS, we set \(N=8\) and \(T=8192\).

Footnote 2: https://github.com/tatsu-lab/alpaca_eval/pull/314

**Sanity check**. Before we use Llama-3-Instruct models as our auto-annotator in the AlpacaEval framework, we conduct a sanity check to see whether they have such evaluation capability. We evaluate different automatic annotators on the AlpacaEval set by comparing 2.5K human annotations collected by Dubois et al. (2023). As shown in Table 3, both Llama-3-8B-Instruct and Llama-3-70B-Instruct show non-trivial human agreement and correlations. More concretely, Llama-3-8B-Instruct is comparable to ChatGPT, and Llama-3-70B-Instruct matches GPT-4 auto-annotator. Thus, it is reasonable to use them as the auto-annotators.

**Is the structured response useful on open-source auto-annotators?** We evaluate the \(\log p(\texttt{winner}=\texttt{NullModel})\) on a subset of the AlpacaEval 2.0 test instructions using different null responses. As shown in Figure 4, the structured response has little effect on Llama-3 auto-annotators. In the case of Llama-3-8B-Instruct, the structured response does not exploit positional weaknesses in this model as the log probabilities for the default and swapped positions are generally similar to different persuasive responses. However, on Llama-3-70B-Instruct, we observe that under the swap setting, the structured response manages to reduce the log probability. Additionally, regarding the positional bias, the Llama-3-8B-Instruct shows little position bias as the probabilities for both default and swapped positions are fairly close. In contrast, Llama-3-70B-Instruct shows a clear positional bias under the swapped setting, with a higher log probability, indicating the model's strong preference for the last output ("M"). The larger Llama-3-70B-Instruct model behaves more similarly to the more advanced GPT-4, as it demonstrates a greater response to both the structured response and positional bias than the smaller 8B model. This suggests that model size may contribute to the susceptibility to our cheating techniques. Overall, the structured response is considerably less effective on the Llama-3 models compared to GPT-4. A possible explanation for this difference is that the instruction-following capabilities of the Llama-3 models, especially the smaller 8B variant, are not as powerful as those of GPT-4, making them less prone to cheating responses.

**Is random search effective on open-source auto-annotators?** The results shown in Table 5 demonstrate the effectiveness of random search on open-source auto-annotators like LLama-3-8B-Instruct and LLama-3-70B-Instruct. For Llama-3-8B-Instruct, without random search, the structured response achieves only a \(2.9\%\) LC win rate and \(1.4\%\) raw win rate. However, when the random search is applied, the win rates surge dramatically to \(95.4\%\) (LC) and \(86.3\%\) (raw), representing a gain of \(92.5\) percentage points in the LC win rate. For Llama-3-70B-Instruct, the structured response alone yields minimal success with a \(0.4\%\) LC win rate and \(0.2\%\) overall. Once random search is applied, these win rates leap to \(95.1\%\) (LC) and \(91.6\%\) (raw), showcasing improvements of \(94.7\) and \(91.4\) percentage points, respectively. These results indicate that random search is highly effective in improving the cheat's success on open-source auto-annotators, driving win rates close to \(100\%\).

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline \multirow{2}{*}{**Auto-annotator**} & **Human** & **Spearman** & **Pearson** & \multirow{2}{*}{**Bias**} & \multirow{2}{*}{**Variance**} & \multirow{2}{*}{**Proba.**} \\  & **agreement** & **corr.** & & & & **prefer longer** \\ \hline GPT-4\({}^{*}\) & 69.2 & 0.97 & 0.93 & 28.4 & 14.6 & 0.68 \\ CoT-GPT-4-Turbo\({}^{*}\) & 68.6 & 0.97 & 0.90 & 29.3 & 18.4 & 0.67 \\ GPT-4-Turbo\({}^{*}\) & 68.1 & 0.93 & 0.82 & 30.2 & 15.6 & 0.65 \\ Human\({}^{*}\) & 65.7 & 1.00 & 1.00 & 0.0 & 34.3 & 0.64 \\ ChatGPT\({}^{*}\) & 57.3 & 0.72 & 0.71 & 39.4 & 34.1 & 0.59 \\ \hline Llama-3-8B-Instruct & 56.0 & 0.70 & 0.77 & 41.4 & 37.6 & 0.62 \\ Llama-3-70B-Instruct & 68.8 & 0.90 & 0.85 & 30.1 & 11.5 & 0.78 \\ \hline \hline \multicolumn{7}{l}{\({}^{*}\) These results are taken from https://github.com/tatsu-lab/alpaca_eval.} \\ \hline \hline \end{tabular}
\end{table}
Table 3: **Evaluation of auto-annotators vs. human annotations on AlpacaEval.** This table compares various auto-annotators to 2.5K human annotations. The human agreement metric measures how well each annotator aligns with the majority preferences of humans, based on approximately 650 examples with cross-annotations from four different human annotations per example. The spearman and pearson correlation metrics assess the correlation between the rankings generated by the auto-annotators and those produced by humans. Additionally, we report the annotators’ bias, variance, and the probability of preferring longer responses over shorter ones.

**Does searching on the test instructions directly help?** We also consider direct cheating. Direct cheating serves as an indicator of the upper bound of transfer cheating. The results shown in Table 4 clearly show that searching directly on the test instructions significantly boosts the cheat's performance. For the Llama-3-8B-Instruct model, using the structured response combined with random search without test instruction access achieves a strong LC win rate of \(95.4\%\) and an overall win rate of \(86.3\%\). However, when the adversarial prefix is optimized directly on the test instructions, the LC win rate jumps to an almost perfect \(99.8\%\), and the overall win rate increases to \(99.4\%\), representing gains of \(4.6\) and \(13.1\) percentage points, respectively. Similarly, for the Llama-3-70B-Instruct model, random search without access to test instructions results in an LC win rate of \(95.1\%\) and an overall win rate of \(91.6\%\). When the test instructions are used, these rates climb to \(99.4\%\) (LC) and \(98.2\%\) (raw), showing improvements of around \(4.3\) percentage points for LC and \(6.6\) for overall win rate. These results highlight that directly searching on the test instructions offers significant advantages, further optimizing the adversarial prefix and nearly achieving perfect performance.

**Can our method be combined with normal responses?** Our method can be combined with normal, informative responses by appending our cheating response to the original responses. As demonstrated in Figure 5, when combined with a more informative model like GPT-3.5-0613, we observe that the initial win rates are already high, even before significant optimization steps are taken. This is evident in Figure 4(b) and 4(d), where the performance (win rate and length-controlled win rate) increases steadily from a high baseline as optimization progresses. However, it is important to emphasize that our setting of using a null, non-informative model is far more challenging. In this setting (Figure 4(a) and 4(c)), the null model starts with much lower win rates because it offers no relevant information to the input queries, making it much harder to trick the auto-annotator. Despite this, as the optimization steps progress, the null model's performance steadily increases, ultimately achieving competitive win rates. This highlights the robustness of our method, showing that it can manipulate LLM-based benchmarks even in the most challenging scenario--where the model outputs irrelevant, non-informative responses. The success of our method under such difficult conditions makes it a valuable stress test of benchmark robustness.

Figure 4: **Boxplot of the \(\log p(\texttt{winner}=\texttt{NullModel})\) using different null responses across different responses and auto-annotators. The structured response (index=0) is not as effective for the Llama models as for GPT-4-1106-Preview. An interesting observation is that, on Llama-3-70B-Instruct, the structured response successfully reduces the log probability under the swap setting. In contrast, the structured response is ineffective on Llama-3-8B-Instruct for both positions, implying that its effectiveness may be related to the model’s ability to follow instructions.**
## 6 Discussion

**Anti-cheating strategies and related work** Due to page limits, we provide the details of anti-cheating strategies and related work at Section A and B in Appendix.

**Conclusion**. In this paper, we uncover even null models can achieve high win rates by exploiting structural weaknesses in the evaluation process. These findings highlight the need for more robust automatic LLM benchmarks to ensure fair and reliable assessments of LLM performance. As the field of AI continues to evolve, we must address these vulnerabilities to maintain trust in the systems we use to evaluate language models. Failure to do so could lead to widespread manipulation of benchmarks, undermining the progress and credibility of AI research. In summary, while automatic LLM benchmarks provide a scalable and efficient way to evaluate models, they are not immune to cheating. The development of anti-cheating mechanisms and the reconsideration of benchmark design will be crucial steps toward ensuring the reliability and fairness of future LLM evaluations.

**Limitations and future work**. Despite the promising findings of our study, there are limitations that must be acknowledged. First, our work primarily focuses on specific benchmarks, and while our results generalize well across them, the cheat's effectiveness on other, less-studied benchmarks remains uncertain. Additionally, our approach relies heavily on the manual crafting of structured responses. Future work could explore more automated methods for generating adversarial outputs, which would allow adversaries to exploit these vulnerabilities on a larger scale. One important area for future research is the development of more robust anti-cheating mechanisms. Current efforts to mitigate cheating on LLM benchmarks have focused on controlling output length and style, but these measures have proven insufficient in the face of structured responses. New defenses will be crucial for maintaining the integrity of LLM benchmarks.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \multirow{2}{*}{**Auto-annotator**} & \multirow{2}{*}{**Reference model**} & \multirow{2}{*}{**Target model**} & \multirow{2}{*}{**Test**} & \multicolumn{3}{c}{**AlpacaEval 2.0**} \\  & & & & LC & Win Rate & Discrete \\ \hline \multirow{4}{*}{\begin{tabular}{c} Llama-3 \\ 8B-Instruct \\ 390 \\ \end{tabular} } & GPT-4 & \begin{tabular}{c} GPT 3.5 Turbo (06/13) \\ \end{tabular} & - & 48.1 & 38.8 & 39.4 \\ \cline{3-6}  & & & Structured & ✗ & 2.9 & 1.4 & 0.7 \\ \cline{3-6}  & & Structured+RS & ✗ & **95.4** & **86.3** & **91.8** \\ \cline{3-6}  & & Structured+RS & ✓ & 99.8 & 99.4 & 99.9 \\ \hline \multirow{4}{*}{\begin{tabular}{c} Llama-3 \\ 70B-Instruct \\ 395 \\ \end{tabular} } & GPT-4 & 
\begin{tabular}{c} GPT 3.5 Turbo (06/13) \\ \end{tabular} & - & 30.5 & 19.7 & 19.8 \\ \cline{3-6}  & & Structured & ✗ & 0.4 & 0.2 & 0.0 \\ \cline{3-6}  & & Structured+RS & ✗ & **95.1** & **91.6** & **93.7** \\ \cline{3-6}  & & Structured+RS & ✓ & 99.4 & 98.2 & 99.5 \\ \hline \hline \end{tabular}
\end{table}
Table 4: **Win rates of the cheat against Llama-3-Instruct family.** We present the win rates of our cheat on AlpacaEval 2.0 when targeting models in the Llama-3-Instruct family. We evaluate different methods (Structured and Structured+Random Search) with and without access to test instructions. The results are measured using LC win rate, raw win rate, and discrete comparison metrics. We also explore the effect of different auto-annotators and random search optimization. The upper-bound win rates are approached by assuming the visibility of test instructions.

Figure 5: **Win rates along the number of steps across different models**. The win rates increase generally as the optimization steps grow. Notably, incorporating an informative model like GPT-3.5-0613 with our cheat has high initial win rates, indicating the challenge of our null model setting. Nonetheless, our cheat drives both models to over \(90\%\) win rates.

## References

* Adlakha et al. (2023) Vaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Nicholas Meade, and Siva Reddy. Evaluating correctness and faithfulness of instruction-following models for question answering. _ArXiv preprint_, 2023.
* Alon & Kamfonas (2023) Gabriel Alon and Michael Kamfonas. Detecting language model attacks with perplexity. _ArXiv preprint_, 2023.
* Alzantot et al. (2018) Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang. Generating natural language adversarial examples. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, 2018.
* Andriushchenko et al. (2024) Maksym Andriushchenko, Francesco Croce, and Nicolas Flammarion. Jailbreaking leading safety-aligned llms with simple adaptive attacks. _ArXiv preprint_, 2024.
* Anil et al. (2024) Cem Anil, Esin Durmus, Mrinank Sharma, Joe Benton, Sandipan Kundu, Joshua Batson, Nina Rimsky, Meg Tong, Jesse Mu, Daniel Ford, et al. Many-shot jailbreaking. In _Advances in Neural Information Processing Systems_, 2024.
* Bai et al. (2022) Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. _ArXiv preprint_, 2022.
* Blum & Hardt (2015) Avrim Blum and Moritz Hardt. The ladder: A reliable leaderboard for machine learning competitions. In _International Conference on Machine Learning_, 2015.
* Bubeck et al. (2023) Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. _ArXiv preprint_, 2023.
* Carlini et al. (2023) Nicholas Carlini, Milad Nasr, Christopher A Choquette-Choo, Matthew Jagielski, Irena Gao, Pang Wei Koh, Daphne Ippolito, Florian Tramer, and Ludwig Schmidt. Are aligned neural networks adversarially aligned? In _Advances in Neural Information Processing Systems_, 2023.
* Chao et al. (2023) Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J Pappas, and Eric Wong. Jailbreaking black box large language models in twenty queries. _ArXiv preprint_, 2023.
* Chen et al. (2024) Changyu Chen, Zichen Liu, Chao Du, Tianyu Pang, Qian Liu, Arunesh Sinha, Pradeep Varakantham, and Min Lin. Bootstrapping language models with dpo implicit rewards. _ArXiv preprint_, 2024a.
* Chen et al. (2024b) Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, and Benyou Wang. Humans or llms as the judge? a study on judgement biases. _ArXiv preprint_, 2024b.
* Chen et al. (2022) Yiming Chen, Chen Zhang, Danqing Luo, Luis Fernando D'Haro, Robby T Tan, and Haizhou Li. Unveiling the achilles' heel of nlg evaluators: A unified adversarial framework driven by large language models. _ArXiv preprint_, 2024c.
* Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot immersing gpt-4 with 90%* chatgpt quality. _See https://vicuna. Imsys. org (accessed 14 April 2023)_, 2023.
* Chiang et al. (2024) Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference. _ArXiv preprint_, 2024.
* Cohen et al. (2023) Roi Cohen, May Hamri, Mor Geva, and Amir Globerson. Lm vs lm: Detecting factual errors via cross examination. _ArXiv preprint_, 2023.
* Cui et al. (2023) Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback. _ArXiv preprint_, 2023.

* Deng et al. (2023) Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, and Lidong Bing. Multilingual jailbreak challenges in large language models. _ArXiv preprint_, 2023.
* Dubois et al. (2023) Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback, 2023.
* Dubois et al. (2024) Yann Dubois, Balazs Galambosi, Percy Liang, and Tatsunori B Hashimoto. Length-controlled al-pacaeval: A simple way to debias automatic evaluators. _ArXiv preprint_, 2024.
* Ebrahimi et al. (2018) Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. HotFlip: White-box adversarial examples for text classification. In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)_, 2018.
* Gade et al. (2023) Pranav Gade, Simon Lermen, Charlie Rogers-Smith, and Jeffrey Ladish. Badllama: cheaply removing safety fine-tuning from llama 2-chat 13b. _ArXiv preprint_, 2023.
* Gao et al. (2023) Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Shiping Yang, and Xiaojun Wan. Human-like summarization evaluation with chatgpt. _ArXiv preprint_, 2023.
* Greshake et al. (2023) Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz. Not what you've signed up for: Compromising real-world llm-integrated applications with indirect prompt injection. In _ACM Workshop on Artificial Intelligence and Security_, 2023.
* Gudibande et al. (2023) Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. The false promise of imitating proprietary llms. _ArXiv preprint_, 2023.
* Hayase et al. (2024) Jonathan Hayase, Ema Borevkovic, Nicholas Carlini, Florian Tramer, and Milad Nasr. Query-based adversarial prompt generation. _ArXiv preprint_, 2024.
* Huang et al. (2024) Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, and Danqi Chen. Catastrophic jailbreak of open-source lms via exploiting generation. In _International Conference on Learning Representations_, 2024.
* Jain et al. (2023) Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Sompealli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, and Tom Goldstein. Baseline defenses for adversarial attacks against aligned language models. _ArXiv preprint_, 2023.
* Jia and Liang (2017) Robin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems. In _Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing_, 2017.
* Jones et al. (2023) Erik Jones, Anca Dragan, Aditi Raghunathan, and Jacob Steinhardt. Automatically auditing large language models via discrete optimization. In _International Conference on Machine Learning_, 2023.
* Kim et al. (2023) Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et al. Prometheus: Inducing fine-grained evaluation capability in language models. In _The Twelfth International Conference on Learning Representations_, 2023.
* Kim et al. (2024) Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. Prometheus 2: An open source language model specialized in evaluating other language models. _ArXiv preprint_, 2024.
* Lapid et al. (2023) Raz Lapid, Ron Langberg, and Moshe Sipper. Open sesame! universal black box jailbreaking of large language models. _ArXiv preprint_, 2023.
* Lermen et al. (2023) Simon Lermen, Charlie Rogers-Smith, and Jeffrey Ladish. Lora fine-tuning efficiently undoes safety training in llama 2-chat 70b. _ArXiv preprint_, 2023.
* Li et al. (2023) Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. Halueval: A large-scale hallucination evaluation benchmark for large language models. _ArXiv preprint_, 2023a.

* Li et al. (2024a) Tianle Li, Anastasios Angelopoulos, and Wei-Lin Chiang. Does style matter? disentangling style and substance in chatbot arena, 2024a. https://lmsys.org/blog/2024-08-28-style-control/.
* Li et al. (2024b) Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E Gonzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. _ArXiv preprint_, 2024b.
* Li et al. (2024c) Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E Gonzalez, and Ion Stoica. From live data to high-quality benchmarks: The arena-hard pipeline, april 2024. _URL https://lmsys. org/blog/2024-04-19-arena-hard_, 2024c.
* Li et al. (2023b) Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, and Bo Han. Deepinception: Hypnotize large language model to be jailbreaker. _ArXiv preprint_, 2023b.
* Li et al. (2023c) Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 2023c.
* Liao & Sun (2024) Zeyi Liao and Huan Sun. Amplecgcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed lms. _ArXiv preprint_, 2024.
* Lin et al. (2024) Bill Yuchen Lin, Yuntian Deng, Khyathi Chandu, Faeze Brahman, Abhilasha Ravichander, Valentina Pyatkin, Nouha Dziri, Ronan Le Bras, and Yejin Choi. Wildbench: Benchmarking lms with challenging tasks from real users in the wild, 2024.
* Liu et al. (2023a) Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. Autodan: Generating stealthy jailbreak prompts on aligned large language models. _ArXiv preprint_, 2023a.
* Liu et al. (2023b) Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg evaluation using gpt-4 with better human alignment. _ArXiv preprint_, 2023b.
* Liu et al. (2023c) Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang, and Yang Liu. Jailbreaking chatgpt via prompt engineering: An empirical study. _ArXiv preprint_, 2023c.
* Liu et al. (2024) Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulic, Anna Korhonen, and Nigel Collier. Aligning with human judgement: The role of pairwise preference in large language model evaluators. _ArXiv preprint_, 2024.
* Luo et al. (2023) Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. Chatgpt as a factual inconsistency evaluator for text summarization. _ArXiv preprint_, 2023.
* Manakul et al. (2023) Potsawee Manakul, Adian Liusie, and Mark JF Gales. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. _ArXiv preprint_, 2023.
* Maus et al. (2023) Natalie Maus, Patrick Chao, Eric Wong, and Jacob Gardner. Black box adversarial prompting for foundation models. _ArXiv preprint_, 2023.
* McAleese et al. (2024) Nat McAleese, Rai Michael Pokorny, Juan Felipe Ceron Uribe, Evgenia Nitishinskaya, Maja Trebacz, and Jan Leike. Llm critics help catch llm bugs. _ArXiv preprint_, 2024.
* Meng et al. (2024) Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with a reference-free reward. _ArXiv preprint_, 2024.
* Meta (2024) Meta. Llama 3 model card, 2024.
* Ni et al. (2024) Jinjie Ni, Fuzhao Xue, Xiang Yue, Yuntian Deng, Mahir Shah, Kabir Jain, Graham Neubig, and Yang You. Mixeval: Deriving wisdom of the crowd from llm benchmark mixtures. _ArXiv preprint_, 2024.
* OpenAI (2023) OpenAI. Gpt-4 technical report, 2023. https://cdn.openai.com/papers/gpt-4.pdf.

* [594] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. In _Advances in neural information processing systems_, 2022.
* [595] Anselm Paulus, Arman Zharmagambetov, Chuan Guo, Brandon Amos, and Yuandong Tian. Advprompter: Fast adaptive adversarial prompting for llms. _ArXiv preprint_, 2024.
* [596] Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. Red teaming language models with language models. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, 2022.
* [607] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. Fine-tuning aligned language models compromises safety, even when users do not intend to! _ArXiv preprint_, 2023.
* [608] Vyas Raina, Adian Liusie, and Mark Gales. Is llm-as-a-jadge robust? investigating universal adversarial attacks on zero-shot llm assessment. _ArXiv preprint_, 2024.
* [610] Abhinav Rao, Sachin Vashistha, Atharva Naik, Somak Aditya, and Monojit Choudhury. Tricking llms into disobedience: Understanding, analyzing, and preventing jailbreaks. _ArXiv preprint_, 2023.
* [611] Yangjun Ruan, Honghua Dong, Andrew Wang, Silviu Pitis, Yongchao Zhou, Jimmy Ba, Yann Dubois, Chris J Maddison, and Tatsunori Hashimoto. Identifying the risks of lm agents with an lm-emulated sandbox. _ArXiv preprint_, 2023.
* [612] Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Scharli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. In _International Conference on Machine Learning_, 2023.
* [621] Jiawen Shi, Zenghui Yuan, Yinuo Liu, Yue Huang, Pan Zhou, Lichao Sun, and Neil Zhenqiang Gong. Optimization-based prompt injection attack to llm-as-a-judge. _ArXiv preprint_, 2024.
* [622] Bowen Tan, Yun Zhu, Lijuan Liu, Eric Xing, Zhiting Hu, and Jindong Chen. Cappy: Outperforming and boosting large multi-task lms with a small scorer. In _Advances in Neural Information Processing Systems_, 2023.
* [623] Yu Tian, Xiao Yang, Jingyuan Zhang, Yinpeng Dong, and Hang Su. Evil geniuses: Delving into the safety of llm-based agents. _ArXiv preprint_, 2023.
* [624] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajiwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _ArXiv preprint_, 2023.
* [635] Sam Toyer, Olivia Watkins, Ethan Adrian Mendes, Justin Svegliato, Luke Bailey, Tiffany Wang, Isaac Ong, Karim Elmaaroufi, Pieter Abbeel, Trevor Darrell, et al. Tensor trust: Interpretable prompt injection attacks from an online game. _ArXiv preprint_, 2023.
* [636] Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. Universal adversarial triggers for attacking and analyzing NLP. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, 2019.
* [640] Tianlu Wang, Ping Yu, Xiaoqing Ellen Tan, Sean O'Brien, Ramakanth Pasunuru, Jane Dwivedi-Yu, Olga Golovneva, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. Shepherd: A critic for language model generation. _ArXiv preprint_, 2023.
* [641] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail? In _Advances in Neural Information Processing Systems_, 2023a.
* [642] Zeming Wei, Yifei Wang, and Yisen Wang. Jailbreak and guard aligned language models with only few in-context demonstrations. _ArXiv preprint_, 2023b.

* Yang et al. (2023) Xianjun Yang, Xiao Wang, Qi Zhang, Linda Petzold, William Yang Wang, Xun Zhao, and Dahua Lin. Shadow alignment: The ease of subverting safely-aligned language models. _ArXiv preprint_, 2023.
* Yuan et al. (2023) Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Pinjia He, Shuming Shi, and Zhaopeng Tu. Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher. _ArXiv preprint_, 2023.
* Zeng et al. (2024) Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, and Weiyan Shi. How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms. _ArXiv preprint_, 2024.
* Zhang et al. (2024) Xuanchang Zhang, Wei Xiong, Lichang Chen, Tianyi Zhou, Heng Huang, and Tong Zhang. From lists to emojis: How format bias affects model alignment. _ArXiv preprint_, 2024.
* Zhao et al. (2024) Hao Zhao, Maksym Andriushchenko, Francesco Croce, and Nicolas Flammarion. Long is more for alignment: A simple but tough-to-beat baseline for instruction fine-tuning. _ArXiv preprint_, 2024.
* Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. In _Advances in Neural Information Processing Systems_, 2023.
* Zheng et al. (2024) Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Jing Jiang, and Min Lin. Improved few-shot jailbreaking can circumvent aligned language models and their defenses. In _Advances in Neural Information Processing Systems_, 2024.
* Zhou et al. (2023) Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. In _Advances in Neural Information Processing Systems_, 2023.
* Zhu et al. (2023) Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang, Furong Huang, Ani Nenkova, and Tong Sun. Autodan: Automatic and interpretable adversarial attacks on large language models. _ArXiv preprint_, 2023.
* Zou et al. (2023) Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. _ArXiv preprint_, 2023.

## Appendix A Anti-cheating strategies

To address the vulnerabilities exposed by our cheat, benchmark developers must take proactive measures to ensure the safety and integrity of automatic LLM evaluation systems. For example, one immediate step could involve integrating specialized detectors designed to identify and mitigate adversarial manipulations targeting LLM-based benchmarks.

**Template paraphrasing.** Previous research has suggested that paraphrasing the input can be an effective defense against jailbreaking on language models (Jain et al., 2023). Building on this idea, one potential defense against our cheat is to release only paraphrased versions of the auto-annotator template, while keeping the real template private. The rationale behind this approach is that the paraphrased templates would be harder for adversaries to exploit directly. To evaluate this defense, we experimented using Llama-3-8B-Instract as the evaluation model. We utilized ChatGPT (\(\mathrm{OpenAI}\), 2023) to rewrite the official auto-annotator template into multiple paraphrased variants as shown in Figures 12, 13, 14 and 15. We next conduct a random search on these rewritten templates and tested the optimized response's effectiveness on AlpacaEval 2.0's original (unseen) official auto-annotator template. As shown in Table 5, despite the template paraphrasing, we are still able to achieve high win rates (e.g. \(92.1\%\) LC win rate). This demonstrates that simply releasing paraphrased templates is insufficient as a defense mechanism, as the cheat remains effective even when the original template is kept private. More robust defenses are required to fully address this issue.

**PPL filter**. We utilize GPT-4-1106-Preview as the auto-annotator to evaluate the effectiveness of a PPL-based filter. The perplexity (PPL) is computed using GPT-2, following the methodology described by Alon and Kamfonas (2023). Specifically, we adopt the windowed PPL approach with a window size of 32, as suggested by Jain et al. (2023), to better capture localized fluctuations in perplexity that may reflect manipulative or adversarial patterns in the output. To ensure that the baseline outputs are not indavetently filtered, we set the PPL threshold to the maximum perplexity observed from GPT-4-1106-Preview baseline outputs. This ensures that all outputs from the reference model remain unaffected by the filter, allowing us to focus on detecting and filtering out adversarial outputs with higher perplexities. As illustrated in Figure 6, our results demonstrate that despite setting a high threshold, the PPL filter fails to consistently identify adversarial outputs. For instance, our structured response with win rates as high as 76.8% still exhibits perplexities below the threshold, rendering the filter ineffective. This suggests that relying solely on PPL, even in a windowed configuration, is insufficient to robustly detect adversarial manipulations aimed at influencing LLM judgments.

Figure 6: **PPL (windowed) of responses from various sources. We plot the windowed perplexity (PPL) for GPT-4 Preview (11/06), GPT-3.5 Turbo (06/13), and LLaMA2-Chat 7B. The cyan dashed line indicates the PPL of our structured response with a \(76.8\%\) LC win rate while the pink one represents the PPL of our RS-augmented structured response with a \(86.5\%\) LC win rate. The results suggest that PPL filter is insufficient to defend our structured response.**

## Appendix B Related work

### LLM-based evaluation

Evaluating open-ended generation poses challenges due to the lack of a single valid ground truth. Human evaluation, though reliable, is expensive and time-consuming. To reduce costs and enable fast evaluation, powerful LLMs are often used as judges, LLM-based evaluators have been used for various specific tasks: providing AI feedback (Bai et al., 2022; Bubeck et al., 2023; Gudibande et al., 2023; Chiang et al., 2023; Zhou et al., 2023; Tan et al., 2023; Wang et al., 2023; Kim et al., 2023; McAleese et al., 2024), evaluating text summarization (Gao et al., 2023; Luo et al., 2023), detecting LLM hallucination (Li et al., 2023; Manakul et al., 2023; Adlakha et al., 2023; Cohen et al., 2023) etc. More recently, people have proposed to use powerful proprietary LLMs like GPT-4 to evaluate the general ability of LLMs as seen in benchmarks like G-eval (Liu et al., 2023), MT-Bench and Chatbot Arena (Zheng et al., 2023), AlpacaEval (Dubois et al., 2023; Li et al., 2023; Dubois et al., 2024), ArenalHard (Li et al., 2024), WildBench (Lin et al., 2024), and MixEval (Ni et al., 2024).

### Attacking LLM-based evaluations

While initially studied in the context of image classification, adversarial examples for language models have more recently been demonstrated for several tasks: question answering (Jia and Liang, 2017; Wallace et al., 2019), document classification (Ebrahimi et al., 2018), sentiment analysis (Alzantot et al., 2018; Maus et al., 2023), and toxicity (Jones et al., 2023; Wallace et al., 2019). More recently, Shi et al. (2023) found that LLM can be distracted by irrelevant context easily. Besides, there are also a lot of analyses to improve the robustness and reduce the bias of LLM-based evaluations. Liu et al. (2024) study the role of pairwise preferences in LLM evaluator alignment. Zheng et al. (2023) discusses the three limitations of LLM-as-a-Judge: position bias, verbosity bias, self-enhancement bias, and limited capability in grading math and reasoning questions. Regarding the verbosity bias, LLM judgers are known to be biased toward longer responses (Dubois et al., 2024; Zhao et al., 2024; Chen et al., 2024).

More recently, there has been growing interest in exploring the adversarial robustness of LLM evaluators themselves. Raina et al. (2024) demonstrated that short, universal adversarial phrases can be concatenated to responses to manipulate LLM evaluators into assigning inflated scores. Similarly, Shi et al. (2024) proposed an optimization-based prompt injection attack that allows an adversary to craft sequences designed to bias the LLM-as-a-Judge toward selecting a particular response, regardless of the input or competing responses. Chen et al. (2024) introduced an adversarial framework targeting natural language generation evaluators, showcasing the vulnerabilities of these systems to manipulation. Independently, we propose "null model" cheating on automatic LLM benchmarks.

Our work differs from these prior efforts in several aspects: 1) Unlike previous attacks that manipulate meaningful responses by appending adversarial suffixes, we propose the use of a completely non-informative "null model" that generates the same irrelevant output for all input instructions. This approach does not rely on producing contextually relevant responses, making it distinct from existing response-based adversarial attacks; 2) While many of the earlier works focus on optimizing individual prompts or attacks specific to a given input (Raina et al., 2024), our approach emphasizes the creation of universal, transferable adversarial prompts. These prompts are designed to work across various instructions without direct access to those instructions, offering a more generalized and powerful cheating strategy; 3) Most existing studies have focused on attacking open-source models or less-used benchmarks. To the best of our knowledge, no prior research has systematically targeted widely-used, state-of-the-art benchmarks like AlpacaEval 2.0 and Arena-Hard-Auto, or demonstrated the ability to achieve top-ranked win rates on these platforms. Our work presents the first comprehensive cheating on these highly influential LLM benchmarks.

### Jailbreaking LLMs

Though cheating automatic LLM benchmarks and jailbreaking are motivated by different research goals, they share similar methodologies. Research in red-teaming has demonstrated that aligned LLMs such as ChatGPT/GPT-4 (OpenAI, 2023) and Llama-2 (Touvron et al., 2023) can be jailbroken to produce harmful or unintended outputs through carefully crafted manual or automated prompts (Chao et al., 2023; Deng et al., 2023; Hayase et al., 2024; Lapid et al., 2023; Li et al., 2023; Liu et al., 2023;c; Perez et al., 2022; Rao et al., 2023; Ruan et al., 2023; Toyer et al., 2023; Yuan et al., 2023; Zhu et al., 2023; Zou et al., 2023; Paulus et al., 2024; Liao and Sun, 2024; Andriushchenko et al., 2024; Wei et al., 2023; Anil et al., 2024; Zheng et al., 2024). Tian et al. (2023) explore the safety risks posed by LLM-based agents, while Greshake et al. (2023) highlight indirect prompt injection as a method for compromising LLM-integrated applications. Wei et al. (2023) attribute the susceptibility of aligned LLMs to jailbreaking to the tension between maximizing capability and ensuring safety, as well as the gap between pretraining and safety-focused training. Additionally, Carlini et al. (2023) argues that neural networks' intrinsic vulnerability to adversarial examples plays a critical role in these weaknesses. Recent work has also shown that fine-tuning aligned LLMs, whether with poisoned or benign data, can degrade their alignment and safety (Gade et al., 2023; Huang et al., 2024; Lermen et al., 2023; Qi et al., 2023; Yang et al., 2023).

## Appendix C Implementation Details

```
0: Judge prompts \(x_{1:n_{1}}^{(1)},\ldots,x_{1:n_{N}}^{(N)}\), prefix \(p_{1:l}\), losses \(\mathcal{L}_{1},\ldots,\mathcal{L}_{N}\), iterations \(T\)\(\mathcal{L}_{\text{Best}}=\infty\)for\(t\in 1,\ldots,T\)do\(\tilde{p}_{1:l}:=p_{1:l}\)\(\tilde{p}_{i}:=\text{Uniform}(X)\), where \(i=\text{Uniform}(l)\)if\(\Sigma_{j=1}^{N}\mathcal{L}(x_{1:n_{j}}^{(j)},\tilde{p}_{1:l})\leq \mathcal{L}_{\text{Best}}\)then\(p_{1:l}:=\tilde{p}_{1:l}\)\(\mathcal{L}_{\text{Best}}=\Sigma_{j=1}^{N}\mathcal{L}(x_{1:n_{j}}^{(j)},\tilde{p}_{1:l})\)endif endfor returnOptimized prefix \(p\) ```

**Algorithm 1** Universal Random Search* ```

Figure 7: The official pairwise comparison prompt template for AlpacaEval 2.0.

* ```

Figure 8: A concrete demonstration of placing our structured cheating response at the default position, where the target preference is “M”.

[MISSING_PAGE_EMPTY:19]

Figure 10: The official pairwise comparison prompt template for Arena-Hard-Auto. Additionally, we show the corresponding structured cheating response and the RS augmented one.

Figure 11: The official single scoring prompt template for MT-Bench. Additionally, we show the corresponding structured cheating response and the RS augmented one.

[MISSING_PAGE_EMPTY:22]

[MISSING_PAGE_EMPTY:23]

[MISSING_PAGE_EMPTY:24]

[MISSING_PAGE_EMPTY:25]

[MISSING_PAGE_EMPTY:26]