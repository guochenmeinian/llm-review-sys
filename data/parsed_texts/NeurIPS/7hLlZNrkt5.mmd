# A Theory of Link Prediction via

Relational Weisfeiler-Leman on Knowledge Graphs

 Xingyue Huang

Department of Computer Science

University of Oxford

Oxford, UK.

xingyue.huang@cs.ox.ac.uk

&Miguel Romero

Department of Computer Science

Universidad Catolica de Chile

& CENIA Chile

mgromero@uc.cl

Ismail Ilkan Ceylan

Department of Computer Science

University of Oxford

Oxford, UK.

ismail.ceylan@cs.ox.ac.uk

&Pablo Barcelo

Inst. for Math. and Comp. Eng.

Universidad Catolica de Chile

& IMFD Chile & CENIA Chile

pbarcelo@uc.cl

###### Abstract

Graph neural networks are prominent models for representation learning over graph-structured data. While the capabilities and limitations of these models are well-understood for simple graphs, our understanding remains incomplete in the context of knowledge graphs. Our goal is to provide a systematic understanding of the landscape of graph neural networks for knowledge graphs pertaining to the prominent task of link prediction. Our analysis entails a unifying perspective on seemingly unrelated models and unlocks a series of other models. The expressive power of various models is characterized via a corresponding relational Weisfeiler-Leman algorithm. This analysis is extended to provide a precise logical characterization of the class of functions captured by a class of graph neural networks. The theoretical findings presented in this paper explain the benefits of some widely employed practical design choices, which are validated empirically.

## 1 Introduction

Graph neural networks (GNNs) [27; 12] are prominent models for representation learning over graph-structured data, where the idea is to iteratively compute vector representations of nodes of an input graph through a series of invariant (resp., equivariant) transformations. While the landscape of GNNs is overwhelmingly rich, the vast majority of such models are instances of _message passing neural networks_[11] which are well-studied, leading to a theoretical understanding of their capabilities and limitations [34; 21]. In turn, our understanding is rather limited for GNN models dedicated to learning over _knowledge graphs_, which are applied in a wide range of domains.

To make our context precise, we first consider an extension of message passing neural networks with relation-specific message functions, which we call _relational_ message passing neural networks. Two prominent examples of this framework are RGCN [28] and CompGCN [32], and their expressive power has recently been characterized through a dedicated relational Weisfeiler-Leman test [4].

While offering principled means for learning over knowledge graphs, the standard relational message passing framework is tailored for computing _unary_ node representations and therefore models of this class are better suited for node-level tasks (e.g., node/entity classification). Actually, it is well-known that even a good node-level representation might not necessarily induce a good edge representation,hindering the applicability of such models for the crucial task of link prediction [39]. This has led to the design of GNN architectures specifically tailored for link prediction over knowledge graphs [30; 40; 33; 18], for which our understanding remains limited.

The goal of this paper is to offer a theory of the capabilities and limitations of a class of relational GNN architectures which compute _pairwise_ node representations to be utilized for link prediction. Although most such architectures can be seen to be subsumed by _higher-order_ message passing neural networks that compute pairwise representations on nodes, the inherently quadratic behavior of the latter justifies local approximations which align better with models used in practice. Of particular interest to us is Neural Bellman-Ford Networks (NBFNets) [40], which define a message passing approach inspired by the Bellman-Ford algorithm. We argue in salient detail that the crucial insight of this approach is in leveraging the idea of computing _conditional_ pairwise-node representations, which leads to more expressive models at a more reasonable computational cost, given its local nature.

Building on this fundamental aspect, we define _conditional_ message passing neural networks, which extend traditional ones by a conditional message passing paradigm: every node representation is conditional on a source node and a query relation, which allows for computing pairwise node representations. This framework strictly contains NBFNets and allows for a systematic treatment of various other models. Through a careful study of this framework, we can explain the conceptual differences between different models along with their respective expressive power.

Our contributions can be summarized as follows:

* We introduce _conditional_ message passing neural networks which encode representations of nodes \(v\) conditioned on a (source) node \(u\) and a query relation \(q\), yielding pairwise node representations. We discuss the model design space, including a discussion on different initialization regimes and the presence of global readout functions in each layer of the network.
* We define a relational Weisfeiler-Leman algorithm (building on similar works such as Barcelo et al. [4]), and prove that _conditional_ message passing neural networks can match the expressive power of this algorithm. This study reveals interesting insights about NBFNets, suggesting that their strong empirical performance is precisely due to the expressive power, which can be matched by other instances of this framework.
* Viewing _conditional_ message passing neural networks (with or without global readouts) as classifiers over pairs of nodes, we give logical characterizations of their expressive power based on formulas definable in some _binary_ variants of _graded modal logic_[8; 19]. This provides us with a declarative and well-studied formal counterpart of C-MPNNs.
* We conduct an experimental analysis to verify the impact of various model choices, particularly pertaining to initialization, history, message computation, and global readout functions. We also conduct both inductive and transductive experiments on various real-world datasets, empirically validating our theoretical findings.

## 2 Related work and motivation

Early GNNs for knowledge graphs are relational variations of message passing neural networks. A prototypical example is the RGCN architecture [28], which extends graph convolutional networks (GCNs) [17] with relation-specific message functions. CompGCN [32] and several other architectures [37] follow this line of work with differences in their aggregate, update, and message functions. These architectures encode _unary_ node representations and typically rely on a _pairwise decoder_ function to predict the likelihood of a link which is known to be suboptimal for link prediction [39]. There is a good understanding of the expressive power of these architectures [4]: we generalize these results in Section 3, since they form the basis for the rest of our results.

A different approach is given for single-relational graphs by SEAL [38], where the idea is to encode (labeled) subgraphs (instead of nodes). GraIL [30] extends this idea to knowledge graphs, and one important virtue of these models is that they are _inductive_ even if there are no node features in the input graph. The idea is to use a form of labeling trick [39] based on pairwise shortest path distances in sampled subgraphs, but these architectures suffer from scalability issues. More recent inductive architectures integrate ideas from earlier path-based link prediction approaches [24; 14] into modern GNN architectures, resulting in proposals such as PathCon [33], Geodesic GNNs [18], and NBFNets [40]. Our study is very closely related to NBFNets which is inspired by the generalized version of the Bellman-Ford algorithm for finding shortest paths. These architectures aggregate over relational paths by keeping track of conditional pairwise-node representations. While NBFNets can be intuitively seen as the neural counterpart of the _generalized_ Bellman-Ford algorithm, they do _not_ provably align with this algorithm since the "semiring assumption" is invalidated through the use of non-linearities (which is explicit in Zhu et al. [40]). This leaves open many questions regarding the capabilities and limitations of these architectures.

In this paper, we argue that the key insight behind architectures such as NBFNets is in locally computing _pairwise representations through conditioning_ on a source node, and this has roots in earlier works, such as ID-GNNs [36]. To formally study this, we introduce conditional message passing neural networks as a strict generalization of NBFNets [40] and related models such as NeuralLP [35], or DRUM [25]. Through this abstraction, we theoretically study the properties of a large class of models in relation to local variants of relational Weisfeiler-Leman algorithms. Broadly, our study can be seen as the relational counterpart of the expressiveness studies conducted for GNNs [34; 21; 3], particularly related to higher-order GNNs [21], which align with higher-order dimensional variants of the WL test. Our characterization relies on _local_ versions of higher-order WL tests [22], albeit not in a relational context. This can be seen as a continuation and generalization of the results given for relational message passing neural networks [4] to a broader class of models.

## 3 Background

### Knowledge graphs and invariants

**Knowledge graphs.** A _knowledge graph_ is a tuple \(G=(V,E,R,c)\), where \(V\) is a set of nodes, \(E\subseteq R\times V\times V\) is a set of labeled edges, or facts, \(R\) is the set of relation types and \(c:V\to D\) is a node coloring. When \(D=\mathbb{R}^{d}\), we also say that \(c\) is a \(d\)-dimensional _feature map_, and typically use \(\bm{x}\) instead of \(c\). We write \(r(u,v)\) to denote a labeled edge, or a fact, where \(r\in R\) and \(u,v\in V\). The _neighborhood_ of a node \(v\in V\) relative to a relation \(r\in R\) is defined as \(\mathcal{N}_{r}(v):=\{u\mid r(u,v)\in E\}\).

**Graph invariants.** We define \(k\)_-ary graph invariants_ following the terminology of Grobe [13], for which we first define isomorphism over knowledge graphs. An _isomorphism_ from a knowledge graph \(G=(V,E,R,c)\) to a knowledge graph \(G^{\prime}=(V^{\prime},E^{\prime},R,c^{\prime})\) is a bijection \(f:V\to V^{\prime}\) such that \(c(v)=c^{\prime}(f(v))\) for all \(v\in V\), and \(r(u,v)\in E\) if and only if \(r(f(u),f(v))\in E^{\prime}\), for all \(r\in R\) and \(u,v\in V\). A \(0\)_-ary graph invariant_ is a function \(\xi\) defined on knowledge graphs such that \(\xi(G)=\xi(G^{\prime})\) for all isomorphic knowledge graphs \(G\) and \(G^{\prime}\). For \(k\geq 1\), a \(k\)_-ary graph invariant_ is a function \(\xi\) that associates with each knowledge graph \(G=(V,E,R,c)\) a function \(\xi(G)\) defined on \(V^{k}\) such that for all knowledge graphs \(G\) and \(G^{\prime}\), all isomorphisms \(f\) from \(G\) to \(G^{\prime}\), and all \(k\)-tuples of nodes \(\bm{v}\in V^{k}\), it holds that \(\xi(G)(\bm{v})=\xi(G^{\prime})(f(\bm{v}))\). If \(k=1\), this defines a _node invariant_, or _unary invariant_, and if \(k=2\), this defines a _binary invariant_, which is central to our study.

**Refinements.** A function \(\xi(G):V^{k}\to D\)_refines_ a function \(\xi^{\prime}(G):V^{k}\to D\), denoted as \(\xi(G)\preceq\xi^{\prime}(G)\), if for all \(\bm{v},\bm{v}^{\prime}\in V^{k}\), \(\xi(G)(\bm{v})=\xi(G)(\bm{v}^{\prime})\) implies \(\xi^{\prime}(G)(\bm{v})=\xi^{\prime}(G)(\bm{v}^{\prime})\). We call such functions _equivalent_, denoted as \(\xi(G)\equiv\xi^{\prime}(G)\), if \(\xi(G)\preceq\xi^{\prime}(G)\) and \(\xi^{\prime}(G)\preceq\xi(G)\). A \(k\)-ary graph invariant \(\xi\)_refines_ a \(k\)-ary graph invariant \(\xi^{\prime}\), if \(\xi(G)\) refines \(\xi^{\prime}(G)\) for all knowledge graphs \(G\).

### Relational message passing neural networks

We introduce _relational message passing neural networks (R-MPNNs)_, which encompass several known models such as RGCN [28] and CompGCN [32]. The idea is to iteratively update the feature of a node \(v\) based on the different relation types \(r\in R\) and the features of the corresponding neighbors in \(\mathcal{N}_{r}(v)\). In our most general model we also allow readout functions, that allow further updates to the feature of \(v\) by aggregating over the features of all nodes in the graph.

Let \(G=(V,E,R,\bm{x})\) be a knowledge graph, where \(\bm{x}\) is a feature map. An _R-MPNN_ computes a sequence of feature maps \(\bm{h}^{(t)}:V\rightarrow\mathbb{R}^{d(t)}\), for \(t\geq 0\). For simplicity, we write \(\bm{h}^{(t)}_{v}\) instead of \(\bm{h}^{(t)}(v)\). For each node \(v\in V\), the representations \(\bm{h}^{(t)}_{v}\) are iteratively computed as:

\[\bm{h}^{(0)}_{v} =\bm{x}_{v}\] \[\bm{h}^{(t+1)}_{v} =\textsc{\textsc{Upd}}\left(\bm{h}^{(f(t))}_{v},\textsc{Agg}( \{\!\!\{\textsc{Msg}_{r}(\bm{h}^{(t)}_{w})|\;w\in\mathcal{N}_{r}(v),r\in R\!\!\} \!\},\textsc{\textsc{Read}}(\{\!\!\{\bm{h}^{(t)}_{w}\mid w\in V\!\!\}})\right),\]where Upd, Agg, Read, and Msg\({}_{r}\) are differentiable _update_, _aggregation_, _global readout_, and relation-specific _message_ functions, respectively, and \(f:\mathbb{N}\rightarrow\mathbb{N}\) is a _history_ function1, which is always non-decreasing and satisfies \(f(t)\leq t\). An R-MPNN has a fixed number of layers \(T\geq 0\), and then, the final node representations are given by the map \(\bm{h}^{(T)}:V\rightarrow\mathbb{R}^{d(T)}\).

Footnote 1: The typical choice is \(f(t)=t\), but other options are considered in the literature, as we discuss later.

The use of a readout component in message passing is well-known [5] but its effect is not well-explored in a relational context. It is of interest to us since it has been shown that standard GNNs can capture a larger class of functions with global readout [3].

An R-MPNN can be viewed as an encoder function \(\mathsf{enc}\) that associates with each knowledge graph \(G\) a function \(\mathsf{enc}(G):V\rightarrow\mathbb{R}^{d(T)}\), which defines a node invariant corresponding to \(\bm{h}^{(T)}\). The final representations can be used for node-level predictions. For link-level tasks, we use a binary decoder \(\mathsf{dec}_{q}:\mathbb{R}^{d(T)}\times\mathbb{R}^{d(T)}\rightarrow\mathbb{R}\), which produces a score for the likelihood of the fact \(q(u,v)\), for \(q\in R\).

In Appendix A.1 we provide a useful characterization of the expressive power of R-MPNNs in terms of a relational variant of the Weisfeiler-Leman test [4]. This characterization is essential for the rest of the results that we present in the paper.

## 4 Conditional message passing neural networks

R-MPNNs have serious limitations for the task of link prediction [39], which has led to several proposals that compute pairwise representations directly. In contrast to the case of R-MPNNs, our understanding of these architectures is limited. In this section, we introduce the framework of conditional MPNNs that offers a natural framework for the systematic study of these architectures.

Let \(G=(V,E,R,\bm{x})\) be a knowledge graph, where \(\bm{x}\) is a feature map. A _conditional message passing neural network_ (C-MPNN) iteratively computes pairwise representations, relative to a fixed query \(q\in R\) and a fixed node \(u\in V\), as follows:

\[\bm{h}^{(0)}_{v|u,q} =\textsc{Init}(u,v,q)\] \[\bm{h}^{(t+1)}_{v|u,q} =\textsc{Upd}(\bm{h}^{f(t)}_{v|u,q},\textsc{Agg}(\{\!\!\! \{\textsc{Msg}_{r}(\bm{h}^{(t)}_{w|u,q},\bm{z}_{q})|\;w\in\mathcal{N}_{r}(v),r \in R\!\!\!\}),\textsc{Read}(\{\!\!\{\bm{h}^{(t)}_{w|u,q}\;|\;w\in V\!\!\!\})\! \}),}\]

where \(\textsc{Init}\), Upd, Agg, Read, and Msg\({}_{r}\) are differentiable _initialization_, _update_, _aggregation_, _global readout_, and relation-specific _message_ functions, respectively, and \(f\) is the history function. We denote by \(\bm{h}^{(t)}_{q}:V\times V\rightarrow\mathbb{R}^{d(t)}\) the function \(\bm{h}^{(t)}_{q}(u,v):=\bm{h}^{(t)}_{v|u,q}\), and denote \(\bm{z}_{q}\) to be a learnable vector representing the query \(q\in R\). A C-MPNN has a fixed number of layers \(T\geq 0\), and the final pair representations are given by \(\bm{h}^{(T)}_{q}\). We sometimes write C-MPNNs _without global readout_ to refer to the class of models which do not use a readout component.

Intuitively, C-MPNNs condition on a source node \(u\) in order to compute representations of \((u,v)\) for all target nodes \(v\). To further explain, we show a visualization of C-MPNNs and R-MPNNs in Figure 1 to demonstrate the differences in the forward pass. Contrary to the R-MPNN model where we carry out relational message passing first and rely on the binary decoder to compute a query fact \(q(u,v)\), the C-MPNN model first initializes all node representations with the zero vector except the representation of the node \(u\) which is assigned a vector with non-zero entry. Following the initialization, we carry out relational message passing and decode the hidden state of the target node \(v\) to obtain the output, which yields the representation of \(v\) conditioned on \(u\).

Observe that C-MPNNs compute binary invariants, provided that the initialization \(\textsc{Init}\) is a binary invariant. To ensure that the resulting model computes pairwise representations, we require \(\textsc{Init}(u,v,q)\) to be a nontrivial function in the sense that it needs to satisfy _target node distinguishability_: for all \(q\in R\) and \(v\neq u\in V\), it holds that \(\textsc{Init}(u,u,q)\neq\textsc{Init}(u,v,q)\).

This is very closely related to the _Labeling Trick_ proposed by Zhang et al. [39], which is an initialization method aiming to differentiate a set \(\{u,v\}\) of target nodes from the remaining nodes in a graph. However, the _Labeling Trick_ only applies when both the source \(u\) and target nodes \(v\) are labeled. Recent state-of-the-art models such as ID-GNN [36] and NBFNet [40] utilize a similar method, but only with the source node \(u\) labeled differently in initialization. Our definition captures precisely this, and we offer a theoretical analysis of the capabilities of the aforementioned architectures accordingly.

One alternative is to directly learn pairwise representations following similar ideas to those of higher-order GNNs, but these algorithms are not scalable. Architectures such as NBFNets represent a trade-off between computational complexity and expressivity. The advantage of learning conditional representations \(\bm{h}_{v|u,q}\) is to be able to learn such representations in parallel for all \(v\in V\), amortizing the computational overhead; see Zhu et al. [40] for a discussion. We have also carried out a runtime analysis comparison among different classes of models in Appendix B.

### Design space and basic model architectures

To specify a C-MPNN architecture, we need to specify the functions \(\textsc{Init}\), \(\textsc{Agg}\), \(\textsc{Msg}_{r}\), \(f\), and Read. In the following, we consider three initialization functions \(\{\textsc{Init}^{1},\textsc{Init}^{2},\textsc{Init}^{3}\}\), two aggregation functions \(\{\textsc{sum},\textsc{PNA}\}\), three message functions \(\{\textsc{Msg}^{1}_{r},\textsc{Msg}^{2}_{r},\textsc{Msg}^{3}_{r}\}\), two history functions \(\{f(t)=t,f(t)=0\}\), and either _sum global readout_ or no readout term.

**Initialization.** We consider the following natural variants for initialization:

\[\textsc{Init}^{1}(u,v,q)=\mathbb{1}_{u=v}*\bm{1},\quad\textsc{Init}^{2}(u,v, q)=\mathbb{1}_{u=v}*\bm{z}_{q},\quad\textsc{Init}^{3}(u,v,q)=\mathbb{1}_{u=v}*( \bm{z}_{q}+\epsilon_{u}),\]

where \(*\) represents element-wise multiplication, the function \(\mathbb{1}_{u=v}(v)\) is the indicator function which returns the all-ones vector \(\bm{1}\) if \(u=v\) and the all-zeros vector \(\bm{0}\) otherwise with corresponding size.

Clearly, both \(\textsc{Init}^{1}\) and \(\textsc{Init}^{2}\) satisfy _target node distinguishability_ assumption if we assume \(\bm{z}_{q}\) has no zero entry, where \(\textsc{Init}^{2}\), in addition, allows query-specific initialization to be considered. Suppose we further relax the condition to be _target node distinguishability in expectation_. Then, \(\textsc{Init}^{3}\) can also distinguish between each conditioned node \(u\) given the same query vector \(\bm{z}_{q}\) by adding an error vector \(\epsilon_{u}\) sampled from \(\mathcal{N}(0,1)\) to the conditioned node's initialization.

**Aggregation.** We consider sum aggregation and Principal Neighborhood Aggregation (PNA) [7].

**Message.** We consider the following variations of _message_ functions:

\[\textsc{Msg}^{1}_{r}(\bm{h}^{(t)}_{w|u,q},\bm{z}_{q}) =\bm{h}^{(t)}_{w|u,q}*\bm{W}^{(t)}_{r}\bm{z}_{q},\] \[\textsc{Msg}^{2}_{r}(\bm{h}^{(t)}_{w|u,q},\bm{z}_{q}) =\bm{h}^{(t)}_{w|u,q}*\bm{b}^{(t)}_{r},\] \[\textsc{Msg}^{3}_{r}(\bm{h}^{(t)}_{w|u,q},\bm{z}_{q}) =\bm{W}^{(t)}_{r}\bm{h}^{(t)}_{w|u,q},\]

where \(\bm{W}^{(t)}_{r}\) are relation-specific transformations. \(\textsc{Msg}^{1}_{r}\) computes a query-dependent message, whereas \(\textsc{Msg}^{2}_{r}\) and \(\textsc{Msg}^{3}_{r}\) are analogous to message functions of CompGCN and RGCN, respectively.

Figure 1: Visualization of R-MPNN and C-MPNN. The dashed arrow is the target query \(q(u,v)\). Arrow colors indicate distinct relation types, while node colors indicate varying hidden states. R-MPNN considers a unary encoder and relies on a binary decoder, while C-MPNN first initializes binary representation based on the target query \(q(u,v)\), and then uses a unary decoder.

**History.** In addition, we can set \(f\), which intuitively is the function that determines the history of node embeddings to be considered. By setting \(f(t)=t\), we obtain a standard message-passing algorithm where the update function considers the representation of the node in the previous iteration. We can alternatively set \(f(t)=0\), in which case we obtain (a generalization of) NBFNets.

**Readout.** We consider a standard readout which sums the representations and applies a linear transformation on the resulting representations. Alternatively, we consider the special case, where we omit this component (we discuss a dedicated readout operation in our empirical analysis later).

**A simple architecture.** Consider a _basic_ C-MPNN architecture with global readout, which, for a query relation \(q\in R\) and a fixed node \(u\), updates the representations as:

\[\bm{h}^{(0)}_{v|u,q} =\mathbb{1}_{u=v}*\bm{z}_{q}\] \[\bm{h}^{(t+1)}_{v|u,q} =\sigma\bigg{(}\bm{W}^{(t)}_{0}\Big{(}\bm{h}^{(t)}_{v|u,q}+\sum_{ r\in R}\sum_{w\in\mathcal{N}_{r}(v)}\textsc{Msg}^{1}_{r}(\bm{h}^{(t)}_{w|u,q}, \bm{z}_{q})\Big{)}+\bm{W}^{(t)}_{1}\sum_{w\in V}\bm{h}^{(t)}_{w|u,q}\bigg{)},\]

where \(\bm{W}^{(t)}_{0},\bm{W}^{(t)}_{1}\) are linear transformations followed by a non-linearity \(\sigma\).

## 5 Characterizing the expressive power

### A relational Weisfeiler-Leman characterization

To analyze the expressive power of C-MPNNs, we introduce the _relational asymmetric local \(2\)-WL_, denoted by \(\mathsf{rawl}_{2}\). In this case, we work with knowledge graphs of the form \(G=(V,E,R,c,\eta)\), where \(\eta:V\times V\to D\) is a _pairwise coloring_. We say that \(\eta\) satisfies _target node distinguishability_ if \(\eta(u,u)\neq\eta(u,v)\) for all \(u\neq v\in V\). The notions of isomorphism and invariants extend to this context in a natural way. For each \(t\geq 0\), we update the coloring as:

\[\mathsf{rawl}_{2}^{(0)}(u,v) =\eta(u,v),\] \[\mathsf{rawl}_{2}^{(t+1)}(u,v) =\tau\big{(}\mathsf{rawl}_{2}^{(t)}(u,v),\{\!\{\!(\mathsf{rawl}_{ 2}^{(t)}(u,w),r)\mid w\in\mathcal{N}_{r}(v),r\in R\}\!\}\big{)},\]

where \(\tau\) injectively maps the above pair to a unique color, which has not been used in previous iterations. Observe that \(\mathsf{rawl}_{2}^{(t)}\) defines a binary invariant, for all \(t\geq 0\).

The test is asymmetric: given a pair \((u,v)\), we only look at neighbors of \((u,v)\) obtained by changing the second coordinate of the pair. In contrast, usual versions of (local) \(k\)-WL are symmetric as neighbors may change any coordinate. Interestingly, this test characterizes the power of C-MPNNs in terms of distinguishing pairs of nodes.

**Theorem 5.1**.: _Let \(G=(V,E,R,\bm{x},\eta)\) be a knowledge graph, where \(\bm{x}\) is a feature map and \(\eta\) is a pairwise coloring satisfying target node distinguishability. Let \(q\in R\) be any query relation. Then:_

1. _For all C-MPNNs with_ \(T\) _layers and initializations_ \(\textsc{Init}\) _with_ \(\textsc{Init}\equiv\eta\)_, and_ \(0\leq t\leq T\)_, we have_ \(\mathsf{rawl}_{2}^{(t)}\preceq\bm{h}_{q}^{(t)}\)_._
2. _For all_ \(T\geq 0\) _and history function_ \(f\)_, there is a C-MPNN without global readout with_ \(T\) _layers and history function_ \(f\) _such that for all_ \(0\leq t\leq T\)_, we have_ \(\mathsf{rawl}_{2}^{(t)}\equiv\bm{h}_{q}^{(t)}\)_._

The idea of the proof is as follows: we first show a correspondent characterization of the expressive power of R-MPNNs in terms of a relational variant of the WL test (Theorem A.1). This result generalizes results from Barcelo et al. [4]. We then apply a reduction from C-MPNNs to R-MPNNs, that is, we carefully build an auxiliary knowledge graph (encoding the pairs of nodes of the original knowledge graph) to transfer our R-MPNN characterization to our sought C-MPNN characterization.

Note that the lower bound (item (2)) holds even for the _basic_ model of C-MPNNs (without global readout) and the three proposed message functions from Section 4.1. The expressive power of C-MPNNs is independent of the history function as in any case it is matched by \(\mathsf{rawl}_{2}\). This suggests that the difference between traditional message passing models using functions \(f(t)=t\) and path-based models (such as NBFNets [40]) using \(f(t)=0\) is not relevant from a theoretical point of view.

### Logical characterization

We now turn to the problem of which _binary classifiers_ can be expressed as C-MPNNs. That is, we look at C-MPNNs that classify each pair of nodes in a knowledge graph as true or false. Following Barcelo et al. [3], we study _logical_ binary classifiers, i.e., those that can be defined in the formalism of first-order logic (FO). Briefly, a first-order formula \(\phi(x,y)\) with two free variables \(x,y\) defines a logical binary classifier that assigns value true to the pair \((u,v)\) of nodes in knowledge graph \(G\) whenever \(G\models\phi(u,v)\), i.e., \(\phi\) holds in \(G\) when \(x\) is interpreted as \(u\) and \(y\) as \(v\). A logical classifier \(\phi(x,y)\) is _captured_ by a C-MPNN \(\mathcal{A}\) if over every knowledge graph \(G\) the pairs \((u,v)\) of nodes that are classified as true by \(\phi\) and \(\mathcal{A}\) are the same.

A natural problem then is to understand what are the logical classifiers captured by C-MPNNs. Fix a set of relation types \(R\) and a set of pair colors \(\mathcal{C}\). We consider knowledge graphs of the form \(G=(V,E,R,\eta)\) where \(\eta\) is a mapping assigning colors from \(\mathcal{C}\) to pairs of nodes from \(V\). In this context, FO formulas can refer to the different relation types in \(R\) and the different pair colors in \(\mathcal{C}\). Our first characterization is established in terms of a simple fragment of FO, which we call \(\mathsf{rFO}^{3}_{\mathrm{cnt}}\), and is inductively defined as follows: First, \(a(x,y)\) for \(a\in\mathcal{C}\), is in \(\mathsf{rFO}^{3}_{\mathrm{cnt}}\). Second, if \(\varphi(x,y)\) and \(\psi(x,y)\) are in \(\mathsf{rFO}^{3}_{\mathrm{cnt}}\), \(N\geq 1\) is a positive integer, and \(r\in R\), then the formulas

\[\neg\varphi(x,y),\quad\varphi(x,y)\wedge\psi(x,y),\quad\exists^{\geq N}z\,( \varphi(x,z)\wedge r(z,y))\]

are also in \(\mathsf{rFO}^{3}_{\mathrm{cnt}}\). Intuitively, \(a(u,v)\) holds in \(G=(V,E,R,\eta)\) if \(\eta(u,v)=a\), and \(\exists^{\geq N}z\,(\varphi(u,z)\wedge r(z,v))\) holds in \(G\) if \(v\) has at least \(N\) incoming edges labeled \(r\in R\) from nodes \(w\) for which \(\varphi(u,w)\) holds in \(G\). We use the acronym \(\mathsf{rFO}^{3}_{\mathrm{cnt}}\) as this logic corresponds to a restriction of FO with three variables and counting. We can show the following result which is the first of its kind in the context of knowledge graphs:

**Theorem 5.2**.: _A logical binary classifier is captured by C-MPNNs without global readout if and only if it can be expressed in \(\mathsf{rFO}^{3}_{\mathrm{cnt}}\)._

The idea of the proof is to show a logical characterization for R-MPNNs (without global readout) in terms of a variant of graded modal logic called \(\mathsf{rFO}^{2}_{\mathrm{cnt}}\) (Theorem A.11), which generalizes results from Barcelo et al. [3] to the case of multiple relations. Then, as in the case of Theorem 5.1, we apply a reduction from C-MPNNs to R-MPNNs (without global readout) using an auxiliary knowledge graph and a useful translation between the logics \(\mathsf{rFO}^{2}_{\mathrm{cnt}}\) and \(\mathsf{rFO}^{3}_{\mathrm{cnt}}\).

Interestingly, arbitrary C-MPNNs (with global readout) are strictly more powerful than C-MPNNs without global readout in capturing logical binary classifiers: they can at least capture a strict extension of \(\mathsf{rFO}^{3}_{\mathrm{cnt}}\), denoted by \(\mathsf{erFO}^{3}_{\mathrm{cnt}}\).

**Theorem 5.3**.: _Each logical binary classifier expressible in \(\mathsf{erFO}^{3}_{\mathrm{cnt}}\) can be captured by a C-MPNN._

Intuitively speaking, our logic \(\mathsf{rFO}^{3}_{\mathrm{cnt}}\) from Theorem 5.2 only allows us to navigate the graph by moving to neighbors of the "current node". The logic \(\mathsf{erFO}^{3}_{\mathrm{cnt}}\) is a simple extension that allows us to move also to non-neighbors (Proposition A.15 shows that this logic actually gives us more power). Adapting the translation from logic to GNNs (from Theorem A.11), we can easily show that C-MPNNs with global readout can capture this extended logic.

The precise definition of \(\mathsf{erFO}^{3}_{\mathrm{cnt}}\) together with the proofs of Theorems 5.2 and 5.3 can be found in Appendices A.3 and A.4, respectively. Let us stress that \(\mathsf{rFO}^{3}_{\mathrm{cnt}}\) and \(\mathsf{erFO}^{3}_{\mathrm{cnt}}\) correspond to some binary variants of _graded modal logic_[8, 19]. As in Barcelo et al. [3], these connections are exploited in our proofs.

### Locating \(\mathsf{rawl}_{2}\) in the relational WL landscape

Let us note that \(\mathsf{rawl}_{2}\) strictly contains \(\mathsf{rwl}_{1}\), since intuitively, we can degrade the \(\mathsf{rawl}_{2}\) test to compute unary invariants such that it coincides with \(\mathsf{rwl}_{1}\). As a result, this allows us to conclude that R-MPNNs are less powerful than C-MPNNs. The \(\mathsf{rawl}_{2}\) test itself is upper bounded by a known relational variant of \(2\)-WL, namely, the _relational (symmetric) local 2-WL_ test, denoted by \(\mathsf{rwl}_{2}\). Given a knowledgegraph \(G=(V,E,R,c,\eta)\), this test assigns pairwise colors via the following update rule:

\[\mathsf{rwl}_{2}^{(t+1)}(u,v)= \tau\big{(}\mathsf{rwl}_{2}^{(t)}(u,v),\big{\{}\!\big{(}\mathsf{rwl} _{2}^{(t)}(w,v),r\big{)}\mid w\in\mathcal{N}_{r}(u),r\in R\big{\}}\big{\}},\] \[\big{\{}\!\big{(}\mathsf{rwl}_{2}^{(t)}(u,w),r\big{)}\mid w\in \mathcal{N}_{r}(v),r\in R\big{\}}\big{\}}\big{)}\]

This test and a corresponding neural architecture, for arbitrary order \(k\geq 2\), have been recently studied in Barcelo et al. [4] under the name of _multi-relational local \(k\)-WL_.

This helps us to locate the test \(\mathsf{rawl}_{2}\) within the broader WL hierarchy, but it does not align perfectly with the practical setup: one common practice in link prediction is to extend knowledge graphs with inverse relations [40; 35; 25; 32] which empirically yields stronger results and hence is used in most practical setups. However, the effect of this choice has never been quantified formally. We formally explain the benefits of this design choice, showing that it leads to provably more powerful models. The idea is to consider tests (and architectures) which are augmented with the inverse edges: we write \(\mathsf{rawl}_{2}^{+}\) and \(\mathsf{rwl}_{2}^{+}\) to denote the corresponding augmented tests and prove that it results in more expressive tests (and hence architectures) in each case, respectively. The precise tests and propositions, along with their proofs can be found in Appendix A.5. We present in Figure 2 the resulting expressiveness hierarchy for all these tests.

## 6 Experimental evaluation

We experiment on knowledge graph benchmarks and aim to answer the following questions: **Q1.** What is the impact of the history function on the model performance? In particular, do models with \(f(t)=t\) perform comparably to those with \(f(t)=0\) as our theory suggests? **Q2.** How do the specific choices for aggregation and message functions affect the performance? **Q3.** What is the impact of the initialization function on the performance? What happens when the target identifiability property does not hold? **Q4.** Do C-MPNNs outperform R-MPNNs empirically? **Q5.** Does the use of a global readout, or a relational version affect the performance?

### Experimental setup

**Datasets.** We use the datasets WN18RR [31] and FB15k-237 [9], for inductive relation prediction tasks, following a standardized train-test split given in four versions [30]. We augment each fact \(r(u,v)\) with an inverse fact \(r^{-1}(v,u)\). There are no node features for either of the datasets, and the initialization is given by the respective initialization function \(\textsc{Init}\). This allows all the proposed GNN models to be applied in the inductive setup and to better align with the corresponding relational Weisfeiler-Leman algorithms. The statistics of the datasets are reported in Table 5 of Appendix C.1. The code for experiments is reported in https://github.com/HxyScothuang/CMPNN.

**Implementation.** All models use 6 layers, each with 32 hidden dimensions. The decoder function parameterizes the probability of a fact \(q(u,v)\) as \(p(v\mid u,q)=\sigma(f(\boldsymbol{h}_{v|u,q}^{(T)}))\), where \(\sigma\) is the sigmoid function, and \(f\) is a 2-layer MLP with 64 hidden dimensions. We adopted layer-normalization [2] and short-cut connection after each aggregation and before applying ReLU. For the experiments concerning the message function \(\textsc{Ms}\mathrm{o}_{r}^{3}\), we follow the basis decomposition for the FB15k-237 dataset with 30 basis functions for sum aggregation, and 15 for PNA aggregation. We ran the experiments for 20 epochs on 1 Tesla T4 GPU. with mild modifications to accommodate all architectures studied in this paper. We discard the edges that directly connect query node pairs to prevent overfitting. The best checkpoint for each model instance is selected based on its performance on the validation set. All hyperparameter details are reported in Table 6 of Appendix C.1.

**Evaluation.** We consider _filtered ranking protocol_[6]: for each test fact \(r(u,v)\), we construct 50 negative samples \(r(u^{\prime},v^{\prime})\), randomly replacing either the head entity or the tail entity, and we report Hits@10, the rate of correctly predicted entities appearing in the top 10 entries for each instance list prediction. We report averaged results of _five_ independent runs for all experiments.

Figure 2: Expressiveness hierarchy: \(A\to B\) iff \(A\preceq B\). By Proposition A.20, \(\mathsf{rwl}_{2}\) and \(\mathsf{rawl}_{2}^{+}\) are incomparable. The case of \(\mathsf{rwl}_{2}^{+}\preceq\mathsf{rawl}_{2}^{+}\) is analogous to Proposition A.18.

### Results for inductive link prediction with C-MPNN architectures

We report inductive link prediction results for different C-MPNN architectures in Table 1, all initialized with \(\textsc{Init}^{2}\). Each row of Table 1 corresponds to a specific architecture, which allows us to compare the model components. Note that while NBFNets [40] use different message functions for different datasets, we separately report for each model architecture to specifically pinpoint the impact of each model component.

**History functions (Q1).** First, we note that there is no significant difference between the models with different history functions. Specifically, for any choice of aggregate and message functions, the model which sets \(f(t)=t\) performs comparably to the one which sets \(f(t)=0\). This supports our theoretical findings, which state that path-based message passing and traditional message passing have the same expressive power. This may appear as a subtle point, but it is important for informing future work: the strength of these architectures is fundamentally due to their ability to compute more expressive _binary invariants_, which holds regardless of the choice of the history function.

**Message functions (Q2).** We highlight that there is no significant difference between different message functions on WN18RR, which is unsurprising: WN18RR splits contain at most 11 relation types, which undermines the impact of the differences in message functions. In contrast, the results on FB15k-237 are informative in this respect: \(\textsc{Msg}_{r}^{2}\) clearly leads to worse performance than all the other choices, which can be explained by the fact that \(\textsc{Msg}_{r}^{2}\) utilizes fewer relation-specific parameters. Importantly, \(\textsc{Msg}_{r}^{3}\) appears strong and robust across models. This is essentially the message function of RGCN and uses basis decomposition to regularize the parameter matrices. Architectures using \(\textsc{Msg}_{r}^{3}\) with fewer parameters (see the appendix) can match or substantially exceed the performance of the models using \(\textsc{Msg}_{r}^{1}\), where the latter is the primary message function used in NBFNets. This may appear counter-intuitive since \(\textsc{Msg}_{r}^{3}\) does not have a learnable query vector \(\bm{z}_{q}\), but this vector is nonetheless part of the model via the initialization function \(\textsc{Init}^{2}\).

**Aggregation functions (Q2).** We experimented with aggregation functions \(\mathrm{sum}\) and \(\mathrm{PNA}\). We do not observe significant trends on WN18RR, but \(\mathrm{PNA}\) tends to result in slightly better-performing architectures. On FB15k-237, there seems to be an intricate interplay between aggregation and message functions. For \(\textsc{Msg}_{r}^{1}\), \(\mathrm{PNA}\) appears to be a better choice than \(\mathrm{sum}\). On the other hand, for both \(\textsc{Msg}_{r}^{2}\) and \(\textsc{Msg}_{r}^{3}\), sum aggregation is substantially better. This suggests that a sophisticated aggregation, such as \(\mathrm{PNA}\), may not always be necessary since it can be matched (and even outperformed) with a sum aggregation. In fact, the model with \(\mathrm{sum}\) aggregation and \(\textsc{Msg}_{r}^{3}\) is very closely related to RGCN and appears to be one of the best-performing models across the board. This supports our theory since, intuitively, this model can be seen as an adaptation of RGCN to compute binary invariants while keeping the choices for model components the same as RGCN.

\begin{table}
\begin{tabular}{c c c c c c c c c c} \hline \hline \multicolumn{2}{c}{**Model architectures**} & \multicolumn{5}{c}{**WN18RR**} & \multicolumn{5}{c}{**FB15k-237**} \\ Agg & \(\textsc{Msg}_{r}\) & \(f(t)\) & **v1** & **v2** & **v3** & **v4** & **v1** & **v2** & **v3** & **v4** \\ \hline sum & \(\textsc{Msg}_{r}^{1}\) & \(0\) & \(0.934\) & \(0.896\) & \(0.894\) & \(0.881\) & \(0.784\) & \(0.900\) & \(0.940\) & \(0.923\) \\ sum & \(\textsc{Msg}_{r}^{1}\) & \(t\) & \(0.932\) & \(0.896\) & \(\mathbf{0.900}\) & \(0.881\) & \(0.794\) & \(0.906\) & \(\mathbf{0.947}\) & \(0.933\) \\ \hline sum & \(\textsc{Msg}_{r}^{2}\) & \(0\) & \(0.939\) & \(\mathbf{0.906}\) & \(0.881\) & \(0.881\) & \(0.734\) & \(0.899\) & \(0.911\) & \(0.941\) \\ sum & \(\textsc{Msg}_{r}^{2}\) & \(t\) & \(0.937\) & \(\mathbf{0.906}\) & \(0.865\) & \(\mathbf{0.884}\) & \(0.728\) & \(0.883\) & \(0.929\) & \(0.931\) \\ \hline sum & \(\textsc{Msg}_{r}^{3}\) & \(0\) & \(\mathbf{0.943}\) & \(0.898\) & \(0.888\) & \(0.877\) & \(\mathbf{0.850}\) & \(0.934\) & \(0.919\) & \(0.941\) \\ sum & \(\textsc{Msg}_{r}^{3}\) & \(t\) & \(0.934\) & \(0.896\) & \(0.892\) & \(0.880\) & \(0.844\) & \(\mathbf{0.943}\) & \(0.926\) & \(\mathbf{0.950}\) \\ \hline \hline PNA & \(\textsc{Msg}_{r}^{1}\) & \(0\) & \(0.943\) & \(0.897\) & \(0.898\) & \(0.886\) & \(0.801\) & \(0.945\) & \(0.934\) & \(\mathbf{0.960}\) \\ PNA & \(\textsc{Msg}_{r}^{1}\) & \(t\) & \(0.941\) & \(0.895\) & \(\mathbf{0.904}\) & \(0.886\) & \(\mathbf{0.804}\) & \(\mathbf{0.949}\) & \(\mathbf{0.945}\) & \(0.954\) \\ \hline PNA & \(\textsc{Msg}_{r}^{2}\) & \(0\) & \(0.946\) & \(0.900\) & \(0.896\) & \(0.887\) & \(0.715\) & \(0.896\) & \(0.887\) & \(0.886\) \\ PNA & \(\textsc{Msg}_{r}^{2}\) & \(t\) & \(\mathbf{0.947}\) & \(\mathbf{0.902}\) & \(0.901\) & \(\mathbf{0.888}\) & \(0.709\) & \(0.899\) & \(0.875\) & \(0.894\) \\ \hline PNA & \(\textsc{Msg}_{r}^{3}\) & \(0\) & \(\mathbf{0.947}\) & \(0.898\) & \(0.899\) & \(0.884\) & \(0.788\) & \(0.908\) & \(0.906\) & \(0.927\) \\ PNA & \(\textsc{Msg}_{r}^{3}\) & \(t\) & \(0.944\) & \(0.897\) & \(0.894\) & \(0.882\) & \(0.795\) & \(0.916\) & \(0.908\) & \(0.926\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Inductive relation prediction with C-MPNNs using \(\textsc{Init}^{2}\) initialization and _no_ readout. The best results for each category are shown in **bold** and the second best results are underlined.

[MISSING_PAGE_FAIL:10]

Acknowledgement

The authors would like to acknowledge the use of the University of Oxford Advanced Research Computing (ARC) facility in carrying out this work (http://dx.doi.org/10.5281/zenodo.22558). We would also like to thank Google Cloud for kindly providing computational resources. Barcelo is funded by ANID-Millennium Science Initiative Program - CodeICN17002. Romero is funded by Fondecyt grant 11200956. Barcelo and Romero are funded by the National Center for Artificial Intelligence CENIA FB210017, BasalANID.

## References

* [1]R. Abboud, I. L. Ceylan, M. Grohe, and T. Lukasiewicz (2021) The surprising power of graph neural networks with random node initialization. In IJCAI, Cited by: SS1.
* [2]L. J. Ba, J. R. Kiros, and G. E. Hinton (2016) Layer normalization. In CoRR, Cited by: SS1.
* [3]P. Barcelo, E. V. Kostylev, M. Monet, J. Perez, J. L. Reutter, and J. P. Silva (2020) The logical expressiveness of graph neural networks. In ICLR, Cited by: SS1.
* [4]P. Barcelo, M. Galkin, C. Morris, and M. Romero (2022) Weisfeiler and leman go relational. In LoG, Cited by: SS1.
* [5]P. W. Battaglia, R. Pascanu, M. Lai, D. J. Rezende, and K. Kavukcuoglu (2016) Interaction networks for learning about objects, relations and physics. In NIPS, Cited by: SS1.
* [6]A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and O. Yakhnenko (2013) Translating embeddings for modeling multi-relational data. In NIPS, Cited by: SS1.
* [7]G. Corso, L. Cavalleri, D. Beaini, P. Lio, and P. Velickovic (2020) Principal neighbourhood aggregation for graph nets. In NeurIPS, Cited by: SS1.
* [8]M. de Rijke (2000) A note on graded modal logic. In Stud Logica, Cited by: SS1.
* [9]T. Dettmers, M. Pasquale, S. Pontus, and S. Riedel (2018) Convolutional 2D knowledge graph embeddings. In AAAI, Cited by: SS1.
* [10]L. Antonio Galarraga, C. Teflioudi, K. Hose, and F. Suchanek (2013) AMIE: association rule mining under incomplete evidence in ontological knowledge bases. In WWW, Cited by: SS1.
* [11]J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl (2017) Neural message passing for quantum chemistry. In ICML, Cited by: SS1.
* [12]M. Gori, G. Monfardini, and F. Scarselli (2005) A new model for learning in graph domains. In IJCNN, Cited by: SS1.
* [13]M. Grohe (2021) The logic of graph neural networks. In LICS, Cited by: SS1.
* [14]A. Grover and J. Leskovec (2016) Node2vec: scalable feature learning for networks. In KDD, Cited by: SS1.
* [15]D. Scott Himmelstein, A. Lizee, C. Hessler, L. Brueggeman, S. L. Chen, D. Hadley, A. Green, P. Khankhanian, and S. E. Baranzini (2017) Systematic integration of biomedical knowledge prioritizes drugs for repurposing. In eLife, Cited by: SS1.
* [16]W. Hu, M. Fey, M. Zitnik, Y. Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec (2020) Open graph benchmark: datasets for machine learning on graphs. In NeurIPS, Cited by: SS1.
* [17]T. Kipf and M. Welling (2017) Semi-supervised classification with graph convolutional networks. In ICLR, Cited by: SS1.

* [18] Lecheng Kong, Yixin Chen, and Muhan Zhang. Geodesic graph neural network for efficient graph representation learning. In _NeurIPS_, 2022.
* [19] Carsten Lutz, Ulrike Sattler, and Frank Wolter. Modal logic and the two-variable fragment. In _CSL_, 2001.
* [20] Christian Meilicke, Manuel Fink, Yanjie Wang, Daniel Ruffinelli, Rainer Gemulla, and Heiner Stuckenschmidt. Fine-grained evaluation of rule- and embedding-based systems for knowledge graph completion. In _IWSW_, 2018.
* [21] Christopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. Weisfeiler and Leman go neural: Higher-order graph neural networks. In _AAAI_, 2019.
* [22] Christopher Morris, Gaurav Rattan, and Petra Mutzel. Weisfeiler and leman go sparse: Towards scalable higher-order graph embeddings. In _NeurIPS_, 2020.
* [23] Martin Otto. Graded modal logic and counting bisimulation. In _arXiv_, 2019.
* [24] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: online learning of social representations. In _KDD_, 2014.
* [25] Ali Sadeghian, Mohammadreza Armandpour, Patrick Ding, and Daisy Zhe Wang. Drum: End-to-end differentiable rule mining on knowledge graphs. In _NIPS_, 2019.
* [26] Ryoma Sato, Makoto Yamada, and Hisashi Kashima. Random features strengthen graph neural networks. In _SDM_, 2021.
* [27] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. _IEEE Transactions on Neural Networks_, 2009.
* [28] Michael Sejr Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. Modeling relational data with graph convolutional networks. In _ESWC_, 2018.
* [29] Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. Rotate: Knowledge graph embedding by relational rotation in complex space. In _ICLR_, 2019.
* [30] Komal K. Teru, Etienne G. Denis, and William L. Hamilton. Inductive relation prediction by subgraph reasoning. In _ICML_, 2020.
* [31] Kristina Toutanova and Danqi Chen. Observed versus latent features for knowledge base and text inference. In _CVSC_, 2015.
* [32] Shikhar Vashishth, Soumya Sanyal, Vikram Nitin, and Partha Talukdar. Composition-based multi-relational graph convolutional networks. In _ICLR_, 2020.
* [33] Hongwei Wang, Hongyu Ren, and Jure Leskovec. Relational message passing for knowledge graph completion. In _KDD_, 2021.
* [34] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In _ICLR_, 2019.
* [35] Fan Yang, Zhilin Yang, and William W Cohen. Differentiable learning of logical rules for knowledge base reasoning. In _NeurIPS_, 2017.
* [36] Jiaxuan You, Jonathan M. Gomes-Selman, Rex Ying, and Jure Leskovec. Identity-aware graph neural networks. In _AAAI_, 2021.
* [37] Donghan Yu, Yiming Yang, Ruohong Zhang, and Yuexin Wu. Generalized multi-relational graph convolution network. In _ICML Workshop on Graph Representation Learning and Beyond_, 2020.
* [38] Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. In _NeurIPS_, 2018.

* [39] Muhan Zhang, Pan Li, Yinglong Xia, Kai Wang, and Long Jin. Labeling trick: A theory of using graph neural networks for multi-node representation learning. In _NeurIPS_, 2021.
* [40] Zhaocheng Zhu, Zuobai Zhang, Louis-Pascal Xhonneux, and Jian Tang. Neural bellman-ford networks: A general graph neural network framework for link prediction. In _NeurIPS_, 2021.

Proofs of technical statements

### Expressive power of R-MPNNs

The expressive power of R-MPNNs has been recently characterized in terms of a relational variant of the Weisfeiler-Leman test [4]. We define the _relational local \(1\)-WL test2_, which we denote by \(\mathsf{rwl}_{1}\). Let \(G=(V,E,R,c)\) be a knowledge graph. For each \(t\geq 0\), the test updates the coloring as follows:

Footnote 2: This test over single-relation graphs is often called _color refinement_[13]. In Barcelo et al. [4] it is also called _multi-relational \(1\)-WL_.

\[\mathsf{rwl}_{1}^{(0)}(v) =c(v),\] \[\mathsf{rwl}_{1}^{(t+1)}(v) =\tau\big{(}\mathsf{rwl}_{1}^{(t)}(v),\{\!\!\{(\mathsf{rwl}_{1}^{ (t)}(v),r)|\;w\in\mathcal{N}_{r}(v),r\in R\}\!\!\}\big{)},\]

where \(\tau\) injectively maps the above pair to a unique color, which has not been used in previous iterations. Hence, \(\mathsf{rwl}_{1}^{(t)}\) defines a node invariant for all \(t\geq 0\).

The following result generalizes results from Barcelo et al. [4].

**Theorem A.1**.: _Let \(G=(V,E,R,c)\) be a knowledge graph._

1. _For all initial feature maps_ \(\bm{x}\) _with_ \(c\equiv\bm{x}\)_, all R-MPNNs with_ \(T\) _layers, and_ \(0\leq t\leq T\)_, it holds that_ \(\mathsf{rwl}_{1}^{(t)}\preceq\bm{h}^{(t)}\)_._
2. _For all_ \(T\geq 0\) _and history function_ \(f\)_, there is an initial feature map_ \(\bm{x}\) _with_ \(c\equiv\bm{x}\) _and an R-MPNN without global readout with_ \(T\) _layers and history function_ \(f\)_, such that for all_ \(0\leq t\leq T\) _we have_ \(\mathsf{rwl}_{1}^{(t)}\equiv\bm{h}^{(t)}\)_._

Intuitively, item (1) states that the relational local \(1\)-WL algorithm upper bounds the power of any R-MPNN \(\mathcal{A}\): if the test cannot distinguish two nodes, then \(\mathcal{A}\) cannot either. On the other hand, item (2) states that R-MPNNs can be as expressive as \(\mathsf{rwl}_{1}\): for any time limit \(T\), there is an R-MPNN that simulates \(T\) iterations of the test. This holds even without a global readout component.

_Remark A.2_.: A direct corollary of Theorem A.1 is that R-MPNNs have the same expressive power as \(\mathsf{rwl}_{1}\), _independently_ of their history function.

In order to prove Theorem A.1, we define a variant of \(\mathsf{rwl}_{1}\) as follows. For a history function \(f:\mathbb{N}\rightarrow\mathbb{N}\) (recall \(f\) is non-decreasing and \(f(t)\leq t\)), and given a knowledge graph \(G=(V,E,R,c)\), we define the \(\mathsf{rwl}_{1,f}\) test via the following update rules:

\[\mathsf{rwl}_{1,f}^{(0)}(v) =c(v)\] \[\mathsf{rwl}_{1,f}^{(t+1)}(v) =\tau\big{(}\mathsf{rwl}_{1,f}^{(f(t))}(v),\{\!\!\{(\mathsf{rwl}_ {1,f}^{(t)}(v),r)|\;w\in\mathcal{N}_{r}(v),r\in R\}\!\!\}\big{)},\]

where \(\tau\) injectively maps the above pair to a unique color, which has not been used in previous iterations. Note that \(\mathsf{rwl}_{1}\) corresponds to \(\mathsf{rwl}_{1,id}\) for the identity function \(id(t)=t\).

We have that \(\mathsf{rwl}_{1,f}^{(t)}\) is always a refinement of \(\mathsf{rwl}_{1,f}^{(t-1)}\). Note that this is trivial for \(f(t)=t\) but not for arbitrary history functions.

**Proposition A.3**.: _Let \(G=(V,E,R,c)\) be a knowledge graph and \(f\) be a history function. Then for all \(t\geq 0\), we have \(\mathsf{rwl}_{1,f}^{(t+1)}\preceq\mathsf{rwl}_{1,f}^{(t)}\)._

Proof.: We proceed by induction on \(t\). For \(t=0\), note that \(\mathsf{rwl}_{1,f}^{(1)}(u)=\mathsf{rwl}_{1,f}^{(1)}(v)\) implies \(\mathsf{rwl}_{1,f}^{(f(0))}(u)=\mathsf{rwl}_{1,f}^{(f(0))}(v)\) and \(f(0)=0\). For the inductive case, suppose \(\mathsf{rwl}_{1,f}^{(t+1)}(u)=\mathsf{rwl}_{1,f}^{(t+1)}(v)\), for \(t\geq 1\) and \(u,v\in V\). By injectivity of \(\tau\), we have that:

\[\mathsf{rwl}_{1,f}^{(f(t))}(u) =\mathsf{rwl}_{1,f}^{(f(t))}(v)\] \[\{\!\!\{(\mathsf{rwl}_{1,f}^{(t)}(w),r)\mid w\in\mathcal{N}_{r}( u),r\in R\}\!\!\} =\{\!\!\{(\mathsf{rwl}_{1,f}^{(t)}(w),r)\mid w\in\mathcal{N}_{r}(v),r\in R \}\!\!\}.\]By inductive hypothesis and the facts that \(f(t-1)\leq f(t)\) (as \(f\) is non-decreasing) and \(\mathsf{rwl}^{(f(t))}_{1,f}(u)=\mathsf{rwl}^{(f(t))}_{1,f}(v)\), we obtain \(\mathsf{rwl}^{(f(t-1))}_{1,f}(u)=\mathsf{rwl}^{(f(t-1))}_{1,f}(v)\). On the other hand, by inductive hypothesis, we obtain that

\[\{(\mathsf{rwl}^{(t-1)}_{1,f}(w),r)\mid w\in\mathcal{N}_{r}(u),r\in R\}=\{( \mathsf{rwl}^{(t-1)}_{1,f}(w),r)\mid w\in\mathcal{N}_{r}(v),r\in R\}.\]

We conclude that \(\mathsf{rwl}^{(t)}_{1,f}(u)=\mathsf{rwl}^{(t)}_{1,f}(v)\). 

As it turns out, \(\mathsf{rwl}^{(t)}_{1,f}\) defines the same coloring independently of \(f\):

**Proposition A.4**.: _Let \(G=(V,E,R,c)\) be a knowledge graph and \(f,f^{\prime}\) be history functions. Then for all \(t\geq 0\), we have that \(\mathsf{rwl}^{(t)}_{1,f}\equiv\mathsf{rwl}^{(t)}_{1,f^{\prime}}\)._

Proof.: We apply induction on \(t\). For \(t=0\), we have \(\mathsf{rwl}^{(0)}_{1,f}\equiv c\equiv\mathsf{rwl}^{(0)}_{1,f^{\prime}}\). For the inductive case, suppose \(\mathsf{rwl}^{(t)}_{1,f}(u)=\mathsf{rwl}^{(t)}_{1,f}(v)\), for \(t\geq 1\) and \(u,v\in V\). Since \(f^{\prime}(t-1)\leq t-1\) and by Proposition A.3, we have that \(\mathsf{rwl}^{(f^{\prime}(t-1))}_{1,f}(u)=\mathsf{rwl}^{(f^{\prime}(t-1))}_{1,f}(v)\). The inductive hypothesis implies that \(\mathsf{rwl}^{(f^{\prime}(t-1))}_{1,f^{\prime}}(u)=\mathsf{rwl}^{(f^{\prime} (t-1))}_{1,f^{\prime}}(v)\). On the other hand, by injectivity of \(\tau\) we have

\[\{(\mathsf{rwl}^{(t-1)}_{1,f}(w),r)\mid w\in\mathcal{N}_{r}(u),r\in R\}=\{( \mathsf{rwl}^{(t-1)}_{1,f}(w),r)\mid w\in\mathcal{N}_{r}(v),r\in R\}.\]

The inductive hypothesis implies that

\[\{(\mathsf{rwl}^{(t-1)}_{1,f^{\prime}}(w),r)\mid w\in\mathcal{N}_{r}(u),r\in R \}=\{(\mathsf{rwl}^{(t-1)}_{1,f^{\prime}}(w),r)\mid w\in\mathcal{N}_{r}(v),r \in R\}.\]

Summing up, we have that \(\mathsf{rwl}^{(t)}_{1,f^{\prime}}(u)=\mathsf{rwl}^{(t)}_{1,f^{\prime}}(v)\), and hence \(\mathsf{rwl}^{(t)}_{1,f}\preceq\mathsf{rwl}^{(t)}_{1,f^{\prime}}\). The case \(\mathsf{rwl}^{(t)}_{1,f^{\prime}}\preceq\mathsf{rwl}^{(t)}_{1,f}\) follows by symmetry. 

Now we are ready to prove Theorem A.1.

We start with item (1). Take an initial feature map \(\bm{x}\) with \(c\equiv\bm{x}\), an R-MPNN with \(T\) layers, and history function \(f\). It suffices to show that \(\mathsf{rwl}^{(t)}_{1,f}\prec\bm{h}^{(t)}\), for all \(0\leq t\leq T\). Indeed, by Proposition A.4, we have \(\mathsf{rwl}^{(t)}_{1,f}\equiv\mathsf{rwl}^{(t)}_{1,id}\equiv\mathsf{rwl}^{( t)}_{1}\), where \(id\) is the identity function \(id(t)=t\), and hence the result follows. We apply induction on \(t\). The case \(t=0\) follows directly as \(\mathsf{rwl}^{(0)}_{1,f}\equiv c\equiv\bm{x}\equiv\bm{h}^{(0)}\). For the inductive case, assume \(\mathsf{rwl}^{(t)}_{1,f}(u)=\mathsf{rwl}^{(t)}_{1,f}(v)\) for \(t\geq 1\) and \(u,v\in V\). By injectivity of \(\tau\) we have:

\[\mathsf{rwl}^{(f(t-1))}_{1,f}(u)=\mathsf{rwl}^{(f(t-1))}_{1,f}(v)\]

By inductive hypothesis we have (recall \(f(t-1)\leq t-1\)):

\[\bm{h}^{(f(t-1))}_{u} =\bm{h}^{(f(t-1))}_{v}\] \[\{\!\{\bm{h}^{(t-1)}_{w}\mid w\in\mathcal{N}_{r}(u)\}\!\}=\{\!\{ \bm{h}^{(t-1)}_{w}\mid w\in\mathcal{N}_{r}(v)\}\!\}\qquad\text{for each $r\in R$}.\]

This implies that \(\{\!\{\!\mathsf{Msg}_{r}(\bm{h}^{(t-1)}_{w})\mid w\in\mathcal{N}_{r}(u)\}\!\}= \{\!\{\!\mathsf{Msg}_{r}(\bm{h}^{(t-1)}_{w})\mid w\in\mathcal{N}_{r}(v)\}\!\}\), for each \(r\in R\), and hence:

\[\{\!\{\!\mathsf{Msg}_{r}(\bm{h}^{(t-1)}_{w})\mid w\in\mathcal{N}_{r}(u),r\in R \}\!\}=\{\!\{\!\mathsf{Msg}_{r}(\bm{h}^{(t-1)}_{w})\mid w\in\mathcal{N}_{r}(v),r \in R\}\!\}.\]

We conclude that

\[\bm{h}^{(t)}_{u} =\mathsf{Upd}\big{(}\bm{h}^{(f(t-1))}_{u},\mathsf{Agg}(\{\! \{\mathsf{Msg}_{r}(\bm{h}^{(t-1)}_{w})\mid w\in\mathcal{N}_{r}(u),r\in R\}\! \}),\mathsf{Read}(\{\!\{\bm{h}^{(t)}_{w}\mid w\in V\}\!\}))\] \[=\mathsf{Upd}\big{(}\bm{h}^{(f(t-1))}_{v},\mathsf{Agg}(\{\! \{\mathsf{Msg}_{r}(\bm{h}^{(t-1)}_{w})\mid w\in\mathcal{N}_{r}(v),r\in R\}\! \}),\mathsf{Read}(\{\!\{\bm{h}^{(t)}_{w}\mid w\in V\}\!\}))\] \[=\bm{h}^{(t)}_{v}.\]For item (2), we refine the proof of Theorem 2 from Barcelo et al. [4], which is based on ideas from Morris et al. [21]. In comparison with Barcelo et al. [4], in our case, we have arbitrary adjacency matrices for each relation type, not only symmetric ones, and arbitrary history functions, not only the identity. However, the arguments still apply. Moreover, the most important difference is that here we aim for a model of R-MPNN without global readout that uses a single parameter matrix, instead of two parameter matrices as in Barcelo et al. [4] (one for self-representations and the other for neighbors representations). This makes the simulation of \(\mathsf{rw}_{1}\) more challenging.

We use models of R-MPNNs without global readout of the following form:

\[\bm{h}_{v}^{(t+1)}=\operatorname{sign}\Big{(}\bm{W}^{(t)}\big{(}\bm{h}_{v}^{( f(t))}+\sum_{r\in R}\sum_{w\in\mathcal{N}_{r}(v)}\alpha_{r}\bm{h}_{w}^{(t)} \big{)}-\bm{b}\Big{)},\]

where \(\bm{W}^{(t)}\) is a parameter matrix and \(\bm{b}\) is the bias term (we shall use the all-ones vector \(\bm{b}=\bm{1}\)). As message function \(\textsc{Msg}_{r}\) we use _vector scaling_, that is, \(\textsc{Msg}_{r}(\bm{h})=\alpha_{r}\bm{h}\), where \(\alpha_{r}\) is a parameter of the model. For the non-linearity, we use the \(\operatorname{sign}\) function \(\operatorname{sign}\). We note that the proof also works for the ReLU function, following arguments from Corollary 16 in Morris et al. [21].

For a matrix \(\bm{B}\), we denote by \(\bm{B}_{i}\) its \(i\)-th column. Let \(n=|V|\) and without loss of generality assume \(V=\{1,\dots,n\}\). We will write features maps \(\bm{h}:V\to\mathbb{R}^{d}\) for \(G=(V,E,R,c)\) also as matrices \(\bm{H}\in\mathbb{R}^{d\times n}\), where the column \(\bm{H}_{v}\) corresponds to the \(d\)-dimensional feature vector for \(v\). Then we can also write our R-MPNN model in matrix form:

\[\bm{H}^{(t+1)}=\operatorname{sign}\Big{(}\bm{W}^{(t)}\big{(}\bm{H}^{(f(t))}+ \sum_{r\in R}\alpha_{r}\bm{H}^{(t)}\bm{A}_{r}\big{)}-\bm{J}\Big{)},\]

where \(\bm{A}_{r}\) is the adjacency matrix of \(G\) for relation type \(r\in R\) and \(\bm{J}\) is the all-ones matrix of appropriate dimensions.

Let \(\bm{Fts}\) be the following \(n\times n\) matrix:

\[\bm{Fts}=\begin{bmatrix}-1&-1&\cdots&-1&-1\\ 1&-1&\ddots&&-1\\ \vdots&\ddots&\ddots&\ddots&\vdots\\ 1&&\ddots&-1&-1\\ 1&1&\cdots&1&-1\end{bmatrix}\]

That is, \((\bm{Fts})_{ij}=-1\) if \(j\geq i\), and \((\bm{Fts})_{ij}=1\) otherwise. Note that the columns of \(\bm{Fts}\) are linearly independent. We shall use the columns of \(\bm{Fts}\) as node features in our simulation.

The following lemma is an adaptation of Lemma 9 from Morris et al. [21].

**Lemma A.5**.: _Let \(\bm{B}\in\mathbb{N}^{n\times p}\) be a matrix such that \(p\leq n\), and all the columns are pairwise distinct and different from the all-zeros column. Then there is a matrix \(\bm{X}\in\mathbb{R}^{n\times n}\) such that the matrix \(\operatorname{sign}(\bm{X}\bm{B}-\bm{J})\in\{-1,1\}^{n\times p}\) is precisely the sub-matrix of \(\bm{Fts}\) given by its first \(p\) columns._

Proof.: Let \(\bm{z}=(1,m,m^{2},\dots,m^{n-1})\in\mathbb{N}^{1\times n}\), where \(m\) is the largest entry in \(\bm{B}\), and \(\bm{b}=\bm{z}\bm{B}\in\mathbb{N}^{1\times p}\). By construction, the entries of \(\bm{b}\) are positive and pairwise distinct. Without loss of generality, we assume that \(\bm{b}=(b_{1},b_{2},\dots,b_{p})\) for \(b_{1}>b_{2}>\dots>b_{p}>0\). As the \(b_{i}\) are ordered, we can choose numbers \(x_{1},\dots,x_{p}\in\mathbb{R}\) such that \(b_{i}\cdot x_{j}<1\) if \(i\geq j\), and \(b_{i}\cdot x_{j}>1\) if \(i<j\), for all \(i,j\in\{1,\dots,p\}\). Let \(\bm{x}=(x_{1},\dots,x_{p},2/b_{p},\dots,2/b_{p})^{T}\in\mathbb{R}^{n\times 1}\). Note that \((2/b_{p})\cdot b_{i}>1\), for all \(i\in\{1,\dots,p\}\). Then \(\operatorname{sign}(\bm{x}\bm{b}-\bm{J})\) is precisely the sub-matrix of \(\bm{Fts}\) given by its first \(p\) columns. We can choose \(\bm{X}=\bm{x}\bm{z}\in\mathbb{R}^{n\times n}\). 

Now we are ready to show item (2). Let \(f\) be any history function and \(T\geq 0\). It suffices to show that there is a feature map \(\bm{x}\) with \(c\equiv\bm{x}\) and an R-MPNN without global readout with \(T\) layers and history function \(f\) such that \(\mathsf{rw}\mathsf{l}_{1,f}^{(t)}\equiv\bm{h}^{(t)}\), for all \(0\leq t\leq T\). Indeed, by Proposition A.4, we have \(\mathsf{rw}\mathsf{l}_{1,f}^{(t)}\equiv\mathsf{rw}\mathsf{l}_{1,id}^{(t)}\equiv \mathsf{rw}\mathsf{l}_{1}^{(t)}\), where \(id\) is the identity function \(id(t)=t\), and then the result follows. We conclude item (2) by showing the following lemma:

**Lemma A.6**.: _There is a feature map \(\bm{h}^{(0)}:V\to\mathbb{R}^{n}\), and for all \(0\leq t<T\), there is a feature map \(\bm{h}^{(t+1)}:V\to\mathbb{R}^{n}\), a matrix \(\bm{W}^{(t)}\in\mathbb{R}^{n\times n}\) and scaling factors \(\alpha_{r}^{(t)}\in\mathbb{R}\), for each \(r\in R\), such that:_

* \(\bm{h}^{(t)}\equiv\mathsf{rwl}_{1,f}^{(t)}\)_._
* _The columns of_ \(\bm{H}^{(t)}\in\mathbb{R}^{n\times n}\) _are columns of_ \(\bm{Fts}\) _(recall_ \(\bm{H}^{(t)}\) _is the matrix representation of_ \(\bm{h}^{(t)}\)_)._
* \(\bm{H}^{(t+1)}=\operatorname{sign}\Big{(}\bm{W}^{(t)}\big{(}\bm{H}^{(f(t))}+ \sum_{r\in R}\alpha_{r}^{(t)}\bm{H}^{(t)}\bm{A}_{r}\big{)}-\bm{J}\Big{)}\)_._

Proof.: We proceed by induction on \(t\). Suppose that the node coloring \(\mathsf{rwl}_{1,f}^{(0)}\equiv c\) uses colors \(1,\dots,p\), for \(p\leq n\). Then we choose \(\bm{h}^{(0)}\) (this is the initial feature map \(\bm{x}\) in the statement of item (2)) such that \(\bm{h}^{(0)}_{v}=\bm{Fts}_{c(v)}\), that is, \(\bm{h}^{(0)}_{v}\) is the \(c(v)\)-th column of \(\bm{Fts}\). We have that \(\bm{h}^{(0)}\) satisfies the required conditions.

For the inductive case, assume that \(\bm{h}^{(t^{\prime})}\equiv\mathsf{rwl}_{1,f}^{(t^{\prime})}\) and the columns of \(\bm{H}^{(t^{\prime})}\) are columns of \(\bm{Fts}\), for all \(0\leq t^{\prime}\leq t<T\). We need to find \(\bm{h}^{(t+1)}\), \(\bm{W}^{(t)}\) and \(\alpha_{r}^{(t)}\) satisfying the conditions. Let \(\bm{M}\in\mathbb{R}^{n\times n}\) be the matrix inverse of \(\bm{Fts}\). If \(\bm{H}^{(t^{\prime})}_{v}\) is the \(i\)-th column of \(\bm{Fts}\), we say that \(v\) has color \(i\) at iteration \(t^{\prime}\). Observe that for all \(0\leq t^{\prime}\leq t\), we have

\[(\bm{M}\bm{H}^{(t^{\prime})})_{iv}=\begin{cases}1&\text{if $v$ has color $i$ at iteration $t^{\prime}$}\\ 0&\text{otherwise}.\end{cases}\]

In other words, the \(v\)-th column of \(\bm{M}\bm{H}^{(t^{\prime})}\) is simply a one-hot encoding of the color of \(v\) at iteration \(t^{\prime}\). For each \(r\in R\) we have

\[(\bm{M}\bm{H}^{(t)}\bm{A}_{r})_{iv}=|\{w\in\mathcal{N}_{r}(v)\mid\text{$w$ has color $i$ at iteration $t$}\}|.\]

Hence the \(v\)-th column of \(\bm{M}\bm{H}^{(t)}\bm{A}_{r}\) is an encoding of the multiset of colors for the neighborhood \(\mathcal{N}_{r}(v)\), at iteration \(t\). Let \(r_{1},\dots,r_{m}\) be an enumeration of the relation types in \(R\). Let \(\bm{D}\in\mathbb{R}^{(m+1)n\times n}\) be the matrix obtained by horizontally concatenating the matrices \(\bm{M}\bm{H}^{(f(t))}\), \(\bm{M}\bm{H}^{(t)}\bm{A}_{r_{1}},\dots,\bm{M}\bm{H}^{(t)}\bm{A}_{r_{m}}\). Since \(\bm{H}^{(f(t))}\equiv\mathsf{rwl}_{1,f}^{(f(t))}\) and \(\bm{H}^{(t)}\equiv\mathsf{rwl}_{1,f}^{(t)}\), we have that \(\bm{D}\equiv\mathsf{rwl}_{1,f}^{(t+1)}\). Now note that \(\bm{D}\equiv\bm{E}\), where

\[\bm{E}=\bm{M}\bm{H}^{(f(t))}+\sum_{i=1}^{m}(n+1)^{i}\bm{M}\bm{H}^{(t)}\bm{A}_{ r_{i}}.\]

Indeed, \(\bm{E}_{iv}\) is simply the \((n+1)\)-base representation of the vector \((\bm{D}_{iv},\bm{D}_{(n+i)v},\bm{D}_{(2n+i)v}\dots,\bm{D}_{(mn+i)v})\), and hence \(\bm{E}_{u}=\bm{E}_{v}\) if and only if \(\bm{D}_{u}=\bm{D}_{v}\) (note that the entries of \(\bm{D}\) are in \(\{0,\dots,n\}\)). In particular, \(\bm{E}\equiv\mathsf{rwl}_{1,f}^{(t+1)}\).

Let \(p\) be the number of distinct columns of \(\bm{E}\) and let \(\widetilde{\bm{E}}\in\mathbb{N}^{n\times p}\) be the matrix whose columns are the distinct columns of \(\bm{E}\) in an arbitrary but fixed order. We can apply Lemma A.5 to \(\widetilde{\bm{E}}\) and obtain a matrix \(\bm{X}\in\mathbb{R}^{n\times n}\) such that \(\operatorname{sign}(\bm{X}\widetilde{\bm{E}}-\bm{J})\) is precisely the sub-matrix of \(\bm{Fts}\) given by its first \(p\) columns. We choose \(\bm{H}^{(t+1)}=\operatorname{sign}(\bm{X}\bm{E}-\bm{J})\in\mathbb{R}^{n\times n}\), \(\bm{W}^{(t)}=\bm{X}\bm{M}\in\mathbb{R}^{n\times n}\) and \(\alpha_{r_{i}}^{(t)}=(n+1)^{i}\). Note that the columns of \(\bm{H}^{(t+1)}\) are columns of \(\bm{Fts}\), and that \(\bm{H}^{(t+1)}\equiv\bm{E}\equiv\mathsf{rwl}_{1,f}^{(t+1)}\). Finally, we have

\[\bm{H}^{(t+1)} =\operatorname{sign}(\bm{X}\bm{E}-\bm{J})\] \[=\operatorname{sign}(\bm{X}\big{(}\bm{M}\bm{H}^{(f(t))}+\sum_{i=1 }^{m}(n+1)^{i}\bm{M}\bm{H}^{(t)}\bm{A}_{r_{i}}\big{)}-\bm{J})\] \[=\operatorname{sign}(\bm{W}^{(t)}\big{(}\bm{H}^{(f(t))}+\sum_{i=1 }^{m}\alpha_{r_{i}}^{(t)}\bm{H}^{(t)}\bm{A}_{r_{i}}\big{)}-\bm{J}).\]

[MISSING_PAGE_FAIL:18]

[MISSING_PAGE_EMPTY:19]

We remark that \(\mathsf{rFO}^{2}_{\mathrm{cnt}}\) is actually the fragment of FO used in Barcelo et al. [3] to characterize GNNs, adapted to multiple relations. It is well-known that \(\mathsf{rFO}^{2}_{\mathrm{cnt}}\) is equivalent to _graded modal logic_[8].

The following proposition provides useful translations from \(\mathsf{rFO}^{2}_{\mathrm{cnt}}\) to \(\mathsf{rFO}^{3}_{\mathrm{cnt}}\) and vice versa. Recall from Section A.2, that given a knowledge graph \(G=(V,E,R,\eta)\) where \(\eta\) is a pairwise coloring, we define the knowledge graph \(G^{2}=(V\times V,E^{\prime},R,c_{\eta})\) where \(E^{\prime}=\{r((u,w),(u,v))\mid r(w,v)\in E,r\in R\}\) and \(c_{\eta}\) is the node coloring \(c_{\eta}((u,v))=\eta(u,v)\).

**Proposition A.10**.: _We have the following:_

1. _For all_ \(\mathsf{rFO}^{3}_{\mathrm{cnt}}\) _formula_ \(\varphi(x,y)\)_, there is a formula_ \(\tilde{\varphi}(x)\) _in_ \(\mathsf{rFO}^{2}_{\mathrm{cnt}}\) _such that for all knowledge graph_ \(G=(V,E,R,\eta)\)_, we have_ \(G,u,v\models\varphi\) _if and only if_ \(G^{2},(u,v)\models\tilde{\varphi}\)_._
2. _For all formula_ \(\varphi(x)\) _in_ \(\mathsf{rFO}^{2}_{\mathrm{cnt}}\)_, there is a_ \(\mathsf{rFO}^{3}_{\mathrm{cnt}}\) _formula_ \(\tilde{\varphi}(x,y)\) _such that for all knowledge graph_ \(G=(V,E,R,\eta)\)_, we have_ \(G,u,v\models\tilde{\varphi}\) _if and only if_ \(G^{2},(u,v)\models\varphi\)_._

Proof.: We start with item (1). We define \(\tilde{\varphi}(x)\) by induction on the formula \(\varphi(x,y)\):

1. If \(\varphi(x,y)=a(x,y)\) for color \(a\), then \(\tilde{\varphi}(x)=a(x)\).
2. If \(\varphi(x,y)=\neg\psi(x,y)\), then \(\tilde{\varphi}(x)=\neg\tilde{\psi}(x)\).
3. If \(\varphi(x,y)=\varphi_{1}(x,y)\wedge\varphi_{2}(x,y)\), then \(\tilde{\varphi}(x)=\tilde{\varphi}_{1}(x)\wedge\tilde{\varphi}_{2}(x)\).
4. If \(\varphi(x,y)=\exists^{\geq N}z\left(\psi(x,z)\wedge r(z,y)\right)\) then \(\tilde{\varphi}(x)=\exists^{\geq N}y\left(\tilde{\psi}(y)\wedge r(y,x)\right)\).

Fix \(G=(V,E,R,\eta)\) and \(G^{2}=(V\times V,E^{\prime},R,c_{\eta})\). We show by induction on the formula \(\varphi\) that \(G,u,v\models\varphi\) if and only if \(G^{2},(u,v)\models\tilde{\varphi}\).

For the base case, that is, case (1) above, we have that \(\varphi(x,y)=a(x,y)\) and hence \(G,u,v\models\varphi\) iff \(\eta(u,v)=a\) iff \(c_{\eta}((u,v))=a\) iff \(G^{2},(u,v)\models\tilde{\varphi}\).

Now we consider the inductive case. For case (2) above, we have \(\varphi(x,y)=\neg\psi(x,y)\). Then \(G,u,v\models\varphi\) iff \(G,u,v\not\models\psi\) iff \(G^{2},(u,v)\not\models\tilde{\psi}\) iff \(G^{2},(u,v)\models\tilde{\varphi}\).

For case (3), we have \(\varphi(x,y)=\varphi_{1}(x,y)\wedge\varphi_{2}(x,y)\). Then \(G,u,v\models\varphi\) iff \(G,u,v\models\varphi_{1}\) and \(G,u,v\models\varphi_{2}\) iff \(G^{2},(u,v)\models\tilde{\varphi_{1}}\) and \(G^{2},(u,v)\models\tilde{\varphi_{2}}\) iff \(G^{2},(u,v)\models\tilde{\varphi}\).

Finally, for case (4), we have \(\varphi(x,y)=\exists^{\geq N}z\left(\psi(x,z)\wedge r(z,y)\right)\). Assume \(G,u,v\models\varphi\), then there exist at least \(N\) nodes \(w\in V\) such that \(G,u,w\models\psi\) and \(r(w,v)\in E\). By the definition of \(G^{2}\), there exist at least \(N\) nodes in \(G^{2}\) of the form \((u,w)\) such that \(G^{2},(u,w)\models\tilde{\psi}\) and \(r((u,w),(u,v))\in E^{\prime}\). It follows that \(G^{2},(u,v)\models\tilde{\varphi}\). On the other hand, suppose \(G^{2},(u,v)\models\tilde{\varphi}\). Then there exist at least \(N\) nodes \((o,o^{\prime})\) in \(G^{2}\) such that \(G,(o,o^{\prime})\models\tilde{\psi}\) and \(r((o,o^{\prime}),(u,v))\in E^{\prime}\). By definition of \(G^{2}\) each \((o,o^{\prime})\) must be of the form \((o,o^{\prime})=(u,w)\) for some \(w\in V\) such that \(r(w,v)\). Then there are at least \(N\) nodes \(w\in V\) such that \(G,u,w\models\psi\) and \(r(w,v)\in E\). It follows that \(G,u,v\models\varphi\).

Item (2) is similar. We define \(\tilde{\varphi}(x,y)\) by induction on the formula \(\varphi(x)\):

1. If \(\varphi(x)=a(x)\) for color \(a\), then \(\tilde{\varphi}(x,y)=a(x,y)\).
2. If \(\varphi(x)=\neg\psi(x)\), then \(\tilde{\varphi}(x,y)=\neg\tilde{\psi}(x,y)\).
3. If \(\varphi(x)=\varphi_{1}(x)\wedge\varphi_{2}(x)\), then \(\tilde{\varphi}(x,y)=\tilde{\varphi}_{1}(x,y)\wedge\tilde{\varphi}_{2}(x,y)\).
4. If \(\varphi(x)=\exists^{\geq N}y\left(\psi(y)\wedge r(y,x)\right)\) then \(\tilde{\varphi}(x,y)=\exists^{\geq N}z\left(\tilde{\psi}(x,z)\wedge r(z,y)\right)\).

Following the same inductive argument from item (1), we obtain that \(G,u,v\models\tilde{\varphi}\) if and only if \(G^{2},(u,v)\models\varphi\). 

The following theorem is an adaptation of Theorem 4.2 from Barcelo et al. [3]. The main difference with Barcelo et al. [3] is that here we need to handle multiple relation types.

**Theorem A.11**.: _A logical classifier is captured by R-MPNNs without global readout if and only if it can be expressed in \(\mathsf{rFO}^{2}_{\mathrm{cnt}}\)._Proof.: We start with the backward direction. Let \(\varphi(x)\) be a formula in \(\mathsf{r}\mathsf{C}^{2}_{\mathrm{cnt}}\) for relation types \(R\) and node colors \(\mathcal{C}\). Let \(\varphi_{1},\ldots,\varphi_{L}\) be an enumeration of the subformulas of \(\varphi\) such that if \(\varphi_{i}\) is a subformula of \(\varphi_{j}\), then \(i\leq j\). In particular, \(\varphi_{L}=\varphi\). We shall define an R-MPNN without global readout \(\mathcal{B}_{\varphi}\) with \(L\) layers computing \(L\)-dimensional features in each layer. The idea is that at layer \(\ell\in\{1,\ldots,L\}\), the \(\ell\)-th component of the feature \(\bm{h}_{v}^{(\ell)}\) is computed correctly and corresponds to \(1\) if \(\varphi_{\ell}\) is satisfied in node \(v\), and \(0\) otherwise. We add an additional final layer that simply outputs the last component of the feature vector.

We use models of R-MPNNs of the following form:

\[\bm{h}_{v}^{(t+1)}=\sigma\Big{(}\bm{W}\bm{h}_{v}^{(t)}+\sum_{r\in R}\sum_{w \in\mathcal{N}_{r}(v)}\bm{W}_{r}\bm{h}_{w}^{(t)}+\bm{b}\Big{)},\]

where \(\bm{W}\in\mathbb{R}^{L\times L}\) is a parameter matrix and \(\bm{b}\in\mathbb{R}^{L}\) is the bias term. As message function \(\textsc{Msg}_{r}\) we use \(\textsc{Msg}_{r}(\bm{h})=\bm{W}_{r}\bm{h}\), where \(\bm{W}_{r}\in\mathbb{R}^{L\times L}\) is a parameter matrix. For the non-linearity \(\sigma\) we use the truncated ReLU function \(\sigma(x)=\min(\max(0,x),1)\). The \(\ell\)-th row of \(\bm{W}\) and \(\bm{W}_{r}\), and the \(\ell\)-th entry of \(\bm{b}\) are defined as follows (omitted entries are 0):

1. If \(\varphi_{\ell}(x)=a(x)\) for a color \(a\in\mathcal{C}\), then \(\bm{W}_{\ell\ell}=1\).
2. If \(\varphi_{\ell}(x)=\neg\varphi_{k}(x)\) then \(\bm{W}_{\ell k}=-1\), and \(b_{\ell}=1\).
3. If \(\varphi_{\ell}(x)=\varphi_{j}(x)\wedge\varphi_{k}(x)\) then \(\bm{W}_{\ell j}=1\), \(\bm{W}_{\ell k}=1\) and \(b_{\ell}=-1\).
4. If \(\varphi_{\ell}(x)=\exists^{\geq N}y\left(\varphi_{k}(y)\wedge r(y,x)\right)\) then \((\bm{W}_{r})_{\ell k}=1\) and \(b_{\ell}=-N+1\).

Let \(G=(V,E,R,c)\) be a knowledge graph with node colors from \(\mathcal{C}\). In order to apply \(\mathcal{B}_{\varphi}\) to \(G\), we choose initial \(L\)-dimensional features \(\bm{h}_{v}^{(0)}\) such that \((\bm{h}_{v}^{(0)})_{\ell}=1\) if \(\varphi_{\ell}=a(x)\) and \(a\) is the color of \(v\), and \((\bm{h}_{v}^{(0)})_{\ell}=0\) otherwise. In other words, the \(L\)-dimensional initial feature \(\bm{h}_{v}^{(0)}\) is a one-hot encoding of the color of \(v\). It follows from the same arguments than Proposition 4.1 in Barcelo et al. [3] that for all \(\ell\in\{1,\ldots,L\}\) we have \((\bm{h}_{v}^{(t)})_{\ell}=1\) if \(G,v\models\varphi_{\ell}\) and \((\bm{h}_{v}^{(t)})_{\ell}=0\) otherwise, for all \(v\in V\) and \(t\in\{\ell,\ldots,L\}\). In particular, after \(L\) layers, \(\mathcal{B}_{\varphi}\) calculates \(\bm{h}_{v}^{(L)}\) such that \((\bm{h}_{v}^{(L)})_{L}=1\) if \(G,v\models\varphi\) and \((\bm{h}_{v}^{(L)})_{L}=0\) otherwise. As layer \(L+1\) extracts the \(L\)-th component of the feature vector, the result follows.

For the forward direction, we follow the strategy of Theorem 4.2 from Barcelo et al. [3]. Given a knowledge graph \(G=(V,E,R,c)\) and a number \(L\in\mathbb{N}\), we define the _unravelling_ of \(v\in V\) at depth \(L\), denoted \(\mathsf{Unr}_{G}^{L}(v)\) is the knowledge graph having:

* A node \((v,u_{1},\ldots,u_{i})\) for each directed path \(u_{i},\ldots,u_{1},v\) in \(G\) of length \(i\leq L\).
* For each \(r\in R\), a fact \(r((v,u_{1},\ldots,u_{i}),(v,u_{1},\ldots,u_{i-1}))\) for all facts \(r(u_{i},u_{i-1})\in E\) (here \(u_{0}:=v\)).
* Each node \((v,u_{1},\ldots,u_{i})\) is colored with \(c(u_{i})\), that is, the same color as \(u_{i}\).

Note that the notion of directed path is defined in the obvious way, as for directed graphs but ignoring relation types. Note also that \(\mathsf{Unr}_{G}^{L}(v)\) is a tree in the sense that the underlying undirected graph is a tree.

The following proposition is a trivial adaptation of Observation C.3 from Barcelo et al. [3]. We write \(\mathsf{Unr}_{G}^{L}(v)\simeq\mathsf{Unr}_{G^{\prime}}^{L}(v^{\prime})\) if there exists an isomorphism \(f\) from \(\mathsf{Unr}_{G}^{L}(v)\) to \(\mathsf{Unr}_{G^{\prime}}^{L}(v^{\prime})\) such that \(f(v)=v^{\prime}\).

**Proposition A.12**.: _Let \(G\) and \(G^{\prime}\) be two knowledge graphs and \(v\) and \(v^{\prime}\) be nodes in \(G\) and \(G^{\prime}\), respectively. Then, for all \(L\in\mathbb{N}\), we have that \(\mathsf{r}\mathsf{w}_{1}^{(L)}(v)\) on \(G\) coincides with \(\mathsf{r}\mathsf{w}_{1}^{(L)}(v^{\prime})\) on \(G^{\prime}\) if and only if \(\mathsf{Unr}_{G}^{L}(v)\simeq\mathsf{Unr}_{G^{\prime}}^{L}(v^{\prime})\)._

As a consequence of Theorem A.1, we obtain:

**Proposition A.13**.: _Let \(G\) and \(G^{\prime}\) be two knowledge graphs and \(v\) and \(v^{\prime}\) be nodes in \(G\) and \(G^{\prime}\), respectively, such that \(\mathsf{Unr}^{L}_{G}(v)\simeq\mathsf{Unr}^{L}_{G^{\prime}}(v^{\prime})\) for all \(L\in\mathbb{N}\). Then for any R-MPNN without global readout with \(T\) layers, we have that \(\bm{h}^{(T)}_{v}\) on \(G\) coincides with \(\bm{h}^{(T)}_{v^{\prime}}\) on \(G^{\prime}\)._

Finally, the following theorem follows from Theorem C.5 in Barcelo et al. [3], which in turn follows from Theorem 2.2 in Otto [23]. The key observation here is that the results from Otto [23] are actually presented for multi-modal logics, that is, multiple relation types.

**Theorem A.14**.: _[_23_]_ _Let \(\alpha\) be a unary FO formula over knowledge graphs. If \(\alpha\) is not equivalent to a \(\mathsf{rFO}^{2}_{\mathrm{cnt}}\) formula, then there exist two knowledge graphs \(G\) and \(G^{\prime}\), and two nodes \(v\) in \(G\) and \(v^{\prime}\) in \(G^{\prime}\) such that \(\mathsf{Unr}^{L}_{G}(v)\simeq\mathsf{Unr}^{L}_{G^{\prime}}(v^{\prime})\) for all \(L\in\mathbb{N}\) and such that \(G,v\models\alpha\) but \(G^{\prime},v^{\prime}\not\models\alpha\)._

Now we are ready to obtain the forward direction of the theorem. Suppose that a logical classifier \(\alpha\) is captured by an R-MPNN without global readout \(\mathcal{B}\) with \(T\) layers, and assume by contradiction that \(\alpha\) is not equivalent to a \(\mathsf{rFO}^{2}_{\mathrm{cnt}}\) formula. Then we can apply Theorem A.14 and obtain two knowledge graphs \(G\) and \(G^{\prime}\), and two nodes \(v\) in \(G\) and \(v^{\prime}\) in \(G^{\prime}\) such that \(\mathsf{Unr}^{L}_{G}(v)\simeq\mathsf{Unr}^{L}_{G^{\prime}}(v^{\prime})\) for all \(L\in\mathbb{N}\) and such that \(G,v\models\alpha\) but \(G^{\prime},v^{\prime}\not\models\alpha\). Applying Proposition A.13, we have that \(\bm{h}^{(T)}_{v}\) on \(G\) coincides with \(\bm{h}^{(T)}_{v^{\prime}}\) on \(G^{\prime}\), and hence \(\mathcal{B}\) classifies either both \(v\) and \(v^{\prime}\) as true over \(G\) and \(G^{\prime}\), respectively, or both as false. This is a contradiction. 

Now Theorem 5.2 follows easily. Let \(\alpha\) be a logical binary classifier and suppose it is captured by a C-MPNN without global readout \(\mathcal{A}\). By Proposition A.7,item (1), we know that \(\mathcal{A}\) can be simulated by a R-MPNN without global readout \(\mathcal{B}\) over \(G^{2}\). In turn, we can apply Theorem A.11 and obtain a formula \(\varphi\) in \(\mathsf{rFO}^{2}_{\mathrm{cnt}}\) equivalent to \(\mathcal{B}\). Finally, we can apply the translation in Proposition A.10, item (2), and obtain a corresponding formula \(\tilde{\varphi}\) in \(\mathsf{rFO}^{3}_{\mathrm{cnt}}\). We claim that \(\tilde{\varphi}\) captures \(\mathcal{A}\). Let \(G\) be a knowledge graph and \(u,v\) two nodes. We have that \(G,u,v\models\tilde{\varphi}\) iff \(G^{2},(u,v)\models\varphi\) iff \(\mathcal{B}\) classifies \((u,v)\) as true over \(G^{2}\) iff \(\mathcal{A}\) classifies \((u,v)\) as true over \(G\).

The other direction is obtained analogously following the reverse translations.

### Proof of Theorem 5.3

We recall the statement of the theorem:

**Theorem 5.3**.: _Each logical binary classifier expressible in \(\mathsf{erFO}^{3}_{\mathrm{cnt}}\) can be captured by a C-MPNN._

We start by formally defining the logic \(\mathsf{erFO}^{3}_{\mathrm{cnt}}\), which is a simple extension of \(\mathsf{rFO}^{3}_{\mathrm{cnt}}\) from Section 5.2. Fix a set of relation types \(R\) and a set of pair colors \(\mathcal{C}\). Recall we consider knowledge graphs of the form \(G=(V,E,R,\eta)\) where \(\eta\) is a mapping assigning colors from \(\mathcal{C}\) to pairs of nodes from \(V\). The logic \(\mathsf{erFO}^{3}_{\mathrm{cnt}}\) contains only binary formulas and it is defined inductively as follows: First, \(a(x,y)\) for \(a\in\mathcal{C}\), is in \(\mathsf{erFO}^{3}_{\mathrm{cnt}}\). Second, if \(\varphi(x,y)\) and \(\psi(x,y)\) are in \(\mathsf{erFO}^{3}_{\mathrm{cnt}}\), \(N\geq 1\) is a positive integer, and \(r\in R\), then the formulas

\[\neg\varphi(x,y),\quad\varphi(x,y)\wedge\psi(x,y),\quad\exists^{\geq N}z\,( \varphi(x,z)\wedge r(z,y)),\quad\exists^{\geq N}z\,(\varphi(x,z)\wedge\neg r(z,y))\]

are also in \(\mathsf{erFO}^{3}_{\mathrm{cnt}}\). As expected, \(a(u,v)\) holds in \(G=(V,E,R,\eta)\) if \(\eta(u,v)=a\), and \(\exists^{\geq N}z\,(\varphi(u,z)\wedge\ell(z,v))\) holds in \(G\), for \(\ell\in R\cup\{\neg r\mid r\in R\}\), if there are at least \(N\) nodes \(w\in V\) for which \(\varphi(u,w)\) and \(\ell(w,v)\) hold in \(G\).

Intuitively, \(\mathsf{erFO}^{3}_{\mathrm{cnt}}\) extends \(\mathsf{rFO}^{3}_{\mathrm{cnt}}\) with _negated modalities_, that is, one can check for non-neighbors of a node. As it turns out, \(\mathsf{erFO}^{3}_{\mathrm{cnt}}\) is strictly more expressive than \(\mathsf{rFO}^{3}_{\mathrm{cnt}}\).

**Proposition A.15**.: _There is a formula in \(\mathsf{erFO}^{3}_{\mathrm{cnt}}\) that cannot be expressed by any formula in \(\mathsf{rFO}^{3}_{\mathrm{cnt}}\)._

Proof.: Consider the following two knowledge graphs \(G_{1}\) and \(G_{2}\) over \(R=\{r\}\) and colors \(\mathcal{C}=\{\mathsf{Green}\}\). The graph \(G_{1}\) has two nodes \(u\) and \(v\) and one edge \(r(u,v)\). The graph \(G_{2}\) has three nodes \(u\), \(v\) and \(w\) and one edge \(r(u,v)\) (hence \(w\) is an isolated node). In both graphs, all pairs are colored Green.

We show first that for every formula \(\varphi(x,y)\) in \(\mathsf{rFO}^{3}_{\mathrm{cnt}}\), it is the case that \(G_{1},u,n\models\varphi\) iff \(G_{2},u,n\models\varphi\), for \(n\in\{u,v\}\). We proceed by induction on the formula. For \(\varphi(x,y)=\mathsf{Green}(x,y)\), we havethat \(G_{1},u,n\models\varphi\) and \(G_{2},u,n\models\varphi\) and hence we are done. Suppose now that \(\varphi(x,y)=\neg\psi(x,y)\). Assume \(G_{1},u,n\models\varphi\) for \(n\in\{u,v\}\). Then \(G_{1},u,n\not\models\psi\), and by inductive hypothesis, we have \(G_{2},u,n\not\models\psi\) and then \(G_{2},u,n\models\varphi\). The other direction is analogous. Assume now that \(\varphi(x,y)=\psi_{1}(x,y)\wedge\psi_{2}(x,y)\). Suppose \(G_{1},u,n\models\varphi\) for \(n\in\{u,v\}\). Then \(G_{1},u,n\models\psi_{1}\) and \(G_{1},u,n\models\psi_{2}\), and by inductive hypothesis, \(G_{2},u,n\models\psi_{1}\) and \(G_{2},u,n\models\psi_{2}\). Then \(G_{2},u,n\models\varphi\). The other direction is analogous. Finally, suppose that \(\varphi(x,y)=\exists^{\geq}Nz(\psi(x,z)\wedge r(z,y))\). Assume that \(G_{1},u,n\models\varphi\) for \(n\in\{u,v\}\). Then there exist at least \(N\) nodes \(w\in\mathcal{N}_{r}(n)\) in \(G_{1}\) such that \(G_{1},u,w\models\psi\). Since \(\mathcal{N}_{r}(u)=\emptyset\), we have \(n=v\). As \(\mathcal{N}_{r}(v)=\{u\}\), we have \(w=u\). Since the neighborhood \(\mathcal{N}_{r}(v)\) is the same in \(G_{1}\) and \(G_{2}\), and by the inductive hypothesis, we have that there are at least \(N\) nodes \(w\in\mathcal{N}_{r}(v)\) in \(G_{2}\) such that \(G_{2},u,w\models\psi\). This implies that \(G_{2},u,v\models\varphi\). The other direction is analogous.

Now consider the \(\mathsf{erFO}^{3}_{\mathsf{cnt}}\) formula \(\varphi(x,y)=\exists^{\geq 2}z(\mathsf{Green}(x,z)\wedge\neg r(z,y))\). We claim that there is no \(\mathsf{rFO}^{3}_{\mathsf{cnt}}\) formula equivalent to \(\varphi(x,y)\). By contradiction, suppose we have such an equivalent formula \(\varphi^{\prime}\). As shown above, we have \(G_{1},u,v\models\varphi^{\prime}\) iff \(G_{2},u,v\models\varphi^{\prime}\). On the other hand, by definition, we have that \(G_{1},u,v\not\models\varphi\) and \(G_{2},u,v\models\varphi\). This is a contradiction. 

Now we are ready to prove Theorem 5.3. We follow the same strategy as in the proof of the backward direction of Theorem A.11.

Let \(\varphi(x,y)\) be a formula in \(\mathsf{erFO}^{3}_{\mathsf{cnt}}\), for relation types \(R\) and pair colors \(\mathcal{C}\). Let \(\varphi_{1},\ldots,\varphi_{L}\) be an enumeration of the subformulas of \(\varphi\) such that if \(\varphi_{i}\) is a subformula of \(\varphi_{j}\), then \(i\leq j\). In particular, \(\varphi_{L}=\varphi\). We will construct a C-MPNN \(\mathcal{A}_{\varphi}\) with \(L\) layers computing \(L\)-dimensional features in each layer. At layer \(\ell\in\{1,\ldots,L\}\), the \(\ell\)-th component of the feature \(\boldsymbol{h}^{(\ell)}_{v|u,q}\) will correspond to \(1\) if \(\varphi_{\ell}\) is satisfied on \((u,v)\), and \(0\) otherwise. The query relation \(q\) plays no role in the construction, that is, for any possible \(q\in R\) the output of \(\mathcal{A}_{\varphi}\) is the same. Hence, for simplicity, in the remaining of the proof we shall write \(\boldsymbol{h}^{(\ell)}_{v|u}\) instead of \(\boldsymbol{h}^{(\ell)}_{v|u,q}\). We add an additional final layer that simply outputs the last component of the feature vector.

We use models of C-MPNNs of the following form:

\[\boldsymbol{h}^{(\ell+1)}_{v|u}=\sigma\Big{(}\boldsymbol{W}_{0}\boldsymbol{ h}^{(\ell)}_{v}+\sum_{r\in R}\sum_{w\in\mathcal{N}_{r}(v)}\boldsymbol{W}_{r} \boldsymbol{h}^{(\ell)}_{w|u}+\boldsymbol{W}_{1}\sum_{w\in V}\boldsymbol{h}^{ (\ell)}_{w|u}+\boldsymbol{b}\Big{)},\]

where \(\boldsymbol{W}_{0},\boldsymbol{W}_{1}\in\mathbb{R}^{L\times L}\) are parameter matrices and \(\boldsymbol{b}\in\mathbb{R}^{L}\) is the bias term. As message function \(\textsc{Msg}_{r}\) we use \(\textsc{Msg}_{r}(\boldsymbol{h})=\boldsymbol{W}_{r}\boldsymbol{h}\), where \(\boldsymbol{W}_{r}\in\mathbb{R}^{L\times L}\) is a parameter matrix. For the non-linearity \(\sigma\) we use the truncated ReLU function \(\sigma(x)=\min(\max(0,x),1)\). The \(\ell\)-th row of \(\boldsymbol{W}_{0}\), \(\boldsymbol{W}_{1}\) and \(\boldsymbol{W}_{r}\), and the \(\ell\)-th entry of \(\boldsymbol{b}\) are defined as follows (omitted entries are \(0\)):

1. If \(\varphi_{\ell}(x,y)=a(x,y)\) for a color \(a\in\mathcal{C}\), then \((\boldsymbol{W}_{0})_{\ell\ell}=1\).
2. If \(\varphi_{\ell}(x,y)=\neg\varphi_{k}(x,y)\) then \((\boldsymbol{W}_{0})_{\ell k}=-1\), and \(b_{\ell}=1\).
3. If \(\varphi_{\ell}(x,y)=\varphi_{j}(x,y)\wedge\varphi_{k}(x,y)\) then \((\boldsymbol{W}_{0})_{\ell j}=1\), \((\boldsymbol{W}_{0})_{\ell k}=1\) and \(b_{\ell}=-1\).
4. If \(\varphi_{\ell}(x,y)=\exists^{\geq N}z\left(\varphi_{k}(x,z)\wedge r(z,y)\right)\) then \((\boldsymbol{W}_{r})_{\ell k}=1\) and \(b_{\ell}=-N+1\).
5. If \(\varphi_{\ell}(x,y)=\exists^{\geq N}z\left(\varphi_{k}(x,z)\wedge\neg r(z,y)\right)\) then \((\boldsymbol{W}_{r})_{\ell k}=-1\), \((\boldsymbol{W}_{1})_{\ell k}=1\) and \(b_{\ell}=-N+1\).

Let \(G=(V,E,R,\eta)\) be a knowledge graph with pair colors from \(\mathcal{C}\). In order to apply \(\mathcal{A}_{\varphi}\) to \(G\), we choose the initialization \(\textsc{Init}\) such that \(L\)-dimensional initial features \(\boldsymbol{h}^{(0)}_{v|u}\) satisfy \((\boldsymbol{h}^{(0)}_{v|u})_{\ell}=1\) if \(\varphi_{\ell}=a(x,y)\) and \(\eta(u,v)=a\) and \((\boldsymbol{h}^{(0)}_{v|u})_{\ell}=0\) otherwise. That is, the \(L\)-dimensional initial feature \(\boldsymbol{h}^{(0)}_{v|u}\) is a one-hot encoding of the color of \((u,v)\). Using the same arguments as in the proof of Theorem A.11, we have \((\boldsymbol{h}^{(t)}_{v|u})_{\ell}=1\) if \(G,u,v\models\varphi_{\ell}\) and \((\boldsymbol{h}^{(t)}_{v|u})_{\ell}=0\) otherwise, for all \(u,v\in V\) and \(t\in\{\ell,\ldots,L\}\). In particular, after \(L\) layers, \(\mathcal{A}_{\varphi}\) calculates \(\boldsymbol{h}^{(L)}_{v|u}\) such that \((\boldsymbol{h}^{(L)}_{v|u})_{L}=1\) if \(G,u,v\models\varphi\) and \((\boldsymbol{h}^{(L)}_{v|u})_{L}=0\) otherwise. As layer \(L+1\) extracts the \(L\)-th component of the feature vector, the result follows.

_Remark A.16_.: We note that in Barcelo et al. [3], it is shown that a more expressive graded modal logic (with more expressive modalities) can be captured by R-MPNNs in the context of single-relation

[MISSING_PAGE_EMPTY:24]

[MISSING_PAGE_EMPTY:25]

Thus, it holds that \(\mathsf{rawl}_{2}^{(k+1)}(u,v)=\mathsf{rawl}_{2}^{(k+1)}(u^{\prime},v^{\prime})\) by definition of \(\mathsf{rawl}_{2}^{(k+1)}(u,v)\) and \(\mathsf{rawl}_{2}^{(k+1)}(u^{\prime},v^{\prime})\).

For counter-example, we show the case for \(t\geq 0\). Consider a relational graph \(G^{\prime}=(V^{\prime},E^{\prime},R,c,\eta)\) such that \(V^{\prime}=\{u,u^{\prime},v,x\}\), \(E^{\prime}=\{r(x,u^{\prime})\}\), and \(R=\{r\}\) with the initial labeling \(\eta\) for node pairs \((u,v)\) and \((u^{\prime},v)\) satisfies \(\mathsf{rwl}_{2}^{(0)}(u,v)=\mathsf{rwl}_{2}^{(0)}(u^{\prime},v)\) and \(\mathsf{rawl}_{2}^{(0)}(u,v)=\mathsf{rawl}_{2}^{(0)}(u^{\prime},v)\). For such graph \(G^{\prime}\), we consider node pair \((u,v)\) and \((u^{\prime},v)\). For \(\mathsf{rawl}_{2}^{(t)}\) where \(t\geq 0\), we show by induction that \(\mathsf{rawl}_{2}^{(t)}(u,v)=\mathsf{rawl}_{2}^{(t)}(u^{\prime},v)\). The base case is trivial by assumption. The inductive step shows that by the inductive hypothesis:

\[\mathsf{rawl}_{2}^{(t+1)}(u,v) =\tau(\mathsf{rawl}_{2}^{(t)}(u,v),\{\!\!\{\}\})\] \[=\tau(\mathsf{rawl}_{2}^{(t)}(u^{\prime},v),\{\!\!\{\}\})\] \[=\mathsf{rawl}_{2}^{(t+1)}(u^{\prime},v)\]

On the other hand, we have

\[\mathsf{rwl}_{2}^{(1)}(u,v) =\tau(\mathsf{rwl}_{2}^{(0)}(u,v),\{\!\!\{\}\},\{\!\})\] \[\neq\mathsf{rwl}_{2}^{(1)}(u,v^{\prime}) =\tau(\mathsf{rwl}_{2}^{(0)}(u,v^{\prime}),\{\!\!\{\!\{\}\!\{ \mathsf{rwl}_{2}^{(0)}(x,v^{\prime}),r)\}\},\{\!\!\{\}\})\]

**Proposition A.19**.: _For all \(t\geq 0\) and all knowledge graph \(G\), let \(\mathsf{rwl}_{2}^{\perp^{(0)}}(G)\equiv\mathsf{rwl}_{2}^{(0)}(G)\). The following statements hold:_

1. _For every_ \(t>0\)_,_ \(\mathsf{rwl}_{2}^{+^{(t)}}(G)\preceq\mathsf{rwl}_{2}^{(t)}(G)\)__
2. _There is a knowledge graph_ \(G^{\prime\prime}\) _and pair of nodes_ \((u,v)\) _and_ \((u^{\prime},v^{\prime})\) _such that_ \(\mathsf{rwl}_{2}^{(t)}(G^{\prime\prime},u,v)=\mathsf{rwl}_{2}^{(t)}(G^{ \prime\prime},u^{\prime},v^{\prime})\) _for all_ \(t\geq 0\) _but_ \(\mathsf{rwl}_{2}^{+^{(1)}}(G^{\prime\prime},u,v)\neq\mathsf{rwl}_{2}^{+^{(1)} }(G^{\prime\prime},u^{\prime},v^{\prime})\)_._

Proof.: As before, we first prove \(\mathsf{rwl}_{2}^{+}(G)\preceq\mathsf{rwl}_{2}(G)\) by induction on iteration \(t\). The base case for \(t=0\) is trivial by the assumption. By the inductive hypothesis,

\[\mathsf{rwl}_{2}^{+^{(k)}}(u,v)=\mathsf{rwl}_{2}^{+^{(k)}}(u^{\prime},v^{ \prime})\implies\mathsf{rwl}_{2}^{(k)}(u,v)=\mathsf{rwl}_{2}^{(k)}(u^{\prime}, v^{\prime})\]for some \(k\). Thus, assuming \(\mathsf{rwl}_{2}^{+\,(k+1)}(u,v)=\mathsf{rwl}_{2}^{+\,(k+1)}(u^{\prime},v^{\prime})\), by the definition of \(\mathsf{rwl}_{2}^{+\,(k+1)}(u,v)\) and \(\mathsf{rwl}_{2}^{+\,(k+1)}(u^{\prime},v^{\prime})\) and by the injectivity of \(\tau\), it holds that

\[\mathsf{rwl}_{2}^{+\,(k)}(u,v) =\mathsf{rwl}_{2}^{+\,(k)}(u^{\prime},v^{\prime})\] \[\{\!(\mathsf{rwl}_{2}^{+\,(k)}(x,v),r)\mid x\in\mathcal{N}_{r}^{ +}(u),r\in R^{+}\}\!\!\!=\{\!(\mathsf{rwl}_{2}^{+\,(k)}(x^{\prime},v^{\prime}),r^{\prime})\mid x^{\prime}\in\mathcal{N}_{r^{\prime}}^{+}(u^{\prime}),r^{ \prime}\in R^{+}\}\!\!\!\}\] \[\{\!(\mathsf{rwl}_{2}^{+\,(k)}(u,x),r)\mid x\in\mathcal{N}_{r}^{ +}(v),r\in R^{+}\}\!\!\!=\{\!(\mathsf{rwl}_{2}^{+\,(k)}(u^{\prime},x^{\prime} ),r^{\prime})\mid x^{\prime}\in\mathcal{N}_{r^{\prime}}^{+}(v^{\prime}),r^{ \prime}\in R^{+}\}\!\!\!\}\]

By the similar argument in proving \(\mathsf{rawl}_{2}^{+}\preceq\mathsf{rawl}_{2}\), we can split the multiset into two equations by decomposing \(\mathcal{N}_{r}^{+}(u)=\mathcal{N}_{r}(u)\cup\mathcal{N}_{r^{-}}(u)\) and \(\mathcal{N}_{r}^{+}(v)=\mathcal{N}_{r}(v)\cup\mathcal{N}_{r^{-}}(v)\). Thus, we have

\[\{\!(\mathsf{rwl}_{2}^{+\,(k)}(x,v),r)\mid x\in\mathcal{N}_{r}(u),r\in R\}\!\!\!=\{\!(\mathsf{rwl}_{2}^{+\,(k)}(x^{\prime},v^{\prime}),r^{ \prime})\mid x^{\prime}\in\mathcal{N}_{r^{\prime}}(u^{\prime}),r^{\prime}\in R\}\] \[\{\!(\mathsf{rwl}_{2}^{+\,(k)}(y,v),r^{-})\mid y\in\mathcal{N}_{ r^{-}}(u),r\in R\}\!\!\!=\{\!(\mathsf{rwl}_{2}^{+\,(k)}(y^{\prime},v^{\prime}),r^{ \prime})\mid y^{\prime}\in\mathcal{N}_{r^{\prime-}}(u^{\prime}),r^{\prime}\in R\}\] \[\{\!(\mathsf{rwl}_{2}^{+\,(k)}(u,x),r)\mid x\in\mathcal{N}_{r}(v ),r\in R\}\!\!\!=\{\!(\mathsf{rwl}_{2}^{+\,(k)}(u^{\prime},x^{\prime}),r^{ \prime})\mid x^{\prime}\in\mathcal{N}_{r^{\prime}}(v^{\prime}),r^{\prime}\in R\}\] \[\{\!(\mathsf{rwl}_{2}^{+\,(k)}(u,y),r^{-})\mid y\in\mathcal{N}_{ r^{-}}(v),r\in R\}\!\!\!=\{\!(\mathsf{rwl}_{2}^{+\,(k)}(u^{\prime},y^{\prime}),r^{ \prime-})\mid y^{\prime}\in\mathcal{N}_{r^{\prime-}}(v^{\prime}),r^{\prime}\in R\}\]

By the inductive hypothesis and unpacking the first and the third equations, we can further imply that

\[\mathsf{rwl}_{2}^{(k)}(u,v) =\mathsf{rwl}_{2}^{(k)}(u^{\prime},v^{\prime})\] \[\{\!(\mathsf{rwl}_{2}^{(k)}(x,v),r)\mid x\in\mathcal{N}_{r}(u),r\in R\}\!\!\!=\{\!(\mathsf{rwl}_{2}^{(k)}(x^{\prime},v^{\prime}),r^{\prime}) \mid x^{\prime}\in\mathcal{N}_{r^{\prime}}(u^{\prime}),r^{\prime}\in R\}\] \[\{\!(\mathsf{rwl}_{2}^{(k)}(u,x),r)\mid x\in\mathcal{N}_{r}(v),r\in R\}\!\!\!=\{\!(\mathsf{rwl}_{2}^{(k)}(u^{\prime},x^{\prime}),r^{\prime}) \mid x^{\prime}\in\mathcal{N}_{r^{\prime}}(v^{\prime}),r^{\prime}\in R\}\]

This would results in \(\mathsf{rwl}_{2}^{(k+1)}(u,v)=\mathsf{rwl}_{2}^{(k+1)}(u^{\prime},v^{\prime})\) by the definition of \(\mathsf{rwl}_{2}^{(k+1)}(u,v)\) and \(\mathsf{rwl}_{2}^{(k+1)}(u^{\prime},v^{\prime})\)

For the counter-example, we consider a relational graph \(G^{\prime\prime}=(V^{\prime\prime},E^{\prime\prime},R^{\prime\prime},c,\eta)\) such that \(V^{\prime\prime}=\{u,u^{\prime},v,v^{\prime},x,x^{\prime}\}\), \(E^{\prime\prime}=\{r_{1}(u,x),r_{2}(u^{\prime},x^{\prime})\}\), and \(R^{\prime\prime}=\{r_{1},r_{2}\}\). For such graph \(G^{\prime\prime}\), we set the initial labeling \(\eta\) for node pairs \((u,v)\) and \((u^{\prime},v^{\prime})\) to satisfy \(\mathsf{rwl}_{2}^{+\,(0)}(u,v)=\mathsf{rwl}_{2}^{+\,(0)}(u^{\prime},v^{\prime})\) and \(\mathsf{rwl}_{2}^{(0)}(u,v)=\mathsf{rwl}_{2}^{(0)}(u^{\prime},v^{\prime})\). For such graph \(G^{\prime\prime}\), we consider node pair \((u,v)\) and \((u^{\prime},v^{\prime})\).

Consider \(\mathsf{rwl}_{2}^{(t)}\) where \(t\geq 0\), we show by induction that \(\mathsf{rwl}_{2}^{(t)}(u,v)=\mathsf{rwl}_{2}^{(t)}(u^{\prime},v^{\prime})\). The base case is trivial by assumption. The inductive step shows that by inductive hypothesis,

\[\mathsf{rwl}_{2}^{(t+1)}(u,v) =\tau(\mathsf{rwl}_{2}^{(t)}(u,v),\{\!\{\!\}\!\},\{\!\})\] \[=\tau(\mathsf{rwl}_{2}^{(t)}(u^{\prime},v^{\prime}),\{\!\{\!\}\!\}, \{\!\})\] \[=\mathsf{rwl}_{2}^{(t+1)}(u^{\prime},v^{\prime})\]

On the other hand, we have

\[\mathsf{rwl}_{2}^{+\,(1)}(u,v) =\tau(\mathsf{rwl}_{2}^{+\,(0)}(u,v),\{\!\!\{\!\}\!\}\,(\mathsf{ rwl}_{2}^{+\,(0)}(x,v),r_{1}^{-})\},\{\!\!\{\!\}\,\{\!\})\] \[\neq\mathsf{rwl}_{2}^{+\,(1)}(u^{\prime},v^{\prime}) =\tau(\mathsf{rwl}_{2}^{+\,(0)}(u^{\prime},v^{\prime}),\{\!\{\!\}\! \},\{\!\{\!\}\,\!\{\!\}\,\!\{\!\}\,\!\,\!\{\!\}\,\!\,\!\{\!\}\,\!\,\!\})\]

**Proposition A.20**.: _For all \(t\geq 0\) and all knowledge graph \(G\), let \(\mathsf{rwl}_{2}^{(0)}(G)\equiv\mathsf{rawl}_{2}^{+\,(0)}(G)\), then the following statement holds:_

1. _There is a knowledge graph_ \(G\) _and pair of nodes_ \((u,v)\) _and_ \((u^{\prime},v^{\prime})\) _such that_ \(\mathsf{rwl}_{2}^{(t)}(G,u,v)=\mathsf{rwl}_{2}^{+\,(t)}(G^{\prime},u,v)=\mathsf{ rwl}_{2}^{+\,(t)}(G^{\prime},u^{\prime},v^{\prime})\) _for all_ \(t\geq 0\) _but_ \(\mathsf{rwl}_{2}^{+\,(1)}(G,u,v)\neq\mathsf{rwl}_{2}^{+\,(1)}(G,u^{\prime},v^{ \prime})\)_._
2. _There is a knowledge graph_ \(G^{\prime}\) _and pair of nodes_ \((u,v)\) _and_ \((u^{\prime},v^{\prime})\) _such that_ \(\mathsf{rwl}_{2}^{+\,(t)}(G^{\prime},u,v)=\mathsf{rwl}_{2}^{+\,(t)}(G^{\prime},u^{ \prime},v^{\prime})\) _for all_ \(t\geq 0\) _but_ \(\mathsf{rwl}_{2}^{(1)}(G^{\prime},u,v)\neq\mathsf{rwl}_{2}^{(1)}(G^{\prime}

[MISSING_PAGE_EMPTY:28]

**Runtime of C-MPNNs.** Let us note that C-MPNNs subsume NBFNets. Indeed, if we consider C-MPNNs without readout and set the history function as \(f(t)=0\), we obtain (a slight generalization of) NBFNets, as stated in the paper. In terms of the runtime, C-MPNNs and NBFNets are comparable: even though the readout component of C-MPNNs incurs a linear overhead, this is dominated by other factors. Thus, as shown in Lemma 2 of Zhu et al. [40], the total complexity for a forward pass is \(\mathcal{O}(T(|E|d+|V|d^{2}))\) to compute \(|V|\) queries at the same time, resulting an amortized complexity of \(\mathcal{O}(T(\frac{|E|d}{|V|}+\frac{d^{2}}{|R||V|}+d))\). See Zhu et al. [40] for detailed discussion and proof.

**Comparison with R-MPNNs.** As directly carrying out relational message passing in R-MPNN for each triplet is costly, a common way to carry out link prediction task is to first compute all node representations and then use a binary decoder to perform link prediction. This would result in \(\mathcal{O}(T(|E|d+|V|d^{2}))\) for a complete forward pass. Consequently, we obtain \(|R||V|^{2}\) triplets, and since passing through binary decoder has a complexity of \(\mathcal{O}(d)\), we have that the amortized complexity for each query is \(\mathcal{O}(T(\frac{|E|d}{|R||V|^{2}}+\frac{d^{2}}{|R||V|}+d))\). Although the complexity of a forward pass for R-MPNN and C-MPNN are the same, we compute \(|R||V|^{2}\) queries with R-MPNN but only \(|V|\) queries with C-MPNN, resulting difference in amortized complexity of a single query. However, as discussed in Section 4, this comes at the cost of expressivity.

**Comparison with the architectures using labeling trick.** GraIL is an architecture using labeling trick and we focus on this to make the comparison concrete. In terms of runtime, architectures that rely on the labeling trick are typically slower, since they need to label both the source and the target node, as we state in Section 4. This yields worse runtime complexity for these models, and particularly for GraIL 4, with an amortized complexity of a query being the same as the one for a forward pass, that is, \(\mathcal{O}(T(|E|d^{2}+|V|d^{2}))\). In C-MPNNs, we only label the source node \(u\), which allows parallel comparison for all queries in the form \(q(u,?)\): a single forward pass would compute all hidden representations of these queries. With the labeling trick, \(|V|\) forward passes need to be carried out as each time, both source \(u\) and target \(v\) need to be specified. See Zhu et al. [40] for detailed discussion and proof.

Footnote 4: Comparing to C-MPNN, the complexity for \(\textsc{MsG}_{r}\) is different because GraIL utilizes RGCN as relational message passing model, which needs \(\mathcal{O}(d^{2})\) for linear transformation in its \(\textsc{MsG}_{r}\).

**Comparison with higher-order GNNs.** Regarding higher-order GNNs, we take \(2\)-RN, the neural architecture from Barcelo et al. [4] corresponding to \(\textsf{rwl}_{2}\). These higher-order models require \(\mathcal{O}((|V||E|d+|V|^{2}d^{2}))\) in each layer (assuming each message function takes \(\mathcal{O}(d)\) as before) to update all pairwise representations in a single forward pass, which is computationally prohibitive in practice. This makes the study of the amortized complexity obsolete, but we provide an analysis for the sake of completeness. Similar to R-MPNN, we need an additional query-specific unary decoder for the task of link prediction on knowledge graphs, resulting in \(\mathcal{O}(d)\) complexity overhead. The amortized complexity of a single query is thus \(\mathcal{O}(T(\frac{|E|d}{|R||V|}+\frac{d^{2}}{|R|}+d))\).

## Appendix C Experiments on inductive link prediction

### Details of the experiments reported in Section 6

In this section, we report the details of the experiments reported in the body of this paper. Specifically, we report the performance of some baseline models (Table 4), dataset statistics (Table 5), hyperparameters (Table 6), and the number of trainable parameters for each model variation (Table 7).

\begin{table}
\begin{tabular}{c c c} \hline \hline
**Model** & **Complexity of a forward pass** & **Amortized complexity of a query** \\ \hline R-MPNNs & \(\mathcal{O}(T(|E|d+|V|d^{2}))\) & \(\mathcal{O}(T(\frac{|E|d}{|R||V|^{2}}+\frac{d^{2}}{|R||V|}+d))\) \\ C-MPNNs & \(\mathcal{O}(T(|E|d+|V|d^{2}))\) & \(\mathcal{O}(T(\frac{|E|d}{|V|}+d^{2}))\) \\ GraIL & \(\mathcal{O}(T(|E|d^{2}+|V|d^{2}))\) & \(\mathcal{O}(T(|E|d^{2}+|V|d^{2}))\) \\ \(2\)-RN & \(\mathcal{O}(T(|V||E|d+|V|^{2}d^{2}))\) & \(\mathcal{O}(T(\frac{|E|d}{|R||V|}+\frac{d^{2}}{|R|}+d))\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Model asymptotic runtime complexities. Observe that the amortized complexity of \(2\)-RN becomes obsolete by the fact that its forward pass being prohibitive in practice (quadratic in \(|V|\)).

\begin{table}
\begin{tabular}{l l l c c} \hline \hline \multicolumn{2}{l}{**Hyperparameter**} & \multicolumn{3}{c}{**WN18RR**} & \multicolumn{1}{c}{**FB15k-237**} \\ \hline \multirow{2}{*}{**GNN Layer**} & Depth(\(T\)) & \(6\) & \(6\) \\  & Hidden Dimension & \(32\) & \(32\) \\ \hline \multirow{2}{*}{**Decoder Layer**} & Depth & \(2\) & \(2\) \\  & Hidden Dimension & \(64\) & \(64\) \\ \hline \multirow{2}{*}{**Optimization**} & Optimizer & Adam & Adam \\  & Learning Rate & 5e-3 & 5e-3 \\ \hline \multirow{2}{*}{**Learning**} & Batch size & \(8\) & \(8\) \\  & \#Negative Samples & \(32\) & \(32\) \\ \cline{1-1}  & Epoch & \(20\) & \(20\) \\ \cline{1-1}  & Adversarial Temperature & \(1\) & \(1\) \\ \hline \hline \end{tabular}
\end{table}
Table 6: Hyperparameters for inductive experiments with C-MPNN.

\begin{table}
\begin{tabular}{l l c c c c c c c} \hline \hline \multicolumn{2}{l}{**Model architectures**} & \multicolumn{5}{c}{**WN18RR**} & \multicolumn{5}{c}{**FB15k-237**} \\ Agg & \(\texttt{Msg}_{r}\) & \(\texttt{v1}\) & \(\texttt{v2}\) & \(\texttt{v3}\) & \(\texttt{v4}\) & \(\texttt{v1}\) & \(\texttt{v2}\) & \(\texttt{v3}\) & \(\texttt{v4}\) \\ \hline sum & \(\texttt{Msg}_{r}^{1}\) & 132k & 144k & 157k & 132k & 2,310k & 2,564k & 2,755k & 2,806k \\ sum & \(\texttt{Msg}_{r}^{2}\) & 21k & 22k & 22k & 21k & 98k & 107k & 113k & 115k \\ sum & \(\texttt{Msg}_{r}^{3}\) & 128k & 141k & 153k & 128k & 2,240k & 2,488k & 2,673k & 2,722k \\ \hline PNA & \(\texttt{Msg}_{r}^{1}\) & 199k & 212k & 225k & 199k & 2,377k & 2,632k & 2,823k & 2,874k \\ PNA & \(\texttt{Msg}_{r}^{2}\) & 89k & 89k & 90k & 89k & 165k & 174k & 181k & 183k \\ PNA & \(\texttt{Msg}_{r}^{3}\) & 196k & 208k & 221k & 196k & 2,308k & 2,555k & 2,740k & 2,790k \\ \hline \hline \end{tabular}
\end{table}
Table 7: Number of trainable parameters used in inductive relation prediction experiments for C-MPNN architectures with \(\texttt{Init}^{2}\) initialization.
All of the model variations minimize the negative log-likelihood of positive and negative facts. We follow the _partial completeness assumption_[10] by randomly corrupting the head entity or the tail entity to generate the negative samples. We parameterize the conditional probability of a fact \(q(u,v)\) by \(p(v\mid u,q)=\sigma(f(\mathbf{h}_{v\mid u,q}^{(T)}))\), where \(\sigma\) is the sigmoid function and \(f\) is 2-layer MLP. Following RotatE [29], we adopt _self-adversarial negative sampling_ by sampling negative triples from the following distribution with \(\alpha\) as the _adversarial temperature_:

\[\mathcal{L}(v\mid u,q)=-\log p(v\mid u,q)-\sum_{i=1}^{k}w_{i,\alpha}\log(1-p( v_{i}^{\prime}\mid u_{i}^{\prime},q))\]

where \(k\) is the number of negative samples for one positive sample and \((u_{i}^{\prime},q,v_{i}^{\prime})\) is the \(i\)-th negative sample. Finally, \(w_{i}\) is the weight for the \(i\)-th negative sample, given by

\[w_{i,\alpha}:=\mathrm{Softmax}\left(\frac{\log(1-p(v_{i}^{\prime}\mid u_{i}^{ \prime},q))}{\alpha}\right).\]

### Experiments for evaluating the effect of initialization functions

**Initialization functions (Q3).** We argued that the initialization function \(\textsc{Init}(u,v,q)\) needs to satisfy the property of _target node distinguishability_ to compute binary invariants. To validate the impact of different initialization regimes, we conduct a further experiment which is reported in Table 8, with the same experiment settings as in Section 6. In addition to the initialization functions \(\textsc{Init}^{1},\textsc{Init}^{2},\textsc{Init}^{3}\) defined in Section 4.1, we also experiment with a simple function \(\textsc{Init}^{0}\) which assigns \(\mathbf{0}\) to all nodes. As expected, using \(\textsc{Init}^{0}\) initialization, results in a very sharp decrease in model performance in WN18RR, but less so in FB15k-237. Intuitively, the model suffers more in WN18RR since there are much fewer relations, and it is harder to distinguish node pairs without an initialization designed to achieve this. Perhaps one of the simplest functions satisfying target node distinguishability criteria is \(\textsc{Init}^{1}=\mathbb{1}_{u=v}*\mathbf{1}\), which pays no respect to the target query relation. Empirically, \(\textsc{Init}^{1}\) achieves strong results, showing that even the simplest function ensuring this property could boost the model performance. Interestingly, the performance of models using \(\textsc{Init}^{1}\) match or exceed models using \(\textsc{Init}^{2}\), even though the latter additionally has a relation-specific learnable query vector. Note, however, that this shall not undermine the role of the learnable query relation: integrating the learnable query vector either in the initialization function _or_ in the message computation function seems to suffice.

**Random initialization (Q3).** The idea of random node initialization is known to lead to more expressive models [26; 1]. Inspired by this, we can incorporate varying degrees of randomization to the initialization, satisfying the target node distinguishability property in expectation. The key advantage is that the resulting models are still inductive and achieve a nontrivial expressiveness gain over alternatives. In this case, the models with \(\textsc{Init}^{3}\) perform closer to the models with \(\textsc{Init}^{2}\), but we do not see a particular advantage on these benchmarks.

## Appendix D Experiments on transductive link prediction

We further conducted transductive link prediction experiments to empirically validate that C-MPNNs are more expressive than R-MPNNs via the abstraction given by \(\textsc{rawl}_{2}\) and \(\textsc{rwl}_{1}\), respectively (**Q4**).

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline
**Initialization** & \multicolumn{4}{c}{**WN18RR**} & \multicolumn{4}{c}{**FB15k-237**} \\ \(\textsc{Init}(u,v,q)\) & **v1** & **v2** & **v3** & **v4** & **v1** & **v2** & **v3** & **v4** \\ \hline \(\textsc{Init}^{0}(u,v,q)\) & \(0.615\) & \(0.715\) & \(0.811\) & \(0.654\) & \(0.777\) & \(0.903\) & \(0.894\) & \(0.910\) \\ \(\textsc{Init}^{1}(u,v,q)\) & \(0.932\) & \(0.894\) & \(\mathbf{0.902}\) & \(\mathbf{0.883}\) & \(\mathbf{0.809}\) & \(\mathbf{0.927}\) & \(0.944\) & \(0.911\) \\ \(\textsc{Init}^{2}(u,v,q)\) & \(0.932\) & \(\mathbf{0.896}\) & \(0.900\) & \(0.881\) & \(0.794\) & \(0.906\) & \(\mathbf{0.947}\) & \(0.933\) \\ \(\textsc{Init}^{3}(u,v,q)\) & \(\mathbf{0.934}\) & \(0.890\) & \(0.894\) & \(0.877\) & \(0.804\) & \(0.924\) & \(0.941\) & \(\mathbf{0.944}\) \\ \hline \hline \end{tabular}
\end{table}
Table 8: Inductive relation prediction of C-MPNN using Agg = sum, \(\textsc{Msg}_{r}=\textsc{Msg}_{r}^{1}\), \(f(t)=t\) and different initialization methods.

[MISSING_PAGE_EMPTY:32]

**Evaluation.** We consider _filtered ranking protocol_[6] with \(1\) negative sample per positive triplet, and report Mean Rank(MR), Mean Reciprocal Rank(MRR), and Hits@10 for each model. We also report averaged results of _five_ independent runs for all experiments.

**Results.** From Table 9, it is evident that models under the class of C-MPNN consistently outperform R-MPNN models across both datasets and all evaluation metrics, with only one exception on FB15k-237 with NeuralLP. This aligns with our theoretical understanding, as C-MPNN models are inherently more expressive than R-MPNNs. In particular, the C-MPNN-_basic_ model stands out by achieving the lowest MR and the highest values for both MRR and Hits@10, surpassing all other models by a significant margin, underscoring its efficiency and robustness in transductive knowledge graph completion tasks.

### Experiments on biomedical datasets

To compare R-MPNNs and C-MPNNs on large-scale graphs, we carried out additional transductive knowledge graph completion experiments on biomedical datasets: Hetionet [15] and ogbl-biokg [16].

**Datasets.** Hetionet [15] is a large biomedical knowledge graph that integrates data from \(29\) different public databases, representing various biological connections and associations, and ogbl-biokg is a large-scale biomedical knowledge graph dataset, developed as a part of the Open Graph Benchmark (OGB) [16] suite. We have used the provided standardized train-test split for both datasets and similarly augmented every fact \(r(u,v)\) with its inverse relation \(r^{-1}(u,v)\). We present the detailed dataset statistics in Table 13.

**Implementation.** We study the _basic_ C-MPNN without readout, referred to as C-MPNN in the Table 12, the epoch number is set to \(1\) due to the datasets being larger and denser, leading to longer execution times for C-MPNN. All hyperparameters used for C-MPNN are reported in Table 14, and we adopt layer-normalization [2] and short-cut connection after each aggregation and before applying ReLU. We also discard the edges that directly connect query node pairs. For RGCN, we consider a batch size of \(65536\), a learning rate of \(0.001\), and \(100\) epochs for both datasets. On Hetionet, we use a hidden dimension size of \(200\), and a learning rate of \(0.001\) across \(4\) layers. On ogbl-biokg, we consider a dimension size of \(500\), and a dropout rate of \(0.2\) across \(2\) layers. For CompGCN on Hetionet, the parameters include a batch size of \(65536\), \(100\) epochs, a dimension size of \(200\), a learning rate of \(0.01\), and a single layer. We adopt _dismult_ scoring function and element-wise multiplication as a composition function. On ogbl-biokg, CompGCN results cannot be reproduced to the best of our effort due to an out-of-memory (OOM) error. The best checkpoint for each model instance is selected based on its performance on the validation set. All experiments are performed on one NVIDIA V100 32GB GPU. In addition, since there are no node features for both of the datasets, we initialize node representations using learnable embeddings with Xavier initialization for RGCN and CompGCN.

**Evaluation.** We consider _filtered ranking protocol_[6], but restrict our ranking to entities of the same type. With each positive triplet, we use \(32\) number of negative samples and report the averaged results of Mean Reciprocal Rank(MRR), Hits@1, and Hits@10 over _five_ independent runs for each model.

**Results.** From Table 12, it is evident that C-MPNN outperforms R-MPNN on both datasets by a large margin, despite the challenges posed by the large and dense nature of biomedical knowledge graphs. These results are reassuring as they have further consolidated our theory, showing that the expressivity gain of C-MPNNs compared to R-MPNNs has a significant impact on real-world biomedical datasets.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \multirow{2}{*}{**Model**} & \multicolumn{3}{c}{**Hetionet**} & \multicolumn{3}{c}{**ogbl-biokg**} \\  & **MRR** & **Hits@1** & **Hits@10** & **MRR** & **Hits@1** & **Hits@10** \\ \hline RGCN & \(0.120\) & \(0.067\) & \(0.228\) & \(0.636\) & \(0.511\) & \(0.884\) \\ CompGCN & \(0.152\) & \(0.083\) & \(0.292\) & OOM & OOM & OOM \\ C-MPNN & \(\mathbf{0.479}\) & \(\mathbf{0.394}\) & \(\mathbf{0.649}\) & \(\mathbf{0.790}\) & \(\mathbf{0.718}\) & \(\mathbf{0.927}\) \\ \hline \hline \end{tabular}
\end{table}
Table 12: Transductive experiments on biomedical knowledge graphs. We use _basic_ C-MPNN architecture with \(\textsc{Agg}=\text{sum}\), \(\textsc{Msg}=\textsc{Msg}^{1}_{r}\), and \(\textsc{Init}=\textsc{Init}^{2}\) with no readout component. OOM stands for out of memory.

## Appendix E Evaluating the power of global readout on a synthetic dataset

We constructed a synthetic experiment to showcase the power of global readout (**Q5**).

**Dataset.** We proposed a synthetic dataset TRI-SQR, which consists of multiple pairs of knowledge graphs in form \((G_{1},G_{2})\) s.t. \(G_{1}=(V_{1},E_{1},R,c_{1})\), \(G_{2}=(V_{2},E_{2},R,c_{2})\) where \(R=\{r_{0},r_{1},r_{2}\}\). For each pair, we constructed as follows:

1. Generate an Erdos-Renyi graph \(G_{\text{init}}\) with \(5\) nodes and a random probability \(p\). We randomly select one of the nodes as the source node \(u\).
2. \(G_{1}\) is constructed by disjoint union \(G_{\text{init}}\) with two triangles, one with edges relation of \(r_{1}\), and the other with \(r_{2}\). The target query is \(r_{3}(u,v)\) for all \(v\) in a triangle with an edge relation of \(r_{1}\).
3. Similarly, \(G_{2}\) is constructed by disjoint union another copy of \(G_{\text{init}}\) with two squares, one with edges relation of \(r_{1}\), and the other with \(r_{2}\). The target query is \(r_{3}(u,v)\) for all \(v\) in a square with an edge relation of \(r_{2}\).

One example of such a pair can be shown in Figure 6. We generate \(100\) graph pairs and assign \(70\) pairs as the training set and the remaining \(30\) pairs as the testing set (\(140\) training graphs and \(60\) testing graphs). In total, there are \(490\) training triplets and \(210\) testing triplets.

**Objective.** For all node \(v\notin G_{\text{init}}\), we want a higher score for all the links \(r_{0}(u,v)\) if either \(v\) is in a triangle consists of \(r_{1}\) relation or \(v\) is in a square consists of \(r_{2}\) relation, and a lower score otherwise. For negative sampling, we choose counterpart triplets for each graph, that is, we take \(r_{0}(u,v)\) for all \(v\) in a triangle with an edge relation of \(r_{2}\) in \(G_{1}\) and in a square with an edge relation of \(r_{1}\) in \(G_{2}\).

**Model architectures.** We have considered two model architectures, namely C-MPNNs with \(\textsc{Init}=\textsc{Init}^{2},\textsc{ag}=\textsc{sum},\textsc{Msg}=\textsc {Msg}_{r}^{2}\), and \(f(t)=t\) without sum global readout:

\[\boldsymbol{h}_{v|u,q}^{(0)} =\mathbb{1}_{u=v}*\mathbf{z}_{q},\] \[\boldsymbol{h}_{v|u,q}^{(t+1)} =\sigma\Big{(}\mathbf{W}_{0}^{(t)}\big{(}\mathbf{h}_{v|u,q}^{(t) }+\sum_{r\in R}\sum_{w\in\mathcal{N}_{r}(v)}\textsc{Msg}_{r}^{2}(\mathbf{h}_ {w|u,q}^{(t)},\mathbf{z}_{q})\big{)}\Big{)},\]

\begin{table}
\begin{tabular}{l l c c c c} \hline \hline \multicolumn{1}{c}{**Hyperparameter**} & \multicolumn{2}{c}{**Hetionet**} & \multicolumn{2}{c}{**ogbl-biokg**} \\ \hline
**GNN Layer** & 
\begin{tabular}{c} Depth\((T)\) \\ Hidden Dimension \\ \end{tabular} & \(4\) & \(6\) \\ \hline
**Decoder Layer** & 
\begin{tabular}{c} Depth \\ Hidden Dimension \\ \end{tabular} & \(2\) & \(2\) \\ \hline
**Optimization** & \begin{tabular}{c} Optimizer \\ Learning Rate \\ \end{tabular} & \begin{tabular}{c} Adam \\ 2e-3 \\ \end{tabular} & \begin{tabular}{c} Adam \\ 2e-4 \\ \end{tabular} \\ \hline \multirow{3}{*}{**Learning**} & 
\begin{tabular}{c} Batch size \\ \#Negative Sample \\ Epoch \\ Adversarial Temperature \\ \end{tabular} & \(64\) & \(8\) \\ \hline \hline \end{tabular}
\end{table}
Table 13: Dataset statistics for transductive biomedical knowledge graph completion.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline \multicolumn{1}{c}{**Dataset**} & **\#Nodes** & **\#Node Type** & **\#Relation** & \multicolumn{2}{c}{**\#Triplet**} \\  & & & & **\#Train** & **\#Valid** & **\#Test** \\ \hline \begin{tabular}{c} Hetionet \\ ogbl-biokg \\ \end{tabular} & \(47{,}031\) & \(11\) & \(24\) & \(1{,}800{,}157\) & \(225{,}020\) & \(225{,}020\) \\ \hline 
\begin{tabular}{c} Hetionet \\ ogbl-biokg \\ \end{tabular} & \(93{,}773\) & \(5\) & \(51\) & \(4{,}762{,}677\) & \(162{,}886\) & \(162{,}870\) \\ \hline \hline \end{tabular}
\end{table}
Table 13: Dataset statistics for transductive biomedical knowledge graph completion.

and the same C-MPNN architecture with sum global readout:

\[\bm{h}_{v|u,q}^{(0)} =\mathbb{1}_{u=v}*\mathbf{z}_{q},\] \[\bm{h}_{v|u,q}^{(t+1)} =\sigma\Big{(}\mathbf{W}_{0}^{(t)}\big{(}\mathbf{h}_{v|u,q}^{(t)}+ \sum_{r\in R}\sum_{w\in\mathcal{N}_{r}(v)}\text{Msg}_{r}^{2}(\mathbf{h}_{w|u,q }^{(t)},\mathbf{z}_{q})\big{)}+\mathbf{W}_{1}^{(t)}\sum_{w\in V}\bm{h}_{w|u,q}^ {(t)}\Big{)},\]

**Design.** We claim that C-MPNNs with sum readout can correctly predict all testing triplets, whereas C-MPNNs without sum readout will fail to learn this pattern and achieve \(50\%\) as random guessing. Theoretically, the TRI-SQR dataset is designed in such a way that any R-MPNN model assigns identical node representations to nodes in triangles and squares with the same relation type. Consequently, any C-MPNN model without global readout will be unable to determine which graph between \(G_{1}\) and \(G_{2}\) in the graph pair is being predicted, making it challenging to learn the conditional representation \(\bm{h}_{u|v,q}\). However, we anticipate that a C-MPNN model with sum readout can differentiate between \(G_{1}\) and \(G_{2}\) in each pair, as it can access global information like the total number of nodes in the graph. This allows it to accurately identify the graph being predicted in the graph pair, even when the representations of the triangle and square with the same relation are identical. As a result, it can learn the target rules and achieve \(100\%\) accuracy.

**Experiment details.** We configure each model variant with four layers and 32 hidden dimensions per layer. We set a learning rate of 1e-4 for both models and train them for \(500\) epochs. Empirically, we find that C-MPNN with global sum readout achieves \(100\%\) accuracy, while C-MPNN without global sum readout reaches \(50\%\) accuracy as random guesses, which is consistent with our theoretical expectations.

Figure 6: Construction of graph pair in TRI-SQR for global readout