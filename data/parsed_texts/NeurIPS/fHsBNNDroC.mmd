# Calibrated Stackelberg Games: Learning Optimal Commitments Against Calibrated Agents

 Nika Haghtalab

University of California, Berkeley, {nika,kunheyang}@berkeley.edu

Chara Podimata

MIT & Archimedes, podimata@mit.edu

Kunhe Yang

University of California, Berkeley, {nika,kunheyang}@berkeley.edu

###### Abstract

In this paper, we introduce a generalization of the standard Stackelberg Games (SGs) framework: _Calibrated Stackelberg Games (CSGs)_. In CSGs, a principal repeatedly interacts with an agent who (contrary to standard SGs) does not have direct access to the principal's action but instead best-responds to _calibrated forecasts_ about it. CSG is a powerful modeling tool that goes beyond assuming that agents use ad hoc and highly specified algorithms for interacting in strategic settings and thus more robustly addresses real-life applications that SGs were originally intended to capture. Along with CSGs, we also introduce a stronger notion of calibration, termed _adaptive calibration_, that provides fine-grained any-time calibration guarantees against adversarial sequences. We give a general approach for obtaining adaptive calibration algorithms and specialize them for finite CSGs. In our main technical result, we show that in CSGs, the principal can achieve utility that converges to the optimum Stackelberg value of the game both in _finite_ and _continuous_ settings, and that no higher utility is achievable. Two prominent and immediate applications of our results are the settings of learning in Stackelberg Security Games and strategic classification, both against _calibrated_ agents.

## 1 Introduction

Stackelberg games (SGs) are a canonical model for strategic principal-agent interactions, considering a principal (or "leader") that commits to a strategy \(\mathbf{h}\) and an agent (or "follower") who observes this strategy and best respond by taking action \(\mathtt{BR}(\mathbf{h})\). These games are inspired by real-world applications such as economic policy design (where a tax policymaker establishes rules for triggering audits before taxes are filed), defense (where a principal allocates security resources to high-risk targets before vulnerabilities are exploited) and many more, see e.g., [7, 22, 49, 35, 21, 15, 17]. By anticipating the agent's best-response, a principal who knows the agent's payoff function can calculate the _optimal Stackelberg strategy_ guaranteeing her utility \(V^{*}\). In recent years, _repeated_ SGs have gained popularity in addressing settings where the agent's payoff function is _unknown_ to the principal. In this setting, the principal, who can only observe the agents' actions, aims to deploy a sequence of strategies \(\mathbf{h}_{1},\ldots,\mathbf{h}_{T}\) over \(T\) rounds whose average payoff is at least as good as \(V^{*}\), i.e., the value of her optimal strategy had she known the agent's payoffs in advance.

Despite the original intent, repeated SGs are often studied under strict assumptions on the agent's knowledge and algorithmic behavior. Examples include requiring the agent to best respond per round using \(y_{t}=\mathtt{BR}(\mathbf{h}_{t})\)[7, 21], necessitating the agent to precisely know the principal's strategy at all times (e.g., the attacker must anticipate the exact probabilistic allocation of the defender's security resources), or employing one of many online optimization algorithms whose every detail (down to the learning step size) can significantly impact the principal's utility [51].

In this paper, instead of working with such restrictive and often unrealistic assumptions on the agent's knowledge and behavior, we build on foundational decision theoretic concepts, such as _forecasts_ and _calibration_[19, 29, 30]. In practice, while agents may not observe the principal's true strategies \(\mathbf{h}_{t}\), they can form _calibrated forecasts_ -- a notion of consistency in beliefs about \(\mathbf{h}_{t}\) -- to which they then best respond. Indeed, such a decision-theoretic perspective on game dynamics led to seminal results on converging to correlated and Nash equilibria in simultaneous multi-player games [29, 38]. Our work brings the perspective of calibrated forecasts to principal-agent games. We introduce _Calibrated Stackelberg Games (CSG)_--a class that is more general than standard SGs-- and ask:

**Q1**.: _What characterizes principal's optimal utility in CSGs?_

**Q2**.: _Are there natural forecasting algorithms for the agent that satisfy calibration?_

**Our Contributions.** We answer both questions completely. For **Q1**, we show that the principal's optimal utility converges _exactly_ to \(V^{\star}\). For **Q2**, we give a general approach for obtaining a fine-grained any-time notion of calibration _of independent interest_ and further specializing it to games.

Before we delve into the details of our contributions, we highlight two key aspects of our results. First, _calibration_ is a common property of forecasting procedures shared by many algorithms, not any one particular algorithm defining the agent's behavior. Despite not constraining the agent to any particular algorithm, our answer to **Q1** shows that the principal can meaningfully converge to \(V^{\star}\), which is the value she could have achieved in a single-shot game had she known the agent's utility. Second, our definition and results immediately apply to two important Stackelberg settings; Stackelberg Security Games [49, 7, 33] and strategic classification [21]. As such, we obtain the first results for learning against calibrated agents in these settings too.

Our work contributes two concepts of independent interest (Section 2): first, _CSGs_ that directly generalize the standard model of repeated SGs, and second, a notion of calibration, termed _adaptive calibration_. This notion which draws inspiration from _adaptive regret bounds_ in online learning, provides fine-grained calibration guarantees for adversarial sequences.

Beyond the introduction of these models, we address an important property of CSGs in our answer to **Q1**. We show that the principal's optimal utility in CSGs converges to \(V^{\star}\), nothing more or less, in games with _finite_ (Section 3) or _continuous_ (Section 5) actions spaces. Note that \(V^{\star}\) is a benchmark that gives both players more power: the principal knows the agent's utility and the agent observes the principal's strategy. We find it somewhat surprising then that the optimal achievable principal utility in CSGs, in which both players work with significantly less knowledge, converges to \(V^{\star}\) exactly.

As for our newly introduced notion of adaptive calibration (Section 4), we provide an answer to **Q2** by giving a general approach for creating adaptively calibrated forecasting algorithms. This shows that adaptive calibration is not just an abstract notion, rather, it is a natural property with deep roots in the theory of online learning. Indeed, to obtain these results we draw inspirations from recent advances in the multicalibration literature [34] regarding simulating no-regret and best-response dynamics and the sleeping experts problem setting which has been a staple of the online learning literature [8, 31]. Furthermore, we specialize our approach for strategic settings by showing how standard calibration concepts (such as the "binning function") can be adapted to account for the agent's best-responses.

### Related work

**Repeated Stackelberg games.** Learning optimal Stackelberg strategies has been studied in the offline [17] and the online setting, where only instantaneous best-responses are observable (i.e., no access to a best-response oracle). Key applications include Stackelberg Security Games (e.g., [12, 7, 47, 50]) and strategic classification (e.g., [21, 15, 3, 4]). Another line of work treats repeated games as an extensive form game and studies optimal strategies for infinite [52] or finite [16] horizons. Other works consider learning in the presence of non-myopic agents that best respond by maximizing discounted utilities [5, 33, 2]. The main distinction to our work is that in our setting, the agents have only calibrated forecasts regarding the principal's strategies (rather than full knowledge of them).

**Stackelberg games beyond best responses.** Recent works have studied variants of repeated Stackelberg games with different agent strategic behaviors beyond best responding. One example is no-regret learning, in which previous works have extensively investigated the relationship between the principal's cumulative utility and the single-shot Stackelberg value against agents that use mean-based learning algorithms [13], gradient descent [24, 25], no-external regret algorithms [13, 20, 51],no-internal/swap regret algorithms [20; 45], and no-counterfactual internal regret algorithms [14]. Another research direction assumes agents approximately best respond due to uncertainty in the principal's strategy [11; 6; 46] or their own [43; 39; 40] and study _robust Stackelberg equilibria_[48; 32]. Most of the works here assume that the principal knows the agent's utility function with the exception of [51]. Core differences to our framework are that (1) we work in an online learning setting where the principal has to learn the agent's utility function from their responses; (2) we do not assume a specific agent algorithm but focus on properties of agent beliefs that are shared by many algorithms.

**Calibration and application in games.** The study of calibration, introduced by Dawid [19], dates back to seminal work by Foster and Vohra [30] and Hart [36] that showed the existence of asymptotic online calibration against any adversarial sequence. Applying calibration to game dynamics, Foster and Vohra [29] introduced the concept of _calibrated learning_, which refers to a player best responding to calibrated forecasts of others' actions. They demonstrated that the game dynamics of all players performing calibrated learning converge to the set of correlated equilibria. This is complemented by the results of [38; 27; 28] that show _smooth and continuous_ variants of calibrated learning dynamics converge to Nash equilibrium. Our work differs from the above by studying game dynamics that converge to a Stackelberg equilibrium, where only the follower (agent) performs calibrated learning.

**Adaptivity and sleeping experts.** The notion of adaptive calibration introduced in Section 2 is related to the study of adaptivity of regret bounds in online learning [44; 18; 37]. Our design of adaptively calibrated forecasting algorithms builds on the _multi-objective learning_ perspective of online (multi-)calibration [41; 34] and the powerful tool of _sleeping experts_[8; 31; 44] which has proven useful in various applications such as fairness [9].

## 2 Model & preliminaries

We begin this section with some basic definitions about forecasts, calibration, and games, and then introduce the class of games that we study; _Calibrated Stackelberg Games_ (CSGs).

**Adaptively Calibrated Forecasts.** We use \(A\) to denote the space of outcomes and \(C\supseteq A\) to denote the space of forecasts. A (stochastic) forecasting procedure \(\sigma\) is an online procedure that takes any adversarial sequence of outcomes \(\mathbf{h}_{t}\in A\) for \(t\in[T]\), and on round \(t\) outputs (possibly at random) forecast \(\mathbf{p}_{t}\in\hat{C}\) solely based on outcomes and forecasts \(\mathbf{h}_{\tau},\mathbf{p}_{\tau}\), for \(\tau\in[t-1]\). To define calibrated forecasts, let us first introduce the notion of _binning functions_.

**Definition 2.1** (Binning [28]).: _We call a set \(\Pi=\{w_{i}\}_{i\in[n]}\) a binning function, if each \(w_{i}:C\rightarrow[0,1]\) maps forecasts to real values in \([0,1]\), and for all \(\mathbf{p}\in C\) we have \(\sum_{i\in[n]}w_{i}(\mathbf{p})=1\)._

With the above binning functions, we define the adaptive calibration error with respect to \(\Pi\) as follows. At a high level, conditioned on any bin, the calibration error measures the difference between the expected forecasts that fall in that bin and the corresponding expected outcome.

**Definition 2.2** (\(\Pi\)-Adaptive Calibration Error).: _For any time interval \([s,t]\), let \(\mathbf{p}_{s:t}\) be the sequence of forecasts and \(\mathbf{h}_{s:t}\) be the sequence of outcomes. For a given binning \(\Pi=\{w_{i}\}_{i\in[n]}\) with size \(n\), and \(\forall i\in[n]\), define the \(\Pi\)-adaptive calibration error as_

\[\mathrm{CalErr}_{i}\left(\mathbf{h}_{s:t},\mathbf{p}_{s:t}\right)\triangleq \frac{n_{[s,t]}(i)}{t-s}\cdot\left\|\bar{\mathbf{p}}_{[s,t]}(i)-\tilde{\mathbf{ h}}_{[s,t]}(i)\right\|_{\infty},\] (1)

_where during interval \([s,t]\), \(n_{[s,t]}(i)\triangleq\sum_{\tau=s}^{t}w_{i}(\mathbf{p}_{\tau})\) is the effective number of times that the forecast belongs to bin \(i\) (i.e., bin \(i\) is activated), \(\bar{\mathbf{p}}_{[s,t]}(i)\triangleq\sum_{\tau=s}^{t}\frac{w_{i}(\mathbf{p} _{\tau})}{n_{[s,t]}(i)}\cdot\mathbf{p}_{\tau}\) is the expected forecast that activates bin \(i\), \(\tilde{\mathbf{h}}_{[s,t]}(i)\triangleq\sum_{\tau=s}^{t}\frac{w_{i}(\mathbf{p} _{\tau})}{n_{[s,t]}(i)}\cdot\mathbf{h}_{\tau}\) is the expected outcomes corresponding to bin \(i\)._

We say that a forecasting procedure is adaptively calibrated if it achieves vanishing calibration error on any adversarial sequence of outcomes and any sub-interval of time steps.

**Definition 2.3** (\((\varepsilon,\Pi)\)-Adaptively Calibrated Forecasts).: _A forecasting procedure \(\sigma\) is \(\varepsilon\)-adaptively calibrated to binning \(\Pi=\{w_{i}\}_{i\in[n]}\) with rate \(r_{\delta}(\cdot)\in o(1)\), if for all adversarial sequences of actions \(\mathbf{h}_{1},\cdots,\mathbf{h}_{T}\), where \(\mathbf{h}_{t}\in A\), \(\sigma\) outputs forecasts \(\mathbf{p}_{t}\in C\) for \(t\in[T]\) such that with probability at least \(1-\delta,\) we have that \(\forall s,t\) such that \(1\leq s<t\leq T\), and \(\forall i\in[n]\):_

\[\mathrm{CalErr}_{i}\left(\mathbf{h}_{s:t},\mathbf{p}_{s:t}\right)\leq r_{ \delta}(t-s)+\varepsilon.\]We remark that without adaptivity (i.e., for \(s=1\) and \(t=T\)), Definition 2.2 is weaker than the standard definition of calibration (e.g., [30], listed for completeness in Appendix A) in two ways: (1) standard calibration takes each prediction \(\mathbf{p}\in C\) as an independent bin, thus having infinitely many binning functions: \(w_{\mathbf{p}}(\cdot)=\delta_{\mathbf{p}}(\cdot)\). Instead, we only require calibration with respect to the predefined binning \(\Pi\) which only contains a finite number of binning functions; (2) standard calibration cares about the summation over calibration error across bins, but we only consider the maximum error.

**Stackelberg Games.** A _Stackelberg game_ is defined as the tuple \((\mathcal{A}_{P},\mathcal{A}_{A},U_{P},U_{A})\), where \(\mathcal{A}_{P}\) and \(\mathcal{A}_{A}\) are the principal and the agent action spaces respectively, and \(U_{P}:\mathcal{A}_{P}\times\mathcal{A}_{A}\to\mathbb{R}_{+}\) and \(U_{A}:\mathcal{A}_{P}\times\mathcal{A}_{A}\to\mathbb{R}_{+}\) are the principal and the agent utility functions respectively. For ease of exposition, we work with _finite_ Stackelberg games (i.e., \(|\mathcal{A}_{P}|=m\) and \(|\mathcal{A}_{A}|=k\)) and generalize our results to continuous games in Section 5. When the principal plays action \(x\in\mathcal{A}_{P}\) and the agent plays action \(y\in\mathcal{A}_{A}\), then the principal and the agent receive utilities \(U_{P}(x,y)\) and \(U_{A}(x,y)\) respectively. We also define the principal's _strategy space_ as the simplex over actions: \(\mathcal{H}_{P}=\Delta(\mathcal{A}_{P})\). For a strategy \(\mathbf{h}\in\mathcal{H}_{P}\), we oftentimes abuse notation slightly and write \(U_{P}(\mathbf{h},y):=\mathbb{E}_{x\sim\mathbf{h}}[U_{P}(x,y)]\).

_Repeated Stackelberg games_ capture the _repeated_ interaction between a principal and an agent over \(T\) rounds. What distinguishes Stackelberg games from other types of games is the inter-temporal relationship between the principal's action/strategy and the agent's response; specifically, the principal first commits to a strategy \(\mathbf{h}_{t}\in\mathcal{H}_{P}\) and the agent subsequently _best-responds_ to it with \(y_{t}\in\mathcal{A}_{A}\). Let \(\mathbf{p}_{t}\in\mathcal{F}_{P}=\mathcal{H}_{P}\) be the agent's _belief_ regarding the principal's strategy at round \(t\). In standard Stackelberg games: \(\mathbf{p}_{t}=\mathbf{h}_{t}\), i.e., the agent has full knowledge of the principal's strategy. In this paper, we consider games where the agent does not in general know \(\mathbf{h}_{t}\) when playing, but they only best-respond according to their belief \(\mathbf{p}_{t}\). The agent's _best-response_ to _belief_\(\mathbf{p}_{t}\) according to her underlying utility function \(U_{A}\) is action \(y_{t}\in\mathcal{A}_{A}\) such that

\[y_{t}\in\mathtt{BR}(\mathbf{p}_{t})\quad\text{where}\quad\mathtt{BR}( \mathbf{p}_{t})=\operatorname*{argmax}_{y\in\mathcal{A}_{A}}\;\operatorname*{ \mathbb{E}}_{x\sim\mathbf{p}_{t}}[U_{A}(x,y)].\] (2)

We often overload notation and write \(U_{A}(\mathbf{p},y):=\mathbb{E}_{x\sim\mathbf{p}}[U_{A}(x,y)]\). Note that from Equation (2), the best-responses to \(\mathbf{p}_{t}\) form _a set_. If this set is not a singleton, we use either a _deterministic_ or a _randomized tie-breaking_ rule. For the _deterministic_ tie-breaking rule, the agent breaks ties according to a predefined preference rule \(\succ\) over the set of actions \(\mathcal{A}_{A}\). For the _randomized_ tie-breaking rule, the agent chooses \(y_{t}\) by sampling from the set \(\mathtt{BR}(\mathbf{p}_{t})\) uniformly at random, i.e., \(y_{t}\sim\mathsf{Unif}(\mathtt{BR}(\mathbf{p}_{t}))\).

The _Stackelberg value_ of the game is the principal's optimal utility when the agent best responds:

\[V^{\star}=\max_{\mathbf{h}^{\star}\in\mathcal{H}_{P}}\max_{y^{\star}\in \mathtt{BR}(\mathbf{h}^{\star})}U_{P}(\mathbf{h}^{\star},y^{\star}).\]

In the above definition \(\mathbf{h}^{\star}\) is referred to as the _principal's optimal strategy_.

For an agent's action \(y\in\mathcal{A}_{A}\), we define the corresponding _best-response polytope_\(P_{y}\) as the set of all of the agent's beliefs that induce \(y\) as the agent's best-response, i.e., \(P_{y}=\{\mathbf{p}\in\mathcal{F}_{P}:y\in\mathtt{BR}(\mathbf{p})\}\). We make the following standard assumption, which intuitively means that there are sufficiently many strategies that induce \(y^{\star}\) as the agent's best-response.

**Assumption 2.4** (Regularity).: _The principal's optimal strategy \(\mathbf{h}^{\star}\in\Delta(\mathcal{A}_{P})\) and the agent's optimal action \(y^{\star}\in\mathtt{BR}(\mathbf{h}^{\star})\) satisfy a regularity condition: \(P_{y^{\star}}\) contains an \(\ell_{2}\) ball of radius \(\eta>0\)._

**Calibrated Stackelberg Games.** In CSGs (see Figure 1 for the principal-agent interaction protocol1), the agent forms \((\varepsilon,\Pi)\)-adaptively calibrated forecasts as their beliefs \(\mathbf{p}_{t}\) regarding \(\mathbf{h}_{t}\).

Footnote 1: If the agent observes action \(x_{t}\sim\mathbf{h}_{t}\) instead of the mixed strategy \(\mathbf{h}_{t}\), then they can still calibrate to the sequence of \(\mathbf{h}_{t}\) with an additional (vanishing) error term that comes from concentration inequalities.

**Calibrated Stackelberg Games.** In CSGs (see Figure 1 for the principal-agent interaction protocol2), the agent forms \((\varepsilon,\Pi)\)-adaptively calibrated forecasts as their beliefs \(\mathbf{p}_{t}\) regarding \(\mathbf{h}_{t}\).

Footnote 2: The agent observes action \(x_{t}\sim\mathbf{h}_{t}\) instead of the mixed strategy \(\mathbf{h}_{t}\), then they can still calibrate to the sequence of \(\mathbf{h}_{t}\) with an additional (vanishing) error term that comes from concentration inequalities.

**Calibrated Stackelberg Games.** In CSGs (see Figure 1 for the principal-agent interaction protocol3), the agent forms \((\varepsilon,\Pi)\)-adaptively calibrated forecasts as their beliefs \(\mathbf{p}_{t}\) regarding \(\mathbf{h}_{t}\).

Footnote 3: The agent observes \(\mathbf{h}_{t}\), or an action sampled from \(\mathbf{h}_{t}\).

[MISSING_PAGE_POST]

We first define binning functions that are especially appropriate for forecasts in games. In CSGs, we define \(\Pi\) based on whether \(i\) is a best-response to the input calibrated forecast, i.e., \(\forall\mathbf{p}\in\mathcal{F}_{P}\):

\[w_{i}(\mathbf{p}) =\mathbf{1}\{i\in\mathtt{BR}(\mathbf{p}),i\succ j,\forall j\neq i\} \text{(for the {\em deterministic} tie-breaking)}\] \[w_{i}(\mathbf{p}) =\frac{\mathbf{1}\{i\in\mathtt{BR}(\mathbf{p})\}}{|\mathtt{BR}( \mathbf{p})|} \text{(for the {\em randomized} tie-breaking)}\]

Note that both binning functions meet the conditions of Definition 2.1. Applying Definition 2.3 for calibrated agent forecasts in CSGs we have the following:

**Definition 2.5** (\(\varepsilon\)-Adaptively Calibrated Agent for CSGs).: _The agent is called \(\varepsilon\)-adaptively calibrated with rate \(r_{\delta}(\cdot)\in o(1)\), if for any sequence of principal strategies \(\mathbf{h}_{1},\cdots,\mathbf{h}_{T}\in\mathcal{H}_{P}\) the agent takes a sequence of actions \(y_{1},\ldots,y_{T}\) that satisfy the following requirements: 1) there is a sequence of forecasts \(\mathbf{p}_{t}\in\mathcal{F}_{P}\) for \(t\in[T]\), such that \(y_{t}\in\mathtt{BR}(\mathbf{p}_{t})\), and 2) forecasts \(\mathbf{p}_{1},\ldots,\mathbf{p}_{T}\) are \(\varepsilon\)-calibrated for binning \(\Pi\) with rate \(r_{\delta}(\cdot)\) with respect to the principal's strategies \(\mathbf{h}_{1},\cdots,\mathbf{h}_{T}\)._

We next review the fundamental constructs from Equation (1) and their intuitive meaning in this setting. \(n_{[s,t]}(i)\triangleq\sum_{\tau\in[s,t]}w_{i}(\mathbf{p}_{\tau})\) is now the expected number of times that the forecast has induced action \(i\) from the agent as their best response during interval \([s,t]\), \(\bar{\mathbf{p}}_{[s,t]}(i)\triangleq\sum_{\tau\in[s,t]}w_{i}(\mathbf{p}_{ \tau})\cdot\mathbf{p}_{\tau}/n_{[s,t]}(i)\) is the expected forecast that induces action \(i\) from the agent as their best response during interval \([s,t]\), and \(\bar{\mathbf{h}}_{[s,t]}(i)\triangleq\sum_{\tau\in[s,t]}w_{i}(\mathbf{p}_{ \tau})\cdot\mathbf{h}_{\tau}/n_{[s,t]}(i)\) is the expected principal strategy that induces action \(i\) from the agent as their best response during interval \([s,t]\). The requirement for an agent to be calibrated is quite mild, as the forecasts are binned only according to the best-response they induce.

## 3 Principal's learning algorithms

In this section (see Appendix C for full proofs and convergence rates), we study the relationship between the principal's Stackelberg value \(V^{\star}\) and the best utility the principal can obtain from learning to play a sequence of strategies \(\{\mathbf{h}_{t}\}_{t\in[T]}\) against calibrated agents, i.e., \(\frac{1}{T}\sum_{t\in[T]}U_{P}(\mathbf{h}_{t},y_{t})\). The relationship between \(V^{\star}\) and \(\frac{1}{T}\sum_{t\in[T]}U_{P}(\mathbf{h}_{t},y_{t})\) is not a priori clear. In the case of calibrated forecasts, the agents do not know the exact \(\mathbf{h}_{t}\) when they choose their response. Instead, they base their decisions on the history of the principal's strategies so far. A principal then may be able to create historical patterns that lead the agents to worse actions, thus obtaining better utility himself. Indeed, several works have shown how historical patterns can afford the principal much better utility than \(V^{\star}\) when the agents are no-regret [13; 20]. Surprisingly, we show that this is not the case when the agents are calibrated; \(\sum_{t\in[T]}U_{P}(\mathbf{h}_{t},y_{t})\) is upper bounded by \(TV^{\star}\) and a term that is sublinear in \(T\) and depends on the calibration parameters.2

Footnote 2: A similar upper-bound on the principal’s utility was proved for no-swap-regret agents [20]. While we prove the theorem directly for calibration, an alternative proof in App. B shows that calibration implies no-swap regret.

**Theorem 3.1**.: _Assume that the agent is \((\varepsilon,\Pi)\)-as calibrated with rate \(r_{\delta}(\cdot)\) and negligible \(\varepsilon\). Then, for any sequence \(\{\mathbf{h}_{t}\}_{t\in[T]}\) for the principal's strategies in a CSG, with probability at least \(1-2\delta\), the principal's utility is upper bounded as: \(\lim_{T\to\infty}\frac{1}{T}\sum_{t\in[T]}U_{P}(\mathbf{h}_{t},y_{t})\leq V^{\star}\)._

Proof sketch.: We sketch the proof for the deterministic tie-breaking, as the randomized one needs just an application of Azuma-Hoeffding. We first rewrite \(\sum_{t\in[T]}U_{P}(\mathbf{h}_{t},y_{t})\) partitioned in the principal's utility for each round-specific forecast that induces action \(i\) as the best-response from the agent, for all actions \(i\in\mathcal{A}_{A}\): \(\sum_{t\in[T]}U_{P}(\mathbf{h}_{t},y_{t})=\sum_{i\in\mathcal{A}_{A}}\sum_{t \in[T]}w_{i}(\mathbf{p}_{t})\cdot U_{P}(\mathbf{h}_{t},i)\). This equivalence holds because each \(\mathbf{p}_{t}\) maps to a _single_ best response \(i\in\mathcal{A}_{A}\) (deterministic tie-breaking). Because of the linearity of the principal's reward in the principal's strategy: \(\sum_{i\in\mathcal{A}_{A}}\sum_{t\in[T]}w_{i}(\mathbf{p}_{t})\cdot U_{P}( \mathbf{h}_{t},i)=\sum_{i\in\mathcal{A}_{A}}n_{T}(i)\cdot U_{P}(\bar{\mathbf{h }}_{T},i)\), where \(n_{T}(i)=n_{[0,T]}(i)\). Adding and subtracting \(U_{P}(\bar{\mathbf{p}}_{T},i)\) from the above, we now need to bound the quantity: \(\sum_{i\in\mathcal{A}_{A}}n_{T}(i)(U_{P}(\bar{\mathbf{p}}_{T}(i),i)+\langle U_{P }(\cdot,i),\bar{\mathbf{h}}_{T}(i)-\bar{\mathbf{p}}_{T}(i)\rangle)\). The first term is upper bounded by \(V^{\star}T\); note that \(i\in\mathtt{BR}(\bar{\mathbf{p}}_{T})\) (and \(V^{\star}=\max_{\mathbf{h}}\max_{y\in\mathtt{BR}(\mathbf{h})}U_{P}(\mathbf{h},y)\)) since \(\mathbf{p}_{t}(i)\in P_{i}\) and hence that should also be true for the average of \(\mathbf{p}_{t}(i)\) over \(t\) rounds. The second summand is bounded by the calibration error of Definition 2.3.

On the other hand, it may seem that because the agent's behavior is less specified when she uses calibrated forecasts (as opposed to full knowledge), the principal may only be able to extract much less utility compared to \(V^{\star}\). Again, we show that this is not the case and that there exist algorithms for the principal such that the sequence of strategies \(\{\mathbf{h}_{t}\}_{t\in[T]}\) is asymptotically approaching \(V^{\star}\).

**Theorem 3.2**.: _There exists an algorithm for the principal in CSGs that achieves average utility: \(\lim_{T\to\infty}\frac{1}{T}\sum_{t\in[T]}U_{P}(\mathbf{h}_{t},y_{t})\geq V^{\star}\)._

Algorithm 1 is an explore-then-commit algorithm; it first estimates an _appropriate_ strategy for the principal \(\tilde{\mathbf{h}}\) (Explore), and then repeatedly plays it until the end (Commit). In the remainder of the section, we sketch the proof for Theorem 3.2 and point to exact lemma statements in the Appendix. Let \(T_{1},T_{2}\) denote the set of rounds that belong in the Explore and Commit phase respectively.

To elaborate on the objectives of the Explore phase, let us first consider a setting with zero calibration error, where the agent's forecasting algorithm is perfectly and adaptively calibrated, leading to \(y_{t}=\mathtt{BR}(\mathbf{h}_{t})\) at every round. The task for the Explore phase simplifies to identifying a near-optimal strategy \(\tilde{\mathbf{h}}\) through best response oracles that satisfies \(U_{P}(\tilde{\mathbf{h}},\mathtt{BR}(\tilde{\mathbf{h}}))\geq V^{\star}- \varepsilon_{1}\) for a predetermined \(\varepsilon_{1}\). We formalize this property in **(P1)**. Given that the agent is perfectly calibrated, in the Commit phase, the agent always plays \(\tilde{y}=\mathtt{BR}(\tilde{\mathbf{h}})\), leading to an upper bound of \(\varepsilon_{1}|T_{2}|\) on the Stackelberg regret. Hence, Algorithm 1's regret is bounded by \(V^{\star}|T_{1}|+\varepsilon_{1}|T_{2}|\).

**(P1)**: \(U_{P}(\tilde{\mathbf{h}},\tilde{y})\geq V^{\star}-\varepsilon_{1}\) for \(\tilde{y}\in\mathtt{BR}(\tilde{\mathbf{h}})\), i.e., \((\tilde{\mathbf{h}},\tilde{y})\) is an approximate Stackelberg equilibrium.

Moving away from the idealized setting, we must account for possible discrepancies between \(y_{t}\) and \(\mathtt{BR}(\mathbf{h}_{t})\) due to calibration error. This introduces: (i) An increased sample complexity \(|T_{1}|\) in the Explore phase, given the necessity to learn a near-optimal strategy from noisy responses; (ii) Potential deviations from the action \(\tilde{y}=\mathtt{BR}(\tilde{\mathbf{h}})\) in the Commit phase due to miscalibrations in belief. To address the first challenge, we employ Algorithm 2, which constructs an approximate best response oracle by repeatedly interacting with a calibrated agent. For the second challenge, we require our learned policy \(\tilde{\mathbf{h}}\) to be robust against inaccurate forecasts. This is reflected in condition **(P2)**, which necessitates the ball of radius \(\varepsilon_{2}\) around \(\tilde{\mathbf{h}}\) to be fully contained in the polytope \(P_{\tilde{y}}\). The critical insight from **(P2)** is: for any forecast \(\mathbf{p}_{t}\) that results in a best response \(y_{t}=\mathtt{BR}(\mathbf{p}_{t})\neq\tilde{y}\), there must be a minimum distance of \(\varepsilon_{2}\) separating \(\mathbf{p}_{t}\) from \(\tilde{\mathbf{h}}\). We will now proceed to formalize **(P2)**, but before delving into that, it is important to introduce some additional notations.

Let \(B_{2}(x,\varepsilon)\) denote the ball of radius \(\varepsilon\) around \(x\), i.e., \(B_{2}(x,\varepsilon)\triangleq\{x^{\prime}:\|x-x^{\prime}\|_{2}\leq\varepsilon\}\). For a convex set \(S\in\mathbb{R}^{n}\), we use \(B_{2}(S,\varepsilon)\) to denote the _union_ of all balls of radius \(\varepsilon\) around the set, i.e., \(B_{2}(S,\varepsilon)\triangleq\bigcup_{x\in S}B_{2}(x,\varepsilon)\). For a convex set \(S\in\mathbb{R}^{n}\), we use \(B_{2}(S,-\varepsilon)\) to denote the set of all points in \(S\) that are "safely" inside \(S\) (i.e., all the points in a ball of radius \(\varepsilon\) around them still belong in \(S\)): \(B_{2}(S,-\varepsilon)\triangleq\{x\in S:\;B_{2}(x,\varepsilon)\subseteq S\}\). We call this last set, the \(\varepsilon\)-_conservative_ of \(S\). See Figure 2 for a pictorial illustration of the notations.

Figure 2: Pictorial representation for notation \(B_{2}(S,\varepsilon)\) (left) and \(B_{2}(S,-\varepsilon)\) (right).

[MISSING_PAGE_FAIL:7]

number of rounds to ensure at least one sample in the \(\nicefrac{{n}}{{2}}\)-ball around the center of \(P_{y}\)' with high probability. To build the approximate membership oracle for the \(\varepsilon_{2}\)-conservative best-response polytope for an action \(y\), we use ApproxMem (Algorithm 4 in Appendix C.2). Specifically, on input \(\mathbf{h}\in\mathcal{H}_{P}\), ApproxMem either asserts \(\mathbf{h}\in B_{2}(P_{y},-\varepsilon_{2}+\varepsilon_{1})\) or \(\mathbf{h}\notin B_{2}(P_{y},-\varepsilon_{2}-\varepsilon_{1})\) with probability at least \(1-\varepsilon_{3}\). To do this, it samples \(\Phi\) points in proximity to \(\mathbf{h}\) and plays each one repeatedly for \(l\) rounds, while registering the best-response action observed for each one of these. If the most frequent best-response for all \(\mathbf{h}_{\phi}\) is \(y\), then we can conclude with good probability that \(\mathbf{h}\) was inside \(B_{2}(P_{y},-\varepsilon_{2})\). See Lemma C.2 for more details.

## 4 Forecasting Algorithm for Adaptive Calibration

In this section, we examine whether there exist natural forecasting procedures that satisfy our Definition 2.3 about adaptively calibrated forecasts. We answer this question positively.

**Theorem 4.1**.: _For all \(\varepsilon>0\) and all binnings \(\Pi=\{w_{i}:\mathbb{R}^{m}\rightarrow[0,1],i\in[k]\}\), there exists a parameter-free forecasting procedure that is \((\varepsilon,\Pi)\)-adaptively calibrated with rate \(r_{\delta}(t)=O(\sqrt{\log(kmt)/t})\). Moreover, when \(\Pi\) is a continuous binning (i.e., each \(w_{i}\) is continuous), there exists a forecasting procedure that is \((0,\Pi)\)-adaptively calibrated with the same rate._

To prove the theorem, we use two main tools; the first one is a well-known algorithm of Luo and Schapire [44] (AdaNormalHedge) applied for online learning in the _sleeping experts_ problem (see Appendix D for details). Roughly speaking, the _sleeping experts_ is a standard online learning problem with \(T\) rounds and \(N\) experts, where at each round \(t\) there is only a subset of the experts being "awake" to be considered by the learner and report their predictions. Let \(I_{t,i}\) be the binary variable indicating whether expert \(i\) was awake at round \(t\) (\(I_{t,i}=1\)) or asleep (\(I_{t,i}=0\)). The interaction protocol between the learner and the adversary at each round \(t\) is: (i) The learner observes which experts are awake, i.e., \(\{I_{t,i}\}_{i\in[N]}\). (ii) The learner selects a probability distribution \(\pi_{t}\in\Delta([N])\) supported on the set of _active_ experts \(A_{t}\triangleq\{i:I_{t,i}=1\}\). (iii) The adversary selects a loss vector \(\{\ell_{t,i}\}_{i\in[N]}\). (iv) The learner incurs expected loss \(\hat{\ell}_{t}=\mathbb{E}_{i\sim\pi_{t}}[\ell_{t,i}]\). AdaNormalHedge is a _parameter-free_ online learning algorithm that when applied on the sleeping experts problem (and with appropriate initialization) obtains regret \(\mathrm{Reg}_{t}(i)=O(\sqrt{T_{i}\log(NT_{i})})\), where \(T_{i}=\sum_{\tau\in[t]}I_{\tau,i}\).

The second tool that we use is _No-Regret vs. Best-Response dynamics (NRBR)_[34]. NRBR are a form of no-regret dynamics between two players, where one of the players must also best-respond on average. Essentially, at each round \(t\in[T]\), the forecasting algorithm with the calibration rate of Theorem 4.1 outputs a randomized forecast \(\mathbf{p}_{t}\in\mathcal{F}_{P}\), by simulating an interaction between two players described below. For the first player, we construct a _sleeping experts_ problem instance, where the set of experts is \(\mathcal{G}=\{g_{(s,i,j,\sigma)}:s\in[T],i\in\mathcal{A}_{A},j\in\mathcal{A}_{P },\sigma\in\{\pm 1\}\}\). For each \(g_{(s,i,j,(\sigma)}\in\mathcal{G}\)and \(t\in[T]\), we define the loss, sleeping/awake indicator, and instantaneous regret respectively as:

\[\ell_{t,g_{(s,i,j,\sigma)}}\triangleq L_{g_{(s,i,j,\sigma)}}(\mathbf{h}_{t},\mathbf{p}_{t})=w_{i}( \mathbf{p}_{t})\cdot\sigma\cdot\left(h_{t,j}-p_{t,j}\right);\] (6) \[I_{t,g_{(s,i,j,\sigma)}}\triangleq \mathbbm{1}\{t\geq s\};\] (7) \[r_{t,g}\triangleq L_{t,g}\cdot\left(\ell_{t,g}-\hat{\ell}_{t}\right).\]

where by \(h_{t,j},p_{t,g}\) we denote the \(j\)-th coordinate of \(\mathbf{h}_{t}\) and \(\mathbf{p}_{t}\) respectively. We defined the losses for our newly constructed sleeping experts' instance as above to make sure that there is a direct correspondence with the calibration error. Similar ideas for calibration (albeit not for the notion of adaptivity we consider) have been used in [41, 34]. We describe next the player interaction in NRBR.

**Player 1.** Runs AdaNormalHedge on expert set \(\mathcal{G}\) with a pre-specified prior \(\pi_{0}\) over \(\mathcal{G}\) and feedback specified in Equations (6), (7). At each round \(t\), Player 1 computes distribution \(\pi_{t}\in\Delta(A_{t}(\mathcal{G}))\), where \(A_{t}(\mathcal{G})\) denotes the set of active experts \(g_{(s,i,j,\sigma)}\in\mathcal{G}\) with \(I_{t,g_{(s,i,j,\sigma)}}=1\).

**Player 2.** Best responds to \(\pi_{t}\) by selecting \(Q_{t}\in\Delta(\mathcal{F}_{P})\) that satisfies:

\[\max_{\mathbf{h}_{t}\in\mathcal{H}_{P}}\mathop{\mathbb{E}}_{ \begin{subarray}{c}\rho\sim t_{t}\\ \mathbf{p}_{t}\sim Q_{t}\end{subarray}}\left[\ell_{t,g}\right]=\max_{\mathbf{h }_{t}\in\mathcal{H}_{P}}\mathop{\mathbb{E}}_{\begin{subarray}{c}\rho\sim t_{ t}\\ \mathbf{p}_{t}\sim Q_{t}\end{subarray}}\left[L_{g}(\mathbf{h}_{t},\mathbf{p}_{t}) \right]\leq\varepsilon.\] (8)

After simulating the game above, the algorithm outputs forecast \(\mathbf{p}_{t}\sim Q_{t}\). The existence of such a distribution \(Q_{t}\) is justified by the min-max theorem ([34, Fact 4.1] or [28, Theorem 5]). In the Appendix, we also give an explicit formula for \(Q_{t}\) in the special case of \(m=2\). When \(\Pi_{0}\) is continuous, player 2 can select a deterministic \(\mathbf{p}_{t}\) that achieves Equation (8) with \(\varepsilon=0\). This stronger property is justified by the outgoing fixed-point theorem [28, Theorem 4]. Note that this algorithm inherits its parameter-free property directly from AdaNormalHedge. We are now ready to provide a proof sketch for Theorem 4.1.

Proof sketch of Theorem 4.1.: Fix an instance of the NRBR game outlined above. We begin by Definition 2.2 of calibration error translated in the sleeping experts instance that we defined above:

\[\mathrm{CalErr}_{i}(\mathbf{h}_{s:t},\mathbf{p}_{s:t})\cdot(t-s)=\max_{j\in[ m]}\max_{\sigma\in\{\pm 1\}}\sum_{\tau\in[s,t]}I_{\tau,g_{(s,i,j,\sigma)}} \ell_{\tau,g_{(s,i,j,\sigma)}}\]

We add and subtract in the above \(\sum_{\tau}\hat{\ell}_{\tau}\) to make the regret of the AdaNormalHedge on the sleeping experts instance appear and so the aforementioned becomes:

\[\mathrm{CalErr}_{i}(\mathbf{h}_{s:t},\mathbf{p}_{s:t})\cdot(t-s)=\underbrace {\mathrm{Reg}_{t}(g_{(s,i,j,\sigma)})}_{O(\sqrt{(t-s)\cdot\log(kmt)})}+\max_{j \in\mathcal{A}_{P}}\max_{\sigma\in\{\pm 1\}}\underbrace{\sum_{\tau\in[s,t]}I_{\tau,g_{(s,i,j, \sigma)}}\hat{\ell}_{\tau,g_{(s,i,j,\sigma)}}}_{\tau}\]

where for the regret, we have substituted the regret obtained by AdaNormalHedge. Note that \(\hat{\ell}_{\tau}=\mathbb{E}_{g\sim\pi_{\tau}}[\ell_{\tau,g}]\). This, together with Equation (6) helps us translate the sleeping experts' loss to a loss that depends on \(\mathbf{h}_{\tau},\mathbf{p}_{\tau}\). Adding and subtracting the term \(\sum_{\tau\in[s,t]}\mathop{\mathbb{E}}_{\begin{subarray}{c}\rho\sim\pi_{\tau} \\ \mathbf{p}\sim Q_{\tau}\end{subarray}}\left[L_{g}(\mathbf{h}_{\tau},\mathbf{p})\right]\):

\[\mathcal{T}\leq\sum_{\tau\in[s,t]}\max_{\mathbf{h}_{\tau}\in\Delta(\mathcal{A }_{P})}\mathop{\mathbb{E}}_{\begin{subarray}{c}\rho\sim\pi_{\tau}\\ \mathbf{p}\sim Q_{\tau}\end{subarray}}\left[L_{g}(\mathbf{h}_{\tau},\mathbf{p}) \right]+\sum_{\tau\in[s,t]}\mathop{\mathbb{E}}_{g\sim P_{\tau}}\left[L_{g}( \mathbf{h}_{\tau},\mathbf{p}_{\tau})-\mathop{\mathbb{E}}_{\mathbf{p}\in Q_{ \tau}}\left[L_{g}(\mathbf{h}_{\tau},\mathbf{p})\right]\right]\]

The first term above is upper bounded by \(\varepsilon\cdot(t-s)\) (Equation (8)) (or \(0\) in the continuous setting). The second can be bound with martingale concentration inequalities. 

## 5 Continuous Games

In this section, we generalize our results for the case of _continuous_ Stackelberg games. The supplementary material can be found in Appendix E.

**Continuous Stackelberg Games.** We use again \(\mathcal{A}_{P}\) and \(\mathcal{A}_{A}\) to denote the principal and the agent action spaces, respectively. Both \(\mathcal{A}_{A},\mathcal{A}_{P}\) are convex, compact sets where \(\mathcal{A}_{P}\subset\mathbb{R}^{m}\) and \(\mathcal{A}_{A}\subset\mathbb{R}^{k}\). The utilities of the principal and the agent are given by continuous functions \(U_{P}:\mathcal{A}_{P}\times\mathcal{A}_{A}\rightarrow\mathbb{R}_{+}\) and \(U_{A}:\mathcal{A}_{P}\times\mathcal{A}_{A}\rightarrow\mathbb{R}_{+}\). In this setting, we assume that both the principal and the agent can only play deterministic strategies, i.e., \(\mathcal{H}_{P}=\mathcal{A}_{P}\). For \(x\in\mathcal{A}_{P}\), let \(\mathtt{BR}(x)\) be the best-response function that is implicitly defined as \(\nabla_{2}U_{A}(x,\mathtt{BR}(x))=0\). Our continuous games satisfy Assumption 5.1: (i)-(iii) are standard assumptions used in previous works (e.g., [24]), but (iv) cannot be derived from (i) and (ii) without further assumptions on the correlation between \(x,y\). Nevertheless, (iv) (and the conditions under which it holds) has been justified in settings such as strategic classification [21, 51].

**Assumption 5.1**.: _Utility functions \(U_{P}\), \(U_{A}\), and the domain \(\mathcal{A}_{P}\) satisfy the following:_

1. _For all_ \(x\in\mathcal{A}_{P},y\in\mathcal{A}_{A}\)_,_ \(U_{P}(x,y)\) _is_ \(L_{1}\)_-Lipschitz and concave in_ \(x\)_,_ \(L_{2}\)_-Lipschitz in_ \(y\)_, and bounded by_ \(W_{P}\) _in_ \(\ell_{2}\) _norm._
2. _The best-response function_ \(\mathtt{BR}:\mathcal{A}_{P}\to\mathcal{A}_{A}\) _is_ \(L_{\mathtt{BR}}\)_-Lipschitz._
3. _Regularity of the feasible set_ \(\mathcal{A}_{P}=\mathcal{H}_{P}=\mathcal{F}_{P}\)_:_ * _The diameter is bounded:_ \(\mathtt{diam}(\mathcal{F}_{P})=\sup_{\mathbf{h},\mathbf{h}^{\prime}\in \mathcal{F}_{P}}\|\mathbf{h}-\mathbf{h}^{\prime}\|_{2}\leq D_{P}\)_._ * \(B(0,r)\subseteq\mathcal{A}_{P}\subseteq B(0,R)\)_._
4. _The function_ \(U_{P}(\mathbf{h},\mathtt{BR}(\mathbf{h}))\) _is concave with respect to_ \(\mathbf{h}\)_, and has Lipschitz constant_ \(L_{U}\)_._

The main result of this section is to show that even in _continuous_ CSGs, we can approximate asymptotically \(V^{\star}\) for the principal's utility, and that no better utility is actually achieved.

**Theorem 5.2**.: _For continuous CSGs satisfying Assumption 5.1, for all \(\varepsilon_{0}>0\), there exists a finite binning \(\Pi_{0}\) such that if the agent is \((0,\Pi_{0})\)4 - adaptively calibrated and the principal runs an appropriately parametrized instance of LazyGDwoG (Algorithm 3) then: \(\lim_{\genfrac{}{}{0.0pt}{}{\delta\to\infty}{\delta\lambda}}\frac{1}{\delta \lambda}\sum_{\phi\in[\Phi]}\sum_{i\in[M]}U_{P}(\mathbf{h}_{\phi},y_{\phi,i}) \geq V^{\star}-\varepsilon_{0}\). Moreover, for any sequence of the principal's actions \(\mathbf{h}_{[1:T]}\), it holds that: \(\lim_{T\to\infty}\frac{1}{T}\sum_{t\in[T]}U_{P}(\mathbf{h}_{t},y_{t})\leq V^ {\star}+\varepsilon_{0}\)._

Footnote 4: We can define \((0,\Pi)\)-adaptive calibration in continuous CSGs due to the continuous case in Theorem 4.1.

We outline next how LazyGDwoG works. LazyGDwoG is a variant of the gradient descent without a gradient algorithm (GDwoG) of Flaxman et al. [26]. The main new component of the algorithm is that it separates the time horizon into epochs and for each epoch it runs an update of the GDwoG algorithm. During all the rounds that comprise an epoch (\(M\) in total), LazyGDwoG presents the same (appropriately smoothed-out) strategy to the agent and observes the \(M\) different responses from the agent. The intuition behind repeating the same strategy for \(M\) rounds is that the principal wants to give the opportunity to the agent to recalibrate for a better forecast, i.e., \(\lim_{M\to\infty}\frac{1}{\lambda t}|\{i\in[M]:\|\mathbf{p}_{i}-\mathbf{h}\| \geq\varepsilon_{0}\}|=0\). The remainder of the proof for Theorem 5.2 focuses on showing that when the calibrated forecasts converge to \(\mathbf{h}_{t}\), then the principal's utility converges to the utility they would have gotten if the agent was perfectly best responding to \(\mathbf{h}_{t}\).

## 6 Discussion and future directions

In this paper we introduced and studied learning in CSGs, where the agents best respond to the principal's actions based on _calibrated_ forecasts that they have about them. Our work opens up several exciting avenues for future research. First, although our main results prove asymptotic convergence, it is an open question whether our exact convergence rates can be improved both for general CSGs and for specific cases of Stackelberg games (e.g., strategic classification in more general models compared to [21], pricing [1]). Second, it is an interesting question whether our definition of adaptive calibration (Def. 2.3) can actually hold for the sum over all binning functions, instead of just the maximum. Finally, to provide our asymptotic convergence results we assumed that the principal has access to the agent's calibration rate \(r_{\delta}(\cdot)\); _some_ information regarding how \(\mathbf{p}_{t}\)'s relate to \(\mathbf{h}_{t}\)'s is necessary to leverage the fact that agents are calibrated. But we think that in some specific settings (e.g., strategic classification) there may actually exist extra information regarding the forecasts (compared to just knowing \(r_{\delta}(\cdot)\)) that can be leveraged to design learning algorithms for the principal with faster convergence rates. We discuss these directions in more detail in Appendix F.

## References

* [1] The value of knowing a demand curve: Bounds on regret for online posted-price auctions. In _44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings._, pages 594-605. IEEE, 2003.
* [2] Jacob D Abernethy, Rachel Cummings, Bhuvesh Kumar, Sam Taggart, and Jamie H Morgenstern. Learning auctions with robust incentive guarantees. _Advances in Neural Information Processing Systems_, 32, 2019.
* [3] Saba Ahmadi, Hedyeh Beyhaghi, Avrim Blum, and Keziah Naggita. The strategic perceptron. In _Proceedings of the 22nd ACM Conference on Economics and Computation (EC)_, pages 6-25, 2021.
* [4] Saba Ahmadi, Avrim Blum, and Kunhe Yang. Fundamental bounds on online strategic classification. In _Proceedings of the 24th ACM Conference on Economics and Computation (EC)_, pages 22-58, 2023.
* [5] Kareem Amin, Afshin Rostamizadeh, and Umar Syed. Learning prices for repeated auctions with strategic buyers. In _Advances in Neural Information Processing Systems_, volume 26, 2013.
* [6] Bo An, David Kempe, Christopher Kiekintveld, Eric Shieh, Satinder Singh, Milind Tambe, and Yevgeniy Vorobeychik. Security games with limited surveillance. In _Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)_, volume 26, pages 1241-1248, 2012.
* [7] Maria-Florina Balcan, Avrim Blum, Nika Haghtalab, and Ariel D Procaccia. Commitment without regrets: Online learning in Stackelberg security games. In _Proceedings of the 16th ACM Conference on Economics and Computation (EC)_, pages 61-78, 2015.
* [8] Avrim Blum. Empirical support for winnow and weighted-majority algorithms: Results on a calendar scheduling domain. _Machine Learning_, 26:5-23, 1997.
* [9] Avrim Blum and Thodoris Lykouris. Advancing subgroup fairness via sleeping experts. In _Innovations in Theoretical Computer Science Conference (ITCS)_, volume 151, pages 55:1-55:24, 2020.
* [10] Avrim Blum and Yishay Mansour. From external to internal regret. _Journal of Machine Learning Research_, 8(6), 2007.
* [11] Avrim Blum, Nika Haghtalab, and Ariel Procaccia. Lazy defenders are almost optimal against diligent attackers. In _Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)_, volume 28, 2014.
* [12] Avrim Blum, Nika Haghtalab, and Ariel Procaccia. Learning optimal commitment to overcome insecurity. In _Advances in Neural Information Processing Systems_, volume 27, pages 1826-1834, 2014.
* [13] Mark Braverman, Jieming Mao, Jon Schneider, and Matt Weinberg. Selling to a no-regret buyer. In _Proceedings of the 19th ACM Conference on Economics and Computation (EC)_, pages 523-538, 2018.
* [14] Modibo K Camara, Jason D Hartline, and Aleck Johnsen. Mechanisms for a no-regret agent: Beyond the common prior. In _Proceedings of the 61st Annual Symposium on Foundations of Computer Science (FOCS)_, pages 259-270. IEEE, 2020.
* [15] Yiling Chen, Yang Liu, and Chara Podimata. Learning strategy-aware linear classifiers. In _Advances in Neural Information Processing Systems_, volume 33, pages 15265-15276, 2020.
* [16] Natalie Collina, Eshwar Ram Arunachaleswaran, and Michael Kearns. Efficient stackelberg strategies for finitely repeated games. In _Proceedings of the 2023 International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS)_, AAMAS '23, page 643-651, 2023.
* [17] Vincent Conitzer and Tuomas Sandholm. Computing the optimal strategy to commit to. In _Proceedings of the 7th ACM Conference on Electronic Commerce (EC)_, pages 82-90, 2006.

* [18] Amit Daniely, Alon Gonen, and Shai Shalev-Shwartz. Strongly adaptive online learning. In _International Conference on Machine Learning (ICML)_, pages 1405-1411. PMLR, 2015.
* [19] A Philip Dawid. The well-calibrated bayesian. _Journal of the American Statistical Association_, 77(379):605-610, 1982.
* [20] Yuan Deng, Jon Schneider, and Balasubramanian Sivan. Strategizing against no-regret learners. In _Advances in Neural Information Processing Systems_, volume 32, pages 1579-1587, 2019.
* [21] Jinshuo Dong, Aaron Roth, Zachary Schutzman, Bo Waggoner, and Zhiwei Steven Wu. Strategic classification from revealed preferences. In _Proceedings of the 19th ACM Conference on Economics and Computation (EC)_, pages 55-70, 2018.
* [22] Fei Fang, Thanh Nguyen, Benjamin Ford, Nicole Sintov, and Milind Tambe. Introduction to green security games. In _International Joint Conference on Artificial Intelligence (IJCAI)_, 2015.
* [23] Uriel Feige and Gideon Schechtman. On the optimality of the random hyperplane rounding technique for max cut. _Random Structures & Algorithms_, 20(3):403-440, 2002.
* [24] Tanner Fiez, Benjamin Chasnov, and Lillian J Ratliff. Convergence of learning dynamics in stackelberg games. _arXiv preprint arXiv:1906.01217_, 2019.
* [25] Tanner Fiez, Benjamin Chasnov, and Lillian Ratliff. Implicit learning dynamics in Stackelberg games: Equilibria characterization, convergence analysis, and empirical study. In _International Conference on Machine Learning (ICML)_, pages 3133-3144. PMLR, 2020.
* [26] Abraham D. Flaxman, Adam Tauman Kalai, and H. B. McMahan. Online convex optimization in the bandit setting: gradient descent without a gradient. In _Proceedings of the 15th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)_, 2004.
* [27] Dean P Foster and Sergiu Hart. Smooth calibration, leaky forecasts, finite recall, and Nash dynamics. _Games and Economic Behavior_, 109:271-293, 2018.
* [28] Dean P Foster and Sergiu Hart. Forecast hedging and calibration. _Journal of Political Economy_, 129(12):3447-3490, 2021.
* [29] Dean P Foster and Rakesh V Vohra. Calibrated learning and correlated equilibrium. _Games and Economic Behavior_, 21(1-2):40, 1997.
* [30] Dean P Foster and Rakesh V Vohra. Asymptotic calibration. _Biometrika_, 85(2):379-390, 1998.
* [31] Yoav Freund, Robert E Schapire, Yoram Singer, and Manfred K Warmuth. Using and combining predictors that specialize. In _Proceedings of the 39th Annual ACM Symposium on Theory of Computing (STOC)_, pages 334-343, 1997.
* [32] Jiarui Gan, Minbiao Han, Jibang Wu, and Haifeng Xu. Robust stackelberg equilibria. In _Proceedings of the 24th ACM Conference on Economics and Computation (EC)_, page 735, 2023.
* [33] Nika Haghtalab, Thodoris Lykouris, Sloan Nietert, and Alexander Wei. Learning in Stackelberg games with non-myopic agents. In _Proceedings of the 23rd ACM Conference on Economics and Computation (EC)_, pages 917-918, 2022.
* [34] Nika Haghtalab, Michael I Jordan, and Eric Zhao. A unifying perspective on multi-calibration: Unleashing game dynamics for multi-objective learning. _arXiv preprint arXiv:2302.10863_, 2023.
* [35] Moritz Hardt, Nimrod Megiddo, Christos Papadimitriou, and Mary Wootters. Strategic classification. In _Innovations in Theoretical Computer Science Conference (ITCS)_, pages 111-122, 2016.
* [36] Sergiu Hart. Calibrated forecasts: The minimax proof. _arXiv preprint arXiv:2209.05863_, 2022.

* [37] Kwang-Sung Jun, Francesco Orabona, Stephen Wright, and Rebecca Willett. Improved strongly adaptive online learning using coin betting. In _Artificial Intelligence and Statistics (AISTATS)_, pages 943-951. PMLR, 2017.
* [38] Sham M Kakade and Dean P Foster. Deterministic calibration and Nash equilibrium. _Journal of Computer and System Sciences_, 74(1):115-130, 2008.
* [39] Christopher Kiekintveld, Towhidul Islam, and Vladik Kreinovich. Security games with interval uncertainty. In _Proceedings of the 2013 international conference on Autonomous agents and multi-agent systems_, pages 231-238, 2013.
* [40] Christian Kroer, Gabriele Farina, and Tuomas Sandholm. In _Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)_, volume 32, 2018.
* [41] Daniel Lee, Georgy Noarov, Mallesh Pai, and Aaron Roth. Online minimax multiobjective optimization: Multicalibeating and other applications. In _Advances in Neural Information Processing Systems_, volume 35, pages 29051-29063, 2022.
* [42] Yin Tat Lee, Aaron Sidford, and Santosh S Vempala. Efficient convex optimization with membership oracles. In _Conference on Learning Theory (COLT)_, pages 1292-1294. PMLR, 2018.
* [43] Joshua Letchford, Vincent Conitzer, and Kamesh Munagala. Learning and approximating the optimal strategy to commit to. In _Algorithmic Game Theory_, pages 250-262. Springer, 2009.
* [44] Haipeng Luo and Robert E Schapire. Achieving all with no parameters: Adanormalhedge. In _Conference on Learning Theory (COLT)_, pages 1286-1304. PMLR, 2015.
* [45] Yishay Mansour, Mehryar Mohri, Jon Schneider, and Balasubramanian Sivan. Strategizing against learners in Bayesian games. In _Conference on Learning Theory (COLT)_, pages 5221-5252. PMLR, 2022.
* [46] Vidya Muthukumar and Anant Sahai. Robust commitments and partial reputation. In _Proceedings of the 20th ACM Conference on Economics and Computation (EC)_, pages 637-638, 2019.
* [47] Binghui Peng, Weiran Shen, Pingzhong Tang, and Song Zuo. Learning optimal strategies to commit to. In _Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)_, volume 33, pages 2149-2156, 2019.
* [48] James Pita, Manish Jain, Milind Tambe, Fernando Ordonez, and Sarit Kraus. Robust solutions to Stackelberg games: Addressing bounded rationality and limited observations in human cognition. _Artificial Intelligence_, 174(15):1142-1171, 2010.
* [49] Milind Tambe. _Security and game theory: algorithms, deployed systems, lessons learned_. Cambridge university press, 2011.
* [50] Haifeng Xu, Long Tran-Thanh, and Nicholas R Jennings. Playing repeated security games with no prior knowledge. In _Proceedings of the 2016 International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS)_, pages 104-112, 2016.
* [51] Tijana Zrnic, Eric Mazumdar, Shankar Sastry, and Michael Jordan. Who leads and who follows in strategic classification? In _Advances in Neural Information Processing Systems_, volume 34, pages 15257-15269, 2021.
* [52] Song Zuo and Pingzhong Tang. Optimal machine strategies to commit to in two-person repeated games. In _Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)_, volume 29, 2015.

Calibrated Forecasts Standard Definition

We give below the standard definition for asymptotic calibration of Foster and Vohra [30] for a sequence of binary outcomes, i.e., \(\mathbf{h}_{t}\in A=\{0,1\},\forall t\in[T]\). The forecasts \(\mathbf{p}_{t}\) take values in \(C=[0,1]\). Let \(X\) denote the adaptive adversary generating the events' sequence (which is of infinite size), where the \(T\) first events are \(\mathbf{h}_{1},\ldots,\mathbf{h}_{T}\).

**Definition A.1**.: _A forecasting procedure \(\sigma\) is asymptotically calibrated if and only if for any adaptive adversary \(X\) that generates the sequence \(\mathbf{h}_{1},\cdots,\mathbf{h}_{T}\in A\) and the forecasting algorithm \(\sigma\) that generates (possibly random) forecasts \(\mathbf{p}_{1},\cdots,\mathbf{p}_{T}\in C\) on the same sequence, we have that the calibration score \(C_{T}(X,\sigma)\) goes to \(0\) as \(T\rightarrow\infty\):_

\[C_{T}(X,\sigma)\triangleq\sum_{\mathbf{p}\in\mathcal{F}_{P}}\frac{n_{T}( \mathbf{p};\mathbf{h},\sigma)}{T}\left|\rho_{T}(\mathbf{p};\mathbf{h},\sigma)- \mathbf{p}\right|\] (9)

_where \(n_{T}(\mathbf{p};\mathbf{h},\sigma)\triangleq\sum_{t\in[T]}\mathbf{1}\{ \mathbf{p}_{t}=\mathbf{p}\}\) is the number of times that \(\sigma\) predicts \(\mathbf{p}\) and \(\rho_{T}(\mathbf{p};\mathbf{h},\sigma)\triangleq\frac{\sum_{t\in[T]}\mathbf{ h}_{t}\{\mathbf{p}_{t}=\mathbf{p}\}}{n_{T}(\mathbf{p};\mathbf{h},\sigma)}\) be the fraction (empirical probability) of these times that the actual event was \(1\)._

Note that in Eq. (9), while \(\mathcal{F}_{P}\) contains an infinite number of distinct \(\mathbf{p}\)'s (hence an infinite number of summands), for every finite \(T\), there is only a finite number of \(\mathbf{p}\) where \(n_{T}(\mathbf{p};\mathbf{h},\sigma)\) is nonzero. Therefore, \(C_{T}\) is well-defined and finite.

Equivalently, the above definition states that for the infinite binning [28]\(\Pi=\left\{w_{x}(\mathbf{p}):x\in C\right\}\) where \(w_{x}(\mathbf{p})=\mathbf{1}\{\mathbf{p}=x\}\), the calibration score can be equivalently expressed as

\[C_{T}(X,\sigma)\triangleq\sum_{w_{x}\in\Pi}\frac{n_{T}(x)}{T}\left\|\bar{ \mathbf{h}}_{T}(x)-\bar{\mathbf{p}}_{T}(x)\right\|,\]

where \(n_{T}(x)\triangleq\sum_{t=1}^{T}w_{x}(\mathbf{p}_{t})\) is the number of times that forecast \(\mathbf{p}_{t}\) falls into bin \(x\), \(\bar{\mathbf{p}}_{T}(x)\triangleq\sum_{t=1}^{T}\frac{w_{x}(\mathbf{p}_{t})}{n_ {T}(x)}\cdot\mathbf{p}_{t}\) is the average forecast that activates bin \(x\), which is equal to \(\sum_{t=1}^{T}\frac{w_{x}(\mathbf{p}_{t})}{n_{T}(x)}\cdot x=x\) because \(w_{x}(\mathbf{p}_{t})\) is nonzero if and only if \(\mathbf{p}_{t}=x\), and \(\bar{\mathbf{h}}_{T}(x)\triangleq\sum_{t=1}^{T}\frac{w_{x}(\mathbf{p}_{t})}{ n_{T}(x)}\cdot\mathbf{h}_{t}\) is the average outcome corresponding to bin \(x\). It follows that the score \(C_{T}\) is a sum of the calibration errors during interval \([1:T]\) for all bins (with \(\mathrm{CalErr}\) defined in Definition 2.3).

\[C_{T}(X,\sigma)=\sum_{w_{x}\in\Pi}\mathrm{CalErr}_{x}(\mathbf{h}_{1:T}, \mathbf{p}_{1:T}).\]

## Appendix B Calibrated Forecasts Lead to No Swap Regret

In this section, we show the connection between no-swap-regret agents and adaptively calibrated ones. As a reminder, no-swap-regret agents (translated to our setting and notation for the ease of exposition) are defined as follows.

**Definition B.1** (Agent's swap regret [10]).: _For a sequence of principal's strategies \(\mathbf{h}_{1},\cdots,\mathbf{h}_{T}\in\mathcal{H}_{P}\) and agent's actions \(y_{1},\cdots,y_{T}\in\mathcal{A}_{A}\), the swap regret is defined as_

\[\mathrm{SwapReg}(\mathbf{h}_{1:T},y_{1:T})=\max_{\pi:\mathcal{A}_{A} \rightarrow\mathcal{A}_{A}}\sum_{t\in[T]}U_{A}(\mathbf{h}_{t},\pi(y_{t}))- \sum_{t\in[T]}U_{A}(\mathbf{h}_{t},y_{t}).\]

_We say that an agent is a no-swap-regret agent, if for the sequence of actions \(\{y_{t}\}_{t\in[T]}\) that they are playing it holds that \(\mathrm{SwapReg}(\mathbf{h}_{1:T},y_{1:T})=o(T)\)._

We next show that calibrated forecasts lead to no swap regret actions for the agent.

**Lemma B.2** (Calibrated forecasts lead to no swap regret).: _If the agent is \((\varepsilon,\Pi)\)-adaptively calibrated, then the agent's swap regret on the sequence \(\mathbf{h}_{1:T}\) is bounded by the calibration error as follows:_

* _If the agent breaks ties deterministically, then with probability_ \(\geq 1-\delta,\)__ \[\mathrm{SwapReg}(\mathbf{h}_{1:T},y_{1:T})\leq 2U_{\max}mkT\left(r_{\delta}(T)+ \varepsilon\right)\in o(T).\]* _If the agent breaks ties randomly, then with probability_ \(\geq 1-2\delta\)_,_ \[\mathrm{SwapReg}(\mathbf{h}_{1:T},y_{1:T})\leq U_{\max}\left(O\left(\sqrt{Tk\log \left(\frac{k}{\delta}\right)}\right)+2mkT\left(r_{\delta}(T)+\varepsilon\right) \right)\in o(T).\]

_where \(U_{\max}=\max_{\mathbf{h}\in\mathcal{A}_{P}}\max_{y\in\mathcal{A}_{A}}U_{A}( \mathbf{h},y)\) is the maximum utility the agent can obtain (without constraining the agent to play best responses)._

Proof.: We first present the proof for the case that the agents break ties deterministically. To simplify notation, we use \(n_{T}(i):=n_{[0:T]}(i)\), \(\bar{\mathbf{p}}_{T}:=\bar{\mathbf{p}}_{[0:T]}(i)\), and \(\bar{\mathbf{h}}_{T}(i):=\bar{\mathbf{h}}_{[0:T]}(i)\).

Fix a \(\pi:\mathcal{A}_{A}\rightarrow\mathcal{A}_{A}\). Then, with probability at least \(1-\delta\), we have that:

\[\sum_{t=1}^{T}U_{A}(\mathbf{h}_{t},\pi(y_{t}))-\sum_{t=1}^{T}U_{A }(\mathbf{h}_{t},y_{t})\] \[=\sum_{i\in\mathcal{A}_{A}}\sum_{t=1}^{T}\mathbf{1}\{y_{t}=i\} \left(U_{A}(\mathbf{h}_{t},\pi(i))-U_{A}(\mathbf{h}_{t},i)\right)\] (rewriting \[y_{t}\] as the exact action) \[\stackrel{{\mathrm{(a)}}}{{=}}\sum_{i\in\mathcal{A} _{A}}\sum_{t=1}^{T}w_{i}(\mathbf{p}_{t})\left(\langle\mathbf{h}_{t},U_{A}( \cdot,\pi(i))\rangle-\langle\mathbf{h}_{t},U_{A}(\cdot,i)\rangle\right)\] (10) \[=\sum_{i\in\mathcal{A}_{A}}n_{T}(i)\left(\langle\bar{\mathbf{h}} _{T}(i),U_{A}(\cdot,\pi(i))\rangle-\langle\bar{\mathbf{h}}_{T}(i),U_{A}(\cdot, i)\rangle\right)\] \[=\sum_{i\in\mathcal{A}_{A}}n_{T}(i)\left(\langle\bar{\mathbf{h}} _{T}(i)-\bar{\mathbf{p}}_{T}(i),U_{A}(\cdot,\pi(i))\rangle+\langle\bar{ \mathbf{p}}_{T}(i),U_{A}(\cdot,\pi(i))-U_{A}(\cdot,i)\rangle+\langle\bar{ \mathbf{p}}_{T}(i)-\bar{\mathbf{h}}_{T}(i),U_{A}(\cdot,i)\rangle\right)\] \[\stackrel{{\mathrm{(b)}}}{{\leq}}\sum_{i\in\mathcal{ A}_{A}}n_{T}(i)\left\|\bar{\mathbf{p}}_{T}(i)-\bar{\mathbf{h}}_{T}(i)\right\|_{ \infty}\left(\|U_{A}(\cdot,\pi(i))\|_{1}+\|U_{A}(\cdot,i)\|_{1}\right)\] \[=2U_{\max}m\cdot\sum_{i\in\mathcal{A}_{A}}T\cdot\mathrm{CalErr}_{ i}(\mathbf{h}_{1:T},\mathbf{p}_{1:T})\] (Def. 2.3) \[\leq 2U_{\max}mkT\left(r_{\delta}(T)+\varepsilon\right).\]

In the above equations, step (a) is due to the fact that agents best respond with a deterministic tie-breaking rule: \(y_{t}=i\) if and only if \(i\in\mathtt{BR}(\mathbf{p}_{t})\) and \(i\succ j,\forall j\neq i\), which is equivalent to \(w_{i}(\mathbf{p}_{t})=1\). We have also used \(U_{A}(\cdot,i)\) to denote the \(m\)-dimensional vector where the \(j\)th entry is the utility \(U_{A}(j,i)\). Step (b) is because the second term

\[\langle\bar{\mathbf{p}}_{T}(i),U_{A}(\cdot,\pi(i))-U_{A}(\cdot,i)\rangle=U_{A} (\bar{\mathbf{p}}_{T}(i),\pi(i))-U_{A}(\bar{\mathbf{p}}_{T}(i),i)\]

is non-positive since each \(\mathbf{p}_{t}\) with \(w_{i}(\mathbf{p}_{t})=1\) belongs to the best response polytope \(P_{i}\), so does their average: \(\bar{\mathbf{p}}_{t}(i)\in P_{i}\iff i\in\mathtt{BR}(\bar{\mathbf{p}}_{t}(i))\).

Since the above inequality holds for any \(\pi\), it also holds after taking the maximum over all \(\pi:\mathcal{A}_{A}\rightarrow\mathcal{A}_{A}\). Therefore, we have the same bound for the agent's swap regret.

Next, we move to the case when the agent breaks ties randomly. For a fixed \(\pi\), we have that at every time step \(t\),

\[\mathbb{E}_{t-1}\left[U_{A}(\mathbf{h}_{t},\pi(y_{t}))\right]=\frac{\sum_{i\in \mathtt{BR}(\mathbf{p}_{t})}U_{A}(\mathbf{h}_{t},\pi(i))}{|\mathtt{BR}( \mathbf{p}_{t})|}=\sum_{i\in\mathcal{A}_{A}}w_{i}(\mathbf{p}_{t})U_{A}(\mathbf{ h}_{t},\pi(i)).\]

Therefore, by Azuma-Hoeffding's inequality, w.p. \(\geq 1-\delta^{\prime}\), we have

\[\sum_{t=1}^{T}U_{A}(\mathbf{h}_{t},\pi(y_{t}))\leq\sum_{t=1}^{T}w_{i}(\mathbf{ p}_{t})\sum_{i\in\mathcal{A}_{A}}U_{A}(\mathbf{h}_{t},\pi(i))+O\left(\sqrt{T\log \left(\frac{1}{\delta^{\prime}}\right)}\right).\] (11)

Since all actions in \(\mathtt{BR}(\mathbf{p}_{t})\) have the same utility for the agents, we also have

\[U_{A}(\mathbf{h}_{t},y_{t})=\frac{\sum_{i\in\mathtt{BR}(\mathbf{p}_{t})}U_{A}( \mathbf{h}_{t},i)}{|\mathtt{BR}(\mathbf{p}_{t})|}=\sum_{i\in\mathcal{A}_{A}}w_ {i}(\mathbf{p}_{t})U_{A}(\mathbf{h}_{t},i).\]

[MISSING_PAGE_EMPTY:16]

### Approximate membership oracle to conservative polytopes

In this section, we formally present the algorithm (Algorithm 4 for constructing an approximate membership oracle to the conservative best response polytope for each of the agent's action. The sample complexity of the oracle will be presented in Lemma C.2.

``` Input: query \(\mathbf{h}\in\mathcal{H}_{P}\), original polytope \(P_{y}\)\((y\in\mathcal{A}_{A})\), approximation factor \(\varepsilon_{1}\), conservatism factor \(\varepsilon_{2}\), failure probability \(\varepsilon_{3}\) Parameters: Number of epochs \(\Phi\), radius \(R\), calibration error \(\varepsilon_{\text{cal}}\).  Let \(l\) be s.t. \(r_{\delta}(l)=\frac{\varepsilon_{\text{cal}}}{k\sqrt{m}}\) forepoch\(\phi\in[\Phi]\)do  Sample a point \(\mathbf{h}_{\phi}\) such that \(\|\mathbf{h}_{\phi}-\mathbf{h}\|_{2}=R\). /* Sample point \(\mathbf{h}_{\phi}\) close to h */ if\(\mathbf{h}_{\phi}\notin\mathcal{H}_{P}\)then  Return False /* Nolonger inside the feasible set */ else  Play strategy \(\mathbf{h}_{\phi}\) for \(l\) rounds. \(y_{\phi}\leftarrow\) most frequent best-response action from agent during the \(l\) rounds. if\(y_{\phi}\neq y\)then Return False /* \(\mathbf{h}_{\phi}\) is too close to \(P_{y_{\phi}}\) */ /* For membership, output \(y_{\Phi}\) if all \(\{y_{\phi}\}_{\phi\in[\Phi]}\) agree, and N/A otherwise */ Return True ```

**Algorithm 4**Approximate membership oracle for the conservative best response polytope (ApproxMem)

In Lemma C.2, we show that the parameters \(\Phi,\varepsilon_{\text{cal}},R\) can be tuned to achieve a wide range of parameters \((\varepsilon_{1},\varepsilon_{2},\varepsilon_{3})\). In Proposition C.3, we provide explicit instantiations of the parameters for three special cases of \((\varepsilon_{1},\varepsilon_{2},\varepsilon_{3})\) that will be useful in later sections.

**Lemma C.2**.: _If the agent is \((\varepsilon,\Pi)\)-adaptively calibrated with rate \(r_{\delta}(\cdot)\) and infinitesimal \(\varepsilon\), and the parameters \(\Phi,R,\varepsilon_{\text{cal}}\) satisfy:_

\[\varepsilon_{1}+\varepsilon_{2}-R\geq\varepsilon_{\text{cal}}\] (Condition 1) \[\Phi\geq 10\sqrt{m}\left(1-\left(\frac{\varepsilon_{\text{cal}} +\varepsilon_{2}}{R}\right)^{2}\right)^{\frac{m-1}{2}}\log\left(\frac{1}{ \varepsilon_{3}-\delta}\right)\] (Condition 2)

_Then Algorithm 4 (ApproxMem) returns an \(\varepsilon_{1}\)-approximate membership oracle to \(P_{y}^{-\varepsilon_{2}}=B_{2}(P_{y},-\varepsilon_{2})\) with probability \(1-\varepsilon_{3}\), using no more than \(N_{\varepsilon_{1},\varepsilon_{2},\varepsilon_{3}}=O\left(\Phi r_{\delta}^{-1 }(\frac{\varepsilon_{\text{cal}}}{k\sqrt{m}})\right)\) rounds of interactions with the agent._

_Specifically, with probability \(1-\varepsilon_{3}\), ApproxMem either returns True which asserts that \(\mathbf{h}\in B_{2}(P_{y}^{-\varepsilon_{2}},+\varepsilon_{1})\), or returns False which asserts that \(\mathbf{h}\notin B_{2}(P_{y}^{-\varepsilon_{2}},-\varepsilon_{1})\)._

**Proposition C.3** (Parameter settings).: _Assume \(\varepsilon_{3}=\delta+T^{-2}\). In the following three cases: \(\frac{\varepsilon_{1}}{\varepsilon_{2}}=\Theta(\sqrt{m}),1,o(1)\), the proposed setting of parameters \(\varepsilon_{\text{cal}},R,\Phi\) satisfy (Condition 1) and (Condition 2) simultaneously._

_Case I. \(\frac{\varepsilon_{1}}{\varepsilon_{2}}=\Theta(\sqrt{m})\)._

\[\varepsilon_{\text{cal}}=\varepsilon_{2},\quad R=\frac{\varepsilon_{1}}{2}, \quad\Phi=10\sqrt{m}\left(1-\frac{4\varepsilon_{2}}{\varepsilon_{1}}\right)^ {\frac{m-1}{2}}\log(T)=O(\sqrt{m}\log(T)).\]

_Case II. \(\frac{\varepsilon_{1}}{\varepsilon_{2}}=1\)._

\[\varepsilon_{\text{cal}}=0.1\varepsilon_{2},\quad R=1.9\varepsilon_{2},\quad \Phi=1.25^{m}\log(T).\]

_Case III. \(\frac{\varepsilon_{1}}{\varepsilon_{2}}=o(1)\)._

\[\varepsilon_{\text{cal}}=\frac{\varepsilon_{1}}{6},\quad R=\left(\varepsilon_{2 }+\frac{\varepsilon_{1}}{6}\right)\left(1+\frac{\varepsilon_{1}}{2\varepsilon_ {2}}\right),\quad\Phi=10\sqrt{m}\left(\frac{\varepsilon_{2}}{\varepsilon_{1}} \right)^{\frac{m}{2}}\log(T).\]Proof of Lemma c.2.: Before we delve into the specifics of the proof, we introduce some notation. In order to make sure that \(\mathbf{h}_{\phi}\) is such that \(\|\mathbf{h}-\mathbf{h}_{\phi}\|=R\), we do the following: \(\mathbf{h}_{\phi}\leftarrow\mathbf{h}+R\mathbf{S}_{\phi}\), where \(\mathbf{S}_{\phi}\) is sampled uniformly at random from the equator \(\mathbb{S}\cap\mathbb{H}\), where \(\mathbb{S}=\{\mathbf{s}\in\mathbb{R}^{m}:\|\mathbf{s}\|_{2}^{2}=1\}\) is the unit sphere and \(\mathbb{H}=\{\mathbf{s}\in\mathbb{R}^{m}:\langle\mathbf{s},\mathbf{1}\rangle=0\}\) is an equatorial hyperplane (\(\mathbf{1}\triangleq(1,\cdots,1)\in\mathbb{R}^{m}\)). Note that this is because we want \(\mathbf{h}_{\phi}\) to remain a valid probability distribution, i.e., that \(\langle\mathbf{h}_{\phi},\mathbf{1}\rangle=1\) and \(\mathbf{h}_{\phi}\geq 0\) coordinate-wise; indeed, since we already have \(\langle\mathbf{h},\mathbf{1}\rangle=1\), we need to make sure that (1) \(\langle\mathbf{S}_{\phi},\mathbf{1}\rangle=0\), which is guaranteed by \(\mathbf{S}_{\phi}\in\mathbb{H}\); (2) \(\mathbf{h}_{\phi}\geq 0\), which is guaranteed by returning False whenever \(\mathbf{h}_{\phi}\notin\mathcal{H}_{P}\).

For the rest of the proof, we condition on the following success event:

\[\mathcal{E}\triangleq\Big{\{}\forall[s,t]\subseteq[1,T],\;\forall i\in \mathcal{A}_{A},\;\mathrm{CalErr}_{i}\left(\mathbf{h}_{s:t},\mathbf{p}_{s:t} \right)\leq r_{\delta}(t-s)\Big{\}}.\]

Since the agent is \((\varepsilon,\Pi)\)-adaptively calibrated with rate \(r_{\delta}\), we have \(\Pr(\mathcal{E})\geq 1-\delta\).

Recall that \(\varepsilon_{\text{cal}}=k\sqrt{mr}_{\delta}(l)\) denote the error that comes from calibration error. We first show that conditioned on \(\mathcal{E}\), \(\forall\phi\in[\Phi]\), \(\mathbf{h}_{\phi}\in B_{2}(P_{y_{\phi}},\varepsilon_{\text{cal}})\). Let \(l_{y_{\phi}}\) be the number of times that agent plays \(y_{\phi}\) during the \(l\) repeats, then we have \(l_{y_{\phi}}\geq l/k\) because \(y_{\phi}\) is the most frequently played action. Then, the calibration error bound in Definition 2.3 guarantees that

\[\frac{l_{y_{\phi}}}{l}\left\|\bar{\mathbf{p}}(y_{\phi})-\mathbf{h }_{\phi}\right\|_{\infty}=\mathrm{CalErr}_{y_{\phi}}(\mathbf{h}_{\phi,1:l}, \mathbf{p}_{\phi,1:l})\leq r_{\delta}(l)+\varepsilon\] \[\Rightarrow \left\|\bar{\mathbf{p}}(y_{\phi})-\mathbf{h}_{\phi}\right\|_{2} \leq\sqrt{m}\left\|\bar{\mathbf{p}}(y_{\phi})-\mathbf{h}_{\phi}\right\|_{ \infty}\leq\sqrt{m}kr_{\delta}(l)=\varepsilon_{\text{cal}}.\] (13)

where the first inequality in equation (13) is because of the norm property \(\|x\|_{2}\leq\sqrt{d}\|x\|_{\infty}\) for a vector \(x\in\mathbb{R}^{d}\). Since \(\bar{\mathbf{p}}(y_{\phi})\in P_{y_{\phi}}\) because the agent always best responds to forecasts, we obtain \(\mathbf{h}_{\phi}\in B_{2}(P_{y_{\phi}},\varepsilon_{\text{cal}})\).

We then prove the following two claims:

1. If \(\mathbf{h}\in B_{2}(P_{y}^{-\varepsilon_{2}},-\varepsilon_{1})\), then ApproxMem returns True.
2. If \(\mathbf{h}\notin B_{2}(P_{y}^{-\varepsilon_{2}},+\varepsilon_{1})\), then ApproxMem returns False with probability \(\geq 1-(\varepsilon_{3}-\delta)\).

Indeed, if the following two claims hold, then we have established that ApproxMem asserts one of two cases correctly with probability \(\geq 1-(\varepsilon_{3}-\delta)\) conditioned on \(\mathcal{E}\). Together with the fact that \(\Pr(\mathcal{E})\geq 1-\delta\), proves the lemma.

Proof of (C1).: Suppose \(\mathbf{h}\in B_{2}(P_{y}^{-\varepsilon_{2}},-\varepsilon_{1})\). Then we know that \(B_{2}(\mathbf{h},\varepsilon_{1})\subseteq P_{y}^{-\varepsilon_{2}}=B_{2}(P_{y },-\varepsilon_{2})\), which further implies \(B_{2}(\mathbf{h},\varepsilon_{1}+\varepsilon_{2})\subseteq P_{y}\). Therefore, the distance between \(\mathbf{h}\) and any other polytope \(P_{y^{\prime}}\)\((y^{\prime}\neq y)\) must be lower bounded by \(\varepsilon_{1}+\varepsilon_{2}\). By triangle inequality, \(\forall\mathbf{h}^{\prime}\in P_{y^{\prime}}\) where \(y^{\prime}\neq y\), we have

\[\forall\phi\in[\Phi],\quad\|\mathbf{h}_{\phi}-\mathbf{h}^{\prime}\|_{2}\geq\| \mathbf{h}-\mathbf{h}^{\prime}\|_{2}-\|\mathbf{h}-\mathbf{h}_{\phi}\|_{2} \geq\varepsilon_{1}+\varepsilon_{2}-R\geq\varepsilon_{\text{cal}},\]

where the last step follows from (Condition 1).

Since this holds for all \(\mathbf{h}^{\prime}\in P_{y^{\prime}}\), it implies \(\mathbf{h}_{\phi}\not\in B_{2}(P_{y^{\prime}},\varepsilon_{\text{cal}})\) whenever \(y^{\prime}\neq y\). Together with the fact that \(\mathbf{h}_{\phi}\in B_{2}(P_{y_{\phi}},\varepsilon_{\text{cal}})\), we must have \(y_{\phi}=y\) for all epochs \(\phi\in[\Phi]\). Therefore, ApproxMem always returns True.

Proof of (C2).: Suppose \(\mathbf{h}\notin B_{2}(P_{y}^{-\varepsilon_{2}},+\varepsilon_{1})\). We first analyze the probability of returning False for a fixed epoch \(\phi\in[\Phi]\). Let \(\partial S\) be the boundary of a convex set \(S\), then by triangle inequality, we have

\[d(\mathbf{h},\partial B_{2}(P_{y},\varepsilon_{\text{cal}}))\leq d(\mathbf{h}, \partial P_{y})+d(\partial P_{y},\partial B_{2}(P_{y},\varepsilon_{\text{cal}})).\]

For the first term, we have \(d(\mathbf{h},\partial P_{y})\leq\varepsilon_{2}\) since

\[\mathbf{h}\notin B_{2}(P_{y}^{-\varepsilon_{2}},+\varepsilon_{1})\Rightarrow h \notin P_{y}^{-\varepsilon_{2}}=B_{2}(P_{y},-\varepsilon_{2})\Rightarrow d( \mathbf{h},P_{y})\leq\varepsilon_{2}.\]

For the second term, we have

\[d(\partial P_{y},\partial B_{2}(P_{y},\varepsilon_{\text{cal}}))\leq\varepsilon_{ \text{cal}},\]

because \(P_{y}\) is a convex set. Combining the two bounds, we know that \(d(\mathbf{h},\partial B_{2}(P_{y},\varepsilon_{\text{cal}}))\leq\varepsilon_{ \text{cal}}+\varepsilon_{2}\).

Since \(\mathbf{h}_{\phi}\) is uniformly sampled from the sphere of radius \(R\) around \(\mathbf{h}\), by convexity of \(B_{2}(P_{y},\varepsilon_{\text{cal}})\) and the rotation invariance property of a unit sphere, we have

\[\Pr[\mathbf{h}_{\phi}\notin B_{2}(P_{y},\varepsilon_{\text{cal}})]\geq\Pr[ \langle R\mathbf{S}_{\phi},\mathbf{v}\rangle\geq\varepsilon_{\text{cal}}+ \varepsilon_{2}]=\Pr\left[\langle\mathbf{S}_{\phi},\mathbf{e}_{1}\rangle\geq \nicefrac{{\varepsilon_{\text{cal}}+\varepsilon_{2}}}{{R}}\right],\]where \(\mathbf{v}\) is a unit vector pointing in the direction of (one of) the projection from \(\mathbf{h}\) to \(\partial B_{2}(P_{y},\varepsilon_{\text{cal}})\), and \(\mathbf{e}_{1}=(1,0,\cdots,0)\in\mathbb{R}^{m}\). According to [23, Lemma 9], we can further lower bound the probability by

\[\Pr[\mathbf{h}_{\phi}\notin B_{2}(P_{y},\varepsilon_{\text{cal}})]\geq\frac{1} {2\sqrt{m}}\left(1-\left(\frac{\varepsilon_{\text{cal}}+\varepsilon_{2}}{R} \right)^{2}\right)^{\frac{m-1}{2}}.\]

Finally, the probability that no epoch returns False (failure of ApproxMem) is at most

\[\left(1-\delta^{\prime}\right)^{\Phi}\leq\varepsilon_{3}-\delta,\text{ where }\delta^{\prime}=\frac{1}{2\sqrt{m}}\left(1-\left(\frac{\varepsilon_{\text{cal}}+ \varepsilon_{2}}{R}\right)^{2}\right)^{\frac{m-1}{2}}.\]

Since \(\delta^{\prime}\ll 1\), the above inequality holds as long as

\[\Phi\geq\frac{2}{\delta^{\prime}}\log\left(\frac{1}{\varepsilon_{3}-\delta} \right)=4\sqrt{m}\left(1-\left(\frac{\varepsilon_{\text{cal}}+\varepsilon_{2} }{R}\right)^{2}\right)^{\frac{m-1}{2}}\log\left(\frac{1}{\varepsilon_{3}- \delta}\right),\]

which follows from (Condition 2). Therefore, \(\mathbf{h}\notin B_{2}(P_{y}^{-\varepsilon_{2}},+\varepsilon_{1})\), then ApproxMem returns False with probability \(\geq 1-(\varepsilon_{3}-\delta)\).

### Complexity of the explore algorithm

**Lemma C.4** (Sample complexity of Algorithm 2).: _Using Algorithm 2, the principal can find with probability at least \(1-(\delta+\varepsilon_{\text{opt}}+T^{-1})\), a strategy \(\tilde{\mathbf{h}}\in\mathcal{H}_{P}\) that satisfies the two properties in Lemma C.5. We refer the reader to Theorems C.8 and C.13 for the sample complexity under two different instantiations of Algorithm 2._

**Lemma C.5** (Optimality and robustness of output policy).: _Under the conditions of Lemma C.4, the output \(\tilde{\mathbf{h}}\) of Algorithm 2 satisfies the following two properties:_

1. \(U_{P}(\tilde{\mathbf{h}},y^{\star})\geq V^{\star}-\varepsilon_{\text{opt}}\)_, i.e.,_ \(\tilde{\mathbf{h}}\) _is an approximate Stackelberg equilibrium._
2. \(\tilde{\mathbf{h}}\in B_{2}(P_{y^{\star}},-\varepsilon_{\text{robust}})\)_, i.e.,_ \(\tilde{\mathbf{h}}\) _lies_ robustly _within the best-response polytope for_ \(y^{\star}\)_._

_We refer the reader to Theorems C.8 and C.13 for the exact values of \(\varepsilon_{\text{robust}},\varepsilon_{\text{opt}}\) under two different instantiations of Algorithm 2._

Before we formally state the proof of Lemmas C.4 and C.5, we first show the sample complexity guarantee of the exploration phase of Algorithm 2 (Lemma C.6), then state a useful lemma from prior work on convex optimization from membership queries [42] (Lemma C.7).

**Lemma C.6** (Initialization set of Algorithm 2).: _Let \(V\) be the volume of an \(\ell_{2}\) ball of radius \(\frac{\eta}{2}\) in \(\mathbb{R}^{m}\). Suppose the principal samples \(\mathbf{h}\) uniformly from \(\mathcal{H}_{P}\) for \(O(V^{-1}\log T)\) times and calls ApproxMem\((\varepsilon_{1}=\varepsilon_{2}=\eta/4,\;\varepsilon_{3}=\delta+T^{-2}/2)\) for the membership of each of them, then with probability at least \(1-\delta-T^{-1}\), the initialization set \(\mathcal{I}\) contains \((\mathbf{h}_{0},y_{0})\) where \(y_{0}=y^{\star}\) is the optimal target, and \(\mathbf{h}_{0}\) is \(\frac{\eta}{2}\)-centered in \(P_{y^{\star}}\), i.e., \(\mathbf{h}_{0}\in B_{2}(P_{y^{\star}},-\frac{\eta}{2})\)._

_The total number of samples required for the initialization phase is \(O\left(V^{-1}1.25^{m}r_{\delta}^{-1}(\frac{\eta}{k\sqrt{m}})\log^{2}T\right)\)._

Proof of Lemma C.6.: By regularity assumption, there exists \(\dot{\mathbf{h}}\in P_{y^{\star}}\), s.t. \(B_{2}(\dot{\mathbf{h}},\eta)\in P_{y^{\star}}\). Therefore, \(\forall\mathbf{h}^{\prime}\in B_{2}(\dot{\mathbf{h}},\frac{\eta}{2})\), we have \(\dot{\mathbf{h}}\in B_{2}(P_{y^{\star}},-\frac{\eta}{2})\). Moreover, since \(\mathbf{h}^{\prime}\) lies robustly inside \(P_{y^{\star}}\), on the success event of ApproxMem\((\varepsilon_{1}=\varepsilon_{2}=\frac{\eta}{4})\), ApproxMem\((\mathbf{h}^{\prime})\) must return membership \(y^{\star}\), otherwise we must have \(\mathbf{h}^{\prime}\notin B_{2}(P_{y^{\star}}^{-\frac{\eta}{4}},-\frac{\eta}{4})=B_ {2}(P_{y^{\star}},-\frac{\eta}{2})\), a contradiction. Since the set of all such \(\mathbf{h}^{\prime}\) takes up nontrivial volume \(V\) in \(\mathcal{H}_{P}\), we know that \(O(V^{-1}\log T)\) uniform samples are guaranteed to hit one with probability \(1-\frac{\eta}{(2T)}\).

Now we upper bound the failure probability of the initialization phase. Since the agent is miscalibrated with probability \(\delta\), random sampling fails to discover centered \(\mathbf{h}^{\prime}\) with probability \(\frac{1}{(2T)}\), and the probability that one of the answers from ApproxMem is wrong with probability \(\frac{1}{(2T)}\), the total failure probability is \(1-\delta-T^{-1}\) as desired.

As for the sample complexity, note that according to Lemma C.2 and proposition C.3, each call to ApproxMem takes \(O\left(1.25^{m}r_{\delta}^{-1}(\frac{\eta}{k\sqrt{m}})\log T\right)\) samples, and the initialization phase calls ApproxMem for \(O(V^{-1}\log T)\) times, the total sample complexity is \(O\left(V^{-1}1.25^{m}r_{\delta}^{-1}(\frac{\eta}{k\sqrt{m}})\log^{2}T\right)\). 

**Lemma C.7** (LSV performance guarantee, Theorems 14 and 15 in [42]).: _Let \(K\) be a convex set specified by a membership oracle, a point \(x_{0}\in\mathbb{R}^{n}\), and \(\eta>0\) such that \(B_{2}(x_{0},\nicefrac{{\eta}}{{2}})\subseteq K\subseteq B_{2}(x_{0},1)\). There exists a universal constant \(\gamma_{0}>1\) such that for any convex function \(f\) given by an evaluation oracle and any \(\varepsilon^{\prime}>0\), there is a randomized algorithm that computes a point \(z\in B_{2}(K,\varepsilon^{\prime})\) such that with probability \(\geq 1-\varepsilon^{\prime}\),_

\[f(z)\leq\min_{x\in K}f(x)+\varepsilon^{\prime}\cdot\left(\max_{x\in K}f(x)- \min_{x\in K}f(x)\right),\]

_with constant probability using \(O\left(n^{2}\log^{O(1)}(\nicefrac{{\eta}}{{\varepsilon^{\prime}\eta}})\right)\) calls to the \(\varepsilon_{1}\)-approximate membership oracle and evaluation oracle, where \(\varepsilon_{1}=\nicefrac{{(\varepsilon^{\prime}\eta/n)}}{{n}}^{\gamma_{0}}\)._

Note that in Lemma C.7, the required accuracy (\(\varepsilon_{1}\)) of the approximate membership oracle is orders of magnitudes smaller than the suboptimality (\(\varepsilon\)) of the output solution, i.e., \(\varepsilon_{1}\ll\varepsilon\). Let \(\tilde{\mathbf{h}}\) denote the \(\varepsilon\)-optimal solution returned by LSV. Since we use ApproxMem which is an \(\varepsilon_{1}\)-approximate membership oracle to \(K=P_{y}^{-\varepsilon_{2}}\), \(\tilde{\mathbf{h}}\) will lie in \(B_{2}(K,\varepsilon)=B_{2}(P_{y}^{-\varepsilon_{2}},\varepsilon)=B_{2}(P_{y}, -\varepsilon_{2}+\varepsilon)\). In order to find a near-optimal solution that lies robustly within \(P_{y}\), we can either simulate ApproxMem with more epochs to guarantee \(\varepsilon_{1}\ll\varepsilon\approx\varepsilon_{2}\) (see Appendix C.4 for more details), or perform a post-process to \(\tilde{\mathbf{h}}\) that pushes it further inside the polytope (see Appendix C.5 for more details).

### More epochs to guarantee robustness

In this section, we show how to use more epochs in the simulation of each ApproxMem oracle to obtain higher accuracy feedbacks. This approach directly guarantees that the output of LSV lies robustly within the polytope.

**Theorem C.8**.: _If the principal uses Algorithm 2 with parameters ApproxMem\((\varepsilon_{1}=(\frac{\varepsilon^{\prime}\eta}{n})^{\gamma_{0}},\varepsilon_{2}=2 \varepsilon^{\prime},\varepsilon_{3}=\delta+T^{-2})\) and LSV\((\varepsilon^{\prime})\), then with probability at least \(1-\delta-T^{-1}-\varepsilon^{\prime}\), the final solution \(\tilde{\mathbf{h}}\) satisfies the following two properties:_

1. \(U_{P}(\tilde{\mathbf{h}},y^{\star})\geq V^{\star}-3\varepsilon^{\prime}\)_, i.e.,_ \(\tilde{\mathbf{h}}\) _is an approximate Stackelberg equilibrium._
2. \(\tilde{\mathbf{h}}\in B_{2}(P_{y^{\star}},-\varepsilon^{\prime})\)_, i.e.,_ \(\tilde{\mathbf{h}}\) _lies_ robustly _within the best-response polytope for_ \(y^{\star}\)_._

_The total number of samples needed is \(\tilde{O}\left(V^{-1}m^{\frac{5+m\gamma_{0}}{2}}\left(\eta\varepsilon^{\prime} \right)^{-\frac{m\gamma_{0}}{2}}r_{\delta}^{-1}(\frac{\varepsilon^{\prime}\eta \gamma_{0}}{km^{\gamma_{0}}\sqrt{m}})\right)\)._

Proof.: We first analyze the failure probability of the algorithm. Since the agent's adaptive calibration error is uniformly bounded by \(r_{\delta}(\cdot)\) with probability \(1-\delta\), the probability that there exists an incorrect answer from ApproxMem is bounded by \(T\cdot T^{-2}=T^{-1}\) conditioned on agent's calibration error being bounded, and the probability that LSV returns a bad solution is bounded by \(\varepsilon^{\prime}\), the total failure probability of the described algorithm is at most \(\delta+T^{-1}+\varepsilon^{\prime}\).

Now we consider the optimization problem for the optimal polytope \(P_{y^{\star}}\), with initial point \(\mathbf{h}_{0}\) that is \(\frac{\eta}{2}\)-centered in \(P_{y^{\star}}\). There exists such a pair \((\mathbf{h}_{0},y^{\star})\in\mathcal{I}\) according to Lemma C.6. Let \(\tilde{\mathbf{h}}\) be the solution output by LSV. According to Lemma C.7, \(\tilde{\mathbf{h}}\in B_{2}(P_{y^{\star}}^{-\varepsilon_{2}},+\varepsilon^{ \prime})\subseteq B_{2}(P_{y^{\star}},-\varepsilon^{\prime})\), which proves **(P2)**. For **(P1)**, note that Lemma C.7 also guarantees

\[V^{\star}-U_{P}(\tilde{\mathbf{h}},y^{\star})\leq \varepsilon^{\prime}+\left(V^{\star}-\max_{\mathbf{h}\in P_{y^{ \star}}^{-\varepsilon_{2}}}U_{P}(\mathbf{h},y^{\star})\right)\leq\varepsilon^{ \prime}+2\varepsilon^{\prime}=3\varepsilon^{\prime}.\]Finally, we compute the total number of samples. By Lemma C.6, the initialization epoch takes \(O\left(V^{-1}1.25^{m}r_{\delta}^{-1}(\frac{\eta}{k\sqrt{m}})\log^{2}T\right)\) samples. According to Lemma C.2 and proposition C.3, each call to the ApproxMem oracle requires \(O\left(\sqrt{m}\left(\frac{\varepsilon_{2}}{\varepsilon_{1}}\right)^{\frac{m}{ 2}}r_{\delta}^{-1}(\frac{\varepsilon_{1}}{k\sqrt{m}})\log(T)\right)=O\left( \sqrt{m}\left(\frac{\eta}{m^{\varepsilon^{\prime}}}\right)^{\frac{m_{0}}{2}}r_ {\delta}^{-1}(\frac{(\varepsilon^{\prime}\eta)^{\gamma_{0}}}{km^{\gamma_{0}} \sqrt{m}})\log(T)\right)\) samples, and Lemma C.7 suggests that there LSV makes \(O\left(m^{2}\log^{O(1)}(\frac{m}{\varepsilon^{\prime}\eta})\right)\) oracle calls, where we run \(O(V^{-1}\log T)\) LSV instances for each initial point in the initialization set. Putting together, the total number of samples is

\[\tilde{O}\left(V^{-1}m^{\frac{5+m\gamma_{0}}{2}}\left(\eta\varepsilon^{\prime }\right)^{-\frac{m\gamma_{0}}{2}}r_{\delta}^{-1}(\frac{(\varepsilon^{\prime} \eta)^{\gamma_{0}}}{km^{\gamma_{0}}\sqrt{m}})\right),\]

where \(\tilde{O}\) hides logarithm terms in \(T,m,1/\eta,1/\varepsilon^{\prime}\). 

### Representation length and post-processing

In this section, we show a post-processing of the output of LSV [42] under the representation length assumption.

**Assumption C.9** (Utility Representation Length).: _Suppose the utility functions \(U_{P}\) and \(U_{A}\) are rational with denominator at most \(a\), and normalized to be \([0,1]\). Therefore, the game's utility representation length is \(L=2mn\log a\)._

The next lemma shows that under Assumption C.9, the optimal solution \(\mathbf{h}^{\star}\) also has a finite representation length.

**Lemma C.10**.: _For any \(y\in\mathcal{A}_{A}\), let \(\mathbf{h}_{y}^{\star}\in\mathbb{R}^{m}\) be the principal's optimal strategy in \(P_{y}\) that achieves \(\max_{\mathbf{h}^{\star}\in P_{y}}U_{P}(\mathbf{h}^{\star},y)\). Suppose the utility functions \(U_{P},U_{A}\) satisfy Assumption C.9. Then for all \(i\in\mathcal{A}_{P}\), the \(i\)-th coordinate of \(\mathbf{h}_{y}^{\star}\) is a rational number with denominator at most \(2^{2mL}\)._

Proof.: The proof of this lemma is similar to [12, Lemma 10], which follows from well-known results in linear programming. We first note that \(\mathbf{h}^{\star}\) is the solution of the linear programming:

\[\max_{\mathbf{h}}U_{P}(\mathbf{h},y),\quad\text{subject to }\mathbf{h}^{\star} \in P_{y},\]

where \(U_{P}(\mathbf{h},y)\) is linear in \(\mathbf{h}\) and the best response polytope \(P_{y}\) can be represented as a system \(\{\mathbf{h}:A\mathbf{h}^{T}\succeq\mathbf{b}\}\), where the set of constraints are the ones that define probability simplex \(\mathcal{H}_{P}\), together with the constraints of the form \(U_{P}(\mathbf{h},y)-U_{P}(\mathbf{h},y^{\prime})\geq 0,\;\forall y^{\prime}\neq y\). Suppose \(A\) is normalized so that each entry is an integer. By Assumption C.9, each coefficient is at most \(2^{L}\). Since the solution to the above LP is the intersection of \(m\) independent constraints of \(A\) (denoted with \(D\)), we have \(\mathbf{h}_{y,i}^{\star}=\frac{\det(D_{i})}{\det(D)}\) by Cramer's rule, where \(D_{i}\) is \(D\) with the \(i\)-th column replaced by \(\mathbf{b}\). According to Hardamard's inequality,

\[\det(D)\leq\prod_{i\in[m]}\sqrt{\prod_{j\in[m]}d_{ij}^{2}}\leq\prod_{i\in[m]} \sqrt{m}2^{L}=m^{\frac{m}{2}}2^{Lm}\leq 2^{2Lm}.\]

We also make the following assumption on the optimal strategy \(\mathbf{h}^{\star}\): any near-optimal strategy \(\tilde{\mathbf{h}}\) must lie in the neighborhood of \(\mathbf{h}^{\star}\).

**Assumption C.11** (Near-optimal strategies).: _There exists a constant c.s.t. for all \(\varepsilon\leq 2^{-4mL}\) and all strategy \(\tilde{\mathbf{h}}\) such that \(U_{P}(\tilde{\mathbf{h}}.y^{\star})\leq V^{\star}-\varepsilon\), we have \(\|\tilde{\mathbf{h}}-\mathbf{h}^{\star}\|_{\infty}\leq c2^{L}\varepsilon\)._

Now we are ready to define the post-processing algorithm.

**Lemma C.12** (Post-processing).: _Suppose that Assumptions C.9 and C.11 holds. Assume \(\varepsilon\leq 2^{-4mL}\), \(\tilde{\mathbf{h}}\) is an \(\varepsilon\)-approximate optimal strategy in \(P_{\tilde{y}}\), and \(\mathbf{h}_{0}\) is \(\nicefrac{{\eta}}{{2}}\)-centered in \(P_{\tilde{y}}\), i.e., \(B_{2}(\mathbf{h}_{0},\nicefrac{{\eta}}{{2}})\subseteq P_{\tilde{y}}\), then using no more than \(O\left(2^{2m}r_{\delta}^{-1}(\frac{\lambda}{k\sqrt{m}})\log T\right)\) rounds of interactions with the agent, PostProcess (Algorithm 5) outputs a strategy \(\mathbf{h}\) that satisfies with probability \(\geq 1-\delta-T^{-1}\):_1. \(\mathbf{h}\) _lies robustly inside_ \(P_{\hat{y}}\)_:_ \(\mathbf{h}\in B_{2}(P_{\hat{y}},-\lambda)\)_;_
2. \(\mathbf{h}\) _is close to_ \(\tilde{\mathbf{h}}\)_:_ \(\|\mathbf{h}-\tilde{\mathbf{h}}\|_{2}\leq\sqrt{m}\left(c2^{L}\varepsilon+\frac{4 \lambda}{\eta}\right)\)_._

Proof.: To prove the lemma, we make the following claim: For all \(\mathbf{h}_{1}\in\mathcal{H}_{P}\), if \(\mathbf{h}_{1}\in P_{\hat{y}}\) and \(\mathbf{h}_{2}=\left(1-\frac{2\lambda}{\eta}\right)\mathbf{h}_{1}+\frac{2 \lambda}{\eta}\mathbf{h}_{0}\), then we have \(\mathbf{h}_{2}\in B_{2}(P_{\hat{y}},-\lambda)\). In fact, for all unit vectors \(\mathbf{s}\in\mathbb{R}^{m}\), we have

\[\mathbf{h}_{2}+\lambda\mathbf{s}=\left(1-\frac{2\lambda}{\eta}\right)\mathbf{ h}_{1}+\frac{2\lambda}{\eta}\left(\mathbf{h}_{0}+\frac{\eta}{2}\mathbf{s} \right).\]

Since \(\mathbf{h}_{1}\in P_{\hat{y}}\) and \(\mathbf{h}_{0}+\frac{\eta}{2}\mathbf{s}\in B_{2}(\mathbf{h}_{0},\nicefrac{{ \eta}}{{2}})\subseteq P_{\hat{y}}\) from the assumption that \(\mathbf{h}_{0}\) is \(\nicefrac{{\eta}}{{2}}\)-centered, their convex combination must also lie in \(P_{\hat{y}}\) because \(P_{\hat{y}}\) is convex. Thus, we have \(\mathbf{h}_{2}+\lambda\mathbf{s}\in P_{\hat{y}}\). Since the above holds for all unit vectors \(\mathbf{s}\), we conclude that \(B_{2}(\mathbf{h}_{2},\lambda)\subseteq P_{\hat{y}}\). We have thus proved the claim.

With the above claim, we first show that when all oracle calls return correctly, \(S\) contains at least one point \(\mathbf{h}^{j}\) such that ApproxMem returns True on \(\mathbf{h}^{j}_{\text{query}}\), and the post-processed version of this strategy satisfies (i) and (ii).

Since \(\tilde{\mathbf{h}}\) is an \(\varepsilon\)-optimal strategy, by Assumption C.11, we know that the true optimal \(\mathbf{h}^{\star}_{\hat{y}}\) satisfies \(\|\mathbf{h}^{\star}_{\hat{y}}-\tilde{\mathbf{h}}\|_{\infty}\leq c2^{L}\varepsilon\). According to Lemma C.10, every coordinate of \(\mathbf{h}^{\star}_{\hat{y}}\) is a rational number with denominator at most \(2^{2mL}\). Combining the two guarantees, we know that \(S\) must contains the true optimal \(\mathbf{h}^{\star}_{\hat{y}}\), which satisfies \(\mathbf{h}^{\star}_{\hat{y}}\in P_{\hat{y}}\). Suppose this point is \(\mathbf{h}^{j}\). According to the above claim, we have \(\mathbf{h}^{j}_{\text{query}}\in B_{2}(P_{\hat{y}},-\lambda)\). Since we simulated the ApproxMem oracle with \(\varepsilon_{1}=\varepsilon_{2}=\frac{\lambda}{2}\), it must return True on strategies that belong to \(B_{2}(P_{\hat{y}}^{-\varepsilon_{2}},-\varepsilon_{1})=B_{2}(P_{y},-\lambda)\), therefore, it must return True on \(\mathbf{h}^{j}_{\text{query}}\). This argument shows that PostProcess always return valid strategies.

However, it could be the case that ApproxMem returns True on strategies \(h^{j}\) that are not \(\mathbf{h}^{\star}_{\hat{y}}\). We argue that the guarantee of PostProcess is unaffected by showing that if ApproxMem returns True on any \(\mathbf{h}^{j}_{\text{query}}\), then we must have \(\mathbf{h}^{j}_{\text{query}}\in B_{2}(P_{\hat{y}}^{-\varepsilon_{2}},+ \varepsilon_{1})\subseteq P_{\hat{y}}\) from the guarantee of ApproxMem (see Lemma C.2). Applying the above claim again, we have that \(\mathbf{h}\in B_{2}(P_{\hat{y}},-\lambda)\) for the returned strategy \(\mathbf{h}\), which proves (i).

As for (ii), note that

\[\|\mathbf{h}-\tilde{\mathbf{h}}\|_{2}\leq \|\tilde{\mathbf{h}}-\mathbf{h}^{j}\|_{2}+\|\mathbf{h}^{j}- \mathbf{h}^{j}_{\text{query}}\|_{2}+\|\mathbf{h}^{j}_{\text{query}}-\mathbf{h} \|_{2}\] \[\leq \sqrt{m}\|\tilde{\mathbf{h}}-\mathbf{h}^{j}\|_{\infty}+\frac{2 \lambda}{\eta}\left(\|\mathbf{h}^{j}-\mathbf{h}_{0}\|_{2}+\|\mathbf{h}^{j}_{ \text{query}}-\mathbf{h}_{0}\|_{2}\right)\] \[\leq c\sqrt{m}2^{L}\varepsilon+\frac{4\lambda\sqrt{m}}{\eta}.\]

Finally, we analyze the failure probability and sample complexity of PostProcess. Since the agent's adaptive calibration error is uniformly bounded by \(r_{\delta}(\cdot)\) with probability \(1-\delta\), and the probability that there exists an incorrect answer from ApproxMem is bounded by \(T\cdot T^{-2}=T^{-1}\) conditioned on agent's calibration error being bounded, the failure probability of PostProcess is at most \(\delta+T^{-1}\).

For the sample complexity, since \(|S|\leq 2^{m}\) and each \(\mathbf{h}^{j}\in S\) requires simulating ApproxMem\({}_{\tilde{g}}\) with \(O\left(1.25^{m}r_{\delta}^{-1}(\frac{\lambda}{k\sqrt{m}})\log T\right)\) samples (see Lemma C.2 and proposition C.3), the total number of samples required is \(O\left(2^{2m}r_{\delta}^{-1}(\frac{\lambda}{k\sqrt{m}})\log T\right)\).

**Theorem C.13**.: _Suppose the utility functions satisfy Assumptions C.9 and C.11. If the principal uses Algorithm 2 with parameters \(\varepsilon_{1}=\sqrt{m}\varepsilon_{2}=\left(\frac{\eta}{m2^{4mL}}\right)^{ \gamma_{0}},\varepsilon_{3}=\delta+T^{-2}\) for the ApproxMem oracle and \(\varepsilon^{\prime}=2^{-4mL-1}\) for \(\mathrm{LSV}\), then feed the output of \(\mathrm{LSV}\) into PostProcess\((\lambda)\), then with probability at least \(1-\delta-T^{-1}-\varepsilon^{\prime}\) the final solution satisfies the following two properties:_

1. \(U_{P}(\tilde{\mathbf{h}},y^{\star})\geq V^{\star}-\frac{5m\lambda}{\eta}\)_, i.e.,_ \(\tilde{\mathbf{h}}\) _is an approximate Stackelberg equilibrium._
2. \(\tilde{\mathbf{h}}\in B_{2}(P_{y^{\star}},-\lambda)\)_, i.e.,_ \(\tilde{\mathbf{h}}\) _lies_ robustly _within the best-response polytope for_ \(y^{\star}\)_._

_The total number of samples needed is expressed in Equation (14)._

Proof.: The analysis for the failure probability is the same with Theorem C.8.

Now we consider the optimization problem for the optimal polytope \(P_{y^{\star}}\), with initial point \(\mathbf{h}_{0}\) that is \(\frac{\eta}{2}\)-centered in \(P_{y^{\star}}\). Let \(\hat{\mathbf{h}}\) be the solution output by \(\mathrm{LSV}\), and let \(\hat{\mathbf{h}}\) be the solution output by running PostProcess on \(\hat{\mathbf{h}}\). According to Lemma C.7, \(\hat{\mathbf{h}}\) satisfies \(\hat{\mathbf{h}}\in B_{2}(P_{y^{\star}}^{-\varepsilon_{2}},+\varepsilon^{ \prime})\subseteq B_{2}(P_{y^{\star}},\varepsilon^{\prime})\), and the suboptimality of \(\hat{\mathbf{h}}\) is at most \(O(\varepsilon^{\prime}+\varepsilon_{2})\leq 2^{-4mL}\). Since \(\varepsilon^{\prime}\leq 2^{-4mL}\), by Lemma C.12, \(\tilde{\mathbf{h}}\in B_{2}(P_{y^{\star}},-\lambda)\), which proves **(P2)**.

To prove **(P1)**, note that since \(\|\hat{\mathbf{h}}-\tilde{\mathbf{h}}\|_{2}\leq\sqrt{m}\left(c2^{L} \varepsilon^{\prime}+\frac{4\lambda}{\eta}\right)\) by Lemma C.12, we have

\[V^{\star}-U_{P}(\tilde{\mathbf{h}},y^{\star})\leq \left(V^{\star}-U_{P}(\hat{\mathbf{h}},y^{\star})\right)+\left(U_{ P}(\hat{\mathbf{h}},y^{\star})-U_{P}(\tilde{\mathbf{h}},y^{\star})\right)\] \[\leq 2^{-4mL}+m\left(c2^{L}\varepsilon^{\prime}+\frac{4\lambda}{\eta }\right)\leq 2^{-3mL}+\frac{4m\lambda}{\eta}\leq\frac{5m\lambda}{\eta}.\]

Finally, we compute the total number of samples. According to Lemma C.2 and proposition C.3, each call to the ApproxMem oracle requires \(O\left(\sqrt{m}r_{\delta}^{-1}(\frac{\varepsilon_{2}}{k\sqrt{m}})\log T \right)=O\left(\sqrt{m}r_{\delta}^{-1}(\frac{1}{km}\left(\frac{\eta}{m2^{4mL}} \right)^{\gamma_{0}})\log T\right)\) samples, and Lemma C.7 suggests that \(\mathrm{LSV}\) makes \(O\left(m^{2}\log^{O(1)}(\frac{m}{\varepsilon^{\prime}\eta})\right)=O\left(m^{ 3}L\log^{O(1)}(\frac{m}{\eta})\right)\) oracle calls, where we run \(O(V^{-1}\log T)\)\(\mathrm{LSV}\) instances for each initial point in the initialization set. In addition, PostProcess takes \(O\left(2^{2m}r_{\delta}^{-1}(\frac{\lambda}{k\sqrt{m}})\log T\right)\) samples (Lemma C.12), and initialization takes \(O\left(V^{-1}1.25^{m}r_{\delta}^{-1}(\frac{\eta}{k\sqrt{m}})\log^{2}T\right)\) samples (Lemma C.6). Adding them all up, the total number of samples is

\[\tilde{O}\left(V^{-1}1.25^{m}r_{\delta}^{-1}(\frac{\eta}{k\sqrt{m}})+V^{-1}m^{ 3.5}Lr_{\delta}^{-1}(\frac{1}{km}\left(\frac{\eta}{m2^{4mL}}\right)^{\gamma_{ 0}})+2^{2m}r_{\delta}^{-1}(\frac{\lambda}{k\sqrt{m}})\right).\] (14)

where \(\tilde{O}\) hildes logarithm factors in \(T,m,\eta^{-1}\). 

### Proof of Theorem 3.2

In this section, we present the main theorem (Theorem 3.2) in Section 3. We first formally restate the theorem with exact convergence rate.

**Theorem C.14** (Formal version of Theorem 3.2).: _There exists an algorithm for the principal in CSGs that achieves average utility with probability \(1-o(1)\):_

\[\lim_{T\to\infty}\frac{1}{T}\sum_{t\in[T]}U_{P}(\mathbf{h}_{t},y_{t})\geq V^{ \star}.\] (15)

_Specifically, for calibrated agents with calibration error rate \(r_{\delta}(t)=O(t^{-1/\beta})\),_

1. _If the principal runs the_ Explore_-Then-Commit _algorithm (Algorithm_ 1_) where the explore phase follows the parameter settings in Theorem_ C.8 _with_ \(\varepsilon^{\prime}=O\left(T^{-\frac{1}{\gamma_{1}}}C_{1}^{\frac{1}{\gamma_{1 }}}\eta^{-\frac{\gamma_{1}-1}{\gamma_{1}}}\right)\)_, then the limit in (_15_) approaches_ \(V^{\star}\) _with rate_ \[\tilde{O}\left(T^{-\frac{1}{\gamma_{1}}}C_{1}^{\frac{1}{\gamma_{1}}}\eta^{- \frac{\gamma_{1}-1}{\gamma_{1}}}\right),\] _where_ \[C_{1}\triangleq V^{-1}m^{\frac{5+m\gamma_{0}+\beta(2\gamma_{0}+1)}{2}}k^{ \beta},\quad\gamma_{1}\triangleq\gamma_{0}\left(\frac{m}{2}+\beta\right)+1.\]
2. _If the principal runs the_ Explore_-Then-Commit _algorithm (Algorithm_ 1_) where the explore phase follows the parameter settings in Theorem_ C.13 _with_ \(\lambda=O\left(\left(\frac{\eta C_{2}}{m}\right)^{\frac{1}{\gamma_{2}}}T^{- \frac{1}{\gamma_{2}}}\right)\)_, then the limit in (_15_) approaches_ \(V^{\star}\) _with rate_ \[\tilde{O}\left(T^{-\frac{1}{\gamma_{2}}}\left(\frac{m}{\eta}\right)^{1-\frac{ 1}{\gamma_{2}}}C_{2}^{\frac{1}{\gamma_{2}}}\right),\] _where_ \[C_{2}\triangleq V^{-1}2^{5mL\beta\gamma_{0}}k^{\beta},\quad\gamma_{2} \triangleq\beta\gamma_{0}+1.\]

Proof.: According to Lemmas C.4 and C.5, we have that with probability \(1-o(1)\), Algorithm 2 (with potential post-processing) returns a policy \(\tilde{\mathbf{h}}\) within \(N\) samples, where \(\tilde{\mathbf{h}}\) satisfies

1. \(U_{P}(\tilde{\mathbf{h}},y^{\star})\geq V^{\star}-\varepsilon_{\text{opt}}\), i.e., \(\tilde{\mathbf{h}}\) is an approximate Stackelberg equilibrium.
2. \(\tilde{\mathbf{h}}\in B_{2}(P_{y^{\star}},-\varepsilon_{\text{robust}})\), i.e., \(\tilde{\mathbf{h}}\) lies _robustly_ within the best-response polytope for \(y^{\star}\).

With properties **(P1)**and **(P2)**, the arguments in Section 3 give us

\[V^{\star}T-\sum_{t\in[T]}U_{P}(\mathbf{h}_{t},y_{t})\lesssim N+\varepsilon_{ \text{opt}}T+\frac{k\sqrt{m}Tr_{\delta}(T)}{\varepsilon_{\text{robust}}}.\]

Now we plug in specific instantiations of Algorithm 1.

1. Suppose the principal follows the parameter settings in Theorem C.8. In this case, we have \(\varepsilon_{\text{opt}}=3\varepsilon^{\prime}\), \(\varepsilon_{\text{robust}}=\varepsilon^{\prime}\), and \[N= \tilde{O}\left(V^{-1}m^{\frac{5+m\gamma_{0}}{2}}(\eta\varepsilon^{ \prime})^{-\frac{m\gamma_{0}}{2}}r_{\delta}^{-1}(\frac{(\varepsilon^{\prime} \eta)^{\gamma_{0}}}{km^{\gamma_{0}}\sqrt{m}})\right)\] \[= \tilde{O}\left(V^{-1}m^{\frac{5+m\gamma_{0}+\beta(2\gamma_{0}+1)} {2}}k^{\beta}(\eta\varepsilon^{\prime})^{-\gamma_{0}(\frac{m}{2}+\beta)} \right).\] Finally, optimizing over \(\varepsilon^{\prime}\) gives the claimed bound.
2. Suppose the principal follows the parameter settings in Theorem C.13. In this case, we have \(\varepsilon_{\text{opt}}=\frac{5m\lambda}{\eta}\), \(\varepsilon_{\text{robust}}=\lambda\), and \[N= \tilde{O}\left(V^{-1}1.25^{m}r_{\delta}^{-1}(\frac{\eta}{k\sqrt{m }})+V^{-1}m^{3.5}Lr_{\delta}^{-1}(\frac{1}{km}\left(\frac{\eta}{m2^{4mL}} \right)^{\gamma_{0}})+2^{2m}r_{\delta}^{-1}(\frac{\lambda}{k\sqrt{m}})\right)\] \[= \tilde{O}\left(V^{-1}1.25^{m}(k\sqrt{m})^{\beta}\eta^{-\beta}+V^{ -1}2^{5mL\beta\gamma_{0}}k^{\beta}\lambda^{-\beta\gamma_{0}}\right).\] Finally, optimizing over \(\lambda\) gives the claimed bound.

**Remark C.15** (Representation length).: _If we assume the optimal solution \(\mathbf{h}^{\star}\) lies in the grid of size \(\text{poly}(\varepsilon^{-1})\), then in case (II) of Theorem C.14, we have \(2^{2mL}=\text{poly}(\varepsilon^{-1})\), which leads to a rate of \(\tilde{O}\left(\text{poly}(\varepsilon,k,m,\eta^{-1},V^{-1})\cdot T^{-\frac{1} {\gamma_{2}}}\right)\), where \(\gamma_{2}\) is a constant that only depends on the agent's calibration error rate \(\beta\). This is the result of replacing Lemma C.10 with the finite grid assumption._

**Remark C.16** (Adaptive regret versus calibration).: _Our primary focus lies on calibration due to its characterization of agents' beliefs and the fact that it provides both upper and lower bounds to the principal's utility. This is particularly useful for the learning direction, as denoted by the lower bounds in Theorems 3.2 and C.14. However, a different form of adaptive guarantee would suffice here: one concerning (external) regret. Nevertheless, we do not focus on regret as a characterization as it doesn't offer the same upper bound guarantees -- in fact, the principal could potentially extract more utility than \(V^{\star}\). Additionally, regret-based assumptions tend to overly emphasize the agent's optimization techniques rather than maintaining a consistent belief about the action being executed._

## Appendix D Supplementary Material for Section 4

### Background on Sleeping Experts and AdaNormalHedge

We start the exposition of this part by introducing the sleeping experts problem [10, 31]. For each expert \(i\in[N]\) and round \(t\in[T]\), let \(\ell_{t,i}\in[0,1]\) be the loss of expert \(i\), and let \(I_{t,i}\) be an indicator that takes value \(I_{t,i}=1\) if expert \(i\) is active at round \(t\) and \(I_{t,i}=0\) if asleep. The interaction protocol at each round \(t\) goes as follows: The indicators \(\left(I_{t,i}\right)_{i\in[N]}\) are revealed to the learner. The learner selects a probability distribution \(\pi_{t}\in\Delta([N])\) that is supported only on the set of active experts \(A_{t}\triangleq\{i:I_{t,i}=1\}\). The adversary selects a loss vector \(\left(\ell_{t,i}\right)_{i\in[N]}\). The learner then suffers expected loss \(\hat{\ell}_{t}=\mathbb{E}_{i\sim\pi_{t}}\left[\ell_{t,i}\right]\). The regret with respect to each expert \(i\) only accounts for the rounds when \(i\) is awake, which, together with the fact that \(\pi_{t}\) is only supported on active experts, implies that

\[\operatorname{Reg}_{T}(i)=\sum_{t\in[T]}I_{t,i}\left(\hat{\ell}_{t}-\ell_{t, i}\right)\quad\Rightarrow\quad\operatorname{Reg}_{T}=\max_{i}\operatorname{ Reg}_{T}(i)\] (16)

One of the algorithms that can be used to provide sublinear regret for the sleeping experts problem is AdaNormalHedge[44]. AdaNormalHedge is a powerful, parameter-free algorithm which provides regret bounds in terms of the cumulative magnitude of the _instantaneous_ regrets, defined as: \(r_{t,i}=\hat{\ell}_{t}-\ell_{t,i}\) for all experts \(i\in[N]\). As its name suggests, AdaNormalHedge uses the well-known algorithm Hedge as a backbone; Hedge maintains a probability distribution over experts at each round \(t\) and draws an expert from said distribution. After the expert's loss is revealed, the probability distribution for the next round \(t+1\) is updated using a multiplicative weights argument. For bandit feedback (i.e., when only the chosen expert's loss is revealed to the learner), the multiplicative weights update rule uses an inverse propensity scoring estimator for each expert's loss in place of their real loss. The new element that AdaNormalHedge brings to the table is a way of defining the weights at each round \(t\); specifically, the weights are updated proportionally to the sum of instantaneous regret for each expert until round \(t\). This allows the learner to obtain finer control over the total regret without needing extra parameters to tune the algorithm at each round. The exact regret guarantee that AdaNormalHedge obtains is stated formally below.

**Lemma D.1** (AdaNormalHedge [44]).: _Let \(r_{t,i}=I_{t,i}\left(\hat{\ell}_{t}-\ell_{t,i}\right)\) be the instantaneous regret of any active expert \(i\in A_{t}\) at round \(t\), and \(c_{t,i}=|r_{t,i}|\). Then, AdaNormalHedge with prior \(q\in\Delta([N])\) selects experts according to the following distribution_

\[\pi_{t,i} \propto q_{i}I_{t,i}w(R_{t-1,i},C_{t-1,i}),\;\text{where}\] \[R_{t-1,i} =\sum_{\tau\in[t-1]}r_{\tau,i},\;C_{t-1,i}=\sum_{\tau\in[t-1]}c_{ \tau,i},\] \[w(R,C) =\frac{1}{2}\left(\Phi(R+1,C+1)-\Phi(R-1,C+1)\right),\] \[\Phi(R,C) =\exp\left(\frac{\max\{0,R\}^{2}}{3C}\right)\]_The regret of AdaNormalHedge against any distribution over experts \(u\in\Delta([N])\) is bounded by_

\[\operatorname{Reg}_{T}(u)\leq O\left(\sqrt{\langle u,C_{T}\rangle\cdot(D_{\text{ KL}}(u\|q)+\log\log T+\log\log N))}\right).\]

_where by \(D_{KL}(u\|q)\) we denote the KL-divergence between distributions \(u\) and \(q\)._

AdaNormalHedge can be used to obtain adaptive regret bounds by creating a sleeping expert \((i,s)\) for each \(i\in[N],s\in[T]\) that has the same loss as expert \(i\) but is only awake after \(s\).

**Corollary D.2**.: _Running AdaNormalHedge for the sleeping expert setting with prior \(q_{(i,s)}\propto\frac{1}{s^{2}}\) gives regret_

\[\operatorname{Reg}_{t}((i,s))\leq O\left(\sqrt{(t-s)\left(\log(Ns)+\log\log T \right)}\right),\]

_where \(T_{i}=\sum_{t=1}^{T}I_{t,i}\) is the total number of rounds in which \(i\) is active._

### Formula for Computing \(Q_{t}\) when \(m=2\)

To obtain the explicit formula for \(Q_{t}\), we first discretize the space of forecasts \(\mathcal{F}_{P}=[0,1]\) (since we focus on the case where \(m=2\)) to form set \(\mathcal{F}_{P}^{e}=\{0,\varepsilon,2\varepsilon,\ldots,1-\varepsilon,1\}\). Then, we have that for each \(\hat{\mathbf{p}}\in\mathcal{F}_{P}^{e}\):

\[\operatorname*{\mathbb{E}}_{g\sim\pi_{t}}\left[\ell_{t,g}\right] =\operatorname*{\mathbb{E}}_{g\sim\pi_{t}}\left[L_{g}\left( \mathbf{h}_{t},\mathbf{p}\right)\right]=\sum_{g\in A_{t}(\mathcal{G})}\pi_{t, g}w_{i}(\mathbf{p})\sigma\left(\mathbf{h}_{t}-\mathbf{p}\right)\] \[=(\mathbf{h}_{t}-\mathbf{p})\underbrace{\sum_{i\in\mathcal{A}_{A }}w_{i}(\mathbf{p})\sum_{s\leq t}\left(\pi_{t,g(s,i,+1)}-\pi_{t,g(s,i,-1)} \right)}_{Z_{\mathbf{p}}}\] (17)

where we have omitted index \(j\) from the sleeping expert \(g\) since because \(m=2\), we can focus on a single action \(j\). We assume WLOG that no forecast \(\mathbf{p}\in\mathcal{F}_{P}^{e}\) falls exactly on the boundary of best response polytopes, so there is no tie-breaking needed. From Equation (17), we have that:

\[\max_{\mathbf{h}_{t}\in\mathcal{H}_{P}}\operatorname*{\mathbb{E}}_{g\sim\pi_{ t}}\left[\ell_{t,g}\right]=\max\{Z_{\mathbf{p}},0\}-\mathbf{p}Z_{\mathbf{p}}\] (18)

where the equation also uses the fact that for \(m=2\), \(\max_{\mathbf{h}_{t}\in\mathcal{H}_{P}}\mathbf{h}_{t}=1\).

In the final step, we map \(\mathbf{p}\) to the discretized grid of \(\mathcal{F}_{P}^{e}\). Let \(j\varepsilon,(j+1)\varepsilon\) be two adjacent discretized points and \(q\in[0,1]\) such that: \(qZ_{j\varepsilon}+(1-q)Z_{(j+1)\varepsilon}0\). Then, setting \(q_{t,j\varepsilon}=q\) and \(q_{t,(j+1)\varepsilon}=1-q\) and using Equation (18) gives that

\[\max_{\mathbf{h}_{t}\in\mathcal{H}_{P}}\operatorname*{\mathbb{E}}_{g\sim\pi_{ t}}\left[\ell_{t,g}\right]\leq\varepsilon.\]

### Proof of Theorem 4.1

Proof of Theorem 4.1.: We first specify how to build a set of sleeping experts settings from our problem definition. For that, consider the following set of experts:

\[\mathcal{G}=\left\{g_{(s,i,j,\sigma)}:s\in[T],i\in\mathcal{A}_{A},j\in \mathcal{A}_{P},\sigma\in\{\pm 1\}\right\},\]

i.e., we create a different expert for each round, each principal-agent action pair, and each \(\sigma\) (the use of which will be made clear in the next paragraph). For each expert \(g_{(s,i,j,(\sigma)}\in\mathcal{G}\) and \(t\in[T]\), we define the loss, sleeping/awake indicator, and instantaneous regret respectively as:

\[\ell_{t,g_{(s,i,j,\sigma)}}\triangleq L_{g_{(i,i,j,\sigma)}}(\mathbf{h}_{t},\mathbf{p}_{t})=w_{i}(\mathbf{p}_{t}) \cdot\sigma\cdot(h_{t,j}-p_{t,j})\,;\] \[I_{t,g_{(s,i,j,\sigma)}}\triangleq \mathbf{1}\{t\geq s\};\] \[r_{t,g}\triangleq I_{t,g}\cdot\left(\ell_{t,g}-\hat{\ell}_{t}\right).\]

where by \(h_{t,j},p_{t,j}\) we denote the \(j\)-th coordinate of the \(\mathbf{h}\) and \(\mathbf{p}\) vectors respectively.

Running AdaNormalHedge on the instance with \(\mathcal{G}\) that we specified above, with prior \(q_{g_{(s,i,j,\sigma)}}\propto\frac{1}{s^{2}}\)[44, Section 5.1] guarantees that \(\forall g_{(s,i,j,\sigma)}\in\mathcal{G}\),

\[\mathrm{Reg}_{\ell}\left(g_{(s,i,j,\sigma)}\right)=\sum_{\tau\in[t]}r_{t,g_{(s,i,\sigma)}}\leq\widetilde{O}\left(\sqrt{(t-s)\log(kmT)}\right).\] (19)

where \(\tilde{O}(\cdot)\) hides lower order poly-logarithmic terms. Therefore, by simulating the NRBR dynamics, we obtain that with probability at least \(\geq 1-\delta\), \(\forall i\in[k]\) and \(1\leq s<t\leq T\),

\[\mathrm{CalErr}_{i}(\mathbf{h}_{s:t},\mathbf{p}_{s:t})=\frac{1}{ t-s}\max_{j\in\mathcal{A}_{P}}\max_{\sigma\in\{\pm 1\}}\sum_{\tau=s}^{t}I_{\tau,g_{(s,i,j, \sigma)}}\cdot\ell_{\tau,g_{(s,i,j,\sigma)}}\] (Definition 2.3) \[=\frac{1}{t-s}\max_{j\in\mathcal{A}_{P}}\max_{\sigma\in\{\pm 1\}}\underbrace{\sum_{\tau=s}^{t}I_{ \tau,g_{(s,i,j,\sigma)}}\cdot\left(\ell_{\tau,g_{(s,i,j,\sigma)}}-\mathop{ \mathbb{E}}_{g\sim\tau_{\tau}}\left[\ell_{\tau,g}\right]\right)}_{\mathrm{Reg}_ {\ell}\left(g_{(s,i,j,\sigma)}\right)}\] \[\qquad\qquad\qquad+\frac{1}{t-s}\max_{j\in\mathcal{A}_{P}}\max_{ \sigma\in\{\pm 1\}}\sum_{\tau=s}^{t}I_{\tau,g_{(s,i,j,\sigma)}}\cdot\mathop{ \mathbb{E}}_{g\sim\tau_{\tau}}\left[\ell_{\tau,g}\right]\] \[=\frac{1}{t-s}\max_{j\in\mathcal{A}_{P}}\max_{\sigma\in\{\pm 1\}} \mathrm{Reg}_{t}\left(g_{(s,i,j,\sigma)}\right)+\frac{1}{t-s}\max_{j\in \mathcal{A}_{P}}\max_{\sigma\in\{\pm 1\}}\underbrace{\sum_{\tau=s}^{t}\mathop{ \mathbb{E}}_{g\sim\tau_{\tau}}\left[L_{g_{(s,i,j,\sigma)}}(\mathbf{h}_{\tau}, \mathbf{p}_{\tau})\right]}_{\mathcal{T}}\] (20)

where for the first derivation we add and subtract \(\sum_{\tau}\hat{\ell}_{\tau}\) and use that because of the NRBR dynamics: \(\hat{\ell}_{\tau}=\mathop{\mathbb{E}}_{g\sim P_{\tau}}\left[\ell_{\tau,g}\right]\), and for the last derivation we have used the definition of \(\ell_{\tau,g}=L_{g}(\mathbf{h}_{\tau},\mathbf{p}_{\tau})\) (Equation (6)). We next upper bound the term \(\mathcal{T}\) as follows:

\[\mathcal{T} =\sum_{\tau=s}^{t}\mathop{\mathbb{E}}_{\begin{subarray}{c}g\sim \tau_{\tau}\\ \mathbf{p}\sim Q_{\tau}\end{subarray}}\left[L_{g}(\mathbf{h}_{\tau},\mathbf{p })\right]+\sum_{\tau=s}^{t}\mathop{\mathbb{E}}_{g\sim\pi_{\tau}}\left[L_{g}( \mathbf{h}_{\tau},\mathbf{p}_{\tau})\right]-\sum_{\tau=s}^{t}\mathop{\mathbb{ E}}_{\begin{subarray}{c}g\sim\pi_{\tau}\\ \mathbf{p}\sim Q_{\tau}\end{subarray}}\left[L_{g}(\mathbf{h}_{\tau},\mathbf{p })\right]\] \[\leq\varepsilon\cdot(t-s)+\sqrt{(t-s)\log((t-s)/\delta)}\]

where the first inequality is by the property of \(\mathbf{h}_{\tau}\) being the best strategy for the principal, and the last one uses the fact that \(\max_{\mathbf{h}_{\tau}\in\Delta(\mathcal{A}_{P})}\mathop{\mathbb{E}}_{ \begin{subarray}{c}g\sim\pi_{\tau}\\ \mathbf{p}\sim Q_{\tau}\end{subarray}}\left[L_{g}(\mathbf{h}_{\tau},\mathbf{p })\right]\leq\varepsilon\) from the NRBR Equation (8) and a martingale concentration bound on the second term.

Plugging the upper bound for \(Q\) back to Equation (20) and using the regret bound for AdaNormalHedge (Equation (19)) we get:

\[\mathrm{CalErr}_{i}(\mathbf{h}_{s:t},\mathbf{p}_{s:t})\leq\widetilde{O}\left( \frac{\log(kmT)}{t-s}\right)+O\left(\frac{\log((t-s)/\delta)}{t-s}\right)+ \varepsilon\leq r_{t}(\delta)+\epsilon.\]

## Appendix E Supplementary Material for Section 5

### Proof of Theorem 5.2

**Theorem 5.2** (Restated).: _For continuous CSGs satisfying Assumption 5.1, for all \(\varepsilon_{0}>0\), there exists a finite binning \(\Pi_{0}\) such that if the agent is \((0,\Pi_{0})\) - adaptively calibrated and the principal runs an appropriately parametrized instance of LazyGDwoG (Algorithm 3) then:_

\[\lim_{\begin{subarray}{c}\mathbf{p}\rightarrow\infty\\ M\rightarrow\infty\end{subarray}}\frac{1}{\Phi M}\sum_{\phi\in[\Phi]}\sum_{i\in[M]}U _{P}(\mathbf{h}_{\phi},y_{\phi,i})\geq V^{\star}-\varepsilon_{0}.\]_Moreover, for any sequence of the principal's actions \(\mathbf{h}_{[1:T]}\), it holds that:_

\[\lim_{T\to\infty}\frac{1}{T}\sum_{t\in[T]}U_{P}(\mathbf{h}_{t},y_{t})\leq V^{ \star}+\varepsilon_{0}.\]

#### e.1.1 Proof of Lower Bound

Before delving into the proof of the lower bound, we first introduce some notations. Let \(C(\mathbf{h})\triangleq U_{P}(\mathbf{h},\mathtt{BR}(\mathbf{h}))\). Let \(V_{\delta}^{\star}\triangleq\max_{\mathbf{h}\in B_{2}(\mathcal{A}_{P},-\delta )}C(\mathbf{h})\) be the optimal utility restricted in the smaller strategy set \(B_{2}(\mathcal{A}_{P},-\delta)\). We use \(\overline{y}_{\phi}\triangleq\frac{1}{M}\sum_{s\in[M]}y_{\phi,s}\) to denote the average feedback that LazyGDwoG uses to update the strategies.

We first consider any fixed \(\varepsilon>0\). Combining the guarantees of Lemmas E.4 and E.5, we conclude that there exists a finite binning \(\Pi_{0}\) and \(M_{\varepsilon}<\infty\), such that if the agent is \((0,\Pi_{0})\)-adaptively calibrated, then \(\forall M\geq M_{\varepsilon}\), the following two inequalities are satisfied at the same time:

\[\sup_{\phi\in[\Phi]}\left\|\overline{y}_{\phi}-\mathtt{BR}( \mathbf{h}_{\phi})\right\|_{2}\leq\varepsilon\] (by Lemma E.4 ) (21) \[\sup_{\phi\in\Phi}\frac{1}{M}\sum_{s\in[M]}U_{P}(\mathbf{h}_{\phi },y_{\phi,s})\geq C(\mathbf{h}_{\phi})-\varepsilon;\] (by Lemma E.5 ) (22)

Set the parameters according to \(\gamma_{\phi}=\gamma_{0}m^{-\frac{1}{2}}\phi^{-\frac{3}{4}}\) and \(\delta_{\phi}\equiv\delta=\delta_{0}m^{\frac{1}{2}}\Phi^{-\frac{1}{4}}\) in Algorithm 3, then similar arguments to [51, Theorem 3.1] guarantee that

\[V_{\delta}^{\star}-\frac{1}{\Phi}\sum_{\phi\in[\Phi]}\mathbb{E} [C(\mathbf{h}_{\phi})]\leq \left(\frac{D_{P}^{2}}{2\gamma_{0}}+\frac{2W_{P}^{2}}{\delta_{0} ^{2}}\right)\sqrt{m}\Phi^{-\frac{1}{4}}+L_{\mathtt{BB}}D_{P}\frac{1}{\Phi}\sum _{\phi\in[\Phi]}\|\overline{y}_{\phi}-\mathtt{BR}(\mathbf{h}_{\phi})\|_{2}\] \[\overset{\text{(a)}}{\leq} \left(\frac{D_{P}^{2}}{2\gamma_{0}}+\frac{2W_{P}^{2}}{\delta_{0} ^{2}}\right)\sqrt{m}\Phi^{-\frac{1}{4}}+L_{\mathtt{BB}}D_{P}\cdot\varepsilon,\]

where (a) is from Equation (21).

Now we upper bound the difference between \(V^{\star}\) and \(V_{\delta}^{\star}=\max_{\mathbf{h}\in B_{2}(\mathcal{A}_{P},-\delta)}C( \mathbf{h})\), then we have

\[V^{\star}-V_{\delta}^{\star}\leq\max_{\mathbf{h}^{\star}\in\mathcal{A}_{P}}\min _{\mathbf{h}^{\prime}\in B_{2}(\mathcal{A}_{P},-\delta)}C(\mathbf{h}^{\star})-C (\mathbf{h}^{\prime})\leq L_{U}\max_{\mathbf{h}^{\star}\in\mathcal{A}_{P}}\min _{\mathbf{h}^{\prime}\in B_{2}(\mathcal{A}_{P},-\delta)}\|\mathbf{h}^{\star}- \mathbf{h}^{\prime}\|_{2}\leq L_{U}\delta,\]

where the second inequality follows from Assumption 5.1 that \(C(\mathbf{h})\) is \(L_{U}\)-Lipschitz.

The next step is to upper bound the difference between the actual average utility and \(\frac{1}{\Phi}\sum_{\phi\in[\Phi]}\mathbb{E}[C(\mathbf{h}_{\phi})]\). From Equation (22), we have

\[\frac{1}{\Phi}\sum_{\phi\in[\Phi]}\mathbb{E}[C(\mathbf{h}_{\phi})]-\frac{1}{ \Phi M}\sum_{\phi\in[\Phi]}\sum_{i\in[M]}U_{P}(\mathbf{h}_{\phi},y_{\phi,i}) \leq\varepsilon.\]

Finally, putting the above inequalities together, we obtain

\[V^{\star}-\frac{1}{\Phi M}\sum_{\phi\in[\Phi]}\sum_{i\in[M]}U_{P }(\mathbf{h}_{\phi},y_{\phi,i})\] \[\leq \left(V^{\star}-V_{\delta}^{\star}\right)+\left(V_{\delta}^{ \star}-\frac{1}{\Phi}\sum_{\phi\in[\Phi]}\mathbb{E}[C(\mathbf{h}_{\phi})] \right)+\left(\frac{1}{\Phi}\sum_{\phi\in[\Phi]}\mathbb{E}[C(\mathbf{h}_{\phi} )]-\frac{1}{\Phi M}\sum_{\phi\in[\Phi]}\sum_{i\in[M]}U_{P}(\mathbf{h}_{\phi},y_ {\phi,i})\right)\] \[\leq L_{U}\delta_{0}m^{\frac{1}{2}}\Phi^{-\frac{1}{4}}+\left(\frac{D_{P }^{2}}{2\gamma_{0}}+\frac{2W_{P}^{2}}{\delta_{0}^{2}}\right)\sqrt{m}\Phi^{- \frac{1}{4}}+L_{\mathtt{BB}}D_{P}\cdot\varepsilon+\varepsilon.\]

Taking the limit of \(\Phi\to\infty\), the above inequalities imply

\[\lim_{\begin{subarray}{c}\psi\to\infty\\ M\to\infty\end{subarray}}\frac{1}{\Phi M}\sum_{\phi\in[\Phi]}\sum_{i\in[M]}U_{P }(\mathbf{h}_{\phi},y_{\phi,i})\geq V^{\star}-\varepsilon\left(L_{\mathtt{BB }}D_{P}+1\right).\]

Since the above arguments hold for all \(\varepsilon>0\), taking \(\varepsilon=\frac{e_{0}}{L_{\mathtt{BB}}D_{P}+1}\) proves the theorem. \(\Box\)

#### e.1.2 Proof of Upper Bound

For a fixed \(\varepsilon>0\), let \(D_{\varepsilon}=\{x_{1},\cdots,x_{I}\}\) be an \(\varepsilon\)-grid of \(\mathcal{F}_{P}\) under \(\ell_{2}\) distance, and let \(\Pi_{0}\) be the continuous binning specified by Equation (26). We have:

\[\sum_{t\in[T]}U_{P}(\mathbf{h}_{t},y_{t}) =\sum_{i\in[I]}\sum_{t\in[T]}w_{i}(\mathbf{p}_{t})U_{P}(\mathbf{ h}_{t},\mathtt{BR}(\mathbf{p}_{t}))\] \[\overset{\mathrm{(a)}}{\leq}\sum_{i\in[I]}\sum_{t\in[T]}w_{i}( \mathbf{p}_{t})\left(U_{P}(\mathbf{h}_{t},\mathtt{BR}(x_{i}))+L_{2}\cdot L_{ \mathtt{BR}}\underbrace{\|\mathbf{p}_{t}-x_{i}\|_{2}}_{\leq 2\varepsilon}\right)\] \[\overset{\mathrm{(b)}}{\leq}\sum_{i\in[I]}\left(\sum_{t\in[T]}w_ {i}(\mathbf{p}_{t})\right)U_{P}\big{(}\frac{\sum_{t\in[T]}w_{i}(\mathbf{p}_{t })\mathbf{h}_{t}}{\sum_{t\in[T]}w_{i}(\mathbf{p}_{t})},\mathtt{BR}(x_{i}) \big{)}+2L_{2}L_{\mathtt{BR}}\varepsilon T\] \[\overset{\mathrm{(c)}}{\leq}\sum_{i\in[I]}n_{T}(i)U_{P}(\overline {\mathbf{h}}_{T}(i),\mathtt{BR}(x_{i}))+2L_{2}L_{\mathtt{BR}}\varepsilon T\] \[\overset{\mathrm{(d)}}{\leq}\sum_{i\in[I]}n_{T}(i)\Big{(}U_{P}( \overline{\mathbf{p}}_{T}(i),\mathtt{BR}(x_{i}))+L_{1}\left\|\overline{ \mathbf{p}}_{T}(i)-\overline{\mathbf{h}}_{T}(i)\right\|_{2}\Big{)}+2L_{2}L_{ \mathtt{BR}}\varepsilon T\] \[=\underbrace{\sum_{i\in[I]}n_{T}(i)U_{P}(\overline{\mathbf{p}}_{ T}(i),\mathtt{BR}(x_{i}))}_{(A)}+\underbrace{L_{1}\sum_{i\in[I]}n_{T}(i) \left\|\overline{\mathbf{p}}_{T}(i)-\overline{\mathbf{h}}_{T}(i)\right\|_{2}} _{(B)}+2L_{2}L_{\mathtt{BR}}\varepsilon T\] (23)

In the above inequalities that lead to (23), step (a) is because \(U_{P}\) is \(L_{2}\)-Lipschitz in the second argument and \(\mathtt{BR}(\cdot)\) is \(L_{\mathtt{BR}}\)-Lipschitz, and the fact that \(w_{i}(\mathbf{p}_{t})>0\) only when \(\|\mathbf{p}_{t}-x_{i}\|_{2}<2\varepsilon\). In step (b), we used Jensen's inequality because \(U_{P}\) in concave in the first argument. Step (c) follows from the definition of \(n_{T}(i)\) and \(\overline{\mathbf{h}}_{T}(i)\) in Definition 2.3. The last inequality (d) uses the fact that \(U_{P}\) is \(L_{1}\)-Lipschitz in the first argument to decompose \(U_{P}(\overline{\mathbf{p}}_{T}(i),\mathtt{BR}(x_{i}))\) into calibration error (i.e., term (B)) and \(U_{P}(\overline{\mathbf{p}}_{T}(i),\mathtt{BR}(x_{i}))\) where the strategy that the agent best responds to is close to the principal's strategy (i.e., term (A)).

We can further bound \((A)\) and \((B)\) in Equation (23) respectively as follows:

\[(A)\leq\sum_{i\in[I]}n_{T}(i)\left(U_{P}(x_{i},\mathtt{BR}(x_{i}))+L_{1}\|x_{ i}-\overline{\mathbf{p}}_{T}(i)\|_{2}\right)\leq V^{\star}T+L_{1}(2\varepsilon)T,\]

and

\[(B)\leq L_{1}T\sum_{i\in[I]}\mathrm{CalErr}_{i}(\mathbf{h}_{1:T},\mathbf{p}_ {1:T})\leq L_{1}|D_{\varepsilon}|r_{\delta}(T)T\quad\text{w.p. }\geq 1-\delta.\]

Therefore, putting the above bounds together, we obtain that with probability \(\geq 1-\delta\),

\[\frac{1}{T}\sum_{t\in[T]}U_{P}(\mathbf{h}_{t},y_{t})\leq V^{\star}+(L_{1}|D_{ \varepsilon}|)r_{\delta}(T)+2(L_{1}+L_{2}L_{\mathtt{BR}})\varepsilon.\]

Since the above derivation holds for any \(\varepsilon>0\), it suffices to take \(\varepsilon\) such that \(2(L_{1}+L_{2}L_{\mathtt{BR}})\varepsilon=\varepsilon_{0}\). Finally, since \(|D_{\varepsilon}|<\infty\) and \(r_{\delta}(T)=o(1)\), taking the limit of \(T\to\infty\) proves the upper bound:

\[\lim_{T\to\infty}\frac{1}{T}\sum_{t\in[T]}U_{P}(\mathbf{h}_{t},y_{t})\leq V^{ \star}+\varepsilon_{0}.\]

### Key lemma: asymptotically correct forecast

In this section, we state and prove the key lemma for establishing Theorem 5.2. Intuitively, this lemma states that for any strategy \(\mathbf{h}\in\mathcal{A}_{P}\), as long as the principal repeatedly plays \(\mathbf{h}\) for enough rounds, the fraction of times where the agent's forecast is close to \(\mathbf{h}\) will converge to \(1\).

**Lemma E.1**.: _For any \(\varepsilon_{0}>0\), there exists a finite binning \(\Pi_{0}\), such that if the principal repeatedly plays any \(\mathbf{h}\in\mathcal{A}_{P}\) for \(M\) rounds and the agent's forecasts \(\mathbf{p}_{1:M}\) are \((0,\Pi_{0})\)- adaptively calibrated, then:_

\[\lim_{M\to\infty}\quad\frac{1}{M}\Big{|}\{s\in[M]:\|\mathbf{p}_{s}- \mathbf{h}\|_{2}\geq\varepsilon_{0}\}\Big{|}=0\] (24)

_In particular, if the calibration error (defined in Definition 2.3) has rate \(r(\cdot)\in o(1)\) with respect to \(\Pi_{0}\), then_

\[\frac{1}{M}\Big{|}\{s\in[M]:\|\mathbf{p}_{s}-\mathbf{h}\|_{2}\geq \varepsilon_{0}\}\Big{|}\leq\frac{8\sqrt{m}|\Pi_{0}|^{2}}{\varepsilon_{0}}r(M).\] (25)

Proof of Lemma e.1.: We first describe the construction of \(\Pi_{0}\). For \(\varepsilon=\frac{1}{4}\varepsilon_{0}\), let \(D_{\varepsilon}=\{x_{1},\cdots,x_{I}\}\) be an \(\varepsilon\)-grid of \(\mathcal{F}_{P}\) under \(\ell_{2}\) distance, and \(\Lambda(\mathbf{p};x,R)\triangleq\left(R-\|\mathbf{p}-x\|_{2}\right)_{+}\) be the tent function with center \(x\) and radius \(R\). Consider the following binning

\[\Pi_{0}=\left\{w_{i}(\mathbf{p})\triangleq\frac{\Lambda(\mathbf{ p};x_{i},2\varepsilon)}{\sum_{j\in[I]}\Lambda(\mathbf{p};x_{j},2\varepsilon)}:x_{i} \in D_{\varepsilon}\right\}.\] (26)

Clearly, \(|\Pi_{0}|=I<\infty\) because the diameter of \(\mathcal{F}_{P}\) is bounded as stated in Theorem 5.1. We can also verify that \(\Pi_{0}\) satisfies \(\sum_{i\in[I]}w_{i}(\mathbf{p})=1\) for all \(\mathbf{p}\in\mathcal{F}_{P}\) because \(w_{i}(\mathbf{p})\) is defined as the normalized tent function.

Now we prove that \(\Pi_{0}\) satisfies the desired property. Since the agent is adaptively calibrated to \(\Pi_{0}\), we have that \(\forall i\in[I]\),

\[\frac{n_{[M]}(i)}{M}\left\|\bar{\mathbf{p}}_{[M]}(i)-\mathbf{h} \right\|_{2}\leq\sqrt{m}\lim_{M\to\infty}\frac{n_{[M]}(i)}{M}\left\|\bar{ \mathbf{p}}_{[M]}(i)-\mathbf{h}\right\|_{\infty}\leq\sqrt{m}r(M).\]

Now, for \(\delta=3\varepsilon=\frac{3}{4}\varepsilon_{0}\), let \(D_{\varepsilon}^{(\delta)}\subseteq D_{\varepsilon}\) be defined as

\[D_{\varepsilon}^{(\delta)}=\left\{x_{i}\in D_{\varepsilon}:\|x_ {i}-\mathbf{h}\|\geq\delta\right\}.\] (27)

Since \(|D_{\varepsilon}^{(\delta)}|\leq|D_{\varepsilon}|=I<\infty\), taking the sum of calibration error over bins in \(D_{\varepsilon}^{(\delta)}\), we obtain

\[\sum_{x_{i}\in D_{\varepsilon}^{(\delta)}}\frac{n_{[M]}(i)}{M} \left\|\bar{\mathbf{p}}_{[M]}(i)-\mathbf{h}\right\|_{2}=\frac{1}{M}\sum_{x_{i} \in D_{\varepsilon}^{(\delta)}}\left\|\sum_{s\in[M]}w_{i}(\mathbf{p}_{s})( \mathbf{h}-\mathbf{p}_{s})\right\|_{2}\leq\sqrt{m}Ir(M).\] (28)

We can further lower bound (28) and get:

\[\frac{1}{M}\sum_{x_{i}\in D_{\varepsilon}^{(\delta)}}\left\|\sum _{s\in[M]}w_{i}(\mathbf{p}_{s})(\mathbf{h}-\mathbf{p}_{s})\right\|_{2}=\frac{ 1}{M}\sum_{x_{i}\in D_{\varepsilon}^{(\delta)}}\left\|\sum_{s\in[M]}w_{i}( \mathbf{p}_{s})\Big{(}(\mathbf{h}-x_{i})+(x_{i}-\mathbf{p}_{s})\Big{)}\right\|_ {2}\] \[\overset{\mathrm{(a)}}{\geq}\frac{1}{M}\sum_{x_{i}\in D_{\varepsilon}^{( \delta)}}\left(\left\|\sum_{s\in[M]}w_{i}(\mathbf{p}_{s})(\mathbf{h}-x_{i}) \right\|_{2}-\left\|\sum_{s\in[M]}w_{i}(\mathbf{p}_{s})(x_{i}-\mathbf{p}_{s}) \right\|_{2}\right)\] \[\overset{\mathrm{(b)}}{\geq}\frac{1}{M}\sum_{x_{i}\in D_{ \varepsilon}^{(\delta)}}\sum_{s\in[M]}w_{i}(\mathbf{p}_{s})\Big{(}\left\| \mathbf{h}-x_{i}\right\|_{2}-\left\|x_{i}-\mathbf{p}_{s}\right\|_{2}\Big{)}\] \[\overset{\mathrm{(c)}}{\geq}\frac{1}{M}\sum_{x_{i}\in D_{ \varepsilon}^{(\delta)}}n_{[M]}(i)(\delta-2\varepsilon)\geq\frac{\varepsilon_{ 0}}{4M}\sum_{x_{i}\in D_{\varepsilon}^{(\delta)}}n_{[M]}(i).\]

In the above inequalities, (a) and (b) are both due to triangle inequalities, and (c) is because \(\|\mathbf{h}-x_{i}\|_{2}\geq\delta\) from the definition of \(D_{\varepsilon}^{(\delta)}\) in (27) and \(\|x_{i}-\mathbf{p}_{s}\|_{2}<2\varepsilon\) whenever \(w_{i}(\mathbf{p}_{s})>0\iff\Lambda(\mathbf{p}_{s};x_{i},2\varepsilon)>0\). Together with (28), the above set of inequalities imply

\[\frac{1}{M}\sum_{x_{i}\in D_{\varepsilon}^{(\delta)}}n_{[t]}(i) \leq\left(\frac{4}{\varepsilon_{0}}\right)\frac{1}{M}\sum_{x_{i}\in D_{ \varepsilon}^{(\delta)}}\left\|\sum_{s\in[M]}w_{i}(\mathbf{p}_{s})(\mathbf{h}- \mathbf{p}_{s})\right\|_{2}\leq\frac{4\sqrt{m}I}{\varepsilon_{0}}r(M).\] (29)On the other hand, since \(D_{\varepsilon}\) is an \(\varepsilon\)-grid of \(\mathcal{F}_{P}\), if \(\|\mathbf{p}_{s}-\mathbf{h}\|_{2}\geq\varepsilon_{0}\), there must exist \(x_{i}\in D_{\varepsilon}\) such that \(\|x_{i}-\mathbf{p}_{s}\|_{2}\leq\varepsilon\), which implies

\[\|x_{i}-\mathbf{h}\|_{2}\geq\|\mathbf{p}_{s}-\mathbf{h}\|_{2}-\|x_{i}- \mathbf{p}_{s}\|_{2}\geq\varepsilon_{0}-\varepsilon=\frac{3}{4}\varepsilon_{ 0}=\delta\quad\Rightarrow\quad x_{i}\in D_{\varepsilon}^{(\delta)}.\]

As for the weight that \(w_{i}\) assigns to \(\mathbf{p}_{s}\), we also have

\[w_{i}(\mathbf{p}_{s})=\frac{\Lambda(\mathbf{p}_{s};x_{i},2\varepsilon)}{\sum_ {j\in[I]}\Lambda(\mathbf{p}_{s};x_{j},2\varepsilon)}\geq\frac{2\varepsilon- \varepsilon}{I\cdot 2\varepsilon}=\frac{1}{2I}.\]

Therefore, we have

\[\frac{1}{M}\Big{|}\{s\in[M]:\|\mathbf{p}_{s}-\mathbf{h}\|\geq \varepsilon_{0}\}\Big{|}\leq\frac{1}{M}\sum_{x_{i}\in D_{\varepsilon}^{( \delta)}}\sum_{s\in[M]}(2I)w_{i}(\mathbf{p}_{s})=\frac{2I}{t}\sum_{x_{i}\in D_ {\varepsilon}^{(\delta)}}n_{[M]}(i)\] (30)

Finally, combining inequalities (29) and (30), we conclude that

\[\frac{1}{M}\Big{|}\{s\in[M]:\|\mathbf{p}_{s}-\mathbf{h}\|\geq \varepsilon_{0}\}\Big{|}\leq(2I)\lim_{M\to\infty}\frac{1}{M}\sum_{x_{i}\in D_ {\varepsilon}^{(\delta)}}n_{[M]}(i)\leq\frac{8\sqrt{m}I^{2}}{\varepsilon_{0}} r(M),\]

which proves (25). The proof is complete by taking the limit of \(M\to 0\), which guarantees \(r(M)\to 0\) and immediately implies the convergence result in (24). 

Note that the rate in Equation (25) does not depend on strategy \(\mathbf{h}\). Therefore, in the context of running LazyGDwoG (Algorithm 3), we can turn Lemma E.1 into the following uniform convergence result across epochs:

**Proposition E.2**.: _For any \(\varepsilon_{0}>0\), there exists a finite binning \(\Pi_{0}\), such that \(\forall\Phi>0\), if the principal runs LazyGDwoG for \(\Phi\) epochs where each epoch has length \(M\), and the agent's forecasts \((\mathbf{p}_{\phi,s})_{\phi\in[\Phi],s\in[M]}\) are adaptively calibrated with respect to \(\Pi_{0}\), then we have the following uniform convergence guarantee:_

\[\lim_{M\to\infty}\sup_{\phi\in[\Phi]}\frac{1}{M}\Big{|}\{s\in[M]:\|\mathbf{p }_{\phi,s}-\mathbf{h}_{\phi}\|_{2}\geq\varepsilon_{0}\}\Big{|}=0\] (31)

**Remark E.3**.: _Note that the rate in (25) has a polynomial dependency on \(|\Pi_{0}|\), which, due to the construction in the proof of Lemma E.1, ends up being exponential in \(m\) because it is the size of a \(\frac{\varepsilon_{0}}{4}\) grid of the domain \(\mathcal{A}_{P}\). To improve on this exponential dependency, one possible approach is to design an adaptive calibration algorithm for the agent that achieves the stronger notion of \(\ell_{1}\) calibration, which is more common in recent literature. For example, Hart [36], Foster and Vohra [29, 30] are defined using \(\ell_{1}\) calibration error rather than \(\ell_{\infty}\). Another approach is to avoid using naive conversion from \(\ell_{\infty}\) to \(\ell_{1}\) calibration error in (28), which leads to a polynomial dependency on the number of bins. These two approaches are equivalent ways of formulating the problem, and they both lead to interesting open directions._

### More auxiliary lemmas: approximate best response and closeness in utility

In this section, we use the results in Appendix E.2 to show that the average feedback \(\frac{1}{M}\sum_{s\in[M]}y_{\phi,s}\) in epoch \(\phi\in[\Phi]\) is close to the best response \(\mathtt{BR}(\mathbf{h}_{\phi})\) (Lemma E.4), and that the principal's average utility in this epoch is close to \(U_{P}(\mathbf{h}_{\phi}),\mathtt{BR}(\mathbf{h}_{\phi}))\) (Lemma E.5).

**Lemma E.4**.: _For any \(\varepsilon_{1}>0\), there exists a finite binning \(\Pi_{0}\) and \(M_{0}<\infty\) such that when agent's forecasts \(\mathbf{p}_{1:t}\) are adaptively calibrated with respect to \(\Pi_{0}\), then we have that \(\forall M\geq M_{0}\),_

\[\sup_{\phi\in[\Phi]}\left\|\frac{1}{M}\sum_{s\in[M]}y_{\phi,s}-\mathtt{BR}( \mathbf{h}_{\phi})\right\|_{2}\leq\varepsilon_{1}.\]

Proof.: Let \(\varepsilon_{0}=\frac{\varepsilon_{1}}{2L_{\mathtt{BR}}}\) and \(\Pi_{0}\) be the binning that satisfies Proposition E.2 for parameter \(\varepsilon_{0}\). Therefore, we know from Equation (31) in Proposition E.2 that for \(\varepsilon_{2}=\frac{\varepsilon_{1}}{2\cdot D_{P}\cdot L_{\mathtt{Im}}}\) there exists \(M_{0}\) such that \(\forall M\geq M_{0}\),

\[\sup_{\phi\in[\Phi]}\frac{1}{M}\Big{|}\{s\in[M]:\|\mathbf{p}_{\phi,s}-\mathbf{ h}_{\phi}\|_{2}\geq\varepsilon_{0}\}\Big{|}\leq\varepsilon_{2}.\] (32)Using Lipschitzness of the best response mapping \(\mathtt{BR}(\cdot)\), we have that \(\forall\phi\in[\Phi]\),

\[\left\|\frac{1}{M}\sum_{s\in[M]}y_{\phi,s}-\mathtt{BR}(\mathbf{h}_{ \phi})\right\|\] \[\leq \frac{1}{M}\sum_{s\in[M]}\left\|y_{\phi,s}-\mathtt{BR}(\mathbf{h}_ {\phi})\right\|_{2}\] (Triangle inequality) \[\leq L_{\mathtt{BR}}\frac{1}{M}\sum_{s\in[M]}\left\|\mathbf{p}_{\phi,s }-\mathbf{h}_{\phi}\right\|_{2}\] ( \[\mathtt{BR}(\cdot)\] is \[L_{\mathtt{BR}}\] -Lipschitz) \[\leq L_{\mathtt{BR}}\frac{1}{M}\left(\sum_{s\in[M]:\|\mathbf{p}_{ \phi,s}-\mathbf{h}_{\phi}\|\geq\varepsilon_{0}}\mathtt{diam}(\mathcal{H}_{P}) +\sum_{s\in[M]:\|\mathbf{p}_{\phi,s}-\mathbf{h}_{\phi}\|<\varepsilon_{0}} \varepsilon_{0}\right)\] \[\leq L_{\mathtt{BR}}\frac{1}{M}\left(\varepsilon_{2}M\cdot D_{P}+M \cdot\varepsilon_{0}\right)\] (Eq. ( 32 ) \[\&\mathtt{diam}(\mathcal{H}_{P})\leq D_{P}\] \[\leq D_{P}\cdot L_{\mathtt{BR}}\cdot\varepsilon_{2}+L_{\mathtt{BR}} \cdot\varepsilon_{0}=\frac{\varepsilon_{1}}{2}+\frac{\varepsilon_{1}}{2}= \varepsilon_{1}.\]

**Lemma E.5**.: _For any \(\varepsilon_{1}>0\), there exists a finite binning \(\Pi_{0}\) and \(M_{0}<\infty\) such that when agent's forecasts \(\mathbf{p}_{1:t}\) are adaptively calibrated with respect to \(\Pi_{0}\), then we have that \(\forall M\geq M_{0}\),_

\[\sup_{\phi\in[\Phi]}\left|\frac{1}{M}\sum_{s\in[M]}U_{P}(\mathbf{h}_{\phi},y_ {\phi,s})-U_{P}(\mathbf{h}_{\phi},\mathtt{BR}(\mathbf{h}_{\phi}))\right|\leq \varepsilon_{1}.\] (33)

Proof.: The proof of this lemma is very similar to that of Lemma E.4, with a different choice of constants \(\varepsilon_{0}\) and \(\varepsilon_{2}\). Note that since \(U_{P}\) is \(L_{2}\)-Lipschitz in the second argument, we have

\[\left|\frac{1}{M}\sum_{s\in[M]}U_{P}(\mathbf{h}_{\phi},y_{\phi,s} )-U_{P}(\mathbf{h}_{\phi},\mathtt{BR}(\mathbf{h}_{\phi}))\right|\leq \frac{1}{M}\sum_{s\in[M]}\left\|U_{P}(\mathbf{h}_{\phi},y_{\phi,s} )-U_{P}(\mathbf{h}_{\phi},\mathtt{BR}(\mathbf{h}_{\phi}))\right\|_{2}\] \[\leq L_{2}\cdot\frac{1}{M}\sum_{s\in[M]}\left\|y_{\phi,s}-\mathtt{ BR}(\mathbf{h}_{\phi})\right\|_{2}.\]

The rest of the proof follows from Lemma E.4 by choosing \(\varepsilon_{0}=\frac{\varepsilon_{1}}{2L_{2}L_{\mathtt{BR}}}\) and \(\varepsilon_{2}=\frac{\varepsilon_{1}}{2\cdot D_{P}\cdot L_{\mathtt{BR}}L_{2}}\). 

## Appendix F Future Directions

Adaptive CalibrationAlthough the results in this paper are all stated in terms of the \(\ell_{\infty}\)-calibration error (e.g., _maximum_ instead of _sum_ over bins), a lot of the existing calibration literature focuses on \(\ell_{1}\)-calibration error [29, 30]. It is an interesting problem whether we can get \(\ell_{1}\)-adaptive calibration error bounds without a polynomial dependency in the number of binning functions, where obtaining such bounds lead to polynomial improvements on the dependency of \(m\) (the number of agent's actions). In the case of continuous calibration, it is an open problem to obtain uniform (adaptive) calibration error bounds for parametric or nonparametric continuous binning function classes. Resolving this open problem could lead to a better rate for the learning direction of Theorem 5.2, as the current result uses naive \(\ell_{\infty}\)-to-\(\ell_{1}\) conversion of calibration error that leads to linear dependency on the number of binning functions, which turns out to be exponential in the dimension of the principal's action space. See Remark E.3 for more details.

Convex optimization from membership oraclesIn the case of finite Stackelberg games, we use the results of Lee et al. [42] to solve the constrained convex optimization problem in each polytope from approximate membership queries, where the key step is a polynomial reduction from approximate separation oracles to approximate membership oracles. However, the precision of the constructed separation oracle is worse than the precision of the membership oracle, which naturally leads to a worse precision of the final optimization solution. To be more specific, given an \(\varepsilon\)-membership oracle to a convex set \(K\), Lee et al. [42] can only guarantee the returned solution to be \(\varepsilon^{1/\gamma_{0}}\)-optimal and contained in \(B_{2}(K,\varepsilon^{1/\gamma_{0}})\), where \(\gamma_{0}>6\) is a fixed constant. An understudied open direction in the optimization community is whether a variant of Lee et al. [42] can return a solution in \(B_{2}(K,\varepsilon+\delta)\) for \(\delta\) that is _any_ tunable parameter, with runtime depending polynomially on \(1/\delta\).

**Open Problem F.1**.: _Given an \(\varepsilon\)-approximate membership oracle of a convex set \(K\subseteq\mathbb{R}^{m}\) and an evaluation oracle of a convex function \(f\), is there an optimization algorithm that, under sufficient regularity conditions, for any tunable \(\delta\), finds a near-optimal solution \(\hat{x}\) such that \(f(\hat{x})\leq\min_{x\in K}f(x)+\varepsilon^{1/\gamma_{0}}\) and \(\hat{x}\in B_{2}(K,\varepsilon+\delta)\) within \(\text{poly}(m,1/\delta,\log(1/\varepsilon))\) calls to both oracles?_

We note that resolving Open Problem F.1 will immediately lead to exponentially improved rates for our Theorem C.14 for any finite Stackelberg games. To see this, recall that the algorithms that achieve Theorem C.14 require repeated calls to the constructed ApproxMem oracle. This oracle provides an \(\varepsilon_{1}\)-approximate response, indicating whether or not the query point belongs to the \(\varepsilon_{2}\)-conservative version of each best response polytope. The number of samples used in simulating each oracle call is exponential in \(\frac{\varepsilon_{2}}{\varepsilon_{1}}\). The algorithm by Lee et al. [42] amplifies the inaccuracy of ApproxMem at a polynomial rate of \(\gamma_{0}\). Consequently, imposing a constraint of \(\varepsilon_{1}\lesssim\varepsilon_{2}^{\gamma_{0}}\). However, resolving Open Problem F.1 removes this constraint and introduces a much milder one, i.e., \(\varepsilon_{1}\lesssim\varepsilon_{2}-\delta\), where \(\delta\) is a tunable parameter. This adjustment leads to exponential improvement in the final sample complexity.