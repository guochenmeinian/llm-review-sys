# Hypothesis Selection with Memory Constraints

 Maryam Aliakbarpour

Department of Computer Science

Rice University

Houston, TX 77005

maryama@rice.edu

&Mark Bun

Department of Computer Science

Boston University

Boston, MA 02215

mbun@bu.edu

Adam Smith

Department of Computer Science

Boston University

Boston, MA 02215

ads22@bu.edu

###### Abstract

Hypothesis selection is a fundamental problem in learning theory and statistics. Given a dataset and a finite set of candidate distributions, the goal is to select a distribution that matches the data as well as possible. More specifically, suppose that we have sample access to an unknown distribution \(P\) over a domain \(\mathcal{X}\) that we know is well-approximated by one of a class of \(n\) distributions (a.k.a. hypotheses), \(\mathcal{H}\coloneqq\{H_{1},H_{2},\ldots,H_{n}\}\). The goal is to design an algorithm that outputs a distribution \(\hat{H}\in\mathcal{H}\) whose total variation distance from \(P\) is nearly minimal. In this work, we study the hypothesis selection problem under memory constraints. We consider a model where samples from \(P\) are presented in a stream and we access each sample \(x\) via "PDF-comparison" queries that allow us to compare the probability densities of any pair of hypotheses at the domain point \(x\) (i.e., is \(H_{i}(x)<H_{j}(x)\)?). This model allows us to study how much needs to be stored, at any point in time, about the portion of the stream seen so far. Our main result is an algorithm that achieves a nearly optimal tradeoff between memory usage and sample complexity. In particular, given \(b\) bits of memory (for \(b\) roughly between \(\log n\) and \(n\)), our algorithm solves the hypothesis selection problem with \(s\) samples, where \(b\cdot s=O(n\log n)\). This result is optimal up to an \(O(\log n)\) factor, for all \(b\).

## 1 Introduction

Learning the probability density function of observed data is a fundamental question in statistics with numerous applications in machine learning.Variants of this problem have been studied for nearly a century. _Hypothesis selection_ is a classic version of this problem where the goal is to learn a distribution within a pre-specified class. Let \(\mathcal{H}\coloneqq\{H_{1},H_{2},\ldots,H_{n}\}\) be a class of \(n\) known distributions over the same domain \(\mathcal{X}\). Suppose that we have sample access to an unknown distribution \(P\) over \(\mathcal{X}\) which is in, or a very close to, a member of \(\mathcal{H}\). The goal is to design an algorithm that outputs a distribution \(\hat{H}\in\mathcal{H}\) whose total variation distance to \(P\) is close to that of the closest distribution in \(\mathcal{H}\). A great deal of effort has been dedicated to solving this problem using a number of samples that does not depend on the domain size \(|\mathcal{X}|\). Perhaps surprisingly, one can learn an unknown distribution \(P\in\mathcal{H}\) with \(O(\log n)\) samples, independent of the domain size. In contrast, learning an arbitrary distribution \(P\) over \(\mathcal{X}\) requires \(\Omega(|\mathcal{X}|)\) samples. Refined guarantees for this problem have been studied extensively, building an understanding of the accuracy, samplecomplexity, and computational efficiency achievable, as well as the compatibility of hypothesis selection with robustness and privacy [12, 13, 14, 15, 16, 17, 18, 19, 20, 21].

In this paper, we expand upon the emerging theory of learning with limited memory [14, 15, 16, 17, 18, 19, 21] by studying hypothesis selection from this new perspective. We strive to answer the following questions: Given a working memory of \(b\) bits, how many data points (samples) do we need to solve the hypothesis selection problem? Prior work assumes that we can store all the samples in the memory, which can be quite large when we aim to learn a distribution over extremely large objects, such as, DNA sequences, gene expression data, text data, and brain image data.

We consider a model where samples are processed one at a time in a stream. Similar to models in [14, 15, 16, 17], we access each sample \(x\) via queries that allow us to compare the PDF of \(H_{i}\)'s at point \(x\), and we measure the size of the memory retained between processing samples. Our main result is an algorithm that achieves a nearly optimal tradeoff between the memory size \(b\) and the number of samples \(s\). Given \(b\) bits of memory (for \(b\) roughly between \(\log n\) and \(n\)), our algorithm solves the hypothesis selection problem with \(s\) samples where \(b\cdot s=O(n\log n)\). A result of Shamir [15, Theorem 2] gives a class of \(n\) hypotheses \(\mathcal{H}\) such that any algorithm that learns unknown distributions in \(\mathcal{H}\) requires \(s\cdot b=\Omega(n)\). Our tradeoff is thus optimal up to a \(O(\log n)\) factor.

### Main result

Suppose we have a class of \(n\) known distributions \(\mathcal{H}\coloneqq\{H_{1},H_{2},\ldots,H_{n}\}\) and an unknown distribution \(P\). We aim to design a _proper learning_ algorithm that outputs a distribution in \(\mathcal{H}\) whose total variation distance to \(P\) is comparable to that of the closest distribution in \(\mathcal{H}\). The algorithm has access to a stream of i.i.d. samples drawn from \(P\). At every given point, the algorithm can look at one sample, namely \(x\), in the stream and query a _PDF-comparator_ oracle by sending \(i,j\in[n]\), and receiving a bit indicating if \(H_{i}(x)<H_{j}(x)\). This is equivalent to asking whether a sample \(x\) is in the _Scheffe set_ of two distributions \(H_{i}\) and \(H_{j}\), which is defined as the set of elements to which \(H_{j}\) assigns higher probability than \(H_{i}\). The algorithm can also discard the current sample and move on to the next one. In addition to having access to the samples of \(P\), we have more direct access to the known distributions in \(\mathcal{H}\). The algorithm can query the probability masses of the Scheffe sets according to each \(H_{i}\) as is customary in the literature [14, 15]. This assumption can be relaxed; only estimates of such probabilities are needed, which can be obtained if we have sample access to \(H_{i}\)'s. Further details are available in Remark 3. To summarize our access model, our algorithm can make one of the following types of queries: 1) Request a new sample. 2) Query the PDF-comparator on the current sample \(x\) and ask if \(H_{i}(x)<H_{j}(x)\) for every pair of indices \(i,j\in[n]\). 3) Asks for the probability mass of Scheffe set between \(H_{i}\) and \(H_{j}\) according to \(H_{i}\) for every pair of indices \(i,j\in[n]\).

The accuracy of our output distribution is measured with respect to how far the _best_ distribution in \(\mathcal{H}\) is from \(P\). We define \(\mathsf{OPT}(\mathcal{H},P)\coloneqq\min_{H\in\mathcal{H}}\|P-H\|_{\mathsf{ TV}}\). When \(\mathcal{H}\) and \(P\) are clear from context, we denote this simply by \(\mathsf{OPT}\). With this setup in mind, we define a _proper learning algorithm (with promise)_ for hypothesis selection. The term "promise" refers to the extra parameter \(\Gamma\) that the algorithm receives as input: the learner may assume that \(\mathsf{OPT}\leq\Gamma\). (See Remark 2 for the case where \(\Gamma\) is not provided.)

**Definition 1.1**.: _Suppose \(\mathcal{A}\) has sample access to an unknown distribution \(P\). And, it can query the probabilities of the Scheffe sets according to each \(H_{i}\) and a PDF-comparator for every pair of hypotheses in \(\mathcal{H}\). Assume \(\mathcal{A}\) is given an additional input parameter \(\Gamma>0\). We say algorithm \(\mathcal{A}\) is an \((\alpha,\epsilon,\delta)\)-proper learner with a promise for \(\mathcal{H}\) if the following holds: for every distribution \(P\), if \(\Gamma\geq\mathsf{OPT}(\mathcal{H},P)\) then, with probability at least \(1-\delta\) over its input sample (drawn i.i.d. from \(P\)) and internal coin tosses, \(\mathcal{A}\) outputs a distribution \(\hat{H}\in\mathcal{H}\) such that:_

\[\|P-\hat{H}\|_{\mathsf{TV}}\leq\alpha\cdot\Gamma+\epsilon\.\] (1)

One can generally reduce \(\epsilon\) and \(\delta\) by increasing the number of samples taken or the time spent, whereas the value of \(\mathsf{OPT}\), and hence \(\Gamma\), is inherent to the problem instance. Therefore, it is more important and challenging to design algorithms that minimize the multiplicative parameter \(\alpha\).

Main theorem:Our main result is a proper learner (with promise) that obtains a nearly optimal tradeoff between memory and data. Formally, we have the following theorem:

**Theorem 1**.: _There exists a constant \(c\) for which the following holds. Let \(\mathcal{H}\) be an arbitrary class consisting of \(n\) distributions, and let \(\epsilon>0\). For every natural number \(b\) where \(c\cdot(\log n+\log((\log n)/\epsilon))\leq b\leq n\), there exists an \((\alpha=9,\epsilon,\delta=0.1)\)-proper learner (with promise) for \(\mathcal{H}\) using \(b\) bits of memory and the following number of samples:_

\[s=\left\{\begin{array}{ll} O\left(\frac{n\,\log n}{b}\cdot \frac{1}{\epsilon^{2}}\log\left(\frac{1}{\epsilon}\right)\right)&\text{ when }b\leq\frac{n}{\log\log n}\,,\\ O\left(\frac{n\,\log n}{b}\cdot\frac{1}{\epsilon^{2}}\log\left( \frac{\log n}{\epsilon}\right)\right)&\text{ when }\frac{n}{\log\log n}\leq b\leq n\,. \end{array}\right.\]

Roughly speaking, this theorem says that given \(b\) bits of memory that suffice to perform basic operations such as keeping track of indices and counting samples (i.e., \(b=\Omega\left(\log n+\log((\log n)/\epsilon)\right)\)), then \(s\cdot b\approx O(n\log(n)/\epsilon^{2})\) samples suffice to solve the hypothesis selection problem. A result of Shamir [20, Theorem 2] gives a class of \(n\) hypotheses \(\mathcal{H}\) such that every algorithm that learns a random \(P\) in \(\mathcal{H}\) requires \(s\cdot b=\Omega(n)\). Our tradeoff is thus optimal up to an \(O(\log n)\) factor. We speculate that our result is tight even for the class considered in [20], but we leave the proof of a tighter lower bound to future work.

**Remark 2**.: _Both versions of the hypothesis selection problem (with and without a promise that \(\mathsf{OPT}\leq\Gamma\)) have been extensively studied (e.g. [1, 1]). For simplicity, we present our result when \(\Gamma\) is given a priori. Section C describes a reduction that yields a similar result when \(\Gamma\) is not provided; the accuracy guarantee is analogous to that of Equation 1, but for slightly larger \(\alpha\)._

### Overview of our key ideas and other results

Background on comparing two hypotheses:Like many previous hypothesis selection algorithms, our algorithm relies on the ability to compare two hypotheses \(H_{1}\) and \(H_{2}\) based on the probabilities of their Scheffe set \(S\). We estimate \(P(S)\), and declare \(H_{i}\) the winner for which \(H_{i}(S)\) is closer to \(P(S)\). This natural approach works especially well when \(P\in\{H_{1},H_{2}\}\) or when \(P\) is much closer to one of the two hypotheses. Intuitively, a near-optimal hypothesis \(H\) should win many comparisons against other hypotheses. A typical hypothesis selection algorithm will run a tournament to identify such an \(H\). The primary advantage of using Scheffe based comparisons is that they are inexpensive to compute. Only \(O(\log n/\epsilon^{2})\) samples are sufficient to estimate the probability masses of all Scheffe sets within an error of \(\epsilon\). This exceptional sample efficiency enables the hypothesis selection algorithms circumvent the lower bound of \(\Omega(\left|\mathcal{X}\right|/\epsilon^{2})\) for distribution learning in cases where we have prior knowledge that \(P\) is close to a particular class of distributions.

That being said, the Scheffe based comparisons are not straightforward. One challenge is that these comparisons are not ideal: even if our estimates of the probabilities of the Scheffe sets were perfect, we might pick the hypothesis that is further from \(P\) in total variation distance (because \(S\) is just one event and might not ideally distinguish \(P\) from either \(H_{1}\) or \(H_{2}\)). Furthermore, comparisons are not transitive (again, because we measure only Scheffe set probabilities). Essentially the only useful property they have is this: if \(H_{1}\) is \(\Gamma\)-close to \(P\), it will win; if \(H_{1}\) is much further from \(P\) (by a constant factor) than both \(\Gamma\) and \(\|H_{2}-P\|_{\mathrm{TV}}\), then it will lose. These issues make the outputs of comparisons hard to interpret and complicate their use for selection.

Our terminology:To elaborate on our approach, we start by defining a _acceptable_ hypotheses, and our general terminology for referring to the quality of the hypothesis. We partition the hypotheses into three groups: 1) Excellent hypotheses, whose distance to \(P\) is \(\Gamma\). 2) Decent hypotheses, whose distance to \(P\) is larger than \(\Gamma\), but smaller than \(3\,\Gamma+\epsilon\). These distances are set in a way that decent hypothesis may win against an excellent hypothesis. Decent hypotheses are challenging to deal with because they may fool us into believing that an excellent hypothesis is not a good output, yet later they may lose against a very far hypothesis. 3) Unacceptable hypotheses, whose distance to \(P\) is larger than \(3\Gamma+\epsilon\), which will lose to every excellent hypotheses. We say a hypothesis is _acceptable_ if it is either excellent or decent. Generally, the goal is to find an acceptable hypothesis.

#### 1.2.1 Random ladder tournament

The main technical tool we introduce is a simple algorithm we call the _random ladder tournament_, which is inspired by _ladder tournaments_, a type of tournaments used in games and sports. Our algorithm finds an acceptable hypothesis using a linear number of comparisons. The algorithm proceeds in rounds as follows. Holding on to a single hypothesis in each round, we sample a uniformly random hypothesis from \(\mathcal{H}\) and compare it to the current hypothesis. The winner proceeds to the next round. Our novel analysis of this tournament shows that after \(O(|\mathcal{H}|)\) rounds, either the final winner produced at the end of the tournament or a winner selected randomly along its trajectory will be acceptable with high probability.

Proof overview:As previously mentioned, comparisons are often far from ideal; Even when the excellent hypothesis wins at a certain step, there is no guarantee that it would remain the winner till the end. To evade this issue, we utilize our knowledge of the upper bound of \(\mathsf{OPT}\), denoted by \(\Gamma\), and modify the comparisons in a way that ensures the excellent hypothesis never loses once it wins. In our comparisons, we favor the current winner; If the distance of the current winner to \(P\) on the Scheffe set is at most \(\Gamma\), we declare the current winner as the winner (even when the opponent hypothesis seems closer). Clearly, the excellent hypothesis will never be more than \(\Gamma\) away for \(P\) on any set. Hence, it will never lose again.

However, what happens when the excellent hypothesis never wins? At each step, we pick each hypothesis in \(\mathcal{H}\) uniformly at random, implying the excellent hypothesis is expected to be chosen in at least \(O(1)\) steps in our algorithms. The fact that the random hypothesis loses to the current winners of those steps indicates that those winners must be acceptable hypotheses; otherwise, the excellent hypothesis would have won them. Additionally, the fact that the excellent hypothesis appears as the random hypothesis in \(O(1)\) steps and loses to all of them, confirms that a constant fraction of the current winners are indeed acceptable. By combining these observations, we prove that the random ladder tournament will either find the excellent hypothesis at the end or encounter many acceptable hypotheses along the way.

Implementing the tournament with memory constraints:This algorithm is a highly effective technical tool for solving our problem. It can be implemented in a very memory-efficient manner. The algorithm processes the hypotheses in an "online" fashion, and it does not need to memorize the result of any past comparisons. At any given moment, it only remembers two hypotheses: the current hypothesis and the next randomly drawn hypothesis. And, if we draw fresh samples for every comparison, we can implement this algorithm by storing essentially \(O(1)\) numbers but sampling \(O(n\,\log(n)/\epsilon^{2})\) data points. On the other extreme, we can implement this algorithm with very little data, i.e., \(O(\log n/\epsilon^{2})\) samples, but a large amount of memory by storing an appropriate "summary" of samples (e.g., the probabilities of all the Scheffe sets in \(\tilde{O}(n^{2})\) bits).

We leverage this flexibility in memory usage of the random ladder tournament and provide an algorithm that only uses \(b\) bits of memory. We perform the comparisons in the random tournament in blocks of size \(t\). For each block of comparisons, we draw new samples and store a summary of them. We pick the parameter \(t\) in way that the summary fits in \(b\) bits, and it is sufficient for us to perform the next the \(t\) comparisons. For example, if \(t\approx\sqrt{b}\), we can store the number of samples in all the Scheffe sets of \(t\) hypotheses in \(b\) bits. This summary is enough for us to perform any comparisons between those hypotheses. Combined with a novel summary--sorted lists, described in Section 2.2.2--we get a tradeoff of the form \(s\cdot b\approx O(n\log^{3}(n)/\epsilon^{4})\). In Section 3.2, we presented this tradeoff alongside another tradeoff (with better dependence to \(\epsilon\), but worse dependence to \(\log n\)). Both of these tradeoff are worse than our main result, in a sense that \(s\cdot b\) is larger by a factor of \(O(\operatorname{poly}(\log(n),1/\epsilon))\). However, the these results are yielding more accurate proper learners; For the random ladder tournament, \(\alpha=3\), while in our main result \(\alpha=9\).

Simpler linear-time selection:The random ladder tournament leads to new results for the hypothesis selection problem outside of the scope of our original motivation for this paper. If we ignore memory constraints and store every sample, the random ladder tournament algorithm is simultaneously near-optimal in terms of number of samples, time, and accuracy -- to our knowledge, it is the first such algorithm. Specifically, it makes a linear number of comparisons and uses \(O(\log n/\epsilon^{2})\) samples from \(P\), which are both optimal for worst-case choices of \(\mathcal{H}\). Moreover, it finds a hypothesis with optimal accuracy under a promise. More precisely, it finds a hypothesis in \(\mathcal{H}\) that is roughly\(3\,\Gamma\)-close to \(P\) given a parameter \(\Gamma\) that is guaranteed to be at least \(\mathsf{OPT}\). This is essentially optimal, as no proper learning algorithm can achieve accuracy better than \(3\,\mathsf{OPT}\)[1] even when \(\Gamma=\mathsf{OPT}\) is given to the algorithm. Furthermore, with a simple adjustment to our algorithm, one can solve the hypothesis selection problem without any knowledge of \(\Gamma\) in nearly linear time and with roughly the same number of samples and obtain a \(5\,\mathsf{OPT}\)-close hypothesis in \(\mathcal{H}\). See Section C.

#### 1.2.2 Improved tradeoff by hypothesis filtration

The flexibility of the random ladder tournament already results in memory-data tradeoffs, but these are suboptimal by a factor of \(\operatorname{poly}(\log n,1/\epsilon)\). To improve the result, we design an algorithm called _filter_, that picks an acceptable hypothesis with a modest chance. While this chance is not high, we can use the output of the filtration and run the random ladder tournament on these filtered hypotheses. Since the quality of the input hypotheses in this second round is better, the random ladder tournament can be implemented effectively on a smaller set \(\mathcal{H}\) and results in a better tradeoff. Hence, we obtain our main result. See Section B.4 for the proof of our main results. Below, we give a short description of the filter algorithm. We defer the full description and the proofs to Section B.3.

hypothesis filtration:The best approach to finding an acceptable hypothesis drastically depends on the quality of the hypotheses in \(\mathcal{H}\). For example, if there is no decent hypothesis in \(\mathcal{H}\), any single elimination tournament will easily find an excellent hypothesis due to the fact that any comparison involving at least one excellent hypothesis will declare the excellent hypothesis the winner. On the other hand, if there are a lot of decent hypotheses, then there is a decent chance that a randomly selected hypothesis is decent, hence an acceptable one. The filtration algorithm works by combining these two observations. The algorithm randomly performs one of the following, each with probability \(1/2\): 1) It draws a set of random hypotheses. Assuming there is no decent hypothesis in the set, it runs a _group elimination tournament_ (see below), and outputs the result. 2) It outputs a random hypothesis in \(\mathcal{H}\). One can show that if the decent hypotheses are scarce in \(\mathcal{H}\), we have a decent chance of getting an excellent hypothesis and no decent hypothesis in the random set; hence, we find an excellent hypothesis via our group elimination algorithm. On the other hand, if the decent hypotheses are abundant, then we have a decent chance of picking one in the second step. By selecting each strategy with a probability \(1/2\), we preserve half of the success probability in whichever case holds. While this probability is not a lot, it is larger than \(1/n\) (the probability of picking the excellent hypothesis from \(\mathcal{H}\)). We take advantage of this fact, and show one can run the random ladder tournament in fewer rounds and obtain the desired result. See Section B.3 for more details.

Group elimination tournament:Our second technical tool is an algorithm that finds an excellent hypothesis if no decent hypothesis exists in \(\mathcal{H}\). This algorithm is a combination of a single-elimination tournament and an _all-go-against-all_ tournament. In this algorithm, at every step, we partition the hypotheses into multiple groups, run a previously known all-go-against-all tournament within each group (say minimum distance estimate [13, 1]), and then send the winners of each group to the next round. If a group contains an excellent hypothesis, then in an all-go-against-all tournament, we will pick an excellent hypothesis as the winner. Hence, an excellent hypothesis will "bubble up" in each round and be declared the final winner. With a careful choice of group sizes and the number of rounds, this algorithm can be implemented using \(O(n)\) bits of memory and \(O(\log n)\) samples under the assumption that the hypotheses are indexed from 1 to \(n\). A technical challenge that arises, however, is that we need to run this algorithm on a subset of roughly \(O(b)\) random hypotheses (so that we can run it within a memory bound of \(b\)). However, storing \(b\) random indices already requires \(O(b\log n)\) space. We show that we can instead draw indices pseudorandomly using a pairwise independent generator that can be implemented in small space. See Section B.1 for more details.

### Related work

Prior to our work, numerous studies have investigated the problem of _hypothesis selection_ in various settings. In their seminal work, Yatracos [14], and Devroye and Lugosi [13, 12, 11] present algorithms to find a close distribution in \(\mathcal{H}\) based on only estimation of the probabilities of the Scheffe sets of pairs of hypothesis in \(\mathcal{H}\). These approaches suggest that we only need accurate estimation of the probability that \(P\) assigns to \(O(|\mathcal{H}|^{2})=O(n^{2})\) sets, resulting in sample complexity proportional to \(O(\log n)\) instead of \(\Omega(|\mathcal{X}|)\). For a comprehensive overview, see Chapter 6 in [13].

Mahalanabis and Stefankovic in [15] provide an algorithm with linearly many probes to \(P\) and \(\alpha=3\) but they require an exponential time pre-processing of the class \(H\).1Daskalakis and Kamathin [4] give a nearly-linear time algorithm that given an upper bound on \(\mathsf{OPT}\leq\Gamma\), returns \(\hat{H}\) such that \(\|\hat{H}-P\|_{\mathsf{TV}}\leq\alpha\cdot\Gamma+\epsilon\) for some large constant \(\alpha=512\). Acharya et al. in [1, 1] provide a \(O(n\log n)\) time algorithm that finds a hypothesis with accuracy guarantee of \(\alpha=9\). For proper learners, Bousquet et al. in [1] showed that the best \(\alpha\) in Equation 1 that one can hope for is \(\alpha=3\) as long as the sample complexity does not grow with the domain size \(|\mathcal{X}|\). Aamand et al. in [1] also study statistical computational tradeoffs (time-data) for the hypotheses over _discrete domains_. Unlike our work, their setting allows pre-processing of the class \(\mathcal{H}\) in polynomial time.

There is a long history of research focusing on a special case of this problem where \(\mathcal{H}\) is a _specific structured class of distributions_ such as mixtures of Gaussians [1, 4, 1, 1, 2, 3, 1, 2], histograms [1, 2], and polynomials [1]. The abstract hypothesis selection we study here is commonly used as a subroutine in solving these problems (usually in conjunction with some sort of a cover method). For a survey of results, see [1].

Lastly, there is another field of study that tackles a question similar to ours: the problem of sorting items with noisy comparisons. One can view hypothesis selection as the task of finding the minimum item. With the exception of [1] that we have discussed above, these probabilistic noise models do not capture the geometric structure presenting in our problem. Therefore, we did not find any of these result yielding an immediate solution to our problem.

## 2 Preliminaries

### Notation

Let \([n]\) denote the set \(\{1,2,\ldots,n\}\). Suppose that we have a probability distribution \(P\) over a domain \(\mathcal{X}\). For element \(x\) in \(\mathcal{X}\), \(P(x)\) denotes the probability of \(x\). We assume \(\mathcal{X}\) is the domain of all the probability distributions in this article. For any subset of the domain \(S\), \(P(S)\) denotes the sum of the probabilities of all the elements in \(S\). We denote the total variation distance between two distributions \(P_{1}\) and \(P_{2}\) by \(\|P_{1}-P_{2}\|_{\mathsf{TV}}\), and it is defined as \(\max_{S\subset\mathcal{X}}|P_{1}(S)-P_{2}(S)|\) where \(S\) is any measurable subset of the domain. We say \(P\) is _\(\epsilon\)-close_ to \(Q\) iff \(\|\bar{P}-Q\|_{\mathsf{TV}}\) is at most \(\epsilon\). Also, we say \(P\) is _\(\epsilon\)-far_ from \(Q\) iff \(\|P-Q\|_{\mathsf{TV}}\) is greater than \(\epsilon\). We denote the Scheffe set of two distributions as follows: \(\mathcal{S}(H_{1},H_{2})\coloneqq\{x\in\mathcal{X}\mid H_{1}(x)<H_{2}(x)\}\).

### Basic tools

In this section, we focus on our primary tools we have used throughout this article. First, we start with a comparisons algorithm that allows us to compare two hypotheses. Next, we use the fact that one can estimate the probabilities of the \(O(n^{2})\) many Scheffe sets up to error \(c\cdot\epsilon\) (for some small constant \(c<1\)) with probability at least \(1-\delta\) using \(O(\log(n/\delta)/\epsilon^{2})\) samples from \(P\).

#### 2.2.1 Comparing two hypotheses

Our algorithms works based on a basic operation that allows us to compare two hypotheses \(H_{1}\) and \(H_{2}\) based on the probabilities of their Scheffe sets. We pick the hypothesis that appears to be closer to \(P\) and declare it the _winner_ (and the other hypothesis the _loser_). The challenging part is that these comparisons are not ideal. As in, even when our estimations of the probabilities of the Scheffe sets are accurate enough, we may pick a hypothesis that is further. However, one can guarantee that if the distance of the further hypothesis among \(H_{1}\) and \(H_{2}\) is much worse than the distance of the closer one, the comparison procedure would never select it as the winner. The algorithm is given three parameters: 1) \(\Gamma\): an upper bound for \(\mathsf{OPT}\); 2) \(\epsilon\): indicating the estimation error for the Scheffe set probabilities is \(\Theta(\epsilon)\); and 3) the confidence parameter \(\delta\). Upon invoking this algorithm, it determines the result of the comparison with the guarantees formalized in the following lemma:

**Lemma 2.1**.: _Upon receiving three parameters: \(\Gamma\), \(\epsilon\), and \(\delta\), Algorithm 1 uses \(O(\log(1/\delta)/\epsilon^{2})\) samples and satisfies the following guarantees with probability at least \(1-\delta\):_

* _If_ \(H_{1}\) _is_ \(\Gamma\)_-close to_ \(P\)_, then the algorithm returns_ \(H_{1}\)_._
* _If_ \(H_{1}\) _is_ \((\max{(\Gamma,\|H_{2}-P\|_{\mathcal{TV}})}+2\,\|H_{2}-P\|_{\mathcal{TV}}+ \epsilon)\)_-far from_ \(P\)_, then the algorithm returns_ \(H_{2}\)The proof of this lemma is provided in Section D.

```
1:procedurecompare(\(H_{1},\ H_{2},\ \Gamma,\ \epsilon,\ \delta\), sample access to \(P\))
2: Draw \(m=\Theta(\log(1/\delta)/\epsilon^{2})\) samples from \(P\)
3:\(\hat{q}\leftarrow\) fraction of samples from \(P\) in \(\mathcal{S}(H_{1},H_{2})\) using the PDF-comparator.
4:\(p_{1}\gets H_{1}\left(\mathcal{S}(H_{1},H_{2})\right)\).
5:\(p_{2}=H_{2}\left(\mathcal{S}(H_{1},H_{2})\right)\).
6:\(\epsilon^{\prime}\leftarrow\epsilon/2\)
7:if\(|p_{1}-\hat{q}|\leq\Gamma+\epsilon^{\prime}\) or \(|p_{1}-\hat{q}|\leq|p_{1}-\hat{q}|\)then
8: Output \(H_{1}\). \(\triangleright\)\(H_{1}\) is the winner.
9:else
10: Output \(H_{2}\). \(\triangleright\)\(H_{2}\) is the winner. ```

**Algorithm 1** Choosing between two hypotheses

**Remark 3**.: _While we have assumed that we have access to the probabilities of the Scheffe sets in the above algorithm, this assumption is not necessary. In fact, one can obtain similar guarantees to Lemma 2.1 as long as we have estimates of these probabilities up to accuracy \(c\cdot\epsilon\) for a sufficiently small constant \(c<1\). Hence, our result can be adjusted to the setting that we have sample access to \(H_{i}\)'s instead._

Our terminology based on comparisons:We say a distribution \(H\) is _excellent_ iff \(\|H-P\|_{\mathrm{TV}}\) is \(\Gamma\). We usually use \(H^{*}\) to denote an excellent hypothesis. We say a distribution \(H\) is decent iff \(\|H-P\|_{\mathrm{TV}}\) is greater than \(\Gamma\), but smaller than \(3\,\Gamma+\epsilon\). It implies that a decent hypothesis may win an excellent \(H^{*}\in\mathcal{H}\). We say a distribution \(H\) is _acceptable_ iff it is excellent or decent. And, we say a distribution \(H\) is _unacceptable_ iff it is not acceptable. Note that it is impossible for an unacceptable hypothesis to win an excellent hypothesis with probability greater than \(\delta\). Throughout this article, when we say _valid comparisons_, we refer to an event in which the Scheffe sets are estimated accurately, so the conditions in Lemma 2.1 hold. Since we have only \(O(n^{2})\) many Scheffe sets, using \(O(\log(n/\delta)/\epsilon^{2})\) samples is enough to assume all the comparisons are valid with a probability of at least \(1-\delta\).

#### 2.2.2 Space efficient sample summaries for comparisons within a batch of hypotheses

In order to obtain sample efficiency, we need sufficient information about samples that enables us to reuse the same set of samples for multiple comparisons. Below, we describe two approaches to _summarize_ the sample set.

Scheffe counts.Observe that to compare two hypotheses \(H_{1}\) and \(H_{2}\), we only use the count of the number of samples in their Scheffe set \(\mathcal{S}(H_{1},H_{2})\). Hence, instead of storing samples directly, for every pair of hypotheses in \(\mathcal{H}\), we store the number of samples in their Scheffe set. We refer to this number as the _Scheffe counts_. If we have \(m\) samples, we can store all the \(O(n^{2})\) Scheffe counts using \(O(n^{2}\log m)\) bits.

Storing each sample via a sorted list.In this approach, we store a succinct summary of each sample that allows us to infer its membership in each Scheffe set. This information suffices to perform the comparisons we need later. To store a sample \(x\), we save an ordered list of hypotheses that is sorted according to the probability of \(x\) (the PDF at point \(x\)). That is, we store a list of indices \(i_{1},i_{2},\ldots,i_{n}\) such that \(H_{i_{1}}(x)\leq H_{i_{2}}(x)\leq\cdots\leq H_{i_{n}}(x)\) (use hypothesis indices to break the tie). The summary of a single sample can be computed in \(O(n\log n)\) time and takes \(O(n\log n)\) bits of space to store. Now, to compare the PDF of two hypotheses \(H_{i}\) and \(H_{j}\), we simply can find \(i\) and \(j\) in the list and check whether \(i\) precedes \(j\) in the list.2 Alternatively if we want to compare PDF's faster, we can store the indices in another list call \(d_{x}\) that maps indices to list positions. We set \(d_{x}[i_{\ell}]=\ell\) for every \(\ell\in[n]\). In this case, to compare the PDF's, we can simple check if \(d_{x}[i]<d_{x}[j]\).

Footnote 2: It is worth noting that we may not answer the PDF-comparison correctly when \(H_{i}(x)=H_{j}(x)\). However, since \(x\) does not contribute to the discrepancy between \(H_{i}\) and \(H_{j}\), its inclusion (or exclusion) to the Scheffe set is inconsequential.

#### 2.2.3 All-go-against-all tournament

There are standard techniques in the literature to solve hypothesis selection problem when the estimates of all the Scheffe sets are available. See Chapter 6 in [1] and [11]. These approachessuggest that we only need accurate estimation of the probability that \(P\) assigns to \(O(\left|\mathcal{H}\right|^{2})=O(n^{2})\) sets, therefore, the sample complexity can be proportionate to \(O(\log n)\) instead of \(O(\left|\mathcal{X}\right|)\). For a formal statement, see Fact D.1.

## 3 Random ladder tournament

In this section, we focus on the _Random ladder tournament_ which is a proper learner (with promise) of \(P\) in a finite class of \(n\) hypotheses, \(\mathcal{H}\). The algorithm is given a parameter \(\Gamma\) as an upper bound for \(\mathsf{OPT}(\mathcal{H},X)\). It outputs \(\hat{H}\in\mathcal{H}\) such that, with high probability we have: \(\|\hat{H}-P\|_{\mathsf{TV}}\leq 3\,\Gamma+\epsilon\,\). Bousquet et al. in [1] have shown that no proper learner can achieve an error better than \(3\,\mathsf{OPT}+\epsilon\) unless the sample complexity grows with the domain size \(\left|\mathcal{X}\right|\). Their lower bound holds even when the algorithm is provided with \(\Gamma=\mathsf{OPT}\). Hence, the factor \(\alpha=3\) above is optimal.

For clarity in our presentation, in Section 3.1, we present the random ladder tournament assuming that: 1) We set the confidence parameter to be a small constant (\(\delta=0.1\)). 2) the algorithm can call the Compare subroutine without specifying how this subroutine is implemented. In Section A.1, we show one can modify this algorithm to work for arbitrary small confidence parameter \(\delta\). Also, in Section 3.2, we discuss how one can implement this algorithm with memory constraints. We use the sample summary approaches described in Section 2.2.2, and we obtain two (sub-optimal) memory-data tradeoffs for this algorithm.

### Random ladder tournament with no memory constraints

In this section, we present the random ladder tournament that solves the hypothesis selection problem while a parameter \(\Gamma\) is given to the algorithm as an upper bound for \(\mathsf{OPT}\). For this result, we assume there is a _meta distribution_ over \(\mathcal{H}\), denoted by \(\mathcal{D}\), with a non-negligible probability \(p_{0}\) to draw a hypothesis that is \(\mathsf{OPT}\)-close. The algorithm considers hypotheses one at a time where at step \(i\), we draw \(R_{i}\) from the meta distribution. While later we use this algorithm with other meta distributions, it may help the reader to view the meta distribution as a uniform distribution over a set of \(n\) hypotheses, \(\mathcal{H}\). In this case, if we draw a random hypothesis from \(\mathcal{H}\), it is guaranteed to be \(\Gamma\)-close to \(P\) with probability \(p_{0}\geq 1/n\). Our algorithm finds a sufficiently close hypothesis using \(\Theta(1/p_{0})\) comparisons.

At a high-level, our algorithm goes through a list of randomly drawn hypotheses from \(\mathcal{D}\), compares them, and keeps track of the _current winner_ hypothesis. We denote the current winner hypothesis at step \(i\) by \(W_{i}\). Initially, we start with \(W_{0}\) being equal to a fake hypothesis that loses to any other hypothesis. Then at every step, we take a randomly drawn hypothesis \(R_{i}\) and compare it with \(W_{i-1}\). We set the current winner of step \(i\), \(W_{i}\), be the winner of a comparison between \(W_{i-1}\) and \(R_{i}\). We show that after \(k=\Theta(1/p_{0})\) steps either the final winner, \(W_{k}\), is an \(\Gamma\)-close hypotheses, or many of \(W_{i}\)'s that we have encountered are close to \(P\). That is, either \(W_{k}\) is an excellent hypothesis or a random \(W_{i}\) is an acceptable choice. To exploit this fact at every step, we add each \(W_{i}\) to a list, namely \(Q\), with some small probability. At the end, we also add \(W_{k}\) to \(Q\). We prove a random hypothesis in \(Q\) will be close to \(P\) with high probability. Algorithm 2 shows the formal description of our approach, and we prove its performance in Theorem 4. Figure 1 illustrates a visual representation of the algorithm.

**Theorem 4**.: _Suppose we can draw i.i.d. random hypotheses from an arbitrary meta distribution \(\mathcal{D}\) over \(\mathcal{H}\). And, we have a hypothesis \(P\) that we aim to learn properly in \(\mathcal{H}\). Assume we are given parameters \(p_{0}\) and \(\Gamma\) such that the probability that a random hypothesis is \(\Gamma\)-close to \(P\) is at least \(p_{0}\). For any \(\epsilon\in(0,1)\), Algorithm 2 is \((\alpha=3,\epsilon,\delta=0.1)\)-proper learner (with promise) for the class \(\mathcal{H}\)._

Proof.: First, note that each comparison, invoked in Line 7, will satisfies the properties of Lemma 2.1 with probability at least \(1-1/(100\,k)\). Hence, using Lemma 2.1 and the union bound, one can assume with probability \(0.99\) for all the comparisons we have:

* If \(W_{i}\) is \(\Gamma\)-close to \(P\), it will not lose; it remains as the current winner for the rest of the steps: \(W_{i}=W_{i+1}=\cdots=W_{k}\).
* If \(W_{i-1}\) is \((3\Gamma+\epsilon)\)-far from \(P\), and \(R_{i}\) is \(\Gamma\)-close to \(P\), then \(W_{i}\) is equal to \(R_{i}\).

For the rest of this proof, we fix the response of comparisons for which the above conditions hold. The randomness used in the rest of this proof only depends on the randomness in the meta distribution over hypotheses and the internal coin tosses of the algorithm.

Our goal is to show that most of the hypotheses in the list, \(Q\), are acceptable, so a randomly selected hypothesis in \(Q\) will be acceptable as well. Our first step is to show that the expected number of acceptable hypotheses in \(Q\) is high compared to the size of \(Q\). For each \(i\) in \([k]\), we define an indicator random variable \(I_{i}\) that is one if we add an acceptable hypothesis to the \(Q\) at step \(i\) in Line 8, and zero otherwise. Also, we define another indicator variable \(I_{k+1}\) corresponding to the event that the final \(W_{k}\), which we added to \(Q\) in Line 9, is an acceptable hypothesis. More formally, we have:

\[I_{i} \coloneqq\mathbbm{1}_{W_{i}\text{is acceptable, and we add }W_{i}\text{ to }Q\text{ in Line 8}.}\hskip 28.452756pt\forall i\in[k]\,,\] \[I_{k+1} \coloneqq\mathbbm{1}_{W_{k}\text{is acceptable. }}\,.\]

Clearly, the sum of \(I_{i}\)'s indicates the number of acceptable hypotheses in \(Q\), so we focus on finding a lower bound for the expected value of this quantity. Let \(\alpha_{i}\) be the probability of \(W_{i}\) being acceptable. Without loss of generality, we set \(\alpha_{0}\) equal to zero. It is not hard to see that: \(\mathbf{E}\big{[}I_{i}\big{]}=p_{0}\cdot\alpha_{i}\) for \(i\in[k]\). Moreover for the last indicator variable, we have:

\[\mathbf{E}\big{[}I_{k+1}\big{]} =\mathbf{Pr}\big{[}W_{k}\text{ being acceptable}\big{]}\geq\mathbf{Pr} \big{[}\|W_{k}-P\|_{\text{TV}}\leq\Gamma\big{]}\] \[=\sum_{i=1}^{k}\mathbf{Pr}\big{[}W_{i-1}\text{ loses to }R_{i}\,,\text{ and }\|R_{i}-P\|_{\text{TV}}\leq\Gamma\,,\text{ and }R_{i}\text{ does not lose to }R_{i+1},R_{i+2},\ldots,R_{k}\big{]}\,.\]

Assuming the pairwise comparisons are done perfectly, \(R_{i}\) being \(\Gamma\)-close to \(P\), automatically implies that \(R_{i}\) does not lose to any of the hypotheses \(R_{i+1},\ldots,R_{k}\). Thus, we have:

\[\mathbf{E}\big{[}I_{k+1}\big{]} \geq\sum_{i=1}^{k}\mathbf{Pr}\big{[}W_{i-1}\text{ loses to }R_{i}\,,\text{ and }\|R_{i}-P\|_{\text{TV}}\leq\Gamma\big{]}\] \[=\sum_{i=1}^{k}\mathbf{Pr}\big{[}W_{i-1}\text{ loses to }R_{i}\mid\|R_{i}-P\|_{\text{TV}}\leq\Gamma\big{]}\cdot \mathbf{Pr}\big{[}\|R_{i}-P\|_{\text{TV}}\leq\Gamma\big{]}\,.\]

Note that the probability of \(\|R_{i}-P\|_{\text{TV}}\leq\Gamma\) is at least \(p_{0}\). Now given that \(R_{i}\) is \(\Gamma\)-close to \(P\), it will certainly win any hypotheses that is \((3\,\Gamma+\epsilon)\)-far from \(P\). Thus, the event that \(W_{i}\) is \((3\,\Gamma+\epsilon)\)-far must have a lower probability than the event that \(W_{i-1}\) loses to \(R_{i}\). Therefore, we obtain the following lower bound:

\[\mathbf{E}\big{[}I_{k+1}\big{]} \geq\sum_{i=1}^{k}\mathbf{Pr}\big{[}\|W_{i-1}-P\|_{\text{TV}}>(3\, \Gamma+\epsilon)\mid\|R_{i}-P\|_{\text{TV}}\leq\Gamma\big{]}\cdot p_{0}\] \[=\sum_{i=1}^{k}\mathbf{Pr}\big{[}W_{i-1}\text{ being unacceptable }\mid\|R_{i}-P\|_{\text{TV}}\leq\Gamma\big{]}\cdot p_{0}\,.\]

Recall that we pick \(R_{i}\) independently from all the previous hypothesis, \(R_{1},R_{2},\ldots,R_{i-1}\), so one can say \(W_{i-1}\) is independent of \(R_{i}\). This implies:\[\mathbf{E}[\textit{I}_{k+1}]\geq\sum_{i=1}^{k}\mathbf{Pr}[\textit{W}_{i-1}\ \text{being unacceptable}]\cdot p_{0}=\sum_{i=1}^{k}(1-\alpha_{i-1})\cdot p_{0}\,.\]

Putting it all together, the expected value of the sum of \(I_{i}\)'s is:

\[\mathbf{E}\left[\sum_{i=1}^{k+1}I_{i}\right]\geq\sum_{i=1}^{k}p_{0}\cdot\alpha _{i}+(1-\alpha_{i-1})\cdot p_{0}=k\cdot p_{0}+p_{0}\cdot(\alpha_{k}-\alpha_{0} )\geq k\cdot p_{0}\,.\]

The last inequality above is due to the fact that we set \(\alpha_{0}\) to zero. Note that the above equation states that the expected number of acceptable hypotheses in \(Q\) is at least \(k\cdot p_{0}\). On the other hand, the expected number of hypotheses in \(Q\) is \(k\cdot p_{0}+1\). Thus, the expected number of unacceptable hypotheses in \(Q\) is at most one. Now, by Markov's inequality, the probability that we have more than 50 unacceptable hypotheses in \(Q\) is at most 0.02. And, by the Chernoff bound, we know that the probability of having less than 1000 many hypothesis in \(Q\) is at most:

\[\mathbf{Pr}[\,\#\text{hypotheses in }Q<1000]=\mathbf{Pr}\bigg{[}\mathbf{Bin}(k,p _{0})<\frac{k\cdot p_{0}}{2}\bigg{]}\leq\exp\left(-\frac{k\cdot p_{0}}{8} \right)\leq 10^{-100}\,.\]

It is not hard to see that the ratio of unacceptable hypotheses to the total number of hypotheses in \(Q\) is at most 50/1000=0.05. Therefore, a random hypothesis in \(Q\) is acceptable with probability at least \(0.95\). Hence, by the union bound the total error probability is bounded by:

\[\mathbf{Pr}[\,\text{Outputting an unacceptable hypothesis}] \leq\mathbf{Pr}[\,\text{Outputting an unacceptable hypothesis }|\ \text{valid comparisons}]\] \[+\mathbf{Pr}[\text{at least one invalid comparisons}]\leq 0.05+0.01<0.1\]

Therefore, the proof is complete. 

### Memory-data tradeoffs of random ladder tournament

As we have discussed, one of the advantages of Algorithm 2 is its flexibility in the usage of memory and data. In the following, we describe a tradeoff that we can obtain for this algorithm using the sample summaries presented in Section 2.2.2. At a high-level, the following is how we achieve the tradeoff: suppose we have \(b\) bits of memory. We choose the largest integer \(t\) so that we can store the sample summary needed to compare \(t\) hypotheses. Then, we run the random ladder tournament while we draw new samples and refresh the sample summary at every \(t\) step. Our two described sample summaries Scheff'e counts and the sorted list lead to the following memory-data tradeoffs. Although these tradeoffs are not as tight as our main result (by factors of \(\log n\) and \(1/\epsilon\)), they have better accuracy guarantees (\(\alpha=3\) instead of \(\alpha=9\) in our main result).

**Lemma 3.1**.: _Suppose we have \(n\) hypotheses in \(\mathcal{H}\). For every \(p_{0}\geq 1/n\), \(\epsilon\), and an integer \(t\) between two and \(k=\Theta(1/p_{0})\), one can implement Algorithm 2 in such away that it uses:_

\[s=O\left(\frac{1}{t\,p_{0}}\cdot\frac{\log p_{0}^{-1}}{\epsilon^{2}}\right)\ \text{samples, and}\qquad b=O\left(t^{2}\log\left(\frac{\log p_{0}^{-1}}{\epsilon} \right)+t\log n\right)\ \text{bits of memory.}\]

**Lemma 3.2**.: _Suppose we have \(n\) hypotheses in \(\mathcal{H}\). For every \(p_{0}\geq 1/n\), \(\epsilon\), and an integer \(t\) between two and \(k=\Theta(1/p_{0})\), one can implement Algorithm 2 in such away that it uses:_

\[s=O\left(\frac{1}{t\,p_{0}}\cdot\frac{\log p_{0}^{-1}}{\epsilon^{2}}\right)\ \text{samples, and}\qquad b=O\left(t\cdot\frac{\log(p_{0}^{-1})\cdot\log n}{ \epsilon^{2}}+t\log n\right)\ \text{bits of memory.}\]

For the proofs of the above lemmas, see Section A.2.

## Acknowledgement

MA was supported by NSF awards CNS-2120667, CNS-2120603, CCF-1934846, and BU's Hariri Institute for Computing. This work was predominantly done while MA was affiliated with Boston University and Northeastern University. MB was supported by NSF awards CCF-1947889 and CNS-2046425, and a Sloan Research Fellowship. AS was supported in part by NSF awards CCF-1763786 and CNS-2120667 as well as Faculty Awards from Google and Apple.

## References

* [AAC\({}^{+}\)23] Anders Aamand, Alexandr Andoni, Justin Y. Chen, Piotr Indyk, Shyam Narayanan, and Sandeep Silwal. Data structures for density estimation. In _Proceedings of the 40th International Conference on Machine Learning_. PMLR, 2023.
* [ABH\({}^{+}\)20] Hassan Ashtiani, Shai Ben-David, Nicholas J. A. Harvey, Christopher Liaw, Abbas Mehrabian, and Yaniv Plan. Near-optimal sample complexity bounds for robust learning of gaussian mixtures via compression schemes. _J. ACM_, 67(6):32:1-32:42, 2020.
* [ABM18] Hassan Ashtiani, Shai Ben-David, and Abbas Mehrabian. Sample-efficient learning of mixtures. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, _Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence_, pages 2679-2686. AAAI Press, 2018.
* [ADLS17] Jayadev Acharya, Ilias Diakonikolas, Jerry Li, and Ludwig Schmidt. Sample-optimal density estimation in nearly-linear time. In _Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA_, pages 1278-1289. SIAM, 2017.
* [AF23] Tomer Adar and Eldar Fischer. Refining the adaptivity notion in the huge object model. _CoRR_, abs/2306.16129, 2023.
* [AFJ\({}^{+}\)18] Jayadev Acharya, Moein Falahatgar, Ashkan Jafarpour, Alon Orlitsky, and Ananda Theertha Suresh. Maximum selection and sorting with adversarial comparators. _The Journal of Machine Learning Research_, 19:59:1-59:31, 2018.
* [AJOS14] Jayadev Acharya, Ashkan Jafarpour, Alon Orlitsky, and Ananda Theertha Suresh. Sorting with adversarial comparators and application to density estimation. In _2014 IEEE International Symposium on Information Theory_, pages 1682-1686, 2014.
* [AMNW22] Maryam Aliakbarpour, Andrew McGregor, Jelani Nelson, and Erik Waingarten. Estimation of entropy in constant space with improved sample complexity. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 32474-32486. Curran Associates, Inc., 2022.
* [BBK\({}^{+}\)21] Olivier Bousquet, Mark Braverman, Gillat Kol, Klim Efremenko, and Shay Moran. Statistically near-optimal hypothesis selection. In _62nd IEEE Annual Symposium on Foundations of Computer Science, FOCS 2021, Denver, CO, USA, February 7-10, 2022_, pages 909-919. IEEE, 2021.
* [BBS22] Gavin Brown, Mark Bun, and Adam Smith. Strong memory lower bounds for learning natural models. In _Proceedings of Thirty Fifth Conference on Learning Theory COLT_, 2022.
* [BKM19] Olivier Bousquet, Daniel Kane, and Shay Moran. The optimal approximation factor in density estimation. In Alina Beygelzimer and Daniel Hsu, editors, _Proceedings of the Thirty-Second Conference on Learning Theory_, volume 99 of _Proceedings of Machine Learning Research_, pages 318-341. PMLR, 25-28 Jun 2019.
* [BKSW19] Mark Bun, Gautam Kamath, Thomas Steinke, and Zhiwei Steven Wu. Private hypothesis selection. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alche-Buc, Emily B. Fox, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information ProcessingSystems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 156-167, 2019.
* [CDKL22] Clement L. Canonne, Ilias Diakonikolas, Daniel Kane, and Sihan Liu. Nearly-tight bounds for testing histogram distributions. In _Advances in Neural Information Processing Systems 35 (NeurIPS)_, 2022. To appear.
* [CDSS14] Siu-on Chan, Ilias Diakonikolas, Rocco A. Servedio, and Xiaorui Sun. Near-optimal density estimation in near-linear time using variable-width histograms. In _Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada_, pages 1844-1852, 2014.
* COLT_, 2023.
* [CKM\({}^{+}\)19] Clement L. Canonne, Gautam Kamath, Audra McMillan, Adam D. Smith, and Jonathan R. Ullman. The structure of optimal private tests for simple hypotheses. In Moses Charikar and Edith Cohen, editors, _Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, STOC 2019, Phoenix, AZ, USA, June 23-26, 2019_, pages 310-321. ACM, 2019.
* 22, 2012_, pages 709-728. ACM, 2012.
* [DGKR19] Ilias Diakonikolas, Themis Gouleakis, Daniel M. Kane, and Sankeerth Rao. Communication and memory efficient testing of discrete distributions. In Alina Beygelzimer and Daniel Hsu, editors, _Proceedings of the Thirty-Second Conference on Learning Theory_, volume 99 of _Proceedings of Machine Learning Research_, pages 1070-1106. PMLR, 25-28 Jun 2019.
* [Dia16] Ilias Diakonikolas. Learning structured distributions. In _CRC Handbook of Big Data_, pages 267-283. 2016.
* [DK14] Constantinos Daskalakis and Gautam Kamath. Faster and sample near-optimal algorithms for proper learning mixtures of gaussians. In _Conference on Learning Theory_, pages 1183-1213. PMLR, 2014.
* [DKS17] Ilias Diakonikolas, Daniel M. Kane, and Alistair Stewart. Statistical query lower bounds for robust estimation of high-dimensional gaussians and gaussian mixtures. In Chris Umans, editor, _58th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2017, Berkeley, CA, USA, October 15-17, 2017_, pages 73-84. IEEE Computer Society, 2017.
* 2512, 1996.
* 2637, 1997.
* [DL01] Luc Devroye and Gabor Lugosi. _Combinatorial methods in density estimation_. Springer, 2001.
* [DS18] Yuval Dagan and Ohad Shamir. Detecting correlations with little memory and communication. In Sebastien Bubeck, Vianney Perchet, and Philippe Rigollet, editors, _Proceedings of the 31st Conference On Learning Theory_, volume 75 of _Proceedings of Machine Learning Research_, pages 1145-1198. PMLR, 06-09 Jul 2018.

* [GKK\({}^{+}\)20] Sivakanth Gopi, Gautam Kamath, Janardhan Kulkarni, Aleksandar Nikolov, Zhiwei Steven Wu, and Huanyu Zhang. Locally private hypothesis selection. In Jacob D. Abernethy and Shivani Agarwal, editors, _Conference on Learning Theory, COLT 2020, 9-12 July 2020, Virtual Event [Graz, Austria]_, volume 125 of _Proceedings of Machine Learning Research_, pages 1785-1816. PMLR, 2020.
* Leibniz-Zentrum fur Informatik.
* [KMV12] Adam Tauman Kalai, Ankur Moitra, and Gregory Valiant. Disentangling gaussians. _Commun. ACM_, 55(2):113-120, 2012.
* [KSS18] Pravesh K. Kothari, Jacob Steinhardt, and David Steurer. Robust moment estimation and improved clustering via sum of squares. In _Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2018, Los Angeles, CA, USA, June 25-29, 2018_, pages 1035-1046. ACM, 2018.
* COLT_, pages 503-512, 2008.
* [Pea95] Karl Pearson. Mathematical contributions to the theory of evolution. ii. skew variation in homogeneous material. [abstract]. _Proceedings of the Royal Society of London_, 57:257-260, 1895.
* [Raz18] Ran Raz. Fast learning requires good memory: A time-space lower bound for parity learning. _Journal of the ACM (JACM)_, 66(1):1-18, 2018.
* [SD15] Jacob Steinhardt and John Duchi. Minimax rates for memory-bounded sparse linear regression. In Peter Grunwald, Elad Hazan, and Satyen Kale, editors, _Proceedings of The 28th Conference on Learning Theory_, volume 40 of _Proceedings of Machine Learning Research_, pages 1564-1587. PMLR, 03-06 Jul 2015.
* [Sha14] Ohad Shamir. Fundamental limits of online and distributed algorithms for statistical learning and estimation. In _Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada_, pages 163-171, 2014.
* [SOAJ14] Ananda Theertha Suresh, Alon Orlitsky, Jayadev Acharya, and Ashkan Jafarpour. Near-optimal-sample estimators for spherical gaussian mixtures. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 27, 2014.
* [SSV19] Vatsal Sharan, Aaron Sidford, and Gregory Valiant. Memory-sample tradeoffs for linear regression with small error. In Moses Charikar and Edith Cohen, editors, _Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, STOC_, pages 890-901. ACM, 2019.
* [Vad12] Salil P. Vadhan. Pseudorandomness. _Foundations and Trends(r) in Theoretical Computer Science_, 7(1-3):1-336, 2012.
* 774, 1985.

## Appendix A Random ladder tournament (cont.)

Figure 1 illustrates a visual representation of the algorithm.

### Amplifying the confidence parameter

In this section, we describe how one can amplify the confidence parameter of the random ladder tournament. Note that to keep \(\alpha=3\), we should avoid any two-step process that comes up with a collection of hypotheses to narrow down the possible choices and then pick an acceptable hypothesis among them. Such approaches generally lead to \(\alpha=9\) or higher. Here, we argue that if we run the random ladder tournament for a longer time, a random hypothesis in \(Q\) will be acceptable with very high probability. In particular, if we set \(k\), the number of steps equal to \(\Theta(1/(\delta\,p_{0}))\) (instead of setting \(k=\Theta(1/p_{0})\)), we find an acceptable hypothesis with probability at least \(1-\delta\).

**Corollary A.1**.: _Suppose that we can draw i.i.d. random hypotheses from an arbitrary meta distribution \(\mathcal{D}\) over \(\mathcal{H}\). And, we have a hypothesis \(P\) that we aim to learn properly in \(\mathcal{H}\). Assume we are given parameters \(p_{0}\) and \(\Gamma\) such that the probability that a random hypothesis drawn from the meta distribution is \(\Gamma\)-close to \(P\) is at least \(p_{0}\). For any \(\epsilon,\delta\in(0,1)\), Algorithm 2 can be modified to an \((\alpha=3,\epsilon,\delta)\)-proper learner (with promise) for class \(\mathcal{H}\) by setting \(k\coloneqq\lceil 2\,\lceil 2/\delta\rceil\,/p_{0}\rceil=\Theta(1/( \delta\,p_{0}))\)._

Proof.: Note that \(\delta\) in Theorem 4 is 0.1. Hence, it suffices to focus on \(\delta<0.1\). As we have shown in the proof of Theorem 4, the expected number of unacceptable hypotheses in \(Q\) is at most one. Let \(\hat{H}\) be a random hypothesis in \(Q\). Let \(t\coloneqq\lceil 2/\delta\rceil\), and set \(k\) to \(\lceil 2t/p_{0}\rceil\). It is not hard to see that the probability of \(\hat{H}\) being unacceptable is bounded by \(\delta\):

Figure 1: Random ladder tournament

[MISSING_PAGE_FAIL:15]

Proof.: The proof is very similar to the proof of Lemma 3.1. However, instead of using the Scheffe counts we use the sorted lists to store the samples. The only difference is that to process \(t\) hypotheses, we will need \(O(m\,t\,logn)=O(t\,\log p_{0}^{-1}\log n/\epsilon^{2})\) bits instead of \(O(t^{2}\log m)\) bits. This observation concludes the proof. 

## Appendix B Main result

In this section, we present an algorithm that achieves the tradeoff described in our main theorem. As we have described in Section 1.2, we obtain our main result by running the random ladder tournament on the hypotheses filtered by a filtering algorithm.

Recall that in the random ladder tournament, the algorithm can draw a random hypothesis such that with a probability of at least \(p_{0}\), it is a \(\Gamma\)-close hypothesis. The number of comparisons made in the random ladder tournament is approximately \(k:=\Theta(1/p_{0})\). To reduce \(k\), we design a randomized filtering algorithm that produces an acceptable hypothesis (\((3\,\Gamma+\epsilon^{\prime})\)-close to \(P\)) with a certain descent probability. Roughly speaking, this probability is \(\min(1/\sqrt{n},b/n)\). Although this probability is not very high, it is still greater than \(1/n\), which allows us to execute the random ladder in fewer steps. The only caveat is that we must now run the random ladder tournament with a new parameter \(\Gamma^{\prime}:=3\,\Gamma+\epsilon^{\prime}\). Hence, the accuracy guarantee of the resulting hypothesis will also be based on \(\Gamma^{\prime}\), and we will obtain a \((9\,\Gamma+\Theta(\epsilon^{\prime}))\)-close hypothesis to \(P\). Figure 2 illustrates a visual representation of this approach.

The filtering algorithm operates on the basis of a simple observation: the excellent hypothesis \(H^{*}\) will not lose to any unacceptable hypotheses (those that are \((3\,\Gamma+\epsilon^{\prime})\)-far). Therefore, if there is no decent hypothesis, one can identify the excellent hypothesis \(H^{*}\) through an elimination tournament, where the algorithm pairs all hypotheses, compares them, and repeats this process while only the winners advance to the next round. Conversely, if numerous decent hypotheses exist, it suggests that a random hypothesis will have a reasonable chance of being decent (and hence acceptable). The

Figure 2: The sketch of our approach for the main result

filtering algorithm combines these two strategies and does the following for each with probability a half:

1. It outputs a random hypothesis.
2. It runs an elimination tournament under the assumption that there are no decent hypotheses, then it outputs the result.

This approach leads to finding an excellent or decent hypothesis with an acceptable chance regardless of how scarce the decent hypotheses are.

Note that to run an elimination algorithm, we need to memorize the winners of each round. Hence, implementing the second part above with limited memory is not trivial. Therefore, we focus on a randomly sampled subset of elements and provide an algorithm that is extremely memory efficient. Roughly speaking, the algorithm processes \(n\) hypotheses using only \(O(n)\) bits of memory. This is one of the key components of our result that allows us to avoid extra \(\log n\) factors and achieve the tradeoff \(s\cdot b\approx O(n\log n)\).

The algorithm operates in rounds, employing a tree-like structure. In each round of this algorithm, we group a carefully chosen number of hypotheses and run an all-against-all tournament among them. Next, the winner of each group proceeds to the next round. We continue this process until we find a single winner.

In Section B.1, we describe the group elimination tournament. In Section B.2, we detail how to conduct the group elimination tournament on a random set of \(n^{\prime}\) hypotheses while utilizing only \(n^{\prime}\) bits of memory. In Section B.3, we outline the filtering algorithm. Finally, we conclude with Section B.4, which combines all the pieces together and presents the proof of our main result.

### Group elimination tournament

In this section, we focus on the case that there is no decent hypothesis in \(\mathcal{H}\). That is, every hypothesis that is \(\mathsf{OPT}\)-far from \(P\) will lose to a hypothesis \(H^{*}\) with \(\|H^{*}-P\|_{\mathsf{TV}}=\mathsf{OPT}\). The main advantage of this assumption is that if we have a _knockout_ style tournament: only an \(\mathsf{OPT}\)-close hypothesis will survive to the top. Here, we propose an algorithm that implements a knockout style tournament, which we call group elimination tournament. However, instead of comparing two hypotheses at every step, we run the all-go-against-all tournament among a small group of hypotheses and send the winner to the next round. For simplicity, in this section, we describe our approach in Algorithm 3 where we assume no memory restriction. We prove the performance of this algorithm in Theorem 5. For a discussion on how to implement this algorithm using roughly \(O(n)\) bits of memory, see Section B.2.

```
1:procedureHypothesis-Selection-No-Decent(\(\mathcal{H}\), \(n\), \(\epsilon\), \(\delta\), sample access to \(P\))
2:\(\mathcal{H}_{1}\leftarrow\mathcal{H}\)
3:for\(\ell=1,2,\ldots,L\)do
4:\(\mathcal{G}_{1},\mathcal{G}_{2},\ldots,\mathcal{G}_{s_{\ell+1}}\leftarrow\) Partition \(\mathcal{H}_{\ell}\) into groups of size \(g_{\ell}\).
5:\(\mathcal{H}_{\ell+1}\leftarrow\emptyset\)
6:\(S\coloneqq\left\{(i,j)|\,\exists\,\mathcal{G}\in\left\{\mathcal{G}_{1}, \mathcal{G}_{2},\ldots,\mathcal{G}_{s_{\ell+1}}\right\}\text{ such that }H_{i}\in\mathcal{G}\text{ and }H_{j}\in \mathcal{G}\right\}\)
7: Draw \(m_{\ell}\) samples. For each sample, update the Scheffe counts of \(\mathcal{S}(H_{i},H_{j})\) if \((i,j)\in S\)
8:for\(\mathcal{G}=\mathcal{G}_{1},\mathcal{G}_{2},\ldots,\mathcal{G}_{s_{\ell+1}}\)do
9: Run a All-go-against-all Tournament\((P,\mathcal{G},\epsilon,\delta/(g_{\ell}))\) over the hypotheses in group \(\mathcal{G}\).
10: Add the winner to the set \(\mathcal{H}_{\ell+1}\).
11: Return the hypothesis in \(\mathcal{H}_{L+1}\). ```

**Algorithm 3** Hypothesis selection with no decent hypothesis

**Lemma B.1**.: _Suppose we have a set of \(n\) hypotheses \(\mathcal{H}=\{H_{1},H_{2},\ldots,H_{n}\}\) and an unknown hypothesis \(P\) which we wish to properly learn in \(\mathcal{H}\). Suppose there are no decent hypothesis in \(\mathcal{H}\), that is all hypotheses in \(\mathcal{H}\) are either \(\mathsf{OPT}\)-close, or they will lose to any \(\mathsf{OPT}\)-close hypothesis. For every \(2\leq r\leq n\), there exists an algorithm that uses \(O((\log(\log_{r}n)\cdot\log(1/\delta)+\log n)/\epsilon^{2})\) samples from \(P\), and it outputs \(\tilde{H}\) that is \(\mathsf{OPT}\)-close to \(P\) with probability \(1-\delta\)._Proof.: The algorithm runs in \(L\) rounds. At round \(\ell\in[L]\), we start with \(s_{\ell}\) hypotheses. We partition them into groups of size \(g_{\ell}\). For each round, we draw \(m_{\ell}\coloneqq\Theta(\log(g_{\ell}^{3}/\delta)/\epsilon^{2})\) fresh samples. We keep track of the number of samples in the Scheffe sets of every pair of hypotheses that are in the same group. Using the Scheffe counts, we run an all-go-against-all tournament in each group \(\mathcal{G}_{i}\) with overall confidence probability \(1-\delta/(6\,g_{i})\) and find the winner. Then, we repeat this process among the winners in the next round until we end up with only one hypothesis. See Algorithm 3 for the description.

Parameters:We set the group sizes as follows:

\[g_{1}=\left\lceil\sqrt{r}\right\rceil\,,\qquad g_{\ell}\coloneqq\left\lceil (\sqrt{r})^{1.5^{\ell-1}}\right\rceil\approx g_{\ell-1}^{1.5}\qquad\forall \ell\in\{2,\ldots,L\}\,.\] (2)

We start with \(s_{1}\coloneqq n\). The number of hypotheses in the \(\ell\)-th round is:

\[s_{\ell}\coloneqq\left\lceil\frac{s_{\ell-1}}{g_{\ell-1}}\right\rceil=\left \lceil\frac{n}{\prod_{i=1}^{\ell-1}g_{i}}\right\rceil\leq\left\lceil\frac{n} {r^{1.5^{\ell-1}-1}}\right\rceil\qquad\forall\ell\in\{2,\ldots,L\}\,.\] (3)

The equation above is due to Fact D.2 and Fact D.3.

Observe that we stop our process when only one hypothesis is left: the ultimate winner. Thus, \(L\) is the smallest integer such that \(s_{L+1}\) is one. That is, \(\prod_{\ell=1}^{L}g_{\ell}\geq n\). It is not hard to show that \(L\) is \(O\left(\log\left(\log_{r}n\right)\right)\) due to the following argument. For any \(L\geq\log_{1.5}\left(\log_{r}n+1\right)\), one can show \(\prod_{\ell=1}^{L}g_{\ell}\) is at least \(n\) via Fact D.3:

\[\prod_{\ell=1}^{L}g_{\ell}\geq r^{1.5^{\ell}-1}\geq n\,.\]

Therefore, \(L\) cannot be larger than \(\left\lceil\log_{1.5}\left(\log_{r}n+1\right)\right\rceil\).

We describe how we set our parameters in Table 1.

\begin{table}
\begin{tabular}{|l|l|l|l|l|} \hline Round \# & \# hypotheses & group size & \# groups or \# winners & space \\ \hline
1 & \(s_{1}=n\) & \(g_{1}=\left\lceil\sqrt{r}\right\rceil\) & \(s_{1}/g_{1}\leq n/\sqrt{r}\) & \(O(n\cdot r\cdot\log(\log(1/\delta)/\epsilon))\) \\ \hline
2 & \(s_{2}=\left\lceil n/g_{1}\right\rceil\) & \(g_{2}=\left\lceil(\sqrt{r})^{1.5}\right\rceil\) & \(s_{2}/g_{2}\leq n/r^{1.25}\) & \(O(n\cdot r\cdot\log(\log(1/\delta)/\epsilon))\) \\ \hline
3 & \(s_{3}=\left\lceil n/(g_{1}g_{2})\right\rceil\) & \(g_{3}=\left\lceil(\sqrt{r})^{2.25}\right\rceil\) & \(s_{3}/g_{3}\leq n/r^{2.375}\) & \(O(n\cdot r\cdot\log(\log(1/\delta)/\epsilon))\) \\ \hline \multicolumn{4}{|c|}{\(\vdots\)} \\ \hline \(\ell=L\) & \(s_{\ell}=\left\lceil\frac{n}{\prod_{\ell=1}^{L-1}g_{\ell}}\right\rceil\) & \(g_{\ell}=\left\lceil(\sqrt{r})^{1.5^{\ell-1}}\right\rceil\) & \(s_{\ell}/g_{\ell}\leq\frac{n}{r^{(1.5)^{\ell}-1}}\) & \(O\left(\frac{s_{\ell}}{g_{\ell}}\cdot g_{\ell}^{2}\cdot\log m_{\ell}\right)=O \left(n\cdot r\cdot\log\left(\frac{\log(1/\delta)}{\epsilon}\right)\right)\) \\ \hline \end{tabular}
\end{table}
Table 1: Branching factors in the tree structure Sample complexity:For round \(\ell\), we draw \(m_{\ell}\) fresh samples. Thus, the total number of samples is:

\[\text{\# samples} =\sum_{\ell=1}^{L}m_{\ell}=\sum_{\ell=1}^{L}O\left(\frac{1}{\epsilon ^{2}}\cdot\log\frac{g_{\ell}^{3}}{\delta}\right)\] \[=O\left(\frac{L\cdot\log(1/\delta)}{\epsilon^{2}}\right)+O\left( \frac{1}{\epsilon^{2}}\right)\cdot\sum_{\ell=1}^{L}\log g_{\ell}\] \[=O\left(\frac{L\cdot\log(1/\delta)}{\epsilon^{2}}\right)+O\left( \frac{1}{\epsilon^{2}}\right)\cdot\log\left(\prod_{\ell=1}^{L}g_{\ell}\right)\] \[=O\left(\frac{L\cdot\log(1/\delta)+\log n}{\epsilon^{2}}\right)=O \left(\frac{\log(\log_{r}n)\cdot\log(1/\delta)+\log n}{\epsilon^{2}}\right)\,.\]

Note that in the last line we use the fact that \(\prod_{\ell=1}^{L}g_{\ell}\) is \(\operatorname{poly}(n)\) which is not difficult to prove using the following argument: We define \(L\) to be the smallest integer such that \(\prod_{\ell=1}^{L}g_{\ell}\) is at least \(n\). Thus, \(\prod_{\ell=1}^{L-1}g_{\ell}\) is at most \(n\). Hence, we have:

\[\prod_{\ell=1}^{L}g_{\ell}=g_{L}\cdot\prod_{\ell=1}^{L-1}g_{\ell}\leq O(g_{L- 1}^{1.5})\cdot\prod_{\ell=1}^{L-1}g_{\ell}\leq O\left(\prod_{\ell=1}^{L-1}g_{ \ell}\right)^{2.5}\leq O\left(n^{2.5}\right)\,.\]

Correctness:Suppose \(\mathcal{G}\) is a group in round \(\ell\) that contains an \(\mathsf{OPT}\)-close hypothesis. For the all-go-against-all tournament in round \(\ell\) we use \(m_{\ell}=O(\log(g_{\ell}^{3}/\delta)/\epsilon^{2})\) samples. That implies we estimate the probability of all the Scheffe sets in \(\mathcal{G}\) with accuracy \(\epsilon\) with probability at least \(1-\delta/(6\,g_{\ell})\). Given that we do not have any decent hypothesis, using Fact D.1, the all-go-against-all tournament has to output an \(\mathsf{OPT}\)-close hypothesis with probability \(1-\delta/(6\,g_{\ell})\).

Now, let \(\mathcal{G}^{(1)}\) be the group in round one that \(H^{*}\) belongs to. Let \(\mathcal{G}^{(2)}\) be the group that the winner of \(\mathcal{G}^{(1)}\) belongs to in round two, and similarly we define: \(\mathcal{G}^{(3)},\ldots,\mathcal{G}^{(L)}\). Clearly, the all-go-against-all tournament over \(\mathcal{G}^{(i)}\) will return an \(\mathsf{OPT}\)-close hypothesis with probability \(1-\delta/(6\,g_{\ell})\) if \(\mathcal{G}^{(i)}\) contains an \(\mathsf{OPT}\)-close hypothesis. Thus, using the union bound, the probability that the final group \(\mathcal{G}^{(L)}\) does not output an \(\mathsf{OPT}\)-close hypothesis is bounded by the following:

\[\mathbf{Pr}[\text{\rm Output hypothesis is not $\mathsf{OPT}- \text{close}$}] \leq\sum_{\ell=1}^{L}\frac{\delta}{6\,g_{\ell}}\leq\frac{\delta}{6} \cdot\sum_{\ell=1}^{L}(\sqrt{r})^{-1.5^{\ell-1}}\leq\frac{\delta}{6}\cdot\sum_ {\ell=1}^{L}(\sqrt{r})^{-\ell/2}\] \[\leq\frac{\delta}{6}\cdot\frac{r^{-1/4}}{1-r^{-1/4}}\leq\delta\,.\]

### Memory usage of Algorithm 3

In this section, we provide an approach to implement Algorithm 3 using roughly \(O(n)\) bits of memory and prove our main theorem.

**Theorem 5**.: _Suppose we have a set of \(n\) hypotheses \(\mathcal{H}=\{H_{1},H_{2},\ldots,H_{n}\}\) and an unknown hypothesis \(P\) which we wish to properly learn in \(\mathcal{H}\). Suppose there are no decent hypothesis in \(\mathcal{H}\), that is all hypotheses in \(\mathcal{H}\) are either \(\mathsf{OPT}\)-close, or they will lose to any \(\mathsf{OPT}\)-close hypothesis. For every \(2\leq r\leq n\), there exists an algorithm that uses \(O(n\cdot r\cdot\log(\log(1/\delta)/\epsilon))\) bits of memory and \(O((\log(\log_{r}n)\cdot\log(1/\delta)+\log n)/\epsilon^{2})\) samples from \(P\), and it outputs \(\hat{H}\) that is \(\mathsf{OPT}\)-close to \(P\) with probability \(1-\delta\)._

Proof.: Note that there are two main sets of values that we need to store. First, the values of the Scheffe counts at every round that allow us to perform the all-go-against-all tournaments. Second, the indices of the winner hypotheses that go to the next round. Below, we discuss bounding the memory usage for each part.

**Memory usage of Scheffe counts:** Here, we show to store the Scheffe counts, we need \(O(n\cdot r\cdot\log(\log(1/\delta)/\epsilon))\) bits at every round. For a single group of size \(g_{\ell}\), there are \(O(g_{\ell}^{2})\) pairs of hypotheses, therefore Scheffe sets, so we require \(O(g_{\ell}^{2}\log m_{\ell})\) bits to count the number of samples in all the sets. Thus, in round \(\ell\), we need the following number of bits:

space \[=\#\text{ groups}\cdot O(g_{\ell}^{2}\cdot\log m_{\ell})=O\left( \frac{s_{\ell}}{g_{\ell}}\cdot g_{\ell}^{2}\cdot\log m_{\ell}\right)\] \[=O\left(s_{\ell}\cdot g_{\ell}\cdot\left(\log g_{\ell}+\log \left(\frac{\log(1/\delta)}{\epsilon^{2}}\right)\right)\right)\] \[=n\cdot\log\left(\frac{\log(1/\delta)}{\epsilon}\right)\cdot O \left(\frac{s_{\ell}}{n}\cdot g_{\ell}\cdot(\log g_{\ell})\right)\]

Next, we show the last term in the last line is \(O(r)\). For round \(1\), the memory bound holds since \(\sqrt{r}\cdot\log(\sqrt{r})\leq r\). Now for \(\ell\geq 2\) using Equation (3), we have:

\[O\left(\frac{s_{\ell}}{n}\cdot g_{\ell}\cdot(\log g_{\ell})\right)=O\left( \frac{g_{\ell}\cdot\log g_{\ell}}{r^{(1.5)^{\ell-1}-1}}\right)=O\left(\frac{r \cdot g_{\ell}\cdot\log g_{\ell}}{g_{\ell}^{2}}\right)=O(r)\,.\]

Hence, the bit complexity at every step is:

\[O\left(n\cdot r\cdot\log\left(\frac{\log(1/\delta)}{\epsilon}\right)\right)\,.\]

**Storing the indices:** Note that to run the comparisons, one needs the true indices of the hypotheses to query the PDF-comparator or draw a sample. Thus, in the trivial implementation of our algorithm, we potentially need \(O(n\log n)\) bits to keep track of the indices of the winners at every round. In this section, we explain how one can implement our algorithm in a memory-efficient manner, so keeping track of the winners' indices does not take more than \(O(n)\) at every round.

We start by the following assumption that there is natural ordering among the hypotheses: \(H_{1},H_{2},\ldots,H_{n}\). In this way, for the first round we do not need to remember the set of indices, we only need to remember \(n\). At every level, we group "adjacent" hypotheses together and preserve this natural ordering. More precisely, at round one with group size \(g_{1}\), the groups are:

\[\mathcal{G}_{1} \coloneqq\left\{H_{1},H_{2},\ldots,H_{g_{1}}\right\},\] \[\mathcal{G}_{2} \coloneqq\left\{H_{g_{1}+1},H_{g_{1}+2},\ldots,H_{2g_{1}}\right\},\] \[\quad\vdots\] \[\mathcal{G}_{\left\lceil\frac{n}{g_{1}}\right\rceil} \coloneqq\left\{H_{\left\lfloor\frac{n-1}{g_{1}}\right\rfloor g_{ 1}+1},H_{\left\lfloor\frac{n-1}{g_{1}}\right\rfloor g_{1}+2},\ldots,H_{n} \right\}.\]

For the next round, we keep a list of \(\lceil n/g_{1}\rceil\) winners. However, instead of fully writing down the index of the winners, we use \(O(\log(g_{1}))\) bits per each, and save the index of the winner within the group. For the next round, we partition the hypotheses into groups of size \(g_{2}\) keeping the "adjacent" hypotheses in the same group. We repeat the same process to store the winners. However now, each winner are "representing" \(O(g_{1}\cdot g_{2})\) hypotheses, so we need \(O(\log(g_{1}\cdot g_{2}))\) bits to store its index. We continue this approach. At round \(\ell>1\), we have a list of \(s_{\ell}=O(n/\prod_{i}^{\ell-1}g_{i})\) hypotheses, and we require \(O(\log\prod_{i}^{\ell-1}g_{i})\) many bits to do so. Thus, at every step we need \(O(n)\) bits of memory.

Note that at every point in the process, it is easy to compute the actual index from the stored index. If right before starting round \(i\), the \(j\)-th index in the list is \(k\), then the true index is:

\[\textsc{NewIndex}(i,j,k)=(j-1)\cdot\prod_{i}^{\ell-1}g_{i}+k\,.\]

### Filtering acceptable hypotheses

In this section, we describe a filtering algorithm that allow us to pick an acceptable hypothesis with a modest chance. Our algorithm is simple we draw roughly \(n^{\prime}\ll n\) hypothesis at random. If one of these hypothesis is \(\mathsf{OPT}\)-close, and there is no decent hypothesis among them, Algorithm 3 returns an excellent hypothesis. On the other hand, if there are a lot of decent hypotheses, then there is a modest chance that a randomly selected hypothesis is decent, hence an acceptable one.

In the description of Algorithm 3, we assume we have a class of hypotheses of size \(n\), and roughly \(O(n)\) bits of memory. However, we aim to use this algorithm in a setting that we can process \(n^{\prime}\) random hypotheses in roughly \(O(n^{\prime})\) bits of memory. The primary challenge here is that we need roughly \(O(n^{\prime}\log n)\) many bits to memorize which hypotheses are participating, and we cannot use the "natural ordering" of the hypotheses that we use in the proof of Theorem 5. Thus, we are looking for a mapping that maps indices in \([n^{\prime}]\) to a random set of indices in \([n]\) in a memory-efficient manner. To do so, we relax our requirement regarding that we need to draw \(n^{\prime}\) random hypotheses uniformly from \(\mathcal{H}\). Instead, we are looking for a set of random pairwise independent indices while finding the index of the selected elements requires small amount of memory. This relaxation gives us a process that _filters_ acceptable hypotheses:

**Theorem 6**.: _Suppose we have a set of \(n\) hypotheses \(\mathcal{H}=\{H_{1},H_{2},\ldots,H_{n}\}\) and an unknown hypothesis \(P\) which we wish to properly learn in \(\mathcal{H}\). For every positive integer \(n^{\prime}<n\), there exists an algorithm that uses \(O(n^{\prime}\cdot\log(1/\epsilon))\) bits of memory and \(O((\log n)/\epsilon^{2})\) samples from \(P\), and it outputs \(\hat{H}\) that is excellent or decent with the probability:_

\[\mathbf{Pr}\left[\hat{H}\text{ is excellent or decent.}\right]\geq\min\left( \frac{n^{\prime}}{16\,n}\,,\frac{1}{4\,n^{\prime}}\right)\,.\]

Proof.: First, we start off by describing how we randomly pick \(n^{\prime}\) hypotheses from the set of \(n\) hypotheses. We use a standard technique for generating pairwise independent random indices described in [12, Chapter 3.5]. The following is the description of a randomized mapping that for any given index \(x\in[n^{\prime}]\) it gives an index \(i\) in \([n]\) that can be computed in a memory efficient manner.

Let \(q\) be a prime number between \(n\) and \(2n\) (\(n<q<2n\)). Such \(q\) always exists via the Bertrand-Chebyshev theorem for any \(n\geq 2\). Without loss of generality, we assume we have \(q\) hypotheses by adding fake hypotheses to \(\mathcal{H}\) that lose to any other hypothesis. Now, consider the finite field of \(\mathbb{Z}_{q}\). Proposition 3.24 in [12] implies that if we use two random numbers \(a\) and \(b\) in \(\mathbb{Z}_{q}\), then the following mapping generates a set of pairwise independent indices:

\[f_{a,b}(x)\coloneqq a\cdot x+b\;(\text{mod}\;q)\,.\]

More precisely, by iterating \(x\) from \(1\) to \(n^{\prime}\), this randomized mapping selects \(n^{\prime}\) hypotheses with the following indices: \(f_{a,b}(1),f_{a,b}(2),\ldots,f_{a,b}(n^{\prime})\).3 Note that to compute this mapping, we only need to memorize \(a\) and \(b\), which requires \(O(\log n)\) bits, significantly smaller compared to \(O(n^{\prime}\log n)\) which we would need to memorize \(n^{\prime}\) random indices. Using the pairwise independence of the indices generated by this mapping, it is not too difficult to show that with some modest chance we pick exactly one excellent hypothesis and no decent hypothesis with this mapping.

Footnote 3: Without loss of generality, assume \(H_{0}\) is \(H_{q}\).

**Lemma B.2**.: _For a prime \(q\), assume we have a class of \(q\) hypotheses that contains \(t_{e}\geq 1\) excellent hypotheses and \(t_{d}\) decent hypotheses. Let \(a\neq 0\) and \(b\) be two random numbers in \(\mathbb{Z}_{q}\) selected uniformly at random. Let \(\mathcal{H}^{\prime}\) be the set of following hypotheses: \(\{H_{f_{a,b}(x)}:x\in[n^{\prime}]\}\) for which \(f_{a,b}(x)\coloneqq a\cdot x+b\;(\text{mod}\;q)\). If \((t_{e}+t_{d})/q\leq 1/(2n^{\prime})\), then the probability that \(\mathcal{H}^{\prime}\) contains exactly one excellent hypothesis and no decent hypothesis is at least \(n^{\prime}\cdot t_{e}/(2\,q)\)._

See Section D for the proof of this lemma.

The algorithm:Assume \(\mathcal{H}\) contains \(t_{e}\geq 1\) prefect hypotheses and \(t_{d}\) decent hypotheses. The algorithm is simple: with probability a half we set \(\hat{H}\) to be a random hypothesis from \(\mathcal{H}\). And, with probability a half we use the randomize mapping to pick \(n^{\prime}\) hypotheses and run Algorithm 3 with parameters \(\delta=0.5\) and \(r=2\) on them. And, we output the output of that algorithm. Figure 3 shows a visual representation of the filtering algorithm.

The probability of getting an acceptable hypothesis by picking a random one is \((t_{e}+t_{d})/n\). Note that if this quantity is at least \(1/2\,n^{\prime}\), the statement of the lemma is true. Thus, assume \((t_{e}+t_{d})/n\) is less than \(1/2\,n^{\prime}\) which implies that \((t_{e}+t_{d})/q\) is less than \(1/2\,n^{\prime}\). Given this condition, by the above lemma, our randomized mapping selects a set of hypotheses with only one excellent hypothesis and no other acceptable hypotheses with probability at least \(n^{\prime}\cdot t_{e}/(2\,q)\). And, since we set \(\delta\) to a half, with probability at least a half, Algorithm 3 will choose the excellent hypothesis as its output. Using that \(q<2\,n\) and \(\textbf{Pr[}C=\text{tail}\textbf{]}=1/2\), with probability at least \(n^{\prime}\cdot t_{e}/(8\,q)\leq n^{\prime}/(16\,n)\), \(\hat{H}\) is the excellent hypothesis as desired in the statement. Hence, the proof is complete.

Figure 3: The sketch of the filtering algorithm

### Putting everything together

**Theorem 1**.: _There exists a constant \(c\) for which the following holds. Let \(\mathcal{H}\) be an arbitrary class consisting of \(n\) distributions, and let \(\epsilon>0\). For every natural number \(b\) where \(c\cdot(\log n+\log((\log n)/\epsilon))\leq b\leq n\), there exists an \((\alpha=9,\epsilon,\delta=0.1)\)-proper learner (with promise) for \(\mathcal{H}\) using \(b\) bits of memory and the following number of samples:_

\[s=\left\{\begin{array}{ll} O\left(\frac{n\,\log n}{b}\cdot \frac{1}{\epsilon^{2}}\log\left(\frac{1}{\epsilon}\right)\right)&\text{ when }b\leq\frac{n}{\log\log n}\,,\\ &\\ O\left(\frac{n\,\log n}{b}\cdot\frac{1}{\epsilon^{2}}\log\left(\frac{\log n}{ \epsilon}\right)\right)&\text{ when }\frac{n}{\log\log n}\leq b\leq n\,.\end{array}\right.\]

Proof.: Here, we combine the filtering algorithm and the random ladder tournament to prove the upper bound. We first use the filtering algorithm, Algorithm 4, to increase the probability of getting an acceptable hypothesis (\(3\,\Gamma+\epsilon^{\prime}\)-close \(P\)) with parameter \(\epsilon^{\prime}=\epsilon/4\). Then, we use the random ladder tournament, Algorithm 2, to select an acceptable hypothesis among the hypotheses that have passed the filtering. Note that in the random ladder tournament, we need to use a new \(\Gamma^{\prime}\) parameter, that is, \(3\,\Gamma+\epsilon^{\prime}\).

Here, we use the memory bounds provided in Theorem 6 and Lemma 3.1 for these two algorithms. Fix two small constants \(c_{0},c_{1}\leq 1\) that we determine later. Let \(t\) be the largest integer such that:

\[t^{2}\log\left(\frac{\log n}{\epsilon}\right)t+t\,\log n\leq c_{0}b\,.\]

Note that given our lower bound for \(b\), one can find a \(t\) that is at least two. Set \(n^{\prime}\) as follows:

\[n^{\prime}\coloneqq\min\left(\frac{c_{1}\cdot b}{\log(1/\epsilon)}\,,2\, \sqrt{n}\right)\,.\]

We consider two cases in the following:

**Case 1: \(\boldsymbol{n^{\prime}<2\sqrt{n}}\).** In this case, \(n^{\prime}\) is equal to \(c_{1}\cdot b/\log(1/\epsilon)\). One can run Algorithm 4 and find an acceptable hypothesis with probability \(p_{0}\coloneqq\min(n^{\prime}/(16\,n),1/(4\,n^{\prime}))=n^{\prime}/(16\,n)\). Now in this case, we invoke Algorithm 4\(O(1/p_{0})\) times, and in each round we use \(O(\log n/\epsilon^{2})\) samples. Furthermore, the random ladder tournament uses \(O((\log p_{0}^{-1})/(t\,p_{0}\,\epsilon^{2}))\) samples. Therefore overall, our sample complexity is:

\[O\left(\frac{1}{p_{0}}\cdot\left(1+\frac{1}{t}\right)\cdot \frac{\log n}{\epsilon^{2}}\right) =O\left(\frac{1}{p_{0}}\cdot\frac{\log n}{\epsilon^{2}}\right)=O \left(\frac{n}{n^{\prime}}\cdot\frac{\log n}{\epsilon^{2}}\right)\] \[=O\left(\frac{1}{p_{0}}\cdot\frac{\log n}{\epsilon^{2}}\right)=O \left(\frac{n\,\log n}{b\,\epsilon^{2}}\cdot\log\frac{1}{\epsilon}\right)\,.\]

**Case 2: \(\boldsymbol{n^{\prime}=2\sqrt{n}}\).** Similar to the previous case, we would like to run Algorithm 4 and find an acceptable hypothesis with probability \(p_{0}\coloneqq\min(n^{\prime}/(16\,n),1/(4\,n^{\prime}))=1/(4\,n^{\prime})\). Now, since \(n^{\prime}\leq c_{1}\cdot b/\log(1/\epsilon)\), we may be able to run multiple instances of Algorithm 4 in parallel while using the same number of samples. Let \(r=c_{1}\cdot b/(n^{\prime}\cdot\log(1/\epsilon))\). Now, the number of samples we use for this stage is:

\[O\left(\frac{1}{r\,p_{0}}\cdot\frac{\log n}{\epsilon^{2}}\right)=O\left(\frac {n^{\prime}}{r}\cdot\frac{\log n}{\epsilon^{2}}\right)=O\left(\frac{\log(1/ \epsilon)\cdot\log n}{b\,\epsilon^{2}}\right)\,.\]

Now, we focus on the number of samples we need for the random ladder tournament. Note that \(t\) should be:\[t=\Theta\left(\min\left(\frac{b}{\log n},\sqrt{\frac{b}{\log\left((\log n)/\epsilon \right)}}\right)\right)\,.\]

Thus, we use the following amount of samples:

\[O\left(\frac{1}{t\,p_{0}}\cdot\frac{\log n}{\epsilon^{2}}\right) =O\left(\frac{\sqrt{n}}{b\cdot\min\left((\log n)^{-1},\left(\sqrt {b}\cdot\log\left(\log n\right)/\epsilon\right)\right)^{-1}}\cdot\frac{\log n} {\epsilon^{2}}\right)\] \[=O\left(\frac{\max\left(\log n,\sqrt{b}\log\left((\log n)/ \epsilon\right)\right)}{\sqrt{n}}\cdot\frac{n\,\log n}{b\,\epsilon^{2}}\right) =O\left(\frac{n\,\log n}{b\,\epsilon^{2}}\log\left(\frac{\log n}{\epsilon} \right)\right)\]

Putting these two cases together, we obtain the desire sample-memory tradeoffs:

\[\#\text{samples}\cdot\#\text{bits}=O\left(\frac{n\,\log n}{\epsilon^{2}}\log \left(\frac{\log n}{\epsilon^{2}}\right)\right)\,.\]

Regarding the accuracy guarantee, it is not hard to see that in the filtration algorithm, with a modest probability, we pick a distribution that is \((3\,\Gamma+\epsilon^{\prime})\)-close to \(P\). Now, we run the random ladder tournament with a new \(\Gamma^{\prime}=3\,\Gamma+\epsilon^{\prime}\). Then, we get a hypothesis with accuracy:

\[3\,\Gamma^{\prime}+\epsilon^{\prime}=3\,(3\,\Gamma+\epsilon^{\prime})+\epsilon ^{\prime}=9\,\Gamma+4\epsilon^{\prime}=9\,\Gamma+\epsilon\,.\]

Hence, the proof is complete. 

## Appendix C Proper learner without promise

Throughout this paper, we mainly focus on proper learners with promise, for which the algorithm is given a parameter \(\Gamma\), and we are promised that \(\mathsf{OPT}\) is at most \(\Gamma\). In this section, we focus on the case where no such information is available to the algorithm. First, we formally define a proper learner without promise.

**Definition C.1**.: _Suppose \(\mathcal{A}\) has sample access to an unknown distribution \(P\). And, it can query the probabilities of the Scheffe sets according to each \(H_{i}\in\mathcal{H}\) and a PDF-comparator for every pair of hypotheses in \(\mathcal{H}\). We say algorithm \(\mathcal{A}\) is an \((\alpha,\epsilon,\delta)\)-proper learner for \(\mathcal{H}\) if the following holds: for every distribution \(P\), and for every class of \(n\) distributions \(\mathcal{H}\), with probability at least \(1-\delta\) over its input sample (drawn i.i.d. from \(P\)) and internal coin tosses, \(\mathcal{A}\) outputs a distribution \(\hat{H}\in\mathcal{H}\) such that:_

\[\|P-\hat{H}\|_{\text{TV}}\leq\alpha\cdot\mathsf{OPT}+\epsilon\,.\]

Here, we provide a reduction from a proper learner without promise to a proper learner with promise. At a high level, we perform a binary search over possible values of \(\Gamma\) in \((0,1)\). At every step, we try a value of \(\Gamma_{m}\) and run \(\mathcal{A}\) with \(\Gamma=\Gamma_{m}\). The main challenge is that regardless of \(\Gamma_{m}\) being at least \(\mathsf{OPT}\) or not, \(\mathcal{A}\) may return a hypothesis that seems close to \(P\), and it is hard to refute that the suggested value of \(\Gamma\) is at least \(\mathsf{OPT}\). For this reason, we look into the output of \(\mathcal{A}\), say \(\tilde{H}\), and see if we find evidence that \(\tilde{H}\) is far from \(P\). We check the distance between \(\tilde{H}\) and \(P\) on every Scheffe set of \(\tilde{H}\). More precisely, we check if \(W(\tilde{H})\) is larger than \((\alpha\cdot\mathsf{OPT}+\epsilon)\) where \(W(H)\) is defined as follows for every hypothesis \(H_{i}\):

\[W(H_{i})\coloneqq\max_{j\in[n]\setminus\{i\}}\left|H_{i}\left(\mathcal{S}(H_ {i},H_{j})\right)-P\left(\mathcal{S}(H_{i},H_{j})\right)\right|\,.\]Now, if \(W(\tilde{H})\) is larger than \((\alpha\cdot\mathsf{OPT}+\epsilon)\), then one can imply that \(\tilde{H}\) is not \((\alpha\cdot\mathsf{OPT}+\epsilon)\)-close to \(P\), therefore, \(\Gamma_{m}\) must have been less than \(\mathsf{OPT}\). On the other hand, if \(W(\tilde{H})\) is at most \((\alpha\cdot\mathsf{OPT}+\epsilon)\), one can show \(\tilde{H}\) is not too far away from \(P\) (even when \(\Gamma_{m}\) is less than \(\mathsf{OPT}\)). Putting these observations together and accounting for errors in estimation of \(W(\tilde{H})\), we obtain a reduction that is described in Algorithm 5, and we prove its accuracy in Theorem 7.

```
1:procedure\(\mathcal{B}(\mathcal{H},\,\alpha,\,\epsilon,\,\delta,\delta^{\prime},\,\) sample access to \(P\), and oracle access to \(\mathcal{A}\))
2:\(\Gamma_{\ell}\gets 0\)
3:\(\Gamma_{h}\gets 1\)
4:\(\hat{H}\leftarrow\mathcal{A}\left(\mathcal{H},\,\Gamma_{h},\,\epsilon,\,\delta\right)\)
5:while\(\Gamma_{h}-\Gamma_{\ell}>\epsilon\)do
6:\(\Gamma_{m}\leftarrow\dfrac{\Gamma_{h}+\Gamma_{\ell}}{2}\)
7:\(\tilde{H}\leftarrow\mathcal{A}\left(\mathcal{H},\,\Gamma_{m},\epsilon,\delta\right)\)
8:\(\tilde{W}(\tilde{H})\leftarrow\) Estimate \(W(\tilde{H})\) with error at most \(\epsilon^{\prime}\) and with probability at least \(1-\delta^{\prime}\).
9:if\(\tilde{W}(\tilde{H})\leq\alpha\,\Gamma_{m}+\epsilon+\epsilon^{\prime}\)then
10:\(\Gamma_{h}\leftarrow\Gamma_{m}\)
11:\(\hat{H}\leftarrow\tilde{H}\)
12:else
13:\(\Gamma_{\ell}\leftarrow\Gamma_{m}\)
14:Output \(\hat{H}\). ```

**Algorithm 5** Reduction from a proper learner without promise to a proper learner with promise

Theorem 7.: _Fix five parameters \(\epsilon,\epsilon^{\prime},\delta,\delta^{\prime}\in(0,1)\), and \(\alpha\geq 1\). Assume that \(\mathcal{A}\) is an \((\alpha,\epsilon,\delta)\)-proper learner (with promise). Suppose that we can estimate the probabilities of all the Scheffe sets according to \(P\) with additive error at most \(\epsilon^{\prime}\) and with probability at least \(1-\delta^{\prime}\). Then, \(\mathcal{B}\), described in Algorithm 5, is an \((\alpha+2,(\alpha+1)\,\epsilon+2\,\epsilon^{\prime},t\cdot\delta+\delta^{ \prime})\) proper learner (without promise) where \(t\) is defined as \(\lceil\log_{2}(1/\epsilon)\rceil+1\)._

Proof.: Our goal here is to show that for the output hypothesis \(\hat{H}\) with probability at least \(1-t\cdot\delta+\delta^{\prime}\), we have:

\[\|\hat{H}-P\|_{\mathsf{TV}}\leq(\alpha+2)\,\mathsf{OPT}+(\alpha+1)\epsilon+2 \,\epsilon^{\prime}\,.\] (4)

Observe that at every iteration of the while loop in the algorithm, \(\Gamma_{h}-\Gamma_{\ell}\) is divided by two. Thus, the while loop takes \(\lceil\log_{2}(1/\epsilon)\rceil\) iterations. Thus, \(t\) is the number of times that we invoke \(\mathcal{A}\) in Algorithm 5. Each time \(\mathcal{A}\) works as expected with probability \(1-\delta\). Hence, using the union bound, with probability \(1-t\cdot\delta\), we can assume all invocations of \(\mathcal{A}\) result in correct answers. In addition, with probability \(1-\delta^{\prime}\), all the estimates we use for \(P\left(\mathcal{S}(\hat{H},H_{i})\right)\) in Line 8 are accurate up to an additive error \(\epsilon^{\prime}\). Hence, \(\left|\tilde{W}(\tilde{H})-W(\tilde{H})\right|\) is at most \(\epsilon^{\prime}\) with probability at least \(1-\delta^{\prime}\). Hence, by the union bound, the following event happens with probability at test \(1-(t\cdot\delta+\delta^{\prime})\): In every invocation of \(\mathcal{A}\), if \(\mathsf{OPT}\) is below the given parameter \(\Gamma\), then \(\mathcal{A}\) returns a hypothesis, \(\tilde{H}\), that is \((\alpha\Gamma+\epsilon)\)-close to \(P\). And, \(\tilde{W}(\tilde{H})\) is at most \(\alpha\Gamma+\epsilon+\epsilon^{\prime}\). Hence, for the rest of this proof, we assume this event holds, and we prove Equation 4.

Consider \(\Gamma_{h}\) and \(\Gamma_{\ell}\) in the final iteration of while loop in Algorithm 5. Observe that the output \(\hat{H}\) is what \(\mathcal{A}\) has found when it was invoked with input \(\Gamma=\Gamma_{h}\). Now, we consider two cases as follows:

Case 1: \(\mathsf{OPT}\leq\Gamma_{h}\):In this case, \(\hat{H}\) is \((\alpha\cdot\Gamma_{h}+\epsilon)\)-close to \(P\) by our assumption that \(\mathcal{A}\) works as expected. Thus, \(\|\hat{H}-P\|_{\mathsf{TV}}\) is at most \(\alpha\cdot\Gamma_{h}+\epsilon\). This upper bound is written based on \(\Gamma_{h}\), while we aim to find an upper bound on the distance that only contains \(\mathsf{OPT}\). For this purpose, we need to use the fact that why the binary search algorithm stopped at this particular \(\Gamma_{h}\). For \(\Gamma_{\ell}\), we have two possibilities: either \(\Gamma_{\ell}\) is zero; Or, we have run \(\mathcal{A}\) with \(\Gamma=\Gamma_{\ell}\), and we have not found a desired \(\tilde{H}\).

In both cases, \(\Gamma_{\ell}\) is a lower bound on \(\mathsf{OPT}\). Thus, we have:

\[\mathsf{OPT}\geq\Gamma_{\ell}\geq\Gamma_{h}-\epsilon\,.\]

Now, using this relationship between \(\Gamma_{h}\) and \(\mathsf{OPT}\), we obtain:

\[\|\hat{H}-P\|_{\mathrm{TV}}\leq\alpha\cdot\Gamma_{h}+\epsilon\leq\alpha\cdot \mathsf{OPT}+\left(\alpha+1\right)\epsilon\,,\]

implying Equation 4.

Case 2: \(\mathsf{OPT}>\Gamma_{h}\):In this case, clearly, \(\Gamma_{h}\) cannot be one (since \(\mathsf{OPT}\) is at most one). Thus, \(\Gamma_{h}\) was set in Line 10 implying that \(\hat{W}(\hat{H})\) is at most \(\alpha\,\Gamma_{h}+\epsilon+\epsilon^{\prime}\). Since we assumed the estimation of \(W(\hat{H})\) was accurate up to error \(\epsilon^{\prime}\), we have:

\[W(\hat{H})\leq\tilde{W}(\hat{H})+\epsilon^{\prime}\leq\alpha\,\Gamma_{h}+ \epsilon+2\,\epsilon^{\prime}\,.\]

Given this bound on \(W(\hat{H})\), we show that \(\hat{H}\) cannot be far from \(P\). Let \(H^{*}\) be a hypothesis in \(\mathcal{H}\) that is \(\mathsf{OPT}\)-close to \(P\). Then, we use the definition of the Scheffe sets and the triangle inequality to bound the distance between \(\hat{H}\) and \(P\):

\[\|\hat{H}-P\|_{\mathrm{TV}} \leq\|\hat{H}-H^{*}\|_{\mathrm{TV}}+\|H^{*}-P\|_{\mathrm{TV}}\] (by triangle inequality) \[=\left|\hat{H}\left(\mathcal{S}\left(\hat{H},H^{*}\right)\right) -H^{*}\left(\mathcal{S}\left(\hat{H},H^{*}\right)\right)\right|+\|H^{*}-P\|_{ \mathrm{TV}}\] (by dfn. of Scheffe set) \[\leq\left|\hat{H}\left(\mathcal{S}\left(\hat{H},H^{*}\right) \right)-P\left(\mathcal{S}\left(\hat{H},H^{*}\right)\right)\right|\] \[+\left|P\left(\mathcal{S}\left(\hat{H},H^{*}\right)\right)-H^{*} \left(\mathcal{S}\left(\hat{H},H^{*}\right)\right)\right|+\|H^{*}-P\|_{ \mathrm{TV}}\] (by triangle inequality) \[\leq\left|\hat{H}\left(\mathcal{S}\left(\hat{H},H^{*}\right) \right)-P\left(\mathcal{S}\left(\hat{H},H^{*}\right)\right)\right|+2\,\|H^{*} -P\|_{\mathrm{TV}}\,.\]

Now, we use the definition of \(W(\hat{H})\) and conclude:

\[\|\hat{H}-P\|_{\mathrm{TV}} \leq W(\hat{H})+2\,\|H^{*}-P\|_{\mathrm{TV}}=W(\hat{H})+2\, \mathsf{OPT}\] \[\leq\alpha\,\Gamma_{h}+\epsilon+2\,\epsilon^{\prime}+2\,\mathsf{ OPT}\leq\left(\alpha+2\right)\mathsf{OPT}+\epsilon+2\,\epsilon^{\prime}\,.\]

Therefore, the proof is complete. 

**Corollary C.2**.: _For every parameters \(\epsilon^{\prime\prime}\) and \(\delta^{\prime\prime}\) in \((0,1)\), there exists an \((\alpha=5,\epsilon=\epsilon^{\prime\prime},\delta=\delta^{\prime\prime})\)-proper learner (without promise) that uses \(O\left(\log(n/\delta^{\prime\prime})/\left(\epsilon^{\prime\prime}\right)^{2}\right)\) samples and runs \(\tilde{O}\left(n/\left(\delta^{\prime\prime}\ \left(\epsilon^{\prime\prime}\right)^{2}\right)\right)\) time._

Proof.: First, we start off with the parameters: We set \(\epsilon\coloneqq\epsilon^{\prime\prime}/6\), \(\epsilon^{\prime}\coloneqq\epsilon^{\prime\prime}/6\), \(\delta^{\prime}=\delta^{\prime\prime}/2\), and \(\delta=\delta^{\prime\prime}/(2\,t)\) where \(t\coloneqq\lceil\log_{2}(1/\epsilon)\rceil+1\) is the number of iterations in Algorithm 5. Using the Chernoff bound and reusing all samples, we can estimate all the Scheffe sets with additive error at most \(\epsilon^{\prime}\) with probability at least \(1-\delta^{\prime}\).

Note that based on Corollary A.1, the random ladder tournament (with an arbitrarily small confidence parameter) is a \((\alpha=3,\epsilon,\delta)\)-proper learner (with promise) that uses \(O\left(\log(n/\delta)/\epsilon^{2}\right)\) samples and runs in \(O\left(n\,\log(n/\delta^{\prime\prime})/\left(\delta^{\prime\prime}\ \left( \epsilon^{\prime\prime}\right)^{2}\right)\right)\) time. We use this learner as our \(\mathcal{A}\) and run the reduction algorithm \(\mathcal{B}\) with the parameters we specified earlier. Now, using Theorem 7, we obtain a proper learner without promise that satisfies the following guarantee with probability at least \(1-\delta^{\prime\prime}=1-(\delta^{\prime}+t\cdot\delta)\):

\[\|\hat{H}-P\|_{\mathrm{TV}}\leq(\alpha+2)\cdot\mathsf{OPT}+(\alpha+1)\epsilon+ 2\,\epsilon^{\prime}=5\cdot\mathsf{OPT}+\epsilon^{\prime\prime}\,.\]Regarding the time complexity, the while loop in Algorithm 5 is repeated \(t\) times. In each iteration, we compute \(W(\hat{H})\) which takes \(O\left(n\,\log\left(n/\delta^{\prime\prime}\right)/\left(\epsilon^{\prime\prime} \right)^{2}\right)\) time, and we call the random ladder tournament (i.e, \(\mathcal{A}\)). Hence, the overall time complexity is:

\[O\left(t\cdot\frac{n\,\log\left(n/\delta^{\prime\prime}\right)}{\delta^{\prime \prime}\left(\epsilon^{\prime\prime}\right)^{2}}\right)=O\left(\frac{n\cdot \log(n/\delta^{\prime\prime})\cdot\log\left(1/\epsilon^{\prime\prime}\right)}{ \delta^{\prime\prime}\left(\epsilon^{\prime\prime}\right)^{2}}\right)=\tilde{O }\left(\frac{n}{\delta^{\prime\prime}\left(\epsilon^{\prime\prime}\right)^{2}} \right)\,.\]

Hence, the proof is complete. 

Memory-data consumption of reduction algorithm:Besides the sample and memory usage of \(\mathcal{A}\), \(\mathcal{B}\) needs to keep track of \(O(1)\) numbers and also computes \(\tilde{W}(\hat{H})\). Now, given \(b\) bits of memory, we can partition \(\mathcal{H}\) into consecutive blocks of size \(r=b/(\log(\log(n)/\epsilon))\). At every round, we process one block of size \(r\), we keep track of the Scheffe counts of \(\hat{H}\) and the hypotheses in the block. For each round we use \(O(\log(n)/\epsilon^{2})\) samples, ane we have \(O(n/r)\) rounds. Thus, the total number of samples is \(s\coloneqq O(n/r\cdot(\log(n)/\epsilon^{2}))\). Thus, for this part we will have:

\[s\cdot b=O\left(\frac{n\log n}{\epsilon^{2}}\cdot\log\left(\frac{\log n}{ \epsilon}\right)\right)\,.\]

**Remark 8**.: _Using the reduction algorithm with the memory-data tradeoff described above, one can translate our main theorem, Theorem 1, to a \((\alpha=11,\epsilon,0.1)\)-proper learner (without promise) incurring additional factors of \(\operatorname{poly}\left(\log\log(n),\log(1/\epsilon)\right)\)._

## Appendix D Auxiliary lemmas and facts

**Fact D.1**.: _[adapted from Theorem 4 in [1]] For every pair \(i,j\in[n]\), let \(q_{ij}\) denote an estimate of the probabilities of the Scheffe set of \(H_{i}\) and \(H_{j}\) according to \(P\). Let \(\hat{H}\) be the hypothesis that minimizes of the following:_

\[\hat{H}\coloneqq\operatorname*{arg\,max}_{H_{i}\in\mathcal{H}}\min_{j\in[n] \setminus\{i\}}\left|H_{i}\left(\mathcal{S}(H_{i},H_{j})\right)-q_{ij}\right|\,.\]

_For every \(\epsilon\) and \(\delta\), if we estimate \(q_{ij}\) using \(O(\log(n/\delta)/\epsilon^{2})\) samples from \(P\), then \(\hat{H}\) satisfies the following guarantee:_

\[\|\hat{H}-X\|_{\mathcal{IV}}\leq 3\,\mathsf{OPT}+\epsilon\,.\]

**Lemma 2.1**.: _Upon receiving three parameters: \(\Gamma\), \(\epsilon\), and \(\delta\), Algorithm 1 uses \(O(\log(1/\delta)/\epsilon^{2})\) samples and satisfies the following guarantees with probability at least \(1-\delta\):_

* _If_ \(H_{1}\) _is_ \(\Gamma\)_-close to_ \(P\)_, then the algorithm returns_ \(H_{1}\)_._
* _If_ \(H_{1}\) _is_ \(\left(\max\left(\Gamma,\|H_{2}-P\|_{\mathcal{IV}}\right)+2\,\|H_{2}-P\|_{ \mathcal{IV}}+\epsilon\right)\)_-far from_ \(P\)_, then the algorithm returns_ \(H_{2}\)_._

Proof.: Let \(q\), \(p_{1}\), and \(p_{2}\) denote the true probability of the Scheffe set of \(H_{1}\) and \(H_{2}\) according to \(P\), \(H_{1}\), and \(H_{2}\):

\[q\coloneqq P(\mathcal{S}(H_{1},H_{2}))\,,\qquad p_{1}\coloneqq H_{1}( \mathcal{S}(H_{1},H_{2}))\,,\qquad p_{2}\coloneqq H_{2}(\mathcal{S}(H_{1},H_{ 2}))\,.\]

And, let \(\hat{q}\) be the estimate of \(q\) from the samples. the Using the Hoeffding bound, one can show that \(|q-\hat{q}|\) is at most \(\epsilon^{\prime}\coloneqq\epsilon/2\) with probability \(1-\delta\), since we use \(O(\log(1/\delta)/\epsilon^{2})\) samples. Now, if \(\|H_{1}-P\|_{\mathcal{IV}}\) is at most \(\Gamma\), then \(|p_{1}-q|\) must be at most \(\Gamma\) implying:

\[|p_{1}-\hat{q}|\leq|p_{1}-q|+|q-\hat{q}|\leq\Gamma+\epsilon^{\prime}\,.\]

[MISSING_PAGE_EMPTY:28]

The last inequality is due to the assumption we have: \((t_{e}+t_{d})/q\leq 1/(2n^{\prime})\). Next, by Moarkov's inequality, the probability that the number of \(y\)'s are at least one (that is two times the expected value) is at most \(1/2\). Hence, if \(x\) is mapped to \(H_{i}\), with probability at least \(1/2\), there is no other excellent or decent hypothesis is selected.

Now, we compute the probability that an excellent hypothesis is selected and no other excellent or decent hypothesis is selected as follows:

\[\mathbf{Pr}_{a,b} \big{[}\text{One excellent hypothesis and no decent one}\big{]}\] \[=\sum_{x\in[n^{\prime}]}\sum_{H_{i}\in\mathcal{H}_{e}}\mathbf{ Pr}\big{[}f_{a,b}(x)=i(\text{mod }q)\big{]}\] \[\cdot\mathbf{Pr}\big{[}\mathbf{no}\;y\in[n^{\prime}]\setminus\{x \}\text{ s.t. }H_{f_{a,b}(y)(\text{mod }q)}\in\mathcal{H}_{e}\cup\mathcal{H}_{d}\mid f_{a,b}(x)=i(\text{mod }q)\big{]}\] \[\geq\sum_{x\in[n^{\prime}]}\sum_{H_{i}\in\mathcal{H}_{e}}\mathbf{ Pr}\big{[}f_{a,b}(x)=i(\text{mod }q)\big{]}\cdot\frac{1}{2}=\frac{n^{\prime}\cdot t_{e}}{2\,q}\]

Hence, the proof is complete. 

**Fact D.2**.: _Let \(n,a,b\) be three positive integers. Then, we have:_

\[\left\lceil\frac{\lceil n/a\rceil}{b}\right\rceil=\left\lceil\frac{n}{a\,b} \right\rceil\,.\]

Proof.: Let \(r\) denote the remainder of \(n\) divided by \(a\). If \(r=0\), then the claim is trivial. Therefore, assume \(r\) is in \(\{1,2,\ldots,a-1\}\). Let \(s\) be the remainder of \(n\) divided by \(a\,b\). Clearly, \(s\) has to be of the form: \(t\cdot a+r\) for \(t\in\{0,1,\ldots,b-1\}\). Hence, we have

\[\left\lceil\frac{\lceil n/a\rceil}{b}\right\rceil =\left\lceil\frac{n}{a\,b}+\frac{a-r}{a\,b}\right\rceil=\left\lfloor \frac{n}{a\,b}\right\rfloor+\left\lceil\frac{s+a-r}{a\,b}\right\rceil\] \[=\left\lfloor\frac{n}{a\,b}\right\rfloor+\left\lceil\frac{(t+1) \cdot a}{a\,b}\right\rceil=\left\lfloor\frac{n}{a\,b}\right\rfloor+1=\left \lceil\frac{n}{a\,b}\right\rceil\,.\]

**Fact D.3**.: _Given the definition of the \(g_{\ell}\)'s in Equation (2), for any positive integer \(\ell\), we have:_

\[\prod_{i=1}^{\ell}g_{i}\geq r^{1.5^{\ell}-1}\,.\]

Proof.: By the properties of the geometric series, we obtain:

\[\prod_{i=1}^{\ell}g_{i}\geq\prod_{i=1}^{\ell}\big{(}\sqrt{r}\big{)}^{1.5^{i-1} }=\big{(}\sqrt{r}\big{)}^{\sum_{i=1}^{\ell}1.5^{i-1}}=\big{(}\sqrt{r}\big{)}^ {(1.5^{\ell}-1)/(1.5-1)}=r^{1.5^{\ell}-1}\,.\]