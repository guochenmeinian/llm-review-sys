# Addressing Uncertainty in LLMs to Enhance Reliability in Generative AI

 Ramneet Kaur1, Colin Samplawski1, Adam D. Cobb1, Anirban Roy1, Brian Matejek1, Manoj Acharya1, Daniel Elenius1, Alexander M. Berenbeim2, John A. Pavlik2, Nathaniel D. Bastian2, Susmit Jha1

###### Abstract

In this paper, we present a dynamic semantic clustering approach inspired by the Chinese Restaurant Process, aimed at addressing uncertainty in the inference of Large Language Models (LLMs). We quantify uncertainty of an LLM on a given query by calculating entropy of the generated semantic clusters. Further, we propose leveraging the (negative) likelihood of these clusters as the (non)conformity score within Conformal Prediction framework, allowing the model to predict a set of responses instead of a single output, thereby accounting for uncertainty in its predictions. We demonstrate the effectiveness of our uncertainty quantification (UQ) technique on two well-known question-answering benchmarks, COQA and TriviaQA, utilizing two LLMs--Llama-2-13b and Mistral-7b. Our approach achieves state-of-the-art (SOTA) performance in UQ, as assessed by metrics such as AUROC, AUARC, and AURAC. The proposed conformal predictor is also shown to produce smaller prediction sets while maintaining the same probabilistic guarantee of including the correct response, in comparison to existing SOTA conformal prediction baseline. Our code is publicly accessible at https://shorturl.at/TyHSq.

## 1 Introduction

Large language models (LLMs) are increasingly being utilized for various tasks, including open-world question answering. However, these models are known to exhibit hallucinations, confidently producing incorrect information or relying on faulty reasoning. Quantifying the uncertainty associated with an LLM for a given input offers a practical method to account for model's reliability on its response. When there is a strong correlation between LLM's accuracy and the computed uncertainty, one can effectively use uncertainty quantification (UQ) approach to determine when to place their trust in the model's responses.

The challenge of UQ for LLMs in generative context differs significantly from that encountered in regression or classification tasks. While the latter has been well-studied Guo et al. (2017); Jha et al. (2019); Xiao and Wang (2019); Hu and Khan (2021); Magesh et al. (2023), UQ for generative models presents unique challenges due to the free-form nature of the responses of varying lengths. Additionally, the syntactic similarity of generated sequences may not necessarily align with their semantic similarity.

Self-consistency theory can be employed to measure LLM's uncertainty on an input query by comparing semantic context of multiple outputs sampled from the LLM for that query (Wang et al., 2022).1 Building on recent observations by Lin et al. (2023); Kuhn et al. (2023), we use embeddingbased semantic similarity to group sampled outputs into semantic clusters, and then quantify LLM's uncertainty by entropy of these clusters. Specifically, we propose a new dynamic semantic clustering algorithm based on the sequential distance dependent Chinese Restaurant Process (DDCRP) Blei and Frazier (2011); Tuncer and Schulz (2016) for quantifying LLM's uncertainty via entropy of the generated clusters.

Furthermore, we present a novel approach towards trustworthy inference from LLMs that combines the model's uncertainty with conformal prediction. Specifically, we propose to use (negative) likelihood of the clusters generated via DDCRP, as the (non)conformity score in the conformal prediction framework (Balasubramanian et al., 2014). This framework takes into account prediction uncertainty by generating a set of a single prediction, where the output set is guaranteed to contain the correct label (or answer in our case) with a certain confidence level. LLMs can generate free-form responses for question answering and so, the conformal prediction set in our approach is different from the usual setting of being a subset of known finite set of labels. We use semantic similarity to identify different or semantically diverse responses while constructing the conformal set. This use of conformal prediction by LLMs in question answering is the key novelty of our approach that aims to combine LLMs uncertainty on an input query with conformal prediction.

We demonstrate the efficacy of our UQ approach on two question-answering benchmarks, COQA and TriviaQA, using two LLMs, Llama-2-13b and Mistral-7b, achieving state-of-the-art (SOTA) results in most test cases using AUROC, AUARC and AURAC as metrics. We also show that our conformal predictor generates smaller prediction sets for the same probabilistic guarantees of including correct response compared to the SOTA conformal prediction baseline by Quach et al. (2023). This highlights not only the validity but also usefulness of our method in practical applications.

## 2 Related Work

**Uncertainty quantification (UQ)** for LLMs has received significant attention over the last few years. One approach is to explicitly query the model for the correctness probability (Kadavath et al., 2022). Another approach relies on utilizing the log-likelihood (Jiang et al., 2020) associated with its generated response by taking a product or an average or other statistical aggregation over the generated tokens. LLMs are known to be not well-calibrated (Mielke et al., 2022) and consequently, methods for calibrating LLMs (Huang et al., 2024) have also been proposed. Semantic entropy or predictive uncertainty that measures the (in)consistency among multiple responses has been proposed as a metric for UQ of LLMs (Kuhn et al., 2023; Lin et al., 2023; Lu et al., 2023). We also use semantic entropy for UQ but adopt a novel semantic clustering approach and empirically demonstrate its effectiveness.

**Conformal prediction (CP)**(Balasubramanian et al., 2014) has been used for deploying deep learning models (Kaur et al., 2022; Haroush et al., 2021; Kaur et al., 2023, 2024; Yang et al., 2024) in high-assurance applications wherein the model predicts a set instead of a single prediction such that one of the responses in the set is guaranteed to be correct with a probability higher than the user specified significance level. In the context of LLMs, conformal prediction has been used for providing coverage guarantees (Ye et al., 2024; Quach et al., 2023). Ye et al. (2024) concentrate on classification settings and propose non-conformity scores in the CP framework accordingly. In contrast, we focus on generative setting for LLMs in applications such as question-answering. Quach et al. (2023) propose generating diverse prediction sets based on the quality of individual responses, and a set scoring function. They utilize CP to derive hyperparameters (\(\lambda\)s) for diversity, quality, and set scoring function in their algorithm for coverage guarantees. We, instead propose, using (negative) likelihood of clusters representing semantically diverse responses, as the (non)conformity score in the CP framework for generating sets with coverage guarantees, and compare our results with Quach et al. (2023)'s approach.

## 3 Clustering by Semantic Equivalence

In this section, we introduce the proposed clustering approach, and then describe its usage in uncertainty quantification and building the conformal predictor for LLMs.

_Semantic entropy_ over \(|C|\) equivalence classes (or clusters) of multiple responses sampled from an LLM on an input query \(x\) has been used as the _UQ measure_ for the LLM on \(x\)(Kuhn et al., 2023):

\[SE(x)=-\sum_{c}p(c|x)\log p(c|x),\]

where the probability of an equivalence class (corresponding to a cluster of embeddings), \(c\), conditioned on \(x\), is given by \(p(c|x)=\sum_{\mathbf{s}\in c}p(\mathbf{s}|x)\) and \(\mathbf{s}\in c\) denotes a sentence (or response) in an equivalence class \(c\in C\). The probability of each sentence is given by the standard product of the conditional token probabilities:

\[p(\mathbf{s}|x)=\prod_{i}p(s_{i}|s_{<i},x).\]

The result of using semantic entropy is to sharpen \(p(c|x)\) when there are many responses with the same meaning, and therefore reduce the predicted entropy.

To generate clusters containing semantically equivalent responses, we need to define a function that checks semantic similarity between responses. Kuhn et al. (2023) choose the conservative approach of using Deberta (Natural Language Inference) model He et al. (2020) to only define two sentences to be semantically equivalent if and only if entailment was classified in both directions. Entailment in both directions is not necessary when one response includes additional information than another but they both contain the same relevant semantic information. For instance, let us consider the following input query: "What is the capital of France?" with the first response \(\mathbf{s_{1}}=\) "Paris", and the second response \(\mathbf{s_{2}}=\) "Paris, the capital city of France, is one of the most iconic and romantic cities in the world. It has a rich history dating back more than 2,000 years". Although, both the responses are semantically equivalent with respect to the input query, \(\mathbf{s_{2}}\) entails \(\mathbf{s_{1}}\) but vice versa is not true. Enforcing entailment in both directions can, therefore, lead to formation of more clusters than required with semantically equivalent information distributed in different clusters.

Thus, our approach computes the probability that two responses, \(\mathbf{s}_{i}\) and \(\mathbf{s}_{j}\), are in the same equivalence class (that is, the same semantic cluster), by taking the maximum entailment score output by Deberta, \(p_{ij}=\max(p(\mathbf{s}_{i}+\mathbf{s}_{j}),(p(\mathbf{s}_{j}\vdash\mathbf{ s}_{i}))\). This choice of taking a maximum can be viewed as a quantitative disjunction of entailment in either direction. We then use this approach to build the score that a response, \(\mathbf{s}_{j}\), belongs to an equivalence class, \(c\), by using the average probability across all the cluster members:

\[w(\mathbf{s}_{j}\in c)=\frac{1}{|c|}\sum_{\mathbf{s}_{i}\in c}p_{ij}\] (1)

This is also novel from the existing approaches for semantic clustering where \(\mathbf{s_{j}}\) is assigned to the cluster \(c\) if it is entailed in both directions by only one member of \(c\). As observed in our qualitative evaluation (and reported in Appendix A.1), we postulate that this could be the reason for assigning semantically irrelevant responses to the same cluster because it is easier to incorrectly assign responses to the same cluster if we rely on only one member instead of all members in the cluster. In contrast, we take an average over all existing members of a cluster making our assignment more robust.

A naive approach would be to greedily assign \(\mathbf{s_{j}}\) to a cluster with the highest probability. However, this is not sufficient since we need a mechanism for forming new equivalence classes (or clusters). Given a set of responses, we need to decide when to form a new cluster and when to assign \(\mathbf{s_{j}}\) to an existing cluster \(c\) that has the highest score \(w(\mathbf{s_{j}}\in c)\). We use the same mechanism as the Distance Dependent Chinese Restaurant Process (DDCRP) for iterative clustering (Blei and Frazier, 2011; Tuncer and Schulz, 2016). The motivation for using DDCRP is due to its probabilistic and dynamic nature for deciding on the formation of a new cluster or membership in an existing cluster based on semantic equivalence. Following DDCRP, the score for a sentence \(\mathbf{s_{j}}\) in a new cluster \(c^{*}\) is defined as:

\[w(\mathbf{s_{j}}\in c^{*})=\frac{\alpha}{\alpha+|C|},\] (2)

where \(|C|\) is the current number of clusters and \(\alpha>0\) is the rate parameter, i.e. prior over forming new clusters. The final (softmax) probabilities of assigning \(\mathbf{s_{j}}\) to an existing cluster (\(c_{i}\)) and new cluster (\(c^{*}\)) are respectively given by:

\[\text{Softmax}(z_{i})=\frac{e^{z_{i}}}{\sum_{j}e^{z_{j}}+e^{z^{*}}}\] \[\text{Softmax}(z^{*})=\frac{e^{z^{*}}}{\sum_{j}e^{z_{j}}+e^{z^{*}}},\] (3)

where \(z_{i}=w(s\in c_{i}),\text{and}\ z^{*}=w(s\in c^{*})\). Since the equivalence class assignment of the new response is related to semantic equivalence with the existing responses, we can map our clustering algorithm as an instance of sequential DDCRP.

Alg. 1 is the proposed clustering approach for iterative clustering of semantically equivalent responses. It is executed in a sequential fashion, starting with an empty set of clusters. After the first response from the LLM, we compute the score for forming a new cluster as: \(\frac{\alpha}{\alpha+0}=1\). This results in a new cluster probability of 1, leading a new cluster to be formed deterministically. In subsequent rounds, we compute the per cluster assignment scores for the new response as per Equations (1) and (2) for existing and new cluster respectively. We, then, compute the softmax probabilities for these scores from Equation (3), and assign the new response to a cluster (either existing cluster \(c_{i}\) or a new cluster \(c^{*}\)) with the maximum probability.

```
1:Input: query \(x\), LLM model \(M\)
2:Parameter: rate parameter \(\alpha>0\), number of responses \(N\)
3:Output: clusters \(C\)
4:Initialize:\(C\leftarrow\emptyset\)
5:for\(i=1\) to \(N\)do
6:\(\mathbf{s}_{i}=M(x)\) [generate with LLM]
7:\(scores\leftarrow\mathbf{0}_{|C|+1}\)
8:for\(c_{j}\) in \(\mathbf{C}\)do
9:\(scores[j]\gets w(\mathbf{s}_{i}\in c_{j})\)
10:endfor
11:\(scores[-1]\leftarrow\frac{\alpha}{\alpha+|C|}\)
12:\(probs\leftarrow\text{Softmax}(scores)\)
13:\(k\gets argmax(probs)\) {cluster assignment}
14:if\(k==|C|+1\)then
15:\(C\gets C\cup\{\mathbf{s}_{i}\}\) {new cluster}
16:else
17:\(c_{k}\gets c_{k}\cup\{\mathbf{s}_{i}\}\)
18:endif
19:endfor
20:
21:return\(C\) ```

**Algorithm 1** Clustering by Semantic Equivalence

In all our experiments, we use the rate parameter of \(\alpha=0.5\). We performed a grid search over \(\alpha\in[0.2,0.3,\ldots,0.7]\) and observed very little variance in performance. We set the value of number of responses \(N\) is set to \(20\), which is consistent with the existing work (Kuhn et al., 2023; Lin et al., 2023; Quach et al., 2023).

### Uncertainty Quantification

Different clusters containing semantically diverse responses to an input query can be used to quantify uncertainty of the LLM on the query. Similar to Kuhn et al. (2023), we also use entropy of the generated clusters from Alg. 1 as a measure of UQ for LLMs on a given query. Both qualitative and quantitative results indicate that the proposed clustering approach yields better results than Kuhn's baseline.

### Conformal Set Prediction

For each cluster \(c\), we have \(p(c|x)\). We, therefore, use the negative log probability, \(\log[1/p(c|x)]\), of the individual clusters, as the non-conformity score in conformal prediction (CP) framework (Balasubramanian et al., 2014), for generating prediction sets. Non-conformity scores of calibration datapoints is used to build a reference empirical distribution to compare against when building the prediction set. Specifically, depending on the desired significance level, \(\epsilon\), prediction set is generated by comparing scores for the test clusters with a threshold from the empirical distribution: non-conformity score of calibration set at \((1-\epsilon)^{th}\) quantile of the distribution.2 Intuitively, clusters with low negative log probability (or high likelihood) are more likely to be included in prediction sets compared to clusters with high negative log probability (or low likelihood). If an LLM outputs many semantically equivalent responses, then we expect the cluster's \(\log[1/\sum_{\mathbf{s}\in c}p(\mathbf{s}|x)]\) to decrease due to the summation over the sentence probabilities by sharpening the cluster probability.

Footnote 2: Calibration set consists of only those clusters whose all responses are accurate.

The use of CP for constructing the prediction sets gives us coverage guarantees on the true answer in the set with the probability greater than or equal to \(1-\epsilon\)(Vovk et al., 2005). Alg. 2 is the proposed CP algorithm for generating prediction sets with coverage guarantees.

For an input query \(x\) (from test or calibration set), clusters are generated via Alg. 1. We use negative log probability (nlp = \(\log[1/p(c|x)]\)) of each generated cluster (\(c\)) for \(x\) as the non-conformity score for the cluster. For the desired significance level \(\epsilon\in(0,1)\), the prediction threshold \(\tau\) is decided as the score at \((1-\epsilon)^{th}\) quantile of the empirical distribution of non-conformity scores for the calibration clusters. Assuming all responses are semantically equivalent in a cluster, a single response from the test cluster (\(c\)) is added to the prediction set if its non-conformity score (nlp(\(c\))) is below the prediction threshold \(\tau\). In our experiments, prediction set is constructed from the first response in the qualified test cluster.

```
1:Input: query \(x\), LLM model \(M\), prediction threshold \(\tau\) from \((1-\epsilon)^{th}\) quantile of the empirical distribution of calibration set non-conformity scores
2:Output: Prediction Set \(\mathcal{O}\) with predictions on \(x\) by \(M\) s.t. \(Pr.(\text{correct answer}\in\mathcal{O})\geq 1-\epsilon\)
3:\(C=\) set of clusters from Alg. 1 on (\(x\), \(M\))
4:\(S=\) set of non-conformity scores for the generated clusters: \(\{\forall c\in C:\)nlp\((c)\}\)
5:\(\mathcal{O}=\{\)a response from \(C[i]\) s.t. \(S[i]\leq\tau:i=1,\dots,|C|\}\)
6: return \(\mathcal{O}\) ```

**Algorithm 2** Conformal Prediction Sets by LLM

## 4 Experimental Results

The experimental evaluation focuses on two research questions. **RQ1:** Does the novel semantic clustering approach inspired from CRP improve the UQ of LLMs? **RQ2:** How does Alg 2 perform compared to the CP baseline on LLMs for free form generative responses?

**Datasets and Models**. We use two question-answer datasets: COQA (Reddy et al., 2019) and TriviaQA (Joshi et al., 2017), over which we compare the performance of two LLMs, LLama-2-13b: non-instruct model (Touvron et al., 2023), and Mistral-7b: instruct model (Jiang et al., 2023). Following existing literature (Lin et al., 2023; Kuhn et al., 2023), we deploy three evaluation methods: (1) We query GPT-4 (Achiam et al., 2023) by asking it to provide a rating on whether a response is correct with a value between 0 and 1, and label the response as correct if its rating \(>0.7\); (2) RougeL score (Lin, 2004) with a threshold \(>0.3\); (3) Deberta (He et al., 2020) to check for entailment of correct answer in the generated response.

### UQ Performance

We report Area Under Accuracy-Rejection Curve (AUARC) (Nadeem et al., 2009), and Area Under Rejection-Accuracy Curve (AURAC) for comparing our performance on UQ with Kuhn et al. (2023)'s,and Lin et al. (2023)'s approaches. For Lin et al. (2023)'s approach, we use EigV as their UQ metric3. While AUARC has been used as an evaluation metric previously (Lin et al., 2023), we include AURAC as a new metric. AUARC is an indicator of the accuracy of accepted (or highly certain) samples, and AURAC is an indicator of the accuracy of rejected (or highly uncertain) samples. In addition to AUARC, AURAC also indicates calibration of the UQ metric: we would like the accuracy of the model on the rejected samples by the UQ metric to be as low as possible, i.e. not rejecting samples on which LLMs are accurate.

Footnote 3: They also propose ‘Ecc’, and ‘Deg’ as other UQ metrics. Consistent with their paper, we found that the best results in most of the cases are with ‘EigV’ metric, and therefore we compare our results with this UQ metric.

The results are reported in Tables 1, and 2. Similar to Kuhn et al. (2023)'s, we also report our results with the UQ score unnormalized (Unnorm)/normalized (Norm) on the response's length. We outperform the baseline by Kuhn et al. (2023) in all the test cases, indicating that the proposed clustering approach performs better in UQ. We achieve competitive results in comparison to the current SOTA by Lin et al. (2023) by outperforming them in most cases. We also report AUROC, and compare with other baselines in Appendix A.2.

### Conformal Prediction Results

The desired properties of a prediction set is that the accuracy of the set should be as high as possible with a smaller set size. So, here we report accuracy and set size as the evaluation metrics.

#### 4.2.1 Comparison with the CP Baseline

Fig. 1 shows Alg. 2 results in comparison with the existing baseline by Quach et al. (2023) on using CP for generating prediction sets with coverage guarantees.

\begin{table}
\begin{tabular}{c|c||c|c|c|c||c|c|c|c} \hline \multicolumn{2}{c}{} & \multicolumn{4}{c||}{**COQA** Dataset} & \multicolumn{4}{c}{**TriviaQA** Dataset} \\ \hline
**Model** & **Eval.** & **Model** & **Sem. Ent.** & **EigV** & **Ours** & **Model** & **Sem. Ent.** & **EigV** & **Ours** \\  & & **Acc.** & Unnorm/Norm & & Unnorm/Norm & **Acc.** & Unnorm/Norm & & Unnorm/Norm \\ \hline \hline Llama-13b & GPT-4 & 73.22 & 85.97/56.90 & **54.63** & 58.42/55.32 & 67.03 & 40.09/04.27 & 39.42 & 39.92/**39.38** \\ Mistral-7b & GPT-4 & 73.38 & 63.06/62.02 & **59.83** & 62.77/61.41 & 60.68 & 35.57/35.04 & **33.19** & 35.13/33.29 \\ \hline Mean & GPT-4 & 73.30 & 61.02/59.46 & **57.23** & 60.60/58.37 & 63.86 & 37.83/37.66 & **36.31** & 37.53/36.34 \\ \hline Llama-13b & Rougel.27 & 72.56 & 56.75/55.53 & 53.78 & 55.78/**52.65** & 64.60 & 39.12/39.39 & 38.93 & 38.81/**38.35** \\ Mistral-7b & Rougel.4 & 44.74 & 27.62/62.95 & **77.12** & 27.37/82.26 & 42.33 & 17.51/19.56 & 17.06 & **16.95**/18.11 \\ \hline Mean & Rougel.5 & 85.75 & 42.19/42.59 & **40.45** & 41.57/40.46 & 53.47 & 28.14/29.48 & 28.00 & **27.88**/28.23 \\ \hline Llama-13b & Deberta & 63.74 & 46.07/46.91 & **42.04** & 45.07/43.56 & 63.33 & 37.23/37.94 & **36.70** & 36.88/36.84 \\ Mistral-7b & Deberta & 11.23 & 3.84/5.70 & 4.13 & **3.82**/5.00 & 33.92 & 11.00/13.54 & 11.35 & **10.89**/12.45 \\ \hline Mean & Deberta & 37.49 & 24.96/26.31 & **23.09** & 24.45/24.28 & 48.63 & 24.12/25.74 & 24.03 & **23.89**/24.65 \\ \hline \end{tabular}
\end{table}
Table 2: AURAC (\(\downarrow\)) results in comparison to Kuhn et al. (2023)’s Semantic Entropy (Sem. Ent.) results in most of the cases are with ‘EigV’ metric, and therefore we compare our results with this UQ metric.

\begin{table}
\begin{tabular}{c|c||c|c|c|c||c|c|c} \hline \multicolumn{2}{c}{} & \multicolumn{4}{c||}{**COQA** Dataset} & \multicolumn{4}{c}{**TriviaQA** Dataset} \\ \hline
**Model** & **Eval.** & **Model** & **Sem. Ent.** & **EigV** & **Ours** & **Model** & **Sem. Ent.** & **EigV** & **Ours** \\  & & **Acc.** & Unnorm/Norm & & Unnorm/Norm & **Acc.** & Unnorm/Norm & & Unnorm/Norm \\ \hline Llama-13b & GPT-4 & 73.22 & 85.97/56.90 & **54.63** & 58.42/55.32 & 67.03 & 40.09/04.27 & 39.42 & 39.92/**39.38** \\ Mistral-7b & GPT-4 & 73.38 & 63.06/62.02 & **59.83** & 62.77/61.41 & 60.68 & 35.57/35.04 & **33.19** & 35.13/33.29 \\ \hline Mean & GPT-4 & 73.30 & 61.02/59.46 & **57.23** & 60.60/58.37 & 63.86 & 37.83/37.66 & **36.31** & 37.53/36.34 \\ \hline Llama-13b & Rougel.27 & 72.56 & 56.75/55.53 & 53.78 & 55.78/**52.65** & 64.60 & 39.12/39.39 & 38.93 & 38.81/**38.35** \\ Mistral-7b & Rougel.4 & 44.74 & 27.62/62.95 & **77.12** & 27.37/82.26 & 42.33 & 17.51/19.56 & 17.06 & **16.95**/18.11 \\ \hline Mean & Rougel.5 & 85.75 & 42.19/42.59 & **40.45** & 41.57/40.46 & 53.47 & 28.14/29.48 & 28.00 & **27.88**/28.23 \\ \hline Llama-13b & Deberta & 63.74 & 46.07/46.91 & **42.04** & 45.07/43.56 & 63.33 & 37.23/37.94 & **36.70** & 36.88/36.84 \\ Mistral-7b & Deberta & 11.23 & 3.84/5.70 & 4.13 & **3.82**/5.00 & 33.92 & 11.00/13.54 & 11.35 & **10.89**/12.45 \\ \hline Mean & Deberta & 37.49 & 24.96/26.31 & **23.09** & 24.45/24.28 & 48.63 & 24.12/25.74 & 24.03 & **23.89**/24.65 \\ \hline \end{tabular}
\end{table}
Table 1: AUARC (\(\uparrow\)) results in comparison to Kuhn et al. (2023)’s Semantic Entropy (Sem. Ent.) results in most of the cases are with ‘EigV’ metric, and therefore we compare our results with this UQ metric.

The coverage guarantee (or guarantee of the correct answer contained in the prediction set by CP) is expected to be \(\geq(1-\epsilon)\). So, as the value of \(\epsilon\) increases, the accuracy and the set size is expected to decrease. This is what we observe for both approaches: ours and the baseline, with both the approaches satisfying the coverage guarantees. Quach et al. (2023) report results with different variations of their proposed algorithm (Algorithm 1 of their paper) in terms of the set scoring function (\(\mathcal{F}\)): First-\(K\), Max, and Sum, and on **TriviaQA with Llama-2-13b**. We outperform these results for all the three variations on both evaluation metrics.

#### 4.2.2 Experiments on COQA

We also evaluate our Alg. 2's performance on COQA. Figure 2 shows these results for both accuracy and set size and with all the three GT evaluation approaches on COQA: GPT-4, RougeL, and Deberta.

Here, we also report the point accuracy, which is the average accuracy of the individual \(N=20\) generations. For \(\epsilon\leq 0.35\), the prediction set accuracy is always higher than the point accuracy. Consistent with the results on TriviQA, here also the value of accuracy and set size decreases with the increase in the value of \(\epsilon\). GPT and RougeL evaluations satisfy the coverage guarantee \(\forall\epsilon\geq 0.15\). Even though conformal prediction provides a rigorous theoretical guarantee, deviations from the coverage guarantee can occur in practice due to limited sample variability in the calibration set (Angelopoulos and Bates, 2021). This justifies the accuracy results with Deberta Evaluation and the other two evaluations with \(\epsilon<0.15\).

One difference to note here from the TriviaQA experiments is in the set of \(\epsilon\) values. For TriviaQA, we reported results with \(\epsilon\in\{0.2,\dots,0.5\}\), and for COQA we reported results with \(\epsilon\in\{0.1,0.2,\dots,0.5\}\). This is because we were getting 'nan' at \(\epsilon=0.1\) from the Quach's baseline on TriviaQA. While further investigation on their code, we figured it out that they are hard-coding the value as 'nan' where the search for their algorithm's hyperparameters (\(\lambda\)s) might be failing.

## 5 Conclusion

This paper takes a step towards enhancing reliability in generative AI by addressing uncertainty in LLMs on a given query. Our approach focuses on the semantic equivalence of responses generated by an LLM when prompted multiple times for the input query. It is based on the idea that an LLM is expected to be accurate if it consistently generates semantically similar outputs when prompted multiple times with the same input. The underlying assumption here is that consistency can serve as an indicator of accuracy. Testing this hypothesis is one of the future works that we intend to investigate. Additionally, we aim to explore alternative scoring methods beyond the probability of response--calculated as the product of conditional token probabilities-- used to determine the likelihood of semantic clusters. This is important because response probability can be sensitive to its length, which poses a significant challenge when dealing with free-form generations produced by LLMs.

Figure 1: Comparison of Accuracy (left) and Set Size of prediction sets (right) with Conformal Prediction baseline (Quach et al., 2023).

## Acknowledgments

This work was supported in part by the United States Air Force and Defense Advanced Research Projects Agency (DARPA) under Contract No.FA8750-23-C-0519, the Defense Advanced Research Projects Agency (DARPA) under Agreement No. HR0011-24-9-0424 and Defense Logistics Agency (DLA) and the Advanced Research Projects Agency for Health (ARPA-H) under Contract Number SP4701-23-C-0073. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the Department of Defense, DARPA, DLA, ARPA-H or the United States Government.

Figure 2: Alg. 2’s Accuracy (left) and Set Size (right) evaluation on COQA for Llama-13b.

## References

* Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_ (2023).
* Anastasios et al. (2021) Anastasios N Angelopoulos and Stephen Bates. 2021. A gentle introduction to conformal prediction and distribution-free uncertainty quantification. _arXiv preprint arXiv:2107.07511_ (2021).
* Balasubramanian et al. (2014) Vineeth Balasubramanian, Shen-Shyang Ho, and Vladimir Vovk. 2014. _Conformal prediction for reliable machine learning: theory, adaptations and applications_. Newnes.
* Blei and Frazier (2011) David M Blei and Peter I Frazier. 2011. Distance dependent Chinese restaurant processes. _Journal of Machine Learning Research_ 12, 8 (2011).
* Guo et al. (2017) Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. 2017. On calibration of modern neural networks. In _International conference on machine learning_. PMLR, 1321-1330.
* Haroush et al. (2021) Matan Haroush, Tzviel Frostig, Ruth Heller, and Daniel Soudry. 2021. A statistical framework for efficient out of distribution detection in deep neural networks. _arXiv preprint arXiv:2102.12967_ (2021).
* He et al. (2020) Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020. Deberta: Decoding-enhancedbert with disentangled attention. _arXiv preprint arXiv:2006.03654_ (2020).
* Hu and Khan (2021) Yibo Hu and Latifur Khan. 2021. Uncertainty-aware reliable text classification. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_. 628-636.
* Huang et al. (2024) Yukun Huang, Yixin Liu, Raghuveer Thirukovalluru, Arman Cohan, and Bhuwan Dhingra. 2024. Calibrating Long-form Generations from Large Language Models. _arXiv preprint arXiv:2402.06544_ (2024).
* Jha et al. (2019) Susmit Jha, Sunny Raj, Steven Fernandes, Sumit K Jha, Somesh Jha, Brian Jalaian, Gunjan Verma, and Ananthram Swami. 2019. Attribution-based confidence metric for deep neural networks. _Advances in Neural Information Processing Systems_ 32 (2019).
* Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7B. _arXiv preprint arXiv:2310.06825_ (2023).
* Jiang et al. (2020) Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know? _Transactions of the Association for Computational Linguistics_ 8 (2020), 423-438.
* Joshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. _arXiv preprint arXiv:1705.03551_ (2017).
* Kadavath et al. (2022) Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. 2022. Language models (mostly) know what they know. _arXiv preprint arXiv:2207.05221_ (2022).
* Kaur et al. (2022) Ramneet Kaur, Susmit Jha, Anirban Roy, Sangdon Park, Edgar Dobriban, Oleg Sokolsky, and Insup Lee. 2022. iDECODE: In-distribution equivariance for conformal out-of-distribution detection. In _Proceedings of the AAAI Conference on Artificial Intelligence_, Vol. 36. 7104-7114.
* Kaur et al. (2023) Ramneet Kaur, Kaustubh Sridhar, Sangdon Park, Yahan Yang, Susmit Jha, Anirban Roy, Oleg Sokolsky, and Insup Lee. 2023. CODiT: Conformal out-of-distribution Detection in time-series data for cyber-physical systems. In _Proceedings of the ACM/IEEE 14th International Conference on Cyber-Physical Systems (with CPS-IoT Week 2023)_. 120-131.
* Kaur et al. (2024) Ramneet Kaur, Yahan Yang, Oleg Sokolsky, and Insup Lee. 2024. Out-of-Distribution Detection in Dependent Data for Cyber-Physical Systems with Conformal Guarantees. _ACM Transactions on Cyber-Physical Systems_ (2024).
* Kaur et al. (2025)Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. _arXiv preprint arXiv:2302.09664_ (2023).
* Lin (2004) Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In _Text summarization branches out_. 74-81.
* Lin et al. (2023) Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. 2023. Generating with confidence: Uncertainty quantification for black-box large language models. _arXiv preprint arXiv:2305.19187_ (2023).
* Magesh et al. (2023) Akshayaa Magesh, Venugopal V. Veeravalli, Anirban Roy, and Susmit Jha. 2023. Principled OOD Detection via Multiple Testing. _2023 IEEE International Symposium on Information Theory (ISIT)_ (2023), 1026-1031. https://api.semanticscholar.org/CorpusID:261081329
* Mielke et al. (2022) Sabrina J Mielke, Arthur Szlam, Emily Dinan, and Y-Lan Boureau. 2022. Reducing conversational agents' overconfidence through linguistic calibration. _Transactions of the Association for Computational Linguistics_ 10 (2022), 857-872.
* Nadeem et al. (2009) Malik Sajjad Ahmed Nadeem, Jean-Daniel Zucker, and Blaise Hanczar. 2009. Accuracy-rejection curves (ARCs) for comparing classification methods with a reject option. In _Machine Learning in Systems Biology_. PMLR, 65-81.
* Quach et al. (2023) Victor Quach, Adam Fisch, Tal Schuster, Adam Yala, Jae Ho Sohn, Tommi S Jaakkola, and Regina Barzilay. 2023. Conformal language modeling. _arXiv preprint arXiv:2306.10193_ (2023).
* Reddy et al. (2019) Siva Reddy, Danqi Chen, and Christopher D Manning. 2019. Coqa: A conversational question answering challenge. _Transactions of the Association for Computational Linguistics_ 7 (2019), 249-266.
* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_ (2023).
* Tuncer and Schulz (2016) Mehmet Ali Cagri Tuncer and Dirk Schulz. 2016. Sequential distance dependent chinese restaurant processes for motion segmentation of 3d lidar data. In _2016 19th International Conference on Information Fusion (FUSION)_. IEEE, 758-765.
* Vovk et al. (2005) Vladimir Vovk, Alex Gammerman, and Glenn Shafer. 2005. _Algorithmic learning in a random world_. Springer Science & Business Media.
* Wang et al. (2022) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-ery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. _arXiv preprint arXiv:2203.11171_ (2022).
* Xiao and Wang (2019) Yijun Xiao and William Yang Wang. 2019. Quantifying uncertainties in natural language processing tasks. In _Proceedings of the AAAI conference on artificial intelligence_, Vol. 33. 7322-7329.
* Yang et al. (2024) Yahan Yang, Ramneet Kaur, Souradeep Dutta, and Insup Lee. 2024. Memory-based Distribution Shift Detection for Learning Enabled Cyber-Physical Systems with Statistical Guarantees. _ACM Transactions on Cyber-Physical Systems_ 8, 2 (2024), 1-28.
* Ye et al. (2024) Fanghua Ye, Mingming Yang, Jianhui Pang, Longyue Wang, Derek F Wong, Emine Yilmaz, Shuming Shi, and Zhaopeng Tu. 2024. Benchmarking LLMs via Uncertainty Quantification. _arXiv preprint arXiv:2401.12794_ (2024).

Appendix

### Qualitative comparison of Clustering Approaches

Here, we provide an example from both COQA and TriviaQA datasets to analyse how our new clustering approach compares to the original approach by Kuhn et al. (2023). We look at the quality of clusters formed by both approaches.

#### a.1.1 An example from COQA Dataset

**Story**: CHAPTER XXXIV Arthur remained at the gate while Ruth climbed Maria's front steps. She heard the rapid click of the type-writer, and when Martin let her in, found him on the last page of a manuscript. She had come to make certain whether or not he would be at their table for Thanksgiving dinner; but before she could broach the subject Martin plunged into the one with which he was full. "Here, let me read you this," he cried, separating the carbon copies and running the pages of manuscript into shape. "It's my latest, and different from anything I've done. It is so altogether different that I am almost afraid of it, and yet I've a sneaking idea it is good. You be judge. It's an Hawaiian story. I've called it 'Wiki-wiki'." His face was bright with the creative glow, though she shivered in the cold room and had been struck by the coldness of his hands at greeting. She listened closely while he read, and though he from time to time had seen only disapprobration in her face, at the close he asked:- "Frankly, what do you think of it?" "I-I don't know," she, answered. "Will it-do you think it will sell?" "I'm afraid not," was the confession. "It's too strong for the magazines. But it's true, on my word it's true." "But why do you persist in writing such things when you know they won't sell?" she went on inexorably. "The reason for your writing is to make a living, isn't it?"

**Question**: 'Did he answer her?"

**Answer**: 'No'

**Generated Responses from Llama-13b**: ['Yes', 'Yes', "He didn't", 'He did, only not directly', 'No', 'No', 'No', 'No', 'He asked her what she thought', 'He told her his latest story', 'Yes', 'No', 'A sneaking yes', 'He ran the manuscript up to Miss Lawton', 'No', 'In the affirmative', 'Yes', 'No', 'No', 'Yes', 'Yes']

**Results**: Figures 3, and 4 show the clusters formed by Kuhn et al. (2023), and our approach respectively. For brevity, we include only unique responses in a cluster. As it can be seen, Kuhn et al. (2023) approach puts semantically different responses in the same cluster (responses 3, 4, and 6 in cluster 1 for 'Yes'), whereas ours separate them out in different clusters.

#### a.1.2 An example from TriviaQA Dataset

**Question**: What is 'The Old Lady of Threadneedle Street'?

**Answer**: Bank of England

**Generated Responses from Llama-13b**: ['Bank of England', 'Bank of England', 'Bank of England', 'The Bank of England', 'A nickname; what was it really?', 'Bank of England', 'The Bank of England', 'The Bank of England', 'The Bank of England', 'The Bank of England', 'The Bank of England', 'The Bank of England', 'Bank of England', 'Bank of England', 'Bank of England', 'The Bank of England', 'Bank of England', 'The Bank of England', 'Bank of England', 'The Bank of England', 'Bank Of England']

Figure 3: Clusters generated by Kuhn et al. (2023)’s approach on COQA example.

**Results**: Figures 3, and 4 show the clusters formed by Kuhn et al. (2023), and our approach respectively. Again for brevity, we include only unique responses in a cluster. As it can be seen, Kuhn et al. (2023) approach puts semantically different responses in the same cluster (response 3 in cluster 1 for 'Bank of England'), whereas ours separate them out in different clusters.

We observed similar results on other stories from COQA, and questions from TriviaQA datasets.

### All UQ results

Here, we include all results on UQ performance from Section 4.1: comparison with additional baselines on AUARC, AUROC  and AUROC evaluation metrics. In addition to Kuhn et al. (2023)'s Sem. Ent. (Unnorm/Norm), and (Lin et al., 2023)'s EigV results reported in the main paper, we include "Numset", "LexiSim", and "SelfProb" baselines here. Numset uses the number of semantic sets (or clusters) as the UQ metric, and has been previously used in (Lin et al., 2023) as one of the baselines. Higher the numset, more uncertain is the LLM on the input query. LexiSim uses the average of RougeL distance between every pair of responses for UQ. Here, higher the Lexisim, lower is the uncertainty. Again, Lexisim has been used as a baseline by Kuhn et al. (2023), and Lin et al. (2023). SelfProb (Kadavath et al., 2022) estimates if the probability of a model's response is correct by asking the model itself, and use that as the UQ metric. We follow the same prompt format as Lin et al. (2023) for asking the model about the probability, and report average over all responses. Here, higher the SelfProb, lower is the uncertainty.

Figure 4: Clusters generated by our approach (Alg. 1) on COQA example.

Figure 5: Clusters generated by Kuhn et al. (2023)’s approach on TriviaQA example.

Figure 6: Clusters generated by our approach (Alg. 1) on TriviaQA example.

[MISSING_PAGE_EMPTY:13]

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c} \hline \multirow{2}{*}{**Model**} & \multirow{2}{*}{**GT**} & \multirow{2}{*}{**Model Acc**} & \multicolumn{2}{c|}{**Sem. Ent.**} & \multirow{2}{*}{**NumSet**} & \multirow{2}{*}{**LexiSim**} & \multirow{2}{*}{**SelfProb**} & \multirow{2}{*}{**EigV**} & \multicolumn{2}{c}{**Ours**} \\  & & & Unnorm/Norm & & & & & Unnorm/Norm \\ \hline Llama-13b & GPT-4 & 73.22 & 58.97/56.90 & 63.45 & 61.35 & 72.29 & **54.63** & 58.42/55.32 \\ Mistral-7b & GPT-4 & 73.38 & 63.06/62.02 & 67.19 & 62.21 & 61.18 & **59.83** & 62.77/61.41 \\ \hline Mean & GPT-4 & 73.30 & 61.02/59.46 & 65.32 & 61.78 & 66.74 & **57.23** & 60.60/58.37 \\ \hline Llama-13b & RougeL & 72.75 & 56.75/55.53 & 65.30 & 58.51 & 73.12 & 53.78 & 55.78/**52.65** \\ Mistral-7b & RougeL & 44.74 & 27.62/29.65 & 40.13 & 34.11 & 33.56 & **27.12** & 27.37/28.26 \\ \hline Mean & RougeL & 58.75 & 42.19/42.59 & 52.72 & 46.31 & 53.34 & **40.45** & 41.58/40.46 \\ \hline Llama-13b & Deberta & 63.74 & 46.07/46.91 & 55.50 & 53.07 & 63.32 & **42.04** & 45.07/43.56 \\ Mistral-7b & Deberta & 11.23 & 3.84/5.70 & 9.84 & 11.83 & 9.45 & 4.13 & **3.82**/5.00 \\ \hline Mean & Deberta & 37.49 & 24.96/26.31 & 32.67 & 32.45 & 36.39 & **23.09** & 24.45/24.28 \\ \hline \end{tabular}
\end{table}
Table 6: AUROC (\(\uparrow\)) results in comparison to all baselines on **TriviaQA**. Best results are in bold and second best are underlined.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c} \hline \multirow{2}{*}{**Model**} & \multirow{2}{*}{**GT**} & \multirow{2}{*}{**Model Acc**} & \multicolumn{2}{c|}{**Sem. Ent.**} & \multirow{2}{*}{**NumSet**} & \multirow{2}{*}{**LexiSim**} & \multirow{2}{*}{**SelfProb**} & \multirow{2}{*}{**EigV**} & \multicolumn{2}{c}{**Ours**} \\  & & & Unnorm/Norm & & & & Unnorm/Norm \\ \hline Llama-13b & GPT-4 & 67.03 & 40.09/40.27 & 43.03 & 54.28 & 60.31 & 39.42 & 39.92/**39.38** \\ Mistral-7b & GPT-4 & 60.68 & 35.57/35.04 & 40.02 & 47.45 & 33.71 & **33.19** & 35.13/33.29 \\ \hline Mean & GPT-4 & 63.86 & 37.83/37.66 & 41.525 & 50.87 & 47.01 & **36.31** & 37.53/36.34 \\ \hline Llama-13b & RougeL & 64.60 & 39.12/39.39 & 42.74 & 50.56 & 58.41 & 38.93 & 38.81/**38.35** \\ Mistral-7b & RougeL & 42.33 & 17.15/19.56 & 25.09 & 32.12 & 21.46 & 17.06 & **16.95**/18.11 \\ \hline Mean & RougeL & 53.47 & 28.14/29.48 & 33.915 & 41.34 & 39.94 & 28.00 & **27.88**/28.23 \\ \hline Llama-13b & Deberta & 63.33 & 37.23/37.94 & 40.09 & 53.01 & 58.23 & **36.70** & 36.88/36.84 \\ Mistral-7b & Deberta & 33.92 & 11.00/13.54 & 18.51 & 27.93 & 15.97 & 11.35 & **10.89**/12.45 \\ \hline Mean & Deberta & 48.63 & 24.12/25.74 & 29.3 & 40.47 & 37.1 & 24.03 & **23.89**/24.65 \\ \hline \end{tabular}
\end{table}
Table 8: AUROC (\(\downarrow\)) results in comparison to all baselines on **TriviaQA**. Best results are in bold and second best are underlined.