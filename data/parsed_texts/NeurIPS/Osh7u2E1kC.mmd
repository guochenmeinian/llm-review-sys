# Leveraging Separated World Model for Exploration in Visually Distracted Environments

 Kaichen Huang\({}^{1,2}\), Shenghua Wan\({}^{1,2}\), Minghao Shao\({}^{1,2}\),

**Hai-Hang Sun\({}^{1,2}\)**, **Le Gan\({}^{1,2}\)**, **Shuai Feng\({}^{3}\)**, **De-Chuan Zhan\({}^{1,2}\)**

\({}^{1}\)School of Artificial Intelligence, Nanjing University, China

\({}^{2}\)National Key Laboratory for Novel Software Technology, Nanjing University, China

\({}^{3}\)School of Cyberspace Science and Technology, Beijing Institute of Technology, China

{huangxc,wansh,shaomh,sunhh}@lamda.nju.edu.cn,

{ganl,zhandc}@nju.edu.cn, fengshuai@bit.edu.cn

Equal contribution.Corresponding author.

###### Abstract

Model-based unsupervised reinforcement learning (URL) has gained prominence for reducing environment interactions and learning general skills using intrinsic rewards. However, distractors in observations can severely affect intrinsic reward estimation, leading to a biased exploration process, especially in environments with visual inputs like images or videos. To address this challenge, we propose a bi-level optimization framework named **S**eparation-assisted **e**X**plorer (SeeX). In the inner optimization, SeeX trains a separated world model to extract exogenous and endogenous information, minimizing uncertainty to ensure task relevance. In the outer optimization, it learns a policy on imaginary trajectories generated within the endogenous state space to maximize task-relevant uncertainty. Evaluations on multiple locomotion and manipulation tasks demonstrate SeeX's effectiveness.

## 1 Introduction

Unsupervised learning has a rich history in computer vision and natural language processing, as demonstrated by methods such as [12, 23, 25, 9, 48, 14]. It leverages unlabeled, task-agnostic data to train models that can be quickly adapted to downstream tasks, addressing sample inefficiency. This approach has also gained traction in unsupervised reinforcement learning (URL) [35, 29, 36], where skills are developed through intrinsic motivation rather than external rewards. A key advantage of URL is that learned dynamics models can gather prior knowledge about the environment during exploration, minimizing the need for extensive interactions during adaptation to downstream tasks.

A major challenge in unsupervised reinforcement learning arises from distractors, which significantly hinder learning in complex real-world environments. For example, a book-finding robot in a library must identify relevant objects (like books) while ignoring irrelevant ones (such as posters or people) to effectively complete its task. These distractors exponentially expand the original state space, creating numerous redundant states that impede efficient exploration [64]. As shown in Figure 1-Left, a single world model encodes both task-relevant and task-irrelevant information in the latent space. Even

Figure 1: Comparison of traditional URL methods using a single world model (left) versus our separated world model (right).

if the agent's state remains unchanged in \(\hat{z}_{i+1,*}\), varying background distractors inflate uncertainty estimates. In contrast, as illustrated in Figure 1-Right, our proposed separation world model distinctly separates task-relevant information \(s^{+}\) from task-irrelevant information \(s^{-}\), allowing for accurate uncertainty estimation unaffected by distractors.

Despite its prevalence in real-world environments, this challenge has received limited attention in unsupervised reinforcement learning. Current unsupervised RL methods: spanning data-driven, knowledge-based, and competence-based approaches (see Section 6), remain vulnerable to redundant states, which can lead agents down unproductive exploration paths [69].

To address visual inputs with complex distractors, we propose a bi-level optimization framework called Separation-Assisted Explorer (SeeX). We extend URLB-pixels to environments with distractors, demonstrating that performance declines vary with task difficulty. Our method, SeeX, separates latent information in the world model into task-relevant and task-irrelevant components. We assume the task-irrelevant part captures transitions, using it to predict rewards and actions.

Our contributions are summarized as follows: **(I)** We introduce Separation-Assisted Explorer (SeeX), a new approach for tackling unsupervised RL tasks with complex distractors in observations. **(II)** We provide a theoretical analysis that formalizes URL with distractors, where the policy maximizes task-relevant uncertainty while the world model minimizes environmental uncertainty. **(III)** We demonstrate the outstanding performance of policies learned by SeeX across various locomotion and manipulation tasks with diverse visual distractors.

## 2 Preliminary

World models are highly effective for reinforcement learning (RL) tasks with visual inputs, performing well in both simulated [20; 21; 56] and robotics [67; 66] environments. In unsupervised settings, agents explore using intrinsic rewards during pre-training. This task-agnostic data facilitates fine-tuning through a world model. URLB-pixels [50] demonstrates that unsupervised methods combined with world models outperform model-free agents. Inspired by these results, we adopted a model-based framework in our approach.

Several studies, including URLB [33] and URLB-pixels [50], have created a unified framework for various unsupervised reinforcement learning (URL) methods, advancing the field. The benchmark consists of two phases: pre-training (PT), where agents explore without task-specific rewards, and fine-tuning (FT), where they apply knowledge gained during PT with limited interactions. While URLB-pixels effectively evaluates pixel-based performance, it does not address complex distractors in visual inputs. To fill this gap, we tested baseline methods on noisy video backgrounds and developed a novel approach to tackle this challenge.

Our focus lies in scenarios where agents must acquire skills from noisy visual observations, often disrupted by task-irrelevant videos. To model the dynamics under these conditions, we adopt the EX-BMDP framework, an adaptation of Block MDP. A Block MDP can be represented as a tuple \(\mathcal{M}=(\mathcal{O},\mathcal{Z},\mathcal{A},\mathcal{T},\mathcal{R}, \mathcal{U})\), where \(\mathcal{O}\), \(\mathcal{Z}\) and \(\mathcal{A}\) denotes the set of observations, latent states and actions respectively; the transition function \(\mathcal{T}:\mathcal{Z}\times\mathcal{A}\rightarrow\Delta(\mathcal{Z})\) maps latent states and actions to probability distributions over next latent states; the reward function \(\mathcal{R}:\mathcal{O}\times\mathcal{A}\rightarrow[0,1]\) assigns reward to observation-action pairs; the emission function \(\mathcal{U}:\mathcal{Z}\rightarrow\Delta(\mathcal{O})\) maps latent states to probability distributions over observations. The EX-BMDP is formulated as follows:

**Definition 2.1**.: (Exogenous Block Markov Decision Process). An EX-BMDP is a BMDP such that the latent state can be decoupled into two parts \(z=(s^{+},s^{-})\) where \(s^{+}\in\mathcal{S}^{+}\) is endogenous state and \(s^{-}\in\mathcal{S}^{-}\) is the exogenous state. For \(z\in\mathcal{Z}\) the initial distribution and transition functions are decoupled, that is: \(\mu(z)=\mu_{+}(s^{+})\mu_{-}(s^{-})\), and \(\mathcal{T}(z^{\prime}|z,a)=\mathcal{T}_{+}(s^{+^{\prime}}|s^{+},a)\mathcal{T}_ {-}(s^{-^{\prime}}|s^{-})\).

## 3 Bi-level Optimization for Exploration in Distracted Environments

The objective of the pre-training phase in URL is to learn a world model capable of handling downstream tasks with diverse reward functions. Following previous work [13; 8; 52], we formalize the pre-training phase as a minimax regret problem.

**Problem 3.1**.: _(Regret Optimization for World Model) In the context of reward-free BMDP, consider a world model \(W\) that defines the latent dynamics \(\widehat{\mathcal{M}}^{R}\), which simulates the real dynamics under a reward function \(R\). Let \(V_{\mathcal{M}^{R}}(\pi)\) denotes the expected value of policy \(\pi\) under dynamics \(\mathcal{M}\) with reward function \(R\). The regret of policy \(\pi\) is defined as \(\text{REGRET}(\pi,\mathcal{M}^{R})=V_{\mathcal{M}^{R}}(\pi^{*})-V_{\mathcal{M}^{R }}(\pi),\pi^{*}=\arg\max_{\pi^{\prime}}V_{\mathcal{M}^{R}}(\pi^{\prime})\), which measures the performance gap between \(\pi\) and the optimal policy. The optimization goal is to find the world model that is robust to different possible reward functions, minimizing the regret of world model policy under real dynamics:_

\[\min_{W}\max_{R}\text{REGRET}(\widehat{\pi}_{R}^{*},\mathcal{M}^{R}),\widehat {\pi}_{R}^{*}=\arg\max_{\pi}V_{\widehat{\mathcal{M}^{R}}}(\pi)\] (1)

In the absence of the reward function during pre-training, computing regret under specific reward functions becomes infeasible. To address this challenge, we propose transforming regret into a novel, reward-free objective using Simulation Lemma [28, 52]. Our proposition is outlined below.

**Proposition 3.2**.: _Denote the learned latent dynamics in the world model as \(\widehat{\mathcal{T}}\) and the true latent dynamics as \(\mathcal{T}\). For any reward function \(R\), the regret of the optimal world model is bounded by:_

\[\begin{split}\text{REGRET}(\widehat{\pi}_{R}^{*},\mathcal{M}^{R})& \leq\frac{2\gamma}{(1-\gamma)^{2}}\Big{[}\mathbb{E}_{z,a\sim d( \pi_{R}^{*},\widehat{\mathcal{M}})}\Big{[}\mathrm{TV}\big{(}\widehat{ \mathcal{T}}(\cdot|z,a),\mathcal{T}(\cdot|z,a)\big{)}\Big{]}\\ &+\mathbb{E}_{z,a\sim d(\widehat{\pi}_{R}^{*},\widehat{\mathcal{ M}})}\Big{[}\mathrm{TV}\big{(}\widehat{\mathcal{T}}(\cdot|z,a),\mathcal{T}( \cdot|z,a)\big{)}\Big{]}\Big{]}\end{split}\] (2)

As outlined in Proposition 3.2, the optimal world model policy exhibits low regret if the latent state-action distribution of both \(\pi_{R}^{*}\) and \(\hat{\pi}_{R}^{*}\) in \(\widehat{\mathcal{M}}\) is accurately captured by \(\widehat{\mathcal{T}}\). However, during unsupervised pre-training, the reward function remains inaccessible, preventing us from directly obtaining these distributions. This is because the state-action distribution induced by \(\pi_{R}^{*}\), and \(\hat{\pi}_{R}^{*}\) is inherently dependent on the reward function. To address this challenge, we introduce an exploration policy \(\pi^{\text{expl}}\), specifically designed to maximize the expected error (in terms of total variation distance) of the latent dynamics model:

\[\pi^{\text{expl}}=\arg\max_{\pi}\mathbb{E}_{z,a\sim d(\pi,\widehat{\mathcal{ M}})}\Big{[}\mathrm{TV}\Big{(}\widehat{\mathcal{T}}(\cdot|z,a),\mathcal{T}( \cdot|z,a)\Big{)}\Big{]}\] (3)

This enables us to derive an upper bound on the regret that is independent of the reward function:

\[\text{REGRET}(\widehat{\pi}_{R}^{*},\mathcal{M}^{R})\leq\frac{4\gamma}{(1- \gamma)^{2}}\mathbb{E}_{z,a\sim d(\pi^{\text{expl}},\widehat{\mathcal{M}})} \Big{[}\mathrm{TV}\Big{(}\widehat{\mathcal{T}}(\cdot|z,a),\mathcal{T}(\cdot| z,a)\Big{)}\Big{]}\text{ for all }R.\] (4)

In the context of EX-BMDPs, the transition dynamics can be decomposed into two components: \(\mathcal{T}(z^{\prime}|z,a)=\mathcal{T}_{+}(s^{+}|s^{+},a)\mathcal{T}_{-}(s^{- }|s^{-})\), where \(z=(s^{+},s^{-})\), \(s^{+}\) represents the endogenous state, and \(s^{-}\) represents the exogenous state. To apply EX-BMDP to visually distracted environments, we further assume that the reward depends solely on \(s^{+}\), implying that decisions are made based only on endogenous state.

To model this separation, we construct the world model as two distinct components: the endogenous part \(\widehat{\mathcal{M}}_{+}\) and the exogenous one \(\widehat{\mathcal{M}}_{-}\), corresponding to the task-relevant and task-irrelevant processes, respectively. Their associated latent dynamics are denoted by \(\widehat{\mathcal{T}}_{+}\) and \(\widehat{\mathcal{T}}_{-}\). Utilizing this decomposition, we can derive a lower bound for the policy optimization target:

\[\begin{split}&\mathbb{E}_{z,a\sim d(\pi,\widehat{\mathcal{M}})} \Big{[}\mathrm{TV}\Big{(}\widehat{\mathcal{T}}(\cdot|z,a),\mathcal{T}(\cdot| z,a)\Big{)}\Big{]}\\ =&\mathbb{E}_{s^{+},s^{-},a\sim d(\pi,\widehat{ \mathcal{M}})}\Big{[}\mathrm{TV}\Big{(}\widehat{\mathcal{T}}_{+}(\cdot|s^{+},a )\widehat{\mathcal{T}}_{-}(\cdot|s^{-}),\mathcal{T}_{+}(\cdot|s^{+},a) \mathcal{T}_{-}(\cdot|s^{-})\Big{)}\Big{]}\\ \geq&\mathbb{E}_{s^{+},a\sim d(\pi,\widehat{\mathcal{ M}}_{+})}\Big{[}\mathrm{TV}\Big{(}\widehat{\mathcal{T}}_{+}(\cdot|s^{+},a), \mathcal{T}_{+}(\cdot|s^{+},a)\Big{)}\Big{]}\end{split}\] (5)

The strategy's optimization objective is a form of model uncertainty. Estimating model uncertainty for the URL serves as a reward for the exploration policy, which is crucial for learning. However, under the EX-BMDP assumption, directly maximizing the reward of the overall model error introduces significant bias. This is because the objective models not only the endogenous part but also the exogenous part, while the strategy is solely based on \(s^{+}\), leading to inefficient exploration. For the model optimization target, using triangle inequality, we have:

\[\mathbb{E}_{z,a\sim d(\pi,\widetilde{\mathcal{M}}_{+})}\Big{[}\text {TV}\Big{(}\widehat{\mathcal{T}}(\cdot|z,a),\mathcal{T}(\cdot|z,a)\Big{)}\Big{]}\] (6) \[\leq \mathbb{E}_{s^{+},a\sim d(\pi,\widetilde{\mathcal{M}}_{+})}\Big{[} \text{TV}\Big{(}\widehat{\mathcal{T}}_{+}(\cdot|s^{+},a),\mathcal{T}_{+}( \cdot|s^{+},a)\Big{)}\Big{]}\] \[+\mathbb{E}_{s^{-}\sim d(\widetilde{\mathcal{M}}_{-})}\Big{[} \text{TV}\Big{(}\widehat{\mathcal{T}}_{-}(\cdot|s^{-}),\mathcal{T}_{-}(\cdot| s^{-})\Big{)}\Big{]}\]

With these bounds, we can reformulate the problem as a bi-level optimization problem:

**Problem 3.3**.: _(Bi-level Optimization of World Model Error) Consider a reward-free EX-BMDP with latent dynamics functions \(\widehat{\mathcal{T}}_{+}\), \(\widehat{\mathcal{T}}_{-}\) and an exploration policy \(\pi\), assuming the reward depends only on the endogenous state. Let \(\mathcal{T}=\mathcal{T}_{+}\times\mathcal{T}_{-}\) denote the true latent dynamics. We use a bi-level optimization: the inner optimization finds the world model minimizing the error, while the outer optimization maximizes the endogenous error with respect to the exploration policy._

\[\text{Inner:}\quad\min_{\widehat{\mathcal{T}}_{+}}\mathbb{E}_{s^{ +},a\sim d(\pi,\widetilde{\mathcal{M}}_{+})}\Big{[}\text{TV}\Big{(}\widehat{ \mathcal{T}}_{+}(\cdot|s^{+},a),\mathcal{T}_{+}(\cdot|s^{+},a)\Big{)}\Big{]}\] (7) \[\quad+\min_{\widehat{\mathcal{T}}_{-}}\mathbb{E}_{s^{-}\sim d( \widetilde{\mathcal{M}}_{-})}\Big{[}\text{TV}\Big{(}\widehat{\mathcal{T}}_{- }(\cdot|s^{-}),\mathcal{T}_{-}(\cdot|s^{-})\Big{)}\Big{]}\] \[\text{Outer:}\quad\max_{\pi}\mathbb{E}_{s^{+},a\sim d(\pi, \widetilde{\mathcal{M}}_{+})}\Big{[}\text{TV}\Big{(}\widehat{\mathcal{T}}_{+ }(\cdot|s^{+},a),\mathcal{T}_{+}(\cdot|s^{+},a)\Big{)}\Big{]}\]

Problem 3.3 optimizes the world model error as a surrogate for Problem 3.1. It seeks a world model with low prediction error for both endogenous and exogenous components, guided by an exploration policy that maximizes endogenous error. This ensures the optimal policy generalizes well across varying exogenous distributions, regardless of the future reward function. Next, we introduce a practical framework to solve Problem 3.3.

## 4 Practical Implementation of SeeX

### Control with Separation-assisted Latent Dynamics

Separated World Model.For high-dimensional inputs like images and videos, model-based frameworks encode past experiences into latent representations to predict future sequences [59; 65; 19]. Building on this, we propose a separated world model to disentangle task-relevant (endogenous) and task-irrelevant (exogenous) information. As shown in Figure 2, the endo-encoder \(h_{t}^{+}=E_{\theta^{+}}(o_{t})\) and exo-encoder \(h_{t}^{-}=E_{\theta^{-}}(o_{t})\) extract these components. Two transition models handle dynamics: \(q_{\theta^{+}}(s_{t}^{+}|s_{t-1}^{+},a_{t-1})\) for task-relevant states and \(q_{\theta^{-}}(s_{t}^{-}|s_{t-1}^{-})\) for task-irrelevant ones. Additionally, inference models \(p_{\theta^{+}}(s_{t}^{+}|s_{t-1}^{+},a_{t-1},h_{t}^{+})\) and \(p_{\theta^{-}}(s_{t}^{-}|s_{t-1}^{-},h_{t}^{-})\) provide posterior estimates.

Observation Model.Some model-based approaches [20; 49] use an auxiliary reconstruction loss to refine the observation encoder, ensuring \(z_{t}\) captures sufficient information from \(o_{t}\) by maximizing the mutual information \(\mathcal{I}(o_{t};z_{t})\). The IM algorithm [6] provides the Barber-Agakov lower bound: \(\mathcal{I}(o_{t};z_{t})\geq\mathbb{E}_{p(o_{t},z_{t})}[\ln q_{\theta}(o_{t}|z _{t})]+\mathcal{H}(p(o_{t}))\), where \(\mathcal{H}(p(o_{t}))\) is the entropy of the observation distribution. In our separation setting, this bound becomes \(\mathcal{I}(o_{t};s_{t}^{+},s_{t}^{-})\geq\mathbb{E}_{p(o_{t},s_{t}^{+},s_{t}^ {-})}[\ln q_{\theta}(o_{t}|s_{t}^{+},s_{t}^{-})]\). Inspired by [63; 18; 24], We design an observation model \(q_{\theta}(o_{t}|s_{t}^{+},s_{t}^{-})\), an endo-decoder \((\hat{o}_{t}^{+},m_{t}^{-})\sim D_{\theta^{+}}(s_{t}^{+})\), an exo-decoder \((\hat{o}_{t}^{-},m_{t}^{-})\sim D_{\theta^{-}}(s_{t}^{-})\) and a mask-mixing model \(m_{t}=M_{\theta}(m_{t}^{+},m_{t}^{-})\). \(\hat{o}_{t}^{+}\) and \(\hat{o}_{t}^{-}\) are expected to reconstruct the task-relevant and task-irrelevant parts of the original observation, while \(m_{t}^{+}\) and \(m_{t}^{-}\) are corresponding masks. So \(q_{\theta}(o_{t}|s_{t}^{+},s_{t}^{-})\) can be implemented as \(\hat{o}_{t}=m_{t}\odot\hat{o}_{t}^{+}+(1-m_{t})\odot\hat{o}_{t}^{-}\). Considering insights from [18; 64] that task-relevant information occupies only a small portion of the observation, we design an exogenous reconstruction (Exo-Rec) model \(\hat{o}_{t}^{\text{exo}}\sim q_{\theta}(o_{t}|s_{t}^{-})\) to ensure that \(s^{-}\) contains the vast majority of information.

World Model Optimization.Similar to a variational autoencoder (VAE) [30; 51], all model components are trained jointly by maximizing the evidence lower bound (ELBO). Our optimization objective extends that of Dreamer [20]: \(\mathcal{L}\doteq\mathbb{E}_{p}(\sum_{t}(\mathcal{L}_{O}^{t}+\mathcal{L}_{R}^{t}+ \mathcal{L}_{KL}^{t}))\). The detailed formulations of each term are presented as follows, and the derivations are shown in Appendix B.1.

\[\mathcal{L}_{O}^{t} \doteq\ln q_{\theta}(o_{t}|s_{t}^{+},s_{t}^{-})+\alpha\ln q_{ \theta}(o_{t}|s_{t}^{-})\] (8) \[\mathcal{L}_{R}^{t} \doteq\ln q_{\theta}(r_{t}|s_{t}^{+})\] (9) \[\mathcal{L}_{KL}^{t} \doteq\text{KL}(p_{\theta^{+}}(s_{t}^{+}|s_{t-1}^{+},a_{t-1},h_{ t}^{+})||q_{\theta^{+}}(s_{t}^{+}|s_{t-1}^{+},a_{t-1}))+\text{KL}(p_{ \theta^{-}}(s_{t}^{-}|s_{t-1}^{-},h_{t}^{-})||q_{\theta^{-}}(s_{t}^{-}|s_{t-1} ^{-},a_{t-1}))\]

Policy Optimization.To enhance sample efficiency, as shown in Figure 2-(b), we apply the policy optimization strategy of [20] to learn a parametric policy with imaginary trajectories. More specifically, we design an actor \(\pi(a_{t}|s_{t}^{+})\) and a reward model \(q_{\theta}(r_{t}|s_{t}^{+})\). The reward model fits the true reward function in the FT phase. The actor selects action based on the current endogenous state \(s_{t}^{+}\) under the guidance of intrinsic reward in the PT phase or the reward model in the FT phase.

### Intrinsic Reward: Compute Endogenous Uncertainty under Separation View

Crafting an effective intrinsic reward function is paramount during the pretraining (PT) phase. A well-designed intrinsic reward can steer the agent towards exploring states with the highest uncertainty, maximizing skill acquisition. A common practice [46; 47; 54] is to maximize the mutual information \(\mathcal{I}(h_{1:T}^{+};\xi|s_{0}^{+},\pi_{\text{expl}})\), where \(\xi\) represents the true unknown dynamics. This objective is motivated by the fact that mutual information quantifies how comprehensively the trajectory explores the environment.

**Proposition 4.1**.: _(Single step's mutual information) If only we find the policy \(\pi_{\text{expl}}\) that maximize every single step's mutual information \(\sum_{t=0}^{T-1}\mathcal{I}(h_{t+1}^{+};\xi|s_{t}^{+},a_{t})\), then the whole trajectories's mutual information \(\mathcal{I}(h_{1:T}^{+};\xi|s_{0}^{+},\pi_{\text{expl}})\) will be also maximized. Proof in Appendix B.2._

With Proposition 4.1, the optimization objective can be written as:

\[\pi_{\text{expl}}^{*} \doteq\arg\max_{\pi_{\text{expl}}}\sum_{t=0}^{T-1}\mathcal{I}(h_ {t+1}^{+};\xi|s_{t}^{+},a_{t})\] (10) \[=\arg\max_{\pi_{\text{expl}}}\sum_{t=0}^{T-1}\mathcal{H}(h_{t+1}^ {+}|s_{t}^{+},a_{t})-\sum_{t=0}^{T-1}\mathcal{H}(h_{t+1}^{+}|\Xi=\xi,s_{t}^{+ },a_{t})\] (11)

To approximate the unknown \(\xi\), we design a set of predictive heads \(\{q_{k}(\hat{h}_{t+1,k}^{+}|\xi_{k},s_{t}^{+},a_{t})\}_{k=1:K}\) which are implemented as conditional Gaussians \(\mathcal{N}(\mu(\Xi=\xi_{k},s_{t}^{+},a_{t}),\sigma^{2})\). Given fixed variance, the conditional entropy does not depend on state or action [54]. Suppose that \(p(\Xi)\) is an uniform distribution on \(\{\xi_{k}\}_{k=1:K}\), then we have \(\mathcal{H}(h_{t+1}^{+}|s_{t}^{+},a_{t})=\frac{1}{K}\sum_{k=1}^{K}\mathcal{H}( h_{t+1}^{+}|\Xi=\xi_{k},s_{t}^{+},a_{t})\). Since the

Figure 2: **(a)** Our separated world model comprises task-relevant and task-irrelevant branches, with ensemble predictive heads providing reward signals. **(b)** Imaginary trajectories within the endogenous branch enhance sample efficiency by reducing real-world interactions.

marginal entropy in Equation (11) lacks a closed-form expression amenable to optimization, we employ the empirical variance across ensemble means as a substitution. It is also an approximation to the world model TV error in Problem 3.3. We define the intrinsic reward as \(r_{t}^{\text{intr}}=\text{Var}(\{\hat{h}_{t+,k}^{+}\}_{k=1:K})\), and employ the exploratory policy \(\pi_{\text{expl}}\) that maximizes \(r_{t}^{\text{intr}}\) as an approximation of \(\pi_{\text{expl}}^{*}\) in Equation (10).

## 5 Experiments

All experiments are conducted with at least three seeds and evaluated for 10 episodes. We conduct experiments to answer the following questions: (1) Can SeeX outperform other counterparts in URLB? (2) How do pre-training steps affect final performance? (3) Can SeeX give a reasonable model uncertainty estimation? (4) How do different components affect the model training? (5) Can the pre-trained world model and policy generalize to OOD distractors? (6) Can data augmentation handle visual distractors?

Environments.Following the URLB evaluation, we select three domains: _Walker_, _Quadruped_, and _Jaco Arm_, spanning twelve downstream tasks: Walker (stand, walk, run, flip), Quadruped (stand, walk, run, jump), and Jaco (reach-top-right, reach-top-left, reach-bottom-right, reach-bottom-left). These tasks vary in difficulty, providing a well-rounded performance assessment. The Jaco Arm domain is particularly challenging due to its multi-joint structure and sparse rewards, only granted upon catching the red ball. Visualizations of the environments are provided in Appendix A. Agents receive only visual inputs (64, 64, 3), with episodes lasting 1000 frames and an action repeat of \(R=2\). Pre-training utilizes up to 2M frames, and fine-tuning employs 100K frames, consistent with URLB-pixels.

Datasets.Leveraging the large-scale and high-quality Kinetics dataset [27], we construct two sub-datasets for our experiments: the **Driving-car** dataset, commonly used in previous works [18; 63], and the **Random-video** dataset, composed of videos randomly selected from other Kinetics classes. To evaluate generalization capabilities, we conducted experiments on the Random-video dataset Section 5.5 (_Note that we only use the Random-video here_). This zero-shot transfer assessment gauges the effectiveness of the policy on unseen random video scenarios.

Baselines.We choose seven different unsupervised methods as baselines, which can be categorized into three types: _Knowledge-based_: ICM [46], RND [10], LBS [43] and Plan2Explore [54] maximize prediction error to better understand the world; _Data-based_: APT [39] encourages exploration by maximizing entropy; _Competence-based_: DAYN [16] and APS [38] maximize mutual information to achieve diverse discovery and generalization. The implementation detail and more concrete introduction of the above methods are shown in Appendix D.2.

Figure 3: We show fine-tuning (FT) performance curves of SeeX and baselines across two domains and eight tasks. Pre-training (PT) used 2M frames, FT 100K. Normalized returns are benchmarked against the expert baseline [33], with mean (solid line) and variance (shaded area).

### Evaluation on URLB with Distractors

The performance curves in Figure 3 show the normalized fine-tuning performance of SeeX and baseline methods pre-trained on 2 million frames. SeeX consistently outperforms or matches other methods across tasks, achieving expert-level performance in the walker-walk task. Notably, SeeX displays higher mean performance and greater variance, reflecting the positive correlation typical in reinforcement learning. In the quadruped domain, even SeeX's lowest performance exceeds the highest curves of other methods, a trend seen across quadruped tasks. Plan2Explore shows relatively weaker results, especially in the walker-stand and walker-walk tasks, likely due to relying solely on predictive heads based on the entire latent space. In contrast, our method demonstrates significant performance improvements. Other baselines like RND and APT perform reasonably well in simpler tasks but struggle with more complex tasks like walker-run and walker-flip. This indicates that using prediction errors, entropy maximization, or mutual information alone is inadequate. In contrast, our separated world model effectively extracts relevant information to enhance policy training.

### Can SeeX Give a Reasonable Model Uncertainty Estimation?

As stated in Section 4.2, model uncertainty is an approximation to the world model TV error in Problem 3.3. Thus we evaluate the accuracy of dynamics fitting for SeeX and Plan2Explore (both incorporating model uncertainty) with pre-trained models of 1M frames. To ensure a comprehensive assessment, we employ two policies: a random policy and SeeX's exploration policy (1M frames). As illustrated in Table 1, consistently across policies and domains, \(U(s^{+})\) values are lower than \(U(z)\) values. This indicates that SeeX achieves a more accurate estimation of true dynamics compared to Plan2Explore, potentially contributing to its superior performance relative to other baseline methods. Furthermore, a notable observation is that \(U(s^{-})\) values significantly exceed \(U(s^{+})\) values. This suggests that the total environmental uncertainty remains unchanged; rather, most uncertainty is concentrated in \(q_{\theta^{-}}\) and \(p_{\theta^{-}}\) due to SeeX's effective capture of task-irrelevant distractor transitions.

### How do pre-training steps affect final performance?

Building upon the promising results from the previous section, where SeeX outperformed seven baseline methods and achieved expert-level performance from [33] (trained in a distractor-free environment, representing the upper bound for our setting) in some tasks, we delve into an intriguing

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline
**Policy** & **Method** & **Walker** & **Quadruped** & **Jaco** \\ \hline \multirow{3}{*}{random} & \(U(s^{+})\) & **0.04\(\pm\)0.00** & **0.17\(\pm\) 0.00** & **0.13\(\pm\)0.00** \\  & \(U(s^{-})\) & 0.64\(\pm\)0.03 & 33.0\(\pm\)31.0 & 0.48\(\pm\)0.03 \\  & \(U(z)\) & 0.31\(\pm\)0.01 & 1.49\(\pm\)0.09 & 0.40\(\pm\)0.01 \\ \hline \multirow{3}{*}{SeeX} & \(U(s^{+})\) & **0.05\(\pm\)0.00** & **0.18\(\pm\)0.00** & **0.14\(\pm\)0.00** \\  & \(U(s^{-})\) & 0.65\(\pm\)0.03 & 45.0\(\pm\)50.0 & 0.52\(\pm\)0.03 \\ \cline{1-1}  & \(U(z)\) & 0.35\(\pm\)0.01 & 1.78\(\pm\)0.08 & 0.44\(\pm\)0.02 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Uncertainty estimation of SeeX and Plan2Explore on the Driving-car dataset. Using two policies, we collected 1000 distinct states and reported the mean and standard deviation of uncertainty. \(U(s^{+})\), \(U(s^{-})\), and \(U(z)\) denote the uncertainty for SeeX (endogenous and exogenous states) and Plan2Explore (latent belief state), respectively.

Figure 4: **(a) Average performance of four Jaco arm tasks. Each row shows comparative probabilities with 95% confidence intervals, indicating Algorithm X outperforms Algorithm Y [1]. Probabilities are based on 150 runs (50 per seed across 3 seeds) per task for robust evaluation. (b) Performance varies with pretraining steps. Given the small gap at 500k on Walker-Stand, a table highlights details, with red marking over 90% expert performance.**question: how does the fine-tuning performance of SeeX vary with different pre-training step sizes? More specifically, can we achieve fine-tuned performance with as few pre-training steps as possible to achieve more than \(90\%\) of expert performance? We conduct experiments on walker-stand and walker-walk. For all methods, we assessed the fine-tuning performance using pre-training frames of (100k, 500k, 1m, 2m). To evaluate SeeX on a more granular level, we add choices of (20k, 50k, 200k, 300k, 400k). As shown in Figure 4-(b), for the Walker-Walk task, SeeX reaches 90% expert performance (red dotted line) within 500k frames. In contrast, no baseline achieves this level, even with 2M pre-training frames. For the simpler Walker-Stand task, a table offers detailed analysis. SeeX requires only 100k frames of random exploration to reach 90% expert performance, while other baselines need significantly more pretraining.

### How do different components affect the model training?

Ablation of modules.We first examine the impact of various modules in SeeX, including the separate world model, Exo-Rec design, and the number of predictive heads (K). As shown in Figure 5-(a), removing either the separate world model or Exo-Rec significantly reduces performance, underscoring the effectiveness of these designs. A small value of \(K\) hinders accuracy, while performance improves with more heads, confirming our intuition. To balance estimation accuracy and computational cost, we choose \(K=5\).

Ablation of policy.In SeeX, we use only the endogenous state \(s^{+}\) to predict actions. To validate this design, we compare two policies: \(\pi(s^{+})\) (SeeX) and \(\pi(s^{+},s^{-})\) (SeeX-both-branch). As shown in Figure 5-(b), SeeX consistently outperforms SeeX-both-branch, particularly in the last three challenging tasks. This demonstrates that relying solely on endogenous information is beneficial in environments with moving distractors.

Ablation of \(\alpha\).We conducted experiments to assess the impact of Exo-Rec's weight \(\alpha\). As shown in Figure 5-(c), the optimal value of \(\alpha\) varies across domains. For Jaco and quadruped tasks, a smaller \(\alpha\) (e.g., 1) enhances performance, whereas walker tasks benefit from a larger weight (e.g., 2 or 3) to effectively extract exogenous information into \(s^{-}\).

### Can the Pre-trained World Model and Policy Generalize to OOD Distractors?

In this subsection, we evaluate the effectiveness of SeeX in addressing the "generalize to distractors from other distributions (OOD distractors)" challenge. To assess generalization ability, we compare SeeX with other baselines on four different walker tasks and report the average normalized return. We pre-trained and fine-tuned agents on the driving-car dataset and evaluated their performance on both driving-car and random-video datasets. The reported results represent the average normalized return across four walker tasks, with 50 runs conducted for each seed. As shown in Figure 6-(b), SeeX outperforms baselines on test distractor datasets and shows minimal performance drop under distribution shifts, highlighting its strong generalization.

Figure 5: Ablation results of SeeX with 500k fixed pretraining frames: **(a)** separation design, Exo-Rec term, and predictive head values; **(b)** different policy design: \(\pi(s^{+})\) (SeeX) and \(\pi(s^{+},s^{-})\) (SeeX both-branch); **(c)** impact of different \(\alpha\) (Exo-Rec weight) across three domains. Due to Jacoâ€™s task complexity, we ran 50 trials per seed and reported the top 30 mean returns.

### Can data augmentation handle visual distractors?

Numerous studies [3; 73; 70; 42] explore data augmentation (DA) in RL as an effective strategy for visual generalization. This subsection highlights the differences between DA and our approach. Our bi-level separation framework extracts task-relevant information, similar to how DA captures task-relevant representations. However, since few approaches use DA for exploration, we integrated it into Plan2Explore and SeeX to assess its impact on performance in the moving distractor setting. Specifically, we applied the classic random shift augmentation from [73] (4-pixel padding) four times per image. To evaluate DA's effectiveness, we tested it during both pretraining (pDA) and finetuning (fDA) on the walker task. The results in Figure 6-(a) reveal: (1) fDA improves performance by mitigating distractors. (2) SeeX's separation design outperforms fDA on tasks with moving distractors. (3) pDA reduces performance, likely by disrupting world model learning, further investigation is planned.

## 6 Related Work

Model-based Control.Learning a dynamics model of the environment is a promising way to tackle the problem of low sample efficiency, and has achieved impressive results in various tasks including continuous control as well as discrete control. To ensure the fairness of comparison, we adopted the official version of Plan2Explore and the implementation of URLB-pixels, which combines all baseline methods into a unified Dreamer-like framework. Plan2Explore makes use of the dreamerv2 framework and takes the uncertainty of prediction of the next latent state as the intrinsic reward. In our work, we focus on the scenarios with complex visual distractors and design an intuitive separated world model to capture the exogenous and endogenous states respectively, and learn policy with imaginary trajectories in latent space.

Unsupervised RL.In recent years, there are many works that tried to promote the unsupervised representation learning manner in various fields including computer vision (CV) [12; 23; 25] and natural language processing (NLP) [9; 48; 14]. Additionally, there have been numerous works applying RL algorithms to real-world applications, such as [37; 68; 44]. These works encourage the RL community to explore the more efficient way of learning [34; 32; 53; 58; 72; 11; 26], however, these works still need to optimize an extrinsic reward. Recently, there have been works to adopt a pure unsupervised manner (reward-free pre-training followed by reward-specific fine-tuning), which can be categorized into three kinds. (i) _Data-based_: maximal entropy RL has enabled agent for diverse exploration and data [39; 55; 71]; (ii) _Knowledge-based_: increase knowledge about the world with self-supervised prediction [46; 47]; (iii) _Competence-based_: methods based on mutual information [16; 22; 38; 57] shows capability for diverse discovery and generalization. URLB-pixels offers a unified model-based framework to implement some methods referred to above, we make use of this benchmark to serve as the baseline. A shared limitation of existing unsupervised exploration methods lies in their exclusive reliance on state information for exploration, lacking the ability to explicitly extract endogenous information. This inherent limitation renders these methods susceptible to performance degradation when confronted with visually rich environments containing complex distractors. Different from the above methods, our method designs an intuitive separated model to separate exogenous and endogenous information and only collect imaginary trajectories in

Figure 6: **(a)** The impact of DA on performance in PT (pDA) and FT (fDA) stages. **(b)** Generalization ability to distractions from other distributions. The rightmost column indicates the percentage drop in performance caused by the distribution shift.

endogenous latent space to train policy. As a result, we largely improved performance compared to the other methods.

Learning with Noisy Observations.To address the issue of learning with noisy observations, diverse approaches have emerged, broadly categorized into four main strategies: (1) Use data augmentation methods to mitigate exogenous noises [72, 17, 74, 7]; (2) Learning task-relevant representations with bisimulation metrics [75, 40]; (3) Design auxiliary tasks to extract endogenous information [4, 5, 15, 31]; (4) Take actions or rewards as discriminating factors to separate exogenous and endogenous information [18, 64, 45, 41]. Our proposed method, SeeX, falls into the intersection of the last two categories, combining the benefits of auxiliary task design and discriminating factors to effectively handle noisy observations in complex real-world environments.

## 7 Conclusion

In this paper, we consider the problem of learning a policy in the visual unsupervised reinforcement learning (URL) setting with moving distractors. To tackle this intricate scenario, we introduce SeeX, a bi-level optimization framework that leverages a separated world model and task-relevant uncertainty maximization to mitigate the impact of distractors and enhance exploration efficiency. Furthermore, Our theoretical analysis formalizes URL with distractors: the policy maximizes task-relevant uncertainty to drive exploration, while the world model minimizes environmental uncertainty to reduce distractor influence. SeeX utilizes a separated world representation to disentangle exogenous and endogenous factors from the original observation domain. Policy training is then conducted exclusively on imaginary trajectories generated within the endogenous latent representation. Extensive experiments on the DMC-suite benchmark demonstrate that SeeX outperforms other baseline methods. Ablation studies provide insights into the contributions of individual components to our final performance.

Limitations and Future Work.Our work presents several areas for improvement: **(I)** Our work focuses on DMC tasks, leaving real-world applications like self-driving and navigation challenges for future exploration. **(II)** Distractors fall into four types based on their impact on rewards and actions: task-irrelevant + action-independent (our focus), task-relevant + action-independent, task-irrelevant + action-dependent, and task-relevant + action-dependent. The latter three will be addressed in future work. **(III)** Using only \(s^{+}\) for policy learning is beneficial in our setting. However, for the task-relevant + action-independent case (e.g., multi-agent systems), we propose incorporating some \(s^{-}\) into policy optimization as a potential solution.

## Acknowledgments and Disclosure of Funding

This work was partially supported by the National Science and Technology Major Project under Grant No. 2022ZD0114805, Collaborative Innovation Center of Novel Software Technology and Industrialization, NSFC (62376118, 62006112, 62250069, 61921006), and the Postgraduate Research & Practice Innovation Program of Jiangsu Province.

## References

* Agarwal et al. [2021] Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare. Deep reinforcement learning at the edge of the statistical precipice. _Advances in neural information processing systems_, 34:29304-29320, 2021.
* Alemi et al. [2016] Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information bottleneck. _arXiv preprint arXiv:1612.00410_, 2016.
* Andrychowicz et al. [2017] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. _Advances in neural information processing systems_, 30, 2017.
* Badia et al. [2020] Adria Puigdomenech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi, Zhaohan Daniel Guo, and Charles Blundell. Agent57: Outperforming the atari human benchmark. In _International conference on machine learning_, pages 507-517. PMLR, 2020.
* Baker et al. [2022] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. _Advances in Neural Information Processing Systems_, 35:24639-24654, 2022.
* Barber and Agakov [2004] David Barber and Felix Agakov. The im algorithm: a variational approach to information maximization. _Advances in neural information processing systems_, 16(320):201, 2004.
* Bertoin et al. [2022] David Bertoin, Adil Zouitine, Mehdi Zouitine, and Emmanuel Rachelson. Look where you look! saliency-guided q-networks for visual rl tasks. In _Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS 2022)_, 2022.
* Beukman et al. [2024] Michael Beukman, Samuel Coward, Michael Matthews, Mattie Fellows, Minqi Jiang, Michael Dennis, and Jakob N. Foerster. Refining minimax regret for unsupervised environment design. _CoRR_, abs/2402.12284, 2024.
* Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* Burda et al. [2018] Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. _arXiv preprint arXiv:1810.12894_, 2018.
* Chen et al. [2021] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. _Advances in neural information processing systems_, 34:15084-15097, 2021.
* Chen et al. [2020] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In _International conference on machine learning_, pages 1597-1607. PMLR, 2020.
* Dennis et al. [2020] Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre M. Bayen, Stuart Russell, Andrew Critch, and Sergey Levine. Emergent complexity and zero-shot transfer via unsupervised environment design. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* Devlin et al. [2018] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* Efroni et al. [2022] Yonathan Efroni, Dylan J Foster, Dipendra Misra, Akshay Krishnamurthy, and John Langford. Sample-efficient reinforcement learning in the presence of exogenous information. In _Conference on Learning Theory_, pages 5062-5127. PMLR, 2022.

* Eysenbach et al. [2018] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without a reward function. _arXiv preprint arXiv:1802.06070_, 2018.
* Fan and Li [2022] Jiameng Fan and Wenchao Li. Dribo: Robust deep reinforcement learning via multi-view information bottleneck. In _International Conference on Machine Learning_, pages 6074-6102. PMLR, 2022.
* Fu et al. [2021] Xiang Fu, Ge Yang, Pulkit Agrawal, and Tommi Jaakkola. Learning task informed abstractions. In _International Conference on Machine Learning_, pages 3480-3491. PMLR, 2021.
* Ha and Schmidhuber [2018] David Ha and Jurgen Schmidhuber. World models. _arXiv preprint arXiv:1803.10122_, 2018.
* Hafner et al. [2019] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. _arXiv preprint arXiv:1912.01603_, 2019.
* Hafner et al. [2020] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. _arXiv preprint arXiv:2010.02193_, 2020.
* Hansen et al. [2019] Steven Hansen, Will Dabney, Andre Barreto, Tom Van de Wiele, David Warde-Farley, and Volodymyr Mnih. Fast task inference with variational intrinsic successor features. _arXiv preprint arXiv:1906.05030_, 2019.
* He et al. [2020] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 9729-9738, 2020.
* He et al. [2017] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In _Proceedings of the IEEE international conference on computer vision_, pages 2961-2969, 2017.
* Henaff [2020] Olivier Henaff. Data-efficient image recognition with contrastive predictive coding. In _International conference on machine learning_, pages 4182-4192. PMLR, 2020.
* Janner et al. [2021] Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence modeling problem. _Advances in neural information processing systems_, 34:1273-1286, 2021.
* Jing and Tian [2020] Longlong Jing and Yingli Tian. Self-supervised visual feature learning with deep neural networks: A survey. _IEEE transactions on pattern analysis and machine intelligence_, 43(11):4037-4058, 2020.
* Kearns and Singh [2002] Michael J. Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. _Mach. Learn._, 49(2-3):209-232, 2002.
* Kim et al. [2023] Seongun Kim, Kyowoon Lee, and Jaesik Choi. Variational curriculum reinforcement learning for unsupervised discovery of skills. _arXiv preprint arXiv:2310.19424_, 2023.
* Kingma and Welling [2013] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_, 2013.
* Lamb et al. [2022] Alex Lamb, Riashat Islam, Yonathan Efroni, Aniket Didolkar, Dipendra Misra, Dylan Foster, Lekan Molu, Rajan Chari, Akshay Krishnamurthy, and John Langford. Guaranteed discovery of control-endogenous latent states with multi-step inverse models. _arXiv preprint arXiv:2207.08229_, 2022.
* Laskin et al. [2020] Michael Laskin, Aravind Srinivas, and Pieter Abbeel. Curl: Contrastive unsupervised representations for reinforcement learning. In _International conference on machine learning_, pages 5639-5650. PMLR, 2020.
* Laskin et al. [2021] Michael Laskin, Denis Yarats, Hao Liu, Kimin Lee, Albert Zhan, Kevin Lu, Catherine Cang, Lerrel Pinto, and Pieter Abbeel. Urlb: Unsupervised reinforcement learning benchmark. _arXiv preprint arXiv:2110.15191_, 2021.
* Laskin et al. [2020] Misha Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Reinforcement learning with augmented data. _Advances in neural information processing systems_, 33:19884-19895, 2020.

* [35] Hojoon Lee, Koanho Lee, Dongyoon Hwang, Hyunho Lee, Byungkun Lee, and Jaegul Choo. On the importance of feature decorrelation for unsupervised representation learning in reinforcement learning. In _International Conference on Machine Learning_, pages 18988-19009. PMLR, 2023.
* [36] Juncheng Li, Xin Wang, Siliang Tang, Haizhou Shi, Fei Wu, Yueting Zhuang, and William Yang Wang. Unsupervised reinforcement learning of transferable meta-skills for embodied navigation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12123-12132, 2020.
* [37] Shuang Li, Yanghui Yan, Ju Ren, Yuezhi Zhou, and Yaoxue Zhang. A sample-efficient actor-critic algorithm for recommendation diversification. _Chinese Journal of Electronics_, 29(1):89-96, 2020.
* [38] Hao Liu and Pieter Abbeel. Aps: Active pretraining with successor features. In _International Conference on Machine Learning_, pages 6736-6747. PMLR, 2021.
* [39] Hao Liu and Pieter Abbeel. Behavior from the void: Unsupervised active pre-training. _Advances in Neural Information Processing Systems_, 34:18459-18473, 2021.
* [40] Qiyuan Liu, Qi Zhou, Rui Yang, and Jie Wang. Robust representation learning by clustering with bisimulation metrics for visual reinforcement learning with distractions. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 8843-8851, 2023.
* [41] Yuren Liu, Biwei Huang, Zhengmao Zhu, Honglong Tian, Mingming Gong, Yang Yu, and Kun Zhang. Learning world models with identifiable factorization. _Advances in Neural Information Processing Systems_, 36, 2024.
* [42] Guozheng Ma, Zhen Wang, Zhecheng Yuan, Xueqian Wang, Bo Yuan, and Dacheng Tao. A comprehensive survey of data augmentation in visual reinforcement learning. _arXiv preprint arXiv:2210.04561_, 2022.
* [43] Pietro Mazzaglia, Ozan Catal, Tim Verbelen, and Bart Dhoedt. Curiosity-driven exploration via latent bayesian surprise. In _Proceedings of the AAAI conference on artificial intelligence_, volume 36, pages 7752-7760, 2022.
* [44] Haowei Meng, Ning Xin, Hao Qin, and Di Zhao. A recursive drl-based resource allocation method for multibeam satellite communication systems. _Chinese Journal of Electronics_, 33(5):1286-1295, 2024.
* [45] Minting Pan, Xiangming Zhu, Yunbo Wang, and Xiaokang Yang. Iso-dream: Isolating and leveraging noncontrollable visual dynamics in world models. _Advances in neural information processing systems_, 35:23178-23191, 2022.
* [46] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In _International conference on machine learning_, pages 2778-2787. PMLR, 2017.
* [47] Deepak Pathak, Dhiraj Gandhi, and Abhinav Gupta. Self-supervised exploration via disagreement. In _International conference on machine learning_, pages 5062-5071. PMLR, 2019.
* [48] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.
* [49] Rafael Rafailov, Tianhe Yu, Aravind Rajeswaran, and Chelsea Finn. Visual adversarial imitation learning using variational models. _Advances in Neural Information Processing Systems_, 34:3016-3028, 2021.
* [50] Sai Rajeswar, Pietro Mazzaglia, Tim Verbelen, Alexandre Piche, Bart Dhoedt, Aaron Courville, and Alexandre Lacoste. Mastering the unsupervised reinforcement learning benchmark from pixels. In _International Conference on Machine Learning_, pages 28598-28617. PMLR, 2023.
* [51] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In _International conference on machine learning_, pages 1278-1286. PMLR, 2014.

* [52] Marc Rigter, Minqi Jiang, and Ingmar Posner. Reward-free curricula for training robust world models. _CoRR_, abs/2306.09205, 2023.
* [53] Max Schwarzer, Ankesh Anand, Rishab Goel, R Devon Hjelm, Aaron Courville, and Philip Bachman. Data-efficient reinforcement learning with self-predictive representations. _arXiv preprint arXiv:2007.05929_, 2020.
* [54] Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak Pathak. Planning to explore via self-supervised world models. In _International conference on machine learning_, pages 8583-8592. PMLR, 2020.
* [55] Younggyo Seo, Lili Chen, Jinwoo Shin, Honglak Lee, Pieter Abbeel, and Kimin Lee. State entropy maximization with random encoders for efficient exploration. In _International Conference on Machine Learning_, pages 9443-9454. PMLR, 2021.
* [56] Younggyo Seo, Danijar Hafner, Hao Liu, Fangchen Liu, Stephen James, Kimin Lee, and Pieter Abbeel. Masked world models for visual control. In _Conference on Robot Learning_, pages 1332-1344. PMLR, 2023.
* [57] Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol Hausman. Dynamics-aware unsupervised discovery of skills. _arXiv preprint arXiv:1907.01657_, 2019.
* [58] Adam Stooke, Kimin Lee, Pieter Abbeel, and Michael Laskin. Decoupling representation learning from reinforcement learning. In _International Conference on Machine Learning_, pages 9870-9879. PMLR, 2021.
* [59] Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. _ACM Sigart Bulletin_, 2(4):160-163, 1991.
* [60] Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. _arXiv preprint arXiv:1801.00690_, 2018.
* [61] Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. _arXiv preprint physics/0004057_, 2000.
* [62] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In _2012 IEEE/RSJ international conference on intelligent robots and systems_, pages 5026-5033. IEEE, 2012.
* [63] Shenghua Wan, Yucen Wang, Minghao Shao, Ruying Chen, and De-Chuan Zhan. Semail: eliminating distractors in visual imitation via separated models. In _International Conference on Machine Learning_, pages 35426-35443. PMLR, 2023.
* [64] Tongzhou Wang, Simon S Du, Antonio Torralba, Phillip Isola, Amy Zhang, and Yuandong Tian. Denoised mdps: Learning world models better than the world itself. _arXiv preprint arXiv:2206.15477_, 2022.
* [65] Manuel Watter, Jost Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control: A locally linear latent dynamics model for control from raw images. _Advances in neural information processing systems_, 28, 2015.
* [66] Daniel Willemsen, Mario Coppola, and Guido CHE de Croon. Mambpo: Sample-efficient multi-robot reinforcement learning using learned world models. In _2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 5635-5640. IEEE, 2021.
* [67] Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter Abbeel, and Ken Goldberg. Day-dreamer: World models for physical robot learning. In _Conference on Robot Learning_, pages 2226-2240. PMLR, 2023.
* [68] Yuqin WU, Congqi SHEN, Shuhan CHEN, Chunming WU, Shunbin LI, and Ruan Wei. Intelligent orchestrating of iot microservices based on reinforcement learning. _Chinese Journal of Electronics_, 31(5):930-937, 2022.

* [69] Xuantang Xiong, Linghui Meng, Jingqing Ruan, Shuang Xu, and Bo Xu. Unec: Unsupervised exploring in controllable space. In _ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 191-195. IEEE, 2024.
* [70] Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous control: Improved data-augmented reinforcement learning. _arXiv preprint arXiv:2107.09645_, 2021.
* [71] Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Reinforcement learning with prototypical representations. In _International Conference on Machine Learning_, pages 11920-11931. PMLR, 2021.
* [72] Denis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. In _International conference on learning representations_, 2020.
* [73] Denis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. In _International conference on learning representations_, 2021.
* [74] Zhecheng Yuan, Guozheng Ma, Yao Mu, Bo Xia, Bo Yuan, Xueqian Wang, Ping Luo, and Huazhe Xu. Don't touch what matters: Task-aware lipschitz data augmentation for visual reinforcement learning. _arXiv preprint arXiv:2202.09982_, 2022.
* [75] Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning invariant representations for reinforcement learning without reconstruction. _arXiv preprint arXiv:2006.10742_, 2020.

## Appendix A Environment

To elucidate the direct impact of video distractors, we present illustrative visualizations in Figure 7. Various tasks present unique challenges. For instance, the multi-jointed Jaco arm with sparse rewards (only awarded for catching the red ball) poses difficulties in accurately reconstructing its dynamics model, hindering the learning process. Additionally, the absence of a physical floor and the introduction of complex distractor videos in quadruped tasks make it challenging for the agent to determine its relative position without relying on the floor. Additionally, Figure 8 highlights the distinct characteristics of the Driving Car and Random Video datasets. Videos in Random-video feature RGB backgrounds that contrast sharply with the grayscale backgrounds in the Driving-car dataset.

## Appendix B Proof

We base our approach on the following theoretical results from the paper.

Figure 8: To facilitate a clear understanding of the differences between the Driving-car and Random-video datasets, we present visualizations of both datasets. Additionally, we showcase the DMC observations of various distractor types. A detailed description of the datasets employed can be found in Section 5.

Figure 7: We present visualization results for the three domains we employed, following the introduction of driving car distractors. Notably, we retain the floor for Walker while removing the floor for Quadruped. This is due to the fact that Quadrupedâ€™s floor occupies the entire observation space, precluding the addition of moving distractions without floor removal.

### Optimization objective of dynamics model

We define an information bottleneck objective [61] to optimize the latent dynamics models same as many previous works [20, 21, 49, 63]. The objective can be written as:

\[\max\underbrace{\mathbb{I}(z_{1:T};(o_{1:T},r_{1:T})|a_{1:T})}_{\text{(a)}}- \beta\underbrace{\mathbb{I}(z_{1:T},i_{1:T}|a_{1:T})}_{\text{(b)}}\] (12)

Note that \(\beta\) is a scalar and \(i_{t}\) denotes indices of the dataset such that \(p(o_{t}|i_{t})=\delta(o_{t}-o_{t}^{\prime})\)[2]. We constrain the capacity of information contained in latent state \(z_{1:T}\), while require them to predict observations and rewards as accurate as possible. We can lower bound term (a) by the non-negativity of the KL divergence:

\[\mathbb{I}(z_{1:T};(o_{1:T},r_{1:T})|a_{1:T}) =\mathbb{E}_{p(o_{1:T},r_{1:T},z_{1:T},a_{1:T})}[\ln p(o_{1:T},r_ {1:T}|z_{1:T},a_{1:T})-\underbrace{\ln p(o_{1:T},r_{1:T}|a_{1:T})}_{\text{ model-relevant}}]\] (13) \[\overset{\pm}{=}\mathbb{E}_{p(o_{1:T},r_{1:T},z_{1:T},a_{1:T})}[ \ln p(o_{1:T},r_{1:T}|z_{1:T},a_{1:T})]\] (14) \[\geq\mathbb{E}_{p(o_{1:T},r_{1:T},z_{1:T},a_{1:T})}[\ln p(o_{1:T},r_{1:T}|z_{1:T},a_{1:T})]\] (15) \[\quad-\text{KL}\left(p(o_{1:T},r_{1:T}|z_{1:T},a_{1:T})||\prod_{t =1}^{T}q(o_{t}|z_{t})q(r_{t}|z_{t})\right)\] (16) \[=\mathbb{E}_{q(z_{1:T}|o_{1:T},a_{1:T})}\left[\sum_{t=1}^{T}\ln q (o_{t}|z_{t})+\ln q(r_{t}|z_{t})\right]\] (17) \[=\sum_{t=1}^{T}\left[\mathbb{E}_{q(s_{t}^{+}|o_{1:t},a_{1:t-1})q( s_{t}^{-}|o_{1:t})}\ln q(o_{t}|s_{t}^{+},s_{t}^{-})+\ln q(r_{t}|s_{t}^{+})\right]\] (18)

The second term of Equation (13) can be dropped because the marginal data probability is irrelevant to the dynamics model \(\widetilde{\mathcal{M}}\). Equation (18) is obtained by the assumption of EX-BMDP that the latent state \(z_{t}\) can be decoupled into endogenous part \(s_{t}^{+}\) and exogenous part \(s_{t}^{-}\).

For term (b), we can directly make use of the derivation in SeMAIL [63] for its Equation (3), that

\[\mathbb{I}(z_{1:T},i_{1:T}|a_{1:T})\leq\sum_{t=1}^{T}\mathbb{E}_{ q(s_{t-1}^{+}|o_{1:t-1},a_{1:t-2})q(s_{t-1}^{-}|o_{1:t-1})}\] (19) \[\left(\text{KL}(p_{\theta^{+}}(s_{t}^{+}|s_{t-1}^{+},a_{t-1},s_{t }^{+})||q_{\theta^{+}}(s_{t}^{+}|s_{t-1}^{+},a_{t-1}))+\text{KL}(p_{\theta^{-} }(s_{t}^{-}|s_{t-1}^{-},s_{t}^{-})||q_{\theta^{-}}(s_{t}^{-}|s_{t-1}^{-},a_{t- 1}))\right)\] (20)

### Proof of Proposition 4.1

**Proposition 4.1** **Restated**.: _(Single step's mutual information) If only we find the policy \(\pi_{\text{expl}}\) that maximize every single step's mutual information \(\sum_{t=0}^{T-1}\mathcal{I}(h_{t+1}^{+};\xi|s_{t}^{+},a_{t})\), then the whole trajectories's mutual information \(\mathcal{I}(h_{1:T}^{+};\xi|s_{0}^{+},\pi_{\text{expl}})\) will be also maximized._

Proof.: \[\mathcal{I}(h_{1:T}^{+};\xi|s_{0}^{+},\pi_{\text{expl}}) =\sum_{t=1}^{T}\mathcal{I}(h_{t}^{+};\xi|h_{t-1}^{+},h_{t-2}^{+},...,h_{1}^{+},s_{0}^{+},\pi_{\text{expl}})\] (21) \[=\sum_{t=1}^{T}\mathcal{I}(h_{t}^{+};\xi|h_{t-1}^{+},h_{t-2}^{+},...,h_{2}^{+},s_{1}^{+}\sim p_{\theta}(\cdot|s_{0}^{+},\pi_{\text{expl}}(s_{0}^ {+}),h_{1}^{+}),\pi_{\text{expl}})\] (22) \[=\sum_{t=1}^{T}\mathcal{I}(h_{t}^{+};\xi|s_{t-1}^{+},\pi_{\text{ expl}})\] (23) \[=\sum_{t=0}^{T-1}\mathcal{I}(h_{t+1}^{+};\xi|s_{t}^{+},a_{t})\] (24)Equation (21) is derived from the Chain Law of Mutual Information. Since we have already established the exploration policy and endogenous inference model \(p_{\theta}\), we can sample the endogenous state \(s^{+}\) for the next step. By iterating this process, we accumulate sufficient information to infer the endogenous state \(s^{+}_{t-1}\), as described in Equation (23).

## Appendix C Additional results

### Visualization of observational trajectories

As illustrated in Figure 9, the observation model effectively reconstructs the original observations, including both background distractors and the agent. This indicates that \(s^{+}_{t}\) and \(s^{-}_{t}\) capture sufficient information from the observations. Furthermore, the endo-decoder outputs successfully reconstruct the endogenous information while eliminating exogenous distractors, demonstrating the ability of our separated model to distinguish task-relevant information from task-irrelevant information, enabling the agent to make decisions without being influenced by extraneous factors.

Figure 10: We present the following: the original observation \(o\), images reconstructed with both the endogenous and exogenous branches \(\hat{o}\), images reconstructed using only the endogenous branch \(\hat{o}^{+}\), images reconstructed using only the exogenous branch \(\hat{o}^{-}\), and images with masking applied.

Figure 9: Presentation of reconstruction results from the observation model and endo-decoder. The first row shows the original observations, while \(\hat{o}_{1:4}\) represents the output of the observation model and \(\hat{o}^{+}_{5:6}\) represents the output of the endo-decoder. Each corresponding index pair indicates a one-to-one relationship between the original observation and the reconstructed observation.

[MISSING_PAGE_FAIL:19]

parameters for SeeX are detailed in Table 2. We introduce a masked image encoder, a novel image decoder for reconstructing the original observation, and an additional RSSM to model the transition of exogenous states.

Elaborating further, the exo-encoder \(E_{\theta^{-}}\) mirrors the structure of the endo-encoder \(E_{\theta^{+}}\) (i.e., the image encoder in [54]). However, we introduce modifications to the conventional decoder to construct our novel endo-decoder \(D_{\theta^{+}}\) and exo-decoder \(D_{\theta^{-}}\), as expressed in the following:

* 1 fully connected layer with 1536 hidden dimensions.
* Reshape tensor into shape of \((-1,1536,1,1)\).
* 4 transposed convolution layers with 6 output channels, stride 2, and ELU activation with \(\alpha=1.0\).
* Decompose 6-channel input into two 3-channel structures. The first represents the reconstruction output, while the remaining three channels encode the corresponding mask.
* 1 output transformation layer of Gaussian distribution.

To effectively reconstruct the observed state, we introduce a novel network architecture for the observation model \(q_{\theta}(\alpha_{t}|s_{t}^{+},s_{t}^{-})\). This network integrates the outputs of the two decoders described earlier to generate the final reconstruction. The detailed network architecture is as follows:

Figure 12: Fine-tuning performance comparison between SeeX and baselines, with 500k pre-training frames. Each bar represents the average normalized return across four walker tasks within a specific domain. We reported the mean of the top 30 highest returns.

Figure 13: Fine-tuning performance comparison between SeeX and baselines, with 1m pre-training frames. Each bar represents the average normalized return across four walker tasks within a specific domain. We reported the mean of the top 30 highest returns.

* Get two images and two masks from \(D_{\theta^{+}}\) and \(D_{\theta^{-}}\) as input.
* 1 2D convolution layer with 3 output channels and kernel size 1. Input: two masks, Output: a final mask.
* Weight two images with the final mask as weights.
* Output the final image.

### Detailed Description about Baselines

Icm.Curiosity is quantified as the discrepancy between the predicted consequences of the agent's actions and the actual outcomes, as represented in a learned visual feature space. This formulation enables efficient exploration, outperforming baseline methods in VizDoom and Super Mario Bros. Notably, ICM [46] exhibits superior exploration capabilities even in the absence of explicit external rewards. The study underscores the significance of evaluating generalization to new scenarios and offers valuable insights into the effectiveness of reinforcement learning algorithms in adapting to novel environments.

Plan2Explore.During exploration, Plan2Explore [54] effectively formulates plans to seek out novel experiences, enabling it to rapidly adapt to downstream tasks in a zero-shot or few-shot setting.

\begin{table}
\begin{tabular}{l l} \hline \hline
**Hyperparameter** & **Value** \\ \hline \(\alpha\) of Exo-Rec & 3 for Walker, 1 for Quadruped and Jaco \\ KL weight & 1 for \(s^{+}\) and \(s^{-}\) \\ Deterministic size & \(32\) \\ Stochastic size & \(32\) \\ Discrete & 32 \\ Embedding size & \(200\) \\ Sequence length \(T\) & \(50\) \\ Batch size & \(50\) \\ Imagine horizon \(H\) & \(15\) \\ Precision & 16 \\ Model optimizer & Adam: (lr: 3e-4, eps: 1e-5, clip: 100, wd: 1e-6) \\ Actor optimizer & Adam: (lr: 8e-5, eps: 1e-5, clip: 100, wd: 1e-6) \\ Critic optimizer & Same as actorâ€™s \\ Action repeat & \(2\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Hyperparameters of SeeX for all experiments.

Figure 14: Fine-tuning performance comparison between SeeX and baselines, with 2m pre-training frames. Each bar represents the average normalized return across four walker tasks within a specific domain. We reported the mean of the top 30 highest returns.

Surpassing existing self-supervised exploration techniques, Plan2Explore achieves competitive zero-shot task performance and eventually outperforms a supervised agent in few-shot scenarios. By leveraging unsupervised exploration to learn a world model, Plan2Explore demonstrates remarkable scalability and data efficiency, paving the way for significant advancements in building real-world reinforcement learning systems.

Rnd.By leveraging a neural network to predict features of observations extracted from a randomly initialized network, RND [10] effectively enhances exploration in Atari games characterized by sparse rewards. Notably, the method surpasses state-of-the-art performance on challenging games like Montezuma's Revenge, achieving remarkable results without relying on human demonstrations or access to the game's underlying state. The paper underscores the flexibility of combining intrinsic and extrinsic rewards, demonstrating that RND facilitates directed exploration at a local level but faces challenges in global exploration over extended time horizons. Overall, the RND method represents significant progress in tackling hard exploration tasks, highlighting the potential of simple yet effective techniques at scale.

Lbs.Latent Bayesian Surprise (LBS) [43] leverages Bayesian surprise in a latent space to generate intrinsic rewards for exploration in Reinforcement Learning (RL). LBS surpasses existing methods on a diverse range of tasks, demonstrating efficient exploration in both continuous-control and discrete-action environments. It exhibits remarkable resilience to stochasticity in the environment dynamics, facilitating in-depth exploration and achieving superior performance in high-dimensional settings such as video games. Notably, LBS significantly reduces computational costs while enhancing exploration capabilities, making it a promising method for enhancing RL algorithms.

Apt.APT [39] actively acquires behaviors and representations by exploring novel states in reward-free environments. This approach maximizes non-parametric entropy in an abstract representation space, circumventing the need for complex density modeling and enabling effective scaling to high-dimensional observation environments. Leveraging intrinsic motivation and particle-based entropy maximization, APT achieves human-level performance on Atari games and surpasses benchmark results on DMControl tasks. Future research directions include reducing sample complexity through model-based RL integration and investigating methods to mitigate catastrophic forgetting during fine-tuning. Overall, the paper showcases APT's efficacy in enhancing performance on challenging RL tasks while requiring substantially fewer samples compared to fully supervised RL algorithms.

Diayn.By maximizing an information-theoretic objective using a maximum entropy policy, DIAYN [16] enables the unsupervised emergence of diverse skills, such as walking and jumping, in simulated robotic tasks. The method demonstrates remarkable effectiveness in exploring complex environments, often solving benchmark tasks without receiving explicit task rewards. Additionally, DIAYN offers strategies for rapid task adaptation, hierarchical reinforcement learning, and imitation learning. By maximizing the mutual information between states and skills, DIAYN enhances the empowerment of a hierarchical agent and ensures skill diversity through maximum entropy policies. Overall, the paper's contributions include proposing a novel method for unsupervised skill learning, showcasing the emergence of diverse skills in various tasks, demonstrating adaptability to new tasks, and providing a foundation for hierarchical reinforcement learning and imitation tasks. DIAYN presents a promising approach for enhancing exploration and data efficiency in reinforcement learning settings.

Aps.By seamlessly integrating variational successor features with nonparametric entropy maximization, APS [38] effectively optimizes the mutual information between tasks and policy-induced states. This method outperforms existing benchmarks on the Atari 100k data-efficiency benchmark by employing entropy maximization to navigate the environment and leveraging data for behavioral learning. To stabilize training and improve convergence, the method incorporates averaging over k nearest neighbors. APS addresses the shortcomings of previous methods by maximizing the entropy of policy-induced states in a lower-dimensional abstract representation space. Empirical results on the Atari benchmark demonstrate state-of-the-art performance, showcasing substantial progress over earlier work. The paper highlights the benefits of utilizing state entropy maximization data for task-conditioned skill discovery and outlines future research directions for further optimization and the integration of diverse approaches.

### Experiments Compute Resources

We implement SeeX with Pytorch and run all the experiments on NVIDIA RTX 3090 for about 7500 GPU hours. The pre-training phase takes 10G of memory and 80 GPU hours for 2M frames, while the fine-tuning phase takes 10G of memory and 6 GPU hours for 100k frames. The codes of SeeX can be found in the supplementary materials.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our theoretical analysis and experimental results in Section 5 and Appendix C provide compelling evidence of the contributions and scope of our work.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: A dedicated subsection Section 7 within the "Contributions" section is devoted to discussing the limitations of our work.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Detailed proofs of all assumptions and propositions presented in this paper can be found in the Appendix.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: To facilitate reproducibility and further exploration, we provide comprehensive supplementary materials in the appendix, including a detailed description of the network framework Appendix D.1, and a comprehensive guide to hyperparameter settings Table 2.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide the raw code and utilized datasets in supplementary materials.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We show a detailed experimental setting in Section 5 and corresponding visualization of environments and distractor videos in Appendix A. Moreover, we provide descriptions of each baseline method in Appendix D.2.
7. **Experiment Statistical Significance**Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: In the main performance results Figure 3, we present the standard deviation, while confidence intervals are shown for the Jaco task results Figure 4.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We show the detailed information of compute resources and relevent memory in Appendix D.3.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed the NeurIPS Code of Ethics and confirm that our submission complies with the guidelines outlined in the document. We have considered the potential harms and societal impacts of our research and have taken steps to mitigate these risks.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Unsupervised visual denoising reinforcement learning holds promise for enhancing image quality with potential benefits in medical imaging, security, and historical preservation. However, societal risks such as deepfakes, algorithmic bias, and privacy concerns must be carefully considered. Mitigation strategies include developing deepfake detection methods, implementing fairness checks, and exploring privacy-preserving techniques. Overall, responsible development and deployment are crucial to ensure that this technology benefits society while minimizing potential harm.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our work on unsupervised visual denoising reinforcement learning does not involve high-risk data or models. The focus is on improving image quality without generating or manipulating sensitive information. Therefore, safeguards for responsible release are not applicable in this context.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The assets used in this paper, including code, training environments, and distractor videos, are all obtained from publicly available sources, and their respective licenses and terms of use have been strictly adhered to. The code for this paper is built upon the code from [50]; the training environment is adapted from [60], with MuJoCo [62] as the underlying simulator; the distractor videos are obtained from [27].
13. **New Assets**Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?

Answer: [NA]

Justification: Our paper does not release new assets.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?

Answer: [NA]

Justification: The paper does not involve crowdsourcing nor research with human subjects.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?

Answer: [NA]

Justification: The paper does not involve crowdsourcing nor research with human subjects.