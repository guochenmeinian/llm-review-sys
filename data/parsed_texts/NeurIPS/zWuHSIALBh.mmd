# Flame+: Factuality-Aware Alignment

for Large Language Models

 Sheng-Chieh Lin\({}^{1}\)+, Luyu Gao\({}^{2}\), Barlas Oguz\({}^{3}\), Wenhan Xiong\({}^{3}\),

**Jimmy Lin\({}^{1}\)**, Wen-tau Yih\({}^{3}\), Xilun Chen\({}^{3}\)+

Footnote †: dagger}\) Xilun and Sheng-Chieh contributed equally to this work.

###### Abstract

Alignment is a procedure to fine-tune pre-trained large language models (LLMs) to follow natural language instructions and serve as helpful AI assistants. We have observed, however, that the conventional alignment process fails to enhance the factual accuracy of LLMs, and often leads to the generation of more false facts (i.e., _hallucination_). In this paper, we study how to make the LLM alignment process more factual, by first identifying factors that lead to hallucination in both alignment steps: supervised fine-tuning (SFT) and reinforcement learning (RL). In particular, we find that training the LLM on new or unfamiliar knowledge can encourage hallucination. This makes SFT less factual as it trains on human-labeled data that may be novel to the LLM. Furthermore, reward functions used in standard RL often inadequately capture factuality and favor longer and more detailed responses, which inadvertently promote hallucination. Based on these observations, we propose _FactuaLity-aware AlignMEnt_ (Flame+), comprised of _factuality-aware SFT_ and _factuality-aware RL_ through direct preference optimization. Experiments show that our proposed Flame+ guides LLMs to output more factual responses while maintaining their instruction-following capability.

Footnote †: dagger}\) Xilun and Sheng-Chieh contributed equally to this work.

## 1 Introduction

Alignment [14] is a procedure to make pre-trained large language models (LLMs) [11, 12] follow human instructions and serve as helpful AI assistants. Despite significant progress in general LLM alignment [14, 15], state-of-the-art aligned LLMs are still prone to generate false claims [17, 18]. In this work, we therefore attempt to advance the understanding of the underlying causes of LLM hallucination as well as its relation to the alignment procedure.

We consider the commonly seen alignment process consisting of two training phases: (1) supervised fine-tuning (SFT) [13]; (2) reinforcement learning (RL) with human [12, 14, 15] or automated feedback [14]. In our study, we find that both the SFT and RL steps in the standard alignment process may actually _encourage_ LLMs to hallucinate. First, in the SFT stage, LLMs are fine-tuned with diverse instructions paired with human-created high-quality responses. While this leads to strong instruction-following capability [14, 15, 16], our study shows that such human-labeled responses may present _new or unknown information_ to the LLM. This, in turn, may inadvertently promote hallucination. Second, we find that the standard reward used in the RL stageoften prefers longer and more detailed responses (Singhal et al., 2023; Chen et al., 2024; Yuan et al., 2024). Consequently, a reward-hacking model ends up with a tendency to produce longer claims with more non-factual information, as shown in the black dots in Figure 1. One possible reason is that most existing RLHF or RLAIF approaches rely on a single scalar reward to represent preference, which struggles to cover multiple alignment skill sets (Ye et al., 2024) and is likely to under-present the aspect of factuality (Hosking et al., 2024).

To address the aforementioned issues, we study the key factors which impact factuality during alignment. In particular, we first conduct a pilot study on the biography generation task (Min et al., 2023) in a more controlled setting where the alignment process focuses solely on factuality (Section 3). Our key observation is that an LLM hallucinates more if it is fine-tuned on new knowledge in either the SFT or the RL stage. For example, an LLM becomes significantly less factual when fine-tuned on responses produced by a model with access to external knowledge (e.g. a retrieval augmented LLM), even though those responses are more factual themselves. Similarly, hallucination is greatly increased if RLAIF is performed on preference pairs that consist of retrieval-augmented LLM output as positive examples and the LLM's own output as negative examples. In comparison, we discover that fine-tuning a pre-trained LLM on a subset of its _own_ generations selected by factuality yields more factual responses and reduces hallucinations.

Next, we apply our findings to improve the factuality of the general LLM alignment process, which is more challenging due to the diversity of instructions. As shown in Figure 2, we observe that some instructions require factual responses while the others do not, and therefore would require different alignment treatments. We first identify fact-based instructions that require factual responses and leverage the findings in our pilot study to create additional training data at both SFT and RL stages to explicitly guide LLMs to output factual responses. Specifically, at the SFT stage, for fact-based instructions, instead of using human created seed training data, we elicit knowledge from the pre-trained LLM and construct training data using its own pre-trained knowledge. This can prevent fine-tuning the LLM on knowledge unknown to itself. At the RL stage, we create additional preference pairs focused on factuality for fact-based instructions, which are combined with the standard preference pairs for instruction following during Direct Preference Optimization (DPO; Rafailov et al., 2023).

We evaluate models on Alpaca Eval (Dubois et al., 2024) and Biography, using win rate for instruction-following capability and FActScore(Min et al., 2023) for factuality evaluation. As shown in Figure 1, using our Flame(r) method (\(\mathrm{SFT}^{\Phi}\) + \(\mathrm{DPO}^{\Phi}\)), a significantly higher FActScore (+5.6 pts) is achieved compared to the standard alignment process (\(\mathrm{SFT}\) + \(\mathrm{DPO}\)), without sacrificing the LLM's instruction-following capability (51.2% win rate). Our ablation study also indicates that identifying fact-based instructions is the key to factual alignment in the general alignment setting.

Figure 1: Models’ helpfulness on Alpaca Eval vs factuality on biography. Helpfulness is measured by models’ win rate over our baseline \(\mathrm{SFT}\) + \(\mathrm{DPO}\) on Alpaca Eval. Dot size represents average length of bio generation.

Related Work

Alignment.Since pre-trained LLMs cannot accurately follow human instructions, a bunch of work has been proposed to improve LLM alignment through SFT and RL. Some propose to improve SFT through data curation (Zhou et al., 2023; Chen et al., 2024), diverse instruction augmentation (Wang et al., 2023; Li et al., 2024) while others focus on RL with human feedback (Ouyang et al., 2022; Bai et al., 2022), AI feedback (Bai et al., 2023; Sun et al., 2024; Yuan et al., 2024). The main goal of these alignment approaches is instruction-following capability (or helpfulness), which may guide LLMs to output detailed and lengthy responses (Singhal et al., 2023) but inevitably encourage hallucination.

Factuality.Prior work has highlighted the issue of hallucination in LLMs (Gao et al., 2022; Kandpal et al., 2023; Mallen et al., 2023). To address the issue, important research lines are factuality evaluation (Min et al., 2023; Wang et al., 2023; Chern et al., 2023) and improvement. Some training-free approaches to improve LLMs' actuality include external knowledge augmentation (Gao et al., 2022; Kandpal et al., 2023; Cheng et al., 2023; Jiang et al., 2023) and specialized decoding (Li et al., 2023; Chuang et al., 2024).

Recent studies apply RL to improve LLMs' factuality. For example, Tian et al. (2024) propose to construct factuality preference pairs for direct preference optimization (DPO; Rafailov et al., 2023), which is closely related to our work. However, they focus solely on enhancing LLMs' factuality through DPO but overlook its potential impact on the models' instruction-following capability, as demonstrated in our experiments. In contrast, our work provides a comprehensive examination of improving LLMs' factuality and instruction-following ability through fine-tuning approaches encompassing both SFT and DPO. Concurrent to our work, Kang et al. (2024) find that LLMs tend to hallucinate when facing unfamiliar queries. They consider improving LLMs' factuality as teaching LLMs to output abstaining or less detailed responses on such unfamiliar queries, a similar behavior observed from our LLMs fine-tuned with Flame (see case studies in Section 6.5). It is worth mentioning that both prior studies focus on a simplified scenario as our pilot study in Section 3: fine-tuning LLMs to improve factuality on a single task (e.g., fine-tuning and evaluating on biography generation). In contrast, we consider the general alignment task, where LLMs are given diverse and complex instructions.

## 3 A Pilot Study on Factual Alignment

In this section, we first study how to align large language models (LLMs) to be more factual. We use biography generation as the task of our pilot study for two main reasons: (1) Biography generation is a simplified setting where factuality is the sole focus of the alignment process. As we will discuss in Section 4, studying factual alignment on diverse human instructions is more complex, as the alignment process encompasses aspects beyond factuality, such as helpfulness and safety. (2) Evaluating the factuality of biography generation is relatively easy since Wikipedia covers sufficient information for public figures and most of the facts about a person are non-debatable (Min et al., 2023).

### Alignment for Biography Generation

A standard alignment procedure consists of supervised fine-tuning (SFT) and reinforcement learning (RL). In this pilot study, our main goal is to teach LLMs to generate biography with reduced misinformation. For the experiment, we compile training and evaluation datasets comprising 500 and 183 diverse human entities, respectively (further details provided in Appendix A.1). We employ FActScore (FS; Min et al., 2023) as the automated metric for assessing factuality, given its fine-grained evaluation capabilities for long-form text generation and its strong correlation with human judgments.3 To study factuality alignment in this pilot study, we posit that training data is needed where the responses are more factual than the LLM's own generations. Thus, we use retrieval-augmented LLMs (RAG; Lewis et al., 2020) to generate training data, which has been shown to output more factual responses (Mialon et al., 2023).

Footnote 3: We use the evaluator: retrieval+llama+npm

Throughout the paper, we refer to the pre-trained (PT), supervised fine-tuned (SFT), and direct preference optimization (DPO) fine-tuned LLMs as \(\mathrm{PT}\), \(\mathrm{SFT}\), and \(\mathrm{DPO}\), respectively.4Sft.We explore two sources of supervision to generate training data (detailed in Appendix A.1): (1) using \(\mathrm{PT}^{\texttt{RAG}}\) with few-shot demonstration to generate biographies for each name entity in training data, where \(\mathrm{PT}^{\texttt{RAG}}\) is \(\mathrm{PT}\) augmented with an off-the-shelf retriever [Lin et al., 2023]; (2) using vanilla \(\mathrm{PT}\) with few-shot demonstration to generate training data as a baseline. As shown in Table 1, \(\mathrm{PT}^{\texttt{RAG}}\) is indeed much more factual than \(\mathrm{PT}\). However, a surprising discovery in the pilot study is that _fine-tuning on such more factual instruction-biography pairs generated by \(\mathrm{PT}^{\texttt{RAG}}\) results in a less factual \(\mathrm{SFT}\) model_ (row 4 vs 3).

Dpo.We further fine-tune the LLMs to be more factual through DPO. An intuitive way to create factuality preference pairs is to directly use the samples from \(\mathrm{PT}^{\texttt{RAG}}\) and \(\mathrm{PT}\) as positives and negatives since \(\mathrm{PT}^{\texttt{RAG}}\) generates more factual biographies than \(\mathrm{PT}\) (row 2 vs 1). Another approach is to employ FactScore (FS) as the reward to select positive and negative samples among the generations from \(\mathrm{PT}\) itself [Tian et al., 2024] (detailed in Appendix A.1). As shown in Table 1, DPO fine-tuned on self-generated data with FS reward guides models to generate more factual responses (row 5 vs 3); however, DPO fine-tuned with the supervision of \(\mathrm{PT}^{\texttt{RAG}}\) makes the models hallucinate even more than its \(\mathrm{SFT}\) counterpart (6 vs 4).

This outcome suggests that compelling models to generate responses akin to \(\mathrm{PT}^{\texttt{RAG}}\) prompts increases hallucination. Conversely, fine-tuning LLMs on their own generations appears to be crucial for factual alignment, a finding applicable to both SFT and DPO fine-tuning.

### Strategies for Factual Alignment

From the pilot study, we find that better quality data (in terms of factuality) for SFT and DPO does not necessarily yield models with better factual alignment. This is likely because the supervision from RAG contains information unknown to the LLM; thus, fine-tuning on RAG generated responses may inadvertently encourage the LLM to output unfamiliar information. To avoid unknown knowledge from being presented to the LLM, a viable strategy is to create SFT and DPO training data using the generated responses from the LLM itself.

## 4 Factuality-Aware Alignment

In the section, we further extend our discussion of factual alignment to encompass more general instructions. Unlike biography generation in Section 3, where factuality is the main alignment objective, human instructions are diverse and complex, necessitating a range of alignment skill sets beyond factuality alone; e.g., logical thinking, problem handling and user alignment [Ye et al., 2024]. Thus, conducting factual alignment with the diverse instructions face two main challenges: (1) different instructions may demand distinct skill sets. For example, in Figure 2, instruction 3, "Please give me a brief history of coffee", necessitates factual accuracy and concise summarization,

\begin{table}
\begin{tabular}{l|c c|c|c} \hline \hline \multicolumn{1}{c|}{\multirow{2}{*}{Llama-2 7B}} & \multicolumn{2}{c|}{src. of supervision} & \multicolumn{2}{c}{**Bio**} \\ \cline{2-5}  & Pos. & Neg. & FS & \# Corr. / Err. \\ \hline \((1)\)\(\mathrm{PT}^{\texttt{RAG}}\) & - & - & 39.1 & 14.4 / 22.0 \\ \((2)\)\(\mathrm{PT}^{\texttt{RAG}}\) & - & - & 55.4 & 18.6 / 15.9 \\ \hline \((3)\)\(\mathrm{SFT}\) & \(\mathrm{PT}\) & - & 37.9 & 13.4 / 21.8 \\ \((4)\)\(\mathrm{SFT}\) & \(\mathrm{PT}^{\texttt{RAG}}\) & - & 35.7 & 13.5 / 23.7 \\ \hline \((5)\)\(\mathrm{DPO}\) & \(\mathrm{PT}^{\texttt{RAG}}\) & \(\mathrm{PT}^{\texttt{RAG}}\) & 41.6 & 15.4 / 20.7 \\ \((6)\) & \(\mathrm{PT}^{\texttt{RAG}}\) & \(\mathrm{PT}^{\texttt{RAG}}\) & 23.5 & 12.7 / 34.9 \\ \hline \hline \multicolumn{5}{l}{\({}^{\star}\)FactScore is used to select positives and negatives.} \\ \end{tabular}
\end{table}
Table 1: Pilot study on bio generation. Pos. denotes the positives for SFT or DPO. Neg. denotes the negatives for DPO. FS denotes FactScore.

Figure 2: Instructions from Open Assistant dataset. The instructions are classified with \(\mathrm{SFT}\) model using the prompt in Appendix Figure 4.

while instruction 8, "Tell me a story about a pig who goes to the moon", prioritizes creativity and imagination over strict factuality. (2) As recent studies have emphasized [23, 14], using a single scalar for reward modeling fails to adequately address multiple alignment skill sets and often under-presents the aspect of factuality.

To tackle the aforementioned challenges, we propose _factuality-aware alignment_ (Flame*). To address the first challenge, we propose to prompt LLMs to classify whether a given instruction demands the response to be factual, as shown in Figure 2. We then apply the factuality fine-tuning strategy for SFT and DPO discussed in Section 3.2 to those fact-based instructions. Furthermore, to address the second challenge, we employ separate rewards to evaluate the factuality and instruction-following capability of an LLM. For simplicity, our work only considers two alignment skill sets: instruction following and factuality. We leave more comprehensive reward modeling to future work.

In the following, we first describe our baseline alignment approach and introduce our proposed factuality-aware alignment built on top of the baseline alignment procedure.

### Baseline Alignment

We initialize \(\mathrm{PT}\) from Llama-2 70B pre-trained model5 and build our baseline alignment procedure following self-rewarding language models [26] due to its simplicity and independence of other strong LLMs (e.g., GPT4) or human evaluators as a reward model. The alignment comprises two steps: (1) building \(\mathrm{SFT}\) model fine-tuned on a high-quality seed data consisting of 3,200 instructions and each instruction is paired with the best response created by humans from Open Assistant dataset [1]; (2) further fine-tuning \(\mathrm{SFT}\) through DPO on instruction-following preference data \((x,y_{+},y_{-})\) constructed by itself (\(\mathrm{SFT}\)) as the reward model, \(\mathrm{RM}^{\mathrm{IF}}\), where \(y_{+}\) and \(y_{-}\) are the positive and negative responses for a given prompt \(x\), respectively. The resulting fine-tuned model is denoted as \(\mathrm{SFT}\) + \(\mathrm{DPO}\). Note that, following [27], we use additional augmented 20K instructions to create the preference training data for DPO fine-tuning. Further details are provided in Appendix A.3.

Footnote 5: https://huggingface.co/meta-llama/Llama-2-70b

### Our Approach

#### 4.2.1 Factuality-Aware SFT (\(\mathrm{SFT}^{\text{\#}}\))

Although leveraging human created high-quality seed data is a reasonable choice for SFT [25], our study in Section 3 suggests that fine-tuning on such high-quality data generated by models other than the LLM itself may present unknown information to the LLM, which may in turn encourage hallucination. To address the above issue, for each instruction from the seed data, we elicit the knowledge from the pre-trained LM itself by generating the responses with a few-shot demonstration. Furthermore, to better use the knowledge from both humans and the pre-trained LLM itself, we propose to utilize human generated responses for non-fact-based instructions, while leveraging the responses sampled from pre-trained LLMs for fact-based instructions to mitigate the introduction of unknown knowledge.

Figure 3: Illustrations of (a) response generation using a pre-trained LLM (\(\mathrm{PT}\)) with few-shot demonstration; (b) factuality-aware alignment.

Specifically, we create factuality-aware alignment training data for SFT with two steps. (1) Classifying instructions: we first prompt \(\mathrm{SFT}\) to judge whether an instruction from the seed data is fact-based (\(x\in X^{\mathrm{fact}}\)) or not.6 (2) Eliciting knowledge from \(\mathrm{PT}\): as illustrated in Figure 3(a), we sample 10 responses from \(\mathrm{PT}\) with 5-shot demonstration, \((x_{0},\mathrm{Human}(x_{0}))\cdots(x_{4},\mathrm{Human}(x_{4}))\), where \(x_{k}\) is the top-\(k\) similar instruction to \(x\) retrieved by DRAGON+ (Lin et al., 2023) from the seed data. \(\mathrm{Human}(x_{k})\) denotes the corresponding human response to \(x_{k}\) in the seed data. As illustrated in Figure 3(b) (upper), the resulting training data for SFT is \((x\notin X^{\mathrm{fact}},\mathrm{Human}(x)),(x\in X^{\mathrm{fact}},\mathrm{ PT}(x))\), where \(\mathrm{PT}(x)\) denotes the set of responses to \(x\) sampled from \(\mathrm{PT}\). The fine-tuned model is denoted as \(\mathrm{SFT}^{\text{\textregistered}}\).

Footnote 6: Prompt for fact-based instruction classification is shown in Appendix Figure 4.

#### 4.2.2 Factuality-Aware DPO (\(\mathrm{DPO}^{\text{\textregistered}}\))

At the second stage of alignment with DPO, we use \(\mathrm{SFT}^{\text{\textregistered}}\) to generate multiple responses \(y_{0},y_{1},\cdots\) for a given instruction \(x\); then, using \(\mathrm{SFT}^{\text{\textregistered}}\) itself as the reward model (\(\mathrm{RM}^{\mathrm{F}}\)) to create a preference pair: \((x,y_{+},y_{-})\).7 The above data creation procedure is the same as the second stage of our baseline alignment in Section 4.1. However, recent studies (Saha et al., 2024; Hosking et al., 2024; Ye et al., 2024) indicate that a single scalar reward from human feedback or LLM reward models may underrepresent the aspect of factuality. To address this limitation, we introduce another factuality reward model (\(\mathrm{RM}^{\mathrm{fact}}\)) to evaluate factuality of responses and create a factuality preference pair for fact-based instructions: \((x\in X^{\mathrm{fact}},y_{\mathrm{me}},y_{\mathrm{false}})\).

Footnote 7: We sample 4 responses for each augmented instruction.

Specifically, we build \(\mathrm{RM}^{\mathrm{fact}}\) with retrieval augmentation to measure the percentage of facts in a response that are correct. \(\mathrm{RM}^{\mathrm{fact}}\) comprises two main components: atomic fact decomposition and retrieval augmented claim verification. We detail the components and ablate their impacts on the quality of \(\mathrm{RM}^{\mathrm{fact}}\) in Appendix A.5. We compute factuality reward for the same responses sampled from \(\mathrm{SFT}^{\text{\textregistered}}\): \(\mathrm{RM}^{\mathrm{fact}}(x,y_{0}),\mathrm{RM}^{\mathrm{fact}}(x,y_{1}),\cdots\). The response with the highest (lowest) factuality reward is chosen as \(y_{\mathrm{me}}\) (\(y_{\mathrm{false}}\)). Note that if the chosen paired responses show large difference in instruction-following reward, we discard the pair; i.e., \(|\mathrm{RM}^{\mathrm{F}}(x,y_{\mathrm{me}})-\mathrm{RM}^{\mathrm{F}}(x,y_{ \mathrm{false}})|>0.5\). As illustrated in Figure 3(b) (lower), in factuality-aware DPO training, the model is initialized from \(\mathrm{SFT}^{\text{\textregistered}}\) and the fine-tuned model is our final factuality-aware aligned model, denoted \(\mathrm{SFT}^{\text{\textregistered}}\) + \(\mathrm{DPO}^{\text{\textregistered}}\). The specific procedures for fine-tuning models in both the SFT and DPO are described in Appendix A.6.

## 5 Experiments

### Evaluation Datasets and Metrics

Instruction Following.We use the 805 instruction-following tasks from Alpaca Eval (Dubois et al., 2024) to evaluate models head-to-head win rate against our baselines using the recommended evaluator: alpaca_eval_gpt4_turbo_fn. We use \(\mathrm{SFT}\) and \(\mathrm{SFT}\) + \(\mathrm{DPO}\) described in Section 4.1 as the baselines for win rate comparisons.

Factuality.We evaluate models on three datasets with diverse knowledge-intensive instructions for factuality. (1) Biography: a knowledge insensitive sub-task of instruction-following tasks. Following our pilot study in Section 3, we use the 183 human entities provided by Min et al. (2023) with the prompt "Tell me a bio of entity name". (2) Alpaca Fact: we extract the fact-based instructions from the 803 instructions using our SFT model (with the prompt shown in Appendix Figure 4), resulting in 241 instructions. (3) FAVA (Mishra et al., 2024)8: the 141 knowledge-intensive instructions from multiple sources, including Open Assistant (Kopf et al., 2023), No Robots (Rajani et al., 2023), WebNLG (Gardent et al., 2017) and manually created datasets. We report FActScore (FS) without length penalty as the metric for all the three datasets. Note that original FS computes proportion of correct facts with additional penalty on short generations with less than 10 atomic facts. This penalty aims to address situations where models provide insufficiently detailed answers. We assume that this aspect is considered in the evaluation of instruction following in Alpaca Eval. In addition, we also report the number of correct and erroneous facts. All the numbers reported are averaged over the instructions in each dataset.

In addition, we also evaluate our fine-tuned models' truthfulness using TruthfulQA [Lin et al., 2022]. We evaluate model performance in the generation task and use ROUGE [Lin, 2004] and BLEU [Papineni et al., 2002] to measure the quality of responses.

### Comparisons of SFT

Table 2 compares the pre-trained Llama-2 70B fine-tuned on OASST dataset with responses from different sources. We list the FActScore (FS) of biography generation using the pre-trained model through Bio 5-shot demonstration as reference (row 0) and \(\mathrm{SFT}\), which is fine-tuned on our seed data with human-created responses, is our baseline (row 1). We first notice that \(\mathrm{SFT}\) shows significant FActScore degradation (53.1 vs 44.7) compared to Bio 5-shot with the pre-trained model. It seems that \(\mathrm{SFT}\) tends to generate more lengthy responses but with more erroneous facts.

When eliciting the knowledge from \(\mathrm{PT}\) by fine-tuning on its own generated responses, \(\mathrm{SFT}^{\mathrm{fact}}\) generates more factual responses in Biography and Alpaca (row 2 vs 1). However, it shows slightly inferior instruction-following capability in Alpaca Eval. This result demonstrates that human responses indeed teach LLMs how to better follow instructions but also encourage LLMs to output more false facts. On the other hand, eliciting the knowledge from the pre-trained model itself avoids the encouragement of hallucination albeit with a slight reduction in instruction-following capability. Finally, \(\mathrm{SFT}^{\mathtt{6}}\) combining supervision from humans and \(\mathrm{PT}\), shows comparable instruction-following capability and output more factual responses on fact-based instructions (row 3 vs 1).

### Comparisons of DPO

Table 3 compares different DPO training recipes. First, we conduct DPO fine-tuning on our SFT baseline, \(\mathrm{SFT}\). When further aligning the model to follow instructions, DPO sees a significant improvement in instruction-following capability (row 2 vs 1) with win rate 72.9 over \(\mathrm{SFT}\); however, the instruction aligned model tends to output lengthy responses with more factual errors (see examples in Appendix Figure 10). On the other hand, when only aligned with factual preference data, DPO\({}^{\mathrm{fact}}\) shows less improvement in instruction-following capability (row 1 vs 3). These results indicate that

\begin{table}
\begin{tabular}{l|c c|c|c|c|c|c|c} \hline \hline \multirow{2}{*}{Llama-2 70B} & \multicolumn{2}{c|}{src. of supervision} & \multicolumn{2}{c|}{**Alpaca Eval**} & \multicolumn{2}{c|}{**Bio**} & \multicolumn{2}{c|}{**Alpaca Fact**} & \multicolumn{2}{c}{**FVA**} \\ \cline{2-9}  & Human & PT & win rate over (1) & FS & \# Corr. / Err. & FS & \# Corr. / Err. & FS & \# Corr. / Err. \\ \hline (0) PT & - & - & - & 53.1 & 15.3 / 13.5 & - & - & - & - \\ \hline (1) SFT & \(\checkmark\) & ✗ & 50.0 & 44.7 & 21.1 / 26.8 & 38.6 & 16.7 / 29.0 & **54.4** & 21.2 / 25.8 \\ (2) SFT\({}^{\mathtt{6}}\) & ✗ & ✓ & 48.1 & 48.5 & 19.6 / 20.6 & **42.0** & 17.5 / 28.4 & 53.3 & 18.3 / 24.2 \\ (3) SFT\({}^{\mathtt{4}}\) & ✓ & ✓ & **51.2** & **49.5** & 19.9 / 19.5 & 41.4 & 18.3 / 27.7 & 54.2 & 19.3 / 22.4 \\ \hline \hline \multicolumn{9}{l}{\({}^{*}\) SFT\({}^{\mathtt{4}}\) uses supervision from \(\mathrm{Human}\) and \(\mathrm{PT}\) for non-fact-based and fact-based instructions, respectively.} \\ \end{tabular}
\end{table}
Table 2: Experimental results of supervised fine-tuning on Open Assistant dataset. \(\mathrm{PT}\) denotes pre-trained Llama2 70B with 5-shot demonstration. \(\mathrm{SFT}^{\mathrm{fact}}\) denotes the variant which only optimizes factuality. FS denotes FActScore.

\begin{table}
\begin{tabular}{l|l c|c|c c|c c|c} \hline \hline \multirow{2}{*}{Llama-2 70B} & \multicolumn{2}{c|}{src. of supervision} & \multicolumn{2}{c|}{**Alpaca Eval**} & \multicolumn{2}{c|}{**Bio**} & \multicolumn{2}{c|}{**Alpaca Fact**} & \multicolumn{2}{c}{**FVA**} \\ \cline{2-9}  & IF & Fact. & win rate over (2) & FS & \# Corr. / Err. & FS & \# Corr. / Err. & FS & \# Corr. / Err. \\ \hline (0) Chat & Proprietary data & 66.2 & 33.2 & 23.4 / 43.6 & 39.3 & 22.3 / 36.4 & 47.5 & 28.0 / 31.3 \\ \hline (1) SFT & - & - & 27.1 & 44.7 & 21.1 / 26.8 & 38.6 & 16.7 / 29.0 & 54.4 & 21.2 / 25.8 \\ (2) + DPO\({}^{\mathtt{4}}\) & ✓ & ✗ & 50.0 & 42.3 & 24.6 / 35.0 & 41.6 & 22.9 / 34.6 & 52.9 & 28.1 / 26.8 \\ (3) + DPO\({}^{\mathtt{6}}\) & ✗ & ✓ & 40.8 & 47.1 & 19.8 / 23.9 & 48.2 & 17.5 / 19.0 & 57.9 & 20.0 / 15.9 \\ (4) + DPO\({}^{\mathtt{6}}\) & ✓ & ✓ & **51.7** & 44.9 & 23.7 / 30.3 & 45.0 & 23.1 / 28.7 & 56.4 & 27.1 / 23.3 \\ \hline (5) SFT\({}^{\mathtt{4}}\) & - & - & 29.1 & **49.5** & 19.9 / 19.5 & 41.4 & 18.3 / 27.7 & 54.2 & 19.3 / 22.4 \\ \hline (6) + DPO & ✓ & ✗ & 50.4 & 46.3 & 24.0 / 28.7 & 43.9 & 21.6 / 28.8 & 55.0 & 25.4 / 22.0 \\ (7) + DPO\({}^{\mathtt{4}}\) & ✓ & ✓ & 51.2 & 47.9 & 25.9 / 28.5 & **48.7** & 24.1 / 25.5 & **58.9** & 29.0 / 22.2 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Experiments of direct preference optimization (DPO). IF. and Fact. denote instruction following \((x,y_{+},y_{-})\) and factuality \((x\in X^{\mathtt{fact}},y_{\mathrm{une}},y_{\mathrm{false}})\) preference data, where \(X^{\mathtt{fact}}\) denotes the set of fact-based instructions. \(\mathrm{DPO}^{\mathtt{fact}}\) denotes the variant which only optimizes factuality. The preference data statistics is listed in Appendix, Table 11.

preference optimization for either instruction following or factuality alone may come at the expense of the other since the former encourages models to output long and detailed responses while the later discourages models to output false claims. When jointly conducting instruction and factuality alignment, \(\mathrm{DPO}^{\mathtt{6}}\) not only better follows instructions but also outputs more factual responses (row 4 vs 1, 2). Finally, initializing from \(\mathrm{SFT}^{\mathtt{4}}\), the DPO fine-tuned models are more factual than their counterparts (i.e., 6 vs 2 and 7 vs 4) without instruction-following capability degrade. We also list the results from Llama-2-Chat 70B (row 0) and observe that despite of its strong instruction-following capability, it tends to output many more incorrect facts. These results demonstrate that standard alignment, even on proprietary commercial data, may encourage LLMs to hallucinate. In contrast, our factuality-aware alignment guides LLMs to output more factual responses without degradation in their general instruction-following capabilities.

It is worth noting that \(\mathrm{SFT}^{\mathtt{fact}}\) and \(\mathrm{DPO}^{\mathtt{fact}}\) are similar to SFT and DPO fine-tuning proposed by Tian et al. (2024), which improve LLMs' factuality but degrade their instruction-following capability. Also, we do not observe our SFT and DPO variants outperform the pre-trained model with few-shot demonstrations on biography generation (row 0 in Table 2. This is possibly due to the alignment tax found in previous work (Ouyang et al., 2022), which degrades LLMs' accuracy on the standard knowledge benchmarks. How to improve both models' instruction-following capability and their accuracy on standard knowledge benchmarks is worth exploring, which we leave for future work.

### Results on TruthfulQA

Table 4 compares models performance on TruthfulQA. Generally, we observe that our factuality-aware alignment training guides LLMs to output more truthful responses. For example, factuality-aware SFT improves LLMs' truthfulness (row 5 vs 1). In addition, DPO fine-tuning on the factuality preference data guides LLMs to output more truthful responses (rows 3,4 vs 2 and 7 vs 6). Note that we observe that \(\mathrm{SFT}\) and \(\mathrm{DPO}\) models show a reverse trend in BLUE and ROUGE. This is likely because \(\mathrm{SFT}\) models tend to generate shorter responses than the \(\mathrm{DPO}\) ones do.

In addition, Table 5 reports models' accuracy in tasks of multiple choices from TruthfulQA. No significant differences between models are observed. This is possibly because we mainly focus on the tasks of long-form response generation while TruthfulQA-MC task is formed by short-form answers. The discrepancy between improving LLMs' factuality on long-form and short-form generation is also found by the previous work (Chuang et al., 2024). Appendix Table 9 reports more evaluation results on other NLP benchmarks.

## 6 Discussion

### Effects of Fact-Based Instruction Classification

In our factuality-aware alignment, we prompt \(\mathrm{SFT}\) to judge whether an instruction requires a factual response and apply our factuality alignment strategy to the fact-based instruction. Without the instruction classification, in our factuality-aware SFT, we cannot create supervision from \(\mathrm{Human}\) and \(\mathrm{PT}\) responses for respective non-fact-based and fact-based instructions. Instead, for each instruction, we create instruction-response pairs from 1 and 10 responses from \(\mathrm{Human}\) and \(\mathrm{PT}\) as supervisions, respectively. Note that, during fine-tuning, for each instruction, we randomly sample instruction-response pair either created from \(\mathrm{Human}\) or \(\mathrm{PT}\) with same probability. The SFT model shows

\begin{table}
\begin{tabular}{l|l c|c c} \hline \hline \multirow{2}{*}{Llama-2 70B} & \multicolumn{2}{c|}{src. of supervision} & \multicolumn{2}{c}{**TruthfulQA**} \\ \cline{2-5}  & IF. & Fact. & BLUE & ROUGE \\ \hline (0) Chat & \multicolumn{2}{c|}{Proprietary data} & 0.21 & 1.16 \\ \hline (1) \(\mathrm{SFT}\) & - & - & 0.37 & 0.20 \\ \hline (2) + \(\mathrm{DPO}\) & ✓ & ✗ & 0.03 & 0.54 \\ (3) + \(\mathrm{DPO}^{\mathrm{fact}}\) & ✗ & ✓ & 0.30 & 1.12 \\ (4) + \(\mathrm{DPO}^{\mathtt{6}}\) & ✓ & ✓ & 0.15 & 0.80 \\ \hline (5) \(\mathrm{SFT}^{\mathtt{6}}\) & - & - & 0.39 & 0.51 \\ (6) + \(\mathrm{DPO}^{\mathtt{6}}\) & ✓ & ✗ & 0.07 & 0.91 \\ (7) + \(\mathrm{DPO}^{\mathtt{6}}\) & ✓ & ✓ & 0.20 & 0.96 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Results on TruthfulQA.

\begin{table}
\begin{tabular}{l|l c|c c} \hline \hline \multirow{2}{*}{Llama-2 70B} & \multicolumn{2}{c|}{src. of supervision} & \multicolumn{2}{c}{**TruthfulQA**-MC} \\ \cline{2-5}  & IF. & Fact. & MC1 & MC2 & MC3 \\ \hline (0) Chat & \multicolumn{2}{c|}{Proprietary data} & 32.50 & 52.54 \\ \hline (1) \(\mathrm{SFT}\) & - & - & 30.8 & 45.7 & 23.9 \\ (2) + \(\mathrm{DPO}\) & ✓ & ✗ & 30.5 & 46.0 & 23.4 \\ (3) + \(\mathrm{DPO}^{\mathtt{fact}}\) & ✗ & ✓ & 31.8 & 46.8 & 24.3 \\ (4) + \(\mathrm{DPO}^{\mathtt{6}}\) & ✓ & ✓ & 30.8 & 46.0 & 23.6 \\ (5) \(\mathrm{SFT}^{\mathtt{6}}\) & - & - & 29.9 & 44.8 & 22.5 \\ (6) + \(\mathrm{DPO}\) & ✓ & ✗ & 31.5 & 47.0 & 24.0 \\ (7) + \(\mathrm{DPO}^{\mathtt{6}}\) & ✓ & ✓ & 30.5 & 45.4 & 23.1 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Results on TruthfulQA multiple choices.

[MISSING_PAGE_FAIL:9]

puts responses with similar length as \(\mathrm{DPO}\) on Alpaca Eval, \(\mathrm{DPO}^{\phi}\) generates a slightly shorter responses for the fact-based instructions in the other three datasets. This results show that our factuality-aware DPO training mainly impacts models' responses for fact-based instructions. The impact is mainly to reduce the false claims, evidenced by the numbers of erroneous facts in rows 2 and 4 of Table 3).

### Case Studies

Figure 10 (in Appendix) showcases the generations of different models, \(\mathrm{SFT}\), \(\mathrm{SFT}\) + \(\mathrm{DPO}\) and \(\mathrm{SFT}^{\phi}\) + \(\mathrm{DPO}^{\phi}\), on Alpaca Eval and Biography. Given the instruction, "What are the names of some famous actors that started their careers on Broadway?", \(\mathrm{SFT}\) only lists some names of Broadway actors while DPO fine-tuned models generate detailed information for each listed Broadway actor. As for biography generations, we observe that given the instruction to generate a biography for a rare name entity, Marianne McAndrew, \(\mathrm{SFT}\) + \(\mathrm{DPO}\) generates a detailed response but with many wrong facts while \(\mathrm{SFT}\) and \(\mathrm{SFT}^{\phi}\) + \(\mathrm{DPO}^{\phi}\) give relatively short responses. For the frequent entity, Ji Sung, all the models generate detailed and mostly correct responses. This qualitative analysis shows that \(\mathrm{SFT}^{\phi}\) + \(\mathrm{DPO}^{\phi}\) tends to generate detailed responses for most instructions, but for those instructions required tailed knowledge (e.g., rare entity) likely unknown to LLMs (Mallen et al., 2023), it reduces erroneous facts by giving less detailed responses, which is also observed by Kang et al. (2024).

## 7 Conclusion and Future Work

In this paper, we present a study to enhance the factuality of large language models (LLMs). We first identify that the standard alignment approach, comprising SFT and RLAIF with DPO, may inadvertently encourage LLMs to produce more erroneous facts. Specifically, during the SFT stage, fine-tuning LLMs with high-quality human responses may introduce unfamiliar information, prompting LLMs to output unknown facts. Additionally, during the DPO stage, enhancing LLMs' ability to follow instructions may result in more detailed and lengthy responses but often leads to increased hallucination. To tackle the shortcomings of the standard alignment, we propose a factuality-aware alignment method, which includes factuality-aware SFT and DPO. Quantitative and qualitative analyses demonstrate that our factuality-aware alignment not only guides LLMs to generate detailed and helpful responses but also helps prevent the generation of false claims.

While we have successfully integrated factuality into standard alignment procedure, our work only considers two alignment skill sets: instruction following (or helpfulness) and factuality. In practice, each instruction may require consideration of multiple and distinct alignment skill sets (Saha et al., 2024). The method to optimize for these skill sets tailored to each query requires further study. In our experiments, we note that optimizing preferences solely for instruction following or factuality could potentially compromise the other. While our factuality-aware alignment demonstrated improvements in both aspects, it is uncertain whether there is a trade-off between the two aspects when integrating our approach to large-scale alignment (Touvron et al., 2023). Finally, as shown in Appendix Figure 7, not all the claims (or sentences) in a response require fact verification, a more accurate factuality reward model should take this factor into account. While our preliminary experiment, which removes non-fact-based sentences from the factuality reward modeling (Section 6.2), shows suboptimal performance, we believe that further study can bring more insights.

## Acknowledgements

We thank Bhargavi Paranjape for sharing fine-tuned Llama-2 7B for atomic fact decomposition and Jing Xu, Weizhe Yuan and Jason Weston for their helpful suggestions.

## References

* Bai et al. (2018) Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan.

Training a helpful and harmless assistant with reinforcement learning from human feedback. _arXiv:2204.05862_, 2022.
* Bai et al. (2022) Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional AI: Harmlessness from AI feedback. _arXiv:2212.08073_, 2023.
* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In _Proc. NeurIPS_, pages 1877-1901, 2020.
* Chen et al. (2022) Jifan Chen, Aniruddh Sriram, Eunsol Choi, and Greg Durrett. Generating literal and implied subquestions to fact-check complex claims. In _Proc. EMNLP_, pages 3495-3516, 2022.
* Chen et al. (2024) Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxia Jin. AlpaGasus: Training a better Alpaca model with fewer data. In _Proc. ICLR_, 2024a.
* Chen et al. (2024) Lichang Chen, Chen Zhu, Davit Soselia, Jiuhai Chen, Tianyi Zhou, Tom Goldstein, Heng Huang, Mohammad Shoeybi, and Bryan Catanzaro. ODIN: Disentangled reward mitigates hacking in RLHF. _arXiv:2402.07319_, 2024b.
* Cheng et al. (2023) Silei Cheng, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, and Lijuan Wang. Prompting GPT-3 to be reliable. In _Proc. ICLR_, 2023.
* Chern et al. (2023) I-Chun Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, Pengfei Liu, et al. FacTool: Factuality detection in generative AI-a tool augmented framework for multi-task and multi-domain scenarios. _arXiv:2307.13528_, 2023.
* Chuang et al. (2024) Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James R. Glass, and Pengcheng He. DoLa: Decoding by contrasting layers improves factuality in large language models. In _Proc. ICLR_, 2024.
* Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021.
* Dubois et al. (2024) Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. AlpacaFarm: A simulation framework for methods that learn from human feedback. _arXiv:2305.14387_, 2024.
* Gao et al. (2022) Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, N. Lao, Hongrae Lee, Da-Cheng Juan, and Kelvin Guu. RARR: Researching and revising what language models say, using language models. In _Proc. ACL_, 2022.
* Gardent et al. (2017) Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. The WebNLG challenge: Generating text from RDF data. In _Proc. INLG_, pages 124-133, 2017.
* Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In _Proc. ICLR_, 2021.
* Hosking et al. (2024) Tom Hosking, Phil Blunsom, and Max Bartolo. Human feedback is not gold standard. In _Proc. ICLR_, 2024.
* Hosking et al. (2020)Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented language models. _Journal of Machine Learning Research_, pages 1-43, 2023.
* Jiang et al. (2023) Zhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. Active retrieval augmented generation. In _Proc. EMNLP_, pages 7969-7992, 2023.
* Kandpal et al. (2023) Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large language models struggle to learn long-tail knowledge. In _Proc ICML_, 2023.
* Kang et al. (2024) Katie Kang, Eric Wallace, Claire Tomlin, Aviral Kumar, and Sergey Levine. Unfamiliar finetuning examples control how language models hallucinate. _arXiv:2403.05612_, 2024.
* democratizing large language model alignment. _arXiv:2304.07327_, 2023.
* Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In _Proc. NeurIPS_, pages 9459-9474, 2020.
* Li et al. (2023) Kenneth Li, Oam Patel, Fernanda Viegas, Hanspeter Pfister, and Martin Wattenberg. Inference-time intervention: Eliciting truthful answers from a language model. In _Proc. NeurIPS_, 2023.
* Li et al. (2024) Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer, Jason E Weston, and Mike Lewis. Self-alignment with instruction backtranslation. In _Proc. ICLR_, 2024.
* Lin (2004) Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In _Text Summarization Branches Out_, pages 74-81, July 2004.
* Lin et al. (2023) Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy Lin, Yashar Mehdad, Wen-tau Yih, and Xilun Chen. How to train your DRAGON: Diverse augmentation towards generalizable dense retrieval. In _Proc. Findings of EMNLP_, pages 6385-6400, 2023.
* Lin et al. (2022) Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. In _Proc. ACL_, pages 3214-3252, 2022.
* Liu et al. (2023) Yixin Liu, Alex Fabbri, Pengfei Liu, Yilun Zhao, Linyong Nan, Ruilin Han, Simeng Han, Shafiq Joty, Chien-Sheng Wu, Caiming Xiong, and Dragomir Radev. Revisiting the gold standard: Grounding summarization evaluation with robust human evaluation. In _Proc. ACL_, pages 4140-4170, 2023.
* Malaviya et al. (2023) Chaitanya Malaviya, Subin Lee, Sihao Chen, Elizabeth Sieber, Mark Yatskar, and Dan Roth. ExpertQA: Expert-curated questions and attributed answers. _arXiv:2309.07852_, 2023.
* Mallen et al. (2023) Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In _Proc. ACL_, pages 9802-9822, 2023.
* Mialon et al. (2023) Gregoire Mialon, Roberto Dessi, Maria Lomeli, Christoforos Nalmparantis, Ramakanth Pasunuru, Roberta Raileanu, Baptiste Roziere, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann LeCun, and Thomas Scialom. Augmented language models: a survey. _Transactions on Machine Learning Research_, 2023.
* Min et al. (2023) Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. In _Proc. EMNLP_, pages 12076-12100, 2023.
* Mishra et al. (2024) Abhika Mishra, Akari Asai, Vidhisha Balachandran, Yizhong Wang, Graham Neubig, Yulia Tsvetkov, and Hannaneh Hajishirzi. Fine-grained hallucination detection and editing for language models. _arXiv:2401.06855_, 2024.
* Mnih et al. (2020)OpenAI. GPT-4 technical report. _arXiv:2303.08774_, 2023.
* Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In _Proc. NeurIPS_, pages 27730-27744, 2022.
* Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for automatic evaluation of machine translation. In _Proc. ACL_, pages 311-318, 2002.
* Rafailov et al. (2023) Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In _Proc. NeurIPS_, pages 53728-53741, 2023.
* Rajani et al. (2023) Nazneen Rajani, Lewis Tunstall, Edward Beeching, Nathan Lambert, Alexander M. Rush, and Thomas Wolf. No robots. _Hugging Face repository_, 2023.
* Saha et al. (2024) Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, and Xian Li. Branch-Solve-Merge improves large language model evaluation and generation. In _Proc. NAACL_, pages 8352-8370, 2024.
* Sanh et al. (2022) Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesh Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Techan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask prompted training enables zero-shot task generalization. In _Proc. ICLR_, 2022.
* Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _arXiv:1707.06347_, 2017.
* Singhal et al. (2023) Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. A long way to go: Investigating length correlations in RLHF. _arXiv:2310.03716_, 2023.
* Sun et al. (2024) Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen, David Daniel Cox, Yiming Yang, and Chuang Gan. SALMON: Self-alignment with principle-following reward models. In _Proc. ICLR_, 2024.
* Tian et al. (2024) Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher D Manning, and Chelsea Finn. Fine-tuning language models for factuality. In _Proc. ICLR_, 2024.
* Touvron et al. (2021) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajiwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcia Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. _arXiv:2307.09288_, 2023.
* Wang et al. (2022) Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Raveshaj Singh Puri,Rushing Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. In _Proc. EMNLP_, pages 5085-5109, 2022.
* Wang et al. (2023a) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-Instruct: Aligning language models with self-generated instructions. In _Proc. ACL_, pages 13484-13508, 2023a.
* Wang et al. (2023b) Yuxia Wang, Revanth Gangi Reddy, Zain Muhammad Mujahid, Arnav Arora, Aleksandr Ruba-shevskii, Jiahui Geng, Osama Mohammed Afzal, Liangming Pan, Nadav Borenstein, Aditya Pillai, Isabelle Augenstein, Iryna Gurevych, and Preslav Nakov. Factcheck-GPT: End-to-end fine-grained document-level fact-checking and correction of LLM output. _arXiv:2311.09000_, 2023b.
* Ye et al. (2024) Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, and Minjoon Seo. FLASK: Fine-grained language model evaluation based on alignment skill sets. In _Proc. ICLR_, 2024.
* Yuan et al. (2024) Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason E Weston. Self-Rewarding language models. In _Proc. ICML_, pages 57905-57923, 2024.
* Zhou et al. (2023) Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. LIMA: Less is more for alignment. In _Proc. NeurIPS_, pages 55006-55021, 2023.

Appendix

### Biography Data Generation

Entities for Training and Evaluation.We use 500 diverse human entities to create training data for SFT and DPO; then, evaluate LLMs' generation factuality on another 183 human entities from Min et al. [2023].9 Note that the human entities for training and evaluation are uniformly sampled from entities across diverse nationalities, professions, and rarities. The instruction is generated with the format: Tell me a bio of entity name.

Footnote 9: https://github.com/shmsw25/FActScore

Creating Training Data for SFT.We randomly sample 5 human entities among the 500 entities for training and generate their biographies using Llama-2-Chat 70B as 5-shot demonstration.10 With the 5-shot demonstration, we use pre-trained Llama-2 7B to generate 10 biographies for each human entity from the remaining 495 ones.11 We set temperature 0.7 and top-p 0.9 when generate multiple responses from LLMs in all our experiments. We use the created 4,950 name entity-biography pairs to fine-tune the pre-trained Llama-2 7B. As for generating training data with RAG, we prepend the top-10 passages from our retrieval system (detailed in Appendix A.2) to each instruction and generate 10 biographies for each entity from RAG with 5-shot demonstrations. Note that we only prepend top-1 passage for each instruction in the demonstration.

Footnote 10: https://huggingface.co/meta-llama-2-70b-chat-hf

Creating Factuality Preference Pairs for DPO.To construct factuality preference pairs, we first compute FactScore (FS) for all the 4,950 biographies previously created by \(\mathrm{PT}\). Then, for each name entity, we compare the FS for all the possible 45 pairs from the 10 generated biographies and construct DPO pairs using the biography with a higher (lower) FS as a positive (negative). Note that we discard the pairs if they show tied FS.

### Retrieval Models

For each query, we retrieve top-\(20\) candidate passages from Wikipedia using DRAGON+ [Lin et al., 2023] and re-rank the candidates using a 12-layer cross-encoder12. We use the Wikipedia version from the Dec. 20, 2021 dump released by Izacard et al. [2023] in this work.

Footnote 12: https://huggingface.co/sentence-transformers/all-HiniLM-L12-v2

Footnote 13: https://huggingface.co/datasets/OpenAssistant/oasst1

### Alignment with Self Rewarding

Sft.At SFT stage, we fine-tune \(\mathrm{PT}\) on two seed datasets: (1) Instruction-following training (IFT) data from Li et al. [2024], consisting of 3200 instruction-response pairs created by humans from Open Assistant dataset [OASST; Kopf et al., 2023], where we only use the first conversational turns in the English that are annotated rank 0;13 (2) evaluation following training (EFT) data from Yuan et al. [2024], the LLM-as-a-Judge data consists of 1630 samples, each of which contains instruction, human response and the corresponding score of 1-5 scale (with chain-of-though evaluation reasoning): \((x,y,r)\), where \((x,y)\) pairs are also selected from OASST other than training pairs and \(r\) is created by the model fine-tuned only on IFT with manual filtering. The purpose of EFT is to enhance a LLM's capability as a reward model to judge the quality of a response in terms of relevance, coverage, usefulness, clarity and expertise. We refer readers to Yuan et al. [2024] for how EFT is created and filtered with minimum human efforts. The prompt template for LLM-as-a-Judge in EFT and an EFT training sample are shown in Appendix, Figure 8 and 9. We refer the baseline model fine-tuned on the IFT and EFT datasets as \(\mathrm{SFT}\).

Footnote 14: https://huggingface.co/meta-llama/Llama-2-70b-chat-hf

DPO for Instruction Following.At the subsequent preference learning with DPO, following Wang et al. [2023a], we augment additional 20K instructions with Llama-2 70B chat model.15 For each augmented instruction \(x\), we use \(\mathrm{SFT}\) to generate 4 responses and evaluate how well the responses follow the instruction with score of 1-5 scale: \(\mathrm{RM}^{\mathrm{IF}}(x,y_{0})\cdots;\mathrm{RM}^{\mathrm{IF}}(x,y_{3})\), where \(y_{0},\cdots,y_{3}\in\mathrm{SFT}(x)\) and \(\mathrm{RM}^{\mathrm{F}}\) is the instruction-following reward model. Note that, in self-rewarding [23], \(\mathrm{RM}^{\mathrm{F}}\) is the same as \(\mathrm{SFT}\) model. In addition, for each instruction-response pair, we use the same prompt in EFT seed data to sample the chain-of-thought evaluation three times and average the scores as the reward. Finally, for each instruction, we use the response with the highest (lowest) reward as the positive (negative) sample to form a preference pair for DPO training: \((x,y_{+},y_{-})\). We discard the pair, if \(\mathrm{RM}^{\mathrm{F}}(x,y_{+})=\mathrm{RM}^{\mathrm{F}}(x,y_{-})\). In the DPO training, the model is initialized from \(\mathrm{SFT}\) and the fine-tuned model is denoted \(\mathrm{SFT}+\mathrm{DPO}\).

### More Evaluation Results on Standard Benchmarks

Table 9 compares the instruction fine-tuned models' (w/o and w/ involving Flame) accuracy the in tasks of MMLU [13] and GSM8K [21]. A slight drop from Flame is observed. This is possibly because we mainly focus on the tasks of long-form response generation while MMLU and GSM8K are the benchmarks with short-form answers. The discrepancy between improving LLMs' factuality on long-form and short-form generation is also found by the previous work [21].

### Factuality Reward Modeling

Factuality Reward Models.We build a reward model \(\mathrm{RM}^{\mathrm{fact}}\) to measure the factuality of each response. The factuality reward model consists of two main modules. (1) fact decomposition: we first use nltk.tokenize to split a response into sentences; then, use our Llama-2 7B model fine-tuned on public datasets [16, 22, 23] to conduct atomic fact decomposition for each sentence.15 (2) Retrieval augmented claim verification: for each decomposed fact (or claim), we use the instruct Llama 7B fine-tuned on Super Natural Instructions [20] to do fact check with the prompt shown in Figure 5.16 We append 10 retrieved supports (using the instruction as query) from our retrieval and re-ranking pipeline in Appendix A.2. Then, we compute the proportion of correct atomic facts in a response as a factuality reward.

Footnote 15: With few-shot demonstration, \(\mathrm{SFT}\) is able to decompose a sentence into atomic facts with acceptable accuracy. Fine-tuning a Llama-2 7B is to reduce the inference time.

Quality of Factuality Reward Models.We conduct ablation study on our factuality reward models. Specifically, we use our factuality reward models to detect the number of error facts in each instruction-response pair. We try different models for fact check using the prompt shown in Figure 5 with different numbers of retrieved supports. We use the LLMs' generated responses with human annotated hallucination provided by Mishra et al. [20] to evaluate the quality of the factuality reward models.17 Specifically, we rank the responses by numbers of errors detected and calculate the Kendall rank correlation (\(\tau\)) between the rank lists by our factuality reward models and humans. As shown in Table 10, conducing fact check with more retrieved supports improves the accuracy of the factuality reward models (row 2 vs 1). In addition, our \(\mathrm{SFT}\), only fine-tuned on the IFT and EFT data, is capable of doing fact check, compared to Instruct Llama 7B fine-tuned on Super Natural Instructions [20]. Finally, instead of computing the number of error facts from decomposed atomic facts, we conduct fact check directly for each sentence in a response and calculate the number of false sentences as error facts. However, the quality of the reward models shows significant decrease (rows 5,6 vs 1,2). We finally adopt row 2 as our factuality reward model.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline  & fact check model & \# sup. & fact unit & \(\tau\) \\ \hline (1) & Instruct Llama 7B & 5 & atom. & 0.32 \\ (2) & Instruct Llama 7B & 10 & atom. & 0.34 \\ (3) & \(\mathrm{SFT}\) (Llama-2 70B) & 5 & atom. & 0.28 \\ (4) & \(\mathrm{10}\) & 0.31 \\ \hline (5) & Instruct Llama 7B & 5 & \(\mathrm{10}\) & sent. & 0.20 \\ (6) & Instruct Llama 7B & 10 & \(\mathrm{10}\) & 0.25 \\ \hline \hline \end{tabular}
\end{table}
Table 10: A comparison of factuality reward models. \(\tau\) denotes the correlation between human annotation.

\begin{table}
\begin{tabular}{l|l|l|l|l} \hline \hline \multirow{2}{*}{Llama-2 70B} & \multicolumn{2}{l|}{src. of supervision} & \multirow{2}{*}{**MMLU**} & \multirow{2}{*}{**GSM8K**} \\  & & IF & Fact. & \\ \hline (1) \(\mathrm{SFT}+\mathrm{DPO}\) & ✓ & ✗ & 69.3 & 59.3 \\ (2) \(\mathrm{SFT}^{\mathsf{T}}+\mathrm{DPO}^{\mathsf{*}}\) & ✓ & ✓ & 69.1 & 58.2 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Results on MMLU and GSM8K.

[MISSING_PAGE_EMPTY:17]

[MISSING_PAGE_EMPTY:18]

[ENDFIGURE]

## 6 Conclusion

Figure 9: An example of EFT data. The texts with the colors of green, red and blue are the instruction, response and the LLM-as-a-judge results (explanation and score), respectively

[MISSING_PAGE_EMPTY:20]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claim of our paper is to advance the understanding of the underlying causes of LLM hallucination as well as its relation to the alignment procedure (from lines 22-23) and study how to make the LLM alignment process more factual (lines 5-6). We thus conduct a pilot study to identify the key factors which impact the LLMs' factuality during alignment procedure in Section 3. Based on the finding, we implement the factuality-aware alignment in Section 4 and demonstrate the effectiveness of our approach through comprehensive experiments in Section 5. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We include the discussion of limitations in Appendix A.7. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: This is an empirical paper; thus, there is no theoretical result included. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We include all the high-level information to reproduce our models in Section 4.1 and 4.2, and due to space limitation, more detailed information are included in Appendix A.3, A.5 and A.6. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.

In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: While we do not provide the code to reproduce the main experimental results, we provide all the necessary information and URL links to training and evaluation data. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide training details in Appendix A.6 and test details in Section 5.1 Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: We follow the instruction following evaluation in Alpaca Eval (Dubois et al., 2024) to report the win rate comparisons between models, which is not suitable for statistical significance test. For FActScore, we follow the established procedure to compare models' average FActScore(Min et al., 2023), which is correlated to human evaluation. Guidelines:* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide the required computation resources in Appendix A.6. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed and made sure our paper conforms the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes]Justification: We have included the discussion of broader impacts in Appendix A.8. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We do not release any new data and models in the paper. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have cited all the papers, which provide the models and datasets used in our paper. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL.

* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.