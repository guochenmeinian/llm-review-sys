# Directed Cyclic Graph for Causal Discovery from Multivariate Functional Data

Saptarshi Roy

Department of Statistics

Texas A&M University

College Station, TX 77843

roys8001@stat.tamu.edu

&Raymond K. W. Wong

Department of Statistics

Texas A&M University

College Station, TX 77843

raywong@tamu.edu

&Yang Ni

Department of Statistics

Texas A&M University

College Station, TX 77843

yni@stat.tamu.edu

###### Abstract

Discovering causal relationship using multivariate functional data has received a significant amount of attention very recently. In this article, we introduce a functional linear structural equation model for causal structure learning when the underlying graph involving the multivariate functions may have cycles. To enhance interpretability, our model involves a low-dimensional causal embedded space such that all the relevant causal information in the multivariate functional data is preserved in this lower-dimensional subspace. We prove that the proposed model is causally identifiable under standard assumptions that are often made in the causal discovery literature. To carry out inference of our model, we develop a fully Bayesian framework with suitable prior specifications and uncertainty quantification through posterior summaries. We illustrate the superior performance of our method over existing methods in terms of causal graph estimation through extensive simulation studies. We also demonstrate the proposed method using a brain EEG dataset.

## 1 Introduction

Motivation.Multivariate functional data arise in many fields such as biomedical research (Wei and Li, 2008; Chiou and Muller, 2016), environmental science (Korte-Stapff et al., 2022), finance (Kowal et al., 2017), plant science (Wong et al., 2019; Park et al., 2022), and sport science (Volkmann et al., 2021) where multiple variables are measured over time or other domains. The increasing availability of functional data in these fields provides us with great opportunities to discover causal relationships among random functions for the better understanding of complex systems, which is helpful for various machine learning and statistics tasks such as representation learning (Scholkopf et al., 2021), fairness (Tang et al., 2023), transfer learning (Rojas-Carulla et al., 2018), and reinforcement learning (Zeng et al., 2023). One motivating example is electroencephalography (EEG) where electrical activity from the brain is recorded non-invasively from electrode channels by placing them on the scalp or directly on the surface of the brain. Given its continuous nature and the short time separation between the adjacent measuring points, it is natural to treat the data at each brain location/region as a function over time. A relevant scientific goal is to estimate brain effective connectivity among different regions, which will potentially allow us to make better decisions, design more effective interventions, and avoid unintended consequences. However, existing structural equation model (SEM) based causal discovery methods assume acyclic relationships among the random functions by imposing a directed acyclic graph (DAG) structure, which may be too restrictive for many real applications. For example, there are strong indications that in brain effective connectivity studies, due to reciprocal polysynaptic connections, the brain regions are far from exhibiting acyclicity (Friston, 2011; Markov et al., 2012), and that in genetic pathways, due to the presence of multiple upstream regulators and downstream targets for every signaling component, feedback loops/directed cycles are regular motifs (Brandmanand Meyer, 2008]. Thus, in light of the prevalence of cycles in complex systems, it is desirable to have a flexible model for causal discovery among random functions that can account for such cyclic causal structures.

Challenges.Causal discovery for multivariate functional data in the presence of cycles is an inherently difficult problem that is not yet well understood. We highlight three prominent challenges. (i) Functional data are infinite-dimensional in nature. It may so happen that the low-frequency spectrum of one curve might causally influence the high-frequency spectrum of another curve. This demands identification of pertinent features that can be used to create a finite-dimensional representation of the data, which is easier to work with and analyze. However, the challenge is that we may not know _a priori_ what the relevant features are when dealing with infinite-dimensional objects. Blind adoption of standard (non-causal-adaptive) low-dimensional features can lead to errors or inaccuracies. (ii) Although the identifiability of causal models for multivariate functional data in the absence of cycles has been established in recent works [Zhou et al., 2022b, Lee and Li, 2022], showing identifiability of causal models from multivariate data, let alone multivariate functions, is still a challenging and complex task in cases where causal relationships are obscured by the presence of cycles. (iii) It is common that functional data are only observed over discrete time points with additional noises. Such incomplete and noisy observations of the functions add another layer of difficulty in probing the causal relationships of interest.

Related work.Causal discovery from multivariate functional data has been studied by a few recent works [Zhou et al., 2022b, Lee and Li, 2022, Yang and Suzuki, 2022], which have already shown some promising results in discovering causality in, e.g., EEG data and longitudinal medical record data. However, all of them are limited to DAGs, which do not allow inference of cyclic causality. While there has been a surge of research on causal discovery methods for scalar random variables in the presence of feedback loops/cycles over the last few decades [Richardson, 1996, Lacerda et al., 2008, Mooij et al., 2011, Hyttinen et al., 2012, Huang et al., 2019, Mooij and Heskes, 2013, Mooij and Claassen, 2020, Zhou et al., 2022a], none of these approaches have been extended to discovering causal dependencies among random functions in multivariate settings. Therefore, how to handle cyclic causal relationships among multivariate functional data while addressing the aforementioned challenges remains a largely unsolved problem.

Contributions.In this paper, we propose an operator-based non-recursive linear structural equation based novel causal discovery framework that identifies causal relationships among functional objects in the presence of cycles and additional measurement/sampling noises. Our major contribution is four-fold.

1. We consider a causal embedding of the functional nodes into a lower-dimensional space for dimension reduction that adapts to causal relationships.
2. We prove that the causal graph of the proposed model is uniquely identifiable under standard causal assumptions.
3. We capture within-function dependencies using a data-driven selection of orthonormal basis that is both interpretable and computationally efficient.
4. To perform inference and uncertainty quantification from finite-sample data, we adopt a fully Bayesian hierarchical formulation with carefully selected prior distributions. Posterior inference is performed using Markov chain Monte Carlo (MCMC). We demonstrate the effectiveness of the proposed method in identifying causal structure and key parameters through simulation studies and apply the framework to the analysis of brain EEG data, illustrating its real-world applicability. Code will be made available on the project's website on Github.

## 2 Model definition and causal identifiability

### Notations

Let \([p]=\{1,\ldots,p\}\) for any positive integer \(p\). A causal directed cyclic graph (DCG) is a graph \(\mathcal{G}=(\mathcal{V},\mathcal{E})\), which consists of a set of vertices or nodes \(\mathcal{V}=[p]\) representing a set of random variablesand a set of directed edges \(\mathcal{E}=\{\ell\to j|j,\ell\in\mathcal{V}\}\) representing the direct causal relationships among the random variables. In a DCG, we do not assume the graph to be acyclic. A causal DCG model is an ordered pair \((\mathcal{G},\mathbb{P})\) where \(\mathbb{P}\) is a joint probability distribution over \(\mathcal{V}\) (more rigorously, the random variables that \(\mathcal{V}\) represents) that satisfies conditional independence relationships encoded by the causal DCG \(\mathcal{G}\). A simple directed cycle is a sequence of distinct vertices \(\{v_{1},\ldots,v_{k}\}\) such that the induced subgraph by these vertices is \(v_{1}\to\cdots\to v_{k}\to v_{1}\). For a vertex \(j\in\mathcal{V}\), we use \(\mathrm{pa}(j)\) to denote the set of parents (direct causes).

### Model framework

Consider a multivariate stochastic process \(\bm{Y}=(Y_{1},\ldots,Y_{p})^{\top}\) where each \(Y_{j}\) is defined on a compact domain \(\mathcal{T}_{j}\subset\mathbb{R}\). Without loss of generality, we assume \(\mathcal{T}_{1}=\cdots=\mathcal{T}_{p}=[0,1]\). Suppose \(Y_{j}\in\mathcal{H}_{j}\) where \(\mathcal{H}_{j}\) is a Hilbert space of functions defined on \(\mathcal{T}_{j}\). We let \(\langle\cdot,\cdot\rangle\) denote the inner product of \(\mathcal{H}_{j}\). We propose a causal model that captures the relationships among \(Y_{1},\ldots,Y_{p}\).

Our proposed model considers an operator-based non-recursive linear structural equation model on the random functions \(\bm{Y}\) as

\[Y_{j}(\cdot)=\sum_{\ell\in\mathrm{pa}(j)}(\mathcal{B}_{j\ell}Y_{\ell})(\cdot)+ f_{j}(\cdot),\quad\forall j\in[p],\] (1)

where \(\mathcal{B}_{j\ell}\) is a linear operator that maps \(\mathcal{H}_{\ell}\) to \(\mathcal{H}_{j}\), and \(f_{j}\in\mathcal{H}_{j}\) is an exogenous stochastic process. Clearly, for any \(j,\ell\in\mathcal{V}\) such that the edge \(\ell\to j\in\mathcal{E}\), \(\mathcal{B}_{j\ell}\) is not a null operator. Now by stacking the \(p\) equations in (1), we obtain

\[\bm{Y}=\mathfrak{B}\bm{Y}+\bm{f},\] (2)

where \(\mathfrak{B}=(\mathcal{B}_{j\ell})_{j,\ell=1}^{p}\) is a matrix of operators and \(\bm{f}=(f_{1},\ldots,f_{p})^{\top}\) is a \(p\)-variate stochastic process. In DAGs, the causal effect matrix can be arranged into a lower block triangular structure given a topological/causal ordering. But since our model allows for cycles, we have no such restriction on the structure of the operator matrix \(\mathfrak{B}\) except that \(\mathcal{B}_{jj},\forall j\in[p]\), is null, i.e., no self-loops.

Model (1) is infinite-dimensional and hence challenging to estimate and interpret. To alleviate such difficulties, we consider a low-dimensional causal embedding structure. Specifically, we assume that the causal relationships are preserved in an unknown low-dimensional subspace \(\mathcal{D}_{j}\) of \(\mathcal{H}_{j}\). Denote the dimension of \(\mathcal{D}_{j}\) by \(K_{j}\). Let \(\mathcal{P}_{j}\) and \(\mathcal{Q}_{j}\) be the projection onto \(\mathcal{D}_{j}\) and its orthogonal complement in \(\mathcal{H}_{j}\) respectively. We assume \(\mathcal{B}_{j\ell}=\mathcal{P}_{j}\mathcal{B}_{j\ell}\mathcal{P}_{\ell}\), which implies that causal effects can be fully described within the low-dimensional subspaces \(\{\mathcal{D}_{j}\}_{j=1}^{p}\). As such, (1) can be split into

\[\mathcal{P}_{j}Y_{j} =\sum_{\ell\in\mathrm{pa}(j)}\mathcal{B}_{j\ell}(\mathcal{P}_{ \ell}Y_{\ell})+\mathcal{P}_{j}f_{j},\] (3) \[\mathcal{Q}_{j}Y_{j} =\mathcal{Q}_{j}f_{j}.\]

We assume that \(\mathcal{P}_{j}f_{j}\) and \(\mathcal{Q}_{j}f_{j}\) are independent of each other. Now, by defining \(\alpha_{j}=\mathcal{P}_{j}Y_{j}\) and \(\epsilon_{j}=\mathcal{P}_{j}f_{j},\forall j\in[p]\), (3) can be compactly written as

\[\bm{\alpha}=\mathcal{B}\bm{\alpha}+\bm{\epsilon},\] (4)

where \(\bm{\alpha}=(\alpha_{1},\ldots,\alpha_{p})^{\top}\) and \(\bm{\epsilon}=(\epsilon_{1},\ldots,\epsilon_{p})^{\top}\) with \(\alpha_{j},\epsilon_{j}\in\mathcal{D}_{j},\forall j\in[p]\).

In practice, the random functions in \(\bm{Y}\) can only be observed over a finite number of (input) locations, possibly with measurement errors. More specifically, for each random function \(Y_{j}\), we observe \(\{(t_{ju},X_{ju})\}_{u=1}^{m_{j}}\), where \(X_{ju}\in\mathbb{R}\) is the measurement of \(Y_{j}\) at location \(t_{ju}\in\mathcal{T}_{j}\) and \(m_{j}\) is the number of measurements obtained from \(Y_{j}\). Defining \(\beta_{j}=\mathcal{Q}_{j}Y_{j}\), we consider the following measurement model:

\[\begin{array}{l}X_{ju}=Y_{j}(t_{ju})+e_{ju}\\ =\alpha_{j}(t_{ju})+\beta_{j}(t_{ju})+e_{ju},\quad\forall u\in[m_{j}],j\in[p], \end{array}\] (5)

with independent noises \(e_{ju}\sim N(0,\sigma_{j}),\forall u\in[m_{j}]\).

More compactly, (5) can be written as

\[\bm{X}=\bm{\alpha}(\bm{t})+\bm{\beta}(\bm{t})+\bm{e},\] (6)where \(\bm{X}=(\bm{X}_{1}^{\top},\ldots,\bm{X}_{p}^{\top})^{\top}\), \(\bm{\alpha}(\bm{t})=(\bm{\alpha}_{1}(\bm{t}_{1})^{\top},\ldots,\bm{\alpha}_{p}( \bm{t}_{p})^{\top})^{\top}\), \(\bm{\beta}(\bm{t})=(\bm{\beta}_{1}(\bm{t}_{1})^{\top},\ldots,\bm{\beta}_{p}( \bm{t}_{p})^{\top})^{\top}\) and \(\bm{e}=(\bm{e}_{1}^{\top},\ldots,\bm{e}_{p}^{\top})^{\top}\) with \(\bm{X}_{j}=(X_{j1},\ldots,X_{jm_{j}})^{\top},\bm{\alpha}_{j}(\bm{t}_{j})=( \alpha_{j}(t_{j1}),\ldots,\alpha_{j}(t_{jm_{j}}))^{\top},\bm{\beta}_{j}(\bm{t} _{j})=(\beta_{j}(t_{j1}),\ldots,\beta_{j}(t_{jm_{j}}))^{\top}\) and \(\bm{e}_{j}=(e_{j1},\ldots,e_{jm_{j}})^{\top}\).

We call our proposed model, **FENCE**, which stands for '**F**unctional **E**mbedded **N**odes for **C**yclic causal Exploration', reflecting its purpose.

### Causal identifiability

In this section, we shall show that the graph structure of the proposed FENCE model is identifiable for functional data measured discretely with random noises under several causal assumptions. We start by defining causal identifiability and state our assumptions.

**Definition 2.1**.: (_Causal Identifiability_) Suppose \(\bm{Y}\) is a \(p\)-variate random function and \(\bm{X}\) is the observed noisy version of \(\bm{Y}\) given by (6). Assume \(\bm{X}\) follows FENCE model \(\mathcal{S}=(\mathcal{G},\mathbb{P})\) where \(\mathcal{G}\) is the underlying graph and \(\mathbb{P}\) is the joint distribution of \(\bm{X}\) over \(\mathcal{G}\). We say that \(\mathcal{S}\) is causally identifiable from \(\bm{X}\) if there does not exist any other \(\mathcal{S}^{*}=(\mathcal{G}^{*},\mathbb{P}^{*})\) with \(\mathcal{G}^{*}\neq\mathcal{G}\) such that the joint distribution \(\mathbb{P}^{*}\) on \(\bm{X}\) induced by \(\mathcal{G}^{*}\) is equivalent to \(\mathbb{P}\) induced by \(\mathcal{G}\).

In other words, for a causal graph to be identifiable, there must not exist any other graph such that the joint distributions induced by the two different graphs are equivalent. Next, we list and discuss a few assumptions to establish the causal identifiability of the proposed model.

**Assumption 1**.: _(Causal Sufficiency) The model \(\mathcal{S}=(\mathcal{G},\mathbb{P})\) is causally sufficient, i.e., there are no unmeasured confounders._

Assuming no unmeasured confounders keeps the causal discovery task more manageable especially for cyclic graphs with purely observational data.

**Assumption 2**.: _(Disjoint Cycles) The cycles in \(\mathcal{G}\) are disjoint, i.e., no two cycles in the graph have two nodes that are common to both._

Assuming disjoint cycles induces a natural topological ordering and forms a directed acyclic hypergraph-like structure within the DCG. The same assumption was made in Lacerda et al. (2008).

**Assumption 3**.: _(Stability) For the model \(\mathcal{S}\), the moduli of the eigenvalues of the finite rank operator \(\mathcal{B}\) are less than or equal to \(1\), and none of the real eigenvalues are equal to \(1\)._

According to Fisher, 1970, the SEM in (4) can be viewed as being in a state of equilibrium, where the finite rank operator \(\mathcal{B}\) represents coefficients in a set of dynamical equations that describe a deterministic dynamical system observed over small time intervals as the time lag approaches zero. The eigenvalue conditions are deemed necessary and sufficient for the limiting behavior to hold, as argued by Fisher, 1970. Such an assumption is widely adopted in e.g., econometrics, and Lacerda et al., 2008 made this assumption as well.

**Assumption 4**.: _(Non-Gaussianity) The exogenous variables have independent mixture of Gaussian distributions. i.e., \(\epsilon_{jk}\stackrel{{\text{ind}}}{{\sim}}\sum_{m=1}^{M_{jk}} \pi_{jkm}\text{N}(\mu_{jkm},\tau_{jkm})\) with \(M_{jk}\geq 2\)._

The assumption of non-Gaussianity on the exogenous variables has been proven useful in causal discovery as it induces model identifiability in the linear SEM framework (Aapo and Petteri, 1999, Shimizu et al., 2006, Lacerda et al., 2008, Spirtes and Zhang, 2016). Mixture of Gaussian can approximate any continuous distribution arbitrarily well given a sufficiently large number of mixture components (Titterington et al., 1985, McLachlan and Peel, 2000, Rossi, 2014). It is also easy to sample, which facilitates our posterior inference.

**Assumption 5**.: _(Non-causal dependency) We assume \(\bm{\beta}(\bm{t})=\bm{C}(\bm{t})\bm{\gamma}\), where \(\bm{C}(\bm{t})=diag(\bm{C}_{11}(\bm{t}_{1}),\ldots,\bm{C}_{pp}(\bm{t}_{p}))\) and \(\bm{\gamma}\) represent another exogenous component of the model. Here \(\bm{C}_{jj}(\bm{t}_{j})\) is a mixing matrix that mixes the independent entires in \(\bm{\gamma}\) to generate temporal dependence within the \(j\)-th block. We assume \(\gamma_{jk}\stackrel{{\text{ind}}}{{\sim}}\sum_{m=1}^{M_{jk}}\pi^{ \prime}_{jkm}\text{N}(\mu^{\prime}_{jkm},\tau^{\prime}_{jkm})\) with \(M_{jk}\geq 1\)._

Since the model assumes that all causal information in \(\bm{Y}\) is preserved in the lower-dimensional space \(\mathcal{D}_{j}\) and not in its orthogonal complement, it is apparent that while each \(\bm{\beta}_{j}(\bm{t}_{j})\) within a block can have temporal dependence, it is independent of \(\bm{\beta}_{l}(\bm{t}_{\ell})\) when \(j\neq\ell\) and \(j,\ell\in[p]\).

For some basis \(\{\phi_{jk}\}_{k=1}^{K_{j}}\) that spans the low-dimensional causal embedded space \(\mathcal{D}_{j}\), \(\alpha_{j}\) in (5) can be further expanded by, \(\alpha_{j}(t_{ju})=\sum_{k=1}^{K_{j}}\tilde{\alpha}_{jk}\phi_{jk}(t_{ju})\). Therefore (6) can be written more compactly as

\[\bm{X}=\bm{\Phi}(\bm{t})\tilde{\bm{\alpha}}+\bm{\beta}(\bm{t})+\bm{e},\] (7)

where \(\bm{\Phi}(\bm{t})=\operatorname{diag}(\bm{\Phi}_{1}(\bm{t}_{1}),\dots,\bm{ \Phi}_{p}(\bm{t}_{p}))\) with \(\bm{\Phi}_{j}(\bm{t}_{j})=(\phi_{jv}(t_{ju}))_{u=1,v=1}^{m_{j},K_{j}}\).

**Assumption 6**.: _(Sufficient sampling locations) The basis matrix \(\bm{\Phi}(\bm{t})\) of size \(\sum_{j=1}^{p}m_{j}\times\sum_{j=1}^{p}K_{j}\) has a full column rank._

This assumption implies enough sampling locations, over which each random function \(Y_{j}\) is observed, to capture all the causal information that \(Y_{j}\) contains.

Given these six assumptions, our main theorem establishes the causal identifiability of the proposed model.

**Theorem 2.1**.: _Under Assumptions 1-6, \(\mathcal{S}=(\mathcal{G},\mathbb{P})\) is causally identifiable._

The proof essentially involves two steps as shown in Figure 1. On the left-hand side (LHS) of the diagram, we depict the hypergraph-like structure that emerges when assuming the existence of disjoint cycles (Assumption 2), whereas, on the right-hand side (RHS), we offer a magnified view of the hypernodes (nodes containing simple directed cycle). Our approach to proving causal identifiability progresses from the LHS to the RHS. That is, we first prove the identifiability of the hypergraph-like structure depicted on the LHS of Figure 1, and then we proceed to establish the identifiability of each simple directed cycle within every hypernode in the hypergraph. The detailed exposition of the proof can be found in Section A of the Supplementary Materials.

## 3 Bayesian model formulation

In this section, we will describe the inference procedure of the proposed model. A straightforward approach would be a two-step procedure where the first step performs functional principal component analysis on each function marginally to reduce the dimension, and then the second step learns causal structure based on the principal components. However, this simple approach has several disadvantages. First, the estimated functional principal components that explain the most variation of each individual function marginally may not optimally capture the cause-effect dependence relationships among different functions. Second, this procedure is unreliable since estimation uncertainty fails to propagate correctly from the first step to the second step. As such, we propose a fully Bayesian approach, which reduces the dimension of functional data adaptively for causal structure learning.

### Model parameters

Let \(\bm{E}=(E_{j\ell})_{j,\ell=1}^{p}\) denote the adjacency matrix where \(E_{j\ell}=1\) indicates the existence of a directed edge from node \(\ell\) to node \(j\), and 0 otherwise. Let \(\{\phi_{k}\}_{k=1}^{S}\) be a set of \(S\) common unknown basis

Figure 1: Two important components of causal identifiability proof: (I) identifiability of directed acyclic hypergraph induced by disjoint cycles, and (II) identifiability of each disjoint cycle.

functions that approximate each random function \(Y_{j}\) i.e., \(Y_{j}=\sum_{k=1}^{S}\tilde{\alpha}_{jk}\phi_{k}\) where \(\{\tilde{\alpha}_{jk}\}_{k=1}^{S}\) denote the set of basis coefficients. Note that \(\{\phi_{k}\}\) is not the basis for the lower-dimensional causal embedded subspace \(\mathcal{D}_{j}\). However, we assume that the first \(K_{j}\) of them actually spans \(\mathcal{D}_{j}\) and our goal is to hunt for them through a properly designed inference procedure. Moreover, according to our assumption, we build our SEM on the first \(K_{j}\) of the basis coefficients \(\tilde{\boldsymbol{\alpha}}_{j}=(\tilde{\alpha}_{j1},\cdots,\tilde{\alpha}_{ jK_{j}})^{\top}\). Defining \(\tilde{\boldsymbol{\alpha}}_{j}=(\tilde{\alpha}_{j,K_{j}+1},\cdots,\tilde{ \alpha}_{jS})^{\top}\) with \(\tilde{\boldsymbol{\alpha}}_{j}=\boldsymbol{\gamma}_{j}\), jointly they can be written as

\[\tilde{\boldsymbol{\alpha}}=\tilde{\boldsymbol{B}}\tilde{\boldsymbol{\alpha }}+\tilde{\boldsymbol{\epsilon}},\] (8)

where \(\tilde{\boldsymbol{\alpha}}=(\tilde{\boldsymbol{\alpha}}_{1}^{\top},\cdots, \tilde{\boldsymbol{\alpha}}_{p}^{\top},\tilde{\boldsymbol{\alpha}}_{1}^{\top },\ldots,\tilde{\boldsymbol{\alpha}}_{p}^{\top})^{\top}\), \(\tilde{\boldsymbol{\epsilon}}=(\tilde{\boldsymbol{\epsilon}}_{1}^{\top}, \cdots,\tilde{\boldsymbol{\epsilon}}_{p}^{\top},\boldsymbol{\gamma}_{1}^{\top },\ldots,\boldsymbol{\gamma}_{p}^{\top})^{\top}\) with \(\tilde{\boldsymbol{\epsilon}}_{j}=(\tilde{\epsilon}_{j1},\cdots,\tilde{ \epsilon}_{jK_{j}})^{\top}\) and \(\boldsymbol{\gamma}_{j}=(\gamma_{j,K_{j}+1},\ldots,\gamma_{jS})^{\top}\). Here \(\tilde{\boldsymbol{B}}=\begin{pmatrix}\boldsymbol{B}&\boldsymbol{0}\\ \boldsymbol{0}&\boldsymbol{0}\end{pmatrix}\) where \(\boldsymbol{B}=((\boldsymbol{B}_{j\ell}(a,b))_{a=1,b=1}^{K_{j},K_{\ell}})_{j, \ell=1}^{p}\) with \(\boldsymbol{B}_{jj}=\boldsymbol{0}\) since we assume the absence of self loops. To carry out inference, we assume \(\tilde{\epsilon}_{jk},\gamma_{jk}\stackrel{{ ind}}{{\sim}}\sum_{m=1}^{M_{jk}}\pi_{jkm}N(\mu_{ jkm},\tau_{jkm})\).

### Adaptive basis expansion

As the \(\phi_{k}\)'s are specifically useful for restricting the original function space for each \(Y_{j}\) to a lower-dimensional causally embedded smooth space of dimension \(K_{j}\), we make the basis \(\{\phi_{k}\}\) adaptive for causal structure learning by further expanding them with known spline basis functions (Kowal et al., 2017), \(\phi_{k}(\cdot)=\sum_{r=1}^{R}A_{kr}b_{r}(\cdot)\), where \(\boldsymbol{b}=(b_{1},\ldots,b_{R})^{\top}\) is the set of fixed cubic B-spline basis functions with equally spaced knots and \(\boldsymbol{A}_{k}=(A_{k1},\ldots,A_{kR})^{\top}\) are the corresponding spline coefficients. Since we do not fix \(\boldsymbol{A}_{k}\)'s _a priori_, the basis functions \(\phi_{k}\)'s can be learned from data _a posteriori_ and hence are adaptive to both data and causal structure (i.e., the basis functions, the functional data, and the causal graph are dependent in their joint distribution).

### Prior specifications

Prior on spline coefficients.The prior on \(A_{k}\) is chosen to serve multiple purposes. (i) It sorts the basis functions by decreasing smoothness and therefore helps to identify the spanning set of size \(K_{j}\) for the underlying smooth causally embedded space \(\mathcal{D}_{j}\). (ii) Although not a strict requirement for modelling purpose, it forces \(\phi_{k}\)'s to be orthonormal, i.e. \(\int\phi_{k}(\omega)\phi_{k^{\prime}}(\omega)\,d\omega=I(k=k^{\prime})\). As such, the orthogonality constraints help eliminate any information overlap between the basis functions, which keeps the total number of necessary basis functions that actually contribute to the causal structure learning to a minimum. (iii) It regularizes the roughness of \(\phi_{k}\)'s to prevent overfitting.

For (iii), more specifically, we restrict the roughness of the basis functions \(\phi_{k}(\cdot)\) by assigning a prior that penalizes its second derivatives (Gu, 1992; Wahba, 1978; Berry et al., 2002):

\[\boldsymbol{A}_{k}\sim N(\boldsymbol{0},\lambda_{k}^{-1}\boldsymbol{\Omega}^{ -}),\]

where \(\boldsymbol{\Omega}^{-}\) is the pseudoinverse of \(\boldsymbol{\Omega}=\int\boldsymbol{b}^{\,\prime\prime}(t)[\boldsymbol{b}^{ \,\prime\prime}(t)]^{\top}\,dt\). Let \(\boldsymbol{\Omega}=\boldsymbol{U}\boldsymbol{D}\boldsymbol{U}^{\top}\) be the singular value decomposition of \(\boldsymbol{\Omega}\). Following Wand and Ormerod, 2010, to facilitate computation, we reparameterize \(\phi_{k}(\cdot)=\sum\limits_{r=1}^{R}\tilde{A}_{kr}\tilde{b}_{k}(\cdot)\) with \(\tilde{\boldsymbol{b}}(\cdot)=(1,t,\boldsymbol{b}^{T}(\cdot)\tilde{\boldsymbol{U }}\tilde{\boldsymbol{D}}^{-\frac{1}{2}})^{\top}\) where \(\tilde{\boldsymbol{D}}\) is the \((R-2)\times(R-2)\) submatrix of \(\boldsymbol{D}\) corresponding to non-zero singular values (note that the rank of \(\boldsymbol{\Omega}\) is \(R-2\) by definition) and \(\tilde{\boldsymbol{U}}\) is the corresponding \(R\times(R-2)\) submatrix of \(\boldsymbol{U}\). This induces a prior on \(\tilde{\boldsymbol{A}}_{k}\) given by

\[\tilde{\boldsymbol{A}}_{k}\sim N(\boldsymbol{0},\boldsymbol{S}_{k})\text{ with }\boldsymbol{S}_{k}=\text{diag}(\infty,\infty,\lambda_{k}^{-1},\ldots,\lambda_{k}^{-1}).\]

In other words, the intercept and the linear term are unpenalized but the non-linear terms are penalized, the degree of which is controlled by \(\lambda_{k}\). In practice, we set the first two diagonal elements of \(\boldsymbol{S}_{k}\) as \(10^{8}\). We constrain the regularization parameters \(\lambda_{1}>\cdots>\lambda_{S}>0\) by putting a uniform prior:

\[\lambda_{k} \sim\text{Uniform}(L_{k},U_{k}),\;\forall\;k\in[S],\] \[U_{1} =10^{8},L_{k}=\lambda_{k+1}\;\forall\;k\in[S-1],\] \[U_{k} =\lambda_{k-1}\;\forall\;k\in\{2,\ldots,S\},L_{S}=10^{-8},\]

which implies that the smoothness of \(\phi_{k}(\cdot)\) decreases as \(k\) gets larger.

Priors on the adjacency matrix.We propose to use an independent uniform-Bernoulli prior on each entry \(E_{j\ell}\) of \(\bm{E}\), i.e., \(E_{j\ell}|_{\rho}\overset{\text{ind}}{\sim}\text{Bernoulli}(\rho)\) and \(\rho\sim\text{Uniform}(0,1)\). The marginal distribution of \(\bm{E}\) with \(\rho\) integrated out is given by

\[p(\bm{E})=\int p(\bm{E}|\rho)p(\rho)\,d\rho=\text{Beta}\left(\sum_{j\neq\ell}E _{j\ell}+1,\sum_{j\neq\ell}(1-E_{j\ell})+1\right).\]

Now, for example, if \(\bm{E}_{0}\) denotes the null adjacency matrix and \(\bm{E}_{1}\) denotes the adjacency matrix with only one edge, then we can see that \(p(\bm{E}_{0})/p(\bm{E}_{1})=p^{2}-p\). Therefore, an empty graph is favored over a graph with one edge by a factor of \(p^{2}-p\), and, importantly, this penalty increases with \(p\). Thus, the uniform-Bernoulli prior prevents false discoveries and leads to a sparse network by increasing the penalty against additional edges as the dimension \(p\) grows.

Prior on the causal effect matrix.Now given \(\bm{E}\), we assume an independent spike and slab prior on the entries of \(\bm{B}=(\bm{B}_{j\ell})_{j,\ell=1}^{p}\):

\[\bm{B}_{j\ell}|E_{j\ell}\sim(1-E_{j\ell})MVN(\bm{B}_{j\ell};\bm{0},s\gamma\bm {I}_{K_{j}},\bm{I}_{K_{\ell}})+E_{j\ell}MVN(\bm{B}_{j\ell};\bm{0},\gamma\bm{I}_ {K_{j}},\bm{I}_{K_{\ell}}),\]

where \(MVN(\bm{B}_{j\ell};\bm{0},\gamma\bm{I}_{K_{j}},\bm{I}_{K_{\ell}})\) is a matrix-variate normal distribution with row and column covariance matrices as \(\gamma\bm{I}_{K_{j}}\) and \(\bm{I}_{K_{\ell}}\), respectively. We assume a conjugate inverse-gamma prior on the causal effect size, \(\gamma\sim\text{InverseGamma}(a_{\gamma},b_{\gamma})\). We choose \(a_{\gamma}=b_{\gamma}=1\). We fix \(s=0.02\) so that when \(E_{j\ell}=0\), \(\bm{B}_{j\ell}\) is negligibly small.

Priors on the parameters of the Gaussian mixture distribution.We choose conjugate priors for the parameters of the Gaussian mixture distribution:

\[(\pi_{jk1},\ldots,\pi_{jkM_{jk}})\sim\text{Dirichlet}(\beta, \ldots,\beta),\quad\forall\ j\in[p],k\in[S]\] \[\mu_{jkm}\sim N(a_{\mu},b_{\mu}),\ \tau_{jkm}\sim\text{InverseGamma}(a_{ \tau},b_{\tau}),\ \forall\ j\in[p],k\in[S],m\in[M_{jk}]\]

We have fixed values for the hyperparameters, \(\beta=1,a_{\mu}=0,b_{\mu}=100,a_{\tau}=b_{\tau}=1\).

Prior on the noise variances.We assume a conjugate prior for \(\sigma_{j}\sim\text{InverseGamma}(a_{\sigma},b_{\sigma}),\)\(\forall\ j\in[p]\). We choose \(a_{\sigma}=b_{\sigma}=0.01\).

We simulate posterior samples through Markov chain Monte Carlo (MCMC). Details are given in Section B of the Supplementary Materials. Sensitivity analyses will be conducted to test the hyperparameters including \((a_{\gamma},b_{\gamma}),(a_{\tau},b_{\tau}),(a_{\sigma},b_{\sigma}),s,R,S,M\) and \(\beta\).

## 4 Simulation study

Data generationThe data were simulated according to various combinations of sample size (\(n\)), number of nodes (\(p\)), and grid size (\(m_{j}=d\)\(\forall j\in[p]\)) where \(n\in\{75,150,300\}\), \(p\in\{20,40,60\}\), and \(d\in\{125,250\}\). The grid evenly spans the unit interval \([0,1]\); the results with unevenly spaced grids are presented in Section C of the Supplementary Materials. The true causal graph \(\tilde{\mathcal{G}}\) was generated randomly with edge formation probability \(2/p\). Given \(\mathcal{G}\), each non-zero block \(\bm{B}_{j\ell}\) of the causal effect matrix was generated from the standard matrix-variate normal distribution. We set the true number of basis functions to be \(K=4\). In order to generate \(K=4\) orthonormal basis functions, we first simulated unnormalized basis functions by expanding them further with 6 cubic B-spline basis functions where the coefficients were drawn from the standard normal distribution and then empirically orthonormalized them. The basis coefficients \(\tilde{\bm{\alpha}}\) were generated following (8) with the exogenous variables \(\tilde{\bm{\epsilon}}_{j}\) drawn independently from Laplace distribution with location parameter \(\mu=0\) and scale parameter \(b=0.2\). We have also considered other non-Gaussian distributions for the exogenous variables; the corresponding results are provided in Section C of the Supplementary Materials. Finally, noisy observations were simulated following (6) with the signal-to-noise ratio, i.e., the mean value of \(|Y_{j}^{(i)}(t)|/\sigma_{j}\) across all \(i\) and \(t\), set to 5. Here, superscript \((i)\) denotes the \(i\)th sample, where \(i\in[n]\).

For the implementation of the proposed FENCE, we fixed the number of mixture components to be 10 and ran MCMC for 5,000 iterations (discarding the first 2,000 iterations as burn-in and retaining every 5th iteration after burn-in). The causal graph \(G\) was then estimated by using the median probability model [1], i.e., by thresholding the posterior probability of inclusion at 0.5.

[MISSING_PAGE_FAIL:8]

Real data application

Brain EEG data.We demonstrate the proposed FENCE model on a brain EEG dataset from an alcoholism study (Zhang et al., 1995). This dataset was earlier used to demonstrate functional undirected graphical models (Zhu et al., 2016; Qiao et al., 2019) and functional Bayesian network (Zhou et al., 2022b). Data were initially obtained from 64 electrodes placed on subjects' scalps, which captured EEG signals at 256 Hz (3.9 ms epoch) during a one-second period. The study consists of 122 subjects, out of which 77 are in the alcoholic group and 45 are in the control group. Each subject completed 120 trials. During each trial, the subject was exposed to either a single stimulus (a single picture) or two stimuli (a pair of pictures) shown on a computer monitor. We particularly focus on the EEG signals filtered at \(\alpha\) frequency bands between 8 and 12.5Hz using the eegfit function of the eeglab toolbox of Matlab as \(\alpha\) band signals are associated with inhibitory control (Knyazev, 2007). Given that the EEG measurements were recorded from each subject over multiple trials, these measurements are not independent of each other due to the time dependency of the trials. Moreover, since the measurements were obtained under various stimuli, the signals may have been affected by different stimulus effects. To mitigate these issues, we calculated the average of the band-filtered EEG signals for each subject across all trials under a single stimulus, resulting in a single event-related potential curve per electrode per subject. By doing so, we eliminated the potential dependence between the measurements and the influence of different stimulus types. We performed separate analyses of the two groups to identify both the similarities and dissimilarities in their brain effective connectivity.

We conducted a Shapiro-Wilk normality test on the observed functions for each of the \(p=64\) scalp positions at each of the \(m_{j}=256\)\(\forall j\in[p]\) time points to evaluate their Gaussianity. The results showed that for numerous combinations of scalp position and time point, the null hypothesis (which assumes that the observations are marginally Gaussian) was rejected. Thus, we conclude that the non-Gaussianity of the proposed model is appropriate. Next, for posterior inference, we ran MCMC for 20,000 iterations, discarded the first half as burn-in, and retained every 10th iteration after burn-in. The estimated causal graph by thresholding the posterior inclusion probability to \(0.9\) is given below in Figure 2.

Results.There are some interesting findings. First, for both groups (alcoholic and control), brain regions that are located in adjacent positions tend to be more connected than the brain regions that are far apart. Second, dense connectivity is observed in the frontal region of the brain in both groups, with multiple cycles being formed. Third, compared to the control group, the alcoholic group has more connectivity across the left parietal and occipital lobes. Fourth, the same cycle of Iz, Cz, and RPA is observed in both groups.

Figure 2: Estimated causal brain connectivity from EEG records by FENCE with posterior probability of inclusion \(\geq 0.9\), separately for the alcoholic (left) and control (right) group. The bi-directed edges are just directed cycles, i.e., \(i\leftrightarrow j\) means \(i\to j\) and \(i\gets j\).

Validity.We now discuss the validity of our real data results. In Hayden et al., 2007, it was observed that alcohol-dependent subjects exhibited frontal asymmetry, distinguishing them from the control group. Our own investigation aligns well with these results, as we have identified denser connectivity across various brain regions in the middle and left areas of the frontal lobe among alcoholic subjects, when compared to controls. Furthermore, Winterer et al., 2003 documented coherent differences between alcoholics and controls in the posterior hemispheres, specifically in the temporal, parietal, and occipital lobes. In accordance with their findings, our study provides additional support for this claim, as we have observed heightened activity with several cycles formed in those same regions within the alcoholic group when compared to the control group.

## 6 Discussion

We briefly highlight here several potential avenues for future development of our current work. First, an intriguing and important direction would be to explore the relaxation of the causal sufficiency assumption in the model identifiability. Second, our current model is based on a linear non-Gaussian assumption over the exogenous variables, but a nonlinear model could be considered as an alternative. Lastly, an alternative approach to determining the effective number of basis functions that span the lower-dimensional causal embedded space would be to utilize the ordered shrinkage priors (Bhattacharya and Dunson, 2011, Legramanti et al., 2020) in order to adaptively eliminate redundant components, resulting in a more flexible methodology.

## Acknowledgements

Ni's research was partially supported by NSF DMS-2112943 and NIH 1R01GM148974-01.

## References

* Aapo and Petteri (1999) Hyvarinen Aapo and Pajunen Petteri. Nonlinear independent component analysis: Existence and uniqueness results. _Neural Networks_, 12:429-439, 1999. URL https://doi.org/10.1016/S0893-6080(98)00140-3.
* 897, 2004. doi: 10.1214/00905360400000238. URL https://doi.org/10.1214/00905360400000238.
* Berry et al. (2002) Scott M Berry, Raymond J Carroll, and David Ruppert. Bayesian smoothing and regression splines for measurement error problems. _Journal of the American Statistical Association_, 97(457):160-169, 2002. doi: 10.1198/016214502753479301. URL https://doi.org/10.1198/016214502753479301.
* Bhattacharya and Dunson (2011) A. Bhattacharya and D. B. Dunson. Sparse Bayesian infinite factor models. _Biometrika_, 98(2):291-306, 06 2011. ISSN 0006-3444. doi: 10.1093/biomet/asr013. URL https://doi.org/10.1093/biomet/asr013.
* Brandman and Meyer (2008) Onn Brandman and Thomas Meyer. Feedback loops shape cellular signals in space and time. _Science_, 322(5900):390-395, 2008.
* Chiou and Muller (2016) Jeng-Min Chiou and Hans-Georg Muller. A pairwise interaction model for multivariate functional and longitudinal data. _Biometrika_, 103(2):377-396, 2016.
* Fisher (1970) Franklin M. Fisher. A correspondence principle for simultaneous equation models. _Econometrica_, 38(1):73-92, 1970. ISSN 00129682, 14680262. URL http://www.jstor.org/stable/1909242.
* Friston (2011) Karl J. Friston. Functional and effective connectivity: A review. _Brain Connectivity_, 1(1):13-36, 2011. doi: 10.1089/brain.2011.0008. URL https://doi.org/10.1089/brain.2011.0008. PMID: 22432952.
* Gu (1992) Chong Gu. Penalized likelihood regression: A bayesian analysis. _Statistica Sinica_, 2:255-264, 1992.
* Gu et al. (2012)Elizabeth Hayden, Ryan Wiegand, Eric Meyer, Lance Bauer, Sean O'Connor, John Nurnberger, David Chorlian, Bernice Porjesz, and Henri Begleiter. Patterns of regional brain activity in alcohol-dependent subjects. _Alcoholism, clinical and experimental research_, 30:1986-91, 01 2007. doi: 10.1111/j.1530-0277.2006.00244.x.
* Huang et al. (2019) Biwei Huang, Kun Zhang, Jiji Zhang, Joseph D. Ramsey, Ruben Sanchez-Romero, Clark Glymour, and Bernhard Scholkopf. Causal discovery from heterogeneous/nonstationary data. _CoRR_, abs/1903.01672, 2019. URL http://arxiv.org/abs/1903.01672.
* Hyttinen et al. (2012) Antti Hyttinen, Frederick Eberhardt, and Patrik O. Hoyer. Causal discovery of linear cyclic models from multiple experimental data sets with overlapping variables. In Nando de Freitas and Kevin P. Murphy, editors, _Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence, Catalina Island, CA, USA, August 14-18, 2012_, pages 387-396. AUAI Press, 2012. URL https://dslpitt.org/uai/displayArticleDetails.jsp?mmnu=1&smnu=2&article_id=2301&proceeding_id=28.
* Kalisch et al. (2018) Markus Kalisch, A. Hauser, Marloes H. Maathuis, and Martin Machler. An overview of the pcalg package for r, 2018.
* Knyazev (2007) Gennady G Knyazev. Motivation, emotion, and their inhibitory control mirrored in brain oscillations. _Neuroscience & Biobehavioral Reviews_, 31(3):377-395, 2007.
* Korte-Stappf et al. (2022) Moritz Korte-Stappf, Drew Yarger, Stilian Stoev, and Tailen Hsing. A multivariate functional-data mixture model for spatio-temporal data: inference and cokriging, 2022.
* Kowal et al. (2017) Daniel R Kowal, David S Matteson, and David Ruppert. A bayesian multivariate functional dynamic linear model. _Journal of the American Statistical Association_, 112(518):733-744, 2017.
* Lacerda et al. (2008) Gustavo Lacerda, Peter L. Spirtes, Joseph Ramsey, and Patrik O. Hoyer. Discovering cyclic causal models by independent components analysis. In _Conference on Uncertainty in Artificial Intelligence_, 2008.
* Lee and Li (2022) Kuang-Yao Lee and Lexin Li. Functional structural equation model. _Journal of the Royal Statistical Society Series B_, 84(2):600-629, April 2022. doi: 10.1111/rssb.12471. URL https://ideas.repec.org/a/bla/jorssb/v84y2022212p600-629.html.
* Legramanti et al. (2020) Sirio Legramanti, Daniele Durante, and David B Dunson. Bayesian cumulative shrinkage for infinite factorizations. _Biometrika_, 107(3):745-752, 05 2020. ISSN 0006-3444. doi: 10.1093/biomet/asaa008. URL https://doi.org/10.1093/biomet/asaa008.
* Markov et al. (2012) N. T. Markov, M. M. Ercsey-Ravasz, A. R. Ribeiro Gomes, C. Lamy, L. Magrou, J. Vezoli, P. Misery, A. Falchier, R. Quilodran, M. A. Gariel, J. Sallet, R. Gamanut, C. Huissoud, S. Clavagnier, P. Giroud, D. Sappey-Marinier, P. Barone, C. Dehay, Z. Toroczkai, K. Knoblauch, D. C. Van Essen, and H. Kennedy. A Weighted and Directed Interareal Connectivity Matrix for Macaque Cerebral Cortex. _Cerebral Cortex_, 24(1):17-36, 09 2012. ISSN 1047-3211. doi: 10.1093/cercor/bhs270. URL https://doi.org/10.1093/cercor/bhs270.
* McLachlan and Peel (2000) G. J. McLachlan and D. Peel. _Finite mixture models_. Wiley Series in Probability and Statistics, New York, 2000.
* Mooij and Claassen (2020) Joris M. Mooij and Tom Claassen. Constraint-based causal discovery using partial ancestral graphs in the presence of cycles. In Ryan P. Adams and Vibhav Gogate, editors, _Proceedings of the Thirty-Sixth Conference on Uncertainty in Artificial Intelligence, UAI 2020, virtual online, August 3-6, 2020_, volume 124 of _Proceedings of Machine Learning Research_, pages 1159-1168. AUAI Press, 2020. URL http://proceedings.mlr.press/v124/m-mooij20a.html.
* Mooij and Heskes (2013) Joris M. Mooij and Tom Heskes. Cyclic causal discovery from continuous equilibrium data. In Ann E. Nicholson and Padhraic Smyth, editors, _Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence, UAI 2013, Bellevue, WA, USA, August 11-15, 2013_. AUAI Press, 2013. URL https://dslplitt.org/uai/displayArticleDetails.jsp?mmnu=1&smnu=2&article_id=2404&proceeding_id=29.
* Mooij et al. (2013)Joris M. Mooij, Dominik Janzing, Tom Heskes, and Bernhard Scholkopf. On causal discovery with cyclic additive noise models. In John Shawe-Taylor, Richard S. Zemel, Peter L. Bartlett, Fernando C. N. Pereira, and Kilian Q. Weinberger, editors, _Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011. Proceedings of a meeting held 12-14 December 2011, Granada, Spain_, pages 639-647, 2011. URL https://proceedings.neurips.cc/paper/2011/hash/d61e4bbd6393c9111e6526ea173a7c8b-Abstract.html.
* Park et al. (2022) Yeonjoo Park, Bo Li, and Yehua Li. Crop yield prediction using bayesian spatially varying coefficient models with functional predictors. _Journal of the American Statistical Association_, pages 1-14, 2022.
* Qiao et al. (2019) Xinghao Qiao, Shaojun Guo, and Gareth M James. Functional graphical models. _Journal of the American Statistical Association_, 114(525):211-222, 2019.
* Ramsey et al. (2018) Joseph D Ramsey, Kun Zhang, Madelyn Glymour, Ruben Sanchez Romero, Biwei Huang, Imme Ebert-Uphoff, Savini Samarasinghe, Elizabeth A Barnes, and Clark Glymour. Tetrad--a toolbox for causal discovery. In _8th international workshop on climate informatics_, 2018.
* Richardson (1996) Thomas Richardson. A discovery algorithm for directed cyclic graphs. In _Proceedings of the Twelfth International Conference on Uncertainty in Artificial Intelligence_, UAI'96, page 454-461, San Francisco, CA, USA, 1996. Morgan Kaufmann Publishers Inc. ISBN 155860412X.
* Rojas-Carulla et al. (2018) Mateo Rojas-Carulla, Bernhard Scholkopf, Richard Turner, and Jonas Peters. Invariant models for causal transfer learning. _J. Mach. Learn. Res._, 19(1):1309-1342, jan 2018. ISSN 1532-4435.
* Rossi (2014) Peter E. Rossi. _Bayesian Non- and Semi-parametric Methods and Applications_. Princeton University Press, 2014. ISBN 9780691145327. URL http://www.jstor.org/stable/j.ctt5hhrfp.
* Scholkopf et al. (2021) Bernhard Scholkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, and Yoshua Bengio. Toward causal representation learning. _Proceedings of the IEEE_, 109(5):612-634, 2021. doi: 10.1109/JPROC.2021.3058954.
* Shimizu et al. (2006) Shohei Shimizu, Patrik O. Hoyer, Aapo Hyvrinen, and Antti Kerminen. A linear non-gaussian acyclic model for causal discovery. _Journal of Machine Learning Research_, 7(72):2003-2030, 2006. URL http://jmlr.org/papers/v7/shimizu06a.html.
* Spirtes and Glymour (1991) Peter Spirtes and Clark Glymour. An algorithm for fast recovery of sparse causal graphs. _Social Science Computer Review_, 9(1):62-72, 1991. doi: 10.1177/089443939100900106. URL https://doi.org/10.1177/089443939100900106.
* Spirtes and Zhang (2016) Peter Spirtes and Kun Zhang. Causal discovery and inference: concepts and recent methodological advances. _Applied Informatics_, 3, 2016. doi: 10.1186/s40535-016-0018-x. URL https://doi.org/10.1186/s40535-016-0018-x.
* Tang et al. (2023) Zeyu Tang, Jiji Zhang, and Kun Zhang. What-is and how-to for fairness in machine learning: A survey, reflection, and perspective. _ACM Computing Surveys_, 55(13s):1-37, jul 2023. doi: 10.1145/3597199. URL https://doi.org/10.1145%2F3597199.
* Titterington et al. (1985) D.M. Titterington, A.F.M. Smith, and U.E. Makov. _Statistical Analysis of Finite Mixture Distributions_. Wiley, New York, 1985.
* Volkmann et al. (2021) Alexander Volkmann, Almond Stocker, Fabian Scheipl, and Sonja Greven. Multivariate functional additive mixed models, 2021.
* Wahba (1978) Grace Wahba. Improper priors, spline smoothing and the problem of guarding against model errors in regression. _Journal of the Royal Statistical Society: Series B (Methodological)_, 40(3):364-372, 1978. doi: https://doi.org/10.1111/j.2517-6161.1978.tb01050.x. URL https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1978.tb01050.x.
* Wand and Ormerod (2010) M. P. Wand and J. T. Ormerod. Corrigendum: On semiparametric regression with o'sullivan penalised splines. _Australian & New Zealand Journal of Statistics_, 52(2):239-239, 2010. doi: https://doi.org/10.1111/j.1467-842X.2010.00578.x. URL https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-842X.2010.00578.x.
* Wand et al. (2018)Zhi Wei and Hongzhe Li. A hidden spatial-temporal Markov random field model for network-based analysis of time course gene expression data. _The Annals of Applied Statistics_, 2(1):408 - 429, 2008. doi: 10.1214/07--AOAS145. URL https://doi.org/10.1214/07--AOAS145.
* Winterer et al. (2003) G Winterer, M.-A Enoch, K White, Mete Saylan, Richard Coppola, and D Goldman. Eeg phenotype in alcoholism: Increased coherence in the depressive subtype. _Acta psychiatatica Scandinavica_, 108:51-60, 08 2003. doi: 10.1034/j.1600-0447.2003.00060.x.
* Wong et al. (2019) Raymond KW Wong, Yehua Li, and Zhengyuan Zhu. Partially linear functional additive models for multivariate functional data. _Journal of the American Statistical Association_, 114(525):406-418, 2019.
* Yang and Suzuki (2022) Tianle Yang and Joe Suzuki. The functional LiNGAM. In _Proceedings of The 11th International Conference on Probabilistic Graphical Models_, volume 186 of _Proceedings of Machine Learning Research_, pages 25-36. PMLR, 05-07 Oct 2022. URL https://proceedings.mlr.press/v186/yang22a.html.
* Zeng et al. (2023) Yan Zeng, Ruichu Cai, Fuchun Sun, Libo Huang, and Zhifeng Hao. A survey on causal reinforcement learning, 2023.
* Zhang et al. (1995) Xiao Lei Zhang, Henri Begleiter, Bernice Porjesz, Wenyu Wang, and Ann Litke. Event related potentials during object recognition tasks. _Brain Research Bulletin_, 38:531-538, 1995.
* Zhou et al. (2022a) Fangting Zhou, Kejun He, and Yang Ni. Causal discovery with heterogeneous observational data. In James Cussens and Kun Zhang, editors, _Uncertainty in Artificial Intelligence, Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence, UAI 2022, 1-5 August 2022, Eindhoven, The Netherlands_, volume 180 of _Proceedings of Machine Learning Research_, pages 2383-2393. PMLR, 2022a. URL https://proceedings.mlr.press/v180/zhou22a.html.
* Zhou et al. (2022b) Fangting Zhou, Kejun He, Kunbo Wang, Yanxun Xu, and Yang Ni. Functional bayesian networks for discovering causality from multivariate functional data, 2022b.
* Zhou et al. (2022c) Yidong Zhou, Satarupa Bhattacharjee, Cody Carroll, Yaqing Chen, Xiongtao Dai, Jianing Fan, Alvaro Gajardo, Pantelis Z. Hadjipantelis, Kyunghee Han, Hao Ji, Changbo Zhu, Hans-Georg Muller, and Jane-Ling Wang. _fdapace: Functional Data Analysis and Empirical Dynamics_, 2022c. URL https://CRAN.R-project.org/package=fdapace. R package version 0.5.9.
* Zhu et al. (2016) Hongxiao Zhu, Nate Strawn, and David B Dunson. Bayesian graphical models for multivariate functional data. _Journal of Machine Learning Research_, 17(204):1-27, 2016.