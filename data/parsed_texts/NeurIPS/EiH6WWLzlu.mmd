[MISSING_PAGE_FAIL:1]

###### Abstract

We present the ShareGPT4Video series, aiming to facilitate the video understanding of large video-language models (LVLMs) and the video generation of text-to-video models (T2VMs) via dense and precise captions. To achieve this, taking aside the non-scalable costly human annotators, we find using GPT4V to caption video with a naive multi-frame or frame-concatenation input strategy leads to less detailed and sometimes temporal-confused results. We argue the challenge of designing a high-quality video captioning strategy lies in three aspects: **1) Inter-frame precise temporal change understanding. 2) Intra-frame detailed content description. 3) Frame-number scalability for arbitrary-length videos.** To this end, we meticulously designed a differential video captioning strategy, which is stable, scalable, and efficient for generating captions for videos with arbitrary resolution, aspect ratios, and length. Based on it, we construct ShareGPT4Video, which contains 40K high-quality videos spanning a wide range of categories, and the resulting captions encompass rich world knowledge, object attributes, camera movements, and crucially, detailed and precise temporal descriptions of events. Based on ShareGPT4Video, we further develop ShareCaptioner-Video, a superior captioner capable of efficiently generating high-quality captions for arbitrary videos. We annotated 4.8M aesthetically appealing videos by it and verified their effectiveness on a 10-second text2video generation task. For video understanding, we verified the effectiveness of ShareGPT4Video on several current LVLM architectures and presented our superb new LVLM ShareGPT4Video-8B. All the models, strategies, and annotations will be open-sourced and we hope it can serve as a pivotal resource for advancing both the LVLMs and T2VMs community. We released the full project at https://sharegpt4video.github.io/.

## 1 Introduction

Recent advancements in multi-modal learning, driven by large language models, have led to progress in image-text dialogue[41, 11, 18, 66, 76, 15, 2, 78, 90, 89] and text-to-image generation tasks[3, 61, 7, 63, 62, 84, 9]. This has inspired a shift towards video understanding[40, 1, 38, 83, 48, 50, 75, 47] and generation tasks[64, 23, 70, 67, 5, 45], allowing for user interaction across video and language modalities. Thus, the detailed and high-fidelity video captions, which bridge the aforementioned modalities, are instrumental in propelling the advancements within the field.

Despite the rich semantic and temporal content of videos, they are often paired with brief captions in existing data. These short descriptions limit the detailed video understanding and the controllability of video generation. While the importance of detailed captions is recognized in image-text dialogue[11, 8, 65] and text-to-image generation tasks[9, 4], similar efforts are lacking in video understanding and generation.

However, creating large-scale, high-quality video captions is challenging. Detailed captioning for long videos is non-trivial and time-consuming even for humans, hindering large-scale annotation. Current open-source LVLMs lack this capability, and closed-source APIs do not yet support video inputs. On the other hand, if we roughly degrade the input from video to multiple frames, even GPT4V struggles to describe the video with satisfied quality. For example, an intuitive idea is to provide multiple frames with timestamps to the GPT4V and generate the caption, while we find that GPT4V is unstable and sometimes misunderstands the temporal relation between the frames, and its performance further degrades with the increasing of video frames. Others such as concatenating all the frames into a large image are non-helpful to the temporal problem, and the caption loses details as the frame number increases. We also showcase these problems in Figure 11 and 12.

We posit that the challenge of devising an effective video captioning strategy is rooted in three fundamental aspects: _1) Inter-frame precise temporal change understanding_: The temporal dimension distinguishes videos from images. An imprecise temporal description can significantly diminish the quality of the video caption and lead to confusion in the training models. _2) Intra-frame detailed content description_: Detailed descriptions [11] are crucial for aligning modalities between image and text, which are also important for video-text alignment. _3) Frame-number scalability for arbitrary_length videos_: Videos encountered in the wild can vary greatly in length. An ideal captioning strategy should be resilient to this variability and generate appropriate captions for videos of any length.

To this end, we present the **Differential Sliding-Window Captioning strategy** (DiffSW), which is _stable, scalable, and efficient for generating captions for arbitrary videos_. The central concept of DiffSW is translating the all-frames-to-caption task into a differential description task. Specifically, we generate a detailed caption for the first frame and apply a sliding window of length two to the subsequent frames in chronological order. The powerful image multi-modal model, GPT4V [54], is tasked with identifying the changes between frames based on three inputs: the previous frame, its differential caption, and the current frame. This encompasses alterations in camera movement, object movement, character actions, and scene transitions. Upon acquiring all differential captions, these are input into GPT4 [53] to construct a comprehensive caption for the entire video. The differential concept allows DiffSW to concentrate on the changes between frames, i.e., the temporal changes. Its sliding design ensures the correctness of temporal order and invariance towards the total number of frames. The constant input frame number guarantees that GPT4V does not overlook details and utilizes the API efficiently, resulting in stable, scalable, and efficient caption quality from DiffSW. Furthermore, the differential design enables the re-caption of any sub-clips of a captioned video by reusing its differential captions.

Based on DiffSW, we construct **ShareGPT4Video**, which contains **40K high-quality video-caption pairs** spanning a wide range of categories, and the resulting captions encompass rich world knowledge, object attributes, camera movements, and crucially, detailed and precise temporal descriptions of events. The videos of ShareGPT4Video are collected from various sources [14; 79; 56; 21; 57; 51], employed with a Semantic-based Data Filtering strategy to mitigate content homogeneity among these videos. A Semantic-aware Key-frame Extraction strategy is then applied to the videos to reduce the temporal redundancy. DiffSW is applied to the keyframes to generate high-quality captions and we further improve its stability and quality with a Hierarchical Prompt Design. Manual quality inspection is employed to ensure the quality of the video captions.

Based on ShareGPT4Video, we present ShareCaptionor-Video, an exceptional video captioner capable of efficiently generating high-quality captions for videos of a wide range of resolution, aspect ratio, and duration. It enables the further scaling of high-quality video caption data with minor cost and satisfactory quality, and we generate high-quality captions for 4.8M aesthetically appealing videos (totaling about 3000 hours) by it.

We conduct extensive experiments in video understanding and generation tasks to demonstrate the value of our high-quality video-caption dataset and our superior video captioner. For video generation, a DiT-based [55] text-to-video model trained on the 4.8M video-captions pairs performs well in generating 10-second high-resolution videos and achieving fine-grained control over content generation. For video understanding, ShareGPT4Video brings consistent performance gain of multiple current LVLMs over multiple benchmarks by replacing a small proportion of training data. We further present ShareGPT4Video-8B, a simple yet superb LVLM that reached SOTA performance on three advancing and comprehensive video benchmarks. The model, strategy, and annotations will be publicly available and we hope this project can serve as a pivotal resource for advancing both the LVLMs and T2VMs community.

## 2 Related Work

**Large video-language models.** The vision-language community has recently made significant advancements in the field of large image-language models [42; 41; 43; 11; 15; 12; 68; 69; 2; 82; 71; 20; 25; 26; 73; 46; 28; 92; 85; 58; 87; 39]. These models typically employ three core components: a visual encoder for extracting visual features, a connector to bridge the visual and language modalities, and a large language model (LLM) to decode the multimodal context. A milestone in this field is LLaVA-1.5 [41], which uses CLIP-Large [59] as the visual encoder, a two-layer MLP as the connector, and Vicuna-v1.5 [16] as the LLM backbone. This model achieves exceptional performance with only affordable data for training. Following this paradigm, the large video-language model field has also seen emerging efforts to encode video into vision tokens suitable for LLMs, enabling understanding and reasoning about video content. For example, VideoChat2 [36] employs a video transformer and a Q-Former [34] to extract and compress visual tokens. VideoLLaVA [40] uses a pre-aligned video encoder with image and language, along with an MLP, to extract and transform visual tokens.

LLaMA-VID [38] introduces the LLAVA-like three-stage training strategy and compresses each frame's vision token into one to support long video understanding. Despite the continuous exploration of training strategies and model architectures, none of these methods have investigated the role of high-quality captions in aligning video and language modalities, as has been done in the large image-language model field [11; 8].

**Text-to-video models.** Recent advancements in the text-to-image field, such as DALL-E [3], and Stable Diffusion [61], have significantly propelled the T2I area. Inspired by these achievements, researchers have increasingly begun to explore the potential of generating high-fidelity videos from textual descriptions. For instance, MagicVideo [88] introduces a 3D U-Net-based architecture and directs temporal attention to ensure efficient, high-fidelity video generation while maintaining temporal consistency and authenticity. PixelDance [81] and SVD [5] leverage pixel-based and latent-based techniques to produce high-resolution videos, while Make-A-Video [64] and Imagen Video [22] extend text-to-image models and techniques to the text-to-video domain. A milestone in this field is Sora [45], which ambitiously employs DiT to train a text-to-video model from scratch. Sora is the first model capable of generating minute-long videos based on user instructions. The success of this remarkable feat relies on extensive high-quality video-caption data, meticulously designed architectural optimizations, and substantial computational power. Despite the pivotal role of high-quality video-caption data in T2VMs, this aspect has not received sufficient attention. Therefore, in this work, we aim to construct a high-quality video-caption dataset to advance the development of T2VMs.

## 3 ShareGPT4Video Dataset

This section provides a detailed exposition of how we construct the ShareGPT4Video dataset. We detail the entire process in Figure 2.

### Data Collection

**Selection of Data Sources.** To serve both video understanding and video generation tasks, we consider the aesthetic quality and content complexity of videos during our collection process. We first consider Panda-70M [14], a high-resolution video dataset sourced from YouTube, featuring clips ranging in one minute. This open-domain source covers diverse areas such as wildlife, cooking, sports, news & TV shows, gaming & 3D rendering. It typically includes complex content and transitions, providing a solid foundation for understanding various real-world scenarios. However, the complexity of these contents and transitions presents a significant challenge for the video generation field. To address this, we also source a large volume of aesthetically appealing videos from some user-uploaded video websites [56; 57; 51]. These videos predominantly consist of scenic views and aesthetically pleasing human activities, involving fewer transitions and simpler events. Finally, we supplement our collection with selected videos from Ego4D [21] and BDD100K [79] to fill the gaps in ego-centric human activities and auto-driving scenarios, ensuring our video sources encompass as many real-world scenes as possible.

**Semantic-Based Data Filtering.** Although our captioning method can support videos of extended lengths, our collection primarily focuses on videos shorter than two minutes due to the trade-off of the duration and amount of videos. We initially filter out videos from our selected data sources longer than two minutes, leaving videos in two minutes as the candidates. We then introduce a semantic-based data filtering strategy to mitigate content homogeneity among these candidates and maintain diversity in the final video dataset. This approach aims to select videos with significant thematic differences from the pool of candidates to compose our final video collection. Specifically, we first use the Panda-Student [14] model to generate a short caption with one sentence for each candidate video, and then maintain a final pool of video candidates. Whenever a new video \(V\) is processed, we encode its corresponding short caption \(S\) using the Bert-Base-Uncased [17] language model to obtain the \(\mathtt{CLS}\) token \(P_{n+1}\in\mathbb{R}^{1\times D}\), which captures high-level semantic expressions. We then calculate the similarity between this \(\mathtt{CLS}\) token \(P_{n+1}\) and the \(\mathtt{CLS}\) tokens \(\{P_{1},P_{2},\dots,P_{n}\}\) of videos already in the final candidate pool. A new video will only be added to the pool if its maximum similarity is below a predefined threshold. We provide the pseudo-code in Figure 14.

### Video Processing

Videos are commonly redundant on the temporal dimension, and keyframe sampling is a general idea to represent a video compactly. However, traditional key-frame extraction methods [91; 6] often struggle to ensure semantic coherence, leading to missing key-frames covering crucial changes and transitions. Consequently, we develop a semantic-aware key-frame extraction method that strikes a balance between reducing temporal redundancy in videos and maintaining the semantic coherence of the content.

**Semantic-aware Key-frame Extraction.** We denote \(V\in\mathbb{R}^{T\times H\times W\times 3}\) as a \(T\) frame set sampled from a video with fixed 2-second intervals. We calculate the keyframe set \(V^{\prime}\in\mathbb{R}^{T^{\prime}\times H\times W\times 3}\) that are sufficiently sparse yet comprehensively cover the evolution of events within the video that \(T^{\prime}<T\). We view the output \(\mathtt{CLS}\) token of the CLIP-Large image encoder [59] as the global semantics of each frame and remove the adjacent frames that have a high semantic similarity. In practice, we initialize the keyframe set \(V^{\prime}\) with the first frame of \(V\). For each frame in \(V\), we calculate its semantic similarity \(d\) with the latest keyframe in \(V^{\prime}\). If \(d\) is lower than the pre-defined threshold, we view the frame as a keyframe and add it to the \(V^{\prime}\). If not, the frame is skipped as redundant. For completeness, the last frame of \(V\) is always added in \(V^{\prime}\). We provide the pseudo-code in Figure 15.

### Captioning Pipeline

As we mentioned in Section 1, we find if we feed all the frames to the GPT4V directly, the GPT4V struggles to stably generate captions with the correct temporal relation between frames, and its performance further worsens with the frame number increasing. On the other hand, if we concatenate all the frames into a large image, the GPT4V loses more details with the increasing frame number, as shown in Figure 11 and 12. To this end, a **stable**, **scalable**, and **efficient** strategy is essential for large-scale annotation of videos with arbitrary length.

Figure 3: **Comprehensive video-caption dataset: (a) The dataset covers a broad spectrum of content, including wildlife, cooking, sports, scenery, ego-centric human activities, auto-driving scenarios, etc. (b) The dataset includes videos ranging from 2 seconds to 2 minutes in length. (c) The captions primarily range from 200 to 400 words, providing rich temporal information that serves video understanding and generation tasks well.**

Figure 2: **Pipeline for generating high-quality video-caption data. We begin by selecting diverse video sources based on aesthetic quality and content complexity. Next, we use semantic-based data filtering to prevent content homogenization. We then apply semantic-aware key-frame extraction for sparse sampling, maintaining significant semantic variations. Finally, we implement a differential sliding-window captioning strategy, utilizing GPT-4V to generate detailed and temporally rich captions.**

**Differential Sliding-window Captioning.** To this end, we develop a differential sliding-window captioning pipeline to generate high-quality captions with detailed temporal descriptions for various videos. Specifically, the input fed to the image multi-modal model each time includes the current key-frame and the previous key-frame along with its differential caption. Then, we introduce the Differential Prompt to guide GPT4V in focusing on the changes between the current and previous frames, such as posture, position, camera angle, etc. Additionally, incorporating the differential caption of the previous frame as supplementary context enhances the response quality and reduces hallucinations. This is because the image embedding and textual caption provide explicit and implicit representations of the image, respectively. The differential caption not only adds extra context but also integrates temporal information from two frames ago, further improving the model's temporal understanding. It's important to note that for the first key-frame, which lacks a preceding frame, its differential caption is replaced directly with the standard caption. Finally, we input all differential captions along with their corresponding timestamps into GPT4. A specific Summary Prompt is designed to instruct the LLM to generate high-quality video captions with precise temporal dynamics and detailed spatial information. In practice, we use GPT-4-Turbo-04-09 for all the annotations.

In the design of the prompts, we discovered that an explicit Hierarchical Prompt Design significantly aids the GPT4V in comprehending its role, its expected format, and its operational boundaries. This approach contributes to the stabilization of the output's format and enhances the overall quality of the results. For more details, please refer to Section A.2

## 4 ShareCaptioner-Video

### Model Design

We fine-tune the IXC2-4KHD [19] using the collected video caption data, resulting in our ShareCaptioner-Video. For flexible usage, we re-organize the data for the following capabilities:

**1. Fast Captioning** The model employs an image-grid format for direct video captioning, providing rapid generation speeds that are ideal for short videos. In practice, we concatenate all the keyframes of a video into a vertically elongated image and train the model on a caption task.

**2. Sliding Captioning** The model supports streaming captioning in a differential sliding-window format, yielding high-quality captions that are suitable for long videos. Similar to the captioning pipeline used in Section 3.3, we take the two adjacent keyframes alongside the previous differential caption as input, and train the model to describe the events occurring between them.

**3. Clip Summarizing** The model can swiftly summarize any clip from ShareGPT4Video or videos that have undergone the differential sliding-window captioning process, eliminating the need to re-process frames. We use all the differential descriptions as input, and the output is the video caption.

**4. Prompt Re-Captioning:** The model can rephrase prompts input by users who prefer specific video generation areas, ensuring that T2VMs trained on high-quality video-caption data maintain format alignment during inference with their training. In practice, we use GPT-4 to generate Sora-style prompts for our dense captions, and we train the re-captioning task in reverse, _i.e._, by using the generated prompt as input and the dense caption as the training target.

Figure 4: The ShareCaptioner-Video is a Four-in-One exceptional video captioning model with the following capabilities: Fast captioning, Sliding Captioning, Clip Summarizing, and Prompt Re-Captioning.

In practice, we fine-tune the model end-to-end over one epoch. We follow the default high-resolution strategy, using 'HD-55' for fast captioning and 'HD-25' for the others. The learning rate is uniform across all model components and warms up from 0 to within the first 1% of steps. The batch size is set to, and we sample the data uniformly.

### Scaling-up Captions

To validate the effectiveness of our ShareCaptioner-Video in the video captioning task and further support the development of the video generation domain, we utilized it to annotate a large volume of aesthetically appealing videos. Specifically, we meticulously collect and process 4.8 million video clips, totaling approximately 3000 hours, from three sources: MixKit [51], Pexels [56], and Pixabay [57]. Subsequently, we employ the sliding captioning mode of ShareCaptioner-Video to generate high-quality captions for these videos. The total captioning process requires approximately 4000 H100 GPU hours. We provide some statistics on generated captions in Figure 8.

## 5 Experiments

### Video Understanding

**Datasets and Benchmarks.** To thoroughly explore the benefits that our high-quality video-caption data bring to LVLMs, we conduct comprehensive evaluations of the model across three multi-modal video benchmarks. VideoBench [52] curates approximately 15,000 QA pairs spanning 10 evaluation dimensions from 13 existing data sources, such as MSVD-QA [74], MSRVTT-QA [74], Activitynet-QA [80], etc. MVBench [36] is designed to challenge LVLMs with video tasks that cannot be effectively resolved by single-frame reliance, featuring 4,000 QA pairs derived from 11 public video benchmarks. TempCompass [44] specifically assesses the nuanced performance of LVLMs across various temporal aspects, such as speed, direction, and attribute changes. It includes 410 videos and 7,540 meticulously collected instructions, emphasizing temporal comprehension and interaction.

**Improving current LVLMs with ShareGPT4Video.** We validate the effectiveness of the high-quality video-caption data collected in ShareGPT4Video to improve the performance of current LVLMs. For fairness and simplicity, we integrate 28K high-quality video-caption data related to complex scenes (Panda-70M [14], Ego4D [21], and BDD100K [79]) of ShareGPT4Video to replace the captions data in the VideoChatGPT-100K [50] conversation data with an equivalent number. Then we train the VideoLLaVA [40] and LLaMA-VID [38] with their default training settings and hyperparameters.

As shown in Table 1, ShareGPT4Video consistently improves the alignment between video and language modalities in different LVLM architectures and scales. Specifically, VideoLLaVA-7B [40] achieves an average performance gain of 1.1 across three comprehensive multi-modal video benchmarks after integrating high-quality captions, while LLaMA-VID-7B and LLaMA-VID-13B achieve an average gain of 2.0 and 2.3, separately. Our high-quality video-caption data is particularly effective in helping LVLMs achieve significant performance improvements on benchmarks that require complex temporal understanding, such as TempCompass [44].

**ShareGPT4Video-8B.** To obtain our final ShareGPT4Video-8B model, we start with the LLaVA-Next-8B [32] image multi-modal model, implemented by the Open-LLaVA-Next codebase [13]. Consistent with previous LVLM approaches [40; 50], we uniformly sample 16 frames from each video and arrange these frames into a 4x4 image grid to form the input for both training and inference, following the IG-VLM [30] strategy. For training data, we first collect 153K VQA data from various instructional video-to-text datasets to build our baseline. This collection includes 13K conversational

\begin{table}
\begin{tabular}{l|c c c|c} \hline \hline Model & \multicolumn{3}{c|}{VideoBench MVBench TempCompass} & Avg. \\ \hline Video1LaVA-7B [40] & 34.5 & 43.0 & 50.6 & 42.7 \\ Video1LaVA-7B+Ours & **35.2** & **43.6** & **52.7** & **43.8** \\ \hline LLaMA-VID-7B [38] & 36.5 & 41.3 & 48.1 & 42.0 \\ LLaMA-VID-7B-Ours & **38.2** & **43.2** & **50.6** & **44.0** \\ \hline LLMaVA-VID-13B [38] & 48.3 & 43.3 & 51.4 & 47.7 \\ LLaMA-VID-13B+Ours & **52.4** & **44.2** & **53.3** & **50.0** \\ \hline \hline \end{tabular}
\end{table}
Table 1: **The gain from high-quality captions is universal among model architectures and scales. We report the baseline based on their public checkpoints. The best results are bold.**

\begin{table}
\begin{tabular}{l c|c c c|c} \hline \hline Caption & Unblock & ViT & VideoBench & MVBench & TempCompass & Avg. \\ \hline – & \(\times\) & 37.3 & 47.2 & 57.2 & 47.2 \\ short & \(\times\) & 36.9 & 47.5 & 56.1 & 46.8 \\ short & \(\checkmark\) & 37.5 & 47.9 & 56.9 & 47.4 \\ detailed & \(\times\) & 40.7 & 50.3 & 60.7 & 50.6 \\ detailed & \(\checkmark\) & **41.2** & **51.2** & **61.5** & **51.3** \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Combined with VQA data, detailed captions can benefit LVLMs more compared to short captions. The baseline (first row) utilizes only 153K VQA data. The best results are in bold.**

[MISSING_PAGE_FAIL:8]

of high-fidelity 10-second videos. For comparison, we fine-tuned a baseline model with the same quantity of video-short-captions pairs. For more training details, please refer to Section A.1.

**Qualitative analysis.** As illustrated in Figure 5, the T2VM can accurately follow detailed prompts and demonstrate remarkable control over semantic content and camera movement when aided by high-quality, detailed captions generated by ShareCaptioner-Video. The resulting video showcases intricate and lively content. In contrast, when provided with brief captions, the T2VM struggles to adhere to complex generation prompts, leading to subpar results.

## 6 Limitations and Social Impacts

**Limitations.** Although our current pipeline for generating high-quality video captions fully utilizes visual and textual information, it is limited by GPT4V's inability to incorporate audio information

Figure 5: **Example of 10-second text-to-video task. The T2VM trained on the detailed video-caption data can exhibit impressive camera control.**

Figure 6: **Influence of T2VM training caption length. Thanks to the high-quality captions generated by ShareCaptioner-Video, the T2VM trained on the detailed video-caption data exhibits impressive semantic content control (video below), while the T2VM with short captions failed to follow the complex prompt (video above).**

\begin{table}
\begin{tabular}{l|c c c c c c c c c c c c c c c c c c c} \hline \hline Model & AS & AR & AA & FA & UA & OE & OS & MO & AL & ST & AC & MC & MA & SC & FP & CO & EN & ER & CI & Avg. \\ \hline Other-7B [33] & 23.0 & 23.0 & 27.5 & 27.0 & 29.5 & 53.0 & 28.0 & 33.0 & 24.5 & 23.5 & 27.5 & 26.0 & 28.5 & 18.0 & 38.5 & 22.0 & 22.0 & 23.5 & 19.0 & 19.5 & 26.8 \\ mPLUG-Out-7B [76] & 22.0 & 28.0 & 34.0 & 29.0 & 29.0 & 40.5 & 27.0 & 31.5 & 27.0 & 23.0 & 29.0 & 31.5 & 27.0 & 40.0 & 44.0 & 24.0 & 31.0 & 26.0 & 20.5 & 29.5 \\ LLAMA-Mapter [86] & 23.0 & 28.0 & 51.0 & 30.0 & 33.0 & 53.5 & 32.5 & 33.5 & 25.5 & 21.5 & 30.5 & 29.0 & 29.2 & 41.5 & 39.5 & 25.0 & 31.5 & 22.5 & 28.0 & 32.0 & 31.7 \\ VideoMMFP-7B [80] & 23.5 & 26.0 & 20.0 & 22.5 & 56.0 & 20.0 & 22.0 & 23.0 & 20.0 & 31.0 & 30.5 & 25.5 & 39.5 & 48.5 & 29.0 & 33.0 & 29.5 & 26.0 & 35.5 & 32.7 \\ VideoLLAMA-7B [82] & 27.5 & 25.5 & 51.0 & 29.0 & 39.0 & 48.0 & 40.5 & 38.0 & 22.5 & 24.0 & 34.0 & 22.5 & 34.5 & 48.5 & 22.5 & 48.5 & 32.0 & 40.0 & 32.0 & 21.0 & 37.0 & 34.1 \\ Video-7B [35] & 23.5 & 26.5 & 56.0 & 33.5 & 43.5 & 40.5 & 40.0 & 35.0 & 25.5 & 27.0 & 48.5 & 35.0 & 24.5 & 46.0 & 26.5 & 41.0 & 23.5 & 33.5 & 36.0 & 35.5 \\ VideoLLAMA-7B [38] & 46.0 & 42.0 & 48.0 & 24.0 & 39.5 & 53.0 & 53.5 & 38.0 & 40.0 & **29.0** & 21.0 & 38.5 & **48.0** & 26.0 & 20.0 & 43.1 & **33.5** & **41.5** & 27.5 & 38.5 & 31.5 & 43.0 \\ LLAMA-VID-7B [38] & 45.5 & 40.5 & 58.0 & 39.5 & **55.0** & 53.5 & 40.0 & 35.5 & 18.5 & 27.5 & **87.0** & 41.5 & 23.0 & 45.5 & 41.0 & 27.0 & 40.0 & **34.5** & **41.5** & 31.5 & 41.3 \\ ShareGPT+Video-8B & **40.5** & 39.5 & **79.5** & **40.0** & 54.5 & **82.5** & **54.5** & 32.5 & **50.5** & **41.5** & 84.5 & 35.5 & **62.5** & **75.0** & **51.0** & 25.5 & **46.5** & 28.5 & 39.0 & **51.5** & **51.2** \\ \hline \hline \end{tabular}
\end{table}
Table 5: **Comparison with SOTA methods on MVBench. * denotes our evaluation results with the public checkpoints. The best results are bold and the second-best results are underlined.**simultaneously. Audio information is beneficial in conversational scenarios involving daily human activities. We plan to introduce audio information in future work, once GPT4o supports audio input, to enhance the quality of our captions further. Additionally, the sampling interval for initial sparsification of the original videos and the window length setting in DiffSW was empirically set to 2 seconds and adjacent 2 frames based on the majority of videos. We plan to make these hyperparameters adaptive to video content in future work to handle a wider variety of video content effectively.

**Social impacts.** 1) Since the large language model involves the generation process of the large-scale captions, we have not manually verified each caption for socially biased content; 2) Although we utilize video data from existing public datasets, we cannot ensure that the selected videos do not contain human faces. Therefore, while there are no restrictions on the use of our generated captions, users must adhere to the licenses of the original video sources when using the videos. Our models can be manipulated or "jailbroken" to produce outputs that are non-inclusive or disrespectful as many LVLMs do. This vulnerability highlights the importance of continuing to improve the robustness and ethical alignment of LVLMs to prevent misuse and ensure they contribute positively to diverse applications.

## 7 Conclusion

In this study, we aim to address the challenge of lacking high-quality video-caption data for large video-language models (LVLMs) and text-to-video models (T2VMs). We develop ShareGPT4Video, a high-quality video-caption dataset, and ShareCaptioner-Video, an advanced and versatile model in the video-language multi-modal area. By employing a series of strategies and designs, we generate 40K detailed captions from advanced image multi-modal model, GPT4V, and 4.8M high-quality captions from our ShareCaptioner-Video. These captions include rich world knowledge, object attributes, camera movements, and detailed temporal descriptions of events. Our extensive experiments validate the effectiveness of our dataset and captioner in enhancing video understanding and generation tasks. We believe that ShareGPT4Video and ShareCaptioner-Video will serve as essential resources for advancing research in the LVLM and T2VM communities.

## 8 Acknowledgments

This work was supported by the Anhui Provincial Natural Science Foundation under Grant 2108085UD12. We acknowledge the partial support of the GPU cluster built by MCC Lab of Information Science and Technology Institution, USTC. This work was also partially supported by the Shanghai Artificial Intelligence Laboratory, the National Key R&D Program of China (2022ZD0160201), the Centre for Perceptual and Interactive Intelligence (CPII) Ltd under the Innovation and Technology Commission (ITC)'s InnoHK. Dahua Lin is a PI of CPII under the InnoHK.

## References

* [1] K. Ataallah, X. Shen, E. Abdelrahman, E. Sleiman, D. Zhu, J. Ding, and M. Elhoseiny. Minigpt4-video: Advancing multimodal llms for video understanding with interleaved visual-textual tokens. _arXiv preprint arXiv:2404.03413_, 2024.
* [2] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. _arXiv preprint arXiv:2308.12966_, 2023.
* [3] J. Betker, G. Goh, L. Jing, T. Brooks, J. Wang, L. Li, L. Ouyang, J. Zhuang, J. Lee, Y. Guo, et al. Improving image generation with better captions. _Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf_, 2(3):8, 2023.
* [4] J. Betker, G. Goh, L. Jing, T. Brooks, J. Wang, L. Li, L. Ouyang, J. Zhuang, J. Lee, Y. Guo, et al. Improving image generation with better captions. _Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf_, 2(3):8, 2023.
* [5] A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch, M. Kilian, D. Lorenz, Y. Levi, Z. English, V. Voleti, A. Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. _arXiv preprint arXiv:2311.15127_, 2023.
* [6] J. Calic and E. Luzierdo. Efficient key-frame extraction and video analysis. In _Proceedings of International Conference on Information Technology: Coding and Computing_, pages 28-33. IEEE, 2002.

* [7] H. Chang, H. Zhang, J. Barber, A. Maschinot, J. Lezama, L. Jiang, M.-H. Yang, K. Murphy, W. T. Freeman, M. Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. _arXiv preprint arXiv:2301.00704_, 2023.
* [8] G. H. Chen, S. Chen, R. Zhang, J. Chen, X. Wu, Z. Zhang, Z. Chen, J. Li, X. Wan, and B. Wang. Allava: Harnessing gpt4v-synthesized data for a lite vision-language model. _arXiv preprint arXiv:2402.11684_, 2024.
* [9] J. Chen, J. Yu, C. Ge, L. Yao, E. Xie, Y. Wu, Z. Wang, J. Kwok, P. Luo, H. Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. _arXiv preprint arXiv:2310.00426_, 2023.
* [10] J. Chen, D. Zhu, K. Haydarov, X. Li, and M. Elhoseiny. Video chatcaptioner: Towards the enriched spatiotemporal descriptions. _arXiv preprint arXiv:2304.04227_, 2023.
* [11] L. Chen, J. Li, X. Dong, P. Zhang, C. He, J. Wang, F. Zhao, and D. Lin. Sharegpt4v: Improving large multi-modal models with better captions. _arXiv preprint arXiv:2311.12793_, 2023.
* [12] L. Chen, J. Li, X. Dong, P. Zhang, Y. Zang, Z. Chen, H. Duan, J. Wang, Y. Qiao, D. Lin, et al. Are we on the right way for evaluating large vision-language models? _arXiv preprint arXiv:2403.20330_, 2024.
* [13] L. Chen and L. Xing. Open-llava-next: An open-source implementation of lava-next series for facilitating the large multi-modal model community. https://github.com/xiaoachen98/Open-LLaVA-NeXT, 2024.
* [14] T.-S. Chen, A. Siarohin, W. Menapace, E. Deyneka, H.-w. Chao, B. E. Jeon, Y. Fang, H.-Y. Lee, J. Ren, M.-H. Yang, et al. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. _arXiv preprint arXiv:2402.19479_, 2024.
* [15] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, Z. Muyan, Q. Zhang, X. Zhu, L. Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. _arXiv preprint arXiv:2312.14238_, 2023.
* [16] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, et al. Vicuna: An open-source chatbot impressions gpt-4 with 90%* chatgpt quality. _See https://vicuna. lmsys. org (accessed 14 April 2023)_, 2023.
* [17] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* [18] X. Dong, P. Zhang, Y. Zang, Y. Cao, B. Wang, L. Ouyang, X. Wei, S. Zhang, H. Duan, M. Cao, et al. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. _arXiv preprint arXiv:2401.16420_, 2024.
* [19] X. Dong, P. Zhang, Y. Zang, Y. Cao, B. Wang, L. Ouyang, S. Zhang, H. Duan, W. Zhang, Y. Li, et al. Internlm-xcomposer2-4khd: A pioneering large vision-language model handling resolutions from 336 pixels to 4k hd. _arXiv preprint arXiv:2404.06512_, 2024.
* [20] H. Duan, J. Yang, Y. Qiao, X. Fang, L. Chen, Y. Liu, X. Dong, Y. Zang, P. Zhang, J. Wang, et al. Vlnevalkit: An open-source toolkit for evaluating large multi-modality models. _arXiv preprint arXiv:2407.11691_, 2024.
* [21] K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar, J. Hamburger, H. Jiang, M. Liu, X. Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18995-19012, 2022.
* [22] J. Ho, W. Chan, C. Saharia, J. Whang, R. Gao, A. Gritsenko, D. P. Kingma, B. Poole, M. Norouzi, D. J. Fleet, et al. Imagen video: High definition video generation with diffusion models. _arXiv preprint arXiv:2210.02303_, 2022.
* [23] W. Hong, M. Ding, W. Zheng, X. Liu, and J. Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. _arXiv preprint arXiv:2205.15868_, 2022.
* [24] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.
* [25] Q. Huang, X. Dong, P. Zhang, B. Wang, C. He, J. Wang, D. Lin, W. Zhang, and N. Yu. Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13418-13427, 2024.

* [26] Q. Huang, X. Dong, P. Zhang, Y. Zang, Y. Cao, J. Wang, D. Lin, W. Zhang, and N. Yu. Deciphering cross-modal alignment in large vision-language models with modality integration rate. _arXiv preprint arXiv:2410.07167_, 2024.
* [27] D. Jiang, R. Zhang, Z. Guo, Y. Wu, J. Lei, P. Qiu, P. Lu, Z. Chen, G. Song, P. Gao, et al. Mmsearch: Benchmarking the potential of large models as multi-modal search engines. _arXiv preprint arXiv:2409.12959_, 2024.
* [28] Y. Jiao, S. Chen, Z. Jie, J. Chen, L. Ma, and Y.-G. Jiang. Lumen: Unleashing versatile vision-centric capabilities of large multimodal models. _arXiv preprint arXiv:2403.07304_, 2024.
* [29] P. Jin, R. Takanobu, C. Zhang, X. Cao, and L. Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. _arXiv preprint arXiv:2311.08046_, 2023.
* [30] W. Kim, C. Choi, W. Lee, and W. Rhee. An image grid can be worth a video: Zero-shot video question answering using a vlm. _arXiv preprint arXiv:2403.18406_, 2024.
* [31] P.-Y. Lab and T. A. etc. Open-sora-plan, Apr. 2024.
* [32] B. Li, K. Zhang, H. Zhang, D. Guo, R. Zhang, F. Li, Y. Zhang, Z. Liu, and C. Li. Llava-next: Stronger llms supercharge multimodal capabilities in the wild, May 2024.
* [33] B. Li, Y. Zhang, L. Chen, J. Wang, J. Yang, and Z. Liu. Otter: A multi-modal model with in-context instruction tuning. _arXiv preprint arXiv:2305.03726_, 2023.
* [34] J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstraping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_, 2023.
* [35] K. Li, Y. He, Y. Wang, Y. Li, W. Wang, P. Luo, Y. Wang, L. Wang, and Y. Qiao. Videochat: Chat-centric video understanding. _arXiv preprint arXiv:2305.06355_, 2023.
* [36] K. Li, Y. Wang, Y. He, Y. Li, Y. Wang, Y. Liu, Z. Wang, J. Xu, G. Chen, P. Luo, et al. Mvbench: A comprehensive multi-modal video understanding benchmark. _arXiv preprint arXiv:2311.17005_, 2023.
* [37] Y. Li, Y. Song, L. Cao, J. Tetreault, L. Goldberg, A. Jaimes, and J. Luo. Tgif: A new dataset and benchmark on animated gif description. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 4641-4650, 2016.
* [38] Y. Li, C. Wang, and J. Jia. Llama-vid: An image is worth 2 tokens in large language models. _arXiv preprint arXiv:2311.17043_, 2023.
* [39] Z. Li, B. Yang, Q. Liu, Z. Ma, S. Zhang, J. Yang, Y. Sun, Y. Liu, and X. Bai. Monkey: Image resolution and text label are important things for large multi-modal models. _arXiv preprint arXiv:2311.06607_, 2023.
* [40] B. Lin, B. Zhu, Y. Ye, M. Ning, P. Jin, and L. Yuan. Video-llava: Learning united visual representation by alignment before projection. _arXiv preprint arXiv:2311.10122_, 2023.
* [41] H. Liu, C. Li, Y. Li, and Y. J. Lee. Improved baselines with visual instruction tuning. _arXiv preprint arXiv:2310.03744_, 2023.
* [42] H. Liu, C. Li, Y. Li, B. Li, Y. Zhang, S. Shen, and Y. J. Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024.
* [43] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. _arXiv preprint arXiv:2304.08485_, 2023.
* [44] Y. Liu, S. Li, Y. Liu, Y. Wang, S. Ren, L. Li, S. Chen, X. Sun, and L. Hou. Tempcompass: Do video llms really understand videos? _arXiv preprint arXiv:2403.00476_, 2024.
* [45] Y. Liu, K. Zhang, Y. Li, Z. Yan, C. Gao, R. Chen, Z. Yuan, Y. Huang, H. Sun, J. Gao, et al. Sora: A review on background, technology, limitations, and opportunities of large vision models. _arXiv preprint arXiv:2402.17177_, 2024.
* [46] Z. Liu, T. Chu, Y. Zang, X. Wei, X. Dong, P. Zhang, Z. Liang, Y. Xiong, Y. Qiao, D. Lin, et al. Mmdu: A multi-turn multi-image dialog understanding benchmark and instruction-tuning dataset for lvlms. _arXiv preprint arXiv:2406.11833_, 2024.
* [47] R. Luo, Z. Zhao, M. Yang, J. Dong, M. Qiu, P. Lu, T. Wang, and Z. Wei. Valley: Video assistant with large language model enhanced ability. _arXiv preprint arXiv:2306.07207_, 2023.

* [48] B. Ma, Z. Zong, G. Song, H. Li, and Y. Liu. Exploring the role of large language models in prompt encoding for diffusion models. _arXiv preprint arXiv:2406.11831_, 2024.
* [49] X. Ma, Y. Wang, G. Jia, X. Chen, Z. Liu, Y.-F. Li, C. Chen, and Y. Qiao. Latte: Latent diffusion transformer for video generation. _arXiv preprint arXiv:2401.03048_, 2024.
* [50] M. Maaz, H. Rasheed, S. Khan, and F. S. Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. _arXiv preprint arXiv:2306.05424_, 2023.
* [51] mikkit. mikkit. https://mikkit.com/videos/, 2023.
* [52] M. Ning, B. Zhu, Y. Xie, B. Lin, J. Cui, L. Yuan, D. Chen, and L. Yuan. Video-bench: A comprehensive benchmark and toolkit for evaluating video-based large language models. _arXiv preprint arXiv:2311.16103_, 2023.
* [53] OpenAI. Chatgpt. https://chat.openai.com/, 2023.
* [54] OpenAI. Gpt-4v(sision) system card. https://cdn.openai.com/papers/GPTV_System_Card.pdf, 2023.
* [55] W. Peebles and S. Xie. Scalable diffusion models with transformers. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4195-4205, 2023.
* [56] Pexels. Pexels. https://www.pexels.com/videos/, 2023.
* [57] pixabay. pixabay. https://pixabay.com/videos/, 2023.
* [58] Y. Qiao, H. Duan, X. Fang, J. Yang, L. Chen, S. Zhang, J. Wang, D. Lin, and K. Chen. Prism: A framework for decoupling and assessing the capabilities of vlms. _arXiv preprint arXiv:2406.14544_, 2024.
* [59] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In _International Conference on Machine Learning_, pages 8748-8763. PMLR, 2021.
* [60] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _The Journal of Machine Learning Research_, 21(1):5485-5551, 2020.
* [61] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.
* [62] N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein, and K. Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22500-22510, 2023.
* [63] H. Shao, S. Qian, H. Xiao, G. Song, Z. Zong, L. Wang, Y. Liu, and H. Li. Visual cot: Unleashing chain-of-thought reasoning in multi-modal language models. _arXiv preprint arXiv:2403.16999_, 2024.
* [64] U. Singer, A. Polyak, T. Hayes, X. Yin, J. An, S. Zhang, Q. Hu, H. Yang, O. Ashual, O. Gafni, et al. Make-a-video: Text-to-video generation without text-video data. _arXiv preprint arXiv:2209.14792_, 2022.
* [65] J. Wang, L. Meng, Z. Weng, B. He, Z. Wu, and Y.-G. Jiang. To see is to believe: Prompting gpt-4v for better visual instruction tuning. _arXiv preprint arXiv:2311.07574_, 2023.
* [66] W. Wang, Q. Lv, W. Yu, W. Hong, J. Qi, Y. Wang, J. Ji, Z. Yang, L. Zhao, X. Song, et al. Cogvlm: Visual expert for pretrained language models. _arXiv preprint arXiv:2311.03079_, 2023.
* [67] Y. Wang, X. Chen, X. Ma, S. Zhou, Z. Huang, Y. Wang, C. Yang, Y. He, J. Yu, P. Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. _arXiv preprint arXiv:2309.15103_, 2023.
* [68] H. Wu, Z. Zhang, E. Zhang, C. Chen, L. Liao, A. Wang, C. Li, W. Sun, Q. Yan, G. Zhai, et al. Q-bench: A benchmark for general-purpose foundation models on low-level vision. _arXiv preprint arXiv:2309.14181_, 2023.
* [69] H. Wu, Z. Zhang, E. Zhang, C. Chen, L. Liao, A. Wang, K. Xu, C. Li, J. Hou, G. Zhai, et al. Q-Instruct: Improving low-level visual abilities for multi-modality foundation models. _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1-16, 2024.

* [70] J. Z. Wu, Y. Ge, X. Wang, S. W. Lei, Y. Gu, Y. Shi, W. Hsu, Y. Shan, X. Qie, and M. Z. Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 7623-7633, 2023.
* [71] P. Xia, S. Han, S. Qiu, Y. Zhou, Z. Wang, W. Zheng, Z. Chen, C. Cui, M. Ding, L. Li, et al. Mmie: Massive multimodal interleaved comprehension benchmark for large vision-language models. _arXiv preprint arXiv:2410.10139_, 2024.
* [72] J. Xiao, X. Shang, A. Yao, and T.-S. Chua. Next-qa: Next phase of question-answering to explaining temporal actions. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9777-9786, 2021.
* [73] L. Xing, Q. Huang, X. Dong, J. Lu, P. Zhang, Y. Zang, Y. Cao, C. He, J. Wang, F. Wu, et al. Pyramidrop: Accelerating your large vision-language models via pyramid visual redundancy reduction. _arXiv preprint arXiv:2410.17247_, 2024.
* [74] D. Xu, Z. Zhao, J. Xiao, F. Wu, H. Zhang, X. He, and Y. Zhuang. Video question answering via gradually refined attention over appearance and motion. In _Proceedings of the 25th ACM International Conference on Multimedia_, pages 1645-1653, 2017.
* [75] Z. Xue, G. Song, Q. Guo, B. Liu, Z. Zong, Y. Liu, and P. Luo. Raphael: Text-to-image generation via large mixture of diffusion paths. _Advances in Neural Information Processing Systems_, 36, 2024.
* [76] Q. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y. Zhou, J. Wang, A. Hu, P. Shi, Y. Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. _arXiv preprint arXiv:2304.14178_, 2023.
* [77] K. Yi, C. Gan, Y. Li, P. Kohli, J. Wu, A. Torralba, and J. B. Tenenbaum. Clevrer: Collision events for video representation and reasoning. _arXiv preprint arXiv:1910.01442_, 2019.
* [78] A. Young, B. Chen, C. Li, C. Huang, G. Zhang, G. Zhang, H. Li, J. Zhu, J. Chen, J. Chang, et al. Yi: Open foundation models by 01. ai. _arXiv preprint arXiv:2403.04652_, 2024.
* [79] F. Yu, H. Chen, X. Wang, W. Xian, Y. Chen, F. Liu, V. Madhavan, and T. Darrell. Bdd100k: A diverse driving dataset for heterogeneous multitask learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2636-2645, 2020.
* [80] Z. Yu, D. Xu, J. Yu, T. Yu, Z. Zhao, Y. Zhuang, and D. Tao. Activitynet-qa: A dataset for understanding complex web videos via question answering. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 33, pages 9127-9134, 2019.
* [81] Y. Zeng, G. Wei, J. Zheng, J. Zou, Y. Wei, Y. Zhang, and H. Li. Make pixels dance: High-dynamic video generation. _arXiv preprint arXiv:2311.10982_, 2023.
* [82] G. Zhang, S. Qu, J. Liu, C. Zhang, C. Lin, C. L. Yu, D. Pan, E. Cheng, J. Liu, Q. Lin, et al. Map-neo: Highly capable and transparent bilingual large language model series. _arXiv preprint arXiv:2405.19327_, 2024.
* [83] H. Zhang, X. Li, and L. Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. _arXiv preprint arXiv:2306.02858_, 2023.
* [84] L. Zhang, A. Rao, and M. Agrawala. Adding conditional control to text-to-image diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 3836-3847, 2023.
* [85] P. Zhang, X. Dong, Y. Zang, Y. Cao, R. Qian, L. Chen, Q. Guo, H. Duan, B. Wang, L. Ouyang, et al. Internlm-xcomposer-2.5: A versatile large vision language model supporting long-contextual input and output. _arXiv preprint arXiv:2407.03320_, 2024.
* [86] R. Zhang, J. Han, A. Zhou, X. Hu, S. Yan, P. Lu, H. Li, P. Gao, and Y. Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. _arXiv preprint arXiv:2303.16199_, 2023.
* [87] B. Zhou, Y. Hu, X. Weng, J. Jia, J. Luo, X. Liu, J. Wu, and L. Huang. Tinyllava: A framework of small-scale large multimodal models. _arXiv preprint arXiv:2402.14289_, 2024.
* [88] D. Zhou, W. Wang, H. Yan, W. Lv, Y. Zhu, and J. Feng. Magicvideo: Efficient video generation with latent diffusion models. _arXiv preprint arXiv:2211.11018_, 2022.
* [89] Y. Zhou, C. Cui, R. Rafailov, C. Finn, and H. Yao. Aligning modalities in vision large language models via preference fine-tuning. _arXiv preprint arXiv:2402.11411_, 2024.

* [90] Y. Zhou, Z. Fan, D. Cheng, S. Yang, Z. Chen, C. Cui, X. Wang, Y. Li, L. Zhang, and H. Yao. Calibrated self-rewarding vision language models. _arXiv preprint arXiv:2405.14622_, 2024.
* [91] Y. Zhuang, Y. Rui, T. S. Huang, and S. Mehrotra. Adaptive key frame extraction using unsupervised clustering. In _Proceedings 1998 International Conference on Image Processing. ICIP98 (cat. no. 98cb36269)_, volume 1, pages 866-870. IEEE, 1998.
* [92] Z. Zong, B. Ma, D. Shen, G. Song, H. Shao, D. Jiang, H. Li, and Y. Liu. Mova: Adapting mixture of vision experts to multimodal context. _arXiv preprint arXiv:2404.13046_, 2024.

Appendix

In the appendix, we provide more results and analysis and summarize them as follows:

* In Section A.1, we detail the experimental setups.
* In Section A.2, we talk about the prompt design philosophy and showcase the prompt for each stage.
* In Section A.3, we present the template of our hierarchical prompt.
* In Section A.4, we introduce the qualitative and quantitative comparison of the caption quality between our ShareCaptioner-Video and other methods.
* In Section A.5, we present more statics of the ShareGPT4Video dataset.
* In Section A.6, we provide the detailed pseudo code of semantic-based data filtering and semantic-aware key-frame extraction.

### Experimental Details

**Training details of ShareGPT4Video-8B.** During training, we employ a batch size of 128 and the AdamW optimizer. We opt to fine-tune the entire model, setting the learning rate for the vision encoder at 2e-6, for the MLP projector at 2e-5, and for the LLM using LoRA [24], the learning rate is set at 2e-4. Such training strategy enables us to obtain an exceptional LVLM, ShareGPT4Video-8B, with 8 A100 GPUs in about 5 hours.

**Implementation details for T2VMs.** We utilize the Latte-XL [49] model with pre-trained weights and a text encoder from T5-XXL [60]. In the first stage, we perform pretraining on a lower number of frames and enable joint image-video training, with the image batch size being four times that of the video. This stage involved 50k training steps. In the second stage, both the video and image batch sizes were reduced to 2. For all training stages, we used AdamW optimizer with a constant learning rate of 2e-5, and the resolution for both images and videos was set to 512x512. For training precision, we used Bf16 since fp16 led to loss becoming NaN. The first stage requires around 6K H100 GPU hours, and the second stage requires around 36K Ascend GPU hours.

### Hierarchical Prompt Design.

We introduce this design to help the multi-modal and language models effectively perform their roles during the captioning process. We separately illustrate the differential-caption prompt and summary prompt in Figure 7 and Figure 8. Hierarchical prompts primarily consist of four components. The Character part provides the model with an overall perception of the role it is to play and the work environment it faces. The Skills section specifies the skills the model needs to possess, ensuring precise compliance with multiple requirements without omissions or confusion. The Constraints section clarifies behaviors that users do not desire and the rules to be followed when constructing outputs. The Structured Input section requires users to set up according to their specific scenarios. For example, when guiding the model to generate differential captions, the Character part informs the model that it is an expert in analyzing video frames. The Skills section requires the model to describe inter-frame target actions and behaviors, changes in environment and background, alterations in target appearance attributes, and camera movements reflecting temporal changes. The Constraints section demands precise descriptions without listing them item by item. In the Structured Input section, the input consists of frame indexes, timestamps, the previous frame, and its differential caption, among others.
* # Character You are an excellent video frame analyst. Utilizing your incredible attention to detail, you provide clear, sequential descriptions for video frames. You excel in identifying and conveying changes in actions, behaviors, environment, states and attributes of objects, and camera movements between adjacent video frames.
* # Skills
* Describe the action or behavior of objects within the frame.
- Notice and describe changes in the actions or behaviors between frames.
* Elaborate on environment and background alterations seen between frames.
* Describe the appearance of objects within the frame.
- Depict variations in the states and attributes of objects between frames.
* Perceive the camera's movements, such as panning or zooming.
- Convey these camera movements and describe how they impact the footage displayed.
* State facts objectively without using any rhetorical devices such as metaphors or personification.
- Stick to a narrative format for descriptions, avoiding list-like itemizations.
- Exclude sounds-related aspects, given the unavailability of audio signals.
- Descriptions should be fluent and precise, avoiding analyzing and waxing lyrical.
- Descriptions need to be concise, describing only the information that can be determined, without analysis or speculation.
- If there is only one inputted frame, that is, the first frame, describe the image details directly and do not concern yourself with connections between other frames.
- Do not mention the frame number and timestamp of the current frame.
* # Structured Input Video frame -idx, n-1- at -timestamp, n-1- Second(s) -frame n-1-sdiff_caption_n-1-s Video frame -idx, n- at -timestamp, n- Second(s) -frame n-s

Figure 8: Summary Caption Prompt Template

Figure 7: Differential Caption Prompt Template

### Comparison of Caption Quality

**Quantitative captioning capability comparison between ShareCaptioner-Video and GPT4V.** We first analyze the linguistic composition of the captions produced by GPT4V and ShareCaptioner-Video, and the results are presented in Table 6. The analysis reveals that the captions generated by our ShareCaptioner-Video contain a comparable information level to those generated by GPT4V. Furthermore, as shown in Table 7, we first generate 100 captions with GPT4V and ShareCaptioner-Video and then invite 10 volunteers to evaluate the captions based on three aspects. These aspects include (1) **Omission and Fabrication** - checking for no key elements missing in the caption and identifying imaged elements not present in the video; (2) **Distortion** - assessing the accuracy of element attributes such as color and size; and (3) **Temporal Mismatch** - evaluating whether the description accurately reflects the evolution of temporal events in the video. Each pair can earn a maximum of 3 points, one for each criterion met. As anticipated, our ShareCaptioner-Video performs on par with the GPT4V.

**Qualitative comparison of caption quality from various sources.** In Figure 9, we compare the quality of captions from different sources. The result which uses the Panda-Student [14] model learned from multiple LVLMs, generates one-sentence captions for videos. While this approach minimizes errors, it provides insufficient textual information for information-dense videos, hindering the alignment of video and language modalities. The Video ChatCaptioner [10], on the other hand, uses ChatGPT to construct questions, employs BLIP2 [34] for VQA, and then uses ChatGPT again to summarize a complete caption based on the QA. Although this method produces longer captions than Panda-Student, the extensive involvement of ChatGPT often leads to over-imagination and hallucinations, negatively impacting the alignment between video and language modalities. Thanks to our carefully designed pipeline for generating high-quality video captions, GPT4V demonstrated impressive captioning capabilities. Additionally, after fine-tuning on the ShareGPT4Video dataset, our ShareCaptioner-Video also exhibits captioning capabilities comparable to GPT-4V.

\begin{table}
\begin{tabular}{l|c c c c c c} \hline \hline Lexical Category & n. & adj. & adv. & v. & num. & prep. \\ \hline GPT4V & 27.5\% & 11.2\% & 2.0\% & 12.3\% & 0.3\% & 11.3\% \\ Share-Captioner & 28.1\% & 2.8\% & 1.5\% & 12.2\% & 0.5\% & 11.4\% \\ \hline \hline \end{tabular}
\end{table}
Table 6: **Comparison of lexical composition of the captions generated by GPT4V and ShareCaptioner-Video.**

\begin{table}
\begin{tabular}{l|c c c c} \hline \hline  & GPT4V & ShareCaptioner-Video & Comparable \\ \hline Percentage & 37.8\% & 36.5\% & 23.7\% \\ Avg. Score & 2.2 & 2.0 & - \\ \hline \hline \end{tabular}
\end{table}
Table 7: **Human preference on ShareCaptioner-Video vs. GPT4V over 100 validation samples and 10 volunteers.*** [15] Panda: The hotel room has a beautiful view of the eiffel tower and breakfast is set up on the table.
* [16] Video ChatCaptioner: In the video, a woman is sitting at a table indoors. She is alone at the table and there are no visible objects on the table. The woman appears to be sitting still and there is no visible movement outside the window. There are no other people in the room and no visible decoration or artwork on the walls. The room is well lit and there are no plants or flowers present.
* [17] 6PT4V: The video presents a progressively closer view of the Eiffel Tower from an interior perspective, beginning with a broad shot capturing the tower through a window or glass door, alongside a window will or table with various items including a glass pitcher or vase and some glasses.

As the video progresses, the focus narrows down on the tower's metallic lattice structure and observation decks, with the interior elements like window frames and curtains framing this iconic Parisian landmark consistently observed.

Then, camera's zoom out reveals more of the room, including a decorative wrought iron or balcony railing, suggesting the viewpoint is from an apartment or room overlooking the tower.

Further pulling back displays more of the room's interior, including a table adorned with glasses, deconters, and decorative pieces, hinting at a dining or living area setting, and a mirror on the wall, adding to the residential ambiance. And then the camera makes a brief pause.

Throughout these transitions, the overcast sky and framing of the Eiffel Tower remain constant, with subtle shifts in sunlight casting changing shadows and light patterns in the room, indicating a progression of time. The positioning of furniture and decorative items remains unchanged, maintaining a consistent interior setting against the backdrop of the iconic Eiffel Tower.

ShareCaptioner-Video: The video begins with a scene set in a well-lit room during the daytime, offering a picturesque view of the Eiffel Tower through an open window. The initial setting is coxy, featuring a breakfast nook with a dining table, chairs, and a teapot on a sideboard. The d decor is

hombely is homely, with a dark-colored curtain partially drawn to one side, allowing ample light into the space and highlighting the serene ambience created by the presence of the iconic landmark outside.

As the video progresses, the camera moves closer to the window, focusing more intently on the Eiffel Tower's intricate lattice structure. With this shift, the details of the room become less discernible, overshadowed by the magnified view of the Tower, which dominates the frame, drawing all attention to its architectural grandeur as seen through the window.

Subsequently, the camera adjusts to include more of the room within the frame, balancing the view of the Tower with the interior details. This broader perspective reveals a neatly arranged room, possibly a dining or sitting area, accented by elegant ddecor and a selection of items on a sideboard, all bathed in natural light that creates a warm atmosphere. The adjustment in camera positioning allows for a harmonious blend of the indoor setting with the external view, encapsulating the juxtaposition of architectural magnificence against the backdrop of everyday life.

The focus then shifts entirely to the interior, providing a detailed view of the room's layout and design. The Eiffel Tower, while still visible through the window, becomes a secondary element to the domestic setting...

Finally, the _scene remains consistent with the previous interior view, showing no significant changes in the arrangement or lighting of the room..._

Figure 9: A qualitative comparison of caption quality from various sources. Mistakes within the captions are highlighted in **red**, whereas detailed and accurate parts are emphasized in blue.

**Following the chopping, the man starts to arrange the ingredients on the cutting board, indicating a transition to combining the salad components. At this stage, he holds a large glass bowl filled with brightly colored tomatoes and prepped ingredients, ready for further action. Soon, he continuously tips the glass bowl to ensure the ingredients are evenly mixed.**

**The focus then shifts to preparing a salad dressing; he** pours olive oil into a smaller bowl and proceeds to add lemon juice, followed by scooping mustard into the mixture, then he shifts back to chopping, this time a shallot or red onion, next to the oil-mustard mixture. The scene broadens to show more of his environment and then zooms in as he starts to mix the dressing with another ingredient. He seasons the mixture with black pepper from a wooden grinder, sprinkles with a powder resembling salt, and then begins to whisk vigorously, aiming for an emulsified consistency.**

**Finally, he** adds croutons **from a metal bowl into the large glass bowl containing the salad. Throughout, the kitchen setting remains consistent, with the man's actions focused on crafting a vibrant and texturally diverse salad.**

Correct

Figure 10: The full video caption generated by our DiffSW, with correct temporal understanding and comprehensive detail description.

Figure 11: The video caption generated by GPT4V with multiple frames alongside timestamps as input. The GPT4V failed to understand the frames with the correct temporal order and outputs an incorrect caption.

[MISSING_PAGE_EMPTY:22]

### Pseudo Code

```
1#Input:video,-Videotobeprocessed
2#timeinterval,-Minimumtiminetervalbetweenkeyframes
3#threshold-Inter-framesimilaritythreshold
4#
5#Accommodatevideoswithdiversity
6
7#Accommodatevideos_pool.append(video) ```

Figure 14: Pseudo code of semantic-based data filtering.

Figure 15: Pseudo code for semantic-aware key-frame extraction.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? [Yes] 3. Did you discuss any potential negative societal impacts of your work? [Yes] 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [No] Computation limited. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes]
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] 2. Did you mention the license of the assets? [Yes] See supplementary material. 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] See supplementary material. 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [No] The data are open-sourced. 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [No]
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [NA] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [NA] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [NA]