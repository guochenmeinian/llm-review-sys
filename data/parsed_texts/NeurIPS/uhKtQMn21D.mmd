# Mechanic: A Learning Rate Tuner

 Ashok Cutkosky

Boston University

Boston, MA

ashok@cutkosky.com &Aaron Defazio

Meta, FAIR

New York, NY

adefazio@meta.com &Harsh Mehta

Google Research

Mountain View, CA

harshm@google.com

###### Abstract

We introduce a technique for tuning the learning rate scale factor of any base optimization algorithm and schedule automatically, which we call mechanic. Our method provides a practical realization of recent theoretical reductions for accomplishing a similar goal in online convex optimization. We rigorously evaluate mechanic on a range of large scale deep learning tasks with varying batch sizes, schedules, and base optimization algorithms. These experiments demonstrate that depending on the problem, mechanic either comes very close to, matches or even improves upon manual tuning of learning rates.

## 1 Introduction

Modern deep learning is driven by first-order stochastic optimization algorithms. These are algorithms that are designed to solve the classical stochastic optimization problem:

\[\min F(\mathbf{x})=\min\operatorname*{\mathbb{E}}_{\mathbf{z}}[f(\mathbf{x}, \mathbf{z})]\]

where \(\mathbf{z}\) is a minibatch of examples, \(\mathbf{x}\in\mathbb{R}^{d}\) is the model parameters, and \(f\) is the loss incurred by using weights \(\mathbf{x}\) on the minibatch \(\mathbf{z}\). A first-order algorithm follows the protocol:

1. Output a \(t\)th iterate \(\mathbf{x}_{t}\).
2. Sample a random minibatch \(\mathbf{z}_{t}\).
3. Compute \(\mathbf{g}_{t}=\nabla f(\mathbf{x}_{t},\mathbf{z}_{t})\) (the gradient is taken with respect to \(\mathbf{x}_{t}\) only).
4. Possibly update some internal algorithm state based upon \(\mathbf{g}_{t}\) in preparation for computing the next iterate \(\mathbf{x}_{t+1}\).

The prototypical such optimization algorithm is stochastic gradient descent (SGD), which exemplifies the attractive features of this approach: it is computationally cheap (running in \(O(d)\) time per update), and with proper tuning obtains minimax optimal convergence guarantees [1; 2]. Modern practice makes use of a wide range of variants of SGD, such SGD with momentum, AdaGrad [3], Adam [4], AdamW [5] or Lion [6]. Interest in such improvements to SGD is driven by the increasing computational demands of training large neural networks: better optimization means cheaper training, which translates to significant savings in terms of time, cost, and environmental impact.

Most modern algorithms for training neural networks are equipped with a scalar "scale factor" or "learning rate" hyperparameter \(s\in\mathbb{R}\). Roughly speaking, these algorithms produce iterates of the form \(\mathbf{x}_{t+1}=\mathbf{x}_{t}+s\cdot\mathbf{u}_{t}\) where \(\mathbf{u}_{t}\) is some _update vector_ produced as a function of the observed gradients \(\mathbf{g}_{1},\ldots,\mathbf{g}_{t}\) (we will use bold font for vectors in \(\mathbb{R}^{d}\) like \(\mathbf{u}\) and normal font for all other quantities like \(s\)). As an example, the classical SGD algorithm sets \(\mathbf{u}_{t}=-\eta_{t}\mathbf{g}_{t}\) for some sequence of scalars \(\{\eta_{t}\}\) typically called the _schedule_. The formula for the SGD update is:

\[\mathbf{x}_{t+1}=\mathbf{x}_{1}-s\cdot\sum_{i=1}^{t}\eta_{i}\mathbf{g}_{i}.\] (1)The process of selecting the optimal \(s\) is called "tuning", and is a key resource sink in machine learning. The typical approach is simply to try many possibilities to find the empirically optimal \(s\), which requires multiple expensive training runs. This paper introduces a technique for choosing \(s\) automatically on-the-fly in order to avoid this expense.

Our procedure, which we call mechanic, is a generic wrapper around any base optimization algorithm (base) that produces a new optimizer which does not require tuning of the scalar \(s\). The base optimization algorithm is allowed to make any kind of update (for example, it may use any kind of schedule, preconditioner or weight decay). If \(\mathbf{x}_{t}^{\textsc{base}}\in\mathbb{R}^{d}\) is the \(t\)th iterate of base, then the wrapper will produce a scalar \(s_{t}\in\mathbb{R}\) and set the \(t\)th iterate of the wrapped algorithm to be \(\mathbf{x}_{t}=\mathbf{x}_{1}^{\textsc{base}}+s_{t}(\mathbf{x}_{t}^{\textsc{ base}}-\mathbf{x}_{1}^{\textsc{base}})\). As an example, suppose that base is the classical SGD algorithm with update equation (1). Then, given \(s_{t}\), we would set \(\mathbf{x}_{t}=\mathbf{x}_{1}^{\textsc{base}}-s_{t}\sum_{i=1}^{t-1}\eta_{i} \mathbf{g}_{i}\). Disregarding for now the fact that the gradients \(\mathbf{g}_{i}\) actually depend on the iterates \(\mathbf{x}_{i}\)1, we see that \(\mathbf{x}_{t}\) is what the \(t\)th iterate of SGD _would have been_ if the schedule were scaled by \(s_{t}\).

Footnote 1: This seems like a significant issue to disregard, but we will provide mathematical justification shortly.

Removing tuning of learning rate scalars is already a well-studied problem. One of the main attractions of early work in "adaptive" optimization such as AdaGrad and Adam [3; 7; 4] is that these algorithms require less tuning than ordinary SGD. Over the last decade, a number of works have aimed to tackle this problem from both an empirical and theoretical perspective [8; 9; 10; 11; 12; 13; 14; 15; 16; 17; 18; 19]. An intuitive approach might take the route of "hy-pergradient descent": that is, differentiating the update step of the optimization algorithm itself (e.g. [8; 9]). Strangely, it appears to be difficult to prove that such schemes behave well: theory-based approaches often adopt rather different strategies. Instead, we start from known theory and propose a few important modifications to produce a simple and effective practical implementation. We then rigorously evaluate our algorithm on a variety of datasets. We emphasize that our primary contribution is not new theoretical development, but instead the translation between theory and practice, which involves fusing several known analytical techniques as well as subtle departures from theory.

Previous works that investigate deep learning performance of "learning-rate free" optimization inspired by theory (e.g. [20; 21; 16; 15]) have already demonstrated impressive results. However, these works typically build "hand-crafted" algorithms that often blend theoretical analysis with specific empirically successful algorithms such as Adam. In contrast, our wrapper works well with any base algorithm and so can seamlessly integrate new empirical advances in optimization: one does not need intimate familiarity with the analysis of our approach to apply it to a new algorithm.

## 2 Background: Online Convex Optimization

We develop our formalism via _online convex optimization_ (OCO) [22; 23; 24]. OCO is a popular framework for design and analysis of stochastic optimization algorithms. In brief, for each of \(T\) rounds (corresponding to \(T\) iterations of optimization), the OCO algorithm must first output a \(t\)th iterate \(\mathbf{x}_{t}\), after which the algorithm is presented with a \(t\)th loss function \(\ell_{t}\). Typically, one envisions the case \(\ell_{t}(\mathbf{x})=\ell(\mathbf{x},\mathbf{z}_{t})\) for some fixed loss \(\ell\) and new data point \(\mathbf{z}_{t}\). The goal of an algorithm alg is to minimize the _regret_\(R^{\textsc{alg}}(\hat{\mathbf{x}})\):

\[R^{\textsc{alg}}(\hat{\mathbf{x}})\triangleq\sum_{t=1}^{T}\ell_{t}(\mathbf{x} _{t})-\ell_{t}(\hat{\mathbf{x}}).\]

Many references focus primarily on the case \(\hat{\mathbf{x}}=\text{argmin}\sum_{t=1}^{T}\ell_{t}(\mathbf{x})\) in order to consider the single scalar value \(\sup_{\hat{\mathbf{x}}}R_{T}(\hat{\mathbf{x}})\)[25; 26], but we will employ the formulation of regret as a function above instead as it is strictly more general. When \(\ell_{t}\) is convex, then with \(\mathbf{g}_{t}\triangleq\nabla\ell_{t}(\mathbf{x}_{t})\) (or, more generally when \(\mathbf{g}_{t}\) is a subgradient of \(\ell_{t}\) at \(\mathbf{x}_{t}\)), we have:

\[R^{\textsc{alg}}(\hat{\mathbf{x}})\leq\sum_{t=1}^{T}\langle\mathbf{g}_{t}, \mathbf{x}_{t}-\hat{\mathbf{x}}\rangle\triangleq R^{\textsc{alg}}_{\text{ linear}}(\hat{\mathbf{x}}).\]

As a result, the vast majority of OCO algorithms provide analysis that bounds only the linearized regret \(R^{\textsc{alg}}_{\text{linear}}(\hat{\mathbf{x}})\). Such algorithms do not need to observe the entire function \(\ell_{t}\): instead, they only make use of the gradients \(\mathbf{g}_{t}\). That is, the \(t\)th output of alg (i.e. \(\mathbf{x}_{t}\)) is purely a function of the previous sequence of gradients \(\mathbf{g}_{1},\ldots,\mathbf{g}_{t-1}\) so that alg is a first-order algorithm.

### Learning the Scale in OCO

Just like stochastic optimization algorithms, most OCO algorithms also require a scale factor \(s\). In fact, many stochastic optimization algorithms (such as SGD and AdaGrad) are _also_ OCO algorithms. Setting \(\eta_{t}=\eta\) for all \(t\), SGD ensures the regret bound:

\[R^{\text{SGD}}(\hat{\mathbf{x}})\leq R^{\text{SGD}}_{\text{linear}}(\hat{ \mathbf{x}})\leq O\left(\frac{\|\hat{\mathbf{x}}-\mathbf{x}_{1}\|^{2}}{s\eta} +s\eta\sum_{t=1}^{T}\|\mathbf{g}_{t}\|^{2}\right).\] (2)

From this equation, one can deduce in hindsight that for any given \(\hat{\mathbf{x}}\), the optimal value for \(s\) is \(\frac{\|\hat{\mathbf{x}}-\mathbf{x}_{1}\|}{\eta\sqrt{\sum_{t=1}^{T}\|\mathbf{g }_{t}\|^{2}}}\), which would provide the bound:

\[R^{\text{SGD with tuned }s}_{\text{linear}}(\hat{\mathbf{x}})\leq O\left(\| \hat{\mathbf{x}}-\mathbf{x}_{1}\|\sqrt{\sum_{t=1}^{T}\|\mathbf{g}_{t}\|^{2}} \right).\] (3)

This result is minimax optimal [27], but requires knowledge of the unknown optimal \(s\). Very recently, [14, 16, 15] have produced algorithms that estimate the value of \(\|\mathbf{x}_{1}^{\text{base}}-\hat{\mathbf{x}}\|\) on-the-fly and use this estimate to quickly identify the optimal scaling value \(s\). These algorithms achieve impressive practical performance, but they require an understanding of the closed-form solution for the optimal \(s\) value above. Our goal is to learn the correct scaling regardless of the base algorithm.

To this end, we will leverage a scheme recently developed by [28] that allows one to automatically tune the scale of a base OCO algorithm using another "meta" OCO algorithm. We reproduce their result below (with notation altered to suit our application) along with the short proof:

**Theorem 1** ([28]).: _Suppose base and tuner are both OCO algorithms. Let \((\mathbf{x}_{t}^{\text{base}})\subset\mathbb{R}^{d}\) indicate the iterates of base in response to an arbitrary sequence of gradients \(\{\mathbf{g}_{t}\}\), and let \(\{s_{t}\}\subset\mathbb{R}\) indicate the iterates of tuner in response to the sequence of scalars \(\{h_{t}=\langle\mathbf{g}_{t},\mathbf{x}_{t}^{\text{base}}-\mathbf{x}_{1}\rangle\}\). Define a new online algorithm mechanic via:_

\[\mathbf{x}_{t}^{\text{mechanic}}=\mathbf{x}_{1}^{\text{base}}+s_{t}\cdot( \mathbf{x}_{t}^{\text{base}}-\mathbf{x}_{1}^{\text{base}}).\]

_Then \(\mathbf{x}_{t}^{\text{mechanic}}\) ensures regret:_

\[R^{\text{mechanic}}_{\text{linear}}(\hat{\mathbf{x}})\leq\inf_{\hat{s}}R^{ \text{tuner}}_{\text{linear}}(\hat{s})+\hat{s}R^{\text{base}}_{\text{linear}} ((\hat{\mathbf{x}}-\mathbf{x}_{1}^{\text{base}})/\hat{s}).\]

Proof.: By definition, for any \(\hat{s}\), we have:

\[R^{\text{mechanic}}_{\text{linear}}(\hat{\mathbf{x}}) =\sum_{t=1}^{T}\langle\mathbf{g}_{t},\mathbf{x}_{1}^{\text{base} }+s_{t}\cdot(\mathbf{x}_{t}^{\text{base}}-\mathbf{x}_{1}^{\text{base}})-\hat{ \mathbf{x}}\rangle\] \[=\sum_{t=1}^{T}\langle\mathbf{g}_{t},\mathbf{x}_{t}^{\text{base} }-\mathbf{x}_{1}^{\text{base}}\rangle(s_{t}-\hat{s})+\hat{s}\sum_{t=1}^{T} \langle\mathbf{g}_{t},\mathbf{x}_{t}^{\text{base}}-\mathbf{x}_{1}^{\text{base }}-(\hat{\mathbf{x}}-\mathbf{x}_{1}^{\text{base}})/\hat{s}\rangle\] \[=R^{\text{tuner}}_{\text{linear}}(\hat{s})+\hat{s}R^{\text{base} }_{\text{linear}}(\mathbf{x}_{1}^{\text{base}}+(\hat{\mathbf{x}}-\mathbf{x}_ {1}^{\text{base}})/\hat{s}).\]

With this result, the job of finding the optimal \(s\) can usually be completely relegated to tuner. Although the value \(\hat{s}\) appears in both terms of the sum \(R^{\text{tuner}}_{\text{linear}}(\hat{s})+\hat{s}R^{\text{base}}_{\text{linear}} (\mathbf{x}_{1}^{\text{base}}+(\hat{\mathbf{x}}-\mathbf{x}_{1}^{\text{base} })/\hat{s})\), it turns out that for essentially all plausible base algorithms, there is a particular value \(\hat{s}\) that causes \(\hat{s}R^{\text{base}}_{\text{linear}}(\mathbf{x}_{1}^{\text{base}}+(\hat{ \mathbf{x}}-\mathbf{x}_{1}^{\text{base}})/\hat{s})\) to obtain the optimal regret bound (3). Thus, by setting \(\hat{s}\) to be this value, which is unknown _a priori_, we need only ensure that \(R^{\text{tuner}}_{\text{linear}}(\hat{s})\) is small enough to not significantly affect the overall regret bound. Note that this setting of \(\hat{s}\) is done entirely in the analysis. For example, if base is actually SGD with a learning rate \(\eta\) and \(s=1\) as in (2), we have

\[R^{\text{mechanic}}(\hat{\mathbf{x}})\leq R^{\text{mechanic}}_{\text{linear}}( \hat{\mathbf{x}})\leq\inf_{\hat{s}}R^{\text{tuner}}_{\text{linear}}(\hat{s})+O \left(\frac{\|\hat{\mathbf{x}}-\mathbf{x}_{1}\|^{2}}{\hat{s}\eta}+\hat{s}\eta \sum_{t=1}^{T}\|\mathbf{g}_{t}\|^{2}\right),\]setting \(\hat{s}=\frac{\|\hat{\mathbf{x}}-\mathbf{x}_{1}\|}{\eta\sqrt{\sum_{t=1}^{T}\| \mathbf{g}_{t}\|^{2}}}\):

\[\leq R_{\text{linear}}^{\text{tuner}}\left(\frac{\|\hat{\mathbf{x}}-\mathbf{x} _{1}\|}{\eta\sqrt{\sum_{t=1}^{T}\|\mathbf{g}_{t}\|^{2}}}\right)+O\left(\|\hat{ \mathbf{x}}-\mathbf{x}_{1}\|\sqrt{\sum_{t=1}^{T}\|\mathbf{g}_{t}\|^{2}}\right).\]

Thus, if tuner obtains low regret, then we will obtain the same regret bound as if we had optimally tuned the scaling factor for SGD. Intuitively, the gradient \(h_{t}\) provided to tuner approximates the gradient over the entire course of the base optimizer rather than just at the most recent iterate. That is, for SGD, \(h_{t}\approx\frac{df(\mathbf{x}_{t},\mathbf{x}_{t})}{ds}\) where \(\mathbf{x}_{t}=\mathbf{x}_{1}-s\sum_{k=1}^{t-1}\eta_{k}\mathbf{g}_{k}\).

### Parameter-Free Online Optimization

The problem with the above result is that we seem to have simply pushed the problem off to tuner: what if tuner itself requires us to set a scale factor? Solving this problem has been the focus of a substantial effort in the online optimization community [29; 30; 10; 11; 28; 12]. The most advanced such algorithms are able to ensure for all \(\hat{s}\) simultaneously:

\[R_{\text{linear}}(\hat{s})=\sum_{t=1}^{T}h_{t}(s_{t}-\hat{s})\leq\tilde{O} \left(|\hat{s}|\sqrt{\sum_{t=1}^{T}h_{t}^{2}}\right).\] (4)

Thus, if we set \(h_{t}=\langle\mathbf{g}_{t},\mathbf{x}_{t}^{\text{BASE}}-\mathbf{x}_{1}^{ \text{BASE}}\rangle\), we obtain:

\[R_{\text{linear}}^{\text{tuner}}\left(\hat{s}\right)\leq\tilde{O}\left(|\hat {s}|\sqrt{\sum_{t=1}^{T}\langle\mathbf{g}_{t},\mathbf{x}_{t}^{\text{BASE}}- \mathbf{x}_{1}^{\text{BASE}}\rangle^{2}}\right).\]

In a theoretical development of this technique, it is necessary to prevent the terms \(\langle\mathbf{g}_{t},\mathbf{x}_{t}^{\text{BASE}}-\mathbf{x}_{1}^{\text{ BASE}}\rangle^{2}\) from becoming too large (as otherwise \(R_{\text{linear}}^{\text{tuner}}\) is too large). Typically, this is accomplished by constraining the base algorithm to satisfy \(\|\mathbf{x}_{t}^{\text{BASE}}-\mathbf{x}_{1}^{\text{BASE}}\|\leq\rho\) for some user-specified arbitrary \(\rho\). Enforcing such a constraint means that the regret bound (2) would only apply to \(\|\hat{\mathbf{x}}\|\leq\rho\), but ensures that \(\langle\mathbf{g}_{t},\mathbf{x}_{t}^{\text{BASE}}-\mathbf{x}_{1}^{\text{ BASE}}\rangle^{2}\leq\rho^{2}\|\mathbf{g}_{t}\|^{2}\). Thus, by setting \(\hat{s}=\|\hat{\mathbf{x}}-\mathbf{x}_{1}^{\text{BASE}}\|/\rho\), the combined algorithm obtains the optimal regret bound of \(O(\|\hat{\mathbf{x}}-\mathbf{x}_{1}^{\text{BASE}}\|\sqrt{\sum_{t=1}^{T}\| \mathbf{g}_{t}\|^{2}})\) (amazingly, the value of \(\rho\) is irrelevant!). In practice however, we do not attempt to explicitly enforce any such constraints and simply rely on the intuition that any non-diverging algorithm is unlikely to produce excessively large iterates.

At no point in this process do we need access to the internal state of the base algorithm base. This means that improvements to base will automatically be reflected in improvements to the overall algorithm. In this paper, we investigate the performance of mechanic on deep learning tasks. We consider a variety of settings for the base algorithm base (i.e. AdamW, Lion, SGD, with various batch sizes and learning rate schedules of various shapes), and employ a parameter-free algorithm as the tuner to automatically find the best scale factor for the base algorithm.

## 3 The mechanic algorithm

Our mechanic algorithm is specified in Algorithm 1. The algorithm is built by applying Theorem 1 to a parameter-free tuner algorithm presented in Algorithm 2, which is described along with theoretical analysis in Appendix D. However, when building mechanic, we modify the "pure" theoretically tractable Algorithm 2 to simplify the implementation while still capturing the essential intuition and maintaining the same performance. In the remainder of this section we will provide some intuition behind the tuner update as used in mechanic as well as describing some potentially unfamiliar subtleties relating to our use of exponentially weighted moving averages.

mechanic takes as input a base algorithm that generates _update vectors_\(\mathbf{u}_{t}\) as described in the previous sections. We then set \(\mathbf{\Delta}_{t+1}=\sum_{k=1}^{t}\mathbf{u}_{k}=\mathbf{x}_{t+1}^{\text{ BASE}}-\mathbf{x}_{1}^{\text{BASE}}\). The majority of the algorithm contains our tuner method, which is a variant of the analytically tractable Algorithm 2, with a few modifications. Note that the indexing on \(\bm{\Delta}\) is very important and may be counterintuitive: the definition of \(h_{t}\) does _not_ include \(\bm{\Delta}_{t+1}\), but rather \(\bm{\Delta}_{t}\). \(h_{t}\) is the "gradient" that is supplied to tuner, as described by Theorem 1.

To gain some intuition behind the update, let us consider the case that \(n=1\) and \(\beta=1.0\) (that is, without employing any exponentially-weighted moving averages). We keep track of the quantity \(W_{t}=s_{init}\cdot m_{t}-\sum_{k=1}^{t}h_{k}s_{k}\), which is usually called the "wealth" of the algorithm (the quantity \(r_{t}=-\sum_{k=1}^{t}h_{k}s_{k}\) is sometimes called the "reward"). \(s_{init}\) specifies the starting value for \(s_{t}\) and should be an under-estimate of the true optimal scaling. We then set \(s_{t+1}=\frac{W_{t}}{\sqrt{v_{t}}}\) (neglecting the \(\epsilon\) included for numerical stability). To understand this update strategy, we can re-write the update as:

\[s_{t+1}=s_{t}\cdot\frac{\sqrt{v_{t-1}}}{\sqrt{v_{t}}}-\frac{s_{t}h_{t}}{\sqrt {v_{t}}}\approx\left(1-\frac{h_{t}^{2}}{2v_{t}}\right)s_{t}-\frac{s_{t}h_{t}}{ \sqrt{v_{t}}}.\]

Thus, the update looks like a combination of an AdaGrad-esque gradient descent step with learning rate scaled by \(s_{t}\) and a kind of "adaptive decay" (multiplication by \(1-\frac{h_{t}^{2}}{2v_{t}}\)). The adaptive decay is very important for stabilizing the algorithm: without it the values for \(s_{t}\) are prone to unchecked exponential growth due to scaling by \(s_{t}\) in \(\frac{s_{t}h_{t}}{\sqrt{v_{t}}}\). Intuitively, this decay is the minimum amount required to prevent instabilities.

In Appendix D, we provide a formal Theorem bounding the regret of a variant of the procedure described above. Roughly speaking, for \(\beta=1\) this result suggests:

\[\sum_{t=1}^{T}h_{t}(s_{t}-\dot{s})\leq O\left((\dot{s}+\max_{t}s_{t})\cdot m_{ T}+\dot{s}\cdot\log(T\dot{s}/s_{init})\sqrt{\sum_{t=1}^{T}h_{t}^{2}}\right).\] (5)

In fact, the dependence of \(O(\log(T))\) in equation (5) can be improved to \(O(\sqrt{\log(T)})\) via more complicated algorithms (e.g. [28, 12, 31, 32]). However, we favor the simpler update and pleasing resemblance to familiar algorithms like AdaGrad via the Taylor expansion analysis above. Of note, the dependence on \(s_{init}\) is very mild: this suggests that we should be able to set \(s_{init}\) to a very small value without damaging performance. In practice, we choose \(s_{init}=10^{-8}\), which we expect to dramatically underestimate the optimal value in all scenarios.

We hypothesize that the simplified tuner we use in mechanic in fact possesses a rigorous theoretical analysis (although perhaps only with respect to simpler non-fully-worst-case adversaries), but demonstrating such a bound appears to involve difficult technical hurdles. In particular, our implementation is designed to be "scale-free": rescaling the values of \(\mathbf{g}_{t}\) by any constant scalar will have no effect on \(s_{t}\). This property was first achieved only recently in theoretical analysis of parameter-free algorithms [12], and as-yet requires significantly more involved algorithms [12; 33].

### The use of \(\beta\)

We include \(\beta\) to introduce some recency bias in the statistics recorded by mechanic, a common feature of practical optimizers. Mathematically, we accomplish this by up-weighting the _t_th feedback to tuner by \(\beta^{-t}\): \(h_{t}\to h_{t}\beta^{-t}\). Thus, for example, we have \(\mathbf{v}_{t}=\sum_{k=1}^{t}h_{k}^{2}\beta^{-2kt}\) and \(r_{t}=-\sum_{k=1}^{t}h_{k}s_{k-1}\beta_{s}^{-k}\). Using these weights directly results in numerical stability issues as the weights become exponentially large. Instead, since we only need to maintain the correct ratio \(\frac{W_{t}}{\sqrt{v_{t}}}\), we can cancel a factor of \(\beta_{s}^{-t}\) from both sides, giving the update equations in Algorithm 2.

We found that tuning the value of \(\beta\) can significantly improve performance on different tasks. Thus, we incorporated multiple \(\beta\) values simultaneously in a way that obviates the need for such tuning.

Our approach is inspired by work on "combining" parameter free algorithms [34]. The idea is simple: parameter-free algorithms typically ensure \(R_{\text{linear}}(0)\leq\epsilon\) for some _constant_\(\epsilon\) set by the user. So, if \(s_{t,1},\ldots,s_{t,n}\) are the outputs of \(n\) parameter-free algorithms with regret bounds \(R_{\text{linear}}^{1}(\hat{s}),\ldots,R_{\text{linear}}^{n}(\hat{s})\), we have for any \(j\):

\[\sum_{t=1}^{T}h_{t}\left(\sum_{i=1}^{n}s_{t,i}-\hat{s}\right) =\sum_{t=1}^{T}h_{t}(s_{t,j}-\hat{s})+\sum_{i\neq j}\sum_{t=1}^{T }h_{t}(s_{t,i}-0)\] \[=R_{\text{linear}}^{j}(\hat{s})+\sum_{i\neq j}R_{\text{linear}}^ {i}(0)\leq R_{\text{linear}}^{j}(\hat{s})+(n-1)\epsilon.\]

So, with small constant additive overhead in the regret, the sum of all the outputs \(s_{t,1}+\cdots+s_{t,n}\) achieves the same regret as the _best_ of all the outputs. Motivated by this observation, we instantiate \(n=6\) copies of tuner with different \(\beta\) values and add their iterates to produce a final scaling.

### Weight decay

Finally, we found that an addition of a peculiar weight-decay-esque term helped significantly on certain tasks, including vision tasks with smaller datasets, multi-objective NLP tasks and especially with reducing the variance in final results for all tasks. Specifically, rather than providing \(h_{t}=\langle\mathbf{g}_{t},\boldsymbol{\Delta}_{t}\rangle\) as the input to the tuner algorithm, we instead provide \(h_{t}=\langle\mathbf{g}_{t}+\frac{\lambda\|\mathbf{g}_{t}\|\left(\sum_{i=1}^{ n}s_{t,i}\right)\mathbf{x}_{t}}{\|\mathbf{x}_{t}\|},\boldsymbol{\Delta}_{t}\rangle\). We conjucture that this term is helpful in the common case that the base algorithm itself is incorporating regularization or weight-decay.

This extra term is the derivative of the regularizer \(\mathbf{x}\mapsto\lambda\|\mathbf{g}_{t}\|\left(\sum_{i=1}^{n}s_{t,i}\right)\| \mathbf{x}\|\). From a standard theoretical perspective, this regularization may seem overly large. However, it may not have as big an impact as one might imagine because the base algorithm does not see this regularization. Instead, the base algorithm may (or may not) perform weight decay using another method that mechanic has no insight into. That said, we do not propose an analytical explanation for this modification. We simply observed that in practice it performed well with a fixed \(\lambda=0.01\).

### Runtime and Memory Cost

mechanic incurs little additional cost over that of base. In Algorithm 1, we denote \(d\)-dimensional vectors with bold font, and \(n\)-dimensional vectors and scalars with normal font (note that typically \(n=6\)). We use 1 additional \(O(d)\) memory slot, and four \(O(d)\)-time steps in lines 8, 9, 10 and 17. All other steps are \(O(1)\) or \(O(n)\) time and so have negligible cost.

## 4 Experiments

In this section we describe our experiments using mechanic to tune various base optimizers on different tasks. Note that almost all base optimizer implementations require a user-specified scale factor which is is not directly visible to mechanic. We set this value to 1.0 before applying mechanic. Since mechanic multiplies the base update by \(s_{t}\), setting the base scale factor to \(1.0\) allows us to interpret \(s_{t}\) as the "correct" value for the base scale.

### Masked Language Modeling

We perform BERT pre-training on the Wikibooks dataset following the procedure from [35] with a few minor changes, most notably, we omit the Next Sentence Prediction (NSP) loss following [36]. Masked language modeling (MLM) requires reconstructing randomly masked tokens given an input sequence of tokens. As shown in Table 1, using mechanic leads to a noticeable improvement in MLM accuracy.

**Varying batch size and model size:** Past works observe that the scale factor \(s\) should decrease as either batch size is decreased or model size is increased [37, 38]. To inspect the scale factor that mechanic learns, we vary the batch size and model size while pre-training BERT using mechanic. As shown in Figure 1, in both cases, mechanic learns to decrease the scale factor \(s\) when decreasing the batch size and when increasing the model size.

**Addition Ablations:** Ablation studies on the effects of \(n\), \(\lambda\), \(s_{init}\) can be found in Appendix B.

**Finetuning pre-trained models:** In addition to pre-training, we evaluate our models on the 5 largest datasets from the GLUE suite [39]. One possible failure mode of mechanic tuned pre-trained models could have been that, even though they lead to high accuracy at pre-training time, transfer learning may fail at finetuning time.

To ensure that standard transfer learning pipelines still work with mechanic pre-trained models, we finetune them without a learning rate tuner using the AdamW optimizer and find that mechanic pre-trained models lead to higher accuracy at pre-training time, and they also outperform in finetuning more often than not. We finetune BERT-B (110M) and BERT-L (340M) models for at most 10 epochs on each of the GLUE datasets and report results on the GLUE dev set in Table 1.

**Using mechanic for finetuning:** We also investigated using mechanic for finetuning. Typically, to not erase the progress already made, a much lower base learning rate is employed at finetuning

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline Model & Size & Pre Opt & MLM & Optimizer & MNLI-m/mm & QNLI & SST-2 & QQP \\ \hline \multirow{3}{*}{BERT-B} & \multirow{3}{*}{110M} & AdamW & 71.5 & \begin{tabular}{c} AdamW \\ \(\mathcal{M}\)-AdamW \\ \end{tabular} & 84.3/84.8 & 91.0 & 92.4 & 90.1 \\  & & \multirow{3}{*}{\(\mathcal{M}\)-AdamW} & **71.7** & \begin{tabular}{c} AdamW \\ \(\mathcal{M}\)-AdamW \\ \end{tabular} & 83.7/83.5 & 90.6 & 91.9 & 90.5 \\  & & & & & & \\ \cline{3-8}  & & & & & & \\ \hline \multirow{3}{*}{BERT-B} & \multirow{3}{*}{110M} & Lion & 71.8 & \begin{tabular}{c} Lion \\ \(\mathcal{M}\)-Lion \\ \end{tabular} & 83.4/83.5 & 86.8 & 89.7 & 89.4 \\  & & & & & & \\ \cline{3-8}  & & & & & & \\ \cline{3-8}  & & & & & & \\ \hline \multirow{3}{*}{BERT-L} & \multirow{3}{*}{340M} & AdamW & **75.4** & \begin{tabular}{c} AdamW \\ \(\mathcal{M}\)-AdamW \\ \end{tabular} & 86.2/86.4 & 92.2 & 93.9 & 91.3 \\  & & & & & & & \\ \cline{3-8}  & & & & & & \\ \cline{3-8}  & & & & & & \\ \hline \multirow{3}{*}{BERT-L} & \multirow{3}{*}{340M} & Lion & **75.7** & 
\begin{tabular}{c} Lion \\ \(\mathcal{M}\)-Lion \\ \end{tabular} & 86.7/86.6 & 90.7 & 92.9 & 91.1 \\  & & & & & & \\ \cline{3-8}  & & & & & & \\ \cline{3-8}  & & & & & & \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparing mechanic on BERT. 5 largest datasets from GLUE. Results reported are peak validation scores averaged over 3 runs, both for the baseline and mechanic tuned models.

time. This could easily have been a potential failure mode of any kind of automatic learning rate tuner as such strategies might "explore" a high learning rate at the beginning of the optimization procedure. Fortunately, we observed that this inductive bias typically baked at finetuning time is still maintained when using mechanic.

### Image Classification

In this Section, we present results on popular Image Classification tasks. Apart from training from scratch, we also perform transfer learning experiments where we pre-train on the JFT-300M [40] dataset and finetune on ImageNet, Cifar-10 and Cifar-100 datasets. We follow the exact setting employed in [41] for both pre-training and finetuning.

As shown in Table 2, mechanic is quite competitive across the board and produces results either very close to the baseline or better. Since mechanic optimizes for the train loss, in general, we observe that it results in better **test** performance on tasks with large amounts of data where the model is unable to overfit to the train set. For instance, we see that mechanic beats the baseline substantially when pre-training ViT models on JFT-300M, whereas it lags slightly behind on smaller datasets like ImageNet-1k or CIFAR-10/100. Even though we fix \(\lambda\) to 0.01 as default for all our reported experiments, we find that for small datasets like CIFAR-10, increasing it led to better test performance.

### Comparison with D-adaptation

Recently, [16] introduced the D-adaptation algorithm, with the same goal of learning the correct scale \(s\) for SGD and Adam base optimizers. D-adaptation showed impressive empirical results on a range of popular deep learning tasks, so we compare mechanic with D-adaptation on a selection of tasks that D-adaptation worked well on, using code provided by the authors. Hyper-parameter settings were kept the same to ensure a fair comparison. In contrast to D-adaptation, mechanic does not require modification for different base optimizers and, as shown in Figure 2, it remains quite

Figure 1: Scaling values \(s\) learned by mechanic while varying batch size and model size.

competitive on small datasets like CIFAR-10/100 while outperforming both a manually tuned baseline and D-adaptation on bigger tasks like IWSLT14 and language modeling on BookWiki dataset. We present additional results in Appendix C.3, including a comparison on a suite of 12 logistic regression problems.

## 5 Conclusion

mechanic is a new technique for scaling the updates of any base optimization algorithm. Our approach provides a practical implementation of recent developments in optimization theory, and is able to match the performance of tuned baselines on large-scale machine learning tasks. This work suggests several natural future directions. First, is there a theoretical motivation for our weight-decay term? Next, is it possible to leverage similar techniques to learn a _per-layer_ scale factor? Such a capacity would not significantly increase computation cost, but by allowing more degrees of freedom may yield a method that significantly outperforms baselines since it is infeasible to manually tune a scale factor for every layer.

#### Acknowledgments

Ashok Cutkosky acknowledges funding support from NSF grant CCF-2211718, an Amazon research award, and Google.

## References

* [1] Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. _SIAM Journal on Optimization_, 23(4):2341-2368, 2013.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline Model & Size & Pre Opt & Pre Acc & Optimizer & I1K & Cifar100 & Cifar10 \\ \hline \multicolumn{8}{c}{CNN from scratch on CIFAR datasets} \\ \hline \multirow{3}{*}{ResNet-18} & \multirow{3}{*}{11M} & - & - & Mom & - & **77.6** & **95.4** \\  & & - & - & \(\mathcal{M}\)-Mom & - & 75.3 & 94.1 \\  & & - & - & \(\mathcal{M}\)-Mom (\(\lambda=0.1\)) & - & 76.6 & 95.3 \\ \hline WRN-40-10 & \multirow{3}{*}{56M} & - & - & Mom & - & **79.9** & - \\  & & - & - & \(\mathcal{M}\)-Mom & - & 79.6 & - \\ \hline \multicolumn{8}{c}{Pre-train on JFT-300M} \\ \hline \multirow{3}{*}{ViT-B/16} & \multirow{3}{*}{86M} & AdamW & 48.5 & Mom & 84.7 & **91.9** & 99.1 \\  & & & \(\mathcal{M}\)-Mom & **84.7** & 90.7 & 99.1 \\  & & \(\mathcal{M}\)-AdamW & **49.9** & Mom & 84.2 & 91.5 & 99.1 \\  & & & \(\mathcal{M}\)-Mom & 84.1 & 90.3 & **99.1** \\ \hline \multirow{3}{*}{ViT-B/16} & \multirow{3}{*}{86M} & Lion & 47.0 & Mom & **85.3** & 92.1 & 99.2 \\  & & & \(\mathcal{M}\)-Mom & 85.2 & 91.0 & 99.2 \\ \cline{2-7}  & & \(\mathcal{M}\)-Lion & **49.6** & Mom & 84.7 & **92.3** & **99.2** \\  & & & \(\mathcal{M}\)-Mom & 84.6 & 90.9 & 99.1 \\ \hline \multirow{3}{*}{ViT-L/16} & \multirow{3}{*}{307M} & AdamW & 54.4 & Mom & **86.7** & **93.9** & 99.5 \\  & & & \(\mathcal{M}\)-Mom & 86.6 & 92.7 & **99.5** \\ \cline{2-7}  & & \(\mathcal{M}\)-AdamW & **54.4** & Mom & 86.3 & 93.4 & 99.4 \\  & & & \(\mathcal{M}\)-Mom & 86.0 & 92.0 & 99.3 \\ \hline \multirow{3}{*}{ViT-L/16} & \multirow{3}{*}{307M} & Lion & 52.0 & Mom & 86.7 & 93.8 & 99.4 \\  & & & \(\mathcal{M}\)-Mom & 86.7 & 93.0 & 99.4 \\ \cline{1-1}  & & & \(\mathcal{M}\)-Lion & **55.4** & Mom & 87.2 & **94.0** & 99.4 \\ \cline{1-1}  & & & \(\mathcal{M}\)-Lion & **85.4** & Mom & **87.2** & 93.4 & **99.4** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparing mechanic on vision models. All fine-tuning results are averaged over 3 independent runs with different seeds.

* [2] Yossi Arjevani, Yair Carmon, John C Duchi, Dylan J Foster, Nathan Srebro, and Blake Woodworth. Lower bounds for non-convex stochastic optimization. _arXiv preprint arXiv:1912.02365_, 2019.
* [3] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. In _Conference on Learning Theory (COLT)_, pages 257-269, 2010.
* [4] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014.
* [5] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _International Conference on Learning Representations_, 2018.
* [6] Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, et al. Symbolic discovery of optimization algorithms. _arXiv preprint arXiv:2302.06675_, 2023.
* [7] H. Brendan McMahan and Matthew Streeter. Adaptive bound optimization for online convex optimization. In _Proceedings of the 23rd Annual Conference on Learning Theory (COLT)_, pages 244-256, 2010.
* [8] Atilim Gunes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Automatic differentiation in machine learning: a survey. _Journal of machine learning research_, 18, 2018.
* [9] Kartik Chandra, Audrey Xie, Jonathan Ragan-Kelley, and Erik Meijer. Gradient descent: The ultimate optimizer. _Advances in Neural Information Processing Systems_, 35:8214-8225, 2022.
* [10] Francesco Orabona and David Pal. Coin betting and parameter-free online learning. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, _Advances in Neural Information Processing Systems 29_, pages 577-585. Curran Associates, Inc., 2016.
* [11] Ashok Cutkosky and Kwabena Boahen. Online learning without prior information. In _Conference on Learning Theory_, pages 643-677, 2017.
* [12] Zakaria Mhammedi and Wouter M Koolen. Lipschitz and comparator-norm adaptivity in online learning. _Conference on Learning Theory_, pages 2858-2887, 2020.

Figure 2: Comparing mechanic with D-adaptation and Adam or SGD with manually tuned learning rates on vision and language tasks.

* [13] Kfir Levy, Ali Kavis, and Volkan Cevher. Storm+: Fully adaptive sgd with recursive momentum for nonconvex optimization. _Advances in Neural Information Processing Systems_, 34:20571-20582, 2021.
* [14] Yair Carmon and Oliver Hinder. Making sgd parameter-free. _Conference on Learning Theory_, 2022.
* [15] Maor Ivgi, Oliver Hinder, and Yair Carmon. Dog is sgd's best friend: A parameter-free dynamic step size schedule. _arXiv preprint arXiv:2302.12022_, 2023.
* [16] Aaron Defazio and Konstantin Mishchenko. Learning-rate-free learning by d-adaptation. _arXiv preprint arXiv:2301.07733_, 2023.
* [17] Naman Agarwal, Rohan Anil, Elad Hazan, Tomer Koren, and Cyril Zhang. Disentangling adaptive gradient methods from learning rates. _arXiv preprint arXiv:2002.11803_, 2020.
* [18] Zhou Lu, Wenhan Xia, Sanjeev Arora, and Elad Hazan. Adaptive gradient methods with local guarantees. _arXiv preprint arXiv:2203.01400_, 2022.
* [19] Xinyi Chen and Elad Hazan. A nonstochastic control approach to optimization. _arXiv preprint arXiv:2301.07902_, 2023.
* [20] Francesco Orabona and Tatiana Tommasi. Training deep networks without learning rates through coin betting. In _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA_, pages 2157-2167, 2017. URL http://papers.nips.cc/paper/6811-training-deep-networks-without-learning-rates-through-coin-betting.
* [21] Ashok Cutkosky and Kwabena A Boahen. Online convex optimization with unconstrained domains and losses. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, _Advances in Neural Information Processing Systems 29_, pages 748-756. Curran Associates, Inc., 2016.
* [22] Shai Shalev-Shwartz. Online learning and online convex optimization. _Foundations and Trends in Machine Learning_, 4(2):107-194, 2011.
* [23] Elad Hazan. Introduction to online convex optimization. _arXiv preprint arXiv:1909.05207_, 2019.
* [24] Francesco Orabona. A modern introduction to online learning. _arXiv preprint arXiv:1912.13213_, 2019.
* [25] Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In _Proceedings of the 20th International Conference on Machine Learning (ICML-03)_, pages 928-936, 2003.
* [26] S. Shalev-Shwartz. _Online Learning: Theory, Algorithms, and Applications_. PhD thesis, The Hebrew University of Jerusalem, 2007.
* [27] Jacob Abernethy, Peter L Bartlett, Alexander Rakhlin, and Ambuj Tewari. Optimal strategies and minimax lower bounds for online convex games. In _Proceedings of the nineteenth annual conference on computational learning theory_, pages 415-424, 2008.
* [28] Ashok Cutkosky and Francesco Orabona. Black-box reductions for parameter-free online learning in Banach spaces. In _Conference On Learning Theory_, pages 1493-1529, 2018.
* [29] Brendan Mcmahan and Matthew Streeter. No-regret algorithms for unconstrained online convex optimization. In _Advances in neural information processing systems_, pages 2402-2410, 2012.
* [30] Brendan McMahan and Jacob Abernethy. Minimax optimal algorithms for unconstrained linear optimization. In _Advances in Neural Information Processing Systems_, pages 2724-2732, 2013.

* Chen et al. [2021] Liyu Chen, Haipeng Luo, and Chen-Yu Wei. Impossible tuning made possible: A new expert algorithm and its applications. In _Conference on Learning Theory_, pages 1216-1259. PMLR, 2021.
* Orabona and Pal [2021] Francesco Orabona and David Pal. Parameter-free stochastic optimization of variationally coherent functions. _arXiv preprint arXiv:2102.00236_, 2021.
* Jacobsen and Cutkosky [2022] Andrew Jacobsen and Ashok Cutkosky. Parameter-free mirror descent. In _Proceedings of Thirty Fifth Conference on Learning Theory_, volume 178 of _Proceedings of Machine Learning Research_, pages 4160-4211. PMLR, 2022.
* Cutkosky [2019] Ashok Cutkosky. Combining online learning guarantees. In _Proceedings of the Thirty-Second Conference on Learning Theory_, pages 895-913, 2019.
* Devlin et al. [2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 4171-4186, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423.
* Liu et al. [2019] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. _ArXiv preprint_, abs/1907.11692, 2019. URL https://arxiv.org/abs/1907.11692.
* Bottou and Bousquet [2008] L. Bottou and O. Bousquet. The tradeoffs of large scale learning. In _Advances in Neural Information Processing Systems (NIPS)_, 2008.
* Gu et al. [2015] Shixiang Shane Gu, Sergey Levine, Ilya Sutskever, and Andriy Mnih. Muprop: Unbiased backpropagation for stochastic neural networks. _CoRR_, abs/1511.05176, 2015.
* Wang et al. [2018] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. _ArXiv_, abs/1804.07461, 2018.
* Sun et al. [2017] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effectiveness of data in deep learning era. _2017 IEEE International Conference on Computer Vision (ICCV)_, Oct 2017. doi: 10.1109/iccv.2017.97. URL http://dx.doi.org/10.1109/iccv.2017.97.
* Dosovitskiy et al. [2021] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=YicbFdNTTy.
* Srivastava et al. [2014] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. _Journal of Machine Learning Research_, 15(56):1929-1958, 2014. URL http://jmlr.org/papers/v15/srivastava14a.html.
* Chen et al. [2023] Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, and Quoc V. Le. Symbolic discovery of optimization algorithms, 2023.
* Cutkosky [2019] Ashok Cutkosky. Artificial constraints and hints for unbounded online learning. In _Proceedings of the Thirty-Second Conference on Learning Theory_, pages 874-894, 2019.
* Mhammedi et al. [2019] Zakaria Mhammedi, Wouter M Koolen, and Tim Van Erven. Lipschitz adaptivity with multiple learning rates in online learning. In _Conference on Learning Theory_, pages 2490-2511. PMLR, 2019.
* Kempka et al. [2019] Michal Kempka, Wojciech Kotlowski, and Manfred K Warmuth. Adaptive scale-invariant online algorithms for learning linear models. In _International Conference on Machine Learning_, pages 3321-3330, 2019.

Limitations

In this paper, we introduce a technique for automatically learning the right scale of the learning rate called mechanic and evaluate it on a broad range of practical deep learning problems and settings. We find that, depending on the problem, mechanic can be quite effective and even surpass performance of manual tuning of learning rates at a fraction of the cost. We also find that in addition to training from scratch, mechanic also works for finetuning.

While the initial set of results are encouraging, many challenges remain. Firstly, we found that mechanic does not seem to work well with dropout [42]. While mechanic is effective against noise from sampling, we believe there may be a more fundamental reason why dropout does not work well with mechanic. Second, mechanic re-purposes the gradient coming from the _train set_ for learning the learning rate, which means it optimizes for train loss. This is different from manual tuning of learning rates where researchers tune it based on performance on the _validation_ set. A principled way to handle this discrepancy is also in interesting avenue of future research.

## Appendix B Additional Ablations

### Setting BASE learning rate with Mechanic

Even though we recommend setting peak learning rate of Base optimizer to 1.0, to make sure that Mechanic truly is insenstive to the peak learning rate and robust to the choice of this hyperparameter, we conduct a study where we vary peak learning rate of AdamW on BERT-B masked language modeling task. As shown in Table 3, Mechanic is largely robust against choice of peak LR set for AdamW on BERT-B MLM task.

### Robustness to \(s_{init}\)

### Ablation of \(\lambda\)

\begin{table}
\begin{tabular}{c|c c c c c} \hline \hline \(s_{init}\) & 1e-8 & 1e-7 & 1e-6 & 1e-5 & 1e-4 \\ \hline Accuracy & 49.8 & 49.8 & 49.9 & 49.7 & 49.6 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Accuracy on JFT-300M using model ViT-B/16 and optimizer \(\mathcal{M}\)-AdamW as a function \(s_{init}\). mechanic is robust to varying this parameter.

\begin{table}
\begin{tabular}{c|c c} \hline \hline Peak LR of BASE with Mechanic & MLM Acc \\ \hline
1e-2 & 71.7 \\
1e-1 & 71.6 \\
1e0 & 71.6 \\
1e1 & 71.4 \\
1e2 & 71.6 \\ \hline \hline \end{tabular}
\end{table}
Table 3: mechanic is largely robust against choice of peak LR set for AdamW on BERT-B MLM task.

\begin{table}
\begin{tabular}{c|c c c c c} \hline \hline \(\lambda\) & 0 & 1e-3 & 1e-2 & 1e-1 & 1e0 \\ \hline Accuracy & 49.7 & 49.8 & 49.9 & 49.7 & Diverged \\ \hline \hline \end{tabular}
\end{table}
Table 5: Accuracy on JFT-300M using model ViT-B/16 and optimizer \(\mathcal{M}\)-AdamW as a function \(\lambda\). We have observed that while \(\lambda\) is helpful in stabilizing mechanic on some problems, as long as it is set to a reasonable small value it does not affect performance by a lot.

[MISSING_PAGE_EMPTY:14]

[MISSING_PAGE_EMPTY:15]

[MISSING_PAGE_EMPTY:16]

Figure 4: Comparing Mechanic with D-adaptation and manually tuned learning rates on a suite of convex tasks.

Figure 5: Complementary train set results of mechanic with D-adaptation and manually tuned learning rates on vision and language tasks.

First, we conjecture that the clip operation using \(\mathbf{q}_{t}/\sqrt{v_{t}}\) may even be unnecessary in theory2: we observed no change from removing this operation in practice, and observe that the update has an intuitive interpretation via the Taylor expansion discussed in Section 3.

Footnote 2: Removing the clip in theory may requiring some additional non-worst-case assumption.

Second, the clip operation on \(h_{t}\) using \(m_{t-1}\) is essentially designed to prevent the wealth \(W_{t}\) from becoming negative or zero using the gradient truncation technique employed by [44, 45, 12]. While less consistent with known theory, we found it simpler to ensure the wealth \(W_{t}\) does become negative simply by clipping \(r_{t}\) directly (we did not clip \(W_{t}\) to be nonnegative as \(W_{t}=0\) would cause the algorithm to output \(s_{t}=0\) for all future iterations). We found these changes simplified the algorithm while having no noticeable effect on the performance. Although these deviations technically do not come with guarantees, the accomplish similar intuitive goals and so we expected (and observed) that they simplified the implementation while not damaging performance.

### Eliminating \(W_{0}\) in favor of \(s_{init}\)

While tuner makes use of the "initial wealth" value \(W_{0}\), mechanic instead adopts a varying value for \(W_{0}\) proportional to \(s_{init}\cdot m_{t}\). This makes the first \(s\) value proposed by mechanic equal to \(s_{init}\), which is more intuitive to set than \(W_{0}\). The exponential growth in \(s\) allows us to set \(s_{init}\) to a very small value of \(10^{-8}\). It also makes the values for \(s\) "scale-free" in the sense that rescaling the updates \(\mathbf{u}_{t}\) by any constant will have no effect on the resulting \(s_{t}\).

### Regret Bound

**Theorem 2**.: _With \(\beta=1\), Algorithm 2 guarantees for all \(\hat{s}\geq 0\):_

\[\sum_{t=1}^{T}h_{t}(s_{t}-\hat{s})\leq W_{0}+(\hat{s}+\max_{t}s_{t})\cdot m_{T }+O\left(\hat{s}\cdot\log(T\hat{s}m_{T}/m_{1}W_{0})\sqrt{\sum_{t=1}^{T}h_{t}^{ 2}}\right).\]

Proof.: First, we employ an argument developed by [44]:

\[\sum_{t=1}^{T}h_{t}(s_{t}-\hat{s}) \leq\sum_{t=1}^{T}\hat{h}_{t}(s_{t}-\hat{s})+\sum_{t=1}^{T}|\hat {h}_{t}-h_{t}|(|s_{t}|+|\hat{s}|)\] \[\leq\sum_{t=1}^{T}\hat{h}_{t}(s_{t}-\hat{s})+(\max_{t}|s_{t}|+| \hat{s}|)\sum_{t=1}^{T}|\hat{h}_{t}-h_{t}|\] \[=\sum_{t=1}^{T}\hat{h}_{t}(s_{t}-\hat{s})+m_{T}(\max_{t}|s_{t}|+ |\hat{s}|).\]So, in the following, it suffices to bound \(\sum_{t=1}^{T}\hat{h}_{t}(s_{t}-\dot{s})\). This is helpful because we will be able to use the bound \(|\hat{h}_{t}|\leq m_{t-1}\), and \(m_{t-1}\) is known _before_\(\hat{h}_{t}\) is revealed.

As is typical in the analysis of parameter-free algorithms, the proof proceeds by lower-bounding the wealth. Define a function \(a(x)\) piecewise by:

\[a(x)=\left\{\begin{array}{ll}0&x\leq 0\\ x^{2}/2&x\in[0,1]\\ x-1/2&x\geq 1\end{array}\right.\]

Notice that \(a(x)\) is differentiable, monotonically increasing and 1-Lipschitz. We are going to roughly show that \(W_{t}\geq\Omega(\exp(a(-\sum_{k=1}^{t}\hat{h}_{k}/\sqrt{v_{t}})))\), after which the regret bound will follow from the wealth-regret duality [10].

The key technical inequality in the proof is the following: for any \(A\), \(B\), \(m\) with \(B\geq 4m^{2}\), and any \(|x|\leq m\), we have:

\[a\left(\frac{-A}{\sqrt{B}}\right)-\frac{x}{\sqrt{B}}\text{clip} \left(\frac{-A}{\sqrt{B}},0,1\right)\geq a\left(\frac{-A-x}{\sqrt{B+x^{2}}} \right)-\frac{x^{2}}{B}.\] (6)

Once (6) is established, we proceed as follows: defining \(c_{t}=\frac{\text{clip}[\sum_{k=1}^{t-1}\hat{h}_{k}/\sqrt{4m_{t-1}^{2}+v_{t-1 }},0,1)}{\sqrt{v_{t}}}\), we have:

\[\log(W_{t}) =\log(W_{t-1})+\log(1-\hat{h}_{t}c_{t})\] \[\geq\log(W_{t-1})-\hat{h}_{t}c_{t}-\hat{h}_{t}^{2}c_{t}^{2},\]

where we have used \(c_{t}\leq 1/2\) and the identity \(\log(1-x)\geq-x-x^{2}\) for \(x\leq 1/2\) (which applies since \(\hat{h}_{t}\leq m_{t-1}\) by definition). Now, set \(A=\sum_{k=1}^{t-1}\hat{h}_{k}\) and \(B=4m_{t-1}^{2}+v_{t-1}\) and \(x=\hat{h}_{t}\) in (6), we see that:

\[\log(W_{t})-\log(W_{t-1}) \geq-\frac{x}{\sqrt{B}}\text{clip}\left(\frac{-A}{\sqrt{B}},0,1 \right)-\frac{\hat{h}_{t}^{2}}{4m_{t-1}^{2}+\mathbf{v}_{t-1}}\] \[\geq a\left(\frac{-A-x}{\sqrt{B+x^{2}}}\right)-a\left(\frac{-A}{ \sqrt{B}}\right)-\frac{x^{2}}{B}-\frac{\hat{h}_{t}^{2}}{4m_{t-1}^{2}+\mathbf{ v}_{t-1}}\] \[\geq a\left(\frac{-\sum_{k=1}^{t}\hat{h}_{k}}{\sqrt{4m_{t-1}^{2}+v _{t}}}\right)-a\left(\frac{-\sum_{k=1}^{t-1}\hat{h}_{k}}{\sqrt{4m_{t-1}^{2}+v _{t-1}}}\right)-\frac{2\hat{h}_{t}^{2}}{4m_{t-1}^{2}+v_{t-1}}\] \[\geq a\left(\frac{-\sum_{k=1}^{t}\hat{h}_{k}}{\sqrt{4m_{t}^{2}+v _{t}}}\right)-a\left(\frac{-\sum_{k=1}^{t-1}\hat{h}_{k}}{\sqrt{4m_{t-1}^{2}+v _{t-1}}}\right)-\frac{2\hat{h}_{t}^{2}}{4m_{t-1}^{2}+v_{t-1}}.\]

Thus by telescoping the sum, we have:

\[\log(W_{T})\geq\log(W_{0})+a\left(\frac{-\sum_{k=1}^{T}\hat{h}_{k}}{\sqrt{v_ {T}}}\right)-\sum_{t=1}^{T}\frac{2\hat{h}_{t}^{2}}{4m_{t-1}^{2}+v_{t-1}}.\]

Now, observe that \(\frac{2\hat{h}_{t}^{2}}{4m_{t-1}^{2}+v_{t-1}}\leq\frac{2\hat{h}_{t}^{2}}{v_{t }}\leq 2(\log(Thus, if we define \(p(H)=\frac{W_{0}m_{1}}{T^{2}m_{T}}\exp\left[a\left(\frac{H}{\sqrt{v_{T}}}\right)\right]\), we have \(W_{T}\geq p(-\sum_{k=1}^{T}\hat{h}_{k})\). Now, we employ the reward-regret duality:

\[\sum_{t=1}^{T}\hat{h}_{t}(s_{t}-\hat{s}) =s_{init}\cdot m+\hat{s}\sum_{t=1}^{T}(-\hat{h}_{t})-W_{T}\] \[\leq W_{0}+\sup_{G}\hat{s}\cdot G-p(G)\] \[=W_{0}+p^{\star}(\hat{s})\] \[\leq W_{0}+O(s\log(sT/s_{init})\sqrt{v_{T}}).\]

Where \(p^{\star}\) is the Fenchel conjugate of \(p\) and the evaluation of the conjugate follows from direct calculation (see, e.g. [10, 28, 46]).

Thus, to prove the theorem we need only show (6). This is established via casework in a manner similar to [46].

**Case 1. \(\frac{-A}{\sqrt{B}}\leq 0\):** In this case, the statement is equivalent to: \(\frac{x^{2}}{B}\geq a\left(\frac{-A-x}{\sqrt{B+x^{2}}}\right)\). Note that since \(\frac{-A}{\sqrt{B}}\leq 0\), we have \(A\geq 0\). Therefore:

\[\frac{-A-x}{\sqrt{B+x^{2}}} =\frac{-A}{\sqrt{B+x^{2}}}-\frac{x}{\sqrt{B+x^{2}}}\] \[\leq-\frac{x}{\sqrt{B+x^{2}}}.\]

Further, we clearly have \(-\frac{x}{\sqrt{B+x^{2}}}\leq 1\) so that:

\[a\left(\frac{-A-x}{\sqrt{B+x^{2}}}\right)\leq a\left(-\frac{x}{\sqrt{B+x^{2}}} \right)=\frac{x^{2}}{2(B+x^{2})}\leq\frac{x^{2}}{B}.\]

So, in the following we assume \(\frac{-A}{\sqrt{B}}\geq 0\).

**Case 2. \(\frac{-A-x}{\sqrt{B+x^{2}}}\leq 0\):** In this case, it suffices to show \(\frac{-x}{\sqrt{B}}\text{clip}\left(\frac{-A}{\sqrt{B}},0,1\right)\geq-\frac{ x^{2}}{B}\). The case assumption implies \(m^{2}\geq x\geq-A\geq 0\). Therefore, since \(B\geq 4m^{2}\), \(\text{clip}\left(\frac{-A}{\sqrt{B}},0,1\right)=\frac{-A}{\sqrt{B}}\) so that \(\frac{-x}{\sqrt{B}}\text{clip}\left(\frac{-A}{\sqrt{B}},0,1\right)=\frac{xA}{ B}\geq\frac{-x^{2}}{B}\) as desired.

So, in the following we now further assume \(\frac{-A-x}{\sqrt{B+x^{2}}}\geq 0\).

**Case 3. \(\frac{-A}{\sqrt{B}}\in[0,1]\):** We have \(a\left(\frac{-A}{\sqrt{B}}\right)=\frac{A^{2}}{2B}\), and also since \(a(z)\leq\frac{1}{2}z^{2}\) for all \(z\), \(a\left(\frac{-A-x}{\sqrt{B+x^{2}}}\right)\leq\frac{(A+x)^{2}}{2(B+x^{2})}\). Thus, it suffices to show that \(\frac{A^{2}}{2B}+\frac{xA}{B}\geq\frac{(A+x)^{2}}{2(B+x^{2})}-\frac{x^{2}}{B}\), but this is equivalent to \(\frac{(A+x)^{2}}{2B}\geq\frac{(A+x)^{2}}{2(B+x^{2})}-\frac{x^{2}}{2B}\), which clearly holds.

**Case 4: \(\frac{-A}{\sqrt{B}}\geq 1\) and \(\frac{-A-x}{\sqrt{B+x^{2}}}\geq 1\):** In this case it suffices to show \(\frac{-A}{\sqrt{B}}-\frac{x}{\sqrt{B}}\geq\frac{-A-x}{\sqrt{B+x^{2}}}-\frac{ x^{2}}{B}\). To see this, we have:

\[\frac{-A-x}{\sqrt{B+x^{2}}} =\frac{-A}{\sqrt{B+x^{2}}}-\frac{x}{\sqrt{B+x^{2}}}\] \[\leq\frac{-A}{\sqrt{B}}-\frac{x}{\sqrt{B}}+x\left(\frac{1}{\sqrt {B}}-\frac{1}{\sqrt{B+x^where in the second-to-last line we have used the fact that \(h\mapsto\frac{1}{\sqrt{B+h}}\) is a convex in \(h\), and in the last line we have used \(\sqrt{B}\geq m\geq x\).

**Case 5: \(\frac{-A}{\sqrt{B}}\geq 1\) and \(\frac{-A-x}{\sqrt{B+x^{2}}}\in[0,1)\):** In this case we need to show \(\frac{-A}{\sqrt{B}}-\frac{1}{2}\geq\frac{(A+x)^{2}}{2(B+x^{2})}-\frac{x^{2}}{ B}+\frac{x}{\sqrt{B}}\). To see this, we first observe that since \(\frac{-A-x}{\sqrt{B+x^{2}}}\in[0,1)\), we have

\[A^{2}+2Ax+x^{2} \leq B+x^{2}\] \[A^{2}+2Ax \leq B.\]

Thus, by quadratic formula, \(A\geq-x-\sqrt{x^{2}+B}\), so that we have \(A\in[-x-\sqrt{x^{2}+B},-\sqrt{B}]\).

Next, our target identity can be rearranged into an equivalent form as follows:

\[\frac{-A}{\sqrt{B}}-\frac{1}{2} \geq\frac{(A+x)^{2}}{2(B+x^{2})}-\frac{x^{2}}{B}+\frac{x}{\sqrt{B}}\] \[0 \geq\frac{(A+x)^{2}}{2(B+x^{2})}+\frac{A}{\sqrt{B}}+\frac{1}{2}- \frac{x^{2}}{B}+\frac{x}{\sqrt{B}},\]

so that it suffices to show the second line above. Notice that the RHS of this expression is convex in \(A\) and so is maximized at the boundary of the range \([-x-\sqrt{x^{2}+B},-\sqrt{B}]\). When \(A=-\sqrt{B}\) we have:

\[\frac{(A+x)^{2}}{2(B+x^{2})}+\frac{A}{\sqrt{B}}+\frac{1}{2}-\frac{x^{2}}{B}+ \frac{x}{\sqrt{B}}\leq\frac{(A+x)^{2}}{2B}+\frac{A}{\sqrt{B}}+\frac{1}{2}- \frac{x^{2}}{B}+\frac{x}{\sqrt{B}}=-\frac{x^{2}}{2B}\leq 0.\]

Alternatively, when \(A=-x-\sqrt{x^{2}+B}\), we have

\[\frac{(A+x)^{2}}{2(B+x^{2})}+\frac{A}{\sqrt{B}}+\frac{1}{2}-\frac {x^{2}}{B}+\frac{x}{\sqrt{B}} =1-\frac{\sqrt{x^{2}+B}}{\sqrt{B}}-\frac{x^{2}}{B}\] \[\leq 0.\]

This establishes the claimed inequality (6) and completes the proof.