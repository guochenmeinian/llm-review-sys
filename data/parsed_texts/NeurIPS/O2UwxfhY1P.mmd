# On the Comparison between Multi-modal and Single-modal Contrastive Learning

 Wei Huang

RIKEN AIP

wei.huang.vr@riken.jp

&Andi Han

RIKEN AIP

andi.han@riken.jp

&Yongqiang Chen

The Chinese University of Hong Kong

yqchen@cse.cuhk.edu.hk

&Yuan Cao

The University of Hong Kong

yuancao@hku.hk

&Zhiqiang Xu

MBZUAI

zhiqiang.xu@mbzuai.ac.ae

&Taiji Suzuki

University of Tokyo & RIKEN AIP

taiji@mist.i.u-tokyo.ac.jp

Equal Contribution.Corresponding Author.

###### Abstract

Multi-modal contrastive learning with language supervision has presented a paradigm shift in modern machine learning. By pre-training on a web-scale dataset, multi-modal contrastive learning can learn high-quality representations that exhibit impressive robustness and transferability. Despite its empirical success, the theoretical understanding is still in its infancy, especially regarding its comparison with single-modal contrastive learning. In this work, we introduce a feature learning theory framework that provides a theoretical foundation for understanding the differences between multi-modal and single-modal contrastive learning. Based on a data generation model consisting of signal and noise, our analysis is performed on a ReLU network trained with the InfoMax objective function. Through a trajectory-based optimization analysis and generalization characterization on downstream tasks, we identify the critical factor, which is the signal-to-noise ratio (SNR), that impacts the generalizability in downstream tasks of both multi-modal and single-modal contrastive learning. Through the cooperation between the two modalities, multi-modal learning can achieve better feature learning, leading to improvements in performance in downstream tasks compared to single-modal learning. Our analysis provides a unified framework that can characterize the optimization and generalization of both single-modal and multi-modal contrastive learning. Empirical experiments on both synthetic and real-world datasets further consolidate our theoretical findings.

## 1 Introduction

Large-scale pre-trained models have achieved unprecedented success, including GPT series [6; 41], LLaMa [53], among many others. CLIP [42] as a typical example, uses a multi-modal contrastive learning framework to learn from a massive scale of image-caption data. The multi-modal contrastive learning in CLIP has shown significant capabilities to learn high-quality representations, which are ready to be adapted to a wide range of downstream tasks, forming the backbone of generative modelslike DALL-E2 [43], prompt learning [61] as well as general purpose multi-modal agents [62; 35]. Given the huge success of models like CLIP that have stellar zero-shot and few-shot capabilities on a wide range of out-of-distribution (OOD) benchmarks, they have been widely recognized as foundation models (FMs). More similar examples are given by ALIGN [28], Florence [59], BLIP [33], Flamingo [1].

Despite the unprecedented success achieved by multi-modal contrastive learning, the fundamental mechanism that leads to greater performance, especially compared to single-modal contrastive learning is still under-explored. Recently, several seminal works provided theoretical explanations for either single-modal [4; 5; 14; 48; 24; 7; 50; 49; 20] or multi-modal contrastive learning [38; 37; 44; 12]. For example, [56] studied how single-modal contrastive learning learns the feature representations for neural networks by analyzing its feature learning process. As for multi-modal contrastive learning, [12; 58] provided explanations for why multi-modal contrastive learning demonstrates zero-shot transferability, and robustness to distribution shifts, than supervised learning, which offer valuable insights. Although both lines of the existing works provide valid theoretical insights under the respective settings, rare work has compared the optimization and generalization of the two types of contrastive learning under a unified framework. This motivates us to establish a systematic feature learning analysis for both single-modal and multi-modal contrastive learning.

In particular, we consider a data generation model that contains two modalities of data, which are generated from signal and noise features. The signal feature correlates in different modalities, while there is no correlation between noise features among modalities. We then study the optimization of single-modal and multi-modal contrastive learning under gradient descent training. By studying the trajectories of signal learning and noise memorization, we establish the convergence conditions and further characterize the generalization ability in the downstream tasks. The results show that, through the cooperation between modalities, multi-modal contrastive learning can achieve better generalization in the downstream task. In contrast, without the help of the second modality, single-modal contrastive learning concentrates on learning noise from the data, and thus generalizes poorly on the downstream tasks. The main contributions of this work are summarized as follows:

* This work establishes the _first systematic comparative optimization analysis_ for single-modal and multi-modal contrastive learning under gradient descent training in non-convex settings. We show that both single-modal and multi-modal can achieve near-zero training error under InfoMax contrastive loss after polynomial number of iterations, by overcoming the non-convex difficulty.
* By a trajectory-based analysis of the signal learning and noise memorization of the ReLU network from the data, we successfully characterize the difference in _generalization_ between single-modal and multi-modal contrastive learning. The distinct SNRs of different modalities lead to a divergence in the generalization of downstream tasks for the two contrastive learning frameworks.
* Our theory suggests that the advantage of multi-modal over single-modal contrastive learning comes from the high quality of the second modality and the cooperation between the two modalities through contrastive learning. This divergence is ultimately reflected in the difference in feature learning and the final gap in downstream task generalization. Experimental results on both synthetic and real-world datasets confirm our theoretical findings and understanding.

## 2 Related Work

**Theoretical Understanding of Single-modal Contrastive Learning.** The seminal work [4] started theoretical research on single-modal contrastive learning. They assumed that different positive samples are independently drawn from the same latent class, making a connection to supervised learning. [55] identified two key properties related to the contrastive loss: alignment and uniformity. Alongside, [32] illustrated that predicting auxiliary prediction tasks helps in learning representations effective for downstream prediction tasks, and [52] provided a theoretical analysis of contrastive learning in the multi-view setting. Besides, [51] proposed a theoretical framework to understand contrastive self-supervised learning from an optimization perspective. [21] proposed a loss that performs spectral decomposition on the population augmentation graph and can be succinctly written as a contrastive learning objective on neural net representations. [46] pointed out the importance of inductive biases of the function class and training algorithm in understanding contrastive learning. The most related work to us is the work by [56]. Similar to them, this work studies ReLU networks and considers the signal-noise data model. However, we do not require the adjustable bias term in the activation function, which plays a critical role in [56]. Furthermore, this work adopts a unified framework to compare with multi-modal contrastive learning, which is out of scope in [56].

**Understanding of Multi-modal Contrastive Learning.** As the multi-modal contrastive learning approaches such as CLIP received great success, recent works have been proposing explanations from empirical perspective. [36] empirically showed that high train-test similarity is insufficient to explain CLIP's OOD performance. [60] illustrated that CLIP behaves similarly to Bags-of-words in language-based image retrieval, i.e., the order of words in the input sentence does not largely affect CLIP to find the corresponding image. Besides, [16] demonstrated that training data diversity and the ability to leverage the diversity as supervised learning is the key to the effective robustness of CLIP. Theoretically, [13] proved that multi-modal contrastive learning can block-identity latent factors shared between modalities by the a generative data model. [44] analyzed the training dynamics of a simple multi-modal contrastive learning model and show that contrastive pairs are important for the model to efficiently balance the learned representations. Furthermore, [38] showed that each step of loss minimization by gradient descent can be seen as performing SVD on a contrastive cross-covariance matrix. Similar to us, [25] tried to answer why multi-modal learning is better than single model learning. However, they did not consider contrastive learning and thus cannot explain the success of multi-modal contrastive multi-modal learning.

**Data Quality Matters for Multi-modal Contrastive Learning.** Aligned with our theoretical results, there is a lot of empirical evidence showing that improving the alignment quality with more descriptive captions improves multi-modal contrastive learning. [16] show that the training distribution mostly determines the generalizability of CLIP. Furthermore, [47; 40; 18; 17] find filtering poorly aligned image-caption samples used for training leads to further improvements. Besides, [45; 39; 15] demonstrate that improving the descriptiveness of the captions could further boost the performance of CLIP. Besides, [34] demonstrated that the caused by a combination of model initialization and contrastive learning optimization. However, their results do not take neural network architecture into consideration, and do not provide an analysis of test errors either.

## 3 Problem Setting

Notation.We use bold-faced letters for vectors and matrices otherwise representing scalar. We use \(\|\cdot\|_{2}\) to denote the Euclidean norm of a vector or the spectral norm of a matrix, while denoting \(\|\cdot\|_{F}\) as the Frobenius norm of a matrix. For a neural network, we denote \(\sigma(\cdot)\) as the activation function and we adopt ReLU activation where \(\sigma(x)=\max\{0,x\}\) in this work. To simplify, we denote \([n]=\{1,2,\ldots,n\}\).

Data Model.In this work, we consider the following data model, which consists of signal and noise. In the first modality, example \((\mathbf{x},y)\sim\mathcal{D}\) is generated as follows:

\[\mathbf{x}=[\mathbf{x}^{(1)\top},\mathbf{x}^{(2)\top}]^{\top}=[y\boldsymbol{ \mu}^{\top},\boldsymbol{\xi}^{\top}]^{\top},\quad y\sim\mathrm{unif}(\{-1,1\}).\] (1)

where \(\mathbf{x}\in\mathbb{R}^{2d}\) is the input feature and \(y\in\{-1,1\}\) is the corresponding label generated from Rademacher distribution. In particular, \(\mathbf{x}^{(1)}=y\boldsymbol{\mu}\in\mathbb{R}^{d}\) is the task-relevant signal vector, and \(\mathbf{x}^{(2)}=\boldsymbol{\xi}\sim\mathcal{N}(\mathbf{0},\sigma_{\xi}^{2} \mathbf{I})\in\mathbb{R}^{d}\) is the task-irrelevant noise vector. Intuitively, if a network learns primarily from signal, it can effectively generalize to unseen data and vice versa. Similar data models have been adopted in recent theoretical works on supervised learning [2; 26; 8; 23; 31; 63; 22; 11] and self-supervised learning [56; 50; 30].

Similarly for the second modality, a sample \((\widetilde{\mathbf{x}},y)\sim\widetilde{\mathcal{D}}\) is generated as

\[\widetilde{\mathbf{x}}=[\widetilde{\mathbf{x}}^{(1)\top},\widetilde{\mathbf{x }}^{(2)\top}]^{\top}=[y\boldsymbol{\mu}^{\top},\widetilde{\boldsymbol{\xi}}^{ \top}]^{\top},\quad y\sim\mathrm{unif}(\{-1,1\}),\] (2)

where the input feature \(\widetilde{\mathbf{x}}\in\mathbb{R}^{2\widetilde{d}}\) and the label \(y\) is shared with the first modality. Besides, the signal is a given vector \(\widetilde{\boldsymbol{\mu}}\in\mathbb{R}^{\widetilde{d}}\), and noise follows \(\widetilde{\boldsymbol{\xi}}\sim\mathcal{N}(\mathbf{0},\sigma_{\xi}^{2} \mathbf{I})\in\mathbb{R}^{\widetilde{d}}\). The linear data models for multi-modal learning have also been studied in previous work [44]. To simplify the analysis, we set \(d=\widetilde{d}\), \(\sigma_{\xi}=\sigma_{\widetilde{\xi}}\). However, we highlight that extensions to deal with unmatched dimension and noise level is possible.

### Single-modal Contrastive Learning

We use a single-layer neural network \(\mathbf{h}:\mathbb{R}^{2d}\rightarrow\mathbb{R}^{m}\) with ReLU activation as our encoder, where \(m\) is the number of neurons, which represents the embedding dimension. More precisely,

\[\mathbf{h}(\mathbf{x})=[\bar{h}_{1}(\mathbf{x}),\dots,\bar{h}_{m}(\mathbf{x})]^ {\top}\in\mathbb{R}^{m},\quad\mathrm{where}\;\bar{h}_{r}(\mathbf{x})=h_{r}( \mathbf{x}^{(1)})+h_{r}(\mathbf{x}^{(2)}),\] (3)

here we let \(h_{r}(\mathbf{x}^{(i)})=\sigma(\langle\mathbf{w}_{r},\mathbf{x}^{(i)}\rangle)\) for \(r\in[m]\), \(i\in[2]\), and \(\sigma(\cdot)\) is the ReLU activation function. We adopt a Gaussian to initialize the weights \(\mathbf{w}_{r}^{(0)}\sim\mathcal{N}(\mathbf{0},\sigma_{0}^{2}\mathbf{I})\), where \(\sigma_{0}\) severs as the strength.

Given a pair of positive data samples, the contrastive loss function is based on the similarity measure defined as the inner product between the representation of two samples \(\mathbf{x},\mathbf{x}^{\prime}\in\mathbb{R}^{2d}\):

\[\mathrm{Sim}_{\mathbf{h}}(\mathbf{x},\mathbf{x}^{\prime})=\frac{1}{m}\sum_{r= 1}^{m}h_{r}(\mathbf{x}^{(1)})\mathrm{sg}(h_{r}(\mathbf{x}^{\prime(1)}))+ \frac{1}{m}\sum_{r=1}^{m}h_{r}(\mathbf{x}^{(2)})\mathrm{sg}(h_{r}(\mathbf{x}^ {\prime(2)})),\] (4)

where the \(\mathrm{sg}(\cdot)\) is the stop-gradient operation, which is inspired by recent empirical works [19; 10] and theoretical work studying contrastive learning [56]. Here we define positive sample as

\[\widehat{\mathbf{x}}=[\widehat{\mathbf{x}}^{(1)\top},\widehat{\mathbf{x}}^{( 2)\top}]^{\top}=[y\boldsymbol{\mu}^{\top},\boldsymbol{\xi}^{\top}+\boldsymbol {\epsilon}^{\top}]^{\top},\;\boldsymbol{\epsilon}\sim\mathcal{N}(\mathbf{0}, \sigma_{\epsilon}^{2}\mathbf{I}).\] (5)

In particular, we consider the form of augmentation where the signal stays invariant while the noise vector is corrupted with added independent noise. Similar setup has been considered in [57]. We consider the contrastive loss presented as follows:

\[L=-\frac{1}{n}\sum_{i=1}^{n}\log(\frac{e^{\mathrm{Sim}_{\mathbf{h}}(\mathbf{x }_{i},\widehat{\mathbf{x}}_{i})/\tau}}{e^{\mathrm{Sim}_{\mathbf{h}}(\mathbf{x }_{i},\widehat{\mathbf{x}}_{i})/\tau}+\sum_{j\neq i}^{M}e^{\mathrm{Sim}_{ \mathbf{h}}(\mathbf{x}_{i},\mathbf{x}_{j})/\tau}}),\] (6)

where \(\tau\) is the temperature parameter, \(n\) is the number of training samples, and \(M\) is the number of negative pairs. In this work, to efficiently optimize the loss to near zero, we require negative sample pairs do not share the same label, i.e., \(y_{j}\neq y_{i}\) in (6). Note that this setting is aligned with supervised contrastive learning [29; 27].

We use gradient descent to optimize the contrastive learning loss, which leads to the gradient update:

\[\mathbf{w}_{r}^{(t+1)}=\mathbf{w}_{r}^{(t)}-\eta\nabla_{\mathbf{ w}_{r}}L(\mathbf{W}^{(t)})=\mathbf{w}_{r}^{(t)}+\frac{\eta}{nm\tau}\sum_{i=1}^{n}(1- \ell_{i}^{\prime(t)})h_{r}^{(t)}(\widehat{\mathbf{x}}_{i}^{(1)})h^{\prime(t)} (\mathbf{x}_{i}^{(1)})y_{i}\boldsymbol{\mu}\] \[+\frac{\eta}{nm\tau}\sum_{i=1}^{n}(1-\ell_{i}^{\prime(t)})h_{r}^{ (t)}(\widehat{\mathbf{x}}_{i}^{(2)})h_{r}^{\prime(t)}(\mathbf{x}_{i}^{(2)}) \boldsymbol{\xi}_{i}-\frac{\eta}{nm\tau}\sum_{i=1}^{n}\sum_{j\neq i}^{M}\ell_ {i,j}^{\prime(t)}h_{r}^{(t)}(\mathbf{x}_{j}^{(1)})h^{\prime(t)}(\mathbf{x}_{ i}^{(1)})y_{i}\boldsymbol{\mu}\] \[-\frac{\eta}{nm\tau}\sum_{i=1}^{n}\sum_{j\neq i}^{M}\ell_{i,j}^{ \prime(t)}h_{r}^{(t)}(\mathbf{x}_{j}^{(2)})h_{r}^{\prime(t)}(\mathbf{x}_{i}^{( 2)})\boldsymbol{\xi}_{i},\] (7)

where we denote \(\bar{h}_{r}^{(t)}(\mathbf{x})=\sigma(\langle\mathbf{w}_{r}^{(t)},\mathbf{x}\rangle)\), \(\eta\) as the learning rate, and we define the loss derivatives as

\[\ell_{i}^{\prime(t)}\triangleq\frac{e^{\mathrm{Sim}_{\mathbf{h}}(\mathbf{x }_{i},\widehat{\mathbf{x}}_{i})/\tau}}{e^{\mathrm{Sim}_{\mathbf{h}}(\mathbf{x },\widehat{\mathbf{x}}_{i})/\tau}+\sum_{j\neq i}^{M}e^{\mathrm{Sim}_{\mathbf{h }}(\mathbf{x}_{i},\mathbf{x}_{j})/\tau}},\;\ell_{i,j}^{\prime(t)}\triangleq \frac{e^{\mathrm{Sim}_{\mathbf{h}}(\mathbf{x}_{i},\mathbf{x}_{j})/\tau}}{e^{ \mathrm{Sim}_{\mathbf{h}}(\mathbf{x}_{i},\widehat{\mathbf{x}}_{i})/\tau}+\sum _{j\neq i}^{M}e^{\mathrm{Sim}_{\mathbf{h}}(\mathbf{x}_{i},\mathbf{x}_{j})/\tau }}.\] (8)

Intuitively, when the similarity between positive pair is high, and the similarity between negative time is low, we can see \(\ell_{i}^{\prime(t)}\approx 1\) and \(\ell_{i,j}^{\prime(t)}\approx 0\), for \(i\in[n]\) and \(j\in[M]\). Therefore, the gradient descent in Eq. (7) is close to zero, indicating the near convergence result. Furthermore, from Eq. (7), we observe that the evolution direction of weight is composed of signal vector \(\boldsymbol{\mu}\) and noise vectors \(\boldsymbol{\xi}_{i}\) for \(i\in[n]\). This observation plays a critical role in our following theoretical analysis.

### Multi-modal Contrastive Learning

We use two neural networks \(\mathbf{h}:\mathbb{R}^{d}\rightarrow\mathbb{R}^{m}\) and \(\mathbf{g}:\mathbb{R}^{\bar{d}}\rightarrow\mathbb{R}^{m}\) to encode two input modality \(\mathbf{x}\) and \(\widetilde{\mathbf{x}}\) respectively. Both neural networks use ReLU activation function. More precisely,

\[\mathbf{h}(\mathbf{x})=[\bar{h}_{1}(\mathbf{x}),\dots,\bar{h}_{m}(\mathbf{x})]^ {\top}\in\mathbb{R}^{m},\quad\mathrm{where}\;\bar{h}_{r}(\mathbf{x})=h_{r}( \mathbf{x}^{(1)})+h_{r}(\mathbf{x}^{(2)})\]\[\mathbf{g}(\widetilde{\mathbf{x}})=[\bar{g}_{1}(\widetilde{\mathbf{x}}),\dots, \bar{g}_{m}(\widetilde{\mathbf{x}})]^{\top}\in\mathbb{R}^{m},\quad\mathrm{ where}\ \bar{g}_{r}(\widetilde{\mathbf{x}})=g_{r}(\widetilde{\mathbf{x}}^{(1)})+g_{r}( \widetilde{\mathbf{x}}^{(2)})\]

we let \(h_{r}(\mathbf{x}^{(i)})=\sigma(\langle\mathbf{w}_{r},\mathbf{x}^{(i)}\rangle)\) and \(g_{r}(\widetilde{\mathbf{x}}^{(i)})=\sigma(\langle\widetilde{\mathbf{w}}_{r},\widetilde{\mathbf{x}}^{(i)}\rangle)\). Here \(\sigma(\cdot)\) is the ReLU activation function, \(\mathbf{w}_{r}\in\mathbb{R}^{d}\) and \(\widetilde{\mathbf{w}}_{r}\in\mathbb{R}^{\widetilde{d}}\) for \(r\in[m]\) are the weights in two networks. Given the embedding, the similarity function of the two modalities is defined as

\[\mathrm{Sim}_{\mathbf{h},\mathbf{g}}(\mathbf{x},\widetilde{ \mathbf{x}}) =\frac{1}{m}\sum_{r=1}^{m}h_{r}(\mathbf{x}^{(1)})\mathrm{sg}(g_{r }(\widetilde{\mathbf{x}}^{(1)}))+\frac{1}{m}\sum_{r=1}^{m}h_{r}(\mathbf{x}^{( 2)})\mathrm{sg}(g_{r}(\widetilde{\mathbf{x}}^{(2)})),\] \[\mathrm{Sim}_{\mathbf{g},\mathbf{h}}(\widetilde{\mathbf{x}}, \mathbf{x}) =\frac{1}{m}\sum_{r=1}^{m}g_{r}(\widetilde{\mathbf{x}}^{(1)}) \mathrm{sg}(h_{r}(\mathbf{x}^{(1)}))+\frac{1}{m}\sum_{r=1}^{m}g_{r}( \widetilde{\mathbf{x}}^{(2)})\mathrm{sg}(h_{r}(\mathbf{x}^{(2)})).\]

The two similarity functions defined above are modality-centered with stop-gradient operation applied. The objective function of contrastive multi-modal learning can be expressed as

\[L =-\frac{1}{n}\sum_{i=1}^{n}\log(\frac{e^{\mathrm{Sim}_{\mathbf{h },\mathbf{g}}(\mathbf{x}_{i},\widetilde{\mathbf{x}}_{i})/\tau}}{e^{\mathrm{ Sim}_{\mathbf{h},\mathbf{g}}(\mathbf{x}_{i},\widetilde{\mathbf{x}}_{i})/ \tau}+\sum_{j\neq i}^{M}e^{\mathrm{Sim}_{\mathbf{h},\mathbf{g}}(\mathbf{x}_{i},\widetilde{\mathbf{x}}_{j})/\tau}})\] \[-\frac{1}{n}\sum_{i=1}^{n}\log(\frac{e^{\mathrm{Sim}_{\mathbf{g },\mathbf{h}}(\widetilde{\mathbf{x}}_{i},\mathbf{x}_{i})/\tau}}{e^{\mathrm{ Sim}_{\mathbf{g},\mathbf{h}}(\widetilde{\mathbf{x}}_{i},\mathbf{x}_{i})/ \tau}+\sum_{j\neq i}^{M}e^{\mathrm{Sim}_{\mathbf{g},\mathbf{h}}(\widetilde{ \mathbf{x}}_{i},\mathbf{x}_{j})/\tau}}).\] (9)

Same to the single-modal learning whose objective function is governed by Eq. (6), the objective function for multi-modal contrastive learning adopt one positive pair and \(M\) negative pairs. Besides, we require the negative pairs do not share the same label. To optimize the objective function (9) for multi-modal learning, gradient descent is applied to train two encoders simultaneously. The gradient descent rule for the first modal network is governed by the following expression.

\[\mathbf{w}_{r}^{(t+1)}=\mathbf{w}_{r}^{(t)}-\eta\nabla_{\mathbf{ w}_{r}}L(\mathbf{W}^{(t)})=\mathbf{w}_{r}^{(t)}+\frac{\eta}{nm\tau}\sum_{i=1}^{n}(1- \ell_{i}^{\prime(t)})g_{r}^{(t)}(\widetilde{\mathbf{x}}_{i}^{(1)})h_{r}^{\prime (t)}(\mathbf{x}^{(1)})y_{i}\boldsymbol{\mu}\] \[+\frac{\eta}{nm\tau}\sum_{i=1}^{n}(1-\ell_{i}^{\prime(t)})g_{r}^{ (t)}(\widetilde{\mathbf{x}}_{i}^{(2)})h_{r}^{\prime(t)}(\mathbf{x}^{(2)}) \boldsymbol{\xi}_{i}-\frac{\eta}{nm\tau}\sum_{i=1}^{n}\sum_{j\neq i}^{M}\ell_{ i,j}^{\prime(t)}g_{r}^{(t)}(\widetilde{\mathbf{x}}_{j}^{(1)})h_{r}^{\prime (t)}(\mathbf{x}^{(1)})y_{i}\boldsymbol{\mu}\] \[-\frac{\eta}{nm\tau}\sum_{i=1}^{n}\sum_{j\neq i}^{M}\ell_{i,j}^{ \prime(t)}g_{r}^{(t)}(\widetilde{\mathbf{x}}_{j}^{(2)})h_{r}^{\prime(t)}( \mathbf{x}^{(2)})\boldsymbol{\xi}_{i}.\] (10)

Here with a slight abuse of notation, we use \(\ell_{i}^{\prime(t)},\ell_{i,j}^{\prime(t)}\) to represent the loss derivatives for both modalities. Compared to signal-modal learning, the main difference for the multi-modal learning is that the corresponding embedding is from another modality. The gradient update for the second modality can be derived similarly, which we omit here for clarity.

### Downstream Task Evaluation

To evaluate the out-of-distribution generalization of single-modal and multi-modal contrastive learning for downstream task, we consider a test distribution \(\mathcal{D}_{\mathrm{test}}\), where a sample \(\mathbf{x}_{\mathrm{test}}=[y\cdot\boldsymbol{\nu}^{\top},\boldsymbol{\zeta} ^{\top}]^{\top}\sim\mathcal{D}_{\mathrm{test}}\) is generated as follows. The test signal \(\boldsymbol{\nu}\) satisfies \(\langle\boldsymbol{\nu},\boldsymbol{\mu}\rangle=O(\|\boldsymbol{\mu}\|_{2}^{2} d^{-1/2})\) and the test noise follows \(\boldsymbol{\zeta}\sim\mathcal{N}(\mathbf{0},\sigma_{\xi}^{2}\mathbf{I})\) and \(y\) follows Rademacher distribution. After the training is complete, we introduce a linear head on top of the learned embedding \(\mathbf{h}(\mathbf{x}_{\mathrm{test}})\) for adapting to test distribution, i.e., \(f(\mathbf{x}_{\mathrm{test}})=\langle\mathbf{w},\mathbf{h}(\mathbf{x}_{\mathrm{ test}})\rangle\). Specifically, we consider the task of classification and define the population 0-1 test error as \(L_{\mathcal{D}_{\mathrm{test}}}=\mathbb{P}_{\mathbf{x}_{\mathrm{test}}\sim \mathcal{D}_{\mathrm{test}}}\big{[}yf(\mathbf{x}_{\mathrm{test}})<0\big{]}\).

## 4 Main Results

In this section, we introduce our key theoretical findings that elucidate the optimization and generalization result for both single-modal and multi-modal contrastive learning through the feature learning analysis. We use a trajectory-based analysis for the iterations induced by gradient descent, following a post-training analysis for the performance on the downstream test set. Below we provide the main assumption and main theorems.

**Assumption 4.1**.: Let \(\mathrm{SNR}=\|\boldsymbol{\mu}\|_{2}/(\sigma_{\xi}\sqrt{d})\). Assume (1) \(d\geq\widetilde{\Omega}(\max\{n^{2},n\sigma_{0}^{-1}\sigma_{\xi}^{-1},\sigma_{0}^ {-2}\|\boldsymbol{\mu}\|_{2}^{-2}\})\). (2) \(\eta\leq O(\min\{m\|\boldsymbol{\mu}\|_{2}^{-2},nm\sigma_{\xi}^{-2}d^{-1}\})\). (3) \(\sigma_{0}\leq\widetilde{O}((\max\{\sigma_{\xi}\sqrt{d},\|\boldsymbol{\mu}\|_{2} \})^{-1})\). (4) \(m,n\geq\widetilde{\Omega}(1)\).

(5) \(\sigma_{\epsilon}\leq\min\{\widetilde{\Theta}(\|\bm{\mu}\|_{2}),\sigma_{\xi}/ \widetilde{\Omega}(1)\}\). (6) \(n\cdot\mathrm{SNR}^{2}=\Theta(1)\). (7) \(C_{\mu}\|\bm{\mu}\|_{2}=\|\widetilde{\bm{\mu}}\|_{2}\), where \(C_{\mu}\geq 2.66\) is a constant.

(1) We adopt a high dimensional setting to ensure enough over-parameterization. (2,3) The learning rate and the strength of initialization are chosen to make sure the that gradient descent can effectively minimize the contrastive loss. (4) The choice of hidden size \(m\) and number of training sample \(n\) is to provide adequate concentration. (5) The strength of augmentation is set to keep the similarity between two positive samples. (6) The relation between number of sample and \(\mathrm{SNR}\) is to distinguish the feature learning process between single-modal and multi-modal contrastive learning. (7) To differentiate single-modal and multi-modal contrastive learning, we introduce a constant \(C_{\mu}\), which enables the cooperation between the two modalities in multi-modal contrastive learning.

**Theorem 4.2** (Single-Modal Contrastive Learning).: _Under the single-modal learning setup, suppose Assumption 4.1 holds. Then after \(T^{*}=\widetilde{\Theta}(\eta^{-1}mn\sigma_{\xi}^{-2}d^{-1}+\eta^{-1}mn\sigma _{\xi}^{-2}d^{-1}\epsilon^{-1})\), the with probability at least \(1-1/d\), it holds that (1) Training error \(L(T^{*})\leq\epsilon\) and (2) Test error at down-stream task \(L_{\mathcal{D}_{\mathrm{test}}}(T^{*})=\Theta(1)\)._

Theorem 4.2 states that despite the small training error achieved by single-modal contrastive learning, the test error is large in the downstream task.

**Theorem 4.3** (Multi-Modal Contrastive Learning).: _Under the single-modal learning setup, suppose Assumption 4.1 holds. Then after \(T^{*}=\widetilde{\Theta}(\eta^{-1}mn\sigma_{\xi}^{-2}d^{-1}+\eta^{-1}mn\sigma _{\xi}^{-2}d^{-1}\epsilon^{-1})\), the with probability at least \(1-1/d\), it holds that (1) Training error \(L(T^{*})\leq\epsilon\) and (2) Test error at down-stream task \(L_{\mathcal{D}_{\mathrm{test}}}(T^{*})=o(1)\)._

Theorem 4.3 demonstrates that trained multi-modal contrastive learning can achieve both small training error and downstream test error. Compared to Theorem 4.2, Theorem 4.3 shows that the generalization of multi-modal contrastive learning in downstream tasks is better than single-modal contrastive learning. The reason behind this difference is that the two modalities can cooperate with each other; the higher quality in one modality can boost the feature learning in the target modality, helping to generalize to the downstream task. On the contrary, augmentation often maintains the same SNR as the original data, so single-modal learning hardly benefits from the augmentation and can only memorize the noise from the data, which is not applicable to downstream tasks.

## 5 Proof Roadmap

### Proof Sketch for Single Modal Contrastive Learning

The proof is constructed by a optimization analysis followed by a generlization analysis in the downstream task. Through the application of the gradient descent rule outlined in Eq. (7), we observe that the gradient descent iterate \(\mathbf{w}_{r}^{(t)}\) is a linear combination of its random initialization \(\mathbf{w}_{r}^{(0)}\), the signal vector \(\bm{\mu}\) and the noise vectors in the training data \(\bm{\xi}_{i}\) for \(i\in[n]\). Consequently, for \(r\in[m]\), the decomposition of weight vector iteration can be expressed:

\[\mathbf{w}_{r}^{(t)}=\mathbf{w}_{r}^{(0)}+\gamma_{r}^{(t)}\|\bm{\mu}\|_{2}^{- 2}\bm{\mu}+\sum_{i=1}^{n}\rho_{r,i}^{(t)}\|\bm{\xi}_{i}\|_{2}^{-2}\bm{\xi}_{i},\] (11)

where \(\gamma_{r}^{(t)}\) and \(\rho_{r,i}^{(t)}\) serve as coefficients and represent signal learning and noise memorization respectively. Based on the the gradient descent update (7), the iteration of \(\gamma_{r}^{(t)}\) and \(\rho_{r,i}^{(t)}\) are given:

**Lemma 5.1** (Single-modal Contrastive Learning).: _The coefficients \(\gamma_{r}^{(t)}\rho_{r,i}^{(t)}\) in decomposition (11) satisfy the following equations:_

\[\gamma_{r}^{(t+1)}=\gamma_{r}^{(t)}+\frac{\eta}{nm\tau}\sum_{i=1}^{n}\big{[}( 1-\ell_{i}^{\prime(t)})h_{r}^{(t)}(\widehat{\mathbf{x}}_{i}^{(1)})-\sum_{j \neq i}^{M}\ell_{i,j}^{\prime(t)}h_{r}^{(t)}(\mathbf{x}_{j}^{(1)})\big{]}h_{r} ^{\prime(t)}(\mathbf{x}_{i}^{(1)})y_{i}\|\bm{\mu}\|_{2}^{2},\] (12)

\[\rho_{r,i}^{(t+1)}=\rho_{r,i}^{(t)}+\frac{\eta}{nm\tau}\big{[}(1-\ell_{i}^{ \prime(t)})h_{r}(\widehat{\mathbf{x}}_{i}^{(2)})-\sum_{j\neq i}^{M}\ell_{i,j} ^{\prime(t)}h_{r}(\mathbf{x}_{j}^{(2)})\big{]}h_{r}^{\prime(t)}(\mathbf{x}_{i} ^{(2)})\|\bm{\xi}_{i}\|_{2}^{2},\] (13)

_where the initialization \(\gamma_{r}^{(0)},\rho_{r,i}^{(0)}=0\)._Lemma 5.1 tells how the coefficients evolve under gradient descent update. In the following, we introduce a two-stage dynamics to characterize the whole training process based on Eq 12 and Eq 13.

**First Stage: Exponential growth.** During the first stage, we show before \(\gamma_{r}^{(t)}\) or \(\rho_{r,i}^{(t)}\) grow to \(\Theta(1)\), the embedding (3) is close to zero, suggesting the similarity is bounded by \(1\leq\mathrm{Sim}_{\mathbf{h}}(\mathbf{x},\mathbf{x}^{\prime})\leq C_{\ell}\) for some constant \(C_{\ell}>1\). The loss derivatives defined in (8) can thus be bounded within some constant range.

_Signal learning._ According to the update for signal learning in (12), we see the propagation can be simplified based on the hard-negative sampling strategy, i.e., the negative pairs do not share the same labels. This suggests the negative term is always zero as \(\sum_{i=1}^{n}\sum_{j:y_{j}\neq y_{i}}^{M}\sigma(\langle\mathbf{w}_{r}^{(t)},y _{j}\boldsymbol{\mu}\rangle)\sigma^{\prime}(\langle\mathbf{w}_{r}^{(t)},y_{i} \boldsymbol{\mu}\rangle)=0\). The resulting update of \(\gamma_{r}^{(t)}\) reduces to \(\gamma_{r}^{(t+1)}=\gamma_{r}^{(t)}+\frac{\eta}{nm\tau}\sum_{i=1}^{n}(1-\ell_ {i}^{\prime(t)})\sigma(\langle\mathbf{w}_{r}^{(t)},y_{i}\boldsymbol{\mu} \rangle)\sigma^{\prime}(\langle\mathbf{w}_{r}^{(t)},y_{i}\boldsymbol{\mu} \rangle)y_{i}\|\boldsymbol{\mu}\|_{2}^{2}\). Examining the propagation of \(\gamma_{r}^{(t)}\), we can divide the dynamics into two groups depending on the sign of weight initialization \(\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\mu}\rangle\). Let \(\mathcal{U}_{+}^{(t)}\triangleq\{r:\langle\mathbf{w}_{r}^{(t)},\boldsymbol{ \mu}\rangle>0\}\) and \(\mathcal{U}_{-}^{(t)}\triangleq\{r:\langle\mathbf{w}_{r}^{(t)},\boldsymbol{ \mu}\rangle<0\}\). Then for \(r\in\mathcal{U}_{+}^{(0)}\), we can show \(\gamma_{r}^{(t)}\geq 0\) increases exponentially and thus the sign of inner produce stays invariant with \(\mathcal{U}_{+}^{(t)}=\mathcal{U}_{+}^{(0)}\) for all \(t\geq 0\). On the other hand, for \(r\in\mathcal{U}_{-}^{(0)}\), we can show \(\gamma_{r}^{(t)}\leq 0\) and decreases exponentially with \(\mathcal{U}_{-}^{(t)}=\mathcal{U}_{-}^{(0)}\) for all \(t\geq 0\).

_Noise memorization._ Compared to signal learning, the behaviour of noise memorization requires more detailed analysis. This is mainly because the negative pairs can not be eliminated simply based on label difference, as the noise patch \(\boldsymbol{\xi}_{i}\) is generated independent of label \(y_{i}\). In addition, the added noise \(\boldsymbol{\epsilon}_{i}\) by augmentation can also contribute to the noise dynamics. We first show when the noise level \(\sigma_{\epsilon}\) is much smaller compared to \(\sigma_{\xi}\), the dynamics of noise memorization is largely remains unaffected. By the sign of \(\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{i}\rangle\), we partition the samples into two sets, i.e., \(\mathcal{I}_{r,+}^{(t)}=\{i:\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{i} \rangle>0\}\), and \(\mathcal{I}_{r,-}^{(t)}=\{i:\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{i} \rangle<0\}\). We can verify for \(i\in\mathcal{I}_{r,-}^{(0)}\), the value of \(\rho_{r,i}^{(t)}\) stays at zero based on the update (13) with an induction argument. For samples \(i\in\mathcal{I}_{r,+}^{(0)}\) with positive initialization, we analyze the noise memorization based on the joint dynamics of samples with the same label. In particular, we define total noise memorization of positive and negative samples respectively as \(B_{r,+}^{(t)}\triangleq\sum_{i:y_{i}=1}(\rho_{r,i}^{(t)}+\langle\mathbf{w}_{r} ^{(0)},\boldsymbol{\xi}_{i}\rangle)\mathds{1}_{i\in\mathcal{I}_{r,+}^{(t)}}\) and \(B_{r,-}^{(t)}\triangleq\sum_{i:y_{i}=-1}(\rho_{r,i}^{(t)}+\langle\mathbf{w}_{r }^{(0)},\boldsymbol{\xi}_{i}\rangle)\mathds{1}_{i\in\mathcal{I}_{r,+}}^{(t)}\). The update of \(\rho_{r,i}^{(t)}\) in (13) then implies the dynamics of \(B_{r,+}^{(t)}\) and \(B_{r,-}^{(t)}\) as follows

\[B_{r,+}^{(t+1)}\approx B_{r,+}^{(t)}+\frac{\eta\sigma_{\xi}^{2}d}{nm\tau} \big{(}B_{r,+}^{(t)}-\frac{1}{2}B_{r,-}^{(t)}\big{)},\quad B_{r,-}^{(t+1)} \approx B_{r,-}^{(t)}+\frac{\eta\sigma_{\xi}^{2}d}{nm\tau}\big{(}B_{r,-}^{(t )}-\frac{1}{2}B_{r,+}^{(t)}\big{)},\]

where the coefficient of \(1/2\) appears as a result of the randomness of the sign of initialization. This result suggests, individual \(\rho_{r,i:y_{i}=1}^{(t)}\) cannot grow too slow compared to the \(\rho_{r,i:y_{i}=-1}^{(t)}\). Following a similar induction argument, we are able to show \(\rho_{r,i:y_{i}=1}^{(t)}\) has an exponential growth lower bound. On the other hand, for samples with \(y_{i}=-1\) but with different neuron, we can use the same strategy to show an exponential growth lower bound for some neurons that satisfy the initialization conditions.

**Lemma 5.2**.: _Under the Assumption 4.1, let \(T_{1}=\log\big{(}20/(\sigma_{0}\sigma_{\xi}\sqrt{d})\big{)}/\log\big{(}1+0.96 \frac{\eta\sigma_{\xi}^{2}d}{nm\tau}\big{)}\), we have \(\gamma_{r}^{(t)}=\widetilde{O}(1/\sqrt{n})\) for all \(r\in[m]\) and \(0\leq t\leq T_{1}\) and \(\max_{r}\rho_{r,i}^{(T_{1})}=\Omega(1)\) for all \(i\in[n]\)._

**Second stage: convergence and scale difference.** At the end of first stage, the noise grows to a constant order while signal learning remains negligible. As a result, the loss derivatives are no longer bounded within some constant range. In the second stage, we aim to show the loss is able to converge to an arbitrarily small value \(\epsilon\). Despite the unsupervised learning setup, we are still able to show loss convergence thanks to the hard negative samples. Let \(F_{0}(\mathbf{W},\mathbf{x}_{i})=\mathrm{Sim}(\mathbf{x}_{i},\widehat{\mathbf{ x}}_{i})\) be the similarity to the argumentation and \(F_{j}(\mathbf{W},\mathbf{x}_{i})=\mathrm{Sim}(\mathbf{x}_{i},\mathbf{x}_{j})\) for \(j=1,...,M\) be the similarity between the negative pairs. Then we can show there exists some \(\mathbf{W}^{*}\) such that \(\langle\nabla F_{0}(\mathbf{W}^{(t)},\mathbf{x}_{i}),\mathbf{W}^{*}\rangle\geq 2 \log(2M/\epsilon)\) while \(\langle\nabla F_{j}(\mathbf{W}^{(t)},\mathbf{x}_{i}),\mathbf{W}^{*}\rangle \leq\log(2M/\epsilon)\) for all \(j=1,...,M\). Then we can bound \(\langle\nabla L_{S}(\mathbf{W}^{(t)}),\mathbf{W}^{(t)}-\mathbf{W}^{*}\rangle\geq \frac{1}{n}\sum_{i=1}^{n}L_{i}(\mathbf{W}^{(t)})-\epsilon/2\). This as a result allows to show a monotonic decrease in the loss function as \(L(\mathbf{W}^{(t)})\leq\frac{1}{n}(\|\mathbf{W}^{(t)}-\mathbf{W}^{*}\|_{F}^{2} -\|\mathbf{W}^{(t+1)}-\mathbf{W}^{*}\|_{F}^{2})+\epsilon\) which guarantees convergence by telescoping over the inequality. Upon the convergence, we can also show 

[MISSING_PAGE_FAIL:8]

4. For \(i\in\mathcal{I}^{(0)}_{r,+}\cap\widetilde{\mathcal{I}}^{(0)}_{r,+}\), without loss of generality, we consider upper bounding noise memorization for the first modality and \(y_{i}=1\). To this end, we first define the individual and joint noise memorization for the first modality as \(\Psi^{(t)}_{r,i}\triangleq\rho^{(t)}_{r,i}+\langle\mathbf{w}^{(0)}_{r},\bm{ \xi}_{i}\rangle\), and \(B^{(t)}_{r,+,+}\triangleq\sum_{i:y_{i}=1}(\rho^{(t)}_{r,i}+\langle\mathbf{w}^{ (0)}_{r},\bm{\xi}_{i}\rangle)\mathds{1}_{i\in\mathcal{I}^{(t)}_{r,+}\cap \widetilde{\mathcal{I}}^{(t)}_{r,+}},B^{(t)}_{r,+,-}\triangleq\sum_{i:y_{i}=1}( \rho^{(t)}_{r,i}+\langle\mathbf{w}^{(0)}_{r},\bm{\xi}_{i}\rangle))\mathds{1}_ {i\in\mathcal{I}^{(t)}_{r,+}\cap\widetilde{\mathcal{I}}^{(t)}_{r,-}}\). Similar definitions exist for the other modality. The joint dynamics of noise memorization can be first upper bounded by the other modality as \(\widetilde{\Psi}^{(t)}_{r,i}\geq\frac{10\Gamma_{G}}{M+1}(B^{(t)}_{r,+,+}+B^{ (t)}_{r,+,-})\). Then we can upper bound the individual noise memorization by \[\Psi^{(t)}_{r,i}\leq(1+\frac{1.06\eta\sigma_{\xi}^{2}d}{nm\tau})^{t}(\Psi^{(0) }_{r,i}+\widetilde{\Psi}^{(0)}_{r,i}).\]

We show the combined dynamics of \(\gamma^{(t)}_{r}\) and \(\rho^{(t)}_{r}\) exhibits exponential growth while the magnitude of their difference shrinks exponentially. The results are summarized as follows

**Lemma 5.4**.: _Under the Assumption 4.1, let \(T_{1}=\log\left(20/(\sigma_{0}\|\bm{\mu}\|_{2})\right)/\log\left(1+0.48C_{\mu} \frac{\eta\|\bm{\mu}\|_{2}^{2}}{m\tau}\right)\), we have \(\rho^{(T_{1})}_{r,i}=\widetilde{O}(1/\sqrt{n})\) for all \(r\in[m]\), \(i\in[n]\), and \(0\leq t\leq T_{1}\) and \(\max_{r}\gamma^{(T_{1})}_{r}=\Omega(1)\)._

**Second Stage: Convergence and scale difference.** The second stage presents similar patterns compared to single-modal learning. Thanks to the correlation between the two modality during gradient descent training, the two neural network converge at the same time, minimizing the training loss. Besides, The scale difference at the end of the first stage is carried over throughout the second stage until convergence. Therefore, it allows to show a monotonic decrease in the loss function as \(L(\mathbf{W}^{(t)},\widetilde{\mathbf{W}}^{(t)})\leq\|\mathbf{W}^{(t)}- \mathbf{W}^{*}\|_{F}^{2}+\|\widetilde{\mathbf{W}}^{(t)}-\widetilde{\mathbf{W} }^{*}\|_{F}^{2}-\|\mathbf{W}^{(t+1)}-\mathbf{W}^{*}\|_{F}^{2}-\|\widetilde{ \mathbf{W}}^{(t+1)}-\widetilde{\mathbf{W}}^{*}\|_{F}^{2}+2\epsilon\), which guarantees convergence by telescoping over the inequality. At the same time, until convergence, we can show the scale difference obtained at the end of the first stage is maintained, namely \(\max_{r,i}\rho^{(t)}_{r,i}=\widetilde{O}(1/\sqrt{n})\) and \(\max_{r}\gamma^{(t)}_{r}=\Omega(1)\). This suggests the signal learning dominates the noise memorization and thus the resulting embeddings are linearly separable, which guarantees a small test error for downstream tasks. The formal convergence result is established in Lemma D.15. Combined with the generalization error demonstrated in Appendix D.3, this completes the proof of Theorem 4.3.

## 6 Experiments

Synthetic experimentsWe conduct synthetic experiments to verify the theoretical results obtained in the previous sections. We generate samples following the theoretical setups, where we set the data dimension \(d=2000\), number of training samples \(n=100\), number of test samples \(n_{\mathrm{test}}=200\), and the hidden size of all encoders as \(m=50\). We adopt gradient descent with a learning rate of \(0.01\) as the optimizer to train the model by \(200\) epochs. In the single-modal setting, the \(\bm{\mu}\) is set to be \([5,0,...,0]^{T}\) and the \(\bm{\xi}\sim\mathcal{N}(\mathbf{0},\mathbf{I})\) for the in-distribution data, and the augmentation vector \(\bm{\epsilon}\sim\mathcal{N}(\mathbf{0},0.01*\mathbf{I})\). For the multi-modal setting, \(\tilde{\bm{\mu}}=[0,15,0,...,0]^{T}\). In addition, for the OOD test data \(\mathbf{x}_{\mathrm{test}}=[\bm{\nu}^{\top},\bm{\zeta}^{\top}]^{\top}\), we set \(\bm{\nu}=[2,0,...,0]\) and \(\bm{\zeta}\sim\mathcal{N}(\mathbf{0},\mathbf{I})\). We perform logistic regression based on the learned features \(\mathbf{h}(\mathbf{x}_{\mathrm{test}})\) and apply the learned classifier head to evaluate OOD generalization error in terms of prediction accuracy.

**Results.** In Figure 1, we see the training loss of both single-modal and multi-modal learning converges rapidly. At the same time, OOD test accuracy of multi-modal learning converges to nearly 1.0 while

Figure 1: Training loss, test accuracy, signal learning and noise memorization of single-modal and multi-modal contrastive learning.

that of single-modal learning stagnates around 0.5. This is primarily because under the setup where the other modality \(\tilde{\bm{\mu}}\) has a higher SNR, signal learning of \(\bm{\mu}\) is lifted. This can be verified from the third plot of Figure 1, where the signal learning of multi-modal framework is significantly higher than single-modal. Further, it can be observed that single-modal contrastive learning exhibits more severe noise memorization, which suppresses signal learning. In contrast, multi-modal contrastive learning exhibits less severe noise memorization which would further encourage signal learning. These phenomena again support and align with our theoretical results.

Real-world experimentsWe now extend the comparison of single-modal and multi-modal learning to realistic image data, ColoredMNIST [3, 54], which is a typical benchmark studying the generalization capability under distribution shifts. The ColoredMNIST dataset is a variation of the standard MNIST dataset, where each digit is assigned a specific color based on its label. The two modalities are image, and text that describes the images. The task is a 10-class classification that recognizes the number of the colored MNIST images. Specifically, we have 10 colors to color 10 digits, and introduce spurious correlations via label noises following the literature:

* For the _training_ set, 10% of labels will be clipped to a random class. For images with class '0' (or '1'), they will be colored as red (or green) with a probability of 77.5%, and as another random color with a probability of 22.5%. The coloring scheme introduces a spurious correlation.
* For the _test_ set, 10% of labels will be clipped to a random class. For images with class '0' (or '1'), they will be colored as green (or red) with a probability of 77.5%, and as another random color with a probability of 22.5%. The coloring scheme can be considered as reversing the training spurious correlations. Therefore, the evaluation on test set can reflect to what extent the model learns to use the spurious features, i.e., colors, to classify images.

We implement the multi-modal learning following the practice in [54], where we consider an ideal language encoder that successfully encodes the caption of the images into one-hot labels of colors and digits. For single-modal learning, we follow the implementation of the SimCLR [9] to construct a set of augmentations to learn the representations.

**Results.** Under the distribution shift, we verify that multi-modal learning archives an out-of-distribution test accuracy of 82.13%, which outperforms that of single-modal learning 12.68%. As a result, we can claim that the effective SNR of invariant features (the shape of the digit) will be degraded under the impact of the injected color. Therefore, the performance of single-modal may be suboptimal as it cannot effectively utilize the information of the digit's shape. On the other hand, multi-modal demonstrates a better capacity for handling this scenario.

## 7 Conclusions

In this work, we have established a comprehensive comparison of the optimization differences during the pre-training stage and the generalization gap between single-modal and multi-modal contrastive learning for downstream tasks. With the cooperation between modalities, multi-modal contrastive learning can achieve better feature learning and generalization on downstream tasks compared to single-modal learning. On the other hand, data augmentation alone can hardly improve data quality and thus cannot boost the performance of single-modal contrastive learning. Together, these results quantitatively demonstrate the superiority of multi-modal learning over single-modal learning and emphasize the importance of data quality in multi-modal contrastive learning.

## Acknowledgments and Disclosure of Funding

We thank the anonymous reviewers for their insightful comments to improve the paper. Wei Huang is supported by JSPS KAKENHI Grant Number 24K20848. Yuan Cao is supported by NSFC 12301657 and HK RGC-ECS 27308624. Taiji Suzuki is partially supported by JSPS KAKENHI (24K02905) and JST CREST (JPMJCR2015).

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Model** & **Train Accuracy** & **Test Accuracy** \\ \hline Single-modal & 88.43\% & 12.68\% \\ Multi-modal & 87.77\% & 82.13\% \\ \hline \hline \end{tabular}
\end{table}
Table 1: Performance comparison for single and multi-modal contrastive learning.

## References

* [1]J. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. (2022) Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems35, pp. 23716-23736. Cited by: SS1.
* [2]Z. Allen-Zhu and Y. Li (2020) Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. arXiv preprint arXiv:2012.09816. Cited by: SS1.
* [3]M. Arjovsky, L. Bottou, I. Gulrajani, and D. Lopez-Paz (2019) Invariant risk minimization. arXiv preprint arXiv:1907.02893. Cited by: SS1.
* [4]S. Arora, H. Khandeparkar, M. Khodak, O. Plevrakis, and N. Saunshi (2019) A theoretical analysis of contrastive unsupervised representation learning. arXiv preprint arXiv:1902.09229. Cited by: SS1.
* [5]R. Balestriero and Y. LeCun (2022) Contrastive and non-contrastive self-supervised learning recover global and local spectral embedding methods. Advances in Neural Information Processing Systems35, pp. 26671-26685. Cited by: SS1.
* [6]T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei (2020) Language models are few-shot learners. In Advances in Neural Information Processing Systems, Cited by: SS1.
* [7]V. Cabannes, B. Kiani, R. Balestriero, Y. LeCun, and A. Bietti (2023) The ssl interplay: augmentations, inductive bias, and generalization. In International Conference on Machine Learning, pp. 3252-3298. Cited by: SS1.
* [8]Y. Cao, Z. Chen, M. Belkin, and Q. Gu (2022) Benign overfitting in two-layer convolutional neural networks. Advances in neural information processing systems35, pp. 25237-25250. Cited by: SS1.
* [9]T. Chen, S. Kornblith, M. Norouzi, and G. Hinton (2020) A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597-1607. Cited by: SS1.
* [10]X. Chen and K. He (2021) Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 15750-15758. Cited by: SS1.
* [11]Y. Chen, W. Huang, K. Zhou, Y. Bian, B. Han, and J. Cheng (2023) Understanding and improving feature learning for out-of-distribution generalization. In Advances in Neural Information Processing Systems, Cited by: SS1.
* [12]Z. Chen, Y. Deng, Y. Li, and Q. Gu (2023) Understanding transferable representation learning and zero-shot transfer in clip. arXiv preprint arXiv:2310.00927. Cited by: SS1.
* [13]I. Daunhawer, A. Bizeul, E. Palumbo, A. Marx, and J. E. Vogt (2023) Identifiability results for multimodal contrastive learning. arXiv preprint arXiv:2303.09166. Cited by: SS1.
* [14]Y. Dubois, S. Ermon, T. B. Hashimoto, and P. S. Liang (2022) Improving self-supervised learning by characterizing idealized representations. Advances in Neural Information Processing Systems35, pp. 11279-11296. Cited by: SS1.
* [15]L. Fan, D. Krishnan, P. Isola, D. Katabi, and Y. Tian (2023) Improving CLIP training with language rewrites. In Advances in Neural Information Processing Systems, Cited by: SS1.
* [16]A. Fang, G. Ilharco, M. Wortsman, Y. Wan, V. Shankar, A. Dave, and L. Schmidt (2022) Data determines distributional robustness in contrastive language image pre-training (CLIP). In International Conference on Machine Learning, Cited by: SS1.

[MISSING_PAGE_POST]

. Goodfellow, S. Bengio, and J. Pouget (2016) Generative adversarial nets. In Advances in neural information processing systems, Cited* [18] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Enetzari, Giannis Daras, Sarah M Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt. Datacomp: In search of the next generation of multimodal datasets. In _Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2023.
* [19] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. _Advances in neural information processing systems_, 33:21271-21284, 2020.
* [20] Jeff Z HaoChen and Tengyu Ma. A theoretical study of inductive biases in contrastive learning. _arXiv preprint arXiv:2211.14699_, 2022.
* [21] Jeff Z HaoChen, Colin Wei, Adrien Gaidon, and Tengyu Ma. Provable guarantees for self-supervised deep learning with spectral contrastive loss. _Advances in Neural Information Processing Systems_, 34:5000-5011, 2021.
* [22] Wei Huang, Yuan Cao, Haonan Wang, Xin Cao, and Taiji Suzuki. Graph neural networks provably benefit from structural information: A feature learning perspective. _arXiv preprint arXiv:2306.13926_, 2023.
* [23] Wei Huang, Ye Shi, Zhongyi Cai, and Taiji Suzuki. Understanding convergence and generalization in federated learning through feature learning theory. In _The Twelfth International Conference on Learning Representations_, 2023.
* [24] Weiran Huang, Mingyang Yi, Xuyang Zhao, and Zihao Jiang. Towards the generalization of contrastive self-supervised learning. _arXiv preprint arXiv:2111.00743_, 2021.
* [25] Yu Huang, Chenzhuang Du, Zihui Xue, Xuanyao Chen, Hang Zhao, and Longbo Huang. What makes multi-modal learning better than single (provably). _Advances in Neural Information Processing Systems_, 34:10944-10956, 2021.
* [26] Samy Jelassi and Yuanzhi Li. Towards understanding how momentum improves generalization in deep learning. In _International Conference on Machine Learning_, pages 9965-10040. PMLR, 2022.
* [27] Wenlong Ji, Zhun Deng, Ryumei Nakada, James Zou, and Linjun Zhang. The power of contrast for feature learning: A theoretical analysis. _Journal of Machine Learning Research_, 24(330):1-78, 2023.
* [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In _International conference on machine learning_, pages 4904-4916. PMLR, 2021.
* [29] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. _Advances in neural information processing systems_, 33:18661-18673, 2020.
* [30] Yiwen Kou, Zixiang Chen, Yuan Cao, and Quanquan Gu. How does semi-supervised learing with pseudo-labelers work? a case study. In _International Conference on Learning Representations_, 2023.
* [31] Yiwen Kou, Zixiang Chen, Yuanzhou Chen, and Quanquan Gu. Benign overfitting for two-layer relu networks. _arXiv preprint arXiv:2303.04145_, 2023.
* [32] Jason D Lee, Qi Lei, Nikunj Saunshi, and Jiacheng Zhuo. Predicting what you already know helps: Provable self-supervised learning. _Advances in Neural Information Processing Systems_, 34:309-323, 2021.
* [33] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In _International Conference on Machine Learning_, pages 12888-12900. PMLR, 2022.

* Liang et al. [2022] Victor Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Y Zou. Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. _Advances in Neural Information Processing Systems_, 35:17612-17625, 2022.
* Liu et al. [2023] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In _NeurIPS_, 2023.
* Mayilvahanan et al. [2023] Prasanna Mayilvahanan, Thaddius Wiedemer, Evgenia Rusak, Matthias Bethge, and Wieland Brendel. Does clip's generalization performance mainly stem from high train-test similarity? _arXiv preprint arXiv:2310.09562_, 2023.
* Ming and Li [2024] Yifei Ming and Yixuan Li. Understanding retrieval-augmented task adaptation for vision-language models. _arXiv preprint arXiv:2405.01468_, 2024.
* Nakada et al. [2023] Ryumei Nakada, Halil Ibrahim Gulluk, Zhun Deng, Wenlong Ji, James Zou, and Linjun Zhang. Understanding multimodal contrastive learning and incorporating unpaired data. In _International Conference on Artificial Intelligence and Statistics_, pages 4348-4380. PMLR, 2023.
* Nguyen et al. [2023] Thao Nguyen, Samir Yitzhak Gadre, Gabriel Ilharco, Sewoong Oh, and Ludwig Schmidt. Improving multimodal datasets with image captioning. In _Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2023.
* Nguyen et al. [2022] Thao Nguyen, Gabriel Ilharco, Mitchell Wortsman, Sewoong Oh, and Ludwig Schmidt. Quality not quantity: On the interaction between dataset design and robustness of CLIP. In _Advances in Neural Information Processing Systems_, 2022.
* Gpt-4 technical report, 2023.
* Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* Ramesh et al. [2022] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 2022.
* Ren and Li [2023] Yunwei Ren and Yuanzhi Li. On the importance of contrastive loss in multimodal learning. _arXiv preprint arXiv:2304.03717_, 2023.
* Santurkar et al. [2023] Shibani Santurkar, Yann Dubois, Rohan Taori, Percy Liang, and Tatsunori Hashimoto. Is a caption worth a thousand images? a study on representation learning. In _International Conference on Learning Representations_, 2023.
* Saunshi et al. [2022] Nikunj Saunshi, Jordan Ash, Surbhi Goel, Dipendra Misra, Cyril Zhang, Sanjeev Arora, Sham Kakade, and Akshay Krishnamurthy. Understanding contrastive learning requires incorporating inductive biases. In _International Conference on Machine Learning_, pages 19250-19286. PMLR, 2022.
* Schuhmann et al. [2022] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5b: An open large-scale dataset for training next generation image-text models. In _Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2022.
* Simon et al. [2023] James B Simon, Maksis Knutins, Liu Ziyin, Daniel Geisz, Abraham J Fetterman, and Joshua Albrecht. On the stepwise nature of self-supervised learning. In _International Conference on Machine Learning_, pages 31852-31876. PMLR, 2023.
* Tian [2022] Yuandong Tian. Understanding deep contrastive learning via coordinate-wise optimization. _Advances in Neural Information Processing Systems_, 35:19511-19522, 2022.
* Tian et al. [2021] Yuandong Tian, Xinlei Chen, and Surya Ganguli. Understanding self-supervised learning dynamics without contrastive pairs. In _International Conference on Machine Learning_, pages 10268-10278. PMLR, 2021.
* Tian et al. [2020] Yuandong Tian, Lantao Yu, Xinlei Chen, and Surya Ganguli. Understanding self-supervised learning with dual deep networks. _arXiv preprint arXiv:2010.00578_, 2020.
* Tosh et al. [2021] Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive learning, multi-view redundancy, and linear models. In _Algorithmic Learning Theory_, pages 1179-1206. PMLR, 2021.

* [53] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* [54] Qizhou Wang, Yong Lin, Yongqiang Chen, Ludwig Schmidt, Bo Han, and Tong Zhang. Do clip models always generalize better than imagenet models? _Advances in Neural Information Processing Systems_, 2024.
* [55] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In _International Conference on Machine Learning_, pages 9929-9939. PMLR, 2020.
* [56] Zixin Wen and Yuanzhi Li. Toward understanding the feature learning process of self-supervised contrastive learning. In _International Conference on Machine Learning_, pages 11112-11122. PMLR, 2021.
* [57] Yihao Xue, Siddharth Joshi, Eric Gan, Pin-Yu Chen, and Baharan Mirzasoleiman. Which features are learnt by contrastive learning? on the role of simplicity bias in class collapse and feature suppression. In _International Conference on Machine Learning_, pages 38938-38970. PMLR, 2023.
* [58] Yihao Xue, Siddharth Joshi, Dang Nguyen, and Baharan Mirzasoleiman. Understanding the robustness of multi-modal contrastive learning to distribution shift. _arXiv preprint arXiv:2310.04971_, 2023.
* [59] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. _arXiv preprint arXiv:2111.11432_, 2021.
* [60] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why vision-language models behave like bags-of-words, and what to do about it? _arXiv e-prints_, pages arXiv-2210, 2022.
* [61] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. _International Journal of Computer Vision_, 130(9):2337-2348, 2022.
* [62] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. _arXiv preprint arXiv:2304.10592_, 2023.
* [63] Difan Zou, Yuan Cao, Yuanzhi Li, and Quanquan Gu. Understanding the generalization of adam in learning neural networks with proper regularization. In _International Conference on Learning Representations_, 2023.

**Appendix**

###### Contents

* A Limitations and broader impact
* B Preliminary Lemmas
* C Single-modal Contrastive Learning: Proof of Theorem 4.2
* C.1 First Stage
* C.1.1 Dynamics of Signal Learning: Upper Bound
* C.1.2 Dynamics of Noise Memorization: Lower Bound
* C.1.3 Noise Memorization: Proof of Lemma 5.2
* C.2 Second Stage
* C.3 Downstream Task Performance
* D Multi-Modal Contrastive Learning: Proof of Theorem 4.3
* D.1 First Stage
* D.1.1 Dynamics of Signal Learning: Lower Bound
* D.1.2 Dynamics of Noise Memorization: Upper Bound
* D.1.3 Signal Learning: Proof of Lemma 5.4
* D.2 Second Stage
* D.3 Downstream Task Performance
* E Additional Experimental Details
Limitations and broader impact

While our theoretical analysis is novel in terms of optimization and generalization, the data model can be further modified to be more practical. Our theoretical analysis may be further used for empirical and theoretical studies of contrastive learning, especially multi-modal contrastive learning. However, we do not foresee a direct social impact from our theory.

## Appendix B Preliminary Lemmas

Before the proof, we introduce lemmas that are useful in proving our main theorem.

**Lemma B.1**.: _Let \(x\sim\mathcal{N}(0,\sigma^{2})\). Then \(\mathbb{P}(|x|\leq c)=2\mathrm{erf}\left(\frac{c}{\sqrt{2}\sigma}\right)\leq 2 \sqrt{1-\exp(-\frac{2c^{2}}{\sigma^{2}\pi})}\)._

Proof.: The probability density function for \(x\) is given by

\[f(x)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{x^{2}}{2\sigma^{2}}\right).\]

Then we know that

\[\mathbb{P}(|x|\leq c)=\frac{1}{\sqrt{2\pi}\sigma}\int_{-c}^{c}\exp\left(-\frac {x^{2}}{2\sigma^{2}}\right)dx.\]

By the definition of \(\mathrm{erf}\) function

\[\mathrm{erf}(c)=\frac{2}{\sqrt{\pi}}\int_{0}^{c}\exp(-x^{2})dx,\]

and variable substitution yields

\[\mathrm{erf}\left(\frac{c}{\sqrt{2}\sigma}\right)=\frac{1}{\sqrt{2\pi}\sigma} \int_{0}^{c}\exp\left(-\frac{x^{2}}{2\sigma^{2}}\right)dx.\]

Therefore, we first conclude \(\mathbb{P}(|x|\leq c)=2\mathrm{erf}\left(\frac{c}{\sqrt{2}\sigma}\right)\).

Next, by the inequality \(\mathrm{erf}(x)\leq\sqrt{1-\exp(-4x^{2}/\pi)}\), we finally obtain

\[\mathbb{P}(|x|\leq c)\leq 2\sqrt{1-\exp\left(-\frac{2c^{2}}{\sigma^{2}\pi} \right)}.\]

Lemma B.1 introduces an anti-concentration result. In later sections, this lemma will be used to show that with a relatively large initialization for the weight vector, some initial properties hold.

**Lemma B.2**.: _Under condition that \(d\geq\frac{400n}{\sigma_{0}\sigma_{\xi}}\sqrt{-\frac{\log(6n/\delta)}{-\pi\log (1-\delta^{2}/(4m^{2}))}}\), and \(\widetilde{d}\geq\frac{400n}{\sigma_{0}\sigma_{\xi}}\sqrt{\frac{\log(6n/\delta )}{-\pi\log(1-\delta^{2}/(4m^{2}))}}\), then with probability at least \(1-\delta\), we can show for all \(r\in[m]\),_

\[|\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\mu}\rangle| \geq 100\cdot\mathrm{SNR}\sqrt{\frac{8\log(6n/\delta)}{d}}n,\] \[|\langle\widetilde{\mathbf{w}}_{r}^{(0)},\widetilde{\boldsymbol{ \mu}}\rangle| \geq 100\cdot\mathrm{SNR}\sqrt{\frac{8\log(6n/\delta)}{\widetilde{d} }}n.\]

Proof of Lemma b.2.: By Lemma B.1, because \(\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\mu}\rangle\sim\mathcal{N}(0,\sigma_ {0}^{2}\|\boldsymbol{\mu}\|_{2}^{2})\), we can show

\[\mathbb{P}\big{(}|\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\mu}\rangle|\leq c \big{)}\leq 2\sqrt{1-\exp\left(-\frac{2c^{2}}{\sigma_{0}^{2}\|\boldsymbol{\mu}\|_ {2}^{2}\pi}\right)}.\]Let \(c=100\cdot\mathrm{SNR}\sqrt{\frac{8\log(6n/\delta)}{d}}n=100n\|\bm{\mu}\|_{2} \sigma_{\xi}^{-1}d^{-1}\sqrt{8\log(6n/\delta)}\) and plug it into the RHS of the above inequality, which becomes:

\[\mathrm{RHS}=2\sqrt{1-\exp\left(-\frac{160000\log(6n/\delta)n^{2}}{\sigma_{0}^{ 2}\sigma_{\xi}^{2}d^{2}\pi}\right)}.\]

Then we can verify that when \(d\) satisfies that \(d\geq\frac{400n}{\sigma_{0}\sigma_{\xi}}\sqrt{\frac{\log(6n/\delta)}{-\pi\log(1 -\delta^{2}/(4m^{2}))}}\), it holds that \(\mathrm{RHS}\leq\delta/m\). This suggests for a single neuron \(r\in[m]\), we have \(\mathbb{P}(|\langle\mathbf{w}_{r}^{(0)},\bm{\mu}\rangle|\leq c)\leq\delta/m\). Applying union bound, we can show the desired result.

Similarly, with the same procedure, we can prove the result for the other modality.

\[|\langle\widetilde{\mathbf{w}}_{r}^{(0)},\widetilde{\bm{\mu}}\rangle|\geq 1 00\cdot\mathrm{SNR}\sqrt{\frac{8\log(6n/\delta)}{\widetilde{d}}}n.\]

**Lemma B.3** ([31]).: _Let \(\mathcal{S}_{1}=\{i\in[n]:y_{i}=1\}\) and \(\mathcal{S}_{-1}=\{i\in[n]:y_{i}=-1\}\). Then with probability at least \(1-\delta\),_

\[|\mathcal{S}_{1}|,|\mathcal{S}_{-1}|\in\left[\frac{n}{2}-\sqrt{\frac{n}{2} \log(4/\delta)},\frac{n}{2}+\sqrt{\frac{n}{2}\log(4/\delta)}\right].\]

Lemma B.3 states that when the label is randomly sampled, the number of positive samples and negative samples is close to \(\frac{n}{2}\), adequately.

**Lemma B.4** ([8]).: _Suppose that \(d\geq\Omega(\log(mn/\delta))\), \(m=\Omega(\log(1/\delta))\). Then with probability at least \(1-\delta\), it satisfies that for all \(r\in[m],i\in[n]\),_

\[|\langle\mathbf{w}_{r}^{(0)},\bm{\mu}\rangle| \leq\sqrt{2\log(8m/\delta)}\sigma_{0}\|\bm{\mu}\|_{2}\] \[|\langle\mathbf{w}_{r}^{(0)},\bm{\xi}_{i}\rangle| \leq 2\sqrt{\log(8mn/\delta)}\sigma_{0}\sigma_{\xi}\sqrt{d}\] \[|\langle\mathbf{w}_{r}^{0},\bm{\epsilon}_{i}\rangle| \leq 2\sqrt{\log(8mn/\delta)}\sigma_{0}\sigma_{\epsilon}\sqrt{d}.\]

_and for all \(i\in[n]\)_

\[\sigma_{0}\|\bm{\mu}\|_{2}/2\leq\max_{r\in[m]}\langle\mathbf{w}_{r}^{(0)}, \bm{\mu}\rangle\leq\sqrt{2\log(8m/\delta)}\sigma_{0}\|\bm{\mu}\|_{2}\] \[\sigma_{0}\sigma_{\xi}\sqrt{d}/4\leq\max_{r\in[m]}\langle\mathbf{ w}_{r}^{(0)},\bm{\xi}_{i}\rangle\leq 2\sqrt{\log(8mn/\delta)}\sigma_{0} \sigma_{\xi}\sqrt{d}.\]

**Lemma B.5** ([8]).: _Suppose that \(\delta>0\) and \(d=\Omega(\log(6n/\delta)))\). Then with probability \(1-\delta\),_

\[\sigma_{\xi}^{2}d/2\leq\|\bm{\xi}_{i}\|_{2}^{2}\leq 3\sigma_{\xi}^ {2}d/2,\] \[|\langle\bm{\xi}_{i},\bm{\xi}_{i^{\prime}}\rangle|\leq 2\sigma_{\xi}^ {2}\sqrt{d\log(6n^{2}/\delta)}\] \[|\langle\bm{\xi}_{i},\bm{\mu}\rangle|\leq\|\bm{\mu}\|_{2}\sigma_{ \xi}\sqrt{2\log(6n/\delta)}\] \[\sigma_{\epsilon}^{2}d/2\leq\|\bm{\epsilon}_{i}\|_{2}^{2}\leq 3 \sigma_{\epsilon}^{2}d/2,\] \[|\langle\bm{\epsilon}_{i},\bm{\xi}_{i^{\prime}}\rangle|\leq 2 \sigma_{\epsilon}\sigma_{\xi}\sqrt{d\log(6n^{2}/\delta)}\] \[|\langle\bm{\epsilon}_{i},\bm{\mu}\rangle|\leq\|\bm{\mu}\|_{2} \sigma_{\epsilon}\sqrt{2\log(6n/\delta)}\]

_for all \(i,i^{\prime}\in[n]\)._

## Appendix C Single-modal Contrastive Learning: Proof of Theorem 4.2

In this section, we provide the proof for Theorem 4.2, which states main results of single modal learning. The training dynamics are based on the coefficient iterations presented in Lemma 5.1. Below, we provide a proof for this lemma:Proof of Lemma 5.1.: Recall that the weight decomposition is expressed as

\[\mathbf{w}_{r}^{(t)}=\mathbf{w}_{r}^{(0)}+\gamma_{r}^{(t)}\|\boldsymbol{\mu}\|_{2 }^{-2}\boldsymbol{\mu}+\sum_{i=1}^{n}\rho_{r,i}^{(t)}\|\boldsymbol{\xi}_{i}\|_{ 2}^{-2}\boldsymbol{\xi}_{i}.\]

We plug it into the gradient descent update as described by Equation 10 yields

\[\mathbf{w}_{r}^{(t+1)}=\mathbf{w}_{r}^{(0)}+\gamma_{r}^{(t+1)}\| \boldsymbol{\mu}\|_{2}^{-2}\boldsymbol{\mu}+\sum_{i=1}^{n}\rho_{r,i}^{(t+1)}\| \boldsymbol{\xi}_{i}\|_{2}^{-2}\boldsymbol{\xi}_{i}\] \[=\mathbf{w}_{r}^{(0)}+\gamma_{r}^{(t)}\|\boldsymbol{\mu}\|_{2}^{ -2}\boldsymbol{\mu}+\sum_{i=1}^{n}\rho_{r,i}^{(t)}\|\boldsymbol{\xi}_{i}\|_{ 2}^{-2}\boldsymbol{\xi}_{i}+\frac{\eta}{nm\tau}\sum_{i=1}^{n}(1-\ell_{i}^{ \prime(t)})h_{r}^{(t)}(\widehat{\mathbf{x}}_{i}^{(1)})h^{\prime(t)}( \mathbf{x}_{i}^{(1)})y_{i}\boldsymbol{\mu}\] \[+\frac{\eta}{nm\tau}\sum_{i=1}^{n}(1-\ell_{i}^{\prime(t)})h_{r}^{ (t)}(\widehat{\mathbf{x}}_{i}^{(2)})h_{r}^{\prime(t)}(\mathbf{x}_{i}^{(2)}) \boldsymbol{\xi}_{i}-\frac{\eta}{nm\tau}\sum_{i=1}^{n}\sum_{j\neq i}^{M}\ell_{ i,j}^{\prime(t)}h_{r}^{(t)}(\mathbf{x}_{j}^{(1)})h^{\prime(t)}(\mathbf{x}_{i}^{(1)})y_ {i}\boldsymbol{\mu}\] \[-\frac{\eta}{nm\tau}\sum_{i=1}^{n}\sum_{j\neq i}^{M}\ell_{i,j}^{ \prime(t)}h_{r}^{(t)}(\mathbf{x}_{j}^{(2)})h_{r}^{\prime(t)}(\mathbf{x}_{i}^{ (2)})\boldsymbol{\xi}_{i}.\]

By comparing the coefficients in front of \(\boldsymbol{\mu}\) and \(\boldsymbol{\xi}_{i}\) on both sides of the equation, we can obtain

\[\gamma_{r}^{(t+1)}=\gamma_{r}^{(t)}+\frac{\eta}{nm\tau}\sum_{i=1} ^{n}\big{[}(1-\ell_{i}^{\prime(t)})h_{r}^{(t)}(\widehat{\mathbf{x}}_{i}^{(1)} )-\sum_{j\neq i}^{M}\ell_{i,j}^{\prime(t)}h_{r}^{(t)}(\mathbf{x}_{j}^{(1)}) \big{]}h_{r}^{\prime(t)}(\mathbf{x}_{i}^{(1)})y_{i}\|\boldsymbol{\mu}\|_{2}^{2},\] \[\rho_{r,i}^{(t+1)}=\rho_{r,i}^{(t)}+\frac{\eta}{nm\tau}\big{[}(1- \ell_{i}^{\prime(t)})h_{r}(\widehat{\mathbf{x}}_{i}^{(2)})-\sum_{j\neq i}^{M} \ell_{i,j}^{\prime(t)}h_{r}(\mathbf{x}_{j}^{(2)})\big{]}h_{r}^{\prime(t)}( \mathbf{x}_{i}^{(2)})\|\boldsymbol{\xi}_{i}\|_{2}^{2},\]

which completes the proof. 

According to the behavior of the defined loss derivative (8), we split the entire training dynamics into two phases. In the first stage, the loss derivative remains close to its initial value as the similarity is small from initialization. Later, as the similarity grows to a constant value, the loss derivative is no longer close to the initial value, and the dynamics transition to the second stage. In this stage, the similarity increases logarithmically, and the empirical loss converges.

### First Stage

In the first stage, the derivative of the loss is close to its initial value because the similarity is small. Below, we provide a useful lemma for establishing such a result.

**Lemma C.1**.: _Suppose that \(\gamma_{r}^{(t)}=O(1)\) and \(\rho_{r,i}^{(t)}=O(1)\) for all \(r\in[m]\) and \(i\in[n]\). Under Assumption 4.1, then for any \(\delta>0\), with probability at least \(1-\delta\)_

\[|\langle\mathbf{w}_{r}^{(t)}-\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_ {i}\rangle-\rho_{r,i}^{(t)}| \leq 5\sqrt{\frac{\log(6n^{2}/\delta)}{d}}n\] \[|\langle\mathbf{w}_{r}^{(t)}-\mathbf{w}_{r}^{(0)},\boldsymbol{ \mu}\rangle-\gamma_{r}^{(t)}| \leq\mathrm{SNR}\sqrt{\frac{8\log(6n/\delta)}{d}}n\]

_for all \(r\in[m]\), \(i\in[n]\)._

Proof of Lemma c.1.: From the signal-noise decomposition of \(\mathbf{w}_{r}^{(t)}\), we derive

\[|\langle\mathbf{w}_{r}^{(t)}-\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i} \rangle-\rho_{r,i}^{(t)}| \stackrel{{(a)}}{{=}}|\gamma_{r}^{(t)}\langle \boldsymbol{\mu},\boldsymbol{\xi}_{i}\rangle\|\boldsymbol{\mu}\|_{2}^{-2}+ \sum_{i^{\prime}=1}^{n}\rho_{r,i}^{(t)}\langle\boldsymbol{\xi}_{i^{\prime}}, \boldsymbol{\xi}_{i}\rangle\|\boldsymbol{\xi}_{i^{\prime}}\|_{2}^{-2}|\] \[\stackrel{{(b)}}{{\leq}}\|\boldsymbol{\mu}\|_{2}^{-1} \sigma_{\xi}\sqrt{2\log(6n/\delta)}+4\sqrt{\frac{\log(6n^{2}/\delta)}{d}}n\]\[\overset{(c)}{\leq}5\sqrt{\frac{\log(6n^{2}/\delta)}{d}}n.\]

Equation (a) results from the weight decomposition (see Equation 11). In the first stage, we used the upper bounds for \(|\gamma_{r}^{(t)}|\) and \(|\rho_{r}^{(t)}|\), and applied Lemma B.5 in inequality (b). Finally, inequality (c) follows from the condition \(n\mathrm{SNR}^{2}=\Theta(1)\).

Further,

\[|\langle\mathbf{w}_{r}^{(t)}-\mathbf{w}_{r}^{(0)},\boldsymbol{\mu}\rangle- \gamma_{r}^{(t)}|=|\sum_{i=1}^{n}\rho_{r,i}^{(t)}||\boldsymbol{\xi}_{i}\|_{2}^ {-2}\langle\boldsymbol{\xi}_{i},\boldsymbol{\mu}\rangle|\leq 2n\cdot\mathrm{SNR} \sqrt{\frac{2\log(6n/\delta)}{d}},\]

where we have used Lemma B.5. 

Now, we proceed to the lemma concerning the derivative of the loss as follows:

**Lemma C.2**.: _If \(\max\{\gamma_{r}^{(t)},\rho_{r,i}^{(t)}\}=O(1)\) and under Assumption 4.1, there exists a constant \(C_{\ell}>1\) such that_

\[\frac{1}{C_{\ell}(1+M)}\leq\ell_{i}^{\prime(t)}\leq\frac{C_{\ell}}{1+M},\quad \frac{1}{C_{\ell}(M+1)}\leq\ell_{i,j}^{\prime(t)}\leq\frac{C_{\ell}}{1+M},\]

_for all \(i\in[n]\)._

Proof of Lemma C.2.: From the update of \(\mathbf{w}_{r}^{(t)}\), we have

\[|\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{i}\rangle| \overset{(a)}{\leq} |\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}\rangle|+\rho_{r, i}^{(t)}+5\sqrt{\frac{\log(6n^{2}/\delta)}{d}}n\] \[\overset{(b)}{\leq} 2\sqrt{\log(8mn/\delta)}\sigma_{0}\sigma_{\xi}\sqrt{d}+\rho_{r, i}^{(t)}+5\sqrt{\frac{\log(6n^{2}/\delta)}{d}}n\] \[\overset{(c)}{=} O(1),\]

where (a) is by Lemma C.1, and (b) is by Lemma B.4. Finally, in inequality (c) we have used the condition that \(\sigma_{0}\leq\frac{1}{2\sqrt{\log(8mn/\delta)}\sigma_{\xi}\sqrt{d}}\) and \(d>n^{2}\log(6n^{2}/\delta)\) according to Assumption 4.1, and \(\max\{\gamma_{r}^{(t)},\rho_{r,i}^{(t)}\}=O(1)\). At the same time,

\[|\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\mu}\rangle| \overset{(a)}{\leq} |\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\mu}\rangle|+\gamma_{r}^{(t)}+ \mathrm{SNR}\sqrt{\frac{8\log(6n/\delta)}{d}}n\] \[\overset{(b)}{\leq} \sqrt{2\log(8m/\delta)}\sigma_{0}\|\boldsymbol{\mu}\|_{2}+\mathrm{ SNR}\sqrt{\frac{8\log(6n/\delta)}{d}}n\] \[\overset{(c)}{=} O(1),\]

where (a) is by Lemma C.1, (b) is Lemma B.4, and (c) is by \(\sigma_{0}\leq\frac{1}{2\sqrt{\log(8m/\delta)}\|\boldsymbol{\mu}\|_{2}}\) and \(d>\mathrm{SNR}^{2}n^{2}\log(6n^{2}/\delta)\) according to Assumption 4.1, and \(\max\{\gamma_{r}^{(t)},\rho_{r,i}^{(t)}\}=O(1)\). Besides,

\[|\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\epsilon}_{i}\rangle| =|\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\epsilon}_{i}\rangle+ \gamma_{r}^{(t)}\|\boldsymbol{\mu}\|_{2}^{-2}\langle\boldsymbol{\mu}, \boldsymbol{\epsilon}_{i}\rangle+\sum_{i=i}^{n}\rho_{r,i}^{(t)}\|\boldsymbol{ \xi}_{i}\|_{2}^{-2}\langle\boldsymbol{\xi}_{i},\boldsymbol{\epsilon}_{i}\rangle|\] \[\overset{(a)}{\leq} |\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\epsilon}_{i}\rangle|+ \|\boldsymbol{\mu}\|_{2}^{-1}\sigma_{\epsilon}\sqrt{2\log(6n/\delta)}+4\sqrt{ \frac{\log(6n^{2}/\delta)}{d}}\sigma_{\epsilon}\sigma_{\xi}^{-1}n\] \[\overset{(b)}{\leq} 2\sqrt{\log(8mn/\delta)}\sigma_{0}\sigma_{\epsilon}\sqrt{d}+\| \boldsymbol{\mu}\|_{2}^{-1}\sigma_{\epsilon}\sqrt{2\log(6n/\delta)}+4\sqrt{ \frac{\log(6n^{2}/\delta)}{d}}\sigma_{\epsilon}\sigma_{\xi}^{-1}n\] \[\overset{(c)}{=} O(1),\]

where (a) follows from Lemma B.5, (b) from Lemma B.4, and (c) from the conditions \(\sigma_{0}\leq\frac{1}{2\sqrt{\log(8mn/\delta)}\sigma_{\epsilon}\sqrt{d}}\), \(\sigma_{\epsilon}\leq\frac{\|\boldsymbol{\mu}\|_{2}}{\sqrt{2\log(6n/\delta)}}\), \(d>n^{2}\log(6n^{2}/\delta)\), and \(\sigma_{\epsilon}<\sigma_{\xi}\).

Next, we calculate the upper bound of the similarity measure. First, we examine the negative pair. For any \(i,j\in[n]\), we have

\[\mathrm{Sim}_{\mathbf{h}}(\mathbf{x}_{i},\mathbf{x}_{j}) =\frac{1}{m}\langle\mathbf{h}(\mathbf{x}_{i}^{(1)}),\mathrm{sg}( \mathbf{h}(\mathbf{x}_{j}^{(1)}))\rangle+\frac{1}{m}\langle\mathbf{h}(\mathbf{ x}_{i}^{(2)}),\mathrm{sg}(\mathbf{h}(\mathbf{x}_{j}^{(2)}))\rangle\] \[=\frac{1}{m}\sum_{r=1}^{m}\sigma(\langle\mathbf{w}_{r}^{(t)}, \boldsymbol{\xi}_{i}\rangle)\sigma(\langle\mathbf{w}_{r}^{(t)},\boldsymbol{ \xi}_{j}\rangle)+\frac{1}{m}\sum_{r=1}^{m}\sigma(\langle\mathbf{w}_{r}^{(t)},y _{i}\boldsymbol{\mu}\rangle)\sigma(\langle\mathbf{w}_{r}^{(t)},y_{j} \boldsymbol{\mu}\rangle)\] \[\leq\max\{|\langle\mathbf{w}_{r}^{(t)},y_{i}\boldsymbol{\mu} \rangle\langle\mathbf{w}_{r}^{(t)},y_{j}\boldsymbol{\mu}\rangle|,|\langle \mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{i}\rangle\langle\mathbf{w}_{r}^{(t)}, \boldsymbol{\xi}_{j}\rangle|\}=O(1).\]

Similarly, for positive pair,

\[\mathrm{Sim}_{\mathbf{h}}(\mathbf{x}_{i},\mathbf{\hat{x}}_{j}) =\frac{1}{m}\sum_{r=1}^{m}\sigma(\langle\mathbf{w}_{r}^{(t)}, \boldsymbol{\xi}_{i}\rangle)\sigma(\langle\mathbf{w}_{r}^{(t)},\boldsymbol{ \xi}_{i}+\boldsymbol{\epsilon}_{i}\rangle)+\frac{1}{m}\sum_{r=1}^{m}\sigma( \langle\mathbf{w}_{r}^{(t)},y_{i}\boldsymbol{\mu}\rangle)\sigma(\langle \mathbf{w}_{r}^{(t)},y_{i}\boldsymbol{\mu}\rangle)\] \[\leq\max\{|\langle\mathbf{w}_{r}^{(t)},y_{i}\boldsymbol{\mu} \rangle\langle\mathbf{w}_{r}^{(t)},y_{j}\boldsymbol{\mu}\rangle|,|\langle \mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{i}\rangle\langle\mathbf{w}_{r}^{(t)}, \boldsymbol{\xi}_{i}+\boldsymbol{\epsilon}_{i}\rangle|\}=O(1).\]

According to the above result, we can say that \(1\leq e^{\mathrm{Sim}_{\mathbf{h}}(\mathbf{x},\mathbf{x}^{\prime})}\leq C_{\ell}\), where \(C_{\ell}\) is a positive constant. Then we can provide the upper bound for \(\ell_{i}^{\prime}\) and \(\ell_{i,j}^{\prime}\)

\[\ell_{i}^{\prime(t)} =\frac{e^{\mathrm{Sim}_{\mathbf{h}}(\mathbf{x}_{i},\mathbf{\hat{ x}}_{i})/\tau}}{e^{\mathrm{Sim}_{\mathbf{h}}(\mathbf{x}_{i},\mathbf{\hat{x}}_{i})/ \tau}+\sum_{j\neq i}^{M}e^{\mathrm{Sim}_{\mathbf{h}}(\mathbf{x}_{i},\mathbf{x} _{j})/\tau}}\leq\frac{C_{\ell}}{1+M},\] \[\ell_{i}^{\prime(t)} =\frac{e^{\mathrm{Sim}_{\mathbf{h}}(\mathbf{x}_{i},\mathbf{\hat{ x}}_{i})/\tau}}{e^{\mathrm{Sim}_{\mathbf{h}}(\mathbf{x}_{i},\mathbf{\hat{x}}_{i})/ \tau}+\sum_{j\neq i}^{M}e^{\mathrm{Sim}_{\mathbf{h}}(\mathbf{x}_{i},\mathbf{x} _{j})/\tau}}\geq\frac{1}{C_{\ell}(1+M)},\] \[\ell_{i,j}^{\prime(t)} =\frac{e^{\mathrm{Sim}_{\mathbf{h}}(\mathbf{x}_{i},\mathbf{x}_{j} )/\tau}}{e^{\mathrm{Sim}_{\mathbf{h}}(\mathbf{x}_{i},\mathbf{\hat{x}}_{i})/ \tau}+\sum_{j\neq i}^{M}e^{\mathrm{Sim}_{\mathbf{h}}(\mathbf{x}_{i},\mathbf{x} _{j})/\tau}}\geq\frac{1}{C_{\ell}(M+1)},\] \[\ell_{i,j}^{\prime(t)} =\frac{e^{\mathrm{Sim}_{\mathbf{h}}(\mathbf{x}_{i},\mathbf{x}_{j} )/\tau}}{e^{\mathrm{Sim}_{\mathbf{h}}(\mathbf{x}_{i},\mathbf{\hat{x}}_{i})/ \tau}+\sum_{j\neq i}^{M}e^{\mathrm{Sim}_{\mathbf{h}}(\mathbf{x}_{i},\mathbf{x} _{j})/\tau}}\leq\frac{C_{\ell}}{M+1}.\]

This completes the proof. 

#### c.1.1 Dynamics of Signal Learning: Upper Bound

In the first stage, the growth rate of signal learning is exponential. We establish an upper bound for the growth of signal learning.

Then we consider the growth of signal learning coefficient \(\gamma_{r}^{(t)}\). Depending on the initialization, we define \(\mathcal{U}_{+}^{(t)}=\{r\in[m]:\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\mu} \rangle>0\}\) and \(\mathcal{U}_{-}^{(t)}=\{r\in[m]:\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\mu} \rangle<0\}\).

**Lemma C.3**.: _Under the condition \(d\geq\frac{400n}{\sigma_{o}\sigma_{\ell}}\sqrt{\frac{\log(6n/\delta)}{-\pi\log( 1-\delta^{2}/(4m^{2}))}}\) and Assumption 4.1, for all \(t\geq 0\), we have \(\mathcal{U}_{+}^{(t)}=\mathcal{U}_{+}^{(0)}\), \(\mathcal{U}_{-}^{(t)}=\mathcal{U}_{-}^{(0)}\) and \(\gamma_{r}^{(t)}>0\) is an increasing sequence for all \(r\in\mathcal{U}_{+}^{(0)}\) and \(\gamma_{r}^{(t)}\leq 0\) and is a decreasing sequence for all \(r\in\mathcal{U}_{-}^{(0)}\)._

Proof of Lemma c.3.: We prove the claims by induction. To better understand the dynamics, we first derive the propagation for signal learning from the first step. For \(r\in\mathcal{U}_{+}^{(0)}\), i.e., \(\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\mu}\rangle>0\), we can see

\[\gamma_{r}^{(1)} =\gamma_{r}^{(0)}+\frac{\eta}{nm\tau}\sum_{i=1}^{n}(1-\ell_{i}^{ \prime(0)})\sigma(\langle\mathbf{w}_{r}^{(0)},y_{i}\boldsymbol{\mu}\rangle) \sigma^{\prime}(\langle\mathbf{w}_{r}^{(0)},y_{i}\boldsymbol{\mu}\rangle)y_{i} \|\boldsymbol{\mu}\|_{2}^{2}\] \[\quad-\frac{\eta}{nm\tau}\sum_{i=1}^{n}\sum_{j\neq i}^{M}\ell_{i,j}^{ \prime(0)}\sigma(\langle\mathbf{w}_{r}^{(0)},y_{j}\boldsymbol{\mu}\rangle) \sigma^{\prime}(\langle\mathbf{w}_{r}^{(0)},y_{i}\boldsymbol{\mu}\rangle)y_{i} \|\boldsymbol{\mu}\|_{2}^{2}\] \[=\gamma_{r}^{(0)}+\frac{\eta}{nm\tau}\sum_{i:y_{i}=1}^{n}(1-\ell_{i}^ {\prime(0)})\sigma(\langle\mathbf{w}_{r}^{(0)},y_{i}\boldsymbol{\mu}\rangle) \sigma^{\prime}(\langle\mathbf{w}_{r}^{(0)},y_{i}\boldsymbol{\mu}\rangle)y_{i} \|\boldsymbol{\mu}\|_{2}^{2}\]\[-\frac{\eta}{nm\tau}\sum_{i:y_{i}=1}^{n}\sum_{j:y_{j}=-1}^{M}\ell_{i,j }^{\prime(0)}\sigma(\langle\mathbf{w}_{r}^{(0)},y_{j}\boldsymbol{\mu}\rangle) \sigma^{\prime}(\langle\mathbf{w}_{r}^{(0)},y_{i}\boldsymbol{\mu}\rangle)y_{i} \|\boldsymbol{\mu}\|_{2}^{2}\] \[=\frac{\eta}{nm\tau}\sum_{i:y_{i}=1}^{n}(1-\ell_{i}^{\prime(0)} \rangle\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\mu}\rangle\|\boldsymbol{\mu} \|_{2}^{2}>0,\]

where last equality is by \(\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\mu}\rangle>0\), \(\gamma_{r}^{(0)}=0\), and the fact that negative samples satisfy \(y_{j}\neq y_{i}\). Thus, we verify that the sign of \(\gamma_{r}^{(1)}\) follow its initialization and \(\gamma_{r}^{(1)}>0\).

Next, we show the propagation of inner product at \(t=1\), for \(r\in\mathcal{U}_{+}^{(0)}\), we have

\[\langle\mathbf{w}_{r}^{(1)},\boldsymbol{\mu}\rangle \stackrel{{(a)}}{{\geq}}\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\mu}\rangle+\gamma_{r}^{(1)}-\mathrm{SNR}\sqrt{\frac{8\log(6n/ \delta)}{d}}n\] \[\stackrel{{(b)}}{{\geq}}0.99\langle\mathbf{w}_{r}^ {(0)},\boldsymbol{\mu}\rangle+\gamma_{r}^{(1)}>0,\]

where inequality (a) is by Lemma C.1, the second inequality (b) is by Lemma B.2, and the last by \(\gamma_{r}^{(1)}>0\). Hence we verify \(\mathcal{U}_{+}^{(1)}=\mathcal{U}_{+}^{(0)}\).

Now suppose at iteration \(t\), the claims are satisfied, namely \(\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\mu}\rangle>0\) and \(\gamma_{r}^{(t)}\geq\gamma_{r}^{(t-1)}\geq 0\) for \(r\in\mathcal{U}_{+}^{(0)}\). Then following similar argument, for \(r\in\mathcal{U}_{+}^{(0)}\)

\[\gamma_{r}^{(t+1)}=\gamma_{r}^{(t)}+\frac{\eta}{nm\tau}\sum_{i:y_{i}=1}^{n}(1- \ell_{i}^{\prime(t)})\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\mu}\rangle\| \boldsymbol{\mu}\|_{2}^{2}\geq\gamma_{r}^{(t)}\geq 0,\]

where we use the induction condition that \(\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\mu}\rangle>0\). Further by Lemma C.1 and B.2

\[\langle\mathbf{w}_{r}^{(t+1)},\boldsymbol{\mu}\rangle\geq 0.99\langle \mathbf{w}_{r}^{(0)},\boldsymbol{\mu}\rangle+\gamma_{r}^{(t+1)}>0,\]

where we use \(\gamma_{r}^{(t+1)}\geq 0\). This completes the induction for \(r\in\mathcal{U}_{+}^{(0)}\).

Similarly, for those neuron \(r\) that satisfies \(\langle\mathbf{w}_{r}^{0},\boldsymbol{\mu}\rangle<0\), i.e., \(r\in\mathcal{U}_{-}^{(0)}\), we have

\[\gamma_{r}^{(1)} =\gamma_{r}^{(0)}+\frac{\eta}{nm\tau}\sum_{i=1}^{n}(1-\ell_{i}^{ \prime(0)})\sigma(\langle\mathbf{w}_{r}^{(0)},y_{i}\boldsymbol{\mu}\rangle) \sigma^{\prime}(\langle\mathbf{w}_{r}^{(0)},y_{i}\boldsymbol{\mu}\rangle)y_{i} \|\boldsymbol{\mu}\|_{2}^{2}\] \[\quad-\frac{\eta}{nm\tau}\sum_{i=1}^{n}\sum_{j\neq i}^{M}\ell_{i,j }^{\prime(0)}\sigma(\langle\mathbf{w}_{r}^{(0)},y_{j}\boldsymbol{\mu}\rangle) \sigma^{\prime}(\langle\mathbf{w}_{r}^{(0)},y_{i}\boldsymbol{\mu}\rangle)y_{i} \|\boldsymbol{\mu}\|_{2}^{2}\] \[=-\frac{\eta}{nm\tau}\sum_{i:y_{i}=-1}^{n}(1-\ell_{i}^{\prime(0)}) \langle\mathbf{w}_{r}^{(0)},y_{i}\boldsymbol{\mu}\rangle\|\boldsymbol{\mu}\|_{ 2}^{2}<0,\]

where the last equality is by \(\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\mu}\rangle<0\), \(y_{i}=-1\), the property of ReLU activation, and the fact that \(y_{j}\neq y_{i}\) in the negative pair term. Hence we see \(\gamma_{r}^{(1)}\leq\gamma_{r}^{(0)}=0\).

Similarly, for the inner product at \(t=1\),

\[\langle\mathbf{w}_{r}^{(1)},\boldsymbol{\mu}\rangle =\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\mu}\rangle+\gamma_{r}^{ (1)}+\sum_{i=1}^{n}\rho_{r,i}^{(t)}\|\boldsymbol{\xi}_{i}\|_{2}^{-2}\langle \boldsymbol{\xi}_{i},\boldsymbol{\mu}\rangle\] \[\leq\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\mu}\rangle+\gamma_{r }^{(1)}+\mathrm{SNR}\sqrt{\frac{8\log(6n/\delta)}{d}}n\] \[\leq-0.99|\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\mu}\rangle|+ \gamma_{r}^{(1)}<0,\]

where the first inequity is by Lemma C.1, the second inequality follows from Lemma B.2, and the last inequality follows from \(\gamma_{r}^{(1)}\leq 0\).

Now suppose at iteration \(t\), the claims are satisfied, namely \(\langle\mathbf{w}_{r}^{(t)},\bm{\mu}\rangle<0\) and \(\gamma_{r}^{(t)}\leq\gamma_{r}^{(t-1)}\leq 0\) for \(r\in\mathcal{U}_{+}^{(0)}\). Then following similar argument, for \(r\in\mathcal{U}_{-}^{(0)}\)

\[\gamma_{r}^{(t+1)}=\gamma_{r}^{(t)}-\frac{\eta}{nm\tau}\sum_{i:y_{i}=-1}^{n}(1- \ell_{i}^{\prime(t)})\langle\mathbf{w}_{r}^{(t)},\bm{\mu}\rangle\|\bm{\mu}\|_{2 }^{2}\leq\gamma_{r}^{(t)}\leq 0,\]

where we use the induction condition that \(\langle\mathbf{w}_{r}^{(t)},\bm{\mu}\rangle<0\). Further

\[\langle\mathbf{w}_{r}^{(t+1)},\bm{\mu}\rangle =\langle\mathbf{w}_{r}^{(0)},\bm{\mu}\rangle+\gamma_{r}^{(t+1)}+ \sum_{i=1}^{n}\rho_{r,i}^{(t)}\|\bm{\xi}_{i}\|_{2}^{-2}\langle\bm{\xi}_{i},\bm {\mu}\rangle\] \[\stackrel{{(a)}}{{\leq}}\langle\mathbf{w}_{r}^{(0)},\bm{\mu}\rangle+\gamma_{r}^{(t+1)}+\mathrm{SNR}\sqrt{\frac{8\log(6n/\delta)} {d}}n\stackrel{{(b)}}{{<}}0,\]

where inequality (a) follows from Lemma C.1; we use \(\gamma_{r}^{(t+1)}\leq 0\) and Lemma B.2 in deriving inequality (b). This completes the induction for \(r\in\mathcal{U}_{-}^{(0)}\).

With Lemma C.3 at hand, we are ready to demonstrate the upper bound of the growth rate for signal learning.

**Lemma C.4**.: _With the same condition as in Lemma C.2 and Lemma C.3 and \(n\geq 2500\log(4/\delta)\), define \(A_{r}^{(t)}=\gamma_{r}^{(t)}+\langle\mathbf{w}_{r}^{(0)},\bm{\mu}\rangle\) for \(r\in\mathcal{U}_{+}^{(0)}\); and \(A_{r}^{(t)}=-\gamma_{r}^{(t)}-\langle\mathbf{w}_{r}^{(0)},\bm{\mu}\rangle\) for \(r\in\mathcal{U}_{-}^{(0)}\). With probability at least \(1-\delta\), we have_

\[A_{r}^{(t)}\leq\left(1+\frac{0.52\eta\|\bm{\mu}\|_{2}^{2}}{m\tau}\right)A_{r}^ {(0)}.\]

Proof.: Lemma C.3 suggests that for \(r\in[m]\), we can upper bound for \(|\gamma_{r}^{(t)}|\). Without loss of generality, we consider \(r\in\mathcal{U}_{+}^{(0)}\). By Lemma C.1, we can see

\[\langle\mathbf{w}_{r}^{(t)},\bm{\mu}\rangle \leq\gamma_{r}^{(t)}+\langle\mathbf{w}_{r}^{(0)},\bm{\mu}\rangle +\mathrm{SNR}\sqrt{\frac{8\log(6n/\delta)}{d}}n\] \[\leq 1.01\big{(}\gamma_{r}^{(t)}+\langle\mathbf{w}_{r}^{(0)},\bm{ \mu}\rangle\big{)},\] (19) \[\langle\mathbf{w}_{r}^{(t)},\bm{\mu}\rangle \geq\gamma_{r}^{(t)}+\langle\mathbf{w}_{r}^{(0)},\bm{\mu}\rangle -\mathrm{SNR}\sqrt{\frac{8\log(6n/\delta)}{d}}n\] \[\geq 0.99\big{(}\gamma_{r}^{(t)}+\langle\mathbf{w}_{r}^{(0)},\bm{ \mu}\rangle\big{)},\] (20)

where we use Lemma B.2 and \(\gamma_{r}^{(t)}\geq 0\).

Then, the update equation for \(\gamma_{r}^{(t)}\) follows

\[\gamma_{r}^{(t+1)}=\gamma_{r}^{(t)}+\frac{\eta}{nm\tau}\sum_{i:y_{i}=1}^{n}(1- \ell_{i}^{\prime(t)})\langle\mathbf{w}_{r}^{(t)},\bm{\mu}\rangle\|\bm{\mu}\|_{2 }^{2}.\]

Then we find that

\[A_{r}^{(t+1)} =A_{r}^{(t)}+\frac{\eta}{nm\tau}\sum_{i:y_{i}=1}(1-\ell_{i}^{(t)} )\langle\mathbf{w}_{r}^{(t)},\bm{\mu}\rangle\|\bm{\mu}\|_{2}^{2}\] \[\stackrel{{(a)}}{{\leq}}A_{r}^{(t)}+\frac{\eta}{nm \tau}\sum_{i:y_{i}=1}\left(1-\frac{1}{C_{\ell}(M+1)}\right)\|\bm{\mu}\|_{2}^{2} 1.01A_{r}^{(t)}\] \[\stackrel{{(b)}}{{\leq}}A_{r}^{(t)}+\frac{\eta\|\bm{ \mu}\|_{2}^{2}}{m\tau}\left(\frac{1}{2}+\sqrt{\frac{1}{2n}\log(4/\delta)} \right)1.01A_{r}^{(t)}\] \[\stackrel{{(c)}}{{\leq}}\left(1+\frac{0.52\eta\| \bm{\mu}\|_{2}^{2}}{m\tau}\right)A_{r}^{(t)},\] (21)where the first inequality (a) is by (19) and Lemma C.2. The second inequality (b) is by Lemma B.3 and \(1-\frac{1}{C_{\varepsilon}(M+1)}<1\). The last inequality (c) is by the condition \(n\geq 2500\log(4/\delta)\).

#### c.1.2 Dynamics of Noise Memorization: Lower Bound

To establish the lower bound of noise memorization in the first stage, we require that the added noise level \(\sigma_{\epsilon}\leq\sigma_{\xi}/C^{\prime}\) for sufficiently large, with \(C^{\prime}\geq\widetilde{\Omega}(1)\). We prove the following result that upper bound the scale of \(\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\epsilon}_{i}\rangle\).

**Lemma C.5**.: _Under the same condition as Lemma C.1 and \(d=\widetilde{\Omega}(\max\{\sigma_{0}^{-2}\|\boldsymbol{\mu}\|_{2}^{-2},n \sigma_{0}^{-1}\sigma_{\xi}^{-1}\})\), there exists a sufficiently large constant \(C_{\xi}>0\) such that_

\[|\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}\rangle|\geq C_{\xi}| \langle\mathbf{w}_{r}^{(t)},\boldsymbol{\epsilon}_{i}\rangle|\]

_for all \(i\in[n]\)._

Proof of Lemma c.5.: According to the decomposition of \(\mathbf{w}_{r}^{(t)}\), we can show

\[|\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\epsilon}_{i}\rangle| =|\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\epsilon}_{i}\rangle+ \gamma_{r}^{(t)}\|\boldsymbol{\mu}\|_{2}^{-2}\langle\boldsymbol{\mu}, \boldsymbol{\epsilon}_{i}\rangle+\sum_{i^{\prime}=1}^{n}\rho_{r,i^{\prime}}^{( t)}\|\boldsymbol{\xi}_{i^{\prime}}\|_{2}^{-2}\langle\boldsymbol{\xi}_{i^{\prime}}, \boldsymbol{\epsilon}_{i}\rangle|\] \[\leq|\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\epsilon}_{i}\rangle |+\gamma_{r}^{(t)}\|\boldsymbol{\mu}\|_{2}^{-2}|\langle\boldsymbol{\mu}, \boldsymbol{\epsilon}_{i}\rangle|+\sum_{i^{\prime}=1}^{n}|\rho_{r,i}^{(t)}|\| \boldsymbol{\xi}_{i^{\prime}}\|_{2}^{-2}|\langle\boldsymbol{\xi}_{i^{\prime}},\boldsymbol{\epsilon}_{i}\rangle|\] \[\stackrel{{(a)}}{{\leq}}2\sqrt{\log(8mn)/\delta} \sigma_{0}\sigma_{\epsilon}\sqrt{d}+\|\boldsymbol{\mu}\|_{2}^{-1}\sigma_{ \epsilon}\sqrt{2\log(6n/\delta)}+4\sigma_{\epsilon}\sigma_{\xi}^{-1}\sqrt{ \frac{\log(6n^{2}/\delta)}{d}}n\] \[\leq 1/C|\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}\rangle|,\]

where the second inequality (a) follows from Lemma B.5 and the last inequality is by the following anti-concentration result.

Because \(\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}\rangle\) is a Gaussian random variable with mean zero and variance \(\sigma_{0}\sigma_{\xi}\sqrt{d}\), by Lemma B.1, we compute

\[\mathbb{P}\big{(}|\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}\rangle|\leq c \big{)}\leq 2\sqrt{1-\exp\left(-\frac{2c^{2}}{\pi\sigma_{0}^{2}\sigma_{\xi}^{2}d }\right)},\]

When \(d\geq\frac{2c^{2}}{-\log(1-\delta^{2}/(4n^{2}))\pi\sigma_{0}^{2}\sigma_{\xi}^ {2}}\), we have \(\mathbb{P}\big{(}|\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}\rangle|\leq c \big{)}\leq\delta/n\) and by union bound, we have with probability at least \(1-\delta\), it holds \(|\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}\rangle|>c\). Here we let \(c=C\big{(}2\sqrt{\log(8mn)/\delta}\sigma_{0}\sigma_{\epsilon}\sqrt{d}+\| \boldsymbol{\mu}\|_{2}^{-1}\sigma_{\epsilon}\sqrt{2\log(6n/\delta)}+4\sigma_{ \epsilon}\sigma_{\xi}^{-1}\sqrt{\frac{\log(6n^{2}/\delta)}{d}}n\big{)}\geq C |\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\epsilon}_{i}\rangle|\). Then we can show the desired result with the condition \(d=\widetilde{\Omega}(\max\{\sigma_{0}^{-2}\|\boldsymbol{\mu}\|_{2}^{-2},n \sigma_{0}^{-1}\sigma_{\xi}^{-1}\})\). 

Define that \(\mathcal{I}_{r,+}^{(t)}=\{i:\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{i} \rangle>0\}\) and \(\mathcal{I}_{r,-}^{(t)}=\{i:\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{i} \rangle<0\}\). To show the result regarding \(\mathcal{I}_{r,+}^{(t)}\) and \(\mathcal{I}_{r,-}^{(t)}\), we prepare the following anti-concentration result:

**Lemma C.6**.: _Under the condition \(d\geq\sqrt{\frac{300\log(6n^{2}/\delta)}{-\log(1-\delta^{2}/4n^{2})\pi\sigma_{0 }^{2}\sigma_{\xi}^{2}}}\), then with probability at least \(1-\delta\), it satisfies_

\[|\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}\rangle|>150\sqrt{\log(6n^{2} /\delta)/d}.\]

Proof of Lemma c.6.: Here we want to show \(|\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}\rangle|\geq 150\sqrt{\log(6n^{2}/ \delta)/d}n\triangleq c\) with high probability. To see this, because \(\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}\rangle\) is a Gaussian random variable with mean zero and variance \(\sigma_{0}^{2}\sigma_{\xi}^{2}d\), by Lemma B.1, we compute

\[\mathbb{P}\big{(}|\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i} \rangle|\leq c\big{)}\leq 2\sqrt{1-\exp\left(-\frac{2c^{2}}{\pi\sigma_{0}^{2} \sigma_{\xi}^{2}d}\right)},\]

Thus when \(d\geq\sqrt{\frac{300\log(6n^{2}/\delta)}{-\log(1-\delta^{2}/4n^{2})\pi\sigma_{ 0}^{2}\sigma_{\xi}^{2}}}\), we have \(\mathbb{P}(|\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}\rangle|\leq c) \leq\delta/n\) and by union bound, we have with probability at least \(1-\delta\), it holds \(|\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}\rangle|>150\sqrt{\log(6n^{2} /\delta)/d}\). 

We then show that neurons with negative inner products with the noise at initialization would stay negative and the corresponding \(\rho\) stays zero.

**Lemma C.7**.: _Under the same condition as Lemma C.1 and Lemma C.6, for all \(t>0\), we have \(\mathcal{I}_{r,-}^{(t)}=\mathcal{I}_{r,-}^{(0)}\) and \(\rho_{r,i}^{(t)}=0\) for all \(i\in\mathcal{I}_{r,-}^{(0)}\)._

Proof of Lemma C.7.: The proof is by induction. We first consider \(i\in\mathcal{I}_{r,-}^{(0)}\). At \(t=1\),

\[\rho_{r,i}^{(1)} =\rho_{r,i}^{(0)}+\frac{\eta}{nm\tau}(1-\ell_{i}^{\prime(0)}) \sigma(\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}+\boldsymbol{\epsilon }_{i}\rangle)\sigma^{\prime}(\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i} \rangle)\|\boldsymbol{\xi}_{i}\|_{2}^{2}\] \[-\frac{\eta}{nm\tau}\sum_{j\neq i}^{M}\ell_{i,j}^{\prime(0)} \sigma(\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{j}\rangle)\sigma^{\prime }(\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}\rangle)\|\boldsymbol{\xi}_ {i}\|_{2}^{2}=0,\]

where we have used the condition that \(\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}\rangle<0\) and the property of ReLU activation.

Next we consider

\[\langle\mathbf{w}_{r}^{(1)},\boldsymbol{\xi}_{i}\rangle =\langle\mathbf{w}_{r}^{(0)}+\gamma_{r}^{(1)}\boldsymbol{\mu}\| \boldsymbol{\mu}\|_{2}^{-2}+\sum_{i=1}^{n}\rho_{r,i}^{(1)}\|\boldsymbol{\xi}_ {i}\|_{2}^{-2}\boldsymbol{\xi}_{i},\boldsymbol{\xi}_{i}\rangle\] \[=\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}\rangle+\gamma_ {r}^{(1)}\langle\boldsymbol{\mu},\boldsymbol{\xi}_{i}\rangle\|\boldsymbol{\mu} \|_{2}^{-2}+\rho_{r,i}^{(1)}+\sum_{i^{\prime}\neq i}^{n}\rho_{r,i^{\prime}}^{(1 )}\|\boldsymbol{\xi}_{i^{\prime}}\|_{2}^{-2}\langle\boldsymbol{\xi}_{i^{\prime }},\boldsymbol{\xi}_{i}\rangle\] \[\stackrel{{(a)}}{{\leq}}\langle\mathbf{w}_{r}^{(0)}, \boldsymbol{\xi}_{i}\rangle+5\sqrt{\frac{\log(6n^{2}/\delta)}{d}}n\stackrel{{ (b)}}{{<}}0,\]

where inequality (a) is by Lemma C.1 and \(\rho_{r,i}^{(1)}=0\), and inequality (b) is by Lemma C.6.

Suppose at iteration \(t\), the claim is satisfied, i.e., \(\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{i}\rangle<0\), \(\rho_{r,i}^{(t)}=0\), for all \(i\in\mathcal{I}_{r,-}^{(0)}\). Then

\[\rho_{r,i}^{(t+1)} =\rho_{r,i}^{(t)}+\frac{\eta}{nm\tau}(1-\ell_{i}^{\prime(t)}) \sigma(\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{i}+\boldsymbol{\epsilon }_{i}\rangle)\sigma^{\prime}(\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{i} \rangle)\|\boldsymbol{\xi}_{i}\|_{2}^{2}\] \[\quad-\frac{\eta}{nm\tau}\sum_{j\neq i}^{M}\ell_{i,j}^{\prime(t)} \sigma(\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{j}\rangle)\sigma^{\prime }(\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{i}\rangle)\|\boldsymbol{\xi}_ {i}\|_{2}^{2}<0,\]

Next we consider the update of inner product as

\[\langle\mathbf{w}_{r}^{(t+1)},\boldsymbol{\xi}_{i}\rangle =\langle\mathbf{w}_{r}^{(0)}+\gamma_{r}^{(t+1)}\boldsymbol{\mu} \|\boldsymbol{\mu}\|_{2}^{-2}+\sum_{i=1}^{n}\rho_{r,i}^{(t+1)}\|\boldsymbol{ \xi}_{i}\|_{2}^{-2}\boldsymbol{\xi}_{i},\boldsymbol{\xi}_{i}\rangle\] \[=\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}\rangle+\gamma_ {r}^{(t+1)}\langle\boldsymbol{\mu},\boldsymbol{\xi}_{i}\rangle\|\boldsymbol{\mu} \|_{2}^{-2}+\rho_{r,i}^{(t+1)}+\sum_{i^{\prime}\neq i}^{n}\rho_{r,i^{\prime}}^{(t +1)}\|\boldsymbol{\xi}_{i^{\prime}}\|_{2}^{-2}\langle\boldsymbol{\xi}_{i^{ \prime}},\boldsymbol{\xi}_{i}\rangle\] \[\leq\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}\rangle+\gamma_ {r}^{(t+1)}|\langle\boldsymbol{\mu},\boldsymbol{\xi}_{i}\rangle|\|_{2}^{-2}+ \sum_{i^{\prime}\neq i}^{n}|\rho_{r,i^{\prime}}^{(t+1)}||\boldsymbol{\xi}_{i^{ \prime}}\|_{2}^{-2}|\langle\boldsymbol{\xi}_{i^{\prime}},\boldsymbol{\xi}_{i}\rangle|\] \[\leq\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}\rangle+5 \sqrt{\frac{\log(6n^{2}/\delta)}{d}}n<0,\]

where the last inequality is by a anti-concentration analysis shown in Lemma C.6. This completes the induction for \(i\in\mathcal{I}_{r,-}^{(t)}\).

Before formally stating the main lemma on the lower bound of noise memorization, we prepare several lemmas that will be useful.

**Lemma C.8**.: _Suppose that \(\delta>0\). Then with probability \(1-\delta\), for all \(r\in[m]\), we have:_

\[\left|\sum_{i:y_{i}=1}\mathds{1}_{i\in\mathcal{I}^{(0)}_{r,+}}- \frac{n}{4}\right|\leq\sqrt{\frac{n}{2}\log(4/\delta)}.\]

Proof.: The proof is by Hoeffding's inequality, for arbitrary \(t>0\), we have that

\[\mathbb{P}\left(\left|\sum_{i:y_{i}=1}\mathds{1}_{i\in\mathcal{I} _{r,+}}-\mathbb{E}\left[\sum_{i:y_{i}=1}\mathds{1}_{i\in\mathcal{I}_{r,+}} \right]\right|\leq t\right)\leq 2\exp\left(-\frac{2t^{2}}{n}\right).\]

By the randomness of initialization and Rademacher distribution, we have:

\[\mathbb{E}\left[\sum_{i:y_{i}=1}\mathds{1}_{i\in\mathcal{I}_{r,+} }\right]=\frac{n}{4}\]

Setting \(t=\sqrt{n/2\log(4/\delta)}\) and taking a union bound over \(r\in[m]\), we conclude with high probability at least \(1-\delta\), it holds:

\[\left|\sum_{i:y_{i}=1}\mathds{1}_{i\in\mathcal{I}_{r,+}}-\frac{n }{4}\right|\leq\sqrt{\frac{n}{2}\log(4/\delta)}.\]

To establish the lower bound for noise memorization we define

\[B^{(t)}_{r,+}\triangleq\sum_{i:y_{i}=+1}(\rho^{(t)}_{r,i}+\langle \mathbf{w}^{(0)}_{r},\boldsymbol{\xi}_{i}\rangle)\mathds{1}_{i\in\mathcal{I}^ {(t)}_{r,+}},\quad B^{(t)}_{r,-}\triangleq\sum_{i:y_{i}=-1}(\rho^{(t)}_{r,i}+ \langle\mathbf{w}^{(0)}_{r},\boldsymbol{\xi}_{i}\rangle)\mathds{1}_{i\in \mathcal{I}^{(t)}_{r,+}}.\]

**Lemma C.9**.: _Suppose \(\delta>0\), the with probability at least \(1-\delta\), we have_

\[B^{(0)}_{r,+}\leq\sigma_{0}\sigma_{\xi}\sqrt{dn}(1+\sqrt{\log(1/ \delta)/2}),\quad B^{(0)}_{r,-}\leq\sigma_{0}\sigma_{\xi}\sqrt{dn}(1+\sqrt{ \log(1/\delta)/2}).\]

Proof.: By Bernstein's inequality, for arbitrary \(t>0\), we have

\[P(|B^{(0)}_{r,+}-\sigma_{0}\sigma_{\xi}\sqrt{nd}|>t)\leq\exp(- \frac{t^{2}}{2n/4\sigma_{0}^{2}\sigma_{\xi}^{2}d}).\]

Setting \(t=\sigma_{0}\sigma_{\xi}\sqrt{nd\log(1/\delta)/2}\), we further have

\[B^{(0)}_{r,+}\leq\sigma_{0}\sigma_{\xi}\sqrt{dn}(1+\sqrt{\log(1 /\delta)/2}).\]

Similarly, the same result holds for \(B^{(0)}_{r,-}\). 

**Lemma C.10**.: _For arbitrary constant \(C>0\), under the condition \(n\geq\frac{8C^{2}\log(1/\delta)}{\log(1/(1-(\delta/m)^{2}))}\), then with probability at least \(1-\delta\), it satisfies_

\[|\langle\mathbf{w}^{(0)}_{r},\boldsymbol{\xi}_{i}\rangle|>\frac{ CB^{(0)}_{r,+}}{n},\quad|\langle\mathbf{w}^{(0)}_{r},\boldsymbol{\xi}_{i} \rangle|>\frac{CB^{(0)}_{r,-}}{n}.\]

Proof of Lemma c.10.: Here we want to show \(|\langle\mathbf{w}^{(0)}_{r},\boldsymbol{\xi}_{i}\rangle|>\frac{B^{(0)}_{r,+} }{n}\) with high probability. To see this, because \(\langle\mathbf{w}^{(0)}_{r},\boldsymbol{\xi}_{i}\rangle\) is a random variable with positive mean and variance \(\sigma_{0}^{2}\sigma_{\xi}^{2}d\). Besides, by Lemma C.9, we have

\[\frac{B^{(0)}_{r,+}}{n}\leq\frac{\sigma_{0}\sigma_{\xi}\sqrt{dn}(1+\sqrt{\log (1/\delta)/2})}{n}.\]By Lemma B.1, we recall

\[\mathbb{P}\big{(}|\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i} \rangle|\leq t\big{)}\leq\sqrt{1-\exp\left(-\frac{8t^{2}}{\pi\sigma_{0}^{2}\sigma _{\xi}^{2}d}\right)},\]

Thus when \(n\geq\frac{8C^{2}\log(1/\delta)}{\log(1/(1-(\delta/m)^{2}))}\), we have

\[\mathbb{P}\big{(}|\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_ {i}\rangle|\leq\frac{C\sigma_{0}\sigma_{\xi}\sqrt{dn}(1+\sqrt{\log(1/\delta)/2 })}{n}\big{)}\leq\delta/m\]

and by union bound, we have with probability at least \(1-\delta\), it holds \(|\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}\rangle|>\frac{B_{r,+}^{(0)}} {n}\) and \(|\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}\rangle|>\frac{B_{r,-}^{(0)}} {n}\). 

Besides, we define

\[\Psi_{r,i}^{(t)} \triangleq\rho_{r,i}^{(t)}+\langle\mathbf{w}_{r}^{(0)}, \boldsymbol{\xi}_{i}\rangle,\text{ with }y_{i}=1,i\in\mathcal{I}_{r,+}^{(t)}\] \[\Phi_{r,i}^{(t)} \triangleq\rho_{r,i}^{(t)}+\langle\mathbf{w}_{r}^{(0)}, \boldsymbol{\xi}_{i}\rangle,\text{ with }y_{i}=-1,i\in\mathcal{I}_{r,+}^{(t)}\]

With all the results (lemmas) and definitions outlined above at hand, we are ready to state the lemmas that provide the lower bound for noise memorization as follows.

**Lemma C.11**.: _Under the same condition as Theorem 4.2, then with probability at least \(1-\delta\),_

\[\Psi_{r,i}^{(t)} \geq(1+\frac{0.96\eta\sigma_{\xi}^{2}d}{nm\tau})^{t}\Psi_{r,i}^{ (0)},\] (22) \[\Psi_{r,i}^{(t)} \geq\frac{101C_{\ell}}{M+1}B_{r,-}^{(t)},\] (23) \[\rho_{r,i:y_{i}=1}^{(t)}\mathds{1}_{i\in\mathcal{I}_{r,+}^{(t)}} \geq 0.\] (24)

Proof.: The proof is by induction. We can check

\[\rho_{r,i:y_{i}=1}^{(0)}\mathds{1}_{i\in\mathcal{I}_{r,+}}=0,\quad\Psi_{r,i}^ {(0)}\geq(1+\frac{0.96\eta\sigma_{\xi}^{2}d}{nm\tau})^{0}\Psi_{r,i}^{(0)}.\]

Besides, by Lemma C.10 and \(M\in\frac{n}{2}(1\pm o(n^{-1/2}))\), it holds that

\[\Psi_{r,i}^{(0)}=\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}\rangle\geq 1 01\frac{C_{\ell}}{M+1}B_{r,-}^{(0)}.\]

Assuming that the results hold at \(t\), we continue the proof by calculating the propagation of \(B_{r,+}^{(t)}\) and \(B_{r,-}^{(t)}\) respectively. First, we can see by Lemma C.1 and by induction, for \(y_{i}=1\) and \(i\in\mathcal{I}_{r,+}\),

\[\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{i}+\boldsymbol{ \epsilon}_{i}\rangle \geq\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}\rangle+\rho_ {r,i}^{(t)}-|\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\epsilon}_{i}\rangle|-6 \sqrt{\frac{\log(6n^{2}/\delta)}{d}}n\] \[\geq(1-o(1))\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i} \rangle+\rho_{r,i}^{(t)}\geq 0.\]

We can show that

\[B_{r,-}^{(t+1)} =B_{r,-}^{(t)}+\frac{\eta}{nm\tau}\sum_{i:y_{i}=-1}\left[(1-\ell_ {i}^{(t)})\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{i}+\boldsymbol{ \epsilon}_{i}\rangle-\sum_{j\neq i}\ell_{i,j}^{(t)}\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{j}\rangle\mathds{1}_{i\in\mathcal{I}_{r,+}}\right]\mathds{1 }_{i\in\mathcal{I}_{r,+}}\|\boldsymbol{\xi}_{i}\|_{2}^{2}\] \[\leq B_{r,-}^{(t)}+\frac{\eta}{nm\tau}\sum_{i:y_{i}=-1}\left[\frac {C_{\ell}(M+1)-1}{C_{\ell}(M+1)}(\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_ {i}\rangle+\rho_{r,i}^{(t)}+|\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\epsilon}_ {i}\rangle|+6\sqrt{\frac{\log(6n^{2}/\delta)}{d}}n)\right.\] \[\left.-\sum_{j\neq i}\mathds{1}_{j\in\mathcal{I}_{r,+}}\frac{1}{ C_{\ell}(M+1)}(\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{j}\rangle+\rho_{r,j}^{(t)} -6\sqrt{\frac{\log(6n^{2}/\delta)}{d}}n)\right]\!\left(\sigma_{\xi}^{2}d+ \sigma_{\xi}^{2}\sqrt{d\log(6n^{2}/\delta)}\right)\mathds{1}_{i\in\mathcal{I} _{r,+}}\]\[\leq B_{r,-}^{(t)}+\frac{\eta}{n\tau m}\bigg{[}\frac{C_{\ell}(M+1)-1}{C_{ \ell}(M+1)}.01B_{r,-}^{(t)}-\sum_{i:y_{i}=-1}\mathds{1}_{i\in\mathcal{I}_{r,+}} \frac{1}{C_{\ell}(M+1)}0.99B_{r,-}^{(t)}\bigg{]}(\sigma_{\xi}^{2}d+\sigma_{\xi} ^{2}\sqrt{d\log(6n^{2}/\delta)})\] \[\quad-\sum_{i:y_{i}=-1}\mathds{1}_{i\in\mathcal{I}_{r,+}}\frac{1 }{C_{\ell}(M+1)}(B_{r,-}^{(t)}-\sum_{j\neq i}6\sqrt{\frac{\log(6n^{2}/\delta)} {d}}n)\bigg{]}(\sigma_{\xi}^{2}d+\sigma_{\xi}^{2}\sqrt{d\log(6n^{2}/\delta)})\] \[\leq B_{r,-}^{(t)}+\frac{\eta}{nm\tau}\bigg{[}1.01B_{r,-}^{(t)}- \frac{0.99^{2}}{2C_{l}}B_{r,+}^{(t)}\bigg{]}1.035\sigma_{\xi}^{2}d\] \[\leq B_{r,-}^{(t)}+\frac{\eta}{nm\tau}\bigg{[}1.05B_{r,-}^{(t)}- 0.5B_{r,+}^{(t)}\bigg{]}\sigma_{\xi}^{2}d\] \[\leq B_{r,-}^{(t)}+\frac{1.05\eta\sigma_{\xi}^{2}d}{nm\tau}B_{r,- }^{(t)},\]

where the first inequality is by Lemma C.2, Lemma C.1 and Lemma B.5. Furthermore, the second inequality is by Lemma C.5, i.e., \(|\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\epsilon}_{i}\rangle|\leq 1/C_{\xi}| \langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}\rangle|\) (for \(C_{\xi}>200\)), \(|0.01\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}\rangle|\geq 6\sqrt{\log(6n^{2}/\delta)d^{-1}}n\), the definition of \(B_{r,+}^{(t)}\) and \(B_{r,-}^{(t)}\) and the induction \(B_{r,+}^{(t)}\geq B_{r,+}^{(t)}\), \(B_{r,-}^{(t)}\geq B_{r,-}^{(t)}\). The third inequality is by \(M\geq 100C_{\ell}-1\), \(M\in\frac{n}{2}(1\pm o(n^{-1/2}))\), \(d>1000\log(6n^{2}/\delta)\) and Lemma C.8. The last inequality is by \(B_{r,+}^{(t)}\geq 0\).

As a result, we conclude that

\[B_{r,-}^{(t)}\leq(1+\frac{1.05\eta\sigma_{\xi}^{2}d}{nm\tau})^{t}B_{r,-}^{(0)}.\]

Finally, the induction step for \(\Psi_{r,i}^{(t)}\) can be calculated as follows:

\[\Psi_{r,i}^{(t+1)} =\Psi_{r,i}^{(t)}+\frac{\eta}{nm\tau}\left[(1-\ell_{i}^{\prime(t) })\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{i}+\boldsymbol{\epsilon}_{i} \rangle-\sum_{j\neq i}\mathds{1}_{j\in\mathcal{I}_{r,+}}\ell_{i,j}^{\prime(t) }\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{j}\rangle\right]\mathds{1}_{i\in \mathcal{I}_{r,+}}\|\boldsymbol{\xi}_{i}\|_{2}^{2}\] \[\geq\Psi_{r,i}^{(t)}+\frac{\eta}{nm\tau}\bigg{[}\frac{(M+1)-C_{ \ell}}{(M+1)}(\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}\rangle+\rho_{r, i}^{(t)}-|\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\epsilon}_{i}\rangle|-6 \sqrt{\frac{\log(6n^{2}/\delta)}{d}}n)\] \[\quad-\sum_{j\neq i}\mathds{1}_{j\in\mathcal{I}_{r,+}}\frac{C_{l} }{M+1}(\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{j}\rangle+\rho_{r,j}^{(t) }+6\sqrt{\frac{\log(6n^{2}/\delta)}{d}}n)\bigg{]}(\sigma_{\xi}^{2}d-\sigma_{ \xi}^{2}\sqrt{d\log(6n^{2}/\delta)})\mathds{1}_{i\in\mathcal{I}_{r,+}}\] \[\geq\Psi_{r,i}^{(t)}+\frac{\eta}{nm\tau}\bigg{[}\frac{(M+1)-C_{ \ell}}{M+1}0.99\Psi_{r,i}^{(t)}-\frac{C_{\ell}}{M+1}1.01B_{r,-}^{(t)}\bigg{]}( \sigma_{\xi}^{2}d-\sigma_{\xi}^{2}\sqrt{d\log(6n^{2}/\delta)})\] \[\geq\Psi_{r,i}^{(t)}+\frac{\eta}{nm\tau}\bigg{[}0.99^{2}\Psi_{r,i} ^{(t)}-1.01\frac{C_{l}}{M+1}B_{r,-}^{(t)}\bigg{]}0.99\sigma_{\xi}^{2}d\] \[\geq\Psi_{r,i}^{(t)}+\frac{\eta\sigma_{\xi}^{2}d}{nm\tau}\left[0. 96\Psi_{r,i}^{(t)}\right],\]

where the first inequality is by Lemma C.2, Lemma C.1 and Lemma B.5. Furthermore, the second inequality is by Lemma C.5, i.e., \(|\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\epsilon}_{i}\rangle|\leq 1/C_{\xi}| \langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}\rangle|\) (for \(C_{\xi}>200\)), \(|0.01\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}\rangle|\geq 6\sqrt{\log(6n^{2}/\delta)d^{-1}}n\), the definition of \(B_{r,+}^{(t)}\) and \(B_{r,-}^{(t)}\). The third inequality is by \(M\geq 100C_{\ell}-1\). The last inequality is by induction (23).

Then we check induction induction (23) through following inequalities:

\[B_{r,-}^{(t)} \leq(1+\frac{1.05\eta\sigma_{\xi}^{2}d}{nm\tau})^{t}B_{r,-}^{(0)},\] \[\Psi_{r,i}^{(t)} \geq(1+\frac{0.96\eta\sigma_{\xi}^{2}d}{nm\tau})^{t}\Psi_{r,i}^{(0)}.\]

Together it confirms that \(\Psi_{r,i}^{(t)}\geq\frac{101C_{\ell}}{M+1}B_{r,-}^{(t)}\).

Finally, we check the sign of \(\rho^{(t)}_{r,i}\) for \(i:y_{i}=1\) and \(i\in\mathcal{I}^{(t)}_{r,+}\):

\[\rho^{(t)}_{r,i}=\Psi^{(t)}_{r,i}-\langle\mathbf{w}^{(0)}_{r},\bm{\xi}\rangle \geq(1+\frac{\eta\sigma_{\xi}^{2}d}{2nm\tau})^{t}\Psi^{(t)}_{r,i}-\langle \mathbf{w}^{(0)}_{r},\bm{\xi}\rangle>\frac{\eta\sigma_{\xi}^{2}d}{2nm\tau}\Psi^ {(t)}_{r,i}>0.\]

This completes the induction proof. 

#### c.1.3 Noise Memorization: Proof of Lemma 5.2

Before proving Lemma 5.2, we require a lower bound for the initialization. Define that \(\mathcal{U}^{(0)}_{+}=\{r:\langle\mathbf{w}^{(0)}_{r},\bm{\xi}_{i}\rangle>0\}\) for \(y_{i}=1\), and \(\mathcal{U}^{(0)}_{-}=\{r:\langle\mathbf{w}^{(0)}_{r},\bm{\xi}_{i}\rangle<0\}\) for \(y_{i}=-1\).

**Lemma C.12**.: _Suppose that \(\delta>0\) and \(m\geq\widetilde{\Omega}(1)\). Then with probability at least \(1-\delta\), we have_

\[\frac{1}{m}\sum_{r\in\mathcal{U}^{(0)}_{+}}\Psi^{(0)}_{r,i}\geq 0.2\sigma_{0} \sigma_{\xi}\sqrt{d},\quad\frac{1}{m}\sum_{r\in\mathcal{U}^{(0)}_{+}}\Phi^{(0 )}_{r,i}\geq 0.2\sigma_{0}\sigma_{\xi}\sqrt{d}.\]

Proof of Lemma c.12.: Consider \(y_{i}=1\). Note that \(\langle\mathbf{w}^{(0)}_{r},\bm{\xi}_{i}\rangle\sim\mathcal{N}(0,\sigma_{0}^ {2}\|\bm{\xi}_{i}\|_{2}^{2})\). We define that the event \(\mathcal{A}=\{r\in[m],\langle\mathbf{w}^{(0)}_{r},\bm{\xi}_{i}\rangle>0\}\). Then we can compute that \(\langle\mathbf{w}^{(0)}_{r},\bm{\xi}_{i}\rangle\mathds{1}(\mathcal{A})\) becomes a half-normal distribution with the expectation

\[\mathbb{E}[\langle\mathbf{w}^{(0)}_{r},\bm{\xi}_{i}\rangle\mathds{1}( \mathcal{A})]=\frac{\sqrt{2}\sigma_{0}\|\bm{\xi}_{i}\|_{2}}{\sqrt{\pi}}.\]

We then apply the sub-Gaussian concentration inequality that with probability at least \(1-\delta\)

\[\left|\sum_{r\in\mathcal{U}^{(0)}_{+}}\Psi^{(0)}_{r,i}-\frac{m\sigma_{0}\|\bm {\xi}_{i}\|_{2}}{\sqrt{2}\pi}\right|\leq\widetilde{O}(m^{-1/2}).\]

Then we have

\[\frac{1}{m}\sum_{r\in\mathcal{U}^{(0)}_{+}}\Psi^{(0)}_{r,i}\geq 0.4\sigma_{0}\| \bm{\xi}_{i}\|_{2}\geq\sigma_{0}\sigma_{\xi}\sqrt{d},\]

where we have used Lemma B.5. Similarly, we can show that for \(y_{i}=-1\), the same result holds.

Proof of Lemma 5.2.: From the upper bound on (21), we take the maximum over \(r\in\mathcal{U}^{(0)}_{+}\), which gives

\[\max_{r}A^{(t)}_{r} \leq\Big{(}1+0.52\frac{\eta\|\bm{\mu}\|_{2}^{2}}{m\tau}\Big{)}^{t }\max_{r}A^{(0)}_{r}\] \[\leq\Big{(}1+0.52\frac{\eta\|\bm{\mu}\|_{2}^{2}}{m\tau}\Big{)}^{t }\sigma_{0}\|\bm{\mu}\|_{2}\sqrt{2\log(8m/\delta)}.\]

Under the SNR condition \(n\cdot\mathrm{SNR}^{2}\leq 1.8\), we can see there exists a scale difference between \(\max_{r,i}\Psi^{(t)}_{r,i}\) and \(\max_{r,i}A^{(t)}_{r}\) at the end of first stage.

At the same time, for noise memorization, from the lower bound established in Lemma C.11, we have that

\[\frac{1}{m}\sum_{r\in\mathcal{U}^{(0)}_{+}}\Psi^{(t)}_{r,i} \geq(1+\frac{0.96\eta\sigma_{\xi}^{2}d}{nm\tau})^{t}\frac{1}{m} \sum_{r\in\mathcal{U}^{(0)}_{+}}\Psi^{(0)}_{r,i}\] \[\geq(1+\frac{0.96\eta\sigma_{\xi}^{2}d}{nm\tau})^{t}0.2\sigma_{0} \sigma\sqrt{d},\]

where the second inequality is due to Lemma C.12.

Let

\[T_{1}=\log\big{(}20/(\sigma_{0}\sigma_{\xi}\sqrt{d})\big{)}/\log\big{(}1+0.96\frac{ \eta\sigma_{\xi}^{2}d}{nm\tau}\big{)}.\]

Then we have \(\frac{1}{m}\sum_{r\in\mathcal{U}_{+}^{(0)}}\Psi_{r,i}^{(t)}\) reach \(3\) within \(T_{1}\) iterations by (22). Similarly, we can also show that \(\frac{1}{m}\sum_{r\in\mathcal{U}_{-}^{(0)}}\Phi_{r,i}^{(t)}\) reach \(3\) within \(T_{1}\) iterations.

On the other hand, we compute the scale of \(\max_{r}A_{r}^{(T_{1})}\) as

\[\max_{r}A_{r}^{(T_{1})} \leq\Big{(}1+0.52\frac{\eta\|\boldsymbol{\mu}\|_{2}^{2}}{m\tau} \Big{)}^{T_{1}}\sigma_{0}\|\boldsymbol{\mu}\|_{2}\sqrt{2\log(8m/\delta)}\] \[=\exp\Big{(}\frac{\log(1+0.52\frac{\eta\|\boldsymbol{\mu}\|_{2}^ {2}}{m\tau})}{\log(1+0.96\frac{\eta\sigma_{\xi}^{2}d}{nm\tau})}\log(12/(\sigma _{0}\sigma_{\xi}\sqrt{d})\Big{)}\sigma_{0}\|\boldsymbol{\mu}\|_{2}\sqrt{2 \log(8m/\delta)}\] \[\leq\exp\big{(}(0.55\cdot n\cdot\mathrm{SNR}^{2}+O((\frac{\eta\| \boldsymbol{\mu}\|_{2}^{2}}{m\tau})^{2})\log(20/(\sigma_{0}\sigma_{\xi}\sqrt{d }))\sigma_{0}\|\boldsymbol{\mu}\|_{2}\sqrt{2\log(8m/\delta)}\] \[\leq\exp\big{(}(0.55\cdot n\cdot\mathrm{SNR}^{2}+0.01)\log(30/( \sigma_{0}\sigma_{\xi}\sqrt{d})\big{)}\sigma_{0}\|\boldsymbol{\mu}\|_{2}\sqrt{ 2\log(8m/\delta)}\] \[\leq\exp\big{(}\log(30/(\sigma_{0}\sigma_{\xi}\sqrt{d})\big{)} \sigma_{0}\|\boldsymbol{\mu}\|_{2}\sqrt{2\log(8m/\delta)}\] \[=20\sqrt{2\log(8m/\delta)}\mathrm{SNR}\] \[=O(\sqrt{\log(m/\delta)/n})\] \[=\widetilde{O}(n^{-1/2})\]

where we choose \(\eta\) sufficiently small for the third inequality. The last inequality is by the SNR condition. Because we can choose \(n\geq C\log(m/\delta)\) for sufficiently large constant \(C\), \(\max_{r}A_{r}^{(T_{1})}=o(1)\).

### Second Stage

**Proposition C.13**.: _Let \(T^{*}\) be the maximum admissible iteration and let \(\alpha=\log(3MT^{*})\). Then we can show_

\[|\gamma_{r}^{(t)}|\leq\alpha,\quad|\rho_{r,i}^{(t)}|\leq\alpha.\]

Proof of Proposition c.13.: We need to show \(\rho_{r,i}^{(t)}\leq\alpha\). We prove the claim by induction. It is clear when \(t=0\), \(\rho_{r,i}^{(t)}=0\leq\alpha\). Suppose for all \(0\leq t\leq\widetilde{T}-1\), we have \(\rho_{r,i}^{(t)}\leq\alpha\). We aim to show the claim holds for \(\widetilde{T}\).

By the update of \(\rho_{r,i}^{(t)}\),

\[\rho_{r,i}^{(t+1)}\leq\rho_{r,i}^{(t)}+\frac{\eta}{nm\tau}(1-\ell_{i}^{\prime( t)})\sigma(\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{i}+\boldsymbol{ \epsilon}_{i}\rangle)\|\boldsymbol{\xi}_{i}\|_{2}^{2}.\]

Here without loss of generality, we consider the \(r,i\) pairs such that \(i\in\mathcal{I}_{r,+}^{(0)}\), i.e., \(\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}\rangle>0\) with \(y_{i}=1\). Let \(t_{r,i}\) be the last time \(t\) such that \(\rho_{r,i}^{(t)}\leq 0.5\alpha\). Then we have

\[\rho_{r,i}^{(\widetilde{T})} \leq\rho_{r,i}^{(t_{r})}+\frac{\eta}{nm\tau}(1-\ell_{i}^{\prime(t_ {r,i})})\sigma(\langle\mathbf{w}_{r}^{(t_{r,i})},\boldsymbol{\xi}_{i}+ \boldsymbol{\epsilon}_{i}\rangle)\|\boldsymbol{\xi}_{i}\|_{2}^{2}\] \[\quad+\sum_{t_{r,i}\leq t\leq\widetilde{T}}\frac{\eta}{nm\tau}(1- \ell_{i}^{\prime(t)})\sigma(\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{i}+ \boldsymbol{\epsilon}_{i}\rangle)\|\boldsymbol{\xi}_{i}\|_{2}^{2}.\] (25)

The second term can be bounded as

\[\frac{\eta}{nm\tau}(1-\ell_{i}^{\prime(t_{r,i})})\sigma(\langle \mathbf{w}_{r}^{(t_{r,i})},\boldsymbol{\xi}_{i}+\boldsymbol{\epsilon}_{i} \rangle)\|\boldsymbol{\xi}_{i}\|_{2}^{2}\leq\frac{\eta}{nm\tau}\big{(}\frac{3} {2}\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}\rangle+\rho_{r,i}^{(t_{r,i}) }\big{)}\|\boldsymbol{\xi}_{i}\|_{2}^{2}\]\[\leq\frac{3\eta\sigma_{\xi}^{2}d}{2nm\tau}(0.3\alpha+0.5\alpha)\] \[\leq 0.25\alpha,\] (26)

where the first inequality is by \(1-\ell_{i}^{\prime(t_{r})}\leq 1\) and modified Lemma C.1 with \(\rho_{r,i}^{(t)}=O(\alpha)\). The second inequality is by \(\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}\rangle\leq 0.2\alpha\) and \(\eta\leq\frac{5}{24}\frac{nm\tau}{\sigma_{\xi}^{2}d}\).

For notation convenience, we let

\[F_{0}(\mathbf{W},\mathbf{x}_{i}) =\mathrm{Sim}_{\mathbf{h}}(\mathbf{x}_{i},\widehat{\mathbf{x}}_{ i})/\tau\] \[=\frac{1}{m\tau}\sum_{r=1}^{m}\sigma(\langle\mathbf{w}_{r},y_{i} \boldsymbol{\mu}\rangle)\mathrm{sg}(\sigma(\langle\mathbf{w}_{r},y_{i} \boldsymbol{\mu}\rangle))+\frac{1}{m\tau}\sum_{r=1}^{m}\sigma(\langle\mathbf{w }_{r},\boldsymbol{\xi}_{i}\rangle)\mathrm{sg}(\sigma(\langle\mathbf{w}_{r}, \boldsymbol{\xi}_{i}+\boldsymbol{\epsilon}_{i}\rangle))\] \[F_{j}(\mathbf{W},\mathbf{x}_{i}) =\mathrm{Sim}_{\mathbf{h}}(\mathbf{x}_{i},\widehat{\mathbf{x}}_{ j})/\tau\] \[=\frac{1}{m\tau}\sum_{r=1}^{m}\sigma(\langle\mathbf{w}_{r},y_{i} \boldsymbol{\mu}\rangle)\mathrm{sg}(\sigma(\langle\mathbf{w}_{r},y_{j} \boldsymbol{\mu}\rangle))+\frac{1}{m\tau}\sum_{r=1}^{m}\sigma(\langle\mathbf{w }_{r},\boldsymbol{\xi}_{i}\rangle)\mathrm{sg}(\sigma(\langle\mathbf{w}_{r}, \boldsymbol{\xi}_{j}\rangle)),\text{ for }j=1,...,M\]

Next, we show the bound for \(F_{0}(\mathbf{W}^{(t)},\mathbf{x}_{i})\) and \(F_{j}(\mathbf{W}^{(t)},\mathbf{x}_{i})\) for \(j=1,...,M\) as

\[F_{0}(\mathbf{W}^{(t)},\mathbf{x}_{i})\geq\frac{1}{m\tau}\sum_{r=1}^{m}\sigma (\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{i}\rangle)\sigma(\langle \mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{i}+\boldsymbol{\epsilon}_{i}\rangle).\]

Further, we have for \(j=1,...,M\)

\[F_{j}(\mathbf{W}^{(t)},\mathbf{x}_{i})=\frac{1}{m\tau}\sum_{r=1}^{m}\sigma( \langle\mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{i}\rangle)\sigma(\langle \mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{j}\rangle).\]

Then we can bound the difference between \(F_{0}(\mathbf{W}^{(t)},\mathbf{x}_{i})\) and \(F_{j}(\mathbf{W}^{(t)},\mathbf{x}_{i})\) as

\[F_{0}(\mathbf{W}^{(t)},\mathbf{x}_{i})-F_{j}(\mathbf{W}^{(t)}, \mathbf{x}_{i}) \geq\frac{1}{m\tau}\sum_{r=1}^{m}\sigma(\langle\mathbf{w}_{r}^{( t)},\boldsymbol{\xi}_{i}\rangle)\sigma(\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{i}+ \boldsymbol{\epsilon}_{i}\rangle)-\frac{1}{m\tau}\sum_{r=1}^{m}\sigma(\langle \mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{i}\rangle)\sigma(\langle\mathbf{w}_{r}^ {(t)},\boldsymbol{\xi}_{j}\rangle)\] \[=\frac{1}{m\tau}\sum_{r=1}^{m}\sigma(\langle\mathbf{w}_{r}^{(t)}, \boldsymbol{\xi}_{i}\rangle)\big{(}\sigma(\langle\mathbf{w}_{r}^{(t)}, \boldsymbol{\xi}_{i}+\boldsymbol{\epsilon}_{i}\rangle)-\sigma(\langle\mathbf{w }_{r}^{(t)},\boldsymbol{\xi}_{j}\rangle)\big{)}\] \[\geq 0.5\alpha(\frac{1}{2}\langle\mathbf{w}_{r}^{(0)},\boldsymbol{ \xi}_{i}\rangle+\rho_{r,i}^{(t)}-\frac{3}{2}\langle\mathbf{w}_{r}^{(0)}, \boldsymbol{\xi}_{j}\rangle-\rho_{r,j}^{(t)})/\tau\] \[\geq 0.5\alpha(\frac{1}{2}\langle\mathbf{w}_{r}^{(0)},\boldsymbol{ \xi}_{i}\rangle+\frac{1}{2}\rho_{r,i}^{(t)}-\frac{3}{2}\langle\mathbf{w}_{r}^{( 0)},\boldsymbol{\xi}_{j}\rangle)/\tau\] \[\geq 0.5\alpha\cdot 0.1\alpha/\tau\] \[\geq\alpha,\] (27)

where the second inequality is by \(\sigma(\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{i}\rangle)\geq\frac{3}{4 }\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}\rangle+\rho_{r,i}^{(t)}\geq \rho_{r,i}^{(t)}\geq 0.5\alpha\) for \(i\in\mathcal{I}_{r,+}^{(0)}\). Further \(\sigma(\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{i}+\boldsymbol{\epsilon}_ {i}\rangle)\geq\frac{1}{2}\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i} \rangle+\rho_{r,i}^{(t)}\), \(\sigma(\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{j}\rangle)\leq\frac{3}{2} \langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{j}\rangle+\rho_{r,j}^{(t)}\). The fourth inequality is by induction and \(\frac{1}{2}\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}\rangle-\frac{3}{2} \langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{j}\rangle\geq-0.15\alpha\). The last inequality is by the condition that \(\alpha\geq 20\tau\).

Further, we bound the loss derivative as

\[1-\ell_{i}^{\prime(t)} =1-\frac{1}{1+\sum_{j=1}^{M}\exp(F_{j}(\mathbf{W}^{(t)},\mathbf{x}_{ i})-F_{0}(\mathbf{W}^{(t)},\mathbf{x}_{i}))}\] \[\leq 1-\frac{1}{1+M\exp(-\alpha)}\] \[\leq 1-\frac{T^{*}}{1+T^{*}}\]\[=\frac{1}{T^{*}+1},\] (28)

where the second inequality is by (27).

Finally, we bound

\[\sum_{t_{r,i}\leq t\leq\widetilde{T}}\frac{\eta}{nm\tau}(1-\ell_{i} ^{\prime(t)})\sigma(\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{i}+ \boldsymbol{\epsilon}_{i}\rangle)\|\boldsymbol{\xi}_{i}\|_{2}^{2} \leq\frac{3\eta\sigma_{\xi}^{2}d}{2nm\tau}\cdot 2\alpha\cdot\sum_{t_{r,i}\leq t \leq\widetilde{T}}(1-\ell_{i}^{\prime(t)})\] \[\leq\frac{3\eta\sigma_{\xi}^{2}d}{nm\tau}\cdot\alpha\cdot\frac{T^ {*}}{T^{*}+1}\] (29)

where the first inequality is by Lemma B.5 and \(\sigma(\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{i}+\boldsymbol{\epsilon }_{i}\rangle)\leq\frac{3}{2}\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i} \rangle+\rho_{r,i}^{(t)}\leq 2\alpha\) by induction. The last inequality is by the condition \(\eta\leq\frac{1}{12}\frac{nm\tau}{\sigma_{\xi}^{2}d}\).

Combining (26) and (29) with (25) gives

\[\rho_{r,i}^{(\widetilde{T})}\leq 0.5\alpha+0.25\alpha+0.25\alpha=\alpha,\]

which completes the induction. 

**Lemma C.14**.: _Under Assumption 4.1, for \(0\leq t\leq T^{*}\), we have_

\[\|\nabla L_{S}(\mathbf{W}^{(t)})\|_{F}^{2}\leq O(\max\{\|\boldsymbol{\mu}\|_{ 2}^{2},\sigma_{\xi}^{2}d\})L_{S}(\mathbf{W}^{(t)}).\]

Proof of Lemma c.14.: First, we can write the gradient of \(\nabla_{\mathbf{w}_{r}}L_{i}(\mathbf{W})\) as

\[\nabla_{\mathbf{w}_{r}}L_{i}(\mathbf{W})=\sum_{j=0}^{M}\frac{\partial L_{i}( \mathbf{W})}{\partial F_{j}(\mathbf{W},\mathbf{x}_{i})}\nabla_{\mathbf{w}_{r }}F_{j}(\mathbf{W},\mathbf{x}_{i}),\]

where

\[\frac{\partial L_{i}(\mathbf{W})}{\partial F_{0}} =-1+\frac{e^{F_{0}(\mathbf{W},\mathbf{x}_{i})}}{e^{F_{0}(\mathbf{ W},\mathbf{x}_{i})}+\sum_{j=1}^{M}e^{F_{j}(\mathbf{W},\mathbf{x}_{i})}}\] \[\frac{\partial L_{i}(\mathbf{W})}{\partial F_{j}} =\frac{e^{F_{j}(\mathbf{W},\mathbf{x}_{i})}}{e^{F_{0}(\mathbf{ W},\mathbf{x}_{i})}+\sum_{j=1}^{M}e^{F_{j}(\mathbf{W},\mathbf{x}_{i})}},\; \text{for}\;j=1,...,M\]

By the derivation of the gradient,

\[\|\nabla L_{S}(\mathbf{W}^{(t)})\|_{F}^{2} \leq\big{(}\frac{1}{n}\sum_{i=1}^{n}\|\nabla L_{i}(\mathbf{W})\| _{F}\big{)}^{2}\] \[=\Big{(}\frac{1}{n}\sum_{i=1}^{n}\sqrt{\sum_{r=1}^{m}\|\nabla_{ \mathbf{w}_{r}}L_{i}(\mathbf{W}^{(t)})\|_{2}^{2}}\Big{)}^{2}\] \[\leq\Big{(}\frac{1}{n}\sum_{i=1}^{n}\sum_{r=1}^{m}\|\nabla_{ \mathbf{w}_{r}}L_{i}(\mathbf{W}^{(t)})\|_{2}\Big{)}^{2}\] \[=\Big{(}\frac{1}{n}\sum_{i=1}^{n}\sum_{r=1}^{m}\|\sum_{j=0}^{M} \frac{\partial L_{i}(\mathbf{W})}{\partial F_{j}(\mathbf{W},\mathbf{x}_{i})} \nabla_{\mathbf{w}_{r}}F_{j}(\mathbf{W},\mathbf{x}_{i})\|_{2}\Big{)}^{2}\] \[\leq\Big{(}\frac{1}{n}\sum_{i=1}^{n}\sum_{r=1}^{m}\sum_{j=0}^{M} \big{|}\frac{\partial L_{i}(\mathbf{W})}{\partial F_{j}(\mathbf{W},\mathbf{x}_ {i})}\big{|}\|\nabla_{\mathbf{w}_{r}}F_{j}(\mathbf{W},\mathbf{x}_{i})\|_{2} \Big{)}^{2},\] (30)

where the first inequality is by triangle inequality and second inequality uses \(\sum_{i}a_{i}^{2}\leq(\sum_{i}a_{i})^{2}\) for \(a_{i}\geq 0\) and the last inequality is by triangle inequality.

Now we upper bound \(\|\nabla_{\mathbf{w}_{r}}F_{j}(\mathbf{W},\mathbf{x}_{i})\|_{2}\) as

\[\|\nabla_{\mathbf{w}_{r}}F_{0}(\mathbf{W},\mathbf{x}_{i})\|_{2} =\frac{1}{m\tau}\|\sigma^{\prime}(\langle\mathbf{w}_{r},y_{i}\bm{ \mu}\rangle)\sigma(\langle\mathbf{w}_{r},y_{i}\bm{\mu}\rangle)y_{i}\bm{\mu}+ \sigma^{\prime}(\langle\mathbf{w}_{r},\bm{\xi}_{i}\rangle)\sigma(\langle \mathbf{w}_{r},\bm{\xi}_{i}+\epsilon_{i}\rangle)\bm{\xi}_{i}\|_{2}\] \[\leq\frac{1}{m\tau}\big{(}\sigma(\langle\mathbf{w}_{r},y_{i}\bm{ \mu}\rangle)\|\bm{\mu}\|_{2}+\sigma(\langle\mathbf{w}_{r},\bm{\xi}_{i}+\bm{ \epsilon}_{i}\rangle)\|\bm{\xi}_{i}\|_{2}\big{)}\] \[\leq\frac{1}{m\tau}\big{(}\sigma(\langle\mathbf{w}_{r},y_{i}\bm{ \mu}\rangle)+\sigma(\langle\mathbf{w}_{r},\bm{\xi}_{i}+\bm{\epsilon}_{i} \rangle)\big{)}O\big{(}\max\{\|\bm{\mu}\|_{2},\sigma_{\xi}\sqrt{d}\}\big{)},\]

where the first inequality is by triangle inequality and the second inequality is by Jensen's inequality. Similarly, we can obtain for \(j=1,...,M\)

For clarity let

\[z_{r,0} =\sigma(\langle\mathbf{w}_{r},y_{i}\bm{\mu}\rangle)+\sigma(\langle \mathbf{w}_{r},\bm{\xi}_{i}+\bm{\epsilon}_{i}\rangle)\] \[z_{r,j} =\sigma(\langle\mathbf{w}_{r},y_{j}\bm{\mu}\rangle)+\sigma( \langle\mathbf{w}_{r},\bm{\xi}_{j}\rangle),\text{ for }j=1,...,M\]

Substituting the above results into (30) gives

\[\|\nabla L_{S}(\mathbf{W}^{(t)})\|_{F}^{2} \leq\Big{(}\frac{1}{nm\tau}\sum_{i=1}^{n}\sum_{r=1}^{m}\sum_{j=0} ^{M}\big{|}\frac{\partial L_{i}(\mathbf{W})}{\partial F_{j}(\mathbf{W}, \mathbf{x}_{i})}\big{|}z_{r,j}\Big{)}^{2}O\big{(}\max\{\|\bm{\mu}\|_{2}^{2}, \sigma_{\xi}^{2}d\}\big{)}\] \[\leq\Big{(}\frac{1}{n\tau}\sum_{i=1}^{n}\big{(}\sum_{j=0}^{M} \big{|}\frac{\partial L_{i}(\mathbf{W})}{\partial F_{j}(\mathbf{W},\mathbf{x} _{i})}\big{|}\big{)}(\frac{1}{m}\sum_{r=1}^{m}\sum_{j=0}^{M}z_{r,j})\Big{)}^{2 }O\big{(}\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{\xi}^{2}d\}\big{)},\]

where the second inequality is by \(\sum_{i}a_{i}b_{i}\leq(\sum_{i}a_{i})(\sum_{i}b_{i})\) for \(a_{i},b_{i}\geq 0\).

Next we can verify that

\[\sum_{j=0}^{M}\big{|}\frac{\partial L_{i}(\mathbf{W})}{\partial F _{j}(\mathbf{W},\mathbf{x}_{i})}\big{|} =1-\frac{e^{F_{0}(\mathbf{W},\mathbf{x}_{i})}}{e^{F_{0}(\mathbf{W},\mathbf{x}_{i})}+\sum_{j=1}^{M}e^{F_{j}(\mathbf{W},\mathbf{x}_{i})}}+\sum_{j= 1}^{M}\frac{e^{F_{j}(\mathbf{W},\mathbf{x}_{i})}}{e^{F_{0}(\mathbf{W},\mathbf{ x}_{i})}+\sum_{j=1}^{M}e^{F_{j}(\mathbf{W},\mathbf{x}_{i})}}\] \[=2\big{(}1-\frac{e^{F_{0}(\mathbf{W},\mathbf{x}_{i})}}{e^{F_{0}( \mathbf{W},\mathbf{x}_{i})}+\sum_{j=1}^{M}e^{F_{j}(\mathbf{W},\mathbf{x}_{i})} }\big{)}\] \[\leq-6\log(\frac{e^{F_{0}(\mathbf{W},\mathbf{x}_{i})}}{e^{F_{0}( \mathbf{W},\mathbf{x}_{i})}+\sum_{j=1}^{M}e^{F_{j}(\mathbf{W},\mathbf{x}_{i})} })=6L_{i}(\mathbf{W}),\] (31)

where the last inequality is by \(1-x\leq-3\log(x)\) for \(x\in[0,1]\). Furthermore, we can show

\[2\big{(}1-\frac{e^{F_{0}(\mathbf{W},\mathbf{x}_{i})}}{e^{F_{0}( \mathbf{W},\mathbf{x}_{i})}+\sum_{j=1}^{M}e^{F_{j}(\mathbf{W},\mathbf{x}_{i}) }}\big{)}(\frac{1}{m}\sum_{r=1}^{m}\sum_{j=0}^{M}z_{r,j})^{2}=O(1),\]

which leads to

\[\|\nabla L_{S}(\mathbf{W}^{(t)})\|_{F}^{2} \leq\Big{(}\frac{1}{n\tau}\sum_{i=1}^{n}\sqrt{\big{(}\sum_{j=0}^ {M}\big{|}\frac{\partial L_{i}(\mathbf{W}^{(t)})}{\partial F_{j}(\mathbf{W}^{(t) },\mathbf{x}_{i})}\big{|}\big{)}^{2}(\frac{1}{m}\sum_{r=1}^{m}\sum_{j=0}^{M}z_{ r,j})^{2}}\Big{)}^{2}O\big{(}\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{\xi}^{2}d\}\big{)}\] \[\leq O\big{(}\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{\xi}^{2}d\}\big{)} \frac{1}{n}\sum_{i=1}^{n}L_{i}(\mathbf{W}^{(t)})\] \[\leq O\big{(}\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{\xi}^{2}d\}\big{)} L_{S}(\mathbf{W}^{(t)}),\]

where the third inequality is by (31) and Cauchy-Schwartz inequality.

We define

\[\mathbf{w}_{r}^{*}=\mathbf{w}_{r}^{(0)}+2\tau\log(2M/\epsilon)\sum_{i=1}^{n}\frac{ \boldsymbol{\xi}_{i}}{\|\boldsymbol{\xi}_{i}\|_{2}^{2}}.\]

Recall in the first stage

\[\mathbf{w}_{r}^{(T_{1})}=\mathbf{w}_{r}^{(0)}+\gamma_{r}^{(T_{1})}\|\boldsymbol {\mu}\|_{2}^{-2}\boldsymbol{\mu}+\sum_{i=1}^{n}\rho_{r,i}^{(T_{1})}\| \boldsymbol{\xi}_{i}\|_{2}^{-2}\boldsymbol{\xi}_{i},\]

and we have

* \(\max_{r}|\gamma_{r}^{(t)}|=\widetilde{O}(n^{-1/2})\) for all \(0\leq t\leq T_{1}\).
* \(\max_{r,i}\rho_{r,i}^{(T_{1})}\geq 2\).

**Lemma C.15**.: _Under Assumption 4.1, we have \(\|\mathbf{W}^{(T_{1})}-\mathbf{W}^{*}\|_{F}\leq\widetilde{O}(m^{1/2}n^{1/2} \sigma_{\xi}^{-1}d^{-1/2})\)._

Proof of Lemma c.15.: We have

\[\|\mathbf{W}^{(T_{1})}-\mathbf{W}^{*}\|_{F} \leq\|\mathbf{W}^{(T_{1})}-\mathbf{W}^{(0)}\|_{F}+\|\mathbf{W}^{ (0)}-\mathbf{W}^{*}\|_{F}\] \[\leq\sum_{r}\frac{\gamma_{r}^{(T_{1})}}{\|\boldsymbol{\mu}\|_{2 }}+O(\sqrt{m})\max_{r}\|\sum_{i=1}^{n}\rho_{r,i}^{(T_{1})}\frac{\boldsymbol{ \xi}_{i}}{\|\boldsymbol{\xi}_{i}\|_{2}^{2}}\|_{2}+O(m^{1/2}n^{1/2}\log(1/ \epsilon)\sigma_{\xi}^{-1}d^{-1/2})\] \[\leq\widetilde{O}(m^{1/2}n^{1/2}\sigma_{\xi}^{-1}d^{-1/2}).\]

**Lemma C.16**.: _Under Assumption 4.1, we have for all \(t\in[T_{1},T^{*}]\)_

\[\langle\nabla F_{0}(\mathbf{W}^{(t)},\mathbf{x}_{i}),\mathbf{W}^ {*}\rangle\geq 2\log(2M/\epsilon),\] \[\langle\nabla F_{j}(\mathbf{W}^{(t)},\mathbf{x}_{i}),\mathbf{W}^ {*}\rangle\leq\log(2M/\epsilon).\]

Proof of Lemma c.16.: Based on the definition of \(\mathbf{W}^{*}\) and \(F_{j}(\mathbf{W}^{(t)},\mathbf{x}_{i})\), we can derive for \(j=0\),

\[\langle\nabla F_{0}(\mathbf{W}^{(t)},\mathbf{x}_{i}),\mathbf{W}^ {*}\rangle\] \[=\sum_{r=1}^{m}\langle\nabla_{\mathbf{w}_{r}}F_{0}(\mathbf{W}^{( t)},\mathbf{x}_{i}),\mathbf{w}_{r}^{*}\rangle\] \[=\frac{1}{m\tau}\sum_{r=1}^{m}\sigma^{\prime}(\langle\mathbf{w}_{ r}^{(t)},y_{i}\boldsymbol{\mu}\rangle)\sigma(\langle\mathbf{w}_{r}^{(t)},y_{i} \boldsymbol{\mu}\rangle)\langle\mathbf{w}_{r}^{*},y_{i}\boldsymbol{\mu}\rangle+ \frac{1}{m\tau}\sum_{r=1}^{m}\sigma^{\prime}(\langle\mathbf{w}_{r}^{(t)}, \boldsymbol{\xi}_{i}\rangle)\sigma(\langle\mathbf{w}_{r}^{(t)},\boldsymbol{ \xi}_{i}+\boldsymbol{\epsilon}_{i}\rangle)\langle\mathbf{w}_{r}^{*}, \boldsymbol{\xi}_{i}\rangle\] \[=\frac{1}{m\tau}\sum_{r=1}^{m}\sigma^{\prime}(\langle\mathbf{w}_{ r}^{(t)},y_{i}\boldsymbol{\mu}\rangle)\sigma(\langle\mathbf{w}_{r}^{(t)},y_{i} \boldsymbol{\mu}\rangle)\Big{(}\langle\mathbf{w}_{r}^{(0)},y_{i}\boldsymbol{ \mu}\rangle+2\tau\log(2M/\epsilon)\sum_{i=1}^{n}\langle\boldsymbol{\xi}_{i},y _{i}\boldsymbol{\mu}\rangle\|\boldsymbol{\xi}_{i}\|_{2}^{-2}\Big{)}\] \[\geq\underbrace{\frac{1}{m\tau}\sum_{r=1}^{m}\sigma^{\prime}( \langle\mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{i}\rangle)\sigma(\langle\mathbf{ w}_{r}^{(t)},\boldsymbol{\xi}_{i}+\boldsymbol{\epsilon}_{i}\rangle)2\tau\log(2M/ \epsilon)}_{I_{1}}-\underbrace{\frac{1}{m\tau}\sum_{r=1}^{m}\sigma(\langle \mathbf{w}_{r}^{(t)},y_{i}\boldsymbol{\mu}\rangle)\widetilde{O}(\sigma_{0} \|\boldsymbol{\mu}\|_{2})}_{I_{2}}\] \[\quad-\underbrace{\frac{1}{m\tau}\sum_{r=1}^{m}\sigma(\langle \mathbf{w}_{r}^{(t)},y_{i}\boldsymbol{\mu}\rangle)2\tau\log(2M/\epsilon) \widetilde{O}(n\|\boldsymbol{\mu}\|_{2}\sigma_{\xi}^{-1}d^{-1})}_{I_{3}}- \underbrace{\frac{1}{m\tau}\sum_{r=1}^{m}\sigma(\langle\mathbf{w}_{r}^{(t)}, \boldsymbol{\xi}_{i}+\boldsymbol{\epsilon}_{i}\rangle)\widetilde{O}(\sigma_{0} \sigma_{\xi}\sqrt{d})}_{I_{4}}\] \[\quad-\underbrace{\frac{1}{m\tau}\sum_{r=1}^{m}\sigma(\langle \mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{i}+\boldsymbol{\epsilon}_{i}\rangle)2\tau \log(2M/\epsilon)\widetilde{O}(nd^{-1/2})}_{I_{5}}\]where the inequality is by Lemma B.5.

Next, we bound \(I_{1},I_{2},I_{3},I_{4},I_{5}\) separately. For \(I_{1}\), we take maximum over \(r\), which results in

\[\frac{1}{m}\sum_{r=1}^{m}\sigma(\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{i }+\boldsymbol{\epsilon}_{i}\rangle)\geq\frac{1}{m}\sum_{r=1}^{m}\sigma(\frac{ 1}{2}\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}\rangle+\rho_{r,i}^{(t) })\geq 2.\]

Thus we obtain

\[I_{1}\geq 4\log(2M/\epsilon).\]

In addition, we can show by upper bound on \(\rho_{r,i}^{(t)}\) and \(\gamma_{r}^{(t)}\),

\[\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{i}+\boldsymbol{ \epsilon}_{i}\rangle \leq\frac{3}{2}|\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i} \rangle|+|\rho_{r,i}^{(t)}|\leq\widetilde{O}(1),\] \[\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\mu}\rangle \leq\frac{3}{2}|\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\mu} \rangle|+|\gamma_{r}^{(t)}|\leq\widetilde{O}(1).\]

This implies

\[I_{2}\leq\widetilde{O}(\sigma_{0}\|\boldsymbol{\mu}\|_{2}),\,I_{3}\leq\log(2M /\epsilon)\widetilde{O}(nm\|\boldsymbol{\mu}\|_{2}\sigma_{\xi}^{-1}d^{-1}),\, I_{4}\leq\widetilde{O}(\sigma_{0}\sigma_{\xi}\sqrt{d}),\,I_{5}\leq \widetilde{O}(nmd^{-1/2}).\]

Based on the conditions on \(\sigma_{0},d\), we can show

\[\langle\nabla F_{0}(\mathbf{W}^{(t)},\mathbf{x}_{i}),\mathbf{W}^{*}\rangle \geq 4\log(2M/\epsilon)-I_{2}-I_{3}-I_{4}-I_{5}\geq 2\log(2M/\epsilon).\]

Now we prove for the claim for \(F_{j}(\mathbf{W}^{(t)},\mathbf{W}^{*})\) for \(j=1,...,M\) as follows.

\[\langle\nabla F_{j}(\mathbf{W}^{(t)},\mathbf{x}_{i}),\mathbf{W}^ {*}\rangle\] \[=\frac{1}{m\tau}\sum_{r=1}^{m}\sigma^{\prime}(\langle\mathbf{w}_ {r}^{(t)},y_{i}\boldsymbol{\mu}\rangle)\sigma(\langle\mathbf{w}_{r}^{(t)},y_{ j}\boldsymbol{\mu}\rangle)\langle\mathbf{w}_{r}^{*},y_{i}\boldsymbol{\mu} \rangle+\frac{1}{m\tau}\sum_{r=1}^{m}\sigma^{\prime}(\langle\mathbf{w}_{r}^{( t)},\boldsymbol{\xi}_{i}\rangle)\sigma(\langle\mathbf{w}_{r}^{(t)}, \boldsymbol{\xi}_{j}\rangle)\langle\mathbf{w}_{r}^{*},\boldsymbol{\xi}_{i}\rangle\] \[=\frac{1}{m\tau}\sum_{r=1}^{m}\sigma^{\prime}(\langle\mathbf{w}_ {r}^{(t)},\boldsymbol{\xi}_{i}\rangle)\sigma(\langle\mathbf{w}_{r}^{(t)}, \boldsymbol{\xi}_{j}\rangle)\langle\mathbf{w}_{r}^{*},\boldsymbol{\xi}_{i}\rangle\] \[\leq I_{3}+I_{4}+I_{5}\leq\log(2M/\epsilon),\]

where the second equality is by \(y_{i}\neq y_{j}\). 

**Lemma C.17**.: _Under Assumption 4.1, we have_

\[\|\mathbf{W}^{(t)}-\mathbf{W}^{*}\|_{F}^{2}-\|\mathbf{W}^{(t+1)}-\mathbf{W}^{* }\|\geq\eta L_{S}(\mathbf{W}^{(t)})-\eta\epsilon.\]

Proof of Lemma c.17.: First, we verify that for \(j=0\),

\[\langle\nabla F_{0}(\mathbf{W}^{(t)},\mathbf{x}_{i}),\mathbf{W}^ {(t)}\rangle =\sum_{r=1}^{m}\langle\nabla_{\mathbf{w}_{r}}F_{0}(\mathbf{W}^{(t )},\mathbf{x}_{i}),\mathbf{w}_{r}^{(t)}\rangle\] \[=\frac{1}{m\tau}\sum_{r=1}^{m}\langle\sigma^{\prime}(\langle \mathbf{w}_{r},y_{i}\boldsymbol{\mu}\rangle)\text{sg}(\sigma(\langle\mathbf{w} _{r},y_{i}\boldsymbol{\mu}\rangle))y_{i}\boldsymbol{\mu},\mathbf{w}_{r}^{(t)}\rangle\] \[\quad+\frac{1}{m\tau}\sum_{r=1}^{m}\langle\sigma^{\prime}(\langle \mathbf{w}_{r},\boldsymbol{\xi}_{i}\rangle)\text{sg}(\sigma(\langle\mathbf{w} _{r},\boldsymbol{\xi}_{i}+\boldsymbol{\epsilon}_{i}\rangle))\boldsymbol{\xi}_{i },\mathbf{w}_{r}^{(t)}\rangle\] \[=\frac{1}{m\tau}\sum_{r=1}^{m}\sigma(\langle\mathbf{w}_{r},y_{i} \boldsymbol{\mu}\rangle)\text{sg}(\sigma(\langle\mathbf{w}_{r},y_{i}\boldsymbol{ \mu}\rangle))\] \[\quad+\frac{1}{m\tau}\sum_{r=1}^{m}\sigma(\langle\mathbf{w}_{r}, \boldsymbol{\xi}_{i}\rangle)\text{sg}(\sigma(\langle\mathbf{w}_{r},\boldsymbol{ \xi}_{i}+\boldsymbol{\epsilon}_{i}\rangle))\] \[=F_{0}(\mathbf{W}^{(t)},\mathbf{x}_{i}).\] (32)Similarly, we can show for \(j=1,...,M\), it satisfies that

\[\langle\nabla F_{j}(\mathbf{W}^{(t)},\mathbf{x}_{i}),\mathbf{W}^{(t)}\rangle=F_{j }(\mathbf{W}^{(t)},\mathbf{x}_{i}).\] (33)

By the update of \(\mathbf{W}^{(t)}\), we have

\[\|\mathbf{W}^{(t)}-\mathbf{W}^{*}\|_{F}^{2}-\|\mathbf{W}^{(t+1)}- \mathbf{W}^{*}\|_{F}^{2}\] \[=2\eta\langle\nabla L_{S}(\mathbf{W}^{(t)}),\mathbf{W}^{(t)}- \mathbf{W}^{*}\rangle-\eta^{2}\|\nabla L_{S}(\mathbf{W}^{(t)})\|_{F}^{2}\] \[=\frac{2\eta}{n}\sum_{i=1}^{n}\sum_{j=0}^{M}\frac{\partial L_{i}( \mathbf{W}^{(t)})}{\partial F_{j}(\mathbf{W}^{(t)},\mathbf{x}_{i})}\langle \nabla F_{j}(\mathbf{W}^{(t)},\mathbf{x}_{i}),\mathbf{W}^{(t)}-\mathbf{W}^{*} \rangle-\eta^{2}\|\nabla L_{S}(\mathbf{W}^{(t)})\|_{F}^{2}\] \[=\frac{2\eta}{n}\sum_{i=1}^{n}\sum_{j=0}^{M}\frac{\partial L_{i}( \mathbf{W}^{(t)})}{\partial F_{j}(\mathbf{W}^{(t)},\mathbf{x}_{i})}\Big{(}F_{ j}(\mathbf{W}^{(t)},\mathbf{x}_{i})-\langle\nabla F_{j}(\mathbf{W}^{(t)}, \mathbf{x}_{i}),\mathbf{W}^{*}\rangle\Big{)}-\eta^{2}\|\nabla L_{S}(\mathbf{ W}^{(t)})\|_{F}^{2}\] \[\geq\frac{2\eta}{n}\sum_{i=1}^{n}\Big{(}\frac{\partial L_{i}( \mathbf{W}^{(t)})}{\partial F_{0}(\mathbf{W}^{(t)},\mathbf{x}_{i})}\big{(}F_{ 0}(\mathbf{W}^{(t)},\mathbf{x}_{i})-2\log(2M/\epsilon)\big{)}+\sum_{j=1}^{M} \frac{\partial L_{i}(\mathbf{W}^{(t)})}{\partial F_{j}(\mathbf{W}^{(t)}, \mathbf{x}_{i})}\big{(}F_{j}(\mathbf{W}^{(t)},\mathbf{x}_{i})-\log(2M/\epsilon )\big{)}\Big{)}\] \[\quad-\eta^{2}\|\nabla L_{S}(\mathbf{W}^{(t)})\|_{F}^{2}\] \[\geq\frac{2\eta}{n}\sum_{i=1}^{n}\big{(}L_{i}(\mathbf{W}^{(t)})+ \log(\frac{e^{2\log(2M/\epsilon)}}{e^{2\log(2M/\epsilon)}+Me^{\log(2M/\epsilon )}})\big{)}-\eta^{2}\|\nabla L_{S}(\mathbf{W}^{(t)})\|_{F}^{2}\] \[=\frac{2\eta}{n}\sum_{i=1}^{n}\big{(}L_{i}(\mathbf{W}^{(t)})-\log (1+\frac{\epsilon}{2})\big{)}-\eta^{2}\|\nabla L_{S}(\mathbf{W}^{(t)})\|_{F}^ {2}\] \[\geq\eta L_{S}(\mathbf{W}^{(t)})-\eta\epsilon,\]

where the third equality is by (32) and (33). The first inequality is by Lemma C.16. The second inequality is due to the convexity of negative log-Softmax function. The last inequality is by Lemma C.14 (and the conditions on \(\eta\)) and \(\log(1+x)\leq x\) for \(x\geq 0\). 

**Lemma C.18**.: _Under Assumption 4.1, let \(T=T_{1}+\lfloor\frac{\|\mathbf{W}^{(T_{1})}-\mathbf{W}^{*}\|_{F}^{2}}{\eta \epsilon}\rfloor=T_{1}+\widetilde{O}(mn\sigma_{\xi}^{-2}d^{-1}\eta^{-1}\epsilon ^{-1})\). Then we have \(\max_{r}|\gamma_{r}^{(t)}|\leq\widetilde{O}(1/\sqrt{n})\) for all \(T_{1}\leq t\leq T\). In addition, we have_

\[\frac{1}{t-T_{1}+1}\sum_{s=T_{1}}^{t}L_{S}(\mathbf{W}^{(s)})\leq\frac{\| \mathbf{W}^{(T_{1})}-\mathbf{W}^{*}\|_{F}^{2}}{\eta(t-T_{1}+1)}+\epsilon\]

_for all \(T_{1}\leq t\leq T\). Thus there exists an iterate \(\mathbf{W}^{(s)}\) for \(s\in[T_{1},T]\) with training loss smaller than \(2\epsilon\)._

Proof of Lemma c.18.: By Lemma C.17, for \(t\in[T_{1},T]\),

\[\|\mathbf{W}^{(s)}-\mathbf{W}^{*}\|_{F}^{2}-\|\mathbf{W}^{(s+1)}-\mathbf{W}^{* }\|\geq\eta L_{S}(\mathbf{W}^{(t)})-\eta\epsilon\]

for all \(s\leq t\). Summing over the inequality and dividing both sides by \(t-T_{1}+1\) yields

\[\frac{1}{t-T_{1}+1}\sum_{s=T_{1}}^{t}L_{S}(\mathbf{W}^{(s)})\leq\frac{\| \mathbf{W}^{(T_{1})}-\mathbf{W}^{*}\|_{F}^{2}+\eta\epsilon(t-T_{1}+1)}{\eta(t -T_{1}+1)}=\frac{\|\mathbf{W}^{(T_{1})}-\mathbf{W}^{*}\|_{F}^{2}}{\eta(t-T_{1} +1)}+\epsilon\leq 2\epsilon,\]

where the last inequality is by the definition of \(T\), for all \(T_{1}\leq t\leq T\). In addition, we have

\[\sum_{t=T_{1}}^{T}L_{S}(\mathbf{W}^{(t)})\leq\frac{2\|\mathbf{W}^{(T_{1})}- \mathbf{W}^{*}\|_{F}^{2}}{\eta}=\widetilde{O}(\eta^{-1}mnd^{-1}\sigma_{\xi}^{ -2}).\] (34)

Next we prove the claim that \(\max_{r}|\gamma_{r}^{(t)}|\leq 3\beta\), where \(\beta=|\max_{r}\gamma_{r}^{(T_{1})}|=\widetilde{O}(1/\sqrt{n})\) for all \(T_{1}\leq t\leq T\). Without loss of generality, we only consider \(r\in\mathcal{U}_{+}^{(0)}\). First it is evident that at \(t=T_{1}\)we have \(\max_{r}\gamma^{(t)}_{r}=\beta\leq 3\beta\). Next suppose there exists \(\widetilde{T}\in[T_{1},T]\) such that \(\max_{r}\gamma^{(t)}_{r}\leq 3\beta\) for all \(t\in[T_{1},\widetilde{T}-1]\). Then we let \(\phi^{(t)}=\max_{r}\gamma^{(t)}_{r}\) and we have by the update of \(\gamma^{(t)}_{r}\),

\[\phi^{(t+1)} \leq\phi^{(t)}+\frac{\eta}{nm\tau}\sum_{i:y_{i}=1}(1-\ell^{\prime (t)}_{i})\max_{r}(\frac{3}{2}\langle\mathbf{w}^{(0)}_{r},\boldsymbol{\mu} \rangle+\phi^{(t)})\|\boldsymbol{\mu}\|_{2}^{2}\] \[\leq\phi^{(t)}+\frac{\eta}{nm\tau}\sum_{i:y_{i}=1}(1-\ell^{\prime (t)}_{i})(\frac{3}{2}\sqrt{2\log(8m/\delta)}\sigma_{0}\|\boldsymbol{\mu}\|_{2} +\phi^{(t)})\|\boldsymbol{\mu}\|_{2}^{2}\] \[\leq\phi^{(t)}+\frac{\eta}{m\tau}L_{S}(\mathbf{W}^{(t)})(\frac{3 }{2}\sqrt{2\log(8m/\delta)}\sigma_{0}\|\boldsymbol{\mu}\|_{2}+\phi^{(t)})\| \boldsymbol{\mu}\|_{2}^{2},\]

where the third inequality is by (31).

Now summing over \(t=T_{1},...,\widetilde{T}-1\), we have

\[\phi^{(\widetilde{T})} \leq\phi^{(T_{1})}+\sum_{t=T_{1}}^{\widetilde{T}-1}\frac{\eta}{m \tau}L_{S}(\mathbf{W}^{(t)})(\frac{3}{2}\sqrt{2\log(8m/\delta)}\sigma_{0}\| \boldsymbol{\mu}\|_{2}+\phi^{(t)})\|\boldsymbol{\mu}\|_{2}^{2}\] \[\leq\phi^{(T_{1})}+O(\frac{\eta\|\boldsymbol{\mu}\|_{2}^{2}}{m \tau})\beta\sum_{t=T_{1}}^{\widetilde{T}-1}L_{S}(\mathbf{W}^{(t)})\] \[\leq\phi^{(T_{1})}+O(n\|\boldsymbol{\mu}\|_{2}^{2}d^{-1}\sigma_{ \xi}^{-2})\beta\] \[\leq\phi^{(T_{1})}+O(n\mathrm{SNR}^{2})\beta\] \[\leq\phi^{(T_{1})}+2\beta\leq 3\beta\]

where the second inequality is by induction and the third inequality is by (34). The last inequality is by the condition of SNR. 

### Downstream Task Performance

Recall that after the pre-training stage on the training data at time \(T\), the signal learning and noise memorization satisfy

\[\max_{r}A^{(T)}_{r} =\widetilde{O}(1/\sqrt{n}),\] \[\max_{r}\Psi^{(T)}_{r,i} =\widetilde{\Omega}(1)\;\mathrm{for}\;i\in[n].\]

Then, on the downstream task, the corresponding embedding can be calculated as follows:

\[h_{r}(\mathbf{x}^{(1)}_{\mathrm{test}})=\sigma(\langle\mathbf{w} ^{(T)}_{r},\mathbf{x}^{(1)}_{\mathrm{test}}\rangle) =\widetilde{O}(1/\sqrt{dn}),\] \[h_{r}(\mathbf{x}^{(2)}_{\mathrm{test}})=\sigma(\langle\mathbf{w} ^{(T)}_{r},\mathbf{x}^{(2)}_{\mathrm{test}}\rangle) =\widetilde{\Omega}(1/\sqrt{d}).\]

Then, it is straightforward to check that the embedding of a finite size of samples during the fine-tuning stage is not linearly separable. Thus, the downstream task performance follows \(L_{\mathcal{D}_{\mathrm{test}}}(T^{*})=\Theta(1)\).

## Appendix D Multi-Modal Contrastive Learning: Proof of Theorem 4.3

### First Stage

Similar to the single-modal case, in the first stage, the loss derivative is close to its initial value for both modalities.

**Lemma D.1**.: _If \(\max\{\gamma^{(t)}_{r},\rho^{(t)}_{r,i},\widetilde{\gamma}^{(t)}_{r}, \widetilde{\rho}^{(t)}_{r,i}\}=O(1)\), there exists a constant \(C_{\ell}>1\) such that_

\[\frac{1}{C_{\ell}(1+M)} \leq\ell^{\prime(t)}_{i}\leq\frac{C_{\ell}}{1+M}\] \[\frac{1}{C_{\ell}(M+1)} \leq\ell^{\prime(t)}_{i,j}\leq\frac{C_{\ell}}{1+M}\]

_for all \(i\in[n]\)._Proof of Lemma D.1.: The proof follows from Lemma C.2. 

**Lemma D.2**.: _Suppose that \(\gamma_{r}^{(t)},\widetilde{\gamma}_{r}^{(t)}=O(1)\) and \(\rho_{r,i}^{(t)},\widetilde{\rho}_{r,i}^{(t)}=O(1)\) for all \(r\in[m]\) and \(i\in[n]\). Under Assumption 4.1, then for any \(\delta>0\), with probability at least \(1-\delta\)_

\[|\langle\mathbf{w}_{r}^{(t)}-\mathbf{w}_{r}^{(0)},\boldsymbol{ \xi}_{i}\rangle-\rho_{r,i}^{(t)}| \leq 5\sqrt{\frac{\log(6n^{2}/\delta)}{d}}n\] \[|\langle\mathbf{w}_{r}^{(t)}-\mathbf{w}_{r}^{(0)},\boldsymbol{ \mu}\rangle-\gamma_{r}^{(t)}| \leq\mathrm{SNR}\sqrt{\frac{8\log(6n/\delta)}{d}}n,\] \[|\langle\widetilde{\mathbf{w}}_{r}^{(t)}-\widetilde{\mathbf{w}}_ {r}^{(0)}\widetilde{\boldsymbol{\xi}}_{i}\rangle-\widetilde{\rho}_{r,i}^{(t)} |\leq 5\sqrt{\frac{\log(6n^{2}/\delta)}{\widetilde{d}}}n\] \[|\langle\widetilde{\mathbf{w}}_{r}^{(t)}-\widetilde{\mathbf{w}}_ {r}^{(0)},\widetilde{\boldsymbol{\mu}}\rangle-\widetilde{\gamma}_{r}^{(t)}| \leq\widehat{\mathrm{SNR}}\sqrt{\frac{8\log(6n/\delta)}{\widetilde{d}}}n\]

_for all \(r\in[m]\), \(i\in[n]\)._

#### d.1.1 Dynamics of Signal Learning: Lower Bound

We first analyze the dynamics of signal learning for both two modalities. Similar as in the single-modal learning, we partition the neurons, depending on the initialization, i.e., \(\mathcal{U}_{+}^{(t)}=\{r\in[m]:\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\mu} \rangle>0\}\) and \(\mathcal{U}_{-}^{(t)}=\{r\in[m]:\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\mu} \rangle<0\}\), \(\widetilde{\mathcal{U}}_{+}^{(t)}=\{r\in[m]:\langle\widetilde{\mathbf{w}}_{r}^ {(t)},\widetilde{\boldsymbol{\mu}}\rangle>0\}\) and \(\widetilde{\mathcal{U}}_{-}^{(t)}=\{r\in[m]:\langle\widetilde{\mathbf{w}}_{r} ^{(t)},\widetilde{\boldsymbol{\mu}}\rangle<0\}\).

**Lemma D.3**.: _Under Assumption 4.1 and the same condition as Lemma D.2, for all \(t>0\), we have_

1. \(\mathcal{U}_{+}^{(t)}\cap\widetilde{\mathcal{U}}_{+}^{(t)}=\mathcal{U}_{+}^{ (0)}\cap\widetilde{\mathcal{U}}_{+}^{(0)}\) _and_ \(\gamma_{r}^{(t)}\geq 0\)_,_ \(\widetilde{\gamma}_{r}^{(t)}\geq 0\) _and are increasing._
2. \(\mathcal{U}_{-}^{(t)}\cap\widetilde{\mathcal{U}}_{-}^{(t)}=\mathcal{U}_{-}^{ (0)}\cap\widetilde{\mathcal{U}}_{-}^{(0)}\) _and_ \(\gamma_{r}^{(t)}\leq 0\)_,_ \(\widetilde{\gamma}_{r}^{(t)}\leq 0\) _and are decreasing._
3. _For_ \(r\in\mathcal{U}_{+}^{(0)}\cap\widetilde{\mathcal{U}}_{-}^{(0)}\) _or_ \(r\in\mathcal{U}_{-}^{(0)}\cap\widetilde{\mathcal{U}}_{+}^{(0)}\)_, there exists a time_ \(t^{\prime}>0\) _such that_ \(r\in\mathcal{U}_{+}^{(t^{\prime})}\cap\widetilde{\mathcal{U}}_{+}^{(t^{\prime})}\) _or_ \(r\in\mathcal{U}_{-}^{(t^{\prime})}\cap\widetilde{\mathcal{U}}_{-}^{(t^{\prime})}\)_._

Proof of Lemma D.3.: We first analyze the neurons \(r\in\mathcal{U}_{+}^{(t)}\cap\widetilde{\mathcal{U}}_{+}^{(t)}\). By the update of \(\gamma_{r}^{(t)}\)

\[\gamma_{r}^{(t+1)} =\gamma_{r}^{(t)}+\frac{\eta}{nm\tau}\sum_{i=1}^{n}(1-\ell_{i}^{ \prime(t)})\sigma(\langle\widetilde{\mathbf{w}}_{r}^{(t)},y_{i}\widetilde{ \boldsymbol{\mu}}\rangle)\sigma^{\prime}(\langle\mathbf{w}_{r}^{(t)},y_{i} \boldsymbol{\mu}\rangle)y_{i}\|\boldsymbol{\mu}\|_{2}^{2}\] \[\quad-\frac{\eta}{nm\tau}\sum_{i=1}^{n}\sum_{j\neq i}^{M}\ell_{i,j }^{\prime(t)}\sigma(\langle\widetilde{\mathbf{w}}_{r}^{(t)},y_{j}\widetilde{ \boldsymbol{\mu}}\rangle)\sigma^{\prime}(\langle\mathbf{w}_{r}^{(t)},y_{i} \boldsymbol{\mu}\rangle)y_{i}\|\boldsymbol{\mu}\|_{2}^{2}\] \[=\gamma_{r}^{(t)}+\frac{\eta}{nm\tau}\sum_{i:y_{i}=1}(1-\ell_{i}^ {\prime(t)})\sigma(\langle\widetilde{\mathbf{w}}_{r}^{(t)},\widetilde{ \boldsymbol{\mu}}\rangle)\|\boldsymbol{\mu}\|_{2}^{2}\geq\gamma_{r}^{(t)},\] (35)

where the second equality is by \(y_{i}=y_{j}\) and the derivative of ReLU activation. Similarly for the other modality,

\[\widetilde{\gamma}_{r}^{(t+1)} =\widetilde{\gamma}_{r}^{(t)}+\frac{\eta}{nm\tau}\sum_{i=1}^{n}(1 -\ell_{i}^{\prime(t)})\sigma(\langle\mathbf{w}_{r}^{(t)},y_{i}\boldsymbol{\mu} \rangle)\sigma^{\prime}(\langle\widetilde{\mathbf{w}}_{r}^{(t)},y_{i} \boldsymbol{\widetilde{\mu}}\rangle)y_{i}\|\widetilde{\boldsymbol{\mu}}\|_{2}^ {2}\] \[\quad-\frac{\eta}{nm\tau}\sum_{i=1}^{n}\sum_{j\neq i}^{M}\ell_{i,j }^{\prime(t)}\sigma(\langle\mathbf{w}_{r}^{(t)},y_{j}\boldsymbol{\mu}\rangle) \sigma^{\prime}(\langle\widetilde{\mathbf{w}}_{r}^{(t)},y_{i}\boldsymbol{ \widetilde{\mu}}\rangle)y_{i}\|\widetilde{\boldsymbol{\mu}}\|_{2}^{2}\] \[=\widetilde{\gamma}_{r}^{(t)}+\frac{\eta}{nm\tau}\sum_{i:y_{i}=1}(1 -\ell_{i}^{\prime(t)})\sigma(\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\mu} \rangle)\|\widetilde{\boldsymbol{\mu}}\|_{2}^{2}\geq\widetilde{\gamma}_{r}^{(t)}.\] (36)Hence we see at \(t=0\), the claim is satisfied as \(\gamma_{r}^{(1)}\geq\gamma_{r}^{(0)}\) for \(r\in\mathcal{U}_{+}^{(0)}\cap\widetilde{\mathcal{U}}_{+}^{(0)}\). Then we prove the claim by induction. Suppose at iteration \(t\) the claims are satisfied, i.e., \(\gamma_{r}^{(t)}\geq\gamma_{r}^{(t-1)}\geq 0\) for \(r\in\mathcal{U}_{+}^{(0)}\) and \(r\in\mathcal{U}_{+}^{(t)}\). Then we can see from (35)

\[\gamma_{r}^{(t+1)}\geq\gamma_{r}^{(t)}\geq 0.\]

Similarly, we can show from (36) that \(\widetilde{\gamma}_{r}^{(t+1)}\geq\widetilde{\gamma}_{r}^{(t)}\geq 0\).

Then for the inner product, we have

\[\langle\mathbf{w}_{r}^{(t+1)},\boldsymbol{\mu}\rangle \stackrel{{(a)}}{{\geq}} \gamma_{r}^{(t+1)}+\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\mu} \rangle-\mathrm{SNR}\sqrt{\frac{8\log(6n/\delta)}{d}}n\] \[\stackrel{{(b)}}{{\geq}}0.99\langle\mathbf{w}_{r}^{ (0)},\boldsymbol{\mu}\rangle>0,\]

where inequality (a) is by Lemma D.2, and inequality (b) is by Lemma B.2. Similarly, the same result holds for the other modality. Together, it shows \(r\in\mathcal{U}_{+}^{(t+1)}\cap\widetilde{\mathcal{U}}_{+}^{(t+1)}\) and the induction is complete.

Similarly, we can use the same strategy to prove claim (2) for \(r\in\mathcal{U}_{-}^{(0)}\cap\widetilde{\mathcal{U}}_{-}^{(0)}\).

Next, we analyze the case where \(r\in\mathcal{U}_{+}^{(t)}\cap\widetilde{\mathcal{U}}_{-}^{(t)}\).

\[\gamma_{r}^{(t+1)} =\gamma_{r}^{(t)}+\frac{\eta}{nm\tau}\sum_{i=1}^{n}(1-\ell_{i}^{ \prime(t)})\sigma(\langle\widetilde{\mathbf{w}}_{r}^{(t)},y_{i}\widetilde{ \boldsymbol{\mu}}\rangle)\sigma^{\prime}(\langle\mathbf{w}_{r}^{(t)},y_{i} \boldsymbol{\mu}\rangle)y_{i}\|\boldsymbol{\mu}\|_{2}^{2}\] \[\quad-\frac{\eta}{nm\tau}\sum_{i=1}^{n}\sum_{j\neq i}^{M}\ell_{i, j}^{\prime(t)}\sigma(\langle\widetilde{\mathbf{w}}_{r}^{(t)},y_{j} \widetilde{\boldsymbol{\mu}}\rangle)\sigma^{\prime}(\langle\mathbf{w}_{r}^{(t )},y_{i}\boldsymbol{\mu}\rangle)y_{i}\|\boldsymbol{\mu}\|_{2}^{2}\] \[=\gamma_{r}^{(t)}-\frac{\eta}{nm\tau}\sum_{i:y_{i}=1}^{n}\sum_{j \neq i}^{M}\ell_{i,j}^{\prime(t)}\sigma(\langle\widetilde{\mathbf{w}}_{r}^{(t )},y_{j}\widetilde{\boldsymbol{\mu}}\rangle)\|\boldsymbol{\mu}\|_{2}^{2}< \gamma_{r}^{(t)},\] (37)

where the second equality follows from \(y_{i}\neq y_{j}\). Similarly,

\[\widetilde{\gamma}_{r}^{(t+1)} =\widetilde{\gamma}_{r}^{(t)}+\frac{\eta}{nm\tau}\sum_{i=1}^{n}(1- \ell_{i}^{\prime(t)})\sigma(\langle\mathbf{w}_{r}^{(t)},y_{i}\boldsymbol{\mu }\rangle)\sigma^{\prime}(\langle\widetilde{\mathbf{w}}_{r}^{(t)},y_{i} \widetilde{\boldsymbol{\mu}}\rangle)y_{i}\|\widetilde{\boldsymbol{\mu}}\|_{2}^ {2}\] \[\quad-\frac{\eta}{nm\tau}\sum_{i=1}^{n}\sum_{j\neq i}^{M}\ell_{i,j} ^{\prime(t)}\sigma(\langle\mathbf{w}_{r}^{(t)},y_{j}\boldsymbol{\mu}\rangle) \sigma^{\prime}(\langle\widetilde{\mathbf{w}}_{r}^{(t)},y_{i}\widetilde{ \boldsymbol{\mu}}\rangle)y_{i}\|\widetilde{\boldsymbol{\mu}}\|_{2}^{2}\] \[=\widetilde{\gamma}_{r}^{(t)}+\frac{\eta}{nm\tau}\sum_{i:y_{i}=-1 }^{n}\sum_{j=1}^{M}\ell_{i,j}^{\prime(t)}\sigma(\langle\mathbf{w}_{r}^{(t)},y _{j}\boldsymbol{\mu}\rangle)\|\widetilde{\boldsymbol{\mu}}\|_{2}^{2}>\widetilde {\gamma}_{r}^{(t)}.\] (38)

Then it is clear that the change of \(\gamma_{r}^{(t)},\widetilde{\gamma}_{r}^{(t)}\) does not align with the sign of its initialization. Therefore there exists some time \(t^{\prime}\geq 0\) such that either \(r\in\mathcal{U}_{+}^{(t^{\prime})}\cap\widetilde{\mathcal{U}}_{+}^{(t^{ \prime})}\) or \(r\in\mathcal{U}_{-}^{(t^{\prime})}\cap\widetilde{\mathcal{U}}_{-}^{(t^{\prime})}\). We prove this claim by contradiction. Suppose for all \(t\geq 0\), \(r\in\mathcal{U}_{+}^{(t)}\cap\widetilde{\mathcal{U}}_{-}^{(t)}\), then by (37) and (38), we see \(\gamma_{r}^{(t+1)}\leq\gamma_{r}^{(t)}\leq 0\) and \(\widetilde{\gamma}_{r}^{(t+1}\geq\widetilde{\gamma}_{r}^{(t)}\geq 0\). Because \(\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\mu}\rangle\leq 1.01\langle\mathbf{w}_{r}^{(0)}, \boldsymbol{\mu}\rangle+\gamma_{r}^{(t)}\) and \(\langle\widetilde{\mathbf{w}}_{r}^{(t)},\widetilde{\boldsymbol{\mu}}\rangle \geq 0.99\langle\widetilde{\mathbf{w}}_{r}^{(0)},\widetilde{\boldsymbol{\mu}} \rangle+\widetilde{\gamma}_{r}^{(t)}\), this raises a contradiction as either \(\langle\mathbf{w}_{r}^{(t)},\boldsymbol{\mu}\rangle\leq 0\) or \(\langle\widetilde{\mathbf{w}}_{r}^{(t)},\widetilde{\boldsymbol{\mu}}\rangle \geq 0\). 

With Lemma D.3 at hand, we are ready to demonstrate the lower bound of the growth rate for signal learning.

**Lemma D.4**.: _With the same condition as in Lemma C.2 and Lemma C.3 and \(n\geq 2500\log(4/\delta)\), define \(A_{r}^{(t)}=\gamma_{r}^{(t)}+\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\mu}\rangle\) for \(r\in\mathcal{U}_{+}^{(0)}\); and \(A_{r}^{(t)}=-\gamma_{r}^{(t)}-\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\mu}\rangle\) for \(r\in\mathcal{U}_{-}^{(0)}\). Similarly, we define \(\widetilde{A}_{r}^{(t)}=\widetilde{\gamma}_{r}^{(t)}+\langle\widetilde{\mathbf{w}} _{r}^{(0)},\widetilde{\boldsymbol{\mu}}\rangle\) for \(r\in\widetilde{\mathcal{U}}_{+}^{(0)}\) and \(\widetilde{A}_{r}^{(t)}=-\widetilde{\gamma}_{r}^{(t)}-\langle\widetilde{\mathbf{w}} _{r}^{(0)},\widetilde{\boldsymbol{\mu}}\rangle\) for \(r\in\widetilde{\mathcal{U}}_{-}^{(0)}\). Then with probability at least \(1-\delta\), we have_

\[A_{r}^{(t)}\geq\Big{(}1+\frac{0.48\eta\|\boldsymbol{\mu}\|_{2}^{2}C_{\mu}}{m \tau}\Big{)}^{t}(A_{r}^{(0)}+\widetilde{A}_{r}^{(0)}/C_{\mu})-1\]\[\widetilde{A}_{r}^{(t)} \geq\Big{(}1+\frac{0.48\eta\|\bm{\mu}\|_{2}^{2}C_{\mu}}{m\tau} \Big{)}^{t}(C_{\mu}A_{r}^{(0)}+\widetilde{A}_{r}^{(0)})-1.\]

Proof.: Without loss of generality, we consider \(r\in\mathcal{U}_{+}^{(0)}\cap\widetilde{\mathcal{U}}_{+}^{(0)}\). Then by the update of \(\gamma_{r}^{(t)},\widetilde{\gamma}_{r}^{(t)}\), we have from (35) and (36)

\[\gamma_{r}^{(t+1)} =\gamma_{r}^{(t)}+\frac{\eta}{nm\tau}\sum_{i:y_{i}=1}(1-\ell_{i} ^{\prime(t)})\sigma(\langle\widetilde{\mathbf{w}}_{r}^{(t)},\widetilde{\bm{ \mu}}\rangle)\|\bm{\mu}\|_{2}^{2}\] (39) \[\widetilde{\gamma}_{r}^{(t+1)} =\widetilde{\gamma}_{r}^{(t)}+\frac{\eta}{nm\tau}\sum_{i:y_{i}=1 }(1-\ell_{i}^{\prime(t)})\sigma(\langle\mathbf{w}_{r}^{(t)},\bm{\mu}\rangle) \|\widetilde{\bm{\mu}}\|_{2}^{2}\] (40)

To achieve the lower bound, we define

\[A_{r}^{(t)} \triangleq\gamma_{r}^{(t)}+\langle\mathbf{w}_{r}^{(0)},\bm{\mu} \rangle,\quad\widetilde{A}_{r}^{(t)}=\widetilde{\gamma}_{r}^{(t)}+\langle \widetilde{\mathbf{w}}_{r}^{(0)},\widetilde{\bm{\mu}}\rangle.\]

By (39), we have the lower bound of update equation,

\[A_{r}^{(t+1)} =A_{r}^{(t)}+\frac{\eta}{nm\tau}\sum_{i:y_{i}=1}(1-\ell_{i}^{ \prime(t)})\sigma(\langle\widetilde{\mathbf{w}}_{r}^{(t)},\widetilde{\bm{\mu} }\rangle)\|\bm{\mu}\|_{2}^{2}\] \[\geq A_{r}^{(t)}+\frac{\eta}{nm\tau}(\frac{n}{2}-O(\sqrt{n}))(1- \frac{C_{\ell}}{1+M})\widetilde{A}_{r}^{(t)}\|\bm{\mu}\|_{2}^{2}\] \[\geq A_{r}^{(t)}+\frac{0.48\eta\|\bm{\mu}\|_{2}^{2}}{m\tau} \widetilde{A}_{r}^{(t)}.\]

The inequality is by Lemma B.3, Lemma D.1 where we recall \(c_{2}=1-\frac{C_{\ell}}{M+1}\). Similarly, by (40) we arrive at

\[\widetilde{A}_{r}^{(t+1)} \geq\widetilde{A}_{r}^{(t)}+\frac{0.48\eta\|\widetilde{\bm{\mu}} \|_{2}^{2}}{m\tau}A_{r}^{(t)}=\widetilde{A}_{r}^{(t)}+\frac{0.48\eta C_{\mu} ^{2}\|\bm{\mu}\|_{2}^{2}}{m\tau}A_{r}^{(t)}.\]

Then by combining the above two inequalities, we conclude that

\[C_{\mu}A_{r}^{(t+1)}+\widetilde{A}_{r}^{(t+1)} =C_{\mu}A_{r}^{(t)}+\widetilde{A}_{r}^{(t)}+\frac{0.48\eta\|\bm{ \mu}\|_{2}^{2}}{m\tau}(C_{\mu}\widetilde{A}_{r}^{(t)}+C_{\mu}^{2}A_{r}^{(t)})\] \[\geq\Big{(}1+\frac{0.48\eta\|\bm{\mu}\|_{2}^{2}C_{\mu}}{m\tau} \Big{)}(C_{\mu}A_{r}^{(t)}+\widetilde{A}_{r}^{(t)}).\]

Thus, we see the joint dynamics of \(\gamma_{r}^{(t)}\) and \(\widetilde{\gamma}_{r}^{(t)}\) exhibits exponential growth, i.e.,

\[C_{\mu}A_{r}^{(t)}+\widetilde{A}_{r}^{(t)} \geq\Big{(}1+\frac{0.48\eta\|\bm{\mu}\|_{2}^{2}C_{\mu}}{m\tau} \Big{)}^{t}(A_{r}^{(0)}+\widetilde{A}_{r}^{(0)}).\] (41)

To characterize the dynamics of \(\gamma_{r}^{(t)}\) individually, we next track the dynamics of \(C_{\mu}A_{r}^{(t)}-\widetilde{A}_{r}^{(t)}\) by subtracting (40) from (39), which gives

\[C_{\mu}A_{r}^{(t+1)}-\widetilde{A}_{r}^{(t+1)} =C_{\mu}A_{r}^{(t)}-\widetilde{A}_{r}^{(t)}+\frac{\eta\|\bm{\mu}\| _{2}^{2}}{nm\tau}\sum_{i:y_{i}=1}(1-\ell_{i}^{\prime(t)})\big{(}C_{\mu}\sigma( \langle\mathbf{v}_{r}^{(t)},\widetilde{\bm{\mu}}\rangle)-C_{\mu}^{2}\sigma( \langle\mathbf{w}_{r}^{(t)},\bm{\mu}\rangle)\big{)}\] \[\geq\Big{(}1-\frac{\eta C_{\mu}\|\bm{\mu}\|_{2}^{2}}{nm\tau}\sum_ {i:y_{i}=1}(1-\ell_{i}^{\prime(t)})\Big{)}\big{(}C_{\mu}A_{r}^{(t)}- \widetilde{A}_{r}^{(t)}\big{)}.\]

Then from the the propagation, we notice that the gap \(C_{\mu}A_{r}^{(t)}-\widetilde{A}_{r}^{(t)}\) exhibits exponential decay regardless of the sign. Thus, we have

\[C_{\mu}A_{r}^{(t)}-\widetilde{A}_{r}^{(t)}\geq\Big{(}1-\frac{\eta C_{\mu}\| \bm{\mu}\|_{2}^{2}}{nm\tau}\sum_{i:y_{i}=1}(1-\ell_{i}^{\prime(t)})\Big{)}^{t} \big{(}C_{\mu}A_{r}^{(0)}-\widetilde{A}_{r}^{(0)}\big{)}.\] (42)

Therefore, combining (41) and (42) yields:

\[C_{\mu}A_{r}^{(t)}+\widetilde{A}_{r}^{(t)}\geq\Big{(}1+\frac{0.48\eta\|\bm{\mu} \|_{2}^{2}C_{\mu}}{m\tau}\Big{)}^{t}(C_{\mu}A_{r}^{(0)}+\widetilde{A}_{r}^{(0)}),\]\[C_{\mu}A_{r}^{(t)}-\widetilde{A}_{r}^{(t)}\geq\Big{(}1-\frac{\eta\|\bm{\mu}\|_{2}^ {2}}{nm\tau}\sum_{i:y_{i}=1}(1-\ell_{i}^{\prime(t)})\Big{)}^{t}\big{(}C_{\mu}A_ {r}^{(0)}-\widetilde{A}_{r}^{(0)}\big{)}.\]

Then it concludes that

\[A_{r}^{(t)} \geq\Big{(}1+\frac{0.48\eta\|\bm{\mu}\|_{2}^{2}C_{\mu}}{m\tau} \Big{)}^{t}(A_{r}^{(0)}+\widetilde{A}_{r}^{(0)}/C_{\mu})+\Big{(}1-\frac{\eta\| \bm{\mu}\|_{2}^{2}}{nm\tau}\sum_{i:y_{i}=1}(1-\ell_{i}^{\prime(t)})\Big{)}^{t} \big{(}A_{r}^{(0)}-\widetilde{A}_{r}^{(0)}/C_{\mu}\big{)}\] \[\geq\Big{(}1+\frac{0.48\eta\|\bm{\mu}\|_{2}^{2}C_{\mu}}{m\tau} \Big{)}^{t}(A_{r}^{(0)}+\widetilde{A}_{r}^{(0)}/C_{\mu})-|A_{r}^{(0)}- \widetilde{A}_{r}^{(0)}/C_{\mu}|,\] \[\widetilde{A}_{r}^{(t)} \geq\Big{(}1+\frac{0.48\eta\|\bm{\mu}\|_{2}^{2}C_{\mu}}{m\tau} \Big{)}^{t}(C_{\mu}A_{r}^{(0)}+\widetilde{A}_{r}^{(0)})-\Big{(}1-\frac{\eta\| \bm{\mu}\|_{2}^{2}}{nm\tau}\sum_{i:y_{i}=1}(1-\ell_{i}^{\prime(t)})\Big{)}^{t} \big{(}C_{\mu}A_{r}^{(0)}-\widetilde{A}_{r}^{(0)}\big{)}\] \[\geq\Big{(}1+\frac{0.48\eta\|\bm{\mu}\|_{2}^{2}C_{\mu}}{m\tau} \Big{)}^{t}(C_{\mu}A_{r}^{(0)}+\widetilde{A}_{r}^{(0)})-|C_{\mu}A_{r}^{(0)}- \widetilde{A}_{r}^{(0)}|\]

where we use the fact that \(\big{(}1-\frac{\eta\|\bm{\mu}\|_{2}^{2}}{nm\tau}\sum_{i:y_{i}=1}(1-\ell_{i}^{ \prime(t)})\big{)}^{t}\leq 1\). Next, we derive an upper bound on \(|A_{r}^{(0)}-\widetilde{A}_{r}^{(0)}/C_{\mu}|\), \(|C_{\mu}A_{r}^{(0)}-\widetilde{A}_{r}^{(0)}|\) as follows.

\[|A_{r}^{(0)}-\widetilde{A}_{r}^{(0)}/C_{\mu}|\leq|A_{r}^{(0)}|+| \widetilde{A}_{r}^{(0)}|/C_{\mu}\leq 2\sqrt{2\log(8m/\delta)}\sigma_{0}\|\bm{ \mu}\|_{2}\leq 1\]

where we recall that \(C_{\mu}\|\bm{\mu}\|_{2}=\|\widetilde{\bm{\mu}}\|_{2}\) and the second inequality is by the condition on \(\sigma_{0}\leq 0.5(2\log(8m/\delta))^{-1/2}\|\bm{\mu}\|_{2}^{-1}=\widetilde{O}(\|\bm{ \mu}\|_{2}^{-1})\). Similarly, we can show \(|C_{\mu}A_{r}^{(0)}-\widetilde{A}_{r}^{(0)}|\leq 1\) and thus we obtain

\[A_{r}^{(t)} \geq\Big{(}1+\frac{0.48\eta\|\bm{\mu}\|_{2}^{2}C_{\mu}}{m\tau} \Big{)}^{t}(A_{r}^{(0)}+\widetilde{A}_{r}^{(0)}/C_{\mu})-1\] (43) \[\widetilde{A}_{r}^{(t)} \geq\Big{(}1+\frac{0.48\eta\|\bm{\mu}\|_{2}^{2}C_{\mu}}{m\tau} \Big{)}^{t}(C_{\mu}A_{r}^{(0)}+\widetilde{A}_{r}^{(0)})-1\] (44)

#### d.1.2 Dynamics of Noise Memorization: Upper Bound

In order to characterize the growth of \(\rho_{r,i}^{(t)},\widehat{\rho}_{r,i}^{(t)}\), we partition the samples according to its sign of inner product.

\[\mathcal{I}_{r,+}^{(t)}=\{i\in[n]:\langle\mathbf{w}_{r}^{(t)}, \bm{\xi}_{i}\rangle>0\}, \widetilde{\mathcal{I}}_{r,+}^{(t)}=\{i\in[n]:\langle\widetilde{\mathbf{w}}_ {r}^{(0)},\widetilde{\bm{\xi}}_{i}\rangle>0\},\] \[\mathcal{I}_{r,-}^{(t)}=\{i\in[n]:\langle\mathbf{w}_{r}^{(t)}, \bm{\xi}_{i}\rangle<0\}, \widetilde{\mathcal{I}}_{r,-}^{(t)}=\{i\in[n]:\langle\mathbf{v}_{r}^{(0)}, \widetilde{\bm{\xi}}_{i}\rangle<0\}.\]

We further define

\[B_{r,+,+}^{(t)}\triangleq\sum_{i:y_{i}=1}(\rho_{r,i}^{(t)}+\langle \mathbf{w}_{r}^{(0)},\bm{\xi}_{i}\rangle)\mathds{1}_{i\in\mathcal{I}_{r,+}^{(t) }\cap\widetilde{\mathcal{I}}_{r,+}^{(t)}},\] \[B_{r,+,-}^{(t)}\triangleq\sum_{i:y_{i}=1}(\rho_{r,i}^{(t)}+\langle \mathbf{w}_{r}^{(0)},\bm{\xi}_{i}\rangle)\mathds{1}_{i\in\mathcal{I}_{r,+}^{(t) }\cap\widetilde{\mathcal{I}}_{r,-}^{(t)}},\] \[B_{r,-,+}^{(t)}\triangleq\sum_{i:y_{i}=-1}(\rho_{r,i}^{(t)}+ \langle\mathbf{w}_{r}^{(0)},\bm{\xi}_{i}\rangle)\mathds{1}_{i\in\mathcal{I}_{r,+}^ {(t)}\cap\widetilde{\mathcal{I}}_{r,-}^{(t)}},\] \[B_{r,-,-}^{(t)}\triangleq\sum_{i:y_{i}=-1}(\rho_{r,i}^{(t)}+ \langle\mathbf{w}_{r}^{(0)},\bm{\xi}_{i}\rangle)\mathds{1}_{i\in\mathcal{I}_{r,+}^ {(t)}\cap\widetilde{\mathcal{I}}_{r,-}^{(t)}},\]

On the other hand, we define

\[\widetilde{B}_{r,+,+}^{(t)}\triangleq\sum_{i:y_{i}=1}(\widetilde{ \rho}_{r,i}^{(t)}+\langle\widetilde{\mathbf{w}}_{r}^{(0)},\widetilde{\bm{\xi}}_{i} \rangle)\mathds{1}_{i\in\widetilde{\mathcal{I}}_{r,+}^{(t)}\cap\mathcal{I}_{r,+ }^{(t)}},\] \[\widetilde{B}_{r,+,-}^{(t)}\triangleq\sum_{i:y_{i}=1}(\widetilde{ \rho}_{r,i}^{(t)}+\langle\widetilde{\mathbf{w}}_{r}^{(0)},\widetilde{\bm{\xi}}_{i} \rangle)\mathds{1}_{i\in\widetilde{\mathcal{I}}_{r,+}^{(t)}\cap\mathcal{I}_{r,- }^{(t)}},\] \[\widetilde{B}_{r,-,+}^{(t)}\triangleq\sum_{i:y_{i}=-1}(\widetilde{ \rho}_{r,i}^{(t)}+\langle\widetilde{\mathbf{w}}_{r}^{(0)},\widetilde{\bm{\xi}}_{i} \rangle)\mathds{1}_{i\in\widetilde{\mathcal{I}}_{r,+}^{(t)}\cap\mathcal{I}_{r,+ }^{(t)}},\]\[\widetilde{B}^{(t)}_{r,-,-}\triangleq\sum_{i:y_{i}=-1}(\widetilde{\rho}^{(t)}_{r,i}+ \langle\widetilde{\mathbf{w}}^{(0)}_{r},\boldsymbol{\xi}_{i}\rangle)\mathds{1}_ {i\in\widetilde{\mathcal{I}}^{(t)}_{r,+}\cap\mathcal{I}^{(t)}_{r,-}}\,.\]

Before we state the main result, we provide some useful lemmas.

**Lemma D.5**.: _Suppose \(\delta>0\), the with probability at least \(1-\delta\), we have_

\[B^{(0)}_{r,+,+} \leq\frac{1}{2}\sigma_{0}\sigma_{\xi}\sqrt{dn}(1+\sqrt{\log(1/ \delta)/2}),\quad B^{(0)}_{r,-,+}\leq\frac{1}{2}\sigma_{0}\sigma_{\xi}\sqrt{dn }(1+\sqrt{\log(1/\delta)/2}).\] \[\widetilde{B}^{(0)}_{r,+,+} \leq\frac{1}{2}\sigma_{0}\sigma_{\xi}\sqrt{dn}(1+\sqrt{\log(1/ \delta)/2}),\quad\widetilde{B}^{(0)}_{r,-,+}\leq\frac{1}{2}\sigma_{0}\sigma_{ \xi}\sqrt{dn}(1+\sqrt{\log(1/\delta)/2}).\]

Proof.: By Bernstein's inequality, for arbitrary \(t>0\), we have

\[P(|B^{(0)}_{r,+,+}-\frac{1}{2}\sigma_{0}\sigma_{\xi}\sqrt{nd}|>t)\leq\exp(- \frac{t^{2}}{2n/4\sigma_{0}^{2}\sigma_{\xi}^{2}d}).\]

Setting \(t=\frac{1}{2}\sigma_{0}\sigma_{\xi}\sqrt{nd\log(1/\delta)/2}\), we further have

\[B^{(0)}_{r,+}\leq\frac{1}{2}\sigma_{0}\sigma_{\xi}\sqrt{dn}(1+\sqrt{\log(1/ \delta)/2}).\]

Similarly, the same result holds for \(B^{(0)}_{r,-}\). 

**Lemma D.6**.: _Under the condition \(d\geq\frac{300\sqrt{2n^{3}\log(6n^{2}/\delta)}}{\sigma_{0}\sigma_{\xi}\sqrt{n }\log(1/(1-(\delta/m)^{2}))}\), then with probability at least \(1-\delta\), it satisfies_

\[\widetilde{B}^{(0)}_{r,+,+}>150\sqrt{\frac{\log(6n^{2}/\delta)}{d}}n^{2},\quad B ^{(0)}_{r,+,+}>150\sqrt{\frac{\log(6n^{2}/\delta)}{d}}n^{2}.\]

Proof of Lemma D.6.: Here we want to show \(\widetilde{B}^{(0)}_{r,+,+}\geq 150\sqrt{\frac{\log(6n^{2}/\delta)}{d}}n^{2}\) with high probability. To see this, because \(\widetilde{B}^{(0)}_{r,+,+}\) is a random variable with positive mean and variance \(\sigma_{0}^{2}\sigma_{\xi}^{2}dn/4\), by Lemma B.1, we compute

\[\mathbb{P}\big{(}\widetilde{B}^{(0)}_{r,+,+}\leq t\big{)}\leq\sqrt{1-\exp \big{(}-\frac{8t^{2}}{\pi\sigma_{0}^{2}\sigma_{\xi}^{2}dn}\big{)}},\]

Thus when \(d\geq\frac{300\sqrt{2n^{3}\log(6n^{2}/\delta)}}{\sigma_{0}\sigma_{\xi}\sqrt{n }\log(1/(1-(\delta/m)^{2}))}\), we have \(\mathbb{P}\big{(}\widetilde{B}^{(0)}_{r,+,+}\leq 150\sqrt{\frac{\log(6n^{2}/ \delta)}{d}}n^{2}\big{)}\leq\delta/m\) and by union bound, we have with probability at least \(1-\delta\), it holds \(\widetilde{B}^{(0)}_{r,+,+}>150\sqrt{\frac{\log(6n^{2}/\delta)}{d}}n^{2}\). 

**Lemma D.7**.: _Under the condition \(n\geq\frac{8\log(1/\delta)}{\log(1/(1-(\delta/m)^{2}))}\), then with probability at least \(1-\delta\), it satisfies_

\[|\langle\mathbf{w}^{(0)}_{r},\boldsymbol{\xi}_{i}\rangle|>\frac{B^{(0)}_{r,+,+ }}{n},\quad|\langle\widetilde{\mathbf{w}}^{(0)}_{r},\boldsymbol{\xi}_{i} \rangle|>\frac{\widetilde{B}^{(0)}_{r,+,+}}{n}.\]

Proof of Lemma D.7.: Here we want to show \(|\langle\mathbf{w}^{(0)}_{r},\boldsymbol{\xi}_{i}\rangle|>\frac{B^{(0)}_{r,+,+ }}{n}\) with high probability. To see this, because \(\langle\mathbf{w}^{(0)}_{r},\boldsymbol{\xi}_{i}\rangle\) is a random variable with positive mean and variance \(\sigma_{0}^{2}\sigma_{\xi}^{2}d\). Besides, by Lemma C.9, we have

\[\frac{B^{(0)}_{r,+,+}}{n}\leq\frac{\sigma_{0}\sigma_{\xi}\sqrt{dn}(1+\sqrt{\log (1/\delta)/2})}{n}.\]

By Lemma B.1, we compute

\[\mathbb{P}\big{(}|\langle\mathbf{w}^{(0)}_{r},\boldsymbol{\xi}_{i}\rangle|\leq t \big{)}\leq\sqrt{1-\exp\big{(}-\frac{8t^{2}}{\pi\sigma_{0}^{2}\sigma_{\xi}^{2}d }\big{)}},\]Thus when \(n\geq\frac{8\log(1/\delta)}{\log(1/(1-(\delta/m)^{2}))}\), we have

\[\mathbb{P}\big{(}|\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i} \rangle|\leq\frac{\sigma_{0}\sigma_{\xi}\sqrt{dn}(1+\sqrt{\log(1/\delta)/2})}{n }\big{)}\leq\delta/m\]

and by union bound, we have with probability at least \(1-\delta\), it holds \(|\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}\rangle|>\frac{B_{r,+}^{(0)} }{n}\).

Similarly, we can conclude that \(|\langle\widetilde{\mathbf{w}}_{r}^{(0)},\widetilde{\boldsymbol{\xi}}_{i} \rangle|>\frac{\widetilde{B}_{r,+}^{(0)}}{n}\). 

**Lemma D.8**.: _Under Assumption 4.1, with probability at least \(1-\delta\), we have_

\[B_{r,+,+}^{(t)} \leq(1+\frac{1.05\eta}{nm\tau})^{t}B_{r,+,+}^{(0)},\quad B_{r,+, -}^{(t)}\leq B_{r,+,-}^{(0)},\] \[B_{r,-,+}^{(t)} \leq(1+\frac{1.05\eta}{nm\tau})^{t}B_{r,-,+}^{(0)},\quad B_{r,-, -}^{(t)}\leq B_{r,-,-}^{(0)},\] \[\widetilde{B}_{r,+,+}^{(t)} \leq(1+\frac{1.05\eta}{nm\tau})^{t}\widetilde{B}_{r,+,+}^{(0)}, \quad\widetilde{B}_{r,+,-}^{(t)}\leq\widetilde{B}_{r,+,-}^{(0)},\] \[\widetilde{B}_{r,-,+}^{(t)} \leq(1+\frac{1.05\eta}{nm\tau})^{t}\widetilde{B}_{r,-,+}^{(0)}, \quad\widetilde{B}_{r,-,-}^{(t)}\leq\widetilde{B}_{r,-,-}^{(0)}.\]

Proof of Lemma d.8.: According to the iteration equations for \(\rho_{r,i}^{(t)}\) and \(\widetilde{\rho}_{r,i}^{(t)}\), we have

\[B_{r,+,+}^{(t+1)} =B_{r,+,+}^{(t)}+\frac{\eta}{nm\tau}\sum_{i:y_{i}=1}\Bigg{[}(1- \ell_{i}^{(t)})\langle\widetilde{\mathbf{w}}_{r}^{(t)},\widetilde{\boldsymbol {\xi}}_{i}\rangle-\sum_{j:y_{j}=-1}\ell_{i,j}^{(t)}\langle\widetilde{\mathbf{ w}}_{r}^{(t)},\widetilde{\boldsymbol{\xi}}_{j}\rangle\mathds{1}_{j\in \widetilde{T}_{r,+}}\Bigg{]}\mathds{1}_{i\in\mathcal{I}_{r,+}\cap\widetilde{T}_ {r,+}}\|\boldsymbol{\xi}_{i}\|_{2}^{2}\] \[\leq B_{r,+,+}^{(t)}+\frac{\eta}{nm\tau}\sum_{i:y_{i}=1}\mathds{1 }_{i\in\mathcal{I}_{r,+}\cap\widetilde{T}_{r,+}}\bigg{[}\frac{C_{\ell}(M+1)-1} {C_{\ell}(M+1)}(\langle\widetilde{\mathbf{w}}_{r}^{(0)},\widetilde{\boldsymbol {\xi}}_{i}\rangle+\widetilde{\rho}_{r,i}^{(t)}+6\sqrt{\frac{\log(6n^{2}/ \delta)}{d}}n)\] \[\quad-\sum_{j:y_{j}=-1}\frac{1}{C_{\ell}(M+1)}(\langle\widetilde{ \mathbf{w}}_{r}^{(0)},\widetilde{\boldsymbol{\xi}}_{j}\rangle+\widetilde{\rho }_{r,j}^{(t)}-6\sqrt{\frac{\log(6n^{2}/\delta)}{d}}n)\mathds{1}_{j\in \widetilde{T}_{r,+}}\bigg{]}(\sigma_{\xi}^{2}d+\sigma_{\xi}^{2}\sqrt{d\log(6n^ {2}/\delta)})\] \[\leq B_{r,+,+}^{(t)}+\frac{\eta}{nm\tau}\bigg{[}\frac{C_{\ell}(M+ 1)-1}{C_{\ell}(M+1)}(\widetilde{B}_{r,+,+}^{(t)}+\sum_{i:y_{i}=1}\mathds{1}_{i \in\mathcal{I}_{r,+}\cap\widetilde{T}_{r,+}}6\sqrt{\frac{\log(6n^{2}/\delta) }{d}}n))\] \[\quad-\sum_{i:y_{i}=1}\mathds{1}_{i\in\mathcal{I}_{r,+}\cap \widetilde{T}_{r,+}}\frac{1}{C_{\ell}(M+1)}(\widetilde{B}_{r,-,+}^{(t)}+ \widetilde{B}_{r,-,-}^{(t)}-\sum_{j\neq i}6\sqrt{\frac{\log(6n^{2}/\delta)}{d} }n)\bigg{]}(\sigma_{\xi}^{2}d+\sigma_{\xi}^{2}\sqrt{d\log(6n^{2}/\delta)})\] \[\leq B_{r,+,+}^{(t)}+\frac{\eta}{nm\tau}\bigg{[}1.01B_{r,+,+}^{(t )}-\frac{0.99^{2}}{4C_{l}}(\widetilde{B}_{r,-,+}^{(t)}+\widetilde{B}_{r,-,-}^ {(t)})\bigg{]}1.035\sigma_{\xi}^{2}d\] \[\leq B_{r,+,+}^{(t)}+\frac{\eta}{nm\tau}\bigg{[}1.05B_{r,+,+}^{(t )}-\frac{1}{4}(\widetilde{B}_{r,-,+}^{(t)}+\widetilde{B}_{r,-,-}^{(t)})\bigg{]} \sigma_{\xi}^{2}d,\]

where the first inequality is by Lemma C.2, Lemma C.1 and Lemma B.5. Furthermore, the second inequality is by Lemma C.8 and the definition of \(\widetilde{B}_{r,-,+}^{(t)}\) and \(\widetilde{B}_{r,-,-}^{(t)}\). The third inequality is by Lemma D.6, \(\widetilde{B}_{r,+,+}^{(0)}>150\sqrt{\frac{\log(6n^{2}/\delta)}{d}}n^{2}\), \(d>10000\log(6n^{2}/\delta)\), and \(n\geq 1280000\log(4/\delta)\). The last inequality is by choosing \(C_{\ell}=1.01\).

Next, we establish the following inequality

\[B_{r,+,-}^{(t+1)} =B_{r,+,-}^{(t)}-\frac{\eta}{nm\tau}\sum_{i:y_{i}=1}\Bigg{[}\sum_ {j:y_{j}=-1}\ell_{i,j}^{(t)}\langle\widetilde{\mathbf{w}}_{r}^{(t)}, \widetilde{\boldsymbol{\xi}}_{j}\rangle\mathds{1}_{j\in\widetilde{T}_{r,+}} \Bigg{]}\mathds{1}_{i\in\mathcal{I}_{r,+}\cap\widetilde{T}_{r,-}}\|\boldsymbol{ \xi}_{i}\|_{2}^{2}\] \[\leq B_{r,+,-}^{(t)}-\frac{\eta}{nm\tau}\sum_{i:y_{i}=1}\mathds{1 }_{i\in\mathcal{I}_{r,+}\cap\widetilde{T}_{r,+}}\bigg{[}\sum_{j:y_{j}=-1}\frac{ 1}{C_{\ell}(M+1)}(\langle\widetilde{\mathbf{w}}_{r}^{(0)},\widetilde{ \boldsymbol{\xi}}_{j}\rangle+\widetilde{\rho}_{r,j}^{(t)}-6\sqrt{\frac{\log(6n^{2}/ \delta)}{d}}n)\mathds{1}_{j\in\widetilde{T}_{r,+}}\bigg{]}\] \[\quad(\sigma_{\xi}^{2}d-\sigma_{\xi}^{2}\sqrt{d\log(6n^{2}/\delta)})\]\[\leq B_{r,+,-}^{(t)}-\frac{\eta}{n\tau m}\sum_{i:y_{i}=1}\bigg{[} \mathbf{1}_{i\in\mathcal{I}_{r,+}\cap\widetilde{\mathcal{I}}_{r,-}+}\frac{1}{C_{ \ell}(M+1)}(\widetilde{B}_{r,-,+}^{(t)}+\widetilde{B}_{r,-,-}^{(t)}-\sum_{j\neq i }6\sqrt{\frac{\log(6n^{2}/\delta)}{d}}n)\bigg{]}\] \[(\sigma_{\xi}^{2}d-\sigma_{\xi}^{2}\sqrt{d\log(6n^{2}/\delta)})\] \[\leq B_{r,+,-}^{(t)}-\frac{\eta}{nm\tau}\bigg{[}\frac{0.99^{2}}{ 4C_{l}}(\widetilde{B}_{r,-,+}^{(t)}+\widetilde{B}_{r,-,-}^{(t)})\bigg{]}0.99 \sigma_{\xi}^{2}d\] \[\leq B_{r,+,-}^{(t)}-\frac{\eta}{nm\tau}\bigg{[}0.24(\widetilde{B }_{r,-,+}^{(t)}+\widetilde{B}_{r,-,-}^{(t)})\bigg{]}\sigma_{\xi}^{2}d,\]

where the first inequality is by Lemma C.2, Lemma C.1 and Lemma B.5. Furthermore, the second inequality is by Lemma C.8 and the definition of \(\widetilde{B}_{r,-,+}^{(t)}\) and \(\widetilde{B}_{r,-,-}^{(t)}\). The third inequality is by Lemma D.6 and \(\widetilde{B}_{r,+,+}^{(0)}>150\sqrt{\frac{\log(6n^{2}/\delta)}{d}}n^{2}\), \(d>10000\log(6n^{2}/\delta)\), and \(n\geq 1280000\log(4/\delta)\).

Similarly, we have,

\[B_{r,-,+}^{(t+1)} \leq B_{r,-,+}^{(t)}+\frac{\eta}{nm\tau}\bigg{[}1.05B_{r,-,+}^{( t)}-0.25(\widetilde{B}_{r,+,+}^{(t)}+\widetilde{B}_{r,+,-}^{(t)})\bigg{]} \sigma_{\xi}^{2}d,\] \[B_{r,-,-}^{(t+1)} \leq B_{r,-,-}^{(t)}-\frac{\eta}{nm\tau}\bigg{[}0.24(\widetilde{B }_{r,+,+}^{(t)}+\widetilde{B}_{r,+,-}^{(t)})\bigg{]}\sigma_{\xi}^{2}d\]

For the second modality, with the same derivative, we could obtain the following inequalities:

\[\widetilde{B}_{r,+,+}^{(t+1)} \leq\widetilde{B}_{r,+,+}^{(t)}+\frac{\eta}{nm\tau}\bigg{[}1.05 \widetilde{B}_{r,+,+}^{(t+1)}-0.25(B_{r,-,+}^{(t)}+B_{r,-,-}^{(t)})\bigg{]} \sigma_{\xi}^{2}d,\] \[\widetilde{B}_{r,+,-}^{(t+1)} \leq\widetilde{B}_{r,+,+}^{(t)}-\frac{\eta}{nm\tau}\bigg{[}0.24(B _{r,-,+}^{(t)}+B_{r,-,-}^{(t)})\bigg{]}\sigma_{\xi}^{2}d,\] \[\widetilde{B}_{r,-,+}^{(t+1)} \leq\widetilde{B}_{r,-,+}^{(t)}+\frac{\eta}{nm\tau}\bigg{[}1.05 \widetilde{B}_{r,-,+}^{(t+1)}-0.25(B_{r,+,+}^{(t)}+B_{r,+,-}^{(t)})\bigg{]} \sigma_{\xi}^{2}d,\] \[\widetilde{B}_{r,-,+}^{(t+1)} \leq\widetilde{B}_{r,-,-}^{(t)}-\frac{\eta}{nm\tau}\bigg{[}0.24(B _{r,+,+}^{(t)}+B_{r,+,-}^{(t)})\bigg{]}\sigma_{\xi}^{2}d,\]

which completes the proof. 

We define

\[\Psi_{r,i}^{(t)} \triangleq\rho_{r,i}^{(t)}+\langle\mathbf{w}_{r}^{(0)},\boldsymbol {\xi}_{i}\rangle,i\in\mathcal{I}_{r,+}^{(t)}\] \[\widetilde{\Psi}_{r,i}^{(t)} \triangleq\rho_{r,i}^{(t)}+\langle\widetilde{\mathbf{w}}_{r}^{(0) },\boldsymbol{\widetilde{\xi}}_{i}\rangle,i\in\widetilde{\mathcal{I}}_{r,+}^{ (t)}\]

**Lemma D.9**.: _Under Assumption 4.1, with probability at least \(1-\delta\), for all \(0\leq t\leq T_{1}\),_

1. _for_ \(i\in\mathcal{I}_{r,-}^{(0)}\cap\widetilde{\mathcal{I}}_{r,-}^{(0)}\)_, we have_ \(\rho_{r,i}^{(t)}=0\)_,_ \(\mathcal{I}_{r,-}^{(t)}=\mathcal{I}_{r,-}^{(0)}\) _and_ \(\widetilde{\rho}_{r,i}^{(t)}=0\)_,_ \(\widetilde{\mathcal{I}}_{r,-}^{(t)}=\widetilde{\mathcal{I}}_{r,-}^{(0)}\)_._
2. _for_ \(i\in\mathcal{I}_{r,-}^{(0)}\cap\widetilde{\mathcal{I}}_{r,+}^{(0)}\)_, we have_ \(\rho_{r,i}^{(t)}=0\)_,_ \(\mathcal{I}_{r,-}^{(t)}=\mathcal{I}_{r,-}^{(0)}\) _and_ \(\widetilde{\rho}_{r,i}^{(t)}\leq 0\)_._
3. _for_ \(i\in\mathcal{I}_{r,+}^{(0)}\cap\widetilde{\mathcal{I}}_{r,-}^{(0)}\)_, we have_ \(\widetilde{\rho}_{r,i}^{(t)}=0\)_,_ \(\widetilde{\mathcal{I}}_{r,-}^{(t)}=\widetilde{\mathcal{I}}_{r,-}^{(0)}\) _and_ \(\rho_{r,i}^{(t)}\leq 0\)_._
4. _for_ \(i\in\mathcal{I}_{r,+}^{(0)}\cap\widetilde{\mathcal{I}}_{r,+}^{(0)}\)_, we have_ \[\Psi_{r,i}^{(t)} \leq(1+\frac{1.06\eta\sigma_{\xi}^{2}d}{nm\tau})^{t}(\Psi_{r,i}^{ (t)}+\widetilde{\Psi}_{r,i}^{(t)}),\] (45) \[\widetilde{\Psi}_{r,i}^{(t)} \geq\frac{101C_{\ell}}{M+1}(B_{r,+,+}^{(t)}+B_{r,+,-}^{(t)}).\] (46)

Proof of Lemma D.9.: We partition the dynamics of \(\rho_{r,i}^{(t)}\) and \(\widetilde{\rho}_{r,i}^{(t)}\) into one of the four cases according to its initialization (1) \(i\in\mathcal{I}_{r,+}^{(0)}\cap\widetilde{\mathcal{I}}_{r,-}^{(0)}\) (2) \(i\in\mathcal{I}_{r,-}^{(0)}\cap\widetilde{\mathcal{I}}_{r,+}^{(0)}\), (3) \(i\in\mathcal{I}_{r,+}^{(0)}\cap\widetilde{\mathcal{I}}_{r,+}^{(0)}\), (4) \(i\in\mathcal{I}_{r,-}^{(0)}\cap\widetilde{\mathcal{I}}_{r,-}^{(0)}\).

(1) In the case of \(i\in\mathcal{I}^{(0)}_{r,-}\cap\widetilde{\mathcal{I}}^{(0)}_{r,-}\), it holds that

\[\rho^{(t)}_{r,i}=0,\ \mathcal{I}^{(t)}_{r,-}=\mathcal{I}^{(0)}_{r,-};\quad \widetilde{\rho}^{(t)}_{r,i}=0,\ \widetilde{\mathcal{I}}^{(t)}_{r,-}=\widetilde{\mathcal{I}}^{(0)}_{r,-}.\]

The proof is by the induction method. It is clear when \(t=0\), \(\rho^{(0)}_{r,i}=0\) and \(\widetilde{\rho}^{(0)}_{r,i}=0\). Therefore, the induction argument holds at the initial step.

Suppose at iteration \(t\), we have \(\rho^{(t)}_{r,i}=0\) and \(\widetilde{\rho}^{(t)}_{r,i}=0\). Then we have

\[\langle\widetilde{\mathbf{w}}^{(t)}_{r},\widetilde{\boldsymbol{ \xi}}_{i}\rangle \leq\frac{1}{2}\langle\widetilde{\mathbf{w}}^{(0)}_{r},\widetilde{ \boldsymbol{\xi}}_{i}\rangle+\widetilde{\rho}^{(t)}_{r,i}=\frac{1}{2}\langle \widetilde{\mathbf{w}}^{(0)}_{r},\widetilde{\boldsymbol{\xi}}_{i}\rangle<0,\] \[\langle\mathbf{w}^{(t)}_{r},\boldsymbol{\xi}_{i}\rangle \leq\frac{1}{2}\langle\mathbf{w}^{(0)}_{r},\boldsymbol{\xi}_{i} \rangle+\rho^{(t)}_{r,i}=\frac{1}{2}\langle\mathbf{w}^{(0)}_{r},\boldsymbol{ \xi}_{i}\rangle<0.\]

Thus by the update of \(\widetilde{\rho}^{(t+1)}_{r,i}\), we have

\[\widetilde{\rho}^{(t+1)}_{r,i} =\widetilde{\rho}^{(t)}_{r,i}+\frac{\eta}{nm\tau}(1-\ell^{(t)}_{ i})\sigma(\langle\mathbf{w}^{(t)}_{r},\boldsymbol{\xi}_{i}\rangle) \sigma^{\prime}(\langle\widetilde{\mathbf{w}}^{(t)}_{r},\widetilde{\boldsymbol {\xi}}_{i}\rangle)\|\widetilde{\boldsymbol{\xi}}_{i}\|_{2}^{2}\] \[\quad-\frac{\eta}{nm\tau}\sum_{j\neq i}^{M}\ell^{(t)}_{i,j} \sigma(\langle\mathbf{w}^{(t)}_{r},\boldsymbol{\xi}_{j}\rangle)\sigma^{\prime }(\langle\widetilde{\mathbf{w}}^{(t)}_{r},\widetilde{\boldsymbol{\xi}}_{i} \rangle)\|\widetilde{\boldsymbol{\xi}}_{i}\|_{2}^{2}=0.\]

Here we have used \(\langle\widetilde{\mathbf{w}}^{(t)}_{r},\widetilde{\boldsymbol{\xi}}_{i} \rangle<0\). Similarly,

\[\rho^{(t+1)}_{r,i} =\rho^{(t)}_{r,i}+\frac{\eta}{nm\tau}(1-\ell^{(t)}_{i})\sigma( \langle\widetilde{\mathbf{w}}^{(t)}_{r},\widetilde{\boldsymbol{\xi}}_{i} \rangle)\sigma^{\prime}(\langle\mathbf{w}^{(t)}_{r},\boldsymbol{\xi}_{i} \rangle)\|\boldsymbol{\xi}_{i}\|_{2}^{2}\] \[\quad-\frac{\eta}{nm\tau}\sum_{j=1}^{M}\ell^{(t)}_{i,j}\sigma( \langle\widetilde{\mathbf{w}}^{(t)}_{r},\widetilde{\boldsymbol{\xi}}_{j} \rangle)\sigma^{\prime}(\langle\mathbf{w}^{(t)}_{r},\boldsymbol{\xi}_{i} \rangle)\|\boldsymbol{\xi}_{i}\|_{2}^{2}=0.\]

(2) In the case of \(i\in\mathcal{I}^{(0)}_{r,-}\cap\widetilde{\mathcal{I}}^{(0)}_{r,+}\), We use induction. It is clear when \(t=0\), we have \(\rho^{(0)}_{r,i}=0\).

Suppose at iteration \(t\), it holds that \(\rho^{(t)}_{r,i}=0\) Then by the propagation of \(\rho^{(t)}_{r,i}\), we have

\[\rho^{(t+1)}_{r,i} =\rho^{(t)}_{r,i}+\frac{\eta}{nm\tau}(1-\ell^{\prime(t)}_{i}) \sigma(\langle\widetilde{\mathbf{w}}^{(t)}_{r},\widetilde{\boldsymbol{\xi}}_{i }\rangle)\sigma^{\prime}(\langle\mathbf{w}^{(t)}_{r},\boldsymbol{\xi}_{i} \rangle)\|\boldsymbol{\xi}_{i}\|_{2}^{2}\] \[\quad-\frac{\eta}{nm\tau}\sum_{j\neq i}^{M}\ell^{(t)}_{i,j}\sigma( \langle\widetilde{\mathbf{w}}^{(t)}_{r},\widetilde{\boldsymbol{\xi}}_{j} \rangle)\sigma^{\prime}(\langle\mathbf{w}^{(t)}_{r},\boldsymbol{\xi}_{i} \rangle)\|\boldsymbol{\xi}_{i}\|_{2}^{2}=0.\]

On the other hand,

\[\widetilde{\rho}^{(t+1)}_{r,i} =\widetilde{\rho}^{(t)}_{r,i}+\frac{\eta}{nm\tau}(1-\ell^{\prime (t)}_{i})\sigma(\langle\mathbf{w}^{(t)}_{r},\boldsymbol{\xi}_{i}\rangle) \sigma^{\prime}(\langle\widetilde{\mathbf{w}}^{(t)}_{r},\widetilde{\boldsymbol {\xi}}_{i}\rangle)\|\widetilde{\boldsymbol{\xi}}_{i}\|_{2}^{2}\] \[\quad-\frac{\eta}{nm\tau}\sum_{j\neq i}^{M}\ell^{\prime(t)}_{i,j} \sigma(\langle\mathbf{w}^{(t)}_{r},\boldsymbol{\xi}_{j}\rangle)\sigma^{\prime }(\langle\widetilde{\mathbf{w}}^{(t)}_{r},\widetilde{\boldsymbol{\xi}}_{i} \rangle)\|\widetilde{\boldsymbol{\xi}}_{i}\|_{2}^{2}\] \[=\widetilde{\rho}^{(t)}_{r,i}-\frac{\eta}{nm\tau}\left[\sum_{j \in\mathcal{I}_{r,+}}\ell^{(t)}_{i,j}\sigma(\langle\mathbf{w}^{(t)}_{r}, \boldsymbol{\xi}_{j}\rangle)\right]\|\widetilde{\boldsymbol{\xi}}_{i}\|_{2}^{2}\] \[\leq\widetilde{\rho}^{(t)}_{r,i},\]

where the first equation is by \(\sigma(\langle\mathbf{w}^{(t)}_{r},\boldsymbol{\xi}_{i}\rangle)\geq 0\).

(3) In the case of \(i\in\mathcal{I}^{(0)}_{r,+}\cap\widetilde{\mathcal{I}}^{(0)}_{r,-}\), we use induction to prove such claim. It is clear when \(t=0\), \(\rho^{(0)}_{r,i}=0\). Suppose at iteration \(t\), we have \(\widetilde{\rho}^{(t)}_{r,i}=0\). Then we have

\[\langle\widetilde{\mathbf{w}}^{(t)}_{r},\widetilde{\boldsymbol{\xi}}_{i} \rangle\leq\frac{1}{2}\langle\widetilde{\mathbf{w}}^{(0)}_{r},\widetilde{ \boldsymbol{\xi}}_{i}\rangle+\widetilde{\rho}^{(t)}_{r,i}=\frac{1}{2}\langle \widetilde{\mathbf{w}}^{(0)}_{r},\widetilde{\boldsymbol{\xi}}_{i}\rangle<0.\]Thus by the update of \(\widetilde{\rho}^{(t+1)}_{r,i}\):

\[\widetilde{\rho}^{(t+1)}_{r,i} =\widetilde{\rho}^{(t)}_{r,i}+\frac{\eta}{nm\tau}(1-\ell^{\prime(t)} _{i})\sigma(\langle\mathbf{w}^{(t)}_{r},\boldsymbol{\xi}_{i}\rangle)\sigma^{ \prime}(\langle\mathbf{w}^{(t)}_{r},\boldsymbol{\xi}_{i}\rangle)\|\boldsymbol{ \xi}_{i}\|_{2}^{2}\] \[\quad-\frac{\eta}{nm\tau}\sum_{j\neq i}^{M}\ell^{\prime(t)}_{i,j} \sigma(\langle\mathbf{w}^{(t)}_{r},\boldsymbol{\xi}_{j}\rangle)\sigma^{\prime }(\langle\mathbf{w}^{(t)}_{r},\boldsymbol{\xi}_{i}\rangle)\|\boldsymbol{\xi}_ {i}\|_{2}^{2},\]

On the other hand,

\[\rho^{(t+1)}_{r,i} =\rho^{(t)}_{r,i}+\frac{\eta}{nm\tau}(1-\ell^{\prime(t)}_{i}) \sigma(\langle\widetilde{\mathbf{w}}^{(t)}_{r},\boldsymbol{\xi}_{i}\rangle) \sigma^{\prime}(\langle\mathbf{w}^{(t)}_{r},\boldsymbol{\xi}_{i}\rangle)\| \boldsymbol{\xi}_{i}\|_{2}^{2}\] \[\quad-\frac{\eta}{nm\tau}\sum_{j\neq i}^{M}\ell^{\prime(t)}_{i,j} \sigma(\langle\widetilde{\mathbf{w}}^{(t)}_{r},\boldsymbol{\xi}_{j}\rangle) \sigma^{\prime}(\langle\mathbf{w}^{(t)}_{r},\boldsymbol{\xi}_{i}\rangle)\| \boldsymbol{\xi}_{i}\|_{2}^{2},\] \[=\rho^{(t)}_{r,i}-\frac{\eta}{nm\tau}\left[\sum_{j\in\widetilde{ \mathcal{I}}_{r,+}}\ell^{\prime(t)}_{i,j}\sigma(\langle\widetilde{\mathbf{w}} ^{(t)}_{r},\boldsymbol{\xi}_{j}\rangle)\right]\|\boldsymbol{\xi}_{i}\|_{2}^{2}\] \[\leq\rho^{(t)}_{r,i},\]

where the first equation is by \(\sigma(\langle\widetilde{\mathbf{w}}^{(t)}_{r},\boldsymbol{\xi}_{i}\rangle) \geq 0\).

(4) Finally, in the case of \(i\in\mathcal{I}^{(0)}_{r,+}\cap\widetilde{\mathcal{I}}^{(0)}_{r,+}\), we have

\[\Psi^{(t+1)}_{r,i} \leq\Psi^{(t)}_{r,i}+\frac{\eta}{nm\tau}(1-\ell^{\prime(t)}_{i}) \sigma(\langle\widetilde{\mathbf{w}}^{(t)}_{r},\boldsymbol{\widetilde{\xi}}_ {i}\rangle)\sigma^{\prime}(\langle\mathbf{w}^{(t)}_{r},\boldsymbol{\xi}_{i} \rangle)\|\boldsymbol{\xi}_{i}\|_{2}^{2}\] \[\leq\Psi^{(t)}_{r,i}+\frac{1.05\eta\sigma_{\xi}^{2}d}{nm\tau} \widetilde{\Psi}^{(t)}_{r,i}.\]

Similarly, for the other modality,

\[\widetilde{\Psi}^{(t+1)}_{r,i} \leq\widetilde{\Psi}^{(t)}_{r,i}+\frac{\eta}{nm\tau}(1-\ell^{ \prime(t)}_{i})\sigma(\langle\mathbf{w}^{(t)}_{r},\boldsymbol{\xi}_{i}\rangle )\sigma^{\prime}(\langle\widetilde{\mathbf{w}}^{(t)}_{r},\boldsymbol{\widetilde {\xi}}_{i}\rangle)\|\boldsymbol{\widetilde{\xi}}_{i}\|_{2}^{2}\] \[\leq\widetilde{\Psi}^{(t)}_{r,i}+\frac{1.05\eta\sigma_{\xi}^{2}d}{ nm\tau}\Psi^{(t)}_{r,i}.\]

Together, we can achieve that

\[\Psi^{(t)}_{r,i}+\widetilde{\Psi}^{(t)}_{r,i}\leq(1+\frac{1.05\eta\sigma_{\xi} ^{2}d}{nm\tau})^{t}(\Psi^{(0)}_{r,i}+\widetilde{\Psi}^{(0)}_{r,i}).\] (47)

On the other hand size, we calculate the upper bound of \(\Psi^{(t)}_{r,i}-\widetilde{\Psi}^{(t)}_{r,i}\) as follows

\[\Psi^{(t+1)}_{r,i}-\widetilde{\Psi}^{(t+1)}_{r,i} =\Psi^{(t)}_{r,i}-\widetilde{\Psi}^{(t)}_{r,i}+\frac{\eta}{nm\tau }(1-\ell^{\prime(t)}_{i})\sigma(\langle\widetilde{\mathbf{w}}^{(t)}_{r}, \boldsymbol{\widetilde{\xi}}_{i}\rangle)\sigma^{\prime}(\langle\mathbf{w}^{(t )}_{r},\boldsymbol{\xi}_{i}\rangle)\|\boldsymbol{\xi}_{i}\|_{2}^{2}\] \[\quad-\frac{\eta}{nm\tau}(1-\ell^{\prime(t)}_{i})\sigma(\langle \mathbf{w}^{(t)}_{r},\boldsymbol{\xi}_{i}\rangle)\sigma^{\prime}(\langle \widetilde{\mathbf{w}}^{(t)}_{r},\boldsymbol{\widetilde{\xi}}_{i}\rangle) \|\boldsymbol{\widetilde{\xi}}_{i}\|_{2}^{2}\] \[\quad-\frac{\eta}{nm\tau}\sum_{j\neq i}^{M}\ell^{\prime(t)}_{i,j} \sigma(\langle\widetilde{\mathbf{w}}^{(t)}_{r},\boldsymbol{\widetilde{\xi}}_{ i}\rangle)\sigma^{\prime}(\langle\mathbf{w}^{(t)}_{r},\boldsymbol{\xi}_{j}\rangle) \|\boldsymbol{\widetilde{\xi}}_{i}\|_{2}^{2}\] \[\quad+\frac{\eta}{nm\tau}\sum_{j\neq i}^{M}\ell^{\prime(t)}_{i,j} \sigma(\langle\mathbf{w}^{(t)}_{r},\boldsymbol{\xi}_{j}\rangle)\sigma^{\prime}( \langle\widetilde{\mathbf{w}}^{(t)}_{r},\boldsymbol{\widetilde{\xi}}_{i}\rangle) \|\boldsymbol{\widetilde{\xi}}_{i}\|_{2}^{2}\] \[\leq\Psi^{(t)}_{r,i}-\widetilde{\Psi}^{(t)}_{r,i}-\frac{0.96\eta \sigma_{\xi}^{2}d}{nm\tau}(\Psi^{(t)}_{r,i}-\widetilde{\Psi}^{(t)}_{r,i})+ \frac{1.01\eta\sigma_{\xi}^{2}d}{nm\tau}\frac{C_{\ell}}{M+1}(B^{(t)}_{r,+,+}+B^{ (t)}_{r,+,-})\] \[\leq(1-\frac{0.95\eta\sigma_{\xi}^{2}d}{nm\tau})(\Psi^{(t)}_{r,i}- \widetilde{\Psi}^{(t)}_{r,i}),\]

where the first inequality is by Lemma D.2 and Lemma D.1, the second inequality is by induction (46). Therefore we conclude that

\[\Psi^{(t)}_{r,i}-\widetilde{\Psi}^{(t)}_{r,i}\leq(1-\frac{0.95\eta\sigma_{\xi}^{2 }d}{nm\tau})^{t}(\Psi^{(0)}_{r,i}-\widetilde{\Psi}^{(0)}_{r,i}).\] (48)Combining (47) and (48) yields

\[\Psi^{(t)}_{r,i} \leq(1+\frac{1.05\eta\sigma_{\xi}^{2}d}{nm\tau})^{t}(\Psi^{(0)}_{r,i }+\widetilde{\Psi}^{(0)}_{r,i})+(1-\frac{0.95\eta\sigma_{\xi}^{2}d}{nm\tau})^{t} (\Psi^{(0)}_{r,i}-\widetilde{\Psi}^{(0)}_{r,i})\] \[\leq(1+\frac{1.06\eta\sigma_{\xi}^{2}d}{nm\tau})^{t}(\Psi^{(0)}_{r,i}+\widetilde{\Psi}^{(0)}_{r,i}).\]

#### d.1.3 Signal Learning: Proof of Lemma 5.4

Before proving Lemma 5.4, we require the following lower bound for the initialization. Recall the definition that \(A^{(t)}_{r}=\gamma^{(t)}_{r}+\langle\mathbf{w}^{(0)}_{r},\boldsymbol{\mu}\rangle\) for \(r\in\mathcal{U}^{(0)}_{+}\); and \(A^{(t)}_{r}=-\gamma^{(t)}_{r}-\langle\mathbf{w}^{(0)}_{r},\boldsymbol{\mu}\rangle\) for \(r\in\mathcal{U}^{(0)}_{-}\). Similarly, we have \(\widetilde{A}^{(t)}_{r}=\widetilde{\gamma}^{(t)}_{r}+\langle\widetilde{ \mathbf{w}}^{(0)}_{r},\widetilde{\boldsymbol{\mu}}\rangle\) for \(r\in\widetilde{\mathcal{U}}^{(0)}_{+}\) and \(\widetilde{A}^{(t)}_{r}=-\widetilde{\gamma}^{(t)}_{r}-\langle\widetilde{ \mathbf{w}}^{(0)}_{r},\widetilde{\boldsymbol{\mu}}\rangle\) for \(r\in\widetilde{\mathcal{U}}^{(0)}_{-}\).

**Lemma D.10**.: _Suppose \(\delta>0\) and \(m\geq\widetilde{\Omega}(1)\). Then with probability at least \(1-\delta\), we have_

\[\frac{1}{m}\sum_{r\in\mathcal{U}^{(0)}_{+}\cap\widetilde{ \mathcal{U}}^{(0)}_{-}}(A^{(0)}_{r}+\widetilde{A}^{(0)}_{r}/C_{\mu}) \geq 0.2\sigma_{0}\|\boldsymbol{\mu}\|_{2}\] \[\frac{1}{m}\sum_{r\in\mathcal{U}^{(0)}_{-}\cap\widetilde{ \mathcal{U}}^{(0)}_{-}}(A^{(0)}_{r}+\widetilde{A}^{(0)}_{r}/C_{\mu}) \geq 0.2\sigma_{0}\|\boldsymbol{\mu}\|_{2}\]

Proof of Lemma D.10.: We first note that \(\langle\mathbf{w}^{(0)}_{r},\boldsymbol{\mu}\rangle\sim\mathcal{N}(0,\sigma _{0}^{2}\|\boldsymbol{\mu}\|_{2}^{2})\) and \(\langle\widetilde{\mathbf{w}}^{(0)}_{r},\widetilde{\boldsymbol{\mu}}\rangle \sim\mathcal{N}(0,\sigma_{0}^{2}\|\widetilde{\boldsymbol{\mu}}\|_{2}^{2})\). We define the event \(\mathcal{A}=\{r\in[m]:\langle\mathbf{w}^{(0)}_{r},\boldsymbol{\mu}\rangle>0, \langle\widetilde{\mathbf{w}}^{(0)}_{r},\widetilde{\boldsymbol{\mu}}\rangle >0\}\). Then we can compute

\[\mathbb{E}[\langle\mathbf{w}^{(0)}_{r},\boldsymbol{\mu}\rangle \mathds{1}(\mathcal{A})+\langle\widetilde{\mathbf{w}}^{(0)}_{r},\widetilde{ \boldsymbol{\mu}}/C_{\mu}\rangle\mathds{1}(\mathcal{A})]\] \[=\mathbb{E}[\langle\mathbf{w}^{(0)}_{r},\boldsymbol{\mu}\rangle \mathds{1}(\langle\mathbf{w}^{(0)}_{r},\boldsymbol{\mu}\rangle>0)]+\frac{1}{ 2}\mathbb{E}[\langle\widetilde{\mathbf{w}}^{(0)}_{r},\widetilde{\boldsymbol{ \mu}}/C_{\mu}\rangle\mathds{1}(\langle\widetilde{\mathbf{w}}^{(0)}_{r}, \widetilde{\boldsymbol{\mu}}\rangle>0)]\] \[=\frac{\sigma_{0}\|\boldsymbol{\mu}\|_{2}}{\sqrt{2\pi}}\]

where we use the independence of neurons in two modalities. Let \(S\coloneqq\sum_{r=1}^{m}\langle\mathbf{w}^{(0)}_{r},\boldsymbol{\mu}\rangle \mathds{1}(\mathcal{A})+\langle\widetilde{\mathbf{w}}^{(0)}_{r},\widetilde{ \boldsymbol{\mu}}/C_{\mu}\rangle\mathds{1}(\mathcal{A})\). Then we apply the sub-Gaussian concentration inequality that with probability at least \(1-\delta/2\),

\[\left|\sum_{r\in\mathcal{A}}\big{(}\langle\mathbf{w}^{(0)}_{r},\boldsymbol{ \mu}\rangle+\langle\widetilde{\mathbf{w}}^{(0)}_{r},\widetilde{\boldsymbol{ \mu}}/C_{\mu}\rangle\big{)}-\frac{m\sigma_{0}\|\boldsymbol{\mu}\|_{2}}{\sqrt{2 \pi}}\right|\leq\widetilde{O}(m^{-1/2}).\]

Then suppose \(m=\widetilde{\Omega}(1)\), we have

\[\frac{1}{m}\sum_{r\in\mathcal{A}}\big{(}\langle\mathbf{w}^{(0)}_{r}, \boldsymbol{\mu}\rangle+\langle\widetilde{\mathbf{w}}^{(0)}_{r},\widetilde{ \boldsymbol{\mu}}/C_{\mu}\rangle\big{)}\geq 0.2\sigma_{0}\|\boldsymbol{\mu}\|_{2}.\]

Similarly, we can show the same for the event where \(\langle\mathbf{w}^{(0)}_{r},\boldsymbol{\mu}\rangle<0,\langle\widetilde{ \mathbf{w}}^{(0)}_{r},\widetilde{\boldsymbol{\mu}}\rangle<0\) and taking the union bound completes the proof. 

Proof of Lemma 5.4.: From the upper bound on noise memorization (45), we take the maximum over \(r,i\), which gives

\[\max_{r,i}\Psi^{(t)}_{r,i} \leq\Big{(}1+1.06\frac{\eta\sigma_{\xi}^{2}d}{nm\tau}\Big{)}^{t} \max_{r,i}\Psi^{(0)}_{r,i}\] \[\leq\Big{(}1+1.06\frac{\eta\sigma_{\xi}^{2}d}{nm\tau}\Big{)}^{t}2 \sqrt{\log(8mn/\delta)}\sigma_{0}\sigma_{\xi}\sqrt{d}.\]At the same time, for signal learning, from the lower bound on signal learning in Lemma D.4, we have for the first modality that

\[A_{r}^{(t)} \geq\Big{(}1+\frac{0.48\eta\|\boldsymbol{\mu}\|_{2}^{2}C_{\mu}}{m \tau}\Big{)}^{t}(A_{r}^{(0)}+\widetilde{A}_{r}^{(0)}/C_{\mu})-1\]

Taking a summation over the \(r\in\mathcal{U}_{+}^{(0)}\cap\widetilde{\mathcal{U}}_{+}^{(0)}\), we obtain

\[\frac{1}{m}\sum_{r\in\mathcal{U}_{+}^{(0)}\cap\widetilde{\mathcal{ U}}_{+}^{(0)}}A_{r}^{(t)} \geq\Big{(}1+\frac{0.48\eta\|\boldsymbol{\mu}\|_{2}^{2}C_{\mu}}{m \tau}\Big{)}^{t}\frac{1}{m}\sum_{r\in\mathcal{U}_{+}^{(0)}\cap\widetilde{ \mathcal{U}}_{+}^{(0)}}(A_{r}^{(0)}+\widetilde{A}_{r}^{(0)}/C_{\mu})-1\] \[\geq\Big{(}1+\frac{0.48\eta\|\boldsymbol{\mu}\|_{2}^{2}C_{\mu}}{m \tau}\Big{)}^{t}0.2\sigma_{0}\|\boldsymbol{\mu}\|_{2}-1\]

where the second inequality is due to Lemma D.10.

Under the SNR condition \(n\cdot\mathrm{SNR}^{2}\geq 1.7\) and \(C_{\mu}>2.66\), we can see there exists a scale difference between \(\max_{r,i}\Psi_{r,i}^{(t)}\) and \(\frac{1}{m}\sum_{r\in\mathcal{U}_{+}^{(0)}\cap\widetilde{\mathcal{U}}_{+}^{( 0)}}A_{r}^{(t)}\) at the end of first stage. Let

\[T_{1}=\log\big{(}20/(\sigma_{0}\|\boldsymbol{\mu}\|_{2})\big{)}/\log\big{(}1+ 0.48C_{\mu}\frac{\eta\|\boldsymbol{\mu}\|_{2}^{2}}{m\tau}\big{)}.\]

Then we have \(\frac{1}{m}\sum_{r\in\mathcal{U}_{+}^{(0)}\cap\widetilde{\mathcal{U}}_{+}^{(0) }}A_{r}^{(t)}\) reach \(3\) within \(T_{1}\) iterations. Using similar analysis, we can show at the same time \(\frac{1}{m}\sum_{r\in\mathcal{U}_{-}^{(0)}\cap\widetilde{\mathcal{U}}_{-}^{(0 )}}A_{r}^{(t)}\), \(\frac{1}{m}\sum_{r\in\mathcal{U}_{+}^{(0)}\cap\widetilde{\mathcal{U}}_{+}^{(0 )}}\widetilde{A}_{r}^{(t)},\frac{1}{m}\sum_{r\in\mathcal{U}_{-}^{(0)}\cap \widetilde{\mathcal{U}}_{-}^{(0)}}\widetilde{A}_{r}^{(t)}\) also reach \(3\).

On the other hand, we compute the scale of \(\max_{r,i}\Psi_{r,i}^{(T_{1})}\) as

\[\max_{r}\Psi_{r,i}^{(T_{1})} \leq\Big{(}1+1.06\frac{\eta\sigma_{\xi}^{2}d}{nm\tau}\Big{)}^{t}2 \sqrt{\log(8mn/\delta)}\sigma_{0}\sigma_{\xi}\sqrt{d}\] \[=\exp\Big{(}\frac{\log(1+1.06\frac{\eta\sigma_{\xi}^{2}d}{nm\tau} )}{\log(1+0.48C_{\mu}\frac{\eta\|\boldsymbol{\mu}\|_{2}^{2}}{m\tau})}\log \big{(}20/(\sigma_{0}\|\boldsymbol{\mu}\|_{2})\big{)}\Big{)}2\sqrt{\log(8mn/ \delta)}\sigma_{0}\sigma_{\xi}\sqrt{d}\] \[\leq\exp\big{(}(2.21/(C_{\mu}n\cdot\mathrm{SNR}^{2})+O((\frac{\eta \sigma_{\xi}^{2}d}{nm\tau}))^{2})\log\big{(}20/(\sigma_{0}\|\boldsymbol{\mu}\|_ {2})\big{)}\big{)}2\sqrt{\log(8mn/\delta)}\sigma_{0}\sigma_{\xi}\sqrt{d}\] \[\leq\exp\big{(}(2.21/(C_{\mu}n\cdot\mathrm{SNR}^{2})+0.01)\log \big{(}20/(\sigma_{0}\|\boldsymbol{\mu}\|_{2})\big{)}\big{)}2\sqrt{\log(8mn/ \delta)}\sigma_{0}\sigma_{\xi}\sqrt{d}\] \[\leq\exp\big{(}0.5\log\big{(}20/(\sigma_{0}\|\boldsymbol{\mu}\|_{2 })\big{)}2\sqrt{\log(8mn/\delta)}\sigma_{0}\sigma_{\xi}\sqrt{d}\] \[=\sqrt{24\log(8mn/\delta)}\frac{\sqrt{\sigma_{0}}\sigma_{\xi}\sqrt {d}}{\sqrt{\|\boldsymbol{\mu}\|_{2}}}\] \[=\widetilde{O}(n^{-1/2}),\]

where we choose \(\eta\) sufficiently small for the second inequality. In third inequality, we have applied the condition that \(n\mathrm{SNR}^{2}=\Theta(1)\) and \(\sigma_{0}\leq\frac{1}{\|\boldsymbol{\mu}\|_{2}}\). The last inequality is by the SNR condition. Because we can choose \(n\geq C\log(m/\delta)\) for sufficiently large constant \(C\), \(\max_{r,i}\Psi_{r,i}^{(T_{1})}=o(1)\). 

### Second Stage

We first show a similar result as in Lemma C.14 for both two modalities.

**Lemma D.11**.: _Under conditions, for \(0\leq t\leq T^{*}\), we have_

\[\|\nabla L_{S}(\mathbf{W}^{(t)})\|_{F}^{2} \leq O(\max\{\|\boldsymbol{\mu}\|_{2}^{2},\sigma_{\xi}^{2}d\})L_{S} (\mathbf{W}^{(t)}),\] \[\|\nabla L_{S}(\widetilde{\mathbf{W}}^{(t)})\|_{F}^{2} \leq O(\max\{\|\widetilde{\boldsymbol{\mu}}\|_{2}^{2},\sigma_{\xi}^ {2}d\})L_{S}(\widetilde{\mathbf{W}}^{(t)}).\]

Proof of Lemma D.11.: The proof follows from that of Lemma C.14 and hence is omitted for clarity.

For notation convenience, we let

\[F_{0}(\mathbf{W},\mathbf{x}_{i}) =\mathrm{Sim}_{\mathbf{h},\mathbf{g}}(\mathbf{x}_{i},\widetilde{ \mathbf{x}}_{i})/\tau\] \[=\frac{1}{m\tau}\sum_{r=1}^{m}\sigma(\langle\mathbf{w}_{r},y_{i} \boldsymbol{\mu}\rangle)\mathrm{sg}(\sigma(\langle\widetilde{\mathbf{w}}_{r},y _{i}\widetilde{\boldsymbol{\mu}}\rangle))+\frac{1}{m\tau}\sum_{r=1}^{m}\sigma( \langle\mathbf{w}_{r},\boldsymbol{\xi}_{i}\rangle)\mathrm{sg}(\sigma(\langle \widetilde{\mathbf{w}}_{r},\widetilde{\boldsymbol{\xi}}_{i}\rangle))\] \[F_{j}(\mathbf{W},\mathbf{x}_{i}) =\mathrm{Sim}_{\mathbf{h},\mathbf{g}}(\mathbf{x}_{i},\widetilde{ \mathbf{x}}_{j})/\tau\] \[=\frac{1}{m\tau}\sum_{r=1}^{m}\sigma(\langle\mathbf{w}_{r},y_{i} \boldsymbol{\mu}\rangle)\mathrm{sg}(\sigma(\langle\widetilde{\mathbf{w}}_{r},y_{j}\widetilde{\boldsymbol{\mu}}\rangle))+\frac{1}{m\tau}\sum_{r=1}^{m} \sigma(\langle\mathbf{w}_{r},\boldsymbol{\xi}_{i}\rangle)\mathrm{sg}(\sigma( \langle\widetilde{\mathbf{w}}_{r},\widetilde{\boldsymbol{\xi}}_{j}\rangle)), \text{ for }j=1,...,M\] \[F_{0}(\widetilde{\mathbf{W}},\widetilde{\mathbf{x}}_{i}) =\mathrm{Sim}_{\mathbf{g},\mathbf{h}}(\widetilde{\mathbf{x}}_{i}, \mathbf{x}_{i})/\tau\] \[=\frac{1}{m\tau}\sum_{r=1}^{m}\mathrm{sg}(\sigma(\langle \mathbf{w}_{r},y_{i}\boldsymbol{\mu}\rangle))\sigma(\langle\widetilde{\mathbf{w} }_{r},y_{i}\widetilde{\boldsymbol{\mu}}\rangle)+\frac{1}{m\tau}\sum_{r=1}^{m} \mathrm{sg}(\sigma(\langle\mathbf{w}_{r},\boldsymbol{\xi}_{i}\rangle))\sigma( \langle\widetilde{\mathbf{w}}_{r},\widetilde{\boldsymbol{\xi}}_{i}\rangle)\] \[\widetilde{F}_{j}(\widetilde{\mathbf{W}},\widetilde{\mathbf{x}}_{i}) =\mathrm{Sim}_{\mathbf{g},\mathbf{h}}(\widetilde{\mathbf{x}}_{i}, \mathbf{x}_{j})/\tau\] \[=\frac{1}{m\tau}\sum_{r=1}^{m}\mathrm{sg}(\sigma(\langle \mathbf{w}_{r},y_{j}\boldsymbol{\mu}\rangle))\sigma(\langle\widetilde{\mathbf{ w}}_{r},y_{i}\widetilde{\boldsymbol{\mu}}\rangle)+\frac{1}{m\tau}\sum_{r=1}^{m} \mathrm{sg}(\sigma(\langle\mathbf{w}_{r},\boldsymbol{\xi}_{j}\rangle))\sigma( \langle\widetilde{\mathbf{w}}_{r},\widetilde{\boldsymbol{\xi}}_{i}\rangle), \text{ for }j=1,...,M\]

It is worth mentioning that \(F_{j}(\mathbf{W},\mathbf{x}_{i})=\widetilde{F}_{j}(\widetilde{\mathbf{W}}, \widetilde{\mathbf{x}}_{i})\) in terms of numerical values. They differ in terms of the derivatives.

We further denote

\[L_{S}(\mathbf{W}) =-\frac{1}{n}\sum_{i=1}^{n}L_{i}(\mathbf{W})=-\frac{1}{n}\sum_{i=1} ^{n}\log\Big{(}\frac{e^{\mathrm{Sim}_{\mathbf{h},\mathbf{g}}(\mathbf{x}_{i}, \widetilde{\mathbf{x}}_{i})/\tau}}{e^{\mathrm{Sim}_{\mathbf{h},\mathbf{g}}( \mathbf{x}_{i},\widetilde{\mathbf{x}}_{i})/\tau}+\sum_{j\neq i}^{M}e^{ \mathrm{Sim}_{\mathbf{h},\mathbf{g}}(\mathbf{x}_{i},\widetilde{\mathbf{x}}_{j} )/\tau}}\Big{)},\] \[\text{where }L_{i}(\mathbf{W}) =-\log\Big{(}\frac{e^{\mathrm{Sim}_{\mathbf{h},\mathbf{g}}( \mathbf{x}_{i},\widetilde{\mathbf{x}}_{i})/\tau}}{e^{\mathrm{Sim}_{\mathbf{h}, \mathbf{g}}(\mathbf{x}_{i},\widetilde{\mathbf{x}}_{i})/\tau}+\sum_{j\neq i}^{M} e^{\mathrm{Sim}_{\mathbf{h},\mathbf{g}}(\mathbf{x}_{i},\mathbf{x}_{j})/\tau}}\Big{)}=- \log\Big{(}\frac{e^{F_{0}(\mathbf{W},\mathbf{x}_{i})}}{e^{F_{0}(\mathbf{W}, \mathbf{x}_{i})}+\sum_{j=1}^{M}e^{F_{j}(\mathbf{W},\mathbf{x}_{i})}}\Big{)}\] \[L_{S}(\widetilde{\mathbf{W}}) =-\frac{1}{n}\sum_{i=1}^{n}L_{i}(\widetilde{\mathbf{W}})=-\frac{1} {n}\sum_{i=1}^{n}\log\Big{(}\frac{e^{\mathrm{Sim}_{\mathbf{g},\mathbf{h}}( \widetilde{\mathbf{x}}_{i},\mathbf{x}_{i})/\tau}}{e^{\mathrm{Sim}_{\mathbf{h}, \mathbf{g}}(\widetilde{\mathbf{x}}_{i},\mathbf{x}_{i})/\tau}+\sum_{j\neq i}^{M} e^{\mathrm{Sim}_{\mathbf{g},\mathbf{h}}(\widetilde{\mathbf{x}}_{i},\mathbf{x}_{j})/\tau}} \Big{)},\] \[\text{where }L_{i}(\widetilde{\mathbf{W}}) =-\log\Big{(}\frac{e^{\mathrm{Sim}_{\mathbf{g},\mathbf{h}}( \widetilde{\mathbf{x}}_{i},\mathbf{x}_{i})/\tau}}{e^{\mathrm{Sim}_{\mathbf{g}, \mathbf{h}}(\widetilde{\mathbf{x}}_{i},\mathbf{x}_{i})/\tau}+\sum_{j\neq i}^{M} e^{\mathrm{Sim}_{\mathbf{g},\mathbf{h}}(\widetilde{\mathbf{x}}_{i}, \mathbf{x}_{j})/\tau}}\Big{)}=-\log\Big{(}\frac{e^{F_{0}(\widetilde{\mathbf{W}},\widetilde{\mathbf{x}}_{i})}}{e^{F_{0}(\widetilde{\mathbf{W}},\widetilde{ \mathbf{x}}_{i})}+\sum_{j=1}^{M}e^{F_{j}(\widetilde{\mathbf{W}},\widetilde{ \mathbf{x}}_{i})}}\Big{)}\] \[\text{$\overline{L}(\mathbf{W},\mathbf{V})$} =L_{S}(\mathbf{W})+L_{S}(\widetilde{\mathbf{W}})\]

Here \(\overline{L}(\mathbf{W},\widetilde{\mathbf{W}})\) is the combined loss function for two modalities.

Let \(\theta_{r}=1\) if \(r\in\mathcal{U}_{+}^{(0)}\), i.e., \(\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\mu}\rangle>0\) and \(\theta_{r}=-1\) if \(r\in\mathcal{U}_{-}^{(0)}\), i.e., \(\langle\mathbf{w}_{r}^{(0)},\boldsymbol{\mu}\rangle<0\). Similarly, we let \(\widetilde{\theta}_{r}=1\) if \(r\in\widetilde{\mathcal{U}}_{+}^{(0)}\) and \(\widetilde{\theta}_{r}=-1\) if \(r\in\widetilde{\mathcal{U}}_{-}^{(0)}\). Then we define

\[\mathbf{w}_{r}^{*} =\mathbf{w}_{r}^{(0)}+2\tau\log(2M/\epsilon)\cdot\theta_{r}\cdot \frac{\boldsymbol{\mu}}{\|\boldsymbol{\mu}\|_{2}^{2}},\] \[\widetilde{\mathbf{w}}_{r}^{*} =\widetilde{\mathbf{w}}_{r}^{(0)}+2\tau\log(2M/\epsilon)\cdot \widetilde{\theta}_{r}\cdot\frac{\boldsymbol{\widetilde{\mu}}}{\|\boldsymbol{ \widetilde{\mu}}\|_{2}^{2}}.\]

**Lemma D.12**.: _Under Assumption 4.1, we have \(\|\mathbf{W}^{(T_{1})}-\mathbf{W}^{*}\|_{F}\leq\widetilde{O}(m^{1/2}\| \boldsymbol{\mu}\|_{2}^{-1})\) and \(\|\widetilde{\mathbf{W}}^{(T_{1})}-\widetilde{\mathbf{W}}^{*}\|_{F}\leq \widetilde{O}(m^{1/2}\|\boldsymbol{\widetilde{\mu}}\|_{2}^{-1})\)_

Proof of Lemma d.12.: The proof follows exactly the same as the proof in single-modal case. we include it here for completeness. Without loss of generality, we focus on the case for \(\mathbf{W}^{(T_{1})}\).

By the scale difference at \(T_{1}\), we have

\[\|\mathbf{W}^{(T_{1})}-\mathbf{W}^{*}\|_{F} \leq\|\mathbf{W}^{(T_{1})}-\mathbf{W}^{(0)}\|_{F}+\|\mathbf{W}^{(0 )}-\mathbf{W}^{*}\|_{F}\] \[\leq\sum_{r}\frac{\gamma_{r}^{(T_{1})}}{\|\boldsymbol{\mu}\|_{2} }+\sum_{r,i}\frac{|\rho_{r,i}^{(T_{1})}|}{\|\boldsymbol{\xi}_{i}\|_{2}}+O(m^{1/2} \log(1/\epsilon))\|\boldsymbol{\mu}\|_{2}^{-1}\]\[\leq O(m\|\boldsymbol{\mu}\|_{2}^{-1})+O(nm\sigma_{0})+O(m^{1/2}\log(1/ \epsilon)\|\boldsymbol{\mu}\|_{2}^{-1})\] \[\leq\widetilde{O}(m^{1/2}\|\boldsymbol{\mu}\|_{2}^{-1})\]

where the first inequality is by triangle inequality and the second inequality is by decomposition of \(\mathbf{W}^{(T_{1})}\) and \(\mathbf{W}^{*}\). The third inequality is by the bound on \(\gamma_{r}^{(T_{1})}\) and \(\rho_{r,i}^{(T_{1})}\) and Lemma B.5. The last inequality is by condition on \(\sigma_{0}\). 

**Lemma D.13**.: _Under Assumption 4.1, we have for all \(t\in[T_{1},T^{*}],\)_

\[\langle\nabla F_{0}(\mathbf{W}^{(t)},\mathbf{x}_{i}),\mathbf{W}^ {*}\rangle \geq 2\log(2M/\epsilon)\] \[\langle\nabla F_{j}(\mathbf{W}^{(t)},\mathbf{x}_{i}),\mathbf{W} ^{*}\rangle \leq\log(2M/\epsilon),\text{ for }j=1,...,M\] \[\langle\nabla F_{0}(\widetilde{\mathbf{W}}^{(t)},\mathbf{x}_{i}), \widetilde{\mathbf{W}}^{*}\rangle \geq 2\log(2M/\epsilon)\] \[\langle\nabla F_{j}(\widetilde{\mathbf{W}}^{(t)},\mathbf{x}_{j}), \widetilde{\mathbf{W}}^{*}\rangle \leq\log(2M/\epsilon),\text{ for }j=1,...,M\]

Proof of Lemma d.13.: The proof follows similarly from Lemma C.16 and here we only show the result for the first modality. Based on the definition of \(\mathbf{W}^{*}\) and \(F_{j}(\mathbf{W}^{(t)},\mathbf{x}_{i})\), we can derive for \(j=0\),

\[\langle\nabla F_{0}(\mathbf{W}^{(t)},\mathbf{x}_{i}),\mathbf{W}^ {*}\rangle\] \[=\sum_{r=1}^{m}\langle\nabla_{\mathbf{w}_{r}}F_{0}(\mathbf{W}^{( t)},\mathbf{x}_{i}),\mathbf{w}_{r}^{*}\rangle\] \[=\frac{1}{m\tau}\sum_{r=1}^{m}\sigma^{\prime}(\langle\mathbf{w}_ {r}^{(t)},y_{i}\boldsymbol{\mu}\rangle)\sigma(\langle\widetilde{\mathbf{w}}_{ r}^{(t)},y_{i}\widetilde{\boldsymbol{\mu}}\rangle)\langle\mathbf{w}_{r}^{*},y_{i} \boldsymbol{\mu}\rangle+\frac{1}{m\tau}\sum_{r=1}^{m}\sigma^{\prime}(\langle \mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{i}\rangle)\sigma(\langle\widetilde{ \mathbf{w}}_{r}^{(t)},\widetilde{\boldsymbol{\xi}}_{i}\rangle)\langle\mathbf{w} _{r}^{*},\boldsymbol{\xi}_{i}\rangle\] \[=\frac{1}{m\tau}\sum_{r=1}^{m}\sigma^{\prime}(\langle\mathbf{w}_ {r}^{(t)},y_{i}\boldsymbol{\mu}\rangle)\sigma(\langle\widetilde{\mathbf{w}}_{ r}^{(t)},y_{i}\widetilde{\boldsymbol{\mu}}\rangle)\Big{(}\langle\mathbf{w}_{r}^{(0)},y_{i} \boldsymbol{\mu}\rangle+2\tau\log(2M/\epsilon)\theta_{r}y_{i}\Big{)}\] \[\quad+\frac{1}{m\tau}\sum_{r=1}^{m}\sigma^{\prime}(\langle \mathbf{w}_{r}^{(t)},\boldsymbol{\xi}_{i}\rangle)\sigma(\langle\widetilde{ \mathbf{w}}_{r}^{(t)},\widetilde{\boldsymbol{\xi}}_{i}\rangle)\Big{(}\langle \mathbf{w}_{r}^{(0)},\boldsymbol{\xi}_{i}\rangle+2\tau\theta_{r}\log(2M/ \epsilon)\langle\boldsymbol{\xi}_{i},y_{i}\boldsymbol{\mu}\rangle\|\boldsymbol {\mu}\|_{2}^{-2}\Big{)}\] \[\geq\underbrace{\frac{1}{m\tau}\sum_{r=1}^{m}\sigma^{\prime}( \langle\mathbf{w}_{r}^{(t)},y_{i}\boldsymbol{\mu}\rangle)\sigma(\langle \widetilde{\mathbf{w}}_{r}^{(t)},y_{i}\widetilde{\boldsymbol{\mu}}\rangle)2 \tau\log(2M/\epsilon)\theta_{r}y_{i}}_{I_{1}}-\underbrace{\frac{1}{m\tau}\sum_ {r=1}^{m}\sigma(\langle\widetilde{\mathbf{w}}_{r}^{(t)},y_{i}\widetilde{ \boldsymbol{\mu}}\rangle)\widetilde{O}(\sigma_{0}\|\boldsymbol{\mu}\|_{2})}_{I _{2}}\] \[\quad-\underbrace{\frac{1}{m\tau}\sum_{r=1}^{m}\sigma(\langle \widetilde{\mathbf{w}}_{r}^{(t)},\widetilde{\boldsymbol{\xi}}_{i}\rangle)2 \tau\log(2M/\epsilon)\widetilde{O}(\sigma_{\xi}\|\boldsymbol{\mu}\|_{2}^{-1}) }_{I_{3}}-\underbrace{\frac{1}{m\tau}\sum_{r=1}^{m}\sigma(\langle\widetilde{ \mathbf{w}}_{r}^{(t)},\widetilde{\boldsymbol{\xi}}_{i}\rangle)\widetilde{O}( \sigma_{0}\sigma_{\xi}\sqrt{d})}_{I_{4}}.\]

First, we can bound \(I_{2}\leq\widetilde{O}(\sigma_{0}\|\boldsymbol{\mu}\|_{2})\), \(I_{3}\leq\log(2M/\epsilon)\widetilde{O}(\sigma_{\xi}\|\boldsymbol{\mu}\|_{2}^{ -1})\), \(I_{4}\leq\widetilde{O}(\sigma_{0}\sigma_{\xi}\sqrt{d})\) by the global bound on \(\sigma(\langle\widetilde{\mathbf{w}}_{r}^{(t)},\widetilde{\boldsymbol{\xi}}_{i} \rangle),\sigma(\langle\widetilde{\mathbf{w}}_{r}^{(t)},y_{i}\widetilde{ \boldsymbol{\mu}}\rangle)=\widetilde{O}(1)\).

Further, we lower bound \(I_{1}\) as follows. Without loss of generality, we suppose \(y_{i}=1\), then we have

\[I_{1}\geq\frac{1}{m\tau}\sum_{r\in\mathcal{U}_{+}^{(0)}\cap\widetilde{ \mathcal{U}}_{+}^{(0)}}\sigma(\langle\widetilde{\mathbf{w}}_{r}^{(t)},y_{i} \widetilde{\boldsymbol{\mu}}\rangle)2\tau\log(2M/\epsilon)\geq 4\log(2M/\epsilon)\]

where the last inequality is by Lemma 5.4 and the monotonicity of \(\widetilde{\gamma}_{r}^{(t)}\).

Then we can obtain

\[\langle\nabla F_{0}(\mathbf{W}^{(t)},\mathbf{x}_{i}),\mathbf{W}^{*}\rangle \geq 4\log(2M/\epsilon)-I_{2}-I_{3}-I-4\geq 2\log(2M/\epsilon).\]

The proof for \(F_{j}(\mathbf{W}^{(t)},\mathbf{W}^{*})\) follows the same argument as in Lemma C.16. 

**Lemma D.14**.: _Under Assumption 4.1, we have for all \(t\in[T_{1},T^{*}]\),_

\[\|\mathbf{W}^{(t)}-\mathbf{W}^{*}\|_{F}^{2}+\|\widetilde{\mathbf{W}}^{(t)}- \widetilde{\mathbf{W}}^{*}\|_{F}^{2}-\|\mathbf{W}^{(t+1)}-\mathbf{W}^{*}\|_{F}^ {2}-\|\widetilde{\mathbf{W}}^{(t+1)}-\widetilde{\mathbf{W}}^{*}\|_{F}^{2}\geq\eta \overline{L}(\mathbf{W}^{(t)},\widetilde{\mathbf{W}}^{(t)})-2\eta\epsilon\]Proof of Lemma D.14.: First, we see that \(\overline{L}(\mathbf{W}^{(t)},\widetilde{\mathbf{W}}^{(t)})=L_{S}(\mathbf{W}^{(t)} )+L_{S}(\widetilde{\mathbf{W}}^{(t)})\) is decomposable in terms of \(\mathbf{W}^{(t)}\) and \(\widetilde{\mathbf{W}}^{(t)}\). This suggests that \(\nabla_{\mathbf{W}}\overline{L}(\mathbf{W}^{(t)},\widetilde{\mathbf{W}}^{(t)} )=\nabla L_{S}(\mathbf{W}^{(t)})\) and \(\nabla_{\widetilde{\mathbf{W}}}\overline{L}(\mathbf{W}^{(t)},\widetilde{ \mathbf{W}}^{(t)})=\nabla\widetilde{L}_{S}(\widetilde{\mathbf{W}}^{(t)})\). Then following similar analysis as in Lemma C.17, we can first show

\[\langle F_{j}(\mathbf{W}^{(t)},\mathbf{x}_{i}),\mathbf{W}^{(t)} \rangle =F_{j}(\mathbf{W}^{(t)},\mathbf{x}_{i}),\text{ for }j=0,...,M,\] (49) \[\langle F_{j}(\widetilde{\mathbf{W}}^{(t)},\widetilde{\mathbf{x }}_{i}),\widetilde{\mathbf{W}}^{(t)}\rangle =F_{j}(\widetilde{\mathbf{W}}^{(t)},\widetilde{\mathbf{x}}_{i}),\text{ for }j=0,...,M.\] (50)

Then by the gradient descent update

\[\|\mathbf{W}^{(t)}-\mathbf{W}^{*}\|_{F}^{2}-\|\mathbf{W}^{(t+1)} -\mathbf{W}^{*}\|_{F}^{2}\] \[=2\eta\langle\nabla L_{S}(\mathbf{W}^{(t)}),\mathbf{W}^{(t)}- \mathbf{W}^{*}\rangle-\eta^{2}\|\nabla L_{S}(\mathbf{W}^{(t)})\|_{F}^{2}\] \[=\frac{2\eta}{n}\sum_{i=1}^{n}\sum_{j=0}^{M}\frac{\partial L_{i}( \mathbf{W}^{(t)})}{\partial F_{j}(\mathbf{W}^{(t)},\mathbf{x}_{i})}\langle \nabla F_{j}(\mathbf{W}^{(t)},\mathbf{x}_{i}),\mathbf{W}^{(t)}-\mathbf{W}^{*} \rangle-\eta^{2}\|\nabla L_{S}(\mathbf{W}^{(t)})\|_{F}^{2}\] \[=\frac{2\eta}{n}\sum_{i=1}^{n}\sum_{j=0}^{M}\frac{\partial L_{i}( \mathbf{W}^{(t)})}{\partial F_{j}(\mathbf{W}^{(t)},\mathbf{x}_{i})}\Big{(}F_{ j}(\mathbf{W}^{(t)},\mathbf{x}_{i})-\langle\nabla F_{j}(\mathbf{W}^{(t)}, \mathbf{x}_{i}),\mathbf{W}^{*}\rangle\Big{)}-\eta^{2}\|\nabla L_{S}(\mathbf{W} ^{(t)})\|_{F}^{2}\] \[\geq\frac{2\eta}{n}\sum_{i=1}^{n}\Big{(}\frac{\partial L_{i}( \mathbf{W}^{(t)})}{\partial F_{0}(\mathbf{W}^{(t)},\mathbf{x}_{i})}\big{(}F_{ 0}(\mathbf{W}^{(t)},\mathbf{x}_{i})-2\log(2M/\epsilon)\big{)}+\sum_{j=1}^{M} \frac{\partial L_{i}(\mathbf{W}^{(t)})}{\partial F_{j}(\mathbf{W}^{(t)}, \mathbf{x}_{i})}\big{(}F_{j}(\mathbf{W}^{(t)},\mathbf{x}_{i})-\log(2M/ \epsilon)\big{)}\Big{)}\] \[\geq\frac{2\eta}{n}\sum_{i=1}^{n}\big{(}L_{i}(\mathbf{W}^{(t)})+ \log(\frac{e^{2\log(2M/\epsilon)}}{e^{2\log(2M/\epsilon)}+Me^{\log(2M/\epsilon )}})\big{)}-\eta^{2}\|\nabla L_{S}(\mathbf{W}^{(t)})\|_{F}^{2}\] \[=\frac{2\eta}{n}\sum_{i=1}^{n}\big{(}L_{i}(\mathbf{W}^{(t)})- \log(1+\frac{\epsilon}{2})\big{)}-\eta^{2}\|\nabla L_{S}(\mathbf{W}^{(t)})\|_{ F}^{2}\] \[\geq\eta L_{S}(\mathbf{W}^{(t)})-\eta\epsilon\]

where the third equality is by (49). The first inequality is by Lemma D.13. The second inequality is due to the convexity of negative log-Softmax function. The last inequality is by Lemma D.11 (and the conditions on \(\eta\)) and \(\log(1+x)\leq x\) for \(x\geq 0\).

Similarly, we can show the same for the other modality as

\[\|\widetilde{\mathbf{W}}^{(t)}-\widetilde{\mathbf{W}}^{*}\|_{F}^{2}-\| \widetilde{\mathbf{W}}^{(t+1)}-\widetilde{\mathbf{W}}^{*}\|_{F}^{2}\geq\eta L_ {S}(\widetilde{\mathbf{W}}^{(t)})-\eta\epsilon\]

Combining the two results completes the proof. 

**Lemma D.15**.: _Under Assumption 4.1, let \(T=T_{1}+\lfloor\frac{\|\mathbf{W}^{(T_{1})}-\mathbf{W}^{*}\|_{F}^{2}+\| \widetilde{\mathbf{W}}^{(T_{1})}-\widetilde{\mathbf{W}}^{*}\|_{F}^{2}}{\eta \epsilon}\rfloor=T_{1}+\widetilde{O}(m\eta^{-1}\epsilon^{-1}\|\boldsymbol{\mu} \|_{2}^{-2})\). Then we have \(\max_{r,i}|\rho_{r,i}^{(t)}|\leq\sigma_{0}\sigma_{\xi}\sqrt{d}\) and \(\max_{r,i}|\widetilde{\sigma}_{r,i}^{(t)}|\leq\sigma_{0}\sigma_{\xi}\sqrt{d}\) for all \(t\in[T_{1},T]\). In addition, we have for all \(T_{1}\leq t\leq T\),_

\[\frac{1}{t-T_{1}+1}\sum_{s=T_{1}}^{t}\overline{L}(\mathbf{W}^{(t)},\widetilde{ \mathbf{W}}^{(t)})\leq\frac{\|\mathbf{W}^{(T_{1})}-\mathbf{W}^{*}\|_{F}^{2}+\| \widetilde{\mathbf{W}}^{(T_{1})}-\widetilde{\mathbf{W}}^{*}\|_{F}^{2}}{\eta(t- T_{1}+1)}+2\epsilon.\]

_Therefore, we can find an iterate \((\mathbf{W}^{(s)},\widetilde{\mathbf{W}}^{(s)})\) for \(s\in[T_{1},T]\) with training loss smaller than \(3\epsilon\)._

Proof of Lemma D.15.: By Lemma D.14, for \(t\in[T_{1},T]\), we have for any \(s\leq t\)

\[\|\mathbf{W}^{(s)}-\mathbf{W}^{*}\|_{F}^{2}+\|\widetilde{\mathbf{W}}^{(s)}- \widetilde{\mathbf{W}}^{*}\|_{F}^{2}-\|\mathbf{W}^{(s+1)}-\mathbf{W}^{*}\|_{F}^{2 }-\|\widetilde{\mathbf{W}}^{(s+1)}-\widetilde{\mathbf{W}}^{*}\|_{F}^{2}\geq\eta \overline{L}(\mathbf{W}^{(s)},\widetilde{\mathbf{W}}^{(s)})-2\eta\epsilon.\]

Summing the inequality yields

\[\sum_{s=T_{1}}^{t}\overline{L}(\mathbf{W}^{(s)},\widetilde{\mathbf{W}}^{(s)}) \leq\frac{\|\mathbf{W}^{(T_{1})}-\mathbf{W}^{*}\|_{F}^{2}+\|\widetilde{\mathbf{W}} ^{(T_{1})}-\widetilde{\mathbf{W}}^{*}\|_{F}^{2}+2\eta\epsilon(t-T_{1}+1)}{\eta}.\]Dividing both sides by \(t-T_{1}+1\) and setting \(t=T\) gives

\[\frac{1}{T-T_{1}+1}\sum_{s=T_{1}}^{t}\overline{L}(\mathbf{W}^{(s)},\widetilde{ \mathbf{W}}^{(s)})\leq\frac{\|\mathbf{W}^{(T_{1})}-\mathbf{W}^{*}\|_{F}^{2}+\| \widetilde{\mathbf{W}}^{(T_{1})}-\widetilde{\mathbf{W}}^{*}\|_{F}^{2}}{\eta(T- T_{1}+1)}+2\epsilon\leq 3\epsilon.\]

### Downstream Task Performance

Recall that after the pre-training stage on the training data at time \(T\), the signal learning and noise memorization satisfy

\[\max_{r}A_{r}^{(T)} =\widetilde{\Omega}(1),\] \[\max_{r}\Psi_{r,i}^{(T)} =\widetilde{O}(1/\sqrt{n})\;\mathrm{for}\;i\in[n].\]

Then, on the downstream task, the corresponding embedding can be calculated as follows:

\[h_{r}(\mathbf{x}_{\text{test}}^{(1)}) =\sigma(\langle\mathbf{w}_{r}^{(T)},\mathbf{x}_{\text{test}}^{( 1)}\rangle)=\widetilde{\Omega}(1/\sqrt{d}),\] \[h_{r}(\mathbf{x}_{\text{test}}^{(2)}) =\sigma(\langle\mathbf{w}_{r}^{(T)},\mathbf{x}_{\text{test}}^{( 2)}\rangle)=\widetilde{O}(1/\sqrt{dn}).\]

Then, it is straightforward to check that the embedding of a finite size of samples during the fine-tuning stage is linearly separable. Thus, the downstream task performance follows \(L_{\mathcal{D}_{\text{test}}}(T^{*})=o(1)\).

## Appendix E Additional Experimental Details

We implement our methods using PyTorch. For the software and hardware configurations, we ensure consistent environments for each dataset. We run all the experiments on Linux servers with NVIDIA V100 graphics cards and CUDA 11.2, completing them within one hour.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and introduction clearly outline the primary contributions of the paper, including the theoretical advancements in optimization and generalization analysis of multi-modal contrastive learning and single-modal contrative learning. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have discussed the limitation of this work in Section 7. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: We have provided all assumptions in Assumption 4.1 in the main paper. The proof sketch and complete proof are provided in Section 5 and Appendices C and D. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We have provided the complete configuration in Section 6. We have also uploaded the code in the supplementary material. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We have uploaded the code with instructions in the supplementary material. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We have provided all the training and test details in Section 6 and uploaded code. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We plot 1-sigma error bar for results shown in Figure 1. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We have provided sufficient information about computer resources in Appendix E. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research does not involve any human subjects, personal data, or interactions that would raise ethical concerns about consent, privacy, or respect for persons. In conclusion, the research aligns with the ethical principles outlined in the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We have discussed the broader impacts in Section A. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: This paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the asshts? Answer: [NA] Justification: This paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.