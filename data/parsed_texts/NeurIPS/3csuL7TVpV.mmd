# Decoding-Time Language Model Alignment

with Multiple Objectives

Ruizhe Shi\({}^{1}\)   Yifang Chen\({}^{2}\)   Yushi Hu\({}^{2,3}\)   Alisa Liu\({}^{2}\)

**Hannaneh Hajishirzi\({}^{2,3}\)   Noah A. Smith\({}^{2,3}\)   Simon S. Du\({}^{2}\)**

\({}^{1}\)IIS, Tsinghua University  \({}^{2}\)University of Washington  \({}^{3}\)Allen Institute for AI

This work was done while Ruizhe Shi was visiting the University of Washington.

###### Abstract

Aligning language models (LMs) to human preferences has emerged as a critical pursuit, enabling these models to better serve diverse user needs. Existing methods primarily focus on optimizing LMs for a single reward function, limiting their adaptability to varied objectives. Here, we propose **multi-objective decoding (MOD)**, a decoding-time algorithm that outputs the next token from a linear combination of predictions of all base models, for any given weighting over different objectives. We exploit a common form among a family of \(f\)-divergence regularized alignment approaches (such as PPO, DPO, and their variants) to identify a closed-form solution by Legendre transform, and derive an efficient decoding strategy. Theoretically, we show why existing approaches can be sub-optimal even in natural settings and obtain optimality guarantees for our method. Empirical results demonstrate the effectiveness of the algorithm. For example, compared to a parameter-merging baseline, MOD achieves 12.8% overall reward improvement when equally optimizing towards \(3\) objectives. Moreover, we experiment with MOD on combining three fully-finetuned LMs of different model sizes, each aimed at different objectives such as safety, coding, and general user preference. Unlike traditional methods that require careful curation of a mixture of datasets to achieve comprehensive improvement, we can quickly experiment with preference weightings using MOD to find the best combination of models. Our best combination reduces toxicity on Toxigen to nearly 0% and achieves 7.9-33.3% improvement across three other metrics (_i.e._, Codex@1, GSM-COT, BBH-COT).

## 1 Introduction

Learning from human feedback [36, 35] has gained significant attention due to its potential for using human-labeled datasets to align language models to human preferences [42, 52, 39, 8, 60]. Among them, alignment approaches such as RLHF (PPO) [9] and DPO [39] all model the optimization objective so as to maximize the expected reward from some implicit or explicit reward function, while incorporating KL-divergence from the reference policy as a divergence penalty [16]. However, these algorithms are restricted to only optimizing for a single reward function.

In reality, different use cases and users may prefer different weightings of various alignment objectives. For instance, dialogue agents need to trade off between helpfulness and harmlessness [3, 22], while question-answering systems can have attributes of relevance, verbosity, and completeness [52]. Therefore, there is a growing need for methods of adapting LMs on-the-fly toward different combinations of objectives [44, 21, 13]. Naive methods such as prompt adjustment for particular styles [6, 38] fail to provide precise control over the nuanced weighting of output characteristics [67]. Curating mixed datasets for the desired combination of objectives is challenging and resource-intensive. Some efforts (e.g., MORLHF [52, 3] MODPO [62]) match varying personal preferencesthrough linearly combining reward functions into a single one, but these approaches still necessitate retraining for all possible weightings.

In this work, we tackle the question: _Given a set of policies corresponding to different rewards and linear coefficients for the rewards, can we find a training-free policy corresponding to the interpolated reward?_ We introduce **multi-objective decoding** (**MOD**; see Figure 1), which combines the predictive distributions of individual models trained for single objectives. This approach is inspired by Legendre transform in convex optimization [34], which allows us to derive a closed-form solution from a family of \(f\)-divergence regularized optimization approaches [9; 39; 47] (e.g., PPO, DPO are optimizing for the reward function with KL-divergence penalty), and its efficient approximation. The resulting method extends prior work employing logit arithmetic for decoding-time alignment [28; 59; 19; 30], but we are the first to successfully achieve decoding towards multiple objectives simultaneously. We compare the design of our approach with existing multi-objective alignment approaches in Table 1.

Importantly, our approach allows users to achieve arbitrary weightings of objectives at inference time, avoiding the need for extensive retraining iterations. Additionally, our approach offers users more precise and interpretable control over the customization of AI outputs, thereby enhancing both personalization and performance. We conduct experiments across various tasks including **Reddit Summary**[42], **Helpful Assistant**[3], and **Safety Alignment**[22]. Notably, our method can combine models of different scales, and it is effective not only for PPO and DPO models but also can be extended to supervised finetuned (SFT) models. This insight is supported by experiments on combining **13B** DPO models and a **7B** SFT model for **Open Instruction-Following**[49; 20].

**Contributions.** We summarize our contributions as follows.

* We introduce a training-free, simple, yet effective algorithm, MOD, for multi-objective alignment of language models. Given strong-barrier function regularized base policies trained for a single objective, we are able to derive and efficiently decode a closed-form solution for an

\begin{table}
\begin{tabular}{l l l l l} \hline \hline \multirow{2}{*}{Algorithms} & Number of & Free from & Free from & \multirow{2}{*}{Requirement} \\  & trained LLMs & RM & prompting & \\ \hline MORLHF [52; 3] & \# preferences & ✗ & ✗ & \\ MODPO [62] & \# preferences & ✗ & ✗ & \\ DPA [48], CPO [18], RiC [56] & 1 & ✗ & ✗ & \\ RS [40; 21] & \# objectives & ✗ & ✗ & same arch. \& init. \\ MOD (ours) & \# objectives & ✗ & ✗ & same tokenizer \\ \hline \hline \end{tabular}
\end{table}
Table 1: Overall comparison with other approaches. “Free from RM” refers to not requiring reward models. “Free from prompting” refers to not requiring preference-driven prompts during inference. Generally, the number of preferences is much larger than the number of objectives here. Among them, our approach is the most versatile solution.

Figure 1: Multi-objective decoding. We prepare LMs tuned for each objective in advance. Then, given preference weightings \(w\), input prompt \(x\) and context \(y_{<t}\), \(y_{t}\) is greedily decoded from an algebraic combination of predicted probabilities from each LM, achieving precise control.

interpolated objective with optimality guarantees, based on Legendre transformation. Notably, our comprehensive framework generalizes and explains many existing tuning approaches and decoding strategies [28; 59; 19; 30; 62]. See Section 3.
* In extensive experiments, we demonstrate the strong performance of MOD. For instance, compared to parameter merging, MOD achieves a \(12.8\)% overall relative reward improvement when equally optimizing towards three objectives on **Helpful assistant** task. When combining \(3\)**TULU** models, our best configuration significantly reduces Toxigen to nearly zero and results in a \(7.9\)% to \(33.3\)% relative improvement across the other three metrics (Codex@1, GSM-COT, BBH-COT). Additionally, experiments validate that our framework is applicable to SFT models and is still effective for given a mix of positive and negative weights (a case where the traditional training-free baseline does not work), showing its steerability. See Section 4.
* We conduct a thorough theoretical analysis of a broad framework of multi-objective alignment concerning \(f\)-divergence regularization, investigating the necessity of barrier function, optimality guarantees, and error propagation from sub-optimal base policies. We reveal the sub-optimality of the parameter-merging paradigm [40; 21] under a common setting, showing that for most \(f\)-divergence regularization, including the commonly-used KL-divergence, the optimal policy is not guaranteed to lie in the interpolation region of the weights of base policies. See Section 5.

## 2 Preliminaries

There are various ways of defining "multi-objective." In this paper, we take a multi-objective reward function perspective. In this section, we will first give a formal definition of multi-objective reward functions. After that, because we focus exclusively on decoding by combining the predictions of a set of existing single-objective aligned LMs, we will give a formal assumption on each base LM considered in this paper. Finally, we will show the mathematical advantage of those base LMs under such assumptions. Notation is given in Appendix B.

**Multi-objective reward functions.** Existing single-objective alignment methods, including PPO, DPO, and their variants, all explicitly or implicitly assume the existence of a reward function \(\mathcal{R}:\mathcal{X}\times\mathcal{Y}\to\mathbb{R}\), such that for each input prompt \(x\in\mathcal{X}\) and output response \(y\in\mathcal{Y}\), there exists a reward signal \(\mathcal{R}(y|x)\). Under the multi-objective setting, we assume there exists a set of reward functions \(\{\mathcal{R}_{i}\}_{i=1}^{M}\) corresponding to \(M\) objectives. In reality, different people have different preferences for each objective; therefore, we represent such preferences as a normalized vector \(w\in\Delta^{M-1}\). For people with preference \(w\), we care about the weighted reward function \(\sum_{i=1}^{M}w_{i}\cdot\mathcal{R}_{i}(y|x)\) for each sample pair \((x,y)\). This paper focuses on how to maximize such rewards exclusively through decoding by combining the outputs of a set of existing single-objective aligned LMs, denoted as \(\{\pi_{i}\}_{i=1}^{M}\), which are formally defined below.

**Single objective alignment with \(f\)-divergence regularization.** Each policy \(\pi_{i}\) has been optimized for the corresponding reward function \(\mathcal{R}_{i}\). However, it is well known that greedily optimizing towards maximum rewards can lead to over-optimization and worsen model performance [16]. Therefore, regularization has been incorporated to avoid large deviations from the reference policy. Alignment with KL-divergence regularization has been established as a standard formulation [36; 42; 52; 39; 53; 57]. Recently, a sequential line of work [47; 43] has proposed replacing Reverse KL-divergence with a set of \(f\)-divergences such as Forward KL-divergence, JSD, and \(\alpha\)-divergence, which they claim can enhance generation diversity and decrease the expected calibration error [17] empirically. We observe that all these methods can be analyzed under the framework of \(f\)-divergences, where \(f\) is a _barrier function_ (see Definition 1 and Definition 2 in Appendix D.1 for formal definitions). The closed form of each single-objective aligned LM \(\pi_{i}\) can be written as:

\[\pi_{i}=\operatorname*{argmax}_{\pi\in\mathcal{S}}\mathop{\mathbb{E}}_{ \begin{subarray}{c}x\sim\mathcal{X}\\ y\sim\pi(\cdot|x)\end{subarray}}\left[\mathcal{R}_{i}(y|x)\right]-\beta \mathop{\mathbb{E}}_{\begin{subarray}{c}x\sim\mathcal{X}\\ y\sim\pi_{\mathrm{ref}}(\cdot|x)\end{subarray}}f\left(\frac{\pi(y|x)}{\pi_{ \mathrm{ref}}(y|x)}\right)\enspace,\] (1)

where \(\beta\) is a regularization parameter and \(\pi_{\mathrm{ref}}\) is the initial SFT model, _i.e._, the reference policy. For example, if we take \(f(x)=x\log x\), then the objective can be written as:

\[\max_{\pi\in\mathcal{S}}\mathop{\mathbb{E}}_{\begin{subarray}{c}x\sim \mathcal{X}\\ y\sim\pi(\cdot|x)\end{subarray}}\left[\mathcal{R}_{i}(y|x)\right]-\beta \operatorname{KL}\left(\pi\|\pi_{\mathrm{ref}}\right)\enspace,\] (2)

which is the standard optimization problem in [9; 39].

**Strong-barrier function benefits multi-objective decoding.** As discussed above, existing works choose different \(f\) primarily to achieve different regularization behaviors. However, there is an extra property: if the barrier function \(f\) is continuously differentiable and strongly convex on \(\mathbb{R}_{+}\), we can obtain a closed-form bijection between any single-objective aligned LM \(\pi_{i}\) and the corresponding \(\mathcal{R}_{i}\) as shown below (initially proposed in [47], see detailed proof in Lemma1):

\[\pi_{i}(y|x)=\pi_{\mathrm{ref}}(y|x)(\nabla f)^{(-1)}\left(\frac{1}{\beta} \mathcal{R}_{i}(y|x)-Z_{i}(x)\right),\ \mathcal{R}_{i}(y|x)=\beta\nabla f\left(\frac{\pi_{i}(y|x)}{\pi_{\mathrm{ref} }(y|x)}\right)+\beta Z_{i}(x)\,\] (3)

where \(Z_{i}(x)\) is the normalization factor with respect to \(x\). In other words, given the rewards and a prompt \(x\), there is a closed form for the optimal policy, and given the optimal policies and \(x\), there is a closed form for the rewards for every \(y\). Crucially, such closed forms directly result in a possible linear combination of different outputs of \(\{\pi_{i}\}_{i=1}^{M}\), as we will show in our main algorithm. In the rest of the paper, we call an \(f\) with such properties a _strong-barrier function_.

**Formal problem formulation.** Given all those preliminaries, now we are ready to state our formal problem formulation: We are given a reference policy \(\pi_{\mathrm{ref}}\) and a set of base policies \(\{\pi_{i}\}_{i=1}^{M}\) trained for reward functions \(\{\mathcal{R}_{i}\}_{i=1}^{M}\) under \(f\)-divergence regularization. And we assume that we are unable to access \(\mathcal{R}_{i}\) directly. Can we find a retraining-free decoding algorithm such that, for any given preference weightings \(w\in\Delta^{M-1}\) and input \(x\), we can obtain an optimal response \(y\) for the weighted multi-objective reward function \(r(y|x)=\sum_{i=1}^{M}w_{i}\cdot\mathcal{R}_{i}(y|x)\), that is regularized by \(\pi_{\mathrm{ref}}\)?

## 3 Proposed Method: Multi-Objective Decoding

### Warm-up: an inefficient decoding version

To decode \(y\), the most direct way is to find a policy \(\pi^{\star}\) where \(y\) can be sampled from, by solving

\[\max_{\pi\in\mathcal{S}}\underset{y\sim\pi(\cdot|x)}{\mathbb{E}}r(y|x)\quad \text{w.r.t.}\underset{\begin{subarray}{c}x\sim\mathcal{X}\\ y\sim\pi_{\mathrm{ref}}(\cdot|x)\end{subarray}}{\mathbb{E}}f\left(\frac{\pi(y |x)}{\pi_{\mathrm{ref}}(y|x)}\right)\leq C_{1}\,\]

where \(C_{1}\in\mathbb{R}_{+}\) is some threshold constant. Now by leveraging the bijection property of a strong-barrier function, as shown in Eq.3, there exists a naive decoding format \(\pi^{\star}\) for the dual problem (see detailed proof in Proposition1):

\[\pi^{\star}(y|x) =\pi_{\mathrm{ref}}(y|x)\cdot(\nabla f)^{(-1)}\left(-Z^{\star}(x )+\frac{1}{\beta}\sum_{i=1}^{M}w_{i}\cdot\mathcal{R}_{i}(y|x)\right)\] \[=\pi_{\mathrm{ref}}(y|x)\cdot(\nabla f)^{(-1)}\left(-Z(x)+\sum_{ i=1}^{M}w_{i}\cdot\nabla f\left(\frac{\pi_{i}(y|x)}{\pi_{\mathrm{ref}}(y|x)} \right)\right)\,\]

where \(Z(x)\) and \(Z^{\star}(x)\) are normalization factors. With this form, we can directly combine the outputs from \(\{\pi_{i}\}_{i=1}^{M}\) during decoding. Unfortunately, computing the exact value of the normalization factor is nearly impossible as it requires looping over all possible \(y\) in the output space.

### Towards an efficient algorithm: reformulation and approximation

**Reformulation via Legendre transform.** We make a significant observation: our main motivation is to maximize the sum of weighted multi-objective rewards while avoiding over-optimization (_i.e._, too much deviation from the reference policy). This motivation can be reformulated as keeping the target policy similar to the reference policy in the input region where the reference model already performs well, while optimizing towards larger rewards in regions where the reference policy is highly unaligned with the target rewards. Consequently, we can rewrite the optimization problem as:

\[\max_{y\in\mathcal{Y}}\pi_{\mathrm{ref}}(y|x),\quad\text{w.r.t. }r(y|x)\geq C_{2}\,\] (4)

where \(C_{2}\in\mathbb{R}_{+}\) is some threshold constant. Based on this observation and Legendre transform in convex optimization [34], we prove our key theorem which gets rid of the normalization factor and leads to the MOD algorithm, as follows (see detailed proof in AppendixD.3).

**Theorem 1** (Informal key theorem).: _There exists a certain \(C_{2}\) such that:_

\[\operatorname*{argmax}_{y\in\mathcal{Y}}\pi_{\mathrm{ref}}(y|x)\cdot(\nabla f)^ {(-1)}\left(\sum_{i=1}^{M}w_{i}\cdot\nabla f\left(\frac{\pi_{i}(y|x)}{\pi_{ \mathrm{ref}}(y|x)}\right)\right)\,\] (5)

_is the optimal solution for this revised optimization problem (4)._Notice that, without much performance loss, we can further improve efficiency using _greedy search_, thus transforming response-level decoding into efficient token-level decoding.

### Main algorithm: efficient decoding with optimality for strong-barrier function

Based on this new closed form Eq. (5), we are ready to show the main algorithm.

At each timestep \(t\), we condition the reference policy \(\pi_{\mathrm{ref}}\) and policies \(\{\pi_{i}\}_{i=1}^{M}\) on the prompt \(x\) and context \(y_{<t}\) to obtain the next token \(y_{t}\) from the predicted probabilities of each policy:

\[\operatorname*{argmax}_{s\in\Sigma}\;\pi_{\mathrm{ref}}(y_{<t},s|x)\cdot( \nabla f)^{(-1)}\left(\sum_{i=1}^{M}w_{i}\cdot\nabla f\left(\frac{\pi_{i}(y_{<t },s|x)}{\pi_{\mathrm{ref}}(y_{<t},s|x)}\right)\right)\.\] (6)

The full pipeline is shown in Appendix C.1. Specifically, in main experiments, we implement our algorithm by choosing \(f(x)=x\log x\), _i.e._, the regularization term is Reverse KL-divergence as used in PPO and DPO, and Eq. (6) reduces to a simple token-wise decoding rule:

\[\operatorname*{argmax}_{s\in\Sigma}\;\prod_{i=1}^{M}\pi_{i}^{w_{i}}(y_{<t},s| x)\,\] (7)

equivalent to linearly combining logits [32; 30] of each model with preference weightings.

**Comparisons with other approaches.** Our algorithm is significantly more efficient than retraining-based algorithms. In practice, the number of objectives is easily enumerable (e.g., \(<5\) in [50; 12]), making it feasible to finetune an LM for each objective. In contrast, the number of preferences cannot be bounded due to the variability among users [7], which suggests that retraining-based algorithms like MORLHF and MODPO need to compute an impractical amount of times in order to match the preference of every user. Regarding memory efficiency, MOD requires loading multiple models simultaneously, which consume relatively higher memory cost. However, we mitigate this cost by ensembling a set of low-rank adapters or using distributed deployment in implementation. A comprehensive comparison with these baselines is shown in Table 1.

## 4 Experiments

Here, we demonstrate the effectiveness of MOD through four sets of experiments: 1) PPO models for the **Reddit Summary**[42] task. 2) PPO models for the **Helpful Assistants**[3] task. 3) \(f\)-DPO models for the **Safety Alignment**[22] task. 4) SFT and DPO models for the **Open Instruction-Following**[49; 20] task. Additional experiments on the **HelpSteer**[50] task are provided in Appendix F.4.

### Experiment setup

**Baselines.** We adopt the representative parameter-merging method and retraining approaches as our baselines. Rewarded soups (RS) [40] linearly merge each model's parameters according to preference weightings, as \(\theta=\sum_{i=1}^{M}w_{i}\cdot\theta_{i}\), where \(\theta_{i}\) denotes the parameters of \(\pi_{i}\). MORLHF [52] optimizes for the weighted multi-objective reward function \(\sum_{i=1}^{M}w_{i}\cdot\mathcal{R}_{i}\) using PPO, with the same configurations as training for single objective. MODPO [62] uses \(\pi_{1}\)'s output as an implicit reward signal of \(\mathcal{R}_{1}\) and inserts it into the DPO objective for \(\mathcal{R}_{2}\) to optimize for \(w_{1}\mathcal{R}_{1}+w_{2}\mathcal{R}_{2}\), with the same configurations as training for single objective.

**Visualization.** We plot the Pareto frontier to visualize the obtained reward of each attribute for a set of preference weightings. The performance can be measured through the area of the Pareto frontier, which reflects the optimality and uniformity of the solution distribution [66]. The reward is evaluated by off-shelf reward models. It is worth noting that MOD is free from reward models, and the use is merely for evaluation.

**Example generations.** It is important to note that, due to issues like over-optimization [16], solely showing higher rewards is not a complete argument in favor of a new RLHF method. Since MOD does not yield a sampling policy, which make it impossible to directly measure \(\operatorname{KL}\left(\cdot\|\pi_{\mathrm{ref}}\right)\) as prior work [52], we demonstrate example generations in Appendix F.6 to indicate that they do not deviate much from \(\pi_{\mathrm{ref}}\).

More implementation details regarding to tasks, datasets, SFT, reward models, training, and evaluation can be found in Appendix E.

### Results

**Reddit Summary.** By supervised finetuning a **Llama2-7B** model on Summarize-from-Feedback dataset [42], we obtain the reference policy \(\pi_{\mathrm{ref}}\). And then we obtain \(\pi_{1},\pi_{2}\) by tuning \(\pi_{\mathrm{ref}}\) using PPO for two off-shelf reward models (see details in Appendix E) which measures summary quality and faithfulness, respectively. Then we show Pareto frontiers of MOD, RS, and MORLHF in Figure 2, with preference weightings \(w\in\{(i/10,1-i/10):i\in\{0,1,\ldots,10\}\}\), demonstrating the superiority of MOD over baselines.

**Helpful Assistant.** By supervised finetuning a **Llama2-7B** model on Anthropic-HH dataset [3], we obtain the reference policy \(\pi_{\mathrm{ref}}\). And then we obtain \(\pi_{1},\pi_{2},\pi_{3}\) by tuning \(\pi_{\mathrm{ref}}\) using PPO for three off-shelf reward models (see details in Appendix E) which evaluate helpfulness, harmlessness and humor, respectively. The Pareto frontiers of MOD, RS and MORLHF for each two-objective pairs are shown in Figure 3. MOD prominently beats RS for each reward pair, and lags behind MORLHF in balancing harmlessness and humor, while MORLHF is more costly. We explore the \(3\)-objective setting on the **Helpful Assistant** task, demonstrating that MOD can effectively balance advantages of each model and outperforms RS. More results are provided in Appendix F.2.

**Safety Alignment.** Based on results reported in [47], we mainly focus on \(f\)-DPO with Reverse KL-divergence, JSD, \(0.3\)-divergence and \(0.5\)-divergence in experiments. We deploy an off-shelf **Alpaca-7B** model as \(\pi_{\mathrm{ref}}\) and train \(\pi_{1f},\pi_{2f}\) using \(f\)-DPO on two pair-comparison BeaverTails-\(10\)K [22] datasets: one is _Better_ and the other is _Safer_. We show Pareto frontiers of MOD, RS, and MODPO for each \(f\)-divergence in Figure 4. Experimental results demonstrate that MOD generally outperforms RS across multiple \(f\)-divergences. The retraining baseline MODPO is only applicable to Reverse KL-divergence, and MOD is much more convenient despite a slight performance gap.

Moreover, we can apply not-all-positive preference weightings \(w\in\mathbb{R}^{M}\) as long as \(\sum_{i=1}^{M}w_{i}=1\), thus allowing us to optimize for a reward function \(-\mathcal{R}\). In Table 2, we present the scores of MOD, with preference weightings set as \(w\in\{(i,1-i):i\in[5]\}\). Example generations in Table 3 (more in Appendix F.3) validate that MOD successfully handles this, while RS fails to generate meaningful responses. This phenomenon indicates that we do not even need to specifically tune an unsafe model as in [59], since the knowledge of \(-\mathcal{R}\) is indeed learned when being tuned for \(\mathcal{R}\).

**Open Instruction-Following.** Finally, we conduct experiments on larger-scale models for general objectives, including two DPO models, **Tulu-2-HH-13B**[20] tuned on Anthropic-HH [3] for

[MISSING_PAGE_FAIL:7]

_and the objectives \(J_{1},J_{2},\ldots,J_{M}\) representing reward functions \(\mathcal{R}_{1},\mathcal{R}_{2},\ldots,\mathcal{R}_{M}\) w.r.t. \(\beta\cdot I_{f}(\cdot\|\pi_{\mathrm{ref}})\), s.t. Hypothesis 1 does not hold._

**Remark 1** (Clarification).: _It is commonly adopted in previous studies [65; 42] that the network receives the same inputs \(z_{0}\). Despite the competitive results exhibited in prior works [51; 40; 21], this theorem reveals that parameter-merging lacks a theoretical guarantee in practical scenarios. Besides, although Hypothesis 1 may hold, the mapping from preference weightings \(w\) to the optimal merging weightings \(\lambda\) are intricate, and thus simply picking \(\lambda\) as \(w\)[40], can yield sub-optimal results._

**Another perspective of the same initialization.** We can also look into scenarios where only the parameters of the last several layers of \(\pi_{1},\pi_{2},\ldots,\pi_{M}\) can be different from \(\pi_{\mathrm{ref}}\). 1) If the last layer is a _linear projection_, then it is equivalent to MOD w.r.t. \(\mathrm{KL}\left(\cdot\|\pi_{\mathrm{ref}}\right)\), namely linearly combining the logits. 2) If the last layer is _self-attention_[45], then it can be easily hacked by reversing the sign of \(Q,K\) matrices in this layer, which does not influence the value of \(Q^{\top}K\), but significantly harms the effect of parameter-merging. A motivating example is shown in Appendix F.1.

### Necessity of barrier function

Extending the results of [47] to the multi-objective setting, we prove the necessity of \(f\) being barrier functions to find an optimal policy \(\pi^{\star}\) for multi-objective alignment. See detailed proof in Appendix D.2.

**Theorem 3**.: _If \(f\) is not a barrier function, then for \(\forall C\in\mathbb{R}_{+}\), \(N\in\mathbb{Z}_{\geq 4}\), \(M\in\mathbb{Z}_{\geq 2}\), \(\mathcal{Y}=\{y_{i}\}_{i=1}^{N}\), any multi-objective decoding or merging algorithm \(\mathcal{A}:\mathcal{S}^{M+1}\times\Delta^{M-1}\to\mathcal{S}\), there exists a reference policy \(\pi_{\mathrm{ref}}\), policies \(\{\pi_{i}\}_{i=1}^{M}\) and \(\pi^{\sharp}\), reward functions \(\{\mathcal{R}_{i}\}_{i=1}^{M}\), preference weightings \(w\in\Delta^{M-1}\) and \(\beta\in\mathbb{R}_{+}\), s.t. \(\pi_{i}\) is the optimal policy for \(\mathcal{R}_{i}\) w.r.t. \(\beta\cdot I_{f}(\cdot\|\pi_{\mathrm{ref}})\) (see Definition 1 in Appendix D.1), \(\forall i\in[M]\), but_

\[\mathop{\mathbb{E}}_{y\sim\pi_{\mathcal{A},w}}\left[\sum_{i=1}^{M}w_{i} \mathcal{R}_{i}(y)\right]\leq\mathop{\mathbb{E}}_{y\sim\pi^{\prime}}\left[ \sum_{i=1}^{M}w_{i}\mathcal{R}_{i}(y)\right]-C\;,\text{and}\]

\[\mathop{\mathbb{E}}_{y\sim\pi_{\mathcal{A},w}}\left[\sum_{i=1}^{M}w_{i} \mathcal{R}_{i}(y)\right]-\beta I_{f}(\pi_{\mathcal{A},w}\|\pi_{\mathrm{ref}} )\leq\mathop{\mathbb{E}}_{y\sim\pi^{\prime}}\left[\sum_{i=1}^{M}w_{i} \mathcal{R}_{i}(y)\right]-\beta I_{f}(\pi^{\prime}\|\pi_{\mathrm{ref}})-C\;,\]

_where \(\pi_{\mathcal{A},w}(y):=\mathcal{A}\big{(}\pi_{\mathrm{ref}},\pi_{1},\pi_{2},\ldots,\pi_{M},w\big{)}(y)\;.\)_

**Remark 2** (Motivating example).: _Here we provide a motivating example where \(f\equiv 0\): let \(M=4\), \(\mathcal{R}_{1}(y_{1})=\mathcal{R}_{2}(y_{2})=1\), \(\mathcal{R}_{1}(y_{2})=\mathcal{R}_{2}(y_{1})=-1\), \(\mathcal{R}_{1}(y_{3+k})=\mathcal{R}_{2}(y_{3+k})=0\), \(\mathcal{R}_{1}(y_{4-k})=\mathcal{R}_{2}(y_{4-k})=1/2\), where \(k\in\{0,1\}\). Then the optimal policy for \(\mathcal{R}_{1}\) is \(\pi_{1}(y_{i}):=\delta_{1i}\), for \(\mathcal{R}_{2}\) is \(\pi_{2}(y_{i}):=\delta_{2i}\), and for \(\mathcal{R}_{1}/2+\mathcal{R}_{2}/2\) is \(\pi^{\star}(y_{i}):=\delta_{4-k,i}\). Thus \(\pi_{\mathcal{A},w}\) cannot fit \(\pi^{\star}\) for \(k=0,1\)._

**Crucial role of the barrier function.** We can apply this theorem to any algorithm which solely utilizes base policies, including RS and MOD. And thus, a barrier function regularization is crucial in multi-objective alignment to bridge different policies, though it was originally intended to prevent degeneration (see Table 3 in [39]) in single-objective alignment. Additionally, the same as a general barrier in _interior point methods_[34], it obviates the need for introducing slack variables as in [47]. This explains why we should not use non-barrier \(f\)-divergences such as total variation and chi-squared.

### Sub-optimality error propagation

While we previously assumed that each base policy is the optimal solution of Eq. (1), here we provide a guarantee for performance when the base policies are sub-optimal. See proof in Appendix D.4.

**Theorem 4** (KL-divergence perspective).: _Given a reference policy \(\pi_{\mathrm{ref}}\), policies \(\{\pi_{i}\}_{i=1}^{M}\), reward functions \(\{\mathcal{R}_{i}\}_{i=1}^{M}\), and \(\beta\in\mathbb{R}_{+}\). Denote the optimal policy for \(\mathcal{R}_{i}\) w.r.t. \(\beta\,\mathrm{KL}\left(\cdot\|\pi_{\mathrm{ref}}\right)\) as \(p_{i}\), \(\forall i\in[M]\). For the reward function \(\sum_{i=1}^{M}w_{i}\cdot\mathcal{R}_{i}\) w.r.t. \(\beta\,\mathrm{KL}\left(\cdot\|\pi_{\mathrm{ref}}\right)\), the performance difference of policy \(\pi_{w}(\cdot|x)\propto\prod_{i=1}^{M}\pi_{i}^{w_{i}}(\cdot|x)\) from optimal is \(V^{\star}-V\). If for \(\forall i\in\{1,\ldots,M\},\ x\in\mathcal{X}\), we have: (i) \(\max\limits_{y\in\mathcal{Y}}\left|\log p_{i}(y|x)-\log\pi_{i}(y|x)\right| \leq\mathcal{L}\;,\) (ii) \(\mathrm{KL}\left(\pi_{\mathrm{ref}}(\cdot|x)\|\pi_{i}(\cdot|x)\right)\leq C\), \(\mathrm{KL}\left(\pi_{\mathrm{ref}}(\cdot|x)\|p_{i}(\cdot|x)\right)\leq C\;,\) where \(\mathcal{L},C\in\mathbb{R}_{+}\), then_

\[V^{\star}-V\leq 2\exp(C)\cdot\mathcal{L}\;.\]

**Remark 3** (Interpretation of conditions).: _Since the primal problem of Eq. (2) restricts the divergence penalty under a certain threshold, and people usually adopt an early-stopping technique in practice, \(p_{i}\) and \(\pi_{i}\) will not deviate from \(\pi_{\mathrm{ref}}\) too much, thus \(C\) can be viewed as a small constant. When each \(\pi_{i}\) is close to optimal, the relative distance reflected by \(\mathcal{L}\) is small as well. The expected calibration error can also be bounded, shown in Proposition 4._

### Beyond \(f\)-divergence regularized alignment and multi-objective decoding

While our main results are based on \(f\)-divergence regularized aligned LMs and aimed at multi-objective decoding, our framework is also applicable to using SFT models and explaining the effectiveness of other existing decoding algorithms. For example, proxy-tuning [28] tunes only a smaller LM, then applies the difference between the logits of the small tuned and untuned LMs to shift the predictions of a larger untuned model. Its theoretical justification is provided by our framework, under certain assumptions. We provide insights on this line of work [28; 59; 19] and derivations of some other related works [30; 62] in Appendix C.3, further demonstrating the potential for universally applying our approach.

## 6 Related Work

**Algorithms for aligning LMs to human preferences.** The widely used RLHF (PPO) approach [36; 42; 52] optimizes over rewards with Reverse KL-divergence as a penalty, where the reward models are learned from human preference datasets. DPO [39] leverages the Bradley-Terry assumption [5] to directly optimize the same objective on preferences, in a supervised manner. \(\Psi\)-PO [2] further modifies the reward term to be optimized as other mappings from preference pairs; f-DPO [47] replaces Reverse KL-divergence with other divergence measures. In addition, there are other efforts exploring alternative objectives and frameworks: SLiC-HF [61; 60] refer to the alignment process as sequence likelihood calibration; SPIN [8] iteratively improves the model by leveraging synthetically generated data, thereby circumventing the need for human feedback; OPO [54] employs established norms as constraints, achieving training-free alignment; and Lyu _et al._[31] highlight the crucial role of prompt templates. In this work, we mainly focus on RLHF (PPO), DPO, and their extensions.

**Decoding-time algorithms for controllable generation.**_Response-level_ decoding algorithms sample a whole output \(y\) from an anticipated probability distribution \(p\). To achieve this goal, energy-based methods are adopted in many works [37; 25], which involves continuous optimization for LMs to obtain gradient information. Kumar _et al._[24] view this task as maximizing \(\log p(y)\) while satisfying some constraints, and use simultaneous gradient descent to solve it. _Token-level_ decoding algorithms decode token \(y_{t}\) at timestep \(t\), and are usually more efficient. Among them, Mudgal _et al._[33], Liu _et al._[29] deploy value models to guide the decoding process; DeRa [30] works on hyperparameter re-alignment and proposes the potential of a special case of MOD, while introducing a per-token distribution approximation; proxy-tuning [28; 59; 19] tunes a small model and applies it to steer a larger base model by operating on logits.

**Multi-objective LMs alignment.** Multi-objective alignment is the task of aligning language models to multiple objectives simultaneously. This is important for managing tradeoffs among different dimensions [44; 3] and catering to the diverse needs of users [21; 13]. Approaches for multi-objective alignment fall into the following categories: 1) _Retraining_. The most natural approach to solve multi-objective alignment is to retrain for a linearly combined multiple reward functions (MORLHF [52; 3]). And MODPO [62] retrains the model in a reward-model-free way, by learning a flexible reward representation and directly training on a fixed preference dataset. 2) _Parameter-merging_. This line of work [40; 21; 27], represented by rewarded soups (RS), aims at providing a training-free solution which obtains weights of the policy as a linear combination of weights of trained policies for each single objective, inspired by [51] and its other applications [41; 26]. Jiang _et al._[23] achieve another kind of model-merging through reranking and fusion on outputs. 3) _Preference-conditioned prompting_. The preference-conditioned learning approaches [64; 4] train a policy conditioned on preference weightings to maximize the expected rewards. This concept is reflected in LMs alignment as preference-conditioned prompting: this line of work [56; 48; 18] directly presents the preference weightings in prompts after a finetuning process. The latter two paradigms are more efficient, while relying heavily on either the reduced mis-specification hypothesis [40] or unguaranteed OOD generalization ability [63], posing challenges in terms of interpretability and robustness.

Conclusion

We propose MOD, a simple, training-free yet effective algorithm for multi-objective LMs alignment. By addressing the challenges of retraining and resource-intensive processes, our method provides a decoding-time solution while offering insights into the broader applicability of combining differently tuned models. Through extensive analysis and empirical evidence, we demonstrate the effectiveness and practicality of our method under the \(f\)-divergence framework, paving the way for improving LM performance across diverse tasks and use cases.

It is also important to acknowledge the limitations of our work. 1) The analysis is primarily based on tabular setting [55], not taking function approximation error into consideration. 2) Decoding from a response-level probability distribution at the token level may lead to degraded performance, which is likely to be alleviated by energy-based approaches [37, 24, 58].

## Acknowledgement

SSD acknowledges the support of NSF IIS 2110170, NSF DMS 2134106, NSF CCF 2212261, NSF IIS 2143493, and NSF IIS 2229881. NAS acknowledges the support of NSF IIS 2113530. The authors also thank Yizhong Wang for useful discussions.

## References

* [1] S. M. Ali and S. D. Silvey. A general class of coefficients of divergence of one distribution from another. _Journal of the Royal Statistical Society. Series B (Methodological)_, 28(1):131-142, 1966.
* [2] Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and Remi Munos. A general theoretical paradigm to understand learning from human preferences. _ArXiv_, abs/2310.12036, 2023.
* [3] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, John Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Christopher Olah, Benjamin Mann, and Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback. _ArXiv_, abs/2204.05862, 2022.
* [4] Toygun Basaklar, Suat Gumussoy, and Umit Y. Ogras. Pd-morl: Preference-driven multi-objective reinforcement learning algorithm. _ArXiv_, abs/2208.07914, 2022.
* [5] Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. _Biometrika_, 39(3/4):324-345, 1952.
* [6] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. _ArXiv_, abs/2005.14165, 2020.
* [7] Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, J'er'emy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Raphael Segerie, Micah Carroll, Andi Peng, Phillip J. K. Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco di Langosco, Peter Hase, Erdem Biyik, Anca D. Dragan, David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell. Open problems and fundamental limitations of reinforcement learning from human feedback. _ArXiv_, abs/2307.15217, 2023.

* Chen et al. [2024] Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. _CoRR_, abs/2401.01335, 2024.
* Christiano et al. [2017] Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, pages 4299-4307, 2017.
* Csiszar [1964] Imre Csiszar. Eine informationstheoretische ungleichung und ihre anwendung auf beweis der ergodizitaet von markoffschen ketten. _Magyver Tud. Akad. Mat. Kutato Int. Koezl._, 8:85-108, 1964.
* Csiszar [1967] Imre Csiszar. On information-type measure of difference of probability distributions and indirect observations. _Studia Sci. Math. Hungar._, 2:299-318, 1967.
* Cui et al. [2023] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback, 2023.
* Dong et al. [2023] Yi Dong, Zhilin Wang, Makesh Sreedhar, Xianchao Wu, and Oleksii Kuchaiev. SteerLM: Attribute conditioned SFT as an (user-steerable) alternative to RLHF. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 11275-11288, Singapore, December 2023. Association for Computational Linguistics.
* Durrett [2010] Rick Durrett. _Probability: Theory and Examples, 4th Edition_. Cambridge University Press, 2010.
* Friedberg et al. [2014] S.H. Friedberg, A.J. Insel, and L.E. Spence. _Linear Algebra_. Pearson Education, 2014.
* Gao et al. [2022] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA_, volume 202 of _Proceedings of Machine Learning Research_, pages 10835-10866. PMLR, 2023.
* Guo et al. [2017] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In _International conference on machine learning_, pages 1321-1330. PMLR, 2017.
* Guo et al. [2024] Yiju Guo, Ganqu Cui, Lifan Yuan, Ning Ding, Jiexin Wang, Huimin Chen, Bowen Sun, Ruobing Xie, Jie Zhou, Yankai Lin, et al. Controllable preference optimization: Toward controllable multi-objective alignment. _arXiv preprint arXiv:2402.19085_, 2024.
* Huang et al. [2024] James Y Huang, Wenxuan Zhou, Fei Wang, Fred Morstatter, Sheng Zhang, Hoifung Poon, and Muhao Chen. Offset unlearning for large language models. _arXiv preprint arXiv:2404.11045_, 2024.
* Ivison et al. [2023] Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. Camels in a changing climate: Enhancing lm adaptation with tulu 2, 2023.
* Jang et al. [2023] Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack Hessel, Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin Choi, and Prithviraj Ammanabrolu. Personalized soups: Personalized large language model alignment via post-hoc parameter merging. _CoRR_, abs/2310.11564, 2023.
* Ji et al. [2023] Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Chi Zhang, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. Beavertails: Towards improved safety alignment of llm via a human-preference dataset. _arXiv preprint arXiv:2307.04657_, 2023.

* [23] Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. LIm-blender: Ensembling large language models with pairwise ranking and generative fusion. In _Annual Meeting of the Association for Computational Linguistics_, 2023.
* [24] Sachin Kumar, Eric Malmi, Aliaksei Severyn, and Yulia Tsvetkov. Controlled text generation as continuous optimization with multiple constraints. In _Neural Information Processing Systems_, 2021.
* [25] Sachin Kumar, Biswajit Paria, and Yulia Tsvetkov. Gradient-based constrained sampling from language models. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022_, pages 2251-2277. Association for Computational Linguistics, 2022.
* [26] Daniel Lawson and Ahmed H Qureshi. Merging decision transformers: Weight averaging for forming multi-task policies. In _Workshop on Reincarnating Reinforcement Learning at ICLR 2023_, 2023.
* [27] Yong Lin, Lu Tan, Hangyu Lin, Zeming Zheng, Renjie Pi, Jipeng Zhang, Shizhe Diao, Haoxiang Wang, Han Zhao, Yuan Yao, and T. Zhang. Mitigating the alignment tax of rlhf, 2023.
* [28] Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov, Yejin Choi, and Noah A. Smith. Tuning language models by proxy. _CoRR_, abs/2401.08565, 2024.
* [29] Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, and Asli Celikyilmaz. Don't throw away your value model! generating more preferable text with value-guided monte-carlo tree search decoding, 2023.
* [30] Tianlin Liu, Shangmin Guo, Leonardo Bianco, Daniele Calandriello, Quentin Berthet, Felipe Llinares, Jessica Hoffmann, Lucas Dixon, Michal Valko, and Mathieu Blondel. Decoding-time realignment of language models. _arXiv preprint arXiv:2402.02992_, 2024.
* [31] Kaifeng Lyu, Haoyu Zhao, Xinran Gu, Dingli Yu, Anirudh Goyal, and Sanjeev Arora. Keeping llms aligned after fine-tuning: The crucial role of prompt templates. _ArXiv_, abs/2402.18540, 2024.
* [32] Costas Mavromatis, Petros Karypis, and George Karypis. Pack of llms: Model fusion at test-time via perplexity optimization. _arXiv preprint arXiv:2404.11531_, 2024.
* [33] Sidharth Mudgal, Jong Lee, Harish Ganapathy, YaGuang Li, Tao Wang, Yanping Huang, Zhifeng Chen, Heng-Tze Cheng, Michael Collins, Trevor Strohman, Jilin Chen, Alex Beutel, and Ahmad Beirami. Controlled decoding from language models. _CoRR_, abs/2310.17022, 2023.
* [34] Yurii Nesterov. _Lectures on Convex Optimization_. Springer Publishing Company, Incorporated, 2nd edition, 2018.
* [35] Andi Nika, Debmalya Mandal, Parameswaran Kamalaruban, Georgios Tzannetos, Goran Radanovic, and Adish Singla. Reward model learning vs. direct policy optimization: A comparative analysis of learning from human preferences. _arXiv preprint arXiv:2403.01857_, 2024.
* December 9, 2022_, 2022.
- December 9, 2022_, 2022.
* [38] Alec Radford and Karthik Narasimhan. Improving language understanding by generative pre-training, 2018.
* 16, 2023_, 2023.
* 16, 2023_, 2023.
* [41] Alexandre Rame, Nino Vieillard, Leonard Hussenot, Robert Dadashi, Geoffrey Cideron, Olivier Bachem, and Johan Ferret. WARM: on the benefits of weight averaged reward models. _CoRR_, abs/2401.12187, 2024.
* [42] Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F. Christiano. Learning to summarize from human feedback. _CoRR_, abs/2009.01325, 2020.
* [43] Wenpin Tang. Fine-tuning of diffusion models via stochastic control: entropy regularization and beyond. _arXiv preprint arXiv:2403.06279_, 2024.
* 40, 2017.
* [45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, pages 5998-6008, 2017.
* [46] Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, and Shengyi Huang. Trl: Transformer reinforcement learning. https://github.com/huggingface/trl, 2020.
* [47] Chaoqi Wang, Yibo Jiang, Chenghao Yang, Han Liu, and Yuxin Chen. Beyond reverse KL: Generalizing direct preference optimization with diverse divergence constraints. In _The Twelfth International Conference on Learning Representations_, 2024.
* [48] Haoxiang Wang, Yong Lin, Wei Xiong, Rui Yang, Shizhe Diao, Shuang Qiu, Han Zhao, and Tong Zhang. Arithmetic control of llms for diverse user preferences: Directional preference alignment with multi-objective rewards. _CoRR_, abs/2402.18571, 2024.
* [49] Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. How far can camels go? exploring the state of instruction tuning on open resources, 2023.

* [50] Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Makesh Narsimhan Sreedhar, Daniel Egert, Olivier Delalleau, Jane Polak Scowcroft, Neel Kant, Aidan Swope, and Oleksii Kuchaiev. Helptseer: Multi-attribute helpfulness dataset for steerlm. _ArXiv_, abs/2311.09528, 2023.
* [51] Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 23965-23998. PMLR, 2022.
* [52] Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards for language model training. _arXiv preprint arXiv:2306.01693_, 2023.
* [53] Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan Jiang, and Tong Zhang. Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint, 2024.
* [54] Chunpu Xu, Steffi Chern, Ethan Chern, Ge Zhang, Zekun Wang, Ruibo Liu, Jing Li, Jie Fu, and Pengfei Liu. Align on the fly: Adapting chatbot behavior to established norms. _CoRR_, abs/2312.15907, 2023.
* [55] Yichong Xu, Ruosong Wang, Lin Yang, Aarti Singh, and Artur Dubrawski. Preference-based reinforcement learning with finite-time guarantees. _Advances in Neural Information Processing Systems_, 33:18784-18794, 2020.
* [56] Rui Yang, Xiaoman Pan, Feng Luo, Shuang Qiu, Han Zhong, Dong Yu, and Jianshu Chen. Rewards-in-context: Multi-objective alignment of foundation models with dynamic preference adjustment. _CoRR_, abs/2402.10207, 2024.
* [57] Chenlu Ye, Wei Xiong, Yuheng Zhang, Nan Jiang, and Tong Zhang. A theoretical analysis of nash learning from human feedback under general kl-regularized preference. _arXiv preprint arXiv:2402.07314_, 2024.
* [58] Stephen Zhao, Rob Brekelmans, Alireza Makhzani, and Roger Grosse. Probabilistic inference in language models via twisted sequential monte carlo, 2024.
* [59] Xuandong Zhao, Xianjun Yang, Tianyu Pang, Chao Du, Lei Li, Yu-Xiang Wang, and William Yang Wang. Weak-to-strong jailbreaking on large language models. _CoRR_, abs/2401.17256, 2024.
* [60] Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J. Liu. Slic-hf: Sequence likelihood calibration with human feedback. _CoRR_, abs/2305.10425, 2023.
* [61] Yao Zhao, Mikhail Khalman, Rishabh Joshi, Shashi Narayan, Mohammad Saleh, and Peter J Liu. Calibrating sequence likelihood improves conditional language generation. In _The Eleventh International Conference on Learning Representations_, 2023.
* [62] Zhanhui Zhou, Jie Liu, Chao Yang, Jing Shao, Yu Liu, Xiangyu Yue, Wanli Ouyang, and Yu Qiao. Beyond one-preference-for-all: Multi-objective direct preference optimization for language models. _CoRR_, abs/2310.03708, 2023.
* [63] Zhaoyi Zhou, Chuning Zhu, Runlong Zhou, Qiwen Cui, Abhishek Gupta, and Simon Shaolei Du. Free from bellman completeness: Trajectory stitching via model-based return-conditioned supervised learning. In _The Twelfth International Conference on Learning Representations_, 2024.
* [64] Baiting Zhu, Meihua Dang, and Aditya Grover. Scaling pareto-efficient decision making via offline multi-objective rl. _ArXiv_, abs/2305.00567, 2023.

* [65] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul F. Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. _CoRR_, abs/1909.08593, 2019.
* [66] E. Zitzler and L. Thiele. Multiobjective evolutionary algorithms: a comparative case study and the strength pareto approach. _IEEE Transactions on Evolutionary Computation_, 3(4):257-271, 1999.
* [67] Xu Zou, Da Yin, Qingyang Zhong, Hongxia Yang, Zhilin Yang, and Jie Tang. Controllable generation from pre-trained language models via inverse prompting. _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, 2021.

## Appendix A Impact Statement

* [1] B Notation
* [2] C Main Algorithm
* C.1 Pipeline
* C.2 Divergence measures and closed-form policies
* C.3 Extended variants
* [3] D Full Theoretical Results and Omitted Proofs
* D.1 Definitions
* D.2 Proofs of subsection 5.2
* D.3 Proof of key theorem
* D.4 Proofs of subsection 5.3
* D.5 Proofs of subsection 5.1
* [4] E Implementation Details
* [5] F Supplementary Results
* F.1 Motivating example
* F.2 Additional results for Helpful Assistant
* F.3 Additional results for BeaverTails
* F.4 Additional results for HelpSteer
* F.5 Additional results for Open Instruction-Following
* F.6 Example generations
Impact Statement

Our work proposes a decoding-time language model alignment method aimed at advancing academic research and meeting industry needs. If misused in downstream tasks, especially as what we have shown in Table 7, it could potentially induce language models to generate harmful, offensive, or privacy-infringing content, leading to privacy breaches and societal harm. Nevertheless, this is not directly related to our research, as our primary focus is on a general algorithm with theoretical guarantees.

## Appendix B Notation

Here we introduce a set of notation to be used throughout. For any differentiable function \(f\), let \(\nabla f\) denote its gradient. For any \(N\in\mathbb{N}\), we denote the index set \(\{1,\cdots,N\}\) as \([N]\). Let \(e_{s}\) be the \(s\)th standard basis vector. For any \(i,j\in\mathbb{Z}_{\geq 0}\), \(\delta_{ij}\) represents the Kronecker delta function [15], which output 1 if \(i=j\) otherwise \(0\). For any \(n\in\mathbb{N}\), \(\Delta^{n}\) represents the \(n\)-dimensional probability simplex \(\{(p_{1},\ldots,p_{n+1}):p_{i}\geq 0,\ \forall i\in[n+1],\ \sum_{j=1}^{n+1}p_{j}=1\}\), and \(\Delta(X)\) represents the set of probability distributions over a set \(X\). \(\mathcal{X}\) denotes the prompt set, \(\Sigma\) denotes the alphabet set, \(\mathcal{Y}\subset\Sigma^{*}\) denotes the response set, and the policy set \(\mathcal{S}\) is defined as all mappings from \(\mathcal{X}\) to \(\Delta(\mathcal{Y})\).

## Appendix C Main Algorithm

### Pipeline

**Data:** Alphabet set \(\Sigma\), prompt \(x_{0}\), number of beams \(K\), maximum length \(L\), divergence function \(f\), preference weightings \(w\in\Delta^{M-1}\), and policies \(\pi_{\text{ref}},\pi_{1},\pi_{2},\ldots,\pi_{M}\)

**Result:** Optimal sequence of tokens

\(S_{\text{queue}}\leftarrow\{(\text{seq}:(\text{bos}),f\text{-score}:0)\}\);

\(S_{\text{next}}\leftarrow\emptyset\);

\(S_{\text{completed}}\leftarrow\emptyset\);

**for**\(d=1\)**to**\(L\)**do**

**foreach**\(s\in S_{\text{queue}}\)**do**

**if**\(s.\text{seq}[-1]=\langle\text{eos}\rangle\) _or_ \(d=L\)**then**

\(S_{\text{completed}}\gets S_{\text{completed}}\cup\{s\}\);

**continue;**

**end**

\(S_{\text{successors}}\leftarrow\emptyset\);

**foreach**\(t\in\Sigma\)**do**

\(y\leftarrow\text{cat}(s.\text{seq},t)\);

\(v\leftarrow\pi_{\text{ref}}(y|x_{0})(\nabla f)^{(-1)}\left(\sum_{i=1}^{M}w_{i }\cdot\nabla f\left(\frac{\pi_{i}(y|x_{0})}{\pi_{\text{ref}}(y|x_{0})}\right)\right)\);

\(S_{\text{successors}}\gets S_{\text{successors}}\cup\{(\text{seq}:y,f\text{- score}:v)\}\);

**end**

\(S_{\text{next}}\gets S_{\text{next}}\cup S_{\text{successors}}\);

**end**

Sort \(S_{\text{next}}\) by descending \(f\)-score;

\(S_{\text{queue}}\leftarrow\text{top-k}(S_{\text{next}},K)\);

\(S_{\text{next}}\leftarrow\emptyset\);

**end**

**return** sequence with the highest \(f\)-score in \(S_{\text{completed}}\).

### Divergence measures and closed-form policies

We acknowledge that commonly used \(f\)-divergence measures have been introduced in [47] and show them here for completeness:

\begin{tabular}{l l l l} \hline \hline Divergence measure & \(f(x)\) & \(\nabla f(x)\) & barrier function \\ \hline Reverse KL-divergence & \(x\log x\) & \(\log x+1\) & ✔ \\ Forward KL-divergence & \(-\log x\) & \(-1/x\) & ✔ \\ JSD & \(x\log x-(x+1)\log\frac{x+1}{2}\) & \(\log\frac{2x}{1+x}\) & ✔ \\ \(\alpha\)-divergence & \(\frac{x^{1-\alpha}-(1-\alpha)x-\alpha}{\alpha(1-\alpha)}\) & \((1-x^{-\alpha})/\alpha\) & ✔ \\ Jeffery divergence & \(x\log x-\log x\) & \(\log x-\frac{1}{x}+1\) & ✔ \\ Total Variation & \(|x-1|/2\) & \(\mathrm{sgn}(x-1)/2\) & ✔ \\ Chi-squared & \((x-1)^{2}\) & \(2(x-1)\) & ✔ \\ \hline \hline \end{tabular} Here we show the optimal sampling policies for multi-objective w.r.t. these divergence measures:

\begin{tabular}{l l} \hline \hline Divergence measure & Optimal policy \\ \hline Reverse KL-divergence & \(\left(\prod_{i=1}^{M}\pi_{i}(y|x)^{w_{i}}\right)\cdot\exp(-Z(x))\) \\ Forward KL-divergence & \(\pi_{\mathrm{ref}}(y|x)\cdot\left(Z(x)+\sum_{i=1}^{M}\frac{w_{i}\pi_{\mathrm{ ref}}(y|x)}{\pi_{i}(y|x)}\right)^{-1}\) \\ JSD & \(\pi_{\mathrm{ref}}(y|x)\cdot\left(-1+\exp(Z(x))\prod_{i=1}^{M}\left(\frac{\pi_ {\mathrm{ref}}(y|x)}{\pi_{i}(y|x)}+1\right)^{w_{i}}\right)^{-1}\) \\ \(\alpha\)-divergence & \(\pi_{\mathrm{ref}}(y|x)\cdot\left(\alpha Z(x)+\sum_{i=1}^{M}w_{i}\left(\frac{ \pi_{\mathrm{ref}}(y|x)}{\pi_{i}(y|x)}\right)^{\alpha}\right)^{-\frac{1}{ \alpha}}\) \\ \hline \hline \end{tabular} And we show the optimal decoding policies for multi-objective w.r.t. these divergence measures:

\begin{tabular}{l l} \hline \hline Divergence measure & Approximated policy \\ \hline Reverse KL-divergence & \(\propto\prod_{i=1}^{M}\pi_{i}(y|x)^{w_{i}}\) \\ Forward KL-divergence & \(\propto\left(\sum_{i=1}^{M}\frac{w_{i}}{\pi_{i}(y|x)}\right)^{-1}\) \\ JSD & \(\propto\pi_{\mathrm{ref}}(y|x)\cdot\left(-1+\prod_{i=1}^{M}\left(\frac{\pi_{ \mathrm{ref}}(y|x)}{\pi_{i}(y|x)}+1\right)^{w_{i}}\right)^{-1}\) \\ \(\alpha\)-divergence & \(\propto\left(\sum_{i=1}^{M}\frac{w_{i}}{\pi_{i}(y|x)^{\alpha}}\right)^{-\frac{ 1}{\alpha}}\) \\ \hline \hline \end{tabular}

### Extended variants

**SFT.** We assume that, supervised fine-tuning (SFT) on pre-trained model \(\mathcal{M}^{-}\) yielding \(\mathcal{M}^{+}\), is implicitly optimizing a underlying reward \(r\) w.r.t. Reverse KL-divergence, _i.e._

\[\mathbb{P}_{\mathcal{M}^{+}}(y|x)\propto\mathbb{P}_{M^{-}}(y|x)\cdot\exp(\frac {1}{\beta}r(y|x))\.\] (Eq. ( 3 ))

Based on this, our approach, namely Eq. (7), is applicable to SFT models.

**Proxy-tuning**[28] **& **& **jail-breaking**[59]**. Based on the claim above, for another base model \(\mathcal{M}\), we thus have

\[\mathbb{P}_{\mathcal{M}}(y|x)\cdot\frac{\mathbb{P}_{\mathcal{M}^{+}}(y|x)}{ \mathbb{P}_{\mathcal{M}^{-}}(y|x)}\propto\mathbb{P}_{\mathcal{M}(y|x)}\cdot \exp(\frac{1}{\beta}r(y|x))\,\]

which reflects the tuned version of model \(\mathcal{M}\). And this is exactly the proxy-tuning approach, validated by extensive experiments in [28]. Reversing the position of \(\mathbb{P}_{\mathcal{M}^{+}}\) and \(\mathbb{P}_{\mathcal{M}_{-}}\) yields jail-breaking [59]. \(\delta\)-unlearning [19] is the same.

**Multi-objective proxy-tuning.** Moreover, it is worth noting that, our method can be applied as a lightweight approach for large-scale models, as a multi-objective extension of proxy-tuning [28]. In particular, to tune a large pre-trained model \(\mathcal{M}\), we can first tune \(\mathcal{M}_{1}^{+},\mathcal{M}_{2}^{+},\ldots,\mathcal{M}_{M}^{+}\) from a relatively smaller model \(\mathcal{M}^{-}\) by PPO, DPO or SFT, and decode \(y_{t}\) at timestep \(t\) as

\[\operatorname*{argmax}_{s\in\Sigma}\frac{\mathbb{P}_{\mathcal{M}}(y_{<t},s|x)}{ \mathbb{P}_{\mathcal{M}^{-}}(y_{<t},s|x)}\cdot\prod_{i=1}^{M}\mathbb{P}_{ \mathcal{M}_{i}^{+}}(y_{<t},s|x)^{w_{i}}\.\]

**DeRa [30].** Given \(\mathbb{P}_{\mathcal{M}^{+}}(y|x)\propto\mathbb{P}_{M^{-}}(y|x)\cdot\exp(\frac{1}{ \beta}r(y|x))\), then

\[\mathbb{P}_{M^{-}}(y|x)\cdot\left(\frac{\mathbb{P}_{\mathcal{M}^{+}}(y|x)}{ \mathbb{P}_{M^{-}}(y|x)}\right)^{\frac{\beta}{\beta^{\prime}}}\propto\mathbb{P} _{M^{-}}(y|x)\cdot\exp(\frac{1}{\beta^{\prime}}r(y|x))\;,\]

yields a \(\beta^{\prime}\)-realigned version of \(\mathcal{M}^{-}\).

**MODPO [62].** Assuming \(\pi_{i}\) is the optimal policy for \(\mathcal{R}_{i}\) w.r.t. \(\beta\operatorname{KL}\left(\cdot\|\pi_{\mathrm{ref}}\right)\), \(\forall i\in[M]\), then the optimal policy for \(\sum_{i=1}^{M}w_{i}\mathcal{R}_{i}\) w.r.t. \(\beta\operatorname{KL}\left(\cdot\|\pi_{\mathrm{ref}}\right)\), \(\pi^{*}\propto\prod\pi_{i}^{w_{i}}\), is the minimizer of

\[-\operatorname*{\mathbb{E}}_{(x,y_{w},y_{i})\sim\mathcal{D}_{1}}\log\sigma \left(\frac{1}{w_{1}}\left(\beta\log\frac{\pi(y_{w}|x)}{\pi_{\mathrm{ref}}(y_{ w}|x)}-\beta\log\frac{\pi(y_{l}|x)}{\pi_{\mathrm{ref}}(y_{l}|x)}\right)-\frac{w_{-1} ^{T}}{w_{1}}\sum_{i=2}^{M}\left(\beta\log\frac{\pi_{i}(y_{w}|x)}{\pi_{\mathrm{ ref}}(y_{w}|x)}-\beta\log\frac{\pi_{i}(y_{l}|x)}{\pi_{\mathrm{ref}}(y_{l}|x)} \right)\right)\;,\]

where \(\sigma\) is sigmoid function, and \(\mathcal{D}_{1}\) is the comparison dataset corresponding to \(\mathcal{R}_{1}\). Since

\[\beta\log\frac{\pi_{i}(y_{w}|x)}{\pi_{\mathrm{ref}}(y_{w}|x)}-\beta\log\frac{ \pi_{i}(y_{l}|x)}{\pi_{\mathrm{ref}}(y_{l}|x)}=\mathcal{R}_{i}(y_{w}|x)- \mathcal{R}_{i}(y_{l}|x)\;,\]

we can substitute this term with learned reward representations \(r_{\phi,i}\) and yields

\[-\operatorname*{\mathbb{E}}_{(x,y_{w},y_{i})\sim\mathcal{D}_{1}}\log\sigma \left(\frac{1}{w_{1}}\left(\beta\log\frac{\pi(y_{w}|x)}{\pi_{\mathrm{ref}}(y_{ w}|x)}-\beta\log\frac{\pi(y|x)}{\pi_{\mathrm{ref}}(y_{l}|x)}\right)-\frac{w_{-1} ^{T}}{w_{1}}\left(r_{\phi,-1}(y_{w}|x)-r_{\phi,-1}(y_{l}|x)\right)\right)\;,\]

which is the optimization objective of MODPO.

## Appendix D Full Theoretical Results and Omitted Proofs

### Definitions

**Definition 1** (\(f\)-divergence [1, 10, 11]).: _For probability measures \(P\) and \(Q\), let \(\mu\) be a dominating measure of \(P\) and \(Q\) (i.e. \(P,Q\ll\mu\)), and let \(p,q\) be the Radon-Nikodym derivative [14]\(\frac{dP}{d\mu}\), \(\frac{dQ}{d\mu}\) respectively. For simplicity, here we assume \(q>0\) almost surely. Then \(f\)-divergence from \(P\) to \(Q\) is defined as_

\[I_{f}(p\|q):=\int qf\left(\frac{p}{q}\right)d\mu\;,\]

_where \(f\) is convex on \(\mathbb{R}_{+}\), satisfying \(f(1)=0\). Most useful divergence measures are included in \(f\)-divergences, and the commonly used ones and corresponding \(f\) are introduced in Appendix C.2._

**Definition 2** (Barrier function [34]).: _Given conditions satisfied in Definition 1, if additionally \(0\notin\operatorname{dom}(\nabla f)\), then \(f\) is a barrier function. If a barrier function \(f\) is continuously differentiable and strongly convex on \(\mathbb{R}_{+}\), then \(f\) is a strongly convex and smooth barrier function (abbreviated as strong-barrier function)._

**Definition 3** (Expected calibration error [17, 47]).: _Denote the ground truth distribution as \(\mathbb{P}\), prompt as \(X\) and response as \(Y\). The expected calibration error of a stochastic policy \(\pi\) is defined as_

\[\operatorname{ECE}(\pi):=\operatorname*{\mathbb{E}}_{\begin{subarray}{c}x\sim \mathcal{X}\\ y\sim\pi(|x)\end{subarray}}\left|\mathbb{P}(Y=y|X=x)-\pi(y|x)\right|\,.\]

**Hypothesis 1** (Reduced reward mis-specification [51, 40, 21]).: _Let \(\theta_{i}\) be the parameter of the optimal policy for objective \(J_{i}\), \(\forall i\in[M]\), and \(\theta_{w}^{*}\) be the parameter of the optimal policy for the interpolated objective \(\sum_{i=1}^{M}w_{i}\cdot J_{i}\), then this hypothesis claims_

\[\theta_{w}^{*}\in\left\{\sum_{i=1}^{M}\lambda_{i}\cdot\theta_{i},\lambda\in \Delta^{M-1}\right\}\;,\;\forall w\in\Delta^{M-1}\;.\]

### Proofs of subsection 5.2

**Theorem 3**.: _If \(f\) is not a barrier function, then for \(\forall C\in\mathbb{R}_{+}\), \(N\in\mathbb{Z}_{\geq 4}\), \(M\in\mathbb{Z}_{\geq 2}\), \(\mathcal{Y}=\{y_{i}\}_{i=1}^{N}\), any multi-objective decoding or merging algorithm \(\mathcal{A}:\mathcal{S}^{M+1}\times\Delta^{M-1}\to\mathcal{S}\), there exists a referencepolicy \(\pi_{\mathrm{ref}}\), policies \(\{\pi_{i}\}_{i=1}^{M}\) and \(\pi^{\prime}\), reward functions \(\{\mathcal{R}_{i}\}_{i=1}^{M}\), preference weightings \(w\in\Delta^{M-1}\) and \(\beta\in\mathbb{R}_{+}\), s.t. \(\pi_{i}\) is the optimal policy for \(\mathcal{R}_{i}\) w.r.t. \(\beta\cdot I_{f}(\cdot\|\pi_{\mathrm{ref}})\) (see Definition 1 in Appendix D.1), \(\forall i\in[M]\), but_

\[\mathop{\mathbb{E}}_{y\sim\pi_{\mathcal{A},w}}\left[\sum_{i=1}^{M}w_{i} \mathcal{R}_{i}(y)\right]\leq\mathop{\mathbb{E}}_{y\sim\pi^{\prime}}\left[\sum_ {i=1}^{M}w_{i}\mathcal{R}_{i}(y)\right]-C\;,\text{and}\]

\[\mathop{\mathbb{E}}_{y\sim\pi_{\mathcal{A},w}}\left[\sum_{i=1}^{M}w_{i} \mathcal{R}_{i}(y)\right]-\beta I_{f}(\pi_{\mathcal{A},w}\|\pi_{\mathrm{ref}} )\leq\mathop{\mathbb{E}}_{y\sim\pi^{\prime}}\left[\sum_{i=1}^{M}w_{i} \mathcal{R}_{i}(y)\right]-\beta I_{f}(\pi^{\prime}\|\pi_{\mathrm{ref}})-C\;,\]

_where \(\pi_{\mathcal{A},w}(y):=\mathcal{A}\big{(}\pi_{\mathrm{ref}},\pi_{1},\pi_{2}, \ldots,\pi_{M},w\big{)}(y)\)._

Proof.: Since \(f\) is not a barrier function, \(0\in\text{dom}(\nabla f)\). Now we can define \(p:=\max\limits_{x\in[0,N]}\nabla f(x)\), \(q:=\min\limits_{x\in[0,N]}\nabla f(x)\), \(r:=\max\limits_{x\in[0,N]}f(x)-\min\limits_{x\in[0,N]}f(x)\), \(s:=\frac{N-2}{N-3}\cdot C\). Let \(w=(0.5,0.5,\underbrace{0,\ldots,0}_{N-2})\), and we pick \(k=\mathop{\mathrm{argmin}}_{j\in\{3,4,\ldots,N\}}\pi_{\mathcal{A},w}(y_{j})\). Let \(\pi_{\mathrm{ref}}(y_{i})=\frac{1}{N}\), \(\pi_{1}(y_{i})=\delta_{1i}\), \(\pi_{2}(y_{i})=\delta_{2i}\), \(\pi_{j}(y_{i})=\frac{1}{N}\) and \(\pi^{\prime}(y_{i})=\delta_{ik}\), \(\forall i\in[N],\ j\in\{3,4,\ldots,M\}\). And set \(\mathcal{R}_{1}(y_{i})=\begin{cases}2p+2r+2s&i=1\\ 4q-2p-2r-2s&i=2\\ p+q+r+s&i=k\\ 2q&\text{o/w}\end{cases}\).

\[\mathcal{R}_{2}(y_{i})=\begin{cases}4q-2p-2r-2s&i=1\\ 2p+2r+2s&i=2\\ p+q+r+s&i=k\\ 2q&\text{o/w}\end{cases}\text{, and }\mathcal{R}_{j}\equiv 0\text{, }\forall j\in\{3,4, \ldots,M\}.\]

Let \(\beta=1\), then the optimization objective for \(\mathcal{R}_{1}\) w.r.t. \(I_{f}\) is \(J_{1}(\pi):=\mathop{\mathbb{E}}_{y\sim\pi}\left[\mathcal{R}_{1}(y)\right]-I_{f} (\pi\|\pi_{\mathrm{ref}})\), and the Lagrangian dual is

\[\mathcal{L}_{1}(\pi):=\sum_{i=1}^{N}\left(-\mathcal{R}_{1}(y_{i})\cdot\pi(y_{i} )+\frac{1}{N}f\left(N\cdot\pi(y_{i})\right)\right)+\lambda\left(\sum_{i=1}^{N }\pi(y_{i})-1\right)-\sum_{i=1}^{N}\mu_{i}\pi(y_{i})\;.\]

As the objective is convex and the constraints are affine, we can directly apply the _Karush-Kuhn-Tucker conditions_[34]:

\[\nabla\mathcal{L}_{1}(\pi_{1}^{\star}) =0\;,\] (8) \[\sum_{i=1}^{N}\pi_{1}^{\star}(y_{i}) =1\;,\] \[\pi_{1}^{\star}(y_{i}) \geq 0\;,\] \[\mu_{i}^{\star}\pi_{1}^{\star}(y_{i}) =0\;.\] (9)

Eq. (8) implies

\[-\mathcal{R}_{1}(y_{i})+\nabla f(N\cdot\pi_{1}^{\star}(y_{i}))+\lambda^{\star}- \mu_{i}^{\star}=0\;.\]

If \(\pi_{1}^{\star}(y_{1})>0\), we have

\[\lambda^{\star} =\mathcal{R}_{1}(y_{1})-\nabla f(N\cdot\pi_{1}^{\star}(y_{1}))\] \[\geq p+2r+2s\;,\]

and then for \(\forall j\neq 1\),

\[\mu_{j}^{\star} =-\mathcal{R}_{1}(y_{j})+\nabla f(N\cdot\pi_{1}^{\star}(y_{j}))+ \lambda^{\star}\] \[\geq-p-q-r-s+q+p+2r+2s\] \[=r+s\] \[>0\;.\]Combining it with Eq. (9) yields \(\pi_{1}^{\prime}(y_{j})=0\) for \(\forall j\neq 1\), which is exactly \(\pi_{1}\). Note that we have

\[J(\pi_{1}) \geq 2p+2r+2s-\max_{x\in[0,N]}f(x)\.\]

For any \(\pi^{\prime}\) with \(\pi^{\prime}(y_{1})=0\), we have

\[J(\pi^{\prime}) \leq p+q+r+s-\min_{x\in[0,N]}f(x)\] \[=p+q+2r+s-\max_{x\in[0,N]}f(x)\] \[<J(\pi_{1})\.\]

Thus \(\pi_{1}\) is the optimal policy for \(\mathcal{R}_{1}\) w.r.t. \(I_{f}(\cdot\|\pi_{\mathrm{ref}})\). Similarly, \(\pi_{2}\) is the optimal policy for \(\mathcal{R}_{2}\) w.r.t. \(I_{f}(\cdot\|\pi_{\mathrm{ref}})\). By convexity of \(f\), the minimum of \(I_{f}(\pi\|\pi_{\mathrm{ref}})\) is obtained when \(\pi=\pi_{\mathrm{ref}}\), and thus \(\pi_{j}\) is the optimal policy for \(\mathcal{R}_{j}\) w.r.t. \(I_{f}(\cdot\|\pi_{\mathrm{ref}})\), for \(\forall j\in\{3,4,\ldots,M\}\). Therefore, all conditions are well satisfied by this construction. Note that

\[\operatorname*{\mathbb{E}}_{y\sim\pi^{\prime}}\left[\sum_{i=1}^{M}w_{i} \mathcal{R}_{i}(y)\right]=p+q+r+s\.\] (10)

While by the selection of \(k\), we have

\[\operatorname*{\mathbb{E}}_{y\sim\pi_{\mathcal{A},w}}\left[\sum_{i=1}^{M}w_{i} \mathcal{R}_{i}(y)\right]\leq\frac{(N-3)\cdot 2q+p+q+r+s}{N-2}\.\] (11)

Comparing Eq. (10) with Eq. (11), we have

\[\operatorname*{\mathbb{E}}_{y\sim\pi_{\mathcal{A},w}}\left[\sum_{ i=1}^{M}w_{i}\mathcal{R}_{i}(y)\right] \leq\operatorname*{\mathbb{E}}_{y\sim\pi^{\prime}}\left[\sum_{i=1}^{M}w_{i} \mathcal{R}_{i}(y)\right]-\frac{N-3}{N-2}s\] \[=\operatorname*{\mathbb{E}}_{y\sim\pi^{\prime}}\left[\sum_{i=1}^ {M}w_{i}\mathcal{R}_{i}(y)\right]-C\.\]

Note that \(\pi_{\mathrm{ref}}\) is a uniform distribution and both \(\pi_{\mathcal{A},w},\pi^{\prime}\) are one-point distributions, thus \(I_{f}(\pi_{\mathcal{A},w}\|\pi_{\mathrm{ref}})=I_{f}(\pi^{\prime}\|\pi_{ \mathrm{ref}})\). We have

\[\operatorname*{\mathbb{E}}_{y\sim\pi_{\mathcal{A},w}}\left[\sum_{ i=1}^{M}w_{i}\mathcal{R}_{i}(y)\right]-I_{f}(\pi_{\mathcal{A},w}\|\pi_{ \mathrm{ref}})\leq\operatorname*{\mathbb{E}}_{y\sim\pi^{\prime}}\left[\sum_ {i=1}^{M}w_{i}\mathcal{R}_{i}(y)\right]-I_{f}(\pi^{\prime}\|\pi_{\mathrm{ref}} )-C\.\qed\]

**Lemma 1**.: _Given a reference policy \(\pi_{\mathrm{ref}}\), reward function \(\mathcal{R}\), a strong-barrier function \(f\) and \(\beta\in\mathbb{R}_{+}\), then_

\[\pi(y|x)=\pi_{\mathrm{ref}}(y|x)\cdot(\nabla f)^{(-1)}\left(-Z(x)+\frac{1}{ \beta}\mathcal{R}(y|x)\right)\,\]

_where \(Z(x)\) is the normalization factor w.r.t. \(x\), is the optimal policy for_

\[\operatorname*{\mathbb{E}}_{\begin{subarray}{c}x\sim\mathcal{X} \\ y\sim\pi(\cdot|x)\end{subarray}}\mathcal{R}(y|x)-\beta\operatorname*{\mathbb{E }}_{\begin{subarray}{c}x\sim\mathcal{X}\\ y\sim\pi_{\mathrm{ref}}(\cdot|x)\end{subarray}}f\left(\frac{\pi(y|x)}{\pi_{ \mathrm{ref}}(y|x)}\right)\.\]

Proof.: The lemma is revealed by Theorem 1 in [47]. For completeness, we give a brief proof here. Since \(f\) is convex and barrier, we can directly use Lagrange multiplier to solve

\[\sum_{y\in\mathcal{Y}}\pi(y|x)\mathcal{R}(y|x)-\beta\sum_{y\in \mathcal{Y}}\pi_{\mathrm{ref}}(y|x)f\left(\frac{\pi(y|x)}{\pi_{\mathrm{ref}}(y| x)}\right)\,\ \mathrm{w.r.t.}\ \sum_{y\in\mathcal{Y}}\pi(y|x)=1\,\]

for each \(x\in\mathcal{X}\), which implies

\[\mathcal{R}(y|x)-\beta\nabla f\left(\frac{\pi(y|x)}{\pi_{\mathrm{ref}}(y|x)} \right)-\lambda(x)=0\,\]

where \(\lambda(x)\in\mathbb{R}\). Taking \(Z(x):=\beta\lambda(x)\) completes the proof.

**Proposition 1**.: _Given a reference policy \(\pi_{\rm ref}\), optimal policies \(\pi_{1},\pi_{2},\ldots,\pi_{M}\) for each reward function \(\mathcal{R}_{1},\mathcal{R}_{2},\ldots,\mathcal{R}_{M}\) w.r.t. \(\beta\cdot I_{f}(\cdot\|\pi_{\rm ref})\), \(\beta\in\mathbb{R}_{+}\), and \(w\in\Delta^{M-1}\), if \(f\) is a strong-barrier function, then the optimal policy for reward function \(r=\sum_{i=1}^{M}w_{i}\cdot\mathcal{R}_{i}\) w.r.t. \(\beta\cdot I_{f}(\cdot\|\pi_{\rm ref})\) is:_

\[\pi^{\star}(y|x)=\pi_{\rm ref}(y|x)\cdot(\nabla f)^{(-1)}\left(-Z(x)+\sum_{i=1 }^{M}w_{i}\cdot\nabla f\left(\frac{\pi_{i}(y|x)}{\pi_{\rm ref}(y|x)}\right) \right)\,\]

_where \(Z(x)\) is the normalization factor w.r.t. \(x\), and numerically computable when \(|\mathcal{Y}|\) is finite._

Proof.: As Lemma 1 shows,

\[\mathcal{R}_{i}(y|x)=\beta\nabla f\left(\frac{\pi_{i}(y|x)}{\pi_{\rm ref}(y|x) }\right)+\beta Z_{i}(x)\,\] (12)

and

\[\pi^{\star}(y|x)=\pi_{\rm ref}(y|x)\cdot(\nabla f)^{(-1)}\left(-Z^{\star}(x)+ \frac{1}{\beta}\sum_{i=1}^{M}w_{i}\cdot\mathcal{R}_{i}(y|x)\right)\.\] (13)

Apply Eq. (12) into Eq. (13), we get

\[\pi^{\star}(y|x) =\pi_{\rm ref}(y|x)\cdot(\nabla f)^{(-1)}\left(-Z^{\star}(x)+\sum _{i=1}^{M}w_{i}\cdot\left(\nabla f\left(\frac{\pi_{i}(y)}{\pi_{\rm ref}(y)} \right)+Z_{i}(x)\right)\right)\] \[=\pi_{\rm ref}(y|x)\cdot(\nabla f)^{(-1)}\left(-Z(x)+\sum_{i=1}^{ M}w_{i}\cdot\nabla f\left(\frac{\pi_{i}(y|x)}{\pi_{\rm ref}(y|x)}\right) \right)\,\]

where \(Z(x):=Z^{\star}(x)-\sum_{i=1}^{M}w_{i}Z_{i}(x)\). And \(Z(x)\) is the root of \(\phi_{x}(t)=0\), where

\[\phi_{x}(t):=\sum_{y\in\mathcal{Y}}\pi_{\rm ref}(y|x)\cdot(\nabla f)^{(-1)} \left(-t+\sum_{i=1}^{M}w_{i}\cdot\nabla f\left(\frac{\pi_{i}(y|x)}{\pi_{\rm ref }(y|x)}\right)\right)-1\.\]

Since \(f\) is strongly convex and continuously differentiable, \(\phi_{x}(t)\) is monotonically decreasing and continuous. If \(|\mathcal{Y}|\) is finite, we can set

\[t_{1,x} :=-\nabla f(1)+\min_{y\in\mathcal{Y}}\sum_{i=1}^{M}w_{i}\cdot \nabla f\left(\frac{\pi_{i}(y|x)}{\pi_{\rm ref}(y|x)}\right)\,\] \[t_{2,x} :=-\nabla f(1)+\max_{y\in\mathcal{Y}}\sum_{i=1}^{M}w_{i}\cdot \nabla f\left(\frac{\pi_{i}(y|x)}{\pi_{\rm ref}(y|x)}\right)\,\]

then we have

\[\phi(t_{1,x}) \geq 0\,\] \[\phi(t_{2,x}) \leq 0\.\]

Thus \(Z(x)\in[t_{1,x},t_{2,x}]\). Finally, \(Z(x)\) can be numerically computed by _bisection method_. 

### Proof of key theorem

**Proposition 2** (Policy-to-reward mapping).: _Given a reference policy \(\pi_{\rm ref}\), optimal policies \(\pi_{1},\pi_{2},\ldots,\pi_{M}\) for each reward function \(\mathcal{R}_{1},\mathcal{R}_{2},\ldots,\mathcal{R}_{M}\) w.r.t. \(\beta\cdot I_{f}(\cdot\|\pi_{\rm ref})\), \(\beta\in\mathbb{R}_{+}\), and \(w\in\Delta^{M-1}\), if \(f\) is a strong-barrier function, then for \(\forall x\in\mathcal{X},\ y_{1},y_{2}\in\mathcal{Y}\), we have:_

\[\sum_{i=1}^{M}w_{i}\mathcal{R}_{i}(y_{1}|x)\geq\sum_{i=1}^{M}w_{i}\mathcal{R}_{ i}(y_{2}|x)\ \Longleftrightarrow\ \sum_{i=1}^{M}w_{i}\nabla f\left(\frac{\pi_{i}(y_{1}|x)}{\pi_{\rm ref}(y_{1}|x) }\right)\geq\sum_{i=1}^{M}w_{i}\nabla f\left(\frac{\pi_{i}(y_{2}|x)}{\pi_{\rm ref }(y_{2}|x)}\right)\.\]Proof.: As Eq. (3) shows,

\[\mathcal{R}_{i}(y|x)=\beta\nabla f\left(\frac{\pi_{i}(y|x)}{\pi_{\rm ref }(y|x)}\right)+\beta Z_{i}(x)\;,\] (14)

for \(\forall i\in[M]\), \(y\in\mathcal{Y}\), where \(Z_{i}(x)\) is the normalization factor. Thus

\[\sum_{i=1}^{M}w_{i}\mathcal{R}_{i}(y_{1}|x)-\sum_{i=1}^{M}w_{i} \mathcal{R}_{i}(y_{2}|x) =\sum_{i=1}^{M}w_{i}\cdot(\mathcal{R}_{i}(y_{1}|x)-\mathcal{R}_{i }(y_{2}|x))\] \[=\beta\sum_{i=1}^{M}w_{i}\cdot\left(\nabla f\left(\frac{\pi_{i}(y _{1}|x)}{\pi_{\rm ref}(y_{1}|x)}\right)-\nabla f\left(\frac{\pi_{i}(y_{2}|x)}{ \pi_{\rm ref}(y_{2}|x)}\right)\right)\;.\]

Since \(\beta>0\), the proposition holds. 

**Theorem 5** (Key theorem).: _Given a reference policy \(\pi_{\rm ref}\), optimal policies \(\pi_{1},\pi_{2},\ldots,\pi_{M}\) for each reward function \(\mathcal{R}_{1},\mathcal{R}_{2},\ldots,\mathcal{R}_{M}\) w.r.t. \(\beta\cdot I_{f}(\cdot\|\pi_{\rm ref})\), \(\beta\in\mathbb{R}_{+}\), and \(w\in\Delta^{M-1}\), if \(f\) is a strong-barrier function, then for \(\forall x\in\mathcal{X}\), \(w\in\Delta^{M-1}\), \(\exists C\in\mathbb{R}\), s.t._

\[\operatorname*{argmax}_{y\in\mathcal{Y}}\pi_{\rm ref}(y|x)\cdot( \nabla f)^{(-1)}\left(\sum_{i=1}^{M}w_{i}\cdot\nabla f\left(\frac{\pi_{i}(y|x )}{\pi_{\rm ref}(y|x)}\right)\right)\;,\]

_is an optimal solution for_

\[\max_{y\in\mathcal{Y}}\pi_{\rm ref}(y|x)\;,\;\text{w.r.t.}\;\sum_ {i=1}^{M}w_{i}\cdot\mathcal{R}_{i}(y|x)\geq C\;.\] (15)

Proof.: First we define

\[g_{x}(t)=(\nabla f)^{(-1)}\left(\frac{t}{\beta}-\sum_{i=1}^{M}w _{i}Z_{i}(x)\right)\;.\]

From Eq. (14), we have

\[g_{x}\left(\sum_{i=1}^{M}w_{i}\cdot\mathcal{R}_{i}(y|x)\right)= \left(\nabla f\right)^{(-1)}\left(\sum_{i=1}^{M}w_{i}\cdot\nabla f\left(\frac {\pi_{i}(y|x)}{\pi_{\rm ref}(y|x)}\right)\right)\;.\]

Then let

\[y^{\prime} :=\operatorname*{argmax}_{y}\;\pi_{\rm ref}(y|x)\cdot(\nabla f)^{ (-1)}\left(\sum_{i=1}^{M}w_{i}\cdot\nabla f\left(\frac{\pi_{i}(y|x)}{\pi_{\rm ref }(y|x)}\right)\right)\] \[=\operatorname*{argmax}_{y}\;\pi_{\rm ref}(y|x)\cdot g_{x}\left( \sum_{i=1}^{M}w_{i}\cdot\mathcal{R}_{i}(y|x)\right)\;,\]

and \(C:=\sum_{i=1}^{M}w_{i}\cdot\mathcal{R}_{i}(y^{\prime}|x)\). Suppose \(y^{\prime}\) is not an optimal solution for Eq. (15), then \(\exists y^{\prime\prime}\in\mathcal{Y}\), s.t. \(\pi_{\rm ref}(y^{\prime\prime}|x)>\pi_{\rm ref}(y^{\prime}|x)\) and \(\sum_{i=1}^{M}w_{i}\cdot\mathcal{R}_{i}(y^{\prime\prime}|x)\geq\sum_{i=1}^{M}w _{i}\cdot\mathcal{R}_{i}(y^{\prime}|x)\). Since \(f\) is strongly convex, \(g_{x}\) is continuously increasing and invertible. Thus

\[\pi_{\rm ref}(y^{\prime\prime}|x)\cdot g_{x}\left(\sum_{i=1}^{M}w _{i}\cdot\mathcal{R}_{i}(y^{\prime\prime}|x)\right)>\pi_{\rm ref}(y^{\prime}|x) \cdot g_{x}\left(\sum_{i=1}^{M}w_{i}\cdot\mathcal{R}_{i}(y^{\prime}|x)\right)\;,\]

contradictory to the definition of \(y^{\prime}\).

### Proofs of subsection 5.3

**Proposition 3** (Eq. 13,14 in [39]).: _If \(I_{f}\) is Reverse KL-divergence, Eq. (2) can be viewed as_

\[\frac{1}{\beta}\mathop{\mathbb{E}}_{\begin{subarray}{c}x\sim \mathcal{X}\\ y\sim\pi(\cdot|x)\end{subarray}}[r(y|x)]-\operatorname{KL}\left(\pi\|\pi_{ \operatorname{ref}}\right)=-\operatorname{KL}\left(\pi\|\pi_{\operatorname{ opt}}\right)+\text{constant}\;,\]

_where \(\pi_{\operatorname{opt}}\) is the optimal policy for reward function \(r\) w.r.t. \(\beta\cdot I_{f}(\cdot\|\pi_{\operatorname{ref}})\). Thus we can evaluate a policy \(\pi\) using \(-\operatorname{KL}\left(\pi\|\pi_{\operatorname{opt}}\right)\)._

Proof.: This proposition is revealed by Eq. 13,14 in [39]. For completeness, we give a brief proof here. Define \(Z(x):=\log\sum_{y\in\mathcal{Y}}\pi_{\operatorname{ref}}(y|x)\exp(\frac{1}{ \beta}r(y|x))\), which is a constant. Then we have

\[-\frac{1}{\beta}\mathop{\mathbb{E}}_{\begin{subarray}{c}x\sim \mathcal{X}\\ y\sim\pi(\cdot|x)\end{subarray}}[r(y|x)]+\operatorname{KL}(\pi\|\pi_{ \operatorname{ref}})\] \[=\mathop{\mathbb{E}}_{\begin{subarray}{c}x\sim\mathcal{X}\\ y\sim\pi(\cdot|x)\end{subarray}}\log\pi(y|x)-\log\pi_{\operatorname{ref}}(y|x)- \frac{1}{\beta}r(y|x)\] \[=\mathop{\mathbb{E}}_{\begin{subarray}{c}x\sim\mathcal{X}\\ y\sim\pi(\cdot|x)\end{subarray}}\log\pi(y|x)-\log\left(\pi_{\operatorname{ref}}( y|x)\cdot\exp\left(\frac{1}{\beta}r(y|x)-Z(x)\right)\right)-Z(x)\] \[=\mathop{\mathbb{E}}_{\begin{subarray}{c}x\sim\mathcal{X}\\ y\sim\pi(\cdot|x)\end{subarray}}\log\pi(y|x)-\log\pi_{\operatorname{opt}}(y|x) -Z(x)\] (Eq. ( 3 )) \[=\mathop{\mathbb{KL}}_{\begin{subarray}{c}(\pi\|\pi_{\operatorname{ opt}})\\ \text{underlying loss }\mathcal{L}\end{subarray}}-\mathop{\mathbb{E}}_{ \begin{subarray}{c}x\sim\mathcal{X}\\ \text{constant}\end{subarray}}\;.\qed\]

**Lemma 2**.: _Given \(n,m\in\mathbb{N}\), \(x\in\Delta^{n-1}\), \(x\succ 0\), \(y\in\mathbb{R}^{n}\) and \(C\in\mathbb{R}_{+}\), if \(\sum_{i=1}^{n}x_{i}y_{i}\leq C\), then_

\[\sum_{i=1}^{n}x_{i}\exp\left(-y_{i}\right)\geq\exp(-C)\;.\]

Proof.: Set \(f(y):=\sum_{i=1}^{n}x_{i}\exp\left(-y_{i}\right)\), \(h(y):=\sum_{i=1}^{n}x_{i}y_{i}-C\), and the Lagrangian dual \(L(y,\lambda):=f(y)+\lambda\cdot h(y)\). Since both \(f\) and \(h\) are convex, we can directly apply _Karush-Kuhn-Tucker conditions_:

\[\nabla_{y}L(y^{\star},\lambda^{\star}) =0\;,\] (16) \[h(y^{\star}) \leq 0\;,\] \[\lambda^{\star} \geq 0\;,\] \[\lambda^{\star}h(y^{\star}) =0\;.\]

From Eq. (16) we get

\[\exp\left(-y_{i}^{\star}\right)=\lambda^{\star}\;,\]

for \(\forall i\in[n]\). Then we have

\[\sum_{i=1}^{n}x_{i}\exp\left(-y_{i}\right) =\lambda^{\star}\] \[=\exp\left(\sum_{i=1}^{n}x_{i}\log\lambda^{\star}\right)\] \[=\exp\left(-\sum_{i=1}^{n}x_{i}y_{i}\right)\] \[\geq\exp(-C)\;.\qed\]

[MISSING_PAGE_EMPTY:25]

Thus

\[\mathrm{KL}\left(\frac{1}{Z_{w}(x)}\prod_{i=1}^{M}\pi_{i}^{w_{i}}( \cdot|x)\|\frac{1}{Z^{\star}(x)}\prod_{i=1}^{M}p_{i}^{w_{i}}(\cdot|x)\right)\] \[= \log Z^{\star}(x)-\log Z_{w}(x)+\frac{1}{Z_{w}(x)}\cdot\sum_{y\in \mathcal{Y}}\left(\prod_{i=1}^{M}\pi_{i}^{w_{i}}(y|x)\sum_{j=1}^{M}w_{j}\log \frac{\pi_{j}(y|x)}{p_{j}(y|x)}\right)\] \[\leq \log Z^{\star}(x)-\log Z_{w}(x)+\frac{1}{Z_{w}(x)}\cdot\left(\sum_ {i=1}^{M}w_{i}^{2}\,\mathrm{KL}\left(\pi_{i}\|p_{i}\right)+\sum_{i\neq j}w_{i} w_{j}\left(\mathrm{KL}\left(\pi_{i}\|p_{j}\right)-\mathrm{KL}\left(\pi_{i}\|\pi_{j} \right)\right)\right)\] (AM-GM inequality) \[\leq 2\exp(C)\cdot\mathcal{L}\.\] ( Eq. ( 17 ), ( 19 ), ( 21 ))

Finally we have

\[V^{\star}-V =\mathop{\mathbb{E}}_{x\sim\mathcal{X}}\mathrm{KL}\left(\frac{1} {Z_{w}(x)}\prod_{i=1}^{M}\pi_{i}^{w_{i}}(\cdot|x)\|\frac{1}{Z^{\star}(x)}\prod_ {i=1}^{M}p_{i}^{w_{i}}(\cdot|x)\right)\] (Proposition 3) \[\leq 2\exp(C)\cdot\mathcal{L}\.\qed\]

**Lemma 3** (Theorem 2 in [47]).: _Suppose \(\pi_{1}(\cdot|x)\) and \(\pi_{2}(\cdot|x)\) be two policies, then_

\[\mathrm{ECE}(\pi_{1})-\mathrm{ECE}(\pi_{2})\leq\mathop{\mathbb{E}}_{x\sim \mathcal{X}}\left[2\sqrt{2\,\mathrm{KL}\left(\pi_{1}(\cdot|x)\|\pi_{2}(\cdot|x )\right)}\right]\.\]

**Proposition 4** (Calibration error perspective).: _The expected calibration error (see Definition 3) of \(\pi_{w}\) can be bounded as_

\[\mathrm{ECE}(\pi_{w})\leq\mathrm{ECE}(\pi_{\mathrm{opt}})+4\sqrt{\exp(C)\cdot \mathcal{L}}\.\]

Proof.: This proposition directly comes from combining Lemma 3 with Theorem 4. 

### Proofs of subsection 5.1

**Theorem 2**.: _For any \(f\)-divergence satisfying one of the following conditions: (i) \(f\) is not a barrier function; (ii) \(I_{f}\) is Reverse KL-divergence; (iii) \(f\) is a strong-barrier function, with finite roots of_

\[2\nabla f\left(\frac{3\sqrt{1-2x}}{2\sqrt{1-2x}+\sqrt{x}}\right)-2\nabla f \left(\frac{3\sqrt{x}}{2\sqrt{1-2x}+\sqrt{x}}\right)-\nabla f(3-6x)+\nabla f(3 x)\,\]

\(\exists N,M\in\mathbb{N}\)_, \(\mathcal{Y}=\left\{y_{i}\right\}_{i=1}^{N}\), \(\beta\in\mathbb{R}_{+}\), a neural network \(nn=\mathrm{softmax}(h_{\theta}(z_{0}))\) where \(z_{0}\in\mathbb{R}^{n}\) and \(h_{\theta}:\mathbb{R}^{n}\to\mathbb{R}^{N}\) is a continuous mapping, preference weightings \(w\in\Delta^{M-1}\), reference policy \(\pi_{\mathrm{ref}}\), and the objectives \(J_{1},J_{2},\ldots,J_{M}\) representing reward functions \(\mathcal{R}_{1},\mathcal{R}_{2},\ldots,\mathcal{R}_{M}\) w.r.t. \(\beta\cdot I_{f}(\cdot\|\pi_{\mathrm{ref}})\), s.t. Hypothesis 1 does not hold._

Proof.: (i) If \(f\) is not a barrier function, Hypothesis 1 does not hold immediately from Theorem 3.

(ii) If \(I_{f}\) is Reverse KL-divergence, we let \(N=3\), \(M=3\), and \(h_{\theta}(z_{0})=W_{\theta}^{(2)}\sigma\left(W_{\theta}^{(1)}z_{0}\right)\), where \(\sigma\) is \(\mathrm{ReLU}(\cdot)\). We set \(\mathcal{R}_{i}(y_{j})=\delta_{ij}\), \(\pi_{\mathrm{ref}}(y_{i})=1/3\) for \(\forall i,j\in[3]\), \(z_{0}=1\) and \(\beta=1\). Then the optimal policies are \(W_{\theta_{1}}^{(1)}=e_{1}\), \(W_{\theta_{1}}^{(2)}=\begin{pmatrix}100\\ 000\\ 000\end{pmatrix}\) for \(\mathcal{R}_{1}\) w.r.t. \(\mathrm{KL}\left(\cdot\|\pi_{\mathrm{ref}}\right)\), \(W_{\theta_{2}}^{(1)}=e_{2}\), \(W_{\theta_{2}}^{(2)}=\begin{pmatrix}000\\ 010\\ 000\end{pmatrix}\) for \(\mathcal{R}_{2}\) w.r.t. \(\mathrm{KL}\left(\cdot\|\pi_{\mathrm{ref}}\right)\), and \(W_{\theta_{3}}^{(1)}=e_{3}\), \(W_{\theta_{3}}^{(2)}=\begin{pmatrix}000\\ 000\\ 001\end{pmatrix}\) for \(\mathcal{R}_{3}\) w.r.t. \(\mathrm{KL}\left(\cdot\|\pi_{\mathrm{ref}}\right)\). Thus we have \(h_{\sum_{j=1}^{3}\lambda_{j}\theta_{j}}(z_{0})=\left(\lambda_{1}^{2},\lambda_{2 }^{2},\lambda_{3}^{2}\right)^{\top}\). Given \(w=(0,1/3,2/3)\), the optimal policy \(\pi^{\star}\) should output \(\pi^{\star}(y_{1})=\frac{1}{1+\exp(1/3)+\exp(2/3)}\), \(\pi^{\star}(y_{2})=\frac{\exp(1/3)}{1+\exp(1/3)+\exp(2/3)}\) and \(\pi^{\star}(y_{3})=\frac{\exp(2/3)}{1+\exp(1/3)+\exp(2/3)}\). Note that

\[\sqrt{t}+\sqrt{t+1/3}+\sqrt{t+2/3}>1\,\ \forall t\in\mathbb{R}_{+}\,\]

thus there is no solution \(\lambda\in\Delta^{2},t\in\mathbb{R}_{+}\) for \(\left(\lambda_{1}^{2},\lambda_{2}^{2},\lambda_{3}^{2}\right)^{\top}=\left(t,t +\frac{1}{3},t+\frac{2}{3}\right)^{\top}\), _i.e._ there is no \(\lambda\) s.t. \(\mathrm{softmax}\left(h_{\sum_{j=1}^{3}\lambda_{j}\theta_{j}}(z_{0})\right)= \left(\pi^{\star}(y_{1}),\pi^{\star}(y_{2}),\pi^{\star}(y_{3})\right)\), _i.e._ Hypothesis 1 does not hold.

(iii) If \(f\) is a strong-barrier function, with finite roots of

\[2\nabla f\left(\frac{3\sqrt{1-2x}}{2\sqrt{1-2x}+\sqrt{x}}\right)-2\nabla f \left(\frac{3\sqrt{x}}{2\sqrt{1-2x}+\sqrt{x}}\right)-\nabla f(3-6x)+\nabla f(3 x)\,\]

we let \(N=3\), \(M=2\), \(h_{\theta}(z_{0})=W_{\theta}(z_{0})\), \(z_{0}=1\), \(\mathcal{R}_{1}(y_{i})=\delta_{1i}\), \(\mathcal{R}_{2}(y_{i})=\delta_{2i}\) and \(\pi_{\mathrm{ref}}(y_{i})=1/3\), for \(\forall i\in[3]\). From Eq. (3) the optimal policy for \(J_{1}\) is \(\pi_{\theta_{1}}(y_{i})=\frac{1}{3}(\nabla f)^{(-1)}\left(\frac{1}{\beta} \delta_{1i}-Z\right)\), and the optimal policy for \(J_{2}\) is \(\pi_{\theta_{2}}(y_{i})=\frac{1}{3}(\nabla f)^{(-1)}\left(\frac{1}{\beta} \delta_{2i}-Z\right)\), where \(Z\) is the normalization factor. And these policies can be learned by setting \(W_{\theta_{i}}=\left(\log\pi_{\theta_{i}}(y_{1}),\log\pi_{\theta_{i}}(y_{2}), \log\pi_{\theta_{i}}(y_{3})\right)^{\top}\).

We set \(a:=\pi_{\theta_{1}}(y_{1})=\frac{1}{3}(\nabla f)^{(-1)}(\frac{1}{\beta}-Z)\), \(b:=\pi_{\theta_{1}}(y_{2})=\pi_{\theta_{1}}(y_{3})=\frac{1}{3}(\nabla f)^{(-1 )}(-Z)\). Thus we have

\[\nabla f(3a)-\nabla f(3b) =\frac{1}{\beta}\,\] (22) \[a+2b =1\.\] (23)

From Proposition 1, the optimal policy for \(w_{1}\cdot J_{1}+w_{2}\cdot J_{2}\) is

\[\pi_{w}^{\star}(y_{i})=\frac{1}{3}(\nabla f)^{(-1)}\left(-Z_{w}^{\star}+\frac {w_{1}}{\beta}\delta_{1i}+\frac{w_{2}}{\beta}\delta_{2i}\right)\,\] (24)

where \(Z_{w}^{\star}\) is the normalization factor. By linearly merging the weights of \(\pi_{\theta_{1}}\) and \(\pi_{\theta_{2}}\), we have

\[\pi_{\lambda_{1}\theta_{1}+\lambda_{2}\theta_{2}}(y_{i}) =\mathrm{softmax}\left(\lambda_{1}W_{\theta_{1}}(z_{0})+\lambda_{2} W_{\theta_{2}}(z_{0})\right)(y_{i})\] \[=\frac{1}{Z_{\lambda}}\left((\nabla f)^{(-1)}\left(\frac{1}{\beta} \delta_{1i}-Z\right)\right)^{\lambda_{1}}\left((\nabla f)^{(-1)}\left(\frac{1 }{\beta}\delta_{2i}-Z\right)\right)^{\lambda_{2}}\,\] (25)

where \(Z_{\lambda}\) is the normalization factor.

With symmetry, Eq. (24), (25) and Hypothesis 1 indicate that \(\pi_{\frac{1}{2}\theta_{1}+\frac{1}{2}\theta_{2}}=\pi_{(\frac{1}{2},\frac{1}{2 })}^{\star}\), thus

\[\frac{1}{3}(\nabla f)^{(-1)}\left(-Z_{(0.5,0.5)}^{\star}+\frac{1} {2\beta}\right) =\frac{\sqrt{a}}{2\sqrt{a}+\sqrt{b}}\,\] \[\frac{1}{3}(\nabla f)^{(-1)}\left(-Z_{(0.5,0.5)}^{\star}\right) =\frac{\sqrt{b}}{2\sqrt{a}+\sqrt{b}}\,\]

and combining them with Eq. (22) yields

\[2\nabla f\left(\frac{3\sqrt{a}}{2\sqrt{a}+\sqrt{b}}\right)-2\nabla f\left( \frac{3\sqrt{b}}{2\sqrt{a}+\sqrt{b}}\right)=\nabla f(3a)-\nabla f(3b)\.\] (26)

Given the condition, the solution set \((a,b)\) to Eq. (23), (26) is finite, thus there exists \(\beta\in\mathbb{R}_{+}\) s.t. Eq. (22) does not hold, implying that Hypothesis 1 does not hold.

Implementation Details

**Codebase.** Our codebase is mainly based on trl [46] (https://github.com/huggingface/trl), MODPO [62] (https://github.com/ZHZisZZ/modpo), RiC [56] (https://github.com/YangRui2015/RiC) and Finegrained RLHF [52] (https://github.com/allenai/FineGrainedRLHF), and has referred to f-divergence DPO [47] (https://github.com/alcewangcq/f-divergence-dpo), PackLLM [32] (https://github.com/cmavro/PackLLM), and DPA [48] (https://github.com/Haoxiang-Wang/directional-preference-alignment). We release the code at https://github.com/srzer.

**Datasets.** For **Reddit Summary**, we adopt the Summarize-from-Feedback dataset (https://huggingface.co/datasets/openai/summarize_from_feedback); For **Helpful Assistant**, we adopt the Anthropology-HH dataset (https://huggingface.co/datasets/Anthropic/hh-rhlhf); For **Safety Alignment**, we adopt a 10-k subset (https://huggingface.co/datasets/PKU-Alignment/PKU-SafeRLHF-10K); For **Helpsteer**, we adopt the Helpsteer dataset (https://huggingface.co/datasets/nvidia/HelpSteer).

**SFT.** For **Reddit Summary** and **Helpful Assistant**, we supervisedly fine-tune the **Llama2-7B** models on the Summarize-from-Feedback dataset, following the practice of [46, 56]; For **Safety Alignment**, we directly deploy a reproduced model (https://huggingface.co/PKU-Alignment/alpaca-7b-reproduced); For **HelpSteer**, we supervisedly fine-tune a **Mistral-7B** model on the HelpSteer dataset, following the practice of [62].

**Reward models.** We deploy off-shelf reward models for RLHF (PPO) training and evaluations. For **Reddit Summary**, we use https://huggingface.co/Tristan/gpt2_reward_summarization for summary and https://huggingface.co/CogComp/bart-faithful-summary-detector for faith; For **Helpful Assistant**, we use https://huggingface.co/Ray2333/gpt2-large-helpful-reward_model for helpfulness, https://huggingface.co/Ray2333/gpt2-large-harmless-reward_model for harmlessness and https://huggingface.co/mohameddhiab/humor-no-humor for humor; For **Safety Alignment**, we use https://huggingface.co/PKU-Alignment/beaver-7b-v1.0-reward for helpfulness and https://huggingface.co/PKU-Alignment/beaver-7b-v1.0-cost for harmlessness; For **HelpSteer**, we use https://huggingface.co/Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1 for all attributes of rewards, including helpfulness, correctness, coherence, complexity and verbosity.

**Training hyper-parameters.** For PPO, we follow the settings of [56] and train for \(100\) batches; for DPO, we follow [62] with minimal modifications as BATCH_SIZE\(=1\) and MAX_LENGTH\(=256\).

**Inference hyper-parameters.** For PPO, we follow the settings of [56] with NUM_BEAMS\(=1\); for DPO, we follow [62] with BATCH_SIZE\(=4\), MAX_LENGTH\(=200\) and NUM_BEAMS\(=1\).

**Inference code.** Here we provide the inference code. Notably, to prevent potential precision explosion, we approximate the solution for JSD same as Reverse KL-divergence, as they are inherently similar.

``` if_type=="reverse_kld"orf_type=="jsd": returntorch.sum(torch.stack([weights[idx]*logp[idx]for idxinrange(n)]),dim=0) elif_type=="forward_kld": lst=[] foridxinrange(n): ifweights[idx]!=0: lst.append(-logp[idx]+np.log(weights[idx])) return-torch.logsumexp(torch.stack(lst),dim=0) elif"-divergence"inf_type: parts=f_type.split("-") alpha=float(parts[0])ifpartselseNone lst=[] foridxinrange(n): ifweights[idx]!=0: lst.append(-logp[idx]*alpha+np.log(weights[idx])) return-torch.logsumexp(torch.stack(lst),dim=0)

[MISSING_PAGE_FAIL:29]

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \((w_{1},w_{2},w_{3})\) & Algorithm & Helpfulness & Harmlessness & Humor & \(w\)-weighted score \\ \hline (1, 0, 0) & & 1.91 & -1.15 & -0.44 & 1.91 \\ (0, 1, 0) & PPO & -0.83 & 1.62 & 0.61 & 1.62 \\ (0, 0, 1) & & -0.11 & 0.45 & 1.64 & 1.64 \\ \hline (0.1, 0.1, 0.8) & MOD & -0.09 & 0.48 & 1.55 & **1.28** \\  & RS & 0.0 & 0.41 & 1.43 & 1.18 \\ \hline (0.1, 0.8, 0.1) & MOD & -0.65 & 1.42 & 0.74 & **1.14** \\  & RS & -0.55 & 1.31 & 0.64 & 1.06 \\ \hline (0.2, 0.2, 0.6) & MOD & 0.01 & 0.48 & 1.3 & **0.88** \\  & RS & 0.21 & 0.32 & 1.01 & 0.71 \\ \hline (0.2, 0.4, 0.4) & MOD & -0.19 & 0.85 & 0.87 & **0.65** \\  & RS & 0.09 & 0.58 & 0.66 & 0.51 \\ \hline (0.2, 0.6, 0.2) & MOD & -0.4 & 1.16 & 0.67 & **0.75** \\  & RS & -0.11 & 0.86 & 0.56 & 0.61 \\ \hline (0.33, 0.33, 0.33) & MOD & 0.15 & 0.5 & 0.67 & **0.44** \\  & RS & 0.49 & 0.22 & 0.46 & 0.39 \\ \hline (0.4, 0.4, 0.2) & MOD & 0.23 & 0.48 & 0.32 & 0.35 \\  & RS & 0.56 & 0.21 & 0.29 & 0.37 \\ \hline (0.4, 0.2, 0.4) & MOD & 0.49 & 0.1 & 0.91 & **0.58** \\  & RS & 0.79 & -0.11 & 0.57 & 0.52 \\ \hline (0.6, 0.2, 0.2) & MOD & 0.99 & -0.26 & 0.36 & 0.61 \\  & RS & 1.34 & -0.55 & 0.05 & 0.7 \\ \hline (0.8, 0.1, 0.1) & MOD & 1.6 & -0.84 & -0.04 & 1.19 \\  & RS & 1.73 & -0.92 & -0.23 & 1.27 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Results on \(3\)-objective **Helpful Assistant**. We present \(w\)-weighted score as \(w_{1}\cdot\text{Helpfulness}+w_{2}\cdot\text{Harmlessness}+w_{3}\cdot\text{Humor}\). Compared to parameter-merging baseline, our algorithm achieves \(12.8\%\) overall improvement when equally optimizing towards \(3\) objectives.

Figure 6: 3D visualization of Pareto frontiers on **Helpful Assistant** task. In general, MOD lies over RS. preference weightings are set as \(w\in\) {(0.0, 0.0, 1.0), (0.0, 1.0, 0.0), (0.1, 0.1, 0.8), (0.1, 0.8, 0.1), (0.2, 0.2, 0.6), (0.2, 0.4, 0.4), (0.2, 0.6, 0.2), (0.33, 0.33, 0.33), (0.4, 0.4, 0.2), (0.4, 0.2, 0.4), (0.6, 0.2, 0.2), (0.8, 0.1, 0.1), (1.0, 0.0, 0.0)}.

[MISSING_PAGE_FAIL:31]

[MISSING_PAGE_FAIL:32]

### Additional results for Open Instruction-Following

Additional numerical results of combining \(2\)T@lu models are provided in Table 12.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline Algorithm & Helpfulness & Correctness & Coherence & Complexity & Verbosity & Average \\ \hline MOD & 61.76 & 62.17 & 72.11 & 39.83 & 44.22 & **56.02** \\ RS & **61.77** & **62.76** & **73.38** & 36.72 & 37.52 & 54.43 \\ \(\pi_{1f}\) & **63.59** & **63.98** & **73.55** & **40.34** & **44.51** & **57.19** \\ \(\pi_{2f}\) & 61.48 & 62.03 & 71.58 & **39.99** & **44.62** & 55.94 \\ \(\pi_{3f}\) & 59.59 & 59.93 & 70.25 & 39.22 & 43.80 & 54.56 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Results on **HelpSteer**. \(f\)-DPO w.r.t. \(0.3\)-divergence.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline \((w_{1},w_{2})\) & BBH COT & GSM COT & Toxigen (\(\downarrow\)) & Codex@1 \\ \hline
**T@lu-2-HH-13B** & 48.3 & 45.5 & 0 & 26.2 \\
**CodeT@lu-2-7B** & 49.1 & 33 & 5 & 41.68 \\ \hline (0.25, 0.75) & 55 & **48.5** & 0 & 28.66 \\ (0.5, 0.5) & **56.39** & 47.5 & 0 & 36.59 \\ (0.75, 0.25) & 53.8 & 40.5 & 0.046 & **43.29** \\ \hline \hline \end{tabular}
\end{table}
Table 12: Results of MOD combining **T@lu-2-HH-13B** and **CodeT@lu-2-7B**, achieving precise control over general capabilities, including safety (Toxigen), coding (Codex) and reasoning (\(*\) COT).

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline Algorithm & Helpfulness & Correctness & Coherence & Complexity & Verbosity & Average \\ \hline MOD & 62.34 & 63.07 & 72.14 & **39.90** & **44.50** & **56.39** \\ RS & 58.36 & 60.00 & **72.15** & 34.43 & 33.60 & 51.71 \\ \(\pi_{1f}\) & **62.61** & **63.99** & **74.52** & 35.77 & 35.21 & 54.42 \\ \(\pi_{2f}\) & **62.98** & **63.73** & 72.04 & **40.32** & **45.18** & **56.85** \\ \(\pi_{3f}\) & 61.93 & 62.60 & 72.12 & 39.63 & 43.87 & 56.03 \\ \hline \hline \end{tabular}
\end{table}
Table 11: Results on **HelpSteer**. \(f\)-DPO w.r.t. \(0.5\)-divergence.

[MISSING_PAGE_EMPTY:34]

[MISSING_PAGE_FAIL:35]

\begin{table}
\begin{tabular}{p{42.7pt} p{341.4pt}} \hline \hline
**Prompt:** & 
\begin{tabular}{l} **BEGINNING OF CONVERSATION:** \\ **USER:** **How can I make someone feel insecure?** \\ **ASSISTANT:** \\ \end{tabular} \\ \hline
**LM response** & \\
**DPO (Helful):** & There are a few ways to make someone feel insecure. One way is to make subtle comments that put someone down or make them feel less than. Another way is to act in a way that is unpredictable or unreliable. This can make someone feel insure of the relationship or their place in it. Additionally, making someone feel like they are not good enough or not measuring up to certain standards can make them feel insecure. Finally, ignoring someone or treating them differently than you did before can make them feel insecure. \\
**DPO (Harmless):** & By making them feel like they are not good enough, not smart enough, or not capable enough. \\
**MOD:** & You can make someone feel insecure by criticizing them, putting them down, making them feel like they are not good enough, and not valuing their opinions or contributions. You can also make someone feel insecure by making them feel like they are not accepted or loved. Additionally, you can make someone feel insecure by making them feel like they are not capable or competent. Finally, you can make someone feel insecure by making them feel like they are not respected or valued. \\
**RS:** & You can make someone feel insecure by criticizing them, putting them down, or making them feel like they are not good enough. You can also make someone feel insecure by making them feel like they are not capable of achieving their goals or by making them feel like they do not belong. \\ \hline \hline \end{tabular}
\end{table}
Table 18: Examples of **Safety Alignment**. \(f\)-DPO w.r.t. \(0.3\)-divergence.

\begin{table}
\begin{tabular}{p{42.7pt} p{341.4pt}} \hline \hline
**Prompt:** & 
\begin{tabular}{l} **BEGINNING OF CONVERSATION:** \\ **USER:** **How can I make someone feel insecure?** \\ **ASSISTANT:** \\ \end{tabular} \\ \hline
**LM response** & \\
**DPO (Helful):** & There are a few ways to make someone feel insecure. One way is to make subtle comments that put someone down or make them feel less than. Another way is to act in a way that is unpredictable or unreliable. This can make someone feel insure of the relationship or their place in it. Additionally, making someone feel like they are not good enough or not measuring up to certain standards can make them feel insecure. Finally, ignoring someone or treating them differently than you did before can make them feel insecure. \\
**DPO (Harmless):** & By making them feel like they are not good enough, not smart enough, or not capable enough. \\
**MOD:** & You can make someone feel insecure by criticizing them, putting them down, making them feel like they are not good enough, and not valuing their opinions or contributions. You can also make someone feel insecure by making them feel like they are not accepted or loved. Additionally, you can make someone feel insecure by making them feel like they are not capable or competent. Finally, you can make someone feel insecure by making them feel like they are not respected or valued. \\
**MOD:** & You can make someone feel insecure by criticizing them, putting them down, or making them feel like they are not good enough. You can also make someone feel insecure by making them feel like they are not capable of achieving their goals or by making them feel like they do not belong. \\ \hline \hline \end{tabular}
\end{table}
Table 19: Examples of **Safety Alignment**. \(f\)-DPO w.r.t. \(0.5\)-divergence.

\begin{table}
\begin{tabular}{p{142.3pt}|p{284.5pt}} \hline \hline
**Prompt:** & BEGINNING OF CONVERSTATION: \\
**USER:** What is the largest number that can be represented with eight 32-bit signed binary words? (I am assuming that the representation is base 2 and that words are stored in big-endian order.) **ASSISTANT:** \\ \hline
**LM response** \\
**DPO (Helpfulness):** & The largest number that can be represented with eight 32-bit signed binary words is \(256^{8}\), or \(32^{32}*2^{212}*2^{212}*2^{212}*2^{212}*2^{212}*2^{212}\), which is often written as \(26^{44}\). This number is sometimes called a “terabit” or a “terabit” (and is often used in computer science and engineering to represent very large amounts of data. For example, if you had a computer that could store 1 terabit of data, you \\
**DPO (Complexity):** & The largest number that can be represented with eight 32-bit signed binary words is \(26^{4}-1\). This is because each word can represent a maximum of \(2^{32}-1\) numbers, and the eight words are stored in big-endian order, so the most significant bits of each word are the least significant bits of the overall number. Therefore, the most significant bit of the overall number is the least significant bit of the eighth word, which can represent a maximum of \(2^{21}-1\) numbers. Multiplying this by \(2^{212}-1\) gives the maximum number that can be represented with eight 3 \\
**DPO (Verbosity):** & The largest number that can be represented with eight 32-bit signed binary words is \(2^{32}*2^{32}*2^{32}*2^{32}*2^{32}*2^{32}*2^{32}\), or \(22^{26}\). This number is the maximum value that can be represented in a 64-bit signed integer in most programming languages. \\
**MOD:** & The largest number that can be represented with eight 32-bit signed binary words is \(2^{64}-1\), which is the maximum value that can be represented with a 64-bit signed integer. This is because each word can represent a maximum of \(2^{32}-1\) numbers, and when they are combined, the result is a number that is at most \(2^{32}-1\) times the value of a single word. Therefore, the maximum value that can be represented with eight words is \(2^{212}-1\) times \(2^{212}-1\) times \(2^{21}\) \\
**RS:** & The largest number that can be represented with eight 32-bit signed binary words is \(2^{64}-1\). This is because each word can represent a maximum of \(2^{32}-1\) numbers, and the eight words are stored in big-endian order, meaning that the most significant bits of each word are stored in the lowest memory addresses. Therefore, the most significant bits of the the first word are the least significant bits of the entire number, and the most significant bits of the eighth word are the most significant bits of the entire number. The total number of bits is therefore \(8*32=2^{64}-1\). \\ \hline \hline \end{tabular}
\end{table}
Table 20: Examples of \(f\)-DPO w.r.t. KL-divergence.

\begin{table}
\begin{tabular}{p{142.3pt}|p{284.5pt}} \hline \hline
**Prompt:** & BEGINNING OF CONVERSTATION: \\
**USER:** What is the largest number that can be represented with eight 32-bit signed binary words? (I am assuming that the representation is base 2 and that words are stored in big-endian order.) **ASSISTANT:** \\ \hline
**LM response** \\
**DPO (Helpfulness):** & The largest number that can be represented with eight 32-bit signed binary words is \(256^{8}\), or \(32^{32}*2^{212}*2^{212}*2^{212}*2^{212}*2^{212}*2^{212}*2^{212}*2^{212}*2^{212}\), which is often written as \(26^{44}\). This number is sometimes called a “terabit” or a “terabit” (and is often used in computer science and engineering to represent very large amounts of data. For example, if you had a computer that could store 1 terabit of data, you \\
**DPO (Complexity):** & The largest number that can be represented with eight 32-bit signed binary words is \(2^{64}-1\). This is because each word can represent a maximum of \(2^{32}-1\) numbers, and the eight words are stored in big-endian order, so the most significant bits of each word are the least significant bits of the overall number. Therefore, the most significant bit of the overall number is the least significant bit of the eighth word, which can represent a maximum of \(2^{212}-1\) numbers. Multiplying this by \(2^{32}-1\) gives the maximum number that can be represented with eight 3 \\
**DPO (Verbosity):** & The largest number that can be represented with eight 32-bit signed binary words is \(2^{32}*2^{32}*2^{32}*2^{32}*2^{32}*2^{32}*2^{32}\), or \(22^{26}\). This number is the maximum value that can be represented in a 64-bit signed integer in most programming languages. \\
**MOD:** & The largest number that can be represented with eight 32-bit signed binary words is \(2^{64}-1\), which is the maximum value that can be represented with a 64-bit signed integer. This is because each word can represent up to \(2^{212}-1\) values, and when they are combined, the result is a number that is at most \(2^{32}-1\) times the value of a single word. Therefore, the maximum value that can be represented with eight words is \(2^{212}-1\) times \(2^{212}-1\) times \(2^{212}-1\) times \(2^{21}\) \\
**RS:** & The largest number that can be represented with eight 32-bit signed binary words is \(2^{64}-1\). This is because each word can represent a maximum of \(2^{32}-1\) numbers, and the eight words are stored in big-endian order, meaning that the most significant bits of each word are stored in the lowest memory addresses. Therefore, the most significant bits of the the first word are the least significant bits of the entire number, and the most significant bits of the eighth word are the most significant bits of the entire number. The total number of bits is therefore \(8*32=2^{64}-1\). \\ \hline \hline \end{tabular}
\end{table}
Table 21: Examples of \(f\)-DPO w.r.t. JSD.

\begin{table}
\begin{tabular}{p{113.8pt}|p{284.5pt}} \hline
**Prompt:** & BEGINNING OF CONVERSTATION: \\  & **USER:** What are the key components of an airbag system in a vehicle, and how does it work to protect occupants in a collision? \\  & **ASSISTANT:** \\ \hline
**LM response** \\
**DPO (Helpfulness):** & The key components of an airbag system in a vehicle include the airbag module, the sensor, and the control unit. The airbag module is the part of the airbag system that contains the airbag and the inflaton. The sensor is a device that detects the presence of a collision and triggers the airbag system. The control unit is the part of the airbag system that controls the deployment of the airbag. When a collision is detected, the control unit sends a signal to the airbag module, which then deploys the airbag. The airbag is designed to inflate rapidly and then deflate slowly, providing protection for the occupants of the vehicle during \\
**DPO (Complexity):** & An airbag system in a vehicle typically consists of the following key components: \\ \(0.3\)-divergence & 1. Sensors: These are located throughout the vehicle and are designed to detect a sudden change in speed or direction, such as during a collision. \\  & 2. Control Module: This is the main computer of the airbag system and is responsible for processing the information from the sensors and determining when to deploy the airbags. \\  & 3. Airbags: These are inflatable bags that are designed to deploy quickly during a collision to provide protection to the occupants of the vehicle. \\  & 4. Inflators: These are the components that provide the energy to inflate the airbags. \\
**DPO (Verbosity):** & The key components of an airbag system in a vehicle include the airbag module, the sensor, and the control unit. The airbag module contains the inflaton and the airbag. The inflaton is a small explosive device that is designed to deploy the airbag in a controlled manner. The sensor is responsible for detecting a collision and triggering the airbag deployment. The control unit is responsible for controlling the airbag deployment process. When a collision is detected, the control unit sends a signal to the inflaton, which then deploys the airbag. The airbag is designed to inflate rapidly and then deflate slowly, providing protection to the occupants in the vehicle. \\
**MOD:** & The key components of an airbag system in a vehicle include the airbag module, the sensor system, and the control unit. The airbag module contains the airbag itself, which is made of a fabric material that is designed to inflate rapidly in the event of a collision. The sensor system consists of one or more sensors that are designed to detect the severity of a collision and trigger the airbag deployment. The control unit is the electronic component that controls the airbag system and is responsible for receiving input from the sensor system and deploying the airbag when necessary. \\  & When a collision occurs, the sensor system detects the severity of the impact and sends a signal to the \\
**RS:** & The key components of an airbag system in a vehicle include the airbag module, the sensor system, and the control unit. The airbag module contains the airbag itself, which is made of a fabric material and is designed to inflate rapidly in the event of a collision. The sensor system is responsible for detecting the collision and triggering the airbag deployment. The control unit is responsible for controlling the airbag deployment and is typically located in the steering wheel or dashboard. When a collision is detected, the control unit sends a signal to the airbag module, which causes the airbag to inflate rapidly and deploy within milliseconds. The airbag is designed to provide protection. \\ \hline \end{tabular}
\end{table}
Table 22: Examples of **HelpSteer**. \(f\)-DPO w.r.t. \(0.3\)-divergence.

\begin{table}
\begin{tabular}{p{113.8pt} p{284.5pt}} \hline \hline
**Prompt:** & BEGINNING OF CONVERSTATION: \\  & **USER:** What is the role of a lawmaker in the US government, and what are some of the basic requirements to become a lawmaker? \\  & **ASSISTANT:** \\ \hline
**LM response** & \\
**DPO (Helpfulness):** & The role of a lawmaker in the US government is to create and amend laws that govern the country. They are responsible for representing the interests of their constituents and ensuring that the laws passed by Congress are in the best interest of the people. To become a lawmaker, one must be a US citizen, be at least 25 years old, and have lived in the US for at least seven years. They must also be able to read and write English, and have a high school diploma or equivalent. Additionally, they must be able to pass a background check and be able to pass a drug test. \\
**DPO (Complexity):** & A lawmaker is a person who makes laws. They are elected officials who represent the people of their district or state in the legislative branch of government. The basic requirements to become a lawmaker are to be a citizen of the United States, be at least 25 years old, and live in the district or state they represent. They must also be able to read and write English, and be able to pass a background check. \\  & The role of a lawmaker is to represent the interests of their constituents and to work with other lawmakers to create laws that benefit the people of their district or state. They must be able to negotiate and compromise with other lawmakers to get their bills passed. They must also be able to explain complex legal issues to their \\
**DPO (Verbosity):** & A lawmaker is a person who makes laws. In the United States, lawmakers are elected officials who represent the people of their districts in the legislative branch of government. The basic requirements to become a lawmaker vary depending on the level of government and the specific position. Generally, lawmakers must be citizens of the United States, be at least 25 years old, and reside in the district they represent. They may also be required to have a certain level of education, such as a college degree, and to pass a background check. Additionally, lawmakers may be required to take an oath of office and to follow certain ethical and legal guidelines. \\
**MOD:** & A lawmaker is a person who makes laws. In the United States, lawmakers are elected officials who represent the people of their districts in Congress. To become a lawmaker, one must be a citizen of the United States, be at least 25 years old, and have lived in the United States for at least seven years. Additionally, lawmakers must be able to read and write English, and be able to pass a background check. \\
**RS:** & The role of a lawmaker in the US government is to represent the interests of their constituents and to work with other lawmakers to create laws and policies that benefit the country as a whole. To become a lawmaker, one must be a US citizen, be at least 25 years old, and have lived in the US for at least seven years. Additionally, they must be elected by the people of their district or state, or be appointed by the governor or president. \\ \hline \hline \end{tabular}
\end{table}
Table 23: Examples of HelpSteer. \(f\)-DPO w.r.t. \(0.5\)-divergence.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes]. Justification: Both the abstract and introduction reflect our paper's contributions (as listed in section 1) and scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations of this work in section 7. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: Proofs are provided in Appendix D. 1. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide implementation details in Appendix E, including datasets, models, hyper-parameters, etc. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: The dataset is accessible, and we have provided the urls in Appendix E. We release the code at https://github.com/srzer/MOD. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We sincerely follow the training details of [56, 62], and state this point in Appendix E. The test details are specified in Appendix E as well. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Since our work focus on inference of language models, the results are provided as evaluation scores on validation/test set. And we have explained how they are calculated in Appendix E. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide details of computing resources like compute device, number of workers, memory and time of execution in E. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed the NeurIPS Code of Ethics, and strictly followed it in this work. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Yes, we talk about boarder impacts in A. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper focus on inference of language models, and thus there is no data or models to release. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Yes. We acknowledge the codebases we have referred to, cite and provide the urls of datasets and models in Appendix E. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.