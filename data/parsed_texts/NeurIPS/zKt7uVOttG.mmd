Gaussian Process Predictions with Uncertain Inputs Enabled by Uncertainty-Tracking Processor Architectures

Janith Petangoda

Signaloid,

Cambridge, UK

&Chatura Samarakoon

University of Cambridge,

Cambridge, UK

&Phillip Stanley-Marbell

Signaloid / University of Cambridge,

Cambridge UK.

The work was mostly carried out at the University of Cambridge.

###### Abstract

Gaussian Processes (GPs) are theoretically-grounded models that capture both aleatoric and epistemic uncertainty, but, the well-known solutions of the GP predictive posterior distribution apply only for deterministic inputs. If the input is uncertain, closed-form solutions aren't generally available and approximation schemes such as moment-matching and Monte Carlo simulation must be used. Moment-matching is only available under restricted conditions on the input distribution and the GP prior and will miss the nuances of the predictive posterior distribution; Monte Carlo simulation can be computationally expensive. In this article, we present a _general_ method that uses a recently-developed processor architecture [1, 2] capable of performing arithmetic on distributions to implicitly calculate the predictive posterior distribution with uncertain inputs. We show that our method implemented to run on a commercially-available implementation [3] of an uncertainty-tracking processor architecture captures the nuances of the predictive posterior distribution while being \(\sim\)\(108.80\)x faster than Monte Carlo simulation.

## 1 Introduction

Gaussian Processes (GPs) are a class of non-parametric models that can intrinsically capture model uncertainty. The prediction of a GP for a unobserved input is referred to as the _predictive posterior distribution_ since it is a distribution over the predictive output conditioned on the observed data. For deterministic inputs, there is a closed-form solution for the predictive posterior distribution of a GP.

GPs can also capture the impact of _input uncertainty_ on the predicted output. With an uncertain input, the GP predictive posterior distribution is in general arbitrary and no general closed-form solution exists. A typical tactic for handling uncertain inputs is to carry out moment-matching, where the first \(k\) (usually \(k=2\)) moments of the true predictive posterior distribution are calculated [4, 5]. Closed-form solutions for even the first two moments are only available in special cases, and even then, such a Gaussian approximation loses key statistical properties of the true posterior distribution, such as multi-modality and skewness. A better approximation could be obtained by carrying out a (potentially) computationally-expensive Monte Carlo simulation.

In this article, we present an algorithm for implicitly computing the GP predictive posterior distribution with uncertain inputs using recent advances in uncertainty-tracking processor architectures (UTPAs) [1, 2]. Our method does not rely on Monte Carlo simulation but rather exploits the internal distributional representation of the UTPA. Our method is _agnostic to the distribution of the input and the GP prior_.

We evaluate the effectiveness of our method against the Monte Carlo method by comparing the accuracies of the solutions to the ground-truth (obtained by a Monte Carlo simulation with a large number of samples). We also compare the run time of each method. We show in Section 6 that running our new algorithm on a commercially-available implementation [3] of a UTPA [1; 2] can achieve orders of magnitude faster performance while achieving the same accuracy.

## 2 Gaussian Processes

A GP is a collection of random variables (Definition A.2 in Appendix A) where any _finite_ subcollection of those random variables is jointly Gaussian. It is a distribution over the _infinitely_-many random variables that satisfy this condition on any _finite_ subcollection of them. We can define a GP implicitly by specifying a mechanism for deriving the mean vector and covariance matrix of the multivariate Gaussian distribution that distributes any given subset of random variables. We can do so using a mean function \(m\) and covariance kernel \(k\) to derive the mean vector and covariance matrix of the multivariate Gaussian distribution using the _indexes_ of the random variables.

**Definition 2.1** (Implicit definition of a Gaussian Process).: Let \(X\) be an index set. Then, for a finite subset of \(X\), \(\{x_{1},...,x_{n}\}\in X\), let the random variables \(Y_{x_{1}},...,Y_{x_{n}}\) be jointly distributed as a multivariate Gaussian, \(\mathcal{N}(\mu,\boldsymbol{\Sigma})\), where the elements \(\mu_{i}\) of the mean vector \(\mu\) and \(\Sigma_{ij}\) of the covariance matrix \(\boldsymbol{\Sigma}\) are implicitly derived using

\[\mu_{i}=m(x_{i}),\]

and

\[\boldsymbol{\Sigma}_{ij}=k(x_{i},x_{j}).\]

The function \(m:X\to\mathbb{R}\) is called the _mean function_ and the function \(k:X\times X\to\mathbb{R}\) is the _covariance kernel_. We denote an implicitly-defined GP by \(\mathcal{GP}(m,k)\).

We denoted the index set with \(X\) and the random variable with \(Y_{x_{i}},\) for \(x_{i}\in X\) to highlight that a GP is a distribution over functions where \(X\) is the input space and \(Y=Y_{x_{i}\in X}\) is the output space. Thus the sample space of each \(Y_{x_{i}}\) is \(Y\). Let \(X=\mathbb{R}\) and \(Y=\mathbb{R}\) be an input and output space respectively and \(\mathcal{F}\) denote a space of functions, where \((f\in\mathcal{F}):X\to Y\) denotes a possible function from \(X\) to \(Y\). We can place a GP prior over \(\mathcal{F}\) by writing \(\mathcal{F}\sim\mathcal{GP}(m,k)\). In this view, a sample from a GP is a sample from all the random variables \(Y_{x}=f(x),\forall x\in X\).

The function space \(\mathcal{F}\) over which a GP is defined is implicitly described by the chosen mean function and the covariance kernel; we have placed a _GP prior over \(\mathcal{F}\)_. Popular kernels include the squared exponential (this is a basic 'all-rounder' kernel) and the Matern class of kernels [6]. The squared exponential kernel is given by

\[k_{c,\lambda}^{sq}(x,x^{\prime})=c\exp(\frac{-(x-x^{\prime})^{T}(x-x^{\prime })}{\lambda^{2}}),\] (1)

where \(c\in\mathbb{R}^{+}\) and \(\lambda\in\mathbb{R}\) are hyperparameters of the kernel. The parameter \(c\) is the signal variance and the parameter \(\lambda\) is the characteristic length scale2. The squared exponential kernel describes a function space of smooth functions that have a variability that is described by the signal variance and the length scale; the signal variable loosely specifies how much the function can vary from the mean function and the length scale specifies how quickly the function can vary from the mean function.

Footnote 2: It is possible to write the squared exponential kernel in terms of a specific length scale for each dimension of the input space. For simplicity, will be using a single, common length scale for all dimensions in this article.

### GP Prediction with Deterministic Inputs

Let \(X=\mathbb{R}^{d}\) denote an input space, where \(d\) denotes the dimension of \(X\), and let \(Y=\mathbb{R}\) denote an output space. Let \(f:X\to Y\) be an arbitrary function from \(X\) to \(Y\) from a function space \(\mathcal{F}\). We define a GP prior over \(\mathcal{F}\) as \(\mathcal{F}\sim\mathcal{GP}(m,k)\). The mean function \(m\) and covariance kernel \(k\) implicitly define the function space \(\mathcal{F}\). Further, let the likelihood function be \(\tilde{y}=f(x)+\epsilon\), where \(\epsilon\in\mathcal{N}(0,\sigma_{\epsilon}^{2})\) and \(\tilde{y}\in(\tilde{Y}=Y)\) denotes a noisy observation. Finally, let \(\textbf{X}\in\mathbb{R}^{k\times d}\) be a matrix of \(k\) input points from \(X\), and let \(\textbf{Y}\in\mathbb{R}^{k\times 1}\) be a matrix of \(k\) noisy output points from \(\tilde{Y}\).

We are interested in the posterior GP over \(\mathcal{F}\) given the data \((\textbf{X},\textbf{Y})\). However, rather than finding an explicit conditional distribution over \(\mathcal{F}\), we express the distribution of the posterior GP evaluated at a particular point \(x^{*}\in X\), which, by Definition 2.1 is a Gaussian random variable. Therefore, for an arbitrary point \(x^{*}\in X\), the predictive posterior density of the random variable \(Y_{x^{*}}=f(x^{*})\) is

\[Y_{x^{*}}=f(x^{*})\sim p_{f(x^{*})}(y^{*}|\mathbf{X},\mathbf{Y})=\mathcal{N}( \mu_{*}(x^{*}),\sigma_{*}^{2}(x^{*})),\] (2)

where,

\[\begin{split}\mu_{*}:X&\to\mathbb{R}\\ x^{*}&\mapsto\mu_{*}(x^{*})=m(x^{*})+k(x^{*}, \mathbf{X})(\mathbf{K}+\sigma_{\epsilon}^{2}\mathbf{I})^{-1}(\mathbf{Y}-m(x^{ *})),\end{split}\] (3)

\[\begin{split}\sigma_{*}^{2}:X&\to\mathbb{R}\\ x^{*}&\mapsto\sigma_{*}^{2}(x^{*})=k(x^{*},x^{*})-k( x^{*},\mathbf{X})(\mathbf{K}+\sigma_{\epsilon}^{2}\mathbf{I})^{-1}k(x^{*}, \mathbf{X})^{T}.\end{split}\]

For a deterministic input, GP prediction can be thought of as a function from the input to a mean and a variance, as Equation 3 shows, that defines the predictive posterior distribution.

## 3 GP Prediction with Uncertain Inputs

A useful enhancement to GP prediction with deterministic input is to consider a distributional input. Let the input to the GP now be a random variable \(X^{*}\), the sample space of which is \(X\). Thus, \(X^{*}\sim p_{X^{*}}(\,\cdot\,)\). We are still interested in the predictive distribution of \(Y_{x^{*}}\sim p_{f(X^{*})}(y|\mathbf{X},\mathbf{Y})\), where \(x^{*}\in X^{*}\). However, we must now marginalize over \(X^{*}\) because it is a random variable:

\[p_{f(X^{*})}(f(x^{*})=y^{*}|\mathbf{X},\mathbf{Y})=\int_{X}p_{f(x^{*})}(y^{*}| \mathbf{X},\mathbf{Y},x^{*})p_{X}(x^{*})\mathrm{d}x^{*}.\] (4)

In general, this integral is intractable. Therefore, we must usually resort to approximation methods. We discuss two approximation methods used in the literature below. In Section 4, we introduce a method for implicitly computing the predictive posterior distribution with uncertain input using a state-of-the-art UTPA [1; 2].

### Moment-matching approximation

We can approximate the intractable predictive posterior distribution as a Gaussian distribution. This is done by defining a Gaussian distribution using the first and second moments of the predictive posterior distribution [4; 5]. This mean and variance can be computed analytically when using the squared exponential kernel and the input is a Gaussian random variable [4; 5]. For more complicated input distributions and GP priors, it is difficult to find closed-form solutions. Furthermore, a Gaussian approximation to a non-Gaussian predictive posterior distribution would lack important statistical information such as multi-modality and skewness.

### Monte Carlo simulation

The Monte Carlo method can be used to obtain a more informative approximation of \(p_{f(X^{*})}(y^{*}|\mathbf{X},\mathbf{Y})\). Monte Carlo simulation [7; 8; 9] applies the Monte Carlo method to approximate the probability density function of a transformed random variable. That is, if we want the probability density function of \(Y=f(X)\), then we first take \(n\) samples of \(x_{i}\sim X\), transform each sample by applying \(f\) to obtain \(n\) samples \(y_{i}=f(x_{i})\) of \(Y\), and create a density estimation (such as a histogram) using the transformed samples. Better approximations are obtained by increasing the number of samples \(n\).

Girard _et al._[5] suggests estimating the integral in Equation 4 using Monte Carlo integration, where,

\[p_{f(X^{*})}(y^{*}|\mathbf{X},\mathbf{Y})=\int p_{f(x^{*})}(y^{*}|\mathbf{X}, \mathbf{Y},x^{*})p_{X}(x^{*})\,\mathrm{d}x^{*}\approx\frac{1}{n}\sum_{i=1}^{n} p_{f(x^{*})}(y^{*}|\mathbf{X},\mathbf{Y},x_{i}^{*})\,\mathrm{d}x^{*}.\] (5)

For this method, we would need to evaluate the sum in Equation 5 for each value that \(f(x^{*})\) can take. Instead, when using Monte Carlo simulation, we will obtain samples of \(f(x^{*})\) under \(p_{f(x^{*})}(y^{*}|\mathbf{X},\mathbf{Y})\) by taking samples of \(x_{i}^{*}\sim X^{*}\), calculating the mean \(\mu_{*}(x_{i}^{*})\) and variance \(\sigma_{*}^{2}(x_{i}^{*})\) according to Equation 3 for \(p_{f(X^{*})}(y^{*}|\mathbf{X},\mathbf{Y},x^{*})\), and taking samples \(y_{i}^{*}\sim\mathcal{N}(\mu_{*}(x_{i}^{*})\), \(\sigma_{*}^{2}(x_{i}^{*}))\).

The Monte Carlo method can be used to obtain an arbitrarily-good approximation of \(p_{f(X^{*})}(y^{*}|\mathbf{X},\mathbf{Y})\). However, doing so can be prohibitively expensive due to its slow convergence rate [9].

Gaussian Process Prediction with Uncertain Inputs on an

Uncertainty-Tracking Processor Architecture

To overcome the accuracy limitations of moment-matching and the computational costs of Monte Carlo simulations, we present a method that is enabled by recent advances in UTPAs [1; 2]. A UTPA is a computer microarchitecture that can represent distributional information in its microarchitectural state and track how these distributions evolve under arithmetic operations transparently to the applications running on it. A UTPA does this by providing an in-processor representation of probability distributions and a suite of arithmetic operations that can manipulate this representation. A UTPA carries out _deterministic computations on probability distributions_ and does not rely on any sampling methods. Therefore, unlike Monte Carlo methods that require convergence to a good solution by increasing the number of Monte Carlo iterations, a UTPA is _convergence-oblivious_ up to the fidelity of its representation.

The UTPA presented by Tsoutsouras _et al._[1; 2] has an associated _representation size_, \(r\), that describes the _precision_ of the representation. Larger representation sizes result in more accurate representations. A useful analogy is the IEEE-754 standard [10; 11] that approximately represents the infinite set of real numbers as floating point numbers on computers that can only handle a finite set of values.

A practical way of thinking of what a UTPA does is that it carries out uncertainty propagation. If we have a random variable \(X\) represented in the UTPA's distribution representation and a function \(f:X\to Y\), then a UTPA directly computes the distribution representation of \(Y=f(X)\) by directly computing it through the function \(f\) without resorting to Monte Carlo sampling.

### Gaussian Process Prediction with Uncertain Inputs as a Transformation of Random Variables

Since a UTPA can represent any distribution for the input random variable \(X\), our goal is to use a UTPA to directly compute the predictive posterior distribution of a GP with an uncertain input. To do this, we first note that a GP is practically two functions that map some input \(x^{*}\) to the mean \(\mu_{*}(x^{*})\) and variance \(\sigma_{*}^{2}(x^{*})\) of the output distribution at \(x^{*}\) (as given by Equation 3), but _not to the output random variable itself_. Simply plugging a UTPA-represented distribution to Equation 3 on a UTPA would result in distributions over the mean and variance and not a distribution of the output random variable that we desire.

We can express a transformation of random variables that results in a random variable that has the desired predictive posterior distribution \(p_{f(X^{*})}(y^{*}|\mathbf{X},\mathbf{Y})\) by using the "reparametrization trick" [12],

\[f(X^{*})=\mu_{*}(X^{*})+\sigma_{*}(X^{*})\epsilon,\] (6)

where \(\epsilon\sim\mathcal{N}(0,1)\), and \(\mu(x^{*})\) and \(\sigma_{*}(x^{*})\) are the mean and standard deviation of the GP posterior at \(x^{*}\) respectively from Equation 3. Theorem 4.1 shows that \(f(X^{*})\) has the desired distribution.

**Theorem 4.1**.: Let \(\mu_{*}(X^{*})\) and \(\sigma_{*}^{2}(X^{*})\) denote the application of the mean and variance functions of the Gaussian Process predictive posterior distribution (as in Equation 3) to a random variable \(X^{*}\sim p_{X}(\,\cdot\,)\) given some input and output data \(\mathbf{X}\) and \(\mathbf{Y}\) respectively, and let \(\epsilon\sim\mathcal{N}(0,1)\). Then, the probability density function of the random variable defined as

\[f(X^{*}|\mathbf{X},\mathbf{Y})=\mu_{*}(X^{*})+\sigma_{*}(X^{*})\epsilon\]

is given by

\[p_{f(X^{*})}(y^{*}|\mathbf{X},\mathbf{Y})=\int p_{f(x^{*})}(y^{*}|\mathbf{X},\mathbf{Y},x^{*})p_{X}(x^{*})\mathrm{d}x^{*}=\int\mathcal{N}\left(\mu_{*}(x^ {*}),\sigma_{*}^{2}(x^{*})\right)p_{X}(x^{*})\mathrm{d}x^{*},\]

where \(p_{f(x^{*})}(y^{*}|\mathbf{X},\mathbf{Y},x^{*})\) is the conditional probability density function of \(f(x^{*})\) given \(\mathbf{X}\), \(\mathbf{Y}\), and a particular value of \(x^{*}\in X^{*}\).

Proof.: See Appendix B. 

Implementing the Transformation of Random Variables on an Uncertainty-Tracking Processor Architecture is Trivial

Using Algorithm 1, we can easily find the GP predictive posterior with an uncertain input using a UTPA. Here, \(X^{*}\), \(\epsilon\), and \(y\) are distributional variables that are represented in the UTPA.

## 5 Methods

We compare the performance of Monte Carlo simulation and Algorithm 1 for computing the predictive posterior distribution. We measure the timing performance in terms of the wall-clock run time of each method and the accuracy in terms of the Wasserstein distance [13] of the result of each method compared to a ground-truth predictive posterior distribution obtained by a large (\(n=1,000,000\)) Monte Carlo simulation.

We carry out experiments using the two methods across different number of iterations \(n\) (of the Monte Carlo simulation) and representation sizes \(r\) (of the UTPA) to compare the trade-offs between converging to the output distribution and the extra time required to do so. For each configuration of \(n\) or \(r\), we carry out 30 repetitions to account for variation in sampling3.

Footnote 3: We calculate the Wasserstein distance for the result of Algorithm 1 by generating 1,000,000 samples from the UTPA representation. Therefore we repeat the experiments of Algorithm 1 on a UTPA 30 times for each representation size despite Algorithm 1 being deterministic and convergence-oblivious.

We set the mean function of the GP prior to \(m(x)=0\). We set \(X\) and \(Y\) to be the real space \(\mathbb{R}\), and generate a dataset \((\mathbf{X},\mathbf{Y})\) of \(k=10\) data points by setting \(x_{0}=-3\pi\), \(x_{i}=x_{i-1}+0.6\pi\)4, and \(y_{i}\) as,

Footnote 4: In other words, we have equally divided the range \([-3\pi,3\pi]\) to \(n=10\) points.

\[y_{i}(x_{i})=\sin(2x_{i})\cos(x_{i})\left(e^{-\frac{1}{\delta}x}+e^{\frac{1}{ \delta}x}\right)+\epsilon_{i},\] (7)

where each \(\epsilon_{i}\sim\mathcal{N}(0,0.01)\)5. For each case, we use the squared exponential kernel \(k_{c=1,\lambda=\sqrt{1.7}}^{sq}\) and we set the uncertain input distribution to \(X^{*}\sim\mathcal{N}(0,4)\) (see Figure 2 in Appendix C).

Footnote 5: We set the random seed to \(42\).

For the Monte Carlo method, we evaluate on \(n\in\{4,256,1152,2048,4096,8192,16000,32000,\)\(64000,128000,512000\}\). For Algorithm 1, we evaluate on \(r\in\{32,64,128,256,512,2048\}\). For the UTPA for Algorithm 1, we use a commercial implementation of a multi-dimensional variation of the UTPA presented by Tsoutsouras _et al._[1; 2] that automatically tracks correlations _between arbitrary random variables_.

We implement both Algorithm 1 and the Monte Carlo simulation using the C programming language and a custom tensor library implemented in C6. The commercial implementation of the UTPA [3] is implemented as a single-threaded emulated processor running on a 3rd Generation Intel Xeon system. As a result, Algorithm 1 is at an inherent disadvantage since it is benefiting only from the algorithmic advances of the distribution representation and its associated arithmetic and is not taking advantage of hardware acceleration; an FPGA or a custom silicon implementation of the underlying UTPA could provide even greater speedups than we present in this paper. We perform the Monte Carlo experiments on the same hardware as the hardware we used to emulate the UTPA. We note that we did not exploit parallelization for the Monte Carlo method (e.g., by using a GPU) since the commercial implementation of the UTPA [3] does not exploit parallelization either. See Appendix C for more detail on our method.

Footnote 6: Code is available on GitHub: https://github.com/physical-computation/uncertain-gaussia-process-code

## 6 Results

Figure 1 summarizes our results; Table 2 in the Appendix shows numerical results for the other configurations. Figure 0(a) shows that the results from Algorithm 1 are almost always at the Paretofrontier. Therefore, for any required accuracy except for the highest, in the ranges that we had tested, Algorithm 1 is at least an order of magnitude faster. Table 1 shows that for \(r=128\), the Monte Carlo method with \(n=128000\) shows a nearly identical Wasserstein distance (albeit with more than twice the standard deviation) while taking approximately \(108.80\times\) longer to compute.

Furthermore, Algorithm 1 does not suffer from large variances in the accuracy since the UTPA carries out _deterministic computation on probability distributions_. We see this from the constant variance shown by the results from Algorithm 1. This uncertainty is due to the fact that we calculated the Wasserstein distance using finite samples from the output distribution. It is possible to calculate the Wasserstein distance directly from the UTPA's representation, but we did not do so at this time. After \(r=128\), the accuracy of Algorithm 1 stagnates and the trade-off with run time increases. Therefore, for this particular application, if larger accuracy is required, the Monte Carlo method must be used.

Figures 0(b) and 0(c) show that the posterior distribution is more complex than a Gaussian. A moment-matched Gaussian solution would, for example, miss that the posterior distribution is multi-modal.

## 7 Conclusions

Existing methods for approximating the often-intractable GP predictive posterior distribution with uncertain inputs can be inaccurate and constrained in their use (moment-matching), or computationally expensive (Monte Carlo simulation). We present a method for computing the desired predictive posterior distribution on a UTPA [1, 2] by making use of the reparametrization trick. Our method can be used on _any input distribution and GP prior_ unlike moment-matching and we show experimentally that our method can be _as accurate as and orders of magnitude faster than Monte Carlo simulation_.

\begin{table}
\begin{tabular}{l l r r r} \hline \hline Problem & Core & \begin{tabular}{c} Representation Size / \\ Number of samples \\ \end{tabular} & \begin{tabular}{c} Wasserstein Distance \\ (mean \(\pm\) std. dev.) \\ \end{tabular} & 
\begin{tabular}{c} Run time (ms) \\ (mean \(\pm\) std. dev.) \\ \end{tabular} \\ \hline Squared Exponential Kernel & Algorithm 1 & 128 & \(0.00290\pm 0.00050\) & \(3.182\pm 0.376\) \\ Squared Exponential Kernel & Traditional Monte Carlo & 128000 & \(0.00324\pm 0.00129\) & \(346.186\pm 1.800\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Selection of results showing the Wasserstein distance and the run time required by the best overall configuration for Algorithm 1 and the close-to-equivalent Monte Carlo configuration. To obtain a similar accuracy, the Monte Carlo method took \(\sim\)\(108.80\times\) more time than Algorithm 1. See Table 2 for complete results.

Figure 1: Summaries of our key results. Subfigure (a) is a Pareto plot between the mean run time and the mean Wasserstein distance from the ground-truth output distribution. The error bars are \(\pm 1\) standard deviation. From our experiments, Algorithm 1 is almost always on the Pareto frontier, even after accounting for uncertainty. Subfigures (b) and (c) are histograms from Algorithm 1 and the Monte Carlo method respectively. The ground-truth distribution is in black, behind each histogram.

Acknowledgements

This work was supported by UKRI Materials Made Smarter Research Centre (EPSRC grant EP/V061798/1).

## References

* [1] V. Tsoutsouras, O. Kaparounakis, B. Bilgin, C. Samarakoon, J. Meech, J. Heck, and P. Stanley-Marbell, "The laplace microarchitecture for tracking data uncertainty and its implementation in a risc-v processor," in _MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture_, pp. 1254-1269, 2021.
* [2] V. Tsoutsouras, O. Kaparounakis, C. Samarakoon, B. Bilgin, J. Meech, J. Heck, and P. Stanley-Marbell, "The laplace microarchitecture for tracking data uncertainty," _IEEE Micro_, vol. 42, no. 4, pp. 78-86, 2022.
* [3] "Signaloid cloud developer platform." signaloid.io, 2023.
* [4] M. Deisenroth and C. E. Rasmussen, "Pilco: A model-based and data-efficient approach to policy search," in _Proceedings of the 28th International Conference on machine learning (ICML-11)_, pp. 465-472, Citeseer, 2011.
* [5] A. Girard, C. Rasmussen, J. Q. Candela, and R. Murray-Smith, "Gaussian process priors with uncertain inputs application to multiple-step ahead time series forecasting," _Advances in neural information processing systems_, vol. 15, 2002.
* [6] C. K. Williams and C. E. Rasmussen, _Gaussian processes for machine learning_, vol. 2. MIT press Cambridge, MA, 2006.
* [7] R. L. Harrison, "Introduction to monte carlo simulation," in _AIP conference proceedings_, vol. 1204, pp. 17-21, American Institute of Physics, 2010.
* [8] C. M. Bishop and N. M. Nasrabadi, _Pattern recognition and machine learning_, vol. 4. Springer, 2006.
* [9] S. Weinzierl, "Introduction to monte carlo methods," _arXiv preprint hep-ph/0006269_, 2000.
* [10] "IEEE standard for floating-point arithmetic," _IEEE Std 754-2019 (Revision of IEEE 754-2008)_, pp. 1-84, 2019.
* [11] D. Goldberg, "What every computer scientist should know about floating-point arithmetic," _ACM computing surveys (CSUR)_, vol. 23, no. 1, pp. 5-48, 1991.
* [12] D. P. Kingma and M. Welling, "Auto-encoding variational bayes," _arXiv preprint arXiv:1312.6114_, 2013.
* [13] L. V. Kantorovich, "Mathematical methods of organizing and planning production," _Management science_, vol. 6, no. 4, pp. 366-422, 1960.
* [14] B. Presnell, "A geometric derivation of the cantor distribution," _The American Statistician_, vol. 76, no. 1, pp. 73-77, 2022.
* [15] V. I. Bogachev and M. A. S. Ruas, _Measure theory_, vol. 1. Springer, 2007.
* [16] M. Galassi, J. Davies, J. Theiler, B. Gough, G. Jungman, P. Alken, M. Booth, F. Rossi, and R. Ulerich, _GNU scientific library_. Network Theory Limited Godalming, third ed., 2009.
* [17] S. Loosemore, R. Stallman, R. McGrath, A. Oram, and U. Drepper, "The gnu c library reference manual: for version 2.38," _Free Software Foundation_, 2022.
* [18] P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau, E. Burovski, P. Peterson, W. Weckesser, J. Bright, S. J. van der Walt, M. Brett, J. Wilson, K. J. Millman, N. Mayorov, A. R. J. Nelson, E. Jones, R. Kern, E. Larson, C. J. Carey, I. Polat, Y. Feng, E. W. Moore, J. VanderPlas, D. Laxalde, J. Perktold, R. Cimrman, I. Henriksen, E. A. Quintero, C. R. Harris, A. M. Archibald, A. H. Ribeiro, F. Pedregosa, P. van Mulbregt, and SciPy 1.0 Contributors, "SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python," _Nature Methods_, vol. 17, pp. 261-272, 2020.

## Appendix A Mathematical Preliminaries

In this section, we define some key mathematical details.

### Random Variables

**Definition A.1** (Probability density function).: Let \(X\) be a set called the sample space and \(p_{X}\) be a map from \(X\) to \(\mathbb{R}^{+}\),

\[p_{X}:X\to\mathbb{R}^{+},\]

that satisfies:

\[\int_{X}p_{X}(x)\,\mathrm{d}x=1.\]

We define \(p_{X}\) to be the _probability density function_ on \(X\).

**Definition A.2** (Random Variable).: Let \(X\) be a set and \(p_{X}\) a probability density function on \(X\). We define the tuple \((X,p_{X})\) as a random variable7.

Footnote 7: There can be measure theoretic random variables, such as the random variable that has the Cantor distribution, that do not admit probability density functions [14]. In this article, we do not consider such exotic random variables.

In the rest of this article, we suppress the notation \((X,p_{X})\) and simply write \(X\) to denote a random variable. We use the simple definition of a random variable above rather than the typical measure-theoretic definition to capture what we think of as a random variable in practice; we often think of a random variable in terms of its probability density function (for example, when we think of a Gaussian random variable, we often think of the bell curve drawn by the Gaussian probability density function).

Often, the set \(X=\mathbb{R}^{d}\), where \(\mathbb{R}^{d}\) is the \(d\)-dimensional real space. In the integration above (and below), we have suppressed the fact that it contains multiple integrals over each dimension of \(X\).

The probability distribution of a random variable is a set function \(\mathbb{P}_{X}:\Omega\to\mathbb{R}^{+}\) that tells us about the probability of the random variable taking on the values inside the given set, where \(\Omega\) is a set of subsets of \(X\). \(\Omega\) must technically be a \(\sigma\)-algebra, which for our purposes, can be thought of as a set of sets each of which can be expressed as a countable union of disjoint sets that also belong to \(\Omega\)8. Since we are only considering random variables that contain probability density functions, we define the distribution of a random variable as:

Footnote 8: The sets in a \(\sigma\)-algebra must also satisfy that countable unions and intersections also belong to \(\Omega\).

**Definition A.3** (Distribution of a Random Variable).: Given a random variable \(X\) with probability density function \(p_{X}\), the distribution of \(X\) is given by the set function

\[\mathbb{P}_{X}:\Omega\to\mathbb{R}^{+}\] (8) \[\omega=\bigcup_{i}U_{i}\mapsto\mathbb{P}_{X}(\omega)=\sum_{i} \int_{U_{i}}p_{X}(x)\,\mathrm{d}x,\]

where \(U_{i}\) is a collection of disjoint sets whose union equals the input set \(\omega\).

**Definition A.4** (Cumulative Distribution Function of a Random Variable).: The cumulative distribution function \(F_{X}\) of the random variable \(X\) with probability density function \(p_{X}\) is given by

\[F_{X}:X \to[0,1]\] (9) \[x \mapsto F_{X}(x)=\mathbb{P}(\{t|t\in X\;\mathrm{and}\;t<x\})=\int _{\{t|t\in X\;\mathrm{and}\;t<x\}}p_{X}(t)\,\mathrm{d}t,\]

Note that in the integrations above, we have suppressed the fact that the integrations need to occur over dimentions of \(X\). By applying the Fundamental Theorem of Calculus to each such integral, we obtain that \(p_{X}(x)=\frac{\partial^{d}F_{X}}{\partial x_{1}\dots\partial x_{d}}(x)\) This also gives us a way of going from a distribution of a randomvariable to its probability density function. Therefore, in this article, we often refer to the distribution and the probability density function interchangeably.

An example of a high dimensional random variable is the multivariate Gaussian random variable:

**Definition A.5** (Multivariate Gaussian random variable).: Let \(\bm{\mu}\in\mathbb{R}^{d}\) and \(\bm{\Sigma}\in\mathbb{R}^{d\times d}\) be a mean vector and a covariance matrix respectively. The multivariate Gaussian random variable \((X^{\mathcal{N}},p_{X}^{\mathcal{N}})\) is defined as:

\[X^{\mathcal{N}}=\mathbb{R}^{d},\]

\[p_{X}^{\mathcal{N}}:\mathbb{R}^{d} \to\mathbb{R}^{+},\] \[x \mapsto\frac{1}{\sqrt{(2\pi)^{d}\det\bm{\Sigma}}}\exp\left(-\frac {1}{2}(x-\bm{\mu})^{T}\bm{\Sigma}^{-1}(x-\bm{\mu})\right).\]

\(p_{X}^{\mathcal{N}}\) is called the multivariate Gaussian distribution. A Gaussian distribution is fully specifed by its mean vector \(\mu\) and its covariance matrix \(\bm{\Sigma}\). Therefore, we can also write that a random variable \(X^{\mathcal{N}}\) is distributed as a multivariate Gaussian by writing

\[X^{\mathcal{N}}\sim\mathcal{N}(\bm{\mu},\bm{\Sigma}).\]

The mean vector \(\bm{\mu}\) is the expectation of the random variable \(X^{\mathcal{N}}\), which is defined for any random variable \(X\) as,

**Definition A.6** (Expectation of a Random Variable).: Let \((X,p_{X})\) be a random variable. The expectation of \(X\) is defined as:

\[\mathbb{E}[X]=\int_{X}xp_{X}(x)\;\text{d}x.\]

An alternative view of a multivariate Gaussian random variable, or any other random variable over \(\mathbb{R}^{d}\), is that it is a collection of univariate (single dimensional) random variables that are jointly distributed as the multivariate distribution. The covariance matrix \(\bm{\Sigma}\) is a matrix the elements \(\Sigma_{ij}=\operatorname{cov}[X_{i},X_{j}]\), where \(X_{i}\) and \(X_{j}\) are the \(i\)-th and \(j\)-th univariate random variables of \(X\) respectively. Generally, the covariance of two random variables is defined as:

**Definition A.7** (Covariance of Random Variables).: Let \((X,p_{X})\) and \((Y,p_{Y})\) be two random variables. The covariance of \(X\) and \(Y\) is defined as:

\[\operatorname{cov}[X,Y]=\mathbb{E}[(X-\mathbb{E}[X])(Y-\mathbb{E}[Y])].\]

We note that the covariance between the same random variable is its variance:

\[\operatorname{cov}[X,X]=\mathbb{E}[(X-\mathbb{E}[X])^{2}]=\operatorname{var}[X].\]

In this article, we write \(\operatorname{cov}[X]\) to denote the variance of \(X\).

Give a collection of random variables \(\{X_{i}\}_{i=1,\dots,n}\), we can construct a covariance matrix \(\bm{\Sigma}\):

**Definition A.8** (Covariance Matrix).: Let \(X\) be an index set and \(\{Y_{i}\}x=1\)9 be a collection of random variables indexed by \(x\in X\). Then, the covariance matrix \(\bm{\Sigma}\in\mathbb{R}^{n\times n}\) of the collection of random variables that contains the pairwise covariances of the random variables:

Footnote 9: \(f\) transforms the set \(X\) such that there exists a valid probability density function \(p_{Y}\) over the set \(Y\).

\[\Sigma_{ij}=\operatorname{cov}[Y_{i},Y_{j}].\]

### Change of variables

Let \(f:X\to Y\) denote a _transformation_ from a random variable \(X\) to a random variable \(Y\) (i.e., \(Y=f(X)\))10. We can also apply \(f\) to an instance value \(x\) of \(X\) to obtain an instance value \(y=f(x)\) of \(Y\).

Footnote 10: \(f\) transforms the set \(X\) such that there exists a valid probability density function \(p_{Y}\) over the set \(Y\).

If \(f\) is invertible and once-differentiable, then Theorem A.1 derives the probability density function of \(Y\), denoted as \(p_{Y}\)[15, Chapter 3.7].

**Theorem A.1** (Change of variables).: Given a random variable \(X\) with a probability density function \(p_{X}\) and an invertible and once-differentiable transformation \(f:X\to Y\), the probability density function \(p_{Y}\) of the random variable \(Y=f(X)\) is given by:

\[p_{Y}:Y \to\mathbb{R}^{+},\] \[y \mapsto p_{Y}(y) =p_{X}\circ f^{-1}(y)|\det\nabla f\left(f^{-1}(y)\right)|^{-1}\] \[=p_{X}\circ f^{-1}(y)|\det\nabla f^{-1}(y)|,\]

where \(f^{-1}\) is the inverse of \(f\) and \(\nabla f(\,\cdot\,)\) and \(\nabla f^{-1}(\,\cdot\,)\) denote the Jacobian matrices of \(f\) and \(f^{-1}\) respectively.

### Gram matrices, kernels and covariance functions

**Definition A.9** (Kernel).: Let \(X\) be an input space. A kernel on \(X\) is a function \(k:X\times X\to\mathbb{R}\).

In this paper, \(X\) will always be a subset of \(\mathbb{R}^{d}\). Given a kernel \(k\) on \(X\), we can construct a Gram matrix:

**Definition A.10** (Gram matrix).: Let \(\{x_{i}\in X\}_{i=1,\ldots,n}\) be a set of \(n\in\mathbb{N}\) points in \(X\), and \(k\) be a kernel on \(X\). The Gram matrix \(\textbf{K}\in\mathbb{R}^{n\times n}\) is a \(n\times n\) matrix, where each element \(K_{ij}=k(x_{i},x_{j})\).

Kernels can have some properties:

**Definition A.11** (Symmetric kernel).: A kernel \(k\) on \(X\) is said to be symmetric if \(k(x,x^{\prime})=k(x^{\prime},x)\) for all \(x,x^{\prime}\in X\).

**Definition A.12** (Positive semi-definite kernel).: A kernel \(k\) on \(X\) is said to be positive semi-definite if it satisfies:

\[\int_{X}\int_{X}k(x,x^{\prime})f(x)f(x^{\prime})\mathrm{d}x\mathrm{d}x^{ \prime}\geq 0,\]

for all square-integrable \(f:X\to\mathbb{R}\).

An alternative definition can be written in terms of the kernel's Gram matrix. A Gram matrix **K** is positive semi-definite if \(\textbf{v}^{T}\textbf{K}\textbf{v}\geq 0\) for all \(\textbf{v}\in\mathbb{R}^{n}\). If, \(\forall n\in\mathbb{N}\) and all sets of \(n\) points \(\{x_{i}\in X\}_{i=1,\ldots,n}\) the Gram matrix **K** is positive semi-definite, then the kernel \(k\) is defined to be positive semi-definite.

A kernel \(k\) on \(X\) which is symmetric and positive semi-definite is called a covariance function because it can be used to define a valid covariance matrix.

**Definition A.13** (Covariance function).: A covariance function \(k\) on \(X\) is a kernel \(k:X\times X\to\mathbb{R}\) which is symmetric and positive semi-definite. A covariance function is also called a covariance kernel.

The Gram matrix of a covariance function is also symmetric and positive semi-definite, which means that it can act as a covariance matrix.

#### a.3.1 Some notes on notation

In this article, we do not make the distinction between a scalar and a vector. Rather, we write \(x\in(X=\mathbb{R}^{d})\) to denote an element of some \(d-\)dimensional real space, where \(d\) should be clear from context, or is an abstract value. Thus, \(x\) contains \(d\) elements, and when \(d=1\), \(x\) represents a scalar. Any vector and matrix operation we write also works for scalar values.

Furthermore, we will be applying a kernel function \(k\) to a variety of different types of inputs, with slight abuse of notation. Let us make clear what we mean by each kind here:

Two vector inputs:\(k(x,x^{\prime})\) is the kernel applied to two points \(x,x^{\prime}\in X\). Thus the output is a scalar value \(k(x,x^{\prime})\in\mathbb{R}\).

A vector input and a matrix input:\(k(x,\textbf{X})\) is the kernel applied to a point \(x\in X\) and a set of points \(n\) points \(\{x_{i}\}_{i=1,\ldots,n}\) collected into a matrix \(\textbf{X}\in\mathbb{R}^{n\times d}\). The output is a vector \(k(x,\textbf{X})\in\mathbb{R}^{n}\), where each element \(k(x,\textbf{X})_{i}=k(x,x_{i})\).

[MISSING_PAGE_EMPTY:11]

### Generating random samples for the Monte Carlo method

The Monte Carlo method requires us to generate samples from several Gaussian distributions. For this, we used the gsl_ran_gaussian_zigurat function provided by GSL [16].

### Measuring the run time

We measure the run time as the sum of the time taken to generate samples incurred during the sampling step of the Monte Carlo method or the initializing step of Algorithm 1, and the time taken for the evaluation step. For both methods, we measure time using the gettimeofday function from the Standard C library [17]. We measure the time from the start of the main entry point until the end of key computations. The reported times omit any time spent by the programs on saving and reporting the results.

In order to explicitly quantify the _post-processing_ step of the Monte Carlo method, we compute the mean and the variance of the samples obtained from the Monte-Carlo-based experiments. In Algorithm 1 we calculate the mean and variance using the representation of the commercial UTPA [3] as provided by its hardware API. We note that we are being generous to the Monte Carlo method, since the mean and the variance alone does not fully capture the shape of a non-Gaussian distribution. In contrast, the representation the uncertainty-tracking processor architecture (UTPA) captures the full distribution in its representation.

### Measuring the Wasserstein Distance

The Wasserstein distance [13] is a metric that measures the distance between probability distributions. We quantify the distance of the outputs to the ground-truth using the Wasserstein distance between the output distribution calculated by each approach and the ground-truth output distribution. We compute the ground-truth output distribution by running the Monte Carlo method with 1,000,000 samples. In our experiments, we calculate the Wasserstein distance using the scipy.stats.wasserstein_distance function from the 5ciPy Python package [18].

### Experimental setup

For Algorithm 1, we vary the representation size of the UTPA. Since the underlying UTPA on which Algorithm 1 is implemented generates in-processor representations of the output distribution, we take samples from this distributional representation to compute the Wasserstein distance. We take 1,000,000 samples, similar to the ground truth. We do not include the time taken for this sampling in the run time because this sampling is done solely to calculate the Wasserstein distance and is not part of a typical use case of Algorithm 1. The commercial implementation [3] of the UTPA

Figure 2: Illustration of the Gaussian Process (with its data points) and the input distribution that are used throughout the experiments in this article.

presented by Tsoutsouras _et al._[1, 2] is implemented as a single-threaded emulated processor running on top of a single x86 3rd Generation Intel Xeon system. As a result, our implementation is at an inherent disadvantage since it is benefiting only from the algorithmic advances of the distribution representation and its associated arithmetic and is not taking advantage of hardware acceleration; an FPGA or custom silicon implementation could provide even greater speedups than we present in this paper.

For the Monte Carlo simulations, we vary the number of Monte Carlo iterations that were used in each simulation. We perform the Monte Carlo experiments on the same x86 hardware as the hardware we used to emulate the UTPA. This provides a baseline for the performance of the Monte Carlo method that can be expected in the real-world. All random samples for the Monte Carlo simulation were carried out using functions made available by GSL [16].

We implement both Algorithm 1 and Monte Carlo experiments using the C programming language and a custom tensor library implemented in C. We compile Monte Carlo implementations with the highest level of compiler optimizations (-03)12.

Footnote 12: Code is available on GitHub: https://github.com/physical-computation/uncertain-gaussia-process-code

## Appendix D Ensuring meaningful timing results

When running the experiments on the Monte Carlo method, each repetition of an experiment was run after a \(5\)s delay. This delay ensures that we avoid buffer cache optimizations carried out by the operating system.

We note that we did not exploit parallelization for the Monte Carlo method (e.g., by using a GPU) since the commercial implementation of the UTPA [3] does not exploit parallelization either. We felt that this provided an apples-to-apples comparison.

## Appendix E Extended Table of Results

Table 2 shows the numerical results from all our experiments.

\begin{table}
\begin{tabular}{l l r r r} \hline \hline \multirow{2}{*}{Problem} & \multirow{2}{*}{Core} & \multicolumn{2}{c}{Representation Size /} & \multicolumn{1}{c}{Wasserstein Distance} & \multicolumn{1}{c}{Run time (ms)} \\  & & \multicolumn{1}{c}{Number of samples} & \multicolumn{1}{c}{(mean \(\pm\) std. dev.)} & \multicolumn{1}{c}{(mean \(\pm\) std. dev.)} \\ \hline Squared Exponential Kernel & Algorithm 1 & 32 & \(0.01871\pm 0.00010\) & \(2.392\pm 0.359\) \\ Squared Exponential Kernel & Algorithm 1 & 64 & \(0.00730\pm 0.00031\) & \(2.524\pm 0.052\) \\
**Squared Exponential Kernel** & **Algorithm 1** & **128** & \(\mathbf{0.00290\pm 0.00050}\) & \(3.182\pm 0.376\) \\ Squared Exponential Kernel & Algorithm 1 & 256 & \(0.00246\pm 0.00050\) & \(6.348\pm 0.148\) \\ Squared Exponential Kernel & Algorithm 1 & 512 & \(0.00173\pm 0.00053\) & \(19.458\pm 2.215\) \\ Squared Exponential Kernel & Algorithm 1 & 2048 & \(0.00211\pm 0.00069\) & \(279.482\pm 2.491\) \\ \hline Squared Exponential Kernel & Traditional Monte Carlo & 4 & \(0.41230\pm 0.22109\) & \(0.129\pm 0.006\) \\ Squared Exponential Kernel & Traditional Monte Carlo & 256 & \(0.05596\pm 0.01936\) & \(0.819\pm 0.015\) \\ Squared Exponential Kernel & Traditional Monte Carlo & 1152 & \(0.02933\pm 0.01174\) & \(3.268\pm 0.087\) \\ Squared Exponential Kernel & Traditional Monte Carlo & 2048 & \(0.02334\pm 0.01216\) & \(5.673\pm 0.023\) \\ Squared Exponential Kernel & Traditional Monte Carlo & 4096 & \(0.01482\pm 0.00580\) & \(11.195\pm 0.051\) \\ Squared Exponential Kernel & Traditional Monte Carlo & 8192 & \(0.01116\pm 0.00578\) & \(22.316\pm 0.092\) \\ Squared Exponential Kernel & Traditional Monte Carlo & 16000 & \(0.00696\pm 0.00301\) & \(43.553\pm 0.654\) \\ Squared Exponential Kernel & Traditional Monte Carlo & 32000 & \(0.00512\pm 0.00219\) & \(86.781\pm 0.447\) \\ Squared Exponential Kernel & Traditional Monte Carlo & 64000 & \(0.00149\pm 0.00177\) & \(173.586\pm 1.463\) \\
**Squared Exponential Kernel** & **Traditional Monte Carlo** & **128000** & \(0.00324\pm 0.00129\) & \(\mathbf{346.186\pm 1.800}\) \\ Squared Exponential Kernel & Traditional Monte Carlo & 512000 & \(0.00213\pm 0.00085\) & \(1385.516\pm 8.283\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Complete results comparing the mean Wasserstein distance (to the ground truth) obtained by the Monte Carlo method and Algorithm 1 to the mean run time taken to run each configuration. In order to obtain a similar accuracy, the Monte Carlo method took approximately \(108.80\times\) more time than Algorithm 1. In order for the Monte Carlo method to obtain better results than Algorithm 1 (at \(r=128\)) by (approximately) more than 1-standard deviation, the Monte Carlo must spend \(435.42\times\) more time.