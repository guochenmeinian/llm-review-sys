# Adaptive Test-Time Personalization for

Federated Learning

 Wenxuan Bao\({}^{1*}\), Tianxin Wei\({}^{1*}\), Haohan Wang\({}^{1}\), Jingrui He\({}^{1}\)

\({}^{1}\)

University of Illinois Urbana-Champaign

{wbao4,twei10,haohanw,jingrui}@illinois.edu

Equal contribution.

###### Abstract

Personalized federated learning algorithms have shown promising results in adapting models to various distribution shifts. However, most of these methods require labeled data on testing clients for personalization, which is usually unavailable in real-world scenarios. In this paper, we introduce a novel setting called test-time personalized federated learning (TTPFL), where clients locally adapt a global model in an unsupervised way without relying on any labeled data during test-time. While traditional test-time adaptation (TTA) can be used in this scenario, most of them inherently assume training data come from a single domain, while they come from multiple clients (source domains) with different distributions. Overlooking these domain interrelations can result in suboptimal generalization. Moreover, most TTA algorithms are designed for a specific kind of distribution shift and lack the flexibility to handle multiple kinds of distribution shifts in FL. In this paper, we find that this lack of flexibility partially results from their pre-defining which modules to adapt in the model. To tackle this challenge, we propose a novel algorithm called ATP to adaptively learns the adaptation rates for each module in the model from distribution shifts among source domains. Theoretical analysis proves the strong generalization of ATP. Extensive experiments demonstrate its superiority in handling various distribution shifts including label shift, image corruptions, and domain shift, outperforming existing TTA methods across multiple datasets and model architectures. Our code is available at https://github.com/baowenxuan/ATP.

## 1 Introduction

Federated learning (FL) is a distributed learning system where multiple clients collaborate to train a machine learning model under the orchestration of the central server, while keeping their data decentralized [31, 18]. However, clients in FL typically exhibit distinct data distributions. For example, in the context of animal image classification, users tend to capture pictures of various animals prevalent in their respective regions, introducing label shift [51] to the local image dataset. Meanwhile, even when capturing images of the same species, the visual appearance can be influenced by the environment and camera settings, introducing feature shift [34]. It is crucial that each client can adapt the model to align with its unique data distribution [44]. Previous personalized federated learning (PFL) works have mainly focused on improving the performance on clients participating in training [41, 37, 25, 4] or generalization to new clients [8, 7, 6], assuming the availability of labeled data. However, in many real-world scenarios, clients do not have labeled data for personalization, which limits the application of PFL algorithms. For example, when employing an animal image classifier to mobile phones, their users may capture images of various animals, but without any accompanying labels indicating the species of the animal.

In this paper, we introduce a novel setting named test-time personalized federated learning (TTPFL). During the training phase, a global model is trained using source clients. During the testing phase, each target client downloads the global model and locally personalizes the model with its unlabeled data during test-time. This setting is particularly well-suited for cross-device FL, especially when generalizing to a large number of target clients that have not participated in the training phase and lack labeled data for supervised personalization. Compared to global FL, which trains a shared global model for all clients, TTPFL enables model adaptation to individual target clients facing complex distribution shifts. Compared to standard PFL, TTPFL does not necessitate additional labeled data from target clients for adaptation.

Test-time adaptation (TTA), which adapts a pretrained model from the source domain to an unlabeled target domain, could be a solution for TTPFL. However, applying current TTA methods to FL poses two challenges. First, most TTA methods assume training data are sampled from a single domain [16; 52]. In FL, where source data are distributed across multiple clients, this simplification neglects interrelations among source domains, impacting generalization. Furthermore, the current TTA methods are usually customized for specific distribution shifts and lack the flexibility to address diverse types of distribution shifts in FL. The inflexibility of existing TTA algorithms largely results from their predefined selection of modules to adapt (e.g., feature extractor [28; 43], final linear layer [16; 36], batch normalization layers [38; 45]). However, different modules encode varying semantic information levels, and adapting specific modules may be effective for certain shifts but not others [20]. Meanwhile, although the distribution shifts among source and target clients cannot be directly inspected, the same type of distribution shifts is likely to exist among source clients. We argue that

_Which modules to adapt should depend on the type of distribution shifts among clients, which can be inferred from source clients._

Motivated by this, we propose a new Adaptive Test-time Personalization algorithm called ATP to learn the adaptation rates from distribution shifts among source clients. During training, each source client simulates unsupervised adaptation and refine the adaptation rates of each module to maximize the effect of unsupervised adaptation. The server aggregates local adaptation rates periodically to improve generalization. During testing, each target client leverages learned adaptation rates to locally adapt the global model, and cumulatively averages adapted models from previous batches to enhance the performance for online TTA. Theoretical analysis confirms ATP's robust generalization due to its utilization of multiple sources and low-dimensional adaptation rates. Extensive experiments demonstrate its superiority in addressing various distribution shifts scenarios, including label shift, image corruptions, and domain shift, consistently outperforming existing TTA methods across multiple datasets and model architectures. _We summarize our contributions as follows._

* We consider TTPFL, a new learning setting in FL, addressing the challenge of generalizing to new unlabeled clients under complex distribution shifts. (Section 3)
* We introduce ATP, which adaptively learns the adaptation rate for each module, enabling it to handle different types of distribution shifts. (Section 4)
* We provide theoretical analysis confirming ATP's robust generalization. (Section 5)
* We empirically evaluate ATP over various distribution shifts scenarios, using a wide range of datasets and models. (Section 6)

## 2 Related works

**Federated learning** (FL) is a distributed learning system where multiple clients collaborate to train a machine learning model under a central server's orchestration while keeping data decentralized [18].

**Personalized federated learning** (PFL) extends this framework by allowing each client to personalize the model to its own local data. The most straightforward PFL method is fine-tuning the global model with a few steps of gradient descent [48; 8; 7]. Similarly, another line of works use the global model as a regularizer [22] during local training. FedTHE [17] focuses on evolving local testing set, and proposes a test-time adaptation algorithm for FL that adaptively combines global and personalized models. However, all these methods require labeled data to construct personalized models. Fed-RoD [6] uses hypernetworks to generate personalized model, relaxing the requirements for labeled data. But it still requires the label distribution of the client. FedUL [30] trains a global model with only unlabeled clients. However, it is limited to label shift where each client shares the label-conditional feature distribution \(p(\bm{x}|\bm{y})\). Our setting is mostly similar to OD-PFL [2], which also focuses on generalization to new unlabeled client. It uses an unsupervised client encoder and a hypernetwork [39] to generate personalized model. However, OD-PFL requires re-training a large hypernetwork, while our TTPFL setting focuses on adapting an existing global model.

**Test-time adaptation** (TTA) aims to adapt a machine learning model to a testing set with dataset shift during test-time without re-accessing training data. Most of the TTA methods focus on either feature shift or label shift. For feature shift (same \(p(\bm{y}|\bm{x})\), different \(p(\bm{x})\)), entropy minimization is frequently used to adapt the model in the unsupervised fashion. Tent [45] minimizes the average prediction entropy by adapting the batch normalization layers [15]. MEMO [52] minimizes the marginal entropy over different augmentations of the sample input image by adjusting all model parameters. SHOT [28] exploits information maximization and pseudo-labeling to achieve target-specific feature extraction. Differently, T3A [16] adjusts the final classification layer, but it is also shown to implicitly reduce the entropy. It is important to notice that all these methods pre-define which modules to be adapted in the network. For label shift (same \(p(\bm{x}|\bm{y})\), different \(p(\bm{y})\)), most of the previous works focus on estimating the shifted label distribution. EM [36; 1] iteratively uses model predictions to estimate the label prior distribution and uses label prior distribution to adjust model predictions. BBSE [29; 3] constructs a confusion matrix on the validation dataset, and uses the prediction distribution to estimate the ground-truth label distribution. The estimated label distribution is used for re-training a model with importance sampling. [49] generalizes these methods to the online dataset shift setting where the label distribution for testing data is evolving over time. However, all these methods heavily rely on the assumption of the same \(p(\bm{x}|\bm{y})\), which can be violated in real applications.

**Comparison with FedTHE [17]** Recently, FedTHE also explored TTA in FL. However, FedTHE focus on the test-time distribution shift for clients that participate in FL training, while we focus on improving the performance on novel clients. Moreover, FedTHE fuses global head and personalized head to get robust prediction. It cannot be easily generalized to target clients which does not have labeled data to train the personalized head.

Our paper is also related to partial fine-tuning and hyperparameter optimization. We discuss these works in Appendix A.1 in detail.

## 3 Motivation

In this section, we first introduce the setting of test-time personalized federated learning, and then show that current TTA methods lack the flexibility to various types of distribution shifts in TTPFL.

### Test-time personalized federated learning

**Preliminary** We consider a standard setting for cross-device FL [46] and domain generalization [47]. Considering an FL system with \(N\) source clients \(\{\mathcal{S}_{i}\}_{i=1}^{N}\) and \(M\) target clients \(\{\mathcal{T}_{j}\}_{j=1}^{M}\). Each source client \(\mathcal{S}_{i}\) has its own _labeled_ dataset \(\mathbb{D}^{\mathcal{S}_{i}}\) with \(n_{i}\) samples \(\{(\bm{x}_{1}^{\mathcal{S}_{i}},\bm{y}_{1}^{\mathcal{S}_{i}}),\cdots,(\bm{x} _{n_{i}}^{\mathcal{S}_{i}},\bm{y}_{n_{i}}^{\mathcal{S}_{i}})\}\) i.i.d. drawn from its distribution \(P^{\mathcal{S}_{i}}(\bm{x},\bm{y})\), where \(\bm{x}\) is the input and \(\bm{y}\) is its corresponding label. Each target client \(\mathcal{T}_{j}\) has its own _unlabeled_ dataset \(\mathbb{X}^{\mathcal{T}_{j}}=\{\bm{x}_{1}^{\mathcal{T}_{j}},\cdots,\bm{x}_{m_ {j}}^{\mathcal{T}_{j}}\}\) i.i.d. drawn from its distribution \(P^{\mathcal{T}_{j}}(\bm{x},\bm{y})\), while the corresponding labels \(\{\bm{y}_{1}^{\mathcal{T}_{j}},\cdots,\bm{y}_{m_{j}}^{\mathcal{T}_{j}}\}\) cannot be accessed. The distributions for different source/target clients are different, sampled from a meta-distribution \(\mathcal{Q}\), i.e., distribution of distributions. _Global federated learning_ (GFL) aims to find a single global model minimizing the expected loss over client population [46]:

\[\mathcal{L}(\bm{w}_{G})=\mathbb{E}_{P\sim\mathcal{Q}}\mathcal{L}_{P}(\bm{w}_{ G}),\text{ where }\mathcal{L}_{P}(\bm{w}_{G})=\mathbb{E}_{(\bm{x},\bm{y})\in P}\ell(f(\bm{x};\bm{w}_{ G});\bm{y})\] (1)

where \(\ell\) represents the loss function and \(f\) represents model. GFL enforces that each client uses the same global model for prediction, which does not allow for adaptation to each client's unique data distribution. In contrast, _personalized federated learning_ (PFL) personalizes the global model \(\bm{w}_{G}\) using its labeled data, and uses the personalized model for prediction, replacing the \(\bm{w}_{G}\) in Eq. (1). However, most of the PFL algorithms [8; 7; 22] require the assumption that the target client also possesses additional labeled data, which is a stronger assumption compared to GFL.

**Test-time personalized federated learning** In this paper, we introduce a novel setting named _test-time personalized federated learning_ (TTPFL), and compare it with the standard GFL and PFLin Figure 1. TTPFL focuses on how to adapt a trained global model to each target client's data distributions during _test-time_, with an adaptation rule \(\mathcal{A}\) only using unlabeled data. The objective function can be formulated as

\[\mathcal{L}(\bm{w}_{G},\mathcal{A})=\mathbb{E}_{P\sim\mathcal{Q}}\mathcal{L}_{P} (\bm{w}_{G},\mathcal{A}),\ \text{where}\ \ \mathcal{L}_{P}(\bm{w}_{G},\mathcal{A})=\mathbb{E}_{(\bm{x},\bm{y})\in P}\ell(f( \bm{x};\mathcal{A}(\bm{w}_{G},\bm{X}));\bm{y})\] (2)

which can be unbiasedly estimated by the average loss over \(M\) target clients unseen during training

\[\hat{\mathcal{L}}(\bm{w}_{G},\mathcal{A})=\frac{1}{M}\sum_{j=1}^{M}\hat{ \mathcal{L}}_{P^{\mathcal{T}_{j}}}(\bm{w}_{G},\mathcal{A}),\ \text{where}\ \ \hat{\mathcal{L}}_{P^{\mathcal{T}_{j}}}(\bm{w}_{G},\mathcal{A})=\frac{1}{m_{j} }\sum_{r=1}^{m_{j}}\ell(f(\bm{x}_{r}^{\mathcal{T}_{j}};\mathcal{A}(\bm{w}_{G},\bm{X}_{r}^{\mathcal{T}_{j}}));\bm{y}_{r}^{\mathcal{T}_{j}})\] (3)

The adaptation rule \(\mathcal{A}\) adapts a the global model with unlabeled samples \(\bm{X}_{r}^{\mathcal{T}_{j}}\). We consider two standard settings: _test-time batch adaptation_ (TTBA) and _online test-time adaptation_ (OTTA) [27]. TTBA individually adapts the global model to each batch of unlabeled samples, where \(\bm{X}_{r}^{\mathcal{T}_{j}}\) is the data batch that \(\bm{x}_{r}^{\mathcal{T}_{j}}\) belongs. OTTA adapts the global model in an online manner, where \(\bm{X}_{r}^{\mathcal{T}_{j}}\) contains all the data batches arriving before or together with \(\bm{x}_{r}^{\mathcal{T}_{j}}\).

### Limitation of test-time adaptation

As the precursor to TTPFL, TTA [45; 52; 29] studies how to adapt a trained model to target dataset under certain types of dataset shifts. Since TTA methods only require unlabeled target data for adaptation, they can be applied in TTPFL. We test state-of-the-art TTA methods with ResNet-18 on CIFAR-10 under two types of distribution shifts: label shift and feature shift, with results presented in Figure 2. As expected, each algorithm can boost the model's accuracy under the distribution shift it is designed for. However, most algorithms improve their performance in one scenario while simultaneously impairing it in another scenario, demonstrating a trade-off in their performance on feature shift and label shift. Moreover, when facing a more complex hybrid of distribution shifts, most TTA methods fail to introduce satisfactory performance gain (Table 1). Therefore, TTA methods are not suitable for TTPFL given the variety of distribution shifts in FL client.

The inflexibility of TTA algorithms largely results from their predefined selection of modules to adapt, e.g., batch normalization (BN) layers [38; 45], the feature extractor [28; 43], or the last linear layer [16; 36]. However, which modules to adapt is closely related to the type of distribution shift. For example, adapting the last linear layer can encode the label shift (Proposition 3.1), while it may fail when the extracted features are already corrupted due to feature shift. Similarly, adapting the BN layers can improve the performance under feature shift by distribution alignment (Proposition 3.2), while distribution alignment can harm the performance under label shift [53].

**Proposition 3.1** (Adapting the last layer to handle label shift).: _Consider two distribution \(p,q\) with \(p(\bm{x}|\bm{y})=q(\bm{x}|\bm{y})\) and \(p(\bm{y})\neq q(\bm{y})\). When a neural network is calibrated on \(p\), i.e., \(f(\bm{x};\bm{w})=p(\cdot|\bm{x})\), it is calibrated on \(q\) after adding \(\log\frac{q(\bm{y})}{p(\bm{y})}\) to the bias term of the final last layer._

**Proposition 3.2** (Adapting the BN layer to handle feature shift [38]).: _When the feature shift only causes differences in the first and second order moments of the feature activations \(\bm{z}=g(\bm{x})\) where

Figure 1: Comparison between the testing phase of GFL, PFL, and TTPFL. TTPFL enables model personalization without requiring labeled data.

is the combination of layers before the BN layer, the feature shift can be removed by adapting running mean and variance of the BN layer._

To verify the connection between distribution shift and the selection of modules for adaptation, we experiment with adapting different subsets of modules within the network to minimize the entropy loss [45]. In Figure 3, we observe a similar performance trade-off between feature shift and label shift: while adapting certain modules can boost the accuracy under one distribution shift, it is less likely to succeed under the other shift. To break the performance trade-off, it is essential to adaptively choose which modules to adapt according to the present type of distribution shift. Moreover, while [20] suggests adapting different blocks in the network, we find it more important to decide (1) which module type to adapt and (2) what is the adaptation rate (i.e., learning rate for adaptation). For example, adapting all BN running means significantly outperforming adapting any one block under feature shift. Meanwhile, employing positive or negative adaptation rates for running means yields contrasting outcomes, favoring adaptation in the presence of label shift or feature shift while impairing the other. These observations motivate us to choose which module to adapt (instead of blocks) while optimizing the adaptation rates for each module.

## 4 ATP: adaptive test-time personalization

In this section, we propose ATP that automatically learns the adaptation rates for each module. We introduce the training and testing phase of ATP in subsection 4.1 and 4.2, respectively.

### Training phase: learn to adapt with source clients

In this part, we introduce how ATP learns adaptation rates from source clients without sharing local data. ATP uses the communication protocol of FedAvg [31] to optimize adaptation rates. In each communication round, each source client first simulates unsupervised adaptation with the current adaptation rates, and then refines the adaptation rates to maximize the effect of adaptation. After local computation, the local adaptation rates are then aggregated on the server to ensure better generalization to target clients. Algorithm 1 gives the overview of the training phase of ATP. We then explain each step in detail.

Unsupervised adaptationWe consider a neural network model \(f(\cdot;\bm{w}_{G})\) with global model parameter \(\bm{w}_{G}\in\mathbb{R}^{D}\). Similar to previous works [45; 38], we consider the model processes a data batch \(\bm{X}_{k}^{\mathcal{S}_{i}}=\{\bm{x}_{k,b}^{\mathcal{S}_{i}}\}_{b=1}^{B}\) at a time where \(B\) is the batch size, \(i\) is the client index and \(k\) is the batch index. In the following, we omit the superscript \(\mathcal{S}_{i}\) for clarity, e.g. \(\bm{X}_{k}^{\mathcal{S}_{i}}\rightarrow\bm{X}_{k}\), as unsupervised adaptation and supervised refinement operate identically across all source clients. The network has \(d\) modules, with corresponding parameters \(\bm{w}^{[1]},\cdots,\bm{w}^{[d]}\). Typically we have \(d\ll D\). During unsupervised adaptation, we allow each module \(\bm{w}^{[l]}\) to have a different adaptation rate \(\alpha^{[l]}\). ATP learns to adapt both trainable parameters and running statistics for batch normalization (BN) [15] layers. To achieve more precise control of adaptation, the'module' in ATP is slightly more fine-grained than the 'layer'. For example, each BN layer has four modules: running mean, running variance, weight, and bias.

Update trainable parametersA common strategy for updating trainable parameters is performing one step of gradient descent to minimize the cross-entropy loss. Since label information are unavailable for computing cross-entropy, we instead minimize the entropy loss \(\ell_{H}(\hat{\bm{Y}})=\frac{1}{B}\sum_{b=1}^{B}(-\sum_{c}\hat{y}_{b,c}\log\hat{y }_{b,c})\), where \(\hat{\bm{Y}}\) is the prediction probabilities over the label space of a data batch. Entropy quantifies the uncertainty of the model prediction, and is frequently used in previous TTA algorithms [45, 52, 28]. For each trainable parameter module \(\bm{w}^{[l]}\), the corresponding unsupervised update direction for each client is the negative gradient direction, i.e.,

\[\bm{h}_{k}^{[l]}=-\nabla_{\bm{w}^{[l]}}\ell_{H}(f(\bm{X}_{k};\bm{w}_{G}))\] (4)

Update running statisticsThe running statistics (mean/variance) in BN layers are not updated by gradient descent. Instead, they are updated by running average.

\[\bm{w}_{k}^{[l]}\leftarrow(1-m)\bm{w}_{G}^{[l]}+m\hat{\bm{w}}_{k}^{[l]}=\bm{w }_{G}^{[l]}+m(\hat{\bm{w}}_{k}^{[l]}-\bm{w}_{G}^{[l]})\]

where \(\bm{w}_{G}^{[l]}\) is the running statistics and \(\hat{\bm{w}}_{k}^{[l]}\) is the statistic for the current batch of inputs. In previous works, the momentum1\(m\) is usually a fixed hyperparameter in \([0,1]\). In ATP, we consider the momentum for each module as an adaptation rate (\(\alpha^{[l]}\in\mathbb{R}\)) to be learned. We define the corresponding update direction as

Footnote 1: Some literatures consider \((1-m)\) as the momentum. Here we follow the definition in PyTorch.

\[\bm{h}_{k}^{[l]}=\hat{\bm{w}}_{k}^{[l]}-\bm{w}_{G}^{[l]}\] (5)

After computing the update direction, each module will be updated along the update direction with its corresponding adaptation rate, i.e., \(\bm{w}_{k}^{[l]}\leftarrow\bm{w}_{G}^{[l]}+\alpha^{[l]}\bm{h}_{k}^{[l]}\). Expressed in a compact form,

\[\bm{w}_{k}\leftarrow\bm{w}_{G}+(\bm{A}\bm{\alpha})\odot\bm{h}_{k}\] (6)

where \(\odot\) is the element-wise product, \(\bm{h}_{k}\in\mathbb{R}^{D}\) is the concatenation of \(\{\bm{h}_{k}^{[l]}\}_{l=1}^{d}\), \(\bm{\alpha}=[\alpha^{[1]},\cdots,\alpha^{[d]}]^{\top}\) and \(\bm{A}\in\mathbb{R}^{D\times d}\) is a 0-1 assignment matrix that maps each adaptation rate \(\alpha^{[l]}\) to the indices of \(l\)-th module's parameters in \(\bm{w}_{G}\).

Supervised refinementAfter unsupervised adaptation, we refine the adaptation rates on each source client with label information to minimize \(\ell_{CE}(f(\bm{X}_{k},\bm{w}_{k}),\bm{Y}_{k})\), where \(\ell_{CE}\) is the cross-entropy loss. We use gradient descent to optimize \(\bm{\alpha}\), i.e.,

\[\bm{\alpha}\leftarrow\bm{\alpha}-\eta\nabla_{\bm{\alpha}}\ell_{CE}(f(\bm{X}_{ k};\bm{w}_{k}),\bm{Y}_{k})\] (7)

where \(\eta\) is the learning rate of adaptation rates. Notice that the gradient of \(\bm{\alpha}\) can be computed as

\[\nabla_{\bm{\alpha}}\ell_{CE}(f(\bm{X}_{k};\bm{w}_{k}),\bm{Y}_{k})=\frac{ \partial\ell_{CE}(f(\bm{X}_{k};\bm{w}_{k}),\bm{Y}_{k})}{\partial\bm{w}_{k}} \frac{\partial\bm{w}_{k}}{\partial\bm{\alpha}}=\bm{A}^{\top}(\bm{h}_{k}\odot \nabla_{\bm{w}_{k}}\ell_{CE}(f(\bm{X}_{k};\bm{w}_{k}),\bm{Y}_{k}))\]

To estimate the gradient of \(\bm{\alpha}\), each training client only needs to adjacently compute the unsupervised and supervised gradient, and compute their module-wise inner products. Different from many meta-learning algorithms [9, 26], ATP is computationally very efficient since it requires no second-order derivatives. In the practical implementation, since each module in the model has significantly different number of parameters, the raw gradient for each \(\alpha^{[l]}\) usually has different scales. Therefore we normalize the gradient with the square root of the number of parameters in the corresponding module.

Server aggregationTo incorporate adaptation knowledge from multiple source clients and enhance generalization to the clients' population, ATP use standard federated aggregation [31] to periodically aggregates the local adaptation rates. In each communication rounds, after each client locally update \(\bm{\alpha}\) for a few iterations, the local adaptation rates are uploaded to the server for averaging (as shown in line 6 of Algorithm 1), and then sent to source clients for the next round of training. With server aggregation, ATP learn the adaptation rates that enables successful adaptation to all source clients in average.

Communication costNotice that ATP only optimizes the adaptation rates \(\bm{\alpha}\) without changing the global model \(\bm{w}_{G}\). Therefore, only the adaptation rates are kept transmitted between the server and each client, while the global model parameter is only broadcasted once at the start of the ATP training. Such design significantly reduces the communication cost from \(2TD\) (for standard FedAvg) to \(D+2Td\).

[MISSING_PAGE_EMPTY:7]

source client. Assuming (1) L-Lipschitz model, and (2) \(H\)-upper-bounded 2-norms for each module's update. For any fixed global model \(\bm{w}_{G}\) and any \(\epsilon>0\), we have_

\[\Pr(\sup_{\bm{\alpha}\in\mathcal{H}}|\varepsilon(\bm{\alpha})-\hat{\varepsilon} (\bm{\alpha})|\geq\epsilon)\leq\left(\frac{12LHR}{\epsilon}\right)^{d}\cdot 4 \exp\left(-\frac{NK\epsilon^{2}}{2(\sqrt{K}+1)^{2}}\right)\] (9)

_where \(\hat{\varepsilon}(\bm{\alpha})\) is the average **post-adaptation** error rate on source clients, and \(\varepsilon(\bm{\alpha})\) is the expected **post-adaptation** error rate on clients' population._

Theorem 5.1 shows that, although ATP improves the model expressiveness by adapting the model to each client's distribution, ATP can still provably generalize well to the clients' population. Especially, this generalization benefit from low dimensionality of adaptation rates, since the bound get looser when \(d\) increases. Moreover, this bound shows the importance of learning adaptation rates from multiple source clients: if we merge all \(N\) source domains with \(K\) batches into one domain with \(NK\) batches, then the bound will be much looser.

## 6 Experiments

In this section, we design experiments to answer the following research questions:

* **RQ1**: Can ATP handle different distribution shift and outperform prior TTA methods?
* **RQ2**: Does ATP learn adaptation rates specific to distribution shift?

SetupWe evaluate ATP on a variety of models, datasets and distribution shifts. We first evaluate on CIFAR-10(-C) with a standard three-way split [50]: we randomly split the dataset to 300 clients: 240 source clients and 60 target clients. Each source client has 160 training samples and 40 validation samples, while each target client has 200 unlabeled testing samples. We simulate three kinds of distribution shifts: feature shift, label shift, and hybrid shift. For feature shift, we follow [12, 17], randomly apply 15 different kinds of corruptions to the source clients, and 4 new kinds of corruptions to the target clients to test the generalization of ATP. For label shift, we use the step partition [5], where each client has 8 minor classes with 5 images per class, and 2 major classes with 80 images per class. For the hybrid shift, we apply both step partition and feature perturbations. To test ATP under more challenging domain shifts, we then evaluate ATP on two domain generalization datasets: Digits-5 [25] and PACS [21]. We adopt the leave-one-domain-out evaluation protocol [10], i.e., one domain is chosen to construct target clients, and the remaining domains are used to construct source clients. We follow similar data preprocessing in [25], while additionally applying step partition to inject label shift. Each domain is divided into 10 clients, leading to 40/10 source/target clients for Digits-5 and 30/10 source/target clients for PACS. For the experiments above, we use ResNet-18 [11] as a common choice in FL experiments [42, 14, 33]. We also test ATP with two different architectures: a five-layer CNN on CIFAR-10(-C) and ResNet-50 on CIFAR-100(-C). Detailed experiment settings are given in Appendix C.1.

### RQ1: Can ATP handle different distribution shift?

We compare ATP with three kinds of baseline TTA methods. For _feature shift_ methods, we compare to BN-Adapt [38] and Tent [45] which adjusts the batch normalization layers, SHOT [28] which adjusts the feature extractor, T3A [16] which adjusts the final classifier, and MEMO [52] which uses augmentation to adjust the whole network. For _label shift_, we compare to EM [36] which adjusts the label priori unsupervisedly with expectation-maximization, and BBSE [29] which uses the validation data to construct a confusion matrix to estimate the label priori. Since re-training a model with different label weights for each client is not realistic in FL. We use the estimated label distribution to adjust the output of a classifier.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Method & Feature shift & Label shift & Hybrid shift & Avg. Rank \\ \hline No-adaptation & 69.42 \(\pm\) 0.13 & 72.98 \(\pm\) 0.24 & 65.68 \(\pm\) 0.24 & 7.7 \\ BN-Adapt & 73.52 \(\pm\) 0.22 & 54.54 \(\pm\) 0.10 & 50.42 \(\pm\) 0.39 & 7.0 \\ SHOT & 71.76 \(\pm\) 0.17 & 48.13 \(\pm\) 0.18 & 44.68 \(\pm\) 0.32 & 9.3 \\ Tent & 71.76 \(\pm\) 0.09 & 51.13 \(\pm\) 0.21 & 46.45 \(\pm\) 0.26 & 8.3 \\ TTA & 69.63 \(\pm\) 0.13 & 70.32 & 62.17 \(\pm\) 0.17 & 8.0 \\ MEMO & 72.43 \(\pm\) 0.22 & 77.30 \(\pm\) 0.15 & 68.97 \(\pm\) 0.28 & 4.3 \\ EM & 65.18 \(\pm\) 0.12 & 80.73 \(\pm\) 0.18 & 69.85 \(\pm\) 0.43 & 5.0 \\ BBSE & 63.98 \(\pm\) 0.17 & 79.30 \(\pm\) 0.17 & 67.96 \(\pm\) 0.43 & 6.7 \\ Surgical & 69.35 \(\pm\) 0.22 & 76.00 \(\pm\) 0.17 & 66.94 \(\pm\) 0.43 & 6.3 \\ ATP-batch & **73.68**\(\pm\) 0.10 & **79.90**\(\pm\) 0.22 & 73.05 \(\pm\) 0.35 & 2.3 \\ ATP-online & **74.06**\(\pm\) **0.18** & **81.96**\(\pm\) **0.14** & **75.37**\(\pm\) **0.22** & **1.0** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Accuracy (mean \(\pm\) s.d. %) on target clients under various distribution shifts on CIFAR-10

[MISSING_PAGE_FAIL:9]

the prediction distribution. We use a toy example in Appendix C.5 to show why negative adaptation rate can improve the model performance under label distribution.

Moreover, we examine whether the learn adaptation rates are specific to the type of distribution shift by training on one distribution shift, but testing on another. We observe in Table 3 that, ATP performs the best when trained and tested with the same type of distribution shifts. However, the adaptation rates trained on feature/label shift fails to boost the performance on the other distribution shift. The adaptation rates trained on hybrid shift can generalize to feature shift and label shift, but still worse than the adaptation rates trained with the same type of distribution shifts. These results show that the learn adaptation rates are specific to the type of distribution shift.

### Further discussion

Ablation studyWe present two variants of ATP to study how trainable parameters and running statistics contribute to the adaptability of ATP. ATP-params only learns to adapt the trainable parameters, while ATP-stats focuses solely on adapting the running statistics. As shown in Table 4, adapting trainable parameters and running statistics both play critical roles in achieving successful adaptation. More specifically, ATP-params primarily facilitate adaptation to label shift, whereas ATP-stats essentially aid in adapting to feature shift.

Hyperparameter sensitivityFigure 5 shows the effects of cohort size and batch size with CIFAR-10 under the hybrid shift, where cohort size refers to the number of clients sampled at each round. ATP demonstrates remarkable consistency in accuracy across different cohort sizes, indicating its robustness. For batch size, we optimize the adaptation rates with \(B=20\) and subsequently evaluate the algorithm with different batch sizes. We find that ATP consistently improves the model's accuracy across different batch sizes, with larger batch sizes yielding greater benefits for the model. ATP-online is more robust to batch size than ATP-batch since it can utilizes information from previous batches.

## 7 Conclusion

In this paper, we propose ATP that unsupervisedly learns the adaptation rate for each module to handle various types of distribution shifts encountered in test-time personalized federated learning. As a potential future direction, incorporating the training of the global model could offer advantages in terms of facilitating easier and better personalization.

\begin{table}
\begin{tabular}{l c c c} \hline \hline \multirow{2}{*}{Train} & \multicolumn{2}{c}{Test} \\ \cline{2-4}  & Feature shift & Label shift & Hybrid shift \\ \hline No adaptation & 69.42 \(\pm\) 0.13 & 72.98 \(\pm\) 0.24 & 63.68 \(\pm\) 0.24 \\ Feature shift & **73.68 \(\pm\) 0.40** & 60.55 \(\pm\) 1.82 & 60.64 \(\pm\) 1.43 \\ Label shift & 67.99 \(\pm\) 0.28 & **79.90 \(\pm\) 0.22** & 69.50 \(\pm\) 0.52 \\ Hybrid shift & 72.69 \(\pm\) 0.14 & 78.92 \(\pm\) 0.34 & **73.68 \(\pm\) 0.35** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Train and test adaptation rates with different distribution shifts, accuracy (mean \(\pm\) s.d. %)

Figure 5: Effect of cohort size and batch size

[MISSING_PAGE_FAIL:11]

* Ioffe and Szegedy [2015] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Francis R. Bach and David M. Blei, editors, _Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015_, volume 37 of _JMLR Workshop and Conference Proceedings_, pages 448-456. JMLR.org, 2015.
* Iwasawa and Matsuo [2021] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for model-agnostic domain generalization. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 2427-2440, 2021.
* Jiang and Lin [2023] Liangze Jiang and Tao Lin. Test-time robust personalization for federated learning. In _The Eleventh International Conference on Learning Representations_, 2023.
* Kairouz et al. [2021] Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aurelien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista A. Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G. L. D'Oliveria, Hubert Eichner, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adria Gascon, Badh Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Guuri Joshi, Mikhail Khodak, Jakub Konecny, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo, Tancrede Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer Ozgur, Rasmus Pagh, Hang Qi, Daniel Ramage, Ramesh Raskar, Mariana Raykova, Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tramer, Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Advances and open problems in federated learning. _Found. Trends Mach. Learn._, 14(1-2):1-210, 2021.
* Khodak et al. [2021] Mikhail Khodak, Renbo Tu, Tian Li, Liam Li, Maria-Florina Balcan, Virginia Smith, and Ameet Talwalkar. Federated hyperparameter tuning: Challenges, baselines, and connections to weight-sharing. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 19184-19197, 2021.
* Lee et al. [2023] Yoonho Lee, Annie S Chen, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang, and Chelsea Finn. Surgical fine-tuning improves adaptation to distribution shifts. In _The Eleventh International Conference on Learning Representations_, 2023.
* Li et al. [2017] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Deeper, broader and artier domain generalization. In _IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017_, pages 5543-5551. IEEE Computer Society, 2017.
* Li et al. [2021] Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated learning through personalization. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pages 6357-6368. PMLR, 2021.
* Li et al. [2020] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. In Inderjit S. Dhillon, Dimitris S. Papailiopoulos, and Vivienne Sze, editors, _Proceedings of Machine Learning and Systems 2020, MLSys 2020, Austin, TX, USA, March 2-4, 2020_. mlsys.org, 2020.
* Li et al. [2020] Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith. Fair resource allocation in federated learning. In _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net, 2020.
* Li et al. [2021] Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. Fedbn: Federated learning on non-iid features via local batch normalization. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021.
* Li et al. [2017] Zhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-sgd: Learning to learn quickly for few shot learning. _CoRR_, abs/1707.09835, 2017.
* Liang et al. [2023] Jian Liang, Ran He, and Tieniu Tan. A comprehensive survey on test-time adaptation under distribution shifts. _CoRR_, abs/2303.15361, 2023.
* Liang et al. [2020] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pages 6028-6039. PMLR, 2020.
* Lipton et al. [2018] Zachary C. Lipton, Yu-Xiang Wang, and Alexander J. Smola. Detecting and correcting for label shift with black box predictors. In Jennifer G. Dy and Andreas Krause, editors, _Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018_, volume 80 of _Proceedings of Machine Learning Research_, pages 3128-3136. PMLR, 2018.

* [30] Nan Lu, Zhao Wang, Xiaoxiao Li, Gang Niu, Qi Dou, and Masashi Sugiyama. Federated learning from only unlabeled data with class-conditional-sharing clients. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.
* [31] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In _Proceedings of the 20th International Conference on Artificial Intelligence and Statistics_, pages 1273-1282. PMLR, 2017.
* [32] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. _Foundations of Machine Learning_. Adaptive computation and machine learning. MIT Press, 2012.
* [33] Jaehoon Oh, Sangmook Kim, and Se-Young Yun. Fedbabu: Toward enhanced representation for federated image classification. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.
* [34] Amirhossein Reisizadeh, Farzan Farnia, Ramin Pedarsani, and Ali Jadbabaie. Robust federated learning: The case of affine distribution shifts. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* [35] Youngmin Ro and Jin Young Choi. Autolr: Layer-wise pruning and auto-tuning of learning rates in fine-tuning of deep networks. In _Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021_, pages 2486-2494. AAAI Press, 2021.
* [36] Marco Saerens, Patrice Latinne, and Christine Decaestecker. Adjusting the outputs of a classifier to new a priori probabilities: A simple procedure. _Neural Comput._, 14(1):21-41, 2002.
* [37] Felix Sattler, Klaus-Robert Muller, and Wojciech Samek. Clustered federated learning: Model-agnostic distributed multitask optimization under privacy constraints. _IEEE Trans. Neural Networks Learn. Syst._, 32(8):3710-3722, 2021.
* [38] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* [39] Aviv Shamsian, Aviv Navon, Ethan Fetaya, and Gal Chechik. Personalized federated learning using hypernetworks. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pages 9489-9502. PMLR, 2021.
* [40] Zhiqiang Shen, Zechun Liu, Jie Qin, Marios Savvides, and Kwang-Ting Cheng. Partial is better than all: Revisiting fine-tuning strategy for few-shot learning. In _Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021_, pages 9594-9602. AAAI Press, 2021.
* [41] Virginia Smith, Chao-Kai Chiang, Maziz Sanjabi, and Ameet Talwalkar. Federated multi-task learning. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, pages 4424-4434, 2017.
* [42] Benyuan Sun, Hongxing Huo, Yi Yang, and Bo Bai. Partialfed: Cross-domain personalized federated learning via partial initialization. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 23309-23320, 2021.
* [43] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A. Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pages 9229-9248. PMLR, 2020.
* [44] Alysa Ziying Tan, Han Yu, Lizhen Cui, and Qiang Yang. Towards personalized federated learning. _CoRR_, abs/2103.00710, 2021.
* [45] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno A. Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021.

* [46] Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H. Brendan McMahan, Blaise Aguera y Arcas, Maruan Al-Shedivat, Galen Andrew, Salman Avestimehr, Katharine Daly, Deepelsa Ma, Suhas N. Diggavi, Hubert Eichner, Adwait Gadhikar, Zachary Garrett, Antonios M. Girgis, Filip Hanzely, Andrew Hard, Chaoyang He, Samuel Horvath, Zhouyuan Huo, Alex Ingerman, Martin Jaggi, Tara Javidi, Peter Kairouz, Satyen Kale, Sai Praneeth Karimireddy, Jakub Konecny, Sanmi Koyejo, Tian Li, Luyang Liu, Mehryar Mohri, Hang Qi, Sashank J. Reddi, Peter Richtarik, Karan Singhal, Virginia Smith, Mahdi Soltanolkotabi, Weikang Song, Ananda Theertha Suresh, Sebastian U. Stich, Ameet Talwalkar, Hongyi Wang, Blake E. Woodworth, Shanshan Wu, Felix X. Yu, Honglin Yuan, Manzil Zaheer, Mi Zhang, Tong Zhang, Chunxiang Zheng, Chen Zhu, and Wennan Zhu. A field guide to federated optimization. _CoRR_, abs/2107.06917, 2021.
* [47] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip S. Yu. Generalizing to unseen domains: A survey on domain generalization. _IEEE Trans. Knowl. Data Eng._, 35(8):8052-8072, 2023.
* [48] Kangkang Wang, Rajiv Mathews, Chloe Kiddon, Hubert Eichner, Francoise Beaufays, and Daniel Ramage. Federated evaluation of on-device personalization. _CoRR_, abs/1910.10252, 2019.
* [49] Ruihan Wu, Chuan Guo, Yi Su, and Kilian Q. Weinberger. Online adaptation to label distribution shift. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 11340-11351, 2021.
* [50] Honglin Yuan, Warren Richard Morningstar, Lin Ning, and Karan Singhal. What do we mean by generalization in federated learning? In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.
* [51] Jie Zhang, Zhiqi Li, Bo Li, Jianghe Xu, Shuang Wu, Shouhong Ding, and Chao Wu. Federated learning with label distribution skew via logits calibration. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 26311-26329. PMLR, 2022.
* [52] Marvin Zhang, Sergey Levine, and Chelsea Finn. MEMO: test time robustness via adaptation and augmentation. In _NeurIPS_, 2022.
* [53] Han Zhao and Geoffrey J. Gordon. Inherent tradeoffs in learning fair representations. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alche-Buc, Emily B. Fox, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 15649-15659, 2019.
* [54] Hao Zhao, Yuejiang Liu, Alexandre Alahi, and Tao Lin. On pitfalls of test-time adaptation. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA_, volume 202 of _Proceedings of Machine Learning Research_, pages 42058-42080. PMLR, 2023.
* [55] Yi Zhou, Parikshit Ram, Theodoros Salonidis, Nathalie Baracaldo, Horst Samulowitz, and Heiko Ludwig. Single-shot general hyper-parameter optimization for federated learning. In _The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023_. OpenReview.net, 2023.

More discussions

### More related works

**Partial fine-tuning**, i.e., updating a subset of modules of a pretrained network on a new dataset, has been studied in supervised settings [26; 35; 40]. In FL, PartialFed [42] adaptively decides whether each parameter is shared or personalized. However, it cannot generalize to testing clients that do not participate in the training. Recently, surgical fine-tuning [20] selectively fine-tunes a subset of blocks with a similar intuition that the type of distribution shift influences which part of the network to be adapted. Different from their method, we focus on the unsupervised setting and propose to refine the adaptation rate for each module.

**Hyperparameter optimization** is also related to our algorithm if considering adaptation rates as a set of hyperparameters. [19] first investigates the problem of federated hyperparameter tuning and proposed FedEX that leverages weight-sharing from neural architecture search to efficiently tune hyperparameters. [55] introduces FloRA that addresses use cases of tabular data and enables single-shot federated hyperparameter tuning. While these methods focus on improving the efficiency of hyperparameter optimization, our paper focuses on finding the optimal adaptation rates that benefit test-time personalization.

### Broader impacts and limitations

**Broader impacts** We are not aware of any potential negative societal impacts regarding our work to the best of our knowledge. For all the used data sets, there is no private personally identifiable information or offensive content.

**Limitations** One possible limitation is that we consider a fix global model for lower communication cost and better generalization, while it might be beneficial to also train a global model for easier personalization, which could be a promising future direction.

Theoretical analysis

In this section, we give theoretical proofs of convergence, and generalization of ATP.

### Approximation analysis

In this subsection, we give detailed proofs of Proposition 3.1 and 3.2 in Section 3 of the main text. These propositions show why certain types of distribution shifts can be handled by adapting certain layers in a neural network.

#### b.1.1 Proof of Proposition 3.1

**Proposition 3.1** (Adapting the last layer to handle label shift).: Consider two distribution \(p,q\) with \(p(\bm{x}|\bm{y})=q(\bm{x}|\bm{y})\) and \(p(\bm{y})\neq q(\bm{y})\). When a neural network is calibrated on \(p\), i.e., \(f(\bm{x};\bm{w})=p(\cdot|\bm{x})\), it is calibrated on \(q\) after adding \(\log\frac{q(\bm{y})}{p(\bm{y})}\) to the bias term of the final last layer.

Proof.: W.l.o.g., assuming the last layer of the neural network is a linear layer. Denoting \(g(\bm{x};\bm{w}_{g})\) as the input of the last layer, where \(\bm{x}\) is the input and \(\bm{w}_{g}\) is the model parameters for the feature extractor (i.e., all layers except for the last classification layer). Denote \(\bm{w}_{1},\cdots,\bm{w}_{K}\) as the weights of the last layer and \(b_{1},\cdots,b_{K}\) as the bias terms of the last layer, assuming \(K\) classes. Then we have

\[f(\bm{x};\bm{w})_{c}=\frac{\exp(\bm{w}_{c}^{\top}g(\bm{x};\bm{w}_{g})+b_{c})} {\sum_{c^{\prime}=1}^{K}\exp(\bm{w}_{c^{\prime}}^{\top}g(\bm{x};\bm{w}_{g})+b _{c^{\prime}})}\]

Since the neural network is calibrated on \(p\), for all class index \(c=1,\cdots,K\), we have

\[f(\bm{x},\bm{w})_{c}=p(\bm{y}=\bm{e}_{c}|\bm{x})\]

where \(\bm{e}_{c}\) is an one-hot vector with its \(c\)-th element as one. For distribution \(q\) with the same conditional distribution and different priori, by Bayes' theorem, \(\forall\bm{x},\bm{y}\)

\[q(\bm{y}|\bm{x})=\frac{q(\bm{x}|\bm{y})q(\bm{y})}{\sum_{\bm{y}}q(\bm{x}|\bm{y} )q(\bm{y})}=\frac{p(\bm{x}|\bm{y})q(\bm{y})}{\sum_{\bm{y}}p(\bm{x}|\bm{y})q( \bm{y})}=\frac{p(\bm{y}|\bm{x})\cdot\frac{q(\bm{y})}{p(\bm{y})}}{\sum_{\bm{y}} p(\bm{y}|\bm{x})\cdot\frac{q(\bm{y})}{p(\bm{y})}}\]

Therefore, we can calibrate the neural network on distribution \(q\) simply by adding \(\log\frac{q(\bm{y})}{p(\bm{y})}\) to the bias terms, i.e.,

\[f_{cal}(\bm{x};\bm{w}_{cal})_{c} =\frac{\exp(\bm{w}_{c}^{\top}g(\bm{x};\bm{w}_{g})+b_{c}+\log\frac {q(\bm{e}_{c})}{p(\bm{e}_{c})})}{\sum_{c^{\prime}=1}^{K}\exp(\bm{w}_{c^{\prime }}^{\top}g(\bm{x};\bm{w}_{g})+b_{c^{\prime}}+\log\frac{q(\bm{e}_{c^{\prime}})} {p(\bm{e}_{c^{\prime}})})}\] \[=\frac{\exp(\bm{w}_{c}^{\top}g(\bm{x};\bm{w}_{g})+b_{c})\cdot\frac {q(\bm{e}_{c})}{p(\bm{e}_{c})}}{\sum_{c^{\prime}=1}^{K}\exp(\bm{w}_{c^{\prime }}^{\top}g(\bm{x};\bm{w}_{g})+b_{c^{\prime}})\cdot\frac{q(\bm{e}_{c^{\prime}}) }{p(\bm{e}_{c^{\prime}})}}\] \[=\frac{p(\bm{e}_{c}|\bm{x})\cdot\frac{q(\bm{e}_{c})}{p(\bm{e}_{c} )}}{\sum_{c^{\prime}=1}^{K}p(\bm{e}_{c^{\prime}}|\bm{x})\cdot\frac{q(\bm{e}_{ c^{\prime}})}{p(\bm{e}_{c^{\prime}})}}\] \[=q(\bm{y}=\bm{e}_{c}|\bm{x})\]

#### b.1.2 Proof of Proposition 3.2

**Proposition 3.2** (Adapting the BN layer to handle feature shift [38]).: When the feature shift only causes differences in the first and second order moments of the feature activations \(\bm{z}=g(\bm{x})\) where \(g\) is the combination of layers before the BN layer, assuming independent activations, the feature shift can be removed by adapting running mean and variance of the BN layer.

Proof.: Denote the source and target feature (marginal) distributions to be \(p(\bm{x})\) and \(q(\bm{x})\). Given independent, activations, we only need to test the marginal distribution of each \(z\in\bm{z}=g(\bm{x})\). For each \(z\), since the feature shift only introduces differences in the first and second order moments, there exists \(\Delta\) and \(r>0\), s.t., \(\forall z_{t}\in\mathbb{R}\)

\[\Pr_{\bm{x}\sim q}(z\geq z_{t})=\Pr_{\bm{x}\sim p}\left(z\geq\frac{z_{t}- \Delta}{r}\right)\]

which indicates that the distribution of \(z\) is first shifted by \(\Delta\) and then scaled by \(r\). Such distribution shift in the feature activation can be removed by adapting the running mean \(\mu_{p}\) and variance \(\sigma_{p}^{2}\)

\[\mu_{q} =r\cdot\mu_{p}+\Delta\] \[\sigma_{q} =\sigma_{p}\cdot r\]

As a result, for all \(t\in\mathbb{R}\)

\[\Pr_{\bm{x}\sim q}\left(\frac{z-\mu_{q}}{\sigma_{q}}\geq t\right) =\Pr_{\bm{x}\sim q}(z\geq\mu_{q}+\sigma_{q}\cdot t)\] \[=\Pr_{\bm{x}\sim p}(z\geq\frac{\mu_{q}+\sigma_{q}\cdot t-\Delta} {r})\] \[=\Pr_{\bm{x}\sim p}(z\geq\mu_{p}+\sigma_{p}\cdot t)\] \[=\Pr_{\bm{x}\sim p}\left(\frac{z-\mu_{p}}{\sigma_{p}}\geq t\right)\]

which indicates that the feature shift is removed after normalization with running statistics \(\mu_{q},\sigma_{q}\)

### Convergence analysis

In this part, we show that ATP has the same convergence guarantee as FedAvg [31]. We first show in Lemma B.5 and B.10 that ATP preserves convexity and smoothness, which are two important conditions in the analysis of convergence. Then we formally prove the convergence of ATP in Theorem B.11.

#### b.2.1 Definitions: local and global objective

For clarity, we first formally define the data generation process, and local/global objectives for optimization.

Data generationWe consider a two-stage sampling process as illustrated in Figure 6.

* There are \(N\) source clients' distributions \(P^{\mathcal{S}_{1}},P^{\mathcal{S}_{2}},\cdots,P^{\mathcal{S}_{N}}\) and \(M\) target clients' distribution \(P^{\mathcal{T}1},P^{\mathcal{T}2},\cdots,P^{\mathcal{T}_{M}}\) i.i.d. drawn from a meta-distribution \(\mathcal{Q}\).
* For each source client \(i\)'s distribution \(P^{\mathcal{S}_{i}}\), there are \(K\)**data batches**\((\boldsymbol{X}_{1}^{\mathcal{S}_{i}},\boldsymbol{Y}_{1}^{\mathcal{S}_{i}})\), \((\boldsymbol{X}_{2}^{\mathcal{S}_{i}},\boldsymbol{Y}_{2}^{\mathcal{S}_{i}})\), \(\cdots,(\boldsymbol{X}_{K}^{\mathcal{S}_{i}},\boldsymbol{Y}_{K}^{\mathcal{S}_ {i}})\) drawn i.i.d. from \(P^{\mathcal{S}_{i}}\).
* Each batch consists of \(B\) samples, \((\boldsymbol{X}_{k}^{\mathcal{S}_{i}},\boldsymbol{Y}_{k}^{\mathcal{S}_{i}})= \{(\boldsymbol{x}_{k,b}^{\mathcal{S}_{i}},\boldsymbol{y}_{k,b}^{\mathcal{S}_{i} })\}_{b=1}^{B}\) where \(B\) is the batch size.
* For simplicity, we assume that all source client has the same number of batches \(K\) and batch size \(B\).

**Definition B.1** (Batch objective).: Define the batch objective of the \(k\)-th batch on client \(\mathcal{S}_{i}\) to be

\[F_{ik}(\boldsymbol{\alpha})=\frac{1}{B}\sum_{b=1}^{B}\ell_{CE}(f(\boldsymbol{x }_{k,b}^{\mathcal{S}_{i}},\boldsymbol{w}_{k}^{\mathcal{S}_{i}},\boldsymbol{y }_{k,b}^{\mathcal{S}_{i}})\]

where \(\boldsymbol{w}_{k}^{\mathcal{S}_{i}}=\boldsymbol{w}_{G}+(\boldsymbol{A} \boldsymbol{\alpha})\odot\boldsymbol{h}_{k}^{\mathcal{S}_{i}}\) and \(\boldsymbol{h}_{k}^{\mathcal{S}_{i}}\) is the update direction computed with \(\boldsymbol{X}_{k}^{\mathcal{S}_{i}}=\{\boldsymbol{x}_{k,b}^{\mathcal{S}_{i} }\}_{b=1}^{B}\) with Eq. (4) and (5).

**Definition B.2** (Local objective).: Define the local objective of client \(i\) to be

\[F_{i}(\boldsymbol{\alpha})=\frac{1}{K}\sum_{k=1}^{K}F_{ik}(\boldsymbol{\alpha})\]

**Definition B.3** (Global objective).: Define the global objective to be

\[F(\boldsymbol{\alpha})=\frac{1}{N}\sum_{i=1}^{N}F_{i}(\boldsymbol{\alpha})\]

Figure 6: Data generation process

#### b.2.2 ATP preserves convexity and smoothness

In this part, we show that ATP preserves convexity and smoothness, which are two important conditions in the analysis of convergence.

**Definition B.4** (Convexity).: A function \(f:\mathbb{R}^{D}\to\mathbb{R}\) is convex if for all \(\bm{x}_{1},\bm{x}_{2}\in\mathbb{R}^{D}\) and \(\lambda\in[0,1]\)

\[f(\lambda\bm{x}_{1}+(1-\lambda)\bm{x}_{2})\leq\lambda f(\bm{x}_{1})+(1- \lambda)f(\bm{x}_{2})\]

**Lemma B.5** (Convexity preserving).: _If \(\ell_{CE}(f(\bm{x};\bm{w}),\bm{y})\) is convex w.r.t. \(\bm{w}\) given any data sample \((\bm{x},\bm{y})\), then \(F_{i}(\bm{\alpha})\) is convex w.r.t. \(\bm{\alpha}\)._

Proof.: Noticing that \(\bm{w}_{k}^{\mathcal{S}_{i}}=\bm{w}_{G}+(\bm{A}\bm{\alpha})\odot\bm{h}_{k}^{ \mathcal{S}_{i}}\) is linear to \(\bm{\alpha}\), linear transformation preserves convexity. For any update direction \(\bm{h}_{k}^{\mathcal{S}_{i}}\) and data sample \((\bm{x}_{k,b}^{\mathcal{S}_{i}},\bm{y}_{k,b}^{\mathcal{S}_{i}})\), we find that

\[\ell_{CE}(f(\bm{x}_{k,b}^{\mathcal{S}_{i}};\bm{w}_{G}+(\bm{A}( \lambda\bm{\alpha}_{1}+(1-\lambda)\bm{\alpha}_{2}))\odot\bm{h}_{k}^{\mathcal{ S}_{i}},\bm{y}_{k,b}^{\mathcal{S}_{i}})\] \[=\ell_{CE}(f(\bm{x}_{k,b}^{\mathcal{S}_{i}};\lambda\left[\bm{w}_{ G}+(\bm{A}\bm{\alpha}_{1})\odot\bm{h}_{k}^{\mathcal{S}_{i}}\right]+(1-\lambda) \left[\bm{w}_{G}+(\bm{A}\bm{\alpha}_{2})\odot\bm{h}_{k}^{\mathcal{S}_{i}} \right],\bm{y}_{k,b}^{\mathcal{S}_{i}})\] \[\leq\lambda\ell_{CE}(f(\bm{x}_{k,b}^{\mathcal{S}_{i}};\bm{w}_{G}+ (\bm{A}\bm{\alpha}_{1})\odot\bm{h}_{k}^{\mathcal{S}_{i}},\bm{y}_{k,b}^{ \mathcal{S}_{i}})+(1-\lambda)\ell_{CE}(f(\bm{x}_{k,b}^{\mathcal{S}_{i}};\bm{w }_{G}+(\bm{A}\bm{\alpha}_{2})\odot\bm{h}_{k}^{\mathcal{S}_{i}},\bm{y}_{k,b}^{ \mathcal{S}_{i}})\]

i.e., \(\ell_{CE}(f(\bm{x}_{k,b}^{\mathcal{S}_{i}};\bm{w}_{G}+(\bm{A}\bm{\alpha})\odot \bm{h}_{k}^{\mathcal{S}_{i}}),\bm{y}_{k,b}^{\mathcal{S}_{i}})\) is convex w.r.t. \(\bm{\alpha}\).

Finally, since

\[F_{i}(\bm{\alpha})=\frac{1}{KB}\sum_{k=1}^{K}\sum_{b=1}^{B}\ell_{CE}(f(\bm{x}_{ k,b}^{\mathcal{S}_{i}};\bm{w}_{G}+(\bm{A}\bm{\alpha})\odot\bm{h}_{k}^{\mathcal{S}_{i }}),\bm{y}_{k,b}^{\mathcal{S}_{i}})\]

which is the average of \(KB\) convex functions, we have that \(F_{i}(\bm{\alpha})\) is also convex to \(\bm{\alpha}\). 

**Definition B.6** (\(\beta\)-smoothness).: A function \(f:\mathbb{R}^{D}\to\mathbb{R}\) is \(L\)-smoothness with \(\beta>0\) if for all \(\bm{x}_{1},\bm{x}_{2}\in\mathbb{R}^{D}\),

\[\|\nabla f(\bm{x}_{1})-\nabla f(\bm{x}_{2})\|_{2}\leq\beta\|\bm{x}_{1}-\bm{x}_ {2}\|_{2}\]

**Definition B.7** (\(H\)-module-wise-bounded update direction).: The update direction is \(H\)-module-wise-bounded for a data batch \(\bm{X}_{k}^{\mathcal{S}_{i}}\) if

\[\|(\bm{h}_{k}^{\mathcal{S}_{i}})^{[l]}\|_{2}\leq H,\quad\forall l=1,\cdots,d\]

where \((\bm{h}_{k}^{\mathcal{S}_{i}})^{[l]}\) is the update direction corresponding to the \(l\)-th module and \(d\) is the number of modules in the neural network.

**Lemma B.8** (Lipschitz parameter).: _If the update direction is \(H\)-module-wise-bounded for a data batch \(\bm{X}_{k}^{\mathcal{S}_{i}}\). Given two adaptation rates \(\bm{\alpha}_{1},\bm{\alpha}_{2}\) and the global model \(\bm{w}_{G}\), we have_

\[\|\bm{w}_{k}^{\mathcal{S}_{i}}(\bm{\alpha}_{1})-\bm{w}_{k}^{\mathcal{S}_{i}}( \bm{\alpha}_{2})\|_{2}\leq H\cdot\|\bm{\alpha}_{1}-\bm{\alpha}_{2}\|_{2}\]

_where \(\bm{w}_{k}^{\mathcal{S}_{i}}(\bm{\alpha}_{1})=\bm{w}_{G}+(\bm{A}\bm{\alpha}_{1}) \odot\bm{h}_{k}^{\mathcal{S}_{i}}\) is the personalized model updated with \(\bm{\alpha}_{1}\) as the adaptation rate._

Proof.: \[\|\bm{w}_{k}^{\mathcal{S}_{i}}(\bm{\alpha}_{1})-\bm{w}_{k}^{\mathcal{S }_{i}}(\bm{\alpha}_{2})\|_{2} =\|(\bm{w}_{G}+(\bm{A}\bm{\alpha}_{1})\odot\bm{h}_{k}^{\mathcal{S}_ {i}})-(\bm{w}_{G}+(\bm{A}\bm{\alpha}_{2})\odot\bm{h}_{k}^{\mathcal{S}_{i}})\|_{2}\] \[=\|(\bm{A}(\bm{\alpha}_{1}-\bm{\alpha}_{2}))\odot\bm{h}_{k}^{ \mathcal{S}_{i}}\|_{2}\] \[=\|\bm{h}_{k}^{\mathcal{S}_{i}}\odot(\bm{A}(\bm{\alpha}_{1}-\bm{ \alpha}_{2}))\|_{2}\] \[=\sqrt{\sum_{l=1}^{d}\left\|(\bm{h}_{k}^{\mathcal{S}_{i}})^{[l]} \right\|_{2}^{2}\left(\alpha_{1}^{[l]}-\alpha_{2}^{[l]}\right)^{2}}\] \[\leq\sqrt{\sum_{l=1}^{d}H^{2}\left(\alpha_{1}^{[l]}-\alpha_{2}^ {[l]}\right)^{2}}\] \[=H\cdot\|\bm{\alpha}_{1}-\bm{\alpha}_{2}\|_{2}\]_Remark B.9_.: Lemma B.8 indicates that when the adaptation rate is perturbed by a little, the personalized model parameter \(\bm{w}_{k}^{\mathcal{S}_{i}}\) is also only perturbed by a little.

**Lemma B.10** (Smoothness preserving).: _If(1) \(\ell_{CE}(f(\bm{x};\bm{w}),\bm{y})\) is \(\beta\)-smooth w.r.t. \(\bm{w}\) given any data sample \((\bm{x},\bm{y})\), and (2) the update direction \(\bm{h}_{k}^{\mathcal{S}_{i}}\) is \(H\)-module-wise-bounded for all data batches \(\bm{X}_{k}^{\mathcal{S}_{i}}\), then \(F_{i}(\bm{\alpha})\) is \((H^{2}\beta)\)-smoothness w.r.t. \(\bm{\alpha}\)._

Proof.: We first give an upper bound of \(\|\bm{A}^{\top}\text{diag}(\bm{h}_{k}^{\mathcal{S}_{i}})\|_{2}\) when \(\bm{h}_{k}^{\mathcal{S}_{i}}\) is \(H\)-module-wise-bounded. The update direction \(\bm{h}_{k}^{\mathcal{S}_{i}}\in\mathbb{R}^{D}\) is the concatenation of update directions for each module \(\{(\bm{h}_{k}^{\mathcal{S}_{i}})^{[l]}\}_{l=1}^{d}\), i.e.,

\[\left(\bm{h}_{k}^{\mathcal{S}_{i}}\right)^{\top}=\left[\left((\bm{h}_{k}^{ \mathcal{S}_{i}})^{[1]}\right)^{\top},\cdots,\left((\bm{h}_{k}^{\mathcal{S}_{ i}})^{[d]}\right)^{\top}\right]\]

where \((\bm{h}_{k}^{\mathcal{S}_{i}})^{[l]}\) is a column vector representing the update direction of the \(l\)-th module in the model. Similarly, any other vector \(\bm{v}\in\mathbb{R}^{D}\) can be correspondingly expressed as

\[\bm{v}^{\top}=\left[\left(\bm{v}^{[1]}\right)^{\top},\cdots, \left(\bm{v}^{[d]}\right)^{\top}\right]\]

Then,

\[\|\bm{A}^{\top}\text{diag}(\bm{h}_{k}^{\mathcal{S}_{i}})\|_{2} =\sup_{\bm{v}\in\mathbb{R}^{D}}\frac{\|\bm{A}^{\top}\text{diag}( \bm{h}_{k}^{\mathcal{S}_{i}})\bm{v}\|_{2}}{\|\bm{v}\|_{2}}\] \[=\sup_{\bm{v}\in\mathbb{R}^{D}}\frac{\|\bm{A}^{\top}(\bm{h}_{k}^{ \mathcal{S}_{i}}\odot\bm{v})\|_{2}}{\|\bm{v}\|_{2}}\] \[=\sup_{\bm{v}\in\mathbb{R}^{D}}\sqrt{\frac{\sum_{l=1}^{d}\left[ \left((\bm{h}_{k}^{\mathcal{S}_{i}})^{[l]}\right)^{\top}\bm{v}^{[l]}\right]^{ 2}}{\sum_{l=1}^{d}\left\|\bm{v}^{[l]}\right\|_{2}^{2}}}\] \[\leq\sup_{\bm{v}\in\mathbb{R}^{D}}\sqrt{\frac{\sum_{l=1}^{d}\left[ \left\|(\bm{h}_{k}^{\mathcal{S}_{i}})^{[l]}\right\|_{2}\cdot\left\|\bm{v}^{[l ]}\right\|_{2}\right]^{2}}{\sum_{l=1}^{d}\left\|\bm{v}^{[l]}\right\|_{2}^{2}}}\] \[\leq\sup_{\bm{v}\in\mathbb{R}^{D}}\sqrt{\frac{\sum_{l=1}^{d}\left[ \left.\left\|\bm{v}^{[l]}\right\|_{2}\right]^{2}}{\sum_{l=1}^{d}\left\|\bm{v} ^{[l]}\right\|_{2}^{2}}}\] (Definition B.7) \[=H\]

We then prove that for any \(H\)-module-wise-bounded update direction \(\bm{h}_{k}^{\mathcal{S}_{i}}\) and data sample \((\bm{x}_{k,b}^{\mathcal{S}_{i}},\bm{y}_{k,b}^{\mathcal{S}_{i}})\), we have \(\ell_{CE}(f(\bm{x}_{k,b}^{\mathcal{S}_{i}},\bm{w}_{k}^{\mathcal{S}_{i}}(\bm{ \alpha})),\bm{y}_{k,b}^{\mathcal{S}_{i}})\) is \(H^{2}\beta\)-smoothness w.r.t. \(\bm{\alpha}\).

\[\|\nabla_{\alpha_{1}}\ell_{CE}(f(\bm{x}_{k,b}^{\mathcal{S}_{i}}; \bm{w}_{k}^{\mathcal{S}_{i}}(\bm{\alpha}_{1})),\bm{y}_{k,b}^{\mathcal{S}_{i}})- \nabla_{\alpha_{2}}\ell_{CE}(f(\bm{x}_{k,b}^{\mathcal{S}_{i}};\bm{w}_{k}^{ \mathcal{S}_{i}}(\bm{\alpha}_{2})),\bm{y}_{k,b}^{\mathcal{S}_{i}})\|_{2}\] \[=\|\bm{A}^{\top}(\bm{h}_{k}^{\mathcal{S}_{i}}\odot\nabla_{\bm{w} _{k}^{\mathcal{S}_{i}}(\bm{\alpha}_{1})}\ell_{CE}(f(\bm{x}_{k,b}^{\mathcal{S} _{i}};\bm{w}_{k}^{\mathcal{S}_{i}}(\bm{\alpha}_{1})),\bm{y}_{k,b}^{\mathcal{ S}_{i}})-\bm{A}^{\top}(\bm{h}_{k}^{\mathcal{S}_{i}}\odot\nabla_{\bm{w}_{k}^{ \mathcal{S}_{i}}(\bm{\alpha}_{2})}\ell_{CE}(f(\bm{x}_{k,b}^{\mathcal{S}_{i}}; \bm{w}_{k}^{\mathcal{S}_{i}}(\bm{\alpha}_{2})),\bm{y}_{k,b}^{\mathcal{S}_{i}}) \|_{2}\] \[\leq\|\bm{A}^{\top}\text{diag}(\bm{h}_{k}^{\mathcal{S}_{i}})\|_{2} \cdot\|\nabla_{\bm{w}_{k}^{\mathcal{S}_{i}}(\bm{\alpha}_{1})}\ell_{CE}(f(\bm{x} _{k,b}^{\mathcal{S}_{i}};\bm{w}_{k}^{\mathcal{S}_{i}}(\bm{\alpha}_{1})),\bm{y }_{k,b}^{\mathcal{S}_{i}})-\nabla_{\bm{w}_{k}^{\mathcal{S}_{i}}(\bm{\alpha}_{2}) }\ell_{CE}(f(\bm{x}_{k,b}^{\mathcal{S}_{i}};\bm{w}_{k}^{\mathcal{S}_{i}}( \bm{\alpha}_{2})),\bm{y}_{k,b}^{\mathcal{S}_{i}})\|_{2}\] \[\leq\|\bm{A}^{\top}\text{diag}(\bm{h}_{k}^{\mathcal{S}_{i}})\|_{2} \cdot\beta\cdot\|\bm{w}_{k}^{\mathcal{S}_{i}}(\bm{\alpha}_{1})-\bm{w}_{k}^{ \mathcal{S}_{i}}(\bm{\alpha}_{2})\|_{2}\] (Definition B.6) \[\leq\|\bm{A}^{\top}\text{diag}(\bm{h}_{k}^{\mathcal{S}_{i}})\|_{2} \cdot\beta\cdot H\cdot\|\bm{\alpha}_{1}-\bm{\alpha}_{2}\|_{2}\] (Lemma B.8) \[\leq H^{2}\cdot\beta\cdot\|\bm{\alpha}_{1}-\bm{\alpha}_{2}\|_{2}\]

#### b.2.3 Convergence of ATP under FedAvg framework

Finally, we show that with preservation of convexity and smoothness, ATP shares the same convergence guarantee as FedAvg [31]. We apply the proof in [46].

**Theorem B.11** (Convergence of ATP).: _Assume that_

1. _At any round_ \(t\)_, each client takes_ \(\tau\) _SGD steps with learning rate_ \(\eta\)_._
2. _Full participation, i.e., each source client participates every round_
3. \(\ell_{CE}(f(\bm{x};\bm{w}),\bm{y})\) _is convex and_ \(\beta\)_-smooth w.r.t._ \(\bm{w}\) _given any data sample_ \((\bm{x},\bm{y})\)_._
4. _The update direction_ \(\bm{h}_{k}^{\mathcal{S}_{i}}\) _is_ \(H\)_-module-wise-bounded for all_ \(i,j\)__
5. _Bounded inner variance: for any_ \(\bm{\alpha}\) _and client_ \(i\)_,_ \[\mathbb{E}_{j}\nabla_{\bm{\alpha}}F_{ij}(\bm{\alpha})=\nabla_{\bm{\alpha}}F_{ i}(\bm{\alpha}),\quad\mathbb{E}_{j}\|\nabla_{\bm{\alpha}}F_{ij}(\bm{\alpha})- \nabla_{\bm{\alpha}}F_{i}(\bm{\alpha})\|_{2}^{2}\leq\sigma^{2}\]
6. _Bounded outer variance: for any_ \(\bm{\alpha}\) _and client_ \(i\)_,_ \[\|\nabla_{\bm{\alpha}}F_{i}(\bm{\alpha})-\nabla_{\bm{\alpha}}F(\bm{\alpha})\|_ {2}^{2}\leq\zeta^{2}\]

_If the client learning rate satisfies \(\eta\geq\frac{1}{4H^{2}\beta}\), then one has_

\[\mathbb{E}\left[\frac{1}{\tau T}\sum_{t=0}^{T-1}\sum_{k=1}^{\tau}F(\bar{\bm{ \alpha}}^{t,k})-F(\bm{\alpha}^{*})\right]\leq\frac{\|\bm{\alpha}_{G}^{0}-\bm{ \alpha}^{*}\|_{2}^{2}}{2\eta\tau T}+\frac{\eta\sigma^{2}}{N}+4\tau\eta^{2}H^{2 }\beta\sigma^{2}+18\tau^{2}\eta^{2}H^{2}\beta\zeta^{2}\]

_where \(\bm{\alpha}^{*}=\operatorname*{arg\,min}_{\bm{\alpha}}F(\bm{\alpha})\) and \(\bar{\bm{\alpha}}^{t,k}=\frac{1}{N}\sum_{i=1}^{N}\bm{\alpha}_{i}^{t,k}\). \(\bm{\alpha}_{i}^{t,k}\) is the local adaptation rates after \(t\) communication rounds and \(k\) local epochs._

Proof.: The optimization process of ATP is similar as FedAvg [31], where the difference is that ATP adapts the adaptation rates instead of model parameter. Lemma B.5 and B.10 that ATP preserves convexity and smoothness, i.e., for each client \(i\), \(F_{i}(\bm{\alpha})\) is convex and \((H^{2}\beta)\)-smoothness. Therefore, we can apply Theorem 1 in [46] to complete the proof. 

_Remark B.12_.: The convergence rate of ATP is \(\mathcal{O}(\frac{1}{\tau T})\).

### Generalization analysis

In this part, we studied how an adaptation rate \(\bm{\alpha}\) learned by ATP that performs well on source clients can generalize to target clients. More specifically, we are interested in _how many different source clients are required to ensure a certain generalization error_. Similar to most of the other generalization analysis, we (1) derive generalization bound for any fixed hypothesis (\(\bm{\alpha}\)), and (2) quantify the size of hypothesis space.

#### b.3.1 Definitions: data generation and error rates

We first formally define the error rates.

**Definition B.13** (Error rate for one data sample).: Let \(f(\cdot;\bm{w}_{k}^{\mathcal{S}_{i}}):\mathcal{X}\rightarrow\Delta^{|\mathcal{ Y}|-1}\) be the neural network with model parameters \(\bm{w}_{k}^{\mathcal{S}_{i}}\) that takes _one_ data sample \(\bm{x}_{k,b}^{\mathcal{S}_{i}}\) as input and outputs a probability distribution over the label space, i.e., \(f(\bm{x}_{k,b}^{\mathcal{S}_{i}};\bm{w}_{k}^{\mathcal{S}_{i}})\geq\bm{0}\) and \(\bm{1}^{\top}f(\bm{x}_{k,b}^{\mathcal{S}_{i}};\bm{w}_{k}^{\mathcal{S}_{i}})=1\). Given adapted model parameters \(\bm{w}_{k}^{\mathcal{S}_{i}}\), define the error rate on one data sample \((\bm{x}_{k,b}^{\mathcal{S}_{i}},\bm{y}_{k,b}^{\mathcal{S}_{i}})\) to be

\[\hat{e}_{ikb}(\bm{w}_{k}^{\mathcal{S}_{i}}):=1-(\bm{y}_{k,b}^{\mathcal{S}_{i}} )^{\top}f(\bm{x}_{k,b}^{\mathcal{S}_{i}};\bm{w}_{k}^{\mathcal{S}_{i}})\]

_Remark B.14_.: Definition B.13 is equivalent to the expected misclassification rate if when making random decision based on the output probability \(f(\bm{x}_{k,b}^{\mathcal{S}_{i}};\bm{w}_{k}^{\mathcal{S}_{i}})\).

**Definition B.15** (Error rate for one data batch).: Given global model parameter \(\bm{w}_{G}\), adaptation rate \(\bm{\alpha}\), and a batch of data \(\bm{X}_{k}^{\mathcal{S}_{i}}=\{\bm{x}_{k,b}^{\mathcal{S}_{i}}\}_{b=1}^{B},\bm{ Y}_{k}^{\mathcal{S}_{i}}=\{\bm{y}_{k,b}^{\mathcal{S}_{i}}\}_{b=1}^{B}\), define the error rate for one data batch

\[\hat{\varepsilon}_{ik}(\bm{\alpha}):=\hat{e}_{ik}(\bm{w}_{k}^{\mathcal{S}_{i}} ):=\frac{1}{B}\sum_{b=1}^{B}\hat{e}_{ikb}(\bm{w}_{k}^{\mathcal{S}_{i}})\]

where

\[\bm{w}_{k}^{\mathcal{S}_{i}}=\bm{w}_{G}+(\bm{A}\bm{\alpha})\odot\bm{h}_{k}^{ \mathcal{S}_{i}}\]

and \(\bm{h}_{k}^{\mathcal{S}_{i}}\) is the update direction computed with \(\bm{X}_{k}^{\mathcal{S}_{i}}\).

**Definition B.16** (Error rates for one client).: Given global model parameter \(\bm{w}_{G}\), adaptation rate \(\bm{\alpha}\), and a source client \(\mathcal{S}_{i}\) with \(K\) data batches \(\{(\bm{X}_{k}^{\mathcal{S}_{i}},\bm{Y}_{k}^{\mathcal{S}_{i}})\}_{k=1}^{K}\), define the _empirical error rate_ for source client \(\mathcal{S}_{i}\)

\[\hat{\varepsilon}_{i}(\bm{\alpha}):=\frac{1}{K}\sum_{k=1}^{K}\hat{\varepsilon }_{ik}(\bm{\alpha})\]

Also, define the _expected error rate_ for source client \(\mathcal{S}_{i}\)

\[\varepsilon_{i}(\bm{\alpha}):=\mathbb{E}_{(\bm{X}_{k}^{\mathcal{S}_{i}},\bm{ Y}_{k}^{\mathcal{S}_{i}})\sim PS_{i}}\left[\hat{\varepsilon}_{ik}(\bm{\alpha})\;\right]\]

_Remark B.17_.: \(\hat{\varepsilon}_{i}(\bm{\alpha})\) quantifies the error rate on client \(\mathcal{S}_{i}\)'s finite dataset \(\mathbb{D}^{\mathcal{S}_{i}}=\{(\bm{X}_{k}^{\mathcal{S}_{i}},\bm{Y}_{k}^{ \mathcal{S}_{i}})\}_{k=1}^{K}\). \(\varepsilon_{i}(\bm{\alpha})\) quantifies the expected error rate on a _new data batch_ from client \(\mathcal{S}_{i}\). Notice that the same definition applies to target clients.

**Definition B.18** (Source error rate and expected target error rate).: Given global model parameter \(\bm{w}_{G}\), adaptation rate \(\bm{\alpha}\), and \(N\) source client \(\mathcal{S}_{1},\cdots,\mathcal{S}_{N}\), each with \(K\) data batches \(\{(\bm{X}_{k}^{\mathcal{S}_{i}},\bm{Y}_{k}^{\mathcal{S}_{i}})\}_{k=1}^{K}\), define the _training error rate_

\[\hat{\varepsilon}(\bm{\alpha}):=\frac{1}{K}\sum_{i=1}^{N}\hat{\varepsilon}_{i} (\bm{\alpha})\]

Also, define the _expected testing error rate_

\[\varepsilon(\bm{\alpha}):=\mathbb{E}_{P^{\mathcal{S}_{i}}\sim\mathcal{Q}} \left[\varepsilon_{i}(\bm{\alpha})\;|\;P^{\mathcal{S}_{i}}\right]=\mathbb{E} _{P^{\mathcal{S}_{i}}\sim\mathcal{Q}}\mathbb{E}_{(\bm{X}_{k}^{\mathcal{S}_{i}}, \bm{Y}_{k}^{\mathcal{S}_{i}})\sim P^{\mathcal{S}_{i}}}\left[\hat{\varepsilon }_{ik}(\bm{\alpha})\right]\]

_Remark B.19_.: \(\hat{\varepsilon}(\bm{\alpha})\) quantifies the averaged error rate across source clients' finite samples. \(\varepsilon(\bm{\alpha})\) quantifies the expected error rate on a _new data batch_ from a _new client_ (target client). Noting that both error rates are defined with respect to the personalized model after adaptation.

#### b.3.2 Generalization bound for one hypothesis

Next, we derive generalization bounds for one fixed adaptation rate \(\bm{\alpha}\). Since we consider fixed \(\bm{\alpha}\), for clarity, we denote

\[Z_{ik} :=\hat{\varepsilon}_{ik}(\bm{\alpha})\] \[\bar{Z}_{i\cdot} :=\frac{1}{K}\sum_{k=1}^{K}Z_{ik}=\hat{\varepsilon}_{i}(\bm{\alpha})\] \[\mu_{i} :=\mathbb{E}_{(\bm{X}_{k}^{S_{i}},\bm{Y}_{k}^{S_{i}})\sim P^{S_{ i}}}[Z_{ik}]=\varepsilon_{i}(\bm{\alpha})\] \[\bar{Z}_{\cdot\cdot} :=\frac{1}{N}\sum_{i=1}^{N}\bar{Z}_{i\cdot}=\hat{\epsilon}(\bm{ \alpha})\] \[\bar{\mu}_{\cdot} :=\frac{1}{N}\sum_{i=1}^{N}\mu_{i}\] \[\mu :=\mathbb{E}_{P^{S_{i}}\sim\mathcal{Q}}\mu_{i}=\epsilon(\bm{\alpha})\]

Intuitively, with enough number of source clients and number of batches, we have \(\bar{Z}_{\cdot\cdot}\approx\bar{\mu}_{\cdot}\approx\mu\).

**Lemma B.20** (Hoeffding's inequality).: _Let \(X_{1},\cdots,X_{n}\) be independent random variables such that \(a_{i}\leq X_{i}\leq b_{i}\) almost surely. Consider the sum of these random variables \(S_{n}=X_{1}+\cdots+X_{n}\). For all \(\epsilon>0\),_

\[\Pr(S_{n}-\mathbb{E}[S_{n}]\geq\epsilon)\leq\exp\left(-\frac{2\epsilon^{2}}{ \sum_{i=1}^{n}(b_{i}-a_{i})^{2}}\right)\]

Proof.: Please refer to [13] 

**Lemma B.21** (Concentration of averaged client expected error rates).: _For any \(\epsilon>0\), we have_

\[\Pr(\bar{\mu}_{\cdot}-\mu\geq\epsilon)\leq\exp(-2N\epsilon^{2})\]

Proof.: Notice that \(\mu_{1},\cdots,\mu_{N}\) are independent given \(\mathcal{Q}\). For all \(i=1,\cdots,N\), \(\mathbb{E}\mu_{i}=\mu\) and \(0\leq\mu_{i}\leq 1\). Therefore,

\[\Pr(\bar{\mu}_{\cdot}-\mu\geq\epsilon) =\Pr\left(\sum_{i=1}^{N}\mu_{i}-N\mu\geq N\epsilon\right)\] \[=\Pr\left(\sum_{i=1}^{N}\mu_{i}-\mathbb{E}\left[\sum_{i=1}^{N}\mu _{i}\right]\geq N\epsilon\right)\] \[\leq\exp\left(-\frac{2\cdot(N\epsilon)^{2}}{N\cdot(1-0)^{2}}\right)\] (Hoeffding's inequality) \[=\exp(-2N\epsilon^{2})\]

**Lemma B.22** (Concentration of client empirical error rate).: _For any \(\epsilon>0\),_

\[\Pr\left(\bar{Z}_{\cdot\cdot}-\bar{\mu}_{\cdot}\geq\epsilon\right)\leq\exp(-2 NK\epsilon^{2})\]

Proof.: Given distributions \(P^{\mathcal{S}_{1}},\cdots,P^{\mathcal{S}_{N}}\), we have \(Z_{11},\cdots,Z_{1K},Z_{21},\cdots,Z_{NK}\) are independent. For any \(i=1,\cdots,N\) and \(k=1,\cdots,K\), we have \(\mathbb{E}_{(\bm{X}_{k}^{S_{i}},\bm{Y}_{k}^{S_{i}})\sim P^{\mathcal{S}_{i}}} \bar{Z}_{ik}=\mu_{i}\) and \(0\leq Z_{ik}\leq 1\). Therefore,

\[\Pr\left(\bar{Z}_{\cdot\cdot}-\bar{\mu}_{\cdot}\geq\epsilon\mid P^{\mathcal{S} _{1}},\cdots,P^{\mathcal{S}_{N}}\right)=\Pr\left(\sum_{i=1}^{N}\sum_{k=1}^{K} Z_{ik}-\sum_{i=1}^{N}K\mu_{i}\geq NK\epsilon\left|\right.\right.\]

[MISSING_PAGE_EMPTY:24]

#### b.3.3 Generalization bound for hypothesis space (proof of Theorem 5.1)

Finally, we derive the generalization bound for the hypothesis space. We first show in Lemma B.27 that \(\hat{\varepsilon}_{ij}(\bm{\alpha})\) is \((LH)\)-Lipschitz to \(\bm{\alpha}\), then we apply standard generalization analysis in Theorem 5.1 based on covering number [32].

**Definition B.25** (\(L\)-Lipschitz).: The neural network \(f(\bm{x};\bm{w})\) is \(L\)-Lipschitz w.r.t. \(\bm{w}\), if \(\forall\bm{x}\) and \(\bm{w}_{1},\bm{w}_{2}\).

\[\|f(\bm{x};\bm{w}_{1})-f(\bm{x};\bm{w}_{2})\|_{2}\leq L\cdot\|\bm{w}_{1}-\bm{ w}_{2}\|_{2}\]

**Definition B.26** (\(H\)-module-wise-bounded update direction).: The update direction is \(H\)-module-wise-bounded for a data batch \(\bm{X}_{k}^{\mathcal{S}_{i}}\) if

\[\|(\bm{h}_{k}^{\mathcal{S}_{i}})^{[l]}\|_{2}\leq H,\quad\forall l=1,\cdots,d\]

where \((\bm{h}_{k}^{\mathcal{S}_{i}})^{[l]}\) is the update direction corresponding to the \(l\)-th module and \(d\) is the number of modules in the neural network.

**Lemma B.27** (Lipschitz error rate).: _Given a data batch \((\bm{X}_{k}^{\mathcal{S}_{i}},\bm{Y}_{k}^{\mathcal{S}_{i}})\), if the update direction is \(H\)-module-wise-bounded, given any two adaptation rates \(\bm{\alpha}_{1},\bm{\alpha}_{2}\) and the global model \(\bm{w}_{G}\), we have_

\[|\hat{\varepsilon}_{ij}(\bm{\alpha}_{1})-\hat{\varepsilon}_{ij}(\bm{\alpha}_ {2})|\leq LH\cdot\|\bm{\alpha}_{1}-\bm{\alpha}_{2}\|_{2}\]

Proof.: \[|\hat{\varepsilon}_{ij}(\bm{\alpha}_{1})-\hat{\varepsilon}_{ij}( \bm{\alpha}_{2})|\] \[=\left|\left(\frac{1}{B}\sum_{b=1}^{B}\left(1-(\bm{y}_{k,b}^{ \mathcal{S}_{i}})^{\top}f(\bm{x}_{k,b}^{\mathcal{S}_{i}};\bm{w}_{k}^{ \mathcal{S}_{i}}(\bm{\alpha}_{1}))\right)\right)-\left(\frac{1}{B}\sum_{b=1}^ {B}\left(1-(\bm{y}_{k,b}^{\mathcal{S}_{i}})^{\top}f(\bm{x}_{k,b}^{\mathcal{S} _{i}};\bm{w}_{k}^{\mathcal{S}_{i}}(\bm{\alpha}_{2}))\right)\right)\right|\] \[=\left|\frac{1}{B}\sum_{b=1}^{B}(\bm{y}_{k,b}^{\mathcal{S}_{i}})^ {\top}\left(f(\bm{x}_{k,b}^{\mathcal{S}_{i}};\bm{w}_{k}^{\mathcal{S}_{i}}( \bm{\alpha}_{1}))-f(\bm{x}_{k,b}^{\mathcal{S}_{i}};\bm{w}_{k}^{\mathcal{S}_{i }}(\bm{\alpha}_{2}))\right)\right|\] \[\leq\frac{1}{B}\sum_{b=1}^{B}\|\bm{y}_{k,b}^{\mathcal{S}_{i}}\|_ {2}\cdot\left\|f(\bm{x}_{k,b}^{\mathcal{S}_{i}};\bm{w}_{k}^{\mathcal{S}_{i}}( \bm{\alpha}_{1}))-f(\bm{x}_{k,b}^{\mathcal{S}_{i}};\bm{w}_{k}^{\mathcal{S}_{i }}(\bm{\alpha}_{2}))\right\|_{2}\] \[\leq\frac{1}{B}\sum_{b=1}^{B}\|\bm{y}_{k,b}^{\mathcal{S}_{i}}\|_ {2}\cdot L\cdot\left\|\bm{w}_{k}^{\mathcal{S}_{i}}(\bm{\alpha}_{1})-\bm{w}_{ k}^{\mathcal{S}_{i}}(\bm{\alpha}_{2})\right\|_{2}\] ( \[L\] -Lipschitz model) \[\leq\frac{1}{B}\sum_{b=1}^{B}\|\bm{y}_{k,b}^{\mathcal{S}_{i}}\|_ {2}\cdot L\cdot H\cdot\left\|\bm{\alpha}_{1}-\bm{\alpha}_{2}\right\|_{2}\] (Lemma B.8) \[=LH\cdot\left\|\bm{\alpha}_{1}-\bm{\alpha}_{2}\right\|_{2}\]

_Remark B.28_.: Intuitively, Lemma B.27 shows that small change in \(\bm{\alpha}\) will result in bounded change on \(\hat{\varepsilon}_{ij}(\bm{\alpha})\).

**Corollary B.29**.: \(\hat{\epsilon}(\bm{\alpha})\) _and \(\epsilon(\bm{\alpha})\) are \((LH)\)-Lipschitz w.r.t. \(\bm{\alpha}\)._

Proof.: \(\hat{\epsilon}(\bm{\alpha})\) and \(\epsilon(\bm{\alpha})\) are expectations of \(\hat{\varepsilon}_{ij}(\bm{\alpha})\) given the empirical and expected distribution of \((\bm{X}_{k}^{\mathcal{S}_{i}},\bm{Y}_{k}^{\mathcal{S}_{i}})\). Lipschitz property is preserved. 

**Theorem 5.1** (Generalization for hypothesis space).: Let \(\mathcal{H}=\{\bm{\alpha}:\|\bm{\alpha}\|_{2}\leq R\}\) be the hypothesis space (space of adaptation rates), \(N\) be the number of source clients, and \(K\) be the number of data batches on each source client. Assuming (1) \(L\)-Lipschitz model, and (2) \(H\)-module-wise-bounded update direction. For any fixed global model \(\bm{w}_{G}\) and any \(\epsilon>0\), we have

\[\Pr(\sup_{\bm{\alpha}\in\mathcal{H}}|\varepsilon(\bm{\alpha})-\hat{\varepsilon} (\bm{\alpha})|\geq\epsilon)\leq\left(\frac{12LHR}{\epsilon}\right)^{d}\cdot 4 \exp\left(-\frac{NK\epsilon^{2}}{2(\sqrt{K}+1)^{2}}\right)\] (10)

where \(\hat{\varepsilon}(\bm{\alpha})\) is the average **post-adaptation** error rate on source clients, and \(\varepsilon(\bm{\alpha})\) is the expected **post-adaptation** error rate on clients' population.

Proof.: We use covering number to derive the generalization bound [32]. Define estimation error

\[\Delta_{\epsilon}(\bm{\alpha})=\varepsilon(\bm{\alpha})-\hat{\varepsilon}(\bm{ \alpha})\]

Then,

\[|\Delta_{\epsilon}(\bm{\alpha}_{1})-\Delta_{\epsilon}(\bm{\alpha}_{2})| =\left|\left[\varepsilon(\bm{\alpha}_{1})-\hat{\varepsilon}(\bm{ \alpha}_{1})\right]-\left[\varepsilon(\bm{\alpha}_{2})-\hat{\varepsilon}(\bm{ \alpha}_{2})\right]\right|\] \[\leq\left|\varepsilon(\bm{\alpha}_{1})-\varepsilon(\bm{\alpha}_ {2})\right|+\left|\hat{\varepsilon}(\bm{\alpha}_{1})-\hat{\varepsilon}(\bm{ \alpha}_{2})\right|\] \[\leq 2LH\cdot\|\bm{\alpha}_{1}-\bm{\alpha}_{2}\|_{2}\] (Corollary B.29)

\(\mathcal{A}=\{\bm{\alpha}:\|\bm{\alpha}\|_{2}\leq R,\bm{\alpha}\in\mathbb{R} ^{d}\}\) can be covered by \(K=\mathcal{N}_{2}(R,r)\) L2 balls with radius \(r=\frac{\epsilon}{4LH}\). Lemma 6.27 in [32] shows that

\[S=\mathcal{N}_{2}\left(R,r\right)\leq\left(\frac{3R}{r}\right)^{d}=\left( \frac{12LHR}{\epsilon}\right)^{d}\]

Denote these L2 balls to be \(B_{1},\cdots,B_{S}\),

\[\Pr\left(\sup_{\bm{\alpha}\in\mathcal{A}}\left|\varepsilon(\bm{\alpha})-\hat{ \varepsilon}(\bm{\alpha})\right|\geq\epsilon\right)\leq\sum_{s=1}^{S}\Pr \left(\sup_{\bm{\alpha}\in B_{s}}\left|\varepsilon(\bm{\alpha})-\hat{ \varepsilon}(\bm{\alpha})\right|\geq\epsilon\right)\]

For each ball \(B_{s}\), \(s=1,\cdots,S\), denote the center to be \(\bm{\alpha}_{s}\). For any \(\bm{\alpha}\in B_{s}\), we have \(\|\bm{\alpha}-\bm{\alpha}_{s}\|\leq\frac{\epsilon}{4LH}\), therefore

\[|\Delta_{\epsilon}(\bm{\alpha}_{s})-\Delta_{\epsilon}(\bm{\alpha})|\leq 2LH \cdot\|\bm{\alpha}-\bm{\alpha}_{s}\|\leq\frac{\epsilon}{2}\]

Intuitively, every \(\bm{\alpha}\in B_{s}\) has similar error rate. Therefore, the error rate for the whole ball is upper bounded, as long as the center \(\bm{\alpha}_{s}\) has a small error rate

\[\Pr\left(\sup_{\bm{\alpha}\in B_{s}}\left|\varepsilon(\bm{\alpha })-\hat{\varepsilon}(\bm{\alpha})\right|\geq\epsilon\right) =\Pr\left(\sup_{\bm{\alpha}\in B_{s}}\left|\Delta_{\epsilon}( \bm{\alpha})\right|\geq\epsilon\right)\] \[\leq\Pr\left(\sup_{\bm{\alpha}\in B_{s}}\left[\left|\Delta_{ \epsilon}(\bm{\alpha}_{s})\right|+\left|\Delta_{\epsilon}(\bm{\alpha}_{s})- \Delta_{\epsilon}(\bm{\alpha})\right|\right]\geq\epsilon\right)\] \[\leq\Pr\left(\sup_{\bm{\alpha}\in B_{s}}\left|\Delta_{\epsilon}( \bm{\alpha}_{s})\right|+\frac{\epsilon}{2}\geq\epsilon\right)\] \[=\Pr\left(\left|\varepsilon(\bm{\alpha}_{s})-\hat{\varepsilon}( \bm{\alpha}_{s})\right|\geq\frac{\epsilon}{2}\right)\]

Finally, by Proposition B.23, for each \(\bm{\alpha}_{s}\)

\[\Pr\left(\left|\varepsilon(\bm{\alpha}_{s})-\hat{\varepsilon}(\bm{\alpha}_{s })\right|\geq\frac{\epsilon}{2}\right)\leq 4\exp\left(-\frac{NK\epsilon^{2}}{2( \sqrt{K}+1)^{2}}\right)\]

Put all together

\[\Pr\left(\sup_{\bm{\alpha}\in\mathcal{A}}\left|\varepsilon(\bm{\alpha})-\hat{ \varepsilon}(\bm{\alpha})\right|\geq\epsilon\right)\leq\left(\frac{12LHR}{ \epsilon}\right)^{d}\cdot 4\exp\left(-\frac{NK\epsilon^{2}}{2(\sqrt{K}+1)^{2}}\right)\]Additional experiments

### Detailed experiment settings

#### c.1.1 CIFAR-10 experiments

Data preparationWe use a benchmarking three-way split [50]: we randomly split the dataset to 300 clients, 240 of them are source clients and 60 are target clients. Each source client has 160 training samples and 40 validation samples, while each target client has 200 testing samples. We simulate three kinds of distribution shifts: feature shift, label shift, and hybrid shift. For feature shift, we follow [12, 17], randomly apply 15 different kinds of corruptions to the source clients (Figure 6(a)), and 4 new kinds of corruptions to the target clients (Figure 6(b)) to test the generalization of ATP. The corruption severity is randomly selected from \(\{1,2,3,4,5\}\). For label shift, we use the step partition [5], where each client has 8 minor classes with 5 images per class, and 2 major classes with 80 images per class. For the hybrid shift, we apply both step partition and feature corruptions.

Global model trainingWe first train a global model with FedAvg [31] over the training sets of source clients.

* ResNet-18: The global model is ResNet-18 with ImageNet pretrained parameter (provided by torchvision). We train the global model for \(T=200\) communication rounds with full participation (cohort size \(C=240\)), local epochs \(E=1\), learning rate \(\eta=0.01\) and batch size \(B=20\).
* Shallow CNN: The global model is a randomly initialized 5-layer CNN. We train the global model for \(T=200\) communication rounds with full participation (cohort size \(C=240\)), local epochs \(E=1\), learning rate \(\eta=0.1\) and batch size \(B=20\).

ATP trainingWe initialize the adaptation rates as a all-zero vector, and optimize it over the validation sets of source clients. We optimize the adaptation rates for \(T=200\) (for ResNet-18) or \(400\) (for Shallow CNN) communication rounds with partial participation (cohort size \(C=60\)), learning rate \(\eta=0.1\) and batch size \(B=20\).

ATP testingWe test the optimized adaptation rates on each target client. We use batch size \(B=20\) by default, and test different batch size in Subsection 6.3.

#### c.1.2 CIFAR-100 experiments

Data preparationThe data preparation is similar to CIFAR-10 experiments. The only difference is for label shift, each client has 98 minor classes with 1 image per class, and 2 major classes with 51 images per class. Same partition is applied to hybrid shift.

Global model trainingWe first train a global model with FedAvg [31] over the training sets of source clients. The global model is ResNet-18 with ImageNet pretrained parameter (provided by torchvision). We train the global model for \(T=200\) communication rounds with full participation (cohort size \(C=240\)), local epochs \(E=1\), learning rate \(\eta=0.01\) and batch size \(B=20\).

Figure 7: \(15+4\) different corruptions we use to construct feature shift

ATP trainingWe initialize the adaptation rates as a all-zero vector, and optimize it over the validation sets of source clients. We optimize the adaptation rates for \(T=200\) communication rounds with partial participation (cohort size \(C=60\)), learning rate \(\eta=0.1\) and batch size \(B=20\).

ATP testingWe test the optimized adaptation rates on each target client. We use batch size \(B=20\).

#### c.1.3 Digits-5 experiments

Data preparationDigits-5 dataset contains five domains: MNIST, SVHN, USPS, SynthDigits, and MNIST-M. We adopt the leave-one-domain-out evaluation protocol [10], i.e., one domain is chosen as the held-out testing domain, and the remaining domains are regarded as source training domains. We follow the data preprocessing in [25], while additionally applying step partition to inject label shift. Each domain is divided into 10 clients, leading to a total of 40 source clients and 10 target clients. Consequently, each client ends up with approximately 743 images spread across 10 classes. Each source client has \(80\%\) of its samples as training set and the remained \(20\%\) as testing set. Each client has 2 major classes and 8 minor class, where the ratio of images per class is approximately \(16:1\) (the same as our CIFAR-10 experiments). Since there is already domain shift, we do not add corruptions.

Global model trainingWe first train a global model with FedAvg [31] over the training sets of source clients. The global model is ResNet-18 with ImageNet pretrained parameter (provided by torchvision). We train the global model for \(T=200\) communication rounds with full participation (cohort size \(C=50\)), local epochs \(E=1\), learning rate \(\eta=0.01\) and batch size \(B=20\).

ATP trainingWe initialize the adaptation rates as a all-zero vector, and optimize it over the validation sets of source clients. We optimize the adaptation rates for \(T=200\) communication rounds with partial participation (cohort size \(C=10\)), learning rate \(\eta=0.5\) and batch size \(B=200\).

ATP testingWe test the optimized adaptation rates on each target client. We use batch size \(B=200\).

#### c.1.4 PACS experiments

Data preparationPACS dataset contains four domains: art, cartoon, photo, and sketch. We adopt the leave-one-domain-out evaluation protocol [10], i.e., one domain is chosen as the held-out testing domain, and the remaining domains are regarded as source training domains. We follow the data preprocessing in [10], while additionally applying step partition to inject label shift. Each domain is divided into 7 clients, leading to a total of 21 source clients and 7 target clients. Each source client has \(80\%\) of its samples as training set and the remained \(20\%\) as testing set. Each client has 2 major classes and 5 minor class, where the ratio of images per class is approximately \(16:1\) (the same as our CIFAR-10 experiments). Since there is already domain shift, we do not add corruptions.

Global model trainingWe first train a global model with FedAvg [31] over the training sets of source clients. The global model is ResNet-18 with ImageNet pretrained parameter (provided by torchvision). We train the global model for \(T=200\) communication rounds with full participation (cohort size \(C=21\)), local epochs \(E=1\), learning rate \(\eta=0.05\) and batch size \(B=20\).

ATP trainingWe initialize the adaptation rates as a all-zero vector, and optimize it over the validation sets of source clients. We optimize the adaptation rates for \(T=500\) communication rounds with full participation (cohort size \(C=21\)), learning rate \(\eta=0.5\) and batch size \(B=200\).

ATP testingWe test the optimized adaptation rates on each target client. We use batch size \(B=200\).

#### c.1.5 Algorithm details

Assignment matrix _A_In the main test, we mentioned that \(\bm{A}\in\mathbb{R}^{D\times d}\) is a \(0-1\) assignment matrix that maps each adaptation rate \(\alpha^{[l]}\) to the indices of the \(l\)-th module's parameters in \(\bm{w}\). Mathematically,

\[A_{kl}=\begin{cases}1,&\text{if the $k$-th parameter in $\bm{w}$ belongs to the $l$-th module}\\ 0,&\text{otherwise}\end{cases}\]If there are \(d=3\) modules, each with 1, 2, and 3 parameters, so \(D=1+2+3=6\), the corresponding assignment matrix will be

\[\bm{A}=\begin{bmatrix}1&0&0\\ 0&1&0\\ 0&1&0\\ 0&0&1\\ 0&0&1\end{bmatrix}\]

#### Computation

We did our experiments with single NVIDIA Tesla V100 GPU. However, our experiment should only require less than 2GB of GPU memory.

### Compatibility to model architecture (RQ1)

In this part, we evaluate ATP with two more model architectures: a 5-layer Shallow CNN as a smaller model and ResNet-50 as a larger model.

From Table 5, we observe that under the new model architecture (and the new dataset), the performance of ATP is highly similar to the results of the ResNet-18 + CIFAR10 experiment in Table 1 in Subsection 6.1. ATP, in all three scenarios, can handle various types of distribution shifts and surpass baseline methods. This suggests that ATP is compatible with multiple model architectures.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{4}{c}{Shallow CNN on CIFAR-10} & \multicolumn{4}{c}{ResNet-50 on CIFAR-100} \\ \cline{2-9}  & Feature shift & Label shift & Hybrid shift & Avg. Rank & Feature shift & Label shift & Hybrid shift & Avg. Rank \\ \hline No adaptation & 64.39 \(\pm\) 0.18 & 69.33 \(\pm\) 0.37 & 61.99 \(\pm\) 0.47 & 7.3 & 45.31 \(\pm\) 0.30 & 51.63 \(\pm\) 0.15 & 40.01 \(\pm\) 0.17 & 7.3 \\ BN-Adapt & 66.46 \(\pm\) 0.22 & 54.99 \(\pm\) 0.38 & 50.40 \(\pm\) 0.43 & 7.0 & 47.75 \(\pm\) 0.29 & 34.85 \(\pm\) 0.26 & 30.31 \(\pm\) 0.09 & 7.3 \\ SHOT & 65.60 \(\pm\) 0.18 & 49.98 \(\pm\) 0.29 & 45.95 \(\pm\) 0.47 & 9.0 & 45.42 \(\pm\) 0.30 & 31.06 \(\pm\) 0.32 & 27.44 \(\pm\) 0.14 & 9.3 \\ Tent & 65.61 \(\pm\) 0.24 & 50.12 \(\pm\) 0.25 & 45.91 \(\pm\) 0.49 & 8.7 & 45.91 \(\pm\) 0.46 & 31.34 \(\pm\) 0.11 & 27.93 \(\pm\) 0.31 & 8.3 \\ T3A & 64.31 \(\pm\) 0.27 & 66.96 \(\pm\) 0.43 & 59.65 \(\pm\) 0.58 & 8.3 & 45.31 \(\pm\) 0.30 & 51.42 \(\pm\) 0.15 & 39.39 \(\pm\) 0.20 & 7.7 \\ MEMO & 65.89 \(\pm\) 0.31 & 71.95 \(\pm\) 0.25 & 64.17 \(\pm\) 0.47 & 5.3 & 48.42 \(\pm\) 0.14 & 55.19 \(\pm\) 0.28 & 42.53 \(\pm\) 0.20 & 3.7 \\ EM & 61.74 \(\pm\) 0.25 & 76.28 \(\pm\) 0.29 & 67.54 \(\pm\) 0.41 & 5.0 & 43.00 \(\pm\) 0.31 & 59.34 \(\pm\) 0.15 & 44.82 \(\pm\) 0.27 & 5.0 \\ BBSE & 65.92 \(\pm\) 0.53 & 75.99 \(\pm\) 0.44 & 66.64 \(\pm\) 0.53 & 6.3 & 37.26 \(\pm\) 0.64 & 56.97 \(\pm\) 0.20 & 40.09 \(\pm\) 0.51 & 7.0 \\ Surgical & 64.45 \(\pm\) 0.12 & 73.75 \(\pm\) 0.42 & 65.67 \(\pm\) 0.44 & 5.7 & 45.18 \(\pm\) 0.38 & 54.83 \(\pm\) 0.26 & 42.50 \(\pm\) 0.33 & 6.7 \\ ATP-batch & 66.90 \(\pm\) 0.05 & 76.23 \(\pm\) 0.32 & 68.88 \(\pm\) 0.35 & 2.3 & 48.35 \(\pm\) 0.45 & 58.06 \(\pm\) 0.53 & 46.82 \(\pm\) 0.32 & 2.7 \\ ATP-online & **67.13**\(\pm\) **0.17** & **78.56**\(\pm\) **0.32** & **71.52**\(\pm\) **0.51** & **1.0** & **49.08**\(\pm\) **0.26** & **61.86**\(\pm\) **0.25** & **49.51**\(\pm\) **0.23** & **1.0** \\ \hline \hline \end{tabular}
\end{table}
Table 5: ATP with different model architectures, accuracy (mean \(\pm\) s.d. %) on target clients

### Robustness to global model

In this subsection, we design experiments to answer the following question: _is ATP robust to the choice of global model?_ Specifically, we have three sub-questions:

* Is ATP robust to the parameter of global model? (C.3.1)
* Is ATP robust to the algorithm to train global model? (C.3.2)

#### c.3.1 Robustness to the parameter of global model (online updated global model)

In the main text, we primarily focused on the scenario where the global model remains fixed. However, in practical FL systems, the global model may also undergo continuous online updates. Therefore, after obtaining the adaptation rates through ATP training, the global model might have been further updated for several rounds. This raises a question: _Are the "outdated" adaptation rates still effective after several rounds of updates to the global model?_

We design experiment to apply the "outdated" adaptation rates to the global model that has undergone additional updates for several rounds, to see if they can still improve the test-time accuracy of the global model. Specifically, we optimize the adaptation rates \(\bm{\alpha}\) with \(\bm{w}_{G}^{T}\) where \(T=200\), but test the adaptation rates with \(\bm{w}_{G}^{T+\Delta T}\) with \(\Delta T=10,20,50,100\) rounds. We use the same setting of hybrid shift on CIFAR-10 experiments. As shown in Table 6, while further optimizing the global model can marginally improve the accuracy, both ATP-batch and ATP-online can effectively enhance the test-time accuracy through personalization, even when \(\bm{\alpha}\) is trained using an outdated version of the global model.

#### c.3.2 Robustness to the algorithm to train global model

In the main text, we used FedAvg [31] to train the global model. However, in real-world FL systems, other FL algorithms may be employed for training the global model, considering stability optimization or fairness. Therefore, we aim to investigate _whether ATP can also be applied to other commonly used FL algorithms_.

In particular, we use FedProx [23], an FL algorithm designed to handle heterogeneous setting, and \(q\)-FFL [24], an FL algorithm enhancing performance fairness among participating clients. For all global model, we use the same setting of hybrid shift on CIFAR-10 experiments. As shown in Table 7, both ATP-batch and ATP-online can consistently improve the test-time accuracy across different FL algorithms to train global models.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Method & \(200+0\) Rounds & \(+10\) Rounds & \(+20\) Rounds & \(+50\) Rounds & \(+100\) Rounds \\ \hline No adaptation & 63.68 \(\pm\) 0.24 & 63.88 \(\pm\) 0.20 & 64.03 \(\pm\) 0.13 & 64.30 \(\pm\) 0.08 & 64.56 \(\pm\) 0.11 \\ ATP-batch & 73.05 \(\pm\) 0.35 & 73.20 \(\pm\) 0.40 & 73.25 \(\pm\) 0.37 & 73.47 \(\pm\) 0.48 & 73.61 \(\pm\) 0.28 \\ ATP-online & 75.37 \(\pm\) 0.22 & 75.61 \(\pm\) 0.23 & 75.69 \(\pm\) 0.20 & 75.80 \(\pm\) 0.15 & 75.83 \(\pm\) 0.28 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Accuracy (%), ATP can learn adaptation rates that generalize to global models with different numbers of communication rounds under hybrid shift on CIFAR-10

\begin{table}
\begin{tabular}{l c c c} \hline \hline Method & FedAvg & FedProx (\(\mu=0.01\)) & \(q\)-FFL (\(q=1\)) \\ \hline No adaptation & 63.68 \(\pm\) 0.24 & 63.77 \(\pm\) 0.25 & 63.87 \(\pm\) 0.23 \\ ATP-batch & 73.05 \(\pm\) 0.35 & 72.95 \(\pm\) 0.33 & 73.15 \(\pm\) 0.21 \\ ATP-online & 75.37 \(\pm\) 0.22 & 75.51 \(\pm\) 0.19 & 75.79 \(\pm\) 0.15 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Accuracy (%), ATP enhances different global models under hybrid shift on CIFAR-10

### Convergence and generalization

In Section 5, Appendix B.2 and B.3, we theoretically show that ATP has good convergence and generalization guarantees. In this section, we visualize the training and testing loss curves to verify the fast convergence and superior generalization of ATP under different cohort size \(C\). The results are shown in Figure 8.

ConvergenceUnder full participation (\(C=240\)), both the training and testing loss converge stably and fast, indicating the reliable convergence of ATP. With partial participation, as the cohort size decreases (\(C=120,60,30,15\)), the training loss curve exhibits greater fluctuations, primarily due to sampling different subsets of clients in each communication round. However, the testing loss curve still converge stably with similar speed, indicating that ATP is robust to partial participation.

GeneralizationUnder full participation (\(C=240\)), the training and testing loss curves decrease synchronously without any overfitting. This implies that our algorithm exhibits excellent generalization. Similar observations can be made for partial participation (\(C=120,60,30,15\)). Additionally, it is worth noting that the test loss is lower than the train loss, which may seem counterintuitive. This is primarily due to the use of different corruptions between the testing and source clients. The accuracy of clients varies significantly under different corruptions, as evidenced by the fluctuations in the training curve when \(C=15\). However, we can still analyze the generalization performance by comparing the trends of the two curves.

Figure 8: Loss curves of ATP under different cohort size \(C\)

### Toy example for negative adaptation rate (RQ2)

In Section 6.2, we notice that ATP learns negative adaptation rates for running means and variance under label shift. In this subsection, we use a toy example to show why negative adaptation rate can improve the model performance under label distribution shift.

We consider a binary classification problem with input \(x\in\mathbb{R}\) and binary output \(y\in\{-1,+1\}\), where \(-1\) is the negative class and \(+1\) is the positive class. Let the feature for negative samples \((x|y=-1)\sim\mathcal{N}(-1,0.8^{2})\) and for positive samples \((x|y=+1)\sim\mathcal{N}(+1,0.8^{2})\). Let the label distribution \(\Pr(y=1)=\frac{1}{2}\) for training set, and \(\Pr(y=1)=\frac{5}{6}\) for testing set. Therefore, for the training distribution, we have

\[\mathbb{E}x =\Pr(y=-1)\mathbb{E}(x|y=-1)+\Pr(y=+1)\mathbb{E}(x|y=+1)=0\] \[Var(x) =\mathbb{E}[Var(x|y)]+Var(\mathbb{E}[x|y])=1.64\]

We consider a simple network with only one BN layer, with both normalization and affine transformation (as a linear classifier). There are four modules, each is a scalar: running mean \(\mu\), running variance \(\sigma^{2}\), weight \(\gamma\), bias \(\beta\).

TrainingDuring training, given enough training data, we have \(\mu_{train}=\mathbb{E}x=0\) and \(\sigma^{2}_{train}=1.64\). Figure 8(a) shows the histogram of \(z=\frac{x-\mu}{\sigma}\), i.e., the intermediate feature after normalization before the transformation. By comparing the histograms of \(z\) of two classes, we notice that the optimal decision boundary is \(z=0\), which indicate that \(\beta_{train}=0\) and \(\gamma_{train}>0\). We store the corresponding \(\mu_{train},\sigma^{2}_{train},\gamma_{train},\beta_{train}\), and only update running statistics \(\mu_{train},\sigma^{2}_{train}\) during testing.

Testing without updating running statistics (\(\alpha=0\))Figure 8(d) shows the testing result when we do not update the running statistics, i.e., \(\alpha=0\). Since two conditional feature distributions are symmetric, the accuracy will not change.

Testing with \(\alpha>0\)Positive adaptation rates align the intermediate feature distribution. When we use \(\alpha=1\), the distribution of \(z\) will be centralized. As shown in Figure 8(b), such alignment greatly reduces the accuracy. Similar result is also observed with any positive \(\alpha\), e.g., \(\alpha=0.5\) in Figure 8(c).

Testing with \(\alpha<0\)While \(\alpha=0\) has stable accuracy under label shift, by comparing the histograms of \(z\) of two classes in Figure 8(d), we notice that \(z=0\) is not the optimal decision boundary anymore, because there are less negative samples than positive samples. By using negative adaptation rate \(\alpha<0\), the normalization layer can further "disalign" the intermediate feature, which can further improve the accuracy, as shown in Figure 8(e).

Figure 9: Adapting batch norm running statistics under label shift.