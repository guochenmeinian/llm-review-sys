# Stratified Prediction-Powered Inference

for Hybrid Language Model Evaluation

 Adam Fisch\({}^{\dagger,*}\) Joshua Maynez\({}^{\dagger,*}\) R. Alex Hofer\({}^{\dagger}\)

Bhuwan Dhingra\({}^{\dagger}\) Amir Globerson\({}^{\ddagger}\) William W. Cohen\({}^{\dagger}\)

\({}^{\dagger}\)Google DeepMind \({}^{\ddagger}\)Google Research

{fisch,joshuahm,rofer,bdhingra,amirg,wohen}@google.com

Equal contribution.

###### Abstract

Prediction-powered inference (PPI) is a method that improves statistical estimates based on limited human-labeled data. PPI achieves this by combining small amounts of human-labeled data with larger amounts of data labeled by a reasonably accurate--but potentially biased--automatic system, in a way that results in tighter confidence intervals for certain parameters of interest (e.g., the mean performance of a language model). In this paper, we propose a method called Stratified Prediction-Powered Inference (StratPPI), in which we show that the basic PPI estimates can be considerably improved by employing simple data stratification strategies. Without making any assumptions on the underlying automatic labeling system or data distribution, we derive an algorithm for computing provably valid confidence intervals for population parameters (such as averages) that is based on stratified sampling. In particular, we show both theoretically and empirically that, with appropriate choices of stratification and sample allocation, our approach can provide substantially tighter confidence intervals than unstratified approaches. Specifically, StratPPI is expected to improve in cases where the performance of the autorater varies across different conditional distributions of the target data.

## 1 Introduction

Evaluating machine learning models requires _evaluation_ data. In particular, to iteratively improve on a method during development, or to reliably report an improvement, one needs to confidently and quantitatively assess the performance of the method. This is especially challenging for large language models (LLMs), where gathering high-quality annotations for generations can be difficult and time-consuming--and can ultimately become quite costly if gathering more than a few hundred examples.

One often-proposed approach to avoiding the evaluation bottleneck is to use a secondary LLM-based system to judge the output of the primary one. For instance, if the primary task is developing an LLM-based question-answering (QA) system, one can use a second LLM-based system that rates question/answer pairs as acceptable or not [6; 8; 17]. However, automated raters (i.e., _autoraters_) may be biased relative to the human raters they are intended to model, as others have noted [1; 9; 18; 23]. This can become substantially worse when models are tailored to hill climb on the autorater metrics, and eventually cease to become a good metric at all--a phenomenon commonly referred to as Goodhart's law, or reward hacking in the context of reinforcement learning from human feedback [13; 24].

We thus have two signals for assessing model performance. The first is human labels, which are typically accurate, but expensive to collect. As a result, usually only a small sample size is available, andan estimate based on these samples alone will have high variance. The second is autorater predictions, which are easy to collect for large sample sizes, but may also be systematically biased. The above may suggest that one must make a choice between either (i) a high-variance, but unbiased, estimate from a small human sample, or (ii) a lower-variance, but biased, autorater-based estimate. However, it turns out there are also statistically valid ways of _combining_ the auto-rater and human data for hybrid evaluations. Following [1; 3; 7] we call such methods _prediction-powered inference_ (PPI) methods. At a high level, PPI-based methods operate by using a small sample of examples labeled by both humans and autoraters to estimate the bias of the autorater. This bias is then used as a rectifier for the autorater estimate. The resulting estimate can then be shown to provide improved (i.e., tighter) confidence intervals for properties of interest for the system being evaluated, such as its true mean accuracy.

A weakness of standard PPI, however, is that it does not take heterogeneity into account. For example, in our QA setting, an autorater may have one accuracy when predicting if a model answer is correct, and a different accuracy when predicting if it is incorrect. This is especially true in cases where a correct answer is easy to verify (e.g., it is also present in Wikipedia), but harder to refute (e.g., no relevant external search results can be retrieved to either support or contradict it). In these settings, it may make sense to apply a different PPI strategy within each subdomain, depending on the local quality of the autorater. Moreover, we claim that such heterogenous settings are to be quite expected in practical applications. Inspired by the rich prior literature on stratified sampling and survey design [12; 19; 21], we therefore propose a _stratified_ approach to PPI, and show that it can be very advantageous when performance varies across subdomains, for either the autorater, or the model being evaluated.

On a technical level, it is not immediately clear how to apply stratification to PPI, as it involves two types of samples: one sample that is labeled by both humans and an autorater, and another (typically much larger) sample that is only labeled by the autorater. Extending the analysis of [3], in this work we show how confidence intervals based on the asymptotic normality of weighted M-estimators [32] can in fact be derived for the stratified PPI setting. The next challenge we address is how to determine the sample sizes used for stratification (i.e., the sample size of each stratum). Similar to recent work for active statistical inference [37], we further derive optimal rates that depend on certain moments of the underlying distribution that are generally unknown--and provide an approach for effectively approximating these. Finally, we provide extensive empirical evidence showing that our stratified approach (StratPPI) leads to considerably improved confidence intervals over both classical inference methods that use only human labels, and the baseline (unstratified) PPI approach.

## 2 Related Work

Prediction Powered Inference (PPI) was introduced in [1] as a method for obtaining tighter confidence intervals for parameters learned in supervised machine learning (e.g., coefficients in logistic regression) by also leveraging other model-based predictions on additional, unlabeled data. Related ideas were explored by [30], but with a focus on bootstrapping as a way for obtaining confidence intervals. PPI was then extended in several directions. For example [36] showed how the labeled data can be used for both estimating the parameters and the autorater model. PPI++ [3] showed how to obtain confidence intervals that are easy to compute efficiently, and introduced a parameter for weighting the predictions of the autorater such that the overall statistical efficiency can be improved. As noted in previous works on PPI, these approaches are closely related to other statistical methods for introducing control variates based on autoevaluators [9], as well as augmented inverse propensity weighting [27] (see discussion in [1; 3]). Like this paper, prior work has also focused on using PPI/PPI++ for evaluating machine learning systems with autoraters, including for ranking chatbots in the Chatbot Arena [7] and evaluating retrieval-augmented generation systems [28].

Most relevant to our setting, the recent work of [37] focuses on _active_ sampling of examples to label during PPI, where the total number of queried labels is random, but less than the sampling budget \(n\) in expectation. Specifically, it proposes to label examples for which the autorater is less confident in its predictions, and corrects for the sampling bias with a variant of inverse propensity weighting. In contrast, our somewhat simplified approach takes inspiration from stratified sampling where a coarse stratification is defined in advance, and the total number of labeled samples is constant. Like [37], we derive an analogous optimal allocation strategy for a given budget, though we only apply it at the stratum level, and not for individual examples. Furthermore, while the variable allocation helps reduce variance in heterogenous settings, our stratified treatment also allows for a stratum-specific extension of [3]'s estimated tuning parameters that further improves performance in complementary ways.

In terms of stratification specifically, concurrent work [14] also proposed Bayesian variants of several PPI methods, including stratified approaches. In contrast, the methods here do not require introducing priors, do not require running expensive Bayesian inference, and give more conventional, frequentist, guarantees of performance. The credible intervals produced by Bayesian methods are related to, but different from, the confidence intervals produced here, and by other prior PPI approaches [1; 3; 7].

Finally, there is a conceptual similarity between the PPI rectifiers and post-hoc calibration of a classifier. There is a rich literature on calibrating classifiers (e.g., [22; 25]), but there is a clear difference in goals between calibration and PPI: the former is aimed at modifying a learned autorater to make better probabilistic predictions, and the latter is aimed at obtaining better confidence intervals by making use of an existing autorater. That said, clearly one approach to improving PPI is to use a better-calibrated classifier, perhaps by taking some of the labeled data and using it for re-calibration. This is similar in motivation to the cross-PPI approach of [36]. We leave such approaches as future work, but we do experimentally explore PPI approaches on both well-calibrated autoraters and poorly calibrated ones.

## 3 Preliminaries

We begin by briefly reviewing PPI[1], and specifically the efficient PPI++ variant of [3]. Here, and in the rest of the paper, upper-case letters (\(X\)) denote random variables; lower-case letters (\(x\)) denote constants, and script letters (\(\mathcal{X}\)) denote sets, unless specified. All proofs are deferred to Appendix A.

Following [1; 3; 7], we assume an empirical sample \(S_{n}=\{(X_{1},Y_{1}),\ldots,(X_{n},Y_{n})\}\) of \(n\) i.i.d. examples drawn from some unknown, but fixed, distribution \(\mathbb{P}\), where \(X_{i}\in\mathcal{X}\) is an input, and \(Y_{i}\in\mathcal{Y}\) is the target output. We also assume a larger sample \(\tilde{S}_{N}=\{(\tilde{X}_{1},f(\tilde{X}_{1})),\ldots,(\tilde{X}_{N},f( \tilde{X}_{N}))\}\) where \(N\gg n\), for which the target outputs are not available, but we have access to an _autorater function_\(f\colon\mathcal{X}\to\mathcal{Y}^{\prime}\) which provides an approximation of \(Y\) given the observed input (we use \(\mathcal{Y}^{\prime}\) to denote the output space of the autorater as it is possible that \(\mathcal{Y}\neq\mathcal{Y}^{\prime}\), e.g., if \(f(X)=\mathbb{E}[Y\mid X]\)).

As an example, assume \(X\) is a _(question, model answer)_ tuple where the answer is produced by an LLM-based QA system we wish to evaluate, \(Y\) is a \(0/1\) "gold" human rating of correctness of the answer, i.e., \(\mathbf{1}\{\)_human rates model answer as correct_\(\}\), and \(f(X)\), the autorater, imperfectly predicts this human rating \(Y\). One value of interest is then the average accuracy of the QA system, \(\mathbb{E}[Y]\). More generally, given any convex loss function \(\ell_{\theta}(x,y)\) satisfying certain regularity conditions (see Definition A.1), we are interested in estimating the \(d\)-dimensional parameter \(\theta^{*}\),

\[\theta^{*}=\operatorname*{argmin}_{\theta\in\Theta}\mathbb{E}[\ell_{\theta}(X, Y)].\] (1)

In the statistics literature, this is broadly referred to as M-estimation (e.g., see [29]). Here, we focus on _mean_ estimation, for which the loss \(\ell_{\theta}(x,y)=\frac{1}{2}\|y-\theta\|^{2}\) has the optimum \(\theta^{*}=\mathbb{E}[Y]\). However, our method generalizes to other typical losses such as the squared loss for linear regression, where the parameters \(\theta^{*}\) are the regression coefficients, or, similarly, the log loss for other generalized linear models. Of course, the true \(\theta^{*}\) is generally not known, because we cannot exactly calculate the expectation in (1). The goal of PPI, which we share in this work, is to use both the labeled and unlabeled data \(S_{n}\) and \(\tilde{S}_{N}\) to obtain an asymptotically valid confidence interval \(\mathcal{C}_{\alpha,j}\) for \(\theta^{*}_{j}\) that for any \(j\in[d]\) satisfies1

Footnote 1: In \(d\)-dimensions, we obtain a confidence interval for each coordinate. See [3] for other possible choices.

\[\lim_{n,N\to\infty}\mathbb{P}(\theta^{*}_{j}\in\mathcal{C}_{\alpha,j})\geq 1-\alpha.\] (2)

The simplest confidence interval can be obtained using standard techniques by using only the labeled sample \(S_{n}\), and ignoring the autorater data \(\tilde{S}_{n}\). However, PPI employs a clever trick which allows for also using the unlabeled sample \(\tilde{S}_{N}\) to get tighter confidence intervals, as described next.

### A rectified prediction-powered loss

The key idea of PPI-based methods is to use autorater predictions to derive a low variance, but unbiased, estimate of the objective in (1). Consider the following "rectified" prediction-powered loss:

\[L^{\mathrm{PP}}(\theta)=\underbrace{\frac{1}{N}\sum_{i=1}^{N}\ell_{\theta}( \tilde{X}_{i},f(\tilde{X}_{i}))}_{\text{autorater data loss}}\ +\ \underbrace{\frac{1}{n}\sum_{i=1}^{n}\ell_{\theta}(X_{i},Y_{i})-\ell_{\theta}(X_ {i},f(X_{i}))}_{\text{loss rectifier}}.\] (3)The rectifier term on the right hand side of (3) removes the bias of the autorater data so that \(L^{\mathrm{PP}}(\theta)\) satisfies \(\mathbb{E}[L^{\mathrm{PP}}(\theta)]=\mathbb{E}\left[\ell_{\theta}(X,Y)\right]\). However, when \(f(X)\) is correlated with \(Y\), the loss \(L^{\mathrm{PP}}(\theta)\) will have lower variance. Unfortunately, \(f(X)\) may not be a good predictor of \(Y\) in all cases. In fact, it is even possible for \(L^{\mathrm{PP}}\) to be higher variance than the classical estimate (e.g., if \(f(X)\) is anti-correlated with \(Y\)). Thus, to adapt to the quality of \(f(X)\), PPI++ also introduces a tuning parameter \(\lambda\in\mathbb{R}\):2

Footnote 2: As noted in [3], for some losses \(\ell_{\theta}\) we must have \(\lambda\in[0,1]\) to guarantee convexity of \(L_{\lambda}^{\mathrm{PP}}\). The main focus of this paper, however, is on mean estimation with \(\ell_{\theta}(x,y)=\frac{1}{2}\|y-\theta\|^{2}\), which is convex for any \(\lambda\in\mathbb{R}\).

\[L_{\lambda}^{\mathrm{PP}}(\theta)=\frac{\lambda}{N}\sum_{i=1}^{N}\ell_{\theta }(\tilde{X}_{i},f(\tilde{X}_{i}))+\frac{1}{n}\sum_{i=1}^{n}\ell_{\theta}(X_{i },Y_{i})-\lambda\ell_{\theta}(X_{i},f(X_{i})).\] (4)

Clearly, \(L_{\lambda}^{\mathrm{PP}}\) still remains unbiased for any value of \(\lambda\). For \(\lambda=0\), the framework reduces to classical inference. In most cases, however, a proper choice of \(\lambda\neq 0\) will result in lower variance. PPI++ also suggests how to automatically choose a data-dependent \(\hat{\lambda}\) that converges to an optimal value.

### A prediction-powered confidence interval

We next describe the PPI++ method for deriving confidence intervals based on the rectified loss. Let

\[\hat{\theta}_{\hat{\lambda}}^{\mathrm{PP}}=\operatorname*{argmin}_{\theta \in\Theta}L_{\hat{\lambda}}^{\mathrm{PP}}(\theta).\] (5)

be the prediction-powered estimate. Standard analysis for M-estimators can then be extended to show that the scaled difference of the estimate \(\hat{\theta}_{\hat{\lambda}}^{\mathrm{PP}}\) and the true \(\theta^{*}\) is asymptotically normally distributed. PPI++ then leverages this result to compute a confidence interval for \(\theta^{*}\) that is asymptotically valid.

**Theorem 1** (Ppi++, [3]).: _Assume that \(\hat{\lambda}\xrightarrow{p}\lambda\) and \(\frac{n}{N}\to r\geq 0\). Let \(H_{\theta^{*}}:=\mathbb{E}[\nabla^{2}\ell_{\theta^{*}}]\), and_

\[V_{f,\theta^{*}}^{\lambda}:=\lambda^{2}\mathrm{Cov}(\nabla\ell_{\theta^{*}}( \tilde{X},f(\tilde{X})),\quad V_{\Delta,\theta^{*}}^{\lambda}:=\mathrm{Cov}( \nabla\ell_{\theta^{*}}(X,Y)-\lambda\nabla\ell_{\theta^{*}}(X,f(X))),\] (6)

_where \(\lambda\in\mathbb{R}\) is a hyper-parameter. Then under the regularity conditions of Definition A.1, we have that \(\sqrt{n}(\hat{\theta}_{\hat{\lambda}}^{\mathrm{PP}}-\theta^{*})\xrightarrow{d} \mathcal{N}(0,\Sigma^{\lambda})\), where \(\Sigma^{\lambda}:=H_{\theta^{*}}^{-1}(r\cdot V_{f,\theta^{*}}^{\lambda}+V_{ \Delta,\theta^{*}}^{\lambda})H_{\theta^{*}}^{-1}\)._

**Corollary 1** (Ppi++ CI, [3]).: _Let \(\widehat{\Sigma}^{\hat{\lambda}}\) be the plug-in estimate for \(\Sigma^{\lambda}\) using \(S_{n}\) and \(\tilde{S}_{N}\). Define_

\[\mathcal{C}_{\alpha,j}^{\mathrm{PP}}:=\left(\hat{\theta}_{\hat{\lambda},j}^{ \mathrm{PP}}\pm z_{1-\frac{\alpha}{2}}\sqrt{n^{-1}\widehat{\Sigma}_{jj}^{ \hat{\lambda}}}\right),\] (7)

_where \(z_{\beta}\) denotes the \(\beta\)-quantile of the standard normal distribution. Then for any \(j\in[d]\) it holds that \(\lim_{n,N\to\infty}\mathbb{P}\left(\theta_{j}^{*}\in\mathcal{C}_{\alpha,j}^{ \mathrm{PP}}\right)\geq 1-\alpha\)._

As mentioned above, [3] further showed that for an appropriate choice of \(\lambda\) (that can be analytically derived and estimated via a \(\hat{\lambda}\)), the trace of the covariance matrix, \(\mathrm{Tr}(\Sigma^{\lambda})\), is at most that of the covariance matrix derived without using autorater data, implying that the PPI++-based confidence set \(\mathcal{C}_{\alpha}^{\mathrm{PP}}\) can always be at least as tight as that of the classical M-estimator (and often much tighter in practice).

## 4 Stratified prediction-powered inference

We now present our approach for improving PPI++ estimates via _stratification_. In particular, we show how optimizing a rectified loss computed via stratified sampling can lead to a consistent, but lower variance, estimate of the optimal parameter \(\theta^{*}\), and correspondingly tighter confidence intervals. Consider the QA example from the previous section. For most autoraters, it is reasonable to assume that the strength of their performance can vary, depending on the type of input being presented. For instance, an autorater might be accurate at predicting whether an answer to an unambiguous question (e.g., _"What is the capital of France?"_) is correct, but relatively poor at inferring if an answer to an open-ended question (e.g., _"What is the best way to cook a hamburger?"_) is acceptable or not. Splitting the problem space into different domains allows us to derive a more specialized form of the prediction powered loss that can better adapt to this autorater heterogeneity via stratified sampling.

Formally, assume that the input space \(\mathcal{X}\) is partitioned in advance into \(K\) non-empty, mutually exclusive, and exhaustive strata \(\mathcal{A}=(\mathcal{A}_{1},\ldots,\mathcal{A}_{K})\), where \(K\) is a finite integer. For each stratum,we further assume that we can estimate \(w_{k}=\mathbb{P}(X\in\mathcal{A}_{k})\) arbitrarily well using large amounts of unlabeled data or prior knowledge; we treat them as known constants here for simplicity. To collect the labeled and unlabeled datasets \(S_{n}\) and \(\tilde{S}_{N}\), we then follow a standard stratified sampling procedure in which we draw two i.i.d. sets of samples of fixed size \(n_{k}\) and \(N_{k}\), respectively, from each stratum \(k\), where \(\sum_{k=1}^{K}n_{k}=n\) and \(\sum_{k=1}^{K}N_{k}=N\). The relative sizes \(n_{k}/n\) and \(N_{k}/N\) are free parameters; they can simply be the natural rates, \(n_{k}/n\approx N_{k}/N\approx w_{k}\), or systematically decided (see SS4.3). Note that this is a fundamentally different sampling model from standard PPI: here examples are i.i.d. within each strata, and independent (but not necessarily identically distributed) across each strata.

Next, we define the stratified loss via the weighted sum, \(L^{\mathrm{SPP}}_{\lambda}(\theta)=\sum_{k=1}^{K}w_{k}L^{\mathrm{PP}}_{k, \lambda_{k}}(\theta),\) where \(w_{k}\) is the stratum weight, \(\lambda=(\lambda_{1},\dots,\lambda_{k})\in\mathbb{R}^{k}\) are now _stratum-specific_ tuning parameters, and \(L^{\mathrm{PP}}_{k}(\theta)\) is the conditional prediction-powered loss computed within each stratum, i.e.,

\[L^{\mathrm{PP}}_{k,\lambda_{k}}(\theta)=\frac{\lambda_{k}}{N_{k}}\sum_{i=1}^{ N_{k}}\ell_{\theta}(\tilde{X}_{ik},f(\tilde{X}_{ik}))+\frac{1}{n_{k}}\sum_{i=1}^{n_{k}} \ell_{\theta}(X_{ik},Y_{ik})-\lambda_{k}\ell_{\theta}(X_{ik},f(X_{ik})).\] (8)

As before, each \(L^{\mathrm{PP}}_{k,\lambda_{k}}\) is an unbiased estimate of the conditional loss. Like PPI++, we also allow for data-dependent parameters \(\hat{\lambda}_{k}\) that we show how to automatically tune for best performance in SS4.2.

### A stratified prediction-powered confidence interval

We now present our main result, which is a confidence interval for \(\theta^{*}\) based on the stratified loss. More precisely, the result states that, as in PPI++, the minimizer of the stratified loss,

\[\hat{\theta}^{\mathrm{SPP}}_{\hat{\lambda}}=\operatorname*{argmin}_{\theta}L ^{\mathrm{SPP}}_{\hat{\lambda}}(\theta),\] (9)

has an asymptotically normal distribution with mean \(\theta^{*}\). See Algorithm 1 for pseudocode.

**Theorem 2**.: _Assume that \(\hat{\lambda}_{k}\overset{p}{\rightarrow}\lambda_{k},\overset{n}{N}\to r\) for any \(r\geq 0\), \(\frac{n_{k}}{n}\rightarrow\rho_{k}\) for any \(\rho_{k}>0\), and \(\frac{N_{k}}{N}\rightarrow\tilde{\rho}_{k}\) for any \(\tilde{\rho}_{k}>0\). Let \(H_{k,\theta^{*}}:=\mathbb{E}[\nabla^{2}\ell_{\theta^{*}}(X,Y)\mid X\in \mathcal{A}_{k}]\), and_

\[V^{\lambda_{k}}_{k,f,\theta^{*}} :=\lambda_{k}^{2}\mathrm{Cov}(\nabla\ell_{\theta^{*}}(\tilde{X}, f(\tilde{X}))\mid\tilde{X}\in\mathcal{A}_{k}),\] (10) \[V^{\lambda_{k}}_{k,\Delta,\theta^{*}} :=\mathrm{Cov}(\nabla\ell_{\theta^{*}}(X,Y)-\lambda_{k}\nabla\ell _{\theta^{*}}(X,f(X))\mid X\in\mathcal{A}_{k}),\] (11)

_where \(\lambda_{k}\in\mathbb{R}\) is a hyper-parameter. Then, under the regularity conditions of Definition A.1,_

\[\sqrt{n}(\hat{\theta}^{\mathrm{SPP}}_{\hat{\lambda}}-\theta^{*})\overset{d}{ \rightarrow}\mathcal{N}(0,\Sigma^{\lambda}_{w}),\] (12)

_where \(\Sigma^{\lambda}_{w}:=A^{-1}_{w}B^{\lambda}_{w}A^{-1}_{w}\) and:_

\[A_{w}:=\sum_{k=1}^{K}w_{k}H_{k,\theta^{*}}\qquad B^{\lambda}_{w}:=\sum_{k=1}^{ K}w_{k}^{2}\left(\frac{r}{\tilde{\rho}_{k}}\cdot V^{\lambda_{k}}_{k,f,\theta^{*}} +\frac{1}{\rho_{k}}\cdot V^{\lambda_{k}}_{k,\Delta,\theta^{*}}\right).\] (13)

To obtain the result, we combine unstratified PPI++ with the asymptotic properties of weighted M-estimators [32]. The resulting confidence interval for \(\theta^{*}\) is then derived analogously to Corollary 1.

**Corollary 2**.: _Let \(\widehat{\Sigma}^{\hat{\lambda}}_{w}\) be the plug-in estimate for \(\Sigma^{\lambda}_{w}\) using \(S_{n}\) and \(\tilde{S}_{N}\). Define_

\[\mathcal{C}^{\mathrm{SPP}}_{\alpha,j}:=\left(\hat{\theta}^{\mathrm{SPP}}_{\hat {\lambda},j}\pm z_{1-\frac{\alpha}{2}}\sqrt{n^{-1}\widehat{\Sigma}^{\hat{ \lambda}}_{w,jj}}\right),\] (14)

_where \(z_{\beta}\) denotes the \(\beta\)-quantile of the standard normal distribution. Then for any \(j\in[d]\),_

\[\lim_{n,N\rightarrow\infty}\mathbb{P}\left(\theta^{*}_{j}\in\mathcal{C}^{ \mathrm{SPP}}_{\alpha,j}\right)\geq 1-\alpha.\] (15)

The form of the stratified prediction-powered confidence interval is similar to that of PPI++, except that it is based off of the weighted stratum-conditional covariance matrices. The effect of this change, however, is significant. In fact, in the case of mean estimation, we show that the asymptotic variance of StratPPI is at most that of PPI++ (even without any additional tuning of \(\lambda_{k}\) and \(\rho_{k}\)).

**Proposition 1**.: _Let \(\lambda_{k}\in\mathbb{R}\) be any constant for all strata, and fix \(\rho_{k}\) and \(\tilde{\rho}_{k}\) to their natural rates \(w_{k}\). Then for \(\ell_{\theta}(x,y)=\frac{1}{2}\|y-\theta\|^{2}\) and any stratification \((\mathcal{A}_{1},\dots,\mathcal{A}_{K})\), we have \(\operatorname{Tr}(\Sigma_{w}^{\lambda})\leq\operatorname{Tr}(\Sigma^{\lambda})\), where \(\Sigma^{\lambda}\) is the asymptotic covariance matrix of PPI++. Furthermore, we have that equality holds if and only if both \(\mathbb{E}[Y\mid X\in\mathcal{A}_{k}]\) and \(\mathbb{E}[f(X)\mid X\in\mathcal{A}_{k}]\) are the same for all strata._

More generally, although our results will hold for arbitrary stratifications, it is best if they are heterogeneous, and chosen such that the individual stratum-conditional variances are minimized. For example, if an autorater systematically _over-estimates_ the model's performance on one subdomain, but systematically _under-estimates_ the model's performance on another, then splitting these subdomains into different strata can result in a lower variance \(L_{\lambda}^{\mathrm{SPP}}\). Similarly, if an autorater has much higher noise on some subdomains than others, it can be beneficial to stratify on those subdomains--and then either lower \(\lambda_{k}\), allocate a higher proportion of samples \(n_{k}/n\) and \(N_{k}/N\), or both for the noisier stratas. In SS5 we empirically demonstrate that the stratified estimator can indeed lead to considerably tighter confidence intervals in practice, especially with additional tuning of \(\lambda_{k}\) and \(\rho_{k}\), as discussed next.

### Optimal weighting of the autorater predictions

In [3] it was shown that the optimal value of \(\lambda\) for PPI++ (i.e., the one minimizing the variances of the estimator and the corresponding confidence interval) could be found in closed form. We now present a simple extension of this result to the stratified case. For notational convenience, we use the shorthand \(\nabla\ell_{k,\theta}:=\nabla\ell_{\theta}(X,Y)\mid X\in\mathcal{A}_{k}\) and \(\nabla\ell_{k,\theta}^{\ell}:=\nabla\ell_{\theta}(X,f(X))\mid X\in\mathcal{A} _{k}\).

**Proposition 2**.: _Assume that \(\tilde{\rho}_{k}\) and \(\rho_{k}\) are fixed. Then the tuning parameters \((\lambda_{1}^{*},\dots,\lambda_{k}^{*})\), where_

\[\lambda_{k}^{*}=\frac{\operatorname{Tr}\left(A_{w}^{-1}(\operatorname{Cov}( \nabla\ell_{k,\theta^{*}},\nabla\ell_{k,\theta^{*}}^{f})+\operatorname{Cov}( \nabla\ell_{k,\theta^{*}}^{f},\nabla\ell_{k,\theta^{*}}^{f}))A_{w}^{-1} \right)}{2\left(1+\frac{n_{k}}{N_{k}}\right)\operatorname{Tr}\left(A_{w}^{-1} \operatorname{Cov}(\nabla\ell_{k,\theta^{*}}^{f})A_{w}^{-1}\right)},\] (16)

_minimize the cumulative asymptotic variance, \(\operatorname{Tr}(\Sigma_{w}^{\lambda})\)._

Note that \(\operatorname{Tr}(\Sigma_{w}^{\lambda})\) is proportional to the total size of the confidence interval \(\mathcal{C}_{\alpha}^{\mathrm{SPP}}\) in (14). Furthermore, as in [3], we can use plug-in estimates for the terms in (16) to compute a \(\hat{\lambda}_{k}\), where \(\hat{\lambda}_{k}\overset{P}{\to}\lambda_{k}^{*}\). From (16), we can see that \(\lambda_{k}^{*}\) is closely related to the correlation coefficient of the (curvature-scaled)gradients for minimizing the loss on an autorater label versus a true label. Intuitively, the more correlated these terms are, the more we can rely on the autorater labels for finding the true minimizer of \(\mathbb{E}[\ell_{\theta}(X,Y)]\). For \(1\)-\(d\) mean estimation in particular, we can see that \(\lambda_{k}^{*}\) takes on a simple form:

**Example 1** (\(\lambda_{k}^{*}\) for mean estimation).: _Consider the \(1\)-\(d\) mean loss: \(\ell_{\theta}(x,y)=\frac{1}{2}(y-\theta)^{2}\). Then:_

\[\lambda_{k}^{*}=\frac{\operatorname{Cov}(Y,f(X))}{(1+\frac{n_{k}}{N_{k}}) \mathrm{Var}(f(X))}\approx\frac{\operatorname{Cov}(Y,f(X))}{\mathrm{Var}(f(X ))}\quad\text{for large }N_{k},\] (17)

_which is equivalent to the optimal linear regression coefficient, \(\min_{\lambda_{k}}\mathbb{E}[|Y-\lambda_{k}f(X)|^{2}\mid X\in\mathcal{A}_{k}]\)._

### Optimal allocation of the sampling budget

Our stratification approach has an additional hyperparameter \(\rho\) that can also be tuned to reduce variance. Recall that \(\rho_{k}\) determines the ratio between the labeled data size \(n_{k}\) for stratum \(k\) and the overall data size \(n\) (i.e., \(\sum_{k}n_{k}=n\)). \(\tilde{\rho}_{k}\) is similarly defined for the unlabeled data. Any strictly positive values of \(\rho_{k}\) and \(\tilde{\rho}_{k}\) are valid to be used, though not all values will improve performance. It turns out that the optimal \(\rho_{k}\) values can be exactly calculated, as the following proposition shows.

**Proposition 3**.: _Assume that \(\lambda_{k}\) is fixed. Then the sampling rates \((\rho_{1}^{*},\dots,\rho_{k}^{*})\) and \((\tilde{\rho}_{1}^{*},\dots,\tilde{\rho}_{k}^{*})\), where_

\[\rho_{k}^{*}=\frac{w_{k}\sqrt{\mathrm{Tr}(A_{w}^{-1}V_{k,\Delta,\theta}^{*}.A_ {w}^{-1})}}{\sum_{k^{\prime}=1}^{K}w_{k^{\prime}}\sqrt{\mathrm{Tr}(A_{w}^{-1} V_{k^{\prime},\Delta,\theta}^{*}.A_{w}^{-1})}}\quad\text{and}\quad\tilde{\rho}_{k}^{*}= \frac{w_{k}\sqrt{\mathrm{Tr}(A_{w}^{-1}V_{k,f,\theta}^{*}.A_{w}^{-1})}}{\sum_ {k^{\prime}=1}^{K}w_{k^{\prime}}\sqrt{\mathrm{Tr}(A_{w}^{-1}V_{k^{\prime},f, \theta}^{*}.A_{w}^{-1})}}\] (18)

_minimize the cumulative asymptotic variance, \(\mathrm{Tr}(\Sigma_{w}^{\lambda})\)._

Although the solution for \(\rho_{k}^{*}\) is informative, it is not necessarily practical, as it depends on knowing \(A_{w}^{-1}V_{k,\Delta,\theta}^{\lambda_{k}}\). \(A_{w}^{-1}\); this in turn depends on \(\theta^{*}\) and \(\mathbb{P}(X,Y)\), which are both unknown.3 In the special case of mean estimation, however, it turns out there is no dependence on \(\theta^{*}\). To address the remaining dependence on \(\mathbb{P}(X,Y)\), we propose to use autorater confidence scores, assuming they are available. Specifically, assume \(\mathcal{Y}\) is discrete, and let \(c(y\mid x)\) be the confidence of the autorater in label \(y\) given \(x\), where \(c(y\mid x)\) approximates \(\mathbb{P}(Y=y\mid X=x)\). This will result in the estimate for \(\mathrm{Tr}(A_{w}^{-1}V_{k,\Delta,\theta}^{\lambda_{k}}.A_{w}^{-1})\) below, which can then be plugged into the expression for \(\rho_{k}^{*}\) in Proposition 3.

Footnote 3: Similarly, the optimal solution to \(\tilde{\rho}_{k}\) also depends on the unknown \(\theta^{*}\) in general, though this term is less important to optimize if we assume \(N\) to be large. In practice, we always keep \(\tilde{\rho}_{k}\) fixed to the natural rate, \(w_{k}\).

**Example 2** (\(\rho_{k}^{*}\) for mean estimation).: _Consider the \(1\)-\(d\) mean loss: \(\ell_{\theta}(x,y)=\frac{1}{2}(y-\theta)^{2}\). Then:_

\[\mathrm{Tr}(A_{w}^{-1}V_{k,\Delta,\theta^{*}}^{\lambda_{k}}A_{w}^{-1})= \mathrm{Var}(Y-\lambda_{k}f(X)\mid X\in\mathcal{A}_{k}).\] (19)

(19) _can then be estimated using the observed, but unlabeled, samples scored by the autorater for each stratum \(k\), \(\tilde{X}_{ik}=\tilde{x}_{ik}\), \(i=1,\dots,N_{k}\), as \(\mathrm{Var}(Y-\lambda_{k}f(X)\mid X\in\mathcal{A}_{k})\approx\hat{\sigma}_{k}^ {2}\), where_

\[\hat{\sigma}_{k}^{2} =\frac{1}{N_{k}}\sum_{i=1}^{N_{k}}\sum_{y\in\mathcal{Y}}c(y\mid \tilde{x}_{ik})\left(y-\lambda_{k}f(\tilde{x}_{ik})-\hat{\mu}_{k}\right)^{2}\quad \text{and}\] (20) \[\hat{\mu}_{k} =\frac{1}{N_{k}}\sum_{i=1}^{N_{k}}\sum_{y\in\mathcal{Y}}c(y\mid \tilde{x}_{ik})\left(y-\lambda_{k}f(\tilde{x}_{ik})\right).\] (21)

For \(d\)-dimensional data, the result is similar, but with a sum of \(d\) variances (one for each dimension). We also provide a simplified expression for \(\hat{\sigma}_{k}^{2}\) in Appendix B when \(f(\tilde{x}):=\sum_{y\in\mathcal{Y}}c(y\mid\tilde{x})\cdot y\). Importantly, as we are free to use any \(\rho_{k}>0\), using this estimate still preserves the asymptotic coverage guarantees in (14), regardless of if the confidence estimate \(c(y\mid x)\) is calibrated or not. Empirically, we show that using this heuristic can indeed lead to substantial improvements.

## 5 Experimental results

We compare our stratified estimator, StratPPI, to two baselines: (i) the classical estimate, which uses only the labeled data, \(S_{n}\); and (ii) PPI++, which uses both \(S_{n}\) and \(\tilde{S}_{n}\). All of our experiments focus on \(1\)-\(d\) mean estimation. We explore three different allocation strategies for StratPPI: the first is to set \(\rho_{k}=w_{k}\) to be data proportional (StratPPI Prop.), the second is to set \(\rho_{k}\) optimally via the oracle \(\rho_{k}=\rho_{k}^{*}\) (StratPPI Opt.), and the third is to use the approximation, \(\rho_{k}\propto w_{k}\sigma_{k}\), in Example 2 for \(\lambda_{k}=1\) when confidence scores are available (StratPPI Heur.). We use \(\lambda\)-tuning for both PPI++ and StratPPI, as outlined in SS4.2. Additional experimental results are given in Appendix C.

### Simulation studies

We start with a simple synthetic experiment that is an analogue of SS7.7.1 in [3]. Our goal is to estimate the mean outcome \(\mathbb{E}[Y]\), where \(Y\sim\mathcal{N}(0,1)\). We assume that the input space \(\mathcal{X}\) is partitioned into \(K=2\) strata, \((\mathcal{A}_{1},\mathcal{A}_{2})\), of equal mass \(\mathbb{P}(X\in\mathcal{A}_{1})=\mathbb{P}(X\in\mathcal{A}_{2})=0.5\). We then assume that predictions are formed as \(f(X_{ik})=Y_{ik}+\mu_{k}+\sigma_{k}\epsilon_{ik}\), where \(\epsilon_{ik}\sim\mathcal{N}(0,1)\). In other words, the predictions do not depend on the covariates \(X_{ik}\), other than to reflect a stratum-specific noise \(\sigma_{k}\) and bias \(\mu_{k}\). We test three different scenarios: (i) where the two strata are homogeneous with \(\mu_{1}=\mu_{2}\) and \(\sigma_{1}=\sigma_{2}\); (ii) where the two strata have different prediction biases, \(\mu_{1}\neq\mu_{2}\); and (iii) where the two strata have different prediction noise levels, \(\sigma_{1}\neq\sigma_{2}\). For each experiment, we sample \(N=10{,}000\) total predictions \(f(\tilde{X})\) using \(\tilde{\rho}_{1}=\tilde{\rho}_{2}=0.5\), i.e., proportional to masses of the two hypothetical, equal-weight strata. We then vary the total number \(n\) of labeled examples \(Y\), where the allocation is chosen according to \(\rho\) (which differs depending on if we are using StratPPI Prop. or StratPPI Opt.). We show results in Figure 1 for the mean confidence interval (CI) size and coverage (i.e., the fraction of the cases where the CI contained the true parameter value \(\theta^{*}\)) of each method, averaged over \(1k\) trials. As the plots in Figure 1 illustrate, when the underlying strata are homogeneous (left column), StratPPI behaves similar to PPI++. However, when the strata are heterogeneous (middle and right columns), StratPPI outperforms both baselines significantly--whereas PPT++ becomes barely

Figure 1: Mean estimation simulation study with \(K=2\) and \(\alpha=0.1\). The top row plots coverage (i.e., the fraction of the cases where the CI contained the true parameter value \(\theta^{*}\)). The middle row plots the mean CI width (\(\downarrow\) is better). Shaded areas plot the \(16/84\) quantiles across \(5\)k trials. The bottom row plots the RMSE of \(\hat{\theta}^{\mathrm{SPP}}\) computed across the \(5k\) trials, which shares the same trend with the mean CI width, as the estimator is unbiased. The left column shows a setting where strata are homogeneous, and StratPPI provides the no benefits over standard PPI++ (but is not worse). The middle and right columns show heterogeneous settings where the autorater has either a different bias (\(\mu\)) or variance \((\sigma)\) per stratum, in which case StratPPI helps substantially. As strata variances are known, we only report proportional and optimal sample allocation results for StratPPI.

more powerful than the classical estimator. Additionally, we see that when the variance differs per stratum (right column), optimal allocation of the sampling budget indeed provides additional benefit.

### Real data studies

We now demonstrate how our method performs on real datasets, where the underlying structure of the autorater is unknown. To partition \(\mathcal{X}\), we choose to focus on stratifications that are based on the autorater's predictions, \(f(X)\), based on the intuition that autorater performance can often differ across the type of predictions that it makes. Concretely, if the output space \(\mathcal{Y}^{\prime}\) of \(f\) is discrete, then we define \(\mathcal{A}=\mathcal{Y}^{\prime}\); otherwise we define \(\mathcal{A}\) based on the equal-mass quantiles of \(\mathcal{Y}^{\prime}\) (which can be estimated by sampling a large set of unlabeled \(X\) and applying the autorater). We set \(K=10\). For all experiments, we plot performance as a function of \(n\), where \(n\) is the number of human ratings our system is allowed to observe from each dataset. The remainder of the dataset (including any data points that are unlabeled, or labeled but with the labels removed) is used for the autorater sample \(\tilde{S}_{N}\).

**Seahorse.** The Seahorse dataset [11] focuses on multilingual summarization. The authors considered generative models that output summaries for a document, and collected labels for many systems that cover serveral dimensions of summary quality. We focus on one quality dimension--whether the summary is fully attributable to the source document--and on one summarization system--a fine-tuned 13B parameter mT5 model [33]. The autorater models for each dimension are also mT5-XXL finetuned models, which output probability scores. The data contains \(2727\) examples for these two tasks, all of which have both human ratings as well as autorater scores.

**AttributedQA.** In attributed question answering [6], the goal of the QA system is to output both an answer, and a retrieved document that provides support for that answer. The system is only considered to be correct if the answer is both correct, and indeed supported by the linked document. We evaluate the highest-scoring "retrieve-and-read" QA system from this dataset,and define our autorater to be an 11B parameter T5 model [26] fine-tuned on a collection of natural language

Figure 2: Mean estimation on real data with \(K=10\) and \(\alpha=0.05\). The \(x\) axis plots the number of human-labeled examples \(n\); the \(y\) axis plots CI width, percent reduction in CI width against the classical estimate, and the effective sample size (the amount of human labels necessary to match the same confidence interval via classical inference). Shaded areas plot the \(16/84\) quantiles across \(1k\) trials. All StratPPI methods improve over classical inference and PPI++.

entailment tasks [15]. Like Seahorse, the model predicts a probability for whether or not the QA system gave an attributable answer. This dataset has \(1000\) human labels and \(3000\) autorater labels.

**Galaxy.** To demonstrate the generality of StratPPI beyond LLM-based settings, we also consider the Galaxy dataset [31], where the task is to estimate the fraction of spiral galaxies in the local universe. The autorater used here is a ResNet classifier applied to images from the Sloan Digital Sky Survey (SDSS) [34]. The classifier estimates the probability that the galaxy in question is spiral. We use \(16{,}743\) observations from the dataset which contain both the human and autorater labels.

**Methodology.** We follow a procedure similar to [1; 3] to study CI estimates as a function of \(n\). We report results on the following estimates: Classical inference, PPI++, StratPPI Prop. and StratPPI Heur. We do not report StratPPI Opt. since it is unknown for real data. For each value of \(n\), we sample \(n\) of the cases with human labels at an allocation rate \(\rho_{k}\) (this rate is determined differently in StratPPI Prop. and StratPPI Heur.). As noted above, we construct the unlabeled dataset \(\tilde{S}_{N}\) by joining the remaining labeled data (without utilizing the true labels) with any unlabeled data available for that dataset. We repeat this over \(1000\) trials, and for each trial we obtain the CI width. We report the mean of these widths in Figure 2, as well as the percent reduction in width over the classical inference baseline, \(\frac{|C_{\alpha}^{\text{classical}}|-|C_{\alpha}^{\text{method}}|}{|C_{ \alpha}^{\text{classical}}|}\times 100\%\). We also report the "effective sample size", which we define as the number of samples required to obtain a CI of the same width as the method at sample size \(n\) when using the classical inference baseline instead.

**Results.** Figure 2 shows a large improvement for StratPPI methods over both PPI++ and classical inference for most datasets. Specifically, though all CIs decrease substantially in absolute size with \(n\) as expected, we see that improvements in the percent reduction in CI width over PPI++ can be as large as \(0.10\to 0.20\), \(0.20\to 0.30\), and \(0.25\to 0.35\) points in Seahorse, AttributedQA, and Galaxy, respectively. Furthermore, we can observe that many of the datasets exhibit heterogeneous characteristics, for which heuristic allocation helps considerably. In Galaxy, this accounts for a \(+10\) percent reduction in CI width. In Seahorse and AttributedQA, the improvements are less strong but still clearly apparent. The practical implication of these results is that when limited by a human rating budget, StratPPI is able to produce an estimate of the mean with fewer human ratings via stratification and sampling allocation. For example, for the Seahorse dataset, we can see from the right column in Figure 2 that StratPPI Heur. with only \(300\) human ratings will be approximately as confident about the mean as the classical estimate that utilizes \(600\) human ratings, a factor of \(2\times\).

## 6 Conclusion

As systems built on top of large language models continue to become more and more advanced, it becomes increasingly challenging to evaluate their performance using automatic tools. Manual labeling, on the other hand, is slow and expensive. Methods which therefore save on annotation cost are critical for reliably evaluating models, and knowing when they are improving--or degrading. Prediction-powered inference (PPI) is a promising class of such hybrid evaluation methods, since it can leveraged to provably produce statistically valid confidence intervals, while also effectively reducing the number of human labels needed to obtain intervals of certain width. Our results demonstrate that we can push PPI even further by introducing a method for performing even lower variance M-estimation by employing stratified sampling. In particular, we find that stratifications based on the predictions of the autoraters themselves proves to be an powerful stratification technique.

**Limitations.** While the confidence intervals produced by StratPPI (and PPI) enjoy coverage guarantees, these guarantees are asymptotic. When finite-sample performance is of particular importance, techniques that afford stronger guarantees might be preferred [2; 4; 5]. We also note some non-trivial aspects of the stratified sampling setup: (i) the number of strata has to be fixed; if \(K\) scales with \(n\) a more careful treatment is required; and (ii) the assumed observation model is different from traditional i.i.d. settings--we must be able to sample fixed sized samples from each partition; and (iii) performance may not be improved if the selected stratification is not statistically useful. Furthermore, in practice, the human data might have already been collected, in which case the studied stratified sampling setup does not directly apply. We leave the study of such post-stratified estimators to future work.

**Broader impacts.** This paper introduces a more powerful statistical method for evaluating LLMs, by merging human evaluations with autoraters in a way that is aware of subdomain differences. Our goal is to help power more reliable evaluations with lower annotation effort, both in terms of cost and time.

**Acknowledgements.** We thank Jacob Eisenstein, Jonathan Berant, Anastasios Angelopoulos, Taylan Cemgil, Arnoud Doucet, and Chris Dyer for helpful comments and discussions.

## References

* [1] Anastasios N. Angelopoulos, Stephen Bates, Clara Fannjiang, Michael I. Jordan, and Tijana Zrnic. Prediction-powered inference. _Science_, 382(6671):669-674, 2023. doi: 10.1126/science.adi6000. URL https://www.science.org/doi/abs/10.1126/science.adi6000.
* [2] Anastasios N. Angelopoulos, Stephen Bates, Adam Fisch, Lihua Lei, and Tal Schuster. Conformal risk control. _arXiv preprint: 2208.02814_, 2023.
* [3] Anastasios N. Angelopoulos, John C. Duchi, and Tijana Zrnic. PPI++: Efficient prediction-powered inference. _arXiv preprint: 2311.01453_, 2023.
* [4] Anastasios Nikolas Angelopoulos, Stephen Bates, Emmanuel J. Candes, Michael I. Jordan, and Lihua Lei. Learn then test: Calibrating predictive algorithms to achieve risk control. _arXiv preprint: 2110.01052_, 2021.
* [5] Stephen Bates, Anastasios Nikolas Angelopoulos, Lihua Lei, Jitendra Malik, and Michael I. Jordan. Distribution free, risk controlling prediction sets. _arXiv preprint: 2101.02703_, 2020.
* [6] Bernd Bohnet, Vinh Q. Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Massimiliano Ciaramita, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, Tom Kwiatkowski, Ji Ma, Jianmo Ni, Lierni Sestorain Saralegui, Tal Schuster, William W. Cohen, Michael Collins, Dipanjan Das, Donald Metzler, Slav Petrov, and Kellie Webster. Attributed question answering: Evaluation and modeling for attributed large language models. _arXiv preprint: 2212.08037_, 2023.
* [7] Pierre Boyeau, Anastasios N. Angelopoulos, Nir Yosef, Jitendra Malik, and Michael I. Jordan. Autoeval done right: Using synthetic data for model evaluation. _arXiv preprint: 2403.07008_, 2024.
* [8] Jannis Bulian, Christian Buck, Wojciech Gajewski, Benjamin Borschinger, and Tal Schuster. Tomayto, tomahto. beyond token-level answer equivalence for question answering evaluation. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 291-305, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.20. URL https://aclanthology.org/2022.emnlp-main.20.
* [9] Arun Chaganty, Stephen Mussmann, and Percy Liang. The price of debiasing automatic metrics in natural language evaluation. In Iryna Gurevych and Yusuke Miyao, editors, _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 643-653, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1060. URL https://aclanthology.org/P18-1060.
* [10] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. Chatbot arena: An open platform for evaluating llms by human preference. _arXiv preprint: 2403.04132_, 2024.
* [11] Elizabeth Clark, Shruti Rijhwani, Sebastian Gehrmann, Joshua Maynez, Roee Aharoni, Vitaly Nikolaev, Thibault Sellam, Aditya Siddhant, Dipanjan Das, and Ankur Parikh. SEAHORSE: A multilingual, multifaceted dataset for summarization evaluation. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 9397-9413, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.584. URL https://aclanthology.org/2023.emnlp-main.584.
* [12] William G. Cochran. _Sampling Techniques, 3rd Edition_. John Wiley, 1977. ISBN 0-471-16240-X.

* Gao et al. [2023] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In _International Conference on Machine Learning_, pages 10835-10866. PMLR, 2023.
* Hofer et al. [2024] R. Alex Hofer, Joshua Maynez, Bhuwan Dhingra, Adam Fisch, Amir Globerson, and William W. Cohen. Bayesian prediction-powered inference. _arXiv preprint: 2405.06034_, 2024.
* Honovich et al. [2022] Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, and Yossi Matias. True: Re-evaluating factual consistency evaluation. _arXiv preprint: 2204.04991_, 2022.
* Jung et al. [2024] Jaehun Jung, Faeze Brahman, and Yejin Choi. Trust or escalate: Llm judges with provable guarantees for human agreement. _arXiv preprint: 2407.18370_, 2024.
* Kamalloo et al. [2023] Ehsan Kamalloo, Nouha Dziri, Charles Clarke, and Davood Rafiei. Evaluating open-domain question answering in the era of large language models. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 5591-5606, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.307. URL https://aclanthology.org/2023.acl-long.307.
* Liu et al. [2016] Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Noseworthy, Laurent Charlin, and Joelle Pineau. How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation. In Jian Su, Kevin Duh, and Xavier Carreras, editors, _Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing_, pages 2122-2132, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1230. URL https://aclanthology.org/D16-1230.
* Lohr [2021] S.L. Lohr. _Sampling: Design and Analysis_. Chapman & Hall/CRC Texts in Statistical Science. CRC Press, 2021. ISBN 9781000478235. URL https://books.google.com/books?id=DaHGEAAQBAJ.
* Newey and McFadden [1994] Whitney K. Newey and Daniel McFadden. Chapter 36 large sample estimation and hypothesis testing. volume 4 of _Handbook of Econometrics_, pages 2111-2245. Elsevier, 1994. doi: https://doi.org/10.1016/S1573-4412(05)80005-4. URL https://www.sciencedirect.com/science/article/pii/S1573441205800054.
* Neyman [1934] Jerzy Neyman. On the two different aspects of the representative method: The method of stratified sampling and the method of purposive selection. _Journal of the Royal Statistical Society_, 97(4):558-625, 1934. ISSN 09528385. URL http://www.jstor.org/stable/2342192.
* Niculescu-Mizil and Caruana [2005] Alexandru Niculescu-Mizil and Rich Caruana. Obtaining calibrated probabilities from boosting. In _UAI_, volume 5, pages 413-20, 2005.
* Novikova et al. [2017] Jekaterina Novikova, Ondrej Dusek, Amanda Cercas Curry, and Verena Rieser. Why we need new evaluation metrics for NLG. In Martha Palmer, Rebecca Hwa, and Sebastian Riedel, editors, _Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing_, pages 2241-2252, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1238. URL https://aclanthology.org/D17-1238.
* Pang et al. [2023] Richard Yuanzhe Pang, Vishakh Padmakumar, Thibault Sellam, Ankur Parikh, and He He. Reward gaming in conditional text generation. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, 2023.
* Patt et al. [1999] John Platt et al. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. _Advances in large margin classifiers_, 10(3):61-74, 1999.
* Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of Machine Learning Research_, 21(140):1-67, 2020. URL http://jmlr.org/papers/v21/20-074.html.

* Robins and Rotnitzky [1995] James M. Robins and Andrea Rotnitzky. Semiparametric efficiency in multivariate regression models with missing data. _Journal of the American Statistical Association_, 90(429):122-129, 1995. ISSN 01621459. URL http://www.jstor.org/stable/2291135.
* Saad-Falcon et al. [2024] Jon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. Ares: An automated evaluation framework for retrieval-augmented generation systems. _arXiv preprint: 2311.09476_, 2024.
* van der Vaart [1998] A. W. van der Vaart. _Asymptotic Statistics_. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 1998.
* Wang et al. [2020] Siruo Wang, Tyler H McCormick, and Jeffrey T Leek. Methods for correcting inference based on outcomes predicted by machine learning. _Proceedings of the National Academy of Sciences_, 117(48):30266-30275, 2020.
* Willett et al. [2013] Kyle W. Willett, Chris J. Lintott, Steven P. Bamford, Karen L. Masters, Brooke D. Simmons, Kevin R. V. Casteels, Edward M. Edmondson, Lucy F. Fortson, Sugata Kaviraj, William C. Keel, Thomas Melvin, Robert C. Nichol, M. Jordan Raddick, Kevin Schawinski, Robert J. Simpson, Ramin A. Skibba, Arfon M. Smith, and Daniel Thomas. Galaxy Zoo 2: detailed morphological classifications for 304 122 galaxies from the Sloan Digital Sky Survey. _Monthly Notices of the Royal Astronomical Society_, 435(4):2835-2860, 09 2013. ISSN 0035-8711. doi: 10.1093/mnras/stt1458. URL https://doi.org/10.1093/mnras/stt1458.
* 470, 2001. URL https://api.semanticscholar.org/CorpusID:123148497.
* Xue et al. [2020] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mt5: A massively multilingual pre-trained text-to-text transformer. _arXiv preprint arXiv:2010.11934_, 2020.
* York et al. [2021] Donald G. York, J. Adelman, Jr. John E. Anderson, Scott F. Anderson, James Annis, Neta A. Bahcall, J. A. Bakken, Robert Barkhouser, Steven Bastian, Eileen Berman, William N. Boroski, Steve Bracker, Charlie Briegel, John W. Briggs, J. Brinkmann, Robert Brunner, Scott Burles, Larry Carey, Michael A. Carr, Francisco J. Castander, Bing Chen, Patrick L. Colestock, A. J. Connolly, J. H. Crocker, Istvan Csabai, Paul C. Czarapata, John Eric Davis, Mamoru Doi, Tom Dombeck, Daniel Eisenstein, Nancy Ellman, Brian R. Elms, Michael L. Evans, Xiaohui Fan, Glenn R. Federwitz, Larry Fiscelli, Scott Friedman, Joshua A. Frieman, Masataka Fukugita, Bruce Gillespie, James E. Gunn, Vijay K. Gurbani, Ernst de Haas, Merle Haldeman, Frederick H. Harris, J. Hayes, Timothy M. Heckman, G. S. Hennessy, Robert B. Hindsley, Scott Holm, Donald J. Holmgren, Chi hao Huang, Charles Hull, Don Husby, Shin-Ichi Ichikawa, Takashi Ichikawa, Zeljko Ivezic, Stephen Kent, Rita S. J. Kim, E. Kinney, Mark Klaene, A. N. Kleinman, S. Kleinman, G. R. Knapp, John Korienek, Richard G. Kron, Peter Z. Kunszt, D. Q. Lamb, B. Lee, R. French Leger, Siriluk Limmongkol, Carl Lindenmeyer, Daniel C. Long, Craig Loomis, Jon Loveday, Rich Lucinio, Robert H. Lupton, Bryan MacKinnon, Edward J. Mannery, P. M. Mantsch, Bruce Margon, Peregrine McGehee, Timothy A. McKay, Avery Meiksin, Aronne Merelli, David G. Monet, Jeffrey A. Munn, Vijay K. Narayanan, Thomas Nash, Eric Neilsen, Rich Neswold, Heidi Jo Newberg, R. C. Nichol, Tom Nicinski, Mario Nonino, Norio Okada, Sadanori Okamura, Jeremiah P. Ostriker, Russell Owen, A. George Pauls, John Peoples, R. L. Peterson, Donald Petravick, Jeffrey R. Pier, Adrian Pope, Ruth Pordes, Angela Prosappio, Ron Rechenmacher, Thomas R. Quinn, Gordon T. Richards, Michael W. Richmond, Claudio H. Rivetta, Constance M. Rockosi, Kurt Ruthmansdorfer, Dale Sandford, David J. Schlegel, Donald P. Schneider, Maki Sekiguchi, Gary Sergey, Kazuhiro Shimasaku, Walter A. Siegmund, Stephen Smee, J. Allyn Smith, S. Snedden, R. Stone, Chris Stoughton, Michael A. Strauss, Christopher Stubbs, Mark SubbaRao, Alexander S. Szalay, Istvan Szapudi, Gyula P. Szokoly, Anirudda R. Thakar, Christy Tremonti, Douglas L. Tucker, Alan Uomoto, Dan Vanden Berk, Michael S. Vogeley, Patrick Waddell, Shu i Wang, Masaru Watanabe, David H. Weinberg, Brian Yanny, and Naoki Yasuda. The sloan digital sky survey: Technical summary. _The Astronomical Journal_, 120(3):1579, sep 2000. doi: 10.1086/301513. URL https://dx.doi.org/10.1086/301513.

* [35] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-judge with MT-bench and chatbot arena. In _Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2023. URL https://openreview.net/forum?id=uccHpGDlao.
* [36] Tijana Zrnic and Emmanuel J. Candes. Cross-prediction-powered inference. _Proceedings of the National Academy of Sciences_, 121(15):e2322083121, 2024. doi: 10.1073/pnas.2322083121. URL https://www.pnas.org/doi/abs/10.1073/pnas.2322083121.
* [37] Tijana Zrnic and Emmanuel J. Candes. Active statistical inference. _arXiv preprint: 2403.03208_, 2024.

Proofs

We begin by defining the basic regularity conditions that we assume \(\ell_{\theta}\) satisfies:

**Definition A.1** (Regularity conditions of \(\ell_{\theta}\)).: _Assume that_

1. \(\Theta\) _is a compact subset of_ \(\mathbb{R}^{d}\)_;_
2. _The minimizer_ \(\theta^{*}\in\operatorname{int}(\Theta)\) _is unique with_ \(\mathbb{E}[\nabla\ell_{\theta^{*}}(X,Y)]=0\)_;_
3. _For all_ \((x,y)\in\mathcal{X}\times\mathcal{Y}\)_,_ \(\ell_{\theta}(x,y)\) _is twice continuously differentiable on_ \(\operatorname{int}(\Theta)\)_;_
4. _For all_ \(\theta\in\Theta\)_,_ \(\ell_{\theta}(X,Y)\)_,_ \(\frac{\partial\ell_{\theta}(X,Y)}{\partial\theta_{i}}\)_, and_ \(\frac{\partial^{2}\ell_{\theta}(X,Y)}{\partial\theta_{i}\partial\theta_{j}}\) _have finite expectations;_
5. \(\mathbb{E}[\nabla^{2}\ell_{\theta}(X,Y)]\) _is non-singular._

These regularity conditions are fairly mild--for example, it is straightforward to verify that the loss of the mean estimator, \(\frac{1}{2}\|y-\theta\|^{2}\), satisfies Definition A.1. We note, however, that PPI++ can be applied to merely "smooth enough" losses, which also include non-continuously differentiable functions losses like the quantile loss. As we primarily focus on mean estimation in this work, for simplicity of analysis we only consider (the still broad class) of losses satisfying Definition A.1, though our results can be expected to readily extend to the more general case following a similar treatment as in [3; 29].

To support our theoretical results, we also provide the following two lemmas.

**Lemma A.1** (Slutsky's Theorem, general form).: _Let \(X_{n}\overset{d}{\to}X\) and \(Y_{n}\overset{p}{\to}c\), where \(X_{n}\), \(X\), and \(Y_{n}\) are random vectors, and \(c\) is a constant vector. Then for any continuous function \(g\), \(g(X_{n},Y_{n})\overset{d}{\to}g(X,c)\)._

Proof.: By Theorem 2.7 of [29] we have \((X_{n},Y_{n})\overset{d}{\to}(X,c)\). The continuous mapping theorem then implies that \(g(X_{n},Y_{n})\overset{d}{\to}g(X,c)\). 

**Lemma A.2**.: _Under the regularity conditions of Definition A.1, we have that \(\hat{\theta}^{\mathrm{SPP}}_{\hat{\lambda}}\overset{p}{\to}\theta^{*}\)._

Proof.: Under the regularity conditions of Definition A.1 and the fact that \(\hat{\lambda}_{k}\overset{p}{\to}\lambda_{k}\) where \(\lambda_{k}\) is constant, the uniform weak law of large numbers can be applied to each term in \(L^{\mathrm{SPP}}_{\hat{\lambda}}(\theta)\) so that

\[\sup_{\theta\in\Theta}\left\|L^{\mathrm{SPP}}_{\hat{\lambda}}(\theta)-\mathbb{ E}[\ell_{\theta}(X,Y)]\right\|\overset{p}{\to}0.\]

As \(\theta^{*}\) is unique, \(\Theta\) is compact, and \(\ell_{\theta}\) is continuous it follows from Theorem 2.1 of [20] that

\[\hat{\theta}^{\mathrm{SPP}}_{\hat{\lambda}}\overset{p}{\to}\theta^{*}.\]

### Proof of Theorem 2

Proof.: For ease of notation, we will define

\[\tilde{L}^{f}_{N_{k}}(\theta) =\frac{1}{N_{k}}\sum_{i=1}^{N_{k}}\ell_{\theta}(\tilde{X}_{ik},f( \tilde{X}_{ik})\] \[L_{n_{k}}(\theta) =\frac{1}{n_{k}}\sum_{i=1}^{n_{k}}\ell_{\theta}(X_{ik},Y_{ik})\] \[L^{f}_{n_{k}}(\theta) =\frac{1}{n_{k}}\sum_{i=1}^{n_{k}}\ell_{\theta}(X_{ik},f(X_{ik})).\]

We will also use the following shorthand for the conditional gradients:

\[\nabla\ell_{k,\theta} :=\nabla\ell_{\theta}(X,Y)\mid X\in\mathcal{A}_{k}\quad\text{and}\] \[\nabla\ell^{f}_{k,\theta} :=\nabla\ell_{\theta}(X,f(X))\mid X\in\mathcal{A}_{k}.\]As samples are i.i.d. within each stratum, the CLT gives

\[\sqrt{N_{k}}\Big{(}\nabla\tilde{L}^{f}_{N_{k}}(\theta^{*})-\mathbb{E}[\nabla\ell^ {f}_{k,\theta^{*}}]\Big{)}\xrightarrow{d}\mathcal{N}\left(0,\mathrm{Cov}\left( \nabla\ell^{f}_{k,\theta^{*}}\right)\right),\] (22)

and

\[\sqrt{n_{k}}\begin{bmatrix}\nabla L_{n_{k}}(\theta^{*})-\mathbb{E}[\nabla\ell _{k,\theta^{*}}]\\ \nabla L^{f}_{n_{k}}(\theta^{*})-\mathbb{E}[\nabla\ell^{f}_{k,\theta^{*}}] \end{bmatrix}\xrightarrow{d}\mathcal{N}\left(\begin{bmatrix}0\\ 0\end{bmatrix},\mathrm{Cov}\left(\begin{bmatrix}\nabla\ell_{k,\theta^{*}}\\ \nabla\ell^{f}_{k,\theta^{*}}\end{bmatrix}\right)\right).\] (23)

Since samples \(S_{n}\) and \(\tilde{S}_{N}\) are independent from each other, we also have that (22) and (23) converge jointly. Applying Lemma A.1 for the following continuous function of \(\hat{\lambda}_{k}\xrightarrow{p}\lambda_{k}\) then gives

\[\sqrt{n}\left(\hat{\lambda}_{k}\cdot\mathcal{N}\Big{(} \nabla\tilde{L}^{f}_{N_{k}}(\theta^{*})-\mathbb{E}[\nabla\ell^{f}_{k,\theta^{ *}}]\Big{)}+\begin{bmatrix}1\\ -\hat{\lambda}_{k}\end{bmatrix}^{\top}\begin{bmatrix}\nabla\tilde{L}_{n}( \theta^{*})-\mathbb{E}[\nabla\ell_{k,\theta^{*}}]\\ \nabla\tilde{L}^{f}_{n_{k}}(\theta^{*})-\mathbb{E}[\nabla\ell^{f}_{k,\theta^{ *}}]\end{bmatrix}\right)\] \[\xrightarrow{d}\mathcal{N}\left(0,\lambda_{k}^{2}\frac{n}{N_{k}} \mathrm{Cov}(\nabla\ell^{f}_{k,\theta^{*}})\right)+\mathcal{N}\left(0,\frac{n }{n_{k}}\mathrm{Cov}(\nabla\ell_{k,\theta^{*}}-\lambda_{k}\nabla\ell^{f}_{k, \theta^{*}})\right)\] (24) \[=\mathcal{N}\left(0,\frac{r}{\tilde{\rho}_{k}}\cdot V^{\lambda_{k }}_{k,f,\theta^{*}}+\frac{1}{\rho_{k}}V^{\lambda_{k}}_{k,\Delta,\theta^{*}} \right).\] (25)

Since \(\theta^{*}\) satisfies \(\mathbb{E}[\nabla\ell_{\theta^{*}}]=0\), by the law of total expectation

\[\sum_{k=1}^{K}w_{k}\mathbb{E}[\nabla\ell_{k,\theta^{*}}]=\mathbb{E}[\nabla \ell_{\theta^{*}}]=0.\] (26)

Using this fact, we can write

\[\nabla L^{\mathrm{SPP}}_{\lambda}(\theta)=\sum_{k=1}^{K}w_{k}\left(\nabla \left(\hat{\lambda}_{k}\tilde{L}^{f}_{N_{k}}(\theta)+L_{n_{k}}(\theta)-\hat{ \lambda}_{k}L^{f}_{n_{k}}(\theta)\right)-\mathbb{E}[\nabla\ell_{k,\theta^{*}} ]\right).\] (27)

Since samples across the \(K\) (where here \(K\) is a constant) stratas are independent, combining the results of (25) with (27) yields

\[\sqrt{n}\nabla L^{\mathrm{SPP}}_{\lambda}(\theta^{*})\xrightarrow{d}\mathcal{ N}(0,B^{\lambda}_{w}).\] (28)

Applying the mean value theorem, we have

\[\nabla L^{\mathrm{SPP}}_{\lambda}(\hat{\theta}^{\mathrm{SPP}}_{\lambda})= \nabla L^{\mathrm{SPP}}_{\lambda}(\theta^{*})+\nabla^{2}L^{\mathrm{SPP}}_{ \lambda}(\tilde{\theta})(\hat{\theta}^{\mathrm{SPP}}_{\lambda}-\theta^{*})\] (29)

for some \(\tilde{\theta}\) between \(\hat{\theta}^{\mathrm{SPP}}_{\lambda}\) and \(\theta^{*}\). Let \(A^{\dagger}\) represent the pseudoinverse of \(A\). Then

\[\hat{\theta}^{\mathrm{SPP}}_{\lambda}-\theta^{*}=\nabla^{2}L^{\mathrm{SPP}}_{ \lambda}(\tilde{\theta})^{\dagger}\left(\nabla L^{\mathrm{SPP}}_{\lambda}( \hat{\theta}^{\mathrm{SPP}}_{\lambda})-\nabla L^{\mathrm{SPP}}_{\lambda}( \theta^{*})\right).\] (30)

As \(\hat{\theta}^{\mathrm{SPP}}_{\lambda}\xrightarrow{p}\theta^{*}\) per Lemma A.2, we have \(\tilde{\theta}\xrightarrow{p}\theta^{*}\). Under the regularity conditions of Definition A.1 and the fact that \(\hat{\lambda}_{k}\xrightarrow{p}\lambda_{k}\), an application of the uniform weak law of large numbers and the continuous mapping theorem then gives

\[\nabla^{2}L^{\mathrm{SPP}}_{\lambda}(\tilde{\theta})^{\dagger}\xrightarrow{p} \nabla^{2}\mathbb{E}[\ell_{\theta*}]^{-1},\] (31)

which is a constant term. Furthermore, by the law of total expectation,

\[\mathbb{E}[\nabla^{2}\ell_{\theta^{*}}]^{-1}=\left(\sum_{k=1}^{K}w_{k}H_{k, \theta^{*}}\right)^{-1}=A^{-1}_{w}.\] (32)

Finally, the fact that \(\hat{\theta}^{\mathrm{SPP}}_{\lambda}\xrightarrow{p}\theta^{*}\) and \(\theta^{*}\in\mathrm{int}(\Theta)\), together with the fact that \(\hat{\theta}^{\mathrm{SPP}}_{\lambda}\) is a minimum of \(L^{\mathrm{SPP}}_{\lambda}\), implies that \(\nabla L^{\mathrm{SPP}}_{\lambda}(\hat{\theta}^{\mathrm{SPP}}_{\lambda}) \xrightarrow{p}0\). Combining these results with (28) via Lemma A.1 and the fact that \(A^{-1}_{w}\) is symmetric for twice continuously differentiable \(\ell_{\theta}\) gives

\[\sqrt{n}(\tilde{\theta}-\theta^{*})\xrightarrow{d}\mathcal{N}(0,A^{-1}_{w}B^{ \lambda}_{w}(A^{-1}_{w})^{\top})=\mathcal{N}(0,A^{-1}_{w}B^{\lambda}_{w}A^{-1}_ {w}).\] (33)

### Proof of Corollary 2

Proof.: The regularity conditions of Definition A.1 allow us to apply Lemma 4.3 of [20] to show that each of the plug-in estimates for each stratum term satisfies

\[\widehat{H}_{k,\hat{\theta}^{\mathrm{SPP}}}\overset{p}{\to}H_{k,\theta^{*}}\quad \text{and}\quad\widehat{V}_{k,f,\hat{\theta}^{\mathrm{SPP}}}^{\lambda_{k}} \overset{p}{\to}\widehat{V}_{k,f,\theta^{*}}^{\lambda_{k}},\quad\text{and} \quad\widehat{V}_{k,\Delta,\hat{\theta}^{\mathrm{SPP}}}^{\lambda_{k}}\overset{p }{\to}\widehat{V}_{k,\Delta,\theta^{*}}^{\lambda_{k}}.\]

which implies that the weighted plug-in estimate for \(\hat{\Sigma}_{w}^{\hat{\Sigma}}\) satisfies \(\hat{\Sigma}_{w}^{\hat{\Sigma}}\overset{p}{\to}\Sigma_{w}^{\lambda}\). (14) is thus equivalent to taking the \(\left(\frac{\alpha}{2},1-\frac{\alpha}{2}\right)\) quantiles of the asymptotic normal distribution of \(\hat{\theta}^{\mathrm{SPP}}\), implying (15). 

### Proof of Proposition 1

Proof.: In the special case of the square loss, the Hessian is the identity matrix. Therefore, simplifying and applying the linearity of the trace, we have

\[\mathrm{Tr}(\Sigma_{w}^{\lambda}) =\sum_{k=1}^{K}w_{k}^{2}\mathrm{Tr}\left(\frac{r}{\tilde{\rho}_{ k}}\cdot V_{k,f,\theta^{*}}^{\lambda_{k}}+\frac{1}{\rho_{k}}V_{k,\Delta,\theta^{*} }^{\lambda_{k}}\right)\] (34) \[=\sum_{k=1}^{K}w_{k}\mathrm{Tr}\left(r\cdot V_{k,f,\theta^{*}}^{ \lambda_{k}}+V_{k,\Delta,\theta^{*}}^{\lambda_{k}}\right)\] (35) \[=r\sum_{k=1}^{K}w_{k}\mathrm{Tr}\left(V_{k,f,\theta^{*}}^{\lambda _{k}}\right)+\sum_{k=1}^{K}w_{k}\mathrm{Tr}\left(V_{k,\Delta,\theta^{*}}^{ \lambda_{k}}\right)\] (36) \[=r\sum_{j=1}^{d}\sum_{k=1}^{K}w_{k}V_{k,f,\theta^{*},jj}^{\lambda _{k}}+\sum_{j=1}^{d}\sum_{k=1}^{K}w_{k}V_{k,\Delta,\theta^{*},jj}^{\lambda_{k}}\] (37)

where we include the subscript \(jj\) to index the diagonal variance terms of both covariance matrices. For ease of notation, denote the conditional variances as

\[V_{f,j}\mid Z =k:=\lambda^{2}\nabla\ell_{\theta^{*}_{j}}(X,f(X))\mid X\in \mathcal{A}_{k}\quad\text{and}\] \[V_{\Delta,j}\mid Z =k:=\nabla\ell_{\theta^{*}_{j}}(X,Y)-\lambda\nabla\ell_{\theta^{*} _{j}}(X,f(X))\mid X\in\mathcal{A}_{k}.\]

Then we can write (37) as

\[r\sum_{j=1}^{d}\sum_{k=1}^{K}w_{k}V_{k,f,\theta^{*},jj}^{\lambda_{k}}+\sum_{j= 1}^{d}\sum_{k=1}^{K}w_{k}V_{k,\Delta,\theta^{*},jj}^{\lambda_{k}}=\sum_{j=1}^{ d}r\mathbb{E}[\mathrm{Var}(V_{f,j}\mid Z)]+\mathbb{E}[\mathrm{Var}(V_{\Delta,j} \mid Z)],\] (38)

and apply the law of total variance to get

\[\mathrm{Tr}(\Sigma_{w}^{\lambda}) =\sum_{j=1}^{d}r\mathbb{E}[\mathrm{Var}(V_{f,j}\mid Z)]+\mathbb{E }[\mathrm{Var}(V_{\Delta,j}\mid Z)]\] (39) \[=\sum_{j=1}^{d}r(\mathrm{Var}(V_{f,j})-\mathrm{Var}(\mathbb{E}[V_ {f,j}\mid Z]))+(\mathrm{Var}(V_{\Delta,j})-\mathrm{Var}(\mathbb{E}[V_{\Delta,j }\mid Z]))\] (40) \[\leq\sum_{j=1}^{d}r\mathrm{Var}(V_{f,j})+\mathrm{Var}(V_{\Delta,j})\] (41) \[=\mathrm{Tr}(\Sigma^{\lambda}).\] (42)

Finally, (41) holds with equality iff both \(\mathrm{Var}(\mathbb{E}[V_{f,j}\mid Z])=0\) and \(\mathrm{Var}(\mathbb{E}[V_{\Delta,j}\mid Z])=0\), which, combined with the assumption that \(\mathbb{P}(Z=k)=w_{k}>0\) for all \(k\), is only satisfied when both \(\mathbb{E}[V_{f,j}\mid Z]\) and \(\mathbb{E}[V_{\Delta,j}\mid Z]\) are constants.

### Proof of Proposition 2

Proof.: By linearity of \(B_{w}^{\lambda}\), we can rewrite \(\Sigma_{w}^{\lambda}=A_{w}^{-1}B_{w}^{\lambda}A_{w}^{-1}\) as

\[\Sigma_{w}^{\lambda} =\sum_{k=1}^{K}w_{k}^{2}A_{w}^{-1}\left(\frac{r}{\tilde{\rho}_{k}} \cdot V_{k,f,\theta^{*}}^{\lambda_{k}}+\frac{1}{\rho_{k}}V_{k,\Delta,\theta^{* }}^{\lambda_{k}}\right)A_{w}^{-1}\] (43) \[=\sum_{k=1}^{K}\frac{w_{k}^{2}}{\rho_{k}}A_{w}^{-1}\left(\frac{r \rho_{k}}{\tilde{\rho}_{k}}\cdot V_{k,f,\theta^{*}}^{\lambda_{k}}+V_{k,\Delta, \theta^{*}}^{\lambda_{k}}\right)A_{w}^{-1}\] (44) \[=\sum_{k=1}^{K}\frac{w_{k}^{2}}{\rho_{k}}A_{w}^{-1}\left(\frac{n_ {k}}{N_{k}}\cdot V_{k,f,\theta^{*}}^{\lambda_{k}}+V_{k,\Delta,\theta^{*}}^{ \lambda_{k}}\right)A_{w}^{-1}.\] (45)

By linearity of the trace, we then also have

\[\mathrm{Tr}(\Sigma_{w}^{\lambda})=\sum_{k=1}^{K}\frac{w_{k}^{2}}{\rho_{k}} \mathrm{Tr}\left(A_{w}^{-1}\left(\frac{n_{k}}{N_{k}}\cdot V_{k,f,\theta^{*}}^ {\lambda_{k}}+V_{k,\Delta,\theta^{*}}^{\lambda_{k}}\right)A_{w}^{-1}\right).\] (46)

As each term in the sum is independent, we can minimize the sum by minimizing each individual term, i.e.,

\[\lambda_{k}^{*}=\operatorname*{argmin}_{\lambda_{k}}\mathrm{Tr}\left(A_{w}^{- 1}\left(\frac{n_{k}}{N_{k}}\cdot V_{k,f,\theta^{*}}^{\lambda_{k}}+V_{k,\Delta, \theta^{*}}^{\lambda_{k}}\right)A_{w}^{-1}\right)\] (47)

for \(k=1,\ldots,K\). We can then directly apply Proposition 2 of [3] to get

\[\lambda_{k}^{*}=\frac{\mathrm{Tr}\left(A_{w}^{-1}(\mathrm{Cov}(\nabla\ell_{k, \theta^{*}},\nabla\ell_{k,\theta^{*}}^{\ell})+\mathrm{Cov}(\nabla\ell_{k, \theta^{*}}^{f},\nabla\ell_{k,\theta^{*}}))A_{w}^{-1}\right)}{2(1+r_{k}) \mathrm{Tr}\left(A_{w}^{-1}\mathrm{Cov}(\nabla\ell_{k,\theta^{*}}^{f})A_{w}^{ -1}\right)},\] (48)

where \(r_{k}=n_{k}/N_{k}\). 

### Proof of Proposition 3

Proof.: Following the same derivation as Proposition 2, we can rewrite

\[\mathrm{Tr}(\Sigma_{w}^{\lambda}) =\sum_{k=1}^{K}w_{k}^{2}\mathrm{Tr}\left(A_{w}^{-1}\left(\frac{r} {\tilde{\rho}_{k}}\cdot V_{k,f,\theta^{*}}^{\lambda_{k}}+\frac{1}{\rho_{k}}V_{ k,\Delta,\theta^{*}}^{\lambda_{k}}\right)A_{w}^{-1}\right)\] (49) \[=r\sum_{k=1}^{K}\frac{w_{k}^{2}}{\tilde{\rho_{k}}}\mathrm{Tr} \left(A_{w}^{-1}V_{k,f,\theta^{*}}^{\lambda_{k}}A_{w}^{-1}\right)+\sum_{k=1}^ {K}\frac{w_{k}^{2}}{\rho_{k}}\mathrm{Tr}\left(A_{w}^{-1}V_{k,\Delta,\theta^{*} }^{\lambda_{k}}A_{w}^{-1}\right),\] (50)

and therefore can optimize \(\tilde{\rho}_{k}\) and \(\rho_{k}\) independently.

We start with \(\rho_{k}\). Let \(z_{k}=\mathrm{Tr}\left(A_{w}^{-1}V_{k,\Delta,\theta^{*}}^{\lambda_{k}}A_{w}^{-1 }\right)\). We then solve the constrained optimization problem

\[\operatorname*{minimize}\ \sum_{k=1}^{K}\frac{w_{k}^{2}}{\rho_{k}}z_{k} \quad\text{s.t.}\quad\sum_{k=1}^{K}\rho_{k}=1,\rho_{k}\geq 0.\] (51)

Turning this into a Lagrangian with additional slack variables \(s_{k}\geq 0\), we have

\[\mathcal{J}(\rho,\mu,\eta,s)=\sum_{k=1}^{K}\frac{w_{k}^{2}}{\rho_{k}}z_{k}+\mu \left(\sum_{i=1}^{K}\rho_{k}-1\right)+\sum_{k=1}^{K}\eta_{k}(\rho_{k}-s_{k}^{2})\] (52)Setting \(\nabla\mathcal{J}(\rho,\mu,\eta)=0\) and solving for \(\rho_{k}\) then gives:

\[\frac{\partial\mathcal{J}}{\partial\rho_{k}} =-\frac{w_{k}^{2}}{\rho_{k}^{2}}z_{k}+\mu+\eta_{k}=0\] (53) \[\frac{\partial\mathcal{J}}{\partial\mu} =1-\sum\rho_{k}=0\] (54) \[\frac{\partial\mathcal{J}}{\partial\eta_{k}} =\rho_{k}-s_{k}^{2}=0\] (55) \[\frac{\partial\mathcal{J}}{\partial s_{k}} =-2\eta_{k}s_{k}=0\] (56)

Assume that the inequality constraint is inactive, and \(\eta_{k}=0\). Solving for \(\rho_{k}^{*}\),

\[\rho_{k}^{*}=s_{k}^{2}=\sqrt{\frac{w_{k}^{2}z_{k}}{\mu}} \Longrightarrow\sum_{k=1}^{K}\sqrt{\frac{w_{k}^{2}z_{k}}{\mu}}=1\] (57) \[\Longrightarrow\sqrt{\mu}=\sum_{k=1}^{K}\sqrt{w_{k}^{2}z_{k}}\] (58) \[\Longrightarrow\rho_{k}^{*}=\frac{w_{k}\sqrt{z_{k}}}{\sum_{k^{ \prime}=1}^{K}w_{k^{\prime}}\sqrt{z_{k^{\prime}}}}.\] (59)

We can now verify that the constraint is inactive, with \(\rho_{k}^{*}\geq 0\), since \(\sum_{k=1}^{K}w_{k}=1,w_{k}\geq 0\) by definition of a valid probability distribution, and we have \(z_{k}\geq 0\) since it is a sum of non-negative variance terms.

The same analysis can then be applied to \(\tilde{\rho_{k}}\), but for \(z_{k}=\operatorname{Tr}\left(A_{w}^{-1}V_{k,f,\sigma}^{\lambda_{k}}.A_{w}^{-1 }\right)\). 

## Appendix B A simplified estimate of the sample allocation for mean estimation

In the setting of Example 2 (i.e., \(1\)-\(d\) mean estimation), assume that \(c(y\mid x)\approx\mathbb{P}(Y=y\mid X=x)\) is a confidence estimate for label \(y\in\mathcal{Y}\), where \(\mathcal{Y}\) is discrete. Then, if we define the autorater as

\[f(x)=\sum_{y\in\mathcal{Y}}c(y\mid x)\cdot y\approx\mathbb{E}[Y\mid X=x],\] (60)

the term \(\operatorname{Var}(Y-\lambda_{k}f(X)\mid X\in\mathcal{A}_{k})\) further simplifies to \(\approx\operatorname{Var}(Y\mid X\in\mathcal{A}_{k})\) for any value of \(\lambda_{k}\). Applying the law of total variance, we can conveniently write \(\operatorname{Var}(Y\mid X\in\mathcal{A}_{k})\) as

\[\operatorname{Var}(Y\mid X\in\mathcal{A}_{k})=\mathbb{E}[\operatorname{Var}( Y\mid X)\mid X\in\mathcal{A}_{k}]+\operatorname{Var}(\mathbb{E}[Y\mid X]\mid X \in\mathcal{A}_{k}),\] (61)

which can be empirically estimated as \(\hat{\sigma}_{k}\) on unlabeled autorater data, \(\tilde{X}_{ik}\), \(i=1,\ldots,N_{k}\), via

\[\hat{\sigma}_{k}^{2}=\widehat{\mathbb{E}}_{N_{k}}\left[\sum_{y\in\mathcal{Y}} c(y\mid\tilde{X}_{ik})\cdot y-f(\tilde{X}_{ik}))\right]+\widehat{\operatorname{ Var}}_{N_{k}}(f(\tilde{X}_{ik})),\] (62)

where \(\widehat{E}_{N_{k}}\) and \(\widehat{\operatorname{Var}}_{N_{k}}\) denote the empirical mean and variance over all \(\tilde{X}_{ik}\), respectively. Lastly, when \(\mathcal{Y}\) is binary (which is the case in all of our experiments in SS5.2), this becomes

\[\hat{\sigma}_{k}^{2}=\widehat{\mathbb{E}}_{N_{k}}\left[f(\tilde{X}_{ik})(1-f( \tilde{X}_{ik}))\right]+\widehat{\operatorname{Var}}_{N_{k}}(f(\tilde{X}_{ik} )),\] (63)

which can be easily calculated in Python as np.mean(yhat * (1 - yhat)) + np.var(yhat).

## Appendix C Additional experimental results

We also evaluate StratPPI on the Chatbot Arena dataset [10], in which we evaluate the win-rate of gpt-4-1106-preview over claude-2.1.4 See Figure 3. Specifically, we use the standard ChatbotArena auto-eval prompt5 and map [[A*B]], [[A*B] to 1, [[A*B] to 0.5, and [[B*A]], [[B*A]] to 0. We then do self-consistency sampling and take the average of 10 samples from Gemini Ultra. This is used as both our final autorater estimate and confidence. We find that stratification also helps in this setting. However, in line with prior and contemporary work [e.g., 35; 16] we found confidence scores from the LLM-as-a-judge to be fairly uncalibrated, which makes our heuristic allocation less effective at larger n (though still effective at small labeled sample sizes \(n\)). Investigating robust heuristics in the face of miscalibration (e.g., via regularization or recalibration) would likely help, and is a direction for future work.

Footnote 5: https://github.com/lm-sys/arena-hard-auto/blob/main/config/judge_config.yaml

Figure 3: Win-rate experiment on Chatbot Arena for gpt-4-1106-preview vs. claude-2.1. Scores are based on the average label (‘better’ = 1 vs. ‘worse’ = 0) over 10 samples from Gemini Ultra acting as a LLM judge. Interestingly, as these confidence scores are not calibrated, our heuristic becomes overly aggressive at higher \(n\). Future work can explore how to best incorporate additional regularization into the estimated optimal sampling ratios \(\rho\).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: All claims are supported in the paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We address this in the conclusion. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: Proofs are included in the appendix.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Results are based on publically available data, and simulations, and all procedures are described. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: The data is publicly available, and the methodology is simple to implement. Pseudocode is given. Code may be made available at a future date. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Detailed information is provided. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Error bars are provided for the main experimental results. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Compute resources required are very light, as no model training is performed. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: No issues. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The paper discusses the advantages of the method for more effective development of large language models. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: NA Guidelines: The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: NA Guidelines: The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: NA Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: NA Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: NA Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.