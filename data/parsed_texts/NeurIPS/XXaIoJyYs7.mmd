# MedJourney: Benchmark and Evaluation of Large Language Models over Patient Clinical Journey

Xian Wu1, Yutian Zhao1, Yunyan Zhang1, Jiageng Wu2, Zhihong Zhu3

**Yingying Zhang1, Yi Ouyang1, Ziheng Zhang1, Huimin Wang1, Zhenxi Lin1**

**Jie Yang4, Shuang Zhao5, Yefeng Zheng6***

1Tencent Youtu Lab, Jarvis Research Center 2Thejiang University

3Peking University 4Harvard Medical School

5 Xiangya Hospital 6Westlake University

{kevinxwu, yutianzhao, yunyanzhang}@tencent.com

jiagengwu@zju.edu.cn, zhinongzhu@stu.pku.edu.cn

{ninzhang, yiouyang, zihengzhang, chalerislin}@tencent.com

jieynlp@gmail.com, shuangxy@csu.edu.cn, yefengzheng@westlake.com.cn

Corresponding author

###### Abstract

Large language models (LLMs) have demonstrated remarkable capabilities in language understanding and generation, leading to their widespread adoption across various fields. Among these, the medical field is particularly well-suited for LLM applications, as many medical tasks can be enhanced by LLMs. Despite the existence of benchmarks for evaluating LLMs in medical question-answering and exams, there remains a notable gap in assessing LLMs' performance in supporting patients throughout their entire hospital visit journey in real-world clinical practice. In this paper, we address this gap by dividing a typical patient's clinical journey into four stages: planning, access, delivery and ongoing care. For each stage, we introduce multiple tasks and corresponding datasets, resulting in a comprehensive benchmark comprising 12 datasets, of which five are newly introduced, and seven are constructed from existing datasets. This proposed benchmark facilitates a thorough evaluation of LLMs' effectiveness across the entire patient journey, providing insights into their practical application in clinical settings. Additionally, we evaluate three categories of LLMs against this benchmark: 1) proprietary LLM services such as GPT-4; 2) public LLMs like QWen; and 3) specialized medical LLMs, like HuatuoGPT2. Through this extensive evaluation, we aim to provide a better understanding of LLMs' performance in the medical domain, ultimately contributing to their more effective deployment in healthcare settings.

## 1 Introduction

Large language models (LLM), such as ChatGPT and GPT-4, have showcased impressive capabilities in comprehending users' intent and generating coherent responses (Zhao et al., 2023). Their flexible input requirements make them suitable for a broad range of tasks across various domains. Among these, the medical domain stands out as a particularly fitting area for LLM applications (Thirunawukarasu et al., 2023). Over the past two decades, hospital IT systems like Hospital Information System (HIS), Laboratory Information System (LIS), and Picture Archiving and Communication System (PACS) have accumulated a wealth of clinical data, providing a robust foundation for training LLMs (Wu et al., 2024). Simultaneously, there is a significant demand for LLM applications in the medical field, such as online inquiries (Liu et al., 2022), diagnostic assistance (Rasmy et al.,2021), medication recommendations (Wu et al., 2022), and discharge summary (Liu et al., 2022c). Implementing LLMs in these medical scenarios can alleviate doctors' workload and enhance clinical efficiency (Wang et al., 2023b).

However, given the critical nature of patient care, the medical domain has little tolerance for errors in LLM outputs. Therefore, a comprehensive evaluation of these models is essential before deployment. Several benchmarks have been proposed to date, which can be categorized into three types: 1) Exam-based, CMExam (Liu et al., 2024b) is built from China National Medical Licensing Examination (CNMLE), MedQA (Jin et al., 2021) is built from United States Medical Licensing Examination (USMLE) and MedMCQA (Pal et al., 2022) is built from All India Institute of Medical Sciences (AIIMS PG) and National Eligibility cum Entrance Test (NEET PG). These licensing exams, designed to judge whether medical school students are qualified to be doctors, provide a reasonable basis for evaluating LLM performance; 2) QA-based, involving single (Abacha et al., 2017) and multi-turn interactions (Liu et al., 2022a) between patients and doctors, which can evaluate the performance of LLM in dealing with patients' inquiries; 3) Task-based, assessing performance in various medical Natural Language Processing (NLP) tasks, such as summarization, medical named entity recognition (NER), etc. For example, CBLUE (Zhang et al., 2022) is a collection of medical tasks. PromptCBLUE (Zhu et al., 2023) further adapts CBLUE for LLM evaluation by adding different forms of prompts. For more detailed information and illustrative examples, please refer to Appendix A.1.2.

A limitation of existing medical benchmarks is that they are organized either by the question type (multiple-choice, question answering) or the task type (NER, Classification, etc.), and many of them do not include clinical text data generated from real-world clinical practice (Wu et al., 2024). In addition, existing datasets are not structured according to the steps of the clinical process in patient care. As a result, it's difficult to assess the performance of LLMs in assisting patients in real clinics. In this paper, we segment the entire patient clinical journey into four stages. For each stage, we introduce multiple tasks. In summary, the contribution of this paper is threefold:

* We introduce a new Chinese benchmark dataset MedJourney 2 that covers the entire workflow of the patient's clinical journey which organizes the benchmark w.r.t patient clinical journey. Footnote 2: https://github.com/Medical-AI-Learning/MedJourney
* In total, we introduce 12 datasets corresponding to 12 different tasks across four stages. Of these, 7 datasets are reconstructed from existing public sources, while 5 are newly proposed in this paper. All new datasets have been meticulously processed by professional doctors, ensuring high quality and reliability. The detailed information can be found in Appendix A.2.
* We evaluate the performance of existing LLMs on MedJourney. We evaluate not only the close source LLMs, like ChatGPT 3 and GPT-4 4 but also open-source LLMs, like QWen (Bai et al., 2023) and ChatGLM (Du et al., 2022). We also include medical LLMs like Huatuo GPT2 (Chen et al., 2023b). Since MedJourney is in Chinese, we didn't include the English-centric LLMs, like Llama series (Touvron et al., 2023a,b). In addition to the accuracy and NLG metrics, we also conduct entity-level evaluations to verify performance from a semantic perspective.

Footnote 3: https://openai.com/index/chatgpt/

Footnote 4: https://openai.com/index/gpt-4/

Footnote 5: https://medbench.opencompass.org.cn/home

## 2 Related Works

MultiMedQA (Singhal et al., 2023) is a widely recognized benchmark for evaluating the performance of LLMs in the medical domain. This benchmark comprises seven datasets: MedQA (Jin et al., 2021), MedMCQA (Pal et al., 2022), PubMedQA (Jin et al., 2019), MMUL clinical topics (Hendrycks et al., 2020), LiveQA (Abacha et al., 2017), MedicationQA (Abacha et al., 2019), and HealthSearchQA. The first four datasets consist of multiple-choice questions, while the last three contain questions requiring long-form free-text answers. This benchmark is in English. Recently, there are also some new clinical benchmarks that focus on diagnostic reasoning (Gao et al., 2023), multi-modal agent (Schmindgall et al., 2024) and doctor-patient conversation (Wang et al., 2023c).

For Chinese benchmarks, MedBench (Cai et al., 2024) primarily included multiple-choice questions from medical examinations. However, the MedBench website (Liu et al., 2024a) 5 featured a broader range of tasks, such as QA and NER. Liu et al. (2024b) constructed a benchmark, CMEExam, for evaluating LLMs based on medical exam data collected from the web. In addition to the answer, the authors appended five types of labels to each question, enabling a detailed performance analysis. CMB (Wang et al., 2023) included both medical exams and some clinical case studies. Zhang et al. (2022) introduced CBLUE, which comprises multiple biomedical tasks. Since CBLUE was not specifically designed to evaluate LLMs' performance, Zhu et al. (2023) introduced multiple prompt templates for each task and restructured each instance in CBLUE into a prompt and target format. For free-form QA datasets, there are CMedQA (Zhang et al., 2017), CMedQA2 (Zhang et al., 2018), and WebMedQA (He et al., 2019). The differences between existing benchmarks and MedJourney are summarized in Table 1.

## 3 Patient Clinical Journey

Inspired by the ideal patient journey in the digital healthcare6, as shown in Figure 1, we divide the patient clinical journey into four stages: planning, access, delivery, and ongoing care, and then propose 12 data sets accordingly, among which, Department Recommendation (DR), Pre-Consultation Dialogue Summary(PCDS), Hospital Reception QA (HQA), Insurance QA (IQA) and Drug QA (DQA) are newly proposed, while the rest seven are rebuilt from existing datasets. More details about how the patient clinical journey is divided can be found in Appendix A.1.1 and more details about how these data sets are built can be found in Appendix A.2.

Footnote 6: https://www.qualtrics.com/au/experience-management/industry/patient-journey/

### Planning

At this stage, patients are becoming aware of potential health issues and are considering seeking medical care at a hospital. We propose three datasets to address the primary needs of patients at this stage: 1) Department Recommendation (DR) assists in identifying the most suitable departments based on patients' primary complaints; 2) Pre-Consultation Dialogue Summary (PCDS) utilizes a multi-turn conversation to gather and summarize patients' information; 3) Hospital Reception QA (HQA) compiles frequently asked questions from patients before and during their hospital visits.

#### 3.1.1 Department Recommendation

As online appointment for medical consultations becomes increasingly popular, a growing number of patients are choosing to use the outpatient intelligent guidance system to select suitable hospitals and departments. Department recommendation is a crucial component of this system, aiming to suggest appropriate departments based on the patient's primary complaints. However, patients often lack adequate medical knowledge, resulting in vague symptom descriptions. Moreover, the complexity of hospital department structures further complicates the task of department recommendation.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline
**Benchmark** & **Journey Organized** & **Multiple Choice** & **QA** & **Summarization** & **Classification** \\ \hline MedExam & ✗ & ✗ & ✗ & ✗ & ✗ \\ MedBench (paper) & ✗ & ✗ & ✗ & ✗ & ✗ \\ MedBench (website) & ✗ & ✓ & ✓ & ✓ & ✓ \\ PromptCBLUE & ✗ & ✗ & ✓ & ✓ & ✓ \\ CMB & ✗ & ✗ & ✓ & ✓ & ✗ \\ MultiMedQA & ✗ & ✗ & ✓ & ✓ & ✗ \\ MedJourney (ours) & ✗ & ✓ & ✓ & ✓ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of Ours and Existing Medical Benchmarks.

Figure 1: Four stages in patient journey which covers patients’ experience workflow. For each stage, we introduce several datasets to evaluate the performance of LLMs.

[MISSING_PAGE_FAIL:4]

[MISSING_PAGE_FAIL:5]

a skilled medical LLM should also understand the small differences and details that can separate similar diseases or conditions. They must be good at recognizing patterns, thinking about different possibilities, and judging the chance of each possible diagnosis based on the evidence they have. This dataset includes 1,761 question-answer pairs that cover a total of 1,490 diseases.

#### 3.3.3 Treatment Prediction

Doctors typically rely on established medical guidelines and evidence-based medicine when devising treatment plans. With all the relevant information at their disposal, they tailor the treatment plan to suit the patient's unique needs. The treatment prediction dataset serves as a tool to assess whether medical LLMs can generate accurate treatment plans that not only address the patient's diagnosis but also take into account their individual circumstances. This dataset comprises a total of 148 question-answer pairs, encompassing 133 distinct treatment plans.

#### 3.3.4 Medication Prediction

When recommending medications to patients, doctors consider the effectiveness, safety, potential side effects, and dosage requirements of the drugs. Prescribing an incorrect medication can lead to serious repercussions, such as not addressing the actual issue, potentially exacerbating the patient's condition, or postponing the necessary treatment. Therefore, it's essential for medical LLMs to possess a deep understanding of medicine and provide the most precise medication recommendations tailored to a patient's unique situation. The medicine prediction dataset comprises a total of 1029 question-answer pairs, covering 722 distinct medications.

### Ongoing Care

At this stage, patients have transitioned from receiving in-hospital care to managing their health independently outside the hospital. There are three key tasks where LLM can be applied: 1) Drug QA (DQA), this task involves addressing common questions patients may have about their medications; 2) Insurance QA (IQA), this task includes answering frequently asked questions about medical insurance; 3) Mental Health QA (MQA), this task involves addressing common questions from patients who may be dealing with mental health issues outside the hospital.

#### 3.4.1 Drug QA

Drug QA is a critical component in the post-treatment phase of a patient's clinical journey, where patients often seek clarity on their prescribed medications. Understanding the indications, contraindications, side effects, dosage instructions, and potential interactions of medications is essential for safe and effective self-care. To address this need, we have curated a dataset comprising 137 question-answer pairs that reflect real-world patient inquiries about various medications. Table 5 provides two examples of medication question-answer pairs.

#### 3.4.2 Insurance QA

Medical insurance is another important aspect of the healthcare system, especially after the patients are discharged from the hospital. The patients need to pay for the medical expenses, and the medical

\begin{table}
\begin{tabular}{|p{142.3pt}|p{142.3pt}|} \hline
**Question** & **Answer** \\ \hline \(\chi^{2}\)**49** & **17261** & **17261** \\ \hline \end{tabular}
\end{table}
Table 4: Example of Examination Prediction.

Figure 2: The distribution of disease subjects.

[MISSING_PAGE_FAIL:7]

like GPT-4 and QWen, incorporating a one-shot approach enhances performance across most tasks. This improvement is particularly noticeable for the summary tasks PDDS, as the LLMs learn the format of the output content. However, for delivery tasks, the performance of medical domain-specific models decreases. The introduced example may act as noise, negatively impacting performance.

#### 4.2.3 Entity Level Evaluation

For QA tasks, we incorporate an entity-based metric alongside general NLG metrics to evaluate LLMs from a semantic perspective. Specifically, we extract medical entities such as disease names, symptom terms, drug names, etc., from the golden answer of each instance. This extraction is initially performed by GPT-4 and subsequently double-checked manually. The frequency of these extracted entities is depicted in Figure 3(a). The 5 most frequent entities are listed in Figure 3(b).

Using these extracted entities, we compute the entity recall for all QA tasks. As demonstrated in Table 8, GPT-4 achieves near-optimal performance on almost all tasks, suggesting that GPT-4 is capable of incorporating key information in its responses.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Model**} & **DR** & **PCDS** & **HQA** & **DRG** & **PDDS** & **FP** & **DP** & **TP** & **MP** & **DQA** & **IOA** & **MQA** \\ Acc & B-4 & B-4 & B-4 & B-4 & Acc & Acc & Acc & Acc & Acc & B-4 & B-4 & B-4 \\ \hline \multicolumn{10}{c}{_Public LLMs_} \\ \hline CharGLM3 & 0.130 & 9.828 & 2.462 & 2.138 & 6.818 & 0.398 & 0.313 & 0.331 & 0.318 & 7.108 & **5.323** & 3.740 \\ QWen-7B & 0.264 & 5.489 & 1.843 & 1.773 & 7.796 & 0.528 & 0.559 & 0.493 & 0.638 & 7.256 & 4.241 & 3.917 \\ QWen-14B & 0.280 & 8.271 & 2.151 & **3.740** & 6.560 & 0.630 & 0.579 & 0.588 & 0.646 & 8.296 & 4.993 & 4.071 \\ QWen-32B & 0.304 & 7.614 & 1.978 & 2.491 & 6.987 & 0.667 & 0.684 & 0.608 & 0.715 & **8.596** & 4.609 & **4.214** \\ QWen-72B & **0.308** & 9.032 & 2.266 & 2.649 & 9.318 & 0.674 & 0.692 & 0.635 & 0.742 & 8.371 & 4.671 & 4.129 \\ \hline \multicolumn{10}{c}{_Private LLMs_} \\ \hline CharGLM & 0.255 & **11.242** & **2.576** & 2.912 & 10.632 & 0.419 & 0.369 & 0.364 & 0.382 & 6.928 & 5.303 & 3.767 \\ GPT-4 & 0.306 & 5.913 & 1.485 & 1.649 & 7.453 & 0.613 & 0.632 & 0.503 & 0.561 & 6.623 & 3.694 & 3.598 \\ \hline \multicolumn{10}{c}{_Specialized LLMs_} \\ \hline HautaoGPT2-7B & 0.214 & 4.387 & 2.094 & 1.410 & 6.200 & 0.681 & 0.688 & 0.554 & 0.629 & 8.079 & 4.165 & 4.035 \\ HuautoGPT2-34B & 0.278 & 7.951 & 1.764 & 1.998 & **12.082** & **0.757** & **0.766** & **0.662** & **0.777** & 8.382 & 4.149 & 4.132 \\ DISC-MedLLM & 0.216 & 2.775 & 2.524 & 1.288 & 6.190 & 0.269 & 0.241 & 0.149 & 0.282 & 5.570 & 3.928 & 2.908 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Zero-Shot Performance on the Clinical Journey Dataset.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Model**} & **DR** & **PCDS** & **HQA** & **DRG** & **PDDS** & **FP** & **DP** & **TP** & **MP** & **DQA** & **IOA** & **MQA** \\ Acc & B-4 & B-4 & B-4 & B-4 & Acc & Acc & Acc & Acc & B-4 & B-4 & B-4 \\ \hline \multicolumn{10}{c}{_Public LLMs_} \\ \hline CharGLM3 & 0.258 & 8.282 & **4.715** & 2.555 & **20.674** & 0.465 & 0.279 & 0.480 & 0.343 & 6.987 & **6.432** & 3.661 \\ QWen-7B & 0.292 & 10.414 & 2.386 & 2.410 & 12.868 & 0.412 & 0.436 & 0.365 & 0.485 & 7.207 & 5.129 & 4.074 \\ QWen-14B & 0.392 & 14.554 & 2.515 & 3.361 & 11.591 & 0.539 & 0.583 & 0.520 & 0.580 & 7.724 & 5.538 & 3.935 \\ QWen-32B & 0.354 & 12.567 & 2.349 & **4.305** & 13.788 & **0.676** & 0.739 & 0.709 & 0.699 & 7.971 & 5.030 & **4.570** \\ QWen-72B & 0.370 & 12.545 & 2.713 & 2.918 & 14.946 & 0.674 & **0.769** & **0.730** & **0.723** & **8.906** & 5.532 & 4.438 \\ \hline \multicolumn{10}{c}{_Private LLMs_} \\ \hline CharGLM & 0.328 & **21.316** & 4.437 & 3.199 & 18.714 & 0.364 & 0.345 & 0.357 & 0.355 & 5.915 & 6.091 & 3.911 \\ GPT-4 & **0.440** & 13.164 & 2.506 & 2.169 & 12.834 & 0.601 & 0.679 & 0.575 & 0.576 & 7.817 & 3.886 & 4.022 \\ \hline \multicolumn{10}{c}{_Specialized LLMs_} \\ \hline HautaoGPT2-7B & 0.208 & 2.182 & 1.674 & 1.526 & 5.224 & 0.410 & 0.399 & 0.250 & 0.266 & 8.178 & 4.545 & 4.061 \\ HautaoGPT2-34B & 0.274 & 12.159 & 2.567 & 2.958 & 16.244 & 0.641 & 0.588 & 0.466 & 0.659 &

#### 4.2.4 LLM Rating

We also leverage the LLMs to evaluate the performance of LLMs. We let GPT-4 to rate the performance of LLMs on benchmark, particularly for the QA questions.

[left=0pt,right=0pt,topsep=0pt,leftmargin=*]

The results are shown in Table 8.

[left=0pt,right=0pt,topsep=0pt,leftmargin=*]

The text box above displays the prompt that instructs GPT-4 to evaluate the performance of LLMs on the QA tasks in MedJourney. As shown in Table 9, if we average the performance on these 7 tasks. QWen-72B, GPT-4 and QWen 32b rank the top 3.

#### 4.2.5 Human Evaluation

In addition to the automatic metrics, we have also carried out a human evaluation of 10 Language Learning Models (LLMs) across 7 QA tasks. For this evaluation, we instructed the annotators to assign a score from 1 to 100, based on the following criteria. Table 10 displays the average human rating of performance of LLMs on each task.

[left=0pt,right=0pt,topsep=0pt,leftmargin=*]

The sentences of Human Evaluation

[left=0pt,right=0pt,topsep=0pt,leftmargin=*]

Based on the 3 criteria below, rate the model performance on a scale of 1-100.

[left=0pt,right=0pt,topsep=0pt,leftmargin=*]

**Accuracy:** The response provided by the large language model is accurate and has no factual errors. Conclusions are not made arbitrarily.

**Helfulness:** The model's response provides the patient with clear, instructive and practical assistance, specifically addressing the medical task.

**Linguistic Quality:** The response logical. The model correctly understands the medical task, and the expressions smooth and natural.

In this analysis, we average the performance across various QA tasks. GPT-4 emerges as the top performer, followed by QWen-32B, which deviates from the LLM rating ranking presented in Section 4.2.4. This raises the question of which metric aligns best with human evaluation: the B-4 metric (Table 6), the entity-based metric (Table 8), or the LLM rating (Table 9). We rank 10 Language Learning Models (LLMs) based on the average metrics across the seven tasks, as shown in Table 11.

To calculate the alignment between each metric and human evaluation, we enumerate all pairs of the 10 models, resulting in a total of 45 pairs. If the order of a pair of models according to one metric is

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline
**Model** & **PCDS** & **HQA** & **DRG** & **PDDS** & **DQA** & **IQA** & **MQA** \\ \hline \multicolumn{8}{c}{_Public LLMs_} \\ ChatGIM3 & 0.461 & 0.227 & 0.132 & 0.381 & 0.151 & 0.287 & 0.220 \\ QWen-7B & 0.495 & 0.281 & 0.155 & 0.428 & 0.188 & 0.321 & **0.287** \\ QWen-14B & 0.513 & 0.262 & 0.096 & **0.428** & **0.192** & 0.326 & 0.286 \\ QWen-32B & 0.524 & 0.270 & 0.175 & 0.420 & 0.176 & 0.340 & 0.259 \\ QWen-72B & 0.495 & **0.288** & 0.182 & 0.417 & 0.188 & 0.324 & 0.255 \\ \hline \multicolumn{8}{c}{_Private LLMs_} \\ ChatGPT & 0.462 & 0.250 & 0.151 & 0.363 & 0.148 & 0.302 & 0.247 \\ GPT-4 & **0.603** & 0.274 & **0.184** & 0.400 & 0.184 & **0.341** & 0.271 \\ \hline \multicolumn{8}{c}{_Specialized LLMs_} \\ HautoGPT2-7B & 0.415 & 0.234 & 0.151 & 0.327 & 0.184 & 0.309 & 0.241 \\ HautoGPT2-34B & 0.441 & 0.239 & 0.142 & 0.357 & 0.194 & 0.314 & 0.265 \\ DISC-MedLLM & 0.312 & 0.212 & 0.157 & 0.274 & 0.126 & 0.211 & 0.180 \\ \hline \hline \end{tabular}
\end{table}
Table 8: The Recall of Medical Entities for QA Tasks.

the same as that in human evaluation, we assign it a vote of 1; otherwise, it receives a vote of 0. We then use the alignment rate to measure the corelation of between a metric and human evaluation.

When comparing the rankings of each pair of LLMs to see if they align with human ratings, we find that the correlation between Human and GPT Judge is 84.4%; between Human and Entity, it's 73.3%; and between Human and B-4, it's 35.5%. However, when we combine the scores from GPT-Judge and Entity (calculated as GPT-Judge score/100 + Entity score), the alignment improves to 91.1%. This combined metric offers a robust measure for evaluating the performance of a Language Learning Model in the medical domain.

## 5 Conclusion

In this paper, we introduce a novel benchmark dubbed MedJourney, which is designed to evaluate the performance of LLMs from the perspective of a patient's clinical journey. We segment the entire journey into four stages, and for each stage, we propose multiple tasks that LLMs can undertake, providing corresponding test sets for evaluation. Of the 12 datasets proposed, seven are constructed from existing corpora, while the remaining five are newly proposed datasets. In addition to the prompt and target pairs, we also provide the key medical entities that needs to be addressed. The proposed MedJourney allows us to assess LLMs from a clinical perspective and identify which stages require further improvement.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline
**Model** & **PCDS** & **HOA** & **DRG** & **PDDS** & **DOA** & **IOA** & **MOA** & **AVERAGE** \\ \hline \multicolumn{8}{c}{_Public LLMs_} \\ ChsuGLM3 & 87 & 46 & 61 & 83 & 75 & 37 & 72 & 66 \\ QWE-7B & 96 & 65 & 70 & 87 & 78 & 53 & 73 & 75 \\ QWE-14B & 97 & 64 & 61 & 88 & 77 & 56 & 73 & 74 \\ QWE-32B & 96 & 65 & 74 & 87 & **79** & 62 & 70 & 76 \\ QWE-72B & 97 & 62 & 73 & 86 & **79** & 51 & 72 & 74 \\ \hline \multicolumn{8}{c}{_Private LLMs_} \\ ChsuGPT & 94 & 59 & 66 & 80 & 75 & 46 & 79 & 71 \\ GPT-4 & **98** & **68** & **76** & **91** & 78 & **64** & **82** & **80** \\ \hline \multicolumn{8}{c}{_Specialized LLMs_} \\ Hutano-GPT2-78 & 90 & 56 & 68 & 76 & 78 & 49 & 70 & 70 \\ hunting-GPT2-34B & 96 & 61 & 74 & 86 & **79** & 58 & 73 & 75 \\ DISC-MeLLM & 68 & 42 & 67 & 80 & 71 & 39 & 64 & 62 \\ \hline \hline \end{tabular}
\end{table}
Table 10: The Human Evaluation on QA Tasks in MedJourney.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline
**Rank** & **B-4** & **Early** & **GPT Judge** & **GPT Judge + Entity** & **Human** \\ \hline
1 & ChsuGPT & GPT-4 & QWE-72B & GPT-4 & QFT-4 & QFT-4 \\
2 & Hunting-GPT2-34B & QWE-32B & QWE-72B & QWE-32B \\
3 & QWE-72B & QWE-72B & QWE-72B & Hunting-GPT2-34B \\
4 & QWE-14B & QWE-72B & QWE-72B & QWE-72B & QWE-72B \\
5 & ChsuGLM3 & QWE-14B & QWE-14B & QWE-14B & QWE-14B & QWE-72B \\
6 & QWE-32B & Hunting-GPT2-34B & Hutano-GPT2-34B & Hutano-GPT2-34B & QWE-14B \\
7 & QWE-7B & ChatGPT & Hunting-GPT2-7B & ChatGPT & ChatGPT \\
8 & GPT-4 & Human-GPT2-72B & ChatGPT & Hunting-GPT2-7B & Hunting-GPT2-7B \\
9 & Hunting-GPT2-7B & ChatGLM3 & ChatGLM3 & ChatGLM3 & ChatGLM3 & ChatGLM3 \\
10 & DISC-MeLLM & DISC-MeLLM & DISC-MeLLM & DISC-MeLLM & DISC-MeLLM \\ \hline \hline \end{tabular}
\end{table}
Table 11: The Ranking of 10 LLMs w.r.t Each Evaluation Metric.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline
**Model** & **PCDS** & **HOA** & **DRG** & **PDDS** & **DOA** & **IOA** & **MOA** & **AVERAGE** \\ \hline \multicolumn{8}{c}{_Public LLMs_} \\ ChsuGLM3 & 80.16 & 78.34 & 81.83 & 82.35 & 82.66 & 82.41 & 87.96 & 82.24 \\ QWE-7B & 87.37 & 83.32 & 85.83 & 86.67 & 87.32 & **87.53** & **90.30** & 86.91 \\ QWE-14B & 89.37 & 85.58 & 81.69 & 87.06 & 87.68 & 86.09 & 89.44 & 86.70 \\ QWE-32B & 88.24 & 85.80 & **86.22** & 87.32 & 88.13 & **87.53** & 90.08 & 87.62 \\ QWE-72B & 89.34 & **86.57** & 85.64 & **87.84** & **85.88** & 86.42 & 89.45 & **87.73** \\ \hline \multicolumn{8}{c}{_Private LLMs_} \\ GhorOPT & 88.18 & 83.10 & 69.95 & 86.05 & 83.43 & 84.06 & 89.89 & 83.52 \\ GPT-4 & **89.44** & 85.86 & 85.86 & 87.51 & 87.80 & 87.33 & 90.19 & 87.71 \\ \hline \multicolumn{8}{c}{_Specialized LLMs_} \\ HutanoGPT2-78B & 81.89 & 79.41 & 82.94 & 81.41 & 87.24 & 84.99 & 88.61 & 83.78 \\ HuangGPT2-34B & 89.11 & 82.44 & 84.58 & 85.75 & 86.34 & 86.28 & 89.38 & 86.27 \\ DISC-MeLLM & 72.45 & 78.89 & 81.83 & 77.69 & 79.37 & 77.01 & 86.31 & 79.08 \\ \hline \hline \end{tabular}
\end{table}
Table 9: The GPT-4 Evaluation on QA Tasks in MedJourney.

## References

* B. Abacha, E. Agichtein, Y. Pinter, and D. Demner-Fushman (2017)Overview of the medical question answering task at TREC 2017 liveQA.. In TREC, Cited by: SS1.
* A. B. Abacha, Y. Mrabet, M. Sharp, T. R. Goodwin, S. E. Shooshan, and D. Demner-Fushman (2019)Bridging the gap between consumers' medication questions and trusted answers.. In MedInfo, pp. 25-29. Cited by: SS1.
* A. A. M. Meta (2024)Llama 3 model card. External Links: https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md Cited by: SS1.
* J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, B. Hui, L. Ji, M. Li, J. Lin, R. Lin, D. Liu, G. Liu, C. Lu, K. Lu, J. Ma, R. Men, X. Ren, X. Ren, C. Tan, S. Tan, J. Tu, P. Wang, S. Wang, W. Wang, S. Wu, B. Xu, J. Xu, A. Yang, H. Yang, J. Yang, S. Yang, Y. Yao, B. Yu, H. Yuan, Z. Yuan, J. Zhang, X. Zhang, Y. Zhang, Z. Zhang, C. Zhou, J. Zhou, and T. Zhu (2023)Qwen technical report. arXiv preprint arXiv:2309.16609. Cited by: SS1.
* Z. Bao, W. Chen, S. Xiao, K. Ren, J. Wu, C. Zhong, J. Peng, X. Huang, and Z. Wei (2023)DISC-MedLLM: bridging general large language models and real-world medical consultation. External Links: 2308.14346 Cited by: SS1.
* Y. Cai, L. Wang, Y. Wang, G. de Melo, Y. Zhang, Y. Wang, and L. He (2024)Medbench: a large-scale chinese benchmark for evaluating medical large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38, pp. 17709-17717. Cited by: SS1.
* J. Chen, X. Wang, A. Gao, F. Jiang, S. Chen, H. Zhang, D. Song, W. Xie, C. Kong, J. Li, et al. (2023)Huatogt-ii, one-stage training for medical adaption of llms. arXiv preprint arXiv:2311.09774. Cited by: SS1.
* W. Chen, Z. Li, H. Fang, Q. Yao, C. Zhong, J. Hao, Q. Zhang, X. Huang, J. Peng, and Z. Wei (2023)A benchmark for automatic medical consultation system: frameworks, tasks and datasets. Bioinformatics39 (1), pp.. External Links: Document Cited by: SS1.
* L. Donaldson, W. Ricciardi, S. Sheridan, and R. Tartaglia (2021)Textbook of patient safety and clinical risk management. External Links: 2001.0221 Cited by: SS1.
* Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang (2022)GLM: general language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 320-335. Cited by: SS1.
* Y. Gao, D. Dligach, T. Miller, J. Caskey, B. Sharma, M. M. Churpek, and M. Afshar (2023)Dr. bench: diagnostic reasoning benchmark for clinical natural language processing. Journal of biomedical informatics138, pp.. External Links: Document Cited by: SS1.
* R. Gualandi, C. Masella, D. Viglione, and D. Tartaglini (2019)Exploring the hospital patient journey: what does the patient experience?. PloS one14 (12), pp. e0224899. Cited by: SS1.
* J. He, M. Fu, and M. Tu (2019)Applying deep matching networks to chinese medical question answering: a study and a dataset. BMC Medical Informatics and Decision Making19 (2), pp. 52. External Links: Document Cited by: SS1.
* D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt (2020)Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300. Cited by: SS1.
* D. Jin, E. Pan, N. Oufatolle, W. Weng, H. Fang, and P. Szolovits (2021)What disease does this patient have? a large-scale open domain question answering dataset from medical exams. Applied Sciences11 (1), pp. 6421. Cited by: SS1.

Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen, and Xinghua Lu. 2019. Pubmedqa: A dataset for biomedical research question answering. _arXiv preprint arXiv:1909.06146_ (2019).
* Liu et al. (2022c) Fenglin Liu, Bang Yang, Chenyu You, Xian Wu, Shen Ge, Zhangdaihong Liu, Xu Sun, Yang Yang, and David Clifton. 2022c. Retrieve, reason, and refine: Generating accurate and faithful patient instructions. _Advances in Neural Information Processing Systems_ 35 (2022), 18864-18877.
* Liu et al. (2024) Junling Liu, Peilin Zhou, Yining Hua, Dading Chong, Zhongyu Tian, Andrew Liu, Helin Wang, Chenyu You, Zhenhua Guo, Lei Zhu, et al. 2024b. Benchmarking Large Language Models on CMExam-A Comprehensive Chinese Medical Exam Dataset. _Advances in Neural Information Processing Systems_ 36 (2024).
* Liu et al. (2024) Mianxin Liu, Jinru Ding, Jie Xu, Weiguo Hu, Xiaoyang Li, Lifeng Zhu, Zhian Bai, Xiaoming Shi, Benyou Wang, Haitao Song, et al. 2024a. Medbench: A comprehensive, standardized, and reliable benchmarking system for evaluating chinese medical large language models. _arXiv preprint arXiv:2407.10990_ (2024).
* Liu et al. (2022a) Wenge Liu, Jianheng Tang, Yi Cheng, Wenjie Li, Yefeng Zheng, and Xiaodan Liang. 2022a. MedDG: an entity-centric medical consultation dataset for entity-aware medical dialogue generation. In _CCF International Conference on Natural Language Processing and Chinese Computing_. Springer, 447-459.
* Liu et al. (2022b) Wenge Liu, Jianheng Tang, Yi Cheng, Wenjie Li, Yefeng Zheng, and Xiaodan Liang. 2022b. MedDG: an entity-centric medical consultation dataset for entity-aware medical dialogue generation. In _CCF International Conference on Natural Language Processing and Chinese Computing_. Springer, 447-459.
* Pal et al. (2022) Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. 2022. Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering. In _Conference on health, inference, and learning_. PMLR, 248-260.
* Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In _Proceedings of the 40th annual meeting of the Association for Computational Linguistics_. 311-318.
* Rasmy et al. (2021) Laila Rasmy, Yang Xiang, Ziqian Xie, Cui Tao, and Degui Zhi. 2021. Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction. _NPJ digital medicine_ 4, 1 (2021), 86.
* Schmidgall et al. (2024) Samuel Schmidgall, Rojin Ziaei, Carl Harris, Eduardo Reis, Jeffrey Jopling, and Michael Moor. 2024. AgentClinic: a multimodal agent benchmark to evaluate AI in simulated clinical environments. _arXiv preprint arXiv:2405.07960_ (2024).
* Sehrawat (2023) Sunil Kumar Sehrawat. 2023. Empowering the Patient Journey: The Role of Generative AI in Healthcare. _International Journal of Sustainable Development Through AI, ML and IoT_ 2, 2 (2023), 1-18.
* Singhal et al. (2023) Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. 2023. Large language models encode clinical knowledge. _Nature_ 620, 7972 (2023), 172-180.
* Sun et al. (2021) Hao Sun, Zhenru Lin, Chujie Zheng, Siyang Liu, and Minlie Huang. 2021. Psyqa: A chinese dataset for generating long counseling text for mental health support. _arXiv preprint arXiv:2106.01702_ (2021).
* Thirunavukarasu et al. (2023) Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. 2023. Large language models in medicine. _Nature medicine_ 29, 8 (2023), 1930-1940.
* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_ (2023).
* Touvron et al. (2020)Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_ (2023).
* Wang et al. (2023c) Junda Wang, Zonghai Yao, Zhichao Yang, Huixue Zhou, Rumeng Li, Xun Wang, Yucheng Xu, and Hong Yu. 2023c. NoteChat: a dataset of synthetic doctor-patient conversations conditioned on clinical notes. _arXiv preprint arXiv:2310.15959_ (2023).
* Wang et al. (2023a) Xidong Wang, Guiming Hardy Chen, Dingjie Song, Zhiyi Zhang, Zhihong Chen, Qingying Xiao, Feng Jiang, Jianquan Li, Xiang Wan, Benyou Wang, et al. 2023a. CMB: A Comprehensive Medical Benchmark in Chinese. _arXiv preprint arXiv:2308.08833_ (2023).
* Wang et al. (2023b) Xiaofei Wang, Hayley M Sanders, Yuchen Liu, Kennarey Seang, Bach Xuan Tran, Atanas G Atanasov, Yue Qiu, Shenglan Tang, Josip Car, Ya Xing Wang, et al. 2023b. ChatGPT: promise and challenges for deployment in low-and middle-income countries. _The Lancet Regional Health-Western Pacific_ 41 (2023).
* Wu et al. (2024) Jiageng Wu, Xiaocong Liu, Minghui Li, Wanxin Li, Zichang Su, Shixu Lin, Lucas Garay, Zhiyun Zhang, Yujie Zhang, Qingcheng Zeng, Jie Shen, Changzheng Yuan, and Jie Yang. 2024. Clinical text datasets for medical artificial intelligence and large language models--a systematic review. _NEJM AI_ (2024), AlTra2400012.
* Wu et al. (2022) Rui Wu, Zhaopeng Qiu, Jiacheng Jiang, Guilin Qi, and Xian Wu. 2022. Conditional generation net for medication recommendation. In _Proceedings of the ACM Web Conference 2022_. 935-945.
* Zhang et al. (2022) Ningyu Zhang, Mosha Chen, Zhen Bi, Xiaozhuan Liang, Lei Li, Xin Shang, Kangping Yin, Chuanqi Tan, Jian Xu, Fei Huang, Luo Si, Yuan Ni, Guotong Xie, Zhifang Sui, Baobao Chang, Hui Zong, Zheng Yuan, Linfeng Li, Jun Yan, Hongying Zan, Kunli Zhang, Buzhou Tang, and Qingcai Chen. 2022. CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association for Computational Linguistics, Dublin, Ireland, 7888-7915. https://doi.org/10.18653/v1/2022.acl-long.544
* Zhang et al. (2017) Sheng Zhang, Xin Zhang, Hui Wang, Jiajun Cheng, Pei Li, and Zhaoyun Ding. 2017. Chinese Medical Question Answer Matching Using End-to-End Character-Level Multi-Scale CNNs. _Applied Sciences_ 7, 8 (2017), 767.
* Zhang et al. (2018) S. Zhang, X. Zhang, H. Wang, L. Guo, and S. Liu. 2018. Multi-Scale Attentive Interaction Networks for Chinese Medical Question Answer Selection. _IEEE Access_ 6 (2018), 74061-74071. https://doi.org/10.1109/ACCESS.2018.2883637
* Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. _arXiv preprint arXiv:2303.18223_ (2023).
* Zhu et al. (2023) Wei Zhu, Xiaoling Wang, Huanran Zheng, Mosha Chen, and Buzhou Tang. 2023. Promptcblue: A chinese prompt tuning benchmark for the medical domain. _arXiv preprint arXiv:2310.14151_ (2023).

Appendix

### Background

#### a.1.1 Stages in the Patient Clinical Journey

In this paper, we segment the clinical process based on the patient's journey, specifically focusing on the stages before visiting the hospital (planning), during the hospital visit (access and delivery), and after the hospital visit (ongoing care). This segmentation aligns with similar divisions proposed in previous works.

In Sehrawat (2023), the authors segment the patient journey into six stages: 1) Initial Contact and Symptom Recognition; 2) Diagnostic Evaluation and Treatment Planning; 3) Treatment and Care Delivery; 4) Follow-up Care and Monitoring; 5) Long-term Management and Disease Prevention; 6) Patient Education and Empowerment. Here, stage 1 aligns with our "planning" stage, stage 2 with "access", stage 3 with "delivery", and stages 4 to 6 with "ongoing care".

In Donaldson et al. (2021), the authors define the patient journey as "encounters with healthcare facilities, a hospital unit, a specialist visit, a primary care clinic, a home health agency". In this definition, "healthcare facilities" corresponds to our planning stage, "hospital unit" and "specialist visit" align with the access and delivery stages, and "primary care clinic" and "home health agency" map to the ongoing care stage.

In Gualandi et al. (2019), the authors describe a patient's journey through surgery, which includes seven stages: 1) Out-patient visit, 2) Examination at out-patient clinics, 3) Hospitalization and surgery, 4) Post-surgical care, 5) Discharge, 6) Rehabilitation Stay, 7) Follow-up visit. Here, stage 1 corresponds to "planning", stage 2 to "access", stages 3 and 4 to "delivery", and stages 5 to 7 to "ongoing care".

During the planning stage, patients experiencing symptoms are guided to the appropriate department through Department Recommendation (DR). The Pre-Consultation Dialogue Summary (PCDS) collects the patient's main complaints in a dialog format and summarizes them for the doctor's review. The Hospital Reception QA (HQA) informs patients about the specifics of their hospital visit, such as what items to bring, dietary restrictions before certain examinations, and so on.

In the access stage, the Doctor Response Generation (DRG) provides potential responses to the doctor based on the patient-doctor conversation. The Patient Doctor Dialogue Summarization (PDDS) further condenses the dialogue for easy reference.

During the delivery stage, Examination Prediction (EP), Disease Prediction (DP), Treatment Prediction (TP), and Medication Prediction (MP) recommend potential actions for the doctor's reference, such as suggesting further examinations, possible diagnoses, appropriate treatments, and medications for the patient.

In the ongoing care stage, Drug QA (DQA) instructs patients on medication usage and related knowledge. Insurance QA (IQA) provides information on medical insurance, and Mental Health QA (MQA) focuses on the patient's mental well-being.

While the proposed 12 tasks do not cover all aspects of a clinical process, such as radiology report generation, they effectively connect the dots of a patient's journey.

#### a.1.2 Categories of Medical Tasks

Typical medical tasks can be categorized into three types: 1) Exam-based, 2) QA-based, 3) Task-based.

For the Exam-based tasks, questions are typically selected from the United States Medical Licensing Examination (USMLE) or the China National Medical Licensing Examination (CNMLE). These questions usually come in the form of a problem, multiple-choice options, and an answer.

For instance:

**Question:** A 77-year-old male presents with progressive right-hand tremors and slow movements. The patient has a history of benign prostatic hyperplasia and mild renal insufficiency. Which medication would be most appropriate for his treatment? Candidate Options: (A) Artane (B) Levodopa (C) Selegiline (D) Amantadine (E) Bromcoripting

**Answer:** (B) Levodopa

For the QA-based tasks, an example can be found below.

**Question:** What are the side effects of Carbamazepine?

**Answer:** Common adverse reactions to Carbamazepine primarily include dizziness, drowsiness, fatigue, nausea, and other gastrointestinal symptoms. It can also cause bone marrow suppression, toxic hepatitis, and, more rarely, exfoliative dermatitis.

For Task-based tasks, one example involves extracting named entities from plain text.

**Prompt:** Please identify the medical name entity in this sentence, "Tetanus Spasm toxin has a long-term effect on the autonomic nerve."

**Target:** Autonomic nerve; Tetanus spasm toxin

### Dataset Description

In this paper, we present 12 data sets to cover the entire patient journey, among which, Department Recommendation (DR), Pre-Consultation Dialogue Summary (PCDS), Hospital Reception QA (HQA), Insurance QA (IQA) and Drug QA (DQA) are newly proposed, while the rest are rebuilt from existing datasets. These five tasks have corresponding applications in Tencent Healthcare Smart Hospital Solutions 8. Based on user behaviours and medical knowledge, profession clinicians generated these 5 data sets either by rewriting existing cases or summarizing common denqueries. Among these 5 data sets, Hospital Reception QA (HQA), Drug QA (DQA), and Insurance QA (IQA) primarily focus on medical knowledge and contain minimal personal information.

Footnote 8: https://healthcare.tencent.com/solution/7

The Department Recommendation (DR) and Pre-Consultation Dialogue (PCD) datasets are derived from users' inquiries prior to doctor visits. Therefore, these datasets focus on patients' primary complaints and do not include in-hospital data like examination, diagnosis, or medication information. In addition, we also employ experienced physicians to recompose the data instead of directly using patients' input, further ensuring the absence of patient information.

#### a.2.1 Department Recommendation

Through the Department Recommendation service, a patient provides a description of their symptoms, and the service suggests the most appropriate department for the patient to visit.

We enlisted the help of three doctors for this annotation process, one of whom acted as the meta-annotator. In the first step, each department was initially assigned to a doctor who composed more than five patient complaints specific to that department. In the second step, another doctor supplemented potential departments based on the patient complaints generated in the first step. Finally, the meta-annotator evaluated each case, retaining the top five cases of the highest quality for each department. Since we target 100 departments, this process culminated in a final dataset comprising 500 entries. Figure 4 illustrates the distribution of the departments.

Figure 4: The distribution of coarse-grained departments.

[MISSING_PAGE_FAIL:16]

[MISSING_PAGE_FAIL:17]

[MISSING_PAGE_FAIL:18]

[MISSING_PAGE_EMPTY:19]

[MISSING_PAGE_FAIL:20]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Guidelines: We have summarized our study in Section Abstract and Introduction. * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Yes, see Section A.3.2 and A.4 Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [N/A]Justification: There is no theoretical result in this paper. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The models evaluated in the experiments are all public, it is quite easy to reproduce the results. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes]

Justification: The data set is uploaded to GitHub. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The models are public and the data sets are uploaded to GitHub. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [N/A] Justification: This is a data set paper, we did not propose a new model and did not need to show the statistical significance. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The models in this paper are all public and only the inference is needed. The computer resources for interencing these models are well known. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: See Section A.3. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: See Section A.3.2 Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [N/A] Justification: There is no high risk for misuse. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have cited the original paper or attached the link to the existing assets used in this paper. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: For the newly proposed data sets, we provide detailed description. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [N/A] Justification: No crowdsourcing in this study Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [N/A] Justification: This paper does not involve crowdsourcing or research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.