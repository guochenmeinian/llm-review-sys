# Interpretable Concept-Based Memory Reasoning

 David Debot

KU Leuven

david.debot@kuleuven.be

&Pietro Barbiero

Universita' della Svizzera Italiana

University of Cambridge

barbiero@tutanota.com

Francesco Giannini

Scuola Normale Superiore

francesco.giannini@sns.it

&Gabriele Ciravegna

DAUIN, Politecnico di Torino

gabriele.ciravegna@polito.it

Michelangelo Diligenti

University of Siena

michelangelo.diligenti@unisi.it

&Giuseppe Marra

KU Leuven

giuseppe.marra@kuleuven.be

###### Abstract

The lack of transparency in the decision-making processes of deep learning systems presents a significant challenge in modern artificial intelligence (AI), as it impairs users' ability to rely on and verify these systems. To address this challenge, Concept-Based Models (CBMs) have made significant progress by incorporating human-interpretable concepts into deep learning architectures. This approach allows predictions to be traced back to specific concept patterns that users can understand and potentially intervene on. However, existing CBMs' task predictors are not fully interpretable, preventing a thorough analysis and any form of formal verification of their decision-making process prior to deployment, thereby raising significant reliability concerns. To bridge this gap, we introduce Concept-based Memory Reasoner (CMR), a novel CBM designed to provide a human-understandable and provably-verifiable task prediction process. Our approach is to model each task prediction as a neural selection mechanism over a memory of learnable logic rules, followed by a symbolic evaluation of the selected rule. The presence of an explicit memory and the symbolic evaluation allow domain experts to inspect and formally verify the validity of certain global properties of interest for the task prediction process. Experimental results demonstrate that CMR achieves better accuracy-interpretability trade-offs to state-of-the-art CBMs, discovers logic rules consistent with ground truths, allows for rule interventions, and allows pre-deployment verification.

## 1 Introduction

The opaque decision process of deep learning (DL) systems represents one of the most fundamental problems in modern artificial intelligence (AI). For this reason, eXplainable AI (XAI) [1, 2, 3] is currently one of the most active research areas in AI. Among XAI techniques, Concept-Based Models (CBMs) [4, 5, 6, 7, 8] represented a significant innovation that made DL models explainable-by-design by introducing a layer of human-interpretable concepts within DL architectures. CBMs consist of at least two functions: a concept encoder, which maps low-level raw features (e.g. an image's pixels) to high-level interpretable concepts (e.g. "red" and "round"), and a task predictor, which uses the learned concepts to solve a downstream task (e.g. "apple"). This way, each task prediction can be traced backto a specific pattern of concepts, thus allowing CBMs to provide explanations in terms of high-level interpretable concepts (e.g. concepts "red" and "round" were both active when the model classified an image as "apple") rather than low-level raw features (e.g. there were 100 red pixels when the model classified an image as "apple"). In other terms, a CBM's task predictor allows understanding _what_ the model sees in a given input rather than simply pointing to _where_ it is looking [9].

However, state-of-the-art CBMs' task predictors are either unable to solve complex tasks (e.g. linear layers), non-differentiable (e.g. decision trees), or black-box neural networks. CBMs employing black-box task predictors are still considered _locally_ interpretable, as concept interventions allow humans to understand how concepts influence predictions for individual input examples. However, they lack _global_ interpretability: the human cannot interpret the model's global behaviour, i.e. on any possible instance. This prevents a proper understanding of the model's working, as well as any chance of formally verifying the task predictor decision-making process prior to deployment, thus raising significant concerns in practical applications. As a result, a knowledge gap persists in the existing literature: the definition of a CBM with a task predictor whose behaviour can be inspected, verified, and potentially intervened upon _before_ the deployment of the system.

To address this gap, we propose Concept-based Memory Reasoner (CMR), a new CBM where the behaviour and explanations can be inspected and verified before the model is deployed. CMR's task predictor offers global interpretability as it utilizes a differentiable memory of learnable logic rules, making all potential decision rules transparent to humans. Additionally, CMR avoids the concept bottleneck that often limits the accuracy of interpretable models when compared to black-box approaches. Our key innovation lies in an attention mechanism that dynamically selects a relevant rule from the memory, which CMR uses to accurately map concepts to downstream classes.

\begin{tabular}{c c}
**CMR** & **= neural selection over a set of human-understandable decision-making processes** \\  & **Accuracy** & **Interpretability** \\ \end{tabular}

We call this paradigm Neural Interpretable Reasoning (NIR), which involves neurally generating (i.c. selecting from memory) an interpretable model (i.c. a logic rule) and symbolically executing it. Once learned, the memory of logic rules can be interpreted as a disjunctive theory, which can be used for explaining and automatic verification. This verification can take place before the model is deployed and, thus, for any possible input the model will face at deployment time. The concept-based nature of CMR allows the automatic verification of properties that are expressed in terms of high-level human-understandable _concepts_ (e.g. "never predict class 'apple' when the concept 'blue' is active") rather than raw features (e.g. "never predict class 'apple' when there are less than ten red pixels").

Our experimental results show that CMR: (i) improves over the accuracy-interpretability performances of state-of-the-art CBMs, (ii) discovers logic rules matching ground truths, (iii) enables rule interventions beyond concept interventions, and (iv) allows verifying properties for their predictions and explanations _before_ deployment. Our code is available at https://github.com/daviddebot/CMR.

## 2 Preliminary

Concept Bottleneck Models (CBNMs)[4; 10] are functions composed of (i) a concept encoder \(g:X\to C\) mapping each entity \(x\in X\subseteq\mathbb{R}^{d}\) (e.g. an image) to a set of \(n_{C}\) concepts \(c\in C\) (e.g. "red", "round"), and (ii) a task predictor \(f:C\to Y\) mapping concepts to the class \(y\in Y\) (e.g. "apple") representing a downstream task. For simplicity, in this paper, a single task class is discussed, as multiple tasks can be encoded by instantiating multiple task predictors. When sigmoid activations are used for concepts and task predictions, we can consider \(g\) and \(f\) as parameterizing a Bernoulli distribution of truth assignments to propositional boolean concepts and tasks. For example, \(g_{red}(x)=0.8\) means that there is an 80% probability for the proposition "\(x\) is red" to be true. During training, concept and class predictions \((c,y)\) are aligned with ground-truth labels \((\hat{c},\hat{y})\). This architecture and training allows CBNMs to provide explanations for class predictions indicating the presence or absence of concepts. Another main advantage of these models is that, at test time, human experts may also _intervene_ on mispredicted concept labels to improve CBNMs' task performance and extract counterfactual explanations [4; 11]. However, the task prediction \(f\) is still often a black-box model to guarantee high performances, thus not providing any insight into which concepts are used and how they are composed to reach the final prediction.

Model

In this section, we introduce Concept-based Memory Reasoner (CMR), the first concept-based model that is _globally interpretable_, _provably verifiable_ and a _universal binary classifier_. CMR consists of three main components: a concept encoder, a rule selector and a task predictor. CMR's task prediction process differs significantly from traditional CBMs. It operates transparently by (1) selecting a logic rule from a set of jointly-learned rules, and (2) symbolically evaluating the chosen rule on the concept predictions. This unique approach enables CMR not only to provide explanations by tracing class predictions back to concept activations, but also to explain which concepts are utilized and how they interact to make a task prediction. Moreover, the set of learned rules remains accessible throughout the learning process, allowing users to analyse the model's behaviour and automatically verify whether some desired properties are being fulfilled at any time. The logical interpretation of CMR's task predictor, combined with its provably verifiable behaviour, distinguishes it sharply from existing CBMs' task predictors.

### Probabilistic graphical model

In Figure 1, we show the probabilistic graphical model of CMR. There are four variables, three of which are standard in (discriminative) CBMs: the observed input \(x\in X\), the concepts encoding \(c\in C\) and the task prediction \(y\in\{0,1\}\). CMR adds an additional variable: the rule \(r\in\{P,N,I\}^{n_{C}}\). A rule is a conjunction in the concept set, like \(c_{1}\wedge\neg c_{3}\). A conjunction is uniquely identified when, for each concept \(c_{i}\), we know whether, in the rule, the concept is _irrelevant (I)_, _positive (P)_ or _negative (N)_. We call \(r_{i}\in\{P,N,I\}\) the role of the \(i\)-th concept in rule \(r\). For example, given the three concepts \(c_{1},c_{2},c_{3}\), the conjunction \(c_{1}\wedge\neg c_{3}\) can be represented as \(r_{1}=P,r_{2}=I,r_{3}=N\), since the role of \(c_{1}\) is positive (P), the role of \(c_{2}\) is irrelevant (I) and the role of \(c_{3}\) is negative (N).

This probabilistic graphical model encodes the joint conditional distribution \(p(y,r,c|x)\) and factorizes as

\[p(y,r,c|x)=p(y|c,r)p(r|x)p(c|x)\] (1)

and consists of the following components:

* \(p(c|x)\) is the **concept encoder**. For concept bottleneck encoders1, it is simply the product of \(n_{C}\) independent Bernoulli distributions \(p(c_{i}|x)\), whose logits are parameterized by some neural network encoder \(g_{i}:X\rightarrow\mathbb{R}\). Footnote 1: In case of encoders that provide additional embeddings other than concept logits, they are modelled as Dirac delta distributions, thus not modelling any uncertainty over them.
* \(p(r|x)\) is the **rule selector**, described in Section 3.1.1. Given an input \(x\), \(p(r|x)\) models the uncertainty over which conjunctive rule must be used.
* \(p(y|c,r)\) is the **task predictor**, which will be described in Section 3.1.2. Given a rule \(r\sim p(r|x)\) and an assignment of truth values \(c\sim p(c|x)\) to the concepts, the task predictor evaluates the rule on the concepts. In all the cases described in this paper, \(p(y|c,r)\) is a degenerate deterministic distribution.

#### 3.1.1 Rule selector

We model the rule selector \(p(r|x)\) as a mixture of \(n_{R}\in\mathbb{N}\) rule distributions. The selector "selects" a rule from a set of rule distributions (i.e. the components of the mixture), and we call this set the _rulebook_. The rulebook is jointly learned with the rest of CMR and can be inspected and verified at every stage of the learning. Architecturally, the selection is akin to an attention mechanism over a differentiable memory [12].

To this end, let \(s\in[1,n_{R}]\subset\mathbb{N}\) be the indicator of the selected component of the mixture, then:

\[p(r|x)=\sum_{s}p(r|s)p(s|x)\] (2)

Figure 1: Probabilistic graphical model of CMR

Here, \(p(s|x)\) is the categorical distribution defining the mixing weights of the mixture. It is parameterized by a neural network \(\phi^{(s)}:X\rightarrow\mathbb{R}^{n_{R}}\), outputting one logit for each of the \(n_{R}\) components. Each \(p(r|s)\) is a distribution over all possible rules and is modelled as a product of \(n_{C}\) categorical distributions, i.e. \(p(r|s)=\prod_{i=1}^{n_{C}}p(r_{i}|s)\). We assign to each component \(s=j\) a _rule embedding_\(e_{j}\in\mathbb{R}^{q}\), and each categorical \(r_{i}\) is then parameterized by a neural network \(\phi^{(r)}_{i}:\mathbb{R}^{q}\rightarrow\mathbb{R}^{3}\), mapping the rule embedding to the logits of the categorical component. Intuitively, for each concept \(c_{i}\), the corresponding categorical distribution \(p(r_{i}|s=j)\)_decodes_ the embedding \(e_{j}\) to the three possible roles \(r_{i}\in\{P,N,I\}\) of concept \(c_{i}\) in rule \(r\). This way, each embedding in the rulebook is the latent representation of a logic rule.2 Lastly, we define the set \(E\) of all rule embeddings, i.e. \(E=\{e_{j}\}_{j\in[1,n_{R}]}\), as the encoded rulebook.

Footnote 2: Actually, each embedding is the latent representation of a _distribution_ over all possible rules, factorized per concept. However, this distribution is typically quite crisp after training (see also Appendix B). After training, we convert each distribution into the most likely rule by taking the argmax among the roles of the concepts (see Section 5). This way, each embedding in the encoded rulebook corresponds with a rule, and the decoded rulebook is a set of rules.

#### 3.1.2 Task predictor

The CMR task predictor \(p(y|r,c)\) provides the final \(y\) prediction given concept predictions \(c\) and the selected rule \(r\). We model the task predictor as a degenerate deterministic distribution, providing the entire probability mass to the unique value \(y\) which corresponds to the logical evaluation of the rule \(r\) on the concept predictions \(c\). In particular, let \(r_{i}\in\{P,N,I\}\) be the role of the \(i\)-th concept in rule \(r\). Then, the symbolic task prediction \(y\) obtained by evaluating rule \(r\) on concept predictions \(c\) is:

\[y\leftarrow\bigwedge_{i=1}^{n_{C}}(r_{i}=I)\vee(((r_{i}=P)\Rightarrow c_{i}) \wedge((r_{i}=N)\Rightarrow\neg c_{i}))\] (3)

Here, the \(y\) prediction is equivalent to a conjunction of \(n_{C}\) different conjuncts, one for each concept. If a concept \(i\) is irrelevant according to the selected rule \(r\) (i.e. \(r_{i}=I\)), the corresponding conjunct is ignored. If \(r_{i}=P\), then the conjunct is True if the corresponding concept is True. Otherwise, i.e. if \(r_{i}=N\), the conjunct is True if the corresponding concept is False.

A graphical representation of the model is shown in Figure 2.

Figure 2: Example prediction of CMR with a rulebook of two rules and three concepts (i.e. \(red\left(R\right)\), \(square\left(S\right)\), \(table\left(T\right)\)). In this figure, we sample (\(\sim\)) for clarity, but in practice, we compute every probability exactly. Every black box is implemented by a neural network, while the white box is a pure symbolic logic evaluation. **(A)** The image is mapped to a concept prediction. **(B)** The image is mapped by the component selector to a distribution over rules. **(C)** This distribution is used to select a rule embedding from the encoded rulebook. **(D)** The rule embedding is decoded into a logic rule by assigning to each of the concepts its role in the rule, i.e. whether it is positive (P), negative (N), or irrelevant (I). Finally, **(E)** the rule is evaluated on the concept prediction to provide the task prediction on the task \(apple\).

## 4 Expressivity, interpretability and verification

In this section, we will discuss the proposed model along three different directions: _expressivity_, _interpretability_ and _verification_.

### Expressivity

An interesting property is that CMR is as expressive as a neural network binary classifier.

**Theorem 4.1**.: _CMR is a universal binary classifier [13] if \(n_{R}\geq 3\)._

Proof.: Recall that the rule selector is implemented by some neural network \(\phi^{(s)}:X\rightarrow\mathbb{R}^{n_{R}}\). Consider the following three rules, easily expressible in CMR as showed on the right of each rule:

\[y\leftarrow\mathit{True}\ \ (\mathrm{i.e.}\,\forall i:r_{i}=I),\quad y \leftarrow\bigwedge_{i=1}^{n_{C}}c_{i}\ \ (\mathrm{i.e.}\,\forall i:r_{i}=P),\quad y \leftarrow\bigwedge_{i=1}^{n_{C}}\neg c_{i}\ \ (\mathrm{i.e.}\,\forall i:r_{i}=N)\]

By selecting one of these three rules, the rule selector can always make a desired \(y\) prediction, regardless of the concepts \(c\). In particular, to predict \(y=1\), the selector can select the first rule. To predict \(y=0\) when at least one concept has probability less than 50% in the concept predictions \(c\) (i.e. \(\exists i:p(c_{i}|x)<0.50\)), it can select the second rule. Lastly, to predict \(y=0\) when all concepts have probability of at least 50% in \(c\) (i.e. \(\forall i:p(c_{i}|x)\geq 0.50\)), it can select the last rule. 

Consequently, CMR can in theory always achieve the same accuracy as a neural network _without concept bottleneck_, irrespective of which concepts are employed in the model. This distinguishes CMR sharply from CBNMs.

### Interpretability

CMR's task prediction is the composition of a (neural) rule selector and the symbolic evaluation of the selected rule. Therefore, we can always inspect the whole rulebook to know exactly the global behaviour of the task prediction. In particular, let \(s\) be the selected rule, \(e_{j}\) the embedding of the \(j\)-th rule, and \(r_{i}^{(j)}\in\{P,N,I\}\) the role of the \(i\)-th concept in the \(j\)-th rule at decision time, i.e. \(r_{i}^{(j)}=\text{argmax}(\phi_{i}^{(r)}(e_{j}))\). Then, CMR's task prediction can be logically defined as the global rule obtained as the disjunction of all decoded rules, each filtered by whether the rule has been selected or not. That is:

\[y\Leftrightarrow\bigvee_{j=1}^{n_{R}}(s=j)\wedge\left(\bigwedge_{i=1}^{n_{C}}( r_{i}^{(j)}=I)\vee(((r_{i}^{(j)}=P)\Rightarrow c_{i})\wedge((r_{i}^{(j)}=N) \Rightarrow\neg c_{i}))\right)\] (4)

It is clear that if the model learns the three rules in the proof of Theorem 4.1, the selector simply becomes a _proxy_ for \(y\). It is safe to say that the interpretability of the selector (and consequently of CMR) fully depends on the interpretability of the learned rules.

**Prototype-based rules.** In order to develop interpretable rules, we focus on standard theories in cognitive science [14] by looking at those rules that are _prototypical_ of the concept activations on which they are applied to. Prototype-based models are often considered one of the main categories of interpretable models [9] and have been investigated in the context of concept-based models [15]. However, CMR distinguishes itself from prototype-based models in two significant ways. First, in prototype-based CBMs, prototypes are built directly in the input space, such as images [16] or on part of it [17], and are often used to automatically build concepts. However, this approach carries similar issues w.r.t. traditional input-based explanations, like saliency maps (i.e. a prototype made by a strange pattern of pixels can still remain unclear to the user). In contrast, in CMR, prototypes (i.e. rules) are built on top of human-understandable concepts and are therefore interpretable as well. Second, differently from prototype-based models, CMR assigns a logical interpretation to prototypes as conjunctive rules. Unlike prototype networks that assign class labels only based on proximity to a prototype, CMR determines class labels through the symbolic evaluation of prototype rules on concepts. Therefore, prototypes should be representative of concept activations _but also_ provide a correct task prediction. This dual role of CMR prototypes adds a constraint: only prototypes of positive instances (i.e. \(y=1\)) can serve as effective classification rules. Thus, when rules are selected for negative instances (i.e. \(y=0\)), they do not need to be representative of the concept predictions.

**Interventions.** In contrast to existing CBMs, which only allow prediction-time concept interventions, the global interpretability of CMR allows humans to intervene also on the rules that will be exploited for task prediction. These interactions may occur during the training phase to directly shape the model that is being learned, and may come in various forms. A first approach involves the manual inclusion of rules into the rulebook, thereby integrating expert knowledge into the model [18]. The selector mechanism within the model learns to choose among the rules acquired through training and those manually incorporated. As a result, the rules that are being acquired through training can change after adding expert rules. A second approach consists of modifying the learned rules themselves, by altering the role \(r_{i}\) of a concept within a rule. For instance, setting the logit of \(P_{i}\) to 0 ensures that \(c_{i}\) cannot exhibit a positive role in that rule, and setting the logits of both \(P_{i}\) and \(N_{i}\) to 0 ensures irrelevance. This type of intervention could be exploited to remove (or prevent) biases and ensure counterfactual fairness [19].

### Verification

One of the main properties of CMR is that, at decision time, it explicitly represents the task prediction as a set of conjunctive rules. Logically, the mixture semantics of the selector can be interpreted as a disjunction, leading to a standard semantics in stochastic logic programming [20; 21]. As the only neural component is in the selection and the concept predictions, task predictions generated using CMR's global formula (cf. Section 4.2) can be automatically verified by using standard tools of formal verification (e.g. model checking), no matter which rule _will_ be selected. Being able to verify properties prior to deployment of the model strongly sets CMR apart from existing models, where verification tasks can only be applied at prediction time. In particular, given any propositional logical formula \(\alpha\) over the propositional language \(\{c_{1},c_{2},...,c_{n_{C}},y\}\), \(\alpha\) can be automatically verified to logically follow from Equation 4. This formula can be converted into a propositional one by (1) evaluating the role expressions (i.e. \(r_{i}^{j}=\cdot\) becomes _True_ or _False_), (2) replacing the selection expressions with a new propositional variable per rule (i.e. \((s=j)\) becomes \(s_{j}\)), and (3) adding mutual exclusivity constraints for the different \(s_{j}\). For example, for \(n_{C}=n_{R}=2,r_{1}^{1}=P,r_{2}^{1}=I,r_{1}^{2}=P,r_{2}^{2}=N\), we get:

\[(s_{1}\oplus s_{2})\wedge(y\Leftrightarrow(s_{1}\wedge c_{1})\vee(s_{2}\wedge c _{1}\wedge\neg c_{2}))\] (5)

with \(\oplus\) the xor connective. In logical terms, if the formula \(\alpha\) is entailed by such a formula, it means that \(\alpha\) must be true every time Equation 4 is used for prediction.

## 5 Learning and inference

### Learning problem

Learning in CMR follows the standard objective in CBM literature, where the likelihood of the concepts and task observations in the data is maximized. Formally, let \(\Omega\) be the set of parameters of the probability distributions, such that CMR's probabilistic graphical model is globally parameterized by \(\Omega\), i.e. \(p(y,r,c|x;\Omega)\). Let \(D=\{(\hat{x},\hat{c},\hat{y})\}\) be a concept-based dataset of i.i.d. triples (input, concepts, task). Then, learning is the optimization problem:

\[\max_{\Omega}\sum_{(\hat{x},\hat{c},\hat{y})\in D}\log p(\hat{y},\hat{c}|\hat {x};\Omega)\] (6)

Due to the factorization of the mixture model in the rule selection, CMR has a tractable likelihood computation. In particular, the following result holds.

**Theorem 5.1** (Log-likelihood).: _The maximum likelihood simplifies to the following \(O(n_{C}\cdot n_{R})\) objective:_

\[\max_{\Omega}\sum_{(\hat{x},\hat{c},\hat{y})\in D}\left(\sum_{i=1}^{n_{C}}\log p (c_{i}=\hat{c}_{i}|\hat{x})\right)+\left(\log\sum_{\hat{s}=1}^{n_{R}}p(s=\hat {s}|\hat{x})\,p(y=\hat{y}|\hat{c},\hat{s})\right)\] (7)

_with:_

\[p(y=1|c,s)=\prod_{i=1}^{n_{C}}\left(p(I_{i}|s)+p(P_{i}|s)\,\mathbbm{1}[c_{i}= 1]+p(N_{i}|s)\,\mathbbm{1}[c_{i}=0]\right)\]

_where \(\mathbbm{1}[\cdot]\) is an indicator function of the condition within brackets._Proof.: See Appendix A 

The maximum likelihood approach only focuses on the prediction accuracy of the model. However, as discussed in Section 4.2, we look for the set of learned rules \(r\) to represent prototypes of concept predictions, as in prototype-based learning [15]. To drive the learning of representative positive prototypes when we observe a positive value for the task, i.e. when \(y=1\), we add a regularization term to the objective. Intuitively, every time a rule is selected for a given input instance \(x\) with task label \(y=1\), we want the rule to be as close as possible to the observed concept prediction. At the same time, since the number of rules is limited and the possible concept activations are combinatorial, the same rule is expected to be selected for different concept activations. When this happens, we will favour rules that assign an irrelevant role to the inconsistent concepts in the activations. The regularized objective is:

\[\max_{\Omega}\sum_{(\hat{x},\hat{x},\hat{y})\in D}\left(\sum_{i=1}^{n_{C}}\log p (c_{i}=\hat{c}_{i}|\hat{x})\right)+\left(\log\sum_{\hat{s}=1}^{n_{R}}p(s=\hat{ s}|\hat{x})\,p(y=\hat{y}|\hat{c},\hat{s})\underbrace{p_{reg}(r=\hat{c}|\hat{s})^{ \hat{y}}}_{\text{Regularization Term}}\right)\] (8)

and:

\[p_{reg}(r=\hat{c}|s)=\prod_{i=1}^{n_{C}}(0.5\,p(r_{i}=I|s)+p(r_{i}=P|s)\, \mathbbm{1}[\hat{c}_{i}=1]+p(r_{i}=N|s)\,\mathbbm{1}[\hat{c}_{i}=0])\]

This term favours the selected rule \(r\) to reconstruct the observed \(\hat{c}\) as much as possible. When such reconstruction is not feasible due to the limited capacity of the rulebook, the term will favour irrelevant roles for concepts. In this way, we will develop rules that have relevant terms (i.e. \(r_{i}\in\{P,N\}\)) only if they are representative of all the instances in which the rule is selected. Appendix B contains an investigation of the influence of this regularization on the optima of the loss.

### Inference

After training, we replace each role distribution \(p(r_{i}|s=j)\) for each concept \(i\) and rule embedding \(j\) with the most likely role. This ensures that each embedding corresponds to a single logic rule rather than a distribution over all possible rules. Moreover, at decision-time, the concepts are unobserved, leading to the following likelihood computation:

\[p(y=1|x)=\sum_{\hat{s}=1}^{n_{R}}p(s=\hat{s}|x)\prod_{i=1}^{n_{C}}(\mathbbm{1 }[r_{i}=I]+\mathbbm{1}[r_{i}=P]\,p(c_{i}|x)+\mathbbm{1}[r_{i}=N]\,p(\neg c_{i }|x))\] (9)

with \(r_{i}=\text{argmax}_{\hat{r}_{i}\in\{P,N,I\}}p(r_{i}=\hat{r}_{i}|s=\hat{s})\).

## 6 Experiments

Our experiments aim to answer the following research questions:

1. **Generalization:** Does CMR attain similar task and concept accuracy as existing CBMs and black boxes? Does CMR generalize well when the concept set is incomplete3? Footnote 3: Incomplete concept sets do not contain all the information present in the input that is useful for task prediction. Models with a concept bottleneck cannot achieve black-box accuracy with them.
2. **Explainability and Intervenability:** Can CMR recover ground truth rules? Can CMR learn meaningful rules when the concept set is incomplete? Are concept interventions and rule interventions effective in CMR?
3. **Verifiability:** Can CMR allow for post-training verification regarding its behaviour?

### Experimental setting

This section describes essential information about experiments. We provide further details in Appendix C.

**Data & task setup.** We base our experiments on four different datasets commonly used to evaluate CBMs: MNIST+ [22], where the task is to predict the sum of two digits; C-MNIST, where we adapted MNIST to the task of predicting whether a coloured digit is even or odd; MNIST+*, where we removed the concepts for the digits 0 and 1 from the concept set; CelebA [23], a large-scale face attributes dataset with more than 200K celebrity images, each with 40 concept annotations4; CUB [24], where the task is to predict bird species based on bird characteristics; and CEBaB [25], a text-based task where reviews are classified as positive or negative based on different criteria (e.g. food, ambience, service, etc). These datasets range across different concept set quality, i.e. complete (MNIST+, C-MNIST, CUB) vs incomplete (CelebA, MNIST+*), and different complexities of the concept prediction task, i.e. easy (MNIST+, MNIST+*, C-MNIST), medium (CEBaB) and hard (CelebA, CUB). All the datasets come with full concept annotations.

Footnote 4: We remove the concepts \(Wavy\_Hair\), \(Black\_Hair\) and \(Male\) from the concept set and instead use them as tasks.

**Evaluation.** To measure classification performance on tasks and concepts, we compute subset accuracy and regular accuracy, respectively. For CUB, we instead compute the Area Under the Receiver Operating Characteristic Curve [26] for the tasks due to the large class imbalance. All metrics are reported using the mean and the standard error of the mean over three different runs with different initializations.

**Baselines.** In our experiments, we compare CMR with existing CBM architectures. We consider Concept Bottleneck Models with different task predictors: linear, multi-layer (MLP), decision-tree (DT) and XGBoost (XG). Moreover, we add two state-of-the-art CBMs: Concept Embedding Models (CEM) [27] and Deep Concept Reasoner (DCR) [11]. We employ hard concepts in CMR and our competitors, avoiding the problem of input distribution leakage that can affect task accuracy [28; 29] (see Appendix C for additional details). Finally, we include a deep neural network without concepts to measure the effect of an interpretable architecture on generalization performance.

We provide an additional experiment serving as an ablation study on CMR's joint rule learning in Appendix C.3.3.

### Key findings & results

#### 6.2.1 Generalization

**CMR's high degree of interpretability does not harm accuracy, which is similar to or better than competitors'**. In Table 1, we compare CMR with its competitors regarding task accuracy. On all data sets, CMR achieves an accuracy close to black-box accuracy, either beating its concept-based competitors or obtaining similar results. In Table 5 of Appendix C, we show that CMR's training does not harm concept accuracy, which is similar to its competitors. Moreover, we provide an experiment showing that CMR's accuracy is robust to the chosen number of rules in Appendix C.3.4.

**CMR obtains accuracy competitive with black boxes even on incomplete concept sets.** We evaluate the performance of CMR on settings with increasingly more incomplete concept sets. Firstly, as shown in Table 1, in MNIST+*, CMR still obtains task accuracy close to the complete setting, beating its competitors which suffer from a concept bottleneck. Secondly, we run an experiment on

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & mnist+ & mnist+* & c-mnist & celeba & cub & cebab \\ \hline CBM+linear & \(0.00\pm 0.00\) & \(0.00\pm 0.00\) & \(99.07\pm 0.31\) & \(49.02\pm 0.20\) & \(50.60\pm 0.69\) & \(45.15\pm 29.93\) \\ CBM+MLP & \(\mathbf{97.41\pm 0.55}\) & \(72.51\pm 2.42\) & \(\mathbf{99.42\pm 0.11}\) & \(50.29\pm 0.60\) & \(55.83\pm 0.33\) & \(83.70\pm 0.30\) \\ CBM+DT & \(96.73\pm 0.39\) & \(77.63\pm 0.44\) & \(\mathbf{99.44\pm 0.03}\) & \(49.60\pm 0.20\) & \(51.83\pm 0.48\) & \(83.15\pm 0.15\) \\ CBM+XG & \(96.73\pm 0.39\) & \(76.54\pm 0.54\) & \(\mathbf{99.44\pm 0.03}\) & \(50.39\pm 0.24\) & \(\mathbf{62.97\pm 0.96}\) & \(\mathbf{83.80\pm 0.01}\) \\ CEM & \(92.44\pm 0.26\) & \(\mathbf{92.94\pm 1.15}\) & \(99.32\pm 0.11\) & \(\mathbf{65.44\pm 0.25}\) & \(57.03\pm 0.80\) & \(83.30\pm 1.59\) \\ DCR & \(90.70\pm 1.21\) & \(92.24\pm 1.37\) & \(98.99\pm 0.08\) & \(35.65\pm 1.53\) & \(50.00\pm 0.00\) & \(67.30\pm 0.93\) \\ \hline Black box & \(83.26\pm 8.71\) & \(83.26\pm 8.71\) & \(99.19\pm 0.11\) & \(65.33\pm 0.60\) & \(64.07\pm 0.33\) & \(88.67\pm 0.19\) \\ \hline
**CMR (ours)** & \(\mathbf{97.25\pm 0.24}\) & \(\mathbf{94.65\pm 1.99}\) & \(99.12\pm 0.04\) & \(\mathbf{63.17\pm 1.13}\) & \(\mathbf{60.07\pm 1.70}\) & \(\mathbf{85.14\pm 0.43}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Task accuracy on all datasets. The best and second best for CBMs are shown in bold (black and purple, respectively).

CelebA where we gradually decrease the number of concepts in the concept set. Figure 3 shows the achieved task accuracies for CMR and the competitors. CMR's accuracy remains high no matter the size of the concept set, while the performance of the competitors with a bottleneck (i.e. all except CEM) strongly degrades.

#### 6.2.2 Explanations and intervention

**CMR discovers ground truth rules.** We quantitatively evaluate the correctness of the rules CMR learns on MNIST+ and C-MNIST. In the former, the ground truth rules have no irrelevant concepts; in the latter, they do. In all runs of these experiments, CMR finds all correct ground truth rules. In C-MNIST, CMR correctly learns that the concepts related to colour are irrelevant for classifying the digit as even or odd (see Table 2).

**CMR discovers meaningful rules in the absence of ground truth.** While the other datasets do not provide ground truth rules, a qualitative inspection shows that they are still meaningful. Table 2 shows two examples for CEBaB, and additional rules can be found in Appendix C.

**Rule interventions during training allow human experts to improve the learned rules.** We show this by choosing a rulebook size for MNIST+ that is too small to learn all ground truth rules. Consequently, CMR learns rules that differ from the ground truth rules. After manually adding rules to the pool in the middle of training, CMR (1) learns to select these new rules for training examples for which they make a correct task prediction, and (2) improves its previously learned rules by eventually converging to the ground truth rules. This is the case for all runs. Table 3 gives some examples of how manually adding rules affects the learned rules. In Appendix C.3.3, we provide an additional experiment with rule interventions, where we add rules extracted from a rule learner. Additionally, as concept interventions are considered a core advantage of CBMs, we show in Appendix C that CMR is equally responsive as competitors, consistently improving its accuracy after concept interventions.

### Verification

**CMR allows verification of desired global properties.** In this task, we automatically verify semantic consistency properties for MNIST+ and CelebA whether CMR's task prediction satisfies some properties of interest. For verification, we exploited a naive model checker that verifies whether the property holds for all concept assignments where the theory holds. When this is not feasible, state-of-the-art model formal verification tools can be exploited, as both the task prediction and the property are simply two propositional formulas. For MNIST+, we can verify that, for each task \(y\), CMR never uses more than one positive concept (i.e. digit) per image. This can be done by

\begin{table}
\begin{tabular}{l l l} \hline \hline \multirow{3}{*}{C-MNIST} & \(y_{even}\gets 0\wedge(red)\wedge(green)\) \\  & \(y_{even}\gets 2\wedge(red)\wedge(green)\) \\  & \(y_{odd}\gets 3\wedge(red)\wedge(green)\) \\ \hline \multirow{3}{*}{CEBaB} & \(y_{neg}\leftarrow\neg food_{g}\wedge\neg amb_{g}\wedge\neg noise_{g}\) \\  & \(y_{pos}\leftarrow\neg food_{b}\wedge\neg amb_{b}\wedge noise_{u}\wedge\neg noise _{g}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Selection of learned rules. For brevity in C-MNIST, negated concepts are not shown and irrelevant concepts are shown between parentheses. We abbreviate _good_ to \(g\), _bad_ to \(b\) and _unknown_ to \(u\).

\begin{table}
\begin{tabular}{l l l l} \hline \hline \multicolumn{2}{c}{Learned rule before intervention} & \multicolumn{1}{c}{Added rule} & \multicolumn{1}{c}{Learned rule after intervention} \\ \hline \(y_{8}\leftarrow(c_{0,3})\wedge(c_{1,5})\wedge(c_{0,4})\wedge(c_{1,4})\) & \(y_{8}\gets c_{0,3}\wedge c_{1,5}\) & \(y_{8}\gets c_{0,4}\wedge c_{1,4}\) \\ \(y_{9}\leftarrow(c_{0,8})\wedge(c_{1,1})\wedge(c_{0,1})\wedge(c_{1,8})\) & \(y_{9}\gets c_{0,8}\wedge c_{1,1}\) & \(y_{9}\gets c_{0,1}\wedge c_{1,8}\) \\ \(y_{9}\leftarrow(c_{0,0})\wedge(c_{1,9})\wedge(c_{0,2})\wedge(c_{1,7})\) & \(y_{9}\gets c_{0,0}\wedge c_{1,9}\) & \(y_{9}\gets c_{0,2}\wedge c_{1,7}\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Selection of rule interventions and their effect on learned rules. For brevity, negated concepts are not shown, and irrelevant concepts are shown between parentheses.

Figure 3: Task accuracy on CelebA with varying numbers of employed concepts.

verifying one formula per concept \(j\) of digit \(k\): \(\forall y,i\neq j:y\wedge c_{k,j}\Rightarrow\neg c_{k,i}\). This is also easily verifiable by simply inspecting the rules in Appendix C. Moreover, in CelebA, we can easily verify that \(\mathit{Bald}\Rightarrow\neg\neg W\mathit{avy\_Hair}\) with the learned rulebook for \(n_{C}=12\) (see Table 10 in Appendix C), as \(\neg\mathit{Bald}\) is a conjunct in each rule that does not trivially evaluate to False.

## 7 Related works

In recent years, XAI techniques have been criticized for their vulnerability to data modifications [30; 31], insensitivity to reparametrizations, [32], and lacking meaningful interpretations for non-expert users [33]. To address these issues, Concept-based methods [34; 35; 5; 10] have emerged, offering explanations in terms of human-interpretable features, a.k.a. concepts. Concept Bottleneck Models [4] go a step further by directly integrating these concepts as explicit intermediate network representations. Concept Embeddings Models (CEMs) [7; 8; 11] close the accuracy gap with black-box models through vectorial concept representations. However, they still harm the interpretability, as it is unclear what information is contained in the embeddings. In contrast, CMR closes the accuracy gap by using a neural rule selector coupled with learned symbolic logic rules. As a result, CMR's task prediction is transparent, allowing experts to see _how_ concepts are being used for task prediction, and allowing intervention and automatic verification of desired properties. To the best of our knowledge, there is only one other attempt at analysing CBMs' task prediction in terms of logical formulae, namely DCR [11]. For a given example, DCR _predicts_ and subsequently evaluates a (fuzzy) logic rule. As rules are predicted on a per-example basis, the global behaviour of DCR cannot be inspected, rendering interaction (e.g. adding expert rules) and verification impossible. In contrast, CMR _learns_ (probabilistic) logic rules in a memory, allowing for inspection, interaction and verification.

The use of logic rules by CMR for interpretability purposes aligns it closely with the field of neurosymbolic AI [36; 37]. Here, logic rules [38; 39; 18] or logic programs [40; 22; 21] are used in combination with neural networks through the use of neural predicates [22]. Concepts in CMR are akin to a propositional version of neural predicates. However, in CMR, the set of rules is learned instead of given by the human and direct concept supervision is used for human alignment. While neurosymbolic rule learning methods have been developed, many are constrained by specific assumptions about the nature of the task, limiting their usability to particular datasets or environments (e.g. requiring multitask scenarios [41] or specific datasets like MNIST [42]). Additionally, some approaches, unlike ours, explore the rule space in a discrete manner [43], which is computationally expensive. Furthermore, they do not provide expressivity results, while we show that CMR is a universal binary classifier.

Finally, the relationships with prototype-based models have already been discussed in Section 4.2.

## 8 Conclusion

We propose CMR, a novel Concept-Based Model that offers a human-understandable and provably-verifiable task prediction process. CMR integrates a neural selection mechanism over a memory of learnable logic rules, followed by a symbolic evaluation of the selected rule. This approach enables global interpretability and verification of task prediction properties. Our results show that (1) CMR achieves near-black-box accuracy, (2) discovers meaningful rules, and (3) facilitates strong interaction with human experts through rule interventions. The development of CMR can have significant societal impact by enhancing transparency, verifiability, and human-AI interaction, thereby fostering trust and reliability in critical decision-making processes.

**Limitations and future works.** CMRs are still fundamental models and several limitations need to be explored further in future works. In particular, CMRs focus on positive-only explanations, while negative-reasoning explanations have not been explored yet. Moreover, the same selection mechanism can be tested in non-logic, globally interpretable settings (like linear models). Finally, the verification capabilities of CMR will be tested on more realistic, safety critical domains, where the model can be verified against safety specifications.

## Acknowledgments and Disclosure of Funding

DD is a fellow of the Research Foundation-Flanders (FWO-Vlaanderen, 1185125N). This research has also received funding from the KU Leuven Research Fund (STG/22/021, CELSA/24/008) and from the Flemish Government under the "Onderzoekpsychogramma Artificiale Intelligence (AI) Vlaanderen" programme. FG has been supported by the Partnership Extended PE00000013 - "FAIR - Future Artificial Intelligence Research" - Spoke 1 "Human-centered AI". PB acknowledges support from SNSF project TRUST-ME (No. 205121L_214991). MD was supported by TAILOR and by HumanE-AI-Net projects funded by EU Horizon 2020 research and innovation programme under GA No 952215 and No 952026, respectively. This study has received funding from the European Union's EU Framework Program for Research and Innovation Horizon under the Grant Agreement No 101073307 (MSCA-DN LeMuR).

## References

* [1] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and Dino Pedreschi. A survey of methods for explaining black box models. _ACM computing surveys (CSUR)_, 51(5):1-42, 2018.
* [2] Amina Adadi and Mohammed Berrada. Peeking inside the black-box: a survey on explainable artificial intelligence (xai). _IEEE access_, 6:52138-52160, 2018.
* [3] Alejandro Barredo Arrieta, Natalia Diaz-Rodriguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, et al. Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai. _Information fusion_, 58:82-115, 2020.
* [4] Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy Liang. Concept bottleneck models. In _International conference on machine learning_, pages 5338-5348. PMLR, 2020.
* [5] David Alvarez Melis and Tommi Jaakkola. Towards robust interpretability with self-explaining neural networks. _Advances in neural information processing systems_, 31, 2018.
* [6] Zhi Chen, Yijie Bei, and Cynthia Rudin. Concept whitening for interpretable image recognition. _Nature Machine Intelligence_, 2(12):772-782, 2020.
* [7] Mateo Espinosa Zarlenga, Pietro Barbiero, Zohreh Shams, Dmitry Kazhdan, Umang Bhatt, Adrian Weller, and Mateja Jamnik. Towards robust metrics for concept representation evaluation. _arXiv preprint arXiv:2301.10367_, 2023.
* [8] Eunji Kim, Dahuin Jung, Sangha Park, Siwon Kim, and Sungroh Yoon. Probabilistic concept bottleneck models. _arXiv preprint arXiv:2306.01574_, 2023.
* [9] Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. _Nature machine intelligence_, 1(5):206-215, 2019.
* [10] Eleonora Poeta, Gabriele Ciravegna, Eliana Pastor, Tania Cerquitelli, and Elena Baralis. Concept-based explainable artificial intelligence: A survey. _arXiv preprint arXiv:2312.12936_, 2023.
* [11] Pietro Barbiero, Gabriele Ciravegna, Francesco Giannini, Mateo Espinosa Zarlenga, Lucie Charlotte Magister, Alberto Tonda, Pietro Lio', Frederic Precioso, Mateja Jamnik, and Giuseppe Marra. Interpretable neural-symbolic concept reasoning. In _ICML_, 2023.
* [12] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. _arXiv preprint arXiv:1410.3916_, 2014.
* [13] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. _Neural networks_, 2(5):359-366, 1989.
* [14] Eleanor Rosch. Principles of categorization. In _Cognition and categorization_, pages 27-48. Routledge, 1978.

* Rudin et al. [2022] Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, and Chudi Zhong. Interpretable machine learning: Fundamental principles and 10 grand challenges. _Statistic Surveys_, 16:1-85, 2022.
* Li et al. [2018] Oscar Li, Hao Liu, Chaofan Chen, and Cynthia Rudin. Deep learning for case-based reasoning through prototypes: A neural network that explains its predictions. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 32, 2018.
* Chen et al. [2019] Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and Jonathan K Su. This looks like that: deep learning for interpretable image recognition. _Advances in neural information processing systems_, 32, 2019.
* Diligenti et al. [2017] Michelangelo Diligenti, Marco Gori, and Claudio Sacca. Semantic-based regularization for learning and inference. _Artificial Intelligence_, 244:143-165, 2017.
* Kusner et al. [2017] Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. _Advances in neural information processing systems_, 30, 2017.
* Cussens [2001] James Cussens. Parameter estimation in stochastic logic programs. _Machine Learning_, 44:245-271, 2001.
* Winters et al. [2022] Thomas Winters, Giuseppe Marra, Robin Manhaeve, and Luc De Raedt. Deepstochlog: Neural stochastic logic programming. _Proceedings of the AAAI Conference on Artificial Intelligence_, 36(9):10090-10100, Jun. 2022. doi: 10.1609/aaai.v36i9.21248. URL https://ojs.aaai.org/index.php/AAAI/article/view/21248.
* Manhaeve et al. [2018] Robin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, and Luc De Raedt. DeepProbLog: Neural Probabilistic Logic Programming. In _NeurIPS_, pages 3753-3763, 2018.
* Liu et al. [2015] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In _ICCV_, 2015.
* Welinder et al. [2010] Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge Belongie, and Pietro Perona. Caltech-ucsd birds 200. Technical Report CNS-TR-201, Caltech, 2010. URL /se3/wp-content/uploads/2014/09/WelinderEtal10_CUB-200.pdf, http://www.vision.caltech.edu/visipedia/CUB-200.html.
* Abraham et al. [2022] Eldar D Abraham, Karel D'Oosterlinck, Amir Feder, Yair Gat, Atticus Geiger, Christopher Potts, Roi Reichart, and Zhengxuan Wu. Cebab: Estimating the causal effects of real-world concepts on nlp model behavior. _Advances in Neural Information Processing Systems_, 35:17582-17596, 2022.
* Hand and Till [2001] David J. Hand and Robert J. Till. A simple generalisation of the area under the roc curve for multiple class classification problems. _Mach. Learn._, 45(2):171-186, oct 2001. ISSN 0885-6125. doi: 10.1023/A:1010920819831. URL https://doi.org/10.1023/A:1010920819831.
* Zarlenga et al. [2022] Mateo Espinosa Zarlenga, Pietro Barbiero, Gabriele Ciarvegna, Giuseppe Marra, Francesco Giannini, Michelangelo Diligenti, Zohreh Shams, Frederic Precioso, Stefano Melacci, Adrian Weller, Pietro Lio, and Mateja Jamnik. Concept embedding models: Beyond the accuracy-explainability trade-off. _Advances in Neural Information Processing Systems_, 35, 2022.
* Marconato et al. [2022] Emanuele Marconato, Andrea Passerini, and Stefano Teso. Glancenets: Interpretable, leak-proof concept-based models. _Advances in Neural Information Processing Systems_, 35:21212-21227, 2022.
* Havasi et al. [2022] Marton Havasi, Sonali Parbhoo, and Finale Doshi-Velez. Addressing leakage in concept bottleneck models. _Advances in Neural Information Processing Systems_, 35:23386-23397, 2022.
* Kindermans et al. [2019] Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T Schutt, Sven Dahne, Dumitru Erhan, and Been Kim. The (un) reliability of saliency methods. _Explainable AI: Interpreting, explaining and visualizing deep learning_, pages 267-280, 2019.

* Ghorbani et al. [2019] Amirata Ghorbani, Abubakar Abid, and James Zou. Interpretation of neural networks is fragile. In _Proceedings of the AAAI conference on artificial intelligence_, volume 33, pages 3681-3688, 2019.
* Adebayo et al. [2018] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. Sanity checks for saliency maps. _Advances in neural information processing systems_, 31, 2018.
* Poursabzi-Sangdeh et al. [2021] Forough Poursabzi-Sangdeh, Daniel G Goldstein, Jake M Hofman, Jennifer Wortman Wortman Wuraghan, and Hanna Wallach. Manipulating and measuring model interpretability. In _Proceedings of the 2021 CHI conference on human factors in computing systems_, pages 1-52, 2021.
* Kim et al. [2018] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, et al. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In _International conference on machine learning_, pages 2668-2677. PMLR, 2018.
* Ghorbani et al. [2019] Amirata Ghorbani, James Wexler, James Y Zou, and Been Kim. Towards automatic concept-based explanations. _Advances in neural information processing systems_, 32, 2019.
* Garcez et al. [2022] Artur d'Avila Garcez, Sebastian Bader, Howard Bowman, Luis C Lamb, Leo de Penning, BV Illuminoo, Hoifung Poon, and COPPE Gerson Zaverucha. Neural-symbolic learning and reasoning: A survey and interpretation. _Neuro-Symbolic Artificial Intelligence: The State of the Art_, 342(1):327, 2022.
* Marra et al. [2024] Giuseppe Marra, Sebastijan Dumancic, Robin Manhaeve, and Luc De Raedt. From statistical relational to neurosymbolic artificial intelligence: A survey. _Artificial Intelligence_, page 104062, 2024.
* Xu et al. [2018] Jingyi Xu, Zilu Zhang, Tal Friedman, Yitao Liang, and Guy Broeck. A semantic loss function for deep learning with symbolic knowledge. In _International conference on machine learning_, pages 5502-5511. PMLR, 2018.
* Baderddine et al. [2022] Samy Baderddine, Artur d'Avila Garcez, Luciano Serafini, and Michael Spranger. Logic tensor networks. _Artificial Intelligence_, 303:103649, 2022.
* Sourek et al. [2018] Gustav Sourek, Vojtech Aschenbrenner, Filip Zelezny, Steven Schockaert, and Ondrej Kuzelka. Lifted relational neural networks: Efficient learning of latent relational structures. _Journal of Artificial Intelligence Research_, 62:69-100, 2018.
* Tang and Ellis [2023] Hao Tang and Kevin Ellis. From perception to programs: regularize, overparameterize, and amortize. In _International Conference on Machine Learning_, pages 33616-33631. PMLR, 2023.
* Daniele et al. [2022] Alessandro Daniele, Tommaso Campari, Sagar Malhotra, and Luciano Serafini. Deep symbolic learning: Discovering symbols and rules from perceptions. _arXiv preprint arXiv:2208.11561_, 2022.
* Evans and Grefenstette [2018] Richard Evans and Edward Grefenstette. Learning explanatory rules from noisy data. _Journal of Artificial Intelligence Research_, 61:1-64, 2018.
* Lecun et al. [1998] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11):2278-2324, 1998. doi: 10.1109/5.726791.
* He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* Devlin et al. [2018] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.

* Wolf et al. [2020] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pages 38-45, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2020.emnlp-demos.6.
* Paszke et al. [2019] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library, 2019.
* Pedregosa et al. [2018] Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Andreas Muller, Joel Nothman, Gilles Louppe, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Edouard Duchesnay. Scikit-learn: Machine learning in python, 2018.
* Hunter [2007] J. D. Hunter. Matplotlib: A 2d graphics environment. _Computing in Science & Engineering_, 9(3):90-95, 2007. doi: 10.1109/MCSE.2007.55.

Maximum likelihood derivation

We show that the maximum likelihood problem in Equation 6 simplifies to:

\[\log p(\hat{y},\hat{c}|\hat{x})=\left(\sum_{i=1}^{n_{C}}\log p(c_{i}=\hat{c}_{i}| \hat{x})\right)+\left(\log\sum_{\hat{s}=1}^{n_{R}}p(s=\hat{s}|\hat{x})\,p(y=\hat {y}|\hat{c},\hat{s})\right)\]

with:

\[p(y|c,s)=\prod_{i=1}^{n_{C}}\left(p(I_{i}|s)+p(P_{i}|s)\,\mathbbm{1}[c_{i}=1]+p (N_{i}|s)\,\mathbbm{1}[c_{i}=0]\right)\]

Proof.: Let \(n:=n_{C}\). First, we express the likelihood as the marginalization of the distribution over the unobserved variables

\[p(y,c|x)=p(c|x)\sum_{s}p(s|x)\sum_{r_{1}}...\sum_{r_{n}}p(r_{1},...,r_{n}|s)p(y |r_{1},...,r_{n},c_{1},...,c_{n})\]

Due to the independence of the individual components of the rule distribution:

\[p(r_{1},...,r_{n}|s)=\prod_{i=1}^{n}p(r_{i}|s)\]

The logical evaluation of a rule can be expressed in terms of indicator functions over the different variables involved.

\[p(y|r_{1},...,r_{n},c_{1},...,c_{n})=\prod_{i=1}^{n}\mathbbm{1}[r_{i}=I]+ \mathbbm{1}[r_{i}=P]\mathbbm{1}[c_{i}=1]+\mathbbm{1}[r_{i}=n]\mathbbm{1}[c_{i }=0]\]

Let us define \(f(r_{i},c_{i}):=\mathbbm{1}[r_{i}=I]+\mathbbm{1}[r_{i}=P]\mathbbm{1}[c_{i}=1]+ \mathbbm{1}[r_{i}=n]\mathbbm{1}[c_{i}=0]\).

Then, the likelihood becomes:

\[p(y,c|x) =p(c|x)\sum_{s}p(s|x)\sum_{r_{1}}...\sum_{r_{n}}\prod_{i=1}^{n}p( r_{i}|s)\,f(r_{i},c_{i})\] \[=p(c|x)\sum_{s}p(s|x)\left(\sum_{r_{1}}p(r_{1}|s)\,f(r_{1},c_{1} )\right)(...)\left(\sum_{r_{n}}p(r_{n}|s)\,f(r_{n},c_{n})\right)\] \[=p(c|x)\sum_{s}p(s|x)\prod_{i=1}^{n}\sum_{r_{i}}p(r_{i}|s)\,f(r_{ i},c_{i})\] \[=p(c|x)\sum_{s}p(s|x)\prod_{i=1}^{n}(p(I_{i}|s)\,f(I_{i},c_{i})+ p(P_{i}|s)\,f(P_{i},c_{i})+p(N_{i}|s)\,f(N_{i},c_{i}))\] \[=p(c|x)\sum_{s}p(s|x)\prod_{i=1}^{n}(p(I_{i}|s)+p(P_{i}|s)\, \mathbbm{1}[c_{i}=1]+p(N_{i}|s)\,\mathbbm{1}[c_{i}=0])\] \[=p(c|x)\sum_{s}p(s|x)\,p(y|c,s)\]

Using \(p(c|x)=\prod_{i=1}^{n_{C}}p(c_{i}|x)\), and applying the logarithm, we find:

\[\log p(y,c|x)=\left(\sum_{i=1}^{n_{C}}\log p(c_{i}|x)\right)+\left(\log\sum_ {s=1}^{n_{R}}p(s|x)\,p(y|c,s)\right)\]

## Appendix B Implementation and optimization details

Selector re-initializationTo promote exploration, we re-initialize the parameters of \(p(s|x)\) multiple times during training, making it easier to escape local optima. The re-initialization frequency is a hyperparameter that differs between experiments, see Appendix C.

Effect of the regularization termFigure 4 shows the probabilities to be maximized when the label \(y=1\), for a selected rule and a single concept, with respect to different roles \(r\) for that concept. Figures 3(a), 3(b) and 3(c) show the probabilities to be maximized without the regularization. Figures 3(d), 3(e) and 3(f) show the regularization probabilities (remember, we only have these if \(y=1\)). Figures 3(g), 3(h) and 3(i) show the probabilities when both are present. As mentioned in the main text, when \(c\) is True for all examples that select the rule, we want that concept's role to be \(P\). However, when optimizing without regularization, it is clear that e.g. \(I\) is also an optimum (Figure 3(a)). Because the regularization only has an optimum in \(P\) (Figure 3(d)), adding the regularization results in the correct optimum (Figure 3(g)). A similar reasoning applies when \(c\) is False for all examples that select the rule; consider Figures 3(b), 3(e) and 3(h). It should be noted that the regularization alone is insufficient to get the correct optima; indeed, when examples with _different_\(c\) select the rule, the regularization term has many optima (Figure 3(f)), of which only \(I\) is desired (as explained in the main text). As the original loss only has an optimum in \(I\) in this case (Figure 3(c)), the only optimum that remains is the correct one (Figure 3(i)). **In summary:** the loss function we use has the desired optima (last row in Figure 4), dropping the regularization (first row in Figure 4) or only keeping the regularization (middle row in Figure 4) both have cases where the optima are undesired.

Figure 4: Likelihoods to be maximized when \(y=1\) w.r.t. the role \(r\) of a single concept in a selected rule, for different situations. As \(P+N+I=1\), irrelevance is the coordinate \((0,0)\). Likelihoods that cannot be achieved (i.e. when \(P+N>1\)) are put to 0. In the first column, the concept label is 1. In the second column, it is 0. In the third column, two examples with opposite labels select the rule.

Passing predicted conceptsWhile probabilistic semantics require passing the ground truth \(c\) to the task predictor during training (as these are observed variables), CBMs typically instead pass the concept predictions (opposite of teacher forcing) to make the model more robust to its own errors. For this reason, we also employ this technique in CMR in the experiments with CEBaB, CUB and CelebA. For competitors, we always employ this technique. However, we pass hard concept predictions in both CMR and the competitors, as to avoid leakage (see Appendix C).

Weight in the lossWe introduce a weight \(\beta\) in the regularized objective of Equation 8 (see below). This adjustment helps to balance the regularization and the task loss. For instance, if the regularization dominates, CMR might learn rules that can only be used to predict \(y=1\) in practice. Additionally, a larger \(\beta\) might assist in settings where achieving high accuracy is difficult for some concepts; in such cases, concepts with lower accuracy can be more easily considered irrelevant by choosing a larger \(\beta\), as they are unable to reliably contribute to correct task prediction. The updated objective is as follows:

\[\max_{\Omega}\sum_{(\hat{x},\hat{c},\hat{y})\in D}\left(\sum_{i=1}^{n_{C}}\log p (c_{i}=\hat{c}_{i}|\hat{x})\right)+\left(\log\sum_{\hat{s}=1}^{n_{R}}p(s=\hat{ s}|\hat{x})\,p(y=\hat{y}|\hat{c},\hat{s})^{\beta}\underbrace{p_{reg}(r=\hat{c}| s)^{\hat{y}}}_{\text{Regularization Term}}\right)\] (10)

## Appendix C Experiments

### Datasets

Mnist+This dataset [22] consists of pairs of images, where each image is an MNIST image of a digit and the task is the sum of the two digits. For these tasks, all concepts are relevant. There is a total of 30,000 training examples and 5,000 test examples.

Mnist+*We create this dataset as MNIST+ except that the concepts for digits 0 and 1 are removed from the concept set. This makes the concept set incomplete.

C-MnistWe derive this dataset from MNIST [44], taking the MNIST training (60,000 examples) and test set (10,000 examples), randomly colouring each digit either red or green, and adding these two colours as concepts. There are two tasks: The first is whether the digit is even, and the second is whether it is odd. For these tasks, the concepts related to colour are irrelevant.

CelebaThis is a large-scale face attributes dataset with more than 200K celebrity images [23]. Each image has 40 concept annotations. As tasks, we take the concepts \(Wavy\_Hair\), \(Black\_Hair\), and \(Male\), removing them from the concept set.

CubIn this dataset, the task is to predict bird species from images, where the concepts are 112 bird characteristics such as tail colour, wing pattern, etc [24]. CUB originally consists of 200 tasks, of which we take the first 10. Some concepts are strongly imbalanced (some are True in only \(\pm 0.5\%\) of examples, others in \(40\%\), etc.) and there is a large task imbalance (each task is True in only \(\pm 0.5\%\) of examples).

CEBaBThis is a text-based dataset that consists of reviews, where the task is to classify them as positive or negative [25]. There are 12 concepts (food, ambience, service, noise, each either unknown, bad or good) and 2 tasks (positive or negative review).

### Training ReproducibilityWe seed all experiments using seeds 1, 2 and 3.

Soft vs hard conceptsWhen departing from pure probabilistic semantics, CBMs allow not only the use of concepts as binary variables, but they allow for concepts to be passed to the task predictor together with their prediction scores, which is called employing _soft concepts_ (vs _hard concepts_). While some CBMs use soft concepts as this results in higher task accuracy, the downside of this is that the use of soft concepts also comes with the introduction of _input distribution leakage_: The concept probabilities can encode much more information than what is related to the concepts, severely harming the interpretability of the model [28, 29]. For this reason, in our experiments, all models use hard concepts, which is realized by thresholding the soft concept predictions at 50%.

Model inputFor MNIST+, MNIST+* and C-MNIST, we train directly on the images. For CelebA and CUB, instead of training on the images, we train the models on pretrained ResNet18 embeddings [45]. Specifically, using the torchvision library, we first resize the images to width and height 224 (using bi-linear interpolation), then normalize them per channel with means \((0.485,0.456,0.406)\) and standard-deviations \((0.229,0.224,0.225)\) (for CelebA only). Finally, we remove the last (classification) layer from the pretrained ResNet18 model, use the resulting model on each image, and flatten the output, resulting in an embedding. For CEBaB, we use a pretrained BERT model [46] to transform the input into embeddings. Specifically, using the transformers library [47], we create a BERT model for sequence classification from the pretrained model 'bert-base-uncased' with 13 labels (1 per concept and 1 representing both tasks). Then, we fine-tune this model for 10 epochs with batch size 2, 500 warm-up steps, weight decay 0.01 and 8 gradient accumulation steps. After training, we use this model to transform each example into an embedding by outputting the last hidden states for that example.

General training informationIn all experiments, we use the AdamW optimizer. All neural competitors are optimized to maximize the log-likelihood of the data with a weight on the likelihood of the task (1 if not explicitly mentioned below). After training, for each neural model in CelebA, CEBaB, MNIST+ and MNIST+*, we restored the weights that resulted in the lowest validation loss. In CUB, C-MNIST and the MNIST+ rule intervention experiment, we do not use a validation set, instead restoring the weights that resulted in the lowest training loss. In CelebA, we use a validation split of 8:2, a learning rate of 0.001, a batch size of 1000, and we train for 100 epochs. In CEBaB, we use a validation split of 8:2, a learning rate of 0.001, a batch size of 128, and we train for 100 epochs. In CUB, we use a learning rate of 0.001, a batch size of 1280, and we train for 300 epochs. In MNIST+ and MNIST+*, we use a validation split of 9:1. We use a learning rate of 0.0001, a batch size of 512, and we train for 300 epochs. In C-MNIST, we also use a learning rate of 0.0001, a batch size of 512, and we train for 300 epochs.

General architecture detailsIn CMR, we use two hidden layers with ReLU activation to transform the input into a different embedding. We use 3 hidden layers with ReLU activation and an output layer with Sigmoid activation to transform that embedding into concept predictions. The component \(p(s|x)\) takes as input that embedding and is implemented by a single hidden layer with ReLU activation and an output layer outputting \(n_{tasks}*n_{R}\) logits, which are reshaped to \((n_{tasks},n_{R})\) and to which a Softmax is applied over the rule dimension. The rulebook is implemented as an embedding module of shape \((n_{tasks}*n_{R},\text{rule emb size})\) that is reshaped to \((n_{tasks},n_{R},\text{rule emb size})\). The rule decoder is implemented by a single hidden layer with ReLU activation and an output layer outputting \(3*n_{concepts}\) logits, which are reshaped to \((n_{concepts},3)\), after which a Softmax is applied to the last dimension; the result corresponds to \(p(r|s)\). At test time, we make \(p(r|s)\) deterministic by setting the probability for the most likely role for each concept to 1 and the others to 0 (effectively collapsing each rule distribution to the most likely rule for that distribution). Then, exactly one rule corresponds with each \(s\).

The deep neural network is a feed-forward neural network consisting of some hidden layers with ReLU activation and an output layer with Sigmoid activation.

Both CBM+linear and CBM+MLP have a concept predictor that is a feed-forward neural network using ReLU activation for the 3 hidden layers and a Sigmoid activation for the output layer. For CBM+linear, the task predictor is a single linear layer per task with Sigmoid activation. For CBM+MLP, this is a feed-forward neural network using ReLU activation for the 3 hidden layers, and a Sigmoid activation for the output layer.

For CEM, we use 4 layers with ReLU activation followed by a Concept Embedding Module where the concept embeddings have a size of 30. The task predictor is a feed-forward neural network using ReLU activation for the 3 hidden layers and a sigmoid activation for the output layer.

For DCR, we also use 4 layers with ReLU activation to transform the input into an embedding. Then, one layer with ReLU activation and one with Sigmoid activation are used to transform the embeddinginto concept predictions. The embedding and the concepts are fed to the task predictor, which is a Concept Reasoning Layer using the product-t norm and a temperature of 10.

For CBM+DT and CBM+XGboost, we train a CBM+MLP and use its concept predictions as training input to the trees.

In the MNIST+ and MNIST+\({}^{*}\) experiments, as mentioned earlier, we use as input the two images instead of embeddings. Therefore, in all models, we use a CNN (trained jointly with the rest of the model) to transform the images into an embedding. This CNN consists of a Conv2d layer with 6 output channels and kernel size 5, a MaxPool2d layer with kernel size and stride 2, ReLU activation, a Conv2d layer with 16 output channels and kernel size 5, another MaxPool2d layer with kernel size and stride 2, and another ReLU activation. The result is flattened, and a linear layer is used to output an embedding per image (with size the same as the number of units in the hidden layers of the rest of the models). This results in 2 embeddings (one per image), which are combined using 10 linear layers, each with ReLU activation (except the last one) and again the same number of units (except the first layer, which has 2*number of units). Additionally, the CNN can output concept predictions by applying 3 linear layers with ReLU activation and a linear layer with a Softmax to each image embedding. This results in 10 concept predictions per image, which are concatenated. The final output of the CNN is the tuple of concept predictions and embedding.

In the C-MNIST experiments, the input is a single image. We use a similar CNN architecture as for MNIST+ but with two differences. Firstly, the activation before the concept prediction is a Softmax on only the first 10 concepts (related to the digits), while the activation on the last 2 concepts is Sigmoid. Secondly, after the flattening operation, we use 3 linear layers with ReLU activation and 1 linear layer without activation to transform the embedding into a different one.

Hyperparameters per experimentWe use _number of hidden units_ and _embedding size_ interchangeably. In CelebA, we always use 500 units in each hidden layer, except for CBM+linear and CMR where we use 100 units instead. For the deep neural network, we use 10 hidden layers. CMR uses a rule embedding size of 100, at most 5 rules per task, a \(\beta\) of 30, and we reset the selector every 35 epochs. The trained CBM+MLP for CBM+DT and CBM+XGboost has a weight on the task of 0.01.

In CEBaB, we use 300 units in the hidden layers, except for CMR where we use 100 units instead. For the deep neural network, we use 10 hidden layers. We additionally put a weight on the task loss of 0.01. For CMR, we use a rule embedding size of 100, at most 15 rules per task, a \(\beta\) of 4, and we reset the selector every 10 epochs. For CBM+DT and CBM+XGboost, the trained CBM+MLP has a weight on the task of 0.01.

In CUB. we use 100 units in the hidden layers, the deep neural network has 2 hidden layers, and we use a weight on the task loss of 0.01 for concept-based competitors. For CMR, we use a \(\beta\) of 3, and additionally down-weigh the loss for negative instances with a weight of 0.005 to deal with the class imbalance. We use a rule embedding size of 500, at most 3 rules per task, and we reset the selector every 25 epochs. For CBM+XGBoost, we add a weight of 200 for the positive instances to deal with the class imbalance. For CBM+DT and CBM+XGboost, we do not train a CBM+MLP, instead using CMR's concept predictor.

In MNIST+ and MNIST+\({}^{*}\), we use the CNN as described earlier. We always use 500 units in the hidden layers, except for CBM+linear and CMR where we use 100 units. For the deep neural network, we use 10 hidden layers. For CMR, we use a rule embedding size of 1000, at most 20 rules per task, we reset the selector every 40 epochs, and \(\beta\) is 0.1. In MNIST+ specifically, instead of passing the CNN's output embedding to the rule selector \(p(s|x)\), we pass the CNN's concept predictions, showing that this alternative can also be used effectively when using a complete concept set. For CBM+DT and CBM+XGboost, we train a CBM+MLP with weight on the task of 0.01 and use its concept predictions as input to the trees. For the rule intervention experiment on MNIST+ (where we give CMR a rulebook size that is too small to learn all ground truth rules), we allow it to learn at most 9 rules per task. Here, the input to the selector is the CNN's output embedding. In MNIST+\({}^{*}\), we pass the CNN's output embedding to the rule selector.

In C-MNIST, we use the second CNN described above. We always use 500 units in the hidden layers, except for CBM+linear where we use 100 units. For the deep neural network, we use 10 hidden layers. For DCR and CEM, we put a weight on the task loss of 0.01. For CMR, the rule book has at most 6 rules per task, with rule embeddings of size 1000. We use a \(\beta\) of 1, reset the selector every 40 epochs, and additionally put a weight on the concept reconstruction relative to the concept counts. The CBM+MLP we train for CBM+DT and CBM+XGboost has a weight of 0.01 on the task.

Hyperparameter searchFor CBM+DT, we tune the maximum depth parameter, trying all values between 1 and \(n_{C}\), and report the results with the highest validation accuracy (training accuracy in absence of a validation set). Parameters for the neural models were chosen that result in the highest validation accuracy (training accuracy in absence of a validation set) (for CMR the \(\beta\) parameter, rulebook size and rule embedding size were also chosen based on the learned rules). For \(\beta\), we searched within the grid \([0.1,1,3,4,10,30]\), for the embedding size within the grid \([10,100,300,500,1000]\), and for the rule embedding size within \([100,500,1000]\). For the deep neural network's number of hidden layers, we searched within the grid \([2,10,20]\). For unmentioned parameters in the competitors, we used the default values.

Remaining setup of the rule intervention experimentWe first train for 300 epochs. Then, we check in an automatic way for rules that differ from the ground truth. A representative example is the rule \(y_{3}\leftarrow(c_{0,1})\wedge(c_{1,2})\wedge(c_{0,3})\wedge(c_{1,0})\)5, which can be used by the selector for correctly predicting that \(1+2=3\) and \(3+0=3\). After this, we add each missing ground truth rule, _except one_, and let CMR continue training for 100 epochs. Consequently, CMR improves its originally learned rules that differed from ground truth to the ground truth ones and learns to correctly select between the learned and manually added rules. For instance, if CMR originally learned a rule \(y_{3}\leftarrow(c_{0,1})\wedge(c_{1,2})\wedge(c_{0,3})\wedge(c_{1,0})\), after manually adding \(y_{3}\gets c_{0,1}\wedge c_{1,2}\) and fine-tuning, the rule improves to \(y_{3}\gets c_{0,3}\wedge c_{1,0}\), and CMR will no longer select this rule for the examples \(3+0\), instead selecting the manually added rule.

Footnote 5: In this notation, for brevity, we only show concepts with a role that is positive (of which none are present in this rule example) or irrelevant (between parentheses); concepts with negative roles are not shown and are the remaining ones.

### Additional results

#### c.3.1 Concept interventions

To measure the effectiveness of concept interventions [4], we report task accuracy before and after replacing the concept predictions with their ground truth. In Table 4, we observe that CMR is responsive to concept interventions: After the interventions, CMR achieves perfect task accuracy, outperforming CEM and DCR. We leave a more extensive investigation of concept interventions for CMR to future work.

#### c.3.2 Concept accuracies

Table 5 gives the concept accuracy of CMR and the competitors on all datasets. CMR's concept accuracy is similar to its competitors across all datasets.

#### c.3.3 Rule interventions with a rule learner

This experiment serves as an ablation study on CMR's end-to-end rule learning component. We investigate whether pre-learning rules using an external rule learner and manually integrating them into CMR's memory impacts its accuracy. We employ a decision tree as the rule learner, although

\begin{table}
\begin{tabular}{l c c} \hline \hline  & before & after \\ \hline CBM+MLP & \(97.41\pm_{0.55}\) & \(100.0\pm_{0.00}\) \\ CEM & \(92.44\pm_{0.26}\) & \(94.68\pm_{0.31}\) \\ DCR & \(90.70\pm_{1.21}\) & \(94.11\pm_{0.83}\) \\ \hline
**CMR (ours)** & \(97.25\pm_{0.24}\) & \(100.0\pm_{0.00}\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Task accuracy before and after concept interventions for MNIST+.

other rule learners could be used as well. We use the CEBaB dataset where we only take the first task and the first 6 concepts.

In Figure 5, we compare CMR's accuracy against 3 alternatives: (1) using a CBM+DT; (2) injecting the rules learned by the CBM+DT for predicting a positive class into CMR while preventing CMR from learning any rules itself; and (3) injecting these same rules but allowing CMR to learn an additional 15 rules.

**CMR's rule learning component enhances accuracy.** When CMR is allowed to learn rules (both CMR (n=15) and DT \(\rightarrow\) CMR (n=15)), its accuracy improves significantly compared to when only the decision tree's rules are used (DT \(\rightarrow\) CMR (n=0)).

**Injecting rules into CMR does not reduce accuracy when CMR is allowed to learn additional rules.** CMR achieves similar levels of accuracy in both cases where it only learns rules (CMR (n=15) and when supplemented with the decision tree's rules (DT \(\rightarrow\) CMR (n=15)).

**CMR's rule selector enhances accuracy beyond the pre-obtained rules.** Even when CMR is restricted from learning any rules (DT \(\rightarrow\) CMR (n=0)), it still performs better than the original CBM+DT (DT) because of the rule selector.

For learning the decision tree (and the corresponding CBM used for predicting the concepts beforehand), we used seed 1.

#### c.3.4 Accuracy robustness to the number of rules

In this experiment, we investigate the robustness of CMR's accuracy with respect to the number of rules hyperparameter (\(n_{R}\)). We use the CEBaB dataset where we only consider the first task, and we train CMR for different values of \(n_{R}\). Figure 6 shows that similar levels of accuracy are obtained regardless of the chosen \(n_{R}\) value.

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline  & mnist+ & mnist+\({}^{*}\) & c-mnist & celeba & cub & cebaB \\ \hline CBM+linear & \(99.76\pm 0.02\) & \(99.73\pm 0.02\) & \(99.72\pm 0.03\) & \(87.63\pm 0.02\) & \(86.24\pm 0.05\) & \(92.69\pm 0.07\) \\ CBM+MLP & \(99.75\pm 0.05\) & \(99.71\pm 0.06\) & \(99.80\pm 0.02\) & \(87.68\pm 0.02\) & \(86.32\pm 0.08\) & \(92.67\pm 0.07\) \\ CBM+DT & \(99.69\pm 0.04\) & \(99.68\pm 0.01\) & \(99.81\pm 0.01\) & \(87.70\pm 0.04\) & \(85.95\pm 0.08\) & \(92.83\pm 0.02\) \\ CBM+XGB & \(99.69\pm 0.04\) & \(99.68\pm 0.01\) & \(99.81\pm 0.01\) & \(87.70\pm 0.02\) & \(85.95\pm 0.08\) & \(92.83\pm 0.02\) \\ CEM & \(99.26\pm 0.04\) & \(99.26\pm 0.13\) & \(99.77\pm 0.01\) & \(87.57\pm 0.04\) & \(85.82\pm 0.23\) & \(92.46\pm 0.13\) \\ DCR & \(99.21\pm 0.09\) & \(99.74\pm 0.02\) & \(99.76\pm 0.02\) & \(85.35\pm 0.26\) & \(85.67\pm 0.15\) & \(92.66\pm 0.04\) \\ \hline
**CMR (ours)** & \(99.74\pm 0.02\) & \(99.73\pm 0.01\) & \(99.82\pm 0.01\) & \(86.80\pm 0.22\) & \(85.95\pm 0.08\) & \(92.70\pm 0.08\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Concept accuracies for all datasets.

#### c.3.5 Decoded rulebooks

In this section, we provide examples of the decoded rulebooks after training with seed 1 for our experiments. We do not provide decoded rulebooks for MNIST+, as these rules always correspond to the ground truth and are the same over all seeds, i.e. for every task \(i\), CMR learns a rule per possible pair of digits that correctly sums to that number \(i\). An example of such a rule is \(y_{3}\gets c_{1,2}\wedge c_{2,1}\wedge D\) where \(D\) is a conjunction of \(n_{C}-2\) conjuncts, where each conjunct is the negation of a different concept. In the tables in this section, we provide the decoded rulebooks for MNIST+\({}^{*}\), C-MNIST, CelebA (for all concept subsets, but not the complete rulebook for \(n_{C}=37\)), CUB (not complete either) and CEBaB. Importantly, for brevity, we use a different notation for the rules for C-MNIST, MNIST+\({}^{*}\) and CUB: We only show concepts in the rule with as role either positive (as is) or irrelevance (between parentheses); we do not show concepts with negative roles. For the other rulebooks, we use the standard notation.

Figure 5: Rule interventions on CEBaB where we predict only the first task and employ 6 concepts. We compare CMR’s accuracy with a CBM using a decision tree (DT) and CMR after adding the decision tree’s rules to CMR’s memory (DT \(\rightarrow\) CMR) without any additional learnable rules (n = 0) and when allowing 15 additional learnable rules (n = 15). The mean and standard deviation is shown over 3 seeds. This means that (1) CMR’s end-to-end rule learning allows it to obtain higher accuracy than when purely integrating pre-obtained rules from other rule learners (removing CMR’s rule learning component), (2) just integrating pre-obtained rules in CMR (while still allowing more rules to be learned) does not decrease its accuracy, and (3) using CMR with only pre-obtained rules still surpasses the performance of the rules in isolation due to the selector.

Figure 6: Robustness of CMR’s accuracy w.r.t. the number of rules on CEBaB where we predict the first task. The mean and standard deviation is shown over 3 seeds. High accuracy can be obtained regardless.

[MISSING_PAGE_FAIL:23]

[MISSING_PAGE_FAIL:24]

\begin{table}
\begin{tabular}{l l} \hline (1) & \(Black\_Hair\leftarrow\) \\ (2) & \(Black\_Hair\gets\)5\_o\_Clock\_Shadow \\ (3) & \(Black\_Hair\leftarrow\)5\_o\_Clock\_Shadow \\ \hline (4) & \(Male\leftarrow\)5\_o\_Clock\_Shadow \\ (5) & \(Male\leftarrow\)5\_o\_Clock\_Shadow \\ (6) & \(Male\leftarrow\)5\_o\_Clock\_Shadow \\ \hline (7) & \(Wavy\_Hair\leftarrow\) \\ (8) & \(Wavy\_Hair\gets\)5\_o\_Clock\_Shadow \\ (9) & \(Wavy\_Hair\leftarrow\)5\_o\_Clock\_Shadow \\ \hline \end{tabular}
\end{table}
Table 9: Rulebook for CelebA (\(n_{C}=1\)) (seed 1).

Figure 8: Selection of training examples satisfying learned rules for CUB. For brevity, we drop some of the irrelevant concepts and replace them with \((...)\).

[MISSING_PAGE_FAIL:26]

Code, licenses and resources

For our experiments, we implemented the models in Python 3.11.5 using open source libraries. This includes PyTorch v2.1.1 (BSD license) [48], PyTorch-Lightning v2.1.2 (Apache license 2.0), scikit-learn v1.3.0 (BSD license) [49] and xgboost v2.0.3 (Apache license 2.0). We used CUDA v12.4. Plots were made using Matplotlib v3.8.0 (BSD license) [50]. Our code is publicly available at https://github.com/daviddebot/CMR under the Apache License, Version 2.0.

All datasets we used are freely available on the web with licenses:

* CC BY-SA 3.0 DEED,
* CC BY 4.0 DEED,
* MIT License 6, Footnote 6: https://huggingface.co/datasets/cassiekang/cub200_dataset
* The CelebA dataset is available for non-commercial research purposes only7.

Footnote 7: https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html

We will not further distribute them.

The experiments for MNIST+, MNIST+\({}^{*}\), C-MNIST, CelebA and CEBaB were run on a machine with an NVIDIA GeForce GTX 1080 Ti, Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz with 128 GB RAM. The experiment for CUB and the fine-tuning of the BERT model used for the CEBaB embeddings were run on a machine with i7-10750H CPU, 2.60GHz x 12, GeForce RTX 2060 GPU with 16 GB RAM. Table 12 shows the estimated total computation time for a single run per experiment.

\begin{table}
\begin{tabular}{l c} \hline \hline
**Experiment** & **Time (hours)** \\ \hline CelebA & 3.7 \\ CUB & 1.8 \\ CEBaB & 0.1 \\ MNIST+ & 4.1 \\ MNIST+ rule int. & 0.2 \\ MNIST+\({}^{*}\) & 4.1 \\ C-MNIST & 4.4 \\ \hline \hline \end{tabular}
\end{table}
Table 12: Estimated total computation time for a single run of each experiment.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The structure of our model (neural nets + interpretable memory), discussion (expressivity, interpretability and verification) and experiments (research questions) closely follow the claimed contributions in both abstract and introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? [Yes] Justification: This is done in our conclusion (Section 8). Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: Theorem 4.1 is proven in Section 4.1 and Theorem 5.1 is proven in Appendix A. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Limited details are given in Section 6, with the remaining details provided in Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The code is provided at https://github.com/daviddebot/CMR. The data is publicly available on the internet, and we do not redistribute it. This is mentioned in the main text. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: These details are given in Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We provide the standard-deviations over multiple seeds in all our tables. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

[MISSING_PAGE_FAIL:31]

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We mentioned and respected all the licences of software and data in Appendix D. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: Documentation for the code is provided in the form of a README file in the corresponding repository. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.