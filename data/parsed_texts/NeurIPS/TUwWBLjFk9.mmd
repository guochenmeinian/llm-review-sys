On the Identifiability of Poisson Branching Structural Causal Model Using Probability Generating Function

 Yu Xiang\({}^{1}\), Jie Qiao\({}^{1}\), Zhefeng Liang\({}^{1}\), Zihuai Zeng\({}^{1}\), Ruichu Cai\({}^{1,2}\), Zhifeng Hao\({}^{3}\)

\({}^{1}\)School of Computer Science, Guangdong University of Technology, Guangzhou, China

\({}^{2}\)Peng Cheng Laboratory, Shenzhen, China

\({}^{3}\)College of Science, Shantou University, Shantou, China

{thexiang2000, qiaojie.chn, lzfeng011021, zzhuaiii, cairuichu}@gmail.com,

haozhifeng@stu.edu.cn

Equal contribution.Corresponding author.

###### Abstract

Causal discovery from observational data, especially for count data, is essential across scientific and industrial contexts, such as biology, economics, and network operation maintenance. For this task, most approaches model count data using Bayesian networks or ordinal relations. However, they overlook the inherent branching structures that are frequently encountered, e.g., a browsing event might trigger an adding cart or purchasing event. This can be modeled by a binomial thinning operator (for branching) and an additive independent Poisson distribution (for noising), known as Poisson Branching Structure Causal Model (PB-SCM). There is a provably sound cumulant-based causal discovery method that allows the identification of the causal structure under a branching structure. However, we show that there still remains a gap in that there exist causal directions that are identifiable while the algorithm fails to identify them. In this work, we address this gap by exploring the identifiability of PB-SCM using the Probability Generating Function (PGF). By developing a compact and exact closed-form solution for the PGF of PB-SCM, we demonstrate that each component in this closed-form solution uniquely encodes a specific local structure, enabling the identification of the local structures by testing their corresponding component appearances in the PGF. Building on this, we propose a practical algorithm for learning causal skeletons and identifying causal directions of PB-SCM using PGF. The effectiveness of our method is demonstrated through experiments on both synthetic and real datasets.

## 1 Introduction

Causal discovery from observational data, particularly for count data is a crucial task that arises in numerous applications, including biology (Wiuf and Stumpf (2006)), economic (Weiss and Kim (2014)), network operation maintenance (Qiao et al. (2023); Cai et al. (2022)), etc. Much effort has been made to model and discover the causal structure from count data. One line of research models the count data as a type of Poisson Bayesian network (Park and Raskutti (2015, 2017)) or ordinal functional model (Ni and Mallick (2022)), based on which various types of discovering methods are investigated. However, the Bayesian network and ordinal modeling ignore the inherent branching structure among the counting relationship, which is frequently encountered in real world (Weiss (2018)). Take Fig. 1 (a) as an example, in online shopping, the purchasing event can be inherited from browsing, cart added, or promotion event, which exhibits a branching structure such that website browsing (\(X_{1}\)) may lead to either purchasing (\(X_{4}\)) directly without adding to the cart, or addingto the cart (\(X_{2}\)) followed by purchasing (\(X_{4}\)), as depicted in Fig. 1 (b). Accurately modeling and identifying the underlying causal mechanisms-particularly the branching structure inherent in these count variables-are of interest for providing valuable insights to service providers.

Branching structure modeling is well studied across various domains, notably within the context of the integer-value autoregressive model (Weiss (2018); Al-Osh and Alzaid (1987); McKenzie (1985)). Most of these studies model the relationship among count variables utilizing a thinning operator '\(\circ\)' (Steutel and van Harn (1979)). For instance, the causal pair \(X_{1}\to X_{4}\) in Fig. 1(a) can be modeled as follows: \(X_{4}=\alpha\circ X_{1}+\epsilon\) where \(\alpha\circ X_{1}\coloneqq\sum_{n=1}^{X_{1}}\xi_{n}^{(\alpha)}\), \(\xi_{n}^{(\alpha)}\stackrel{{\mathrm{i.i.d.}}}{{\sim}}\text{ Bernoulli}(\alpha)\), \(\epsilon\sim\text{Pois}(\mu)\). That is, the thinning operator models the branching of an event, e.g., a website browsing event leads to a purchase with probability \(\alpha\), while the Poisson noise models the exogenous event. Such modeling can be formalized as the Poisson Branching Structure Causal Model (PB-SCM) (Qiao et al. (2024)). Recently, Qiao et al. (2024) further explores the identifiability of PB-SCM and proposes a likelihood-based heuristic searching method for learning causal skeleton and a cumulant-based method for identifying causal direction. Although some identifiability results are developed by Qiao et al. (2024) using cumulant, we show that there still remains a gap in that there exist causal directions that are identifiable while the algorithm fails to identify them. For example, the current approach cannot fully identify the causal directions among \(X_{1}\), \(X_{2}\), and \(X_{3}\) in Fig. 1 (a). Moreover, the likelihood-based heuristic searching method requires extensive coefficient estimating and structure searching suffering from computational complexity and potential convergence issues.

In this paper, we aim to develop the identifiability of the Poisson Branching Structural Causal Model (PB-SCM) using the Probability Generating Function (PGF) as the characterization of the distribution. Specifically, we first develop the closed-form solution for the PGF of PB-SCM which establishes the connection with the causal structure. With this connection, we find that each component in this closed-form solution is directly associated with a unique local structure, enabling us to learn the skeleton and analyze causal asymmetries without the need for the entire structure. Based on this, we provide an exact pseudo-polynomial time causal structure learning algorithm (i.e., polynomial in the magnitude of the observed variables). We demonstrate the effectiveness of the proposed causal discovery method using synthetic data and real-world data.

Our contributions are threefold. 1) We develop the closed-form solution for PGF of PB-SCM and establish a connection between the closed-form solution of PGF and the graphical patterns. 2) We exploit the closed-form solution of PGF to identify the causal skeleton as well as the causal direction of PB-SCM, which allows us to identify the adjacency and causal direction directly. 3) We propose a practical structure learning algorithm and demonstrate its efficiency and effectiveness through synthetic and real-world data experiments.

## 2 Related work

For brevity, we review causal discovery methods that are applicable and fully identifiable on observational discrete data. Numerous methods and theories have been developed for learning causal structure from observational data (Spirtes et al. (2000); Zhang et al. (2018); Glymour et al. (2019); Cai et al. (2018); Qiao et al. (2024)). In particular, for discrete data, one can employ constraint-based methods (Pearl (2009); Spirtes et al. (1995)), score-based methods (Chickering (2002); Tsamardinos et al. (2006)) to identify the causal structure by exploring the conditional independence relation

Figure 1: (a) An example of the causal graph of count data in an online shopping service. (b) An illustration of the branching structure inherent in count data, where browsing events trigger subsequent events that are summarized as counts.

among variables, but the conditional independence relation can only identify up to the Markov equivalent class (Pearl (2009)). Recently, numerous approaches have been developed for categorical or ordinal data that can identify beyond Markov equivalent class (Cai et al. (2018), Ni (2022), Ni and Mallick (2022), Peters et al. (2010), Leonelli and Varando (2023), Figueiredo and Oliveira (2023)). However, these methods are rarely suitable for count data. Although some recent works explore the Poisson Bayesian network for the Poisson data (Park and Raskutti (2015, 2017)), and further extend it to the zero-inflated Poisson data (Choi et al. (2020)), there still remains a gap in the identifiability of the Possion branching structure model.

## 3 Poisson Branching Structural Causal Model

In this section, we first introduce the Poisson branching structural causal model (PB-SCM), and then we introduce the preliminary of PGF.

### Problem Formulation

Let \(\mathbf{X}=\{X_{1},\ldots,X_{d}\}\) denote a \(d\)-dimensional random count vector, of which the causal relationship consists of a causal directed acyclic graph (DAG) \(G(\mathbf{V},\mathbf{E})\) with the vertex set \(\mathbf{V}=\{1,2,...,d\}\) and edge set \(\mathbf{E}\). We use \(\text{{Pa}}(i)=\{j|j\to i\in\mathbf{E}\}\), \(\text{{Ch}}(i)=\{j|i\to j\in\mathbf{E}\}\) and \(\text{{Des}}(i)=\{j|i\sim j\in\mathbf{E}\}\) denote the sets of parents, children and descendants of vertex \(i\) in \(G(\mathbf{V},\mathbf{E})\). We assume that any variable in \(\mathbf{X}\) satisfies the following Poisson Branching Structural Causal Model (PB-SCM):

**Definition 1** (Poisson Branching Structural Causal Model).: _For each random variable \(X_{i}\in\mathbf{X}\), let \(\epsilon_{i}\sim\text{{Pois}}(\mu_{i})\) be the noise component of \(X_{i}\), \(X_{i}\) is generated by:_

\[X_{i}=\sum_{j\in\text{{Pu}}(i)}\alpha_{j,i}\circ X_{j}+\epsilon_{i},\] (1)

_where \(\alpha_{j,i}\in(0,1]\) is the coefficient from vertex \(j\) to \(i\), and \(\alpha\circ X_{i}\) is a Binomial thinning operator such that \(\alpha\circ X_{i}=\sum_{n=1}^{X_{i}}\xi_{n}^{(\alpha)}\), where \(\xi_{n}^{(\alpha)}\overset{\text{i.i.d.}}{\sim}\mathrm{Bernoulli}(\alpha)\), independently of \(X_{i}\)._

The formulation is consistent with the existing literature, such as Qiao et al. (2024), Al-Osh and Alzaid (1987). We further assume that the _faithfulness assumption_, the _causal Markov assumption_, and the _causal sufficient assumption_ hold. These assumptions are commonly used in constraint-based causal discovery methods, e.g., PC algorithm (Spirtes et al. (2000)). With these assumptions, we formalize our goal as follows:

Goal.Given i.i.d. samples \(\mathcal{D}=\{x_{1}^{(j)},\ldots,x_{d}^{(j)}\}_{j=1}^{m}\) from the joint distribution \(P(\mathbf{X})\) generated by PB-SCM, our goal is to identify the unknown causal structure \(G\) from \(\mathcal{D}\).

### Preliminary

To develop the identifiability of PB-SCM, we resort to using the probability generating function as the proxy to analyze the distribution and its asymmetry property. Here, we recall the definition of the probability generating function:

**Definition 2** (Probability Generating Function).: _Given discrete random vector \(\mathbf{X}=[X_{1},...,X_{d}]^{T}\) taking values in the non-negative integers \(\mathbb{Z}^{\geqslant 0}\), the probability generating function of \(\mathbf{X}\) is defined as:_

\[G_{\mathbf{X}}(\mathbf{z})=\mathbb{E}[z_{1}^{X_{1}}\cdots z_{d}^{X_{d}}]=\sum _{x_{1},...,x_{d}=0}^{\infty}p(x_{1},...,x_{d})z_{1}^{x_{1}}\cdots z_{d}^{x_{ d}},\] (2)

_where \(p\) is the probability mass function of \(\mathbf{X}\). The power series converges absolutely at least for all complex vectors \(\mathbf{z}=[z_{1},...,z_{d}]^{T}\in\mathbb{C}^{d}\) with \(\max\{|z_{1}|,...,|z_{d}|\}\leqslant 1\)._

Since the PGF is uniquely defined by the probability mass function, such uniqueness of the power series expansion will in turn define the probabilities, meaning that the analysis of the distribution can be conducted on PGF as a proxy (Johnson et al. (2005)).

## 4 Structure Learning of PB-SCM Using Probability Generating Function

In this section, we demonstrate how to utilize PGF to identify causal structures. We first develop the closed-form solution for the PGF of PB-SCM, which has a compact and exact representation capturing the global structure information. Building on this, we delve into the components of the expansion of the PGF's closed form, demonstrating that each component in PGF is connected to a specific local structure. To effectively capture these components, we develop a local PGF. Finally, based on such a connection, we present theoretical results regarding the graphical implications of the component captured by local PGF, which can be used to discover causal structures.

### Motivating Example

Before the formal discussion, we first present a motivating example illustrating how PGF helps analyze the identifiability of PB-SCM.

Taking the triangular structure in Fig. 2 as an example, the corresponding closed form of PGF is given in Fig. 3. We can observe that the closed form of the PGF can be separated into three parts, starting at \(X_{1}\), \(X_{2}\), and \(X_{3}\), respectively. Each part of PGF encapsulates the path information in the overall causal structure. For example, considering the term \(z_{1}z_{2}z_{3}^{2}\) in 1, the \(z_{3}^{2}\) represents that there are two paths from \(X_{1}\) to \(X_{3}\) if the path \(\alpha_{1,2},\alpha_{2,3},\alpha_{1,3}\) are 'open'. By this, one can deduce that for the reserved direction, e.g., \(X_{3}\to X_{2}\), such a term \(z_{3}^{2}\) will not exist, which justifies that the causal direction between \(X_{2},X_{3}\) is identifiable.

Another attractive property of the PGF is the ability to analyze the local behavior of the structure. For example, by properly setting \(z_{1}\) approach \(0\), we can deduce whether the exist of adjacency between \(X_{2},X_{3}\) by testing whether the term \(z_{2}z_{3}\) exists as shown in term 1. Furthermore, as shown in Fig. 1, by analyzing the local PGF, we can show that the causal direction \(X_{1}\to X_{2}\gets X_{3}\) is identifiability while the previous cumulant-based method (Qiao et al. (2024)) can not.

In light of the connection between PGF and the causal structure, we can develop a PGF-based method to effectively identify the causal structure. In the following section, we first demonstrate how the PGF encapsulates the causal structure by developing the closed-form solution of the PGF. Then, benefiting from the local analysis of PGF, we address the identifiability gap of PB-SCM and present a more general identifiability result.

Figure 3: Illustration of the graphical implication in the closed-form solution for the PGF of PB-SCM. Here, \(\alpha_{i,j}T_{X_{j}}(1)\) indicates that \(X_{j}\) is reached from \(X_{i}\), while \((1-\alpha_{i,j})T_{X_{j}}(0)\) indicates the opposite.

Figure 2: Example triangular structure.

### Closed Form Solution of Probability Generating Function

In order to analyze the identifiability of PB-SCM, we first establish a fundamental theorem that provides the closed-form solution for the PGF of PB-SCM:

**Theorem 1** (Closed-form solution for PGF of PB-SCM).: _Given a random vector \(\mathbf{X}=[X_{1},...,X_{n}]^{T}\) following PB-SCM, let \(\mathbf{z}_{(j)}=\{z_{l}|l\in\text{Des}(j)\cup\{j\}\}\), the PGF of \(P(\mathbf{X})\) is given by \(G_{\mathbf{X}}(\mathbf{z})=\prod_{i\in[d]}G_{\epsilon_{i}}\Big{(}z_{i}\times \prod_{j\in\text{Ch}(i)}G_{i,j}(\mathbf{z}_{(j)})\Big{)}\), where_

\[G_{i,j}(\mathbf{z}_{(j)})=\begin{cases}G_{B(\alpha_{i,j})}\left(z_{j}\times \prod_{k\in\text{Ch}(j)}G_{j,k}(\mathbf{z}_{(k)})\right)&,\text{Ch}(j)\neq \emptyset\\ G_{B(\alpha_{i,j})}(z_{j})&,\text{Otherwise}\end{cases},\] (3)

_in which \(G_{\epsilon_{i}}(\cdot)\) is the PGF of Poisson noise \(\epsilon_{i}\) and \(G_{B(\alpha_{i,j})}(\cdot)\) is the PGF of Bernoulli distribution with parameter \(\alpha_{i,j}\)._

The main idea of the proof is to decompose the expectation of PGF according to the causal structure, e.g., a conditional expectation in PGF can be derived as \(\mathbb{E}\left[z_{3}^{\alpha_{1,30}\epsilon_{1}}|\epsilon_{1}\right]=G_{B( \alpha_{1,3})}(z_{3})^{\epsilon_{1}}\) since \(\alpha_{1,3}\circ\epsilon_{1}|\epsilon_{1}\sim\mathrm{Binomial}(n=\epsilon_{1 },p=\alpha_{1,3})\). Applying the above rule recursively with the law of expectation, we can obtain Theorem 1. The overall proof is given in Appendix A.1.

Theorem 1 reveals the inherent relationship between the causal structure and the close-formed solution of PGF. To see this connection more concretely, we further develop the following theorem:

**Theorem 2**.: _Given a random vector \(\mathbf{X}=[X_{1},...,X_{d}]^{T}\) following PB-SCM, the PGF of \(P(\mathbf{X})\) can be expressed by:_

\[G_{\mathbf{X}}(\mathbf{z})=\prod_{i\in[d]}\exp\{\mu_{i}\times(T_{X_{i}}(1)-1)\},\] (4)

_where \(T_{X_{i}}(1)=z_{i}\sum_{\mathbf{s}\in\{0,1\}^{\mathrm{[\text{Ch}(i)]}}}\prod_{ j\in\text{Ch}(i)}\alpha_{i,j}^{s_{j}}T_{X_{j}}(s_{j}),T_{X_{i}}(0)=1\) and \(\alpha_{i,j}^{s_{j}}=\alpha_{i,j}\) if \(s_{j}=1\) and \(\alpha_{i,j}^{s_{j}}=1-\alpha_{i,j}\) if \(s_{j}=0\)._

Theorem 2 provides a more compact closed-form solution for the PGF of PB-SCM. Intuitively, it consists of different path information starting at different vertex. Taking the triangular structure in Fig. 3 as an example, the closed-form solution of PGF can be expressed as follows:

\[G_{\mathbf{X}}(\mathbf{z}) =\exp\Big{\{}\mu_{1}\times[\underbrace{(1-\alpha_{1,2})(1-\alpha _{1,3})z_{1}}_{\mbox{\includegraphics[width=14.226378pt]{T1}}}+\underbrace{ \alpha_{1,2}(1-\alpha_{1,3})(1-\alpha_{2,3})z_{1}z_{2}+\alpha_{1,2}\alpha_{2,3 }(1-\alpha_{1,3})z_{1}z_{2}z_{3}}_{\mbox{\includegraphics[width=14.226378pt]{ T2}}}\] (5) \[+\underbrace{\alpha_{1,3}(1-\alpha_{1,2})z_{1}z_{3}}_{\mbox{ \includegraphics[width=14.226378pt]{T2}}}+\underbrace{\alpha_{1,2}\alpha_{1,3 }(1-\alpha_{2,3})z_{1}z_{2}z_{3}+\alpha_{1,2}\alpha_{2,3}\alpha_{1,3}z_{1}z_{ 2}z_{3}^{2}}_{\mbox{\includegraphics[width=14.226378pt]{T2}}}-\mu_{1}\Big{\}}\] \[\times\exp\Big{\{}\mu_{2}\times[\underbrace{(1-\alpha_{2,3})z_{2} }_{\mbox{\includegraphics[width=14.226378pt]{T2}}}+\underbrace{\alpha_{2,3}z_{ 2}z_{3}}_{\mbox{\includegraphics[width=14.226378pt]{T2}}}-\mu_{2}\Big{\}} \times\exp\{\mu_{3}\times(\underbrace{z_{3}}_{\mbox{\includegraphics[width=14.226 378pt]{T2}}}-1)\},\]

which is the combination of three main terms \(T_{X_{1}}(1)\), \(T_{X_{2}}(1)\), and \(T_{X_{3}}(1)\) given in Theorem 2. Here \(T_{X_{i}}(1)\) contains all possible path information from \(X_{i}\) to all its descendants while \(T_{X_{i}}(0)\) denotes the vertex is 'close' which will not be reached at the current path. Theorem 2 shows that for each \(T_{X_{i}}(1)\), it enumerates all possible combinations of 'open' and 'close' of the children of \(X_{i}\) recursively. For example, in terms \(\mbox{\includegraphics[width=14.226378pt]{T2}}\), we have \(X_{2}\) 'close' and \(X_{3}\) 'open' such that only \(z_{3}\) appears in this term and we obtain \(\alpha_{1,3}(1-\alpha_{1,2})z_{1}z_{3}\). Formally, we provide the graphical implication of the closed-form solution of PGF as follows:

**Proposition 1** (Graphical implication of closed-form solution of PGF).: _Given a random vector \(\mathbf{X}=[X_{1},...,X_{d}]^{T}\) following PB-SCM, for any subgraph \(G(\mathbf{L},\mathbf{E})\) with the subset of vertices \(\mathbf{L}\subseteq\mathbf{V}\) such that the \(i\)-th vertex is the root vertex in \(G(\mathbf{L},\mathbf{E})\), a component \(Cz_{i}\prod_{j\in L\setminus\{i\}}z_{j}^{p_{j}}\) with constant \(C\neq 0\), exponent \(p_{j}\in\mathbb{Z}^{+}\) exits in \(T_{X_{i}}(1)\), if and only if there exists a subgraph \(G(\mathbf{L},\mathbf{E}^{\prime})\) with subset of the edges \(\mathbf{E}^{\prime}\subseteq\mathbf{E}\) such that for each \(j\in\mathbf{L}\setminus\{i\}\), there are at least \(p_{j}\) directed paths from \(X_{i}\) to \(X_{j}\) in the subgraph \(G(\mathbf{L},\mathbf{E}^{\prime})\)._

Proposition 1 reveals the graphical implication of each term in the closed-form solution of PGF that each term is related to a number of directed paths in a certain subset graph. In particular, for thehighest order, in terms 4, \(z_{1}z_{2}z_{3}^{2}\) indicates that there are two paths from \(X_{1}\) to \(X_{3}\) and one path from \(X_{1}\) to \(X_{2}\) in this triangular structure, which implies that \(X_{1},X_{2}\) is the cause of \(X_{3}\) offering a way to identify the causal direction by detecting whether \(z_{3}^{2}\) exists. In fact, the cumulant-based method is exactly the method that identifies the causal direction by detecting the highest order of \(z_{i}\) using cumulant (Qiao et al. (2024)).

However, one drawback of detecting the highest order is that the highest order of \(z_{i}\) does not always contain full identification for PB-SCM. For example, in Fig. 1 (a), since \(z_{1}z_{2}z_{4}^{2},z_{2}z_{3}z_{4}^{2}\) has the order of 2 while \(z_{1}z_{2},z_{2}z_{3}\) only has an order of 1, we can only conclude the causal direction from \(X_{1},X_{2},X_{3}\) to \(X_{4}\) but not causal relation among \(X_{1},X_{2},X_{3}\) using merely the highest order. Fortunately, the causal structure \(X_{1}\to X_{2}\gets X_{3}\) remains identifiable since the term \(z_{1}z_{2}z_{3}\) does not exist in the PGF. This indicates that there is no directed path starting from any vertex that passes through all three vertices \(X_{1},X_{2},X_{3}\). In this case, we can conclude that it must be the collider structure and allows us to identify the causal structure by detecting whether such a term exists. Another drawback of detecting the highest order term is that it requires multiple derivatives of PGF or complex construction with higher order cumulant, hindering the analysis of the identifiability. In this work, benefiting the local property of the PGF, we explore a consistent way to identify the causal structure while only lower-order information is required.

### Local Probability Generating Function

One of the attractive properties of PGF is the ability to isolate and examine some specific components by strategically setting the other variables \(z\) approach zero within the probability generating function, which allows us to investigate the local structure and devise a practical structure learning algorithm. Formally, we define the following local PGF:

**Definition 3** (Local Probability Generating Function).: _Given random vector \(\mathbf{X}=[X_{1},...,X_{d}]^{T}\) following PB-SCM and its PGF \(G_{\mathbf{X}}(\mathbf{z})\). The local PGF of vertices \(\mathbf{L}\subset[d]\) is the pointwise limit of \(G_{\mathbf{X}}(\mathbf{z})\) as the \(\mathbf{z}_{\setminus\mathbf{L}}\) approach \(\mathbf{0}\), \(\mathbf{z}_{\setminus\mathbf{L}}=\{z_{i}|i\in[d]\setminus\mathbf{L}\}\), denoted as: \(G_{\mathbf{X}}^{\mathbf{L}}(\mathbf{z})=\lim_{\mathbf{z}_{\setminus\mathbf{L }}\to\mathbf{0}}G_{\mathbf{X}}(\mathbf{z})\)._

The local PGF can be constructed using the original PGF by setting the limit of \(z\). By this, similarly to Theorem 2, we have the following closed-form solution for the local PGF:

**Theorem 3**.: _Given a random vector \(\mathbf{X}=[X_{1},...,X_{n}]^{T}\) following PB-SCM and its PGF \(G_{\mathbf{X}}(\mathbf{z})\). For a subset \(\mathbf{L}\subset[d]\) of the set of vertices, and the set of the children of vertex \(i\) within \(\mathbf{L}\), denoted by \(\text{Ch}_{\mathbf{L}}(i)=\text{Ch}(i)\cap\mathbf{L}\), the local PGF of \(\mathbf{L}\) can be expressed by:_

\[G_{\mathbf{X}}^{\mathbf{L}}(\mathbf{z})=\prod_{i\in\mathbf{L}}\exp\left\{\mu_ {i}\times\left(T_{X_{i}}^{\mathbf{L}}(1)-1\right)\right\}\prod_{i\in[d] \setminus\mathbf{L}}\exp\{-\mu_{i}\},\] (6)

_where \(T_{X_{i}}^{\mathbf{L}}(1)=z_{i}\sum\limits_{\mathbf{s}\in\{0,1\}^{\{\text{Ch} _{\mathbf{L}}(i)\}}}\prod\limits_{j\in\text{Ch}_{\mathbf{L}}(i)}\alpha_{i,j}^{ s_{j}}T_{X_{j}}^{\mathbf{L}}(s_{j})\prod\limits_{j\in\text{Ch}(i)\setminus \text{Ch}_{\mathbf{L}}(i)}(1-\alpha_{i,j})\) and \(T_{X_{i}}^{\mathbf{L}}(0)=1\)._

Taking Fig. 3 as an example, the local PGF of vertices \(\mathbf{L}=\{X_{2},X_{3}\}\) is \(G_{\mathbf{X}}^{\{X_{2},X_{3}\}}=\exp\{-\mu_{1}\}\exp\{\mu_{2}[(1-\alpha_{2,3} )z_{2}+\alpha_{2,3}z_{2}z_{3}]-\mu_{2}\}\exp\{\mu_{3}z_{3}-\mu_{3}\}\). Such local probability generating function allow us to analysis and to identify the local structure, e.g., the term \(z_{2}z_{3}\) in the local PGF \(G_{\mathbf{X}}^{\{X_{2},X_{3}\}}\) represent the adjacent relation between \(X_{2}\) and \(X_{3}\).

In the following sections, we will further explore the identifiability of several specific structures that serve as fundamental local structures within a graph.

### Identifiability

In this section, we address the identifiability of the PB-SCM using the PGF. Our focus is on identifying three fundamental local structures to reconstruct the causal graph from causal skeleton to causal direction: (i) the adjacent relation, (ii) the local triangular structure, and (iii) the local collider structure.

**Adjacent relation.** We first address the identifiability of the adjacent relation, which is involved in identifying the causal skeleton. For each pair of vertices \(X_{i},X_{j}\in\mathbf{X}\), the component \(z_{i}z_{j}\) appears in the PGF if and only if \(X_{i}\) and \(X_{j}\) are adjacent, indicating that there exists a path either from \(X_{i}\) to \(X_{j}\) or from \(X_{j}\) to \(X_{i}\). Therefore, we can detect the adjacency relation by testing whether the second partial derivative \(\frac{\partial^{2}\log G_{\mathbf{X}}^{\{i,j\}}(\mathbf{z})}{\partial z_{i}\partial z _{j}}\) equal to zero in a local structure. In order to construct a hypothesis test for such a condition, we formulate the condition as a rank condition. Formally, the adjacent relation can be identified using the following theorem:

**Theorem 4** (Identifiability of adjacent vertices).: _Let \(X_{i},X_{j}\in\mathbf{X}\) be two arbitrary vertices with the corresponding local PGF \(G_{\mathbf{X}}^{\{i,j\}}(\mathbf{z})\). Define the matrix \(\mathbf{A}^{\{i,j\}}=\begin{pmatrix}G_{\mathbf{X}}^{\{i,j\}}(\mathbf{z})& \frac{\partial G_{\mathbf{X}}^{\{i,j\}}(\mathbf{z})}{\partial z_{i}}\\ \frac{\partial G_{\mathbf{X}}^{\{i,j\}}(\mathbf{z})}{\partial z_{j}}&\frac{ \partial^{2}G_{\mathbf{X}}^{\{i,j\}}(\mathbf{z})}{\partial z_{i}\partial z_{ j}}\end{pmatrix}\) with \(z_{i},z_{j}\) approach \(1\), the condition \(\mathrm{Rank}(\mathbf{A}^{\{i,j\}})=1\) if and only if \(X_{i}\) is non-adjacent to \(X_{j}\)._

Intuitively, Theorem 4 identify the adjacent relation by detecting the existence of \(z_{i}z_{j}\) in the logarithm of the PGF, i.e., the \(T_{X_{i}}(i)\) inside the exponential function. By this, the identification of the causal skeleton is given. Next, we show the identifiability of a local triangular structure.

**Triangular structure.** For any three vertices \(X_{i},X_{j},X_{k}\in\mathbf{X}\) forming a triangular structure. Such a structure must have one and only one vertex with an in-degree of \(2\). This asymmetry is captured by the second-order derivative of local PGF, e.g., \(z_{i}z_{j}z_{k}^{2}\) where \(X_{k}\) has an in-degree of \(2\). By this, the causal direction \(X_{i}\to X_{k}\) and \(X_{j}\to X_{k}\) in the local triangular structure is identifiable as follows:

**Theorem 5** (Identifiability of local triangular structure).: _Let \(X_{i},X_{j},X_{k}\in\mathbf{X}\) form a triangular structure with the corresponding local PGF \(G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})\). Define the matrix \(\mathbf{B}^{\{i,j,k\}}=\begin{pmatrix}G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})& \frac{\partial G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})}{\partial z_{k}}\\ \frac{\partial G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})}{\partial z_{k}}&\frac{ \partial^{2}G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})}{\partial z_{k}^{2}}\end{pmatrix}\) with \(z_{i},z_{j},z_{k}\) approach \(1\), the condition \(\mathrm{Rank}(\mathbf{B}^{\{i,j,k\}})=2\) if and only if \(X_{k}\) is the vertex with an in-degree of \(2\) in this triangular structure, i.e., \(X_{i}\widehat{\cdots X_{k}}\widehat{\cdots X_{j}}\)._

Note that the causal direction in the triangular structure is not fully identifiable because it does not exhibit asymmetry and can always construct an equivalent PGF in the reversed direction which is also discussed in Qiao et al. (2024). Next, we consider the identifiability of the local collider structure, which is also referred to as the unshielded collider structure.

**Local collider structure.** Given three adjacent vertices \(X_{i}-X_{j}-X_{k}\). If they form a collider structure \(X_{i}\to X_{j}\gets X_{k}\), the corresponding pattern in the closed-form solution of PGF is \(z_{i}z_{j}+z_{j}z_{k}\) but no \(z_{i}z_{j}z_{k}\), reflecting the absence of non-block path among these three vertices. Thus we have \(\frac{\partial^{2}\log G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})}{\partial z_{i} \partial z_{k}}=0\). Similarly, we can construct a rank condition for identifying such local collider structure using the following theorem:

**Theorem 6** (Identifiability of local collider structure).: _Let \(X_{i},X_{j},X_{k}\in\mathbf{X}\) be three adjacent vertices with the corresponding local PGF \(G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})\). Define the matrix \(\mathbf{C}^{\{i,j,k\}}=\begin{pmatrix}G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})& \frac{\partial G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})}{\partial z_{i}}\\ \frac{\partial G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})}{\partial z_{k}}&\frac{ \partial^{2}G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})}{\partial z_{i}\partial z_{ k}}\end{pmatrix}\) with \(z_{i},z_{j},z_{k}\) approach \(1\), the condition \(\mathrm{Rank}(\mathbf{C}^{\{i,j,k\}})=1\) if and only if the vertices \(X_{i},X_{j},X_{k}\) form the structure \(X_{i}\to X_{j}\gets X_{k}\)._

Combining the identifiability in Theorem 5 and Theorem 6 of the local structure, we conclude the following graphical implication of the identification of PB-SCM.

**Theorem 7** (Graphical implication of identifiability).: _Given a pair of adjacent vertices \(X_{i},X_{j}\in\mathbf{X}\) following PB-SCM, the causal direction of \(X_{i}\to X_{j}\) is identifiable if there exists a vertex \(X_{k}\in\mathbf{X}\setminus\{X_{i},X_{j}\}\) such that \(X_{k}\to X_{j}\)._

### Learning Causal Structure of PB-SCM Using Probability Generating Function

In this section, we propose a practical algorithm for learning the causal structure of PB-SCM using PGF. Note that the probability generating function can be estimated by employing the empirical probability generating function (Nakamura and Perez-Abreu (1993)). The algorithm is given in Alg. 1. Our algorithm involves two steps: learning the skeleton of DAG \(G\) using the result developed in Theorem 4 and inferring the causal direction using the results developed in Theorem 5 and 6.

**Learning Causal Skeleton.** To learn the causal skeleton, following the Theorem 4, we construct the matrix \(\mathbf{A}^{\{i,j\}}\) for each pair of vertices \(X_{i},X_{j}\in\mathbf{X}\), and assess its rank to determine adjacency between the vertices (Line 2-4). Notably, learning the causal skeleton is efficient because each pair of vertices requires only one examination.

**Learning Causal Direction.** Given the skeleton, we orient the causal direction following the Theorem 5 and Theorem 6. Our initial focus is on orienting within triangular structures. For each triangular structure, we enumerate the matrix \(\mathbf{B}^{\{i,j,k\}}\) to test whether \(X_{k}\) has indegree of two based on Theorem 5 and orient \(X_{i}\to X_{k}\) and \(X_{j}\to X_{k}\) if detected (Line 5-7). After the orientation in the triangular structure, we focus on orienting the remaining undirected edges following Theorem 6. Specifically, for each undirected edge \(X_{i}-X_{j}\), we first consider testing the causal direction in a pattern like \(X_{i}-X_{j}\gets X_{k}\). Then, we consider testing the causal direction in pattern \(X_{i}-X_{j}-X_{k}\). Such a test is conducted by constructing the matrix \(\mathbf{C}^{\{i,j,k\}}\) and to test whether the rank is \(1\), and then orient \(X_{i}\to X_{j}\gets X_{k}\) if detected (Line 8-10).

**Rank Test.** To test the rank of \(\mathbf{A}^{\{i,j\}},\mathbf{B}^{\{i,j,k\}}\) and \(\mathbf{C}^{\{i,j,k\}}\), we employ a rank test by testing whether the second (minimum) eigenvalue \(\lambda_{2}\) is zero, i.e., \(H_{0}:\lambda_{2}=0\)\(v.s.\)\(H_{1}:\lambda_{2}\neq 0\). Since the trace of the matrix converges to a normal distribution based on the central limit theorem, and thus the sum of eigenvalues is also Gaussian. Thus, if \(H_{0}\) is true, we approximate the eigenvalue as a normal distribution with zero mean. We further employ bootstrap method (Efron and Tibshirani (1994)) to estimate the variance of such distribution by calculating the bootstrapping statistic \(\lambda_{2}^{+}\) from \(N\) resampling dataset \(\mathcal{D}^{+}\in\{\mathcal{D}^{+}_{i}|\mathcal{D}^{+}_{i=1,..,N}\subset \mathcal{D},\}\) and estimate the variance of \(\lambda_{2}^{+}\). Building on this, the p-value of \(\lambda_{2}\) from the original dataset can be obtained.

**Complexity Analysis.** In the step of learning skeleton, we determine whether each pair of vertices is adjacent by testing the rank of the matrix following Theorem 4, and hence the complexity of skeleton learning is \(\mathcal{O}(\frac{1}{2}d(d-1))\). In the step of learning causal direction, we consider the complete graph, where there are \(\binom{d}{3}=\frac{d(d-1)(d-2)}{6}\) triangular structures, and for each triangular structures, we test the rank of the matrix for three vertices following Theorem 5, and hence the complexity is \(\mathcal{O}(\frac{d(d-1)(d-2)}{2})\). Similarly, in orienting collider structures from a complete graph, we choose three adjacent vertices and test the rank once following Theorem 6, hence the complexity is \(\mathcal{O}(\frac{d(d-1)(d-2)}{6})\). Therefore, the total complexity is \(\mathcal{O}(\frac{1}{2}d(d-1)+\frac{2d(d-1)(d-2)}{3})\).

## 5 Experiment

### Synthetic Experiments

In this section, we test our proposed method on synthetic data. We first conduct control experiments on synthetic data to evaluate the sensitivity of our method to sample size and different indegree rates. Following this, we present case studies involving 3, 4, and 5 vertices to further illustrate the identifiability of our approach. The baseline methods include the cumulant-based method (Cumulant) (Qiao et al. (2024)), OCD (Ni and Mallick (2022)), PC (Spirtes et al. (2000)), GES (Chickering (2002)).

Sensitivity ExperimentIn the sensitivity experiment, we synthesize data with fixed parameters while traversing the target parameter. The default settings are as follows, sample size=30000, number of vertices=10, indegree rate=3.0, range of causal coefficient \(\alpha_{i,j}\in[0.1,0.3]\), range of the mean of Poisson noise \(\mu_{i}\in[0.05,0.15]\). Each simulation is repeated 15 times.

In the control experiments on the average in-degree given in Table 1, as the average in-degree controls the sparse of causal structure, the higher the in-degree rate, the less sparse in causal structure leading to a decrease of performance of the baseline methods, PC, GES and OCD. In contrast, our method and Cumulant show improved performance as they benefit from the denser structure which provides more identifiable structures. Additionally, our method outperforms Cumulant by overcoming identifiability limitations through leveraging PGF, demonstrating better performance.

In the control experiments on sample size presented in Table 2, our method's performance improves with increasing sample sizes, consistently outperforming all baseline methods. Furthermore, it surpasses Cumulant under the same conditions due to its efficient identification of directions in local structures without relying on high-order statistics, thus enhancing accuracy.

Case StudyTo demonstrate the identifiability of the proposed PGF-based method, we present case studies using causal graphs with 3, 4, and 5 vertices. The results of our method and the baseline methods are summarized in Table 3.

Generally, our PGF-based method successfully identifies almost all the edges, except for \(X_{1}\to X_{2}\) in the graphs with 3 and 5 vertices. This is because there is neither a vertex with an indegree of 2 to provide causal asymmetry, nor an additional vertex to form the local collider structure, which aligns with our theoretical results. Notably, the cumulant-based method fails to identify the structure \(X_{1}\to X_{2}\gets X_{3}\) in the 4-vertex graph and the edge \(X_{4}\to X_{5}\) in the 5-vertex graph, as it cannot leverage low-order information. This limitation, however, is successfully addressed by the PGF-based method, which can utilize such information effectively. Regarding other baseline methods, PC and GES encounter difficulties in identifying the Markov equivalent class, while OCD illustrates identifiability issues in PB-SCM.

\begin{table}
\begin{tabular}{c|c c c c c c c c} \hline \multicolumn{6}{c}{F1\(\uparrow\)} & \multicolumn{6}{c}{SHD\(\downarrow\)} \\ \hline Avg. In-degree & 2.0 & 2.5 & 3.0 & 3.5 & 2.0 & 2.5 & 3.0 & 3.5 \\ \hline Ours & \(\mathbf{0.74\pm 0.05}\) & \(\mathbf{0.81\pm 0.07}\) & \(\mathbf{0.86\pm 0.03}\) & \(\mathbf{0.89\pm 0.04}\) & \(\mathbf{9.67\pm 1.99}\) & \(\mathbf{8.33\pm 2.28}\) & \(\mathbf{6.40\pm 1.68}\) & \(\mathbf{5.27\pm 1.39}\) \\ Cumulant & \(0.73\pm 0.03\) & \(0.77\pm 0.02\) & \(0.80\pm 0.04\) & \(0.83\pm 0.03\) & \(13.40\pm 1.28\) & \(14.10\pm 1.51\) & \(13.00\pm 2.37\) & \(13.20\pm 2.23\) \\ PC & \(0.60\pm 0.17\) & \(0.62\pm 0.11\) & \(0.54\pm 0.12\) & \(0.60\pm 0.12\) & \(9.90\pm 3.45\) & \(11.80\pm 2.48\) & \(15.90\pm 3.91\) & \(16.10\pm 3.21\) \\ GCS & \(0.48\pm 0.14\) & \(0.48\pm 0.11\) & \(0.41\pm 0.11\) & \(0.37\pm 0.10\) & \(14.90\pm 4.48\) & \(19.56\pm 4.61\) & \(25.90\pm 4.18\) & \(30.5\pm 4.06\) \\ OCD & \(0.23\pm 0.22\) & \(0.27\pm 0.23\) & \(0.28\pm 0.16\) & \(0.37\pm 0.14\) & \(16.10\pm 3.70\) & \(19.40\pm 5.50\) & \(23.60\pm 4.62\) & \(24.30\pm 4.67\) \\ \hline \end{tabular}
\end{table}
Table 1: Sensitivity to Avg. In-degree Rate.

\begin{table}
\begin{tabular}{c|c c c|c c|c c c} \hline \multicolumn{6}{c}{F1\(\uparrow\)} & \multicolumn{6}{c}{SHD\(\downarrow\)} \\ \hline Sample Size & 5000 & 15000 & 30000 & 50000 & 5000 & 15000 & 30000 & 50000 \\ \hline Ours & \(\mathbf{0.75\pm 0.09}\) & \(\mathbf{0.82\pm 0.04}\) & \(\mathbf{0.86\pm 0.03}\) & \(\mathbf{0.87\pm 0.04}\) & \(\mathbf{11.50\pm 3.43}\) & \(\mathbf{9.27\pm 2.37}\) & \(\mathbf{6.40\pm 1.68}\) & \(\mathbf{5.87\pm 1.25}\) \\ Cumulant & \(0.72\pm 0.04\) & \(0.78\pm 0.02\) & \(0.80\pm 0.04\) & \(0.80\pm 0.03\) & \(19.90\pm 3.35\) & \(15.00\pm 4.11\) & \(13.00\pm 2.49\) & \(13.60\pm 2.63\) \\ PC & \(0.45\pm 0.11\) & \(0.54\pm 0.11\) & \(0.54\pm 0.13\) & \(0.66\pm 0.09\) & \(19.50\pm 4.30\) & \(15.70\pm 3.77\) & \(15.90\pm 4.12\) & \(13.00\pm 2.79\) \\ GES & \(0.39\pm 0.10\) & \(0.44\pm 0.20\) & \(0.41\pm 0.11\) & \(0.43\pm 0.22\) & \(23.70\pm 4.14\) & \(22.70\pm 7.85\) & \(25.90\pm 4.41\) & \(24.10\pm 8.54\) \\ OCD & \(0.30\pm 0.12\) & \(0.35\pm 0.18\) & \(0.28\pm 0.16\) & \(0.38\pm 0.20\) & \(21.90\pm 3.35\) & \(20.80\pm 4.57\) & \(23.60\pm 4.62\) & \(20.90\pm 5.74\) \\ \hline \end{tabular}
\end{table}
Table 2: Sensitivity to Sample Size.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c} \hline Causal Graph & Ours & Cumulant & PC & GES & OCD \\ \hline \(X_{1}\) & \(X_{2}\) & \(X_{3}\) & \(X_{4}\) & \(X_{5}\) & \(X_{6}\) & \(X_{7}\) \\ \hline \(X_{3}\) & \(X_{4}\) & \(X_{5}\) & \(X_{6}\) & \(X_{7}\) & \(X_{8}\) & \(X_{9}\) \\ \hline \(X_{5}\) & \(X_{6}\) & \(X_{7}\) & \(X_{8}\) & \(X_{9}\) & \(X_{1}\) & \(X_{1}\) \\ \hline \(X_{7}\) & \(X_{8}\) & \(X_{9}\) & \(X_{1}\) & \(X_{2}\) & \(X_{3}\) & \(X_{4}\) & \(X_{5}\) \\ \hline \(X_{8}\) & \(X_{9}\) & \(X_{1}\) & \(X_{2}\) & \(

### Real World Experiments

In this section, we evaluate the performance of our proposed method on two real-world datasets to assess its effectiveness in realistic scenarios.

Football Events DatasetWe test the proposed method on a real-world football events dataset1, which includes 941,009 events from 9,074 games across Europe. The experiment focused on analyzing the causal relationships between specific events such as Foul, Yellow card, Second yellow card, Red card, and Substitution, as depicted in Fig. 4. The goal is to identify causal relationships from the counts of these events.

Footnote 1: https://www.kaggle.com/datasets/secareanualin/football-events

In detail, we orientate in the local triangular structures \(Y_{1}-Y_{2}-F\), \(Y_{2}-R-F\), and the local collider structure \(Y_{1}-S-R\). As a result, Our method successfully identifies adjacent vertex \(\text{Foul}-\text{First Yellow Card}\) and other six causal directions, which is consistent with our theoretical result.

Shopping Mall Paid Search Campaign Count DataWe also evaluate our method using a shopping small paid search campaign dataset2. This dataset contains information from a five-month paid search campaign for a U.S. shopping mall, spanning from July to November 2021. In this analysis, we focus on the count variables _Impressions_, _Clicks_, and _Conversions_, which represent fundamental count data in e-commerce scenarios. These variables exhibit the following causal relationships: _Impressions \(\rightarrow\) Clicks_, _Clicks \(\rightarrow\) Convers_, and _Impressions \(\rightarrow\) Convers_, forming a triangular structure.

Footnote 2: https://www.kaggle.com/datasets/marceak182/shopping-mall-paid-search-campaign-dataset

Our experimental results demonstrate that the proposed method successfully identifies the adjacent relation between _Impressions_ and _Clicks_, and the other two causal directions. These results align with our theoretical findings, suggesting that the method has the potential to be applied to real-world scenarios involving count data.

## 6 Conclusion

In this work, we investigate the identifiability of the Poisson branching structural causal model using the probability generating function. We derive a nontrivial closed-form solution for the PGF of PB-SCM and further establish the connection between the closed-form solution of PGF to the causal structure, showing that the closed-form solution of PGF encompasses the path information with various subgraphs. With this connection, we employ the local property of PGF and propose a simple yet efficient way to identify the local causal structure of PB-SCM by constructing a matrix with rank test. By this, we provide a practical algorithm and a hypothesis test approach for testing the causal structure and verifying the effectiveness of the algorithm via synthesis and real-world data. The proposed theoretical results take a meaningful step in understanding the causal mechanism and completing the identifiability result of PB-SCM.

The main limitation of this work is that the explicit estimation of the probability generating function does not scale well to the number of nodes. Developing a sample-efficient estimating method could be a promising direction. In addition, causal faithfulness and sufficient assumption may restrict the usage of this work in a border scenario, which needs to be processed with extra detection steps to eliminate the effect.

## 7 Acknowledgments

This research was supported in part by National Key R&D Program of China (2021ZD0111501), National Science Fund for Excellent Young Scholars (62122022), Natural Science Foundation of China (61876043, 61976052, 62406080), the major key project of PCL (PCL2021A12). We sincerely appreciate the comments from anonymous reviewers, which greatly helped to improve the paper.

Figure 4: Football Event Graph (\(F\): Foul, \(Y_{1}\): Yellow card, \(Y_{2}\): Second yellow card, \(R\): Red card, \(S\): Substitution)

## References

* Al-Osh and Alzaid (1987) Mohamed A Al-Osh and Aus A Alzaid. First-order integer-valued autoregressive (INAR (1)) process. _Journal of Time Series Analysis_, 8(3):261-275, 1987.
* Cai et al. (2018a) Ruichu Cai, Jie Qiao, Kun Zhang, Zhenjie Zhang, and Zhifeng Hao. Causal discovery from discrete data using hidden compact representation. _Advances in neural information processing systems_, 31, 2018a.
* Cai et al. (2018b) Ruichu Cai, Jie Qiao, Zhenjie Zhang, and Zhifeng Hao. Self: structural equational likelihood framework for causal discovery. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 32, 2018b.
* Cai et al. (2022) Ruichu Cai, Siyu Wu, Jie Qiao, Zhifeng Hao, Keli Zhang, and Xi Zhang. THPs: Topological Hawkes Processes for Learning Causal Structure on Event Sequences. _IEEE Transactions on Neural Networks and Learning Systems_, 2022.
* Chickering (2002) David Maxwell Chickering. Optimal structure identification with greedy search. _Journal of machine learning research_, 3(Nov):507-554, 2002.
* Choi et al. (2020) Junsouk Choi, Robert Chapkin, and Yang Ni. Bayesian causal structural learning with zero-inflated poisson bayesian networks. _Advances in neural information processing systems_, 33:5887-5897, 2020.
* Efron and Tibshirani (1994) Bradley Efron and Robert J Tibshirani. _An introduction to the bootstrap_. Chapman and Hall/CRC, 1994.
* Figueiredo and Oliveira (2023) Mario A. T. Figueiredo and Catarina Oliveira. Distinguishing cause from effect on categorical data: The uniform channel model. In Mihaela van der Schaar, Cheng Zhang, and Dominik Janzing, editors, _Proceedings of the Second Conference on Causal Learning and Reasoning_, volume 213 of _Proceedings of Machine Learning Research_, pages 122-141. PMLR, 11-14 Apr 2023. URL https://proceedings.mlr.press/v213/figueiredo23a.html.
* Glymour et al. (2019) Clark Glymour, Kun Zhang, and Peter Spirtes. Review of causal discovery methods based on graphical models. _Frontiers in genetics_, 10:524, 2019.
* Johnson et al. (2005) Norman L Johnson, Adrienne W Kemp, and Samuel Kotz. _Univariate discrete distributions_, volume 444. John Wiley & Sons, 2005.
* Leonelli and Varando (2023) Manuele Leonelli and Gherardo Varando. Context-specific causal discovery for categorical data using staged trees. In _International conference on artificial intelligence and statistics_, pages 8871-8888. PMLR, 2023.
* McKenzie (1985) Ed McKenzie. Some simple models for discrete variate time series 1. _JAWRA Journal of the American Water Resources Association_, 21(4):645-650, 1985.
* Nakamura and Perez-Abreu (1993) Miguel Nakamura and Victor Perez-Abreu. Empirical probability generating function: An overview. _Insurance: Mathematics and Economics_, 12(3):287-295, 1993.
* Ni (2022) Yang Ni. Bivariate causal discovery for categorical data via classification with optimal label permutation. _Advances in neural information processing systems_, 35:10837-10848, 2022.
* Ni and Mallick (2022) Yang Ni and Bani Mallick. Ordinal causal discovery. In _Uncertainty in Artificial Intelligence_, pages 1530-1540. PMLR, 2022.
* Park and Raskutti (2015) Gunwoong Park and Garvesh Raskutti. Learning large-scale poisson dag models based on overdispersion scoring. _Advances in neural information processing systems_, 28, 2015.
* Park and Raskutti (2017) Gunwoong Park and Garvesh Raskutti. Learning Quadratic Variance Function (QVF) DAG Models via OverDispersion Scoring (ODS). _Journal of Machine Learning Research_, 18(224):1-44, 2017.
* Pearl (2009) Judea Pearl. _Causality_. Cambridge university press, 2009.
* Peters et al. (2010) Jonas Peters, Dominik Janzing, and Bernhard Scholkopf. Identifying cause and effect on discrete data using additive noise models. In _Proceedings of the thirteenth international conference on artificial intelligence and statistics_, pages 597-604. JMLR Workshop and Conference Proceedings, 2010.
* Park and Raskutti (2015)Jie Qiao, Ruichu Cai, Siyu Wu, Yu Xiang, Keli Zhang, and Zhifeng Hao. Structural hawkes processes for learning causal structure from discrete-time event sequences. In _Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-23_, pages 5702-5710, 8 2023. doi: 10.24963/ijcai.2023/633. URL https://doi.org/10.24963/ijcai.2023/633.
* Qiao et al. (2024a) Jie Qiao, Zhengming Chen, Jianhua Yu, Ruichu Cai, and Zhifeng Hao. Identification of causal structure in the presence of missing data with additive noise model. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 20516-20523, 2024a.
* Qiao et al. (2024b) Jie Qiao, Yu Xiang, Zhengming Chen, Ruichu Cai, and Zhifeng Hao. Causal discovery from poisson branching structural causal model using high-order cumulant with path analysis. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 20524-20531, 2024b.
* Spirtes et al. (1995) Peter Spirtes, Christopher Meek, and Thomas Richardson. Causal inference in the presence of latent variables and selection bias. In _Proceedings of the Eleventh conference on Uncertainty in artificial intelligence_, pages 499-506, 1995.
* Spirtes et al. (2000) Peter Spirtes, Clark N Glymour, and Richard Scheines. _Causation, prediction, and search_. MIT press, 2000.
* Steutel and van Harn (1979) Fred W Steutel and Klaas van Harn. Discrete analogues of self-decomposability and stability. _The Annals of Probability_, pages 893-899, 1979.
* Tsamardinos et al. (2006) Ioannis Tsamardinos, Laura E Brown, and Constantin F Aliferis. The max-min hill-climbing Bayesian network structure learning algorithm. _Machine learning_, 65:31-78, 2006.
* Weiss (2018) Christian H Weiss. _An introduction to discrete-valued time series_. John Wiley & Sons, 2018.
* Weiss and Kim (2014) Christian H Weiss and Hee-Young Kim. Diagnosing and modeling extra-binomial variation for time-dependent counts. _Applied Stochastic Models in Business and Industry_, 30(5):588-608, 2014.
* Wiuf and Stumpf (2006) Carsten Wiuf and Michael PH Stumpf. Binomial subsampling. _Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences_, 462(2068):1181-1195, 2006.
* Zhang et al. (2018) Kun Zhang, Bernhard Scholkopf, Peter Spirtes, and Clark Glymour. Learning causality and causality-related learning: some recent progress. _National science review_, 5(1):26-29, 2018.

## Appendix A Proof of Closed-Form Solution for PGF of PB-SCM

### Proof of Theorem 1

**Theorem 1** (Closed form of PGF within PB-SCM).: _Given a random vector \(\mathbf{X}=[X_{1},...,X_{n}]^{T}\) following PB-SCM, let \(\mathbf{z}_{(j)}=\{z_{i}|l\in\text{Des}(j)\cup\{j\}\}\), the PGF of \(P(\mathbf{X})\) is given by \(G_{\mathbf{X}}(\mathbf{z})=\prod_{i\in[d]}G_{\epsilon_{i}}\Big{(}z_{i}\times \prod_{j\in Ch(i)}G_{i,j}(\mathbf{z}_{(j)})\Big{)}\), where_

\[G_{i,j}(\mathbf{z}_{(j)})=\begin{cases}G_{B(\alpha_{i,j})}\left(z_{j}\times \prod_{k\in Ch(j)}G_{j,k}(\mathbf{z}_{(k)})\right)&,\text{Ch}(j)\neq\emptyset \\ G_{B(\alpha_{i,j})}(z_{j})&,\text{Otherwise}\end{cases},\] (A.1)

_in which \(G_{\epsilon_{i}}(\cdot)\) is the PGF of Poisson noise \(\epsilon_{i}\) and \(G_{B(\alpha_{i,j})}(\cdot)\) is the PGF of Bernoulli distribution with parameter \(\alpha_{i,j}\)._

#### a.1.1 An Illustrative Example for Deriving the Closed Form

Before formal proof, we first provide an intuition of proof through a detailed example. Deriving the closed form of a multivariate PGF involves decomposing the expectation using the law of total expectation. This ensures that each decomposed expectation involves only one random variable, each of which has a closed-form PGF, as most univariate PGFs admit a closed form. We first introduce some closed forms of univariate PGFs used in the proof. Specifically, the univariate PGFs of the Poisson and Bernoulli distributions are given by: \(G_{\epsilon}(z)=\mathbb{E}[z^{\epsilon}]=\exp\{\mu(z-1)\}\) when \(\epsilon\sim\mathrm{Poisson}(\mu)\) and \(G_{B(\alpha)}(z)=\mathbb{E}[z^{x}]=1-\alpha+\alpha z\) where \(x\sim B(\alpha)\), the Bernoulli distribution with parameter \(\alpha\).

Consider the triangular structure in Fig. 5, the corresponding PGF is expressed as follows:

\[G_{\mathbf{X}}(\mathbf{z})=\mathbb{E}\left[z_{1}^{X_{1}}z_{2}^{X_{2}}z_{3}^{X_ {3}}\right]=\mathbb{E}\left[z_{1}^{\epsilon_{1}}z_{2}^{\alpha_{1,2}\circ \epsilon_{1}+\epsilon_{2}}z_{3}^{\alpha_{2,3}\circ\alpha_{1,2}\circ\epsilon_{1 }+\alpha_{2,3}\circ\epsilon_{2}+\epsilon_{3}}\right].\] (A.2)Since PB-SCM is a kind of additive noise model, we can represent each variable by the noise of ancestors with the corresponding coefficient along with the causal path. Next, since noises are independent of each other, by rearranging the components that share the same noise, we can further decompose the expectation:

\[G_{\mathbf{X}}(\mathbf{z})=\mathbb{E}\left[z_{1}^{\epsilon_{1}}z_{2}^{\alpha_{1, 2}\circ\epsilon_{1}}z_{3}^{\alpha_{1,3}\circ\epsilon_{1}}z_{3}^{\alpha_{2,3} \circ\alpha_{1,2}\circ\epsilon_{1}}\right]\mathbb{E}\left[z_{2}^{\epsilon_{2}} z_{3}^{\alpha_{2,3}\circ\epsilon_{2}}\right]\mathbb{E}\left[z_{3}^{\epsilon_{3}} \right].\] (A.3)

This form reveals the underlying causal mechanism, where the power of \(z_{j}\) in the expectation involving \(z_{i}^{\epsilon_{i}}\) is the noise component from \(X_{i}\) to \(X_{j}\) with the corresponding path coefficients. That is, each expectation captures the noise's potential influences, along with their respective causal paths.

To analyze this expression more closely, we focus on the first expectation involving \(z_{1}^{\epsilon_{1}}\) in Eq. A.3. By applying the law of total expectation, we decompose this expectation, ensuring that each decomposed inner expectation depends on only one variable:

\[\mathbb{E}\left[z_{1}^{\epsilon_{1}}z_{2}^{\alpha_{1,2}\circ \epsilon_{1}}z_{3}^{\alpha_{1,3}\circ\epsilon_{1}}z_{3}^{\alpha_{2,3}\circ \alpha_{1,2}\circ\epsilon_{1}}\right]=\mathbb{E}\left[z_{1}^{\epsilon_{1}} \mathbb{E}\left[z_{3}^{\alpha_{1,3}\circ\epsilon_{1}}|\epsilon_{1}\right] \mathbb{E}\left[z_{2}^{\alpha_{1,2}\circ\epsilon_{1}}z_{3}^{\alpha_{2,3} \circ\alpha_{1,2}\circ\epsilon_{1}}|\epsilon_{1}\right]\right]\] (A.4) \[=\mathbb{E}\left[z_{1}^{\epsilon_{1}}\mathbb{E}\left[z_{3}^{\alpha _{1,3}\circ\epsilon_{1}}|\epsilon_{1}\right]\mathbb{E}\left[z_{2}^{\alpha_{1, 2}\circ\epsilon_{1}}\mathbb{E}\left[z_{3}^{\alpha_{2,3}\circ\alpha_{1,2} \circ\epsilon_{1}}|\alpha_{1,2}\circ\epsilon_{1}\right]|\epsilon_{1}\right] \right].\]

An immediate observation from Eq. A.4 is that, to establish conditional independence and decompose the expectation, we condition the variables following the causal order (\(\epsilon_{1}\) corresponds to \(X_{1}\) and \(\alpha_{1,2}\circ\epsilon_{1}\) corresponds to \(X_{2}\)), thereby encapsulating the graph structure within the PGF.

Next, each decomposed expectation involves only one variable, e.g., \(\mathbb{E}\left[z_{3}^{\alpha_{1,3}\circ\epsilon_{1}}|\epsilon_{1}\right]\) is the PGF of \(\alpha_{1,3}\circ\epsilon_{1}|\epsilon_{1}\sim\mathrm{Binomial}(n=X,p=\alpha)\), has a closed form PGF \(G_{B(\alpha_{1,3})}(z_{3})^{\epsilon_{1}}\). Similar to other expectations, we can finally obtain the closed form of \(G_{\mathbf{X}}(\mathbf{z})\):

\[G_{\mathbf{X}}(\mathbf{z}) =\mathbb{E}\left[\left[z_{1}\times G_{B(\alpha_{1,3})}(z_{3}) \right]^{\epsilon_{1}}\times\mathbb{E}\left[\left[z_{2}\times G_{B(\alpha_{2, 3})}(z_{3})\right]^{\alpha_{1,2}\circ\epsilon_{1}}|\epsilon_{1}\right]\right]\] (A.5) \[=\mathbb{E}\left[\left[z_{1}\times G_{B(\alpha_{1,3})}(z_{3}) \times G_{B(\alpha_{1,2})}(z_{2}\times G_{B(\alpha_{2,3})}(z_{3}))\right]^{ \epsilon_{1}}\right]\] \[=G_{\epsilon_{1}}(z_{1}\times G_{B(\alpha_{1,3})}(z_{3})\times G _{B(\alpha_{1,2})}(z_{2}\times G_{B(\alpha_{2,3})}(z_{3}))).\]

Note that, \(\mathbb{E}\left[\left[z_{2}\times G_{B(\alpha_{2,3})}(z_{3})\right]^{\alpha_{1,2}\circ\epsilon_{1}}|\epsilon_{1}\right]\) is a PGF of \(\alpha_{1,2}\circ\epsilon_{1}|\epsilon_{1}\) where the input is \(z_{2}\times G_{B(\alpha_{2,3})}(z_{3})\). Similar to this, we have:

\[\mathbb{E}\left[z_{2}^{\epsilon_{2}}z_{3}^{\alpha_{2,3}\circ\epsilon_{2}} \right]=\mathbb{E}\left[z_{2}^{\epsilon_{2}}\mathbb{E}\left[z_{3}^{\alpha_{2,3} \circ\epsilon_{2}}|\epsilon_{2}\right]\right]=\mathbb{E}\left[z_{2}^{\epsilon_ {2}}G_{B(\alpha_{2,3})}(z_{3})^{\epsilon_{2}}\right]=G_{\epsilon_{2}}(z_{2}G _{B(\alpha_{2,3})}(z_{3})),\] (A.6)

and \(\mathbb{E}\left[z_{3}^{\epsilon_{3}}\right]=G_{\epsilon_{3}}(z_{3})\). Then the close form of PGF of Fig. 5 is as follows:

\[G_{\mathbf{X}}(\mathbf{z})= G_{\epsilon_{1}}(z_{1}\times G_{B(\alpha_{1,3})}(z_{3})\times G _{B(\alpha_{1,2})}(z_{2}\times G_{B(\alpha_{2,3})}(z_{3})))\] (A.7) \[\times G_{\epsilon_{2}}(z_{2}\times G_{B(\alpha_{2,3})}(z_{3})) \times G_{\epsilon_{3}}(z_{3}).\]

Therefore, to derive the closed form of PGF, we need to apply the law of total expectation following the causal order, which constructs a recursive formula. To formalize this in a recursive formula, we further define some graphical concepts.

**Notation.** We use \(\mathbf{P}^{i\leftrightarrow j}=\left\{P_{k}^{i\leftrightarrow j}\right\}_{k=1}^ {\mathbf{P}^{i\leftrightarrow j}|}\) denotes the set of all directed paths from vertex \(i\) to \(j\), where \(P_{k}^{i\leftrightarrow j}=(i,k_{1},k_{2},...,k_{p},j)\), \(p=|P_{k}^{i\leftrightarrow j}|-2\), denote the \(k\)-th directed path from vertex \(i\) to \(j\). For each directed path \(P_{k}^{i\leftrightarrow j}\), we use \(A_{k}^{i\leftrightarrow j}=(\alpha_{i,k_{1}},\alpha_{k_{1},k_{2}},\ldots,\alpha_{k _{p},j})\) denote the corresponding _coefficients sequence_ of path \(P_{k}^{i\leftrightarrow j}\). We let \(\mathbf{P}^{i\leftrightarrow i}=\left\{P^{i\leftrightarrow i}\right\}\) also be a valid directed path for simplicity. Besides, we use \(A_{k}^{i\leftrightarrow j}\circ X_{i}\coloneqq\alpha_{k_{p},j}\circ\cdots\circ \alpha_{k_{1},k_{2}}\circ\alpha_{i,k_{1}}\circ X_{i}\) denote to perform a consecutive thinning operation on \(X_{i}\) based on the path sequence.

Figure 5: Example triangular structure.

#### a.1.2 Formal Proof

Following the definitions, the PGF of joint distribution \(P(X_{1},...,X_{n})\) under PB-SCM is given by \(G_{\mathbf{X}}(\mathbf{z})=E[z_{1}^{X_{1}}\cdots z_{n}^{X_{n}}]\). Since PB-SCM is a kind of additive noise model, we can represent each variable by the noise of ancestors with the corresponding coefficient sequence along with the directed path:

\[X_{i}=\sum_{h\in\text{An}(i)}\sum_{p=1}^{|P^{h\sim i}|}A_{p}^{h\sim i}\circ \epsilon_{h}+\epsilon_{i},\] (A.8)

where \(\text{An}(i)\) denotes the set of ancestors of vertex \(i\).

Next, by rearranging the components that share the same noise, we can further decompose the expectation as noises are independent of each other:

\[\begin{split} G_{\mathbf{X}}(\mathbf{z})&=\mathbb{ E}\left[z_{1}^{\sum_{h\in\text{An}(i)}\sum_{p=1}^{|P^{h\sim i}|}A_{p}^{h\sim i }\circ\epsilon_{h}+\epsilon_{1}}\times\cdots\times z_{n}^{\sum_{h\in\text{An} (n)}\sum_{p=1}^{|P^{h\sim n}|}A_{p}^{h\sim n}\circ\epsilon_{h}+\epsilon_{n}} \right]\\ &=\mathbb{E}\left[\prod_{i\in[n]}\left[z_{i}^{\epsilon_{i}}\prod _{j\in\text{Des}(i)}\hskip-14.226378ptz_{j}^{\sum_{p=1}^{|P^{i\sim j}|}A_{p}^ {i\sim j}\circ\epsilon_{i}}\right]\right]=\prod_{i\in[n]}\mathbb{E}\left[z_{i}^ {\epsilon_{i}}\prod_{j\in\text{Des}(i)}\prod_{p=1}^{|P^{i\sim j}|}z_{j}^{A_{p }^{i\sim j}\circ\epsilon_{i}}\right],\end{split}\] (A.9)

where \(\text{Des}(i)\) is the descendent set of vertex \(i\).

We start by introducing the following Lemmas.

**Lemma 1**.: _For each \(\mathbb{E}\left[z_{i}^{\epsilon_{i}}\prod_{j\in\text{Des}(i)}\prod_{p=1}^{|P^{ i\sim j}|}z_{j}^{A_{p}^{i\sim j}\circ\epsilon_{i}}\right]\), we have:_

\[\mathbb{E}\left[z_{i}^{\epsilon_{i}}\prod_{j\in\text{Des}(i)} \prod_{p=1}^{|P^{i\sim j}|}z_{j}^{A_{p}^{i\sim j}\circ\epsilon_{i}}\right]=G_{ \epsilon_{i}}\left(z_{i}\prod_{j\in Ch(i)}\mathbb{E}\left[z_{j}^{\xi_{n}^{( \alpha_{i,j})}}\prod_{k\in\text{Des}(j)}\prod_{p=1}^{|P^{j\sim k}|}z_{k}^{A_{p }^{j\sim k}\circ\xi_{n}^{(\alpha_{i,j})}}\right]\right).\] (A.10)

Proof.: We first condition \(\epsilon_{i}\) using the law of total expectation and obtain:

\[\begin{split}\mathbb{E}\left[z_{i}^{\epsilon_{i}}\prod_{j\in \text{Des}(i)}\prod_{p=1}^{|P^{i\sim j}|}z_{j}^{A_{p}^{i\sim j}\circ\epsilon_{ i}}\right]&=\mathbb{E}\left[z_{i}^{\epsilon_{i}}\mathbb{E}\left[ \prod_{j\in\text{Des}(i)}\prod_{p=1}^{|P^{i\sim j}|}z_{j}^{A_{p}^{i\sim j} \circ\epsilon_{i}}\middle|\epsilon_{i}\right]\right]\\ &=\mathbb{E}\left[z_{i}^{\epsilon_{i}}\mathbb{E}\left[\prod_{j\in Ch (i)}\left(z_{j}^{\alpha_{i,j}\circ\epsilon_{i}}\prod_{k\in\text{Des}(j)}\prod_ {p=1}^{|P^{j\sim k}|}z_{k}^{A_{p}^{j\sim k}\circ\alpha_{i,j}\circ\epsilon_{ i}}\right)\middle|\epsilon_{i}\right]\right]\end{split}\] (A.11)

Now, regarding the definition of thinning operation \(\alpha_{i,j}\circ X=\sum_{n=1}^{X}\xi_{n}^{(\alpha_{i,j})}\), where \(\xi_{n}^{(\alpha_{i,j})}\sim\text{Bernoulli}(\alpha)\) are i.i.d. Bernoulli random variables, Eq. A.11 can be rewritten as follows:

\[\begin{split}\mathbb{E}\left[z_{i}^{\epsilon_{i}}\prod_{j\in \text{Des}(i)}\prod_{p=1}^{|P^{i\sim j}|}z_{j}^{A_{p}^{i\sim j}\circ\epsilon_{ i}}\right]&=\mathbb{E}\left[z_{i}^{\epsilon_{i}}\prod_{j\in Ch(i)} \mathbb{E}\left[z_{j}^{\sum_{n=1}^{\epsilon_{i}}\xi_{n}^{(\alpha_{i,j})}}\prod_ {k=\text{Des}(j)}\prod_{p=1}^{|P^{j\sim k}|}z_{k}^{\sum_{n=1}^{\epsilon_{i}}A_{ p}^{j\sim k}\circ\xi_{n}^{(\alpha_{i,j})}}\middle|\epsilon_{i}\right]\right]\\ &=\mathbb{E}\left[z_{i}^{\epsilon_{i}}\prod_{j\in Ch(i)}\mathbb{E} \left[\prod_{n=1}^{\epsilon_{i}}z_{j}^{\xi_{n}^{(\alpha_{i,j})}}\prod_{k=\text{ Des}(j)}\prod_{p=1}^{|P^{j\sim k}|}\prod_{n=1}^{\epsilon_{i}}z_{k}^{A_{p}^{j \sim k}\circ\xi_{n}^{(\alpha_{i,j})}}\middle|\epsilon_{i}\right]\right]\\ &=\mathbb{E}\left[z_{i}^{\epsilon_{i}}\prod_{j\in Ch(i)}\mathbb{E} \left[\prod_{n=1}^{\epsilon_{i}}\left(z_{j}^{\xi_{n}^{(\alpha_{i,j})}}\prod_{k= \text{Des}(j)}\prod_{p=1}^{|P^{j\sim k}|}z_{k}^{A_{p}^{j\sim k}\circ\xi_{n}^{( \alpha_{i,j})}}\right)\middle|\epsilon_{i}\right]\right].\end{split}\] (A.12)Since we condition the \(\epsilon_{i}\), which can be regarded as a constant in the expectation, we have:

\[\mathbb{E}\left[z_{i}^{\epsilon_{i}}\prod_{j=\text{Des}(i)}\prod_{p=1 }^{|P^{i\sim j}|}z_{j}^{A_{p}^{i\sim j}\circ\epsilon_{i}}\right] =\mathbb{E}\left[z_{i}^{\epsilon_{i}}\prod_{j\in Ch(i)}\mathbb{E} \left[\left(z_{j}^{\epsilon_{n_{i},j}^{(\alpha_{i,j})}}\prod_{k=\text{Des}(j)} \prod_{p=1}^{|P^{j\sim k}|}z_{k}^{A_{p}^{j\sim k}\circ\xi_{n}^{(\alpha_{i,j})}} \right)^{\epsilon_{i}}\right]\epsilon_{i}\right]\] \[=\mathbb{E}\left[z_{i}^{\epsilon_{i}}\prod_{j\in Ch(i)}\mathbb{E} \left[z_{j}^{\epsilon_{n_{i},j}^{(\alpha_{i,j})}}\prod_{k=\text{Des}(j)}\prod_ {p=1}^{|P^{j\sim k}|}z_{k}^{A_{p}^{j\sim k}\circ\xi_{n}^{(\alpha_{i,j})}} \right]^{\epsilon_{i}}\right]\] \[=\mathbb{E}\left[\left(z_{i}\prod_{j\in Ch(i)}\mathbb{E}\left[z_ {j}^{\epsilon_{n_{i},j}^{(\alpha_{i,j})}}\prod_{k=\text{Des}(j)}\prod_{p=1}^{ |P^{j\sim k}|}z_{k}^{A_{p}^{j\sim k}\circ\xi_{n}^{(\alpha_{i,j})}}\right] \right)^{\epsilon_{i}}\right],\] (A.13)

which is the PGF of \(\epsilon_{i}\), with the input being \(z_{i}\prod_{j\in Ch(i)}\mathbb{E}\left[z_{j}^{\epsilon_{n}^{(\alpha_{i,j})}} \prod_{k=\text{Des}(j)}\prod_{p=1}^{|P^{j\sim k}|}z_{k}^{A_{p}^{j\sim k}\circ \xi_{n}^{(\alpha_{i,j})}}\right]\). Therefore, we finally have:

\[\mathbb{E}\left[z_{i}^{\epsilon_{i}}\prod_{j=\text{Des}(i)}\prod_{p=1}^{|P^{i \sim j}|}z_{j}^{A_{p}^{i\sim j}\circ\epsilon_{i}}\right]=G_{\epsilon_{i}}\left( z_{i}\prod_{j\in Ch(i)}\mathbb{E}\left[z_{j}^{\epsilon_{n_{i},j}^{(\alpha_{i,j})}} \prod_{k=\text{Des}(j)}\prod_{p=1}^{|P^{j\sim k}|}z_{k}^{A_{p}^{j\sim k}\circ \xi_{n}^{(\alpha_{i,j})}}\right]\right).\] (A.14)

This completes the proof of Lemma 1. 

**Lemma 2**.: _Let \(G_{i,j}(\mathbf{z}_{(j)})=\mathbb{E}\left[z_{j}^{\xi_{n}^{(\alpha_{i,j})}} \prod_{k=\text{Des}(j)}\prod_{p=1}^{|P^{j\sim k}|}z_{k}^{A_{p}^{j\sim k}\circ \xi_{n}^{(\alpha_{i,j})}}\right]\), we have:_

\[G_{i,j}(\mathbf{z}_{(j)})=\begin{cases}G_{B(\alpha_{i,j})}\left(z_{j}\times \prod_{k\in Ch(j)}G_{j,k}(\mathbf{z}_{(k)})\right)&\text{,}\text{Ch}(j)\neq \emptyset\\ G_{B(\alpha_{i,j})}(z_{j})&\text{,}\text{Otherwise}\end{cases}.\] (A.15)

Proof.: (i) If \(\text{Ch}(j)=\emptyset\), which means vertex \(j\) has no descendant, we have:

\[G_{i,j}(\mathbf{z}_{(j)})=\mathbb{E}\left[z_{j}^{\xi_{n}^{(\alpha_{i,j})}} \prod_{k=\text{Des}(j)}\prod_{p=1}^{|P^{j\sim k}|}z_{k}^{A_{p}^{j\sim k}\circ \xi_{n}^{(\alpha_{i,j})}}\right]=\mathbb{E}\left[z_{j}^{\xi_{n}^{(\alpha_{i,j})} }\right],\] (A.16)

where \(\xi_{n}^{(\alpha_{i,j})}\) is the Bernoulli distribution with parameter \(\alpha_{i,j}\). Therefore, \(\mathbb{E}\left[z_{j}^{\xi_{n}^{(\alpha_{i,j})}}\right]\) is the PGF of \(\text{Bernoulli}(\alpha_{i,j})\) with input \(z_{j}\), we have \(G_{i,j}(\mathbf{z}_{(j)})=\mathbb{E}\left[z_{j}^{\xi_{n}^{(\alpha_{i,j})}} \right]=G_{B(\alpha_{i,j})}(z_{j})\).

(ii) If \(\text{Ch}(j)\neq\emptyset\), we condition the Bernoulli variable \(\xi_{n}^{(\alpha_{i,j})}\), and obtain:

\[G_{i,j}(\mathbf{z}_{(j)}) =\mathbb{E}\left[z_{j}^{\xi_{n}^{(\alpha_{i,j})}}\mathbb{E} \left[\prod_{k\in\text{Des}(j)}\prod_{p=1}^{|P^{j\sim k}|}z_{k}^{A_{p}^{j\sim k }\circ\xi_{n}^{(\alpha_{i,j})}}|\xi_{n}^{(\alpha_{i,j})}\right]\right]\] \[=\mathbb{E}\left[z_{j}^{\xi_{n}^{(\alpha_{i,j})}}\prod_{k\in \text{Ch}(j)}\mathbb{E}\left[z_{k}^{\alpha_{j,k}\circ\xi_{n}^{(\alpha_{i,j})}} \prod_{l\in\text{Des}(k)}\prod_{p=1}^{|P^{k\sim l}|}z_{l}^{A_{p}^{k\sim l}\circ \alpha_{j,k}\circ\xi_{n}^{(\alpha_{i,j})}}\right]\xi_{n}^{(\alpha_{i,j})}\right]\] (A.17)Similar to Lemma 1, we have \(\alpha_{j,k}\circ\xi_{n}^{(\alpha_{i,j})}=\sum_{n=1}^{\xi_{(\alpha_{i,j})}}\xi_{n} ^{(\alpha_{j,k})}\). Therefore, Eq. A.17 can be rewritten as follows:

\[G_{i,j}(\mathbf{z}_{(j)}) =\mathbb{E}\left[z_{j}^{\xi_{n}^{(\alpha_{i,j})}}\prod_{k\in \text{Ch}(j)}\mathbb{E}\left[\left(z_{k}^{\xi_{n}^{(\alpha_{j,k})}}\prod_{l\in \text{Des}(k)}\prod_{p=1}^{|P^{k\sim l}|}z_{l}^{A_{p}^{k\sim l}\circ\xi_{n}^{( \alpha_{j,k})}}\right)^{\xi_{n}^{(\alpha_{i,j})}}|\xi_{n}^{(\alpha_{i,j})} \right]\right]\] (A.18) \[=\mathbb{E}\left[z_{j}^{\xi_{n}^{(\alpha_{i,j})}}\prod_{k\in \text{Ch}(j)}\mathbb{E}\left[z_{k}^{\xi_{n}^{(\alpha_{j,k})}}\prod_{l\in\text {Des}(k)}\prod_{p=1}^{|P^{k\sim l}|}z_{l}^{A_{p}^{k\sim l}\circ\xi_{n}^{( \alpha_{j,k})}}\right]^{\xi_{n}^{(\alpha_{i,j})}}\right]\] \[=\mathbb{E}\left[\left(z_{j}\prod_{k\in\text{Ch}(j)}\mathbb{E} \left[z_{k}^{\xi_{n}^{(\alpha_{j,k})}}\prod_{l\in\text{Des}(k)}\prod_{p=1}^{|P ^{k\sim l}|}z_{l}^{A_{p}^{k\sim l}\circ\xi_{n}^{(\alpha_{j,k})}}\right]\right) ^{\xi_{n}^{(\alpha_{i,j})}}\right],\]

which is the PGF of Bernoulli variable \(\xi_{n}^{(\alpha_{i,j})}\). Therefore, we have

\[G_{i,j}(\mathbf{z}_{(j)})=G_{B(\alpha_{i,j})}\left(z_{j}\prod_{k\in\text{Ch}(j )}\mathbb{E}\left[z_{k}^{\xi_{n}^{(\alpha_{j,k})}}\prod_{l\in\text{Des}(k)} \prod_{p=1}^{|P^{k\sim l}|}z_{l}^{A_{p}^{k\sim l}\circ\xi_{n}^{(\alpha_{j,k})} }\right]\right),\] (A.19)

where \(\mathbb{E}\left[z_{k}^{\xi_{n}^{(\alpha_{i,j})}}\prod_{l\in\text{Des}(k)}\prod _{p=1}^{|P^{k\sim l}|}z_{l}^{A_{p}^{k\sim l}\circ\xi_{n}^{(\alpha_{j,k})}}\right]\) is \(G_{j,k}(\mathbf{z}_{(k)})\) according to the definition, that is

\[G_{i,j}(\mathbf{z}_{(j)})=G_{B(\alpha_{i,j})}\left(z_{j}\times\prod_{k\in\text {Ch}(j)}G_{j,k}(\mathbf{z}_{(k)})\right),\] (A.20)

which completes the proof of Lemma 2. 

Regarding to the Eq. A.9, by leveraging the Lemma 1, we have:

\[G_{\mathbf{X}}(\mathbf{z}) =\prod_{i\in[n]}\mathbb{E}\left[z_{i}^{\epsilon_{i}}\prod_{j\in \text{Des}(i)}\prod_{p=1}^{|P^{i\sim j}|}z_{j}^{A_{p}^{i\sim j}\circ\epsilon_{ i}}\right]\] (A.21) \[=\prod_{i\in[n]}G_{\epsilon_{i}}\left(z_{i}\prod_{j\in\text{Ch}(i )}\mathbb{E}\left[z_{j}^{\xi_{n}^{(\alpha_{i,j})}}\prod_{k\in\text{Des}(j)} \prod_{p=1}^{|P^{j\sim k}|}z_{j}^{A_{p}^{j\sim k}\circ\xi_{n}^{(\alpha_{i,j})} }\right]\right)\]

Next, according to the Lemma 2, we have:

\[G_{\mathbf{X}}(\mathbf{z})=\prod_{i\in[n]}G_{\epsilon_{i}}\left(z_{i}\prod_{j \in\text{Ch}(i)}\mathbb{E}\left[z_{j}^{\xi_{n}^{(\alpha_{i,j})}}\prod_{k\in \text{Des}(j)}\prod_{p=1}^{|P^{j\sim k}|}z_{k}^{A_{p}^{j\sim k}\circ\xi_{n}^{( \alpha_{i,j})}}\right]\right)=\prod_{i\in[n]}G_{\epsilon_{i}}\left(z_{i}\prod_{j \in\text{Ch}(i)}G_{i,j}(\mathbf{z}_{(j)})\right)\] (A.22)

where \(G_{i,j}(\mathbf{z}_{(j)})=\begin{cases}G_{B(\alpha_{i,j})}\left(z_{j}\times \prod_{k\in\text{Ch}(j)}G_{j,k}(\mathbf{z}_{(k)})\right)&,\text{Ch}(j)\neq \emptyset\\ G_{B(\alpha_{i,j})}(z_{j})&,\text{Otherwise}\end{cases}.\) This completes the proof.

### Proof of Theorem 2

**Theorem 2**.: _Given a random vector \(\mathbf{X}=[X_{1},...,X_{d}]^{T}\) following PB-SCM, the PGF of \(P(\mathbf{X})\) can be expressed by:_

\[G_{\mathbf{X}}(\mathbf{z})=\prod_{i\in[d]}\exp\{\mu_{i}\times(T_{X_{i}}(1)-1)\},\] (A.23)

_where \(T_{X_{i}}(1)=z_{i}\sum_{\mathbf{s}\in\{0,1\}^{|\Omega(i)|}}\prod_{j\in\text{Ch} (i)}\alpha_{i,j}^{s_{j}}T_{X_{j}}(s_{j}),T_{X_{i}}(0)=1\) and \(\alpha_{i,j}^{s_{j}}=\alpha_{i,j}\) if \(s_{j}=1\) and \(\alpha_{i,j}^{s_{j}}=1-\alpha_{i,j}\) if \(s_{j}=0\)._Proof.: According to Theorem 1, we have

\[G_{\mathbf{X}}(\mathbf{z})=\prod\nolimits_{i\in[n]}G_{\epsilon_{i}}\left(z_{i} \times\prod\nolimits_{j\in Ch(i)}G_{i,j}(\mathbf{z}_{(j)})\right).\] (A.24)

Since \(G_{\epsilon_{i}}(\cdot)\) is the PGF of Poisson noise \(\epsilon_{i}\), we have:

\[G_{\mathbf{X}}(\mathbf{z})=\prod\nolimits_{i\in[n]}\exp\left\{\mu\left(z_{i} \times\prod\nolimits_{j\in Ch(i)}G_{i,j}(\mathbf{z}_{(j)})-1\right)\right\}.\] (A.25)

Therefore, to complete the proof, it suffices to show that:

\[T_{X_{i}}(1)=z_{i}\times\prod\nolimits_{j\in Ch(i)}G_{i,j}(\mathbf{z}_{(j)})\] (A.26)

holds for each vertex in the recursive process.

We proceed by structural induction. Specifically, we first show that Eq. A.26 holds for the leaf vertices. Then, assuming that the expression holds for each child vertex \(j\) of a parent vertex \(i\) (i.e., \(j\in\text{Ch}(i)\)), we demonstrate that the expression also holds for the parent vertex \(i\).

Base caseGiven an arbitrary leaf vertex \(l\), since \(\text{Ch}(l)=\emptyset\) and by the definition of \(T_{X_{i}}(1)\), we have \(T_{X_{l}}(1)=z_{l}\times 1\). Therefore, Eq. A.26 holds for the leaf vertex.

Inductive StepNext, Assuming that for each child vertex \(j\) of \(i\), the following holds: \(T_{X_{j}}(1)=z_{j}\times\prod_{k\in Ch(j)}G_{j,k}(\mathbf{z}_{(k)})\). Our goal is to prove the \(T_{X_{i}}(1)=z_{i}\times\prod_{j\in Ch(i)}G_{i,j}(\mathbf{z}_{(j)})\).

When \(Ch(i)\neq\emptyset\), according to the definition of \(G_{i,j}(\mathbf{z}_{(j)})\), we have:

\[\begin{split}\prod\nolimits_{j\in Ch(i)}G_{i,j}(\mathbf{z}_{(j)} )&=\prod\nolimits_{j\in Ch(i)}G_{B(\alpha_{i,j})}\left(z_{j} \times\prod\nolimits_{k\in Ch(j)}G_{j,k}(\mathbf{z}_{(k)})\right)\\ &=\prod\nolimits_{j\in Ch(i)}\left(1-\alpha_{i,j}+\alpha_{i,j}z_ {j}\times\prod\nolimits_{k\in Ch(j)}G_{j,k}(\mathbf{z}_{(k)})\right).\end{split}\] (A.27)

By expanding Eq. A.27, we have:

\[\prod\nolimits_{j\in Ch(i)}G_{i,j}(\mathbf{z}_{(j)})=\sum_{S\subseteq Ch(i)} \left[\prod\nolimits_{j\in Ch(i)\setminus S}(1-\alpha_{i,j})\prod\nolimits_{ j\in S}\alpha_{i,j}\left(z_{j}\prod\nolimits_{k\in Ch(j)}G_{j,k}(\mathbf{z}_{(k)}) \right)\right],\] (A.28)

where \(S\) represents all subsets of \(\text{Ch}(i)\), corresponding to all possible combinations of child vertices of vertex \(i\).

Next, regarding the definition of \(T_{X_{i}}(1)\), since \(\alpha_{i,j}^{s_{j}}=\begin{cases}\alpha_{i,j}&s_{j}=1\\ 1-\alpha_{i,j}&s_{j}=0\end{cases}\), we have:

\[\begin{split} T_{X_{i}}(1)&=z_{i}\sum_{\mathbf{s}\in\{0,1 \}\setminus\alpha^{(i)}}\prod\nolimits_{j\in\text{Ch}(i)}\alpha_{i,j}^{s_{j}} T_{X_{j}}(s_{j})\\ &=z_{i}\sum_{S\subseteq Ch(i)}\left[\prod\nolimits_{j\in Ch(i) \setminus S}\alpha_{i,j}^{0}T_{X_{j}}(0)\prod\nolimits_{j\in S}\alpha_{i,j}^ {1}T_{X_{j}}(1)\right]\\ &=z_{i}\sum_{S\subseteq Ch(i)}\left[\prod\nolimits_{j\in Ch(i) \setminus S}(1-\alpha_{i,j})\prod\nolimits_{j\in S}\alpha_{i,j}\left(z_{j} \prod\nolimits_{k\in Ch(j)}G_{j,k}(\mathbf{z}_{(k)})\right)\right],\end{split}\] (A.29)

given \(T_{X_{j}}(1)=z_{j}\times\prod_{k\in Ch(j)}G_{j,k}(\mathbf{z}_{(k)})\). Then, comparing Eq. A.28 and Eq. A.29, we find that:

\[T_{X_{i}}(1)=z_{i}\prod\nolimits_{j\in Ch(i)}G_{i,j}(\mathbf{z}_{(j)}).\] (A.30)

Substituting back into A.25, we have:

\[\begin{split} G_{\mathbf{X}}(\mathbf{z})&=\prod \nolimits_{i\in[n]}\exp\left\{\mu_{i}\left(z_{i}\times\prod\nolimits_{j\in Ch (i)}G_{i,j}(\mathbf{z}_{(j)})-1\right)\right\}\\ &=\prod\nolimits_{i\in[n]}\exp\{\mu_{i}(T_{X_{i}}(1)-1)\},\end{split}\] (A.31)

which completes the proof.

### Proof of Theorem 3

**Theorem 3**.: _Given a random vector \(\mathbf{X}=[X_{1},...,X_{n}]^{T}\) following PB-SCM and its PGF \(G_{\mathbf{X}}(\mathbf{z})\). For a subset \(\mathbf{L}\subset[d]\) of the set of vertices, and the set of the children of vertex \(i\) within \(\mathbf{L}\), denoted by \(\text{Ch}_{\mathbf{L}}(i)=\text{Ch}(i)\cap\mathbf{L}\), the local PGF of \(\mathbf{L}\) can be expressed by:_

\[G_{\mathbf{X}}^{\mathbf{L}}(\mathbf{z})=\prod_{i\in\mathbf{L}}\exp\left\{\mu_{ i}\times\left(T_{X_{i}}^{\mathbf{L}}(1)-1\right)\right\}\prod_{i\in[d]\setminus \mathbf{L}}\exp\{-\mu_{i}\},\] (A.32)

_where \(T_{X_{i}}^{\mathbf{L}}(1)=z_{i}\sum\limits_{\mathbf{s}\in\{0,1\}^{|\alpha_{ \mathbf{L}}(i)|}}\prod\limits_{j\in\mathcal{C}\text{{h}}_{\mathbf{L}}(i)} \alpha_{i,j}^{s_{j}}T_{X_{j}}^{\mathbf{L}}(s_{j})\prod\limits_{j\in Ch(i) \setminus\mathcal{C}\text{{h}}_{\mathbf{L}}(i)}(1-\alpha_{i,j})\) and \(T_{X_{i}}^{\mathbf{L}}(0)=1\)._

Proof.: According to the definition of Local PGF, we have:

\[G_{\mathbf{X}}^{\mathbf{L}}(\mathbf{z}) =\lim_{\mathbf{z}_{\setminus\mathbf{L}}\to\mathbf{0}}\prod_{i\in[d]}G_{e_ {i}}\bigg{(}z_{i}\times\prod_{j\in\text{Ch}(i)}G_{i,j}(\mathbf{z}_{(j)})\bigg{)}\] \[=\lim_{\mathbf{z}_{\setminus\mathbf{L}}\to\mathbf{0}}\prod_{i\in \mathbf{L}}G_{e_{i}}\bigg{(}z_{i}\times\prod_{j\in\text{Ch}(i)}G_{i,j}( \mathbf{z}_{(j)})\bigg{)}\prod_{j\in[d]\setminus\mathbf{L}}G_{e_{i}}\bigg{(}z _{i}\times\prod_{j\in\text{Ch}(i)}G_{i,j}(\mathbf{z}_{(j)})\bigg{)}\] \[=\lim_{\mathbf{z}_{\setminus\mathbf{L}}\to\mathbf{0}}\prod_{i\in \mathbf{L}}G_{e_{i}}\bigg{(}z_{i}\times\prod_{j\in\text{Ch}(i)}G_{i,j}( \mathbf{z}_{(j)})\bigg{)}\prod_{j\in[d]\setminus\mathbf{L}}\lim_{\mathbf{z}_{ \setminus\mathbf{L}}\to\mathbf{0}}G_{e_{i}}\bigg{(}z_{i}\times\prod_{j\in \text{Ch}(i)}G_{i,j}(\mathbf{z}_{(j)})\bigg{)}.\] (A.33)

Since \(z_{j}\) approaches zero for each \(j\in[d]\setminus\mathbf{L}\), we have:

\[\prod_{j\in[d]\setminus\mathbf{L}}\lim_{\mathbf{z}_{\setminus \mathbf{L}}\to\mathbf{0}}G_{e_{i}}\bigg{(}z_{i}\times\prod_{j\in\text{Ch}(i)} G_{i,j}(\mathbf{z}_{(j)})\bigg{)}\] (A.34) \[=\prod_{j\in[d]\setminus\mathbf{L}}\lim_{\mathbf{z}_{\setminus \mathbf{L}}\to\mathbf{0}}\exp\left\{\mu_{j}\left(z_{j}\times\prod_{k\in Ch(j)} G_{j,k}(\mathbf{z}_{(k)})-1\right)\right\}=\prod_{j\in[d]\setminus\mathbf{L}} \exp\{-\mu_{j}\}.\] (A.35)

Therefore, we have:

\[\log G_{\mathbf{X}}^{\mathbf{L}}(\mathbf{z})=\lim_{\mathbf{z}_{\setminus \mathbf{L}}\to\mathbf{0}}\prod_{i\in\mathbf{L}}G_{e_{i}}\bigg{(}z_{i}\times \prod_{j\in\text{Ch}(i)}G_{i,j}(\mathbf{z}_{(j)})\bigg{)}\prod_{j\in[d]\setminus \mathbf{L}}\exp\{-\mu_{i}\}.\] (A.36)

Next, we consider the \(\lim_{\mathbf{z}_{\setminus\mathbf{L}}\to\mathbf{0}}G_{e_{i}}\Big{(}z_{i} \times\prod_{j\in\text{Ch}(i)}G_{i,j}(\mathbf{z}_{(j)})\Big{)}\). According to Theorem 2, it can be expressed as follows :

\[\lim_{\mathbf{z}_{\setminus\mathbf{L}}\to\mathbf{0}}G_{e_{i}} \bigg{(}z_{i}\times\prod_{j\in\text{Ch}(i)}G_{i,j}(\mathbf{z}_{(j)})\bigg{)}\] \[=\lim_{\mathbf{z}_{\setminus\mathbf{L}}\to\mathbf{0}}\exp\{\mu_{ i}(T_{X_{i}}(1)-1)\}\] (A.37) \[=\exp\left\{\mu_{i}\left(\lim_{\mathbf{z}_{\setminus\mathbf{L}} \to\mathbf{0}}T_{X_{i}}(1)-1\right)\right\}\] \[=\exp\left\{\mu_{i}\times\left(z_{i}\lim_{\mathbf{z}_{\setminus \mathbf{L}}\to\mathbf{0}}\sum_{\mathbf{s}\in\{0,1\}^{|\alpha(i)|}}\prod_{j\in \text{Ch}(i)}\alpha_{i,j}^{s_{j}}T_{X_{j}}(s_{j})-1\right)\right\}.\]

Next, we are going to show that, when \(\lim_{\mathbf{z}_{\setminus\mathbf{L}}\to\mathbf{0}}T_{X_{j}}(s_{j})=T_{X_{j}}^ {\mathbf{L}}(s_{j})\), we have

\[\lim_{\mathbf{z}_{\setminus\mathbf{L}}\to\mathbf{0}}T_{X_{i}}(1)=z_{i}\sum_{ \mathbf{s}\in\{0,1\}^{|\alpha_{\mathbf{L}}(i)|}}\prod_{j\in\text{Ch}_{\mathbf{L }}(i)}\alpha_{i,j}^{s_{j}}T_{X_{j}}(s_{j})\prod_{j\in Ch(i)\setminus\text{Ch}_{ \mathbf{L}}(i)}(1-\alpha_{i,j})=T_{X_{i}}^{\mathbf{L}}(1).\] (A.38)

Let \(\text{Ch}_{\mathbf{L}}(i)=Ch(i)\cap\mathbf{L}\), we have:

\[\lim_{\mathbf{z}_{\setminus\mathbf{L}}\to\mathbf{0}}G_{e_{i}} \bigg{(}z_{i}\times\prod_{j\in\text{Ch}(i)}G_{i,j}(\mathbf{z}_{(j)})\bigg{)}\] \[=\lim_{\mathbf{z}_{\setminus\mathbf{L}}\to\mathbf{0}}\exp\left\{\mu_ {i}\times\left(z_{i}\sum_{\mathbf{s}\in\{0,1\}^{|\alpha(i)|}}\prod_{j\in\text{ Ch}_{\mathbf{L}}(i)}\alpha_{i,j}^{s_{j}}T_{X_{j}}(s_{j})\prod_{j\in Ch(i)\setminus\text{Ch}_{ \mathbf{L}}(i)}\alpha_{i,j}^{s_{j}}T_{X_{j}}(s_{j})-1\right)\right\}.\] (A.39)In scenarios where \(j\in\textit{Ch}(i)\setminus\textit{Ch}_{\textbf{L}}(i)\) and \(s_{j}=1\), considering that \(z_{j}\in\textbf{z}_{\setminus\textbf{L}}\) and \(z_{j}\to 0\), (that is, \(z_{j}\) approach to zero for those vertices \(j\notin\textbf{L}\) ),we have:

\[\lim_{\textbf{z}_{\setminus\textbf{L}}\rightarrow\textbf{0}}T_{X_{j}}(s_{j})= \lim_{\textbf{z}_{\setminus\textbf{L}}\rightarrow\textbf{0}}z_{j}\sum_{ \textbf{s}\in\{0,1\}^{\Omega(j)}}\prod_{k\in\textit{Ch}(j)}\alpha_{j,k}^{s_{k }}T_{X_{k}}(s_{k})=0.\] (A.40)

Otherwise, when \(j\in\textit{Ch}_{\textbf{L}}(i)\), we have:

\[\begin{split}\lim_{\textbf{z}_{\setminus\textbf{L}}\rightarrow \textbf{0}}T_{X_{j}}(s_{j})&=\lim_{\textbf{z}_{\setminus\textbf{ L}}\rightarrow\textbf{0}}z_{j}\sum_{\textbf{s}\in\{0,1\}^{\Omega(j)}}\prod_{k \in\textit{Ch}(j)}\alpha_{j,k}^{s_{k}}T_{X_{k}}(s_{k})\\ &=z_{j}\sum_{\textbf{s}\in\{0,1\}^{\Omega(j)}}\prod_{k\in\textit{ Ch}(j)}\alpha_{j,k}^{s_{k}}\lim_{\textbf{z}_{\setminus\textbf{L}}\rightarrow \textbf{0}}T_{X_{k}}(s_{k})\\ &=z_{j}\sum_{\textbf{s}\in\{0,1\}^{\Omega(j)}}\prod_{k\in\textit{ Ch}(j)}\alpha_{j,k}^{s_{k}}T_{X_{k}}^{\textbf{L}}(s_{k}),\end{split}\] (A.41)

and when \(j\in\textit{Ch}(i)\setminus\textit{Ch}_{\textbf{L}}(i)\) and \(s_{j}=0\), we have \(\lim_{\textbf{z}_{\setminus\textbf{L}}\rightarrow\textbf{0}}T_{X_{j}}(0)=1\). Therefore, we have:

\[\begin{split}&\lim_{\textbf{z}_{\setminus\textbf{L}}\rightarrow \textbf{0}}G_{\epsilon_{i}}\bigg{(}z_{i}\times\prod_{j\in\textit{Ch}(i)}G_{i, j}(\textbf{z}_{(j)})\bigg{)}\\ &=\exp\left\{\mu_{i}\times\left(z_{i}\sum_{\textbf{s}\in\{0,1\}^{ \Omega_{\textbf{L}}(i)}}\prod_{j\in\textit{Ch}_{\textbf{L}}(i)}\alpha_{i,j}^{ s_{j}}\lim_{\textbf{z}_{\setminus\textbf{L}}\rightarrow\textbf{0}}T_{X_{j}}(s_{j}) \prod_{j\in\textit{Ch}(i)\setminus\textit{Ch}_{\textbf{L}}(i)}(1-\alpha_{i,j}) -1\right)\right\}\\ &=\exp\left\{\mu_{i}\times\left(z_{i}\sum_{\textbf{s}\in\{0,1\}^{ \Omega_{\textbf{L}}(i)}}\prod_{j\in\textit{Ch}_{\textbf{L}}(i)}\alpha_{i,j}^{ s_{j}}T_{X_{j}}^{\textbf{L}}(s_{j})\prod_{j\in\textit{Ch}(i)\setminus\textit{Ch}_{ \textbf{L}}(i)}(1-\alpha_{i,j})-1\right)\right\}.\end{split}\] (A.42)

According to the definition of \(T_{X_{i}}^{\textbf{L}}\), we obtain:

\[\lim_{\textbf{z}_{\setminus\textbf{L}}\rightarrow\textbf{0}}G_{\epsilon_{i}} \bigg{(}z_{i}\times\prod_{j\in\textit{Ch}(i)}G_{i,j}(\textbf{z}_{(j)})\bigg{)} =\exp\left\{\mu_{i}\times\left(T_{X_{i}}^{\textbf{L}}(1)-1\right)\right\}.\] (A.43)

By substituting Eq. A.43 into Eq. A.36, we finally have:

\[\log G_{\textbf{X}}^{\textbf{L}}(\textbf{z})=\prod_{i\in\textbf{L}}\exp\left\{ \mu_{i}\times\left(T_{X_{i}}^{\textbf{L}}(1)-1\right)\right\}\prod_{i\in[d] \setminus\textbf{L}}\exp\{-\mu_{i}\},\] (A.44)

which completes the proof. 

### Proof of Proposition 1

**Proposition 1** (Graphical implication of closed-form PGF).: _Given a random vector \(\textbf{X}=[X_{1},...,X_{d}]^{T}\) following PB-SCM, for any subgraph \(G(\textbf{L},\textbf{E})\) with the subset of vertices \(\textbf{L}\subseteq\textbf{V}\) such that the \(i\)-th vertex is the root vertex in \(G(\textbf{L},\textbf{E})\), a component \(Cz_{i}\prod_{j\in L\setminus\{i\}}z_{j}^{p_{j}}\) with constant \(C\neq 0\), exponent \(p_{j}\in\mathbb{Z}^{+}\) exits in \(T_{X_{i}}(1)\), if and only if there exists a subgraph \(G(\textbf{L},\textbf{E}^{\prime})\) with subset of the edges \(\textbf{E}^{\prime}\subseteq\textbf{E}\) such that for each \(j\in\textbf{L}\setminus\{i\}\), there are at least \(p_{j}\) directed paths from \(X_{i}\) to \(X_{j}\) in the subgraph \(G(\textbf{L},\textbf{E}^{\prime})\)._

Proof.: We consider the subgraph \(G(\textbf{L},\textbf{E}^{\prime})\) as a local structure associated with the node set \(L\). According to Theorem 3, we have:

\[T_{X_{i}}^{\textbf{L}}(1)=z_{i}\sum_{\textbf{s}\in\{0,1\}^{|C\textbf{h}_{ \textbf{L}}(i)|}}\prod_{j\in C\textbf{h}_{\textbf{L}}(i)}\alpha_{i,j}^{s_{j}}T_{ X_{j}}^{\textbf{L}}(s_{j})\prod_{j\in\textit{Ch}(i)\setminus\textit{Ch}_{ \textbf{L}}(i)}(1-\alpha_{i,j})\] (A.45)

We select a subset of edges \(\textbf{E}^{\prime}\subseteq\textbf{E}\), indicating that edges in \(\textbf{E}^{\prime}\) are 'open' and edges in \(\textbf{E}\setminus\textbf{E}^{\prime}\) are 'closed' within the local structure containing the vertex set \(L\). This means that edges not included in \(\textbf{E}^{\prime}\) are considered non-existent in the subgraph. Consequently, in Eq. A.45, we have: (1) \(\alpha_{i,j}^{s_{j}=1}T_{X_{j}}^{\textbf{L}}(s_{j}=1)\) if the edge \(i\to j\) is in \(\textbf{E}^{\prime}\), and (2) \(\alpha_{i,j}^{s_{j}=0}T_{X_{j}}^{\textbf{L}}(s_{j}=0)\) if the edge \(i\to j\) is not in \(\mathbf{E}^{\prime}\). That is, each subset \(\mathbf{E}^{\prime}\) corresponds to an element in \(\{0,1\}^{|Ch_{\mathbf{L}}(i)|}\), representing a possible combination of whether each edge is 'open' or is 'close'.

We describe the term \(Cz_{i}\prod_{j\in L\setminus\{i\}}z_{j}^{p_{j}}\) through a recursive process. Initially considering the children of \(X_{i}\), there exists a term in \(T_{X_{i}}^{\mathbf{X}}(1)\) as follows:

\[z_{i}\prod_{\begin{subarray}{c}j\in Ch_{\mathbf{L}}(i)\\ i\to j\in\mathbf{E}^{\prime}\end{subarray}}\alpha_{i,j}^{1}T_{X_{j}}^{ \mathbf{L}}(1)\prod_{\begin{subarray}{c}i\in Ch_{\mathbf{L}}(i)\\ i\to j\in\mathbf{E}^{\prime}\end{subarray}}\alpha_{i,j}^{0}T_{X_{j}}^{ \mathbf{X}}(0)\prod_{j\in Ch_{\mathbf{L}}(i)\setminus C_{h\mathbf{L}}(i)}(1- \alpha_{i,j})\] \[=z_{i}\prod_{\begin{subarray}{c}j\in Ch_{\mathbf{L}}(i)\\ i\to j\in\mathbf{E}^{\prime}\end{subarray}}\alpha_{i,j}z_{j}\left(\sum_{ \mathbf{s}\in\{0,1\}^{|C_{\mathbf{L}}(j)|}}\prod_{k\in Ch_{\mathbf{L}}(j)} \alpha_{j,k}^{s_{k}}T_{X_{k}}^{\mathbf{L}}(s_{k})\right)\underbrace{\prod_{ \begin{subarray}{c}j\in Ch_{\mathbf{L}}(i)\\ i\to j\notin\mathbf{E}^{\prime}\end{subarray}}(1-\alpha_{i,j})}_{\begin{subarray} {c}j\in Ch_{\mathbf{L}}(i)\setminus C_{h\mathbf{L}}(i)\\ \text{Constant coefficient}\end{subarray}}(1-\alpha_{i,j})\] \[=C\times z_{i}\prod_{\begin{subarray}{c}j\in Ch_{\mathbf{L}}(i)\\ i\to j\in\mathbf{E}^{\prime}\end{subarray}}z_{j}\left(\sum_{\mathbf{s}\in \{0,1\}^{|C_{h\mathbf{L}}(j)|}}\prod_{k\in Ch_{\mathbf{L}}(j)}\alpha_{j,k}^{s_ {k}}T_{X_{k}}^{\mathbf{L}}(s_{k})\right)\] (A.46)

Continuing from the Eq. A.46, for each term in the product expansion, we can similarly expand based on the children of \(X_{j}\). This expansion includes the following term:

\[C\times z_{i}\prod_{\begin{subarray}{c}j\in Ch_{\mathbf{L}}(i)\\ i\to j\in\mathbf{E}^{\prime}\end{subarray}}z_{j}\left(\prod_{\begin{subarray}{c }k\in Ch_{\mathbf{L}}(j)\\ j\to k\in\mathbf{E}^{\prime}\end{subarray}}\alpha_{j,k}^{1}T_{X_{k}}^{ \mathbf{L}}(1)\underbrace{\prod_{\begin{subarray}{c}k\in Ch_{\mathbf{L}}(j)\\ j\to k\notin\mathbf{E}^{\prime}\end{subarray}}\alpha_{j,k}^{0}T_{X_{j}}^{ \mathbf{L}}(0)\prod_{k\in Ch(j)\setminus C_{h\mathbf{L}}(j)}(1-\alpha_{j,k})}_{ \text{Constant coefficient}}\right)\] \[=C\times z_{i}\prod_{\begin{subarray}{c}j\in Ch_{\mathbf{L}}(i)\\ i\to j\in\mathbf{E}^{\prime}\end{subarray}}z_{j}\left(\prod_{ \begin{subarray}{c}k\in Ch_{\mathbf{L}}(j)\\ j\to k\in\mathbf{E}^{\prime}\end{subarray}}z_{k}\sum_{\mathbf{s}\in\{0,1\}^{|C _{h\mathbf{L}}(k)|}}\prod_{l\in Ch_{\mathbf{L}}(k)}\alpha_{k,l}^{s_{l}}T_{X_{l }}^{\mathbf{L}}(s_{l})\prod_{l\in Ch(k)\setminus C_{h\mathbf{L}}(k)}(1-\alpha_ {k,l})\right)\] \[=C\times z_{i}\prod_{\begin{subarray}{c}j\in Ch_{\mathbf{L}}(i)\\ i\to j\in\mathbf{E}^{\prime}\end{subarray}}z_{j}\prod_{\begin{subarray}{c}k\in Ch _{\mathbf{L}}(j)\\ j\to k\in\mathbf{E}^{\prime}\end{subarray}}z_{k}\left(\sum_{\mathbf{s}\in\{0,1 \}^{|C_{h\mathbf{L}}(k)|}}\prod_{l\in Ch_{\mathbf{L}}(k)}\alpha_{k,l}^{s_{l}}T_{ X_{l}}^{\mathbf{L}}(s_{l})\prod_{l\in Ch(k)\setminus C_{h\mathbf{L}}(k)}(1-\alpha_ {k,l})\right)\] (A.47)

The term "\(z_{i}\prod_{\begin{subarray}{c}j\in Ch_{\mathbf{L}}(i)\\ i\to j\in\mathbf{E}^{\prime}\end{subarray}}z_{j}\prod_{\begin{subarray}{c }k\in Ch_{\mathbf{L}}(j)\\ j\to k\in\mathbf{E}^{\prime}\end{subarray}}z_{k}\)" describes how \(X_{i}\) can reach its descendants \(X_{k}\), through the children \(X_{j}\). This shows that all reachable vertices are expressed in the form of the product of \(z\) variables, indicating that in the subgraph \(G(\mathbf{L},\mathbf{E}^{\prime})\), the number of paths \(X_{i}\) takes to reach its descendants \(k\in\text{Des}(i)\) is given in the power of \(z_{k}\). Thus, the corresponding term for the subgraph \(G(\mathbf{L},\mathbf{E}^{\prime})\), \(Cz_{i}\prod_{\begin{subarray}{c}j\in L\setminus\{i\}\\ j\end{subarray}}z_{j}^{p_{j}}\), is included in \(T_{X_{i}}(1)\), where \(p_{j}\) represents the number of paths from \(X_{i}\) to \(X_{j}\) in the subgraph \(G(\mathbf{L},\mathbf{E}^{\prime})\). Conversely, if such a subgraph \(G(\mathbf{L},\mathbf{E}^{\prime})\) does not exist, then \(T_{X_{i}}(1)\) will not contain such a term \(Cz_{i}\prod_{\begin{subarray}{c}j\in L\setminus\{i\}\\ j\end{subarray}}z_{j}^{p_{j}}\), since the number of directed paths from \(i\) to \(j\) is less then \(p_{j}\). This completes the proof. 

## Appendix B Proof of Identifiability

We first introduce the following necessary Lemmas for our proof.

**Lemma 3**.: _Given two vertices \(i,j\in\mathbf{V}\) and the corresponding local PGF \(G_{\mathbf{X}}^{\{i,j\}}(\mathbf{z})\), \(\frac{\partial^{2}\log G_{\mathbf{X}}^{\{i,j\}}(\mathbf{z})}{\partial z_{i} \partial z_{j}}=0\) if and only if vertex \(i\) is non-adjacent to vertex \(j\)._

**Lemma 4**.: _Given three vertices \(i,j,k\in\mathbf{V}\) and the corresponding local PGF \(G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})\), \(\frac{\partial^{2}\log G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})}{\partial z_{i} \partial z_{k}}=0\) if and only if vertices \(i,j,k\) form the structure \(i\to j\gets k\)._

**Lemma 5**.: _Given three vertices \(i,j,k\in\mathbf{V}\) and the corresponding local PGF \(G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})\), \(\frac{\partial^{2}\log G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})}{\partial z_{k} ^{2}}\neq 0\) if and only if vertices \(i,j,k\) form the structure \(i\to k\gets j\) and vertex \(i\) is adjacent to vertex \(j\)._The proofs of Lemma 3, Lemma 4, and Lemma 5 are deferred to section B.5.

### Proof of Theorem 4

**Theorem 4** (Identifiability of adjacent vertices).: _Let \(X_{i},X_{j}\in\mathbf{X}\) be two arbitrary vertices with the corresponding local PGF \(G_{\mathbf{X}}^{\{i,j\}}(\mathbf{z})\). Define the matrix \(\mathbf{A}^{\{i,j\}}=\begin{pmatrix}G_{\mathbf{X}}^{\{i,j\}}(\mathbf{z})& \frac{\partial G_{\mathbf{X}}^{\{i,j\}}(\mathbf{z})}{\partial z_{j}}\\ \frac{\partial G_{\mathbf{X}}^{\{i,j\}}(\mathbf{z})}{\partial z_{j}}&\frac{ \partial^{2}G_{\mathbf{X}}^{\{i,j\}}(\mathbf{z})}{\partial z_{i}\partial z_{j }}\end{pmatrix}\) with \(z_{i},z_{j}\) approach \(1\), the condition \(\mathrm{Rank}(\mathbf{A}^{\{i,j\}})=1\) if and only if \(X_{i}\) is non-adjacent to \(X_{j}\)._

Proof.: Considering the partial derivative of \(\log G_{\mathbf{X}}^{\{i,j\}}(\mathbf{z})\), we have

\[\frac{\partial\log G_{\mathbf{X}}^{\{i,j\}}(\mathbf{z})}{\partial z _{i}} =\frac{1}{G_{\mathbf{X}}^{\{i,j\}}(\mathbf{z})}\frac{\partial G_{ \mathbf{X}}^{\{i,j\}}(\mathbf{z})}{\partial z_{i}}\] (B.1) \[\frac{\partial^{2}\log G_{\mathbf{X}}^{\{i,j\}}(\mathbf{z})}{ \partial z_{i}\partial z_{j}} =\frac{1}{G_{\mathbf{X}}^{\{i,j\}}(\mathbf{z})}\frac{\partial^{2 }G_{\mathbf{X}}^{\{i,j\}}(\mathbf{z})}{\partial z_{i}\partial z_{j}}-\frac{1}{ \left(G_{\mathbf{X}}^{\{i,j\}}(\mathbf{z})\right)^{2}}\frac{\partial G_{ \mathbf{X}}^{\{i,j\}}(\mathbf{z})}{\partial z_{i}}\frac{\partial G_{\mathbf{X }}^{\{i,j\}}(\mathbf{z})}{\partial z_{j}}\] (B.2)

If partIf vertex \(i\) in non-adjacent to vertex \(j\), according to Lemma 3, we have \(\frac{\partial^{2}\log G_{\mathbf{X}}^{\{i,j\}}(\mathbf{z})}{\partial z_{i} \partial z_{j}}=0\), that is

\[\frac{1}{G_{\mathbf{X}}^{\{i,j\}}(\mathbf{z})}\frac{\partial^{2}G_{\mathbf{X }}^{\{i,j\}}(\mathbf{z})}{\partial z_{i}\partial z_{j}}-\frac{1}{\left(G_{ \mathbf{X}}^{\{i,j\}}(\mathbf{z})\right)^{2}}\frac{\partial G_{\mathbf{X}}^{ \{i,j\}}(\mathbf{z})}{\partial z_{i}}\frac{\partial G_{\mathbf{X}}^{\{i,j\}} (\mathbf{z})}{\partial z_{j}}=0.\] (B.3)

By rearranging the equation, we obtain:

\[G_{\mathbf{X}}^{(i,j)}(\mathbf{z})\frac{\partial^{2}G_{\mathbf{X }}^{(i,j)}(\mathbf{z})}{\partial z_{i}\partial z_{j}}-\frac{\partial G_{ \mathbf{X}}^{(i,j)}(\mathbf{z})}{\partial z_{i}}\frac{\partial G_{\mathbf{X }}^{(i,j)}(\mathbf{z})}{\partial z_{j}}=0.\] (B.4)

Notably, Eq. B.4 is the determinant of matrix \(\mathbf{A}^{\{i,j\}}=\begin{pmatrix}G_{\mathbf{X}}^{\{i,j\}}(\mathbf{z})& \frac{\partial G_{\mathbf{X}}^{\{i,j\}}(\mathbf{z})}{\partial z_{i}}\\ \frac{\partial G_{\mathbf{X}}^{\{i,j\}}(\mathbf{z})}{\partial z_{j}}&\frac{ \partial^{2}G_{\mathbf{X}}^{\{i,j\}}(\mathbf{z})}{\partial z_{i}\partial z_{j }}\end{pmatrix}\), and the \(\det(\mathbf{A}^{\{i,j\}})=0\) means that the rank of matrix \(\mathbf{A}^{\{i,j\}}\) is \(1\), i.e., \(\mathrm{Rank}\left(\mathbf{A}^{\{i,j\}}\right)=1\). This completes the proof of the if part.

Only If partIf \(\mathrm{Rank}(\mathbf{A}^{\{i,j\}})=1\), we have \(\det(\mathbf{A}^{\{i,j\}})=0\), which means:

\[G_{\mathbf{X}}^{\{i,j\}}(\mathbf{z})\frac{\partial^{2}G_{\mathbf{X}}^{\{i,j\}} (\mathbf{z})}{\partial z_{i}\partial z_{j}}-\frac{\partial G_{\mathbf{X}}^{ \{i,j\}}(\mathbf{z})}{\partial z_{i}}\frac{\partial G_{\mathbf{X}}^{\{i,j\}} (\mathbf{z})}{\partial z_{j}}=0\] (B.5)

Divide both sides by \(\left(G_{\mathbf{X}}^{\{i,j\}}(\mathbf{z})\right)^{2}\). we have:

\[\frac{1}{G_{\mathbf{X}}^{\{i,j\}}(\mathbf{z})}\frac{\partial^{2}G_{\mathbf{X }}^{\{i,j\}}(\mathbf{z})}{\partial z_{i}\partial z_{j}}-\frac{1}{\left(G_{ \mathbf{X}}^{\{i,j\}}(\mathbf{z})\right)^{2}}\frac{\partial G_{\mathbf{X}}^{ \{i,j\}}(\mathbf{z})}{\partial z_{i}}\frac{\partial G_{\mathbf{X}}^{\{i,j\}} (\mathbf{z})}{\partial z_{j}}=0\] (B.6)

Comparing to Eq. B.2, we have \(\frac{\partial^{2}\log G_{\mathbf{X}}^{\{i,j\}}(\mathbf{z})}{\partial z_{i} \partial z_{j}}=0\). Therefore, according to Lemma 3, \(X_{i}\) is non-adjacent to \(X_{j}\). This completes the proof of Theorem 4. 

### Proof of Theorem 5

**Theorem 5** (Identifiability of triangle structure).: _Let \(X_{i},X_{j},X_{k}\in\mathbf{X}\) form a triangular structure with the corresponding local PGF \(G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})\). Define the matrix 

[MISSING_PAGE_EMPTY:23]

Proof.: The proof of Theorem 6 is similar to that of Theorem 4. Consider the partial derivative of \(\log G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})\), we have

\[\frac{\partial^{2}\log G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})}{ \partial z_{i}\partial z_{k}}=\frac{1}{G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})} \frac{\partial^{2}G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})}{\partial z_{i} \partial z_{k}}-\frac{1}{\left(G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})\right)^{ 2}}\frac{\partial G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})}{\partial z_{i}}\frac {\partial G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})}{\partial z_{k}}\] (B.13)

**If part** If vertex \(i,j,k\) form the structure \(i\to j\gets k\), according to Lemma 4, we have \(\frac{\partial^{2}\log G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})}{\partial z_{i} \partial z_{k}}=0\), that is

\[\frac{1}{G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})}\frac{\partial^ {2}G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})}{\partial z_{i}\partial z_{k}}- \frac{1}{\left(G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})\right)^{2}}\frac{ \partial G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})}{\partial z_{i}}\frac{ \partial G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})}{\partial z_{k}}=0.\] (B.14)

By rearranging the equation, we obtain:

\[G_{\mathbf{X}}^{(i,j,k)}(\mathbf{z})\frac{\partial^{2}G_{\mathbf{X}}^{\{i,j, k\}}(\mathbf{z})}{\partial z_{i}\partial z_{k}}-\frac{\partial G_{\mathbf{X}}^{ \{i,j,k\}}(\mathbf{z})}{\partial z_{i}}\frac{\partial G_{\mathbf{X}}^{\{i,j, k\}}(\mathbf{z})}{\partial z_{k}}=0.\] (B.15)

Notably, Eq. B.15 is the determinant of matrix \(\mathbf{C}^{\{i,j,k\}}=\begin{pmatrix}G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})& \frac{\partial G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})}{\partial z_{i}}\\ \frac{\partial G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})}{\partial z_{k}}&\frac{ \partial^{2}G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})}{\partial z_{i}\partial z_ {k}}\end{pmatrix}\), and the \(\det(\mathbf{C}^{\{i,j,k\}})=0\) means the rank of matrix \(\mathbf{C}^{\{i,j,k\}}\) is 1, i.e., \(\text{Rank}\left(\mathbf{C}^{\{i,j,k\}}\right)=1\). This completes the proof of the if part.

Only if partIf \(\text{Rank}(\mathbf{C}^{\{i,j,k\}})=1\), we have \(\det(\mathbf{C}^{\{i,j,k\}})=0\), which means:

\[G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})\frac{\partial^{2}G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})}{\partial z_{i}\partial z_{k}}-\frac{\partial G_{\mathbf{X}}^ {\{i,j,k\}}(\mathbf{z})}{\partial z_{i}}\frac{\partial G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})}{\partial z_{k}}=0\] (B.16)

Divide both sides by \(\left(G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})\right)^{2}\). we have:

\[\frac{1}{G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})}\frac{\partial^{2}G_{\mathbf{ X}}^{\{i,j,k\}}(\mathbf{z})}{\partial z_{i}\partial z_{k}}-\frac{1}{\left(G_{ \mathbf{X}}^{\{i,j,k\}}(\mathbf{z})\right)^{2}}\frac{\partial G_{\mathbf{X}}^ {\{i,j,k\}}(\mathbf{z})}{\partial z_{i}}\frac{\partial G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})}{\partial z_{k}}=0\] (B.17)

Regarding to Eq. B.13, we have \(\frac{\partial^{2}\log G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})}{\partial z_{i} \partial z_{k}}=0\), according to Lemma 4, vertices \(i,j,k\) form the structure \(i\to j\gets k\). 

### Proof of Theorem 7

**Theorem 7** (Graphical Implication of Identifiability).: _Given a pair of adjacent vertices \(X_{i},X_{j}\in\mathbf{X}\) following PB-SCM, the causal direction of \(X_{i}\to X_{j}\) is identifiable if there exists a vertex \(X_{k}\in\mathbf{X}\setminus\{X_{i},X_{j}\}\) such that \(X_{k}\to X_{j}\)._

Proof.: Given a pair of adjacent vertices such that \(X_{i}\to X_{j}\), suppose there exists \(X_{k}\in\mathbf{X}\setminus\{X_{i},X_{j}\}\) such that \(X_{k}\to X_{j}\). We consider two cases: (i) \(X_{i}\) is adjacent to \(X_{k}\) and (ii) \(X_{i}\) is non-adjacent to \(X_{k}\): (i) If \(X_{i}\) is adjacent to \(X_{k}\), then \(X_{i},X_{j},X_{k}\) form a triangular structure where \(X_{i}\to X_{j}\gets X_{k}\). According to Theorem 5, we can identify that \(X_{j}\) is the vertex with an indegree of 2, confirming that \(X_{i}\to X_{j}\); (ii) If \(X_{i}\) is non-adjacent to \(X_{k}\), then \(X_{i},X_{j},X_{k}\) form a collider structure \(X_{i}\to X_{j}\gets X_{k}\). This structure can be identified according to Theorem 6, which completes the proof. 

### Proof of Lemma 3, 4 and 5

We first introduce some notations and necessary lemmas concerning the limits of the local PGF.

Notation.For simplify, we define \(G_{\epsilon_{i}}^{*}(\mathbf{z}_{(i)}):=G_{\epsilon_{i}}\left(z_{i}\times\prod_{j \in Ch(i)}G_{i,j}(\mathbf{z}_{(j)})\right)\) as the component involving \(\epsilon_{i}\) of PGF. According to Theorem A.1, we have \(G_{\mathbf{X}}(\mathbf{z})=\prod_{i\in[n]}G_{\epsilon_{i}}^{*}(\mathbf{z}_{(i )})\).

**Lemma 6**.: _Considering the limit of function \(G_{\epsilon_{i}}^{*}(\mathbf{z}_{(i)})\) as \(z_{i}\to 0\), we have \(\lim_{z_{i}\to 0}G_{\epsilon_{i}}^{*}(\mathbf{z}_{(i)})=e^{-\mu_{i}}\)._

Proof.: Since \(G_{\epsilon_{i}}^{*}(\mathbf{z}_{(i)})=G_{\epsilon_{i}}\left(z_{i}\times\prod _{j\in Ch(i)}G_{i,j}(\mathbf{z}_{(j)})\right)\), where \(G_{\epsilon_{i}}(\cdot)\) is the PGF of Poisson noise \(\epsilon_{i}\), we have:

\[G_{\epsilon_{i}}^{*}(\mathbf{z}_{(i)})=\lim_{z_{i}\to 0}G_{\epsilon_{i}} \left(z_{i}\times\prod_{j\in Ch(i)}G_{i,j}(\mathbf{z}_{(j)})\right)=G_{ \epsilon_{i}}(0)=e^{-\mu_{i}},\] (B.18)

which completes the proof. 

**Lemma 7**.: _Considering the limit of function \(G_{i,j}(\mathbf{z}_{(j)})\) as \(z_{j}\to 0\), we have \(\lim_{z_{j}\to 0}G_{i,j}(\mathbf{z}_{(j)})=1-\alpha_{i,j}\)._

Proof.: Since \(G_{i,j}(\mathbf{z}_{(j)})=G_{B(\alpha_{i,j})}\left(z_{j}\times\prod_{k\in Ch(j )}G_{j,k}(\mathbf{z}_{(k)})\right)\), where \(G_{B(\alpha_{i,j})}(\cdot\,)\) is the PGF of Bernoulli distribution with parameter \(\alpha_{i,j}\), we have:

\[\lim_{z_{j}\to 0}G_{B(\alpha_{i,j})}\left(z_{j}\times\prod_{k\in Ch(j)}G_{j,k}( \mathbf{z}_{(k)})\right)=G_{B(\alpha_{i,j})}(0)=1-\alpha_{i,j},\] (B.19)

which completes the proof. 

#### b.5.1 Proof of Lemma 3

Proof.: According to the definition of local PGF, we have

\[G_{\mathbf{X}}^{\{i,j\}}(\mathbf{z})=\lim_{\mathbf{z}_{\setminus\{i,j\}} \rightarrow\mathbf{0}}G_{\mathbf{X}}(\mathbf{z})=\lim_{\mathbf{z}_{\setminus\{ i,j\}}\rightarrow\mathbf{0}}\prod_{l\in[d]}G_{\epsilon_{l}}^{*}(\mathbf{z}_{(l)})\] (B.20)

According to Lemma 6, we have \(\lim_{z_{l}\to 0}G_{\epsilon_{l}}^{*}(\mathbf{z}_{(l)})=e^{-\mu_{l}}\) for \(l\neq i,j\), consequently, we have:

\[G_{\mathbf{X}}^{\{i,j\}}(\mathbf{z}) =\lim_{\mathbf{z}_{\setminus\{i,j\}}\rightarrow\mathbf{0}}G_{ \epsilon_{i}}^{*}(\mathbf{z}_{(i)})G_{\epsilon_{j}}^{*}(\mathbf{z}_{(j)})\prod _{l\in[d]\setminus\{i,j\}}G_{\epsilon_{l}}^{*}(\mathbf{z}_{(l)})\] (B.21) \[=\lim_{\mathbf{z}_{\setminus\{i,j\}}\rightarrow\mathbf{0}}G_{ \epsilon_{i}}^{*}(\mathbf{z}_{(i)})G_{\epsilon_{j}}^{*}(\mathbf{z}_{(j)})\prod _{l\in[d]\setminus\{i,j\}}\lim_{\mathbf{z}_{\setminus\{i,j\}}\rightarrow \mathbf{0}}G_{\epsilon_{l}}^{*}(\mathbf{z}_{(l)})\] \[=\lim_{\mathbf{z}_{\setminus\{i,j\}}\rightarrow\mathbf{0}}G_{ \epsilon_{i}}^{*}(\mathbf{z}_{(i)})G_{\epsilon_{j}}^{*}(\mathbf{z}_{(j)})\prod _{l\in[d]\setminus\{i,j\}}e^{-\mu_{l}}\]

If part.Suppose vertex \(i\) is not adjacent to vertex \(j\), we have:

\[\lim_{\mathbf{z}_{\setminus\{i,j\}}\rightarrow\mathbf{0}}G_{\epsilon_{i}}^{*} (\mathbf{z}_{(i)})=\lim_{\mathbf{z}_{\setminus\{i,j\}}\rightarrow\mathbf{0}}G _{\epsilon_{i}}\left(z_{i}\times\prod_{k\in Ch(i)}G_{i,k}(\mathbf{z}_{(k)}) \right).\] (B.22)

Since \(j\notin\text{Ch}(i)\), therefore \(z_{k}\in\mathbf{z}_{\setminus\{i,j\}}\) for all \(k\in\text{Ch}(i)\), we have:

\[\lim_{\mathbf{z}_{\setminus\{i,j\}}\rightarrow\mathbf{0}}G_{\epsilon_{i}}^{*} (\mathbf{z}_{(i)})=G_{\epsilon_{i}}\left(z_{i}\times\prod_{k\in Ch(i)}\lim_{ \mathbf{z}_{\setminus\{i,j\}}\rightarrow\mathbf{0}}G_{i,k}(\mathbf{z}_{(k)}) \right),\] (B.23)

where

\[\lim_{\mathbf{z}_{\setminus\{i,j\}}\rightarrow\mathbf{0}}G_{i,k}(\mathbf{z}_{ (k)})=\lim_{\mathbf{z}_{\setminus\{i,j\}}\rightarrow\mathbf{0}}G_{B(\alpha_{i, k})}\left(z_{k}\times\prod_{l\in Ch(k)}G_{k,l}(\mathbf{z}_{(l)}) \right)=G_{B(\alpha_{i,k})}(0)\] (B.24)

Substituting Eq.B.24 into Eq. B.23, we have

\[\begin{split}\lim_{\mathbf{z}_{\setminus\{i,j\}}\rightarrow \mathbf{0}}G_{\epsilon_{i}}^{*}(\mathbf{z}_{(i)})&=G_{\epsilon_{i}} \left(z_{i}\times\prod_{k\in Ch(i)}G_{B(\alpha_{i,k})}(0)\right)\\ &=\exp\left\{\mu_{i}z_{i}\times\prod_{k\in Ch(i)}(1-\alpha_{i,k})- \mu_{i}\right\}.\end{split}\] (B.25)The derivation of \(\lim_{\mathbf{z}_{\setminus\{i,j\}\to 0}}G^{*}_{\epsilon_{j}}(\mathbf{z}_{(j)})\) is similar to \(\lim_{\mathbf{z}_{\setminus\{i,j\}\to 0}}G^{*}_{\epsilon_{i}}(\mathbf{z}_{(i)})\), it follows that,

\[\lim_{\mathbf{z}_{\setminus\{i,j\}\to 0}}G^{*}_{\epsilon_{j}}(\mathbf{z}_{(j)})= \exp\left\{\mu_{j}z_{j}\times\prod\nolimits_{k\in Ch(j)}(1-\alpha_{j,k})-\mu_ {j}\right\}.\] (B.26)

Substituting Eq. B.25 and Eq. B.26 into Eq.B.21, we have

\[G^{\{i,j\}}_{\mathbf{X}}(\mathbf{z}) =\lim_{\mathbf{z}_{\setminus\{i,j\}\to 0}}G^{*}_{\epsilon_{i}}( \mathbf{z}_{(i)})G^{*}_{\epsilon_{j}}(\mathbf{z}_{(j)})\prod_{l\in[d]\setminus \{i,j\}}e^{-\mu_{l}}\] (B.27) \[=\exp\left\{\mu_{i}z_{i}\times\prod\nolimits_{k\in Ch(i)}(1- \alpha_{i,k})+\mu_{j}z_{j}\times\prod\nolimits_{k\in Ch(j)}(1-\alpha_{j,k})- \sum\limits_{l\in[d]}\mu_{l}\right\}.\]

Taking the logarithm of Eq. B.27, we obtain:

\[\log G^{\{i,j\}}_{\mathbf{X}}(\mathbf{z})=\mu_{i}z_{i}\times\prod\nolimits_{k \in Ch(i)}(1-\alpha_{i,k})+\mu_{j}z_{j}\times\prod\nolimits_{k\in Ch(j)}(1- \alpha_{j,k})-\sum\limits_{l\in[d]}\mu_{l}.\] (B.28)

Consequently, Taking the partial derivative of Eq. B.28, we finally have:

\[\frac{\partial\log G^{\{i,j\}}_{\mathbf{X}}(\mathbf{z})}{\partial z_{i}} =\mu_{i}\times\prod\nolimits_{k\in Ch(i)}(1-\alpha_{i,k}),\] (B.29) \[\frac{\partial^{2}\log G^{\{i,j\}}_{\mathbf{X}}(\mathbf{z})}{ \partial z_{i}\partial z_{j}} =0,\] (B.30)

which completes the proof of the if part.

Only if part.We prove by contradiction, i.e., suppose that \(j\) is a child vertex of vertex \(i\), we aim to prove that \(\frac{\partial^{2}\log G^{\{i,j\}}_{\mathbf{X}}(\mathbf{z})}{\partial z_{i} \partial z_{j}}\neq 0\).

We first consider \(G^{*}_{\epsilon_{i}}(\mathbf{z}_{(i)})\) in Eq. B.21, since \(j\in\text{Ch}(i)\), we have

\[\lim_{\mathbf{z}_{\setminus\{i,j\}\to 0}}G^{*}_{\epsilon_{i}}( \mathbf{z}_{(i)}) =\lim_{\mathbf{z}_{\setminus\{i,j\}\to 0}}G_{\epsilon_{i}}\left(z_{i} \times G_{i,j}(\mathbf{z}_{(j)})\times\prod\nolimits_{k\in Ch(i)\setminus j}G _{i,k}(\mathbf{z}_{(k)})\right)\] (B.31) \[=G_{\epsilon_{i}}\left(z_{i}\times\lim_{\mathbf{z}_{\setminus\{ i,j\}\to 0}}G_{i,j}(\mathbf{z}_{(j)})\times\prod\nolimits_{k\in Ch(i)\setminus j}\lim \nolimits_{\mathbf{z}_{\setminus\{i,j\}\to 0}}G_{i,k}(\mathbf{z}_{(k)})\right)\] \[=G_{\epsilon_{i}}\left(z_{i}\times\lim_{\mathbf{z}_{\setminus\{ i,j\}\to 0}}G_{i,j}(\mathbf{z}_{(j)})\times\prod\nolimits_{k\in Ch(i)\setminus j}G_{B( \alpha_{i,j})}(0)\right).\]

We next focus on \(\lim_{\mathbf{z}_{\setminus\{i,j\}\to 0}}G_{i,j}(\mathbf{z}_{(j)})\):

\[\lim_{\mathbf{z}_{\setminus\{i,j\}\to 0}}G_{i,j}(\mathbf{z}_{(j)}) =\lim_{\mathbf{z}_{\setminus\{i,j\}\to 0}}G_{B(\alpha_{i,j})}\left(z_{j} \times\prod\nolimits_{k\in Ch(j)}G_{j,k}(\mathbf{z}_{(k)})\right)\] (B.32) \[=G_{B(\alpha_{i,j})}\left(z_{j}\times\prod\nolimits_{k\in Ch(j)} \lim_{\mathbf{z}_{\setminus\{i,j\}\to 0}}G_{j,k}(\mathbf{z}_{(k)})\right)\] \[=G_{B(\alpha_{i,j})}\left(z_{j}\times\prod\nolimits_{k\in Ch(j)}G _{B(\alpha_{j,k})}(0)\right)\] \[=1-\alpha_{i,j}+\alpha_{i,j}z_{j}\prod\nolimits_{k\in Ch(j)}G_{B( \alpha_{j,k})}(0).\]

Substituting Eq. B.32 into Eq. B.31, we have:

\[\lim_{\mathbf{z}_{\setminus\{i,j\}\to 0}}G^{*}_{\epsilon_{i}}( \mathbf{z}_{(i)}) =G_{\epsilon_{i}}\left(z_{i}\times\left(1-\alpha_{i,j}+\alpha_{i,j} z_{j}\prod\nolimits_{k\in Ch(j)}G_{B(\alpha_{j,k})}(0)\right)\times\prod\nolimits_{k \in Ch(i)\setminus j}G_{B(\alpha_{i,k})}(0)\right)\] (B.33) \[=\exp\left\{\prod\nolimits_{k\in Ch(i)}(1-\alpha_{i,j})\mu_{i}z_{i }+\prod\nolimits_{k\in Ch(i)\setminus j}(1-\alpha_{i,k})\prod\nolimits_{k\in Ch (j)}(1-\alpha_{j,k})\mu_{i}\alpha_{i,j}z_{i}z_{j}-\mu_{i}\right\}.\]Next, we consider \(G^{*}_{\epsilon_{j}}(\mathbf{z}_{(j)})\) in Eq. B.34, as \(i\notin\text{Ch}(j)\), therefore \(z_{k}\in\mathbf{z}_{\setminus\{i,j\}}\) for all \(k\in\text{Ch}(j)\). Similar to the if part, we have

\[\lim_{\mathbf{z}_{\setminus\{i,j\}}\to\mathbf{0}}G^{*}_{\epsilon_{j}}( \mathbf{z}_{(j)})=\exp\left\{\prod_{k\in Ch(j)}(1-\alpha_{j,k})\mu_{j}z_{j}- \mu_{j}\right\}.\] (B.34)

Substituting Eq. B.33 and Eq. B.34 into Eq. B.21 and taking the logarithm of it, we have

\[\begin{split}\log G^{\{i,j\}}_{\mathbf{X}}(\mathbf{z})& =\prod_{k\in Ch(i)}(1-\alpha_{i,j})\mu_{i}z_{i}+\prod_{k\in Ch(j)}(1- \alpha_{j,k})\mu_{j}z_{j}\\ &\quad+\prod_{k\in Ch(i)\setminus j}(1-\alpha_{i,k})\prod_{k\in Ch (j)}(1-\alpha_{j,k})\mu_{i}\alpha_{i,j}z_{i}z_{j}-\sum_{l\in[d]}\mu_{l}.\end{split}\] (B.35)

Since the presence of \(z_{i}z_{j}\), we have

\[\frac{\partial^{2}\log G^{\{i,j\}}_{\mathbf{X}}(\mathbf{z})}{\partial z_{i} \partial z_{j}}=\prod_{k\in Ch(i)\setminus j}(1-\alpha_{i,k})\prod_{k\in Ch(j )}(1-\alpha_{j,k})\mu_{i}\neq 0,\] (B.36)

which completes the proof of the only if part. 

#### b.5.2 Proof of Lemma 4

Proof.: The proof of Lemma 4 is similar to that of Lemma 3. We aim to show that the component \(z_{i}z_{j}z_{k}\) is absent in \(\log G^{\{i,j,k\}}_{\mathbf{X}}(\mathbf{z})\) if and only if the vertices \(i,j,k\) form the structure \(i\to j\gets k\).

According to the definition of local PGF, we have

\[G^{\{i,j,k\}}_{\mathbf{X}}(\mathbf{z})=\lim_{\mathbf{z}_{\setminus\{i,j,k\}} \to\mathbf{0}}G_{\mathbf{X}}(\mathbf{z})=\lim_{\mathbf{z}_{\setminus\{i,j,k\} }\to\mathbf{0}}\prod_{l\in[d]}G^{*}_{\epsilon_{l}}(\mathbf{z}_{(l)})\] (B.37)

According to Lemma 6, we have \(\lim_{z_{l}\to 0}G^{*}_{\epsilon_{l}}(\mathbf{z}_{(l)})=e^{-\mu_{l}}\) for \(l\neq i,j,k\), consequently, we have:

\[G^{\{i,j,k\}}_{\mathbf{X}}(\mathbf{z}) =\lim_{\mathbf{z}_{\setminus\{i,j,k\}}\to\mathbf{0}}G^{*}_{ \epsilon_{i}}(\mathbf{z}_{(i)})G^{*}_{\epsilon_{j}}(\mathbf{z}_{(j)})G^{*}_{ \epsilon_{k}}(\mathbf{z}_{(k)})\prod_{l\in[d]\setminus\{i,j,k\}}G^{*}_{ \epsilon_{l}}(\mathbf{z}_{(l)})\] (B.38) \[=\lim_{\mathbf{z}_{\setminus\{i,j,k\}}\to\mathbf{0}}G^{*}_{ \epsilon_{i}}(\mathbf{z}_{(i)})G^{*}_{\epsilon_{j}}(\mathbf{z}_{(j)})G^{*}_{ \epsilon_{k}}(\mathbf{z}_{(k)})\prod_{l\in[d]\setminus\{i,j,k\}}e^{-\mu_{l}}\] (B.39)

Taking the logarithm of Eq. B.39 we have:

\[\log G^{\{i,j,k\}}_{\mathbf{X}}(\mathbf{z})=\lim_{\mathbf{z}_{\setminus\{i,j, k\}}\to\mathbf{0}}\left(\log G^{*}_{\epsilon_{i}}(\mathbf{z}_{(i)})+\log G^{*}_{ \epsilon_{j}}(\mathbf{z}_{(j)})+\log G^{*}_{\epsilon_{k}}(\mathbf{z}_{(k)}) \right)-\sum_{l\in[d]\setminus\{i,j,k\}}\mu_{l}\] (B.40)

If partSuppose \(i,j,k\) form the structure \(i\to j\gets k\). we aim to prove that \(\frac{\partial^{2}\log G^{\{i,j\}}_{\mathbf{X}}(\mathbf{z})}{\partial z_{i} \partial z_{k}}=0\) by showing that none of the functions \(\lim_{\mathbf{z}_{\setminus\{i,j,k\}}}G^{*}_{\epsilon_{i}}(\mathbf{z}_{(i)})\), \(\lim_{\mathbf{z}_{\setminus\{i,j,k\}}}G^{*}_{\epsilon_{j}}(\mathbf{z}_{(j)})\), \(\lim_{\mathbf{z}_{\setminus\{i,j,k\}}}G^{*}_{\epsilon_{k}}(\mathbf{z}_{(k)})\) in Eq. B.40 involve the component \(z_{i}z_{k}\), which implies that there is no a directed path between \(i\) and \(k\).

Since the \(j\) is the child of \(i\) and \(k\), and \(j\) has no child, similar to proof of Lemma 3, we have:

\[\lim_{\mathbf{z}_{\setminus\{i,j,k\}}\to\mathbf{0}}G^{*}_{\epsilon_{j}}(\mathbf{ z}_{(j)})=\exp\left\{\mu_{j}z_{j}\times\prod_{k\in Ch(j)}(1-\alpha_{j,k})-\mu_{j} \right\},\] (B.41)

\[\lim_{\mathbf{z}_{\setminus\{i,j,k\}}\to\mathbf{0}}G^{*}_{\epsilon_{i}}( \mathbf{z}_{(i)})=\exp\left\{\prod_{l_{1}\in Ch(i)}(1-\alpha_{i,l_{1}})\mu_{i}z _{i}+\prod_{l_{1}\in Ch(i)\setminus j}(1-\alpha_{i,l_{1}})\prod_{l_{1}\in Ch(j) }(1-\alpha_{j,l_{1}})\mu_{i}\alpha_{i,j}z_{i}z_{j}-\mu_{i}\right\},\] (B.42)

\[\lim_{\mathbf{z}_{\setminus\{i,j,k\}}\to\mathbf{0}}G^{*}_{\epsilon_{k}}( \mathbf{z}_{(k)})=\exp\left\{\prod_{l_{2}\in Ch(k)}(1-\alpha_{k,l_{2}})\mu_{k}z _{k}+\prod_{l_{2}\in Ch(k)\setminus j}(1-\alpha_{k,l_{2}})\prod_{l_{2}\in Ch(j) }(1-\alpha_{j,l_{2}})\mu_{k}\alpha_{k,j}z_{k}z_{j}-\mu_{k}\right\}.\] (B.43)Since none of Eq. B.41, Eq. B.42, Eq. B.43 contain the term involving \(z_{i}z_{k}\), the partial derivatives of the logarithms of these equations with respect to \(z_{i}\) and \(z_{k}\) are all zero. Therefore we have :

\[\frac{\partial^{2}\log G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})}{\partial z_{i} \partial z_{k}}=0,\] (B.44)

which completes the proof of the if part.

Only if partWe prove by contradiction. Suppose that \(i,j,k\) do not form the structure \(i\to j\gets k\). This implies \(i,j,k\) form either (i) the chain structure \(i\to j\to k\) or (ii) the fork structure \(i\gets j\to k\). We aim to prove that \(\frac{\partial^{2}\log G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})}{\partial z_{i} \partial z_{k}}\neq 0\).

We first discuss the case (i). If \(i,j,k\) form the chain structure \(i\to j\to k\), we prove that \(\frac{\partial^{2}\log G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})}{\partial z_{i} \partial z_{k}}\neq 0\) by showing that the \(\lim\limits_{\mathbf{z}_{\setminus\{i,j,k\}}\to\mathbf{0}}\log G_{\epsilon_ {i}}^{*}(\mathbf{z}_{(i)})\) in Eq. B.40 contains the component \(z_{i}z_{j}z_{k}\).

According the Theorem 1 and Lemma 7, we have:

\[\begin{split}\lim\limits_{\mathbf{z}_{\setminus\{i,j,k\}}\to \mathbf{0}}G_{\epsilon_{i}}^{*}(\mathbf{z}_{(i)})&=\lim\limits_{ \mathbf{z}_{\setminus\{i,j,k\}}\to\mathbf{0}}G_{\epsilon_{i}}\left(z_{i}\times \prod\nolimits_{j\in Ch(i)}G_{i,j}(\mathbf{z}_{(j)})\right)\\ &=G_{\epsilon_{i}}\left(z_{i}\times\lim\limits_{\mathbf{z}_{ \setminus\{i,j,k\}}\to\mathbf{0}}G_{i,j}(\mathbf{z}_{(j)})\times\prod \nolimits_{l\in Ch(i)\setminus\{j\}}\lim\limits_{\mathbf{z}_{\setminus\{i,j,k \}}\to\mathbf{0}}G_{i,l}(\mathbf{z}_{(l)})\right)\\ &=G_{\epsilon_{i}}\left(z_{i}\times\lim\limits_{\mathbf{z}_{ \setminus\{i,j,k\}}\to\mathbf{0}}G_{i,j}(\mathbf{z}_{(j)})\times\prod \nolimits_{l\in Ch(i)\setminus\{j\}}(1-\alpha_{i,l})\right).\end{split}\] (B.45)

Consequently, we expand the \(G_{i,j}(\mathbf{z}_{j})\) and apply the Lemma 7, we have

\[\begin{split}&\lim\limits_{\mathbf{z}_{\setminus\{i,j,k\}}\to \mathbf{0}}G_{i,j}(\mathbf{z}_{(j)})\\ &=G_{B(\alpha_{i,j})}\left(z_{j}\times G_{B(\alpha_{j,k})}\left(z _{k}\times\prod\nolimits_{n\in Ch(k)}\lim\limits_{\mathbf{z}_{\setminus\{i,j, k\}}\to\mathbf{0}}G_{k,n}(\mathbf{z}_{(n)})\right)\times\prod\nolimits_{m\in Ch (j)\setminus\{k\}}\lim\limits_{\mathbf{z}_{\setminus\{i,j,k\}}\to\mathbf{0}}G _{j,m}(\mathbf{z}_{(m)})\right)\\ &=G_{B(\alpha_{i,j})}\left(z_{j}\times G_{B(\alpha_{j,k})}\left(z _{k}\times\prod\nolimits_{n\in Ch(k)}(1-\alpha_{n,n})\right)\times\prod \nolimits_{m\in Ch(j)\setminus\{k\}}(1-\alpha_{j,m})\right)\\ &=G_{B(\alpha_{i,j})}\left(\left((1-\alpha_{j,k})z_{j}+\alpha_{j,k}z_{j}z_{k}\underbrace{\prod\nolimits_{n\in Ch(k)}(1-\alpha_{k,n})}_{:= \beta_{1}}\right)\underbrace{\prod\nolimits_{m\in Ch(j)\setminus\{k\}}(1- \alpha_{j,m})}_{:=\beta_{2}}\right)\\ &=G_{B(\alpha_{i,j})}((1-\alpha_{j,k})\beta_{2}z_{j}+\alpha_{j,k}\beta_{1}\beta_{2}z_{j}z_{k})\\ &=1-\alpha_{i,j}+(1-\alpha_{j,k})\alpha_{i,j}\beta_{2}z_{j}+ \alpha_{i,j}\alpha_{j,k}\beta_{1}\beta_{2}z_{j}z_{k}.\end{split}\] (B.46)

For simplicity, the constant coefficients in the equation are replaced with \(\beta_{1},\beta_{2}\). Substituting Eq. B.46 into Eq. B.45, we have:

\[\begin{split}\lim\limits_{\mathbf{z}_{\setminus\{i,j,k\}}\to \mathbf{0}}G_{\epsilon_{i}}^{*}(\mathbf{z}_{(i)})&=G_{\epsilon_{ i}}\left(z_{i}\times[1-\alpha_{i,j}+(1-\alpha_{j,k})\alpha_{i,j}\beta_{2}z_{j}+ \alpha_{i,j}\alpha_{j,k}\beta_{1}\beta_{2}z_{j}z_{k}]\times\prod\nolimits_{l \in Ch(i)\setminus\{j\}}(1-\alpha_{i,l})\right)\\ &=G_{\epsilon_{i}}\left([(1-\alpha_{i,j})z_{i}+(1-\alpha_{j,k}) \alpha_{i,j}\beta_{2}z_{i}z_{j}+\alpha_{i,j}\alpha_{j,k}\beta_{1}\beta_{2}z_{ i}z_{j}z_{k}]\times\prod\nolimits_{l\in Ch(i)\setminus\{j\}}(1-\alpha_{i,l}) \right).\end{split}\] (B.47)

Clearly, the equation contains the term involving \(z_{i}z_{j}z_{k}\). We have:

\[\frac{\partial^{2}\log G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})}{\partial z_{i} \partial z_{k}}=\alpha_{i,j}\alpha_{j,k}\beta_{1}\beta_{2}\prod\nolimits_{l \in Ch(i)\setminus\{j\}}(1-\alpha_{i,l})\times z_{j}\neq 0.\] (B.48)

Then we consider the case (ii). If \(i,j,k\) form the fork structure \(i\gets j\to k\),, we prove that \(\frac{\partial^{2}\log G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})}{\partial z_{i} \partial z_{k}}\neq 0\) by showing that the \(\lim\limits_{\mathbf{z}_{\setminus\{i,j,k\}}\to\mathbf{0}}\log G_{\epsilon_{j}}^ {*}(\mathbf{z}_{(j)})\) contain the component \(z_{i}z_{j}z_{k}\).

According the Theorem 1 and Lemma 7, we have:

\[\lim_{\mathbf{z}_{\setminus\{i,j,k\}}\to\mathbf{0}}G_{\epsilon_{i}}^{ *}(\mathbf{z}_{(i)}) =\lim_{\mathbf{z}_{\setminus\{i,j,k\}}\to\mathbf{0}}G_{\epsilon_{i}} \left(z_{i}\times\prod_{j\in Ch(i)}G_{i,j}(\mathbf{z}_{(j)})\right)\] (B.49) \[=G_{\epsilon_{i}}\left(z_{i}\times\lim_{\mathbf{z}_{\setminus\{i, j,k\}}\to\mathbf{0}}G_{i,j}(\mathbf{z}_{(j)})\times\lim_{\mathbf{z}_{\setminus\{i, j,k\}}\to\mathbf{0}}G_{i,k}(\mathbf{z}_{(k)})\times\prod_{l\in Ch(i)\setminus\{i,j \}}\lim_{\mathbf{z}_{\setminus\{i,j,k\}}\to\mathbf{0}}G_{i,l}(\mathbf{z}_{(l) })\right)\] \[=G_{\epsilon_{i}}\left(z_{i}\times\lim_{\mathbf{z}_{\setminus\{i, j,k\}}\to\mathbf{0}}G_{i,j}(\mathbf{z}_{(j)})\times\lim_{\mathbf{z}_{\setminus\{i, j,k\}}\to\mathbf{0}}G_{i,k}(\mathbf{z}_{(k)})\times\prod_{l\in Ch(i)\setminus\{i,j \}}(1-\alpha_{i,l})\right).\]

Consequently, we expand the \(G_{i,j}(\mathbf{z}_{j})\), \(G_{i,k}(\mathbf{z}_{k})\) and apply the Lemma 7:

\[\lim_{\mathbf{z}_{\setminus\{i,j,k\}}\to\mathbf{0}}G_{i,j}( \mathbf{z}_{(j)}) =G_{B(\alpha_{i,j})}\left(z_{j}\times\lim_{\mathbf{z}_{\setminus \{i,j,k\}}\to\mathbf{0}}\prod_{m\in Ch(j)}G_{j,m}(\mathbf{z}_{(m)})\right)\] (B.50) \[=G_{B(\alpha_{i,j})}\left(z_{j}\times\prod_{m\in Ch(j)}(1-\alpha_ {i,m})\right)\] \[=1-\alpha_{i,j}+\underbrace{\alpha_{i,j}\prod_{m\in Ch(j)}(1- \alpha_{i,m})}_{:=\beta_{1}}z_{j},\]

and

\[\lim_{\mathbf{z}_{\setminus\{i,j,k\}}\to\mathbf{0}}G_{i,k}( \mathbf{z}_{(k)}) =G_{B(\alpha_{i,k})}\left(z_{k}\times\lim_{\mathbf{z}_{\setminus \{i,j,k\}}\to\mathbf{0}}\prod_{n\in Ch(k)}G_{k,n}(\mathbf{z}_{(n)})\right)\] (B.51) \[=G_{B(\alpha_{i,k})}\left(z_{k}\times\prod_{n\in Ch(k)}(1-\alpha_ {k,n})\right)\] \[=1-\alpha_{i,k}+\underbrace{\alpha_{i,k}\prod_{n\in Ch(k)}(1- \alpha_{k,n})}_{:=\beta_{2}}z_{k}.\]

For simplicity, the constant coefficients in the equation are replaced with \(\beta_{1},\beta_{2}\). Substituting Eq. B.50 and Eq. B.51 into Eq. B.49, we have

\[\lim_{\mathbf{z}_{\setminus\{i,j,k\}}\to\mathbf{0}}G_{\epsilon_{i}}^{*}( \mathbf{z}_{(i)})=G_{\epsilon_{i}}\left(z_{i}\times(1-\alpha_{i,j}+\beta_{1}z_ {j})\times(1-\alpha_{i,k}+\beta_{2}z_{k})\times\prod_{l\in Ch(i)\setminus\{i,j,k\}}(1-\alpha_{i,l})\right).\] (B.52)

Clearly, the equation contains the term \(z_{i}z_{j}z_{k}\), Then we have

\[\frac{\partial^{2}\log G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})}{\partial z_{i} \partial z_{k}}=\beta_{1}\beta_{2}\prod_{l\in Ch(i)\setminus\{i,j,k\}}(1- \alpha_{i,l})\times z_{j}\neq 0,\] (B.53)

which completes the proof of the only if part. 

#### b.5.3 Proof of Lemma 5

Proof.: According to the Theorem 3, we have

\[\log G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z}) =\log\prod_{l\in\{i,j,k\}}\exp\left\{\mu_{l}\times\left(T_{X_{l}} ^{\{i,j,k\}}(1)-1\right)\right\}\prod_{l\in[d]\setminus\{i,j,k\}}\exp\{-\mu_{l}\}\] (B.54) \[=\mu_{i}\left(T_{X_{i}}^{\{i,j,k\}}(1)-1\right)+\mu_{j}\left(T_{X_ {j}}^{\{i,j,k\}}(1)-1\right)+\mu_{k}\left(T_{X_{k}}^{\{i,j,k\}}(1)-1\right)- \sum_{l\in[d]\setminus\{i,j,k\}}\mu_{l}\] \[=\mu_{i}T_{X_{i}}^{\{i,j,k\}}(1)+\mu_{j}T_{X_{j}}^{\{i,j,k\}}(1)+ \mu_{k}T_{X_{k}}^{\{i,j,k\}}(1)-\sum_{l\in[d]}\mu_{l}.\]

If partSuppose that \(i,j,k\) form the structure \(i\to k\gets j\) and \(i\to j\), where vertex \(i\) has two directed path lead to vertex \(j\). We aim to demonstrate that \(T_{X_{i}}^{\{i,j,k\}}(1)\) contain the term involving \(z_{i}z_{j}z_{k}^{2}\) such that \(\frac{\partial^{2}z_{i}z_{j}z_{k}^{2}}{\partial z_{k}^{2}}=2z_{i}z_{j}\neq 0\). Consequently, it follows that \(\frac{\partial^{2}\log G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})}{\partial z_{k}^{2}}\neq 0\).

According to the Theorem 3, \(T_{X_{i}}^{\{i,j,k\}}(1)\) is expressed as follows:

\[T_{X_{i}}^{\{i,j,k\}}(1)=z_{i}\sum_{\mathbf{s}\in\{0,1\}^{|\mathcal{O}_{\{i,j,k \}}(i)|}}\prod_{j\in\text{\it Ch}_{\{i,j,k\}}(i)}\alpha_{i,j}^{s_{j}}T_{X_{j}}^{ \{i,j,k\}}(s_{j})\prod_{j\in Ch(i)\setminus\text{\it Ch}_{\{i,j,k\}}(i)}(1- \alpha_{i,j}),\] (B.55)

where \(\text{\it Ch}_{\{i,j,k\}}(i)=\{j,k\}\) in this structure. For simplicity, let \(\beta_{1}=\prod_{j\in Ch(i)\setminus\text{\it Ch}_{\{i,j,k\}}(i)}(1-\alpha_{i,j})\), we have:

\[T_{X_{i}}^{\{i,j,k\}}(1)=\beta_{1}z_{i}\sum_{\mathbf{s}\in\{0,1\}^{2}}\alpha_ {i,j}^{s_{j}}T_{X_{j}}^{\{i,j,k\}}(s_{j})\alpha_{i,k}^{s_{k}}T_{X_{k}}^{\{i,j, k\}}(s_{k}).\] (B.56)

We consider the term in B.56 where both \(s_{j}=1\) and \(s_{k}=1\), which is given by:

\[\alpha_{i,j}^{s_{j}=1}T_{X_{j}}^{\{i,j,k\}}(1)\alpha_{i,k}^{s_{k}=1}T_{X_{k}} ^{\{i,j,k\}}(1)=\alpha_{i,j}\alpha_{i,k}T_{X_{j}}^{\{i,j,k\}}(1)T_{X_{k}}^{\{i,j,k\}}(1)\] (B.57)

Consequently, since vertex \(j\) has child \(k\) and vertex \(k\) has no child in this structure, we have:

\[T_{X_{k}}^{\{i,j,k\}}(1) =z_{k}\prod_{l\in Ch(k)}(1-\alpha_{k,l})\] (B.58) \[T_{X_{j}}^{\{i,j,k\}}(1) =z_{j}\sum_{\mathbf{s}\in\{0,1\}}\alpha_{j,k}^{s_{k}}T_{X_{k}}^{ \{i,j,k\}}(s_{k})\prod_{l\in Ch(j)\setminus\text{\it Ch}_{\text{\it L}}(j)}(1- \alpha_{j,l})\] \[=\left(z_{j}\alpha_{j,k}T_{X_{k}}^{\{i,j,k\}}(1)+(1-\alpha_{j,k}) T_{X_{k}}^{\{i,j,k\}}(0)\right)\prod_{l\in Ch(j)\setminus\text{\it Ch}_{ \text{\it L}}(j)}(1-\alpha_{j,l})\] \[=\left(1-\alpha_{j,k}+\alpha_{j,k}z_{j}z_{k}\prod_{l\in Ch(k)}(1- \alpha_{k,l})\right)\prod_{l\in Ch(j)\setminus\text{\it Ch}_{\text{\it L}}(j) }(1-\alpha_{j,l}).\]

By combining Eq. B.56, B.57, and B.58, we can observe that \(z_{i}z_{j}z_{k}^{2}\) is present. Therefore \(\frac{\partial^{2}\log G_{X_{k}}^{\{i,j,k\}}(\mathbf{z})}{\partial z_{k}^{2}}\neq 0\), this complete the if part.

Only if partWe prove by contradiction. Suppose that vertices \(i,j,k\) do not form the triangular structure that \(i\to k\gets j\) and \(i\) is adjacent to \(j\), we aim to prove that \(\frac{\partial^{2}\log G_{X_{k}}^{\{i,j,k\}}(\mathbf{z})}{\partial z_{k}^{2}}=0\). This setup implies two cases: (i) \(i,j,k\) do not form a triangular structure, (ii) vertices \(i,j,k\) form the triangular structure where the vertex has the indegree of \(2\) is not \(k\).

We first consider the case (i). When \(i,j,k\) do not form a triangular structure, they could instead form a collider, chain, or fork structure. According to Lemma 4 and the intermediate results derived in its proof, in each of these structures, the local PGFs do not contain a term involving \(z_{i}z_{j}z_{k}^{2}\) such that \(\frac{\partial^{2}\log G_{X_{k}}^{\{i,j,k\}}(\mathbf{z})}{\partial z_{k}^{2}}=0\), which creates a contradiction.

Then we consider the case (ii). Suppose that vertices \(i,j,k\) form the triangular \(k\to i\gets j\) and \(j\to k\). Recall the Eq. B.54, the \(\log G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})\) is given by:

\[\log G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})=\mu_{i}T_{X_{i}}^{\{i,j,k\}}(1)+\mu _{j}T_{X_{j}}^{\{i,j,k\}}(1)+\mu_{k}T_{X_{k}}^{\{i,j,k\}}(1)-\sum_{l\in[d]}\mu _{l}.\] (B.59)

Our goal is to show that none of the terms \(T_{X_{i}}^{\{i,j,k\}}(1)\), \(T_{X_{j}}^{\{i,j,k\}}(1)\) and \(T_{X_{k}}^{\{i,j,k\}}(1)\) include a component involving \(z_{i}z_{j}z_{k}^{2}\).

We first consider \(T_{X_{i}}^{\{i,j,k\}}(1)\). Since vertex \(i\) has no child in this structure, we have:

\[T_{X_{i}}^{\{i,j,k\}}(1)=z_{i}\prod_{l\in Ch(i)}(1-\alpha_{k,l}),\] (B.60)

which only involves \(z_{i}\).

Next, we consider the \(T_{X_{k}}^{\{i,j,k\}}(1)\), since vertex \(k\) has only one child \(i\) in this structure, we have:

\[T_{X_{k}}^{\{i,j,k\}}(1) =z_{k}\sum_{\mathbf{s}\in\{0,1\}^{2}}\alpha_{k,i}^{s_{i}}T_{X_{i}} ^{\{i,j,k\}}(s_{i})\prod_{l\in Ch(k)\setminus\{i\}}(1-\alpha_{k,l})\] (B.61) \[=\left(z_{k}\alpha_{k,i}T_{X_{i}}^{\{i,j,k\}}(1)+z_{k}(1-\alpha_ {k,i})T_{X_{i}}^{\{i,j,k\}}(0)\right)\prod_{l\in Ch(k)\setminus\{i\}}(1- \alpha_{k,l})\] \[=\left(z_{k}(1-\alpha_{k,i})+\alpha_{k,i}\prod_{l\in Ch(i)}(1- \alpha_{k,l})z_{i}z_{k}\right)\prod_{l\in Ch(k)\setminus\{i\}}(1-\alpha_{k,l}),\]

which leads to terms involving \(z_{k}\) and \(z_{i}z_{k}\).

Finally, we consider \(T_{X_{j}}^{\{i,j,k\}}(1)\). Since vertex \(j\) has two children \(i\) and \(k\) in this structure, similar to the if part, we have:

\[T_{X_{j}}^{\{i,j,k\}}(1)=z_{j}\sum_{\mathbf{s}\in\{0,1\}^{2}}\alpha_{j,i}^{s_ {j}}T_{X_{i}}^{\{i,j,k\}}(s_{i})\alpha_{j,k}^{s_{k}}T_{X_{k}}^{\{i,j,k\}}(s_{k })\prod_{j\in Ch(i)\setminus\mathcal{C}_{\{i,j,k\}}(i)}(1-\alpha_{i,j})\] (B.62)

In analyzing the equation, we will focus on the terms \(z_{j}\sum\limits_{\mathbf{s}\in\{0,1\}^{2}}T_{X_{i}}^{\{i,j,k\}}(s_{i})T_{X_{ k}}^{\{i,j,k\}}(s_{k})\), treating the other components as constants for simplicity. We have:

\[z_{j}\sum_{\mathbf{s}\in\{0,1\}^{2}}T_{X_{i}}^{\{i,j,k\}}(s_{i}) T_{X_{k}}^{\{i,j,k\}}(s_{k}) =z_{j}T_{X_{i}}^{\{i,j,k\}}(0)T_{X_{k}}^{\{i,j,k\}}(0)+z_{j}T_{X_{i }}^{\{i,j,k\}}(1)T_{X_{k}}^{\{i,j,k\}}(0)\] \[\quad+z_{j}T_{X_{i}}^{\{i,j,k\}}(0)T_{X_{k}}^{\{i,j,k\}}(1)+z_{j} T_{X_{i}}^{\{i,j,k\}}(1)T_{X_{k}}^{\{i,j,k\}}(1)\] (B.63)

Since \(T_{X_{i}}^{\{i,j,k\}}(0)=T_{X_{k}}^{\{i,j,k\}}(0)=1\), we have:

\[z_{j}\sum_{\mathbf{s}\in\{0,1\}^{2}}T_{X_{i}}^{\{i,j,k\}}(s_{i}) T_{X_{k}}^{\{i,j,k\}}(s_{k}) =z_{j}+z_{j}T_{X_{i}}^{\{i,j,k\}}(1)+z_{j}T_{X_{k}}^{\{i,j,k\}}(1)+z_{j}T_{X_{i }}^{\{i,j,k\}}(1)T_{X_{k}}^{\{i,j,k\}}(1)\] (B.64)

According to previous discussion, we have \(T_{X_{i}}^{\{i,j,k\}}(1)\) involves \(z_{i}\), \(T_{X_{k}}^{\{i,j,k\}}(1)\) involves \(z_{k}\) and \(z_{i}z_{k}\). Consequently, we derive the following result:

* \(z_{j}T_{X_{i}}^{\{i,j,k\}}(1)\) involves \(z_{i}z_{j}\),
* \(z_{j}T_{X_{k}}^{\{i,j,k\}}(1)\) involves \(z_{j}z_{k}\) and \(z_{i}z_{j}z_{k}\),
* \(z_{j}T_{X_{i}}^{\{i,j,k\}}(1)T_{X_{k}}^{\{i,j,k\}}(1)\) involves \(z_{i}z_{j}z_{k}\) and \(z_{i}^{2}z_{j}z_{k}\).

We can observe that \(z_{k}\) appears only to the first power within Eq. B.61, and B.62. In other words, \(\log G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})\) does not contain any term involving \(z_{k}^{2}\). Therefore, the second partial derivative of \(\log G_{\mathbf{X}}^{\{i,j,k\}}(\mathbf{z})\) with respect to \(z_{k}\) equal to zero, which creates a contradiction. This completes the only if part.

Additional Experiment Details

Each experiment reported in the main paper was conducted on a 12th Gen Intel(R) Core(TM) i3-12100 CPU with 16GB RAM, without the use of a GPU. The runtime for each experiment is provided to facilitate reproduction under similar conditions. The significance level (alpha) for the rank hypothesis test used in these experiments is set at 0.01.

Additional MetricsThe main paper presents the F1 scores and Structural Hamming Distance (SHD) from synthetic data experiments. This section extends these results by providing Precision, Recall, and runtime metrics for each experiment, as detailed in the following Table.4, Table. 5 and Table. 6.

## Appendix D Additional Discussion

### Discussion on the Benefit of Local PGF

In this section, we illustrate the benefit of introducing local PGF through a toy example. First, it is important to note that, based on the closed form of the PGF, it is theoretically possible to identify causal structures using the original PGF, as the closed form encapsulates the entire graph structure. This could be achieved by exhaustively analyzing all terms present in the PGF. However, such an approach is intractable because the search space for these terms grows exponentially with the number of vertices. Additionally, the method would involve high-order differentiations, which are difficult to estimate accurately.

To illustrate this point, we provide a toy example with a causal structure of 5 vertices, as shown in Fig. 6. In this example, we focus on identifying the edge \(X_{4}\to X_{5}\). For the correct structure where \(X_{4}\to X_{5}\), the terms \(C_{1}\times z_{1}z_{2}z_{3}z_{4}^{2}z_{5}^{2}\) and \(C_{2}\times z_{1}z_{2}z_{3}z_{4}^{2}z_{5}^{4}\) exist in the PGF. Conversely, for the reverse direction \(X_{4}\gets X_{5}\), the terms \(C_{1}^{\prime}\times z_{1}z_{2}z_{3}z_{4}^{2}z_{5}^{2}\) and \(C_{2}^{\prime}\times z_{1}z_{2}z_{3}z_{4}^{4}z_{5}^{2}\) appear in the PGF.

Notably, as shown in Fig. 6 (c) and (f), the term \(z_{1}z_{2}z_{3}z_{4}^{2}z_{5}^{2}\) exists in the PGF for both structures. This is because there are always at least two directed paths from \(X_{1}\) to each of \(X_{4}\) and \(X_{5}\), regardless of the direction between \(X_{4}\) and \(X_{5}\). This implies that taking the second derivative with respect to \(z_{4}\) does not reveal any asymmetry between the structures. Therefore, if we want to identify the direction using the global PGF, we have to apply a test involving the third derivative that distinguishes the correct structure by showing the absence of terms with \(z_{4}^{3}\) in the PGF for the correct direction.

In conclusion, when the graph structure becomes more complex and the number of paths between vertices increases, higher-order derivatives are required, making implementation challenging. Local PGF offers an alternative approach, enabling us to avoid these difficulties.

\begin{table}
\begin{tabular}{c c c c c c c} \hline  & Ours & Cumulant & PC & GES & OCD \\ \hline Runtime (second) & \(7.94\pm 0.75\) & \(77.07\pm 4.94\) & \(5.00\pm 1.35\) & \(6.90\pm 1.86\) & \(9216\pm 1368\) \\ \hline \end{tabular}
\end{table}
Table 6: Runtime of each method under the default setting.

\begin{table}
\begin{tabular}{c|c c c|c c c c} \hline  & \multicolumn{4}{c}{Recall!} & \multicolumn{4}{c}{Precision!} \\ \hline Avg. Indegree & 2.0 & 2.5 & 3.0 & 3.5 & 2.0 & 2.5 & 3.0 & 3.5 \\ \hline Ours & \(0.88\pm 0.07\) & \(0.92\pm 0.08\) & \(0.91\pm 0.04\) & \(0.92\pm 0.05\) & \(\mathbf{0.63\pm 0.06}\) & \(\mathbf{0.73\pm 0.06}\) & \(\mathbf{0.81\pm 0.04}\) & \(\mathbf{0.87\pm 0.04}\) \\ Cumulant & \(\mathbf{0.96\pm 0.04}\) & \(\mathbf{0.98\pm 0.02}\) & \(\mathbf{0.96\pm 0.03}\) & \(\mathbf{0.98\pm 0.01}\) & \(0.58\pm 0.03\) & \(0.63\pm 0.03\) & \(0.69\pm 0.04\) & \(0.72\pm 0.04\) \\ PC & \(0.64\pm 0.20\) & \(0.66\pm 0.13\) & \(0.57\pm 0.13\) & \(0.63\pm 0.13\) & \(0.56\pm 0.16\) & \(0.58\pm 0.10\) & \(0.52\pm 0.12\) & \(0.58\pm 0.10\) \\ GES & \(0.55\pm 0.15\) & \(0.56\pm 0.11\) & \(0.47\pm 0.12\) & \(0.41\pm 0.11\) & \(0.43\pm 0.14\) & \(0.43\pm 0.11\) & \(0.37\pm 0.10\) & \(0.34\pm 0.09\) \\ OCD & \(0.24\pm 0.22\) & \(0.27\pm 0.23\) & \(0.28\pm 0.16\) & \(0.36\pm 0.14\) & \(0.23\pm 0.21\) & \(0.27\pm 0.23\) & \(0.27\pm 0.17\) & \(0.37\pm 0.14\) \\ \hline \end{tabular}
\end{table}
Table 4: Sensitivity to Avg. Indegree Rate.

\begin{table}
\begin{tabular}{c|c c c c|c c c c} \hline  & \multicolumn{4}{c}{Recall!} & \multicolumn{4}{c}{Precision!} \\ \hline Sample Size & 5000 & 15000 & 30000 & 50000 & 5000 & 15000 & 30000 & 50000 \\ \hline Ours & \(0.78\pm 0.11\) & \(0.87\pm 0.05\) & \(0.91\pm 0.04\) & \(0.92\pm 0.05\) & \(\mathbf{0.72\pm 0.08}\) & \(\mathbf{0.77\pm 0.04}\) & \(\mathbf{0.81\pm 0.04}\) & \(\mathbf{0.82\pm 0.04}\) \\ Cumulant & \(\mathbf{0.92\pm 0.07}\) & \(\mathbf{0.96\pm 0.03}\) & \(\mathbf{0.97\pm 0.03}\) & \(\mathbf{0.97\pm 0.03}\) & \(\mathbf{0.60\pm 0.05}\) & \(0.65\pm 0.03\) & \(0.69\pm 0.01\) & \(0.68\pm 0.04\) \\ PC & \(0.43\pm 0.10\) & \(0.56\pm 0.11\) & \(0.57\pm 0.14\) & \(0.71\pm 0.11\) & \(0.46\pm 0.13\) & \(0.53\pm 0.11\) & \(0.52\pm 0.12\) & \(0.62\pm 0.08\) \\ GES & \(0.41\pm 0.11\) & \(0.48\pm 0.20\) & \(0.47\pm 0.13\) & \(0.48\pm 0.22\) & \(0.38\pm 0.10\) & \(0.41\pm 0.20\) & \(0.37\pm 0.10\) & \(0.39\pm 0.22\) \\ OCD & \(0.28\pm 0.10\) & \(0.35\pm 0.18\) & \(0.28\pm 0.16\) & \(0.39\pm 0.21\) & \(0.34\pm 0.14\) & \(0.35\pm 0.19\) & \(0.27\pm 0.17\) & \(0.36\pm 0.20\) \\ \hline \end{tabular}
\end{table}
Table 5: Sensitivity to Sample Size.

### Discussion on the connection with the cumulant-based method

In this section, we discuss the connection between the cumulant-based method and our method, as well as the advantages of our method.

The cumulant-based method (Qiao et al. [2024b]) introduces the concept of \(k\)-path cumulants summation, denoted as \(\tilde{\Lambda}_{k}(X_{i}\rightsquigarrow X_{j})\). Here, \(\tilde{\Lambda}_{k}(X_{i}\rightsquigarrow X_{j})\neq 0\) indicates the existence of a common ancestor \(m\) for vertices \(i\) and \(j\), with \(k\) directed paths leading from \(m\) to \(j\). Such path information, specifically the number of directed paths from one vertex to another, is encoded within the PGF. If there are \(k\) directed paths from vertex \(m\) to \(j\), then terms involving \(z_{m}z_{j}^{k}\) will appear in the PGF. Therefore, detecting the number of directed paths to vertex \(j\) is equivalent to identifying the order of \(z_{j}\) in the PGF.

However, only the highest non-zero order of \(\tilde{\Lambda}_{k}(X_{i}\rightsquigarrow X_{j})\) is useful for identifying the causal direction, as it does not yield asymmetry at lower orders. For example, consider vertices \(X_{i}\), \(X_{j}\), and \(X_{m}\), where \(X_{i}\to X_{j}\), and there are \(k\) directed paths from \(X_{m}\) to \(X_{i}\), and \(k+p\) directed paths from \(X_{m}\) to \(X_{j}\) where \(p\geq 1\). In this scenario, it has \(\tilde{\Lambda}_{k}(X_{i}\rightsquigarrow X_{j})\neq 0\) and \(\tilde{\Lambda}_{k}(X_{j}\rightsquigarrow X_{i})\neq 0\), and \(\tilde{\Lambda}_{k+1}(X_{i}\rightsquigarrow X_{j})\neq 0\) while \(\tilde{\Lambda}_{k+1}(X_{j}\rightsquigarrow X_{i})=0\). The latter shows an asymmetry that the former does not. This means that the cumulant-based method must detect the highest non-zero order of \(\tilde{\Lambda}_{k}(X_{j}\rightsquigarrow X_{i})\), i.e., the highest order of \(z_{j}\) in the corresponding terms in the PGF, to reveal the asymmetry. Moreover, certain unshielded collider structures, such as \(X_{1}\to X_{2}\) and \(X_{3}\to X_{2}\) in Fig. 1(a) of the paper, are non-identifiable using this method. This is because both the correct and reverse directions result in \(\tilde{\Lambda}_{k=1}\neq 0\) and \(\tilde{\Lambda}_{k=2}=0\), which leads to non-identifiability.

In contrast, our method can fully leverage lower-order information to identify causal directions due to the local property of the PGF. By removing redundant directed paths involving a vertex through setting the corresponding \(z\) to approach zero in the PGF, we can focus on identifying within a small local structure, thereby avoiding the need for high-order information.

## Appendix E Broader Impacts

We identify several important societal impacts of our proposed method, including both positive and potential negative impacts:

Figure 6: Illustration of causal graphs and their possible branching structures with the corresponding terms in PGF. (a) Correct causal graph where \(X_{4}\to X_{5}\). (b)-(c) Branching structures of (a) with corresponding PGF terms: (b) includes all paths; (c) excludes path from \(X_{4}\) to \(X_{5}\). (d) Causal graph with reversed direction \(X_{5}\to X_{4}\). (e)-(f) Branching structures of (d) with corresponding PGF terms: (e) includes all paths; (f) excludes path from \(X_{5}\) to \(X_{4}\).

1. This paper introduces advancements in the modeling and identification of causal relationships from count data, thereby revealing the causal mechanism of Poisson count data.
2. Inadequate data and training can result in inaccurate causal graphs, potentially leading to a misunderstanding of the underlying causal relationships. Such misunderstandings may prompt inappropriate or risky decision-making by stakeholders relying on these insights.

To mitigate the potential negative societal impacts mentioned above, we encourage research and practice to follow these instructions:

1. Integration of human oversight is recommended, where domain experts should verify and complement model outputs with their expertise to guide decision-making effectively.
2. Continuous monitoring and updating of model parameters should be implemented to align with real-world data and expert feedback, ensuring the accuracy and applicability of causal predictions.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We emphasize our contributions in the abstract and introduction.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have discussed the limitations of our works in the Section 6
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We have provided the full set of assumptions and a complete proof.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide detailed experimental parameters used in synthetic experiments and the web address where the real-world data can be accessed.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The code will be available as open access through a GitHub repository. We will also provide detailed instructions for reproducing the experimental results, ensuring that all materials necessary for replication are accessible.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide the alpha value of the hypothesis test for learning the causal structure in the Appendix C.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The paper appropriately reports the standard deviation (std) for each experiment.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes]Justification: Yes, our paper provides comprehensive details on the computer resources required to reproduce the experiments. All pertinent information is included in the Appendix C.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The dataset used in the paper is properly cited.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We have provided the broader impacts in Appendix E.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our paper does not involve data or models that have a high risk for misuse, such as pre-trained language models, image generators, or scraped datasets. Consequently, the specific safeguards typically necessary for such contexts are not relevant to our research.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The paper correctly credits the creators or original owners of all assets used, including code, data, and models.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The Python implementation of our proposed method is well documented, with detailed documentation to be released alongside the code upon acceptance of the paper.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects.