The Implicit Bias of Gradient Descent toward Collaboration between Layers: A Dynamic Analysis of Multilayer Perceptions

 Zheng Wang  Geyong Min

Department of Computer Science

University of Exeter

{zw360;G.Min}@exeter.ac.uk

&Wenjie Ruan

School of Computer Science

USTC

rwjie@ustc.edu.cn

Corresponding Author

###### Abstract

The _implicit bias_ of _gradient descent_ has long been considered the primary mechanism explaining the superior _generalization_ of over-parameterized neural networks without overfitting, even when the training error is zero. However, the implicit bias toward _adversarial robustness_ has rarely been considered in the research community, although it is crucial for the _trustworthiness_ of machine learning models. To fill this gap, in this paper, we explore whether layers in _neural networks_ collaborate to strengthen adversarial robustness during gradient descent. By quantifying this collaboration between layers using our proposed concept, _co-correlation_, we demonstrate a monotonically increasing trend in co-correlation, which implies a decreasing trend in adversarial robustness during gradient descent. Additionally, we observe different behaviours between narrow and wide neural networks during gradient descent. We conducted extensive experiments that verified our proposed theorems.

## 1 Introduction

As Artificial Intelligence (AI) has been widely applied in many industrial sectors, understanding the theoretical properties behind modern machine learning models is important, especially for neural networks due to their black-box nature. One such property is _implicit bias_, stemming from the phenomenon where over-parameterized neural networks, trained in a _gradient descent_ manner, often exhibit great generalization without over-fitting. This implicit bias of gradient descent is often explained as steering neural networks towards solutions characterized by max-margin [3; 23; 12].

Another intriguing phenomenon is the existence of adversarial examples -- imperceptible perturbations of inputs that alter classification results. Apart from existing work on _attacks_ -- algorithms for generating adversarial examples [5; 39; 37], _defences_ against attacks, e.g., adversarial training [25; 36] and distillation [28], and _verification_[40; 34] to identify safe regions guaranteeing the absence of adversarial examples, recently some works are aiming at a theoretical understanding behind adversarial robustness. However, a principled way to comprehend the core contributors to the vulnerability of neural networks, especially a theoretical understanding of their relation to generalization capabilities, remains fragmented. This fragmentation is largely due to the intricate nature of neural networks, where robustness is interconnected with many factors spanning across input data distribution [33; 14], sampling complexity [1; 27], optimization techniques [25], weight initialization strategies [41], and model capacity and architectures [32; 2; 18; 35]. Not to mention that only very few works can address both generalization and adversarial robustness in a uniform framework. One such research by Frei et al. [13] investigates the _implicit bias_ concerning both _generalization_ and _adversarial robustness_,asserting that while implicit bias leads to solutions with improved generalization, it results in weaker adversarial robustness. However, this work ignores the architectural factor of neural networks and is hard to generalize to neural networks with more than two layers.

Inspired by this work and to investigate whether neural network layers collaborate against adversarial examples during training, this paper first adopts the novel concept of Dirichlet energy--originating from Partial Differential Equations (PDEs) to assess the variability of a function [11]--to evaluate the adversarial robustness of neural networks. We then theoretically demonstrate that Dirichlet energy serves as an effective measure of adversarial risk. By decomposing the Dirichlet energy across the entire neural network into its constituent layers, we can quantify the interactions between adjacent layers concerning adversarial robustness. We term this interaction _collaboration correlation (co-correlation)_ and find that this metric reflects _'alignments'_ in feature selection between neighbouring layers. Furthermore, we conduct a dynamic analysis of co-correlation in two-layer MLPs, demonstrating with high probability a monotonic increase under gradient descent, which indicates diminishing adversarial robustness. Additionally, our experiments show that two-layer MLPs with small widths tend to enhance their performance through strengthened co-correlation, a pattern not observed in wide two-layer MLPs. Our key contributions in this paper can be summarized as follows:

1. To the best of our knowledge, this work is the first to study the implicit bias of interaction between layers. We have quantified the interactions between adjacent layers and theoretically demonstrated that co-correlation between layers strengthens during gradient descent in neural networks under mild assumptions, suggesting that it not only fails to collaborate against adversarial perturbations but may even hinder resistance to them during gradient descent.
2. We demonstrate how neural networks with a large width differ in behaviour from the neural network of a small width, showing that MLPs with larger widths exhibit more resistance to increased co-correlation and, therefore, are more adversarial robust, which is complementary for the work by Dohmatob and Bietti [8].
3. Extensive experiments have been conducted to validate our proposed framework. By controlling the weight initialization, a perspective also suggested by Zhu et al. [41], we challenge the argument, as proposed by Huang et al. [18], that a wide neural network does not necessarily lead to better adversarial robustness, through the lens of cross-layer collaboration.

## 2 Related Works

### Implicit Bias of Gradient Descent

The mystery of over-parameterized neural network trained with _gradient descent_ manner hardly over-fitting has long been studied. Chizat and Bach [3] study the two-layer neural network with infinite width and homogeneous activations, showing that gradient flow can be characterized as a max-margin classifier on exponentially tailed losses. Lyu et al. [24], Sarussi et al. [29] study the two-layer Leaky ReLU neural network on linearly separable data and claim that networks converge to a max-margin linear predictor by gradient descent manner. Frei et al. [12] confirms those claims on high-dimensional nearly orthogonal data.

Lyu and Li [23], Ji and Telgarsky [19] claims that the homogeneous neural networks with exponentially-tailed classification losses converge to a _KKT_ point of a maximum-margin problem. Kunin et al. [21] extend these results to a more boarder family of _quasi-homogeneous_ neural networks. A more recent research [13] considers both generalization and robustness for two-layer ReLU neural networks, arguing that gradient descent is biased towards solutions that generalise well but are more vulnerable against adversarial examples, even the neural network is highly over-parameterized.

### Theoretical Investigation of Adversarial Robustness

Since the phenomenon of adversarial examples has been discovered [15], various works have been proposed to understand the theoretical fundamentals behind it, especially for neural networks. Some researchers argue that the source of adversarial vulnerability comes from the input data [33; 8; 31; 26; 14; 7; 30; 1; 27]. The more recent researches investigate the fragility of neural networks from an architectural perspective. Simon-Gabriel et al. [32] study the vulnerability of feed-forward neural networks measured by \(L_{p}\) norm of the loss function w.r.t. input data, suggesting that the vulnerability increases with input dimension independent of model structures. Daniely and Shacham [6] examined the ReLU neural network characterized by decreasing dimensions at each layer. They asserted that the manifestation of adversarial robustness is intrinsically tied to the network's architecture, which contrasts with the propositions put forth by Simon-Gabriel et al. [32]. Bubeck et al. [2] expanded the findings of Daniely and Shacham's work on two-layer neural networks from an "under-complete case" scenario to an "over-complete" one where the number of neurons surpasses the input dimension. They further broadened the conclusions drawn by Daniely and Shacham [6] and Bubeck et al. [2] to encompass Deep ReLU networks, hinting at a crucial role played by bottleneck layers in these networks. Zhu et al. [41], instead of merely considering random weights as the standard configuration, conducted a comprehensive analysis of the effects of weight initialization on adversarial robustness.

Unlike previous studies that focus solely on the overall assessment of neural networks while overlooking layer interactions, our research examines the synergistic involvement between layers within neural networks, taking into account both weight initialization and optimization.

## 3 Preliminary

### General Setting

We follow the binary classification setting where the input data is \(\mathcal{X}\subseteq\mathbb{R}^{d}\), with the label \(\mathcal{Y}\subseteq\{0,1\}\). Given data set \(\mathcal{D}=\{(\bm{x}_{i},y_{i})\}_{i=1}^{n}\) drawn from an unknown probability measure \(P\) on \(\mathcal{X}\times\mathcal{Y}\) and the neural network \(f:\mathcal{X}\times\Theta\rightarrow\mathbb{R}\), where \(\Theta\) denotes the set of parameters, our objective is to optimize \(f\) by updating the weights with _gradient descent_ method such that it can predict the label accurately. The prediction result is shown in Equation (1).

\[y_{pred}=\begin{cases}1,\mathit{sig}(f_{W}(\bm{x}))>0.5\\ 0,\mathit{sig}(f_{W}(\bm{x}))\leq 0.5,\end{cases}\] (1)

where \(sig\) is the _sigmoid_ function, i.e., \(sig(x)=1/(1+e^{-x})\). We use _Binary Cross-Entropy (BCE) loss_ in Equation (2) as our loss function. For simplicity, we denote \(u_{i}=f(\bm{x}_{i},W),i\in[n]\) as the output of the neural network for input \(\bm{x}_{i}\), where \([n]=\{k\in\mathbb{N}^{+}|k\leq n\}\).

\[L(f,y)=\frac{1}{n}\sum_{i=1}^{n}L(sig(u_{i}),y_{i})=-\frac{1}{n}\sum_{i=1}^{n} \bigg{[}y_{i}\log(sig(u_{i}))+(1-y_{i})\log(1-sig(u_{i}))\bigg{]}.\] (2)

### Neural Networks and Adversarial Risk

Our exploration starts from a basic linear model, then to _Multilayer Perceptrons (MLP)_. We provide the proof for both linear and 2-layer MLPs, which can be extended to MLPs with more layers. They are defined as

\[f_{linear}(\bm{x},W)=\bm{a}^{T}(W\bm{x})\] (3a) \[f_{mlp}(\bm{x},W)=\bm{a}^{T}(\sigma(W\bm{x})),\] (3b)

where \(\bm{x}\in\mathcal{X}\) is the input data, \(W\in\mathbb{R}^{m\times d}\) denotes the linear transformation, \(m\) is the width of the networks. \(\sigma\) denotes the element-wise _activation functions_.

We follow the initialization setting in [9], where \(\bm{a}\) is randomly initialized and fixed from a binary selection of \(\{-\frac{1}{\sqrt{m}},\frac{1}{\sqrt{m}}\}^{m}\). Additionally, we introduce a slightly different setting for the weights \(W\), which are randomly initialized following the normal distribution \(N(0,\frac{1}{m^{1+2q}})\) with \(q>0\), instead of \(N(0,\frac{1}{m})\). However, in our experiment, different settings of \(q\) are considered, including the scenario that \(q\leq 0\).

Generalization ability is one of the most important concepts for machine learning models. Classifiers with better generalization power indicate lower _natural risk_ for unseen data. Given data points \((\bm{x},y)\sim P\) and classifier \(f\), the natural risk is defined as

\[R(f)=\underset{(\bm{x},y)\sim P}{\mathbb{E}}[L(f(\bm{x}),y))].\] (4)

When it comes to \(0\)-\(1\) loss, the natural risk becomes the probability of misclassification for unseen data points.

Similar to natural risk, the _adversarial risk_ is defined as the probability of misclassification under adversarial perturbations as is shown in Definition 3.1.

**Definition 3.1** (Adversarial Risk).: Given data points \((\bm{x},y)\sim P\), and a perturbation \(\bm{\varepsilon}\) within a norm-ball, i.e.,

\[B_{r}=\{\|\bm{\varepsilon}\|_{2}\leq r\},\] (5)

where \(r>0\) indicates the \(L_{2}\)-norm budget for perturbations. The adversarial risk for neural network \(f\) on loss function \(L\) is defined as

\[R^{rob}(f,r)=\operatorname*{\mathbb{E}}_{(\bm{x},y)\sim P}\left[\sup_{\bm{ \varepsilon}\in B_{r}}L(f(\bm{x}+\bm{\varepsilon}),y))\right]\] (6)

Since adversarial perturbations are almost invisible to human eyes, we expect \(r\) to be quite small.

### Dirichlet Energy

The concept of _Dirichlet energy_, originating from _Partial Differential Equations (PDEs)_, serves as a tool to assess the variability of a function [11]. However, as argued by Dohmatob and Bietti [8], it serves as a more effective measure of adversarial robustness than the _Lipschitz constant_. We extend this concept to mappings to make it more suitable for multi-dimensional problems, which is formally defined in Definition 3.2.

**Definition 3.2** (Dirichlet Energy for Mappings).: Let convex set \(\mathcal{Z}_{1}\subseteq\mathbb{R}^{m_{1}}\) and \(\mathcal{Z}_{2}\subseteq\mathbb{R}^{m_{2}}\). Given a differentiable mapping \(\bm{\phi}:\mathcal{Z}_{1}\rightarrow\mathcal{Z}_{2}\), the Dirichlet Energy w.r.t. \(\bm{x}\sim P_{\bm{x}}\) is defined as

\[\mathfrak{S}(\bm{\phi})\triangleq\sqrt{\|J_{\bm{\phi}}(\bm{x})\|_{L^{2}(P_{ \bm{x}})}^{2}}=\sqrt{\operatorname*{\mathbb{E}}_{\bm{x}\sim P_{\bm{x}}}\left[ \|J_{\bm{\phi}}(\bm{x})\|_{2}^{2}\right]},\] (7)

where \(\|\cdot\|_{2}\) refers to the operator norm and \(J_{\bm{\phi}}(\bm{x})\) denotes the Jacobian matrix of \(\bm{\phi}\) w.r.t. its input \(\bm{x}\).

To be clear, we use \(\|\cdot\|_{2}\) to denote the \(L_{2}\)-norm for vectors and operator norm for matrices throughout our analysis. This concept can be extended to layers with ReLU activation by extending the derivative to \(ReLU^{\prime}(x)=\mathbf{1}_{\{x\geq 0\}}\), where \(\mathbf{1}_{\{x\geq 0\}}\) is the indicator function. With the concepts defined, we now establish the relationship between adversarial risk and Dirichlet energy.

## 4 Measure Adversarial Risk by Dirichlet Energy

In this section, we establish the relationship between the _adversarial risks_ and _Dirichlet energy_, showing that Dirichlet Energy is a proper measurement for adversarial risk. Dohmatob and Bietti [8] only compares the Dirichlet energy with the Lipschitz constant. We, instead, illustrate that Dirichlet energy is a proper representation of the gap between natural risk and adversarial risk.

**Theorem 4.1**.: _Given data points \((\bm{x},y)\sim P\) and \(\bm{x}\sim P_{\bm{x}}\), the relationship between adversarial risk and Dirichlet energy for classifier \(f\) with differentiable loss function \(L\) is shown as_

\[R^{rob}(f,r)\lesssim R(f)+r\mathfrak{S}(L(f)),\] (8)

_where \(r>0\) is the largest perturbation budget and \(\mathfrak{S}(L(f))=\sqrt{\operatorname*{\mathbb{E}}_{\bm{x}\sim P_{\bm{x}}} \left[\|\nabla_{\bm{f}}L^{T}\cdot J_{\bm{f}}(\bm{x})\|_{2}^{2}\right]}\) indicating the Dirichlet energy of the classifier on loss \(L\)._

As is shown, the Dirichlet energy is a proper representation of the gap between generalization and adversarial risk which indicates the adversarial robustness. The proof relies on the linear approximation of \(L(f)\). The detailed proof is shown in Appendix A.

We also empirically show that the Dirichlet energy for classifier \(f\), i.e., \(\mathfrak{S}(f)\) instead of \(\mathfrak{S}(L(f))\), is the part that influences the adversarial robustness, as is shown in Figure 1 where we compare the level of Dirichlet energy for \(f\) with the _robust accuracy_ of the classifier attacked by _Auto-attack_[5].

Since the Dirichlet energy defined in Equation (7) can be used to measure the _variability_ of mappings, it follows that this metric could be employed to evaluate the adversarial robustness of individual layers or modules within neural networks. Consequently, this allows for an assessment of whether there is collaboration between these components in terms of adversarial robustness.

[MISSING_PAGE_FAIL:5]

_Remark 4.4_ (Linear Correlation for \(L_{2}\)-norm of the Jacobian).: We have \(\rho_{\bm{\phi},\bm{\varphi}}=1\) iff there exist \(t\in\mathbb{R}\) such that

\[P(t\|J_{\bm{\phi}}(\bm{\varphi})\|_{2}=\|J_{\bm{\varphi}}(\bm{x})\|_{2})=1,\]

implying that \(\rho_{\bm{\phi},\bm{\varphi}}\) certainly can be used to assess the linear correlations. It reduces to _Pearson correlation coefficient_[4] when the _mean_ of both random variables equals zero.

Now we give the theorem that binds them together.

**Theorem 4.5** (Robustness Decomposition).: _Given the same assumption in Definition 4.2, the measurement for overall adversarial robustness can be decomposed as_

\[\mathfrak{S}(\bm{\phi}\circ\bm{\varphi}) =\left(\mathbb{E}_{\bm{x}\sim P}\big{[}\|J_{\bm{\phi}\circ\bm{ \varphi}}(\bm{x})\|_{2}^{2}\big{]}\right)^{\frac{1}{2}}\] \[=\varrho_{\bm{\phi},\bm{\varphi}}\Big{(}1+\frac{var_{\bm{\phi}, \bm{\varphi}}}{\mu_{\bm{\phi},\bm{\varphi}}^{2}}\Big{)}^{\frac{1}{2}}\rho_{ \bm{\phi},\bm{\varphi}}\mathfrak{S}(\bm{\phi})\mathfrak{S}(\bm{\varphi})\] (15)

The proof is straightforward from the definition. Based on this theorem, we have conducted experiments with various linear models and 2-layer MLPs. These experiments demonstrate that, apart from the co-correlation, all other statistics are negligible, as shown in Figure 5 in the Appendix D. Consequently, our analysis primarily focuses on the co-correlation \(\varrho\).

## 5 On Dynamics of Co-Correlation

### Dynamics for Linear Model

Before delving into our analysis, we state our assumptions explicitly.

**Assumption 5.1**.: We assume that each element \(w_{i,j}\) in the weight matrix \(W(0)\in\mathbb{R}^{m\times d}\) at initialization follows the Gaussian distribution \(N(0,\frac{1}{m^{1+2q}})\), with \(q>0\). Additionally, each element \(a_{r},r\in[m]\) in \(\bm{a}\) is randomly selected from the set \(\{-\frac{1}{\sqrt{m}},\frac{1}{\sqrt{m}}\}\), and fixed during training.

**Assumption 5.2**.: We assume that for each \((\bm{x}_{i},y_{i})\in D,i\in[n]\), \(\bm{x}_{i}\) is \(L_{2}\) norm bounded such that \(\|\bm{x}_{i}\|_{2}=1\) for all \(i\in[n]\).

Since we only assume bounded inputs and a specific weight initialization method, compared to existing works [23; 19; 21; 12], our approach can be easily extended to MLPs with more than two layers.

Now let us focus on the co-correlation defined in Equation (9) and show the dynamics of \(\varrho_{\bm{\phi},\bm{\varphi}}\) for each step of gradient descent. We start from the linear model described in Equation (3a). Given the binary classification problem and the linear model described. Our first theorem demonstrates that co-correlation \(\varrho_{\bm{\phi},\bm{\varphi}}\) gradually accumulates throughout gradient descent optimization. Despite the simplicity of the linear model, it effectively exhibits most of the core properties under consideration.

Figure 1: For all MLPs considered, lower value of Dirichlet Energy (Figure 0(b)) corresponds to larger robust accuracy on test-set attacked by \(L_{2}\)-norm Auto-attack with \(\epsilon=0.5\). The dynamics of the co-correlation \(\varrho\) for linear and MLPs is shown in Figure 0(c) and 0(d). The architectures of neural networks are defined in Equation (3b). The parameters to control the weight initialization in Assumption 5.1 is set to \(q=0.25\)

Since the weights are updated by the gradient descent, the update of the weights at step \(t\in\mathbb{N}\) is

\[\Delta\bm{w}_{r}(t)=\eta a_{r}\frac{1}{n}\sum_{i=1}^{n}\big{(}y_{i}- sig(u_{i}(t))\big{)}\bm{x}_{i},\]

where \(\eta\) denotes the learning rate. Therefore, the dynamics of the weights can be expressed as

\[\dot{W}(t)=\bm{a}\otimes\widetilde{\bm{x}}^{T}(t),\] (16)

where \(\otimes\) denotes the Kronecker product and \(\widetilde{\bm{x}}(t)\) is the _error weighted input_ such that

\[\widetilde{\bm{x}}(t)=\frac{1}{n}\sum_{i=1}^{n}\Big{[}y_{i}-sig \big{(}u_{i}(t)\big{)}\Big{]}\bm{x}_{i}.\] (17)

Given the weight updates, we demonstrate that the dynamics of the co-correlation for the linear model, denoted as \(\dot{\varrho}_{\bm{a},W}(t)\), exhibit an increasing trend, particularly during the initial steps of training when most predictions are still essentially random.

**Theorem 5.3** (Dynamics of the Co-correlation for Linear Model).: _Given the linear model defined in Equation (3a) and training dataset \(\mathcal{D}=\{(\bm{x}_{i},y_{i})\}_{i=1}^{n}\). Assume that assumptions 5.1 and 5.2 hold for \(W\) and \(\bm{a}\). The gradient descent applied to the weights results in the dynamics of the co-correlation being expressed as:_

\[\dot{\varrho}_{\bm{a},W}(t)=\eta C(t)\varrho_{\bm{a},W},\] (18)

_and with high probability,_

\[C(t)\geq\frac{\sum_{\tau=1}^{t}\widetilde{\bm{x}}(\tau)^{T} \widetilde{\bm{x}}(t)}{\|W(t)\|_{2}^{2}}\cdot\Big{(}1-\big{(}\bm{v}(t)^{T}\bm{ a}\big{)}^{2}\Big{)}+\mathcal{O}\bigg{(}\frac{1}{m^{q}}\bigg{)}\] (19)

_where the \(\bm{v(t)}\) is the dominate eigenvector for \(W(t)W(t)^{T}\)._

_When \(m\) is sufficiently large, and during the initial steps of the optimization process, \(\widetilde{\bm{x}}(\tau),\tau\in[t]\) are quite similar to each other in terms of cosine similarity, implying an acute angle to each other, which leads to \(\sum_{\tau=1}^{t}\widetilde{\bm{x}}(\tau)^{T}\widetilde{\bm{x}}(t)\geq 0\). As a result, we can conclude that \(C(t)\geq 0\)._

The detailed proof can be found in the Appendix B. This assertion is also corroborated by the results of our experiments as shown in Figure 0(c). Even though Theorem 5.3 is based on a linear model, the essential properties are universally applicable and can be summarized as follows:

**Property 1**.: The co-correlation \(\varrho_{\bm{a},W}\) develops during the initial stages of training and becomes saturated as training progresses to its later stages.

**Property 2**.: The speed of the accumulation of co-correlation \(\varrho_{\bm{a},W}\) is inversely related to the operator norm of weights \(\|W(t)\|_{2}\).

Under the same weight initialization conditions specified in Assumption 5.1, an increase in network width leads to a decrease in the \(L_{2}\)-norm of the weight, consequently causing a substantial rise in co-correlation.

### Dynamics for MLP Model

For the non-linear case, we make certain assumptions regarding activation functions.

**Assumption 5.4**.: The derivative of the activation function \(\sigma^{\prime}(x)\) in non-linear neural networks is bounded by \(M\). In other words, we have \(|\sigma^{\prime}(x)|\leq M\).

With Assumption 5.4, Theorem 5.3 can be extended to MLPs, and the two properties still hold. Different from the linear model, the update of weights for non-linear MLP defined in (3b) is

\[\Delta\bm{w}_{r}=\eta a_{r}\frac{1}{n}\sum_{i=1}^{n}\big{(}y_{i}- sig(u_{i})\big{)}\sigma^{\prime}(\bm{w}_{r}^{T}\bm{x}_{i})\bm{x}_{i},\] (20)

where \(\sigma^{\prime}(\bm{w}_{r}^{T}\bm{x}_{i})\) is the derivative of activation function w.r.t. its input. Hence,

\[\Delta W=\eta\begin{pmatrix}a_{1}\widetilde{\bm{x}}_{1}^{T}\\ \vdots\\ a_{m}\widetilde{\bm{x}}_{m}^{T}\end{pmatrix},\]where for each \(r\in[m]\),

\[\widetilde{\bm{x}}_{r}=\frac{1}{n}\sum_{i=1}^{n}\sigma^{\prime}(\bm{w}_{r}^{T}\bm {x}_{i})\big{(}y_{i}-sig(u_{i})\big{)}\bm{x}_{i}.\]

As is shown in Equation (20), \(\bm{a}\) is not the dominant eigenvector for \(\Delta W\Delta W^{T}\) due to the difference of \(\widetilde{\bm{x}}_{r},r\in[m]\) from \(\widetilde{\bm{x}}\). Thus to average out the difference, we define the weighted sum of inputs for the non-linear model as

\[\widetilde{\bm{x}}_{\star}(t)\triangleq\frac{1}{n}\sum_{i=1}^{n}\alpha_{i}(t, \bm{x})\big{(}y_{i}-sig(u_{i}(t))\big{)}\bm{x}_{i},\] (21)

where \((\bm{x}_{i},y_{i})\in\mathcal{D}\) are realized r.v., and \(\bm{x}\sim P_{\bm{x}}\). Given \(w_{i,j}\overset{i.i.d.}{\sim}N(0,\frac{1}{m^{1+2q}})\) for each element in \(W(0)\), we have \(\bm{w}(0)\sim N(0,\frac{1}{m^{1+2q}}\mathbf{I}_{d})\). Furthermore, given that \(W(t)\) is calculated from \(W(0)\), it is evident that each row in \(W(t)\), denoted as \(\bm{w}(t)\), constitutes a random variable. As a result, both \(\sigma^{\prime}(\bm{w}(t)^{T}\bm{x})\) and \(\sigma^{\prime}(\bm{w}(t)^{T}\bm{x}_{i})\) are bounded random variables that are contingent upon \(W(0)\). Hence, we define \(\alpha_{i}(t,\bm{x})\) as

\[\alpha_{i}(t,\bm{x})\triangleq\mathbb{E}_{W(0)}\Big{[}\sigma^{\prime}(\bm{w}( t)^{T}\bm{x})\sigma^{\prime}(\bm{w}(t)^{T}\bm{x}_{i})\Big{]}.\]

Since, \(\bm{x}\sim P_{\bm{x}}\), \(\alpha_{i}(t,\bm{x})\) is still r.v. contingent to \(\bm{x}\) and so does the \(\widetilde{\bm{x}}_{\star}(t)\). Now we show the dynamics of co-correlation for two-layer MLP defined in Equation (3b).

**Theorem 5.5**.: _(Dynamics of the Co-correlation for MLP) Given the MLP defined in Equation (3) with training dataset \(\mathcal{D}=\{(\bm{x}_{i},y_{i})\}_{i=1}^{n}\), \(\bm{x}\in\mathcal{X}\) such that \(\bm{x}\sim P_{\bm{x}}\). Assume that Assumption 5.1 and 5.2 hold for \(W\) and \(\bm{a}\), and Assumption 5.4 holds for the activation function. we have_

\[\dot{\varrho}_{\bm{a},\sigma\circ W}(t)=\eta C(t)\varrho_{\bm{a},\sigma\circ W }(t).\]

_With high probability,_

\[C(t)\geq\frac{\sum_{\tau=1}^{t}\big{(}1-\bm{a}^{T}\bm{v}(\tau)\bm{a}^{T}\bm{v }(t)\big{)}\mathbb{E}_{\bm{x}\sim P_{\bm{x}}}\big{[}\widetilde{\bm{x}}_{\star }^{T}(\tau)\widetilde{\bm{x}}_{\star}(t)\big{]}}{\mathbb{E}_{\bm{x}\sim P_{ \bm{x}}}\|D(t)W(t)\|_{2}^{2}}+\max\bigg{\{}\mathcal{O}\bigg{(}\frac{1}{\sqrt{m }}\bigg{)},\mathcal{O}\bigg{(}\frac{1}{m^{q}}\bigg{)}\bigg{\}},\]

_where_

\[D(t)=diag(\sigma^{\prime}(\bm{w}_{1}(t)^{T}\bm{x}),\cdots,\sigma^{\prime}(\bm{ w}_{m}(t)^{T}\bm{x})),\]

_and \(\bm{v}(t)\) denotes the dominant eigenvector for \(W(t)W(t)^{T}\), with \(\widetilde{\bm{x}}_{\star}^{T}\) is defined in Equation (21). Similar to the Theorem 5.3, when \(m\) is sufficiently large, and during the initial steps of the optimization where the error-weighted inputs \(\widetilde{\bm{x}}_{\star}^{T}(\tau),\tau\in[t]\) do not significantly fluctuate, we have that \(C(t)\geq 0\)._

The detailed proof is in Appendix C. In Theorem 5.5, \(\widetilde{\bm{x}}_{\star}\) serves a similar purpose as \(\widetilde{\bm{x}}\) for the linear model. In addition, it considers the influence of the activation function. Property 1 still holds for the MLPs, and Property 2 extends to \(\mathbb{E}_{\bm{x}\sim P_{\bm{x}}}\|D(t)W(t)\|_{2}^{2}=\|J_{\sigma\circ W}(\bm {x})\|_{L(P_{\bm{x}})}^{2}\).

Figure 2: The dynamics of co-correlation for ResNet50 and WRN50 under different way of partition. The way of partition is illustrated in Figure 1(a). A1-A2 and B1-B2 represent the separations that distinguish the head and tail separately.

## 6 Experiments

To estimate the co-correlation more efficiently and in parallel, we employ the _Power Iteration_ algorithm [10] in conjunction with _Functorch_[17]. The corresponding pseudo-code is presented in the Appendix E.

We verify our proposed theorem on linear and MLP models on the MNIST dataset [22]. The width of hidden layers varied from \(2^{4}\) to \(2^{13}\), with weights initialized via a Gaussian distribution \(N(0,\frac{1}{m^{1+2\cdot 2_{q}}})\) where \(q\) was set to values ranging from \(0.25\) to \(-0.15\). We train both the linear and MLP for 50 epochs with a batch size of 512 using the SGD optimizer with a learning rate of \(0.003\). In addition, we also conduct the experiment on more complex ResNet50 [16] and WRN50 [38]. For both models, we opted for default random weight initialization and used the Adam optimizer with a learning rate of \(0.0005\) on CIFAR10 [20]. To calculate the co-correlation and other statistics, we cover the entire testset. We consider using \(L_{2}\) Auto-attack [5] with \(\epsilon=0.5\) for all MLPs we trained. The experiments were executed on a Nvidia RTX3090 GPU, using Python 3.9.7 and PyTorch 1.9.1. The code for the experiment is available at https://github.com/squarewang2077/co-correlation.

### Empirical Evidence for Proposed Theorem

Figure 1 presents a comparison between the robust accuracy and the Dirichlet energy \(\mathfrak{S}(f)\) across all trained MLPs. As observed in Figure 0(a) and 0(b), models with lower levels of Dirichlet energy \(\mathfrak{S}(f)\) tend to exhibit higher robust accuracy, suggesting that Dirichlet energy is an effective representation of adversarial robustness. Another noteworthy finding is that wider neural networks, with the same level of weight initialization, demonstrate improved adversarial robustness.

Figure 1 also depicts the dynamic behaviour of shallow neural networks with a weight initialization parameter of \(q=0.25\). As is shown in Figure 0(c) and 0(d), the co-correlation \(q\) increases throughout training. Except for narrow widths like \(2^{4}\) and \(2^{5}\), the majority of networks demonstrate an upward trend. This trend, however, flattens for non-linear models, suggesting potentially stronger adversarial robustness due to the non-linearity of the activation function introduced in MLPs.

Figure 2 shows the dynamics of the co-correlation on ResNet50 and Wide-ResNet50. Both networks are trained on CIFAR10 using the Adam optimizer. We divide them by the pattern of A1-A2 and B1-B2, as shown in Figure 1(a). The co-correlation outcomes for these divisions are displayed in Figure 1(b) and Figure 1(c), it shows that even with the Adam optimizer, without specific weight initialization considerations, there is a noticeable rise in co-correlation.

### The Impact of Width and Weight Initialization

Figure 3 illustrates the co-correlation dynamics under varying \(q\) for both linear and MLPs with widths of 32, 512, 2,048, and 8,192. The figure highlights that our proposed theorems' assumption of \(q>0\) is quite tight, as all trajectories with \(q<0\) remain flat throughout training. We can also observe that the speed of accumulation significantly increases with larger network widths.

Figure 4 displays the accuracy on testset and co-correlation for both linear and MLPs as heat-maps. Each cell in the heat-map represents a trained network. From Figure 3(c) and Figure 3(d), we observe that the best performance and robustness are shown by the MLPs with the largest widths (\(\text{width}=8192\)) and the smallest weight initializations (\(q=-0.15\)). And when we alter the weight initialization to

Figure 3: The dynamic of co-correlation under different set-up of weight initialization. MLP network defined in Equation (3) with ReLU activation function for width \(32\), \(512\), \(2048\) and \(8192\) are included.

control the co-correlation, making it increase from \(0.25\) to \(0.83\), the accuracy declines accordingly from \(0.93\) to \(0.68\). On the contrary, for another extreme case of models with a width of \(16\), enhanced performance is accompanied by increased co-correlation. Consequently, an interesting conclusion can be drawn about the diverse behaviour of neural networks with small and larger width. Gradient descent tends to enhance the training of neural networks with smaller width by fostering co-correlation among layers, which is intrinsically brittle. However, wide networks are trained with less reliance on interlayer correlation, resulting in inherently more robust models.

## 7 Conclusion and Limitation

Our work investigates the implicit bias of gradient descent toward adversarial robustness from the perspective of collaboration between layers. By adapting Dirichlet energy to estimate the adversarial robustness of neural networks' individual components, we characterized the collaboration behaviour between consecutive layers and identified two fundamental properties for dynamics of the co-correlation. The first property shows that the co-correlation for MLPs will build up during gradient descent under mild assumptions for weight initialization. The second property shows that the speed of accumulation for co-correlation is inversely related to the operator norm of Jacobian for the corresponding sub-modules. In addition, we observed that networks with small widths tend to foster co-correlation among layers to improve performance, whereas wide networks' performance improvement does not heavily rely on establishing such co-correlation. Future research can expand upon this by examining the effects of increased network depth and more sophisticated structures on the observed phenomena.

LimitationOur work can be easily extended to multi-layer neural networks since we only assume that the inputs are bounded by the \(L_{2}\)-norm. However, like many theoretical studies, extending our approach to more complex models is challenging. It remains unknown whether complex models exhibit the same behaviors.

## References

* [1] R. Bhattacharjee, S. Jha, and K. Chaudhuri. Sample complexity of robust linear classification on separated data. In _International Conference on Machine Learning_, pages 884-893. PMLR, 2021.
* [2] S. Bubeck, Y. Cherapanamjeri, G. Gidel, and R. Tachet des Combes. A single gradient step finds adversarial examples on random two-layers neural networks. _Advances in Neural Information Processing Systems_, 34:10081-10091, 2021.
* [3] L. Chizat and F. Bach. Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss. In _Conference on Learning Theory_, pages 1305-1338. PMLR, 2020.
* [4] I. Cohen, Y. Huang, J. Chen, J. Benesty, J. Benesty, J. Chen, Y. Huang, and I. Cohen. Pearson correlation coefficient. _Noise reduction in speech processing_, pages 1-4, 2009.

Figure 4: Accuracy and Co-correlation under different initialization and width. Figure 3(a) and 3(b) show the heat map for linear model, and Figure 3(c) and 3(d) is for MLP ReLU.

* [5] F. Croce and M. Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In _International conference on machine learning_, pages 2206-2216. PMLR, 2020.
* [6] A. Daniely and H. Shacham. Most relu networks suffer from \(l^{2}\) adversarial perturbations. _Advances in Neural Information Processing Systems_, 33:6629-6636, 2020.
* [7] E. Dobriban, H. Hassani, D. Hong, and A. Robey. Provable tradeoffs in adversarially robust classification. _arXiv preprint arXiv:2006.05161_, 2020.
* [8] E. Dohmatob and A. Bietti. On the (non-) robustness of two-layer neural networks in different learning regimes. _arXiv preprint arXiv:2203.11864_, 2022.
* [9] S. S. Du, X. Zhai, B. Poczos, and A. Singh. Gradient descent provably optimizes over-parameterized neural networks. _arXiv preprint arXiv:1810.02054_, 2018.
* [10] J. F. Epperson. _An introduction to numerical methods and analysis_. John Wiley & Sons, 2021.
* [11] L. C. Evans. _Partial differential equations_, volume 19. American Mathematical Society, 2022.
* [12] S. Frei, G. Vardi, P. L. Bartlett, N. Srebro, and W. Hu. Implicit bias in leaky relu networks trained on high-dimensional data. _arXiv preprint arXiv:2210.07082_, 2022.
* [13] S. Frei, G. Vardi, P. L. Bartlett, and N. Srebro. The double-edged sword of implicit bias: Generalization vs. robustness in relu networks. _arXiv preprint arXiv:2303.01456_, 2023.
* [14] J. Gilmer, L. Metz, F. Faghri, S. S. Schoenholz, M. Raghu, M. Wattenberg, and I. Goodfellow. Adversarial spheres. _arXiv preprint arXiv:1801.02774_, 2018.
* [15] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. _arXiv preprint arXiv:1412.6572_, 2014.
* [16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [17] R. Z. Horace He. functorch: Jax-like composable function transforms for pytorch. https://github.com/pytorch/functorch, 2021.
* [18] H. Huang, Y. Wang, S. Erfani, Q. Gu, J. Bailey, and X. Ma. Exploring architectural ingredients of adversarially robust deep neural networks. _Advances in Neural Information Processing Systems_, 34:5545-5559, 2021.
* [19] Z. Ji and M. Telgarsky. Directional convergence and alignment in deep learning. _Advances in Neural Information Processing Systems_, 33:17176-17186, 2020.
* [20] A. Krizhevsky. Learning multiple layers of features from tiny images. _Master's thesis, University of Tront_, 2009.
* [21] D. Kunin, A. Yamamura, C. Ma, and S. Ganguli. The asymmetric maximum margin bias of quasi-homogeneous neural networks. _arXiv preprint arXiv:2210.03820_, 2022.
* [22] Y. LeCun, C. Cortes, and C. Burges. Mnist handwritten digit database. _ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist_, 2, 2010.
* [23] K. Lyu and J. Li. Gradient descent maximizes the margin of homogeneous neural networks. _arXiv preprint arXiv:1906.05890_, 2019.
* [24] K. Lyu, Z. Li, R. Wang, and S. Arora. Gradient descent on two-layer nets: Margin maximization and simplicity bias. _Advances in Neural Information Processing Systems_, 34:12978-12991, 2021.
* [25] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to adversarial attacks. _arXiv preprint arXiv:1706.06083_, 2017.

* [26] S. Mahloujifar, D. I. Diochnos, and M. Mahmoody. The curse of concentration in robust learning: Evasion and poisoning attacks from concentration of measure. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 33, pages 4536-4543. Association for the Advancement of Artificial Intelligence (AAAI), 2019.
* [27] Y. Min, L. Chen, and A. Karbasi. The curious case of adversarially robust models: More data can help, double descend, or hurt generalization. In _Uncertainty in Artificial Intelligence_, pages 129-139. PMLR, 2021.
* [28] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In _2016 IEEE symposium on security and privacy (SP)_, pages 582-597. IEEE, 2016.
* [29] R. Sarussi, A. Brutzkus, and A. Globerson. Towards understanding learning in neural networks with linear teachers. In _International Conference on Machine Learning_, pages 9313-9322. PMLR, 2021.
* [30] L. Schmidt, S. Santurkar, D. Tsipras, K. Talwar, and A. Madry. Adversarially robust generalization requires more data. _Advances in neural information processing systems_, 31, 2018.
* [31] A. Shafahi, W. R. Huang, C. Studer, S. Feizi, and T. Goldstein. Are adversarial examples inevitable? _arXiv preprint arXiv:1809.02104_, 2018.
* [32] C.-J. Simon-Gabriel, Y. Ollivier, L. Bottou, B. Scholkopf, and D. Lopez-Paz. First-order adversarial vulnerability of neural networks and input dimension. In _International conference on machine learning_, pages 5809-5817. PMLR, 2019.
* [33] D. Tsipras, S. Santurkar, L. Engstrom, A. Turner, and A. Madry. Robustness may be at odds with accuracy. _arXiv preprint arXiv:1805.12152_, 2018.
* [34] F. Wang, P. Xu, W. Ruan, and X. Huang. Towards verifying the geometric robustness of large-scale neural networks. _arXiv preprint arXiv:2301.12456_, 2023.
* [35] Z. Wang and W. Ruan. Understanding adversarial robustness of vision transformers via cauchy problem. In _Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2022, Grenoble, France, September 19-23, 2022, Proceedings, Part III_, pages 562-577. Springer, 2023.
* [36] X. Yin and W. Ruan. Boosting adversarial training via fisher-rao norm-based regularization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 24544-24553, 2024.
* [37] X. Yin, W. Ruan, and J. Fieldsend. Dimba: discretely masked black-box attack in single object tracking. _Machine Learning_, 113(4):1705-1723, 2024.
* [38] S. Zagoruyko and N. Komodakis. Wide residual networks. _arXiv preprint arXiv:1605.07146_, 2016.
* [39] H. Zhang, Y. Yu, J. Jiao, E. Xing, L. El Ghaoui, and M. Jordan. Theoretically principled trade-off between robustness and accuracy. In _International conference on machine learning_, pages 7472-7482. PMLR, 2019.
* [40] T. Zhang, W. Ruan, and J. E. Fieldsend. Proa: A probabilistic robustness assessment against functional perturbations. In _Joint European Conference on Machine Learning and Knowledge Discovery in Databases_, pages 154-170. Springer, 2022.
* [41] Z. Zhu, F. Liu, G. Chrysos, and V. Cevher. Robustness in deep learning: The good (width), the bad (depth), and the ugly (initialization). _Advances in Neural Information Processing Systems_, 35:36094-36107, 2022.

The Proof for Theorem 4.1

Suppose that the loss function \(L\) is differentiable w.r.t. \(f\). Given data points \((\bm{x},y)\sim P\) and \(\bm{x}\sim P_{\bm{x}}\), we have

\[R^{rob}(f,r) =\operatorname*{\mathbb{E}}_{(\bm{x},y)\sim P}\sup_{\varepsilon \in B_{r}}L(f(\bm{x}+\bm{\varepsilon}),y))\] (22) \[=\operatorname*{\mathbb{E}}_{(\bm{x},y)\sim P}\left[\sup_{ \varepsilon\in B_{r}}L(f(\bm{x}),y))+\sup_{\varepsilon\in B_{r}}L(f(\bm{x}+ \bm{\varepsilon}),y))-\sup_{\varepsilon\in B_{r}}L(f(\bm{x},y))\right]\] (23) \[\leq\operatorname*{\mathbb{E}}_{(\bm{x},y)\sim P}\left[L(f(\bm{x} ),y))\right]+\sqrt{\operatorname*{\mathbb{E}}_{(\bm{x},y)\sim P}\left[\sup_{ \varepsilon\in B_{r}}\left(L(f(\bm{x}+\bm{\varepsilon}),y))-L(f(\bm{x},y)) \right)^{2}\right]}\] (24) \[\approx\operatorname*{\mathbb{E}}_{(\bm{x},y)\sim P}\left[L(f( \bm{x}),y))\right]+\sqrt{\operatorname*{\mathbb{E}}_{(\bm{x},y)\sim P}\left[ \sup_{\varepsilon\in B_{r}}\left(\nabla_{f}L^{T}\cdot J_{f}(\bm{x})\bm{ \varepsilon}\right)^{2}\right]}\] (25) \[=\operatorname*{\mathbb{E}}_{(\bm{x},y)\sim P}\left[L(f(\bm{x}),y ))\right]+r\sqrt{\operatorname*{\mathbb{E}}_{(\bm{x},y)\sim P}\lVert\nabla_{f} L^{T}\cdot J_{f}(\bm{x})\rVert_{2}^{2}},\] (26)

where \(\nabla_{f}L\) is the gradients of \(L\) w.r.t. \(f\), \(\nabla_{f}L^{T}\) indicates that it is a row vector. \(J_{f}(\bm{x})\) is the Jacobian matrix for classifier \(f\).

## Appendix B The Proof of Theorem 5.3

Before formally proving the theorem, we provide some lemmas which are useful for our proof.

**Lemma B.1** (Dynamic of Weights for Linear Model).: _Given gradient descent to optimize the weights, the dynamic of the weights at step \(t\) is_

\[\dot{W}(t)=\bm{a}\otimes\widetilde{\bm{x}}^{T}(t),\] (27)

Proof.: Given the step size \(\eta\), the update for \(r_{th}\) row of the weight matrix \(W\) is

\[\Delta\bm{w}_{r} =-\frac{\eta}{n}\sum_{i=1}^{n}\frac{\partial l}{\partial u_{i}} \frac{\partial f(W,\bm{x}_{i})}{\partial\bm{w}_{r}}\] (28) \[=\eta a_{r}\frac{1}{n}\sum_{i=1}^{n}\big{(}y_{i}-sig(u_{i})\big{)} \bm{x}_{i},\] (29)

where \(sig(\cdot)\) denotes the \(sigmoid\) function. Hence, for weight matrix \(W\), we have

\[\Delta W =\begin{pmatrix}\Delta\bm{w}_{1}^{T}\\ \vdots\\ \Delta\bm{w}_{m}^{T}\end{pmatrix}\] (30) \[=\eta\begin{pmatrix}a_{1}\frac{1}{n}\sum_{i=1}^{n}\big{(}y_{i}-sig( u_{i})\big{)}\bm{x}_{i}^{T}\\ \vdots\\ a_{m}\frac{1}{n}\sum_{i=1}^{n}\big{(}y_{i}-sig(u_{i})\big{)}\bm{x}_{i}^{T} \end{pmatrix}.\] (31)

After replacing with

\[\widetilde{\bm{x}}=\frac{1}{n}\sum_{i=1}^{n}\big{(}y_{i}-sig(u_{i})\big{)}\bm {x}_{i},\] (32)we have

\[\Delta W =\eta\begin{pmatrix}a_{1}\widetilde{\bm{x}}^{T}\\ \vdots\\ a_{m}\widetilde{\bm{x}}^{T}\end{pmatrix}\] (33) \[=\eta\bm{a}\otimes\widetilde{\bm{x}}^{T}.\] (34)

Since \(\|\bm{x}_{i}\|_{2}=1\), we also have \(\|\widetilde{\bm{x}}\|_{2}\leq 1\). Therefore, we can say that the dynamics of weights, i.e. \(\dot{W}(t)\), is

\[\dot{W}(t)=\bm{a}\otimes\widetilde{\bm{x}}^{T}(t).\] (35)

**Lemma B.2** (The Concentration for \(L_{2}\) Norm of Gaussian r.v.).: _Let \(I_{m}\) be identical matrix of size \(m\times m\). Given \(n\) Gaussian random vectors \(\bm{z}_{1},\cdots,\bm{z}_{n}\) such that \(\bm{z}_{i}\overset{i.i.d.}{\sim}N(0,\frac{1}{m^{1+2q}}\mathbf{I}_{m}),q>0, \forall i\in[n]\). With probability at least \(1-\delta\), we have several conclusions: 1. Average of the norm_

\[\frac{1}{n}\sum_{i=1}^{n}\|\bm{z}_{i}\|_{2}=\frac{1}{m^{q}}+\mathcal{O}\bigg{(} \sqrt{\frac{8\log(2n/\delta)}{m^{1+2q}}}\bigg{)}.\] (36)

2. Average of the square norm

\[\frac{1}{n}\sum_{i=1}^{n}\|\bm{z}_{i}\|_{2}^{2}=\frac{1}{m^{2q}}+\mathcal{O} \bigg{(}\max\bigg{\{}\frac{8\log(2/\delta)}{m^{1+2q}n},\sqrt{\frac{8\log(2/ \delta)}{m^{1+4q}n}}\bigg{\}}\bigg{)}.\] (37)

3. Square root of the average of the square norm

\[\sqrt{\frac{1}{n}\sum_{i=1}^{n}\|\bm{z}_{i}\|_{2}^{2}}=\frac{1}{m^{q}}+ \mathcal{O}\bigg{(}\sqrt{\frac{8\log(2/\delta)}{m^{1+2q}n}}\bigg{)}.\] (38)

Proof.: We first prove the concentration property for the average of the norm. Since \(\sqrt{m^{1+2q}}z_{i,r}\overset{i.i.d.}{\sim}N(0,1),i\in[n],r\in[m]\), we have that \(m^{1+2q}z_{i,r}^{2}\sim\chi^{2}(1)\). Denote the sub-Exponential distribution as \(SE(\nu^{2},\alpha)\), we have \(m^{1+2q}z_{i,r}^{2}-1\in SE(4,4)\). Hence \(\forall i\in[n]\),

\[\mathbb{P}\bigg{(}\Big{|}\frac{1}{m}\sum_{r=1}^{m}\big{(}\sqrt{m^{1+2q}}z_{i, r}\big{)}^{2}-1\Big{|}\geq\epsilon\bigg{)}\leq\begin{cases}2\exp\bigg{(}- \frac{m\epsilon^{2}}{8}\bigg{)}&\epsilon\in(0,1)\\ 2\exp\bigg{(}-\frac{m\epsilon}{8}\bigg{)}&\epsilon\geq 1\end{cases}.\] (39)

Therefore, we have \(\forall\epsilon>0\)

\[\mathbb{P}\bigg{(}\Big{|}\frac{1}{\sqrt{m}}\|\sqrt{m^{1+2q}}\bm{ z}_{i}\|_{2}-1\Big{|}\geq\epsilon\bigg{)} \leq\mathbb{P}\bigg{(}\Big{|}\frac{1}{m}\|\sqrt{m^{1+2q}}\bm{ z}_{i}\|_{2}^{2}-1\bigg{|}\geq\max\{\epsilon,\epsilon^{2}\}\bigg{)}\] (40) \[\leq 2\exp\Big{(}-\frac{m\epsilon^{2}}{8}\Big{)}.\] (41)

Equation (40) is because of the fact that given \(c>0\) fixed, \(\forall x>0\) we have

\[|x-1|\geq c\Rightarrow|x^{2}-1|\geq\max\{c,c^{2}\}\] (42)

[MISSING_PAGE_FAIL:15]

with probability at least \(1-\delta\), we have

\[\left|m^{q}\sqrt{\frac{1}{n}\sum_{i=1}^{n}\|\bm{z}_{i}\|_{2}^{2}}-1 \right| \leq\sqrt{\frac{8}{mn}log\frac{2}{\delta}}\] (56) \[\Rightarrow\left|\sqrt{\frac{1}{n}\sum_{i=1}^{n}\|\bm{z}_{i}\|_{2 }^{2}}-\frac{1}{m^{q}}\right| \leq\frac{1}{m^{q}}\sqrt{\frac{8}{mn}log\frac{2}{\delta}}\] (57)

Now, we prove the theorem 5.3. Given the linear model and training dataset \(\mathcal{D}=\{(\bm{x}_{i},y_{i})\}_{i=1}^{n}\). Assume that each \(w_{i,j}\overset{i.i.d.}{\sim}N(0,\frac{1}{m^{1+2q}}),q>0\) and \(\bm{a}\) is randomly initialized subject to the constraint \(\|\bm{a}\|_{2}=1\) and fixed during training. Hence, with high probability, we have

\[\dot{\varrho}_{\bm{a},W}\geq\eta\varrho_{\bm{a},W}\cdot\left(\frac{\sum_{\tau =1}^{t}\widetilde{\bm{x}}(\tau)^{T}\widetilde{\bm{x}}(t)}{\|W(t)\|_{2}^{2}} \cdot\left(1-\left(\bm{v}(t)^{T}\bm{a}\right)^{2}\right)+\mathcal{O}\!\left( \frac{1}{m^{q}}\right)\right)\] (58)

Proof.: We first show that the derivative of co-correlation is

\[\dot{\varrho}_{\bm{a},W}(t) =\frac{d}{dt}\varrho_{\bm{a},W}(t)\] (59) \[=\frac{d}{dt}\frac{\mathbb{E}_{\bm{x}}[\|\bm{a}^{T}W(t)\|_{2}^{2} ]^{\frac{1}{2}}}{\mathbb{E}_{\bm{x}}[\|W(t)\|_{2}^{2}]^{\frac{1}{2}}}\] (60) \[=\frac{d}{dt}\frac{\|\bm{a}^{T}W(t)\|_{2}}{\|W(t)\|_{2}}\] (61) \[=\frac{\varrho_{\bm{a},W}(t)}{2}\bigg{(}\frac{d\|\bm{a}^{T}W(t)\| _{2}^{2}/dt}{\|\bm{a}^{T}W(t)\|_{2}^{2}}-\frac{d\|W(t)\|_{2}^{2}/dt}{\|W(t)\| _{2}^{2}}\bigg{)}\] (62) \[\geq\frac{\varrho_{\bm{a},W}(t)}{2\|W(t)\|_{2}^{2}}\bigg{(}\frac{ d\|\bm{a}^{T}W(t)\|_{2}^{2}}{dt}-\frac{d\|W(t)\|_{2}^{2}}{dt}\bigg{)},\] (63)

by the law of derivatives for inner product of matrices and the eigenvalue of matrix \(W(t)W(t)^{T}\), we have

\[\frac{d\|\bm{a}^{T}W(t)\|_{2}^{2}}{dt} =\bm{a}^{T}(\dot{W}(t)W(t)^{T}+W(t)\dot{W}(t)^{T})\bm{a}\] (64) \[\frac{d\|W(t)\|_{2}^{2}}{dt} =\bm{v(t)}^{T}(\dot{W}(t)W(t)^{T}+W(t)\dot{W}(t)^{T})\bm{v}(t),\] (65)

where \(\bm{v}(t)\) is the dominant eigenvector for \(W(t)W(t)^{T}\). Equation (60) is because \(J_{W}(x)=W\) and \(\|\bm{a}\|_{2}=1\) by assumption. Since Equation (60) do not depend on \(x\), we can safely drop the Expectation \(\mathbb{E}_{\bm{x}}\) as is shown in Equation (61). Equation (63) is because \(\|\bm{a}^{T}W\|_{2}^{2}\leq\|\bm{a}\|_{2}^{2}\|W\|_{2}^{2}=\|W\|_{2}^{2}\). By Lemma B.1, the weight matrix \(W(t)\) at training step \(t\) can be approximated as

\[W(t)=W(0)+\sum_{\tau=1}^{t}\Delta W(\tau).\] (66)then

\[\dot{W}(t)W(t)^{T} =\bm{a}\otimes\widetilde{\bm{x}}^{T}(t)\bigg{(}W(0)^{T}+\sum_{\tau=1 }^{t}\Delta W(\tau)^{T}\bigg{)}\] (67) \[=\bm{a}\otimes\widetilde{\bm{x}}^{T}(t)\bigg{(}W(0)^{T}+\eta\sum_{ \tau=1}^{t}\bm{a}^{T}\otimes\widetilde{\bm{x}}(\tau)\bigg{)}\] (68) \[=\bm{a}\otimes\widetilde{\bm{x}}^{T}(t)\bigg{(}W(0)^{T}+\eta\bm{a }^{T}\otimes\sum_{\tau=1}^{t}\widetilde{\bm{x}}(\tau)\bigg{)}\] (69) \[=\bm{a}\otimes\widetilde{\bm{x}}^{T}(t)W(0)^{T}+\eta\bigg{(}\bm{a }\otimes\widetilde{\bm{x}}^{T}(t)\bigg{)}\bigg{(}\bm{a}^{T}\otimes\sum_{\tau= 1}^{t}\widetilde{\bm{x}}(\tau)\bigg{)}\] (70) \[=\bm{a}\widetilde{\bm{x}}^{T}(t)W(0)^{T}+\eta\bigg{(}\sum_{\tau=1 }^{t}\widetilde{\bm{x}}(\tau)^{T}\widetilde{\bm{x}}(t)\bigg{)}\bm{a}\bm{a}^{T},\] (71)

similarity,

\[W(t)\dot{W}(t)^{T} =\bigg{(}W(0)+\sum_{\tau=1}^{t}\Delta W(\tau)\bigg{)}\bm{a}^{T} \otimes\widetilde{\bm{x}}(t)\] (72) \[=\bigg{(}W(0)+\eta\bm{a}\otimes\sum_{\tau=1}^{t}\widetilde{\bm{ x}}^{T}(\tau)\bigg{)}\bm{a}^{T}\otimes\widetilde{\bm{x}}(t)\] (73) \[=W(0)\widetilde{\bm{x}}(t)\bm{a}^{T}+\eta\bigg{(}\sum_{\tau=1}^{t }\widetilde{\bm{x}}(\tau)^{T}\widetilde{\bm{x}}(t)\bigg{)}\bm{a}\bm{a}^{T}.\] (74)

Hence

\[\frac{1}{2}\bigg{(}\frac{d\|\bm{a}^{T}W(t)\|_{2}^{2}}{dt}-\frac{d \|W(t)\|_{2}^{2}}{dt}\bigg{)}\] (75) \[=\underbrace{\eta\bigg{(}1-\big{(}\bm{a}^{T}\bm{v}(t)\big{)}^{2 }\bigg{)}\bigg{(}\sum_{\tau=1}^{t}\widetilde{\bm{x}}(\tau)^{T}\widetilde{\bm{ x}}(t)\bigg{)}}_{\mbox{1}}+\underbrace{\bm{a}^{T}W(0)\widetilde{\bm{x}}(t)\bm{a}^{T} \bm{a}-\bm{v}(t)^{T}W(0)\widetilde{\bm{x}}(t)\bm{a}^{T}\bm{v}(t)}_{\mbox{2}}\] (76)

1 is the main part of our theorem. And for 2, it can be bounded as

\[\left|\sqrt[2]\right| =\left|\big{(}\bm{a}-\bm{a}^{T}\bm{v}(t)\bm{v}(t)\big{)}^{T}W(0) \widetilde{\bm{x}}(t)\right|\] (77) \[\leq\frac{1}{n}\sum_{i=1}^{n}\Big{|}\big{(}y_{i}-sig(u_{i})\big{)} \big{(}\bm{a}-\bm{a}^{T}\bm{v}(t)\bm{v}(t)\big{)}^{T}W(0)\bm{x}_{i}\Big{|}\] (78) \[\leq\frac{1}{n}\sum_{i=1}^{n}\underbrace{|y_{i}-sig(u_{i})|\sqrt{ 1-(\bm{a}^{T}\bm{v}(t))^{2}}}_{0\leq\ldots\leq 1}\|W(0)\bm{x}_{i}\|_{2}\] (79) \[\leq\frac{1}{n}\sum_{i=1}^{n}\|W(0)\bm{x}_{i}\|_{2}\] (80)

Now since \(w_{i,j}\sim N(0,\frac{1}{m^{1+2q}}),q>0\) and each \(w_{i,j}\) is independent with each other, for given \(\bm{x}_{i},i=1,...,n\), we have

\[W(0)\bm{x}_{i}=\begin{pmatrix}\bm{w}_{1}(0)^{T}\bm{x}_{i}\\ \vdots\\ \bm{w}_{m}(0)^{T}\bm{x}_{i}\end{pmatrix}\sim N\bigg{(}0,\frac{\|\bm{x}_{i}\|_{2 }^{2}}{m^{1+2q}}I_{m}\bigg{)},\] (81)where \(\|\bm{x}_{i}\|_{2}^{2}=1\) by assumption. By the lemma B.2, with high probability,

\[\frac{1}{n}\sum_{i=1}^{n}\|W(0)\bm{x}_{i}\|_{2}=\frac{1}{m^{q}}+ \mathcal{O}\bigg{(}\sqrt{\frac{8\log(2n/\delta)}{m^{1+2q}}}\bigg{)}.\] (82)

Therefore,

\[|\bm{\bar{2}}|=\mathcal{O}\bigg{(}\frac{1}{m^{q}}\bigg{)}\] (83)

with high probability, and it comes to our conclusion. 

## Appendix C The Proof of Theorem 5.5

To prove the Theorem 5.5, the following lemma is crucial.

**Lemma C.1** (Dynamic of Weights for A Nonlinear Model for Initial Steps).: _Given gradient descent to optimize the weights, the dynamic of the weights at the initial step \(t\) for the non-linear model is_

\[\dot{W}(t)=\begin{pmatrix}a_{1}\widetilde{\bm{x}}_{1}^{T}(t)\\ \vdots\\ a_{m}\widetilde{\bm{x}}_{m}^{T}(t)\end{pmatrix},\] (84)

_where_

\[\widetilde{\bm{x}}_{r}(t)=a_{r}\frac{1}{n}\sum_{i=1}^{n}\big{(}y _{i}-sig(u_{i}(t))\big{)}\sigma^{\prime}(\bm{w}_{r}^{T}(t)\bm{x}_{i})\bm{x}_{ i},\quad r\in[m].\] (85)

Proof.: Similar to the linear case, the update for the \(r_{th}\) row for the weight matrix is

\[\Delta\bm{w}_{r} =-\frac{\eta}{n}\sum_{i=1}^{n}\frac{\partial l}{\partial u_{i}} \frac{\partial f(W,\bm{x}_{i})}{\partial\bm{w}_{r}}\] (86) \[=\eta a_{r}\frac{1}{n}\sum_{i=1}^{n}\big{(}y_{i}-sig(u_{i})\big{)} \sigma^{\prime}(\bm{w}_{r}^{T}\bm{x}_{i})\bm{x}_{i}.\] (87)

Hence that the update of the weight matrix \(W(t)\) is

\[\Delta W =\begin{pmatrix}\Delta\bm{w}_{1}^{T}\\ \vdots\\ \Delta\bm{w}_{m}^{T}\end{pmatrix}\] (88) \[=\eta\begin{pmatrix}a_{1}\frac{1}{n}\sum_{i=1}^{n}\big{(}y_{i}- sig(u_{i})\big{)}\sigma^{\prime}(\bm{w}_{1}^{T}\bm{x}_{i})\bm{x}_{i}^{T}\\ \vdots\\ a_{m}\frac{1}{n}\sum_{i=1}^{n}\big{(}y_{i}-sig(u_{i})\big{)}\sigma^{\prime}( \bm{w}_{m}^{T}\bm{x}_{i})\bm{x}_{i}^{T}\end{pmatrix}\] (89) \[=\eta\begin{pmatrix}a_{1}\widetilde{\bm{x}}_{1}^{T}\\ \vdots\\ a_{m}\widetilde{\bm{x}}_{m}^{T}\end{pmatrix}.\] (90)

Hence our conclusion. 

**Lemma C.2** (Concentration for Weighted Sum and Square Root Scalar for Bounded Variables).: _Suppose \(X_{r}-\mu\in[l,u],\forall i\in[m]\) is independent identically distributed r.v. with mean 0. Given a vector \(\bm{v}\in\mathbb{R}^{m},\|\bm{v}\|_{2}=1\), and \(\bm{a}\) such that \(a_{r}\in\{-1,1\},r\in[m]\) we have concentration inequality_

\[\mathbb{P}\bigg{(}\Big{|}\frac{1}{\sqrt{m}}\sum_{r=1}^{m}a_{r}v_{r }X_{r}-\frac{1}{\sqrt{m}}\bm{a}^{T}\bm{v}\mu\Big{|}\geq\epsilon\bigg{)}\leq 2 \exp\bigg{\{}-\frac{m\epsilon^{2}}{2(u-l)^{2}}\bigg{\}}.\] (91)Proof.: We first show the moment generation function of the r.v. we would like to estimate.

\[\mathbb{E}\bigg{[}\exp\bigg{\{}t\Big{(}\frac{1}{\sqrt{m}}\sum_{r=1}^{m}a_{r}v_{r} X_{r}-\frac{1}{\sqrt{m}}\bm{a}^{T}\bm{v}\mu\Big{)}\bigg{\}}\bigg{]}\leq\exp\bigg{\{} \frac{t^{2}}{2m}(u-l)^{2}\bigg{\}}.\] (92)

Now, we prove this inequality.

\[\mathbb{E}\bigg{[}\exp\bigg{\{}t\Big{(}\frac{1}{\sqrt{m}}\sum_{r=1 }^{m}a_{r}v_{r}X_{r}-\frac{1}{\sqrt{m}}\bm{a}^{T}\bm{v}\mu\Big{)}\bigg{\}}\bigg{]} =\mathbb{E}\bigg{[}\exp\bigg{\{}\frac{t}{\sqrt{m}}\Big{(}\sum_{r= 1}^{m}a_{r}v_{r}X_{r}-\sum_{r=1}^{m}a_{r}v_{r}\mu\Big{)}\bigg{\}}\bigg{]}\] (93) \[=\prod_{r=1}^{m}\mathbb{E}\bigg{[}\exp\bigg{\{}\frac{ta_{r}v_{r}} {\sqrt{m}}\Big{(}X_{r}-\mu\Big{)}\bigg{\}}\bigg{]}\] (94) \[\leq\prod_{r=1}^{m}\exp\bigg{\{}\frac{v_{r}^{2}t^{2}}{2m}(u-l)^{2 }\bigg{\}}\] (95) \[=\exp\bigg{\{}\frac{t^{2}}{2m}(u-l)^{2}\sum_{r=1}^{m}v_{r}^{2} \bigg{\}}\] (96) \[=\exp\bigg{\{}\frac{t^{2}}{2m}(u-l)^{2}\bigg{\}},\] (97)

where \(t\geq 0\). Hence by Chernoff's bound, we have that

\[\mathbb{P}\bigg{(}\Big{|}\frac{1}{\sqrt{m}}\sum_{r=1}^{m}a_{r}v_ {r}X_{r}-\frac{1}{\sqrt{m}}\bm{a}^{T}\bm{v}\mu\Big{|}\geq\epsilon\bigg{)} \leq 2\inf_{t\geq 0}\frac{\exp\big{\{}\frac{t^{2}}{2m}(u-l)^{2} \big{\}}}{\exp\{t\epsilon\}}\] (98) \[=2\exp\bigg{\{}-\frac{m\epsilon^{2}}{2(u-l)^{2}}\bigg{\}}\] (99)

Here, we prove the theorem 5.5. Given a shallow neural network, with high probability we have

\[\dot{\varrho}_{\bm{a},\sigma\circ W}(t) =\frac{\eta\varrho_{\bm{a},\sigma\circ W}(t)}{\mathbb{E}_{\bm{x}} \|D(t)W(t)\|_{2}^{2}}\bigg{[}\sum_{\tau=1}^{t}(1-\bm{a}^{T}\bm{v}(\tau)\bm{a}^ {T}\bm{v}(t))\mathbb{E}_{\bm{x}}\big{[}\widetilde{\bm{x}}^{*}(\tau)^{T} \widetilde{\bm{x}}^{*}(t)\big{]}\] (100) \[+\max\bigg{\{}\mathcal{O}\bigg{(}\frac{1}{\sqrt{m}}\bigg{)}, \mathcal{O}\bigg{(}\frac{1}{m^{q}}\bigg{)}\bigg{\}}\bigg{]}\] (101)

Proof.: Because of activation function depends on the input \(\bm{x}\), we cannot drop the expectation \(\mathbb{E}_{\bm{x}}\). Hence, we have

\[\dot{\varrho}_{\bm{a},W}(t) =\frac{d}{dt}\varrho_{\bm{a},W}(t)\] (102) \[=\frac{d}{dt}\frac{\mathbb{E}_{\bm{x}}[\|\bm{a}^{T}D(t)W(t)\|_{2} ^{2}]^{\frac{1}{2}}}{\mathbb{E}_{\bm{x}}[\|D(t)W(t)\|_{2}^{2}]^{\frac{1}{2}}}\] (103) \[=\frac{\varrho_{\bm{a},W}(t)}{2}\bigg{(}\frac{\mathbb{E}_{\bm{x}} d\|\bm{a}^{T}D(t)W(t)\|_{2}^{2}/dt}{\mathbb{E}_{\bm{x}}\|\bm{a}^{T}D(t)W(t)\|_{2}^{ 2}}-\frac{\mathbb{E}_{\bm{x}}d\|D(t)W(t)\|_{2}^{2}/dt}{\mathbb{E}_{\bm{x}}\|D (t)W(t)\|_{2}^{2}}\bigg{)}\] (104) \[\geq\frac{\varrho_{\bm{a},W}(t)}{2\mathbb{E}_{\bm{x}}\|D(t)W(t)\| _{2}^{2}}\mathbb{E}_{\bm{x}}\bigg{(}\frac{d\|\bm{a}^{T}D(t)W(t)\|_{2}^{2}}{dt} -\frac{d\|D(t)W(t)\|_{2}^{2}}{dt}\bigg{)},\] (105)

where \(D(t)=diag(\sigma^{\prime}(\bm{w}_{1}^{T}\bm{x}),...,\sigma^{\prime}(\bm{w}_{m }^{T}\bm{x}))\). And for ReLu activation function, \(\sigma^{\prime}(\bm{w}_{m}^{T}\bm{x})=\bm{1}_{\{\bm{w}_{m}^{T}\bm{x}\geq 0\}}\). We assume that the activation does not change at an infinitely small change of \(t\), implying \(\dot{D}(t)=\bm{0}\). Hence, we have \[\frac{d\|\bm{a}^{T}D(t)W(t)\|_{2}^{2}}{dt} =\bm{a}^{T}\bigg{(}\dot{D}(t)W(t)W(t)^{T}D(t)+D(t)W(t)W(t)^{T}\dot{D}(t)\] \[+D(t)\dot{W}(t)W(t)^{T}D(t)+D(t)W(t)\dot{W}(t)^{T}D(t)\bigg{)}\bm{a}\] \[=\bm{a}^{T}\bigg{(}D(t)\dot{W}(t)W(t)^{T}D(t)+D(t)W(t)\dot{W}(t)^{ T}D(t)\bigg{)}\bm{a}\] \[=2\underbrace{\bm{a}^{T}D(t)\eta\sum_{\tau=1}^{t}\begin{pmatrix} a_{1}\widetilde{\bm{x}}_{1}^{T}(\tau)\\ \vdots\\ a_{m}\widetilde{\bm{x}}_{m}^{T}(\tau)\end{pmatrix}(a_{1}\widetilde{\bm{x}}_{ 1}(t)\quad\cdots\quad a_{m}\widetilde{\bm{x}}_{m}(t))\,D(t)\bm{a}}_{\mbox{ 1}}\] \[+2\underbrace{\bm{a}^{T}D(t)\begin{pmatrix}a_{1}\widetilde{\bm{x}} _{1}^{T}(t)\\ \vdots\\ a_{m}\widetilde{\bm{x}}_{m}^{T}(t)\end{pmatrix}W(0)^{T}D(t)\bm{a}}_{\mbox{ 2}}.\]

And similarly,

\[\frac{d\|D(t)W(t)\|_{2}^{2}}{dt} =\bm{v}^{T}(t)\bigg{(}D(t)\dot{W}(t)W(t)^{T}D(t)+D(t)W(t)\dot{W}( t)^{T}D(t)\bigg{)}\bm{v}(t)\] \[=2\underbrace{\bm{v}^{T}(t)D(t)\eta\sum_{\tau=1}^{t}\begin{pmatrix} a_{1}\widetilde{\bm{x}}_{1}^{T}(\tau)\\ \vdots\\ a_{m}\widetilde{\bm{x}}_{m}^{T}(\tau)\end{pmatrix}(a_{1}\widetilde{\bm{x}}_{1 }(t)\quad\cdots\quad a_{m}\widetilde{\bm{x}}_{m}(t))\,D(t)\bm{v}(t)}_{\mbox{ 1}}\] \[+2\underbrace{\bm{v}^{T}(t)D(t)\begin{pmatrix}a_{1}\widetilde{\bm{x }}_{1}^{T}(t)\\ \vdots\\ a_{m}\widetilde{\bm{x}}_{m}^{T}(t)\end{pmatrix}W(0)^{T}D(t)\bm{v}(t)}_{\mbox{ 2}}.\]

1 can be

\[\mbox{1} =\eta\sum_{\tau=1}^{t}\bigg{(}\sum_{r=1}^{m}a_{r}^{2}\sigma^{ \prime}(\bm{w}_{r}(\tau)^{T}\bm{x})\widetilde{\bm{x}}_{r}^{T}(\tau)\bigg{)} \bigg{(}\sum_{r=1}^{m}a_{r}^{2}\sigma^{\prime}(\bm{w}_{r}(t)^{T}\bm{x}) \widetilde{\bm{x}}_{r}(t)\bigg{)}\] (106) \[=\eta\sum_{\tau=1}^{t}\bigg{[}\frac{1}{n}\sum_{i=1}^{n}\sum_{r=1} ^{m}\Big{[}a_{r}^{2}\sigma^{\prime}(\bm{w}_{r}(\tau)^{T}\bm{x})\sigma^{\prime }(\bm{w}_{r}(\tau)^{T}\bm{x}_{i})\Big{]}(y-sig(u_{i}))\bm{x}_{i}^{T}\bigg{]}\] (107) \[\cdot\bigg{[}\frac{1}{n}\sum_{i=1}^{n}\sum_{r=1}^{m}\Big{[}a_{r}^ {2}\sigma^{\prime}(\bm{w}_{r}(t)^{T}\bm{x})\sigma^{\prime}(\bm{w}_{r}(t)^{T} \bm{x}_{i})\Big{]}(y-sig(u_{i}))\bm{x}_{i}\bigg{]}.\] (108)

Since \(a_{r}\in\{-\frac{1}{\sqrt{m}},\,\frac{1}{\sqrt{m}}\}\) and the derivative activation function \(\sigma^{\prime}\) is bounded by \(M\) by assumption, we have that

\[\mathbb{P}_{W(0)}\bigg{(}\bigg{|}\sum_{r=1}^{m}a_{r}^{2}\sigma^{ \prime}(\bm{w}_{r}(t)^{T}\bm{x})\sigma^{\prime}(\bm{w}_{r}(t)^{T}\bm{x}_{i})- \alpha_{i}(t;\bm{x})\bigg{|}\geq\epsilon\bigg{)}\] (109) \[=\mathbb{P}_{W(0)}\bigg{(}\bigg{|}\frac{1}{m}\sum_{r=1}^{m}\sigma ^{\prime}(\bm{w}_{r}(t)^{T}\bm{x})\sigma^{\prime}(\bm{w}_{r}(t)^{T}\bm{x}_{i})- \alpha_{i}(t;\bm{x})\bigg{|}\geq\epsilon\bigg{)}\leq 2e^{-\frac{m\epsilon^{2}}{2 3M^{4}}},\] (110)where

\[\alpha_{i}(t,\bm{x})=\mathbb{E}_{W(0)}\Big{[}\sigma^{\prime}(\bm{w}(t)^{T}\bm{x}) \sigma^{\prime}(\bm{w}(t)^{T}\bm{x}_{i})\Big{]}.\] (111)

Equivalently, with probability at least \(1-\delta\), we have

\[\Big{|}\sum_{r=1}^{m}a_{r}^{2}\sigma^{\prime}(\bm{w}_{r}(t)^{T}\bm{x})\sigma^{ \prime}(\bm{w}_{r}(t)^{T}\bm{x}_{i})-\alpha_{i}(t;\bm{x})\Big{|}\leq\sqrt{ \frac{2M^{4}}{m}log\frac{2}{\delta}}\] (112)

Let

\[\widetilde{\bm{x}}^{*}(t) \stackrel{{\triangle}}{{=}}\frac{1}{n}\sum_{i=1}^{n }\alpha_{i}(t,\bm{x})\big{(}y_{i}-sig(u_{i}(t))\big{)}\bm{x}_{i}\] (113) \[\widetilde{\bm{x}}(t) \stackrel{{\triangle}}{{=}}\frac{1}{n}\sum_{i=1}^{n }\big{(}y_{i}-sig(u_{i}(t))\big{)}\bm{x}_{i}\] (114)

Hence, with probability at least \(1-\delta\),

\[\text{\textcircled{1}} =\eta\sum_{\tau=1}^{t}\bigg{[}\frac{1}{n}\sum_{i=1}^{n}\alpha_{i }(\tau,\bm{x})\big{(}y_{i}-sig(u_{i}(\tau))\big{)}\bm{x}_{i}^{T}+\mathcal{O} \bigg{(}\sqrt{\frac{1}{m}\log\frac{2}{\delta}}\bigg{)}\widetilde{\bm{x}}(\tau )^{T}\bigg{]}\] (115) \[\cdot\bigg{[}\frac{1}{n}\sum_{i=1}^{n}\alpha_{i}(t,\bm{x})\big{(} y_{i}-sig(u_{i}(t))\big{)}\bm{x}_{i}+\mathcal{O}\bigg{(}\sqrt{\frac{1}{m}\log \frac{2}{\delta}}\bigg{)}\widetilde{\bm{x}}(t)\bigg{]}\] (116) \[=\eta\sum_{\tau=1}^{t}\bigg{[}\widetilde{\bm{x}}^{*}(\tau)^{T}+ \mathcal{O}\bigg{(}\sqrt{\frac{1}{m}\log\frac{2}{\delta}}\bigg{)}\widetilde{ \bm{x}}(\tau)^{T}\widetilde{\bm{x}}(t)\bigg{]}\cdot\bigg{[}\widetilde{\bm{x}}^ {*}(t)+\mathcal{O}\bigg{(}\sqrt{\frac{1}{m}\log\frac{2}{\delta}}\bigg{)} \widetilde{\bm{x}}(t)\bigg{]}\] (117) \[=\eta\sum_{\tau=1}^{t}\widetilde{\bm{x}}^{*}(\tau)^{T}\widetilde{ \bm{x}}^{*}(t)+\mathcal{O}\bigg{(}\sqrt{\frac{1}{m}\log\frac{2}{\delta}} \bigg{)}\widetilde{\bm{x}}(\tau)^{T}\widetilde{\bm{x}}^{*}(t)\] (118) \[+\mathcal{O}\bigg{(}\sqrt{\frac{1}{m}\log\frac{2}{\delta}}\bigg{)} \widetilde{\bm{x}}^{*}(\tau)^{T}\widetilde{\bm{x}}(t)+\mathcal{O}\bigg{(} \frac{1}{m}\log\frac{2}{\delta}\bigg{)}\widetilde{\bm{x}}(t)^{T}\widetilde{ \bm{x}}(t).\] (119)

Since

\[\|\widetilde{\bm{x}}^{*}(t)\|_{2} \leq\frac{1}{n}\bigg{\|}\begin{pmatrix}\alpha_{1}(\tau,\bm{x}) \big{(}y_{1}-sig(u_{1}(t))\big{)}\\ \vdots\\ \alpha_{n}(\tau,\bm{x})\big{(}y_{n}-sig(u_{n}(t))\big{)}\end{pmatrix}\bigg{\|}_ {2}\bigg{\|}\begin{pmatrix}\bm{x}_{1}^{T}\\ \vdots\\ \bm{x}_{n}^{T}\end{pmatrix}\bigg{\|}_{2}\leq M^{2}\] (120) \[\|\widetilde{\bm{x}}(t)\|_{2} \leq\frac{1}{n}\bigg{\|}\begin{pmatrix}y_{1}-sig(u_{1}(t))\\ \vdots\\ y_{n}-sig(u_{n}(t))\end{pmatrix}\bigg{\|}_{2}\bigg{\|}\begin{pmatrix}\bm{x}_{1}^ {T}\\ \vdots\\ \bm{x}_{n}^{T}\end{pmatrix}\bigg{\|}_{2}\leq 1,\] (121)

we have that for at least \(1-\delta\),

\[\text{\textcircled{1}}=\eta\sum_{\tau=1}^{t}\widetilde{\bm{x}}^{*}(\tau)^{T} \widetilde{\bm{x}}^{*}(t)+\mathcal{O}\bigg{(}\sqrt{\frac{1}{m}\log\frac{2}{ \delta}}\bigg{)}\] (122)

Similarly,

\[\text{\textcircled{1}}^{\prime} =\eta\sum_{\tau=1}^{t}\bigg{[}\frac{1}{n}\sum_{i=1}^{n}\sum_{r=1}^ {m}\Big{[}v_{r}(t)a_{r}\sigma^{\prime}(\bm{w}_{r}(\tau)^{T}\bm{x})\sigma^{ \prime}(\bm{w}_{r}(\tau)^{T}\bm{x}_{i})\Big{]}(y_{i}-sig(u_{i}))\bm{x}_{i}^{T} \bigg{]}\] (123) \[\cdot\bigg{[}\frac{1}{n}\sum_{i=1}^{n}\sum_{r=1}^{m}\Big{[}v_{r}(t )a_{r}\sigma^{\prime}(\bm{w}_{r}(t)^{T}\bm{x})\sigma^{\prime}(\bm{w}_{r}(t)^{T} \bm{x}_{i})\Big{]}(y_{i}-sig(u_{i}))\bm{x}_{i}\bigg{]},\] (124)Let \(\bm{a}^{*}=\sqrt{m}\bm{a}\in\{-1,1\}^{m}\) and by the conclusion of lemma C.2,

\[\mathbb{P}_{W(0)}\bigg{(}\bigg{|}\sum_{r=1}^{m}v_{r}(t)a_{r}\sigma^{ \prime}(\bm{w}_{r}(t)^{T}\bm{x})\sigma^{\prime}(\bm{w}_{r}(t)^{T}\bm{x}_{i})- \sum_{r=1}^{m}v_{r}(t)a_{r}\alpha_{i}(t,\bm{x})\bigg{|}\geq\epsilon\bigg{)}\] (125) \[=\mathbb{P}_{W(0)}\bigg{(}\bigg{|}\frac{1}{\sqrt{m}}\sum_{r=1}^{m }a_{r}^{*}v_{r}(t)\sigma^{\prime}(\bm{w}_{r}(t)^{T}\bm{x})\sigma^{\prime}(\bm{ w}_{r}(t)^{T}\bm{x}_{i})-\bm{a}^{T}\bm{v}(t)\alpha_{i}(t;\bm{x})\bigg{|}\geq\epsilon \bigg{)}\] (126) \[\leq 2\exp\bigg{\{}-\frac{m\epsilon^{2}}{2M^{4}}\bigg{\}}.\] (127)

Equivalently, with probability at least \(1-\delta\),

\[\bigg{|}\frac{1}{\sqrt{m}}\sum_{r=1}^{m}a_{r}^{*}v_{r}(t)\sigma^{\prime}(\bm {w}_{r}(t)^{T}\bm{x})\sigma^{\prime}(\bm{w}_{r}(t)^{T}\bm{x}_{i})-\bm{a}^{T} \bm{v}(t)\alpha_{i}(t;\bm{x})\bigg{|}\leq\sqrt{\frac{2M^{4}}{m}\log\frac{2}{ \delta}}.\] (128)

Hence,

\[\mathbb{\hat{1}}^{\prime} =\eta\sum_{\tau=1}^{t}\bigg{[}\frac{1}{n}\sum_{i=1}^{n}\bm{a}^{T} \bm{v}(\tau)\alpha_{i}(\tau,\bm{x})\big{(}y_{i}-sig(u_{i}(\tau))\big{)}\bm{x} _{i}^{T}+\mathcal{O}\bigg{(}\sqrt{\frac{1}{m}\log\frac{2}{\delta}}\bigg{)} \widetilde{\bm{x}}(\tau)^{T}\bigg{]}\] (129) \[\cdot\bigg{[}\frac{1}{n}\sum_{i=1}^{n}\bm{a}^{T}\bm{v}(t)\alpha_{ i}(t,\bm{x})\big{(}y_{i}-sig(u_{i}(t))\big{)}\bm{x}_{i}+\mathcal{O}\bigg{(} \sqrt{\frac{1}{m}\log\frac{2}{\delta}}\bigg{)}\widetilde{\bm{x}}(t)\bigg{]}\] (130) \[=\eta\sum_{\tau=1}^{t}\bigg{[}\bm{a}^{T}\bm{v}(\tau)\widetilde{ \bm{x}}^{*}(\tau)^{T}+\mathcal{O}\bigg{(}\sqrt{\frac{1}{m}\log\frac{2}{\delta} }\bigg{)}\widetilde{\bm{x}}(\tau)^{T}\bigg{]}\cdot\bigg{[}\bm{a}^{T}\bm{v}(t) \widetilde{\bm{x}}^{*}(t)+\mathcal{O}\bigg{(}\sqrt{\frac{1}{m}\log\frac{2}{ \delta}}\bigg{)}\widetilde{\bm{x}}(t)\bigg{]}\] (131) \[=\eta\sum_{\tau=1}^{t}\bm{a}^{T}\bm{v}(\tau)\bm{a}^{T}\bm{v}(t) \widetilde{\bm{x}}^{*}(\tau)^{T}\widetilde{\bm{x}}^{*}(t)\] (132) \[+\mathcal{O}\bigg{(}\sqrt{\frac{1}{m}\log\frac{2}{\delta}}\bigg{)} \widetilde{\bm{x}}(\tau)^{T}\widetilde{\bm{x}}^{*}(t)+\mathcal{O}\bigg{(} \sqrt{\frac{1}{m}\log\frac{2}{\delta}}\bigg{)}\widetilde{\bm{x}}^{*}(\tau)^{T} \widetilde{\bm{x}}(t)+\mathcal{O}\bigg{(}\frac{1}{m}\log\frac{2}{\delta} \bigg{)}\widetilde{\bm{x}}(t)^{T}\widetilde{\bm{x}}(t).\] (133)

We have that for at least \(1-\delta\),

\[\mathbb{\hat{1}}^{\prime}=\eta\sum_{\tau=1}^{t}\bm{a}^{T}\bm{v}(\tau)\bm{a}^{T} \bm{v}(t)\widetilde{\bm{x}}^{*}(\tau)^{T}\widetilde{\bm{x}}^{*}(t)+\mathcal{O} \bigg{(}\sqrt{\frac{1}{m}\log\frac{2}{\delta}}\bigg{)}\] (134)

For \(\mathbb{\hat{2}}\), we have

\[\mathbb{\hat{2}}| =\big{|}\bm{a}^{T}D(t)\begin{pmatrix}a_{1}\widetilde{\bm{x}}_{1}^{ T}(t)\\ \vdots\\ a_{m}\widetilde{\bm{x}}_{m}^{T}(t)\end{pmatrix}W(0)^{T}D(t)\bm{a}\big{|}\] (135) \[\leq\|\bm{a}^{T}D(t)\|_{2}^{2}\|\left(a_{1}W(0)\widetilde{\bm{x} }_{1}(t)\quad\cdots\quad a_{m}W(0)\widetilde{\bm{x}}_{m}(t)\right)\big{\|}_{2}\] (136) \[\leq M^{2}\Big{\|}\left(a_{1}W(0)\widetilde{\bm{x}}_{1}(t)\quad \cdots\quad a_{m}W(0)\widetilde{\bm{x}}_{m}(t)\right)\big{\|}_{F}\] (137) \[\leq M^{2}\sqrt{\sum_{r=1}^{m}a_{r}^{2}\|W(0)\widetilde{\bm{x}}_{r }\|_{2}^{2}},\] (138)where

\[\|W(0)\widetilde{\bm{x}}_{r}\|_{2}^{2} =\Big{\|}\frac{1}{n}\sum_{i=1}^{n}\sigma^{\prime}(\bm{w}_{r}(0)^{T} \bm{x}_{i})(y_{i}-sig(u_{i}))W(0)\bm{x}_{i}\Big{\|}_{2}^{2}\] (139) \[\leq\frac{1}{n^{2}}\bigg{\|}\begin{pmatrix}\sigma^{\prime}(\bm{w} _{r}(0)^{T}\bm{x}_{1})(y_{1}-sig(u_{1}))\\ \vdots\\ \sigma^{\prime}(\bm{w}_{r}(0)^{T}\bm{x}_{n})(y_{n}-sig(u_{n}))\end{pmatrix} \bigg{\|}_{2}^{2}\bigg{\|}\left(W(0)\bm{x}_{1}\quad\cdots\quad W(0)\bm{x}_{n} \right)\bigg{\|}_{2}^{2}\] (140) \[\leq\frac{1}{n^{2}}\sum_{i=1}^{n}\underbrace{\sigma^{\prime}(\bm {w}_{r}(0)^{T}\bm{x}_{i})^{2}(y_{i}-sig(u_{i}))^{2}}_{0\leq..\leq M^{2}}\bigg{\|} \left(W(0)\bm{x}_{1}\quad\cdots\quad W(0)\bm{x}_{n}\right)\bigg{\|}_{F}^{2}\] (141) \[\leq\frac{M^{2}}{n}\sum_{i=1}^{n}\|W(0)\bm{x}_{i}\|_{2}^{2}\] (142)

Similar to the proof of the linear case, and accords to Lemma B.2, we have that with probability at least \(1-\delta\),

\[\sqrt{\frac{1}{n}\sum_{i=1}^{n}\|W(0)\bm{x}_{i}\|_{2}^{2}}=\frac{1 }{m^{q}}+\mathcal{O}\bigg{(}\sqrt{\frac{8\log(2/\delta)}{m^{1+2q}n}}\bigg{)}.\] (143)

Therefore, with high probability,

\[|\widetilde{\ref{eq:1}}| \leq M^{4}\sqrt{\sum_{r=1}^{m}a_{r}^{2}\frac{1}{n}\sum_{i=1}^{n} \|W(0)\bm{x}_{i}\|_{2}^{2}}\] (144) \[=M^{4}\sqrt{\frac{1}{n}\sum_{i=1}^{n}\|W(0)\bm{x}_{i}\|_{2}^{2}}\] (145) \[=\mathcal{O}\bigg{(}\frac{1}{m^{q}}\bigg{)}\] (146)

Follow the exact same procedure. With high probability, we have

\[|\widetilde{\ref{eq:1}}^{\prime}| =\Big{|}\bm{v}^{T}(t)D(t)\begin{pmatrix}a_{1}\widetilde{\bm{x}}_ {1}^{T}(t)\\ \vdots\\ a_{m}\widetilde{\bm{x}}_{m}^{T}(t)\end{pmatrix}W(0)^{T}D(t)\bm{v}(t)\Big{|}\] (147) \[\leq\|\bm{v}(t)^{T}D(t)\|_{2}^{2}\big{\|}\left(a_{1}W(0)\widetilde {\bm{x}}_{1}(t)\quad\cdots\quad a_{m}W(0)\widetilde{\bm{x}}_{m}(t)\right) \big{\|}_{2}\] (148) \[\leq M^{2}\sqrt{\sum_{r=1}^{m}a_{r}^{2}\|W(0)\widetilde{\bm{x}}_ {r}\|_{2}^{2}}\] (149) \[=\mathcal{O}\bigg{(}\frac{1}{m^{q}}\bigg{)}\] (150)

We put all the information together, with high probability, we have

\[\dot{\varrho}_{\bm{a},\sigma\circ W}(t) \geq\frac{\theta_{\bm{a},\sigma\circ W}(t)}{\mathbb{E}_{\bm{x}} \|D(t)W(t)\|_{2}^{2}}\mathbb{E}_{\bm{x}}\bigg{(}\raisebox{-0.86pt}{\hbox{$ \mathbbm{1}$}}-\raisebox{0.86pt}{\hbox{$\mathbbm{1}$}}^{\prime}+\raisebox{-0.86pt}{\hbox{$\mathbbm{2}$}}-\raisebox{0.86pt}{\hbox{$\mathbbm{2}$}}^{ \prime}\bigg{)}\] (151) \[\geq\frac{\theta_{\bm{a},\sigma\circ W}(t)}{\mathbb{E}_{\bm{x}} \|D(t)W(t)\|_{2}^{2}}\bigg{[}\eta\sum_{\tau=1}^{t}(1-\bm{a}^{T}\bm{v}(\tau)\bm {a}^{T}\bm{v}(t))\mathbb{E}_{\bm{x}}\big{[}\widetilde{\bm{x}}^{*}(\tau)^{T} \widetilde{\bm{x}}^{*}(t)\big{]}\] (152) \[+\max\bigg{\{}\mathcal{O}\bigg{(}\frac{1}{m^{\frac{1}{2}}}\bigg{)},\mathcal{O}\bigg{(}\frac{1}{m^{q}}\bigg{)}\bigg{\}}\bigg{]}\] (153)

## Appendix D Extra Experiment Results

Figure 5 summarizes the statistics of linear correlation and relative variability, illustrating that co-correlation is the dominant factor in the decomposition outlined in Theorem 4.5.

Figure 6 shows the accumulated change in co-correlation during training across varying network widths. Each line, represented by different colours, corresponds to a unique weight initialization setting of \(q\). A clear inference from the figure is the positive relationship between the accumulated co-correlation change and network width under the same \(q\)--the wider the network, the greater the accumulated co-correlation change. Moreover, as we shift the weight initialization setting from \(q=-0.15\) to \(q=0.25\), the accumulation of co-correlation also increases. These observations align with our Theorem 5.3 and 5.5, indicating that the increase in co-correlation is inversely proportional to \(\|W(t)\|_{2}\).

Figure 5: Linear Correlation \(\&\) Relative Std. The linear correlation and the standard deviation over the mean are given for all MLPs with the initialization parameter \(q=0.25\).

Figure 6: The accumulation of co-correlation under different width. X-axis denotes the width of the neural network, and the line in different color shows the result under different setting of weight initialization. The lower plot is the result for linear model and the result for shallow ReLU is at the top.

Algorithm

We use the _Power Iteration algorithm_[10] along with _Functorch_[17] to compute co-correlation and other relevant statistics. This combination assists in approximating the \(L_{2}\)-norm of the Jacobian for the layers under consideration. The algorithm to estimate the related statistics including co-correlation are outlined in Algorithm 1. The power iteration used to obtain the \(L_{2}\)-norm of the Jacobian is summarized in Algorithm 2.

```
0: Input dataset and modules \(\phi,\varphi\) for the given blocks in sequential \(\{\bm{x}_{i},i=1,..,N\}\)
0: Approximation of co-correlations \(\widehat{\phi}_{\phi,\varphi}\), linear correlation \(\widehat{\rho}_{\phi,\varphi}\), mean \(\widehat{\mu}_{\phi},\widehat{\mu}_{\varphi}\) and variance \(\sigma_{\phi},\sigma_{\varphi}\) of the Jacobian of the module w.r.t. their inputs
1: Computation of Jacobian of \(\|J_{\varphi}(\bm{x}_{i})\|_{2},\|J_{\phi}(\varphi(\bm{x}_{i}))\|_{2}\) for \(\{\bm{x}_{i},i\in[n]\}\) w.r.t. their inputs
2:\(\widehat{\phi}_{\phi,\varphi}\leftarrow\frac{\frac{1}{N}\sum_{i=1}^{N}\|J_{ \phi}(\varphi(\bm{x}_{i}))\|_{2}\cdot J_{\varphi}(\bm{x}_{i})\|_{2}}{N\sum_{i= 1}^{N}\|J_{\phi}(\varphi(\bm{x}_{i}))\|_{2}\|J_{\varphi}(\bm{x}_{i})\|_{2}}\)
3:\(\widehat{\rho}_{\phi,\varphi}=\frac{\frac{1}{N}\sum_{i=1}^{N}\|J_{\phi}(\varphi (\bm{x}_{i}))\|_{2}\frac{\|J_{\varphi}(\bm{x}_{i})\|_{2}}{2}}{\left(\frac{1}{ N}\sum_{i=1}^{N}\|J_{\phi}(\varphi(\bm{x}_{i}))\|_{2}\right)^{\frac{1}{2}}\left( \frac{1}{N}\sum_{i=1}^{N}\|J_{\phi}(\varphi(\bm{x}_{i}))\|_{2}^{2}\right)^{ \frac{1}{2}}}\)
4:\(\widehat{\mu}_{\phi}\leftarrow\frac{1}{N}\sum_{i=1}^{N}\|J_{\phi}(\varphi(\bm{x }_{i}))\|_{2}\|J_{\varphi}(\bm{x}_{i})\|_{2}\)
5:\(\widehat{\sigma}_{\phi}\leftarrow\left[\frac{1}{N-1}\sum_{i=1}^{N}(\|J_{\phi}( \varphi(\bm{x}_{i}))\|_{2}\|J_{\varphi}(\bm{x}_{i})\|_{2}-\hat{\mu}_{\phi})^{ 2}\right]^{\frac{1}{2}}\) ```

**Algorithm 1** Estimation of Related Statistics

```
0: Input dataset and module for the given sequential blocks \(\{\bm{x}_{i},i=1,..,N\},\phi,\varphi\)
0: Approximation of co-correlations \(\rho_{\phi,\varphi}\), linear correlation \(\rho_{\phi,\varphi}\), mean \(\mu_{\phi},\mu_{\varphi}\) and variance \(\sigma_{\phi},\sigma_{\varphi}\) of the Jacobian of the module w.r.t. their inputs
1: Computation of \(\|J_{\varphi}(\bm{x}_{i})\|_{2}\) for\(\{\bm{x}_{i},i\in[n]\}\)
2:while\(i\leq N_{b}\)do
3: sample randomly \(\bm{u}\sim U([0,1])\)
4:\(\bm{u}\leftarrow\bm{u}/\|\bm{u}\|_{2}\)
5: generate jacobian-vector product function \(jvp(\cdot;\bm{x}_{i})\)
6: generating vector-jacobian product function \(vjp(\cdot;\bm{x}_{i})\)
7:\(\|J_{\varphi}(\bm{x}_{i})\|_{2}^{old}\gets None\)
8:while\(err\geq 10^{-6}\)do
9:\(\bm{v}\gets jvp(\bm{u};\bm{x}_{i})\)
10:\(\bm{u}\gets vjp(\bm{v};\bm{x}_{i})\)
11:\(\bm{u}\leftarrow\bm{u}/\|\bm{u}\|_{2}\)
12:\(\bm{v}\leftarrow\bm{v}/\|\bm{v}\|_{2}^{1}\)
13:\(\|J_{\varphi}(\bm{x}_{i})\|_{2}^{new}\leftarrow\frac{\bm{u}/\|\bm{u}\|_{2}}{ \bm{v}/\|\bm{v}\|_{2}}\)
14:if\(\|J_{\varphi}(\bm{x}_{i})\|_{2}^{old}\neq None\)then
15:\(err=\left|\frac{\|J_{\varphi}(\bm{x}_{i})\|_{2}^{new}-\|J_{\varphi}(\bm{x}_{i}) \|_{2}^{old}}{\|J_{\varphi}(\bm{x}_{i})\|_{2}^{old}}\right|\)
16:endif
17:\(\|J_{\varphi}(\bm{x}_{i})\|_{2}^{old}=\|J_{\varphi}(\bm{x}_{i})\|_{2}^{new}\)
18:endwhile
19:endwhile ```

**Algorithm 2** Power Iteration Based Computation

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and introduction have clearly stated the claims made, including the contributions made in the paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: A separate section "Discussion and Limitations" has been designed and can be seen in the end of this paper. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: The complete proofs are provided in the appendix. Theorems and Lemmas are properly referenced in this paper. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The algorithm and detailed experiment settings are provided. Guidelines: The code is provided in the supplemental material. * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The code is uploaded in github, including detailed README file. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All details can be checked in Experiment section. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The detailed methods can be found in Experiment section. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The paper introduces the configuration of the computer in Experiment section. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have read the NeurIPS Code of Ethics and confirm that the research conforms in every respect. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: There is no societal impact of the work performed. This is a theoretical paper to understand existing ML methods. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have cited the original paper that produced the code package or dataset properly. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.