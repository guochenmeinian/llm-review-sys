# Schedule Your Edit: A Simple yet Effective Diffusion Noise Schedule for Image Editing

 Haonan Lin\({}^{1}\) Yan Chen\({}^{1}\) Jiahao Wang\({}^{1}\) Wenbin An\({}^{3}\) Mengmeng Wang\({}^{2,5}\)

**Feng Tian\({}^{1}\) Yong Liu\({}^{4,5}\) Guang Dai\({}^{5}\) Jingdong Wang\({}^{6}\) Qianying Wang\({}^{7}\)**

\({}^{1}\) School of Comp. Science & Technology, MOEKLINNS Lab, Xi'an Jiaotong University

\({}^{2}\) College of Comp. Science & Technology, Zhejiang University of Technology

\({}^{3}\) School of Auto. Science & Engineering, MOEKLINNS Lab, Xi'an Jiaotong University

\({}^{4}\) Institute of Cyber-Systems and Control, Zhejiang University

\({}^{5}\) SGIT AI Lab, State Grid Corporation of China

\({}^{6}\) Baidu Inc \({}^{7}\) Lenovo Research

Corresponding Author.This work was completed during the internship at SGIT AI Lab, State Grid Corporation of China.

###### Abstract

Text-guided diffusion models have significantly advanced image editing, enabling high-quality and diverse modifications driven by text prompts. However, effective editing requires inverting the source image into a latent space, a process often hindered by prediction errors inherent in DDIM inversion. These errors accumulate during the diffusion process, resulting in inferior content preservation and edit fidelity, especially with conditional inputs. We address these challenges by investigating the primary contributors to error accumulation in DDIM inversion and identify the singularity problem in traditional noise schedules as a key issue. To resolve this, we introduce the _Logistic Schedule_, a novel noise schedule designed to eliminate singularities, improve inversion stability, and provide a better noise space for image editing. This schedule reduces noise prediction errors, enabling more faithful editing that preserves the original content of the source image. Our approach requires no additional retraining and is compatible with various existing editing methods. Experiments across eight editing tasks demonstrate the _Logistic Schedule_'s superior performance in content preservation and edit fidelity compared to traditional noise schedules, highlighting its adaptability and effectiveness. (Project page: https://lonelvino.github.io/SYE/)

## 1 Introduction

Text-guided diffusion models have emerged as a leading technique in image generation, offering remarkable visual quality and diversity [2, 42, 50, 69]. The noise latent space of these models can be leveraged to retain and modify images [32, 66, 68], enabling text-guided editing where a source image is adjusted based on a target prompt. This requires first inverting the source image into a latent variable (_e.g._, via DDIM inversion), due to the absence of its predefined latent space [28, 39].

While DDIM inversion proves effective for unconditional diffusion models [43, 55], it results in inferior content preservation and suboptimal edit fidelity when applied to conditional inputs [12, 17]. This phenomenon is particularly evident in image editing, which requires incorporating new conditionals into the generation process [16, 59, 33, 61]. DDIM converts the DDPM into adeterministic process by approximating the Markov process as a non-Markov process based on a local linearization assumption [55]. This approximation introduces noise prediction errors that accumulate throughout the diffusion process, leading to deviations in the inverted latent representation from its original distribution, as illustrated in Fig. 2 left. Recently, inversion-based editing methods have emerged as a promising paradigm to address these issues by aligning the reconstruction path more closely with the DDIM inversion trajectory, thereby ensuring the preservation of the original content in the edited images [41, 15, 44, 10, 25]. However, these methods still heavily rely on the accuracy of the DDIM inversion. This leads us to a fundamental question: **What if we correct the DDIM inversion errors to naturally reduce the loss of original content in the edited images?**

Unlike previous inversion-based editing methods that focus on minimizing the distance between \(\mathbf{x}_{\prime\prime}^{\prime\prime}\) and \(\mathbf{x}_{\prime}^{*}\) (Fig. 2 left), we investigate the primary reason for error accumulation in DDIM inversion. Based on the fact that DDIM samplers can be derived by deterministic ODE processes [3, 38, 71], our analysis reveals that these traditional noise schedule designs result in a singularity problem (Fig. 2 right) when treating the DDIM inversion process as solving a differentiable ODE. This results in unreliable noise predictions from the start, and as errors accumulate, the editing results degrade (Fig. 1). This insight motivates us to address the problem at its source: the noise schedule itself. To our knowledge, this is the first work focusing on designing noise schedules specifically for image editing, providing an optimized solution without requiring complete model retraining [14, 20, 23, 29, 26, 34].

We present a simple yet effective noise schedule, _Logistic Schedule_, designed to resolve the singularity problem of previous noise schedules and enhance inverted latents for image editing. The key ideas behind _Logistic Schedule_ are twofold: (1) creating a well-defined noise schedule to improve inversion stability, and (2) providing a better noise space that enables editing faithful to the source image. Specifically, _Logistic Schedule_ eliminates singularities at the beginning of the inversion process, thereby reducing noise prediction errors in the inverted latents. It enables more stable data perturbation to preserve the original content of the source image in the edited image. Importantly, this design is effective and compatible with other editing methods without requiring additional retraining.

Figure 1: Compared to linear noise schedule, _Logistic Schedule_ demonstrates high fidelity in attributes content editing (a, b) with EF-DDPM [21], preserves the high-level semantics of the source image while performing object translation (c) with pix2pix-zero [45] and style/scene transferring (d, e) with StyleDiffusion [63], and successfully conducts non-rigid alteration (f) via MasaCtrl [6]. Text prompts corresponding to each input image are presented beneath each sample, with words introduced for image editing distinctly highlighted in red.

We conducted experiments across eight distinct editing tasks using approximately 1600 images from diverse scenes. Fig. 1 illustrates that our _Logistic Schedule_ effectively enhances editing results in terms of essential content preservation and edit fidelity compared to commonly used noise schedules like the linear schedule. Moreover, our schedule can be seamlessly integrated with various existing diffusion-based editing techniques, demonstrating its versatility and effectiveness. Our main contributions are summarized as follows: (1) **Theoretical Analysis**: We analyze the failure of DDIM inversion in real-image editing step by step, identifying the singularity in the noise schedule as the key issue to address. (2) **Methodology**: We introduce _Logistic Schedule_, a novel diffusion noise schedule specifically tailored for real-image editing, which effectively reduces prediction errors during inversion. (3) **Superiority**: We showcase _Logistic Schedule_'s adaptability by integrating it with various editing methods and demonstrate its consistent superior performance across different editing tasks.

## 2 Background

This section will introduce diffusion models and their noise schedules, along with DDIM inversion, which are crucial for text-guided editing of real images.

**Diffusion Models.** Denoising Diffusion Probabilistic Models (DDPM) [18] are designed to transform a random noise vector \(\mathbf{x}_{T}\) into a series of intermediate samples \(\mathbf{x}_{t}\), and eventually a final image \(\mathbf{x}_{0}\) by progressively adding Gaussian noise \(\epsilon\sim\mathcal{N}(0,\mathbf{I})\) according to a noise schedule \(\beta_{1},\dots,\beta_{T}\):

\[\mathbf{x}_{t}=\sqrt{1-\beta_{t}}\mathbf{x}_{t-1}+\sqrt{\beta_{t}}\epsilon_{t -1},\]

where \(t\sim[1,T]\) and \(T\) denotes the number of timesteps. The noise schedule determines the distribution of noise scales and is designed to ensure that the noise scale at each step is proportional to the remaining signal, which is usually fixed without additional learning. According to the properties of conditional Gaussian distributions, \(\mathbf{x}_{t}\) can be derived from a real image \(\mathbf{x}_{0}\) in the following closed form by reparameterizing \(\alpha_{t}=1-\beta_{t},\tilde{\alpha_{t}}=\prod_{i=1}^{t}\alpha_{i}\):

\[\mathbf{x}_{t}=\sqrt{\tilde{\alpha}_{t}}\mathbf{x}_{0}+\sqrt{1-\tilde{\alpha }_{t}}\epsilon.\] (1)

Another commonly used sampling method is Denoising Diffusion Implicit Models (DDIM) [55], which formulate a denoising process to generate \(\mathbf{x}_{t-1}\) from a sample \(\mathbf{x}_{t}\) via:

\[\mathbf{x}_{t-1}=\sqrt{\alpha_{t-1}}\underbrace{\left(\frac{\mathbf{x}_{t}- \sqrt{1-\alpha_{t}}\epsilon_{\theta}^{(t)}\left(\mathbf{x}_{t}\right)}{\sqrt{ \alpha_{t}}}\right)}_{\text{predicted }\mathbf{x}_{0}}+\underbrace{\sqrt{1-\alpha_{t-1}- \sigma_{t}^{2}}\cdot\epsilon_{\theta}^{(t)}\left(\mathbf{x}_{t}\right)}_{ \text{direction pointing to }\mathbf{x}_{t}}+\underbrace{\sigma_{t}\epsilon_{t}}_{ \text{random noise}},\] (2)

Figure 2: **Illustration of the DDIM inversion in image editing and its challenges. Left: starting from the source image \(\mathbf{x}_{0}\), the ideal latent \(\mathbf{x}_{t}\) is approximated by the inverted latent \(\mathbf{x}_{t}^{*}\) using DDIM inversion. The perturbed noisy latent \(\mathbf{x}_{T}^{*}\) is then sampled in two branches—one for the source condition and one for the target condition—yielding the reconstructed and edited images respectively. Right: the numerical computations of \(\text{d}\mathbf{x}_{t}/\text{d}t\) for scaled linear and cosine noise schedules, highlighting the singularity at \(t=0\) that leads to potential inaccuracies in noise prediction during inversion.**

where \(\epsilon_{t}\sim\mathcal{N}(0,\mathbf{I})\), \(\sigma_{t}\) is the variance schedule, and \(\epsilon_{\theta}\) is a network trained to predict the noise added. When \(\sigma_{t}=\sqrt{\frac{1-\alpha_{t-1}}{1-\alpha_{t}}}\sqrt{1-\frac{\alpha_{t}}{ \alpha_{t-1}}}\) for all \(t\), the forward process becomes Markovian, and the generation process becomes a DDPM. And in a special case when \(\sigma_{t}=0\) for all \(t\), the forward process become deterministic given \(\mathbf{x}_{t-1}\) and \(\mathbf{x}_{0}\), except for \(t=1\), and the generate process becomes a DDIM.

Inversion in Image Editing.Although text-to-image diffusion models [50; 52; 19] have advanced feature spaces that support various downstream tasks [67; 37; 36], applying them to real images (non-generated images) is challenging because these images lack a natural diffusion feature space. Editing a real image first requires obtaining the latent variables \(\mathbf{x}_{T}\) from the original image \(\mathbf{x}_{0}\) and then performing the generation process under new conditions. To bridge this gap, DDIM inversion [55] is predominantly used due to its deterministic process, which can be represented by reversing the generation process in Eq. 2 with \(\sigma_{t}=0\):

\[\mathbf{x}_{t}^{*}=\frac{\sqrt{\alpha_{t}}}{\sqrt{\alpha_{t-1}}}\mathbf{x}_{t -1}^{*}+\sqrt{\alpha_{t}}\left(\sqrt{\frac{1}{\alpha_{t}}-1}-\sqrt{\frac{1}{ \alpha_{t-1}}-1}\right)\epsilon_{\theta}\left(\mathbf{x}_{t-1}^{*},t-1\right).\]

However, existing editing methods that rely on vanilla DDIM inversion struggle to achieve both content preservation and edit fidelity when applied to real images [1; 4; 16]. Recently, inversion-based editing methods have improved the edited results by maintaining two simultaneous procedures: reconstruction and editing, as shown in Fig. 2 left. These methods align the reconstruction path (\(\mathbf{x}\) ) more closely with the DDIM inversion trajectory (\(\mathbf{x}^{*}\)), ensuring better preservation of the original content in the edited image [41; 15; 44; 25; 10]. Despite their effectiveness, these methods still heavily rely on the accuracy of the inverted latents obtained from DDIM inversion. In contrast, we start from a different perspective, focusing on improving the DDIM inversion accuracy to naturally enhance the edited results. In the following section, we begin with the transition from DDPM to DDIM, emphasizing the need for a better noise schedule for the inversion process.

## 3 On the Failure of DDIM Inversion

### Warmup: Error Accumulation of DDIM

DDIM inversion for real images is unstable due to its reliance on a local linearization assumption at each step, leading to error accumulation and content loss from the original image. Specifically, DDIM assumes that the denoising process in Eq. 2 is roughly invertible, meaning \(\mathbf{x}_{t}^{*}\) can be approximately recovered from \(\mathbf{x}_{t-1}^{*}\) via:

\[\mathbf{x}_{t}^{*}=\frac{\mathbf{x}_{t-1}^{*}-b_{t}\epsilon(\mathbf{x}_{t}^{* },t)}{a_{t}}\approx\frac{\mathbf{x}_{t-1}^{*}-b_{t}\epsilon(\mathbf{x}_{t-1}^ {*},t)}{a_{t}},\] (3)

where \(a_{t}=\sqrt{\alpha_{t-1}/\alpha_{t}}\) and \(b_{t}=-\sqrt{\alpha_{t-1}(1-\alpha_{t})/\alpha_{t}}+\sqrt{1-\alpha_{t-1}}\). This approximation assumes \(\epsilon(\mathbf{x}_{t}^{*},t)\approx\epsilon(\mathbf{x}_{t-1}^{*},t)\), and the inversion's accuracy depends on this assumption. However, ensuring accurate inversion under this assumption requires a sufficient number of discretization steps, which increases time costs and is impractical for many applications. With fewer timesteps or higher noise levels, error accumulation becomes more pronounced, resulting in distorted reconstructions, as shown in Fig. 2 left. This occurs because once we deviate from the linearization assumption, the interpolation operation in Eq. 3 fails. The primary issue arises when estimating the "predicted \(\mathbf{x}_{0}\)" in Eq. 2 at the initial step (\(t=1\), indicated by the red arrow in Fig. 2 left), where a simple expression for the posterior mean conditioned on \(\mathbf{x}_{t}\) no longer exists [55]. Moreover, this problem is exacerbated in image editing, where the denoising process must incorporate new conditions into the image content. This increases the difficulty of noise predictions, leading to more severe artifacts and distortions.

### The Devil Is in the Singularities

To get around this issue, our first insight is to reduce the prediction error at the beginning of the forward (inversion) process. But before we can figure out how to fix the error, we need to pinpoint the problem. We first provide the continuous generalization of DDPM, since sampling from diffusion models can be viewed alternatively as solving the corresponding ODE process [57; 38]:

\[\mathrm{d}\mathbf{x}=\left[\mathbf{f}(\mathbf{x},t)-\frac{1}{2}g(t)^{2}\nabla _{\mathbf{x}}\log p_{t}(\mathbf{x})\right]\mathrm{d}t,\] (4)where \(\mathbf{f}(\cdot,t)\) is a vector-valued function called the _drift_ coefficient of \(\mathbf{x}(t)\), and \(g(\cdot)\) is a scalar function known as the _diffusion_ coefficient of \(\mathbf{x}(t)\). And the ODE form of DDIM is equivalent to a special case of Eq. 4, as long as \(\alpha_{t}\) and \(\alpha_{t-\Delta t}\) are close enough (refer to details in _Appendix_ A).

By treating the DDIM inversion process as solving a differentiable ODE, we emphasize that precise and stable computation of \(\mathrm{d}\mathbf{x}_{t}/\mathrm{d}t\) at each timestep \(t\) is crucial for accurate noise prediction, especially at the start of the inversion process. Fig. 2 right highlights the pitfalls of widely-used scaled linear [18] and cosine [43] noise schedules through numerical computations of \(\mathrm{d}\mathbf{x}_{t}/\mathrm{d}t\).

**Proposition 3.1** (Singularity in Inversion Process).: _During the inversion process, there exists a singularity at \(t=0\) for both the scaled linear and cosine schedule (Fig. 2 right):_

\[\text{When }t=0,\left.\frac{\mathrm{d}\mathbf{x}_{t}}{\mathrm{d}t}\right|_{t \to 0}=\frac{0}{0}\cdot\text{sign}(\epsilon)=\infty\cdot\text{sign}( \epsilon).\]

_This singularity significantly affects the starting point of the inversion process during image editing tasks. Properly modeling \(\mathrm{d}\mathbf{x}_{t}/\mathrm{d}t\) ensures that the inversion closely aligns with the true continuous dynamics of the diffusion process, thereby reducing errors and enhancing the fidelity of the inverted latents, which is critical for high-quality image editing. The proof can be found in Appendix B._

We argue that singularities in modeling \(\mathrm{d}\mathbf{x}_{t}/\mathrm{d}t\) cause significant issues in inversion-based text-guided image editing. Specifically, **(1) the instability in the inversion process** arises from the singularity of the derivatives at \(t=0\), leading to inaccurate noise component estimates, making the starting point inconsistent with the data's true characteristics. The fast sampling in DDIM exacerbates error accumulation, where minor initial errors lead to substantial deviations in the final inverted latents. As a result, reconstructed or edited images may display visual inconsistencies, distorted details, or unnatural artifacts, reducing the overall quality and fidelity. Furthermore, the singularity can also lead to **(2) poor handling of complex data distributions** in the real world. Discontinuities in derivatives result in the model receiving inconsistent and unreliable signals during the diffusion probabilistic modeling. This hinders the model's ability to capture intricate patterns and details, disrupting the consistency and integrity within an image [30].

## 4 Better Noise Schedule Helps Inversion and Editing

### Well-Defined Schedule Improve Inversion Stability

To address the issues highlighted in _Proposition 3.1_, we propose a new noise schedule in terms of \(\tilde{\alpha}_{t}\), since \(\tilde{\alpha}_{t}\) represents the remaining signals in the latents during the diffusion process (Eq. 1). Following the recommendations from iDDPM [43], the noise schedule should ensure that noise is added more slowly at the beginning to preserve image information in the middle of the diffusion process. We introduce our logistic noise schedule as follows:

\[\tilde{\alpha}_{t}=\frac{1}{1+e^{-k\left(t-t_{0}\right)}},\] (5)

where \(k\) and \(t\) are hyperparameters that control the steepness and midpoint of the logistic function, respectively. In our experiments, we set \(k=0.015\) and \(t_{0}=\mathrm{int}(0.6T)\), as discussed in Section 5.3.1.

Figure 3: Left: trends of \(\sqrt{1-\alpha_{t}}\) (noise scales) for scaled linear, cosine, and logistic noise schedules. Right: \(\mathrm{d}\mathbf{x}_{t}/\mathrm{d}t\) for the logistic schedule, highlighting its smooth transition, which prevents singularities and maintains the integrity of the initial latent vector \(\mathbf{x}_{0}\).

Our logistic schedule is designed to have a linear drop-off of \(\alpha_{t}\) in the middle of the diffusion process, with minimal changes near the extremes of \(t=0\) and \(t=T\), thus preventing abrupt changes in the noise level. Fig. 3 left demonstrates the progression of \(\sqrt{1-\alpha_{t}}\) for different schedules, in which linear and cosine schedules tend to add noise too quickly during the early stage of the inversion process. Crucially, our logistic noise schedule avoid the singularity of \(\mathrm{d}\mathbf{x}_{t}/\mathrm{d}t\) at \(t=0\). For simplicity in expression, we set \(k=0.015\) and \(t_{0}=30\), resulting in the following:

\[\text{When }t=0,\left.\frac{\mathrm{d}\mathbf{x}_{t}}{\mathrm{d}t}\right|_{t \to 0}=1.486e^{-3}\epsilon-1.318e^{-3}\mathbf{x}_{0}\]

The proof is provided in _Appendix_ B, and the trend of the derivatives of our logistic schedule is illustrated in Fig. 5 right. By ensuring a smooth and continuous transition in noise levels, the logistic schedule maintains the integrity of the initial latent vector \(\mathbf{x}_{0}\). This alignment with the diffusion process's continuous dynamics prevents undesired deviations, reduces errors, and leads to more accurate and stable latent predictions, improving the inversion process's fidelity.

### Exploring Noise Space of Logistic Schedule

We now explore the properties of our logistic noise schedule and its influence on the noise space, specifically comparing the logSNR trends and inversion processes of different noise schedules.

**Steady Information Perturbation.** As depicted in Fig. 4 (left), the linear and cosine schedules tend to drastically degrade image information at the initial stage of inversion, as evidenced by the rapid drop in logSNR. In contrast, our logistic schedule exhibits a more linear decrease in logSNR before the final stage, ensuring steady data perturbation. This steadiness allows the logistic schedule to capture a richer set of features and nuances from the original image, facilitating more detailed reproduction and higher fidelity in the edited images.

**Comprehensive Pattern Capture.** As shown in Fig. 4 (right), we visualize the latents during the inversion (forward) process, using 50 timesteps with the final step at 981 instead of 999. In the early stage, our _Logistic Schedule_ preserves more original image information, reflecting the logSNR trend. Considering the later stage, linear and cosine schedules retain more low-frequency components due to higher endpoint SNRs, explaining why their noise maps don't fully cover the image. In contrast, our _Logistic Schedule_ ensures that the inverted latent closely resembles pure Gaussian noise, minimizing the retention of low-frequency components. This thorough process ensures that the inversion encodes a broader array of the original image's information, thereby enhancing the quality and fidelity of the edited images.

## 5 Experiments

In the section below, we evaluate our method both quantitatively and qualitatively on text-guided editing of real images. To validate the versatility and effectiveness of our proposed _Logistic Schedule_, we compare it with linear and concise schedules by employing different editing approaches across various editing tasks. Refer to _Appendix_ E for detailed experimental results.

Figure 4: **Analysis of noise space for different schedules**. Left: logSNR trends, where the logistic schedule maintains a more gradual decline. Right: inversion processes, with the logistic schedule preserving more details in the initial stage and minimizing low-frequency retention in the final stage.

### Experimental Settings

**Implementation Details.** We perform the inference of different editing and inversion methods under consistent conditions. We use Stable Diffusion v1.5 as the base model, with 100 timesteps, an inversion guidance scale of 3.5, and a reverse guidance scale of 7.5. All experiments are conducted on a single Nvidia A100 GPU. Quantitative results are averaged over 10 random runs. Additional implementation details are provided in _Appendix_ D.2.

**Datasets.** Experiments are conducted on the PIEBench dataset [25]. Recognizing the dataset's limited size and scenarios, we extend it by incorporating face images from FFHQ [27] and AFHQ [11], as well as indoor/outdoor common objects from MS-COCO [35]. This results in approximately 1600 images in total, across eight editing types (see _Appendix_ D.1).

**Evaluation Metrics.** As the editing process involves altering both the foreground and background of the images, we follow Ju et al. in adopting three types of metrics: structure (DINO-I [7, 58, 51]),

Figure 5: **Qualitative comparison of the _Logistic Schedule_ with linear and cosine schedules across various image editing tasks. To preserve background content during 1� attribute editing tasks (_e.g._, colors, and materials), we employ _Edit Friendly DDPM_[21]; for tasks requiring background preservation such as 2� object translation, we use _Zero-shot Pix2Pix_[45]; for tasks involving 3 scene or style transfer, while maintaining object semantics, we utilize _StyleDiffusion_[63]; to validate spatial context preservation in 4 non-rigid editing tasks (_e.g._, motion, pose), we consider _MasaCtrl_[6].**background preservation (PSNR, LPIPS [24, 72], MSE, SSIM [64]), and image-image, text-image consistency (CLIP score [48]). Detailed descriptions of each metric can be found in _Appendix_D.3.

### Qualitative and Quantitative Comparison

**Qualitative Comparison.** As shown in Fig. 5, our _Logistic Schedule_ demonstrates superior content preservation in each task. In tasks requiring fine-grained editing, such as attributes editing, the _Logistic Schedule_ better preserves other attributes while making the desired changes. For tasks involving high-level semantics, such as object translation and style/scene transfer, the _Logistic Schedule_ maintains the overall structure and pose more effectively. In tasks that involve low-level semantics like color and texture, such as pose and attributes editing, the _Logistic Schedule_ shows better fidelity and consistency. For tasks that require background preservation, such as object translation and pose editing, the _Logistic Schedule_ excels in maintaining the background integrity. Overall, the _Logistic Schedule_ ensures higher edit fidelity across various tasks, whereas the linear and cosine schedules sometimes fail to maintain the desired quality and consistency.

**Quantitative Comparison.** Table 1 shows that when employing the _Logistic Schedule_, all editing tasks exhibit improved retention of background and overall structure. While in some situations, the _Logistic Schedule_ achieves slightly lower text alignment than the linear schedule, its preservation of background and structure is significantly superior.

### Ablation Studies

In this section, we investigate the effects of different configurations of the Logistic Schedule and the adaptability of the Logistic Schedule with various inversion techniques and diffusion models. More experiments on hyperparameters (_e.g._, guidance scale, input scale) can be found in _Appendix_E. The comparison with more design of the noise scheduler is provided in _Appendix_E.4.

#### 5.3.1 Effects of Configuration of Logistic Schedule

We conduct experiments with different configurations of the Logistic Schedule in Eq. 5, providing further evidence for the noise space analysis (Section 4.2). The parameters of the Logistic Schedule (Eq. 5)--specifically the steepness (\(k\)) and the midpoint (\(t_{0}\))--play a crucial role in balancing content preservation and edit fidelity. Table 2 provides the quantitative results of varying \(k\) and \(t_{0}\).

\begin{table}
\begin{tabular}{l|c|c c c c|c c} \hline \hline \multirow{2}{*}{**Schedule**} & \multicolumn{2}{c|}{**Structure**} & \multicolumn{3}{c}{**Background Preservation**} & \multicolumn{3}{c}{**CLIP Similarity (\%)**} \\  & \multicolumn{1}{c|}{**Dist \({}_{\times 10^{-3}}\)**} & \multicolumn{1}{c|}{**PSNR \(\uparrow\)**} & \multicolumn{1}{c}{**LPIPS \({}_{\times 10^{-3}}\)**} & \multicolumn{1}{c}{**MSE \({}_{\times 10^{-4}}\)**} & \multicolumn{1}{c}{**SSIM \({}_{\times 10^{-2}}\)**} & \multicolumn{1}{c}{**Visual \(\uparrow\)**} & \multicolumn{1}{c}{**Textual \(\uparrow\)**} \\ \hline \multicolumn{8}{c}{**Attributes Editing (with _Edit Friendly DDPM_)**} \\ \hline Linear & 35.66 & 20.70 & 134.88 & 113.61 & 77.60 & 79.82 & 23.06 \\ Cosine & 26.57 & 22.38 & 110.52 & 80.01 & 80.15 & 81.35 & 22.39 \\
**Logistic** & **17.37 \({}_{\text{34.6\%}}\)** & **24.78 \({}_{\text{10.7\%}}\)** & **81.80 \({}_{\text{26.0\%}}\)** & **49.47 \({}_{\text{38.2\%}}\)** & **82.97 \({}_{\text{3.5\%}}\)** & **82.44 \({}_{\text{0.8\%}}\)** & **23.62 \({}_{\text{2.4\%}}\)**\(\uparrow\)** \\ \hline \multicolumn{8}{c}{**Object Switch (with _Zero-Shot Pix2Pix_)**} \\ \hline Linear & 39.02 & 19.93 & 134.64 & 138.99 & 74.63 & 83.33 & 22.30 \\ Cosine & 30.83 & 21.15 & 113.03 & 107.46 & 77.23 & 84.32 & 22.46 \\
**Logistic** & **22.4 \({}_{\text{27\%}}\)** & **22.91 \({}_{\text{8\%}}\)** & **90.75 \({}_{\text{20\%}}\)** & **82.05 \({}_{\text{24\%}}\)** & **79.32 \({}_{\text{3\%}}\)** & **84.52 \({}_{\text{0.1\%}}\)** & **22.65 \({}_{\text{0.8\%}}\)**\(\uparrow\)** \\ \hline \multicolumn{8}{c}{**Style/Scene Transferring (with _StyleDiffusion_)**} \\ \hline Linear & 38.06 & 21.17 & 93.70 & 111.01 & 81.85 & 77.65 & **25.39** \\ Cosine & 28.44 & 22.70 & 75.75 & 78.93 & 83.74 & 79.23 & 23.92 \\
**Logistic** & **18.64 \({}_{\text{34.4\%}}\)** & **24.81 \({}_{\text{9.3\%}}\)** & **56.79 \({}_{\text{25.0\%}}\)** & **48.96 \({}_{\text{38.0\%}}\)** & **85.84 \({}_{\text{2.5\%}}\)** & **80.81 \({}_{\text{2.0\%}}\)** & 24.77 \({}_{\text{2.4\%}}\)**\(\downarrow\)** \\ \hline \multicolumn{8}{c}{**Non-ridig Editing (with _Masactr_)**} \\ \hline Linear & 30.83 & 21.15 & 113.03 & 107.46 & 77.23 & 83.13 & **22.65** \\ Cosine & 22.40 & 22.91 & 90.75 & 82.05 & 79.32 & 83.33 & 21.81 \\
**Logistic** & **15.87 \({}_{\text{29.2\%}}\)** & **24.66 \({}_{\text{7.7\%}}\)** & **75.18 \({}_{\text{17.2\%}}\)** & **59.22 \({}_{\text{27.8\%}}\)** & **81.11 \({}_{\text{2.3\%}}\)** & **84.32 \({}_{\text{0.1\%}}\)** & 22.30 \({}_{\text{1.5\%}}\)**\(\downarrow\)** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparative table of diffusion noise schedules and their performance metrics. Bold values indicate the best results, while underlined values denote the second-best results.

**Different \(k\): Changing the steepness of logSNR.** When \(k\) is larger, the logSNR values span a larger range (Fig.6, left). However, if the range is too large, excessive steepness of logSNR results in excessive loss of original image information in edited images (Fig.6, right). Interestingly, when \(k\) is small, the logSNR resembles that of linear and cosine schedules, but the logistic schedule better preserves the original image content without altering the overall structure. This further supports Proposition 3.1 that the singularity in linear and cosine schedules tends to destroy original image information, causing undesired changes.

**Different \(t_{0}\): Introducing shifts in logSNR.** When \(t_{0}\) is close to 0, the lower bound of logSNR is higher, affecting editability by reducing diversity and fidelity, as shown in Fig. 7. Conversely, when \(t_{0}\) is close to \(T\), the original information is lost too quickly, degrading content preservation.

Balancing these parameters, we find that \(k=0.015\) and \(t_{0}=\text{int}(0.6T)\) strike the optimal trade-off between content preservation and edit fidelity, providing robust performance across various tasks.

\begin{table}
\begin{tabular}{c|c|c c c c|c c} \hline \hline \multirow{2}{*}{**Settings**} & \multirow{2}{*}{
\begin{tabular}{c} **Structure** \\ **Dist \(\times 10^{-3}\)** \\ \end{tabular} } & \multicolumn{4}{c}{**Background Preservation**} & \multicolumn{3}{c}{**CLIP Similarity (\%)**} \\  & & **PSNR \(\uparrow\)** & **LPIPS \(\times 10^{-3}\)** & **MSE \(\times 10^{-4}\)** & **SSIM \(\times 10^{-2}\)** & **Visual \(\uparrow\)** & **Textual \(\uparrow\)** \\ \hline \(k=0.015\) & \multirow{2}{*}{**17.37**} & \multirow{2}{*}{24.78} & \multirow{2}{*}{81.80} & \multirow{2}{*}{49.47} & \multirow{2}{*}{82.97} & \multirow{2}{*}{82.44} & \multirow{2}{*}{23.62} \\ \(t_{0}=\text{int}(0.6T)\) & & & & & & & \\ \hline \(k=0.008\) & 16.27 & **26.45** & **75.62** & **43.20** & **85.28** & **84.07** & 20.47 \\ \(k=0.011\) & 16.64 & 25.80 & 77.90 & 48.83 & 84.15 & 83.76 & 21.46 \\ \(k=0.017\) & 22.79 & 22.33 & 99.98 & 57.32 & 81.52 & 82.10 & 23.25 \\ \(k=0.029\) & 27.82 & 21.05 & 103.45 & 64.36 & 78.48 & 80.66 & 23.81 \\ \(t_{0}=\text{int}(0.4T)\) & 24.31 & 22.41 & 97.21 & 60.84 & 79.72 & 79.47 & 20.33 \\ \(t_{0}=\text{int}(0.8T)\) & 29.47 & 21.64 & 95.58 & 63.89 & 75.14 & 77.05 & 22.68 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Quantitative results of the Logistic Schedule across various hyperparameter settings. The best method is indicated in bold, and the worst method is shown in purple.

Figure 6: **Impact of \(k\) on the logistic schedule**. Left: change in \(\tilde{\alpha}_{t}\) and logSNR with different \(k\) values. Right: the effect of \(k\) on edited images.

Figure 7: **Impact of \(t_{0}\) on the logistic schedule**. Left: change in \(\tilde{\alpha}_{t}\) and logSNR with different \(t_{0}\) values. Right: each column represents edited results within three random seeds, under a specific \(t_{0}\).

#### 5.3.2 Adapting Inversion Techniques and Diffusion Models

To validate the adaptability and robustness of the _Logistic Schedule_, we first apply it with Plug-and-Play [59] using Stable Diffusion v1.5 [50] as the baseline. We then design experiments with two other diffusion models, Stable Diffusion v2.1 and Stable Diffusion XL [47], and incorporate three advanced inversion approaches: Null-Text Inversion [41], Negative Prompt Inversion (NPI) [40], and Direct Inversion [25]. As shown in Table 3, while more advanced stable diffusion models increase textual similarity, they degrade content preservation. Conversely, incorporating advanced inversion approaches improves both content preservation and edit fidelity.

Furthermore, Table 4 presents the detailed comparison between the _Logistic Schedule_ and the scaled linear schedule across different inversion techniques.

## 6 Conclusion

This paper presents the _Logistic Schedule_, a novel noise schedule that eliminates singularities and improves inversion stability for image editing. Our method enhances content preservation and edit fidelity without requiring additional retraining, making it a plug-and-play solution for existing workflows. Through in-depth analysis of the diffusion inversion process, we identify that current schedulers suffer from singularity issues at the start of inversion. The proposed Logistic Schedule provides a straightforward solution to this problem, offering superior performance and adaptability across various image editing tasks.

## 7 Acknowledgement

This work was supported by National Natural Science Foundation of China (62403429, 62293551, 62377038, 62177038, 62277042). Project of China Knowledge Centre for Engineering Science and Technology, Project of Chinese academy of engineering "The Online and Offline Mixed Educational Service System for "The Belt and Road' Training in MOOC China". "LENOVO-XJTU" Intelligent Industry Joint Laboratory Project.

\begin{table}
\begin{tabular}{c|c|c c c c|c c} \hline \hline \multirow{2}{*}{**Variants**} & **Structure** & \multicolumn{4}{c}{**Background Preservation**} & \multicolumn{3}{c}{**CLIP Similarity**} \\  & **Dist \(\downarrow\)** & **PSNR \(\uparrow\)** & **LPIPS \(\downarrow\)** & **MSE \(\downarrow\)** & **SSIM \(\uparrow\)** & **Visual \(\uparrow\)** & **Textual \(\uparrow\)** \\ \hline Null-Text + Linear & 21.00 & 23.00 & 95.00 & 63.00 & 81.50 & 81.00 & 21.30 \\ Null-Text + Logistic & 18.67 & 23.80 & 89.64 & 57.97 & 82.97 & 82.46 & 21.95 \\ \hline NPI + Linear & 28.00 & 22.40 & 105.00 & 78.00 & 78.50 & 78.50 & 20.90 \\ NPI + Logistic & 24.82 & 23.17 & 99.19 & 71.24 & 80.26 & 80.16 & 21.53 \\ \hline Direct + Linear & 19.00 & 24.90 & 78.00 & 45.00 & 83.50 & 82.90 & 21.90 \\ Direct + Logistic & 16.06 & 25.73 & 74.17 & 41.19 & 85.61 & 83.29 & 22.03 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Comparison of inversion techniques with the scaled linear schedule and our proposed Logistic Schedule.

\begin{table}
\begin{tabular}{c|c|c|c c c|c c|c c} \hline \hline \multirow{2}{*}{**Variants**} & **Structure** & \multicolumn{4}{c}{**Background Preservation**} & \multicolumn{3}{c}{**CLIP Similarity**} \\  & **Dist \(\times_{10^{-3}}\downarrow\)** & **PSNR \(\uparrow\)** & **LPIPS \(\times_{10^{-3}}\downarrow\)** & **MSE \(\times_{10^{-4}}\downarrow\)** & **SSIM \(\times_{10^{-2}}\uparrow\)** & **Visual \(\uparrow\)** & **Textual \(\uparrow\)** \\ \hline
**PnP+SD-1.5** & 26.66 & 22.46 & 111.27 & 77.74 & 80.02 & 81.24 & 21.74 \\ \hline \multicolumn{8}{c}{**Changing the Base Model**} \\ \hline
**SD-2.1** & 34.74 \({}_{30.3\%}\)\(\uparrow\) & 19.94 \({}_{11.2\%}\)\(\downarrow\) & 152.25 \({}_{36.8\%}\)\(\uparrow\) & 122.86 \({}_{58.0\%}\)\(\uparrow\) & 74.41 \({}_{7.0\%}\)\(\downarrow\) & 79.38 \({}_{1.5\%}\)\(\downarrow\) & 22.87 \({}_{5.2\%}\) \\
**SDXL** & 28.33 \({}_{6.3\%}\)\(\uparrow\) & 21.57 \({}_{4.0\%}\)\(\downarrow\) & 122.14 \({}_{9.8\%}\)\(\uparrow\) & 89.02 \({}_{14.5\%}\)\(\uparrow\) & 77.25 \({}_{3.5\%}\)\(\uparrow\) & 77.52 \({}_{2.9\%}\)\(\downarrow\) & **23.59** \({}_{8.5\%}\)\(\uparrow\) \\ \hline \hline \multicolumn{8}{c}{**Incorporating Advanced Inversion Approaches**} \\ \hline
**+ Null-Text** & 18.67 \({}_{30.0\%}\)\(\downarrow\) & 23.80 \({}_{6.0\%}\)\(\uparrow\) & 89.64 \({}_{19.4\%}\)\(\downarrow\) & 57.97 \({}_{25.4\%}\)\(\downarrow\) & 82.97 \({}_{3.7\%}\)\(\downarrow\) & 82.46 \({}_{1.0\%}\)\(\uparrow\) & 21.95 \({}_{1.0\%}\)\(\uparrow\) \\
**+ NPI** & 24.82 \({}_{6.9\%}\)\(\downarrow\) & 23.17 \({}_{3.2\%}\)\(\uparrow\) & 99.19 \({}_{10.9\%}\)\(\uparrow\) & 71.24 \({}_{8.4\%}\)\(\downarrow\) & 80.26 \({}_{0.3\%}\)\(\uparrow\) & 80.16 \({}_{0.9\%}\)\(\downarrow\) & 21.53 \({}_{1.0\%}\)\(\downarrow\) \\
**+ Direct** & **16.06** \({}_{39.8\%}\)\(\downarrow\) & **25.73** & 14.6\% & **74.17** & 33.3\% & **41.19** \({}_{47.0\%}\)\(\downarrow\) & **85.61** \({}_{7.0\%}\)\(\uparrow\) & **83.29** \({}_{1.6\%}\)\(\uparrow\) & 22.03 \({}_{1.3\%}\)\(\uparrow\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Comparative performance metrics with different base models and inversion techniques.

## References

* Avraham et al. [2023] Omri Avraham, Ohad Fried, and Dani Lischinski. Blended latent diffusion. _ACM Transactions on Graphics (TOG)_, 42(4):1-11, 2023.
* Avraham et al. [2023] Omri Avraham, Thomas Hayes, Oran Gafni, Sonal Gupta, Yaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried, and Xi Yin. Spatext: Spatio-textual representation for controllable image generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18370-18380, 2023.
* Bansal et al. [2024] Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Cold diffusion: Inverting arbitrary image transforms without noise. _Advances in Neural Information Processing Systems_, 36, 2024.
* Brack et al. [2024] Manuel Brack, Felix Friedrich, Dominik Hintersdorf, Lukas Struppek, Patrick Schramowski, and Kristian Kersting. Sega: Instructing text-to-image models using semantic guidance. _Advances in Neural Information Processing Systems_, 36, 2024.
* Brooks et al. [2023] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18392-18402, 2023.
* Cao et al. [2023] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 22560-22570, 2023.
* Caron et al. [2021] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 9650-9660, 2021.
* Chen et al. [2018] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. _Advances in neural information processing systems_, 31, 2018.
* Chen et al. [2023] Ting Chen, Lala Li, Saurabh Saxena, Geoffrey Hinton, and David J Fleet. A generalist framework for panoptic segmentation of images and videos. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 909-919, 2023.
* Cho et al. [2024] Hansam Cho, Jonghyun Lee, Seoung Bum Kim, Tae-Hyun Oh, and Yonghyun Jeong. Noise map guidance: Inversion with spatial context for real image editing. _arXiv preprint arXiv:2402.04625_, 2024.
* Choi et al. [2020] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for multiple domains. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 8188-8197, 2020.
* Dhariwal and Nichol [2021] Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat GANs on image synthesis. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021. URL https://openreview.net/forum?id=AAWuCvzaVt.
* Gal et al. [2023] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=NAQvF08TcyG.
* Gu et al. [2023] Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Miguel Angel Bautista, and Joshua M. Susskind. f-DM: A multi-stage diffusion model via progressive signal transformation. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=iBdwKISg4m.

* Han et al. [2023] Ligong Han, Song Wen, Qi Chen, Zhixing Zhang, Kunpeng Song, Mengwei Ren, Ruijiang Gao, Yuxiao Chen, Ding Liu, Qilong Zhangli, Anastasis Stathopoulos, Xiaoxiao He, Jindong Jiang, Zhaoyang Xia, Akash Srivastava, and Dimitris N. Metaxas. Proxedit: Improving tuning-free real image editing with proximal guidance. _2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)_, pages 4279-4289, 2023. URL https://api.semanticscholar.org/CorpusID:259287564.
* Hertz et al. [2023] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-or. Prompt-to-prompt image editing with cross-attention control. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=_CDixzkzeyb.
* Ho and Salimans [2021] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In _NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications_, 2021. URL https://openreview.net/forum?id=qv8AKxfYbI.
* Ho et al. [2020] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.
* Ho et al. [2022] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. _Journal of Machine Learning Research_, 23(47):1-33, 2022.
* Hoogeboom et al. [2023] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. In _International Conference on Machine Learning_, pages 13213-13232. PMLR, 2023.
* Huberman-Spiegelglas et al. [2023] Inbar Huberman-Spiegelglas, Vladimir Kulikov, and Tomer Michaeli. An edit friendly ddpm noise space: Inversion and manipulations. _arXiv preprint arXiv:2304.06140_, 2023.
* Hyvarinen and Dayan [2005] Aapo Hyvarinen and Peter Dayan. Estimation of non-normalized statistical models by score matching. _Journal of Machine Learning Research_, 6(4), 2005.
* Jabri et al. [2023] Allan Jabri, David J. Fleet, and Ting Chen. Scalable adaptive computation for iterative generation. In _Proceedings of the 40th International Conference on Machine Learning_, ICML'23, page 21. JMLR.org, 2023.
* Johnson et al. [2016] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14_, pages 694-711. Springer, 2016.
* Ju et al. [2024] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu. Pnp inversion: Boosting diffusion-based editing with 3 lines of code. In _The Twelfth International Conference on Learning Representations_, 2024. URL https://openreview.net/forum?id=FoMZ4ljhVw.
* Karras et al. [2022] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. _Advances in Neural Information Processing Systems_, 35:26565-26577, 2022.
* Kazemi and Sullivan [2014] Vahid Kazemi and Josephine Sullivan. One millisecond face alignment with an ensemble of regression trees. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1867-1874, 2014.
* Kim et al. [2022] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models for robust image manipulation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2426-2435, 2022.
* Kingma et al. [2021] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. _Advances in neural information processing systems_, 34:21696-21707, 2021.
* Kingma and Welling [2013] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_, 2013.

* Kloeden et al. [2012] Peter Eris Kloeden, Eckhard Platen, and Henri Schurz. _Numerical solution of SDE through computer experiments_. Springer Science & Business Media, 2012.
* Kwon et al. [2023] Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusion models already have a semantic latent space. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=pd1P2eUBVfq.
* Lin [2024] Haonan Lin. Dreamsalon: A staged diffusion framework for preserving identity-context in editable face generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8589-8598, 2024.
* Lin et al. [2024] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and sample steps are flawed. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 5404-5411, 2024.
* Lin et al. [2014] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13_, pages 740-755. Springer, 2014.
* Liu et al. [2023] Zhenzhen Liu, Jin Peng Zhou, Yufan Wang, and Kilian Q Weinberger. Unsupervised out-of-distribution detection with diffusion inpainting. In _International Conference on Machine Learning_, pages 22528-22538. PMLR, 2023.
* Lovelace et al. [2024] Justin Lovelace, Varsha Kishore, Chao Wan, Eliot Shekhtman, and Kilian Q Weinberger. Latent diffusion for language generation. _Advances in Neural Information Processing Systems_, 36, 2024.
* Lu et al. [2022] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. _Advances in Neural Information Processing Systems_, 35:5775-5787, 2022.
* Meng et al. [2022] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=aB8CjcPu_tE.
* Miyake et al. [2023] Daiki Miyake, Akihiro Johara, Yuriko Saito, and Toshiyuki TANAKA. Negative-prompt inversion: Fast image inversion for editing with text-guided diffusion models. _ArXiv_, abs/2305.16807, 2023. URL https://api.semanticscholar.org/CorpusID:258947366.
* Mokady et al. [2023] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6038-6047, 2023.
* Nichol et al. [2021] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In _International Conference on Machine Learning_, 2021. URL https://api.semanticscholar.org/CorpusID:245335086.
* Nichol and Dhariwal [2021] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In _International conference on machine learning_, pages 8162-8171. PMLR, 2021.
* Pan et al. [2023] Zhihong Pan, Riccardo Gherardi, Xiufeng Xie, and Stephen Huang. Effective real image editing with accelerated iterative diffusion inversion. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 15912-15921, 2023.
* Parmar et al. [2023] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. In _ACM SIGGRAPH 2023 Conference Proceedings_, pages 1-11, 2023.
* Pehlivan et al. [2023] Hamza Pehlivan, Yusuf Dalva, and Aysegul Dundar. Styleres: Transforming the residuals for real image editing with stylegan. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1828-1837, 2023.

* Podell et al. [2024] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In _The Twelfth International Conference on Learning Representations_, 2024. URL https://openreview.net/forum?id=di52zR8xgf.
* Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* Ramesh et al. [2022] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 2022.
* Rombach et al. [2022] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 10684-10695, June 2022.
* Ruiz et al. [2023] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22500-22510, 2023.
* Saharia et al. [2022] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in Neural Information Processing Systems_, 35:36479-36494, 2022.
* Sauer et al. [2023] Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis. In _International conference on machine learning_, pages 30105-30118. PMLR, 2023.
* Simonyan and Zisserman [2015] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In Yoshua Bengio and Yann LeCun, editors, _3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings_, 2015. URL http://arxiv.org/abs/1409.1556.
* Song et al. [2021] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=St1giarCHLP.
* Song and Ermon [2019] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. _Advances in neural information processing systems_, 32, 2019.
* Song et al. [2020] Yang Song, Jascha Narain Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. _ArXiv_, abs/2011.13456, 2020. URL https://api.semanticscholar.org/CorpusID:227209335.
* Tumanyan et al. [2022] Narek Tumanyan, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Splicing vit features for semantic appearance transfer. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10748-10757, 2022.
* Tumanyan et al. [2023] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1921-1930, 2023.
* Wallace et al. [2023] Bram Wallace, Akash Gokul, and Nikhil Naik. Edict: Exact diffusion inversion via coupled transformations. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22532-22541, 2023.
* Wang et al. [2024] Jiahao Wang, Caixia Yan, Haonan Lin, and Weizhan Zhang. Oneactor: Consistent character generation via cluster-conditioned guidance. _arXiv preprint arXiv:2404.10267_, 2024.

* Wang et al. [2023] Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-Tuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah Laszlo, David J Fleet, Radu Soricut, et al. Imagen editor and editbench: Advancing and evaluating text-guided image inpainting. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 18359-18369, 2023.
* Wang et al. [2023] Zhizhong Wang, Lei Zhao, and Wei Xing. Stylediffusion: Controllable disentangled style transfer via diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 7677-7689, 2023.
* Wang et al. [2004] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. _IEEE transactions on image processing_, 13(4):600-612, 2004.
* Wei et al. [2022] Tianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Weiming Zhang, Lu Yuan, Gang Hua, and Nenghai Yu. E2style: Improve the efficiency and effectiveness of stylegan inversion. _IEEE Transactions on Image Processing_, 31:3267-3280, 2022.
* Wu et al. [2023] Qiucheng Wu, Yujian Liu, Handong Zhao, Ajinkya Kale, Trung Bui, Tong Yu, Zhe Lin, Yang Zhang, and Shiyu Chang. Uncovering the disentanglement capability in text-to-image diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1900-1910, 2023.
* Yang and Mandt [2024] Ruihan Yang and Stephan Mandt. Lossy image compression with conditional diffusion models. _Advances in Neural Information Processing Systems_, 36, 2024.
* Yang et al. [2023] Tao Yang, Yuwang Wang, Yan Lu, and Nanning Zheng. Disdiff: Unsupervised disentanglement of diffusion probabilistic models. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023. URL https://openreview.net/forum?id=3ofe0lpwQP.
* Zhang et al. [2023] Cheng Zhang, Xuanbai Chen, Siqi Chai, Chen Henry Wu, Dmitry Lagun, Thabo Beeler, and Fernando De la Torre. Iti-gen: Inclusive text-to-image generation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 3969-3980, 2023.
* Zhang et al. [2024] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: A manually annotated dataset for instruction-guided image editing. _Advances in Neural Information Processing Systems_, 36, 2024.
* Zhang et al. [2023] Qinsheng Zhang, Molei Tao, and Yongxin Chen. gDDIM: Generalized denoising diffusion implicit models. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=1hkE9qjvz-.
* Zhang et al. [2018] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 586-595, 2018.
* Zhang et al. [2024] Yuechen Zhang, Jinbo Xing, Eric Lo, and Jiaya Jia. Real-world image variation by aligning diffusion inversion chain. _Advances in Neural Information Processing Systems_, 36, 2024.
* Zhuang et al. [2021] Peiye Zhuang, Oluwasanmi O Koyejo, and Alex Schwing. Enjoy your editing: Controllable {gan}s for image editing via latent space navigation. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=HOFxeCutxZR.

## Appendix

### Table of Contents

* A Neural ODEs of DDIM
* A.1 Preliminaries: Score-Based Generative Modeling with SDEs
* A.2 Rewrite the DDIM Process as ODEs
* B Proofs
* B.1 Proof Preliminaries
* B.2 Derivation of Singularities _w.r.t._ Linear and Cosine Schedules
* B.3 Derivatives of the Logistic Schedule
* C Related Works
* D Experimental Settings
* D.1 Introduction of Editing Types
* D.2 Implementation Details
* D.3 Evaluation Metrics
* E Experimental Results
* E.1 Quantitative Comparison Across Editing Types
* E.2 Qualitative Comparison Across Editing Types
* E.3 Broader Application: Training-Based Methods
* E.4 Comparison With Other Noise Schedulers
* E.5 Reconstruction Ability of Different Noise Schedule
* E.6 Effects of Guidance Scale
* E.7 Effects of Input Scale
* F Limitations and Future Works
* G Broader Impacts
* H Ethics StatementNeural ODEs of DDIM

To support the analysis in Section 3 on the failure of DDIM in inversion, we present the following connections to neural ODEs and DDIM.

### Preliminaries: Score-Based Generative Modeling with SDEs

We beginning with the process of constructing a diffusion process using SDEs, extending DDPM to infinite noise scales for evolving data distributions from initial to prior distributions.

**Perturbing Process with SDEs.** DDPM [18] sets noise scales so that \(\mathbf{x}_{T}\) approximates \(\mathcal{N}(0,\mathbf{I})\), leveraging multiple noise scales for success. Song et al. extended this to infinite noise scales, evolving the data distribution via an SDE. The goal is to construct a diffusion process \(\{\mathbf{x}(t)\}_{t=0}^{T}\), where \(\mathbf{x}(0)\sim p_{0}\) (data distribution) and \(\mathbf{x}(T)\sim p_{T}\) (prior distribution). The process is modeled by the SDE:

\[\mathrm{d}\mathbf{x}=\mathbf{f}(\mathbf{x},t)\mathrm{d}t+g(t)\mathrm{d} \mathbf{w}\] (6)

where \(\mathbf{w}\) is the standard Wiener process with time flowing backwards from \(T\) to \(0\), \(\mathbf{f}(\cdot,t)\) is the drift coefficient, and \(g(\cdot)\) is the diffusion coefficient.

**Generating Samples by Reversing the SDE.** Starting from \(\mathbf{x}(T)\sim p_{T}\) and reversing the process, we can obtain \(\mathbf{x}(0)\sim p_{0}\), given by the reverse-time SDE:

\[\mathrm{d}\mathbf{x}=\left[\mathbf{f}(\mathbf{x},t)-g(t)^{2}\nabla_{\mathbf{ x}}\log p_{t}(\mathbf{x})\right]\mathrm{d}t+g(t)\mathrm{d}\mathbf{w}.\] (7)

The score \(\nabla_{\mathbf{x}}\log p_{t}(\mathbf{x})\) can be estimated by training a score-based model on samples using score matching [22; 56].

**Solving Reverse-Time SDE: Probability Flow ODE.** Numerical solvers approximate trajectories from SDEs. General-purpose methods like Euler-Maruyama and stochastic Runge-Kutta [31] discretize the stochastic dynamics. In addition to these, score-based models enable solving the reverse-time SDE via a deterministic process, known as the _probability flow ODE_:

\[\mathrm{d}\mathbf{x}=\left[\mathbf{f}(\mathbf{x},t)-\frac{1}{2}g(t)^{2}\nabla _{\mathbf{x}}\log p_{t}(\mathbf{x})\right]\mathrm{d}t\] (8)

This ODE is determined from the SDE once scores are known. When the score function is approximated by a neural network, it exemplifies a neural ODE [8].

**From Score-Based Models to DDPM: VE, VP SDEs** The noise perturbations in SMLD [56] and DDPM [18] are discretizations of two SDEs: _Variance Exploding (VE)_ SDE and _Variance Preserving (VP)_ SDE.

For SMLD with \(N\) noise scales, each perturbation kernel \(p_{\sigma_{i}}(\mathbf{x}\mid\mathbf{x}_{0})\) corresponds to the distribution of \(\mathbf{x}_{i}\) in this Markov chain:

\[\mathbf{x}_{i}=\mathbf{x}_{i-1}+\sqrt{\sigma_{i}^{2}-\sigma_{i-1}^{2}}\mathbf{ z}_{i-1},\quad i=1,\cdots,N,\] (9)

where \(\mathbf{z}_{i-1}\sim\mathcal{N}(0,\mathbf{I})\) and \(\sigma_{0}=0\). As \(N\to\infty\), \(\left\{\sigma_{i}\right\}_{i=1}^{N}\) becomes \(\sigma(t)\), \(\mathbf{z}_{i}\) becomes \(\mathbf{z}(t)\), and the Markov chain \(\left\{\mathbf{x}_{i}\right\}_{i=1}^{N}\) becomes a continuous stochastic process \(\{\mathbf{x}(t)\}_{t=0}^{1}\), given by the SDE:

\[\mathrm{d}\mathbf{x}=\sqrt{\frac{\mathrm{d}[\sigma^{2}(t)]}{\mathrm{d}t}} \mathrm{d}\mathbf{w}.\] (10)

For DDPM, the perturbation kernels \(\left\{p_{\alpha_{i}}(\mathbf{x}\mid\mathbf{x}_{0})\right\}_{i=1}^{N}\) follow this Markov chain:

\[\mathbf{x}_{i}=\sqrt{1-\beta_{i}}\mathbf{x}_{i-1}+\sqrt{\beta_{i}}\mathbf{z} _{i-1},\quad i=1,\cdots,N.\] (11)

As \(N\to\infty\), this converges to the SDE:

\[\mathrm{d}\mathbf{x}=-\frac{1}{2}\beta(t)\mathbf{x}\mathrm{d}t+\sqrt{\beta(t )}\mathrm{d}\mathbf{w}.\] (12)

Thus, noise perturbations in SMLD and DDPM correspond to the SDEs 10 and 12, respectively. Notably, the SDE 10 results in an exploding variance as \(t\to\infty\), while the SDE 12 maintains a fixed variance of one, demonstrating the superiority of VP SDE for stable variance preservation.

### Rewrite the DDIM Process as ODEs

**DDIM's Local Linearization Assumption:** DDIM inversion for real images is unstable due to its reliance on a local linearization assumption at each step, leading to error accumulation and content loss. DDIM assumes that the denoising process in Eq. 2 is roughly invertible:

\[\mathbf{x}_{t}^{*}=\frac{\mathbf{x}_{t-\Delta t}^{*}-b_{t}\epsilon(\mathbf{x}_{t }^{*},t)}{a_{t}}\approx\frac{\mathbf{x}_{t-\Delta t}^{*}-b_{t}\epsilon( \mathbf{x}_{t-\Delta t}^{*},t)}{a_{t}},\]

where \(a_{t}=\sqrt{\alpha_{t-\Delta t}/\alpha_{t}}\) and \(b_{t}=-\sqrt{\alpha_{t-\Delta t}(1-\alpha_{t})/\alpha_{t}}+\sqrt{1-\alpha_{t- \Delta t}}\). This assumes \(\epsilon(\mathbf{x}_{t}^{*},t)\approx\epsilon(\mathbf{x}_{-\Delta t}^{*},t)\), and inversion accuracy depends on this assumption. Moreover, estimating the "predicted \(\mathbf{x}_{0}\)" at the beginning (\(t=1\)) lacks a simple expression for the posterior mean conditioned on \(\mathbf{x}_{t}\). This deviating from the linearization assumption causes the interpolation to break down from the start, resulting in server error accumulation problem.

**Relevance to Neural ODEs:** Under this assumption, the DDIM iteration process (Eq. 2) can be rewritten in a format similar to Euler integration for solving ODEs:

\[\frac{\mathbf{x}_{t-\Delta t}}{\sqrt{\alpha_{t-\Delta t}}}=\frac{\mathbf{x}_{ t}}{\sqrt{\alpha_{t}}}+\left(\sqrt{\frac{1-\alpha_{t-\Delta t}}{\alpha_{t- \Delta t}}}-\sqrt{\frac{1-\alpha_{t}}{\alpha_{t}}}\right)\epsilon_{\theta}^{(t )}\left(\mathbf{x}_{t}\right).\] (13)

Reparameterizing \((\sqrt{1-\alpha/\sqrt{\alpha}})\) with \(\sigma\) and \((\mathbf{x}/\sqrt{\alpha})\) with \(\tilde{\mathbf{x}}\), in the continuous case, \(\sigma\) and \(\mathbf{x}\) are functions of \(t\), with \(\sigma:\mathbb{R}_{\geq 0}\rightarrow\mathbb{R}_{\geq 0}\) continuous and increasing, \(\sigma(0)=0\). Eq. 13 can be seen as an Euler method over the ODE:

\[\mathrm{d}\tilde{\mathbf{x}}(t)=\epsilon_{\theta}^{(t)}\left(\frac{\tilde{ \mathbf{x}}(t)}{\sqrt{\sigma^{2}+1}}\right)\mathrm{d}\sigma(t),\] (14)

which corresponds to the Eq. 10 of probability flow ODE. This suggests that with enough discretization steps and the optimal model \(\epsilon_{\theta}^{(t)}\), the generation process Eq. 2 can be reversed, encoding \(\mathbf{x}_{0}\) to \(\mathbf{x}_{T}\) and simulating the reverse of the ODE in Eq. 14.

**Theorem A.1** (Ddim ODEs).: _While the ODEs are equivalent, the sampling procedures differ significantly. The Euler method for the probability flow ODE updates:_

\[\frac{\mathbf{x}_{t-\Delta t}}{\sqrt{\alpha_{t-\Delta t}}}=\frac{\mathbf{x}_{ t}}{\sqrt{\alpha_{t}}}+\frac{1}{2}\left(\frac{1-\alpha_{t-\Delta t}}{ \alpha_{t-\Delta t}}-\frac{1-\alpha_{t}}{\alpha_{t}}\right)\cdot\sqrt{\frac{ \alpha_{t}}{1-\alpha_{t}}}\cdot\epsilon_{\theta}^{(t)}(\mathbf{x}_{t}),\] (15)

_which is equivalent to Eq. 13 if \(\alpha_{t}\) and \(\alpha_{t-\Delta t}\) are close enough. However, achieving this closeness is challenging with fewer time steps, and an inferior model can exacerbate the errors from this assumption. Moreover, the Variance Exploding SDE (VE SDE) has inherent flaws compared to the Variance Preserving SDE (VP SDE). VE SDEs tend to increase variance exponentially, leading to instability and less accurate representations, whereas VP SDEs maintain a stable variance, ensuring a more consistent and reliable modeling process._

Modeling with \(\mathrm{d}t\) in Euler steps, as done in the probability flow ODE, ensures that the step size directly correlates with the temporal evolution, maintaining the integrity of the stochastic process and providing a more faithful representation of the underlying data distribution over time. [55] state that the ODE of DDIM is a special case of the probability flow ODE (continuous-time analog of DDPM).

Proof.: We consider \(t\) as a continuous, independent "time" variable and \(\mathbf{x}\) and \(\alpha\) as functions of \(t\). Let's reparameterize DDIM and VE-SDE using \(\tilde{\mathbf{x}}\) and \(\sigma\):

\[\tilde{\mathbf{x}}(t)=\tilde{\mathbf{x}}(0)+\sigma(t)\epsilon,\quad\epsilon \sim\mathcal{N}(0,I),\]

for \(t\in[0,\infty)\) and a continuous function \(\sigma:\mathbb{R}_{\geq 0}\rightarrow\mathbb{R}_{\geq 0}\) where \(\sigma(0)=0\).

Define \(\alpha(t)\) and \(\mathbf{x}(t)\) for DDIM as:

\[\tilde{\mathbf{x}}(t)=\frac{\mathbf{x}(t)}{\sqrt{\alpha(t)}},\quad\sigma(t)= \sqrt{\frac{1-\alpha(t)}{\alpha(t)}}.\]

This implies:

\[\mathbf{x}(t)=\frac{\tilde{\mathbf{x}}(t)}{\sqrt{\sigma^{2}(t)+1}},\quad\alpha (t)=\frac{1}{1+\sigma^{2}(t)}.\]From Equation 1, noting \(\alpha(0)=1\):

\[\frac{\mathbf{x}(t)}{\sqrt{\alpha(t)}}=\frac{\mathbf{x}(0)}{\sqrt{\alpha(0)}}+ \sqrt{\frac{1-\alpha(t)}{\alpha(t)}}\epsilon,\]

which reparameterizes to:

\[\bar{\mathbf{x}}(t)=\bar{\mathbf{x}}(0)+\sigma(t)\epsilon.\]

**ODE form for DDIM:** Simplify Equation 13 to:

\[\bar{\mathbf{x}}(t-\Delta t)=\bar{\mathbf{x}}(t)+(\sigma(t-\Delta t)-\sigma(t) )\cdot\epsilon_{\theta}^{(t)}(x(t)).\]

Dividing by \(-\Delta t\) and taking \(\Delta t\to 0\):

\[\frac{d\bar{\mathbf{x}}(t)}{dt}=\frac{d\sigma(t)}{dt}\epsilon_{\theta}^{(t)} \left(\frac{\bar{\mathbf{x}}(t)}{\sqrt{\sigma^{2}(t)+1}}\right),\] (16)

matching Equation 14.

**ODE form for VE-SDE:** Define \(p_{t}(\bar{\mathbf{x}})\) as the data distribution perturbed with \(\sigma^{2}(t)\) Gaussian noise. The probability flow for VE-SDE is given by:

\[d\bar{\mathbf{x}}=-\frac{1}{2}g(t)^{2}\nabla_{\bar{\mathbf{x}}}\log p_{t}( \bar{\mathbf{x}})dt,\]

where \(g(t)=\sqrt{\frac{d\sigma^{2}(t)}{dt}}\). The perturbed score function \(\nabla_{\bar{\mathbf{x}}}\log p_{t}(\bar{\mathbf{x}})\) minimizes:

\[\nabla_{\bar{\mathbf{x}}}\log p_{t}=\arg\min_{g_{t}}\mathbb{E}_{x(0)\cdot q(x ),\,\epsilon\sim N(0,I)}\big{[}\big{\|}g_{t}(\bar{\mathbf{x}})+\epsilon/\sigma (t)\big{\|}_{2}^{2}\big{]},\]

where \(\bar{\mathbf{x}}=\bar{\mathbf{x}}(t)+\sigma(t)\epsilon\).

The equivalence between \(\mathbf{x}(t)\) and \(\bar{\mathbf{x}}(t)\) gives:

\[\nabla_{\bar{\mathbf{x}}}\log p_{t}(\bar{\mathbf{x}})=-\frac{\epsilon_{ \theta}^{(t)}\left(\frac{\bar{\mathbf{x}}(t)}{\sqrt{\sigma^{2}(t)+1}}\right)} {\sigma(t)}.\]

Using Equation A.2, and the definition of \(g(t)\):

\[\frac{d\bar{\mathbf{x}}(t)}{dt}=\frac{1}{2}\frac{d\sigma^{2}(t)}{dt}\frac{ \epsilon_{\theta}^{(t)}\left(\frac{\bar{\mathbf{x}}(t)}{\sqrt{\sigma^{2}(t)+ 1}}\right)}{\sigma(t)}dt,\]

rearranging terms:

\[\frac{d\bar{\mathbf{x}}(t)}{dt}=\frac{d\sigma(t)}{dt}\epsilon_{\theta}^{(t)} \left(\frac{\bar{\mathbf{x}}(t)}{\sqrt{\sigma^{2}(t)+1}}\right),\]

which matches Equation 16. Both initial conditions are \(\bar{\mathbf{x}}(T)\sim\mathcal{N}(0,\sigma^{2}(T)I)\), showing that the ODEs are identical. 

However, the above proof is based on several assumptions as follows:

1. **Equivalence Between \(\mathbf{x}(t)\) and \(\bar{\mathbf{x}}(t)\):** The bijective mapping between the variables \(\mathbf{x}(t)\) and \(\bar{\mathbf{x}}(t)\) is crucial for transforming the DDIM formulation into the VE-SDE framework. If this equivalence does not hold perfectly, the transformation could introduce errors. Small discrepancies can accumulate over time, leading to significant deviations in the modeling process, resulting in unreliable outcomes.

2. **Gaussian and Constant Noise \(\epsilon\):** The noise \(\epsilon\) is assumed to be Gaussian \(\mathcal{N}(0,I)\) and constant throughout the process, which simplifies the mathematical formulation and integration. However, in real-world scenarios, the noise might not be perfectly Gaussian or constant. Variations in the noise can affect the accuracy of the model's predictions, leading to inconsistencies and unreliable results.

3. **Continuity and Differentiability of \(\alpha(t)\) and \(\sigma(t)\):** The functions \(\alpha(t)\) and \(\sigma(t)\) are assumed to be continuous and differentiable. This ensures smooth transitions and allows for the derivation of the differential equations. If \(\alpha(t)\) or \(\sigma(t)\) are not continuous or differentiable, the resulting differential equations may not accurately represent the underlying processes. This can lead to instability and errors in the model's behavior.

4. **Optimal Model \(\epsilon_{\theta}^{(t)}\):** The model \(\epsilon_{\theta}^{(t)}\) is assumed to be optimal, meaning it perfectly minimizes the given loss function. In practice, achieving an optimal model is challenging. Suboptimal models can lead to inaccuracies in the predictions, and the error can propagate, reducing the reliability of the entire process.

5. **Closeness of \(\alpha_{t}\) and \(\alpha_{t-\Delta t}\):** It is assumed that \(\alpha_{t}\) and \(\alpha_{t-\Delta t}\) are close enough, which is necessary for the equivalence between the DDIM and VE-SDE formulations to hold. With fewer time steps, this assumption may not hold, leading to significant errors. Additionally, if the model is inferior, the errors arising from this assumption can be magnified, resulting in an unreliable process.

## Appendix B Proofs

In this section, we first provide the detailed expressions of \(\mathbf{x}_{t}\) with respect to different noise schedules. Then we provide the proof of the singularities problem in Proposition 3.1.

By reparameterizing:

\[\alpha_{t}=1-\beta_{t},\qquad\bar{\alpha_{t}}=\prod_{i=1}^{t}\alpha_{i},\] (17)

the forward process of DDPM can be expressed as:

\[\mathbf{x}_{t}=\sqrt{\bar{\alpha}_{t}}\mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t }}\epsilon.\]

Apply the chain rule to \(\mathrm{d}\mathbf{x}_{t}/\mathrm{d}t\), we get:

\[\frac{\mathrm{d}\mathbf{x}_{t}}{\mathrm{d}t}=\frac{1}{2}\frac{1}{\sqrt{\bar{ \alpha}_{t}}}\mathbf{x}_{0}\frac{\mathrm{d}\bar{\alpha}_{t}}{\mathrm{d}t}+ \frac{1}{2}\frac{-1}{\sqrt{1-\bar{\alpha}_{t}}}\epsilon\frac{\mathrm{d}\bar{ \alpha}_{t}}{\mathrm{d}t}\] (18)

### Proof Preliminaries

#### b.1.1 Scaled Linear Schedule

The linear beta schedule is defined by:

\[\beta_{t}=\beta_{\mathrm{start}}+t\cdot\frac{\beta_{\mathrm{end}}-\beta_{ \mathrm{start}}}{T-1}\]

where

\[\beta_{\mathrm{start}}=\frac{0.0001\cdot 1000}{T}=\frac{0.1}{T}\]

and

\[\beta_{\mathrm{end}}=\frac{0.02\cdot 1000}{T}=\frac{20}{T}\]

Thus,

\[\beta_{t}=\frac{0.1}{T}+t\cdot\frac{\frac{20}{T}-\frac{0.1}{T}}{T-1}=\frac{0.1}{T}+t\cdot\frac{19.9}{T(T-1)}\]

In general form, the expression for \(\beta_{t}\) is:

\[\beta_{t}=\frac{0.1}{T}+\frac{19.9\cdot t}{T(T-1)},\quad t=0,1,2,\ldots,T-1\]

Incorporating Eq. 17, the \(\bar{\alpha}_{t}\) of scaled linear schedule is given by:

\[\bar{\alpha}_{t}=\prod_{i=1}^{t}\left(1-\frac{0.1}{T}-\frac{19.9\cdot i}{T(T- 1)}\right)\] (19)

#### b.1.2 Cosine Schedule

The cosine schedule is proposed in the iDDPM [43], where the definition of the schedule is given by:

\[\bar{\alpha}_{t}=\frac{f(t)}{f(0)},\quad f(t)=\cos\left(\frac{t/T+s}{1+s}\cdot \frac{\pi}{2}\right)^{2},\] (20)

where \(s\) is a small offset to prevent \(\beta_{t}\) from being too small near \(t=0\). Nichol and Dhariwal chose this setting since they found that having tiny amounts of noise at the beginning of the process made it hard for the network to predict accurately enough. Specifically, \(s\) is set as \(0.008\) such that \(\sqrt{\beta}_{0}\) was slightly smaller than the pixel bin size 1/127.5.

Plugging \(f(t)\) into the expression, we get:

\[\bar{\alpha}_{t}=\frac{\cos^{2}(\frac{t/T+s}{1+s}\cdot\frac{\pi}{2})}{\cos^{2} (\frac{s}{1+s}\cdot\frac{\pi}{2})}\]

#### b.1.3 Sigmoid Schedule

The sigmoid schedule is introduced in Jabri et al., which is designed for scalable data generation, especially for high-dimensional data, without addressing the challenges of DDIM inversion. The formulation of the sigmoid schedule can be presented as below:

\[\bar{\alpha}_{t}=\frac{-\left(\frac{t(e-s)+s}{r}\right)\cdot\text{sigmoid}( )+v_{e}}{v_{e}-v_{s}}\] (21)

where \(s\) and \(e\) are the start and end of the sigmoid function's range, and \(v_{s}=(s/r)\cdot\text{sigmoid}()\), \(v_{e}=(e/r)\cdot\text{sigmoid}()\).

#### b.1.4 Logistic Schedule

Recall the expression of the logistic schedule in Eq. 5:

\[\bar{\alpha}_{t}=\text{Normalized}\left(\frac{1}{1+e^{-k(t-t_{0})}}\right),\]

where \(k\) and \(t\) are hyperparameters that control the steepness and midpoint of the logistic function, respectively.

### Derivation of Singularities _w.r.t._ Linear and Cosine Schedules

Recall the Proposition 3.1:

**Proposition B.1** (Singularity in Inversion Process).: _During the inversion process, there exists a singularity at \(t=0\) for both the scaled linear and cosine schedule:_

\[\text{When }t=0,\left.\frac{\mathrm{d}\mathbf{x}_{t}}{\mathrm{d}t}\right|_{t \to 0}=\frac{0}{0}\cdot\text{sign}(\epsilon)=\infty\cdot\text{sign}( \epsilon).\]

Next, we provide the derivatives for scaled linear and cosine in order, to support Proposition 3.1.

#### b.2.1 Scaled Linear Schedule

For \(\mathrm{d}\mathbf{x}_{t}/\mathrm{d}t\), where \(\mathbf{x}_{t}=\sqrt{\bar{\alpha}_{t}}\mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t}}\epsilon\), cannot use \(\bar{\alpha}_{t}=\prod_{i=1}^{t}\left(1-\frac{0.1}{T}-\frac{19.9\cdot i}{T(T-1 )}\right)\) to find the feasible derivatives. Since the expression \(\bar{\alpha}_{t}=\prod_{i=1}^{t}\left(1-\frac{0.1}{T}-\frac{19.9\cdot i}{T(T-1 )}\right)\) represents a product of terms, which makes it difficult to differentiate directly. Taking the derivative of a product involves applying the product rule multiple times, which becomes impractical as the number of terms increases. Instead, we can use logarithms to simplify the expression into a sum, which is easier to handle analytically. This approach allows us to find an analytic approximation for \(\bar{\alpha}_{t}\) and subsequently for \(\mathrm{d}\mathbf{x}_{t}/\mathrm{d}t\).

Proof.: The logarithm of the product in Eq. 19 reads:

\[\log(\tilde{\alpha}_{t})=\sum_{i=1}^{t}\log\left(1-\frac{0.1}{T}-\frac{19.9\cdot i }{T(T-1)}\right)\]

Given the small terms \(\frac{0.1}{T}\) and \(\frac{19.9\cdot i}{T(T-1)}\), we can consider using a first-order Taylor expansion for the logarithm around 1. The Taylor expansion of \(\log(1-x)\) around \(x=0\) is \(\log(1-x)\approx-x\) for small \(x\). Substituting, we get:

\[\log(\tilde{\alpha}_{t})\approx-\sum_{i=1}^{t}\left(\frac{0.1}{T}+\frac{19.9 \cdot i}{T(T-1)}\right)\]

Plugging \(\sum_{i=1}^{t}i=\frac{t(t+1)}{2}\) into the expression, we get:

\[\log(\tilde{\alpha}_{t})\approx-\sum_{i=1}^{t}\frac{0.1}{T}-\sum_{i=1}^{t} \frac{19.9\cdot i}{T(T-1)}=-\frac{0.1t}{T}-\frac{19.9}{T(T-1)}\cdot\frac{t(t+1 )}{2}\]

Let:

\[f(t)=-\frac{0.1t}{T}-\frac{19.9t(t+1)}{2T(T-1)}\]

We have:

\[f^{\prime}(t)=-\frac{0.1}{T}-\frac{19.9}{2T(T-1)}\;(2t+1)=-\frac{0.1}{T}-\frac {19.9(2t+1)}{2T(T-1)}\]

Plug \(\tilde{\alpha}_{t}=e^{f(t)}\) into the chain rule of \(\frac{\mathrm{d}\tilde{\alpha}_{t}}{\mathrm{d}t}\) and substituting \(f(t)\) and \(f^{\prime}(t)\), we have:

\[\frac{\mathrm{d}\tilde{\alpha}_{t}}{\mathrm{d}t} =\frac{\mathrm{d}}{\mathrm{d}t}\left(e^{f(t)}\right)=e^{f(t)} \cdot f^{\prime}(t)\] \[=\exp\left(-\frac{0.1t}{T}-\frac{19.9t(t+1)}{2T(T-1)}\right)\cdot \left(-\frac{0.1}{T}-\frac{19.9(2t+1)}{2T(T-1)}\right)\]

Substituting \(\frac{\mathrm{d}\tilde{\alpha}_{t}}{\mathrm{d}t}\) back into the expression for \(\frac{\mathrm{d}\mathbf{x}_{t}}{\mathrm{d}t}\) in Eq. 18:

\[\frac{\mathrm{d}\mathbf{x}_{t}}{\mathrm{d}t}=\frac{\left(e\epsilon e^{-\frac{t (0.1T+9.95t+9.85)}{T(T-1)}}-x_{0}\sqrt{1-e^{-\frac{t(0.1T+9.95t+9.85)}{T(T-1)} }}\sqrt{e^{-\frac{t(0.1T+9.95t+9.85)}{T(T-1)}}}\right)(0.1T+19.9t+9.85)}{2T \sqrt{1-e^{-\frac{t(0.1T+9.95t+9.85)}{T(T-1)}}}(T-1)}\] (22)

So, we have:

\[\text{When }t=0,\left.\frac{\mathrm{d}\mathbf{x}_{t}}{\mathrm{d}t}\right|_{t \to 0}=\frac{\otimes\epsilon(0.1T+9.85)}{T(T-1)}=\tilde{\otimes}\cdot\text{ sign}(\epsilon).\]

where \(\tilde{\otimes}\) denotes an unspecified directed infinity in the complex plane.

#### b.2.2 Cosine Schedule

Proof.: Given the expression:

\[\mathbf{x}_{t}=\sqrt{\tilde{\alpha}_{t}}\mathbf{x}_{0}+\sqrt{1-\tilde{\alpha }_{t}}\epsilon,\]

where:

\[\tilde{\alpha}_{t}=\frac{\cos^{2}\left(\frac{t/T+s}{1+s}\cdot\frac{\pi}{2} \right)}{\cos^{2}\left(\frac{s}{1+s}\cdot\frac{\pi}{2}\right)}\]

[MISSING_PAGE_FAIL:23]

### Derivatives of the Logistic Schedule

Proof.: The \(\tilde{\alpha}_{t}\) given by the _Logistic Schedule_ is:

\[\tilde{\alpha}_{t}=\frac{1}{1+e^{-k\left(t-b_{0}\right)}}\]

By differentiating \(\tilde{\alpha}_{t}\), we have:

\[\frac{\mathrm{d}\tilde{\alpha}_{t}}{\mathrm{d}t}=\frac{d}{dt}\left(\frac{1}{1+e ^{-k\left(t-b_{0}\right)}}\right)=\frac{ke^{-k\left(t-t_{0}\right)}}{\left(1+e ^{-k\left(t-b_{0}\right)}\right)^{2}}\]

Substituting \(\frac{\mathrm{d}\tilde{\alpha}_{t}}{\mathrm{d}t}\) back into the expression for \(\frac{\mathrm{d}\mathbf{x}_{t}}{\mathrm{d}t}\) in Eq. 18:

\[\frac{\mathrm{d}\mathbf{x}_{t}}{\mathrm{d}t}=\frac{1}{2}\left(\frac{1}{\sqrt{ \tilde{\alpha}_{t}}}\mathbf{x}_{0}-\frac{1}{\sqrt{1-\tilde{\alpha}_{t}}} \epsilon\right)\cdot\frac{ke^{-k\left(t-b_{0}\right)}}{\left(1+e^{-k\left(t-b _{0}\right)}\right)^{2}}\]

Substitute \(\tilde{\alpha}_{t}\) back into the expression:

\[\frac{\mathrm{d}\mathbf{x}_{t}}{\mathrm{d}t} =\frac{ke^{-k\left(t-t_{0}\right)}}{2\left(1+e^{-k\left(t-b_{0} \right)}\right)^{2}}\left(\frac{\mathbf{x}_{0}}{\sqrt{1+e^{-k\left(t-b_{0} \right)}}}-\frac{\epsilon}{\sqrt{1-\frac{1}{1+e^{-k\left(t-b_{0}\right)}}}}\right)\] (25) \[=\frac{ke^{-k\left(t-t_{0}\right)}}{2\left(1+e^{-k\left(t-b_{0} \right)}\right)^{2}}\left(\mathbf{x}_{0}\sqrt{1+e^{-k\left(t-b_{0}\right)}}- \epsilon\sqrt{\frac{e^{-k\left(t-b_{0}\right)}}{1+e^{-k\left(t-b_{0}\right)}}}\right)\] (26)

When \(t\to 0\):

\[\frac{\mathrm{d}\mathbf{x}_{t}}{\mathrm{d}t}\bigg{|}_{t\to 0}=\frac{0.5 \epsilon k\left(\frac{1}{e^{k_{0}+1.0}}\right)^{0.5}e^{kt_{0}}}{e^{k_{0}}+1.0 }-\frac{0.5kx_{0}e^{kt_{0}}}{\left(1.0-\frac{1}{e^{k_{0}}+1.0}\right)^{0.5} \left(e^{kt_{0}}+1.0\right)^{2}}\]

Substitute the setting \(k=0.015,t_{0}=\mathrm{int}(0.3T)\) and \(T=100\) into the expression, we have:

\[\frac{\mathrm{d}\mathbf{x}_{t}}{\mathrm{d}t}\bigg{|}_{t\to 0}=1.486e^{-3} \epsilon-1.318e^{-3}\mathbf{x}_{0}\]

## Appendix C Related Works

**Text-guided Image Editing.** Text-guided image editing significantly enhances the controllability and accessibility of visual manipulation by following human commands. With the advancement of large-scale training, diffusion models [49, 52, 50] have shown remarkable capabilities in transforming images based on human-given instructions [13, 62, 70]. Some approaches train end-to-end models for image editing [5, 28], while others propose training-free methods that merge information from source and target images using masks for controllability [39, 1]. A breakthrough by Hertz et al. leveraged the attention maps within the UNet to eliminate the need for manual masks, achieving promising results. This insight has been adopted and improved upon across multiple tasks by several works [59, 5, 6, 15, 73]. However, most current image editing approaches still rely on predefined noise schedules without evaluating their effectiveness. In this paper, we propose a newly designed noise schedule for image editing that provides high content preservation and enhanced editability.

**Inversion-based Image Editing.** Editing real images requires first inverting the image back to the latent space of the diffusion model due to the lack of a native latent space for these real images [46, 53, 65, 74], a process called image inversion. To address this, DDIM [55] introduced a deterministic sampling process for diffusion, allowing the inversion of the sampling process to recover the latentnoise. However, the invertible properties of DDIM rely on its linearization assumption, which introduces deviations that drive the inverted latent away from its true distribution. As the Markov properties of the diffusion process come into play, these deviations gradually enlarge, resulting in suboptimal inverted latents that degrade reconstruction and editing quality. Recently, several inversion-based methods have been proposed to mitigate this issue [41, 60, 40, 25]. These methods attempt to correct errors on the reconstruction path to the desired DDIM trajectory, ensuring that the original content in the source image is highly preserved and can be injected into the editing process for better content preservation. However, these methods still rely on the accuracy of DDIM inversion. This brings us to the root of the issue: correcting the DDIM errors themselves.

**Noise Schedule Adjustments.** Previous work on noise scheduling focuses on training diffusion models from scratch to improve image quality or optimize the variational lower bound [29, 23, 26, 14, 20, 34]. Hoogeboom et al. propose noise schedule adjustments and other strategies to effectively train standard denoising diffusion models on high-resolution images without additional sampling modifiers. Lin et al. reveal that common diffusion noise schedules fail to enforce zero terminal SNR, causing discrepancies between training and inference. However, none focus on designing an off-the-shelf noise schedule for image editing--a downstream task that does not require training from scratch and leverages existing models for sampling. This highlights the need for a simple but effective noise schedule tailored for downstream tasks like image editing.

## Appendix D Experimental Settings

### Introduction of Editing Types

We conducts eight editing tasks based on the real images to verify the effectiveness and versatility, along with the corresponding challenges for each task (Table 5):

1. **Attributes Content Editing (Fig. 13)**: Modifies specific attributes, like changing a cat's expression. The **challenge** is ensuring high fidelity and preserving the original content without artifacts.
2. **Attributes Color Editing (Fig. 14)**: Alters color attributes, like changing a bird's color or eye color. The **challenge** is maintaining natural and coherent lighting and shading.
3. **Attributes Material Editing (Fig. 15)**: Changes material properties, like transforming a tiger into a silver sculpture. The **challenge** is accurately rendering new materials while preserving shape and avoiding unrealistic artifacts.
4. **Object Switch (Fig. 16)**: Switches objects, like replacing bread with meat or transforming a fox into a horse. The **challenge** is maintaining scene composition and seamlessly integrating new objects.

\begin{table}
\begin{tabular}{p{113.8pt}|p{113.8pt}|p{113.8pt}} \hline \hline
**Task Name** & **Source Prompt** & **Target Prompt** \\ \hline
**Attributes Content** & a close up of a cat with yellow eyes & a close up of a cat with yellow eyes \\  & & _with its mouth open_ \\ \hline
**Attributes Color** & a smiling woman with _brown_ eyes & a smiling woman with _blue_ eyes \\ \hline
**Attributes Material** & a tiger is sitting in the grass & a _silver tiger sculpture_ is sitting in the grass \\ \hline
**Object Switch** & _bread_ on a table with tomatoes and a napkin & _meat_ on a table with tomatoes and a napkin \\ \hline
**Object Addition** & a close up of a dog & a close up of a dog _with sunglasses_ \\ \hline
**Non-Rigid Editing** & a tiger _walking_ across a field in the wild & a tiger _standing still_ on a field in the wild \\ \hline
**Scene Transferring** & a bench chair in front of _mountains_ & a bench chair in front of _the sea_ \\ \hline
**Style Transferring** & a man riding a skateboard on a ramp & a _watecolor painting_ of a man riding a skateboard on a ramp \\ \hline \hline \end{tabular}
\end{table}
Table 5: Editing tasks with example source and target prompts. The change parts are noted in red.

5. **Object Addition (Fig. 17)**: Adds new objects, like sunglasses to a dog or more strawberries in a bowl. The **challenge** is naturally integrating new objects, ensuring consistent lighting, shadows, and perspective.
6. **Non-Rigid Editing (Fig. 18)**: Makes non-rigid modifications, like changing the pose of a tiger. The **challenge** is preserving anatomical correctness and natural appearance while making pose changes.
7. **Scene Transferring (Fig. 19)**: Transfers scene context, like changing the background from mountains to the sea. The **challenge** is blending new backgrounds seamlessly with the foreground, maintaining consistent lighting, shadows, and color tones.
8. **Style Transferring (Fig. 20)**: Transfers artistic style, like converting a photo into a water-color painting. The **challenge** is preserving essential details and content while accurately applying the new artistic style.

### Implementation Details

All primary experiments are conducted using Stable Diffusion v1.52, with an image size of 512x512x3 and a latent space of 64x64x4. For ablation studies (Section 5.3.2), SD v2.13 and SDXL4 are employed. Experiments run on a single Nvidia A100 GPU with 100 timesteps. The inversion (forward) guidance scale is set to 3.5, and the generation (reverse) guidance scale is set to 7.5. For the logistic schedule, \(k\) is set to 0.015, and \(t_{0}\) is set to \(\text{int}(0.3T)\), where \(T\) is the number of timesteps. Default hyperparameter settings are used unless otherwise specified. For each incorporated method, default hyperparameters are as follows:

Footnote 2: https://huggingface.co/runwayml/stable-diffusion-v1-5

Footnote 3: https://huggingface.co/stabilityai/stable-diffusion-2-1

Footnote 4: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0

Footnote 5: https://huggingface.co/facebook/dino-vitb8

* _Edit Friendly DDPM_[21]: \(T_{\text{skip}}=36\), starting generation from timestep \(T-T_{\text{skip}}\).
* _StyleDiffusion_[63]: Uses SD v1.5 with 1000 inference timesteps and \(T_{\text{trans}}=301\) for style transfer.
* _MasaCtrl_[6]: Starts mutual self-attention control at step \(S=4\) and layer \(L=10\).
* _Pix2Pix Zero_[45]: Applies noise regularization for 5 iterations at each timestep with a weight \(\lambda\) of 20.
* _Null-Text Inversion_[41]: 500 iterations for null-text optimization, with early stopping at \(\epsilon=1e^{-5}\).
* _Negative Prompt Inversion_[40]: Uses early stopping with a threshold increasing linearly from \(1e^{-5}\) by a factor of \(2e^{-5}\) through the sampling steps.

These settings ensure a consistent evaluation framework across different experiments and methods.

### Evaluation Metrics

In this work, seven metrics are employed to evaluate the effectiveness of the _Logistic Schedule_, including three aspects introduced below.

**Structure Distance:** Structure Distance: To evaluate the structure distance between the source and edited images, we leverage _DINO-1_5, which was proposed by DreamBooth [51] to emphasize unique properties of identities. For _DINO-1_, we calculate the cosine similarity between the ViT-B/16 DINO [7] embeddings of source and generated images. Additionally, we consider fine-tuning and editing time as metrics to evaluate the efficiency of the editing process. Since DINO is trained in a self-supervised manner, it highlights general features rather than category-based distinctions, making it suitable for capturing the structural integrity of images.

Footnote 5: https://huggingface.co/facebook/dino-vitb8

**Background Preservation:** To measure how the background is preserved during editing, we apply _PSNR_, _LPIPS_[24, 72], _MSE_, and _SSIM_[64] in the area outside of the annotated masks. These metrics serve different roles in evaluating image quality and preservation:* _PSNR (Peak Signal-to-Noise Ratio)_ measures the ratio between the maximum possible power of a signal and the power of corrupting noise, reflecting the overall quality of the image: \[\text{PSNR}=10\cdot\log_{10}\left(\frac{\text{MAX}^{2}}{\text{MSE}}\right),\] where MAX is the maximum possible pixel value of the image (e.g., 255 for an 8-bit image), and MSE is the Mean Squared Error.
* _LPIPS (Learned Perceptual Image Patch Similarity)_ evaluates perceptual similarity by comparing the differences in deep feature space using a pretrained deep network (such as VGG [54]), capturing human visual perception better than pixel-wise metrics, by calculating the Euclidean distance between the feature representations of two images: \[\text{LPIPS}=\sum_{l}w_{l}\left\|\phi_{l}(x)-\phi_{l}(y)\right\|_{2}^{2},\] where \(\phi_{l}\) denotes the feature map at layer \(l\) of the pretrained network, and \(w_{l}\) are the weights for each layer.
* _MSE (Mean Squared Error)_ calculates the average squared difference between original and edited image pixels, indicating the overall fidelity and error magnitude: \[\text{MSE}=\frac{1}{N}\sum_{i=1}^{N}(I_{1}[i]-I_{2}[i])^{2},\] where \(N\) is the number of pixels in the image, \(I_{1}\) and \(I_{2}\) are the original and edited images respectively, and \(i\) indexes the pixels.
* _SSIM (Structural Similarity Index Measure)_ assesses image similarity by comparing luminance, contrast, and structure, providing a holistic view of image quality. The SSIM index between two images \(x\) and \(y\) is calculated as: \[\text{SSIM}(x,y)=\frac{(2\mu_{x}\mu_{y}+C_{1})(2\sigma_{xy}+C_{2})}{(\mu_{x}^{ 2}+\mu_{y}^{2}+C_{1})(\sigma_{x}^{2}+\sigma_{y}^{2}+C_{2})},\] where \(\mu_{x}\) and \(\mu_{y}\) are the average pixel values of \(x\) and \(y\), \(\sigma_{x}^{2}\) and \(\sigma_{y}^{2}\) are the variances of \(x\) and \(y\), \(\sigma_{xy}\) is the covariance of \(x\) and \(y\), and \(C_{1}\) and \(C_{2}\) are constants to stabilize the division when the denominator is close to zero.

Incorporating these metrics together can demonstrate background preservation more robustly since multiple metrics offer a comprehensive evaluation from different perspectives.

**Text-Image Consistency:** CLIP Similarity [48] evaluates the text-image consistency between the edited images and the corresponding target editing text prompts. _CLIP-I_ and _CLIP-T_ assess visual similarity and text-image alignment, respectively. For _CLIP-I_, we calculate the CLIP visual similarity between the source and generated images. For _CLIP-T_, we calculate the CLIP text-image similarity between the generated images and the given text prompts. These metrics help ensure that the edited images accurately reflect the intended modifications described in the text prompts.

## Appendix E Experimental Results

### Quantitative Comparison Across Editing Types

We provide the performance of _Logistic Schedule_ following the editing methods configuration in Section 5 in Table 6. The results vary across different editing types. Attributes content editing shows high PSNR and CLIP visual similarity. We attribute this to the relatively straightforward nature of modifying content attributes, which allows for high fidelity and coherence in the edited images compared to more complex edits. In the more challenging editing types, such as object addition (5th row) and non-rigid editing (e.g., pose, motion, 6th row), the model shows minimal changes, resulting in relatively better evaluation results in essential content preservation metrics. The limited alterations required in these tasks help maintain the original structure and details, leading to higher PSNR and SSIM values. Object switch shows high CLIP visual similarity. This can be attributed to the clear and distinct nature of object switching, which allows for more precise visual matching with the target object compared to other editing tasks. Style transferring (8th row) shows the highest CLIP text similarity. This is likely because the task involves applying well-defined artistic styles that closely align with the textual descriptions, resulting in edits that match the intended style effectively. Scene and style transferring (7th and 8th row) show high LPIPS and SSIM. We attribute this to the nature of the task, which involves changing backgrounds or styles while keeping the main subjects intact. This process maintains the overall scene coherence and structure consistency, resulting in enhanced perceptual quality and structural similarity.

### Qualitative Comparison Across Editing Types

The logistic noise schedule consistently outperforms linear and cosine schedules across various editing tasks. It excels in preserving the original image content while making specified changes without artifacts (Fig. 13), maintaining natural and coherent lighting in color edits (Fig. 14), and accurately rendering new material properties (Fig. 15). For object switching and addition, it ensures seamless integration with consistent lighting and spatial relationships (Figs. 16, 17). In non-rigid editing, it preserves anatomical correctness and smooth transitions (Fig. 18). It also blends new

\begin{table}
\begin{tabular}{c|c|c c c c|c c} \hline \hline \multirow{2}{*}{**Edit Task**} & \multirow{2}{*}{
\begin{tabular}{c} **Structure** \\ **Dist \(\times 10^{-3}\)** \\ \end{tabular} } & \multicolumn{4}{c}{**Background Preservation**} & \multicolumn{3}{c}{**CLIP Similarity (\%)**} \\  & **PSNR \(\uparrow\)** & **LPIPS \(\times 10^{-3}\)** & **MSE \(\times 10^{-4}\)** & **SSIM \(\times 10^{-2}\)** & **Visual**\(\uparrow\)** & **Textual**\(\uparrow\) \\ \hline
**Attr. Content** & 15.74\({}_{-0.9}\) & **26.58\({}_{-1.3}\)** & 70.69\({}_{-2.1}\) & 51.04\({}_{-1.7}\) & 84.48\({}_{-3.0}\) & **89.30\({}_{-3.2}\)** & 22.05\({}_{-1.6}\) \\
**Attr. Color** & 16.78\({}_{-4.1}\) & 23.81\({}_{-1.2}\) & 89.65\({}_{-3.5}\) & 53.10\({}_{-1.1}\) & 81.04\({}_{-4.7}\) & 81.26\({}_{-2.3}\) & 19.41\({}_{-1.0}\) \\
**Attr. Material** & 18.67\({}_{-0.8}\) & 25.73\({}_{-0.9}\) & 74.17\({}_{-3.4}\) & 41.19\({}_{-1.6}\) & 81.61\({}_{-5.7}\) & 78.06\({}_{-3.3}\) & 24.03\({}_{-1.5}\) \\
**Obj. Switch** & 22.24\({}_{-0.8}\) & 22.91\({}_{-1.2}\) & 90.75\({}_{-5.0}\) & 82.05\({}_{-1.4}\) & 79.32\({}_{-4.6}\) & 86.72\({}_{-3.3}\) & 22.65\({}_{-1.9}\) \\
**Obj. Add** & **11.11\({}_{-1.1}\)** & 25.04\({}_{-1.5}\) & 63.05\({}_{-3.2}\) & **40.52\({}_{-1.0}\)** & 85.32\({}_{-4.6}\) & 76.33\({}_{-3.3}\) & 23.09\({}_{-0.9}\) \\
**Non-rigid** & 15.87\({}_{-1.7}\) & 24.66\({}_{-1.3}\) & 75.18\({}_{-2.5}\) & 59.22\({}_{-1.2}\) & 81.14\({}_{-4.8}\) & 81.26\({}_{-3.6}\) & 22.30\({}_{-1.1}\) \\
**Scene Trans.** & 17.63\({}_{-1.4}\) & 24.79\({}_{-1.3}\) & **55.57\({}_{-2.3}\)** & 48.51\({}_{-1.7}\) & **85.95\({}_{-6.2}\)** & 81.63\({}_{-1.6}\) & 22.11\({}_{-1.0}\) \\
**Style Trans.** & 19.66\({}_{-1.5}\) & 25.50\({}_{-1.8}\) & 60.24\({}_{-4.2}\) & 49.79\({}_{-1.3}\) & 85.60\({}_{-6.0}\) & 80.24\({}_{-5.6}\) & **25.73\({}_{-1.1}\)** \\ \hline \hline \end{tabular}
\end{table}
Table 6: **Performance of _Logistic Schedule_ on different editing tasks** in have ten independent runs with random seeds. Bold values indicate the best results, while underlined values denote the second-best results. ‘Attr.’, ‘Obj.’ and ‘Trans.’ denote ‘Attributes’, ‘Objects’, and ‘Transferring’, respectively.

Figure 8: **Impact of different combinations of inversion and reverse guidance scales on various performance metrics**. Results are averaged on Attributes Editing tasks using Prompt-to-Prompt as the editing method [16], highlighting optimal scale settings for balanced performance across tasks.

backgrounds naturally in scene transfers (Fig. 19) and maintains essential details while applying new artistic styles (Fig. 20).

### Broader Application: Training-Based Methods

While the Logistic Schedule has shown broad applicability in various image editing tasks, we further explore its use in training-based methods, specifically in Text-to-Image synthesis. For this, we conducted experiments by fine-tuning the UNet using DreamBooth [51], leveraging approximately 100 images for each configuration.

The related qualitative comparisons are shown in Fig. 9. These results demonstrate that training DreamBooth with the Logistic Schedule improves performance across key metrics such as DINO and CLIP-I similarity, as well as PSNR and PRES, outperforming both linear and cosine schedules.

### Comparison With Other Noise Schedulers

We conducted experiments comparing our Logistic Schedule with other schedules under the DDIM paradigm, such as exponential, sigmoid, hyperbolic, and geometric schedules. Table 8 displays the quantitative results. The best-performing method is indicated in **bold**, the worst method is marked in purple, and the second-best method is underlined.

The results demonstrate that our Logistic Schedule achieves competitive performance across various metrics. Notably, it offers significant improvements in content preservation and edit fidelity compared to other schedules. As shown in Fig. 10, the Logistic Schedule preserves the visual characteristics of the source image more faithfully during reconstruction and enables more precise control during editing.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline
**Setting** & **DINO** (\(\uparrow\)) & **CLIP-I** (\(\uparrow\)) & **CLIP-T** (\(\uparrow\)) & **PSNR** (\(\uparrow\)) & **PRES** (\(\uparrow\)) \\ \hline Real Images & 0.823 & 0.902 & N/A & 26.86 & 0.653 \\ DreamBooth (SD-1.5) + Linear & 0.726 & 0.823 & 0.268 & 24.41 & 0.567 \\ DreamBooth (SD-1.5) + Cosine & 0.745 & 0.857 & 0.264 & 24.75 & 0.554 \\ DreamBooth (SD-1.5) + **Logistic** & **0.761** & **0.877** & **0.293** & **25.62** & **0.580** \\ \hline \hline \end{tabular}
\end{table}
Table 7: Performance comparison of DreamBooth fine-tuning using different noise schedules on the SD-1.5 model. The best results are in **bold**.

Figure 9: Qualitative comparisons of fine-tuning DreamBooth using different noise schedules (Linear, Cosine, and Logistic). The top column presents the fine-tune samples, and the instruction prompts, and the below column displays the corresponding fine-tuned outputs. The Logistic Schedule produces superior outputs with improved fidelity and alignment to the prompts.

### Reconstruction Ability of Different Noise Schedule

To demonstrate the ability of the _Logistic Schedule_ to better align inversion by eliminating the singularity at the start point, we evaluate the reconstruction results of DDIM Inversion [55] and Direct Inversion [25] using scaled linear, cosine, and our logistic schedule. During reconstruction, the condition is the source prompt, which is also applied as a condition during inversion. The comparison of reconstruction quality is shown in Table 9.

\begin{table}
\begin{tabular}{l|c c c c c c} \hline \hline
**Schedule** & **Dist \(\downarrow\)** & **PSNR \(\uparrow\)** & **LPIPS \(\downarrow\)** & **MSE \(\downarrow\)** & **SSIM \(\uparrow\)** & **Visual \(\uparrow\)** & **Textual \(\uparrow\)** \\ \hline Linear & 35.66 & 20.70 & 134.88 & 113.61 & 77.60 & 79.82 & 23.06 \\ Cosine & 26.57 & 22.38 & 110.52 & 80.01 & 80.15 & 81.35 & 22.39 \\ Exponential & 16.22 & 25.20 & 80.45 & 47.11 & 82.23 & 82.78 & 19.50 \\ Hyperbolic & 36.55 & 20.95 & 140.55 & 119.78 & 79.89 & 79.20 & 23.20 \\ Geometric & 18.12 & 24.10 & 92.45 & 62.13 & 82.05 & 82.20 & 20.45 \\ Sigmoid & 27.80 & 22.55 & 115.32 & 85.60 & 80.22 & 81.50 & 22.55 \\ Logistic & 17.37 & 24.78 & 81.80 & 49.47 & 82.97 & 82.44 & 23.62 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Comparison of different noise schedules. Metrics include Structure Distance (\(\times 10^{-3}\)), PSNR (higher is better), LPIPS (\(\times 10^{-3}\), lower is better), MSE (\(\times 10^{-4}\), lower is better), SSIM (\(\times 10^{-2}\), higher is better), and CLIP Similarity for both visual and textual content.

Figure 10: Qualitative comparison between different noise schedules for both reconstruction and editing tasks.

\begin{table}
\begin{tabular}{l|c|c|c c c c} \hline \hline \multirow{2}{*}{**Inversion**} & \multirow{2}{*}{**Schedule**} & \multirow{2}{*}{
\begin{tabular}{c} **Structure** \\ **Dist \(\times 10^{-3}\)** \\ \end{tabular} } & \multicolumn{4}{c}{**Background Preservation**} \\  & & & **PSNR \(\uparrow\)** & **LPIPS \(\times 10^{-3}\)** & **MSE \(\times 10^{-4}\)** & **SSIM \(\times 10^{-2}\)** \(\uparrow\)** \\ \hline \multirow{3}{*}{**DDIM Inversion**} & **Linear** & 7.96 & 27.46 & 58.49 & 30.08 & 84.54 \\  & **Cosine** & 7.43 & 27.36 & 61.22 & 28.49 & 82.66 \\  & **Logistic** & 7.04 & 25.78 & 71.87 & 37.59 & 80.07 \\ \hline \multirow{3}{*}{**Direct Inversion**} & **Linear** & 2.78 & 29.58 & 36.36 & 20.23 & 85.28 \\  & **Cosine** & 2.75 & 30.00 & 33.80 & 21.70 & 85.90 \\ \cline{1-1}  & **Logistic** & 2.54 & 31.30 & 31.16 & 12.27 & 88.94 \\ \hline \hline \end{tabular}
\end{table}
Table 9: **Comparison of reconstruction quality** using different noise schedules for DDIM inversion and Direct Inversion, showing the superior performance of the Logistic Schedule.

Figure 11: **Qualitative comparison of varying guidance scales during the inversion (forward) and denoising (reversing) processes of DDIM. The guidance scales for inversion are varied across the columns (1 to 10), and the guidance scales for denoising are varied across the rows (3 to 25).**

### Effects of Guidance Scale

We investigate the impact of the guidance scale on the inversion (forward) and generation (reserve) processes of DDIM with the _Logistic Schedule_, consequently affecting the editing results. We illustrate the impact of varying guidance scales during the inversion and denoising processes of DDIM on performance metrics in Fig.8, with an example of how the edited images are affected shown in Fig.11. When keeping the inverse guidance scale constant, we observed that as the reverse guidance scales increased gradually, background preservation initially decreased. The inflection point occurred when the inverse guidance scale equaled the forward guidance scale. In contrast, CLIP similarity showed a consistently increasing trend until the reverse guidance scale exceeded 15. The quantitative heatmaps and qualitative results highlight a noticeable trade-off between essential content preservation and edit fidelity. Optimal incorporation of both scales ensures a balance between structural preservation, perceptual quality, and text-image consistency, with a combination of 5.0 for inversion and 7.5 for reverse generally providing the best performance across most metrics. This trade-off arises because current editing methods struggle to differentiate between regions needing modification and those that do not, leading to substantial alterations of the source image and conflicting with content preservation objectives.

### Effects of Input Scale

Chen et al. proposed to modify noise scheduling by scaling the input \(\mathbf{x}_{0}\) by a constant factor \(b\) via:

As the scaling factor \(b\) decreases, the original image's strength lessens and noise levels grow [9]. As previous image editing works have not extensively investigated the effects of input scale, we investigate the effects of input scale in image editing in this work. We change the input scale from 0.5 to 1.4 with a step size of 0.05, and illustrate the effects of the input scale on both content preservation and edit fidelity in Fig. 12. As observed, the input scale significantly impacts the balance between content preservation and edit fidelity. Higher input scales (closer to 1.4) better preserve the original image structure, as shown by lower structure distances and higher SSIM values but reduce edit fidelity (lower CLIP-T and CLIP-I scores). Conversely, lower input scales (closer to 0.5) enhance edit fidelity but degrade content preservation. The optimal input scale, found to be 0.8-0.95, achieves a balance between these objectives. This is because slightly higher noise levels improve editability while maintaining acceptable content preservation, providing a satisfactory trade-off in image editing.

A strategy to improve training a diffusion model from scratch is to normalize \(\mathbf{x}_{t}\) by its variance to ensure it has unit variance before feeding it to the denoising network. This prevents performance issues caused by variance changes in \(\mathbf{x}_{t}\) when \(\mathbf{x}_{0}\) has the same mean and variance as \(\epsilon\)[26]. However, in our image editing work using off-the-shelf diffusion models, we find that normalization does not improve content preservation or edit fidelity, as shown in Table 10.

\begin{table}
\begin{tabular}{c|c|c c c c c|c} \hline \multirow{2}{*}{**Input Scale**} & \multirow{2}{*}{**Structure**} & \multicolumn{5}{c}{**Background Preservation**} & \multicolumn{3}{c}{**CLIP Similarity (\%)**} \\  & & **Dist\({}_{\times 10^{-3}}\)** & \multirow{2}{*}{**PSNR \(\uparrow\)**} & **LPIPS \({}_{\times 10^{-3}}\)** & **MSE \({}_{\times 10^{-4}}\)** & **SSIM \({}_{\times 10^{-2}}\)** & \multirow{2}{*}{**Visual \(\uparrow\)**} & **Testual \(\uparrow\)** \\ \hline
**w/o Normalizing \(b\)** & 22.40 & 22.91 & 90.75 & 82.05 & 79.32 & 86.72 & 22.65 \\
**w. Normalizing \(b\)** & 24.46 & 22.06 & 108.43 & 83.47 & 79.16 & 86.24 & 22.48 \\ \hline \end{tabular}
\end{table}
Table 10: **Performance comparison of input scale normalization** in object switch task using Zero-shot Pix2Pix, showing no improvement in content preservation or edit fidelity.

Figure 12: **Impact of input scale on content preservation and edit fidelity**. The optimal input scale balances the preservation of the original image structure (low structure distance, high SSIM) and the quality of the edits (high CLIP-T and CLIP-I).

## Appendix F Limitations and Future Works

This work endeavors to enhance inversion-based editing methods, focusing on improving noise schedule design during the inversion process. Even though this work reveals that modifying the noise schedule using off-the-shelf diffusion models can lead to editing improvements, there is still a lack of research on how other designs of noise schedules can lead to different effects. For example, is it possible to design dynamic adjustments of the noise schedule at each time step to achieve better results? Furthermore, the editing capabilities of the Logistic Schedule are inherently constrained by the limitations of inversion-based methods. For example, MasaCtrl [6] editing requires manual determination of timesteps and layers for attention control, limiting its ability to automatically adapt to diverse real-world objects with varying attributes.

Even though extensive experiments prove the effectiveness of the Logistic Schedule, it is worth diving deeper into the schedule's performance in the generation task. Due to computational resource constraints, we have not conducted training on the diffusion model from scratch to validate the full potential of the Logistic Schedule. Future work will include training diffusion models using the Logistic Schedule to validate its generation ability.

Another potential future research direction lies in exploring whether a steadier decrease in logSNR during perturbation, as described in Section 4, enhances editing quality. Additional experiments on both generation and editing are required to confirm if this trend extends to the generation process as well.

Figure 13: Comparison of linear, cosine, and logistic schedules in editing **attributes content**. The logistic schedule shows superior fidelity in preserving the original image content while accurately making the specified changes, without introducing artifacts or inconsistencies.

Figure 14: Comparison of linear, cosine, and logistic schedules in editing **color attributes**. The logistic schedule excels in maintaining natural and coherent lighting and shading, resulting in more realistic and seamless color changes without introducing visual inconsistencies.

Figure 15: Comparison of linear, cosine, and logistic schedules in **changing material properties**. The logistic schedule excels in accurately rendering new material properties, such as reflections and textures while preserving the shape and form of the original objects, avoiding unrealistic artifacts, and ensuring a natural appearance.

## Appendix G Broader Impacts

Our work introduces a novel editing technique for manipulating real images using state-of-the-art text-to-image diffusion models. While this technology could potentially be exploited by malicious parties to create fake content and spread disinformation, this is a common issue across all image editing techniques. Significant progress is already being made in identifying and preventing such malicious editing. Our research contributes to this effort by providing a detailed analysis of the inversion and editing processes in text-to-image diffusion models, thereby aiding in the development of more robust detection and prevention methods.

## Appendix H Ethics Statement

Generative models for synthesizing images carry several ethical concerns, particularly when used by bad actors to generate disinformation or potentially displace creative workers through automation. These models, trained on large amounts of user data from the internet without explicit consent, may generate augmentations that resemble or copy such data. This issue is not unique to our work but inherent to large-scale models like Stable Diffusion, which we employ in our data augmentation strategy. To mitigate this, we allow for the deletion of harmful or copyrighted concepts from the model's weights before augmentation, ensuring such material cannot be copied during the process.

Figure 16: Comparison of linear, cosine, and logistic schedules in **switching objects**. The logistic schedule excels in maintaining the overall composition and context of the scene while seamlessly integrating the new objects with consistent lighting, shadows, and spatial relationships, ensuring a natural and coherent appearance.

Figure 17: Comparison of linear, cosine, and logistic schedules in **adding objects**. The logistic schedule excels in naturally integrating the new objects into the scene, ensuring consistent lighting, shadows, and perspective with the existing elements, resulting in a realistic and seamless appearance.

Figure 18: Comparison of linear, cosine, and logistic schedules in making **non-rigid** modifications. The logistic schedule excels in preserving anatomical correctness and natural appearance while making significant pose changes, ensuring smooth transitions and avoiding unnatural distortions.

Figure 19: Comparison of linear, cosine, and logistic schedules in **transferring scenes**. The logistic schedule excels in seamlessly blending the new background with the foreground objects, maintaining consistent lighting, shadows, and color tones, avoiding visible seams, and ensuring a natural and coherent appearance.

Figure 20: Comparison of linear, cosine, and logistic schedules in **transferring styles**. The logistic schedule excels in preserving essential details and content of the original image while accurately applying the new artistic style, ensuring consistency across the entire image and avoiding artifacts, resulting in a more natural and coherent style transfer.

Despite these concerns, these tools may also foster growth and improve accessibility in the creative industry.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims made in the abstract and introduction (Section 1) accurately reflect the paper's contributions and scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitations of the work are discussed in _Appendix_ F. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: Concerning Proposition 3.1, _Appendix_ B provides its corresponding proofs, with a short proof sketch to provide intuition. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Refer to _Appendix_ D.1 and D.2 for implementation details, the provided code in the supplementary materials ensures the reproducibility as well. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Refer to the provided code in the supplementary materials. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Table 5 specifies all the tasks included in this work. _Appendix_ D.2 provides the employed hyperparameters. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: _Appendix_ E.1 provides the statistical significance of the experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: _Appendix_ D.2 provide sufficient information on the computer resources. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The authors have reviewed and understand the NeurIPS Code of Ethics, and confirm that their research conforms to it in every respect. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Potential societal impacts of the work are discussed in _Appendix_ G. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The original papers of assets, and the corresponding versions are provided in Section 5, _References_ and _Appendix_ D.2. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We will document the assets well when we officially release our code. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not describe potential risks incurred by study participants. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.