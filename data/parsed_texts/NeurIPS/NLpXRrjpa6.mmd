# Policy Gradient for Rectangular Robust Markov Decision Processes

 Navdeep Kumar

Technion

&Esther Derman

MILA, Universite de Montreal

Matthieu Geist

Goodle Deepmind

&Kfir Levy

Technion

&Shie Mannor

Technion, NVIDIA Research

###### Abstract

Policy gradient methods have become a standard for training reinforcement learning agents in a scalable and efficient manner. However, they do not account for transition uncertainty, whereas learning robust policies can be computationally expensive. In this paper, we introduce robust policy gradient (RPG), a policy-based method that efficiently solves rectangular robust Markov decision processes (MDPs). We provide a closed-form expression for the worst occupation measure. Incidentally, we find that the worst kernel is a rank-one perturbation of the nominal. Combining the worst occupation measure with a robust Q-value estimation yields an explicit form of the robust gradient. Our resulting RPG can be estimated from data with the same time complexity as its non-robust equivalent. Hence, it relieves the computational burden of convex optimization problems required for training robust policies by current policy gradient approaches.

## 1 Introduction

Markov decision processes (MDPs) provide an analytical framework to solve sequential decision-making problems and seek the best performance in a fixed environment. Since the resulting policy can be highly sensitive to parameter values [18], the robust MDP setting alternatively maximizes return under the worst scenario, thus yielding robustness to uncertain environments [20, 12]. In practice, the robust MDP paradigm quantifies the level of uncertainty through a set \(\mathcal{U}\) determining the possible range of model perturbations. Then, a policy is said to be robust-optimal if it reaches maximal performance under the most adversarial model within the uncertainty set. Developing efficient solvers for robust MDPs is of great interest, as it can lead to behavior policies with generalization guarantees [34].

If not computationally expensive, robust MDPs can be strongly NP-hard [33]. Thus, to preserve tractability, we commonly assume that \(\mathcal{U}\) is convex and \(s\)-rectangular, i.e., \(\mathcal{U}=\times_{s\in\mathcal{S}}\mathcal{U}_{s}\)[20, 12, 33]. Well established in the robust reinforcement learning (RL) literature [8, 3, 9, 10, 28], the latter assumption means that the overall uncertainty should be designed independently for each state. Further simplification may consider \((s,a)\)-rectangular uncertainty sets of the form \(\mathcal{U}=\times_{(s,a)\in\mathcal{X}}\mathcal{U}_{(s,a)}\), albeit this naturally leads to more conservative strategies. In any case, planning in robust MDPs can be computationally costly, as it involves successive max-min problems [9, 1, 33]. To address this issue, the works [3, 14] have established an equivalence between robustness and regularization in RL in order to derive efficient robust planning methods for \(s\) and \((s,a)\)-rectangular robust MDPs. Indeed, it appears that resorting to proper regularization instead of solving a minimization problem can yield robust behavior without requiring the polynomial time complexity of convex optimization problems [3].

Alternatively to planning, policy gradient algorithms (PG) directly learn an optimal policy by applying gradient steps towards better performance [25]. Due to its scalability, ease of implementation, and adaptability to many different settings such as model-free and continuous state-action spaces [13; 24], PG has become the workhorse of RL. Although regularization techniques such as max-entropy [7] or Tsallis [15] have shown robust behavior without impairing computational cost, they only account for adversarial reward [2; 6; 3]. Differently, robust PG formulations (RPG) formulations aim to address uncertainty to reward _and_ transition functions.

Despite their ability to propel robust behavior, RPG methods that target robust optimal policies are still rare in the RL literature. The global convergence of RPG established in [16; 30] already ensures global convergence of our proposed algorithm, but motivates a practical method for estimating the gradients. Indeed, [16; 30] occult the estimation part, as they assume full access to the policy gradient. Alternatively, the inner loop solution proposed in [30][Sec. 4.1] requires solving convex optimization problems to find the worst model, which represents a larger time complexity of \(O(S^{4}A\log\epsilon^{-1})\) for \((s,a)\)-rectangular, or \(O(S^{4}A^{3}\log\epsilon^{-1})\) for \(s\)-rectangular uncertainty sets. These worst kernel and reward models are needed to compute RPG using the policy gradient theorem [26]. Other approaches that elicit an expression for RPG rely on a specific type of uncertainty set such as reward uncertainty with known kernel [3; 5], \(r\)-contaminated kernel with known reward [32], or \((s,a)\)-rectangular uncertainty [16], whereas we aim to tackle more general robust MDPs.

In this work, we introduce an RPG method for both \(s\) and \((s,a)\)-rectangular ball-constrained uncertainty sets, with similar complexity as non-robust PG. Our approach provides a closed-form expression of RPG without relying on an oracle while applying to the most common robust MDPs. To this end, we derive the worst reward and transition functions, thus revealing the adversarial nature of the corresponding uncertainty set. Surprisingly, we also find that the worst kernel is a rank-one perturbation of the nominal kernel. Leveraging this rank-one perturbation enables us to derive a robust occupation measure. We concurrently propose an alternative definition of the robust Q-value together with an efficient way to estimate it. Combining these results enables us to obtain RPG in closed form. Our resulting RPG update requires \(O(S^{2}A\log\epsilon^{-1})\) computations, thus showing similar time complexity as non-robust PG.

To summarize our contributions: _(i)_ We establish the worst reward and transition models in closed-form; _(ii)_ We show that the worst-case transition function is a rank-one perturbation of the nominal; _(iii)_ We introduce alternative robust Q-values that can be evaluated through efficient Bellman recursion while retrieving the robust value function; _(iv)_ We establish an expression of RPG that can be estimated with similar time complexity as non-robust PG. Experiments show that our RPG speeds up state-of-the-art robust PG updates by 2 orders of magnitude.

## 2 Related work

Although some previous works use gradient methods to learn robust policies, they seek empirical robustness to adversarial behavior rather than robust MDP solutions [21; 29; 4]. In that sense, our study differs from adversarial RL as we explicitly optimize the max-min objective to find a robust optimal policy. Accordingly, the risk-averse approach focuses on the _internal uncertainty_ due to the stochasticity of the system, whereas robust RL addresses the _external uncertainty_ of the system's dynamics. As a result, common risk-averse objectives can be reformulated as robust problems with specific uncertainty sets [27].

Previous studies that did aim to derive robust policy-based methods are [3; 32; 30]. These are summarized in Table 1, which also displays the complexity of existing approaches. [3] established RPG for \(s\)-rectangular reward-robust MDPs, i.e., robust MDPs with uncertain reward but given kernel. Although it applies to general norms, their result does not account for transition perturbation. Differently, in [32], the authors introduced RPG for \(r\)-contaminated MDPs, i.e., robust MDPs with uncertainty set \(\mathcal{U}:=\{R_{0}\}\times[(1-r)P_{0}+r\Delta_{S}^{\mathcal{S}\times\mathcal{ A}}]\). Although it has similar complexity as non-robust PG, by construction, their setting is limited to \((s,a)\)-rectangularity with known reward and mixed transition. As such, the proof techniques in [32] are tailor-made to the \(r\)-contamination framework and do not apply to more general robust MDPs. In fact, we remark that the \(r\)-contamination setting is equivalent to the action robustness approach introduced in [29], which emphasizes its limitation to action perturbation. Differently, our RPG holds whenever the worst kernel is a rank-one perturbation of the nominal transition function (see Lemma 4.4).

The work [16] provides a convergence proof of robust policy mirror-descent in the \((s,a)\)-rectangular case, whereas we study robust policy optimization for \(s\)-rectangular uncertainty sets. In fact, its restriction to the \((s,a)\)-case prevents us from transposing the analysis to our setting. This is due to the fact that the standard robust Bellman operator on \(Q\)-functions can no longer be applied on \(s\)-rectangular sets. To address generic robust MDPs, [30] recently introduced RPG for general uncertainty sets. Their gradient update has a complexity of \(O(S^{6}A^{4}\epsilon^{-4})\), which is more expensive than non-robust PG by a factor of \(S^{4}A^{3}\epsilon^{-4}\). Both works [16; 30] additionally assume access to an oracle gradient of the robust return with respect to the transition model. Avoiding this oracle assumption naturally leads to even higher time complexity in [30] which is not scalable. At the same time, the two works [16; 30] guarantee global convergence of projected robust gradient iterates, thus establishing the potential promise of RPG. In fact, equipped with RPG convergence, the remaining challenge in making it practical is to efficiently estimate the gradient. This represents the main focus of our study: We aim to explicit an RPG method that generalizes existing results on specific uncertainty sets [3; 32] while holding for \(s\)-rectangular robust MDPs.

## 3 Preliminaries

Notation:We denote the cardinal of an arbitrary finite set \(\mathcal{Z}\) by \(|\mathcal{Z}|\). Given two real functions \(\mathbf{a},\mathbf{b}:\mathcal{Z}\to\mathbb{R}\), their inner product is \(\langle\mathbf{a},\mathbf{b}\rangle_{\mathcal{Z}}:=\sum_{z\in\mathcal{Z}} \mathbf{a}(z)\,\mathbf{b}(z)\), which induces the \(\ell_{2}\)-norm \(\|\mathbf{a}\|_{2}:=\sqrt{\langle\mathbf{a},\mathbf{a}\rangle_{\mathcal{Z}}}\). More generally, the \(\ell_{p}\)-norm of \(\mathbf{a}\) is denoted by \(\|\mathbf{a}\|_{p}\) whose conjugate norm is \(\|\mathbf{a}\|_{q}:=\max_{\|\mathbf{b}\|_{q}\leq 1}\langle\mathbf{a}, \mathbf{b}\rangle_{\mathcal{Z}}\) with \(q^{-1}=1-p^{-1}\), by Holder's inequality. The vector of all zeros (resp. all ones) with appropriate dimensions is denoted by \(\mathbf{0}\) (resp. \(\mathbf{1}\)); the probability simplex over \(\mathcal{Z}\) by \(\Delta_{\mathcal{Z}}:=\{\mathbf{a}:\mathcal{Z}\to\mathbb{R}_{+}|\langle \mathbf{a},\mathbf{1}\rangle_{\mathcal{Z}}=1\}\), and \(I_{\mathcal{Z}}\) designates the identity matrix in \(\mathbb{R}^{\mathcal{Z}\times\mathcal{Z}}\). Given \(v\in\mathbb{R}^{\mathcal{Z}}\), we finally define the variance function as \(\kappa_{q}(v)=\min_{w\in\mathbb{R}}\|v-w\mathbf{1}\|_{q}\) and the mean function as \(\omega_{q}(v)\in\arg\min_{w\in\mathbb{R}}\|v-w\mathbf{1}\|_{q}\) (see Tab. 2 for their closed-form expression when \(q\in\{1,2,\infty\}\)).

### Markov Decision Processes

A Markov decision process (MDP) is a tuple \((\mathcal{S},\mathcal{A},\gamma,\mu,P,R)\) such that \(\mathcal{S}\) and \(\mathcal{A}\) are finite state and action spaces of cardinal \(S\) and \(A\) respectively, \(\gamma\in[0,1)\) is a discount factor and \(\mu\in\Delta_{\mathcal{S}}\) the initial state distribution. Denoting \(\mathcal{X}:=\mathcal{S}\times\mathcal{A}\), the couple \((P,R)\) corresponds to the MDP model with \(P:\mathcal{X}\to\Delta_{\mathcal{S}}\) being a transition kernel and \(R:\mathcal{X}\to\mathbb{R}\) a reward function. A policy \(\pi:\mathcal{S}\to\Delta_{\mathcal{A}}\) maps each state to a probability distribution over \(\mathcal{A}\), and we denote by \(\Pi\) the set of such functions. For any policy \(\pi\in\Pi\), \(R^{\pi}\in\mathbb{R}^{\mathcal{S}}\) is the expected immediate reward defined as \(R^{\pi}(s):=\langle\pi_{s},R(s,\cdot)\rangle_{\mathcal{A}},\quad\forall s\in \mathcal{S}\), where \(\pi_{s}\) is a shorthand for \(\pi(\cdot|s)\). We similarly define the stochastic matrix induced by \(\pi\) as \(P^{\pi}(s^{\prime}|s):=\langle\pi_{s},P(s^{\prime}|s,\cdot)\rangle_{\mathcal{A}}, \quad\forall s,s^{\prime}\in\mathcal{S}\), and extend the occupation measure to an arbitrary initial vector \(\nu\in\mathbf{R}^{\mathcal{S}}\) by defining

\[d_{P,\nu}^{\pi}:=\nu^{\top}(I_{\mathcal{S}}-\gamma P^{\pi})^{-1}.\]

\begin{table}
\begin{tabular}{l l l} \hline \hline Uncertainty set \(\mathcal{U}\) & Time Complexity & Reference \\ \hline \(\{R_{0}\}\times\{P_{0}\}\) & \(S^{2}A\log\epsilon^{-1}\) & [26] \\ \hline \(\{R_{0}\}\times[(1-r)P_{0}+r\Delta_{\mathcal{S}}^{\mathcal{S}\times\mathcal{A }}]\) & \(S^{2}A\log\epsilon^{-1}\) & [32] \\ \((s,a)\)-rectangular ball \(\mathcal{U}_{p}^{\pi}\) & \(S^{2}A\log\epsilon^{-1}\) & **This work** \\ \((s,a)\)-rectangular, convex \(\mathcal{U}^{\mathsf{sa}}\) & \(S^{4}A\log\epsilon^{-1}\) & Convex optimization \\ \hline \(s\)-rectangular ball \(\mathcal{U}_{p}^{\mathsf{s}}\) & \(S^{2}A\log\epsilon^{-1}\) & **This work** \\ \(s\)-rectangular ball \((R_{0}+\mathcal{R}_{p}^{\mathsf{s}})\times\{P_{0}\}\) & \(S^{2}A\log\epsilon^{-1}\) & [3] \\ \(s\)-rectangular, convex \(\mathcal{U}^{\mathsf{s}}\) & \(S^{4}A^{3}\log\epsilon^{-1}\) & Convex optimization \\ \(s\)-rectangular, convex \(\mathcal{U}^{\mathsf{s}}\) & \(S^{6}A^{4}\epsilon^{-4}\) & [30] \\ \(s\)-rectangular, non-convex \(\mathcal{U}^{\mathsf{s}}\) & NP-hard & [33] \\ \hline Non-rectangular, convex \(\mathcal{U}\) & NP-hard & [33] \\ \hline \hline \end{tabular}
\end{table}
Table 1: Time complexity of RPG update according to the type of uncertainty set. For conciseness, the displayed complexity hides logarithmic factors in \(A\) and \(S\). Our RPG method has the same complexity as non-robust PG while it generalizes other RPG methods with similar efficiency.

Note that the initial vector here is not necessarily a probability measure: it can be the initial state distribution, but also the balanced value function introduced in Sec. 4[Eq. (2)]. The performance measure we aim to maximize is the value function \(v^{\pi}_{(P,R)}:=(I_{\mathcal{S}}-\gamma P^{\pi})^{-1}R^{\pi}\), or alternatively, the return \(\rho^{\pi}_{(P,R)}:=\langle\mu,v^{\pi}_{(P,R)}\rangle_{\mathcal{S}}\). We denote the optimal value function (resp. optimal return) by \(v^{*}_{(P,R)}=\max_{\pi\in\Pi}v^{\pi}_{(P,R)}\) (resp. \(\rho^{*}_{(P,R)}=\langle\mu,v^{*}_{(P,R)}\rangle\)). It can be obtained using Bellman operators, which are defined as \(T^{\pi}_{{}_{(P,R)}}v:=R^{\pi}+\gamma P^{\pi}v\) and \(T^{*}_{{}_{(P,R)}}v:=\max_{\pi\in\Pi}T^{\pi}_{{}_{(P,R)}}v,\quad\forall v\in \mathbb{R}^{\mathcal{S}}\), respectively [22]. For any vector \(v\in\mathbb{R}^{\mathcal{S}}\), we associate its Q-function \(Q\in\mathbb{R}^{\mathcal{X}}\) such that

\[Q(s,a)=R(s,a)+\gamma\langle P(\cdot|s,a),v\rangle_{\mathcal{S}},\quad\forall( s,a)\in\mathcal{X}\,.\]

With a slight abuse of notation, we can similarly define a Bellman operator over Q-values as

\[T^{\pi}_{{}_{(P,R)}}Q(s,a):=R(s,a)+\gamma\sum_{(s^{\prime},a^{ \prime})\in\mathcal{X}}P(s^{\prime}|s,a)\pi_{s^{\prime}}(a^{\prime})Q(s^{ \prime},a^{\prime}),\quad\forall(s,a)\in\mathcal{X}\,.\]

### Robust Markov Decision Processes

In a robust MDP setting, we assume that \((P,R)\in\mathcal{U}\) and aim to maximize return under the worst model from the set. We denote the robust performance of a policy \(\pi\in\Pi\) by \(\rho^{\pi}_{\mathcal{U}}:=\min_{(P,R)\in\mathcal{U}}\rho^{\pi}_{(P,R)}\). It is maximal when it reaches \(\rho^{*}_{\mathcal{U}}:=\max_{\pi\in\Pi}\rho^{\pi}_{\mathcal{U}}\) at an optimal robust policy \(\pi^{*}_{\mathcal{U}}\in\arg\max_{\pi\in\Pi}\rho^{\pi}_{\mathcal{U}}\). When considering the robust value function \(v^{\pi}_{\mathcal{U}}:=\min_{(P,R)\in\mathcal{U}}v^{\pi}_{(P,R)}\), we further need to assume that \(\mathcal{U}\) is convex and rectangular so that an optimal robust policy realizing \(v^{*}_{\mathcal{U}}:=\max_{\pi}v^{\pi}_{\mathcal{U}}\) can be computed in polynomial time [33]. We thus assume \(\mathcal{U}\) to be convex and rectangular in the remainder of this work. Specifically, we denote an \((s,a)\)-rectangular uncertainty set by \(\mathcal{U}^{\mathtt{sa}}:=\times_{(s,a)\in\mathcal{X}}(\mathcal{P}_{(s,a)}, \mathcal{R}_{(s,a)})\). It represents a particular case of \(s\)-rectangular uncertainty which we similarly denote by \(\mathcal{U}^{\mathtt{a}}:=\times_{s\in\mathcal{S}}(\mathcal{P}_{s},\mathcal{R }_{s})\). In both cases, there exists an optimal robust policy that is stationary, although all optimal ones may be stochastic [33].

Similarly to non-robust MDPs, robust MDPs can be solved through Bellman recursion. Indeed, the robust value function \(v^{\pi}_{\mathcal{U}}\) (resp., optimal robust value function \(v^{*}_{\mathcal{U}}\)) is known to be the unique fixed point of the robust Bellman operator \(T^{\pi}_{\mathcal{U}}v:=\min_{(P,R)\in\mathcal{U}}T^{\pi}_{(P,R)}v\) (resp., the optimal robust Bellman operator \(T^{*}_{\mathcal{U}}v:=\max_{\pi\in\Pi}T^{\pi}_{\mathcal{U}}\)), \(\forall v\in\mathbb{R}^{\mathcal{S}}\), both being \(\gamma\)-contractions for the sup-norm. Although this ensures linear convergence of robust value iteration, the evaluation of each Bellman operator can still be prohibitive for practical use.

#### 3.2.1 Ball Constrained Uncertainty set

To facilitate the computation of robust Bellman updates, we consider uncertainty sets that are centered around a nominal model \((P_{0},R_{0})\), i.e., of the form \(\mathcal{U}=(P_{0},R_{0})+(\mathcal{P},\mathcal{R})\), and constrained according

\begin{table}
\begin{tabular}{l l l l} \hline \hline  & \(\omega_{q}(v)\) & \(\kappa_{q}(v)\) & \(\nabla_{v}\kappa_{q}(v)\) \\ \hline \(q\) & \(\arg\min_{w\in\mathbb{R}}\lVert v-w\mathbf{1}\rVert_{q}\) & \(\min_{\omega\in\mathbb{R}}\lVert v-\omega\mathbf{1}\rVert_{q}\) & \(\frac{\partial\kappa_{q}(v)}{\partial v(s_{i})}\) \\ \(\infty\) & \(\frac{v(s_{1})+v(s_{S})}{2}\) & \(\frac{v(s_{1})-v(s_{S})}{2}\) & \(\begin{cases}\frac{1}{2}&\text{if }i=1\\ -\frac{1}{2}&\text{if }i=S\\ 0&\text{o.w.}\end{cases}\) \\ \(2\) & \(\frac{\sum_{i=1}^{S}v(s_{i})}{S}\) & \(\sqrt{\sum_{i=1}^{S}(v(s_{i})-\omega_{2}(v))^{2}}\) & \(\frac{v(s_{i})-\omega_{2}(v)}{\kappa_{2}(v)}\) \\ \(1\) & \(\frac{v(s_{n_{l}})+v(s_{n_{u}})}{2}\) & \(\sum_{i=1}^{n_{l}}(v(s_{i})-v(s_{S-i}))\) & \(\begin{cases}1&\text{if }i<n_{l}\\ -1&\text{if }i>n_{u}\\ 0&\text{o.w.}\end{cases}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Expressions of the \(q\)-mean, the \(q\)-variance, and its gradient. We assume that the vector \(v\) is sorted, i.e., \(v(s_{i})\geq v(s_{i+1}),\forall i\in\{1,2,\cdots,S\}\), and denote \(n_{l}:=\lfloor(S+1)/2\rfloor,n_{u}:=\lceil(S+1)/2\rceil\).

to \(\ell_{p}\)-norm balls [3, 14, 9, 1]. In the \((s,a)\)-rectangular case, the corresponding uncertainty set is denoted by \(\mathcal{U}_{p}^{\mathbf{sa}}:=\mathcal{R}_{p}^{\mathbf{sa}}\times\mathcal{P}_{p} ^{\mathbf{sa}}=\times_{(s,a)\in\mathcal{X}}(\mathcal{P}_{(s,a)},\mathcal{R}_{(s,a)})\) where for any \((s,a)\in\mathcal{X}\),

\[\mathcal{R}_{(s,a)}=\left\{r\in\mathbb{R}\mid|r|\leq\alpha_{s,a}\right\}, \quad\text{and}\quad\mathcal{P}_{(s,a)}=\left\{p\in\mathbb{R}^{\mathcal{S}} \mid\langle p,\mathbf{1}\rangle_{\mathcal{S}}=0,\|p\|_{p}\leq\beta_{s,a}\right\}.\]

Similarly, an \(s\)-rectangular norm-constrained uncertainty is denoted by \(\mathcal{U}_{p}^{\mathbf{s}}:=\times_{s\in\mathcal{S}}(\mathcal{P}_{s}, \mathcal{R}_{s})\) where for any \(s\in\mathcal{S}\),

\[\mathcal{R}_{s}=\{r\in\mathbb{R}^{\mathcal{A}}\mid\|r\|_{p}\leq\alpha_{s}\}, \quad\text{and}\quad\mathcal{P}_{s}=\{p\in\mathbb{R}^{\mathcal{X}}\mid\langle p (\cdot,a),\mathbf{1}\rangle_{\mathcal{S}}=0\quad\forall a\in\mathcal{A},\|p \|_{p}\leq\beta_{s}\}.\]

In both cases, the kernel uncertainty set conceals linear constraints ensuring all entries in \(P_{0}+\mathcal{P}\) are non-negative. Indeed, we generally ignore \(P_{0}\) to satisfy these constraints in practice [23]. Although it may include absurd models and unnecessarily lead to conservative policies, this proxy region is appropriate for model-free robust learning. Moreover, the norm-ball structure on uncertainty sets above enables us to compute robust Bellman updates with similar time complexity as non-robust ones using regularization [3, 14]. We formalize this below.

**Proposition 3.1**.: _([14, Thm. 2-3].) For any policy \(\pi\in\Pi\) and any rectangular \(\ell_{p}\)-ball-constraint uncertainty set, the robust Bellman operator is equivalent to its regularized form:_

\[(T_{\mathcal{U}}^{\pi}v)(s)=T_{\langle p_{0},R_{0}\rangle}^{\pi}v(s)+\Omega_{q }(\alpha,\beta,v),\]

_where \(\Omega_{q}(\alpha,\beta,v):=-\langle\pi_{s},\alpha_{s,\cdot}+\gamma\kappa_{q} (v)\beta_{s,\cdot}^{P}\rangle_{\mathcal{A}}\) for \((s,a)\)-rectangular uncertainty \(\mathcal{U}_{p}^{\mathbf{sa}}\), and \(\Omega_{q}(\alpha,\beta,v):=-(\alpha_{s}+\gamma\beta_{s}\kappa_{q}(v))\|\pi_{ s}\|_{q}\) for \(s\)-rectangular uncertainty \(\mathcal{U}_{p}^{\mathbf{sa}}\)._

In the following, we leverage the regularized formulation of robust value functions to explicitly derive RPG for rectangular \(\ell_{p}\)-ball uncertainty sets.

#### 3.2.2 Robust Gradient Method

Since the robust return can be non-differentiable, we need to follow the projected sub-gradient ascent rule in order to optimize the robust return, namely, update \(\pi_{k+1}:=\textbf{proj}_{\Pi}(\pi_{k}+\eta\partial_{\pi}\rho_{\mathcal{U}}^{ \pi_{k}})\) where

\[\partial_{\pi}\rho_{\mathcal{U}}^{\pi}:=\nabla_{\pi}\rho_{(P,R)}^{\pi}\ \Big{|}_{(P,R)=(P_{\mathcal{U}}^{\pi},R_{\mathcal{U}}^{\pi})},\] (1)

\(\eta\) is the learning rate, \(\textbf{proj}_{\Pi}\) denotes the orthogonal projection on \(\Pi\), and \((P_{\mathcal{U}}^{\pi},R_{\mathcal{U}}^{\pi})\) is the worst model associated with \(\pi\in\Pi\) and \(\mathcal{U}\), i.e., \((P_{\mathcal{U}}^{\pi},R_{\mathcal{U}}^{\pi})\in\arg\inf_{(P,R)\in\mathcal{U}} \rho_{(P,R)}^{\pi}\).

Given oracle access to sub-gradient \(\partial\rho_{\mathcal{U}}^{\pi}\), projected gradient ascent converges to an \(\epsilon\)-optimal policy \(\pi_{\mathcal{U}}^{*}\). Moreover, under similar conditions as in the non-robust setting, projected gradient ascent holds an iteration complexity of \(O(S^{4}A^{2}\epsilon^{-4})\)[30]. Yet, the sub-gradient in (1) is generally intractable, particularly because general convex uncertainty sets may yield NP-hard complexity. Instead, we propose to focus on ball-constrained uncertainty sets in order to efficiently compute RPG updates.

## 4 Towards RPG: Expressing the worst quantities

In this section, we provide all the ingredients needed for deriving RPG. Before diving into the gradient expression, we first settle on the general framework of policy gradient. Secondly, in Sec. 4.1, we focus on expressing the worst model according to the nominal explicitly. Surprisingly, we find that the worst transition kernel is a rank-one perturbation of the nominal. This finding enables us to derive the robust occupancy measure, i.e., the visitation frequency of the worst kernel in Sec. 4.2. As a last piece, in Sec. 4.3, we propose an alternative definition of robust Q-value and show that it can be estimated from a specific Bellman recursion.

Consider again the projected gradient ascent rule:

\[\pi_{k+1}:= \textbf{proj}_{\Pi}(\pi_{k}+\eta\partial_{\pi}\rho_{\mathcal{U}}^{ \pi_{k}}).\]

By definition of the sub-gradient in (1) and applying the standard PG theorem [26], it holds that:

\[\partial_{\pi}\rho_{\mathcal{U}}^{\pi}=\sum_{(s,a)\in\mathcal{X}}d_{\mathcal{U} }^{\pi}(s)Q_{\mathcal{U}}^{\pi}(s,a)\nabla\pi_{s}(a),\]where \(Q^{\pi}_{\mathcal{U}}:=Q^{\pi}_{(P^{\pi}_{\mathcal{U}},R^{\pi}_{\mathcal{U}})}\) is the Q-value associated with the worst-case model, and \(d^{\pi}_{\mathcal{U}}:=d^{\pi}_{\mathcal{U}^{\pi}_{\mathcal{U}}}\) the occupation measure of the worst transition kernel. In fact, for the uncertainty sets we focus on in this work, the worst Q-value \(Q^{\pi}_{\mathcal{U}}\) retrieves the common definition of robust Q-value [20, 28] (see the appendix for a detailed discussion). Therefore, for conciseness and with a slight abuse, we shall designate \(Q^{\pi}_{\mathcal{U}}\) by the robust Q-value, and \(d^{\pi}_{\mathcal{U}}\) by the robust visitation frequency. The remaining question is how to compute these quantities and in particular, can we efficiently find the worst parameters \((P^{\pi}_{\mathcal{U}},R^{\pi}_{\mathcal{U}})\)? The following part of our study aims to address these questions.

Given an uncertainty set \(\mathcal{U}\), let first define the normalized and balanced robust value function as:

\[u^{\pi}_{\mathcal{U}}(s):=\frac{\text{sign}(v^{\pi}_{\mathcal{U}}(s)-\omega_{ q}(v^{\pi}_{\mathcal{U}}))\|v^{\pi}_{\mathcal{U}}(s)-\omega_{q}(v^{\pi}_{ \mathcal{U}})\|^{q-1}}{\kappa_{q}(v^{\pi}_{\mathcal{U}})^{q-1}}.\] (2)

By construction, it has zero mean and unit norm, i.e., \(\langle u^{\pi}_{\mathcal{U}},\mathbf{1}\rangle_{\mathcal{S}}=0\) and \(\|u^{\pi}_{\mathcal{U}}\|_{p}=1\). In fact, as stated in the result below, \(u^{\pi}_{\mathcal{U}}\) is the gradient of the \(q\)-variance function, and correlates with the (unnormalized, unbalanced) robust value function according to the same \(q\)-variance.

**Proposition 4.1**.: _For any policy \(\pi\in\Pi\) and \(\ell_{p}\)-ball rectangular uncertainty set, the following holds:_

\[u^{\pi}_{\mathcal{U}} =\nabla_{v}k_{q}(v)\;\Big{|}_{v=v^{\pi}_{\mathcal{U}}},\] \[\langle u^{\pi}_{\mathcal{U}},v^{\pi}_{\mathcal{U}}\rangle =\kappa_{q}(v^{\pi}_{\mathcal{U}}).\]

### Worst Kernel and Reward

In the following results, we explicit the relationship between the nominal and the worst-case model for \((s,a)\) and \(s\)-rectangular \(\ell_{p}\)-balls. We will then leverage this relationship to compute the robust Q-values and the robust occupation measure, both necessary for RPG.

**Theorem 4.2** (\((s,a)\)-rectangular case).: _Given uncertainty set \(\mathcal{U}=\mathcal{U}^{\pi a}_{p}\) and any policy \(\pi\in\Pi\), the worst model is related to the nominal one through:_

\[R^{\pi}_{\mathcal{U}}(s,a)=R_{0}(s,a)-\alpha_{s,a}\qquad\text{and}\qquad P^{ \pi}_{\mathcal{U}}(\cdot|s,a)=P_{0}(\cdot|s,a)-\beta_{s,a}u^{\pi}_{\mathcal{U }}.\]

Based on Thm. 4.2, it follows that in the \((s,a)\)-rectangular case, the worst reward function is independent of the employed policy. As we establish in Thm. 4.3 below, this no longer applies under \(s\)-rectangularity. In either case, the worst kernel is policy-dependent, discouraging the system to move toward high-rewarding states and directing it to low-rewarding ones instead. Surprisingly, the vector penalty \(u^{\pi}_{\mathcal{U}}\in\mathbb{R}^{\mathcal{S}}\) additionally illustrates that the worst kernel is a rank-one perturbation of the nominal. Indeed, considering the stochastic matrix induced by any policy \(\pi\in\Pi\), we have

\[[P^{\pi}_{\mathcal{U}}-P^{\pi}_{0}](s^{\prime}|s)=-\left(\sum_{a\in\mathcal{A }}\beta_{s,a}\pi_{s}(a)\right)u^{\pi}_{\mathcal{U}}(s^{\prime}),\quad\forall s \in\mathcal{S},\]

so that the perturbation matrix \(P^{\pi}_{\mathcal{U}}-P^{\pi}_{0}\) is of rank one. In the sequel, we will leverage this finding to compute the robust visitation frequency.

**Theorem 4.3** (\(s\)-rectangular case).: _Given uncertainty set \(\mathcal{U}=\mathcal{U}^{s}_{p}\) and any policy \(\pi\in\Pi\), the worst model is related to the nominal one through:_

\[R^{\pi}_{\mathcal{U}}(s,a)=R_{0}(s,a)-\alpha_{s}\left(\frac{\pi_{s}(a)}{\|\pi_ {s}\|_{q}}\right)^{q-1}\quad\text{and}\quad P^{\pi}_{\mathcal{U}}(\cdot|s,a)=P _{0}(\cdot|s,a)-\beta_{s}u^{\pi}_{\mathcal{U}}\left(\frac{\pi_{s}(a)}{\|\pi_{ s}\|_{q}}\right)^{q-1}.\]

Similarly to the \((s,a)\)-case, the adversarial kernel is a rank-one perturbation of the nominal. Yet, an extra dependence on the policy through the coefficient \(\left(\frac{\pi_{s}(a)}{\|\pi_{s}\|_{q}}\right)^{q-1}\) appears in the \(s\)-case, affecting both the worst reward and the worst kernel. Intuitively, it means that the worst model cannot be chosen independently for each action, but must instead depend on the agent's policy. This further explains why optimal policies can all be stochastic in \(s\)-rectangular robust MDPs [33].

Thms. 4.2 and 4.3 enable us to derive the worst MDP model in closed form with time complexity \(O(S^{2}A\log\epsilon^{-1})\), up to logarithmic factors (please see the appendix for a detailed discussion). It thus holds the same complexity as non-robust value iteration, since we additionally need to compute the value function to derive its corresponding regularizer [3, 14]. On the other hand, if we employ convex optimization using value methods instead, obtaining the worst model requires a time complexity of \(O(S^{4}A\log\epsilon^{-1})\) in the \((s,a)\)-rectangular case, and \(O(S^{4}A^{3}\log\epsilon^{-1})\) in the \(s\)-rectangular case [30][Sec. 4.1].

### Robust Occupation Measure

We finally derive the robust occupation measure using nominal values, which will lead to an explicit RPG. Although intractable in general, we show that focusing on ball-constrained uncertainty enables deriving the robust occupation matrix efficiently from the (nominal) occupation measure. We first establish the lemma below, which leverages the fact that the worst transition function is a rank-one perturbation of the nominal and represents our core contribution.

**Lemma 4.4**.: _Let \(b,k\in\mathbb{R}^{\mathcal{S}}\) and \(P_{0},P_{1}\in(\Delta_{\mathcal{S}})^{\mathcal{S}}\) two transition matrices. If \(P_{1}=P_{0}-bk^{\top}\), i.e., \(P_{1}\) is a rank-one perturbation of \(P_{0}\), then their occupation matrices \(D_{i}:=(I-\gamma P_{i})^{-1},i=0,1\) are related through:_

\[D_{1}=D_{0}-\gamma\frac{D_{0}bk^{\top}D_{0}}{(1+\gamma k^{\top}D_{0}b)}.\]

Combining Thms. 4.2 and 4.3 with the above lemma, we obtain the robust occupation in terms of the nominal, as stated in Thm. 4.6 below. Prior to this, we introduce the notion of _expected transition uncertainty_ below.

**Definition 4.5**.: _Let \(\mathcal{U}\) a rectangular \(\ell_{p}\)-ball-constrained uncertainty set of transition radius \(\beta\). For any policy \(\pi\in\Pi\), the expected transition uncertainty at any state \(s\in\mathcal{S}\) is given by \(\beta_{s}^{\pi}:=\sum_{a\in\mathcal{A}}\pi_{s}(a)\beta_{s,a}\) if \(\mathcal{U}=\mathcal{U}_{p}^{s\bullet}\), and \(\beta_{s}^{\pi}:=\beta_{s}\|\pi_{s}\|_{q}\) if \(\mathcal{U}=\mathcal{U}_{p}^{s}\)._

**Theorem 4.6**.: _For any rectangular \(\ell_{p}\)-ball-constrained uncertainty and \(\pi\in\Pi\), it holds that:_

\[d^{\pi}_{\mathcal{U},\mu}=d^{\pi}_{P_{0},\mu}-\gamma\frac{\langle d^{\pi}_{P_{ 0},\mu},\beta^{\pi}\rangle_{\mathcal{S}}}{1+\gamma\langle d^{\pi}_{P_{0},u^{ \pi}_{\mathcal{U}}},\beta^{\pi}\rangle_{\mathcal{S}}}d^{\pi}_{P_{0},u^{\pi}_{ \mathcal{U}}}.\] (3)

Thm. 4.6 explicitly highlights the relationship between the robust visitation frequency and the nominal one. Thus, according to Eq. (3), the standard non-robust occupation measure in the first term needs to be penalized by another one, \(d^{\pi}_{P_{0},u^{\pi}_{\mathcal{U}}}=(u^{\pi}_{\mathcal{U}})^{\top}(I_{ \mathcal{S}}-\gamma P_{0}^{\pi})^{-1}\), to obtain the robust occupation measure. Recall that \(u^{\pi}_{\mathcal{U}}\) is the balanced-scaled value function determined by \(\pi\in\Pi\) and uncertainty set \(\mathcal{U}\). Thus, the penalty term \(d^{\pi}_{P_{0},u^{\pi}_{\mathcal{U}}}\) tends to zero if all coordinates of the robust value function vector converge to the same value.

Nonetheless, our expression (3) does present some challenges. First, the visitation frequency appearing in the correction term indicates that instead of taking a fixed initial state distribution, we should start from a _varying_ and _signed_ measure represented by the balanced value function. Although it suggests putting more weight on worst-performing states, obtaining a non-biased estimator for this occupancy measure remains unclear in model-free learning. One may use importance sampling, but as any off-policy approach, both variance and bias would need to be controlled then. Such statistical analysis goes beyond the scope of this work.

### Robust Q-values

In this section, we focus on the last element needed for RPG and aim to estimate the robust Q-value denoted previously by \(Q^{\pi}_{\mathcal{U}}:=Q^{\pi}_{(P^{\pi}_{\mathcal{U}},R^{\pi}_{\mathcal{U}})}\). Define its associated value function as \(v^{\pi}_{\mathcal{U}}(s)=\langle\pi_{s},Q^{\pi}_{\mathcal{U}}(s,\cdot)\rangle, \forall s\in\mathcal{S},\pi\in\Pi\). Based on standard Bellman recursion, it thus holds that:

\[Q^{\pi}_{\mathcal{U}}(s,a)=R^{\pi}_{\mathcal{U}}(s,a)+\gamma\langle P^{\pi}_{ \mathcal{U}}(\cdot|s,a),v^{\pi}_{\mathcal{U}}\rangle,s,\quad\forall(s,a)\in \mathcal{X},\pi\in\Pi,\]

while \(Q^{\pi}_{\mathcal{U}}\) is the unique fixed point of the \(\gamma\)-contracting operator

\[(\mathcal{L}^{\pi}_{\mathcal{U}})Q(s,a):=T^{\pi}_{(P^{\pi}_{\mathcal{U}},R^{ \pi}_{\mathcal{U}})}Q(s,a),\quad\forall Q\in\mathbb{R}^{\mathcal{X}}\,.\] (4)

The relations above hold for general uncertainty sets, provided that we have access to the worst model. The \(s\)-rectangularity assumption additionally enables us to retrieve the robust value function using the Bellman operator above [33]. Concretely, we have: \(v^{\pi}_{\mathcal{U}}=\min_{(P,R)\in\mathcal{U}}v^{\pi}_{(P,R)}=v^{\pi}_{(P^{ \pi}_{\mathcal{U}},R^{\pi}_{\mathcal{U}})}\).

The following result derives a regularized operator equivalent to \(\mathcal{L}^{\pi}_{\mathcal{U}}\), which results in an efficient iteration method to compute the robust Q-value.

**Proposition 4.7**.: _The Bellman operator \(\mathcal{L}^{\pi}_{\mathcal{U}}\) defined in Eq. (4) is equivalent to:_

\[(\mathcal{L}^{\pi}_{\mathcal{U}})Q(s,a)=T^{\pi}_{(P_{0},R_{0})}Q(s,a)+\Omega^{ \prime}_{q}(\alpha_{s,a},\beta_{s,a},v),\]

_where \(v(s):=\langle\pi_{s},Q(s,\cdot)\rangle_{\mathcal{A}}\), \(\Omega^{\prime}_{q}(\alpha,\beta,v):=-(\alpha_{s,a}+\gamma\beta_{s,a}\kappa_{q }(v))\) for \((s,a)\)-rectangular uncertainty \(\mathcal{U}^{s\bullet}_{p}\), and \(\Omega^{\prime}_{q}(\alpha,\beta,v):=-\left(\frac{\pi_{s}(a)}{\|\pi_{s}\|_{q }}\right)^{q-1}(\alpha_{s}+\gamma\beta_{s}\kappa_{q}(v))\) for \(s\)-rectangular \(\mathcal{U}^{s}_{p}\)._Robust Policy Gradient

We are now able to derive an RPG by combining our previous results. Notably, unlike previous works that need to sample next-state transitions based on all models from the uncertainty set [21; 17; 4], here, we only need the nominal kernel to get the occupation measures.

**Theorem 5.1** (RPG).: _For any rectangular \(\ell_{p}\)-ball-constrained uncertainty, the robust policy gradient is given by:_

\[\partial_{\pi}\rho_{\mathcal{U}}^{\pi}=\sum_{(s,a)\in\mathcal{X}}\left(d_{ \mathcal{P}_{0},\mu}^{\pi}(s)-c^{\pi}(s)\right)Q_{\mathcal{U}}^{\pi}(s,a) \nabla\pi_{s}(a),\] (5)

_where_

\[c^{\pi}(s):=\frac{\gamma\langle d_{\mathcal{P}_{0},\mu}^{\pi},\beta^{\pi} \rangle s}{1+\gamma\langle d_{\mathcal{P}_{0},u_{\mathcal{U}}^{\pi}}^{\pi}, \beta^{\pi}\rangle S}d_{\mathcal{P}_{0},u_{\mathcal{U}}^{\pi}}^{\pi}(s),\quad \forall s\in\mathcal{S}\,.\]

The implementation of RPG directly follows and can be found in Alg. 1. Thm. 5.1 is a straightforward application of non-robust PG, as its proof simply consists in plugging Eq. (3) into the standard PG expression \(\partial_{\pi}\rho_{\mathcal{U}}^{\pi}=\sum_{(s,a)\in\mathcal{X}}d_{\mathcal{ U},\mu}^{\pi}(s)Q_{\mathcal{U}}^{\pi}(s,a)\nabla\pi_{s}(a)\). We obtain a regular PG in the first term, with the robust Q-value instead of the non-robust one, plus a correction term \(c^{\pi}\) resulting from taking the visitation frequency of the worst kernel instead of the nominal. Unlike previous work that uses policy regularization to achieve empirical robustness in PG methods [2; 11], Thm. 5.1 establishes an RPG that accounts for transition uncertainty and targets a robust optimal policy. It crucially relies on the rank-one-perturbation structure of the worst transition kernel (see Lem. 4.4). As established in Thm. 4.6, \(\ell_{p}\)-ball uncertainty implies such property, but the converse as to whether any convex set leads to the worst transition kernel being a rank-one perturbation of the nominal remains an open question. For example, it would be interesting to investigate the structural properties needed on the uncertainty set for the rank-one perturbation to hold.

```
0:\(\mu,\eta\) Initialize:\(v_{k},\pi_{k}\)
1:for\(k=1,2,\ldots\)do
2:\(\partial_{\pi}\rho_{k}=\sum_{(s,a)\in\mathcal{X}}\left(d_{\mathcal{P}_{0}, \mu}^{\pi}(s)-c^{\pi}(s)\right)Q_{\mathcal{U}}^{\pi}(s,a)\nabla\pi_{s}(a)\)\(\triangleright\) Compute policy gradient
3:\(\pi_{k}\leftarrow\textbf{proj}_{\Pi}(\pi_{k}+\eta\partial_{\pi}\rho_{k})\)\(\triangleright\) Update policy
4:endfor ```

**Algorithm 1** RPG

### Complexity Analysis

A major concern in solving robust MDPs is time complexity [33]. Similarly, it is of major importance to assess the additional time required for computing an RPG update, compared to its non-robust variant. Although previous work has analyzed the convergence rate of RPG to a global optimum [30], it assumes access to an oracle gradient, thus occulting the computational concerns raised from gradient estimation. In fact, the NP-hardness of non-rectangular and/or non-convex robust MDPs [33] already indicates that their resulting RPG can be intractable.

To compute RPG in Thm. 5.1, we first need to evaluate the robust Q-value. Based on Lemma 4.7 and the Bellman operators introduced there, our evaluation method involves an additional estimation of the variance function \(\kappa_{p}\). According to [14], this takes logarithmic time at most, using binary search. As to the compensation term \(c^{\pi}\) in Eq. (11), it requires computing occupancy measures with respect to two different initial vectors, namely the balanced value function and the initial distribution. Thus, the computational cost for estimating \(c^{\pi}\) is the same as estimating a non-robust occupancy measure. Tab. 1 summarizes the complexity of different approaches while a detailed discussion can be found in the appendix. We refer to [30][Sec. 4.1] for the complexity of RPG based on convex optimization.

Generalization to arbitrary norms.Until now, we have focused on \(\ell_{p}\)-norm for concreteness. However, the above results apply to any norm \(\|\cdot\|\), at least if the uncertainty set is \((s,a)\)-rectangular, in which case the variance function changes to \(\kappa(v):=\min_{\|c\|\leq 1,\mathbf{1}^{\top}=0}\langle c,v\rangle\) and the balanced value to \(\arg\min_{\|c\|\leq 1,\mathbf{1}^{\top}=0}\langle c,v\rangle\). The rank-one perturbation structure of the worst kernel is preserved, so the robust occupation measure can be obtained similarly using Lemma 4.4. The \(s\)-rectangular is more involved. We defer its discussion to the appendix and leave its complete derivation for future work.

Experiments

In order to test the effectiveness of our RPG update, we evaluate its increased time complexity relative to non-robust PG. In the following experiments, we randomly generate nominal models for arbitrary state-action space sizes. Each experiment was averaged over 100 runs. We refer the reader to the appendix for more details on the radius levels and other implementation choices.

We first focus on \(\ell_{1}\)-robust MDPs to compare our RPG with a convex optimization approach. Specifically, we consider a robust PG with an optimization solver, which we designate by LP-RPG. Indeed, recall that \(\ell_{1}\)-ball-constraints induce a linear program (LP) rather than a more general convex optimization problem. Therefore, to compute the robust value function for a given policy, we iteratively evaluate the robust Bellman operator using LP [30, Section 4.1]. Using this approximated value function, we can compute the worst value parameters to apply PG theorem by [26] and deduce an LP-based robust PG update. Differently, our RPG method relies on the regularized formulation of robust value iteration proposed in [3; 14], from which we deduce the normalized-balanced value function as in Eq. (2). We finally apply Thm. 4.6 to compute the robust occupation measure, and Prop. 4.7 to obtain the robust Q-value.

Tab. 3 displays the results obtained for the two alternative methods described above. In all experiments, the standard deviation was typically 2-10% so we omitted it for brevity. As can be seen in Tab. 3, LP-RPG does not scale well compared to RPG, whereas RPG has similar time complexity as PG. Notably, the running time of \(s\)-rectangular LP-RPG scales much better with the space size than its \((s,a)\)-rectangular equivalent, which confirms the theoretical complexities from Tab. 1. Yet, since these methods were time-consuming, we repeated these for a few runs only. In fact, LP-RPG is more expensive than RPG by 1-3 orders of magnitude, which illustrates its inefficiency. We emphasize that here, we only focused on \(\ell_{1}\)-robust MDPs to leverage LP solvers in robust policy evaluation. We expect the computational cost of LP-RPG to scale even more poorly for other \(\ell_{p}\)-robust MDPs that involve polynomial time-consuming convex programs.

We further compare our RPG to non-robust PG on different \(\ell_{p}\)-balls. Tab. 4 confirms the comparable time complexity of RPG to non-robust PG, thus demonstrating the effectiveness of our method. We note that for \(p\in\{1,2,\infty\}\), the corresponding regularization quantities can be computed in closed form, whereas they involve a binary search for other values [14]. We thus get a slight running-time increase for \(p\in\{5,10\}\).

\begin{table}
\begin{tabular}{c c||c c||c c||c c||c c} \hline \hline S & A & \(\{(P_{0},R_{0})\}\) & \(\mathcal{U}_{2}^{\text{sa}}\) & \(\mathcal{U}_{2}^{\text{s}}\) & \(\mathcal{U}_{5}^{\text{sa}}\) & \(\mathcal{U}_{5}^{\text{s}}\) & \(\mathcal{U}_{10}^{\text{sa}}\) & \(\mathcal{U}_{10}^{\text{s}}\) & \(\mathcal{U}_{\infty}^{\text{sa}}\) & \(\mathcal{U}_{\infty}^{\text{s}}\) \\ \hline
10 & 10 & 1 & 1.5 & 1.5 & 4.9 & 4.7 & 4.7 & 4.9 & 1.5 & 1.6 \\
30 & 10 & 1 & 1.4 & 1.5 & 4.2 & 4.3 & 4.2 & 4.0 & 1.4 & 1.4 \\
50 & 10 & 1 & 1.5 & 1.4 & 4.5 & 4.1 & 4.0 & 4.0 & 1.4 & 1.4 \\
100 & 20 & 1 & 1.4 & 1.3 & 2.6 & 2.5 & 2.5 & 2.4 & 1.3 & 1.2 \\
500 & 50 & 1 & 1.2 & 1.2 & 1.7 & 1.7 & 1.7 & 1.2 & 1.3 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Relative running time for computing RPG under different types of uncertainty sets.

\begin{table}
\begin{tabular}{c c||c||c c||c c} \hline \hline  & & \(\{(P_{0},R_{0})\}\) & \multicolumn{2}{c||}{\(\mathcal{U}_{1}^{\text{sa}}\)} & \multicolumn{2}{c}{\(\mathcal{U}_{1}^{\text{s}}\)} \\ \hline S & A & PG & RPG & LP-RPG & RPG & LP-RPG \\ \hline
10 & 10 & 1 & 1.4 & 326 & 1.4 & 77 \\
30 & 10 & 1 & 1.4 & 351 & 1.4 & 109 \\
50 & 10 & 1 & 1.4 & 408 & 1.4 & 159 \\
100 & 20 & 1 & 1.5 & 469 & 1.3 & 268 \\
500 & 50 & 1 & 1.3 & 925 & 1.3 & 5343 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Comparison of the relative running time between RPG and the convex optimization approach (here, LP). Our method is faster than LP-based updates by 1 to 3 orders of magnitude.

Discussion

This paper introduced an explicit expression of RPG for rectangular robust MDPs. Our approach involved auxiliary results such as deriving the worst model in closed form and showing that it is a rank-one perturbation of the nominal kernel. The resulting RPG extends vanilla PG with additional correction terms that can be derived in closed form as well. Thus, the computational time of RPG is similar to its non-robust variant.

A key assumption that would be interesting to relax is the normed-ball structure of the uncertainty sets considered in this study. Indeed, since the proofs of our technical results rely on norm properties, it is still unclear if and how RPG can generalize to metric-based or \(f\)-divergence uncertainty sets. The latter type of uncertainty can be particularly useful for data-driven settings, as the radius can be chosen according to cross-validation or statistical bounds [10]. Another compelling direction would be to explore other variants of RPG using mirror descent or natural policy gradient and examine their compatibility with deep architectures, which would further demonstrate the practical efficiency of our RPG method.

## References

* Behzadian et al. [2021] Bahram Behzadian, Marek Petrik, and Chin Pang Ho. Fast algorithms for \(\ell_{\infty}\)-constrained s-rectangular robust MDPs. _Advances in Neural Information Processing Systems_, 34:25982-25992, 2021.
* Brekelmans et al. [2022] Rob Brekelmans, Tim Genewein, Jordi Grau-Moya, Gregoire Deletang, Markus Kunesch, Shane Legg, and Pedro Ortega. Your policy regularizer is secretly an adversary. _Transactions on Machine Learning Research (TMLR)_, 2022.
* Derman et al. [2021] Esther Derman, Matthieu Geist, and Shie Mannor. Twice regularized MDPs and the equivalence between robustness and regularization. _Advances in Neural Information Processing Systems_, 34:22274-22287, 2021.
* Derman et al. [2018] Esther Derman, Daniel J. Mankowitz, Timothy A. Mann, and Shie Mannor. Soft-robust actor-critic policy-gradient. _AUAI press for Association for Uncertainty in Artificial Intelligence_, pages 208-218, 2018.
* Derman et al. [2023] Esther Derman, Yevgeniy Men, Matthieu Geist, and Shie Mannor. Twice regularized Markov decision processes: The equivalence between robustness and regularization. _arXiv preprint arXiv:2303.06654_, 2023.
* Eysenbach and Levine [2022] Benjamin Eysenbach and Sergey Levine. Maximum entropy RL (provably) solves some robust RL problems. _International Conference on Learning Representations_, 2022.
* Haarnoja et al. [2018] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In _International Conference on Machine Learning_, pages 1861-1870. PMLR, 2018.
* Ho et al. [2018] Chin Pang Ho, Marek Petrik, and Wolfram Wiesemann. Fast Bellman updates for robust MDPs. In _International Conference on Machine Learning_, pages 1979-1988. PMLR, 2018.
* Ho et al. [2021] Chin Pang Ho, Marek Petrik, and Wolfram Wiesemann. Partial policy iteration for \(l_{1}\)-robust Markov decision processes. _J. Mach. Learn. Res._, 22:275-1, 2021.
* Ho et al. [2022] Chin Pang Ho, Marek Petrik, and Wolfram Wiesemann. Robust \(\phi\)-divergence MDPs. _arXiv preprint arXiv:2205.14202_, 2022.
* Husain et al. [2021] Hisham Husain, Kamil Ciosek, and Ryota Tomioka. Regularized policies are reward robust. In _International Conference on Artificial Intelligence and Statistics_, pages 64-72. PMLR, 2021.
* Iyengar [2005] Garud N Iyengar. Robust dynamic programming. _Mathematics of Operations Research_, 30(2):257-280, 2005.
* Kakade and Langford [2002] Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In _International Conference on Machine Learning_. Citeseer, 2002.

* [14] Navdeep Kumar, Kfir Levy, Kaixin Wang, and Shie Mannor. Efficient policy iteration for robust markov decision processes via regularization, 2022.
* [15] Kyungjae Lee, Sungjoon Choi, and Songhwai Oh. Sparse Markov decision processes with causal sparse Tsallis entropy regularization for reinforcement learning. _IEEE Robotics and Automation Letters_, 3(3):1466-1473, 2018.
* [16] Yan Li, Tuo Zhao, and Guanghui Lan. First-order policy optimization for robust Markov decision process. _arXiv preprint arXiv:2209.10579_, 2022.
* [17] Daniel Mankowitz, Timothy Mann, Pierre-Luc Bacon, Doina Precup, and Shie Mannor. Learning robust options. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 32, 2018.
* [18] Shie Mannor, Duncan Simester, Peng Sun, and John N Tsitsiklis. Bias and variance approximation in value function estimates. _Management Science_, 53(2):308-322, 2007.
* [19] Paul Milgrom and Ilya Segal. Envelope theorems for arbitrary choice sets. _Econometrica_, 70:583-601, 02 2002.
* [20] Arnab Nilim and Laurent El Ghaoui. Robust control of Markov decision processes with uncertain transition matrices. _Operations Research_, 53(5):780-798, 2005.
* [21] Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust adversarial reinforcement learning. In _International Conference on Machine Learning_, pages 2817-2826. PMLR, 2017.
* [22] Martin L Puterman. _Markov decision processes: discrete stochastic dynamic programming_. John Wiley & Sons, 2014.
* [23] Aurko Roy, Huan Xu, and Sebastian Pokutta. Reinforcement learning under model mismatch. _Advances in Neural Information Processing Systems_, 2017.
* [24] John Schulman, Xi Chen, and Pieter Abbeel. Equivalence between policy gradients and soft q-learning, 2017.
* [25] Richard S. Sutton and Andrew G. Barto. _Reinforcement Learning: An Introduction_. The MIT Press, second edition, 2018.
* [26] Richard S Sutton, David A McAllester, Satinder P Singh, Yishay Mansour, et al. Policy gradient methods for reinforcement learning with function approximation. In _Advances in Neural Information Processing Systems_, volume 99, pages 1057-1063. Citeseer, 1999.
* [27] Aviv Tamar, Yinlam Chow, Mohammad Ghavamzadeh, and Shie Mannor. Policy gradient for coherent risk measures. _Advances in neural information processing systems_, 28, 2015.
* [28] Aviv Tamar, Shie Mannor, and Huan Xu. Scaling up robust MDPs using function approximation. In _International Conference on Machine Learning_, pages 181-189. PMLR, 2014.
* [29] Chen Tessler, Yonathan Efroni, and Shie Mannor. Action robust reinforcement learning and applications in continuous control. In _International Conference on Machine Learning_, pages 6215-6224. PMLR, 2019.
* [30] Qiuhao Wang, Chin Pang Ho, and Marek Petrik. Policy gradient in robust MDPs with global convergence guarantee. In _International Conference on Machine Learning_, pages 35763-35797. PMLR, 2023.
* [31] Yue Wang, Fei Miao, and Shaofeng Zou. Robust constrained reinforcement learning. _arXiv preprint arXiv:2209.06866_, 2022.
* [32] Yue Wang and Shaofeng Zou. Policy gradient method for robust reinforcement learning. In _International Conference on Machine Learning_, pages 23484-23526. PMLR, 2022.
* [33] Wolfram Wiesemann, Daniel Kuhn, and Berc Rustem. Robust Markov decision processes. _Mathematics of Operations Research_, 38(1):153-183, 2013.
* [34] Huan Xu and Shie Mannor. Robustness and generalization. _Machine learning_, 86:391-423, 2012.

* [1] A Balanced and Normed Vectors A.1 Proof of Proposition 4.1
* [2] B Worst Kernel and Reward B.1 Proof of Theorem 4.2 B.2 Proof of Theorem 4.3
* [3] C Occupation Matrix C.1 Proof of Lemma 4.4 C.2 Proof of Theorem 4.6
* [4] D Robust Q-value D.1 Basic Properties D.2 Evaluation D.3 Convergence
* [5] E Robust Policy Gradient
* [6] F Complexity Analysis F.1 Helper results for Robust MDPs F.2 RPG Complexity
* [7] G Generalization to arbitrary norms G.1 \((s,a)\)-rectangular robust MDPs G.2 \(s\)-rectangular case G.3 Generalization to metrics and divergences
* [8] H Experiments H.1 RPG by LPBalanced and Normed Vectors

In this section, we lay down some basic properties of \(p\)-normalized-balanced vectors.

First recall the \(p\)-variance and the \(p\)-mean defined as:

\[\kappa_{p}(v)=\min_{\omega\in\mathbb{R}}\lVert v-\omega\mathbf{1}\rVert_{p}, \qquad\omega_{p}(v)=\operatorname*{arg\,min}_{\omega\in\mathbb{R}}\lVert v- \omega\mathbf{1}\rVert_{p}.\]

Given any \(v\in\mathbb{R}^{\mathcal{S}}\), let also the \(p\)-balanced-normalized function:

\[u_{p}(v)(s):=\textsc{sign}(v(s)-\omega_{p}(v))\left(\frac{|v(s)-\omega_{p}(v)| }{\kappa_{p}(v)}\right)^{p-1},\quad\forall v\in\mathbb{R}^{\mathcal{S}},s\in \mathcal{S}\,.\]

According to [14][Sec. 16.1, Lemma 1], the following holds:

\[\kappa_{q}(v)=-\frac{1}{\epsilon}\left[\min_{\lVert c\rVert_{p}\leq\epsilon, (c,\mathbf{1})=0}\langle c,v\rangle\right],\] (6)

namely, the \(p\)-variance function is the optimal value of a linear optimization under kernel noise constraint. The result below further characterizes the solution to the above problem.

**Lemma A.1**.: _The vector defined as \(c^{*}:=-\epsilon u_{q}(v)\) is an optimal solution to the optimization problem_

\[\min_{\lVert c\rVert_{p}\leq\epsilon,(c,\mathbf{1})_{\mathcal{S}}=0}\langle c,v\rangle.\]

Proof.: It suffices to show that \(c^{*}\) satisfies both constraints \(\lVert c^{*}\rVert_{p}\leq\epsilon\) and \(\langle c^{*},\mathbf{1}\rangle_{\mathcal{S}}=0\), and that it reaches optimal value, i.e., \(-\frac{1}{\epsilon}\langle c^{*},v\rangle=\kappa_{q}(v)\). We thus compute:

\[\lVert c^{*}\rVert_{p} =\left(\sum_{s\in\mathcal{S}}\lvert c^{*}(s)\rvert^{p}\right)^{ \frac{1}{p}}\] \[=\left(\left(\frac{\epsilon}{\kappa_{q}(v)^{q-1}}\right)^{p}\sum_ {s\in\mathcal{S}}\lvert\left(\frac{|v(s)-\omega_{q}(v)|}{\kappa_{q}(v)}\right) ^{q-1}\rvert^{p}\right)^{\frac{1}{p}}\] \[=\frac{\epsilon}{\kappa_{q}(v)^{q-1}}\left(\sum_{s\in\mathcal{S} }\lvert v(s)-\omega_{q}(v)\rvert^{(q-1)p}\right)^{\frac{1}{p}}\] \[=\frac{\epsilon}{\kappa_{q}(v)^{q-1}}\left(\sum_{s\in\mathcal{S} }\lvert v(s)-\omega_{q}(v)\rvert^{q}\right)^{\frac{1}{p}} \text{(By assumption, }\frac{p+q}{pq}=1\text{)}\] \[=\frac{\epsilon}{\kappa_{q}(v)^{q-1}}\kappa_{q}(v)^{\frac{q}{p}} \text{(By definition, }\kappa_{q}(v)=\lVert v-\omega_{q}\mathbf{1}\rVert_{q}\text{)}\] \[=\epsilon, (\frac{q}{p}-(q-1)=\frac{q-pq+p}{p}=0)\]

so the norm constraint is satisfied. We check the noise constraint by computing:

\[\sum_{s\in\mathcal{S}}c^{*}(s) =\sum_{s\in\mathcal{S}}-\epsilon\textsc{sign}(v(s)-\omega_{q}(v) )\left(\frac{|v(s)-\omega_{q}(v)|}{\kappa_{q}(v)}\right)^{q-1}\] \[=\frac{-\epsilon}{\kappa_{q}(v)^{q-1}}\sum_{s\in\mathcal{S}} \textsc{sign}(v(s)-\omega_{q}(v))|v(s)-\omega_{q}(v)|^{q-1}.\]

Now, considering the real function \(\varphi:w\rightarrow\lVert v-w\mathbf{1}\rVert_{q}\) and taking its derivative, we remark the proportional relation:

\[\sum_{s\in\mathcal{S}}c^{*}(s)=C\cdot\varphi^{\prime}(\omega_{q}(v)),\]where \(C\in\mathbb{R}\) is the proportionality coefficient. By construction, \(\omega_{q}(v)\) is a minimizer of \(\varphi\), so we must have \(\varphi^{\prime}(\omega_{q}(v))=0\) and \(c^{*}\) satisfies the noise constraint.

We finally show that \(c^{*}\) reaches the optimal value:

\[-\frac{1}{\epsilon}\langle c^{*},v\rangle =-\frac{1}{\epsilon}\langle c^{*},v-\omega_{q}(v)\mathbf{1}\rangle (\langle c^{*},\mathbf{1}\rangle_{\mathcal{S}}=0)\] \[= \sum_{s\in\mathcal{S}}\frac{|v(s)-\omega_{q}(v)|^{q}}{\kappa_{q}( v)^{q-1}} \text{(Putting the value of $c^{*}$)}\] \[= \frac{\kappa_{q}(v)^{q}}{\kappa_{q}(v)^{q-1}} \text{(}\kappa_{q}(v)=\|v-\omega_{q}\mathbf{1}\|_{q}\text{)}\] \[= \kappa_{q}(v).\]

### Proof of Proposition4.1

**Proposition**.: _For any policy \(\pi\in\Pi\) and \(\ell_{p}\)-ball rectangular uncertainty set, the following holds:_

\[u_{\mathcal{U}}^{\pi} =\nabla_{v}\kappa_{q}(v)\Bigm{|}_{v=v_{\mathcal{U}}^{\pi}},\] \[\langle u_{\mathcal{U}}^{\pi},v_{\mathcal{U}}^{\pi}\rangle =\kappa_{q}(v_{\mathcal{U}}^{\pi}).\]

Proof.: The second claim directly follows from LemmaA.1 applied to \(v:=v_{\mathcal{U}}^{\pi}\), so that by optimality, \(\kappa_{q}(v_{\mathcal{U}}^{\pi})=\langle u_{\mathcal{U}}^{\pi},v_{\mathcal{ U}}^{\pi}\rangle\). For the first claim, we take the gradient of \(\kappa_{p}(v):=\min_{v\in\mathbb{R}}\|v-w\mathbf{1}\|_{p}\) w.r.t. \(v\) using the envelope theorem [19]. Then, the \(p\)-balanced-normalized vector \(u_{p}(v)\) is a sub-gradient of \(\kappa_{p}(v)\), that is,

\[u_{p}(v)=\nabla\kappa_{q}(v),\]

which we apply to \(v:=v_{\mathcal{U}}^{\pi}\). 

We have the additional properties below:

* The variance function \(\kappa_{q}\) is translation-invariant in all-ones directions, i.e., for all \(\omega\in\mathbb{R},\kappa_{q}(v)=\kappa_{q}(v+\omega\mathbf{1})\). As a result, \(\langle\nabla\kappa_{q}(v),\mathbf{1}\rangle_{\mathcal{S}}=0\).
* The balanced-normalized vector \(u_{p}(v)\) has unit norm, i.e., \(\|u_{p}(v)\|_{p}=1\) by LemmaA.1.

## Appendix B Worst Kernel and Reward

Here we present the proofs for the worst/adversarial kernel and reward function characterization.

### Proof of Theorem4.2

**Theorem** (\((s,a)\)-rectangular case).: _Given uncertainty set \(\mathcal{U}=\mathcal{U}_{p}^{\text{sa}}\) and any policy \(\pi\in\Pi\), the worst model is related to the nominal one through:_

\[R_{\mathcal{U}}^{\pi}(s,a)=R_{0}(s,a)-\alpha_{s,a}\qquad\text{and}\qquad P_{ \mathcal{U}}^{\pi}(\cdot|s,a)=P_{0}(\cdot|s,a)-\beta_{s,a}u_{\mathcal{U}}^{\pi}.\]

Proof.: By definition,

\[(P_{\mathcal{U}_{p}^{\text{sa}}}^{\pi},R_{\mathcal{U}_{p}^{\text{sa}}}^{\pi}) \in\operatorname*{arg\,min}_{(P,R)\in\mathcal{U}_{p}^{\text{sa}}}T_{(P,R)}^{ \pi}v_{\mathcal{U}_{p}^{\text{sa}}}^{\pi}.\]

Additionally, since \(\mathcal{U}_{p}^{\text{sa}}=(R_{0}+\mathcal{R})\times(P_{0}+\mathcal{P})\), it results that:

\[(R_{\mathcal{U}_{p}^{\text{sa}}}^{\pi},P_{\mathcal{U}_{p}^{\text{sa}}}^{\pi}) =(P_{0}+P^{*},R_{0}+R^{*})\]

where

\[(P^{*},R^{*})\in\operatorname*{arg\,min}_{(P,R)\in\mathcal{P}\times\mathcal{ R}}T_{(P,R)}^{\pi}v_{\mathcal{U}_{p}^{\text{sa}}}^{\pi}.\]By the \((s,a)\)-rectangularity assumption, we get that for all \((s,a)\in\mathcal{X}\),

\[(P^{*}(\cdot|s,a),R^{*}(s,a))\in\operatorname*{arg\,min}_{(p_{s,a},r_{s,a})\in \mathcal{P}_{s,a}\times\mathcal{R}_{s,a}}\left\{r_{s,a}+\gamma\sum_{s^{\prime} \in\mathcal{S}}p_{s,a}(s^{\prime})v_{\mathcal{U}^{\pi}_{p}}^{\pi}(s^{\prime})\right\}\]

It is clear from the above that the worst reward is independent of policy \(\pi\). Thus, by the ball constraint, it is given by

\[R^{*}(s,a)=-\alpha_{s,a},\quad\forall(s,a)\in\mathcal{X}\,.\]

Differently, the worst kernel depends on the value function which itself depends on the policy. It is given by

\[P^{*}(\cdot|s,a)=\operatorname*{arg\,min}_{p_{s,a}\in\mathcal{P}_{sa}}\left\{ \sum_{s^{\prime}\in\mathcal{S}}p_{s,a}(s^{\prime})v_{\mathcal{U}^{\pi}_{p}}^{ \pi}(s^{\prime})\right\},\quad\forall(s,a)\in\mathcal{X}\,.\]

The optimization is of the form

\[\operatorname*{arg\,min}_{\|c\|_{p}\leq\beta,\langle c,\mathbf{1}\rangle=0} \langle c,v\rangle,\]

so by Lemma A.1,

\[P^{*}(s^{\prime}|s,a)=-\beta_{s,a}\textsc{sign}\left(v_{\mathcal{U}^{\pi}_{p} }^{\pi}(s^{\prime})-\omega_{q}(v_{\mathcal{U}^{\pi}_{p}}^{\pi})\right)\frac{ \left|v_{\mathcal{U}^{\pi}_{p}}^{\pi}(s^{\prime})-\omega_{q}(v_{\mathcal{U}^{ \pi}_{p}}^{\pi})\right|^{q-1}}{\kappa_{q}(v)^{q-1}}.\]

As a result, we proved that for all \((s,a)\in\mathcal{X}\), \(R^{\pi}_{\mathcal{U}^{\pi}_{p}}(s,a)=R_{0}(s,a)-\alpha_{s,a}\) and

\[P^{\pi}_{\mathcal{U}^{\pi}_{p}}(s^{\prime}|s,a)=P_{0}(s^{\prime}|s,a)-\beta_ {s,a}\textsc{sign}\left(v_{\mathcal{U}^{\pi}_{p}}^{\pi}(s^{\prime})-\omega_{ q}(v_{\mathcal{U}^{\pi}_{p}}^{\pi})\right)\frac{\left|v_{\mathcal{U}^{\pi}_{p}}^{ \pi}(s^{\prime})-\omega_{q}(v_{\mathcal{U}^{\pi}_{p}}^{\pi})\right|^{q-1}}{ \kappa_{q}(v)^{q-1}}.\]

### Proof of Theorem 4.3

**Theorem** (\(s\)-rectangular case).: _Given uncertainty set \(\mathcal{U}=\mathcal{U}^{s}_{p}\) and any policy \(\pi\in\Pi\), the worst model is related to the nominal one through:_

\[R^{\pi}_{\mathcal{U}}(s,a)=R_{0}(s,a)-\alpha_{s}\left(\frac{\pi_{s}(a)}{\|\pi _{s}\|_{q}}\right)^{q-1}\quad\text{and}\quad P^{\pi}_{\mathcal{U}}(\cdot|s,a) =P_{0}(\cdot|s,a)-\beta_{s}u_{\mathcal{U}}^{\pi}\left(\frac{\pi_{s}(a)}{\|\pi _{s}\|_{q}}\right)^{q-1}.\]

Proof.: By definition,

\[(P^{\pi}_{\mathcal{U}^{\pi}_{p}},R^{\pi}_{\mathcal{U}^{\pi}_{p}})\in \operatorname*{arg\,min}_{(P,R)\in\mathcal{U}^{\pi}_{p}}T^{\pi}_{(P,R)}v_{ \mathcal{U}^{\pi}_{p}}^{\pi},\]

and since \(\mathcal{U}^{\pi}_{p}=(R_{0}+\mathcal{R})\times(P_{0}+\mathcal{P})\), we have

\[(P^{\pi}_{\mathcal{U}^{\pi}_{p}},R^{\pi}_{\mathcal{U}^{\pi}_{p}})=(P_{0}+P^{*}, R_{0}+R^{*})\]

where

\[(P^{*},R^{*})\in\operatorname*{arg\,min}_{(P,R)\in\mathcal{P}\times\mathcal{R} }T^{\pi}_{(P,R)}v_{\mathcal{U}^{\pi}_{p}}^{\pi_{\text{sa}}}.\]

By the \(s\)-rectangularity assumption, we get that for all \(s\in\mathcal{S}\)

\[(P^{*}(\cdot|s,\cdot),R^{*}(s,\cdot))=\operatorname*{arg\,min}_{(p_{s},r_{s}) \in\mathcal{P}_{s}\times\mathcal{R}_{s}}\sum_{a\in\mathcal{A}}\pi_{s}(a) \left\{r_{s,a}+\gamma\sum_{s^{\prime}\in\mathcal{S}}p_{s,a}(s^{\prime})v_{ \mathcal{U}^{\pi}_{p}}^{\pi}(s^{\prime})\right\}.\]

Here, the worst reward does depend on policy \(\pi\) and is given by

\[R^{*}(s,a)=-\alpha_{s}\frac{\pi_{s}(a)^{q-1}}{\sum_{a}\pi_{s}(a)^{q-1}},\qquad \forall(s,a)\in\mathcal{X}\,.\]As for the worst kernel, it depends both on the value function and the policy. It is given by

\[P^{*}(\cdot|s,\cdot)= \operatorname*{arg\,min}_{p_{s}\in\mathcal{P}_{s}}\left\{\sum_{a\in \mathcal{A}}\pi_{s}(a)\sum_{s^{\prime}\in\mathcal{S}}p_{s,a}(s^{\prime})v_{ \mathcal{U}_{\mathcal{H}_{p}^{*}}^{*}}^{*}(s^{\prime})\right\}.\]

The optimization of interest is of the form

\[\min_{\|c_{a}\|_{p}\leq\beta_{s},(c_{a},\mathbf{1})=0,a\in\mathcal{A}}\left\{ \sum_{a^{\prime}\in\mathcal{A}}\pi_{s}(a^{\prime})\langle c_{a^{\prime}},v \rangle\right\},\]

which is equivalent to the following two-fold minimization:

\[\min_{\sum_{a\in A}(\beta_{s,a})^{p}\leq(\beta_{s})^{p}}\min_{\|c_{a}\|_{p} \leq\beta_{s},(c_{a},\mathbf{1})=0,a\in\mathcal{A}}\left\{\sum_{a^{\prime}\in \mathcal{A}}\pi_{s}(a^{\prime})\langle c_{a^{\prime}},v\rangle\right\}.\]

Thus, rewriting the problem in our context,

\[\min_{\sum_{a}(\beta_{s,a})^{p}\leq(\beta_{s})^{p}}\min_{\|p_{sa} \|_{p}\leq\beta_{s,a},\sum_{s^{\prime}}p_{sa}(s^{\prime})=0}\quad\sum_{a}\pi_{ s}(a)\langle p_{s,a},v\rangle\] \[= \min_{\sum_{a}(\beta_{s,a})^{p}\leq(\beta_{s})^{p}}\sum_{a}\pi_{ s}(a)\quad\min_{\|p_{sa}\|_{p}\leq\beta_{s,a},\sum_{s^{\prime}}p_{sa}(s^{\prime})=0} \quad\langle p_{s,a},v\rangle\] \[= \min_{\sum_{a}(\beta_{sa})^{p}\leq(\beta_{s})^{p}}\sum_{a}\pi_{ s}(a)(-\beta_{sa}\kappa_{q}(v))\] (By Lemma A.1) \[= -\kappa_{q}(v)\max_{\sum_{a}(\beta_{sa})^{p}\leq(\beta_{s})^{p}} \sum_{a}\pi_{s}(a)\beta_{sa}.\]

Computing the optimal \(\beta\) above, the optimization is now the same as in the \((s,a)\)-rectangular case. Hence, we have

\[P^{*}(s^{\prime}|s,a)=-\beta_{s}\frac{\pi_{s}(a)^{q-1}}{\|\pi_{s}\|_{q}^{q-1}} \textsc{sign}(v_{\mathcal{U}_{\mathcal{H}_{p}^{*}}^{*}}^{*}(s^{\prime})- \omega_{q}(v_{\mathcal{U}_{\mathcal{H}_{p}^{*}}^{*}}^{*}))\frac{\left|v_{ \mathcal{U}_{\mathcal{H}_{p}^{*}}^{*}}^{*}(s^{\prime})-\omega_{q}(v_{\mathcal{ U}_{\mathcal{H}_{p}^{*}}^{*}}^{*})\right|^{q-1}}{\kappa_{q}(v)^{q-1}},\]

which ends the proof by definition of the balanced value function \(u_{\mathcal{U}}^{*}\). 

## Appendix C Occupation Matrix

### Proof of Lemma 4.4

**Lemma**.: _Let \(b,k\in\mathbb{R}^{\mathcal{S}}\) and \(P_{0},P_{1}\in(\Delta_{\mathcal{S}})^{\mathcal{S}}\) two transition matrices. If \(P_{1}=P_{0}-bk^{\top}\), i.e., \(P_{1}\) is a rank-one perturbation of \(P_{0}\), then their occupation matrices \(D_{i}:=(I-\gamma P_{i})^{-1},i=0,1\) are related through:_

\[D_{1}=D_{0}-\gamma\frac{D_{0}bk^{\top}D_{0}}{(1+\gamma k^{\top}D_{0}b)}.\]

Proof.: By definition, \(D_{1}=(I_{\mathcal{S}}-\gamma P_{1})^{-1}\) so it follows that:

\[(I_{\mathcal{S}}-\gamma P_{1})D_{1}=I_{\mathcal{S}}\] \[\iff I_{\mathcal{S}}+\gamma P_{1}D_{1}=D_{1}\] \[\iff I_{\mathcal{S}}+\gamma(P_{0}-bk^{\top})D_{1}=D_{1}\] (By assumption, \[P_{1}=P_{0}-bk^{\top}\] ) \[\iff I_{\mathcal{S}}-\gamma bk^{\top}D_{1}=(I_{\mathcal{S}}-\gamma P_{0})D _{1}\] \[\iff (I_{\mathcal{S}}-\gamma P_{0})^{-1}(I_{\mathcal{S}}-\gamma bk^{ \top}D_{1})=D_{1}\] (Multiplying both sides by \[(I_{\mathcal{S}}-\gamma P_{0})^{-1}\] ) \[\iff D_{0}(I_{\mathcal{S}}-\gamma bk^{\top}D_{1})=D_{1}\] (By definition, \[D_{0}=(I_{\mathcal{S}}-\gamma P_{0})^{-1}\] ) \[\iff D_{0}-\gamma D_{0}bk^{\top}D_{1}=D_{1}.\] (7)Now, multiplying both sides by \(k\) and noticing that \(k^{\top}D_{0}b\) is a scalar we get

\[k^{\top}D_{0}-\gamma k^{\top}D_{0}bk^{\top}D_{1}=k^{\top}D_{1}\] \[\Longleftrightarrow k^{\top}D_{0}=(1+\gamma k^{\top}D_{0}b)k^{ \top}D_{1}\] \[\Longleftrightarrow k^{\top}D_{1}=\frac{k^{\top}D_{0}}{(1+ \gamma k^{\top}D_{0}b)}.\] (8)

Combining Eqs. (7) and (8) thus yields:

\[D_{1}=D_{0}-\gamma\frac{D_{0}bk^{\top}D_{0}}{(1+\gamma k^{\top}D_{0}b)},\]

which concludes the proof. 

### Proof of Theorem 4.6

**Theorem**.: _For any rectangular \(\ell_{p}\)-ball-constrained uncertainty and \(\pi\in\Pi\), it holds that:_

\[d^{\pi}_{\mathcal{U},\mu}=d^{\pi}_{P_{0},\mu}-\gamma\frac{\langle d^{\pi}_{P_ {0},\mu},\beta^{\pi}\rangle_{\mathcal{S}}}{1+\gamma\langle d^{\pi}_{P_{0},u^ {\pi}_{\mathcal{U}}},\beta^{\pi}\rangle_{\mathcal{S}}}d^{\pi}_{P_{0},u^{\pi}_ {\mathcal{U}}}.\]

Proof.: From Thms. 4.2 and 4.3, it holds that:

\[P^{\pi}_{\mathcal{U}}(s^{\prime}|s)=P^{\pi}_{0}(s^{\prime}|s)-\beta^{\pi}_{s} u^{\pi}_{\mathcal{U}}(s^{\prime}),\quad\forall s,s^{\prime}\in\mathcal{S}\,.\]

Therefore, setting \(P_{1}:=P^{\pi}_{\mathcal{U}}\), \(P_{0}:=P^{\pi}_{0}\), \(b:=\beta^{\pi}\) and \(k:=u^{\pi}_{\mathcal{U}}\), we can apply Lemma 4.4 and relate the corresponding occupation matrices. Additionally multiplying both sides of the relation by \(\mu^{\top}\in\mathbb{R}^{1\times\mathcal{S}}\) yields the desired result. 

## Appendix D Robust Q-value

### Basic Properties

In the literature, robust Q-values are defined in various ways that turn out to be conflicting for \(s\) but non-\((s,a)\) rectangular uncertainty sets. In this section, we propose to define the robust Q-value solely based on the worst model. Define the robust Q-value, the robust value function, and the robust occupation respectively as:

\[Q^{\pi}_{\mathcal{U}}:=Q^{\pi}_{(P^{\pi}_{\mathcal{U}},R^{\pi}_{\mathcal{U}}) },\quad d^{\pi}_{\mathcal{U}}:=d^{\pi}_{(P^{\pi}_{\mathcal{U}},R^{\pi}_{ \mathcal{U}})},\quad v^{\pi}_{\mathcal{U}}:=v^{\pi}_{(P^{\pi}_{\mathcal{U}},R^ {\pi}_{\mathcal{U}})}.\]

For \(s\)-rectangular uncertainty sets (in particular, for \((s,a)\)-rectangular), the above definition of robust value function coincides with the common one, i.e., \(v^{\pi}_{(P^{\pi}_{\mathcal{U}},R^{\pi}_{\mathcal{U}})}=\min_{(P,R)\in \mathcal{U}}v^{\pi}_{(P,R)}\)[33]. If the uncertainty set is additionally \((s,a)\)-rectangular (as in [31] or [3, 14]), the above definition of robust Q-value also coincides with the common one because then,

\[Q^{\pi}_{\mathcal{U}^{\text{ex}}}(s,a)=\min_{(P,R)\in\mathcal{U}^{\text{ex}}} \left(R(s,a)+\gamma\sum_{s^{\prime}\in\mathcal{S}}P(s^{\prime}|s,a)v^{\pi}_{ \mathcal{U}^{\text{ex}}}(s^{\prime})\right),\quad\forall(s,a)\in\mathcal{X}\,.\]

Getting back to our own definition, robust Q-value and value functions are related through:

\[v^{\pi}_{\mathcal{U}}(s) =\langle\pi_{s},Q^{\pi}_{\mathcal{U}}(s,\cdot)\rangle_{\mathcal{ A}}\] \[Q^{\pi}_{\mathcal{U}}(s,a) =R^{\pi}_{\mathcal{U}}(s,a)+\gamma\sum_{s^{\prime}\in\mathcal{S} }\pi_{s}(a)P^{\pi}_{\mathcal{U}}(s^{\prime}|s,a)v^{\pi}_{\mathcal{U}}(s^{ \prime}),\]

as both quantities are defined based on worst kernel and reward, i.e., \(Q^{\pi}_{\mathcal{U}}:=Q^{\pi}_{(P^{\pi}_{\mathcal{U}},R^{\pi}_{\mathcal{U}})}\) and \(v^{\pi}_{\mathcal{U}}:=v^{\pi}_{(P^{\pi}_{\mathcal{U}},R^{\pi}_{\mathcal{U}})}\).

Given optimal robust policy \(\pi^{*}_{\mathcal{U}}\), we further use \(P^{*}_{\mathcal{U}},R^{*}_{\mathcal{U}},v^{*}_{\mathcal{U}},Q^{*}_{\mathcal{U} },d^{\pi}_{\mathcal{U}}\) as a shorthand for \(P^{\pi^{*}_{\mathcal{U}}}_{\mathcal{U}},R^{\pi^{*}_{\mathcal{U}}}_{\mathcal{U} },v^{\pi^{*}_{\mathcal{U}}}_{\mathcal{U}},Q^{\pi^{*}_{\mathcal{U}}}_{ \mathcal{U}},d^{\pi^{*}_{\mathcal{U}}}_{\mathcal{U}}\) respectively. For \((s,a)\)-rectangular uncertainty set \(\mathcal{U}^{\text{ex}}\), the optimal value function is the best optimal Q-value, that is

\[v^{*}_{\mathcal{U}^{\text{ex}}}(s)=\max_{a\in\mathcal{A}}Q^{*}_{\mathcal{U}^{ \text{ex}}}(s,a),\quad\forall s\in\mathcal{S}\,.\]because an optimal policy deterministically takes the action with the highest Q-value [20; 12]. This does no longer hold for \(s\)-rectangular or coupled uncertainty sets, as there, an optimal policy may be stochastic [33]. Still, based on Thms. 4.2 and 4.3, we get the Bellman recursion below.

**Proposition D.1**.: _Let an \(\ell_{p}\)-ball constrained uncertainty set. Then, for all \((s,a)\in\mathcal{X}\) and \(\pi\in\Pi\), the robust Q-value satisfies the following recursion in the \((s,a)\) and \(s\)-rectangular case respectively:_

\[Q^{\pi}_{\mathcal{U}^{\pi}_{p}}(s,a)= T^{\pi}_{(p_{0},R_{0})}Q^{\pi}_{\mathcal{U}^{\pi}_{p}}(s,a)- \alpha_{sa}-\gamma\beta_{sa}\kappa_{q}(v^{\pi}_{\mathcal{U}^{\pi}_{p}}),\] \[Q^{\pi}_{\mathcal{U}^{\pi}_{p}}(s,a)= T^{\pi}_{(p_{0},R_{0})}Q^{\pi}_{\mathcal{U}^{\pi}_{p}}(s,a)-\left( \frac{\pi_{s}(a)}{\|\pi_{s}\|_{q}}\right)^{q-1}\left(\alpha_{s}+\gamma\beta_ {s}\kappa_{q}(v^{\pi}_{\mathcal{U}^{\pi}_{p}})\right).\]

Proof.: We give proof for the \((s,a)\)-rectangular case only. The \(s\)-rectangular case follows the exact same lines except that it uses Thm. 4.3 instead of Thm. 4.2. We have:

\[Q^{\pi}_{\mathcal{U}}(s,a) =Q^{\pi}_{(\mathcal{U}^{\pi}_{\mathcal{U}^{\pi}_{p}},R^{\pi}_{ \mathcal{U}})}(s,a)\] (By definition) \[=R^{\pi}_{\mathcal{U}}(s,a)+\sum_{s^{\prime}\in\mathcal{S}}P^{\pi }_{\mathcal{U}}(s^{\prime}|s,a)v^{\pi}_{\mathcal{U}^{\pi}_{p}}(s^{\prime})\] \[=R_{0}(s,a)-\alpha_{sa}+\gamma\sum_{s^{\prime}\in\mathcal{S}} \left(P_{0}(s^{\prime}|s,a)-\beta_{sa}u^{\pi}_{\mathcal{U}^{\pi}_{p}}(s^{ \prime})\right)v^{\pi}_{\mathcal{U}^{\pi}_{p}}(s^{\prime})\] (By Thm. 4.2 ) \[=R_{0}(s,a)-\alpha_{sa}+\gamma\sum_{s^{\prime}\in\mathcal{S}}P_{0 }(s^{\prime}|s,a)v^{\pi}_{\mathcal{U}^{\pi}_{p}}(s^{\prime})-\gamma\beta_{sa} \kappa_{q}(v^{\pi}_{\mathcal{U}^{\pi}_{p}})\] (2d statement of Prop. 4.1 ) \[=R_{0}(s,a)+\gamma\sum_{s^{\prime},a^{\prime}}P_{0}(s^{\prime}|s,a)\pi_{s^{\prime}}(a^{\prime})Q^{\pi}_{\mathcal{U}^{\pi}_{p}}(s^{\prime},a^{ \prime})-\alpha_{sa}-\gamma\beta_{sa}\kappa_{q}(v^{\pi}_{\mathcal{U}^{\pi}_{p} })\] \[=T^{\pi}_{(p_{0},R_{0})}Q^{\pi}_{\mathcal{U}^{\pi}_{p}}(s,a)- \alpha_{sa}-\gamma\beta_{sa}\kappa_{q}(v^{\pi}_{\mathcal{U}^{\pi}_{p}}).\]

The above recursion applies the standard Bellman operator on robust Q-values. We can similarly apply it on the robust value function (itself can be computed efficiently based on [3; 14]).

**Corollary D.2**.: _Let an \(\ell_{p}\)-ball constrained uncertainty set. Then, for all \((s,a)\in\mathcal{X}\) and \(\pi\in\Pi\), the robust Q-value satisfies the following recursion in the \((s,a)\) and \(s\)-rectangular case respectively:_

\[Q^{\pi}_{\mathcal{U}^{\pi}_{p}}(s,a) =R_{0}(s,a)+\gamma\sum_{s^{\prime}}P_{0}(s^{\prime}|s,a)v^{\pi}_ {\mathcal{U}^{\pi}_{p}}(s^{\prime})-\alpha_{sa}-\gamma\beta_{sa}\kappa_{q}(v^{ \pi}_{\mathcal{U}^{\pi}_{p}}),\] \[Q^{\pi}_{\mathcal{U}^{\pi}_{p}}(s,a) =R_{0}(s,a)+\gamma\sum_{s^{\prime}}P_{0}(s^{\prime}|s,a)v^{\pi}_ {\mathcal{U}^{\pi}_{p}}(s^{\prime})-\left(\frac{\pi_{s}(a)}{\|\pi_{s}\|_{q}} \right)^{q-1}\left(\alpha_{s}+\gamma\beta_{s}\kappa_{q}(v^{\pi}_{\mathcal{U}^ {\pi}_{p}})\right).\]

### Evaluation

Based on the Bellman recursion above, we now derive robust Q-learning equations to learn a robust Q-value. Precisely, we investigate if the linear operator below is contracting and can be evaluated efficiently:

\[(\mathcal{L}^{\pi}_{\mathcal{U}}Q)(s,a):=R^{\pi}_{\mathcal{U},v}(s,a)+\gamma \sum_{(s^{\prime},a^{\prime})\in\mathcal{X}}P^{\pi}_{\mathcal{U},v}(s^{\prime }|s,a)\pi_{s^{\prime}}(a^{\prime})Q(s^{\prime},a^{\prime}),\quad\forall Q\in \mathbb{R}^{\mathcal{X}},\] (9)

where \((P^{\pi}_{\mathcal{U},v},R^{\pi}_{\mathcal{U},v})\in\arg\min_{(P,R)\in\mathcal{ U}}T^{\pi}_{(P,R)}v\) and \(v(s)=\langle\pi_{s},Q(s,\cdot)\rangle_{\mathcal{A}},\quad\forall s\in\mathcal{ S}\).

**Proposition D.3**.: _Consider an \(\ell_{p}\)-ball constrained uncertainty set. Then, for all \(Q\in\mathbb{R}^{\mathcal{X}}\) and \(\pi\in\Pi\), the operator \(\mathcal{L}^{\pi}\) can be evaluated as:_

\[(\mathcal{L}^{\pi}_{\mathcal{U}^{\pi}_{p}}Q)(s,a) =T^{\pi}_{(p_{0},R_{0})}Q(s,a)-\alpha_{sa}-\gamma\beta_{sa}\kappa _{q}(v),\] \[(\mathcal{L}^{\pi}_{\mathcal{U}^{\pi}_{p}}Q)(s,a) =T^{\pi}_{(p_{0},R_{0})}Q(s,a)-\left(\frac{\pi_{s}(a)}{\|\pi_{s} \|_{q}}\right)^{q-1}\left(\alpha_{s}+\gamma\beta_{s}\kappa_{q}(v)\right),\]

_where for all \(Q\in\mathbb{R}^{\mathcal{X}}\), its corresponding value is \(v(s):=\langle\pi_{s},Q(s,\cdot)\rangle,\quad\forall s\in\mathcal{S}\)._Proof.: We give proof for the \((s,a)\)-rectangular case only. The \(s\)-rectangular case follows the exact same lines except that we take the worst model for \(s\)-rectangular balls. By definition,

\[(\mathcal{L}^{\pi}_{\mathcal{U}^{\text{aa}}_{p}}Q)(s,a) =\min_{(P,R)\in\mathcal{U}^{\text{aa}}_{p}}\left\{R(s,a)+\gamma \sum_{(s^{\prime},a^{\prime})\in\mathcal{X}}P(s^{\prime}|s,a)\pi_{s^{\prime}}( a^{\prime})Q(s^{\prime},a^{\prime})\right\}\] \[=\min_{R\in\mathcal{R}^{\pi}_{p}}R(s,a)+\gamma\min_{P\in\mathcal{ P}^{\pi}_{p}}\left\{\sum_{s^{\prime}\in\mathcal{S}}P(s^{\prime}|s,a)v(s^{ \prime})\right\}\] \[=R_{0}(s,a)-\alpha_{s,a}+\gamma\sum_{s^{\prime}\in\mathcal{S}}P_ {0}(s^{\prime}|s,a)v(s^{\prime})-\beta_{s,a}\kappa_{q}(v)\] (By [14]) \[=R_{0}(s,a)-\alpha_{s,a}+\gamma\sum_{(s^{\prime},a^{\prime})\in \mathcal{X}}P_{0}(s^{\prime}|s,a)\pi_{s^{\prime}}(a^{\prime})Q(s^{\prime},a^{ \prime})-\beta_{s,a}\kappa_{q}(v)\] \[=T^{\pi}_{{(P_{0},R_{0})}}Q(s,a)-\alpha_{s,a}-\beta_{s,a}\kappa_{ q}(v).\]

### Convergence

In the remainder of this section, we focus on \(\ell_{p}\)-ball constrained uncertainty sets of the form \(\mathcal{U}^{\text{aa}}_{p}\) or \(\mathcal{U}^{\text{s}}_{p}\). Let our Q-value iteration \(Q_{n+1}:=\mathcal{L}^{\pi}_{\mathcal{U}}Q_{n}\), and denote \(v_{n}(s)=\langle\pi_{s},Q_{n}(s,\cdot)\rangle_{\mathcal{A}},\forall s\in \mathcal{S},n\in\mathbb{N}\).

**Proposition D.4**.: _For all \(Q\in\mathbb{R}^{\mathcal{X}}\), denote \(v(s):=\langle\pi_{s},Q(s,\cdot)\rangle_{\mathcal{A}},\forall s\in\mathcal{S}\). Then, for any policy \(\pi\in\Pi\), the Q-value iteration defined according to \(Q_{n+1}=\mathcal{L}^{\pi}_{\mathcal{U}}Q_{n}\) induces_

\[v_{n+1}:=\mathcal{T}^{\pi}_{\mathcal{U}}v_{n}.\]

Proof.: By construction, for all \(s\in\mathcal{S}\) we have

\[v_{n+1}(s) =\langle\pi_{s},Q_{n+1}(s,\cdot)\rangle_{\mathcal{A}}\] \[=\langle\pi_{s},(\mathcal{L}^{\pi}_{\mathcal{U}}Q_{n})(s,\cdot) \rangle_{\mathcal{A}}\] \[=\sum_{a\in\mathcal{A}}\pi_{s}(a)\left[R^{\pi}_{\mathcal{U},v_{n} }(s,a)+\gamma\sum_{(s^{\prime},a^{\prime})\in\mathcal{X}}P^{\pi}_{\mathcal{U},v_{n}}(s^{\prime}|s,a)\pi_{s^{\prime}}(a^{\prime})Q_{n}(s^{\prime},a^{\prime})\right]\] (By Eq. 9) \[=\sum_{a\in\mathcal{A}}\pi_{s}(a)\left[R^{\pi}_{\mathcal{U},v_{n} }(s,a)+\gamma\sum_{s^{\prime}\in\mathcal{S}}P^{\pi}_{\mathcal{U},v_{n}}(s^{ \prime}|s,a)v_{n}(s^{\prime})\right]\] (By definition of

\[v_{n}\]

) \[=(\mathcal{T}^{\pi}_{\mathcal{U}}v_{n})(s),\]

where the last equality holds because \((P^{\pi}_{\mathcal{U},v_{n}},R^{\pi}_{\mathcal{U},v_{n}})\in\arg\min_{(P,R)\in \mathcal{U}}\mathcal{T}^{\pi}_{{(P,R)}}v_{n}\). 

As a result of the above proposition, the value iteration induced by our Q-value iteration rule converges linearly to the robust value function, i.e., \(\|v_{n}-v^{\pi}_{\mathcal{U}}\|_{\infty}\leq\gamma^{n}\|v_{0}\|_{\infty}\). Therefore, Q-value iterates converge to a fixed point. Precisely, \(v_{n}\to_{n}v^{\pi}_{\mathcal{U}}\) implies that \((P^{\pi}_{\mathcal{U},v_{n}},R^{\pi}_{\mathcal{U},v_{n}})\to_{n}(P^{\pi}_{ \mathcal{U}},R^{\pi}_{\mathcal{U}})\), which in turn implies that \(Q_{n}\to_{n}Q^{\pi}_{\mathcal{U}}\). The result below further characterizes the convergence rate.

**Proposition D.5**.: _For any policy \(\pi\in\Pi\), the recursion \(Q_{n+1}:=\mathcal{L}^{\pi}_{\mathcal{U}}Q_{n},n\in\mathbb{N}\) converges linearly to \(Q^{\pi}_{\mathcal{U}}\)._

Proof.: Thanks to Prop. D.4, we have:

\[\|Q_{n+1}-Q^{\pi}_{\mathcal{U}}\|_{\infty}= \|R^{\pi}_{\mathcal{U},v_{n}}+\gamma P^{\pi}_{\mathcal{U},v_{n}}v_ {n}-R^{\pi}_{\mathcal{U}}+\gamma P^{\pi}_{\mathcal{U}}v^{\pi}_{\mathcal{U}}\|_ {\infty}\] ( \[R^{\pi}_{\mathcal{U},v}=R^{\pi}_{\mathcal{U}},\quad\forall v\] \[= \gamma\|(P_{0}-B^{\pi}u_{n})v_{n}-(P_{0}-B^{\pi}u^{\pi}_{\mathcal{U} })v^{\pi}_{\mathcal{U}}\|_{\infty},\]where by the worst kernel characterization, \(B^{\pi}(s,a):=\beta_{s,a}\) for \(\mathcal{U}=\mathcal{U}_{p}^{\text{sa}}\) and \(B^{\pi}(s,a):=\beta_{s}\left(\frac{\pi_{s}(a)}{\|\pi_{s}\|_{a}}\right)^{q-1}\) for \(\mathcal{U}=\mathcal{U}_{p}^{\text{sa}}\). This implies

\[\|Q_{n+1}-Q_{\mathcal{U}}^{\pi}\|_{\infty}\leq \gamma\|P_{0}(v_{n}-v_{\mathcal{U}}^{\pi})\|_{\infty}+\gamma\|B^{ \pi}(u_{n})^{\top}v_{n}-B^{\pi}(u_{\mathcal{U}}^{\pi})^{\top}v_{\mathcal{U}}^{ \pi}\|_{\infty}\] \[\leq \gamma^{n+1}\|v_{0}-v_{\mathcal{U}}^{\pi}\|_{\infty}+\gamma\|(u_ {n})^{\top}v_{n}-(u_{\mathcal{U}}^{\pi})^{\top}v_{\mathcal{U}}^{\pi}\|\] ( \[B^{\pi}(s,a)\leq 1\] ), \[\leq \gamma^{n+1}\|v_{0}-v_{\mathcal{U}}^{\pi}\|_{\infty}+\gamma\|(u_ {n})^{\top}(v_{n}-v_{\mathcal{U}}^{\pi})\|+\gamma\|(u_{n}-u_{\mathcal{U}}^{ \pi})^{\top}v_{\mathcal{U}}^{\pi}\|\] \[\leq \gamma^{n+1}\|v_{0}-v_{\mathcal{U}}^{\pi}\|_{\infty}+\gamma^{n+1} S\|v_{0}-v_{\mathcal{U}}^{\pi}\|_{\infty}+\gamma\frac{\|\mathcal{R}\|_{\infty}}{1- \gamma}\|u_{n}-u_{\mathcal{U}}^{\pi}\|_{\infty}.\]

Here, \(u_{n},u_{\mathcal{U}}^{\pi}\) is the balanced-normalized vector associated with vector \(v_{n}\) and \(v_{\mathcal{U}}^{\pi}\), respectively. Recall that the \(p\)-balanced normalized vector \(u\) associated with vector \(v\) is given by

\[u(s):=\frac{\text{sign}(v(s)-\omega_{q}(v)\|v(s)-\omega_{q}(v)\|^{q-1}}{\kappa_ {q}(v)^{q-1}},\] (10)

where \(\kappa_{p}(v)=\min_{w}\|v-w\|_{p}\) and \(\omega_{p}(v)=_{w}\|v-w\|_{p}\). It is easy to see that \(\omega_{p},\kappa\) are Lipschitz in \(v\). Hence, \(\|u_{n}-u_{\mathcal{U}}^{\pi}\|_{\infty}\leq C\cdot\text{Pol}(\|v_{n}-v_{ \mathcal{U}}^{\pi}\|_{\infty})\cdot\psi(\kappa(v_{\mathcal{U}}^{\pi}),S,A)\), where Pol is a polynomial and \(\psi\) a real function. This implies that

\[\|Q_{n+1}-Q_{\mathcal{U}}^{\pi}\|_{\infty}\leq\gamma^{n+1}\psi^{\prime}( \kappa(v_{\mathcal{U}}^{\pi}),\|v_{0}-v_{\mathcal{U}}^{\pi}\|_{\infty},S,A),\]

which concludes our proof. 

## Appendix E Robust Policy Gradient

**Theorem** (RPG).: _For any rectangular \(\ell_{p}\)-ball-constrained uncertainty, the robust policy gradient is given by:_

\[\partial_{\pi}\rho_{\mathcal{U}}^{\pi}=\sum_{(s,a)\in\mathcal{X}}\left(d_{ \tilde{P}_{0},\mu}^{\pi}(s)-c^{\pi}(s)\right)Q_{\mathcal{U}}^{\pi}(s,a)\nabla \pi_{s}(a),\] (11)

_where_

\[c^{\pi}(s):=\frac{\gamma\langle d_{\tilde{P}_{0},\mu}^{\pi},\beta^{\pi} \rangle_{\mathcal{S}}}{1+\gamma\langle d_{\tilde{P}_{0},u_{\mathcal{U}}^{\pi }}^{\pi},\beta^{\pi}\rangle_{\mathcal{S}}}d_{\tilde{P}_{0},u_{\mathcal{U}}^{ \pi}}^{\pi}(s),\quad\forall s\in\mathcal{S}\,.\]

Proof.: The proof directly follows from plugging the robust occupation measure of Thm. 4.6 and the robust Q-value into standard policy-gradient theorem [26]. 

## Appendix F Complexity Analysis

In this section, we aim to derive the complexity of one RPG iteration for different uncertainty sets. We first focus on non-robust MDPs, to then see how the complexity increases with the uncertainty structure.

**Non-robust MDPs.** Computing non-robust Q-values and occupation measure takes \(O(S^{2}A\log(\epsilon^{-1}))\) each, which represent the most expensive computations in PG. The product of \(d^{\pi},Q^{\pi}\) and \(\nabla\pi\) in PG requires \(O(SA)\) operations, which is insignificant. Hence, the total cost of PG corresponds to the computational cost of Q-values. More precisely, approximate the Q-value by \(Q\) and the occupation measure by \(d\) with \(\frac{\epsilon}{SA}\) tolerance, that is, \(\|Q-Q^{\pi}\|_{\infty}\leq\frac{\epsilon}{SA}\) and \(\|d-d^{\pi}\|_{\infty}\leq\frac{\epsilon}{SA}\). This involves \(O(S^{2}A\log(SA\epsilon^{-1}))\) operations for each. Then, we have

\[\sum_{(s,a)\in\mathcal{X}}d^{\prime}(s)Q^{\prime}(s,a)\nabla\pi_{s}(a)=\sum_{(s,a)\in\mathcal{X}}(Q^{\pi}(s,a)+\epsilon_{1}(s,a))(d^{\pi}(s,a)+\epsilon_{2}(s,a))\nabla\pi_{s}(a)\]

where \(\epsilon_{1}(s,a):=Q(s,a)-Q^{\pi}(s,a)\) and \(\epsilon_{2}(s,a):=d(s,a)-d^{\pi}(s,a)\). Let \(B\) be an upper bound of \(\|Q^{\pi}\|_{\infty}\) and \(\|d^{\pi}\|_{\infty}\). Then

\[\sum_{(s,a)\in\mathcal{X}}d^{\prime}(s)Q^{\prime}(s,a)\nabla\pi_{s }(a) =\sum_{(s,a)\in\mathcal{X}}(Q^{\pi}(s,a)+\epsilon_{1}(s,a))(d^{\pi}(s,a)+\epsilon_{2}(s,a))\nabla\pi_{s}(a)\] \[=\sum_{(s,a)\in\mathcal{X}}Q^{\pi}(s,a)d^{\pi}(s,a)\nabla\pi_{s}(a) +O(\epsilon),\]so the exact complexity of policy gradient for non-robust MDP is \(O(S^{2}A\log(SA\epsilon^{-1})\).

**Convex non-rectangular uncertainty set.** Robust policy improvement is strongly NP-Hard for non-rectangular uncertainty sets, even if convex [33]. The PG method finds global optimal given oracle access to policy gradient in polynomial time [30]. This implies that the computation of RPG must be NP-Hard.

### Helper results for Robust MDPs

**Variance and mean functions.** Computing \(\kappa_{p}(v)\) (resp. \(\omega_{p}(v)\)) with \(\epsilon\)-tolerance requires \(O(S\log(Se^{-1}))\) (resp. \(O(S\log(\epsilon^{-1}))\)) computations if we use binary search [14].

**Occupation measure.** Let \(k\in\mathbb{R}^{\mathcal{S}}\) be any vector. By definition,

\[d^{\pi}_{P,k}=\sum_{n=0}^{\infty}\gamma^{n}k^{\top}(P^{\pi})^{n}\]

and

\[\left\|d^{\pi}_{P,k}-\sum_{n=0}^{N-1}\gamma^{n}k^{\top}(P^{\pi})^{n}\right\|= \left\|\sum_{n=N}^{\infty}\gamma^{n}k^{\top}(P^{\pi})^{n}\right\|\leq\|k\|\sum _{n=N}^{\infty}\|\gamma P^{\pi}\|^{n}.\]

Since \(P^{\pi}\) is a stochastic matrix, \(\|P^{\pi}\|\leq 1\) and

\[\left\|d^{\pi}_{P,k}-\sum_{n=0}^{N-1}\gamma^{n}k^{\top}(P^{\pi})^{n}\right\| \leq\frac{\|k\|\gamma^{N}\|P^{\pi}\|^{N}}{1-\gamma\|P^{\pi}\|}\leq\frac{\|k\| \gamma^{N}}{1-\gamma}.\]

This implies that \(\sum_{n=0}^{N-1}\gamma^{n}k^{\top}(P^{\pi})^{n}\) is an \(O(\gamma^{N})\) approximation of \(d^{\pi}_{P,k}\). Now, take \(u_{0}=k\) and

\[u_{n+1}:=\gamma(u_{n})^{\top}P,\]

then \(\sum_{n=0}^{N-1}\gamma^{n}k^{\top}(P^{\pi})^{n}=\sum_{n=0}^{N-1}u_{n}\). Each iteration takes \(O(S^{2})\) computations, leading to total cost \(O(S^{2}N)\) for \(N\) iterations. Computing \(P^{\pi}\) from \(P\) is \(O(S^{2}A)\). We conclude that computing the occupation measure has a complexity of \(O(S^{2}A+S^{2}\log(\epsilon^{-1}))\).

**Lemma F.1**.: _We can approximate \(d^{\pi}_{P,k}\) by \(\sum_{n=0}^{N-1}\gamma^{n}(k^{\prime})^{\top}(P^{\pi})^{n}\) with complexity \(O(S^{2}A+S^{2}\log(\epsilon^{-1}))\) and tolerance_

\[\left\|d^{\pi}_{P,k}-\sum_{n=0}^{N-1}\gamma^{n}k^{\prime\top}(P^{\pi})^{n} \right\|\leq O\left(\frac{\|k\|\gamma^{N}+\|k-k^{\prime}\|}{1-\gamma}\right).\]

Proof.: \[\left\|d^{\pi}_{P,k}-\sum_{n=0}^{N-1}\gamma^{n}k^{\prime\top}(P^ {\pi})^{n}\right\| \leq\left\|d^{\pi}_{P,k}-\sum_{n=0}^{N-1}\gamma^{n}k^{\top}(P^{ \pi})^{n}\right\|+\left\|\sum_{n=0}^{N-1}\gamma^{n}k^{\prime\top}(P^{\pi})^{n} -\sum_{n=0}^{N-1}\gamma^{n}k^{\top}(P^{\pi})^{n}\right\|\] \[\leq O\left(\frac{\|k\|\gamma^{N}}{1-\gamma}\right)+\left\|\sum_{n =0}^{N-1}\gamma^{n}k^{\top}(P^{\pi})^{n}-\sum_{n=0}^{N-1}\gamma^{n}k^{\prime \top}(P^{\pi})^{n}\right\|\] \[\leq O\left(\frac{\|k\|\gamma^{N}}{1-\gamma}\right)+U\left(\frac{ \|k-k^{\prime}\|}{1-\gamma}\right).\]

**Computing Q-value given value function.** Let \(v\) be an \(\epsilon_{1}\) approximation of robust value function \(v_{\mathcal{U}}^{\pi}\), that is \(\|v-v_{\mathcal{U}}^{\pi}\|_{\infty}\leq\epsilon_{1}\). We want to compute the Q-value using the relation:

\[Q_{\mathcal{U}}^{\pi}(s,a) =R_{\mathcal{U}}^{\pi}(s,a)+\sum_{s,a}\pi_{s}(a)P_{\mathcal{U}}^{ \pi}(s^{\prime}|s,a)v_{\mathcal{U}}^{\pi}(s^{\prime})\] \[=R_{0}(s,a)+\gamma\sum_{s^{\prime}}P_{0}(s^{\prime}|s,a)v_{ \mathcal{U}}^{\pi}(s^{\prime})-\Omega_{\mathcal{U}}(v_{\mathcal{U}}^{\pi},\pi).\]

where \(\Omega_{\mathcal{U}_{p}^{\pi}}(v_{\mathcal{U}_{p}^{\pi}}^{\pi},\pi)=\frac{\pi _{s}(a)^{q-1}}{\|\pi_{s}\|_{\infty}^{q-1}}\left(\begin{array}{c}\alpha_{s}+ \gamma\beta_{s}\kappa_{q}(v_{\mathcal{U}_{p}^{\pi}}^{\pi})\end{array}\right)\) and \(\Omega_{\mathcal{U}_{p}^{\text{an}}}(v_{\mathcal{U}_{p}^{\text{an}}}^{\pi}, \pi)=\alpha_{sa}+\gamma\beta_{sa}\kappa_{q}(v_{\mathcal{U}_{p}^{\text{an}}}^{ \pi})\). Let \(Q\) be approximated from \(v\) by

\[Q(s,a)=R_{0}(s,a)+\gamma\sum_{s^{\prime}}P_{0}(s^{\prime}|s,a)v(s^{\prime})- \Omega_{\mathcal{U}}(v,\pi).\]

We thus have

\[\|Q_{\mathcal{U}}^{\pi}(s,a)-Q(s,a)\|_{\infty} =\gamma\Bigg{\|}\sum_{s^{\prime}\in\mathcal{S}}P_{0}(s^{\prime}|s,a)(v(s^{\prime})-v_{\mathcal{U}}^{\pi})\Bigg{\|}+\|\Omega_{\mathcal{U}}(v, \pi)-\Omega_{\mathcal{U}}(v_{\mathcal{U}}^{\pi},\pi)\|\] \[\leq\gamma\epsilon_{1}+\|\Omega_{\mathcal{U}}(v,\pi)-\Omega_{ \mathcal{U}}(v_{\mathcal{U}}^{\pi},\pi)\|\] \[\leq\gamma\epsilon_{1}+\|\beta\|_{\infty}\|\kappa_{q}(v)-\kappa_ {q}(v_{\mathcal{U}}^{\pi})\|\] \[\leq\gamma\epsilon_{1}+\|\beta\|_{\infty}S^{\frac{1}{q}}\epsilon_ {1}\] (By Lemma F.2) \[=O(S^{\frac{1}{q}}\epsilon_{1}),\]

so that \(\|Q-Q_{\mathcal{U}}^{\pi}\|_{\infty}\leq O(S^{\frac{1}{q}}\epsilon_{1})\).

**Lemma F.2**.: _The variance function \(\kappa_{p}\) is Lipschitz. More precisely,_

\[\|\kappa_{p}(v_{1})-\kappa_{p}(v_{2})\|\leq S^{\frac{1}{p}}\|v_{1}-v_{2}\|_{ \infty}\leq S^{\frac{1}{p}}\|v_{1}-v_{2}\|_{\infty},\quad\forall v_{1},v_{2} \in\mathbb{R}^{\mathcal{S}}\,.\]

Proof.: Let \(v_{i}\in\mathbb{R}^{\mathcal{S}}\) and \(w_{i}\in\operatorname*{arg\,min}_{w\in\mathbb{R}}\|v_{i}-w\mathbf{1}\|_{p}\) for \(i=1,2\). Without loss of generality, further assume that \(\kappa_{p}(v_{1})\geq\kappa_{p}(v_{2})\). Then,

\[\|\kappa_{p}(v_{1})-\kappa_{p}(v_{2})\| =\kappa_{p}(v_{1})-\kappa_{p}(v_{2})\] \[=\min_{w\in\mathbb{R}}\|v_{1}-w\mathbf{1}\|_{p}-\min_{w\in \mathbb{R}}\|v_{2}-w\mathbf{1}\|_{p}\] (By definition) \[\leq\|v_{1}-w_{2}\mathbf{1}\|_{p}-\|v_{2}-w_{2}\mathbf{1}\|_{p}\] (By definition of

\[\omega_{2}\] ) \[\leq\|(v_{1}-w_{2}\mathbf{1})-(v_{2}-w_{2}\mathbf{1})\|_{p}\] (Reverse triangle inequality) \[=\|v_{1}-v_{2}\|_{p}\] \[=\left(\sum_{s\in\mathcal{S}}(v_{1}(s)-v_{2}(s))^{p}\right)^{ \frac{1}{p}}\leq S^{\frac{1}{p}}\|v_{1}-v_{2}\|_{\infty}.\]

**Lemma F.3**.: \(Q_{\mathcal{U}_{p}^{\text{an}}}^{\pi}\) _can be approximated to \(\epsilon\) tolerance with the same complexity as computing \(v_{\mathcal{U}_{p}^{\text{an}}}^{\pi}\) to \(S^{-\frac{1}{q}}\epsilon\)._

Proof.: We can compute the value function with \(S^{-\frac{1}{q}}\epsilon\) tolerance. The rest of the operations are insignificant. The result follows from above. 

### RPG Complexity

Let \(O_{p}^{\text{an}}(\epsilon)\) be the complexity of computing \(v_{\mathcal{U}_{p}^{\text{an}}}^{\pi}\) with \(\epsilon\)-tolerance (see [14] for more details). Calculating Q-value up to \(\epsilon_{1}\) requires \(O_{p}^{\text{an}}(S^{-\frac{1}{q}}\epsilon_{1})\) computations according to Lemma F.3. Letting \(d_{1}\) and \(d_{2}\) be \(\epsilon_{2}\)-approximations of \(d_{P_{0},\mu}^{\pi}\) and \(d_{P_{0},k}^{\pi}\) respectively, their complexity is insignificant compared to \(O_{p}^{\text{an}}(S^{-\frac{1}{q}}\epsilon_{2})\). Now, approximating the gradient using \(d_{1},d_{2},Q,\nabla\pi\) as in Thm. 5.1 has a complexityof \(O(SA)\). Since the uncertainty set \(\mathcal{U}\) is compact, all quantities are bounded. There are \(O(SA)\) operations in Thm. 5.1, so taking \(\epsilon_{1},\epsilon_{2}=O(\frac{\epsilon}{SA})\), we get \(O(\epsilon)\) for the gradient. Hence, the total complexity is \(O_{p}^{\text{ns}}(S^{-\frac{1}{q}-1}A^{-1}\epsilon)\) which is \(\tilde{O}(S^{2}A\log(\epsilon^{-1}))\) by hiding log factors. A similar analysis follows for the \(s\)-rectangular case.

## Appendix G Generalization to arbitrary norms

In this section, we analyze how to generalize our result to general norms that are not \(\ell_{p}\).

### \((s,a)\)-rectangular robust MDPs

Consider an \((s,a)\)-rectangular uncertainty set \(\mathcal{U}=\mathcal{U}_{\|\cdot\|}^{\text{ss}}\) constrained by:

\[\mathcal{U}_{\|\cdot\|}^{\text{ss}} =(P_{0}+\mathcal{P})\times(R_{0}+\mathcal{R}), \text{where} (\mathcal{P},\mathcal{R}) =(\times_{s,a}\mathcal{P}_{sa},\times_{s,a}\mathcal{P}_{sa}),\] \[\mathcal{R}_{(s,a)} =\left\{r\in\mathbb{R}\ |\ \|r\|\leq\alpha_{s,a}\right\}, \text{and} \mathcal{P}_{(s,a)} =\left\{p\in\mathbb{R}^{\mathcal{S}}\ |\ \langle p,\mathbf{1}\rangle_{ \mathcal{S}}=0,\|p\|\leq\beta_{s,a}\right\}.\]

The robust Bellman operator \(\mathcal{T}_{\mathcal{U}}^{\pi}\) can be evaluated as

\[(\mathcal{T}_{\mathcal{U}}^{\pi}v)(s)=\sum_{a}\pi_{s}(a)\ \Big{[}\ R(s,a)- \gamma\beta_{s,a}\kappa_{\|\cdot\|}(v)+\gamma\sum_{s^{\prime}}P(s^{\prime}|s, a)v(s^{\prime})\ \Big{]},\]

where the variance function is defined as

\[\kappa_{\|\cdot\|}(v):=\min_{\langle u,\mathbf{1}\rangle_{\mathcal{S}}=0,\|u \|\leq 1}\langle u,v_{\mathcal{U}}^{\pi}\rangle.\]

This can be used to compute the robust value function. Then the worst values can found using robust Bellman operator \(\mathcal{T}_{\mathcal{U}}^{\pi}\) and robust value function \(v_{\mathcal{U}}^{\pi}\) as

\[(P_{\mathcal{U}}^{\pi},R_{\mathcal{U}}^{\pi})\in\operatorname*{arg\,min}_{(P, R)\in\mathcal{U}}\mathcal{T}_{(P,R)}^{\pi}v_{\mathcal{U}}^{\pi},\text{ \qquad\@@cite[cite]{[\@@bibref{}{G1}{}{}]}}.\]

It is easy to see that the worst values are given as

\[R_{\mathcal{U}}^{\pi}(s,a)=R_{0}(s,a)-\alpha_{s,a}\quad\text{and} \quad P_{\mathcal{U}}^{\pi}(\cdot|s,a)=P_{0}^{\pi}(\cdot|s,a)-\beta_{s,a}u_{ \mathcal{U}}^{\pi},\]

where normalized-balanced value function \(u_{\mathcal{U}}^{\pi}\) is a solution to

\[\min_{\langle u,\mathbf{1}\rangle_{\mathcal{S}}=0,\|u\|\leq 1}\langle u,v_{ \mathcal{U}}^{\pi}\rangle.\]

Observe that the worst kernel is still a rank-one perturbation of the nominal kernel. Hence, the robust occupation measure can be obtained using Lemma 4.4 as

\[d_{\mathcal{U},\mu}^{\pi}=d_{P_{0},\mu}^{\pi}-\gamma\frac{\langle d_{P_{0}, \mu}^{\pi},\beta^{\pi}\rangle_{\mathcal{S}}}{1+\gamma\langle d_{P_{0},u_{ \mathcal{U}}^{\pi}}^{\pi},\beta^{\pi}\rangle_{\mathcal{S}}}d_{P_{0},u_{ \mathcal{U}}^{\pi}}^{\pi},\] (12)

where \(\beta^{\pi}(s)=\sum_{a}\pi_{s}(a)\beta_{s,a}\). The last ingredient to compute RPG is the robust Q-value which can be computed using robust value function and worst values. However, it can be computed directly using the following iterates:

\[Q_{n+1}(s) =\min_{(P,R)\in\mathcal{U}}\ \Big{[}\ R(s,a)+\gamma\sum_{s^{\prime}}P(s^{ \prime}|s,a)v(s^{\prime})\ \Big{]}\] \[=R(s,a)-\alpha_{s,a}-\gamma\beta_{s,a}\kappa_{\|\cdot\|}(v)+ \gamma\sum_{s^{\prime}}P(s^{\prime}|s,a)v(s^{\prime}),\]

as \(Q_{n}\) converges to robust Q-value \(Q_{\mathcal{U}}^{\pi}\) linearly.

The proofs of the above claims are very similar to the \(\ell_{p}\) case so they are omitted. Finally, computing the variance function \(\kappa_{\|\cdot\|}\) and the normalized-value function \(u_{\mathcal{U}}^{\pi}\) can be done using numerical convex optimization methods for general norms. For the \(\ell_{p}\) case, these can be obtained in concrete form, so we choose to focus on \(\ell_{p}\) in our main study.

### \(s\)-rectangular case

Generalization to \(s\)-rectangular balls of a general norm is not straightforward and may not be possible for all types of norms. The crucial property of the \(\ell_{p}\)-norm exploited in our rank-one perturbation proof is 'decoupling', that is, for all \(x\in\mathbf{R}^{\mathcal{A}\times\mathcal{S}}\), there must exist \(k,l,m\in\mathbb{R}_{+}\) such that

\[\|x\|_{p}^{k}=\sum_{a\in\mathcal{A}}\lVert x_{a}\rVert_{m}^{l}.\]

This holds in the \(\ell_{p}\) case with \(k=l=m=p\). Further analysis of this setting is left for future work.

### Generalization to metrics and divergences

Generalizing RPG to distance or divergence-based uncertainty sets is also not obvious. Our RPG method, in particular the robust occupation measure, crucially relies on the rank-one perturbation characterization of the worst kernel, which might not apply for example with KL-ball constraints. We leave this analysis for future work.

## Appendix H Experiments

**Parameters.** All the nominal transition kernels and reward functions are generated randomly. The number of states and the number of actions are varied. Discount factor \(\gamma=0.9\), reward noise radius \(\alpha_{s,a},\alpha_{s}=0.1\), transition noise kernel \(\beta_{s,a},\beta_{s}=\frac{0.01}{SA}\).

**Hardware** Experiments are done on the machine with the following configuration: Intel(R) Core(TM) i7-6700 CPU @3.40GHZ, size:3598MHz, capacity 4GHz, width 64 bits, memory size 64 GiB.

**Software and codes** All the experiments were done in Python using numpy, matplotlib. All codes and results are available at https://github.com/navdtech/rpg.

**Procedure and Results.** All the experiments were repeated 100 times, except for Linear Programming (LP) cases as LP methods were very time-consuming. In LP methods, experiments were repeated 5 times except for the case (\(S=500,A=100\)) which was done only once. As this case was prohibitively expensive. Standard deviation in all cases was less than \(10\%\), and typically \(1-2\%\). This conveniently illustrates the superiority of our methods over LP methods.

**Observations**

* **Scalability of our methods.** Note that our methods scale very well with large state-action space. It takes a (small) constant times the time required by non-robust MDPs. On the other hand, LP methods explode. Both observations confirm the theoretical time complexity.
* **sa-case vs s case in LP methods.** We see s-case outperforms sa-case for small state-action spaces via LP methods. This is opposite to the theoretical time complexity of s-case which expensive than sa-case. We believe this is due to the internal implementation issues. Note that computing the robust value function is the most expensive step which requires evaluation of the robust Bellman operator. In sa case, one evaluation requires solving \(SA\) LP programs with \(S\) variables each, while for s-case, it is \(S\) LP programs with \(SA\) variables each. To solve LP, scipy.linprog is used, we believe it does some parallelization for large LPs. Hence, we observe less cost for sa-case. However, we observe that the cost of s-case increases much faster than s-case, and eventually under-performing than sa-case.

### RPG by LP

We compute RPG using LP in the following steps:

1. **(Robust Value Iteration)** Approximately compute the robust value function \(v_{l\mathcal{U}}^{\pi}\) using the iterates \(v_{n+1}:=\mathcal{T}_{l\mathcal{U}}^{\pi}v_{n}\). Evaluation of robust Bellman operator \(\mathcal{T}_{l\mathcal{U}}^{\pi}\) is done via LPs as described below. This is the most expensive step as it requires evaluating robust Bellman operators \(O(\log(\epsilon^{-1}))\) times, and each evaluation requires many LPs.

2. **(Adversarial Values)** Compute the worst values \((P_{\mathcal{U}}^{\pi},R_{\mathcal{U}}^{\pi})\) using the robust value function from the following relation: \[(P_{\mathcal{U}}^{\pi},R_{\mathcal{U}}^{\pi})\in\operatorname*{arg\,min}_{(P,R) \in\mathcal{U}}\mathcal{T}_{\mathcal{U}}^{\pi}v_{\mathcal{U}}^{\pi}.\] This is also solved by LP.
3. **(Policy Gradient Theorem)** We now compute the RPG using policy gradient Theorem [26] w.r.t. the adversarial values computed above, as \[\partial\rho_{\mathcal{U}}^{\pi}=\sum_{s,a}d_{P_{\mathcal{U}}^{\pi}}^{\pi}(s) Q_{P_{\mathcal{U}}^{\pi},R_{\mathcal{U}}^{\pi}}^{\pi}(s,a)\nabla\pi_{s}(a).\] Observe that \(d_{P}^{\pi}\) can be approximated as \(\sum_{n=0}^{n}(\gamma P^{\pi})^{n}\) for large \(n\) enough, and \(Q_{P,R}^{\pi}\) can be approximated by dynamic programming [25]. Notably, this step and the second step are negligible as compared to the first step.

**Robust Value Iteration by LP**

**sa-rectangular robust MDPs.** We first consider \(\mathtt{sa}\)-rectangular \(L_{1}\) constrained uncertainty set \(\mathcal{U}_{p}^{\mathtt{sa}}=\mathcal{P}\times\mathcal{R}\). Robust Bellman operator is given by

\[(\mathcal{T}_{\mathcal{U}_{p}^{\mathtt{sa}}}^{\pi}v)(s)=\max_{a}\min_{ \begin{subarray}{c}p\in\mathcal{P}_{sa,r}\in\mathcal{R}_{sa}\end{subarray}} \Big{[}\ r+\gamma\sum_{s^{\prime}}p(s^{\prime})v(s^{\prime})\ \Big{]} \over\text{LP with }\widehat{S}\ \text{variable}}.\]

Note that the above can be solved by \(A\) LPs as uncertainty set \(\mathcal{U}_{p}^{\mathtt{sa}}=\mathcal{P}\times\mathcal{R}\) induces linear constraint and the objective is also linear with \(S\) variables. Hence, evaluation of \(\mathcal{T}_{\mathcal{U}_{p}^{\mathtt{sa}}}^{\pi}v\) requires solving \(SA\) LPs with \(S\) variable each.

\(s\)**-rectangular robust MDPs.** We now consider \(s\)-rectangular \(L_{1}\) constrained uncertainty set \(\mathcal{U}_{p}^{\mathtt{sa}}=\mathcal{P}\times\mathcal{R}\). Robust Bellman operator is given by

\[(\mathcal{T}_{\mathcal{U}_{p}^{\mathtt{sa}}}^{\pi}v)(s)=\underbrace{\min_{p \in\mathcal{P}_{s,r}\in\mathcal{R}_{s}}\sum_{a}\pi_{s}(a)\ \Big{[}\ r(a)+\gamma\sum_{s^{\prime}}p(s^{\prime}|a)v(s^{\prime})\ \Big{]}}_{\text{LP with }\widehat{SA}\ \text{ variable}}.\]

Note that the above can be solved by one LP as uncertainty set \(\mathcal{U}_{p}^{\mathtt{s}}=\mathcal{P}\times\mathcal{R}\) induces linear constraint and the objective is also linear with \(SA\) variables. Hence, evaluation of \(\mathcal{T}_{\mathcal{U}_{p}^{\mathtt{sa}}}^{\pi}v\) requires solving \(S\) LPs with \(SA\) variable each.