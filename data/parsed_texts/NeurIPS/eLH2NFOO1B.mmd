# Equivariant flow matching

Leon Klein

Freie Universitat Berlin

leon.klein@fu-berlin.de &Andreas Kramer

Freie Universitat Berlin

andreas.kraemer@fu-berlin.de &Frank Noe

Microsoft Research AI4Science

Freie Universitat Berlin

Rice University

franknoe@microsoft.com

###### Abstract

Normalizing flows are a class of deep generative models that are especially interesting for modeling probability distributions in physics, where the exact likelihood of flows allows reweighting to known target energy functions and computing unbiased observables. For instance, Boltzmann generators tackle the long-standing sampling problem in statistical physics by training flows to produce equilibrium samples of many-body systems such as small molecules and proteins. To build effective models for such systems, it is crucial to incorporate the symmetries of the target energy into the model, which can be achieved by equivariant continuous normalizing flows (CNFs). However, CNFs can be computationally expensive to train and generate samples from, which has hampered their scalability and practical application. In this paper, we introduce equivariant flow matching, a new training objective for equivariant CNFs that is based on the recently proposed optimal transport flow matching. Equivariant flow matching exploits the physical symmetries of the target energy for efficient, simulation-free training of equivariant CNFs. We demonstrate the effectiveness of flow matching on rotation and permutation invariant many-particle systems and a small molecule, alanine dipeptide, where for the first time we obtain a Boltzmann generator with significant sampling efficiency without relying on tailored internal coordinate featurization. Our results show that the equivariant flow matching objective yields flows with shorter integration paths, improved sampling efficiency, and higher scalability compared to existing methods.

## 1 Introduction

Generative models have achieved remarkable success across various domains, including images [1, 2, 3, 4], language [5, 6, 7], and applications in the physical sciences [8, 9, 10]. Among the rapidly growing subfields in generative modeling, normalizing flows have garnered significant interest. Normalizing flows [11, 12, 13, 14] are powerful exact-likelihood generative neural networks that transform samples from a simple prior distribution into samples that follow a desired target probability distribution. Previous studies [15, 16, 17, 18, 19] have emphasized the importance of incorporating symmetries of the target system into the flow model. In this work, we focus specifically on many-body systems characterized by configurations \(x\in\mathbb{R}^{N\times D}\) of \(N\) particles in \(D\) spatial dimensions. The symmetries of such systems arise from the invariances of the potential energy function \(U(x)\). Theassociated probability distribution, known as the Boltzmann distribution, is given by

\[\mu(x)\propto\exp{\left(-\frac{U(x)}{k_{B}T}\right)}, \tag{1}\]

where \(k_{B}\) represents the Boltzmann constant and \(T\) denotes the temperature.

Generating equilibrium samples from the Boltzmann distribution is a long-standing problem in statistical physics, typically addressed using iterative methods like Markov Chain Monte Carlo or Molecular Dynamics (MD). In contrast, Boltzmann generators (BGs) [8] utilize normalizing flows to directly generate independent samples from the Boltzmann distribution. Moreover, they allow the reweighting of the generated density to match the unbiased target Boltzmann distribution \(\mu\). When dealing with symmetric densities, which are ubiquitous in physical systems, BGs employ two main approaches: (i) describing the system using internal coordinates [8; 20] or (ii) describing the system in Cartesian coordinates while incorporating the symmetries into the flow through equivariant models [17]. In this work, we focus on the latter approach, which appears more promising as such architectures can in principle generalize across different molecules and do not rely on tailored system-specific featurizations. However, current equivariant Boltzmann generators based on continuous normalizing flows (CNFs) face limitations in scalability due to their high computational cost during both training and inference. Recently, flow matching [21] has emerged as a fast, simulation-free method for training CNFs. A most recent extension, optimal transport (OT) flow matching [22], enables learning optimal transport maps, which facilitates fast inference through simple integration paths. In this work, we apply OT flow matching to train flows for highly symmetric densities and find that the required batch size to approximate the OT map adequately can become prohibitively large (Figure 1c). Thus, the learned flow paths will deviate strongly from the OT paths (Figure 1d), which increases computational cost and numerical errors. We tackle this problem by proposing a novel _equivariant_ OT flow matching objective for training equivariant flows on invariant densities, resulting in optimal paths for faster inference (see Figure 1e,f).

Our main contributions in this work are as follows:

1. We propose a novel flow matching objective designed for invariant densities, yielding nearly optimal integration paths. Concurrent work of [23] proposes nearly the same objective with the same approximation method, which they also call equivariant flow matching.

Figure 1: Results for the 13 particle Lennard-Jones system (LJ13) for different flow matching training methods. (a, b) Sample pairs generated with the different flow matching objectives during training. (c) Mean transport cost (squared distance) for training batches. (d, e) Integration paths per particle for samples generated by models trained with OT flow matching and equivariant OT flow matching, respectively. (f) Integration path length, i.e. arc length between prior and output sample, distribution compared with the OT path length between the prior and push-forward distribution of the flows.

2. We compare different flow matching objectives to train equivariant continuous normalizing flows. Through our evaluation, we demonstrate that only our proposed equivariant flow matching objective enables close approximations to the optimal transport paths for invariant densities. However, our proposed method is most effective for larger highly symmetric systems, while achieving sometimes inferior results for the smaller systems compared to optimal transport flow matching.
3. We introduce a new invariant dataset of alanine dipeptide and a large Lennard-Jones cluster. These datasets serve as valuable resources for evaluating the performance of flow models on invariant systems.
4. We present the first Boltzmann Generator capable of producing samples from the equilibrium Boltzmann distribution of a molecule in Cartesian coordinates. Additionally, we demonstrate the reliability of our generator by accurately estimating the free energy difference, in close agreement with the results obtained from umbrella sampling. Concurrent work of [24] also introduce a Boltzmann Generator, based on coupling flows instead of CNFs, in Cartesian coordinates for alanine dipeptide. However, they investigate alanine dipeptide at \(T=800K\) instead of room temperature.

## 2 Related work

Normalizing flows and Boltzmann generators [8] have been applied to molecular sampling and free energy estimation [25; 20; 26; 27; 28]. Previous flows for molecules only achieved significant sampling efficiency when employing either system-specific featurization such as internal coordinates [8; 29; 25; 20; 30; 31] or prior distributions close to the target distribution [32; 31; 33]. Notably, our equivariant OT flow matching method could also be applied in such scenarios, where the prior distribution is sampled by MD at a different thermodynamic state (e.g., higher temperature or lower level of theory). A flow model for molecules in Cartesian coordinates has been developed in [34] where a transferable coupling flow is used to sample small peptide conformations by proposing iteratively large time steps instead of sampling from the target distribution directly. Equivariant diffusion models [35; 36] learn a score-based model to generate molecular conformations. The score-based model is parameterized by an equivariant function similar to the vector field used in equivariant CNFs. However, they do not target the Boltzmann distribution. As an exception, [37] propose a diffusion model in torsion space and use the underlying probability flow ODE as a Boltzmann generator. Moreover, [38] use score-based models to learn the transition probability for multiple time-resolutions, accurately capturing the dynamics.

To help speed up CNF training and inference, various authors [39; 40; 41] have proposed incorporating regularization terms into the likelihood training objective for CNFs to learn an optimal transport (OT) map. While this approach can yield flows similar to OT flow matching, the training process itself is computationally expensive, posing limitations on its scalability. The general idea underlying flow matching was independently conceived by different groups [21; 42; 43] and soon extended to incorporate OT [22; 44]. OT maps with invariances have been studied previously to map between learned representations [45], but have not yet been applied to generative models. Finally, we clarify that we use the term flow matching exclusively to refer to the training method for flows introduced by Lipman et al. [21] rather than the flow-matching method for training energy-based coarse-grained models from flows introduced at the same time [46].

## 3 Method

In this section, we describe the key methodologies used in our study, highlighting important prior work.

### Normalizing flows

Normalizing flows [14; 47] provide a powerful framework for learning complex probability densities \(\mu(x)\) by leveraging the concept of invertible transformations. These transformations, denoted as \(f_{\theta}:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}\), map samples from a simple prior distribution \(q(x_{0})=\mathcal{N}(x_{0}|0,I)\) to samples from a more complicated output distribution. This resulting distribution \(p(x_{1})\), known as the _push-forward_

[MISSING_PAGE_FAIL:4]

where \(p(z)\) is some arbitrary conditioning distribution independent of \(x\) and \(t\). For a derivation see [21] and [22]. There are different ways to efficiently parametrize \(u_{t}(x|z)\) and \(p_{t}(x|z)\). We here focus on a parametrization that gives rise to the optimal transport path, as introduced in [22]

\[z =(x_{0},x_{1})\quad\text{and}\quad p(z)=\pi(x_{0},x_{1}) \tag{10}\] \[u_{t}(x|z) =x_{1}-x_{0}\quad\text{and}\quad p_{t}(x|z)=\mathcal{N}(x|t\cdot x _{1}+(1-t)\cdot x_{0},\sigma^{2}), \tag{11}\]

where the conditioning distribution is given by the 2-Wasserstein optimal transport map \(\pi(x_{0},x_{1})\) between the prior \(q(x_{0})\) and the target \(\mu(x_{1})\). The 2-Wasserstein optimal transport map is defined by the 2-Wasserstein distance

\[W_{2}^{2}=\inf_{\pi\in\Pi}\int c(x_{0},x_{1})\pi(dx_{0},dx_{1}), \tag{12}\]

where \(\Pi\) is the set of couplings as usual and \(c(x_{0},x_{1})=||x_{0}-x_{1}||_{2}^{2}\) is the squared Euclidean distance in our case. Following [22], we approximate \(\pi(x_{0},x_{1})\) by only considering a batch of prior and target samples. Hence, for each batch we generate samples from \(p(z)\) as follows:

1. sample batches of points \((x_{0}^{1},\ldots,x_{0}^{B})\sim q\) and \((x_{1}^{1},\ldots,x_{1}^{B})\sim\mu\),
2. compute the cost matrix \(M\) for the batch, i.e. \(M_{ij}=||x_{0}^{i}-x_{1}^{j}||_{2}^{2}\),
3. solve the discrete OT problem defined by \(M,\)
4. generate training pairs \(z^{i}=(x_{0}^{i},x_{1}^{i})\) according to the OT solution.

We will refer to this training procedure as _OT flow matching_.

### Equivariant flows

Symmetries can be described in terms of a _group_\(G\) acting on a finite-dimensional vector space \(V\) via a matrix representation \(\rho(g);g\in G\). A map \(I:V\to V^{\prime}\) is called \(G\)-invariant if \(I(\rho(g)x)=I(x)\) for all \(g\in G\) and \(x\in V\). Similarly, a map \(f:V\to V\) is called \(G\)-equivariant if \(f(\rho(g)x)=\rho(g)f(x)\) for all \(g\in G\) and \(x\in V\).

In this work, we focus on systems with energies \(U(x)\) that exhibit invariance under the following symmetries: (i) _Permutations_ of interchangeable particles, described by the symmetric group \(S(N^{\prime})\) for each interchangeable particle group. (ii) _Global rotations and reflections_, described by the orthogonal group \(O(D)\) (iii) _Global translations_, described by the translation group \(\mathbb{T}\). These symmetries are commonly observed in many particle systems, such as molecules or materials.

In [17], it is shown that equivariant CNFs can be constructed using an equivariant vector field \(v_{\theta}\) (Theorem 2). Moreover, [17; 50] show that the push-forward distribution \(\tilde{p}(x_{1})\) of a \(G\)-equivariant flow with a \(G\)-invariant prior density is also \(G\)-invariant (Theorem 1). Note that this is only valid for orthogonal maps, and hence not for translations. However, translation invariance can easily be achieved by assuming mean-free systems as proposed in [17]. An additional advantage of mean-free systems is that global rotations are constrained to occur around the origin. By ensuring that the flow does not modify the geometric center and the prior distribution is mean-free, the resulting push-forward distribution will also be mean-free.

Although equivariant flows can be successfully trained with the OT flow matching objective, the trained flows display highly curved vector fields (Figure 1d) that do not match the linear OT paths. Fortunately, the OT training objective can be modified so that it yields better approximations to the OT solution for finite batch sizes.

## 4 Equivariant optimal transport flow matching

Prior studies [51; 22] indicate that medium to small batch sizes are often sufficient for effective OT flow matching. However, when dealing with highly symmetric densities, accurately approximating the OT map may require a prohibitively large batch size. This is particularly evident in cases involving permutations, where even for small system sizes, it is unlikely for any pair of target-prior samples \((x_{0},x_{1})\) to share the exact same permutation. This is because the number of possible permutations scales with the number of interchangeable particles as \(N!\), while the number of combinations of the sample pairs scales only with squared batch size (Figure 1). To address this challenge, we propose using a cost function

\[\tilde{c}(x_{0},x_{1})=\min_{g\in G}||x_{0}-\rho(g)x_{1}||_{2}^{2}, \tag{13}\]that accurately accounts for the underlying symmetries of the problem in the OT flow matching algorithm. Hence, instead of solely reordering the batch, we instead also align samples along their orbits.

We summarize or main theoretical findings in the following theorem.

**Theorem 1**.: _Let \(G\) be a compact group that acts on an Euclidean \(n\)-space by isometries. Let \(T\colon x\mapsto y\) be an OT map between \(G\)-invariant measures \(\nu_{1}\) and \(\nu_{2}\), using the cost function \(c\). Then_

1. \(T\) _is_ \(G\)_-equivariant and the corresponding OT plan_ \(\pi(\nu_{1},\nu_{2})\) _is_ \(G\)_-invariant._
2. _For all pairs_ \((x,T(x))\) _and_ \(y\in G\cdot T(x):\)__ \[c(x,T(x))=\int_{G}c(g\cdot x,g\cdot T(x))d\mu(g)=\min_{g\in G}c(x,g\cdot y)\] (14)
3. \(T\) _is also an OT map for the cost function_ \(\tilde{c}\)_._

Refer to Appendix B.1 for an extensive derivation and discussion. The key insights can be summarized as follows: (i) Given the \(G\)-equivariance of the target OT map \(T\), it is natural to employ an \(G\)-equivariant flow model for its learning. (ii) From 2. and 3., we can follow that our proposed cost function \(\tilde{c}\), effectively aligns pairs of samples in a manner consistent with how they are aligned under the \(G\)-equivariant OT map \(T\).

In this work we focus on \(O(D)\)- and \(S(N)\)-invariant distributions, which are common for molecules and multi-particle systems. The minimal squared distance for a pair of points \((x_{0},x_{1})\), taking into account these symmetries, can be obtained by minimizing the squared Euclidean distance over all possible combinations of rotations, reflections, and permutations

\[\tilde{c}(x_{0},x_{1})=\min_{r\in O(D),s\in S(N)}||x_{0}-\rho(rs)x_{1}||_{2}^{ 2}. \tag{15}\]

However, computing the exact minimal squared distance is computationally infeasible in practice due to the need to search over all possible combinations of rotations and permutations. Therefore, we approximate the minimizer by performing a sequential search

\[\tilde{c}(x_{0},x_{1})=\min_{r\in SO(D)}||x_{0}-\rho(r\tilde{s})x_{1}||_{2}^{ 2},\quad\tilde{s}=\operatorname*{arg\,min}_{s\in S(N)}||x_{0}-\rho(s)x_{1}||_{ 2}^{2}. \tag{16}\]

We demonstrate in Section 6 that this approximation results in nearly OT integration paths for equivariant flows, even for small batch sizes. While we also tested other approximation strategies in Appendix A.9, they did not yield significant changes in our results, but come at with additional computational overhead. We hence alter the OT flow matching procedure as follows: For each element of the cost matrix \(M\), we first compute the optimal permutation with the Hungarian algorithm [52] and then align the two samples through rotation with the Kabsch algorithm [53]. The other steps of the OT flow matching algorithm remain unchanged. We will refer to this loss as _Equivariant OT flow matching_. Although aligning samples in that way comes with a significant computational overhead, this reordering can be done in parallel before or during training (see Appendix C.4). It is worth noting that in the limit of infinite batch sizes, the Euclidean cost will still yield the correct OT map for invariant densities.

## 5 Architecture

The vector field \(v_{\theta}(t,x)\) is parametrized by an \(O(D)\)- and \(S(N)\)-equivariant graph neural network, similar to the one used in [18] and introduced in [54]. The graph neural network consists of \(L\) consecutive layers. The update for the \(i\)-th particle is computed as follows

\[h_{i}^{0} =(t,a_{i}),\quad m_{ij}^{l}=\phi_{e}\left(h_{i}^{l},h_{j}^{l},d_{j }^{2}\right), \tag{17}\] \[x_{i}^{l+1} =x_{i}^{l}+\sum_{j\neq i}\frac{\left(x_{i}^{l}-x_{j}^{l}\right)}{ d_{ij}+1}\phi_{d}(m_{ij}^{l}),\] (18) \[h_{i}^{l+1} =\phi_{h}\left(h_{i}^{l},m_{i}^{l}\right),\quad m_{i}^{l}=\sum_{ j\neq i}\phi_{m}(m_{ij}^{l})m_{ij}^{l},\] (19) \[v_{\theta}(t,x^{0})_{i} =x_{i}^{L}-x_{i}^{0}, \tag{20}\]where \(\phi_{\alpha}\) are neural networks, \(d_{ij}\) is the Euclidean distance between particle \(i\) and \(j\), and \(a_{i}\) is an embedding for the particle type. Notably, the update conserves the geometric center if all particles are of the same type (see Appendix B.2), otherwise we subtract the geometric center after the last layer. This ensures that the resulting equivariant vector field \(v_{\theta}(t,x)\) conserves the geometric center. When combined with a symmetric mean-free prior distribution, the push-forward distribution of the CNF will be \(O(D)\)- and \(S(N)\)-invariant.

## 6 Experiments

In this section, we demonstrate the advantages of equivariant OT flow matching over existing training methods using four datasets characterized by invariant energies. We explore three different training objectives: (i) likelihood-based training, (ii) OT flow matching, and (iii) equivariant OT flow matching. For a comprehensive overview of experimental details, including dataset parameters, error bars, learning rate schedules, computing infrastructure, and additional experiments, please refer to Appendix A and Appendix C in the supplementary material. For all experiments, we employ a mean-free Gaussian prior distribution. The number of layers and parameters in the equivariant CNF vary across datasets while remaining consistent within each dataset (see Appendix C). We provide naive flow matching as an additional baseline in Appendix A.11. However, the results are very similar to the ones obtained with OT flow matching, although the integration paths are generally slightly longer.

### DW4 and LJ13

We first evaluate the different loss functions on two many-particle systems, DW4 and LJ13, that were specifically designed for benchmarking equivariant flows as described in [17]. These systems feature pair-wise double-well and Lennard-Jones interactions with \(4\) and \(13\) particles, respectively (see Appendix C.2 for more details). While state-of-the-art results have been reported in [18], their evaluations were performed on very small test sets and were biased for the LJ13 system. To provide a fair comparison, we retrain their model using likelihood-based training as well as the two flow matching losses on resampled training and test sets for both systems. Additionally, we demonstrate improved likelihood performance of flow matching on their biased test set in Appendix A.3. The results, presented in Table 1, show that the two flow matching objectives outperform likelihood-based training while being computationally more efficient. The effective sample sizes (ESS) and negative log likelihood (NLL) are comparable for the flow matching runs. However, for the LJ13 system, the equivariant OT flow matching objective significantly reduces the integration path length compared

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Training type** & **NLL** (\(\downarrow\)) & **ESS** (\(\uparrow\)) & **Path length** (\(\downarrow\)) \\ \cline{2-4}  & & DW4 & \\ \cline{2-4} Likelihood [18] & \(1.72\pm 0.01\) & \(86.87\pm 0.19\%\) & \(3.11\pm 0.04\) \\ OT flow matching & \(1.70\pm 0.02\) & \(\mathbf{92.37}\pm\mathbf{0.89}\%\) & \(2.94\pm 0.02\) \\ Equivariant OT flow matching & \(1.68\pm 0.01\) & \(88.71\pm 0.40\%\) & \(2.92\pm 0.01\) \\ \cline{2-4}  & & LJ13 & \\ \cline{2-4} Likelihood [18] & \(-15.83\pm 0.07\) & \(39.78\pm 6.19\%\) & \(5.08\pm 0.22\) \\ OT flow matching & \(-16.09\pm 0.03\) & \(54.36\pm 5.43\%\) & \(2.84\pm 0.01\) \\ Equivariant OT flow matching & \(-16.07\pm 0.02\) & \(\mathbf{57.98}\pm\mathbf{2.19}\%\) & \(\mathbf{2.15}\pm\mathbf{0.01}\) \\ \cline{2-4}  & & LJ55 & \\ \cline{2-4} OT flow matching & \(-88.45\pm 0.04\) & \(3.74\pm 1.06\%\) & \(7.53\pm 0.02\) \\ Equivariant OT flow matching & \(-\mathbf{89.27}\pm\mathbf{0.04}\) & \(\mathbf{4.42}\pm\mathbf{0.35}\%\) & \(\mathbf{3.52}\pm\mathbf{0.01}\) \\ \cline{2-4}  & & Alanine dipeptide & \\ \cline{2-4} OT flow matching & \(-\mathbf{107.54}\pm\mathbf{0.03}\) & \(\mathbf{1.41}\pm\mathbf{0.04}\%\) & \(10.19\pm 0.03\) \\ Equivariant OT flow matching & \(-106.78\pm 0.02\) & \(0.69\pm 0.05\%\) & \(\mathbf{9.46}\pm\mathbf{0.01}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of flows trained with different training objectives. Errors are computed over three runs.

to other methods due to the large number of possible permutations (refer to Figure 1 for a visual illustration).

### LJ55

The effectiveness of equivariant OT flow matching becomes even more pronounced when training on larger systems, where likelihood training is infeasible (see Appendix A.2). To this end, we investigate a large Lennard-Jones cluster with 55 particles (_LJ55_).

We observe that the mean batch transport cost of training pairs is about \(10\) times larger for OT flow matching compared to equivariant OT flow matching (Figure 2b), resulting in curved and twice as long integration paths during inference (Figure 2a,c). However, the integration paths for OT flow matching are shorter than those seen during training, see Appendix A.1 for a more detailed discussion. We compare the integration error caused by using a fixed step integrator (rk4) instead of an adaptive solver (dropi5 [55]). As the integration paths follow nearly straight lines for the equivariant OT flow matching, the so resulting integration error is minimal, while the error is significantly larger for OT flow matching (Figure 2d). Hence, we can use a fixed step integrator, with e.g. \(20\) steps, for sampling for equivariant OT flow matching, resulting in a three times speed-up over OT flow matching (Figure 2e), emphasizing the importance of accounting for symmetries in the loss for large, highly symmetric systems. Moreover, the equivariant OT flow matching objective outperforms OT flow matching on all evaluation metrics (Table 1). To ensure that the flow samples all states, we compute the energy distribution (Figure 2f) and perform deterministic structure minimization of the samples, similar to [17], in Appendix A.7.

### Alanine dipeptide

In our final experiment, we focus on the small molecule alanine dipeptide (Figure 3a) in Cartesian coordinates. The objective is to train a Boltzmann Generator capable of sampling from the equilibrium Boltzmann distribution defined by the semi-empirical _GFN2-xTB_ force-field [56]. This semi-empirical potential energy is invariant under permutations of atoms of the same chemical element and global rotations and reflections. However, here we consider the five backbone atoms defining the \(\varphi\) and \(\psi\) dihedral angles as distinguishable to facilitate analysis. Since running MD simulations with semi-empirical force-fields is computationally expensive, we employ a surrogate training set, further challenging the learning task.

Figure 2: Results for the LJ55 system (a) Integration paths per particle for OT flow matching (left) and equivariant OT flow matching (right). (b) Mean transport cost (squared distance) for training batches. (c) Integration path length distribution. (d) Integration error for a fixed step size integrator (rk4) with respect to a reference solution generated by an adaptive solver (dropi5). (e) Effective samples vs number of function evaluations, i.e. evaluations of the vector field, for a sampling batch size of \(1000\). (f) Energy histograms for a flow trained with equivariant OT flow matching.

Alanine dipeptide data set generationThe alanine dipeptide training data set is generated through two steps: (i) Firstly, we perform an MD simulation using the classical _Amber ff99SBildn_ force-field for a duration of \(1\) ms [20]. (ii) Secondly, we relax \(10^{5}\) randomly selected states from the MD simulation using the semi-empirical _GFN2-xTB_ force-field for \(100\) fs each. For more detailed information, refer to Appendix C.2 in the supplementary material. This training data generation is significantly cheaper than performing a long MD simulation with the semi-empirical force field.

Although the Boltzmann generator is trained on a biased training set, we can generate asymptotically unbiased samples from the semi-empirical target distribution by employing reweighting (Appendix B.3) as demonstrated in Figure 3e. In this experiment, OT flow matching outperforms equivariant OT flow matching in terms of effective sample size (ESS) and negative log likelihood (NLL) as shown in Table 1. However, the integration path lengths are still longer for OT flow matching compared to equivariant OT flow matching, as depicted in Figure 3c. Since the energy of alanine dipeptide is invariant under global reflections, the flow generates samples for both chiral states. While the presence of the mirror state reflects the symmetries of the energy, in practical scenarios, molecules typically do not change their chirality spontaneously. Therefore, it may be undesirable to have samples from both chiral states. However, it is straightforward to identify samples of the undesired chirality and apply a mirroring operation to correct them. Alternatively, one may use a SO(3) equivariant flow without reflection equivariance [54] to prevent the generation of mirror states.

Alanine dipeptide - free energy differenceThe computation of free energy differences is a common challenge in statistical physics as it determines the relative stability of metastable states. In the specific case of alanine dipeptide, the transition between negative and positive \(\varphi\) dihedral angle is the slowest process (Figure 3d), and equilibrating the free energy difference between these two states from MD simulation requires simulating numerous transitions, i.e. millions of consecutive MD steps, which is expensive for the semi-empirical force field. In contrast, we can train a Boltzmann generator from data that is not in global equilibrium, i.e. using our biased training data, which is significantly cheaper to generate. Moreover, as the positive \(\varphi\) state is much less likely, we can even bias our training data to have nearly equal density in both states, which helps compute a more precise free energy estimate (see Appendix C.5). The equivariant Boltzmann generator is trained using the OT flow matching loss, which exhibited slightly better performance for alanine dipeptide. We obtain similar results for the equivariant OT flow matching loss (see Appendix A.4). To obtain an accurate estimation of the free energy difference, five umbrella sampling simulations are conducted along the \(\varphi\) dihedral angle using the semi-empirical force-field (see Appendix C.2). The free energy difference estimated by the Boltzmann generator demonstrates good agreement with the results

Figure 3: Results for the alanine dipeptide system (a) Alanine dipeptide molecule. (b) Mean transport cost (squared distance) for training batches. (c) Integration path length distribution. (d) Ramachandran plot depicting the generated joint marginal distribution over the backbone dihedral angles \(\varphi\) and \(\psi\) after filtering out samples with right-handed chirality and high energies. (e) Energy histograms for samples generated by a flow trained with OT flow matching. (f) Free energy distribution along the slowest transition (\(\varphi\) dihedral angle) computed with umbrella sampling and the equivariant flow.

of these simulations (Table 2 and Figure 2(f)), whereas both the relaxed training data and classical Molecular Dynamics simulation overestimate the free energy difference.

## 7 Discussion

We have introduced a novel flow matching objective for training equivariant continuous normalizing flows on invariant densities, leading to optimal transport integration paths even for small training batch sizes. By leveraging flow matching objectives, we successfully extended the applicability of equivariant flows to significantly larger systems, including the large Lennard-Jones cluster (LJ55). We conducted experiments comparing different training objectives on four symmetric datasets, and our results demonstrate that as the system size increases, the importance of accounting for symmetries within the flow matching objective becomes more pronounced. This highlights the critical role of our proposed flow matching objective in scaling equivariant CNFs to even larger systems.

Another notable contribution of this work is the first successful application of a Boltzmann generator to model alanine dipeptide in Cartesian coordinates. By accurately estimating free energy differences using a semi-empirical force-field, our approach of applying OT flow matching and equivariant OT flow matching to equivariant flows demonstrates its potential for reliable simulations of complex molecular systems.

## 8 Limitations / Future work

While we did not conduct experiments to demonstrate the transferability of our approach, the architecture and proposed loss function can potentially be used to train transferable models. We leave this for future research. Although training with flow matching is faster and computationally cheaper than likelihood training for CNFs, the inference process still requires the complete integration of the vector field, which can be computationally expensive. However, if the model is trained with the equivariant OT flow matching objective, faster fixed-step integrators can be employed during inference. Our suggested approximation of Equation (15) includes the Hungarian algorithm, which has a computational complexity of \(\mathcal{O}(N^{3})\). To improve efficiency, this step could be replaced by heuristics relying on approximations to the Hungarian algorithm [57]. Moreover, flow matching does not allow for energy based training, as this requires integration similar to NLL training. A potential alternative approach is to initially train a CNF using flow matching with a small set of samples. Subsequent sample generation through the CNF, followed by reweighting to the target distribution, allows these samples to be added iteratively to the training set, similarly as in [27; 37]. Notably, in our experiments, we exclusively examined Gaussian prior distributions. However, flow matching allows to transform arbitrary distributions. Therefore, our equivariant OT flow matching method holds promise for application in scenarios where the prior distribution is sampled through MD at a distinct thermodynamic state, such as a higher temperature or a different level of theory [32; 33]. In these cases, where both the prior and target distributions are close, with samples alignable through rotations and permutations, we expect that the advantages of equivariant OT flow matching will become even more pronounced than what we observed in our experiments.

Building upon the success of training equivariant CNFs for larger systems using our flow matching objective, future work should explore different architectures for the vector field. Promising candidates, such as [58; 59; 60; 61; 62], could be investigated to improve the modeling capabilities of equivariant CNFs. While our focus in this work has been on symmetric physical systems, it is worth noting that equivariant flows have applications in other domains that also exhibit symmetries, such as traffic data generation [63], point cloud and set modeling [64; 65], as well as invariant distributions on arbitrary manifolds [66]. Our equivariant OT flow matching approach can be readily applied to these areas of application without modification.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & MD & relaxed MD & Umbrella sampling & Boltzmann generator \\ Free energy difference / \(k_{B}T\) & \(5.31\) & \(5.00\) & \(4.10\pm 0.26\) & \(4.10\pm 0.08\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Dimensionless free energy differences for the slowest transition of alanine dipeptide estimated from various methods. Umbrella sampling yields a converged reference solution. Errors over five runs.

## Acknowledgements

We gratefully acknowledge support by the Deutsche Forschungsgemeinschaft (SFB1114, Projects No. C03, No. A04, and No. B08), the European Research Council (ERC CoG 772230 "ScaleCell"), the Berlin Mathematics center MATH+ (AA1-6), and the German Ministry for Education and Research (BIFOLD - Berlin Institute for the Foundations of Learning and Data). We gratefully acknowledge the computing time granted by the Resource Allocation Board and provided on the supercomputer Lise at NHR@ZIB and NHR@Gottingen as part of the NHR infrastructure. We thank Maaike Galama, Michele Invernizzi, Jonas Kohler, and Max Schebek for insightful discussions. Moreover, we would like to thank Lea Zimmermann and Felix Draxler, who tested our code and brought to our attention a critical bug.

## References

* Kingma et al. [2014] Durk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised learning with deep generative models. _Advances in neural information processing systems_, 27, 2014.
* Dinh et al. [2017] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. In _International Conference on Learning Representations_, 2017.
* Kingma and Dhariwal [2018] Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018.
* Karras et al. [2020] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 8110-8119, 2020.
* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in Neural Information Processing Systems_, 30, 2017.
* Devlin et al. [2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
* Floridi and Chiriatti [2020] Luciano Floridi and Massimo Chiriatti. Gpt-3: Its nature, scope, limits, and consequences. _Minds and Machines_, 30:681-694, 2020.
* Noe et al. [2019] Frank Noe, Simon Olsson, Jonas Kohler, and Hao Wu. Boltzmann generators -- sampling equilibrium states of many-body systems with deep learning. _Science_, 365:eaaw1147, 2019.
* Jumper et al. [2021] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Zidek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. _Nature_, 596(7873):583-589, 2021.
* Watson et al. [2010] Joseph L. Watson, David Juergens, Nathaniel R. Bennett, Brian L. Trippe, Jason Yim, Helen E. Eisenach, Woody Ahern, Andrew J. Borst, Robert J. Ragotte, Lukas F. Milles, Basile I. M. Wicky, Nikita Hanikel, Samuel J. Pellock, Alexis Courbet, William Sheffler, Jue Wang, Preetham Venkatesh, Isaac Sappington, Susana Vazquez Torres, Anna Lauko, Valentin De Bortoli, Emile Mathieu, Regina Barzilay, Tommi S. Jaakkola, Frank DiMaio, Minkyung Baek, and David Baker. Broadly applicable and accurate protein design by integrating structure prediction networks and diffusion generative models. _bioRxiv_, 2022.
* Tabak et al. [2010] Esteban G Tabak, Eric Vanden-Eijnden, et al. Density estimation by dual ascent of the log-likelihood. _Communications in Mathematical Sciences_, 8(1):217-233, 2010.
* Tabak and Turner [2013] Esteban G Tabak and Cristina V Turner. A family of nonparametric density estimation algorithms. _Communications on Pure and Applied Mathematics_, 66(2):145-164, 2013.

* [13] George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. _CoRR_, 2019.
* [14] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In _International conference on machine learning_, pages 1530-1538. PMLR, 2015.
* [15] Gurtej Kanwar, Michael S. Albergo, Denis Boyda, Kyle Cranmer, Daniel C. Hackett, Sebastien Racaniere, Danilo Jimenez Rezende, and Phiala E. Shanahan. Equivariant flow-based sampling for lattice gauge theory. _Phys. Rev. Lett._, 125:121601, Sep 2020.
* [16] Isay Katsman, Aaron Lou, Derek Lim, Qingxuan Jiang, Ser Nam Lim, and Christopher M De Sa. Equivariant manifold flows. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 10600-10612. Curran Associates, Inc., 2021.
* [17] Jonas Kohler, Leon Klein, and Frank Noe. Equivariant flows: exact likelihood generative learning for symmetric densities. In _International conference on machine learning_, pages 5361-5370. PMLR, 2020.
* [18] Victor Garcia Satorras, Emiel Hoogeboom, Fabian Fuchs, Ingmar Posner, and Max Welling. E(n) equivariant normalizing flows. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 4181-4192. Curran Associates, Inc., 2021.
* [19] Jonas Kohler, Michele Invernizzi, Pim de Haan, and Frank Noe. Rigid body flows for sampling molecular crystal structures. In _International Conference on Machine Learning, ICML 2023_, volume 202 of _Proceedings of Machine Learning Research_, pages 17301-17326. PMLR, 2023.
* [20] Jonas Kohler, Andreas Kramer, and Frank Noe. Smooth normalizing flows. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 2796-2809. Curran Associates, Inc., 2021.
* [21] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In _The Eleventh International Conference on Learning Representations_, 2023.
* [22] Alexander Tong, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Kilian Fatras, Guy Wolf, and Yoshua Bengio. Conditional flow matching: Simulation-free dynamic optimal transport. _arXiv preprint arXiv:2302.00482_, 2023.
* [23] Yuxuan Song, Jingjing Gong, Minkai Xu, Ziyao Cao, Yanyan Lan, Stefano Ermon, Hao Zhou, and Wei-Ying Ma. Equivariant flow matching with hybrid probability transport for 3d molecule generation. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [24] Laurence Illing Midgley, Vincent Stimper, Javier Antoran, Emile Mathieu, Bernhard Scholkopf, and Jose Miguel Hernandez-Lobato. SE(3) equivariant augmented coupling flows. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [25] Manuel Dibak, Leon Klein, Andreas Kramer, and Frank Noe. Temperature steerable flows and Boltzmann generators. _Phys. Rev. Res._, 4:L042005, Oct 2022.
* [26] Peter Wirnsberger, Andrew J Ballard, George Papamakarios, Stuart Abercrombie, Sebastien Racaniere, Alexander Pritzel, Danilo Jimenez Rezende, and Charles Blundell. Targeted free energy estimation via learned mappings. _The Journal of Chemical Physics_, 153(14):144112, 2020.
* [27] Laurence Illing Midgley, Vincent Stimper, Gregor N. C. Simm, Bernhard Scholkopf, and Jose Miguel Hernandez-Lobato. Flow annealed importance sampling bootstrap. In _The Eleventh International Conference on Learning Representations_, 2023.
* [28] Xinqiang Ding and Bin Zhang. Deepbar: A fast and exact method for binding free energy computation. _Journal of Physical Chemistry Letters_, 12:2509-2515, 3 2021.

* [29] Hao Wu, Jonas Kohler, and Frank Noe. Stochastic normalizing flows. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 5933-5944. Curran Associates, Inc., 2020.
* [30] Xinqiang Ding and Bin Zhang. Computing absolute free energy with deep generative models. _Biophysical Journal_, 120(3):195a, 2021.
* [31] Andrea Rizzi, Paolo Carloni, and Michele Parrinello. Multimap targeted free energy estimation, 2023.
* [32] Andrea Rizzi, Paolo Carloni, and Michele Parrinello. Targeted free energy perturbation revisited: Accurate free energies from mapped reference potentials. _Journal of Physical Chemistry Letters_, 12:9449-9454, 2021.
* [33] Michele Invernizzi, Andreas Kramer, Cecilia Clementi, and Frank Noe. Skipping the replica exchange ladder with normalizing flows. _The Journal of Physical Chemistry Letters_, 13:11643-11649, 2022.
* [34] Leon Klein, Andrew Y. K. Foong, Tor Erlend Fjelde, Bruno Kacper Mlodozeniec, Marc Brockschmidt, Sebastian Nowozin, Frank Noe, and Ryota Tomioka. Timewarp: Transferable acceleration of molecular dynamics by learning time-coarsened dynamics. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [35] Emiel Hoogeboom, Victor Garcia Satorras, Clement Vignac, and Max Welling. Equivariant diffusion for molecule generation in 3D. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 8867-8887. PMLR, 17-23 Jul 2022.
* [36] Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. Geodiff: A geometric diffusion model for molecular conformation generation. In _International Conference on Learning Representations_, 2022.
* [37] Bowen Jing, Gabriele Corso, Jeffrey Chang, Regina Barzilay, and Tommi Jaakkola. Torsional diffusion for molecular conformer generation. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 24240-24253. Curran Associates, Inc., 2022.
* [38] Mathias Schreiner, Ole Winther, and Simon Olsson. Implicit transfer operator learning: Multiple time-resolution models for molecular dynamics. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [39] Chris Finlay, Jorn-Henrik Jacobsen, Levon Nurbekyan, and Adam Oberman. How to train your neural ode: the world of jacobian and kinetic regularization. In _International conference on machine learning_, pages 3154-3164. PMLR, 2020.
* [40] Alexander Tong, Jessie Huang, Guy Wolf, David Van Dijk, and Smita Krishnaswamy. Trajectorynet: A dynamic optimal transport network for modeling cellular dynamics. In _International conference on machine learning_, pages 9526-9536. PMLR, 2020.
* [41] Derek Onken, Samy Wu Fung, Xingjian Li, and Lars Ruthotto. Ot-flow: Fast and accurate continuous normalizing flows via optimal transport. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 9223-9232, 2021.
* [42] Michael Samuel Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In _The Eleventh International Conference on Learning Representations_, 2023.
* [43] Xingchao Liu, Chengyue Gong, and qiang liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In _The Eleventh International Conference on Learning Representations_, 2023.
* [44] Aram-Alexandre Pooladian, Heli Ben-Hamu, Carles Domingo-Enrich, Brandon Amos, Yaron Lipman, and Ricky Chen. Multisample flow matching: Straightening flows with minibatch couplings. _arXiv preprint arXiv:2304.14772_, 2023.

* [45] David Alvarez-Melis, Stefanie Jegelka, and Tommi S. Jaakkola. Towards optimal transport with global invariances. In Kamalika Chaudhuri and Masashi Sugiyama, editors, _Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics_, volume 89 of _Proceedings of Machine Learning Research_, pages 1870-1879. PMLR, 16-18 Apr 2019.
* [46] Jonas Kohler, Yaoyi Chen, Andreas Kramer, Cecilia Clementi, and Frank Noe. Flow-matching: Efficient coarse-graining of molecular dynamics without forces. _Journal of Chemical Theory and Computation_, 19(3):942-952, 2023.
* [47] George Papamakarios, Eric T Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. _J. Mach. Learn. Res._, 22(57):1-64, 2021.
* [48] Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. In _Advances in neural information processing systems_, pages 6571-6583, 2018.
* [49] Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, and David Duvenaud. Scalable reversible generative models with free-form continuous dynamics. In _International Conference on Learning Representations_, 2019.
* [50] Danilo Jimenez Rezende, Sebastien Racaniere, Irina Higgins, and Peter Toth. Equivariant Hamiltonian flows. _arXiv preprint arXiv:1909.13739_, 2019.
* [51] Kilian Fatras, Younes Zine, Szymon Majewski, Remi Flamary, Remi Gribonval, and Nicolas Courty. Minibatch optimal transport distances; analysis and applications. _arXiv preprint arXiv:2101.01792_, 2021.
* [52] Harold W Kuhn. The hungarian method for the assignment problem. _Naval research logistics quarterly_, 2(1-2):83-97, 1955.
* [53] Wolfgang Kabsch. A solution for the best rotation to relate two sets of vectors. _Acta Crystallographica Section A: Crystal Physics, Diffraction, Theoretical and General Crystallography_, 32(5):922-923, 1976.
* [54] Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E (n) equivariant graph neural networks. In _International conference on machine learning_, pages 9323-9332. PMLR, 2021.
* [55] John R Dormand and Peter J Prince. A family of embedded runge-kutta formulae. _Journal of computational and applied mathematics_, 6(1):19-26, 1980.
* [56] Christoph Bannwarth, Sebastian Ehlert, and Stefan Grimme. Gfn2-xtb--an accurate and broadly parametrized self-consistent tight-binding quantum chemical method with multipole electrostatics and density-dependent dispersion contributions. _Journal of Chemical Theory and Computation_, 15(3):1652-1671, 2019. PMID: 30741547.
* [57] Jerome M Kurtzberg. On approximation methods for the assignment problem. _Journal of the ACM (JACM)_, 9(4):419-439, 1962.
* [58] Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael John Lamarre Townshend, and Ron Dror. Learning from protein structure with geometric vector perceptrons. In _International Conference on Learning Representations_, 2021.
* [59] Kristof Schutt, Oliver Unke, and Michael Gastegger. Equivariant message passing for the prediction of tensorial properties and molecular spectra. In _International Conference on Machine Learning_, pages 9377-9388. PMLR, 2021.
* [60] Yi-Lun Liao and Tess Smidt. Equiformer: Equivariant graph attention transformer for 3d atomistic graphs. In _The Eleventh International Conference on Learning Representations_, 2023.
* [61] Johannes Gasteiger, Florian Becker, and Stephan Gunnemann. Gemnet: Universal directional graph neural networks for molecules. _Advances in Neural Information Processing Systems_, 34:6790-6802, 2021.

* [62] Weitao Du, Yuanqi Du, Limei Wang, Dieqiao Feng, Guifeng Wang, Shuiwang Ji, Carla Gomes, and Zhi-Ming Ma. A new perspective on building efficient and expressive 3d equivariant graph neural networks. _arXiv preprint arXiv:2304.04757_, 2023.
* [63] Berend Zwartsenberg, Adam Scibior, Matthew Niedoba, Vasileios Lioutas, Yunpeng Liu, Justice Sefas, Setaren Dabiri, Jonathan Wilder Lavington, Trevor Campbell, and Frank Wood. Conditional permutation invariant flows. _arXiv preprint arXiv:2206.09021_, 2022.
* [64] Marin Bilos and Stephan Gunnemann. Equivariant normalizing flows for point processes and sets, 2021.
* [65] Yang Li, Haidong Yi, Christopher Bender, Siyuan Shan, and Junier B Oliva. Exchangeable neural ode for set modeling. _Advances in Neural Information Processing Systems_, 33:6936-6946, 2020.
* [66] Isay Katsman, Aaron Lou, Derek Lim, Qingxuan Jiang, Ser Nam Lim, and Christopher M De Sa. Equivariant manifold flows. _Advances in Neural Information Processing Systems_, 34:10600-10612, 2021.
* [67] Pierre Monteiller, Sebastian Claici, Edward Chien, Farzaneh Mirzazadeh, Justin M Solomon, and Mikhail Yurochkin. Alleviating label switching with optimal transport. _Advances in Neural Information Processing Systems_, 32, 2019.
* [68] Yann Brenier. Polar factorization and monotone rearrangement of vector-valued functions. _Communications on pure and applied mathematics_, 44(4):375-417, 1991.
* [69] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In _Advances in Neural Information Processing Systems 32_, pages 8024-8035. Curran Associates, Inc., 2019.
* [70] Michael Poli, Stefano Massaroli, Atsushi Yamashita, Hajime Asama, Jinkyoo Park, and Stefano Ermon. Torchdyn: Implicit models and neural numerical methods in pytorch. In _Neural Information Processing Systems, Workshop on Physical Reasoning and Inductive Biases for the Real World_, volume 2, 2021.
* [71] Remi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya, Aurelie Boisbunon, Stanislas Chambon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier, Leo Gautheron, Nathalie T.H. Gayraud, Hicham Janati, Alain Rakotomamonjy, Ievgen Redko, Antoine Rolet, Antony Schutz, Vivien Seguy, Danica J. Sutherland, Romain Tavenard, Alexander Tong, and Titouan Vayer. Pot: Python optimal transport. _Journal of Machine Learning Research_, 22(78):1-8, 2021.
* [72] Peter Eastman, Jason Swails, John D Chodera, Robert T McGibbon, Yutong Zhao, Kyle A Beauchamp, Lee-Ping Wang, Andrew C Simmonett, Matthew P Harrigan, Chaya D Stern, et al. Openmm 7: Rapid development of high performance algorithms for molecular dynamics. _PLoS computational biology_, 13(7):e1005659, 2017.
* [73] Ask Hjorth Larsen, Jens Jorgen Mortensen, Jakob Blomqvist, Ivano E Castelli, Rune Christensen, Marcin Dulak, Jesper Friis, Michael N Groves, Bjork Hammer, Cory Hargus, Eric D Hermes, Paul C Jennings, Peter Bjerre Jensen, James Kermode, John R Kitchin, Esben Leonhard Kolsbjerg, Joseph Kubal, Kristen Kaasbjerg, Steen Lysgaard, Jon Bergmann Maronsson, Tristan Maxson, Thomas Olsen, Lars Pastewka, Andrew Peterson, Carsten Rostgaard, Jakob Schiotz, Ole Schutt, Mikkel Strange, Kristian S Thygesen, Tejs Vegge, Lasse Vilhelmsen, Michael Walter, Zhenhua Zeng, and Karsten W Jacobsen. The atomic simulation environment--a python library for working with atoms. _Journal of Physics: Condensed Matter_, 29(27):273002, 2017.
* [74] H Wiegand. Kish, l.: Survey sampling. john wiley & sons, inc., new york, london 1965, ix+ 643 s., 31 abb., 56 tab., preis 83 s., 1968.

## Supplementary Material

### Additional experiments

#### Equivariant flows can achieve better paths than they are trained for

We make an interesting observation regarding the performance of equivariant flows trained with the OT flow matching objective. As shown in Figure 4a-c, these flows generate integration paths that are significantly shorter compared to their training counterparts. While they do not learn the OT path and exhibit curved trajectories with notable directional changes (as seen in Figure 1d and Figure 2a), an interesting finding is the absence of path crossings. Any permutation of the samples would increase the distance between the latent sample and the push-forward sample. To quantify the length of these paths, we compute the Euclidean distance between the latent sample and the push-forward sample. The relatively short path lengths can be attributed, in part, to the architecture of the flow model. As the flow is equivariant with respect to permutations, it prevents the crossing of particle paths and forces the particles to change the direction instead, as an average vector field is learned.

#### Memory requirement

As the system size increases, the memory requirements for likelihood training become prohibitively large. In the case of the LJ55 system, even a batch size of 5 necessitates more than 24 GB of memory, making effective training infeasible. In contrast, utilizing flow matching enables training the same model with a batch size of 256 while requiring less than 12 GB of memory.

For the alanine dipeptide system, likelihood training with a batch size of 8 demands more than 12 GB of memory, and a batch size of 16 exceeds 24 GB. On the other hand, employing flow matching allows training the same model with a batch size of 256, consuming less than 3 GB of memory.

The same holds for energy based training, as we need to evaluate Equation (3) and Equation (4) as well during training. Therefore, we opt to solely utilize flow matching for these two datasets.

#### Comparison with prior work on equivariant flows

Both [17] and [18] evaluate their models on the DW4 and LJ13 systems. However, [17] do not provide the numerical values for the negative log likelihood (NLL), only presenting them visually. On the other hand, [18] use a small test set for both experiments. Additionally, their test set for LJ13 is biased as it originates from the burn-in phase of the sampling.

Although the test sets for both systems are insufficient, we still compare our OT flow matching and equivariant OT flow matching to these previous works in Table 3. It should be noted that the value range differs significantly from the values reported in Table 1, as the likelihood in this case includes the normalization constant of the prior distribution.

#### Alanine dipeptide - free energy difference

Here we present additional results for the free energy computation of alanine dipeptide, as discussed in Section 6.3. The effective sample size for the trained models is \(0.50\pm 0.13\%\), which is lower due to training on the biased dataset (compare to Table 1). We illustrate the Ramachandran plot in

Figure 4: Path lengths distributions for models trained with OT flow matching. a) LJ13 system b) Alanine dipeptide system c) LJ55 system.

Figure 4(a), the energy distribution in Figure 4(b), and the free energy projections for Umbrella sampling, model samples, and relaxed MD samples in Figure 4(c).

We obtain similar results with equivariant OT flow matching, as shown in Table 4.

### Alanine dipeptide integration paths

We compare the integration paths of the models trained on the alanine dipeptide dataset in Figure 5(a),b. Despite the higher number of atoms in the alanine dipeptide system (22 atoms), we observe a comparable difference in the integration paths as observed in the LJ13 system. This similarity arises from the presence of different particle types in the alanine dipeptide, resulting in a similar number of possible permutations compared to the LJ13 system.

### Integration error

The integration error, which arises from the use of a fixed-step integration method (rk4) instead of an adaptive solver (dopri5), is quantified by calculating the sum of absolute differences in sample positions and changes in log probability (dlogp). The discrepancies in position are depicted in Figure 3(d), while the difference in log probability change are presented in Figure 5(c). Both of these are crucial in ensuring the generation of unbiased samples that accurately represent the target Boltzmann distribution of interest.

### Structure minimization

Following a similar approach as the deterministic structure minimization conducted for the LJ13 system in [17], we extend our investigation to the sampled minima of the LJ55 system. We minimize the energy of samples generated by a model trained with equivariant OT flow matching, as well as samples obtained from the test data. The resulting structures represent numerous local minima within the potential energy distribution.

In Figure 5(d), we observe a good agreement between the distributions obtained from both methods. Furthermore, it is noteworthy that both approaches yield samples corresponding to the global minimum. The energies of the samples at \(300\)K are considerably higher than those of the minimized structures.

\begin{table}
\begin{tabular}{l c c c} \hline
**Dataset** & Likelihood [18] & OT flow matching & Equivariant OT flow matching \\ \hline DW4 NLL (\(\downarrow\)) & \(7.48\pm 0.05\) & \(7.21\pm 0.03\) & \(7.18\pm 0.02\) \\ LJ13 NLL (\(\downarrow\)) & \(30.41\pm 0.16\) & \(30.34\pm 0.09\) & \(30.30\pm 0.03\) \\ \end{tabular}
\end{table}
Table 3: Comparison of different training methods for the DW4 and (biased) LJ13 dataset as used in [18] for \(10^{5}\) and \(10^{4}\) training points, respectively.

Figure 5: Alanine dipeptide - free energy experiments. The model is trained on the biased dataset. (a) Ramachandran plot depicting the generated joint marginal distribution over the backbone dihedral angles \(\varphi\) and \(\psi\) after filtering out samples with right-handed chirality and high energies. (b) Energy histograms for samples generated by a flow trained with OT flow matching. (c) Free energy distribution along the slowest transition (\(\varphi\) dihedral angle) computed with umbrella sampling, the equivariant flow, and a relaxed MD trajectory.

### Log weight distribution

Samples generated with a trained Boltzmann generators can be reweighted to the target distribution via Equation (29). Generally, the lower the variance of the log weight distribution, the higher the ESS. Although large outliers might reduce the ESS significantly. We present the log weight distribution for the LJ55 system in Figure 7a, showing that the weight distribution for the BG trained with equivariant OT flow matching has lower variance.

### Approximation methods for equivariant OT flow matching

The approximation given in Equation (16) is in practice performed using the Hungarian algorithm for permutations and the Kabsch algorithm for rotations. However, one could also think of other approximations, such as performing the rotation first or alternating multiple permutations and rotations. We compare different approximation strategies in Figure 7b,c. The baseline reference is computed with an expensive search over the approximation given in Equation (16). Namely, we evaluate Equation (16) for 100 random rotations combined with the global reflection, denoted as \(O_{200}\), for each sample, i.e.

\[\hat{c}(x_{0},x_{1})=\min_{o\in O_{200}(D)}\tilde{c}(x_{0},\rho(o)x_{1}),\]

where \(\tilde{c}(x_{0},x_{1})\) is given by Equation (16). Hence, this is 200 times more expensive than our approach. This baseline should be much closer to the true batch OT solution. Applying our approximation multiple times reduced the transportation cost slightly. Performing the rotations first, lead to inferior results. We observe the same behavior for the other systems. Therefore, we used the approximation given in Equation (16) throughout the experiments, showing that this is sufficient to learn a close approximation of the OT map.

### Different data set sizes

Prior work [17; 18] investigates different training set sizes to train equivariant flows, showing that for simple datasets already quite a few samples are sufficient to achieve good NLL. We test if this also holds for flow matching for the more complex alanine dipeptide system. To this end, we run our flow matching experiments also for smaller data set sizes of \(10000\) and \(1000\). We report our findings in Table 5. In agreement with prior work, the NLL becomes worse the smaller the dataset. The same is true for the ESS. However, the sampled integration path lengths are nearly independent on the dataset size.

\begin{table}
\begin{tabular}{c c c} \hline \hline  & Umbrella sampling & Equivariant OT flow matching \\ Free energy difference / \(k_{B}T\) & \(4.10\pm 0.26\) & \(3.95\pm 0.19\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Dimensionless free energy differences for the slowest transition of alanine dipeptide estimated with a Boltzmann Generator trained with the equivariant OT flow matching objective. Umbrella sampling yields a converged reference solution. Errors over five runs.

Figure 6: (a,b) Integration paths for alanine dipeptide for models trained with OT flow matching and equivariant OT flow matching, respectively. The color code for the atoms is as usual: gray - H, black - C, blue - N, red - O. (c) Integration error for a fixed step integrator for the LJ55 system. (d) Minimized samples from a model trained on the LJ55 system with equivariant OT flow matching.

### Naive flow matching

We provide naive flow matching, i.e. flow matching without the OT reordering, as an additional baseline. Naive flow matching results in even longer integration paths, while the ESS and likelihoods are close to the results of OT flow matching, as shown in Table 6. Flow matching with a non equivariant architecture, e.g. a fully connected neural network, failed for all systems but DW4 and is hence not reported.

## Appendix B Proofs and derivations

### Equivariant OT flow matching

We show that our equivariant OT flow matching converges to the OT solution, loosely following the theory in [67] and extending it to infinite groups. Let \(G\) be a compact (topological) group that acts on an Euclidean \(n\)-space \(X\) by isometries, i.e., \(d(g\cdot x,g\cdot y)=d(x,y),\) where \(d\) is the Euclidean distance. This definition includes the symmetric group \(S_{n}\), the orthogonal group \(O(n),\) and all their subgroups. Consider the OT problem of minimizing the cost function \(c(x,y)=d(x,y)^{p},\)\(p\geq 1\) between \(G\)-invariant measures \(\nu_{1}\) and \(\nu_{2}.\)

Let us further define the pseudometric \(\tilde{d}(x,y)^{p}=\min_{g\in G}d(x,g\cdot y)^{p}.\) We want to show that any OT map \(T:X\to X\) for cost functions \(c(x,y)=d(x,y)^{p}\) is also an OT map for \(\tilde{c}(x,y)=\tilde{d}(x,y)^{p}.\)

**Lemma 1**.: _Let \(\nu_{1}\) and \(\nu_{2}\) be \(G\)-invariant measures, then there exists an \(G\)-invariant OT plan \(\pi(g\cdot x,g\cdot y)=\pi(x,y).\)_

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Dataset** & **NLL** (\(\downarrow\)) & **ESS** (\(\uparrow\)) & **Path length** (\(\downarrow\)) \\ \cline{2-4} DW4 & \(1.68\pm 0.01\) & \(93.01\pm 0.12\%\) & \(3.41\pm 0.02\) \\ \cline{2-4} LJ13 & \(-16.10\pm 0.01\) & \(57.55\pm 2.20\%\) & \(3.77\pm 0.01\) \\ \cline{2-4} LJ55 & \(-88.44\pm 0.03\) & \(3.15\pm 1.04\%\) & \(8.66\pm 0.03\) \\ \cline{2-4} Alanine dipeptide & \(-107.56\pm 0.09\) & \(1.42\pm 0.51\%\) & \(11.00\pm 0.02\) \\ \hline \hline \end{tabular}
\end{table}
Table 6: Results for naive flow matching. Same hyperparameters as for OT flow matching. Errors over three runs.

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Training type** & **NLL** (\(\downarrow\)) & **ESS** (\(\uparrow\)) & **Path length** (\(\downarrow\)) \\ \cline{2-4}  & \multicolumn{3}{c}{Alanine dipeptide - \(1000\) training samples} \\ \cline{2-4} OT flow matching & \(-\mathbf{104.97\pm 0.05}\) & \(0.25\pm 0.04\%\) & \(10.14\pm 0.05\) \\ Eq OT flow matching & \(-103.38\pm 0.03\) & \(\mathbf{0.39\pm 0.08\%}\) & \(\mathbf{9.27\pm 0.02}\) \\ \cline{2-4}  & \multicolumn{3}{c}{Alanine dipeptide - \(10000\) training samples} \\ \cline{2-4} OT flow matching & \(-\mathbf{105.16\pm 0.17}\) & \(0.33\pm 0.07\%\) & \(10.13\pm 0.02\) \\ Eq OT flow matching & \(-103.36\pm 0.15\) & \(0.40\pm 0.06\%\) & \(\mathbf{9.32\pm 0.01}\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Comparison of flows trained with different training set sizes. Errors over three runs.

Figure 7: (a) Log weight distribution for samples generated for the LJ55 system. (b, c) Comparison of different approximation methods to approximate Equation (16) for alanine dipeptide.

Proof.: Let \(\pi^{\prime}\in\Pi(\nu_{1},\nu_{2})\) be a (not necessarily invariant) OT plan. We average the OT plan over the group

\[\bar{\pi}(x,y)=\int_{G}\pi^{\prime}(g\cdot x,g\cdot y)d\mu(g), \tag{21}\]

where \(\mu\) is the Haar measure on \(G\).

An important property of the Haar integral is that

\[\int_{G}f(hg)d\mu(g)=\int_{G}f(g)d\mu(g) \tag{22}\]

for any measurable function \(f\) and \(h\in G\). Therefore the average plan is \(G\)-invariant:

\[\bar{\pi}(h\cdot x,h\cdot y)=\int_{G}\pi^{\prime}(hg\cdot x,hg\cdot y)d\mu(g)= \int_{G}\pi^{\prime}(g\cdot x,g\cdot y)d\mu(g)=\bar{\pi}(x,y).\]

Next we compute the marginals

\[\int_{X}\bar{\pi}(x,dy) =\int_{X}\int_{G}\pi^{\prime}(g\cdot x,g\cdot dy)d\mu(g)=\int_{G} \int_{X}\pi^{\prime}(g\cdot x,g\cdot dy)d\mu(g)\] \[=\int_{G}\nu_{1}(g\cdot x)d\mu(g)=\int_{G}\nu_{1}(x)d\mu(g)=\nu_{ 1}(x)\]

and analogously \(\int_{X}\bar{\pi}(dx,y)=\nu_{2}(y)\). Hence, the average plan is also a coupling, \(\bar{\pi}\in\Pi(\nu_{1},\nu_{2})\).

The cost of the average plan is

\[\int_{X^{2}} c(x,y)\bar{\pi}(dx,dy)\] \[=\int_{X^{2}}c(x,y)\int_{G}\pi^{\prime}(g\cdot dx,g\cdot dy)d\mu(g) (\text{Definition of }\bar{\pi})\] \[=\int_{G}\int_{X^{2}}c(x,y)\pi^{\prime}(g\cdot dx,g\cdot dy)d\mu(g) (\text{Fubini's theorem})\] \[=\int_{G}\int_{X^{2}}c(g^{-1}\cdot x,g^{-1}\cdot y)\pi^{\prime}( dx,dy)d\mu(g) (\text{Volume-preserving substitution})\] \[=\int_{G}\int_{X^{2}}c(x,y)\pi^{\prime}(dx,dy)d\mu(g) (\text{Isometric group action})\] \[=\int_{X^{2}}c(x,y)\pi^{\prime}(dx,dy),\]

and hence the average plan is optimal.

**Lemma 2**.: _Let \(T\colon x\mapsto y\) be an \(G\)-equivariant OT map, \(x\in X\), and \(g^{*}:=\operatorname*{arg\,min}_{g\in G}c(x,g\cdot T(x))\). Then \(g^{*}=\operatorname{id}\)._

Proof.: \(T\) maps the orbit of \(x\), denoted as \(G\cdot x=\{g\cdot x|g\in G\}\), to the corresponding orbit of \(T(x)\), due to equivariance.

If there exists an \(x^{*}\in X\) such that \(g^{*}\neq\operatorname{id}\) then this property holds true for the entire orbit of \(x^{*}\).

Now, let us define an \(G\)-equivariant map \(T^{\prime}\colon x\mapsto y\) such that \(T^{\prime}(G\cdot x^{*})=g^{*}T(G\cdot x^{*})\) and \(T^{\prime}(x)=T(x)\) everywhere else. However, in this case, \(T\) would not be the OT map, as \(c(x^{*},T^{\prime}(x^{*}))<c(x^{*},T(x^{*}))\). Therefore, it follows that \(g^{*}=\operatorname{id}\) holds for the \(G\)-equivariant OT map. 

Discrete optimal transportConsider a finite number of \(\operatorname{id}\) samples from the two \(G\)-invariant distributions \(\nu_{1},\nu_{2}\), denoted as \(\{x_{i}\}_{i=1}^{n}\), \(\{y_{i}\}_{i=1}^{n}\), respectively. Let further \(\pi(\nu_{1},\nu_{2})\) be an \(G\)-invariant OT plan. The discrete optimal transport problem seeks the solution to

\[\gamma^{*}=\operatorname*{arg\,min}_{\gamma\in\mathcal{P}}\sum_{ij}\gamma_{ij }M_{ij}, \tag{23}\]where \(M_{ij}=c(x_{i},y_{j})\) represents the cost matrix and \(\mathcal{P}\) is the set of probability matrices defined as \(\mathcal{P}=\{\gamma\in(\mathbb{R}^{+})^{n\times n}|\gamma\mathbf{1}_{n}=\mathbf{ 1}_{n}/n,\gamma^{T}\mathbf{1}_{n}=\mathbf{1}_{n}/n\}\). The total transportation cost \(C\) is given by

\[C=\min_{\gamma\in\mathcal{P}}\sum_{ij}\gamma_{ij}M_{ij}. \tag{24}\]

Approximately generating sample pairs from the OT plan \(\pi(\nu_{1},\nu_{2})\), as required for flow matching (Section 3.4), can be achieved by utilizing the probabilities assigned to sample pairs by \(\gamma^{*}\). Let further

\[\tilde{\gamma}^{*}=\operatorname*{arg\,min}_{\gamma\in\mathcal{P}}\sum_{ij} \gamma_{ij}\tilde{M}_{ij} \tag{25}\]

denote the optimal transport matrix for the cost function \(\tilde{c}(x,y)\) and \(\tilde{C}\) the corresponding transportation cost. Sample pairs \((x_{i},y^{\prime}_{j})\) generated according to \(\tilde{\gamma}^{*}\) are orientated along their orbits to have minimal cost, i.e.

\[(x_{i},y^{\prime}_{j})=(x_{i},g^{*}y_{j}),\quad g^{*}:=\operatorname*{arg\, min}_{g\in G}c(x_{i},g\cdot y_{j}). \tag{26}\]

Note that the group action \(g\) does not change the probability under the marginals, as \(\nu_{1}\) and \(\nu_{2}\) are both \(G\)-invariant.

**Lemma 3**.: _Let \(\gamma^{*}\) and \(\tilde{\gamma}^{*}\) be defined as in Equation (23) and Equation (25), respectively. Then, sample pairs generated based on \(\tilde{\gamma}^{*}\) have a lower average cost than sample pairs generated according to \(\gamma^{*}\)._

Proof.: The total transportation cost \(\tilde{C}\) is always less than or equal to \(C\) since the cost function \(\tilde{c}(x_{i},y_{j})\leq c(x_{i},y_{j})\) for all \(i\) and \(j\), by definition. Note that as the number of samples approaches infinity, the inequality between \(\tilde{C}\) and \(C\) becomes an equality. 

Note that in practice, samples generated wrt \(\tilde{\gamma}^{*}\) are a much better approximation, as shown in Section 6. Moreover, by construction, sample pairs generated according to \(\tilde{\gamma}^{*}\) satisfy \(\arg\min_{g\in G}c(x_{i},g\cdot y_{j})=\operatorname{id}\), as indicated in Equation (26). This property, which is also true for samples from the \(G\)-invariant OT plan according to Lemma 2, ensures that the generated pairs are correctly matched within their respective orbits. However, sample pairs generated according to \(\gamma^{*}\) do not generally possess this property.

We now combine our findings in the following Theorem.

**Theorem 1**.: _Let \(T\colon x\mapsto y\) be an OT map between \(G\)-invariant measures \(\nu_{1}\) and \(\nu_{2}\), using the cost function \(c\). Then_

1. \(T\) _is_ \(G\)_-equivariant and the corresponding OT plan_ \(\pi(\nu_{1},\nu_{2})\) _is_ \(G\)_-invariant._
2. _For all pairs_ \((x,T(x))\) _and_ \(y\in G\cdot T(x):\)__ \[c(x,T(x))=\int_{G}c(g\cdot x,g\cdot T(x))d\mu(g)=\min_{g\in G}c(x,g\cdot y)\] (27)
3. \(T\) _is also an OT map for the cost function_ \(\tilde{c}\)_._

Proof.: We prove the parts of Theorem 1 in order.

1. Follows from Lemma 1 and the uniqueness of the OT plan for convex cost functions [68].
2. Follows from Lemma 2.
3. Follows directly from 2., Lemma 2, and \[\int_{X^{2}}\min_{g\in G}c(x,g\cdot y)\pi(dx,dy)=\int_{X^{2}}c(x,y)\pi(dx,dy).\] (28) However, this OT map is not unique.

Hence, we can generate better sample pairs, approximating the OT plan \(\pi(\nu_{1},\nu_{2})\), by using \(\tilde{c}\) as a cost function instead of \(c\). Training pairs \((x_{i},y^{\prime}_{j})\) are orientated along their orbits to have minimal cost (Equation (26)).

These findings provide the motivation for our proposed equivariant flow matching objective, as described in Section 4, as well as the utilization of \(G\)-equivariant normalizing flows, as discussed in Section 5.

### Mean free update for identical particles

The update, described in Section 5, conserves the geometric center if all particles are of the same type.

Proof.: The update of layer \(l\) does not alter the geometric center / center mass if all particles are of the same type as \(m^{l}_{ij}=m^{l}_{ji}\) and then

\[\bar{x}^{l+1} =\sum_{i}x^{l+1}_{i}=\sum_{i}x^{l}_{i}+\sum_{i,j\neq i}\frac{(x^{ l}_{i}-x^{l}_{j})}{d_{ij}+1}\phi_{d}(m^{l}_{ij})\] \[=\bar{x}^{l}+\sum_{i,j>i}\frac{\left(x^{l}_{i}-x^{l}_{j}\right)+ \left(x^{l}_{j}-x^{l}_{i}\right)}{d_{ij}+1}\phi_{d}(m^{l}_{ij})\] \[=\bar{x}^{l}\]

### Selective reweighting for efficient computation of observables

As normalizing flows are exact likelihood models, we can reweight the push-forward distribution to the target distribution of interest. This allows the unbiased evaluation of expectation values of observables \(O\) through importance sampling

\[\langle O\rangle_{\mu}=\frac{\mathbb{E}_{x_{1}\sim\tilde{p}_{1}(x_{1})}[w(x_{1 })O(x_{1})]}{\mathbb{E}_{x_{1}\sim\tilde{p}_{1}(x_{1})}[w(x_{1})]},\quad w(x_{1 })=\frac{\mu(x_{1})}{\tilde{p}_{1}(x_{1})}. \tag{29}\]

## Appendix C Technical details

### Code libraries

Flow models and training are implemented in _Pytorch_[69] using the following code libraries: _bgflow_[8, 17], _torchdyn_[70], _Pot: Python optimal transport_[71], and the code corresponding to [18]. The MD simulations are run using _OpenMM_[72], _ASE_[73], and _xtb-python_[56].

The code will be integrated in the bgflow library [https://github.com/noegroup/bgflow](https://github.com/noegroup/bgflow).

### Benchmark systems

For the DW4 and LJ13 system, we choose the same parameters as in [17, 18].

Dw4The energy \(U(x)\) for the DW4 system is given by

\[U^{\text{DW}}(x)=\frac{1}{2\tau}\sum_{i,j}a\left(d_{ij}-d_{0}\right)+b\left(d _{ij}-d_{0}\right)^{2}+c\left(d_{ij}-d_{0}\right)^{4}, \tag{30}\]

where \(d_{ij}\) is the Euclidean distance between particle \(i\) and \(j\), the parameters are chosen as \(a=0,b=-4,c=0.9,d_{0}=4\), and \(\tau=1\), which is a dimensionless temperature factor.

#### lj13

The energy \(U(x)\) for the LJ13 system is given by

\[U^{\text{LJ}}(x)=\frac{\epsilon}{2\tau}\left[\sum_{i,j}\left(\left(\frac{r_{m}}{d _{ij}}\right)^{12}-2\left(\frac{r_{m}}{d_{ij}}\right)^{6}\right)\right], \tag{31}\]

with parameters \(r_{m}=1\), \(\epsilon=1\), and \(\tau=1\).

#### lj55

We choose the same parameters as for the LJ13 system.

Both the LJ13 and the LJ55 system were generated with MCMC with \(1000\) parallel chains, where each chain is run for \(10000\) steps after a long burn-in phase of \(200000\) steps starting from a random generated initial state.

Alanine dipeptideThe alanine dipeptide training data at temperature \(T=300\)K set is generated through two steps: (i) Firstly, we perform an MD simulation, using the classical _Amber f99SBildn_ force-field, at \(300\)K for implicit solvent for a duration of \(1\) ms [20] using the openMM library.

(ii) Secondly, we relax \(10^{5}\) randomly selected states from the MD simulation for \(100\) fs each, using the semi-empirical _GFN2-xTB_ force-field [56] and the ASE library [73] with a friction constant of \(0.5\) a.u.

Both simulations use a time step of \(1\) fs. We create a test set in the same way.

Alanine dipeptide umbrella samplingTo accurately estimate the free energy difference, we performed five umbrella sampling simulations along the \(\varphi\) dihedral angle using the semi-empirical _GFN2-xTB_ force-field. The simulations were carried out at 25 equispaced \(\varphi\) angles. For each angle, the simulation was initiated by relaxing the initial state for \(2000\) steps with a high friction value of \(0.5\) a.u. Subsequently, molecular dynamics (MD) simulations were performed for a total of \(10^{6}\) steps using the _GFN2-xTB_ force field, with a friction constant of 0.02 a.u. The system states were recorded every 1000 steps during the second half of the simulation. A timestep of \(1\) fs was utilized for the simulations.

The datasets are available at [https://osf.io/srqg7/?view_only=28deeba0845546fb96d1b2f355db0da5](https://osf.io/srqg7/?view_only=28deeba0845546fb96d1b2f355db0da5).

### Hyperparameters

Depending on the dataset, different model sizes were used, as reported in Table 7. All neural networks \(\phi_{\alpha}\) have one hidden layer with \(n_{\mathrm{hidden}}\) neurons and _SiLU_ activation functions. The embedding \(a_{i}\) is given by a single linear layer with \(n_{\mathrm{hidden}}\) neurons.

We report the used training schedules in Table 8. Note that \(5e\)-\(4\)/\(5e\)-\(5\) in the second row means that the training was started with a learning rate of \(5e\)-\(4\) for \(200\) epochs and then continued with a learning rate of \(5e\)-\(5\) for another \(200\) epochs. All batches were reordered prior to training the model. The only exception is alanine dipeptide trained with equivariant OT flow matching.

### Parallel OT batch generation

As Equation (16) needs to be evaluated for each possible sample pair in a given batch, the computation cost of equivariant OT flow matching is quite high. However, a simple way to speed up the equivariant OT training is to generate the batches beforehand or in parallel to the training process. This process is highly parallelizable and can be performed on CPUs, which are in practice usually more available

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Dataset** & \(L\) & \(n_{\mathrm{hidden}}\) & Num. of parameters \\ \hline DW4, LJ13 & \(3\) & \(32\) & \(22468\) \\ LJ55 & \(7\) & \(64\) & \(204936\) \\ alanine dipeptide & \(5\) & \(64\) & \(147599\) \\ \hline \hline \end{tabular}
\end{table}
Table 7: Model hyperparametersthan GPUs and also in higher numbers. This also allows for larger batch sizes for the equivariant OT model and comes at little additional cost. Hence, scaling equivariant flow matching to even larger systems should not be an issue. The wall-clock time for the reordering of a single batch on a single CPU is reported in Table 9.

### Biasing target samples

In the case of alanine dipeptide, the transition between negative and positive \(\varphi\) dihedral angles is the slowest process, as depicted in Figure 2(d). Since the positive \(\varphi\) state is less probable, we can introduce a bias in our training data to achieve a nearly equal density in both states. This approach aids in obtaining a more precise estimation of the free energy. To ensure a smooth density function, we incorporate weights based on the von Mises distribution \(f_{\mathrm{vM}}\). The weights \(\omega\) are computed along the \(\varphi\) dihedral angle as

\[\omega(\varphi)=150\cdot f_{\mathrm{vM}}\left(\varphi|\mu=1,\kappa=10\right)+1. \tag{32}\]

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \multirow{2}{*}{**Training type**} & \multirow{2}{*}{**Batch size**} & \multirow{2}{*}{**Learning rate**} & \multirow{2}{*}{**Epochs**} & \multirow{2}{*}{**Training time**} \\ \cline{3-3} \cline{5-6}  & & & & DW4 & \\ \hline Likelihood [18] & \(256\) & \(5e\)-\(4\) & \(20\) & \(3\)h \\ OT flow matching & \(256\) & \(5e\)-\(4\)/\(5e\)-\(5\) & \(200\)/\(200\) & \(0.5\)h \\ Equivariant OT flow matching & \(256\) & \(5e\)-\(4\)/\(5e\)-\(5\) & \(200\)/\(200\) & \(0.5\)h \\ \hline \multicolumn{6}{c}{LJ13} \\ \cline{2-6} Likelihood [18] & \(64\) & \(5e\)-\(4\) & \(5\) & \(13\)h \\ OT flow matching & \(256\) & \(5e\)-\(4\)/\(5e\)-\(5\) & \(1000\)/\(1000\) & \(3\)h \\ Equivariant OT flow matching & \(256\) & \(5e\)-\(4\)/\(5e\)-\(5\) & \(1000\)/\(1000\) & \(3\)h \\ \hline \multicolumn{6}{c}{LJ55} \\ \cline{2-6} OT flow matching & \(256\) & \(5e\)-\(4\)/\(5e\)-\(5\) & \(600\)/\(400\) & \(17\)h \\ Equivariant OT flow matching & \(256\) & \(5e\)-\(4\)/\(5e\)-\(5\) & \(600\)/\(400\) & \(17\)h \\ \hline \multicolumn{6}{c}{Alanine dipeptide} \\ \cline{2-6} OT flow matching & \(256\) & \(5e\)-\(4\)/\(5e\)-\(5\) & \(1000\)/\(1000\) & \(6.5\)h \\ Equivariant OT flow matching & \(32\) & \(5e\)-\(4\)/\(5e\)-\(5\) & \(200\)/\(200\) & \(25\)h \\ \hline \hline \end{tabular}
\end{table}
Table 8: Training schedules

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Training type** & **Batch size** & **Wall-clock time** \\ \hline  & & DW4 & \\ \hline \(256\) & & \(3.6\)s \\ \hline \multicolumn{3}{c}{LJ13} \\ \cline{2-4} Equivariant OT flow matching & \(256\) & \(4.5\)s \\ \hline \multicolumn{3}{c}{LJ55} \\ \cline{2-4} OT flow matching & \(256\) & \(0.01\)s \\ \(256\) & \(20.2\)s \\ \hline \multicolumn{3}{c}{Alanine dipeptide} \\ \cline{2-4} OT flow matching & \(256\) & \(0.01\)s \\ \(256\) & \(22.4\)s \\ Equivariant OT flow matching & \(32\) & \(0.4\)s \\ \hline \hline \end{tabular}
\end{table}
Table 9: Wall-clock time to reorder a single batch for the two OT flow matching methods. The times for the DW4 and LJ13 system are below \(0.01\) seconds for OT flow matching and are therefore not reported.

We then draw training samples based on the weighted distribution.

### Effective samples sizes

The effective sample sizes are computed with Kish's equation [74]. We use \(5\times 10^{5}\) samples for the DW4 and LJ13 system, \(2\times 10^{5}\) for alanine dipeptide, and \(1\times 10^{5}\) for the LJ55 system per model.

### Error bars

Error bars in all plots are given by one standard deviation, averaged over 3 runs, if not otherwise indicated. The same applies for all errors reported in the tables. Exceptions are the _Mean batch transport cost_ plots, where we average over \(10\) batches and the _integration error plots_, where we average over \(100\) samples for each number of steps.

### Computing infrastructure

All experiments for the DW4 and LJ13 system were conducted on a _GeForce GTX 1080 Ti_ with 12 GB RAM. The training for alanine dipeptide and the LJ55 system were conducted on a _GeForce RTX 3090_ with 24 GB RAM. Inference was performed on _NVIDIA A100 GPUs_ with 80GB RAM for alanine dipeptide and the LJ55 system.