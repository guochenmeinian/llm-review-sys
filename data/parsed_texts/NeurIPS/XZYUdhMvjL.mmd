# MultiOrg: A Multi-rater Organoid-detection Dataset

Christina Bukas\({}^{1}\), Harshavardhan Subramanian\({}^{1}\), Fenja See\({}^{2}\), Carina Steinchen\({}^{2}\)

**Ivan Ezhov\({}^{3}\), Gowtham Boosarpu\({}^{4}\), Sara Asgharpour\({}^{2}\), Gerald Burgstaller\({}^{2}\) Mareike Lehmann\({}^{2,4}\), Florian Kofler\({}^{1,5,6}\), Marie Piraud\({}^{1}\)\({}^{1}\)Helmholtz AI, Computational Health Center (CHC), Helmholtz Munich, Neuherberg, Germany

\({}^{2}\)Institute of Lung Health and Immunity (LHI), Comprehensive Pneumology Center (CPC),

Helmholtz Munich, Member of the German Center for Lung Research (DZL), Neuherberg, Germany

\({}^{3}\)Technical University of Munich, School of Computation, Information and Technology,

Department of Computer Science, Munich, Germany

\({}^{4}\)Institute for Lung Research, Philipps-University Markup, Universities of Giessen and Marburg Lung Center, Member of the German Center for Lung Research (DZL), Marburg, Germany

\({}^{5}\) Department of Neuroradiology, Technical University of Munich, Munich, Germany.

\({}^{6}\) Department of Quantitative Biomedicine, University of Zurich, Switzerland.

{christina.bukas,mareike.lehmann,marie.piraud}@helmholtz-munich.de

###### Abstract

High-throughput image analysis in the biomedical domain has gained significant attention in recent years, driving advancements in drug discovery, disease prediction, and personalized medicine. Organoids, specifically, are an active area of research, providing excellent models for human organs and their functions. Automating the quantification of organoids in microscopy images would provide an effective solution to overcome substantial manual quantification bottlenecks, particularly in high-throughput image analysis. However, there is a notable lack of open biomedical datasets, in contrast to other domains, such as autonomous driving, and, notably, only few of them have attempted to quantify annotation uncertainty. In this work, we present _MultiOrg_ a comprehensive organoid dataset tailored for object detection tasks with uncertainty quantification. This dataset comprises over 400 high-resolution 2d microscopy images and curated annotations of more than 60,000 organoids. Most importantly, it includes three label sets for the test data, independently annotated by two experts at distinct time points. We additionally provide a benchmark for organoid detection, and make the best model available through an easily installable, interactive plugin for the popular image visualization tool Napari, to perform organoid quantification.

## 1 Introduction

Accurate and efficient object detection methods in biomedical image analysis are crucial for research and diagnostics. Designing such methods requires diverse, well-curated datasets of high-resolution images reflecting real-world complexities. The annotation of biomedical datasets represents a labor-intensive and subjective process relying on human experts. This work represents a multi-rater organoid dataset designed for benchmarking object detection algorithms in a label-noise-aware setting that embraces the subjectivity in labels.

Organoids are miniature three-dimensional (3d) models of organs grown in vitro from stem cells. They mimic the complexity and functionality of real organs, making them extremely valuable for medical research, disease modeling, and drug testing (Barkauskas et al., 2017; Kim et al., 2020; Ingber, 2022). Organoid cultures, deriving from healthy and diseased or genetically engineered cells and undergoing different conditions and treatments, can be grown for several months (Youk et al.,2020; Huch and Koo, 2015). These high-throughput experiments are monitored via microscopic imaging and, therefore, necessitate fast and objective detection, quantification, and tracking methods (Rios and Clevers, 2018; Du et al., 2023). Detection of organoids in real-world lab-culture images is associated with many challenges (Kassis et al., 2019). Beyond the typical challenges associated with microscopy (out-of-focus, lightning, padding, etc...), those 3d cultures are imaged in 2d, leading to overlapping structures. Organoids can highly vary in size, shape, and appearance (Domenech-Moreno et al., 2023), and be difficult to distinguish from dust and debris present in the culture (Matthews et al., 2022; Keles et al., 2022). Finally, the high number of objects to analyze per image poses a big hurdle to a human prone to distraction and fatigue (Haja et al., 2023). Manual annotation of this data, which is still state-of-the-art (Costa et al., 2021; Wu et al., 2022) is, therefore, error- and bias-prone, which introduces noise in the labels. However, evaluating learning algorithms for organoid detection, involves comparing predicted outcomes to those manual annotations or '_Ground Truth (GT)_' during training and testing. As shown in our previous work, deep learning algorithms can outperform even highly-trained human annotators (Koffer et al., 2021). In complex real-life datasets, understanding the shortcomings that label uncertainty creates in the '_GT_' is, therefore, pivotal before training and benchmarking _Deep Learning (DL)_ models. Moreover, quantifying the label noise by assessing the intra- and inter-rater reliability is crucial to interpret similarity metrics between model predictions and reference annotations (Koffer et al., 2023).

In this work, see Figure 1, we release _MultiOrg_, a large multi-rater 2d microscopy imaging dataset of lung organoids for benchmarking object detection methods. The dataset comprises more than 400 images of an entire microscopy plate well and more than 60,000 annotated organoids, deriving from different biological study setups, with two types of organoids growing under varying conditions. Most importantly, we introduce three unique label sets derived from the two annotators at different times, allowing for the quantification of label noise (see Fig. 2). Such a dataset can enable the community to explore biases in annotations, investigate the effect these have on model training, and promote the active area of research for uncertainty quantification. To our knowledge, this is the second largest organoid dataset to date to be made freely available to the community (Bremer et al., 2022). It is also the first organoid dataset and one of the very few biomedical object-detection datasets to introduce multiple labels (Nguyen et al., 2022; Amgad et al., 2022). We benchmarked this dataset by training and testing four widely established _DL_ models for object detection tasks using both one-stage and two-stage architectures. Finally, along with the dataset and model, we release a tool for quantifying lung organoids, enabling users to visualize and correct the detected organoids before extracting useful features for downstream tasks. This tool solves the bottleneck of manual quantification of lung organoids, enabling high-throughput image analysis for biological studies.

In summary, the contributions of this work are as follows:

* We release _MultiOrg_, an object detection bio-medical dataset of more than 400 microscopy images comprising around 60,000 lung organoids annotated by two expert annotators.
* We provide quantification of label uncertainty through a Kaggle benchmark challenge that evaluates the submissions on the different test label sets.
* We benchmark our dataset on four standard object detection methods, show how performance varies depending on the selected annotations, and release the models on zenodo.
* We release the best model in a napari plugin, _napari-organoid-counter_(Bukas, 2022), which allows users to curate predictions, thus enabling high-throughput analysis.

## 2 Related work

Kassis et al. (2019) proposed _OrganoQuant_, a manually-annotated, human-intestinal-organoid dataset of around 14,000 organoids, along with an object detection pipeline based on Faster R-CNN Ren et al. (2015), to locate and quantify human intestinal organoids in brightfield images. Though object detection performance is satisfactory and the quantification process is robust, inference is performed on cropped patches of a well. Similarly, Matthews et al. (2022) released a dataset of brightfield and phase-contrast microscopy images and proposed an image analysis platform, _OrganoID_, based on U-Net Falk et al. (2019), which segments and tracks different types of organoids. They trained their model on images of pancreatic cancer organoids and validated it on pancreatic, lung, colon, and adenoid cystic carcinoma organoids. This work introduces several types of organoids. However, the dataset is small, including only 66 images featuring 5 to 50 organoids each. In Haja et al. (2023), _OrganelX_ platform was released to enable segmentation of murine liver organoids using Mask-RCNN He et al. (2017). Furthermore, Bian et al. (2021) introduced a high-throughput image dataset of liver organoids for detection and tracking. They also propose a novel deep neural network architecture to track organoids dynamically and detect them quickly and accurately. However, here, too, the dataset size is relatively small, with 75 images containing a total of 6,482 organoids. Bremer et al. (2022) used a multicentric dataset consisting of 729 images containing 90,210 annotated organoids, including multiple organoid systems like liver, intestine, tumor, and lung, and proposes an organoid annotation tool, _GOAT_, which uses Mask R-CNN (He et al., 2017), for unbiased quantification. The corresponding dataset contains six organoid types, generated in four centers and acquired with five microscopes. More recently, Domenech-Moreno et al. (2023) proposed an object detection algorithm, based on YOLO v5 Ultralytics (2021), _Tellin_, to classify and detect intestinal organoids of different types. The tool also enables automated analysis of intestinal organoid morphology and fast and accurate classification of organoids.

_MultiOrg_ is, therefore, the second largest organoid dataset (see Table 1). It is noisier than those introduced above; it is not the densest but contains clumps of organoids and displays an extensive range of sizes, presenting one of the most challenging settings for object detection. We introduce

Figure 1: _MultiOrg_ workflow. a) Dataset creation, b) Multi-rater annotation at time points \(t^{0}\) and \(t^{1}\), c) Model benchmark, and d) Release on Kaggle and napari plugin

Figure 2: Multiple label sets in _MultiOrg_. Full test image (left) and crops of areas A, B, and C overlaid with \(test^{0}\), \(test^{1}_{A}\) and \(test^{1}_{B}\) (right). The square crops are of sizes 1800, 1200, and 500 px. \(test^{0}\) in images 4 and 16 (respectively 24 and 43) originates from Annotator A (resp. B). ’Macros’ are typically noisier, as the cultures initially contain more cells (Appendix A.1.1).We observe a reduction in the number of annotations at time \(t^{1}\), as the annotators do not consider some small organoids that were annotated at \(t^{0}\). In image 24, Annotator B annotates clumps of organoids as one large object at \(t^{1}\). The large structure in image 43 is an experimental matrigel artifact. The image-wise intra-rater Recall scores are 0.776, 0.532, 0.667 and 0.503 for images 4, 16, 24, and 43, respectively (with \(test^{0}\) as \(GT\)).

[MISSING_PAGE_FAIL:5]

\(test_{B}^{1}\)) refer to the re annotation from annotators A (resp. B), at time point \(t^{1}\) on all 55 images of the test set.

### Object detection metrics

We compare the multiple label sets and assess the quality of model predictions using several evaluation metrics. _True Positives (TPs)_, _False Positives (FPs)_, and _False Negatives (FNs)_ are computed for each image, for a given _Intersection-over-Union (IOU)_ threshold, using one of the label sets as the '_GT_'. Their total numbers are then aggregated on the entire test set. For comparing the three available label sets, we compute Precision and Recall at an _IOU_ of 0.5 and the F1-score. While Precision measures the percentage of correct predictions against a considered true label, Recall (i.e., sensitivity) measures the proportion of true positive predictions identified correctly. For evaluating model performance, we use the _Precision-Recall (P-R)_ curves as the primary tool. It consists of Precision and Recall values at different model confidence thresholds, at a fixed _IOU_ threshold (here we use 0.5 unless specified otherwise). We also report _Mean Average Precision (mAP)_, by integrating precision across Recall levels from 0 to 1. We compute them using the standard library (Padilla et al., 2021) which follows the PASCAL VOC challenge technique for interpolating points on the curve (Everingham et al.).

### Multi-rater analysis

We compute inter- and intra-rater uncertainties to quantify annotation variance and assess the consistency of the two raters over time and against each other. Inter-rater scores assess the inconsistency in the assessments made by different raters when evaluating the same image and can permit the detection of biases and different expertise levels. The variability in assessments made by one rater when evaluating the same image multiple times (intra-rater) can permit the detection of errors and ambiguities associated with the complexity of the task.

Figure 3 shows intra-rater scores, with label set \(test^{0}\) used as the _GT_ (note that switching the choice of _GT_ does not impact the F1-score and switchs Recall and Precision). For Annotator A, we compare the annotations for test images 1-22, i.e., \(test^{0}_{A}\) with the corresponding subset of \(test^{1}_{A}\), while for Annotator B we compare annotations of images 23-55, i.e., \(test^{0}_{B}\) with the corresponding subset of \(test^{1}_{B}\). We find that Annotator A is more consistent over time, especially for the 'Normal' studies, which is in line with the reported change of annotation setup of Annotator B. Additionally, annotation of 'Macros' seems more challenging than 'Normal' images. We also find a reduction in the number of annotations at time point \(t^{1}\), already reported in Table 2. The qualitative inspection of Figure 2 suggests that some pseudo labels used as a starting point for all manual annotations were not removed in \(test^{0}\), probably indicating improvement of the annotations over time. Further comparison of the pseudo labels to the label sets can be seen in Table A.9. We also observe that Annotator B, unlike Annotator A, annotated overlapping clumps of organoids differently at \(t^{1}\), which could be the source of the higher reported inconsistency.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline
**Study Type** & \multicolumn{2}{c}{Normal} & \multicolumn{2}{c}{Macros} & \multicolumn{2}{c}{Combined} \\ \cline{2-7}  & \# Images & \# Organoids & \# Images & \# Organoids & \# Images & \# Organoids \\ \hline \multicolumn{7}{c}{**Train set**} \\ \hline \(train_{A}\) & 181 & 30,710 & 15 & 2,669 & 196 & 33,379 \\ \(train_{B}\) & 135 & 20,263 & 25 & 1,781 & 160 & 22,044 \\
**Total** & **316** & **50,973** & **40** & **4,450** & **356** & **55,423** \\ \hline \multicolumn{7}{c}{**Test set**} \\ \hline \(test^{0}_{B}\) & 8 & 1,145 & 14 & 1,865 & 22 & 3,010 \\ \(test^{0}_{B}\) & 20 & 3,020 & 13 & 1,493 & 33 & 4,513 \\
**Total (Label set test\({}^{\sigma}\))** & **28** & **4,165** & **27** & **3,358** & **55** & **7,523** \\ \hline
**Label set test\({}^{1}_{A}\)** & **28** & **2,748** & **27** & **1,981** & **55** & **4,729** \\ \hline
**Label set test\({}^{1}_{B}\)** & **28** & **2,655** & **27** & **2,301** & **55** & **4,956** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Overview of the label sets (_train_, _test\({}^{0}\)_, _test\({}^{1}_{A}\)_, and _test\({}^{1}_{B}\)_). Number of images and organoid labels stratified by study type (for all) and annotator (only relevant for _train_ and _test\({}^{0}\)_). All _test_ label sets refer to the same images. We see a reduction in the number of labels between \(t^{0}\) and \(t^{1}\).

Inter-rater scores can be computed for the entire test set between \(test_{A}^{1}\) and \(test_{B}^{1}\), as shown in Figure 3. We again observe that the annotation of 'Macros' is generally more challenging, and those images consistently appear noisier in Figure 2. In Figure A.6(bottom), we also display the inter-rater scores on the image subsets 1-22 and 23-55, as well as across \(t^{0}\) and \(t^{1}\). This corroborates the larger evolution of Annotator B between \(t^{0}\) and \(t^{1}\) and indicates a convergence of the two annotation styles. Table A.6, Table A.7, and Table A.8, provide all statistics for these multi-rater scores.

### Dataset availability

We make _MultiOrg_ available to the community. All images are public on Kaggle, together with label sets \(train\) and \(test^{0}\), to ensure that the steps presented in Section 4 can be reproduced. The label sets \(test_{A}^{1}\) and \(test_{B}^{1}\) can be queried by participating in the _MultiOrg_ challenge, where our leaderboard returns the average of _mAP_ on \(test_{A}^{1}\) and \(test_{B}^{1}\). We invite scientists to participate, to promote research in the field of uncertainty estimation.

## 4 Model Benchmarking

We benchmark four standard object-detection \(DL\) models on _MultiOrg_:

* _Faster R-CNN_(Ren et al., 2015)
* _Single Shot MultiBox Detector (SSD)_(Liu et al., 2016)
* _You Only Look Once, Version 3 (YOLOv3)_(Redmon and Farhadi, 2018)
* _Real-Time Models for object Detection (RTMDet)_(Lyu et al., 2022).

Figure 3: Multi-rater scores. **Top**: Intra-rater F1-score (left), Precision (middle), and Recall (right), where \(test^{0}\) is considered the _GT_, for both annotators and according to study type. Annotator A appears more consistent on ’Normal’ images (higher scores), and annotation of ’Macros’ seems more challenging (with lower scores). Both annotators show an overall higher Precision and lower Recall, indicating that \(test^{0}\) has many more annotations which are treated here as _FNs_. **Bottom**: Inter-rater F1-score (left), Precision (middle), and Recall (right) on the test set between \(test_{A}^{1}\) and \(test_{B}^{1}\), where \(test_{A}^{1}\) is considered the _GT_, split according to study type. Raters agree more on ’Normal’ images, indicating that the annotation of ’Macros’ images is more challenging. Individual differences are generally lower than in-between raters (lower inter-rater than intra-rater scores).

All trained models can be found on zenodo, and code and documentation to reproduce the training is available on Kaggle 1.

Footnote 1: In addition to the tested methods, we initially implemented DETR with standard hyper-parameters for our dataset, but the training was quite unstable, and the performance much worse than other models. We therefore decided not to report the scores in the manuscript. Furthermore, MedSAM (Ma et al., 2024) and Cellpose (Stringer et al., 2021), segmentation-based approaches, did not work well out-of-the box. Since they would also require segmentation labels for fine-tuning to be useful we did not include these methods for benchmarking.

### Training and Testing

For training, the images in the training set were split into patches of 512x512 px, resulting in a total of 20,011 patches. Bounding boxes extending beyond the borders of the patches were omitted in the _GT_ since these organoids can be captured by a sliding window approach at inference, resulting in 44,418 bounding box labels for training. For training and validation, we used the _mmdetection_(Chen et al., 2019) toolbox, with the original configuration for each model adapted such that the input and parameters for all models is the same (details in Appendix A.2).

During testing, sliding window inference was performed on the full images of the test set. We slide over each image twice with different window sizes and down-sampling factors to detect both small and large organoids. We empirically choose the following parameters: window size set to 512 and 2048 px, while the down-sampling factor is set to two and eight, respectively, with a window overlap of 0.5 and _Non max suppression (NMS)_ for post-processing with a threshold of 0.5. For each model, we choose the checkpoint with the highest _mAP_ on \(test^{b}\), thus using this label set for validation during training. We report those, along with training and inference times in Table A.10.

### Benchmark models evaluation

We evaluate the model performance on \(test^{0}\), \(test^{1}_{A}\) and \(test^{1}_{B}\). Figure 4 shows _P-R_ curves for the different models on label set \(test^{0}\), as well as the curves for all three label sets on the best performing model, _SSD_. Notably, though the model was trained and validated on labels created at \(t^{0}\), the best _P-R_ curve is obtained for \(test^{1}_{B}\), indicating that the trained model is more in agreement with these labels. This suggests that the higher label noise present in \(test^{0}\), as assumed in Section 3.3, was not picked up by the model during training, illustrating once more the resilience of _DL_ to label noise (Rolnick et al., 2017). Table 3 presents further evaluation metrics and confirms that _SSD_ is the best-performing model overall, while at the standard model confidence threshold of 0.5 _YOLOv3_ performs equally well if not better on some label sets. Interestingly, different models exhibit very different Precision-Recall trade-offs at 0.5 model confidence.

Figure 4: Model Benchmark. _P-R_ curves using \(test^{0}\) as the _GT_ for all models (left) and using all three label sets for _SSD_ (right). We observe that overall the SSD model predictions are more in agreement with the annotations and have a better trade-off between precision and recall. Although the model was trained and validated with labels from \(t^{0}\) it is more in agreement with annotations from timepoint \(t^{1}\).

### Napari plugin

As described in Appendix A.1.3, _MultiOrg_ was created using the open-source image analysis tool _Napari_(Ahlers et al., 2019), together with the initial release, _v.0.1.0_, of the _napari-organoid-counter_ plugin (Bukas, 2022). In this work, we release a new version of the plugin, _v.0.2.2_, using the model from our benchmark with the better trade-off between performance and inference time as the backbone (see Table A.10), i.e. _YOLOv3_, along with added functionalities. For example, the model confidence threshold can now be adjusted at run time by the user, depending on whether for the task at hand, a higher Recall or Precision is most practical. Plugin details can be found in Appendix A.3. Code and tutorials for installation are distributed through the napari hub

## 5 Discussion

In this work, we release _MultiOrg_, a large multi-rater dataset for organoid detection in 2d microscopy images. Our dataset consists of more than 60,000 annotated lung organoids, labeled by two expert annotators. As even expert annotators can disagree on what constitutes an organoid in those images while also being susceptible to human error and biases, we provide three label sets for the test data, enabling quantification of label uncertainty on a multi- and single-annotator level. Additionally, we have carefully included diversity in our dataset through several study setups and cell lines to ensure good generalization. We performed preliminary tests of the selected model on different lung cell types not present in the dataset, from both human and mouse organoids, and seen that it generalizes quite well. We also tested it successfully on colon organoids and speculate that it can be used for all organoids with similar shape and size. This dataset is, therefore, uniquely situated between the fields of microscopy and uncertainty quantification. We invite researchers to use it, participate in the _MultiOrg_ challenge, and assist us in studying label noise in challenging real-life biomedical settings, and we believe that future models should, as much as possible, refrain from being trained without considering these aspects. We additionally publish a benchmark for organoid object detection, provide all models in zenodo and the best one as a Napari plugin, thus enabling scientists potentially use them on their own data.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline
**Metric** & **Label set** & **Faster R-CNN** & **SSD** & **YOLOv3** & **RTMDet** \\ \hline \multirow{4}{*}{Precision} & \(test^{0}\) & 0.23 & 0.61 & **0.73** & 0.64 \\  & \(test^{1}_{A}\) & 0.16 & 0.44 & 0.58 & 0.54 \\  & \(test^{1}_{B}\) & 0.18 & 0.50 & 0.67 & 0.56 \\ \cline{2-6}  & **mean** & **0.19** & **0.52** & **0.66** & **0.58** \\ \hline \multirow{4}{*}{Recall} & \(test^{0}\) & 0.84 & 0.67 & 0.48 & 0.51 \\  & \(test^{1}_{A}\) & 0.92 & 0.78 & 0.62 & 0.69 \\  & \(test^{1}_{B}\) & **0.97** & 0.83 & 0.67 & 0.68 \\ \cline{2-6}  & **mean** & **0.91** & **0.76** & **0.59** & **0.63** \\ \hline \multirow{4}{*}{F1-score} & \(test^{0}\) & 0.36 & 0.64 & 0.58 & 0.57 \\  & \(test^{1}_{A}\) & 0.27 & 0.57 & 0.60 & 0.61 \\  & \(test^{1}_{B}\) & 0.30 & 0.62 & **0.67** & 0.62 \\ \cline{2-6}  & **mean** & **0.31** & **0.61** & **0.62** & **0.60** \\ \hline \multirow{4}{*}{mAP@0.5IoU (\%)} & \(test^{0}\) & 56.56 & 64.40 & 62.55 & 57.71 \\  & \(test^{1}_{A}\) & 57.09 & 65.79 & 61.11 & 63.87 \\ \cline{1-1}  & \(test^{1}_{B}\) & 68.36 & **73.88** & 70.25 & 63.23 \\ \cline{1-1} \cline{2-6}  & **mean** & **60.67** & **68.09** & **64.64** & **61.60** \\ \hline \multirow{4}{*}{mAP@0.75IoU (\%)} & \(test^{0}\) & 17.48 & 21.81 & 19.15 & 22.56 \\  & \(test^{1}_{A}\) & 23.53 & 23.42 & 19.13 & 30.13 \\ \cline{1-1}  & \(test^{1}_{B}\) & **46.98** & **46.48** & 39.01 & 32.85 \\ \cline{1-1} \cline{2-6}  & **mean** & **29.33** & **30.57** & **25.76** & **28.51** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Benchmark metrics on the three label sets. Precision, Recall, and F1-score are reported at 0.5 _IOU_ threshold and model confidence. _mAP_ is reported at 0.5 and 0.75 IoU threshold. The models exhibit different Precision-Recall tradeoffs. The performance of _SSD_ is overall better when considering _mAP_, while at the standard model confidence threshold of 0.5 _YOLOv3_ performs equally well if not better on some label sets.

This work offers a valuable dataset that can be leveraged to advance both object detection methods and uncertainty quantification techniques. The current setting does not, however, permit the incorporation of several label sets in the training loop. Furthermore, it is important to note that although two organoid types are present in the images, they have not been annotated as different classes. Treating this task as a multi-class detection problem may boost the overall performance of the object detection task (Zhang et al., 2022) and would provide added value to the biologists. However, while providing several label sets or multi-class labels on the training data could be beneficial, it represents substantial manual work. It would also be interesting to observe how the label sets change by using _DL_ models as a baseline for annotation rather than the pseudo labels. Despite these limitations, we are confident that the release of the _MultiOrg_ dataset offers several invaluable contributions to the machine learning community.

## Acknowledgements

This work was funded by the German Center for Lung Research (DZL) and from the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation- 512453064) as well as from the Stiftung Atemweg. We would like to thank Kathrin Federl for her excellent technical assistance during the creation of the dataset. We would also like to thank Isra Mekki and Francesco Campi for reviewing the code for reproducing the benchmark, and Theresa Willem for her help with the Ethics Statement.

## References

* Ahlers et al. (2019) Jannis Ahlers, Daniel Althviz More, Oren Amsalem, Ashley Anderson, Grzegorz Bokota, Peter Boone, Jordao Bragantini, Genevieve Buckley, Alister Burt, Matthias Bussonnier, Ahmet Can Solak, Clement Caporal, Draga Doncila Pop, Kira Evans, Jeremy Freeman, Lorenzo Gailas, Christoph Gohlke, Kabilar Gunalan, Hagai Har-Gil, Mark Harfouche, Kyle I. S. Harrington, Volker Hilsenstein, Katherine Hutchings, Talley Lambert, Jessy Lauer, Gregor Lichtner, Ziyang Liu, Lucy Liu, Alan Lowe, Luca Marconato, Sean Martin, Abigail McGovern, Lukasz Migas, Nadalyn Miller, Hector Munoz, Jan-Hendrik Muller, Christopher Nauroth-Kress, Juan Nunez-Iglesias, Constantin Pape, Kim Peevy, Gonzalo Pena-Castellanos, Andrea Pierre, Jaime Rodriguez-Guerna, David Ross, Loic Royer, Craig T. Russell, Gabriel Selzer, Paul Smith, Peter Sobolewski, Konstantin Sofiuki, Nicholas Soffroniev, David Stansby, Andrew Sweet, Wouter-Michiel Vierdag, Pam Wadwa, Melissa Weber Mendonca, Jonas Windhaeger, Philip Winston, and Kevin Yamauchi. napari: a multi-dimensional image viewer for Python. _napari: a multi-dimensional image viewer for Python_, 2019.
* Almazroa et al. (2017) Ahmed Almazroa, Sami Alodhayb, Essameldin Osman, Eslam Ramadan, Mohammed Hummadi, Mohammed Dlain, Muhannad Alkatee, Kaamran Raahemifar, and Vasudevan Lakshminarayanan. Agreement among ophthalmologists in marking the optic disc and optic cup in fundus images. _International ophthalmology_, 37:701-717, 2017.
* Amgad et al. (2022) Mohamed Amgad, Lamees A Atteya, Hagar Hussein, Kareem Hosny Mohammed, Ehab Hafiz, Maha AT Elsebaie, Ahmed M Alhusseiny, Mohamed Atef AlMoslemany, Abdelmagid M Elmatboly, Philip A Pappalardo, et al. Nucls: A scalable crowdsourcing approach and dataset for nucleus classification and segmentation in breast cancer. _GigaScience_, 11:giac037, 2022.
* Armato III et al. (2011) Samuel G Armato III, Geoffrey McLennan, Luc Bidaut, Michael F McNitt-Gray, Charles R Meyer, Anthony P Reeves, Binsheng Zhao, Denise R Aberle, Claudia I Henschke, Eric A Hoffman, et al. The lung image database consortium (lidc) and image database resource initiative (idri): a completed reference database of lung nodules on ct scans. _Medical physics_, 38(2):915-931, 2011.
* Aung et al. (2015) Min SH Aung, Sebastian Kaltwang, Bernardino Romera-Paredes, Brais Martinez, Aneesha Singh, Matteo Cella, Michel Valstar, Hongying Meng, Andrew Kemp, Moshen Shafizadeh, et al. The automatic detection of chronic pain-related expression: requirements, challenges and the multimodal emogain dataset. _IEEE transactions on affective computing_, 7(4):435-451, 2015.
* Barkauskas et al. (2013) Christina E Barkauskas, Michael J Cronce, Craig R Rackley, Emily J Bowie, Douglas R Keene, Barry R Stripp, Scott H Randell, Paul W Noble, Brigid LM Hogan, et al. Type 2 alveolar cells are stem cells in adult lung. _The Journal of clinical investigation_, 123(7):3025-3036, 2013.
* Barkauskas et al. (2017) Christina E Barkauskas, Mei-I Chung, Bryan Fioret, Xia Gao, Hiroaki Katsura, and Brigid LM Hogan. Lung organoids: current uses and future promise. _Development_, 144(6):986-997, 2017.
* Brakauskas et al. (2018)Xuesheng Bian, Gang Li, Cheng Wang, Weiquan Liu, Xiuhong Lin, Zexin Chen, Mancheung Cheung, and Xiongbiao Luo. A deep learning model for detection and tracking in high-throughput images of organoid. _Computers in Biology and Medicine_, 134:104490, 2021.
* Bran et al. [2024] Hongwei Bran, Fernando Navarro, Ivan Ezhov, Amirhossein Bayat, Dhrifiman Das, Florian Kofler, Suprosanna Shit, Diana Waldmannstetter, Johannes C Paetzold, Xiaobin Hu, et al. Qubiq: Uncertainty quantification for biomedical image segmentation challenge. _arXiv preprint arXiv:2405.18435_, 2024.
* Bremer et al. [2022] Jan P Bremer, Martin E Baumdick, Marius S Knorr, Lucy HM Wegner, Jasmin Wesche, Ana Jordan-Paiz, Johannes M Jung, Andrew J Highton, Julia Jager, Ole Hinrichs, et al. Goat: Deep learning-enhanced generalized organoid annotation tool. _bioRxiv_, pages 2022-09, 2022.
* Bukas [2022] Christina Bukas. HelmholtzAI-Consultants-Munich/napari-organoid- counter: Latest versions of dependencies. _HelmholtzAI-Consultants-Munich/napari-organoid- counter: Latest versions of dependencies_, 2022.
* Chen et al. [2019] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark. _arXiv preprint arXiv:1906.07155_, 2019.
* Costa et al. [2021] Rita Costa, Darcy E Wagner, Ali Doryab, Martina M De Santis, Kenji Schorpp, Ina Rothenaigner, Mareike Lehmann, Hoeke A Baarsma, Xueping Liu, Otmar Schmid, et al. A drug screen with approved compounds identifies amlexanox as a novel wnt/\(\beta\)-catenin activator inducing lung epithelial organoid formation. _British journal of pharmacology_, 178(19):4026-4041, 2021.
* Domenech-Moreno et al. [2023] Eva Domenech-Moreno, Anders Brandt, Toni T Lemmetyinen, Linnea Wartiovaara, Tomi P Makela, and Saara Ollila. Telu-an object-detector algorithm for automatic classification of intestinal organoids. _Disease Models & Mechanisms_, 16(3):dmm049756, 2023.
* Du et al. [2023] Xuan Du, Zaozao Chen, Qiwei Li, Sheng Yang, Lincao Jiang, Yi Yang, Yanhui Li, and Zhongze Gu. Organoids revealed: Morphological analysis of the profound next generation in-vitro model with artificial intelligence. _Bio-design and Manufacturing_, pages 1-21, 2023.
* Everingham et al. [2012] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results. http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html.
* Falk et al. [2019] Thorsten Falk, Dominic Mai, Robert Bensch, Ozgun Cicek, Ahmed Abdulkadir, Yassine Marrakchi, Anton Bohm, Jan Deubner, Zoe Jackel, Katharina Seiwald, et al. U-net: deep learning for cell counting, detection, and morphometry. _Nature methods_, 16(1):67-70, 2019.
* Haja et al. [2023] Asmaa Haja, Jose M Horcas-Nieto, Barbara M Bakker, and Lambert Schomaker. Towards automatization of organoid analysis: A deep learning approach to localize and quantify organoid images. _Computer Methods and Programs in Biomedicine Update_, 3:100101, 2023.
* He et al. [2017] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In _Proceedings of the IEEE international conference on computer vision_, pages 2961-2969, 2017.
* Huch and Koo [2015] Meritrell Huch and Bon-Kyoung Koo. Modeling mouse and human development using organoid cultures. _Development_, 142(18):3113-3125, 2015.
* Ingber [2022] Donald E Ingber. Human organs-on-chips for disease modelling, drug development and personalized medicine. _Nature Reviews Genetics_, 23(8):467-491, 2022.
* Kassis et al. [2019] Timothy Kassis, Victor Hernandez-Gordillo, Ronit Langer, and Linda G Griffith. Orgaquant: human intestinal organoid localization and quantification using deep convolutional neural networks. _Scientific reports_, 9(1):12479, 2019.
* Kastlmeier et al. [2023] Miriam T Kastlmeier, Erika Gonzalez-Rodriguez, Phoebe Cabanis, Eva M Guenther, Ann-Christine Konig, Lianyong Han, Stefanie M Hauck, Feniaje See, Sara Asgharpour, Christina Bukas, et al. Cytokine signaling converging on il11 in ild fibroblasts provokes aberrant epithelial differentiation signatures. _Frontiers in Immunology_, 14:1128239, 2023.
* Keles et al. [2022] Hakan Keles, Christopher A Schofield, Helena Rannikmae, Erin Elizabeth Edwards, and Lisa Mohamed. A scalable 3d high-content imaging protocol for measuring a drug induced dna damage response using immunofluorescence subnuclear \(\gamma\)h2ax spots in patient derived ovarian cancer organoids. _ACS Pharmacology & Translational Science_, 6(1):12-21, 2022.
* Kessler et al. [2019]Jihoon Kim, Bon-Kyoung Koo, and Juergen A Knoblich. Human organoids: model systems for human biology and medicine. _Nature Reviews Molecular Cell Biology_, 21(10):571-584, 2020.
* Kirillov et al. [2023] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4015-4026, 2023.
* Kofler et al. [2021] Florian Kofler, Ivan Ezhov, Fabian Isensee, Fabian Balsiger, Christoph Berger, Maximilian Koerner, Beatrice Demiray, Julia Rackerseder, Johannes Paetzold, Hongwei Li, et al. Are we using appropriate segmentation metrics? identifying correlates of human expert perception for cnn training beyond rolling the dice coefficient. _arXiv preprint arXiv:2103.06205_, 2021.
* Kofler et al. [2023] Florian Kofler, Johannes Wahle, Ivan Ezhov, Sophia J Wagner, Rami Al-Maskari, Emilia Gryska, Mihail Todorov, Christina Bukas, Felix Meissen, Tingying Peng, et al. Approaching peak ground truth. In _2023 IEEE 20th International Symposium on Biomedical Imaging (ISBI)_, pages 1-6. IEEE, 2023.
* Lehmann et al. [2020] Mareike Lehmann, Qianjiang Hu, Yan Hu, Kathrin Hafner, Rita Costa, Anastasia van den Berg, and Melanie Konigshoff. Chronic wnt/\(\beta\)-catenin signaling induces cellular senescence in lung epithelial cells. _Cellular signalling_, 70:109588, 2020.
* Lesjak et al. [2018] Ziga Lesjak, Alifia Galimzianova, Ales Koren, Matej Lukin, Franjo Pernus, Bostjan Likar, and Ziga Spiclin. A novel public mr image dataset of multiple sclerosis patients with lesion segmentations based on multi-rater consensus. _Neuroinformatics_, 16:51-63, 2018.
* Lin et al. [2015] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollar. Microsoft coco: Common objects in context, 2015.
* Liu et al. [2016] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part I 14_, pages 21-37. Springer, 2016.
* Lyu et al. [2022] Chengqi Lyu, Wenwei Zhang, Haian Huang, Yue Zhou, Yudong Wang, Yanyi Liu, Shilong Zhang, and Kai Chen. Rtmdet: An empirical study of designing real-time object detectors. _arXiv preprint arXiv:2212.07784_, 2022.
* Ma et al. [2024] Jun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, and Bo Wang. Segment anything in medical images. _Nature Communications_, 15:1-9, 2024.
* Matthews et al. [2022] Jonathan M Matthews, Brooke Schuster, Sara Saheb Kashaf, Ping Liu, Rakefet Ben-Yishay, Dana Ishay-Ronen, Evgeny Izumchenko, Le Shen, Christopher R Weber, Margaret Bielski, et al. Organoid: A versatile deep learning platform for tracking and analysis of single-organoid dynamics. _PLoS computational biology_, 18(11):e1010584, 2022.
* Mehta et al. [2022] Raghav Mehta, Angelos Filos, Ujjwal Baid, Chiharu Sako, Richard McKinley, Michael Rebsamen, Katrin Dtwyler, Raphael Meier, Piotr Radojewski, Gowtham Krishnan Murugesan, et al. Qu-brats: Miccai brats 2020 challenge on quantifying uncertainty in brain tumor segmentation-analysis of ranking scores and benchmarking results. _The journal of machine learning for biomedical imaging_, 2022, 2022.
* Nguyen et al. [2022] Ha Q Nguyen, Khanh Lam, Linh T Le, Hieu H Pham, Dat Q Tran, Dung B Nguyen, Dung D Le, Chi M Pham, Hang TT Tong, Diep H Dinh, et al. Vindr-cxr: An open dataset of chest x-rays with radiologist's annotations. _Scientific Data_, 9(1):429, 2022.
* Orlando et al. [2020] Jose Ignacio Orlando, Huazhu Fu, Joao Barbosa Breda, Karel Van Keer, Deepti R Bathula, Andres Diaz-Pinto, Ruogu Fang, Pheng-Ann Heng, Jeyoung Kim, JoonHo Lee, et al. Refuge challenge: A unified framework for evaluating automated methods for glaucoma assessment from fundus photographs. _Medical image analysis_, 59:101570, 2020.
* Padilla et al. [2021] Rafael Padilla, Wesley L. Passos, Thadeu L. B. Dias, Sergio L. Netto, and Eduardo A. B. da Silva. A comparative analysis of object detection metrics with a companion open-source toolkit. _Electronics_, 10(3), 2021.
* Redmon and Farhadi [2018] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. _arXiv preprint arXiv:1804.02767_, 2018.
* Ren et al. [2015] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. _Advances in neural information processing systems_, 28, 2015.
* Rios and Clevers [2018] Anne C Rios and Hans Clevers. Imaging organoids: a bright future ahead. _Nature methods_, 15(1):24-26, 2018.
* Rolnick et al. [2017] David Rolnick, Andreas Veit, Serge Belongie, and Nir Shavit. Deep learning is robust to massive label noise. _arXiv preprint arXiv:1705.10694_, 2017.
* Raghav et al. [2018]Jayanthi Sivaswamy, S Krishnadas, Arunava Chakravarty, G Joshi, A Syed Tabish, et al. A comprehensive retinal image dataset for the assessment of glaucoma from the optic nerve head analysis. _JSM Biomedical Imaging Data Papers_, 2(1):1004, 2015.
* Stringer et al. [2021] Carsen Stringer, Tim Wang, Michalis Michaelos, and Marius Pachitariu. Cellpose: a generalist algorithm for cellular segmentation. _Nature methods_, 18(1):100-106, 2021.
* Styner et al. [2008] Martin Styner, Joonhi Lee, Brian Chin, M Chin, Olivier Commowick, H Tran, S Markovic-Plese, V Jewells, and S Warfield. 3d segmentation in the clinic: A grand challenge ii: Ms lesion segmentation. _Midas Journal_, 2008:1-6, 2008.
* Ultralytics [2021] Ultralytics. YOLOv5: A state-of-the-art real-time object detection system. https://docs.ultralytics.com, 2021.
* Wu et al. [2022] Xinhui Wu, I Sophie T Bos, Thomas M Conlon, Meshal Ansari, Vicky Verschut, Luke Van der Koog, Lars A Verkleij, Angela D'Ambrosi, Aleksey Matveyenko, Herbert B Schiller, et al. A transcriptomics-guided drug target discovery strategy identifies receptor ligands for lung regeneration. _Science Advances_, 8(12):eabj9949, 2022.
* Youk et al. [2020] Jeonghwan Youk, Taewoo Kim, Kelly V Evans, Young-Il Jeong, Yongsuk Hur, Seon Pyo Hong, Je Hyoung Kim, Kijong Yi, Su Yeon Kim, Kwon Joong Na, et al. Three-dimensional human alveolar stem cell culture models reveal infection response to sars-cov-2. _Cell Stem Cell_, 27(6):905-919, 2020.
* Zhang et al. [2022] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Harold Li, Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-Neng Hwang, and Jianfeng Gao. Glipv2: unifying localization and vl understanding. In _Proceedings of the 36th International Conference on Neural Information Processing Systems_, pages 36067-36080, 2022.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? [Yes] See Section 5 3. Did you discuss any potential negative societal impacts of your work? [Yes] See appendix A.4 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] See appendix A.4
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] Links for all assets released are provided in Appendix C 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Table A.4, Section 4.1, and Appendix A.2 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [No] 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Section 4.1 and Table A.10
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] 2. Did you mention the license of the assets? [N/A] 3. Did you include any new assets either in the supplemental material or as a URL? See appendix C, or alternatively section 3.4, section 4, and section 4.3 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes]5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes]
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]Appendix

### Data Description

#### a.1.1 Biological study setup

Two different study setups were used to create our dataset. In the first, we used isolated murine distal epithelial cells, enriched for alveolar epithelial type II cells (AEC2), which have the stem-cell function of differentiating into other cell types, hence serving as progenitor cells in the lung (Barkauskas et al., 2013). In addition to the AEC2, we used a murine fibroblast cell line as mesenchymal support cells. Images containing organoids deriving from these biological cultures, are henceforth mentioned as belonging to the 'Normal' setup.

As a second study setup, we added a cell type, namely macrophages, to the organoid culture, which we henceforth refer to as the 'Macros' setup. Isolation of cells and culturing of organoids were performed as previously described in Lehmann et al. (2020). Organoids were seeded as duplicates in 96-well imaging plates with a glass bottom (see Figure 1).

#### a.1.2 Image acquisition

The plates used for culturing and imaging were _Falcon@_ 96-well Black/Clear Flat Bottom TC-treated Imaging Microplates. The brightfield images were acquired with a Life Cell Imaging Microscope (_LifeCellImagerObserver.Z1_) at a 5x objective. During the acquisition of the images, each well was divided into tiles and stacks to capture the 3d growth of organoids. Per well, 24 tiles and 10-15 stacks were acquired. Individual tiles were stitched together to form one single image per plate. We observed that most object detection methods can successfully detect organoids on the borders of two or more patches, even if the stitching mechanism is imperfect. Therefore, in our setup, we decided to work with the stitched images rather than the individual patches.

Maximum projections were generated by the _Zen 2 Blue_ software by _Carl Zeiss Microscopy GmbH_ to process the image stacks into one plain 2D image, such that each pixel in the final image derives from the slice in the stack which is most in focus at that location. Since organoid structures are relatively spherical, one can easily approximate their area using a 2D projection. Additionally, labeling images in 2D greatly speeds up the annotation procedure. Images were exported in the _CZI_ file format.

The resulting 2D images have varying sizes, between 5719 and 6240 pixels in the \(x\) and 5551 and 6940 pixels in the \(y\) axis, respectively. Each pixel in the image is equivalent to 1.29 \(\mu m\) in each axis. At this point, the images were examined, and eight plates were dropped, either due to lower image quality or because the organoid formation did not work well, resulting in noisy images. The latter was mainly observed in the Macros study setup, which resulted in fewer data from this setup in our final dataset. Finally, the imaged wells were randomly selected by plates and study setups to be annotated using our annotation tool of choice (see Appendix A.1.3).

#### a.1.3 Annotation Procedure

The annotation process was carried out by running the initial release, _v.0.1.0_, of the _napari-organoid-counter_ tool (Bukas, 2022), a plugin developed for _Napari_(Ahlers et al., 2019). The tool parameters were set to a down-sampling of one, minimal diameter of 30 \(\upmu\)m and sigma of three. After running the _napari-organoid-counter_ with these parameters, all detected organoids were examined. All spherical structures consisting of visibly more than one cell and measuring more than 30 \(\upmu\)m, were recognized as organoids. The wrongfully created box was manually deleted if the counter detected a _False Positive (FP)_. If the counter detected the organoid size or exact location incorrectly, the box was manually moved or adjusted according to the correct size and location. If the counter detected accumulations of organoids as one single object, the box was deleted and correctly sized boxes for the single organoids were created. If the counter did not detect an organoid, a box according to the organoid's size was manually created.

#### a.1.4 Data preparation for release

After all the data was collected and annotated, all images were converted from the proprietary CZI to the open TIFF format. Additionally, all studies were renamed to ensure consistency and all images and annotation information for each image were anonymized.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline
**Study Setup** & **Plate Name** & **Number of Wells** & **Image IDs** & **Data split** & **Annotator** \\ \hline Normal & Plate\_11 & 13 & 1-13 & train & A \\ Normal & Plate\_13 & 1 & 14 & train & A \\ Macros & Plate\_13 & 6 & 15-20 & train & A \\ Normal & Plate\_19 & 6 & 21-26 & train & A \\ Normal & Plate\_20 & 20 & 27-46 & train & A \\ Normal & Plate\_26 & 34 & 47-80 & train & A \\ Normal & Plate\_29 & 26 & 81-106 & train & A \\ Normal & Plate\_3 & 5 & 107-111 & train & A \\ Normal & Plate\_32 & 19 & 112-130 & train & A \\ Normal & Plate\_33 & 16 & 131-146 & train & A \\ Normal & Plate\_34 & 16 & 147-162 & train & A \\ Macros & Plate\_6 & 9 & 163-171 & train & A \\ Normal & Plate\_8 & 10 & 172-181 & train & A \\ Normal & Plate\_9 & 15 & 182-196 & train & A \\ Normal & Plate\_16 & 17 & 197-213 & train & B \\ Macros & Plate\_16 & 9 & 214-222 & train & B \\ Normal & Plate\_17 & 18 & 223-240 & train & B \\ Macros & Plate\_17 & 4 & 241-244 & train & B \\ Normal & Plate\_18 & 34 & 245-278 & train & B \\ Macros & Plate\_18 & 6 & 279-284 & train & B \\ Normal & Plate\_24 & 10 & 285-294 & train & B \\ Macros & Plate\_25 & 6 & 295-300 & train & B \\ Normal & Plate\_36 & 15 & 301-315 & train & B \\ Normal & Plate\_39 & 17 & 316-332 & train & B \\ Normal & Plate\_40 & 24 & 333-356 & train & B \\ Normal & Plate\_37 & 6 & 1-6 & test & A \\ Normal & Plate\_4 & 2 & 7-8 & test & A \\ Macros & Plate\_4 & 14 & 9-22 & test & A \\ Normal & Plate\_15 & 12 & 23-34 & test & B \\ Normal & Plate\_31 & 8 & 35-42 & test & B \\ Macros & Plate\_15 & 7 & 43-49 & test & B \\ Macros & Plate\_23 & 6 & 50-55 & test & B \\ \hline \hline \end{tabular}
\end{table}
Table 4: A detailed overview of the dataset. The training set consists of 356 images derived from 25 studies, and the test set consists of 55 images from 7 studies.

Figure A.5: Bounding box sizes. Box plots of the bounding box areas in \(test^{0}\), \(test^{1}_{A}\) and \(test^{1}_{B}\), stratified by study type, on a logarithmic scale.

\begin{table}
\begin{tabular}{c c c c} \hline \hline Label Subset & Study Type & Normal & Macros & Combined \\ \hline \(test_{A}^{0}\) & 11,887(\(\pm\) 51,000) & 4,891(\(\pm\) 31,918) & 7,553(\(\pm\) 40,400) \\ \(test_{B}^{0}\) & 17,064(\(\pm\) 57,976) & 9,654(\(\pm\) 25,272) & 14,612(\(\pm\) 49,726) \\ \(test_{A}^{1}\) & 19,413(\(\pm\) 64,855) & 8,714(\(\pm\) 36,128) & 14,931(\(\pm\) 14,931) \\ \(test_{B}^{1}\) & 21,315(\(\pm\) 68,956) & 8,188(\(\pm\) 35,042) & 15,220(\(\pm\) 56,215) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Bounding box sizes. Mean and standard deviation of the bounding box areas (in \(\upmu\)m\({}^{2}\)) in \(test_{A}^{0}\), \(test_{B}^{0}\), \(test_{A}^{1}\), and \(test_{B}^{1}\), stratified by study type and combined.

[MISSING_PAGE_EMPTY:18]

[MISSING_PAGE_FAIL:19]

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Metric** & \(test^{0}\) & \(test^{1}_{A}\) & \(test^{1}_{B}\) \\ \hline Precision & 0.51 & 0.40 & 0.48 \\ Recall & 0.23 & 0.28 & 0.33 \\ F1-score & 0.32 & 0.33 & 0.39 \\ \hline \hline \end{tabular}
\end{table}
Table A.9: Comparison of the label sets to pseudo labels. Precision, Recall, and F1-score computed between the pseudo labels and \(test^{0}\), \(test^{1}_{A}\), \(test^{1}_{B}\) (considered as _GT_ in this computation). The pseudo labels are used as a starting point by each annotator to annotate the data. The F1-score shows that \(test^{1}_{B}\) was curated the least, while the higher Recall values at time point \(t^{1}\) confirm that more of the pseudo labels were removed than at \(t^{0}\).

Figure A.6: Inter-rater scores for the two subsets of the test set. **Top**: F1-score (left), Precision (middle), and Recall (right) shown for Intra- and Inter-rater scores for both annotators. Scores are split for the two test subsets and according to study type. \(test^{0}\) is always considered the _GT_ for computing these scores. **Bottom**: F1-score on images 1-22 (left), where \(test^{1}_{B}\) is used as the _GT_ and scores are computed against \(test^{0}_{A}\) and \(test^{1}_{A}\), and on images 23-55 (right), where \(test^{1}_{A}\) is used as the _GT_ and scores are computed against \(test^{1}_{B}\) and \(test^{1}_{B}\). On the left, we see that when compared to a third independent label set corresponding to a different annotator, annotator A has slightly changed their style of annotation at timepoint \(t^{1}\), slightly converging annotator B. On the right, we see an even bigger shift in annotation style. These results suggest that annotators exchanged best practices in annotation styles between timepoints \(t^{0}\) and \(t^{1}\).

### Model benchmark

#### a.2.1 Training details

For the training and validation pipeline of our benchmark the _mmdetection_(Chen et al., 2019) toolbox was used, a well-established open-source toolbox for object detection. To make our dataset compatible with the toolbox, the bounding boxes were converted to the COCO format (Lin et al., 2015). We adapted the original configuration for each model to set a number of fixed parameters for all models. AdamW was used as the optimizer with a base learning rate of 1e-05, along with a linear learning rate scheduler. The batch size was set to 16 and the training setup included standard image augmentations: Gaussian Blur, Random Flip, Random Shift, Random Affine, and Photometric Distortion with a probability of 0.5. For all models, the pretrained COCO weights were used as initialization and the final layer was adapted to accommodate our single class. All models were trained for 400 epochs and validated using the COCO metrics on \(test^{0}\). Training and validation were performed on an internal cluster that uses an NVIDIA A100 GPU with four cores and 40 GB VRAM. The training time varied slightly depending on the model, but all were trained in less than 21 hours (Table A.10).

#### a.2.2 Additional results

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
**Model** & Faster R-CNN & SSD & YOLOv3 & RTMDet \\ \hline Train time (hours) & 15 & 16 & 10 & 20 \\ Best epoch & 68 & 86 & 27 & 323 \\ Model size (MB) & 172 & 99 & 249 & 454 \\ GPU utilization (\%) & 60-70 & 30-90 & 55-65 & 40-80 \\ Inference time (seconds) & 18 & 59 & 13 & 114 \\ \hline \hline \end{tabular}
\end{table}
Table A.10: Training and testing of benchmark models. Train time is the duration of training the model once on the entire train set for 400 epochs. The best epoch is the epoch with the highest mAP on the test set \(test^{0}\). GPU utilization indicates the approximate range of GPU utilization during training. Inference time is the average time per image inference using a single core.

Figure A.7: Model benchmark. _P-R_ curves across all models of the benchmark for all three label sets. It is interesting to observe that though the model checkpoints were selected based on \(test^{0}\), they are consistently more in agreement with labels of \(test^{1}_{B}\).

Figure A.8: Model benchmark. _P-R_ curves across all models of the benchmark for subsets of the label sets: \(test^{0}_{A:1-22}\) and \(test^{0}_{B:23-55}\), along with the corresponding subsets for \(test^{1}_{A}\) and \(test^{1}_{B}\), i.e. \(test^{1}_{A:1-22}\) and \(test^{1}_{B:23-55}\) such that a direct comparison of the same subsets of the test set can be made. We see that all models are more in agreement with Annotator B at timepoint \(t^{1}\) compared to timepoint \(t^{0}\) for images 23-55 of the test set.

Figure A.9: Example Predictions. Predicted Bounding boxes and model confidence from the _SSD_ model for various image crops of size 1000x1000 pixels of the ’Normal’ (top) and ’Macros’ (bottom) study types. The three label sets are also displayed for comparison. The ’Macros’ images are noisier and, therefore, more challenging for the model and the annotators alike.

### Extensions of naMultiOrg Datasheet

This data sheet serves as supplementary documentation aimed at improving reproducibility. It is based on the guidelines outlined in **Datasheets for Datasets2**, a working paper developed for the machine learning community.

Footnote 2: https://arxiv.org/abs/1803.09010

* **For what purpose was the dataset created?** This dataset was created with two goals in mind: a. to facilitate research in uncertainty quantification methods in machine learning and b. to enable the development of object detection models for the automated detection of lung organoids, which can accelerate the annotation process for similar data in future studies.
* **Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?** The dataset was created as a collaborative effort by the authors of this work, i.e. biologists of the Institute of Lung Health and Immunity (LHI) of the Helmholtz Zentrum Munchen and the Philipp University of Marburg, as well as computer scientists Helmholtz AI at the Helmholtz Zentrum Munchen.
* **Who funded the creation of the dataset?** This work was partly funded by the German Center for Lung Research (DZL) and from the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation- 512453064) as well as from the Stiftung Atemweg. Additionally, it was developed as part of the daily work of the creators and was indirectly funded via their salaries.

### Composition

* **What do the instances that comprise the dataset represent?** Our dataset consists of 2D images of microscopy plate wells, consisting of lung organoids derived from murine cells. Along with the imaging data, annotations for each image are provided, in the form of bounding boxes fit around the organoids, and metadata information on the annotator and time point of annotation.
* **How many instances are there in total (of each type, if appropriate)?** In total 411 fully annotated images are released as part of this dataset, annotated by two annotators at two different timepoints and consisting of 26 different experiments across two biological study setups. Please see Section 3 and Table 4 for more details and stratification between study types, annotators, train and test splits.
* **Does the dataset contain all possible instances or is it a sample of instances from a larger set?** The dataset is a subset of a larger set of biological experiments. During the first curation of the data, the 26 experiments were selected to have the best representation of our data, with the least noise stemming from the image acquisition and with an acceptable number of organoids in the well, neither too many, which would make it hard to distinguish them from one another, nor too few, with little information present in the image.
* **What data does each instance consist of?** Each instance is an image of size between 5719 and 6240 pixels in the \(x\) and 5551 and 6940 pixels in the \(y\) axis respectively. Each pixel in the image is equivalent to 1.29 \(\mu m\) in each axis.
* **Is there a label or target associated with each instance?** Yes, specifically for the training set one target per image is available, while for the images of the test set, we release three sets of labels. The first, which in this work is named \(test^{0}\), is directly available, while the other two can be indirectly accessed by participating in our Kaggle competition and submitting results to our leaderboard.
* **Is any information missing from individual instances?** No, to the best of our knowledge, all available information has been provided.
* **Are relationships between individual instances made explicit?** Yes, Table 4 and the structure of the available data make relationships explicit. Relationships become apparent with the data structure provided in the metadata file. For example, all image wells belonging to the experiment "Plate_1" can be found in a folder named "Plate_1".

* **Are there recommended data splits (e.g., training, development/validation, testing)?** Yes, we provided the data already split into train and test sets. As discussed in the main manuscript, for validation we use the test data with one of the three label sets.
* **Are there any errors, sources of noise, or redundancies in the dataset?** Noise is always present in microscopy data, and is one of the reasons for which machine learning tasks in the biomedical domain are much harder compared to natural images. This noise can derive from the microscope itself, the imaging parameters, or the biological specimen. Nevertheless, we tried to eliminate these as much as possible when curating the dataset, by, as mentioned above, selecting our experiments out of a larger pool and Another source of noise in our case is the stitching of the imaging tiles to form the 2D image well, which we discuss in Section 3. No redundancies are present in the dataset.
* **Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)?** The provided dataset is self-contained.
* **Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals' non-public communications)?** No.
* **Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?** No.
* **Does the dataset relate to people?** No.

### Collection process

* **How was the data associated with each instance acquired?** The data was acquired by cell culture and imaging in a Life Cell Imaging Microscope.
* **What mechanisms or procedures were used to collect the data?** The biological experiments consisted of isolating and culturing murine cells, which received various treatments and formed variable organoids in the process. Subsequently, images were taken to document and analyze the effects of different treatments.
* **If the dataset is a sample from a larger set, what was the sampling strategy?** The dataset is a subset of a larger set of biological experiments. During the first curation of the data, the 26 experiments were selected to have the best representation of our data, with the least noise stemming from the image acquisition and with an acceptable number of organoids in the well, neither too many, which would make it hard to distinguish them from one another, nor too few, with little information present in the image.
* **Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?** The data was collected by university students with guest contracts and employees of the Helmholtz Zentrum Munich.
* **Over what timeframe was the data collected?** The data was collected at specific timepoints over a period of two weeks. All experiments took place over a timeframe of two years.
* **Were any ethical review processes conducted (e.g., by an institutional review board)?** No
* **Does the dataset relate to people?** No, it is a murine dataset.

### Preprocessing/cleaning/labeling

* **Was any preprocessing/cleaning/labeling of the data done?** The data was indeed preprocessed, cleaned, and labeled. In the Appendix, we describe these processes under Appendix A.1.3 and Appendix A.1.2.
* **Was the 'raw' data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)?** Yes, the raw data was saved and stored.
* **Is the software used to preprocess/clean/label the instances available?** The software used for preprocessing the data, _Zen 2 Blue_ software by _Carl Zeiss Microscopy GmbH_, is proprietary. The software used for annotating the data of the _napari-organoid-counter_ tool (Bukas, 2022), a plugin developed for _Napari_, is open-source and freely available.

### Uses

* **Has the dataset been used for any tasks already?** The dataset was used as part of this work to create the benchmark presented in Section 4. The trained models are also made publicly available through this work and can be accessed on zenodo. Moreover, the latest version of the _napari-organoid-counter_ tool described in Section 4.3 also uses one of the models from this benchmark, trained with the current dataset.
* **Is there a repository that links to any or all papers or systems that use the dataset?** If so, please provide a link or other access point. No, please refer to Appendix C instead.
* **What (other) tasks could the dataset be used for?** We discussed previously how the intended usage of our dataset is two-fold, a. to facilitate research in uncertainty quantification methods in machine learning and b. to enable the development of object detection models for the automated detection of lung organoids, which can accelerate the annotation process for similar data in future studies. Aside from these tasks, one could use this dataset to benchmark new model architectures for object detection, or to develop unsupervised learning methods for organoid classification (using the bounding boxes to extract single organoid images), since we mention that our dataset consists of two different types of organoids.
* **Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?** No.
* **Are there tasks for which the dataset should not be used?** Not to the best of our knowledge.

### Distribution

* **Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created?** Yes, the dataset is hereby made publicly available under the CC BY-NC-SA 4.0 License, and can therefore be used by third parties.
* **How will the dataset will be distributed (e.g., tarball on the website, API, GitHub)?** The dataset is hereby made available via the Kaggle platform and can be accessed through the link: https://www.kaggle.com/datasets/christinabukas/mutliorg/ and DOI: 10.34740/kaggle/ds/5097172
* **When will the dataset be distributed?** The dataset is made available along with the submission of the current manuscript.
* **Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)?** Our dataset is open source and made available under the CC BY-NC-SA 4.0 License.
* **Have any third parties imposed IP-based or other restrictions on the data associated with the instances?** No.
* **Do any export controls or other regulatory restrictions apply to the dataset or to individual instances?** No.

### Maintenance

* **Who is supporting/hosting/maintaining the dataset?** The dataset is maintained by the authors of this work. The dataset is currently hosted on the Kaggle3 platform. Footnote 3: https://www.kaggle.com
* **How can the owner/curator/manager of the dataset be contacted (e.g., email address)?** Kaggle offers a discussion tab under the dataset repository. This can be used for any data-related discussions, while there is also a discussion tab available on the website of our competition. Naturally, the corresponding authors of this work may also be contacted directly with any questions via email.
* **Is there an erratum?** There is currently no erratum for this dataset.

* **Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)?** There are currently no immediate plans for updating the dataset. We are eager to first see how it will be accepted by the community and which needs will arise for future versions/extensions. We, of course, plan to maintain the dataset, e.g. if errors are found in the data, we will update the dataset and release a newer version.
* **If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances?** The dataset does not relate to people.
* **Will older versions of the dataset continue to be supported/hosted/maintained?** If and when newer versions of the dataset are released we expect these to be an improvement upon the original version, and will therefore concentrate our efforts on maintaining the latest version of the dataset.
* **If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?** Researchers are more than welcome to extend our dataset. In the discussion section of our manuscript, Section 5, we mention how future versions of our dataset could include even more label sets both for the train and test sets. Such versions would surely increase its value and the development of uncertainty quantification techniques.

Availability: data, benchmark, and software tool

Below we list all assets made publicly available with the release of this work:

* Trained model weights from our benchmark can be found on zenodo with a DOI: 10.5281/zenodo.11258022
* The MultiOrg dataset can be found on Kaggle with a DOI: 10.34740/kaggle/ds/5097172
* The notebooks for reproducing our benchmark can be found under the same repository on the Kaggle MultiOrg dataset page
* The Croissant metadata record documenting the dataset can also be found on the Kaggle MultiOrg dataset page here
* The napari plugin can be found on the napari-hubAuthor Statement

As the authors of this dataset, we hereby declare that we bear full responsibility for any and all consequences arising from the use, distribution, and publication of the dataset. This includes but is not limited to, any violations of privacy, intellectual property rights, or any other legal rights.

We confirm that all data included in this dataset has been collected, processed, and shared in compliance with applicable laws and regulations. We have obtained all necessary permissions and consents from individuals or entities involved, and we affirm that the data does not infringe upon the rights of any third parties.

Furthermore, we confirm that the dataset is being released under the following license: CC BY-NC-SA 4.0. his license allows others to use, share, and adapt the dataset, provided that appropriate credit is given, a link to the license is provided, and any changes are indicated.

By submitting this dataset to the NeurIPS 2024 dataset track, we agree to adhere to the terms and conditions set forth by the NeurIPS conference organizers and acknowledge that we are solely responsible for any issues related to the dataset's legal and ethical use.

Hosting, licensing, and maintenance plan.

As described in the Maintenance section of Appendix B, the dataset is open source and available to researchers via the Kaggle4 platform, under the CC BY-NC-SA 4.0 license. Additionally, on kaggle we offer notebooks to reproduce the model benchmark performed in this work, and links to our zenodo repository which stores our pretrained models. All notebooks are under the Apache 2.0 license and model weights are available under the Creative Commons Attribution 4.0 International license. All the above, will be maintained actively by the authors of this work. If errors are found by users in the code or data, this can be communicated via the discussion tab available on the dataset webpage, and a newer version will be uploaded.

Footnote 4: https://www.kaggle.com