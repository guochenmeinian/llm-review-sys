# Front-door Adjustment Beyond Markov Equivalence

with Limited Graph Knowledge

 Abhin Shah

Massachusetts Institute of Technology

abhin@mit.edu

&Karthikeyan Shanmugam

Google Research

karthikeyanvs@google.com

&Murat Kocaoglu

Purdue University

mkocaoglu@purdue.edu

###### Abstract

Causal effect estimation from data typically requires assumptions about the cause-effect relations either explicitly in the form of a causal graph structure within the Pearlian framework, or implicitly in terms of (conditional) independence statements between counterfactual variables within the potential outcomes framework. When the treatment variable and the outcome variable are confounded, front-door adjustment is an important special case where, given the graph, causal effect of the treatment on the target can be estimated using _post-treatment_ variables. However, the exact formula for front-door adjustment depends on the structure of the graph, which is difficult to learn in practice. In this work, we provide testable conditional independence statements to compute the causal effect using front-door-like adjustment without knowing the graph under limited structural side information. We show that our method is applicable in scenarios where knowing the Markov equivalence class is not sufficient for causal effect estimation. We demonstrate the effectiveness of our method on a class of random graphs as well as real causal fairness benchmarks.

## 1 Introduction

Causal effect estimation is at the center of numerous scientific, societal, and medical questions (Nabi et al., 2019; Castro et al., 2020). The \(do(\cdot)\) operator of Pearl represents the effect of an experiment on a causal system. For example, the probability distribution of a target variable \(y\) after setting a treatment \(t\) to \(t\) is represented by \(\mathbb{P}(y|do(t=t))\) and is known as an interventional distribution. Learning this distribution for any realization \(t=t^{\downarrow}\) is what causal effect estimation entails. This distribution is different from the conditional distribution \(\mathbb{P}(y|t=t)\) as there may be unobserved confounders between treatment and outcome that cannot be controlled for.

A causal graph, often depicted as a directed acyclic graph, captures the cause-and-effect relationships between variables and explains the causal system under consideration. A semi-Markovian causal model represents a causal model that includes unobserved variables influencing multiple observed variables (Verma and Pearl, 1990; Acharya et al., 2018). In a semi-Markovian graph, directed edges between observed variables represent causal relationships, while bi-directed edges between observed variables represent unobserved common confounding (see Figure 1). Given any semi-Markoviangraph, complete identification algorithms for causal effect estimation are known. For example, if \(\mathbb{P}(\bm{\nu}|do(t=t))\) is uniquely determined by the observational distribution and the causal graph, the algorithm by Shpitser and Pearl (2006) utilizes the graph to derive an _estimand_, i.e., the functional form mapping the observational distribution to the interventional distribution.

Certain special cases of estimands have found widespread use across several domains. One such special case is the _back-door adjustment_(Pearl, 1993) shown in Figure 1(left). The back-door adjustment utilizes the pre-treatment variable \(\bm{z}\) (that blocks back-door paths) to control for unobserved confounder as follows:

\[\mathbb{P}(\bm{\nu}|do(t=t))=\sum_{\bm{z}}\mathbb{P}(\bm{\nu}|t=t,\bm{z}=\bm{z })\mathbb{P}(\bm{z}=\bm{z}),\] (1)

where the do-calculus rules of Pearl (1995) are used to convert interventional distributions into observational distributions by leveraging the graph structure. However, the back-door adjustment is often inapplicable, e.g., in the presence of an unobserved confounder between \(t\) and \(\bm{\nu}\). Surprisingly, in such scenarios, it is sometimes possible to find the causal effect using the _front-door adjustment_(Pearl, 1995) shown in Figure 1(right). Utilizing the front-door variable \(\bm{z}\), the front-door adjustment estimates the causal effect from observational distributions using the following formula (which is also obtained through the do-calculus rules and the graph structure):

\[\mathbb{P}(\bm{\nu}|do(t=t))\!=\!\sum_{\bm{z}}\Big{(}\sum_{t^{\prime}}\mathbb{ P}(\bm{\nu}|t=t^{\prime},\bm{z}=\bm{z})\mathbb{P}(\bm{t}=t^{\prime})\Big{)} \mathbb{P}(\bm{z}=\bm{z}|t=t)\] (2)

Recently, front-door adjustment has gained popularity in analyzing real-world data (Glynn and Kashin, 2017; Bellemare et al., 2019; Hunernmund and Bareinboim, 2019) due to its ability to utilize post-treatment variables to estimate effects even in the presence of confounding between \(t\) and \(\bm{\nu}\). However, in general, front-door adjustment also relies on knowing the causal graph, which may not always be feasible, especially in domains with many variables.

An alternative approach uses observational data to infer a Markov equivalence class, which is a collection of causal graphs that encode the same conditional independence relations (Spirtes et al., 2000). A line of work (Perkovic et al., 2018; Jaber et al., 2019) provide identification algorithms for causal effect estimation from partial ancestral graphs (PAGs) (Zhang, 2008), a prominent representation of the Markov equivalence class, whenever every causal graph in the collection shares the same causal effect estimand. However, learning PAGs from data is challenging in practice due to the sequential nature of their learning algorithms, which can propagate errors between tests (Strobl et al., 2019). Further, to the best of our knowledge, there is no existing algorithm that can incorporate side information, such as known post-treatment variables, into PAG structure learning.

In this work, we ask the following question: _Can the causal effect be estimated with a testable criteria on observational data by utilizing some structural side information without knowing the graph?_

Recent research has developed such testable criteria to enable back-door adjustment without knowing the full causal graph (Entner et al., 2013; Cheng et al., 2020; Gultchin et al., 2020; Shah et al., 2022). These approaches leverage structural side information, such as a known and observed parent of the treatment variable \(t\). However, no such results have been established for enabling front-door adjustment. We address this gap by focusing on the case of unobserved confounding between \(t\) and \(\bm{\nu}\), where back-door adjustment is inapplicable. Traditionally, this scenario has been addressed by leveraging the presence of an instrumental variable (Mogstad and Torgovitsky, 2018) or performing sensitivity analysis (Veitch and Zaveri, 2020), both of which provide only bounds in the non-parametric case. In contrast, we achieve identifiability by utilizing structural side information.

**Contributions.** We propose a method for estimating causal effects without requiring the knowledge of causal graph in the presence of unobserved confounding between treatment and outcome. Our approach utilizes front-door-like adjustments based on post-treatment variables and relies on conditional independence statements that can be directly tested from observational data. We

Figure 1: Representative graphs for back-door adjustment (left) and front-door adjustment (right).

require one structural side information which can be obtained from an expert and is less demanding than specifying the entire causal graph. We illustrate that our framework provides identifiability in random ensembles where existing PAG-based methods are not applicable. Further, we illustrate the practical application of our approach to causal fairness analysis by estimating the total effect of a sensitive attribute on an outcome variable using the German credit data with fewer structural assumptions. The source code of our implementation is available at https://github.com/abhin-shah/FD-adjustment-with-limited-graph.

### Related Work

**Effect estimation from causal graphs/Markov equivalence Class:** The problem of estimating interventional distributions with the knowledge of the semi-Markovian model has been studied extensively in the literature, with important contributions such as Tian and Pearl (2002) and Shpitser and Pearl (2006). Perkovic et al. (2018) presented a complete and sound algorithm for identifying valid adjustments from PAGs. Going beyond valid adjustments, Jaber et al. (2019) proposed a complete and sound algorithm for identifying causal effect from PAGs. However, our method can recover the causal effect in scenarios where these algorithms are inapplicable.

**Effect estimation via front-door adjustment with causal graph:** Several recent works have contributed to a better understanding of the statistical properties of front-door estimation (Kuroki, 2000; Kuroki and Cai, 2012; Glynn and Kashin, 2018; Gupta et al., 2021), proposed robust generalizations (Hunermund and Bareinboim, 2019; Fulcher et al., 2020), and developed procedures to enumerate all possible front-door adjustment sets (Jeong et al., 2022; Wienobst et al., 2022). However, all of these require knowing the underlying causal graph. By contrast, Bhattacharya and Nabi (2022) verified the front-door criterion without knowing the causal graph using Verma constraint-based methodology. While their method was limited to a small set of graphs, ours leverages conditional independence, making it applicable to a much broader class of graphs. We note that they're applicable in different settings, depending on what the analyst knows about the problem.

## 2 Preliminaries and Problem Formulation

**Notations.** For a sequence of realizations \(r_{1},\cdots,r_{n}\), we define \(\bm{r}\triangleq\{r_{1},\cdots,r_{n}\}\). For a sequence of random variables \(r_{1},\cdots,r_{n}\), we define \(\bm{r}\triangleq\{r_{1},\cdots,r_{n}\}\). Let \(\mathds{1}\) denote the indicator function.

**Semi-Markovian Model and Effect Estimation.** We consider a causal effect estimation task where \(\mathbf{x}\) represents the set of observed features, \(\mathbf{t}\) represents the observed treatment variable, and \(\bm{\mathsf{y}}\) represents the observed outcome variable. We denote the set of all observed variables jointly by \(\mathcal{V}\triangleq\{\mathbf{x},\,\mathbf{t},\bm{\mathsf{y}}\}\). Let \(\mathcal{U}\) denote the set of unobserved features that could be correlated with the observed variables.

We assume \(\mathcal{W}\triangleq\mathcal{V}\cup\mathcal{U}\) follows a semi-Markovian causal model (Tian and Pearl, 2002) as below.

**Definition 1**.: _A semi-Markovian causal model (SMCM) \(\mathcal{M}\) is specified as follows:_

1. \(\mathcal{G}\) _is a directed acyclic graph (DAG) over the set of vertices_ \(\mathcal{W}\) _such that each element of the set_ \(\mathcal{U}\) _has no parents._
2. \(\forall\bm{\mathsf{v}}\in\mathcal{V}\)_, let_ \(\pi^{(o)}(\bm{\mathsf{v}})\subseteq\mathcal{V}\) _and_ \(\pi^{(u)}(\bm{\mathsf{v}})\subseteq\mathcal{U}\) _denote the set of parent of_ \(\bm{\mathsf{v}}\) _in_ \(\mathcal{V}\) _and_ \(\mathcal{U}\)_, respectively._
3. \(\mathbb{P}(\mathbf{u})\) _is the unobserved joint distribution over the unobserved features._
4. _The observational distribution is given by_ \(\mathbb{P}(\bm{\mathsf{v}})=\mathbb{E}_{\bm{\mathsf{u}}}\Big{[}\prod\limits_{ \begin{subarray}{c}\nu\in\mathcal{V}\end{subarray}}\mathbb{P}(\bm{\mathsf{v}} |\pi^{(o)}(\bm{\mathsf{v}}),\pi^{(u)}(\bm{\mathsf{v}}))\Big{]}\)_._
5. _The interventional distribution when the variables_ \(\mathbf{r}\subset\mathcal{V}\) _are set to a fixed value_ \(\bm{r}\) _is given by_ \[\mathbb{P}(\bm{\mathsf{v}}|do(\mathbf{r}=\bm{r}))=\mathds{1}_{\bm{\mathsf{r}}= \bm{\mathsf{r}}}\cdot\mathbb{E}_{\bm{\mathsf{u}}}\Big{[}\prod\limits_{ \begin{subarray}{c}\nu\in\mathcal{V}\setminus\mathbf{r}\end{subarray}} \mathbb{P}(\bm{\mathsf{v}}|\pi^{(o)}(\bm{\mathsf{v}}),\pi^{(u)}(\bm{\mathsf{v}}) )\Big{]}.\] (3)
6. _For any_ \(\bm{\mathsf{v}}_{1},\bm{\mathsf{v}}_{2}\in\mathcal{V}\)_, if_ \(\pi^{(u)}(\bm{\mathsf{v}}_{1})\cap\pi^{(u)}(\bm{\mathsf{v}}_{2})\neq\emptyset\)_, then_ \(\bm{\mathsf{v}}_{1}\) _and_ \(\bm{\mathsf{v}}_{2}\) _have a bi-directed edge in_ \(\mathcal{G}\)_._

In this work, we are interested in the causal effect of \(\mathbf{t}\) on \(\bm{\mathsf{y}}\), i.e., \(\mathbb{P}(\bm{\mathsf{y}}|do(\mathbf{t}=t))\). We define this formally by marginalizing all variables except \(\bm{\mathsf{y}}\) in the interventional distribution in (3).

**Definition 2**.: _The causal effect of \(\mathbf{t}\) (when forced to a value \(t\)) on \(\bm{\mathsf{y}}\) is given by:_

\[\mathbb{P}(\bm{\mathsf{y}}|do(\mathbf{t}=t))\!=\!\!\sum\limits_{ \begin{subarray}{c}\bm{\mathsf{v}}\setminus\{\bm{\mathsf{y}}\}\end{subarray}} \mathbb{P}\big{(}\bm{\mathsf{v}}\setminus\{\bm{\mathsf{y}}\},\bm{\mathsf{y}}|do( \mathbf{t}=t)\big{)}.\] (4)Next, we define the notion of average treatment effect for a binary treatment \(\mathfrak{t}\).

**Definition 3**.: _The average treatment effect (ATE) of a binary treatment \(\mathfrak{t}\) on outcome \(\gamma\) is given by \(\mathit{ATE}=\mathbb{E}[\gamma|do(\mathfrak{t}=1)]-\mathbb{E}[\gamma|do( \mathfrak{t}=0)]\)._

Next, we define when the causal effect (Definition 2) is said to be identifiable from the observational distribution and the causal graph.

**Definition 4**.: _(Causal effect identifiability) Given an observational distribution \(\mathbb{P}(\mathbf{v})\) and a causal graph \(\mathcal{G}\), the causal effect \(\mathbb{P}(\gamma|do(\mathfrak{t}=\mathfrak{t}))\) is identifiable if it is identical for every semi-Markovian Causal model with \((a)\) same graph \(\mathcal{G}\) and \((b)\) same observational distribution \(\mathbb{P}(\mathbf{v})\)._

In a causal graph \(\mathcal{G}\), a path is an ordered sequence of distinct nodes where each node is connected to the next in the sequence by an edge. A path starting at node \(w_{1}\) and ending at node \(w_{2}\) in \(\mathcal{G}\) is _blocked_ by a set \(\mathbf{w}\subset\mathcal{W}\setminus\{w_{1},w_{2}\}\) if there exists \(w\in\mathbf{w}\) such that (a) \(w\) is not a collider or (b) \(w\) is a collider and neither \(w\) nor any of it's descendant is in \(\mathbf{w}\). Further, \(w_{1}\) and \(w_{2}\) are said to be _\(d\)-separated_ by \(\mathbf{w}\) in \(\mathcal{G}\) if \(\mathbf{w}\) blocks every path between \(w_{1}\) and \(w_{2}\) in \(\mathcal{G}\). Let \(w_{1}\perp_{d}w_{2}|\mathbf{w}\) denote that \(w_{1}\) and \(w_{2}\) are d-separated by \(\mathbf{w}\) in \(\mathcal{G}\). Similarly, let \(w_{1}\perp_{p}w_{2}|\mathbf{w}\) denote that \(w_{1}\) and \(w_{2}\) are conditionally independent given \(\mathbf{w}\). We assume causal faithfulness, i.e., any conditional independence \(w_{1}\perp_{p}w_{2}|\mathbf{w}\) implies a d-separation relation \(w_{1}\perp_{d}w_{2}|\mathbf{w}\) in the causal graph \(\mathcal{G}\).

### Adjustment using pre-treatment variables

It is common in causal effect estimation to consider pre-treatment variables, i.e., variables that occur before the treatment in the causal ordering, and identify sets of variables that are _valid adjustments_. Specifically, a set \(\mathbf{z}\subset\mathcal{V}\) forms a valid adjustment if the causal effect can be written as \(\mathbb{P}(\gamma|do(\mathfrak{t}=\mathfrak{t}))=\sum_{\mathbf{z}}\mathbb{P} (\gamma|\mathfrak{t}=\mathfrak{t},\mathbf{z}=\mathbf{z})\mathbb{P}(\mathbf{z} =\mathbf{z})\). In other words, a valid adjustment \(\mathbf{z}\) averages an estimate of \(\gamma\) regressed on \(\mathfrak{t}\) and \(\mathbf{z}\) with respect to the marginal distribution of \(\mathbf{z}\), A popular criterion to find valid adjustments is to find a set \(\mathbf{z}\subset\mathcal{V}\) that satisfies the _back-door criterion_(Pearl, 2009). Formally, a set \(\mathbf{z}\) satisfies the back-door criterion if (a) it blocks all back-door paths, i.e., paths between \(\mathfrak{t}\) and \(\gamma\) that have an arrow pointing at \(\mathfrak{t}\) and (b) no element of \(\mathbf{z}\) is a descendant of \(\mathfrak{t}\). While, in general, back-door sets can be found with the knowledge of the causal graph, recent works (see the survey Cheng et al. (2022)) have proposed testable criteria for identifying back-door sets with some causal side information, without requiring the entire graph.

### Adjustment using post-treatment variables

While back-door adjustment is widely used, there are scenarios where no back-door set exists, e.g., when there is an unobserved confounder between \(\mathfrak{t}\) and \(\gamma\). If no back-door set can be found from the pre-treatment variables, Pearlian theory can be used to identify post-treatment variables, i.e., the variables that occur after the treatment in the causal ordering, to obtain a _front-door adjustment_.

**Definition 5** (_Front-door criterion_).: _A set \(\mathbf{z}\subset\mathcal{V}\) satisfies the front-door criterion with respect to \(\mathfrak{t}\) and \(\gamma\) if (a) \(\mathbf{z}\) intercepts all directed paths from \(\mathfrak{t}\) to \(\gamma\) (b) all back-door paths between \(\mathfrak{t}\) and \(\mathbf{z}\) are blocked, and (c) all back-door paths between \(\mathbf{z}\) and \(\gamma\) are blocked by \(\mathfrak{t}\)._

If a set \(\mathbf{z}\) satisfies the front-door criterion, then the causal effect can be written as

\[\mathbb{P}(\gamma|do(\mathfrak{t}=\mathfrak{t}))\!=\!\sum_{\mathbf{z}}\Big{(} \sum_{\mathfrak{t}^{\prime}}\mathbb{P}(\gamma|\mathfrak{t}=\mathfrak{t}^{ \prime},\mathbf{z}=\mathbf{z})\mathbb{P}(\mathfrak{t}=\mathfrak{t}^{\prime}) \Big{)}\mathbb{P}(\mathbf{z}=\mathbf{z}|\mathfrak{t}=\mathfrak{t}).\] (5)

Intuitively, front-door adjustment estimates the causal effect of \(\mathfrak{t}\) on \(\gamma\) as a composition of two effects: \((a)\) the effect of \(\mathfrak{t}\) on \(\mathbf{z}\) and \((b)\) the effect of \(\mathbf{z}\) on \(\gamma\). However, one still needs the knowledge of the causal graph \(\mathcal{G}\) to find a set satisfying the front-door criterion.

Inspired by the progress in finding back-door sets without knowing the entire causal graph, we ask: _Can testable conditions be derived to identify front-door-like sets using only partial structural information about post-treatment variables?_ To that end, we consider the following side information.

**Assumption 1**.: _The outcome \(\gamma\) is a descendant of the treatment \(\mathfrak{t}\)._

**Assumption 2**.: _There is an unobserved confounder between the outcome \(\gamma\) and the treatment \(\mathfrak{t}\)._

**Assumption 3**.: \(\mathbf{b}\)_, the set of all children of the treatment \(\mathfrak{t}\), is observed and known._Assumption 1 is a fundamental assumption in most causal inference works, as it forms the basis for estimating non-trivial causal effects. Without it, the causal effect would be zero. Assumption 2 rules out the existence of sets that satisfy the back-door criteria, necessitating a different way of estimating the causal effect. Assumption 3 captures our side information by requiring every children of the treatment to be known and observed. To contrast, the side information in data-driven works on back-door adjustment requires a parent of the treatment to be known and observed (Shah et al., 2022).

Our assumptions imply that \(\mathbf{b}\) intercepts all the directed paths from \(\mathbf{t}\) to \(\mathbf{\mathsf{y}}\). Given this, it is natural to ask whether \(\mathbf{b}\) satisfies the front-door criterion (Definition 5). We note that, in general, this is not true. We illustrate this via Figure 2 where we provide a causal graph \(\mathcal{G}^{toy}\) satisfying our assumptions. However, \(\mathbf{b}\) is not a valid front-door set in \(\mathcal{G}^{toy}\) as the back-door path between \(\mathbf{b}\) and \(\mathbf{\mathsf{y}}\) via \(\mathbf{z}^{(i)}\) is not blocked by \(\mathbf{t}\). Therefore, estimating the causal effect by assuming \(\mathbf{b}\) is a front-door set might not always give an unbiased estimate. In the next section, we leverage the given side information and provide testable conditions to identify front-door-like sets.

## 3 Front-door Adjustment Beyond Markov Equivalence

In this section, we provide our main results, an algorithm for ATE estimation, and discuss the relationship to PAG-based methods. Our main results use observational criteria for causal effect estimation under Assumptions 1 to 3 using post treatment variables.

### Causal effect estimation using post-treatment variables

First, we state a conditional independence statement implying causal identifiability. Then, we provide additional conditional independence statements resulting in a unique formula for effect estimation.

Causal identifiability (Definition 4) implies that the causal effect is uniquely determined given an observational distribution \(\mathbb{P}(\mathcal{V})\) and the corresponding causal graph \(\mathcal{G}\). We now show that satisfying a conditional independence statement (which can be tested solely from observational data, without requiring the graph \(\mathcal{G}\)) guarantees identifiability. We provide a proof in Appendix D.

**Theorem 3.1** (**Causal Identifiability**).: _Suppose Assumptions 1 to 3 hold. If there exists a set \(\mathbf{z}\subseteq\mathcal{V}\setminus\{t,\mathbf{b},\mathbf{\mathsf{y}}\}\) such that \(\mathbf{b}\perp\!\!\!\perp_{d}\mathbf{\mathsf{y}}|t,\mathbf{z}\), then the causal effect of \(t\) on \(\mathbf{\mathsf{y}}\) is identifiable from observational data without the knowledge of the underlying causal graph \(\mathcal{G}\)._

While the above result leads to identifiability, it does not provide a formula to compute the causal effect. In fact, the conditional independence \(\mathbf{b}\perp\!\!\!\perp_{d}\mathbf{\mathsf{y}}|t,\mathbf{z}\) alone is _insufficient_ to establish a unique formula, and different causal graphs lead to different formula. To illustrate this, we provide two SMCMs where Assumptions 1 to 3 and \(\mathbf{b}\perp\!\!\!\perp_{d}\mathbf{\mathsf{y}}|t,\mathbf{z}\) hold, i.e., causal effect is identifiable from observational data via Theorem 3.1, but the formula is different. First, consider the SMCM in Figure 3(top) with \(\mathbf{z}=(\mathbf{z}_{1},\mathbf{z}_{2})\) where causal effect is given by following formula (derived in Appendix D):

\[\mathbb{P}(\mathbf{\mathsf{y}}|do(\mathbf{t}=t))=\sum_{z_{1},z_{2},\mathbf{b }}\Big{(}\sum_{t^{\prime}}\mathbb{P}(\mathbf{\mathsf{y}}|z_{1},z_{2},t^{ \prime})\mathbb{P}(t^{\prime}|z_{1})\Big{)}\times\mathbb{P}(z_{2}|\mathbf{b}) \mathbb{P}(\mathbf{b}|t,z_{1})\mathbb{P}(z_{1}).\] (6)

Next, consider the SMCM in Figure 3(bottom) with \(\mathbf{z}=\mathbf{z}_{2}\) where the causal effect is given by the front-door adjustment formula in (5) as \(\mathbf{z}\) satisfies the front-door criterion. It remains to explicitly show that the formula in (6) is different from (5). To this end, we create a synthetic structural equation model (SEM) respecting the graph in Figure 3(top) and show that the formula in (5) gives a non-zero ATE error. In our SEM, the unobserved variable has a uniform distribution over \([1,2]\). Each observed variable except \(t\) is a sum of \((i)\) a linear combination of its parents with

Figure 3: SMCMs on (top) & (bottom) satisfy \(\mathbf{b}\perp\!\!\!\perp_{d}\mathbf{\mathsf{y}}|t,\mathbf{z}\) but have different causal effect estimation formulae.

Figure 2: The graph \(\mathcal{G}^{toy}\) satisfying Assumptions 1 to 3 where \(u_{i}\) are unobserved.

[MISSING_PAGE_FAIL:6]

### Algorithm for ATE estimation

The ATE can be computed by taking the first moment version of (9) or (10). In Algorithm 1, we provide a systematic way to estimate the ATE using Theorem 3.2 by searching for a set \(\mathbf{z}\in\mathcal{Z}\triangleq\mathcal{V}\setminus\{t,\mathbf{b},\mathbf{y}\}\) such that (a) p-value of conditional independence in (7) passes a threshold \(p_{v}\) and (b) there exists a decomposition \(\mathbf{z}=(\mathbf{z}^{(i)},\mathbf{z}^{(o)})\) such that p-values of conditional independencies in (8) pass the threshold \(p_{v}\). Then, for every such \(\mathbf{z}\), the algorithm computes the ATE using the first moment version of (9), and averages. The algorithm produces another estimate by using (10) instead of (9).

### Relation to PAG-based algorithms

Now, we exhibit how our approach can recover the causal effect in certain scenarios where PAG-based methods are not suitable. PAGs depict ancestral relationships (not necessarily direct) with directed edges and ambiguity in orientations (if they exist across members of the equivalence class) by circle marks. Figure 4(c) shows the PAG consistent with SMCM in Figure 4(a). While we formally define PAGs in Appendix B, we refer interested readers to Triantafillou and Tsamardinos (2015). The IDP algorithm of Jaber et al. (2019) is sound and complete for identifying causal effect from PAGs.

Consider SMCM in Figure 4(a) where our approach recovers the causal effect as \((i)\) Assumptions 1 to (3), \((ii)\) (7), and \((iii)\) (8) hold (where \((ii)\) and \((iii)\) can be tested from observational data). However, the IDP algorithm fails to recover the effect from the PAG. To see this, consider SMCM in Figure 4(b) which is Markov equivalent to SMCM in Figure 4(a), i.e., the PAG in Figure 4(c) is also consistent with SMCM in Figure 4(b). Intuitively, when the strength of the edge between \(t\) and \(\mathbf{b}\) is very small but the strength of the edge between \(t\) and \(\mathbf{y}\) is very high for both Figure 4(a) and Figure 4(b), causal effect in Figure 4(b) remains high while the causal effect in Figure 4(a) goes to zero.

We note that Assumptions 1 and 3, and (7) do not hold for the SMCM in Figure 4(b).

**Remark 2**.: _Obtaining a PAG requires a large number of sequential conditional independence tests where the choice of the next test depends on the previous ones (Claassen et al., 2013). The erroneous tests and orientation steps can potentially alter the structure of the PAG non-locally (see Strobl et al. (2016) for an example). This makes it difficult to control the false discovery rate for PAG based methods. Moreover, incorporating arbitrary side information into a PAG in a systematic way is still an open problem. In contrast, our approach does not rely on constructing a graphical object such as a PAG and the conditional independence tests could be carried out in parallel. Thus, our method can be viewed as a way to mitigate the issues associated with sequential testing by using structural side information. However, in scenarios where the structural side information is not available, we may have to resort to PAG-based methods._

## 4 Empirical Evaluation

We evaluate our approach empirically in 3 ways: \((i)\) we demonstrate the applicability of our method on a class of random graphs, \((ii)\) we assess the effectiveness of our method in estimating the ATE using finite samples, and \((iii)\) we showcase the potential of our method for causal fairness analysis.

### Applicability to a class of random graphs

In this experiment, we create a class of random SMCMs, sample 100 SMCMs from this class, and check if (7) and (8) hold by checking for corresponding d-separations in the SMCMs.

Figure 4: (a) An SMCM satisfying (7) and (8). (b) An SMCM obtained from (a) by modifying the edges between \(t\) and \(\mathbf{b}\) and between \(t\) and \(\mathbf{y}\). (c) The PAG corresponding to SMCM in (a) and (b).

**Creation of random SMCMs.** Let \(p\triangleq|\mathcal{V}|\) denote the dimension of observed variables including \(\mathbf{x}\), \(t\), and \(\mathpzc{y}\). Let \(\nu_{\mathsf{t}},\cdots,\nu_{\mathsf{p}}\) denote a causal ordering of these variables. Our random ensemble depends on two parameters: \((i)\)\(d\leq p/2\) which is the expected in-degree of variables \(\nu_{\mathsf{z}d},\cdots,\nu_{\mathsf{p}}\) and \((ii)\)\(q\leq p\) which controls the number of unobserved features. For \(1\leq i<j\leq p\), we add \(\nu_{\mathsf{i}}\longrightarrow\nu_{\mathsf{j}}\) with probability 0.5 if \(j\leq 2d\) and with probability \(d/(j-1)\) if \(j>2d\). We note that this procedure is such that the expected in-degree of variables \(\nu_{\mathsf{z}d},\cdots,\nu_{\mathsf{p}}\) is same (and equal to \(d\)) which is consistent with other recent work (e.g., Addanki et al. [2020]). Next, for \(1\leq i<j\leq p\), we add \(\nu_{\mathsf{i}}\ \ \texttt{\leftarrow}\ \ \nu_{\mathsf{j}}\) with probability \(q/p\) such that the expected number of unobserved features is \(q(p-1)/2\). Then, we choose \(\nu_{\mathsf{p}}\) as \(\mathpzc{y}\), any variable that is ancestor of \(\mathpzc{y}\) but not its parent or grandparent as \(\mathsf{t}\), and all children of \(\mathsf{t}\) as \(\mathbf{b}\). Finally, we add \(\mathsf{t}\ \ \texttt{\leftarrow}\ \ \mathpzc{y}\) if missing.

**Results.** We compare the success rate of two approaches: \((i)\) exhaustive search for \(\mathbf{z}\) satisfying (7) and (8) which is exponential in \(p\) and \((ii)\) search for a \(\mathbf{z}\) of size at-most 5 satisfying (7) and (8) which is polynomial in \(p\). We provide the number of successes of these approaches as a tuple in Table 1 for various \(p\), \(d\), and \(q\). We see that the two approaches have comparable performances. We also compare with the IDP algorithm by providing it the true PAG. However, it gives 0 successes across various \(p\), \(d\), and \(q\). We provide results for another random ensemble in Appendix G.

### Estimating the ATE

In this experiment, we generate synthetic data using the 6 random SMCMs in Section 4.1 for \(p=10\), \(d=2\), and \(q=1.0\) where our approach was successful indicating existence of \(\mathbf{z}=(\mathbf{z}^{(i)},\mathbf{z}^{(j)})\) such that the conditional independence statements in Theorem 3.2 hold. Then, we use Algorithm 1 to compute the error in estimating ATE and compare against a Baseline which uses the front-door adjustment in (5) with \(\mathbf{z}=\mathbf{b}\) given the side information in Assumption 3. We provide the results for the same experiment for specific choices of SMCMs including the one in Figure 2 in Appendix G. We also provide the 6 random SMCMs in Appendix G. We use RCoT hypothesis test [Strobl et al., 2019] for conditional independence testing from finite data.

**Data generation.** We use the following procedure to generate data from every SMCM. We generate unobserved variables independently from \(\operatorname{Unif}[1,2]\) which denotes the uniform distribution over \([1,2]\). For every observed variable \(\mathpzc{v}\in\mathcal{V}\), let \(\pi(\mathpzc{v})\triangleq(\pi^{(\mathpzc{v})}(\mathpzc{v}),\pi^{(\mathpzc{u} )}(\mathpzc{v}))\in\mathbb{R}^{d_{v}\times 1}\) denote the set of observed and unobserved parents of \(\mathpzc{v}\) stacked as a column vector. Then, we generate \(\mathpzc{v}\in\mathcal{V}\) as

\[\mathpzc{v}=\mathbf{a}_{\mathpzc{v}}^{\top}\pi(\mathpzc{v})+0.1\ \ \mathcal{N}(0,1)\text{ for } \mathpzc{v}\in\mathcal{V}\setminus\{\mathpzc{t}\}\quad\text{and}\quad\mathsf{ t}=\operatorname{Bernoulli}(\operatorname{Sigmoid}(\mathbf{a}_{t}^{\top}\pi(\mathpzc{t})))\] (11)

where the coefficients \(\mathbf{a}_{\mathpzc{v}}\in\mathbb{R}^{d_{v}\times 1}\) with every entry sampled independently from \(\operatorname{Unif}[1,2]\). Also, to generate the true ATE, we intervene on the generation model in (11) by setting \(t=0\) and \(t=1\).

**Results.** For every SMCM, we generate \(n\) samples of every observed variable in every run of the experiment. We average the ATE error over 10 such runs where the coefficients in (11) vary across runs. We report the average of these averages over the 6 SMCMs in Figure 5 for various \(n\). While the error rates of Baseline and Algorithm 1 are of the similar order for \(n=100\), Algorithm 1 gives much lower errors for \(n=1000\) and \(n=10000\) showing the efficacy of our method.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline  & & \(p=10\) & & & \(p=15\) & \\  & \(d=2\) & \(d=3\) & \(d=4\) & \(d=2\) & \(d=3\) & \(d=4\) \\ \hline \hline \(q=0.0\) & \((43,43)\) & \((20,20)\) & \((21,21)\) & \((27,26)\) & \((9,9)\) & \((4,2)\) \\ \(q=0.5\) & \((23,23)\) & \((16,16)\) & \((7,7)\) & \((18,17)\) & \((4,3)\) & \((0,0)\) \\ \(q=1.0\) & \((6,6)\) & \((4,4)\) & \((5,5)\) & \((9,9)\) & \((10,9)\) & \((0,0)\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Number of successes out of 100 random graphs for our methods shown as a tuple. The first method searches a \(\mathbf{z}\) exhaustively and the second method searches a \(\mathbf{z}\) with size at-most 5.

Figure 5: Average ATE for Algorithm 1 and Baseline vs. number of samples.

### Experiments with real-world fairness benchmarks

Next, we describe how our results enable finding front-door-like adjustment sets in fairness problems. In a typical fairness problem, the goal is to ensure that the outcome variable \(\gamma\) does not unfairly depend on the sensitive/protected attribute, e.g., race or gender (which we define to be treatment variable \(t\)), which would reflect undesirable biases. Often, the outcome is a descendant of the sensitive attribute (as per Assumption 1), and both outcome and sensitive attribute are confounded by unobserved variables (as per Assumption 2). Furthermore, there are be a multitude of measured post-sensitive-attribute variables that can affect the outcome. This stands in contrast to the usual settings for causal effect estimation, where pre-treatment variables are primarily utilized.

Fairness problems are typically evaluated using various fairness metrics, such as causal fairness metrics or observational metrics. Causal metrics require knowing the underlying causal graph, which can be a challenge in practice. Observational criteria can be decomposed into three types of effects (Zhang and Bareinboim, 2018; Plecko and Bareinboim, 2022): spurious effects, direct effects, and indirect effects (through descendants of sensitive attribute). In some scenarios, capturing the sum of direct and indirect effects is of interest, but even this requires knowing the causal graph.

Now, we demonstrate the application of our adjustment formulae in Theorem 3.2 to compute the sum of direct and indirect effects of the sensitive attribute on the outcome, while separating it from spurious effects. The sum of these effects is indeed the causal effect of sensitive attribute on the outcome. In other words, we consider the following fairness metric: \(\mathbb{E}[\gamma|do(t=1)]-\mathbb{E}[\gamma|do(t=0)]\). We assume that all the children of the sensitive attribute are known, which may be easier to justify compared to the typical assumption in causal fairness literature of knowing the entire causal graph.

German Credit Dataset.The German Credit dataset (Hofmann, 1994) is used for credit risk analysis where the goal is to predict whether a loan applicant is a good or bad credit risk based on applicant's 20 demographic and socio-economic attributes. The binary credit risk is the outcome \(\gamma\) and the applicant's age (binarized by thresholding at \(25\)(Kamiran and Calders, 2009)) is the sensitive attribute \(t\). Further, the categorical attributes are one-hot encoded.

We apply Algorithm 1 with \(n_{r}=100\) and \(p_{v}=0.1\) where we search for a set \(\mathbf{z}=(\mathbf{z}^{(o)},\mathbf{z}^{(i)})\) of size at most \(3\) under the following two distinct assumptions on the set of all children \(\mathbf{b}\) of \(t\):

1. When considering \(\mathbf{b}\!=\!\{\#\text{ of people financially dependent on the applicant, applicant's savings, applicant's job}\}\), Algorithm 1 results in \(\mathbf{z}^{(i)}\!=\!\{\text{purpose for which the credit was needed, indicator of whether the applicant was a foreign worker}\}\), \(\mathbf{z}^{(o)}\!=\!\{\text{installment plans from providers other than the credit-giving bank}\}\), \(\mathbf{ATE}_{z}=0.0125\pm 0.0011\), and \(\mathbf{ATE}_{s}=0.0105\pm 0.0018\).
2. When considering \(\mathbf{b}\!=\!\{\#\text{ of people financially dependent on the applicant, applicant's savings}\}\), Algorithm 1 results in \(\mathbf{z}^{(i)}\!=\!\{\text{purpose for which the credit was needed, applicant's checking account status with the bank}\}\), \(\mathbf{z}^{(o)}\!=\!\{\text{installment plans from providers other than the credit-giving bank}\}\), \(\mathbf{ATE}_{z}=0.0084\pm 0.0008\), and \(\mathbf{ATE}_{s}=-0.0046\pm 0.0021\).

Under the first assumption above, the causal effect using the adjustment formulae in (9) and (10) have same sign and are close in magnitude. However, under the second assumption, the effect flips sign. The results suggest that the second hypothesis regarding \(\mathbf{b}\) is incorrect, implying that applicant's job may indeed be a direct child of applicant's age, which aligns with intuition.

The dataset has only 1000 samples, which increases the possibility of detecting independencies in our criterion by chance, even with the size of \(\mathbf{z}\) constrained. To address this issue, we use 100 random bootstraps with a sample size equal to half of the training data and evaluate the p-value of our conditional independence criteria for all subsets returned by our algorithm. We select the subset \(\mathbf{z}\) with the highest median p-value (computed over the bootstraps) and use it in our adjustment formulae on a held out test set. To assess the conditional independencies associated with the selected \(\mathbf{z}\), we plot a histogram of the corresponding p-values for all these bootstraps. If the conditional independencies hold, we expect the p-values to be spread out, which we observe in the histograms in Figure 6 for the first choice of \(\mathbf{b}\). We report similar results for the second choice of \(\mathbf{b}\) in Appendix G.

**Adult Dataset:** We perform a similar analysis on the Adult dataset (Kohavi and Becker, 1996). With suitable choices of \(\mathbf{b}\), Algorithm 1 was unable to find a suitable \(\mathbf{z}\) satisfying \(\mathbf{b}\perp_{p}\gamma|\mathbf{z}\), \(t\). This suggests that in this dataset, there may not be any non-child descendants of the sensitive attribute, which is required for our criterion to hold. More details can be found in Appendix G.

## 5 Conclusion, Limitations, and Future Work

In this work, we proposed sufficient conditions for causal effect estimation through a generalized front-door adjustment given structural side information irrespective of the number or complexity of the latent confounders. Our approach can identify causal effect in graphs where known Markov equivalence classes do not allow identification. However, our approach relies primarily on two assumptions: Assumption 2 and Assumption 3.

Assumption 2 plays a crucial role in Theorem 3.2 (see the discussion in Section E.3) as it requires the presence of an unobserved confounder between the treatment variable and the outcome variable. This assumption is necessary for the applicability of our approach. If Assumption 2 does not hold, it implies that there is a set that satisfies the back-door criterion, and existing methods for finding back-door adjustment sets (Entner et al., 2013; Cheng et al., 2020; Shah et al., 2022) can be utilized. This suggests that our results could be derived under the weaker condition that there is an unblockable back-door path between \(\mathsf{t}\) and \(\mathsf{y}\). However, in many real-world scenarios, the presence of unobserved variables that potentially confound the treatment and the outcome is common, and we expect Assumption 2 to be true.

Assumption 3 is the requirement of knowing the entire set of children of the treatment variable within the causal graph. While this is strictly less demanding than specifying the entire causal graph, it may still present practical challenges in real-world scenarios. For instance, in large-scale observational studies or domains with numerous variables, exhaustively identifying all the children may be computationally demanding. It remains unclear whether one can estimate the causal effect using front-door-like adjustment with even less side information, e.g., knowing only one child of the treatment variable or knowing any subset of children of the treatment variable. An important future direction could be to approximate the causal effect when only the children corresponding to weak edges are unknown. Such variations around our condition are promising directions for future work. However, until then, one could seek input from domain experts. These experts possess valuable knowledge and insights about the specific domain under study, which can aid in identifying all the relevant variables that serve as children of the treatment.

The time complexity of Algorithm 1 is exponential due to its search over all possible subsets of observed variables (except \(t,\mathbf{b},\mathsf{y}\)). While this is inherent in the general case, recent work by Shah et al. (2022) proposed a scalable approximation for conditional independence testing using continuous optimization by exploiting the principle of invariant risk minimization, specifically for back-door adjustment without the need for the causal graph. However, extending this approach to multiple conditional independence tests, as required in (7) and (8), remains an open challenge. Therefore, exploring the development of continuous optimization-based methods for scalability of front-door adjustment in the absence of the causal graph is crucial. Further, Algorithm 1 could potentially be augmented with ideas from double machine learning and inverse variance weighting for bias and variance reduction.

Lastly, it is important to note that Theorem 3.2 provides only sufficient conditions for causal effect estimation. This means that there may be cases where front-door-like adjustment is possible, but the conditions stated in Theorem 3.2 do not hold. Specifically, there could be scenarios, such as when the outcome variable is a child of the children of the treatment variable, i.e., \(y\) is a child of \(\mathbf{b}\), where conditions (7) and (8) are not satisfied.

Figure 6: Histograms of p-values of the conditional independencies in (9) and (10) over 100 bootstrap runs for \(\mathbf{b}=\{\mathsf{\#}\text{ of people financially dependent on the applicant, applicant’s savings, applicant’s job}\}\).

## Acknowledgements

We thank the anonymous reviewers for their comments and suggestions. Murat Kocaoglu acknowledges the support of NSF Grant CAREER 2239375.

## References

* Acharya et al. (2018) J. Acharya, A. Bhattacharyya, C. Daskalakis, and S. Kandasamy. Learning and testing causal models with interventions. _Advances in Neural Information Processing Systems_, 31, 2018.
* Addanki et al. (2020) R. Addanki, S. Kasiviswanathan, A. McGregor, and C. Musco. Efficient intervention design for causal discovery with latents. In _International Conference on Machine Learning_, pages 63-73. PMLR, 2020.
* Bellemare et al. (2019) M. F. Bellemare, J. R. Bloem, and N. Wexler. The paper of how: Estimating treatment effects using the front-door criterion. Technical report, Working paper, 2019.
* Bhattacharya and Nabi (2022) R. Bhattacharya and R. Nabi. On testability of the front-door model via verma constraints. In _Uncertainty in Artificial Intelligence_, pages 202-212. PMLR, 2022.
* Castro et al. (2020) D. C. Castro, I. Walker, and B. Glocker. Causality matters in medical imaging. _Nature Communications_, 11(1):1-10, 2020.
* Cheng et al. (2020) D. Cheng, J. Li, L. Liu, K. Yu, T. D. Lee, and J. Liu. Towards unique and unbiased causal effect estimation from data with hidden variables. _arXiv preprint arXiv:2002.10091_, 2020.
* Cheng et al. (2022) D. Cheng, J. Li, L. Liu, J. Liu, and T. D. Le. Data-driven causal effect estimation based on graphical causal modelling: A survey. _arXiv preprint arXiv:2208.09590_, 2022.
* Claassen et al. (2013) T. Claassen, J. M. Mooij, and T. Heskes. Learning sparse causal models is not np-hard. In _Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence_, pages 172-181, 2013.
* Entner et al. (2013) D. Entner, P. Hoyer, and P. Spirtes. Data-driven covariate selection for nonparametric estimation of causal effects. In _Artificial Intelligence and Statistics_, pages 256-264. PMLR, 2013.
* Fulcher et al. (2020) I. R. Fulcher, I. Shpitser, S. Marealle, and E. J. Tchetgen Tchetgen. Robust inference on population indirect causal effects: the generalized front door criterion. _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_, 82(1):199-214, 2020.
* Glynn and Kashin (2017) A. N. Glynn and K. Kashin. Front-door difference-in-differences estimators. _American Journal of Political Science_, 61(4):989-1002, 2017.
* Glynn and Kashin (2018) A. N. Glynn and K. Kashin. Front-door versus back-door adjustment with unmeasured confounding: Bias formulas for front-door and hybrid adjustments with application to a job training program. _Journal of the American Statistical Association_, 113(523):1040-1049, 2018.
* Gultunin et al. (2020) L. Gultunin, M. Kusner, V. Kanade, and R. Silva. Differentiable causal backdoor discovery. In _International Conference on Artificial Intelligence and Statistics_, pages 3970-3979. PMLR, 2020.
* Gupta et al. (2021) S. Gupta, Z. C. Lipton, and D. Childers. Estimating treatment effects with observed confounders and mediators. In _Uncertainty in Artificial Intelligence_, pages 982-991. PMLR, 2021.
* Hofmann (1994) H. Hofmann. Statlog (German Credit Data). UCI Machine Learning Repository, 1994.
* Hunermund and Bareinboim (2019) P. Hunermund and E. Bareinboim. Causal inference and data fusion in econometrics. _arXiv preprint arXiv:1912.09104_, 2019.
* Jaber et al. (2019) A. Jaber, J. Zhang, and E. Bareinboim. Causal identification under markov equivalence: Completeness results. In _International Conference on Machine Learning_, pages 2981-2989. PMLR, 2019.
* Jeong et al. (2022) H. Jeong, J. Tian, and E. Bareinboim. Finding and listing front-door adjustment sets. _arXiv preprint arXiv:2210.05816_, 2022.
* Kamiran and Calders (2009) F. Kamiran and T. Calders. Classifying without discriminating. In _2009 2nd international conference on computer, control and communication_, pages 1-6. IEEE, 2009.
* Koca and Koca (2018)R. Kohavi and B. Becker. UCI machine learning repository, 1996. URL http://archive.ics.uci.edu/ml/datasets/adult.
* Kuroki [2000] M. Kuroki. Selection of post-treatment variables for estimating total effect from empirical research. _Journal of the Japan Statistical Society_, 30(2):115-128, 2000.
* Kuroki and Cai [2012] M. Kuroki and Z. Cai. Selection of identifiability criteria for total effects by using path diagrams. _arXiv preprint arXiv:1207.4140_, 2012.
* Mogstad and Torgovitsky [2018] M. Mogstad and A. Torgovitsky. Identification and extrapolation of causal effects with instrumental variables. _Annual Review of Economics_, 10:577-613, 2018.
* Nabi et al. [2019] R. Nabi, D. Malinsky, and I. Shpitser. Learning optimal fair policies. In _International Conference on Machine Learning_, pages 4674-4682. PMLR, 2019.
* Pearl [1993] J. Pearl. [bayesian analysis in expert systems]: Comment: graphical models, causality and intervention. _Statistical Science_, 8(3):266-269, 1993.
* Pearl [1995] J. Pearl. Causal diagrams for empirical research. _Biometrika_, 82(4):669-688, 1995.
* Pearl [2009] J. Pearl. _Causality_. Cambridge university press, 2009.
* Perkovic et al. [2018] E. Perkovic, J. Textor, M. Kalisch, and M. H. Maathuis. Complete graphical characterization and construction of adjustment sets in markov equivalence classes of ancestral graphs. _The Journal of Machine Learning Research_, 2018.
* Plecko and Bareinboim [2022] D. Plecko and E. Bareinboim. Causal fairness analysis. _arXiv preprint arXiv:2207.11385_, 2022.
* Shah et al. [2022] A. Shah, K. Shanmugam, and K. Ahuja. Finding valid adjustments under non-ignorability with minimal dag knowledge. In _International Conference on Artificial Intelligence and Statistics_, pages 5538-5562. PMLR, 2022.
* Shpitser and Pearl [2006] I. Shpitser and J. Pearl. Identification of joint interventional distributions in recursive semi-markovian causal models. In _Proceedings of the National Conference on Artificial Intelligence_, volume 21, page 1219. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999, 2006.
* Spirtes et al. [2000] P. Spirtes, C. N. Glymour, and R. Scheines. _Causation, prediction, and search_. MIT press, 2000.
* Strobl et al. [2016] E. V. Strobl, P. L. Spirtes, and S. Visweswaran. Estimating and controlling the false discovery rate for the pc algorithm using edge-specific p-values. _arXiv preprint arXiv:1607.03975_, 2016.
* Strobl et al. [2019a] E. V. Strobl, P. L. Spirtes, and S. Visweswaran. Estimating and controlling the false discovery rate of the pc algorithm using edge-specific p-values. _ACM Transactions on Intelligent Systems and Technology (TIST)_, 10(5):1-37, 2019a.
* Strobl et al. [2019b] E. V. Strobl, K. Zhang, and S. Visweswaran. Approximate kernel-based conditional independence tests for fast non-parametric causal discovery. _Journal of Causal Inference_, 7(1), 2019b.
* Tian and Pearl [2002] J. Tian and J. Pearl. _A general identification condition for causal effects_. eScholarship, University of California, 2002.
* Triantafillou and Tsamardinos [2015] S. Triantafillou and I. Tsamardinos. Constraint-based causal discovery from multiple interventions over overlapping variable sets. _The Journal of Machine Learning Research_, 16(1):2147-2205, 2015.
* Veitch and Zaveri [2020] V. Veitch and A. Zaveri. Sense and sensitivity analysis: Simple post-hoc analysis of bias due to unobserved confounding. _Advances in Neural Information Processing Systems_, 33:10999-11009, 2020.
* Verma and Pearl [1990] T. Verma and J. Pearl. Causal networks: Semantics and expressiveness. In _Machine intelligence and pattern recognition_, volume 9, pages 69-76. Elsevier, 1990.
* Wienobst et al. [2022] M. Wienobst, B. van der Zander, and M. Liskiewicz. Finding front-door adjustment sets in linear time. _arXiv preprint arXiv:2211.16468_, 2022.
* Wienobst et al. [2019]J. Zhang. On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias. _Artificial Intelligence_, 172(16-17):1873-1896, 2008.
* Zhang and Bareinboim (2018) J. Zhang and E. Bareinboim. Fairness in decision-making--the causal explanation formula. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 32, 2018.

Broader impact

In this work, we propose an algorithm for estimating causal effects from observational studies without relying on expert knowledge of the causal model. Our approach is particularly valuable in scenarios where conducting randomized control trials (RCTs) is challenging or unethical, such as in healthcare settings where consensus treatment protocols are often determined based on observational data. While our algorithm shows promise in providing accurate causal effect estimates, it is crucial to address the potential negative impact of incorrect results that may arise from our work.

One significant concern is the possibility of miscalculating the treatment effect due to limitations in testing power at finite sample sizes or the misidentification of certain features as direct children of the treatment variable. This introduces the risk of inaccurate estimations, which could have detrimental consequences when making decisions or establishing treatment protocols based on the conclusions derived from our algorithm. It is essential to approach the interpretation of our algorithm's results with caution and subject them to critical evaluation. It is worth noting that the potential for incorrect results is not unique to our algorithm but is inherent in most observational studies and effect estimation algorithms. Acknowledging these potential negative impacts emphasizes the need for further research to improve the reliability and accuracy of causal effect estimation in observational studies.

## Appendix B Preliminaries about ancestral graphs

In this section, we provide the definition of _partial ancestral graphs_ (PAGs). PAGs are defined using _maximal ancestral graphs_ (MAGs). Below, we define MAGs and PAGs based on their construction from directed acyclic graphs (DAGs).

A MAG can be obtained from a DAG as follows: if two observed nodes \(\mathsf{x}_{1}\) and \(\mathsf{x}_{2}\) cannot be \(\mathsf{d}\)-separated conditioned on any subset of observed variables, then \((i)\)\(\mathsf{x}_{1}\)\(\rightarrow\)\(\mathsf{x}_{2}\) is added in the MAG if \(\mathsf{x}_{1}\) is an ancestor of \(\mathsf{x}_{2}\) in the DAG, \((ii)\)\(\mathsf{x}_{2}\)\(\rightarrow\)\(\mathsf{x}_{1}\) is added in the MAG if \(\mathsf{x}_{2}\) is an ancestor of \(\mathsf{x}_{1}\) in the DAG, and \((iii)\)\(\mathsf{x}_{1}\)\(\leftarrow\)\(\rightarrow\)\(\mathsf{x}_{2}\) is added in the MAG if \(\mathsf{x}_{1}\) and \(\mathsf{x}_{2}\) are not ancestrally related in the DAG. \((iv)\) After the above three operations, if both \(\mathsf{x}_{1}\)\(\leftarrow\)\(\rightarrow\)\(\mathsf{x}_{2}\) and \(\mathsf{x}_{1}\)\(\rightarrow\)\(\mathsf{x}_{2}\) are present, we retain only the directed edge. In general, a MAG represents a collection of DAGs that share the same set of observed variables and exhibit the same independence and ancestral relations among these observed variables. It is possible for different MAGs to be Markov equivalent, meaning they represent the exact same independence model.

A PAG shares the same adjacencies as any MAG in the observational equivalence class of MAGs. An end of an edge in the PAG is marked with an arrow (\(>\) or \(<\)) if the edge appears with the same arrow in all MAGs in the equivalence class. An end of an edge in the PAG is marked with a circle (\(o\)) if the edge appears as an arrow (\(>\) or \(<\)) and a tail (\(-\)) in two different MAGs in the equivalence class.

## Appendix C Rules of do-calculus

In this section, we provide the do-calculus rules of Pearl (1995) that are used to prove our main results in the following sections. We build upon the definition of semi-Markovian causal model from Section 2.

For any \(\mathbf{v}\in\mathcal{W}\), let \(\mathcal{G}_{\!\mathbf{v}}\) be the graph obtained by removing the edges going into \(\mathbf{v}\) in \(\mathcal{G}\), and let \(\mathcal{G}_{\!\mathbf{v}}\) be the graph obtained by removing the edges going out of \(\mathbf{v}\) in \(\mathcal{G}\).

**Theorem C.1** (Rules of do-calculus, Pearl (1995)).: _For any disjoint subsets \(\mathbf{v}_{1},\mathbf{v}_{2},\mathbf{v}_{3},\mathbf{v}_{4}\subseteq\mathcal{W}\), we have the following rules._

1. _[label=Rule 0:]_
2. \(\mathbb{P}(\mathbf{v}_{1}|do(\mathbf{v}_{2}),\mathbf{v}_{3},do(\mathbf{v}_{4})) =\mathbb{P}(\mathbf{v}_{1}|do(\mathbf{v}_{2}),\mathbf{v}_{3}, \mathbf{v}_{4})\) _if_ \(\mathbf{v}_{1}\)__\(\perp\)\(\perp\)\(\perp\)\(\perp\)\(\mathbf{v}_{4}|\mathbf{v}_{2},\mathbf{v}_{3}\) _in_ \(\mathcal{G}_{\!\mathbf{v}_{2},\mathbf{v}_{4}}\)_._
3. \(\mathbb{P}(\mathbf{v}_{1}|do(\mathbf{v}_{2}),\mathbf{v}_{3},do(\mathbf{v}_{4})) =\mathbb{P}(\mathbf{v}_{1}|do(\mathbf{v}_{2}),\mathbf{v}_{3})\) _if_ \(\mathbf{v}_{1}\)__\(\perp\)\(\perp\)\(\perp\)\(\mathbf{v}_{4}|\mathbf{v}_{2},\mathbf{v}_{3}\) _in_ \(\mathcal{G}_{\!\mathbf{v}_{2},\mathbf{v}_{4}}\)_._

_where \(\mathbf{v}_{4}(\mathbf{v}_{3})\) is the set of nodes in \(\mathbf{v}_{4}\) that are not ancestors of any node in \(\mathbf{v}_{3}\) in \(\mathcal{G}_{\!\mathbf{v}_{2}}\). Pearl (1995) also gave an alternative criterion for Rule 3._

1. _[label=Rule 0:]_
2. \(\mathbb{P}(\mathbf{v}_{1}|do(\mathbf{v}_{2}),\mathbf{v}_{3},do(\mathbf{v}_{4})) =\mathbb{P}(\mathbf{v}_{1}|do(\mathbf{v}_{2}),\mathbf{v}_{3})\) _if_ \(\mathbf{v}_{1}\)__\(\perp\)\(\perp

[MISSING_PAGE_FAIL:15]

[MISSING_PAGE_FAIL:16]

A generalized front-door condition

In this section, we prove Theorem 3.2. We begin by stating a few d-separation statements used in this proof. See Appendix F for a proof.

**Lemma 2**.: _Suppose Assumptions 1 to 3 and d-separation criteria in Theorem 3.2, i.e., (7) and (8) hold. Then,_

1. \(\gamma\perp\!\!\!\perp_{d}F_{t}|\bm{z},\mathbf{b}\) _in_ \(\mathcal{G}_{\mathbf{b}}^{t}\) _and_ \(\gamma\perp\!\!\!\perp_{d}F_{t}|\bm{z}^{(i)},\mathbf{b}\) _in_ \(\mathcal{G}_{\mathbf{b}}^{t}\)_._
2. \(t\perp\!\!\!\perp_{d}\mathbf{b}\) _in_ \(\mathcal{G}_{\mathbf{f}}\)_,_
3. \(t\perp\!\!\!\perp_{d}\bm{z}^{(i)}|\mathbf{b}\) _in_ \(\mathcal{G}_{\mathbf{f}}\)_,_
4. \(t\perp\!\!\!\perp_{d}F_{\mathbf{b}}|\bm{z}^{(i)}\) _in_ \(\mathcal{G}^{\mathbf{b}}\)_, and_
5. \(\gamma\perp\!\!\!\perp_{d}\mathbf{b}|t,\bm{z}^{(i)}\) _in_ \(\mathcal{G}_{\mathbf{b}}\)_._

Now, we proceed with the proof in two parts. In the first part, we prove (9), and in the second part, we prove (10).

### Proof of (9)

First, using the law of total probability, we have

\[\mathbb{P}(\gamma=y|do(t=t))=\sum_{\bm{z}}\mathbb{P}(\gamma=y|do(t=t),\bm{z} =\bm{z})\mathbb{P}(\bm{z}=\bm{z}|do(t=t)).\] (32)

Now, we show that the two terms in RHS of (32) can be simplified as follows

\[\mathbb{P}(\gamma=y|do(t=t),\bm{z}=\bm{z})=\sum_{t^{\prime}}\mathbb{P}(\gamma =y|\bm{z}=\bm{z},t=t^{\prime})\mathbb{P}(t=t^{\prime}).\] (33)

\[\mathbb{P}(\bm{z}=\bm{z}|do(t=t))=\mathbb{P}(\bm{z}=\bm{z}|t=t),\] (34)

Combining (33) and (34) completes the proof of (9).

Proof of (33):We have

\[\mathbb{P}(\gamma=y|do(t=t),\bm{z}=\bm{z}) \stackrel{{(a)}}{{=}}\mathbb{P}(\gamma=y|do(t=t), \bm{z}=\bm{z},\mathbf{b}=\bm{b})\] (35) \[\stackrel{{(b)}}{{=}}\mathbb{P}(\gamma=y|do(t=t), \bm{z}=\bm{z},do(\mathbf{b}=\bm{b}))\] (36) \[\stackrel{{(c)}}{{=}}\mathbb{P}(\gamma=y|\bm{z}=\bm{z },do(\mathbf{b}=\bm{b}))\] (37) \[\stackrel{{(d)}}{{=}}\sum_{t^{\prime}}\mathbb{P}( \gamma=y|\bm{z}=\bm{z},do(\mathbf{b}=\bm{b}),t=t^{\prime})\mathbb{P}(t=t^{ \prime}|\bm{z}=\bm{z},do(\mathbf{b}=\bm{b})),\] (38)

where \((a)\) follows from Rule 1, (7), and Fact 1, \((b)\) follows from Rule 2, (7), and Fact 1, and \((c)\) follows from Rule 3a and Lemma 2(a), and \((d)\) follows from the law of total probability.

Now, we simplify the first term in (38) as follows:

\[\mathbb{P}(\gamma=y|\bm{z}=\bm{z},do(\mathbf{b}=\bm{b}),t=t^{ \prime}) \stackrel{{(a)}}{{=}}\mathbb{P}(\gamma=y|\bm{z}=\bm{z },\mathbf{b}=\bm{b},t=t^{\prime})\] (39) \[\stackrel{{(\gamma)}}{{=}}\mathbb{P}(\gamma=y|\bm{z }=\bm{z},t=t^{\prime}),\] (40)

where \((a)\) follows from Rule 2, (7), and Fact 1. Likewise, we simplify the second term in (38) as follows:

\[\mathbb{P}(t=t^{\prime}|do(\mathbf{b}=\bm{b}),\bm{z}=\bm{z}) \stackrel{{(a)}}{{=}}\mathbb{P}(t=t^{\prime}|do(\mathbf{b}=\bm{b }),\bm{z}^{(i)}=\bm{z}^{(i)}) \stackrel{{(b)}}{{=}}\mathbb{P}(t=t^{\prime}|\bm{z}^{(i )}=\bm{z}^{(i)})\] (41) \[\stackrel{{(c)}}{{=}}\mathbb{P}(t=t^{\prime}),\] (42)where \((a)\) follows from Rule 1, (8), and Fact 1, \((b)\) follows from Rule 3a and Lemma 2(d), and \((c)\) follows (8).

Putting together (38), (40), and (42) results in (33).

Proof of (34):From the law of total probability, we have

\[\mathbb{P}(\mathbf{z}=\bm{z}|do(\mathbf{t}=t))=\sum_{\bm{b}} \mathbb{P}(\mathbf{z}=\bm{z}|do(\mathbf{t}=t),\mathbf{b}=\bm{b})\mathbb{P}( \mathbf{b}=\bm{b}|do(\mathbf{t}=t)).\] (43)

Now, we simplify the first term in (43) as follows:

\[\mathbb{P}(\mathbf{z}=\bm{z}|do(\mathbf{t}=t),\mathbf{b}=\bm{b})\] (44) \[\overset{(a)}{=}\mathbb{P}(\mathbf{z}^{(i)}=\bm{z}^{(i)}|do( \mathbf{t}=t),\mathbf{b}=\bm{b})\cdot\mathbb{P}(\mathbf{z}^{(o)}=\bm{z}^{(o)} |do(\mathbf{t}=t),\mathbf{b}=\bm{b},\mathbf{z}^{(i)}=\bm{z}^{(i)})\] (45) \[\overset{(b)}{=}\mathbb{P}(\mathbf{z}^{(i)}=\bm{z}^{(i)}|do( \mathbf{t}=t),\mathbf{b}=\bm{b})\cdot\mathbb{P}(\mathbf{z}^{(o)}=\bm{z}^{(o)} |\mathbf{t}=t,\mathbf{b}=\bm{b},\mathbf{z}^{(i)}=\bm{z}^{(i)})\] (46) \[\overset{(c)}{=}\mathbb{P}(\mathbf{z}^{(i)}=\bm{z}^{(i)}| \mathbf{t}=t,\mathbf{b}=\bm{b})\cdot\mathbb{P}(\mathbf{z}^{(o)}=\bm{z}^{(o)}| \mathbf{t}=t,\mathbf{b}=\bm{b},\mathbf{z}^{(i)}=\bm{z}^{(i)})\] (47) \[\overset{(d)}{=}\mathbb{P}(\mathbf{z}=\bm{z}|\mathbf{t}=t, \mathbf{b}=\bm{b}),\] (48)

where \((a)\) and \((d)\) follow from the definition of conditional probability, \((b)\) follows from Rule 2, (8), and Fact 1, and \((c)\) follows from Rule 2 and Lemma 2(c). Likewise, we simplify the second term in (43) as follows:

\[\mathbb{P}(\mathbf{b}=\bm{b}|do(\mathbf{t}=t))\overset{(a)}{=} \mathbb{P}(\mathbf{b}=\bm{b}|\mathbf{t}=t),\] (49)

where \((a)\) follows from Rule 2 and Lemma 2(b).

Putting together (43), (48), and (49), results in (34) as follows:

\[\mathbb{P}(\mathbf{z}=\bm{z}|do(\mathbf{t}=t))=\sum_{\bm{b}} \mathbb{P}(\mathbf{z}=\bm{z}|\mathbf{t}=t,\mathbf{b}=\bm{b})\mathbb{P}( \mathbf{b}=\bm{b}|\mathbf{t}=t)\overset{(a)}{=}\mathbb{P}(\mathbf{z}=\bm{z}| \mathbf{t}=t),\] (50)

where \((a)\) follows from the law of total probability.

### Proof of (10)

First, using the law of total probability, we have

\[\mathbb{P}(\bm{\mathsf{y}}=y|do(\mathbf{t}=t))=\sum_{\bm{b}} \mathbb{P}(\bm{\mathsf{y}}=y|do(\mathbf{t}=t),\mathbf{b}=\bm{b})\mathbb{P}( \mathbf{b}=\bm{b}|do(\mathbf{t}=t)).\] (51)

Now, we show that the first term in RHS of (51) can be simplified as follows

\[\mathbb{P}(\bm{\mathsf{y}}=y|do(\mathbf{t}=t),\mathbf{b}=\bm{b})\] (52) \[=\sum_{\bm{z}^{(i)}}\Big{(}\sum_{t^{\prime}}\mathbb{P}(\bm{ \mathsf{y}}=y|\mathbf{s}=\bm{s},\mathbf{t}=t^{\prime})\mathbb{P}(\mathbf{t}=t^ {\prime})\Big{)}\mathbb{P}(\mathbf{z}^{(i)}=\bm{z}^{(i)}|\mathbf{b}=\bm{b}, \mathbf{t}=t).\] (53)

where \(\mathbf{s}\triangleq(\mathbf{b},\mathbf{z}^{(i)})\). Using (49) and (53) in (51), completes the proof of (10) as follows:

\[\mathbb{P}(\bm{\mathsf{y}}=y|do(\mathbf{t}=t))\] (54) \[=\sum_{\bm{s}}\Big{(}\sum_{t^{\prime}}\mathbb{P}(\bm{\mathsf{y} }=y|\mathbf{s}=\bm{s},\mathbf{t}=t^{\prime})\mathbb{P}(\mathbf{t}=t^{\prime}) \Big{)}\mathbb{P}(\mathbf{z}^{(i)}=\bm{z}^{(i)}|\mathbf{b}=\bm{b},\mathbf{t}=t )\mathbb{P}(\mathbf{b}=\bm{b}|\mathbf{t}=t)\] (55) \[\overset{(a)}{=}\sum_{\bm{s}}\Big{(}\sum_{t^{\prime}}\mathbb{P}( \bm{\mathsf{y}}=y|\mathbf{s}=\bm{s},\mathbf{t}=t^{\prime})\mathbb{P}(\mathbf{t}= t^{\prime})\Big{)}\mathbb{P}(\mathbf{s}=\bm{s}|\mathbf{t}=t),\] (56)

where \((a)\) follows from the definition of conditional probability.

Proof of (53):From the law of total probability, we have

\[\mathbb{P}(\bm{\nu}=y|do(\bm{t}=t),\bm{b}=\bm{b})\] (57) \[= \sum_{\bm{z}^{(i)}}\mathbb{P}(\bm{\nu}=y|\bm{b}=\bm{b},\bm{z}^{(i)} \!=\!\bm{z}^{(i)},do(\bm{t}=t))\mathbb{P}(\bm{z}^{(i)}\!=\!\bm{z}^{(i)}|\bm{b}= \bm{b},do(\bm{t}=t)).\] (58)

Now, we simplify the first term in (58) as follows:

\[\mathbb{P}(\bm{\nu}=y|\bm{b}=\bm{b},\bm{z}^{(i)}=\bm{z}^{(i)},do( \bm{t}=t))\] (59) \[\overset{(a)}{=}\mathbb{P}(\bm{\nu}=y|do(\bm{b}=\bm{b}),\bm{z}^{ (i)}=\bm{z}^{(i)},do(\bm{t}=t))\] (60) \[\overset{(b)}{=}\mathbb{P}(\bm{\nu}=y|do(\bm{b}=\bm{b}),\bm{z}^{ (i)}=\bm{z}^{(i)})\] (61) \[\overset{(c)}{=}\sum_{t^{\prime}}\mathbb{P}(\bm{\nu}=y|do(\bm{b} =\bm{b}),\bm{z}^{(i)}=\bm{z}^{(i)},\bm{t}=t^{\prime})\mathbb{P}(\bm{t}=t^{ \prime}|do(\bm{b}=\bm{b}),\bm{z}^{(i)}=\bm{z}^{(i)}),\] (62)

where \((a)\) follows from Rule 2, Lemma 2(e), and Fact 1, \((b)\) follows from Rule 3a and Lemma 2(a), and \((c)\) follows from the law of total probability. We further simplify the first term in (62) as follows:

\[\mathbb{P}(\bm{\nu}=y|do(\bm{b}=\bm{b}),\bm{z}^{(i)}=\bm{z}^{(i)},\bm{t}=t^{ \prime})\overset{(a)}{=}\mathbb{P}(\bm{\nu}=y|\bm{b}=\bm{b},\bm{z}^{(i)}=\bm{ z}^{(i)},\bm{t}=t^{\prime}),\] (63)

where \((a)\) follows from Rule 2 and Lemma 2(e). Using (63) and (42) in (62), we have

\[\mathbb{P}(\bm{\nu}=y|\bm{b}=\bm{b},\bm{z}^{(i)}=\bm{z}^{(i)},do(\bm{t}=t))= \sum_{t^{\prime}}\mathbb{P}(\bm{\nu}=y|\bm{b}=\bm{b},\bm{z}^{(i)}=\bm{z}^{(i) },\bm{t}=t^{\prime})\mathbb{P}(\bm{t}=t^{\prime}).\] (64)

Now, we simplify the second term in (58) as follows:

\[\mathbb{P}(\bm{z}^{(i)}=\bm{z}^{(i)}|\bm{b}=\bm{b},do(\bm{t}=t))\overset{(a)} {=}\mathbb{P}(\bm{z}^{(i)}=\bm{z}^{(i)}|\bm{b}=\bm{b},\bm{t}=t),\] (65)

where \((a)\) follows from Rule 2 and Lemma 2(c).

Putting together (58), (64), and (65) results in (53).

### Necessity of Assumption 2

In this section, we provide an example to signify the importance of Assumption 2 to Theorem 3.2. Consider the semi-Markovian causal model in Figure 7 where Assumptions 1 and 3 hold but Assumption 2 does not hold.

While \(\bm{z}=(\bm{z}^{(i)},\bm{z}^{(o)})\) satisfies (7) and (8) where \(\bm{z}^{(i)}=\emptyset\), the causal effect is not equal to the formulae in (9) or (10). To see this, we note that the set \(\{\bm{a}\}\) is a back-door set in Figure 7 implying

\[\mathbb{P}(\bm{\nu}|do(\bm{t}=t))=\sum_{a}\mathbb{P}(\bm{\nu}|\bm{a},t) \mathbb{P}(\bm{a}).\] (66)

Now, we simplify the right hand side of (66) to show explicitly that it is not equivalent to (9). From the law of total probability, we have

\[\mathbb{P}(\bm{\nu}|\bm{a},\bm{t})=\sum_{\bm{z}}\mathbb{P}(\bm{\nu}|\bm{z}, \bm{a},\bm{t})\mathbb{P}(\bm{z}|\bm{a},\bm{t})\] (67)

Figure 7: An SMCM signifying the importance of Assumption 2

\[\stackrel{{(a)}}{{=}}\sum_{\mathbf{z}}\Big{(}\sum_{t^{\prime}} \mathbb{P}(\mathbf{\nu}|\mathbf{z},\mathsf{a},t)\mathbb{P}(t^{\prime})\Big{)} \mathbb{P}(\mathbf{z}|\mathsf{a},t)\stackrel{{(b)}}{{=}}\sum_{ \mathbf{z}}\Big{(}\sum_{t^{\prime}}\mathbb{P}(\mathbf{\nu}|\mathbf{z},t^{ \prime})\mathbb{P}(t^{\prime})\Big{)}\mathbb{P}(\mathbf{z}|\mathsf{a},t),\] (68)

where \((a)\) follows because \(\sum_{t^{\prime}}\mathbb{P}(t^{\prime})=1\) and \((b)\) follows \(\mathbf{\nu}\) is independent of every other variable conditioned on \(\mathbf{z}\). Plugging (68) in (66), we have

\[\mathbb{P}(\mathbf{\nu}|do(t=t))=\sum_{\mathbf{z}}\Big{(}\sum_{t^{\prime}} \mathbb{P}(\mathbf{\nu}|\mathbf{z},t^{\prime})\mathbb{P}(t^{\prime})\Big{)} \Big{(}\sum_{a}\mathbb{P}(\mathbf{z}|\mathsf{a},t)\mathbb{P}(\mathsf{a}) \Big{)}.\] (69)

Lastly, using the law of total probability, (9) can be rewritten as

\[\mathbb{P}(\mathbf{\nu}|do(t=t))=\sum_{\mathbf{z}}\Big{(}\sum_{t^{\prime}} \mathbb{P}(\mathbf{\nu}|\mathbf{z},t^{\prime})\mathbb{P}(t^{\prime})\Big{)} \Big{(}\sum_{a}\mathbb{P}(\mathbf{z}|\mathsf{a},t)\mathbb{P}(\mathsf{a}|t) \Big{)}.\] (70)

Therefore, the variables \(\mathsf{a}\) and \(t\) could be such that (69) is different from (70). We note that similar steps can be used to show that (66) is not equivalent to (10). In conclusion, Assumption 2 is crucial for the formulae in (9) and (10) to hold.

## Appendix F Proof of Lemma 2

First, we state the following \(\mathsf{d}\)-separation criterion used to prove Lemma 2 and Lemma 2. See Appendix F.1 for a proof.

**Lemma 3**.: _Suppose Assumptions 1 to 3 hold. Then, \(\mathsf{t}\perp\!\!\!\perp_{d}\mathbf{b}|\mathbf{z}^{(i)}\) in \(\mathcal{G}_{\mathsf{f}}\)._

Now, we prove each part of Lemma 2 one-by-one.

Proof of Lemma 2(a)In \(\mathcal{G}_{\mathbf{b}}^{t}\), all edges going into \(\mathbf{b}\) are removed. Under Assumption 3, this implies that all edges going out of \(\mathsf{t}\) are removed. Now, consider any path \(\mathcal{P}(F_{t},\mathbf{\nu})\) between \(F_{t}\) and \(\mathbf{\nu}\) in \(\mathcal{G}_{\mathbf{b}}^{t}\). This path takes one of the following two forms: \((a)\)\(F_{t}\to\mathsf{t}\leftarrow\cdots\mathbf{\nu}\) or \((b)\)\(F_{t}\to\mathsf{t}\leftarrow\cdots\mathbf{\nu}\). In either case, there is a collider at \(\mathsf{t}\) in \(\mathcal{P}(F_{t},\mathbf{\nu})\). This collider is blocked when \(\mathbf{z}\) and \(\mathbf{b}\) are conditioned on because \(t\notin\mathbf{z}\), \(t\notin\mathbf{b}\), and \(\mathsf{t}\) does not have any descendants in \(\mathcal{G}_{\mathbf{b}}^{t}\). Therefore, \(\mathsf{y}\perp\!\!\!\perp_{d}F_{t}|\mathbf{z}\), \(\mathbf{b}\) in \(\mathcal{G}_{\mathbf{b}}^{t}\). Similarly, the collider is blocked when \(\mathbf{z}^{(i)}\) and \(\mathbf{b}\) are conditioned on because \(t\notin\mathbf{z}^{(i)}\), \(t\notin\mathbf{b}\), and \(\mathsf{t}\) does not have any descendants in \(\mathcal{G}_{\mathbf{b}}^{t}\). Therefore, \(\mathsf{y}\perp\!\!\!\perp_{d}F_{t}|\mathbf{z}^{(i)},\mathbf{b}\) in \(\mathcal{G}_{\mathbf{b}}^{t}\).

Proof of Lemma 2(b)We prove this by contradiction. Assume there exists at least one unblocked path between \(\mathsf{t}\) and some \(b\in\mathbf{b}\) in \(\mathcal{G}_{\mathsf{f}}\). Let \(\mathcal{P}(\mathsf{t},\mathsf{b})\) denote any such unblocked path.

Suppose we condition on \(\mathbf{z}^{(i)}\). From Lemma 3, \(\mathcal{P}(\mathsf{t},\mathsf{b})\) is blocked in \(\mathcal{G}_{\mathsf{f}}\) when \(\mathbf{z}^{(i)}\) is conditioned on. Let \(\mathsf{\nu}\) be any node at which \(\mathcal{P}(\mathsf{t},\mathsf{b})\) is blocked in \(\mathcal{G}_{\mathsf{f}}\) when \(\mathbf{z}^{(i)}\) is conditioned on. We must have that \(\mathsf{\nu}\in\mathcal{P}(\mathsf{t},\mathsf{b})\setminus\{\mathsf{t}, \mathsf{b}\}\) and \(\mathsf{\nu}\in\mathbf{z}^{(i)}\). Then, the path \(\mathcal{P}(\mathsf{t},\mathsf{\nu})\subset\mathcal{P}(\mathsf{t},\mathsf{b})\) is unblocked in \(\mathcal{G}_{\mathsf{f}}\) when \(\mathbf{z}^{(i)}\) is unconditioned on. However, this contradicts \(\mathsf{t}\perp\!\!\!\perp_{d}\mathbf{z}^{(i)}\) in \(\mathcal{G}_{\mathsf{f}}\) (which follows from (8)\((i)\) and Fact 1).

Proof of Lemma 2(c)We prove this by contradiction. Assume there exists at least one unblocked path between \(\mathsf{t}\) and some \(\mathsf{z}^{(i)}\in\mathbf{z}^{(i)}\) in \(\mathcal{G}_{\mathsf{f}}\) when \(\mathbf{b}\) is conditioned on. Let \(\mathcal{P}(\mathsf{t},\mathsf{z}^{(i)})\) denote any such unblocked path.

Suppose, we uncondition on \(\mathbf{b}\). From (8)(i) and Fact 1, we have \(\mathsf{t}\perp\!\!\!\perp_{d}\mathbf{z}^{(i)}\) in \(\mathcal{G}_{\mathsf{f}}\). Therefore, \(\mathcal{P}(\mathsf{t},\mathsf{z}^{(i)})\) is blocked in \(\mathcal{G}_{\mathsf{f}}\) when \(\mathbf{b}\) is unconditioned on. Now, we create a set \(\mathbf{v}\) consisting of all the nodes at which \(\mathcal{P}(\mathsf{t},\mathsf{z}^{(i)})\) is blocked in \(\mathcal{G}_{\mathsf{f}}\) when \(\mathbf{b}\) is unconditioned on. Define the set \(\mathbf{v}\) such that for any \(\mathsf{\nu}\in\mathbf{v}\), the following are true: \((a)\)\(\mathsf{\nu}\in\mathcal{P}(\mathsf{t},\mathsf{z}^{(i)})\setminus\{\mathsf{t}, \mathsf{z}^{(i)}\}\), \((b)\)\(\mathcal{P}(\mathsf{t},\mathsf{z}^{(i)})\) contains a collider at \(\mathsf{\nu}\) in \(\mathcal{G}_{\mathsf{f}}\), and \((c)\) there exists an unblocked descendant path from \(\mathsf{\nu}\) to some \(b\in\mathbf{b}\) in \(\mathcal{G}_{\mathsf{f}}\).

Now, we must have \(\mathbf{v}\neq\emptyset\), since \(\mathcal{P}(\mathsf{t},\mathsf{z}^{(i)})\) is blocked in \(\mathcal{G}_{\mathsf{f}}\) when \(\mathbf{b}\) is unconditioned on. Let \(\mathsf{\nu}_{c}\in\mathbf{v}\) be that node which is closest to \(\mathsf{t}\) in the path \(\mathcal{P}(\mathsf{t},\mathsf{z}^{(i)})\), and let \(\mathcal{P}(\mathsf{\nu}_{c},\mathsf{b})\) be an unblocked descendant path from \(\mathsf{\nu}\) to some \(b\in\mathbf{b}\) in \(\mathcal{G}_{\mathsf{f}}\) (there must be one from the definition of the set \(\mathbf{v}\)). Consider the path \(\mathcal{P}(\mathsf{t},\mathsf{b})\) obtained after concatenating \(\mathcal{P}(\mathsf{t},\mathsf{\nu}_{c})\subset\mathcal{P}(\mathsf{t},\mathsf{ z}^{(i)})\) and \(\mathcal{P}(\mathsf{\nu}_{c},\mathsf{b})\). By the definition of \(\mathbf{v}\) and the choice of \(\mathsf{\nu}_{c}\), \(\mathcal{P}(\mathsf{t},\mathsf{b})\) is unblocked in \(\mathcal{G}_{\mathsf{f}}\) since \((a)\)\(\mathcal{P}(\mathsf{t},\mathsf{\nu}_{c})\) is unblocked in \(\mathcal{G}_{\mathsf{f}}\), \((b)\)\(\mathcal{P}(\mathsf{\nu}_{c},\mathsf{b})\) is unblocked in \(\mathcal{G}_{\mathsf{f}}\), and \((c)\) there is no collider at \(\mathsf{\nu}_{c}\) in \(\mathcal{P}(\mathsf{t},\mathsf{b})\). However, this contradicts \(\mathsf{t}\perp\!\!\!\perp_{d}\mathbf{b}\) in \(\mathcal{G}_{\mathsf{f}}\) (which follows from Lemma 2).

Proof of Lemma 2(d)We prove this by contradiction. Assume there exists at least one unblocked path between \(t\) and \(F_{\mathbf{b}}\) in \(\mathcal{G}^{\mathbf{b}}\) when \(\mathbf{z}^{(i)}\) is conditioned on. Let \(\mathcal{P}(t,F_{\mathbf{b}})\) denote the shortest of these unblocked path. By definition of \(\mathcal{G}^{\mathbf{b}}\), this path has to be of the form: \(t\cdots,b\longleftarrow F_{\mathbf{b}}\) for some \(b\in\mathbf{b}\). Now, we have the following three cases:

1. \(\mathcal{P}(t,F_{\mathbf{b}})\) contains \(t\longrightarrow b\): In this case, because a path is a sequence of distinct nodes, \(\mathcal{P}(t,F_{\mathbf{b}})\) has to be \(t\longrightarrow b\longleftarrow F_{\mathbf{b}}\). By assumption, \(\mathcal{P}(t,F_{\mathbf{b}})\) is unblocked when \(\mathbf{z}^{(i)}\) is conditioned on. Since there is a collider at \(b\) in \(\mathcal{P}(t,F_{\mathbf{b}})\), there exists at least one unblocked descendant path from \(b\) to \(\mathbf{z}^{(i)}\) when \(\mathbf{z}^{(i)}\) is conditioned on. Let \(\mathcal{P}(b,\mathbf{z}^{(i)})\) denote the shortest of these paths from \(b\) to some \(\mathbf{z}^{(i)}\in\mathbf{z}^{(i)}\) in \(\mathcal{G}^{\mathbf{b}}\). We note that this path also exists in \(\mathcal{G}\) and is of the form \(b\longrightarrow\cdots\longrightarrow\mathbf{z}^{(i)}\) Suppose we uncondition on \(\mathbf{z}^{(i)}\). Consider the path \(\mathcal{P}(t,\mathbf{z}^{(i)})\supset\mathcal{P}(b,\mathbf{z}^{(i)})\) between \(t\) and \(\mathbf{z}^{(i)}\) of the form \(t\longrightarrow b\longrightarrow\cdots\longrightarrow\mathbf{z}^{(i)}\) in \(\mathcal{G}\). This path remains unblocked even when \(\mathbf{z}^{(i)}\) is unconditioned on as it does not have any colliders. This contradicts \(\mathbf{z}^{(i)}\perp_{d}t\) (which follows from (8)).
2. \(\mathcal{P}(t,F_{\mathbf{b}})\) contains \(t\longrightarrow b_{1}\) for some \(b_{1}\in\mathbf{b}\) such that \(b_{1}\neq b\): In this case, the path \(\mathcal{P}(t,F_{\mathbf{b}})\) has to be of the form \(t\longrightarrow b_{1}\cdots b\longleftarrow F_{\mathbf{b}}\). Therefore, there exists at least one collider on the path \(\mathcal{P}(t,F_{\mathbf{b}})\). Let \(\nu\in\mathcal{P}(t,F_{\mathbf{b}})\setminus\{t,F_{\mathbf{b}}\}\) be the collider on the path \(\mathcal{P}(t,F_{\mathbf{b}})\) that is closest to \(b_{1}\). Consider the path \(\mathcal{P}(t,\nu)\subset\mathcal{P}(t,F_{\mathbf{b}})\). We note that this path also exists in \(\mathcal{G}\) and is of the form \(t\longrightarrow b_{1}\longrightarrow\cdots\longrightarrow\nu\). By assumption, \(\mathcal{P}(t,F_{\mathbf{b}})\) is unblocked when \(\mathbf{z}^{(i)}\) is conditioned on. Since there is a collider at \(\nu\) in \(\mathcal{P}(t,F_{\mathbf{b}})\), there exists at least one unblocked descendant path from \(\nu\) to \(\mathbf{z}^{(i)}\) when \(\mathbf{z}^{(i)}\) is conditioned on. Let \(\mathcal{P}(\nu,\mathbf{z}^{(i)})\) denote the shortest of these paths from \(\nu\) to some \(\mathbf{z}^{(i)}\in\mathbf{z}^{(i)}\) in \(\mathcal{G}^{\mathbf{b}}\). We note that this path also exists in \(\mathcal{G}\) and is of the form \(\nu\longrightarrow\cdots\longrightarrow\mathbf{z}^{(i)}\). Suppose we uncondition on \(\mathbf{z}^{(i)}\). Consider the path \(\mathcal{P}(t,\mathbf{z}^{(i)})\) between \(t\) and \(\mathbf{z}^{(i)}\) in \(\mathcal{G}\) obtained after concatenating \(\mathcal{P}(t,\nu)\subset\mathcal{P}(t,F_{\mathbf{b}})\) and \(\mathcal{P}(\nu,\mathbf{z}^{(i)})\). This path, of the form \(t\longrightarrow b_{1}\longrightarrow\cdots\longrightarrow\nu\longrightarrow \cdots\longrightarrow\mathbf{z}^{(i)}\), remains unblocked even when \(\mathbf{z}^{(i)}\) is unconditioned on as it does not have any colliders. This contradicts \(\mathbf{z}^{(i)}\perp_{d}t\) (which follows from (8)).
3. \(\mathcal{P}(t,F_{\mathbf{b}})\) does not contain \(t\longrightarrow b_{1}\) for every \(b_{1}\in\mathbf{b}\): By assumption, \(\mathcal{P}(t,F_{\mathbf{b}})\) is unblocked in \(\mathcal{G}^{\mathbf{b}}\) when \(\mathbf{z}^{(i)}\) is conditioned on. Therefore, if \(\mathcal{P}(t,F_{\mathbf{b}})\) does not contain the edge \(t\longrightarrow b_{1}\) for any \(b_{1}\in\mathbf{b}\), there exists a path \(\mathcal{P}(t,b)\) between \(t\) to \(b\) in \(\mathcal{G}\) that is unblocked when \(\mathbf{z}^{(i)}\) is conditioned on, and takes one of the following two forms: \((a)\ t\longleftarrow\cdots b\) or \((b)\)\(t\ \longleftarrow\ \cdots b\). Then, it is easy to see that the path \(\mathcal{P}(t,b)\) also remains unblocked in \(\mathcal{G}_{t}\) while \(\mathbf{z}^{(i)}\) is conditioned on. However, this contradicts \(t\perp_{d}\mathbf{b}|\mathbf{z}^{(i)}\) in \(\mathcal{G}_{t}\) (which follows from Lemma 3).

Proof of Lemma 2(e)We prove this by contradiction. Assume there exists at least one unblocked path between \(\nu\) and some \(b\in\mathbf{b}\) in \(\mathcal{G}_{\mathbf{b}}\) when \(t\) and \(\mathbf{z}^{(i)}\) are conditioned on. Let \(\mathcal{P}(b,\nu)\) denote the shortest of these unblocked path. Therefore, no \(b_{1}\in\mathbf{b}\), such that \(b_{1}\neq b\), is on the path \(\mathcal{P}(b,y)\), i.e., \(b_{1}\notin\mathcal{P}(b,y)\). Further, \(\mathcal{P}(b,\nu)\) takes one of the following two forms because all the edges going out of \(\mathbf{b}\) are removed in \(\mathcal{G}_{\mathbf{b}}\): \((a)\ b\longleftarrow\cdots\nu\) or \((b)\ b\ \longleftarrow\ \cdots\nu\).

Suppose we condition on \(\mathbf{z}^{(o)}\) (while \(t\) and \(\mathbf{z}^{(i)}\) are still conditioned on). From (7) and Fact 1, we have \(\nu\perp_{d}b|t\), \(\mathbf{z}\) in \(\mathcal{G}_{\mathbf{b}}\). Therefore, the path \(\mathcal{P}(b,\nu)\) is blocked in \(\mathcal{G}_{\mathbf{b}}\) when \(\mathbf{z}^{(o)}\) is conditioned on (while \(t\) and \(\mathbf{z}^{(i)}\) are still conditioned on). Let \(\nu\) be any node at which \(\mathcal{P}(b,\nu)\) is blocked in \(\mathcal{G}_{\mathbf{b}}\) when \(\mathbf{z}^{(o)}\) is conditioned on (while \(t\) and \(\mathbf{z}^{(i)}\) are still conditioned on). We must have that \(\nu\in\mathcal{P}(b,\nu)\setminus\{\nu,b\}\) and \(\nu\in\mathbf{z}^{(o)}\). Suppose we uncondition on \(\mathbf{z}^{(o)}\) (while \(t\) and \(\mathbf{z}^{(i)}\) are still conditioned on). Then, the path \(\mathcal{P}(b,\nu)\subset\mathcal{P}(b,\nu)\) is unblocked in \(\mathcal{G}_{\mathbf{b}}\).

We consider the following two scenarios depending on whether or not \(\mathcal{P}(b,\nu)\) contains \(t\). In both scenarios, we show that there is an unblocked path between \(t\) and \(\nu\) in \(\mathcal{G}_{\mathbf{b}}\) when we condition on \(\mathbf{b}\) (while \(t\) and \(\mathbf{z}^{(i)}\) are still conditioned on).

1. \(\mathcal{P}(b,\nu)\) contains \(t\): Consider the path \(\mathcal{P}(t,\nu)\subset\mathcal{P}(b,\nu)\) which is unblocked in \(\mathcal{G}_{\mathbf{b}}\) when \(t\) and \(\mathbf{z}^{(i)}\) are conditioned on. Further, by the choice of \(\mathcal{P}(b,\nu)\), no \(b_{1}\in\mathbf{b}\) is on the path \(\mathcal{P}(t,\nu)\). Therefore, the path \(\mathcal{P}(t,\nu)\) in \(\mathcal{G}_{\mathbb{b}}\) remains unblocked when we condition on \(\mathbb{b}\) (while \(t\) and \(\mathbf{z}^{(i)}\) are still conditioned on).
2. \(\mathcal{P}(b,\nu)\) does not contain \(t\): Consider the path \(\mathcal{P}(t,\nu)\supset\mathcal{P}(b,\nu)\) (by including the extra edge \(t\to b\)) which takes one of the following two forms: \((a)\ t\to b\longleftarrow\cdots\nu\) or \((b)\ t\to b\ \longleftarrow\ \cdots\nu\). Further, by the choice of \(\mathcal{P}(b,\nu)\), no \(b_{1}\in\mathbb{b}\) (\(b_{1}\neq b\)) is on the path \(\mathcal{P}(t,\nu)\). Suppose we condition on \(\mathbb{b}\) (while \(t\) and \(\mathbf{z}^{(i)}\) are still conditioned on). Then, the path \(\mathcal{P}(t,\nu)\) in \(\mathcal{G}_{\mathbb{b}}\) is unblocked because \((a)\) the collider at \(b\) is unblocked when \(\mathbb{b}\) is conditioned on and \((b)\) the path \(\mathcal{P}(b,\nu)\) in \(\mathcal{G}_{\mathbb{b}}\) remains unblocked when \(\mathbb{b}\) is conditioned on (while \(t\) and \(\mathbf{z}^{(i)}\) are still conditioned on).

Now, suppose we uncondition on \(t\) (while \(\mathbb{b}\) and \(\mathbf{z}^{(i)}\) are still conditioned on). We have the following two scenarios depending on whether or not \(\mathcal{P}(t,\nu)\) in \(\mathcal{G}_{\mathbb{b}}\) remains unblocked. In both scenarios, we show that there is an unblocked path between \(t\) and \(\nu\) in \(\mathcal{G}_{\mathbb{b}}\) when we uncondition on \(t\) (while \(\mathbb{b}\) and \(\mathbf{z}^{(i)}\) are still conditioned on).

1. If \(\mathcal{P}(t,\nu)\) remains unblocked: In this case, \(\mathcal{P}(t,\nu)\) in \(\mathcal{G}_{\mathbb{b}}\) is an unblocked path between \(t\) and \(\nu\) when \(\mathbf{z}^{(i)}\) and \(\mathbb{b}\) are conditioned on, as desired.
2. If \(\mathcal{P}(t,\nu)\) does not remain unblocked: In this case, it is the unconditioning on \(t\) (while \(\mathbb{b}\) and \(\mathbf{z}^{(i)}\) are still conditioned on) that blocks \(\mathcal{P}(t,\nu)\). Now, we create a set \(\mathbb{w}\) consisting of all the nodes at which \(\mathcal{P}(t,\nu)\) is blocked in \(\mathcal{G}_{\mathbb{b}}\) when \(t\) is unconditioned on (while \(\mathbb{b}\) and \(\mathbf{z}^{(i)}\) are still conditioned on). Define the set \(\mathbb{w}\) such that for any \(w\in\mathbb{w}\), the following are true: \((a)\ w\in\mathcal{P}(t,\nu)\setminus\{t,\nu\},(b)\ \mathcal{P}(t,\nu)\) contains a collider at \(w\) in \(\mathcal{G}_{\mathbb{b}}\), and \((c)\) there exists an unblocked descendant path from \(w\) to \(t\) in \(\mathcal{G}_{\mathbb{b}}\). Now, we must have \(\mathbb{w}\neq\emptyset\), since \(\mathcal{P}(t,\nu)\) is blocked in \(\mathcal{G}_{\mathbb{b}}\) when \(t\) is unconditioned on (while \(\mathbb{b}\) and \(\mathbf{z}^{(i)}\) are still conditioned on). Let \(w_{c}\in\mathbb{w}\) be that node which is closest to \(\nu\) in the path \(\mathcal{P}(t,\nu)\), and let \(\mathcal{P}(w_{c},t)\) be an unblocked descendant path from \(w_{c}\) to \(t\) in \(\mathcal{G}_{\mathbb{b}}\) (there must be one from the definition of the set \(\mathbb{w}\)). Consider the path \(\mathcal{P}^{\prime}(\nu,t)\) obtained after concatenating \(\mathcal{P}(\nu,w_{c})\subset\mathcal{P}(t,\nu)\) and \(\mathcal{P}(w_{c},t)\). By the definition of \(\mathbb{w}\) and the choice of \(w_{c}\), \(\mathcal{P}^{\prime}(\nu,t)\) is unblocked in \(\mathcal{G}_{\mathbb{b}}\) when \(t\) is unconditioned on (while \(\mathbb{b}\) and \(\mathbf{z}^{(i)}\) are still conditioned on) since \((a)\ \mathcal{P}(\nu,w_{c})\) is unblocked, \((b)\ \mathcal{P}(w_{c},t)\) is unblocked, and \((c)\) there is no collider at \(w_{c}\) in \(\mathcal{P}^{\prime}(\nu,t)\). Therefore, we have an unblocked path between \(t\) and \(\nu\) in \(\mathcal{G}_{\mathbb{b}}\) when \(\mathbf{z}^{(i)}\) and \(\mathbb{b}\) are conditioned on, as desired.

To conclude the proof, we note that the existence of an unblocked path between \(t\) and \(\nu\in\mathbf{z}^{(o)}\) in \(\mathcal{G}_{\mathbb{b}}\) when \(\mathbf{z}^{(i)}\) and \(\mathbb{b}\) are conditioned on contradicts \(\mathbf{z}^{(o)}\perp\!\!\!\perp_{d}t|\mathbf{b},\mathbf{z}^{(i)}\) in \(\mathcal{G}_{\mathbb{b}}\) (which follows from (8) and Fact 1).

### Proof of Lemma 3

First, we claim \(t\perp\!\!\!\perp_{d}\mathbf{b}|\mathbf{z}\) in \(\mathcal{G}_{\mathbb{f}}\). We assume this claim and proceed to prove the statement in the Lemma by contradiction. Assume there exists at least one unblocked path between \(t\) and some \(b\in\mathbb{b}\) in \(\mathcal{G}_{\mathbb{t}}\) when \(\mathbf{z}^{(i)}\) is conditioned on. Let \(\mathcal{P}(t,b)\) denote the shortest of these unblocked path. Therefore, no \(b_{1}\in\mathbb{b}\) such that \(b_{1}\neq b\) is not on the path \(\mathcal{P}(t,b)\), i.e., \(b_{1}\notin\mathcal{P}(t,b)\).

Suppose we condition on \(\mathbf{z}^{(o)}\) (while \(\mathbf{z}^{(i)}\) is still conditioned on). From the claim, \(\mathcal{P}(t,b)\) is blocked in \(\mathcal{G}_{\mathbb{t}}\) when \(\mathbf{z}^{(o)}\) is conditioned on (while \(\mathbf{z}^{(i)}\) is still conditioned on). Let \(\nu\) be any node at which \(\mathcal{P}(t,b)\) is blocked in \(\mathcal{G}_{\mathbb{f}}\) when \(\mathbf{z}^{(o)}\) is conditioned on (while \(\mathbf{z}^{(i)}\) is still conditioned on). We must have that \(\nu\in\mathcal{P}(t,b)\setminus\{t,b\}\) and \(\nu\in\mathbf{z}^{(o)}\). Then, the path \(\mathcal{P}(t,\nu)\subset\mathcal{P}(t,b)\) is unblocked in \(\mathcal{G}_{\mathbb{f}}\) when \(\mathbf{z}^{(o)}\) is unconditioned on (while \(\mathbf{z}^{(i)}\) is still conditioned on). Further, no \(b\in\mathbb{b}\) is on the path \(\mathcal{P}(t,\nu)\). As a result, the path \(\mathcal{P}(t,\nu)\) remains unblocked when \(\mathbb{b}\) is conditioned on (while \(\mathbf{z}^{(i)}\) is still conditioned on). However, this contradicts \(t\perp\!\!\!\perp_{d}\mathbf{z}^{(o)}|b,\mathbf{z}^{(i)}\) in \(\mathcal{G}_{\mathbb{f}}\) (which follows from (8)\((ii)\) and Fact 1).

_Proof of Claim - \(t\perp\!\!\!\perp_{d}\mathbf{b}|\mathbf{z}\) in \(\mathcal{G}_{\mathbb{f}}\):_ It remains to prove the claim \(t\perp\!\!\!\perp_{d}\mathbf{b}|\mathbf{z}\) in \(\mathcal{G}_{\mathbb{f}}\). We prove this by contradiction. Assume there exists at least one unblocked path between \(t\) and some \(b\in\mathbb{b}\) in \(\mathcal{G}_{\mathbb{f}}\) when \(\mathbf{z}\) is conditioned on. Let \(\mathcal{P}(t,b)\) denote any such unblocked path. This path takes one of the following two forms: \((a)\ t\longleftarrow\cdots b\) or \((b)\ t\ \longleftarrow\ \cdots b\) because all edges going out of \(t\) are removed in \(\mathcal{G}_{\mathbb{f}}\).

[MISSING_PAGE_EMPTY:23]

In Figure 11, we provide the 6 random SMCMs used in Section 4.2. As mentioned in Section 4.1, we choose the last variable in the causal ordering as \(\gamma\) and a variable that is ancestor of \(\gamma\) but not its parent or grandparent as \(t\). We also show the corresponding \(\mathbf{z}=(\mathbf{z}^{(i)},\mathbf{z}^{(o)})\) satisfying (7) and (8).

### German Credit dataset

As in Section 4.3, we assess the conditional independence associated with the selected \(\mathbf{z}\) for the choice of \(\mathbf{b}\!=\!\{\#\) of people financially dependent on the applicant, applicant's savings\(\}\), Algorithm 1 results in \(\mathbf{z}^{(i)}\!=\!\{\text{purpose for which the credit was needed, applicant's checking account status with the bank\(\}\) via 100 random bootstraps. We show the corresponding p-values for these bootstraps in a histogram in Figure 10 below. As expected, we observe the p-values to be spread out.

Figure 8: The causal graphs used to further validate our theoretical results. These are obtained by adding additional edges (shown in red) to \(\mathcal{G}^{toy}\) in Figure 2. We denote these graphs (from left to right) by \(\mathcal{G}^{toy}_{1}\), \(\mathcal{G}^{toy}_{2}\), and \(\mathcal{G}^{toy}_{3}\), respectively.

Figure 9: Performance of Algorithm 1 for different p-value thresholds \(p_{v}\) on \(\mathcal{G}^{toy}\) in Figure 2 on top left, on \(\mathcal{G}^{toy}_{1}\) from Figure 8 on top right, on \(\mathcal{G}^{toy}_{2}\) in Figure 8 on bottom left, and on \(\mathcal{G}^{toy}_{3}\) from Figure 8 on bottom right

### Adult dataset

The Adult dataset [Kohavi and Becker, 1996] is used for income analysis where the goal is to predict whether an individual's income is more than $50,000 using 14 demographic and socio-economic features. The sensitive attribute \(t\) is the individual's sex, either male or female. Further, the categorical attributes are one-hot encoded. As with German Credit dataset, we apply Algorithm 1 with \(n_{r}=100\) and \(p_{v}=0.1\) where we search for a set \(\mathbf{z}=(\mathbf{z}^{(o)},\mathbf{z}^{(i)})\) of size at most \(3\) under the following two assumptions on the set of all children \(\mathbf{b}\) of \(t\): (1) \(\mathbf{b}=\{\#\text{ individual's relationship status (which includes wife/husband)}\}\) and (2) \(\mathbf{b}=\{\#\text{ individual's relationship status (which includes wife/husband), individual's occupation}\}\). In either case, Algorithm 1 was unable to find a suitable \(\mathbf{z}\) satisfying \(\mathbf{b}\perp_{p}y|\mathbf{z},t\). This suggests that in this dataset, there may not be any non-child descendants of the sensitive attribute, which is required for our criterion to hold.

### Licenses

In this work, we used a workstation with an AMD Ryzen Threadripper 3990X 64-Core Processor (128 threads in total) with 256 GB RAM and 2x Nvidia RTX 3090 GPUs. However, our simulations only used the CPU resources of the workstation.

We mainly relied on the following Python repositories -- (a) networkx (https://networkx.org), (b) causal-learn (https://causal-learn.readthedocs.io/en/latest/), (c) RCoT [Strobl et al., 2019b] and (d) ridgeCV, (https://github.com/scikit-learn/scikit-learn/tree/15a949460/sklearn/linear_model/_ridge.py). We did not modify any of the code under licenses; we only installed these repositories as packages.

In addition to these, we used two public datasets (a) German Credit dataset (https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)) and (b) Adult dataset (https://archive.ics.uci.edu/ml/datasets/adult). These datasets are commonly used benchmark datasets for causal fairness, which is why we chose them for our comparisons.

Figure 10: Histograms of p-values of the conditional independencies in (9) and (10) over 100 bootstrap runs for \(\mathbf{b}=\{\#\text{ of people financially dependent on the applicant, applicant’s savings}\}\), Algorithm 1 results in \(\mathbf{z}^{(i)}=\{\text{purpose for which the credit was needed, applicant’s checking account status with the bank}\}\).

Figure 11: The SMCMs used in Section 4.2 to compare Algorithm 1 with the Baseline that uses \(\mathbf{b}\) for front-door adjustment. These are the 6 out of the 100 random graphs in Section 4.1 for \(p=10\), \(d=2\), and \(q=1.0\) where our approach was successful indicating existence of \(\mathbf{z}=(\mathbf{z}^{(i)},\mathbf{z}^{(o)})\) such that the conditional independence statements in Theorem 3.2 hold.