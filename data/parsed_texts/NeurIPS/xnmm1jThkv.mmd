# Hybrid Top-Down Global Causal Discovery

with Local Search

for Linear and Nonlinear Additive Noise Models

 Sujai Hiremath

Cornell Tech

sh2583@cornell.edu

&Jacqueline Maasch

Cornell Tech

jam887@cornell.edu

&Mengxiao Gao

Tsinghua University

gaomx21@mails.tsinghua.edu.cn

&Promit Ghosal

University of Chicago

promit@uchicago.edu

&Kyra Gan

Cornell Tech

kyragan@cornell.edu

###### Abstract

Learning the unique directed acyclic graph corresponding to an unknown causal model is a challenging task. Methods based on functional causal models can identify a unique graph, but either suffer from the curse of dimensionality or impose strong parametric assumptions. To address these challenges, we propose a novel hybrid approach for global causal discovery in observational data that leverages local causal substructures. We first present a topological sorting algorithm that leverages ancestral relationships in linear structural causal models to establish a compact top-down hierarchical ordering, encoding more causal information than linear orderings produced by existing methods. We demonstrate that this approach generalizes to nonlinear settings with arbitrary noise. We then introduce a nonparametric constraint-based algorithm that prunes spurious edges by searching for local conditioning sets, achieving greater accuracy than current methods. We provide theoretical guarantees for correctness and worst-case polynomial time complexities, with empirical validation on synthetic data.

## 1 Introduction

Causal graphical models compactly represent the _data generating processes_ (DGP) of complex systems, including physical, biological, and social domains. Access to the true causal graph or its substructures can offer mechanistic insights [31; 13] and enable downstream causal inference, including effect estimation [11; 34; 2; 8; 16; 35]. In practice, the true causal graph is often unknown, and can be challenging to assume using domain knowledge. In such limited-knowledge settings, we can instead rely on causal discovery algorithms that learn the causal graph from observational data in a principled, automated manner [41; 7].

Traditional approaches to causal discovery infer causal relationships either through conditional independence relations (PC [40]) or goodness-of-fit measures (GES [3], GRaSP [12]). While these discovery methods can provide flexibility by not requiring assumptions over the functional form of the DGP, they are generally worst-case exponential in time complexity and learn Markov equivalence classes (MEC) rather than unique _directed acyclic graphs_ (DAGs) [19]. Therefore, additional modeling assumptions are often necessary for time-efficient and accurate global discovery.

Certain parametric assumptions can enable recovery of the unique ground truth DAG, e.g., assuming a particular _functional causal model_ (FCM) [48]. Under the _additive noise model_ (ANM), we obtain unique identifiability by assuming linear causal mechanisms with non-Gaussian noise distributions[37; 36] or nonlinear causal functions with arbitrary noise [10]. Under the independent additive noise assumption, the causal parents of a variable are statistically independent of its noise term. For this class of models, discovery often entails regressing the variable of interest against its hypothesized parent set and testing for marginal independence between this set and the residual term [25].

Current FCM approaches to global causal discovery trade off between two main issues, suffering from either 1) strong parametric assumptions over the noise or functional form (or both) or 2) the use of high-dimensional nonparametric regressions, which require large sample sizes for reliable estimation and do not scale to large graphs. In addition, current FCM-based methods are ill-suited for causal discovery in sparse causal graphs, a setting that characterizes many high-dimensional applications (e.g., analysis of genetic data in healthcare applications) [6; 47; 46; 15].

**Contributions** We propose a hybrid causal discovery approach to graph learning that combines functional causal modeling with constraint-based discovery. We depart from previous methods by characterizing conditions that allow us to search for and exploit local, rather than global, causal relationships between vertices. These local relationships stem from root vertices: this motivates a top-down, rather than bottom-up, approach. Thus, we learn the topological sort and discover true edges starting from the roots rather than the leaves, as in existing methods [25; 30; 21]. This approach leverages sparsity in both the ordering phase and edge discovery phase to reduce the size of conditioning sets, as well as the number of high-dimensional regressions. We summarize our major contributions as follows:

* We introduce a topological ordering algorithm LHTS for linear non-Gaussian ANMs that exploits local ancestor-descendent relationships to obtain a compact hierarchical sort.
* We introduce a topological ordering algorithm NHTS for nonlinear ANMs that exploits local parent-child relationships to run fewer high-dimensional regressions than traditional methods, achieving lower sample complexity.
* We introduce a constraint-based algorithm ED that nonparametrically prunes spurious edges from a discovered topological ordering, leveraging local properties of causal edges to use smaller conditioning sets than traditional sparse regression techniques.
* We achieve accurate causal discovery in synthetic data, outperforming baseline methods.

**Organization** After describing the preliminaries in Section 2, we introduce the linear problem setting in Section 3, establishing the connection between ancestral relationships and causal active paths and introducing a _linear hierarchical topological sorting_ algorithm (LHTS). Next, we extend our method to the nonlinear setting in Section 4 by establishing the connection between parental relationships and active causal paths, introducing a _nonlinear hierarchical topological sorting_ algorithm (NHTS). In section 5, we establish a sufficient conditioning set for determining edge relations and introduce an efficient _edge discovery_ algorithm (ED). We then test LHTS, NHTS and ED in synthetic experiments in Section 6. To conclude, we discuss future work that might generalize our approach to full ANMs.

**Related Work** Our work is related to two kinds of discovery methods that explicitly leverage the topological structure of DAGs: 1) permutation-based approaches, and 2) FCM-based approaches.

The original permutation-based approach SP [27] searches over the space of variable orderings to find permutations that induce DAGs with minimal edge counts. Authors in [39] introduce greedy variants of SP (such as GSP) that maintain asymptotic consistency; GRaSP [12] relaxes the assumptions of prior methods to obtain improvements in accuracy. These methods highlight the importance of using permutations for efficient causal discovery, but generally suffer from the need to bound search runtime with heuristics, poor sample efficiency in high dimensional settings, and are unable to recover a unique topological ordering or DAG ([22]).

On the other hand, the recent stream of FCM-based approaches decompose graph learning into two phases: 1) learning the topological sort, i.e., inferring a causal ordering of the variables; and 2) edge discovery, i.e., identifying edges consistent with the causal ordering [38; 25; 1; 30; 21; 32; 20].

The literature on topological ordering algorithms for ANMs is organized along the types of parametric assumptions made on both the functional forms and noise distributions of the underlying DGP. Early approaches like ICA-LiNGAM [37] and DirectLiNGAM [38] focus on learning DAGs generated by linear functions and non-Gaussian noise terms. Recent work leverages score matching to obtain the causal ordering in settings with nonlinear functions and Gaussian noise: SCORE [30] and DAS [21] exploit particular variance properties, while DiffAN estimates the score function with a diffusion model [32]. NoGAM [20] generalizes the score-matching procedure of SCORE to nonlinear causal mechanisms with arbitrary noise distributions. RESIT [25] leverages residual independence results in nonlinear ANMs to identify topological orderings when the noise distribution is arbitrary. NoGAM and RESIT both rely on high-dimensional nonparametric regression.

Once a topological ordering is obtained, spurious edges are pruned. Works that are agnostic to the distribution of noise often use a parametric approach, implementing either a form of sparse regression (e.g., Lasso regression [18]) or a version of additive hypothesis testing with generalized additive models (GAMs) [17] (e.g., CAM-pruning [1]). RESIT [25] provides another alternative edge pruning procedure for nonlinear ANMs, relying again on high-dimensional nonparametric regression.

## 2 Preliminaries

We focus on _structural causal models_ (SCMs) represented as DAGs. These graphs describe the causal relationships between variables, where an edge \(x_{i}\to x_{j}\) implies that \(x_{i}\) has a direct causal influence on \(x_{j}\). Let \(G=(V,E)\) be a DAG on \(|V|=d\) vertices, where \(E\) represents directed edges. To define pairwise relationships between vertices, we let \(\text{Ch}(x_{i})\) denote the children of \(x_{i}\) such that \(x_{j}\in\text{Ch}(x_{i})\) if and only if \(x_{i}\to x_{j}\), and \(\text{Pa}(x_{i})\) denote the parents of \(x_{i}\) such that \(x_{j}\in\text{Pa}(x_{i})\) if and only if \(x_{j}\to x_{i}\). Similarly, let \(\text{An}(x_{i})\) denote the ancestors of \(x_{i}\) such that \(x_{j}\in\text{An}(x_{i})\) if and only if there exists a directed path \(x_{j}\dashrightarrow x_{i}\), and \(\text{De}(x_{i})\) denote the descendants of \(x_{i}\) such that \(x_{j}\in\text{De}(x_{i})\) if and only if there exists a directed path \(x_{i}\dashrightarrow x_{j}\). Vertices can be classified based on the totality of their pairwise relationships: \(x_{i}\) is a _root_ if and only if \(\text{Pa}(x_{i})=\emptyset\), a _leaf_ if and only if \(\text{Ch}(x_{i})=\emptyset\), an isolated vertex if \(x_{i}\) is both a root and a leaf, and an intermediate vertex otherwise. See an illustrative DAG in Figure 1. Vertices can also be classified in terms of triadic relationships: \(x_{i}\) is a confounder of \(x_{j},x_{k}\) if and only if \(x_{i}\in\text{An}(x_{j})\cap\text{An}(x_{k})\); a mediator of \(x_{j}\) to \(x_{k}\) if and only if \(x_{i}\in\text{De}(x_{j})\cap\text{An}(x_{k})\); and a collider between \(x_{j}\) and \(x_{k}\) if and only if \(x_{i}\in\text{De}(x_{j})\cap\text{De}(x_{k})\).

Undirected paths that transmit causal information between vertices \(x_{j},x_{k}\) can be differentiated into _frontdoor_ and _backdoor paths_[42]. A frontdoor path is a directed path \(x_{j}\dashrightarrow\dots\dashrightarrow x_{k}\) that starts with an edge out of \(x_{j}\), and ends with an edge into \(x_{k}\). A backdoor path is a path \(x_{j}\dashrightarrow\dots\dashrightarrow x_{k}\) that starts with an edge into \(x_{j}\), and ends with an edge into \(x_{k}\). Paths that start and end with an edge out of \(x_{j}\) and \(x_{k}\) (\(x_{k}\dashrightarrow\dots\dashrightarrow x_{k}\)) do not transmit causal information between \(x_{j},x_{k}\).

Paths between two vertices are further classified, relative to a vertex set \(\mathbf{Z}\), as either _active_ or _inactive_[42]. A path between vertices \(x_{j},x_{k}\) is active relative to \(\mathbf{Z}\) if every node on the path is active relative to \(\mathbf{Z}\). Vertex \(x_{i}\) on a path is active if one of the following holds: 1) \(x_{i}\) is not a collider and \(x_{i}\not\in\mathbf{Z}\), 2) \(x_{i}\) is a collider and \(x_{i}\in\mathbf{Z}\), 3) \(x_{i}\) is a collider and \(x_{i}\not\in\mathbf{Z}\), but \(\text{De}(x_{i})\cap\mathbf{Z}\neq\emptyset\). An inactive path is simply a path that is not active. Following convention, throughout the rest of the paper we will describe causal paths as active or inactive with respect to \(\mathbf{Z}=\emptyset\) unless otherwise specified.

**Definition 2.1** (Topological Orderings).: _Consider a given DAG \(G=(V,E)\). A topological sort (linear order) is a mapping \(\pi:V\rightarrow\{0,1,\dots,|V|-1\}\), such that if \(x_{i}\in\text{Pa}(x_{j})\), then \(x_{i}\) appears before \(x_{j}\) in the sort \(\pi\): \(\pi(x_{i})<\pi(x_{j})\). A hierarchical sort (between a partial and linear order) is a mapping \(\pi_{L}:V\rightarrow\{0,1,\dots,|V|-1\}\), such that if \(\text{Pa}(x_{i})=\emptyset\), then \(\pi_{L}(x_{i})=0\), and if \(\text{Pa}(x_{i})\neq\emptyset\), then \(\pi_{L}(x_{i})\) equals the maximum length of the longest directed path from each root vertex to \(x_{i}\), i.e., \(\pi_{L}(x_{i})=1+\max\left\{\pi_{L}(x_{j}):x_{j}\in\text{Pa}(x_{i})\right\}\)._

We note that the hierarchical sort is unique, and that it coincides with a topological sort when the number of layers equals \(|V|\), i.e., the DAG is complete.

**Definition 2.2** (AnMs).: _ANMs [9] are a popular general class of SCMs defined over a DAG \(G\) with_

\[x_{i}=f_{i}(\text{Pa}(x_{i}))+\varepsilon_{i},\forall x_{i}\in V,\] (1)

_where \(f_{i}\)s are arbitrary functions and \(\varepsilon_{i}\)s are independent arbitrary noise distributions._

This model implicitly assumes the causal Markov condition and acyclicity; we adopt the aforementioned assumptions, as well as faithfulness [41].

## 3 Linear Setting

We first restrict our attention to ANMs that feature only linear causal functions \(f\), known as Linear Non-Gaussian Acyclic causal Models (LiNGAMs). Following [38], we note that a LiNGAM can be represented as a \(d\times d\) adjacency matrix \(B=\{b_{ij}\}\), where \(b_{ij}\) is the coefficient from \(x_{j}\) to \(x_{i}\). Note that, for any topological ordering \(\pi\) of a LiNGAM, if \(\pi(x_{j})>\pi(x_{i})\), then \(b_{ji}=0\). Thus, each \(x_{i}\in V\) admits the following compact representation: \(x_{i}=\sum_{\pi(x_{j})<\pi(x_{i})}b_{ij}x_{j}+\varepsilon_{i}\).

**Identifiability** Identifiability conditions for LiNGAMs [37] primarily concern the distribution of errors \(\varepsilon_{i}\): under Gaussianity, distinct linear DGPs can admit the same joint distribution, making them impossible to distinguish. Shimizu et al. [37] generalize this intuition with _independent component analysis_ (ICA) [4] to provide a multivariate identifiability condition for LiNGAMs (see Appendix A.1). In this section, we adopt the aforementioned condition.

**Ancestral Relations and Active Causal Paths** We first establish the connection between ancestral relationships and active causal paths. We exhaustively enumerate and define the potential pairwise causal ancestral path relations in Figure 2 and Lemma 3.1 (proof in Appendix A.2):

**Lemma 3.1** (Active Causal Ancestral Path Relation Enumeration).: _Each pair of distinct nodes \(x_{i},x_{j}\in V\) can be in one of four possible active causal ancestral path relations: AP1) no active path exists between \(x_{i},x_{j}\); AP2) there exists an active backdoor path between \(x_{i},x_{j}\), but there is no active frontdoor path between them; AP3) there exists an active frontdoor path between \(x_{i},x_{j}\), but there is no active backdoor path between them; AP4) there exists an active backdoor path between \(x_{i},x_{j}\), and there exists an active frontdoor path between them._

Next, in Lemma 3.2, we summarize the connection between causal paths and ancestral relationships (proof in Appendix A.3):

**Lemma 3.2**.: _The ancestral relationship between a pair of distinct nodes \(x_{i},x_{j}\in V\) can be expressed using active causal path relations: \(x_{i},x_{j}\) are not ancestrally related if and only if they are in AP1 or AP2 relation; and \(x_{i},x_{j}\) are ancestrally related if and only if they are in AP3 or in AP4 relation._

The active causal ancestral path relation of a pair of nodes \(x_{i},x_{j}\) that are not ancestrally related can be determined through marginal independence testing and sequential univariate regressions as illustrated in Lemmas 3.3 and 3.4 (proofs in Appendices A.4, A.5):

**Lemma 3.3** (Ap1).: _Vertices \(x_{i},x_{j}\) are in AP1 relation if and only if \(x_{i}\perp\!\!\!\perp x_{j}\)._

**Lemma 3.4** (Ap2).: _Let \(M\) be the set of mutual ancestors between a pair of vertices \(x_{i}\) and \(x_{j}\), i.e., \(M=\text{An}(x_{i})\cap\text{An}(x_{j})\). Let \(x_{i}^{M},x_{j}^{M}\) be the result of sequentially regressing all mutual ancestors in \(M\) out of \(x_{i},x_{j}\) with univariate regressions, in any order. Then, let \(r_{i}^{j}\) be the residual of \(x_{i}^{M}\) regressed on \(x_{i}^{M}\), and \(r_{j}^{i}\) be the residual of \(x_{i}^{M}\) regressed on \(x_{j}^{M}\). Suppose \(x_{i}\not\perp\!\!\!\perp x_{j}\). Then, \(x_{i},x_{j}\) are in AP2 relation if and only if \(r_{i}^{j}\perp\!\!\!\perp x_{i}^{M}\) and \(r_{j}^{i}\perp\!\!\!\perp x_{j}^{M}\)._

If a pair of nodes \(x_{i},x_{j}\) is ancestrally related, fully ascertaining their ancestral relation involves discerning between the ancestor and descendent. As illustrated in Lemmas 3.5 and 3.6 (proofs in Appendices A.6, A.7), this can be determined through marginal independence testing after sequential univariate regressions with respect to the mutual ancestor set.

**Lemma 3.5** (Ap3).: _Let \(r_{i}^{j}\) be the residual of the \(x_{j}\) regressed on \(x_{i}\), and \(r_{j}^{i}\) be the residual of \(x_{i}\) regressed on \(x_{j}\). Vertices \(x_{i},x_{j}\) are in AP3 relation if and only if \(x_{i}\not\perp\!\!\!\perp x_{j}\) and one of the following

Figure 1: Illustrative DAG, where \(x_{1}\) is a root, \(x_{3}\) is a leaf, \(x_{3}\in\text{Ch}(x_{2}),x_{3}\in\text{De}(x_{1})\).

Figure 2: Enumeration of active causal path relation types between a pair of nodes \(x_{i}\) and \(x_{j}\). Dashed arrows indicate ancestorship.

[MISSING_PAGE_EMPTY:5]

between active causal paths and parent-child relationships. We assume the unique identifiability of the nonlinear ANM as described by Peters et al. [25], and provide the conditions in Appendix B.1.

Nonlinear Topological SortIn the linear setting, we determined ancestral relations through a sequence of pairwise regressions that led to independent residuals. However, a naive extension of this method into the nonlinear setting would fail, as regressions yield independent residuals under different conditions in the nonlinear case. For clarity, we demonstrate how LHTS fails to correctly recover causal relationships in an exemplary 3-node DAG with nonlinear causal mechanisms.

Consider a DAG \(G\) with three vertices \(x_{1},x_{2},x_{3}\), where \(x_{1}\to x_{3},x_{2}\to x_{3}\). The functional causal relationships are nonlinear, given by \(x_{1}=\varepsilon_{1},x_{2}=\varepsilon_{2},x_{3}=x_{1}x_{2}+\varepsilon_{3}\), where \(\varepsilon_{i}\) are mutually independent noise variables. We focus on whether LHTS can recover the parent-child relationship between \(x_{1}\) and \(x_{3}\). LHTS finds that the relationship between \(x_{1},x_{3}\) is unknown in Stage 1. In Stage 2, LHTS runs pairwise regressions between \(x_{1},x_{3}\) but _incorrectly concludes that \(x_{1},x_{3}\) are not in AP3 relation_ because neither pairwise regression provides an independent residual; both parents of \(x_{3}\) must be included in the covariate set for an independent residual to be recovered.

To handle nonlinear causal relationships, we shift our focus to searching for a different set of local substructures: the connection between active causal parental paths and the existence of parent-child relationships. We will use the existence of specific parent-child relationships to first obtain a superset of root vertices, then prune away non-roots. Once all root vertices are identified, we build the hierarchical topological sort through nonparametric regression, layer by layer.

Given a vertex \(x_{j}\) and one of its potential parents \(x_{i}\), we first provide an enumeration of all potential active casual parental path types between them with respect to \(C=\text{PA}(x_{j})\setminus x_{i}\) (which could potentially be the empty set) in Figure 3 and Lemma 4.1 (proof in Appendix B.2).

**Lemma 4.1** (Active Causal Parental Path Relation Enumeration).: _Let \(x_{i},x_{j}\in V\) be a pair of distinct nodes, where \(x_{i}\) is one of the potential parents of \(x_{j}\). Let \(C=\text{PA}(x_{j})\setminus x_{i}\). There are in total four possible types of active causal parental path relations between \(x_{i}\) and \(x_{j}\) with respect to \(C\): PP1) \(x_{i}\) and \(x_{j}\) are not directly causally related, and no active path exists between \(x_{i}\) and \(C\); PP2) \(x_{i}\) directly causes \(x_{j}\) (\(x_{i}\to x_{j}\)), and no active path exists between \(x_{i}\) and \(C\); PP3) \(x_{i}\) directly causes \(x_{j}\) (\(x_{i}\to x_{j}\)), and there exists an active path between \(x_{i}\) and \(C\); PP4) \(x_{i}\) and \(x_{j}\) are not directly causally related, and there exists an active path between \(x_{i}\) and \(C\)._

Next, for a pair of distinct vertices \(x_{i},x_{j}\in V\), we establish the connection between pairwise independence properties and active causal parental path relations in Lemma 4.2 (proof in Appendix B.3). This allows us to reduce the cardinality of the potential pairs of vertices under consideration in the later stages of the algorithm.

**Lemma 4.2** (Non-PP1).: _Vertices \(x_{i},x_{j}\in V\) are not in PP1 relation if and only if \(x_{i}\not\perp x_{j}\)._

In Lemma 4.3, we show that all pairs of vertices that are in PP2 relation can be identified through local nonparametric regressions (proof in B.4).

**Lemma 4.3** (PP2).: _Let \(x_{i},x_{j}\in V\), \(P_{ij}=\{x_{k}\in V:x_{k}\perp\!\!\!\perp x_{i},x_{k}\not\perp\!\!\!\perp x_{j}\}\), \(r_{i}^{j}\) be the residual of \(x_{j}\) nonparametrically regressed on \(x_{i}\), and \(r_{i,P}^{j}\) be the residual of \(x_{j}\) nonparametrically regressed on \(x_{i}\) and all \(x_{k}\in P_{ij}\). Suppose \(x_{i}\) and \(x_{j}\) are not in PP1 relation. Then, \(x_{i}\) and \(x_{j}\) are in PP2 relation if and only if one of the following holds: 1) \(x_{i}\perp\!\!\!\perp r_{i}^{j}\) or 2) \(x_{i}\perp\!\!\!\perp r_{i,P}^{j}\)._

Condition 1) of Lemma 4.3 is relevant when \(C=\emptyset\): in this case, pairwise regression identifies \(x_{i}\) as a parent of \(x_{j}\). Condition 2) is relevant when \(C\neq\emptyset\): we leverage the independence of \(x_{i}\) from the rest

Figure 3: Enumeration of the potential active causal paths among a fixed variable \(x_{j}\), one of its potential parents \(x_{i}\), and \(C=\text{PA}(x_{j})\setminus x_{i}\). Solid arrows denote parenthood relations, and undirected dashed connections indicate the existence of active paths.

of \(x_{j}\)'s parents to generate \(P_{ij}\), a set that contains \(C\), but does not contain any of \(x_{j}\)'s descendants. If an independent residual were to be recovered by nonparametrically regressing \(x_{j}\) onto \(x_{i}\) and \(P_{ij}\), we identify \(x_{i}\) as a parent of \(x_{j}\).

Let \(W\) be the set of all parent vertices that are in PP2 relation with at least one vertex, i.e., the union of \(x_{i}\) satisfying either condition of Lemma 4.3. We now show that all non-isolated root vertices are contained in \(W\), and they can be differentiated from non-roots in \(W\) that are also in PP2 relations.

**Lemma 4.4** (Roots).: _All non-isolated root vertices are contained in \(W\). In addition, \(x_{i}\in W\) is a root vertex if and only if 1) \(x_{i}\) is not a known descendant of any \(x_{j}\in W\), and 2) for each \(x_{j}\in W\), either a) \(x_{i}\perp\!\!\!\perp x_{j}\), b) \(x_{j}\) is in PP2 relation to \(x_{i}\), i.e., \(x_{j}\in\text{Ch}(x_{i})\), or c) there exists a child of \(x_{i}\), denoted by \(x_{k}\), that cannot be d-separated from \(x_{j}\) given \(x_{i}\), i.e., \(x_{j}\not\perp\!\!\!\perp x_{k}|x_{i}\)._

Lemma 4.4 relies on the following intuition: for any non-isolated root vertex \(x_{i}\) and descendant \(x_{k}\in\text{{\bf De}}(x_{i})\), there exists a child of \(x_{i}\) that 1) lies on a directed path between \(x_{i}\) and \(x_{k}\), and 2) is in PP2 relation with \(x_{i}\). Vertex \(x_{i}\) will fail to d-separate this specific child from the marginally dependent non-root descendant \(x_{k}\).1 On the other hand, non-roots in \(W\) must d-separate the children they are in PP2 relation to from all marginally dependent roots: this asymmetry allows non-roots to be pruned. See Appendix B.5 for a detailed proof.

Footnote 1: Vertices \(x_{i},x_{j}\) are said to be d-separated by a set \(\mathbf{Z}\) iff there is no active path between \(x_{i},x_{j}\) relative to \(\mathbf{Z}\).

We propose a method, Algorithm 2, that leverages the above results to discover a hierarchical topological ordering: we first use the above lemmas to identify the roots, then use nonparametric regression to discover the rest of the hierarchical sort.

```
1:input: vertices \(x_{1},\ldots,x_{d}\in V\).
2:output: hierarchical sort \(\pi_{L}(V)\).
3:initialize: parent relations set \(\mathrm{PRS}\).
4:Stage 1:Not-PP1 Relations
5:for all pairs \(x_{i},x_{j}\)do
6:if\(x_{i}\not\perp\!\!\!\perp x_{j}\)then
7:\(\mathrm{PRS}:x_{i},x_{j}\) not in PP1 relations.
8:if\(x_{i}\) is in PP1 relation with all vertices then
9: PRS: \(x_{i}\) is isolated, sort \(x_{i}\): \(\pi_{L}(x_{i})=0\).
10:Stage 2:PP2 Relations
11:for all \(x_{i},x_{j}\) not in PP1 relations do
12: Set \(r_{i}^{j}\) as residual of \(x_{j}\) regressed on \(x_{i}\).
13: Set \(r_{i,P}^{j}\) as residual of \(x_{j}\) regressed on \(x_{i},P_{ij}\).
14:if\(x_{i}\perp\!\!\!\perp r_{i}^{j}\)or\(x_{i}\not\perp r_{i,P}^{j}\)then
15:\(\mathrm{PRS}:x_{i},x_{j}\) are in PP2 relations, \(x_{i}\in\text{{\bf Pa}}(x_{j})\).
16:Stage 3:Root Identification
17:for\(x_{i}\in\) PP2 relation do
18: Check if \(x_{i}\) does not d-separate any child from all marginally dependent vertices. If so, then update \(\mathrm{PRS}:x_{i}\) is a root, sort \(x_{i}\) into layer 0, \(\pi_{L}(x_{i})=0\).
19:Stage 4:Obtain Sort
20:for\(k\in\{1,\ldots,d-1\}\)do
21:for all unsorted \(x_{i}\)do
22: Set \(r_{i}\) as the residual of \(x_{i}\) regressed on sorted features in \(\pi_{H}\).
23: If \(r_{i}\perp\!\!\!\perp x_{j}\forall x_{j}\in\pi_{H}\), add \(x_{i}\) into the current layer \(\pi_{L}(x_{i})=k\).
24:return\(\pi_{L}(V)\) ```

**Algorithm 2****NITS**: Nonlinear Hierarchical Topological Sort

In Stage 1, we discover all pairs \(x_{i},x_{j}\) not in PP1 relation by testing for marginal dependence; in Stage 2, we leverage Lemma 4.3 to find the vertex pairs that are in PP2 relations, a superset of the root vertices; in Stage 3 we prune non-roots by finding they d-separate their children from at least one dependent vertex in \(W\) (Lemma 4.4); in Stage 4 we identify vertices in the closest unknown layer by regressing them on sorted nodes and finding independent residuals. We show the correctness of Algorithm 2 in Theorem 4.5 (proof in Appendix B.6), and the worst case time complexity in Theorem 4.6 (proof in Appendix B.7). We provide a walk-through of Algorithm 2 in Appendix B.8.

**Theorem 4.5**.: _Given a graph \(G\), Algorithm 2 asymptotically finds a correct hierarchical sort of \(G\)._

**Theorem 4.6**.: _Given \(n\) samples of \(d\) vertices generated by a identifiable nonlinear ANM, the worst case runtime complexity of Algorithm 2 is upper bounded by \(O(d^{3}n^{3})\)._

The number of nonparametric regressions run by NHTS in each step is actually inversely related to the size of the covariate sets, while the number of regressions in each step of RESIT and NoGAM are directly proportionate to the covariate set size. We provide a formal analysis of the reduction in complexity for the worst case (fully connected DAG) in Theorem 4.7 (proof in Appendix B.9):

**Theorem 4.7**.: _Consider a fully connected DAG \(G=(V,E)\) with nonlinear ANM. Let \(d:=|V|\). Let \(n_{k}^{\mathrm{NHTS}}\) be the number of nonparametric regressions with covariate set size \(k\in[d-2]\) run by NHTS when sorting \(V\); we similarly define \(n_{k}^{\mathrm{RESIT}}\) and \(n_{k}^{\mathrm{NoGAM}}\) respectively. Then, \(n_{k}^{\mathrm{NHTS}}=d-k\), and \(n_{k}^{\mathrm{RESIT}}=n_{k}^{\mathrm{NoGAM}}=k+1\). This implies that for all \(k>\frac{d}{2}\), \(n_{k}^{\mathrm{NHTS}}<n_{k}^{\mathrm{RESIT}}=n_{k}^{\mathrm{NoGAM}}\)._

## 5 Edge Discovery

Parent selection from a topological ordering \(\pi\) via regression is traditionally a straightforward task in the infinite sample setting: for each vertex \(x_{i}\), \(\pi\) establishes \(J_{i}=\{x_{k}:\pi(x_{k})<\pi(x_{i})\}\), a superset of \(x_{i}\)'s parents that contains none of \(x_{i}\)'s descendants. A general strategy for pruning \(\text{Pa}(x_{i})\) from \(J_{i}\) is to regress \(x_{i}\) on \(J_{i}\) and check which \(x_{k}\in J_{i}\) are relevant predictors. The key issue is that \(J_{i}\) grows large in high-dimensional graphs: current edge pruning methods either make strong parametric assumptions or suffer in sample complexity. Lasso and GAM methods impose linear and additive models, failing to correctly identify parents in highly nonlinear settings. RESIT assumes a more general nonlinear ANM, but requires huge sample sizes and oracle independence tests for accurate parent set identification: authors in [25] confirm this, saying "despite [our] theoretical guarantee[s], RESIT does not scale well to a high number of nodes."

We propose an entirely nonparametric constraint-based method that uses a local conditioning set \(Z_{ij}\) to discover whether \(x_{i}\in\text{Pa}(x_{j})\), rather than \(J_{i}\), outperforming previous methods by relaxing parametric assumptions and conditioning on fewer variables. The following lemma outlines a sufficient condition for determining whether an edge exists between two vertices (proof in Appendix C.1), visualized in Figure 4.

**Lemma 5.1** (Parent Discovery).: _Let \(\pi\) be a topological ordering, \(x_{i},x_{j}\) such that \(\pi(x_{i})<\pi(x_{j})\). Let \(Z_{ij}=C_{ij}\cup M_{ij}\), where \(C_{ij}=\{x_{k}:x_{k}\in\text{Pa}(x_{i}),x_{k}\not\perp x_{j}\},M_{ij}=\{x_{k}: x_{k}\in\text{Pa}(x_{j}),\pi(x_{i})<\pi(x_{k})<\pi(x_{j})\}\). Then, \(x_{i}\to x_{j}\iff x_{i}\not\perp x_{j}|Z_{ij}\)._

The intuition is that to determine whether \(x_{i}\to x_{j}\), instead of conditioning on all potential ancestors of \(x_{j}\), it suffices to condition on potential confounders of \(x_{i},x_{j}\) (\(C_{ij}\)) and potential mediators between \(x_{i}\) and \(x_{j}\) (\(M_{ij}\)). This renders all backdoor and frontdoor paths inactive, except the frontdoor path corresponding to a potential direct edge from \(x_{i}\) to \(x_{j}\).

**Edge Discovery** We propose an algorithm that leverages the above results to discover the true edges admitted by any topological ordering by running the described conditional independence test in a specific ordering. We give the implementation for pruning a topological sort here, but our approach can be generalized to a hierarchical version (see Appendix C.2). We show the correctness of Algorithm 3 in Theorem 5.2 (proof in Appendix C.3).

**Theorem 5.2**.: _Given a correct topological ordering of \(G\), Algorithm 3 asymptotically finds correct parent sets \(\text{PA}(x_{i}),\forall x_{i}\in G\)._

The key insight is to check each potential parent-offspring relation using the conditional independence test \(x_{i}\perp\!\!\!\perp x_{j}|Z_{ij}\) such that previous steps in the algorithm obtain all vertices in both \(C_{ij}\) and \(M_{ij}\)

Figure 4: DAG corresponding to Lemma 5.1, which tests whether \(x_{i}\in\text{Pa}(x_{j})\) (i.e., whether the red arrow exists).

We first fix a vertex \(x_{j}\) whose parent set we want to discover. We then check if vertices ordered before \(x_{j}\) are parents of \(x_{j}\) in reverse order, starting with the vertex immediately previous to \(x_{j}\) in \(\pi\). This process starts at the beginning of \(\pi\), meaning we discover parent-offspring relations from root to leaf (see Appendix C.3 for a detailed walk-through and proof). We show the worst case time complexity of Algorithm 3 in Theorem 5.3 (proof in Appendix C.4).

**Theorem 5.3**.: _Given \(n\) samples of \(d\) vertices generated by a model corresponding to a DAG \(G\), the runtime complexity of ED is upper bounded by \(O(d^{2}n^{3})\)._

## 6 Experiments

**Setup** Methods2 are evaluated on 20 DAGs in each trial. The DAGs are randomly generated with the Erdos-Renyi model [5]; the probability of an edge is set such that the average number of edges in each \(d\)-dimensional DAG is \(d\). Gaussian, Uniform, or Laplace noise is used as the exogenous error. In experiments for linear topological sorting methods (Figure 5), we use linear causal mechanisms to generate the data; in experiments for nonlinear topological sorting methods (Figure 6) and edge pruning algorithms (Figure 7), we use quadratic causal mechanisms to generate the data. Existing ANM methods are prone to exploiting artifacts that are more common in simulated ANMs than real-world data [28; 29], inflating their performance on synthetic DAGs and leaving real-world applicability an open question. To reduce concerns about such artifacts, data were generated with reduced \(R^{2}\)-sortability [29], and standardized to zero mean and unit variance [28].

Footnote 2: Code: https://github.com/Sujai1/hybrid-discovery.

**Metrics**\(A_{top}\) is equal to the percentage of edges that can be recovered by the returned topological ordering (an edge cannot be recovered if a child is sorted before a parent). We note that \(A_{top}\) is a normalized version of the topological ordering divergence \(D_{top}\) defined in [30]. Edge pruning algorithms return a list of predicted parent sets for each vertex: \(F_{1}=2\frac{\text{Precision-Recall}}{\text{Precision + Recall}}\) measures the performance of these predictions.

**Linear Topological Sorts** Figure 5 demonstrates the performance of our linear topological ordering algorithm, LHTS, in comparison with the benchmark algorithms, DirectLiNGAM [38] and \(R^{2}\)-Sort [29]. \(R^{2}\)-Sort is a heuristic sorting algorithm that exploits artifacts common in simulated ANMs; both benchmarks are agnostic to the noise distribution. We observe that both DirectLiNGAM and LHTS significantly outperform \(R^{2}\)-sort. LHTS demonstrates asymptotic correctness in Figure 5(c), achieving near-perfect \(A_{top}\) at \(n=2000\). However, LHTS has consistently lower \(A_{top}\) than DirectLiNGAM in Figure 5(a). On the other hand, LHTS encodes more causal information: the orderings produced by LHTS in Figure 5(b) had roughly \(\sim 70\%\) fewer layers than the orderings produced by DirectLiNGAM, reducing the size of potential parent sets \(J\) by identifying many non-causal relationships.

Figure 5: Performance of LHTS on synthetic data. Top row: \(n=500\) with varying dimension \(d\). Bottom row: \(d=10\) with varying sample size \(n\). See Appendix D.1 for runtime results.

Nonlinear Topological SorsFigure 6 illustrates the performance of our nonlinear topological sorting algorithm. We take GES [3], GRaSP [12], GSP [39], DirectLiNGAM [38], NoGAM [20], and \(R^{2}\)-Sort [29] as baseline comparators that are all agnostic to the noise distribution. We excluded PC and RESIT since in general they perform much worse than baseline methods [20]. We note that as DirectLiNGAM, NoGAM, and NHTS are FCM-based methods, they each return a unique topological ordering; however, as GES, GRaSP, and GSP are scoring-based methods [7], they return only a MEC. All topological orderings contained within an MEC satisfy every conditional independence constraint in the data, and therefore are all _equally valid_. To enable a fair comparison, we randomly select one ordering permitted by an outputted MEC for evaluation. We note that NHTS outperformed all baselines, achieving the highest median \(A_{top}\) in all trials. Furthermore, as expected from Theorem 4.6, NHTS ran up to \(4\times\) faster than NoGAM (see Appendix D.2). We provide additional experiments over DAGs with increasing edge density in Appendix D.3.

Edge PruningFigure 7 illustrates the performance of our edge pruning algorithm ED. We take covariate hypothesis testing with GAMs (CAM-pruning [1]), Lasso regression, and RESIT as baseline comparators that are all agnostic to the noise distribution. All algorithms were given correct topological sorts: ED significantly outperformed all baselines, with the highest median \(F_{1}\) score in all trials. ED was slower than Lasso, but was significantly faster than the other nonlinear edge pruning algorithms, CAM-pruning and RESIT. RESIT was excluded from higher-dimensional tests due to runtime issues. The poor performance of baseline methods highlights the need for a sample efficient nonparametric method for accurate causal discovery of nonlinear DGPs. We provide additional experiments in settings with increasing density and varying noise distributions in Appendix D.4.

DiscussionIn this paper we developed novel global causal discovery algorithms by searching for and leveraging local causal relationships. We improved on previous topological ordering methods by running fewer regressions, each with lower dimensionality, producing hierarchical topological sorts. Additionally, we improved on previous edge pruning procedures by introducing a nonparametric constraint-based method that conditions on far fewer variables to achieve greater recovery of parent sets. We tested our methods on robustly generated synthetic data, and found that both our nonlinear sort NHTS and edge pruning algorithm ED significantly outperformed baselines. Future work includes extending the topological sorting algorithms to the full ANM setting with both linear and nonlinear functions, simultaneously exploiting both ancestor-descendent and parent-child relations, as well as adapting our approach to handle various forms of unmeasured confounding. Additionally, we aim to develop statistical guarantees of sample complexity for our methods, extending previous results [50] derived in the setting of nonlinear ANMs with Gaussian noise.

Figure 6: Performance of NHTS on synthetic data, \(n=300\), dimension \(d=10\), with varying error distributions: Gaussian, Laplace, Uniform (left, middle, right). See Appendix D.2 for runtime results.

Figure 7: Performance of ED on synthetic data, uniform noise. Left, middle: \(n=300\) with varying dimension \(d\). Right: \(d=10\) with varying sample size \(n\). See Appendix D.5 for runtime results.

## References

* Buhlmann et al. [2014] Buhlmann, P., Peters, J., and Ernest, J. CAM: Causal additive models, high-dimensional order search and penalized regression. _The Annals of Statistics_, 2014. URL http://arxiv.org/abs/1310.1533.
* Cheng et al. [2023] Cheng, D., Li, J., Liu, L., Yu, K., Duy Le, T., and Liu, J. Toward Unique and Unbiased Causal Effect Estimation From Data With Hidden Variables. _IEEE Transactions on Neural Networks and Learning Systems_, 2023. URL https://ieeexplore.ieee.org/document/9674196/.
* Chickering [2002] Chickering, D. M. Learning Equivalence Classes of Bayesian Network Structures. _Journal of Machine Learning Research_, 2002. URL https://arxiv.org/abs/1302.3566.
* Comon [1994] Comon, P. Independent component analysis, a new concept? _Signal Processing_, 1994. URL https://hal.science/hal-00417283/document.
* Erdos and Renyi [1960] Erdos, P. and Renyi, A. On the evolution of random graphs. _Publicationes Mathematicae_, 1960. URL https://publi.math.unideb.hu/load_doi.php?pdoi=10_5486_PMD_1959\(6\)3_4_12.
* Gan et al. [2021] Gan, K., Jia, S., and Li, A. Greedy approximation algorithms for active sequential hypothesis testing. _Advances in Neural Information Processing Systems_, 2021. URL https://dl.acm.org/doi/proceedings/10.5555/3540261.
* Glymour et al. [2019] Glymour, C., Zhang, K., and Spirtes, P. Review of Causal Discovery Methods Based on Graphical Models. _Frontiers in Genetics_, 2019. URL https://www.frontiersin.org/article/10.3389/fgene.2019.00524/full.
* Gupta et al. [2023] Gupta, S., Childers, D., and Lipton, Z. C. Local Causal Discovery for Estimating Causal Effects. In _Proceedings of the 2nd Conference on Causal Learning and Reasoning (CLEaR)_, 2023. URL http://arxiv.org/abs/2302.08070.
* Hoyer and Hyttinen [2009] Hoyer, P. O. and Hyttinen, A. Bayesian discovery of linear acyclic causal models. In _Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence_, 2009. URL https://arxiv.org/abs/1205.2641.
* Hoyer et al. [2008] Hoyer, P. O., Janzing, D., Mooij, J. M., Peters, J., and Scholkopf, B. Nonlinear causal discovery with additive noise models. In _Advances in Neural Information Processing Systems_, 2008. URL https://dl.acm.org/doi/10.5555/2981780.2981866.
* Hoyer et al. [2008] Hoyer, P. O., Shimizu, S., Kerminen, A. J., and Palviainen, M. Estimation of causal effects using linear non-Gaussian causal models with hidden variables. _International Journal of Approximate Reasoning_, 2008. URL https://linkinghub.elsevier.com/retrieve/pii/S0888613X08000212.
* Lam et al. [2022] Lam, W.-Y., Andrews, B., and Ramsey, J. Greedy Relaxations of the Sparsest Permutation Algorithm. _Proceedings of the 38th Conference on Uncertainty in Artificial Intelligence_, 2022. URL https://proceedings.mlr.press/v180/lam22a/lam22a.pdf.
* Lee et al. [2022] Lee, J. J., Srinivasan, R., Ong, C. S., Alejo, D., Schena, S., Shpitser, I., Sussman, M., Whitman, G. J., and Malinsky, D. Causal determinants of postoperative length of stay in cardiac surgery using causal graphical learning. _The Journal of Thoracic and Cardiovascular Surgery_, 2022. URL https://linkinghub.elsevier.com/retrieve/pii/S002252232200900X.
* Li et al. [2022] Li, A., Lee, J., Montagna, F., Trevino, C., and Ness, R. Dodiscover: Causal discovery algorithms in Python., 2022. URL https://github.com/py-why/dodiscover.
* Linder et al. [2022] Linder, J., Koplik, S. E., Kundaje, A., and Seelig, G. Deciphering the impact of genetic variation on human polyadenylation using aparent2. _Genome biology_, 2022. URL https://pubmed.ncbi.nlm.nih.gov/36335397/.
* Maasch et al. [2024] Maasch, J., Pan, W., Gupta, S., Kuleshov, V., Gan, K., and Wang, F. Local discovery by partitioning: Polynomial-time causal discovery around exposure-outcome pairs. In _Proceedings of the 40th Conference on Uncertainty in Artificial Intelligence_, 2024. URL https://arxiv.org/abs/2310.17816.

* Marra and Wood [2023] Marra, G. and Wood, S. Practical variable selection for generalized additive models. _Computational Statistics and Data Analysis_, 2023. URL https://www.sciencedirect.com/science/article/abs/pii/S0167947311000491.
* Marra and Wood [2023] Marra, G. and Wood, S. Regression Shrinkage and Selection via the Lasso. _Computational Statistics and Data Analysis_, 2023. URL https://www.sciencedirect.com/science/article/abs/pii/S0167947311000491.
* Montagna et al. [2023] Montagna, F., Mastakouri, A. A., Eulig, E., Nocetti, N., Rosasco, L., Janzing, D., Aragam, B., and Locatello, F. Assumption violations in causal discovery and the robustness of score matching. _In 37th Conference on Neural Information Processing Systems_, 2023. URL http://arxiv.org/abs/2310.13387.
* Montagna et al. [2023] Montagna, F., Nocetti, N., Rosasco, L., Zhang, K., and Locatello, F. Causal Discovery with Score Matching on Additive Models with Arbitrary Noise. _Proceedings of the 2nd Conference on Causal Learning and Reasoning_, 2023. URL http://arxiv.org/abs/2304.03265.
* Montagna et al. [2023] Montagna, F., Nocetti, N., Rosasco, L., Zhang, K., and Locatello, F. Scalable Causal Discovery with Score Matching. _Proceedings of the 2nd Conference on Causal Learning and Reasoning_, 2023. URL http://arxiv.org/abs/2304.03382.
* Niu et al. [2024] Niu, W., Gao, Z., Song, L., and Li, L. Comprehensive Review and Empirical Evaluation of Causal Discovery Algorithms for Numerical Data, 2024. URL https://arxiv.org/abs/2407.13054.
* Pearl et al. [2016] Pearl, J., Glymour, M., and Jewell, N. P. _Causal inference in statistics: a primer_. Wiley, 2016. URL https://www.datascienceassn.org/sites/default/files/CAUSAL%20INFERENCE%20IN%20STATISTICS.pdf.
* Pedregosa et al. [2011] Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E. Scikit-learn: Machine learning in Python. _Journal of Machine Learning Research_, 2011. URL https://jmlr.csail.mit.edu/papers/volume12/pedregosa11a/pedregosa11a.pdf.
* Peters et al. [2014] Peters, J., Mooij, J., Janzing, D., and Scholkopf, B. Causal Discovery with Continuous Additive Noise Models. _Journal of Machine Learning Research_, 2014. URL http://arxiv.org/abs/1309.6779.
* Ramos-Carreno and Torrecilla [2023] Ramos-Carreno, C. and Torrecilla, J. dcor: Distance correlation and energy statistics in python. _SoftwareX_, 2023. URL https://www.sciencedirect.com/science/article/pii/S2352711023000225.
* Raskutti and Uhler [2013] Raskutti, G. and Uhler, C. Learning directed acyclic graphs based on sparsest permutations. _STAT_, 2013. URL https://arxiv.org/abs/1307.0366.
* Reisach et al. [2021] Reisach, A., Seiler, C., and Weichwald, S. Beware of the simulated dag! causal discovery benchmarks may be easy to game. _Advances in Neural Information Processing Systems_, 2021. URL https://arxiv.org/abs/2102.13647.
* Reisach et al. [2023] Reisach, A. G., Tami, M., Seiler, C., Chambaz, A., and Weichwald, S. A Scale-Invariant Sorting Criterion to Find a Causal Order in Additive Noise Models. _37th Conference on Neural Information Processing Systems_, 2023. URL http://arxiv.org/abs/2303.18211.
* Rolland et al. [2022] Rolland, P., Cevher, V., Kleindessner, M., Russel, C., Scholkopf, B., Janzing, D., and Locatello, F. Score Matching Enables Causal Discovery of Nonlinear Additive Noise Models. In _Proceedings of the 39th International Conference on Machine Learning_, 2022. URL https://arxiv.org/abs/2203.04413.
* Runge et al. [2019] Runge, J., Bathiany, S., Bollt, E., Camps-Valls, G., Coumou, D., Deyle, E., Glymour, C., Kretschmer, M., Mahecha, M. D., Munoz-Mari, J., van Nes, E. H., Peters, J., Quax, R., Reichstein, M., Scheffer, M., Scholkopf, B., Spirtes, P., Sugihara, G., Sun, J., Zhang, K., and Zscheischler, J. Inferring causation from time series in Earth system sciences. _Nature Communications_, 2019. URL http://www.nature.com/articles/s41467-019-10105-3.

* Sanchez et al. [2023] Sanchez, P., Liu, X., O'Neil, A. Q., and Tsaftaris, S. A. Diffusion models for causal discovery via topological ordering. _Proceedings of the 39th International Conference on Machine Learning_, 2023. URL https://arxiv.org/abs/2210.06201.
* Seabold and Perktold [2010] Seabold, S. and Perktold, J. statsmodels: Econometric and statistical modeling with python. In _9th Python in Science Conference_, 2010. URL https://www.statsmodels.org/stable/index.html.
* Shah et al. [2022] Shah, A., Shanmugam, K., and Ahuja, K. Finding Valid Adjustments under Non-ignorability with Minimal DAG Knowledge. _Proceedings of the 25th International Conference on Artificial Intelligence and Statistics_, 2022. URL http://arxiv.org/abs/2106.11560.
* Shah et al. [2023] Shah, A., Shanmugam, K., and Kocaoglu, M. Front-door adjustment beyond markov equivalence with limited graph knowledge. _Advances in Neural Information Processing Systems_, 2023. URL https://arxiv.org/abs/2306.11008.
* Shimizu [2014] Shimizu, S. Lingam: Non-Gaussian Methods for Estimating Causal Structures. _Behaviormetrika_, 2014. URL http://link.springer.com/10.2333/bhmk.41.65.
* Shimizu et al. [2006] Shimizu, S., Hoyer, P. O., Hyvarinen, A., and Kerminen, A. A Linear Non-Gaussian Acyclic Model for Causal Discovery. _Journal of Machine Learning Research_, 2006. URL https://dl.acm.org/doi/10.5555/1248547.1248619.
* Shimizu et al. [2011] Shimizu, S., Inazumi, T., Sogawa, Y., Hyvarinen, A., Kawahara, Y., Washio, T., Hoyer, P. O., Bollen, K., and Hoyer, P. Directlingam: A direct method for learning a linear non-gaussian structural equation model. _Journal of Machine Learning Research_, 2011. URL https://dl.acm.org/doi/10.5555/1953048.2021040.
* Solus et al. [2021] Solus, L., Wang, Y., and Uhler, C. Consistency guarantees for greedy permutation-based causal inference algorithms. _Biometrika_, 2021. URL https://academic.oup.com/biomet/article-abstract/108/4/795/6062392?redirectedFrom=fulltext.
* Spirtes [2001] Spirtes, P. An Anytime Algorithm for Causal Inference. _Proceedings of Machine Learning Research_, 2001. URL https://proceedings.mlr.press/r3/spirtes01a/spirtes01a.pdf.
* Spirtes and Zhang [2016] Spirtes, P. and Zhang, K. Causal discovery and inference: concepts and recent methodological advances. _Applied Informatics_, 2016. URL https://applied-informatics-j.springeropen.com/articles/10.1186/s40535-016-0018-x.
* Spirtes et al. [2000] Spirtes, P., Glymour, C., and Scheines, R. _Causation, Prediction, and Search_. Springer New York, 2000. URL http://link.springer.com/10.1007/978-1-4612-2748-9.
* Squires [2021] Squires, C. Graphical Model Learning, 2021. URL https://github.com/uhlerlab/graphical_model_learning.
* Steeg [2014] Steeg, G. Non-parametric Entropy Estimation Toolbox, 2014. URL https://github.com/gregversteeg/NPEET.
* Takashi et al. [2023] Takashi, I., Mayumi, I., Yan, Z., Takashi Nicholas, M., and Shohei, S. Python package for causal discovery based on lingam. _Journal of Machine Learning Research_, 2023. URL https://www.jmlr.org/papers/volume24/21-0321/21-0321.pdf.
* Wang et al. [2022] Wang, S. K., Nair, S., Li, R., Kraft, K., Pampari, A., Patel, A., Kang, J. B., Luong, C., Kundaje, A., and Chang, H. Y. Single-cell multiome of the human retina and deep learning nominate causal variants in complex eye diseases. _Cell genomics_, 2022. URL https://www.sciencedirect.com/science/article/pii/S2666979X22001069.
* Wong et al. [2021] Wong, A. K., Sealfon, R. S., Thesfeld, C. L., and Troyanskaya, O. G. Decoding disease: from genomes to networks to phenotypes. _Nature Reviews Genetics_, 2021. URL https://www.nature.com/articles/s41576-021-00389-x.
* Zhang and Hyvarinen [2009] Zhang, K. and Hyvarinen, A. On the Identifiability of the Post-Nonlinear Causal Model. _Uncertainty in Artificial Intelligence_, 2009. URL https://arxiv.org/pdf/1205.2599.

* Zheng et al. [2024] Zheng, Y., Huang, B., Chen, W., Ramsey, J., Gong, M., Cai, R., Shimizu, S., Spirtes, P., and Zhang, K. Causal-learn: Causal discovery in python. _Journal of Machine Learning Research_, 2024. URL https://arxiv.org/abs/2307.16405.
* Zhu et al. [2023] Zhu, Z., Locatello, F., and Cevher, V. Sample Complexity Bounds for Score-Matching: Causal Discovery and Generative Modeling, 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/0a3dc35a2391cabcb59a6b123544e3db-Paper-Conference.pdf.

## Appendix A Assumptions, Proofs, and Walkthrough for LHTS

### Identifiability in Linear Setting

As an example [25]: suppose \(X,N\) are normally distributed, \(X\perp\!\!\!\perp N\), and \(Y=aX+N\). If \(\bar{a}=\frac{aVar(X)}{\bar{a}^{2}Var(X)+\sigma^{2}}\neq\frac{1}{a}\) and \(\bar{N}=X-\bar{a}Y\), then \(\bar{N}\perp\!\!\!\perp Y\) and the following DGP also fits the same distribution of \(X,Y,N\): \(X=\bar{a}Y+\bar{N}\). To generalize the intuition that non-Gaussianity could lead to identifiability, [37] use independent component analysis [4] to provide the following theorem:

**Theorem A.1**.: _Assume a linear SEM with graph \(G\), where \(x_{i}=\sum_{k(j)<k(i)}b_{ij}x_{j}+\varepsilon_{i},\forall j=1,\ldots,d,\) where all \(\varepsilon_{j}\) are jointly independent and non-Gaussian distributed. Additionally, for each \(j\in\{1,\ldots,d\}\) we require \(b_{ij}\neq 0\) for all \(j\in\text{Pa}(x_{i})\). Then, \(G\) is identifiable from the distribution._

We assume that the identifiability conditions described in Theorem A.1 hold throughout Section 3.

### Proof of Lemma 3.1

**Lemma 3.1** (Active Causal Ancestral Path Relation Enumeration).: _Each pair of distinct nodes \(x_{i},x_{j}\in V\) can be in one of four possible active causal ancestral path relations: AP1) no active path exists between \(x_{i},x_{j}\); AP2) there exists an active backdoor path between \(x_{i},x_{j}\), but there is no active frontdoor path between them; AP3) there exists an active frontdoor path between \(x_{i},x_{j}\), but there is no active backdoor path between them; AP4) there exists an active backdoor path between \(x_{i},x_{j}\), and there exists an active frontdoor path between them._

Proof of Lemma 3.1.: For a pair of nodes \(x_{i}\) and \(x_{j}\), either \(x_{i}\perp\!\!\!\perp x_{j}\) or \(x_{i}\not\perp\!\!\!\perp x_{j}\). If the former, then \(x_{i},x_{j}\) are in AP1 relation. Suppose the latter is true: \(x_{i}\perp\!\!\!\perp x_{j}\) implies \(\exists\) at least one active path between \(x_{i},x_{j}\). Active paths can either be backdoor or frontdoor paths: therefore either a frontdoor path exists, a backdoor path exists, or both a frontdoor and backdoor path exist. Thus, \(x_{i},x_{j}\) are either in AP2, AP3, or AP4 relation.

### Proof of Lemma 3.2

**Lemma 3.2**.: _The ancestral relationship between a pair of distinct nodes \(x_{i},x_{j}\in V\) can be expressed using active causal path relations: \(x_{i},x_{j}\) are not ancestrally related if and only if they are in AP1 or AP2 relation; and \(x_{i},x_{j}\) are ancestrally related if and only if they are in AP3 or in AP4 relation._

Proof of Lemma 3.2.: Note, \(x_{i},x_{j}\) are ancestrally related if and only if there exists an active frontdoor path between them. The conclusion follows immediately. 

### Proof of Lemma 3.3

**Lemma 3.3** (Ap1).: _Vertices \(x_{i},x_{j}\) are in AP1 relation if and only if \(x_{i}\perp\!\!\!\perp x_{j}\)._

Proof of Lemma 3.3.: Note, \(x_{i}\perp\!\!\!\perp x_{j}\) if and only if there does not exist an active causal path between them. The conclusion follows immediately. 

### Proof of Lemma 3.4

**Lemma 3.4** (Ap2).: _Let \(M\) be the set of mutual ancestors between a pair of vertices \(x_{i}\) and \(x_{j}\), i.e., \(M=\text{An}(x_{i})\cap\text{An}(x_{j})\). Let \(x_{i}^{M},x_{j}^{M}\) be the result of sequentially regressing all mutual ancestors in \(M\) out of \(x_{i},x_{j}\) with univariate regressions, in any order. Then, let \(r_{i}^{j}\) be the residual of \(x_{j}^{M}\) regressed on \(x_{i}^{M}\), and \(r_{j}^{j}\) be the residual of \(x_{i}^{M}\) regressed on \(x_{j}^{M}\). Suppose \(x_{i}\not\perp\!\!\!\perp x_{j}\). Then, \(x_{i},x_{j}\) are in AP2 relation if and only if \(r_{i}^{j}\perp\!\!\!\perp x_{i}^{M}\) and \(r_{j}^{i}\perp\!\!\!\perp x_{j}^{M}\)._Proof of Lemma 3.4.: Under an identifiable LiNGAM, and given a linear topological ordering \(k:V\mapsto\mathbb{R}\), \(x_{i}\) and \(x_{j}\) admit the following representation:

\[x_{i} =\sum_{k(m)<k(i)}\alpha_{im}\varepsilon_{m}+\varepsilon_{i},\] \[x_{j} =\sum_{k(m)<k(j)}\alpha_{jm}\varepsilon_{m}+\varepsilon_{j},\]

where \(\varepsilon_{m}\) are jointly independent noise terms and non-Gaussian distributed, following from Theorem A.1. Additionally, for each \(m:k(m)<k(i)\) we require \(\alpha_{im}\neq 0\) for all \(m\in\text{{Pa}}(x_{i})\). Similarly for \(\alpha_{jm}\).

We first show the forward direction. Suppose \(x_{i},x_{j}\) are in AP2 relation. Note, \(x_{i}\not\perp x_{j}\). Let \(\overline{M}\) be the complement of \(M\), i.e., \(\overline{M}=V\setminus M\). Then, \(\exists\) the following decomposition of \(x_{i},x_{j}\):

\[x_{i} =\sum_{x_{m}\in M}\alpha_{im}\varepsilon_{m}+\sum_{x_{m}\in \overline{M}\cap\text{{An}}(x_{i})}\alpha_{im}\varepsilon_{m}+\varepsilon_{i},\] \[x_{j} =\sum_{x_{m}\in M}\alpha_{jm}\varepsilon_{m}+\sum_{x_{m}\in \overline{M}\cap\text{{An}}(x_{j})}\alpha_{jm}\varepsilon_{m}+\varepsilon_{i}.\]

Then, the \(x_{i}^{M},x_{j}^{M}\) have the general form

\[x_{i}^{M} =\sum_{x_{m}\in Y}\beta_{im}\varepsilon_{m}+\varepsilon_{i},\] \[x_{j}^{M} =\sum_{x_{m}\in Z}\beta_{jm}\varepsilon_{m}+\varepsilon_{j},\]

where \(Y\subseteq\overline{M}\cap\text{{An}}(x_{j}))\), \(Z\subseteq\overline{M}\cap\text{{An}}(x_{j}))\). Note, \(Y\cap Z=\emptyset\). As \(\varepsilon_{i}\) are all mutually independent, this implies that \(r_{i}^{j}\perp\!\!\!\perp r_{j}^{i}\).

We now show the reverse direction. Suppose \(x_{i}\not\perp\!\!\!\perp x_{j}\), \(r_{i}^{j}\perp\!\!\!\perp x_{i}^{M}\), and \(r_{j}^{i}\perp\!\!\!\perp x_{j}^{M}\). Note that \(x_{i}\not\perp\!\!\!\perp x_{j}\implies x_{i},x_{j}\) are not in AP1 relation. Suppose for contradiction that \(x_{i},x_{j}\) are in AP3 or AP4 relation, where \(x_{i}\in\text{{An}}(x_{j})\) (WLOG). Then \(\exists\) a frontdoor path between \(x_{i},x_{j}\), WLOG \(x_{i}\in\text{{An}}(x_{j})\). Regressing the descendent on the ancestor will always result in a dependent residual, even after projecting out the influence of the mutual ancestors. Therefore, \(x_{j}^{M}\not\perp r_{i}^{i}\), leading to a contradiction. Therefore, \(x_{i},x_{j}\) must be in AP2 relation. 

### Proof of Lemma 3.5

**Lemma 3.5** (Ap3).: _Let \(r_{i}^{j}\) be the residual of the \(x_{j}\) regressed on \(x_{i}\), and \(r_{j}^{i}\) be the residual of \(x_{i}\) regressed on \(x_{j}\). Vertices \(x_{i},x_{j}\) are in AP3 relation if and only if \(x_{i}\not\perp\!\!\!\perp x_{j}\) and one of the following holds: 1) \(x_{i}\perp\!\!\!\perp r_{i}^{j}\) and \(x_{j}\not\perp\!\!\!\perp r_{j}^{i}\), corresponding to \(x_{i}\in\text{{An}}(x_{j})\), or 2) \(x_{i}\not\perp\!\!\!\perp r_{i}^{j}\) and \(x_{j}\perp\!\!\!\perp r_{j}^{i}\), corresponding to \(x_{j}\in\text{{An}}(x_{i})\)._

Proof of Lemma 3.5.: We first show the forwards direction. Suppose \(x_{i},x_{j}\) are in \(AP3\) relation. Note an active path exists between \(x_{i},x_{j}\), so \(x_{i}\not\perp\!\!\!\perp x_{j}\). WLOG, let \(x_{i}\in\text{{An}}(x_{j})\). Then, \(x_{i},x_{j}\) admit the following representation:

\[x_{i} =x_{i}\] \[x_{j} =b_{ji}x_{i}+\sum_{x_{m}\in\text{{An}}(x_{j})\setminus x_{i}} \alpha_{jm}x_{m}+\varepsilon_{j}.\]

Note, as there does not exist a backdoor path between \(x_{i},x_{j}\), we have \(x_{i}\perp\!\!\!\perp x_{m}\forall x_{m}\in\text{{An}}(x_{j})\setminus x_{i}\). Therefore, \(r_{i}^{j}\perp\!\!\!\perp x_{i}\). Note that \(x_{j}\in\text{{De}}(x_{i})\): pairwise regression will leave \(r_{j}^{i}\not\perp\!\!\!\perp x_{j}\) as this an example of reverse causality, a special case of endogeneity [23].

We now show the reverse direction. WLOG, suppose \(x_{i},x_{j}\) satisfy condition 1) in Lemma 3.5. As \(x_{i}\not\perp\!\!\!\perp x_{j}\), they cannot be in AP1 relation. Suppose for contradiction that they are in AP2 or AP4 relation. Then, \(\exists\) a backdoor path between \(x_{i},x_{j}\), and a confounding \(x_{z}\) on that backdoor path. As \(x_{z}\) confounds \(x_{i},x_{j}\) and is not adjusted for in a pairwise regression, we have \(x_{i}\not\perp\!\!\!\perp r_{i}^{j},x_{j}\not\perp\!\!\!\perp x_{j}^{i}\), a contradiction. Therefore, \(x_{i},x_{j}\) must be in AP2 relation. 

### Proof of Lemma 3.6

**Lemma 3.6** (Ap4).: _Let \(M\), \(r_{i}^{j},r_{j}^{i},x_{i}^{M},x_{j}^{M}\) be as defined in Lemma 3.4. Suppose \(x_{i},x_{j}\) not in AP3 relation. Then, \(x_{i},x_{j}\) are in AP4 relation if and only if \(x_{i}\not\perp\!\!\!\perp x_{j}\) and one of the following holds: \(r_{i}^{j}\perp\!\!\!\perp x_{i}^{M}\) and \(r_{j}^{i}\not\perp\!\!\!\perp x_{j}^{M}\) corresponding to \(x_{i}\in\text{An}(x_{j})\), or 2) \(r_{i}^{j}\not\perp\!\!\!\perp x_{i}^{M}\) and \(r_{j}^{i}\perp\!\!\!\perp x_{j}^{M}\) corresponding to \(x_{j}\in\text{An}(x_{i})\)._

Proof of Lemma 3.6.: We first show the forward direction. Suppose \(x_{i},x_{j}\) are in AP4 relation. Note that an active path exists between \(x_{i},x_{j}\), so \(x_{i}\not\perp\!\!\!\perp x_{j}\). WLOG, suppose \(x_{i},\in\text{An}(x_{j})\). Note, \(x_{i}^{M},x_{j}^{M}\) are the result of projecting mutual ancestors \(M\) out of \(x_{i},x_{j}\). Therefore, \(x_{i}^{M},x_{j}^{M}\) admit the following representation:

\[x_{i}^{M} =\sum_{x_{m}\in\overline{M}\cap\text{An}(x_{i})}\alpha_{im} \varepsilon_{m}+\varepsilon_{i}\] \[x_{j}^{M} =\alpha_{ji}\varepsilon_{i}+\sum_{x_{m}\in\overline{M}\cap\text{ An}(x_{j})}\alpha_{jm}\varepsilon_{m}+\varepsilon_{j}\]

Note that \(\overline{M}\cap\text{An}(x_{i})\cap(\overline{M}\cap\text{An}(x_{j})\cup x_ {j})=\emptyset\). Therefore, \(r_{i}^{j}\perp\!\!\!\perp x_{i}^{M}\). Note that \(\varepsilon_{i}\in\text{both}\ x_{i}^{M},x_{j}^{M}\), therefore \(r_{j}^{i}\not\perp\!\!\!\perp x_{j}^{M}\).

We now show the reverse direction. Suppose condition 1) holds, i.e., \(x_{i}\not\perp\!\!\!\perp x_{j}\) and WLOG \(r_{i}^{j}\perp\!\!\!\perp x_{i}^{M},r_{j}^{i}\not\perp\!\!\!\perp x_{j}^{M}\). Suppose for contradiction that \(x_{i},x_{j}\) not in AP4 relation. They cannot be in AP1 relation because \(x_{i}\not\perp\!\!\!\perp x_{j}\). They cannot be in AP2 relation because \(x_{j}^{M}\not\perp\!\!\!\perp r_{j}^{i}\). By assumption they are not in AP3 relation. Therefore, we have a contradiction, therefore they are in AP4 relation. 

### Ancestor Sort

```
1:input: set of ancestral relations \(\mathrm{ARS}\).
2:initialize: hierarchical topological order \(O\)[], remaining variables \(\mathrm{RV}\)[\(x_{1},\dots,x_{d}\)], layer \(\mathrm{L}\)[].
3:while\(\mathrm{RV}\) is not empty do
4:Stage 1:Determine Roots
5:for\(x_{i}\in RV\)do
6:if\(x_{i}\) has no ancestors \(\in\mathrm{RV}\)then
7: Append \(x_{i}\) to \(L\).
8:Stage 2:Update Sort and Remaining Variables
9: Append \(L\) to \(O\).
10: Remove all vertices in \(L\) from \(RV\).
11:return\(O\) ```

**Algorithm 4****AS**: Ancestor Sort

OverviewIn each iteration, this algorithm uses the set of ancestral relations to identify which vertices have no ancestors amongst the vertices that have yet to be sorted. It peels those nodes off, adding them as a layer to the hierarchical topological sort. First the roots are peeled off, then the next layer, and so on.

Proof of Correctness

Proof.: The input to the algorithm is an ancestor table \(ARS\) cataloging all ancestral relations for every pair of nodes in an unknown DAG \(G\). Let \(H\) be the minimum number of layers (\(H-1\) is the length of the longest directed path in the graph) necessary in a valid hierarchical topological sort of \(G\).

**Base Cases (1,2)**

1. \(H=1\). None of the nodes have ancestors, so all nodes are added to layer 1 in Stage 1.
2. \(H=2\). Nodes with no ancestors are added to layer 1 in Stage 1, and then removed from the remaining variables in Stage 2. As \(H=2\), the longest directed path is 1, so all nodes in the remaining variables must have no ancestors in the remaining variables (otherwise, this would make the longest directed path 2). The nodes are added to layer 2 in Stage 1, and removed from the remaining variables in Stage 2. Remaining variables is empty, so now the order is correctly returned.

Inductive AssumptionFor any graph \(G\) with \(H<k\), Hierarchical Topological Sort will return a minimal hierarchical topological ordering.

We now show that hierarchical topological sort now yields a minimal hierarchical topological ordering for \(G\) where \(H=k\).

1. In the first iteration of Stage 1, nodes with no ancestors will be added to layer 1, then removed from the remaining variables in Stage 2.
2. At this point, note that we can consider the induced subgraph formed by the nodes left in remaining, \(G^{\prime}\). The minimal hierarchical topological ordering that represents \(G^{\prime}\) must have \(k-1\) layers. It cannot be greater than \(k-1\), because \(G^{\prime}\) is a subgraph of \(G\) and we removed nodes in layer 1 of, reducing the maximal path length by 1. It cannot be less than \(k-1\), because that would imply that \(k\) was not the minimal number of layers needed to represent \(G\).
3. By Inductive Assumption, Hierarchical Topological sort will return the a correct minimal hierarchical topological ordering for the induced subgraph \(G^{\prime}\).
4. Appending the layer 1 and the sort produced by \(G^{\prime}\) yields the full hierarchical topological sort for \(G\).

Inductive assumption is satisfied for \(H=k\), so for a graph \(G\) with arbitrary \(H\), the algorithm recovers the correct hierarchical topological sort.

### Proof of Correctness for Linear Hierarchical Topological Sort

**Theorem A.2**.: _3.7 Given a graph \(G\), Algorithm 1 asymptotically finds a correct hierarchical sort of \(G\)._

Proof.: The goal is to show that all ancestral relations between distinct pair of nodes \(x_{i},x_{j}\in V\) determined by LHTS in Stages 1, 2, and 3 are correct. By A.8, Stage 4 will return a valid hierarchical topological sort given fully identified ancestral relations.

Stage 1 identifies \(x_{i},x_{j}\) as in AP1 relaton if and only if \(x_{i}\perp\!\!\!\perp x_{j}\): it follows by Lemma 3.3 that vertices in AP1 relation are correctly identified, and vertices in AP2, AP3 and AP4 relation are not identified.

Stage 2 identifies \(x_{i},x_{j}\) unidentified in Stage 1 as in AP3 relation if and only if after pairwise regression, one of the residuals is dependent, and the other is independent: it follows by Lemma 3.5 vertices in AP3 relation are correctly identified, and vertices in AP2 or AP4 relation are not identified.

Consider a hierarchical topological sort of DAG \(G\)\(\pi_{H}\), with \(h\) layers. Note, Layer 1 of \(\pi_{H}\) consists of root nodes. Ancestral relations between root nodes and all other nodes are of type AP1 and AP3, and were discovered in Stage 1 and Stage 2.

We induct on layers to show that Stage 3 recovers all ancestral relations of type AP2 and AP4, one layer at a time in each iteration.

**Base Iterations (1,2)**

1. All ancestral relationships are known for nodes in layer 1 of \(\pi_{H}\), therefore all mutual ancestors of layer 2 and every lower layer are known. Then, by Lemma 3.4 and Lemma 3.6 ancestral relations of type AP2 and AP3 between vertices in layer 2 and all lower layers are discovered in iteration 1.
2. All ancestral relationships are known for nodes in layer 1 and 2 of \(\pi_{H}\), therefore all mutual ancestors of layer 3 and every lower layer are known. Then, by Lemma 3.4 and Lemma 3.6 ancestral relations of type AP2 and AP3 between vertices in layer 2 and all lower layers are discovered in iteration 2.

Iteration \(k-1\), Inductive AssumptionWe have recovered all ancestral relationships of nodes in layers \(0\) to \(k-1\).

1. As ancestral relationships of nodes in layers \(0\) to \(k-1\) are known, mutual ancestors of vertices in layer \(k\) and every lower layer are known, so by Lemma 3.4 and 3.4 all ancestral relations of type AP2 and AP4 between vertices in layer \(k\) and every lower layer are discovered.

Iteration \(k-1\) Inductive Assumption is satisfied for iteration \(k\), therefore we recover all ancestral relations of type AP2 and AP4 and 4) for nodes in layers \(0\) to \(k\). Thus, for a DAG with an arbitrary number of layers, Stage 3 recovers all ancestral relations of type AP2 and AP4.

### Time Complexity Proof for Linear Hierarchical Topological Sort

Proof.: In Stage 1, LHTS runs \(O(d^{2})\) marginal independence tests that each have \(O(n^{2})\) complexity. In each step for Stage 2 and Stage 3, LHTS runs \(O(d^{2})\) marginal independence tests each with \(O(n^{2})\) complexity. In the worse case of a fully connected DAG, there are \(d^{2}\) steps in total, across Stage 2 and Stage 3: this is because in each step one layer of the layered sort DAG is identified, and a fully connected DAG has \(d\) layers. Therefore, the overall sample complexity of LHTS is \(O(d^{3}n^{2})\). 

### Walk-through of LHTS

The following diagram illustrates each stage of LHTS on an exemplary 5-node DAG:

All vertices are dependent on each other, so no AP1 relations are discovered in Stage 1. In Stage 2, vertex A is discovered in AP2 relation to all vertices, and vertex D is discovered in AP2 relation to vertex E. In Stage 3, vertices B and C are discovered to be in AP3 relation to each other; then, we discover vertex B in AP4 relation to vertex D, and vertex C in AP4 relation to vertex D. Therefore, in Stage 4, we recover the topological sort after running the AS subroutine.

### Proof of Lemma 3.9: Root Active Causal Ancestral Path Relations

**Lemma 3.9**.: _A vertex is a root vertex if and only if it has AP1 or AP3 relations with all other vertices._

Proof of Lemma 3.9.: We first show the forward direction. If \(x_{i}\) is a root, then \(\text{De}(x_{i})=\emptyset\). Therefore, it cannot have a backdoor path between it and any other vertex, therefore it must be in either \(AP1\) or \(AP3\) relation with other vertices.

We now show the reverse direction. If \(x_{i}\) is in AP1 or AP3 (as an ancestor) relations with all other nodes, it cannot have any parents. Therefore, \(x_{i}\) is a root.

Assumptions, Proofs, and Walkthrough for NHTS

### Identifiability in Nonlinear Setting

Following the style of [19], we first observe that the following condition guarantees that the observed distribution of a pair of variables \(x_{i},x_{j}\) can only be generated by a unique ANM:

**Condition B.1** (Hoyer & Hyttinen [9]).: _Given a bivariate model \(x_{i}=\varepsilon_{i},x_{j}=f_{j}(x_{i})+\varepsilon_{j}\) generated according to (1), we call the SEM an identifiable bivariate ANM if the triple \((f_{i},p_{\varepsilon_{i}},p_{\varepsilon_{j}})\) does not solve the differential equation \(k^{\prime\prime\prime}=k^{\prime\prime}(-\frac{g^{\prime\prime\prime}f^{ \prime}}{g^{\prime\prime}}+\frac{f^{\prime\prime}}{f^{\prime}})-2g^{\prime \prime}f^{\prime\prime}f^{\prime}+g^{\prime}f^{\prime\prime\prime}+\frac{g^{ \prime}g^{\prime\prime\prime}f^{\prime}}{g^{\prime\prime}}-\frac{g^{\prime}(f^ {\prime\prime})^{2}}{f^{\prime}}\) for all \(x_{i},x_{j}\) such that \(f^{\prime}(x_{i})g^{\prime\prime}(x_{j}-f_{j}(x_{i}))\neq 0\), where \(p_{\varepsilon_{i}},p_{\varepsilon_{j}}\) are the density of \(\varepsilon_{i},\varepsilon_{j}\), \(f=f_{j},k=\log p_{\varepsilon_{i}},g=p_{\varepsilon_{j}}\). The arguments \(x_{j}-f_{j}(x_{i}),x_{i}\) and \(x_{i}\) of \(g,k\) and \(f\) respectively, are removed for readability._

There is a generalization of this condition to the multivariate nonlinear ANM proved by [25]:

**Theorem B.2**.: _(Peters et al. [25]). An ANM corresponding to DAG \(G\) is identifiable if \(\forall x_{j}\in V,x_{i}\in\text{Pa}(x_{j})\) and all sets \(S\subseteq V\) with \(\text{Pa}(x_{j})\setminus\{i\}\subseteq S\subseteq\overline{\text{De}(j)} \setminus\{x_{i},x_{j}\}\), \(\exists\ X_{S}\) with positive joint density such that the triple \(\Big{(}f_{j}(\text{Pa}(j)\setminus\{x_{i}\},x_{i}),p_{x_{i}|X_{s}},p_{ \varepsilon_{j}}\Big{)}\) satisfies Condition B.1, and \(f_{j}\) are non-constant in all arguments._

We assume that the identifiability conditions described in Theorem B.2 hold throughout Section 4.

### Proof of Lemma 4.1

**Lemma 4.1** (Active Causal Parental Path Relation Enumeration).: _Let \(x_{i},x_{j}\in V\) be a pair of distinct nodes, where \(x_{i}\) is one of the potential parents of \(x_{j}\). Let \(C=\text{PA}(x_{j})\setminus x_{i}\). There are in total four possible types of active causal parental path relations between \(x_{i}\) and \(x_{j}\) with respect to \(C\): PP1) \(x_{i}\) and \(x_{j}\) are not directly causally related, and no active path exists between \(x_{i}\) and \(C\); PP2) \(x_{i}\) directly causes \(x_{j}\) (\(x_{i}\to x_{j}\)), and no active path exists between \(x_{i}\) and \(C\); PP3) \(x_{i}\) directly causes \(x_{j}\) (\(x_{i}\to x_{j}\)), and there exists an active path between \(x_{i}\) and \(C\); PP4) \(x_{i}\) and \(x_{j}\) are not directly causally related, and there exists an active path between \(x_{i}\) and \(C\)._

Proof.: Either \(x_{i}\mathrel{\hbox to 0.0pt{\hbox{\kern 2.0pt\perp}}\raisebox{-3.0pt}{\hbox{ \kern-2.0pt\hbox{\kern 2.0pt\perp}}}}x_{j}\) or \(x_{i}\mathrel{\hbox to 0.0pt{\hbox{\kern 2.0pt\perp}}\raisebox{-3.0pt}{\hbox{ \kern-2.0pt\hbox{\kern 2.0pt\perp}}}}x_{j}\): if the former, then \(x_{i},x_{j}\) are in PP1 relation. Suppose the latter is true. Then, either \(x_{i}\mathrel{\hbox to 0.0pt{\hbox{\kern 2.0pt\perp}}\raisebox{-3.0pt}{\hbox{ \kern-2.0pt\hbox{\kern 2.0pt\perp}}}}C\) or \(x_{i}\mathrel{\hbox to 0.0pt{\hbox{\kern 2.0pt\perp}}\raisebox{-3.0pt}{\hbox{ \kern-2.0pt\hbox{\kern 2.0pt\perp}}}}C\): if the former, then \(x_{i},x_{j}\) are in PP2 relation. Suppose the latter is true. Then, either \(x_{i}\in\text{PA}(x_{j})\) or \(x_{i}\not\in\text{PA}(x_{j})\). If the former, then \(x_{i},x_{j}\) are in PP3 relation, and if the latter, then \(x_{i},x_{j}\) are in PP4 relation. 

### Proof of Lemma 4.2

**Lemma 4.2** (Non-PP1).: _Vertices \(x_{i},x_{j}\in V\) are not in PP1 relation if and only if \(x_{i}\mathrel{\hbox to 0.0pt{\hbox{\kern 2.0pt\perp}}\raisebox{-3.0pt}{\hbox{ \kern-2.0pt\hbox{\kern 2.0pt\perp}}}}x_{j}\)._

Proof.: We first show the forward direction. If \(x_{i},x_{j}\) are not in PP1 relation, then either \(x_{i}\in\text{Pa}(x_{j})\) or there exists an active causal path between \(x_{i},x_{j}\); therefore, \(x_{i}\mathrel{\hbox to 0.0pt{\hbox{\kern 2.0pt\perp}}\raisebox{-3.0pt}{\hbox{ \kern-2.0pt\hbox{\kern 2.0pt\perp}}}}x_{j}\).

We now show the reverse direction. If \(x_{i}\mathrel{\hbox to 0.0pt{\hbox{\kern 2.0pt\perp}}\raisebox{-3.0pt}{\hbox{ \kern-2.0pt\hbox{\kern 2.0pt\perp}}}}x_{j}\) there exists an active causal path between \(x_{i},x_{j}\), therefore they cannot be in PP1 relation. 

### Proof of Lemma 4.3

**Lemma 4.3** (Pp2).: _Let \(x_{i},x_{j}\in V\), \(P_{ij}=\{x_{k}\in V:x_{k}\mathrel{\hbox to 0.0pt{\hbox{\kern 2.0pt\perp}} \raisebox{-3.0pt}{\hbox{\kern -2.0pt\hbox{\kern 2.0pt\perp}}}}x_{i},x_{k}\mathrel{\hbox to 0.0pt{\hbox{\kern 2.0pt\perp}} }x_{j}\}\), \(r_{i}^{j}\) be the residual of \(x_{j}\) nonparametrically regressed on \(x_{i}\), and \(r_{i,P}^{j}\) be the residual of \(x_{j}\) nonparametrically regressed on \(x_{i}\) and all \(x_{k}\in P_{ij}\). Suppose \(x_{i}\) and \(x_{j}\) are not in PP1 relation. Then, \(x_{i}\) and \(x_{j}\) are in PP2 relation if and only if one of the following holds: 1) \(x_{i}\mathrel{\hbox to 0.0pt{\hbox{\kern 2.0pt\perp}}\raisebox{-3.0pt}{\hbox{ \kern-2.0pt\hbox{\kern 2.0pt\perp}}}}r_{i}^{j}\) or 2) \(x_{i}\mathrel{\hbox to 0.0pt{\hbox{\kern 2.0pt\perp}}\raisebox{-3.0pt}{\hbox{ \kern-2.0pt\hbox{\kern 2.0pt\perp}}}}r_{i,P}^{j}\)._

Proof.: Note, there exists two sub cases of PP2 relation: PP2a) \(x_{i}\) is the only parent of \(x_{j}\), i.e. \(|C|=0\) and PP2b) \(x_{i}\) is not the only parent of \(x_{j}\), i.e., \(|C|>0\).

[MISSING_PAGE_EMPTY:22]

Proof.: This proof can be broken into two parts: we first show 1) all root vertices are identified after Stage 3, then we show 2) that Stage 4 recovers a correct hierarchical topological sort given the root vertices.

Note that Stage 1 identifies \(x_{i},x_{j}\) as not in PP1 relation if and only if \(x_{i}\not\perp x_{j}\): it follows by Lemma 4.2 that these identifications are correct.

Note that Stage 2 identifies the set \(x_{i}\in W\) if and only if \(\exists x_{j}\) such that either 1) \(x_{i}\perp\!\!\!\perp r_{i}^{j}\) or 2) \(x_{i}\perp\!\!\!\perp r_{i,P}^{j}\): it follows by Lemma 4.3 that these identifications are correct.

Note that Stage 3 explicitly uses a condition from Lemma 4.4 to find all non-isolated root vertices - it is therefore correct. Note that all isolated root vertices were identified in Stage 1. Therefore, all roots are identified.

Consider a hierarchical sort of DAG \(G\)\(\pi_{L}\): suppose the maximal directed path in \(G\) has size \(h\). Note that layer 0 of \(\pi_{L}\) consists of root nodes, and therefore has been identified. We note the following **observation**: when \(x_{i}\) is nonparametrically regressed on all \(x_{k}\in\pi_{L}\) to obtain residual \(r_{i}\), if \(\exists\) an ancestor of \(x_{i}\) not in \(\pi_{L}\), then \(r_{i}\) will be dependent on at least one \(x_{i}\in\pi_{L}:\) due to omitted variable bias [23].

We now induct on the layers of \(\pi_{L}\) to show that Stage 4 correctly recovers all layers.

Base Iterations (1,2) 1. All roots are identified, so layer 0 of \(\pi_{L}\) is identified. 2. By **observation**, only \(x_{i}\) in layer 1 of \(\pi_{L}\) will have independent residuals \(r_{i}\) after nonparametric regression on \(x_{i}\in\pi_{L}\), so layer 1 is correctly identified.

Iteration \(k-1\), Inductive AssumptionWe have recovered all layers of \(\pi_{L}\) up to layer \(k-1\).

1. By **observation**, only \(x_{i}\) in layer \(k\) of \(\pi_{L}\) will have independent residuals \(r_{i}\) after nonparametric regression on \(x_{i}\in\pi_{L}\), so layer \(k\) is correctly identified.

Iteration \(k-1\) Inductive Assumption is satisfied for iteration \(k\), therefore we recover all layers of \(\pi_{L}\) from 0 to \(k\). Thus, for a DAG with an arbitrary number of layers, Stage 4 recovers the full hierarchical sort. 

### Time Complexity for NHTS

**Theorem 4.6**.: _Given \(n\) samples of \(d\) vertices generated by a identifiable nonlinear ANM, the worst case runtime complexity of Algorithm 2 is upper bounded by \(O(d^{3}n^{3})\)._

Proof.: In Stage 1, NHTS runs \(O(d^{2})\) marginal independence tests that each have \(O(n^{3})\) complexity. In Stage 2, NHTS runs \(O(d^{2})\) nonparametric regressions and \(O(d^{2})\) marginal independence tests, each of which have \(O(n^{3})\) complexity. In Stage 3 NHTS runs at most \(O(d^{2})\) conditional independence tests, each of which has \(O(n^{3})\) complexity. In the worst case of a fully connected DAG, NHTS goes through \(O(d)\) steps in Stage 4: in each step of Stage 4, NHTS runs \(O(d)\) nonparametric regressions and \(O(d^{2})\) marginal independence tests, each of which has \(O(n^{3})\) complexity. Therefore, the overall sample complexity of NHTS is \(O(d^{3}n^{3})\). 

### Walk-through of NHTS

The following diagram illustrates each stage of NHTS on an exemplary 5-node DAG:

In Stage 1, we discover that none of the vertices are in PP1 relations. In Stage 2, we discover that vertex A is in PP2 relation to vertices B and C, and vertex D is in PP2 relation to vertex E: therefore, A and D are our potential roots. In Stage 3, we find that vertex D d-separates its child, vertex E, from vertex A: therefore A is the root vertex. In Stage 4, we regress the unsorted vertices onto A, finding that B,C are independent of the residual in the first round, then D, then E in the last round.

### Time Complexity of NHTS vs RESIT, NoGAM

**Theorem 4.7**.: _Consider a fully connected DAG \(G=(V,E)\) with nonlinear ANM. Let \(d:=|V|\). Let \(n_{k}^{\mathrm{NHTS}}\) be the number of nonparametric regressions with covariate set size \(k\in[d-2]\) run by NHTS when sorting \(V\); we similarly define \(n_{k}^{\mathrm{RESIT}}\) and \(n_{k}^{\mathrm{NoGAM}}\) respectively. Then, \(n_{k}^{\mathrm{NHTS}}=d-k\), and \(n_{k}^{\mathrm{RESIT}}=n_{k}^{\mathrm{NoGAM}}=k+1\). This implies that for all \(k>\frac{d}{2}\), \(n_{k}^{\mathrm{NHTS}}<n_{k}^{\mathrm{RESIT}}=n_{k}^{\mathrm{NoGAM}}\)._

Proof.: RESIT and NoGAM both identify leaf vertices in an iterative fashion, regressing each unsorted vertex on the rest of the unsorted vertices; RESIT tests the residual for independence with the covariate set while NoGAM uses the residual for score matching. Therefore, the number of regressions run in both methods in each step equals one plus the covariate set size. Therefore, when the covariate set size is \(k>\frac{d}{2}\), there are \(k+1\) regressions run.

In the case of a fully directed graph, the first stage of NHTS only runs pairwise regressions with empty conditioning sets. After the first stage, NHTS regresses each unsorted vertex onto all sorted vertices, finding vertices with independent residuals. Therefore, the number of regressions run is equal to \(d\) minus the size of the covariate set. Therefore, when the covariate set size is \(k>\frac{d}{2}\), there are \(d-k\) regressions run.

Proofs and Walkthrough for ED

### Proof of Lemma

**Lemma 5.1** (Parent Discovery).: _Let \(\pi\) be a topological ordering, \(x_{i},x_{j}\) such that \(\pi(x_{i})<\pi(x_{j})\). Let \(Z_{ij}=C_{ij}\cup M_{ij}\), where \(C_{ij}=\{x_{k}:x_{k}\in\text{Pa}(x_{i}),x_{k}\not\perp x_{j}\},M_{ij}=\{x_{k}:x _{k}\in\text{Pa}(x_{j}),\pi(x_{i})<\pi(x_{k})<\pi(x_{j})\}\). Then, \(x_{i}\to x_{j}\iff x_{i}\not\perp x_{j}|Z_{ij}\)._

Proof.: Note, \(C_{i,j}\) blocks all backdoor paths between \(x_{i},x_{j}\) and \(M_{i,j}\) blocks all frontdoor paths between \(x_{i},x_{j}\), except the direct edge.

We first show the forward direction. If \(x_{i}\not\perp x_{j}|Z_{i,j}\), there must be an direct edge between \(x_{i},x_{j}\), and as \(k(i)<k(j)\), we have \(x_{i}\to x_{j}\).

We now show the reverse direction. If \(x_{i}\to x_{j}\), then there does not exist a conditioning set that makes \(x_{i}\perp\!\!\!\perp x_{j}\), which implies \(x_{i}\not\perp x_{j}|Z_{i,j}\). 

### Hierarchical Version of Edge Discovery

The current implementation of Edge Discovery takes a linear hierarchical topological sort as input; this is equivalent to a hierarchical topological sort where every layer contains only one vertex. To generalize ED to a hierarchical sort, we simply adjust how the algorithm loops over the sort, and which vertices are included in which conditioning sets \(C_{ij},M_{ij}\). We give an example of how the latter would change: suppose the algorithm is checking whether \(x_{i}\in\text{Pa}(x_{j})\) where \(x_{i}\) is in layer 2 (\(\pi_{L}(x_{i})=2\)) and \(x_{j}\) is layer 6 (\(\pi_{L}(x_{j})=6\)). \(M_{ij}\) would be equal to the vertices who are parents of \(x_{j}\) that are in layers that are between layer 2 and 6: i.e. \(M_{ij}\) equals \(x_{k}\in\text{Pa}(x_{j})\) such that \(2<\pi_{L}(x_{k})<6\). \(C_{ij}\), would stay the same, just being equal to \(\text{Pa}(x_{i})\). Now, to generalize the looping part, notice that the linear version of ED loops over indices of the linear topological sort. The hierarchical version of Edge Discovery would loop over and within layers of the topological sort. We give an example of this: suppose the algorithm has found all parents of vertices in layer 4 or lower: the next step would be to find the parents of vertices in layer 5. The algorithm sets any vertex in layer 5 as \(x_{j}\), and first finds all parents of \(x_{j}\) in the immediately preceding layer, layer 4. The algorithm then finds all parents of \(x_{j}\) in the layer preceding layer 4, layer 3. This is essentially the same approach as the linear version, except the size of the conditioning sets and number of conditional tests run is far lower, as the layers provide extra knowledge, limiting which variables can be parents or confounders of other variables.

### Edge Discovery

We first provide a walk-through of ED on an exemplary 5-node DAG:We first use a topological ordering algorithm to obtain a sort from the DAG: the sort corresponds to a fully connected DAG, from which we will prune spurious edges. In the first two iterations we find that \(A\to B\), but \(B\not\to C\), as \(B\perp\!\!\!\perp C|A\). In the third iteration we find that \(B\to D,C\to D\), but \(A\not\to D\), as \(A\perp\!\!\!\perp D|B,C\). In the final iteration, we find that \(D\to E\), but \(A\not\to E,B\not\to E,C\not\to E\), given \(A\perp\!\!\!\perp E|D,B\perp\!\!\!\perp E|D,C\perp\!\!\!\perp E|D\). Therefore, we recover the correct set causal edges, removing all spurious edges. We now present a statement of correctness and proof for ED:

**Theorem 5.2**.: _Given a correct topological ordering of \(G\), Algorithm 3 asymptotically finds correct parent sets \(\mbox{{PA}}(x_{i}),\forall x_{i}\in G\)._

Proof of Correctness

BackgroundWe are given a linear topological sort \(\pi(V)=[x_{1},x_{2},\ldots,x_{d}]\) (where \(x_{1}\) has no parents). For \(x_{a}\) that appears before \(x_{b}\), let \(Z_{a,b}=C_{a,b}\cup M_{a,b}\), where \(C_{a,b}\) is the set of potential confounders of \(x_{a},x_{b}\), and \(M_{a,b}\) is the set of known mediators of \(x_{a},x_{b}\) (defined precisely in Lemma 5.1). Let \(x\perp\!\!\!\perp y|\cdot\) be the value of a conditional independence test between \(x\) and \(y\).

Proof.: **Base Case Iterations (1,2,3,4)**

1. Finding parents of \(x_{1}\): by the topological sort, \(x_{1}\) has no parents.
2. Finding parents of \(x_{2}\): As \(x_{1}\) is the only possible parent of \(x_{2}\), there are no possible confounders or mediators between \(x_{1}\) and \(x_{2}\), so we initialize \(Z_{1,2}=\emptyset\), then by \(L3\)\(x_{1}\not\perp\!\!\!\perp x_{2}|Z_{1,2}=x_{1}\not\perp\!\!\!\perp x_{2}\iff x_{1} \to x_{2}\). Thus, we recover all possible edges between \([x_{1},x_{2}]\).
3. Finding parents of \(x_{3}\): first we check whether \(x_{2}\to x_{3}\), then we check whether \(x_{1}\to x_{3}\). From iteration 2, we know all edges between \([x_{1},x_{2}]\). **Case 1**: if \(x_{1}\to x_{2}\), then it is a possible confounder. Therefore, we initialize \(C_{2,3}=x_{1}\). By topological sort there is no mediator between \(x_{2}\) and \(x_{3}\), therefore we initialize \(M_{2,3}=\emptyset\). Then by Lemma 5.1\(x_{2}\not\perp\!\!\!\perp x_{3}|Z_{2,3}=x_{2}\not\perp\!\!\!\perp x_{3}|x_{1} \iff x_{2}\to x_{3}\). If an edge between \(x_{2}\) and \(x_{3}\) exists, initialize \(M_{1,3}\) accordingly. Note, \(x_{1}\) has no parents, so we initialize \(C_{1,3}=\emptyset\). Then, by Lemma 5.1\(x_{1}\not\perp\!\!\!\perp x_{3}|Z_{1,3}\iff x_{1}\to x_{3}\). We recover all possible edges between \([x_{1},x_{2},x_{3}]\). **Case 2**: if \(x_{1}\not\to x_{2}\), there are no possible confounders between \(x_{2}\) and \(x_{3}\). Therefore, we initialize \(C_{2,3}=\emptyset\). By topological sort there is no mediator between \(x_{2}\) and \(x_{3}\), therefore we initialize \(M_{2,3}=\emptyset\). Then by Lemma 5.1\(x_{2}\not\perp\!\!\!\perp x_{3}|Z_{2,3}=x_{2}\not\perp\!\!\!\perp x_{3}\iff x_{2} \to x_{3}\). Note, \(x_{1}\) has no parents, so we initialize \(C_{1,3}=\emptyset\). Note, as \(x_{1}\not\to x_{2}\), there are no possible mediators between \(x_{1}\) and \(x_{3}\): we initialize \(M_{1,3}=\emptyset\). Then, by Lemma 5.1\(x_{1}\not\perp\!\!\!\perp x_{3}|Z_{1,3}=x_{1}\not\perp\!\!\!\perp x_{3}\iff x_{1} \to x_{3}\). We recover all possible edges between \([x_{1},x_{2},x_{3}]\).
4. Finding parents of \(x_{4}\): first we check whether \(x_{3}\to x_{4}\), then whether \(x_{2}\to x_{4}\), then whether \(x_{1}\to x_{4}\). From iteration 3, we know all edges between \([x_{1},x_{2},x_{3}]\). **Case 1**: if \([x_{1},x_{2},x_{3}]\) has no edges, then no node causes \(x_{3}\) directly or indirectly, therefore we initialize \(C_{3,4}=\emptyset\). There are no possible mediators between \(x_{3}\) and \(x_{4}\), so \(M_{3,4}=\emptyset\). Therefore, by Lemma 5.1\(x_{3}\not\perp\!\!\!\perp x_{4}|Z_{3,4}=x_{3}\not\perp\!\!\!\perp x_{4}\iff x_{3} \to x_{4}\). As \([x_{1},x_{2},x_{3}]\) has no edges, no node causes \(x_{2}\) directly or indirectly, therefore we initialize \(C_{2,4}=\emptyset\). Note, \(x_{2}\not\to x_{3}\), so there are no possible mediators between \(x_{2}\) and \(x_{4}\), so we initialize \(M_{2,4}=\emptyset\). Then, by Lemma 5.1\(x_{2}\not\perp\!\!\!\perp x_{4}|Z_{2,4}=x_{2}\not\perp\!\!\!\perp x_{4}\iff x_{2} \to x_{4}\). Note, \(x_{1}\) has no parents, so we initialize \(C_{1,4}=\emptyset\). As, \(x_{1}\) does not cause \(x_{2}\) or \(x_{3}\) there are no possible mediators between \(x_{1}\) and \(x_{4}\), therefore we initialize \(M_{1,4}=\emptyset\). Then, by Lemma 5.1\(x_{1}\not\perp\!\!\!\perp x_{4}|Z_{1,4}=x_{1}\not\perp\!\!\!\perp x_{4}\iff x_{1} \to x_{4}\). We recover all possible edges between \([x_{1},x_{2},x_{3},x_{4}]\). **Case 2**: if \(x_{1}\to x_{2}\) is the only edge between the nodes \([x_{1},x_{2},x_{3}]\), then no node causes \(x_{3}\) directly or indirectly, therefore \(C_{3,4}=\emptyset\). By topological sort there is no mediator between \(x_{3},x_{4}\), so we initialize \(M_{3,4}=\emptyset\). Then, by Lemma 5.1\(x_{3}\not\perp\!\!\!\perp x_{4}|Z_{3,4}=x_{3}\not\perp\!\!\!\perp x_{4}\iff x_{3} \to x_{4}\). As \(x_{1}\to x_{2}\), we initialize \(C_{2,4}=x_{1}\). As \(x_{2}\not\to x_{3}\), there are no possible mediators between \(x_{2}\) and \(x_{4}\), so we initialize \(M_{2,4}=\emptyset\). Then, by Lemma 5.1\(x_{2}\not\perp\!\!\!\perp x_{4}|Z_{2,4}=x_{2}\not\perp\!\!\!\perp x_{4}|x_{1} \iff x_{2}\to x_{4}\). If an edge exists between \(x_{2}\) and \(x_{4}\), we initialize \(M_{1,4}\) accordingly. As \(x_{1}\) has no parents, we initialize \(C_{1,4}=\emptyset\). Then, by Lemma 5.1\(x_{1}\not\perp\!\!\!\perp x_{4}|Z_{1,4}\iff x_{1}\to x_{4}\). We recover all possible edges between \([x_{1},x_{2},x_{3},x_{4}]\).

**Case 3**: if \(x_{1}\to x_{3}\) is the only edge between the nodes \([x_{1},x_{2},x_{3}]\), then \(x_{1}\) is the only potential confounder of \(x_{3}\) and \(x_{4}\) so we initialize \(C_{3,4}=x_{1}\). There are no possible mediators between \(x_{3}\) and \(x_{4}\), so we initialize \(M_{3,4}=\emptyset\). Then, by Lemma 5.1\(x_{3}\not\perp\!\!\!\perp x_{4}|Z_{3,4}=x_{3}\not\perp\!\!\!\perp x_{4}|x_{1} \iff x_{3}\to x_{4}\). Note, \(x_{2}\not\to x_{3}\), so we initialize \(M_{2,4}=\emptyset\). Then, by Lemma 5.1\(x_{2}\not\perp\!\!\!\perp x_{4}|Z_{2,4}=x_{2}\not\perp\!\!\!\perp x_{4}\iff x_{2} \to x_{4}\). If an edge exists between \(x_{3}\) and \(x_{4}\), we initialize \(M_{1,4}\) accordingly. As \(x_{1}\) has no parents, we initialize \(C_{1,4}=\emptyset\). Then, by Lemma 5.1\(x_{1}\not\perp\!\!\!\perp x_{4}|Z_{1,4}\iff x_{1}\to x_{4}\). We recover all possible edges between \([x_{1},x_{2},x_{3},x_{4}]\).

**Case 4**: if \(x_{1}\to x_{2},x_{2}\to x_{3}\) are the only edges between nodes \([x_{1},x_{2},x_{3}]\), then \(x_{1}\) and \(x_{2}\) cause \(x_{3}\) either indirectly or directly. Therefore, we initialize \(C_{3,4}=\{x_{1},x_{2}\}\). There are no possible mediators between \(x_{3}\) and \(x_{4}\), so we initialize \(M_{3,4}=\emptyset\). Then, by Lemma 5.1\(x_{3}\not\perp\!\!\!\perp x_{4}|Z_{3,4}=x_{3}\not\perp\!\!\!\perp x_{4}|x_{1},x_{2}\iff x_{3}\to x_{4}\). If an edge exists between \(x_{3}\) and \(x_{4}\), we initialize \(M_{2,4}\) accordingly. Note, \(x_{1}\) is a parent of \(x_{2}\), so we initialize \(C_{2,4}=x_{1}\). Then, by Lemma 5.1\(x_{2}\not\perp\!\!\!\perp x_{4}|Z_{2,4}\iff x_{2}\to x_{4}\). If an edge exists between \(x_{3}\) and \(x_{4}\), and/or between \(x_{2}\) and \(x_{4}\), initialize \(M_{1,4}\) accordingly. Note, \(x_{1}\) has no parents, so initialize \(C_{1,4}=\emptyset\). Then by Lemma 5.1\(x_{1}\not\perp\!\!\!\perp x_{4}|Z_{1,4}\iff x_{1}\to x_{4}\). We recover all possible edges between \([x_{1},x_{2},x_{3},x_{4}]\).

**Case 5**: if \(x_{1}\to x_{3},x_{2}\to x_{3}\)are the only edges between nodes \([x_{1},x_{2},x_{3}]\), then \(x_{1}\) and \(x_{2}\) cause \(x_{3}\) directly. Therefore, we initialize \(C_{3,4}=\{x_{1},x_{2}\}\). There are no possible mediators between \(x_{3}\) and \(x_{4}\), so we initialize \(M_{3,4}=\emptyset\). Then, by \(L3\ x_{3}\not\perp\!\!\!\perp x_{4}|Z_{3,4}=x_{3}\not\perp\!\!\!\perp x_{4}|x_{1 },x_{2}\iff x_{3}\to x_{4}\). If an edge exists between \(x_{3}\) and \(x_{4}\), we initialize \(M_{2,4}\) accordingly. Note, \(x_{2}\) has no parents, so we initialize \(C_{2,4}=\emptyset\). Then,by \(L3\ x_{2}\not\perp\!\!\!\perp x_{4}|Z_{2,4}\iff x_{2}\to x_{4}\). If an edge exists between \(x_{3}\) and \(x_{4}\), initialize \(M_{1,4}\) accordingly. Note, \(x_{1}\) has no parents, so initialize \(C_{1,4}=\emptyset\). Then by \(L3\ x_{1}\not\perp\!\!\!\perp x_{4}|Z_{1,4}\iff x_{1}\to x_{4}\). We recover all possible edges between \([x_{1},x_{2},x_{3},x_{4}]\).

**Case 6**: if \(x_{1}\to x_{2},x_{1}\to x_{3},x_{2}\to x_{3}\) are the only edges between nodes \([x_{1},x_{2},x_{3}]\), then \(x_{1}\) and \(x_{2}\) cause \(x_{3}\) directly. Therefore, we initialize \(C_{3,4}=\{x_{1},x_{2}\}\). There are no possible mediators between \(x_{3}\) and \(x_{4}\), so we initialize \(M_{3,4}=\emptyset\). Then, by Lemma 5.1\(x_{3}\not\perp\!\!\!\perp x_{4}|Z_{3,4}=x_{3}\not\perp\!\!\!\perp x_{4}|x_{1 },x_{2}\iff x_{3}\to x_{4}\). If an edge exists between \(x_{3}\) and \(x_{4}\), we initialize \(M_{2,4}\) accordingly. Note, \(x_{1}\) is a parent of \(x_{2}\), so we initialize \(C_{2,4}=x_{1}\). Then, by Lemma 5.1\(x_{2}\not\perp\!\!\!\perp x_{4}|Z_{2,4}\iff x_{2}\to x_{4}\). If an edge exists between \(x_{3}\) and \(x_{4}\), and/or between \(x_{2}\) and \(x_{4}\), initialize \(M_{1,4}\) accordingly. Note, \(x_{1}\) has no parents, so initialize \(C_{1,4}=\emptyset\). Then by Lemma 5.1\(x_{1}\not\perp\!\!\!\perp x_{4}|Z_{1,4}\iff x_{1}\to x_{4}\). We recover all possible edges between \([x_{1},x_{2},x_{3},x_{4}]\).

5. Finding parents of \(x_{5}\): first we check whether \(x_{4}\to x_{5}\), then whether \(x_{3}\to x_{5}\), then whether \(x_{2}\to x_{5}\), then whether \(x_{1}\to x_{5}\).

\(\vdots\)

Iteration \(k-1\) Inductive AssumptionWe have recovered the edges between \([x_{1},\ldots,x_{k-1}]\).

Iteration \(k\)We now find all nodes in \([x_{1},\ldots,x_{k-1}]\) that cause \(x_{k}\) (which yields all edges between \([x_{1},\ldots,x_{k}]\)).

Base Case Sub-Iteration (1,2)

1. We first check whether \(x_{k-1}\to x_{k}\). We initialize the potential confounders \(C_{k-1,k}\) using\([x_{1},\ldots,x_{k-1}]\). The set of mediators \(M_{k-1,k}\) is empty by the topological sort. Then, by Lemma 5.1\(x_{k-1}\to x_{k}\iff x_{k-1}\not\perp x_{k}|Z_{k-1,k}\).

2. We now check whether \(x_{k-2}\to x_{k}\). We initialize the potential confounders \(C_{k-1,k}\) using\([x_{1},\ldots,x_{k-2}]\). Only \(x_{k-1}\) can be a mediator: we know whether \(x_{k-2}\) causes \(x_{k-1}\), and in our previous step we found whether \(x_{k-1}\) causes \(x_{k}\). Thus, we initialize \(M_{k-2,k}\) accordingly. Thus, by Lemma 5.1\(x_{k-2}\to x_{k}\iff x_{k-2}\not\perp x_{k}|Z_{k-2,k}\). \(\vdots\)

Sub-Iteration \(j\) Inductive AssumptionWe have recovered edges between \([x_{j},\ldots,x_{k}]\)

Sub-Iteration \(j-1\)We now find if \(x_{j-1}\) causes \(x_{k}\).

1. For node \(x_{j-1}\) where \(1\leq j-1<k\), we obtain \(C_{j-1,k}\) by Iteration \(k-1\) Inductive Assumption and \(M_{j-1,k}\) by Sub-Iteration \(j\) Inductive Assumption. Then, by Lemma 5.1\(x_{j-1}\to x_{k}\iff x_{j-1}\not\perp x_{k}|Z_{j-1,k}\).

Sub-Iteration \(j\) Inductive Assumption is satisfied for \(j-1\), therefore we recover all nodes in \([x_{1},\ldots,x_{k-1}]\) that cause \(x_{k}\). This satisfies Iteration \(k-1\) Inductive Assumption for \(k\), which means we recover all edges between \([x_{1},\ldots,x_{k}]\). Thus, for a topological sort of arbitrary length, the algorithm recovers all possible edges.

### Time Complexity for Edge Discovery

**Theorem 5.3**.: _Given \(n\) samples of \(d\) vertices generated by a model corresponding to a DAG \(G\), the runtime complexity of ED is upper bounded by \(O(d^{2}n^{3})\)._

Proof.: ED checks for the existence of every edge permitted by a topological sort \(\pi\) by running one conditional independence test that has complexity \(O(n^{3})\). In the worst case, there are \(O(d^{2})\) possible edges, so the overall complexity is \(O(d^{2}n^{3})\).

## Appendix D Additional Experiments and Runtimes

### Runtimes for Linear Topological Sort

### Runtimes for Nonlinear Topological Sort

### Topological Sorts on Nonlinear Data

In figure 12 we provide additional experiments for nonlinear topological sorts (\(d=10,n=300\)); we see that NHTS maintains superior performance even as the density of the underlying graph increases, although the performance gap decreases, especially for laplacian noise, as the graph becomes denser.

Figure 11: Runtimes for nonlinear topological sorts, see Figure 6.

Figure 12: Performance of NHTS on data generated with Gaussian, Laplace, or Uniform noise (left, middle, right columns), the average number of edges set to \(2d\), \(3d\), or \(4d\) (top, middle, bottom rows).

Figure 10: Runtimes for linear topological sorts, left: top row; right: bottom row, see Figure 5.

### Edge Pruning on Nonlinear Data

In figure 13 we provide additional experiments for edge pruning methods (\(d=20,n=300\)); ED generally maintains superior performance as the noise distribution is varied and density is increased, although the performance gap decreases for all noise distributions as the graph becomes denser.

### Runtimes for Edge Pruning

Figure 14: Runtimes for edge pruning: tables correspond to the graphs in Figure 7, from left to right.

Figure 13: Performance of ED on data generated with Gaussian, Laplace, or Uniform noise (left, middle, right columns), the average number of edges set to \(2d\), \(3d\), or \(4d\) (top, middle, bottom rows).

Implementation Details

All tests were done in Python. All runtimes were computed locally on an Apple M2 Pro Chip, 16 Gb of RAM, with no parallelization.

### Topological Sort for LiNGAM

DirectLiNGAM was imported from the lingam [45] package, \(R^{2}-\)sort was imported from the CausalDisco[28, 29] package, and LHTS was implemented using the Sklearn[24] package. All assets used have a CC-BY 4.0 license. We follow [38] to generate the data for Figure 5, using linear causal mechanisms with randomly drawn coefficient values, plus independent uniform noise. Data is standardized to remove shortcuts [28]. See Github repository for more details. Cutoff values for independence tests were set to \(\alpha=0.05\) for all methods.

### Topological Sort for Nonlinear ANM

GES and GRaSP were imported from the causal-learn[49] package; GSP was imported from the graphical_model_learning[43] package. DirectLiNGAM was imported from the lingam package [45]. NoGAM was imported from the dodiscovery[14] package. \(R^{2}-\)sort was imported from the CausalDisco package. NHTS and LoSAM were implemented using the kernel ridge regression (KRR) function from the Sklearn package, used independence tests from either the causal-learn package or the dcor[26] package, and a mutual information estimator from the npeet[44] package. All assets used have a CC-BY 4.0 license. We follow [16] to generate the data used for Figure 6 and Figure 12, using quadratic causal mechanisms with randomly drawn coefficient values, plus independent gaussian, laplace or uniform noise. Features generated with quadratic mechanisms were standardized after being generated to remove shortcuts [28] and to prevent the quadratic mechanisms from driving all values close to 0 (ensuring stability). See Github repository for more details. Note that, to enable a fair comparison between NHTS and other topological ordering methods, we implement a version of NHTS that returns a linear topological sort, rather than a hierarchical topological sort, by adding only one vertex to the sort in each iteration of its sorting procedure. For NHTS, in Stage 2 we used KRR with polynomial kernel, \(\alpha=1\), degree = 3, coeff0 = 1, and in Stage 4 we used KRR with RBF kernel, \(\alpha=0.1\), \(\gamma=0.01\). Cutoff values for independence tests were set to \(\alpha=0.05\) for all methods, no cross validation was allowed for any method. Otherwise, default settings were used for all baselines.

### Edge Pruning

Lasso and RESIT were implemented using the sklearn package, hypothesis testing with GAMs was implemented using Bsplines and GLMGam from the statsmodel[33] package. Independence tests used either the causal-learn package or the dcor package. All assets used have a CC-BY 4.0 license. We follow [16] to generate the data used for Figure 7 and Figure 13, using quadratic causal mechanisms with randomly drawn coefficient values, plus independent uniform, gaussian, or laplace noise. Features generated with quadratic mechanisms were standardized after being generated to remove shortcuts [28] and to prevent the quadratic mechanisms from driving all values close to 0 (ensuring stability). See Github repository for more details. Cutoff values for independence tests were set to \(\alpha=0.05\) for all methods.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We address all claims made in the abstract and contributions section throughout the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We clearly outline the assumptions and identifiability conditions needed for our methods to hold, only claiming aysymptotic correctness when appropriate. See Definition 2.2, Appendix A.1 and Appendix B.1.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Each theoretical result includes the necessary assumptions, and links to a correct proof in the Appendix.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Experimental procedures are described in both Section 6 and Appendix E, and code is released on github.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Code for the algorithms and data generation is provided in the supplemental material and github link.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: See Section 6, Appendix E, and the supplemental material or github link.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Error bars for experiments for linear topological sorts and edge pruning methods in the main text are standard deviations. Error bars for nonlinear topological sorts and edge pruning experiments not in the main text are whiskers on a boxplot (1.5 times the IQR).
8. **Experiments Compute Resources**Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See Appendix E.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: All parts of the Code of Ethics were followed.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [No] Justification: This paper addresses a foundational problem in the field of causal discovery, an issue not fundamentally tied to any specific set of applications.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Not applicable.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: See Appendix E.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: No new assets introduced in the paper.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Not applicable.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Not applicable.