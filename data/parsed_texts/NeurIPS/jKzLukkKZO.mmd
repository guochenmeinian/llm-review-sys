# Learning to Control the Smoothness of GCN Features

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

The pioneering work of Oono & Suzuki [ICLR, 2020] and Cai & Wang [arXiv:2006.13318] analyze the smoothness of graph convolutional network (GCN) features. Their results reveal an intricate empirical correlation between node classification accuracy and the ratio of smooth to non-smooth feature components. However, the optimal ratio that favors node classification is unknown, and the non-smooth features of deep GCN with ReLU or leaky ReLU activation function diminish. In this paper, we propose a new strategy to let GCN learn node features with a desired smoothness to enhance node classification. Our approach has three key steps: (1) We establish a geometric relationship between the input and output of ReLU or leaky ReLU. (2) Building on our geometric insights, we augment the message-passing process of graph convolutional layers (GCLs) with a learnable term to modulate the smoothness of node features with computational efficiency. (3) We investigate the achievable ratio between smooth and non-smooth feature components for GCNs with the augmented message passing scheme. Our extensive numerical results show that the augmented message passing remarkably improves node classification for GCN and some related models.

## 1 Introduction

Let \(G=(V,E)\) be an undirected graph with \(V=\{v_{i}\}_{i=1}^{n}\) and \(E\) be the set of nodes and edges, resp. Let \(\bm{A}\in\mathbb{R}^{n\times n}\) be the adjacency matrix of the graph with \(A_{ij}=\bm{1}_{(i,j)\in E}\), where \(\bm{1}\) is the indicator function. Furthermore, let \(\bm{G}\) be the following (augmented) normalized adjacency matrix

\[\bm{G}\coloneqq(\bm{D}+\bm{I})^{-\frac{1}{2}}(\bm{I}+\bm{A})(\bm{D}+\bm{I})^{ -\frac{1}{2}}=\tilde{\bm{D}}^{-\frac{1}{2}}\tilde{\bm{A}}\tilde{\bm{D}}^{- \frac{1}{2}},\] (1)

where \(\bm{I}\) is the identity matrix, \(\bm{D}\) is the degree matrix with \(D_{ii}=\sum_{j=1}^{n}A_{ij}\), and \(\tilde{\bm{A}}:=\bm{A}+\bm{I}\) and \(\tilde{\bm{D}}:=\bm{D}+\bm{I}\). Starting from the initial node features \(\bm{H}^{0}:=[(\bm{h}_{1}^{0})^{\top},\dots,(\bm{h}_{n}^{0})^{\top}]^{\top}\in \mathbb{R}^{d\times n}\) with \(\bm{h}_{i}^{0}\in\mathbb{R}^{d}\) being the \(i^{th}\) node feature vector, the graph convolutional network (GCN) [20] learns node representations using the following graph convolutional layer (GCL) transformation

\[\bm{H}^{t}=\sigma(\bm{W}^{t}\bm{H}^{t-1}\bm{G}),\] (2)

where \(\sigma\) is the activation function, e.g. ReLU [25], and \(\bm{W}^{l}\in\mathbb{R}^{d\times d}\) is learnable. GCL smooths feature vectors of the neighboring nodes. The smoothness of features helps node classification; see e.g. [22, 31, 5], resonating with the idea of classical semi-supervised learning approaches [41, 38]. Accurate node classification requires a balance between smooth and non-smooth components of GCN features [27]. Besides graph convolutional networks (GCNs) stacking GCLs, many other graph neural networks (GNNs) have been developed using different mechanisms, including spectral methods [3, 9], spatial methods [12, 30], sampling methods [13, 36], and the attention mechanism [30]. Many other GNN models can be found in recent surveys or monographs; see, e.g. [15, 1, 33, 39, 14].

Deep neural networks usually outperform shallow architectures, and a remarkable example is convolutional neural networks [21, 16]. However, this does not carry to GCNs; deep GCNs tend to performsignificantly worse than shallow models [5]. In particular, the node feature vectors learned by deep GCNs tend to be identical over each connected component of the graph; this phenomenon is referred to as _over-smoothing_[22, 26, 27, 4, 5, 32], which not only occurs for GCN but also for many other GNNs, e.g., GraphSage [13] and MPNN [12]. Intuitively, each GCL smooths neighboring node features, benefiting node classification [22, 31, 5]. However, stacking these smoothing layers will inevitably homogenize node features. Algorithms have been developed to alleviate the over-smoothing issue of GNNs, including decoupling prediction and message passing [11], skip connection and batch normalization [18, 7, 6], graph sparsification [29], jumping knowledge [34], scattering transform [24], PairNorm [37], and controlling the Dirichlet energy of node features [40].

From a theoretical perspective, it is proved that deep GCNs using ReLU or leaky ReLU activation function learn homogeneous node features [27, 4]. In particular, [27] shows that the distance of node features to the eigenspace \(\mathcal{M}\) - corresponding to the largest eigenvalue 1 of matrix \(\bm{G}\) in (1) - goes to zero when the depth of GCN with ReLU goes to infinity. Meanwhile, [27] empirically studies the intricate correlation between node classification accuracy and the ratio between smooth and non-smooth components of GCN node features, i.e., projections of node features onto eigenspace \(\mathcal{M}\) and its orthogonal complement \(\mathcal{M}^{\perp}\), resp. The empirical results of [27] indicate that _both smooth and non-smooth components of node features are crucial for accurate node classification_, while the ratio between smooth and non-smooth components to achieve optimal accuracy is unknown and task-dependent. Furthermore, [4] proves that the Dirichlet energy - another smoothness measure for node features - goes to zero when the depth of GCN with ReLU or leaky ReLU goes to infinity.

A crucial step in the proofs of [27, 4] is that ReLU and leaky ReLU reduce the distance of feature vectors to \(\mathcal{M}\) and their Dirichlet energy. However, [4] points out that _over-smoothing - characterized by the distance of features to eigenspace \(\mathcal{M}\) or the Dirichlet energy - is a misnomer_; the real smoothness should be characterized by a _normalized smoothness_, e.g., normalizing the Dirichlet energy by the magnitude of the features. _The ratio between smooth and non-smooth components of node features - studied in [27] - is closely related to the normalized smoothness_. Nevertheless, analyzing the normalized smoothness of node features learned by GCN with ReLU or leaky ReLU remains an open problem [4]. Moreover, it is interesting to ask if analyzing the normalized smoothness can result in any new understanding of GCN features and algorithms to improve GCN's performance.

### Our contribution

We aim to (1) establish a new geometric understanding of how GCL smooths GCN features and (2) develop an efficient algorithm to let GCN and related models learn node features with a desired normalized smoothness to improve node classification. We summarize our main contributions towards achieving our goal as follows:

* We prove that there is a high-dimensional sphere underlying the input and output vectors of ReLU or leaky ReLU. This geometric characterization not only implies theories in [27, 4] but also informs that adjusting the projection of input onto eigenspace \(\mathcal{M}\) can alter the smoothness of the output vectors. See Section 3 for details.
* We show that both ReLU and leaky ReLU reduce the distance of node features to eigenspace \(\mathcal{M}\), i.e., ReLU and leaky ReLU smooth their input vectors without considering their magnitude. In contrast, when taking the magnitude into account, ReLU and leaky ReLU can increase, decrease, or preserve the normalized smoothness of each dimension of the input vectors; see Sections 3 and 4.
* no matter how we adjust the input by changing its projection onto \(\mathcal{M}\). In contrast, adjusting the projection of input vectors onto \(\mathcal{M}\) can change the normalized smoothness of output to any desired value; see details in Section 4.
* for both homophilic and heterophilic graphs
- using a few of the most representative GCN-style models. See Sections 5 and 6 for details.

As far as we know, our work is the first thorough study of how ReLU and leaky ReLU affect the smoothness of node features both with and without considering their magnitude.

### Additional related works

Controlling the smoothness of node features to improve the performance of GCNs is another line of related work. For instance, [37] designs a normalization layer to prevent node features from becoming too similar to each other, and [40] constrains the Dirichlet energy to control the smoothness of node features without considering the effects of nonlinear activation functions. While there has been effort in understanding and alleviating the over-smoothing of GCNs and controlling the smoothness of node features, there is a shortage of theoretical examination of how activation functions affect the smoothness of node features, specifically accounting for the magnitude of features.

### Notation and Organization

**Notation.** We denote the \(\ell_{2}\)-norm of a vector \(\bm{u}\) as \(\|\bm{u}\|\). For vectors \(\bm{u}\) and \(\bm{v}\), we use \(\langle\bm{u},\bm{v}\rangle\), \(\bm{u}\odot\bm{v}\), and \(\bm{u}\otimes\bm{v}\) to denote their inner, Hadamard, and Kronecker product, resp. For a matrix \(\bm{A}\), we denote its \((i,j)^{th}\) entry, transpose, and inverse as \(A_{ij}\), \(\bm{A}^{\top}\), and \(\bm{A}^{-1}\), resp. We denote the trace of \(\bm{A}\in\mathbb{R}^{n\times n}\) as \(\mathrm{Trace}(\bm{A})=\sum_{i=1}^{n}A_{ii}\). For two matrices \(\bm{A}\) and \(\bm{B}\), we denote the Frobenius inner product as \(\langle\bm{A},\bm{B}\rangle_{F}:=\mathrm{Trace}(\bm{A}\bm{B}^{\top})\) and the Frobenius norm of \(\bm{A}\) as \(\|\bm{A}\|_{F}:=\sqrt{\langle\bm{A},\bm{A}\rangle}\).

**Organization.** We provide preliminaries in Section 2. In Section 3, we establish a geometric characterization of how ReLU and leaky ReLU affect the smoothness of their input vectors. We study the smoothness of each dimension of node features and take their magnitude into account in Section 4. Our proposed SCT is presented in Section 5. We comprehensively verify the efficacy of the proposed SCT in Section 6. Technical proofs and more experimental results are provided in the appendix.

## 2 Preliminaries and Existing Results

From the spectral graph theory [8], we can sort eigenvalues of matrix \(\bm{G}\) in (1) as \(1=\lambda_{1}=\ldots=\lambda_{m}>\lambda_{m+1}\geq\ldots\geq\lambda_{n}>-1\), where \(m\) is the number of connected components of the graph. We decompose \(V=\{v_{k}\}_{k=1}^{n}\) into \(m\) connected components \(V_{1},\ldots,V_{m}\). Let \(\bm{u}_{i}=(\bm{1}_{\{v_{k}\in V_{i}\}})_{1\leq k\leq n}\) be the indicator vector of \(V_{i}\), i.e., the \(k^{th}\) coordinate of \(\bm{u}_{i}\) is one if the \(k^{th}\) node \(v_{k}\) lies in the connected component \(V_{i}\); zero otherwise. Moreover, let \(\bm{e}_{i}\) be the eigenvector associated with \(\lambda_{i}\), then \(\{\bm{e}_{i}\}_{i=1}^{n}\) forms an orthonormal basis of \(\mathbb{R}^{n}\). Notice that \(\{\bm{e}_{i}\}_{i=1}^{m}\) spans the eigenspace \(\mathcal{M}\) - corresponding to eigenvalue 1 of matrix \(\bm{G}\), and \(\{\bm{e}_{i}\}_{i=m+1}^{n}\) spans the orthogonal complement of \(\mathcal{M}\), denoted by \(\mathcal{M}^{\perp}\). The paper [27] connects the indicator vectors \(\bm{u}_{i}\)s with the space \(\mathcal{M}\). In particular, we have

**Proposition 2.1** ([27]).: _All eigenvalues of matrix \(\bm{G}\) lie in the interval \((-1,1]\). Furthermore, the nonnegative vectors \(\{\tilde{\bm{D}}^{\frac{1}{2}}\bm{u}_{i}/\|\tilde{\bm{D}}^{\frac{1}{2}}\bm{u}_{ i}\|\}_{1\leq i\leq m}\) form an orthonormal basis of \(\mathcal{M}\)._

For any matrix \(\bm{H}:=[\bm{h}_{1},\ldots,\bm{h}_{n}]\in\mathbb{R}^{d\times n}\), we have the decomposition \(\bm{H}=\bm{H}_{\mathcal{M}}+\bm{H}_{\mathcal{M}^{\perp}}\) with \(\bm{H}_{\mathcal{M}}=\sum_{i=1}^{m}\bm{H}\bm{e}_{i}\bm{e}_{i}^{\top}\) and \(\bm{H}_{\mathcal{M}^{\perp}}=\sum_{i=m+1}^{n}\bm{H}\bm{e}_{i}\bm{e}_{i}^{\top}\) such that \(\langle\bm{H}_{\mathcal{M}},\bm{H}_{\mathcal{M}^{\perp}}\rangle_{F}=\mathrm{ Trace}\big{(}\sum_{i=1}^{m}\bm{H}\bm{e}_{i}\bm{e}_{i}^{\top}(\sum_{j=m+1}^{n}\bm{H}\bm{e}_{j} \bm{e}_{j}^{\top})^{\top}\big{)}=0\), implying that \(\|\bm{H}\|_{F}^{2}=\|\bm{H}_{\mathcal{M}}\|_{F}^{2}+\|\bm{H}_{\mathcal{M}^{ \perp}}\|_{F}^{2}\).

### Existing smoothness notions of node features

**Distance to the eigenspace \(\mathcal{M}\)**. Oono et al. [27] study the smoothness of features \(\bm{H}:=[\bm{h}_{1},\ldots,\bm{h}_{n}]\) using their distance to the eigenspace \(\mathcal{M}\) as an unnormalized smoothness notion.

**Definition 2.2** ([27]).: Let \(\mathbb{R}^{d}\otimes\mathcal{M}\) be the subspace of \(\mathbb{R}^{d\times n}\) consisting of the sum \(\sum_{i=1}^{m}\bm{w}_{i}\otimes\bm{e}_{i}\), where \(\bm{w}_{i}\in\mathbb{R}^{d}\) and \(\{\bm{e}_{i}\}_{i=1}^{m}\) is an orthonormal basis of the eigenspace \(\mathcal{M}\). Then we define \(\|\bm{H}\|_{\mathcal{M}^{\perp}}\) - the distance of node features \(\bm{H}\) to the eigenspace \(\mathcal{M}\) - as follows:

\[\|\bm{H}\|_{\mathcal{M}^{\perp}}\coloneqq\inf_{\bm{Y}\in\mathbb{R}^{d}\otimes \mathcal{M}}\|\bm{H}-\bm{Y}\|_{F}=\big{\|}\bm{H}-\sum_{i=1}^{m}\bm{H}\bm{e}_{i }\bm{e}_{i}^{\top}\big{\|}_{F}.\]

With the decomposition \(\bm{H}=\bm{H}_{\mathcal{M}}+\bm{H}_{\mathcal{M}^{\perp}}\), \(\|\cdot\|_{\mathcal{M}^{\perp}}\) can be related to \(\|\cdot\|_{F}\) as follows:

\[\|\bm{H}\|_{\mathcal{M}^{\perp}}=\|\bm{H}-\bm{H}_{\mathcal{M}}\|_{F}=\|\bm{H}_{ \mathcal{M}^{\perp}}\|_{F}.\] (3)

**Dirichlet energy.** The paper [4] studies the unnormalized smoothness of node features using Dirichlet energy, which is defined as follows:

**Definition 2.3** ([4]).: Let \(\tilde{\Delta}=\bm{I}-\bm{G}\) be the (augmented) normalized Laplacian, then the Dirichlet energy \(\|\bm{H}\|_{E}\) of node features \(\bm{H}\) is defined by \(\|\bm{H}\|_{E}^{2}\coloneqq\mathrm{Trace}(\bm{H}\tilde{\Delta}\bm{H}^{\top})\).

**Normalized Dirichlet energy.**[4] points out that the real smoothness of node features \(\bm{H}\) should be measured by the normalized Dirichlet energy \(\operatorname{Trace}(\bm{H}\hat{\Delta}\bm{H}^{\top})/\|\bm{H}\|_{F}^{2}\). This normalized measurement is essential because data often originates from various sources with diverse measurement units or scales. By normalization, we can mitigate biases resulting from these different scales.

### Two existing theories of over-smoothing

Let \(\lambda=\max\{|\lambda_{i}|\mid\lambda_{i}<1\}\) be the second largest magnitude of \(\bm{G}\)'s eigenvalues, and \(s_{l}\) be the largest singular value of weight matrix \(\bm{W}^{l}\). [27] shows that \(\|\bm{H}^{l}\|_{\mathcal{M}^{\perp}}\leq s_{l}\lambda\|\bm{H}^{l-1}\|_{ \mathcal{M}^{\perp}}\) under GCL when \(\sigma\) is ReLU. Therefore, \(\|\bm{H}^{l}\|_{\mathcal{M}^{\perp}}\to 0\) as \(l\to\infty\) if \(s_{l}\lambda<1\), indicating node features converge to \(\mathcal{M}\) and results in over-smoothing. A crucial step in the analysis in [27] is that \(\|\sigma(\bm{Z})\|_{\mathcal{M}^{\perp}}\leq\|\bm{Z}\|_{\mathcal{M}^{\perp}}\), for any matrix \(\bm{Z}\) when \(\sigma\) is ReLU, i.e., ReLU reduces the distance to \(\mathcal{M}\). [27] points out that it is hard to extend the above result to other activation functions even leaky ReLU.

Instead of considering \(\|\bm{H}\|_{\mathcal{M}^{\perp}}\), [4] shows that \(\|\bm{H}^{l}\|_{E}\leq s_{l}\lambda\|\bm{H}^{l-1}\|_{E}\) under GCL when \(\sigma\) is ReLU or leaky ReLU. Hence, \(\|\bm{H}^{l}\|_{E}\to 0\) as \(l\to\infty\), implying over-smoothing of GCNs. Note that \(\|\bm{H}\|_{\mathcal{M}^{\perp}}=0\) or \(\|\bm{H}^{l}\|_{E}=0\) indicates homogeneous node features. The proof in [4] applies to GCN with both ReLU and leaky ReLU by establishing the inequality \(\|\sigma(\bm{Z})\|_{E}\leq\|\bm{Z}\|_{E}\) for any matrix \(\bm{Z}\).

## 3 Effects of Activation Functions: A Geometric Characterization

In this section, we present a geometric relationship between the input and output vectors of ReLU or leaky ReLU. We use \(\|\bm{H}\|_{\mathcal{M}^{\perp}}\) as the unnormalized smoothness notion for all subsequent analyses since we observe that \(\|\bm{H}\|_{\mathcal{M}^{\perp}}\) and \(\|\bm{H}\|_{E}\) are equivalent as seminorms. In particular, we have

**Proposition 3.1**.: \(\|\bm{H}\|_{\mathcal{M}^{\perp}}\) _and \(\|\bm{H}\|_{E}\) are two equivalent seminorms, i.e., there exist two constants \(\alpha,\beta>0\) s.t. \(\alpha\|\bm{H}\|_{\mathcal{M}^{\perp}}\leq\|\bm{H}\|_{E}\leq\beta\|\bm{H}\|_{ \mathcal{M}^{\perp}}\), for any \(\bm{H}\in\mathbb{R}^{d\times n}\)._

### ReLU

Let \(\sigma(x)=\max\{x,0\}\) be ReLU. The first main result of this paper is that there is a high-dimensional sphere underlying the input and output of ReLU; more precisely, we have

**Proposition 3.2** (ReLU).: _For any \(\bm{Z}=\bm{Z}_{\mathcal{M}}+\bm{Z}_{\mathcal{M}^{\perp}}\in\mathbb{R}^{d\times n}\), let \(\bm{H}=\sigma(\bm{Z})=\bm{H}_{\mathcal{M}}+\bm{H}_{\mathcal{M}^{\perp}}\). Then \(\bm{H}_{\mathcal{M}^{\perp}}\) lies on the high-dimensional sphere centered at \(\bm{Z}_{\mathcal{M}^{\perp}}/2\) with radius \(\|\bm{Z}_{\mathcal{M}^{\perp}}/2\|_{F}\) and hence we have \(\|\bm{H}\|_{\mathcal{M}^{\perp}}\leq\|\bm{Z}\|_{\mathcal{M}^{\perp}}\)._

### Leaky ReLU

Now we consider leaky ReLU \(\sigma_{a}(x)=\max\{x,ax\}\), where \(0<a<1\) is a positive scalar. Similar to ReLU, we have the following result for leaky ReLU

**Proposition 3.3** (Leaky ReLU).: _For any \(\bm{Z}=\bm{Z}_{\mathcal{M}}+\bm{Z}_{\mathcal{M}^{\perp}}\in\mathbb{R}^{d\times n}\), let \(\bm{H}=\sigma_{a}(\bm{Z})=\bm{H}_{\mathcal{M}}+\bm{H}_{\mathcal{M}^{\perp}}\). Then \(\bm{H}_{\mathcal{M}^{\perp}}\) lies on the high-dimensional sphere centered at \((1+a)\bm{Z}_{\mathcal{M}^{\perp}}/2\) with radius_

\[r_{a}\coloneqq\left(\|(1-a)\bm{Z}_{\mathcal{M}^{\perp}}/2\|_{F}^{2}-\langle \bm{H}_{\mathcal{M}}-\bm{Z}_{\mathcal{M}},\bm{H}_{\mathcal{M}}-a\bm{Z}_{ \mathcal{M}}\rangle_{F}\right)^{1/2}.\]

_In particular, \(\bm{H}_{\mathcal{M}^{\perp}}\) lies inside the ball centered at \((1+a)\bm{Z}_{\mathcal{M}^{\perp}}/2\) with radius \(\|(1-a)\bm{Z}_{\mathcal{M}^{\perp}}/2\|_{F}\) and hence we see that \(a\|\bm{Z}\|_{\mathcal{M}^{\perp}}\leq\|\bm{H}\|_{\mathcal{M}^{\perp}}\leq\| \bm{Z}\|_{\mathcal{M}^{\perp}}\)._

### Implications of the above geometric characterizations

Propositions 3.2 and 3.3 imply that the precise location of \(\bm{H}_{\mathcal{M}^{\perp}}\) (or \(\|\bm{H}_{\mathcal{M}^{\perp}}\|_{F}=\|\bm{H}\|_{\mathcal{M}^{\perp}}\)) depends on the center and the radius \(r\) or \(r_{a}\). Given a fixed \(\bm{Z}_{\mathcal{M}^{\perp}}\), the center of the spheres remains unchanged, and \(r\) and \(r_{a}\) are only affected by changes in \(\bm{Z}_{\mathcal{M}}\). This observation motivates us to investigate _how changes in \(\bm{Z}_{\mathcal{M}}\) impact \(\|\bm{H}\|_{\mathcal{M}^{\perp}}\), i.e., the unnormalized smoothness of node features_.

Propositions 3.2 and 3.3 imply both ReLU and leaky ReLU reduce the distance of node features to eigenspace \(\mathcal{M}\), i.e. \(\|\bm{H}\|_{\mathcal{M}^{\perp}}\leq\|\bm{Z}\|_{\mathcal{M}^{\perp}}\). Moreover, this inequality is independent of \(\bm{Z}_{\mathcal{M}}\); consider \(\bm{Z},\bm{Z}^{\prime}\in\mathbb{R}^{d\times n}\) s.t. \(\bm{Z}_{\mathcal{M}^{\perp}}=\bm{Z}_{\mathcal{M}^{\perp}}^{\prime}\) but \(\bm{Z}_{\mathcal{M}}\neq\bm{Z}_{\mathcal{M}^{\prime}}^{\prime}\). Let \(\bm{H}\) and \(\bm{H}^{\prime}\) be the output of \(\bm{Z}\) and \(\bm{Z}^{\prime}\) via ReLU or leaky ReLU, resp. Then we have \(\|\bm{H}\|_{\mathcal{M}^{\perp}}\leq\|\bm{Z}\|_{\mathcal{M}^{\perp}}\) and \(\|\bm{H}^{\prime}\|_{\mathcal{M}^{\perp}}\leq\|\bm{Z}^{\prime}\|_{\mathcal{M}^{ \perp}}\). Since \(\bm{Z}_{\mathcal{M}^{\perp}}=\bm{Z}_{\mathcal{M}^{\perp}}^{\prime}\), we deduce that \(\|\bm{H}^{\prime}\|_{\mathcal{M}^{\perp}}\leq\|\bm{Z}\|_{\mathcal{M}^{\perp}}\). In other words, when \(\bm{Z}_{\mathcal{M}^{\perp}}=\bm{Z}_{\mathcal{M}^{\perp}}^{\prime}\) is fixed, _changing \(\bm{Z}_{\mathcal{M}}\) to \(\bm{Z}_{\mathcal{M}}^{\prime}\) can change the unnormalized smoothness of the output features but cannot change the fact that ReLU and leaky ReLU smooth node features_; we demonstrate this result inFig. 1a) in Section 4.1. Notice that without considering the nonlinear activation function, changing \(\bm{Z}_{\mathcal{M}}\) does not affect the unnormalized smoothness of node features measured by \(\|\bm{H}\|_{\mathcal{M}^{\perp}}\).

In contrast to the unnormalized smoothness, _if one considers the normalized smoothness, we find that adjusting \(\bm{Z}_{\mathcal{M}}\) can result in a less smooth output_; we will discuss this in Section 4.1.

## 4 How Adjusting \(\bm{Z}_{\mathcal{M}}\) Affects the Smoothness of the Output

Throughout this section, we let \(\bm{Z}\) and \(\bm{H}\) be the input and output of ReLU or leaky ReLU. The smoothness notions based on the distance of feature to \(\mathcal{M}\) or their Dirichlet energy do not account for the magnitude of each dimension of the features; [4] points out that analyzing the normalized smoothness of features \(\bm{Z}\), given by \(\|\bm{Z}\|_{E}/\|\bm{Z}\|_{F}\), is an open problem. However, these two smoothness notions aggregate the smoothness of node features across all dimensions; when the magnitude of some dimensions is much larger than others, the smoothness will be dominated by them.

Motivated by the discussion in Section 3.3, we study _the disparate effects of adjusting \(\bm{Z}_{\mathcal{M}}\) on the normalized and unnormalized smoothness_ in this section. For the sake of simplicity, we assume the graph is connected (\(m=1\)); all the following results can be extended to graphs with multiple connected components easily. Due to the equivalence between seminorms \(\|\cdot\|_{\mathcal{M}}\) and \(\|\cdot\|_{E}\), we introduce the following definition of the dimension-wise normalized smoothness of node features.

**Definition 4.1**.: Let \(\bm{Z}\in\mathbb{R}^{d\times n}\) be the features over \(n\) nodes with \(\bm{z}^{(i)}\in\mathbb{R}^{n}\) being its \(i^{th}\) row, i.e., the \(i^{th}\) dimension of the features over all nodes. We define the normalized smoothness of \(\bm{z}^{(i)}\) as follows:

\[s(\bm{z}^{(i)})\coloneqq\|\bm{z}^{(i)}_{\mathcal{M}}\|/\|\bm{z}^{(i)}\|,\]

where we set \(s(\bm{z}^{(i)})=1\) when \(\bm{z}^{(i)}=\bm{0}\).

_Remark 4.2_.: Notice that the normalized smoothness \(s(\bm{z}^{(i)})=\|\bm{z}^{(i)}_{\mathcal{M}}\|/\|\bm{z}^{(i)}\|\) is closely related to the ratio between the smooth and non-smooth components of node features \(\|\bm{z}^{(i)}_{\mathcal{M}}\|/\|\bm{z}^{(i)}_{\mathcal{M}^{\perp}}\|\).

The graph is connected implies that \(\bm{z}^{(i)}_{\mathcal{M}}=\langle\bm{z}^{(i)},\bm{e}_{1}\rangle\bm{e}_{1}\) and \(\|\bm{z}^{(i)}_{\mathcal{M}}\|=|\langle\bm{z}^{(i)},\bm{e}_{1}\rangle|\). Without ambiguity, we write \(\bm{z}\) for \(\bm{z}^{(i)}\) and \(\bm{e}\) for \(\bm{e}_{1}\) - the eigenvector of \(\bm{G}\) associated with the eigenvalue \(1\). Moreover, we have

\[s(\bm{z})=\frac{\|\bm{z}_{\mathcal{M}}\|}{\|\bm{z}\|}=\frac{|\langle\bm{z}, \bm{e}\rangle|}{\|\bm{z}\|}=\frac{|\langle\bm{z},\bm{e}\rangle|}{\|\bm{z}\| \cdot\|\bm{e}\|}\Rightarrow 0\leq s(\bm{z})\leq 1,\] (4)

It is evident that _the larger \(s(\bm{z})\) is, the smoother the node feature \(\bm{z}\) is1_. In fact, we have

Footnote 1: Here, \(\bm{z}\in\mathbb{R}^{n}\) is a vector whose \(i^{th}\) entry is the 1D feature associated with node \(i\).

\[s(\bm{z})^{2}+\Big{(}\frac{\|\bm{z}\|_{\mathcal{M}^{\perp}}}{\|\bm{z}\|}\Big{)} ^{2}=\frac{\|\bm{z}_{\mathcal{M}}\|^{2}}{\|\bm{z}\|^{2}}+\frac{\|\bm{z}_{ \mathcal{M}^{\perp}}\|^{2}}{\|\bm{z}\|^{2}}=1,\]

where \(\|\bm{z}\|_{\mathcal{M}^{\perp}}/\|\bm{z}\|\) decreases as \(s(\bm{z})\) increases.

To discuss how the smoothness \(s(\bm{h})=s(\sigma(\bm{z}))\) or \(s(\sigma_{a}(\bm{z}))\) can be adjusted by changing \(\bm{z}_{\mathcal{M}}\), we consider the function

\[\bm{z}(\alpha)=\bm{z}-\alpha\bm{e}.\]

It is clear that

\[\bm{z}(\alpha)_{\mathcal{M}^{\perp}}=\bm{z}_{\mathcal{M}^{\perp}}\text{ and }\bm{z}(\alpha)_{\mathcal{M}}=\bm{z}_{\mathcal{M}}-\alpha\bm{e},\]

where we see that \(\alpha\) only alters \(\bm{z}_{\mathcal{M}}\) while preserves \(\bm{z}_{\mathcal{M}^{\perp}}\). Moreover, it is evident that

\[s(\bm{z}(\alpha))=\sqrt{1-\frac{\|\bm{z}(\alpha)_{\mathcal{M}^{\perp}}\|^{2} }{\|\bm{z}(\alpha)\|^{2}}}=\sqrt{1-\frac{\|\bm{z}_{\mathcal{M}^{\perp}}\|^{2} }{\|\bm{z}(\alpha)\|^{2}}}.\]

It follows that \(s(\bm{z}(\alpha))=1\) if and only if \(\bm{z}_{\mathcal{M}^{\perp}}=\bm{0}\) (include the case \(\bm{z}=\bm{0}\)), showing that when \(\bm{z}_{\mathcal{M}^{\perp}}=\bm{0}\), the vector \(\bm{z}\) is the smoothest one.

The disparate effects of \(\alpha\) on \(\|\cdot\|_{\mathcal{M}^{\perp}}\) and \(s(\cdot)\): Empirical results

Let us empirically study possible values that the unnormalized smoothness \(\|\sigma(\bm{z}(\alpha))\|_{\mathcal{M}^{\perp}}\), \(\|\sigma_{a}(\bm{z}(\alpha))\|_{\mathcal{M}^{\perp}}\) and the normalized smoothness \(s(\sigma(\bm{z}(\alpha)))\), \(s(\sigma_{a}(\bm{z}(\alpha)))\) can take when \(\alpha\) varies.

We denote \(\bm{z}_{\alpha}\coloneqq\bm{z}(\alpha)=\bm{z}-\alpha\bm{e}\). We consider a connected synthetic graph with \(100\) nodes, and each node is assigned a random degree between \(2\) to \(10\). Then we assign an initial node feature \(\bm{z}\in\mathbb{R}^{100}\), sampled uniformly on the interval \([-1.5,1.5]\), to the graph with each node feature being a scalar. Also, we compute \(\bm{e}\) by the formula \(\bm{e}=\tilde{\bm{D}}^{\frac{1}{2}}\bm{u}/\|\tilde{\bm{D}}^{\frac{1}{2}}\bm{u}\|\) from Proposition 2.1, where \(\bm{u}\in\mathbb{R}^{100}\) is the vector whose entries are all ones and \(\tilde{\bm{D}}\) is the (augmented) degree matrix. We examine two different smoothness notions for the input \(\bm{z}\) and the output \(\sigma(\bm{z}_{\alpha})\) and \(\sigma_{a}(\bm{z}_{\alpha})\), where the smoothness is measured for various values of the smoothness control parameter \(\alpha\in[-1.5,1.5]\). In Fig. 0(a), we study the unnormalized smoothness measured by \(\|\cdot\|_{\mathcal{M}^{\perp}}\); we see that \(\|\sigma(\bm{z}_{\alpha})\|_{\mathcal{M}^{\perp}}\) and \(\|\sigma_{a}(\bm{z}_{\alpha})\|_{\mathcal{M}^{\perp}}\) are always no greater than \(\|\bm{z}\|_{\mathcal{M}^{\perp}}\). This coincides with the discussion in Section 3.3; adjusting the projection of \(\bm{z}\) onto the eigenspace \(\mathcal{M}\) can not change the fact that \(\|\sigma(\bm{z}_{\alpha})\|_{\mathcal{M}^{\perp}}\leq\|\bm{z}\|_{\mathcal{M}^ {\perp}}\) and \(\|\sigma_{a}(\bm{z}_{\alpha})\|_{\mathcal{M}^{\perp}}\leq\|\bm{z}\|_{\mathcal{ M}^{\perp}}\). Nevertheless, an interesting result is that _defining the eigenspace \(\bm{e}\) projection can adjust the unnormalized smoothness of the output_: notice that altering the eigenspace projection does not change its distance to \(\mathcal{M}\), i.e., the smoothness of the input is unchanged, but the smoothness of the output after activation function can be changed.

In contrast, when studying the normalized smoothness \(s(\cdot)\) in Fig. 0(b), we find that \(s(\sigma(\bm{z}(\alpha)))\) and \(s(\sigma_{a}(\bm{z}(\alpha)))\) can be adjusted by \(\alpha\) to values smaller than \(s(\bm{z})\). More precisely, we see that by adjusting \(\alpha\), \(s(\sigma(\bm{z}(\alpha)))\) and \(s(\sigma_{a}(\bm{z}(\alpha)))\) can achieve most of the values in \([0,1]\). In other words, both smoother and less smooth features can be obtained by adjusting \(\alpha\).

### Theoretical results on the smooth effects of ReLU and leaky ReLU

In this subsection, we build theoretical understandings of the above empirical findings on the achievable smoothness shown in Fig. 1. Notice that if \(\bm{z}_{\mathcal{M}^{\perp}}=\bm{0}\), the inequalities presented in Propositions 3.2 and 3.3 indicate that \(\|\sigma(\bm{z}(\alpha))\|_{\mathcal{M}^{\perp}}\) and \(\|\sigma_{a}(\bm{z}(\alpha))\|_{\mathcal{M}^{\perp}}\) vanish. So we have \(s(\sigma(\bm{z}(\alpha)))=1\) for any \(\alpha\) when \(\bm{z}_{\mathcal{M}^{\perp}}=\bm{0}\). Then we may assume \(\bm{z}_{\mathcal{M}^{\perp}}\neq\bm{0}\) for the following study.

**Proposition 4.3** (ReLU).: _Suppose \(\bm{z}_{\mathcal{M}^{\perp}}\neq\bm{0}\). Let \(\bm{h}(\alpha)=\sigma(\bm{z}(\alpha))\) with \(\sigma\) being ReLU, then_

\[\min_{\alpha}s(\bm{h}(\alpha))=\sqrt{\frac{\sum_{x_{i}=\max\bm{x}}a_{i}}{\sum_ {j=1}^{n}d_{j}}}\;\;\text{and}\;\;\max_{\alpha}s(\bm{h}(\alpha))=1,\]

_where \(\bm{x}\coloneqq\tilde{\bm{D}}^{-\frac{1}{2}}\bm{z}\), \(\max\bm{x}=\max_{1\leq i\leq n}x_{i}\), and \(\tilde{\bm{D}}\) is the augmented degree matrix with diagonals \(d_{1},d_{2},\ldots,d_{n}\). In particular, the normalized smoothness \(s(\bm{h}(\alpha))\) is monotone increasing as \(\alpha\) decreases whenever \(\alpha<\|\tilde{\bm{D}}^{\frac{1}{2}}\bm{u}_{n}\|\max\bm{x}\) and it has range \([\min_{\alpha}s(\bm{h}(\alpha)),1]\)._

**Proposition 4.4** (Leaky ReLU).: _Suppose \(\bm{z}_{\mathcal{M}^{\perp}}\neq\bm{0}\). Let \(\bm{h}(\alpha)=\sigma_{a}(\bm{z}(\alpha))\) with \(\sigma_{a}\) being leaky ReLU, then (1) \(\min_{\alpha}s(\bm{h}(\alpha))=0\), and (2) \(\sup_{\alpha}s(\bm{h}(\alpha))=1\) and \(s(\bm{h}(\alpha))\) has range \([0,1)\)._

Proposition 4.4 also holds for other variants of ReLU, e.g., ELU2 and SELU3:; see Appendix C. We summarize Propositions 3.2, 3.3, 4.3, and 4.4 in the following corollary, which qualitatively explains the empirical results in Fig. 1.

Footnote 2: The ELU function is defined by \(f(x)=\max(x,0)+\min(0,a\cdot(e^{x}-1))\) where \(a>0\).

Footnote 3: The SELU function is defined by \(f(x)=c(\max(x,0)+\min(0,a\cdot(e^{x}-1)))\) where \(a,c>0\).

**Corollary 4.5**.: _Suppose \(\bm{z}_{\mathcal{M}^{\perp}}\neq\bm{0}\). Let \(\bm{h}(\alpha)=\sigma(\bm{z}(\alpha))\) or \(\sigma_{a}(\bm{z}(\alpha))\) with \(\sigma\) being ReLU and \(\sigma_{a}\) being leaky ReLU. Then we have \(\|\bm{z}\|_{\mathcal{M}^{\perp}}\geq\|\bm{h}(\alpha)\|_{\mathcal{M}^{\perp}}\) for any \(\alpha\in\mathbb{R}\); however, \(s(\bm{h}(\alpha))\) can be smaller than, larger than, or equal to \(s(\bm{z})\) for different values of \(\alpha\)._

Propositions 4.3 and 4.4, and Corollary 4.5, provide a theoretical basis for the empirical results in Fig. 1. Moreover, our results indicate that for any given vector \(\bm{z}\), altering \(\bm{z}_{\mathcal{M}}\) can change both the unnormalized and the normalized smoothness of the output vector \(\bm{h}=\sigma(\bm{z})\) or \(\sigma_{a}(\bm{z})\). In particular, the normalized smoothness of \(\bm{h}=\sigma(\bm{z})\) or \(\sigma_{a}(\bm{z})\) can be adjusted to any value in the range shown in Propositions 4.3 and 4.4. This provides us with insights to control the smoothness of features to improve the performance of GCN and we will discuss this in the next section.

## 5 Controlling Smoothness of Node Features

We do not know how smooth features are ideal for a given node classification task. Nevertheless, our theory indicates that both normalized and unnormalized smoothness of the output of each GCL can be adjusted by altering the input's projection onto \(\mathcal{M}\). As such, we propose the following learnable smoothness control term to modulate the smoothness of each dimension of the learned node features

\[\bm{B}_{\bm{\alpha}}^{l}=\sum_{i=1}^{m}\bm{\alpha}_{i}^{l}\bm{e}_{i}^{\top},\] (5)where \(l\) is the layer index, \(\{\bm{e}_{i}\}_{i=1}^{m}\) is the orthonormal basis of the eigenspace \(\mathcal{M}\), and \(\bm{\alpha}^{l}:=\{\bm{\alpha}_{i}^{l}\}_{i=1}^{m}\) is a collection of learnable vectors with \(\bm{\alpha}_{i}^{l}\in\mathbb{R}^{d}\) being approximated by a multi-layer perceptron (MLP). The detailed configuration of \(\bm{\alpha}_{i}^{l}\) will be specified in each experiment later. One can see that \(\bm{B}_{\bm{\alpha}}^{l}\) always lies in \(\mathbb{R}^{d}\otimes\mathcal{M}\). We integrate SCT into GCL, resulting in

\[\bm{H}^{l}=\sigma(\bm{W}^{l}\bm{H}^{l-1}\bm{G}+\bm{B}_{\bm{\alpha}}^{l}).\] (6)

We call the corresponding model GCN-SCT. Again, the idea is that _we alter the component in eigenspace to control the smoothness of features_. Each dimension of \(\bm{H}^{l}\) can be smoother, less smooth, or the same as \(\bm{H}^{l-1}\) in normalized smoothness, though \(\bm{H}^{l}\) gets closer to \(\mathcal{M}\) than \(\bm{H}^{l-1}\).

To design SCT, we introduce a learnable matrix \(\bm{A}^{l}\in\mathbb{R}^{d\times m}\) for layer \(l\), whose columns are \(\bm{\alpha}_{i}^{l}\), where \(m\) is the dimension of the eigenspace \(\mathcal{M}\) and \(d\) is the dimension of the features. We observe in our experiments that the SCT performs best when informed by degree pooling over the subcomponents of the graph. The matrix of the orthogonal basis vectors, denoted by \(\bm{Q}\coloneqq[\bm{e}_{1},\dots,\bm{e}_{m}]\in\mathbb{R}^{n\times m}\), is used to perform pooling \(\bm{H}^{l}\bm{Q}\) for input \(\bm{H}^{l}\). In particular, we let \(\bm{A}^{l}=\bm{W}\odot(\bm{H}^{l}\bm{Q})\), where \(\bm{W}\in\mathbb{R}^{d\times m}\) is learnable and performs pooling over \(\bm{H}^{l}\) using the eigenvectors \(\bm{Q}\). The second architecture uses a residual connection with hyperparameter \(\beta_{l}=\log(\theta/l+1)\) and learnable matrices \(\bm{W}_{0},\bm{W}_{1}\in\mathbb{R}^{d\times d}\) and the softmax function \(\phi\). Resulting in \(\bm{A}^{l}=\phi(\bm{H}^{l}\bm{Q})\odot(\beta_{l}\bm{W}_{0}\bm{H}^{l}\bm{Q}+(1- \beta_{l})\bm{W}_{1}\bm{H}^{l}\bm{Q})\). In Section 6, we use the first architecture for GCN-SCT as GCN uses only \(\bm{H}^{l}\) information at each layer. We use the second architecture for GCNII-SCT and EGNN-SCT which use both \(\bm{H}^{0}\) and \(\bm{H}^{l}\) information at each layer. There are two particular advantages of the above design of SCT: (1) it can effectively change the normalized smoothness of the learned features, and (2) it is computationally efficient since we only use the eigenvectors corresponding to the eigenvalue 1 of matrix \(\bm{G}\), which is determined based on the connectivity of the graph.

### Integrating SCT into other GCN-style models

In this subsection, we present other usages of the proposed SCT. Due to the page limit, we carefully select two other most representative models. The first example is GCNII [6], GCNII extends GCN to express an arbitrary polynomial filter rather than the Laplacian polynomial filter and achieves state-of-the-art (SOTA) performance among GCN-style models on various tasks [6; 23], and we aim to show that SCT can even improve the accuracy of the GCN-style model that achieves SOTA performance on many node classification tasks. The second example is energetic GNN (EGNN) [40], which controls the smoothness of node features by constraining the lower and upper bounds of the Dirichlet energy of features and assuming the activation function is linear. In this case, we aim to show that our new theoretical understanding of the role of activation functions and the proposed SCT can boost the performance of EGNN with considering nonlinear activation functions.

**GCNII**. Each GCNII layer uses a skip connection to the initial layer \(\bm{H}^{0}\) and given as follows:

\[\bm{H}^{l}=\sigma\big{(}((1-\alpha_{l})\bm{H}^{l-1}\bm{G}+\alpha_{l}\bm{H}^{0}) ((1-\beta_{l})\bm{I}+\beta_{l}\bm{W}^{l})\big{)},\]

where \(\alpha_{l},\beta_{l}\in(0,1)\) are learnable scalars. We integrate SCT \(\bm{B}_{\bm{\alpha}}^{l}\) into GCNII, resulting in the following GCNII-SCT layers

\[\bm{H}^{l}=\sigma\big{(}((1-\alpha_{l})\bm{H}^{l-1}\bm{G}+\alpha_{l}\bm{H}^{0}) ((1-\beta_{l})\bm{I}+\beta_{l}\bm{W}^{l})+\bm{B}_{\bm{\alpha}}^{l}\big{)},\]

where the residual connection and identity mapping are consistent with GCNII.

**EGNN.** Each EGNN layer can be written as follows:

\[\bm{H}^{l}=\sigma\big{(}\bm{W}^{l}(c_{1}\bm{H}^{0}+c_{2}\bm{H}^{l-1}+(1-c_{ \min})\bm{H}^{l-1}\bm{G})\big{)},\] (7)

where \(c_{1},c_{2}\) are learnable weights that satisfy \(c_{1}+c_{2}=c_{\min}\) with \(c_{\min}\) being a hyperparameter. To constrain Dirichlet energy, EGNN initializes trainable weights \(\bm{W}^{l}\) as a diagonal matrix with explicit singular values and regularizes them to keep the orthogonality during the model training. Ignoring the activation function \(\sigma\), \(\bm{H}^{l}\) - node features at layer \(l\) of EGNN satisfies

\[c_{\min}\|\bm{H}^{0}\|_{E}\leq\|\bm{H}^{l}\|_{E}\leq c_{\max}\|\bm{H}^{0}\|_{E},\]

where \(c_{\max}\) is the square of the maximal singular value of the initialization of \(\bm{W}^{1}\). Similarly, we modify EGNN to result in the following EGNN-SCT layer

\[\bm{H}^{l}=\sigma\big{(}\bm{W}^{l}((1-c_{\min})\bm{H}^{l-1}\bm{G}+c_{1}\bm{H}^{ 0}+c_{2}\bm{H}^{l-1})+\bm{B}_{\bm{\alpha}}^{l}\big{)},\]

where everything remains the same as the EGNN layer except that we add our proposed SCT \(\bm{B}_{\bm{\alpha}}^{l}\).

[MISSING_PAGE_FAIL:8]

the classification accuracy of baseline models; in particular, the improvement can be remarkable for GCN and GCNII. However, EGNN-SCT (using ReLU or leaky ReLU) performs occasionally worse than EGNN (using SReLU), and this is because of the choice of activation functions. In Appendix D.3, we report the results of EGNN-SCT using SReLU, showing that EGNN-SCT outperforms EGNN in all tasks. In fact, SReLU is a shifted version of ReLU, and our theory for ReLU applies to SReLU as well. The model size and computational time are reported in Table 4 in the appendix.

Table 1 also shows that even with SCT, the accuracy of GCN drops when the depth is 16 or 32. This motivates us to investigate the smoothness of the node features learned by GCN and GCN-SCT. Fig. 3 plots the heatmap of the normalized smoothness of each dimension of the learned node features learned by GCN and GCN-SCT with 32 layers for Citeseer node classification. In these plots, the horizontal and vertical dimensions denote the feature dimension and the layer of the model, resp. We notice that the normalized smoothness of each dimension of the features - from layers 14 to 32 learned by GCN - closes to 1, confirming that deep GCN learns homogeneous features. In contrast, the features learned by GCN-SCT are inhomogeneous, as shown in Fig. 2(b). Therefore, we believe the performance degradation of deep GCN-SCT is due to other factors. Compared to GCNII/GCNII-SCT and EGNN/EGNN-SCT, GCN-SCT does not use skip connections, which is known to help avoid vanishing gradients in training deep neural networks [16; 17]. In Appendix D.3, we show that training GCN and GCN-SCT do suffer from the vanishing gradient issue; however, the other models do not. Besides Citeseer, we notice similar behavior occurs for training GCN and GCN-SCT for Cora and Coauthor-Physics node classification tasks.

**Other datasets.** We further compare different models trained on different datasets using 10-fold cross-validation and fixed \(48/32/20\%\) splits following [28]. Table 2 compares GCN and GCNII with and without SCT, using leaky ReLU, for classifying five heterophilic node classification datasets. We exclude EGNN as these heterophilic datasets are not considered in [40]. We report the average accuracy of GCN and GCNII from [6]. We tune all other models using a Bayesian meta-learning algorithm to maximize the mean validation accuracy. We report the best test accuracy for each model of depth searched over the set \(\{2,4,8,16,32\}\). SCT can significantly improve the classification accuracy of the baseline models. Table 2 also contrasts the computational time (on Tesla T4 GPUs from Google Colab) per epoch of models that achieve the best test accuracy; the models using SCT can even save computational time to achieve the best accuracy which is because the best accuracy is achieved at a moderate depth (Table 8 in Appendix D.4 lists the mean and standard deviation for the test accuracies on all five datasets. Table 9 in Appendix D.4 lists the computational time per epoch for each model of depth 8, showing that using SCT only takes a small amount of computational overhead.

## 7 Concluding Remarks

In this paper, we establish a geometric characterization of how ReLU and leaky ReLU affect the smoothness of the GCN features. We further study the dimension-wise normalized smoothness of the learned node features, showing that activation functions not only smooth node features but also can reduce or preserve the normalized smoothness of the features. Our theoretical findings inform the design of a simple yet effective SCT for GCN. The proposed SCT can change the smoothness, in terms of both normalized and unnormalized smoothness, of the learned node features by GCN.

**Limitations:** Our proposed SCT provides provable guarantees for controlling the smoothness of features learned by GCN and related models. A key aspect to establish our theoretical results is demonstrating that, without SCT, the features of the vanilla model tend to be overly smooth; without this condition, SCT cannot ensure performance guarantees.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multicolumn{2}{c}{**Cornel**} & \multicolumn{2}{c}{**Texas**} & \multicolumn{2}{c}{**Wisconsin**} & \multicolumn{2}{c}{**Chameleon**} & **Squirrel** \\ \hline \(52.70\%\) & \(58.95\) (\(0.71\)/\(1.8\)) & \(52.16\)/\(\bm{62.16}\) (\(0.70\)/\(8.5\)) & \(45.85\)/\(\bm{54.71}\) (\(0.70\)/\(8.5\)) & \(28.18\)/\(\bm{88.44}\) (\(0.6/0.7\)) & \(23.96\)/\(\bm{35.31}\) (\(1.67\)/\(4.0\)) \\ \(74.86\)/\(\bm{75.41}\) (\(2.02\)/\(2.0\)) & \(69.46\)/\(\bm{63.34}\) (\(3.12\)/\(2.0\)) & \(74.12\)/\(\bm{68.08}\) (\(2.01\)/\(5.0\)) & \(60.61\)/\(\bm{64.52}\) (\(1.51\)/\(3.3\)) & \(38.47\)/\(\bm{47.51}\) (\(5.53\)/\(7.5\)) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Mean test accuracy and average computational time per epoch in the parenthesis) for the Web-bkB and WikipediaNetwork datasets with fixed \(48/32/20\%\) splits. First row: GCN/GCN-SCT. Second row: GCNII/GCNII-SCT. (Unit:% for accuracy and \(\times 10^{-2}\) second for computational time.)

Figure 3: The normalized smoothness – of each dimension of the feature vectors at a given layer – for a) GCN and b) GCN-SCT on the Citeseer dataset with 32 layers and 16 hidden dimensions. GCN features become entirely smooth since layer 14, while GCN-SCT controls the smoothness for each feature at any depth. Horizontal and vertical axes represent the index of the feature dimension and the intermediate layer, resp. on the Citeseer dataset with 32 layers and 16 hidden dimensions. GCN features become entirely smooth since layer 14, while GCN-SCT controls the smoothness for each feature at any depth. Horizontal and vertical axes represent the index of the feature dimension and the intermediate layer, resp. on the ResNet accuracy is achieved at a moderate depth (Table 8 in Appendix D.4 lists the mean and standard deviation for the test accuracies on all five datasets. Table 9 in Appendix D.4 lists the computational time per epoch for each model of depth 8, showing that using SCT only takes a small amount of computational overhead.

## 8 Broader Impacts

Our paper focuses on developing new theoretical understandings of the smoothness of node features learned by graph convolutional networks. The paper is mainly theoretical. We do not see any potential ethical issues in our research; all experiments are carried out using existing benchmark settings and datasets.

Our paper brings new insights into building new graph neural networks with improved performance over existing models, which is crucial for many applications. In particular, for applications where graph neural network is the method of choice. We expect our approach to play a role in material science and biophysics applications.

## References

* Battaglia et al. [2018] Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. _arXiv preprint arXiv:1806.01261_, 2018.
* Biewald [2020] Lukas Biewald. Experiment tracking with weights and biases, 2020. Software available from wandb.com.
* Bruna et al. [2014] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and deep locally connected networks on graphs. In _2nd International Conference on Learning Representations, ICLR 2014_, 2014.
* Cai and Wang [2020] Chen Cai and Yusu Wang. A note on over-smoothing for graph neural networks. _arXiv preprint arXiv:2006.13318_, 2020.
* Chen et al. [2020] Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the over-smoothing problem for graph neural networks from the topological view. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 3438-3445, 2020.
* Chen et al. [2020] Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph convolutional networks. In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 1725-1735. PMLR, 13-18 Jul 2020.
* Chen et al. [2019] Zhengdao Chen, Lisha Li, and Joan Bruna. Supervised community detection with line graph neural networks. In _International Conference on Learning Representations_, 2019.
* Chung [1997] Fan RK Chung. _Spectral graph theory_, volume 92. American Mathematical Soc., 1997.
* Defferrard et al. [2016] Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. _Advances in neural information processing systems_, 29, 2016.
* Fey and Lenssen [2019] Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In _ICLR Workshop on Representation Learning on Graphs and Manifolds_, 2019.
* Gasteiger et al. [2019] Johannes Gasteiger, Aleksandar Bojchevski, and Stephan Gunnemann. Combining neural networks with personalized pagerank for classification on graphs. In _International Conference on Learning Representations_, 2019.
* Volume 70_, ICML'17, page 1263-1272. JMLR.org, 2017.
* Hamilton et al. [2017] William Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. _Advances in neural information processing systems_, 30, 2017.
* Hamilton [2020] William L Hamilton. _Graph representation learning_. Morgan & Claypool Publishers, 2020.

* [15] William L Hamilton, Rex Ying, and Jure Leskovec. Representation learning on graphs: Methods and applications. _arXiv preprint arXiv:1709.05584_, 2017.
* [16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2016.
* [17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV 14_, pages 630-645. Springer, 2016.
* [18] Tatsuro Kawamoto, Masashi Tsubaki, and Tomoyuki Obuchi. Mean-field theory of graph neural networks in graph partitioning. _Advances in Neural Information Processing Systems_, 31, 2018.
* [19] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [20] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In _International Conference on Learning Representations_, 2017.
* [21] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. _Communications of the ACM_, 60(6):84-90, 2017.
* [22] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In _Thirty-Second AAAI conference on artificial intelligence_, 2018.
* [23] Sitao Luan, Chenqing Hua, Qincheng Lu, Jiaqi Zhu, Mingde Zhao, Shuyuan Zhang, Xiao-Wen Chang, and Doina Precup. Revisiting heterophily for graph neural networks. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [24] Yimeng Min, Frederik Wenkel, and Guy Wolf. Scattering gcn: Overcoming oversmoothness in graph convolutional networks. _Advances in Neural Information Processing Systems_, 33:14498-14508, 2020.
* [25] Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In _Proceedings of the 27th international conference on machine learning (ICML-10)_, pages 807-814, 2010.
* [26] Hoang Nt and Takanori Maehara. Revisiting graph neural networks: All we have is low-pass filters. _arXiv preprint arXiv:1905.09550_, 2019.
* [27] Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node classification. In _International Conference on Learning Representations_, 2020.
* [28] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric graph convolutional networks. In _International Conference on Learning Representations_, 2020.
* [29] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classification. In _International Conference on Learning Representations_, 2020.
* [30] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. In _International Conference on Learning Representations_, 2018.
* [31] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplifying graph convolutional networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pages 6861-6871. PMLR, 09-15 Jun 2019.
* [32] Xinyi Wu, Zhengdao Chen, William Wei Wang, and Ali Jadbabaie. A non-asymptotic analysis of oversmoothing in graph neural networks. In _The Eleventh International Conference on Learning Representations_, 2023.

* Wu et al. [2020] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A comprehensive survey on graph neural networks. _IEEE transactions on neural networks and learning systems_, 32(1):4-24, 2020.
* Xu et al. [2018] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In _International conference on machine learning_, pages 5453-5462. PMLR, 2018.
* Yang et al. [2016] Zhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with graph embeddings. In _International conference on machine learning_, pages 40-48. PMLR, 2016.
* Ying et al. [2018] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec. Graph convolutional neural networks for web-scale recommender systems. In _Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining_, pages 974-983, 2018.
* Zhao and Akoglu [2020] Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In _International Conference on Learning Representations_, 2020.
* Zhou et al. [2003] Dengyong Zhou, Olivier Bousquet, Thomas Lal, Jason Weston, and Bernhard Scholkopf. Learning with local and global consistency. _Advances in neural information processing systems_, 16, 2003.
* Zhou et al. [2020] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applications. _AI Open_, 1:57-81, 2020.
* Zhou et al. [2021] Kaixiong Zhou, Xiao Huang, Daochen Zha, Rui Chen, Li Li, Soo-Hyun Choi, and Xia Hu. Dirichlet energy constrained learning for deep graph neural networks. _Advances in Neural Information Processing Systems_, 34:21834-21846, 2021.
* Zhu et al. [2003] Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using gaussian fields and harmonic functions. In _Proceedings of the 20th International conference on Machine learning (ICML-03)_, pages 912-919, 2003.

## Appendix for "Learning to Control the Smoothness of GCN Features"

### Details of Notations

For two vectors \(\bm{u}=(u_{1},u_{2},\ldots,u_{d})\) and \(\bm{v}=(v_{1},v_{2},\ldots,v_{d})\), their inner product is defined as

\[\langle\bm{u},\bm{v}\rangle=\sum_{i=1}^{d}u_{i}v_{i},\]

their Hadamard product is defined as

\[\bm{u}\odot\bm{v}=(u_{1}v_{1},u_{2}v_{2},\ldots,u_{d}v_{d}),\]

and their Kronecker product is defined as

\[\bm{u}\otimes\bm{v}=\bm{uv}^{\top}=\begin{pmatrix}u_{1}v_{1}&u_{1}v_{2}& \ldots&u_{1}v_{d}\\ u_{2}v_{1}&u_{2}v_{2}&\ldots&u_{2}v_{d}\\ \vdots&\vdots&\ddots&\vdots\\ u_{d}v_{1}&u_{d}v_{2}&\ldots&u_{d}v_{d}\end{pmatrix}.\]

The Kronecker product can be defined for two vectors of different lengths in a similar manner as above.

### Proofs in Section 3

First, we prove that the two smoothness notions used in [27, 4] are two equivalent seminorms, i.e., we prove Proposition 3.1 below.

Proof of Proposition 3.1.: The matrix \(\bm{H}\) can be decomposed as \(\bm{H}=\sum_{i=1}^{n}\bm{H}\bm{e}_{i}\bm{e}_{i}^{\top}\), where each \(\bm{e}_{i}\) is the eigenvector of \(\bm{G}\) associated with eigenvalue \(\lambda_{i}\). This indicates that

\[\bm{H}\tilde{\Delta} =\bm{H}(\bm{I}-\bm{G})\] \[=\sum_{i=1}^{n}\bm{H}\bm{e}_{i}\bm{e}_{i}^{\top}(\bm{I}-\bm{G})\] \[=\sum_{i=1}^{n}(\bm{H}\bm{e}_{i}\bm{e}_{i}^{\top}-\bm{H}\bm{e}_{i} \bm{e}_{i}^{\top}\bm{G})\] \[=\sum_{i=1}^{n}(\bm{H}\bm{e}_{i}\bm{e}_{i}^{\top}-\bm{H}\bm{e}_{i }(\lambda_{i}\bm{e}_{i})^{\top})\] \[=\sum_{i=1}^{n}(1-\lambda_{i})\bm{H}\bm{e}_{i}\bm{e}_{i}^{\top}\] \[=\sum_{i=m+1}^{n}(1-\lambda_{i})\bm{H}\bm{e}_{i}\bm{e}_{i}^{\top}.\]Then using the fact that \(1-\lambda_{i}\geq 0\) for each \(i\), we obtain

\[\|\bm{H}\|_{E}^{2} =\mathrm{Trace}(\bm{H}\tilde{\Delta}\bm{H}^{\top})\] \[=\mathrm{Trace}\Big{(}\sum_{i=m+1}^{n}(1-\lambda_{i})\bm{H}\bm{e}_ {i}\bm{e}_{i}^{\top}(\sum_{j=1}^{n}\bm{H}\bm{e}_{j}\bm{e}_{j}^{\top})^{\top} \Big{)}\] \[=\mathrm{Trace}\Big{(}\sum_{i=m+1}^{n}\sum_{j=1}^{n}(1-\lambda_{i })\bm{H}\bm{e}_{i}\bm{e}_{i}^{\top}\bm{e}_{j}\bm{e}_{j}^{\top}\bm{H}^{\top} \Big{)}\] \[=\mathrm{Trace}\Big{(}\sum_{i=m+1}^{n}(1-\lambda_{i})\bm{H}\bm{e} _{i}\bm{e}_{i}^{\top}\bm{e}_{i}\bm{e}_{i}^{\top}\bm{H}^{\top}\Big{)}\] \[=\mathrm{Trace}\Big{(}\sum_{i=m+1}^{n}\sqrt{1-\lambda_{i}}\bm{H} \bm{e}_{i}\bm{e}_{i}^{\top}\bm{e}_{i}\bm{e}_{i}^{\top}\bm{H}^{\top}\sqrt{1- \lambda_{i}}\Big{)}\] \[=\mathrm{Trace}\Big{(}\sum_{i=m+1}^{n}\sqrt{1-\lambda_{i}}\bm{H} \bm{e}_{i}\bm{e}_{i}^{\top}(\sum_{j=m+1}^{n}\sqrt{1-\lambda_{j}}\bm{H}\bm{e}_ {j}\bm{e}_{j}^{\top})^{\top}\Big{)}\] \[=\Big{\|}\sum_{i=m+1}^{n}\sqrt{1-\lambda_{i}}\bm{H}\bm{e}_{i}\bm{e }_{i}^{\top}\Big{\|}_{F}^{2}.\]

That is,

\[\|\bm{H}\|_{E}=\Big{\|}\sum_{i=m+1}^{n}\sqrt{1-\lambda_{i}}\bm{H}\bm{e}_{i}\bm {e}_{i}^{\top}\Big{\|}_{F}.\]

On the other hand, (3) implies

\[\|\bm{H}\|_{\mathcal{M}^{\perp}}=\|\bm{H}_{\mathcal{M}^{\perp}}\|_{F}=\Big{\|} \sum_{i=m+1}^{n}\bm{H}\bm{e}_{i}\bm{e}_{i}^{\top}\Big{\|}_{F}.\]

We first show that both \(\|\bm{H}\|_{\mathcal{M}^{\perp}}\) and \(\|\bm{H}\|_{E}\) are seminorms. Since \(\|c\bm{H}\|_{F}=|c|\cdot\|\bm{H}\|_{F}\) for any \(c\in\mathbb{R}\), we have \(\|c\bm{H}\|_{\mathcal{M}^{\perp}}=|c|\cdot\|\bm{H}\|_{\mathcal{M}^{\perp}}\) and \(\|c\bm{H}\|_{E}=|c|\cdot\|\bm{H}\|_{E}\). Moreover, for any two matrices \(\bm{H}^{1}\) and \(\bm{H}^{2}\) s.t. \(\bm{H}=\bm{H}^{1}+\bm{H}^{2}\), we have

\[\sum_{i=m+1}^{n}\bm{H}^{1}\bm{e}_{i}\bm{e}_{i}^{\top}+\sum_{i=m+ 1}^{n}\bm{H}^{2}\bm{e}_{i}\bm{e}_{i}^{\top}=\sum_{i=m+1}^{n}\bm{H}\bm{e}_{i}\bm {e}_{i}^{\top},\] \[\sum_{i=m+1}^{n}\sqrt{1-\lambda_{i}}\bm{H}^{1}\bm{e}_{i}\bm{e}_{i }^{\top}+\sum_{i=m+1}^{n}\sqrt{1-\lambda_{i}}\bm{H}^{2}\bm{e}_{i}\bm{e}_{i}^{ \top}=\sum_{i=m+1}^{n}\sqrt{1-\lambda_{i}}\bm{H}\bm{e}_{i}\bm{e}_{i}^{\top}.\]

Then the triangle inequality of \(\|\cdot\|_{F}\) implies that of \(\|\bm{H}\|_{\mathcal{M}^{\perp}}\) and \(\|\bm{H}\|_{E}\), respectively.

Now since \(0<1-\lambda_{m+1}\leq 1-\lambda_{i}\leq 2\) for any \(i=m+1,\ldots,n\), we may take \(\alpha=\sqrt{1-\lambda_{m+1}}\) and \(\beta=\sqrt{2}\). Then

\[\alpha\|\bm{H}\|_{\mathcal{M}^{\perp}}=\Big{\|}\alpha\sum_{i=m+1}^ {n}\bm{H}\bm{e}_{i}\bm{e}_{i}^{\top}\Big{\|}_{F} \leq\Big{\|}\sum_{i=m+1}^{n}\sqrt{1-\lambda_{i}}\bm{H}\bm{e}_{i} \bm{e}_{i}^{\top}\Big{\|}_{F}\] \[\leq\Big{\|}\beta\sum_{i=m+1}^{n}\bm{H}\bm{e}_{i}\bm{e}_{i}^{\top} \Big{\|}_{F}\] \[=\beta\|\bm{H}\|_{\mathcal{M}^{\perp}}.\]

The result thus follows from \(\|\bm{H}\|_{E}=\Big{\|}\sum_{i=m+1}^{n}\sqrt{1-\lambda_{i}}\bm{H}\bm{e}_{i}\bm{ e}_{i}^{\top}\Big{\|}_{F}\). 

### ReLU

We present a crucial tool to characterize how ReLU affects its input.

**Lemma B.1**.: _Let \(\bm{Z}\in\mathbb{R}^{d\times n}\), and let \(\bm{Z}^{+}=\max(\bm{Z},0)\) and \(\bm{Z}^{-}=\max(-\bm{Z},0)\) be the positive and negative parts of \(\bm{Z}\). Then (1) \(\bm{Z}^{+},\bm{Z}^{-}\) are (component-wise) nonnegative and \(\bm{Z}=\bm{Z}^{+}-\bm{Z}^{-}\) and (2) \(\langle\bm{Z}^{+},\bm{Z}^{-}\rangle_{F}=0\)._

Proof of Lemma b.1.: Notice that for any \(a\in\mathbb{R}\), we have

\[\max(a,0)=\begin{cases}a&\text{if }a\geq 0\\ 0&\text{otherwise}\end{cases}\text{ and }\max(-a,0)=\begin{cases}0&\text{if }a\geq 0\\ -a&\text{otherwise}\end{cases}.\]

This implies that \(a=\max(a,0)-\max(-a,0)\) and \(\max(a,0)\cdot\max(-a,0)=0\).

Let \(Z_{ij}\) be the \((i,j)^{th}\) entry of \(\bm{Z}\). Then \(\bm{Z}=\bm{Z}^{+}-\bm{Z}^{-}\) follows from \(Z_{ij}=\max(Z_{ij},0)-\max(-Z_{ij},0)\). Also, one can deduce that

\[\langle\bm{Z}^{+},\bm{Z}^{-}\rangle_{F}=\mathrm{Trace}((\bm{Z}^{+})^{\top}\bm {Z}^{-})=\sum_{i=1}^{d}\sum_{j=1}^{j}\max(Z_{ij},0)\max(-Z_{ij},0)=0.\]

Before proving Proposition 3.2, we notice the following relation between \(\bm{Z}\) and \(\bm{H}\).

**Lemma B.2**.: _Given \(\bm{Z}\in\mathbb{R}^{d\times n}\), let \(\bm{H}=\sigma(\bm{Z})\) with \(\sigma\) being ReLU, then \(\bm{H}\) lies on the high-dimensional sphere, in \(\|\cdot\|_{F}\) norm, that is centered at \(\bm{Z}/2\) and with radius \(\|\bm{Z}/2\|_{F}\). That is, \(\bm{H}\) and \(\bm{Z}\) satisfy the following equation_

\[\Big{\|}\bm{H}-\frac{\bm{Z}}{2}\Big{\|}_{F}^{2}=\Big{\|}\frac{\bm{Z}}{2}\Big{\|} _{F}^{2}.\] (8)

Proof of Lemma b.2.: We observe that \(\bm{H}=\sigma(\bm{Z})=\max(\bm{Z},0)=\bm{Z}^{+}\) is the positive part of \(\bm{Z}\). Then

\[\langle\bm{H},\bm{Z}\rangle_{F}=\langle\bm{H},\bm{Z}^{+}-\bm{Z}^{-}\rangle_{F }=\langle\bm{H},\bm{Z}^{+}\rangle_{F}-\langle\bm{H},\bm{Z}^{-}\rangle_{F}= \langle\bm{H},\bm{H}\rangle_{F},\]

where we have used \(\bm{Z}=\bm{Z}^{+}-\bm{Z}^{-}\) and \(\langle\bm{H},\bm{Z}^{-}\rangle_{F}=\langle\bm{Z}^{+},\bm{Z}^{-}\rangle_{F}=0\) from Lemma B.1.

Therefore, one can deduce the desired result as follows

\[\langle\bm{H},\bm{H}\rangle_{F}-\langle\bm{H},\bm{Z}\rangle_{F}=0\Rightarrow \|\bm{H}\|_{F}^{2}-2\Big{\langle}\bm{H},\frac{\bm{Z}}{2}\Big{\rangle} _{F}+\Big{\|}\frac{\bm{Z}}{2}\Big{\|}_{F}^{2}=\Big{\|}\frac{\bm{Z}}{2}\Big{\|} _{F}^{2}\] \[\Rightarrow \Big{\|}\bm{H}-\frac{\bm{Z}}{2}\Big{\|}_{F}^{2}=\Big{\|}\frac{ \bm{Z}}{2}\Big{\|}_{F}^{2}.\]

Applying \(\|\bm{H}\|_{F}^{2}=\|\bm{H}_{\mathcal{M}}+\bm{H}_{\mathcal{M}^{\perp}}\|_{F}^{ 2}=\|\bm{H}_{\mathcal{M}}\|_{F}^{2}+\|\bm{H}_{\mathcal{M}^{\perp}}\|_{F}^{2}\), to both \(\frac{\bm{Z}}{2}\) and \(\bm{H}-\frac{\bm{Z}}{2}\), we obtain

\[\Big{\|}\frac{\bm{Z}}{2}\Big{\|}_{F}^{2}=\Big{\|}\frac{\bm{Z}_{\mathcal{M}^{ \perp}}}{2}\Big{\|}_{F}^{2}+\Big{\|}\frac{\bm{Z}_{\mathcal{M}}}{2}\Big{\|}_{F} ^{2},\]

and

\[\Big{\|}\bm{H}-\frac{\bm{Z}}{2}\Big{\|}_{F}^{2}=\Big{\|}\bm{H}_{\mathcal{M}^{ \perp}}-\frac{\bm{Z}_{\mathcal{M}^{\perp}}}{2}\Big{\|}_{F}^{2}+\Big{\|}\bm{H}_ {\mathcal{M}}-\frac{\bm{Z}_{\mathcal{M}}}{2}\Big{\|}_{F}^{2}.\]

Then (8) becomes

\[\Big{\|}\frac{\bm{Z}_{\mathcal{M}^{\perp}}}{2}\Big{\|}_{F}^{2}-\Big{\|}\bm{H}_ {\mathcal{M}^{\perp}}-\frac{\bm{Z}_{\mathcal{M}^{\perp}}}{2}\Big{\|}_{F}^{2}= \Big{\|}\bm{H}_{\mathcal{M}}-\frac{\bm{Z}_{\mathcal{M}}}{2}\Big{\|}_{F}^{2}- \Big{\|}\frac{\bm{Z}_{\mathcal{M}}}{2}\Big{\|}_{F}^{2}\] (9)

By direct calculation, we have

\[\Big{\|}\bm{H}_{\mathcal{M}}-\frac{\bm{Z}_{\mathcal{M}}}{2}\Big{\|}_{F}^{2}- \Big{\|}\frac{\bm{Z}_{\mathcal{M}}}{2}\Big{\|}_{F}^{2} =\langle\bm{H}_{\mathcal{M}},\bm{H}_{\mathcal{M}}\rangle_{F}-2 \Big{\langle}\bm{H}_{\mathcal{M}},\frac{\bm{Z}_{\mathcal{M}}}{2}\Big{\rangle}_{F}\] (10) \[=\langle\bm{H}_{\mathcal{M}},\bm{H}_{\mathcal{M}}-\bm{Z}_{\mathcal{M }}\rangle_{F}.\]

Combining (9) and (10), we obtain the following result

**Lemma B.3**.: _For any \(\bm{Z}=\bm{Z}_{\mathcal{M}}+\bm{Z}_{\mathcal{M}^{\perp}}\), let \(\bm{H}=\sigma(\bm{Z})=\bm{H}_{\mathcal{M}}+\bm{H}_{\mathcal{M}^{\perp}}\), then_

\[\left\|\frac{\bm{Z}_{\mathcal{M}^{\perp}}}{2}\right\|_{F}^{2}-\left\|\bm{H}_{ \mathcal{M}^{\perp}}-\frac{\bm{Z}_{\mathcal{M}^{\perp}}}{2}\right\|_{F}^{2}= \langle\bm{Z}_{\mathcal{M}}^{+},\bm{Z}_{\mathcal{M}}^{-}\rangle_{F}.\]

_where \(\bm{Z}_{\mathcal{M}}^{+}=\sum_{i=1}^{m}\bm{Z}^{+}\bm{e}_{i}\bm{e}_{i}^{\top}, \bm{Z}_{\mathcal{M}}^{-}=\sum_{i=1}^{m}\bm{Z}^{-}\bm{e}_{i}\bm{e}_{i}^{\top}\)._

Proof of Lemma b.3.: Recall that \(\bm{H}=\sigma(\bm{Z})=\max(\bm{Z},0)=\bm{Z}^{+}\). Also, \(\bm{Z}=\bm{Z}^{+}-\bm{Z}^{-}\) implies \(\bm{Z}_{\mathcal{M}}=\bm{Z}_{\mathcal{M}}^{+}-\bm{Z}_{\mathcal{M}}^{-}=\bm{ H}_{\mathcal{M}}^{+}-\bm{Z}_{\mathcal{M}}^{-}\). Therefore, we see that

\[\langle\bm{H}_{\mathcal{M}},\bm{H}_{\mathcal{M}}-\bm{Z}_{\mathcal{M}}\rangle _{F}=\langle\bm{Z}_{\mathcal{M}}^{+},\bm{Z}_{\mathcal{M}}^{-}\rangle_{F}.\]

By using the fact that \(\langle\bm{Z}_{\mathcal{M}}^{+},\bm{Z}_{\mathcal{M}}^{-}\rangle_{F}\geq 0\) in Lemma B.3, we reveal a geometric relation between \(\bm{Z}\) and \(\bm{H}\) mentioned in Proposition 3.2.

Proof of Proposition 3.2.: Since \(\bm{Z}^{+},\bm{Z}^{-}\geq 0\) are nonnegative and all the eigenvectors \(\bm{e}_{i}\) are also nonnegative, we see that \(\bm{Z}_{\mathcal{M}}^{+}=\sum_{i=1}^{m}\bm{Z}^{\pm}\bm{e}_{i}\bm{e}_{i}^{\top}\) and \(\bm{Z}_{\mathcal{M}}^{-}=\sum_{i=1}^{m}\bm{Z}^{-}\bm{e}_{i}\bm{e}_{i}^{\top}\) are nonnegative. This indicates that

\[\langle\bm{Z}_{\mathcal{M}}^{+},\bm{Z}_{\mathcal{M}}^{-}\rangle_{F}=\mathrm{ Trace}\Big{(}\bm{Z}_{\mathcal{M}}^{+}(\bm{Z}_{\mathcal{M}}^{-})^{\top}\Big{)} \geq 0.\]

Then according to Lemma B.3, we obtain

\[\left\|\frac{\bm{Z}_{\mathcal{M}^{\perp}}}{2}\right\|_{F}^{2}-\left\|\bm{H}_{ \mathcal{M}^{\perp}}-\frac{\bm{Z}_{\mathcal{M}^{\perp}}}{2}\right\|_{F}^{2}= \langle\bm{Z}_{\mathcal{M}}^{+},\bm{Z}_{\mathcal{M}}^{-}\rangle_{F}\geq 0.\]

So we have

\[\left\|\bm{H}_{\mathcal{M}^{\perp}}-\frac{\bm{Z}_{\mathcal{M}^{ \perp}}}{2}\right\|_{F} =\sqrt{\left\|\frac{\bm{Z}_{\mathcal{M}^{\perp}}}{2}\right\|_{F}^{2 }-\langle\bm{Z}_{\mathcal{M}}^{+},\bm{Z}_{\mathcal{M}}^{-}\rangle_{F}}\] \[=\sqrt{\left\|\frac{\bm{Z}_{\mathcal{M}}}{2}\right\|_{F}^{2}- \langle\bm{H}_{\mathcal{M}},\bm{H}_{\mathcal{M}}-\bm{Z}_{\mathcal{M}}\rangle_ {F}},\]

which shows that \(\bm{H}_{\mathcal{M}^{\perp}}\) lies on the high-dimensional sphere that we have claimed. Furthermore, we conclude that

\[0\leq\left\|\bm{H}_{\mathcal{M}^{\perp}}-\frac{\bm{Z}_{\mathcal{M}^{\perp}}}{ 2}\right\|_{F}\leq\left\|\frac{\bm{Z}_{\mathcal{M}^{\perp}}}{2}\right\|_{F}.\] (11)

This demonstrates that \(\bm{H}_{\mathcal{M}^{\perp}}\) lies on the high-dimensional sphere we have stated.

Since the sphere \(\left\|\bm{H}_{\mathcal{M}^{\perp}}-\frac{\bm{Z}_{\mathcal{M}^{\perp}}}{2} \right\|_{F}^{2}=\left\|\frac{\bm{Z}_{\mathcal{M}^{\perp}}}{2}\right\|_{F}^{2}\) passes through the origin, the distance of any \(\bm{H}_{\mathcal{M}^{\perp}}\) to the origin must be no greater than the diameter of this sphere, i.e., \(\|\bm{H}_{\mathcal{M}^{\perp}}\|_{F}\leq\|\bm{Z}_{\mathcal{M}^{\perp}}\|_{F}\). Also, this can be derived from

\[\|\bm{H}_{\mathcal{M}^{\perp}}\|_{F}-\left\|\frac{\bm{Z}_{\mathcal{M}^{\perp}}}{ 2}\right\|_{F}\leq\left\|\bm{H}_{\mathcal{M}^{\perp}}-\frac{\bm{Z}_{\mathcal{M}^ {\perp}}}{2}\right\|_{F}\leq\left\|\frac{\bm{Z}_{\mathcal{M}^{\perp}}}{2} \right\|_{F}.\]

One can see that the maximal smoothness \(\|\bm{H}_{\mathcal{M}^{\perp}}\|_{F}=\|\bm{Z}_{\mathcal{M}^{\perp}}\|_{F}\) is attained when \(\bm{H}_{\mathcal{M}^{\perp}}=\bm{Z}_{\mathcal{M}^{\perp}}\), the intersection of the surface and the line passing through the center and the origin.

After all, we complete the proof by using the fact that \(\|\bm{Z}_{\mathcal{M}^{\perp}}\|_{F}=\|\bm{Z}\|_{\mathcal{M}^{\perp}}\) for any matrix \(\bm{Z}\), which implies \(\|\bm{H}\|_{\mathcal{M}^{\perp}}=\|\bm{H}_{\mathcal{M}^{\perp}}\|_{F}\leq\|\bm{Z }_{\mathcal{M}^{\perp}}\|_{F}=\|\bm{Z}_{\mathcal{M}^{\perp}}\).

### Leaky ReLU

For the leaky ReLU activation function, we have

**Lemma B.4**.: _If \(\bm{H}=\sigma_{a}(\bm{Z})\) with \(\sigma_{a}\) being leaky ReLU, then \(\bm{H}\) lies on the high-dimensional sphere centered at \((1+a)\bm{Z}/2\) with radius \(\|(1-a)\bm{Z}/2\|_{F}\)._Proof of Lemma b.4.: Notice that

\[\bm{H}=\sigma_{a}(\bm{Z})=\bm{Z}^{+}-a\bm{Z}^{-}.\]

Then \(\bm{H}-\bm{Z}=(1-a)\bm{Z}^{-}\) and \(\bm{H}-a\bm{Z}=(1-a)\bm{Z}^{+}\). Using \(\langle\bm{Z}^{-},\bm{Z}^{+}\rangle_{F}=0\), we have

\[\langle\bm{H}-\bm{Z},\bm{H}-a\bm{Z}\rangle_{F}=0\Rightarrow \|\bm{H}\|_{F}^{2}-2\Big{\langle}\bm{H},\frac{(1+a)\bm{Z}}{2} \Big{\rangle}_{F}+a\|\bm{Z}\|_{F}^{2}=0\] \[\Rightarrow \|\bm{H}\|_{F}^{2}-2\Big{\langle}\bm{H},\frac{(1+a)\bm{Z}}{2} \Big{\rangle}_{F}=-a\|\bm{Z}\|_{F}^{2}\] \[\Rightarrow \Big{\|}\bm{H}-\frac{(1+a)}{2}\bm{Z}\Big{\|}_{F}^{2}=\Big{\|} \frac{(1+a)}{2}\bm{Z}\Big{\|}_{F}^{2}-a\|\bm{Z}\|_{F}^{2}=\Big{\|}\frac{(1-a )}{2}\bm{Z}\Big{\|}_{F}^{2}.\]

Moreover, we notice that

**Lemma B.5**.: _For any \(\bm{Z}=\bm{Z}_{\mathcal{M}}+\bm{Z}_{\mathcal{M}^{\perp}}\), let \(\bm{H}=\sigma_{a}(\bm{Z})=\bm{H}_{\mathcal{M}}+\bm{H}_{\mathcal{M}^{\perp}}\), then_

\[\Big{\|}\frac{(1-a)}{2}\bm{Z}_{\mathcal{M}^{\perp}}\Big{\|}_{F}^{2}-\Big{\|} \bm{H}_{\mathcal{M}^{\perp}}-\frac{(1+a)}{2}\bm{Z}_{\mathcal{M}^{\perp}} \Big{\|}_{F}^{2}=(1-a)^{2}\langle\bm{Z}_{\mathcal{M}}^{+},\bm{Z}_{\mathcal{M }}^{-}\rangle_{F}\]

Proof of Lemma b.5.: Similar to the proof of Lemma B.3, the orthogonal decomposition implies that

\[\Big{\|}\frac{(1-a)}{2}\bm{Z}_{\mathcal{M}^{\perp}}\Big{\|}_{F}^ {2}-\Big{\|}\bm{H}_{\mathcal{M}^{\perp}}-\frac{(1+a)}{2}\bm{Z}_{\mathcal{M}^{ \perp}}\Big{\|}_{F}^{2}= \Big{\|}\bm{H}_{\mathcal{M}}-\frac{(1+a)}{2}\bm{Z}_{\mathcal{M}} \Big{\|}_{F}^{2}-\Big{\|}\frac{(1-a)}{2}\bm{Z}_{\mathcal{M}}\Big{\|}_{F}^{2}\] \[= \langle\bm{H}_{\mathcal{M}}-\bm{Z}_{\mathcal{M}},\bm{H}_{\mathcal{ M}}-a\bm{Z}_{\mathcal{M}}\rangle_{F}\] \[= \langle(1-a)\bm{Z}_{\mathcal{M}}^{-},(1-a)\bm{Z}_{\mathcal{M}}^{+ }\rangle_{F}\] \[= (1-a)^{2}\langle\bm{Z}_{\mathcal{M}}^{-},\bm{Z}_{\mathcal{M}}^{+ }\rangle_{F}.\]

Proof of Proposition 3.3.: Similar to the proof of Proposition 3.2, we apply \(\langle\bm{Z}_{\mathcal{M}}^{-},\bm{Z}_{\mathcal{M}}^{+}\rangle_{F}\geq 0\) to Lemma B.5 and hence obtain the geometric condition as follows

\[\Big{\|}\bm{H}_{\mathcal{M}^{\perp}}-\frac{(1+a)}{2}\bm{Z}_{ \mathcal{M}^{\perp}}\Big{\|}_{F}=\sqrt{\Big{\|}\frac{(1-a)}{2}\bm{Z}_{ \mathcal{M}^{\perp}}\Big{\|}_{F}^{2}-\langle\bm{H}_{\mathcal{M}}-\bm{Z}_{ \mathcal{M}},\bm{H}_{\mathcal{M}}-a\bm{Z}_{\mathcal{M}}\rangle_{F}}.\]

Then we have the following inequality

\[0\leq\Big{\|}\bm{H}_{\mathcal{M}^{\perp}}-\frac{(1+a)}{2}\bm{Z}_{ \mathcal{M}^{\perp}}\Big{\|}_{F}\leq\Big{\|}\frac{(1-a)}{2}\bm{Z}_{\mathcal{ M}^{\perp}}\Big{\|}_{F}.\]

Moreover, we deduce that

\[\bigg{\|}\|\bm{H}_{\mathcal{M}^{\perp}}\|_{F}-\Big{\|}\frac{(1+a)}{2}\bm{Z}_{ \mathcal{M}^{\perp}}\Big{\|}_{F}\bigg{|}\leq\Big{\|}\bm{H}_{\mathcal{M}^{ \perp}}-\frac{(1+a)}{2}\bm{Z}_{\mathcal{M}^{\perp}}\Big{\|}_{F}\leq\Big{\|} \frac{(1-a)}{2}\bm{Z}_{\mathcal{M}^{\perp}}\Big{\|}_{F}.\]

and hence

\[-\Big{\|}\frac{(1-a)}{2}\bm{Z}_{\mathcal{M}^{\perp}}\Big{\|}_{F}\leq\|\bm{H}_ {\mathcal{M}^{\perp}}\|_{F}-\Big{\|}\frac{(1+a)}{2}\bm{Z}_{\mathcal{M}^{\perp }}\Big{\|}_{F}\leq\Big{\|}\frac{(1-a)}{2}\bm{Z}_{\mathcal{M}^{\perp}}\Big{\|} _{F}.\]

Therefore, we obtain \(a\|\bm{Z}_{\mathcal{M}^{\perp}}\|_{F}\leq\|\bm{H}_{\mathcal{M}^{\perp}}\|_{F} \leq\|\bm{Z}_{\mathcal{M}^{\perp}}\|_{F}\). (Remark that \(\bm{H}_{\mathcal{M}^{\perp}}\) achieves its maximal norm when it is equal to \(\bm{Z}_{\mathcal{M}^{\perp}}\), the intersection of the surface and the line passing through the center and the origin. )

By using the fact that \(\|\bm{Z}_{\mathcal{M}^{\perp}}\|_{F}=\|\bm{Z}\|_{\mathcal{M}^{\perp}}\) for any matrix \(\bm{Z}\), we conclude that \(a\|\bm{Z}\|_{\mathcal{M}^{\perp}}\leq\|\bm{H}\|_{\mathcal{M}^{\perp}}\leq\|\bm{Z }\|_{\mathcal{M}^{\perp}}\).

Proofs in Section 4

Throughout this section, we assume that \(\bm{z}_{\mathcal{M}^{\perp}}\neq\bm{0}\).

Proof of Proposition 4.3.: Recall that \(\bm{e}=\tilde{\bm{D}}^{\frac{1}{2}}\bm{u}_{n}/c\) has only positive entries where \(\tilde{\bm{D}}\) is the augmented degree matrix and \(\bm{u}_{n}=[1,\ldots,1]^{\top}\in\mathbb{R}^{n}\) and \(c=\|\tilde{\bm{D}}^{\frac{1}{2}}\bm{u}_{n}\|\). Let \(d_{i}\) be the \(i^{th}\) diagonal entry of \(\tilde{\bm{D}}\). Then we have \(\bm{e}=[\sqrt{d_{1}}/c,\sqrt{d_{2}}/c,\ldots,\sqrt{d_{n}}/c]^{\top}\) and \(c=\sqrt{\sum_{i=1}^{n}d_{i}}\).

Note that \(\bm{z}(\alpha)=\bm{z}-\alpha\bm{e}=\bm{z}-\frac{\alpha}{c}\tilde{\bm{D}}^{ \frac{1}{2}}\bm{u}_{n}=\tilde{\bm{D}}^{\frac{1}{2}}(\tilde{\bm{D}}^{-\frac{1} {2}}\bm{z}-\frac{\alpha}{c}\bm{u}_{n})=\tilde{\bm{D}}^{\frac{1}{2}}(\bm{x}- \frac{\alpha}{c}\bm{u}_{n})\), where we assume \(\bm{x}:=\tilde{\bm{D}}^{-\frac{1}{2}}\bm{z}\). Then we observe that when \(\sigma\) is the ReLU activation function,

\[\bm{h}(\alpha)=\sigma(\bm{z}(\alpha))=\sigma\Big{(}\tilde{\bm{D}}^{\frac{1}{2 }}(\bm{x}-\frac{\alpha}{c}\bm{u}_{n})\Big{)}=\tilde{\bm{D}}^{\frac{1}{2}} \sigma\Big{(}\bm{x}-\frac{\alpha}{c}\bm{u}_{n}\Big{)},\]

and hence

\[\langle\bm{h}(\alpha),\bm{e}\rangle\] \[=\Big{\langle}\sigma\Big{(}\bm{x}-\frac{\alpha}{c}\bm{u}_{n} \Big{)},\tilde{\bm{D}}^{\frac{1}{2}}\bm{e}\Big{\rangle}=\Big{\langle}\sigma \Big{(}\bm{x}-\frac{\alpha}{c}\bm{u}_{n}\Big{)},\tilde{\bm{D}}\bm{u}_{n} \Big{\rangle}.\]

We may now assume \(\bm{x}=[x_{1},\ldots,x_{n}]^{\top}\) is well-ordered s.t. \(x_{1}\geq x_{2}\geq\ldots\geq x_{n}\). Indeed, there is a collection of indices \(\{k_{1},...,k_{l}\}\) s.t.

\[x_{1}=\ldots,x_{k_{1}}\text{ and }x_{k_{1}}>x_{k_{1}+1},\] \[x_{k_{j-1}+1}=\ldots=x_{k_{j}}\text{ and }x_{k_{j}}>x_{k_{j}+1} \text{ for any }j=2,\ldots,l-1,\] \[x_{k_{l-1}+1}=\ldots=x_{k_{l}}\text{ and }k_{l}=n.\]

That is, \(x_{1}=x_{2}=\ldots=x_{k_{1}}>x_{k_{1}+1}=\ldots=x_{k_{2}}>x_{k_{2}+1}=\ldots=x _{k_{3}}>x_{k_{3}+1}\ldots\)

We first restrict the domain of \(\alpha\) s.t. \(\bm{h}(\alpha)\neq 0\). Note that we have

\[\bm{h}(\alpha)=0\Leftrightarrow \sigma\Big{(}\bm{x}-\frac{\alpha}{c}\bm{u}_{n}\Big{)}=0\] \[\Leftrightarrow x_{i}-\frac{\alpha}{c}\leq 0\text{ for }i=1,\ldots,n\] \[\Leftrightarrow x_{1}-\frac{\alpha}{c}\leq 0\] \[\Leftrightarrow \alpha\geq cx_{1}.\]

So we will study the smoothness \(s(\bm{h}(\alpha))\) when \(\alpha<cx_{1}\).

Let \(\epsilon>0\) and consider \(\alpha=c(x_{1}-\epsilon)\). When \(\epsilon\leq x_{1}-x_{k_{1}+1}=x_{1}-x_{k_{2}}\), we see that

\[\bm{x}-\frac{\alpha}{c}\bm{u}_{n}=[\epsilon,\ldots,\epsilon,\epsilon-(x_{1}-x _{k_{1}+1}),\ldots,\epsilon-(x_{1}-x_{n})]^{\top},\]

where only the first \(k_{1}\) entries are positive since \(x_{1}-x_{i}\geq\epsilon\) for any \(i\geq k_{1}+1\). Therefore,

\[\bm{h}(\alpha)=\tilde{\bm{D}}^{\frac{1}{2}}\sigma\Big{(}\bm{x}- \frac{\alpha}{c}\bm{u}_{n}\Big{)} =\tilde{\bm{D}}^{\frac{1}{2}}[\epsilon,\ldots,\epsilon,0,\ldots,0] ^{\top}\] \[=[\epsilon\sqrt{d_{1}},\ldots,\epsilon\sqrt{d_{k_{1}}},0,\ldots,0 ]^{\top}.\]

and hence we can compute that \(\|\bm{h}(\alpha)\|=\epsilon\sqrt{\sum_{i=1}^{k_{1}}d_{i}}\). Also, we have

\[\|\bm{h}(\alpha)\|_{\mathcal{M}}=|\langle\bm{h}(\alpha),\bm{e}\rangle| =[\epsilon\sqrt{d_{1}},\ldots,\epsilon\sqrt{d_{k_{1}}},0,\ldots,0 ]^{\top}[\sqrt{d_{1}}/c,\sqrt{d_{2}}/c,\ldots,\sqrt{d_{n}}/c]\] \[=\frac{\epsilon}{c}\sum_{i=1}^{k_{1}}d_{i}.\]

Then we obtain the smoothness \(s(\bm{h}(\alpha))\) as follows

\[s(\bm{h}(\alpha))=\frac{\|\bm{h}(\alpha)\|_{\mathcal{M}}}{\|\bm{h}(\alpha)\|}= \frac{\frac{\epsilon}{c}\sum_{i=1}^{k_{1}}d_{i}}{\epsilon\sqrt{\sum_{i=1}^{k_{1}} d_{i}}}=\frac{\sqrt{\sum_{i=1}^{k_{1}}d_{i}}}{c}=\frac{K_{1}}{c}<1,\]where \(K_{1}\coloneqq\sqrt{\sum_{i=1}^{k_{1}}d_{i}}\). Similarly, we may denote \(\sqrt{\sum_{i=k_{j-1}+1}^{k_{j}}d_{i}}\) by \(K_{j}\) for \(j=2,\ldots,l\).

Now we are going to show that the smoothness \(s(\bm{h}(\alpha))\) is increasing as \(\alpha\) gets smaller whenever \(\alpha<cx_{1}\), implying \(\frac{K_{1}}{c}\) is the minimum of the smoothness \(s(\bm{h}(\alpha))\). Remember that we are considering \(\alpha=c(x_{1}-\epsilon)\) and we have studied the case when \(0<\epsilon\leq x_{1}-x_{k_{1}+1}=x_{1}-x_{k_{2}}\).

Let \(\delta_{j}\coloneqq x_{1}-x_{k_{j}}\) for \(1\leq j\leq l\). Clearly, we have \(\delta_{1}=0\) and \(\delta_{j}<\delta_{j+1}\) for \(1\leq j\leq l-1\). Fix a \(j^{\prime}\in\{2,\ldots,l-1\}\), we see that when \(\delta_{j^{\prime}}<\epsilon\leq x_{1}-x_{k_{j^{\prime}}+1}\),

\[\bm{x}-\frac{\alpha}{c}\bm{u}_{n}\] \[=\Big{[}\epsilon-\delta_{1},\ldots,\epsilon-\delta_{1},\epsilon -\delta_{2},\ldots,\epsilon-\delta_{2},\epsilon-\delta_{3},\ldots,\epsilon- \delta_{j^{\prime}},\epsilon-(x_{1}-x_{k_{j^{\prime}}+1}),\ldots,\epsilon-(x _{1}-x_{n})\Big{]}^{\top},\]

where we have \(\epsilon-\delta_{j}>0\) for \(2\leq j\leq j^{\prime}\) and \(\epsilon-(x_{1}-x_{i})\leq 0\) for any \(i\geq k_{j^{\prime}}+1\). Consequently,

\[\bm{h}(\alpha)=\tilde{\bm{D}}^{\frac{1}{2}}\sigma(\bm{x}-\frac{ \alpha}{c}\bm{u}_{n})=[(\epsilon-\delta_{1})\sqrt{d_{1}},\ldots,(\epsilon- \delta_{1})\sqrt{d_{k_{1}}},(\epsilon-\delta_{2})\sqrt{d_{k_{1}+1}},\ldots,( \epsilon-\delta_{2})\sqrt{d_{k_{2}}},\] \[(\epsilon-\delta_{3})\sqrt{d_{k_{2}+1}},\ldots,(\epsilon-\delta_ {j^{\prime}})\sqrt{d_{k_{j^{\prime}}}},0,\ldots,0]^{\top}.\]

Then we can compute

\[\|\bm{h}(\alpha)\|=\sqrt{\sum_{j=1}^{j^{\prime}}\sum_{i=k_{j-1}+1}^{k_{j}}d_{i }(\epsilon-\delta_{j})^{2}}=\sqrt{\sum_{j=1}^{j^{\prime}}K_{j}^{2}(\epsilon- \delta_{j})^{2}},\]

where we set \(k_{0}\coloneqq 0\) for simplicity and \(K_{j}=\sqrt{\sum_{i=k_{j-1}+1}^{k_{j}}d_{i}}\) for \(j=1,\ldots,j^{\prime}\). Also, we have

\[\|\bm{h}(\alpha)\|_{\mathcal{M}}=|\langle\bm{h}(\alpha),\bm{e} \rangle|=\sum_{j=1}^{j^{\prime}}\sum_{i=k_{j-1}+1}^{k_{j}}\frac{d_{i}(\epsilon -\delta_{j})}{c}=\frac{1}{c}\sum_{j=1}^{j^{\prime}}K_{j}^{2}(\epsilon-\delta_ {j}).\]

A careful calculation shows that \(\frac{\partial}{\partial\epsilon}s(\bm{h}(\alpha))>0\) whenever \(\delta_{j^{\prime}}<\epsilon\leq x_{1}-x_{k_{j^{\prime}}+1}\) which implies that \(s(\bm{h}(\alpha))\) is increasing as \(\epsilon\) increases. Indeed, we have

\[\frac{\partial}{\partial\epsilon}s(\bm{h}(\alpha))\] \[= \frac{\partial}{\partial\epsilon}\bigg{(}\frac{\sum_{j=1}^{j^{ \prime}}K_{j}^{2}(\epsilon-\delta_{j})}{c\sqrt{\sum_{j=1}^{j^{\prime}}K_{j}^{2 }(\epsilon-\delta_{j})^{2}}}\bigg{)}\] \[= \frac{\Big{(}\frac{\partial}{\partial\epsilon}\sum_{j=1}^{j^{ \prime}}K_{j}^{2}(\epsilon-\delta_{j})\Big{)}\sqrt{\sum_{j=1}^{j^{\prime}}K_{j}^{ 2}(\epsilon-\delta_{j})^{2}}-\sum_{j=1}^{j^{\prime}}K_{j}^{2}(\epsilon-\delta_ {j})\Big{(}\frac{\partial}{\partial\epsilon}\sqrt{\sum_{j=1}^{j^{\prime}}K_{j}^ {2}(\epsilon-\delta_{j})^{2}}\Big{)}}{c\sum_{j=1}^{j^{\prime}}K_{j}^{2}( \epsilon-\delta_{j})^{2}}\] \[= \frac{\Big{(}\sum_{j=1}^{j^{\prime}}K_{j}^{2}\Big{)}\sqrt{\sum_{j= 1}^{j^{\prime}}K_{j}^{2}(\epsilon-\delta_{j})^{2}}-\sum_{j=1}^{j^{\prime}}K_{j}^ {2}(\epsilon-\delta_{j})\Big{(}\frac{\partial}{\partial\epsilon}\frac{\sum_{j= 1}^{j^{\prime}}K_{j}^{2}(\epsilon-\delta_{j})^{2}}{2\sqrt{\sum_{j=1}^{j^{\prime }}K_{j}^{2}(\epsilon-\delta_{j})^{2}}}\Big{)}}{c\sum_{j=1}^{j^{\prime}}K_{j}^ {2}(\epsilon-\delta_{j})^{2}}\] \[= \frac{\Big{(}\sum_{j=1}^{j^{\prime}}K_{j}^{2}\Big{)}\sum_{j=1}^{j^ {\prime}}K_{j}^{2}(\epsilon-\delta_{j})^{2}-\sum_{j=1}^{j^{\prime}}K_{j}^{2}( \epsilon-\delta_{j})\Big{(}\sum_{j=1}^{j^{\prime}}K_{j}^{2}(\epsilon-\delta_{j}) \Big{)}}{c\sum_{j=1}^{j^{\prime}}K_{j}^{2}(\epsilon-\delta_{j})^{2}}.\]

Then to show that \(\frac{\partial}{\partial\epsilon}s(\bm{h}(\alpha))>0\), it suffices to show that the numerator is positive, i.e.

\[\Big{(}\sum_{j=1}^{j^{\prime}}K_{j}^{2}\Big{)}\sum_{j=1}^{j^{\prime}}K_{j}^{2}( \epsilon-\delta_{j})^{2}-\Big{(}\sum_{j=1}^{j^{\prime}}K_{j}^{2}(\epsilon-\delta_ {j})\Big{)}^{2}>0,\]since the denominator \(c\sum_{j=1}^{j^{\prime}}K_{j}^{2}(\epsilon-\delta_{j})^{2}\sqrt{\sum_{j=1}^{j^{ \prime}}K_{j}^{2}(\epsilon-\delta_{j})^{2}}>0\) is always positive. In fact, this follows from the Cauchy inequality \(\|\bm{v}\|\|\|\bm{u}\|\geq\langle\bm{v},\bm{u}\rangle\), where we set

\[\bm{v}\coloneqq[K_{1},K_{2},\dots,K_{J^{\prime}}]^{\top},\ \ \bm{u}\coloneqq[K_{1}( \epsilon-\delta_{1}),K_{2}(\epsilon-\delta_{2}),\dots,K_{j^{\prime}}(\epsilon- \delta_{j^{\prime}})]^{\top}.\]

Moreover, equality happens only when \(\bm{v}\) is parallel to \(\bm{u}\). This is, however, impossible since \(\epsilon-\delta_{j}>\epsilon-\delta_{j+1}\) for any \(j=1,\dots,j^{\prime}-1\) and each \(K_{j}\) is positive.

So we see that \(s(\bm{h}(\alpha))\) is increasing as \(\epsilon\) increases whenever \(0<\epsilon\), and hence the smoothness \(s(\bm{h}(\alpha))\) is increasing as \(\alpha\) decreases whenever \(cx_{n}\leq\alpha<cx_{1}\).

For the case \(j^{\prime}=l\) where \(\delta_{l}=x_{1}-x_{n}<\epsilon\), we have \(x_{n}-\alpha/c=x_{n}-(x_{1}-\epsilon)=\epsilon-(x_{1}-x_{n})>0\), implying \(\alpha<cx_{n}\) and \(\bm{h}(\alpha)=\bm{z}(\alpha)\). We have shown that the smoothness is increasing as \(\alpha\) is going far from \(\langle\bm{z},\bm{e}\rangle\); in particular, when \(\alpha<\langle\bm{z},\bm{e}\rangle\) and \(\alpha\) is decreasing. One can check that

\[cx_{n}=\frac{\sum_{i=1}^{n}d_{i}x_{n}}{c}=\left\langle x_{n}\bm{u}_{n},\frac{ \tilde{\bm{D}}\bm{u}_{n}}{c}\right\rangle\leq\left\langle\bm{x},\frac{\tilde {\bm{D}}\bm{u}_{n}}{c}\right\rangle=\left\langle\tilde{\bm{D}}^{\frac{1}{2}} \bm{x},\frac{\tilde{\bm{D}}^{\frac{1}{2}}\bm{u}_{n}}{c}\right\rangle=\langle \bm{z},\bm{e}\rangle,\]

which means the smoothness is increasing as \(\alpha\) decreases whenever \(\alpha<cx_{n}\).

We conclude that the smoothness increases as \(\alpha\) decreases provided \(\alpha<cx_{1}\). Also, we have \(\sup_{\alpha<cx_{1}}s(\bm{h}(\alpha))=1\) as the case in the proof of Proposition C.1. One can check that \(s(\bm{h}(\alpha))\) is a continuous function for \(\alpha<cx_{1}\) and thus it has range \([K_{1}/c,1)\) by the mean value theorem.

Finally, we can establish the result: \(K_{1}/c=\sqrt{\frac{\sum_{x_{i}=\max x}d_{i}}{\sum_{j=1}^{n}d_{j}}}\) is the minimum of \(s(\bm{h}(\alpha))\) and \(1\) is the maximum of \(s(\bm{h}(\alpha))\) occurring whenever \(\alpha\geq cx_{1}=\sqrt{\sum_{j=1}^{n}d_{j}}\max_{i}x_{i}\). Moreover, \(s(\bm{h}(\alpha))\) has a monotone property when \(\alpha<\sqrt{\sum_{j=1}^{n}d_{j}}\max_{i}x_{i}\) and has range \(\left[\sqrt{\frac{\sum_{x_{j}=\max x}d_{i}}{\sum_{j=1}^{n}d_{j}}},1\right]\).

It is clear that the assumption on the ordering of the entries of \(\bm{x}\) will not affect this result. 

To prove Proposition 4.4, we first prove an analogous result for the identity function, that is, \(\bm{h}=\sigma(\bm{z})=\bm{z}\).

**Proposition C.1**.: _Suppose \(\bm{z}_{\mathcal{M}^{\perp}}\neq\bm{0}\), then \(s(\bm{z}(\alpha))\) achieves its minimum \(0\) if \(\alpha=\langle\bm{z},\bm{e}\rangle\). Moreover, \(\sup_{\alpha}s(\bm{z}(\alpha))=1\) where \(s(\bm{z}(\alpha))\) is close to \(1\) when \(\alpha\) is far away from \(\langle\bm{z},\bm{e}\rangle\)._

Notice that Proposition C.1 does not consider the activation function.

Proof of Proposition C.1.: We know that \(0\leq s(\bm{z}(\alpha))\leq 1\) and

\[s(\bm{z}(\alpha))=\sqrt{1-\frac{\|\bm{z}_{\mathcal{M}^{\perp}}\|^ {2}}{\|\bm{z}(\alpha)\|^{2}}} =\sqrt{1-\frac{\|\bm{z}_{\mathcal{M}^{\perp}}\|^{2}}{\|\bm{z}_{ \mathcal{M}^{\perp}}\|^{2}+\|\bm{z}(\alpha)_{\mathcal{M}}\|^{2}}}\] \[=\sqrt{1-\frac{\|\bm{z}_{\mathcal{M}^{\perp}}\|^{2}}{\|\bm{z}_{ \mathcal{M}^{\perp}}\|^{2}+\|\bm{z}_{\mathcal{M}}-\alpha\bm{e}\|^{2}}}.\]

Suppose \(s(\bm{z}(\alpha))=1\). Then we have \(\frac{\|\bm{z}_{\mathcal{M}^{\perp}}\|^{2}}{\|\bm{z}_{\mathcal{M}^{\perp}}\|^{ 2}+\|\bm{z}_{\mathcal{M}^{\perp}}-\alpha\bm{e}\|^{2}}=0\) which forces \(\|\bm{z}_{\mathcal{M}^{\perp}}\|=0\). However, this contradicts the hypothesis \(\bm{z}_{\mathcal{M}^{\perp}}\neq 0\). So \(s(\bm{z}(\alpha))\) cannot attain its maximum.

But for any \(0\leq t<1\), one can see that \(s(\bm{z}(\alpha))=t\) if and only if

\[\sqrt{1-\frac{\|\bm{z}_{\mathcal{M}^{\perp}}\|^{2}}{\|\bm{z}_{ \mathcal{M}^{\perp}}\|^{2}+\|\bm{z}_{\mathcal{M}}-\alpha\bm{e}\|^{2}}} =t\Leftrightarrow\frac{\|\bm{z}_{\mathcal{M}^{\perp}}\|^{2}}{\| \bm{z}_{\mathcal{M}^{\perp}}\|^{2}+\|\bm{z}_{\mathcal{M}}-\alpha\bm{e}\|^{2}}= 1-t^{2}\] \[\Leftrightarrow\|\bm{z}_{\mathcal{M}^{\perp}}\|^{2}=(1-t^{2}) \big{(}\|\bm{z}_{\mathcal{M}^{\perp}}\|^{2}+\|\bm{z}_{\mathcal{M}}-\alpha\bm{e} \|^{2}\big{)}\] \[\Leftrightarrow t^{2}\|\bm{z}_{\mathcal{M}^{\perp}}\|^{2}=(1-t^{2}) \|\bm{z}_{\mathcal{M}}-\alpha\bm{e}\|^{2}\] \[\Leftrightarrow\|\bm{z}_{\mathcal{M}}-\alpha\bm{e}\|=\sqrt{\frac{t^{2} }{1-t^{2}}}\cdot\|\bm{z}_{\mathcal{M}^{\perp}}\|\]This implies that \(\sup_{\alpha}s(\bm{z}(\alpha))=1\) and \(s(\bm{z}(\alpha))\) achieves its minimum \(0\) if and only if \(\alpha=\langle\bm{z},\bm{e}\rangle\). It is clear that \(s(\bm{z}(\alpha))\) get closer to \(1\) when \(\alpha\) is going far away from \(\langle\bm{z},\bm{e}\rangle\). i.e., \(|\alpha-\langle\bm{z},\bm{e}\rangle|=\|\bm{z}_{\mathcal{M}}-\alpha\bm{e}\|\) is increasing. 

Proof of Proposition 4.4.: First, we notice that leaky ReLU has the following two properties

1. \(\sigma_{a}(x)>0\) for \(x\gg 0\) and \(\sigma_{a}(x)<0\) for \(x\ll 0\).
2. \(\sigma_{a}\) is a non-trivial linear map for \(x\gg 0\).

We will use Property \(1\) to show that \(\min_{\alpha}s(\bm{h}(\alpha))=0\) and Property \(2\) to show that \(\sup_{\alpha}s(\bm{h}(\alpha))=1\). Notice that \(\sigma_{a}(x)<0\) for \(x\ll 0\) implies that there exists a sufficient small \(\alpha_{2}<0\) s.t. all of the entries of \(\bm{h}(\alpha_{2})\) are negative and hence \(|\langle\bm{h}(\alpha_{2}),\bm{e}\rangle|<0\). Similarly, \(\sigma_{a}(x)>0\) for \(x\gg 0\) implies that there exists a sufficient large \(\alpha_{1}>0\) s.t. all of the entries of \(\bm{h}(\alpha_{1})\) are positive and hence \(|\langle\bm{h}(\alpha_{1}),\bm{e}\rangle|>0\). Since \(|\langle\bm{h}(\alpha),\bm{e}\rangle|\) is a continuous function of \(\alpha\) on \([\alpha_{1},\alpha_{2}]\), the Intermediate Value Theorem follows that there exists an \(\alpha\in(\alpha_{1},\alpha_{2})\) s.t. \(|\langle\bm{h}(\alpha),\bm{e}\rangle|=0\). Thus by definition \(s(\bm{h}(\alpha))=|\langle\bm{h}(\alpha),\bm{e}\rangle|/\|\bm{h}(\alpha)\|\), we see that \(\min_{\alpha_{\alpha}}s(\bm{h}(\alpha))=0\).

On the other hand, since \(\sigma_{a}\) is a non-trivial linear map for \(x\gg 0\), we may assume \(\sigma_{a}(x)=cx\) for \(x>x_{0}\) where \(c\neq 0\) is some non-zero constant and \(x_{0}>0\) is some positive constant. Then we can choose an \(\alpha_{0}>\langle\bm{z},\bm{e}\rangle\) s.t. for any \(\alpha\geq\alpha_{0}\), all of the entries of \(\bm{z}(\alpha)\) are greater than \(x_{0}\). Then whenever \(\alpha\geq\alpha_{0}\), we have \(\bm{h}(\alpha)=\sigma_{a}(\bm{z}(\alpha))=c\bm{z}(\alpha)\). This implies

\[s(\bm{h}(\alpha))=\frac{|\langle\bm{h}(\alpha),\bm{e}\rangle|}{\|\bm{h}( \alpha)\|}=\frac{|\langle c\bm{z}(\alpha),\bm{e}\rangle|}{\|c\bm{z}(\alpha)\| }=\frac{|\langle\bm{z}(\alpha),\bm{e}\rangle|}{\|\bm{z}(\alpha)\|}=s(\bm{z}( \alpha)).\]

Thus \(\sup_{\alpha}s(\bm{h}(\alpha))=1\) follows from the Proof of Proposition C.1 where we see that \(\sup_{\alpha}s(\bm{z}(\alpha))=1\) since \(s(\bm{z}(\alpha))\) gets closer to \(1\) as \(\alpha\) increases.

_Remark C.2_.: Indeed, it holds for any continuous function \(f:\mathbb{R}\to\mathbb{R}\) satisfying the following

1. \(f(x)>0\) for \(x\gg 0\), \(f(x)<0\) for \(x\ll 0\) or \(f(x)<0\) for \(x\gg 0\), \(f(x)>0\) for \(x\ll 0\),
2. \(f\) is a non-trivial linear map for \(x\gg 0\) or \(x\ll 0\).

One can check the proof above only depends on these two properties. It is worth mentioning that most activation functions, e.g. leaky LU, SiLU, \(\tanh\), satisfy condition 1.

Proof of Corollary 4.5.: For any \(\alpha\), we notice that \(\|\bm{z}\|_{\mathcal{M}^{\perp}}=\|\bm{z}_{\mathcal{M}^{\perp}}\|_{F}=\|\bm{z} (\alpha)\|_{\mathcal{M}^{\perp}}\) since \(\alpha\) only changes the component of \(\bm{z}\) in the eigenspace \(\mathcal{M}\). Also, Propositions 3.2 and 3.3 show that \(\|\bm{z}(\alpha)\|_{\mathcal{M}^{\perp}}\geq\|\bm{h}(\alpha)\|_{\mathcal{M}^{ \perp}}\) whenever \(\bm{h}(\alpha)=\sigma(\bm{z}(\alpha))\) or \(\sigma_{a}(\bm{z}(\alpha))\). Therefore, we see that \(\|\bm{z}\|_{\mathcal{M}^{\perp}}\geq\|\bm{h}(\alpha)\|_{\mathcal{M}^{ \perp}}\) holds for any \(\alpha\). Since \(\bm{z}_{\mathcal{M}^{\perp}}\neq 0\), \(s(\bm{z})\) must lie in \([0,1)\).

## Appendix D Experimental Details

This part includes the missing details about experimental configurations and additional experimental results for Section 6. All tasks we run using Nvidia RTX 3090, GV100, and Tesla T4 GPUs. All computational performance metrics, including timing procedures, are run using Tesla T4 GPUs from Google Colab.

### Dataset details

In this section, we briefly describe the benchmark datasets used. Table 3 provides additional details about the underlying graph representation.

**Citation Datasets:** The five citation datasets considered are Cora, Citeseer PubMed, Coauthor-Physics, and Ogbn-arxiv. Each dataset is represented by a graph with nodes representing academic publications, features encoding a bag-of-words description, labels classifying the publication type, and edges representing citations.

**Web Knowledge-Base Datasets:** The three web knowledge-base datasets are Cornell, Texas, and Wisconsin. Each dataset is represented by a graph with nodes representing CS department webpages, features encoding a bag-of-words description, edges representing hyper-link connections, and labels classifying the webpage type.

**Wikipedia Network Datasets:** The two Wikipedia network datasets are Chameleon and Squirrel. Each dataset is represented by a graph with nodes representing CS department webpages, features encoding a bag-of-words description, edges representing hyper-link connections, and labels classifying the webpage type.

### Model size and computational time for citation datasets

Table 4 compares the model size and computational time for experiments on citation datasets in Section 6.2.

\begin{table}
\begin{tabular}{c|c c c} \hline \hline  & \# Parameters & Training Time (s) & Inference Time (ms) \\ \hline \multicolumn{4}{c}{**Cora**} \\ \hline GCN & 100,423 & 8.4 & 1.6 \\ GCNII & 110,535 & 10.0 & 2.1 \\ GCNII & 708,743 & 57.6 & 12.3 \\ GCNII-SCT & 123,217,127 & 110.3 & 29.6 \\ EGNN & 121,839 & 65.6 & 14.4 \\ EGNN-SCT & 316,551 & 24.8 & 4.5 \\ \hline \hline \multicolumn{4}{c}{**Citeseer**} \\ \hline GCN & 245,638 & 8.3 & 1.5 \\ GCN-SCT & 301,830 & 15.5 & 4.0 \\ GCNII & 999,174 & 57.6 & 12.3 \\ GCNII-SCT & 1,001,222 & 65.9 & 15.7 \\ EGNN & 739,078 & 39.6 & 7.2 \\ EGNN-SCT & 540,934 & 24.0 & 5.8 \\ \hline \hline \multicolumn{4}{c}{**PubMed**} \\ \hline GCN & 40,451 & 9.0 & 1.8 \\ GCN-SCT & 40,707 & 11.1 & 2.2 \\ GCNII & 326,659 & 98.2 & 12.8 \\ GCNII-SCT & 590,851 & 71.7 & 17.4 \\ EGNN & 592,899 & 93.7 & 2.5 \\ EGNN-SCT & 130,563 & 16.0 & 3.1 \\ \hline \multicolumn{4}{c}{**Coauthor-Physics**} \\ \hline GCN & 547,141 & 35.2 & 8.0 \\ GCN-SCT & 547,397 & 33.9 & 8.3 \\ GCNII & 555,333 & 49.1 & 10.3 \\ GCNII-SCT & 555,461 & 67.0 & 9.5 \\ EGNN & 672,069 & 176.4 & 47.9 \\ EGNN-SCT & 572,229 & 51.7 & 14.8 \\ \hline \hline \multicolumn{4}{c}{**Oghn-arxiv**} \\ \hline GCN & 27,240 & 50.4 & 21.1 \\ GCN-SCT & 28,392 & 62.6 & 24.4 \\ GCNII & 76,392 & 205.4 & 94.8 \\ GCNII-SCT & 80,616 & 253.0 & 108.9 \\ EGNN & 77,416 & 206.8 & 98.0 \\ EGNN-SCT & 81,640 & 254.0 & 112.3 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Number of model parameters for varying numbers of layers using the optimal model hyperparameters. The SCT is added at each layer and the size of the additional parameters scales with the number of eigenvectors with an eigenvalue of one for matrix \(\bm{G}\) in (2).

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline  & **\# Nodes** & **\# Edges** & **\# Features** & **\# Classes** & Splits (Train/Val/Test) \\ \hline Cornell & \(183\) & \(295\) & \(1,703\) & \(5\) & 48/32/20\% \\ Texas & \(181\) & \(309\) & \(1,703\) & \(5\) & 48/32/20\% \\ Wisconsin & \(251\) & \(499\) & \(1,703\) & \(5\) & 48/32/20\% \\ Chameleon & \(2,277\) & \(36,101\) & \(2,325\) & \(5\) & 48/32/20\% \\ Squirrel & \(5,201\) & \(217,073\) & \(2,089\) & \(5\) & 48/32/20\% \\ Citeseer & \(3,727\) & \(4,732\) & \(3,703\) & \(6\) & 120/50/1000 \\ Cora & \(2,708\) & \(5,429\) & \(1,433\) & \(7\) & 140/500/1000 \\ PubMed & \(19,717\) & \(44,338\) & \(500\) & \(3\) & 60/500/1000 \\ Coauthor-Physics & \(34,493\) & \(247,962\) & \(8415\) & \(5\) & 100/150/34,243 \\ Ogbn-arxiv & \(169,343\) & \(1,166,243\) & \(128\) & \(40\) & 90,941/29,799/48,603 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Graph statistics.

[MISSING_PAGE_FAIL:23]

\begin{table}
\begin{tabular}{c|c c c c c c c c} \hline \hline  & \multicolumn{6}{c}{Cens} & \multicolumn{6}{c}{Italy ReLU} \\ \hline Logs & 2 & 4 & 16 & 32 & 2 & 4 & 16 & 32 \\ \hline GCN-ST & 81.2 & 80.3 & 71.4 & 67.2 & 82.9 & 82.8 & 64.0 & 65.5 \\ GCN-ST & 83.5 & 83.8 & 82.7 & 83.3 & 83.8 & 84.8 & 84.8 & 85.5 \\ GCN-ST & 84.1 & 83.8 & 82.3 & 80.8 & 83.7 & 84.5 & 83.3 & 82.0 \\ \hline \hline  & \multicolumn{6}{c}{Censure} & \multicolumn{6}{c}{Italy ReLU} \\ \hline Logs & 2 & 4 & 16 & 32 & 2 & 4 & 16 & 32 \\ \hline GCN-ST & 69.0 & 67.3 & 51.5 & 50.3 & 69.9 & 57.7 & 53.4 & 51.0 \\ GCN-ST & 72.8 & 72.8 & 72.8 & 73.3 & 72.8 & 72.9 & 73.8 & 72.7 \\ GCN-ST & 72.5 & 72.0 & 70.2 & 71.8 & 73.1 & 71.7 & 72.6 & 72.9 \\ \hline \multicolumn{6}{c}{Published} & \multicolumn{6}{c}{Italy ReLU} \\ \hline Logs & 2 & 4 & 16 & 32 & 2 & 4 & 16 & 32 \\ \hline GCN-ST & 73.4 & 82.7 & 75.9 & 77.0 & 78.8 & 78.4 & 78.1 & 78.9 \\ GCN-ST & 79.7 & 80.1 & 80.7 & 80.7 & 79.6 & 80.0 & 80.3 & 80.7 \\ \hline \multicolumn{6}{c}{Censure} & \multicolumn{6}{c}{Censure-Pisots} \\ \hline Logs & 2 & 4 & 16 & 32 & 2 & 4 & 16 & 32 \\ \hline GCN-ST & 91.8 \(\pm\) 1.0 & 91.6 \(\pm\) 3.0 & 44.5 \(\pm\) 13.0 & 42.6 \(\pm\) 17.0 & 92.6 \(\pm\) 1.0 & 92.5 \(\pm\) 5.9 & 50.9 \(\pm\) 15.0 & 43.6 \(\pm\) 16.0 \\ GCN-ST & 94.4 \(\pm\) 0.4 & 93.5 \(\pm\) 1.2 & 93.7 \(\pm\) 0.7 & 93.8 \(\pm\) 0.6 & 94.0 \(\pm\) 0.4 & 94.2 \(\pm\) 0.3 & 93.3 \(\pm\) 0.7 & 94.1 \(\pm\) 1.0 \\ GCN-ST & 93.6 \(\pm\) 0.7 & 94.1 \(\pm\) 0.4 & 93.4 \(\pm\) 0.8 & 93.8 \(\pm\) 1.3 & 93.9 \(\pm\) 0.7 & 94.0 \(\pm\) 0.7 & 94.0 \(\pm\) 0.7 & 93.3 \(\pm\) 0.9 \\ \hline \hline  & \multicolumn{6}{c}{Ours} & \multicolumn{6}{c}{Ours} & \multicolumn{6}{c}{Ours} & \multicolumn{6}{c}{Ours} \\ \hline Logs & 2 & 4 & 16 & 32 & 2 & 4 & 16 & 32 \\ \hline GCN-ST & 71.7 \(\pm\) 0.3 & 72.0 \(\pm\) 0.3 & 71.4 \(\pm\) 0.2 & 71.9 \(\pm\) 0.3 & 72.1 \(\pm\) 0.3 & 72.7 \(\pm\) 0.3 & 72.3 \(\pm\) 0.2 & 72.3 \(\pm\) 0.3 \\ GCN-ST & 71.4 \(\pm\) 0.3 & 72.1 \(\pm\) 0.3 & 72.2 \(\pm\) 0.2 & 71.8 \(\pm\) 0.2 & 72.0 \(\pm\) 0.3 & 72.2 \(\pm\) 0.2 & 72.4 \(\pm\) 0.3 & 72.1 \(\pm\) 0.3 \\ GCN-ST & 68.5 \(\pm\) 0.6 & 71.0 \(\pm\) 0.5 & 72.8 \(\pm\) 0.5 & 72.1 \(\pm\) 0.6 & 67.7 \(\pm\) 0.5 & 71.3 \(\pm\) 0.5 & 72.3 \(\pm\) 0.5 & 72.3 \(\pm\) 0.5 \\ \hline \hline  & \multicolumn{6}{c}{} & \multicolumn{6}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ \end{tabular}
\end{table}
Table 7: Test accuracy results for models of varying depth with ReLU or leaky ReLU activation function on the citation network datasets using the split discussed in Section 6.2.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline  & **Cornell** & **Texas** & **Wisconsin** & **Chameleon** & **Squirrel** \\ \hline GCN-ST & \(55.95\pm 8.5\) & \(62.16\pm 5.7\) & \(54.71\pm 4.4\) & \(38.44\pm 4.3\) & \(35.31\pm 1.9\) \\ GCNI-ST & \(75.41\pm 2.2\) & \(83.34\pm 4.5\) & \(86.08\pm 3.8\) & \(64.52\pm 2.2\) & \(47.51\pm 1.4\) \\ \hline \hline \end{tabular}
\end{table}
Table 8: Test mean \(\pm\) standard deviation accuracy from \(10\) fold cross validation on five heterophilic datasets with fixed \(48/32/20\%\) splits. The depth of each model is 8 layers with 16 hidden channels. (Unit: second)

Figure 4: Training gradients for \(||\partial\bm{H}^{\text{out}}/\partial\bm{H}^{\prime}||\) for \(l\in[0,32]\) layers and 100 training epochs on the Citeseer dataset. Here, all models have 32 layers and 16 hidden dimensions for each layer. We observe that (a) GCN suffers from vanishing gradients. By contrast (c) GCNI and (e) EGNN do not suffer from vanishing gradients, and we can observe their skip connection to \(\bm{H}^{\text{0}}\). Because these models (GCNII/GCNII-SCT and EGNN/EGNN-SCT) connect \(\bm{H}^{\text{0}}\) to every layer, the gradient at the first layer is nonzero. We notice that while SCT does not overcome vanishing gradients for (b) GCN-SCT, it is able to increase the norm of the gradients for the intermediate layers in (d) GCNII-SCT and (f) EGNN-SCT.

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: See details in Sections 3, 4, 5, and 6. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See Section 7.

Guidelines:

* The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: See Sections 3 and 4 for details. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: See Section 6 and supplementary materials for details. Guidelines: * The answer NA means that the paper does not include experiments.

* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general, releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: See supplementary materials for details. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).

* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: See Section 6 for details. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: See Section 6 for details. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See Section 6 for details. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.

* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have fully complied with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: See Section 8 for details. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [Yes] Justification: The data used in this paper are all benchmark tasks established by the community. Guidelines:* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
* **Licensees for existing assets*
* Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have fully acknowledged baseline models, codes, and data in our paper. Guidelines:
* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We have provided details documents for the codes. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA]Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.