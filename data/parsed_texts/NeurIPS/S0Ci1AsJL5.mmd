Gaussian Approximation and Multiplier Bootstrap for Polyak-Ruppert Averaged Linear Stochastic Approximation with Applications to TD Learning

 Sergey Samsonov

HSE University

svsamsonov@hse.ru

&Eric Moulines

Ecole Polytechnique,

MBUZAI

&Qi-Man Shao

Department of Statistics and Data Science,

Shenzhen International Center of Mathematics,

Southern University of Science and Technology

&Zhuo-Song Zhang

Department of Statistics and Data Science,

Shenzhen International Center of Mathematics,

Southern University of Science and Technology

&Alexey Naumov

HSE University,

Steklov Mathematical Institute

of Russian Academy of Sciences

###### Abstract

In this paper, we obtain the Berry-Esseen bound for multivariate normal approximation for the Polyak-Ruppert averaged iterates of the linear stochastic approximation (LSA) algorithm with decreasing step size. Moreover, we prove the non-asymptotic validity of the confidence intervals for parameter estimation with LSA based on multiplier bootstrap. This procedure updates the LSA estimate together with a set of randomly perturbed LSA estimates upon the arrival of subsequent observations. We illustrate our findings in the setting of temporal difference learning with linear function approximation.

## 1 Introduction

Stochastic approximation (SA) methods are a central component for solving various optimization problems that arise in machine learning [32; 26], empirical risk minimization [72] and reinforcement learning [42; 67]. There is a vast number of contributions in the literature, which cover both asymptotic [48; 53] and non-asymptotic [45; 15; 36] properties of the SA estimates. The primarily important property among the asymptotic ones of the SA estimates is their asymptotic normality [53], which is important due to its role in constructing (asymptotic) confidence intervals and hypothesis testing [71]. However, a natural question of the rate of convergence in the appropriate central limit theorems (CLT) is not well addressed in literature even in the relatively simple setting of the linear stochastic approximation (LSA) [21; 34; 9].

Alternatively, confidence sets for SA algorithms can be constructed in a non-asymptotic manner based on concentration inequalities [4]. These bounds are often regarded as loose [60], yielding suboptimal performance of the statistical procedures based on the latter estimates [28]. In contrast, for statistical inference procedures based on independent and identically distributed (i.i.d.) observations, such as \(M\)-estimators [71], there is a machinery of non-parametric methods for constructing confidence sets with the bootstrap [19; 58]. This approach is accompanied with theoretical guarantees, showing the non-asymptotic validity of the bootstrap-based confidence intervals for parameters in linear regression [64] and statistical tests [12]. Extending theoretical guarantees to a non-classical situation with online learning algorithms encounters serious problems, essentially related to the problem of obtaining rate of convergence in the corresponding CLTs. At the same time, many phenomena arising in the analysis of nonlinear SA algorithms already appear in the analysis of LSA problems.

The LSA procedure aims to find an approximate solution for the linear system \(\bar{\mathbf{A}}\theta^{\star}=\bar{\mathbf{b}}\) with a unique solution \(\theta^{\star}\) based on a sequence of observations \(\{(\mathbf{A}(Z_{k}),\mathbf{b}(Z_{k}))\}_{k\in\mathbb{N}}\). Here \(\mathbf{A}:\mathcal{Z}\rightarrow\mathbb{R}^{d\times d}\) and \(\mathbf{b}:\mathcal{Z}\rightarrow\mathbb{R}^{d}\) are measurable functions and \((Z_{k})_{k\in\mathbb{N}}\) is a sequence of noise variables taking values in some measurable space \((\mathcal{Z},\mathcal{Z})\) with a distribution \(\pi\) satisfying \(\mathbb{E}[\mathbf{A}(Z_{k})]=\bar{\mathbf{A}}\) and \(\mathbb{E}[\mathbf{b}(Z_{k})]=\bar{\mathbf{b}}\). We focus on the setting of independent and identically distributed (i.i.d.) observations \(\{Z_{k}\}_{k\in\mathbb{N}}\). With a sequence of decreasing step sizes \((\alpha_{k})_{k\in\mathbb{N}}\) and the starting point \(\theta_{0}\in\mathbb{R}^{d}\), we consider the estimates \(\{\bar{\theta}_{n}\}_{n\in\mathbb{N}}\) given by

\[\theta_{k}=\theta_{k-1}-\alpha_{k}\{\mathbf{A}(Z_{k})\theta_{k-1}-\mathbf{b}( Z_{k})\}\;,\;\;k\geq 1,\quad\bar{\theta}_{n}=n^{-1}\sum_{k=n}^{2n-1} \theta_{k}\;,\;\;n\geq 1\;.\] (1)

Here, we have fixed the size of the _burn-in_ period (see, e.g., [16; 44]) to \(n_{0}=n\). Provided that \(n\) is large enough, the burn-in size affects only a constant factor in the subsequent bounds. The sequence \(\{\theta_{k}\}_{k\in\mathbb{N}}\) corresponds to the standard LSA iterates, while \(\{\bar{\theta}_{n}\}_{n\in\mathbb{N}}\) corresponds to the Polyak-Ruppert (PR) averaged iterates [59; 53]. It is known that \(\bar{\theta}_{n}\) is asymptotically normal with a minimax-optimal covariance matrix (see [53] and [23] for discussion). Specifically, under appropriate technical conditions on the step sizes \(\{\alpha_{k}\}\) and noisy observations \(\{\mathbf{A}(Z_{k})\}\), it holds that

\[\sqrt{n}(\bar{\theta}_{n}-\theta^{\star})\stackrel{{ d}}{{ \rightarrow}}\mathcal{N}(0,\Sigma_{\infty})\;,\]

where \(\Sigma_{\infty}\) is the asymptotic covariance matrix defined later in Section 3.1. There is a long list of contributions to the non-asymptotic analysis of \(\bar{\theta}_{n}\), particularly [43] and [16], which study moment and Bernstein-type concentration bounds for \(\sqrt{n}(\bar{\theta}_{n}-\theta^{\star})\). Unfortunately, such bounds do not imply Berry-Esseen type inequalities for \(\sqrt{n}(\bar{\theta}_{n}-\theta^{\star})\), that is, they do not allow us to control the quantity

\[\rho_{n}^{\mathrm{Conv}}=\sup_{B\in\mathrm{Conv}(\mathbb{R}^{d})}\left|\mathbb{ P}\big{(}\sqrt{n}(\bar{\theta}_{n}-\theta^{\star})\in B\big{)}-\mathbb{P}( \Sigma_{\infty}^{1/2}\eta\in B)\right|\;,\] (2)

where \(\mathrm{Conv}(\mathbb{R}^{d})\) refers to the set of convex sets in \(\mathbb{R}^{d}\). While the Berry-Esseen bounds are a popular subject of study in probability theory, starting from the classical work [20], most results are obtained for sums of random variables or martingale difference sequences [52; 8]. We can only mention a few results for SA algorithms, see Section 2 for more details. This paper aims to provide the latter bounds for the specific setting of the LSA procedure. Our primary contribution is twofold:

* We establish a BerryEsseen bound for accuracy of normal approximation of the distribution of Polyak-Ruppert averaged LSA iterates with a polynomially decreasing step size. Our results suggest that the best rate of normal approximation, in the sense of (2), is of order \(n^{-1/4}\) up to logarithmic factors in \(n\), where \(n\) denotes the number of samples. Interestingly, this rate is achieved with an aggressive step size, \(\alpha_{k}=c_{0}/\sqrt{k}\). Our proof technique follows the Berry-Esseen bounds for nonlinear statistics provided in [63].
* We provide non-asymptotic confidence bounds for the distribution of the PR-averaged statistic \(\sqrt{n}(\bar{\theta}_{n}-\theta^{\star})\) using the multiplier bootstrap procedure. In particular, our bounds imply that the quantiles of the exact distribution of \(\sqrt{n}(\bar{\theta}_{n}-\theta^{\star})\) can be approximated at a rate of \(n^{-1/4}\), where \(n\) is the number of samples used in the procedure, provided that \(n\) is sufficiently large (see A4 for exact conditions). To the best of our knowledge, this is the first non-asymptotic bound on the accuracy of bootstrap approximation in SA algorithms. We apply the proposed methodology to the temporal difference learning (TD) algorithm for policy evaluation in reinforcement learning.

The rest of the paper is organized as follows. In Section 2, we provide a literature review on the non-asymptotic analysis of the LSA algorithm and bootstrap methods. Next, in Section 3, we analyze the convergence rate of Polyak-Ruppert averaged LSA iterates to the normal distribution. In Section 4, we discuss the multiplier bootstrap approach for LSA and establish bounds on the accuracy of approximating the quantiles of the true distribution. Finally, we apply our findings to TD learning and present numerical illustrations in Section 5.

**Notations.** For matrix \(A\in\mathbb{R}^{d\times d}\) we denote by \(\|A\|\) its operator norm. For symmetric matrix \(Q=Q^{\top}\succ 0\;,\;Q\in\mathbb{R}^{d\times d}\) and \(x\in\mathbb{R}^{d}\) we define the corresponding norm \(\|x\|_{Q}=\sqrt{x^{\top}Qx}\), and define the respective matrix \(Q\)-norm of the matrix \(B\in\mathbb{R}^{d\times d}\) by \(\|B\|_{Q}=\sup_{x\neq 0}\|Bx\|_{Q}/\|x\|_{Q}\). For sequences \(a_{n}\) and \(b_{n}\), we write \(a_{n}\lesssim b_{n}\) if there exist a constant \(c>0\) such that \(a_{n}\leq cb_{n}\) for \(c>0\). For simplicity we state the main results of the paper up to constant factors.

Related works

Among contributions to the analysis of the LSA algorithm, we should mention the papers [53; 34; 9; 6]. These works investigate the asymptotic properties of the LSA estimates (such as asymptotic normality and almost sure convergence) under i.i.d. and Markov noise. Non-asymptotic results for the LSA and PR-averaged LSA estimates were obtained in [55; 47; 7; 35; 44], where MSE bounds were established, and in [43; 17; 16], which provided high-probability error bounds. The latter results enable the construction of Bernstein-type confidence intervals for the error \(\bar{\theta}_{n}-\theta^{\star}\). Unfortunately, the corresponding bounds typically depend on unknown problem properties of (1), related to the design matrix \(\tilde{\mathbf{A}}\) and the noise variables \(\mathbf{A}(Z_{k})\), \(\mathbf{b}(Z_{k})\). For this reason, applying these error bounds in practice is complicated. Furthermore, concentration bounds for the LSA error [43; 17; 16] do not imply convergence rates of the rescaled error \(\sqrt{n}(\bar{\theta}_{n}-\theta^{\star})\) to the normal distribution in Wasserstein or Kolmogorov distance. Non-asymptotic convergence rates were previously studied in [2] using the Stein method, but the resulting rate corresponds to a smoothed Wasserstein distance. Recent work [65] investigates convergence rates to the normal distribution in Wasserstein distance for LSA with Markovian observations. Both papers yield bounds that are less tight with respect to their dependence on trajectory length \(n\) than those presented in the present work, see a detailed comparison after Theorem 2.

A popular method for constructing confidence intervals in the context of parametric estimation is based on the bootstrap approach ([19]). Its analysis has attracted many contributions, in particular a series of papers [12] and [13] that validate a bootstrap procedure for a test based on the maximum of a large number of statistics. Their study shows a close relationship between bootstrap validity results, Gaussian comparison and anticoncentration bounds for rectangular sets. The papers [64] and [27] investigate the applicability of likelihood-based statistics for finite samples and large parameter dimensions under possible model misspecification. The important step in proving bootstrap validity is again based on Gaussian comparison and anticoncentration bounds, but now for spherical sets. The bootstrap procedure for spectral projectors of covariance matrices is discussed in [46] and [31]. The authors follow the same steps to prove the validity of the bootstrap.

Extending the classical bootstrap approach to online learning algorithms is a challenge. For example, the iterates \(\{\theta_{k}\}_{k\in\mathbb{N}}\) determined by (1) are not necessarily stored in memory, which makes the classical bootstrap inapplicable. This problem can be solved by performing randomly perturbed updates of the online procedure, as proposed in [22] for the iterates of the Stochastic Gradient Descent (SGD) algorithm. The authors in [56] used the same procedure for the case of Markov noise and policy evaluation algorithms in reinforcement learning, but in both papers the authors only consider the asymptotic validity. In our paper we use the same multiplier bootstrap approach (see Section 4), but we provide an explicit error bound for the bootstrap approximation of the distribution of the statistics \(\sqrt{n}(\bar{\theta}_{n}-\theta^{\star})\).

In addition to the bootstrap approach, one can also use the pivotal statistics [37; 40; 41] or various estimates of the asymptotic covariance matrix [73] to construct the confidence intervals for \(\theta^{\star}\). The latter approach can be based on the plug-in estimators [39], batch mean estimators [11] or in combination with the multiplier bootstrap approach [74]. However, the theoretical guarantees for mentioned methods remain purely asymptotic.

## 3 Accuracy of normal approximation for LSA

We first study the rate of normal approximation for the tail-averaged LSA procedure. When there is no risk of ambiguity, we use simply the notations \(\mathbf{A}_{k}=\mathbf{A}(Z_{k})\) and \(\mathbf{b}_{k}=\mathbf{b}(Z_{k})\). Starting from the definition (1), we get with elementary transformations that

\[\theta_{k}-\theta^{\star}=(\mathrm{I}-\alpha_{k}\mathbf{A}_{k})(\theta_{k-1}- \theta^{\star})-\alpha_{k}\varepsilon_{k}\,\] (3)

where we have set \(\varepsilon_{k}=\varepsilon(Z_{k})\) with

\[\varepsilon(z)=\tilde{\mathbf{A}}(z)\theta^{\star}-\tilde{\mathbf{b}}(z)\,\quad\tilde{\mathbf{A}}(z)=\mathbf{A}(z)-\bar{\mathbf{A}}\,\quad\tilde{\mathbf{b}}(z)=\mathbf{b}(z)-\bar{ \mathbf{b}}\ \.\]

Here the random variable \(\varepsilon(Z_{k})\) can be viewed as a noise, measured at the optimal point \(\theta^{\star}\). We now assume the following technical conditions:

**A 1**.: _Sequence \(\{Z_{k}\}_{k\in\mathbb{N}}\) is a sequence of i.i.d. random variables defined on a probability space \((\Omega,\mathcal{F},\mathbb{P})\) with distribution \(\pi\)._

**A2**.: \(\int_{Z}{\bf A}(z){\rm d}\pi(z)=\bar{\bf A}\) _and \(\int_{Z}{\bf b}(z){\rm d}\pi(z)=\bar{\bf b}\), with the matrix \(-\bar{\bf A}\) being Hurwitz. Moreover, \(\|\varepsilon\|_{\infty}=\sup_{z\in{\bf Z}}\|\varepsilon(z)\|<+\infty\), and the mapping \(z\to{\bf A}(z)\) is bounded, that is,_

\[{\rm C}_{\bf A}=\sup_{z\in{\bf Z}}\|{\bf A}(z)\|\vee\sup_{z\in{\bf Z}}\|\bar{ \bf A}(z)\|<\infty\.\] (4)

_Moreover, for the noise covariance matrix_

\[\Sigma_{\varepsilon}=\int_{Z}\varepsilon(z)\varepsilon(z)^{\top}{\rm d}\pi(z)\] (5)

_it holds that its smallest eigenvalue is bounded away from \(0\), that is,_

\[\lambda_{\min}:=\lambda_{\min}(\Sigma_{\varepsilon})>0\.\] (6)

It is possible to change (4) to the moment-type bound as it was previously considered in [43] and [16], see the detailed discussion after Theorem 2. The fact that the matrix \(-\bar{\bf A}\) is Hurwitz implies that the linear system \(\bar{\bf A}\theta=\bar{\bf b}\) has a unique solution \(\theta^{\star}\). Moreover, this fact is sufficient to show that the matrix \({\rm I}-\alpha\bar{\bf A}\) is a contraction in an appropriate matrix \(Q\)-norm for small enough \(\alpha>0\). Precisely, the following result holds:

**Proposition 1**.: _Let \(-\bar{\bf A}\) be a Hurwitz matrix. Then for any \(P=P^{\top}\succ{\rm I}\), there exists a unique matrix \(Q=Q^{\top}\succ{\rm I}\), satisfying the Lyapunov equation \(\bar{\bf A}^{\top}Q+Q\bar{\bf A}=P\). Moreover, setting_

\[a=\frac{\lambda_{\min}(P)}{2\|Q\|}\,\quad\text{and}\quad\alpha_{\infty}= \frac{\lambda_{\min}(P)}{2\kappa_{Q}\|{\bf A}\|_{Q}^{2}}\wedge\frac{\|Q\|}{ \lambda_{\min}(P)}\,\] (7)

_where \(\kappa_{Q}=\lambda_{\max}(Q)/\lambda_{\min}(Q)\), it holds for any \(\alpha\in[0,\alpha_{\infty}]\) that \(\alpha a\leq 1/2\), and_

\[\|{\rm I}-\alpha\bar{\bf A}\|_{Q}^{2}\leq 1-\alpha a\.\] (8)

The proof of Proposition 1 is provided in Appendix D.1. Note that it is possible to set \(P={\rm I}\) as in [18], yet it is possible that other choices of \(P\) could be more beneficial for particular applications. Now consider an assumption on the step sizes \(\alpha_{k}\) and number of observations \(n\):

**A3**.: _The step sizes \(\{\alpha_{k}\}_{k\in\mathbb{N}}\) has a form \(\alpha_{k}=c_{0}/k^{\gamma}\), where \(\gamma\in[1/2;1)\) and \(c_{0}\in(0;\alpha_{\infty}\wedge a\wedge(1-\gamma)]\). Moreover, we assume that \(n\geq d\), and_

\[\begin{cases}\frac{\sqrt{n}}{(1+\log n)\log n}\geq\frac{c_{0}\kappa_{Q}\,{ \rm C}_{\bar{\bf A}}^{\prime}}{a(1-\sqrt{2/2})}\vee\frac{4}{ac_{0}(1-\sqrt{2/2 })}\,\ \text{if}\ \gamma=1/2\,\\ \frac{n^{1-\gamma}}{\log n}\geq\frac{2c_{0}\kappa_{Q}\,{\rm C}_{\bar{\bf A}} ^{\prime}}{a(2\gamma-1)(1-(1/2)^{1-\gamma})}\vee\frac{8\gamma(1-\gamma)}{ac_{ 0}(1-(1/2)^{1-\gamma}}\,\ \text{if}\ 1/2<\gamma<1\.\end{cases}\] (9)

The main aim of lower bounding \(n\) is to ensure that the number of observations is large enough in order that the LSA error related to the choice of initial condition \(\theta_{0}-\theta^{\star}\) becomes small.

### Central limit theorem for Polyak-Ruppert averaged LSA iterates.

It is known that the assumptions A1-A3 guarantee that the CLT applies to the iterates of \(\bar{\theta}_{n}\), namely,

\[\sqrt{n}(\bar{\theta}_{n}-\theta^{\star})\stackrel{{ d}}{{\to}}{ \cal N}(0,\Sigma_{\infty})\,\] (10)

where the asymptotic covariance matrix \(\Sigma_{\infty}\) has a form

\[\Sigma_{\infty}=\bar{\bf A}^{-1}\Sigma_{\varepsilon}\bar{\bf A}^{-\top},\] (11)

and \(\Sigma_{\varepsilon}\) is defined in (5). This result can be found for example in [53] and [23]. We are interested in the Berry-Esseen type bound for the rate of convergence in (10), that is, we aim to bound \(\rho_{n}^{\rm Conv}\) defined in (2) w.r.t. the available sample size \(n\). We control \(\rho_{n}^{\rm Conv}\) using a method from [63] based on randomized multivariate concentration inequality. Below we briefly state its setting and required definitions. Let \(X_{1},\ldots,X_{n}\) be independent random variables taking values in \({\cal X}\) and \(T=T(X_{1},\ldots,X_{n})\) be a general \(d\)-dimensional statistics such that \(T=W+D\), where

\[W=\sum_{\ell=1}^{n}\xi_{\ell},\quad D:=D(X_{1},\ldots,X_{n})=T-W,\] (12)\(\xi_{\ell}=h_{\ell}(\mathcal{X}_{\ell})\) and \(h_{\ell}:\mathcal{X}\to\mathbb{R}^{d}\) is a Borel measurable function. Here the statistics \(D\) can be non-linear and is treated as an error term, which is "small" compared to \(W\) in an appropriate sense. Assume that \(\mathbb{E}[\xi_{\ell}]=0\) and \(\sum_{\ell=1}^{n}\mathbb{E}[\xi_{\ell}\xi_{\ell}^{\top}]=\mathrm{I}_{d}\). Let \(\Upsilon=\Upsilon_{n}=\sum_{\ell=1}^{n}\mathbb{E}[\|\xi_{\ell}\|^{3}]\). Then, with \(\eta\sim\mathcal{N}(0,\mathrm{I}_{d})\),

\[\sup_{B\in\mathrm{Conv}(\mathbb{R}^{d})}|\mathbb{P}(T\in A)-\mathbb{P}(\eta \in A)|\leq 259d^{1/2}\Upsilon+2\mathbb{E}[\|W\|\|D\|]+2\sum_{\ell=1}^{n} \mathbb{E}[\|\xi_{\ell}\|\|D-D^{(\ell)}\|],\] (13)

where \(D^{(\ell)}=D(X_{1},\ldots,X_{\ell-1},X_{\ell}^{\prime},X_{\ell+1},\ldots,X_{n})\) and \(X_{\ell}^{\prime}\) is an independent copy of \(X_{\ell}\). This result is due to [63, Theorem 2.1]. One can modify the bound (13) for the setting when \(\sum_{\ell=1}^{n}\mathbb{E}[\xi_{\ell}\xi_{\ell}^{\top}]=\Sigma\succ 0\). This result due to [63, Corollary 2.3]. Following the construction (12), we set \(T=\sqrt{n}\bar{\mathbf{A}}(\bar{\theta}_{n}-\theta^{\star})\) and consider it as a nonlinear statistic of i.i.d. random variables \(Z_{1},\ldots,Z_{2n}\), which drive the LSA dynamics (1). We can exactly represent \(T\) as a sum of linear (\(W\)) and non-linear parts (\(D\)), where

\[W=-\frac{1}{\sqrt{n}}\sum_{k=n}^{2n-1}\varepsilon_{k+1},\quad D =\frac{1}{\sqrt{n}}\frac{\theta_{n}-\theta^{\star}}{\alpha_{n}}-\frac{1}{\sqrt {n}}\frac{\theta_{2n}-\theta^{\star}}{\alpha_{2n}}-\frac{1}{\sqrt{n}}\sum_{k= n+1}^{2n}(\mathbf{A}_{k}-\bar{\mathbf{A}})(\theta_{k-1}-\theta^{\star})\\ +\frac{1}{\sqrt{n}}\sum_{k=n+1}^{2n}\left(\theta_{k-1}-\theta^{ \star}\right)\left(\frac{1}{\alpha_{k}}-\frac{1}{\alpha_{k-1}}\right).\]

The proof of this result can be bound in Proposition 3. To obtain a bound for the approximation accuracy in (2) using the bound (13), we need to upper bound \(\mathbb{E}^{1/2}[\|D(Z_{1},\ldots,Z_{2n})\|^{2}]\) and \(\mathbb{E}[\|D-D^{(i)}\|]\). The first result below provides a second moment bound on \(D\):

**Theorem 1**.: _Assume A1, A2, and A3. Then we obtain the following error bound:_

\[\mathbb{E}^{1/2}\left[\|D(Z_{1},\ldots,Z_{2n})\|^{2}\right] \lesssim\frac{\sqrt{\kappa_{Q}}\|\varepsilon\|_{\infty}}{\sqrt{ac_ {0}}}\left(\frac{1}{n^{(1-\gamma)/2}}+\frac{c_{0}\,\mathsf{C}_{\mathbf{A}}}{ \sqrt{1-\gamma n^{\gamma/2}}}\right)\] \[\quad+\sqrt{\kappa_{Q}}\Delta_{1}\exp\biggl{\{}-\frac{c_{0}an^{1 -\gamma}}{2(1-\gamma)}\biggr{\}}\|\theta_{0}-\theta^{\star}\|\,\]

_where \(\lesssim\) stands for inequality up to an absolute constant, and \(\Delta_{1}=\Delta_{1}(n,a,\mathsf{C}_{\mathbf{A}},c_{0})\) is a polynomial function defined in Appendix A.3, eq. (29)._

The proof of Theorem 1 is provided in Appendix A.3. Now it remains to upper bound the term \(\mathbb{E}[\|D-D^{(i)}\|]\), which is done in Appendix B.1 using the synchronous coupling methods [10]. Combining these bounds, we obtain the following theorem:

**Theorem 2**.: _Assume A1, A2, and A3. Then the following bound holds:_

\[\rho_{n}^{\mathrm{Conv}}\lesssim\frac{d^{1/2}\|\varepsilon\|_{\infty}^{3}}{ \lambda_{\min}^{3/2}\sqrt{n}}+\frac{1}{\lambda_{\min}}\left(\frac{\mathsf{C}_{1 }}{n^{(1-\gamma)/2}}+\frac{\mathsf{C}_{2}}{n^{\gamma/2}}\right)+\frac{\Delta_{2 }}{\lambda_{\min}}\exp\biggl{\{}-\frac{c_{0}an^{1-\gamma}}{2(1-\gamma)}\biggr{\}} \|\theta_{0}-\theta^{\star}\|\,\] (14)

_where \(\Delta_{2}=\Delta_{2}(n,a,\mathsf{C}_{\mathbf{A}},\operatorname{Tr}\Sigma_{ \varepsilon},c_{0})\) is a polynomial function defined in (35), and constants \(\mathsf{C}_{1},\mathsf{C}_{2}\), depending upon \(a,\mathsf{C}_{\mathbf{A}},\kappa_{Q},\operatorname{Tr}\Sigma_{\varepsilon},c_ {0}\), are defined in Appendix B, eq. (36)._

The proof of Theorem 2 is provided in Appendix B. Note that the assumption A2 requires that \(\varepsilon(Z_{1})\) is almost sure bounded. It is a strong assumption, but it can be partially relaxed. Following the stability of matrix products technique, used in [17, Proposition 3], it is possible to consider the setting when the random variable \(\|\bar{\mathbf{A}}(Z_{1})\|\) has only finite number of moments. In particular, we expect that assuming finite third moment of \(\|\bar{\mathbf{A}}(Z_{1})\|\) and \(\|\varepsilon(Z_{1})\|\) is sufficient to obtain a counterpart to Theorem 1. However, this generalization requires non-trivial technical work on generalizing the stability of matrix products result (see Corollary 4 in Appendix D ).

Note that the bound of Theorem 2 predicts the optimal error of normal approximation for Polyak-Ruppert averaged estimates of order \(n^{-1/4}\), which is achieved with the aggressive step size \(\alpha_{k}=c_{0}/\sqrt{k}\), that is, when setting \(\gamma=1/2\) in (14). In this case we obtain the optimized bound

\[\rho_{n}^{\mathrm{Conv}}\lesssim\frac{\mathsf{C}_{3}}{\lambda_{\min}n^{1/4}}+ \frac{d^{1/2}\|\varepsilon\|_{\infty}^{1}}{\lambda_{\min}^{3/2}\sqrt{n}}+\frac{ \Delta_{1}\exp\bigl{\{}-c_{0}a\sqrt{n}\bigr{\}}}{\lambda_{\min}}\|\theta_{0}- \theta^{\star}\|\,\] (15)

where \(\mathsf{C}_{3}=\mathsf{C}_{3}(a,\mathsf{C}_{\mathbf{A}},\kappa_{Q}, \operatorname{Tr}\Sigma_{\varepsilon},\|\varepsilon\|_{\infty})\) is provided in (36).

Discussion.Our proof technique of Theorem 2 reveals an interesting feature: fastest rate of convergence in the convex distance \(\rho_{n}^{\mathrm{Conv}}\) corresponds to the learning rate schedule that admits the fastest decay of the second-order term in the MSE bound for remainder statistics \(D\) (see Theorem 1). Results similar to the one of Theorem 2 have been recently obtained in the literature in [65] and [2]. The author in [65] considers the LSA problem specified to the temporal-difference learning (see Section 5) with Markov noise and obtains convergence rate in Wasserstein distance of order \(n^{-1/4}\), which corresponds to the "optimal" step size schedule \(\alpha_{k}=c_{0}/k^{3/4}\). Using the bound of [49, eq. (3)] (see also section 2 in [57]), this result yield a suboptimal bound of order \(n^{-1/8}\) for the convex distance \(\rho_{n}^{\mathrm{Conv}}\). Such an upper bound may be loose for some classes of distributions, but it is not clear if in particular setting of LSA the bound of [65] could imply scaling of order \(n^{-1/4}\) for \(\rho_{n}^{\mathrm{Conv}}\). At the same time, in case of \(X_{1},\ldots,X_{n}\) forming a Markov chain in (12) there is no available counterpart of the bound (13). Generalizing (13) is an interesting research direction that would allow to obtain a counterpart of Theorem 2 in case of Markovian dynamics. Similarly, the result of [2] holds for much stronger metrics, which controls the convergence of moments of twice differentiable functions. We provide additional details about connections between this metric and \(\rho_{n}^{\mathrm{Conv}}\) in Appendix B.2. At the same time, the authors in [2] cover the non-linear setting of PR-averaged iterates of stochastic gradient descent algorithm under strong convexity.

**Remark 1**.: _The leading (with respect to \(n\)) terms of the bound from Theorem 1 have an implicit dependence on the problem dimension \(d\) due to the presence of \(\lambda_{\mathrm{min}}\). Yet the result of Theorem 1 can be improved in a sense of dependence in dimension if one is interested not in the rates of convergence for \(\sqrt{n}(\bar{\theta}_{n}-\theta^{\star})\), but in the projected iterated \(\sqrt{n}\Pi^{\top}(\bar{\theta}_{n}-\theta^{\star})\) for some \(\Pi\in\mathbb{R}^{d\times m}\), \(m\leq d\). If this is the case, one may apply (13) for the class \(\mathrm{Conv}_{m}=\mathrm{Conv}(\mathbb{R}^{m})\) of convex sets in \(\mathbb{R}^{m}\) and obtain, setting step size \(\alpha_{k}=c_{0}/\sqrt{k}\), and \(\Sigma_{\varepsilon}^{(\Pi)}=\Pi\Sigma_{\varepsilon}\Pi^{\top}\), that_

\[\rho_{n}^{\mathrm{Conv}}\lesssim\frac{\mathsf{C}_{4}}{\lambda_{\mathrm{min}}n ^{1/4}}+\frac{m^{1/2}\|\varepsilon\|_{\infty}^{3}}{\lambda_{\mathrm{min}}^{3/2 }\sqrt{n}}+\frac{\Delta_{2}\mathrm{e}^{-c_{0}a\sqrt{n}}}{\lambda_{\mathrm{min} }}\|\theta_{0}-\theta^{\star}\|\,\]

_and the constant \(\mathsf{C}_{4}=\mathsf{C}_{4}(a,\mathsf{C}_{\mathrm{A}},\kappa_{Q},\mathrm{Tr} \,\Sigma_{\varepsilon}^{(\Pi)},\|\varepsilon\|_{\infty})\) is provided in (36)._

**Remark 2**.: _Results similar to Theorem 1 can be obtained not only for the Polyak-Ruppert averaged estimator \(\bar{\theta}_{n}\), but also for the last iterate \(\theta_{n}\). In particular, it is known (see e.g. [23]), that the last iterate error \(\theta_{n}-\theta^{\star}\) is also asymptotically normal:_

\[\tfrac{\theta_{n}-\theta^{\star}}{\sqrt{\alpha_{n}}}\to\mathcal{N}(0,\Sigma_{ \mathrm{last}})\,\]

_where the covariance matrix \(\Sigma_{\mathrm{last}}\) is different from \(\Sigma_{\infty}\). In such a case \(\Sigma_{\mathrm{last}}\) can be found as a solution to appropriate Lyapunov equation, see [23]. Then, we expect that it is possible to use the perturbation-expansion approach from [1] together with randomized concentration inequalities [63] (see (13)), in order to obtain the Berry-Esseen bound_

\[\sup_{B\in\mathrm{Conv}(\mathbb{R}^{d})}\left|\mathbb{P}\big{(}\tfrac{\theta_ {n}-\theta^{\star}}{\sqrt{\alpha_{n}}}\in B\big{)}-\mathbb{P}(\Sigma_{ \mathrm{last}}^{1/2}\eta\in B)\right|\lesssim\sqrt{\alpha_{n}}\.\]

_We leave the detailed derivation for future work._

## 4 Multiplier bootstrap for LSA

In order to perform statistical inference with the Polyak-Ruppert estimator \(\bar{\theta}_{n}\), we propose an online bootstrap resampling procedure, which recursively updates the LSA estimate as well as a large number of randomly perturbed LSA estimates, upon the arrival of each data point. The suggested procedure follows the one outlined in [22]. It has the following advantages: it does not rely on the asymptotic distribution of the error \(\sqrt{n}(\bar{\theta}_{n}-\theta^{\star})\), does not require to know the moments of \(\sqrt{n}(\bar{\theta}_{n}-\theta^{\star})\) or its asymptotic covariance matrix \(\Sigma_{\infty}\), and does not involve any data splitting.

We state the suggested procedure as follows. Let \(\mathcal{W}^{2n}=\{W_{\ell}\}_{1\leq\ell\leq 2n}\) be a set of i.i.d. random variables, independent of \(\mathcal{Z}^{2n}=\{Z_{\ell}\}_{1\leq\ell\leq 2n}\), with \(\mathbb{E}[W_{1}]=1\) and \(\mathrm{Var}[W_{1}]=1\). We write, respectively, \(\mathbb{P}^{\mathrm{b}}=\mathbb{P}(\cdot|\mathcal{Z}^{2n})\) and \(\mathbb{E}^{\mathrm{b}}=\mathbb{E}(\cdot|\mathcal{Z}^{2n})\) for the corresponding conditional probability and expectation. In parallel with procedure (1) that generates \(\{\theta_{k}\}_{1\leq k\leq 2n}\) and \(\bar{\theta}_{n}\), we generate \(M\) independent samples \((w_{n}^{\ell},\ldots,w_{2n}^{\ell})\), \(1\leq\ell\leq M\) distributed as \(\mathcal{W}^{2n}\), and recursively update \(M\) randomly perturbed LSA estimates, that is,

\[\begin{split}\theta_{k}^{\mathrm{b},\ell}&=\theta_{k -1}^{\mathrm{b},\ell}-\alpha_{k}w_{k}^{\ell}\{\mathbf{A}(Z_{k})\theta_{k-1}^{ \mathrm{b},\ell}-\mathbf{b}(Z_{k})\}\,\ \ k\geq n+1\,\ \theta_{n}^{\mathrm{b},\ell}=\theta_{n}\,\\ \bar{\theta}_{n}^{\mathrm{b},\ell}&=n^{-1}\sum_{k=n}^ {2n-1}\theta_{k}^{\mathrm{b},\ell}\,\ \ n\geq 1\.\end{split}\] (16)We use a short notation \(\bar{\theta}_{n}^{\text{b}}\) for \(\bar{\theta}_{n}^{\text{b},1}\). The key idea of the procedure (16) is that the "Bootstrap-world" distribution (that is, the one conditional on \(\mathcal{Z}^{2n}\)) of the perturbed samples \(\sqrt{n}(\bar{\theta}_{n}^{\text{b}}-\bar{\theta}_{n})\) is close to the distribution of the quantity of interest, that is, \(\sqrt{n}(\bar{\theta}_{n}-\theta^{\star})\). Precisely, the main result of this section will show that the quantity

\[\sup_{B\in\operatorname{Conv}(\mathbb{R}^{d})}|\mathbb{P}^{\text{b}}(\sqrt{n} (\bar{\theta}_{n}^{\text{b}}-\bar{\theta}_{n})\in B)-\mathbb{P}(\sqrt{n}(\bar{ \theta}_{n}-\theta^{\star})\in B)|\] (17)

is small. Although an analytic expression for \(\mathbb{P}^{\text{b}}(\sqrt{n}(\bar{\theta}_{n}^{\text{b}}-\bar{\theta}_{n}) \in B)\) is not available, one can approximate it from numerical simulations according to (16) by generating sufficiently large number \(M\) of perturbed trajectories. Standard arguments, see e.g. [62, Section 5.1] suggest that the accuracy of Monte-Carlo approximation is of order \(M^{-1/2}\). To analyze the suggested procedure, we shall impose an additional assumption on the trajectory length \(n\):

**A4**.: _Assumption A3 holds with \(\gamma=1/2\), and \(c_{0}\leq 1/(\mathrm{C}_{\mathbf{A}}^{2}\,\kappa_{Q}\mathrm{e})\). Moreover, setting_

\[h(n)=\left\lceil\left(\frac{4\,\mathrm{C}_{\mathbf{A}}\,\kappa_{Q}^{1/2}}{( \sqrt{2}-1)a}\right)^{2}(1+2\log{(2n^{4})})^{2}\right\rceil\,,\] (18)

_it holds that_

\[\frac{\sqrt{n}}{h(n)}\geq\frac{2}{a(\sqrt{2}-1)}\vee\frac{c_{0}}{\alpha_{\infty }}\,,\text{ and }\frac{\sqrt{n}}{\log^{2}n}\geq\frac{c_{0}(1\,\mathrm{C}_{\mathbf{A}}^{2}) }{a}\vee c_{0}a\,\mathrm{C}_{\mathbf{A}}^{2}\vee\frac{4}{ac_{0}}\] (19)

_Moreover, we assume that for \(\lambda_{\min}\) defined in (6) it holds that_

\[\lambda_{\min}\geq 8\|\varepsilon\|_{\infty}\sqrt{\frac{\|\Sigma_{\varepsilon} \|\log n}{n}}+\frac{8(\|\Sigma_{\varepsilon}\|+\|\varepsilon\|_{\infty}^{2}) \log n}{n}\] (20)

Note that the new bound (19) simply states that \(\sqrt{n}/\log^{2}(n)\) is sufficiently large, since \(h(n)\) scales as \(\log^{2}n\). We discuss the assumption A4 in more details in the proof scheme. Now we formulate the main result of this section. We analyze only the setting of polynomially decaying step size with \(\gamma=1/2\), since decay rate of (17) essentially depends on the approximation rate of Theorem 2, with the fastest rate achieved when \(\gamma=1/2\). For other learning rates the decay rate of right-hand side in Theorem 3 will be slower. For simplicity, we do not trace the dependence of the bound below on the parameter \(c_{0}\).

**Theorem 3**.: _Assume A1, A2, A3 with \(\gamma=1/2\), and A4. Then with \(\mathbb{P}\) - probability at least \(1-6/n\) it holds that_

\[\sup_{B\in\operatorname{Conv}(\mathbb{R}^{d})}|\mathbb{P}^{\text{b}}(\sqrt{n} (\bar{\theta}_{n}^{\text{b}}-\bar{\theta}_{n})\in B)-\mathbb{P}(\sqrt{n}(\bar {\theta}_{n}-\theta^{\star})\in B)|\lesssim\frac{\kappa_{Q}^{2}(\mathrm{C}_{ \mathbf{A}}^{4}\,\,\lor 1)(1+\|\varepsilon\|_{\infty}^{2})\log n}{a^{5/2} \lambda_{\min}n^{1/4}}\]

_where \(\Delta_{3}=\Delta_{3}(n,a,\mathrm{C}_{\mathbf{A}},\|\varepsilon\|_{\infty})\) is a polynomial function defined in Appendix C, eq. (46)._

The proof of Theorem 3 is based on the Gaussian approximation performed both in the "real" world and bootstrap world together with an appropriate Gaussian comparison inequality. The main steps of the proof are illustrated by the following scheme:

\[\text{Real world:}\qquad\sqrt{n}\bar{\mathbf{A}}(\bar{\theta}_{n}-\theta^{ \star})\ \xleftarrow{\text{Gaussian approximation, Th. \ref{eq:gaussian approximation}}}\xi\sim\mathcal{N}(0,\Sigma_{ \varepsilon})\\ \text{\text{Gaussian comparison, Theorem \ref{eq:gaussian approximation}}}\xi\]

\[\text{Bootstrap world:}\qquad\sqrt{n}\bar{\mathbf{A}}(\bar{\theta}_{n}^{\text{b}}-\bar{ \theta}_{n})\xleftarrow{\text{Gaussian approx. in Bootstrap world, Th. \ref{eq:gaussian approximation}}}\xi^{\text{b}}\sim\mathcal{N}(0,\Sigma_{ \varepsilon}^{\text{b}})\]

In the above scheme we have denoted by \(\Sigma_{\varepsilon}^{\text{b}}=n^{-1}\sum_{\ell=n}^{2n-1}\varepsilon_{\ell} \varepsilon_{\ell}^{\top}\) the sample covariance matrix approximating \(\Sigma_{\varepsilon}\). Gaussian approximation for the true distribution of \(\sqrt{n}\bar{\mathbf{A}}(\bar{\theta}_{n}-\theta^{\star})\) follows from Theorem 2. Proof of Gaussian approximation in the Bootstrap world Theorem 4 is also based on the inequality (13), but is more complicated and involves the expansion analysis of the LSA error from [1]. This technique allows to separate the LSA error into different scales with respect to the step sizes \(\{\alpha_{k}\}\), see Appendix C.4 for details. However, this technique requires to impose additional assumption A4 - eq. (19). Proof of the Gaussian comparison part of Theorem 5 is based on Pinsker's inequality and matrix Bernstein inequality. The latter result requires that \(n\) is large enough to ensure that minimal eigenvalue of \(\Sigma_{\varepsilon}^{\mathsf{b}}\) is close to \(\lambda_{\min}\), justifying the assumption A4 - eq. (20). Detailed proof if provided in Appendix C.

**Discussion.** We emphasize that the Gaussian approximation result of Theorem 2 (with Bootstrap world generalization in Theorem 4) is a key result to prove the above bootstrap validity. This argument was missing in the earlier works studying confidence intervals for stochastic optimization algorithms [11, 73, 74], where the authors considered procedures to estimate \(\Sigma_{\infty}\) in (11). They combine _non-asymptotic_ bounds on the accuracy of recovering \(\Sigma_{\infty}\) with only _asymptotic_ validity of the resulting confidence intervals. We expect that our proof technique for Theorem 2 can be used to provide similar non-asymptotic validity results for outlined approaches for constructing confidence intervals based on the estimation of the asymptotic covariance matrix.

**Corollary 1**.: _(Set of Euclidean balls or ellipsoids) Suppose that we are interested in estimating quantile of a given order \(\alpha\in(0,1)\) and some matrix \(B\in\mathbb{R}^{d\times d}\), that is, the quantity_

\[t_{\alpha}=\inf\{t>0:\mathbb{P}(\sqrt{n}\|B(\bar{\theta}_{n}-\theta^{\star}) \|\geq t)\leq\alpha\}.\]

_We define its counterpart in the Bootstrap world, \(t_{\alpha}^{\mathsf{b}}\), as_

\[t_{\alpha}^{\mathsf{b}}=\inf\{t>0:\mathbb{P}^{\mathsf{b}}(\sqrt{n}\|B(\bar{ \theta}_{n}^{\mathsf{b}}-\bar{\theta}_{n})\|\geq t)\leq\alpha\}.\]

_Note that \(t_{\alpha}^{\mathsf{b}}\) is defines with respect to the bootstrap measure, therefore, it depends on the data \(\mathcal{Z}^{2n}\). This bootstrap critical value \(t_{\alpha}^{\mathsf{b}}\) is applied in the Bootstrap world to build the confidence set_

\[\mathcal{E}(\alpha)=\{\theta\in\mathbb{R}^{d}:\sqrt{n}\|B(\theta-\bar{\theta} _{n})\|\leq t_{\alpha}^{\mathsf{b}}\}\;.\]

_Theorem 3 justifies this construction and evaluate the coverage probability of the true value \(\theta^{\star}\) by this set. It states that_

\[\mathbb{P}(\theta^{\star}\notin\mathcal{E}(\alpha))=\mathbb{P}(\sqrt{n}\|B( \bar{\theta}_{n}-\theta^{\star})\|>t_{\alpha}^{\mathsf{b}})\approx\alpha\;,\]

_with the error of order \(n^{-1/4}\) in the right-hand side. Although an analytic expression for \(t_{\alpha}^{\mathsf{b}}\) is not available, one can approximate it by generating a large number \(M\) of independent samples of \(\mathcal{W}_{n}\) and computing from them the empirical distribution function of \(\sqrt{n}\|B(\bar{\theta}_{n}^{\mathsf{b}}-\bar{\theta}_{n})\|\), following (16)._

**Remark 3**.: _A natural question that arises after Theorem 3 is whether it is possible to prove similar bounds for the iterates of first-order stochastic optimization algorithms. There are several MSE bounds for corresponding algorithms with explicit dependence on the step size \(\alpha_{k}\); see, for example, [45, 5]. Therefore, we expect that it is possible to obtain a counterpart to Theorem 2. At the same time, for general first-order stochastic optimization algorithms, unlike LSA, there are no counterparts to the precise error expansions of [1]. Thus, proving the counterpart of Theorem 3 in this setting is more challenging. Similarly, we emphasize that generalizations of the procedure (16) to cases where \(\{Z_{k}\}_{k\in\mathbb{N}}\) are dependent, for example, form a Markov chain, are complicated. The approach of [63] is not directly applicable in this setting, and appropriate generalization of (13) is a separate and challenging research direction._

## 5 Applications to the TD learning and numerical results

We illustrate our findings for the setting of temporal difference (TD) learning algorithm [66, 67] for policy evaluation in RL. Non-asymptotic error bounds for this algorithm attracted lot of contributions [43, 16, 30, 51, 38]. At the same time, confidence intervals for TD were studied in [22, 56] only in terms of their asymptotic validity. In the TD algorithm we consider a discounted MDP (Markov Decision Process) given by a tuple \((\mathcal{S},\mathcal{A},\mathrm{P},r,\gamma)\). Here \(\mathcal{S}\) and \(\mathcal{A}\) stand for state and action spaces, and \(\gamma\in(0,1)\) is a discount factor. Assume that \(\mathcal{S}\) is a complete metric space with metric \(\mathsf{d}_{\mathcal{S}}\) and Borel \(\sigma\)-algebra \(\mathcal{B}(\mathcal{S})\). \(\mathrm{P}\) stands for the transition kernel \(\mathrm{P}(B|s,a)\), which determines the probability of moving from state \(s\) to a set \(B\in\mathcal{B}(\mathcal{S})\) when action \(a\) is performed. Reward function \(r\colon\mathcal{S}\times\mathcal{A}\to[0,1]\) is assumed to be deterministic. _Policy_\(\pi(\cdot|s)\) is the distribution over action space \(\mathcal{A}\) corresponding to agent's action preferences in state \(s\in\mathcal{S}\). We aim to estimate _value function_

\[V^{\pi}(s)=\mathbb{E}\big{[}\sum_{k=0}^{\infty}\gamma^{k}r(s_{k},a_{k})|s_{0}=s \big{]}\;,\]where \(a_{k}\sim\pi(\cdot|s_{k})\), and \(s_{k+1}\sim\mathrm{P}(\cdot|s_{k},a_{k})\) for any \(k\in\mathbb{N}\). Define the transition kernel under \(\pi\),

\[\mathrm{P}_{\pi}(B|s)=\int_{\mathcal{A}}\mathrm{P}(B|s,a)\pi(\mathrm{d}a|s)\,\] (21)

which corresponds to the \(1\)-step transition probability from state \(s\) to a set \(B\in\mathcal{B}(\mathcal{S})\). The state space \(\mathcal{S}\) here can be arbitrary. It is a common option to consider the _linear function approximation_ for \(V^{\pi}(s)\), defined for \(s\in\mathcal{S}\), \(\theta\in\mathbb{R}^{d}\), and a feature mapping \(\varphi\colon\mathcal{S}\to\mathbb{R}^{d}\) as \(V^{\pi}_{q}(s)=\varphi^{\top}(s)\theta\). Here \(d\) is the dimension of feature space. Our goal is to find a parameter \(\theta^{\star}\) which is defined as a unique solution to the projected Bellman equation, see [70]. We denote by \(\mu\) the invariant distribution over the state space \(\mathcal{S}\) induced by \(\mathrm{P}^{\pi}(\cdot|s)\) in (21). We define the _design matrix_\(\Sigma_{\varphi}\) as

\[\Sigma_{\varphi}=\mathbb{E}_{\mu}[\varphi(s)\varphi(s)^{\top}]\in\mathbb{R}^{ d\times d}\.\] (22)

Consider the following assumptions on the generative mechanism and on the feature mapping \(\varphi(\cdot)\):

**TD 1**.: _Tuples \((s,a,s^{\prime})\) are generated i.i.d.with \(s\sim\mu\), \(a\sim\pi(\cdot|s)\), \(s^{\prime}\sim\mathrm{P}(\cdot|s,a)\)._

**TD 2**.: _Matrix \(\Sigma_{\varphi}\) is non-degenerate with the minimal eigenvalue \(\lambda_{\min}(\Sigma_{\varphi})>0\). Moreover, the feature mapping \(\varphi(\cdot)\) satisfies \(\sup_{s\in\mathcal{S}}\|\varphi(s)\|\leq 1\)._

In the setting of linear function approximation the estimation of \(V^{\pi}(s)\) reduces to estimating \(\theta^{\star}\in\mathbb{R}^{d}\), which can be done via the LSA procedure. Here, the \(k\)-th step randomness is given by the tuple \(Z_{k}=(s_{k},a_{k},s^{\prime}_{k})\). Then, the corresponding LSA update can be written as

\[\theta_{k}=\theta_{k-1}-\alpha_{k}(\mathbf{A}_{k}\theta_{k-1}-\mathbf{b}_{k})\,\] (23)

where \(\mathbf{A}_{k}\) and \(\mathbf{b}_{k}\) are given, respectively, by

\[\mathbf{A}_{k}=\varphi(s_{k})\{\varphi(s_{k})-\gamma\varphi(s^{\prime}_{k})\} ^{\top}\,\quad\mathbf{b}_{k}=\varphi(s_{k})r(s_{k},a_{k})\.\]

We provide the expressions for the corresponding system matrix \(\tilde{\mathbf{A}}=\mathbb{E}[\mathbf{A}_{k}]\) and the right-hand side \(\tilde{\mathbf{b}}\) in Appendix E. We verify that assumption A2 holds and, furthermore, we provide a tighter counterpart to the result of Proposition 1. This result closely follows [51] and [61].

**Proposition 2**.: _Let \(\{\theta\}_{k\in\mathbb{N}}\) be a sequence of TD updates generated by (23) under **TD 1** and **TD 2**. Then this update scheme satisfies assumption A2 with_

\[\mathrm{C}_{\mathbf{A}}=2(1+\gamma)\,\quad\|\varepsilon\|_{\infty}=2(1+ \gamma)(\|\theta^{\star}\|+1)\,\]

_moreover, one can check that \(\|\mathrm{I}-\alpha\tilde{A}\|^{2}\leq 1-\alpha a\) with_

\[a=(1-\gamma)\lambda_{\min}(\Sigma_{\varphi})\,\quad\alpha_{\infty}=(1- \gamma)/(1+\gamma)^{2}\,\]

_that is, Proposition 1 holds with \(Q=\mathrm{I}\)._

Proof of Proposition 2 is provided in Appendix E. Since all the assumptions in A2 are fulfilled, we can verify tightness of the bound Theorem 2 for different learning rate schedules \(\alpha_{k}\) in (23).

Numerical results.Efficiency of the multiplier bootstrap approach (16) to the problems of constructing confidence sets in online algorithms has been demonstrated in the works [22] and [56]. We aim to illustrate the tightness of our bounds for normal approximation outlined in Theorem 2 in the setting of TD learning with linear function approximation. To this end, we consider the classical Garnet problem [3], in the simplified version proposed by [25]. This problem is characterized by the number of states \(N_{s}\), number of actions \(a\), and branching factor \(b\) (i.e. the number of neighbors of each state in the MDP). We set these values to \(N_{s}=10\), \(a=2\) and \(b=3\), and aim to evaluate the value function of the randomly generated policy \(\pi(\cdot|s)\). Details on the way the policy \(\pi\) is set can be found in Appendix F. We consider the problem of policy evaluation in this MDP using the TD learning algorithm with identity feature mapping, that is, \(\phi(s)=e_{s}\) (that is, \(s\)-th coordinate vector) for \(s\in\{1,\ldots,N_{s}\}\). We run the procedure (23) with the learning rates \(\alpha_{k}=c_{0}/k^{\gamma}\) and different powers \(\gamma\in\{0.5,0.65,0.7\}\). For each of the experiments we aim to estimate the supremum

\[\Delta_{n}:=\sup_{x\in\mathbb{R}}\lVert\mathbb{P}(\sqrt{n}\|\bar{\theta}_{n}- \theta^{\star}\|\leq x)-\mathbb{P}(\|\Sigma_{\infty}^{1/2}\eta\|\leq x)\big{|}\,\] (24)

\(\eta\sim\mathcal{N}(0,\mathrm{I}_{N_{s}})\), and show that this supremum scales as \(n^{-1/4}\) when \(\gamma=1/2\) and admits slower decay for other powers of \(\gamma\). We approximate true probability \(\mathbb{P}(\|\Sigma_{\infty}^{1/2}\eta\|\leq x)\) by the corresponding empirical probabilities based on sample of size \(M\gg n\). Second, for \(n\in\{1600,\ldots,1638400\}\), where next sample size is twice larger than the previous one, we generate \(N=6553600\) trajectories of TD algorithm and approximate the distribution of \(\sqrt{n}\|\bar{\theta}_{n}-\theta^{\star}\|\) based on the corresponding empirical distribution. We report our results in Figure 1, showing that the smallest values of \(\Delta_{n}\) correspond to the step size schedule \(\gamma=1/2\), moreover, the decay rate \(n^{-1/4}\) seems to be tight, otherwise one should expect further decay of \(\Delta_{n}n^{1/4}\). Additional simulations are provided in Appendix F.

## 6 Conclusion

In this paper, we have established, to the best of our knowledge, the first fully non-asymptotic confidence bounds for parameter estimation in the LSA algorithm using the multiplier bootstrap. This result is based on a novel Berry-Esseen bound for the Polyak-Ruppert averaged LSA iterates, which is of independent interest. Our paper suggests several interesting directions for further research. First, our Berry-Esseen bounds are obtained using the randomized concentration inequality [63], and it would be valuable to generalize this approach to the setting of Markov chains. Second, it is natural to extend our results to the first-order gradient methods, both for stochastic optimization and variational inequalities. Third, it becomes possible to prove the fully non-asymptotic validity of confidence intervals obtained with plug-in techniques or other estimators of the asymptotic covariance matrix of \(\bar{\theta}_{n}\). These could then be compared with the multiplier bootstrap confidence intervals in terms of their dependence on problem dimension \(d\) and other instance-dependent quantities.

## Acknowledgement

The work of S. Samsonov and A. Naumov was prepared within the framework of the HSE University Basic Research Program. The work of E. Moulines has been partly funded by the European Union (ERC-2022-SYG-OCEAN-101071601). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them. The work of Q.-M. Shao is partially supported by National Nature Science Foundation of China NSFC 12031005 and Shenzhen Outstanding Talents Training Fund, China. The work of Z.-S. Zhang is partially supported by National Nature Science Foundation of China NSFC 12301183 and National Nature Science Found for Excellent Young Scientists Fund. This research was supported in part through computational resources of HPC facilities at HSE University [33].

Figure 1: Subfigure (a): Rescaled error \(\sqrt{n}\|\bar{\theta}_{n}-\theta^{*}\|\), averaged over \(N\) independent TD trajectories for different trajectory lengths \(n\). Subfigure (b): approximate quantity \(\Delta_{n}\) from (24) for different powers \(\gamma\) and \(n\). Subfigure (c): \(\Delta_{n}\), rescaled by a factor \(n^{1/4}\), predicted by Theorem 2.

## References

* [1] Rafik Aguech, Eric Moulines, and Pierre Priouret. On a perturbation approach for the analysis of stochastic tracking algorithms. _SIAM Journal on Control and Optimization_, 39(3):872-899, 2000.
* [2] Andreas Anastasiou, Krishnakumar Balasubramanian, and Murat A. Erdogdu. Normal approximation for stochastic gradient descent via non-asymptotic rates of martingale CLT. In Alina Beygelzimer and Daniel Hsu, editors, _Proceedings of the Thirty-Second Conference on Learning Theory_, volume 99 of _Proceedings of Machine Learning Research_, pages 115-137. PMLR, 25-28 Jun 2019.
* [3] TW Archibald, KIM McKinnon, and LC Thomas. On the generation of markov decision processes. _Journal of the Operational Research Society_, 46(3):354-361, 1995.
* [4] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. _Machine learning_, 47:235-256, 2002.
* [5] F. Bach and E. Moulines. Non-strongly-convex smooth stochastic approximation with convergence rate o(1/n). In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 26. Curran Associates, Inc., 2013.
* [6] A. Benveniste, M. Metivier, and P. Priouret. _Adaptive algorithms and stochastic approximations_, volume 22. Springer Science & Business Media, 2012.
* [7] J. Bhandari, D. Russo, and R. Singal. A finite time analysis of temporal difference learning with linear function approximation. In _Conference On Learning Theory_, pages 1691-1692, 2018.
* 688, 1982.
* [9] Vivek S Borkar. _Stochastic Approximation: A Dynamical Systems Viewpoint_. Cambridge University Press, 2008.
* [10] Nicolas Brosse, Alain Durmus, and Eric Moulines. The promises and pitfalls of stochastic gradient langevin dynamics. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018.
* 273, 2020.
* [12] Victor Chernozhukov, Denis Chetverikov, and Kengo Kato. Gaussian approximations and multiplier bootstrap for maxima of sums of high-dimensional random vectors. _Ann. Statist._, 41(6):2786-2819, 2013.
* [13] Victor Chernozhukov, Denis Chetverikov, and Kengo Kato. Central limit theorems and bootstrap in high dimensions. _Ann. Probab._, 45(4):2309-2352, 2017.
* [14] G. Dalal, Balazs Szorenyi, and G. Thoppe. A tale of two-timescale reinforcement learning with the tightest finite-time bound. _arXiv preprint arXiv:1911.09157_, 2019.
* [15] John C Duchi, Alekh Agarwal, Mikael Johansson, and Michael I Jordan. Ergodic mirror descent. _SIAM Journal on Optimization_, 22(4):1549-1578, 2012.
* [16] Alain Durmus, Eric Moulines, Alexey Naumov, and Sergey Samsonov. Finite-time high-probability bounds for Polyak-Ruppert averaged iterates of linear stochastic approximation. _Mathematics of Operations Research_, 2024.
* [17] Alain Durmus, Eric Moulines, Alexey Naumov, Sergey Samsonov, Kevin Scaman, and Hoi-To Wai. Tight high probability bounds for linear stochastic approximation with fixed stepsize. In M. Ranzato, A. Beygelzimer, K. Nguyen, P. S. Liang, J. W. Vaughan, and Y. Dauphin, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 30063-30074. Curran Associates, Inc., 2021.

* [18] Alain Durmus, Eric Moulines, Alexey Naumov, Sergey Samsonov, and Hoi-To Wai. On the stability of random matrix product with markovian noise: Application to linear stochastic approximation and td learning. In Mikhail Belkin and Samory Kpotufe, editors, _Proceedings of Thirty Fourth Conference on Learning Theory_, volume 134 of _Proceedings of Machine Learning Research_, pages 1711-1752. PMLR, 15-19 Aug 2021.
* [19] Bradley Efron. Bootstrap methods: another look at the jackknife. In _Breakthroughs in statistics: Methodology and distribution_, pages 569-593. Springer, 1992.
* 125, 1945.
* [21] E. Eweda and O. Macchi. Quadratic mean and almost-sure convergence of unbounded stochastic approximation algorithms with correlated observations. _Ann. Inst. H. Poincare Sect. B (N.S.)_, 19(3):235-255, 1983.
* [22] Yixin Fang, Jinfeng Xu, and Lei Yang. Online bootstrap confidence intervals for the stochastic gradient descent estimator. _Journal of Machine Learning Research_, 19(78):1-21, 2018.
* [23] G. Fort. Central limit theorems for stochastic approximation with controlled Markov chain dynamics. _ESAIM: PS_, 19:60-80, 2015.
* [24] Robert E Gaunt and Siqi Li. Bounding Kolmogorov distances through Wasserstein and related integral probability metrics. _Journal of Mathematical Analysis and Applications_, 522(1):126985, 2023.
* [25] Matthieu Geist, Bruno Scherrer, et al. Off-policy learning with eligibility traces: a survey. _J. Mach. Learn. Res._, 15(1):289-333, 2014.
* [26] Ian J. Goodfellow, Yoshua Bengio, and Aaron Courville. _Deep Learning_. MIT Press, Cambridge, MA, USA, 2016. http://www.deeplearningbook.org.
* [27] Friedrich Gotze, Alexey Naumov, Vladimir Spokoiny, and Vladimir Ulyanov. Large ball probabilities, Gaussian comparison and anti-concentration. _Bernoulli_, 25(4A):2538-2563, 2019.
* [28] Botao Hao, Yasin Abbasi Yadkori, Zheng Wen, and Guang Cheng. Bootstrapping upper confidence bound. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* [29] De Huang, Jonathan Niles-Weed, Joel A Tropp, and Rachel Ward. Matrix concentration for products. _Foundations of Computational Mathematics_, pages 1-33, 2021.
* [30] Dongyan Huo, Yudong Chen, and Qiaomin Xie. Bias and extrapolation in markovian linear stochastic approximation with constant stepsizes. In _Abstract Proceedings of the 2023 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems_, pages 81-82, 2023.
* [31] Moritz Jirak and Martin Wahl. Quantitative limit theorems and bootstrap approximations for empirical spectral projectors. _Probability Theory and Related Fields_, 190(1):119-177, 2024.
* [32] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [33] PS Kostenetskiy, RA Chulkevich, and VI Kozyrev. Hpc resources of the higher school of economics. In _Journal of Physics: Conference Series_, volume 1740, page 012050. IOP Publishing, 2021.
* [34] Harold Kushner and G George Yin. _Stochastic approximation and recursive algorithms and applications_, volume 35. Springer Science & Business Media, 2003.
* [35] C. Lakshminarayanan and C. Szepesvari. Linear stochastic approximation: How far does constant step-size and iterate averaging go? In _International Conference on Artificial Intelligence and Statistics_, pages 1347-1355, 2018.

* [36] Guanghui Lan. An optimal method for stochastic composite optimization. _Mathematical Programming_, 133(1-2):365-397, 2012.
* [37] Sokbae Lee, Yuan Liao, Myung Hwan Seo, and Youngki Shin. Fast inference for quantile regression with tens of millions of observations. _Journal of Econometrics_, page 105673, 2024.
* [38] Gen Li, Weichen Wu, Yuejie Chi, Cong Ma, Alessandro Rinaldo, and Yuting Wei. High-probability sample complexities for policy evaluation with linear function approximation. _IEEE Transactions on Information Theory_, 70(8):5969-5999, 2024.
* [39] Xiang Li, Jiadong Liang, Xiangyu Chang, and Zhihua Zhang. Statistical estimation and online inference via local sgd. In Po-Ling Loh and Maxim Raginsky, editors, _Proceedings of Thirty Fifth Conference on Learning Theory_, volume 178 of _Proceedings of Machine Learning Research_, pages 1613-1661. PMLR, 02-05 Jul 2022.
* [40] Xiang Li, Jiadong Liang, and Zhihua Zhang. Online statistical inference for nonlinear stochastic approximation with Markovian data. _arXiv preprint arXiv:2302.07690_, 2023.
* [41] Xiang Li, Wenhao Yang, Jiadong Liang, Zhihua Zhang, and Michael I Jordan. A statistical analysis of Polyak-Ruppert averaged Q-learning. In _International Conference on Artificial Intelligence and Statistics_, pages 2207-2261. PMLR, 2023.
* [42] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. _Nature_, 518(7540):529-533, 2015.
* [43] Wenlong Mou, Chris Junchi Li, Martin J Wainwright, Peter L Bartlett, and Michael I Jordan. On linear stochastic approximation: Fine-grained Polyak-Ruppert and non-asymptotic concentration. In _Conference on Learning Theory_, pages 2947-2997. PMLR, 2020.
* [44] Wenlong Mou, Ashwin Pananjady, Martin J Wainwright, and Peter L Bartlett. Optimal and instance-dependent guarantees for markovian linear stochastic approximation. _Mathematical Statistics and Learning_, 7(1):41-153, 2024.
* [45] Eric Moulines and Francis Bach. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. _Advances in neural information processing systems_, 24:451-459, 2011.
* [46] Alexey Naumov, Vladimir Spokoiny, and Vladimir Ulyanov. Bootstrap confidence sets for spectral projectors of sample covariance. _Probab. Theory Related Fields_, 174(3-4):1091-1132, 2019.
* [47] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. _SIAM Journal on optimization_, 19(4):1574-1609, 2009.
* [48] Arkadij Semenovic Nemirovskij and David Borisovich Yudin. Problem complexity and method efficiency in optimization. 1983.
* [49] Ivan Nourdin, Giovanni Peccati, and Xiaochuan Yang. Multivariate normal approximation on the wiener space: new bounds in the convex distance. _Journal of Theoretical Probability_, 35(3):2020-2037, 2022.
* [50] A. Osekowski. _Sharp Martingale and Semimartingale Inequalities_. Monografie Matematyczne 72. Birkhauser Basel, 1 edition, 2012.
* [51] Gandharv Patil, LA Prashanth, Dheeraj Nagaraj, and Doina Precup. Finite time analysis of temporal difference learning with linear function approximation: Tail averaging and regularisation. In _International Conference on Artificial Intelligence and Statistics_, pages 5438-5448. PMLR, 2023.

* [52] V. Petrov. _Sums of Independent Random Variables_. Ergebnisse der Mathematik und ihrer Grenzgebiete. 2. Folge. Springer Berlin Heidelberg, 1975.
* [53] Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. _SIAM journal on control and optimization_, 30(4):838-855, 1992.
* [54] A. S. Poznyak. _Advanced Mathematical Tools for Automatic Control Engineers: Deterministic Techniques_. Elsevier, Oxford, 2008.
* [55] Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for strongly convex stochastic optimization. In _Proceedings of the 29th International Coference on International Conference on Machine Learning_, pages 1571-1578, 2012.
* [56] Pratik Ramprasad, Yuantong Li, Zhuoran Yang, Zhaoran Wang, Will Wei Sun, and Guang Cheng. Online bootstrap inference for policy evaluation in reinforcement learning. _J. Amer. Statist. Assoc._, 118(544):2901-2914, 2023.
* 293, 2011.
* [58] Donald B Rubin. The bayesian bootstrap. _The annals of statistics_, pages 130-134, 1981.
* [59] David Ruppert. Efficient estimations from a slowly convergent robbins-monro process. Technical report, Cornell University Operations Research and Industrial Engineering, 1988.
* [60] Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. _Mathematics of Operations Research_, 39(4):1221-1243, 2014.
* [61] Sergey Samsonov, Daniil Tiapkin, Alexey Naumov, and Eric Moulines. Improved High-Probability Bounds for the Temporal Difference Learning Algorithm via Exponential Stability. In Shipra Agrawal and Aaron Roth, editors, _Proceedings of Thirty Seventh Conference on Learning Theory_, volume 247 of _Proceedings of Machine Learning Research_, pages 4511-4547. PMLR, 30 Jun-03 Jul 2024.
* [62] Jun Shao. _Mathematical statistics_. Springer Science & Business Media, 2003.
* [63] Qi-Man Shao and Zhuo-Song Zhang. Berry-Esseen bounds for multivariate nonlinear statistics with applications to M-estimators and stochastic gradient descent algorithms. _Bernoulli_, 28(3):1548-1576, 2022.
* 2675, 2015.
* [65] R Srikant. Rates of convergence in the central limit theorem for markov chains, with an application to TD learning. _arXiv preprint arXiv:2401.15719_, 2024.
* [66] R. S Sutton. Learning to predict by the methods of temporal differences. _Machine learning_, 3(1):9-44, 1988.
* [67] R. S. Sutton and Andrew G. Barto. _Reinforcement Learning: An Introduction_. The MIT Press, second edition, 2018.
* [68] Joel A. Tropp. Freedman's inequality for matrix martingales. _Electron. Commun. Probab._, 16:262-270, 2011.
* [69] Joel A Tropp et al. An introduction to matrix concentration inequalities. _Foundations and Trends(r) in Machine Learning_, 8(1-2):1-230, 2015.
* [70] J. N. Tsitsiklis and B. Van Roy. An analysis of temporal-difference learning with function approximation. _IEEE Transactions on Automatic Control_, 42(5):674-690, May 1997.
* [71] A. W. Van Der Vaart and J. A. Wellner. _Weak convergence and empirical processes_. Springer Series in Statistics, 1996.
* [72] Vladimir Vapnik. _The nature of statistical learning theory_. Springer science & business media, 2013.

* [73] Xi Chen Wanrong Zhu and Wei Biao Wu. Online Covariance Matrix Estimation in Stochastic Gradient Descent. _Journal of the American Statistical Association_, 118(541):393-404, 2023.
* [74] Yanjie Zhong, Todd Kuffner, and Soumendra Lahiri. Online Bootstrap Inference with Nonconvex Stochastic Gradient Descent Estimator. _arXiv preprint arXiv:2306.02205_, 2023.
* [75] Vladimir Mikhailovich Zolotarev. Probability metrics. _Theory of Probability & Its Applications_, 28(2):278-302, 1984.

Proofs for accuracy of normal approximation

### Expansion of the error of LSA equipped with the Polyak-Ruppert averaging

**Proposition 3**.: _The following expansion holds:_

\[\sqrt{n}\bar{\mathbf{A}}(\bar{\theta}_{n}-\theta^{\star})=-\underbrace{ \frac{1}{\sqrt{n}}\sum_{k=n+1}^{2n}}_{W}\varepsilon_{k}+\underbrace{\frac{1}{ \sqrt{n}}\frac{\theta_{n}-\theta^{\star}}{\alpha_{n}}}_{D_{1}}-\underbrace{ \frac{1}{\sqrt{n}}\frac{\theta_{2n}-\theta^{\star}}{\alpha_{2n}}}_{D_{2}}\\ -\underbrace{\frac{1}{\sqrt{n}}\sum_{k=n+1}^{2n}(\mathbf{A}_{k}- \bar{\mathbf{A}})(\theta_{k-1}-\theta^{\star})}_{D_{3}}+\underbrace{\frac{1}{ \sqrt{n}}\sum_{k=n+1}^{2n}\left(\theta_{k-1}-\theta^{\star}\right)\left(\frac{ 1}{\alpha_{k}}-\frac{1}{\alpha_{k-1}}\right)}_{D_{4}}\] (25)

Proof.: We use the recurrence (3) and rewrite it as

\[\theta_{k}-\theta^{\star}=(\mathrm{I}-\alpha_{k}\bar{\mathbf{A}})(\theta_{k- 1}-\theta^{\star})-\alpha_{k}(\mathbf{A}_{k}-\bar{\mathbf{A}})(\theta_{k-1}- \theta^{\star})-\alpha_{k}\varepsilon_{k}\;.\]

The previous equation implies, after algebraic manipulation and division by \(\alpha_{k}\), that

\[\bar{\mathbf{A}}(\theta_{k-1}-\theta^{\star})=\frac{\theta_{k-1}-\theta^{ \star}}{\alpha_{k}}-\frac{\theta_{k}-\theta^{\star}}{\alpha_{k}}-(\mathbf{A}_ {k}-\bar{\mathbf{A}})(\theta_{k-1}-\theta^{\star})-\varepsilon_{k}\;.\]

Taking average for \(k\) from \(n+1\) to \(2n\) and multiplying by \(\sqrt{n}\), we obtain (25). 

### Bounding the error of the LSA algorithm last iterate

We begin with of technical lemma on the behavior of the last iterate \(\theta_{k}\) of the LSA procedure given in (1). We aim to show that \(\mathbb{E}^{1/p}[\|\theta_{k}-\theta^{\star}\|^{p}]\) scales as \(\sqrt{\alpha_{k}}\), provided that \(k\) is large enough. This result is classical and appears in a number of papers, e.g. [7, 14, 43, 17]. We provide the proof here for completeness. Our analysis of the bootstrap procedure and the last iterate error of LSA procedure is based on the error expansion technique from [1], see also [16]. Namely, to perform the expansion, we decompose the LSA iterates \(\theta_{k}\) defined in (1) into a transient and fluctuation terms:

\[\theta_{k}-\theta^{\star}=\tilde{\theta}_{k}^{(\mathsf{tr})}+\tilde{\theta}_ {k}^{(\mathsf{fi})}\;,\]

where we have defined the quantities

\[\tilde{\theta}_{k}^{(\mathsf{tr})}=\Gamma_{1:k}\{\theta_{0}-\theta^{\star}\} \;,\quad\tilde{\theta}_{k}^{(\mathsf{fi})}=-\sum_{j=1}^{k}\alpha_{j}\Gamma_{j +1:k}\varepsilon_{j}\;,\] (26)

setting

\[\Gamma_{m:k}=\prod_{i=m}^{k}(\mathrm{I}-\alpha_{i}\mathbf{A}(Z_{i}))\;,\quad m,k\in\mathbb{N},m\leq k\;,\;\text{with the convention }\Gamma_{m:k}=\mathrm{I}\;,m>k\;.\] (27)

The dependence of \(\Gamma_{m:k}\) upon the stepsizes \((\alpha_{j})\) is implicit in (27). Here the quantity \(\tilde{\theta}_{k}^{(\mathsf{tr})}\) is the transient component of the error, which determines the rate at which the initial error \(\theta_{0}-\theta^{\star}\) is forgotten. The term \(\tilde{\theta}_{k}^{(\mathsf{fi})}\) corresponds to the fluctuation component of the error and is determined by the oscillations of the last iterate \(\theta_{k}\) around \(\theta^{\star}\).

**Proposition 4**.: _Assume A.1, A.2, and A.3. Then for any \(k\geq n\), where \(n\) satisfies (9), it holds for \(2\leq p\leq\log n^{2}\), that_

\[\mathbb{E}^{1/p}[\|\theta_{k}-\theta^{\star}\|^{p}]\leq\sqrt{\kappa_{Q}} \mathrm{e}\mathrm{exp}\big{\{}-(a/2)\sum_{\ell=1}^{k}\alpha_{\ell}\big{\}}\| \theta_{0}-\theta^{\star}\|+\frac{4\mathrm{e}\sqrt{\kappa_{Q}}\|\varepsilon\|_ {\infty}p}{\sqrt{a}}\sqrt{\alpha_{k}}\;.\]Proof.: Expanding the decomposition (26), we obtain that

\[\mathbb{E}^{1/p}[\|\theta_{k}-\theta^{\star}\|^{p}]\leq\mathbb{E}^{1/p}[\|\Gamma_ {1:k}\{\theta_{0}-\theta^{\star}\}\|^{p}]+\mathbb{E}^{1/p}[\|\sum_{j=1}^{k} \alpha_{j}\Gamma_{j+1:k}\varepsilon_{j}\|^{p}]\;,\] (28)

and we bound both terms separately. Since the sample size \(n\) satisfies (9), we get applying Corollary 4 (see equation (71)), that for \(2\leq p\leq\log n^{2}\) it holds

\[\mathbb{E}^{1/p}[\|\Gamma_{1:k}\{\theta_{0}-\theta^{\star}\}\|^{p}]\leq\sqrt{ \kappa_{Q}}\mathrm{e}\exp\bigl{\{}-(a/2)\sum_{\ell=1}^{k}\alpha_{\ell}\bigr{\}} \|\theta_{0}-\theta^{\star}\|\;.\]

Now we proceed with the second term in (28). Applying Burkholder's inequality [50, Theorem 8.6] and Lemma 3 with \(b=a/4\), we obtain that

\[\mathbb{E}^{1/p}[\|\sum_{j=1}^{k}\alpha_{j}\Gamma_{j+1:k}\varepsilon _{j}\|^{p}] \leq p\left(\mathbb{E}^{2/p}\left[\left(\sum_{j=1}^{k}\alpha_{j} ^{2}\|\Gamma_{j+1:k}\varepsilon_{j}\|^{2}\right)^{p/2}\right]\right)^{1/2}\] \[\leq p\sqrt{\kappa_{Q}}\mathrm{e}\|\varepsilon\|_{\infty}\biggl{(} \sum_{j=1}^{k}\alpha_{j}^{2}\prod_{\ell=j+1}^{k}\bigl{(}1-\frac{a\alpha_{\ell }}{4}\bigr{)}\biggr{)}^{1/2}\] \[\leq\frac{4\mathrm{e}\sqrt{\kappa_{Q}}\|\varepsilon\|_{\infty}p}{ \sqrt{a}}\sqrt{\alpha_{k}}\;.\]

**Corollary 2**.: _Under assumptions of Proposition 4, it holds that_

\[\mathbb{P}\left(\exists k\in[n,2n-1]:\|\theta_{k}-\theta^{\star}\|\geq g(k,\| \theta_{0}-\theta^{\star}\|,n)\right)\leq\frac{1}{n}\;,\]

_where we have defined_

\[g(k,\|\theta_{0}-\theta^{\star}\|,n)=\sqrt{\kappa_{Q}}\mathrm{e}^{2}\exp\bigl{\{} -(a/2)\sum_{\ell=1}^{k}\alpha_{\ell}\bigr{\}}\|\theta_{0}-\theta^{\star}\|+ \frac{8\mathrm{e}^{2}\sqrt{\kappa_{Q}}\|\varepsilon\|_{\infty}\log n}{\sqrt{ a}}\sqrt{\alpha_{k}}\;.\]

Proof.: We first note that Lemma 1 implies, setting \(\delta=1/n^{2}\), that for every fixed \(k\in[n;2n-1]\),

\[\mathbb{P}\left(\|\theta_{k}-\theta^{\star}\|\geq\sqrt{\kappa_{Q}}\mathrm{e}^ {2}\exp\bigl{\{}-(a/2)\sum_{\ell=1}^{k}\alpha_{\ell}\bigr{\}}\|\theta_{0}- \theta^{\star}\|+\frac{8\mathrm{e}^{2}\sqrt{\kappa_{Q}}\|\varepsilon\|_{\infty }\log n}{\sqrt{a}}\sqrt{\alpha_{k}}\right)\leq\frac{1}{n^{2}}\;.\]

Application of the union bound concludes the proof. 

We conclude this part with a simple consequence of Markov's inequality.

**Lemma 1**.: _Fix \(\delta\in(0,1/\mathrm{e}^{2})\) and let \(Y\) be a positive random variable, such that_

\[\mathbb{E}^{1/p}[Y^{p}]\leq C_{1}+C_{2}p\]

_for any \(2\leq p\leq\log\left(1/\delta\right)\). Then it holds with probability at least \(1-\delta\), that_

\[Y\leq\mathrm{e}C_{1}+\mathrm{e}C_{2}\log\left(1/\delta\right)\;.\]

Proof.: Applying Markov's inequality, for any \(t\geq 0\) we get that

\[\mathbb{P}(Y\geq t)\leq\frac{\mathbb{E}[Y^{p}]}{t^{p}}\leq\frac{(C_{1}+C_{2}p) ^{p}}{t^{p}}\;.\]

Now we set \(p=\log\left(1/\delta\right)\), \(t=\mathrm{e}C_{1}+\mathrm{e}C_{2}\log\left(1/\delta\right)\), and aim to check that

\[\frac{(C_{1}+C_{2}\log\left(1/\delta\right))^{\log\left(1/\delta\right)}}{( \mathrm{e}C_{1}+\mathrm{e}C_{2}\log\left(1/\delta\right))^{\log\left(1/\delta \right)}}\leq\delta\;.\]

Taking logarithms from both sides, the latter inequality is equivalent to

\[\log\left(1/\delta\right)\log\frac{C_{1}+C_{2}\log\left(1/\delta\right)}{ \mathrm{e}(C_{1}+C_{2}\log\left(1/\delta\right))}\leq\log\delta\;,\]

which turns into exact equality.

### Proof of Theorem 1

We first define explicitly the remainder term outlined in the statement of Theorem 1:

\[\Delta_{1}(n,a,\mathrm{C}_{\mathbf{A}},c_{0})=\frac{n^{\gamma-1/2}}{c_{0}}+\frac{ \mathrm{C}_{\mathbf{A}}}{n^{(1-\gamma)/2}\sqrt{c_{0}a}}+\frac{n^{2\gamma-3/2}}{ ac_{0}^{2}}\.\] (29)

Proof.: Since both terms in the right-hand side of the error bound of Proposition 4 scales linearly with \(\sqrt{\kappa_{Q}}\), for simplicity we do not trace it in the subsequent bounds (i.e. assume \(\kappa_{Q}=1\)), and then keep the required scaling with \(\kappa_{Q}\) only in the final bounds. The decomposition (25) is a key element of our proof and allows to treat different error sources \(D_{1}-D_{4}\) separately. For the last iterate we have, using Proposition 4, that

\[\mathbb{E}^{1/2}\big{[}\|\theta_{n}-\theta^{\star}\|^{2}\big{]} \lesssim\frac{\|\varepsilon\|_{\infty}}{\sqrt{a}}\sqrt{\alpha_{n} }+\exp\biggl{\{}-(a/2)\sum_{\ell=1}^{n}\alpha_{\ell}\biggr{\}}\|\theta_{0}- \theta^{\star}\|\] \[\mathbb{E}^{1/2}\big{[}\|\theta_{2n}-\theta^{\star}\|^{2}\big{]} \lesssim\frac{\|\varepsilon\|_{\infty}}{\sqrt{a}}\sqrt{\alpha_{2n} }+\exp\biggl{\{}-(a/2)\sum_{\ell=1}^{2n}\alpha_{\ell}\biggr{\}}\|\theta_{0}- \theta^{\star}\|\.\]

Thus, using that \(\sum_{k=1}^{n}\alpha_{k}\geq\frac{c_{0}(n^{1-\gamma}-1)}{1-\gamma}\) and \(c_{0}\leq 1-\gamma\), we obtain that

\[\mathbb{E}^{1/2}\big{[}\|D_{1}\|^{2}\big{]} \lesssim\frac{\|\varepsilon\|_{\infty}}{\sqrt{ac_{0}}n^{(1- \gamma)/2}}+\frac{n^{\gamma-1/2}}{c_{0}}\exp\biggl{\{}-\frac{c_{0}an^{1- \gamma}}{2(1-\gamma)}\biggr{\}}\|\theta_{0}-\theta^{\star}\|\,\] \[\mathbb{E}^{1/2}\big{[}\|D_{2}\|^{2}\big{]} \lesssim\frac{\|\varepsilon\|_{\infty}}{\sqrt{ac_{0}}n^{(1- \gamma)/2}}+\frac{n^{\gamma-1/2}}{c_{0}}\exp\biggl{\{}-\frac{c_{0}a(2n)^{1- \gamma}}{1-\gamma}\biggr{\}}\|\theta_{0}-\theta^{\star}\|\.\]

Now we proceed with \(D_{3}\). Since it is a sum of a martingale-difference sequence w.r.t. \(\mathcal{F}_{k}=\sigma(Z_{\ell},\ell\leq k)\), we get using Proposition 4, that

\[\mathbb{E}\big{[}\|D_{3}\|^{2}\big{]} \lesssim\frac{\mathrm{C}_{\mathbf{A}}^{2}}{n}\sum_{k=n}^{2n-1} \mathbb{E}[\|\theta_{k}-\theta^{\star}\|^{2}]\] \[\lesssim\frac{\mathrm{C}_{\mathbf{A}}^{2}}{n}\sum_{k=n+1}^{2n} \frac{\|\varepsilon\|_{\infty}^{2}\alpha_{k}}{a}+\frac{\mathrm{C}_{\mathbf{A} }^{2}}{n}\sum_{k=n+1}^{2n}\exp\biggl{\{}-a\sum_{\ell=1}^{k}\alpha_{\ell} \biggr{\}}\underbrace{\vphantom{\sum_{k=n+1}^{2n}\alpha_{k}\exp\biggl{\{}-a \sum_{\ell=n+1}^{k}\alpha_{\ell}\biggr{\}}}}_{S_{1}}\|\theta_{0}-\theta^{ \star}\|^{2}\] \[\lesssim\frac{c_{0}\,\mathrm{C}_{\mathbf{A}}^{2}\,\|\varepsilon\| _{\infty}^{2}}{a(1-\gamma)n^{\gamma}}+\frac{\mathrm{C}_{\mathbf{A}}^{2}}{n^{1- \gamma}c_{0}a}\exp\biggl{\{}-\frac{c_{0}an^{1-\gamma}}{1-\gamma}\biggr{\}}\| \theta_{0}-\theta^{\star}\|^{2}\,\]

where we additionally used that \(S_{1}\lesssim 1/a\) due to Lemma 3. Now it remains to bound the term \(D_{4}\) from the representation (25). Using Minkowski's inequality and Proposition 4, we get that

\[\mathbb{E}^{1/2}\big{[}\|D_{4}\|^{2}\big{]} \lesssim\frac{1}{\sqrt{n}}\sum_{k=n}^{2n-1}\mathbb{E}^{1/2}\big{[} \|\theta_{k}-\theta^{\star}\|^{2}\big{]}\left(\frac{1}{\alpha_{k}}-\frac{1}{ \alpha_{k-1}}\right)\] \[\lesssim\frac{1}{\sqrt{n}}\sum_{k=n}^{2n-1}\frac{\|\varepsilon\| _{\infty}(k^{\gamma}-(k-1)^{\gamma})}{c_{0}\sqrt{a}}\sqrt{\alpha_{k}}\] \[\qquad\qquad\qquad+\frac{1}{c_{0}\sqrt{n}}\sum_{k=n}^{2n-1}(k^{ \gamma}-(k-1)^{\gamma})\exp\biggl{\{}-(a/2)\sum_{\ell=1}^{k}\alpha_{\ell} \biggr{\}}\|\theta_{0}-\theta^{\star}\|\] \[\overset{(a)}{\lesssim}\frac{\|\varepsilon\|_{\infty}}{\sqrt{ac_ {0}}\sqrt{n}}\sum_{k=n}^{2n-1}\frac{1}{k^{1-\gamma/2}}+\frac{n^{2\gamma-3/2}}{ ac_{0}^{2}}\exp\biggl{\{}-\frac{c_{0}an^{1-\gamma}}{2(1-\gamma)}\biggr{\}}\| \theta_{0}-\theta^{\star}\|\] \[\lesssim\frac{\|\varepsilon\|_{\infty}}{\sqrt{ac_{0}}n^{(1-\gamma)/ 2}}+\frac{n^{2\gamma-3/2}}{ac_{0}^{2}}\exp\biggl{\{}-\frac{c_{0}an^{1-\gamma}}{2 (1-\gamma)}\biggr{\}}\|\theta_{0}-\theta^{\star}\|\.\]Here in (a) we additionally used that \(k^{\gamma}-(k-1)^{\gamma}\lesssim k^{1-\gamma}\) together with Lemma 3. Combining the estimates above yields the result of Theorem 1. 

We conclude this section with some technical lemmas.

**Lemma 2** (Lemma 24 in [18]).: _Let \(b>0\) and \((\alpha_{k})_{k\geq 0}\) be a non-increasing sequence such that \(\alpha_{0}\leq 1/b\). Then_

\[\sum_{j=1}^{n+1}\alpha_{j}\prod_{l=j+1}^{n+1}(1-\alpha_{l}b)=\frac{1}{b}\left\{ 1-\prod_{l=1}^{n+1}(1-\alpha_{l}b)\right\}\]

Proof.: The proof of this statement is given in [18], we provide it here for completeness. Let us denote \(u_{j:n+1}=\prod_{l=j}^{n+1}(1-\alpha_{l}b)\). Then, for \(j\in\{1,\ldots,n+1\}\), \(u_{j+1:n+1}-u_{j:n+1}=b\alpha_{j}u_{j+1:n+1}\). Hence,

\[\sum_{j=1}^{n+1}\alpha_{j}\prod_{l=j+1}^{n+1}(1-\alpha_{l}b)=\frac{1}{b}\sum_ {j=1}^{n+1}(u_{j+1:n+1}-u_{j:n+1})=b^{-1}(1-u_{1:n+1})\,\]

and the statement follows. 

**Lemma 3** (Modified Lemma 25 in [18]).: _Let \(b>0\) and let \(\alpha_{\ell}=c_{0}/\ell^{\gamma}\), \(\gamma\in[1/2;1)\), such that \(c_{0}\leq 1/b\). Then for any \(n\) satisfying_

\[n\geq 2+2\bigg{(}\frac{2\gamma}{c_{0}b}\bigg{)}^{1/(1-\gamma)}\,\quad\text{and}\quad\frac{n^{1-\gamma}}{1+\log(n)}\geq\frac{2\gamma(1- \gamma)}{c_{0}b(1-(1/2)^{1-\gamma}}\,\] (30)

_and any \(k\geq n\), it holds that_

\[\sum_{j=1}^{k+1}\alpha_{j}^{2}\prod_{\ell=j+1}^{k+1}(1-\alpha_{\ell}b)\leq(4/b )\alpha_{k+1}\.\]

Proof.: From elementary algebra, we obtain that

\[\alpha_{\ell}-\alpha_{\ell+1}=\frac{c_{0}}{\ell^{\gamma}}-\frac{c_{0}}{(\ell+ 1)^{\gamma}}=\frac{c_{0}((1+1/\ell)^{\gamma}-1)}{(\ell+1)^{\gamma}}\leq\frac{ c_{0}}{(\ell+1)^{\gamma}}\frac{\gamma}{\ell}\,\] (31)

where we used the fact that \((1+x)^{\gamma}\leq 1+\gamma x\) for \(\gamma\in[1/2;1)\) and \(x\in[0,1]\). Hence,

\[\frac{\alpha_{\ell}}{\alpha_{\ell+1}}\leq 1+\frac{\gamma}{\ell}\.\]

Thus we obtain that, since \(k\geq n\),

\[\sum_{j=1}^{k+1}\alpha_{j}^{2} \prod_{\ell=j+1}^{k+1}(1-\alpha_{\ell}b)=\alpha_{k+1}\sum_{j=1}^{k +1}\alpha_{j}\prod_{\ell=j+1}^{k+1}\bigg{(}\frac{\alpha_{\ell-1}}{\alpha_{ \ell}}\bigg{)}(1-\alpha_{\ell}b)\] \[\leq\alpha_{k+1}\sum_{j=1}^{k+1}\alpha_{j}\prod_{\ell=j+1}^{k+1} \bigg{(}1+\frac{\gamma}{\ell-1}\bigg{)}\left(1-\alpha_{\ell}b\right)\] \[\leq\alpha_{k+1}\sum_{j=1}^{k+1}\alpha_{j}\exp\left\{\sum_{\ell=j +1}^{n}\frac{\gamma}{\ell-1}\right\}\exp\left\{-\sum_{\ell=j+1}^{n}\alpha_{ \ell}b\right\}\exp\left\{-\frac{b}{2}\sum_{\ell=n+1}^{k+1}\alpha_{\ell}b \right\}\.\]

In the last identity we used the fact that, since \(n\) satisfies (30), it holds for \(\ell\geq n/2\) that

\[\frac{\gamma}{\ell-1}\leq\alpha_{\ell}b/2\.\] (32)We will now prove that for \(j\leq n-1\), it holds

\[\sum_{\ell=j+1}^{n}\frac{\gamma}{\ell-1}\leq(b/2)\sum_{\ell=j+1}^{n}\alpha_{\ell }\,\] (33)

For \(j\geq n/2\), the bound (33) directly follows from (32). We now turn to the proof of (33) for \(j\leq\lceil n/2\rceil\). Note first that

\[\sum_{\ell=j+1}^{n}\frac{\gamma}{\ell-1}\leq\sum_{\ell=2}^{n}\frac{\gamma}{ \ell-1}\leq\gamma(\log(n)+1)\.\]

On the other hand, we get

\[\sum_{\ell=j+1}^{n}\frac{1}{\ell^{\gamma}}\geq\int_{j+1}^{n+1}\frac{\mathrm{d} x}{x^{\gamma}}=\frac{\big{(}(n+1)^{1-\gamma}-(j+1)^{1-\gamma}\big{)}}{1-\gamma}\.\]

Comparing the above bounds, to ensure that (33) holds, it is enough to check that

\[\gamma(1+\log{(n)})\leq\frac{c_{0}b}{2(1-\gamma)}\big{(}n^{1-\gamma}-(n/2)^{1- \gamma}\big{)}\,.\] (34)

Note that (34) is guaranteed by (30). Using that \(e^{-x}\leq 1-x/2\) for \(x\in[0,1]\), we obtain that

\[\sum_{j=1}^{k+1}\alpha_{j}^{2}\prod_{\ell=j+1}^{k+1}(1-\alpha_{ \ell}b) \leq\alpha_{k+1}\sum_{j=1}^{k+1}\alpha_{j}\exp\biggl{\{}-(b/2) \sum_{\ell=j+1}^{k+1}\alpha_{\ell}\biggr{\}}\] \[\leq\alpha_{k+1}\sum_{j=1}^{k+1}\alpha_{j}\prod_{\ell=j+1}^{k+1} \big{(}1-(b/4)\alpha_{\ell}\big{)}\] \[\leq(4/b)\,\alpha_{k+1}\,\]

where the last inequality follows from Lemma 2. 

## Appendix B Proof of Theorem 2

We first define explicitly the remainder term outlined in the statement of Theorem 2:

\[\Delta_{2}(n,a,\mathrm{C}_{\mathbf{A}},\mathrm{Tr}\,\Sigma_{\varepsilon},c_{0} )=\kappa_{Q}\left(\sqrt{\mathrm{Tr}\,\Sigma_{\varepsilon}}\Delta_{1}+\frac{ \sqrt{\mathrm{Tr}\,\Sigma_{\varepsilon}}(\mathrm{C}_{\mathbf{A}}\lor 1)^{2}n^{ \gamma-1/2}}{ac_{0}}\right)\,\] (35)

and constants \(\mathsf{C}_{1},\mathsf{C}_{2},\mathsf{C}_{3},\mathsf{C}_{4}\) from Theorem 2, optimized bound (15), and Remark 1, respectively:

\[\mathsf{C}_{1} =\frac{\sqrt{\kappa_{Q}}\|\varepsilon\|_{\infty}\sqrt{\mathrm{ Tr}\,\Sigma_{\varepsilon}}}{\sqrt{ac_{0}}}+\frac{\kappa_{Q}(\mathrm{Tr}\, \Sigma_{\varepsilon}+\mathrm{C}_{\mathbf{A}}\,\sqrt{\mathrm{Tr}\,\Sigma_{ \varepsilon}}\|\varepsilon\|_{\infty})}{ac_{0}}\,\] (36) \[\mathsf{C}_{2} =\frac{\sqrt{\kappa_{Q}}\|\varepsilon\|_{\infty}\sqrt{\mathrm{ Tr}\,\Sigma_{\varepsilon}}c_{0}\,\mathrm{C}_{\mathbf{A}}}{\sqrt{ac_{0}}(1- \gamma)}+\kappa_{Q}\,\mathrm{C}_{\mathbf{A}}\,\sqrt{\mathrm{Tr}\,\Sigma_{ \varepsilon}}(\|\varepsilon\|_{\infty}+\sqrt{\mathrm{Tr}\,\Sigma_{\varepsilon} }+\mathrm{C}_{\mathbf{A}}\,\|\varepsilon\|_{\infty})}{ac_{0}}\,\] \[\mathsf{C}_{3} =\frac{\kappa_{Q}(c_{0}\,\mathrm{C}_{\mathbf{A}}\,\lor 1)\sqrt{ \mathrm{Tr}\,\Sigma_{\varepsilon}}(\|\varepsilon\|_{\infty}+\sqrt{\mathrm{Tr}\, \Sigma_{\varepsilon}^{(\Pi)}}+\mathrm{C}_{\mathbf{A}}\,\|\varepsilon\|_{ \infty})}{ac_{0}}\,\] \[\mathsf{C}_{4} =\frac{\kappa_{Q}(c_{0}\,\mathrm{C}_{\mathbf{A}}\,\lor 1)\sqrt{ \mathrm{Tr}\,\Sigma_{\varepsilon}^{(\Pi)}}\big{(}\|\varepsilon\|_{\infty}+\sqrt {\mathrm{Tr}\,\Sigma_{\varepsilon}^{(\Pi)}}+\mathrm{C}_{\mathbf{A}}\,\| \varepsilon\|_{\infty}\big{)}}{ac_{0}}\.\]

To complete the proof we only need to combine (13) with the bounds of Theorem 1. Note that we apply (13) with

\[\xi_{\ell}=\frac{\varepsilon_{\ell}}{\sqrt{n}}\.\]

Thus, for \(\Upsilon_{n}\) defined in (13) we have

\[\Upsilon_{n}\leq\frac{\|\varepsilon\|_{\infty}^{3}}{n^{1/2}}\.\]Applying the Cauchy-Schwartz inequality, we get

\[\mathbb{E}[\|D\|\|W\|]\leq\mathbb{E}^{1/2}[\|D\|^{2}]\mathbb{E}^{1/2}[ \|W\|^{2}]\lesssim\frac{\sqrt{\kappa_{Q}}\|\varepsilon\|_{\infty}\sqrt{\operatorname {Tr}\Sigma_{\varepsilon}}}{\sqrt{ac_{0}}}\left(\frac{1}{n^{(1-\gamma)/2}}+ \frac{c_{0}\operatorname{C}_{\mathbf{A}}}{\sqrt{1-\gamma}n^{\gamma/2}}\right)\] \[\qquad\qquad\qquad\qquad+\sqrt{\kappa_{Q}}\sqrt{\operatorname{Tr} \Sigma_{\varepsilon}}\Delta_{1}\exp\biggl{\{}-\frac{c_{0}an^{1-\gamma}}{2(1- \gamma)}\biggr{\}}\|\theta_{0}-\theta^{\star}\|\.\]

Now it remains to bound the last term in (13). Using the Cauchy-Schwartz inequality and Lemma 4, we obtain that

\[n^{-1/2}\mathbb{E}[\sum_{i=n}^{2n-1}\|\varepsilon_{i}\|\|D-D^{(i) }\|]\leq n^{-1/2}\mathbb{E}^{1/2}[\|\varepsilon_{1}\|^{2}]\sum_{i=n}^{2n-1} \mathbb{E}^{1/2}[\|D-D^{(i)}\|^{2}]\] \[\lesssim\frac{\kappa_{Q}(\operatorname{Tr}\Sigma_{\varepsilon}+ \operatorname{C}_{\mathbf{A}}\sqrt{\operatorname{Tr}\Sigma_{\varepsilon}}\| \varepsilon\|_{\infty})}{ac_{0}n^{1-\gamma}}+\frac{\kappa_{Q}\operatorname{C} _{\mathbf{A}}\sqrt{\operatorname{Tr}\Sigma_{\varepsilon}}(\|\varepsilon\|_{ \infty}+\sqrt{\operatorname{Tr}\Sigma_{\varepsilon}}+\operatorname{C}_{ \mathbf{A}}\|\varepsilon\|_{\infty})}{n^{\gamma/2}}\] \[+\frac{\kappa_{Q}\sqrt{\operatorname{Tr}\Sigma_{\varepsilon}}( \operatorname{C}_{\mathbf{A}}\sqrt{1)^{2}n^{\gamma-1/2}}}{ac_{0}}\exp\biggl{\{} -\frac{c_{0}an^{1-\gamma}}{2(1-\gamma)}\biggr{\}}\|\theta_{0}-\theta^{\star}\|\,\]

and the statement follows from [63, Corollary 2.3].

### Proof of auxiliary lemmas for Theorem 2.

Our proof of Theorem 2 is based on the key lemma below, which allows us to bound \(\mathbb{E}^{1/2}[\|D-D^{(i)}\|^{2}]\) for \(i\in\{n+1,\dots,2n\}\).

**Lemma 4**.: _Assume A1, A2, and A3. Then_

\[\sum_{i=n+1}^{2n}\mathbb{E}^{1/2}[\|D-D^{(i)}\|^{2}]\lesssim \frac{\kappa_{Q}(\sqrt{\operatorname{Tr}\Sigma_{\varepsilon}}+ \operatorname{C}_{\mathbf{A}}\|\varepsilon\|_{\infty})}{ac_{0}}n^{\gamma-1/2}\] \[\qquad+\kappa_{Q}\operatorname{C}_{\mathbf{A}}\left(\|\varepsilon \|_{\infty}+\sqrt{\operatorname{Tr}\Sigma_{\varepsilon}}+\operatorname{C}_{ \mathbf{A}}\|\varepsilon\|_{\infty}\right)n^{\frac{1-\gamma}{2}}\] \[\qquad+\frac{\kappa_{Q}(\operatorname{C}_{\mathbf{A}}\sqrt{1})^{ 2}n^{\gamma-1/2}}{ac_{0}}\exp\biggl{\{}-\frac{c_{0}an^{1-\gamma}}{2(1-\gamma)} \biggr{\}}\|\theta_{0}-\theta^{\star}\|\.\]

Proof.: Since both terms in the right-hand side of the error bound of Proposition 4 scales linearly with \(\sqrt{\kappa_{Q}}\), for simplicity we do not trace it in the subsequent bounds (i.e. assume \(\kappa_{Q}=1\)), and then keep the required scaling with \(\kappa_{Q}\) only in the final bounds. Consider the sequences of noise variables

\[(Z_{1},\dots,Z_{i-1},Z_{i},Z_{i+1},\dots,Z_{2n})\text{ and }(Z_{1},\dots,Z_{i-1},Z_{i}^{\prime},Z_{i+1},\dots,Z_{2n})\,\]

which differ only in position \(i\), \(n+1\leq i\leq 2n\), with \(Z_{i}^{\prime}\) being an independent copy of \(Z_{i}\). Consider the associated SA processes

\[\theta_{k} =\theta_{k-1}-\alpha_{k}\{\mathbf{A}(Z_{k})\theta_{k-1}-\mathbf{b }(Z_{k})\}\,\quad k\geq 1,\quad\theta_{0}=\theta_{0}\in\mathbb{R}^{d}\] (37) \[\theta_{k}^{(i)} =\theta_{k-1}^{(i)}-\alpha_{k}\{\mathbf{A}(Y_{k})\theta_{k-1}^{(i )}-\mathbf{b}(Y_{k})\}\,\quad k\geq 1\,\quad\theta_{0}^{(i)}=\theta_{0}\in\mathbb{R}^{d}\,\]

where \(Y_{k}=Z_{k}\) for \(k\neq i\) and \(Y_{i}=Z_{i}^{\prime}\). From the above representations we easily observe that \(\theta_{k}=\theta_{k}^{(i)}\) for \(k<i\), moreover,

\[\theta_{i}-\theta_{i}^{(i)} =\alpha_{i}\bigl{\{}(\mathbf{A}(Z_{i}^{\prime})-\mathbf{A}(Z_{i}) )\theta_{i-1}-\mathbf{b}(Z_{i}^{\prime})+\mathbf{b}(Z_{i})\bigr{\}}\] (38) \[=\alpha_{i}(\mathbf{A}(Z_{i}^{\prime})-\mathbf{A}(Z_{i}))(\theta_{ i-1}-\theta^{\star})-\alpha_{i}(\varepsilon_{i}-\varepsilon_{i}^{\prime})\,\]

where \(\varepsilon_{i}=\varepsilon(Z_{i})\) and \(\varepsilon_{i}^{\prime}=\varepsilon(Z_{i}^{\prime})\). Representation (38) implies, together with Proposition 4 and \(c_{0}\leq a\), that

\[\mathbb{E}^{1/2}[\|\theta_{i}-\theta_{i}^{(i)}\|^{2}] \lesssim\alpha_{i}\sqrt{\operatorname{Tr}\Sigma_{\varepsilon}}+ \operatorname{C}_{\mathbf{A}}\|\varepsilon\|_{\infty}\alpha_{i}^{3/2}+\alpha_{i} \operatorname{C}_{\mathbf{A}}\exp\biggl{\{}-\frac{a}{2}\sum_{j=1}^{i-1}\alpha_ {j}\biggr{\}}\|\theta_{0}-\theta^{\star}\|\] (39) \[\lesssim\alpha_{i}\bigl{(}\sqrt{\operatorname{Tr}\Sigma_{ \varepsilon}}+\operatorname{C}_{\mathbf{A}}\|\varepsilon\|_{\infty}\bigr{)}+ \alpha_{i}\operatorname{C}_{\mathbf{A}}\exp\biggl{\{}-\frac{a}{2}\sum_{j=1}^{i-1} \alpha_{j}\biggr{\}}\|\theta_{0}-\theta^{\star}\|\.\]Moreover, for any \(j>i\) one observes, expanding (37), that

\[\theta_{j}-\theta_{j}^{(i)}=\bigg{\{}\prod_{k=i+1}^{j}(\mathrm{I}-\alpha_{k} \mathbf{A}(Z_{k}))\bigg{\}}(\theta_{i}-\theta_{i}^{(i)})\.\] (40)

We use the above representations to estimate \(\mathbb{E}^{1/2}[\|D-D^{(i)}\|^{2}]\). Using Minkowski's inequality,

\[\mathbb{E}^{1/2}[\|D-D^{(i)}\|^{2}]\leq\sum_{j=1}^{4}\mathbb{E}^{1/2}[\|D_{j}- D_{j}^{(i)}\|^{2}]\,\] (41)

and bound the respective differences separately. Recall that here \(D_{1}-D_{4}\) are defined in (25), and \(D_{1}^{(i)}-D_{4}^{(i)}\) are their respective counterparts with \(Z_{i}\) substituted with \(Z_{i}^{\prime}\). First we note that the term \(D_{1}=D_{1}^{(i)}\) for any \(n+1\leq i\leq 2n\). Next, using (40) and (39), we get

\[\mathbb{E}^{1/2}[\|D_{2}-D_{2}^{(i)}\|^{2}] =\frac{1}{\sqrt{n}\alpha_{2n}}\mathbb{E}^{1/2}[\|\theta_{2n}- \theta_{2n}^{(i)}\|^{2}]\] \[\leq\frac{1}{\sqrt{n}\alpha_{2n}}\mathbb{E}^{1/2}\big{[}\|\prod_ {k=i+1}^{2n}(\mathrm{I}-\alpha_{k}\mathbf{A}_{k})\|^{2}\big{]}\mathbb{E}^{1/2} [\|\theta_{i}-\theta_{i}^{(i)}\|^{2}]\] \[\stackrel{{(a)}}{{\lesssim}}\frac{\alpha_{i}(\sqrt {\mathrm{Tr}\,\Sigma_{e}}+\mathrm{C}_{\mathbf{A}}\,\|\varepsilon\|_{\infty})}{ \sqrt{n}\alpha_{2n}}\exp\bigl{\{}-\frac{a}{2}\sum_{k=i+1}^{2n}\alpha_{k} \bigr{\}}\] \[\qquad\qquad+\frac{\alpha_{i}\,\mathrm{C}_{\mathbf{A}}}{\sqrt{n} \alpha_{2n}}\exp\bigl{\{}-\frac{a}{2}\sum_{k=1}^{2n}\alpha_{k}\bigr{\}}\| \theta_{0}-\theta^{\star}\|\.\]

In the inequality (a) above we additionally used the stability of matrix product introduced from Corollary 4. Summing the above inequality for \(i=n+1\) to \(2n\) and applying Lemma 2, we get

\[\sum_{i=n+1}^{2n}\mathbb{E}^{1/2}[\|D_{2}-D_{2}^{(i)}\|^{2}]\lesssim \frac{\sqrt{\mathrm{Tr}\,\Sigma_{e}}+\mathrm{C}_{\mathbf{A}}\,\|\varepsilon\|_ {\infty}}{a\sqrt{n}\alpha_{2n}}+\frac{\mathrm{C}_{\mathbf{A}}}{a\sqrt{n}\alpha _{2n}}\exp\bigl{\{}-\frac{a}{2}\sum_{k=1}^{n}\alpha_{k}\bigr{\}}\|\theta_{0}- \theta^{\star}\|\] \[\qquad\qquad\lesssim\frac{\sqrt{\mathrm{Tr}\,\Sigma_{e}}+\mathrm{ C}_{\mathbf{A}}\,\|\varepsilon\|_{\infty}}{ac_{0}}n^{\gamma-1/2}+\frac{\mathrm{C}_{ \mathbf{A}}\,n^{\gamma-1/2}}{ac_{0}}\exp\biggl{\{}-\frac{c_{0}an^{1-\gamma}}{2( 1-\gamma)}\biggr{\}}\|\theta_{0}-\theta^{\star}\|\.\] (42)

Now we proceed with the difference \(D_{3}-D_{3}^{(i)}\). Using (25), we get

\[D_{3}-D_{3}^{(i)}=\frac{1}{\sqrt{n}}(\mathbf{A}_{i}-\mathbf{A}_{i}^{\prime})( \theta_{i-1}-\theta^{\star})+\frac{1}{\sqrt{n}}\sum_{k=i+1}^{2n}(\mathbf{A}_{k }-\bar{\mathbf{A}})(\theta_{k-1}-\theta_{k-1}^{(i)})\.\]

The expression above is a sum of martingale-difference terms w.r.t. filtration \(\mathcal{F}_{k}^{\prime}=\sigma(Z_{i}^{\prime},Z_{\ell},\ell\leq k)\). Hence, we get, using (40) and Proposition 4, that

\[\mathbb{E}[\|D_{3}-D_{3}^{(i)}\|^{2}] \lesssim\frac{\mathrm{C}_{\mathbf{A}}^{2}}{n}\mathbb{E}[\|\theta _{i-1}-\theta^{\star}\|^{2}]+\frac{\mathrm{C}_{\mathbf{A}}^{2}}{n}\sum_{k=i+1}^ {2n}\mathbb{E}[\|\theta_{k-1}-\theta_{k-1}^{(i)}\|^{2}]\] (43) \[\lesssim\frac{\mathrm{C}_{\mathbf{A}}^{2}\,\|\varepsilon\|_{\infty }^{2}\alpha_{i}}{na}+\frac{\mathrm{C}_{\mathbf{A}}^{2}\,\|\theta_{0}-\theta^{ \star}\|^{2}}{n}\exp\bigl{\{}-a\sum_{j=1}^{i-1}\alpha_{j}\bigr{\}}\] \[\qquad\qquad+\frac{\mathrm{C}_{\mathbf{A}}^{2}}{n}\mathbb{E}[\| \theta_{i}-\theta_{i}^{(i)}\|^{2}]\sum_{k=i+1}^{2n}\exp\bigl{\{}-a\sum_{j=i+1}^ {k-1}\alpha_{j}\bigr{\}}\.\]Using now the bound (39), we obtain that

\[\mathbb{E}[\|\theta_{i}-\theta_{i}^{(i)}\|^{2}]\sum_{k=i+1}^{2n}\exp \bigl{\{}-a\sum_{j=i+1}^{k-1}\alpha_{j}\bigr{\}}\] \[\lesssim\alpha_{i}^{2}\left(\operatorname{Tr}\Sigma_{\varepsilon} +\operatorname{C}_{\mathbf{A}}^{2}\|\varepsilon\|_{\infty}^{2}\right)\sum_{k=i+ 1}^{2n}\exp\bigl{\{}-a\sum_{j=i+1}^{k-1}\alpha_{j}\bigr{\}}+\alpha_{i}^{2} \operatorname{C}_{\mathbf{A}}^{2}\|\theta_{0}-\theta^{\star}\|^{2}\sum_{k=i+1} ^{2n}\exp\bigl{\{}-a\sum_{j=1}^{k-1}\alpha_{i}\bigr{\}}\] \[\lesssim\frac{\alpha_{i}^{2}}{\alpha_{2n}}\left(\operatorname{Tr }\Sigma_{\varepsilon}+\operatorname{C}_{\mathbf{A}}^{2}\|\varepsilon\|_{\infty} ^{2}\right)\sum_{k=i+1}^{2n}\alpha_{k}\exp\bigl{\{}-a\sum_{j=i+1}^{k-1}\alpha_ {j}\bigr{\}}+\frac{\alpha_{i}^{2}\operatorname{C}_{\mathbf{A}}^{2}}{\alpha_{2 n}}\|\theta_{0}-\theta^{\star}\|^{2}\sum_{k=i+1}^{2n}\alpha_{k}\exp\bigl{\{}-a \sum_{j=1}^{k-1}\alpha_{i}\bigr{\}}\] \[\overset{(a)}{\lesssim}\frac{\alpha_{i}^{2}(\operatorname{Tr} \Sigma_{\varepsilon}+\operatorname{C}_{\mathbf{A}}^{2}\|\varepsilon\|_{\infty} ^{2})}{a\alpha_{2n}}+\frac{\alpha_{i}^{2}\operatorname{C}_{\mathbf{A}}^{2}}{a \alpha_{2n}}\|\theta_{0}-\theta^{\star}\|^{2}\exp\bigl{\{}-a\sum_{j=1}^{i-1} \alpha_{j}\bigr{\}}\;.\]

In the above formula in (a) we additionally used that, since \(\alpha_{i}a\leq 1/2\),

\[\sum_{k=i+1}^{2n}\alpha_{k}\exp\bigl{\{}-a\sum_{j=i+1}^{k-1}\alpha_{j}\bigr{\}} \lesssim\int_{0}^{+\infty}\exp\{-ax\}\,dx=\frac{1}{a}\;.\] (44)

Hence, combining everything in (43), and using additionally that \(\alpha_{i}\leq a\), we get

\[\mathbb{E}^{1/2}[\|D_{3}-D_{3}^{(i)}\|^{2}]\lesssim\frac{ \operatorname{C}_{\mathbf{A}}}{\sqrt{na}}\left(\|\varepsilon\|_{\infty}\sqrt{ \alpha_{i}}+\frac{\alpha_{i}(\sqrt{\operatorname{Tr}\Sigma_{\varepsilon}}+ \operatorname{C}_{\mathbf{A}}\|\varepsilon\|_{\infty})}{\sqrt{\alpha_{2n}}} \right)+\\ \frac{\operatorname{C}_{\mathbf{A}}}{\sqrt{n}}\left(1+\frac{ \alpha_{i}\operatorname{C}_{\mathbf{A}}}{\sqrt{a\alpha_{2n}}}\right)\exp\bigl{\{} -\frac{a}{2}\sum_{j=1}^{i-1}\alpha_{i}\bigr{\}}\|\theta_{0}-\theta^{\star}\|\;.\]

Summing the above inequality for \(i=n+1\) to \(2n\), and using that \(\alpha_{k}=c_{0}/k^{\gamma}\), we get

\[\sum_{i=n+1}^{2n}\sqrt{\alpha_{i}}\lesssim\sqrt{c_{0}}n^{1-\gamma/2}\;,\quad \sum_{i=n+1}^{2n}\frac{\alpha_{i}}{\sqrt{\alpha_{2n}}}\lesssim\sqrt{c_{0}}n^{ 1-\gamma/2}\;,\]

and, hence, using again \(\alpha_{i}\leq a\), we get

\[\sum_{i=n+1}^{2n}\mathbb{E}^{1/2}[\|D_{3}-D_{3}^{(i)}\|^{2}]\] \[\lesssim\frac{\operatorname{C}_{\mathbf{A}}\sqrt{c_{0}}}{\sqrt{a} }\left(\|\varepsilon\|_{\infty}+\sqrt{\operatorname{Tr}\Sigma_{\varepsilon}}+ \operatorname{C}_{\mathbf{A}}\|\varepsilon\|_{\infty}\right)n^{\frac{1-\gamma }{2}}+\frac{(\operatorname{C}_{\mathbf{A}}\vee 1)^{2}\|\theta_{0}-\theta^{\star}\|}{\sqrt{n} \alpha_{2n}}\sum_{i=n+1}^{2n}\alpha_{i}\exp\bigl{\{}-\frac{a}{2}\sum_{j=1}^{i- 1}\alpha_{j}\bigr{\}}\] \[\lesssim\frac{\operatorname{C}_{\mathbf{A}}\sqrt{c_{0}}}{\sqrt{a} }\left(\|\varepsilon\|_{\infty}+\sqrt{\operatorname{Tr}\Sigma_{\varepsilon}}+ \operatorname{C}_{\mathbf{A}}\|\varepsilon\|_{\infty}\right)n^{\frac{1-\gamma}{ 2}}+\frac{n^{\gamma-1/2}(\operatorname{C}_{\mathbf{A}}\vee 1)^{2}\|\theta_{0}-\theta^{\star}\|}{ac_{0}} \exp\biggl{\{}-\frac{c_{0}an^{1-\gamma}}{2(1-\gamma)}\biggr{\}}\;,\]

where for the last identity we used the fact that \(\alpha_{k}=c_{0}/k^{\gamma}\), and (44). It remains to upper bound the difference \(D_{4}-D_{4}^{(i)}\). Note first that, proceeding as in (31), we get

\[\alpha_{k-1}-\alpha_{k}\leq\frac{\gamma}{k-1}\frac{1}{\alpha_{k-1}}\lesssim \frac{1}{(k-1)^{1-\gamma}}\;.\]

Using now the definition of \(D_{4}\) in (25), we have that

\[\mathbb{E}^{1/2}[\|D_{4}-D_{4}^{(i)}\|^{2}] =\frac{1}{\sqrt{n}}\mathbb{E}^{1/2}[\|\sum_{k=i+1}^{2n}\bigl{(} \theta_{k-1}-\theta_{k-1}^{(i)}\bigr{)}\left(\frac{1}{\alpha_{k}}-\frac{1}{ \alpha_{k-1}}\right)\|^{2}]\] \[\leq\frac{1}{\sqrt{n}}\mathbb{E}^{1/2}[\|\theta_{i}-\theta_{i}^{( i)}\|^{2}]\sum_{k=i+1}^{2n}\left(\frac{1}{\alpha_{k}}-\frac{1}{\alpha_{k-1}} \right)\exp\bigl{\{}-\frac{a}{2}\sum_{j=i+1}^{k-1}\alpha_{j}\bigr{\}}\;.\]

[MISSING_PAGE_FAIL:24]

where the constant \(C\) in the above inequality depends on the covariance matrix of vector \(Y\). This inequality justifies comparison of our bounds of Theorem 2 with the result of [65]. The authors in [2] considered integral probability metric \(\mathsf{d}_{[2]}\) and obtained rate of convergence

\[\mathsf{d}_{[2]}(\sqrt{n}(\bar{\theta}_{n}-\theta^{\star}),Y)\leq\frac{C_{1}}{ \sqrt{n}}\,\]

where \(Y\sim\mathcal{N}(0,\Sigma_{\infty})\), and \(C_{1}\) in the above inequality stands for a constant depending upon problem dimension \(d\) and other instance-dependent parameters from A2. Applying the result of [24, Proposition 2.6] yields

\[\mathsf{d}_{K}(\sqrt{n}(\bar{\theta}_{n}-\theta^{\star}),Y)\lesssim\big{(} \mathsf{d}_{[2]}(\sqrt{n}(\bar{\theta}_{n}-\theta^{\star}),Y)\big{)}^{1/3} \lesssim\frac{1}{n^{1/6}}\.\]

Thus, the result of [2] implies rate of convergence of \(\sqrt{n}(\bar{\theta}_{n}-\theta^{\star})\) to normal law \(\mathcal{N}(0,\Sigma_{\infty})\) of order \(n^{-1/6}\) in a sense of Kolmogorov distance \(\mathsf{d}_{K}\). Our result of Theorem 2 implies the respective rate of order \(n^{-1/4}\). At the same time, it is not clear if \(\rho_{n}^{\mathrm{Conv}}\) can be directly related to \(\mathsf{d}_{[2]}\).

## Appendix C Bootstrap validity proof

### Proof of Theorem 3

We first define explicitly the remainder term \(\Delta_{3}\) outlined in the statement of Theorem 3, that is,

\[\Delta_{3}(n,a,\mathrm{C}_{\mathbf{A}},\|\varepsilon\|_{\infty})=\frac{\kappa _{Q}^{3/2}(\mathrm{C}_{\mathbf{A}}^{3}\lor 1)\|\varepsilon\|_{\infty}n^{1/4} \sqrt{\log n}}{a^{3/2}}\.\] (46)

In the above bounds we do not trace the precise dependence on the constant \(c_{0}\) from the definition of the step size. We now define the following sets, with the convention \(\alpha_{\ell}=c_{0}/\sqrt{\ell}\):

\[\Omega_{1} =\left\{\forall k\in[n,2n-1]:\|\theta_{k}-\theta^{\star}\|\geq \sqrt{\kappa_{Q}}\mathrm{e}^{2}\exp\!\left\{-\frac{a}{2}\sum_{\ell=1}^{k} \alpha_{\ell}\right\}\|\theta_{0}-\theta^{\star}\|\right.\] (47) \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad+\left. \frac{8\mathrm{e}^{2}\sqrt{\kappa_{Q}}\|\varepsilon\|_{\infty}\log n}{\sqrt{a }}\sqrt{\alpha_{k}}\right\}\,\] \[\Omega_{2} =\left\{n+1\leq m\leq k\leq 2n:\ \|\Gamma_{mk}\|\leq\sqrt{ \kappa_{Q}}\mathrm{e}^{2}\prod_{j=m}^{k}\bigl{(}1-\frac{a\alpha_{j}}{4}\bigr{)} \right\}\,\] \[\Omega_{3} =\left\{\|\Sigma_{\varepsilon}^{-1/2}\Sigma_{\varepsilon}^{ \mathsf{b}}\Sigma_{\varepsilon}^{-1/2}-\mathrm{I}\|\leq 4\|\varepsilon\|_{\infty} \sqrt{\frac{\log(n)}{\sigma n}}+\frac{4(1+\|\varepsilon\|_{\infty}^{2}/ \sigma^{2})\log(n)}{n}\right\}\,\] \[\Omega_{4} =\left\{\forall\ell\in[n,2n-1]:\ \left\|\sum_{k=\ell+1}^{2n}( \mathbf{A}_{k}-\bar{\mathbf{A}})\Gamma_{\ell+1:k-1}\right\|\leq\frac{8\, \mathrm{C}_{\mathbf{A}}\,\sqrt{\kappa_{Q}}\mathrm{e}^{2}\sqrt{\log n}}{\sqrt{ a\alpha_{\ell}}}+6\,\mathrm{C}_{\mathbf{A}}\,\sqrt{\kappa_{Q}}\mathrm{e}\log n \right\}\,\] \[\Omega_{5} =\left\{\forall h\in[1;n]\,\,\forall m\in[n,2n-h]:\ \|\sum_{\ell=m+1}^{m+h} \alpha_{\ell}(\mathbf{A}_{\ell}-\bar{\mathbf{A}})\|_{Q}\leq 2\,\mathrm{C}_{ \mathbf{A}}\,\sqrt{\kappa_{Q}}\sqrt{\sum_{\ell=m+1}^{m+h}\alpha_{\ell}^{2}}\log (2n^{4})\right\}\,\] \[\Omega_{6} =\left\{\|\Sigma_{\varepsilon}^{\mathsf{b}}-\Sigma_{\varepsilon} \|\leq 4\|\varepsilon\|_{\infty}\sqrt{\frac{\|\Sigma_{\varepsilon}\|\log(n) }{n}}+\frac{4(\|\Sigma_{\varepsilon}\|+\|\varepsilon\|_{\infty}^{2})\log(n)}{n} \right\}\,\]

Then, due to Corollary 2, we have that \(\mathbb{P}(\Omega_{1})\geq 1-\frac{1}{n}\). Similarly, due to Corollary 5, \(\mathbb{P}(\Omega_{2})\geq 1-\frac{1}{n}\). The bounds on \(\mathbb{P}(\Omega_{3})\) and \(\mathbb{P}(\Omega_{4})\) follows from Lemma 5 and Lemma 6, respectively. Similarly, Proposition 7 implies that \(\mathbb{P}(\Omega_{5})\geq 1-\frac{1}{n}\). Hence, based on the sets above, we can construct

\[\Omega_{0}=\Omega_{1}\cap\Omega_{2}\cap\Omega_{3}\cap\Omega_{4}\cap\Omega_{5} \cap\Omega_{6}\,\]such that \(\mathbb{P}(\Omega_{0})\geq 1-\frac{6}{n}\). All further on, we restrict ourselves to the event \(\Omega_{0}\). Restricting to this event, we obtain that, with Minkowski's inequality,

\[\sup_{B\in\operatorname{Conv}(\mathbb{R}^{d})} |\mathbb{P}^{\mathsf{b}}(\sqrt{n}(\bar{\theta}_{n}^{\mathsf{b}}- \bar{\theta}_{n})\in B)-\mathbb{P}(\sqrt{n}(\bar{\theta}_{n}-\theta^{*})\in B)|\] \[\leq\sup_{B\in\operatorname{Conv}(\mathbb{R}^{d})}\left|\mathbb{ P}^{\mathsf{b}}(\sqrt{n}(\bar{\theta}_{n}^{\mathsf{b}}-\bar{\theta}_{n})\in B )-\mathbb{P}^{\mathsf{b}}(\xi^{\mathsf{b}}\in B)\right|\] \[+\sup_{B\in\operatorname{Conv}(\mathbb{R}^{d})}\left|\mathbb{P} (\xi\in B)-\mathbb{P}^{\mathsf{b}}(\xi^{\mathsf{b}}\in B)\right|\] \[+\sup_{B\in\operatorname{Conv}(\mathbb{R}^{d})}\left|\mathbb{P} \big{(}\sqrt{n}(\bar{\theta}_{n}-\theta^{*})\in B\big{)}-\mathbb{P}(\xi\in B )\right|\,,\]

where we set \(\xi^{\mathsf{b}}\sim\mathcal{N}(0,\bar{\mathbf{A}}^{-1}\Sigma_{\varepsilon}^ {\mathsf{b}}\bar{\mathbf{A}}^{-\top})\), \(\Sigma_{\varepsilon}^{\mathsf{b}}=n^{-1}\sum_{\ell=1}^{n}\varepsilon_{\ell} \varepsilon_{\ell}^{\top}\), and \(\xi\sim\mathcal{N}(0,\Sigma_{\infty})\), where \(\Sigma_{\infty}=\bar{\mathbf{A}}^{-1}\Sigma_{\varepsilon}\bar{\mathbf{A}}^{- \top}\). Now we control the first supremum using Theorem 4, second one using Theorem 5, and third with Theorem 2.

**Lemma 5**.: _Assume A1, A2, A3 with \(\gamma=1/2\), and A4. Then_

\[\mathbb{P}(\Omega_{3})\geq 1-1/n.\]

Proof.: The proof follows directly from the matrix Bernstein inequality, e.g. [69]. We note that

\[\|\Sigma_{\varepsilon}^{-1/2}\varepsilon_{\ell}\varepsilon_{\ell}^{\top} \Sigma_{\varepsilon}^{-1/2}-\mathrm{I}\|\leq 1+\|\Sigma_{\varepsilon}^{-1/2} \varepsilon\|_{\infty}^{2}\.\]

and

\[\|\sum_{k=n+1}^{2n}\mathbb{E}[(\Sigma_{\varepsilon}^{-1/2}\varepsilon_{\ell} \varepsilon_{\ell}^{\top}\Sigma_{\varepsilon}^{-1/2}-\mathrm{I})^{2}]\|\leq n \mathbb{E}\|\Sigma_{\varepsilon}^{-1/2}\varepsilon\|_{\infty}^{2}\.\]

**Lemma 6**.: _Assume A1, A2, A3 with \(\gamma=1/2\), and A4. Then_

\[\mathbb{P}\left(\Omega_{4}\cap\Omega_{2}\right)\geq 1-\frac{1}{n}.\]

Proof.: Denote

\[X_{k}=(\mathbf{A}_{k}-\bar{\mathbf{A}})\Gamma_{\ell+1:k-1}.\]

and let \(\mathcal{F}_{k,l+1}=\sigma\{Z_{j},\ell+1\leq j\leq k\}\), \(\ell+1\leq k\leq 2n\). Then \(\mathbb{E}[X_{k}|\mathcal{F}_{k-1,\ell+1}]=0\). Let \(S_{\ell}=\sum_{k=\ell+1}^{n}X_{k}\). Note that on \(\Omega_{2}\), quadratic variation of \(S_{\ell}\) can be controlled as

\[\operatorname{Var}^{2} :=\max(\|\sum_{k=\ell+1}^{2n}\mathbb{E}[X_{k}X_{k}^{\top}| \mathcal{F}_{k-1,l+1}]\|,\|\sum_{k=\ell+1}^{2n}\mathbb{E}[X_{k}^{\top}X_{k}| \mathcal{F}_{k-1,l+1}]\|)\] \[\leq\kappa_{Q}\mathrm{e}^{4}\,\mathrm{C}_{\mathbf{A}}^{2}\sum_{k= \ell+1}^{2n}\prod_{j=\ell+1}^{k-1}(1-a\alpha_{j}/4)^{2}\leq\frac{4\kappa_{Q} \mathrm{e}^{4}\,\mathrm{C}_{\mathbf{A}}^{2}}{a\alpha_{\ell}}\]

Furthermore, on \(\Omega_{2}\)

\[\|X_{k}\|\leq\sqrt{\kappa_{Q}}\mathrm{e}^{2}\,\mathrm{C}_{\mathbf{A}}\prod_{j =\ell+1}^{k-1}(1-a\alpha_{j}/4)\leq\sqrt{\kappa_{Q}}\mathrm{e}^{2}\,\mathrm{C} _{\mathbf{A}}\.\]

It remains to apply the Freedman inequality for matrix-values martingales [68] and use the union bound over \(\ell\in[n,2n-1]\). 

**Lemma 7**.: _Assume A1, A2, A3 with \(\gamma=1/2\), and A4. Then_

\[\mathbb{P}(\Omega_{5})\geq 1-1/n.\]Proof.: We first fix \(h\in[1;n]\), \(m\in[n,2n-h]\), and consider the random variable

\[T_{n}=\|\sum_{\ell=m+1}^{m+h}\alpha_{\ell}(\mathbf{A}_{\ell}-\bar{\mathbf{A}})\|\.\]

Then we control its variance as

\[\max(\|\sum_{\ell=m+1}^{m+h}\alpha_{\ell}^{2}\mathbb{E}[(\mathbf{A}_{\ell}- \bar{\mathbf{A}})(\mathbf{A}_{\ell}-\bar{\mathbf{A}})^{\top}]\|,\|\sum_{\ell=m+ 1}^{m+h}\alpha_{\ell}^{2}\mathbb{E}[(\mathbf{A}_{\ell}-\bar{\mathbf{A}})^{\top }(\mathbf{A}_{\ell}-\bar{\mathbf{A}})]\|)\leq\mathrm{C}_{\mathbf{A}}^{2}\sum_{ \ell=m+1}^{m+h}\alpha_{\ell}^{2}\,\]

moreover, \(\|(\mathbf{A}_{\ell}-\bar{\mathbf{A}})(\mathbf{A}_{\ell}-\bar{\mathbf{A}})^{ \top}\|\leq\mathrm{C}_{\mathbf{A}}^{2}\). Applying now the matrix Bernstein inequality [69], we obtain that with probability at least \(1-1/n^{3}\), we have

\[T_{n}\leq\mathrm{C}_{\mathbf{A}}\sqrt{2\sum_{\ell=m+1}^{m+h}\alpha_{\ell}^{2} \sqrt{\log{(2n^{3}d)}}+\frac{\alpha_{m+1}\,\mathrm{C}_{\mathbf{A}}}{3}\log{(2n ^{3}d)}}\leq 2\,\mathrm{C}_{\mathbf{A}}\sqrt{\sum_{\ell=m+1}^{m+h}\alpha_{\ell}^{2} \log{(2n^{4})}}\.\]

In the last line here we used that \(d\leq n\). Rest of the proof follows by taking union bound over \(h\) and \(m\) together with \(\|B\|_{Q}^{2}\leq\kappa_{Q}\|B\|^{2}\) valid for any matrix \(B\in\mathbb{R}^{d\times d}\). 

**Lemma 8**.: _Assume A1, A2, A3 with \(\gamma=1/2\), and A4. Then_

\[\mathbb{P}(\Omega_{6})\geq 1-1/n.\]

Proof.: It is easy to check that \(\|\varepsilon_{\ell}\varepsilon_{\ell}^{\top}-\Sigma_{\varepsilon}\|\leq\| \Sigma_{\varepsilon}\|+\|\varepsilon\|_{\infty}^{2}\). Moreover,

\[\mathrm{Var}^{2}:=\|\sum_{\ell=1}^{n}(\varepsilon_{\ell}\varepsilon_{\ell}^{ \top}-\Sigma_{\varepsilon})^{2}\|\leq n\|\varepsilon\|_{\infty}^{2}\|\Sigma_{ \varepsilon}\|.\]

It remains to apply the matrix Bernstein inequality together with the bound \(n\geq d\) from A3. 

### Rate of Gaussian approximation in the bootstrap world

The main result of this section is the following theorem.

**Theorem 4**.: _Assume A1, A2, A3 with \(\gamma=1/2\), and A4. Then, conditionally on the event \(\Omega_{0}\), the following error bound holds:_

\[\sup_{B\in\mathrm{Conv}(\mathbb{R}^{4})}\bigg{|}\mathbb{P}^{\text {\rm b}}(\sqrt{n}(\bar{\theta}_{n}^{\text{\rm b}}-\bar{\theta}_{n})\in B)- \mathbb{P}^{\text{\rm b}}(\xi^{\text{\rm b}}\in B)\bigg{|}\lesssim\frac{d^{1/2 }\|\varepsilon\|_{\infty}^{3}}{\lambda_{\min}^{3/2}\sqrt{n}}+\frac{\kappa_{Q}^ {3/2}(\mathrm{C}_{\mathbf{A}}^{3}\,\lor 1)\|\varepsilon\|_{\infty}^{2}\log{n}}{a^{3/2} \lambda_{\min}n^{1/4}}\] \[\qquad\qquad\qquad+\frac{\kappa_{Q}^{3/2}(\mathrm{C}_{\mathbf{A} }^{3}\,\lor 1)\|\varepsilon\|_{\infty}n^{1/4}\sqrt{\log{n}}}{a^{3/2}\lambda_{\min }}\exp\bigl{\{}-\frac{a}{4}\sum_{j=1}^{n}\alpha_{j}\bigr{\}}\|\theta_{0}- \theta^{*}\|\,\]

_where \(\xi^{\text{\rm b}}\sim\mathcal{N}(0,\bar{\mathbf{A}}^{-1}\Sigma_{\varepsilon}^ {\text{\rm b}}\bar{\mathbf{A}}^{-\top})\) and \(\Sigma_{\varepsilon}^{\text{\rm b}}=n^{-1}\sum_{\ell=1}^{n}\varepsilon_{\ell} \varepsilon_{\ell}^{\top}\)._

Proof.: Since both terms in the right-hand side of the error bound of Proposition 4 scales linearly with \(\sqrt{\kappa_{Q}}\), for simplicity we do not trace it in the subsequent bounds (i.e. assume \(\kappa_{Q}=1\)), and then keep the required scaling with \(\kappa_{Q}\) only in the final bounds. Recall first that the quantities \(\theta_{k}^{\text{\rm b}}\) and \(\theta_{k}\) are defined in (16). We start from the following decomposition:

\[\theta_{k}^{\text{\rm b}}-\theta_{k}=(\mathrm{I}-\alpha_{k}\bar{\mathbf{A}})( \theta_{k-1}^{\text{\rm b}}-\theta_{k-1})-\alpha_{k}(w_{k}-1)\varepsilon_{k}- \alpha_{k}(\mathbf{A}_{k}-\bar{\mathbf{A}})(\theta_{k-1}^{\text{\rm b}}-\theta_ {k-1})-\alpha_{k}(w_{k}-1)\mathbf{A}_{k}(\theta_{k-1}^{\text{\rm b}}-\theta^{ *})\.\]Taking average for \(k\) from \(n+1\) to \(2n\), we get after multiplying by \(\sqrt{n}\) that

\[\sqrt{n}\bar{\mathbf{A}}(\bar{\theta}_{n}^{\text{b}}-\bar{\theta}_{n })=-\underbrace{\frac{1}{\sqrt{n}}\sum_{k=n+1}^{2n}(w_{k}-1)\varepsilon_{k}}_{W ^{\text{b}}}+\underbrace{\frac{1}{\sqrt{n}}\frac{\theta_{n}^{\text{b}}-\theta_ {n}}{\alpha_{n}}}_{D_{1}^{\text{b}}}-\underbrace{\frac{1}{\sqrt{n}}\frac{\theta _{2n}^{\text{b}}-\theta_{2n}}{\alpha_{2n}}}_{D_{2}^{\text{b}}}\\ -\underbrace{\frac{1}{\sqrt{n}}\sum_{k=n+1}^{2n}(w_{k}-1)\mathbf{ A}_{k}(\theta_{k-1}^{\text{b}}-\theta^{\star})}_{D_{3}^{\text{b}}}+\underbrace{ \frac{1}{\sqrt{n}}\sum_{k=n+1}^{2n}\left(\theta_{k-1}^{\text{b}}-\theta_{k-1} \right)\left(\frac{1}{\alpha_{k}}-\frac{1}{\alpha_{k-1}}\right)}_{D_{4}^{ \text{b}}}\\ -\underbrace{\frac{1}{\sqrt{n}}\sum_{k=n+1}^{2n}(\mathbf{A}_{k}- \bar{\mathbf{A}})(\theta_{k-1}^{\text{b}}-\theta_{k-1})}_{D_{3}^{\text{b}}}\.\] (48)

The formula (48) resembles the key representation \(T^{\text{b}}:=\sqrt{n}\bar{\mathbf{A}}(\bar{\theta}_{n}^{\text{b}}-\bar{ \theta}_{n})=W^{\text{b}}+D^{\text{b}}\), where

\[D^{\text{b}}=D_{1}^{\text{b}}+\ldots+D_{5}^{\text{b}}\,\] (49)

and \(D_{1}^{\text{b}}-D_{5}^{\text{b}}\) are defined in (48). Now we aim to apply the result of [63]:

\[\sup_{B\in\operatorname{Conv}(\mathbb{R}^{d})}|\mathbb{P}^{\text{ b}}(T^{\text{b}}\in B)-\mathbb{P}^{\text{b}}(\xi^{\text{b}}\in B)|\leq 259d^{1/2}\Upsilon+2 \mathbb{E}^{\text{b}}[\|W^{\text{b}}\|\|D^{\text{b}}\|]\\ +2\sum_{\ell=n+1}^{2n}\mathbb{E}^{\text{b}}[\|\xi_{\ell}\|\|D^{ \text{b}}-D^{(\text{b},\ell)}\|]\,\] (50)

where \(\xi_{\ell}=\frac{1}{\sqrt{n}}(w_{\ell}-1)\varepsilon_{\ell}\). We finish the proof by the application of the formula (50). In order to bound the quantities \(\mathbb{E}^{\text{b}}\|D^{\text{b}}\|^{2}\) and \(\mathbb{E}^{\text{b}}\|D^{(\text{b},i)}\|^{2}\), we apply the respective results of Proposition 5 and Proposition 6, respectively. Namely, applying the Cauchy-Schwartz inequality together with Proposition 5, we get that on the event \(\Omega_{0}\) it holds

\[\mathbb{E}^{\text{b}}[\|D^{\text{b}}\|\|W^{\text{b}}\|]\leq \left\{\mathbb{E}^{\text{b}}[\|D^{\text{b}}\|^{2}]\right\}^{1/2} \{\mathbb{E}^{\text{b}}[\|W^{\text{b}}\|^{2}]\}^{1/2}\lesssim\frac{\kappa_{Q} ^{2}(\mathrm{C}_{\mathbf{A}}^{4}\lor 1)\|\varepsilon\|_{\infty}\sqrt{\operatorname{ Tr}\Sigma_{\varepsilon}^{\text{b}}}\log n}{n^{1/4}a^{5/2}}\\ +\kappa_{Q}^{3/2}\|\varepsilon\|_{\infty}\bigg{(}\frac{(\mathrm{C} _{\mathbf{A}}^{3}\lor 1)n^{1/4}}{\sqrt{a}}+\frac{(\mathrm{C}_{\mathbf{A}}^{5}\lor 1) \sqrt{\log n}}{\sqrt{n}a}\bigg{)}\exp\biggl{\{}-\frac{c_{0}a\sqrt{n}}{2} \biggr{\}}\|\theta_{0}-\theta^{\star}\|\.\]

Similarly, applying Minkowski's inequality and Proposition 6, we obtain that

\[\mathbb{E}^{\text{b}}[\sum_{i=n}^{2n-1}\|\xi_{i}\|\|D^{\text{b}}-D^ {(\text{b},i)}\|]\leq\left\{\mathbb{E}^{\text{b}}[\|\xi_{1}\|^{2}]\right\}^{1/ 2}\sum_{i=n}^{2n-1}\left\{\mathbb{E}^{\text{b}}[\|D^{\text{b}}-D^{(\text{b},i )}\|^{2}]\right\}^{1/2}\\ \lesssim\frac{\kappa_{Q}^{3/2}(\mathrm{C}_{\mathbf{A}}^{3}\lor 1) \|\varepsilon\|_{\infty}^{2}\log n}{a^{3/2}n^{1/4}}+\frac{\kappa_{Q}\| \varepsilon\|_{\infty}^{2}\mathrm{C}_{\mathbf{A}}^{2}}{a^{2}\sqrt{n}}+\frac{ \kappa_{Q}^{3/2}(\mathrm{C}_{\mathbf{A}}^{3}\lor 1)\|\varepsilon\|_{\infty}n^{1/4} \sqrt{\log n}}{a^{3/2}}\exp\bigl{\{}-\frac{a}{4}\sum_{j=1}^{n}\alpha_{j}\} \|\theta_{0}-\theta^{\star}\|\.\]

Now it remains to combine the bounds above in (50). 

**Proposition 5**.: _Assume A1, A2, A3 with \(\gamma=1/2\), and A4. Then, conditionally on the event \(\Omega_{0}\), the following error bound holds:_

\[\left\{\mathbb{E}^{\text{b}}\left[\|D^{\text{b}}(w_{1},\ldots,w_{2n },Z_{1},\ldots,Z_{2n})\|^{2}\right]\right\}^{1/2}\lesssim\frac{\kappa_{Q}^{2}( \mathrm{C}_{\mathbf{A}}^{4}\lor 1)\|\varepsilon\|_{\infty}\log n}{n^{1/4}a^{5/2}}\\ +\kappa_{Q}^{3/2}\bigg{(}\frac{(\mathrm{C}_{\mathbf{A}}^{3}\lor 1)n^{1/4} }{\sqrt{a}}+\frac{(\mathrm{C}_{\mathbf{A}}^{5}\lor 1)\sqrt{\log n}}{\sqrt{n}a}\bigg{)}\exp \biggl{\{}-\frac{c_{0}a\sqrt{n}}{2}\biggr{\}}\|\theta_{0}-\theta^{\star}\|\,\]

_where \(\lesssim\) stands for inequality up to an absolute constant._

Proof of Proposition 5 is provided below in Appendix C.5. The lemma below is a direct counterpart of Lemma 4.

**Proposition 6**.: _Assume A1, A2, A3 with \(\gamma=1/2\), and A4. Then, conditionally on the event \(\Omega_{0}\), the following error bound holds:_

\[\sum_{i=n+1}^{2n}\mathbb{E}^{\mathsf{b}}[\|D^{\mathsf{b}}-D^{(\mathsf{ b},i)}\|^{2}]^{1/2}\lesssim\frac{(\mathrm{C}_{\mathbf{A}}^{3}\,\lor 1)\|\varepsilon\|_{ \infty}}{a^{3/2}}n^{1/4}\log n+\frac{\|\varepsilon\|_{\infty}\,\mathrm{C}_{ \mathbf{A}}^{2}}{a^{2}}\\ +\frac{(\mathrm{C}_{\mathbf{A}}^{3}\,\lor 1)n^{3/4}\sqrt{\log n}}{a ^{3/2}}\exp\{-\frac{a}{4}\sum_{j=1}^{n}\alpha_{j}\}\|\theta_{0}-\theta^{\star} \|\,\]

Proof of Proposition 6 is provided below in Appendix C.6.

**Lemma 9**.: _For any \(k\geq n\) on the set \(\Omega_{0}\) the following inequality holds:_

\[\mathbb{E}^{\mathsf{b}}[\|\theta_{k}^{\mathsf{b}}-\theta_{k}\|^{2}]\lesssim \frac{\alpha_{k}\|\varepsilon\|_{\infty}^{2}\,\mathrm{C}_{\mathbf{A}}^{2}}{a ^{3}}+\mathrm{C}_{\mathbf{A}}^{2}\,k\prod_{j=1}^{k}(1-a\alpha_{j}/4)^{2}\| \theta_{0}-\theta^{\star}\|^{2}\.\]

Proof.: A direct application of Lemma 11 with \(L=0\) yields that

\[\mathbb{E}^{\mathsf{b}}[\|\theta_{k}^{\mathsf{b}}-\theta_{k}\|^{2}]\lesssim \frac{\alpha_{k}\|\varepsilon\|_{\infty}^{2}}{a}\left(1+\frac{\mathrm{C}_{ \mathbf{A}}^{2}}{a^{2}}\right)+\mathrm{C}_{\mathbf{A}}^{2}\,k\prod_{j=1}^{k}(1 -a\alpha_{j}/4)^{2}\|\theta_{0}-\theta^{\star}\|^{2}\.\]

Now to complete the proof it remains to notice that \(\mathrm{C}_{\mathbf{A}}\geq a\). 

**Lemma 10**.: _For any matrix-valued sequences \((U_{n})_{n\in\mathbb{N}_{\mathsf{c}}}\)\((V_{n})_{n\in\mathbb{N}}\) and for any \(M\in\mathbb{N}\), it holds that:_

\[\prod_{k=1}^{M}U_{k}-\prod_{k=1}^{M}V_{k}=\sum_{k=1}^{M}\{\prod_{j=k+1}^{M}V_{ j}\}(U_{k}-V_{k})\{\prod_{j=1}^{k-1}U_{j}\}\.\]

### Gaussian comparison inequality

**Theorem 5**.: _Assume A1 and A2. Then on the set \(\Omega_{3}\)_

\[\sup_{B\in\mathrm{Conv}(\mathbb{R}^{d})}|\mathbb{P}(\xi\in B)-\mathbb{P}^{ \mathsf{b}}(\xi^{\mathsf{b}}\in B)|\leq 4\|\Sigma_{\varepsilon}^{-1/2} \varepsilon\|_{\infty}\sqrt{\frac{d\log n}{n}}+\frac{4\sqrt{d}(1+\|\Sigma_{ \varepsilon}^{-1/2}\varepsilon\|_{\infty}^{2})\log n}{n}\]

Proof.: We will use the following inequality

\[\|\mathcal{N}(0,\Sigma_{1})-\mathcal{N}(0,\Sigma_{2})\|_{\mathsf{TV}}\leq\frac {1}{2}\|\Sigma_{1}^{-1/2}\Sigma_{2}\Sigma^{-1/2}-\mathrm{I}\|_{\mathsf{Fr}}\] (51)

Applying (51) we obtain

\[\sup_{B\in\mathrm{Conv}(\mathbb{R}^{d})}|\mathbb{P}(\xi\in B)-\mathbb{P}^{ \mathsf{b}}(\xi^{\mathsf{b}}\in B)|\leq\frac{\sqrt{d}}{2}\|\Sigma_{\varepsilon }^{-1/2}\Sigma_{\varepsilon}^{\mathsf{b}}\Sigma_{\varepsilon}^{-1/2}-\mathrm{ I}\|.\]

It remains to apply definition of \(\Omega_{3}\). 

### Auxiliary technical results.

For the analysis of the difference term \(\theta_{k}^{\mathsf{b}}-\theta_{k}\) we use the perturbation expansion technique introduced in [1], see also [16]. Within this approach, we represent the fluctuation component of the error \(\tilde{\theta}_{n}^{(\mathsf{ft})}\) defined in (26) as

\[\tilde{\theta}_{n}^{(\mathsf{ft})}=J_{n}^{(0)}+H_{n}^{(0)}\,\]

where the latter terms are defined by the following pair of recursions

\[J_{n}^{(0)}=\left(\mathrm{I}-\alpha_{n}\bar{\mathbf{A}}\right)J_ {n-1}^{(0)}-\alpha_{n}\varepsilon(Z_{n})\, J_{0}^{(0)}=0\,\] (52) \[H_{n}^{(0)}=\left(\mathrm{I}-\alpha_{n}\mathbf{A}(Z_{n})\right)H_ {n-1}^{(0)}-\alpha_{n}\tilde{\mathbf{A}}(Z_{n})J_{n-1}^{(0)}\, H_{0}^{(0)}=0\.\]Moreover, it is known that for \(L\geq 1\) the term \(H_{n}^{(0)}\) can be further decomposed as follows:

\[H_{n}^{(0)}=\sum_{\ell=1}^{L}J_{n}^{(\ell)}+H_{n}^{(L)}\;.\]

Here the terms \(J_{n}^{(\ell)}\) and \(H_{n}^{(\ell)}\) are given by the following recurrences:

\[\begin{split}& J_{n}^{(\ell)}=\left(\mathrm{I}-\alpha_{n}\tilde{ \mathbf{A}}\right)J_{n-1}^{(\ell)}-\alpha_{n}\tilde{\mathbf{A}}(Z_{n})J_{n-1}^ {(\ell-1)}\;,\qquad\quad J_{0}^{(\ell)}=0\;,\\ & H_{n}^{(\ell)}=\left(\mathrm{I}-\alpha_{n}\mathbf{A}(Z_{n}) \right)H_{n-1}^{(\ell)}-\alpha_{n}\tilde{\mathbf{A}}(Z_{n})J_{n-1}^{(\ell)}\;, \quad H_{0}^{(\ell)}=0\;.\end{split}\] (53)

The expansion depth \(L\) here controls the desired approximation accuracy. Informally, one can show that \(\mathbb{E}^{1/p}[\|J_{n}^{(\ell)}\|^{p}]\lesssim\alpha_{n}^{(\ell+1)/2}\), and similarly \(\mathbb{E}^{1/p}[\|H_{n}^{(\ell)}\|^{p}]\lesssim\alpha_{n}^{(\ell+1)/2}\). Using the outlined expansion, we prove the following lemma:

**Lemma 11**.: _Assume A1, A2, A3 with \(\gamma=1/2\), and A4. Then for any \(k\geq n\) and \(L\in\mathbb{N}\) the following decomposition holds:_

\[\theta_{k}^{\text{b}}-\theta_{k}=J_{k}^{\text{b},0}+\sum_{j=1}^{L}J_{k}^{ \text{b},j}+H_{k}^{\text{b},L},\] (54)

_where_

\[J_{k}^{\text{b},0} =-\sum_{\ell=n+1}^{k}\alpha_{\ell}(w_{\ell}-1)\Gamma_{\ell+1:k} \tilde{\varepsilon}_{\ell},\] \[J_{k}^{\text{b},j} =-\sum_{\ell=n+1}^{k}\alpha_{\ell}(w_{\ell}-1)\Gamma_{\ell+1:k}A_ {\ell}J_{\ell-1}^{\text{b},j-1},\quad j\in[1,L]\] \[H_{k}^{\text{b},L} =-\sum_{\ell=n+1}^{k}\alpha_{\ell}(w_{\ell}-1)\Gamma_{\ell+1:k}^{ \text{b}}A_{\ell}J_{\ell-1}^{\text{b},L}\;,\] (55)

_and the quantities \(\tilde{\varepsilon}_{\ell}\) are defined as_

\[\tilde{\varepsilon}_{\ell}=\mathbf{A}_{\ell}(\theta_{\ell-1}-\theta^{\star})+ \varepsilon_{\ell}\;.\]

_Moreover, on the event \(\Omega_{0}\),_

\[\mathbb{E}^{\text{b}}[\|J_{k}^{\text{b},j}\|^{2}] \lesssim\frac{\alpha_{k}^{j+1}\|\varepsilon\|_{\infty}^{2}\, \mathrm{C}_{\mathbf{A}}^{2j}}{a^{j+1}}+\mathrm{C}_{\mathbf{A}}^{2j+2}\prod_{j =1}^{k}(1-a\alpha_{j}/4)^{2}\|\theta_{0}-\theta^{\star}\|^{2}\;,\quad j\in[0,L]\] (56) \[\mathbb{E}^{\text{b}}[\|H_{k}^{\text{b},L}\|^{2}] \lesssim\frac{\alpha_{k}^{L+1}\,\mathrm{C}_{\mathbf{A}}^{2(L+1)} \,\|\varepsilon\|_{\infty}^{2}}{a^{L+3}}+\mathrm{C}_{\mathbf{A}}^{2(L+1)}\,k \prod_{j=1}^{k}(1-a\alpha_{j}/4)^{2}\|\theta_{0}-\theta^{\star}\|^{2}\;.\] (57)

Proof.: We start from the decomposition

\[\theta_{k}^{\text{b}}-\theta_{k}=(\mathrm{I}-\alpha_{k}w_{k} \mathbf{A}_{k})(\theta_{k-1}^{\text{b}}-\theta_{k-1})-\alpha_{k}(w_{k}-1) \tilde{\varepsilon}_{k}.\] (58)

Expanding the recurrence above till \(k=n\), and using the fact that \(\theta_{n}^{\text{b}}=\theta_{n}\), we get running the recurrence (58), that

\[\theta_{k}^{\text{b}}-\theta_{k}=-\sum_{\ell=n+1}^{k}\alpha_{\ell} (w_{\ell}-1)\Gamma_{\ell+1:k}^{\text{b}}\tilde{\varepsilon}_{\ell}\;.\]

Hence, proceeding as in (52), we obtain the representation

\[J_{k}^{(\text{b},0)}=\left(\mathrm{I}-\alpha_{k}\mathbf{A}_{k} \right)J_{k-1}^{(\text{b},0)}-\alpha_{k}(w_{k}-1)\tilde{\varepsilon_{k}}\;, \qquad\qquad\qquad\qquad\quad J_{0}^{(\text{b},0)}=0\;,\] \[H_{k}^{(\text{b},0)}=\left(\mathrm{I}-\alpha_{k}w_{k}\mathbf{A}_ {k}\right)H_{k-1}^{(\text{b},0)}-\alpha_{k}(w_{k}-1)\mathbf{A}_{k}J_{k-1}^{( \text{b},0)}\;,\qquad\qquad H_{0}^{(\text{b},0)}=0\;.\]It is easy to check that \(J_{k}^{(\text{b},0)}+H_{k}^{(\text{b},0)}=\theta_{k}^{\text{b}}-\theta_{k}\). Similarly, with further expansion of \(H_{k}^{(\text{b},0)}\) along the lines of (53), we arrive at the decomposition (54). Since \(w_{k}\) for \(k=n+1,\ldots,2n\) are i.i.d., we get using the definition of the events \(\Omega_{1}\) and \(\Omega_{2}\), that on the event \(\Omega_{0}\):

\[\begin{split}\|\tilde{\varepsilon}_{\ell}\|^{2}& \lesssim\|\varepsilon\|_{\infty}^{2}+\text{C}_{\mathbf{A}}^{2} \exp\bigl{\{}-a\sum_{j=1}^{\ell-1}\alpha_{j}\bigr{\}}\|\theta_{0}-\theta^{ \star}\|^{2}+\frac{\alpha_{\ell}\|\varepsilon\|_{\infty}^{2}\log^{2}n}{a}\\ &\lesssim\|\varepsilon\|_{\infty}^{2}+\text{C}_{\mathbf{A}}^{2} \prod_{j=1}^{\ell-1}(1-a\alpha_{j}/2)^{2}\|\theta_{0}-\theta^{\star}\|^{2}+ \frac{\alpha_{\ell}\|\varepsilon\|_{\infty}^{2}\log^{2}n}{a}\\ &\lesssim\|\varepsilon\|_{\infty}^{2}+\text{C}_{\mathbf{A}}^{2} \prod_{j=1}^{\ell-1}(1-a\alpha_{j}/2)^{2}\|\theta_{0}-\theta^{\star}\|^{2}\;, \end{split}\] (59)

where for the last bound we have additionally used that \(\alpha_{\ell}\log^{2}n/a\leq 1\) for \(\ell\geq n\). The latter bound is guaranteed by A4. Hence, using the bound (59) together with the definition of \(J_{k}^{\text{b},0}\), we obtain that

\[\begin{split}\mathbb{E}^{\text{b}}[\|J_{k}^{\text{b},0}\|^{2}]& =\sum_{\ell=n+1}^{k}\alpha_{\ell}^{2}\|\Gamma_{\ell+1:k}\tilde{ \varepsilon}_{\ell}\|^{2}=\sum_{\ell=n+1}^{k}\alpha_{\ell}^{2}\|\Gamma_{\ell+ 1:k}\bigl{(}\mathbf{A}_{\ell}(\theta_{\ell-1}-\theta^{\star})+\varepsilon_{ \ell}\bigr{)}\|^{2}\\ &\lesssim\|\varepsilon\|_{\infty}^{2}\sum_{\ell=n+1}^{k}\alpha_{ \ell}^{2}\prod_{j=\ell+1}^{k}(1-a\alpha_{j}/4)^{2}+\text{C}_{\mathbf{A}}^{2} \sum_{\ell=n+1}^{k}\alpha_{\ell}^{2}\prod_{j=1}^{k}(1-a\alpha_{j}/4)^{2}\| \theta_{0}-\theta^{\star}\|^{2}\\ &\qquad\qquad+\frac{\|\varepsilon\|_{\infty}^{2}\,\text{C}_{ \mathbf{A}}^{2}\log^{2}n}{a}\sum_{\ell=n+1}^{k}\alpha_{\ell}^{3}\prod_{j=\ell+ 1}^{k}(1-a\alpha_{j}/4)^{2}\\ &\lesssim\frac{\|\varepsilon\|_{\infty}^{2}\alpha_{k}}{a}+\text{ C}_{\mathbf{A}}^{2}\log\left(\frac{k}{n}\right)\prod_{j=1}^{k}(1-a\alpha_{j}/4)^{2}\| \theta_{0}-\theta^{\star}\|^{2}+\frac{\alpha_{\ell}^{2}\|\varepsilon\|_{\infty }^{2}\,\text{C}_{\mathbf{A}}^{2}\log^{2}n}{a^{2}}\\ &\lesssim\frac{\|\varepsilon\|_{\infty}^{2}\alpha_{k}}{a}+\text{ C}_{\mathbf{A}}^{2}\prod_{j=1}^{k}(1-a\alpha_{j}/4)^{2}\|\theta_{0}-\theta^{\star}\|^{2}\;, \end{split}\]

where we additionally used the fact that \(k\in[n;2n]\) and \(n\) satisfies A4. Assume now that the bound on \(J_{k}^{\text{b},j-1}\) has a form

\[\mathbb{E}^{\text{b}}[\|J_{k}^{\text{b},j-1}\|^{2}]\lesssim\frac{\|\varepsilon \|_{\infty}^{2}\alpha_{k}^{j}}{a^{j}}+\text{C}_{\mathbf{A}}^{2j}\prod_{\ell=1}^ {k}(1-a\alpha_{\ell}/4)^{2}\|\theta_{0}-\theta^{\star}\|^{2}\;.\]

Then, using the martingale property of \(J_{k}^{\text{b},j}\), we write that

\[\begin{split}\mathbb{E}^{\text{b}}[\|J_{k}^{\text{b},j}\|^{2}]& =\sum_{\ell=n+1}^{k}\alpha_{\ell}^{2}\mathbb{E}^{\text{b}}[\| \Gamma_{\ell+1:k}A_{\ell}J_{\ell-1}^{\text{b},j-1}\|^{2}]\\ &\lesssim\sum_{\ell=n+1}^{k}\frac{\alpha_{\ell}^{j+2}\|\varepsilon \|_{\infty}^{2}\,\text{C}_{\mathbf{A}}^{2j}}{a^{j}}\prod_{j=\ell+1}^{k}(1-a \alpha_{j}/4)^{2}+\text{C}_{\mathbf{A}}^{2j+2}\sum_{\ell=n+1}^{k}\alpha_{\ell} ^{2}\prod_{j=1}^{k}(1-a\alpha_{j}/4)^{2}\|\theta_{0}-\theta^{\star}\|^{2}\\ &\lesssim\frac{\alpha_{k}^{j+1}\|\varepsilon\|_{\infty}^{2}\,\text {C}_{\mathbf{A}}^{2j}}{a^{j+1}}+\text{C}_{\mathbf{A}}^{2j+2}\prod_{j=1}^{k}(1-a \alpha_{j}/4)^{2}\|\theta_{0}-\theta^{\star}\|^{2}\;,\end{split}\]and thus the bound (56) is proved. Moreover, using (55) and Minkowski's inequality, we obtain that

\[(\mathbb{E}^{\mathsf{b}}[\|H_{k}^{\mathsf{b},L}\|^{2}])^{1/2} \leq\mathrm{C}_{\mathbf{A}}\sum_{\ell=n+1}^{k}\alpha_{\ell}( \mathbb{E}^{\mathsf{b}}[\|\Gamma_{\ell+1:k}^{\mathsf{b}}\|^{2}])^{1/2}( \mathbb{E}^{\mathsf{b}}[\|J_{\ell-1}^{\mathsf{b},L}\|^{2}])^{1/2}\] \[\lesssim\mathrm{C}_{\mathbf{A}}\sum_{\ell=n+1}^{k}\frac{\alpha_{ \ell}^{(L+3)/2}\|\varepsilon\|_{\infty}\,\mathrm{C}_{\mathbf{A}}^{L}}{a^{(L+1)/ 2}}\prod_{j=\ell+1}^{k}(1-a\alpha_{j}/4)\] \[\qquad\qquad\qquad+\mathrm{C}_{\mathbf{A}}^{L+1}\sum_{\ell=n+1}^ {k}\alpha_{\ell}\prod_{j=1}^{k}(1-a\alpha_{j}/4)^{2}\|\theta_{0}-\theta^{ \star}\|\] \[\lesssim\frac{\alpha_{k}^{(L+1)/2}\,\mathrm{C}_{\mathbf{A}}^{L+1} \,\|\varepsilon\|_{\infty}}{a^{(L+3)/2}}+\mathrm{C}_{\mathbf{A}}^{L+1}\, \sqrt{k}\prod_{j=1}^{k}(1-a\alpha_{j}/4)^{2}\|\theta_{0}-\theta^{\star}\|\,\]

and (57) follows. 

### Proof of Proposition 5

Recall that the quantity \(D^{\mathsf{b}}\) is defined in (49). Since \(\theta_{n}^{\mathsf{b}}=\theta_{n}\), we conclude that \(D^{\mathsf{b}}_{1}=0\). To estimate other terms we will use the main error decomposition outlined in Lemma 11, that is, the expansion

\[\theta_{k}^{\mathsf{b}}-\theta_{k}=\sum_{\ell=0}^{L}J_{k}^{\mathsf{b},\ell}+H _{k}^{\mathsf{b},L},\]

applied with different \(L\geq 0\). To bound \(D^{\mathsf{b}}_{2}\) we take \(L=0\) and obtain

\[\mathbb{E}^{\mathsf{b}}[\|D^{\mathsf{b}}_{2}\|^{2}] \lesssim\mathbb{E}^{\mathsf{b}}[\|J_{2n}^{\mathsf{b},j}\|^{2}]+ \mathbb{E}^{\mathsf{b}}[\|H_{2n}^{\mathsf{b},L}\|^{2}]\lesssim\frac{\| \varepsilon\|_{\infty}^{2}}{n\alpha_{2n}a}\left(1+\frac{\mathrm{C}_{\mathbf{ A}}^{2}}{a^{2}}\right)+\frac{\mathrm{C}_{\mathbf{A}}^{2}}{\alpha_{2n}^{2}} \prod_{j=1}^{2n}(1-a\alpha_{j}/4)^{2}\|\theta_{0}-\theta^{\star}\|^{2}\] \[\lesssim\frac{\|\varepsilon\|_{\infty}^{2}}{a\sqrt{n}}\left(1+ \frac{\mathrm{C}_{\mathbf{A}}^{2}}{a^{2}}\right)+n\,\mathrm{C}_{\mathbf{A}}^{ 2}\prod_{j=1}^{2n}(1-a\alpha_{j}/4)^{2}\|\theta_{0}-\theta^{\star}\|^{2}\.\]

To estimate \(D^{\mathsf{b}}_{3}\) we note that

\[D^{\mathsf{b}}_{3}=\frac{1}{\sqrt{n}}\sum_{k=n+1}^{2n}(w_{k}-1)\mathbf{A}_{k}( \theta_{k-1}^{\mathsf{b}}-\theta^{\star})=D^{\mathsf{b}}_{3,1}+D^{\mathsf{b}}_ {3,2}\,\]

where we have set, respectively,

\[D^{\mathsf{b}}_{3,1} =\frac{1}{\sqrt{n}}\sum_{k=n+1}^{2n}(w_{k}-1)\mathbf{A}_{k}( \theta_{k-1}^{\mathsf{b}}-\theta_{k-1}),\] \[D^{\mathsf{b}}_{3,2} =\frac{1}{\sqrt{n}}\sum_{k=n+1}^{2n}(w_{k}-1)\mathbf{A}_{k}( \theta_{k-1}-\theta^{\star})\.\]

It follows from Lemma 9 that on the event \(\Omega_{0}\) it holds

\[\mathbb{E}^{\mathsf{b}}[\|D^{\mathsf{b}}_{3,1}\|^{2}] \leq\frac{\mathrm{C}_{\mathbf{A}}^{2}}{n}\sum_{k=n+1}^{2n}\mathbb{ E}^{\mathsf{b}}[\|\theta_{k-1}^{\mathsf{b}}-\theta_{k-1}\|^{2}]\] \[\lesssim\frac{\mathrm{C}_{\mathbf{A}}^{2}}{n}\sum_{k=n+1}^{2n} \frac{\alpha_{k}\|\varepsilon\|_{\infty}^{2}}{a}\left(1+\frac{\mathrm{C}_{ \mathbf{A}}^{2}}{a^{2}}\right)+\mathrm{C}_{\mathbf{A}}^{4}\sum_{k=n+1}^{2n} \prod_{j=1}^{k}(1-a\alpha_{j}/4)^{2}\|\theta_{0}-\theta^{\star}\|^{2}\] \[\lesssim\frac{\mathrm{C}_{\mathbf{A}}^{2}\|\varepsilon\|_{\infty} ^{2}}{a\sqrt{n}}\left(1+\frac{\mathrm{C}_{\mathbf{A}}^{2}}{a^{2}}\right)+\frac{ \mathrm{C}_{\mathbf{A}}^{4}\sqrt{n}}{a}\exp\biggl{\{}-c_{0}a\sqrt{n}\biggr{\}} \|\theta_{0}-\theta^{\star}\|^{2}\]Moreover, on the set \(\Omega_{0}\) it holds (since \(\Omega_{0}\subseteq\Omega_{1}\)), that

\[\mathbb{E}^{\mathsf{b}}[\|D^{\mathsf{b}}_{3,2}\|^{2}] =\frac{1}{n}\sum_{k=n+1}^{2n}\|\mathbf{A}_{k}(\theta_{k-1}-\theta^ {\star})\|^{2}\] \[\lesssim\frac{\mathrm{C}_{\mathbf{A}}^{2}}{n}\sum_{k=n+1}^{2n} \left(\exp\bigl{\{}-a\sum_{\ell=1}^{k}\alpha_{\ell}\bigr{\}}\|\theta_{0}- \theta^{\star}\|^{2}+\frac{\alpha_{k}\|\varepsilon\|_{\infty}^{2}\log^{2}n}{a }\right)\] \[\lesssim\frac{\mathrm{C}_{\mathbf{A}}^{2}}{na\alpha_{2n}}\exp \bigl{\{}-a\sum_{\ell=1}^{n}\alpha_{\ell}\bigr{\}}\|\theta_{0}-\theta^{\star} \|^{2}+\frac{\mathrm{C}_{\mathbf{A}}^{2}\,\|\varepsilon\|_{\infty}^{2}\log^{2} n}{na}\sum_{k=n+1}^{2n}\alpha_{k}\] \[\lesssim\frac{\mathrm{C}_{\mathbf{A}}^{2}}{a\sqrt{n}}\exp\biggl{\{} -c_{0}a\sqrt{n}\biggr{\}}\|\theta_{0}-\theta^{\star}\|^{2}+\frac{\mathrm{C}_{ \mathbf{A}}^{2}\,\|\varepsilon\|_{\infty}^{2}\log^{2}n}{a\sqrt{n}}\.\]

Combining the above bounds, we get

\[\mathbb{E}^{\mathsf{b}}[\|D^{\mathsf{b}}_{3}\|^{2}] \lesssim\mathbb{E}^{\mathsf{b}}[\|D^{\mathsf{b}}_{3,1}\|^{2}]+ \mathbb{E}^{\mathsf{b}}[\|D^{\mathsf{b}}_{3,2}\|^{2}]\] \[\lesssim\frac{\mathrm{C}_{\mathbf{A}}^{2}\,\|\varepsilon\|_{ \infty}^{2}\log^{2}n}{a\sqrt{n}}\left(1+\frac{\mathrm{C}_{\mathbf{A}}^{2}}{a^{ 2}}\right)+\frac{\mathrm{C}_{\mathbf{A}}^{4}\,\sqrt{n}}{a}\exp\biggl{\{}-c_{0 }a\sqrt{n}\biggr{\}}\|\theta_{0}-\theta^{\star}\|^{2}\.\]

Now we proceed with the term \(D^{\mathsf{b}}_{4}\). Applying Minkowski's inequality, we get

\[\{\mathbb{E}^{\mathsf{b}}[\|D^{\mathsf{b}}_{4}\|^{2}]\}^{1/2} \leq\frac{1}{\sqrt{n}}\sum_{k=n+1}^{2n}\left(\frac{1}{\alpha_{k} }-\frac{1}{\alpha_{k-1}}\right)\{\mathbb{E}^{\mathsf{b}}[\|\theta_{k-1}^{ \mathsf{b}}-\theta_{k-1}\|^{2}]\}^{1/2}\] \[\lesssim\frac{1}{\sqrt{n}}\sum_{k=n+1}^{2n}\frac{1}{\sqrt{k}} \left(\frac{\sqrt{\alpha_{k}}\|\varepsilon\|_{\infty}}{\sqrt{a}}\left(1+\frac {\mathrm{C}_{\mathbf{A}}}{a}\right)+\mathrm{C}_{\mathbf{A}}\,\sqrt{k}\prod_{j =1}^{k}(1-a\alpha_{j}/4)\|\theta_{0}-\theta^{\star}\|\right)\] \[\lesssim\frac{\|\varepsilon\|_{\infty}}{n^{1/4}\sqrt{a}}\left(1+ \frac{\mathrm{C}_{\mathbf{A}}}{a}\right)+\mathrm{C}_{\mathbf{A}}\exp\biggl{\{} -\frac{c_{0}a\sqrt{n}}{2}\biggr{\}}\|\theta_{0}-\theta^{\star}\|\.\]

It remains to upper bound the term \(D^{\mathsf{b}}_{5}\). Using the decomposition, suggested by Lemma 11 with \(L=2\), we get that

\[D^{\mathsf{b}}_{5} =\frac{1}{\sqrt{n}}\sum_{k=n+1}^{2n}(\mathbf{A}_{k}-\bar{\mathbf{ A}})(\theta_{k-1}^{\mathsf{b}}-\theta_{k-1})=\underbrace{\frac{1}{\sqrt{n}}\sum_{k=n+1}^ {2n}(\mathbf{A}_{k}-\bar{\mathbf{A}})J_{k-1}^{\mathsf{b},0}}_{D^{\mathsf{b}}_{5,1}}\] \[\quad+\underbrace{\frac{1}{\sqrt{n}}\sum_{k=n+1}^{2n}(\mathbf{A}_ {k}-\bar{\mathbf{A}})J_{k-1}^{\mathsf{b},1}}_{D^{\mathsf{b}}_{5,2}}+ \underbrace{\frac{1}{\sqrt{n}}\sum_{k=n+1}^{2n}(\mathbf{A}_{k}-\bar{\mathbf{A}} )J_{k-1}^{\mathsf{b},2}}_{D^{\mathsf{b}}_{5,3}}+\underbrace{\frac{1}{\sqrt{n}} \sum_{k=n+1}^{2n}(\mathbf{A}_{k}-\bar{\mathbf{A}})H_{k-1}^{\mathsf{b},2}}_{D^{ \mathsf{b}}_{5,4}}\.\]

Here we have to consider expansion until \(H^{\mathsf{b},2}\), since dealing with the latter term (outlined as \(D^{\mathsf{b}}_{5,4}\) in the above expansion) is possible only with Minkowski's inequality. Now we consider the summands \(D^{\mathsf{b}}_{5,1}-D^{\mathsf{b}}_{5,4}\) separately. Consider first the term \(D^{\mathsf{b}}_{5,1}\). Changing the summation order, we obtain

\[D^{\mathsf{b}}_{5,1} =-\frac{1}{\sqrt{n}}\sum_{k=n+1}^{2n}(\mathbf{A}_{k}-\bar{\mathbf{ A}})\sum_{\ell=n+1}^{k-1}\alpha_{\ell}(w_{\ell}-1)\Gamma_{\ell+1:k-1}\tilde{ \varepsilon}_{\ell}\] \[=-\frac{1}{\sqrt{n}}\sum_{\ell=n+1}^{2n-1}\alpha_{\ell}(w_{\ell}- 1)\biggl{(}\sum_{k=\ell+1}^{2n}(\mathbf{A}_{k}-\bar{\mathbf{A}})\Gamma_{\ell+1:k -1}\biggr{)}\tilde{\varepsilon}_{\ell}\.\]Then on the event \(\Omega_{0}\) we get, since \(\Omega_{0}\subseteq\Omega_{4}\), and using that \(n\) satisfies A4,

\[\|\sum_{k=\ell+1}^{2n}(\mathbf{A}_{k}-\bar{\mathbf{A}})\Gamma_{\ell+1:k-1}\|^{2} \lesssim\frac{\log n}{a\alpha_{\ell}}\.\] (60)

Combining the above bound together with the one provided by (59), we obtain that

\[\mathbb{E}^{\text{b}}[\|D^{\text{b}}_{5,1}\|^{2}] \lesssim\frac{\|\varepsilon\|_{\infty}^{2}}{n}\sum_{\ell=n+1}^{2 n-1}\frac{\alpha_{\ell}\log n}{a}+\frac{\text{C}^{2}_{\mathbf{A}}}{n}\sum_{ \ell=n+1}^{2n-1}\frac{\alpha_{\ell}\log n}{a}\prod_{j=1}^{\ell-1}(1-a\alpha_ {j}/2)^{2}\|\theta_{0}-\theta^{\star}\|^{2}\] \[\lesssim\frac{\|\varepsilon\|_{\infty}^{2}\log n}{\sqrt{n}a}+ \frac{\text{C}^{2}_{\mathbf{A}}\log n}{a^{2}n}\exp\{-a\sum_{j=1}^{n}\alpha_{ j}\}\|\theta_{0}-\theta^{\star}\|^{2}\.\]

Similarly, for the term \(D^{\text{b}}_{5,2}\) we get, changing the order of summation, that

\[D^{\text{b}}_{5,2}=\frac{1}{\sqrt{n}}\sum_{\ell=n+1}^{2n-1}\alpha_{\ell}(w_{ \ell}-1)\bigg{(}\sum_{k=\ell+1}^{2n}(\mathbf{A}_{k}-\bar{\mathbf{A}})\Gamma_{ \ell+1:k-1}\bigg{)}A_{\ell}J^{\text{b},0}_{\ell-1}\.\]

Hence, using the bound (60) together with (56), we get

\[\mathbb{E}^{\text{b}}[\|D^{\text{b}}_{5,2}\|^{2}] \lesssim\frac{1}{n}\sum_{\ell=n+1}^{2n-1}\frac{\alpha_{\ell}\log n }{a}\,\text{C}^{2}_{\mathbf{A}}\bigg{(}\frac{\alpha_{\ell}\|\varepsilon\|_{ \infty}^{2}}{a}+\text{C}^{2}_{\mathbf{A}}\prod_{j=1}^{\ell-1}(1-a\alpha_{j}/4) ^{2}\|\theta_{0}-\theta^{\star}\|^{2}\bigg{)}\] \[\lesssim\frac{\text{C}^{2}_{\mathbf{A}}\,\|\varepsilon\|_{\infty }^{2}\log n}{na^{2}}\sum_{\ell=n+1}^{2n-1}\alpha_{\ell}^{2}+\frac{\text{C}^{ 4}_{\mathbf{A}}\log n}{na}\sum_{\ell=n+1}^{2n-1}\alpha_{\ell}\prod_{j=1}^{\ell -1}(1-a\alpha_{j}/4)^{2}\|\theta_{0}-\theta^{\star}\|^{2}\] \[\lesssim\frac{\text{C}^{2}_{\mathbf{A}}\,\|\varepsilon\|_{\infty }^{2}\log n}{na^{2}}+\frac{\text{C}^{4}_{\mathbf{A}}\log n}{na}\exp\{-(a/2) \sum_{j=1}^{n}\alpha_{j}\}\|\theta_{0}-\theta^{\star}\|^{2}\.\]

We proceed with \(D^{\text{b}}_{5,3}\). We change the summation order and proceed exactly as with \(D^{\text{b}}_{5,2}\). Indeed,

\[D^{\text{b}}_{5,3}=\frac{1}{\sqrt{n}}\sum_{\ell=n+1}^{2n-1}\alpha_{\ell}(w_{ \ell}-1)\bigg{(}\sum_{k=\ell+1}^{2n}(\mathbf{A}_{k}-\bar{\mathbf{A}})\Gamma_{ \ell+1:k-1}\bigg{)}A_{\ell}J^{\text{b},1}_{\ell-1}\,\]

and

\[\mathbb{E}^{\text{b}}[\|D^{\text{b}}_{5,3}\|^{2}] \lesssim\frac{1}{n}\sum_{\ell=n+1}^{2n-1}\frac{\alpha_{\ell}\log n }{a}\,\text{C}^{2}_{\mathbf{A}}\bigg{(}\frac{\alpha_{\ell}^{2}\|\varepsilon\|_ {\infty}^{2}\,\text{C}^{2}_{\mathbf{A}}}{a^{2}}+\text{C}^{4}_{\mathbf{A}}\prod _{j=1}^{\ell-1}(1-a\alpha_{j}/4)^{2}\|\theta_{0}-\theta^{\star}\|^{2}\bigg{)}\] \[\lesssim\frac{\text{C}^{4}_{\mathbf{A}}\,\|\varepsilon\|_{\infty }^{2}\log n}{n^{3/2}a^{3}}+\frac{\text{C}^{6}_{\mathbf{A}}\log n}{na}\exp\{-(a/ 2)\sum_{j=1}^{n}\alpha_{j}\}\|\theta_{0}-\theta^{\star}\|^{2}\.\]

It remains to upper bound \(D^{\text{b}}_{5,4}\). Proceeding as above, we change the summation order, and obtain

\[D^{\text{b}}_{5,4}=\frac{1}{\sqrt{n}}\sum_{\ell=n+1}^{2n-1}\alpha_{\ell}(w_{ \ell}-1)\bigg{(}\sum_{k=\ell+1}^{2n}(\mathbf{A}_{k}-\bar{\mathbf{A}})\Gamma^{ \text{b}}_{\ell+1:k-1}\bigg{)}A_{\ell}J^{\text{b},2}_{\ell-1}\.\]Applying Minkowski's inequality, we get

\[(\mathbb{E}^{\mathsf{b}}[\|D^{\mathsf{b}}_{5,4}\|^{2}])^{1/2} \lesssim\frac{\mathrm{C}_{\mathbf{A}}^{2}}{\sqrt{n}}\sum_{\ell=n+1 }^{2n-1}\alpha_{\ell}\sum_{k=\ell+1}^{2n}(\mathbb{E}^{\mathsf{b}}[\|\Gamma^{ \mathsf{b}}_{\ell+1:k-1}\|^{2}])^{1/2}(\mathbb{E}^{\mathsf{b}}[\|J^{\mathsf{b},2}_{\ell-1}\|^{2}])^{1/2}\] \[\lesssim\frac{\mathrm{C}_{\mathbf{A}}^{2}}{\sqrt{n}}\sum_{\ell=n +1}^{2n-1}\alpha_{\ell}^{5/2}\sum_{k=\ell+1}^{2n}\exp\bigl{\{}-\frac{a}{4}\sum_ {j=\ell+1}^{k-1}\alpha_{j}\bigr{\}}\frac{\|\varepsilon\|_{\infty}\,\mathrm{C}_ {\mathbf{A}}^{2}}{a^{3/2}}\] \[\qquad\qquad\qquad\qquad\qquad+\frac{\mathrm{C}_{\mathbf{A}}^{ 5}}{\sqrt{n}}\sum_{\ell=n+1}^{2n-1}\alpha_{\ell}\sum_{k=\ell+1}^{2n-1}\exp \bigl{\{}-\frac{a}{4}\sum_{j=1}^{k-1}\alpha_{j}\bigr{\}}\|\theta_{0}-\theta^{ \star}\|\] \[\lesssim\frac{\mathrm{C}_{\mathbf{A}}^{4}\,\|\varepsilon\|_{ \infty}}{\sqrt{n}a^{5/2}}\sum_{\ell=n+1}^{2n-1}\alpha_{\ell}^{3/2}+\frac{ \mathrm{C}_{\mathbf{A}}^{5}}{\sqrt{n}a}\exp\bigl{\{}-\frac{a}{4}\sum_{j=1}^{n} \alpha_{j}\bigr{\}}\|\theta_{0}-\theta^{\star}\|\] \[\lesssim\frac{\mathrm{C}_{\mathbf{A}}^{4}\,\|\varepsilon\|_{ \infty}}{n^{1/4}a^{5/2}}+\frac{\mathrm{C}_{\mathbf{A}}^{5}}{\sqrt{n}a}\exp \bigl{\{}-\frac{a}{4}\sum_{j=1}^{n}\alpha_{j}\bigr{\}}\|\theta_{0}-\theta^{ \star}\|\.\]

Now the result follows from the representation (49) and combinations of the above bounds for \(D^{\mathsf{b}}_{1}-D^{\mathsf{b}}_{5}\).

### Proof of Proposition 6

Consider the sequences of weights

\[(w_{1},\ldots,w_{i-1},w_{i},w_{i+1},\ldots,w_{2n})\text{ and }(w_{1},\ldots,w_{i-1},w_{i}^{ \prime},w_{i+1},\ldots,w_{2n})\,\] (61)

which differs only in position \(i\), \(n+1\leq i\leq 2n\), with \(w_{i}^{\prime}\) being an independent copy of \(w_{i}\). Consider the associated SA processes

\[\theta^{\mathsf{b}}_{k} =\theta^{\mathsf{b}}_{k-1}-\alpha_{k}w_{k}\{\mathbf{A}(Z_{k}) \theta_{k-1}-\mathbf{b}(Z_{k})\}\,\quad k\geq n+1,\quad\theta^{\mathsf{b}}_{n}=\theta_{n}\in \mathbb{R}^{d}\] (62) \[\theta^{(\mathsf{b},i)}_{k} =\theta^{(\mathsf{b},i)}_{k-1}-\alpha_{k}w^{(i)}_{k}\{\mathbf{A}( Z_{k})\theta^{(\mathsf{b},i)}_{k-1}-\mathbf{b}(Z_{k})\}\,\quad k\geq n+1\,\quad\theta^{(\mathsf{b},i)}_{n}=\theta_{n}\in \mathbb{R}^{d}\,\]

where \(w^{(i)}_{k}=w_{k}\) for \(k\neq i\) and \(w^{(i)}_{i}=w^{\prime}_{i}\). Respective random variables \(D^{\mathsf{b}}\) and \(D^{(\mathsf{b},i)}\) are based on the first and second sequences from (61), respectively, and are constructed according to the equation (48). From the above representations we easily observe that \(\theta^{\mathsf{b}}_{k}=\theta^{(\mathsf{b},i)}_{k}\) for \(k<i\), moreover,

\[\theta^{\mathsf{b}}_{i}-\theta^{(\mathsf{b},i)}_{i} =-\alpha_{i}(w_{i}-w^{\prime}_{i})\bigl{\{}\mathbf{A}(Z_{i})) \theta^{\mathsf{b}}_{i-1}-\mathbf{b}(Z_{i})\bigr{\}}\] \[=-\alpha_{i}(w_{i}-w^{\prime}_{i})\bigl{\{}\mathbf{A}(Z_{i}))( \theta^{\mathsf{b}}_{i-1}-\theta_{i-1})-\mathbf{b}(Z_{i})\bigr{\}}\] \[=-\alpha_{i}(w_{i}-w^{\prime}_{i})\bigl{\{}\mathbf{A}(Z_{i}))( \theta^{\mathsf{b}}_{i-1}-\theta_{i-1})+\tilde{\varepsilon}_{i}\bigr{\}}\.\]

where \(\varepsilon_{i}=\varepsilon(Z_{i})\) and \(\varepsilon^{\prime}_{i}=\varepsilon(Z^{\prime}_{i})\). From the above representation we get, applying Lemma 9 and (59), that

\[\{\mathbb{E}^{\mathsf{b}}[\|\theta^{\mathsf{b}}_{i}-\theta^{( \mathsf{b},i)}_{i}\|^{2}]\}^{1/2} \lesssim\alpha_{i}\,\mathrm{C}_{\mathbf{A}}\{\mathbb{E}^{\mathsf{ b}}[\|\theta^{\mathsf{b}}_{i}-\theta_{i}\|^{2}]\}^{1/2}+\alpha_{i}\, \mathrm{C}_{\mathbf{A}}\,\|\tilde{\varepsilon}_{i}\|\] (63) \[\lesssim\alpha_{i}\,\mathrm{C}_{\mathbf{A}}\,\|\varepsilon\|_{ \infty}+\frac{\alpha_{i}^{3/2}\|\varepsilon\|_{\infty}\,\mathrm{C}_{\mathbf{A}}^{ 2}}{a^{3/2}}+(\mathrm{C}_{\mathbf{A}}^{2}\lor 1)\sqrt{i}\prod_{j=1}^{i}(1-a\alpha_{j}/4)\| \theta_{0}-\theta^{\star}\|\] \[\lesssim\frac{\alpha_{i}\|\varepsilon\|_{\infty}\,\mathrm{C}_{ \mathbf{A}}^{2}}{a}+(\mathrm{C}_{\mathbf{A}}^{2}\lor 1)\sqrt{i}\prod_{j=1}^{i}(1-a\alpha_{j}/4)\| \theta_{0}-\theta^{\star}\|\,\]

where for the last line we have additionally assumed that \(\alpha_{i}\lesssim a\) for \(i\geq n\). Moreover, for any \(j>i\) one observes, expanding (62), that

\[\theta^{\mathsf{b}}_{j}-\theta^{(\mathsf{b},i)}_{j}=\bigg{\{}\prod_{k=i+1}^{j}( \mathrm{I}-\alpha_{k}w_{k}\mathbf{A}(Z_{k}))\bigg{\}}(\theta^{\mathsf{b}}_{i}- \theta^{(\mathsf{b},i)}_{i})=\Gamma^{\mathsf{b}}_{i+1:j}\,(\theta^{\mathsf{b}}_{i} -\theta^{(\mathsf{b},i)}_{i})\.\]Thus, similarly to (41), we obtain that

\[\{\mathbb{E}^{\mathsf{b}}[\|D^{\mathsf{b}}-D^{(\mathsf{b},i)}\|^{2}]\}^{1/2}\leq \sum_{j=1}^{5}\{\mathbb{E}^{\mathsf{b}}[\|D^{\mathsf{b}}_{j}-D^{(\mathsf{b},i)}_{ j}\|^{2}]\}^{1/2}\,\]

and bound the respective differences separately. By the construction of the process above, we note that \(D^{\mathsf{b}}_{1}=D^{(\mathsf{b},i)}_{1}\). Proceeding further, and using the equation (48), we obtain that

\[\{\mathbb{E}^{\mathsf{b}}[\|D^{\mathsf{b}}_{2}-D^{(\mathsf{b},i)}_ {2}\|^{2}]\}^{1/2} =\frac{1}{\sqrt{n}\alpha_{2n}}\{\mathbb{E}^{\mathsf{b}}[\|\theta ^{\mathsf{b}}_{2n}-\theta^{(\mathsf{b},i)}_{2n}\|^{2}]\}^{1/2}\] \[\leq\frac{1}{\sqrt{n}\alpha_{2n}}\{\mathbb{E}^{\mathsf{b}}[\| \Gamma^{\mathsf{b}}_{i+1:2n}\|^{2}]\}^{1/2}\{\mathbb{E}^{\mathsf{b}}[\|\theta ^{\mathsf{b}}_{i}-\theta^{(\mathsf{b},i)}_{i}\|^{2}]\}^{1/2}\] \[\lesssim\frac{\alpha_{i}\|\varepsilon\|_{\infty}\,\mathrm{C}^{ \mathrm{2}}_{\mathbf{A}}}{\sqrt{n}\alpha_{2n}a}\exp\bigl{\{}-\frac{a}{4}\sum_{ j=i+1}^{2n}\alpha_{j}\bigr{\}}+\frac{(\mathrm{C}^{\mathrm{2}}_{\mathbf{A}} \lor 1)\sqrt{i}}{\sqrt{n}\alpha_{2n}}\exp\bigl{\{}-\frac{a}{4}\sum_{j=1}^{2n} \alpha_{j}\bigr{\}}\|\theta_{0}-\theta^{\star}\|\.\]

Thus, taking sum for \(i\) from \(n+1\) to \(2n\), and applying Lemma 2, we get that

\[\sum_{i=n+1}^{2n}\{\mathbb{E}^{\mathsf{b}}[\|D^{\mathsf{b}}_{2}-D ^{(\mathsf{b},i)}_{2}\|^{2}]\}^{1/2} \lesssim\sum_{i=n+1}^{2n}\frac{\alpha_{i}\|\varepsilon\|_{\infty }\,\mathrm{C}^{\mathrm{2}}_{\mathbf{A}}}{\sqrt{n}\alpha_{2n}a}\exp\bigl{\{}- \frac{a}{4}\sum_{j=i+1}^{2n}\alpha_{j}\bigr{\}}\] \[\lesssim\frac{\|\varepsilon\|_{\infty}\,\mathrm{C}^{\mathrm{2}}_{ \mathbf{A}}}{\sqrt{n}\alpha_{2n}a^{2}}+\frac{(\mathrm{C}^{\mathrm{2}}_{\mathbf{ A}}\lor 1)n}{\alpha_{2n}}\exp\bigl{\{}-\frac{a}{4}\sum_{j=1}^{2n}\alpha_{j} \bigr{\}}\|\theta_{0}-\theta^{\star}\|\] \[\lesssim\frac{\|\varepsilon\|_{\infty}\,\mathrm{C}^{\mathrm{2}}_{ \mathbf{A}}}{a^{2}}+(\mathrm{C}^{\mathrm{2}}_{\mathbf{A}}\lor 1)n^{3/2}\exp \bigl{\{}-\frac{a}{4}\sum_{j=1}^{2n}\alpha_{j}\bigr{\}}\|\theta_{0}-\theta^{ \star}\|\] \[\lesssim\frac{\|\varepsilon\|_{\infty}\,\mathrm{C}^{\mathrm{2}}_{ \mathbf{A}}}{a^{2}}+(\mathrm{C}^{\mathrm{2}}_{\mathbf{A}}\lor 1)n^{3/4}\exp \bigl{\{}-\frac{a}{4}\sum_{j=1}^{n}\alpha_{j}\bigr{\}}\|\theta_{0}-\theta^{ \star}\|\.\] (64)

Here in the last line above we used a particular form \(\alpha_{k}=c_{0}/\sqrt{k}\), and relied on the bound

\[n^{3/4}\exp\bigl{\{}-\frac{a}{4}\sum_{j=n+1}^{2n}\alpha_{j}\bigr{\}}\leq 1\,\]

which is guaranteed by the lower bound on the trajectory length \(n\) of the form

\[\frac{\sqrt{n}}{\log n}\geq\frac{3}{2(\sqrt{2}-1)ac_{0}}\.\]

The latter condition is guaranteed by A4. Now we proceed with \(D^{\mathsf{b}}_{3}-D^{(\mathsf{b},i)}_{3}\). Using its definition in (48), we get

\[D^{\mathsf{b}}_{3}-D^{(\mathsf{b},i)}_{3}=\frac{1}{\sqrt{n}}(w_{i}-w_{i}^{ \prime})\mathbf{A}_{i}(\theta^{\mathsf{b}}_{i-1}-\theta^{\star})+\frac{1}{ \sqrt{n}}\sum_{k=i+1}^{2n}(w_{k}-1)\mathbf{A}_{k}(\theta^{\mathsf{b}}_{k-1}- \theta^{(\mathsf{b},i)}_{k-1})\.\]

Since the latter term is a martingale-difference, we obtain that

\[\mathbb{E}^{\mathsf{b}}[\|D^{\mathsf{b}}_{3}-D^{(\mathsf{b},i)}_{3} \|^{2}] \lesssim\frac{\mathrm{C}^{\mathrm{2}}_{\mathbf{A}}}{n}\mathbb{E}^{ \mathsf{b}}[\|\theta^{\mathsf{b}}_{i-1}-\theta^{\star}\|^{2}]+\frac{\mathrm{C} ^{\mathrm{2}}_{\mathbf{A}}}{n}\sum_{k=i+1}^{2n}\mathbb{E}^{\mathsf{b}}[\|\theta ^{\mathsf{b}}_{k-1}-\theta^{(\mathsf{b},i)}_{k-1}\|^{2}]\] \[\lesssim\frac{\mathrm{C}^{\mathrm{2}}_{\mathbf{A}}}{n}\mathbb{E}^{ \mathsf{b}}[\|\theta^{\mathsf{b}}_{i-1}-\theta_{i-1}\|^{2}]+\frac{\mathrm{C} ^{\mathrm{2}}_{\mathbf{A}}}{n}\|\theta_{i-1}-\theta^{\star}\|^{2}+\frac{\mathrm{C} ^{\mathrm{2}}_{\mathbf{A}}}{n}\sum_{k=i+1}^{2n}\mathbb{E}^{\mathsf{b}}[\| \theta^{\mathsf{b}}_{k-1}-\theta^{(\mathsf{b},i)}_{k-1}\|^{2}]\] \[\lesssim\frac{\mathrm{C}^{\mathrm{2}}_{\mathbf{A}}}{n}\mathbb{E}^{ \mathsf{b}}[\|\theta^{\mathsf{b}}_{i-1}-\theta_{i-1}\|^{2}]+\frac{\mathrm{C} ^{\mathrm{2}}_{\mathbf{A}}}{n}\|\theta_{i-1}-\theta^{\star}\|^{2}+\frac{\mathrm{C} ^{\mathrm{2}}_{\mathbf{A}}}{n}\sum_{k=i+1}^{2n}\mathbb{E}^{\mathsf{b}}[\| \Gamma^{\mathsf{b}}_{i+1:k-1}\|^{2}\|\theta^{\mathsf{b}}_{i}-\theta^{(\mathsf{b},i)}_{i}\|^{2}]\,.\]

[MISSING_PAGE_EMPTY:37]

Similarly, with the definition of \(D^{\text{b}}_{5}\) in (48), we write

\[D^{\text{b}}_{5}-D^{(\text{b},i)}_{5} =\frac{1}{\sqrt{n}}\sum_{k=i+1}^{2n}(\mathbf{A}_{k}-\bar{\mathbf{A} })(\theta^{\text{b}}_{k-1}-\theta^{(\text{b},i)}_{k-1})=\frac{1}{\sqrt{n}} \left\{\sum_{k=i+1}^{2n}(\mathbf{A}_{k}-\bar{\mathbf{A}})\Gamma^{\text{b}}_{i+1 :k-1}\right\}(\theta^{\text{b}}_{i}-\theta^{(\text{b},i)}_{i})\] \[=\underbrace{\frac{1}{\sqrt{n}}\left\{\sum_{k=i+1}^{2n}(\mathbf{A }_{k}-\bar{\mathbf{A}})\Gamma_{i+1:k-1}\right\}(\theta^{\text{b}}_{i}-\theta^ {(\text{b},i)}_{i})}_{T_{2}}\] \[\qquad\qquad+\underbrace{\frac{1}{\sqrt{n}}\left\{\sum_{k=i+1}^ {2n}(\mathbf{A}_{k}-\bar{\mathbf{A}})(\Gamma^{\text{b}}_{i+1:k-1}-\Gamma_{i+1 :k-1})\right\}(\theta^{\text{b}}_{i}-\theta^{(\text{b},i)}_{i})}_{T_{3}}\;.\]

Now we bound the terms \(T_{2}\) and \(T_{3}\) separately. Indeed, for the term \(T_{2}\) we get, applying the definition of the set \(\Omega_{4}\), that

\[\mathbb{E}^{\text{b}}[\|T_{2}\|^{2}] \lesssim\frac{1}{n}\left(\frac{\mathrm{C}^{2}_{\mathbf{A}}\log n }{a\alpha_{i}}+\mathrm{C}^{2}_{\mathbf{A}}\log^{2}n\right)\mathbb{E}^{\text{b }}[\|\theta^{\text{b}}_{i}-\theta^{(\text{b},i)}_{i}\|^{2}]\] \[\lesssim\frac{\mathrm{C}^{2}_{\mathbf{A}}\log n}{na\alpha_{i}} \mathbb{E}^{\text{b}}[\|\theta^{\text{b}}_{i}-\theta^{(\text{b},i)}_{i}\|^{2 }]\;.\]

In the above bounds we have used that \(\alpha_{\ell}\leq\frac{1}{a\log n}\). For the term \(T_{3}\) we get, applying Lemma 10, that for any vector \(v\in\mathbb{R}^{d}\),

\[\sum_{k=i+1}^{2n}(\mathbf{A}_{k}-\bar{\mathbf{A}})(\Gamma^{\text {b}}_{i+1:k-1}-\Gamma_{i+1:k-1})v =\sum_{k=i+1}^{2n}\sum_{\ell=i+1}^{k-1}(\mathbf{A}_{k}-\bar{ \mathbf{A}})\Gamma_{\ell+1:k-1}\alpha_{\ell}(w_{\ell}-1)\mathbf{A}_{\ell} \Gamma^{\text{b}}_{i+1:\ell-1}\;v\] \[=\sum_{\ell=i+1}^{2n-1}\alpha_{\ell}(w_{\ell}-1)\bigg{\{}\sum_{k= \ell+1}^{2n}(\mathbf{A}_{k}-\bar{\mathbf{A}})\Gamma_{\ell+1:k-1}\bigg{\}} \mathbf{A}_{\ell}\Gamma^{\text{b}}_{i+1:\ell-1}\;v\;.\]

From the above representation we obtain, using the definition of the set \(\Omega_{4}\), that

\[\mathbb{E}^{\text{b}}[\|T_{3}\|^{2}] \lesssim\frac{\mathrm{C}^{2}_{\mathbf{A}}}{n}\sum_{\ell=i+1}^{2n- 1}\alpha_{\ell}^{2}\left(\frac{\mathrm{C}^{2}_{\mathbf{A}}\log n}{a\alpha_{ \ell}}+\mathrm{C}^{2}_{\mathbf{A}}\log^{2}n\right)\exp\biggl{\{}-\frac{a}{4} \sum_{j=i+1}^{\ell-1}\alpha_{j}\biggr{\}}\mathbb{E}^{\text{b}}[\|\theta^{\text {b}}_{i}-\theta^{(\text{b},i)}_{i}\|^{2}]\] \[\lesssim\frac{\mathrm{C}^{4}_{\mathbf{A}}\log n}{na^{2}}\;\mathbb{ E}^{\text{b}}[\|\theta^{\text{b}}_{i}-\theta^{(\text{b},i)}_{i}\|^{2}]\;.\]

Combining the above bounds, we obtain that

\[\mathbb{E}^{\text{b}}[\|D^{\text{b}}_{5}-D^{(\text{b},i)}_{5}\|^{2}] \lesssim\frac{\mathrm{C}^{2}_{\mathbf{A}}\log n}{na\alpha_{i}} \left(\frac{1}{\alpha_{i}}+\frac{\mathrm{C}^{2}_{\mathbf{A}}}{a}\right) \mathbb{E}^{\text{b}}[\|\theta^{\text{b}}_{i}-\theta^{(\text{b},i)}_{i}\|^{2}]\] \[\lesssim\frac{\mathrm{C}^{2}_{\mathbf{A}}\log n}{na\alpha_{i}} \mathbb{E}^{\text{b}}[\|\theta^{\text{b}}_{i}-\theta^{(\text{b},i)}_{i}\|^{2}]\;,\]where we have additionally used that \(\alpha_{i}\leq a/\operatorname{C}_{\mathbf{A}}^{2}\). Thus, using the upper bound (63), we obtain that

\[\sum_{i=n+1}^{2n}\{\mathbb{E}^{\mathbb{b}}[\|D_{5}^{\mathbf{b}}-D_{ 5}^{(\mathbf{b},i)}\|^{2}]\}^{1/2}\] \[\qquad\lesssim\sum_{i=n+1}^{2n}\frac{\operatorname{C}_{\mathbf{A }}\sqrt{\log n}}{\sqrt{\alpha_{i}an}}\frac{\alpha_{i}\|\varepsilon\|_{\infty} \operatorname{C}_{\mathbf{A}}^{2}}{a}+\sum_{i=n+1}^{2n}\frac{\operatorname{C} _{\mathbf{A}}\sqrt{\log n}}{\sqrt{\alpha_{i}an}}(\operatorname{C}_{\mathbf{A} }^{2}\lor 1)\sqrt{i}\prod_{j=1}^{i}(1-a\alpha_{j}/4)\|\theta_{0}-\theta^{*}\|\] \[\qquad\qquad\qquad\qquad+\frac{(\operatorname{C}_{\mathbf{A}}^{3 }\lor 1)\sqrt{\log n}}{a^{1/2}a_{2n}^{3/2}}\exp\bigl{\{}-\frac{a}{4}\sum_{j=1} ^{n}\alpha_{j}\bigr{\}}\biggl{\{}\sum_{i=n+1}^{2n}\alpha_{i}\prod_{j=n+1}^{i}( 1-a\alpha_{j}/4)\biggr{\}}\|\theta_{0}-\theta^{*}\|\] \[\qquad\lesssim\frac{\operatorname{C}_{\mathbf{A}}^{3}\|\varepsilon \|_{\infty}}{a^{3/2}}n^{1/4}\sqrt{\log n}+\frac{(\operatorname{C}_{\mathbf{A}}^ {3}\lor 1)n^{3/4}\sqrt{\log n}}{a^{3/2}}\exp\bigl{\{}-\frac{a}{4}\sum_{j=1} ^{n}\alpha_{j}\bigr{\}}\|\theta_{0}-\theta^{*}\|\.\] (68)

Now it remains to combine the bounds outlined above in (64), (66), (67), and (68), and the statement follows.

## Appendix D Proof of stability of random matrix product

### Proof of Proposition 1

The fact that there exists a unique matrix \(Q\), such that the following Lyapunov equation holds:

\[\bar{\mathbf{A}}^{\top}Q+Q\bar{\mathbf{A}}=P\,\] (69)

follows directly from (54, Lemma \(9.1\), p. 140). In order to show the second part of the statement, we note that for any non-zero vector \(x\in\mathbb{R}^{d}\), we have

\[\frac{x^{\top}(\operatorname{I}-\alpha\bar{\mathbf{A}})^{\top}Q (\operatorname{I}-\alpha\bar{\mathbf{A}})x}{x^{\top}Qx} =1-\alpha\frac{x^{\top}(\bar{\mathbf{A}}^{\top}Q+Q\bar{\mathbf{A} })x}{x^{\top}Qx}+\alpha^{2}\frac{x^{\top}\bar{\mathbf{A}}^{\top}Q\bar{ \mathbf{A}}x}{x^{\top}Qx}\] \[=1-\alpha\frac{x^{\top}Px}{x^{\top}Qx}+\alpha^{2}\frac{x^{\top} \bar{\mathbf{A}}^{\top}Q\bar{\mathbf{A}}x}{x^{\top}Qx}\] \[\leq 1-\alpha\frac{\lambda_{\min}(P)}{\|Q\|}+\alpha^{2}\,\frac{\| \bar{\mathbf{A}}\|_{Q}^{2}}{\lambda_{\min}(Q)}\] \[\leq 1-\alpha a\,\]

where we set

\[a=\frac{1}{2}\frac{\lambda_{\min}(P)}{\lambda_{\max}(Q)}\,\]

and used the fact that \(\alpha\leq\alpha_{\infty}\), where \(\alpha_{\infty}\) is defined in (7).

### Proofs for auxiliary results on products of random matrix

In order to bound the moment \(\mathbb{E}[\|\theta_{k}-\theta^{*}\|^{p}]\), we first prove a stability results on the products of random matrices \(\Gamma_{m:k}\) arising in the LSA recursion. Towards this aim we first introduce some notations and definitions. For a matrix \(B\in\mathbb{R}^{d\times d}\) we denote by \((\sigma_{\ell}(B))_{\ell=1}^{d}\) its singular values. For \(q\geq 1\), the Shatten \(q\)-norm of \(B\) is denoted by \(\|B\|_{q}=\{\sum_{\ell=1}^{d}\sigma_{\ell}^{q}(B)\}^{1/q}\). For \(q,p\geq 1\) and a random matrix \(\mathbf{X}\) we write \(\|\mathbf{X}\|_{q,p}=\{\mathbb{E}[\|\mathbf{X}\|_{q}^{p}]\}^{1/p}\). Our proof technique is based on the stability results arising in [29], see also [16].

**Lemma 12** (Proposition 15 in [16]).: _Let \(\{\mathbf{Y}_{\ell}\}_{\ell\in\mathbb{N}}\) be an independent sequence and \(P\) be a positive definite matrix. Assume that for each \(\ell\in\mathbb{N}\) there exist \(m_{\ell}\in(0,1)\) and \(\sigma_{\ell}>0\) such that \(\|\mathbb{E}[\mathbf{Y}_{\ell}]\|_{P}^{2}\leq 1-m_{\ell}\) and \(\|\mathbf{Y}_{\ell}-\mathbb{E}[\mathbf{Y}_{\ell}]\|_{P}\leq\sigma_{\ell}\) almost surely. Define \(\mathbf{Z}_{k}=\prod_{\ell=0}^{k}\mathbf{Y}_{\ell}=\mathbf{Y}_{k}\mathbf{Z}_{k-1}\), for \(k\geq 1\) and starting from \(\mathbf{Z}_{0}\). Then, for any \(2\leq q\leq p\) and \(k\geq 1\),_

\[\|\mathbf{Z}_{k}\|_{p,q}^{2}\leq\kappa_{P}\prod_{\ell=1}^{k}(1-m_{\ell}+(p-1) \sigma_{\ell}^{2})\|P^{1/2}\mathbf{Z}_{0}P^{-1/2}\|_{p,q}^{2}\,\] (70)_where we recall that \(\kappa_{P}=\lambda_{\text{min}}^{-1}(P)\lambda_{\text{max}}(P)\)._

Now we aim to bound \(\Gamma_{m:k}\) defined in (27) using Lemma 12. We identify the latter with \(\prod_{\ell=m}^{k}\mathbf{Y}_{\ell}\), where \(\mathbf{Y}_{\ell}=\mathrm{I}-\alpha_{\ell}\mathbf{A}_{\ell},\ell\geq 1\), and \(\mathbf{Y}_{0}=\mathrm{I}\). Applying the bound (8), we get \(\|\mathbb{E}[\mathbf{Y}_{\ell}]\|_{Q}^{2}=\|\mathrm{I}-\alpha_{\ell}\bar{ \mathbf{A}}\|_{Q}^{2}\leq 1-a\alpha_{\ell}\). Further, assumption A2 implies that almost surely,

\[\|\mathbf{Y}_{\ell}-\mathbb{E}[\mathbf{Y}_{\ell}]\|_{Q}=\alpha_{\ell}\| \mathbf{A}_{\ell}-\bar{\mathbf{A}}\|_{Q}\leq\alpha_{\ell}\sqrt{\kappa_{Q}} \operatorname{C}_{\mathbf{A}}=b_{Q}\alpha_{\ell}\.\]

Therefore, (70) holds with \(m_{\ell}=a\alpha_{\ell}\) and \(\sigma_{\ell}=b_{Q}\alpha_{\ell}\). As \(\|\mathrm{I}\|_{p}=d^{1/p}\), we obtain the following corollary.

**Corollary 3**.: _Assume A1 and A2. Then, for any \(\alpha_{\ell}\in[0,\alpha_{\infty}]\), \(2\leq q\leq p\), and \(1\leq m\leq k\), it holds_

\[\mathbb{E}^{1/q}\left[\|\Gamma_{m:k}\|^{q}\right]\leq\|\Gamma_{m:k}\|_{p,q} \leq\sqrt{\kappa_{Q}}d^{1/p}\prod_{\ell=m}^{k}(1-a\alpha_{\ell}+(p-1)b_{Q}^{2} \alpha_{\ell}^{2})\,\]

_where \(\alpha_{\infty}\) was defined in (7), and \(b_{Q}=\sqrt{\kappa_{Q}}\operatorname{C}_{\mathbf{A}}\)._

**Corollary 4**.: _Assume A1, A2, and A3. Then for any \(2\leq q\leq\log n\), and any \(k\geq n\), \(1\leq m\leq k\), it holds that_

\[\mathbb{E}^{1/q}\left[\|\Gamma_{m:k}\|^{q}\right]\leq\sqrt{\kappa_{Q}} \mathrm{e}\exp\left\{-(a/2)\sum_{\ell=m}^{k}\alpha_{\ell}\right\}\,\] (71)

_where \(\alpha_{\infty}\) is defined in (7). Moreover,_

\[\mathbb{E}^{1/q}\left[\|\Gamma_{m:k}\|^{q}\right]\leq\sqrt{\kappa_{Q}} \mathrm{e}\prod_{\ell=m}^{k}\bigl{(}1-\frac{a\alpha_{\ell}}{4}\bigr{)}\] (72)

Proof.: We first apply the result of Corollary 3. Indeed, for \(k\geq n\), and any \(2\leq q\leq p\), it holds, setting \(b_{Q}=\sqrt{\kappa_{Q}}\operatorname{C}_{\mathbf{A}}\), that

\[\mathbb{E}^{1/q}\left[\|\Gamma_{m:k}\|^{q}\right] \leq\sqrt{\kappa_{Q}}d^{1/p}\prod_{\ell=m}^{k}(1-a\alpha_{\ell}+( p-1)b_{Q}^{2}\alpha_{\ell}^{2})\] \[\leq\sqrt{\kappa_{Q}}d^{1/p}\exp\left\{-a\sum_{\ell=m}^{k}\alpha _{\ell}+(p-1)b_{Q}^{2}\sum_{\ell=m}^{k}\alpha_{\ell}^{2}\right\}\.\]

Note that, setting \(p=\log n\), and provided that \(n\) satisfies (9), we easily obtain that, for \(\ell\geq n/2\),

\[(\log n)b_{Q}^{2}\alpha_{\ell}^{2}\leq a\alpha_{\ell}/2\.\] (73)

Hence, for \(m\geq n/2\), we have

\[\mathbb{E}^{1/q}\left[\|\Gamma_{m:k}\|^{q}\right]\leq\sqrt{\kappa_{Q}} \mathrm{e}\exp\bigl{\{}-\frac{a}{2}\sum_{\ell=m}^{k}\alpha_{\ell}\bigr{\}}\,\]

and the statement follows. Suppose now that \(m<n/2\). In such a case we have, applying (73), that

\[\mathbb{E}^{1/q}\left[\|\Gamma_{m:k}\|^{q}\right] \leq\sqrt{\kappa_{Q}}\mathrm{e}\exp\left\{-a\sum_{\ell=m}^{k} \alpha_{\ell}+(\log n)b_{Q}^{2}\sum_{\ell=m}^{k}\alpha_{\ell}^{2}\right\}\] \[\leq\sqrt{\kappa_{Q}}\mathrm{e}\exp\left\{-a\sum_{\ell=m}^{n} \alpha_{\ell}+(\log n)b_{Q}^{2}\sum_{\ell=m}^{n}\alpha_{\ell}^{2}\right\}\exp \left\{-(a/2)\sum_{\ell=n+1}^{k}\alpha_{\ell}\right\}\,\] (74)

and we need to bound the first term in the product. We first consider \(\alpha_{\ell}=c_{0}\ell^{-1/2}\), and use the inequalities

\[\sum_{\ell=m}^{n}\frac{1}{\ell}\leq\left(1+\int_{m}^{n}\frac{dx}{x}\right) \wedge\left(\int_{m-1}^{n}\frac{dx}{x}\right)=\left(1+\log\frac{n}{m}\right) \wedge\left(\log\frac{n}{m-1}\right)\,\] (75)\[\sum_{\ell=m}^{n}\frac{1}{\sqrt{\ell}}\geq\int_{m}^{n}\frac{dx}{\sqrt{x}}=2(\sqrt{n }-\sqrt{m})\.\] (76)

Thus, it is enough to satisfy the constraint

\[(\log n)b_{Q}^{2}c_{0}^{2}(1+\log n-\log m)\leq ac_{0}(\sqrt{n}-\sqrt{m})\.\]

Since \(m<n/2\), it is enough to ensure that

\[(1+\log n)(\log n)b_{Q}^{2}c_{0}^{2}\leq ac_{0}(\sqrt{n}-\sqrt{n/2})\,\]

or, equivalently,

\[\frac{\sqrt{n}}{(1+\log n)\log n}\geq\frac{c_{0}b_{Q}^{2}}{a(1-1/\sqrt{2})}\,\]

which is granted by A3. Combining the above bounds in (74), we obtain that the lemma's statement (71) holds for the step size \(\alpha_{\ell}=c_{0}/\ell^{1/2}\). Similarly, for \(\alpha_{\ell}=c_{0}/\ell^{\gamma}\) with \(\gamma\in(1/2;1)\), we get for \(m\geq n/2\) that

\[\mathbb{E}^{1/q}\left[\|\Gamma_{m:k}\|^{q}\right]\leq\sqrt{\kappa_{Q}}{\rm e} \exp\bigl{\{}-\frac{a}{2}\sum_{\ell=m}^{k}\alpha_{\ell}\bigr{\}}\,\]

since the relation (73) holds. Similarly, for \(m<n/2\), the desired upper bound would follow from the inequality

\[\sum_{\ell=m}^{n}\frac{1}{\ell^{2\gamma}}\leq\int_{m-1}^{n}\frac{dx}{x^{2 \gamma}}=\frac{(m-1)^{1-2\gamma}-n^{1-2\gamma}}{2\gamma-1}\leq\frac{1}{2 \gamma-1}\,\]

together with an inequality

\[\frac{(\log n)b_{Q}^{2}c_{0}^{2}}{2\gamma-1}\leq(a/2)c_{0}(n^{1-\gamma}-(n/2)^ {1-\gamma})\.\]

The latter inequality can be re-written as

\[\frac{n^{1-\gamma}}{\log n}\geq\frac{2c_{0}b_{Q}^{2}}{a(2\gamma-1)(1-(1/2)^{1 -\gamma}}\,\]

which is also granted by A3. Combining the above inequalities implies that (71) holds for \(\alpha_{\ell}=c_{0}/\ell^{\gamma}\). The bound (72) can be immediately obtained from (71) using the fact that \({\rm e}^{-x}\leq 1-x/2\) for \(x\in[0;1]\). 

**Corollary 5**.: _Under conditions of Corollary 4 it holds with \(\mathbb{P}\) - probability at least \(1-1/n^{2}\) that_

\[\|\Gamma_{m:k}\|\leq\sqrt{\kappa_{Q}}{\rm e}^{2}\exp\left\{-(a/2)\sum_{\ell= m}^{k}\alpha_{\ell}\right\}\,\]

_and_

\[\|\Gamma_{m:k}\|\leq\sqrt{\kappa_{Q}}{\rm e}^{2}\prod_{\ell=m}^{k}\bigl{(}1- \frac{a\alpha_{\ell}}{4}\bigr{)}\]

Proof.: It is sufficient to choose \(q=2\log n\) and use Markov's inequality together with the union bound. 

**Proposition 7**.: _Assume A1, A2, A3 with \(\gamma=1/2\), and A4. Then on the set \(\Omega_{5}\) defined in (47), it holds for any \(n\leq m\leq k\leq 2n\), that_

\[\bigl{\{}\mathbb{E}^{\mathfrak{b}}[\|\Gamma_{m+1:k}^{\mathfrak{b}}\|^{2}] \bigr{\}}^{1/2}\leq\kappa_{Q}^{3/2}{\rm e}^{9/8}\exp\left\{-\frac{a}{4}\sum_{ \ell=m+1}^{k}\alpha_{\ell}\right\}\.\]Proof.: Our proof relies on the auxiliary result of Lemma 13 below together with the blocking technique. Indeed, let us represent

\[k-m=Nh+r\,\]

where \(r<h\) and \(h=h(n)\) is a block size defined in (18). Then we obtain, using the independence of bootstrap weights \(w_{m+1},\ldots,w_{k}\), that

\[\{\mathbb{E}^{\mathsf{b}}[\|\Gamma^{\mathsf{b}}_{m+1:k}\|_{Q}^{2}] \}^{1/2} \leq\sqrt{\kappa_{Q}}\{\mathbb{E}^{\mathsf{b}}[\|\Gamma^{\mathsf{ b}}_{m+1:k}\|_{Q}^{2}]\}^{1/2}\] \[=\sqrt{\kappa_{Q}}\prod_{j=1}^{N}\left\{\mathbb{E}^{\mathsf{b}}[ \|\Gamma^{\mathsf{b}}_{m+1+(j-1)h:m+jh}\|_{Q}^{2}]\right\}^{1/2}\bigl{\{} \mathbb{E}^{\mathsf{b}}[\|\Gamma^{\mathsf{b}}_{m+1+Nh:k}\|_{Q}^{2}]\bigr{\}}^{ 1/2}\] \[\leq\sqrt{\kappa_{Q}}\exp\left\{-\frac{a}{4}\sum_{\ell=m+1}^{k} \alpha_{\ell}\right\}\left\{\mathbb{E}^{\mathsf{b}}[\|\Gamma^{\mathsf{b}}_{m+ 1+Nh:k}\|_{Q}^{2}]\right\}^{1/2}\exp\left\{\frac{a}{4}\sum_{\ell=m+1+Nh:k}^{k }\alpha_{\ell}\right\}\.\]

In the last inequality we applied Lemma 13 to each of the blocks of length \(h\) in the first bound. It remains to upper bound the residual terms. Since the remainder block has length less then \(h\), we have due to (81) (which holds according to A4), that

\[\exp\left\{\frac{a}{4}\sum_{\ell=m+1+Nh:k}^{k}\alpha_{\ell}\right\}\leq\exp \left\{\frac{\alpha_{\infty}a}{4}\right\}\leq\mathrm{e}^{1/8}\,\]

where the last inequality is due to Proposition 1. Next,

\[\bigl{\{}\mathbb{E}^{\mathsf{b}}[\|\Gamma^{\mathsf{b}}_{m+1+Nh:k} \|_{Q}^{2}]\bigr{\}}^{1/2} \leq\kappa_{Q}\prod_{\ell=m+1+Nh:k}^{k}\{\mathbb{E}^{\mathsf{b}}[ \|(\mathrm{I}-\alpha_{\ell}w_{\ell}\mathbf{A}_{\ell})\|^{2}]\}^{1/2}\] \[\leq\kappa_{Q}\prod_{\ell=m+1+Nh:k}^{k}\{\mathbb{E}^{\mathsf{b}}[ (1+\alpha_{\ell}|w_{\ell}|\operatorname{C}_{\mathbf{A}})^{2}]\}^{1/2}\] \[\leq\kappa_{Q}\prod_{\ell=m+1+Nh:k}^{k}\{\mathbb{E}^{\mathsf{b}}[ 1+2\alpha_{\ell}|w_{\ell}|\operatorname{C}_{\mathbf{A}}+\alpha_{\ell}^{2}w_{ \ell}^{2}\operatorname{C}_{\mathbf{A}}^{2}]\}^{1/2}\.\]

Since

\[\mathbb{E}[|w_{\ell}|]\leq\sqrt{\mathbb{E}[w_{\ell^{2}}]}\leq\sqrt{(\mathbb{E }[w_{\ell}])^{2}+\operatorname{Var}w_{\ell}}=\sqrt{2}\,\]

we get from previous bound

\[\bigl{\{}\mathbb{E}^{\mathsf{b}}[\|\Gamma^{\mathsf{b}}_{m+1+Nh:k} \|_{Q}^{2}]\bigr{\}}^{1/2} \leq\kappa_{Q}\prod_{\ell=m+1+Nh:k}^{k}(1+2\sqrt{2}\alpha_{\ell} \operatorname{C}_{\mathbf{A}}+2\alpha_{\ell}^{2}\operatorname{C}_{\mathbf{A}} ^{2})^{1/2}\] \[\leq\kappa_{Q}\exp\left\{\sqrt{2}\operatorname{C}_{\mathbf{A}} \sum_{\ell=m+1+Nh:k}^{k}\alpha_{\ell}\right\}\leq\kappa_{Q}\mathrm{e}^{ \sqrt{2}\operatorname{C}_{\mathbf{A}}\circ b_{h}/\sqrt{n}}\leq\kappa_{Q} \mathrm{e}\,\]

where in the last line we additionally used (19). 

**Lemma 13**.: _Assume A1, A2, A3 with \(\gamma=1/2\), and A4. On the set \(\Omega_{5}\) defined in (47), it holds for \(h=h(n)\) defined in (18) and any \(m\in[n;2n-h]\), that_

\[\bigl{\{}\mathbb{E}^{\mathsf{b}}[\|\Gamma^{\mathsf{b}}_{m+1:m+h}\|_{Q}^{2}] \bigr{\}}^{1/2}\leq\exp\left\{-\frac{a}{4}\sum_{\ell=m+1}^{m+h}\alpha_{\ell} \right\}\.\]

Proof.: Recall that we use the notation \(\mathbb{E}^{\mathsf{b}}[\cdot]=\mathbb{E}[\cdot|\mathcal{Z}^{2n}]\), where \(\mathcal{Z}^{2n}=(Z_{1},\ldots,Z_{2n})\) are the random variables used in the construction of the iterates \(\{\theta_{k}\}_{1\leq k\leq n}\) in (1).

Let \(h\in\mathbb{N}\) be a block length, which value will be determined later, and consider a product

\[\Gamma^{\mathsf{b}}_{m+1:m+h}=\prod_{\ell=m+1}^{m+h}(\mathrm{I}-\alpha_{\ell}w _{\ell}\mathbf{A}_{\ell})\.\] (77)Expanding the product of matrices (77), we obtain

\[\Gamma^{\text{b}}_{m:m+h}=\text{I}-\sum_{\ell=m+1}^{m+h}\alpha_{\ell}\mathbf{A}_{ \ell}-\mathbf{S}+\mathbf{R}=\text{I}-\sum_{\ell=m+1}^{m+h}\alpha_{\ell}\bar{ \mathbf{A}}-\sum_{\ell=m+1}^{m+h}\alpha_{\ell}(\mathbf{A}_{\ell}-\bar{\mathbf{ A}})-\mathbf{S}+\mathbf{R}\,\] (78)

where \(\mathbf{S}=\sum_{\ell=m+1}^{m+h}\alpha_{\ell}(w_{\ell}-1)\mathbf{A}_{\ell}\) is a linear statistics in \(\{w_{\ell}\}_{\ell=m+1}^{m+h}\), and the remainder \(\mathbf{R}\) collects the higher-order terms in the products

\[\mathbf{R}=\sum_{r=2}^{h}(-1)^{r}\sum_{(i_{1},\dots,i_{r})\in\mathbb{I}_{r}^{ \ell}}\prod_{u=1}^{r}\alpha_{i_{u}}w_{i_{u}}\mathbf{A}_{i_{u}}\.\]

with \(\mathbb{I}_{r}^{\ell}=\{(i_{1},\dots,i_{r})\in\{m+1,\dots,m+h\}^{r}\,:\,i_{1} <\dots<i_{r}\}\). We first consider the contracting part in matrix \(Q\)-norm. Indeed, applying (8), we obtain that

\[\|\text{I}-\sum_{\ell=m+1}^{m+h}\alpha_{\ell}\bar{\mathbf{A}}\|_{Q}^{2}\leq 1 -a\sum_{\ell=m+1}^{m+h}\alpha_{\ell}\,\]

provided that \(h\) is set in such a manner that \(\sum_{\ell=m+1}^{m+h}\alpha_{\ell}\leq\alpha_{\infty}\), where \(\alpha_{\infty}\) is defined in (7). Hence, we get from the above inequality that for any \(u\in\mathbb{R}^{d}\), it holds that

\[\|\text{I}-\sum_{\ell=m+1}^{m+h}\alpha_{\ell}\bar{\mathbf{A}}\|_{Q}\leq 1-(a/ 2)\sum_{\ell=m+1}^{m+h}\alpha_{\ell}\.\]

Now we need to estimate the remainders in the representation (78). On the set \(\Omega_{5}\), it holds that

\[\|\sum_{\ell=m+1}^{m+h}\alpha_{\ell}(\mathbf{A}_{\ell}-\bar{\mathbf{A}})\|_{Q }\leq 2\,\mathrm{C}_{\mathbf{A}}\,\sqrt{\kappa_{Q}}\sqrt{\sum_{\ell=m+1}^{m+h} \alpha_{\ell}^{2}}\log(2n^{4})\.\]

Moreover, it is straightforward to check that

\[\mathbb{E}^{\text{b}}[\|\mathbf{S}\|_{Q}^{2}]\leq\mathrm{C}_{\mathbf{A}}^{2} \,\kappa_{Q}\sum_{\ell=m+1}^{m+h}\alpha_{\ell}^{2}\.\]

In order to bound the remainder term \(\mathbf{R}\), we note that

\[\mathbb{E}^{\text{b}}[\|\mathbf{R}\|_{Q}] \leq\sum_{r=2}^{h}\binom{h}{r}\alpha_{m+1}^{r}(2\,\mathrm{C}_{ \mathbf{A}})^{r}\kappa_{Q}^{r/2}\leq\alpha_{m+1}^{2}(2\,\mathrm{C}_{\mathbf{A} })^{2}\kappa_{Q}\sum_{r=0}^{h-2}\binom{h}{r+2}\alpha_{m+1}^{r}(2\,\mathrm{C}_{ \mathbf{A}})^{r}\kappa_{Q}^{r/2}\] \[\leq\frac{\alpha_{m+1}^{2}h^{2}(2\,\mathrm{C}_{\mathbf{A}})^{2} \kappa_{Q}\mathrm{e}}{2}\.\]

To complete the proof it remains to set the parameter \(h\) in such a way that we can guarantee

\[\mathrm{C}_{\mathbf{A}}\,\sqrt{\kappa_{Q}}\sqrt{\sum_{\ell=m+1}^{m+h}\alpha_{ \ell}^{2}}\bigg{(}1+2\log(2n^{4})\bigg{)}+\frac{\alpha_{m+1}^{2}h^{2}\,\mathrm{ C}_{\mathbf{A}}^{2}\,\kappa_{Q}\mathrm{e}}{2}\leq\frac{a}{4}\sum_{\ell=m+1}^{m+h} \alpha_{\ell}\,\] (79)

keeping at the same time the constraint

\[\sum_{\ell=m+1}^{m+h}\alpha_{\ell}\leq\alpha_{\infty}\.\] (80)

Recall that \(\alpha_{\ell}=c_{0}/\sqrt{\ell}\). Thus, using the bounds (75) and (76), we obtain that

\[\frac{a}{4}\sum_{\ell=m+1}^{m+h}\alpha_{\ell}\geq\frac{ac_{0}}{2}(\sqrt{m+h}- \sqrt{m+1})\geq\frac{ac_{0}}{2}(\sqrt{m+h}-\sqrt{m})\,\] (81)\[\sum_{\ell=m+1}^{m+h}\alpha_{\ell}^{2}=\sum_{\ell=m+1}^{m+h}\frac{c_{0}^{2}}{\ell} \leq c_{0}^{2}(\log{(m+h)}-\log{m})\.\] (82)

Hence, taking into account (81) and (82), and \(\frac{1}{m+1}\leq\frac{1}{m}\), the inequality (79) would follow from the bound

\[\mathrm{C}_{\mathbf{A}}\,\sqrt{\kappa_{Q}}\sqrt{\log(m+h)-\log(m)}\bigg{(}1+2 \log(2n^{4})\bigg{)}+\frac{c_{0}h^{2}\,\mathrm{C}_{\mathbf{A}}^{2}\,\kappa_{Q} \mathrm{e}}{2m}\leq\frac{a}{2}(\sqrt{m+h}-\sqrt{m})\.\] (83)

Since \(\log{(1+x)}\leq x\) for \(x\geq 0\) and \(c_{0}\,\mathrm{C}_{\mathbf{A}}^{2}\,\kappa_{Q}\mathrm{e}\leq 1\), the latter inequality is satisfied if

\[\mathrm{C}_{\mathbf{A}}\,\sqrt{\kappa_{Q}}\frac{\sqrt{h}}{\sqrt{m}}\bigg{(}1+ 2\log(2n^{4})\bigg{)}+\frac{h^{2}}{2m}\leq\frac{a}{2}(\sqrt{m+h}-\sqrt{m})\.\]

Now we use one more lower bound

\[\sqrt{m+h}-\sqrt{m}=\sqrt{m}(\sqrt{1+h/m}-1)\geq\frac{\sqrt{m}(\sqrt{2}-1)h}{ m}=\frac{(\sqrt{2}-1)h}{\sqrt{m}}\,\]

which follows from an elementary inequality \(\sqrt{1+x}\geq 1+(\sqrt{2}-1)x\), valid for \(0\leq x\leq 1\). Hence, (83) would from the inequality

\[\mathrm{C}_{\mathbf{A}}\,\sqrt{\kappa_{Q}}\frac{\sqrt{h}}{\sqrt{m}}\bigg{(}1+ 2\log(2n^{4})\bigg{)}+\frac{h^{2}}{2m}\leq\frac{a(\sqrt{2}-1)h}{2\sqrt{m}}\.\] (84)

Setting \(h\) is such a manner that

\[\frac{h}{\sqrt{m}}\leq\frac{a(\sqrt{2}-1)}{2}\,\]

inequality (84) would follow from

\[\mathrm{C}_{\mathbf{A}}\,\sqrt{\kappa_{Q}}\frac{\sqrt{h}}{\sqrt{m}}\bigg{(}1+ 2\log(2n^{4})\bigg{)}\leq\frac{a(\sqrt{2}-1)h}{4\sqrt{m}}\.\]

The latter inequality is satisfied, if the block size \(h\) satisfies

\[h\geq\bigg{(}\frac{4\,\mathrm{C}_{\mathbf{A}}\,\kappa_{Q}^{1/2}}{(\sqrt{2}-1) a}\bigg{)}^{2}(1+2\log{(2n^{4})})^{2}\.\]

Thus, setting \(h(n)\) as in (18), all previous inequalities will be fulfilled, provided that

\[\begin{cases}\frac{h(n)}{\sqrt{n}}&\leq\frac{a(\sqrt{2}-1)}{2}\\ \frac{c_{0}h(n)}{\sqrt{n}}&\leq\alpha_{\infty}\.\end{cases}\]

Here last inequality follows from (80) and the following simple bounds, where we use that \(m\geq n\) and \(\sqrt{1+x}\leq 1+x/2\):

\[\sum_{\ell=m+1}^{m+h}\alpha_{\ell}\leq\sum_{\ell=n+1}^{n+h}\alpha_{\ell}=c_{0} \sum_{\ell=n+1}^{n+h}\frac{1}{\sqrt{\ell}}\leq c_{0}\int_{n}^{n+h}\frac{dx}{ \sqrt{x}}=2c_{0}(\sqrt{n+h}-\sqrt{n})\leq\frac{c_{0}h}{\sqrt{n}}\.\]

Now (78) implies that

\[\big{\{}\mathbb{E}^{\mathrm{b}}[\|\Gamma_{m+1:m+h}^{\mathrm{b}}\|_{Q}^{2}] \big{\}}^{1/2}\leq 1-(a/4)\sum_{\ell=m+1}^{m+h}\alpha_{\ell}\,\]

and the statement follows from an elementary inequality \(1+x\leq\mathrm{e}^{x}\). 
Applications to the TD learning

Recall that the temporal difference learning algorithm in the LSA's setting can be written as

\[\theta_{k}=\theta_{k-1}-\alpha_{k}(\mathbf{A}_{k}\theta_{k-1}-\mathbf{b}_{k})\,\] (85)

where \(\mathbf{A}_{k}\) and \(\mathbf{b}_{k}\) are given by

\[\mathbf{A}_{k} =\varphi(s_{k})\{\varphi(s_{k})-\gamma\varphi(s^{\prime}_{k})\}^{ \top}\,\] (86) \[\mathbf{b}_{k} =\varphi(s_{k})r(s_{k},a_{k})\.\]

Recall that our aim is to estimate the agent's _value function_

\[V^{\pi}(s)=\mathbb{E}[\sum_{k=0}^{\infty}\gamma^{k}r(s_{k},a_{k})|s_{0}=s]\,\]

where \(a_{k}\sim\pi(\cdot|s_{k})\), and \(s_{k+1}\sim\mathrm{P}(\cdot|s_{k},a_{k})\), for any \(k\in\mathbb{N}\). We define the transition kernel under policy \(\pi\)

\[\mathrm{P}_{\pi}(B|s)=\int_{\mathcal{A}}\mathrm{P}(B|s,a)\pi(\mathrm{d}a|s)\,\] (87)

which corresponds to the \(1\)-step transition probability from state \(s\) to a set \(B\in\mathcal{B}(\mathcal{S})\). We denote by \(\mu\) the invariant distribution over the state space \(\mathcal{S}\) induced by the transition kernel \(\mathrm{P}_{\pi}(\cdot|s)\) in (87). In this case the TD learning updates (85) correspond to the approximate solution of the deterministic system \(\bar{\mathbf{A}}\theta^{\star}=\bar{\mathbf{b}}\), where we have set, respectively,

\[\bar{\mathbf{A}} =\mathbb{E}_{s\sim\mu,s^{\prime}\sim\mathrm{P}_{\pi}(\cdot|s)}[ \varphi(s)\{\varphi(s)-\gamma\varphi(s^{\prime})\}^{\top}]\] (88) \[\bar{\mathbf{b}} =\mathbb{E}_{s\sim\mu,a\sim\pi(\cdot|s)}[\varphi(s)r(s,a)]\.\]

### Proof of Proposition 2

We first need to check that the matrix \(\bar{\mathbf{A}}+\bar{\mathbf{A}}^{\top}\), where \(\bar{\mathbf{A}}\) is defined in (88), is positive-definite. In order to show this fact we closely follow the exposition of [61, Lemma 18] and [51, Lemma 5]. Define a random matrix \(\mathbf{A}\) as an independent copy of \(\mathbf{A}_{k}\) from (86), that is,

\[\mathbf{A}=\varphi(s)\{\varphi(s)-\gamma\varphi(s^{\prime})\}^{\top}\,\]

where \(s\sim\mu\), and \(s^{\prime}\sim\mathrm{P}_{\pi}(\cdot|s)\). With the definition of \(\mathbf{A}\), we get that

\[\mathbf{A}+\mathbf{A}^{\top} =\varphi(s)\{\varphi(s)-\gamma\varphi(s^{\prime})\}^{\top}+\{ \varphi(s)-\gamma\varphi(s^{\prime})\}\varphi(s)^{\top}\] \[=2\varphi(s)\varphi(s)^{\top}-\gamma\{\varphi(s)\varphi(s^{ \prime})^{\top}+\varphi(s^{\prime})\varphi(s)^{\top}\}\] \[\succeq(2-\gamma)\varphi(s)\varphi(s)^{\top}-\gamma\varphi(s^{ \prime})\varphi(s^{\prime})^{\top}\,\]

where we used an elementary inequality \(uv^{\top}+vu^{\top}\preceq(uu^{\top}+vv^{\top})\) valid for any \(u,v\in\mathbb{R}^{d}\). Hence, with the definition of \(\Sigma_{\varphi}\) in (22), we get

\[\bar{\mathbf{A}}+\bar{\mathbf{A}}^{\top}=\mathbb{E}[\mathbf{A}+\mathbf{A}^{ \top}]\succeq 2(1-\gamma)\Sigma_{\varphi}\.\] (89)

Hence, \(\bar{\mathbf{A}}+\bar{\mathbf{A}}^{\top}\) is positive-definite, and we can set \(P=\bar{\mathbf{A}}+\bar{\mathbf{A}}^{\top}\) in the right-hand side of the Lyapunov equation (69). Obviously, \(Q=\mathrm{I}\) is a solution to the corresponding Lyapunov equation

\[\bar{\mathbf{A}}^{\top}Q+Q\bar{\mathbf{A}}=\bar{\mathbf{A}}+\bar{\mathbf{A}}^ {\top}\.\]

Moreover, applying [61, Lemma 18], we obtain

\[\bar{\mathbf{A}}^{\top}\bar{\mathbf{A}}\preceq\mathbb{E}[\mathbf{A}^{\top} \mathbf{A}]\preceq(1+\gamma)^{2}\Sigma_{\varphi}\.\] (90)

Hence, we get for \(\alpha\leq(1-\gamma)/(1+\gamma)^{2}\), and applying (89) and (90), that

\[(\mathrm{I}-\alpha\bar{\mathbf{A}})^{\top}(\mathrm{I}-\alpha\bar{ \mathbf{A}}) =\mathrm{I}-\alpha(\bar{\mathbf{A}}^{\top}+\bar{\mathbf{A}})+ \alpha^{2}\bar{\mathbf{A}}^{\top}\bar{\mathbf{A}}\] \[\preceq\mathrm{I}-2\alpha(1-\gamma)\Sigma_{\varphi}+\alpha^{2}(1 +\gamma)^{2}\Sigma_{\varphi}\] \[\preceq\mathrm{I}-\alpha(1-\gamma)\Sigma_{\varphi}\] \[\preceq(1-\alpha(1-\gamma)\lambda_{\min}(\Sigma_{\varphi})) \mathrm{I}\.\]

Hence, the bound (8) holds with \(a=(1-\gamma)\lambda_{\min}(\Sigma_{\varphi})\) and \(\alpha_{\infty}=(1-\gamma)/(1+\gamma)^{2}\).

Experimental details for the TD learning

Here we provide some details on numerical experiments. Code to run experiments is provided in https://github.com/svsamsonov/BootstrapLSA. For the considered Garnet problem we choose the policy \(\pi\) in the following way. For any \(a\in\mathcal{A}\), we set

\[\pi(a|s)=\frac{U_{i}^{(s)}}{\sum_{i=1}^{|\mathcal{A}|}U_{i}^{(s)}}\,\]

where the \(U_{i}^{(s)}\) are independent random variables following uniform distribution \(\mathcal{U}[0,1]\). Here we assume that each action \(a\in\mathcal{A}\) can be selected at any state \(s\in\{1,\ldots,N_{s}\}\). We generate an instance of Garnet problem with mentioned parameters, and find analytically the true parameter \(\theta^{\star}\). In order to estimate the supremum

\[\Delta_{n}:=\sup_{x\in\mathbb{R}}\bigl{|}\mathbb{P}(\sqrt{n}\|\bar{\theta}_{n }-\theta^{\star}\|\leq x)-\mathbb{P}(\|\Sigma_{\infty}^{1/2}\eta\|\leq x) \bigr{|}\,\]

\(\eta\sim\mathcal{N}(0,\mathrm{I}_{N_{s}})\), and show that this supremum scales as \(n^{-1/4}\) when \(\gamma=1/2\) and admits slower decay for other powers of \(\gamma\). We first approximate true probability \(\mathbb{P}(\|\Sigma_{\infty}^{1/2}\eta\|\leq x)\) by the corresponding empirical probabilities based on sample of size \(M\gg n\). We fix \(M=5\cdot 10^{7}\). We choose trajectory lengths

\[n\in\{1600,3200,6400,12800,25600,51200,102400,204800,409600,819200,1638400\}\,\]

fix the length of burn-in period \(n_{0}=102400\), and generate \(N=6553600\) independent trajectories starting in the fixed point \(\theta_{0}\in\mathbb{R}^{N_{s}}\). We set the learning rate schedule as \(\alpha_{k}=c_{0}/k^{\gamma}\) and try different values \(\gamma\in\{0.5,0.65,0.7\}\), and \(c_{0}=4.0\). Unfortunately, even the chosen order of trajectory length \(n\) seems to be insufficient in order to significantly distinguish, for example, between \(\gamma=0.5\) and \(\gamma=0.65\). However, learning rate schedule with faster decay performs worse in terms of \(\Delta_{n}\). Note that the current experiment is already rather computationally intense for artificial problem and takes about 12 hours of compute on a Core i9 - 10920x processor with 12 cores with 3.7 GHz.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Main results are, respectively, the ones of Theorem 2 and Theorem 3, their statements are complete and supported by the proofs in the Appendix section. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations of our setting related to the LSA problem, and not more general non-linear stochastic optimisation problems. We highlight the potential generalizations of Theorem 2 to the non-linear setting and discuss why generalizing Theorem 3 might be more challenging. We also discuss the limitation related to i.i.d. observations. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: All our theoretical results are provided with references to assumptions, that are stated in Section 3 and Section 4. All results are given with proofs, that are correctly referenced for each theorem and corollary. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Numerical results are stated with a complete description of the environments that are used, as well as the precise sets of hyperparameters that we used. The code (in Python) is provided as supplementary with the paper, making it easy for one to reproduce our numerical experiments. At the same time, tracing the second-order terms in the normal approximation is computationally involved and can take sufficiently large amount of time. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).

* We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
* **Open access to data and code*
* Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: All code is open source, link to a github repository is included. Guidelines:
* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
* **Experimental Setting/Details*
* Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The algorithm used in the numerical experiments are exactly the algorithms described in the paper. The Garnet environements are given with the parameters used for generation, and with reference to the original problem. Guidelines:
* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
* **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Unfortunately, error bars for computing the second-order terms in normal approximation are quite computationally intense, moreover, tracing the terms of order \(n^{1/4}\) requires quick increase of trajectory length \(n\).

Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: All necessary information to reproduce experiments is provided in Appendix F. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: This paper is of purely theoretical nature, and the proposed methods do not deal with sensitive attributes that could induce unfairness or privacy issues. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?Answer: [NA] Justification: This paper is of purely theoretical nature. We do not foresee any societal harm from the proof of non-asymptotic bootstrap validity and normal approximation bounds in Kolmogorov distance. Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Not applicable. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: Not applicable: no existing assets are used. Guidelines:

* The answer NA means that the paper does not use existing assets.

* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Not applicable: paper does not release new assets. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Not applicable: paper does not involve crowdsourcing nor research on human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA]Justification: Not applicable: paper does not involve crowdsourcing nor research on human subjects.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.