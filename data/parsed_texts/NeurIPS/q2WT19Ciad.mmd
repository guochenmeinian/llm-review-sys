# Benchmarking Generative Models on Computational Thinking Tests in Elementary Visual Programming

 Victor-Alexandru Padurean

MPI-SWS

vpadurea@mpi-sws.org &Adish Singla

MPI-SWS

adishs@mpi-sws.org

###### Abstract

Generative models have demonstrated human-level proficiency in various benchmarks across domains like programming, natural sciences, and general knowledge. Despite these promising results on competitive benchmarks, they still struggle with seemingly simple problem-solving tasks typically carried out by elementary-level students. How do state-of-the-art models perform on standardized programming-related tests designed to assess computational thinking and problem-solving skills at schools? In this paper, we curate a novel benchmark involving computational thinking tests grounded in elementary visual programming domains. Our initial results show that state-of-the-art models like GPT-4o and Llama3 barely match the performance of an average school student. To further boost the performance of these models, we fine-tune them using a novel synthetic data generation methodology. The key idea is to develop a comprehensive dataset using symbolic methods that capture different skill levels, ranging from recognition of visual elements to multi-choice quizzes to synthesis-style tasks. We showcase how various aspects of symbolic information in synthetic data help improve fine-tuned models' performance. We will release the full implementation and datasets to facilitate further research on enhancing computational thinking in generative models.

## 1 Introduction

The recent advances in generative models and large language models (LLMs) have the potential to positively impact a wide variety of domains, such as medicine [1; 2; 3], arts [4; 5], and education [6; 7; 8; 9]. This potential is reflected by their success on a wide range of popular competitive benchmarks assessing their knowledge of natural sciences and day-to-day facts [10; 11; 12; 13; 14] and their skills in programming. For example, GPT-4o [10] is capable of obtaining a high accuracy on two popular programming benchmarks: \(90.2\%\) on HumanEval [15] and \(87.5\%\) on MBPP [16]. Previous studies also showed that GPT-4 [17] is capable of passing assessments in higher education programming courses, achieving course totals greater than \(79\%\)[18].

Despite these promising results, state-of-the-art models struggle with seemingly simple tasks. These models often underperform in tasks requiring mathematical reasoning, planning, and problem-solving [19; 20; 21; 22]. For example, they fail to solve planning tasks involving stacking of colored blocks [23]. Moreover, generative models often face problems with basic algebra and counting [19], or coming up with correct codes in visual programming domains [24], tasks which can successfully be carried out by elementary-level school students. These weaknesses seem to contradict the generative models' impressive performance in complex programming tasks. Based on these observations, we aim to study how generative models tackle programming tasks specifically designed to foster computational thinking and problem-solving skills in elementary-level students. This leads to our main research question: _How do state-of-the-art models perform on standardized programming-related tests designed to assess computational thinking and problem-solving skills at schools?_In this paper, we introduce a novel benchmark for assessing generative models' computational thinking and problem-solving capabilities. We conduct extensive experiments with various models and our results show that state-of-the-art models struggle with the computational thinking tests in our benchmark. Figure 0(a) illustrates how GPT-4o barely matches the performance of an average school student, with Llama3 [11] performing even worse.

We make the following contributions towards improving the models' performance on computational thinking tests: (1) We introduce a novel data generation methodology based on symbolic methods. An important aspect of the generated dataset is that it captures different skill levels, ranging from recognition of visual elements to multi-choice questions to synthesis-style tasks. (2) We fine-tune Llama3-8B and obtain the LlamaCT family of models, the best of which achieves an accuracy on par with GPT-4o (see Figure 0(a)). We further analyze how various aspects of symbolic information in our synthetic dataset help improve the fine-tuned models' performance. (3) We will release the data and implementation to promote further research on enhancing computational thinking in generative models.1

Footnote 1: GitHub repo: https://github.com/machine-teaching-group/neurips2024-benchmark-ct.

## 2 Related Work

We identify two key research themes in the literature: one focuses on the programming capabilities of generative models, and the other focuses on their general reasoning capabilities. To our knowledge, this paper is the first to evaluate numerous generative models on a comprehensive set of computational thinking tests grounded in elementary visual programming. Figure 2 presents a comparison between our work and the most relevant benchmarks in the literature.

**Benchmarks assessing programming capabilities.** Several works benchmark the programming capabilities of models, with popular examples of benchmarks including HumanEval [15], MBPP [16], and APPS [33]. For example, HumanEval [15] focuses on Python code generation but lacks mul

Figure 1: **(a)** shows the performance of school students compared to various models on a scale of \(0\) to \(100\). We break down the ACE [25] test into its constituent parts: _Analyzing_ (ACE[01-07]), _Evaluating_ (ACE[08-14]), and _Creating_ (ACE[15-21]). **(b)** shows Maze16 from _Hour of Code:Maze Challenge_ (HoC) [26; 27], an example of a solution synthesis task. **(c)** shows a _Creating_ multi-choice question task from the ACE test. **(d)** shows a multi-choice question task from the CT-Test[28; 29].

timodal elements and human comparisons, as shown in Figure 2. A more recent benchmark, MMCode [30], includes visual information in traditional coding tasks to assess multimodal model capabilities. However, this benchmark does not include comparisons between model and human performance, nor does it explore potential improvements in model performance through fine-tuning or other methods. Besides program generation, other benchmarks handle code completion, translation, summarization, debugging or explanation generation [34; 35; 36; 37], thus analyzing numerous programming-related tasks. However, these works typically focus on generating code or explanations and do not evaluate the core computational thinking and problem-solving skills of models. In contrast, our paper seeks to provide a deeper understanding of these capabilities. Additionally, we train models on our dataset and compare their performance to human counterparts, addressing gaps in previous studies.

**Benchmarks assessing reasoning capabilities.** For general reasoning, benchmarks like MathVista [31] and MMMU [32] assess models on tasks involving multiple-choice and free-form questions, with a focus on multimodal data. These, along with other benchmarks, evaluate reasoning in fields such as mathematics and the natural sciences [14; 20; 32; 31], planning [22; 23; 38], and causal reasoning [39; 40]. Our benchmark goes beyond these by including a variety of tasks that assess computational thinking through visual programming. This relatively underexplored area can offer intriguing insights into the reasoning capabilities of generative models. Previous efforts [24] address visual tasks without the multimodal component and do not include human comparisons. In contrast, our benchmark integrates multimodal tasks combining both programming and visual reasoning, while also comparing models directly against human performance, offering a more comprehensive evaluation of reasoning abilities in generative models.

## 3 Computational Thinking Tests in Elementary Visual Programming

This section first provides a background on visual programming, and then introduces the sources we use for curating our benchmark and as the basis for our synthetic dataset generation methodology.

### Preliminaries and Definitions for Elementary Visual Programming

**The space of grids.** A visual grid, denoted as \(\mathsf{G}\), includes an avatar with an initial position (row, column) and orientation (north, east, south, west), alongside free cells, wall cells, and a goal or multiple markers. The avatar is required to reach the goal or interact with the markers. The resulting grid space includes visual grids based on HoC by Code.org [26; 27], such as the grids in Figures 0(b) and 0(c), and Karel [41], such as the grid in Figure 0(d).

**The space of codes.** The set of valid codes \(\mathbb{C}\) is defined via a domain-specific language (DSL). We adopt DSLs previously used in literature for visual programming [42; 43; 44]. A code \(\mathsf{C}\in\mathbb{C}\) is characterized by its size \(\mathsf{C}_{\text{size}}\), utilized constructs \(\mathsf{C}_{\text{blocks}}\), and programming concepts exercised in terms of its nesting structure \(\mathsf{C}_{\text{sketch}}\). For example, the code in Figure 0(c) uses \(\mathsf{C}_{\text{size}}=5\)

Figure 2: Comparison of our work with related benchmarks. The first column shows the name of the benchmark and the work it was introduced in. “Domain” specifies the domain for which the benchmark was designed, and “Evaluation tasks” outlines the tasks involved. “Multimodal” indicates if the benchmark includes both visual and textual data. “Benchmark evaluation size” shows the number of samples in each benchmark. “Trained model” notes whether any model is trained in the work, and “Human comparison” indicates if model performance was compared to that of humans.

blocks, with constructs \(\mathsf{C}_{\mathsf{blocks}}=\{\mathsf{move},\mathsf{turnRight},\mathsf{RepeatUntil}, \mathsf{IfELse}\}\), and is structured as \(\mathsf{C}_{\mathsf{sketch}}=\mathsf{RepeatUntil}\{\mathsf{IFELSE}\}\). Executing a code on a grid generates a sequence of avatar locations, referred to as trace, along with a sequence of basic actions executed i.e., constructs from \(\{\mathsf{move},\mathsf{turnLeft},\mathsf{turnRight}\}\). A code is considered to solve a grid if it successfully navigates the avatar to the goal or interacts correctly with the markers (e.g., collects them when intended).

**Solution synthesis tasks.** A solution synthesis task is defined by the following elements: a grid \(\mathsf{G}\), an allowed set of constructs called \(\mathsf{Store}\), and a maximum code size \(\mathsf{maxSize}\). The objective is to write a solution code \(\mathsf{C}\) that successfully solves \(\mathsf{G}\) while respecting \(\mathsf{C}_{\mathsf{blocks}}\subseteq\mathsf{Store}\) and \(\mathsf{C}_{\mathsf{size}}\leq\mathsf{maxSize}\). Figure 0(b) exemplifies a solution synthesis task, where a solution code \(\mathsf{C}\) should solve \(\mathsf{G}\), have \(\mathsf{C}_{\mathsf{blocks}}\subseteq\{\mathsf{move},\mathsf{turnRight}, \mathsf{turnLeft},\mathsf{RepeatUntil},\mathsf{IFELSE}\}\) and \(\mathsf{C}_{\mathsf{size}}\leq 5\).

**Multi-choice question tasks.** A multi-choice question (MCQ) task is defined by the following elements: a text description, a set of grids or codes, one correct option, and three distractor options. The objective is to choose the correct option out of four options. For example, Figures 0(c) and 0(d) have a text description inside the gray area, a given grid and a given code, and four options. The correct option for Figure 0(c) is Option A, and the correct option for Figures 0(d) is Option A as well.

### Three Different Computational Thinking Tests

Our benchmark is based on two pedagogically validated computational thinking tests comprising multiple-choice question tasks [25; 28; 29] and a popular curriculum comprising code-writing tasks [26]. Henceforth, we refer to these three as tests, and we will use them throughout the paper to measure the performance of generative models. These tests have been carefully designed by educational experts to assess or teach a diverse set of skills in elementary visual programming within the duration of a typical one-hour school lesson. They are representative of computational thinking in this domain, providing valuable data on student performance, which we can use as a basis to benchmark the performance of generative models. Figure 3 gives an overview of the programming concepts \(\mathsf{C}_{\mathsf{sketch}}\) utilized by tasks in each test. Next, we provide details for each test.

**HoC.** This test includes 20 code-writing tasks from Code.org's popular block-based visual programming lesson _Hour of Code:Maze Challenge_[26]. The tasks mainly cover concepts such as basic actions, \(\mathsf{Repeat}\) and \(\mathsf{RepeatUntil}\) loops, as well as \(\mathsf{IF}\) and \(\mathsf{IFELSE}\) branching (see Figure 3). This curriculum has been used by millions of learners to get acquainted with programming and to assess students' programming background [25; 26; 27].

**ACE.** This test includes 21 multi-choice question tasks from the ACE test, which was designed to evaluate higher cognitive levels of Bloom's taxonomy: _Analyzing_, _Evaluating_, and _Creating_[25; 45; 46]. These tasks were selected from a larger pool to ensure balanced coverage of cognitive levels and programming concepts, being validated using standardized pedagogical tools. Figure 3 categorizes each task by cognitive level and programming concepts covered.

Figure 3: Programming concepts \(\mathsf{C}_{\mathsf{sketch}}\) required for solving tasks in HoC [26], ACE [25], and CT-Test [28; 29]. HoC comprises code-writing tasks. ACE and CT-Test comprise multi-choice question tasks. ACE is further split according to the higher cognitive levels of Bloom’s taxonomy [45; 46].

**CT-Test.** This test is based on CT-Test, one of the earliest and most popular computational thinking tests in block-based visual programming [28; 29]. Out of 28 tasks in the original set, we curate 24 tasks compatible with our definitions and representation. Figure 3 shows the programming concepts covered, with the original task numbering: if its number is not in the table, we have not included the task.

## 4 Synthetic Data Generation to Fine-tune Models for Computational Thinking

In this section, we introduce our novel data generation methodology for computational thinking and problem-solving skills. With the resulting data (see Figure 4), we aim to fine-tune models to increase performance on all three tests. Next, we present our three main methods for generating data.

### Synthetic Data for Solution Synthesis

We first generate data for solution synthesis tasks. Our process will start with generating a dataset of pairs (\(\mathtt{C}\),\(\mathtt{\{G\}}\)), where \(\mathtt{C}\) is a code and \(\mathtt{\{G\}}\) is a set of grids solved by \(\mathtt{C}\). To obtain (\(\mathtt{C}\),\(\mathtt{\{G\}}\)), we employ existing techniques for synthesizing code \(\mathtt{C}\) and grid \(\mathtt{G}\)[42; 43; 44]. We then split the sets into pairs of one solution code and one grid (\(\mathtt{C}\),\(\mathtt{G}\)). We extract \(\mathtt{Store}\) and \(\mathtt{maxSize}\) from code \(\mathtt{C}\). Then, we treat (\(\mathtt{G},\mathtt{Store},\mathtt{maxSize}\)) as input for the task, and keep \(\mathtt{C}\) as target output.

To enhance the fine-tuning process, we aim to train the model to first produce a trace and sequence of basic actions that the avatar should execute to reach the goal, and then to produce the solution code. We refer to the trace and sequence of basic actions as an explanation for the produced answer. This method is grounded in previous research, which has shown that smaller models benefit from richer signals while being fine-tuned, leading to more careful reasoning at inference [47; 48]. However, unlike literature, we cannot rely on more powerful models like GPT-4 to produce these explanations, as state-of-the-art models struggle with computational thinking (see Figure 0(a)). So, we rely on symbolic methods such as executing codes on grids via an emulator to produce correct traces and basic action sequences as explanations.

### Synthetic Data for Multi-choice Programming Questions

We now focus on generating MCQ tasks similar to those in ACE and CT-Test[25; 28; 29]. We generate MCQs starting from the same (\(\mathtt{C}\),\(\mathtt{\{G\}}\)) used for generating solution synthesis tasks, using a template-based approach, with manually written text descriptions for each task type. Next, we present our task types covering all the higher cognitive levels in Bloom's taxonomy - _Analyzing_,

Figure 4: Our synthetically generated training dataset. Subsampling is done only in the case of basics.

Evaluating_, _Creating_[25; 45; 46]. We also augment MCQs with explanations similar to the ones we use for solution synthesis tasks.

_Analyzing._ First, we describe the process of generating _Analyzing_ cognitive level tasks. For this level, we generate three task types: tasks that require selecting a solution code for a given grid, tasks that require indicating which given grids are solved by a given code, and tasks that require reasoning about the trace of a given code on a given grid and selecting the cells visited by the avatar. To offer an overview of our method, we explain the generation process of one task type, namely reasoning about the trace. We start from a pair (C,G) and the text description specific to this type of task. Then, we generate the correct option by executing C on G and selecting random cells visited by the avatar. We generate distractor options by randomly picking free cells that were not visited by the avatar. Note that this task is correct by construction, unlike some more complex task types below that need validation. Finally, we have the task containing text description, C, G, the correct option, and three distractor options.

_Evaluating._ Second, we describe the process of generating _Evaluating_ cognitive level tasks. For this level, we generate four task types: tasks that require identifying bugs, tasks that require requiring bugs, tasks that require evaluating code equivalence with no given grid, and tasks that require evaluating code equivalence given a grid. We explain generation process for the task type that requires repairing bugs. We start from pair (C,G) and corresponding text description. We generate a mutation and apply it to code C to obtain C\({}_{\text{mut}}\)[44; 49]. The correct option is obtained as the reverse mutation that would transform C\({}_{\text{mut}}\) back to C. Distractor options are obtained by generating three other mutations. We validate the task by applying reverse mutation on C\({}_{\text{mut}}\) and checking whether resulting code solves G. We also apply distractor mutations on C\({}_{\text{mut}}\) and make sure that resulting codes do not solve G. Finally, we have the task containing text description, C\({}_{\text{mut}}\), G, the correct option, and three distractor options.

_Creating._ Third, we describe the process of generating _Creating_ cognitive level tasks. For this level, we generate six types of tasks that require reasoning about modifying an incomplete grid such that the given code solves the modified grid. We generate: tasks that require placing the avatar, tasks that require reasoning about the number of possible initial avatar locations, tasks that require placing the goal, tasks that require counting possible goal positions, tasks that require placing walls, and tasks that require counting the minimum number of walls needed. For example, a task similar to Figure 0(c) can be synthesized by starting from a pair (C,G) and the text description. We set the correct option by randomly picking the number of walls to remove from \(\{1,2,3\}\), in this case \(1\). We remove one wall from G, obtaining G\({}_{\text{mut}}\). We generate three distractor options by applying arithmetic operations to the correct option. To validate the task correctness, we check whether C solves any grid obtained via adding all possible combinations of walls less than the correct option. For this specific example, as the correct option for the example is \(1\), we just need to check if the grid will be solved with no added walls. Finally, we have the task containing text description, C, G\({}_{\text{mut}}\), the correct option, and three distractor options.

### Synthetic Data at Fine-grained Skills

We now introduce new kinds of tasks, aimed at improving fine-grained skills fundamental to making the models better understand the domains of our computational thinking tests. The main intuition for using various fine-grained skills is due to inter-task transfer observed during instruction-tuning [50; 51; 52], which can enhance performance on solution synthesis tasks and MCQ tasks. We now give details about generating three kinds of tasks for improving fine-grained skills.

**Basics.** We describe the process of generating tasks aimed at familiarizing models with the fundamental aspects of visual programming. We generate four types of basic tasks: tasks that require locating the avatar in a given grid, tasks that require specifying the new location of the avatar after executing a given basic action on a given grid, and tasks which require specifying the outcome of applying a given condition to a given grid. For example, we generate the input for the task in Figure 4(a) starting from a grid G, a randomly selected condition present in the DSL, and a fixed text description. The target output is obtained as the outcome of applying the condition on G, in this case True as the avatar has a free cell to its right. As the number of obtainable basic tasks is very large, we subsample to 2% of the original size (see Figure 3(a)). We have empirically chosen this percentage, analyzing the performance on a validation segment corresponding to basic tasks.

**Tracing.** Next, we describe the process of generating tasks aimed at enhancing the model's understanding of the interaction between the basic actions, conditions, and grids, crucial for answering the MCQs corresponding to the _Analyzing_ cognitive level in tests. We generate two types of tasks: tasks requiring to produce the trace obtained by applying a sequence of basic actions to a given grid, and tasks requiring to produce the trace of a given code on a given grid. For example, in Figure 4(b), we use a pair (C,G) and a fixed text description as input and treat the trace of C on G as the target output.

**Grid synthesis.** Finally, we describe the generation of tasks aimed at boosting the model's understanding of the role of each grid element and how it can influence the execution of a code, crucial for answering the MCQs corresponding to the _Creating_ cognitive level in tests. We generate five types of tasks: tasks that require placing the avatar, tasks that require placing the goal, tasks that require placing both the avatar and the goal, tasks that require placing walls, and tasks that require designing a full grid. For example, we generate the task in Figure 4(c) starting from a pair (C,G) and a fixed text description, removing the avatar and the goal from G, and giving the incomplete grid and C as input. The target output is the avatar and goal positions from G. Similarly, in Figure 4(d), we keep only C as input, and require as output a natural language description of G. We also include tracing information and sequences of basic actions as explanations during fine-tuning in a style similar to solution synthesis and MCQ tasks.

## 5 Experiments

In this section, we compare performance of open-access models, OpenAI's GPT family of models, and our fine-tuned LlamaCT. We also include school students' performance based on studies in existing literature [25; 29]. We evaluate LlamaCT variants to assess the impact of training on different data segments and the use of explanations. We also provide insights into models' reasoning process.

### Techniques Evaluated

We start with Naive technique, a baseline that generates random tokens for HoC tasks and selects most frequent answer from four options for ACE and CT-Test tasks. Note that because of non-uniform distribution of options, Naive yields better results for ACE and CT-Test than randomly choosing an option. Next, we present techniques based on generative models and performance of school students. Figure 6 shows a summary of our model-based techniques.

**Open-access models.** We select smaller, instruction-tuned models from the Llama family, such as the 7B parameter version of CodeLlama [53] and the 8B parameter version of Llama3 [11], alongside the 7B parameter version of Llava [54]. These are referred to as CodeLlama-7B-Instruct,

Figure 5: Illustrative examples for synthetically generated tasks with target outputs for the fine-grained skills. The tasks have been adapted for readability. For example, in **(d)**, we illustrate the target output visually, but the actual target output is in textual form.

Llama3-8B-Instruct, and Llava1.5-7B, respectively. For Llava1.5-7B, we incorporate both natural language and visual representations of grids to utilize its vision capabilities. All techniques are prompted to use chain-of-thought (CoT) [55].

**GPT family.** This group includes techniques based on GPT-3.5 [56] and GPT-4 [10, 17]. We start with GPT3.5 technique which processes tasks, including grids, only in natural language, as it has no vision capabilities. Similarly, GPT4\({}_{\text{text}}\) is solely based on natural language. Next, for the GPT4\({}_{\text{vis}}\), we input the grids solely as visual representation, while the rest of the task is represented through natural language. GPT4\({}_{\text{vis+text}}\) technique combines textual and visual representations for grids, with the rest of the task in natural language. We also include similar techniques based on the newer GPT-4o [10], namely GPT4O\({}_{\text{text}}\), GPT4O\({}_{\text{vis}}\), and GPT4O\({}_{\text{vis+text}}\). All techniques are prompted to use CoT.2

Footnote 2: Few-shot prompting did not improve results. All results are based on zero-shot CoT prompting.

**Fine-tuned models.** We fine-tune the instruction-tuned 8B parameter version of the Llama3 model using LoRA [58] and obtain the LlamaCT family. LlamaCT:HoC and LlamaCT:HoC\({}_{\text{exp}}\) are fine-tuned using only the generated solution synthesis tasks, LlamaCT:HoC+MCQ and LlamaCT:HoC+MCQ\({}_{\text{exp}}\) are trained using both generated solution synthesis tasks and generated MCQ tasks, and LlamaCT:HoC+MCQ+Aug\({}_{\text{exp}}\) is trained on the full synthetic dataset. LlamaCT:HoC\({}_{\text{exp}}\), LlamaCT:HoC+MCQ\({}_{\text{exp}}\), and LlamaCT:HoC+MCQ+Aug\({}_{\text{exp}}\) are trained on target outputs enriched with explanations. Additionally, LlamaCT:HoC+MCQ+Aug\({}_{\text{exp}}\) simulates an ideal scenario where the correct reasoning process is known at inference time.

**Human students.** We benchmark these models against the performance of students observed in literature, reporting results for one group of students for HoC and ACE [25], and for a different group of students for CT-Test [29]. GradeAll comprises the average performance of students across grades 3-7 for HoC and ACE, and the average performance of students across grades 5-10 for CT-Test. Grade\({}^{\intercal}\)top25 represents the top \(25\%\) of grade 7 students for HoC and ACE, and the top \(25\%\) of grade 7-8 students for CT-Test, showing the performance of the best students.

### Performance on Computational Thinking Tests

We evaluate techniques on HoC, ACE, and CT-Test, introduced in Section 3.2. Figure 7 shows results, with accuracy computed as percentage of correctly answered tasks in one trial out of total tasks per test. We set temperature to 0 and assess over three seeds, reporting average results as mean (stderr).

**Combining language and vision enhances performance.** Providing input in both text and visual modality leads to better results for GPT4O\({}_{\text{vis+text}}\) when compared with GPT4O\({}_{\text{vis}}\) and GPT4O\({}_{\text{text}}\). Similar results hold for GPT4\({}_{\text{vis+text}}\) when compared with GPT4\({}_{\text{vis}}\) and GPT4\({}_{\text{text}}\).

**Symbolic information-based explanations improve outcomes.** Template-based explanations derived from execution information used while training enhance reasoning at inference and boost model

Figure 6: Table summarizing techniques based on generative models, showing the base model and whether the input grid is represented visually or in text (modality). For fine-tuned models (e.g., LlamaCT), the table specifies the data segment used for training and whether models were trained with no explanations, to generate explanations, or to receive explanations during inference.

performance. Specifically, LlamaCT:HoCexp and LlamaCT:HoC+MCQexp, which are trained with explanations, outperform their counterparts LlamaCT:HoC and LlamaCT:HoC+MCQ trained only for generating an answer with no explanation.

**Fine-grained skills make LlamaCT comparable to GPT-4.** We notice an increase of at least \(10\%\) in performance on HoC, ACE, and CT-Test for the model fine-tuned with fine-grained skills data. Fine-tuning with explanations and across the full dataset allows LlamaCT:HoC+MCQ+Augexp to achieve overall results comparable to those of GPT4Ovis+text. This shows that a better understanding of the visual domain is key to better performance on all three tests.

**Reasoning for MCQ tasks is harder than for solution synthesis tasks.** We analyze the reasoning capabilities of three selected models, Llama3-8B-Instruct, GPT4Ovis+text, and LlamaCT:HoC+MCQ+Augexp, through manual annotations of their reasoning process. A model's reasoning is considered correct if it respects grid constraints, correctly maps codes to traces and sequences, and avoids introducing unnecessary details. We follow a strict binary metric, where the reasoning process is marked as correct (i.e., \(1\)) if the entire reasoning process is correct and incorrect (i.e., \(0\)) otherwise. The reasoning processes were reviewed by two independent annotators3. Figure 8 compares accuracy in correctly answered tasks with reasoning correctness averaged across annotators and aggregated over three seeds, for HoC, ACE, and CT-Test tasks. Llama3-8B-Instruct often tries guessing answers without providing any reasoning, while GPT4Ovis+text struggles with grid layouts, sometimes missing walls. LlamaCT:HoC+MCQ+Augexp traces tasks well in HoC but faces challenges with converting sequences to minimal codes and tracing in ACE and CT-Test.

Footnote 3: The annotators obtained a Cohen’s kappa score of \(0.84\), indicating high agreement [59].

**Symbolic information at inference leads to human-level performance.** Including explanations with a correct reasoning process in the input prompts increases performance, bringing it closer to that of school students. However, LlamaCT:HoC+MCQ+Augexp simulates an ideal scenario, as correct explanations are usually not available at inference as input.

Figure 8: Comparison of accuracy in correctly answered tasks and reasoning correctness across domains for representative models in HoC, ACE, and CT-Test tasks, reported as mean (stderr). Reasoning correctness results are based on manual annotations done by two independent annotators.

Figure 7: Results on HoC, ACE, CT-Test, and overall performance.

**Human students are better at solution synthesis.** Figure 8(a) showcases that state-of-the-art and fine-tuned models have slightly better performance than the average grade 3-7 student across three analyzed levels of Bloom's taxonomy, and that state-of-the-art models struggle with solution synthesis. Figure 8(b) shows a deeper analysis of performance on HoC, breaking down the performance per concept. It shows that by fine-tuning, a model's understanding of programming concepts grows similarly to that of an average student. Finally, Figure 8(c) compares models' performance on HoC and ACE tests with that of students from various grades. It reveals that models have not yet reached the problem-solving capabilities of grade 3 students on HoC tasks. Besides spatial reasoning, adhering to constraints such as the required size and constructs is another reason for this weak performance. Interestingly, models can match the performance of grade 7 students on ACE tests, where answer options are available.

## 6 Concluding Discussion

In this paper, we introduced a new benchmark for assessing generative models on computational thinking tests grounded in elementary visual programming. We made a detailed analysis of the performance of open-access models such as Llama3 and the GPT family of models, comparing it to that of school students. To boost performance of Llama3-8B, we fine-tuned it using our novel synthetic generation methodology based on symbolic information. The best fine-tuned model has a performance similar to state-of-the-art models, even though it is much smaller and does not use vision capabilities.

While our analysis gives a deep insight into the computational thinking and problem-solving capabilities of generative models, there are some limitations of our current work and directions to tackle them in future work. First, we assess multi-modal models on our benchmark but do not fine-tune them to improve performance. An interesting direction for future work is fine-tuning multi-modal models for solving computational thinking and problem-solving tasks. Second, one of our techniques naively uses correct explanations provided at inference time to help it reach an answer. An interesting direction for future work is developing techniques where generative models interact with symbolic tools to obtain this kind of information at inference time, possibly via multiple rounds of interaction.

Figure 9: Comparison between the performance of the best techniques and school students (grade 3-7) on a scale of 0 to \(100\). For better visualization and comparison, we present results only for HoC and ACE. **(a)** shows that state-of-the-art and fine-tuned models have a similar performance to an average grade 3-7 student on the ACE, but lag behind for HoC. **(b)** shows that fine-tuning can help models’ problem-solving skills get closer to an average grade 3-7 student for simpler concepts. (RU stands for RepeatUntil). **(c)** shows how every grade is dominating models for HoC. It also shows that state-of-the-art and fine-tuned models are close to the average grade 7 students’ performance on ACE. However, the performance of the best 25% grade 7 students is still far from reach for generative models.

### Acknowledgements

Funded/Co-funded by the European Union (ERC, TOPS, 101039090). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them.

## References

* [1] Karan Singhal et al. Large Language Models Encode Clinical Knowledge. _CoRR_, abs/2212.13138, 2022.
* [2] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day. In _Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks_, 2023.
* [3] Renqian Luo, Liali Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining. _Briefings in Bioinformatics_, 2022.
* [4] Anqi Wang, Zhizhuo Yin, Yulu Hu, Yuanyuan Mao, and Pan Hui. Exploring the Potential of Large Language Models in Artistic Creation: Collaboration and Reflection on Creative Programming. _CoRR_, abs/2402.09750, 2024.
* [5] Tyler Angert, Miroslav Ivan Suzara, Jenny Han, Christopher Lawrence Pondoc, and Hariharan Subramonyam. Spellburst: A Node-based Interface for Exploratory Creative Coding with Natural Language Prompts. In _Proceedings of the Annual Symposium on User Interface Software and Technology (UIST)_, 2023.
* [6] Khan Academy. Khanmigo. https://www.khanmigo.ai/, 2023.
* [7] Manh Hung Nguyen, Sebastian Tschiatschek, and Adish Singla. Large Language Models for In-Context Student Modeling: Synthesizing Student's Behavior in Visual Programming from One-Shot Observation. In _Proceedings of the International Conference on Educational Data Mining (EDM)_, 2023.
* [8] Paul Denny, Sumit Gulwani, Neil T. Heffernan, Tanja Kaser, Steven Moore, Anna N. Rafferty, and Adish Singla. Generative AI for Education (GAIED): Advances, Opportunities, and Challenges. _CoRR_, abs/2402.01580, 2024.
* [9] Tung Phung, Victor-Alexandru Padurean, Anjali Singh, Christopher Brooks, Jose Cambronero, Sumit Gulwani, Adish Singla, and Gustavo Soares. Automating Human Tutor-Style Programming Feedback: Leveraging GPT-4 Tutor Model for Hint Generation and GPT-3.5 Student Model for Hint Validation. In _Proceedings of the Learning Analytics and Knowledge Conference (LAK)_, 2024.
* [10] OpenAI. Hello GPT-4o. https://openai.com/index/hello-gpt-4o/, 2024.
* [11] Meta. Llama 3. https://llama.meta.com/llama3/, 2024.
* [12] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring Massive Multitask Language Understanding. In _International Conference on Learning Representations (ICLR)_, 2021.
* Volume 1_, 2019.
* [14] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. _CoRR_, abs/1803.05457, 2018.

* [15] Mark Chen et al. Evaluating Large Language Models Trained on Code. _CoRR_, abs/2107-03374, 2021.
* [16] Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program Synthesis with Large Language Models. _CoRR_, abs/2108.07732, 2021.
* [17] OpenAI. GPT-4 Technical Report. 2023.
* Volume 1_, 2023.
* [19] Sebastien Bubeck et al. Sparks of Artificial General Intelligence: Early Experiments with GPT-4. _CoRR_, abs/2303.12712, 2023.
* [20] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring Mathematical Problem Solving With the MATH Dataset. In _Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks_, 2021.
* [21] Boshi Wang, Xiang Yue, and Huan Sun. Can ChatGPT Defend the Truth? Automatic Dialetical Evaluation Elicits LLMs' Deficiencies in Reasoning. _CoRR_, abs/2305.13160, 2023.
* [22] Karthik Valmeekam, Matthew Marquez, Alberto Olmo Hernandez, Sarath Sreedharan, and Subbarao Kambhampati. PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change. In _Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks_, 2023.
* A Critical Investigation. In _Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS)_, 2023.
* Volume 2_, 2023.
* [25] Ahana Ghosh, Lina Malva, and Adish Singla. Analyzing-Evaluating-Creating: Assessing Computational Thinking and Problem Solving in Visual Programming Domains. In _Proceedings of the Technical Symposium on Computer Science Education (SIGCSE)_, 2024.
* [26] Code.org. Hour of Code: Classic Maze Challenge. https://studio.code.org/s/hourofcode, 2013.
* [27] Code.org. Code.org: Learn Computer Science. https://code.org/, 2013.
* [28] Marcos Roman Gonzalez. Computational Thinking Test: Design Guidelines and Content Validation. In _Proceedings of the International Conference on Education and New Learning Technologies (EDULEARN)_, 2015.
* [29] Marcos Roman-Gonzalez, Juan-Carlos Perez-Gonzalez, and Carmen Jimenez-Fernandez. Which Cognitive Abilities Underlie Computational Thinking? Criterion Validity of the Computational Thinking Test. _Computers in Human Behavior_, 72, 2017.
* [30] Kaixin Li, Yuchen Tian, Qisheng Hu, Ziyang Luo, and Jing Ma. MMCode: Evaluating Multi-Modal Code Large Language Models with Visually Rich Programming Problems. _CoRR_, abs/2404.09486, 2024.
* [31] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. MathVista: Evaluating Math Reasoning in Visual Contexts with GPT-4V, Bard, and Other Large Multimodal Models. _CoRR_, abs/2310.02255, 2023.
* [32] Xiang Yue et al. MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI. _CoRR_, abs/2311.16502, 2023.

* [33] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. Measuring Coding Challenge Competence with APPS. In _Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks_, 2021.
* [34] Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, and Shayne Longpre. OctoPack: Instruction Tuning Code Large Language Models. _CoRR_, abs/2308.07124, 2023.
* Volume 2_, 2023.
* [36] Runchu Tian, Yining Ye, Yujia Qin, Xin Cong, Yankai Lin, Yinxu Pan, Yesai Wu, Zhiyuan Liu, and Maosong Sun. DebugBench: Evaluating Debugging Capability of Large Language Models. _CoRR_, abs/2401.04621, 2024.
* [37] Tianyang Liu, Canwen Xu, and Julian J. McAuley. RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems. _CoRR_, abs/2306.03091, 2023.
* [38] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents. In _Proceedings of the International Conference on Machine Learning (ICML)_, 2022.
* [39] Zhijing Jin, Yuen Chen, Felix Leeb, Luigi Gresele, Ojasv Kamal, Zhiheng Lyu, Kevin Blin, Fernando Gonzalez Adauto, Max Kleiman-Weiner, Mrinmaya Sachan, and Bernhard Scholkopf. CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language Models. In _Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS)_, 2023.
* [40] Zhijing Jin, Jiarui Liu, Zhiheng LYU, Spencer Poff, Mrinmaya Sachan, Rada Mihalcea, Mona T. Diab, and Bernhard Scholkopf. Can Large Language Models Infer Causation from Correlation? In _International Conference on Learning Representations (ICLR)_, 2024.
* [41] Richard E. Pattis. _Karel the Robot: A Gentle Introduction to the Art of Programming_. John Wiley & Sons, Inc., 1981.
* [42] Victor-Alexandru Padurean, Georgios Tzannetos, and Adish Singla. Neural Task Synthesis for Visual Programming. _Transactions on Machine Learning Research (TMLR)_, 2024.
* [43] Rudy Bunel, Matthew J. Hausknecht, Jacob Devlin, Rishabh Singh, and Pushmeet Kohli. Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis. In _International Conference on Learning Representations (ICLR)_, 2018.
* [44] Umair Z. Ahmed, Maria Christakis, Aleksandr Efremov, Nigel Fernandez, Ahana Ghosh, Abhik Roychoudhury, and Adish Singla. Synthesizing Tasks for Block-based Programming. In _Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS)_, 2020.
* [45] Benjamin S Bloom, Max D Engelhart, Edward J Furst, Walker H Hill, and David R Krathwohl. _Taxonomy of Educational Objectives: The Classification of Educational Goals. Handbook 1: Cognitive Domain_. Longman New York, 1956.
* [46] David R Krathwohl. A Revision of Bloom's Taxonomy: An Overview. _Theory into Practice_, 2002.
* [47] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive Learning from Complex Explanation Traces of GPT-4. _CoRR_, abs/2306.02707, 2023.
* [48] Arindam Mitra et al. Orca 2: Teaching Small Language Models How to Reason. _CoRR_, abs/2311.11045, 2023.

* [49] Ahana Ghosh, Sebastian Tschiatschek, Sam Devlin, and Adish Singla. Adaptive Scaffolding in Block-Based Programming via Synthesizing New Tasks as Pop Quizzes. In _Proceeding of the International Conference on Artificial Intelligence in Education AIED_, 2022.
* [50] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned Language Models are Zero-Shot Learners. In _International Conference on Learning Representations (ICLR)_, 2022.
* Volume 1_, 2022.
* Volume 1_, 2023.
* [53] Baptiste Roziere et al. Code Llama: Open Foundation Models for Code. _CoRR_, abs/2308.12950, 2023.
* [54] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual Instruction Tuning. In _Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS)_, 2023.
* [55] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. In _Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS)_, 2022.
* [56] OpenAI. ChatGPT. https://openai.com/blog/chatgpt, 2023.
* [57] OpenAI. GPT-4V(ision) System Card. https://openai.com/blog/chatgpt, 2023.
* [58] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. In _International Conference on Learning Representations (ICLR)_, 2022.
* 46, 1960.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] See Sections 4 and 5. 2. Did you describe the limitations of your work? [Yes] See Section 6. 3. Did you discuss any potential negative societal impacts of your work? [No] We do not foresee any potential negative societal impacts of this work. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] See the GitHub repository. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? See the supplemental material. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? See Section 5. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See the supplemental material.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? See Section 3 and the supplemental material. 2. Did you mention the license of the assets? See the supplemental material. 3. Did you include any new assets either in the supplemental material or as a URL? See the GitHub repository. 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A] 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]