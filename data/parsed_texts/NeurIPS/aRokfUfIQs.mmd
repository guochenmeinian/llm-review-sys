# Sequential Signal Mixing Aggregation for Message Passing Graph Neural Networks

 Mitchell Keren Taraday

Department of Computer Science

Technion

Haifa, Israel

butovsky.mitchell@gmail.com &Almog David

Department of Computer Science

Technion

Haifa, Israel

almogdavid@gmail.com &Chaim Baskin

School of Electrical and Computer Engineering

Ben-Gurion University of the Negev

Be'er Sheva, Israel

chaimbaskin@bgu.ac.il

Equal contribution.

###### Abstract

Message Passing Graph Neural Networks (MPGNNs) have emerged as the preferred method for modeling complex interactions across diverse graph entities. While the theory of such models is well understood, their aggregation module has not received sufficient attention. Sum-based aggregators have solid theoretical foundations regarding their separation capabilities. However, practitioners often prefer using more complex aggregations and mixtures of diverse aggregations. In this work, we unveil a possible explanation for this gap. We claim that sum-based aggregators fail to "mix" features belonging to distinct neighbors, preventing them from succeeding at downstream tasks. To this end, we introduce Sequential Signal Mixing Aggregation (SSMA), a novel plug-and-play aggregation for MPGNNs. SSMA treats the neighbor features as 2D discrete signals and sequentially convolves them, inherently enhancing the ability to mix features attributed to distinct neighbors. By performing extensive experiments, we show that when combining SSMA with well-established MPGNN architectures, we achieve substantial performance gains across various benchmarks, achieving new state-of-the-art results in many settings. We published our code at https://almogdavid.github.io/SSMA/

## 1 Introduction

Message-passing Graph Neural Networks (MPGNNs) have established themselves as the major workhorses for graph representation learning over the past decade [24]. These models have been proven to be effective in graph-structured problems in a variety of domains, ranging from social networks [18] to natural sciences [14, 23, 4] and having some non-trivial applications in computer vision and natural language processing [26, 33, 45, 28].

Such renowned models of this nature owe their success to their high efficiency, along with good generalization capabilities and simplicity. A typical MPGNN takes graph-structured data containing node and edge features as input. It then iteratively updates node representations by combining their egocentric view with a symmetrized aggregation of their proximate neighbor features.

The key insight regarding the expressive power of such models is their equivalence to the Weisfeller-Lehman (WL) graph isomorphism test [47]. Consequently, past research directions were majorly directed toward developing models that surpass the vanilla WL test by tackling the graph learnability problem from various perspectives, including stronger notions of the WL test [31, 35], spectral graph methods [46, 13, 44] and graph transformers [49, 36].

However, one subtle but often overlooked detail in such expressivity analyses is the existence of a Hash function, which compresses the neighbor features into a fixed-sized representation. Such Hash function need not only be injective but also differentiable and efficient in terms of memory and computation. The seminal DeepSets paper [50] showcased such a sum-based construction for the Hash function. While this construction was very simple and computationally efficient, the theoretical representation size required in this construction is exponential in the node feature dimension. Although the bound on this representation size was improved in later works [17, 1], sum-based aggregations seem to lag behind the aggregators used in practice [11].

In this work, we suggest that a possible explanation for this gap is the inability of sum-based aggregators to "mix" features belonging to distinct neighbors. We formalize the "neighbor-mixing" property and show that sum-based aggregators have limited neighbor-mixing capability. This observation is later verified by conducting an experiment showing that sum-based aggregators struggle with approximating even a very simple function requiring neighbor-mixing.

With this motivation in mind, we propose a new aggregation module that treats the neighbor features as two-dimensional discrete signals and sequentially convolves them - hence coined as Sequential Signal Mixing Aggregation (SSMA). SSMA has a provably polynomial representation size \(m=\mathcal{O}(n^{2}d)\) (where \(n\) is the number of neighbors and \(d\) is feature dimensionality). The theoretical construction underlying SSMA provides a positive answer to a lasting mystery regarding DeepSets [50] - "Can the _DeepSets_ polynomial be **efficiently** generalized to handle vector features?"as depicted in Figure 1.

As later investigated, the convolutional component in SSMA allows it to directly mix features attributed to distinct neighbors, inducing a higher-order notion of neighbor mixing. We then discuss some practical aspects of SSMA. Particularly, we discuss how to implement it in a computationally efficient manner, how to scale it to larger graphs and how to make it easier to optimize.

Finally, we demonstrate that when integrated into a wide range of well established MPGNN architectures, SSMA greatly enhances their performance. We observe significant gains across all benchmarks tested, including the TU datasets [32], open graph benchmark (OGB) [21] datasets, long-range graph benchmarks (LRGB) [16] datasets and the ZINC [19] molecular property prediction dataset achieving state-of-the-art results in many settings.

Contributions.Our contributions may be summarized as follows:

1. We define the notion of "neighbor-mixing" and show that sum-based aggregators have limited neighbor-mixing power. We verify this idea by conducting an experiment on a simple and natural synthetic task.
2. We propose Sequential Signal Mixing Aggregation (SSMA) - an aggregation module of dimension \(m=\mathcal{O}(n^{2}d)\) which treats the neighbor features as discrete signals and sequentially convolves them. The theoretical construction underlying SSMA builds upon the _DeepSets_ polynomial, efficiently extending it to multidimensional features.
3. We introduce a few practices for stabilizing the optimization process of SSMA and show how to scale it to larger graphs.
4. Finally, we conduct extensive experiments showing that enriching prominent MPGNN architectures with SSMA yields large improvements on a variety of benchmarks, achieving state-of-the-art results.

Figure 1: An efficient and provable generalization of the _DeepSets_ polynomial to vector features.

## 2 Preliminaries and related work

Let \(\mathcal{X}\) be some domain. We are interested in representing **multisets** (sets in which repeated elements are allowed) over that domain. We denote multisets by \(\{\{x_{1},...,x_{n}\}\}\) where each \(x_{i}\in\mathcal{X}\), and denote by \(\mathcal{M}_{n}:=(\mathcal{X})^{n}\) the \(n\)-tuple space over \(\mathcal{X}\). We seek a (possibly learnable) permutation invariant mapping \(f:\mathcal{M}_{n}\rightarrow\mathbb{R}^{m}\) separating distinct multisets 2. When combined with a learnable compression network \(g_{\theta}:\mathbb{R}^{m}\rightarrow\mathcal{X}\), their composition \(\gamma=g_{\theta}\circ f\) can be utilized as an aggregation module for MPGNNs over the domain \(\mathcal{X}\).

Footnote 2: meaning that distinct multisets should be mapped to distinct representations, hereby requiring a sufficiently large representation dimension \(m\).

Particularly, we are interested in continuous features, namely the domains \(\mathcal{X}=\mathbb{R}\) and \(\mathcal{X}=\mathbb{R}^{d}\). We consider the symmetry group \(\mathcal{G}=S_{n}\) acting on \(\mathcal{M}_{n}=\mathbb{R}^{n}\) by \([\sigma.\bm{x}]_{i}=\bm{x}_{\sigma^{-1}(i)}\) and on \(\mathcal{M}_{n}=\mathbb{R}^{n\times d}\) by \([\sigma.\bm{\mathcal{X}}]_{ij}=\bm{\mathsf{X}}_{\sigma^{-1}(i)j}\) correspondingly. It is widely agreed that finding a good representation \(f:\mathcal{M}_{n}\rightarrow\mathbb{R}^{m}\) for these domains is crucial for building better aggregation modules \(\gamma\) and has a direct influence on the performance of the model on a variety of downstream tasks [47; 12; 27; 40].

DeepSets [50] was the pioneering work introducing a sum-based aggregator with a provably finite representation size \(m\): \(\gamma(\{x_{1},...,x_{n}\})=\rho(\sum_{k=1}^{n}\phi(x_{k}))\) where \(\phi:\mathbb{R}^{d}\rightarrow\mathbb{R}^{m}\) and \(\rho:\mathbb{R}^{m}\rightarrow\mathbb{R}^{d}\). Their construction consisted of "hand-crafted" moment-based features. Despite being efficient for scalar-based features, the representation size grew exponentially with the node feature dimensionality, \(m\in\mathcal{O}(\binom{n+d}{d})\). This upper bound was later improved to \(\mathcal{O}(n^{2}d)\) and eventually to a tight \(\Theta(nd)\)[17]. While moment-based features served as a powerful tool for achieving theoretical separation, learnable neural features are favored over such hand-crafted features in practice. As was unveiled, neural features can achieve theoretical separation as well, as long as non-polynomial analytic activations are used [1].

Despite their clear theoretical advantages, sum-based aggregators seem to have limited performance in practice [12; 27]. Consequently, many works focused on different species of permutation invariant aggregators. For instance, attention-based aggregators have been proposed to capture the most important signals incoming from the neighborhood [2; 7]. Others suggested using a mixture of symmetric aggregators such as min, max, mean, sum, std as each of these aggregators helps separate different kinds of multisets [47; 41; 12]. Other works focused on aggregations preserving intrinsic properties of the neighborhood data such as variance and fisher-information [38; 30].

Another intriguing type of work deals with the relaxation of the neighbor ordering invariance constraint. Particularly, regularizing recurrent neural network-based aggregations to maintain permutation invariance - either by choosing a random neighbor permutation [20] or by explicit regularization terms [10; 34] has raised some interest.

## 3 On the limited neighbor-mixing of sum-based aggregators

Despite their provable separation power, sum-based aggregators seem to lag behind other aggregators used in practice [11]. We claim that a possible explanation for this phenomenon lies in their inability to "mix" the neighbor's features, in that the mutual effect of perturbing the features of two distinct neighbors on each aggregation output is very small. In practice, many downstream tasks require high "mixing" values as the aggregator should mix information from different distinct neighbors to produce a useful representation for tackiling the downstream task.

**Definition 3.1**.: Let \(\gamma:\mathbb{R}^{n\times d}\rightarrow\mathbb{R}^{d}\) be some aggregation function that is continuously twice differentiable. We define the _neighbor mixing_ of the \(\ell\)-th aggregation output with respect to the neighbor pair \((i,j)\):

\[\text{mix}^{(\ell)}_{i,j}:=\left\|\frac{\partial^{2}}{\partial x_{i}\partial x _{j}}\gamma^{(\ell)}(x_{1},...,x_{n})\right\|_{2}\] (1)

At an intuitive level, sum-based aggregators have small \(\text{mix}^{(\ell)}_{i,j}\) values as the result of the local pooling operation is summed across the neighbors. Namely, without explicitly "mixing" features from distinct neighbors before the summation. Indeed, given \(\gamma(\{\{x_{1},...,x_{n}\}\})=\sum_{k=1}^{n}\phi(x_{k})\) we have:\[\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}\sum_{k=1}^{n}\phi^{(\ell)}(x_{ k})=0\] (2)

Formally, to account for mixing that may occur in any subsequent (global) transformation we have the following proposition:

**Proposition 3.2**.: _Let \(\gamma(\{\{x_{1},...,x_{n}\}\})=\rho\left(\sum_{k=1}^{n}\phi(x_{k})\right)\) where \(\phi:\mathbb{R}^{d}\rightarrow\mathbb{R}^{m}\) is a local operator and \(\rho:\mathbb{R}^{m}\rightarrow\mathbb{R}^{d}\) is a pooling operator that is continuously twice differentiable. Then, we have \(\forall i\neq j\):_

\[\mathsf{mix}_{i,j}^{(\ell)}\leq\left\|J_{\phi}(x_{i})\right\|_{2}\cdot\left\|H _{\rho^{(\ell)}}(\sum_{k=1}^{n}\phi(x_{k}))\right\|_{2}\cdot\left\|J_{\phi}(x _{j})\right\|_{2}\] (3)

_Where \(J_{\phi}(.)\) is the Jacobian matrix of \(\phi\) and \(H_{\rho^{(\ell)}}(.)\) is the Hessian matrix corresponding to \(\ell\)-th output of \(\rho\). Particularly, for typical choices of \(\phi\) and \(\rho\) it follows: \(\mathsf{mix}_{i,j}^{(\ell)}\in\mathcal{O}(\left\|\theta\right\|_{2}^{2})\) where \(\theta\) is the concatenation of the parameters in \(\phi\) and \(\rho\)._

The proof of Proposition 3.2 is given in Appendix A.1.

Motivated by the above observation, we propose a new species of aggregation module which is convolution-based rather than sum-based.

## 4 \(\mathrm{SSMA}\)- Sequential Signal Mixing Aggregation

### Warm-up: _DeepSets_ polynomial from a _convolutional_ point of view

Let \(\overline{\bm{x}}=\{\{\bm{x}_{1},...,\bm{x}_{n}\}\}\) be a scalar multiset. We define its _DeepSets_ polynomial by considering a polynomial of variable \(t\) having the multiset elements as its roots:

\[p_{\overline{\bm{x}}}(t):=\prod_{i=1}^{n}(t-\bm{x}_{i})\] (4)

Its coefficients, we denote by \(e_{k}(\bm{x})\), are permutation invariant functions. Moreover, the \((e_{k}(\bm{x}))_{k=0}^{m}\)s form an ensemble of invariant separators 3.

Footnote 3: If for two multisets we have \(e_{k}(\bm{x})=e_{k}(\bm{y})\) for all \(0\leq k\leq m\), then \(p_{\overline{\bm{x}}}(t)=p_{\overline{\bm{y}}}(t)\) for all \(t\), and therefore each one of the roots of the left-hand side polynomial corresponds to some root of the right-hand side polynomial. An inductive argument shows that this implies that both polynomials have the same roots.

Instead of describing a polynomial by its coefficients, one can represent a polynomial by evaluating it on some fixed set of points. Given a set of \(n+1\) fixed points, the polynomial may be represented by evaluating its value on these points. One can switch from this representation back to the coefficients by solving a system of linear equations, which always has a unique solution. Now, by allowing the evaluation points to be complex, we can choose them as the roots of unity. By doing so, we get the discrete Fourier transform (DFT) of the polynomial coefficients:

\[\bm{\zeta}_{j}(\bm{x})=\sum_{k=0}^{n}e_{k}(\bm{x})\cdot e^{-\frac{2\pi ij}{n+ 1}k}\quad(j=0,...,n)\] (5)

Next, we denote the factors in \(p_{\overline{\bm{x}}}(t)=\prod_{i=1}^{n}p_{i}(t)\) where \(p_{i}(t):=t-\bm{x}_{i}\). The (padded) coefficients of each \(p_{i}(t)\) are then given by the affine transformation:

\[\bm{h}(\bm{x}_{i})=[-\bm{x}_{i},1,0,...,0]\in\mathbb{R}^{n+1}\] (6)

The nice thing about representing a polynomial by evaluating its values at a list of fixed points is that polynomial multiplication becomes _point-wise_. It can be deduced that the coefficients of \(p_{\overline{\bm{x}}}(t)\) can be computed by transforming the coefficients \(\bm{h}(\bm{x}_{i})\) of each \(p_{i}(t)\) to the Fourier domain, performingelementwise multiplication and then transforming back to the coefficients domain. According to the circular convolution theorem, this exactly amounts to sequentially convolving the coefficients \(\bm{h}(\bm{x}_{i})\). We combine the above ideas into the following theorem:

**Theorem 4.1**.: _Scalar multisets \(\overline{\bm{x}}=\{\!\{\bm{x}_{1},...,\bm{x}_{n}\}\}\) can be represented by an invariant and separating map \(f_{conv}\):_

\[f_{conv}(\bm{x})=\mathop{\oplus}\limits_{i=1}^{n}\bm{h}(\bm{x}_{i})\] (7)

_Where \(\bm{h}:\mathbb{R}\rightarrow\mathbb{R}^{m}\) is an affine map, \(\mathop{\oplus}\limits_{i=1}^{n}\) is the circular convolution operator, and the number of separators is \(m=n+1\in\mathcal{O}(n)\)._

Theorem 4.1 simply states that sequential convolution can be utilized to compute the renowned _DeepSets_ polynomial coefficients. While not particularly surprising, Theorem 4.1 shows that the coefficients of the _DeepSets_ polynomial can be efficiently computed and directly utilized as a multiset representation. Moreover, it paves the way for our construction, as seen in the next section.

### Efficient generalization to multidimensional features

"How does the _DeepSets_ polynomial can be **efficiently** extended to handle vector features?"

The key idea underlying our answer to this question is to encode each feature vector as another polynomial, and then to reduce the problem to the scalar case.

Generalized DeepSets Polynomial

We encode each element \(\bm{\mathsf{X}}_{i}\) belonging to the multiset \(\overline{\bm{\mathsf{X}}}=\{\!\{\bm{\mathsf{X}}_{1},...,\bm{\mathsf{X}}_{n}\}\}\) as a polynomial of _another_ variable \(z\):

\[\mathsf{Enc}(\bm{\mathsf{X}}_{i})=\sum_{j=1}^{d}\bm{\mathsf{X}}_{ij}\cdot z^{j-1}\] (8)

Then, we can perform a reduction to the scalar case by replacing each \(\bm{\mathsf{X}}_{i}\) with \(\mathsf{Enc}(\bm{\mathsf{X}}_{i})\):

\[p_{i}(t,z):=t-\mathsf{Enc}(\bm{\mathsf{X}}_{i})=t-\sum_{j=1}^{d}\bm{\mathsf{ X}}_{ij}\cdot z^{j-1}\] (9)

And define the generalized _DeepSets_ polynomial:

\[p_{\overline{\bm{\mathsf{X}}}}(t,z):=\prod_{i=1}^{n}p_{i}(t,z)=\sum_{k,l}e_{k \ell}(\bm{\mathsf{X}})\cdot t^{k}z^{\ell}\] (10)

Where \(e_{k\ell}(\bm{\mathsf{X}})\) is the coefficient of \(t^{k}z^{\ell}\) in \(p_{\overline{\bm{\mathsf{X}}}}(t,z)\). Note \(0\leq k\leq n\) while \(0\leq\ell\leq n(d-1)\).

Opposed to the scalar case, it is not evident why the obtained coefficients \((e_{k\ell}(\bm{\mathsf{X}}))_{k,\ell}\) in the above construction form an ensemble of separators. We prove injectivity by utilizing ideas from ring theory, particularly the notions of unique factorization domains (UFDs) and Gauss's lemma in Appendix A.2.

We can now repeat the steps in Section 4.1 to achieve the actual representation. We compute the coefficient _matrix_ of each \(p_{i}(t,z)\) and sequentially perform two-dimensional circular convolution.

This leads us to an analogous theorem for the \(d\)-dimensional case:

**Theorem 4.2**.: _Vector multisets \(\overline{\bm{\mathsf{X}}}=\{\!\{\bm{\mathsf{X}}_{1},...,\bm{\mathsf{X}}_{n}\}\}\) can be represented by an invariant and separating map \(f_{conv}\):_

\[f_{conv}(\bm{\mathsf{X}})=\mathop{\oplus}\limits_{i=1}^{n}\bm{\Phi}(\bm{ \mathsf{X}}_{i})\] (11)

_Where \(\bm{\Phi}:\mathbb{R}^{d}\rightarrow\mathbb{R}^{m_{1}\times m_{2}}\) is an affine map, \(\mathop{\oplus}\limits\) is the 2D circular convolution operator and the number of separators is \(m=m_{1}\times m_{2}=(n+1)(n(d-1)+1)\in\mathcal{O}(n^{2}d)\)._

The full proof of Theorem 4.2 is given in Appendix A.3.

### How does circular convolution impact neighbor Mixing?

Let \(\bm{u}_{1},...,\bm{u}_{n}\in\mathbb{R}^{m}\) be discrete signals representing the locally-transformed neighbors before being aggregated. For the sake of simplicity, we slightly override the notation in this section, and refer to the \(j\)-th element of the \(i\)-th signal as \(\bm{u}_{i}^{j}\) with \(j\) starting from \(0\).

The core factor causing the neighbor mixing bottleneck of sum-based aggregators \(\bm{h}=\sum_{i=1}^{n}\bm{u}_{i}\) lies within the fact that no mixing is done in the representation, but only in the MLP compressor that comes afterward:

\[\frac{\partial^{2}}{\partial\bm{u}_{i}^{k}\partial\bm{u}_{j}^{\ell}}\bm{h}=0\] (12)

On the contrary, each element of sequential circular convolution is composed of sums of terms of the form \(\bm{u}_{1}^{j_{1}}\bm{u}_{2}^{j_{2}}\cdot...\cdot\bm{u}_{n}^{j_{n}}\). Particularly:

\[\bm{h}^{k}=\sum_{\begin{subarray}{c}j_{1}+...+j_{n}\equiv k\\ (\text{mod }m)\end{subarray}}\bm{u}_{1}^{j_{1}}\bm{u}_{2}^{j_{2}}\cdot... \cdot\bm{u}_{n}^{j_{n}}\] (13)

This implies that the convolutional representation achieves, in fact, a generalized, higher-order notion of the mix values:

\[\forall 0\leq j_{1},...,j_{n}\leq m-1\;\exists k:\quad\frac{\partial^{n}}{ \partial\bm{u}_{1}^{j_{1}}\partial\bm{u}_{2}^{j_{2}}...\partial\bm{u}_{n}^{j_ {n}}}\bm{h}^{k}=1\] (14)

This notion of higher-order neighbor mixing is visualized in Figure 2. We refer the reader to Appendix A.4 for further theoretical discussions on the stability of permutation-invariant representations.

### Practical considerations

Combining Theorem 4.2 with an MLP compressor yields the "vanilla" version of SSMA: it first applies the local affine map, then computes 2D circular convolution across the neighbor axis and finally compresses the result back using MLP as a universal compressor. The circular convolution is implemented by applying FFT, performing product aggregation along the neighbor axis and then transforming the result back using IFFT. As "scatter_mul" is not implemented for complex numbers in standard libraries, we convert complex values to their polar representation in which multiplication is equivalent to multiplying the magnitudes and summing up the arguments. The "vanilla" version of SSMA is presented in Figure 3.

We now suggest a few practical adjustments to the "vanilla" version of SSMA:

Normalizing the circular convolution.As SSMA performs a product over the neighbors' axis, the optimization process of the vanilla SSMA might get unstable. To address this instability, we normalize the element-wise magnitudes of the product by taking their geometric means.

Figure 2: Visualization of the higher order notion of neighbor mixing. We visualize the convolution result \(h\) for \(3\)-dimensional features, considering \(2\) neighbors \(u,v\) (left) and \(3\) neighbors \(u,v,w\) (right). We demonstrate for each \(n\)-tuple matching a feature per node, the corresponding \(n\)-th order derivative of exactly one entry of \(h\) is \(1\).

Low-rank compressor.Since the number of parameters in the MLP compressor rapidly increases with the representation dimension \(m\), we opted for a single linear layer as our compressor. To accommodate a higher number of neighbor slots and allow for a larger hidden dimension, we reduced the number of parameters in the linear layer by splitting it into two consecutive linear layers that squeezes the representation to low dimension and than expands it back. This effectively performs a low-rank factorization of the weight matrix of the original single linear layer.

Neighbor selection methods.The representation size of the vanilla SSMA\(m=\mathcal{O}(n^{2}d)\) may become prohibitively high in dense neighborhoods (e.g in transductive settings). To address this issue, we employ two neighbor selection techniques that reduce the original neighborhood to a new set of \(\kappa\) neighbors. The first technique simply draws at most \(\kappa\) random neighbors without replacement. The second technique draws inspiration from Graph Attention Networks (GAT) and attention slots [29; 42] and map the neighbors into \(\kappa\) attention slots. The attention coefficient for the edge \(e:j\to i\) for the \(k\)-th slot is expressed as follows:

\[e_{k}(\bm{h}_{i},\bm{h}_{j})=\mathrm{LeakyReLU}(\bm{a}_{k}^{T} \bm{h}_{i}+\bm{b}_{k}^{T}\bm{h}_{j})\] (15) \[\alpha_{ij}^{(k)}=\mathrm{softmax}_{j}e_{k}(\bm{h}_{i},\bm{h}_{ j})=\frac{\exp(e_{k}(\bm{h}_{i},\bm{h}_{j}))}{\sum_{j^{\prime}\in\mathcal{N}_{i }(i)}\exp(e_{k}(\bm{h}_{i},\bm{h}_{j^{\prime}}))}\] (16)

Where \(\bm{a}_{k},\bm{b}_{k}\in\mathbb{R}^{d}\) are per-slot learnable weight vectors. Thereafter, the \(k\)-th slot for the \(i\)-th node is produced by considering the weighted average of the incoming neighbors:

\[s_{i}^{(k)}=\sum_{j\in\mathcal{N}_{in}(i)}\alpha_{ij}^{(k)}\bm{h}_{j}\] (17)

## 5 Experiments

### Synthetic task

To empirically demonstrate the success of SSMA in managing tasks characterized by high neighbor mixing (opposed to sum-based aggregators), we introduce a synthetic regression task we name SumOfGram. In this task, we sample random neighbor features and then generate the labels by considering the sum of the Gramian matrix corresponding to the neighbor features.

In some sense, the SumOfram task is the "simplest" task that involves neighbor mixing:

\[\forall i\neq j:\mathsf{mix}_{i,j}=\left\|\frac{\partial^{2}}{\partial x_{i} \partial x_{j}}\textsc{SumOfGram}(x_{1},...,x_{n})\right\|_{2}=\left\| \mathbb{I}_{d\times d}\right\|_{2}=1\] (18)

Figure 3: Visualization of the Sequential Signal Mixing Aggregation. Left: demonstration of the aggregation stage in an off-the-shelf MPGNN layer. The goal is to create a compressed view of \(t\)’s incoming neighbors. Right: our proposed aggregation. We convert the neighbor features into two-dimensional discrete signals. We then apply \(2\)D circular convolution by applying \(2\)D FFT, performing pointwise multiplication and transforming back using IFFT. Finally, we compress the result back into a \(d\)-dimensional vector using a multi-layer perceptron as a universal compressor.

We train both the sum aggregator and our proposed Sequential Signal Mixing Aggregation until convergence with varying representation sizes \(m\) on the SumOfGram task.

As may be observed in Figure 4, sum aggregators fail at this task, even when used in conjunction with analytic activations, which as claimed previously [1], is sufficient to achieve separation. This shows that sole separation is insufficient for performing arbitrary downstream tasks. On the contrary, \(\operatorname{\textsc{SSMA}}\) has low regression errors, consistently along different activation functions.

### Benchmarking \(\operatorname{\textsc{SSMA}}\)

**Experimental Setup.** We test the effectiveness of \(\operatorname{\textsc{SSMA}}\) by incorporating it into popular MPGNN architectures. We evaluate both original and augmented architectures across a wide range of benchmarks. These benchmarks cover learning in both the transductive and inductive settings, node and graph-level prediction tasks, regression and classification problems, feature-oriented as well as purely topological data and tasks that involve challenging neighborhood configurations including dense neighborhoods and distant neighbor dependencies. As \(\operatorname{\textsc{SSMA}}\) introduces learnable parameters, we ensure a fair comparison by maintaining an equal total parameter count to that of the original architectures in each experimental setting, adjusting the architecture's hidden dimension to adhere to the budget constraints. For a detailed discussion on the parameter budget in each experiment, please refer to Appendix C.4. Given the budget for each experiment, we conduct a hyperparameter search (HPS) on \(\operatorname{\textsc{SSMA}}\) parameters to find the best configuration. We further perform ablation studies to closely examine the effect of each hyperparameter, as detailed in Appendix E.

Results.We observe substantial performance gains across all tested combinations of benchmarks and MPGNN architectures. Notably, the most significant relative improvements were observed on the IMDB-B benchmark, which lacks node and edge features. This phenomenon is likely attributed to \(\operatorname{\textsc{SSMA}}\)'s neighbor mixing capabilities, enabling it to learn joint topological relationships among neighbors. The Improvements observed on the LRGB [16] datasets indicate that \(\operatorname{\textsc{SSMA}}\) better extracts relevant neighborhood information to be propagated to distant parts of the graph. Additionally, the experiments on the OGBN networks (OGBN-Arxiv and OGBN-Products) [21] confirm that \(\operatorname{\textsc{SSMA}}\) is robust to dense neighborhoods and highlight the efficiency of its attentional neighbor selection mechanism. Another noteworthy observation is that \(\operatorname{\textsc{SSMA}}\) utilizes the hidden dimension more effectively. Since we use the same parameter budget, experiments with \(\operatorname{\textsc{SSMA}}\) employ a lower hidden dimensionality than those using'sum' aggregation. This is because \(\operatorname{\textsc{SSMA}}\) allocates learnable parameters, whereas'sum' aggregation does not. Despite a smaller hidden dimension, \(\operatorname{\textsc{SSMA}}\) outperforms'sum' aggregation, indicating its efficiency in retaining relevant information for downstream tasks. Benchmarks for more common aggregation functions is in Appendix F

Figure 4: SumOfGram train and test regression \(L_{1}\) errors for different activation functions. The sum aggregator (not dashed) performs poorly and fails to scale with the capacity of the aggregation module, even when used in conjunction with analytic activations. On the contrary, \(\operatorname{\textsc{SSMA}}\) (dashed) consistently achieves low regression errors and scales well with the number of learnable parameters.

[MISSING_PAGE_FAIL:9]

## References

* [1] T. Amir, S. J. Gortler, I. Avni, R. Ravina, and N. Dym. Neural injective functions for multisets, measures and graphs via a finite witness theorem. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [2] J. Baek, M. Kang, and S. J. Hwang. Accurate learning of graph representations with graph multiset pooling. In _International Conference on Learning Representations_, 2021.
* [3] B. Bevilacqua, F. Frasca, D. Lim, B. Srinivasan, C. Cai, G. Balamurugan, M. M. Bronstein, and H. Maron. Equivariant subgraph aggregation networks. In _International Conference on Learning Representations_, 2022.
* [4] J. Bradshaw, M. J. Kusner, B. Paige, M. H. S. Segler, and J. M. Hernandez-Lobato. A generative model for electron paths. In _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019_. OpenReview.net, 2019.
* [5] X. Bresson and T. Laurent. Residual gated graph convnets. _arXiv preprint arXiv:1711.07553_, 2017.
* [6] S. Brody, U. Alon, and E. Yahav. How attentive are graph attention networks? In _International Conference on Learning Representations_, 2022.
* [7] D. Buterez, J. P. Janet, S. J. Kiddle, D. Oglic, and P. Lio. Graph neural networks with adaptive readouts. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [8] J. Cahill, J. W. Iverson, and D. G. Mixon. Towards a bilipschitz invariant theory, 2024.
* [9] W.-L. Chiang, X. Liu, S. Si, Y. Li, S. Bengio, and C.-J. Hsieh. Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks. In _Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining_, pages 257-266, 2019.
* [10] E. Cohen-Karlik, A. B. David, and A. Globerson. Regularizing towards permutation invariance in recurrent models. In _Proceedings of the 34th International Conference on Neural Information Processing Systems_, NIPS '20, Red Hook, NY, USA, 2020. Curran Associates Inc.
* [11] G. Corso, L. Cavalleri, D. Beaini, P. Lio, and P. Velickovic. Principal neighbourhood aggregation for graph nets. _Advances in Neural Information Processing Systems_, 33:13260-13271, 2020.
* [12] G. Corso, L. Cavalleri, D. Beaini, P. Lio, and P. Velickovic. Principal neighbourhood aggregation for graph nets. In _Advances in Neural Information Processing Systems_, 2020.
* [13] M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 29. Curran Associates, Inc., 2016.
* [14] D. Duvenaud, D. Maclaurin, J. Aguilera-Iparraguirre, R. Gomez-Bombarelli, T. Hirzel, A. Aspuru-Guzik, and R. P. Adams. Convolutional networks on graphs for learning molecular fingerprints. _CoRR_, abs/1509.09292, 2015.
* [15] V. P. Dwivedi, C. K. Joshi, A. T. Luu, T. Laurent, Y. Bengio, and X. Bresson. Benchmarking graph neural networks. _Journal of Machine Learning Research_, 24(43):1-48, 2023.
* [16] V. P. Dwivedi, L. Rampasek, M. Galkin, A. Parviz, G. Wolf, A. T. Luu, and D. Beaini. Long range graph benchmark. _Advances in Neural Information Processing Systems_, 35:22326-22340, 2022.
* [17] N. Dym and S. J. Gortler. Low-dimensional invariant embeddings for universal geometric learning. _Foundations of Computational Mathematics_, pages 1-41, 2024.
* [18] W. Fan, Y. Ma, Q. Li, J. Wang, G. Cai, J. Tang, and D. Yin. A graph neural network framework for social recommendations. _IEEE Trans. Knowl. Data Eng._, 34(5):2033-2047, 2022.

* [19] R. Gomez-Bombarelli, J. N. Wei, D. Duvenaud, J. M. Hernandez-Lobato, B. Sanchez-Lengeling, D. Sheberla, J. Aguilera-Iparraguirre, T. D. Hirzel, R. P. Adams, and A. Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. _ACS central science_, 4(2):268-276, 2018.
* [20] W. L. Hamilton, R. Ying, and J. Leskovec. Inductive representation learning on large graphs. In _Proceedings of the 31st International Conference on Neural Information Processing Systems_, NIPS'17, page 1025-1035, Red Hook, NY, USA, 2017. Curran Associates Inc.
* [21] W. Hu, M. Fey, M. Zitnik, Y. Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec. Open graph benchmark: Datasets for machine learning on graphs. _Advances in neural information processing systems_, 33:22118-22133, 2020.
* [22] J. Irwin, T. Sterling, M. Mysinger, E. Bolstad, and R. Coleman. Zinc: A free tool to discover chemistry for biology. _Journal of chemical information and modeling_, 52, 05 2012.
* [23] W. Jin, K. Yang, R. Barzilay, and T. S. Jaakkola. Learning multimodal graph-to-graph translation for molecule optimization. In _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019_. OpenReview.net, 2019.
* [24] T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. In _International Conference on Learning Representations_, 2017.
* [25] R. Kortvelesy, S. Morad, and A. Prorok. Generalised f-mean aggregation for graph neural networks. _Advances in Neural Information Processing Systems_, 36:34439-34450, 2023.
* ECCV 2022
- 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXV_, volume 13695 of _Lecture Notes in Computer Science_, pages 195-212. Springer, 2022.
* [27] G. Li, C. Xiong, A. Thabet, and B. Ghanem. Deepergcn: All you need to train deeper gcns. _arXiv preprint arXiv:2006.07739_, 2020.
* [28] X. Liu, X. You, X. Zhang, J. Wu, and P. Lv. Tensor graph convolutional networks for text classification. In _The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020_, pages 8409-8416. AAAI Press, 2020.
* [29] F. Locatello, D. Weissenborn, T. Unterthiner, A. Mahendran, G. Heigold, J. Uszkoreit, A. Dosovitskiy, and T. Kipf. Object-centric learning with slot attention. _Advances in Neural Information Processing Systems_, 33:11525-11538, 2020.
* [30] T. L. Makinen, J. Alsing, and B. D. Wandelt. Fishnets: Information-optimal, scalable aggregation for sets and graphs, 2023.
* [31] H. Maron, H. Ben-Hamu, H. Serviansky, and Y. Lipman. Provably powerful graph networks. In H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. B. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 2153-2164, 2019.
* [32] C. Morris, N. M. Kriege, F. Bause, K. Kersting, P. Mutzel, and M. Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. _arXiv preprint arXiv:2007.08663_, 2020.
* [33] M. Narasimhan, S. Lazebnik, and A. G. Schwing. Out of the box: Reasoning with graph convolution nets for factual visual question answering. _CoRR_, abs/1811.00538, 2018.
* [34] E. Ong and P. Velickovic. Learnable commutative monoids for graph neural networks. In _The First Learning on Graphs Conference_, 2022.

* [35] O. Puny, D. Lim, B. T. Kiani, H. Maron, and Y. Lipman. Equivariant polynomials for graph neural networks. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, _International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA_, volume 202 of _Proceedings of Machine Learning Research_, pages 28191-28222. PMLR, 2023.
* [36] L. Rampasek, M. Galkin, V. P. Dwivedi, A. T. Luu, G. Wolf, and D. Beaini. Recipe for a general, powerful, scalable graph transformer. _CoRR_, abs/2205.12454, 2022.
* [37] L. Rampasek, M. Galkin, V. P. Dwivedi, A. T. Luu, G. Wolf, and D. Beaini. Recipe for a General, Powerful, Scalable Graph Transformer. _Advances in Neural Information Processing Systems_, 35, 2022.
* [38] L. Schneckenreiter, R. Freinschlag, F. Sestak, J. Brandstetter, G. Klambauer, and A. Mayr. GNN-VPA: A variance-preserving aggregation strategy for graph neural networks. In _The Second Tiny Papers Track at ICLR 2024_, 2024.
* [39] L. Schneckenreiter, R. Freinschlag, F. Sestak, J. Brandstetter, G. Klambauer, and A. Mayr. Gnn-vpa: A variance-preserving aggregation strategy for graph neural networks. _arXiv preprint arXiv:2403.04747_, 2024.
* [40] S. A. Tailor, F. Opolka, P. Lio, and N. D. Lane. Adaptive filters for low-latency and memory-efficient graph neural networks. In _International Conference on Learning Representations_, 2022.
* [41] S. A. Tailor, F. Opolka, P. Lio, and N. D. Lane. Do we need anistropic graph neural networks? In _International Conference on Learning Representations_, 2022.
* [42] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio. Graph attention networks. _arXiv preprint arXiv:1710.10903_, 2017.
* [43] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio. Graph Attention Networks. _International Conference on Learning Representations_, 2018. accepted as poster.
* [44] X. Wang and M. Zhang. How powerful are spectral graph neural networks. _ICML_, 2022.
* [45] Z. Wang, Q. Lv, X. Lan, and Y. Zhang. Cross-lingual knowledge graph alignment via graph convolutional networks. In E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsujii, editors, _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 349-357, Brussels, Belgium, Oct.-Nov. 2018. Association for Computational Linguistics.
* [46] F. Wu, A. Souza, T. Zhang, C. Fifty, T. Yu, and K. Weinberger. Simplifying graph convolutional networks. In _Proceedings of the 36th International Conference on Machine Learning_, pages 6861-6871. PMLR, 2019.
* [47] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? _arXiv preprint arXiv:1810.00826_, 2018.
* [48] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? In _International Conference on Learning Representations_, 2019.
* [49] S. Yun, M. Jeong, R. Kim, J. Kang, and H. J. Kim. Graph transformer networks. In H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. B. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 11960-11970, 2019.
* [50] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. R. Salakhutdinov, and A. J. Smola. Deep sets. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017.

Proofs and extended theory discussion

### Proof of Proposition 3.2

Proof.: Let us consider a general sum-based aggregator: \(F(x_{1},...,x_{n})=\rho(\sum_{k=1}^{n}\phi(x_{k}))\).

Then, we have that for \(i\neq j\) and for the \(\ell\)-th output:

\[\mathsf{mix}_{i,j}^{(\ell)}=\] (19) \[\left\|\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}\rho^{( \ell)}(\sum_{k=1}^{n}\phi(x_{k}))\right\|_{2}=\] (20) \[\left\|\frac{\partial}{\partial x_{j}}\left\{\frac{\partial}{ \partial x_{i}}\rho^{(\ell)}(\sum_{k=1}^{n}\phi(x_{k}))\right\}\right\|_{2}=\] (21) \[\left\|\frac{\partial}{\partial x_{j}}\left\{\nabla\rho^{(\ell)}( \sum_{k=1}^{n}\phi(x_{k}))\cdot J_{\phi}(x_{i})\right\}\right\|_{2}=\] (22) \[\left\|\frac{\partial}{\partial x_{j}}\left\{\nabla\rho^{(\ell)}( \sum_{k=1}^{n}\phi(x_{k}))\right\}\cdot J_{\phi}(x_{i})\right\|_{2}=\] (23) \[\left\|J_{\phi}(x_{j})^{T}\cdot H_{\rho^{(\ell)}}(\sum_{k=1}^{n} \phi(x_{k}))\cdot J_{\phi}(x_{i})\right\|_{2}\leq\] (24) \[\left\|J_{\phi}(x_{i})\right\|_{2}\cdot\left\|H_{\rho^{(\ell)}}( \sum_{k=1}^{n}\phi(x_{k}))\right\|_{2}\cdot\left\|J_{\phi}(x_{j})\right\|_{2}\] (25)

Typically, the local operator \(\phi:\mathbb{R}^{d}\rightarrow\mathbb{R}^{m}\) is of the form \(\phi(x)=\sigma(Ax+b)\) where \(\sigma\) is an activation function applied element-wise and \(A\in\mathbb{R}^{m\times d},b\in\mathbb{R}^{m}\) are learnable parameters.

Usually \(|\sigma^{\prime}(z)|\) is bounded by some small constant \(c_{1}\).

Therefore, we have:

\[\left\|J_{\phi}(x)\right\|_{2}=\left\|\mathsf{diag}(\sigma^{\prime}(Ax+b)) \cdot A\right\|\leq c_{1}\cdot\left\|A\right\|_{2}\] (26)

Moreover, the global pooling operator \(\rho:\mathbb{R}^{m}\rightarrow\mathbb{R}^{d}\) is an MLP of the form:

\[\rho(z)=W_{2}\cdot\sigma(W_{1}z+\beta_{1})+\beta_{2}\] (27)

where \(W_{1}\in\mathbb{R}^{m\times m},\beta_{1}\in\mathbb{R}^{m},W_{2}\in\mathbb{R}^ {d\times m},\beta_{2}\in\mathbb{R}^{d}\).

Therefore:

\[\frac{d}{dz}\rho^{(\ell)}(z)=W_{2}[\ell,:]^{T}\cdot\mathsf{diag}(\sigma^{ \prime}(W_{1}z+\beta_{1}))\cdot W_{1}\] (28)

Let us denote:

\[u^{(\ell)}(z):=W_{2}[\ell,:]^{T}\cdot\mathsf{diag}(\sigma^{\prime}(W_{1}z+ \beta_{1}))=(W_{2}[\ell,k]\cdot\sigma^{\prime}([W_{1}z+\beta_{1}]_{k}))_{k=1}^ {m}\] (29)

Then,

\[\frac{d^{2}}{dz^{2}}\rho^{(\ell)}(z)=\frac{d}{dz}u^{(\ell)}(z)\cdot W_{1}\] (30)

We compute \(\frac{d}{dz}u^{(\ell)}(z)\) for each output dimension \(1\leq k\leq m\) separately:\[\frac{d}{dz}\left\{u^{(\ell)}(z)[k]\right\}=\frac{d}{dz}\left\{W_{2}[ \ell,k]\cdot\sigma^{\prime}(W_{1}[k,;]z+\beta_{1}[k])\right\}=\] (31) \[W_{2}[\ell,k]\cdot\sigma^{\prime\prime}(W_{1}[k,;]z+\beta_{1}[k ])\cdot W_{1}[k,;]\] (32)

Summarizing this for all dimensions \(k\) yields:

\[\frac{d}{dz}u^{(\ell)}(z)=\text{diag}_{k}(W_{2}[\ell,k])\cdot\text{diag}( \sigma^{\prime\prime}(W_{1}z+\beta_{1}))\cdot W_{1}\] (33)

So under the assumption \(|\sigma^{\prime\prime}(z)|\leq c_{2}\) we get the following bound on the Hessian norm:

\[\left\|H_{\rho^{(\ell)}}(z)\right\|_{2}\leq c_{2}\cdot\left\|W_{2}[\ell,;] \right\|_{2}\cdot\left\|W_{1}\right\|_{2}^{2}\] (34)

And the final bound on \(\text{mix}_{i,j}^{(\ell)}\) is given by:

\[\text{mix}_{i,j}^{(\ell)}\leq c_{2}\cdot c_{1}^{2}\cdot\left\|W_{2}[\ell,;] \right\|_{2}\cdot\left\|W_{1}\right\|_{2}^{2}\cdot\left\|A\right\|_{2}^{2}\in \mathcal{O}(\left\|\theta\right\|_{2}^{2})\] (35)

### Ring theory and the factorization lemma

A **ring** is an algebraic structure that generalizes the notion of a field. In particular, univariate and multivariate polynomials obey this structure. Formally, a ring \(R\) is a set associated with two binary operations \(+\) (addition) and \(\cdot\) (multiplication) satisfying the ring axioms:

1. \(R\) is an abelian group under the addition operation.
2. \(R\) is a monoid under the multiplication operation: 1. associativity: \((a\cdot b)\cdot c=a\cdot(b\cdot c)\) for all \(a,b,c\in R\). 2. existence of identity: there is an element \(1\in R\) such that: \(1\cdot a=a\cdot 1=a\).
3. Distributively: \(a\cdot(b+c)=a\cdot b+a\cdot c\) and \((b+c)\cdot a=b\cdot a+c\cdot a\).

A ring is said to be **commutative** if its elements commute under multiplication: \(a\cdot b=b\cdot a\) for all \(a,b\in R\).

Given a commutative ring \(R\), we can define its corresponding univariate **polynomial ring** denoted as \(R[X]\) by considering a set of formal expressions \(\sum_{i=0}^{n}\alpha_{i}X^{i}\) where \(n\) is a non-negative integer and \(\alpha_{i}\in R\). We consider \(X\) a formal variable and define addition and multiplication according to the ordinary rules for manipulating algebraic expressions. Each polynomial \(p\in R[X]\) has a **degree** defined as \(\max_{i}\alpha_{i}\neq 0\). For each \(r\in R\), we can define an **evaluation map**\(T_{r}\) which takes some polynomial as an input and returns an element in the underlying ring by substituting \(X=r\), namely: \(T_{r}(\sum_{i=0}^{n}\alpha_{i}X^{i})=\sum_{i=0}^{n}\alpha_{i}r^{i}\).

**Multivariate polynomials** can be defined similarly by considering multiple formal variables. Alternatively, note that since the polynomial ring of some commutative ring \(R\) is also a commutative ring by itself, we can equivalently define multivariate polynomials by considering the ring of polynomials above \(R[X]\), namely: \((R[X])[Y]\cong R[X,Y]\).

An **integral domain** is a nonzero commutative ring in which the product of any two nonzero elements is nonzero. In particular, any field is an integral domain, and any polynomial ring is an integral domain, given that its underlying ring is itself an integral ring. Am immediate conclusion is that \(\mathbb{R}[x]\) and \(\mathbb{R}[x_{1},...,x_{n}]\) are integral domains.

An **irreducible element** of an integral domain is a non-zero element that is not invertible and is not the product of two non-invertible elements. For instance, for every commutative ring \(R\), every polynomial of the form \(x-r\) where \(r\in R\) is an irreducible element of \(R[x]\).

A **unique factorization domain** (UFD) is an integral domain \(R\) in which every non-zero element \(r\) of \(R\) can be written as a product (an empty product if \(x\) is invertible) of irreducible elements \(p_{i}\) of \(R\) and an invertible element \(u\): \(r=u\cdot\prod_{i=1}^{n}p_{i}\). This representation ought to be unique up to multiplication with invertible elements. The key fact underlying our construction is the fact that a polynomial ring of a UFD is by itself a UFD (known as **Gauss's lemma**). In conjunction with the fact that any field \(\mathbb{F}\) is a UFD, we get that \(\mathbb{R}[x_{1},...,x_{n}]\) is a UFD for any amount of variables.

**Lemma A.1**.: _Let \(p(t,z)\in\mathbb{R}[t,z]\) be some polynomial that can be factorized as:_

\[p(t,z)=\prod_{i=1}^{n}(t-u_{i}(z))\] (36)

_where \(u_{i}(z)\) is some polynomial of \(z\)._

_Then, such factorization is unique to the order of the terms \((t-u_{i}(z))\)._

Proof.: Gauss's lemma implies that \(\mathbb{R}[t,z]\cong(\mathbb{R}[z])[t]\) is a unique factorization domain. Since every polynomial of the form \(t-r(z)\) is an irreducible element in \((\mathbb{R}[z])[t]\), it follows that a factorization of the form \(p(t,z)=\prod_{i}(t-q_{i}(z))\) is unique up to permutation of the terms. 

**Corollary A.2**.: _The coefficients of the polynomial \(p_{\overline{\mathbf{X}}}(t,z)\) defined in Section 4.2 form an ensemble of separating invariants._

### Proof of Theorem 4.2

Proof.: Let \(p_{\mathbf{\bar{X}}}(t,z)\) be the polynomial in the construction. Corollary A.2 implies that its coefficients form an ensemble of separating invariants. Consequently, we repeat the steps in Section 4.1 to get an analogous result to Theorem 4.1, arriving at the desired form.

We represent \(p_{\mathbf{\bar{X}}}(t,z)\) by evaluating its value on a grid of points \((u_{s},v_{b})\) where \(0\leq s\leq n,0\leq b\leq\tau\). Specifically, we can choose \(u_{s}:=e^{-\frac{2\pi i}{n+1}s}\) and \(v_{b}:=e^{-\frac{2\pi i}{\tau+1}b}\) and to get the two-dimensional DFT of the polynomial coefficients:

\[p_{\mathbf{\bar{X}}}(u_{s},v_{b})=\sum_{k=0}^{n}\sum_{\ell=0}^{\tau}e_{k\ell}( \textbf{X})\cdot(u_{s})^{k}(v_{b})^{\ell}=\sum_{k,\ell}e_{k\ell}(\textbf{X}) \cdot e^{-\frac{2\pi i}{n+1}k}\cdot e^{-\frac{2\pi i\ell}{\tau+1}\ell}\] (37)

Again, by setting \(\mathbf{\Phi}(\textbf{X}_{i})\) to be the coefficients matrix of \(p_{i}(t,z)=t-q_{\textbf{X}_{i}}(z)\):

\[\mathbf{\Phi}(\textbf{X}_{i})=\begin{bmatrix}-\textbf{X}_{i1}&-\textbf{X}_{i2 }&\cdots&-\textbf{X}_{id}&\cdots&0\\ 1&0&0&0&\cdots&0\\ \vdots&\vdots&\vdots&\vdots&\ddots&0\\ 0&0&0&0&\cdots&0\end{bmatrix}\] (38)

we get that:

\[e_{k\ell}(\textbf{X})=\mathcal{F}_{2D}^{-1}\left\{\sum_{i=1}^{n}\mathcal{F}_{ 2D}\left\{\mathbf{\Phi}(\textbf{X}_{i})\right\}\right\}\] (39)

Where the multiplication is elementwise. According to the 2D circular convolution theorem, this exactly amounts to convolving the vector coefficients of \(p_{i}(t,z)\)s:

\[e_{k\ell}(\textbf{X})=\overset{n}{\underset{i=1}{\overset{n}{\oplus}}}\mathbf{ \Phi}(\textbf{X}_{i})\] (40)

### On the stability of permutation-invariant representations

Ideally, we would like our representation to be numerically stable. That is, to require that if two distinct multisets are close to each other, then their representations should also be close to each other, and vice versa. This was formalized previously [1] using the notions of bi-Lipschitzness and Wasserstein distance as we now recapitulate.

Let \(\overline{\mathbf{X}},\overline{\mathbf{Y}}\) be two multisets of \(d\) dimensional vectors with \(|\overline{\mathbf{X}}|=|\overline{\mathbf{Y}}|=n\).

Wasserstein distance.we measure the distance between equally sized multisets \(\subseteq\mathbb{R}^{d}\) using the notion of Wasserstein distance:

\[\mathcal{W}_{p}(\overline{\mathbf{X}},\overline{\mathbf{Y}}):=\min_{\pi\in S _{n}}(\frac{1}{n}\sum_{i=1}^{n}\left\|\mathbf{X}_{i}-\mathbf{Y}_{\pi(i)} \right\|^{p})^{1/p}\] (41)

Where \(\left\|.\right\|\) is the \(L_{1}\) norm over \(\mathbb{R}^{d}\).

We are interested in the bi-Lipschitzness property - whether there exist constants \(c,C>0\) such that:

\[c\cdot\mathcal{W}_{p}(\overline{\mathbf{X}},\overline{\mathbf{Y}})\leq\left\| \hat{f}(\mathbf{X})-\hat{f}(\mathbf{Y})\right\|\leq C\cdot\mathcal{W}_{p}( \overline{\mathbf{X}},\overline{\mathbf{Y}})\] (42)

Unfortunately, it turns out that this notion of stability is unattainable by any differentiable permutation-invariant representation. We prove this by generalizing such results for sum-based aggregators [1].

**Proposition A.3**.: _Let \(\hat{f}\) be some differentiable multiset representation. Then, there exist \(n,d\) such that for every \(\epsilon>0\) there exist \(\overline{\mathbf{X}}_{\epsilon},\overline{\mathbf{Y}}_{\epsilon}\subseteq \mathbb{R}^{d}\) two multisets of size \(n\) such that:_

\[\left\|\hat{f}(\mathbf{X}_{\epsilon})-\hat{f}(\mathbf{Y}_{\epsilon})\right\| \leq\epsilon\cdot\mathcal{W}_{p}(\overline{\mathbf{X}_{\epsilon}},\overline{ \mathbf{Y}_{\epsilon}})\] (43)

An independent proof for general invariant embeddings is given in [8].

Proof.: Let \(\hat{f}\) be some permutation-invariant representation. We use the same construction as in [1] and generalize the proof to arbitrary symmetric and differential representations.

We choose \(n=2\) and some arbitrary \(d\in\mathbb{N}\).

Let \(\mathbf{x_{0}},\mathbf{d}\in\mathbb{R}^{d}\) where \(\mathbf{d}\) has a unit norm, and consider \(S_{h}\) = \(\{\{\mathbf{x_{0}}+h\mathbf{d},\mathbf{x_{0}}-h\mathbf{d}\}\}\).

We note that the Wasserstein distance \(\mathcal{W}_{p}(S_{h},S_{0})\) is \(h\). Thus, it is sufficient to show that:

\[\lim_{h\to 0}\frac{\left\|\hat{f}(S_{h})-\hat{f}(S_{0})\right\|}{h}=0\] (44)

Since \(\hat{f}_{k}:\mathbb{R}^{2d}\rightarrow\mathbb{R}\) is invariant we have that for every \(\mathbf{u},\mathbf{v}\in\mathbb{R}^{d}\): \(\hat{f}_{k}(\mathbf{u},\mathbf{v})=\hat{f}_{k}(\mathbf{v},\mathbf{u})\).

The differentiability of \(\hat{f}_{k}\) at \((\mathbf{u},\mathbf{v})\) implies that for every \(1\leq i\leq d\):

\[\partial_{i}\hat{f}_{k}(\mathbf{u},\mathbf{v})=\lim_{h\to 0} \frac{\hat{f}_{k}(u_{1},...,u_{i}+h,...,u_{d},\mathbf{v})-\hat{f}_{k}( \mathbf{u},\mathbf{v})}{h}=\] (45) \[\lim_{h\to 0}\frac{\hat{f}_{k}(\mathbf{v},u_{1},...,u_{i}+h,...,u_{d})-\hat{f}_{k}(\mathbf{v},\mathbf{u})}{h}=\partial_{d+i}\hat{f}_{k}( \mathbf{v},\mathbf{u})\] (46)

Particularly, this implies that \(\partial_{i}\hat{f}_{k}(\mathbf{x_{0}},\mathbf{x_{0}})=\partial_{d+i}\hat{f}_ {k}(\mathbf{x_{0}},\mathbf{x_{0}})\).

Using the differentiability of \(\hat{f}_{k}\) at \((\mathbf{x_{0}},\mathbf{x_{0}})\) we can write:

\[\hat{f}_{k}(\mathbf{x_{0}}+\delta_{\mathbf{1}},\mathbf{x_{0}}+ \delta_{\mathbf{2}})=\] (47) \[\hat{f}_{k}(\mathbf{x_{0}},\mathbf{x_{0}})+\sum_{i=1}^{d}\partial _{i}\hat{f}_{k}(\mathbf{x_{0}},\mathbf{x_{0}})\cdot\delta_{1i}+\sum_{i=1}^{d} \partial_{d+i}\hat{f}_{k}(\mathbf{x_{0}},\mathbf{x_{0}})\cdot\delta_{2i}+o_{k} (\left\|(\delta_{\mathbf{1}},\delta_{\mathbf{2}})\right\|)\] (48)Plugging in \(\delta_{\mathbf{1}}=h\mathbf{d}\) and \(\delta_{\mathbf{2}}=-h\mathbf{d}\) we get:

\[\hat{f}_{k}(\mathbf{x_{0}}+h\mathbf{d},\mathbf{x_{0}}-h\mathbf{d})- \hat{f}_{k}(\mathbf{x_{0}},\mathbf{x_{0}})=\] (49) \[h\sum_{i=1}^{d}\partial_{i}\hat{f}_{k}(\mathbf{x_{0}},\mathbf{x_ {0}})\cdot d_{i}-h\sum_{i=1}^{d}\partial_{d+i}\hat{f}_{k}(\mathbf{x_{0}}, \mathbf{x_{0}})\cdot d_{i}+o_{k}(h)=o_{k}(h)\] (50)

All in all, we have:

\[\lim_{h\to 0}\frac{\left\|\hat{f}(S_{h})-\hat{f}(S_{0})\right\|}{h}=\lim_{h \to 0}\frac{\sum_{k=1}^{m}|\hat{f}_{k}(\mathbf{x_{0}}+h\mathbf{d},\mathbf{x_ {0}}-h\mathbf{d})-\hat{f}_{k}(\mathbf{x_{0}},\mathbf{x_{0}})|}{h}=\] (51) \[\lim_{h\to 0}\frac{\sum_{k=1}^{m}|o_{k}(h)|}{h}=\sum_{k=1}^{m} \lim_{h\to 0}\frac{|o_{k}(h)|}{h}=0\] (52)

Meaning that for every \(\epsilon>0\) there exists sufficiently small \(h\) such that:

\[\left\|\hat{f}(S_{h})-\hat{f}(S_{0})\right\|\leq\epsilon\cdot h=\epsilon\cdot \mathcal{W}_{p}(S_{h},S_{0})\] (53)Complexity analysis

### Theoretical analysis

In this section, we provide a theoretical complexity analysis of SSMA for both its "vanilla" and refined versions. We consider the total cost of the aggregation stage in a single MPGNN layer within a graph \(G=(V,E)\). We let \(d\) be the hidden dimension and \(m=m_{1}\cdot m_{2}\) be the total representation size. We let \(\kappa\) be the number of slots in the refined version of SSMA. We first analyze the vanilla version of SSMA step-by-step:

1. **Local affine layer:** locally transforms each node by an affine transformation \(\mathcal{O}(|V|\cdot m\cdot d)\).
2. **Local FFT:** the FFT is computed per node yielding a cost of \(\mathcal{O}(|V|\cdot m\log(m))\).
3. **Product aggregation:** The complex variant of scatter_mul aggregation - \(\mathcal{O}(|E|\cdot m)\).
4. **Local IFFT:** Same as stage 2.
5. **MLP compressor:** We used linear layer as a compressor to the original dimension - \(\mathcal{O}(|V|\cdot m\cdot d)\).

All in all we get: \(\mathcal{O}(m(d+\log(m))|V|+m|E|)\) compared to the standard \(\mathcal{O}(md|V|+m|E|)\) which is a negligible slowdown.

In the refined version of SSMA, we consider the modifications we applied:

* **Neighbor selection:** If the random neighbor selection is used, then the cost is simply \(\mathcal{O}(m\cdot\kappa\cdot|V|+|E|)\) for this stage. Alternatively, if the soft-attentional neighbor selection is picked, the attention weights in Equation (16) may be computed per edge and attention slot in a total running time of \(\mathcal{O}(m\cdot\kappa\cdot|V|+\kappa\cdot|E|)\) and the contents of the slots may be implemented using sum aggregation in \(\mathcal{O}(m\cdot\kappa\cdot|E|)\). We get a total of \(\mathcal{O}(m\cdot\kappa\cdot|V|+|E|)\) for the random neighbor selection or \(\mathcal{O}(m\cdot\kappa\cdot(|V|+|E|))\) for the soft-attentional neighbor selection for this stage.
* **Other modifications:** the normalization method does not affect the complexity of the aggregation. While the complexity of the MLP compressor is slightly reduced, the asymptotic complexity remains the same due to the local affine layer bottleneck.
* **Application of SSMA on the new neighborhood**: We consider the complexity obtained in the analysis of the vanilla SSMA and replace the number of edges, \(|E|\), with \(\kappa\cdot|V|\). The complexity of this stage then becomes: \(\mathcal{O}(m(d+\log(m))|V|+m\cdot\kappa\cdot|V|)=\mathcal{O}(m(d+\log(m)+ \kappa)|V|)\).

All in all, we get \(\mathcal{O}(m(d+\log(m)+\kappa)|V|+|E|)\) for the random neighbor selection refined SSMA or \(\mathcal{O}(m(d+\log(m)+\kappa)|V|+m\kappa|E|)\) for the soft neighbor selection refined version of SSMA.

### Runtime measurement

To further show that SSMA is scalable we computed the runtime of it compared to other common networks, we measured the runtime of the layer with our aggregation with and without attention, for each configuration we show the downstream task results. As can be seen in Table 3 SSMA runtime is comparable to other methods while achieving higher downstream task performance.

[MISSING_PAGE_FAIL:20]

binary labels, such as whether a molecule inhibits HIV virus replication. The ogbg-molpcba dataset includes multiple tasks, some of which may contain 'nan' values indicating that the corresponding label is not assigned to the molecule. These datasets differ in size: ogbg-molhiv is smaller, while ogbg-molpcba is medium-sized. The evaluation metrics used are ROC-AUC for ogbg-molhiv and Average Precision (AP) for ogbg-molpcba. We used the official train/validation/test splits provided by the OGB team.

OGB - Node property prediction [21].We benchmark on two dense node-level citation graph datasets from OGB-N: ogbn-products and ogbn-arxiv. The login-products dataset is an undirected and unweighted graph representing an Amazon product co-purchasing network. Nodes represent products sold on Amazon, and edges indicate co-purchased products. Node features are generated by extracting bag-of-words features from product descriptions, followed by Principal Component Analysis for dimensionality reduction. The prediction task is multiclass classification to predict a product's category.

The ogbn-arxiv dataset is a smaller directed graph representing the citation network among Computer Science papers indexed by the Microsoft Academic Graph (MAG). Each node represents an arXiv paper, and the directed edges indicate citations between papers. Node embeddings are created by averaging the embeddings of words in the paper titles and abstracts, computed using the skip-gram model over the MAG corpus. The task is to predict the 40 subject areas of the arXiv CS papers.

We used the graph obtained from the OGB python package without any preprocessing.

Long Range Graph Benchmark (LRGB) [16].We benchmark on two graph-level datasets from the Long-Range Graph Benchmark (LRGB): peptides-func and peptides-struct. Each graph in these datasets represents a peptide, a short chain of amino acids shorter than proteins and abundant in nature. Each amino acid is composed of many heavy atoms, making the molecular graph of a peptide much larger than that of a small drug-like molecule. Peptide graphs have a low average node degree. Still, they have significantly larger diameters compared to other drug-like molecules, making them ideal for studying long-range dependencies in Graph Neural Networks (GNNs). Both datasets use the same set of graphs but differ in their prediction tasks. Peptides-func is a multi-label graph classification dataset based on peptide function, while Peptides-struct is a multi-label graph regression dataset based on the 3D structure of peptides. More details can be found in the LRGB GitHub repository. We used the versions of the datasets available via PYG without any further preprocessing.

Dataset statistics.The statistics of the datasets we used in our experiments are shown in Table 4.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline Dataset & Avg. Nodes & Avg. Edges & \# Graphs & Avg. in deg & STD in deg & Median deg \\ \hline \hline ogbg-molhiv & 25.51 & 54.94 & 41127 & 2.15 & 0.77 & 2 \\ ogbg-molpcba & 25.97 & 56.22 & 437929 & 2.16 & 0.71 & 2 \\ ogbn-arxiv & 169343 & 1166243 & 1 & 6.89 & 67.6 & 1 \\ ogbn-products & 2,449,029 & 123,718,280 & 1 & 50.52 & 95.91 & 26 \\ mutag & 17.93 & 39.59 & 188 & 2.21 & 0.74 & 2 \\ enzymes & 32.63 & 124.27 & 600 & 3.81 & 1.15 & 4 \\ proteins & 39.06 & 145.63 & 1113 & 3.73 & 1.15 & 4 \\ ptc-mr & 14.29 & 29.38 & 344 & 2.06 & 0.81 & 2 \\ imdb-binary & 19.77 & 193.06 & 1000 & 9.76 & 7.43 & 7 \\ zinc & 23.16 & 49.83 & 12000 & 2.15 & 0.72 & 2 \\ peptides-func & 150.94 & 307.3 & 15535 & 2.04 & 0.79 & 2 \\ peptides-struct & 150.94 & 307.3 & 15535 & 2.04 & 0.79 & 2 \\ \hline \hline \end{tabular}
\end{table}
Table 4: The statistics of the datasets used in our experiments

### Preprocessing

We did not modify the features of the graph topology in any of the experiments except in the following cases:

* For purely topological datasets lacking node features, we assigned the zero vector as the initial node feature.
* For the large graphs "OGBN-arxiv" and "OGBN-Products," we undirected the graphs and then clustered them following [9]. We did it so we will be able to fit them into memory.
* In the ESAN experiments [3], we followed the preprocessing steps outlined by the authors, as these were integral parts of their suggested method. Specifically, we used the configuration that yielded the best empirical results reported in the paper, "DSS-GNN (GIN) (EGO+)."

### Tailoring GNN architectures to different benchmarks

In all our experiments, we used consistent architectures with minor variations tailored to each dataset. The primary difference is in the initial layer, which adapts to the specific characteristics of the dataset's node and edge features. This initial layer can be either an embedding layer or a linear layer designed to project the input into the network's hidden space. Following the initial layer, the architecture consists of stacked message-passing layers with residual connections and batch normalization layers. For graph-prediction benchmarks, a readout function is applied. Finally, a two-layer multilayer perceptron (MLP) with ReLU activation produces the output.

#### c.3.1 GraphGPS configuration

Due to the flexibility of GraphGPS and its numerous configuration options, we selected and focused on a specific setup. We utilized the Residual Gated Graph ConvNet from [5] as our convolution operator.For the attention module we used, dropout rate of 0.5 and 4 attention heads. For the experiments that uses Positional Encoding we used random walk with length 20. This configuration was taken from graph-gps repository. The full initialization details are available in our code.

### Parameter budget

We made our best effort to find the most common parameter budget for each benchmark. Specifically:

* For the ZINC [19] dataset, we employed a 100k parameter budget in order to obtain a fair comparison to previous works [12; 38; 3; 15].
* For the TUDatasets (MUTAG, ENZYMES, PROTEINS, IMDB-BINARY), where there is no consensus on the parameter budget, we used a 500,000 parameter budget. An exception was made for ESAN, for which we used a 100,000-parameter budget to allow fair comparison.
* For the OGB-N datasets, we used a 500,000 parameter budget. This is reasonable when considering the parameter counts of models on the ogbn-arxiv leaderboard and ogbn-products leaderboard.
* For OGB-G datasets, we used 500,000 parameters for ogbg-molhiv and 2,000,000 for ogbg-molpcba since its prediction task is more complex and contains 128 label prediction tasks. These are reasonable numbers as can be seen from ogbg-molhiv leaderboard & ogbg-molpcba leaderboard
* For the LRGB datasets, we also used a 2,000,000 parameter budget as those datasets require deeper GNNs because they need to embed long-range dependencies in the graph.

A summary of the parameter budget can be seen in Table 5

### Implementation details

We conduct our experiments using PyTorch Geometric as the underlying framework, running them on NVIDIA RTX A5000 GPUs. Detailed information on the selected hyperparameters for each dataset and layer configuration, along with instructions for reproducibility, can be found in our GitHub repository (https://almogdavid.github.io/SSMA/ ). For ESAN [3] and VPA [38], we integrated our SSMA directly into the official implementation shared by the authors (ESAN, VPA). We performed hyperparameter search only within the space the authors used, without altering the training or evaluation protocols.

### Hyper parameter search

We use the Weights & Biases platform to perform hyperparameter searches (HPS), aiming to identify the optimal configuration for each dataset. For each configuration, we determine the maximum hidden dimension that fits within the predetermined parameter budget.

* **MLP compression strength**: We search for the optimal compression rate of the low-rank MLP compressor rate. We define the compression rate \(\gamma\) as the ratio between the bottleneck rank and the inner aggregation representation dimension \(m\). We perform a simple range search over [0.1, 0.25, 0.5, 0.75, 1.0].
* **Neighbor selection method:** We search for the best neighbor selection method. We perform a simple range search over [random, attention_slots].
* **Effective neighborhood size**: we search for the optimal neighborhood size \(\kappa\). We perform a simple range search over [2,3,...,CLIP(max_neighbors,7)].

\begin{table}
\begin{tabular}{l c} \hline \hline Dataset & Parameter budget \\ \hline \hline ogbg-molhiv* & 500,000 \\ ogbg-molpcba & 2,000,000 \\ ogbn-arxiv & 500,000 \\ ogbn-products & 500,000 \\ mutag** & 500,000 \\ enzymes & 500,000 \\ proteins** & 500,000 \\ ptc-mr** & 500,000 \\ imdb-binary** & 500,000 \\ zinc & 100,000 \\ peptides-func & 2,000,000 \\ peptides-struct & 2,000,000 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Parameter budget used for each dataset. *for the ESAN, we used a 300,000 parameter budget.

Additional Experiments

### Comparison to variance preserving aggregation

We further evaluate our approach against a recently proposed method [39], we substitute the suggested aggregation technique with our own while retaining the original architecture and training protocol outlined in the work. We refer the reader to the original paper for an overview of architecture and training procedures. As illustrated in Table 6, our method demonstrates notable superiority over existing method without additional adjustments. Furthermore, optimizing hyperparameters and architecture selection has the potential for further enhancement.

### Comparison to Generalised f-Mean Aggregation

We conducted a further evaluation of our approach against a recently proposed aggregation function that parameterizes a function space encompassing all standard aggregators [25]. This aggregation was incorporated into our framework, and the experiments were performed using the identical setup described in Appendix C. For the new aggregation method, we employed the configuration specified by the authors in their experiments, as detailed in their repository). As shown in Appendix D.2, SSMA outperforms the proposed method. This demonstrates that even a method capable of learning a variety of aggregation functions experiences a relative decline in performance if it cannot effectively mix node features like SSMA. This underscores the critical importance of feature mixing in SSMA.

### Comparison to GraphGPS with Positionl-Encoding

Given the significant improvement in GraphGPS performance with positional encoding, we conducted additional experiments involving GraphGPS with positional encoding. The experiment details are provided in Appendix C. As shown in Appendix D.3, SSMA enhances GraphGPS performance even with positional encoding.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline
**Module** & **IMDB-B** & **IMDB-M** & **RDT-B** & **RDT-MSK** & **COLLAB** & **MTLG** & **PROTENS** & **PTC** & **NCII** \\ \hline GCN + VPR & 71.72.43 & 46.79.18 & 85.52.53 & 54.82.4 & 73.71.74 & 76.169.43 & 73.94.53 & 61.346.59 & 79.041.8 \\ GCN + SSMA & 71.72.43 & 49.38.78 & 87.263.53 & 83.24.24 & 76.44.33 & 81.928.43 & 75.764.54 & 66.835.88 & 78.248.8 \\ GAT + PSA & 71.714.48 & 40.54.58 & 87.263.53 & 85.284 & 69.949.21 & 83.381.58 & 75.664.42 & 60.843.88 & 76.422.8 \\ GAT + SSA & 70.683.43 & 50.864.58 & 80.663.54 & 76.374.51 & 88.341.58 & 76.664.49 & 60.741.38 & 86.29 \\ GCN + SSA & 72.048.48 & 87.44.22 & 80.643.54 & 76.374.44 & 87.241.5 & 77.245.44 & 60.741.38 & 81.241.2 \\ GIN + SSA & **73.112.93** & **49.726.07** & **89.463.51** & **57.725.45** & **74.084.27** & **87.721.77** & **73.935.53** & **64.122.23** & **81.723.31** \\ \hline \hline Avg. improvement (Si) & 5.19 & 7.80 & 2.93 & 6.74 & 3.88 & 7.85 & 2.68 & 7.32 & 3.88 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Test accuracy (higher is better). Shown is the mean \(\pm\) STD of 10-fold cross-validation runs, VPA results are taken directly from [39], SSMA results are generated by us using the code provided in [39] without any architecture or training protocol modifications

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline
**Module** & **ENZYMES \(\uparrow\)** & **PTC-MR \(\uparrow\)** & **MUTAG \(\uparrow\)** & **PROTEINS \(\uparrow\)** & **IMDB-B** \(\uparrow\) & **ZINC \(\downarrow\)** \\ \hline GCN\({}^{\dagger}\)[24] & 44.334.54 & 58.6146.83 & 84.1512.32 & 72.1344.42 & 69.0062.51 & 0.2940.01 \\ GCN + SSMA & **54.837.55** & **62.294.93** & **89.794.761** & **76.268.319** & **75.243.9** & **0.2809.02** \\ GAT [43] & 50.1774.60 & 59.1641.97 & 83.2510.53 & 73.754.54 & 68.106.49 & 0.3949.33 \\ GAT + SSMA & **56.673.72** & **66.145.09** & **89.194.58** & **80.186.1** & **74.545.44** & **0.2232.028** \\ GATv2\({}^{\dagger}\)[6] & 51.6748.05 & 57.4347.97 & 77.0512.75 & 70.9645.47 & 71.8459 & 0.2640.01 \\ GATv2\({}^{\ast}\)[83] & **52.508.03** & **61.646.80** & **88.801.190** & **75.284.80** & **72.848.92** & **0.32540.003** \\ GIN [48] & 15.332.45 & 57.0667.24 & 70.944.07 & 68.6247.66 & 57.684.1 & 0.27740.04 \\ GIN + SSMA & **51.693.04** & **61.2849.23** & **90.516.97** & **75.194.73** & **74.152.02** & **0.22240.003** \\ GraphGPS [37] & 20.3311.15 & 59.02610.28 & 55.7318.04 & 44.8511.03 & 53.556.35 & 0.2540.01 \\ GraphGPS + SSMA & **49.172.15** & **63.0249.38** & **86.077.295** & **75.5664.24** & **71.154.79** & **0.22240.005** \\ \hline Improvement (\%) & 83.45 & 7.92 & 22.22 & 19.83 & 16.26 & 17.13 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Results for TU datasets [32] & ZINC [19] using the aggregation from [25] aggregation as a baseline. We report the TU datasets’ accuracy mean and STD of a 10-fold cross-validation run. For the ZINC dataset, we report mean MAE and STD on the test set according to 5 distinct runs. \({}^{\dagger}\) indicates reproduced results while \({}^{\ast}\) indicates the reported results from the relevant paper.

## Appendix E Ablation studies

### Neighbor selection method

In this experiment, we compare the strategy of selecting random neighbors for each node with our proposed soft-neighbor selection mechanism. There are two main reasons for this comparison. i) To demonstrate that SSMA can establish strong aggregation capabilities independently of the aggregation occurring in the attention slots. ii) To provide empirical justification for the proposed soft-neighbor selection mechanism.

To achieve this, we conducted ablation experiments on two different datasets:

1. "OGBN-Arxiv," a citation network where most nodes have a very low in-degree, while a few nodes have an extremely high in-degree.
2. "Proteins," a dataset with an in-degree distribution highly concentrated around the mean.

For each one of these datasets, we compared the test accuracy using both neighbor selection methods for a varying number of neighbors and types of MPGNN layers. The results are presented in Figure 5.

The experiment results confirm that SSMA achieves strong performance even with the random neighbor selection and that the soft-neighbor selection works better in most cases. Interestingly, we observe that an increase in the number of neighbors does not necessarily correlate with improved performance, which was apparent most prominently in "OGBN-Arxiv."

Furthermore, we find that the GAT layer does not benefit from the proposed slot attention mechanism. This lack of improvement may be attributed to the intrinsic attention mechanism within the GAT architecture, rendering the additional attention mechanism redundant.

Figure 5: Comparison of the neighbor selection methods across different neighbor counts and MPGNN layer types on the ”OGBN-Arxiv” and ”Proteins” datasets.

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Dataset** & **GraphGPS** & **GraphGPS + \(\mathrm{SSMA}\)** & **GraphGPS + PE** & **GraphGPS + PE + \(\mathrm{SSMA}\)** \\ \hline ENZYMES \(\uparrow\) & 48.33\(\pm\)6.71 & **49.17\(\pm\)3.15** & 57.66\(\pm\)8.43 & **58.83\(\pm\)5.35** \\ PTC-MR \(\uparrow\) & 61.41\(\pm\)6.91 & **63.02\(\pm\)4.93** & 59.58\(\pm\)5.34 & **64.76\(\pm\)5.72** \\ MUTAG \(\uparrow\) & 79.91\(\pm\)0.23 & **86.07\(\pm\)9.95** & 90.34\(\pm\)7.78 & **91.37\(\pm\)5.76** \\ PROTENIS \(\uparrow\) & 73.76\(\pm\)0.65 & **75.65\(\pm\)6.24** & 73.57\(\pm\)7.41 & **76.02\(\pm\)2.37** \\ IMDB + P & 69.65\(\pm\)5.44 & **71.14\(\pm\)7.79** & 70.94\(\pm\)6 & **72.54\(\pm\)6.68** \\ ZINC \(\downarrow\) & 0.251\(\pm\)0.012 & **0.22\(\pm\)0.005** & 0.102\(\pm\)0.004 & **0.100\(\pm\)0.003** \\ PEPTIDES-F \(\uparrow\) & 58.81\(\pm\)1.22 & **0.34\(\pm\)1.49** & 57.71\(\pm\)0.86 & **59.87\(\pm\)7.72** \\ PEPTIDES-S \(\downarrow\) & 0.28\(\pm\)0.01 & **0.27\(\pm\)0.03** & 0.278\(\pm\)0.003 & **0.265\(\pm\)0.004** \\ OGBN-ARXIV \(\uparrow\) & 63.87\(\pm\)0.68 & **66.71\(\pm\)0.73** & 53.53\(\pm\)1.98 & **62.85\(\pm\)4.4** \\ OGBN-PRODUCTS \(\uparrow\) & 48.89\(\pm\)7.47 & **67.62\(\pm\)5.46** & 39.01\(\pm\)0.36 & **61.82\(\pm\)2.9** \\ OGBG-MOLHY \(\uparrow\) & 76.24\(\pm\)7.72 & **78.44\(\pm\)1.83** & 74.44\(\pm\)2.369 & **75.73\(\pm\)1.894** \\ OGBG-MOLPCBA \(\uparrow\) & 0.19\(\pm\)0.01 & **0.22\(\pm\)0.01** & 0.196\(\pm\)0.006 & **0.199\(\pm\)0.006** \\ \hline Improvement (\%) & \multicolumn{2}{c}{8.2} & \multicolumn{2}{c}{8.64} \\ \hline \hline \end{tabular}
\end{table}
Table 8: Results for GraphGPS with positional encoding, the aggregation used for the baselines is Add. See Appendix C for more information.

### Performance of \(\mathrm{SSMA}\) under different budget constraints

In this experiment, we explore the performance of \(\mathrm{SSMA}\) compared to sum-based aggregators across various hidden dimension sizes used by the MPGNN architectures. This investigation is motivated by two key objectives: i) To assess the relative gain of \(\mathrm{SSMA}\) over sum-based aggregators in both the low-budget and high-budget regimes. ii) To analyze how the scaling behavior of sum-based aggregators compares to that of \(\mathrm{SSMA}\).

We conducted ablation studies on the "IMDB-B" and "MUTAG" datasets to achieve these goals. For each dataset, we measured the test accuracy using both sum-based aggregators and \(\mathrm{SSMA}\), varying the hidden dimensions and types of MPGNN layers. The results of these experiments are illustrated in Figure 6.

The experimental results clearly indicate that \(\mathrm{SSMA}\) outperforms sum-based aggregators across all parameter regimes, showcasing its effectiveness in propagating relevant information for downstream tasks. Additionally, \(\mathrm{SSMA}\) does not always benefit from higher dimensionality, reaching saturation much earlier than its counterpart aggregators. This further demonstrates the efficiency of \(\mathrm{SSMA}\).

Figure 6: \(\mathrm{SSMA}\) achieves peak performance with significantly lower hidden dimensions.

### On the effectiveness of low-rank compressors

This experiment investigates the impact of low-rank MLP compression on the test performance of diverse architectures. As detailed in Section 4.4, low-rank compression significantly reduces learnable parameters while maintaining good expressive power. This allows for an intriguing trade-off: fewer parameters for a larger number of hidden units or slots. We evaluate this trade-off on the "OGBN-Arxiv" and "ZINC" datasets to understand its effect on performance.

The experiment results are presented in Figure 7.

As demonstrated above, we can observe that SSMA exhibits resilience to strong compression, maintaining high performance. Notably, the best results are achieved at compression strengths significantly lower than one, demonstrating the expressive power of the MLP compressor even under significant compression and the efficiency of SSMA in propagating information.

Figure 7: The performance under different compression strengths. SSMA can handle strong compression rates without losing much of its performance.

### Learning the affine transformation in \(\mathrm{SSMA}\)

To validate the selection of our affine transformation, we conducted an ablation study where we initialized the affine transformation with our proposed configuration and allowed the model to optimize it further. We used the ZINC dataset for this study and performed a brief hyperparameter search to identify the optimal configuration. The best results are presented in Table 9. The experiments show a slight improvement when the affine transformation is learned, but the gain is minimal, supporting the validity of our proposed affine transformation. Additionally, we examined the differences between the learned transformation and our proposed one. The learned transformation was similar to our proposal, with an average absolute difference of \(0.023\pm 0.019\), while the average norm of the affine weights is \(0.026\pm 0.057\).

\begin{table}
\begin{tabular}{l c c} \hline \hline Layer & Learnable Affine & Constant Affine \\ \hline GCN & \(\mathbf{0.2822\pm 0.008}\) & \(0.2836\pm 0.012\) \\ GAT & \(\mathbf{0.2278\pm 0.005}\) & \(0.2323\pm 0.003\) \\ GIN & \(\mathbf{0.2210\pm 0.004}\) & \(0.2260\pm 0.003\) \\ \hline \hline \end{tabular}
\end{table}
Table 9: Learning the affine transformation on the ZINC dataset

[MISSING_PAGE_EMPTY:29]

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline
**Module** & **ENZYMES \(\uparrow\)** & **PTC-MR \(\uparrow\)** & **MUTAG \(\uparrow\)** & **PROTEINS \(\uparrow\)** & **IMDB-B \(\uparrow\)** & **ZINC \(\downarrow\)** \\ \hline GCN’ [24] & 45.67±7.42 & 56.97±5.01 & 87.18±6.43 & 73.12±4.52 & 68.9±4.01 & 0.35±0.01 \\ GCN + SSMA & **54.83±7.58** & **62.29±±3.3** & **89.79±6.71** & **76.28±3.19** & **75.2±2.9** & **0.280±0.02** \\ GAT’ [43] & 46.33±96 & 56.53±9.89 & 80.68±2.92 & 71.60±4.12 & 65.8±3.99 & 0.27±0.01 \\ GAT+ SSMA & **56.67±3.72** & **66.41±5.69** & **89.19±4.58** & **80.18±0.14** & **74.5±4.14** & **0.223±0.02** \\ GATv2’ [6] & 41.5±18.11 & 57.72±7.09 & 82.91±2.96 & 70.43±4.21 & 64.9±4.98 & 0.27±0.01 \\ GATv2 + SSMA & **52.50±4.83** & **61.64±6.80** & **88.80±11.80** & **75.28±4.80** & **72.8±4.92** & **0.235±0.03** \\ GATv2 + SSMA & **52.50±4.83** & **61.64±6.80** & **88.80±11.80** & **75.28±4.80** & **72.8±4.92** & **0.235±0.03** \\ GATv2 + SSMA & **51.69±3±4** & **61.28±2.93** & **86.07±7.95** & **75.56±4.24** & **71.1±4.79** & **0.222±0.005** \\ \hline Improvement (\%) & 40.33 & 9.59 & 13.76 & 11.8 & 25.21 & 28.51 \\ \hline \hline \end{tabular}
\end{table}
Table 13: Results for TU datasets [32] & ZINC [19] using **min pooling** aggregation as a baseline. We report the TU datasets’ accuracy mean and STD of a 10-fold cross-validation run. For the ZINC dataset, we report the TU datasets’ accuracy mean and STD of a 10-fold cross-validation run. For the ZINC dataset, we report mean MAE and STD on the test set according to 5 distinct runs. \({}^{\dagger}\) indicates reproduced results while \({}^{*}\) indicates the reported results from the relevant paper.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline
**Module** & **ENZYMES \(\uparrow\)** & **PTC-MR \(\uparrow\)** & **MUTAG \(\uparrow\)** & **PROTEINS \(\uparrow\)** & **IMDB-B \(\uparrow\)** & **ZINC \(\downarrow\)** \\ \hline GCN’ [24] & 45.58±3±69 & 54.88±6.07 & 86.07±7.03 & 73.21±4.07 & 70.7±4±0.6 & 0.31±0.01 \\ GCN + SSMA & **54.83±7.56** & **62.29±±3.3** & **89.79±6.71** & **76.28±3.19** & **75.2±2.9** & **0.280±0.02** \\ GAT+ SSMA & **54.83±7.58** & **62.29±±3.3** & **89.79±6.71** & **76.28±3.19** & **75.2±2.9** & **0.280±0.02** \\ GAT+ SSMA & **54.83±7.58** & **62.29±±3.3** & **89.79±6.71** & **76.28±3.19** & **75.2±2.9** & **0.280±0.02** \\ \hline GCN’ [24] & 45.58±3±69 & 54.88±6.07 & 86.07±7.03 & 73.21±4.07 & 70.7±4.06 & 0.31±0.01 \\ GCN + SSMA & **54.83±7.56** & **62.29±±3.3** & **89.79±6.71** & **76.28±3.19** & **75.2±2.9** & **0.280±0.02** \\ GAT+ SSMA & **54.83±7.58** & **62.29±±3.3** & **89.79±6.71** & **76.28±3.19** & **75.2±2.9** & **0.280±0.02** \\ GAT+ SSMA & **51.69±3±4.8** & **61.28±3.23** & **89.79±6.71** & **76.28±3.19** & **75.2±2.9** & **0.22±0.003** \\ \hline \hline \end{tabular}
\end{table}
Table 12: Results for TU datasets [32] & ZINC [19] using **mean** aggregation as a baseline. We report the TU datasets’ accuracy mean and STD of a 10-fold cross-validation run. For the ZINC dataset, we report mean MAE and STD on the test set according to 5 distinct runs. \({}^{\dagger}\) indicates reproduced results while \({}^{*}\) indicates the reported results from the relevant paper.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline
**Module** & **ENZYMES \(\uparrow\)** & **PTC-MR \(\uparrow\)** & **MUTAG \(\uparrow\)** & **PROTEINS \(\uparrow\)** & **IMDB-B \(\uparrow\)** & **ZINC \(\downarrow\)** \\ \hline GCN’ [24] & 29.67±7.11 & 56.55±7.57 & 77.91±10.23 & 60.57±7.61 & 65.4±4.4 & **0.235±0.01** \\ GCN + SSMA & **54.83±7.58** & **62.29±3.3** & **89.79±6.71** & **76.28±3.19** & **75.2±2.9** & **0.280±0.02** \\ GAT+ SSMA & **54.83±7.58** & **62.29±3.3** & **89.79±6.71** & **76.28±3.19** & **75.2±2.9** & **0.280±0.02** \\ GAT+ SSMA & **54.83±7.58** & **62.29±3.3** & **89.79±6.71** & **76.28±3.19** & **75.2±2.9** & **0.280±0.02** \\ \hline GCN+ SSMA & **54.83±7.58** & **62.29±3.3** & **89.79±6.71** & **76.28±3.19** & **75.2±2.9** & **0.250±0.02** \\ \hline GCN+ SSMA & **54.83±7.58** & **62.29±3.3** & **89.79±6.71** & **76.28±3.19** & **75.2±2.9** & **0.250±0.02** \\ \hline \hline \end{tabular}
\end{table}
Table 14: Results for TU datasets [32] & ZINC [19] using **multiplication** aggregation as a baseline. We report the TU datasets’ accuracy mean and STD of a 10-fold cross-validation run. For the ZINC dataset, we report the TU datasets’ accuracy mean and STD on the test set according to 5 distinct runs. \({}^{\dagger}\) indicates reproduced results while \({}^{*}\) indicates the reported results from the relevant paper.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: In this paper, we introduce a novel aggregation method that enhances expressive power capabilities for various message-passing graph neural networks. We provide theoretical justification and conduct extensive experiments on multiple datasets and architectures. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We provided information on method limitations in section 4.4. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) the full set of assumptions and a complete?Answer: [Yes] Justification: In sections 3 and 4, where we discuss the proposed method's theoretical proposition, we provide a full set of assumptions and a complete proof. The appendix references part of the proofs. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide a fully detailed experiment setup in Section 5 and additional, comprehensive information on evaluations and datasets in the appendix. We also provided the code for our experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide the code of our experiments in the Abstract. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All implementation details for experiments have been provided in Section 5 and Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in the appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We reported the standard deviation error of the mean and all the results in section 5 and the appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Compute resources used are presented in Appendix C.5. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed the NeurIPS Code of Ethics and ensured our compliance with its guidelines Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: As the proposed methods improve the expressive power of existing techniques that operate on datasets with no direct social impact, we believe this work also does not have a direct social impact. Guidelines:* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: All the datasets used in this work are known and widely used. We are not aware of any risks involved in these datasets. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: In our code repository we share all the packages we used for our code, and we also specify the version of each package The repository is here:https://almogdavid.github.io/SSMA/ Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL.

* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We document our experiments thoroughly, anonymize the data, and provide the code. The code link is available in the Abstract. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Guidelines:* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.