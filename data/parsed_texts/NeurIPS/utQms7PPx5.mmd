# All Points Matter: Entropy-Regularized Distribution Alignment for Weakly-supervised 3D Segmentation

 Liyao Tang\({}^{1}\), Zhe Chen\({}^{2}\), Shanshan Zhao\({}^{1}\), Chaoyue Wang\({}^{1}\), Dacheng Tao\({}^{1}\)

\({}^{1}\) The University of Sydney, Australia \({}^{2}\) La Trobe University, Australia

ltan9687@uni.sydney.edu.au, zhe.chen@latrobe.edu.au

chaoyue.wang@outlook.com, {sshan.zhao00, dacheng.tao}@gmail.com

###### Abstract

Pseudo-labels are widely employed in weakly supervised 3D segmentation tasks where only sparse ground-truth labels are available for learning. Existing methods often rely on empirical label selection strategies, such as confidence thresholding, to generate beneficial pseudo-labels for model training. This approach may, however, hinder the comprehensive exploitation of unlabeled data points. We hypothesize that this selective usage arises from the noise in pseudo-labels generated on unlabeled data. The noise in pseudo-labels may result in significant discrepancies between pseudo-labels and model predictions, thus confusing and affecting the model training greatly. To address this issue, we propose a novel learning strategy to regularize the generated pseudo-labels and effectively narrow the gaps between pseudo-labels and model predictions. More specifically, our method introduces an Entropy Regularization loss and a Distribution Alignment loss for weakly supervised learning in 3D segmentation tasks, resulting in an ERDA learning strategy. Interestingly, by using KL distance to formulate the distribution alignment loss, it reduces to a deceptively simple cross-entropy-based loss which optimizes both the pseudo-label generation network and the 3D segmentation network simultaneously. Despite the simplicity, our method promisingly improves the performance. We validate the effectiveness through extensive experiments on various baselines and large-scale datasets. Results show that ERDA effectively enables the effective usage of all unlabeled data points for learning and achieves state-of-the-art performance under different settings. Remarkably, our method can outperform fully-supervised baselines using only 1% of true annotations. Code and model will be made publicly available at https://github.com/LiyaoTang/ERDA.

## 1 Introduction

Point cloud semantic segmentation is a crucial task for 3D scene understanding and has various applications [27, 83], such as autonomous driving, unmanned aerial vehicles, and augmented reality. Current state-of-the-art approaches heavily rely on large-scale and densely annotated 3D datasets, which are costly to obtain [21, 33]. To avoid demanding exhaustive annotation, weakly-supervised point cloud segmentation has emerged as a promising alternative. It aims to leverage a small set of annotated points while leaving the majority of points unlabeled in a large point cloud dataset for learning. Although current weakly supervised methods can offer a practical and cost-effective way to perform point cloud segmentation, their performance is still sub-optimal compared to fully-supervised approaches.

During the exploration of weak supervision [85], a significant challenge is the insufficient training signals provided by the highly sparse labels. To tackle this issue, pseudo-labeling methods [85, 94, 32] have been proposed, which leverage predictions on unlabeled points as labels to facilitate the learning of the segmentation network. Despite some promising results, these pseudo-label methods have been outperformed by some recent methods based on consistency regularization [47, 87]. We tend to attribute this to the use of label selection on pseudo-labels, such as confidence thresholding, whichcould lead to unlabeled points being wasted and under-explored. We hypothesize that the need for label selection arises from the low-confidence pseudo-labels assigned to unlabeled points, which are known for their noises [21] and potential unintended bias [60; 100]. These less reliable and noisy pseudo-labels could contribute to discrepancies between the pseudo-labels and the model predictions, which might confuse and impede the learning process to a great extent.

By addressing the above problem for label selection in weakly supervised 3D segmentation, we propose a novel learning-based approach in this study. Our method aims to leverage the information from all unlabeled points by mitigating the negative effects of the noisy pseudo-labels and the distributional discrepancy.

Specifically, we introduce two learning objectives for the pseudo-label generation process. Firstly, we introduce an _entropy regularization_ (ER) objective to reduce the noise and uncertainty in the pseudo-labels. This regularization promotes more informative, reliable, and confident pseudo-labels, which helps alleviate the limitations of noisy and uncertain pseudo-labels. Secondly, we propose a _distribution alignment_ (DA) loss that minimizes statistical distances between pseudo-labels and model predictions. This ensures that the distribution of generated pseudo-labels remains close to the distribution of model predictions when regularizing their entropy.

In particular, we discover that formulating the distribution alignment loss using KL distance enables a simplification of our method into a cross-entropy-style learning objective that optimizes both the pseudo-label generator and the 3D segmentation network simultaneously. This makes our method straightforward to implement and apply. By integrating the entropy regularization and distribution alignment, we achieve the ERDA learning strategy, as shown in Fig. 1.

Empirically, we comprehensively experiment with three baselines and different weak supervision settings, including 0.02% (1-point, or 1pt), 1%, and 10%. Despite its concise design, our ERDA outperforms existing weakly-supervised methods on large-scale point cloud datasets such as S3DIS [2], ScanNet [16], and SensatUrban [33]. Notably, our ERDA can surpass the fully-supervised baselines using only 1% labels, demonstrating its significant effectiveness in leveraging pseudo-labels. Furthermore, we validate the scalability of our method by successfully generalizing it to more other settings, which illustrates the benefits of utilizing dense pseudo-label supervision with ERDA.

## 2 Related Work

**Point cloud segmentation.** Point cloud semantic segmentation aims to assign semantic labels to 3D points. The cutting-edge methods are deep-learning-based and can be classified into projection-based and point-based approaches. Projection-based methods project 3D points to grid-like structures, such as 2D image [84; 55; 39; 12; 4; 45] or 3D voxels [15; 71; 67; 28; 22; 23; 76]. Alternatively, point-based

Figure 1: While existing pseudo-labels (a) are limited in the exploitation of unlabeled points, ERDA (b) simultaneously optimizes the pseudo-labels \(\mathbf{p}\) and predictions \(\mathbf{q}\) taking the same and simple form of cross-entropy. By reducing the noise via entropy regularization and bridging their distributional discrepancies, ERDA produces informative pseudo-labels that neglect the need for label selection. As in (c), it thus enables the model to consistently benefit from more pseudo-labels, surpasses other methods and its fully-supervised baseline, and can be extended to advance the fully-supervised performance.

methods directly operate on 3D points [56; 57]. Recent efforts have focused on novel modules and backbones to enhance point features, such as 3D convolution [3; 48; 78; 68; 51; 58], attentions [34; 26; 97; 79; 42; 59], graph-based methods [74; 44], and other modules such as sampling [18; 86; 88; 7] and post-processing [54; 35; 66]. Although these methods have made significant progress, they rely on large-scale datasets with point-wise annotation and struggle with few labels [85]. To address the demanding requirement of point-wise annotation, our work explores weakly-supervised learning for 3D point cloud segmentation.

**Weakly-supervised point cloud segmentation.** Compared to weakly-supervised 2D image segmentation [99; 49; 75; 1; 64], weakly-supervised 3D point cloud segmentation is less explored. In general, weakly-supervised 3D segmentation task focus on highly sparse labels: only a few scattered points are annotated in large point cloud scenes. Xu and Lee [85] first propose to use 10x fewer labels to achieve performance on par with a fully-supervised point cloud segmentation model. Later studies have explored more advanced ways to exploit different forms of weak supervision [77; 14; 40] and human annotations [53; 69]. Recent methods tend to introduce perturbed self-distillation [95], consistency regularization [85; 62; 80; 81; 43], and leverage self-supervised learning [62; 37; 47; 87] based on contrastive learning [29; 10]. Pseudo-labels are another approach to leverage unlabeled data, with methods such as pre-training networks on colorization tasks [94], using iterative training [32], employing separate networks to iterate between learning pseudo-labels and training 3D segmentation networks [53], or using super-point graph [44] with graph attentional module to propagate the limited labels over super-points [13]. However, these existing methods often require expensive training due to hand-crafted 3D data augmentations [95; 87; 80; 81], iterative training [53; 32], or additional modules [87; 32], complicating the adaptation of backbone models from fully-supervised to weakly-supervised learning. In contrast, our work aims to achieve effective weakly supervised learning for the 3D segmentation task with straightforward motivations and simple implementation.

**Pseudo-label refinement.** Pseudo-labeling [46], a versatile method for entropy minimization [24], has been extensively studied in various tasks, including semi-supervised 2D classification [82; 60], segmentation [64; 89], and domain adaptation [98; 73]. To generate high-quality supervision, various label selection strategies have been proposed based on learning status [72; 91; 20], label uncertainty [60; 98; 73; 50], class balancing [100], and data augmentations [64; 89; 100]. Our method is most closely related to the works addressing bias in supervision, where mutual learning [20; 70; 92] and distribution alignment [100; 31; 41] have been discussed. However, these works typically focus on class imbalance [100; 31] and rely on iterative training [70; 20; 92; 41], label selection [20; 31], and strong data augmentations [100; 31], which might not be directly applicable to 3D point clouds. For instance, common image augmentations [64] like cropping and resizing may translate to point cloud upsampling [96], which remains an open question in the related research area. Rather than introducing complicated mechanisms, we argue that proper regularization on pseudo-labels and its alignment with model prediction can provide significant benefits using a very concise learning approach designed for the weakly supervised 3D point cloud segmentation task.

Besides, it is shown that the data augmentations and repeated training in mutual learning [70; 38] are important to avoid the feature collapse, _i.e.,_ the resulting pseudo-labels being uniform or the same as model predictions. We suspect the cause may originate from the entropy term in their use of raw statistical distance by empirical results, which potentially matches the pseudo-labels to noisy and confusing model prediction, as would be discussed in Sec. 3.2. Moreover, in self-supervised learning based on clustering [5] and distillation [6], it has also been shown that it would lead to feature collapse if matching to a cluster assignment or teacher output of a close-uniform distribution with high entropy, which agrees with the intuition in our ER term.

## 3 Methodology

### Formulation of ERDA

As previously mentioned, we propose the ERDA approach to alleviate noise in the generated pseudo-labels and reduce the distribution gaps between them and the segmentation network predictions. In general, our ERDA introduces two loss functions, including the entropy regularization loss and the distribution alignment loss for the learning on pseudo-labels. We denote the two loss functions as \(L_{ER}\) and \(L_{DA}\), respectively. Then, we have the overall loss of ERDA as follows:

\[L_{p}=\lambda L_{ER}+L_{DA},\] (1)where the \(\lambda>0\) modulates the entropy regularization which is similar to the studies [46; 24].

Before detailing the formulation of \(L_{ER}\) and \(L_{DA}\), we first introduce the notation. While the losses are calculated over all unlabeled points, we focus on one single unlabeled point for ease of discussion. We denote the pseudo-label assigned to this unlabeled point as \(\mathbf{p}\) and the corresponding segmentation network prediction as \(\mathbf{q}\). Each \(\mathbf{p}\) and \(\mathbf{q}\) is a 1D vector representing the probability over classes.

**Entropy Regularization loss.** We hypothesize that the quality of pseudo-labels can be hindered by noise, which in turn affects model learning. Specifically, we consider that the pseudo-label could be more susceptible to containing noise when it fails to provide a confident pseudo-labeling result, which leads to the presence of a high-entropy distribution in \(\mathbf{p}\).

To mitigate this, for the \(\mathbf{p}\), we propose to reduce its noise level by minimizing its Shannon entropy, which also encourages a more informative labeling result [61]. Therefore, we have:

\[L_{ER}=H(\mathbf{p}),\] (2)

where \(H(\mathbf{p})=\sum_{i}-p_{i}\log p_{i}\) and \(i\) iterates over the vector. By minimizing the entropy of the pseudo-label as defined above, we promote more confident labeling results to help resist noise in the labeling process1.

Footnote 1: We note that our entropy regularization aims for entropy _minimization_ on pseudo-labels; and we consider noise as the uncertain predictions by the pseudo-labels, instead of incorrect predictions.

**Distribution Alignment loss.** In addition to the noise in pseudo-labels, we propose that significant discrepancies between the pseudo-labels and the segmentation network predictions could also confuse the learning process and lead to unreliable segmentation results. In general, the discrepancies can stem from multiple sources, including the noise-induced unreliability of pseudo-labels, differences between labeled and unlabeled data [100], and variations in pseudo-labeling methods and segmentation methods [92; 20]. Although entropy regularization could mitigate the impact of noise in pseudo-labels, significant discrepancies may still persist between the pseudo-labels and the predictions of the segmentation network. To mitigate this issue, we propose that the pseudo-labels and network can be jointly optimized to narrow such discrepancies, making generated pseudo-labels not diverge too far from the segmentation predictions. Therefore, we introduce the distribution alignment loss.

To properly define the distribution alignment loss (\(L_{DA}\)), we measure the KL divergence between the pseudo-labels (\(\mathbf{p}\)) and the segmentation network predictions (\(\mathbf{q}\)) and aim to minimize this divergence. Specifically, we define the distribution alignment loss as follows:

\[L_{DA}=KL(\mathbf{p}||\mathbf{q}),\] (3)

where \(KL(\mathbf{p}||\mathbf{q})\) refers to the KL divergence. Using the above formulation has several benefits. For example, the KL divergence can simplify the overall loss \(L_{p}\) into a deceptively simple form that demonstrates desirable properties and also performs better than other distance measurements. More details will be presented in the following sections.

**Simplified ERDA.** With the \(L_{ER}\) and \(L_{DA}\) formulated as above, given that \(KL(\mathbf{p}||\mathbf{q})=H(\mathbf{p},\mathbf{q})-H(\mathbf{p})\) where \(H(\mathbf{p},\mathbf{q})\) is the cross entropy between \(\mathbf{p}\) and \(\mathbf{q}\), we can have a simplified ERDA formulation as:

\[L_{p}=H(\mathbf{p},\mathbf{q})+(\lambda-1)H(\mathbf{p}).\] (4)

In particular, when \(\lambda=1\), we obtain the final ERDA loss2:

Footnote 2: We would justify the choice of \(\lambda\) in the following Sec. 3.2 as well as Sec. 4.3

\[L_{p}=H(\mathbf{p},\mathbf{q})=\sum_{i}-p_{i}\log q_{i}\] (5)

The above simplified ERDA loss describes that the entropy regularization loss and distribution alignment loss can be represented by a single cross-entropy-based loss that optimizes both \(\mathbf{p}\) and \(\mathbf{q}\).

We would like to emphasize that Eq. (5) is distinct from the conventional cross-entropy loss. The conventional cross-entropy loss utilizes a fixed label and only optimizes the term within the logarithm function, whereas the proposed loss in Eq. (5) optimizes both \(\mathbf{p}\) and \(\mathbf{q}\) simultaneously.

### Delving into the Benefits of ERDA

To formulate the distribution alignment loss, different functions can be employed to measure the differences between \(\mathbf{p}\) and \(\mathbf{q}\). In addition to the KL divergence, there are other distance measurements like mean squared error (MSE) or Jensen-Shannon (JS) divergence for replacement. Although many mutual learning methods [20; 92; 41; 38] have proven the effectiveness of KL divergence, a detailed comparison of KL divergence against other measurements is currently lacking in the literature. In this section, under the proposed ERDA learning framework, we show by comparison that \(KL(\mathbf{p}||\mathbf{q})\) is a better choice and ER is necessary for weakly-supervised 3D segmentation.

To examine the characteristics of different distance measurements, including \(KL(\mathbf{p}||\mathbf{q})\), \(KL(\mathbf{q}||\mathbf{p})\), \(JS(\mathbf{p}||\mathbf{q})\), and \(MSE(\mathbf{p}||\mathbf{q})\), we investigate the form of our ERDA loss \(L_{p}\) and its impact on the learning for pseudo-label generation network given two situations during training.

More formally, we shall assume a total of \(K\) classes and define that a pseudo-label \(\mathbf{p}=[p_{1},...,p_{K}]\) is based on the confidence scores \(\mathbf{s}=[s_{1},...,s_{K}]\), and that \(\mathbf{p}=\text{softmax}(\mathbf{s})\). Similarly, we have a segmentation network prediction \(\mathbf{q}=[q_{1},...,q_{K}]\) for the same point. We re-write the ERDA loss \(L_{p}\) in various forms and investigate the learning from the perspective of gradient update, as in Tab. 1.

**Situation 1: Gradient update given confident pseudo-label \(\mathbf{p}\).** We first specifically study the case when \(\mathbf{p}\) is very certain and confident, _i.e.,_\(\mathbf{p}\) approaching a one-hot vector. As in Tab. 1, most distances yield the desired zero gradients, which thus retain the information of a confident and reliable \(\mathbf{p}\). In this situation, however, the \(KL(\mathbf{q}||\mathbf{p})\), rather than \(KL(\mathbf{p}||\mathbf{q})\) in our method, produces non-zero gradients that would actually increase the noise among pseudo-labels during its learning, which is not favorable according to our motivation.

**Situation 2: Gradient update given confusing prediction \(\mathbf{q}\).** In addition, we are also interested in how different choices of distance and \(\lambda\) would impact the learning on pseudo-label if the segmentation model produces confusing outputs, _i.e.,_\(\mathbf{q}\) tends to be uniform. In line with the motivation of ERDA learning, we aim to regularize the pseudo-labels to mitigate potential noise and bias, while discouraging uncertain labels with little information. However, as in Tab. 1, most implementations yield non-zero gradient updates to the pseudo-label generation network. This update would make \(\mathbf{p}\) closer to the confused \(\mathbf{q}\), thus increasing the noise and degrading the training performance. Conversely, only \(KL(\mathbf{p}||\mathbf{q})\) can produce a zero gradient when integrated with the entropy regularization with \(\lambda=1\). That is, only ERDA in Eq. (5) would not update the pseudo-label generation network when \(\mathbf{q}\) is not reliable, which avoids confusing the \(\mathbf{p}\). Furthermore, when \(\mathbf{q}\) is less noisy but still close to a uniform vector, it is indicated that there is a large close-zero plateau on the gradient surface of ERDA, which benefits the learning on \(\mathbf{p}\) by resisting the influence of noise in \(\mathbf{q}\).

In addition to the above cases, the gradients of ERDA in Eq. (5) could be generally regarded as being aware of the noise level and the confidence of both pseudo-label \(\mathbf{p}\) and the corresponding prediction \(\mathbf{q}\). Especially, ERDA produces larger gradient updates on noisy pseudo-labels, while smaller updates on confident and reliable pseudo-labels or given noisy segmentation prediction. Therefore, our formulation demonstrates its superiority in fulfilling our motivation of simultaneous noise reduction and distribution alignment, where both \(L_{ER}\) and KL-based \(L_{DA}\) are necessary. We provide more empirical studies in ablation (Sec. 4.3) and detailed analysis in the supplementary.

### Implementation Details on Pseudo-Labels

In our study, we use a prototypical pseudo-label generation process due to its popularity as well as simplicity [94]. Specifically, prototypes [63] denote the class centroids in the feature space, whichare calculated based on labeled data, and pseudo-labels are estimated based on the feature distances between unlabeled points and class centroids.

As shown in Fig. 2, we use a momentum-based prototypical pseudo-label generation process due to its popularity as well as simplicity [94; 85; 93]. Specifically, prototypes [63] denote the class centroids in the feature space, which are calculated based on labeled data, and pseudo-labels are estimated based on the feature distances between unlabeled points and class centroids. To avoid expensive computational costs and compromised representations for each semantic class [94; 87; 47], momentum update is utilized as an approximation for global class centroids.

Based on the momentum-updated prototypes, we attach an MLP-based projection network to help generate pseudo-labels and learn with our method. Aligned with our motivation, we do not introduce thresholding-based label selection or one-hot conversion [46; 94] to process generated pseudo-labels. More details are in the supplementary.

More formally, we take as input a point cloud \(\mathcal{X}\), where the labeled points are \(\mathcal{X}^{l}\) and the unlabeled points are \(\mathcal{X}^{u}\). For a labeled point \(\mathbf{x}\in\mathcal{X}^{l}\), we denote its label by \(y\). The pseudo-label generation process can be described as follows:

\[\hat{C}_{k}=\frac{1}{N_{k}^{l}}\sum_{\mathbf{x}\in\mathcal{X}^{l }\wedge y=k}g\circ f(\mathbf{x})\;,\;C_{k}\gets mC_{k}+(1-m)\hat{C}_{k},\] \[\forall\mathbf{x}\in\mathcal{X}^{u}\;,\;s_{k}=d(g\circ f( \mathbf{x}),C_{k})\;,\;\mathbf{p}=\text{softmax}(\mathbf{s}),\]

where \(C_{k}\) denotes the global class centroid for the \(k\)-th class, \(N_{k}^{l}\) is the number of labeled points of the \(k\)-th class, \(g\circ f=g(f(\cdot))\) is the transformation through the backbone network \(f\) and the projection network \(g\), \(m\) is the momentum coefficient, and we use cosine similarity for \(d(\cdot,\cdot)\) to generate the score \(\mathbf{s}\). By default, we use 2-layer MLPs for the projection network \(g\) and set \(m=0.999\).

Besides, due to the simplicity of ERDA, we are able to follow the setup of the baselines for training, which enables straightforward implementation and easy adaptation on various backbone models with little overhead.

**Overall objective.** Finally, with ERDA learning in Eq. (5), we maximize the same loss for both labeled and unlabeled points, segmentation task, and pseudo-label generation, where we allow the gradient to back-propagate through the (pseudo-)labels. The final loss is given as

\[L=\frac{1}{N^{l}}\sum_{\mathbf{x}\in\mathcal{X}^{l}}L_{ce}(\mathbf{q},y)+ \alpha\frac{1}{N^{u}}\sum_{\mathbf{x}\in\mathcal{X}^{u}}L_{p}(\mathbf{q}, \mathbf{p}),\] (6)

where \(L_{p}(\mathbf{q},\mathbf{p})=L_{ce}(\mathbf{q},\mathbf{p})=H(\mathbf{q}, \mathbf{p})\) is the typical cross-entropy loss used for point cloud segmentation, \(N^{l}\) and \(N^{u}\) are the numbers of labeled and unlabeled points, and \(\alpha\) is the loss weight.

## 4 Experiments

We present the benefits of our proposed ERDA by experimenting with multiple large-scale datasets, including S3DIS [2], ScanNet [16], SensatUrban [33] and Pascal [19]. We also provide ablation studies for better investigation.

Figure 2: Detailed illustration of our ERDA with the prototypical pseudo-label generation process, which is shared for both (a) and (b) in Fig. 1.

### Experimental Setup

We choose RandLA-Net [34] and CloserLook3D [51] as our primary baselines following previous works. Additionally, while transformer models [17; 52] have revolutionized the field of computer vision as well as 3D point cloud segmentation [97; 42], none of the existing works have addressed the training of transformer for point cloud segmentation with weak supervision, even though these models are known to be data-hungry [17]. We thus further incorporate the PointTransformer (PT) [97] as our baseline to study the amount of supervision demanded for effective training of transformer.

For training, we follow the setup of the baselines and set the loss weight \(\alpha=0.1\). For a fair comparison, we follow previous works [94; 95; 32] and experiment with different settings, including the 0.02% (1pt), 1% and 10% settings, where the available labels are randomly sampled according to the ratio3. More details are given in the supplementary.

Footnote 3: Some super-voxel-based approaches, such as OTOC [53], additionally leverage the super-voxel partition from the dataset annotations [32]. We thus treat them as a different setting and avoid direct comparison.

### Performance Comparison

**Results on S3DIS.** S3DIS [2] is a large-scale point cloud segmentation dataset that covers 6 large indoor areas with 272 rooms and 13 semantic categories. As shown in Tab. 2, ERDA significantly improves over different baselines on all settings and almost all classes. In particular, for confusing classes such as column, window, door, and board, our method provides noticeable and consistent improvements in all weak supervision settings. We also note that PT suffers from severe over-fitting and feature collapsing under the supervision of extremely sparse labels of "1pt" setting; whereas it is alleviated with ERDA, though not achieving a satisfactory performance. Such observation agrees with the understanding that transformer is data-hungry [17].

Impressively, ERDA yields competitive performance against most supervised methods. For instance, with only \(1\%\) of labels, it achieves performance better than its stand-alone baselines with full supervision. Such result indicates that the ERDA is more successful than expected in alleviating the lack of training signals, as also demonstrated qualitatively in Fig. 3.

Therefore, we further extend the proposed method to fully-supervised training, _i.e.,_ in setting "Fully" in Tab. 2. More specifically, we generate pseudo-labels for all points and regard the ERDA as an auxiliary loss for fully-supervised learning. Surprisingly, we observe non-trivial improvements (+3.7 for RandLA-Net and +3.4 for CloserLook3D) and achieve the state-of-the-art performance of 72.6 (+2.2) in mIoU with PT. We suggest that the improvements are due to the noise-aware learning from ERDA, which gradually reduces the noise during the model learning and demonstrates to be generally effective. Moreover, considering that the ground-truth labels could suffer from the problem of label noise [38, 90, 65], we also hypothesize that pseudo-labels from ERDA learning could stabilize fully-supervised learning and provide unexpected benefits.

We also conduct the 6-fold cross-validation, as reported in Tab. 3. We find our method achieves a leading performance among both weakly and fully-supervised methods, which further validates the effectiveness of our method.

**Results on ScanNet.** ScanNet [16] is an indoor point cloud dataset that covers 1513 training scenes and 100 test scenes with 20 classes. In addition to the common settings, _e.g.,_ 1% labels, it also provides official data efficient settings, such as 20 points, where for each scene there are a pre-defined

\begin{table}
\begin{tabular}{c|c|c|c} \hline settings & methods & mIoU \\ \hline \multirow{5}{*}{Fully} & PointNet [56] & 23.7 \\  & RandLA-Net [34] & 64.5 \\  & RCPonv [68] & 76.6 \\  & HybridCR [47] & 70.7 \\  & \(\text{TP}\) [79] & 73.5 \\  & PointNet\(\times\) [74] & 74.4 \\  & RandLA-Net + **ERDA** & 57.0 \\ \hline \multirow{5}{*}{1\%} & \multirow{5}{*}{20pts} & All-Trans [87] & 54.4 \\  & CloserLook3D + **ERDA** & 57.0 \\ \cline{1-1}  & CloserLook3D + **ERDA** & 57.0 \\ \cline{1-1}  & & \(\text{SQN}\) [32] & 54.0 \\ \cline{1-1}  & & RandLA-Net + **ERDA** & 56.4 \\ \cline{1-1} \cline{2-4}  & & \(\text{Zhang}\) [41] & 51.1 \\ \cline{1-1}  & PSD [95] & 68.0 \\ \cline{1-1}  & HybridCR [47] & 69.2 \\ \cline{1-1}  & RandLA-Net + **ERDA** & 69.4 \\ \cline{1-1}  & CloserLook3D + **ERDA** & 72.3 \\ \cline{1-1} \cline{2-4}  & & \(\text{TP}\) **+ ERDA** & 73.5 \\ \hline \end{tabular}
\end{table}
Table 4: Results on ScanNet test.

Figure 3: We show obvious improvement of our ERDA over baseline (RandLA-Net) on different scenes from S3DIS Area 5. In the office and hallway (top 2), ERDA produces more detailed and complete segmentation for windows and doors, and avoids over-expansion of the board and bookcase on the wall, thanks to the informative pseudo-labels. In more cluttered scenes (bottom 2), ERDA tends to make cleaner predictions by avoiding improper situations such as desk inside clutter and preserving important semantic classes such as columns.

\begin{table}
\begin{tabular}{c|c|c|c} \hline settings & methods & mIoU \\ \hline \multirow{5}{*}{Fully} & PointNet [56] & 23.7 \\  & RandLA-Net [34] & 64.5 \\  & RCPonv [68] & 76.6 \\  & HybridCR [47] & 70.7 \\  & \(\text{TP}\) [79] & 73.5 \\  & PointNet\(\times\) [74] & 74.4 \\  & RandLA-Net + **ERDA** & 71.0 \\  & CloserLook3D + **ERDA** & 73.7 \\ \hline \multirow{5}{*}{1\%} & \multirow{5}{*}{20pts} & All-Trans [87] & 54.4 \\  & CloserLook3D + **ERDA** & 57.0 \\ \cline{1-1}  & & \(\text{SQN}\) [32] & 54.0 \\ \cline{1-1}  & & RandLA-Net + **ERDA** & 56.4 \\ \cline{1-1} \cline{2-4}  & & \(\text{Zhang}\) [41] & 53.9 \\ \cline{1-1}  & PSD [95] & 68.0 \\ \cline{1-1}  & HybridCR [47] & 69.2 \\ \cline{1-1}  & RandLA-Net + **ERDA** & 69.4 \\ \cline{1-1}  & CloserLook3D + **ERDA** & 72.3 \\ \cline{1-1} \cline{2-4}  & & \(\text{TP}\) **+ ERDA** & 73.5 \\ \hline \end{tabular}
\end{table}
Table 5: Results on SensatUrban test.

set of 20 points with the ground truth label. We evaluate on both settings and report the results in Tab. 4. We largely improve the performance under \(0.1\%\) and \(1\%\) labels. In 20pts setting, we also employ a convolutional baseline (CloserLook3D) for a fair comparison. With no modification on the model, we surpass MIL-transformer [87] that additionally augments the backbone with transformer modules and multi-scale inference. Besides, we apply ERDA to baseline under fully-supervised setting and achieve competitive performance. These results also validate the ability of ERDA in providing effective supervision signals.

**Results on SensatUrban.** SensatUrban [33] is an urban-scale outdoor point cloud dataset that covers the landscape from three UK cities. In Tab. 5, ERDA surpasses SQN [32] under the same 0.1% setting as well as its fully-supervised baseline, and also largely improves under full supervision. It suggests that our method can be robust to different types of datasets and effectively exploits the limited annotations as well as the unlabeled points.

**Generalizing to 2D Pascal.** As our ERDA does not make specific assumptions on the 3D data, we explore its potential in generalizing to similar 2D settings. Specifically, we study an important task of semi-supervised segmentation on image [72; 89; 8] and implement our method to the popular baseline, FixMatch [64], which combines the pseudo-labels with weak-to-strong consistency and is shown to benefit from stronger augmentation [89]. We use DeepLabv3+ [9] with ResNet-101 [30].

As in Tab. 6, we show that ERDA brings consistent improvement from low to high data regime, despite the existence of strong data augmentation and the very different data as well as setting4. It thus indicates the strong generalization of our method. We also see that the improvement is less significant than the 3D cases. It might be because 2D data are more structured (_e.g.,_ pixels on a 2D grid) and are thus less noisy than the 3D point cloud.

Footnote 4: We note that the number in the first row in Tab. 6 denotes the number of labeled images, which is different from the setting of sparse labels in 3D point clouds.

### Ablations and Analysis

We mainly consider the 1% setting and ablates in Tab. 7 to better investigate ERDA and make a more thorough comparison with the current pseudo-label generation paradigm. For more studies on hyper-parameters, please refer to the supplementary.

**Individual effectiveness of ER and DA.** To validate our initial hypothesis, we study the individual effectiveness of \(L_{ER}\) and \(L_{DA}\) in Tab. 6(a). While the pseudo-labels essentially improve the baseline performance, we remove its label selection and one-hot conversion when adding the ER or DA term. We find that using ER alone can already be superior to the common pseudo-labels and largely reduce the entropy of pseudo-labels (ent.) as expected, which verifies that the pseudo-labels are noisy, and reducing these noises could be beneficial. The improvement with the DA term alone is even more significant, indicating that a large discrepancy is indeed existing between the pseudo-labels and model prediction and is hindering the model training. Lastly, by combining the two terms, we obtain the ERDA that reaches the best performance but with the entropy of its pseudo-labels larger than ER only and smaller than DA only. It thus also verifies that the DA term could be biased to uniform distribution and that the ER term is necessary.

**Different choices of ER and DA.** Aside from the analysis in Sec. 3.2, we empirically compare the results under different choices of distance for \(L_{DA}\) and \(\lambda\) for \(L_{ER}\). As in Tab. 6(b), the outstanding result justifies the choice of \(KL(\mathbf{q}||\mathbf{p})\) with \(\lambda=1\). Additionally, all different choices and combina

\begin{table}

\end{table}
Table 7: Ablations on ERDA. If not specified, the model is RandLA-Net trained with ERDA as well as dense pseudo-labels on S3DIS under the 1% setting and reports in mIoU. Default settings are marked in gray.

tions of ER and DA terms improve over the common pseudo-labels (63.3), which also validates the general motivation for ERDA.

**Ablating label selection.** We explore in more detail how the model performance is influenced by the amount of exploitation on unlabeled points, as ERDA learning aims to enable full utilization of the unlabeled points. In particular, we consider three pseudo-labels types: common one-hot pseudo-labels, soft pseudo-labels (\(\mathbf{p}\)), and soft pseudo-labels with ERDA learning. To reduce the number of pseudo-labels, we select sparse but high-confidence pseudo-labels following a common top-\(k\) strategy [94, 53] with various values of \(k\) to study its influence. As in Tab. 6(c), ERDA learning significantly improves the model performance under all cases, enables the model to consistently benefit from more pseudo-labels, and thus neglects the need for label selection such as top-\(k\) strategy, as also revealed in Fig. 1. Besides, using soft pseudo-labels alone can not improve but generally hinders the model performance, as one-hot conversion may also reduce the noise in pseudo-labels, which is also not required with ERDA.

## 5 Limitation and Future Work

While we mostly focus on weak supervision in this paper, our method also brings improvements under fully-supervised setting. We would then like to further explore the effectiveness and relationship of \(L_{ER}\) and \(L_{DA}\) under full supervision as a future work. Besides, despite promising results, our method, like other weak supervision approaches, assumes complete coverage of semantic classes in the available labels, which may not always hold in real-world cases. Point cloud segmentation with missing or novel classes should be explored as an important future direction.

## 6 Conclusion

In this paper, we study the weakly-supervised 3D point cloud semantic segmentation, which imposes the challenge of highly sparse labels. Though pseudo-labels are widely used, label selection is commonly employed to overcome the noise, but it also prevents the full utilization of unlabeled data. By addressing this, we propose a new learning scheme on pseudo-labels, ERDA, that reduces the noise, aligns to the model prediction, and thus enables comprehensive exploitation of unlabeled data for effective training. Experimental results show that ERDA outperforms previous methods in various settings and datasets. Notably, it surpasses its fully-supervised baselines and can be further generalized to full supervision as well as 2D images.

**Acknowledgement.** This project is supported in part by ARC FL-170100117, and IC-190100031.

## References

* [1] Jiwoon Ahn and Suha Kwak. Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation. _2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4981-4990, 2018.
* [2] Iro Armeni, Sasha Sax, Amir R Zamir, and Silvio Savarese. Joint 2d-3d-semantic data for indoor scene understanding. _arXiv preprint arXiv:1702.01105_, 2017.
* [3] Alexandre Boulch, Gilles Puy, and Renaud Marlet. Fkaconv: Feature-kernel alignment for point cloud convolution, 2020.
* [4] A. Boulch, B. Le Saux, and N. Audebert. Unstructured point cloud semantic labeling using deep segmentation networks. In _Proceedings of the Workshop on 3D Object Retrieval_, 3Dor '17, page 17-24, Goslar, DEU, 2017. Eurographics Association.
* [5] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. _ArXiv_, abs/2006.09882, 2020.
* [6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. _2021 IEEE/CVF International Conference on Computer Vision (ICCV)_, Oct 2021.
* [7] Chen Chen, Zhe Chen, Jing Zhang, and Dacheng Tao. Sasa: Semantics-augmented set abstraction for point-based 3d object detection. In _AAAI_, 2022.
* [8] Hao Chen, Ran Tao, Yue Fan, Yidong Wang, Jindong Wang, Bernt Schiele, Xing Xie, Bhiksha Raj, and Marios Savvides. Softmatch: Addressing the quantity-quality trade-off in semi-supervised learning. 2023.

* [9] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In _European Conference on Computer Vision_, 2018.
* [10] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. _arXiv preprint arXiv:2002.05709_, 2020.
* [11] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big self-supervised models are strong semi-supervised learners. _arXiv preprint arXiv:2006.10029_, 2020.
* [12] Zhe Chen, Jing Zhang, and Dacheng Tao. Progressive lidar adaptation for road detection. _IEEE/CAA Journal of Automatica Sinica_, 6:693-702, 05 2019.
* [13] Mingmei Cheng, Le Hui, Jin Xie, and Jian Yang. Sspc-net: Semi-supervised semantic 3d point cloud segmentation network. 2021.
* ECCV 2022_, pages 681-699, 2022.
* [15] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4d spatio-temporal convnets: Minkowski convolutional neural networks. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 3075-3084, 2019.
* [16] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Niessner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In _Proc. Computer Vision and Pattern Recognition (CVPR), IEEE_, 2017.
* [17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. _ICLR_, 2021.
* [18] Oren Dovrat, Itai Lang, and Shai Avidan. Learning to sample. _2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, Jun 2019.
* [19] M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal visual object classes challenge: A retrospective. _International Journal of Computer Vision_, 111(1):98-136, Jan. 2015.
* [20] Zhengyang Feng, Qianyu Zhou, Qiqi Gu, Xin Tan, Guangliang Cheng, Xuequan Lu, Jianping Shi, and Lizhuang Ma. Dmt: Dynamic mutual training for semi-supervised learning. _Pattern Recognition_, 130:108777, Oct 2022.
* [21] Biao Gao, Yancheng Pan, Chengkun Li, Sibo Geng, and Huijing Zhao. Are we hungry for 3d lidar data for semantic segmentation? a survey of datasets and methods. _IEEE Transactions on Intelligent Transportation Systems_, 23(7):6063-6081, Jul 2022.
* [22] Benjamin Graham, Martin Engelcke, and Laurens van der Maaten. 3d semantic segmentation with submanifold sparse convolutional networks. _2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition_, Jun 2018.
* [23] Benjamin Graham and Laurens van der Maaten. Submanifold sparse convolutional networks. _ArXiv_, abs/1706.01307, 2017.
* [24] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In L. Saul, Y. Weiss, and L. Bottou, editors, _Advances in Neural Information Processing Systems_, volume 17. MIT Press, 2004.
* [25] Jean-Bastien Grill, Florian Strub, Florent Altch'e, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap your own latent: A new approach to self-supervised learning. _ArXiv_, abs/2006.07733, 2020.
* [26] Meng-Hao Guo, Junxiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Ralph R. Martin, and Shi-Min Hu. PCT: point cloud transformer. _CoRR_, abs/2012.09688, 2020.
* [27] Yulan Guo, Hanyun Wang, Qingyong Hu, Hao Liu, Li Liu, and Mohammed Bennamoun. Deep learning for 3d point clouds: A survey. _IEEE transactions on pattern analysis and machine intelligence_, 2020.
* [28] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance segmentation. _2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 2937-2946, 2020.
* [29] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. _arXiv preprint arXiv:1911.05722_, 2019.
* [30] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 770-778, Los Alamitos, CA, USA, jun 2016. IEEE Computer Society.
* [31] Ruifei He, Jihan Yang, and Xiaojuan Qi. Re-distributing biased pseudo labels for semi-supervised semantic segmentation: A baseline investigation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 6930-6940, 2021.

* [32] Qingyong Hu, Bo Yang, Guangchi Fang, Yulan Guo, Ales Leonardis, Niki Trigoni, and Andrew Markham. Sqn: Weakly-supervised semantic segmentation of large-scale 3d point clouds. In _European Conference on Computer Vision_, 2022.
* [33] Qingyong Hu, Bo Yang, Sheikh Khalid, Wen Xiao, Niki Trigoni, and Andrew Markham. Sensaturban: Learning semantics from urban-scale photogrammetric point clouds. _International Journal of Computer Vision_, 130(2):316-343, 2022.
* [34] Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan Guo, Zhihua Wang, Niki Trigoni, and Andrew Markham. Randla-net: Efficient semantic segmentation of large-scale point clouds. _CoRR_, abs/1911.11236, 2019.
* [35] Zeyu Hu, Mingmin Zhen, Xuyang Bai, Hongbo Fu, and Chiew-Lan Tai. Jsenet: Joint semantic segmentation and edge detection network for 3d point clouds. _CoRR_, abs/2007.06888, 2020.
* [36] Zhuo Huang, Zhiyou Zhao, Banghuai Li, and Jungong Han. Lcoformeter: Towards effective 3d point cloud analysis via local context propagation in transformers. _ArXiv_, abs/2210.12755, 2022.
* [37] Li Jiang, Shaoshuai Shi, Zhuotao Tian, Xin Lai, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Guided point contrastive learning for semi-supervised point cloud semantic segmentation. In _ICCV_, pages 6403-6412, 10 2021.
* [38] Yi Kun and Wu Jianxin. Probabilistic End-to-end Noise Correction for Learning with Noisy Labels. In _The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2019.
* [39] Abhijit Kundu, Xiaoqi Yin, Alireza Fathi, David A. Ross, Brian Brewington, Thomas A. Funkhouser, and Caroline Pantofaru. Virtual multi-view fusion for 3d semantic segmentation. In _ECCV_, 2020.
* [40] Hyeokjun Kweon and Kuk-Jin Yoon. Joint learning of 2d-3d weakly supervised semantic segmentation. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [41] Donghyeon Kwon and Suha Kwak. Semi-supervised semantic segmentation with error localization network. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 9957-9967, June 2022.
* [42] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi, and Jiaya Jia. Stratified transformer for 3d point cloud segmentation. _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 8490-8499, 2022.
* [43] Yuxiang Lan, Yachao Zhang, Yanyun Qu, Cong Wang, Chengyang Li, Jia Cai, Yuan Xie, and Zongze Wu. Weakly supervised 3d segmentation via receptive-driven pseudo label consistency and structural consistency. _Proceedings of the AAAI Conference on Artificial Intelligence_, 37(1):1222-1230, Jun. 2023.
* [44] L. Landrieu and M. Simonovsky. Large-scale point cloud semantic segmentation with superpoint graphs. In _2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4558-4567, 2018.
* [45] Felix Jaremo Lawin, Martin Danelljan, Patrik Tosteberg, Goutam Bhat, Fahad Shahbaz Khan, and Michael Felsberg. Deep projective 3d semantic segmentation. _Lecture Notes in Computer Science_, page 95-107, 2017.
* [46] Dong-Hyun Lee. Pseudo-label : The simple and efficient semi-supervised learning method for deep neural networks. 2013.
* [47] Mengtian Li, Yuan Xie, Yunhang Shen, Bo Ke, Ruizhi Qiao, Bo Ren, Shaohui Lin, and Lizhuang Ma. Hybrider: Weakly-supervised 3d point cloud semantic segmentation via hybrid contrastive regularization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 14930-14939, June 2022.
* [48] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. Pointcnn: Convolution on x-transformed points. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018.
* [49] Di Lin, Jifeng Dai, Jiaya Jia, Kaiming He, and Jian Sun. Scribblesup: Scribble-supervised convolutional networks for semantic segmentation. _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 3159-3167, 2016.
* [50] Yuyuan Liu, Yu Tian, Yuanhong Chen, Fengbei Liu, Vasileios Belagiannis, and Gustavo Carneiro. Perturbed and strict mean teachers for semi-supervised semantic segmentation. _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, Jun 2022.
* [51] Ze Liu, Han Hu, Yue Cao, Zheng Zhang, and Xin Tong. A closer look at local aggregation operators in point cloud analysis. _ECCV_, 2020.
* [52] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. _arXiv preprint arXiv:2103.14030_, 2021.
* [53] Zhengzhe Liu, Xiaojuan Qi, and Chi-Wing Fu. One thing one click: A self-training approach for weakly supervised 3d semantic segmentation. _CoRR_, abs/2104.02246, 2021.

* [54] Tao Lu, Limin Wang, and Gangshan Wu. Cga-net: Category guided aggregation for point cloud semantic segmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 11693-11702, June 2021.
* [55] A. Milioto, I. Vizzo, J. Behley, and C. Stachniss. Rangenet++: Fast and accurate lidar semantic segmentation. In _2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 4213-4220, 2019.
* [56] Charles Ruizhongtai Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. _CoRR_, abs/1612.00593, 2016.
* [57] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J. Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. _CoRR_, abs/1706.02413, 2017.
* [58] Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, Hasan Hammoud, Mohamed Elhoseiny, and Bernard Ghanem. Pointnet: Revisiting pointnet++ with improved training and scaling strategies. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [59] Haibo Qiu, Baosheng Yu, and Dacheng Tao. Collect-and-distribute transformer for 3d point cloud analysis. _arXiv preprint arXiv:2306.01257_, 2023.
* [60] Mamshad Nayeem Rizve, Kevin Duarte, Yogesh S Rawat, and Mubarak Shah. In defense of pseudo-labeling: An uncertainty-aware pseudo-label selection framework for semi-supervised learning. In _International Conference on Learning Representations_, 2021.
* [61] Claude Elwood Shannon. A mathematical theory of communication. _The Bell System Technical Journal_, 27:379-423, 1948.
* [62] Hanyu Shi, Jiacheng Wei, Ruibo Li, Fayao Liu, and Guosheng Lin. Weakly supervised segmentation on outdoor 4d point clouds with temporal matching and spatial graph propagation. In _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 11830-11839, 2022.
* [63] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In _Advances in Neural Information Processing Systems_, 2017.
* [64] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 596-608. Curran Associates, Inc., 2020.
* [65] Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. Learning from noisy labels with deep neural networks: A survey. _IEEE Transactions on Neural Networks and Learning Systems_, pages 1-19, 2022.
* [66] Liyao Tang, Yibing Zhan, Zhe Chen, Baosheng Yu, and Dacheng Tao. Contrastive boundary learning for point cloud segmentation. In _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 8479-8489, 2022.
* [67] Lyne Tchapmi, Christopher Choy, Iro Armeni, JunYoung Gwak, and Silvio Savarese. Segcloud: Semantic segmentation of 3d point clouds. _2017 International Conference on 3D Vision (3DV)_, Oct 2017.
* [68] Hugues Thomas, Charles R. Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, Francois Goulette, and Leonidas J. Guibas. Kpconv: Flexible and deformable convolution for point clouds. _CoRR_, abs/1904.08889, 2019.
* [69] Ozan Unal, Dengxin Dai, and Luc Van Gool. Scribble-supervised lidar semantic segmentation. _ArXiv_, abs/2203.08537, 2022.
* [70] Guo-Hua Wang and Jianxin Wu. Repetitive reprodiction deep decipher for semi-supervised learning. _Proceedings of the AAAI Conference on Artificial Intelligence_, 34(04):6170-6177, Apr. 2020.
* [71] Peng-Shuai Wang, Yang Liu, Yu-Xiao Guo, Chun-Yu Sun, and Xin Tong. O-CNN: Octree-based Convolutional Neural Networks for 3D Shape Analysis. _ACM Transactions on Graphics (SIGGRAPH)_, 36(4), 2017.
* [72] Yidong Wang, Hao Chen, Qiang Heng, Wenxin Hou, Yue Fan,, Zhen Wu, Jindong Wang, Marios Savvides, Takahiro Shinozaki, Bhiksha Raj, Bernt Schiele, and Xing Xie. Freematch: Self-adaptive thresholding for semi-supervised learning. 2023.
* [73] Yuxi Wang, Junran Peng, and Zhaoxiang Zhang. Uncertainty-aware pseudo label refinery for domain adaptive semantic segmentation. In _2021 IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 9072-9081, 2021.
* [74] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and Justin M. Solomon. Dynamic graph CNN for learning on point clouds. _CoRR_, abs/1801.07829, 2018.
* [75] Yuchao Wang, Haochen Wang, Yujun Shen, Jingjing Fei, Wei Li, Guoqiang Jin, Liwei Wu, Rui Zhao, and Xinyi Le. Semi-supervised semantic segmentation using unreliable pseudo-labels. _ArXiv_, abs/2203.03884, 2022.
* [76] Zongji Wang and Feng Lu. Voxsegnet: Volumetric cnns for semantic part segmentation of 3d shapes. _IEEE transactions on visualization and computer graphics_, 2018.
** [77] Jiacheng Wei, Guosheng Lin, Kim-Hui Yap, Tzu-Yi Hung, and Lihua Xie. Multi-path region mining for weakly supervised 3d semantic segmentation on point clouds. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4384-4393, 2020.
* [78] Wenxuan Wu, Zhongang Qi, and Fuxin Li. Pointconv: Deep convolutional networks on 3d point clouds. _CoRR_, abs/1811.07246, 2018.
* [79] Xiaoyang Wu, Yixing Lao, Li Jiang, Xihui Liu, and Hengshuang Zhao. Point transformer v2: Grouped vector attention and partition-based pooling. In _NeurIPS_, 2022.
* [80] Zhonghua Wu, Yicheng Wu, Guosheng Lin, and Jianfei Cai. Reliability-adaptive consistency regularization for weakly-supervised point cloud segmentation, 2023.
* [81] Zhonghua Wu, Yicheng Wu, Guosheng Lin, Jianfei Cai, and Chen Qian. Dual adaptive transformations for weakly supervised point cloud segmentation. _arXiv preprint arXiv:2207.09084_, 2022.
* [82] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improves imagenet classification. _arXiv preprint arXiv:1911.04252_, 2019.
* [83] Y. Xie, J. Tian, and X. X. Zhu. Linking points with labels in 3d: A review of point cloud semantic segmentation. _IEEE Geoscience and Remote Sensing Magazine_, 8(4):38-59, 2020.
* [84] Chenfeng Xu, Bichen Wu, Zining Wang, Wei Zhan, Peter Vajda, Kurt Keutzer, and Masayoshi Tomizuka. Squeezesgv3: Spatially-adaptive convolution for efficient point-cloud segmentation. _ArXiv_, abs/2004.01803, 2020.
* [85] Xun Xu and Gim Hee Lee. Weakly supervised semantic point cloud segmentation: Towards 10x fewer labels. In _CVPR_, 2020.
* [86] Xu Yan, Chaoda Zheng, Zhen Li, Sheng Wang, and Shuguang Cui. Pointansnl: Robust point clouds processing using nonlocal neural networks with adaptive sampling. _CoRR_, abs/2003.00492, 2020.
* [87] Cheng-Kun Yang, Ji-Jia Wu, Kai-Syun Chen, Yung-Yu Chuang, and Yen-Yu Lin. An mil-derived transformer for weakly supervised point cloud segmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 11830-11839, June 2022.
* [88] Jiancheng Yang, Qiang Zhang, Bingbing Ni, Linguo Li, Jinxian Liu, Mengdie Zhou, and Qi Tian. Modeling point clouds with self-attention and gumbel subset sampling. _CoRR_, abs/1904.03375, 2019.
* [89] Lhe Yang, Lei Qi, Litong Feng, Wayne Zhang, and Yinghuan Shi. Revisiting weak-to-strong consistency in semi-supervised semantic segmentation. In _CVPR_, 2023.
* [90] Shuquan Ye, Dongdong Chen, Songfang Han, and Jing Liao. Learning with noisy labels for robust point cloud segmentation. _International Conference on Computer Vision_, 2021.
* [91] Bowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jindong Wang, Manabu Okumura, and Takahiro Shinozaki. Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling. 2021.
* [92] Pan Zhang, Bo Zhang, Ting Zhang, Dong Chen, and Fang Wen. Robust mutual learning for semi-supervised semantic segmentation, 2021.
* [93] Xiaolong Zhang, Zuqiang Su, Xiaolin Hu, Yan Han, and Shuxian Wang. Semisupervised momentum prototype network for gearbox fault diagnosis under limited labeled samples. _IEEE Transactions on Industrial Informatics_, 18(9):6203-6213, 2022.
* [94] Yachao Zhang, Zonghao Li, Yuan Xie, Yanyun Qu, Cuihua Li, and Tao Mei. Weakly supervised semantic segmentation for large-scale point cloud. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 3421-3429, 2021.
* [95] Yachao Zhang, Yanyun Qu, Yuan Xie, Zonghao Li, Shanshan Zheng, and Cuihua Li. Perturbed self-distillation: Weakly supervised large-scale point cloud semantic segmentation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 15520-15528, 2021.
* [96] Yan Zhang, Wenhan Zhao, Bo Sun, Ying Zhang, and Wen Wen. Point cloud upsampling algorithm: A systematic review. _Algorithms_, 15(4), 2022.
* [97] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, and Vladlen Koltun. Point transformer, 2021.
* [98] Zhedong Zheng and Yi Yang. Rectifying pseudo label learning via uncertainty estimation for domain adaptive semantic segmentation. _International Journal of Computer Vision_, 129(4):1106-1120, Jan 2021.
* [99] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep features for discriminative localization. _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 2921-2929, 2016.
* [100] Yanning Zhou, Hang Xu, Wei Zhang, Bin Gao, and Pheng-Ann Heng. C3-semiseg: Contrastive semi-supervised segmentation via cross-set learning and dynamic class-balancing. In _2021 IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 7016-7025, 2021.

Supplementary: Introduction

In this supplementary material, we provide more details regarding implementation details in Appendix B, more analysis of ERDA in Appendix C, full experimental results in Appendix D, and studies on parameters in Appendix E.

## Appendix B Supplementary: Implementation and Training Details

For the RandLA-Net [34] and CloserLook3D [51] baselines, we follow the instructions in their released code for training and evaluation, which are here (RandLA-Net) and here (CloserLook3D), respectively. Especially, in CloserLook3D[51], there are several local aggregation operations and we use the "Pseudo Grid" (KPConv-like) one, which provides a neat re-implementation of the popular KPConv [68] network (rigid version). For point transformer (PT) [97], we follow their paper and the instructions in the code base that claims to have the official code (here). For FixMatch [64], we use the publicly available implementation here.

Our code and pre-trained models will be released.

## Appendix C Supplementary: Delving into ERDA with More Analysis

Following the discussion in Sec. 3, we study the impact of entropy regularization as well as different distance measurements from the perspective of gradient updates.

In particular, we study the gradient on the score of the \(i\)-th class _i.e.,_\(s_{i}\), and denote it as \(g_{i}=\frac{\partial L_{p}}{\partial s_{i}}\). Given that \(\frac{\partial p_{j}}{\partial s_{i}}=\mathbb{1}_{(i=j)}p_{i}-p_{i}p_{j}\), we have \(g_{i}=p_{i}\sum_{j}p_{j}(\frac{\partial L_{p}}{\partial p_{i}}-\frac{\partial L _{p}}{\partial p_{j}})\). As shown in Tab. 8, we demonstrate the gradient update \(\Delta=-g_{i}\) under different situations.

In addition to the analysis in Sec. 3.2, we find that, when \(\mathbf{q}\) is certain, _i.e.,_\(\mathbf{q}\) approaching a one-hot vector, the update of our choice \(KL(\mathbf{p}||\mathbf{q})\) would approach the infinity. We note that this could be hardly encountered since \(\mathbf{q}\) is typically also the output of a softmax function. Instead, we would rather regard it as a benefit because it would generate effective supervision on those model predictions with high certainty, and the problem of gradient explosion could also be prevented by common operations such as gradient clipping.

In Fig. 4, we provide visualization for a more intuitive understanding on the impact of different formulations for \(L_{DA}\) as well as their combination with \(L_{ER}\). Specifically, we consider a simplified case of binary classification and visualize their gradient updates when \(\lambda\) takes different values. We also visualize the gradient updates of \(L_{ER}\). By comparing the gradient updates, we observe that only \(KL(\mathbf{p}||\mathbf{q})\) with \(\lambda=1\) can achieve small updates when \(\mathbf{q}\) is close to uniform (\(q=\frac{1}{2}\) under the binary case), and that there is a close-0 plateau as indicated by the sparse contour lines.

Additionally, we also find that, when increasing the \(\lambda\), all distances, except the \(KL(\mathbf{p}||\mathbf{q})\), are modulated to be similar to the updates of having \(L_{ER}\) alone; whereas \(KL(\mathbf{p}||\mathbf{q})\) can still produce effective updates, which may indicate that \(KL(\mathbf{p}||\mathbf{q})\) is more robust to the \(\lambda\).

## Appendix D Supplementary: Full Results

We provide full results for the experiments reported in the main paper. For S3DIS [2], we provide the full results of S3DIS with 6-fold cross-validation in Tab. 10. For ScanNet [16] and SensatUrban [33], we evaluate on their online test servers, which are here and here, and provide the full results in Tab. 11 and Tab. 12, respectively.

Figure 4: Contour visualization of the gradient update with binary classes for better understanding. For a clearer view, we use red for positive updates and blue for negative updates, the darker indicates larger absolute values and the lighter indicates smaller absolute values.

\begin{table}
\begin{tabular}{c|c|c|c} \hline \(L_{p,t}\) & \(KL(\mathbf{p}|\mathbf{q})\) & \(KL(\mathbf{q}|\mathbf{p})\) & \(JS_{p,\mathbf{q}}\) & \(MS(\mathbf{p}_{\mathbf{q}})\) \\ \hline \(L_{p}\) & \(H[\mathbf{p}_{\mathbf{q}}-(1-\lambda y|\mathbf{p})]\) & \(H[\mathbf{q}_{\mathbf{q}}(\mathbf{p}_{\mathbf{q}}-\mathbf{H}(q)+M|\mathbf{p})]\) & \(H[\frac{\mathbf{p}+\frac{\lambda}{2}}{2}(\frac{1}{2}\lambda y|\mathbf{p}- \frac{1}{2}\lambda y|\mathbf{p}-\frac{1}{2}\lambda y|\mathbf{p}-\frac{1}{2} \lambda y|\mathbf{p})]\) & \(\frac{1}{2}\sum_{i=0}^{\lambda}\lambda^{i}+M|\mathbf{p})\) \\ \(\mathbf{n}\) & \(p_{\mathbf{n}}\sum_{j}(-1\log\frac{\mathbf{n}}{\gamma}+(1-\lambda y)\log\frac{ \mathbf{n}}{\gamma})\) & \(p_{\mathbf{n}}-\gamma_{\mathbf{n}}\sum_{j}\log\frac{\mathbf{n}}{\gamma}\) & \(p_{\mathbf{n}}\sum_{j}(-1\log\frac{\mathbf{n}}{\gamma}+(\frac{\lambda}{2}- \lambda y|\mathbf{p}-\frac{1}{2}\lambda y|\mathbf{p}-\frac{1}{2}\lambda y| \mathbf{p}-\frac{1}{2}\lambda y|\mathbf{p})]\) & \(h(-\mathbf{n})-p_{\mathbf{n}}\sum_{j}(-1)-\lambda y\sum_{j}\log\frac{\mathbf{n} }{\gamma}\) \\ \hline \hline \multicolumn{3}{c}{**Simulation**} & \multicolumn{1}{c}{\(\mathbf{a}=-\alpha_{\mathbf{n}}\)} & \multicolumn{1}{c}{\(\mathbf{0}\)} \\ \hline \(p_{\mathbf{n}}\to 1\) & \(0\) & \(0\) & \(0\) \\ \(q_{\mathbf{n}}\to -\alpha_{\mathbf{n}}\) & \((-1\log\frac{\mathbf{n}}{\gamma})\sum_{j}\log\frac{\mathbf{n}}{\gamma}\) & \(\frac{1}{2}\sum_{j}\log\frac{\mathbf{n}}{\gamma}\) & \(\sum_{j,\mathbf{n}}\sum_{j}\log\frac{\mathbf{n}}{\gamma}\) \\ \(q_{\mathbf{n}}\to 1\) & \(+\alpha_{\mathbf{n}}\sum_{j}\log\frac{\mathbf{n}}{\gamma}\sum_{j}\log \frac{\mathbf{n}}{\gamma}\) & \(p_{\mathbf{n}}\sum_{j}\log\frac{\mathbf{n}}{\gamma}\sum_{j}\log\frac{\mathbf{n }}{\gamma}\log\frac{\mathbf{n}}{\gamma}\) & \(-q_{\mathbf{n}}^{\prime}\log\frac{\mathbf{n}}{\gamma}+(1-\lambda)\sum_{j}\sum _{j}\log\frac{\mathbf{n}}{\gamma}\) \\ \(q_{\mathbf{n}}\to 1\) & \(-\alpha_{\mathbf{n}}\) & \(-p_{\mathbf{n}}+\gamma_{\mathbf{n}}\sum_{j}\log\frac{\mathbf{n}}{\gamma}\) & \(p_{\mathbf{n}}\sum_{j}\log\frac{\mathbf{n}}{\gamma}\) \\ \hline \end{tabular}
\end{table}
Table 8: The formulation of \(L_{p}\) using different functions to formulate \(L_{DA}\). We present the gradients \(g_{i}=\frac{\partial L_{p}}{\partial s_{i}}\), and the corresponding update \(\Delta=-g_{i}\) under different situations. Analysis can be found in the Sec. 3.2 and Appendix C.

[MISSING_PAGE_FAIL:17]