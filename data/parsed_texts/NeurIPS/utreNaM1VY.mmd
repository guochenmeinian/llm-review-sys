# Can semi-supervised learning use all the data effectively? A lower bound perspective

 Alexandru Tifrea

ETH Zurich

alexandru.tifrea@inf.ethz.ch

&Gizem Yuce\({}^{*}\)

EPFL

gizem.yuce@epfl.ch

&Amartya Sanyal

Max Planck Institute for Intelligent Systems, Tubingen

amsa@di.ku.dk

&Fanny Yang

ETH Zurich

fan.yan@inf.ethz.ch

###### Abstract

Prior theoretical and empirical works have established that semi-supervised learning algorithms can leverage the unlabeled data to improve over the labeled sample complexity of supervised learning (SL) algorithms. However, existing theoretical work focuses on regimes where the unlabeled data is sufficient to learn a good decision boundary using unsupervised learning (UL) alone. This begs the question: Can SSL algorithms simultaneously improve upon both UL _and_ SL? To this end, we derive a tight lower bound for 2-Gaussian mixture models that explicitly depends on the labeled and the unlabeled dataset size as well as the signal-to-noise ratio of the mixture distribution. Surprisingly, our result implies that no SSL algorithm improves upon the minimax-optimal statistical error rates of SL or UL algorithms for these distributions. Nevertheless, in our real-world experiments, SSL algorithms can often outperform UL and SL algorithms. In summary, our work suggests that while it is possible to prove the performance gains of SSL algorithms, this would require careful tracking of constants in the theoretical analysis.1

Footnote 1: Please consult the arxiv version of the paper for an updated manuscript.

## 1 Introduction

Semi-Supervised Learning (SSL) has recently garnered significant attention, often surpassing traditional supervised learning (SL) methods in practical applications [5; 10; 22]. Within this framework, the learning algorithm leverages both labeled and unlabeled datasets sampled from the same distribution. Numerous empirical studies suggest that SSL can effectively harness the information from both datasets, outperforming both SL and unsupervised learning (UL) approaches [21; 42; 17; 25]. This observation prompts the question: how fundamental is the improvement of SSL over SL and UL? 2

Footnote 2: By error of UL we mean the prediction error up to sign. We formalize this paradigm of using UL first and then identifying the correct sign as UL+ in Section 2.2.

Prior theoretical results do not provide a consensus on this topic. One line of work demonstrates that under certain settings, SSL is capable of lowering the labeled sample complexity. In these settings, the unlabeled data possesses a significant amount of information about the conditional distribution (up to permutation). Some examples of this setting include distributions where unlabeled samples are enough to obtain small estimation error in mixture models [30; 18] or when the data is clusterable [31]. Therefore, despite SSL performing provably better than SL in these settings, it is only as good as UL (up to permutation). Another line of work challenges the above literature by identifying scenarios where SSL achieves the same error rate as SL. In these scenarios, even oracle knowledge about themarginal distribution fails to improve upon the error rates of SL algorithms since the marginal does not carry any information about the labeling i.e. the conditional distribution. Therefore, these settings do not allow SSL to improve upon SL but only upon UL.

In summary, the previous bounds do not provide a conclusive answer on the benefits of SSL; the positive and negative results consider different regimes dependent on sample sizes and the "compatibility" of the marginal distribution with the conditional distribution. In particular, they either improve upon UL or upon SL, but never both simultaneously. In this paper, we provide a first answer to the following question

_Can semi-supervised classification algorithms simultaneously improve_

_over the minimax rates of both SL and UL?_

Specifically, in Sections 2 and 3, we study this question in the context of linear classification for symmetric 2-Gaussian mixture models (GMMs) as done in several works in this domain [30; 18; 2; 39]. In this setting, we derive minimax error rates for semi-supervised learning that specifically depend on the "regime", characterized by three quantities: the amount of available unlabeled data \(n_{u}\), amount of available labeled data \(n_{l}\), and the inherent signal-to-noise ratio (SNR) that quantifies the amount of information the marginal input distribution has about the conditional label distribution (see 3 for more details). An SNR-dependent minimax rate allows us to analyze the whole spectrum of problem difficulties for 2-GMMs. By contrasting the SSL minimax rates with established minimax rates for SL and UL, we find that in no regime can SSL surpass the statistical rates of both SL and UL. In conclusion, the optimal algorithm is the one that can adeptly switch between SL and UL algorithms depending on the regime and hence, it never uses the available data fully.

Nevertheless, statistical rates may not offer a complete picture for explaining the practical benefits of SSL algorithms. Several prevalent SSL algorithms, such as self-training, are more sophisticated than UL approaches and use labeled data not only for determining the sign but also for learning the decision boundary. In Section 4, we show that an SSL ensembling method and self-training [41; 7] can indeed improve upon the best of SL and UL algorithms even in proof-of-concept experiments for linear classification tasks on both synthetic and real-world datasets. Since the improvements cannot be captured by statistical rates, our results highlight the significance of the constant factors in future theoretical work that analyzes the advantage of SSL algorithms.

## 2 Problem setting and background

Before providing our main results, in this section, we first define the problem setting for our theoretical analysis, the evaluation metrics and the types of learning algorithms that we compare. We then describe how we compare the rates between SSL and supervised and unsupervised learning

### Linear classification for 2-GMM data

Data distribution.We consider linear binary classification problems where the data is drawn from a Gaussian Mixture Model consisting of two identical spherical Gaussians with identity covariance and uniform mixing weights. The means of the two components \(\bm{\theta}^{*},-\bm{\theta}^{*}\) are symmetric with respect to the origin but can have arbitrary non-zero norm. We denote this family of joint distributions as \(\mathcal{P}_{\text{2-GMM}}:=\{P_{XY}^{\bm{\theta}^{*}}:\bm{\theta}^{*}\in \mathbb{R}^{d}\}\) that can be factorized so that the density reads \(p_{XY}^{\bm{\theta}^{*}}(x,y)=p_{X|Y}^{\bm{\theta}^{*}}(x|y)p_{Y}(y)\) with

\[P_{Y}=\text{Unif}\{-1,1\}\text{ and }P_{X|Y}^{\bm{\theta}^{*}}=\mathcal{N}(Y \bm{\theta}^{*},I_{d}).\] (1)

This family of distributions has often been considered in the context of analysing both SSL [30; 18] and SL/UL [2; 24; 39] algorithms. For \(s\in(0,\infty)\), we denote by \(\mathcal{P}_{\text{2-GMM}}^{(s)}\subset\mathcal{P}_{\text{2-GMM}}\) and \(\Theta^{(s)}\subset\mathbb{R}^{d}\) the set of distributions \(P_{XY}^{\bm{\theta}^{*}}\) and the set of parameters with \(\|\bm{\theta}^{*}\|=s\), respectively. With this definition, we will be able to obtain refined bounds that depend explicitly on \(s\). We consider algorithms \(\mathcal{A}\) that take as input a labeled dataset \(\mathcal{D}_{l}\sim\left(P_{XY}^{\bm{\theta}^{*}}\right)^{n_{l}}\) of size \(n_{l}\), an unlabeled dataset \(\mathcal{D}_{u}\sim\left(P_{X}^{\bm{\theta}^{*}}\right)^{n_{u}}\) of size \(n_{u}\), or both, and output an estimator \(\hat{\bm{\theta}}=\mathcal{A}\left(\mathcal{D}_{l},\mathcal{D}_{u}\right)\in \mathbb{R}^{d}\). The estimator is used to predict the label of a test point \(x\) as \(\hat{y}=\operatorname{sign}\left(\langle\hat{\bm{\theta}},x\rangle\right)\).

Evaluation metricsIn this work, we consider two natural error metrics for this class of problems: prediction error and parameter estimation error3. For any vector \(\theta\), we define its

Footnote 3: For linear classification and a 2-GMM distribution from the family \(\mathcal{P}_{\text{2-GMM}}\), a low estimation error implies not only good predictive performance, but also good calibration under a logistic model [29].

\[\textbf{Prediction error:}\ \mathcal{R}_{\text{pred}}\left(\bm{\theta},\bm{ \theta}^{*}\right):=P_{XY}^{\bm{\theta}^{*}}\left(\operatorname*{sign}\left( \left\langle\bm{\theta},X\right\rangle\right)\neq Y\right),\quad\text{and}\] (2) \[\textbf{Estimation error:}\ \mathcal{R}_{\text{estim}}\left(\bm{\theta},\bm{ \theta}^{*}\right):=\left\|\bm{\theta}-\bm{\theta}^{*}\right\|_{2}.\] (3)

When the true \(\bm{\theta}^{*}\) is clear from the context, we drop the second argument for simplicity. In our discussions, we focus on the prediction error, but include a minimax rate for the estimation error for completeness. In particular, we bound the excess risk of \(\bm{\theta}\), defined as the distance between the risk of \(\bm{\theta}\) and the Bayes optimal risk

\[\textbf{Excess prediction error:}\ \mathcal{E}\left(\bm{\theta},\bm{\theta}^{*} \right):=\mathcal{R}_{\text{pred}}\left(\bm{\theta},\bm{\theta}^{*}\right)- \inf_{\bm{\theta}}\mathcal{R}_{\text{pred}}\left(\bm{\theta},\bm{\theta}^{*} \right).\]

where \(\inf_{\bm{\theta}}\mathcal{R}_{\text{pred}}\left(\bm{\theta},\bm{\theta}^{*}\right)\) is achieved at \(\bm{\theta}^{*}\) but can be non-zero.

For the set of all classification algorithms, we study the minimax expected error over a set of parameters \(\Theta\). This worst-case error over \(\Theta\) indicates the limits of what is achievable with the algorithm class. For instance, the minimax optimal expected excess error of the algorithm class over \(\Theta\) takes the form:

\[\textbf{Minimax excess error:}\ \epsilon\left(n_{l},n_{u},\Theta\right):=\inf_{ \mathcal{A}}\sup_{\bm{\theta}^{*}\in\Theta}\mathbb{E}\left[\mathcal{E}\left( \mathcal{A}(\mathcal{D}_{l},\mathcal{D}_{u}),\bm{\theta}^{*}\right)\right].\] (4)

### Minimax optimal rates of supervised and unsupervised learning

We distinguish between three kinds of learning that can be used in the SSL setting to learn a decision boundary \(\hat{\bm{\theta}}\) but are designed to leverage the available data differently. For simplification, our discussion is tailored towards learning \(\mathcal{P}_{\text{2-GMM}}\), though the ideas hold more generally.

1) Semi-supervised learning (SSL)SSL algorithms, denoted as \(\mathcal{A}_{\text{SSL}}\), can utilize both labeled \(\mathcal{D}_{l}\) and unlabeled samples \(\mathcal{D}_{u}\) to learn the decision boundary and to produce an estimator \(\hat{\bm{\theta}}_{\text{SSL}}=\mathcal{A}_{\text{SSL}}\left(\mathcal{D}_{l}, \mathcal{D}_{u}\right)\). The promise of SSL is that by combining labeled and unlabeled data, SSL can reduce both the labeled and unlabeled sample complexities compared to algorithms solely dependent on either dataset.

2) Supervised learning (SL)SL algorithms, represented by \(\mathcal{A}_{\text{SL}}\), can only use the labeled dataset \(\mathcal{D}_{l}\) to yield an estimator \(\hat{\bm{\theta}}_{\text{SL}}=\mathcal{A}_{\text{SL}}\left(\mathcal{D}_{l}, \emptyset\right)\). The minimax rates of SL for distributions from \(\mathcal{P}_{\text{2-GMM}}^{(s)}\) (see Table 1) are achieved by the mean estimator \(\hat{\bm{\theta}}_{\text{SL}}=\frac{1}{n_{l}}\sum_{i=1}^{n_{l}}Y_{i}X_{i}\), for both excess risk and estimation error.

3) Unsupervised learning (UL)Traditionally, UL algorithms are tailored to learning the generative model for marginal distributions. For \(\mathcal{P}_{\text{2-GMM}}\) the marginal is governed by \(\bm{\theta}^{*}\) and UL algorithms output a set of estimators \(\{\hat{\bm{\theta}}_{\text{UL}},-\hat{\bm{\theta}}_{\text{UL}}\}=\mathcal{A}_ {\text{UL}}\left(\emptyset,\mathcal{D}_{u}\right)\) one of which is guaranteed to be close to the true \(\bm{\theta}^{*}\). To evaluate prediction performance, we define the minimax rate of UL algorithms as the minimax rate for the closer (to the true \(\bm{\theta}^{*}\)) of the two estimators. This minimax rate of UL algorithms over \(\mathcal{P}_{\text{2-GMM}}^{(s)}\) is known for both the excess risk and the estimation error [39, 24] (see Table 1). These rates are achieved by the unsupervised estimator \(\hat{\bm{\theta}}_{\text{UL}}=\sqrt{(\hat{\lambda}-1)_{+}}\hat{v}\), where \((\hat{\lambda},\hat{v})\) is the leading eigenpair of the sample covariance matrix \(\hat{\Sigma}=\frac{1}{n_{u}}\sum_{j=0}^{n_{u}}X_{j}X_{j}^{T}\) and we use the notation \((x)_{+}:=\max(0,x)\).

\begin{table}
\begin{tabular}{c c c} \hline \hline Learning paradigm & Excess risk rate & Estimation error rate \\ \hline SL & \(e^{-s^{2}/2}\frac{d}{sn_{l}}\) & \(\sqrt{\frac{d}{n_{l}}}\) \\ \hline UL(+) & \(e^{-s^{2}/2}\frac{d}{s^{3}n_{u}}\) & \(\sqrt{\frac{d}{s^{2}n_{u}}}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Known minimax rates of SL and UL for learning 2-GMMs [39, 24]. The minimax rates for UL are up to choosing the correct sign. This rate is the same as for UL+ if \(n_{l}\geq\log(n_{u})\). The notation \(f(x)\asymp g(x)\) is equivalent to \(f=\Theta(g)\).

### A "wasteful" type of SSL algorithm

Several SSL algorithms used in practice (e.g. SimCLR [9], SwAV [8]) follow a two-stage procedure: i) determine decision boundaries using only unlabeled data; and ii) label decision regions using only labeled data. We refer to this class of two-stage algorithms as **UL+** and denote them by \(\mathcal{A}_{\text{UL+}}\). Early analyses of semi-supervised learning focus, in fact, on algorithms that fit the description of UL+ [30; 31].

For linear binary classification, the two-stage framework is depicted in Algorithm 1. The following proposition upper bounds the excess risk and estimation error incurred by the UL+ estimator given by

\[\hat{\bm{\theta}}_{\text{UL+}}=\operatorname{sign}\left(\hat{\bm{\theta}}_{ \text{SL}}^{\top}\hat{\bm{\theta}}_{\text{UL}}\right)\hat{\bm{\theta}}_{\text {UL}}\quad\text{ with }\hat{\bm{\theta}}_{\text{SL}}=\mathcal{A}_{\text{SL}}\left( \mathcal{D}_{l}\right).\] (5)

**Proposition 1** (Upper bounds for \(\hat{\bm{\theta}}_{\text{UL+}}\) ).: _Let \(\hat{\bm{\theta}}_{\text{UL+}}\) be the estimator defined in Equation (5). For any \(s\in(0,1]\) the following holds when \(n_{u}\geq(160/s)^{2}d\) and \(d\geq 2\):_

\[\mathbb{E}\left[\mathcal{R}_{\text{pred}}\left(\hat{\bm{\theta}} _{\text{UL+}},\bm{\theta}^{\star}\right)\right] \lesssim\sqrt{\frac{d}{s^{2}n_{u}}}+se^{-\frac{1}{2}n_{ls}s^{2} \left(1-c_{0}\sqrt{\frac{d\log(n_{u})}{s^{2}n_{u}}}\right)^{2}},\quad\text{ and}\] \[\mathbb{E}\left[\mathcal{E}\left(\hat{\bm{\theta}}_{\text{UL+}}, \bm{\theta}^{\star}\right)\right] \lesssim e^{-\frac{1}{2}s^{2}}\frac{d}{s^{3}n_{u}}+e^{-\frac{1}{2}s^{2}n_{l} \left(1-c_{0}\sqrt{\frac{d\log(n_{u})}{s^{2}n_{u}}}\right)^{2}}.\]

Appendix A contains the complete statement of the proposition including logarithmic factors and the proof. Note that for \(n_{l}=o\left(\frac{1}{s^{2}}\log\left(n_{u}\right)\right)\), the first term in the upper bound dominates, thereby resulting in the same rate as the minimax rate of UL up to choosing the correct sign. We remark that, while Algorithm 1 defines UL+ algorithms as only using the unlabeled dataset \(\mathcal{D}_{u}\) for the unsupervised learning step, one can also use the labeled dataset \(\mathcal{D}_{l}\) without labels in that step. However, typically, in practice, UL+ style algorithms (e.g. SimCLR, SwAV) do not use the labeled data in this way, as they operate in a regime where unlabeled data is far more numerous than labeled data. Thus, we analyze Algorithm 1 in this work.

Why UL+ algorithms are "wasteful"As indicated in Algorithm 1, UL+ type algorithms follow a precise structure where labeled data is used solely to select from the set of estimators output by a UL algorithm. Intuitively, such algorithms do not take full advantage of the labeled data as it is not used to refine the decision boundary. In prior empirical and theoretical studies, this inefficiency has not been a problem since they have focused on the regime \(n_{u}=\omega(n_{l})\), where unlabeled data is often orders of magnitude more abundant than labeled data. When \(n_{u}=\Theta(n_{l})\) or even \(n_{u}\ll n_{l}\), however, Proposition 1 shows how this two-stage approach of UL+ can become strikingly ineffective. For simplicity, consider the extreme scenario where \(n_{u}\) is finite, but \(n_{l}\to\infty\). The error of a UL+ algorithm will, at best, mirror the error of a UL algorithm with the correct sign (e.g. \(\Theta\left(\nicefrac{{d}}{{n_{u}}}\right)\) for the excess risk). Thus, despite using both labeled and unlabeled data, UL+ algorithms bear a close resemblance to UL algorithms that only use unlabeled data.

``` Input :\(\mathcal{D}_{l}\), \(\mathcal{D}_{u}\) \(\{\hat{\bm{\theta}}_{\text{UL}},-\hat{\bm{\theta}}_{\text{UL}}\}\leftarrow \mathcal{A}_{\text{UL}}(\mathcal{D}_{u})\) \(\hat{\bm{\theta}}_{\text{UL+}}\leftarrow\text{Select from }\{\{\hat{\bm{\theta}}_{\text{UL}},-\hat{\bm{\theta}}_{\text{UL}}\}\text{ using }\mathcal{D}_{l}\}\) return\(\hat{\bm{\theta}}_{\text{UL+}}\) ```

**Algorithm 1**UL+ algorithms \(\mathcal{A}_{\text{UL+}}\)

### Brief overview of prior error bounds for SSL

In this section, we discuss prior theoretical works that aim to show benefits and limitations of SSL.

Upper boundsThere are numerous known upper bounds on the excess risk of SSL algorithms for \(\mathcal{P}_{2\text{-GMM}}\) distributions. However, despite showing better dependence on the labeled set size, these earlier bounds primarily match the UL+ rates [30; 31] or exhibit slower rates than UL+ [18]. That is, we cannot conclude from existing results that SSL algorithms can consistently outperform _both_ SL and UL+, which is the question we aim to address in this paper.

Lower boundsIn contrast to the upper bounds that aim to demonstrate benefits of SSL, three distinct minimax lower bounds for SSL have been proposed to show the limitations of SSL. Each proves, in different settings, that there exists a distribution \(P_{XY}\) where SSL cannot outperform the SL minimax rate. Ben-David et al. [6] substantiate this claim for learning thresholds from univariate datasourced from a uniform distribution on \([0,1]\). Gopfert et al. [20] expand upon this by considering arbitrary marginal distributions \(P_{X}\) and a "rich" set of realizable labeling functions, such that no volume of unlabeled data can differentiate between possible hypotheses. Lastly, Tolstikhin and Lopez-Paz [36] set a lower bound for scenarios with no implied association between the labeling function and the marginal distribution, a condition recognized as being unfavorable for SSL improvements [32]. Each of the aforementioned results contends that a particular worst-case distribution \(P_{XY}\) exists, where the labeled sample complexity for SSL matches that of SL, even with limitless unlabeled data.

To summarize, the upper bounds show that SSL improves upon SL in settings where \(n_{u}\) is significantly larger than \(n_{l}\). In fact, for such large unlabeled sample size even UL+ can achieve small error. On the other hand, the lower bounds prove the futility of SSL for distributions where even infinite unlabeled data cannot help i.e. \(P_{X}\) does not contain sufficient information about the conditional distribution. However these works fail to answer the question: does there exist a relatively moderate \(n_{u}\) regime where SSL is better than both SL and UL+?

In the family of \(\mathcal{P}_{\text{2-GMM}}\) distributions, the above lower bounds translate to the hard setting where \(n_{u}\ll\nicefrac{{1}}{{s}}\). We now aim to prove a minimax lower bound for a fixed difficulty \(s>0\), that allows us to answer the above question in different regimes in terms of \(n_{l},n_{u}\).

## 3 Minimax rates for SSL

In this section we provide tight minimax lower bounds for SSL algorithms and 2-GMM distributions in \(\mathcal{P}_{\text{2-GMM}}^{(s)}\). Our results indicate that it is, in fact, not possible for SSL algorithms to simultaneously achieve faster minimax rates than both SL and UL+.

### Minimax rate

We begin by introducing tight lower bounds on the excess risk (4) of a linear estimator obtained using both labeled and unlabeled data. In addition, we also present tight lower bounds on estimation error (3) for the means of class-conditional distributions obtained similarly using labelled and unlabelled data. This is especially relevant when addressing linear classification of symmetric and spherical GMMs. In this setting, a reduced estimation error points to not only a low excess risk but also suggests a small calibration error under the assumption of a logistic noise model [29]. Both of these results are presented in Theorem 1. We present short proof sketches here and relegate the formal conditions required by the theorem to hold as well as the full proofs to Appendices B and C (for the estimation error and excess risk, respectively).

**Theorem 1** (SSL Minimax Rate for Excess Risk and Estimation Error).: _Assume the conditions in Proposition 1 and additionally let \(n_{l}>O(\frac{\log n_{u}}{s^{2}})\). Then for any \(s\in(0,1]\), we have_

\[\inf_{\mathcal{ASL}}\sup_{\|\bm{\theta}^{*}\|=s}\mathbb{E}\left[ \mathcal{E}\left(\mathcal{A}_{\text{SSL}}\left(\mathcal{D}_{l},\mathcal{D}_{ u}\right),\bm{\theta}^{*}\right)\right] \asymp e^{-s^{2}/2}\min\left\{s,\frac{d}{sn_{l}+s^{3}n_{u}}\right\},\quad \text{and}\] \[\inf_{\mathcal{ASL}}\sup_{\|\bm{\theta}^{*}\|=s}\mathbb{E}\left[ \mathcal{R}_{\text{estim}}(\mathcal{A}_{\text{SSL}}(\mathcal{D}_{l},\mathcal{ D}_{u}),\bm{\theta}^{*})\right] \asymp\min\left\{s,\sqrt{\frac{d}{n_{l}+s^{2}n_{u}}}\right\},\]

_where the infimum is over all the possible SSL algorithms that have access to both unlabeled and labeled data and the expectation is over \(\mathcal{D}_{l}\sim\left(P_{XY}^{\bm{\theta}^{*}}\right)^{n_{l}}\) and \(\mathcal{D}_{u}\sim\left(P_{X}^{\bm{\theta}^{*}}\right)^{n_{u}}\)._

In the rest of the section, we focus solely on the bound for excess risk; however, we note that the discussion here transfers to estimation error as well. In Section 3.2, we discuss the new insights that this bound provides. A direct implication of the result is that \(\epsilon_{\text{SSL}}\left(n_{l},n_{u},\Theta^{(s)}\right)\asymp\min\left( \epsilon_{\text{SL}}\left(n_{l},0,\Theta^{(s)}\right),\epsilon_{\text{UL+}} \left(n_{l},n_{u},\Theta^{(s)}\right)\right)\), that is, the minimax rate of SSL is the same as either that of SL or UL+, depending on the value of \(s\) and the rate of growth of \(n_{u}\) compared to \(n_{l}\). Therefore, we can conclude that _no SSL algorithm can simultaneously improve the rates of both SL and UL+ for \(\theta\in\Theta^{(s)}\)_.We provide a full discussion of the rate improvements in more detail in Section 3.2.

Proof sketchThe proof of the lower bound for excess risk is presented in Appendix C. For this proof, we adopt the packing construction in Li et al. [24] and apply Fano's method. Since the algorithms have access to both labeled and unlabeled datasets in the semi-supervised setting, KLdivergences between both the marginal and the joint distributions appear in the lower bound after the application of Fano's method, which is the key difference from its SL and UL counterparts.

We then show that the rate can be matched by the **SSL Switching Algorithm (SSL-S)** in Algorithm 2 - an oracle algorithm that switches between using a (minimax optimal) SL or UL+ algorithm based on the values of \(s,n_{l},\) and \(n_{u}\). The upper bound then follows as a corollary from Proposition 1 and the upper bounds for supervised learning.

For the parameter estimation error lower bound, we use Fano's method with the packing construction in Wu and Zhou [39], who have employed this method to derive lower bounds in the context of unsupervised learning. Similar to the excess risk analysis, the lower bound reveals that the SSL rate is either determined by the SL rate or the UL+ rate depending on \(s\) and the ratio of the sizes of the labeled and unlabeled samples. Once again, the minimax error rate is matched by the SSL Switching algorithm presented in Algorithm 2.

Discussion of the details of the theoremWe note that the SSL-S algorithm chooses to output the trivial estimator \(\hat{\bm{\theta}}_{\text{SSL-S}}=\bm{0}\) if the SNR \(s\) is low. In the low-SNR regime, the trivial estimator \(\hat{\bm{\theta}}=\bm{0}\) achieves a small excess risk of \(\mathcal{E}\left(\bm{0},\bm{\theta}^{\star}\right)=\Phi\left(s\right)-\Phi \left(0\right)\simeq e^{-s^{2}/2}s\), where \(\Phi\) is the CDF of the standard normal distribution.

Moreover, the technical condition \(s\in\left(0,1\right]\) is not overly restrictive as this range for the SNR is already sufficient for seeing all the relevant regimes, e.g. SSL-S switches from using SL to UL+ within this range. Finally, while the _rates_ of either SL or UL+ cannot be improved further using SSL algorithms, it is nonetheless possible to improve the error by a constant factor, independent of \(n_{l}\) and \(n_{u}\). To see this, in Section 4 we describe an algorithm that uses both \(\mathcal{D}_{l}\) and \(\mathcal{D}_{u}\) effectively and can hence achieve a provable improvement in error over both SL and UL+.

### Comparison of SSL with UL+ and SL in different regimes

To understand whether an SSL algorithm is using the labeled and unlabeled data effectively, we compare the error rate of SSL algorithms to the minimax rates for SL and UL+ algorithms.

A gap in these rates for a certain SSL algorithm would indicate that the algorithm can obtain lower error than both SL and UL+ when presented with the same amount of (large enough) data. We study the _improvement rates_ defined below, which capture the error ratio for the worst-case distributions between the minimax rate for SSL and the minimax rate for SL or UL+.

**Definition 1** (SSL improvement rates).: _For a set of parameters \(\Theta\subseteq\mathbb{R}^{d}\), we define the improvement rates of SSL over SL and UL+ as \(h_{l}\) and \(h_{u}\), respectively, where_

\[h_{l}(n_{l},n_{u},\Theta):=\frac{\inf_{\mathcal{A}_{\text{SL}}} \sup_{\bm{\theta}^{\star}\in\Theta}\mathbb{E}\left[\mathcal{E}\left(\mathcal{ A}_{\text{SSL}}\left(\mathcal{D}_{l},\mathcal{D}_{u}\right),\bm{\theta}^{ \star}\right)\right]}{\inf_{\mathcal{A}_{\text{SL}}}\sup_{\bm{\theta}^{\star} \in\Theta}\mathbb{E}\left[\mathcal{E}\left(\mathcal{A}_{\text{SL}}\left( \mathcal{D}_{l},\emptyset\right),\bm{\theta}^{\star}\right)\right]},\] (6) \[h_{u}(n_{l},n_{u},\Theta):=\frac{\inf_{\mathcal{A}_{\text{SL}}} \sup_{\bm{\theta}^{\star}\in\Theta}\mathbb{E}\left[\mathcal{E}\left(\mathcal{ A}_{\text{SSL}}\left(\mathcal{D}_{l},\mathcal{D}_{u}\right),\bm{\theta}^{ \star}\right)\right]}{\inf_{\mathcal{A}_{\text{UL}}}\sup_{\bm{\theta}^{\star} \in\Theta}\mathbb{E}\left[\mathcal{E}\left(\mathcal{A}_{\text{UL}}\left( \mathcal{D}_{l},\mathcal{D}_{u}\right),\bm{\theta}^{\star}\right)\right]},\] (7)

_where the expectations are over \(\mathcal{D}_{l}\sim\left(P_{XY}^{\bm{\theta}^{\star}}\right)^{n_{l}}\) and \(\mathcal{D}_{u}\sim\left(P_{X}^{\bm{\theta}^{\star}}\right)^{n_{u}}\)._

To simplify notation, we denote the improvement rates of SL and UL+ over \(\Theta^{(s)}\) as \(h_{l}(n_{l},n_{u},s)\) and \(h_{u}(n_{l},n_{u},s)\), respectively. A straightforward upper bound for these rates is \(h_{l},h_{u}\leq 1\), achieved when utilizing an SL and UL+ algorithm, respectively. SSL demonstrates an enhanced error rate over SL and UL+, if both \(\lim_{n_{l},n_{u}\to\infty}h_{l}(n_{l},n_{u},\Theta)=0\) and \(\lim_{n_{l},n_{u}\to\infty}h_{u}(n_{l},n_{u},\Theta)=0\). If \(h_{l}\) or \(h_{u}\) lies in \((0,1)\) without converging to zero as \(n_{l},n_{u}\to\infty\), then SSL surpasses SL or UL+, respectively, only by a constant factor.

Given Theorem 1, we can directly derive the improvement rates of SSL in the following corollary.

**Corollary 1**.: _Assuming the setting of Theorem 1, the improvement rates of SSL can be written as:_

\[\textbf{Improvement rate over SL:}\ h_{l}\left(n_{l},n_{u},s\right) \asymp\frac{n_{l}}{n_{l}+s^{2}n_{u}}.\] \[\textbf{Improvement rate over UL+:}\ h_{u}\left(n_{l},n_{u},s\right) \asymp\frac{s^{2}n_{u}}{n_{l}+s^{2}n_{u}}.\]

Based on this corollary, we now argue how no SSL algorithm can simultaneously improve the rates of both SL and UL+ for any regime of \(s,n_{l}\) and \(n_{u}\), as summarized in Table 2.

1. SSL rate is not faster than SL, but is faster than UL+.For extremely low values of SNR (i.e. \(s\to 0\) faster than \(\sqrt{n_{u}}\rightarrow\infty\)), we have that \(\lim_{n_{l},n_{u}\rightarrow\infty}h_{l}(n_{l},n_{u},s)=c_{SL}>0\) and hence SSL fails to improve the SL rates even with infinite unlabeled data. This setting has been the focus of previous worst-case analyses for SSL [6; 36; 20]. Alternatively, the SSL rate is also not better than the SL rate for a fixed SNR, when the labeled dataset is significantly larger than the unlabeled data, i.e. \(n_{u}=o(n_{l})\).

2. SSL rate is faster than SL, but not faster than UL+.As mentioned in Gopfert et al. [20], in order for SSL to lead to a rate improvement compared to SL, it is _necessary_ that the unlabeled set size is at least a superlinear function of the labeled set size, i.e. \(n_{u}=\omega(n_{l})\). Corollary 1 shows that this condition is, in fact, _sufficient_ for 2-GMM distributions: for \(s>0\), as long as \(n_{u}\) grows superlinearly with respect to \(n_{l}\), \(h_{l}(n_{l},n_{u},s)\to 0\), and hence, SSL can achieve faster rates than SL. Despite the improvement over SL, for this setting, the asymptotic error ratio between SSL and UL+ does not vanish, i.e. \(\lim_{n_{l},n_{u}\rightarrow\infty}h_{u}\left(n_{l},n_{u},s\right)=c_{\rm UL}>0\).

3. SSL rate is not faster than either SL or UL+.Finally, in the regime where the unlabeled dataset size depends linearly on the size of the labeled set i.e. \(\lim_{n_{l},n_{u}\rightarrow\infty}\frac{n_{u}}{n_{l}}\to c\) for some constant \(c>0\), neither of the improvement rates vanishes for \(n_{l},n_{u}\rightarrow\infty\).

As explained in Section 2.2, prior UL+ algorithms that work well in practice do not use the labeled data for the unsupervised learning step. Interestingly, for the case when both the labeled and unlabeled data are used for the unsupervised step, the trends in Table 2 remain the same, with only one exception: the improvement rates in the last line of the table (i.e. \(\frac{n_{u}}{n_{l}}\to c\in(0,\infty)\)) would only change by a small constant. More importantly, the main takeaway from Table 2 remains the same: SSL cannot achieve better rates than both UL+ and SL at the same time since there is no regime for which \(h_{l}\) and \(h_{u}\) are simultaneously 0.

## 4 Empirical improvements over the rate-optimal SSL-S algorithm

Section 3 shows that the SSL minimax rates can be achieved with a simple algorithm that switches between a minimax optimal SL and a minimax optimal UL+ algorithm. Despite being optimal in terms of statistical rates, this SSL Switching algorithm does not make the most effective use of the available data: SL algorithms solely uses the labeled data whereas UL+ learns the decision boundary using just the unlabeled data and the labeled data is used to only label the different prediction regions. Alternatively, one could use both labeled and unlabeled data to learn the decision boundary. In this section, we investigate the following question: Is it possible to improve over the error of SSL-S even though the rate would be, at best, as good as the 'wasteful' SSL-S algorithm as per Section 3?In this section, we present experiments to show that, in fact, a remarkably simple algorithm (i.e. a weighted ensemble of the SL and UL+ classifiers) can outperform the minimax optimal SSL-S algorithm. We show that algorithms such as self-training, which have been shown to excel in practice [40, 34, 22], can also improve over SSL-S. Consequently, it remains an exciting avenue for future work to derive tight analyses that characterize the improvement of such algorithms over SL/UL+.

A simple algorithm more effective than SSL-SA natural approach to improve upon the naive SSL-S algorithm is to construct a weighted ensemble of an SL and a UL+ estimator, trained on \(\mathcal{D}_{l}\) and \(\mathcal{D}_{u}\), respectively, with a controllable weighting hyperparameter \(t\). We call this the **SSL Weighted algorithm (SSL-W)** shown in Algorithm 3. With an appropriate choice of \(t\), it is possible to show that the SSL-W algorithm performs better (up to sign permutation) than SSL-S. The formal statement of this result together with the proof are deferred to Appendix D. In practice, one can fix the sign permutation of the \(\hat{\bm{\theta}}_{\text{SSL-W}}\) estimator using a small amount of labeled data. The intuition for this improvement is that the ensemble estimator \(\hat{\bm{\theta}}_{\text{SSL-W}}\) achieves lower error than the constituent estimators of the ensemble (i.e. \(\hat{\bm{\theta}}_{\text{SL}}\) and \(\hat{\bm{\theta}}_{\text{UL+}}\)), which, in turn, determine the error of the SSL-S algorithm.

### Empirical improvements over the minimax optimal SSL-S algorithm

In this section we present linear classification experiments on synthetic and real-world data to show that there indeed exist SSL algorithms that can improve over the error of the (minimax optimal) SSL Switching Algorithm. For both synthetic and real-world data, we use \(\hat{\bm{\theta}}_{\text{SL}}=\frac{1}{n_{l}}\sum_{i=1}^{n_{l}}Y_{i}X_{i}\) as the SL estimator and an Expectation-Maximization (EM) algorithm for the UL algorithm (see Appendix E for details). The optimal switching point for SSL-S and the optimal weight for SSL-W, as well as the optimal \(\ell_{2}\) penalty for logistic regression are chosen using a validation set.

Synthetic data.We consider data drawn from a symmetric and isotropic 2-GMM distribution \(P^{\bm{\theta}^{*}}_{XY}\) over \(\mathbb{R}^{2}\). The labeled and unlabeled set sizes are set to \(20\) and \(2000\), respectively. Figure 0(a) shows the gap between SSL-W and SL or UL+ as a function of the SNR \(s\) (Figure 3 in Appendix F shows the dependence of the error gap on \(n_{l}\)). There are two main takeaways. First, for varying SNR values \(s\), SSL-W always outperforms SL and UL+, and hence, also SSL-S. Second, as argued in Section 3.2, SSL-W improves more over UL+ for small values of the SNR \(s\), and it improves more over SL for large values of the SNR.

Real-world data.We consider \(10\) binary classification real-world datasets: five from the OpenML repository [37] and five 2-class subsets of the MNIST dataset [13]. For the MNIST subsets, we choose class pairs that have a linear Bayes error varying between \(0.1\%\) and \(2.5\%\).4 We choose from OpenML datasets that have a large enough number of samples compared to dimensionality (see Appendix E for details on how we choose the datasets). The OpenML datasets span a range of Bayes errors that varies between \(3\%\) and \(34\%\).

Footnote 4: We estimate the Bayes error of a dataset by training a linear classifier on the entire labeled dataset.

In the absence of the exact data-generating process, we quantify the difficulty of SSL on real-world datasets using a notion of _compatibility_, reminiscent of Balcan and Blum [4]. Specifically, we consider the compatibility given by \(\rho^{-1}\) with \(\rho:=\frac{1}{2\sqrt{d}}\left(\Delta(\bm{\theta}^{*}_{UL+},\bm{\theta}^{*}_{ \text{Bayes}})+\mathcal{R}_{\text{pred}}(\bm{\theta}^{*}_{\text{Bayes}})\right)\), where \(\Delta(\bm{\theta}^{*}_{UL+},\bm{\theta}^{*}_{\text{Bayes}}):=\frac{\mathcal{R }_{\text{pred}}(\bm{\theta}^{*}_{U+})-\mathcal{R}_{\text{pred}}(\bm{\theta}^{ *}_{\text{Bayes}})}{\mathcal{R}_{\text{pred}}(\bm{\theta}^{*}_{\text{Bayes}})}\), \(d\) is the dimension of the data, \(\bm{\theta}^{*}_{\text{Bayes}}\) is obtained via SL on the entire dataset and \(\bm{\theta}^{*}_{\text{UL+}}\) determines the predictor with optimal sign obtained via UL on the entire dataset. Intuitively, this notion of compatibility captures both the Bayes error of a dataset, as well as how compatible the 2-GMM parametric assumption actually is for the given data.

In addition to SSL-S (Algorithm 2) and SSL-W (Algorithm 3) we also evaluate the performance of self-training, using a procedure similar to the one analyzed in Frei et al. [18]. We use a logistic regression estimator for the pseudolabeling, and train logistic regression with a ridge penalty in the second stage of the self-training procedure. Note that an \(\ell_{2}\) penalty corresponds to input consistency regularization [38] with respect to \(\ell_{2}\) perturbations.

Figure (b)b shows the improvement in classification error of SSL algorithms (i.e. SSL-W and self-training) compared to SL and UL+. The positive values of the error gap indicate that SSL-W outperforms both SL and UL+ even on real-world datasets. This finding suggests that the intuition presented in this section carries over to more generic distributions beyond just 2-GMMs. Finally, Figure 4 in Appendix F shows that SSL-W improves over the error of the minimax optimal SSL-S algorithm, for varying values of \(n_{l}\). These findings provide some proof-of-concept evidence that existing SSL algorithms may already be obtaining lower error than the minimax optimal SSL-S algorithm. We hope this observation will encourage future research into characterizing this error gap for practically relevant algorithms such as self-training.

## 5 Related work

Other theoretical analyses of SSL algorithms.Beyond the theoretical studies highlighted in Section 2, there are a few others pertinent to our research. Specifically, Azizyan et al. [1], Singh et al. [33] present upper bounds for semi-supervised regression, which are contingent on the degree to which the marginal \(P_{X}\) informs the labeling function. This is akin to the results we derive in this work. However, obtaining a minimax lower bound for semi-supervised regression remains an exciting direction for future work. We refer to [27] for an overview of prior theoretical results for SSL.

Balcan and Blum [4] introduced a compatibility score, denoted as \(\chi(f,P_{X})\in[0,1]\), which connects the space of marginal distributions to the space of labeling functions. While their findings hint that SSL may surpass the SL minimax rates, they offer no comparisons with UL/UL+. Moreover, the paper does not discuss minimax optimality of the proposed SSL algorithms.

On another note, even though SSL does not enhance the rates of UL, Sula and Zheng [35] demonstrate that labeled samples can bolster the convergence speed of Expectation-Maximization within the context of our study.

To conclude, Scholkopf et al. [32] leveraged a causality framework to pinpoint scenarios where SSL does not offer any advantage over SL. In essence, when the covariates, represented by \(X\), act as causal ancestors to the labels \(Y\), the independent causal mechanism assumption dictates that the marginal \(P_{X}\) offers no insights about the labeling function.

Minimax rates for SL and UL.The proofs in this work rely on techniques used to derive minimax rates for SL and UL algorithms. Most of these prior results consider the same distributional assumptions as our paper. Wu and Zhou [39] show a tight minimax lower bound for estimation error for spherical 2-GMMs from \(\mathcal{P}_{\text{2-GMM}}\). Moreover, Azizyan et al. [2], Li et al. [24] derive minimax rates over \(\mathcal{P}_{\text{2-GMM}}\) for classification and clustering (up to permutation).

In addition to the SL and UL algorithms considered in Section 3, Expectation-Maximization (EM) is another family of algorithms that is commonly analyzed for the same distributional setting considered in our paper. For instance, Wu and Zhou [39] rely on techniques from several previous seminal papers [12; 14; 15; 3; 16] to obtain upper bounds for EM-style algorithms.

Figure 1: Error gap between SL or UL+ and SSL-W for varying task difficulties. We assess problem difficulty using the SNR (for 2-GMMs) or the compatibility \(\rho^{-1}\) (for real-world data). We see the same trends for both synthetic and real-world data. Moreover, self-training also exhibits the same trend as \(\hat{\boldsymbol{\theta}}_{\text{SSL-W}}\).

Conclusions and limitations

In this paper, we establish a tight lower bound for semi-supervised classification within the class of 2-GMM distributions. Our findings demonstrate that SSL cannot simultaneously improve the error rates of both SL and UL across all signal-to-noise ratios. However, empirical evidence suggests that SSL _can_ improve upon the error of minimax optimal SL or UL algorithms. This observation calls for careful analyses of the error of SSL algorithms that also track constant factors, not only rates.

Our theoretical analysis focuses exclusively on isotropic and symmetric GMMs due to limitations in the technical tools employed for the proofs. Similar constraints can be observed in recent analyses of SL or UL algorithms [24; 39]. However, it is worth noting that whenever the bounds for SL and UL can be extended to more general distributions in the future, these results can be seamlessly used to also extend Theorem 1 to these settings.

## Acknowledgement

AT was supported by a PhD fellowship from the Swiss Data Science Center. We also thank the anonymous reviewers for their helpful comments.

## References

* [1] Martin Azizyan, Aarti Singh, and Larry Wasserman. Density-sensitive semisupervised inference. _The Annals of Statistics_, 2013.
* [2] Martin Azizyan, Aarti Singh, and Larry Wasserman. Minimax theory for high-dimensional gaussian mixtures with sparse mean separation. In _Advances in Neural Information Processing Systems_, 2013.
* [3] Sivaraman Balakrishnan, Martin J. Wainwright, and Bin Yu. Statistical guarantees for the EM algorithm: From population to sample-based analysis. _The Annals of Statistics_, 2017.
* [4] Maria-Florina Balcan and Avrim Blum. A discriminative model for semi-supervised learning. _Journal of the ACM_, 2010.
* [5] Randall Balestriero, Mark Ibrahim, Vlad Sobal, Ari Morcos, Shashank Shekhar, Tom Goldstein, Florian Bordes, Adrien Bardes, Gregoire Mialon, Yuandong Tian, et al. A cookbook of self-supervised learning. _arXiv:2304.12210_, 2023.
* [6] Shai Ben-David, Tyler Lu, and David Pal. Does unlabeled data provably help? Worst-case analysis of the sample complexity of semi-supervised learning. In _Annual Conference on Learning Theory (COLT)_, 2008.
* [7] Avrim Blum and Tom Mitchell. Combining labeled and unlabeled data with co-training. In _Annual Conference on Learning Theory (COLT)_, 1998.
* [8] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. _Advances in Neural Information Processing Systems 33_, 2020.
* [9] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In _37th International Conference on Machine Learning_, 2020.
* [10] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big self-supervised models are strong semi-supervised learners. In _Advances in Neural Information Processing Systems 33_, 2020.
* [11] Thomas M. Cover and Joy A. Thomas. _Elements of Information Theory_. 2006.
* [12] Constantinos Daskalakis, Christos Tzamos, and Manolis Zampetakis. Ten steps of EM suffice for mixtures of two gaussians. In _Annual Conference on Learning Theory (COLT)_, 2017.
* [13] Li Deng. The mnist database of handwritten digit images for machine learning research. _IEEE Signal Processing Magazine_, 2012.
* [14] Raaz Dwivedi, Nhat Ho, Koulik Khamaru, Michael I. Jordan, Martin J. Wainwright, and Bin Yu. Singularity, misspecification and the convergence rate of em. _The Annals of Statistics_, 2018.
* [15] Raaz Dwivedi, Nhat Ho, Koulik Khamaru, Martin J Wainwright, and Michael I Jordan. Theoretical guarantees for EM under misspecified gaussian mixture models. In _Advances in Neural Information Processing Systems 31_, 2018.
* [16] Raaz Dwivedi, Nhat Ho, Koulik Khamaru, Martin Wainwright, Michael Jordan, and Bin Yu. Sharp analysis of expectation-maximization for weakly identifiable models. In _International Conference on Artificial Intelligence and Statistics_, 2020.
* [17] Alaaeldin El-Nouby, Gautier Izacard, Hugo Touvron, Ivan Laptev, Herve Jegou, and Edouard Grave. Are large-scale datasets necessary for self-supervised pre-training? _arXiv:2112.10740_, 2021.
* [18] Spencer Frei, Difan Zou, Zixiang Chen, and Quanquan Gu. Self-training converts weak learners to strong learners in mixture models. In _International Conference on Artificial Intelligence and Statistics_, 2022.

* Giraud [2021] Christophe Giraud. _Introduction to high-dimensional statistics_. CRC Press, 2021.
* Gopfert et al. [2019] Christina Gopfert, Shai Ben-David, Olivier Bousquet, Sylvain Gelly, Ilya O. Tolstikhin, and Ruth Urner. When can unlabeled data improve the learning rate? In _Annual Conference Computational Learning Theory_, 2019.
* Goyal et al. [2019] Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra. Scaling and benchmarking self-supervised visual representation learning. In _International Conference on Computer Vision_, 2019.
* Grill et al. [2020] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. _Advances in Neural Information Processing Systems 33_, 2020.
* LeCun and Cortes [2010] Yann LeCun and Corinna Cortes. MNIST handwritten digit database, 2010. URL http://yann.lecun.com/exdb/mnist/.
* Li et al. [2017] Tianyang Li, Xinyang Yi, Constantine Carmanis, and Pradeep Ravikumar. Minimax Gaussian Classification & Clustering. In _20th International Conference on Artificial Intelligence and Statistics_, 2017.
* Lucas et al. [2022] Thomas Lucas, Philippe Weinzaepfel, and Gregory Rogez. Barely-supervised learning: semi-supervised learning with very few labeled images. In _AAAI Conference on Artificial Intelligence_, 2022.
* Massart [2007] Pascal Massart. _Concentration inequalities and model selection: Ecole d'Ete de Probabilites de Saint-Flour XXXIII-2003_. Springer, 2007.
* Mey and Loog [2020] Alexander Mey and Marco Loog. Improvability through semi-supervised learning: A survey of theoretical results. _arXiv:1908.09574_, 2020.
* Pedregosa et al. [2011] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. _Journal of Machine Learning Research_, 2011.
* Platt et al. [1999] John Platt et al. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. _Advances in Large Margin Classifiers_, 1999.
* Ratsaby and Venkatesh [1995] Joel Ratsaby and Santosh S. Venkatesh. Learning from a mixture of labeled and unlabeled examples with parametric side information. In _Annual Conference on Learning Theory (COLT)_, 1995.
* Rigollet [2006] Philippe Rigollet. Generalization error bounds in semi-supervised classification under the cluster assumption. _Journal of Machine Learning Research_, 2006.
* Scholkopf et al. [2012] Bernhard Scholkopf, Dominik Janzing, Jonas Peters, Eleni Sgouritsa, Kun Zhang, and Joris Mooij. On causal and anticausal learning. In _29th International Conference on Machine Learning_, 2012.
* Singh et al. [2008] Aarti Singh, Robert D. Nowak, and Xiaojin Zhu. Unlabeled data: Now it helps, now it doesn't. In _Advances in Neural Information Processing Systems 21_, 2008.
* Sohn et al. [2020] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. FixMatch: Simplifying semi-supervised learning with consistency and confidence. In _Advances in Neural Information Processing Systems_, 2020.
* Sula and Zheng [2022] Erixhen Sula and Lizhong Zheng. On the semi-supervised expectation maximization. _arXiv:2211.00537_, 2022.
* Tolstikhin and Lopez-Paz [2016] Ilya O. Tolstikhin and David Lopez-Paz. Minimax lower bounds for realizable transductive classification. _arXiv:1602.03027_, 2016.

* [37] Joaquin Vanschoren, Jan N. van Rijn, Bernd Bischl, and Luis Torgo. OpenML: networked science in machine learning. _SIGKDD Explorations_, 2013.
* [38] Colin Wei, Kendrick Shen, Yining Chen, and Tengyu Ma. Theoretical analysis of self-training with deep networks on unlabeled data. In _International Conference on Learning Representations_, 2021.
* [39] Yihong Wu and Harrison H. Zhou. Randomly initialized EM algorithm for two-component gaussian mixture achieves near optimality in \(O(\sqrt{n})\) iterations. _Mathematical Statistics and Learning_, 2021.
* [40] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V. Le. Self-training with noisy student improves ImageNet classification. June 2020.
* [41] David Yarowsky. Unsupervised word sense disambiguation rivaling supervised methods. In _33rd Annual Meeting on Association for Computational Linguistics_, 1995.
* [42] Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin Dogus Cubuk, and Quoc Le. Rethinking pre-training and self-training. In _Advances in Neural Information Processing Systems 33_, 2020.

Proof of Proposition 1

In this section we provide the formal statement of Proposition 1 and its proof.

**Proposition 2** (Fixing the sign of \(\hat{\bm{\theta}}_{\text{UL}}\)).: _There exist universal constants \(c_{0},C_{1},C_{2},C_{3},C_{4}>0\) such that for \(n_{u}\geq(160/s)^{2}d\), \(d\geq 2\) and \(s\in(0,1]\), it holds that:_

\[\mathbb{E}\left[\|\hat{\bm{\theta}}_{\text{UL}+}-\bm{\theta}^{*} \|\right]\leq C_{1}\sqrt{\frac{d}{s^{2}n_{u}}}+C_{2}se^{-\frac{1}{2}n_{l}s^{2 }(1-c_{0}\sqrt{\frac{d\log(n_{u})}{s^{2}n_{u}}})^{2}},\quad\text{ and }\] \[\mathbb{E}\left[\mathcal{E}(\hat{\bm{\theta}}_{\text{UL}+}) \right]\leq C_{3}e^{-\frac{1}{2}s^{2}}\frac{d\log(dn_{u})}{s^{3}n_{u}}+C_{4}e^ {-\frac{1}{2}s^{2}n_{l}\left(1-c_{0}\sqrt{\frac{d\log(n_{u})}{s^{2}n_{u}}} \right)^{2}}.\]

Proof.: Recall that we consider the UL+ estimator \(\hat{\bm{\theta}}_{\text{UL+}}=\operatorname{sign}\left(\hat{\bm{\theta}}_{ \text{SL}}^{\top}\hat{\bm{\theta}}_{\text{UL}}\right)\hat{\bm{\theta}}_{\text {UL}}\) and denote \(\hat{\beta}:=\operatorname{sign}\left(\hat{\bm{\theta}}_{\text{SL}}^{\top} \hat{\bm{\theta}}_{\text{UL}}\right)\). Now let \(\beta:=\operatorname{sign}(\bm{\theta}^{*\top}\hat{\bm{\theta}}_{\text{UL}}) =\operatorname{arg\,min}_{\hat{\beta}\in\{-1,+1\}}\|\hat{\beta}\hat{\bm{ \theta}}_{\text{UL}}-\bm{\theta}^{*}\|^{2}\).

Note that we can write the expected estimation error of \(\hat{\bm{\theta}}_{\text{UL+}}\) as

\[\mathbb{E}\left[\|\hat{\bm{\theta}}_{\text{UL+}}-\bm{\theta}^{*} \|\right] =\mathbb{E}\left[\|\hat{\beta}\hat{\bm{\theta}}_{\text{UL}}-\bm{ \theta}^{*}\|\right]\] \[=\mathbb{E}\left[\mathbbm{1}_{\left\{\hat{\beta}=\hat{\beta} \right\}}\|\beta\hat{\bm{\theta}}_{\text{UL}}-\bm{\theta}^{*}\|+\mathbbm{1}_{ \left\{\hat{\beta}\neq\hat{\beta}\right\}}\|\beta\hat{\bm{\theta}}_{\text{UL }}+\bm{\theta}^{*}\|\right]\] \[\leq\mathbb{E}\left[\mathbbm{1}_{\left\{\hat{\beta}=\hat{\beta} \right\}}\|\beta\hat{\bm{\theta}}_{\text{UL}}-\bm{\theta}^{*}\|\right]+ \mathbb{E}\left[\mathbbm{1}_{\left\{\hat{\beta}\neq\hat{\beta}\right\}}(\| \beta\hat{\bm{\theta}}_{\text{UL}}-\bm{\theta}^{*}\|+2\|\bm{\theta}^{*}\|)\right]\] \[\leq\mathbb{E}\left[\|\beta\hat{\bm{\theta}}_{\text{UL}}-\bm{ \theta}^{*}\|\right]+2s\mathbb{P}(\hat{\beta}\neq\beta).\] (8)

Next, we recall a result by Wu and Zhou [39] established for this particular UL estimator, namely that \(\mathbb{E}\left[\|\beta\hat{\bm{\theta}}_{\text{UL}}-\bm{\theta}^{*}\|\right] \lesssim\sqrt{\frac{d}{s^{2}n_{u}}}\). Moreover, the probability of incorrectly estimating the sign (permutation) can be written as

\[\mathbb{P}(\hat{\beta}\neq\beta) =\mathbb{P}\Big{(}\operatorname{sign}\left(\hat{\bm{\theta}}_{ \text{SL}}^{\top}\hat{\bm{\theta}}_{\text{UL}}\right)\neq\operatorname{sign} \left(\bm{\theta}^{*\top}\hat{\bm{\theta}}_{\text{UL}}\right)\Big{)}\text{, where }\hat{\bm{\theta}}_{\text{SL}}\sim\mathcal{N}(\bm{\theta}^{*},\frac{1}{n_{l}}I_{d})\] \[\leq\mathbb{P}\left(\operatorname{sign}(\tilde{Z})\neq\operatorname {sign}\left(\bm{\theta}^{*\top}\hat{\bm{\theta}}_{\text{UL}}\right)\right) \text{, where }\tilde{Z}\sim\mathcal{N}(\hat{\bm{\theta}}_{\text{UL}}^{\top}\bm{\theta}^{*}, \frac{1}{n_{l}}(\hat{\bm{\theta}}_{\text{UL}}^{\top}\hat{\bm{\theta}}_{\text {UL}}))\] \[\leq\mathbb{P}\left(Z^{\prime}\geq|\hat{\bm{\theta}}_{\text{UL}}^ {\top}\bm{\theta}^{*}|\right)\text{, where }Z^{\prime}\sim\mathcal{N}(0,\frac{1}{n_{l}}(\hat{\bm{\theta}}_{\text{UL}}^{ \top}\hat{\bm{\theta}}_{\text{UL}}))\] \[=\mathbb{P}\Big{(}Z\geq\sqrt{n_{l}s^{2}}S_{C}(\hat{\bm{\theta}}_{ \text{UL}},\bm{\theta}^{*})\Big{)},\]

where \(Z\sim\mathcal{N}(0,1)\) and \(S_{C}(\hat{\bm{\theta}}_{\text{UL}},\bm{\theta}^{*})=\frac{|\hat{\bm{\theta}}_{ \text{UL}}^{\top}\bm{\theta}^{*}|}{\|\hat{\bm{\theta}}_{\text{UL}}\|\|\theta ^{*}\|}\) Therefore, for any \(A\) we have:

\[\mathbb{P}(\hat{\beta}\neq\beta) \leq\mathbb{P}(Z\geq\sqrt{n_{l}s^{2}}(1-A))+\mathbb{P}\Big{(}S_{ C}(\hat{\bm{\theta}}_{\text{UL}},\bm{\theta}^{*})\leq 1-A\Big{)}\] \[\leq e^{-\frac{1}{2}n_{l}s^{2}(1-A)^{2}}+\mathbb{P}\Big{(}S_{C}( \hat{\bm{\theta}}_{\text{UL}},\bm{\theta}^{*})\leq 1-A\Big{)},\]

where we used the Chernoff bound in the last step. Finally, setting \(A=c_{0}\sqrt{\frac{d\log(n_{u})}{s^{2}n_{u}}}\) as a corollary of Proposition 6 in Azizyan et al. [2] for \(n_{u}\geq(160/s)^{2}d\) we have \(\mathbb{P}\Big{(}S_{C}(\hat{\bm{\theta}}_{\text{UL}},\bm{\theta}^{*})\leq 1-A \Big{)}\leq\frac{d}{n_{u}}\). Therefore, for \(n_{u}\geq(160/s)^{2}d\), we have the following upper bound on estimating the sign incorrectly:

\[\mathbb{P}(\hat{\beta}\neq\beta)\leq e^{-\frac{1}{2}n_{l}s^{2}\left(1-c_{0} \sqrt{\frac{d\log(n_{u})}{s^{2}n_{u}}}\right)^{2}}+\frac{d}{n_{u}}.\] (9)Combining this result with Equation (8) finishes the proof of the estimation error bound in the proposition, as we obtain that for some \(C_{1},C_{2}>0\), the following holds:

\[\mathbb{E}\left[\|\hat{\bm{\theta}}_{\text{UL+}}-\bm{\theta}^{*}\|\right]\leq C_ {1}\sqrt{\frac{d}{s^{2}n_{u}}}+C_{2}se^{-\frac{1}{4}n_{i}s^{2}(1-c_{0}\sqrt{ \frac{d\log(n_{u})}{s^{2}n_{u}}})^{2}}.\]

Similarly, we can decompose the expected excess risk as follows:

\[\mathbb{E}\left[\mathcal{E}(\hat{\bm{\theta}}_{\text{UL+}})\right]= \mathbb{E}\left[\mathcal{E}(\hat{\beta}\hat{\bm{\theta}}_{\text{UL}})\right] =\mathbb{E}\left[\mathbbm{1}_{\left\{\hat{\beta}=\beta\right\}} \mathcal{E}(\hat{\beta}\hat{\bm{\theta}}_{\text{UL}})+\mathbbm{1}_{\left\{ \hat{\beta}\neq\beta\right\}}\mathcal{E}(\hat{\beta}\hat{\bm{\theta}}_{\text{ UL}})\right]\] \[=\mathbb{E}\left[\mathcal{E}(\beta\hat{\bm{\theta}}_{\text{UL}}) \right]+\mathbb{P}(\hat{\beta}\neq\beta),\]

where in Equation (10) we upper bound the indicator function and the excess risk terms by 1. Finally, combining Equation (9) with the upper bound in Li et al. [24] for \(\mathbb{E}\left[\mathcal{E}(\beta\hat{\bm{\theta}}_{\text{UL}})\right]\), we obtain the desired result. More concretely, we get that for some \(C_{3},C_{4}>0\),

\[\mathbb{E}\left[\mathcal{E}(\hat{\bm{\theta}}_{\text{UL+}})\right]\leq C_{3}e ^{-\frac{1}{2}s^{2}}\frac{d\log(dn_{u})}{s^{3}n_{u}}+C_{4}e^{-\frac{1}{2}s^{2} n_{l}\left(1-c_{0}\sqrt{\frac{d\log(n_{u})}{s^{2}n_{u}}}\right)^{2}}.\]

## Appendix B Proof of SSL Minimax Rate for Estimation Error in Theorem 1

In this section we provide the proofs for the lower and upper bounds on the estimation error presented in Theorem 1. The proof for the excess risk rates can be found in Appendix C.

### Proof of estimation error lower bound

We first prove the estimation error lower bound in Theorem 1. As discussed in Section 2, consider the 2-GMM distributions from \(\mathcal{P}^{(s)}_{\text{2-GMM}}\), with isotropic components and identical covariance matrices.

Consider an arbitrary set of predictors \(\mathcal{M}=\{\bm{\theta}_{i}\}_{i=0}^{M}\).. We can apply Fano's method [11] to obtain that the following holds:

\[\inf_{\mathcal{A}\text{ss.}}\sup_{\|\bm{\theta}^{*}\|=s}\mathbb{E }_{\mathcal{D}_{l},\mathcal{D}_{u}}\left[\mathcal{R}_{\text{estim}}(\mathcal{A }_{\text{SSL}}(\mathcal{D}_{l},\mathcal{D}_{u}),P^{\bm{\theta}^{*}}_{XY})\right]\] \[\geq\frac{1}{2}\min_{\begin{subarray}{c}i,j\in[M]\\ i\neq j\end{subarray}}\|\bm{\theta}_{i}-\bm{\theta}_{j}\|\left(1-\frac{1+\frac {1}{M}\sum_{i=1}^{M}D\left(P^{\bm{\theta}_{i}}_{XY}||P^{\bm{\theta}_{0}}_{XY} \right)+n_{u}D\left(P^{\bm{\theta}_{i}}_{X}||P^{\bm{\theta}_{0}}_{X}\right)} {log(M)}\right)\] (11) \[=\frac{1}{2}\min_{\begin{subarray}{c}i,j\in[M]\\ i\neq j\end{subarray}}\|\bm{\theta}_{i}-\bm{\theta}_{j}\|\left(1-\frac{1+\frac {1}{M}\sum_{i=1}^{M}n_{l}D\left(P^{\bm{\theta}_{i}}_{XY}||P^{\bm{\theta}_{0}}_{ XY}\right)+n_{u}D\left(P^{\bm{\theta}_{i}}_{X}||P^{\bm{\theta}_{0}}_{X} \right)}{log(M)}\right)\] \[\geq\frac{1}{2}\min_{\begin{subarray}{c}i,j\in[M]\\ i\neq j\end{subarray}}\|\bm{\theta}_{i}-\bm{\theta}_{j}\|\left(1-\frac{1+n_{l} \max_{i\in[M]}D\left(P^{\bm{\theta}_{i}}_{XY}||P^{\bm{\theta}_{0}}_{XY} \right)+n_{u}\max_{i\in[M]}D\left(P^{\bm{\theta}_{i}}_{X}||P^{\bm{\theta}_{0} }_{X}\right)}{log(M)}\right),\] (12)where \(D\left(\cdot||\cdot\right)\) denotes the KL divergence. In Equation (11), we use the fact that the labeled and unlabeled samples are drawn i.i.d. from \(P_{X}\) and \(P_{XY}\) and in Equation (12) we upper bound the average with the maximum. The next step of the proof consists in choosing an appropriate packing \(\{\bm{\theta}_{i}\}_{i=1}^{M}\) and \(\bm{\theta}_{0}\) on the sphere of radius \(s\), i.e. \(\frac{1}{s}\bm{\theta}_{i}\in S^{d-1}\), that optimizes the trade-off between the minimum and the maxima in Equation (12).

For the packing, we use the same construction that was employed by Wu and Zhou [39] for deriving adaptive bounds for unsupervised learning. This construction has the advantage that it also leads to a tight lower bound for the supervised setting. Let \(c_{0}\) and \(C_{0}\) be positive absolute constants and let \(\tilde{\mathcal{M}}=\{\psi_{1},...,\psi_{M}\}\) be a \(c_{0}\)-net on the unit sphere \(S^{d-2}\) such that we have \(|\tilde{\mathcal{M}}|=M\geq e^{C_{0}d}\). For an absolute constant \(\alpha\in\left[0,1\right],\) we construct the following packing of the sphere of radius \(s\) in \(\mathbb{R}^{d}\):

\[\mathcal{M}=\left\{\bm{\theta}_{i}=s\left[\frac{\sqrt{1-\alpha^{2}}}{\alpha \psi_{i}}\right]\middle|\psi_{i}\in\tilde{\mathcal{M}}\right\},\]

and define \(\bm{\theta}_{0}=[s,0,...,0]\). Note that, by definition, \(\|\bm{\theta}_{i}-\bm{\theta}_{j}\|\geq c_{0}s\alpha\), for any distinct \(i,j\in[M]\), which lower bounds the first term in (12). Furthermore, \(\|\bm{\theta}_{i}-\bm{\theta}_{0}\|\leq\sqrt{2}\alpha s\), for all \(i\in[M]\).

In the next step, we upper bound the maxima in Equation (12). First, we write the KL divergence between two GMMs with identity covariance matrices:

\[D\left(P_{XY}^{\bm{\theta}_{i}}||P_{XY}^{\bm{\theta}_{0}}\right)=\frac{1}{2} \|\bm{\theta}_{i}-\bm{\theta}_{0}\|_{2}^{2}\leq\alpha^{2}s^{2},\text{ for all }i=[M].\] (13)

Second, we can upper bound the KL divergence between marginal distributions, namely \(D\left(P_{X}^{\bm{\theta}_{i}}||P_{X}^{\bm{\theta}_{0}}\right)\), using Lemma 27 in Wu and Zhou [39], which implies that:

\[\max_{i\in[M]}D\left(P_{X}^{\bm{\theta}_{i}}||P_{X}^{\bm{\theta}_{0}}\right) \leq C\max_{i\in[M]}\lVert\frac{1}{s}\bm{\theta}_{i}-\frac{1}{s}\bm{\theta}_ {0}\rVert^{2}s^{4}\leq 2C\alpha^{2}s^{4}.\] (14)

Plugging Equations (13) and (14) into Equation (12) we obtain the following lower bound for the minimax error, which holds for any \(\alpha\leq 1\):

\[\inf_{\mathcal{A}_{\text{SSL}}}\sup_{\|\bm{\theta}^{*}\|=s}\mathbb{E}_{ \mathcal{D}_{t},\mathcal{D}_{u}}\left[\mathcal{R}_{\text{estim}}(\mathcal{A}_ {\text{SSL}}(\mathcal{D}_{l},\mathcal{D}_{u}),\bm{\theta}^{*})\right]\geq\frac {1}{2}c_{o}\alpha s\left(1-\frac{1+n_{l}s^{2}\alpha^{2}+n_{u}C_{1}s^{4}\alpha ^{2}}{C_{0}d}\right).\]

Minimizing over \(\alpha\) yields the optimum value \(\alpha=\min\left\{1,\sqrt{\frac{C_{0}d-1}{3s^{2}n_{l}+3C_{1}s^{4}n_{u}}}\right\}\), where the minimum comes from how we have constructed the packing, which requires that \(\alpha\leq 1\). Using this value for \(\alpha\) concludes the proof.

### Proof of estimation error upper bound

We now prove the tightness of our lower bound by establishing the upper bound for the estimation error of the SSL Switching algorithm presented in Algorithm 2. We choose the following minimax optimal SL and UL+ estimators

\[\hat{\bm{\theta}}_{\text{SL}} =\frac{1}{n_{l}}\sum_{i=1}^{n_{l}}Y_{i}X_{i}\] (15) \[\hat{\bm{\theta}}_{\text{UL+}} =\operatorname{sign}\left(\hat{\bm{\theta}}_{\text{SL}}^{\top} \hat{\bm{\theta}}_{\text{UL}}\right)\hat{\bm{\theta}}_{\text{UL}}\text{, with }\hat{\bm{\theta}}_{\text{UL}}=\sqrt{(\hat{\lambda}-1)_{+}}\hat{v},\] (16)

where \((\hat{\lambda},\hat{v})\) is the leading eigenpair of the sample covariance matrix \(\hat{\Sigma}=\frac{1}{n_{u}}\sum_{j=0}^{n_{u}}X_{j}X_{j}^{T}\) and we use the notation \((x)_{+}:=\max(0,x)\). It is known that this UL estimator is minimax optimal [39]. As unsupervised learning can only obtain classifiers up to a sign permutation, it is necessary to endow the UL+ algorithm with a means to discern between \(\hat{\bm{\theta}}_{\text{UL}}\) and \(-\hat{\bm{\theta}}_{\text{UL}}\), as explained in Section 2.2. Inthe case of the estimator introduced in Equation (16), the sign is selected so as to obtain an estimator that aligns better with the \(\hat{\bm{\theta}}_{\text{SL}}\). The upper bound for the expected error incurred by the UL+ estimator is given in Proposition 2.

For the SL estimator \(\hat{\bm{\theta}}_{\text{SL}}\), we apply standard results for Gaussian distributions to upper bound the estimation error that holds for any regime of \(n\) and \(d\):

\[\mathbb{E}_{\mathcal{D}_{l}}\left[\|\hat{\bm{\theta}}_{\text{SL}}-\bm{\theta}^{ \star}\|\right]\leq\sqrt{\frac{d}{n_{l}}}.\] (17)

Using Equation (17) and Proposition 2 and switching between \(\hat{\bm{\theta}}_{\text{SL}}\) and \(\hat{\bm{\theta}}_{\text{UL+}}\) according to the conditions in Algorithm 2 that pick the better performing of the two algorithms depending on the regime, we can show that there exist universal constants \(C,c_{0}>0\) such that for \(0\leq s\leq 1\), \(d\geq 2\) and \(n_{u}\geq(160/s)^{2}d\), it holds that

\[\mathbb{E}\left[\|\hat{\bm{\theta}}_{\text{SSL-S}}-\bm{\theta}^{\star}\| \right]\leq C\min\left\{s,\sqrt{\frac{d}{n_{l}}},\sqrt{\frac{d}{s^{2}n_{u}}} +se^{-\frac{1}{2}n_{l}s^{2}\left(1-c_{0}\sqrt{\frac{dlog(n_{u})}{s^{2}n_{u}}} \right)^{2}}\right\},\] (18)

where the expectation is over \(\mathcal{D}_{l}\sim\left(P_{XY}^{\bm{\theta}^{\star}}\right)^{n_{l}}\) and \(\mathcal{D}_{u}\sim\left(P_{X}^{\bm{\theta}^{\star}}\right)^{n_{u}}\).

Matching lower and upper bound.When \(n_{l}>O(\frac{\log(n_{u})}{s^{2}})\), the exponential term becomes negligible and the first additive component dominates in the last term in the right-hand side of Equation (18). Basic calculations then yield that the expected error of the switching algorithm is upper bounded by \(C^{\prime}\min\left\{s,\sqrt{\frac{d}{n_{l}+s^{2}n_{u}}}\right\}\) for some constant \(C^{\prime}\), which concludes the proof of the theorem.

## Appendix C Proof of SSL Minimax Rate for Excess Risk in Theorem 1

In this section, we prove the minimax lower bound on excess risk for an algorithm that uses both labeled and unlabeled data and a matching (up to logarithmic factors) upper bound.

### Proof of excess risk lower bound

We first prove the excess error minimax lower bound in Theorem 1, namely we aim to show that there exists a constant \(C_{0}>0\) such that for any \(s>0\), \(n_{u},n_{l}\geq 0\) and \(d\geq 4\), we have

\[\inf_{\mathcal{A}_{\text{SSL}}}\sup_{\|\bm{\theta}^{\star}\|=s}\mathbb{E} \left[\mathcal{E}\left(\mathcal{A}_{\text{SSL}}\left(\mathcal{D}_{l}, \mathcal{D}_{u}\right),\bm{\theta}^{\star}\right)\right]\geq C_{0}e^{-s^{2}/2} \min\left\{\frac{d}{sn_{l}+s^{3}n_{u}},s\right\},\] (19)

where the expectation is over \(\mathcal{D}_{l}\sim\left(P_{XY}^{\bm{\theta}^{\star}}\right)^{n_{l}}\) and \(\mathcal{D}_{u}\sim\left(P_{X}^{\bm{\theta}^{\star}}\right)^{n_{u}}\). Our approach to proving this lower bound is again to apply Fano's method [19] using the excess risk as the evaluation method. The reduction from estimation to testing usually hinges on the triangle inequality in a metric space. Since the excess risk does not satisfy the metric axioms, we employ a technique introduced in Azizyan et al. [2] to derive an alternative sufficient condition for applying Fano's inequality.

Let \(\bm{\theta}_{1},\ldots,\bm{\theta}_{M}\in\Theta\), \(M\geq 2\), and \(\gamma>0\). If for all \(1\leq i\neq j\leq M\) and \(\hat{\bm{\theta}}\),

\[\mathcal{E}\left(\hat{\bm{\theta}},\bm{\theta}_{i}\right)<\gamma\ \text{ implies }\ \ \mathcal{E}\left(\hat{\bm{\theta}},\bm{\theta}_{j}\right)\geq\gamma,\] (20)

then

\[\inf_{\mathcal{A}_{\text{SSL}}}\max_{i\in[0..M]} \mathbb{E}\left[\mathcal{E}\left(\mathcal{A}_{\text{SSL}}( \mathcal{D}_{l},\mathcal{D}_{u}),\bm{\theta}_{i}\right)\right]\] (21) \[\geq\gamma\left(1-\frac{1+n_{l}\max_{i\neq j}D\left(P_{XY}^{\bm{ \theta}_{i}}||P_{XY}^{\bm{\theta}_{i}}\right)+n_{u}\max_{i\neq j}D\left(P_{X} ^{\bm{\theta}_{i}}||P_{X}^{\bm{\theta}_{j}}\right)}{\log(M)}\right),\]where the expectation is over \(\mathcal{D}_{l}\sim\left(P_{XY}^{\bm{\theta}_{i}}\right)^{n_{l}}\) and \(\mathcal{D}_{u}\sim\left(P_{X}^{\bm{\theta}_{i}}\right)^{n_{u}}\).

In order to get a lower bound, we again pick \(\bm{\theta}_{i},\ldots,\bm{\theta}_{M}\) to be an appropriate packing, so that the condition in Equation (20) can be satisfied. For this purpose, we can simply use the construction from Li et al. [24], which was previously used to obtain tight bounds for supervised and unsupervised settings. Let \(p=(d-1)/6\). By Lemma 4.10 in Massart [26], there exists a set \(\tilde{\mathcal{M}}=\{\psi_{1},\ldots,\psi_{M}\}\), such that \(\|\psi_{i}\|_{0}=p\), \(\psi_{i}\in\{0,1\}^{d-1}\), the Hamming distance \(\delta\left(\psi_{i},\psi_{j}\right)>p/2\) for all \(1\leq i<j\leq M=|\tilde{\mathcal{M}}|\), and \(\log M\geq\frac{p}{5}\log\frac{d}{p}\geq d\log(6)/60=c_{1}d\).

Define

\[\mathcal{M}=\left\{\bm{\theta}_{i}=\left[\sqrt{s^{2}-p\alpha^{2}}\right] \middle|\psi_{i}\in\tilde{\mathcal{M}}\right\}\]

for some absolute constant \(\alpha\). Note that since \(\|\bm{\theta}_{i}\|=s\) and \(\|\bm{\theta}_{i}-\bm{\theta}_{j}\|^{2}=\alpha^{2}\delta\left(\psi_{i},\psi_ {j}\right)\), we have

\[\frac{p\alpha^{2}}{2}\leq\|\bm{\theta}_{i}-\bm{\theta}_{j}\|^{2}\leq 2p\alpha^{2}\] (22)

and

\[s^{2}-p\alpha^{2}\leq\bm{\theta}_{i}^{\top}\bm{\theta}_{j}\leq s^{2}-p\alpha^ {2}/4.\] (23)

First, we show that the excess risk satisfies the condition in Equation (20). As in the proof of Theorem 1 in Li et al. [24], we have that for any \(\bm{\theta}\),

\[\mathcal{E}(\bm{\theta},\bm{\theta}_{i})+\mathcal{E}(\bm{\theta},\bm{\theta}_ {j})\geq 2c_{0}e^{-s^{2}/2}\frac{p\alpha^{2}}{s},\]

and thus, for all \(i\) and \(j\neq i\), it holds that

\[\mathcal{E}(\bm{\theta},\bm{\theta}_{i})\leq c_{0}e^{-s^{2}/2}\frac{p\alpha^ {2}}{s}\implies\mathcal{E}(\bm{\theta},\bm{\theta}_{j})\geq c_{0}e^{-s^{2}/2 }\frac{p\alpha^{2}}{s}.\] (24)

Then since the condition in Equation (20) is satisfied, we obtain

\[\begin{split}&\inf_{\mathcal{A}_{\text{SSL}}}\sup_{\|\bm{\theta}^ {*}\|=s}\mathbb{E}_{\mathcal{D}_{l},\mathcal{D}_{u}}\left[\mathcal{E}\left( \mathcal{A}_{\text{SSL}}\left(\mathcal{D}_{l},\mathcal{D}_{u}\right),\bm{ \theta}^{*}\right)\right]\\ &\qquad\geq\inf_{\mathcal{A}_{\text{SSL}}}\max_{i\in[0..M]} \mathbb{E}\left[\mathcal{E}\left(\mathcal{A}_{\text{SSL}}(\mathcal{D}_{l}, \mathcal{D}_{u}),\bm{\theta}_{i}\right)\right]\\ &\qquad\geq c_{0}e^{-s^{2}/2}\frac{p\alpha^{2}}{s}\left(1-\frac{ 1+n_{l}\max_{i\neq j}D\left(P_{XY}^{\bm{\theta}_{i}}\|P_{XY}^{\bm{\theta}_{j}} \right)+n_{u}\max_{i\neq j}D\left(P_{X}^{\bm{\theta}_{i}}\|P_{X}^{\bm{\theta}_ {j}}\right)}{\log(M)}\right).\end{split}\] (25)

Next, we bound the KL divergences between the two joint distributions and between the two marginals that appear in Equation (25). For the joint distributions we have that:

\[D\left(P_{XY}^{\bm{\theta}_{i}}\|P_{XY}^{\bm{\theta}_{j}}\right)=\frac{1}{2}\| \bm{\theta}_{i}-\bm{\theta}_{j}\|_{2}^{2}\leq p\alpha^{2},\] (26)

where the inequality follows from Equation (22). Using Proposition 24 in Azizyan et al. [2], we bound the KL divergence between the two marginals

\[D\left(P_{X}^{\bm{\theta}_{i}}||P_{X}^{\bm{\theta}_{j}}\right)\lesssim s^{4} \left(1-\frac{\bm{\theta}_{i}^{\top}\bm{\theta}_{j}}{\|\bm{\theta}_{i}\|\|\bm{ \theta}_{j}\|}\right)\leq ps^{2}\alpha^{2},\] (27)

where the inequality follows from (23). Plugging (26) and (27) into (25) and setting

\[\alpha^{2}=c_{3}\min\left\{\frac{c_{1}d-\log 2}{8(pn_{l}+s^{2}pn_{u})},\frac{s^{2} }{p}\right\},\]

gives the desired result

\[\inf_{\mathcal{A}_{\text{SSL}}}\sup_{\|\bm{\theta}^{*}\|=s}\mathbb{E}_{ \mathcal{D}_{l},\mathcal{D}_{u}}\left[\mathcal{E}\left(\mathcal{A}_{\text{SSL}} \left(\mathcal{D}_{l},\mathcal{D}_{u}\right),\bm{\theta}^{*}\right)\right] \gtrsim e^{-s^{2}/2}\min\left\{\frac{d}{sn_{l}+s^{3}n_{u}},s\right\}.\]

### Proof of excess risk upper bound

Next, we prove the upper bound on the excess risk of the SSL switching estimator \(\hat{\bm{\theta}}_{\text{SSL-S}}\) output by Algorithm 2 with the supervised and unsupervised estimators defined in Appendix B.2 to show the tightness of the excess risk lower bound in Theorem 1. In particular, we show that there exist universal constants \(C,c_{0}>0\) such that for \(0\leq s\leq 1\), \(d\geq 2\) and for sufficiently large \(n_{u}\) and \(n_{l}\),

\[\mathbb{E}\left[\mathcal{E}(\hat{\bm{\theta}}_{\text{SSL-S}})\right]\leq Ce^{ -\frac{1}{2}s^{2}}\min\left\{s,\frac{d\log(n_{l})}{sn_{l}},\frac{d\log(dn_{u}) }{s^{3}n_{u}}+e^{-\frac{1}{2}s^{2}\left(n_{l}\left(1-c_{0}\sqrt{\frac{d\log(n_{ u})}{s^{2}n_{u}}}\right)^{2}-1\right)}\right\},\]

where the expectation is over \(\mathcal{D}_{l}\sim\left(P_{XY}^{\bm{\theta}^{*}}\right)^{n_{l}}\) and \(\mathcal{D}_{u}\sim\left(P_{X}^{\bm{\theta}^{*}}\right)^{n_{u}}\).

The proof follows the same arguments as the proof presented in Appendix B.2 where we instead use excess risk upper bounds for SL and UL from Li et al. [24].

In addition, we also use a result that follows from Proposition 2 to choose the sign of the UL+ estimator.

Note that the upper bound on the excess risk of \(\hat{\bm{\theta}}_{\text{SSL-S}}\) is matching the lower bound in Equation (19), up to logarithmic factors. We conjecture that the logarithmic factors are an artifact of the analysis and can be removed. For instance, it may be possible to extend results in Ratsaby and Venkatesh [30] that bound the excess risk using the estimation error upper bound without incurring logarithmic factors. However, their results are not directly applicable here because they are only valid under the assumption that the estimation error is arbitrarily small and hence not enough for bounding the expectation.

## Appendix D Theoretical motivation for the SSL Weighted Algorithm

In this section, we show theoretically that the SSL-W procedure introduced in Section 4 can achieve lower squared estimation error (up to sign permutation) compared to SSL-S. This result shows that it is possible to improve the error of the naive SSL-S algorithm by making effective use of _all_ the data that is available.

For the purpose of the theoretical analysis, we consider a slightly different SSL-W estimator compared to the one introduced in Section 4. First, recall that for the classification problem we consider, unsupervised learning produces a set of two feasible predictors \(\{\hat{\bm{\theta}}_{\text{UL}},-\hat{\bm{\theta}}_{\text{UL}}\}\) and cannot discern between them without access to a (small) labeled dataset. We denote by \(\hat{\bm{\theta}}_{\text{UL}}^{*}\) the UL estimator with correct sign, namely \(\hat{\bm{\theta}}_{\text{UL}}^{*}:=\arg\min_{\bm{\theta}\in\{\hat{\bm{\theta }}_{\text{UL}},-\hat{\bm{\theta}}_{\text{UL}}\}}\mathbb{E}\left[\left\|\bm{ \theta}-\bm{\theta}^{*}\right\|^{2}\right]\). Here, the hat symbol indicates that a finite sample is used to obtain the set of estimators \(\{\hat{\bm{\theta}}_{\text{UL}},-\hat{\bm{\theta}}_{\text{UL}}\}\), while the star symbol indicates that oracle information is employed to choose the sign of the estimator.

In what follows, we study theoretically the error of the SSL-W estimator constructed using \(\hat{\bm{\theta}}_{\text{UL}}^{*}\), i.e. \(\hat{\bm{\theta}}_{\text{SSL-W}}^{*}(t):=t\hat{\bm{\theta}}_{\text{SL}}+(1-t) \hat{\bm{\theta}}_{\text{UL}}^{*}\). Therefore, our result characterizes the error of the SSL-W estimator up to a sign permutation. To choose the correct sign, one needs only a small labeled dataset, similar in size to what is prescribed by Proposition 2. While this step is not captured by Proposition 3, SSL-S is unlikely to close the gap to SSL-W when provided with this small amount of additional labeled data. Moreover, for a fairer comparison we also give the switching estimator access to the UL estimator with correct sign and define \(\hat{\bm{\theta}}_{\text{SSL-S}}^{*}:=\arg\min_{\bm{\theta}\in\{\hat{\bm{ \theta}}_{\text{SL-W}},\hat{\bm{\theta}}_{\text{UL}}^{*}\}}\mathbb{E}\left[ \left\|\bm{\theta}-\bm{\theta}^{*}\right\|^{2}\right]\)

We can now state Proposition 3, which shows that there exists an optimal weight for which the SSL-W predictor achieves lower estimation error than the SSL Switching predictor, \(\hat{\bm{\theta}}_{\text{SSL-S}}^{*}\).

**Proposition 3**.: _Consider a distribution \(P_{XY}^{\bm{\theta}^{*}}\in\mathcal{P}_{2\text{-GMM}}^{(s)}\) and let \(d\geq 2\), and \(n_{l},n_{u}>0\). Let \(\hat{\bm{\theta}}_{\text{SSL-W}}^{*}(t^{*})\) be the SSL-W estimator introduced above. Then there exists a \(t^{*}\in(0,1)\) for which_

\[\mathbb{E}\left[\left\|\hat{\bm{\theta}}_{\text{SSL-S}}^{*}-\bm{\theta}^{*} \right\|^{2}\right]-\mathbb{E}\left[\left\|\hat{\bm{\theta}}_{\text{SSL-W}}^{* }(t^{*})-\bm{\theta}^{*}\right\|^{2}\right]=\min\left\{r,\frac{1}{r}\right\} \mathbb{E}\left[\left\|\hat{\bm{\theta}}_{\text{SSL-W}}^{*}(t^{*})-\bm{\theta}^ {*}\right\|^{2}\right],\] (28)

_where \(r=\frac{\mathbb{E}\left[\left\|\hat{\bm{\theta}}_{\text{UL}}^{*}-\bm{\theta}^{ *}\right\|^{2}\right]}{\mathbb{E}\left[\left\|\hat{\bm{\theta}}_{\text{UL}}- \bm{\theta}^{*}\right\|^{2}\right]}\), and the expectations are over \(\mathcal{D}_{l}\sim\left(P_{XY}^{\bm{\theta}^{*}}\right)^{n_{l}},\mathcal{D}_{u }\sim\left(P_{X}^{\bm{\theta}^{*}}\right)^{n_{u}}\)._Since the RHS of Equation (28) is always positive, \(\hat{\bm{\theta}}_{\text{SSL-W}}^{*}(t^{*})\) always outperforms \(\hat{\bm{\theta}}_{\text{SSL-S}}^{*}\) as long as the conditions of Proposition 3 are satisfied. The magnitude of the error gap between SSL-S and SSL-W depends on the gap between SL and UL+ (see Figure 2). The maximum gap is reached for \(\mathbb{E}\left[\left\|\hat{\bm{\theta}}_{\text{UL}}^{*}-\bm{\theta}^{*} \right\|^{2}\right]\approx\mathbb{E}\left[\left\|\hat{\bm{\theta}}_{\text{SL}} -\bm{\theta}^{*}\right\|^{2}\right]\) when SSL-W obtains half the error of SSL-S.

### Proof of Proposition 3

The first step in proving Proposition 3 is to express the estimation error of \(\hat{\bm{\theta}}_{\text{SSL-W}}^{*}(t^{*})\) in terms of the estimation errors of \(\hat{\bm{\theta}}_{\text{SL}}\) and \(\hat{\bm{\theta}}_{\text{UL}}^{*}\) which is captured by Lemma 1.

**Lemma 1**.: _Let \(\hat{\bm{\theta}}_{1}\) and \(\hat{\bm{\theta}}_{2}\) be two statistically independent estimators of \(\bm{\theta}^{*}\in\mathbb{R}^{d}\) and let \(\hat{\bm{\theta}}_{1}\) be unbiased, i.e. \(\mathbb{E}\left[\hat{\bm{\theta}}_{1}\right]=\bm{\theta}^{*}\). Then, the expected squared error of the weighted estimator \(\hat{\bm{\theta}}_{t^{*}}=t^{*}\hat{\bm{\theta}}_{1}+(1-t^{*})\hat{\bm{\theta} }_{2}\) with \(t^{*}=\frac{\mathbb{E}\left[\|\hat{\bm{\theta}}_{2}-\bm{\theta}^{*}\|^{2} \right]}{\mathbb{E}\left[\|\hat{\bm{\theta}}_{1}-\bm{\theta}^{*}\|^{2}\right] +\mathbb{E}\left[\|\hat{\bm{\theta}}_{2}-\bm{\theta}^{*}\|^{2}\right]}\) is given by_

\[\mathbb{E}\left[\|\hat{\bm{\theta}}_{t^{*}}-\bm{\theta}^{*}\|^{2}\right]= \left(\frac{1}{\mathbb{E}\left[\|\hat{\bm{\theta}}_{1}-\bm{\theta}^{*}\|^{2} \right]}+\frac{1}{\mathbb{E}\left[\|\hat{\bm{\theta}}_{2}-\bm{\theta}^{*}\|^{ 2}\right]}\right)^{-1}.\]

We can apply Lemma 1, since \(\hat{\bm{\theta}}_{\text{SL}}\) is unbiased and \(\hat{\bm{\theta}}_{\text{SL}}\) and \(\hat{\bm{\theta}}_{\text{UL}}^{*}\) are trained on \(\mathcal{D}_{l}\) and \(\mathcal{D}_{u}\) respectively, and hence, are independent. The proof then follows from calculating the difference between the harmonic mean and the minimum of estimation errors of \(\hat{\bm{\theta}}_{\text{SL}}\) and \(\hat{\bm{\theta}}_{\text{UL+}}\). Let \(x,y\in\mathbb{R}_{+}\) and w.l.o.g. assume \(x\leq y\). Then we have:

\[x-\left(\frac{1}{x}+\frac{1}{y}\right)^{-1}=x-\frac{xy}{x+y}=\frac{x^{2}}{x+y}= \frac{x}{y}\frac{xy}{x+y}.\]

Applying this identity concludes the proof after choosing \(t^{*}=\frac{\mathbb{E}\left[\|\hat{\bm{\theta}}_{2}-\bm{\theta}^{*}\|^{2} \right]}{\mathbb{E}\left[\|\hat{\bm{\theta}}_{1}-\bm{\theta}^{*}\|^{2}\right] +\mathbb{E}\left[\|\hat{\bm{\theta}}_{2}-\bm{\theta}^{*}\|^{2}\right]}\) and for \(x=\min\left\{\mathbb{E}[\|\hat{\bm{\theta}}_{\text{UL}}^{*}-\bm{\theta}^{*}\|^{ 2}],\mathbb{E}[\|\hat{\bm{\theta}}_{\text{SL}}-\bm{\theta}^{*}\|^{2}]\right\}\) and \(y=\max\left\{\mathbb{E}[\|\hat{\bm{\theta}}_{\text{UL}}^{*}-\bm{\theta}^{*}\|^{ 2}],\mathbb{E}[\|\hat{\bm{\theta}}_{\text{SL}}-\bm{\theta}^{*}\|^{2}]\right\}\).

Remark.Note that this lemma holds for arbitrary distributions and estimators as long as they are independent and one of them is unbiased. Therefore, future results that derive upper bounds for SL and UL+ for other distributional assumptions and estimators can seamlessly be plugged into Lemma 1. By the same argument, \(\hat{\bm{\theta}}_{\text{SSL-W}}\) obtained by other SL and UL+ estimators can also be expected to improve over the respective SL and UL+ estimators, given that one of them is unbiased.

Figure 2: Estimation error gap between SSL-S and SSL-W as revealed by Proposition 3 for varying SNR and \(n_{l}\) (\(n_{u}=10000\)). The maximum gap is reached at the switching point, indicated by the vertical dashed lines.

### Proof of Lemma 1

By definition of \(\hat{\bm{\theta}}_{t^{*}}\), we have

\[\mathbb{E}\left[\|\hat{\bm{\theta}}_{t^{*}}-\bm{\theta}^{*}\|^{2}\right] =\mathbb{E}\left[\|t^{*}\hat{\bm{\theta}}_{1}+(1-t^{*})\hat{\bm{ \theta}}_{2}-\bm{\theta}^{*}\|^{2}\right]\] \[=\mathbb{E}\left[t^{*2}\|\hat{\bm{\theta}}_{1}-\bm{\theta}^{*}\|^ {2}+(1-t^{*})^{2}\|\hat{\bm{\theta}}_{2}-\bm{\theta}^{*}\|^{2}+2t^{*}(1-t^{*} )(\hat{\bm{\theta}}_{1}-\bm{\theta}^{*})^{\top}(\hat{\bm{\theta}}_{2}-\bm{ \theta}^{*})\right]\] \[=\mathbb{E}\left[t^{*2}\|\hat{\bm{\theta}}_{1}-\bm{\theta}^{*}\|^ {2}+(1-t^{*})^{2}\|\hat{\bm{\theta}}_{2}-\bm{\theta}^{*}\|^{2}\right],\]

where the last equality holds due to the independence of \(\hat{\bm{\theta}}_{1}\) and \(\hat{\bm{\theta}}_{2}\) and the unbiasedness of \(\hat{\bm{\theta}}_{1}\).

Plugging in \(t^{*}=\frac{\mathbb{E}\|\hat{\bm{\theta}}_{2}-\bm{\theta}^{*}\|^{2}}{\mathbb{E }\|\hat{\bm{\theta}}_{1}-\bm{\theta}^{*}\|^{2}+\mathbb{E}\|\hat{\bm{\theta}}_{ 2}-\bm{\theta}^{*}\|^{2}}\), we get

\[\mathbb{E}\|\hat{\bm{\theta}}_{t^{*}}-\bm{\theta}^{*}\|^{2} =\left(\frac{\mathbb{E}\|\hat{\bm{\theta}}_{2}-\bm{\theta}^{*}\|^ {2}}{\mathbb{E}\|\hat{\bm{\theta}}_{1}-\bm{\theta}^{*}\|^{2}+\mathbb{E}\|\hat{ \bm{\theta}}_{2}-\bm{\theta}^{*}\|^{2}}\right)^{2}\mathbb{E}\|\hat{\bm{\theta} }_{1}-\bm{\theta}^{*}\|^{2}\] \[\qquad\qquad+\left(\frac{\mathbb{E}\|\hat{\bm{\theta}}_{1}-\bm{ \theta}^{*}\|^{2}}{\mathbb{E}\|\hat{\bm{\theta}}_{1}-\bm{\theta}^{*}\|^{2}+ \mathbb{E}\|\hat{\bm{\theta}}_{2}-\bm{\theta}^{*}\|^{2}}\right)^{2}\mathbb{E} \|\hat{\bm{\theta}}_{2}-\bm{\theta}^{*}\|^{2}\] \[=\frac{\mathbb{E}\|\hat{\bm{\theta}}_{1}-\bm{\theta}^{*}\|^{2} \mathbb{E}\|\hat{\bm{\theta}}_{2}-\bm{\theta}^{*}\|^{2}}{\mathbb{E}\|\hat{\bm {\theta}}_{1}-\bm{\theta}^{*}\|^{2}+\mathbb{E}\|\hat{\bm{\theta}}_{2}-\bm{ \theta}^{*}\|^{2}}\] \[=\frac{1}{\frac{\mathbb{E}\|\hat{\bm{\theta}}_{1}-\bm{\theta}^{*} \|^{2}}{\mathbb{E}\|\hat{\bm{\theta}}_{2}-\bm{\theta}^{*}\|^{2}}}.\]

## Appendix E Simulation details

### Methodology

We split each dataset in a test set, a validation set and a training set. The unlabeled set size is fixed to \(5000\) for the synthetic experiments and \(4000\) for the real-world datasets. The size of the labeled set \(n_{l}\) is varied in each experiment. For each dataset, we draw a different labeled subset \(20\) times and report the average and the standard deviation of the error gap (or the error) over these runs. The (unlabeled) validation set and the test set have \(1000\) labeled samples each.

We use logistic regression from Scikit-Learn [28] as the supervised algorithm. We use the validation set to select the ridge penalty for SL. For the unsupervised algorithm, we use an implementation of Expectation-Maximization from the Scikit-Learn library. We also use the self-training algorithm from Scikit-Learn with a logistic regression estimator. The best confidence threshold for the pseudolabels is selected using the validation set. Moreover, the optimal weight for SSL-W is also chosen with the help of the validation set. Since we use an unlabeled validation set, we need to employ an appropriate criterion for hyperparameter selection. Therefore, we select models that lead to a large (average) margin measured on the unlabeled validation set. We give SSL-S the benefit of choosing the optimal switching point between SL and UL+ by using the test set. Even with this important advantage, SSL-W (and sometimes self-training) still manage to outperform SSL-S.

### Details about the real-world datasets

Tabular data.We select tabular datasets from the OpenML repository [37] according to a number of criteria. We focus on high-dimensional data with \(100\leq d<1000\), where the two classes are not suffering from extreme class imbalance, i.e. the imbalance ratio between the majority and the minority class is at most \(5\). Moreover, we only consider datasets that have substantially more samples than the number of features, i.e. \(\frac{n}{d}>10\). In the end, we are left with 5 datasets, that span a broad range of application domains, from ecology to chemistry and finance.

To assess the difficulty of the datasets, we train logistic regression on the entire data that is available, and report the training error. Since we train on substantially more samples than the number of

[MISSING_PAGE_FAIL:22]

Figure 4: Error gap between SSL-S/self-training and SSL-W on real-world datasets. The positive gap indicates that SSL-W (and, in turn, self-training) outperforms SSL-S (and hence, also SL and UL+) for a broad range of \(n_{l}\) values.