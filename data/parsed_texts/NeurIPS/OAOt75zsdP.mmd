# Geometry-Calibrated DRO: Combating Over-Pessimism with Free Energy Implications

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Machine learning algorithms minimizing average risk are susceptible to distributional shifts. Distributionally Robust Optimization (DRO) addresses this issue by optimizing the worst-case risk within an uncertainty set. However, DRO suffers from over-pessimism, leading to low-confidence predictions, poor parameter estimations as well as poor generalization. In this work, we conduct a theoretical analysis of a probable root cause of over-pessimism: excessive focus on noisy samples. To alleviate the impact of noise, we incorporate data geometry into calibration terms in DRO, resulting in our novel Geometry-Calibrated DRO (GCDRO) for regression. We establish that our risk objective aligns with the Helmholtz free energy in statistical physics, and this free-energy-based risk can extend to standard DRO methods. Leveraging gradient flow in Wasserstein space, we develop an approximate minimax optimization algorithm with a bounded error ratio and standard convergence rate and elucidate how our approach mitigates noisy sample effects. Comprehensive experiments confirm GCDRO's superiority over conventional DRO methods.

## 1 Introduction

Machine learning algorithms with empirical risk minimization (ERM) have been shown to perform poorly under distributional shifts, especially sub-population shifts where substantial data subsets are underrepresented in the average risk due to their small sample sizes. As an alternative, Distributionally Robust Optimization (DRO) (Namkoong and Duchi, 2017; Blanchet and Murthy, 2019; Blanchet et al., 2019; Duchi and Namkoong, 2021; Zhai et al., 2021; Liu et al., 2022; Gao and Kleywegt, 2022; Gao et al., 2022) aims to optimize against the worst-case risk distribution within a predefined uncertainty set. This uncertainty set is centered around the training distribution, and generalization performance can be guaranteed when the test distribution falls within this set.

However, DRO methods have been found to experience the over-pessimism problem in practice (Hu et al., 2018; Zhai et al., 2021) (_i.e._, low-confidence predictions, poor parameter estimations, and generalization), recent studies have sought to address this issue. From the _uncertainty set perspective_, Blanchet et al. (2019); Liu et al. (2022, 2022) proposed data-driven methods to learn distance metrics from data. However, these approaches remain vulnerable to noisy samples, as demonstrated in Table 2. Recently, Slowik and Bottou (2022); Agarwal and Zhang (2022) observed that DRO may overly focus on sub-populations with higher noise levels, leading to suboptimal generalization. Consequently, from the _risk objective perspective_, they suggest incorporating calibration terms to mitigate this issue. Nevertheless, applicable calibration terms either require expert knowledge or are computationally intensive, and few practical algorithms have been proposed.

To devise a practical calibration term for DRO, we first aim to identify the root causes of over-pessimism, which we attribute to the excessive focus on noisy samples that frequently exhibit higherprediction errors. For typical DRO methods (Namkoong and Duchi, 2017; Staib and Jegelka, 2019; Duchi and Namkoong, 2021; Liu et al., 2022b), based on a simple yet insightful linear example, we theoretically demonstrate that the variance of estimated parameters becomes substantially large when noisy samples have higher densities, in line with the empirical findings reported in (Zhai et al., 2021). Furthermore, we demonstrate that existing outlier-robust regression methods are not directly applicable for mitigating noisy samples in DRO scenarios where both noisy samples and distribution shifts coexist, highlighting the non-trivial nature of this problem.

In this work, inspired by the ideas in (Slowik and Bottou, 2022; Agarwal and Zhang, 2022), we design calibration terms, \(i.e.\), total variation and entropy regularization, to prevent DRO from excessively focusing on random noisy samples. In conjunction with the Geometric Wasserstein uncertainty set (Liu et al., 2022b) utilized in our methods, these calibration terms effectively incorporate information from the data manifold, leading to improved regulation of the worst-case distribution in DRO. Specifically, during the optimization, the total variation term penalizes the variation of weighted prediction errors along the data manifold, preventing random noisy samples from gaining excessive densities. The entropy regularization term, also used in (Liu et al., 2022b), acts as a non-linear graph Laplacian operator that enforces the smoothness of the sample weights along the manifold. These calibration terms work together to render the worst-case distribution more _reasonable_ for DRO, leading to our Geometry-Calibrated DRO (GCDRO) approach. We validate the effectiveness of our GCDRO on both simulation and real-world data.

Furthermore, from a statistical physics perspective, we demonstrate that our risk objective corresponds to the Helmholtz free energy, comprising three components: interaction energy, potential energy, and entropy. The free energy formulation generalizes typical DRO methods such as KL-DRO, \(\chi^{2}\)-DRO (Duchi and Namkoong, 2021), MMD-DRO (Staib and Jegelka, 2019) and GDRO (Liu et al., 2022b). This physical interpretation provides a novel perspective for understanding different DRO methods by drawing parallels between the worst-case distribution and the steady state in statistical physics, offering valuable insights. From the free energy point of view, our GCDRO _specifically addresses the interaction energy between samples to mitigate the effects of noisy samples_. Motivated by the study of the Fokker-Planck equation (FPE, Chow et al. (2017); Esposito et al. (2021)), through gradient flow in the Geometric Wasserstein space, we derive an approximate minimax algorithm with a bounded error ratio \(e^{-CT_{in}}\) after \(T_{in}\) inner-loop iterations and a convergence rate of \(\mathcal{O}(1/\sqrt{T_{out}})\) after \(T_{out}\) outer-loop iterations. Our optimization method supports any quadratic form of interaction energy, potentially paving the way for designing more effective calibration terms for DRO in the future.

## 2 Preliminaries: Noisy Samples Bring Over-Pessimism in DRO

Notations.\(X\in\mathcal{X}\) denotes the covariates, \(Y\in\mathcal{Y}\) denotes the target, \(f_{\theta}(\cdot):\mathcal{X}\rightarrow\mathcal{Y}\) is the predictor parameterized by \(\theta\in\Theta\). \(\hat{P}_{N}\) denotes the empirical counterpart of distribution \(P(X,Y)\) with \(N\) samples, and \(\mathbf{p}=(p_{1},\ldots,p_{N})^{T}\in\mathbb{R}_{+}^{N}\) is the probability vector. \([N]=\{1,2,\ldots,N\}\) denotes the set of integers from 1 to \(N\). The random variable of data points is denoted by \(Z=(X,Y)\in\mathcal{Z}\). The random vector of \(n\) dimension is denoted by \(\widetilde{h}_{n}=(h_{1},\ldots,h_{n})^{T}\). \(G_{N}=(V,E,W)\) denotes a finite weighted graph with \(N\) nodes, where \(V=[N]\) is the vertex set, \(E\) is the edge set and \(W=\{w_{ij}\}_{(i,j)\in E}\) is the weight matrix of the graph. And \((x)_{+}=\max(x,0)\).

Distributionally Robust Optimization (DRO) is formulated as:

\[\theta^{*}(P)=\arg\min_{\theta\in\Theta}\sup_{Q\in\mathcal{P}(P)}\mathbb{E}_ {Q}[\ell(f_{\theta}(X),Y)]\] (1)

where \(\ell\) is the loss function (typically mean square error) and \(\mathcal{P}(P)=\{Q:\text{Dist}(Q,P)\leq\rho\}\) denotes the \(\rho\)-radius uncertainty ball around the distribution \(P\). Different distance metrics derive different DRO methods, e.g., \(f\)-divergence DRO (\(f\)-DRO, Namkoong and Duchi (2017); Duchi and Namkoong (2021)) with the Cressie-Read family of Renyi divergence, Wasserstein DRO (WDRO, Sinha et al. (2018); Blanchet and Murthy (2019); Blanchet et al. (2019, 2019)), MMD-DRO (Staib and Jegelka, 2019) with maximum mean discrepancy, and Geometric DRO (GDRO, Liu et al. (2022b)) with Geometric Wasserstein distance. Although DRO methods are designed to resist sub-population shifts, they have been observed to have poor generalization performances (Hu et al., 2018; Frogner et al., 2019; Slowik and Bottou, 2022) in practice, which is referred to as over-pessimism.

In this section, we identify one of the root causes of the over-pessimism of DRO: the _excessive focus on noisy samples with typically high prediction errors_.

\(\bullet\) We showcase DRO methods' excessive focus on noisy samples in practice and reveal their probability densities are linked to high prediction errors in worst-case distributions.

\(\bullet\) Through a simple yet insightful regression example, we prove that such a phenomenon leads to high estimation variances and subsequently poor generalization performance.

\(\bullet\) We demonstrate that existing outlier-robust regression methods are not directly applicable for mitigating noisy samples in DRO scenarios, emphasizing the non-trivial nature of this problem.

Problem SettingGiven the _underlying_ clean distribution \(P_{clean}=(1-\alpha)P_{major}+\alpha P_{minor},0<\alpha<\frac{1}{2}\), the **goal of DRO can be viewed as achieving good performance across all possible sub-populations \(P_{minor}\)**. Denote the observed contaminated training distribution by \(P_{train}\). Based on Huber's \(\epsilon\)-contamination model (Huber, 1992), we formulate \(P_{train}\) as:

\[P_{train}=(1-\epsilon)P_{clean}+\epsilon\tilde{Q}=\underbrace{(1-\epsilon)(1- \alpha)P_{major}}_{\text{mixer sub-population}}+\underbrace{(1-\epsilon)\alpha P _{minor}}_{\text{minor sub-population}}+\underbrace{\epsilon\tilde{Q}}_{\text{ noisy sub-population}},\] (2)

where \(\tilde{Q}\) is an arbitrary _noisy_ distribution (typically with larger noise scale), \(0<\epsilon<\frac{1}{2}\) is the noise level. Note that the _minor sub-population could represent any distribution with a proportion of \(\alpha\) in \(P\)_. However, we explicitly specify it here to emphasize the distinction between our setting and the traditional Huber's \(\epsilon\)-contaminated setting, as the latter does _not_ take sub-population shifts into account.

Empirical Observations.Following a typical regression setting (Duchi and Namkoong, 2021; Liu et al., 2022b), we demonstrate the worst-case distribution of KL-DRO, \(\chi^{2}\)-DRO, and GDRO in Figure 1, where the size of each point is proportional to its density. In this scenario, the underlying distribution \(P\) comprises a known major sub-population (95%, blue points) and a minor sub-population (5%, green points). And the noise level \(\epsilon\) in \(P_{train}\) is \(2\%\). DRO methods are expected to upweigh samples from minor sub-population to learn a model with uniform performances w.r.t. sub-populations. However, from Figure 1, we could observe that KL-DRO, \(\chi^{2}\)-DRO and GDRO excessively focus on noisy samples, resulting in a noise level 10 to 15 times larger than the original. This observation helps to explain their poor performance on this task (detailed results can be found in Table 2).

Theoretical Analysis.To support our observations, we first analyze the worst distribution of KL-DRO, \(\chi^{2}\)-DRO and GDRO, shedding light on the underlying reasons for this phenomenon.

**Proposition 2.1** (Worst-case Distribution).: _Let \(\hat{Q}_{N}^{*}=(q_{1}^{*},q_{2}^{*},\ldots,q_{N}^{*})^{T}\in\mathbb{R}_{+}^{N}\) denotes the worst-case distribution, and \(\ell(f_{\theta}(x_{i}),y_{i})\) (abbr. \(\ell_{i}\)) denotes the prediction error of sample \(i\in[N]\). For different choices of \(\text{Dist}(\cdot,\cdot)\) in \(\mathcal{P}(P)=\{Q:\text{Dist}(Q,P)\leq\rho\}\), we have: \(\bullet\) KL-DRO: \(q_{i}^{*}/q_{j}^{*}\propto\exp(\ell_{i}-\ell_{j})\); \(\bullet\) GDRO's final state (gradient flow step \(T\rightarrow\infty\)): \(q_{i}^{*}/q_{j}^{*}\propto\exp(\ell_{i}-\ell_{j})\); \(\bullet\)\(\chi^{2}\)-DRO: \(q_{i}^{*}/q_{j}^{*}=(\ell_{i}-\lambda)_{+}/(\ell_{j}-\lambda)_{+}\), and \(\lambda\geq 0\) is the dual parameter independent of \(i\)._

Proposition 2.1 demonstrates that for KL-DRO, \(\chi^{2}\)-DRO, and GDRO (large gradient flow step), the _relative density_ between samples is solely determined by their prediction errors, indicating that a larger prediction error results in a higher density. However, in our problem setting, samples from _both_ minor sub-population \(P_{minor}\)_and_ noisy sub-population \(\tilde{Q}\) exhibit high prediction errors. The primary goal of DRO is to focus on the minor sub-population \(P_{minor}\), but the presence of noisy samples in \(\tilde{Q}\) significantly interferes with this objective and hurts model learning. As shown in Figure 1, for KL-DRO, \(\chi^{2}\)-DRO and GDRO, noisy samples attract much density. Intuitively, it is not surprising that an excessive focus on noisy samples can have a detrimental impact. As KL-DRO, \(\chi^{2}\)-DRO, and

Figure 1: Visualizing the Worst-Case Distribution for Different DRO Methods: We show the data manifold and sample weights for each point, where blue points represent the major group, green ones represent the minor group, and red ones are noisy samples. The bars display the total sample weights of different groups, and the _original_ group ratio is major (**93.1%**), minor (**4.9%**), (noisy **2%**).

GDRO can be viewed as optimization within a weighted empirical distribution, we use the following simple example with the weighted least square model to demonstrate how this excessive focus on noisy samples can lead to high estimation variance, ultimately causing over-pessimism.

**Example** (Weighted Least Square).: _Consider the data generation process as \(Y=kX+\xi\), where \(X,Y\in\mathbb{R}\) and random noise \(\xi\) satisfies \(\xi\perp X\), \(\mathbb{E}[\xi]=0\) and \(\mathbb{E}[\xi^{2}]\) (abbr. \(\sigma^{2}\)) is finite. Assume that the training dataset \(X_{D}\) consists of clean samples \(\{x_{c}^{(i)},y_{c}^{(i)}\}_{i\in[N_{c}]}\) and noisy samples \(\{x_{o}^{(i)},y_{o}^{(i)}\}_{i\in[N_{c}]}\) with \(\sigma_{c}^{2}<\sigma_{o}^{2}\). Consider the weighted least-square model \(f(X)=\theta X\). Denote the sample weight of a clean sample \((x_{c}^{(i)},y_{c}^{(i)})\) as \(w_{c}^{(i)}\in\mathbb{R}_{+},i\in[N_{c}]\), and the sample weight of a noisy sample \((x_{o}^{(i)},y_{o}^{(i)})\) as \(w_{o}^{(i)}\in\mathbb{R}_{+},i\in[N_{o}]\) with \(\sum_{i\in[N_{c}]}w_{c}^{(i)}+\sum_{i\in[N_{c}]}w_{o}^{(i)}=1\). The variance of the estimator \(\hat{\theta}\) is given by:_

\[\text{Var}[\hat{\theta}|X_{D}]=\frac{\sum_{i=1}^{N_{c}}(w_{c}^{(i)})^{2}(x_{c} ^{(i)})^{2}\sigma_{c}^{2}+\sum_{i=1}^{N_{o}}(w_{o}^{(i)})^{2}(x_{o}^{(i)})^{2} \sigma_{o}^{2}}{\left[\sum_{i=1}^{N_{c}}w_{c}^{(i)}(x_{c}^{(i)})^{2}+\sum_{i=1 }^{N_{o}}w_{o}^{(i)}(x_{o}^{(i)})^{2}\right]^{2}},\] (3)

_where \(X_{D}=\{x_{c}^{(i)}\}_{1}^{N_{c}}\cup\{x_{o}^{(i)}\}_{1}^{N_{o}}\) are the sampled covariates in the dataset. Besides, the minimum variance is achieved if and only if \(\forall 1\leq i\leq N_{c},1\leq j\leq N_{o},w_{o}^{(j)}/w_{c}^{(i)}=\sigma_{c}^ {2}/\sigma_{o}^{2}<1\)._

From the results, we make the following remarks:

\(\bullet\) If noisy samples have higher weights than clean samples (e.g., \(w_{o}/w_{c}>1\)), the variance of the estimated parameter \(\hat{\theta}\) will be larger, suggesting that the learned \(\hat{\theta}\) could be significantly unstable.

\(\bullet\) In conjunction with Proposition 2.1, DRO methods tend to assign high weights to noisy samples, which can lead to unstable parameter estimation. While this example is relatively simple, this phenomenon aligns with the empirical findings in Zhai et al. (2021), which demonstrate that DRO methods can be quite unstable when confronted with label noise.

Relationship with Conventional Outlier-robust Regression.We would like to explain why conventional outlier-robust regression methods cannot be directly applied to our problem. The main challenge stems from the _coexistence_ of noisy samples and minor sub-populations, both of which typically exhibit high prediction errors, leading to a misleading worst-case distribution in DRO. Conventional outlier-robust regression methods (Diakonikolas and Kane, 2018; Klivans et al., 2018; Diakonikolas et al., 2022) primarily focus on mitigating the effects of outliers without considering sub-population shifts. For instance, the \(L_{2}\)-estimation-error of outlier-robust linear regression is \(\mathcal{O}(\epsilon\log(1/\epsilon))\)(Diakonikolas and Kane, 2018), where \(\epsilon\) represents the noise level in Equation 1. However, as analyzed in Proposition 2.1 and demonstrated in Figure 1, during the optimization of DRO, the noise level \(\epsilon\) significantly increases, rendering even outlier-robust estimation quite inaccurate. Moreover, Klivans et al. (2018) propose finding a pseudo distribution with minimal prediction errors to avoid outliers (see Algorithm 5.2 in (Klivans et al., 2018)). Nevertheless, this approach might inadvertently exclude minor sub-populations, which should be the focus under sub-population shifts, due to the main challenge: the _coexistence_ of noisy samples and minor sub-populations. Zhai et al. (2021) incorporate this idea into DRO. Still, their method requires an implicit assumption that the prediction errors of noisy samples are higher than those of minor sub-populations, which does not always hold in practice. And Bennouna and Van Parys (2022) build the uncertainty set via two measures, KL-divergence and Wasserstein distance, leading to a combined approach of KL-DRO and ridge regression. Despite this, as we discussed earlier, DRO tends to increase the noise level in data, making it difficult to fix using ridge regression.

Based on the analysis above, we stress the importance of integrating more data-derived information. In pursuit of this, we propose to leverage the unique geometric properties that distinguish noisy samples from minor sub-populations to address this issue.

## 3 Proposed Method

In this work, with a focus on regression, we introduce our Geometry-Calibrated DRO (GCDRO). The fundamental idea is to utilize data geometry to distinguish between random noisy samples and minor sub-populations. It is motivated by the fact that prediction errors for minor sub-populations typically exhibit local smoothness along the data manifold, a property that is not shared by noisy samples.

Discrete Geometric Wasserstein Distance.We briefly revisit the definition of the discrete geometric Wasserstein distance. Given a weighted finite graph \(G_{N}=(V,E,W)\), the probability set \(\mathscr{P}(G_{N})\) supported on the vertex set \(V\) is defined as \(\mathscr{P}(G_{N})=\{\mathbf{p}\in\mathbb{R}^{N}|\sum_{i=1}^{N}p_{i}=1,p_{i}\geq 0,\text {for }i\in V\}\), and its interior is denoted as \(\mathscr{P}_{o}(G_{N})\). A velocity field \(\mathbf{v}=(v_{ij})_{i,j\in V}\in\mathbb{R}^{N\times N}\) on \(G_{N}\) is defined on the edge set \(E\) satisfying that \(v_{ij}=-v_{ji}\) if \((i,j)\in E\). \(\xi_{ij}(\mathbf{p})\) is a function interpolated with the associated nodes' densities \(p_{i},p_{j}\). The flux function \(\mathbf{p}\mathbf{v}\in\mathbb{R}^{N\times N}\) on \(G_{N}\) is defined as \(\mathbf{p}\mathbf{v}:=(v_{ij}\xi_{ij}(\mathbf{p}))_{(i,j)\in E}\) and its divergence is defined as \(\text{div}_{G_{N}}(\mathbf{p}\mathbf{v}):=-(\sum_{j\in V:(i,j)\in E}\sqrt{w_{ ij}}v_{ij}\xi_{ij}(\mathbf{p}))_{1}^{N}\in\mathbb{R}^{N}\). Then for distributions \(\mathbf{p}_{0},\mathbf{p}_{1}\in\mathscr{P}_{o}(G_{N})\), the discrete geometric Wasserstein distance (Chow et al., 2017; Liu et al., 2022b) is defined as:

\[\mathcal{GW}_{G_{N}}^{2}(\mathbf{p}_{0},\mathbf{p}_{1}):=\inf_{v}\left\{\int_{ 0}^{1}\frac{1}{2}\sum_{(i,j)\in E}\xi_{ij}(\mathbf{p}(t))v_{ij}^{2}dt\quad \text{s.t.}\frac{d\mathbf{p}}{dt}+\text{div}_{G_{N}}(\mathbf{p}\mathbf{v})=0, \mathbf{p}(0)=\mathbf{p}_{0},\mathbf{p}(1)=\mathbf{p}_{1}\right\}.\] (4)

Equation 4 computes the shortest (geodesic) length among all potential plans, integrating the total kinetic energy of the velocity field throughout the transportation process. A key distinction from the Wasserstein distance is that it only permits density to appear at the graph nodes.

FormulationGiven training dataset \(D_{tr}=\{(x_{i},y_{i})\}_{i=1}^{N}\) and a finite weighted graph \(G_{N}=(V,E,W)\) representing the inherent structure of sample covariates. Denote the empirical marginal distribution as \(\hat{P}_{X}\), the formulation of GCDRO is:

\[\min_{\theta\in\Theta}\underbrace{\sup_{\mathbf{q},\mathcal{GW}_{G_{N}}^{2}( \hat{P}_{X},\mathbf{q})\leq\rho}}_{\text{Geometric Wasserstein set}}\bigg{\{} \mathcal{R}_{N}(\theta,\mathbf{q}):=\sum_{i=1}^{N}q_{i}\ell(f_{\theta}(x_{i}),y _{i})-\underbrace{\frac{\alpha}{2}\cdot\sum_{(i,j)\in E}w_{ij}q_{i}q_{j}(\ell_ {i}-\ell_{j})^{2}}_{\text{Calibration Term 1}}-\underbrace{\beta\cdot\sum_{i=1}^{N}q_{i}\log q_{i}}_{\text{ Calibration Term 1}}\bigg{\}},\] (5)

where \(\rho\) is the pre-defined radius of the uncertainty set, \(\ell_{i}\) is the loss on the \(i\)-th sample and \(w_{ij}\in W\) denotes the edge weight between sample \(i\) and \(j\). \(\alpha\) and \(\beta\) are hyper-parameters.

Illustrations.In our formulation, for any distribution \(\mathbf{q}\) within the uncertainty set,

Calibration term **I** (\(\sum_{(i,j)\in E}w_{ij}q_{i}q_{j}(\ell_{i}-\ell_{j})^{2}\)) calculates the _graph total variation_ of prediction errors along the data manifold that is characterized by \(G_{N}\). Intuitively, when _selecting the worst-case distribution_, this term imposes a penalty on distributions that allocate high densities to random noisy samples, as this allocation significantly amplifies the overall variation in prediction errors. Conversely, this term does not penalize distributions that allocate high densities to minor sub-populations, as their errors are smooth and have a relatively small impact on the total variation along the manifold. This differing phenomenon arises from the distinct geometric properties of random noisy samples and minor sub-populations, as samples from the latter typically cluster together on the data manifold. Further, _during the optimization of model parameter \(\theta\)_, this term acts like a variance term, resulting in a quantile-like risk objective, which helps to mitigate the effects of outliers.

Calibration term **II** (\(\sum_{i=1}^{N}q_{i}\log q_{i}\)) represents the negative entropy of distribution \(\mathbf{q}\). As discussed in Section 3.2, during optimization, this term transforms into a non-linear _graph Laplacian operator_ that encourages sample weights to be smooth along the manifold, avoiding extreme sample weights in the worst-case distribution.

### Free Energy Implications on Worst-case Distribution

We first demonstrate the free energy implications of our risk objective \(\mathcal{R}_{N}(\theta,\mathbf{q})\). Intuitively, the change of sample weights across \(N\) samples (the inner maximization problem of \(\mathcal{R}_{N}(\theta,\mathbf{q})\)) can be analogously related to the dynamics of particles in a system, wherein the concentration of densities coincides with the aggregation of particle masses at \(N\) distinct locations (in the case of infinite samples, these locations converge to the data manifold). As a result, a deeper understanding of the steady state in a particle system can offer _valuable insights into the worst-case distribution_ for DRO.

Building on this analogy, we can dive deeper into the physics of particle interactions. When particles exist within a potential energy field, they are subject to external forces. Simultaneously, there are interactions among the particles themselves, leading to a constant state of motion within the system. In statistical physics, a key point of interest is identifying when a system reaches a steady state. In a standard process like the reversible isothermal process, it is established that spontaneous reactions consistently move in the direction of decreasing _Helmholtz free energy_(Fu et al., 1990; Reichl, 1999;Friston, 2010), which consists of interaction energy, potential energy and the negative entropy:

\[\mathcal{E}(\mathbf{q})=\underbrace{\mathbf{q}^{\top}K\mathbf{q}}_{\text{ Interaction Energy}}+\underbrace{\mathbf{q}^{\top}V}_{\text{Potential Energy}}-\beta\sum_{i=1}^{N}(-q_{i}\log q_{i})=-\mathcal{R}_{N}(\theta,\mathbf{q}).\] (6)

By taking \(V=-\vec{\ell}\) and \(K_{ij}=\frac{\alpha}{2}w_{ij}(\ell_{i}-\ell_{j})^{2}\) for \((i,j)\in E\), our risk objective is a special case of Helmholtz free energy, where the potential energy of sample \(i\) is \(-\ell_{i}q_{i}\) and the interaction energy between sample \(i\) and \(j\) is \(\frac{\alpha}{2}w_{ij}(\ell_{i}-\ell_{j})^{2}q_{i}q_{j}\). Specifically, such mutual interactions can manifest as _repulsive forces between adjacent particles_, thereby preventing the concentration of mass in locations where local prediction errors are significantly high. And this explains from a physical perspective why our calibration term **I** could mitigate random noisy samples.

Additionally, Proposition 3.1 offers physical interpretations to comprehend the worst-case distribution of various DRO methods. We make some remarks: (1) current DRO methodologies, except MMD-DRO, do not explicitly formulate the interaction term between samples in their design considerations (\(\chi^{2}\)-DRO does not involve interaction between samples), despite the corresponding interaction energy between particles being a common phenomenon in physics; (2) MMD-DRO simply uses kernel gram matrix for interaction and lacks efficient optimization algorithms; (3) by _considering this interaction energy_, our proposed GCDRO is capable of mitigating the impacts of random noisy samples.

**Proposition 3.1** (Free Energy Implications).: _The dual reformulations of some typical DRO methods are equivalent to the free-energy-based minimax problem \(\min_{\theta\in\Theta,\lambda\geq 0}\max_{\mathbf{q}\in\mathcal{P}}\left\{ \lambda\rho-\mathcal{E}(\mathbf{q},\theta,\lambda)\right\}\) with different choices of \(\mathcal{P},\rho\) and \(K,V,H[q]\) in the free energy \(\mathcal{E}\). Details are shown in Table 1._

Through free energy, we could understand the type of energy or steady state that DRO methods strive to achieve, and design better interaction energy terms in DRO. Moreover, our optimization, as outlined in Section 3.2, could accommodate multiple quadratic forms of interaction energy.

### Optimization

Then we derive an approximate minimax optimization for our GCDRO. For the _inner maximization_ problem, we approximately deal with it via the gradient flow of \(-\mathcal{R}_{N}(\theta,Q)\) w.r.t. \(Q\) in the geometric Wasserstein space \((\mathcal{P}_{o}(G_{N}),\mathcal{GW}_{G_{N}})\). We show that the error rate is \(\mathcal{O}(e^{-CT_{in}})\) after \(T_{in}\) iterations inner loop, which gives a nice approximation. For the _outer minimization_ w.r.t. model parameters \(\theta\), we analyze the convergence rate of \(\mathcal{O}(1/\sqrt{T_{out}})\) after \(T_{out}\) iterations outer loop when the risk function satisfies Lipschitzian smoothness conditions.

**Inner Maximization**. We denote the _Continuous gradient flow_ as \(\mathbf{q}:[0,T]\rightarrow\mathcal{P}_{o}(G_{N})\), the probability density of sample \(i\) at time \(t\) is abbreviated as \(q_{i}(t)\), and the _Time-discretized gradient flow_ with time step \(\tau\) as \(\hat{\mathbf{q}}_{\tau}\). For inner maximization, we utilize the \(\tau\)-time-discretized gradient flow (Villani, 2021) for \(-\mathcal{R}_{N}(\theta,\mathbf{q})\) in the geometric Wasserstein space \((\mathcal{P}_{o}(G_{N}),\mathcal{GW}_{G_{N}}^{2})\) as:

\[\hat{\mathbf{q}}_{\tau}(t+\tau)=\operatorname*{argmax}_{\mathbf{q}\in\mathcal{ S}_{o}(G_{N})}\mathcal{R}_{N}(\theta,\mathbf{q})-\frac{1}{2\tau}\mathcal{GW}_{G_{N}}^{2}( \hat{\mathbf{q}}_{\tau}(t),\mathbf{q}).\] (7)

The gradient of \(\mathbf{q}\) in Equation 7 is given as (when \(\tau\to 0\)):

\[\frac{dq_{i}}{dt}=\sum_{(i,j)\in E}w_{ij}\xi_{ij}\bigg{(}\mathbf{q},\ \ \ell_{i}-\ell_{j}+\beta(\log q_{j}-\log q_{i})+\alpha\big{(}\sum_{h\in N(j)}( \ell_{h}-\ell_{j})^{2}w_{jh}q_{h}-\sum_{h\in N(i)}(\ell_{h}-\ell_{i})^{2}w_{ih} q_{h}\big{)}\bigg{)},\] (8)

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{3}{c}{Energy Type} & \multicolumn{3}{c}{Specific Formulation} \\ \cline{2-7}  & Interaction & Potential & Entropy & \(K\) & \(V\) & \(H[\mathbf{q}]\) & \(\mathcal{P}\) \\ \hline KL-DRO & ✗ & ✗ & ✗ & - & \(-\vec{\ell}\) & \(H[\mathbf{q}]\) & \(\Delta_{N}\) \\ \(\chi^{2}\)-DRO & ✗ & ✗ & \(\lambda I\) & - & - & \(\Delta_{N}\) \\ MMD-DRO & ✗ & ✗ & Kernel Gram & & \(-\vec{\ell}-\frac{\lambda}{2}K^{\top}\mathbf{1}\) & - & \(\Delta_{N}\) \\ Marginal \(\chi^{2}\)-DRO & ✗ & ✗ & ✗ & - & \(-(\vec{\ell}-\psi)_{+}\) & - & \(\Delta_{N}\) with Hölder \\  & & & & & & & continuity \\ GDRO & ✗ & ✗ & ✗ & - & \(-\vec{\ell}\) & \(H[\mathbf{q}]\) & \(\text{Geometric}\) \\ GCDRO & ✗ & ✗ & ✗ & Interaction & \(-\vec{\ell}\) & \(H[\mathbf{q}]\) & \(\text{Geometric}\) \\ GCDRO & ✗ & ✗ & ✗ & Matrix \(K\) & \(-\vec{\ell}\) & \(H[\mathbf{q}]\) & \(\text{Wasserstein Set}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Free energy implications of some DRO methods. \(\Delta_{N}\) denotes the \(N\)-dimensional simplex, \(\eta\) in marginal DRO is the dual parameter.

where \(E\) is the edge set of \(G_{N}\), \(w_{ij}\) is the edge weight between node \(i\) and \(j\), \(N(i)\) denotes the set of neighbors of node \(i\), \(\ell_{i}\) denotes the loss of sample \(i\), and \(\xi_{ij}(\cdot,\cdot):\mathscr{P}(G_{N})\times\mathbb{R}\to\mathbb{R}\) is:

\[\xi_{ij}(\mathbf{q},v):=v\cdot(\mathbb{I}(v>0)q_{j}+\mathbb{I}(v\leq 0)q_{i}),v \in\mathbb{R},\] (9)

which is the _upwind interpolation_ commonly used in statistical physics and guarantees that the probability vector \(\mathbf{q}\) keeps positive. From the gradient, we could see that the entropy regularization acts as a non-linear graph Laplacian operator to make the sample weights smooth along the manifold. In our algorithm, we fix the steps of the gradient flow to be \(T_{in}\) and prove that the error ratio is \(e^{-CT_{in}}\) compared with the _ground-truth_ worst-case risk \(\mathcal{R}_{N}(\theta,\mathbf{q}^{*})\) constrained in an \(\rho(\theta,T_{in})\)-radius ball.

**Proposition 3.2** (Approximation Error Ratio).: _Given the model parameter \(\theta\), denote the distribution after time \(T_{in}\) as \(\mathbf{q}^{T_{in}}(\theta)\), and the distance to training distribution \(\hat{P}_{X}\) as \(\rho(\theta,T_{in}):=\mathcal{GW}_{G_{N}}^{2}(\hat{P}_{X},\mathbf{q}^{T_{in}} (\theta))\) (abbr. \(\rho(\theta)\)). Assume \(\mathcal{R}_{N}(\theta,\mathbf{q})\) is convex w.r.t \(\mathbf{q}\). Then define the ground-truth worst-case distribution \(q^{*}(\theta)\) within the \(\rho(\theta)\)-radius ball as:_

\[\mathbf{q}^{*}(\theta):=\arg\sup_{\mathbf{q}:\mathcal{GW}_{G_{N}}^{2}(\hat{P}_ {X},\mathbf{q})\leq\rho(\theta)}\mathcal{R}_{N}(\theta,\mathbf{q}).\] (10)

_The upper bound of the error rate of the objective function \(\mathcal{R}_{N}(\theta,\mathbf{q}^{T_{in}})\) satisfies:_

\[(\mathcal{R}_{N}(\theta,\mathbf{q}^{*})-\mathcal{R}_{N}(\theta,\mathbf{q}^{T_ {in}}))/\left(\mathcal{R}_{N}(\theta,\mathbf{q}^{*})-\mathcal{R}_{N}(\theta, \hat{P}_{X})\right)<e^{-CT_{in}},\] (11)

\[C=2m\lambda_{\text{acc}}(\hat{L})\lambda_{\text{min}}(\nabla^{2}\mathcal{R}_{ N})\frac{1}{(r+1)^{2}}>0,\] (12)

_where \(\hat{L}\) is the Laplacian matrix of \(G_{N}\). \(\lambda_{\text{acc}},\lambda_{\text{min}}\) are the second smallest and smallest eigenvalue, \(m,r\) are constants depending on \(\mathcal{R}_{N},G_{N},\beta\)._

We make some remarks:

\(\bullet\) For the assumption that \(\mathcal{R}_{N}\) is convex w.r.t. \(\mathbf{q}\), the Hessian is given by \(\nabla^{2}\mathcal{R}_{N}=\beta\text{diag}(1/q_{1},...,1/q_{N})+2K\). Since \(K\) is a sparse matrix whose nonzero elements in each row is far smaller than \(N\), it is easily satisfied in empirical settings that the Hessian matrix \(\nabla^{2}\mathcal{R}\) is diagonally dominant and thus positive definite, making the inner maximization concave w.r.t \(\mathbf{q}\).

\(\bullet\) During the optimization, our algorithm finds an approximate worst-case distribution that is close to the ground-truth one within a \(\rho(\theta)\)-radius uncertainty set. Our robustness guarantee is similar to Sinha et al. (2018) (see Equation 12 in Sinha et al. (2018)).

\(\bullet\) The error ratio is \(e^{-CT_{in}}\), enabling to find a nice approximation efficiently with finite \(T_{in}\) steps.

**Outer Minimization.** The convergence property relies on the risk objective \(\mathcal{R}_{N}(\theta,\mathbf{q})\). When \(\mathcal{R}_{N}(\theta,\mathbf{q})\) is _smooth_ w.r.t. \(\theta\), the following proposition guarantees convergence to a stationary point of problem 5 at a standard rate of \(\mathcal{O}(1/\sqrt{T})\).

**Proposition 3.3** (Convergence).: _Assume \(F(\theta):=\sup_{\mathbf{q}:\mathcal{GW}_{G_{N}}^{2}(\hat{P}_{X},\mathbf{q}) \leq\rho(\theta)}\mathcal{R}_{N}(\theta,\mathbf{q})\) is \(L\)-smooth, and \(\mathcal{R}_{N}(\theta,\mathbf{q})\) is \(L_{q}\)-smooth w.r.t. \(\mathbf{q}\) such that \(\|\nabla_{\mathbf{q}}\mathcal{R}_{N}(\theta,\mathbf{q})-\nabla_{\mathbf{q}} \mathcal{R}_{N}(\theta,\mathbf{q}^{\prime})\|_{2}\leq L_{q}\|\mathbf{q}- \mathbf{q}^{\prime}\|_{2}\). \(\rho(\theta)\) follows the definition in Proposition 3.2. Take a constant \(\Delta_{F}\geq F(\theta^{(0)})-\inf_{\theta}F(\theta)\) and set step size as \(\alpha=\sqrt{\Delta_{F}/(LT)}\). For \(\|\mathbf{q}^{T_{in}}-\mathbf{q}^{*}\|_{2}^{2}\leq\gamma\) of the inner maximization problem, we have:_

\[\frac{1}{T}\mathbb{E}\left[\sum_{t=1}^{T}\|\nabla_{\theta}F(\theta^{(t)})\|_{2 }^{2}\right]-\frac{1+2L\alpha}{1-2L\alpha}L_{q}^{2}\gamma\leq\frac{2\Delta_{F }}{\sqrt{\Delta_{F}T}-2L\Delta_{F}}.\] (13)

From Proposition 3.3, as \(T\to\infty\), \(\nabla_{\theta}F(\theta^{(t)})\) exhibits a standard square-root convergence. Furthermore, the parameter \(\gamma\) can be effectively controlled, owing to the concavity inherent in the inner maximization problem and the rapidly diminishing error ratio as described in Proposition 3.2.

### Mitigate the Effects of Random Noisy Samples

Finally, we prove that our GCDRO method effectively de-emphasizes 'noisy samples' with locally non-smooth prediction errors. Due to the challenge of assessing intermediate states in gradient flow, we focus on its final state (as \(T_{in}\to\infty\)). Notably, in Proposition 3.2, the convergence rate of gradient flow is \(\mathcal{O}(e^{-CT_{in}})\), implying that an efficient approximation of the final state is feasible.

For the worst-case distribution \(q^{*}\), we denote the density ratio between samples as \(\gamma(i,j):=q_{i}^{*}/q_{j}^{*}\). In sensitivity analysis, when _only_ sample \(i\) is perturbed with label noises, we denote the density ratio 

[MISSING_PAGE_FAIL:8]

5 is effective to mitigate label noises. From Figure 1, the worst-case distribution of our GCDRO _significantly upweighs on the minority_ (green points) and does not put much density on the noisy data (red points), while the others put much higher weights on the noisy samples and perform poorly.

### Real-world Data

We use three real-world regression datasets with natural distributional shifts, including bike-sharing prediction, house price, and temperature prediction. For all these experiments, we use a two-layer _MLP_ model with mean square error (MSE). We use the Adam optimizer Kingma and Ba (2015) with the default learning rate \(1e-3\). And all methods are trained for \(5e3\) epochs.

Datasets.(1) **Bike-sharing** dataset (Dua and Graff, 2017) contains the daily count of rental bikes in the Capital bike-sharing system with the corresponding 11 weather and seasonal covariates. The task is to predict the count of rental bikes of _casual users_. Note that the count of casual users is likely to be more _random and noisy_, which is suitable to verify the effectiveness of our method. We split the dataset according to the season for natural shifts. In the training data, the ratio of four seasons' data is \(9:7:5:3\). We test on the rest of the data and report the prediction error of each season.

(2) **House Price** dataset1 contains house sales prices from King County, USA. The task is to predict the transaction price of the house via 17 predictive covariates such as the number of bedrooms, square footage of the house, etc. We divide the data into 5 sub-populations according to the built year of each house with each sub-population covering a span of 25 years. In training, we use data from the first group (built year \(<1920\)) and report the prediction error for each testing group.

(3) **Temperature** dataset (Dua and Graff, 2017) is largely composed of the LDAPS model's next day's forecast data, in-situ maximum and minimum temperatures of present-day, and geographic auxiliary variables in South Korea from 2013 to 2017. The task is to predict the next-day's maximum air temperatures based on the 22 covariates. We divide the data into 5 groups corresponding with 5 years. In the training data, the ratio of five years' data is \(9:7:5:3:1\). We test on the rest of the data and report the prediction error of each year. More details could be found in Appendix.

Footnote 1: https://www.kaggle.com/c/house-prices-advanced-regression- techniques/data

Analysis.(1) From the results in Figure 4.1, we could see that the performances of ERM drop a lot under distributional shifts, and DRO methods have better performance as well as robustness. (2) Our proposed GCDRO outperforms all baselines under strong shifts, with the most stable performances under natural distributional shifts. (3) As for the \(k\)NN graph's fitting accuracy of the data manifold, we visualize the learned manifold in Appendix and we could see that the learned \(k\)NN graph fits the data manifold well. Besides, we show in Appendix that the performances of our GCDRO are relatively stable across different choices of \(k\). Also, our GCDRO only needs the input graph \(G_{N}\) to represent the data structure and _any manifold learning or graph learning_ methods could be plugged in to give a better estimation of \(G_{N}\).

## 5 Future Directions

Our work deals with the over-pessimism in DRO via geometric calibration terms and provides free energy implications. The high-level idea could inspire future research on (1) relating free energy with DRO; (2) designing more reasonable calibration terms in DRO; (3) incorporating data geometry in general risk minimization algorithms.

Figure 2: Results of real-world datasets with natural shifts. We do not manually add label noises here, since real-world datasets intrinsically contain noises.

## References

* Agarwal and Zhang (2022) Agarwal, A. and Zhang, T. (2022). Minimax regret optimization for robust machine learning under distribution shift. _arXiv preprint arXiv:2202.05436_.
* Bennouna and Van Parys (2022) Bennouna, A. and Van Parys, B. (2022). Holistic robust data-driven decisions. _arXiv preprint arXiv:2207.09560_.
* Blanchet et al. (2019a) Blanchet, J., Kang, Y., and Murthy, K. (2019a). Robust wasserstein profile inference and applications to machine learning. _Journal of Applied Probability_, 56(3):830-857.
* Blanchet and Murthy (2019) Blanchet, J. and Murthy, K. (2019). Quantifying distributional model risk via optimal transport. _Mathematics of Operations Research_, 44(2):565-600.
* Blanchet et al. (2019b) Blanchet, J. H., Kang, Y., Murthy, K. R. A., and Zhang, F. (2019b). Data-driven optimal transport cost selection for distributionally robust optimization. In _2019 Winter Simulation Conference, WSC 2019, National Harbor, MD, USA, December 8-11, 2019_, pages 3740-3751. IEEE.
* Chow et al. (2017) Chow, S.-N., Li, W., and Zhou, H. (2017). Entropy dissipation of fokker-planck equations on graphs. _arXiv preprint arXiv:1701.04841_.
* Diakonikolas and Kane (2018) Diakonikolas, I. and Kane, D. M. (2018). Algorithmic high-dimensional robust statistics. _Webpage http://www. iliasdiakonikolas. org/simons-tutorial-robust. html_.
* Diakonikolas et al. (2022) Diakonikolas, I., Kane, D. M., Pensia, A., and Pittas, T. (2022). Streaming algorithms for high-dimensional robust statistics. In _International Conference on Machine Learning_, pages 5061-5117. PMLR.
* Dua and Graff (2017) Dua, D. and Graff, C. (2017). Uci machine learning repository.
* Duchi and Namkoong (2021) Duchi, J. C. and Namkoong, H. (2021). Learning models with uniform performance via distributionally robust optimization. _The Annals of Statistics_, 49(3):1378-1406.
* Esposito et al. (2021) Esposito, A., Patacchini, F. S., Schlichting, A., and Slepcev, D. (2021). Nonlocal-interaction equation on graphs: gradient flow structure and continuum limit. _Archive for Rational Mechanics and Analysis_, 240(2):699-760.
* Friston (2010) Friston, K. (2010). The free-energy principle: a unified brain theory? _Nature reviews neuroscience_, 11(2):127-138.
* Frogner et al. (2019) Frogner, C., Claici, S., Chien, E., and Solomon, J. (2019). Incorporating unlabeled data into distributionally robust learning. _arXiv preprint arXiv:1912.07729_.
* Fu et al. (1990) Fu, X., Shen, W., Yao, T., and Hou, W. (1990). Physical chemistry. _Higher Education, Beijing_.
* Gao et al. (2022) Gao, R., Chen, X., and Kleywegt, A. J. (2022). Wasserstein distributionally robust optimization and variation regularization. _Operations Research_.
* Gao and Kleywegt (2022) Gao, R. and Kleywegt, A. (2022). Distributionally robust stochastic optimization with wasserstein distance. _Mathematics of Operations Research_.
* Hu et al. (2018) Hu, W., Niu, G., Sato, I., and Sugiyama, M. (2018). Does distributionally robust supervised learning give robust classifiers? In _International Conference on Machine Learning_, pages 2029-2037. PMLR.
* Huber (1992) Huber, P. J. (1992). Robust estimation of a location parameter. _Breakthroughs in statistics: Methodology and distribution_, pages 492-518.
* Kingma and Ba (2015) Kingma, D. P. and Ba, J. (2015). Adam: A method for stochastic optimization. In Bengio, Y. and LeCun, Y., editors, _3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings_.
* Klivans et al. (2018) Klivans, A., Kothari, P. K., and Meka, R. (2018). Efficient algorithms for outlier-robust regression. In _Conference On Learning Theory_, pages 1420-1430. PMLR.

* Liu et al. (2022a) Liu, J., Shen, Z., Cui, P., Zhou, L., Kuang, K., and Li, B. (2022a). Distributionally robust learning with stable adversarial training. _IEEE TKDE_.
* Liu et al. (2022b) Liu, J., Wu, J., Li, B., and Cui, P. (2022b). Distributionally robust optimization with data geometry. In _Advances in Neural Information Processing Systems_.
* Namkoong and Duchi (2017) Namkoong, H. and Duchi, J. C. (2017). Variance-based regularization with convex objectives. _Advances in neural information processing systems_, 30.
* Reichl (1999) Reichl, L. E. (1999). A modern course in statistical physics.
* May 3, 2018, Conference Track Proceedings_. OpenReview.net.
* Slowik and Bottou (2022) Slowik, A. and Bottou, L. (2022). On distributionally robust optimization and data rebalancing. In _International Conference on Artificial Intelligence and Statistics_, pages 1283-1297. PMLR.
* Staib and Jegelka (2019) Staib, M. and Jegelka, S. (2019). Distributionally robust optimization and generalization in kernel methods. _Advances in Neural Information Processing Systems_, 32.
* Villani (2021) Villani, C. (2021). Topics in optimal transportation. 58.
* Zhai et al. (2021) Zhai, R., Dan, C., Kolter, Z., and Ravikumar, P. (2021). Doro: Distributional and outlier robust optimization. In _International Conference on Machine Learning_, pages 12345-12355. PMLR.