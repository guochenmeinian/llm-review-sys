# FinBen: A Holistic Financial Benchmark for Large Language Models

 Qianqian Xie\({}^{b,a}\), Weiguang Han\({}^{b}\), Zhengyu Chen\({}^{b}\), Ruoyu Xiang\({}^{a}\), Xiao Zhang\({}^{a}\), Yueru He\({}^{a}\), Mengxi Xiao\({}^{b}\), Dong Li\({}^{b}\), Yongfu Dai\({}^{g}\), Duanyu Feng\({}^{g}\), Yijing Xu\({}^{a}\), Haoqiang Kang\({}^{c}\), Ziyan Kuang\({}^{l}\), Chenhan Yuan\({}^{c}\), Kaihai Yang\({}^{c}\), Zheheng Luo\({}^{c}\), Tianlin Zhang\({}^{c}\), Zhiwei Liu\({}^{c}\), Guojun Xiong\({}^{j}\), Zhiyang Deng\({}^{i}\), Yuchen Jiang\({}^{i}\), Zhiyuan Yao\({}^{i}\), Haohang Li\({}^{i}\), Yangyang Yu\({}^{i,*}\), Gang Hu\({}^{h}\), Jiajia Huang\({}^{k}\), Xiao-Yang Liu\({}^{c,*}\), Alejandro Lopez-Lira\({}^{d,*}\), Benyou Wang\({}^{f}\), Yanzhao Lai\({}^{m}\), Hao Wang\({}^{g}\), Min Peng\({}^{b,*}\), Sophia Ananiadou\({}^{c,*}\), Jimin Huang\({}^{a,*}\)

The Fin AI, \({}^{b}\)Wuhan University, \({}^{c}\)The University of Manchester, \({}^{d}\)University of Florida,

\({}^{e}\)Columbia University, \({}^{f}\)The Chinese University of Hong Kong, Shenzhen,

\({}^{g}\)Sichuan University, \({}^{h}\)Yunnan University, \({}^{i}\)Stevens Institute of Technology

\({}^{j}\)Stony Brook University, \({}^{k}\)Nanjing Audit University,

\({}^{l}\)Jiangxi Normal University, \({}^{m}\)Southwest Jiaotong University

###### Abstract

LLMs have transformed NLP and shown promise in various fields, yet their potential in finance is underexplored due to a lack of comprehensive benchmarks, the rapid development of LLMs, and the complexity of financial tasks. In this paper, we introduce FinBen, the first extensive open-source evaluation benchmark, including \(42\) datasets spanning \(24\) financial tasks, covering eight critical aspects: information extraction (IE), textual analysis, question answering (QA), text generation, risk management, forecasting, decision-making, and bilingual (English and Spanish). FinBen offers several key innovations: a broader range of tasks and datasets, the first evaluation of stock trading, novel agent and Retrieval-Augmented Generation (RAG) evaluation, and two novel datasets for regulations and stock trading. Our evaluation of \(21\) representative LLMs, including GPT-4, ChatGPT, and the latest Gemini, reveals several key findings: While LLMs excel in IE and textual analysis, they struggle with advanced reasoning and complex tasks like text generation and forecasting. GPT-4 excels in IE and stock trading, while Gemini is better at text generation and forecasting. Instruction-tuned LLMs improve textual analysis but offer limited benefits for complex tasks such as QA. FinBen has been used to host the first financial LLMs shared task at the FinNLP-AgentScen workshop during IJCAI-2024, attracting \(12\) teams. Their novel solutions outperformed GPT-4, showcasing FinBen's potential to drive innovations in financial LLMs. All datasets and code are publicly available for the research community2, with results shared and updated regularly on the Open Financial LLM Leaderboard3.

Footnote 2: https://github.com/The-FinAI/PIXIU

3
Footnote 3: Now under the umbrella of FINOS at Linux Foundation, https://finosfoundation/Open-Financial-LLM-Leaderboard

[FIGURE

## 1 Introduction

Recently, Large Language Models (LLMs) (Brown et al., 2020) such as ChatGPT4 and GPT-4 (OpenAI, 2023), have reshaped the field of natural language processing (NLP) and exhibited remarkable capabilities in specialized domains across mathematics, coding, medicine, law, and finance (Bubeck et al., 2023). Within the financial domain, recent several studies (Xie et al., 2023; Lopez-Lira and Tang, 2023; Li et al., 2023; Xie et al., 2023; Liu et al., 2023; Yang et al., 2023; Xie et al., 2024) have shown the great potential of LLMs such as GPT-4 on financial text analysis and prediction tasks. While their potential is evident, a comprehensive understanding of their capabilities and limitations for finance remains largely unexplored. This is due to a lack of extensive evaluation studies and benchmarks, and the inherent complexities associated with the professional nature of financial tasks.

Footnote 4: https://openai.com/chatgpt

Existing financial domain evaluation benchmarks, including PIXIU (Xie et al., 2023b), FinanceBench (Islam et al., 2023) and BizBench (Koncel-Kedziorski et al., 2023), have **limited evaluation tasks** and primarily **focus on financial NLP tasks**, as shown in Table 1. Most existing benchmarks cover only a small number of evaluation tasks and are centered on NLP capabilities, such as information extraction (IE) and question answering (QA) (Huang et al., 2024; Liu et al., 2024; Hu et al., 2024; Yang et al., 2024; Zhao et al., 2024, 2024, 2024). While PIXIU stands out by covering the highest number of tasks, it includes only one evaluation task in most categories. This narrow focus limits their ability to comprehensively evaluate LLMs across the diverse and complex landscape of financial applications, such as forecasting, risk management, and decision-making. It is insufficient for a thorough evaluation of LLM capabilities, especially in the financial area.

To bridge this gap, we propose FinBen, a novel comprehensive open-source evaluation benchmark developed through the collaborative efforts of experts in both computer science and finance. As shown in Figure 1, FinBen comprises \(42\) datasets spanning \(24\) financial tasks, meticulously organized to assess LLMs across eight critical aspects: information extraction (IE), textual analysis (TA), question answering (QA), text generation (TG), risk management (RM), forecasting (FO), decision-making (DM), and bilingual (English and Spanish). Each category targets specific skills of financial data processing and analysis, ensuring a thorough evaluation of LLMs and showcasing their proficiency in managing complex financial scenarios.

FinBen introduces several innovations over existing benchmarks: 1) **New tasks**: FinBen introduces a significantly larger number of tasks and datasets, making it the most holistic benchmark for financial LLMs with the highest number of tasks and datasets. This extensive range provides a more robust evaluation of LLM capabilities in diverse financial contexts. 2) **Broader coverage**: Covering eight aspects of the financial sector, FinBen is the first benchmark to include the evaluation of stock trading, which is the fundamental task in the financial sector, involving complex decision-making processes that impact market dynamics and investment strategies. 3) **New evaluation strategy**: FinBen is the first benchmark to include agent-based evaluation and retrieval-augmented generation (RAG)

\begin{table}
\begin{tabular}{l l c c c c c c c c} \hline \hline
**Benchmark** & **Language** & **Dataset Task IE** & **TA** & **QA** & **TG** & **RM** & **FO** & **DM** & **SP** \\ \hline CFBenchmark & Chinese & 8 & 7 & 1 & 3 & 3 & **X** & **X** & **X** \\ Fin-Eva & Chinese & 1 & 1 & **X** & **X** & 1 & **X** & **X** & **X** \\ PIXIU & English & 15 & 8 & 1 & 3 & 1 & 1 & 1 & **X** \\ FinanceBench & English & 1 & 1 & **X** & 1 & **X** & **X** & **X** & **X** \\ BizBench & English & 8 & 5 & 2 & **X** & 2 & 1 & **X** & **X** & **X** \\ FinBen & English, Spanish & 42 & 24 & 6 & 8 & 3 & 1 & 4 & 1 & 4 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of different financial benchmarks based on the number of tasks and datasets and the task counts across aspects: information extraction (IE), textual analysis (TA), question answering (QA), text generation (TG), risk management (RM), forecasting (FO), decision-making (DM), and spanish (SP).

Figure 1: FinBen’s evaluation datasets with sizes ranging from \(100\) to \(4,000\).

based evaluation. These innovative strategies provide a more dynamic and realistic assessment of LLMs, reflecting their ability to interact with and retrieve relevant information from vast datasets. \(4\)) **Novel datasets**: FinBen proposes two novel open-source datasets of QA and stock trading tasks for the research community, pushing the boundaries of what LLMs can achieve and setting a new standard for dataset comprehensiveness. 5) **Empowering financial LLMs research**: Leveraging financial tasks in FinBen, we hosted the first shared task (see Appendix G for details) focused on financial LLMs at the FinNLP-AgentScen workshop during IJCAI-2024 5. This event attracted 12 teams, leveraging our benchmark to develop novel LLMs-based solutions within the financial domain. Remarkably, the proposed methods achieved superior performance compared to GPT-4, demonstrating the benchmark's potential to foster innovations and advance the state-of-the-art (SOTA) in financial LLMs.

Footnote 5: https://sites.google.com/nlg.csie.ntu.edu.tw/finnlp-agentscen

Based on FinBen, we assess \(21\) representative general LLMs such as GPT-4, ChatGPT, and the latest Gemini, and financial LLMs, and have the following findings: 1) **Superior Capabilities with Limitations**: While LLMs exhibit exceptional prowess in IE and textual analysis tasks, they underperform in areas necessitating advanced reasoning and complex IE, such as text generation and forecasting. 2) **Potential in Stock Trading**: SOTA LLMs have demonstrated considerable promise in stock trading applications. However, there remains significant room for improvement due to their limitations in reasoning and comprehensive forecasting abilities. 3) **Closed-Source Superiority**: Closed-source commercial LLMs continue to lead in performance within the financial domain. Specifically, GPT-4 excels in IE, text analysis, QA, and intricate stock trading tasks, while Gemini shows superior capabilities in text generation and forecasting. 4) **Open-Source Improvements and Limitations**: While open-source, instruction-tuned financial LLMs have shown notable enhancements in textual analysis and IE tasks, the advantages of instruction-tuning are less pronounced when it comes to complex tasks such as QA, text generation, and forecasting.

In summary, the main contributions of this paper are: 1) we present FinBen, the first comprehensive open-sourced evaluation benchmark for LLMs in the financial domain, 2) we utilize a novel taxonomy covering eight aspects for organizing financial evaluation tasks, 3) we develop two novel evaluation datasets for the research community, and 4) we conduct systematic evaluation of \(21\) LLMs using FinBen, showcasing their advantages and limitations and highlighting directions for future work.

## 2 FinBen

In this section, we delve into the specifics of FinBen, detailing the evaluation taxonomy, data sources, and evaluation tasks.

### The Taxonomy of Financial Evaluation Tasks

In the dynamic landscape of financial technology, evaluating the capabilities of LLMs necessitates a comprehensive and structured approach. We propose a novel taxonomy for financial evaluation tasks, categorizing and assessing LLMs across eight financial domains inspired by established taxonomies in financial tasks (Cao, 2022; Li et al., 2023b; Zhao et al., 2024b): **Information Extraction (IE)**, **Textual Analysis (TA)**, **Question Answering (QA)**, **Text Generation (TG)**, **Risk Management (RM)**, **Forecasting (FO)**, **Decision-Making (DM)**, and **Spanish (SP)**. **Information Extraction** focuses on identifying key entities and relationships within financial documents, transforming unstructured data into structured insights (Costantino and Coletti, 2008). **Textual Analysis** delves into content and sentiment analysis of financial texts, aiding in market trend understanding (Loughran and McDonald, 2020). **Question Answering** evaluates the model's ability to comprehend and respond to financial queries (Maia et al., 2018). **Text Generation** assesses the production of coherent financial text (La Quatra and Cagliero, 2020). **Risk Management** involves evaluating creditworthiness, detecting fraud, and ensuring regulatory compliance (Aziz and Dowling, 2019). **Forecasting** predicts future financial trends, enabling strategic responses to market dynamics (Abu-Mostafa and Atiya, 1996). **Decision-Making** assesses the model's proficiency in making informed financial decisions, such as developing trading strategies and optimizing investment portfolios (Paiva et al., 2019). Finally, **Spanish** evaluates the model's capabilities in other languages except for English, particularly in low-resource languages.

### Data Sources

FinBen's evaluation tasks are drawn from three primary data sources: 1) open-sourced datasets from existing studies originally released for non-LLM evaluation settings. Domain experts have designed diverse prompts and reformulated these datasets into instruction-response pairs, making them suitable for evaluating the zero-shot performance of LLMs. 2) datasets from existing evaluation benchmarks such as PIXIU. These datasets have already been transformed into the instruction tuning format, allowing for seamless integration and direct use in FinBen. 3) novel datasets introduced in this paper. These datasets are designed to address gaps in existing benchmarks and provide unique challenges for financial LLMs evaluation. Novel datasets include (As shown in Table 2):

**FinTrade**. The FinTrade dataset is developed specifically for stock trading tasks, integrating historical stock prices, filings data, and news data for 10 stocks over a one-year period. It provides a robust foundation for evaluating LLMs in agent-based financial trading scenarios. The dataset is composed of three main components6: (1) **Stock Price Data**: Historical price data for 497 trading days, obtained via the yfinance API from Yahoo Finance, includes OHLCV (open, high, low, close, adjusted close price, and volume) metrics. Adjusted close prices are used to maintain consistency in the return series, minimizing the impact of corporate actions like dividends and stock splits. (2) **Filings Data**: Summary sections from Form 10-Q (quarterly reports) and Form 10-K (annual reports) are retrieved from the EDGAR database of the U.S. Securities and Exchange Commission (SEC). Over one year, each stock is linked to three quarterly reports and one annual report, providing crucial quarterly insights. (3) **News Data**: Daily news data, compiled from multiple publicly accessible datasets, provides short-term market perspectives, enabling the agent to account for market sentiment. The table below summarizes the data statistics.

Footnote 6: Please see Appendix for more details

**Regulations**. The Regulations dataset focuses on long-form question answering related to Over-the-Counter (OTC) derivatives and financial regulations within the European Union. Derived from the European Securities and Markets Authority's (ESMA) comprehensive document on Regulation (EU) No 648/2012 (EMIR), it maps QA pairs to relevant articles from EMIR and other directives. EMIR, implemented to enhance transparency and reduce risks in derivatives trading, governs OTC derivatives, central counterparties, and trade repositories. The dataset includes 254 QA pairs, meticulously curated with domain experts to ensure relevance and accuracy, addressing key regulatory issues such as reporting requirements, clearing thresholds, and obligations for financial and non-financial counterparties. The QAs are updated to reflect ongoing regulatory changes, providing a dynamic resource for testing LLMs' understanding of complex regulatory frameworks. This dataset serves as a critical tool for both regulatory compliance and academic research.

### Tasks

Table 2 and Figure 1 shows all tasks, datasets, data statistics, and evaluation metrics covered by FinBen7.

Footnote 7: For detailed instructions of each dataset, please see Appendix D

Information extraction:It spans seven datasets across six information extraction tasks. _1) Named entity recognition_ extracts entities like LOCATION, ORGANIZATION, and PERSON from financial agreements and SEC filings, using the NER (Alvarado et al., 2015) and FINER-ORD (Shah et al., 2023b) datasets. _2) Relation extraction_ identifies relationships such as "product/material produced" and "manufacturer" in financial news and earnings transcripts with the FINRED dataset (Sharma et al., 2022). _3) Causal classification_ discerns whether sentences from financial news and SEC filings convey causality using the SC dataset (Mariko et al., 2020). _4) Causal detection_ identifies cause and effect spans in financial texts with the CD dataset (Mariko et al., 2020). _5) Numeric labeling_ tags numeric spans in financial documents using the FNXL dataset (Sharma et al., 2023), focusing on automating the assignment of labels from a large taxonomy to numeral spans in sentences. _6) Textual analogy parsing_ involves identifying common attributes and comparative elements in textual analogies by extracting analogy frames, utilizing the FSRL dataset (Lamm et al., 2018), which maps analogous facts to semantic role representations and identifies the analogical relations between them. The evaluation of these tasks is focused on the F1 score (Goutte and Gaussier, 2005), Entity F1 score (Derczynski, 2016), and the Exact Match Accuracy (EM Accuracy) metric (Kim et al., 2023).

Textual analysis:This encompasses eight classification tasks for evaluating LLMs. _1) Sentiment analysis_ focuses on extracting sentiment information (positive, negative, or neutral) from financial texts, using three datasets: the Financial Phrase Bank (FPB) (Malo et al., 2014), FiQA-SA (Maia et al., 2018), and TSA (Cortis et al., 2017). _2) News headline classification_ analyzes additional information, like price movements in financial texts, using the Headlines dataset (Sinha and Khandait, 2021). _3) Hawkish-Dovish classification_ aims to classify sentences from monetary policy texts as 'hawkish' or 'dovish' focusing on the nuanced language and economic implications of financial texts, using the FOMC (Shah et al., 2023a) dataset. _4) Argument unit classification_ categorizes sentences as claims or premises using the FinArg AUC dataset (Sy et al., 2023). _5) Argument relation detection_ identifies relationships (attack, support, or irrelevant) between social media posts using the FinArg ARC dataset (Sy et al., 2023). _6) Multi-class classification_ targets categorizing a variety of financial texts, including analyst reports, news articles, and investor comments, utilizing the MultiFin dataset (Jorgensen et al., 2023). _7) Denolpleteness classification_ predicts if mergers and acquisitions events are "completed" or remain "rumors" based on news and tweets, employing the MA dataset (Yang et al., 2020a). _8) ESG issue identification_ focuses on detecting Environmental, Social, and Governance (ESG) concerns in financial documents using the MLESG dataset (Chen et al., 2023a). For all datasets, evaluation utilizes the accuracy and F1 Score.

Question answering.It includes 4 datasets from three QA tasks, challenging LLMs to respond to financial queries. _1) Numerical QA_ focuses on solving questions through multi-step numerical reasoning with financial reports and tables, utilizing the FinQA (Chen et al., 2021) and TATQA (Zhu et al., 2021) dataset. _2) Multi-turn QA_ is an extension of QA with multi-turn questions and answers based on financial earnings reports and tables, using the ConvFinQA dataset (Chen et al., 2022b). F1

\begin{table}
\begin{tabular}{l l l l l} \hline \hline \multicolumn{1}{c}{**Data**} & \multicolumn{1}{c}{**Task**} & \multicolumn{1}{c}{**Test**} & \multicolumn{1}{c}{**Evaluation**} & \multicolumn{1}{c}{**License**} \\ \hline NER (Alvarado et al., 2015) & named entity recognition & 980 & Entity F1 & CC BY-SA 3.0 \\ FFNER-ORD (Shah et al., 2023b) & named entity recognition & 1,890 & Entity F1 & CC BY-NC 4.0 \\ FinRD (Sharma et al., 2022) & relation extraction & 1,068 & F1, Entity F1 & Public \\ SC (Marke et al., 2020) & causal classification & 8,630 & F1, Entity F1 & CC BY 4.0 \\ CD (Marke et al., 2020) & causal detection & 226 & F1, Entity F1 & CC BY 4.0 \\ FNAL, Sharma et al., 2023) & numeric labeling & 318 & F1, EM Accuracy & Public \\ FSRL (Lamn et al., 2018) & textual analogy parsing & 97 & F1, EM Accuracy & MIT License \\ \hline FPB (Malo et al., 2014) & sentiment analysis & 970 & F1, Accuracy & CC BY-SA 3.0 \\ FiQA-SA (Mais et al., 2018) & sentiment analysis & 235 & F1 & Public \\ TSA (Cortis et al., 2017) & sentiment analysis & 561 & F1, Accuracy & CC BY-NC-SA 4.0 \\ Headlines (Sohn and Khandait, 2021) & news headline classification & 2,238 & Avg F1 & CC BY 9X.0 \\ FOMC (Shah et al., 2023a) & hash-both classification & 496 & F1, Accuracy & CC BY-NC-SA 4.0 \\ FinArg-ACC (Sy et al., 2023) & argument unit classification & 969 & F1, Accuracy & CC BY-NC-SA 4.0 \\ FinArg-ACC (Sy et al., 2023) & argument relations classification & 496 & F1, Accuracy & CC BY-NC-SA 4.0 \\ MultiFin (Jorgensen et al., 2023) & multi-class classification & 690 & F1, Accuracy & Public \\ MA (Yang et al., 2020a) & dual components classification & 500 & F1, Accuracy & Public \\ MLBSG (Chen et al., 2023a) & ESG Issue identification & 300 & F1, Accuracy & CC BY-NC-ND \\ \hline FinQA (Chen et al., 2021) & question answering & 1,147 & EM Accuracy & MIT License \\ TATQA (Zhu et al., 2021) & question answering & 1,668 & F1, EM Accuracy & MIT License \\ “Regulations & long-term question answering & 254 & RUOLBERTScore & Public \\ ComFinQA (Chen et al., 2022b) & multi-turn question answering & 1,490 & EM Accuracy & MIT License \\ \hline ECTSM (Mahieber et al., 2022) & text summarization & 495 & RUOLBERTscore, BATRScore & Public \\ EDTSum (Xie et al., 2023b) & text summarization & 2,000 & RUOLGE, BETRScore, BATRScore & Public \\ \hline BigData22 (Soun et al., 2022) & stock movement prediction & 1,470 & Accuracy, MCC & Public \\ ACL18 (Van and Cohen, 2018) & stock movement prediction & 3,720 & Accuracy, MCC & MIT License \\ CLNIKIscore (Derczynski, 2016) and the Exact Match Accuracy (EM Accuracy) metric (Kim et al., 2023) are used to evaluate these tasks. _3) Long-form QA_ involves presenting models with complex, detailed questions that require extensive and nuanced answers, often incorporating legal interpretations and practical applications. In our evaluation, we utilize our newly proposed Regulations dataset, which focuses on intricate questions and answers related to financial regulations like EMIR. We assess the model responses using ROUGE (Lin, 2004) and BERTScore (Zhang et al., 2019).

**Text generation.** This task assesses the models' ability to produce coherent and informative text. Our focus is on _text summarization_, utilizing the ECTSUM (Mukherjee et al., 2022) dataset for summarizing earnings call transcripts. We also include EDTSUM, specifically designed for condensing financial news articles into concise summaries, constructed from original data in (Zhou et al., 2021). Evaluation employs ROUGE (Lin, 2004), BERTScore (Zhang et al., 2019), and BART Score (Yuan et al., 2021) to measure alignment, factual consistency, and information retention between machine-generated and expert summaries.

**Forecasting.** The forecasting task challenges models to predict future market and investor behaviors from emerging patterns. We focus on the _stock movement prediction_ task, forecasting stock directions as either positive or negative, based on historical prices and tweets. Three datasets are included: BigData22 (Soun et al., 2022), ACL18 (Xu and Cohen, 2018) and CIKM18 (Wu et al., 2018).

**Risk management**. It challenges LLMs to accurately identify, extract, and analyze relevant risk-related information, interpret numerical data, and understand complex relationships. We include 4 tasks: _1) Credit scoring_ classifies individuals as "good" or "bad" credit risks using historical customer data, employing datasets including: German (Hofmann, 1994), Australia (Quinlan, [n. d.]) and LendingClub (Feng et al., 2023). _2) Fraud detection_ involve categorizes transactions as "fraudulent" or "non-fraudulent", using two datasets: ccf (Feng et al., 2023) and ccFraud (Feng et al., 2023). _3) Financial distress identification_ aims to predict a company's bankruptcy risk, using the polish (Feng et al., 2023) and taiwan dataset (Feng et al., 2023). Note that the dataset name describes only the region of the company, and the content within the datasets is in English. _4) Claim analysis_ anonymizes client data for privacy, labeling a "target" to indicate claim status, using two datasets: PortoSeguro (Feng et al., 2023) and travelinsurance (Feng et al., 2023). It is noticed that the dataset name such as German and taiwan, only indicates customer sources and all content is in English. F1 score and Matthews correlation coefficient (MCC) (Chicco and Jurman, 2020) are used for evaluating these tasks.

**Decision-making.** Strategic decision-making (Punt, 2017) evaluates the model's proficiency in synthesizing diverse information to formulate and implement trading strategies, a challenge even for experts. We innovatively introduce the SOTA financial LLM agent FinMem (Yu et al., 2023, 2024) to evaluate LLMs on the _stock trading_ task. We construct the novel FinTrade dataset, containing 10 stocks, simulating real-world trading through historical prices, news, and sentiment analysis. Performance is measured by Cumulative Return (CR) (Ariel, 1987), Sharpe Ratio (SR) (Sharpe, 1998), Daily (DV) and Annualized volatility (AV) (Zhou et al., 2023), and Maximum Drawdown (MD) (Magdon-Ismail and Atiya, 2004), offering a comprehensive assessment of profitability, risk management, and decision-making prowess.

**Spanish.** Spanish financial datasets (Zhang et al., 2024) evaluate model performance in low-resource language settings. We include six datasets in our analysis: TSA-ES (Zhang et al., 2024) and FinanceES (Zhang et al., 2024), both designed for sentiment analysis in the Spanish financial domain, where model performance is measured using F1 score. For multi-class classification, we utilize the Spanish subset of the MultiFin dataset (Jorgensen et al., 2023), with F1 score as the primary metric. The EFP (Zhang et al., 2024) and EFPA (Zhang et al., 2024) datasets, focused on Spanish financial question-answering, are evaluated using F1 score to assess the accuracy of predicted answers. Finally, for summarization tasks, the FNS-2023 (Zhang et al., 2024) dataset, which consists of Spanish company reports, is evaluated using ROUGE scores to measure the quality of generated summaries.

## 3 Evaluation

We evaluate the zero-shot (from our evaluation) and few-shots (results from previous papers) performance of \(21\) representative general LLMs and financial LLMs on the FinBen benchmark, including: 1) ChatGPT: A LLM developed by OpenAI. 2) GPT-4 (OpenAI, 2023): The SOTA commercialized LLMs proposed by OpenAI. 3) Gemini Pro (Team et al., 2023): A multimodal LLM with 50Tparameters, released by Google. 4) LLaMA2-7/70B-chat (Touvron et al., 2023b): An open-sourced instruction-following LLM with 7B and 70B parameters developed by MetaAI. 5) LLaMA3-8B8: An open-sourced LLMs developed by MetaAI, using more training data than LLaMA2. 6) ChatGLM3-6B (Du et al., 2022): A conversational LLM with 6B parameters, jointly released by Zhipu AI and Tsinghua KEG. 7) Baiduhan2-6B (Baiduhan, 2023): An open-source LLM with 6B parameters, launched by Baiduhan Intelligent Technology. 8) InternLM-7B (Team, 2023): An open-sourced 7B parameter base model tailored for practical scenarios, proposed by SenseTime. 9) Falcon-7B (Almazrouei et al., 2023): A 7B parameter causal decoder-only LLM model trained on 1500B tokens of RefinedWeb enhanced with curated corpora. 10) Mixtral 8\(\times\)7B (Jiang et al., 2024): A LLM with the Sparse Mixture of Experts (SMoE) architecture. 11) Code LLaMA-7B (Roziere et al., 2023): An open-source LLM model for generating programming code, launched by Meta AI with 7B parameters. 12) FinGPT (Yang et al., 2023a): A 7B instruction finetuned financial LLM based on LLaMA 7B (Touvron et al., 2023a) with sentiment analysis tasks. 13) FinMA-7B (Xie et al., 2023b): A 7B instruction finetuned financial LLM based on LLaMA 7B with multiple NLP and forecasting tasks. 14) DISC-FinLLM (Chen et al., 2023b): An open-sourced financial LLM, fine-tuned from Baiduhan-13B-Chat (Baiduhan, 2023). 15) CFGPT (Li et al., 2023a): An open-source LLM, specifically designed for the financial sector and trained on Chinese financial datasets, which comprises 7B parameters. 16) Qwen2-7B/72B (qwe, 2024): Instruction-tuned LLMs developed by Alibaba Cloud with 7B and 72B parameters, optimized for financial and general NLP tasks. 17) Xuanyuan-6B/70B (Zhang et al., 2023c): Instruction-tuned LLMs designed for financial NLP tasks with 6B and 70B parameters. 18) LLaMA3.1-8B/70B (Dubey et al., 2024): LLaMA3 series models with 8B and 70B parameters, fine-tuned with enhanced data for a wide range of NLP tasks.

**Experimental Settings** We set the maximum generation tokens for LLMs to 1024 and the batch size to 20,000 for all experiments. These experiments are exclusively conducted on 16 NVIDIA A100 80G GPUs, taking approximately 600 hours to complete. Including the GPT-4 API costs, the total expenditure amounts to approximately $51,000.

## 4 Results

Table 3 and Table 4 shows the performance of 14 representative LLMs on all datasets in the FinBen. We also report results of non-LLM methods (traditional methods) in Appendix H.

### Information Extraction and Textual Analysis Results

As shown in Table 3, for IE tasks, GPT-4 demonstrates superior performance in named entity recognition tasks, including NER, FINER-ORD, and FinRED. InternLM 7B achieves the best results in causal classification (SC). However, for more complex information extraction tasks, such as causal detection (CD) and numerical understanding (FNXL and FSRL), even GPT-4's performance is limited, with Gemini showing only slightly better results, still falling short of expectations. Additionally, while financial domain-specific LLMs developed by instruction tuning such as FinMA 7B exhibit improvements over general domain LLMs such as LLaMA2 7B-chat, they continue to struggle with both named entity recognition and complex extraction tasks. These findings highlight significant opportunities for advancement in financial causal detection and numerical understanding for LLMs.

Regarding TA tasks, instruction fine-tuned models like FinMA 7B exhibit the best performance in sentiment analysis tasks, including FPB, FiQA-SA, and Headlines. However, the generalization ability of FinMA 7B is limited due to the diversity of TA tasks in the financial domain. It performs even worse than general domain LLMs such as LLaMA2-7B-chat on other TA tasks, where GPT-4, Gemini, and LLaMA2 70B show superior results. This underscores the limitations of instruction fine-tuned models, which may be constrained by the parameter size and ability of their base models.

Models tailored for the Chinese language, such as CFGPT sft-7B-Full, which is fine-tuned on Chinese financial data, exhibit limited improvement on some datasets and even a decline in performance on others like MultiFin compared to its base model InternLM 7B. This trend suggests a language-based discrepancy, indicating that fine-tuning with Chinese data may adversely affect performance on English tasks. These findings underscore the complexities of cross-lingual adaptation in model training, highlighting the challenges in achieving consistent performance across different languages.

[MISSING_PAGE_FAIL:8]

sentences. Among open-source LLMs, LLaMA2 70B stands out in text summarization. Conversely, CFGPT sft-7B-Full consistently shows a decrease in performance compared to its foundational model, InternLM 7B.

### Forecasting and Risk Management Results

For forecasting, it is crucial to acknowledge that all LLMs fail to meet expected outcomes and lag behind traditional methodologies. This consistent observation with existing studies Xie et al. (2023b) underlines a notable deficiency in LLMs' capacity to tackle forecasting as effectively as traditional methods. Even the best-performing models, such as GPT-4 and Gemini, only perform slightly better than random guessing. This reveals significant potential for enhancement in LLMs, including industry leaders like GPT-4 and Gemini, particularly in forecasting tasks that demand complex reasoning abilities.

In RM tasks, such as credit scoring, fraud detection, and identifying financial distress, data often exhibit significant imbalances. Instances representing individuals with low credit scores, those prone to fraud, and companies at risk of financial distress constitute only a small percentage of the overall dataset. In such scenarios, LLMs with low instruction-following abilities (such as LLaMA2-7B-chat and LLaMA2-70B) tend to classify all cases into a single class, resulting in an MCC score of 0. These tasks, with tabular inputs and highly imbalanced distribution, pose a significant challenge for LLMs in the financial domain.

### Decision Making Results

The comparative analysis of various LLMs on the complex task of stock trading, is presented in Table 49. This task requires models to understand, summarize, and reason with multimodal financial data (texts and time series), leading to sophisticated trading decisions that necessitate a range of skills, from fundamental comprehension and summarization to reasoning and decision-making.

Footnote 9: For detailed trading performance, please see Appendix F

Among the evaluated LLMs, GPT-4 distinguishes itself by achieving the highest Sharpe Ratio (SR) over 1, indicating superior investment performance through optimal risk-return balance. It also records the minimal Maximum Drawdown (MDD), suggesting effective limitation of potential losses, thereby offering a more secure investment avenue compared to other models, including those using reinforcement learning methods like DQN, PPO, and A2C, which show significantly lower SR and higher MDD.

Tables 4 and 10 reinforce these findings, highlighting GPT-4's exceptional performance in this challenging domain. Additional results and analyses from these models in Table 5 contrast their performances with the traditional _Buy & Hold_ strategy, which considerably lags behind.

In contrast, ChatGPT exhibits significantly lower performance metrics, indicating limitations in its financial decision-making capabilities. Gemini, on the other hand, secures the position of second-best performer, showcasing lower risk and volatility compared to GPT-4, yet maintaining commendable returns. When considering open-source models, LLaMA-70B, despite its lower volatility, yields the least profit among the LLMs, highlighting a trade-off between risk management and profitability.

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline
**Model** & **CR (\%)\(\uparrow\)** & **SR\(\uparrow\)** & **DV (\%)\(\downarrow\)** & **AV (\%)\(\downarrow\)** & **MD (\%)\(\downarrow\)** \\ \hline Buy \& Hold & -4.00 \(\pm\) 22.39 & 0.02 \(\pm\) 0.87 & 3.59 \(\pm\) 1.34 & 56.43 \(\pm\) 21.00 & 30.67 \(\pm\) 17.48 \\ GPT-4 & **28.19 \(\pm\) 25.27** & **1.51 \(\pm\) 1.08** & 2.52 \(\pm\) 1.30 & 39.88 \(\pm\) 20.66 & **18.34 \(\pm\) 9.77** \\ GPT-4o & -5.54 \(\pm\) 19.12 & -0.19 \(\pm\) 0.84 & 2.73 \(\pm\) 1.30 & 43.62 \(\pm\) 20.67 & 29.96 \(\pm\) 18.89 \\ GPT3-Turbo & 4.48 \(\pm\) 22.23 & 0.15 \(\pm\) 0.82 & 2.84 \(\pm\) 1.47 & 45.39 \(\pm\) 23.35 & 28.83 \(\pm\) 15.40 \\ Ilam2-70B & 4.02 \(\pm\) 24.65 & 0.52 \(\pm\) 1.48 & 2.18 \(\pm\) 1.28 & 34.86 \(\pm\) 20.38 & 25.55 \(\pm\) 16.83 \\ Ilam3-70B & -2.57 \(\pm\) 22.63 & -0.04 \(\pm\) 1.19 & 2.71 \(\pm\) 1.54 & 43.42 \(\pm\) 24.65 & 29.31 \(\pm\) 15.57 \\ gemini & 14.95 \(\pm\) 28.04 & 1.03 \(\pm\) 1.24 & **2.17 \(\pm\) 1.39** & **34.67 \(\pm\) 22.23** & 20.13 \(\pm\) 11.36 \\ \hline \hline \end{tabular}
\end{table}
Table 4: The average trading performance (95% Confidence Interval) comparison for different LLMs across 10 stocks. The results include large LLMs only (\(\geq 70B\)), as models with smaller contexts have difficulty understanding the instructions and producing a static strategy of holding.

For smaller models with parameters less than 70 billion, a marked inability to adhere to trading instructions consistently across transactions is noted. This is attributed to their limited comprehension, extraction capabilities, and constrained context windows. This limitation underscores the critical challenges smaller LLMs face in tasks requiring intricate financial reasoning and decision-making, thereby spotlighting the necessity for more advanced models to tackle decision making tasks effectively.

### Spanish Results

Table 3 presents the performance of various models on six Spanish financial datasets, highlighting significant language disparities. ChatGPT, GPT-4 and Gemini show limited performance compared with English datasets. Miktral 7B performs competitively, showing that the multilingual ability can improve language-specific tasks. Smaller models, particularly from the LLaMA family, struggle with domain complexities, reinforcing the importance of robust multilingual pretraining. While top models excel in sentiment analysis, all models underperform in summarization tasks on FNS, stressing the need for enhanced adaptation to specialized Spanish financial language.

## 5 Conclusion

In this work, we present FinBen, a comprehensive benchmark specifically designed to evaluate LLMs in the financial domain. FinBen includes \(42\) diverse datasets spanning 24 tasks, meticulously organized to assess LLMs across eight critical aspects: information extraction, textual analysis, question answering, text generation, risk management, forecasting, decision-making, and Spanish. This breadth of coverage sets FinBen apart from existing financial benchmarks, enabling a more robust and nuanced evaluation of LLM capabilities. Our evaluation of \(21\) LLMs, including GPT-4, ChatGPT, and Gemini, reveals their key advantages and limitations, highlighting directions for future work. Looking ahead, FinBen continuously evolves into an open FinLLM leaderboard (Lin et al., 2024). We will incorporate additional languages and multimodal financial tasks (Yanglet and Deng, 2024) and expand the range of financial tasks to further enhance its applicability and impact.

**Openness**: Our FinBen project follows the model openness framework (White et al., 2024) by providing a comprehensive set of financial datasets and evalution codes under OSI-approved licenses.

**Limitations**: We acknowledge several limitations that could impact FinBen's effectiveness and applicability. The restricted size of available datasets may affect the models' financial understanding and generalization across various contexts. Computational constraints limited our evaluation to the LLaMA 70B model, potentially overlooking the capabilities of larger models. Additionally, the tasks are based on American market data and English texts, which may limit the benchmark's applicability to global financial markets. Responsible usage and safeguards are essential to prevent potential misuse, such as financial misinformation or unethical market influence10.

Footnote 10: For a detailed limitation concerning this work, please see Appendix.

**Ethical Statement**: The authors take full responsibility for any potential legal issues arising from FinBen's development and dissemination. All data used are publicly available, non-personal, and shared under the MIT license, adhering to privacy and ethical guidelines. This manuscript and associated materials are for academic and educational use only and do not provide financial, legal, or investment advice. The authors disclaim any liability for losses or damages from using the material, and users agree to seek professional consultation and indemnify the authors against any claims arising from its use11.

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline Model & Cumulative Return & Sharpe Ratio & Standard Deviation & Annualized Volatility & Max Drawdown \\ \hline A2C & -4.2232 & -0.2586 & 2.7522 & 43.6898 & 30.5819 \\ PPO & -0.5586 & 0.0085 & 2.7531 & 43.7048 & 28.9496 \\ DQN & -2.9924 & -0.1656 & 2.7486 & 43.6319 & 31.78 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Traditional model performances on stock trading.

## Acknowledgements

The authors acknowledge UFIT Research Computing, NVAITC, and HPG for providing computational resources and support that have contributed to the research results reported in this publication. URL: http://www.rc.ufl.edu. This work is supported by the project JPNP20006 from New Energy and Industrial Technology Development Organization (NEDO). This work has also been partially supported by project MIS 5154714 of the National Recovery and Resilience Plan Greece 2.0 funded by the European Union under the Next Generation EU Program. Additionally, we gratefully acknowledge FINOS (Finttech Open Source Foundation) for supporting the Open Financial LLM Leaderboard initiative. Xiao-Yang Liu acknowledges the support from NSF IUCRC CRAFT center research grant (CRAFT Grant 22017) for this research. The opinions expressed in this publication do not necessarily represent the views of NSF IUCRC CRAFT. Haoqiang Kang and Xiao-Yang Liu also acknowledge the support from Columbia's SIRS and STAR Program, The Tang Family Fund for Research Innovations in FinTech, Engineering, and Business Operations.

## References

* (2024) Qwen2 Technical Report. (2024).
* Abu-Mostafa and Atiya (1996) Yaser S Abu-Mostafa and Amir F Atiya. 1996. Introduction to financial forecasting. _Applied intelligence_ 6 (1996), 205-213.
* Almazrouei et al. (2023) Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. 2023. The falcon series of open language models. _arXiv preprint arXiv:2311.16867_ (2023).
* Alvarado et al. (2015) Julio Cesar Salinas Alvarado, Karin Verspoor, and Timothy Baldwin. 2015. Domain adaption of named entity recognition to support credit risk assessment. In _Proceedings of the Australasian Language Technology Association Workshop 2015_. 84-90.
* Araci (2019) Dogu Araci. 2019. FinBERT: Financial Sentiment Analysis with Pre-trained Language Models. arXiv:1908.10063 [cs.CL]
* Ariel (1987) Robert Ariel. 1987. A monthly effect in stock returns. _Journal of financial economics_ 18, 1 (1987), 161-174.
* Aziz and Dowling (2019) Saqib Aziz and Michael Dowling. 2019. _Machine learning and AI for risk management_. Springer International Publishing.
* Baichuan (2023) Baichuan. 2023. Baichuan 2: Open Large-scale Language Models. _arXiv preprint arXiv:2309.10305_ (2023). https://arxiv.org/abs/2309.10305
* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. _Advances in Neural Information Processing Systems_ 33 (2020), 1877-1901.
* Bubeck et al. (2023) Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. _arXiv preprint arXiv:2303.12712_ (2023).
* Cao (2022) Longbing Cao. 2022. Ai in finance: challenges, techniques, and opportunities. _ACM Computing Surveys (CSUR)_ 55, 3 (2022), 1-38.
* Chen et al. (2023a) Chung-Chi Chen, Yu-Min Tseng, Juyeon Kang, Anais Lhuissier, Min-Yuh Day, Teng-Tsai Tu, and Hsin-Hsi Chen. 2023a. Multi-Lingual ESG Issue Identification. In _Proceedings of the Fifth Workshop on Financial Technology and Natural Language Processing and the Second Multimodal AI For Financial Forecasting_. 111-115.
* Chen et al. (2023b) Wei Chen, Qiushi Wang, Zefei Long, Xianyin Zhang, Zhongtian Lu, Bingxuan Li, Siyuan Wang, Jiarong Xu, Xiang Bai, Xuanjing Huang, et al. 2023b. Disc-finllm: A chinese financial large language model based on multiple experts fine-tuning. _arXiv preprint arXiv:2310.15205_ (2023).

Zhiyu Chen, Wenhu Chen, Charese Smiley, and et al. Sameena Shah. 2022a. FinQA: A Dataset of Numerical Reasoning over Financial Data. arXiv:2109.00122 [cs.CL]
* Chen et al. (2021) Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan R Routledge, et al. 2021. FinQA: A Dataset of Numerical Reasoning over Financial Data. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_. 3697-3711.
* Chen et al. (2022b) Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and William Yang Wang. 2022b. ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering. arXiv:2210.03849 [cs.CL]
* Chicco and Jurman (2020) Davide Chicco and Giuseppe Jurman. 2020. The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation. _BMC genomics_ 21, 1 (2020), 1-13.
* Cortis et al. (2017) Keith Cortis, Andre Freitas, Tobias Daudert, Manuela Huerlimann, Manel Zarrouk, Siegfried Handschuh, and Brian Davis. 2017. Semeval-2017 task 5: Fine-grained sentiment analysis on financial microblogs and news. In _Proceedings of the 11th international workshop on semantic evaluation (SemEval-2017)_. 519-535.
* Costantino and Coletti (2008) Marco Costantino and Paolo Coletti. 2008. _Information extraction in finance_. Vol. 8. Wit Press.
* Dai et al. (2024) Yongfu Dai, Duanyu Feng, Jimin Huang, Haochen Jia, Qianqian Xie, Yifang Zhang, Weiguang Han, Wei Tian, and Hao Wang. 2024. LAiW: A Chinese Legal Large Language Models Benchmark. arXiv:2310.05620 [cs.CL]
* Derczynski (2016) Leon Derczynski. 2016. Complementarity, F-score, and NLP Evaluation. In _Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16)_, Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Sara Goggi, Marko Grobelnik, Bente Maegaard, Joseph Mariani, Helene Mazo, Asuncion Moreno, Jan Odijk, and Stelios Piperidis (Eds.). European Language Resources Association (ELRA), Portoroz, Slovenia, 261-266. https://aclanthology.org/L16-1040
* Du et al. (2022) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM: General Language Model Pretraining with Autoregressive Blank Infilling. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_. 320-335.
* Dubey et al. (2024) Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. _arXiv preprint arXiv:2407.21783_ (2024).
* Feng et al. (2024) Duanyu Feng, Yongfu Dai, Jimin Huang, Yifang Zhang, Qianqian Xie, Weiguang Han, Zhengyu Chen, Alejandro Lopez-Lira, and Hao Wang. 2024. Empowering Many, Biasing a Few: Generalist Credit Scoring through Large Language Models. arXiv:2310.00566 [cs.LG]
* Feng et al. (2023) Duanyu Feng, Yongfu Dai, Jimin Huang, Yifang Zhang, Qianqian Xie, Weiguang Han, Alejandro Lopez-Lira, and Hao Wang. 2023. Empowering many, biasing a few: Generalist credit scoring through large language models. _arXiv preprint arXiv:2310.00566_ (2023).
* Goutte and Gaussier (2005) Cyril Goutte and Eric Gaussier. 2005. A probabilistic interpretation of precision, recall and F-score, with implication for evaluation. In _European conference on information retrieval_. Springer, 345-359.
* Han et al. (2023a) Weiguang Han, Jimin Huang, Qianqian Xie, Boyi Zhang, Yanzhao Lai, and Min Peng. 2023a. Mastering Pair Trading with Risk-Aware Recurrent Reinforcement Learning. arXiv:2304.00364 [q-fin.CP]
* Han et al. (2023b) Weiguang Han, Boyi Zhang, Qianqian Xie, Min Peng, Yanzhao Lai, and Jimin Huang. 2023b. Select and Trade: Towards Unified Pair Trading with Hierarchical Reinforcement Learning. _arXiv preprint arXiv:2301.10724_ (2023).
* Han et al. (2023c)Hans Hofmann. 1994. Statlog (German Credit Data). UCI Machine Learning Repository. DOI: https://doi.org/10.24432/C5NC77.
* Hongyuan et al. (2023) Dong Hongyuan, Che Wanxiang, He Xiaoyu, Zheng Guidong, and Wen Junjie. 2023. FinBART: A Pre-trained Seq2seq Language Model for Chinese Financial Tasks. In _Proceedings of the 22nd Chinese National Conference on Computational Linguistics_, Maosong Sun, Bing Qin, Xipeng Qiu, Jing Jiang, and Xianpei Han (Eds.). Chinese Information Processing Society of China, Harbin, China, 906-917. https://aclanthology.org/2023.ccl-1.77
* Hu et al. (2024) Gang Hu, Ke Qin, Chenhan Yuan, Min Peng, Alejandro Lopez-Lira, Benyou Wang, Sophia Ananiadou, Wanlong Yu, Jimin Huang, and Qianqian Xie. 2024. No Language is an Island: Unifying Chinese and English in Financial Large Language Models, Instruction Data, and Benchmarks. _arXiv preprint arXiv:2403.06249_ (2024).
* Huang et al. (2024) Jiajia Huang, Haoran Zhu, Chao Xu, Tianming Zhan, Qianqian Xie, and Jimin Huang. 2024. AuditWen: An Open-Source Large Language Model for Audit. _arXiv preprint arXiv:2410.10873_ (2024).
* Islam et al. (2023) Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and Bertie Vidgen. 2023. FinanceBench: A New Benchmark for Financial Question Answering. _arXiv preprint arXiv:2311.11944_ (2023).
* Jiang et al. (2024) Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts. _arXiv preprint arXiv:2401.04088_ (2024).
* Jorgensen et al. (2023) Rasmus Jorgensen, Oliver Brandt, Mareike Hartmann, Xiang Dai, Christian Igel, and Desmond Elliott. 2023. MultiFin: A Dataset for Multilingual Financial NLP. In _Findings of the Association for Computational Linguistics: EACL 2023_. 864-879.
* Kim et al. (2023) Kisub Kim, Xin Zhou, Dongsun Kim, Julia Lawall, Kui Liu, Tegawende F Bissyande, Jacques Klein, Jaekwon Lee, and David Lo. 2023. How are We Detecting Inconsistent Method Names? An Empirical Study from Code Review Perspective. _arXiv preprint arXiv:2308.12701_ (2023).
* Koncel-Kedziorski et al. (2023) Rik Koncel-Kedziorski, Michael Krumdick, Viet Lai, Varshini Reddy, Charles Lovering, and Chris Tanner. 2023. Bizbench: A quantitative reasoning benchmark for business and finance. _arXiv preprint arXiv:2311.06602_ (2023).
* Quatra and Cagliero (2020) Moreno La Quatra and Luca Cagliero. 2020. End-to-end training for financial report summarization. In _Proceedings of the 1st Joint Workshop on Financial Narrative Processing and MultiLine Financial Summarisation_. 118-123.
* Lamm et al. (2018) Matthew Lamm, Arun Tejasvi Chaganty, Christopher D Manning, Dan Jurafsky, and Percy Liang. 2018. Textual analogy parsing: What's shared and what's compared among analogous facts. _arXiv preprint arXiv:1809.02700_ (2018).
* Lee et al. (2024) Jean Lee, Nicholas Stevens, Soyeon Caren Han, and Minseok Song. 2024. A Survey of Large Language Models in Finance (FinLLMs). arXiv:2402.02315 [cs.CL]
* Lei et al. (2023) Yang Lei, Jiangtong Li, Ming Jiang, Junjie Hu, Dawei Cheng, Zhijun Ding, and Changjun Jiang. 2023. CFBenchmark: Chinese Financial Assistant Benchmark for Large Language Model. arXiv:2311.05812 [cs.CL]
* Li et al. (2023) Jiangtong Li, Yuxuan Bian, Guoxuan Wang, Yang Lei, Dawei Cheng, Zhijun Ding, and Changjun Jiang. 2023a. CFGPT: Chinese Financial Assistant with Large Language Model. arXiv:2309.10654 [cs.CL]
* Li et al. (2023) Xianzhi Li, Xiaodan Zhu, Zhiqiang Ma, Xiaomo Liu, and Sameena Shah. 2023c. Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? An Examination on Several Typical Tasks. _arXiv preprint arXiv:2305.05862_ (2023).
* Li et al. (2023b) Yinheng Li, Shaofei Wang, Han Ding, and Hang Chen. 2023b. Large Language Models in Finance: A Survey. arXiv:2311.10723 [q-fin.GN]* Lin (2004) Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In _Text summarization branches out_. 74-81.
* Lin et al. (2024) Shengyuan Colin Lin, Keyi Wang Felix Tian, Xingjian Zhao, Jimin Huang, Qianqian Xie, Luca Borella, Matt White, Christina Dan Wang, Kairong Xiao, Xiao-Yang Liu Yanglet, and Li Deng. 2024. Open FinLLM Leaderboard: Towards Financial AI Readiness. _International Workshop on Multimodal Financial Foundation Models (MFFMs), ACM ICAIF_ (2024).
* Liu et al. (2023a) Xiao-Yang Liu, Guoxuan Wang, and Daochen Zha. 2023a. Data-Centric FinGPT: Democratizing Internet-scale data for financial large language models. _Workshop on Instruction Tuning and Instruction Following, NeurIPS_ (2023).
* Liu et al. (2022) Xiao-Yang Liu, Ziyi Xia, Jingyang Rui, Jiechao Gao, Hongyang Yang, Ming Zhu, Christina Dan Wang, Zhaoran Wang, and Jian Guo. 2022. FinRL-Meta: Market Environments and Benchmarks for Data-Driven Financial Reinforcement Learning. _NeurIPS, Special Track on Datasets and Benchmarks_ (2022).
* Liu et al. (2023b) Xiao-Yang Liu, Ziyi Xia, Hongyang Yang, Jiechao Gao, Daochen Zha, Ming Zhu, Christina Dan Wang, Zhaoran Wang, and Jian Guo. 2023b. Dynamic Datasets and Market Environments for Financial Reinforcement Learning. _Machine Learning Journal, Springer Nature_ (2023).
* Liu et al. (2024) Xiao-Yang Liu, Jie Zhang, Guoxuan Wang, Weiqing Tong, and Anwar Walid. 2024a. FinGPT-HPC: Efficient Pretraining and Finetuning Large Language Models for Financial Applications with High-Performance Computing. _arXiv preprint arXiv:2402.13533_ (2024).
* Liu et al. (2020) Zhuang Liu, Degen Huang, Kaiyu Huang, Zhuang Li, and Jun Zhao. 2020. FinBERT: A Pre-trained Financial Language Representation Model for Financial Text Mining. In _Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20_, Christian Bessiere (Ed.). International Joint Conferences on Artificial Intelligence Organization, 4513-4519. Special Track on AI in FinTech.
* Liu et al. (2024b) Zhiwei Liu, Xin Zhang, Kailai Yang, Qianqian Xie, Jimin Huang, and Sophia Ananiadou. 2024b. FMDLlama: Financial Misinformation Detection based on Large Language Models. _arXiv preprint arXiv:2409.16452_ (2024).
* Lopez-Lira and Tang (2023) Alejandro Lopez-Lira and Yuehua Tang. 2023. Can chatgpt forecast stock price movements? return predictability and large language models. _arXiv preprint arXiv:2304.07619_ (2023).
* Loughran and McDonald (2020) Tim Loughran and Bill McDonald. 2020. Textual analysis in finance. _Annual Review of Financial Economics_ 12 (2020), 357-375.
* Magdon-Ismail and Atiya (2004) Malik Magdon-Ismail and Amir F Atiya. 2004. Maximum drawdown. _Risk Magazine_ 17, 10 (2004), 99-102.
* Maia et al. (2018) Macedo Maia, Siegfried Handschuh, Andre Freitas, Brian Davis, Ross McDermott, Manel Zarrouk, and Alexandra Balahur. 2018. WWW'18 Open Challenge: Financial Opinion Mining and Question Answering. _WWW '18: Companion Proceedings of the The Web Conference 2018_, 1941-1942.
* Malo et al. (2014) Pekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wallenius, and Pyry Takala. 2014. Good debt or bad debt: Detecting semantic orientations in economic texts. _Journal of the Association for Information Science and Technology_ 65, 4 (2014), 782-796.
* Mariko et al. (2020) Dominique Mariko, Hanna Abi Akl, Estelle Labludurie, Stephane Durfort, Hugues De Mazancourt, and Mahmoud El-Haj. 2020. Financial document causality detection shared task (fincausal 2020). _arXiv preprint arXiv:2012.02505_ (2020).
* Mukherjee et al. (2022) Rajdeep Mukherjee, Abhinav Bohra, Akash Banerjee, Soumya Sharma, Manjunath Hegde, Afreen Shaikh, Shivani Shrivastava, Koustuv Dasgupta, Niloy Ganguly, Saptarshi Ghosh, et al. 2022. Ectsum: A new benchmark dataset for bullet point summarization of long earnings call transcripts. _arXiv preprint arXiv:2210.12467_ (2022).
* OpenAI (2023) OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
* Zhang et al. (2020)Felipe Dias Paiva, Rodrigo Tomas Nogueira Cardoso, Gustavo Peixoto Hanaoka, and Wendel Moreira Duarte. 2019. Decision-making for financial trading: A fusion approach of machine learning and portfolio selection. _Expert Systems with Applications_ 115 (2019), 635-655.
* Punt (2017) Andre E Punt. 2017. Strategic management decision-making in a complex world: quantifying, understanding, and using trade-offs. _ICES Journal of Marine Science_ 74, 2 (2017), 499-510.
* Quinlan ([n. d.]) Ross Quinlan. [n. d.]. Statlog (Australian Credit Approval). UCI Machine Learning Repository. DOI: https://doi.org/10.24432/C59012.
* Roziere et al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, et al. 2023. Code llama: Open foundation models for code. _arXiv preprint arXiv:2308.12950_ (2023).
* Alvarado et al. (2015) Julio Cesar Salinas Alvarado, Karin Verspoor, and Timothy Baldwin. 2015. Domain Adaption of Named Entity Recognition to Support Credit Risk Assessment. In _Proceedings of the Australasian Language Technology Association Workshop 2015_, Ben Hachey and Kellie Webster (Eds.). Paramatta, Australia, 84-90. https://aclanthology.org/U15-1010
* Shah et al. (2023a) Agam Shah, Suvan Paturi, and Sudheer Chava. 2023a. Trillion Dollar Words: A New Financial Dataset, Task & Market Analysis. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 6664-6679.
* Shah et al. (2023b) Agam Shah, Ruchit Vithani, Abhinav Gullapalli, and Sudheer Chava. 2023b. Finer: Financial named entity recognition dataset and weak-supervision model. _arXiv preprint arXiv:2302.11157_ (2023).
* Shah et al. (2022) Raj Shah, Kunal Chawla, Dheeraj Eidnani, Agam Shah, Wendi Du, Sudheer Chava, Natraj Raman, Charese Smiley, Jiaoa Chen, and Diyi Yang. 2022. When FLUE Meets FLANG: Benchmarks and Large Pretrained Language Model for Financial Domain. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_. 2322-2335.
* Sharma et al. (2023) Soumya Sharma, Subhendu Khatuya, Manjunath Hegde, Afreen Shaikh, Koustuv Dasgupta, Pawan Goyal, and Niloy Ganguly. 2023. Financial Numeric Extreme Labelling: A dataset and benchmarking. In _Findings of the Association for Computational Linguistics: ACL 2023_. 3550-3561.
* Sharma et al. (2022) Soumya Sharma, Tapas Nayak, Arusarka Bose, Ajay Kumar Meena, Koustuv Dasgupta, Niloy Ganguly, and Pawan Goyal. 2022. FinRED: A dataset for relation extraction in financial domain. In _Companion Proceedings of the Web Conference 2022_. 595-597.
* Sharpe (1998) William F Sharpe. 1998. The sharpe ratio. _Streetwise-the Best of the Journal of Portfolio Management_ 3 (1998), 169-85.
* Sinha and Khandait (2021) Ankur Sinha and Tanmay Khandait. 2021. Impact of news on the commodity market: Dataset and results. In _Advances in Information and Communication: Proceedings of the 2021 Future of Information and Communication Conference (FICC), Volume 2_. Springer, 589-601.
* Soun et al. (2022) Yejun Soun, Jaemin Yoo, Minyong Cho, Jihyeong Jeon, and U Kang. 2022. Accurate Stock Movement Prediction with Self-supervised Learning from Sparse Noisy Tweets. In _2022 IEEE International Conference on Big Data (Big Data)_. IEEE, 1691-1700.
* Sy et al. (2023) Eugene Sy, Tzu-Cheng Peng, Shih-Hsuan Huang, Heng-Yu Lin, and Yung-Chun Chang. 2023. Fine-Grained Argument Understanding with BERT Ensemble Techniques: A Deep Dive into Financial Sentiment Analysis. In _Proceedings of the 35th Conference on Computational Linguistics and Speech Processing (ROCLING 2023)_. 242-249.
* Team et al. (2023) Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_ (2023).
* Team (2023) InternLM Team. 2023. Internlm: A multilingual language model with progressively enhanced capabilities.
* Tschir et al. (2020)Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_ (2023).
* Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_ (2023b).
* White et al. (2024) Matt White, Ibrahim Haddad, Cailean Osborne, Xiao-Yang Yanglet Liu, Ahmed Abdelmonsef, Sachin Varghese, and Arnaud Le Hors. 2024. The model openness framework: Promoting completeness and openness for reproducibility, transparency and usability in Artificial Intelligence. _arXiv preprint arXiv:2403.13784_ (2024).
* Wu et al. (2018) Huizhe Wu, Wei Zhang, Weiwei Shen, and Jun Wang. 2018. Hybrid deep sequential modeling for social text-driven stock prediction. In _Proceedings of the 27th ACM international conference on information and knowledge management_. 1627-1630.
* Wu et al. (2023) Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. 2023. BloombergGPT: A Large Language Model for Finance. arXiv:2303.17564 [cs.LG]
* Xie et al. (2023a) Qianqian Xie, Weiguang Han, Yanzhao Lai, Min Peng, and Jimin Huang. 2023a. The Wall Street Neophyte: A Zero-Shot Analysis of ChatGPT Over MultiModal Stock Movement Prediction Challenges. _arXiv preprint arXiv:2304.05351_ (2023).
* Xie et al. (2023b) Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro Lopez-Lira, and Jimin Huang. 2023b. PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmarks for Finance. _Advances in Neural Information Processing Systems, Special Track on Datasets and Benchmarks_ (2023).
* Xie et al. (2024) Qianqian Xie, Dong Li, Mengxi Xiao, Zihao Jiang, Ruoyu Xiang, Xiao Zhang, Zhengyu Chen, Yueru He, Weiguang Han, Yuzhe Yang, et al. 2024. Open-FinLLMs: Open multimodal large language models for financial applications. _arXiv preprint arXiv:2408.11878_ (2024).
* Xu and Cohen (2018) Yumo Xu and Shay B Cohen. 2018. Stock movement prediction from tweets and historical prices. In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_. 1970-1979.
* Yang et al. (2023a) Hongyang Yang, Xiao-Yang Liu, and Christina Dan Wang. 2023a. FinGPT: Open-Source Financial Large Language Models. _Symposium on FinLLM, IJCAI 2023_ (2023).
* Yang et al. (2020a) Linyi Yang, Eoin M Kenny, Tin Lok James Ng, Yi Yang, Barry Smyth, and Ruihai Dong. 2020a. Generating plausible counterfactual explanations for deep transformers in financial text classification. _arXiv preprint arXiv:2010.12512_ (2020).
* Yang et al. (2023b) Yi Yang, Yixuan Tang, and Kar Yan Tam. 2023b. InvestLM: A Large Language Model for Investment using Financial Domain Instruction Tuning. arXiv:2309.13064 [q-fin.GN]
* Yang et al. (2020b) Yi Yang, Mark Christopher Siy UY, and Allen Huang. 2020b. FinBERT: A Pretrained Language Model for Financial Communications. arXiv:2006.08097 [cs.CL]
* Yang et al. (2024) Yuzhe Yang, Yifei Zhang, Yan Hu, Yilin Guo, Ruoli Gan, Yueru He, Mingcong Lei, Xiao Zhang, Haining Wang, Qianqian Xie, et al. 2024. UCFE: A User-Centric Financial Expertise Benchmark for Large Language Models. _arXiv preprint arXiv:2410.14059_ (2024).
* Yanglet and Deng (2024) Xiao-Yang Liu Yanglet and Li Deng. 2024. Multimodal Financial Foundation Models (MFFMs): Progress, Prospects, and Challenges. _International Workshop on Multimodal Financial Foundation Models (MFFMs) at 5th ACM International Conference on AI in Finance (MFFM at ICAIF '24)_, (2024).
* Yu et al. (2023) Yangyang Yu, Haohang Li, Zhi Chen, Yuechen Jiang, Yang Li, Denghui Zhang, Rong Liu, Jordan W. Suchow, and Khaldoun Khashanah. 2023. FinMem: A Performance-Enhanced LLM Trading Agent with Layered Memory and Character Design. arXiv:2311.13743 [q-fin.CP]Yangyang Yu, Zhiyuan Yao, Haohang Li, Zhiyang Deng, Yupeng Cao, Zhi Chen, Jordan W Suchow, Rong Liu, Zhenyu Cui, Denghui Zhang, et al. 2024. FinCon: A Synthesized LLM Multi-Agent System with Conceptual Verbal Reinforcement for Enhanced Financial Decision Making. _arXiv preprint arXiv:2407.06567_ (2024).
* Yuan et al. (2021) Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text generation. _Advances in Neural Information Processing Systems_ 34 (2021), 27263-27277.
* Zhang et al. (2023a) Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, and Guoyin Wang. 2023a. Instruction Tuning for Large Language Models: A Survey. arXiv:2308.10792 [cs.CL]
* Zhang et al. (2019) Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. _arXiv preprint arXiv:1904.09675_ (2019).
* Zhang et al. (2023b) Xuanyu Zhang, Bingbing Li, and Qing Yang. 2023b. CGCE: A Chinese Generative Chat Evaluation Benchmark for General and Financial Domains. arXiv:2305.14471 [cs.CL]
* Zhang et al. (2024) Xiao Zhang, Ruoyu Xiang, Chenhan Yuan, Duanyu Feng, Weiguang Han, Alejandro Lopez-Lira, Xiao-Yang Liu, Sophia Ananiadou, Min Peng, Jimin Huang, and Qianqian Xie. 2024. Dolares or Dollars? Unraveling the Bilingual Prowess of Financial LLMs Between Spanish and English. arXiv:2402.07405 [cs.CL]
* Zhang et al. (2023c) Xuanyu Zhang, Qing Yang, and Dongliang Xu. 2023c. XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters. arXiv:2305.12002 [cs.CL]
* Zhao et al. (2024) Huaqin Zhao, Zhengliang Liu, Zihao Wu, Yiwei Li, Tianze Yang, Peng Shu, Shaochen Xu, Haixing Dai, Lin Zhao, Gengchen Mai, et al. 2024b. Revolutionizing finance with llms: An overview of applications and insights. _arXiv preprint arXiv:2401.11641_ (2024).
* Zhao et al. (2024a) Yilun Zhao, Hongjun Liu, Yitao Long, Rui Zhang, Chen Zhao, and Arman Cohan. 2024a. Finance-MATH: Knowledge-Intensive Math Reasoning in Finance Domains. In _Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, LunWeiKu, AndreMartins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 12841-12858. https://aclanthology.org/2024.acl-long.693
* Zhao et al. (2024c) Yilun Zhao, Yitao Long, Hongjun Liu, Ryo Kamoi, Linyong Nan, Lyuhao Chen, Yixin Liu, Xiangru Tang, Rui Zhang, and Arman Cohan. 2024c. DocMath-Eval: Evaluating Math Reasoning Capabilities of LLMs in Understanding Long and Specialized Documents. In _Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, LunWeiKu, AndreMartins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 16103-16120. https://aclanthology.org/2024.acl-long.852
* Zhou et al. (2023) Xianzheng Zhou, Hui Zhou, and Huaigang Long. 2023. Forecasting the equity premium: Do deep neural network models work? _Modern Finance_ 1, 1 (2023), 1-11.
* Zhou et al. (2021) Zhihan Zhou, Liqian Ma, and Han Liu. 2021. Trade the Event: Corporate Events Detection for News-Based Event-Driven Trading. arXiv:2105.12825 [cs.CL]
* Zhu et al. (2021) Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. 2021. TAT-QA: A question answering benchmark on a hybrid of tabular and textual content in finance. _arXiv preprint arXiv:2105.07624_ (2021).

Contributions

**Science Leadership**: Qianqian Xie, Min Peng, Sophia Ananiadou, Alejandro Lopez-Lira, Hao Wang, Yanzhao Lai, Benyou Wang, Xiao-Yang Liu, Gang Hu, Jiajia Huang, Jimin Huang.

**Contributors**: Mengxi Xiao, Dong Li, Weiguang Han, Zhengyu Chen, Ruoyu Xiang, Xiao Zhang, Yueru He, Yongfu Dai, Duanyu Feng, Yijing Xu, Haoqiang Kang, Ziyan Kuang, Chenhan Yuan, Kailai Yang, Zheheng Luo, Tianlin Zhang, Zhiwei Liu, Guojun Xiong, Zhiyang Deng, Yuechen Jiang, Zhiyuan Yao, Haohang Li, Yangyang Yu

## Appendix B Fintrade Dataset

## Appendix C Other LLMs Performance

Table 7 presents other LLMs' performance in the FinBen.

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Ticker** & **Number of News** & **Number of 10-K/10-Q Flies** & **Numerical Price Data** \\ \hline TSLA & 3,233 & 8 & 497 \\ NFLX & 965 & 8 & 497 \\ AMZN & 1,675 & 8 & 497 \\ MSFT & 1,362 & 8 & 497 \\ AAPL & 2,082 & 8 & 497 \\ GOOG & 1,144 & 7 & 497 \\ DIS & 1,445 & 9 & 497 \\ GM & 2,252 & 9 & 497 \\ NIO & 957 & 0 & 497 \\ COIN & 1,022 & 0 & 497 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Summary of FinTrade dataset statistics.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c} \hline \hline
**Dataset** & **Metrics** & **Problems** & **Coat-L2-A1-A1-A1** & **H-B-C** & **Coat-L3-B1** & **Qu-a1** & **Kuang-a1** & **Kuang-a1** & **Kuang-a1** & **Kuang-a1** \\ \hline TSLA & 3,233 & 8 & 497 \\ NFLX & 965 & 8 & 497 \\ AMZN & 1,675 & 8 & 497 \\ MSFT & 1,362 & 8 & 497 \\ AAPL & 2,082 & 8 & 497 \\ GOOG & 1,144 & 7 & 497 \\ DIS & 1,445 & 9 & 497 \\ GM & 2,252 & 9 & 497 \\ NIO & 957 & 0 & 497 \\ COIN & 1,022 & 0 & 497 \\ \hline \hline \end{tabular}
\end{table}
Table 7: The zero-shot and few-shots performance of other LLMs on the FinBen.

## Appendix D Instructions

For detail instruction of each dataset, please see Table 8 and Table 9.

## Appendix E Related Work

### Financial Large Language Models

Recent years have seen a significant surge in research on finance-specific LLMs, expanding on the groundwork laid by general-purpose language models (Lee et al., 2024; Liu et al., 2023; Xie et al., 2023a; Zhang et al., 2024; Dai et al., 2024; Xie et al., 2024). Financial pre-trained language models (FinPLMs) like FinBERT (Araci, 2019; Yang et al., 2020; Liu et al., 2020), derived from BERT, and FLANG (Shah et al., 2022), based on ELECTRA, have been developed using domain-specific data for enhanced performance in tasks like sentiment analysis and stock prediction. The open-source release of Meta AI's LLaMA (Touvron et al., 2023, 2023) has fueled further innovation in Financial LLMs (FinLLMs), with models like FinMA (Xie et al., 2023), InvestLM (Yang et al., 2023), and FinGPT (Liu et al., 2023, 2024; Yang et al., 2023) leveraging advanced tuning strategies (Zhang et al., 2023) for financial applications. BloombergGPT (Wu et al., 2023) stands out as a BLOOM-based, closed-source model tailored for the financial industry. Additionally, the Chinese financial sector has seen the emergence of models like XuanYuan 2.0 (Zhang et al., 2023), integrating broad and specialized knowledge, FinBART (Hongyuan et al., 2023) for financial communication, and CFGPT (Li et al., 2023), which includes a comprehensive dataset for targeted pre-training and fine-tuning.

### Financial Evaluation Benchmarks

Financial evaluation benchmarks, such as the pioneering FLUE (Shah et al., 2022), have been introduced to measure model performance in the financial sector, covering five key NLP tasks: financial sentiment analysis (Shah et al., 2022), news headline classification (Sinha and Khandait, 2021), named entity recognition (NER) (Salinas Alvarado et al., 2015), structure boundary detection and question answering (QA) (Chen et al., 2022). Building upon FLUE, FLARE (Xie et al., 2023) added the evaluation of time-series processing capabilities, i.e., forecasting stock price movements. In addition, in Chinese financial benchmarks, there are more recently released Chinese datasets like CFBenchmark (Lei et al., 2023), DISC-FINSFT (Chen et al., 2023), and CGCE (Zhang et al., 2023). However, these benchmarks have a limited scope and have not yet addressed more complex financial NLP tasks such as event detection (Zhou et al., 2021), and realistic financial tasks, despite the fact that there were previous efforts on stock trading (Liu et al., 2022; Han et al., 2023, 20).

## Appendix F Trading Accumulative Returns

Table 10 and the figures below show detailed trading performance.

Figure 2: Accumulative Returns of LLM Trading Strategies on AAPL

\begin{table}
\begin{tabular}{l p{284.5pt}} \hline \hline
**Data** & **Prompt** \\ \hline FPB & “Analyze the sentiment of this statement extracted from a financial news article. \\  & provide your answer as either negative, positive or neutral. \\  & For instance, The company’s stocks plummeted following the scandal.’ would be classified as negative.* \\ \hline FQA-SA & “What is the sentiment of the following financial (category). \\  & Positive, Negative, or Neutral?” \\ \hline Headline & “Consider whether the health meetings the price of gold. \\  & It shows a Price or vice for its gold commodity market indexed in the news headline? \\  & Please answer Yes or No.” \\ \hline NER & “In the sentences extracted from financial agreements in U.S. SBC flings, \\  & identify the semantically entities that represent a person (PREA), an organization ('ORG'), or a location (LOC). The frequent answer from: “entity name, entity type”. \\  & For instance, in "Ebox Mais, CEO of Served, announced the lunch from CEO Cane-Cunerval.’, \\  & the entities would be: "Ebox Mais, CEO of Served, XXXeon, GOG, Cape Camers, LOC" \\ \hline FFNER-ORD & “In the list of tokens, identify [id]/each accordingly. \\  & If the entity was multiple tokens, the foreign's-B-ERC, or B-ORG for the first token, and I-PER, \\  & I-LOC, or B-ORG for the subsequent tokens of that entity. \\  & The beginning of each separate entity should always be labeled with a B-PER, R-LOC, or B-ORG prefix. \\  & If the token does not fit into any of the three named categories, or is not named entity, label it as “O.” \\ \hline FinQA & “Given the financial data and expert analysis, please answer this question.” \\ \hline Regulations & “Please answer following questions.* \\ \hline \multirow{4}{*}{Coo-FauQA} & “In the context of this series of interconnected finance-related queries and the additional information provided by the pretest, table data, and post post text from a company’s financial filings, please provide a response to the final question. This may require extracting information from the context and performing mathematical calculations. Please take into account the information provided in the preceding question and their answers when formulating your response.” \\ \hline BigData22 & “Contempplate the data and tweets to press whether the closing price of [id] will super or decline at [point]. \\  & Please declare with either Rise or Fall.* \\ \hline ACL18 & “Scrithering the data and tweets to envisage of the closing price of [id]/will well or context at [point]. \\  & Respond with either Rise or Fall.* \\ \hline CIKM18 & “Reflect on the provided data and tweets to anticipate if the closing price of [id] is going to increase or decrease at [point]. \\  & Respond with either Rise or Fall.* \\ \hline ECTSum & “Given the following article, please produce a list of 0 and 1, each separated by ” to indicate which sentences should be included in the final summary. The article’s sentences have been split by ”. Please mark each sentence with I if it should be included in the summary and 0 if it should not.” \\ \hline EDTSSum & “You are given a set that consists of multiple sentences. Your task is to perform abstracting summarization on this text. Use your understanding of the content to express the main ideas and crucial details in a shorter, coherent, and natural sounding text.” \\ \hline German & “Asses the contributions of a customer using the following table attributes for financial states. Repword with either 'good’ or "bad". And the table attributes including 13 categorical attributes and 7 numerical attributes are as follows.” \\ \hline Australian & “Asses the contributions of a customer using the following table attributes for financial states. Repword with either 'good' or "bad". And the table attributes including 13 categorical attributes and 7 numerical attributes and values have been changed to meaningless symbols to protect confidentiality of the data. * \\ \hline FOMC & “Examine the excerpt from a central bank’s closing block. Classify is its HMXRSH if it abroaches for a tightening of monetary policy. POWSH if it suggests an instance of monetary policy, NEUTRAL if the stance is unbiased. Your response should return only HARKISH, DOVISER, or NEUTRAL.” \\ \hline TSA & “Given the following financial text, return a sentiment score for Abhead as a floating-point number ranging from -1 (indicating a very negative branch sentiment) to 1 (indicating a very positive or blink sentiment), with 0 designating neutral sentiment. Return only the mentioned score first, follow it with a brief reasoning behind your score.” \\ \hline FFNLarge - ACC & “Analyze sentences from camsing conference calls and identify their argumentative function. \\  & Each sentence is either a premise, offering evidence or reasoning, or a claim, assuming a conclusion or viewpoint. Return only premiive or claim.” \\ \hline FFNLarge - ACC & “In this task, you are given a part of sentences. \\ \hline FFNLarge - ACC & “The problem of the “best-identity”, “including the discernible relation between the sentences. \\  & “Sporter’, indicating that the first sentence supports the second, “Attack”, indicating that the first sentence disputes or contracts the second. Return only one of the three classifications: “neoriation, "support”, or "attack.’ \\ \hline MultiFin & “In this task, you are working with English headlines from the MULTIFAN dataset. \\ \hline MultiFin & This dataset is made up of real-world article headlines from a large co-running firm’s websites. \\  & Your objective is to categorize each headline according to its primary topic. \\  & The potential categories are (category). \\ \hline MA & “In this task, you will be given Mergers and acquisitions news articles or tweets. \\  & Your task in to classify each article or tweet based on whether the mentioned dual was completed or remained a number. \\  & Your response should be a single word - either complete or “rumor” - \\  & representing the outcome of the deal mentioned in the provided text.* \\ \hline MLESG & “You’s five given English news articles related to Environmental, Social, and Corporate Governance (ESG) issues. \\  & Your task is to classify each author based on the ESG issue it permits to, according to the MSC ESC rising guidelines. \\  & The ESG issues include (category). \\ \hline \hline \end{tabular}
\end{table}
Table 8: Quantification task datasets prompt overview.

[MISSING_PAGE_FAIL:21]

[MISSING_PAGE_EMPTY:22]

### Tasks and Datasets

**Task 1: Financial Classification**. This task, inherited from FinBen's financial classification task, focuses on argument unit classification to test the capabilities of LLMs to identify and categorize texts as premises or claims. It consists of \(7.75K\) training data and \(969\) test data to categorize sentences as claims or premises. We use two metrics to evaluate classification capability, like F1 and Accuracy. F1 score is used as the final ranking metric.

Figure 4: Accumulative Returns of LLM Trading Strategies on COIN

Figure 5: Accumulative Returns of LLM Trading Strategies on GOOG

Figure 3: Accumulative Returns of LLM Trading Strategies on AMZN

Figure 6: Accumulative Returns of LLM Trading Strategies on MSFT

**Task 2: Financial Text Summarization**. This task, inherited from FinBen's generation task, is designed to test the capabilities of LLMs to generate coherent summaries. It provides 8k training data and 2k test data for abstracting financial news articles into concise summaries. We utilize three metrics, such as ROUGE (1, 2, and L) and BERTScore, to evaluate generated summaries in terms of Relevance. ROUGE -1 score is used as the final ranking metric.

Figure 8: Accumulative Returns of LLM Trading Strategies on NIO

Figure 10: Accumulative Returns of LLM Trading Strategies on DIS

Figure 7: Accumulative Returns of LLM Trading Strategies on NFLX

Figure 9: Accumulative Returns of LLM Trading Strategies on TSLA

**Task 3: Single Stock Trading**. This task, inherited from FinBen's Trading task, aims to evaluate LLMs' ability to make sophisticated decisions in trading activities, which is currently restricted by human's limited ability to process large volumes of data rapidly. It specifically provides 291 data different from FinBen datasets, to evaluate LLMs on sophisticated stock Decisions. We offer a comprehensive assessment of profitability, risk management, and decision-making prowess by a series of metrics, such as Sharpe Ratio (SR), Cumulative Return (CR), Daily (DV) and Annualized volatility (AV), and Maximum Drawdown (MD). Sharpe Ratio (SR) score is used as the final ranking metric.

### Model Cheating Detection

To measure the risk of data leakage from the test set used in training, we introduce the Data Leakage Test (DLT). The DLT calculates the difference in perplexity between the training set and the test set. A larger difference indicates a lower likelihood of model cheating, while a smaller difference suggests a higher likelihood. For our FinLLM Challenge, we invite Top-3 participant teams per task for cheating detection.

### Participants and Automatic Evaluation

There are \(35\) teams registered for FinLLM Challenge, with \(12\) teams submitting their system description papers. Participants can opt to join one or more task(s).

As shown in Table 12, the top 3 teams achieved outstanding performance in Task 1. Their models' F1 scores were comparable to LlaMA3-8B, although slightly inferior to GPT-4 and LLaMA2-70B, yet significantly outperformed FinMA and other models. The results in Table 12 further demonstrate that our FinLLM share task provides an excellent framework for participating teams to achieve superior experimental outcomes.

\begin{table}
\begin{tabular}{c c c c} \hline Terms & ACC & F1 & MCC \\ \hline Team Barclays & 0.7662 & 0.5237 & 0.7427 \\ Alphours & 0.7574 & 0.5174 & 0.7555 \\ LNTC & 0.7544 & 0.5149 & 0.7581 \\ Wealth Guide & 0.7513 & 0.5018 & 0.7406 \\ Finance Wizard & 0.7286 & 0.4554 & 0.7008 \\ CalMenon & 0.711 & 0.4199 & 0.6818 \\ Upaya & 0.709 & 0.4166 & 0.6941 \\ Vidra & 0.7079 & 0.4141 & 0.69 \\ \(y\) & 0.4933 & 0.0141 & 0.5905 \\ \hline \end{tabular}
\end{table}
Table 12: The Result of Tasks 1: Financial Classification

Figure 11: Accumulative Returns of LLM Trading Strategies on GM

\begin{table}
\begin{tabular}{c c c c c} \hline Category & Tasks & Datasets & Evaluation Metrics \\  & & Training set & Test set & \\ \hline Task 1 & Financial Classification & 7.75k & 969 & F1 Score, Acc \\ Task 2 & Financial Text Summarization & 8k & 2k & ROUGE-1, ROUGE-2, ROUGE-1, BERTScore \\ Task 3 & Single Stock Trading & 291 & 225 & Sharpe Ratio, Cumulative Return, \\  & & & Maximum Drawdown, Daily and Annualized Volatility. \\ \hline \end{tabular}
\end{table}
Table 11: Tasks and Datasets of FinLLM Challenge.

As illustrated in Table 13, in terms of the Rouge-1 metric, the models of these three teams surpassed all other models, demonstrating superior performance. The results in Table 2 indicate that, for financial generation tasks, our provided dataset and model framework help participating teams leverage their strengths and achieve better outcomes.

As shown in Table14, the Top-1 Wealth Guide team excelled in the Sharpe Ratio metric, surpassing other teams and demonstrating outstanding performance. While it may not match the performance of GPT-4, it still outperforms other large models. These results from Table 3 once again underscore the significance of organizing the FinLLM share task. The FinLLM Challenge not only assesses the performance of large language models (LLMs) but also fosters further research into applying LLMs in the financial domain.

## Appendix H Performances of non-LLM methods

In this section, we present the performances of non-LLM methods on stock movement prediction and financial NLP tasks from previous papers. Note that non-LLM methods are task-oriented, each model can only run on a specific task.

### Stock Movement Prediction

Stock movement prediction performance of non-LLM models are shown in Table 15. The results are from (Xie et al., 2023b).

### Financial NLP Tasks

BERT-based model results of financial NLP tasks are shown in Table 16. The results are from (Shah et al., 2022).

\begin{table}
\begin{tabular}{c c c c c c} \hline Trans. & Range-1 & Range-2 & Range-L- & Best-Score & Bar-Score \\ \hline Wealth Guide & 0.308893532 & 0.17946897 & 0.28193420 & 0.8599999 & -4.961457408 \\ Abtowens & 0.369077581 & 0.2010585935 & 0.322684316 & 0.872049115 & -3.931526929 \\ LBF & 0.53461261 & 0.331542645 & 0.49217958 & 0.917132047 & -3.407506172 \\ LXTC & 0.3669452 & 0.187210467 & 0.304617067 & 0.87530743 & -4.25712837 \\ Finance Wizard & 0.521073018 & 0.3460983 & 0.473510018 & 0.90846845 & -3.497898865 \\ Vidas & 0.249545646 & 0.11974689 & 0.22488961 & 0.85882767 & -4.1674907035 \\ Reveita & 0.50011169 & 0.313023818 & 0.46536744 & 0.97018743 & -3.305486962 \\ Uspa & 0.529459817 & 0.35920218 & 0.486046685 & 0.910644962 & -3.45155009 \\ \hline \end{tabular}
\end{table}
Table 13: The Result of Tasks 2: Financial Text Summarization

\begin{table}
\begin{tabular}{c c c c c c c} \hline Method & \multicolumn{2}{c}{BiGDATA22} & \multicolumn{2}{c}{ACL18} & \multicolumn{2}{c}{CIKM18} \\  & ACC & MCC & ACC & MCC & ACC & MCC \\ \hline Logistic regression (LR) & 0.53 & 0.02 & 0.52 & 0.04 & 0.53 & -0.04 \\ Random forest (RF) & 0.47 & -0.11 & 0.52 & 0.03 & 0.54 & 0.01 \\ LSTM & 0.51 & 0.01 & 0.53 & 0.06 & 0.53 & 0.02 \\ Attention LSTM (ALSTM) & 0.49 & -0.03 & 0.52 & 0.04 & 0.53 & -0.01 \\ Adv-ALSTM & 0.50 & 0.01 & 0.53 & 0.07 & 0.54 & 0.02 \\ DTML & 0.52 & 0.07 & 0.58 & 0.18 & 0.54 & -0.00 \\ XGBoost & 0.52 & -0.04 & 0.49 & -0.02 & **0.58** & 0.07 \\ XGBoesser & 0.46 & -0.13 & 0.50 & -0.01 & 0.53 & -0.03 \\ ALSTM-W & 0.48 & -0.01 & 0.53 & 0.08 & 0.54 & 0.03 \\ ALSTM-D & 0.49 & 0.01 & 0.53 & 0.07 & 0.50 & -0.04 \\ StockNet & 0.53 & -0.00 & 0.54 & -0.03 & 0.52 & -0.02 \\ SLOT & **0.55** & **0.10** & **0.59** & **0.21** & 0.56 & **0.09** \\ \hline \end{tabular}
\end{table}
Table 15: Stock movement prediction performance of non-LLM models, measured with the accuracy (ACC) and the Matthews correlation coefficient (MCC). The best performance is in bold.

### Financial Risk Management Tasks

Traditional model results of financial risk management tasks are shown in Table 17. The results are from (Feng et al., 2024).

## Limitations

Despite the novel efforts to benchmark LLMs in the financial domain through FinBen, we acknowledge several inherent limitations that could impact the benchmark's effectiveness and applicability: **Dataset Size Limitations**: The restricted size of available datasets, a common issue in open-source financial data, may affect the models' financial understanding and generalization across various contexts. **Model Size Limitations**: Due to computational constraints, our evaluation was limited to the LLaMA 70B model, potentially overlooking the capabilities of larger or differently architected models. **Generalizability**: The tasks, particularly trading and forecasting, are based on American market data and English texts, possibly limiting the benchmark's applicability to global financial markets. **Potential Negative Impacts**: While FinBen aims to advance financial language understanding, it is crucial to consider potential misuse, such as propagating financial misinformation or exerting unethical influence on markets. Responsible usage and further safeguards are essential13.

Footnote 13: For a detailed ethical and legal statement concerning this work, please see Appendix.

## Ethical Statement

The development and dissemination of the FinBen by the authors carry full responsibility for any potential violation of rights or arising legal issues. All raw data we used are publicly available and do not contain any personal information. Diligent efforts have been undertaken to ensure the construction of the FinBen respects privacy and conforms to established ethical guidelines. The datasets compiled within FinBen are shared under the MIT license, with the expectation that users agree to adhere to its conditions.

This manuscript, inclusive of any associated source codes, datasets, and appendices ("Material"), is designated exclusively for academic and educational pursuits. It is crucial to acknowledge that the Material does not provide financial, legal, or investment counsel, nor should it be utilized as a foundation for any form of decision-making.

While the authors have exerted reasonable diligence to verify the accuracy and reliability of the Material, no explicit or implied warranty is extended regarding its completeness or suitability for any

\begin{table}
\begin{tabular}{c c c c c} \hline Method & FPB & Headline & NER & FJA SA \\  & Accuracy & AvgF1 & F1 & MSE \\ \hline BERT-base & 0.856 & 0.967 & 0.79 & 0.073 \\ FinBERT & 0.872 & 0.968 & 0.8 & 0.070 \\ FLANG-BERT & 0.912 & 0.972 & 0.83 & 0.054 \\ ELECTRA & 0.881 & 0.966 & 0.78 & 0.066 \\ FLANG-ELECTRA & 0.919 & 0.98 & 0.82 & 0.034 \\ \hline \end{tabular}
\end{table}
Table 16: Financial NLP tasks performances of BERT-based models. The best performance is in bold.

\begin{table}
\begin{tabular}{c c c c} \hline
**Dataset** & **Method** & **Metric** & **Value** \\ \hline Credit Card Fraud & ANN & F1 & 0.85 \\  & & MCC & 0.17 \\ ccfraud & EGRNN++ & F1 & **0.90** \\  & & MCC & **0.34** \\ Polish & Bayesian & F1 & **0.99** \\  & & MCC & **0.57** \\ Travel Insurance & Random Forest & F1 & 0.91 \\  & & MCC & 0.15 \\ \hline \end{tabular}
\end{table}
Table 17: Performance of various models on financial risk management datasets. The best performance for each metric is in bold.

specific application. The authors, along with their affiliated entities, absolve themselves of liability for any losses, damages, or other consequences, whether direct or indirect, that may emanate from the employment or reliance upon the Material. It is incumbent upon the user to seek professional consultation for financial, legal, or investment determinations.

By referencing or employing this Material, individuals consent to indemnify, defend, and hold the authors, along with any affiliated organizations or persons, harmless against any claims or damages that may arise from such utilization.

**Disclaimer: We are sharing codes for academic purposes under open-source license. Nothing herein is financial advice, and NOT a recommendation to trade real money. Please use common sense and always first consult a professional before trading or investing.**

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? [Yes] See Limitation (Section H.3). 3. Did you discuss any potential negative societal impacts of your work? [Yes] See Ethical Statement (Section H.3). 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] See Ethical Statement (Section H.3).
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] See Introduction (Section 1). 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [N/A] Our benchmark only includes the evaluation process. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] See Table 4. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Experimental Settings (Section 3).
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] 2. Did you mention the license of the assets? [Yes] 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] Our Introduction (Section 1) contains a link for all data used in FinBen. 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] Our dataset statistics (Table 2) contains licenses for all used datasets. 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] See Ethical Statement (Section H.3).
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]