# Unbounded Differentially Private Quantile and Maximum Estimation

David Durfee

Anonym Inc.

david@anonymco.com

###### Abstract

In this work we consider the problem of differentially private computation of quantiles for the data, especially the highest quantiles such as maximum, but with an unbounded range for the dataset. We show that this can be done efficiently through a simple invocation of AboveThreshold, a subroutine that is iteratively called in the fundamental Sparse Vector Technique, even when there is no upper bound on the data. In particular, we show that this procedure can give more accurate and robust estimates on the highest quantiles with applications towards clipping that is essential for differentially private sum and mean estimation. In addition, we show how two invocations can handle the fully unbounded data setting. Within our study, we show that an improved analysis of AboveThreshold can improve the privacy guarantees for the widely used Sparse Vector Technique that is of independent interest. We give a more general characterization of privacy loss for AboveThreshold which we immediately apply to our method for improved privacy guarantees. Our algorithm only requires one \(O(n)\) pass through the data, which can be unsorted, and each subsequent query takes \(O(1)\) time. We empirically compare our unbounded algorithm with the state-of-the-art algorithms in the bounded setting. For inner quantiles, we find that our method often performs better on non-synthetic datasets. For the maximal quantiles, which we apply to differentially private sum computation, we find that our method performs significantly better.

## 1 Introduction

In statistics, quantiles are values that divide the data into specific proportions, such as median that divides the data in half. Quantiles are a central statistical method for better understanding a dataset. However, releasing quantile values could leak information about specific individuals within a sensitive dataset. As a result, it becomes necessary to ensure that individual privacy is ensured within this computation. Differential privacy offers a rigorous method for measuring the amount that one individual can change the output of a computation. Due it's rigorous guarantees, differential privacy has become the gold standard for measuring privacy. This measurement method then offers an inherent tradeoff between accuracy and privacy with outputs of pure noise achieving perfect privacy. Thus, the goal of designing algorithms for differentially private quantile computation is to maximize accuracy for a given level of privacy.

There are a variety of previous methods for computing a given quantile of the dataset that we will cover in Section 1.2, but each of these requires known bounds on the dataset. The most effective and practical method invokes the exponential mechanism Smith (2011). For computing multiple quantiles this method can be called iteratively. Follow-up work showed that it could be called recursively by splitting the dataset at each call to reduce the privacy cost of composition Kaplan et al. (2022). Further, a generalization can be called efficiently in one shot Gillenwater et al. (2021).

### Our contributions

In this work, we offer an alternative practical and accurate approach, Unbounded Quantile Estimation (UQE), that also invokes a well-known technique and can additionally be applied to the unbounded setting. While the commonly-used technique designs a distribution to draw from that is specific to the dataset, our method will simply perform a noisy guess-and-check. Initially we assume there is only a lower bound on the data, as non-negative data is common in real world datasets with sensitive individual information. Our method will simply iteratively increase the candidate value by a small percentage and halt when the number of data points below the value exceeds the desired amount dictated by the given quantile. While the relative increase will be small each iteration, the exponential nature still implies that the candidate value will become massive within a reasonable number of iterations. As a consequence, our algorithm can handle the unbounded setting where we also show that two calls to this procedure can handle fully unbounded data. Computing multiple quantiles can be achieved by applying the recursive splitting framework from Kaplan et al. (2022).

Performing our guess-and-check procedure with differential privacy exactly fits AboveThreshold, a method that is iteratively called in the Sparse Vector Technique Dwork et al. (2009). We also take a deeper look at AboveThreshold and unsurprisingly show that similar to report noisy max algorithms, the noise addition can come from the Laplace, Gumbel or Exponential distributions. We further push this analysis to show that for monotonic queries, a common class of queries which we will also utilize in our methods, the privacy bounds for composition within the Sparse Vector Technique can be further improved. Given the widespread usage of this technique,1 we believe this result is of independent interest. Furthermore, we give a more general characterization of query properties that can improve the privacy bounds of AboveThreshold. We immediately utilize this characterization in our unbounded quantile estimation algorithm to improve privacy guarantees.

Footnote 1: For example, see Roth and Roughgarden (2010); Hardt and Rothblum (2010); Dwork et al. (2015); Nissim et al. (2016); Nissim and Stemmer (2018); Kaplan et al. (2020, 2020); Hasidim et al. (2020); Bun et al. (2017); Bassily et al. (2018); Cummings et al. (2020); Ligett et al. (2017); Barthe et al. (2016, 2016); Steinke and Ullman (2016); Cummings et al. (2015); Ullman (2015); Nandi and Bassily (2020); Shokri and Shmatikov (2015); Hsu et al. (2013); Sajed and Sheffet (2019); Feldman and Steinke (2017); Blum et al. (2015); Chen et al. (2016)

While the commonly used algorithms for quantile estimation can still apply incredibly loose bounds to ensure the data is contained within, this can have a substantial impact upon the accuracy for estimating the highest quantiles such as maximum. This leads to an especially important application for our algorithm, differentially private sum computation, which can thereby be used to compute mean as well. Performing this computation practically without assumptions upon the distribution often requires clipping the data and adding noise proportionally. Clipping too high adds too much noise, and clipping too low changes the sum of the data too much. The highest quantiles of the data are used for clipping to optimize this tradeoff. The unbounded nature of our approach fundamentally allows us to estimate the highest quantiles more robustly and improve the accuracy of differentially private sum computation.

This improvement in differentially private sum computation is further evidenced by our empirical evaluation, with significant improvements in accuracy. Our empericial comparison will be upon the same datasets from previous work in the bounded setting. We also compare private computation of the inner quantiles on these datasets. For synthetic datasets generated from uniform or guassian distributions, we see that the more structured approach of designing a distribution for the data from the exponential mechanism consistently performs better. However, for the real-world datasets, we see that our unstructured approach tends to perform better even within this bounded setting. By design our algorithm is less specific to the data, so our alternative approach becomes advantageous when less is known about the structure and bounds of the data _a priori_. As such, for large-scale privacy systems that provide statistical analysis for a wide variety of datasets, our methods will be more flexible to handle greater generality accurately.

### Background literature

The primary algorithm for privately computing a given quantile, by which we compare our technique, applies the exponential mechanism with a utility function based upon closeness to the true quantile Smith (2011). We will discuss this algorithm, which we denote as Exponential Quantile (EMQ), in greater detail in Appendix A. This approach was then extended to computing multiple quantiles more cleverly by recursively splitting the data and establishing that only one partition ofthe dataset can change between neighbors thereby reducing the composition costs Kaplan et al. (2022). Additional follow-up work showed that a generalization of the utility function to multiple quantiles could be efficiently drawn upon in one shot Gillenwater et al. (2021). Another recent result examined this problem in the streaming data setting and gave a method that only uses strongly sub-linear space complexity Alabi et al. (2022).

Quantile computation can also be achieved through CDF estimation Bun et al. (2015); Kaplan et al. (2020). However these techniques offer limited practicality as they rely upon several reductions and parameter tuning. Recursively splitting the data is also done for CDF estimation algorithms where the statistics from each split can be aggregated for quantile computation Dwork et al. (2010); Chan et al. (2011). These techniques tend to be overkill for quantile estimation and thus suffer in accuracy comparatively.

We will also give improved privacy analysis of the Sparse Vector Technique which was originally introduced in Dwork et al. (2009). A more detailed analysis of the method can be found in Lyu et al. (2017). Additional recent work has shown that more information can be output from the method at no additional privacy cost Kaplan et al. (2021); Ding et al. (2023).

### Organization

We provide the requisite notation and definitions in Section 2. In Section 3, we review the AboveThreshold algorithm from the literature and show that privacy analysis can be further improved. In Section 4, we provide our unbounded quantile estimation method. In Section 5, we test our method compared to the previous techniques on synthetic and real world datasets. In Appendix A, we consider the estimation of the highest quantiles which has immediate application to differentially private sum and mean estimation. In Appendix B, we give further results on the AboveThreshold algorithm and provide the missing proofs from Section 3. In Appendix C, we provide further variants and extensions of our unbounded quantile estimation technique.

## 2 Preliminaries

We will let \(x,x^{\prime}\) denote datasets in our data universe \(\mathcal{X}\).

**Definition 2.1**.: _Datasets \(x,x^{\prime}\in\mathcal{X}\) are neighboring if at most one individual's data has been changed._

Note that we use the _swap_ definition, but our analysis of the AboveThresholdalgorithm will be agnostic to the definition of neighboring. Using this definition as opposed to the _add-subtract_ definition is necessary to apply the same experimental setup as in Gillenwater et al. (2021). Our differentially private quantile estimation will apply to either and we will give the privacy guarantees if we instead use the _add-subtract_ definition in Appendix C.2.

**Definition 2.2**.: _A function \(f\,:\,\mathcal{X}\to\mathbb{R}\) has sensitivity \(\Delta\) if for any neighboring datasets \(|f(x)-f(x^{\prime})|\leq\Delta\)_

**Definition 2.3**.: _Dwork et al. (2006, 2006) A mechanism \(M\,:\,\mathcal{X}\to\mathcal{Y}\) is \((\epsilon,\delta)\)-differentially-private (DP) if for any neighboring datasets \(x,x^{\prime}\in\mathcal{X}\) and \(S\subseteq\mathcal{Y}\):_

\[\Pr[M(x)\in S]\leqslant e^{\prime}\Pr[M(x^{\prime})\in S]+\delta.\]

We will primarily work with pure differential privacy in this work where \(\delta=0\). We will also be considering the composition properties of the Sparse Vector Technique, and the primary method for comparison will be Concentrated Differential Privacy that has become widely used in practice due to it's tighter and simpler advanced composition properties Bun and Steinke (2016). This definition is instead based upon Reny divergence where for probability distributions \(P,Q\) over the same domain and \(\alpha>1\)

\[D_{\alpha}(P\|Q)=\frac{1}{\alpha-1}\ln\,\operatorname*{\mathbf{E}}_{z-P}\left[ \left(\frac{P(z)}{Q(z)}\right)^{\alpha-1}\right]\]

**Definition 2.4**.: _Bun and Steinke (2016) A mechanism \(M\,:\,\mathcal{X}\to\mathcal{Y}\) is \(\rho\)-zero-concentrated-differentially-private (zCDP) if for any neighboring datasets \(x,x^{\prime}\in\mathcal{X}\) and all \(\alpha\in(1,\infty)\):_

\[D_{\alpha}(M(x)\|M(x^{\prime}))\leq\rho\alpha.\]We can translate DP into zCDP in the following way.

**Proposition 1**.: _Bun and Steinke (2016) If \(M\) satisfies \(\varepsilon\)-DP then \(M\) satisfies \(\frac{1}{2}\varepsilon^{2}\)-zCDP_

In our examination of AboveThreshold we will add different types of noise, similar to the report noisy max algorithms Ding et al. (2021). Accordingly, we will consider noise from the Laplace, Gumbel and Exponential distributions where \(\mathsf{Lap}(b)\) has PDF \(p_{\mathsf{Lap}}(z;b)\), \(\mathsf{Gumbel}(b)\) has PDF \(p_{\mathsf{Gumbel}}(z;b)\), and \(\mathsf{Expo}(b)\) has PDF \(p_{\mathsf{Expo}}(z;b)\) where

\[p_{\mathsf{Lap}}(z;b) =\frac{1}{2b}\exp\left(-|z|/b\right) p_{\mathsf{Gumbel}}(z;b) =\frac{1}{b}\exp\left(-\left(z/b+e^{-z/b}\right)\right)\] \[p_{\mathsf{Expo}}(z;b) =\begin{cases}\frac{1}{b}\exp\left(-z/b\right)&z\geq 0\\ 0&z<0\end{cases}\]

We let \(\mathsf{Noise}(b)\) denote noise addition from any of \(\mathsf{Lap}(b)\), \(\mathsf{Gumbel}(b)\), or \(\mathsf{Expo}(b)\). We will also utilize the definition of the exponential mechanism to analyze the addition of \(\mathsf{Gumbel}\) noise.

**Definition 2.5**.: _McSherry and Talwar (2007)_ _The Exponential Mechanism is a randomized mapping \(M:\mathcal{X}\to\mathcal{Y}\) such that_

\[Pr\left[M(x)=y\right]\propto\exp\left(\frac{\varepsilon\cdot q(x,y)}{2\Delta}\right)\]

_where \(q:\mathcal{X}\times\mathcal{Y}\to\mathds{R}\) has sensitivity \(\Delta\)._

## 3 Improved Analysis for Sparse Vector Technique

In this section, we review the AboveThreshold algorithm from the literature. To our knowledge, this technique has only been used with Laplace noise in the literature. Unsurprisingly, we show that Gumbel and Exponential noise can also be applied, with the former allowing for a closed form expression of each output probability. We further show that for monotonic queries the privacy analysis of the Sparse Vector Technique, which iteratively applies AboveThreshold, can be improved. All proofs are be pushed to Appendix B where we also give a more general characterization of query properties that can improve the privacy bounds of AboveThreshold.

### Above Threshold Algorithm

We first provide the algorithm for AboveThreshold where noise can be applied from any of the Laplace, Gumbel or Exponential distributions.

```
0: Input dataset \(x\), a stream of queries \(\{f_{i}:\mathcal{X}\to\mathds{R}\}\) with sensitivity \(\Delta\), and a threshold \(T\)
1: Set \(\hat{T}=T+\mathsf{Noise}(\Delta/\varepsilon_{1})\)
2:for each query \(i\)do
3: Set \(v_{i}=\mathsf{Noise}(\Delta/\varepsilon_{2})\)
4:if\(f_{i}(x)+v_{i}\geq\hat{T}\)then
5: Output \(\top\) and halt
6:else
7: Output \(\bot\)
8:endif
9:endfor ```

**Algorithm 1**AboveThreshold

We will also define a common class of queries within the literature that is often seen to achieve a factor of 2 improvement in privacy bounds.

**Definition 3.1**.: _We say that stream of queries \(\{f_{i}:\mathcal{X}\to\mathds{R}\}\) with sensitivity \(\Delta\) is monotonic if for any neighboring \(x,x^{\prime}\in\mathcal{X}\) we have either \(f_{i}(x)\leq f_{i}(x^{\prime})\) for all \(i\) or \(f_{i}(x)\geq f_{i}(x^{\prime})\) for all \(i\)._To our knowledge all previous derivations of AboveThreshold in the literature apply Lap noise which gives the following privacy guarantees.

**Lemma 3.1** (Theorem 2 and 3 of Lyu et al. (2017)).: _If the noise addition is Lap then Algorithm 1 is \((\varepsilon_{1}+2\varepsilon_{2})\)-DP for general queries and is \((\varepsilon_{1}+\varepsilon_{2})\)-DP for monotonic queries._

Given that Expo noise is one-sided Lap noise, it can often be applied for comparative algorithms such as this one and report noisy max as well. We will show this extension in the appendix for completeness.

**Corollary 3.1**.: _If the noise addition is_ Expo _then Algorithm 1 is \((\varepsilon_{1}+2\varepsilon_{2})\)-DP for general queries and \((\varepsilon_{1}+\varepsilon_{2})\)-DP for monotonic queries._

While the proofs for Expo noise generally follow from the Lap noise proofs, it will require different techniques to show that Gumbel noise can be applied as well. In particular, we utilize the known connection between adding Gumbel noise and the exponential mechanism.

**Lemma 3.2**.: _If the noise addition is_ Gumbel _and \(\varepsilon_{1}=\varepsilon_{2}\) then Algorithm 1 is \((\varepsilon_{1}+2\varepsilon_{2})\)-DP for general queries and \((\varepsilon_{1}+\varepsilon_{2})\)-DP for monotonic queries._

We defer the proof of this to the appendix. In all of our empirical evaluations we will use Expo noise in our calls to AboveThreshold because it has the lowest variance for the same parameter. While we strongly believe that Expo noise will be most accurate under the same noise parameters, we leave a more rigorous examination to future work. We also note that this examination was implicitly done for report noisy max between Gumbel and Expo noise in McKenna and Sheldon (2020), where their algorithm is equivalent to adding Expo noise Ding et al. (2021), and Expo noise was shown to be clearly superior.

### Improved privacy analysis for Sparse Vector Technique

In this section, we further consider the iterative application of AboveThreshold which is known as the sparse vector technique. We show that for monotonic queries, we can improve the privacy analysis of sparse vector technique to obtain better utility for the same level of privacy. Our primary metric for measuring privacy through composition will be zCDP which we defined in Section 2 and has become commonly used particularly due to the composition properties. We further show in the appendix that our analysis also enjoys improvement under the standard definition of differential privacy. These improved properties immediately apply to our unbounded quantile estimation algorithm as our queries will be monotonic.

**Theorem 1**.: _If the queries are monotonic, then for any noise addition of Lap, Gumbel, or Expo we have that Algorithm 1 is \(\frac{1}{2}\varepsilon^{2}\)-zCDP where \(\varepsilon=\frac{\varepsilon_{1}}{2}+\varepsilon_{2}\). If the noise addition is_ Gumbel _then we further require \(\varepsilon_{1}=\varepsilon_{2}\)_

Note that applying Proposition 1 will instead give \(\varepsilon=\varepsilon_{1}+\varepsilon_{2}\). It will require further techniques to reduce this by \(\varepsilon_{1}/2\) which will immediately allow for better utility with the same privacy guarantees. This bound also follows the intuitive factor of 2 improvement that is often expected for monotonic queries. The analysis will be achieved through providing a range-bounded property, a definition that was introduced in Durfee and Rogers (2019). This definition is ideally suited to characterizing the privacy loss of selection algorithms, by which we can view AboveThreshold. As such it will also enjoy the improved composition bounds upon the standard differential privacy definition shown in Dong et al. (2020). This range bounded property was then unified with zCDP with improved privacy guarantees in Cesar and Rogers (2021).

We give a proof of this theorem along with further discussion in Appendix B.3. We will also give a generalized characterization of when we can take advantage of properties of the queries to tighten the privacy bounds in AboveThreshold. We will immediately utilize this characterization to improve the privacy guarantees for our method in Appendix C.

## 4 Unbounded Quantile Estimation

In this section we give our method for unbounded quantile estimation. We focus upon the lower bounded setting which we view as most applicable to real-world problems, where non-negativedata is incredibly common, particularly for datasets that contain information about individuals. This method can be symmetrically apply to upper bounded data, and in the appendix we will show how this approach can be extended to the fully unbounded setting.

For the quantile problem we will assume our data \(x\in\mathbb{R}^{n}\). Given quantile \(q\in[0,1]\) and dataset \(x\in\mathbb{R}^{n}\) the goal is to find \(t\in\mathbb{R}\) such that \(|\{x_{j}\in x|x_{j}<t\}|\) is as close to \(qn\) as possible.

### Unbounded quantile mechanim

The idea behind unbounded quantile estimation will be a simple guess-and-check method that will just invoke AboveThreshold. In particular, we will guess candidate values \(t\) such that \(|\{x_{j}\in x|x_{j}<t\}|\) is close to \(qn\). We begin with the smallest candidate, recall that we assume lower bounded data here and generalize later, and iteratively increase by a small percentage. At each iteration we check if \(|\{x_{j}\in x|x_{j}<t\}|\) has exceeded \(qn\), and terminate when it does, outputting the most recent candidate. In order to achieve this procedure privately, we will simply invoke AboveThreshold. We also discuss how this procedure can be achieved efficiently in Section 4.2.

Thus we give our algorithm here where the lower bound of the data, \(\ell\), is our starting candidate and \(\beta\) is the scale at which the threshold increases. Given that we want our candidate value to increase by a small percentage it must start as a positive value. As such we will essentially just shift the data such that the lower bound is instead 1. In all our experiments we set \(\beta=1.001\), so the increase is by \(0.1\%\) each iteration.

```
0: Input dataset \(x\), a quantile \(q\), a lower bound \(\ell\), and parameter \(\beta>1\)
1: Run AboveThreshold with \(x\), \(T=qn\) and \(f_{i}(x)=|\{x_{j}\in x|x_{j}-\ell+1<\beta^{i}\}|\)
2: Output \(\beta^{k}+\ell-1\) where \(k\) is the query that AboveThreshold halted at ```

**Algorithm 2** Unbounded quantile mechanism

Given that our method simply calls AboveThreshold it will enjoy all the privacy guarantees from Section 3.1. Furthermore we will show that our queries are monotonic.

**Lemma 4.1**.: _For any sequence of thresholds \(\{t_{i}\in\mathbb{R}\}\) let \(f_{i}(x)=|\{x_{j}\in x|x_{j}<t_{i}\}|\) for all \(i\). For any neighboring dataset under Definition 2.1, we have that \(\{f_{i}\}\) are monotonic queries with sensitivity 1._

Proof.: Let \(x_{j}\) be the value that differs between neighbors \(x,x^{\prime}\). Define \(S_{x,t}=\{x_{j}\in x|x_{j}<t\}\). We consider the case \(x^{\prime}_{j}>x_{j}\) and the other will follow symmetrically. For all thresholds \(t_{i}\in(-\infty,x_{j}]\) we have \(x_{j}\notin S_{x,t_{i}}\) and \(x^{\prime}_{j}\notin S_{x^{\prime},t_{i}}\), so \(f_{i}(x)=f_{i}(x^{\prime})\). For all thresholds \(t_{i}\in(x^{\prime}_{j},\infty)\) we have \(x_{j}\in S_{x,t_{i}}\) and \(x^{\prime}_{j}\in S_{x^{\prime},t_{i}}\), so \(f_{i}(x)=f_{i}(x^{\prime})\). Finally, for all thresholds \(t_{i}\in(x_{j},x^{\prime}_{j}]\) we have \(x_{j}\in S_{x,t_{i}}\) and \(x^{\prime}_{j}\notin S_{x^{\prime},t_{i}}\), so \(f_{i}(x)=f_{i}(x^{\prime})+1\). Therefore, \(f_{i}(x)\geq f_{i}(x^{\prime})\) for all \(i\), and the sensitivity is 1.

Note that for the _swap_ definition of neighboring the threshold remains constant. We will discuss how to extend our algorithm and further improve the privacy bounds for the _add-subtract_ definition of neighboring in the appendix.

### Simple and scalable implementation

In this section we show how our call to AboveThreshold can be done with a simple linear time pass through the data, and each subsequent query takes \(O(1)\) time. While the running time could potentially be infinite, if we set \(\beta=1.001\), then after 50,000 iterations our threshold is already over \(10^{21}\) and thus highly likely to have halted. Unless the scale of the data is absurdly high or the \(\beta\) value chosen converges to 1, our guess-and-check process will finish reasonably quickly. 2

Footnote 2: Note that the process can also be terminated at any time without affecting the privacy guarantees.

In our initial pass through the data, for each data point \(x_{j}\) we will find the index \(i\) such that \(\beta^{i}\leq x_{j}-\ell+1<\beta^{i+1}\), which can be done by simply computing \(\{\log_{\beta}(x_{j}-\ell+1)\}\) as our lower bound ensures \(x_{j}-\ell+1\geq 1\). Using a dictionary or similar data structure we can efficiently store \(|\{x_{j}\in x|\beta^{i}\leq x_{j}-\ell+1<\beta^{i+1}\}|\) for each \(i\) with the default being 0. This preprocessing does not require sorted data and takes \(O(n)\) arithmetic time, where we note that the previous algorithms also measure runtime arithmetically.

Finally, for each query if we already have \(|\{x_{j}\in x|x_{j}-\ell+1<\beta^{i}\}|\), then we can add \(|\{x_{j}\in x|\beta^{i}\leq x_{j}-\ell+1<\beta^{i+1}\}|\) in O(1) time to get \(|\{x_{j}\in x|x_{j}-\ell+1<\beta^{i+1}\}|\). Inductively, each query will take O(1) time. We provide the code in the appendix for easier reproducibility.

### Extension to multiple quantiles

The framework for computing multiple quantiles set up in Kaplan et al. (2022) is agnostic to the technique used for computing a single quantile. Their method will first compute the middle quantile and split the data according to the result. Through recursive application the number of levels of computation will be logarithmic. Furthermore, at each level we can see that at most one partition of the data will differ between neighbors, allowing for instead a logarithmic number of compositions. As such our approach can easily be applied to this recursive splitting framework to achieve the same improvements in composition. This will require some minor updating of their proofs to the _swap_ definition that we will do in the appendix.

## 5 Empirical Evaluation

In this section we empirically evaluate our approach compared to the previous approaches. We give further detail of the previous approaches, particularly EMQ, in A along with strong intuition upon why our approach will better handle maximal quantile estimation for data clipping. We first go over the datasets and settings used for our experiments which will follow recent related work Gillenwater et al. (2021); Kaplan et al. (2022). Next we evaluate how accurately our method estimates quantiles for the different datasets in the bounded setting. Finally, we will consider the application of computing differentially private sum, which also gives mean computation, and show how our algorithm allows for a significantly more robust and accurate method when tight bounds are not known for the dataset.

### Datasets

We borrow the same setup and datasets as Gillenwater et al. (2021); Kaplan et al. (2022). We test our algorithm compared to the state-of-the-art on six different datasets. Two datasets will be synthetic. One draws 10,000 data points from the uniform distribution in the range \([-5,5]\) and the other draws 10,000 data points from the normal distribution with mean zero and standard deviation of five. Two datasets will come from Soumik (2019) with 11,123 data points, where one has book ratings and the other has book page counts. Two datasets will come from Dua and Graf (2019) with 48,842 data points, where one has the number of hours worked per week and the other has the age for different people. We provide histograms of our datasets for better understanding in Figure 1.

### Quantile estimation experiments

For our quantile estimation experiments, for a given quantile \(q\in[0,1]\) we consider the error of outcome \(o_{q}\) from one of the private methods to be \(|o_{q}-t_{q}|\) where \(t_{q}\) is the true quantile value. We use the in-built quantile function in the numpy library with the default settings to get the true quantile value. As in previous related works, we randomly sample 1000 datapoints from each dataset and run the quantile computation on each method. This process is then iterated upon 100 times and the error is averaged. We set \(\varepsilon=1\) as in the previous works, which will require setting \(\varepsilon_{1}=\varepsilon_{2}=1/2\) for the call to AboveThreshold in our method.

We will also tighten the ranges to the following, \([-5,5]\) for the uniform dataset, \([-25,25]\) for the normal dataset, \([0,10]\) for the ratings dataset, \([0,1000]\) for the pages dataset, \([0,100]\) for the hours dataset, and \([0,100]\) for the ages dataset. Given that EMQ suffers performance when many datapoints are equal we add small independent noise to our non-synthetic datasets. This noise will be from the normal distribution with standard deviation \(0.001\) for the ratings dataset and \(0.1\) for the other three that have integer values. Our method does not require the noise addition but we will use the perturbed dataset for fair comparison. True quantiles are still computed upon the original data. For the datasets with integer values we rounded each output to the nearest integer. For our method we set \(\beta=1.001\) for all datasets.

For these experiments we only compare our method UQE and the previous method EMQ, using the implementations from Gillenwater et al. (2021). The other procedures, discussed in the appendix are more generalized and thus for this specific setting do not perform nearly as well which can be seen in the previous experiments Gillenwater et al. (2021); Kaplan et al. (2022), so we omit them from our results. For this experiment we consider estimating each quantile from 5% to 95% at a 1% interval. In Figure 2 we plot the mean absolute error of each normalized by the mean absolute error of UQE to make for an easier visualization.

As we can see in Figure 2, EMQ consistently performs better on synthetic data, and UQE tends to performs better on the non-synthetic data. This fits with our intuition that UQE will be best suited to situations where the data is unstructured and less is known about the dataset beforehand because our guess-and-check methodology is designed to better handle ill-behaving datasets.

Figure 1: Histograms for each of our datasets.

Figure 2: Plots of UQE = (mean absolute error UQE) / (mean absolute error UQE) and EMQ = (mean absolute error EMQ) / (mean absolute error UQE). Normalizing in this way will make for an easier visualization. When EMQ is below UQE then it’s error is lower, and when EMQ is above UQE then it’s error is higher

### Sum estimation experiments

As the primary application of our method we will also be considering differentially private sum computation, which can thereby compute mean as well. We will be using the following \(2\varepsilon\)-DP general procedure for computing the sum of non-negative data:

1. Let \(\Delta=\mathtt{PrivateQuantile}(x,q,\varepsilon)\) where \(\mathtt{PrivateQuantile}\) is any differentially private computation algorithm and \(q\approx 1\).
2. Output \(\mathtt{Lap}(\Delta/\varepsilon)+\sum_{j=1}^{n}\min(x_{j},\Delta)\)

We further test this upon the non-synthetic datasets. For large-scale privacy systems that provide statistical analysis for a wide variety of datasets, if we use a \(\mathtt{PrivateQuantile}\) that requires an upper bound then we ideally want this bound to be agnostic to the dataset. The is particularly true for sum computations upon groupby queries as the range and size can differ substantially amongst groups. As such, we fix the range at \([0,10000]\) to encompass all the datasets. We will otherwise use the same general setup as in Section 5.2.

We will measure the error of this procedure as the absolute error of the output and the true sum of the dataset. For each of the 100 iterations of choosing 1000 samples randomly from the full dataset, we also add \(\mathtt{Lap}\) noise 100 times. Averaging over all these iterations gives our mean absolute error.

We will run this procedure with \(\varepsilon\in\{0.1,0.5,1\}\). Further we will use \(q=0.99\) always for our method, but give the absolute error for the best performing \(q\in\{0.95,0.96,0.97,0.98,0.99\}\) for the other methods. It is important to note that this value would have to be chosen ahead of time, which would add more error to the other methods. The previous methods we consider here are again the EMQ, but also the aggregate tree (AT) methods, were we use both the implementation along with generally best performing height (3) and branching factor (10) from Gilfenwater et al. (2021). We also implemented the bounding technique using inner quartile range within Algorithm 1 of Smith (2011), but this performed notably worse than the others so we omitted the results from our table.

As we can see in Table 1, our method is far more robust and accurate. Furthermore, for our method the choice of \(q\) remained constant and we can see that our results still stayed consistently accurate when \(\varepsilon\) changed. Note that the noise added to the clipped sum is also scaled proportional to \(\varepsilon\) so the amount the error increased as \(\varepsilon\) decreased for our method is what would be expected proportionally. Once again these findings are consistent with our intuition. Our technique is more robust to differing datasets and privacy parameters, and especially better performing for this important use case.

Recall that the sampled data had size 1000 so dividing accordingly can give the error on mean estimates. There is a long line of literature on differentially private mean estimation.3 To our knowledge, all of these more complex algorithms either require assumptions upon the data distribution,

\begin{table}
\begin{tabular}{l l l l l l} \hline Privacy & Method & Ratings data & Pages data & Ages data & Work hours data \\ \hline \multirow{3}{*}{\(\varepsilon=1\)} & UQE & \(4.78_{0.21}\) & \(4385.23_{2077.16}\) & \(103.051_{60.04}\) & \(180.48_{44.92}\) \\  & EMQ & \(5.73_{0.54}\) & \(4324.38_{2343.25}\) & \(187.063_{35.55}\) & \(339.06_{75.82}\) \\  & AT & \(8.75_{0.31}\) & \(4377.45_{2340.93}\) & \(293.11_{117.32}\) & \(471.98_{174.07}\) \\ \hline \hline \multirow{3}{*}{\(\varepsilon=0.5\)} & UQE & \(9.22_{0.31}\) & \(7102.34_{3093.13}\) & \(180.61_{27.03}\) & \(277.89_{77.60}\) \\  & EMQ & \(6906.13_{7123.36}\) & \(7601.47_{3963.52}\) & \(678.22_{1931.11}\) & \(2131.80_{4669.11}\) \\  & AT & \(29.01_{115.04}\) & \(7491.97_{4367.31}\) & \(473.19_{238.19}\) & \(582.58_{369.29}\) \\ \hline \hline \multirow{3}{*}{\(\varepsilon=0.1\)} & UQE & \(44.59_{1.79}\) & \(21916.37_{6423.75}\) & \(821.77_{157.68}\) & \(981.10_{219.66}\) \\  & EMQ & \(45861.79_{28366.46}\) & \(46552.09_{27944.18}\) & \(50558.32_{28843.72}\) & \(47185.58_{29437.22}\) \\ \cline{1-1}  & AT & \(11351.77_{21423.40}\) & \(30830.49_{20422.05}\) & \(14490.06_{25268.71}\) & \(9928.18_{20159.17}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Mean absolute error for differentially private sum estimation. The standard deviation over the 100 iterations is also provided for each in the subscript. UQE = Our unbounded quantile estimation method. EMQ = The exponential mechanism based quantile estimation method. AT = The aggregate tree method for quantile estimation. For our method we only use \(q=0.99\). For the others we use the best performance for \(q\in\{0.95,0.96,0.97,0.98,0.99\}\).

such as sub-Gaussian or bounded moments, or bounds upon the data range or related parameters, and most often require both. These results also focus upon proving strong theoretical guarantees of accuracy with respect to asymptotic sample complexity. We first note that our approach will provide better initial bounds upon the data as seen in our experiments, which directly improve the theoretical guarantees in the results that require a data range. But also our focus here is upon practical methods that are agnostic to data distributions and more widely applicable to real-world data. Consequently, a rigorous comparison among all of these methods would be untenable and outside the scope of this work.

### Parameter tuning

We kept our \(\beta\) parameter fixed in all experiments for consistency but also to make our method data agnostic. However, our choice was aggressively small in order to achieve higher precision in the inner quantile estimation comparison in Section 5.2. This choice was still highly resilient to changes in \(\epsilon\) for our sum experiments as we see our error only scaled proportional to the increase in noise. But for more significant decreases in \(\epsilon\) or in the data size, i.e. the conditions under which all private algorithms suffer substantial accuracy loss, this choice of \(\beta\) could be too small. Those settings imply that the noise added is larger and the distance between queries and threshold shrinks, so our method is more likely to terminate earlier than desired. Smaller \(\beta\) values will then intensify this issue as the candidate values increase more slowly. For the clipping application, we generally think using a value of \(\beta=1.01\) would be a more stable choice. In fact, replicating our empirical testing with this value actually improves our results in Table 1. Furthermore, increasing \(\beta\) will also reduce the number of queries and thus the computational cost. In general, we find that setting \(\beta\in[1.01,1.001]\) gives good performance with \(\beta=1.01\) as a default for computational efficiency. We leave to future work a more thorough analysis of this parameter to fully optimize setting it with relation to data size and privacy parameter. Additionally, all our methods and proofs only require an increasing sequence of candidate values and it's possible that other potential sequences would be even more effective. For example, if tight upper and lower bounds are known on the data, such as within the recursive computation of multiple quantiles, then it likely makes more sense to simply uniformly partition the interval and check in increasing order. But we leave more consideration upon this to future work as well.

Our choice of \(q=0.99\) in the differentially private sum experiments was also to maintain consistency with the choices for the previous method but also to keep variance of the estimates lower. This will create some negative bias as we then expect to clip the data. As we can see in our illustrative example Figure 3, the PDF will exponentially decrease once we pass the true quantile value, but will do so less sharply once all the queries have value \(n\). Accordingly, setting \(q=1.0\) would add slightly more variance to the estimation but initial testing showed improvement in error. However, if the user would prefer slightly higher variance to avoid negative bias, then setting the threshold at \(n\) or even \(n+1/\epsilon\), would make it far more likely that the process terminates with a value slightly above the maximum. This is particularly useful for heavy-tailed data, where clipping at the 99th percentile can have an out-sized impact on the bias.

## Acknowledgments and Disclosure of Funding

We thank our colleagues Matthew Clegg, James Honaker, and Kenny Leftin for helpful discussions and feedback. We also thank anonymous reviewers for their helpful feedback.

## References

* Alabi et al. (2022) Alabi, D., Ben-Eliezer, O., and Chaturvedi, A. (2022). Bounded space differentially private quantiles. _arXiv preprint arXiv:2201.03380_.
* Barthe et al. (2016a) Barthe, G., Fong, N., Gaboardi, M., Gregoire, B., Hsu, J., and Strub, P.-Y. (2016a). Advanced probabilistic couplings for differential privacy. In _Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security_, pages 55-67.
* Barthe et al. (2016b) Barthe, G., Gaboardi, M., Gregoire, B., Hsu, J., and Strub, P.-Y. (2016b). Proving differential privacy via probabilistic couplings. In _Proceedings of the 31st Annual ACM/IEEE Symposium on Logic in Computer Science_, pages 749-758.
* Barthe et al. (2016)Bassily, R., Thakkar, O., and Guha Thakurta, A. (2018). Model-agnostic private learning. _Advances in Neural Information Processing Systems_, 31.
* Blum et al. (2015) Blum, A., Morgenstern, J., Sharma, A., and Smith, A. (2015). Privacy-preserving public information for sequential games. In _Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science_, pages 173-180.
* Bun et al. (2015) Bun, M., Nissim, K., Stemmer, U., and Vadhan, S. (2015). Differentially private release and learning of threshold functions. In _2015 IEEE 56th Annual Symposium on Foundations of Computer Science_, pages 634-649. IEEE.
* Bun and Steinke (2016) Bun, M. and Steinke, T. (2016). Concentrated differential privacy: Simplifications, extensions, and lower bounds. In _Theory of Cryptography: 14th International Conference, TCC 2016-B, Beijing, China, October 31-November 3, 2016, Proceedings, Part I_, pages 635-658. Springer.
* Bun et al. (2017) Bun, M., Steinke, T., and Ullman, J. (2017). Make up your mind: The price of online queries in differential privacy. In _Proceedings of the twenty-eighth annual ACM-SIAM symposium on discrete algorithms_, pages 1306-1325. SIAM.
* Cesar and Rogers (2021) Cesar, M. and Rogers, R. (2021). Bounding, concentrating, and truncating: Unifying privacy loss composition for data analytics. In _Algorithmic Learning Theory_, pages 421-457. PMLR.
* Chan et al. (2011) Chan, T.-H. H., Shi, E., and Song, D. (2011). Private and continual release of statistics. _ACM Transactions on Information and System Security (TISSEC)_, 14(3):1-24.
* Chen et al. (2016) Chen, Y., Machanavajjhala, A., Reiter, J. P., and Barrientos, A. F. (2016). Differentially private regression diagnostics. In _ICDM_, pages 81-90.
* Cummings et al. (2015) Cummings, R., Kearns, M., Roth, A., and Wu, Z. S. (2015). Privacy and truthful equilibrium selection for aggregative games. In _Web and Internet Economics: 11th International Conference, WINE 2015, Amsterdam, The Netherlands, December 9-12, 2015, Proceedings 11_, pages 286-299. Springer.
* Cummings et al. (2020) Cummings, R., Krehbiel, S., Lut, Y., and Zhang, W. (2020). Privately detecting changes in unknown distributions. In _International Conference on Machine Learning_, pages 2227-2237. PMLR.
* Ding et al. (2021) Ding, Z., Kifer, D., Steinke, T., Wang, Y., Xiao, Y., Zhang, D., et al. (2021). The permute-and-flip mechanism is identical to report-noisy-max with exponential noise. _arXiv preprint arXiv:2105.07260_.
* Ding et al. (2023) Ding, Z., Wang, Y., Xiao, Y., Wang, G., Zhang, D., and Kifer, D. (2023). Free gap estimates from the exponential mechanism, sparse vector, noisy max and related algorithms. _The VLDB Journal_, 32(1):23-48.
* Dong et al. (2020) Dong, J., Durfee, D., and Rogers, R. (2020). Optimal differential privacy composition for exponential mechanisms. In _International Conference on Machine Learning_, pages 2597-2606. PMLR.
* Dua and Graf (2019) Dua, D. and Graf, C. (2019). Uci machine learning repository. In _http://archive.ics.uci.edu/ ml_.
* Durfee and Rogers (2019) Durfee, D. and Rogers, R. M. (2019). Practical differentially private top-k selection with pay-what-you-get composition. _Advances in Neural Information Processing Systems_, 32.
* Dwork et al. (2015) Dwork, C., Feldman, V., Hardt, M., Pitassi, T., Reingold, O., and Roth, A. L. (2015). Preserving statistical validity in adaptive data analysis. In _Proceedings of the forty-seventh annual ACM symposium on Theory of computing_, pages 117-126.
* Dwork et al. (2006a) Dwork, C., Kenthapadi, K., McSherry, F., Mironov, I., and Naor, M. (2006a). Our data, ourselves: Privacy via distributed noise generation. In _Advances in Cryptology-EUROCRYPT 2006: 24th Annual International Conference on the Theory and Applications of Cryptographic Techniques, St. Petersburg, Russia, May 28-June 1, 2006. Proceedings 25_, pages 486-503. Springer.
* Dwork et al. (2006b) Dwork, C., McSherry, F., Nissim, K., and Smith, A. (2006b). Calibrating noise to sensitivity in private data analysis. In _Theory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3_, pages 265-284. Springer.
* Dwork et al. (2015)Dwork, C., Naor, M., Pitassi, T., and Rothblum, G. N. (2010). Differential privacy under continual observation. In _Proceedings of the forty-second ACM symposium on Theory of computing_, pages 715-724.
* Dwork et al. (2009) Dwork, C., Naor, M., Reingold, O., Rothblum, G. N., and Vadhan, S. (2009). On the complexity of differentially private data release: efficient algorithms and hardness results. In _Proceedings of the forty-first annual ACM symposium on Theory of computing_, pages 381-390.
* Feldman and Steinke (2017) Feldman, V. and Steinke, T. (2017). Generalization for adaptively-chosen estimators via stable median. In _Conference on learning theory_, pages 728-757. PMLR.
* Gillenwater et al. (2021) Gillenwater, J., Joseph, M., and Kulesza, A. (2021). Differentially private quantiles. In _International Conference on Machine Learning_, pages 3713-3722. PMLR.
* Hardt and Rothblum (2010) Hardt, M. and Rothblum, G. N. (2010). A multiplicative weights mechanism for privacy-preserving data analysis. In _2010 IEEE 51st annual symposium on foundations of computer science_, pages 61-70. IEEE.
* Hasidim et al. (2020) Hasidim, A., Kaplan, H., Mansour, Y., Matias, Y., and Stemmer, U. (2020). Adversarially robust streaming algorithms via differential privacy. _Advances in Neural Information Processing Systems_, 33:147-158.
* Hsu et al. (2013) Hsu, J., Roth, A., and Ullman, J. (2013). Differential privacy for the analyst via private equilibrium computation. In _Proceedings of the forty-fifth annual ACM symposium on Theory of computing_, pages 341-350.
* Kamath et al. (2022) Kamath, G., Mouzakis, A., Singhal, V., Steinke, T., and Ullman, J. (2022). A private and computationally-efficient estimator for unbounded gaussians. In _Conference on Learning Theory_, pages 544-572. PMLR.
* Kaplan et al. (2020a) Kaplan, H., Ligett, K., Mansour, Y., Naor, M., and Stemmer, U. (2020a). Privately learning thresholds: Closing the exponential gap. In _Conference on Learning Theory_, pages 2263-2285. PMLR.
* Kaplan et al. (2021) Kaplan, H., Mansour, Y., and Stemmer, U. (2021). The sparse vector technique, revisited. In _Conference on Learning Theory_, pages 2747-2776. PMLR.
* Kaplan et al. (2022) Kaplan, H., Schnapp, S., and Stemmer, U. (2022). Differentially private approximate quantiles. In _International Conference on Machine Learning_, pages 10751-10761. PMLR.
* Kaplan et al. (2020b) Kaplan, H., Sharir, M., and Stemmer, U. (2020b). How to find a point in the convex hull privately. _SoCG_.
* Ligett et al. (2017) Ligett, K., Neel, S., Roth, A., Waggoner, B., and Wu, S. Z. (2017). Accuracy first: Selecting a differential privacy level for accuracy constrained err. _Advances in Neural Information Processing Systems_, 30.
* Lyu et al. (2017) Lyu, M., Su, D., and Li, N. (2017). Understanding the sparse vector technique for differential privacy. _Proceedings of the VLDB Endowment_, 10(6).
* McKenna and Sheldon (2020) McKenna, R. and Sheldon, D. R. (2020). Permute-and-flip: A new mechanism for differentially private selection. _Advances in Neural Information Processing Systems_, 33:193-203.
* McSherry and Talwar (2007) McSherry, F. and Talwar, K. (2007). Mechanism design via differential privacy. In _48th Annual IEEE Symposium on Foundations of Computer Science (FOCS'07)_, pages 94-103. IEEE.
* Nandi and Bassily (2020) Nandi, A. and Bassily, R. (2020). Privately answering classification queries in the agnostic pac model. In _Algorithmic Learning Theory_, pages 687-703. PMLR.
* Nissim et al. (2007) Nissim, K., Raskhodnikova, S., and Smith, A. (2007). Smooth sensitivity and sampling in private data analysis. In _Proceedings of the thirty-ninth annual ACM symposium on Theory of computing_, pages 75-84.
* Nissim and Stemmer (2018) Nissim, K. and Stemmer, U. (2018). Clustering algorithms for the centralized and local models. In _Algorithmic Learning Theory_, pages 619-653. PMLR.
* Nissim et al. (2017)Nissim, K., Stemmer, U., and Vadhan, S. (2016). Locating a small cluster privately. In _Proceedings of the 35th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems_, pages 413-427.
* Roth and Roughgarden (2010) Roth, A. and Roughgarden, T. (2010). Interactive privacy via the median mechanism. In _Proceedings of the forty-second ACM symposium on Theory of computing_, pages 765-774.
* Sajed and Sheffet (2019) Sajed, T. and Sheffet, O. (2019). An optimal private stochastic-mab algorithm based on optimal private stopping rule. In _International Conference on Machine Learning_, pages 5579-5588. PMLR.
* Shokri and Shmatikov (2015) Shokri, R. and Shmatikov, V. (2015). Privacy-preserving deep learning. In _Proceedings of the 22nd ACM SIGSAC conference on computer and communications security_, pages 1310-1321.
* Smith (2011) Smith, A. (2011). Privacy-preserving statistical estimation with optimal convergence rates. In _Proceedings of the forty-third annual ACM symposium on Theory of computing_, pages 813-822.
* Soumik (2019) Soumik (2019). Goodreads-books dataset. In _https://www.kaggle.com/jealousleopard/ goodreadsbooks_.
* Steinke and Ullman (2016) Steinke, T. and Ullman, J. (2016). Between pure and approximate differential privacy. _Journal of Privacy and Confidentiality_.
* Ullman (2015) Ullman, J. (2015). Private multiplicative weights beyond linear queries. In _Proceedings of the 34th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems_, pages 303-312.

Maximal Quantiles Estimation

In this section, we specifically consider applying our unbounded quantile estimation to computing the highest quantiles. This is commonly needed for clipping the dataset to privately compute the sum and mean, which we empirically test in Section 5.3. Our primary goal here is to build intuition upon why our approach fundamentally gives more accurate and robust estimations of the highest quantiles when only loose upper bounds on the data are known. We first give more details upon the previous methods, particularly the EMQ method. Then we give a more detailed look at how these methods are negatively affected by loose upper bounds and why ours performs well in comparison.

### Previous techniques

The most effective and practical previous method for quantile estimation, EMQ, builds a distribution over an assumed bounded range \([a,b]\) through an invocation of the exponential mechanism. Assuming the data is sorted, it will partition the range based upon the data and select an interval \([x_{j},x_{j+1}]\) with probability proportional to

\[\exp\left(-\frac{\varepsilon|j-qn|}{2}\right)\left(x_{j+1}-x_{j}\right)\]

where the intervals \([a,x_{1}]\) and \([x_{n},b]\) are also considered. A uniformly random point is then drawn from within the selected interval. Note that this utility function is not monotonic and will not enjoy the same improved privacy bounds.

There are other approaches that recursively partition the data range while computing statistics on each partition, then aggregate these statistics across logarithmic levels to reduce the noise for quantile computation Dwork et al. (2010); Chan et al. (2011). Another technique is to instead utilize the local sensitivity while maintaining privacy by using a smoothing procedure that can also be used for computing quantiles Nissim et al. (2007). There is also a similar approach to ours within Chen et al. (2016) for bounding an unbounded dataset that increases the bounds by a factor of 2 each iteration but we will discuss in Section A.2 why this variant performs substantially worse.

### Effect of bounds upon maximum quantiles

As mentioned, EMQ will construct a probability distribution over the range \([a,b]\). The corresponding PDF is a unimodal step function of the intervals defined by the data with the peak interval \([x_{j},x_{j+1}]\) being such that \(j\) closest to \(qn\). Note then that if \(b>>x_{n}\) then the probability of selecting \([x_{n},b]\) increases dramatically. The exponential decay as the PDF moves away from the peak interval diminishes the impact of \([x_{n},b]\) significantly if \(n\) is far away from \(qn\). However, for computing the highest quantiles, we want \(q\) close to 1 by definition, and the looseness of the upper bound drastically effects the accuracy.

In contrast, as soon as the candidate value for our method exceeds the true quantile value, each successive query will have an output of at least \(qn\) which is the threshold. The probability of continuing for \(\lambda\) more queries then decreases in \(\lambda\). 4 For ease of comparison, we can modify our Algorithm 2 such that in the last step, the output is drawn uniformly from \([\beta^{k-1},\beta^{k}]\), assuming \(\ell=1\) for simplicity. This would then create a continuous probability distribution that would similarly be a step function with each interval \([\beta^{k-1},\beta^{k}]\) being a step.

Footnote 4: We expect \(\lambda\) to most likely be small and this only adds a factor of \(\beta^{1}\) additional error. Note that setting \(\beta\) too high, such as \(\beta=2\) which gives a similar algorithm to Chen et al. (2016), implies that even taking five additional queries will lead to a value that is over 32 times too large.

For better intuition we plot the approximate PDF of each in Figure 3. We use the Gumbel noise in the call to AboveThreshold to take advantage of the closed form expression in Lemma B.2 for easier PDF computation.

As we can see in Figure 3, the upper bound, \(b\), increasing from 10 to 20 dramatically increases the probability of selecting a point in \([x_{n},b]\) which changes the normalization for the other intervals, significantly altering the PDF of EMQ. Given that our method is unbounded, the PDF for UQE stays the same in both figures and sees the expected exponential decay once the candidate quantile passes the true quantile.

There are also alternative methods for bounding the data range by computing the interquartile range more accurately and scaling up. However these methods make strong assumptions upon the data distribution being close to the normal distribution as well as bounds upon the third moment Smith (2011). For real-world datasets, the tails of the data can vary significantly, and scaling up in a data-agnostic manner can often be similarly inaccurate. We also found this to be true in our experiments.

The smooth sensitivity framework will run into similar issues when \(q\) is closer to \(1\) because fewer data points need to be changed to push the quantile value to the upper bound. If this upper bound is large, then the smooth local sensitivity will still be substantial. Aggregate tree methods are slightly more robust to loose upper bounds as they aren't partitioning specific to the data. However, for accurate estimates the partitioning of the range still requires the data be well-distributed across the evenly split partitions, which is significantly affected by loose upper bounds.

## Appendix B Improved analysis for Sparse Vector Technique

In this section we complete the analysis for improving the privacy bounds for sparse vector technique with monotonic queries. We will first give a more generalized characterization of query properties by which we can improve the privacy bounds for AboveThreshold. While this characterization is more complex, we will immediately utilize it in Section C to improve the privacy bounds of our methods under an alternate common definition of neighboring. Additionally, the class of queries that we apply it to are not monotonic, so improvements can be made in a multitude of settings. Next we will review the definition of range-bounded, a property of privacy mechanisms that can be used to improve composition bounds, and apply it to our setting. Finally we will utilize these results to show that the privacy analysis of the sparse vector technique can be improved more generally, but also specifically for monotonic queries.

### Generalized characterization

We provide a more general characterization for the privacy loss of AboveThreshold that is specific to the input stream of queries. This will be achieved by providing a one-sided privacy parameter for each pair of neighboring datasets where order matters. For a given pair \(x,x^{\prime}\), our goal will be to upper bound the output distribution of \(x\) by the output distribution of \(x^{\prime}\) up to an exponential factor of the one-sided privacy parameter. The specificity of the parameter to the neighboring datasets will reduce conciseness but it is for this reason that we will be able to further improve privacy analysis. This precise characterization will also help improve bounds for our method in Section C without requiring onerous analysis.

Figure 3: Illustrative example of how the approximate PDF of the previous method, EMQ is affected by looser upper bounds compared to our unbounded method UQE. A small amount of data was drawn uniformly from \([0,10]\) and we set \(q=0.9\), so accurate output would be about \(9\). The left and right side assumed a range of \([0,20]\) and \([0,10]\) respectively for EMQ.

**Definition B.1**.: _For a stream of queries \(\{f_{i}\}\) with sensitivity \(\Delta\), we define our one-sided privacy loss for neighboring data sets \(x,x^{\prime}\) as_

\[\varepsilon(x,x^{\prime})=\max_{k}\left(\frac{\varepsilon_{1}}{\Delta}\Delta_{k }(x,x^{\prime})+\frac{\varepsilon_{2}}{\Delta}\max\{0,\Delta_{k}(x,x^{\prime}) -(f_{k}(x^{\prime})-f_{k}(x))\}\right)\]

_where \(\Delta_{k}(x,x^{\prime})=\max_{i<k}\max\{0,f_{i}(x^{\prime})-f_{i}(x)\}\)._

Given that our definition is meant to encompass the one-sided privacy loss for AboveThreshold we will now prove that fact for Expo and Gumbel noise (and Lap follows equivalently to Expo). We first prove this conjecture for Expo noise.

**Lemma B.1**.: _For a stream of queries \(\{f_{i}\}\) with sensitivity \(\Delta\), threshold \(T\) and neighboring data sets \(x,x^{\prime}\), if we run Algorithm 1 with Expo noise, then for any given outcome \(\{\bot^{k-1},\top\}\) we have_

\[\Pr\left[\mathsf{AboveThreshold}(x,\{f_{i}\},T)=\{\bot^{k-1}, \top\}\right]\leq\\ \exp(\varepsilon(x,x^{\prime}))Pr\left[\mathsf{AboveThreshold}(x ^{\prime},\{f_{i}\},T)=\{\bot^{k-1},\top\}\right]\]

Proof.: Let \(v_{i}\sim\mathsf{Expo}(\Delta/\varepsilon_{2})\) denote the noise drawn for query \(f_{i}\), and let \(v\sim\mathsf{Expo}(\Delta/\varepsilon_{1})\) denote the noise drawn for the threshold. Further let \(v_{i\leq k}\) denote all \(v_{i}\) such that \(i\leq k\). By construction, we have

\[\Pr\left[\mathsf{AboveThreshold}(x,\{f_{i}\},T)=\{\bot^{k-1},\top\}\right]= \Pr_{v_{i}v_{i+k}}\left[\max_{i<k}f(x)+v_{i}<T+v<f_{k}(x)+v_{k}\right]\]

We will fix the randomness of \(v_{i<k}\), denoting \(v_{i}\) such that \(i<k\), for datasets \(x\) and \(x^{\prime}\), and let \(\tau=\max_{i<k}f(x)+v_{i}\) and \(\tau^{\prime}=\max_{i<k}f(x^{\prime})+v_{i}\). It suffices then to show that for any fixed randomness \(v_{i<k}\) we have

\[\Pr_{v,v_{k}}\left[\tau<T+v<f_{k}(x)+v_{k}\right]\leq\exp(\varepsilon(x,x^{ \prime}))\Pr_{v,v_{k}}\left[\tau^{\prime}<T+v<f_{k}(x^{\prime})+v_{k}\right]\]

Let \(\Delta_{1}=\max\{0,\tau^{\prime}-\tau\}\) and \(\Delta_{2}=\max\{0,\Delta_{1}-(f_{k}(x^{\prime})-f_{k}(x))\}\) and let \(v^{\prime}=v+\Delta_{1}\) and \(v^{\prime}_{k}=v_{k}+\Delta_{2}\). It suffices to show that for every pair of draws \(v,v_{k}\) that satisfies \(\tau<T+v<f_{k}(x)+v_{k}\), then the corresponding \(v^{\prime},v^{\prime}_{k}\) are i) non-negative, ii) satisfy \(\tau^{\prime}<T+v^{\prime}<f_{k}(x^{\prime})+v^{\prime}_{k}\) and iii)

\[p_{\mathsf{Expo}}(v;\Delta/\varepsilon_{1})\cdot p_{\mathsf{Expo}}(v_{k}; \Delta/\varepsilon_{2})\leq\exp(\varepsilon(x,x^{\prime}))p_{\mathsf{Expo}}(v ^{\prime};\Delta/\varepsilon_{1})\cdot p_{\mathsf{Expo}}(v^{\prime}_{k}; \Delta/\varepsilon_{2})\]

We note that condition iii) does not require any scalar on the RHS as our change of variable is a shift for each so the Jacobian of the mapping is the identity matrix with determinant of \(1\).

Condition i) follows from the fact that \(v,v_{k}\) are non-negative because they're drawn from the exponential distribution, and \(\Delta_{1},\Delta_{2}\) are non-negative by construction.

For condition ii), if \(\tau<T+v\) we must have \(\tau^{\prime}<T+v+\Delta_{1}\) by definition of \(\Delta_{1}\). Similarly, if \(T+v<f_{k}(x)+v_{k}\) then \(T+v+\Delta_{1}<f_{k}(x^{\prime})+v_{k}+\Delta_{2}\) because \(\Delta_{2}\geq\Delta_{1}-(f_{k}(x^{\prime})-f_{k}(x))\).

For condition iii), the PDF of the exponential distribution implies \(p_{\mathsf{Expo}}(v;\Delta/\varepsilon_{1})\leq\exp(\frac{\varepsilon_{1} \Delta_{1}}{\Delta})p_{\mathsf{Expo}}(v^{\prime};\Delta/\varepsilon_{1})\) and \(p_{\mathsf{Expo}}(v_{k};\Delta/\varepsilon_{2})\leq\exp(\frac{\varepsilon_{2} \Delta_{1}}{\Delta})p_{\mathsf{Expo}}(v^{\prime}_{k};\Delta/\varepsilon_{2})\). Due to the fixing of randomness of \(v_{i<k}\), we have that \(\tau^{\prime}-\tau\leq\max_{i<k}(f_{i}(x^{\prime})-f_{i}(x))\) so \(\Delta_{1}\leq\Delta_{k}(x,x^{\prime})\) from Definition B.1 and \(\Delta_{2}\leq\max\{0,\Delta_{k}(x,x^{\prime})-(f_{k}(x^{\prime})-f_{k}(x))\}\) which implies condition iii). 

This proof then easily applies to Lap as well. The proof for Gumbel will differ though as we no longer have similar closeness of PDF properties, but it will follow from the fact that we can give a closed form expression for each probability.

**Lemma B.2**.: _Given a dataset \(x\), a stream of queries \(\{f_{i}\}\) and threshold \(T\), for any outcome \(\{\bot^{k-1},\top\}\) from running AboveThreshold with Gumbel noise and \(\varepsilon=\varepsilon_{1}=\varepsilon_{2}\), then_\[\begin{split} Pr\left[\text{AboveThreshold}(x,\{f_{\!i}\},T)=\{\bot^{k-1 },\top\}\right]&=\\ &\frac{\exp(\frac{\varepsilon}{\Delta}f_{\!i}(x))}{\exp(\frac{ \varepsilon}{\Delta}T)+\sum_{i=1}^{k}\exp(\frac{\varepsilon}{\Delta}f_{\!i}( x))}\cdot\frac{\exp(\frac{\varepsilon}{\Delta}T)}{\exp(\frac{\varepsilon}{\Delta}T)+\sum_{i=1}^{k-1 }\exp(\frac{\varepsilon}{\Delta}f_{\!i}(x))}\end{split}\]

Proof.: We add noise \(\mathsf{Gumbel}(\Delta/\varepsilon)\) to all \(f_{\!i}(x)\) and the threshold \(T\), and in order to output \(\{\bot^{k-1},\top\}\) we must have that for the first \(k\) queries, the noisy value of \(f_{\!k}(x)\) is the largest and the noisy threshold is the second largest. It is folklore in the literature that adding \(\mathsf{Gumbel}\) noise (with the same noise parameter) and choosing the largest index is equivalent to the exponential mechanism. This can also be extended to showing that adding \(\mathsf{Gumbel}\) noise and choosing the top-k indices in order is equivalent to the peeling exponential mechanism. The peeling exponential mechanism first selects the top index using the exponential mechanism and removes it from the candidate set, repeating iteratively until accumulating the top-k indices. A formal proof of this folklore result is also provided in Lemma 4.2 of Durfee and Rogers (2019). Applying this result and the definition of the exponential mechanism gives the desired equality.

We then utilize this closed form expression to get our desired one-sided privacy bounds when using Gumbel noise.

**Lemma B.3**.: _For a stream of queries \(\{f_{\!i}\}\) with sensitivity \(\Delta\), threshold \(T\) and neighboring data sets \(x,x^{\prime}\), if we run Algorithm 1 with \(\mathsf{Gumbel}\) noise with \(\varepsilon_{1}=\varepsilon_{2}\), then for any outcome \(\{\bot^{k-1},\top\}\) we have_

\[\begin{split} Pr\left[\text{AboveThreshold}(x,\{f_{\!i}\},T)=\{ \bot^{k-1},\top\}\right]&\leq\\ &\exp(\varepsilon(x,x^{\prime}))Pr\left[\text{AboveThreshold}(x^{ \prime},\{f_{\!i}\},T)=\{\bot^{k-1},\top\}\right]\end{split}\]

Proof.: Let \(\varepsilon=\varepsilon_{1}=\varepsilon_{2}\). From Lemma B.2 we know that

\[\begin{split} Pr\left[\text{AboveThreshold}(x,\{f_{\!i}\},T)=\{ \bot^{k-1},\top\}\right]&=\\ &\frac{\exp(\frac{\varepsilon}{\Delta}f_{\!i}(x))}{\exp(\frac{ \varepsilon}{\Delta}T)+\sum_{i=1}^{k}\exp(\frac{\varepsilon}{\Delta}f_{\!i}( x))}\cdot\frac{\exp(\frac{\varepsilon}{\Delta}T)}{\exp(\frac{\varepsilon}{\Delta}T)+ \sum_{i=1}^{k-1}\exp(\frac{\varepsilon}{\Delta}f_{\!i}(x))}\end{split}\]

Similar to the proof of Lemma B.1, we let \(\Delta_{1}=\max\{0,\max_{i<k}(f_{\!i}(x^{\prime})-f_{\!i}(x))\}\) and \(\Delta_{2}=\max\{0,\Delta_{1}+(f_{\!k}(x)-f_{\!k}(x^{\prime}))\}\). We first want to show that

\[\frac{\exp(\frac{\varepsilon}{\Delta}T)}{\exp(\frac{\varepsilon}{\Delta}T)+ \sum_{i=1}^{k-1}\exp(\frac{\varepsilon}{\Delta}f_{\!i}(x))}\leq e^{\frac{ \varepsilon\Delta_{1}}{\Delta}}\frac{\exp(\frac{\varepsilon}{\Delta}T)}{\exp( \frac{\varepsilon}{\Delta}T)+\sum_{i=1}^{k-1}\exp(\frac{\varepsilon}{\Delta}f_{ \!i}(x^{\prime}))}\] (1)

This inequality reduces to showing \(\exp(\frac{\varepsilon}{\Delta}T)+\sum_{i=1}^{k-1}\exp(\frac{\varepsilon}{ \Delta_{1}}f_{\!i}(x^{\prime}))\leq e^{\frac{\varepsilon\Delta_{1}}{\Delta}}( \exp(\frac{\varepsilon}{\Delta}T)+\sum_{i=1}^{k-1}\exp(\frac{\varepsilon}{ \Delta}f_{\!i}(x)))\). This holds from the fact that \(\exp(\frac{\varepsilon}{\Delta}T)\leq\exp(\varepsilon\Delta_{1}/\Delta)\exp( \frac{\varepsilon}{\Delta}T)\) because \(\varepsilon\Delta_{1}/\Delta\geq 0\) and \(\exp(\frac{\varepsilon}{\Delta}f_{\!i}(x^{\prime}))\leq\exp(\varepsilon\Delta_{1} /\Delta)\exp(\frac{\varepsilon}{\Delta_{1}}f_{\!i}(x))\) for all \(i<k\) by our definition of \(\Delta_{1}\).

\[\frac{\exp(\frac{\varepsilon}{\Delta}f_{\!i}(x))}{\exp(\frac{\varepsilon}{\Delta }T)+\sum_{i=1}^{k}\exp(\frac{\varepsilon}{\Delta}f_{\!i}(x))}\leq e^{\frac{ \varepsilon\Delta_{1}}{\Delta}}\frac{\exp(\frac{\varepsilon}{\Delta}f_{\!i}(x^{ \prime}))}{\exp(\frac{\varepsilon}{\Delta}T)+\sum_{i=1}^{k}\exp(\frac{ \varepsilon}{\Delta}f_{\!i}(x^{\prime}))}\] (2)

First we let \(\Delta^{\prime}_{1}=\max\{0,\max_{\ell\leq k}(f_{\!i}(x^{\prime})-f_{\!i}(x))\}\) and \(\Delta^{\prime}_{2}=f_{\!k}(x)-f_{\!k}(x^{\prime})\), and we will achieve our desired inequality (2) by bounding the numerator with respect to \(\Delta^{\prime}_{2}\) and the denominator with respect to \(\Delta^{\prime}_{1}\). We have \(\exp(\frac{\varepsilon}{\Delta}f_{\!k}(x))=\exp(\varepsilon\Delta^{\prime}_{2}/ \Delta)\exp(\frac{\varepsilon}{\Delta}f_{\!k}(x^{\prime}))\) by definition of \(\Delta^{\prime}_{2}\). Further \(\exp(\frac{\varepsilon}{\Delta}f_{\!i}(x^{\prime}))\leq\exp(\varepsilon\Delta^{ \prime}_{1}/\Delta)\exp(\frac{\varepsilon}{\Delta}f_{\!i}(x))\) by definition of \(\Delta^{\prime}_{1}\), so

\[\exp(\frac{\varepsilon}{\Delta}T)+\sum_{i=1}^{k}\exp(\frac{\varepsilon}{\Delta}f_{ \!i}(x^{\prime}))\leq e^{\frac{\varepsilon\Delta^{\prime}_{1}}{\Delta}}(\exp( \frac{\varepsilon}{\Delta}T)+\sum_{i=1}^{k}\exp(\frac{\varepsilon}{\Delta}f_{\!i}( x)))\]because \(\epsilon\Delta_{1}^{\prime}/\Delta\geq 0\). In order to show our desired inequality (2) it then suffices to show \(\Delta_{2}\geq\Delta_{1}^{\prime}+\Delta_{2}^{\prime}\). By definition we know \(\Delta_{2}\geq\Delta_{1}+\Delta_{2}^{\prime}\). Furthermore, if \(\Delta_{1}^{\prime}>\Delta_{1}\) then \(\Delta_{1}^{\prime}=f_{\!k}(x^{\prime})-f_{\!k}(x)=-\Delta_{2}^{\prime}\) which implies \(\Delta_{1}^{\prime}+\Delta_{2}^{\prime}=0\), and we know \(\Delta_{2}\geq 0\) by construction.

Combining inequality (1) and (2) along with the fact that \(\Delta_{1}=\Delta_{k}(x,x^{\prime})\) from Definition B.1 completes the proof.

Now that we've bounded the one-sided privacy loss for neighboring datasets for each type of noise, this will immediately imply an overall privacy bound over all neighbors that utilizes this general characterization.

**Corollary B.1**.: _For a stream of queries \(\{f_{!}\}\) with sensitivity \(\Delta\) and threshold \(T\), Algorithm 1 with \(\mathsf{Lap}\), \(\mathsf{Gumbel}\), or \(\mathsf{Expo}\) noise is \(\epsilon\)-DP where_

\[\epsilon=\max_{x\sim x^{\prime}}\epsilon(x,x^{\prime})\]

_and for \(\mathsf{Gumbel}\) noise we must have \(\epsilon_{1}=\epsilon_{2}\)._

### Range-bounded definition and properties

The definition of _range-bounded_ was originally introduced in Durfee and Rogers (2019) to tighten the privacy bounds on the composition of exponential mechanisms. The analysis was further extended in Dong et al. (2020) to give the optimal composition of range-bounded mechanisms. This definition was then unified with other definitions in Cesar and Rogers (2021).

**Definition B.2** (Durfee and Rogers (2019)).: _A mechanism \(M:\mathcal{X}\to\mathcal{Y}\) is \(\epsilon\)-range-bounded if for any neighboring datasets \(x,x^{\prime}\in\mathcal{X}\) and outcomes \(y,y^{\prime}\in\mathcal{Y}\):_

\[\frac{Pr\left[M(x)=y\right]}{Pr\left[M(x^{\prime})=y\right]}\leq\epsilon^{ \epsilon}\frac{Pr\left[M(x)=y^{\prime}\right]}{Pr\left[M(x^{\prime})=y^{ \prime}\right]}\]

If we have bounds upon this property that are stronger than those immediately implied by DP, then it was shown in Dong et al. (2020) that substantial improvements can be made in bounding the overall DP over the composition of these mechanisms. While these improvements can be applied to our results here as well, we focus upon the privacy bounds with respect to zCDP as it is much cleaner to work with in providing strong composition bounds. From the proof of Lemma 3.4 in Cesar and Rogers (2021) we see that the range-bounded property can give a significant improvement in zCDP properties compared to similar DP guarantees.

**Proposition 2** (Cesar and Rogers (2021)).: _If a mechanism \(M\) is \(\epsilon\)-range-bounded then it is \(\frac{1}{8}\epsilon^{2}\)-zCDP._

Our general characterization is already set up to provide range-bounded properties of \(\mathsf{AboveThreshold}\). This will allow us to give tighter guarantees on the range-boundedness which can be translated to better privacy composition bounds.

**Corollary B.2**.: _For a stream of queries \(\{f_{!}\}\) and threshold \(T\), Algorithm 1 with \(\mathsf{Lap}\), \(\mathsf{Gumbel}\), or \(\mathsf{Expo}\) noise is \(\epsilon\)-range-bounded where_

\[\epsilon=\max_{x\sim x^{\prime}}\left(\epsilon(x,x^{\prime})+\epsilon(x^{ \prime},x)\right)\]

_and for \(\mathsf{Gumbel}\) noise we must have \(\epsilon_{1}=\epsilon_{2}\)._

Proof.: We can rewrite the inequality from Definition B.2 as equivalently

\[\Pr\left[M(x)=y\right]\cdot\Pr\left[M(x^{\prime})=y^{\prime}\right]\leq \epsilon^{\epsilon}\Pr\left[M(x^{\prime})=y\right]\cdot\Pr\left[M(x)=y^{ \prime}\right]\]

Our claim then follows immediately by applying Lemma B.1 and Lemma B.3

### Improved composition analysis of SVT

In this section we complete our analysis for improving the privacy bounds for sparse vector technique. While our generalized characterization was in a more complex form, we simplify it here for general queries and monotonic queries. This will then match up to the privacy guarantees that we gave in Section 3 for AboveThreshold.

**Lemma B.4**.: _Given a stream of queries \(\{f_{i}\}\) with sensitivity \(\Delta\) then for any neighboring datasets \(\varepsilon(x,x^{\prime})\leq\varepsilon_{1}+2\varepsilon_{2}\). If the queries are monotonic then for any neighboring datasets \(\varepsilon(x,x^{\prime})\leq\varepsilon_{1}+\varepsilon_{2}\), and furthermore, \(\varepsilon(x,x^{\prime})+\varepsilon(x^{\prime},x)\leq\varepsilon_{1}+2 \varepsilon_{2}\)._

Proof.: By Definition 2.2 we must always have \(\Delta_{k}(x,x^{\prime})\leq\Delta\) for any neighboring datasets. Furthermore, we must also have \(f_{k}(x^{\prime})-f_{k}(x)\geq-\Delta\) for any neighboring datasets. This implies \(\varepsilon(x,x^{\prime})\leq\varepsilon_{1}+2\varepsilon_{2}\) for any neighboring datasets.

Now assume the queries are monotonic. Without loss of generality assume \(f_{k}(x)\geq f_{k}(x^{\prime})\) for all \(k\). Therefore \(\Delta_{k}(x,x^{\prime})=0\) and \(f_{k}(x^{\prime})-f_{k}(x)\geq-\Delta\), which implies \(\varepsilon(x,x^{\prime})\leq\varepsilon_{2}\). Similarly we have \(\Delta_{k}(x^{\prime},x)\leq\Delta\) but also \(f_{k}(x)-f_{k}(x^{\prime})\geq 0\), which implies \(\varepsilon(x^{\prime},x)\leq\varepsilon_{1}+\varepsilon_{2}\). Combining these implies \(\varepsilon(x^{\prime},x)\leq\varepsilon_{1}+\varepsilon_{2}\) and \(\varepsilon(x^{\prime},x)+\varepsilon(x,x^{\prime})\leq\varepsilon_{1}+2 \varepsilon_{2}\) for any neighboring datasets. 

This lemma along with Corollary B.1 immediately imply Corollary 3.1 and Lemma 3.2

Proof of Theorem 1.: From Lemma B.4 and Corollary B.2 we have that Algorithm 1 is \(\varepsilon_{1}+2\varepsilon_{2}\)-range-bounded. Applying Proposition 2 then gives the zCDP guarantees. 

The sparse vector technique is simply an iterative call to AboveThreshold and as such the privacy bounds will come from the composition. By analyzing the composition through zCDP which has become commonplace in the privacy community, we can immediately improve the privacy guarantees for monotonic queries in the sparse vector technique. Furthermore we can also improve the privacy guarantees under the standard definition using the improved composition bounds for range-bounded mechanisms from Dong et al. (2020).

We also note here that the generalized Sparse Vector Technique given in Lyu et al. (2017) only adds noise once to the threshold and only has to scale the noise to the number of calls to AboveThreshold for the queries and not the threshold. Our analysis can extend to give the same properties for Lap and Expo noise, but utilizing advanced composition properties will give far more accuracy. More specifically, the threshold and queries having noise proportional to \(\sqrt{c}/\varepsilon\), is preferable to all the queries having noise proportional to \(c/\varepsilon\), and only the threshold proportional to \(1/\varepsilon\), where \(c\) is the number of calls to AboveThreshold.

### Iterative exponential mechanism

It is folklore that the peeling exponential mechanism is equivalent to taking the top-\(k\) after adding Gumbel noise, as formally shown in Lemma 4.2 of Durfee and Rogers (2019). We show a similar property here for AboveThreshold with Gumbel noise that it is equivalent to iteratively running the exponential mechanism.

```
1:Input dataset \(x\), a stream of queries \(\{f_{i}\,:\,\mathcal{X}\to\mathbb{R}\}\) with sensitivity \(\Delta\), and a threshold \(T\)
2:for each query \(i\)do
3: Run exponential mechanism with \(\mathcal{Y}=\{0,...,i\}\) where \(q(x,0)=T\) and \(q(x,j)=f_{j}(x)\) for all \(j>0\)
4:if\(i\) is selected then
5: Output \(\top\) and halt
6:else
7: Output \(\bot\)
8:endif
9:endfor ```

**Algorithm 3** Iterative Exponential Mechanism

**Lemma B.5**.: _If \(\varepsilon_{1}=\varepsilon_{2}=\varepsilon/2\) then Algorithm 1 with_ Gumbel _noise gives the equivalent output distribution to Algorithm 3._

Proof.: We first define

\[p_{k}=\frac{\exp(\frac{\varepsilon}{2\Delta}f_{k}(x))}{\exp(\frac{\varepsilon} {2\Delta}T)+\sum_{i=1}^{k}\exp(\frac{\varepsilon}{2\Delta}f_{i}(x))}\]

By construction, the probability of Algorithm 3 outputting \(\{\bot^{k-1},\top\}\) is equal to \(p_{k}\prod_{i=1}^{k-1}(1-p_{i})\). Furthermore, by our definition of \(p_{k}\) we have

\[1-p_{k}=\frac{\exp(\frac{\varepsilon}{2\Delta}T)+\sum_{i=1}^{k-1}\exp(\frac{ \varepsilon}{2\Delta}f_{i}(x))}{\exp(\frac{\varepsilon}{2\Delta}T)+\sum_{i=1} ^{k}\exp(\frac{\varepsilon}{2\Delta}f_{i}(x))}\]

Through telescoping cancellation

\[\prod_{i=1}^{k-1}(1-p_{i})=\frac{\exp(\frac{\varepsilon}{2\Delta}T)}{\exp( \frac{\varepsilon}{2\Delta}T)+\sum_{i=1}^{k-1}\exp(\frac{\varepsilon}{2\Delta }f_{i}(x))}\]

From Lemma B.2 we then see that the output probabilities are equivalent.

## Appendix C Additional unbounded quantile estimation results

In this section we first extend our method to the fully unbounded setting by simply making two calls to AboveThreshold and essentially searching through both positive and negative numbers in each respective call. Next we show how our methods can also extend to the _add-subtract_ definition of neighboring. Further, we'll apply our approach to the framework set up in Kaplan et al. (2022) for computing multiple quantiles and extend it to the _swap_ definition.

### Fully unbounded quantile estimation

Recall that our unbounded quantile estimation algorithm assumed that the data was lower bounded. It then slowly increased that bound by a small percentage until the appropriate amount of data fell below the threshold for the given quantile. In order to extend to the fully unbounded setting, we will simply first apply this guess and check method to the positive numbers and then apply it to the negative numbers.

```
1:Input dataset \(x\), a quantile \(q\), and parameter \(\beta>1\)
2:Run AboveThreshold with \(x\), \(T=qn\) and \(f_{i}(x)=|\{x_{j}\in x|x_{j}+1<\beta^{i}\}|\)
3:Run AboveThreshold with \(x\), \(T=\{1-qn\}n\) and \(f(x)=|\{x_{j}\in x|x_{j}-1>-\beta^{i}\}|\)
4:If the first AboveThreshold halts at \(k>0\) then output \(\beta^{k}-1\)
5:If the second AboveThreshold halts at \(k>0\) then output \(-\beta^{k}+1\)
6:Otherwise return 0 ```

**Algorithm 4** Fully unbounded quantile mechanism

Note that the second call could equivalently be achieved by flipping the sign of all the datapoints and again applying queries \(f_{i}(x)=|\{x_{j}\in x|x_{j}+1<\beta^{i}\}|\). Therefore all our privacy guarantees from Section 4 will still apply, but composing over two calls to AboveThreshold, which is the Sparse Vector Technique.

In the first call to AboveThreshold we are assuming a lower bound of 0, and searching the positive numbers. If it terminates immediately then it is likely that more than a \(q\)th fraction of the data is below 0. We then symmetrically search through the negative numbers by assuming an upper bound of 0. If this halts immediately then it is likely that the quantile is already near 0. We could also apply other variants of this, such as three total calls to get the maximum and minimum of the data if we wanted full data bounds.

Further note that computing the smallest quantiles will be challenging for our algorithm because it may take many queries to get to the appropriate threshold, but each query will have a reasonable chance of terminating if \(q\) is very small. To account for these queries in the lower-bounded setting, we can invert all the datapoints and instead search for quantile \(1-q\) on this transformed data, then invert our resulting estimate. We would also need to reduce our parameter \(\beta\) a reasonable amount in this setting.

### Extension to _add-subtract_ neighbors

We also extend our results to the _add-subtract_ definition of neighboring datasets.

**Definition C.1**.: _Datasets \(x,x^{\prime}\in\mathcal{X}\) are neighboring if one of them can be obtained from the other by adding or removing one individual's data._

Under this definition we see that our threshold \(T=qn\) is no longer fixed, so we will instead set each query to be \(f_{i}(x)=[x_{j}\in x|x_{j}-\ell+1<\beta^{i}]-qn\). Unfortunately this query will no longer be monotonic, so we will instead take advantage of our more general characterization in Section B.1 to further tighten the privacy bounds in this setting.

**Lemma C.1**.: _Given the stream of queries \(f_{i}(x)=|\{x_{j}\in x|x_{j}-\ell+1<\beta^{i}\}|-qn\) with sensitivity \(\Delta=1\), for any neighboring datasets \(x,x^{\prime}\) under Definition C.1 and quantile \(q\in[0,1]\) we have \(\epsilon(x,x^{\prime})\leq\max\{(1-q)\epsilon_{1},q\epsilon_{1}+\epsilon_{2}\}\)._

Proof.: Without loss of generality, assume that \(x\) has one more individuals data, so \(x\in\mathbb{R}^{n}\) and \(x^{\prime}\in\mathbb{R}^{n-1}\). Let \(g_{i}(x)=|\{x_{j}\in x|x_{j}-\ell+1<\beta^{i}\}|\). By construction we must have \(g_{i}(x)-g_{i}(x^{\prime})\in\{0,1\}\) because \(x\) has an additional datapoint. Furthermore, because the thresholds are increasing, if \(g_{i}(x)-g_{i}(x^{\prime})=1\) then \(g_{i}(x)-g_{i}(x^{\prime})=1\) for all \(i>k\). Similarly if \(g_{k}(x)-g_{k}(x^{\prime})=0\) then \(g_{i}(x)-g_{i}(x^{\prime})=0\) for all \(i<k\). We further see that \(f_{i}(x)-f_{i}(x^{\prime})=g_{i}(x)-g_{i}(x^{\prime})-q\) for all \(i\).

First consider the case when \(g_{k}(x)-g_{k}(x^{\prime})=0\). We therefore have \(g_{i}(x)-g_{i}(x^{\prime})=0\) for all \(i<k\) so \(\Delta_{k}(x,x^{\prime})=q\), and also \(\Delta_{k}(x,x^{\prime})-(f_{k}(x^{\prime})-f_{k}(x))=0\), so \(\epsilon(x,x^{\prime})\leq q\epsilon_{1}\). Similarly, we have \(\Delta_{k}(x^{\prime},x)=0\) and \(\Delta_{k}(x,x^{\prime})-(f_{k}(x^{\prime})-f_{k}(x))=q\), so \(\epsilon(x^{\prime},x)\leq q\epsilon_{2}\).

Next consider the case when \(g_{k}(x)-g_{k}(x^{\prime})=1\). Therefore we have \(\Delta_{k}(x,x^{\prime})\leq q\) and also \(f_{k}(x^{\prime})-f_{k}(x)=q-1\) which implies \(\Delta_{k}(x,x^{\prime})-(f_{k}(x^{\prime})-f_{k}(x))\leq 1\), so \(\epsilon(x,x^{\prime})\leq q\epsilon_{1}+\epsilon_{2}\). Similarly, we have that \(\Delta_{k}(x^{\prime},x)\leq 1-q\), and thus \(\Delta_{k}(x^{\prime},x)-(f_{k}(x)-f_{k}(x^{\prime}))=0\), so \(\epsilon(x^{\prime},x)\leq(1-q)\epsilon_{1}\).

Combining these bounds gives our desired result.

With these improved bounds we can then show that our methods also extend to the _add-subtract_ definition of neighboring with tighter privacy guarantees.

**Corollary C.1**.: _If we run Algorithm 2 with \(\epsilon_{1}=\epsilon_{2}\) for a quantile \(q\) under Definition C.1 then it is \((q\epsilon_{1}+\epsilon_{2})\)-DP._

Proof.: Combining Lemma C.1 and Corollary B.1 immediately imply the desired result

### Extension to multiple quantile estimation

As previously established, the framework in Kaplan et al. (2022) is agnostic to the single quantile estimation method. However their proof is for the _add-subtract_ neighbors, although they note that it can be extended easily to the _swap_ neighbors. We also discuss here how it can be extended for completeness.

For the _swap_ neighboring definition, at each level of the recursive partitioning scheme either two partitions differ under the _add-subtract_ definition, or one partition differs under the _swap_ definition. Applying their framework to our algorithm, we will instead compute all the thresholds in advance of the splitting. Accordingly these thresholds will remain fixed. If the thresholds are fixed then we can actually further improve our privacy bounds for the _add-subtract_ definition. Once again we will use our more general characterization from Section B.1.

**Lemma C.2**.: _Given the stream of queries \(f_{i}(x)=\{x_{j}\in x|x_{j}-\ell+1<\beta^{i}\}|\) with sensitivity \(\Delta=1\), for any neighboring datasets \(x,x^{\prime}\) under Definition C.1 we have \(\varepsilon(x,x^{\prime})\leq\max\{\varepsilon_{1},\varepsilon_{2}\}\)_

Proof.: Without loss of generality, assume that \(x\) has one more individuals data, so \(x\in\mathbb{R}^{n}\) and \(x^{\prime}\in\mathbb{R}^{n-1}\). By construction we must have \(f_{i}(x)-f_{i}(x^{\prime})\in\{0,1\}\) because \(x\) has an additional datapoint. Furthermore, because the thresholds are increasing, if \(f_{k}(x)-f_{k}(x^{\prime})=0\) then \(f_{i}(x)-f_{i}(x^{\prime})=0\) for all \(i<k\). Thus the case of \(f_{k}(x)-f_{k}(x^{\prime})=0\) is easy.

Instead consider \(f_{k}(x)-f_{k}(x^{\prime})=1\). We know \(\Delta_{k}(x,x^{\prime})=0\) so \(\Delta_{k}(x,x^{\prime})-(f_{k}(x^{\prime})-f_{k}(x))=1\), so \(\varepsilon(x,x^{\prime})\leq\varepsilon_{2}\). Further we must have \(\Delta_{k}(x^{\prime},x)\leq 1\) and \(\Delta_{k}(x^{\prime},x)-(f_{k}(x)-f_{k}(x^{\prime}))=0\), so \(\varepsilon(x^{\prime},x)\leq\varepsilon_{1}\). Combining these implies \(\varepsilon(x^{\prime},x)\leq\max\{\varepsilon_{1},\varepsilon_{2}\}\) for any neighboring datasets.

Applying these bounds we see that applying the framework of Kaplan et al. (2022) to the swap definition either leads to one composition of \((\varepsilon_{1}+\varepsilon_{2})\)-DP for the _swap_ definition or two compositions of \(\max\{\varepsilon_{1},\varepsilon_{2}\}\) for the _add-subtract_ definition at each level. Setting \(\varepsilon_{1}=\varepsilon_{2}\) we then have that the privacy cost of computing \(m\) quantiles with this framework will be equivalent to \(\log(m+1)\) compositions of \((\varepsilon_{1}+\varepsilon_{2})\)-DP.

## Appendix D Implementation code

For ease of implementation, we provide some simple python code to run our method with access to the respective Noise generator of the users choosing. In our experiments we used exponential noise from the numpy library.

``` defunboundeddQuantile(data,1,b,q,eps_1,eps_2): d=defaultdict(int) forxindata: i=math.log(x-1+1,b)//1 d[i]+=1 t=q*len(data)+noise(1/eps_1) cur,i=0,0 whileTrue: cur+=d[i] i+=1 ifcur+noise(1/eps_2)>t: break returnbs*i-1+1 ```