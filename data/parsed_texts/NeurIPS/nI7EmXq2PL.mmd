# \(\mathcal{H}\)-Consistency Bounds:

Characterization and Extensions

 Anqi Mao

Courant Institute

New York, NY 10012

aqmao@cims.nyu.edu &Mehryar Mohri

Google Research & CIMS

New York, NY 10011

mohri@google.com &Yutao Zhong

Courant Institute

New York, NY 10012

yutao@cims.nyu.edu

###### Abstract

A series of recent publications by Awasthi et al. (2022) have introduced the key notion of _\(\mathcal{H}\)-consistency bounds_ for surrogate loss functions. These are upper bounds on the zero-one estimation error of any predictor in a hypothesis set, expressed in terms of its surrogate loss estimation error. They are both non-asymptotic and hypothesis set-specific and thus stronger and more informative than Bayes-consistency. However, determining if they hold and deriving these bounds have required a specific proof and analysis for each surrogate loss. Can we derive more general tools and characterizations? This paper provides both a general characterization and an extension of \(\mathcal{H}\)-consistency bounds for multi-class classification. We present new and tight \(\mathcal{H}\)-consistency bounds for both the family of constrained losses and that of comp-sum losses, which covers the familiar cross-entropy, or logistic loss applied to the outputs of a neural network. We further extend our analysis beyond the completeness assumptions adopted in previous studies and cover more realistic bounded hypothesis sets. Our characterizations are based on error transformations, which are explicitly defined for each formulation. We illustrate the application of our general results through several special examples. A by-product of our analysis is the observation that a recently derived multi-class \(\mathcal{H}\)-consistency bound for cross-entropy reduces to an excess bound and is not significant. Instead, we prove a much stronger and more significant guarantee.

## 1 Introduction

Bayes-consistency is an important property of surrogate loss functions. It requires that minimizing the surrogate excess error over the family of all measurable functions leads to the minimization of the target error loss in the limit Steinwart (2007). This property applies to a broad family of convex margin-based losses in binary classification Zhang (2004); Bartlett et al. (2006), as well as some extensions in multi-class classification Tewari and Bartlett (2007). However, Bayes-consistency does not apply to the hypothesis sets commonly used for learning, such as the family of linear models or that of neural networks, which of course do not include all measurable functions. Furthermore, it is also only an asymptotic property and does not supply any convergence guarantee.

To address these limitations, a series of recent publications by Awasthi et al. (2022) introduced the key notion of _\(\mathcal{H}\)-consistency bounds_ for surrogate loss functions. These are upper bounds on the zero-one estimation error of any predictor in a hypothesis set, expressed in terms of its surrogate loss estimation error. They are both non-asymptotic and hypothesis set-specific and thus stronger and more informative than Bayes-consistency. However, determining the validity of these bounds and deriving them have required a specific proof and analysis for each surrogate loss. Can we derive more general tools and characterizations for \(\mathcal{H}\)-consistency bounds?This paper provides both a general characterization and an extension of \(\mathcal{H}\)-consistency bounds for multi-class classification. Previous approaches to deriving these bounds required the development of new proofs for each specific case. In contrast, we introduce the general concept of an _error transformation function_ that serves as a very general tool for deriving such guarantees with tightness guarantees. We show that deriving an \(\mathcal{H}\)-consistency bound for comp-sum losses and constrained losses for both complete and bounded hypothesis sets can be reduced to the calculation of their corresponding error transformation function. Our general tools and tight bounds show several remarkable advantages: first, they improve existing bounds for complete hypothesis sets previously proven in [11]; second, they encompass all previously comp-sum and constrained losses studied thus far as well as many new ones [11, 12]; third, they extend beyond the completeness assumption adopted in previous work; fourth, they provide novel guarantees for bounded hypothesis sets; and, finally, they help prove a much stronger and more significant guarantee for logistic loss with linear hypothesis set than [13].

**Previous work.** Here, we briefly discuss recent studies of \(\mathcal{H}\)-consistency bounds by Awasthi et al. [2022, 13], Mao et al. [2023] and Zheng et al. [2023]. Awasthi et al. [2022] introduced and studied \(\mathcal{H}\)-consistency bounds in binary classification. They provided a series of _tight_\(\mathcal{H}\)-consistency bounds for _bounded_ hypothesis set of linear models and one-hidden-layer neural networks. The subsequent study [11] further generalized the framework to multi-class classification and presented an extensive study of \(\mathcal{H}\)-consistency bounds for diverse multi-class surrogate losses, including negative results for _max losses_[14] and positive results for _sum losses_[12], and _constrained losses_[15]. However, the hypothesis sets examined in their analysis were assumed to be complete, which rules out the bounded hypothesis sets typically used in practice. Moreover, the final bounds derived from [11] are based on ad hoc methods and may not be tight. [12] complemented this previous work by studying a wide family of _comp-sum losses_ in the multi-class classification, which generalizes the _sum-losses_ and includes as special cases the logistic loss [13, 145, 144, 149], the _generalized cross-entropy loss_[13], and the _mean absolute error loss_[15]. Here too, the completeness assumption on the hypothesis sets was adopted and their \(\mathcal{H}\)-consistency bounds do not apply to common bounded hypothesis sets in practice. Recently, Zheng et al. [2023] proved \(\mathcal{H}\)-consistency bounds for multi-class logistic loss with bounded linear hypothesis sets. However, their bounds require a crucial distributional assumption, under which the mimizability gaps coincide with the approximation errors. Thus, their bounds can be recovered as excess error bounds, which are less significant.

Other related work on \(\mathcal{H}\)-consistency bounds includes \(\mathcal{H}\)-consistency bounds for pairwise ranking [12, 12]; theoretically grounded surrogate losses and algorithms for learning with abstention supported by \(\mathcal{H}\)-consistency bounds, including the study of score-based abstention [12, 12], predictor-rejector abstention [12, 13] and learning to abstain with a fixed predictor with application in decontextualization [14]; Andor, Choi, Collins, Mao, and Zhong [2023]; principled approaches for learning to defer with multiple experts that benefit from strong \(\mathcal{H}\)-consistency bounds, including the single-stage scenario [12, 13] and a two-stage scenario [12, 13]; \(\mathcal{H}\)-consistency theory and algorithms for adversarial robustness [11, 13, 12, 12, 12, 12]; and efficient algorithms and loss functions for structured prediction with stronger \(\mathcal{H}\)-consistency guarantees [12].

**Structure of this paper.** We present new and tight \(\mathcal{H}\)-consistency bounds for both the family of comp-sum losses (Section 4.1) and that of constrained losses (Section 5.1), which cover the familiar cross-entropy, or logistic loss applied to the outputs of a neural network. We further extend our analysis beyond the completeness assumptions adopted in previous studies and cover more realistic bounded hypothesis sets (Section 4.2 and 5.2). Our characterizations are based on error transformations, which are explicitly defined for each formulation. We illustrate the application of our general results through several special examples. A by-product of our analysis is the observation that a recently derived multi-class \(\mathcal{H}\)-consistency bound for cross-entropy reduces to an excess bound independent of the hypothesis set. Instead, we prove a much stronger and more significant guarantee (Section 4.2).

We give a comprehensive discussion of related work in Appendix A. We start with some basic definitions and notation in Section 2.

## 2 Preliminaries

We denote by \(\mathcal{X}\) the input space, by \(\mathcal{Y}\) the output space, and by \(\mathcal{D}\) a distribution over \(\mathcal{X}\times\mathcal{Y}\). We consider the standard scenario of multi-class classification, where \(\mathcal{Y}=\{1,\ldots,n\}\). Given a hypothesis set \(\mathcal{H}\) of functions mapping \(\mathcal{X}\times\mathcal{Y}\) to \(\mathbb{R}\), the multi-class classification problem consists of finding a hypothesis \(h\in\mathcal{H}\) with small generalization error \(\mathcal{R}_{\ell_{0-1}}(h)\), defined by \(\mathcal{R}_{\ell_{0-1}}(h)=\mathbb{E}_{(x,y)\sim\mathcal{D}}[\ell_{0-1}(h,x,y)]\), where \(\ell_{0-1}(h,x,y)=\mathds{1}_{h(x)\neq y}\) is the multi-class zero-one loss with \(\mathsf{h}(x)=\operatorname*{argmax}_{y\in\mathcal{Y}}h(x,y)\) the prediction of \(h\) for the input point \(x\). We also denote by \(\mathsf{H}(x)\) the set of all predictions associated to input \(x\) generated by functions in \(\mathcal{H}\), that is, \(\mathsf{H}(x)=\left\{\mathsf{h}(x)\colon h\in\mathcal{H}\right\}\).

We will analyze the guarantees of surrogate multi-class losses in terms of the zero-one loss. We denote by \(\ell\) a surrogate loss and by \(\mathcal{R}_{\ell}(h)\) its generalization error, \(\mathcal{R}_{\ell}(h)=\mathbb{E}_{(x,y)\sim\mathcal{D}}[\ell(h,x,y)]\). For a loss function \(\ell\), we define the best-in-class generalization error within a hypothesis set \(\mathcal{H}\) as \(\mathcal{R}_{\ell}^{*}(\mathcal{H})=\inf_{h\in\mathcal{H}}\mathcal{R}_{\ell}(h)\), and refer to \(\mathcal{R}_{\ell}(h)-\mathcal{R}_{\ell}^{*}(\mathcal{H})\) as the _estimation error_. We will study the key notion of _\(\mathcal{H}\)-consistency bounds_(Awasthi et al., 2022a,b), which are upper bounds on the zero-one estimation error of any predictor in a hypothesis set, expressed in terms of its surrogate loss estimation error, for some real-valued function \(f\) that is non-decreasing:

\[\forall h\in\mathcal{H},\ \mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}_{\ell_{0-1}}^{* }(\mathcal{H})\leq f(\mathcal{R}_{\ell}(h)-\mathcal{R}_{\ell}^{*}(\mathcal{H} )).\]

These bounds imply that the zero-one estimation error is at most \(f(\epsilon)\) whenever the surrogate loss estimation error is bounded by \(\epsilon\). Thus, the learning guarantees provided by \(\mathcal{H}\)-consistency bounds are both non-asymptotic and hypothesis set-specific. The function \(f\) appearing in these bounds is expressed in terms of a _minimizability gap_, which is a quantity measuring the difference of best-in-class error \(\mathcal{R}_{\ell}^{*}(\mathcal{H})\) and the expected _best-in-class conditional error_\(\mathbb{E}_{x}[\mathcal{C}_{\ell}^{*}(\mathcal{H},x)]\): \(\mathcal{M}_{\ell}(\mathcal{H})=\mathcal{R}_{\ell}^{*}(\mathcal{H})-\mathbb{E }_{X}[\mathcal{C}_{\ell}^{*}(\mathcal{H},x)]\), where \(\mathcal{C}_{\ell}(h,x)=\mathbb{E}_{y|x}[\ell(h,x,y)]\) and \(\mathcal{C}_{\ell}^{*}(\mathcal{H},x)=\inf_{h\in\mathcal{H}}\mathcal{C}_{\ell }(h,x)\) are the _conditional error_ and _best-in-class conditional error_ respectively. We further write \(\Delta\mathcal{C}_{\ell,\mathcal{H}}=\mathcal{C}_{\ell}(h,x)-\mathcal{C}_{ \ell}^{*}(\mathcal{H},x)\) to denote the _conditional regret_. Note that that the minimizability gap is an inherent quantity depending on a hypothesis set \(\mathcal{H}\) and the loss function \(\ell\).

By Lemma 1, the minimizability gap for the zero-one loss, \(\mathcal{M}_{\ell_{0-1}}(\mathcal{H})\), coincides with its approximation error \(\mathcal{A}_{\ell_{0-1}}(\mathcal{H})=\mathcal{R}_{\ell_{0-1}}^{*}(\mathcal{H })-\mathcal{R}_{\ell_{0-1}}^{*}(\mathcal{M}_{\mathrm{all}})\) when the set of all possible predictions generated by \(\mathcal{H}\) covers the label space \(\mathcal{Y}\). This holds for typical hypothesis sets used in practice. However, for a surrogate loss \(\ell\), the minimizability gap \(\mathcal{M}_{\ell}(\mathcal{H})\) is always upper bounded by and in general finer than its approximation error \(\mathcal{A}_{\ell}(\mathcal{H})=\mathcal{R}_{\ell}^{*}(\mathcal{H})-\mathcal{R} _{\ell}^{*}(\mathcal{H}_{\mathrm{all}})\) since \(\mathcal{M}_{\ell}(\mathcal{H})=\mathcal{A}_{\ell}(\mathcal{H})-I_{\ell}( \mathcal{H})\), where \(\mathcal{H}_{\mathrm{all}}\) is the family of all measurable functions and \(I_{\ell}(\mathcal{H})=\mathbb{E}_{x}\left[\mathcal{C}_{\ell}^{*}(\mathcal{H},x )-\mathcal{C}_{\ell}^{*}(\mathcal{H}_{\mathrm{all}},x)\right]\) (see Appendix B for a more detailed discussion). Thus, an \(\mathcal{H}\)-consistency bound, expressed as follows for some increasing function \(\Gamma\):

\[\mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}_{\ell_{0-1}}^{*}(\mathcal{H})+\mathcal{M }_{\ell_{0-1}}(\mathcal{H})\leq\Gamma(\mathcal{R}_{\ell}(h)-\mathcal{R}_{\ell}^ {*}(\mathcal{H})+\mathcal{M}_{\ell}(\mathcal{H})),\] (1)

is more favorable than an excess error bound expressed in terms of approximation errors \(\mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}_{\ell_{0-1}}^{*}(\mathcal{H})+\mathcal{A }_{\ell_{0-1}}(\mathcal{H})\leq\Gamma(\mathcal{R}_{\ell}(h)-\mathcal{R}_{\ell} ^{*}(\mathcal{H})+\mathcal{A}_{\ell}(\mathcal{H}))\). Here, \(\Gamma\) is typically linear or the square-root function modulo constants. When \(\mathcal{H}=\mathcal{H}_{\mathrm{all}}\), the family of all measurable functions, an \(\mathcal{H}\)-consistency bound coincides with the excess error bound and implies Bayes-consistency by taking the limit. It is therefore a stronger guarantee than an excess error bound and Bayes-consistency.

The minimizability gap is always non-negative, since the infimum of the expectation is greater than or equal to the expectation of infimum. Furthermore, as shown in Appendix B, when \(\mathcal{H}\) is the family of all measurable functions or when the Bayes-error coincides with the best-in-class error, that is, \(\mathcal{R}_{\ell}^{*}(\mathcal{H})=\mathcal{R}_{\ell}^{*}(\mathcal{H}_{ \mathrm{all}})\), the minimizability gap vanishes. In such cases, (1) implies the \(\mathcal{H}\)-consistency of a surrogate loss \(\ell\) with respect to the zero-one loss \(\ell_{0-1}\):

\[\mathcal{R}_{\ell}(h_{n})-\mathcal{R}_{\ell}^{*}(\mathcal{H})\xrightarrow{n\to+ \infty}0\implies\mathcal{R}_{\ell_{0-1}}(h_{n})-\mathcal{R}_{\ell_{0-1}}^{*}( \mathcal{H})\xrightarrow{n\to+\infty}0.\]

In the next sections, we will provide both a general characterization and an extension of \(\mathcal{H}\)-consistency bounds for multi-class classification. Before proceeding, we first introduce a useful lemma from (Awasthi et al., 2022b) which characterizes the conditional regret of zero-one loss explicitly. We denote by \(p(x)=(p(x,1),\ldots,p(x,n))\) as the conditional distribution of \(y\) given \(x\).

**Lemma 1**.: _For zero-one loss \(\ell_{0-1}\), the best-in-class conditional error and the conditional regret for \(\ell_{0-1}\) can be expressed as follows: for any \(x\in\mathcal{X}\), we have_

\[\mathcal{C}_{\ell_{0-1}}^{*}(\mathcal{H},x)=1-\max_{y\in\mathsf{H}(x)}p(x,y) \quad\text{and}\quad\Delta\mathcal{C}_{\ell_{0-1},\mathcal{H}}(h,x)=\max_{y\in \mathsf{H}(x)}p(x,y)-p(x,\mathsf{h}(x)).\]Comparison with previous work

Here, we briefly discuss previous studies of \(\mathcal{H}\)-consistency bounds (Awasthi et al., 2022a,b, Zheng et al., 2023; Mao et al., 2023) in standard binary or multi-class classification and compare their results with those we present.

Awasthi et al. (2022a) studied \(\mathcal{H}\)-consistency bounds in binary classification. They provided a series of _tight_\(\mathcal{H}\)-consistency bounds for the _bounded_ hypothesis set of linear models \(\mathcal{H}_{\mathrm{lin}}^{\mathrm{bi}}\) and one-hidden-layer neural networks \(\mathcal{H}_{\mathrm{NN}}^{\mathrm{bi}}\), defined as follows:

\[\mathcal{H}_{\mathrm{lin}}^{\mathrm{bi}}=\big{\{}x\mapsto w\cdot x +b\mid\|w\|\leq W,|b|\leq B\big{\}}\] \[\mathcal{H}_{\mathrm{NN}}^{\mathrm{bi}}=\big{\{}x\mapsto\sum_{j= 1}^{n}u_{j}(w_{j}\cdot x+b)_{+}\mid\|u\|_{1}\leq\Lambda,\|w_{j}\|\leq W,|b|\leq B \big{\}},\]

where \(B\), \(W\), and \(\Lambda\) are positive constants and where \((\cdot)_{+}=\max(\cdot,0)\). We will show that our bounds recover these binary classification \(\mathcal{H}\)-consistency bounds.

The scenario of multi-class classification is more challenging and more crucial in applications. Recent work by Awasthi et al. (2022b) showed that _max losses_(Crammer and Singer, 2001), defined as \(\ell^{\max}(h,x,y)=\max_{y^{\prime}\neq y}\Phi\big{(}h(x,y)-h(x,y^{\prime}) \big{)}\) for some convex and non-increasing function \(\Phi\), cannot admit meaningful \(\mathcal{H}\)-consistency bounds, unless the distribution is deterministic. They also presented a series of \(\mathcal{H}\)-consistency bounds for _sum losses_(Weston and Watkins, 1998) and _constrained losses_(Lee et al., 2004) for _symmetric_ and _complete_ hypothesis sets, that is such that:

\[\mathcal{H}=\{h:\mathcal{X}\times\mathcal{Y}\rightarrow\mathbb{R} \colon h(\cdot,y)\in\mathcal{F},\forall y\in\mathcal{Y}\}\] (symmetry) \[\forall x\in\mathcal{X},\{f(x)\colon f\in\mathcal{F}\}=\mathbb{R},\] (completeness)

for some family \(\mathcal{F}\) of functions mapping from \(\mathcal{X}\) to \(\mathbb{R}\). The completeness assumption rules out the bounded hypothesis sets typically used in practice such as \(\mathcal{H}_{\mathrm{lin}}\). Moreover, the final bounds derived from (Awasthi et al., 2022b) are based on ad hoc proofs and may not be tight. In contrast, we will study both the complete and bounded hypothesis sets, and provide a very general tool to derive \(\mathcal{H}\)-consistency bounds. Our bounds are tighter than those of Awasthi et al. (2022b) given for complete hypothesis sets and extend beyond the completeness assumption.

(Mao et al., 2023) complemented the work of (Awasthi et al., 2022b) by studying a wide family of _comp-sum losses_ in multi-class classification, which generalized the _sum-losses_ and included as special cases the logistic loss (Verhulst, 1838, 1845; Berkson, 1944, 1951), the _generalized cross-entropy loss_(Zhang and Sabuncu, 2018), and the _mean absolute error loss_(Ghosh et al., 2017). Here too, the completeness assumption was adopted, thus their \(\mathcal{H}\)-consistency bounds do not apply to common bounded hypothesis sets used in practice. We illustrate the application of our general results through a broader set of surrogate losses than (Mao et al., 2023) and significantly generalize the bounds of (Mao et al., 2023) to bounded hypothesis sets.

Recently, Zheng et al. (2023) proved \(\mathcal{H}\)-consistency bounds for logistic loss with linear hypothesis sets in the multi-class classification: \(\mathcal{H}_{\mathrm{lin}}=\{x\mapsto w_{y}\cdot x+b_{y}\mid\|w_{y}\|\leq W,|b _{y}|\leq B,y\in\mathcal{Y}\}\). However, their bounds require a crucial distributional assumption under which, the minimizability gaps \(\mathcal{M}_{\ell_{0}\text{-}}(\mathcal{H}_{\mathrm{lin}})\) and \(\mathcal{M}_{\ell_{\mathrm{log}}}(\mathcal{H}_{\mathrm{lin}})\) coincide with the approximation errors \(\mathcal{R}_{\ell_{0}\text{-}1}(\mathcal{H}_{\mathrm{lin}})\text{-}\mathcal{ R}_{\ell_{0}\text{-}1}^{*}(\mathcal{H}_{\mathrm{all}})\) and \(\mathcal{R}_{\ell_{\mathrm{log}}}(\mathcal{H}_{\mathrm{lin}})\text{-}\mathcal{ R}_{\ell_{\mathrm{log}}}(\mathcal{H}_{\mathrm{all}})\) respectively (see the note before (Zheng et al., 2023, Appendix F)). Thus, their bounds can be recovered as excess error bounds \(\mathcal{R}_{\ell_{0}\text{-}1}(h)-\mathcal{R}_{\ell_{0}\text{-}1}^{*}( \mathcal{H}_{\mathrm{all}})\leq\sqrt{2}\Big{(}\mathcal{R}_{\ell_{\mathrm{log}} }(h)-\mathcal{R}_{\ell_{\mathrm{log}}}^{*}(\mathcal{H}_{\mathrm{all}})\Big{)} ^{\frac{1}{2}}\), which are less significant. In contrast, our \(\mathcal{H}_{\mathrm{lin}}\)-consistency bound are much finer and take into account the role of the parameter \(B\) and that of the number of labels \(n\). Thus, we provide stronger and more significant guarantees for logistic loss with linear hypothesis set than (Zheng et al., 2023).

In summary, our general tools offer the remarkable advantages of deriving tight bounds, which improve upon the existing bounds of Awasthi et al. (2022b) given for complete hypothesis sets, cover the comp-sum and constrained losses considered in (Awasthi et al., 2022a; Mao et al., 2023) as well as new ones, extend beyond the completeness assumption with novel guarantees valid for bounded hypothesis sets, and are much stronger and more significant guarantees for logistic loss with linear hypothesis sets than those of Zheng et al. (2023).

[MISSING_PAGE_FAIL:5]

\begin{table}
\begin{tabular}{l|l|l|l|l} \hline \hline Auxiliary function \(\Phi\) & \(-\log(t)\) & \(\frac{1}{t}-1\) & \(\frac{1}{q}(1-t^{q}),q\in(0,1)\) & \(1-t\) & \((1-t)^{2}\) \\ \hline Transformation \(\mathcal{T}^{\mathrm{comp}}\) & \(\frac{1+t}{2}\log(1+t)+\frac{1+t}{2}\log(1-t)\) & \(1-\sqrt{1-t^{2}}\) & \(\frac{1}{q^{\mathrm{exp}}}\left(\frac{(1+1)^{\frac{1}{2t}+(1-t)^{2t}}}{2} \right)^{1-q}-\frac{1}{q^{\mathrm{exp}}}\) & \(\frac{1}{n}\) & \(\frac{t^{2}}{4}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: \(\mathcal{H}\)-estimation error transformation for common comp-sum losses.

_with \(\overline{\mathcal{T}}^{\mathrm{comp}}\) the \(\overline{\mathcal{H}}\)-estimation error transformation for comp-sum losses defined for all \(t\in[0,1]\) by \(\overline{\mathcal{T}}^{\mathrm{comp}}(t)=\)_

\[\begin{cases}\inf_{\tau\in\left[0,\frac{1}{2}\right]\mu\in\left\{s_{\min}-\tau, 1-\tau-s_{\min}\right\}}\left\{\frac{1+t}{2}\big{[}\Phi(\tau)-\Phi(1-\tau-\mu) \big{]}+\frac{1-t}{2}\big{[}\Phi(1-\tau)-\Phi(\tau+\mu)\big{]}\right\}&n=2\\ P\big{\{}\frac{1}{\pi-1}\nu,1\big{\}}\inf_{\begin{subarray}{c}S_{\min}\subseteq \tau\leq 1\end{subarray}S_{\min}\subseteq\tau_{1}\subseteq S_{\max}}\sup_{\mu \in C}\{\frac{P+t}{2}\big{[}\Phi(\tau_{2})-\Phi(\tau_{1}-\mu)\big{]}+\frac{P-t }{2}\big{[}\Phi(\tau_{1})-\Phi(\tau_{2}+\mu)\big{]}\}&n>2,\end{cases}\]

_where \(C=[\max\{s_{\min}-\tau_{2},\tau_{1}-s_{\max}\},\min\{s_{\max}-\tau_{2},\tau_{ 1}-s_{\min}\}]\), \(s_{\max}=\frac{1}{1+(n-1)e^{-2\ln s_{\max}(x)}}\) and \(s_{\min}=\frac{1}{1+(n-1)e^{2\ln t_{\max}(x)}}\). Furthermore, for any \(t\in[0,1]\), there exist a distribution \(\mathcal{D}\) and \(h\in\mathcal{H}\) such that \(\mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}_{\ell_{0-1}}^{*}(\mathcal{H})+ \mathcal{M}_{\ell_{0-1}}(\mathcal{H})=t\) and \(\mathcal{R}_{\ell^{\mathrm{comp}}}(h)-\mathcal{R}_{\ell^{\mathrm{comp}}}^{*}( \mathcal{H})+\mathcal{M}_{\ell^{\mathrm{comp}}}(\mathcal{H})=\mathcal{T}^{ \mathrm{comp}}(t)\)._

This theorem significantly broadens the applicability of our framework as it encompasses bounded hypothesis sets. The last statement of the theorem further shows the tightness of the \(\mathcal{H}\)-consistency bounds derived using this error transformation function. We now illustrate the application of our theory through several examples.

**A. Example: logistic loss.** We first consider the multinomial logistic loss, that is \(\ell^{\mathrm{comp}}\) with \(\Phi(u)=-\log(u)\), for which we give the following guarantee.

**Theorem 6** (\(\overline{\mathcal{H}}\)**-consistency bounds for logistic loss).: _For any \(h\in\overline{\mathcal{H}}\) and any distribution, we have_

\[\mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}_{\ell_{0-1}}^{*}(\overline{\mathcal{H }})+\mathcal{M}_{\ell_{0-1}}\big{(}\overline{\mathcal{H}}\big{)}\leq\Psi^{-1} \Big{(}\mathcal{R}_{\ell_{\log}}(h)-\mathcal{R}_{\ell_{\log}}^{*}\big{(} \overline{\mathcal{H}}\big{)}+\mathcal{M}_{\ell_{\log}}\big{(}\overline{ \mathcal{H}}\big{)}\Big{)},\]

_where \(\ell_{\log}=-\log\Big{(}\frac{e^{h(x,y)}}{\sum_{y^{\prime}\leq y}e^{h(x,y^{ \prime})}}\Big{)}\) and \(\Psi(t)=\begin{cases}\frac{1+t}{2}\log(1+t)+\frac{1-t}{2}\log(1-t)&t\leq\frac {s_{\max}-s_{\min}}{s_{\min}+s_{\max}}\\ \frac{t}{2}\log\Big{(}\frac{s_{\max}}{s_{\min}}\Big{)}+\log\Big{(}\frac{2\sqrt {s_{\max}s_{\min}}}{s_{\max}+s_{\min}}\Big{)}&\mathrm{otherwise}.\end{cases}\)_

The proof of Theorem 6 is given in Appendix E.2. With the help of some simple calculations, we can derive a simpler upper bound:

\[\Psi^{-1}(t)\leq\Gamma(t)=\begin{cases}\sqrt{2t}&t\leq\frac{(s_{\max}-s_{\min })^{2}}{2(s_{\min}+s_{\max})^{2}}\\ \frac{2(s_{\min}+s_{\max})}{s_{\max}-s_{\min}}t&\mathrm{otherwise}.\end{cases}\]

When the relative difference between \(s_{\min}\) and \(s_{\max}\) is small, the coefficient of the linear term in \(\Gamma\) explodes. On the other hand, making that difference large essentially turns \(\Gamma\) into a square-root function for all values. In general, \(\Lambda\) is not infinite since a regularization is used, which controls both the complexity of the hypothesis set and the magnitude of the scores.

**Comparison with (Mao et al., 2023h).** For the symmetric and complete hypothesis sets \(\mathcal{H}\) considered in (Mao et al., 2023h), \(\Lambda(x)=+\infty\), \(s_{\max}=1\), \(s_{\min}=0\), \(\Psi(t)=\frac{1+t}{2}\log(1+t)+\frac{1-t}{2}\log(1-t)\) and \(\Gamma(t)=\sqrt{2t}\). By Theorem 6, this yields an \(\mathcal{H}\)-consistency bound for the logistic loss.

**Corollary 7** (\(\mathcal{H}\)**-consistency bounds for logistic loss).: _Assume that \(\mathcal{H}\) is symmetric and complete. Then, for any \(h\in\mathcal{H}\) and any distribution, we have_

\[\mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}_{\ell_{0-1}}^{*}(\mathcal{H})\leq\Psi^{- 1}\Big{(}\mathcal{R}_{\ell_{\log}}(h)-\mathcal{R}_{\ell_{\log}}^{*}(\mathcal{H })+\mathcal{M}_{\ell_{\log}}(\mathcal{H})\Big{)}-\mathcal{M}_{\ell_{0-1}}( \mathcal{H})\]

_where \(\Psi(t)=\frac{1+t}{2}\log(1+t)+\frac{1-t}{2}\log(1-t)\) and \(\Psi^{-1}(t)\leq\sqrt{2t}\)._

Corollary 7 recovers the \(\mathcal{H}\)-consistency bounds of Mao et al. (2023h).

**Comparison with (Awasthi et al., 2022a) and (Zheng et al., 2023).** For the linear models \(\mathcal{H}_{\mathrm{lin}}=\big{\{}(x,y)\mapsto w_{y}\cdot x+b_{y}\bigm{|}\big{|}w _{y}\big{|}\leq W,|b_{y}|\leq B\big{\}}\), we have \(\Lambda(x)=W\|x\|+B\). By Theorem 6, we obtain \(\mathcal{H}_{\mathrm{lin}}\)-consistency bounds for logistic loss.

**Corollary 8** (\(\mathcal{H}_{\mathrm{lin}}\)**-consistency bounds for logistic loss).: _For any \(h\in\mathcal{H}_{\mathrm{lin}}\) and any distribution,_

\[\mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}_{\ell_{0-1}}^{*}(\mathcal{M}_{\mathrm{lin }})\leq\Psi^{-1}\Big{(}\mathcal{R}_{\ell_{\log}}(h)-\mathcal{R}_{\ell_{\log}}^{*} (\mathcal{H}_{\mathrm{lin}})+\mathcal{M}_{\ell_{\log}}(\mathcal{H}_{\mathrm{lin}}) \Big{)}-\mathcal{M}_{\ell_{0-1}}(\mathcal{H}_{\mathrm{lin}})\]

_where \(\Psi(t)=\begin{cases}\frac{1+t}{2}\log(1+t)+\frac{1-t}{2}\log(1-t)&t\leq\frac{( n-1)(e^{2B}-e^{-2B})}{2+(n-1)(e^{2B}+e^{-2B})}\\ \frac{t}{2}\log\Big{(}\frac{1+(n-1)e^{2B}}{1+(n-1)e^{-2B}}\Big{)}+\log\Big{(} \frac{2\sqrt{(1+(n-1)e^{2B})}(1+(n-1)e^{-2B})}{2+(n-1)(e^{2B}+e^{-2B})}\Big{)}& \mathrm{otherwise}.\end{cases}\)_For \(n=2\), we have \(\Psi(t)=\begin{cases}\frac{t+1}{2}\log(t+1)+\frac{1-t}{2}\log(1-t)&t\leq\frac{e^{2 B}-1}{e^{2B}+1}\\ \frac{1}{2}\log\Bigl{(}\frac{1+e^{2B}}{1+e^{-2B}}\Bigr{)}+\log\Bigl{(}\frac{2 \sqrt{(1+e^{2B})(1+e^{-2B})}}{2+e^{2B}+e^{-2B}}\Bigr{)}&\text{otherwise},\end{cases}\) which coincides with the \(\mathcal{H}_{\rm lin}\)-estimation error transformation in [Awasthi et al., 2022a]. Thus, Corollary 8 includes as a special case the \(\mathcal{H}_{\rm lin}\)-consistency bounds given by Awasthi et al. [2022a] for binary classification.

Our bounds of Corollary 8 improves upon the multi-class \(\mathcal{H}_{\rm lin}\)-consistency bounds of recent work [Zheng et al., 2023, Theorem 3.3] in the following ways: i) their bound holds only for restricted distributions while our bound holds for any distribution; ii) their bound holds only for restricted values of the estimation error \(\mathcal{R}_{\ell_{\log}}(h)-\mathcal{R}_{\ell_{\log}}^{*}(\mathcal{H}_{\rm lin})\) while ours holds for any value in \(\mathbb{R}\) and more precisely admits a piecewise functional form; iii) under their distributional assumption, the mimizability gaps \(\mathcal{M}_{\ell_{\log-1}}(\mathcal{H}_{\rm lin})\) and \(\mathcal{M}_{\ell_{\log}}(\mathcal{H}_{\rm lin})\) coincide with the approximation errors \(\mathcal{R}_{\ell_{0-1}}(\mathcal{H}_{\rm lin})-\mathcal{R}_{\ell_{0-1}}^{*}( \mathcal{H}_{\rm all})\) and \(\mathcal{R}_{\ell_{\log}}(\mathcal{H}_{\rm lin})-\mathcal{R}_{\ell_{\log}}^{*} (\mathcal{H}_{\rm all})\) respectively (see the note before [Zheng et al., 2023, Appendix F]). Thus, their bounds can be recovered as an excess error bound \(\mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}_{\ell_{0-1}}^{*}(\mathcal{H}_{\rm all })\leq\sqrt{2}\Bigl{[}\mathcal{R}_{\ell_{\log}}(h)-\mathcal{R}_{\ell_{\log}}^ {*}(\mathcal{H}_{\rm all})\Bigr{]}^{\frac{1}{2}}\), which is not specific to the hypothesis set \(\mathcal{H}\) and thus not as significant. In contrast, our \(\mathcal{H}_{\rm lin}\)-consistency bound is finer and takes into account the role of the parameter \(B\) as well as the number of labels \(n\); iv) [Zheng et al., 2023, Theorem 3.3] only offers approximate bounds that are not tight; in contrast, by Theorem 5, our bound is tight.

Note that our \(\mathcal{H}\)-consistency bound in Theorem 6 are not limited to specific hypothesis set forms. They are directly applicable to various types of hypothesis sets including neural networks. For example, the same derivation can be extended to one-hidden-layer neural networks studied in [Awasthi et al., 2022a] and their multi-class generalization by calculating and substituting the corresponding \(\Lambda(x)\). As a result, we can obtain novel and tight \(\mathcal{H}\)-consistency bounds for bounded neural network hypothesis sets in multi-class classification, which highlights the versatility of our general tools.

**B. Example: sum exponential loss**. We then consider the sum exponential loss, that is \(\ell^{\rm comp}\) with \(\Phi(u)=\frac{1-u}{u}\). By computing the error transformation in Theorem 5, we obtain the following result.

**Theorem 9** (\(\overline{\mathcal{H}}\)-consistency bounds for sum exponential loss).: _For any \(h\in\mathcal{H}\) and any distribution,_

\[\mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}_{\ell_{0-1}}^{*}\bigl{(}\overline{ \mathcal{H}}\bigr{)}+\mathcal{M}_{\ell_{0-1}}\bigl{(}\overline{\mathcal{H}} \bigr{)}\leq\Psi^{-1}\bigl{(}\mathcal{R}_{\ell_{\rm exp}}(h)-\mathcal{R}_{\ell _{\rm exp}}^{*}\bigl{(}\overline{\mathcal{H}}\bigr{)}+\mathcal{M}_{\ell_{\rm exp }}\bigl{(}\overline{\mathcal{H}}\bigr{)}\bigr{)}\]

_where \(\ell_{\rm exp}=\sum_{y^{\prime}\ast y}e^{h(x,y^{\prime})-h(x,y)}\) and \(\Psi(t)=\begin{cases}1-\sqrt{1-t^{2}}&t\leq\frac{s_{\rm max}^{2}-s_{\rm min}^{2 }}{s_{\rm min}^{2}+s_{\rm min}^{2}}\\ \frac{s_{\rm max}-s_{\rm min}}{2s_{\rm max}s_{\rm min}}t-\frac{(s_{\rm max}-s_ {\rm min})^{2}}{2s_{\rm max}s_{\rm min}(s_{\rm max}+s_{\rm min})}&\text{ otherwise}.\end{cases}\)_

The proof of Theorem 9 is given in Appendix E.3. Observe that \(1-\sqrt{1-t^{2}}\geq t^{2}/2\). By Theorem 9, making \(s_{\rm min}\) close to zero, that is making \(\Lambda\) close to infinite for any \(x\in\mathcal{X}\), essentially turns \(\Psi\) into a square function for all values. In general, \(\Lambda\) is not infinite since a regularization is used in practice, which controls both the complexity of the hypothesis set and the magnitude of the scores.

**C. Example: generalized cross-entropy loss and mean absolute error loss**. Due to space limitations, we present the results for these loss functions in Appendix E.

## 5 Constrained losses

In this section, we present a general characterization of \(\mathcal{H}\)-consistency bounds for _constrained loss_, that is loss functions defined via a constraint, as in [Lee et al., 2004]:

\[\ell^{\rm cstnd}(h,x,y)=\sum_{y^{\prime}\ast y}\Phi(-h(x,y^{\prime}))\] (5)

with the constraint that \(\sum_{y\in y}h(x,y)=0\) for a non-negative and non-increasing auxiliary function \(\Phi\).

### \(\mathcal{H}\)-consistency bounds

As in the previous section, we prove a result that supplies a very general tool, an _error transformation function_ for deriving \(\mathcal{H}\)-consistency bounds for constrained losses. When \(\mathcal{T}^{\rm cstnd}\) is convex, to make these guarantees explicit, we only need to calculate \(\mathcal{T}^{\rm cstnd}\).

**Theorem 10** (\(\mathcal{I}\)-consistency bound for constrained losses).: _Assume that \(\mathcal{H}\) is symmetric and complete. Assume that \(\mathcal{T}^{\mathrm{cstd}}\) is convex. Then, for any hypothesis \(h\in\mathcal{H}\) and any distribution,_

\[\mathcal{T}^{\mathrm{cstd}}\big{(}\mathcal{R}_{\ell_{0-1}}(h)- \mathcal{R}^{*}_{\ell_{0-1}}(\mathcal{H})+\mathcal{M}_{\ell_{0-1}}(\mathcal{H })\big{)}\leq\mathcal{R}_{\ell^{\mathrm{cstd}}}(h)-\mathcal{R}^{*}_{\ell^{ \mathrm{cstd}}}(\mathcal{H})+\mathcal{M}_{\ell^{\mathrm{cstd}}}(\mathcal{H})\]

_with \(\mathcal{H}\)-estimation error transformation for constrained losses defined on \(t\in[0,1]\) by \(\mathcal{T}^{\mathrm{cstd}}(t)=\begin{cases}\inf\limits_{\tau\geq 0}\sup_{\mu\in \mathbb{R}}\{\frac{1-t}{2}\big{[}\Phi(\tau)-\Phi(-\tau+\mu)\big{]}+\frac{1+t}{2} \big{[}\Phi(-\tau)-\Phi(\tau-\mu)\big{]}\}&n=2\\ \inf\limits_{P\in\big{[}\frac{1}{n-1},1\big{]}\tau_{1}\geq\max\{\tau_{2},0\}} \sup_{\mu\in\mathbb{R}}\{\frac{2-P-t}{2}\big{[}\Phi(-\tau_{2})-\Phi(-\tau_{1}+ \mu)\big{]}+\frac{2-P+t}{2}\big{[}\Phi(-\tau_{1})-\Phi(-\tau_{2}-\mu)\big{]} \}&n>2.\end{cases}\]

_Furthermore, for any \(t\in[0,1]\), there exist a distribution \(\mathcal{D}\) and a hypothesis \(h\in\mathcal{H}\) such that \(\mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}^{*}_{\ell_{0-1}}(\mathcal{H})+\mathcal{ M}_{\ell_{0-1}}(\mathcal{H})=t\) and \(\mathcal{R}_{\ell^{\mathrm{cstd}}}(h)-\mathcal{R}^{*}_{\ell^{\mathrm{cstd}}}( \mathcal{H})+\mathcal{M}_{\ell^{\mathrm{cstd}}}(\mathcal{H})=\mathcal{T}^{ \mathrm{cstd}}(t)\)._

Here too, the theorem guarantees the tightness of the bound. This general expression of \(\mathcal{T}^{\mathrm{cstd}}\) can be considerably simplified under some broad assumptions, as shown by the following result.

**Theorem 11** (**characterization of \(\mathcal{T}^{\mathrm{cstd}}\))**.: _Assume that \(\Phi\) is convex, differentiable at zero and \(\Phi^{\prime}(0)<0\). Then, \(\mathcal{T}^{\mathrm{cstd}}\) can be expressed as follows:_

\[\mathcal{T}^{\mathrm{cstd}}(t) =\begin{cases}\Phi(0)-\inf_{\mu\in\mathbb{R}}\big{\{}\frac{1-t}{ 2}\Phi(\mu)+\frac{1+t}{2}\Phi(-\mu)\big{\}}&n=2\\ \inf_{\tau\geq 0}\Big{\{}\big{(}2-\frac{1}{n-1}\big{)}\Phi(-\tau)-\inf_{\mu\in \mathbb{R}}\Big{\{}\frac{2-t-\frac{1}{n-1}}{2}\Phi\big{(}-\tau+\mu\big{)}+\frac {2+t-\frac{1}{n-1}}{2}\Phi\big{(}-\tau-\mu\big{)}\Big{\}}\Big{\}}&n>2\end{cases}\] \[\geq\begin{cases}\Phi(0)-\inf_{\mu\in\mathbb{R}}\big{\{}\frac{1-t }{2}\Phi(\mu)+\frac{1+t}{2}\Phi(-\mu)\big{\}}&n=2\\ \inf_{\tau\geq 0}\big{\{}2\Phi(-\tau)-\inf_{\mu\in\mathbb{R}}\big{\{}\frac{2-t }{2}\Phi(-\tau+\mu)+\frac{2+t}{2}\Phi(-\tau-\mu)\big{\}}\big{\}}&n>2.\end{cases}\]

The proof of all the results in this section are given in Appendix D.

**Examples.** We now compute the \(\mathcal{H}\)-estimation error transformation for constrained losses and present the results in Table 2. Here, we present the simplified \(\mathcal{T}^{\mathrm{cstd}}\) by using the lower bound in Theorem 11. Remarkably, by applying Theorem 10, we are able to obtain tighter \(\mathcal{H}\)-consistency bounds for constrained losses with \(\Phi=\Phi_{\mathrm{hinge}},\Phi_{\mathrm{sq-hinge}},\Phi_{\mathrm{exp}}\) than those derived using ad hoc methods in (Awasthi et al., 2022), and a novel \(\mathcal{H}\)-consistency bound for the new constrained loss \(\ell^{\mathrm{cstd}}(h,x,y)=\sum_{y^{\prime}\neq y}(1+h(x,y^{\prime}))^{2}\) with \(\Phi(t)=(1-t)^{2}\).

### Extension to non-complete or bounded hypothesis sets

As in the case of comp-sum losses, we extend our results beyond the completeness assumption adopted in previous work and establish \(\overline{\mathcal{H}}\)-consistency bounds for bounded hypothesis sets. This significantly broadens the applicability of our framework.

**Theorem 12** (\(\overline{\mathcal{H}}\)-consistency bound for constrained losses).: _Assume that \(\overline{\mathcal{T}}^{\mathrm{cstd}}\) is convex. Then, the following inequality holds for any hypothesis \(h\in\overline{\mathcal{H}}\) and any distribution:_

\[\overline{\mathcal{T}}^{\mathrm{cstd}}\big{(}\mathcal{R}_{\ell_{0-1} }(h)-\mathcal{R}^{*}_{\ell_{0-1}}(\overline{\mathcal{H}})+\mathcal{M}_{\ell_{0 -1}}(\overline{\mathcal{H}})\big{)}\leq\mathcal{R}_{\ell^{\mathrm{cstd}}}(h)- \mathcal{R}^{*}_{\ell^{\mathrm{cstd}}}(\overline{\mathcal{H}})+\mathcal{M}_{\ell^ {\mathrm{cstd}}}(\overline{\mathcal{H}}).\] (6)

_with \(\overline{\mathcal{T}}^{\mathrm{cstd}}\) the \(\overline{\mathcal{H}}\)-estimation error transformation for constrained losses defined for all \(t\in[0,1]\) by \(\overline{\mathcal{T}}^{\mathrm{cstd}}(t)=\begin{cases}\inf\limits_{\tau\geq 0} \sup_{\mu\in\{\tau-\Lambda_{\min},\tau+\Lambda_{\min}\}}\Big{\{}\frac{1-t}{2} \big{[}\Phi(\tau)-\Phi(-\tau+\mu)\big{]}+\frac{1+t}{2}\big{[}\Phi(-\tau)-\Phi( \tau-\mu)\big{]}\big{\}}&n=2\\ \inf\limits_{P\in\big{[}\frac{1}{n-1},1\big{]}\tau_{1}\geq\max\{\tau_{2},0\}} \sup_{\mu\in C}\{\frac{2-P-t}{2}\big{[}\Phi(-\tau_{2})-\Phi(-\tau_{1}+\mu) \big{]}+\frac{2-P+t}{2}\big{[}\Phi(-\tau_{1})-\Phi(-\tau_{2}-\mu)\big{]}\}&n>2, \end{cases}\]

_where \(C=\big{[}\max\{\tau_{1},-\tau_{2}\}-\Lambda_{\min},\min\{\tau_{1},-\tau_{2}\}+ \Lambda_{\min}\big{]}\) and \(\Lambda_{\min}=\inf_{x\in\mathcal{H}}\Lambda(x)\). Furthermore, for any \(t\in[0,1]\), there exist a distribution \(\mathcal{D}\) and a hypothesis \(h\in\mathcal{H}\) such that \(\mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}^{*}_{\ell_{0-1}}(\mathcal{H})+\mathcal{ M}_{\ell^{\mathrm{cstd}}}(h)-\mathcal{R}^{*}_{\ell^{\mathrm{cstd}}}( \mathcal{H})+\mathcal{M}_{\ell^{\mathrm{cstd}}}(\mathcal{H})=\mathcal{T}^{ \mathrm{cstd}}\big{(}t)\)._

\begin{table}
\begin{tabular}{l|l|l|l|l} \hline Auxiliary function \(\Phi\) & \(\Phi_{\mathrm{exp}}(t)=e^{-t}\) & \(\Phi_{\mathrm{hinge}}(t)=\max(0,1-t)\) & \(\Phi_{\mathrm{sq-hinge}}(t)=(1-t)^{2}\mathbb{1}_{t\leq 1}\) & \(\Phi_{\mathrm{sq}}=(1-t)^{2}\) \\ \hline Transformation \(\mathcal{T}^{\mathrm{cstd}}\) & \(\mathcal{T}^{\mathrm{cstd}}(t)=2-\sqrt{4-t^{2}}\) & \(\mathcal{T}^{\mathrm{cstd}}(t)=t\) & \(\mathcal{T}^{\mathrm{cstd}}(t)=\frac{t^{2}}{2}\) & \(\mathcal{T}^{\mathrm{cstd}}(t)=\frac{t^{2}}{2}\) \\ \hline \end{tabular}
\end{table}
Table 2: \(\mathcal{H}\)-estimation error transformation for common constrained losses.

The proof is presented in Appendix F.1. Next, we illustrate the application of our theory through an example of constrained exponential losses, that is \(\ell^{\mathrm{cstnd}}\) with \(\Phi(t)=e^{-t}\). By using the error transformation in Theorem 12, we obtain new \(\overline{\mathcal{H}}\)-consistency bounds in Theorem 13 (see Appendix F.2 for the proof) for bounded hypothesis sets \(\overline{\mathcal{H}}\).

**Theorem 13** (\(\overline{\mathcal{H}}\)**-consistency bounds for constrained exponential loss)**.: _Let \(\Phi(t)=e^{-t}\). For any \(h\in\overline{\mathcal{H}}\) and any distribution,_

\[\mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}_{\ell_{0-1}}^{*}\big{(} \overline{\mathcal{H}}\big{)}+\mathcal{M}_{\ell_{0-1}}\big{(}\overline{ \mathcal{H}}\big{)}\leq\Psi^{-1}\big{(}\mathcal{R}_{\ell^{\mathrm{cstnd}}}(h)- \mathcal{R}_{\ell^{\mathrm{cstnd}}}^{*}\big{(}\overline{\mathcal{H}}\big{)}+ \mathcal{M}_{\ell^{\mathrm{cstnd}}}\big{(}\overline{\mathcal{H}}\big{)}\big{)}\]

_where \(\Psi(t)=\begin{cases}1-\sqrt{1-t^{2}}&t\leq\frac{e^{2\Lambda_{\min}}-1}{e^{2 \Lambda_{\min}}+1}.\\ \frac{t}{2}\big{(}e^{\Lambda_{\min}}-e^{-\Lambda_{\min}}\big{)}+\frac{2-e^{ \lambda_{\min}}-e^{-\Lambda_{\min}}}{2}&\mathrm{otherwise}.\end{cases}\)_

Awasthi et al. (2022b) proves \(\mathcal{H}\)-consistency bounds for constrained exponential losses when \(\mathcal{H}\) is symmetric and complete. Theorem 13 significantly generalizes those results to the non-complete hypothesis sets. Different from the complete case, the functional form of our new bounds has two pieces which corresponds to the linear and the square root convergence respectively, modulo the constants. Furthermore, the coefficient of the linear piece depends on the the magnitude of \(\Lambda_{\min}\). When \(\Lambda_{\min}\) is small, the coefficient of the linear term in \(\Psi^{-1}\) explodes. On the other hand, making \(\Lambda_{\min}\) large essentially turns \(\Psi^{-1}\) into a square-root function.

## 6 Discussion

Here, we further elaborate on the practical value of our tools and \(\mathcal{H}\)-consistency bounds. Our contributions include a more general and convenient mathematical tool for proving \(\mathcal{H}\)-consistency bounds, along with tighter bounds that enable a better comparison of surrogate loss functions and extensions beyond previous completeness assumptions. As mentioned by (Awasthi et al., 2022b), given a hypothesis set \(\mathcal{H}\), \(\mathcal{H}\)-consistency bounds can be used to compare different surrogate loss functions and select the most favorable one, which depends on the functional form of the \(\mathcal{H}\)-consistency bound; the smoothness of the surrogate loss and its optimization properties; approximation properties of the surrogate loss function controlled by minimizability gaps; and the dependency on the number of classes in the multiplicative constant. Consequently, a tighter \(\mathcal{H}\)-consistency bound provides a more accurate comparison, as a loose bound might not adequately capture the full advantage of using one surrogate loss. In contrast, Bayes-consistency does not take into account the hypothesis set and is an asymptotic property, thereby failing to guide the comparison of different surrogate losses.

Another application of our \(\mathcal{H}\)-consistency bounds involves deriving generalization bounds for surrogate loss minimizers (Mao et al., 2023), expressed in terms of the same quantities previously discussed. Therefore, when dealing with finite samples, a tighter \(\mathcal{H}\)-consistency bound could also result in a corresponding tighter generalization bound. Moreover, our novel results extend beyond previous completeness assumptions, offering guarantees applicable to bounded hypothesis sets commonly used with regularization. This enhancement provides meaningful learning guarantees. Technically, our error transformation function serves as a very general tool for deriving \(\mathcal{H}\)-consistency bounds with tightness guarantees. These functions are defined within each class of loss functions including comp-sum losses and constrained losses, and their formulation depends on the structure of the individual loss function class, the range of the hypothesis set and the number of classes. To derive explicit bounds, all that is needed is to calculate these error transformation functions. Under some broad assumptions on the auxiliary function within a loss function, these error transformation functions can be further distilled into more simplified forms, making them straightforward to compute.

## 7 Conclusion

We presented a general characterization and extension of \(\mathcal{H}\)-consistency bounds for multi-class classification. We introduced new tools for deriving such bounds with tightness guarantees and illustrated their benefits through several applications and examples. Our proposed method is a significant advance in the theory of \(\mathcal{H}\)-consistency bounds for multi-class classification. It can provide a general and powerful tool for deriving tight bounds for a wide variety of other loss functions and hypothesis sets. We believe that our work will open up new avenues of research in the field of multi-class classification consistency.

## References

* Agarwal and Agarwal (2015) A. Agarwal and S. Agarwal. On consistent surrogate risk minimization and property elicitation. In _Conference on Learning Theory_, pages 4-22, 2015.
* Awasthi et al. (2021a) P. Awasthi, N. Frank, A. Mao, M. Mohri, and Y. Zhong. Calibration and consistency of adversarial surrogate losses. In _Advances in Neural Information Processing Systems_, pages 9804-9815, 2021a.
* Awasthi et al. (2021b) P. Awasthi, A. Mao, M. Mohri, and Y. Zhong. A finer calibration analysis for adversarial robustness. _arXiv preprint arXiv:2105.01550_, 2021b.
* Awasthi et al. (2022a) P. Awasthi, A. Mao, M. Mohri, and Y. Zhong. \(\mathcal{H}\)-consistency bounds for surrogate loss minimizers. In _International Conference on Machine Learning_, pages 1117-1174, 2022a.
* Awasthi et al. (2022b) P. Awasthi, A. Mao, M. Mohri, and Y. Zhong. Multi-class \(\mathcal{H}\)-consistency bounds. In _Advances in neural information processing systems_, 2022b.
* Awasthi et al. (2023a) P. Awasthi, A. Mao, M. Mohri, and Y. Zhong. Theoretically grounded loss functions and algorithms for adversarial robustness. In _International Conference on Artificial Intelligence and Statistics_, pages 10077-10094, 2023a.
* Awasthi et al. (2023b) P. Awasthi, A. Mao, M. Mohri, and Y. Zhong. DC-programming for neural network optimizations. _Journal of Global Optimization_, 2023b.
* Bartlett et al. (2006) P. L. Bartlett, M. I. Jordan, and J. D. McAuliffe. Convexity, classification, and risk bounds. _Journal of the American Statistical Association_, 101(473):138-156, 2006.
* Berkson (1944) J. Berkson. Application of the logistic function to bio-assay. _Journal of the American Statistical Association_, 39:357---365, 1944.
* Berkson (1951) J. Berkson. Why I prefer logits to probits. _Biometrics_, 7(4):327---339, 1951.
* Blondel (2019) M. Blondel. Structured prediction with projection oracles. In _Advances in neural information processing systems_, 2019.
* Chen and Sun (2006) D.-R. Chen and T. Sun. Consistency of multiclass empirical risk minimization methods based on convex loss. _Journal of Machine Learning Research_, 7:2435-2447, 2006.
* Chen and Xiang (2006) D.-R. Chen and D.-H. Xiang. The consistency of multicategory support vector machines. _Advances in Computational Mathematics_, 24(1):155-169, 2006.
* Ciliberto et al. (2016) C. Ciliberto, L. Rosasco, and A. Rudi. A consistent regularization approach for structured prediction. In _Advances in neural information processing systems_, 2016.
* Cortes et al. (2016a) C. Cortes, G. DeSalvo, and M. Mohri. Learning with rejection. In _Algorithmic Learning Theory_, pages 67-82, 2016a.
* Cortes et al. (2016b) C. Cortes, G. DeSalvo, and M. Mohri. Boosting with abstention. In _Advances in Neural Information Processing Systems_, pages 1660-1668, 2016b.
* Cortes et al. (2023) C. Cortes, G. DeSalvo, and M. Mohri. Theory and algorithms for learning with rejection in binary classification. _Annals of Mathematics and Artificial Intelligence_, to appear, 2023.
* Crammer and Singer (2001) K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based vector machines. _Journal of machine learning research_, 2(Dec):265-292, 2001.
* Dembczynski et al. (2012) K. Dembczynski, W. Kotlowski, and E. Hullermeier. Consistent multilabel ranking through univariate losses. _arXiv preprint arXiv:1206.6401_, 2012.
* Dogan et al. (2016) U. Dogan, T. Glasmachers, and C. Igel. A unified view on multi-class support vector classification. _Journal of Machine Learning Research_, 17:1-32, 2016.
* Finocchiaro et al. (2022) J. Finocchiaro, R. M. Frongillo, and B. Waggoner. An embedding framework for the design and analysis of consistent polyhedral surrogates. _arXiv preprint arXiv:2206.14707_, 2022.
* Fonseca et al. (2016)

[MISSING_PAGE_FAIL:12]

B. A. Pires and C. Szepesvari. Multiclass classification calibration functions. _arXiv preprint arXiv:1609.06385_, 2016.
* Pires et al. [2013] B. A. Pires, C. Szepesvari, and M. Ghavamzadeh. Cost-sensitive multiclass classification risk bounds. In _International Conference on Machine Learning_, pages 1391-1399, 2013.
* Ramaswamy and Agarwal [2012] H. G. Ramaswamy and S. Agarwal. Classification calibration dimension for general multiclass losses. In _Advances in Neural Information Processing Systems_, 2012.
* Ramaswamy and Agarwal [2016] H. G. Ramaswamy and S. Agarwal. Convex calibration dimension for multiclass loss matrices. _Journal of Machine Learning Research_, 17(1):397-441, 2016.
* Ramaswamy et al. [2013] H. G. Ramaswamy, S. Agarwal, and A. Tewari. Convex calibrated surrogates for low-rank loss matrices with applications to subset ranking losses. In _Advances in Neural Information Processing Systems_, 2013.
* Ramaswamy et al. [2015] H. G. Ramaswamy, A. Tewari, and S. Agarwal. Consistent algorithms for multiclass classification with a reject option. _arXiv preprint arXiv:1505.04137_, 2015.
* Ravikumar et al. [2011] P. Ravikumar, A. Tewari, and E. Yang. On NDCG consistency of listwise ranking methods. In _International Conference on Artificial Intelligence and Statistics_, pages 618-626, 2011.
* Steinwart [2007] I. Steinwart. How to compare different loss functions and their risks. _Constructive Approximation_, 26(2):225-287, 2007.
* Tewari and Bartlett [2007] A. Tewari and P. L. Bartlett. On the consistency of multiclass classification methods. _Journal of Machine Learning Research_, 8(36):1007-1025, 2007.
* Thilagar et al. [2022] A. Thilagar, R. Frongillo, J. J. Finocchiaro, and E. Goodwill. Consistent polyhedral surrogates for top-k classification and variants. In _International Conference on Machine Learning_, pages 21329-21359, 2022.
* Uematsu and Lee [2017] K. Uematsu and Y. Lee. On theoretically optimal ranking functions in bipartite ranking. _Journal of the American Statistical Association_, 112(519):1311-1322, 2017.
* Verhulst [1838] P. F. Verhulst. Notice sur la loi que la population suit dans son accroissement. _Correspondance mathematique et physique_, 10:113---121, 1838.
* Verhulst [1845] P. F. Verhulst. Recherches mathematiques sur la loi d'accroissement de la population. _Nouveaux Memoires de l'Academie Royale des Sciences et Belles-Lettres de Bruxelles_, 18:1---42, 1845.
* Wang and Scott [2020] Y. Wang and C. Scott. Weston-Watkins hinge loss and ordered partitions. In _Advances in neural information processing systems_, pages 19873-19883, 2020.
* Wang and Scott [2023] Y. Wang and C. D. Scott. On classification-calibration of gamma-phi losses. _arXiv preprint arXiv:2302.07321_, 2023.
* Weston and Watkins [1998] J. Weston and C. Watkins. Multi-class support vector machines. Technical report, Citeseer, 1998.
* Williamson et al. [2016] R. C. Williamson, E. Vernet, and M. D. Reid. Composite multiclass losses. _Journal of Machine Learning Research_, 17:1-52, 2016.
* Zhang and Agarwal [2020] M. Zhang and S. Agarwal. Bayes consistency vs. H-consistency: The interplay between surrogate loss functions and the scoring function class. In _Advances in Neural Information Processing Systems_, pages 16927-16936, 2020.
* Zhang et al. [2020] M. Zhang, H. G. Ramaswamy, and S. Agarwal. Convex calibrated surrogates for the multi-label f-measure. In _International Conference on Machine Learning_, pages 11246-11255, 2020.
* Zhang [2004a] T. Zhang. Statistical behavior and consistency of classification methods based on convex risk minimization. _The Annals of Statistics_, 32(1):56-85, 2004a.
* Zhang [2004b] T. Zhang. Statistical analysis of some multi-category large margin classification methods. _Journal of Machine Learning Research_, 5(Oct):1225-1251, 2004b.

Z. Zhang and M. Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy labels. In _Advances in neural information processing systems_, 2018.
* Zheng et al. [2023] C. Zheng, G. Wu, F. Bao, Y. Cao, C. Li, and J. Zhu. Revisiting discriminative vs. generative classifiers: Theory and implications. In _International Conference on Machine Learning_, 2023.

###### Contents of Appendix

* A Related work
* B Minimizability gap
* B.1 Zero minimizability
* B.2 Relationship with approximation error
* B.3 Significance of \(\mathcal{H}\)-consistency bounds
* C Proofs for comp-sum losses
* C.1 Proof of \(\mathcal{H}\)-consistency bounds with \(\mathcal{T}^{\mathrm{comp}}\) (Theorem 2)
* C.2 Characterization of \(\mathcal{T}^{\mathrm{comp}}\) (Theorem 3)
* C.3 Computation of examples
* D Proofs for constrained losses
* D.1 Proof of \(\mathcal{H}\)-consistency bounds with \(\mathcal{T}^{\mathrm{cstd}}\) (Theorem 10)
* D.2 Characterization of \(\mathcal{T}^{\mathrm{cstd}}\) (Theorem 11)
* D.3 Computation of examples
* E Extensions of comp-sum losses
* E.1 Proof of \(\overline{\mathcal{H}}\)-consistency bounds with \(\overline{\mathcal{T}}^{\mathrm{comp}}\) (Theorem 5)
* E.2 Logistic loss
* E.3 Sum exponential loss
* E.4 Generalized cross-entropy loss
* E.5 Mean absolute error loss
* F Extensions of constrained losses
* F.1 Proof of \(\overline{\mathcal{H}}\)-consistency bound with \(\overline{\mathcal{T}}^{\mathrm{cstd}}\) (Theorem 12)
* F.2 Constrained exponential loss
Related work

The notions of Bayes-consistency (also known as consistency) and calibration have been well studied not only with respect to the binary zero-one loss (Zhang, 2004a; Bartlett et al., 2006; Steinwart, 2007; Mohri et al., 2018), but also with respect to the multi-class zero-one loss (Zhang, 2004b; Tewari and Bartlett, 2007), the general multi-class losses (Ramaswamy and Agarwal, 2012; Narasimhan et al., 2015; Ramaswamy and Agarwal, 2016), the multi-class SVMs (Chen and Sun, 2006; Chen and Xiang, 2006; Liu, 2007; Dogan et al., 2016; Wang and Scott, 2020), the gamma-phi losses (Wang and Scott, 2023), the multi-label losses (Gao and Zhou, 2011; Dembczynski et al., 2012; Zhang et al., 2020), the losses with a reject option (Ramaswamy et al., 2015; Cortes et al., 2016a, b, 2023), the ranking losses (Ravikumar et al., 2011; Ramaswamy et al., 2013; Gao and Zhou, 2015; Uematsu and Lee, 2017), the cost sensitive losses (Pires et al., 2013; Pires and Szepesvari, 2016), the structured losses (Ciliberto et al., 2016; Osokin et al., 2017; Blondel, 2019), the polyhedral losses (Frongillo and Waggoner, 2021; Finocchiaro et al., 2022), the Top-\(k\) classification losses (Thilagar et al., 2022), the proper losses (Agarwal and Agarwal, 2015; Williamson et al., 2016) and the losses of ordinal regression (Pedregosa et al., 2017).

Bayes-consistency only holds for the full family of measurable functions, which of course is distinct from the more restricted hypothesis set used by a learning algorithm. Therefore, a hypothesis set-dependent notion of \(\mathcal{H}\)-consistency has been proposed by Long and Servedio (2013) in the realizable setting, used by Zhang and Agarwal (2020) for linear models, and generalized by Kuznetsov et al. (2014) to the structured prediction case. Long and Servedio (2013) showed that there exists a case where a Bayes-consistent loss is not \(\mathcal{H}\)-consistent while inconsistent losses can be \(\mathcal{H}\)-consistent. Zhang and Agarwal (2020) further investigated the phenomenon in (Long and Servedio, 2013) and showed that the situation of losses that are not \(\mathcal{H}\)-consistent with linear models can be remedied by carefully choosing a larger piecewise linear hypothesis set. Kuznetsov et al. (2014) proved positive results for the \(\mathcal{H}\)-consistency of several multi-class ensemble algorithms, as an extension of \(\mathcal{H}\)-consistency results in (Long and Servedio, 2013).

Recently, Awasthi et al. (2022a, b), Mao et al. (2023), Zheng et al. (2023) presented a series of results providing \(\mathcal{H}\)-consistency bounds. These are upper bounds on the zero-one estimation error of any predictor in a hypothesis set, expressed in terms of its surrogate loss estimation error. They are more informative guarantees than similar excess error bounds derived in the literature, which correspond to the special case where \(\mathcal{H}\) is the family of all measurable functions (Zhang, 2004a; Bartlett et al., 2006; Mohri et al., 2018). Awasthi et al. (2022a) studied \(\mathcal{H}\)-consistency bounds in binary classification. They provided a series of _tight_\(\mathcal{H}\)-consistency bounds for _bounded_ hypothesis set of linear models and one-hidden-layer neural networks. The subsequent study (Awasthi et al., 2022b) further generalized the framework to multi-class classification, where they presented a extensive study of \(\mathcal{H}\)-consistency bounds for diverse multi-class surrogate losses including negative results for _max losses_(Crammer and Singer, 2001) and positive results for _sum losses_(Weston and Watkins, 1998), and _constrained losses_(Lee et al., 2004). However, the hypothesis sets adopted there were assumed to be complete, which rules out the bounded hypothesis sets typically used in practice. Moreover, the final bounds derived from (Awasthi et al., 2022b) are based on ad hoc methods and may not be tight. (Mao et al., 2023) complemented the previous work by studying a wide family of _comp-sum losses_ in the multi-class classification, which generalized the _sum-losses_ and included as special cases the logistic loss (Verhulst, 1838, 1845; Berkson, 1944, 1951), the _generalized cross-entropy loss_(Zhang and Sabuncu, 2018), and the _mean absolute error loss_(Ghosh et al., 2017). Here too, the completeness assumption on the hypothesis sets was adopted and their \(\mathcal{H}\)-consistency bounds do not apply to common bounded hypothesis sets in practice. Zheng et al. (2023) proved \(\mathcal{H}\)-consistency bounds for multi-class logistic loss with bounded linear hypothesis sets. However, their bounds require a crucial distributional assumption under which, the minimizability gaps coincide with the approximation errors. Thus, their bounds can be recovered as excess error bounds, which are less significant.

This paper provides both a general characterization and an extension of \(\mathcal{H}\)-consistency bounds for multi-class classification. Our general tools and tight bounds show several remarkable advantages: first, they improve existing bounds for complete hypothesis sets previously proven in (Awasthi et al., 2022b), second, they encompass all previously comp-sum and constrained losses studied thus far as well as many new ones (Awasthi et al., 2022a; Mao et al., 2023), third, they extend beyond the completeness assumption adopted in previous work, fourth, they give novel guarantees for bounded hypothesis sets, and finally they help prove a much stronger and more significant guarantee for logistic loss with linear hypothesis set than [Zheng et al., 2023].

Other related work on \(\mathcal{H}\)-consistency bounds includes: \(\mathcal{H}\)-consistency bounds for pairwise ranking [Mao et al., 2023d,e]; theoretically grounded surrogate losses and algorithms for learning with abstention supported by \(\mathcal{H}\)-consistency bounds, including the study of score-based abstention [Mao et al., 2023f], predictor-rejector abstention [Mao et al., 2023c] and learning to abstain with a fixed predictor with application in decontextualization [Mohri et al., 2023]; principled approaches for learning to defer with multiple experts that benefit from strong \(\mathcal{H}\)-consistency bounds, including the single-stage scenario [Mao et al., 2023b] and a two-stage scenario [Mao et al., 2023a]; \(\mathcal{H}\)-consistency theory and algorithms for adversarial robustness [Awasthi et al., 2021a,b, 2023a, Mao et al., 2023b, Awasthi et al., 2023b]; and efficient algorithms and loss functions for structured prediction with stronger \(\mathcal{H}\)-consistency guarantees [Mao et al., 2023g].

## Appendix B Minimizability gap

This is a brief discussion of minimizability gaps and their properties. By definition, for any loss function \(\ell\), the minimizability gap is defined by

\[\mathcal{M}_{\ell}(\mathcal{H})=\inf_{h\in\mathcal{H}}\Biggl{\{}\underset{(x, y)\sim\mathcal{D}}{\mathbb{E}}[\ell(h,x,y)]\Biggr{\}}-\underset{x}{\mathbb{E}} \Biggl{[}\inf_{h\in\mathcal{H}}\underset{y|x}{\mathbb{E}}[\ell(h,x,y)]\Biggr{]} =\mathcal{R}_{\ell}^{*}(\mathcal{H})-\underset{x}{\mathbb{E}}[\mathcal{C}_{ \ell}^{*}(\mathcal{H},x)].\]

### Zero minimizability

**Lemma 14**.: _Let \(\ell\) be a surrogate loss such that for \((x,y)\in\mathcal{X}\times\mathcal{Y}\) and any measurable function \(h\in\mathcal{H}_{\mathrm{all}}\), the loss \(\ell(h,x,y)\) only depends on \(h(x)\) and \(y\) (thus we can write \(\ell(h,x,y)=\overline{\ell}(h(x),y)\) for some function \(\overline{\ell}\)). Then, the minimizability gap vanishes: \(\mathcal{M}_{\ell}(\mathcal{H}_{\mathrm{all}})=0\)._

Proof.: Fix \(\epsilon>0\). Then, by definition of the infimum, for any \(x\in\mathcal{X}\), there exists \(h_{x}\in\mathcal{H}_{\mathrm{all}}\) such that

\[\underset{y|x}{\mathbb{E}}[\ell(h_{x},x,y)]\leq\inf_{h\in\mathcal{H}_{\mathrm{all }}}\underset{y|x}{\mathbb{E}}[\ell(h,x,y)]+\epsilon.\]

Now, define the function \(h\) by \(h(x)=h_{x}(x)\), for all \(x\in\mathcal{X}\). \(h\) can be shown to be measurable, for example, when \(\mathcal{X}\) admits a countable dense subset. Then,

\[\underset{(x,y)\sim\mathcal{D}}{\mathbb{E}}[\ell(h,x,y)]= \underset{(x,y)\sim\mathcal{D}}{\mathbb{E}}[\overline{\ell}(h(x),y)] =\underset{(x,y)\sim\mathcal{D}}{\mathbb{E}}[\overline{\ell}(h_{x}(x),y)]\] \[=\underset{(x,y)\sim\mathcal{D}}{\mathbb{E}}[\ell(h_{x},x,y)]\] \[\leq\underset{x}{\mathbb{E}}\Biggl{[}\inf_{h\in\mathcal{H}_{ \mathrm{all}}}\underset{y|x}{\mathbb{E}}[\ell(h,x,y)]+\epsilon\Biggr{]}\] \[=\underset{x}{\mathbb{E}}[\mathcal{C}_{\ell}^{*}(\mathcal{H}_{ \mathrm{all}},x)]+\epsilon.\]

Thus, we have

\[\inf_{h\in\mathcal{H}_{\mathrm{all}}}\underset{(x,y)\sim\mathcal{D}}{\mathbb{E} }[\ell(h,x,y)]\leq\underset{x}{\mathbb{E}}[\mathcal{C}_{\ell}^{*}(\mathcal{H} _{\mathrm{all}},x)]+\epsilon.\]

Since the inequality holds for any \(\epsilon>0\), we have \(\mathcal{R}_{\ell}^{*}(\mathcal{H}_{\mathrm{all}})=\inf_{h\in\mathcal{H}_{ \mathrm{all}}}\underset{(x,y)\sim\mathcal{D}}{\mathbb{E}}_{(x,y)\sim\mathcal{ D}}[\ell(h,x,y)]\leq\underset{x}{\mathbb{E}}_{x}[\mathcal{C}_{\ell}^{*}(\mathcal{H}_{ \mathrm{all}},x)]\). This implies equality since the inequality \(\mathcal{R}_{\ell}^{*}(\mathcal{H})\geq\underset{x}{\mathbb{E}}_{x}[ \mathcal{C}_{\ell}^{*}(\mathcal{H},x)]\) holds for any \(\mathcal{H}\). 

### Relationship with approximation error

Let \(\mathcal{A}_{\ell}\) denote the approximation error of a loss function \(\ell\) and a hypothesis set \(\mathcal{H}\): \(\mathcal{A}_{\ell}(\mathcal{H})=\mathcal{R}_{\ell}^{*}(\mathcal{H})-\mathcal{R }_{\ell}^{*}(\mathcal{H}_{\mathrm{all}})\). We will denote by \(I_{\ell}(\mathcal{H})\) the difference of pointwise infima \(I_{\ell}(\mathcal{H})=\underset{x}{\mathbb{E}}_{x}\left[\mathcal{C}_{\ell}^{*}( \mathcal{H},x)-\mathcal{C}_{\ell}^{*}(\mathcal{H}_{\mathrm{all}},x)\right]\), which is non-negative. The minimizability gap can be decomposed as follows in terms of the approximation error and the difference of pointwise infima:

\[\mathcal{M}_{\ell}(\mathcal{M}) =\mathcal{R}_{\ell}^{*}(\mathcal{M})-\mathbb{E}_{x}\left[\mathcal{C} _{\ell}^{*}(\mathcal{M},x)\right]\] \[=\mathcal{R}_{\ell}^{*}(\mathcal{M})-\mathcal{R}_{\ell}^{*}( \mathcal{H}_{\mathrm{all}})+\mathcal{R}_{\ell}^{*}(\mathcal{H}_{\mathrm{all}}) -\mathbb{E}_{x}\left[\mathcal{C}_{\ell}^{*}(\mathcal{M},x)\right]\] \[=\mathcal{A}_{\ell}(\mathcal{M})+\mathcal{R}_{\ell}^{*}( \mathcal{H}_{\mathrm{all}})-\mathbb{E}_{x}\left[\mathcal{C}_{\ell}^{*}( \mathcal{M},x)\right]\] \[=\mathcal{A}_{\ell}(\mathcal{M})+\mathbb{E}_{x}\left[\mathcal{C} _{\ell}^{*}(\mathcal{H}_{\mathrm{all}},x)-\mathcal{C}_{\ell}^{*}(\mathcal{M}, x)\right]\] (By Lemma 14) \[=\mathcal{A}_{\ell}(\mathcal{M})-I_{\ell}(\mathcal{M}).\]

The decomposition immediately implies the following result.

**Lemma 15**.: _Let \(\ell\) be a surrogate loss such that for \((x,y)\in\mathcal{X}\times\mathbb{Y}\) and any measurable function \(h\in\mathcal{H}_{\mathrm{all}}\), the loss \(\ell(h,x,y)\) only depends on \(h(x)\) and \(y\) (thus we can write \(\ell(h,x,y)=\bar{\ell}(h(x),y)\) for some function \(\overline{\ell}\)). Then, for any loss function \(\ell\) and hypothesis set \(\mathcal{H}\), we have: \(\mathcal{M}_{\ell}(\mathcal{H})\leq\mathcal{A}_{\ell}(\mathcal{H})\)._

By Lemma 1, when \(\ell\) is the zero-one loss, \(I_{\ell}(\mathcal{H})=0\) when the hypothesis set generates labels that cover all possible outcomes for each input. However, for a surrogate loss function, \(I_{\ell}(\mathcal{H})\) is non-negative, and is generally non-zero.

Take the example of binary classification and denote the conditional distribution as \(\eta(x)=D(Y=1|X=x)\). Let \(\mathcal{H}\) be a family of functions \(h\) with \(|h(x)|\leq\Lambda\) for all \(x\in\mathcal{X}\) and such that all values in \([-\Lambda,+\Lambda]\) can be reached. Consider for example the exponential-based margin loss: \(\ell(h,x,y)=e^{-yh(x)}\). Then, \(\mathcal{C}_{\ell}(h,x)=\eta(x)e^{-h(x)}+(1-\eta(x))e^{h(x)}\). Upon observing this, it becomes apparent that the infimum over all measurable functions can be expressed in the following way, for all \(x\):

\[\mathcal{C}_{\ell}^{*}(\mathcal{H}_{\mathrm{all}},x)=2\sqrt{\eta(x)(1-\eta(x) )},\]

while the infimum over \(\mathcal{H}\), \(\mathcal{C}_{\ell}^{*}(\mathcal{H},x)\), depends on \(\Lambda\) and can be expressed as

\[\mathcal{C}_{\ell}^{*}(\mathcal{H},x)=\begin{cases}\max\{\eta(x),1-\eta(x)\}e ^{-\Lambda}+\min\{\eta(x),1-\eta(x)\}e^{\Lambda}&\Lambda<\frac{1}{2}\Big{|} \log\frac{\eta(x)}{1-\eta(x)}\Big{|}\\ 2\sqrt{\eta(x)(1-\eta(x))}&\text{otherwise}.\end{cases}\]

Thus, in the deterministic scenario,

\[I_{\ell}(\mathcal{H})=\mathbb{E}_{x}[\mathcal{C}_{\ell}^{*}(\mathcal{H},x)- \mathcal{C}_{\ell}^{*}(\mathcal{H}_{\mathrm{all}},x)]=e^{-\Lambda}.\]

### Significance of \(\mathcal{H}\)-consistency bounds

As shown in the previous section, for target loss \(\ell_{0-1}\), the minimizability gap coincides with the approximation error \(\mathcal{M}_{\ell_{0-1}}(\mathcal{H})=\mathcal{A}_{\ell_{0-1}}(\mathcal{H})\) when the hypothesis set generates labels that cover all possible outcomes for each input. However, for a surrogate loss \(\ell\), the minimizability gap is generally strictly less than the approximation error \(\mathcal{M}_{\ell}(\mathcal{H})<\mathcal{A}_{\ell}(\mathcal{H})\). Thus, an \(\mathcal{H}\)-consistency bound, expressed as follows for some increasing function \(\Gamma\):

\[\mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}_{\ell_{0-1}}^{*}(\mathcal{H})+ \mathcal{M}_{\ell_{0-1}}(\mathcal{H})\leq\Gamma(\mathcal{R}_{\ell}(h)- \mathcal{R}_{\ell}^{*}(\mathcal{H})+\mathcal{M}_{\ell}(\mathcal{H})).\]

is more favorable than an excess error bound expressed in terms of approximation errors:

\[\mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}_{\ell_{0-1}}^{*}(\mathcal{H})+ \mathcal{A}_{\ell_{0-1}}(\mathcal{H})\leq\Gamma(\mathcal{R}_{\ell}(h)- \mathcal{R}_{\ell}^{*}(\mathcal{H})+\mathcal{A}_{\ell}(\mathcal{H})).\]

Here, \(\Gamma\) is typically linear or the square-root function modulo constants. When \(\mathcal{H}=\mathcal{H}_{\mathrm{all}}\), the family of all measurable functions, by Lemma 14, the \(\mathcal{H}\)-consistency bound coincides with the excess error bound and implies Bayes-consistency by taking the limit. It is therefore a stronger guarantee than an excess error bound and Bayes-consistency.

## Appendix C Proofs for comp-sum losses

Let \(y_{\max}=\operatorname*{argmax}_{y\in\mathbb{Y}}p(x,y)\) and \(\mathsf{h}(x)=\operatorname*{argmax}_{y\in\mathbb{Y}}h(x,y)\), where we choose the label with the highest index under the natural ordering of labels as the tie-breaking strategy.

### Proof of \(\mathcal{H}\)-consistency bounds with \(\mathcal{T}^{\mathrm{comp}}\) (Theorem 2)

**Theorem 2** (\(\mathcal{H}\)-consistency bound for comp-sum losses).: _Assume that \(\mathcal{H}\) is symmetric and complete and that \(\mathcal{T}^{\mathrm{comp}}\) is convex. Then, the following inequality holds for any hypothesis \(h\in\mathcal{H}\) and any distribution_

\[\mathcal{T}^{\mathrm{comp}}\big{(}\mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}_{ \ell_{0-1}}^{*}(\mathcal{H})+\mathcal{M}_{\ell_{0-1}}(\mathcal{H})\big{)} \leq\mathcal{R}_{\ell^{\mathrm{comp}}}(h)-\mathcal{R}_{\ell^{\mathrm{comp}}}^{ *}(\mathcal{H})+\mathcal{M}_{\ell^{\mathrm{comp}}}(\mathcal{H}),\] (3)

_with \(\mathcal{T}^{\mathrm{comp}}\) an \(\mathcal{H}\)-estimation error transformation for comp-sum losses defined for all \(t\in[0,1]\) by_

\[\mathcal{T}^{\mathrm{comp}}(t)=\] \[\begin{cases}\inf_{\tau\in[0,\frac{1}{2}]}\sup_{\mu\in[-\tau,1- \tau]}\bigl{\{}\frac{1+t}{2}\bigl{[}\Phi(\tau)-\Phi(1-\tau-\mu)\bigr{]}+\frac{ 1-t}{2}\bigl{[}\Phi(1-\tau)-\Phi(\tau+\mu)\bigr{]}\bigr{\}}&n=2\\ \inf_{P\in[\frac{1}{n-1}\lor t,1]}\inf_{\tau_{1}\geq\max\{\tau_{2},1/n\}\atop \tau_{1}+\tau_{2}\leq 1,\tau_{2}\geq 0}\sup_{\mu\in[-\tau_{2},\tau_{1}]} \bigl{\{}\frac{P+t}{2}\bigl{[}\Phi(\tau_{2})-\Phi(\tau_{1}-\mu)\bigr{]}+\frac{ P-t}{2}\bigl{[}\Phi(\tau_{1})-\Phi(\tau_{2}+\mu)\bigr{]}\bigr{\}}&n>2.\end{cases}\]

_Furthermore, for any \(t\in[0,1]\), there exist a distribution \(\mathcal{D}\) and a hypothesis \(h\in\mathcal{H}\) such that \(\mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}_{\ell_{0-1}}^{*}(\mathcal{H})+ \mathcal{M}_{\ell_{0-1}}(\mathcal{H})=t\) and \(\mathcal{R}_{\ell^{\mathrm{comp}}}(h)-\mathcal{R}_{\ell^{\mathrm{comp}}}^{*}( \mathcal{H})+\mathcal{M}_{\ell^{\mathrm{comp}}}(\mathcal{H})=\mathcal{T}^{ \mathrm{comp}}(t)\)._

Proof.: For the comp-sum loss \(\ell^{\mathrm{comp}}\), the conditional \(\ell^{\mathrm{comp}}\)-risk can be expressed as follows:

\[\mathcal{C}_{\ell^{\mathrm{comp}}}(h,x) =\sum_{y\in\mathcal{Y}}p(x,y)\ell^{\mathrm{comp}}(h,x,y)\] \[=\sum_{y\in\mathcal{Y}}p(x,y)\Phi\biggl{(}\frac{e^{h(x,y)}}{\sum_ {y^{\prime}\in\mathcal{Y}}e^{h(x,y^{\prime})}}\biggr{)}\] \[=\sum_{y\in\mathcal{Y}}p(x,y)\Phi(S_{h}(x,y))\] \[=p(x,y_{\max})\Phi(S_{h}(x,y_{\max}))+p(x,\mathsf{h}(x))\Phi(S_{h }(x,\mathsf{h}(x)))\] \[\qquad+\sum_{yt\{y_{\max},\mathsf{h}(x)\}}p(x,y)\Phi(S_{h}(x,y)).\]

where we let \(S_{h}(x,y)=\frac{e^{h(x,y)}}{\sum_{y^{\prime}\in\mathcal{Y}}e^{h(x,y^{\prime}) }}\) for any \(y\in\mathcal{Y}\) with the constraint that \(\sum_{y\in\mathcal{Y}}S_{h}(x,y)=1\). For any \(h\in\mathcal{H}\) such that \(\mathsf{h}(x)\neq y_{\max}\) and \(x\in\mathcal{X}\), we can always find a family of hypotheses \(\{h_{\mu}\}\subset\mathcal{H}\) such that \(S_{h,\mu}(x,\cdot)=\frac{e^{h_{\mu}(x,\cdot)}}{\sum_{y^{\prime}\neq y}e^{h_{\mu} (x,y^{\prime})}}\) take the following values:

\[S_{h,\mu}(x,y)=\begin{cases}S_{h}(x,y)&\text{if }y\not\in\{y_{\max},\mathsf{h}(x)\}\\ S_{h}(x,y_{\max})+\mu&\text{if }y=\mathsf{h}(x)\\ S_{h}(x,\mathsf{h}(x))-\mu&\text{if }y=y_{\max}.\end{cases}\]

Note that \(S_{h,\mu}\) satisfies the constraint:

\[\sum_{y\in\mathcal{Y}}S_{h,\mu}(x,y)=\sum_{y\in\mathcal{Y}}S_{h}(x,y)=1.\]Let \(p_{1}=p(x,y_{\max})\), \(p_{2}=p(x,\mathsf{h}(x))\), \(\tau_{1}=S_{h}(x,\mathsf{h}(x))\) and \(\tau_{2}=S_{h}(x,y_{\max})\) to simplify the notation. Then, by the definition of \(S_{h,\mu}\), we have for any \(h\in\mathcal{H}\) and \(x\in\mathcal{X}\),

\[\mathcal{C}_{\ell^{\mathrm{comp}}}\big{(}h,x\big{)}-\inf_{\mu\in \llbracket-\tau_{2},\tau_{1}\rrbracket}\mathcal{C}_{\ell^{\mathrm{comp}}}(h_{ \mu},x)\] \[=\sup_{\mu\in\llbracket-\tau_{2},\tau_{1}\rrbracket}\Bigg{\{}p_{1} \big{[}\Phi(\tau_{2})-\Phi(\tau_{1}-\mu)\big{]}+p_{2}\big{[}\Phi(\tau_{1})-\Phi (\tau_{2}+\mu)\big{]}\Bigg{\}}\] \[=\sup_{\mu\in\llbracket-\tau_{2},\tau_{1}\rrbracket}\Bigg{\{}\frac {P+p_{1}-p_{2}}{2}\big{[}\Phi(\tau_{2})-\Phi(\tau_{1}-\mu)\big{]}+\frac{P-p_{1 }+p_{2}}{2}\big{[}\Phi(\tau_{1})-\Phi(\tau_{2}+\mu)\big{]}\Bigg{\}}\] \[(P=p_{1}+p_{2}\in\big{[}\tfrac{1}{n-1}\lor p_{1}-p_{2},1\big{]})\] \[\leq\inf_{P\in\llbracket\frac{1}{n-1}\lor p_{1}-p_{2},1\rrbracket} \inf_{\tau_{1}\geq 2\max\{\tau_{2},1/n\}\atop\tau_{1}+\tau_{2}\leq 1,\tau_{2} \geq 0}\sup_{\mu\in\llbracket-\tau_{2},\tau_{1}\rrbracket}\Bigg{\{}\frac{P+p_{1}- p_{2}}{2}\big{[}\Phi(\tau_{2})-\Phi(\tau_{1}-\mu)\big{]}\] \[\qquad+\frac{P-p_{1}+p_{2}}{2}\big{[}\Phi(\tau_{1})-\Phi(\tau_{2} +\mu)\big{]}\Bigg{\}}\qquad\qquad(\tau_{1}\geq\max\{\tau_{2},1/n\},\tau_{1}+ \tau_{2}\leq 1,\tau_{2}\geq 0)\] \[=\mathcal{T}^{\mathrm{comp}}(p_{1}-p_{2})\] \[=\mathcal{T}^{\mathrm{comp}}(\Delta\mathcal{C}_{\ell_{0-1}, \mathcal{H}}(h,x)),\] (by Lemma 1)

where for \(n=2\), an additional constraint \(\tau_{1}+\tau_{2}=1\) is imposed and the expression of \(\mathcal{T}^{\mathrm{comp}}\) is simplified. Since \(\mathcal{T}^{\mathrm{comp}}\) is convex, by Jensen's inequality, we obtain for any hypothesis \(h\in\mathcal{H}\) and any distribution,

\[\mathcal{T}^{\mathrm{comp}}\big{(}\mathcal{R}_{\ell_{0-1}}(h)- \mathcal{R}_{\ell_{0-1}}^{*}(\mathcal{H})+\mathcal{M}_{\ell_{0-1}}(\mathcal{H })\big{)}\] \[=\mathcal{T}^{\mathrm{comp}}\bigg{(}\underset{\mathcal{X}}{ \mathbb{E}}\big{[}\Delta\mathcal{C}_{\ell_{0-1},\mathcal{H}}(h,x)\big{]}\bigg{)}\] \[\leq\underset{\mathcal{X}}{\mathbb{E}}\big{[}\mathcal{T}^{ \mathrm{comp}}(\Delta\mathcal{C}_{\ell_{0-1},\mathcal{H}}(h,x))\big{]}\] \[\leq\underset{\mathcal{X}}{\mathbb{E}}\big{[}\Delta\mathcal{C}_{ \ell^{\mathrm{comp}},\mathcal{H}}(h,x)\big{]}\] \[=\mathcal{R}_{\ell^{\mathrm{comp}}}(h)-\mathcal{R}_{\ell^{ \mathrm{comp}}}^{*}(\mathcal{H})+\mathcal{M}_{\ell^{\mathrm{comp}}}(\mathcal{H }).\]

For the second part, we first consider \(n=2\). For any \(t\in[0,1]\), we consider the distribution that concentrates on a singleton \(\{x\}\) and satisfies \(p(x,1)=\frac{1+t}{2}\), \(p(x,2)=\frac{1-t}{2}\). For any \(\epsilon>0\), by the definition of infimum, we can take \(h\in\mathcal{H}\) such that \(S_{h}(x,1)=\tau_{\epsilon}\in\big{[}0,\frac{1}{2}\big{]}\) and satisfies

\[\sup_{\mu\in\llbracket-\tau_{\epsilon},1-\tau_{\epsilon}\rrbracket}\bigg{\{} \frac{1+t}{2}\big{[}\Phi(\tau_{\epsilon})-\Phi(1-\tau_{\epsilon}-\mu)\big{]}+ \frac{1-t}{2}\big{[}\Phi(1-\tau_{\epsilon})-\Phi(\tau_{\epsilon}+\mu)\big{]} \bigg{\}}<\mathcal{T}^{\mathrm{comp}}(t)+\epsilon.\]

Then,

\[\mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}_{\ell_{0-1}}^{*}( \mathcal{H})+\mathcal{M}_{\ell_{0-1}}(\mathcal{H}) =\mathcal{R}_{\ell_{0-1}}(h)-\mathbb{E}_{X}\big{[}\mathcal{C}_{ \ell_{0-1}}^{*}(\mathcal{H},x)\big{]}\] \[=\mathcal{C}_{\ell_{0-1}}(h,x)-\mathcal{C}_{\ell_{0-1}}^{*}( \mathcal{H},x)\] \[=t\]

and

\[\mathcal{T}^{\mathrm{comp}}(t) \leq\mathcal{R}_{\ell^{\mathrm{comp}}}(h)-\mathcal{R}_{\ell^{ \mathrm{comp}}}^{*}(\mathcal{H})+\mathcal{M}_{\ell^{\mathrm{comp}}}(\mathcal{H })\] \[=\mathcal{R}_{\ell^{\mathrm{comp}}}(h)-\mathbb{E}_{X}\big{[} \mathcal{C}_{\ell^{\mathrm{comp}}}^{*}(\mathcal{H},x)\big{]}\] \[=\mathcal{C}_{\ell^{\mathrm{comp}}}(h,x)-\mathcal{C}_{\ell^{ \mathrm{comp}}}^{*}(\mathcal{H},x)\] \[=\sup_{\mu\in\llbracket-\tau_{\epsilon},1-\tau_{\epsilon}\rrbracket} \bigg{\{}\frac{1+t}{2}\big{[}\Phi(\tau_{\epsilon})-\Phi(1-\tau_{\epsilon}-\mu) \big{]}+\frac{1-t}{2}\big{[}\Phi(1-\tau_{\epsilon})-\Phi(\tau_{\epsilon}+\mu) \big{]}\bigg{\}}\] \[<\mathcal{T}^{\mathrm{comp}}(t)+\epsilon.\]

By letting \(\epsilon\to 0\), we prove the tightness for \(n=2\). The proof for \(n>2\) directly extends from the case when \(n=2\). Indeed, for any \(t\in[0,1]\), we consider the distribution that concentrates on a singleton \(\{x\}\) and satisfies \(p(x,1)=\frac{1+t}{2}\), \(p(x,2)=\frac{1-t}{2}\), \(p(x,y)=0\), \(3\leq y\leq n\). For any \(\epsilon>0\), by the definition of infimum, we can take \(h\in\mathcal{H}\) such that \(S_{h}(x,1)=\tau_{1,\epsilon}\), \(S_{h}(x,2)=\tau_{2,\epsilon}\), \(S_{h}(x,y)=0\), \(3\leq y\leq n\) and satisfies \(\tau_{1,\epsilon}+\tau_{2,\epsilon}=1\), and

\[\inf_{P\{\Phi\big{[}\frac{1}{n-1}\lor t,1\big{]}\}}\sup_{\mu\in\{ \tau-\tau_{2,\epsilon},\tau_{1,\epsilon}\}}\bigg{\{}\frac{P+t}{2}\big{[}\Phi( \tau_{2,\epsilon})-\Phi(\tau_{1,\epsilon}-\mu)\big{]}+\frac{P-t}{2}\big{[}\Phi( \tau_{1,\epsilon})-\Phi(\tau_{2,\epsilon}+\mu)\big{]}\bigg{\}}\] \[=\sup_{\mu\in\{\tau-\tau_{2,\epsilon},\tau_{1,\epsilon}\}}\bigg{\{} \frac{1+t}{2}\big{[}\Phi(\tau_{2,\epsilon})-\Phi(\tau_{1,\epsilon}-\mu)\big{]} +\frac{1-t}{2}\big{[}\Phi(\tau_{1,\epsilon})-\Phi(\tau_{2,\epsilon}+\mu) \big{]}\bigg{\}}\] \[<\mathcal{T}^{\mathrm{comp}}(t)+\epsilon.\]

Then,

\[\mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}^{*}_{\ell_{0-1}}( \mathcal{H})+\mathcal{M}_{\ell_{0-1}}(\mathcal{H})=t\]

and

\[\mathcal{T}^{\mathrm{comp}}(t)<\mathcal{T}^{\mathrm{comp}}(t)+\epsilon.\]

By letting \(\epsilon\to 0\), we prove the tightness for \(n>2\). 

### Characterization of \(\mathcal{T}^{\mathrm{comp}}\) (Theorem 3)

**Theorem 3** (**characterization of \(\mathcal{T}^{\mathrm{comp}}\))**.: _Assume that \(\Phi\) is convex, differentiable at \(\frac{1}{2}\) and \(\Phi^{\prime}\big{(}\frac{1}{2}\big{)}<0\). Then, \(\mathcal{T}^{\mathrm{comp}}\) can be expressed as follows:_

\[\mathcal{T}^{\mathrm{comp}}(t)=\begin{cases}\Phi\big{(}\frac{1}{2} \big{)}-\inf_{\mu\in\{-\frac{1}{2},\frac{1}{2}\}}\big{[}\frac{1-t}{2}\Phi\big{(} \frac{1}{2}+\mu\big{)}+\frac{1+t}{2}\Phi\big{(}\frac{1}{2}-\mu\big{)}\big{\}} &n=2\\ \inf_{\tau\in\{\frac{1}{2},\frac{1}{2}\}}\big{[}\Phi(\tau)-\inf_{\mu\in\{-\tau, \tau\}}\big{\{}\frac{1+t}{2}\Phi(\tau-\mu)+\frac{1-t}{2}\Phi(\tau+\mu)\big{\}} \big{\}}&n>2.\end{cases}\]

Proof.: For \(n=2\), we have

\[\mathcal{T}^{\mathrm{comp}}(t) =\inf_{\tau\in\{0,\frac{1}{2}\}}\sup_{\mu\in\{-\tau,1-\tau\}} \bigg{\{}\frac{1+t}{2}\big{[}\Phi(\tau)-\Phi(1-\tau-\mu)\big{]}+\frac{1-t}{2} \big{[}\Phi(1-\tau)-\Phi(\tau+\mu)\big{]}\bigg{\}}\] \[=\inf_{\tau\in\{0,\frac{1}{2}\}}\bigg{(}\frac{1+t}{2}\Phi(\tau)+ \frac{1-t}{2}\big{[}\Phi(1-\tau)\big{]}-\inf_{\mu\in\{-\frac{1}{2},\frac{1}{2} \}}\bigg{[}\frac{1-t}{2}\Phi\bigg{(}\frac{1}{2}+\mu\bigg{)}+\frac{1+t}{2}\Phi \bigg{(}\frac{1}{2}-\mu\bigg{)}\bigg{\}}\] \[\geq\inf_{\tau\in\{0,\frac{1}{2}\}}\bigg{(}\Phi\bigg{(}\frac{1}{2} \bigg{)}+\Phi^{\prime}\bigg{(}\frac{1}{2}\bigg{)}t\bigg{(}\tau-\frac{1}{2} \bigg{)}\bigg{)}-\inf_{\mu\in\{-\frac{1}{2},\frac{1}{2}\}}\bigg{\{}\frac{1-t}{ 2}\Phi\bigg{(}\frac{1}{2}+\mu\bigg{)}+\frac{1+t}{2}\Phi\bigg{(}\frac{1}{2}-\mu \bigg{)}\bigg{\}}\] ( \[\Phi\] is convex) \[=\Phi\bigg{(}\frac{1}{2}\bigg{)}-\inf_{\mu\in\{-\frac{1}{2},\frac{1 }{2}\}}\bigg{\{}\frac{1-t}{2}\Phi\bigg{(}\frac{1}{2}+\mu\bigg{)}+\frac{1+t}{2 }\Phi\bigg{(}\frac{1}{2}-\mu\bigg{)}\bigg{\}} \hskip 28.452756pt(\Phi^{\prime}\big{(}\frac{1}{2}\big{)}<0,\,t\big{(}\tau- \frac{1}{2}\big{)}\leq 0)\]

where the equality can be achieved by \(\tau=\frac{1}{2}\).

For \(n>2\), we have

\[\mathcal{T}^{\mathrm{comp}}(t)=\inf_{P\in\{\frac{1}{n-1},1\}}\inf_{\tau_{1}\geq \max\{\tau_{2},1/n\}}\sup_{\mu\in\{-\tau_{2},\tau_{1}\}}F(P,\tau_{1},\tau_{2},\mu)\]

where we let \(F(P,\tau_{1},\tau_{2},\mu)=\frac{P+t}{2}\big{[}\Phi(\tau_{2})-\Phi(\tau_{1}-\mu) \big{]}+\frac{P-t}{2}\big{[}\Phi(\tau_{1})-\Phi(\tau_{2}+\mu)\big{]}\). For simplicity, we assume that \(\Phi\) is differentiable. For general convex \(\Phi\), we can proceed by using left and right derivatives, which are non-decreasing. Differentiating \(F\) with respect to \(\mu\), we have

\[\frac{\partial F}{\partial\mu}=\frac{P+t}{2}\Phi^{\prime}(\tau_{1}-\mu)+\frac{t -P}{2}\Phi^{\prime}(\tau_{2}+\mu).\]

Using the fact that \(P\in\big{[}\frac{1}{n-1}\lor t,1\big{]},t\in[0,1]\) and \(\Phi^{\prime}\) is non-decreasing, we obtain that \(\frac{\partial F}{\partial\mu}\) is non-increasing. Furthermore, \(\Phi^{\prime}\) is non-decreasing and non-positive, \(\Phi\) is non-negative, we obtain that \(\Phi^{\prime}(+\infty)=0\). This implies that \(\frac{\partial F}{\partial\mu}(+\infty)\leq 0\) and \(\frac{\partial F}{\partial\mu}(-\infty)\geq 0\). Therefore, there exists \(\mu_{0}\in\mathbb{R}\) such that

\[\frac{\partial F}{\partial\mu}(\mu_{0})=\frac{P+t}{2}\Phi^{\prime}(\tau_{1}- \mu_{0})+\frac{t-P}{2}\Phi^{\prime}(\tau_{2}+\mu_{0})=0\]

By taking \(\mu=\tau_{1}-\tau_{2}\) and using the fact that \(\tau_{2}\leq\frac{1}{2}\), \(\Phi^{\prime}(\frac{1}{2})<0\), we have

\[\frac{\partial F}{\partial\mu}(\tau_{1}-\tau_{2})=\frac{P+t}{2}\Phi^{\prime}( \tau_{2})+\frac{t-P}{2}\Phi^{\prime}(\tau_{1})<0.\]

Thus, since \(\frac{\partial F}{\partial\mu}\) is non-increasing, we obtain \(\mu_{0}<\tau_{1}-\tau_{2}\). Differentiate \(F\) with respect to \(\tau_{2}\) at \(\mu_{0}\), we have

\[\frac{\partial F}{\partial\tau_{2}}=\frac{P+t}{2}\Phi^{\prime}(\tau_{2})+ \frac{t-P}{2}\Phi^{\prime}(\tau_{2}+\mu_{0}).\]

Since \(\Phi^{\prime}\) is non-decreasing, we obtain

\[\frac{\partial F}{\partial\tau_{2}}\leq\frac{P+t}{2}\Phi^{\prime}(\tau_{2})+ \frac{t-P}{2}\Phi^{\prime}(\tau_{2}+\tau_{1}-\tau_{2})=\frac{\partial F}{ \partial\mu}(\tau_{1}-\tau_{2})<0,\]

which implies that the infimum \(\inf_{\tau_{1}\geq\max\{\tau_{2},\frac{1}{n}\}}\) is achieved when \(\tau_{2}=\tau_{1}\). Differentiate \(F\) with respect to \(P\) at \(\mu_{0}\) and \(\tau_{1}=\tau_{2}\), by the convexity of \(\Phi\), we obtain

\[\frac{\partial F}{\partial P}=\Phi(\tau_{1})-\Phi(\tau_{1}-\mu_{0})+\Phi(\tau _{1})-\Phi(\tau_{1}+\mu_{0})\leq 0,\]

which implies that the infimum \(\inf_{P\in\left[\frac{1}{n-1},1\right]}\) is achieved when \(P=1\). Above all, we obtain

\[\begin{split}\mathcal{T}^{\mathrm{comp}}(t)&=\inf_{ \tau\in\left[\frac{1}{n},\frac{1}{2}\right]}\sup_{\mu\in\left[-\tau,\tau\right] }F(1,\tau,\tau,\mu)\\ &=\inf_{\tau\in\left[\frac{1}{n},\frac{1}{2}\right]}\biggl{\{} \Phi(\tau)-\inf_{\mu\in\left[-\tau,\tau\right]}\biggl{\{}\frac{1+t}{2}\Phi( \tau-\mu)+\frac{1-t}{2}\Phi(\tau+\mu)\biggr{\}}\biggr{\}}.\end{split}\]

### Computation of examples

Example:\(\Phi(t)=-\log(t)\).For \(n=2\), plugging in \(\Phi(t)=-\log(t)\) in Theorem 3, gives

\[\begin{split}\mathcal{T}^{\mathrm{comp}}&=\log 2-\inf_{\mu\in \left[-\frac{1}{2},\frac{1}{2}\right]}\biggl{\{}-\frac{1-t}{2}\log(\frac{1}{2}+ \mu)-\frac{1+t}{2}\log(\frac{1}{2}-\mu)\biggr{\}}\\ &=\frac{1+t}{2}\log(1+t)+\frac{1-t}{2}\log(1-t).\qquad\qquad\qquad \text{(minimum achieved at $\mu=-\frac{t}{2}$)}\end{split}\]

Similarly, for \(n>2\), plugging in \(\Phi(t)=-\log(t)\) in Theorem 3 yields

\[\begin{split}\mathcal{T}^{\mathrm{comp}}&=\inf_{ \tau\in\left[\frac{1}{n},\frac{1}{2}\right]}\biggl{\{}-\log\tau-\inf_{\mu\in \left[-\tau,\tau\right]}\biggl{\{}-\frac{1-t}{2}\log(\tau+\mu)-\frac{1+t}{2} \log(\tau-\mu)\biggr{\}}\biggr{\}}\\ &=\frac{1+t}{2}\log(1+t)+\frac{1-t}{2}\log(1-t).\qquad\qquad\qquad \text{(minimum achieved at $\mu=-\tau t$)}\end{split}\]

Example:\(\Phi(t)=\frac{1}{t}-1\).For \(n=2\), plugging in \(\Phi(t)=\frac{1}{t}-1\) in Theorem 3, gives

\[\begin{split}\mathcal{T}^{\mathrm{comp}}&=2-\inf_{ \mu\in\left[-\frac{1}{2},\frac{1}{2}\right]}\biggl{\{}\frac{1-t}{2}\frac{1}{ \frac{1}{2}+\mu}+\frac{1+t}{2}\frac{1}{\frac{1}{2}-\mu}\biggr{\}}\\ &=1-\sqrt{1-t^{2}}.\qquad\qquad\qquad\qquad\qquad\text{(minimum achieved at $\mu=\frac{(1-t)^{\frac{1}{2}}-(1+t)^{\frac{1}{2}}}{2\left(1+t\right)^{\frac{1}{2}}+(1-t)^{ \frac{1}{2}}}$)}\]

Similarly, for \(n>2\), plugging in \(\Phi(t)=\frac{1}{t}-1\) in Theorem 3 yields

\[\begin{split}\mathcal{T}^{\mathrm{comp}}&=\inf_{\tau \in\left[\frac{1}{n},\frac{1}{2}\right]}\biggl{\{}\frac{1}{\tau}-\inf_{\mu\in \left[-\tau,\tau\right]}\biggl{\{}\frac{1+t}{2}\frac{1}{\tau-\mu}+\frac{1+t}{2 }\frac{1}{\tau+\mu}\biggr{\}}\biggr{\}}\\ &=\inf_{\tau\in\left[\frac{1}{n},\frac{1}{2}\right]}\frac{1}{2 \tau}\Bigl{(}1-\sqrt{1-t^{2}}\Bigr{)}\qquad\qquad\qquad\text{(minimum achieved at $\mu=\frac{(1-t)^{\frac{1}{2}}-(1+t)^{\frac{1}{2}}}{(1+t)^{\frac{1}{2}}+(1-t)^{\frac{1}{2}}} \tau$)}\\ &=1-\sqrt{1-t^{2}}.\qquad\qquad\qquad\qquad\qquad\qquad\text{( minimum achieved at $\tau=\frac{1}{2}$)}\end{split}\]Example:\(\Phi(t)=\frac{1}{q}(1-t^{q}),q\in(0,1)\).For \(n=2\), plugging in \(\Phi(t)=\frac{1}{q}(1-t^{q})\) in Theorem 3, gives

\[\mathcal{T}^{\mathrm{comp}} =-\frac{1}{q2^{q}}-\inf_{\mu\in\left[-\frac{1}{2},\frac{1}{2} \right]}\left\{-\frac{1-t}{2q}\left(\frac{1}{2}+\mu\right)^{q}-\frac{1+t}{2q} \left(\frac{1}{2}-\mu\right)^{q}\right\}\] \[=\frac{1}{q2^{q}}\left(\frac{(1+t)^{\frac{1}{1-q}}+(1-t)^{\frac{ 1}{1-q}}}{2}\right)^{1-q}-\frac{1}{q2^{q}}.\] (minimum achieved at

Similarly, for \(n>2\), plugging in \(\Phi(t)=\frac{1}{q}(1-t^{q})\) in Theorem 3 yields \[\mathcal{T}^{\mathrm{comp}} =\inf_{\tau\in\left[\frac{1}{n},\frac{1}{2}\right]}\left\{-\frac{ \tau^{q}}{q}-\inf_{\mu\in\left[-\tau,\tau\right]}\left\{-\frac{1+t}{2q}(\tau -\mu)^{q}-\frac{1-t}{2q}(\tau+\mu)^{q}\right\}\right\}\] \[=\inf_{\tau\in\left[\frac{1}{n},\frac{1}{2}\right]}\frac{\tau^{q} }{q}\left(\frac{(1+t)^{\frac{1}{1-q}}+(1-t)^{\frac{1}{1-q}}}{2}\right)^{1-q} -\frac{\tau^{q}}{q}\] (minimum achieved at

\[\mu=\frac{(1-t)^{\frac{1}{1-q}}-(1+t)^{\frac{1}{1-q}}}{(1+t)^{ \frac{1}{1-q}}+(1-t)^{\frac{1}{1-q}}}\tau\)\] \[=\frac{1}{qn^{q}}\left(\frac{(1+t)^{\frac{1}{1-q}}+(1-t)^{\frac{1 }{1-q}}}{2}\right)^{1-q}-\frac{1}{qn^{q}}.\] (minimum achieved at

\[\tau=\frac{1}{n}\]

Example:\(\Phi(t)=1-t\).For \(n=2\), plugging in \(\Phi(t)=1-t\) in Theorem 3, gives

\[\mathcal{T}^{\mathrm{comp}} =\frac{1}{2}-\inf_{\mu\in\left[-\frac{1}{2},\frac{1}{2}\right]} \left\{\frac{1-t}{2}\left(\frac{1}{2}-\mu\right)+\frac{1+t}{2}\left(\frac{1}{ 2}+\mu\right)\right\}=\frac{1}{2}-\frac{1-t}{2}=\frac{t}{2}.\]

Similarly, for \(n>2\), plugging in \(\Phi(t)=1-t\) in Theorem 3 yields

\[\mathcal{T}^{\mathrm{comp}} =\inf_{\tau\in\left[\frac{1}{n},\frac{1}{2}\right]}\left\{(1- \tau)-\inf_{\mu\in\left[-\tau,\tau\right]}\left\{\frac{1+t}{2}(1-\tau+\mu)+ \frac{1-t}{2}(1-\tau-\mu)\right\}\right\}\] \[=\inf_{\tau\in\left[\frac{1}{n},\frac{1}{2}\right]}\tau\,t\] (minimum achieved at

\[\mu=-\tau\]

) \[=\frac{t}{n}.\] (minimum achieved at

\[\tau=\frac{1}{n}\]

) \[\text{Example:}\ \Phi(t)=(1-t)^{2}\]

.For \(n=2\), plugging in \(\Phi(t)=(1-t)^{2}\) in Theorem 3, gives

\[\mathcal{T}^{\mathrm{comp}} =\frac{1}{4}-\inf_{\mu\in\left[-\frac{1}{2},\frac{1}{2}\right]} \left\{\frac{1-t}{2}\left(\frac{1}{2}-\mu\right)^{2}+\frac{1+t}{2}\left(\frac {1}{2}+\mu\right)^{2}\right\}=\frac{1}{4}-\frac{1-t^{2}}{4}=\frac{t^{2}}{4}.\]

Similarly, for \(n>2\), plugging in \(\Phi(t)=(1-t)^{2}\) in Theorem 3 yields

\[\mathcal{T}^{\mathrm{comp}} =\inf_{\tau\in\left[\frac{1}{n},\frac{1}{2}\right]}\left\{(1- \tau)^{2}-\inf_{\mu\in\left[-\tau,\tau\right]}\left\{\frac{1+t}{2}(1-\tau+\mu) ^{2}+\frac{1-t}{2}(1-\tau-\mu)^{2}\right\}\right\}\] \[=\inf_{\tau\in\left[\frac{1}{n},\frac{1}{2}\right]}\left\{(1- \tau)^{2}t^{2}\right\}\] (minimum achieved at

\[\mu=t(\tau-1)\]

) \[=\frac{t^{2}}{4}.\] (minimum achieved at

\[\tau=\frac{1}{2}\]

) \[=\frac{1}{4}.\]

## Appendix D Proofs for constrained losses

Let \(y_{\max}=\operatorname*{argmax}_{y\neq y}p(x,y)\) and \(\operatorname{h}(x)=\operatorname*{argmax}_{y\neq y}h(x,y)\), where we choose the label with the highest index under the natural ordering of labels as the tie-breaking strategy.

[MISSING_PAGE_FAIL:24]

where for \(n=2\), an additional constraint \(\tau_{1}+\tau_{2}=0\) is imposed and the expression of \(\mathcal{T}^{\mathrm{comp}}\) is simplified. Since \(\mathcal{T}^{\mathrm{cstnd}}\) is convex, by Jensen's inequality, we obtain for any hypothesis \(h\in\mathcal{H}\) and any distribution,

\[\mathcal{T}^{\mathrm{cstnd}}\big{(}\mathcal{R}_{\ell_{0-1}}(h)- \mathcal{R}^{*}_{\ell_{0-1}}(\mathcal{H})+\mathcal{M}_{\ell_{0-1}}(\mathcal{H })\big{)}\] \[=\mathcal{T}^{\mathrm{cstnd}}\Big{(}\mathbb{E}_{X}[\Delta \mathcal{C}_{\ell_{0-1},\mathcal{H}}(h,x)]\Big{)}\] \[\leq\mathbb{E}_{X}\big{[}\mathcal{T}^{\mathrm{cstnd}}\big{(} \Delta\mathcal{C}_{\ell_{0-1},\mathcal{H}}(h,x)\big{)}\big{]}\] \[\leq\mathbb{E}_{X}[\Delta\mathcal{C}_{\ell^{\mathrm{cstnd}}, \mathcal{H}}(h,x)]\] \[=\mathcal{R}_{\ell^{\mathrm{cstnd}}}(h)-\mathcal{R}^{*}_{\ell^{ \mathrm{cstnd}}}(\mathcal{H})+\mathcal{M}_{\ell^{\mathrm{cstnd}}}(\mathcal{H}).\]

For the second part, we first consider \(n=2\). For any \(t\in[0,1]\), we consider the distribution that concentrates on a singleton \(\{x\}\) and satisfies \(p(x,1)=\frac{1+t}{2}\), \(p(x,2)=\frac{1-t}{2}\). For any \(\epsilon>0\), by the definition of infimum, we can take \(h\in\mathcal{H}\) such that \(h(x,2)=\tau_{\epsilon}\geq 0\) and satisfies

\[\sup_{\mu\in\mathbb{R}}\Big{\{}\frac{1-t}{2}[\Phi(\tau_{\epsilon})-\Phi(-\tau_ {\epsilon}+\mu)]+\frac{1+t}{2}[\Phi(-\tau_{\epsilon})-\Phi(\tau_{\epsilon}-\mu )]\Big{\}}<\mathcal{T}^{\mathrm{cstnd}}(t)+\epsilon.\]

Then,

\[\mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}^{*}_{\ell_{0-1}}(\mathcal{H })+\mathcal{M}_{\ell_{0-1}}(\mathcal{H}) =\mathcal{R}_{\ell_{0-1}}(h)-\mathbb{E}_{X}\big{[}\mathcal{C}^{* }_{\ell_{0-1}}(\mathcal{H},x)\big{]}\] \[=\mathcal{C}_{\ell_{0-1}}(h,x)-\mathcal{C}^{*}_{\ell_{0-1}}( \mathcal{H},x)\] \[=t\]

and

\[\mathcal{T}^{\mathrm{cstnd}}(t) \leq\mathcal{R}_{\ell^{\mathrm{cstnd}}}(h)-\mathcal{R}^{*}_{\ell ^{\mathrm{cstnd}}}(\mathcal{H})+\mathcal{M}_{\ell^{\mathrm{cstnd}}}(\mathcal{H})\] \[=\mathcal{R}_{\ell^{\mathrm{cstnd}}}(h)-\mathbb{E}_{X}[\mathcal{C }^{*}_{\ell^{\mathrm{cstnd}}}(\mathcal{H},x)]\] \[=\mathcal{C}_{\ell^{\mathrm{cstnd}}}(h,x)-\mathcal{C}^{*}_{\ell ^{\mathrm{cstnd}}}(\mathcal{H},x)\] \[=\sup_{\mu\in\mathbb{R}}\Big{\{}\frac{1-t}{2}[\Phi(\tau_{ \epsilon})-\Phi(-\tau_{\epsilon}+\mu)]+\frac{1+t}{2}[\Phi(-\tau_{\epsilon})- \Phi(\tau_{\epsilon}-\mu)]\Big{\}}\] \[<\mathcal{T}^{\mathrm{cstnd}}(t)+\epsilon.\]

By letting \(\epsilon\to 0\), we conclude the proof. The proof for \(n>2\) directly extends from the case when \(n=2\). Indeed, For any \(t\in[0,1]\), we consider the distribution that concentrates on a singleton \(\{x\}\) and satisfies \(p(x,1)=\frac{1+t}{2}\), \(p(x,2)=\frac{1-t}{2}\), \(p(x,y)=0,\,3\leq y\leq n\). For any \(\epsilon>0\), by the definition of infimum, we can take \(h\in\mathcal{H}\) such that \(h(x,1)=\tau_{1,\epsilon}\), \(h(x,2)=\tau_{2,\epsilon}\), \(h(x,y)=0,\,3\leq y\leq n\) and satisfies \(\tau_{1,\epsilon}+\tau_{2,\epsilon}=0\), and

\[\inf_{P\in\{\frac{1}{n-1},1\}}\sup_{\mu\in\mathbb{R}}\Big{\{}\frac{2-P-t}{2} [\Phi(-\tau_{2,\epsilon})-\Phi(-\tau_{1,\epsilon}+\mu)]+\frac{2-P+t}{2}[\Phi(- \tau_{1,\epsilon})-\Phi(-\tau_{2,\epsilon}-\mu)]\Big{\}}\] \[=\sup_{\mu\in\mathbb{R}}\Big{\{}\frac{1-t}{2}[\Phi(-\tau_{2, \epsilon})-\Phi(-\tau_{1,\epsilon}+\mu)]+\frac{1+t}{2}[\Phi(-\tau_{1,\epsilon})- \Phi(-\tau_{2,\epsilon}-\mu)]\Big{\}}\] \[<\mathcal{T}^{\mathrm{cstnd}}(t)+\epsilon.\]

Then,

\[\mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}^{*}_{\ell_{0-1}}(\mathcal{H})+ \mathcal{M}_{\ell_{0-1}}(\mathcal{H})=t\]

and

\[\mathcal{T}^{\mathrm{cstnd}}(t)\leq\mathcal{R}_{\ell^{\mathrm{cstnd}}}(h)- \mathcal{R}^{*}_{\ell^{\mathrm{cstnd}}}(\mathcal{H})+\mathcal{M}_{\ell^{ \mathrm{cstnd}}}(\mathcal{H})<\mathcal{T}^{\mathrm{cstnd}}(t)+\epsilon.\]

### Characterization of \(\mathcal{T}^{\mathrm{cstnd}}\) (Theorem 11)

**Theorem 11** (**characterization of \(\mathcal{T}^{\mathrm{cstnd}}\)**).: _Assume that \(\Phi\) is convex, differentiable at zero and \(\Phi^{\prime}(0)<0\). Then, \(\mathcal{T}^{\mathrm{cstnd}}\) can be expressed as follows:_

\[\mathcal{T}^{\mathrm{cstnd}}(t) =\begin{cases}\Phi(0)-\inf_{\mu\in\mathbb{R}}\{\frac{1-t}{2}\Phi( \mu)+\frac{1+t}{2}\Phi(-\mu)\}&n=2\\ \inf_{\tau\geq 0}\biggl{\{}\bigl{(}2-\frac{1}{n-1}\bigr{)}\Phi(-\tau)-\inf_{\mu \in\mathbb{R}}\biggl{\{}\frac{2-t-\frac{1}{n-1}}{2}\Phi(-\tau+\mu)+\frac{2+t- \frac{1}{n-1}}{2}\Phi(-\tau-\mu)\biggr{\}}\biggr{\}}&n>2\end{cases}\] \[\geq\begin{cases}\Phi(0)-\inf_{\mu\in\mathbb{R}}\{\frac{1-t}{2} \Phi(\mu)+\frac{1+t}{2}\Phi(-\mu)\}&n=2\\ \inf_{\tau\geq 0}\bigl{\{}2\Phi(-\tau)-\inf_{\mu\in\mathbb{R}}\{\frac{2-t }{2}\Phi(-\tau+\mu)+\frac{2+t}{2}\Phi(-\tau-\mu)\}\bigr{\}}&n>2.\end{cases}\]

Proof.: For \(n=2\), we have

\[\mathcal{T}^{\mathrm{cstnd}}(t) =\inf_{\tau\geq 0}\sup_{\mu\in\mathbb{R}}\biggl{\{}\frac{1-t}{2} \bigl{[}\Phi(\tau)-\Phi(-\tau+\mu)\bigr{]}+\frac{1+t}{2}\bigl{[}\Phi(-\tau)- \Phi(\tau-\mu)\bigr{]}\biggr{\}}\] \[=\inf_{\tau\geq 0}\biggl{(}\frac{1-t}{2}\Phi(\tau)+\frac{1+t}{2} \bigl{[}\Phi(-\tau)\bigr{]}\biggr{)}-\inf_{\mu\in\mathbb{R}}\biggl{\{}\frac{1 -t}{2}\Phi(-\tau+\mu)+\frac{1+t}{2}\Phi(-\mu)\biggr{\}}\] \[\geq\inf_{\tau\geq 0}\bigl{(}\Phi(0)-\Phi^{\prime}(0)t\tau \bigr{)}-\inf_{\mu\in\mathbb{R}}\biggl{\{}\frac{1-t}{2}\Phi(\mu)+\frac{1+t}{2 }\Phi(-\mu)\biggr{\}}\] ( \[\Phi\] is convex) \[=\Phi(0)-\inf_{\mu\in\mathbb{R}}\biggl{\{}\frac{1-t}{2}\Phi(\mu) +\frac{1+t}{2}\Phi(-\mu)\biggr{\}} (\Phi^{\prime}(0)<0,\,t\tau\geq 0)\]

where the equality can be achieved by \(\tau=0\).

For \(n>2\), we have

\[\mathcal{T}^{\mathrm{cstnd}}(t)=\inf_{P\in\bigl{\{}\frac{1-t}{n-1},1\bigr{\}} \bigr{\}}\inf_{\tau_{1}\geq\max\{\tau_{2},0\}}\sup_{\mu\in\mathbb{R}}F(P,\tau _{1},\tau_{2},\mu)\]

where we let \(F(P,\tau_{1},\tau_{2},\mu)=\frac{2-P-t}{2}\bigl{[}\Phi(-\tau_{2})-\Phi(-\tau_ {1}+\mu)\bigr{]}+\frac{2-P+t}{2}\bigl{[}\Phi(-\tau_{1})-\Phi(-\tau_{2}-\mu) \bigr{]}\). For simplicity, we assume that \(\Phi\) is differentiable. For general convex \(\Phi\), we can proceed by using left and right derivatives, which are non-decreasing. Differentiate \(F\) with respect to \(\mu\), we have

\[\frac{\partial F}{\partial\mu}=\frac{P+t-2}{2}\Phi^{\prime}(-\tau_{1}+\mu)+ \frac{2-P+t}{2}\Phi^{\prime}(-\tau_{2}-\mu).\]

Using the fact that \(P\in\bigl{[}\frac{1}{n-1},1\bigr{]},t\in[0,1]\) and \(\Phi^{\prime}\) is non-decreasing, we obtain that \(\frac{\partial F}{\partial\mu}\) is non-increasing. Furthermore, \(\Phi^{\prime}\) is non-decreasing and non-positive, \(\Phi\) is non-negative, we obtain that \(\Phi^{\prime}(+\infty)=0\). This implies that \(\frac{\partial F}{\partial\mu}(+\infty)\leq 0\) and \(\frac{\partial F}{\partial\mu}(-\infty)\geq 0\). Therefore, there exists \(\mu_{0}\in\mathbb{R}\) such that

\[\frac{\partial F}{\partial\mu}(\mu_{0})=\frac{P+t-2}{2}\Phi^{\prime}(-\tau_{1}+ \mu_{0})+\frac{2-P+t}{2}\Phi^{\prime}(-\tau_{2}-\mu_{0})=0\]

By taking \(\mu=\tau_{1}-\tau_{2}\) and using the fact that \(\Phi^{\prime}(0)<0\), we have

\[\frac{\partial F}{\partial\mu}(\tau_{1}-\tau_{2})=\frac{P+t-2}{2}\Phi^{\prime}( -\tau_{2})+\frac{2-P+t}{2}\Phi^{\prime}(-\tau_{1})<0.\]

Thus, since \(\frac{\partial F}{\partial\mu}\) is non-increasing, we obtain \(\mu_{0}<\tau_{1}-\tau_{2}\). Differentiate \(F\) with respect to \(\tau_{2}\) at \(\mu_{0}\), we have

\[\frac{\partial F}{\partial\tau_{2}}=\frac{P+t-2}{2}\Phi^{\prime}(-\tau_{2})+ \frac{2-P+t}{2}\Phi^{\prime}(-\tau_{2}-\mu_{0}).\]

Since \(\Phi^{\prime}\) is non-decreasing, we obtain

\[\frac{\partial F}{\partial\tau_{2}}\leq\frac{P+t-2}{2}\Phi^{\prime}(-\tau_{2})+ \frac{2-P+t}{2}\Phi^{\prime}(-\tau_{2}-\tau_{1}+\tau_{2})=\frac{\partial F}{ \partial\mu}(\tau_{1}-\tau_{2})<0,\]

which implies that the infimum \(\inf_{\tau_{1}\geq\max\{\tau_{2},0\}}\) is achieved when \(\tau_{2}=\tau_{1}\). Differentiate \(F\) with respect to \(P\) at \(\mu_{0}\) and \(\tau_{1}=\tau_{2}\), by the convexity of \(\Phi\), we obtain

\[\frac{\partial F}{\partial P}=\Phi(-\tau_{1}+\mu_{0})-\Phi(-\tau_{1})-\Phi(-\tau_{1 })+\Phi(-\tau_{1}-\mu_{0})\geq 0,\]which implies that the infimum \(\inf_{P\in\left[\frac{1}{n-1},1\right]}\) is achieved when \(P=\frac{1}{n-1}\). Above all, we obtain

\[\mathcal{T}^{\mathrm{c}\mathrm{t}\mathrm{a}\mathrm{t}}(t) =\inf_{\tau\geq 0}\sup_{\mu\in\mathbb{R}}F\bigg{(}\frac{1}{n-1}, \tau,\tau,\mu\bigg{)}\] \[=\inf_{\tau\geq 0}\Biggl{\{}\bigg{(}2-\frac{1}{n-1}\bigg{)}\Phi(- \tau)-\inf_{\mu\in\mathbb{R}}\Biggl{\{}\frac{2-t-\frac{1}{n-1}}{2}\Phi(-\tau+ \mu)+\frac{2+t-\frac{1}{n-1}}{2}\Phi(-\tau-\mu)\Biggr{\}}\Biggr{\}}\] \[\geq\inf_{\tau\geq 0}\sup_{\mu\in\mathbb{R}}F(0,\tau,\tau,\mu)\] \[=\inf_{\tau\geq 0}\biggl{\{}2\Phi(-\tau)-\inf_{\mu\in\mathbb{R}} \biggl{\{}\frac{2-t}{2}\Phi(-\tau+\mu)+\frac{2+t}{2}\Phi(-\tau-\mu)\biggr{\}} \biggr{\}}.\]

### Computation of examples

Example:\(\Phi(t)=\Phi_{\mathrm{exp}}(t)=e^{-t}\).For \(n=2\), plugging in \(\Phi(t)=e^{-t}\) in Theorem 11, gives

\[\mathcal{T}^{\mathrm{comp}} =1-\inf_{\mu\in\mathbb{R}}\biggl{\{}\frac{1-t}{2}e^{-\mu}+\frac{1 +t}{2}e^{\mu}\biggr{\}}\] \[=1-\sqrt{1-t^{2}}. \text{(minimum achieved at }\mu=\frac{1}{2}\log\frac{1-t}{1+t}\bigr{)}\]

For \(n>2\), plugging in \(\Phi(t)=e^{-t}\) in Theorem 11 yields

\[\mathcal{T}^{\mathrm{comp}} \geq\inf_{\tau\geq 0}\biggl{\{}2e^{\tau}-\inf_{\mu\in\mathbb{R}} \biggl{\{}\frac{2-t}{2}e^{\tau-\mu}+\frac{2+t}{2}e^{\tau+\mu}\biggr{\}}\biggr{\}}\] \[\geq 2-\inf_{\mu\in\mathbb{R}}\biggl{\{}\frac{2-t}{2}e^{-\mu}+ \frac{2+t}{2}e^{\mu}\biggr{\}}\] (minimum achieved at

\[\tau=0\]

) \[=2-\sqrt{4-t^{2}}. \text{(minimum achieved at }\mu=\frac{1}{2}\log\frac{2-t}{2+t}\bigr{)}\]

Example:\(\Phi(t)=\Phi_{\mathrm{hinge}}(t)=\max\{0,1-t\}\).For \(n=2\), plugging in \(\Phi(t)=\max\{0,1-t\}\) in Theorem 11, gives

\[\mathcal{T}^{\mathrm{comp}} =1-\inf_{\mu\in\mathbb{R}}\biggl{\{}\frac{1-t}{2}\max\{0,1-\mu\} +\frac{1+t}{2}\max\{0,1+\mu\}\biggr{\}}\] \[=t. \text{(minimum achieved at }\mu=-1\bigr{)}\]

For \(n>2\), plugging in \(\Phi(t)=\max\{0,1-t\}\) in Theorem 11 yields

\[\mathcal{T}^{\mathrm{comp}} \geq\inf_{\tau\geq 0}\biggl{\{}2\max\{0,1+\tau\}-\inf_{\mu\in \mathbb{R}}\biggl{\{}\frac{2-t}{2}\max\{0,1+\tau-\mu\}+\frac{2+t}{2}\max\{0,1 +\tau+\mu\}\biggr{\}}\biggr{\}}\] \[=2-\inf_{\mu\in\mathbb{R}}\biggl{\{}\frac{2-t}{2}\max\{0,1-\mu\} +\frac{2+t}{2}\max\{0,1+\mu\}\biggr{\}}\] (minimum achieved at

\[\tau=0\]

) \[=t. \text{(minimum achieved at }\mu=-1\bigr{)}\]

Example:\(\Phi(t)=\Phi_{\mathrm{sq-hinge}}(t)=(1-t)^{2}\mathds{1}_{t\leq 1}\).For \(n=2\), plugging in \(\Phi(t)=(1-t)^{2}\mathds{1}_{t\leq 1}\) in Theorem 11, gives

\[\mathcal{T}^{\mathrm{comp}} =1-\inf_{\mu\in\mathbb{R}}\biggl{\{}\frac{1-t}{2}(1-\mu)^{2} \mathds{1}_{\mu\leq 1}+\frac{1+t}{2}(1+\mu)^{2}\mathds{1}_{\mu\geq 1}\biggr{\}}\] \[=t^{2}. \text{(minimum achieved at }\mu=-t\bigr{)}\]

For \(n>2\), plugging in \(\Phi(t)=(1-t)^{2}\mathds{1}_{t\leq 1}\) in Theorem 11 yields

\[\mathcal{T}^{\mathrm{comp}} \geq\inf_{\tau\geq 0}\biggl{\{}2(1+\tau)^{2}\mathds{1}_{\tau \geq 1}-\inf_{\mu\in\mathbb{R}}\biggl{\{}\frac{2-t}{2}(1+\tau-\mu)^{2}\mathds{1}_{ \tau\rightarrow+\mu\leq 1}+\frac{2+t}{2}(1+\tau+\mu)^{2}\mathds{1}_{\tau \rightarrow+\mu\geq-1}\biggr{\}}\] \[\geq 2-\inf_{\mu\in\mathbb{R}}\biggl{\{}\frac{2-t}{2}(1-\mu)^{2} \mathds{1}_{\mu\leq 1}+\frac{2+t}{2}(1+\mu)^{2}\mathds{1}_{\mu\geq 1}\biggr{\}}\] \[=\frac{t^{2}}{2}. \text{(minimum achieved at }\mu=-\frac{t}{2}\bigr{)}\]Example: \(\Phi(t)=\Phi_{\mathrm{sq}}(t)=(1-t)^{2}\).For \(n=2\), plugging in \(\Phi(t)=(1-t)^{2}\) in Theorem 11, gives

\[\mathcal{T}^{\mathrm{comp}} =1-\inf_{\mu\in\mathbb{R}}\biggl{\{}\frac{1-t}{2}(1-\mu)^{2}+\frac {1+t}{2}(1+\mu)^{2}\biggr{\}}\] \[=t^{2}.\] (minimum achieved at

\[\mu=-t\]

)

For \(n>2\), plugging in \(\Phi(t)=(1-t)^{2}\) in Theorem 11 yields

\[\mathcal{T}^{\mathrm{comp}} \geq\inf_{\tau\geq 0}\biggl{\{}2(1+\tau)^{2}-\inf_{\mu\in \mathbb{R}}\biggl{\{}\frac{2-t}{2}(1+\tau-\mu)^{2}+\frac{2+t}{2}(1+\tau+\mu)^{ 2}\biggr{\}}\biggr{\}}\] \[\geq 2-\inf_{\mu\in\mathbb{R}}\biggl{\{}\frac{2-t}{2}(1-\mu)^{2}+ \frac{2+t}{2}(1+\mu)^{2}\biggr{\}}\] (minimum achieved at

\[\tau=0\]

) \[=\frac{t^{2}}{2}.\] (minimum achieved at

\[\mu=-\frac{t}{2}\]

)

## Appendix E Extensions of comp-sum losses

Proof of \(\overline{\mathcal{H}}\)-consistency bounds with \(\overline{\mathcal{J}}^{\mathrm{comp}}\) (Theorem 5)

**Theorem 5** (\(\overline{\mathcal{H}}\)-consistency bound for comp-sum losses).: _Assume that \(\overline{\mathcal{J}}^{\mathrm{comp}}\) is convex. Then, the following inequality holds for any hypothesis \(h\in\overline{\mathcal{H}}\) and any distribution:_

\[\overline{\mathcal{J}}^{\mathrm{comp}}\bigl{(}\mathcal{R}_{\ell_{0-1}}(h)- \mathcal{R}_{\ell_{0-1}}^{*}(\overline{\mathcal{H}})+\mathcal{M}_{\ell_{0-1}} (\overline{\mathcal{H}})\bigr{)}\leq\mathcal{R}_{\ell^{\mathrm{comp}}}(h)- \mathcal{R}_{\ell^{\mathrm{comp}}}^{*}(\overline{\mathcal{H}})+\mathcal{M}_{ \ell^{\mathrm{comp}}}(\overline{\mathcal{H}})\]

_with \(\overline{\mathcal{J}}^{\mathrm{comp}}\) the \(\overline{\mathcal{H}}\)-estimation error transformation for comp-sum losses defined for all \(t\in[0,1]\) by \(\overline{\mathcal{J}}^{\mathrm{comp}}(t)=\)_

\[\inf_{\begin{subarray}{c}\tau\in[0,\frac{1}{2}]\\ \inf_{\tau\in[0,\frac{1}{2}]}\inf_{\mu\in[s_{\min}-\tau,1-\tau-s_{\min}]} \end{subarray}}\Bigl{\{}\frac{1+t}{2}\bigl{[}\Phi(\tau)-\Phi(1-\tau-\mu) \bigr{]}+\frac{1-t}{2}\bigl{[}\Phi(1-\tau)-\Phi(\tau+\mu)\bigr{]}\Bigr{\}} \qquad n=2\]

_where \(C=[\max\{s_{\min}-\tau_{2},\tau_{1}-s_{\max}\},\min\{s_{\max}-\tau_{2},\tau_{1 }-s_{\min}\}]\), \(s_{\max}=\frac{1}{1+(n-1)e^{-2\inf_{x}\Lambda(t)}}\) and \(s_{\min}=\frac{1}{1+(n-1)e^{2\inf_{x}\Lambda(t)}}\). Furthermore, for any \(t\in[0,1]\), there exist a distribution \(\mathcal{D}\) and \(h\in\mathcal{H}\) such that \(\mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}_{\ell_{0-1}}^{*}(\mathcal{H})+\mathcal{ M}_{\ell_{0-1}}(\mathcal{H})=t\) and \(\mathcal{R}_{\ell^{\mathrm{comp}}}(h)-\mathcal{R}_{\ell^{\mathrm{comp}}}^{*}( \mathcal{H})+\mathcal{M}_{\ell^{\mathrm{comp}}}(\mathcal{H})=\mathcal{T}^{ \mathrm{comp}}(t)\)._

Proof.: For the comp-sum loss \(\ell^{\mathrm{comp}}\), the conditional \(\ell^{\mathrm{comp}}\)-risk can be expressed as follows:

\[\mathcal{C}_{\ell^{\mathrm{comp}}}(h,x)\] \[=\sum_{y\in\mathcal{Y}}p(x,y)\ell^{\mathrm{comp}}(h,x,y)\] \[=\sum_{y\in\mathcal{Y}}p(x,y)\Phi\biggl{(}\frac{e^{h(x,y)}}{\sum_ {y^{\prime}\in\mathcal{Y}}e^{h(x,y^{\prime})}}\biggr{)}\] \[=\sum_{y\in\mathcal{Y}}p(x,y)\Phi(S_{h}(x,y))\] \[=p(x,y_{\max})\Phi(S_{h}(x,y_{\max}))+p(x,\mathsf{h}(x))\Phi(S_{h} (x,\mathsf{h}(x)))+\sum_{y\notin\{y_{\max},\mathsf{h}(x)\}}p(x,y)\Phi(S_{h}(x, y))\]

where we let \(S_{h}(x,y)=\frac{e^{h(x,y)}}{\sum_{y^{\prime}\in\mathcal{Y}}e^{h(x,y^{\prime})}}\) for any \(y\in\mathcal{Y}\) with the constraint that \(\sum_{y\in\mathcal{Y}}S_{h}(x,y)=1\). Note that for any \(h\in\mathcal{H}\),

\[\frac{1}{1+(n-1)e^{2\Lambda(x)}}=\frac{e^{-\Lambda(x)}}{e^{-\Lambda(x)}+(n-1)e^{ \Lambda(x)}}\leq S_{h}(x,y)\leq\frac{e^{\Lambda(x)}}{e^{\Lambda(x)}+(n-1)e^{- \Lambda(x)}}=\frac{1}{1+(n-1)e^{-2\Lambda(x)}}\]

Therefore for any \((x,y)\in\mathcal{X}\times\mathcal{Y}\), \(S_{h}(x,y)\in[S_{\min},S_{\max}]\), where we let \(S_{\max}=\frac{1}{1+(n-1)e^{-2\Lambda(x)}}\) and \(S_{\min}=\frac{1}{1+(n-1)e^{2\Lambda(x)}}\). Furthermore, all values in \([S_{\min},S_{\max}]\) of \(S_{h}\) can be reached for some \(h\in\mathcal{H}\).

[MISSING_PAGE_FAIL:29]

where we use the fact that \(s_{\max}+s_{\min}=1\) and \(P=1\) when \(n=2\). Since \(\mathcal{T}^{\mathrm{comp}}\) is convex, by Jensen's inequality, we obtain for any hypothesis \(h\in\mathcal{H}\) and any distribution,

\[\mathcal{T}^{\mathrm{comp}}\big{(}\mathcal{R}_{\ell_{0-1}}(h)- \mathcal{R}^{*}_{\ell_{0-1}}(\mathcal{H})+\mathcal{M}_{\ell_{0-1}}(\mathcal{H}) \big{)}\] \[=\mathcal{T}^{\mathrm{comp}}\Big{(}\mathbb{E}[\Delta\mathcal{E}_ {\ell_{0-1},\mathcal{H}}(h,x)]\Big{)}\] \[\leq\mathbb{E}[\mathcal{T}^{\mathrm{comp}}(\Delta\mathcal{E}_{ \ell_{0-1},\mathcal{H}}(h,x))]\] \[\leq\mathbb{E}[\Delta\mathcal{E}_{\ell^{\mathrm{comp}},\mathcal{H }}(h,x)]\] \[=\mathcal{R}_{\ell^{\mathrm{comp}}}(h)-\mathcal{R}^{*}_{\ell^{ \mathrm{comp}}}(\mathcal{H})+\mathcal{M}_{\ell^{\mathrm{comp}}}(\mathcal{H}).\]

For the second part, we first consider \(n=2\). For any \(t\in[0,1]\), we consider the distribution that concentrates on a singleton \(\{x\}\) and satisfies \(p(x,1)=\frac{1+t}{2}\), \(p(x,2)=\frac{1-t}{2}\). For any \(\epsilon>0\), by the definition of infimum, we can take \(h\in\mathcal{H}\) such that \(S_{h}(x,1)=\tau_{\epsilon}\in\left[0,\frac{1}{2}\right]\) and satisfies

\[\sup_{\mu\in\{s_{\min}-\tau_{\epsilon},1-\tau_{\epsilon}-s_{\min}\}}\bigg{\{} \frac{1+t}{2}[\Phi(\tau_{\epsilon})-\Phi(1-\tau_{\epsilon}-\mu)]+\frac{1-t}{2} [\Phi(1-\tau_{\epsilon})-\Phi(\tau_{\epsilon}+\mu)]\bigg{\}}<\mathcal{T}^{ \mathrm{comp}}(t)+\epsilon.\]

Then,

\[\mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}^{*}_{\ell_{0-1}}( \mathcal{H})+\mathcal{M}_{\ell_{0-1}}(\mathcal{H}) =\mathcal{R}_{\ell_{0-1}}(h)-\mathbb{E}_{X}\Big{[}\mathcal{C}^{*} _{\ell_{0-1}}(\mathcal{H},x)\Big{]}\] \[=\mathcal{C}_{\ell_{0-1}}(h,x)-\mathcal{C}^{*}_{\ell_{0-1}}( \mathcal{H},x)\] \[=t\]

and

\[\mathcal{T}^{\mathrm{comp}}(t) \leq\mathcal{R}_{\ell^{\mathrm{comp}}}(h)-\mathcal{R}^{*}_{ \ell^{\mathrm{comp}}}(\mathcal{H})+\mathcal{M}_{\ell^{\mathrm{comp}}}(\mathcal{H})\] \[=\mathcal{R}_{\ell^{\mathrm{comp}}}(h)-\mathbb{E}_{X}\big{[} \mathcal{C}^{*}_{\ell^{\mathrm{comp}}}(\mathcal{H},x)\big{]}\] \[=\mathcal{C}_{\ell^{\mathrm{comp}}}(h,x)-\mathcal{C}^{*}_{\ell^{ \mathrm{comp}}}(\mathcal{H},x)\] \[=\sup_{\mu\in\{s_{\min}-\tau_{\epsilon},1-\tau_{\epsilon}-s_{ \min}\}}\bigg{\{}\frac{1+t}{2}[\Phi(\tau_{\epsilon})-\Phi(1-\tau_{\epsilon}- \mu)]+\frac{1-t}{2}[\Phi(1-\tau_{\epsilon})-\Phi(\tau_{\epsilon}+\mu)]\bigg{\}}\] \[<\mathcal{T}^{\mathrm{comp}}(t)+\epsilon.\]

By letting \(\epsilon\to 0\), we conclude the proof. The proof for \(n>2\) directly extends from the case when \(n=2\). Indeed, For any \(t\in[0,1]\), we consider the distribution that concentrates on a singleton \(\{x\}\) and satisfies \(p(x,1)=\frac{1+t}{2}\), \(p(x,2)=\frac{1-t}{2}\), \(p(x,y)=0,\,3\leq y\leq n\). For any \(\epsilon>0\), by the definition of infimum, we can take \(h\in\mathcal{H}\) such that \(S_{h}(x,1)=\tau_{1,\epsilon}\), \(S_{h}(x,2)=\tau_{2,\epsilon}\) and \(S_{h}(x,y)=0,\,3\leq y\leq n\) and satisfies \(\tau_{1,\epsilon}+\tau_{2,\epsilon}=1\), and

\[\inf_{P\in\{\frac{1}{n-1}\cup t,1\}}\sup_{\mu\in\mathcal{C}}\bigg{\{} \frac{P+t}{2}[\Phi(\tau_{2,\epsilon})-\Phi(\tau_{1,\epsilon}-\mu)]+\frac{P-t} {2}[\Phi(\tau_{1,\epsilon})-\Phi(\tau_{2,\epsilon}+\mu)]\bigg{\}}\] \[=\sup_{\mu\in\mathcal{C}}\bigg{\{}\frac{1+t}{2}[\Phi(\tau_{2, \epsilon})-\Phi(\tau_{1,\epsilon}-\mu)]+\frac{1-t}{2}[\Phi(\tau_{1,\epsilon} )-\Phi(\tau_{2,\epsilon}+\mu)]\bigg{\}}\] \[<\mathcal{T}^{\mathrm{comp}}(t)+\epsilon.\]

Then,

\[\mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}^{*}_{\ell_{0-1}}(\mathcal{H})+ \mathcal{M}_{\ell_{0-1}}(\mathcal{H})=t\]

and

\[\mathcal{T}^{\mathrm{comp}}(t)\leq\mathcal{R}_{\ell^{\mathrm{comp}}}(h)- \mathcal{R}^{*}_{\ell^{\mathrm{comp}}}(\mathcal{H})+\mathcal{M}_{\ell^{ \mathrm{comp}}}(\mathcal{H})<\mathcal{T}^{\mathrm{comp}}(t)+\epsilon.\]

By letting \(\epsilon\to 0\), we conclude the proof. 

### Logistic loss

**Theorem 6** (\(\overline{\mathcal{H}}\)-consistency bounds for logistic loss).: _For any \(h\in\overline{\mathcal{H}}\) and any distribution, we have_

\[\mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}^{*}_{\ell_{0-1}}\big{(} \overline{\mathcal{H}}\big{)}+\mathcal{M}_{\ell_{0-1}}\big{(}\overline{ \mathcal{H}}\big{)}\leq\Psi^{-1}\Big{(}\mathcal{R}_{\ell_{\mathrm{log}}}(h)- \mathcal{R}^{*}_{\ell_{\mathrm{log}}}\big{(}\overline{\mathcal{H}}\big{)}+ \mathcal{M}_{\ell_{\mathrm{log}}}\big{(}\overline{\mathcal{H}}\big{)}\Big{)},\]

_where \(\ell_{\mathrm{log}}=-\log\bigg{(}\frac{e^{h(x,y)}}{\sum_{y^{\prime}\neq y}e^{h(x,y^ {\prime})}}\bigg{)}\) and \(\Psi(t)=\begin{cases}\frac{1+t}{2}\log(1+t)+\frac{1-t}{2}\log(1-t)&t\leq\frac{s_{ \max}-s_{\min}}{s_{\min}+s_{\max}}\\ \frac{t}{2}\log\Big{(}\frac{s_{\max}}{s_{\min}}\Big{)}+\log\Big{(}\frac{2\sqrt{s _{\max}s_{\min}}}{s_{\max}+s_{\min}}\Big{)}&\text{otherwise}.\end{cases}\)_Proof.: For the multinomial logistic loss \(\ell_{\log}\), plugging in \(\Phi(t)=-\log(t)\) in Theorem 5, gives \(\overline{\mathcal{T}}^{\mathrm{comp}}\)

\[\geq\inf_{P\left[\frac{1}{n-1}\lor t,1\right]}\inf_{\begin{subarray}{c}S_{\min} \subseteq\tau_{2}\leq 5\tau_{1}\leq S_{\max}\\ \tau_{1}+\tau_{2}\leq 1\end{subarray}}\sup_{\mu\in C}\biggl{\{}\frac{P+t}{2} \bigl{[}-\log(\tau_{2})+\log(\tau_{1}-\mu)\bigr{]}+\frac{P-t}{2}\bigl{[}-\log (\tau_{1})+\log(\tau_{2}+\mu)\bigr{]}\biggr{\}}\]

where \(C=[\max\{s_{\min}-\tau_{2},\tau_{1}-s_{\max}\},\min\{s_{\max}-\tau_{2},\tau_{1} -s_{\min}\}]\). Here, we only compute the expression for \(n>2\). The expression for \(n=2\) will lead to the same result since it can be viewed as a special case of the expression for \(n>2\). By differentiating with respect to \(\tau_{2}\) and \(P\), we can see that the infimum is achieved when \(\tau_{1}=\tau_{2}=\frac{s_{\min}+s_{\max}}{2}\) and \(P=1\) modulo some elementary analysis.

Thus, \(\overline{\mathcal{T}}^{\mathrm{comp}}\) can be reformulated as

\[\overline{\mathcal{T}}^{\mathrm{comp}} =\sup_{\mu\in C}\left\{\frac{1+t}{2}\biggl{[}-\log\biggl{(}\frac{s _{\min}+s_{\max}}{2}\biggr{)}+\log\biggl{(}\frac{s_{\min}+s_{\max}}{2}-\mu \biggr{)}\right\}\] \[\qquad+\frac{1-t}{2}\biggl{[}-\log\biggl{(}\frac{s_{\min}+s_{\max }}{2}\biggr{)}+\log\biggl{(}\frac{s_{\min}+s_{\max}}{2}+\mu\biggr{)}\biggr{]}\] \[=-\log\biggl{(}\frac{s_{\min}+s_{\max}}{2}\biggr{)}+\sup_{\mu\in C }g(\mu)\]

where \(C=\bigl{[}\frac{s_{\min}-s_{\max}}{2},\frac{s_{\max}-s_{\min}}{2}\bigr{]}\) and \(g(\mu)=\frac{1+t}{2}\log\bigl{(}\frac{s_{\min}+s_{\max}}{2}-\mu\bigr{)}+\frac {1-t}{2}\log\bigl{(}\frac{s_{\min}+s_{\max}}{2}+\mu\bigr{)}\). Since \(g\) is continuous, it attains its supremum over a compact set. Note that \(g\) is concave and differentiable. In view of that, the maximum over the open set \((-\infty,+\infty)\) can be obtained by setting its gradient to zero. Differentiate \(g(\mu)\) to optimize, we obtain

\[g(\mu^{*})=0,\quad\mu^{*}=-\frac{t(s_{\min}+s_{\max})}{2}.\]

Moreover, by the concavity, \(g(\mu)\) is non-increasing when \(\mu\geq\mu^{*}\). Since \(s_{\max}-s_{\min}\geq 0\), we have

\[\mu^{*}\leq 0\leq\frac{s_{\max}-s_{\min}}{2}\]

In view of the constraint \(C\), if \(\mu^{*}\geq\frac{s_{\min}-s_{\max}}{2}\), the maximum is achieved by \(\mu=\mu^{*}\). Otherwise, if \(\mu^{*}<\frac{s_{\min}-s_{\max}}{2}\), since \(g(\mu)\) is non-increasing when \(\mu\geq\mu^{*}\), the maximum is achieved by \(\mu=\frac{s_{\min}-s_{\max}}{2}\). Since \(\mu^{*}\geq\frac{s_{\min}-s_{\max}}{2}\) is equivalent to \(t\leq\frac{s_{\max}-s_{\min}}{s_{\min}+s_{\max}}\), the maximum can be expressed as

\[\max_{\mu\in C}g(\mu)=\begin{cases}g(\mu^{*})&t\leq\frac{s_{\max}-s_{\min}}{s _{\min}+s_{\max}}\\ g\bigl{(}\frac{s_{\min}-s_{\max}}{2}\bigr{)}&\text{otherwise}\end{cases}\]

Computing the value of \(g\) at these points yields:

\[g(\mu^{*}) =\frac{1+t}{2}\log\frac{(1+t)(s_{\min}+s_{\max})}{2}+\frac{1-t}{ 2}\log\frac{(1-t)(s_{\min}+s_{\max})}{2}\] \[g\biggl{(}\frac{s_{\min}-s_{\max}}{2}\biggr{)} =\frac{1+t}{2}\log(s_{\max})+\frac{1-t}{2}\log(s_{\min})\]

Then, if \(t\leq\frac{s_{\max}-s_{\min}}{s_{\min}+s_{\max}}\), we obtain

\[\overline{\mathcal{T}}^{\mathrm{comp}} =-\log\biggl{(}\frac{s_{\min}+s_{\max}}{2}\biggr{)}+\frac{1+t}{2} \log\frac{(1+t)(s_{\min}+s_{\max})}{2}+\frac{1-t}{2}\log\frac{(1-t)(s_{\min}+s _{\max})}{2}\] \[=\frac{1+t}{2}\log(1+t)+\frac{1-t}{2}\log(1-t).\]

Otherwise, we obtain

\[\overline{\mathcal{T}}^{\mathrm{comp}} =-\log\biggl{(}\frac{s_{\min}+s_{\max}}{2}\biggr{)}+\frac{1+t}{2} \log(s_{\max})+\frac{1-t}{2}\log(s_{\min})\] \[=\frac{t}{2}\log\biggl{(}\frac{s_{\max}}{s_{\min}}\biggr{)}+\log \biggl{(}\frac{2\sqrt{s_{\max}s_{\min}}}{s_{\max}+s_{\min}}\biggr{)}.\]

Since \(\overline{\mathcal{T}}^{\mathrm{comp}}\) is convex, by Theorem 5, for any \(h\in\overline{\mathcal{H}}\) and any distribution,

\[\mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}_{\ell_{0-1}}^{*}\bigl{(}\overline{ \mathcal{H}}\bigr{)}+\mathcal{M}_{\ell_{0-1}}\bigl{(}\overline{\mathcal{H}} \bigr{)}\leq\Psi^{-1}\Bigl{(}\mathcal{R}_{\ell_{\log}}(h)-\mathcal{R}_{\ell_{ \log}}^{*}\bigl{(}\overline{\mathcal{H}}\bigr{)}+\mathcal{M}_{\ell_{\log}}\bigl{(} \overline{\mathcal{H}}\bigr{)}\Bigr{)},\]

where

\[\Psi(t)=\begin{cases}\frac{1+t}{2}\log(1+t)+\frac{1-t}{2}\log(1-t)&t\leq\frac{ s_{\max}-s_{\min}}{s_{\min}+s_{\max}}\\ \frac{t}{2}\log\Bigl{(}\frac{s_{\max}}{s_{\min}}\Bigr{)}+\log\Bigl{(}\frac{2\sqrt{s_ {\max}s_{\min}}}{s_{\max}+s_{\min}}\Bigr{)}&\text{otherwise}.\end{cases}\]

### Sum exponential loss

**Theorem 9** (\(\overline{\mathcal{H}}\)-consistency bounds for sum exponential loss).: _For any \(h\in\mathcal{H}\) and any distribution,_

\[\mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}_{\ell_{0-1}}^{*}\big{(} \overline{\mathcal{H}}\big{)}+\mathcal{M}_{\ell_{0-1}}\big{(}\overline{ \mathcal{H}}\big{)}\leq\Psi^{-1}\big{(}\mathcal{R}_{\ell_{\mathrm{exp}}}(h)- \mathcal{R}_{\ell_{\mathrm{exp}}}^{*}\big{(}\overline{\mathcal{H}}\big{)}+ \mathcal{M}_{\ell_{\mathrm{exp}}}\big{(}\overline{\mathcal{H}}\big{)}\big{)}\]

_where \(\ell_{\mathrm{exp}}=\sum_{y^{\prime}y^{\prime}}e^{h(x,y^{\prime})-h(x,y)}\) and \(\Psi(t)=\begin{cases}1-\sqrt{1-t^{2}}&t\leq\frac{s_{\mathrm{max}}^{2}-s_{ \mathrm{min}}^{2}}{s_{\mathrm{min}}^{2}+s_{\mathrm{max}}^{2}}\\ \frac{s_{\mathrm{max}}-s_{\mathrm{min}}}{2s_{\mathrm{max}}s_{\mathrm{min}}}t- \frac{(s_{\mathrm{max}}-s_{\mathrm{min}})^{2}}{2s_{\mathrm{max}}s_{\mathrm{ min}}(s_{\mathrm{max}}+s_{\mathrm{min}})}&\mathrm{otherwise}.\end{cases}\)__

Proof.: For the sum exponential loss \(\ell_{\mathrm{exp}}\), plugging in \(\Phi(t)=\frac{1}{t}-1\) in Theorem 5, gives \(\overline{\mathcal{H}}^{\mathrm{comp}}\)

\[\geq\inf_{P\in\left[\frac{1}{n-1}\lor t,1\right]S_{\min}\inf_{ \begin{subarray}{c}\tau_{1}\tau_{2}\tau_{1}\leq S_{\mathrm{max}}\\ \tau_{1}+\tau_{2}\leq 1\end{subarray}}\sup_{\mu\in C}\left\{\frac{P+t}{2} \bigg{[}\frac{1}{\tau_{2}}-\frac{1}{\tau_{1}-\mu}\bigg{]}+\frac{P-t}{2} \bigg{[}\frac{1}{\tau_{1}}-\frac{1}{\tau_{2}+\mu}\bigg{]}\right\}\]

where \(C=\left[\max\{s_{\mathrm{min}}-\tau_{2},\tau_{1}-s_{\mathrm{max}}\},\min\{s_ {\mathrm{max}}-\tau_{2},\tau_{1}-s_{\mathrm{min}}\}\right]\). Here, we only compute the expression for \(n>2\). The expression for \(n=2\) will lead to the same result since it can be viewed as a special case of the expression for \(n>2\). By differentiating with respect to \(\tau_{2}\) and \(P\), we can see that the infimum is achieved when \(\tau_{1}=\tau_{2}=\frac{s_{\mathrm{min}}+s_{\mathrm{max}}}{2}\) and \(P=1\) modulo some elementary analysis. Thus, \(\overline{\mathcal{H}}^{\mathrm{comp}}\) can be reformulated as

\[\overline{\mathcal{H}}^{\mathrm{comp}} =\sup_{\mu\in C}\left\{\frac{1+t}{2}\bigg{[}\frac{2}{s_{\mathrm{ min}}+s_{\mathrm{max}}}-\frac{2}{s_{\mathrm{min}}+s_{\mathrm{max}}-2\mu} \bigg{]}\right.\] \[\qquad+\frac{1-t}{2}\bigg{[}\frac{2}{s_{\mathrm{min}}+s_{\mathrm{ max}}}-\frac{2}{s_{\mathrm{min}}+s_{\mathrm{max}}+2\mu}\bigg{]}\right\}\] \[=\frac{2}{s_{\mathrm{min}}+s_{\mathrm{max}}}+\sup_{\mu\in C}g(\mu)\]

where \(C=\left[\frac{s_{\mathrm{min}}-s_{\mathrm{max}}}{2},\frac{s_{\mathrm{max}}-s_ {\mathrm{min}}}{2}\right]\) and \(g(\mu)=-\frac{1+t}{s_{\mathrm{min}}+s_{\mathrm{max}}-2\mu}-\frac{1-t}{s_{ \mathrm{min}}+s_{\mathrm{max}}+2\mu}\). Since \(g\) is continuous, it attains its supremum over a compact set. Note that \(g\) is concave and differentiable. In view of that, the maximum over the open set \((-\infty,+\infty)\) can be obtained by setting its gradient to zero. Differentiate \(g(\mu)\) to optimize, we obtain

\[g(\mu^{*})=0,\quad\mu^{*}=\frac{s_{\mathrm{min}}+s_{\mathrm{max}}}{2}\frac{ \sqrt{1-t}-\sqrt{1+t}}{\sqrt{1+t}+\sqrt{1-t}}\]

Moreover, by the concavity, \(g(\mu)\) is non-increasing when \(\mu\geq\mu^{*}\). Since \(s_{\mathrm{max}}-s_{\mathrm{min}}\geq 0\), we have

\[\mu^{*}\leq 0\leq\frac{s_{\mathrm{max}}-s_{\mathrm{min}}}{2}\]

In view of the constraint \(C\), if \(\mu^{*}\geq\frac{s_{\mathrm{min}}-s_{\mathrm{max}}}{2}\), the maximum is achieved by \(\mu=\mu^{*}\). Otherwise, if \(\mu^{*}<\frac{s_{\mathrm{min}}-s_{\mathrm{max}}}{2}\), since \(g(\mu)\) is non-increasing when \(\mu\geq\mu^{*}\), the maximum is achieved by \(\mu=\frac{s_{\mathrm{min}}-s_{\mathrm{max}}}{2}\). Since \(\mu^{*}\geq\frac{s_{\mathrm{min}}-s_{\mathrm{max}}}{2}\) is equivalent to \(t\leq\frac{s_{\mathrm{max}}^{2}-s_{\mathrm{min}}^{2}}{s_{\mathrm{min}}^{2}+s_{ \mathrm{max}}^{2}}\), the maximum can be expressed as

\[\max_{\mu\in C}g(\mu)=\begin{cases}g(\mu^{*})&t\leq\frac{s_{\mathrm{max}}^{2}-s_ {\mathrm{min}}^{2}}{s_{\mathrm{min}}^{2}+s_{\mathrm{max}}^{2}}\\ g\big{(}\frac{s_{\mathrm{min}}-s_{\mathrm{max}}}{2}\big{)}&\mathrm{otherwise} \end{cases}\]

Computing the value of \(g\) at these points yields:

\[g(\mu^{*}) =1-\sqrt{1-t^{2}}-\frac{2}{s_{\mathrm{min}}+s_{\mathrm{max}}}\] \[g\bigg{(}\frac{s_{\mathrm{min}}-s_{\mathrm{max}}}{2}\bigg{)} =-\frac{1+t}{2s_{\mathrm{max}}}-\frac{1-t}{2s_{\mathrm{min}}}\]

Then, if \(t\leq\frac{s_{\mathrm{max}}^{2}-s_{\mathrm{min}}^{2}}{s_{\mathrm{min}}^{2}+s_{ \mathrm{max}}^{2}}\), we obtain

\[\overline{\mathcal{H}}^{\mathrm{comp}} =\frac{2}{s_{\mathrm{min}}+s_{\mathrm{max}}}+1-\sqrt{1-t^{2}}-\frac{ 2}{s_{\mathrm{min}}+s_{\mathrm{max}}}\] \[=1-\sqrt{1-t^{2}}.\]Otherwise, we obtain

\[\overline{\mathcal{J}}^{\mathrm{comp}} =\frac{2}{s_{\min}+s_{\max}}-\frac{1+t}{2s_{\max}}-\frac{1-t}{2s_{ \min}}\] \[=\frac{s_{\max}-s_{\min}}{2s_{\max}s_{\min}}t-\frac{\left(s_{\max}- s_{\min}\right)^{2}}{2s_{\max}s_{\min}(s_{\max}+s_{\min})}.\]

Since \(\overline{\mathcal{J}}^{\mathrm{comp}}\) is convex, by Theorem 5, for any \(h\in\overline{\mathcal{H}}\) and any distribution,

\[\mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}_{\ell_{0-1}}^{*}\big{(} \overline{\mathcal{H}}\big{)}+\mathcal{M}_{\ell_{0-1}}\big{(}\overline{ \mathcal{H}}\big{)}\leq\Psi^{-1}\big{(}\mathcal{R}_{\ell_{\mathrm{exp}}}(h)- \mathcal{R}_{\ell_{\mathrm{exp}}}^{*}\big{(}\overline{\mathcal{H}}\big{)}+ \mathcal{M}_{\ell_{\mathrm{exp}}}\big{(}\overline{\mathcal{H}}\big{)}\big{)},\]

where

\[\Psi(t)=\begin{cases}1-\sqrt{1-t^{2}}&t\leq\frac{s_{\max}^{2}-s_{\min}^{2}}{s _{\min}^{2}+s_{\max}^{2}}\\ \frac{s_{\max}-s_{\min}}{2s_{\max}s_{\min}}-\frac{\left(s_{\max}-s_{\min} \right)^{2}}{2s_{\max}s_{\min}(s_{\max}+s_{\min})}&\mathrm{otherwise}.\end{cases}\]

### Generalized cross-entropy loss

**Theorem 16** (\(\overline{\mathcal{H}}\)-consistency bounds for generalized cross-entropy loss).: _For any \(h\in\overline{\mathcal{H}}\) and any distribution, we have_

\[\mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}_{\ell_{0-1}}^{*}\big{(} \overline{\mathcal{H}}\big{)}+\mathcal{M}_{\ell_{0-1}}\big{(}\overline{ \mathcal{H}}\big{)}\leq\Psi^{-1}\big{(}\mathcal{R}_{\ell_{\mathrm{sec}}}(h)- \mathcal{R}_{\ell_{\mathrm{sec}}}^{*}\big{(}\overline{\mathcal{H}}\big{)}+ \mathcal{M}_{\ell_{\mathrm{sec}}}\big{(}\overline{\mathcal{H}}\big{)}\big{)},\]

_where \(\Psi(t)=\begin{cases}\frac{1}{q}\big{(}\frac{s_{\min}+s_{\max}}{2}\big{)}^{q} \Bigg{[}\bigg{(}\frac{(1+t)^{\frac{1}{1-q}}+(1-t)^{\frac{1}{1-q}}}{2}\bigg{)} ^{1-q}-1\\ \frac{t}{2q}\Big{(}s_{\max}^{q}-s_{\min}^{q}\Big{)}+\frac{1}{q}\Big{(}\frac{s_ {\min}^{q}+s_{\max}^{q}}{2}-\big{(}\frac{s_{\min}+s_{\max}}{2}\big{)}^{q} \Big{)}&\mathrm{otherwise}.\end{cases}\) and \(\ell_{\mathrm{sec}}=\frac{1}{q}\bigg{[}1-\bigg{(}\frac{e^{h(x,y)}}{\sum_{y^{ \prime}\leq y}e^{h(x,y^{\prime})}}\bigg{)}^{q}\bigg{]}\)._

Proof.: For generalized cross-entropy loss \(\ell_{\mathrm{sec}}\), plugging \(\Phi(t)=\frac{1}{q}(1-t^{q})\) in Theorem 5, gives \(\overline{\mathcal{J}}^{\mathrm{comp}}\)

\[\geq\inf_{P\in\left[\frac{1}{n-1}\lor t,1\right]}\inf_{\begin{subarray}{c}S_ {\min}\leq\overline{\mathcal{H}}\leq\overline{\mathcal{H}}\leq\overline{ \mathcal{H}}\leq\\ \tau_{1}+\tau_{2}\leq 1\end{subarray}}\sup_{\mu\in C}\left\{\frac{P+t}{2}\bigg{[} \!-\!\frac{1}{q}(\tau_{2})^{q}+\frac{1}{q}(\tau_{1}-\mu)^{q}\bigg{]}+\frac{ P-t}{2}\bigg{[}\!-\!\frac{1}{q}(\tau_{1})^{q}+\frac{1}{q}(\tau_{2}+\mu)^{q}\bigg{]}\!\right\}\]

where \(C=[\max\{s_{\min}-\tau_{2},\tau_{1}-s_{\max}\},\min\{s_{\max}-\tau_{2},\tau_{ 1}-s_{\min}\}]\). Here, we only compute the expression for \(n>2\). The expression for \(n=2\) will lead to the same result since it can be viewed as a special case of the expression for \(n>2\). By differentiating with respect to \(\tau_{2}\) and \(P\), we can see that the infimum is achieved when \(\tau_{1}=\tau_{2}=\frac{s_{\min}+s_{\max}}{2}\) and \(P=1\) modulo some elementary analysis. Thus, \(\overline{\mathcal{J}}^{\mathrm{comp}}\) can be reformulated as

\[\overline{\mathcal{J}}^{\mathrm{comp}} =\sup_{\mu\in C}\left\{\frac{1+t}{2q}\bigg{[}\!-\!\bigg{(}\frac{s_{ \min}+s_{\max}}{2}\bigg{)}^{q}+\bigg{(}\frac{s_{\min}+s_{\max}}{2}-\mu\bigg{)}^{q }\bigg{]}\right.\] \[\qquad+\frac{1-t}{2q}\bigg{[}\!-\!\bigg{(}\frac{s_{\min}+s_{\max} }{2}\bigg{)}^{q}+\bigg{(}\frac{s_{\min}+s_{\max}}{2}+\mu\bigg{)}^{q}\bigg{]}\! \bigg{\}}\] \[=-\frac{1}{q}\bigg{(}\frac{s_{\min}+s_{\max}}{2}\bigg{)}^{q}+\sup_ {\mu\in C}g(\mu)\]

where \(C=\left[\frac{s_{\min}-s_{\max}}{2},\frac{s_{\max}-s_{\min}}{2}\right]\) and \(g(\mu)=\frac{1+t}{2q}\big{(}\frac{s_{\min}+s_{\max}}{2}-\mu\big{)}^{q}+\frac{1-t }{2q}\big{(}\frac{s_{\min}+s_{\max}}{2}+\mu\big{)}^{q}\). Since \(g\) is continuous, it attains its supremum over a compact set. Note that \(g\) is concave and differentiable. In view of that, the maximum over the open set \((-\infty,+\infty)\) can be obtained by setting its gradient to zero. Differentiate \(g(\mu)\) to optimize, we obtain

\[g(\mu^{*})=0,\quad\mu^{*}=\frac{(1-t)^{\frac{1}{1-q}}-(1+t)^{\frac{1}{1-q}}}{(1+ t)^{\frac{1}{1-q}}+(1-t)^{\frac{1}{1-q}}}\frac{s_{\min}+s_{\max}}{2}.\]Moreover, by the concavity, \(g(\mu)\) is non-increasing when \(\mu\geq\mu^{*}\). Since \(s_{\max}-s_{\min}\geq 0\), we have

\[\mu^{*}\leq 0\leq\frac{s_{\max}-s_{\min}}{2}\]

In view of the constraint \(C\), if \(\mu^{*}\geq\frac{s_{\min}-s_{\max}}{2}\), the maximum is achieved by \(\mu=\mu^{*}\). Otherwise, if \(\mu^{*}<\frac{s_{\min}-s_{\max}}{2}\), since \(g(\mu)\) is non-increasing when \(\mu\geq\mu^{*}\), the maximum is achieved by \(\mu=\frac{s_{\min}-s_{\max}}{2}\). Since \(\mu^{*}\geq\frac{s_{\min}-s_{\max}}{2}\) is equivalent to \(t\leq\frac{s_{\max}^{1-q}-s_{\min}^{1-q}}{s_{\min}^{1-q}+s_{\max}^{1-q}}\), the maximum can be expressed as

\[\max_{\mu\in C}g(\mu)=\begin{cases}g(\mu^{*})&t\leq\frac{s_{\max}^{1-q}-s_{ \min}^{1-q}}{s_{\min}^{1-q}+s_{\max}^{1-q}}\\ g\big{(}\frac{s_{\min}-s_{\max}}{2}\big{)}&\text{otherwise}\end{cases}\]

Computing the value of \(g\) at these points yields:

\[g(\mu^{*}) =\frac{1}{q}\bigg{(}\frac{s_{\min}+s_{\max}}{2}\bigg{)}^{q}\bigg{(} \frac{(1+t)^{\frac{1}{1-q}}+(1-t)^{\frac{1}{1-q}}}{2}\bigg{)}^{1-q}\] \[g\bigg{(}\frac{s_{\min}-s_{\max}}{2}\bigg{)} =\frac{1+t}{2q}(s_{\max})^{q}+\frac{1-t}{2q}(s_{\min})^{q}\]

Then, if \(t\leq\frac{s_{\max}^{1-q}-s_{\min}^{1-q}}{s_{\min}^{1-q}+s_{\max}^{1-q}}\), we obtain

\[\overline{\mathfrak{J}}^{\mathrm{comp}} =\frac{1}{q}\bigg{(}\frac{s_{\min}+s_{\max}}{2}\bigg{)}^{q} \bigg{(}\frac{(1+t)^{\frac{1}{1-q}}+(1-t)^{\frac{1}{1-q}}}{2}\bigg{)}^{1-q}- \frac{1}{q}\bigg{(}\frac{s_{\min}+s_{\max}}{2}\bigg{)}^{q}\] \[=\frac{1}{q}\bigg{(}\frac{s_{\min}+s_{\max}}{2}\bigg{)}^{q}\bigg{[} \bigg{(}\frac{(1+t)^{\frac{1}{1-q}}+(1-t)^{\frac{1}{1-q}}}{2}\bigg{)}^{1-q}- 1\bigg{]}\]

Otherwise, we obtain

\[\overline{\mathfrak{J}}^{\mathrm{comp}} =-\frac{1}{q}\bigg{(}\frac{s_{\min}+s_{\max}}{2}\bigg{)}^{q}+ \frac{1+t}{2q}(s_{\max})^{q}+\frac{1-t}{2q}(s_{\min})^{q}\] \[=\frac{t}{2q}\big{(}s_{\max}^{q}-s_{\min}^{q}\big{)}+\frac{1}{q} \bigg{(}\frac{s_{\min}^{q}+s_{\max}^{q}}{2}-\bigg{(}\frac{s_{\min}+s_{\max}}{ 2}\bigg{)}^{q}\bigg{)}\]

Since \(\overline{\mathfrak{J}}^{\mathrm{comp}}\) is convex, by Theorem 5, for any \(h\in\overline{\mathcal{H}}\) and any distribution,

\[\mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}_{\ell_{0-1}}^{*}\big{(}\overline{ \mathcal{H}}\big{)}+\mathcal{M}_{\ell_{0-1}}\big{(}\overline{\mathcal{H}} \big{)}\leq\Psi^{-1}\big{(}\mathcal{R}_{\ell_{\mathrm{ece}}}(h)-\mathcal{R}_ {\ell_{\mathrm{ece}}}^{*}\big{(}\overline{\mathcal{H}}\big{)}+\mathcal{M}_{ \ell_{\mathrm{ece}}}\big{(}\overline{\mathcal{H}}\big{)}\big{)},\]

where

\[\Psi(t)=\begin{cases}\frac{1}{q}\big{(}\frac{s_{\min}+s_{\max}}{2}\big{)}^{q} \Bigg{[}\bigg{(}\frac{(1+t)^{\frac{1}{1-q}}+(1-t)^{\frac{1}{1-q}}}{2}\bigg{)} ^{1-q}-1\Bigg{]}&t\leq\frac{s_{\max}^{1-q}-s_{\min}^{1-q}}{s_{\min}^{1-q}+s_{ \max}^{1-q}}\\ \frac{t}{2q}\big{(}s_{\max}^{q}-s_{\min}^{q}\big{)}+\frac{1}{q}\big{(}\frac{s_{ \min}^{q}+s_{\max}^{q}}{2}-\big{(}\frac{s_{\min}+s_{\max}}{2}\big{)}^{q}\big{)} &\text{otherwise}.\end{cases}\]

### Mean absolute error loss

**Theorem 17** (\(\overline{\mathcal{H}}\)-consistency bounds for mean absolute error loss).: _For any \(h\in\overline{\mathcal{H}}\) and any distribution, we have_

Proof.: For mean absolute error loss \(\ell_{\max}\), plugging \(\Phi(t)=1-t\) in Theorem 5, gives \(\overline{\mathfrak{J}}^{\mathrm{comp}}\)

\[\geq\inf_{P\in\big{[}\frac{1}{n-1}\lor t,1\big{]}}\inf_{\begin{subarray}{c}s_{ \min}\leq\tau_{2}\leq\tau_{1}\leq S_{\max}\\ \tau_{1}+\tau_{2}\leq 1\end{subarray}}\sup_{\mu\in C}\bigg{\{}\frac{P+t}{2} \big{[}-(\tau_{2})+(\tau_{1}-\mu)\big{]}+\frac{P-t}{2}\big{[}-(\tau_{1})+( \tau_{2}+\mu)\big{]}\bigg{\}}\]where \(C=[\max\{s_{\min}-\tau_{2},\tau_{1}-s_{\max}\},\min\{s_{\max}-\tau_{2},\tau_{1}-s_{ \min}\}]\). Here, we only compute the expression for \(n>2\). The expression for \(n=2\) will lead to the same result since it can be viewed as a special case of the expression for \(n>2\). By differentiating with respect to \(\tau_{2}\) and \(P\), we can see that the infimum is achieved when \(\tau_{1}=\tau_{2}=\frac{s_{\min}+s_{\max}}{2}\) and \(P=1\) modulo some elementary analysis. Thus, \(\overline{\mathcal{F}}^{\mathrm{comp}}\) can be reformulated as

\[\overline{\mathcal{F}}^{\mathrm{comp}} =\sup_{\mu\in C}\left\{\frac{1+t}{2}\bigg{[}-\left(\frac{s_{\min} +s_{\max}}{2}\right)+\left(\frac{s_{\min}+s_{\max}}{2}-\mu\right)\right]\] \[\qquad+\frac{1-t}{2}\bigg{[}-\left(\frac{s_{\min}+s_{\max}}{2} \right)+\left(\frac{s_{\min}+s_{\max}}{2}+\mu\right)\bigg{]}\bigg{\}}\] \[=\sup_{\mu\in C}-t\mu\]

where \(C=\left[\frac{s_{\min}-s_{\max}}{2},\frac{s_{\max}-s_{\min}}{2}\right]\). Since \(-t\mu\) is monotonically non-increasing, the maximum over \(C\) can be achieved by

\[\mu^{*}=\frac{s_{\min}-s_{\max}}{2},\quad\overline{\mathcal{F}}^{\mathrm{comp} }=\frac{s_{\max}-s_{\min}}{2}\,t.\]

Since \(\overline{\mathcal{F}}^{\mathrm{comp}}\) is convex, by Theorem 5, for any \(h\in\overline{\mathcal{H}}\) and any distribution,

\[\mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}_{\ell_{0-1}}^{*}\big{(}\overline{ \mathcal{H}}\big{)}+\mathcal{M}_{\ell_{0-1}}\big{(}\overline{\mathcal{H}} \big{)}\leq\frac{2\big{(}\mathcal{R}_{\ell_{\max}}(h)-\mathcal{R}_{\ell_{ \max}}^{*}\big{(}\overline{\mathcal{H}}\big{)}+\mathcal{M}_{\ell_{\max}} \big{(}\overline{\mathcal{H}}\big{)}\big{)}}{s_{\max}-s_{\min}}.\]

## Appendix F Extensions of constrained losses

Proof of \(\overline{\mathcal{H}}\)-consistency bound with \(\overline{\mathcal{F}}^{\mathrm{stnd}}\) (Theorem 12)

**Theorem 12** (\(\overline{\mathcal{H}}\)**-consistency bound for constrained losses)**.: _Assume that \(\overline{\mathcal{F}}^{\mathrm{stnd}}\) is convex. Then, the following inequality holds for any hypothesis \(h\in\overline{\mathcal{H}}\) and any distribution:_

\[\overline{\mathcal{F}}^{\mathrm{stnd}}\big{(}\mathcal{R}_{\ell_{0-1}}(h)- \mathcal{R}_{\ell_{0-1}}^{*}(\overline{\mathcal{H}})+\mathcal{M}_{\ell_{0-1}} \big{(}\overline{\mathcal{H}}\big{)}\big{)}\leq\mathcal{R}_{\ell^{\mathrm{ stnd}}}(h)-\mathcal{R}_{\ell^{\mathrm{stnd}}}^{*}\big{(}\overline{\mathcal{H}} \big{)}+\mathcal{M}_{\ell^{\mathrm{stnd}}}\big{(}\overline{\mathcal{H}}\big{)}.\] (6)

_with \(\overline{\mathcal{F}}^{\mathrm{stnd}}\) the \(\overline{\mathcal{H}}\)-estimation error transformation for constrained losses defined for all \(t\in[0,1]\) by \(\overline{\mathcal{F}}^{\mathrm{stnd}}(t)=\)_

\[\begin{array}{ll}\inf_{\tau\geq 0}\sup_{\mu\in[\tau-A_{\min},\tau+A_{ \min}]}\big{\{}\frac{1-t}{2}[\Phi(\tau)-\Phi(-\tau+\mu)]+\frac{1+t}{2}[\Phi(- \tau)-\Phi(\tau-\mu)]\big{\}}&n=2\\ \inf_{P\in\left[\frac{1}{n-1},1\right]}\inf_{\tau\geq 2\max\{\tau,2\}} \sup_{\mu\in C}\bigl{\{}\frac{2-P-t}{2}[\Phi(-\tau_{2})-\Phi(-\tau_{1}+\mu)]+ \frac{2-P+t}{2}[\Phi(-\tau_{1})-\Phi(-\tau_{2}-\mu)]\big{\}}&n>2,\end{array}\]

_where \(C=[\max\{\tau_{1},-\tau_{2}\}-\Lambda_{\min},\min\{\tau_{1},-\tau_{2}\}+ \Lambda_{\min}]\) and \(\Lambda_{\min}=\inf_{x\in\mathcal{X}}\Lambda(x)\). Furthermore, for any \(t\in[0,1]\), there exist a distribution \(\mathcal{D}\) and a hypothesis \(h\in\mathcal{H}\) such that \(\mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}_{\ell_{0-1}}^{*}(\mathcal{H})+\mathcal{ M}_{\ell_{0-1}}(\mathcal{H})=t\) and \(\mathcal{R}_{\ell^{\mathrm{stnd}}}(h)-\mathcal{R}_{\ell^{\mathrm{stnd}}}^{*}( \mathcal{H})+\mathcal{M}_{\ell^{\mathrm{stnd}}}(\mathcal{H})=\mathcal{F}^{ \mathrm{stnd}}(t)\)._

Proof.: For the constrained loss \(\ell^{\mathrm{stnd}}\), the conditional \(\ell^{\mathrm{stnd}}\)-risk can be expressed as follows:

\[\mathcal{C}_{\ell^{\mathrm{stnd}}}(h,x) =\sum_{y\in\mathcal{Y}}p(x,y)\ell^{\mathrm{cstnd}}(h,x,y)\] \[=\sum_{y\in\mathcal{Y}}p(x,y)\sum_{y^{\prime}\neq y}\Phi(-h(x,y^{ \prime}))\] \[=\sum_{y\in\mathcal{Y}}\Phi(-h(x,y))\sum_{y^{\prime}\neq y}p(x,y^{ \prime})\] \[=\sum_{y\in\mathcal{Y}}\Phi(-h(x,y))(1-p(x,y))\] \[=\Phi(-h(x,y_{\max}))(1-p(x,y_{\max}))+\Phi(-h(x,\mathsf{h}(x)))(1 -p(x,\mathsf{h}(x)))\] \[\qquad+\sum_{yt\{y_{\max},\mathsf{h}(x)\}}\Phi(-h(x,y))(1-p(x,y)).\]For any \(h\in\overline{\mathcal{H}}\) and \(x\in\mathcal{X}\), by the definition of \(\overline{\mathcal{H}}\), we can always find a family of hypotheses \(\{h_{\mu}\}\subset\mathcal{H}\) such that \(h_{\mu}(x,\cdot)\) take the following values:

\[h_{\mu}(x,y)=\begin{cases}h(x,y)&\text{if }y\not\in\{y_{\max},\mathsf{h}(x)\}\\ h(x,y_{\max})+\mu&\text{if }y=\mathsf{h}(x)\\ h(x,\mathsf{h}(x))-\mu&\text{if }y=y_{\max}.\end{cases}\]

Note that the hypotheses \(h_{\mu}\) satisfies the constraint:

\[\sum_{y\in\mathcal{Y}}h_{\mu}(x,y)=\sum_{y\in\mathcal{Y}}h(x,y)=0,\;\forall\mu \in\mathbb{R}.\]

Since \(h_{\mu}(x,y)\in[-\Lambda(x),\Lambda(x)]\), we have the following constraints on \(\mu\):

\[-\Lambda(x)-h(x,y_{\max}) \leq\mu\leq\Lambda(x)-h(x,y_{\max})\] \[-\Lambda(x)+h(x,\mathsf{h}(x)) \leq\mu\leq\Lambda(x)+h(x,\mathsf{h}(x).\]

Let \(p_{1}=p(x,y_{\max})\), \(p_{2}=p(x,\mathsf{h}(x))\), \(\tau_{1}=h(x,\mathsf{h}(x))\) and \(\tau_{2}=h(x,y_{\max})\) to simplify the notation. Then, the constraint on \(\mu\) can be expressed as

\[\mu\in\overline{C},\quad\overline{C}=[\max\{\tau_{1},-\tau_{2}\}-\Lambda(x), \min\{\tau_{1},-\tau_{2}\}+\Lambda(x)]\]

Since \(\max\{\tau_{1},-\tau_{2}\}-\min\{\tau_{1},-\tau_{2}\}=|\tau_{1}+\tau_{2}|\leq |\tau_{1}|+|\tau_{2}|\leq 2\Lambda(x)\), \(C\) is not an empty set. By the definition of \(h_{\mu}\), we have for any \(h\in\mathcal{H}\) and \(x\in\mathcal{X}\),

\[\mathcal{C}_{\ell^{\mathrm{extend}}}(h,x)-\inf_{\mu\in\overline{C}} \mathcal{C}_{\ell^{\mathrm{extend}}}(h_{\mu},x)\] \[=\sup_{\mu\in\overline{C}}\left\{(1-p_{1})[\Phi(-\tau_{2})-\Phi (-\tau_{1}+\mu)]+(1-p_{2})[\Phi(-\tau_{1})-\Phi(-\tau_{2}-\mu)]\right\}\] \[=\sup_{\mu\in\overline{C}}\left\{\frac{2-P-p_{1}+p_{2}}{2}[\Phi( -\tau_{2})-\Phi(-\tau_{1}+\mu)]+\frac{2-P+p_{1}-p_{2}}{2}[\Phi(-\tau_{1})-\Phi (-\tau_{2}-\mu)]\right\}\] \[(P=p_{1}+p_{2}\in\left[\frac{1}{n-1},1\right])\] \[=\inf_{P\in\left[\frac{1}{n-1},1\right]}\inf_{\tau_{1}\geq\max\{ \tau_{2},0\}}\sup_{\mu\in\overline{C}}\left\{\frac{2-P-p_{1}+p_{2}}{2}[\Phi(- \tau_{2})-\Phi(-\tau_{1}+\mu)]\right.\] \[\qquad+\frac{2-P+p_{1}-p_{2}}{2}[\Phi(-\tau_{1})-\Phi(-\tau_{2}- \mu)]\right\}\] \[(C=[\max\{\tau_{1},-\tau_{2}\}-\Lambda_{\min},\min\{\tau_{1},- \tau_{2}\}+\Lambda_{\min}]\subset\overline{C}\text{ since }\Lambda_{\min}\leq\Lambda(x))\] \[=\inf_{P\in\left[\frac{1}{n-1},1\right]}\inf_{\tau_{1}\geq\max\{ \tau_{2},0\}}\left\{\frac{2-P-p_{1}+p_{2}}{2}\Phi(-\tau_{2})+\frac{2-P+p_{1}- p_{2}}{2}\Phi(-\tau_{1})\right.\] \[\qquad\left.-\inf_{\mu\in C}\left\{\frac{2-P-p_{1}+p_{2}}{2}\Phi( -\tau_{1}+\mu)+\frac{2-P+p_{1}-p_{2}}{2}\Phi(-\tau_{2}-\mu)\right\}\right\}\] \[=\mathcal{T}^{\mathrm{cstnd}}(p_{1}-p_{2})\] (by Lemma 1)

Since \(\mathcal{T}^{\mathrm{cstnd}}\) is convex, by Jensen's inequality, we obtain for any hypothesis \(h\in\mathcal{H}\) and any distribution,

\[\mathcal{T}^{\mathrm{cstnd}}\big{(}\mathcal{R}_{\ell_{0-1}}(h)- \mathcal{R}^{*}_{\ell_{0-1}}(\mathcal{H})+\mathcal{M}_{\ell_{0-1}}(\mathcal{H}) \big{)}\] \[=\mathcal{T}^{\mathrm{cstnd}}\Big{(}\frac{\mathbb{E}}{\mathcal{N} }[\Delta\mathcal{E}_{\ell_{0-1},\mathcal{H}}(h,x)]\Big{)}\] \[\leq\frac{\mathbb{E}}{\mathcal{N}}\big{[}\mathcal{T}^{\mathrm{ cstnd}}(\Delta\mathcal{E}_{\ell_{0-1},\mathcal{H}}(h,x))\big{]}\] \[\leq\frac{\mathbb{E}}{\mathcal{N}}\big{[}\Delta\mathcal{C}_{\ell ^{\mathrm{cstnd}},\mathcal{H}}(h,x)\big{]}\] \[=\mathcal{R}_{\ell^{\mathrm{cstnd}}}(h)-\mathcal{R}^{*}_{\ell^{ \mathrm{cstnd}}}(\mathcal{H})+\mathcal{M}_{\ell^{\mathrm{cstnd}}}(\mathcal{H}).\]

Let \(n=2\). For any \(t\in[0,1]\), we consider the distribution that concentrates on a singleton \(\{x\}\) and satisfies \(p(x,1)=\frac{1+t}{2}\), \(p(x,2)=\frac{1-t}{2}\). For any \(\epsilon>0\), by the definition of infimum, we can take \(h\in\mathcal{H}\) such that \(h(x,2)=\tau_{\epsilon}\geq 0\) and satisfies

\[\sup_{\mu\in[\tau_{\epsilon}-\Lambda_{\min},\tau_{\epsilon}+\Lambda_{\min}]} \bigg{\{}\frac{1-t}{2}[\Phi(\tau_{\epsilon})-\Phi(-\tau_{\epsilon}+\mu)]+\frac {1+t}{2}[\Phi(-\tau_{\epsilon})-\Phi(\tau_{\epsilon}-\mu)]\bigg{\}}<\mathcal{T }^{\mathrm{cstnd}}(t)+\epsilon.\]

Then,

\[\mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}^{*}_{\ell_{0-1}}( \mathcal{H})+\mathcal{M}_{\ell_{0-1}}(\mathcal{H}) =\mathcal{R}_{\ell_{0-1}}(h)-\mathbb{E}_{X}\big{[}\mathcal{C}^{*} _{\ell_{0-1}}(\mathcal{H},x)\big{]}\] \[=\mathcal{C}_{\ell_{0-1}}(h,x)-\mathcal{C}^{*}_{\ell_{0-1}}( \mathcal{H},x)\] \[=t\]

and

\[\mathcal{T}^{\mathrm{cstnd}}(t) \leq\mathcal{R}_{\ell^{\mathrm{cstnd}}}(h)-\mathcal{R}^{*}_{\ell ^{\mathrm{cstnd}}}(\mathcal{H})+\mathcal{M}_{\ell^{\mathrm{cstnd}}}(\mathcal{H})\] \[=\mathcal{R}_{\ell^{\mathrm{cstnd}}}(h)-\mathbb{E}_{X}[\mathcal{C }^{*}_{\ell^{\mathrm{cstnd}}}(\mathcal{H},x)]\] \[=\mathcal{C}_{\ell^{\mathrm{cstnd}}}(h,x)-\mathcal{C}^{*}_{\ell^{ \mathrm{cstnd}}}(\mathcal{H},x)\] \[=\sup_{\mu\in[\tau_{\epsilon}-\Lambda_{\min},\tau_{\epsilon}+ \Lambda_{\min}]}\bigg{\{}\frac{1-t}{2}[\Phi(\tau_{\epsilon})-\Phi(-\tau_{ \epsilon}+\mu)]+\frac{1+t}{2}[\Phi(-\tau_{\epsilon})-\Phi(\tau_{\epsilon}-\mu) ]\bigg{\}}\] \[<\mathcal{T}^{\mathrm{cstnd}}(t)+\epsilon.\]

By letting \(\epsilon\to 0\), we conclude the proof. The proof for \(n>2\) directly extends from the case when \(n=2\). Indeed, for any \(t\in[0,1]\), we consider the distribution that concentrates on a singleton \(\{x\}\) and satisfies \(p(x,1)=\frac{1+t}{2}\), \(p(x,2)=\frac{1-t}{2}\), \(p(x,y)=0,3\leq y\leq n\). For any \(\epsilon>0\), by the definition of infimum, we can take \(h\in\mathcal{H}\) such that \(h(x,1)=\tau_{1,\epsilon}\), \(h(x,2)=\tau_{2,\epsilon}\), \(h(x,3)=0\), \(3\leq y\leq n\) and satisfies \(\tau_{1,\epsilon}+\tau_{2,\epsilon}=0\), and

\[\inf_{P\in[\frac{n-1}{n-1},1]}\sup_{\mu\in C}\bigg{\{}\frac{2-P-t}{2} \big{[}\Phi(-\tau_{2,\epsilon})-\Phi(-\tau_{1,\epsilon}+\mu)\big{]}+\frac{2-P+t }{2}\big{[}\Phi(-\tau_{1,\epsilon})-\Phi(-\tau_{2,\epsilon}-\mu)\big{]}\bigg{\}}\] \[=\sup_{\mu\in C}\bigg{\{}\frac{1-t}{2}[\Phi(-\tau_{2,\epsilon})- \Phi(-\tau_{1,\epsilon}+\mu)]+\frac{1+t}{2}[\Phi(-\tau_{1,\epsilon})-\Phi(- \tau_{2,\epsilon}-\mu)]\bigg{\}}\] \[<\mathcal{T}^{\mathrm{cstnd}}(t)+\epsilon.\]

Then,

\[\mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}^{*}_{\ell_{0-1}}(\mathcal{H})+\mathcal{M} _{\ell_{0-1}}(\mathcal{H})=t\]

and

\[\mathcal{T}^{\mathrm{cstnd}}(t)\leq\mathcal{R}_{\ell^{\mathrm{cstnd}}}(h)- \mathcal{R}^{*}_{\ell^{\mathrm{cstnd}}}(\mathcal{H})+\mathcal{M}_{\ell^{ \mathrm{cstnd}}}(\mathcal{H})<\mathcal{T}^{\mathrm{cstnd}}(t)+\epsilon.\]

By letting \(\epsilon\to 0\), we conclude the proof. 

### Constrained exponential loss

**Theorem 13** (\(\overline{\mathcal{H}}\)-consistency bounds for constrained exponential loss).: _Let \(\Phi(t)=e^{-t}\). For any \(h\in\overline{\mathcal{H}}\) and any distribution,_

\[\mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}^{*}_{\ell_{0-1}}\big{(}\overline{ \mathcal{H}}\big{)}+\mathcal{M}_{\ell_{0-1}}\big{(}\overline{\mathcal{H}} \big{)}\leq\Psi^{-1}\big{(}\mathcal{R}_{\ell^{\mathrm{cstnd}}}(h)-\mathcal{R}^ {*}_{\ell^{\mathrm{cstnd}}}\big{(}\overline{\mathcal{H}}\big{)}+\mathcal{M}_{\ell^ {\mathrm{cstnd}}}\big{(}\overline{\mathcal{H}}\big{)}\big{)}\]

_where \(\Psi(t)=\begin{cases}1-\sqrt{1-t^{2}}&t\leq\frac{e^{2\Lambda_{\min}}-1}{e^{2 \Lambda_{\min}}+1}.\\ \frac{t}{2}\big{(}e^{\Lambda_{\min}}-e^{-\Lambda_{\min}}\big{)}+\frac{2-e^{ \Lambda_{\min}}-e^{-\Lambda_{\min}}}{2}&\mathrm{otherwise}.\end{cases}\)_Proof.: For \(n=2\), plugging in \(\Phi(t)=e^{-t}\) in Theorem 12, gives

\[\overline{\mathcal{J}}^{\mathrm{cstnd}}(t)=\inf_{\tau\geq 0}\sup_{\mu\in[ \tau-\Lambda_{\min},\tau+\Lambda_{\min}]}\bigg{\{}\frac{1-t}{2}[e^{-\tau}-e^{ \tau-\mu}]+\frac{1+t}{2}[e^{\tau}-e^{-\tau+\mu}]\bigg{\}}.\]

By differentiating with respect to \(\tau\), we can see that the infimum is achieved when \(\tau=0\) modulo some elementary analysis. Thus, \(\overline{\mathcal{J}}^{\mathrm{cstnd}}\) can be reformulated as

\[\overline{\mathcal{J}}^{\mathrm{cstnd}}= \sup_{\mu\in[-\Lambda_{\min},\Lambda_{\min}]}\bigg{\{}\frac{1-t}{ 2}[1-e^{-\mu}]+\frac{1+t}{2}[1-e^{\mu}]\bigg{\}}\] \[= 1+\sup_{\mu\in[-\Lambda_{\min},\Lambda_{\min}]}g(\mu).\]

where \(g(\mu)=-\frac{1-t}{2}e^{-\mu}-\frac{1+t}{2}e^{\mu}\). Since \(g\) is continuous, it attains its supremum over a compact set. Note that \(g\) is concave and differentiable. In view of that, the maximum over the open set \((-\infty,+\infty)\) can be obtained by setting its gradient to zero. Differentiate \(g(\mu)\) to optimize, we obtain

\[g(\mu^{*})=0,\quad\mu^{*}=\frac{1}{2}\log\frac{1-t}{1+t}\]

Moreover, by the concavity, \(g(\mu)\) is non-increasing when \(\mu\geq\mu^{*}\). Since \(\mu^{*}\leq 0\) and \(\Lambda_{\min}\geq 0\), we have

\[\mu^{*}\leq 0\leq\Lambda_{\min}\]

In view of the constraint, if \(\mu^{*}\geq-\Lambda_{\min}\), the maximum is achieved by \(\mu=\mu^{*}\). Otherwise, if \(\mu^{*}<-\Lambda_{\min}\), since \(g(\mu)\) is non-increasing when \(\mu\geq\mu^{*}\), the maximum is achieved by \(\mu=-\Lambda_{\min}\). Since \(\mu^{*}\geq-\Lambda_{\min}\) is equivalent to \(t\leq\frac{e^{2\Lambda_{\min}-1}}{e^{2\Lambda_{\min}+1}}\), the maximum can be expressed as

\[\max_{\mu\in[-\Lambda_{\min},\Lambda_{\min}]}g(\mu)=\begin{cases}g(\mu^{*})&t \leq\frac{e^{2\Lambda_{\min}-1}}{e^{2\Lambda_{\min}+1}}\\ g(-\Lambda_{\min})&\text{otherwise}\end{cases}\]

Computing the value of \(g\) at these points yields:

\[g(\mu^{*}) =-\sqrt{1-t^{2}}\] \[g(-\Lambda_{\min}) =-\frac{1-t}{2}e^{\Lambda_{\min}}-\frac{1+t}{2}e^{-\Lambda_{\min }}.\]

Then, if \(t\leq\frac{e^{2\Lambda_{\min}-1}}{e^{2\Lambda_{\min}+1}}\), we obtain

\[\overline{\mathcal{J}}^{\mathrm{cstnd}}=1-\sqrt{1-t^{2}}.\]

Otherwise, we obtain

\[\overline{\mathcal{J}}^{\mathrm{cstnd}} =1-\frac{1-t}{2}e^{\Lambda_{\min}}-\frac{1+t}{2}e^{-\Lambda_{\min}}\] \[=\frac{t}{2}\big{(}e^{\Lambda_{\min}}-e^{-\Lambda_{\min}}\big{)} +\frac{2-e^{\Lambda_{\min}}-e^{-\Lambda_{\min}}}{2}.\]

For \(n>2\), plugging in \(\Phi(t)=e^{-t}\) in Theorem 12, gives

\[\overline{\mathcal{J}}^{\mathrm{cstnd}}(t)=\inf_{P\in[\frac{1}{n-1},1]}\inf_{ \tau_{1}\geq\max\{\tau_{2},0\}}\sup_{\mu\in C}\bigg{\{}\frac{2-P-t}{2}[e^{ \tau_{2}}-e^{\tau_{1}-\mu}]+\frac{2-P+t}{2}[e^{\tau_{1}}-e^{\tau_{2}+\mu}] \bigg{\}}.\]

where \(C=[\max\{\tau_{1},-\tau_{2}\}-\Lambda_{\min},\min\{\tau_{1},-\tau_{2}\}+ \Lambda_{\min}]\). By differentiating with respect to \(\tau_{2}\) and \(P\), we can see that the infimum is achieved when \(\tau_{2}=\tau_{1}=0\) and \(P=1\) modulo some elementary analysis. Thus, \(\overline{\mathcal{J}}^{\mathrm{cstnd}}\) can be reformulated as

\[\overline{\mathcal{J}}^{\mathrm{cstnd}} =\sup_{\mu\in C}\bigg{\{}\frac{1-t}{2}[1-e^{-\mu}]+\frac{1+t}{2}[1 -e^{\mu}]\bigg{\}}\] \[=1+\sup_{\mu\in C}g(\mu).\]where \(C=[-\Lambda_{\min},\Lambda_{\min}]\) and \(g(\mu)=-\frac{1-t}{2}e^{-\mu}-\frac{1+t}{2}e^{\mu}\). Since \(g\) is continuous, it attains its supremum over a compact set. Note that \(g\) is concave and differentiable. In view of that, the maximum over the open set \((-\infty,+\infty)\) can be obtained by setting its gradient to zero. Differentiate \(g(\mu)\) to optimize, we obtain

\[g(\mu^{*})=0,\quad\mu^{*}=\frac{1}{2}\log\frac{1-t}{1+t}\]

Moreover, by the concavity, \(g(\mu)\) is non-increasing when \(\mu\geq\mu^{*}\). Since \(\mu^{*}\leq 0\) and \(\Lambda_{\min}\geq 0\), we have

\[\mu^{*}\leq 0\leq\Lambda_{\min}\]

In view of the constraint, if \(\mu^{*}\geq-\Lambda_{\min}\), the maximum is achieved by \(\mu=\mu^{*}\). Otherwise, if \(\mu^{*}<-\Lambda_{\min}\), since \(g(\mu)\) is non-increasing when \(\mu\geq\mu^{*}\), the maximum is achieved by \(\mu=-\Lambda_{\min}\). Since \(\mu^{*}\geq-\Lambda_{\min}\) is equivalent to \(t\leq\frac{e^{2\Lambda_{\min}-1}}{e^{2\Lambda_{\min}}+1}\), the maximum can be expressed as

\[\max_{\mu\in[-\Lambda_{\min},\Lambda_{\min}]}g(\mu)=\begin{cases}g(\mu^{*})&t \leq\frac{e^{2\Lambda_{\min}-1}}{e^{2\Lambda_{\min}+1}}\\ g(-\Lambda_{\min})&\text{otherwise}\end{cases}\]

Computing the value of \(g\) at these points yields:

\[g(\mu^{*}) =-\sqrt{1-t^{2}}\] \[g(-\Lambda_{\min}) =-\frac{1-t}{2}e^{\Lambda_{\min}}-\frac{1+t}{2}e^{-\Lambda_{\min }}.\]

Then, if \(t\leq\frac{e^{2\Lambda_{\min}-1}}{e^{2\Lambda_{\min}+1}}\), we obtain

\[\overline{\mathcal{Y}}^{\mathrm{cstnd}}=1-\sqrt{1-t^{2}}.\]

Otherwise, we obtain

\[\overline{\mathcal{Y}}^{\mathrm{cstnd}} =1-\frac{1-t}{2}e^{\Lambda_{\min}}-\frac{1+t}{2}e^{-\Lambda_{\min}}\] \[=\frac{t}{2}\big{(}e^{\Lambda_{\min}}-e^{-\Lambda_{\min}}\big{)} +\frac{2-e^{\Lambda_{\min}}-e^{-\Lambda_{\min}}}{2}.\]

Since \(\overline{\mathcal{Y}}^{\mathrm{cstnd}}\) is convex, by Theorem 12, for any \(h\in\overline{\mathcal{Y}}\) and any distribution,

\[\mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}_{\ell_{0-1}}^{*}\big{(}\overline{ \mathcal{Y}}\big{)}+\mathcal{M}_{\ell_{0-1}}\big{(}\overline{\mathcal{Y}} \big{)}\leq\Psi^{-1}\big{(}\mathcal{R}_{\ell^{\mathrm{cstnd}}}(h)-\mathcal{R}_ {\ell^{\mathrm{cstnd}}}^{*}\big{(}\overline{\mathcal{Y}}\big{)}+\mathcal{M}_{ \ell^{\mathrm{cstnd}}}\big{(}\overline{\mathcal{Y}}\big{)}\big{)}\]

where

\[\Psi(t)=\begin{cases}1-\sqrt{1-t^{2}}&t\leq\frac{e^{2\Lambda_{\min}-1}}{e^{2 \Lambda_{\min}+1}}\\ \frac{t}{2}\big{(}e^{\Lambda_{\min}}-e^{-\Lambda_{\min}}\big{)}+\frac{2-e^{ \Lambda_{\min}-e^{-\Lambda_{\min}}}}{2}&\text{otherwise}.\end{cases}\]