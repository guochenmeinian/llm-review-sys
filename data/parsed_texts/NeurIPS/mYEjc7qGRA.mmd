# Towards Robust Multimodal Sentiment Analysis

with Incomplete Data

 Haoyu Zhang\({}^{1,2}\), Wenbin Wang\({}^{3}\), Tianshu Yu\({}^{1,}\)

\({}^{1}\)School of Data Science, The Chinese University of Hong Kong, Shenzhen

\({}^{2}\)Department of Computer Science, University College London

\({}^{3}\)School of Computer Science, Wuhan University

{zhanghaoyu, yutianshu}@cuhk.edu.cn

haoyu.zhang.23@ucl.ac.uk

wangwenbin97@whu.edu.cn

the corresponding authorThe code is available at: https://github.com/Haoyu-ha/LNLN

###### Abstract

The field of Multimodal Sentiment Analysis (MSA) has recently witnessed an emerging direction seeking to tackle the issue of data incompleteness. Recognizing that the language modality typically contains dense sentiment information, we consider it as the dominant modality and present an innovative Language-dominated Noise-resistant Learning Network (LNLN) to achieve robust MSA. The proposed LNLN features a dominant modality correction (DMC) module and dominant modality based multimodal learning (DMML) module, which enhances the model's robustness across various noise scenarios by ensuring the quality of dominant modality representations. Aside from the methodical design, we perform comprehensive experiments under random data missing scenarios, utilizing diverse and meaningful settings on several popular datasets (_e.g.,_ MOSI, MOSEI, and SIMS), providing additional uniformity, transparency, and fairness compared to existing evaluations in the literature. Empirically, LNLN consistently outperforms existing baselines, demonstrating superior performance across these challenging and extensive evaluation metrics.1

Footnote 1: The code is available at: https://github.com/Haoyu-ha/LNLN

## 1 Introduction

The field of Multimodal Sentiment Analysis (MSA) is at the vanguard of human sentiment identification by assimilating heterogeneous data types, such as video, audio, and language. Its applicability spans numerous fields, prominent among healthcare and human-computer interaction (Jiang et al., 2020). MSA has become essential in enhancing both the precision and the robustness of sentiment analysis by drawing sentiment cues from diverse perspectives.

Changing trends in recent research has taken a turn towards modeling data in natural scenarios from laboratory conditions (Tsai et al., 2019; Hazarika et al., 2020; Yu et al., 2021; Zhang et al., 2023). The shift has created a wider application space in the real-world for MSA, though concerns arise due to problems like sensor failures and problems with Automatic Speech Recognition (ASR), leading to inconsistencies such as incomplete data in real-world deployment.

Numerous impactful solutions have been proposed against this primary concern of incomplete data in multimodal sentiment analysis. For instance, Yuan et al. (2021) introduced a Transformer-based feature reconstruction mechanism, TFR-Net, aiming to strengthen the robustness of the model handling random missing in unaligned multimodal sequences via reconstructing missing data.

Furthermore, Yuan et al. (2024) proposed the Noise Intimating-based Adversarial Training (NIAT) model, which superiorly learns a unified joint representation between an original-noisy instance pair, utilizing the attention mechanism and adversarial learning. Lastly, Li et al. (2024) design a Unified Multimodal Missing Modality Self-Distillation Framework (UMDF) which leverages a single network to learn robust inherent representations from consistent multimodal data distributions. Yet, despite these developments, the evaluation metrics for these models are inconsistent, and the evaluation settings are not sufficiently comprehensive. This inconsistency limits effective comparisons and hinders the dissemination of knowledge in the field.

Addressing this gap, our paper aims to offer a comprehensive evaluation on three widely-used datasets, namely MOSI (Zadeh et al., 2016), MOSEI (Zadeh et al., 2018) and SIMS (Yu et al., 2020) datasets. We introduce random data missing instances and subsequently compare the performance of existing methods on these datasets. This endeavor seeks to provide an all-encompassing outlook for evaluating the effectiveness and robustness of various methods in the face of incomplete data, thereby sparking new insights in the field. Additionally, inspired by the previous work ALMT (Zhang et al., 2023), we hypothesize that model robustness improves when the integrity of the dominant modality is preserved despite varying noise levels. Therefore, we introduce a novel model, namely Language-dominated Noise-resistant Learning Network (LNLN), to enhance MSA's robustness over incomplete data. LNLN aims to augment the integrity of the language modality's features, regarded as the dominant modality due to its richer sentiment cues, with the support of other auxiliary modalities. The LNLN's robustness against varying levels of data incompleteness is achieved through a dominant modality correction (DMC) module for dominant modality construction, a dominant modality based multimodal learning (DMML) module for multimodal fusion and classification, and a reconstructor for reconstructing missing information to shield the dominate modality from noise interference. This approach ensures a high-quality dominant modality feature, which significantly bolsters the robustness of LNLN under diverse noise conditions. Consequently, extensive experimental results demonstrate the LNLN's superior performance across these challenging and evaluation metrics.

In summary, this paper conducts a comprehensive evaluation of existing advanced MSA methods. This analysis highlights the strengths and weaknesses of various methods when contending with incomplete data. We believe, it can improve the understanding of different MSA methods' performance under complex real-world scenarios, thereby informing technology's future trajectory. Our proposed LNLN also offers valuable insight and guidelines for thriving in this research space.

## 2 Related Work

### Multimodal Sentiment Analysis

Multimodal Sentiment Analysis (MSA) methods can be categorized into Context-based MSA and Noise-aware MSA, depending on the modeling approach. Most of previous works (Zadeh et al., 2017; Tsai et al., 2019; Mai et al., 2020; Hazarika et al., 2020; Liang et al., 2020; Rahman et al., 2020; Yu et al., 2021; Han et al., 2021; Lv et al., 2021; Yang et al., 2022; Guo et al., 2022; Zhang et al., 2023) can be classified to Context-based MSA. This line of work primarily focuses on learning unified multimodal representations by analyzing contextual relationships within or between modalities. For example, Zadeh et al. (2017) explore computing the relationships between different modalities using the Cartesian product. Tsai et al. (2019) utilize pairs of Transformers to model long dependencies between different modalities. Yu et al. (2021) propose generating pseudo-labels for each modality to further mine the information of consistency and discrepancy between different modalities.

Despite these advances, context-based methods are usually suboptimal under varying levels of noise effects (_e.g._ random data missing). Several recent works (Mittal et al., 2020; Yuan et al., 2021, 2024; Li et al., 2024) have been proposed to tackle this issue. For example, Mittal et al. (2020) introduce a modality check step to distinguish invalid and valid modalities, achieving higher robustness. Yuan et al. (2024) propose learning a unified joint representation between constructed "original-noisy" instance pairs. Although there have been some advances in improving the model's robustness under noise scenarios, no extant method has provided a comprehensive and in-depth comparative analysis.

### Robust Representation Learning in MSA

Context-based MSA and Noise-aware MSA differ in their approaches to robust representation learning. In Context-based MSA, robust representation learning typically relies on modeling intra- and inter-modality relationships. For instance, Hazarika et al. (2020) and Yang et al. (2022) apply feature disentanglement to each modality, modeling multimodal representations from multiple feature subspaces and perspectives. Yu et al. (2021) and Liang et al. (2020) explore self-supervised learning and semi-supervised learning to enhance multimodal representations, respectively. Tsai et al. (2019) and Rahman et al. (2020) introduce Transformer to learn the long dependencies of modalities. Zhang et al. (2023) devise a language-guided learning mechanism that uses modalities with more intensive sentiment cues to guide the learning of other modalities. In contrast, Noise-aware MSA focuses more on perceiving and eliminating the noise present in the data. For example, Mittal et al. (2020) design a modality check module based on metric learning and Canonical Correlation Analysis (CCA) to identify the modality with greater noise. Yuan et al. (2021) design a feature reconstruction network to predict the location of missing information in sequences and reconstruct it. Yuan et al. (2024) introduce adversarial learning (Goodfellow et al., 2014) to perceive and generate cleaner representations.

In this work, the LNLN belongs to the noise-aware MSA category. Inspired by Zhang et al. (2023), we explore the capability of language-guided mechanisms in resisting noise and aim to provide new perspectives for the study of MSA in noisy scenarios.

## 3 Method

### Overview

The overall pipeline for the proposed Language-dominated Noise-resistant Learning Network (LNLN) in robust multimodal sentiment analysis is illustrated in Figure 1. As depicted, a crucial initial step involves forming a multimodal input with random data missing, which sets the stage for LNLN training. Once the input is prepared, LNLN first utilizes an embedding layer to standardize the dimension of each modality, ensuring uniformity. Recognizing that language is the dominant modality in MSA (Zhang et al., 2023), a specially designed Dominant Modality Correction (DMC) module employs adversarial learning and a dynamic weighted enhancement strategy to mitigate noise impacts. This module first enhances the quality of the dominant feature computed from the language modality and then integrates them with the auxiliary modalities (visual and audio) in the dominant modality based multimodal learning (DMML) module for effective multimodal fusion and classification. This process significantly bolsters LNLN's robustness against various noise levels. Moreover, to refine the network's capability for fine-grained sentiment analysis, a simple reconstructor is implemented to reconstruct missing data, further enhancing the system's robustness.

### Input Construction and Multimodal Input

Given the challenges of random data missing, we have constructed data sets that simulate these conditions based on MOSI, MOSEI, and SIMS datasets.

**Random Data Missing.** Following the previous method (Yuan et al., 2021), for each modality, we randomly erased changing proportions of information (from 0% to 100%). Specifically, for visual and audio modalities, we fill the erased information with zeros. For language modality, we fill the erased information with [UNK] which indicates the unknown word in BERT (Kenton and Toutanova, 2019).

**Multimodal Input.** For each sample in the dataset, we incorporate data from three modalities: language, audio, and visual data. Consistent with previous works (Zhang et al., 2023), each modality is processed using widely-used tools: language data is encoded using BERT (Kenton and Toutanova, 2019), audio features are extracted through Librosa (McFee et al., 2015), and visual features are obtained using OpenFace (Baltrusaitis et al., 2018). These pre-processed inputs are represented as sequences, denoted by \(U^{0}_{m}\in\mathbb{R}^{T_{m}\times d_{m}}\), where \(m\in\{l,v,a\}\) represents the modality type (\(l\) for language, \(v\) for visual, \(a\) for audio), \(T_{m}\) indicates the sequence length and \(d_{m}\) refers to the dimension of each modality's vector. With obtained \(U^{0}_{m}\), we apply random data missing to \(U^{0}_{m}\), thus forming the noise-corrupted multimodal input \(U^{1}_{m}\in\mathbb{R}^{T_{m}\times d_{m}}\).

### Dominant Modality based Multimodal Learning

Inspired by previous work ALMT, we hypothesize that model robustness improves when the integrity of the dominant modality is preserved despite varying noise levels. We improve ALMT based on the designed DMC module and Reconstructor, thus implementing dominant modality based DMML module for sentiment analysis under random data missing scenarios. Here, we mainly introduce the parts that differ from ALMT. Further details are available in Zhang et al. (2023).

**Modality Embedding.** For multimodal input \(U_{m}^{1}\), we employ an Embedding Encoder \(\mathrm{E}_{m}^{1}\) with two Transformer encoder layers to extract and unify the feature. Each modality begins with a randomly initialized low-dimensional token \(H_{m}^{0}\in\mathbb{R}^{T\times d_{m}}\). These tokens are then processed by the Transformer encoder layer, embedding essential modality information and producing unified features, represented as \(H_{m}^{1}\in\mathbb{R}^{T\times d}\). The process is formalized by:

\[H_{m}^{1}=\mathrm{E}_{m}^{1}\left(\mathrm{concat}\left(H_{m}^{0},U_{m}^{1} \right)\right),\] (1)

where \(\mathrm{E}_{m}^{1}(\cdot)\) extracts features for each modality, and \(\mathrm{concat}(\cdot)\) represents the concatenation operation.

**Adaptive Hyper-modality Learning.** In the original ALMT, each Adaptive Hyper-modality Learning layer contains a Transformer and two multi-head attention (MHA) modules. These are applied to learn language representations at different scales and hyper-modality representations from visual and audio modalities, guided by the language modality. Considering the possibility of severe interference in the language modality (_i.e._ dominant modality) due to random data missing, we designed a Dominant Modality Correction (DMC) module to generate the proxy dominant feature \(H_{p}^{1}\) and construct corrected dominate feature \(H_{d}^{1}\) (more details can be found in Section 3.4). Specifically, the process of learning corrected dominated representation \(H_{d}^{i}\) at different scales can be described as:

\[H_{d}^{i}=\mathrm{E}_{m}^{i}\left(H_{d}^{i-1}\right),\] (2)

where \(i\in\{2,3\}\) means the \(i\)-th layer of Adaptive Hyper-modality Learning module, \(\mathrm{E}_{m}^{i}(\cdot)\) is the i-th Transformer encoder layer, \(H_{d}^{i}\in\mathbb{R}^{T\times d}\) is corrected dominated feature at different scale. To learn the hyper-modality representation, the corrected dominated feature and audio/visual features are used to calculate Query and Key/Value, respectively. Briefly, the process can be written as follows:

\[H_{hyper}^{i}=H_{hyper}^{i-1}+\mathrm{MHA}(H_{d}^{i},H_{a}^{1})+\mathrm{MHA}( H_{d}^{i},H_{v}^{1}),\] (3)

where \(\mathrm{MHA}(\cdot)\) represents multi-head attention, \(H_{hyper}^{i}\in\mathbb{R}^{T\times d}\) is the hyper-modality feature. Note that the feature \(H_{hyper}^{0}\in\mathbb{R}^{T\times d}\) is a random initialized vector.

Figure 1: Overall pipeline. Note: \(H_{l}^{0}\), \(H_{v}^{0}\), \(H_{a}^{0}\), \(H_{cc}\), and \(H_{p}^{0}\) are randomly initialized learnable vectors.

**Multimodal Fusion and Prediction.** With obtained \(H_{d}^{3}\) and \(H_{hyper}^{3}\), a Transformer encoder with a classifier at a depth of 4 layers is employed for multimodal fusion and sentiment prediction:

\[\hat{y}=\mathrm{CrossTransformer}(H_{d}^{3},H_{hyper}^{3}),\] (4)

where \(\hat{y}\) is the sentiment prediction.

### Dominant Modality Correction

This module consists of two steps, _i.e.,_ completeness check of dominant modality and proxy dominant feature generation using adversarial learning (Ganin and Lempitsky, 2015; Goodfellow et al., 2014).

**Completeness Check.** We apply an encoder \(\mathrm{E}_{cc}\) that consists of a Transformer encoder with a depth of two layers and a classifier for completeness check. For example, if the missing rate of dominate modality is 0.3, the label of completeness is 0.7. This completeness prediction \(w\) can be obtained as follows:

\[w=\mathrm{E}_{cc}\left(\mathrm{Concat}\left(H_{cc},H_{l}^{1}\right)\right),\] (5)

where \(H_{cc}\in\mathbb{R}^{T\times d}\) is a randomly initialized token for completeness prediction. We optimize this process using L2 loss:

\[\mathcal{L}_{cc}=\frac{1}{N_{b}}\sum_{k=0}^{N_{b}}\left\|w^{k}-\hat{w}^{k} \right\|_{2}^{2},\] (6)

where \(N_{b}\) is the number of samples in the training set, \(\hat{w}^{k}\) is the label of completeness of \(k\)-th sample.

**Proxy Dominant Feature Generation.** With the randomly initialized feature \(H_{p}^{0}\in\mathbb{R}^{T\times d}\), visual feature \(H_{v}^{1}\) and audio feature \(H_{a}^{1}\), we employ a Proxy Dominant Feature Generator \(E_{DFG}\), which consists of two Transformer encoder layers. This setup generates the proxy dominant features \(H_{p}^{1}\in\mathbb{R}^{T\times d}\), designed to complement and correct the dominant modality. The corrected dominant feature \(H_{d}^{1}\in\mathbb{R}^{T\times d}\) is calculated by combining \(H_{p}^{1}\) and the language feature \(H_{l}^{1}\), weighted by the predicted completeness \(w\):

\[H_{p}^{1}=\mathrm{E}_{DFG}\left(\mathrm{Concat}\left(H_{p}^{0},H_{a}^{1},H_{v} ^{1}\right),\theta_{DFG}\right),\] (7)

\[H_{d}^{1}=(1-w)*H_{p}^{1}+w*H_{l}^{1},\] (8)

where \(\theta_{DFG}\) denotes the parameters of the Proxy Dominant Feature Generator \(E_{DFG}\).

To ensure that the agent feature offers a distinct perspective from the visual and audio features, we utilize an effectiveness discriminator \(D\). This discriminator includes a binary classifier and a Gradient Reverse Layer (GRL) (Ganin and Lempitsky, 2015) and is tasked with identifying the origin of the agent features:

\[\hat{y}_{p}=\mathrm{D}\left(H_{p}^{1}/H_{l}^{1},\theta_{D}\right),\] (9)

where \(\theta_{D}\) represents the parameters of the effectiveness discriminator \(D\), and \(\hat{y}_{p}\) indicates the prediction of whether the input feature originates from the language modality.

In practice, the generator and the discriminator engage in an adversarial learning structure. The discriminator aims to identify whether the features are derived from the language modality, while the generator's objective is to challenge the discriminator's ability to make accurate predictions. This dynamic is encapsulated in the adversarial learning objective:

\[\min_{\theta_{D}}\max_{\theta_{DFG}}\mathcal{L}_{adv}=-\frac{1}{N_{b}}\sum_{k=0 }^{N_{b}}y_{p}^{k}\cdot\log\hat{y}_{p}^{k},\] (10)

where \(N_{b}\) is the number of samples in the training set, and \(y_{p}^{k}\) indicates the label determining whether the input feature for the \(k\)-th sample originates from the visual or audio modality.

### Reconstructor

Our experiments demonstrate that reconstructing missing information can significantly enhance regression metrics. More details about this are shown in Table 4. To address this, we have developeda reconstructor, denoted as \(E_{rec}\), which comprises two Transformer layers designed to effectively rebuild missing information of each modality. The operational equation for the reconstructor is:

\[\hat{U}_{m}^{0}=\mathrm{E}_{m}^{rec}\left(U_{m}^{1}\right)\] (11)

where \(\hat{U}_{m}^{0}\) is the reconstructed feature corresponding to the feature \(U_{m}^{0}\).

To optimize the performance of the reconstructor, we apply an L2 loss function:

\[\mathcal{L}_{rec}=\frac{1}{N_{b}}\sum_{h=0}^{N_{b}}\sum_{m}\left\|U_{m}^{0\;k} -\hat{U}_{m}^{0\;k}\right\|_{2}^{2},\] (12)

where \(U_{m}^{0\;k}\) and \(\hat{U}_{m}^{0\;k}\) represent the original and reconstructed features with missing information for the \(k\)-th sample, respectively. This loss function helps minimize the discrepancies between the original and reconstructed features, thereby improving the accuracy of other components, such as Dominant Modality Correction and final sentiment prediction.

### Overall Learning Objectives

To sum up, our method involves four learning objectives, including a completeness check loss \(\mathcal{L}_{ce}\), an adversarial learning loss \(\mathcal{L}_{adv}\) for proxy dominant feature generation, a reconstruction loss \(\mathcal{L}_{rec}\) and one final sentiment prediction loss \(\mathcal{L}_{sp}\). The sentiment prediction loss \(\mathcal{L}_{sp}\) can be described as:

\[\mathcal{L}_{sp}=\frac{1}{N_{b}}\sum_{n=0}^{N_{b}}\left\|y^{n}-\hat{y}^{n} \right\|_{2}^{2},\] (13)

Therefore, the overall loss \(\mathcal{L}\) can be written as:

\[\mathcal{L}=\alpha\mathcal{L}_{ce}+\beta\mathcal{L}_{adv}+\gamma\mathcal{L}_{ rec}+\delta\mathcal{L}_{sp},\] (14)

where \(\alpha\), \(\beta\), \(\gamma\) and \(\delta\) are hyperparameters. On MOSI and MOSEI datasets, we empirically set them to 0.9, 0.8, 0.1, and 1.0, respectively. On the SIMS dataset, we empirically set them to 0.9, 0.6, 0.1, and 1.0, respectively.

## 4 Experiments and Analysis

In this section, we provide a comprehensive and fair comparison between the proposed LNLN and previous representative MSA methods on MOSI (Zadeh et al., 2016), MOSEI (Zadeh et al., 2018) and SIMS (Yu et al., 2020) datasets.

### Datasets

**MOSI.** The dataset includes 2,199 multimodal samples, integrating visual, audio, and language modalities. It is divided into a training set of 1,284 samples, a validation set of 229 samples, and a test set of 686 samples. Every single sample has been given a sentiment score, varying from -3, indicating strongly negative sentiment, to 3, signifying strongly positive sentiment.

**MOSEI.** The dataset consists of 22,856 video clips sourced from YouTube. The sample is divided into 16,326 clips for training, 1,871 for validation, and 4,659 for testing. Each clip is labeled with a score, ranging from -3, denoting the strongly negative, to 3, denoting the strongly positive.

**SIMS.** The dataset is a Chinese multimodal sentiment dataset that includes 2,281 video clips sourced from different movies and TV series. It has been partitioned into 1,368 samples for training, 456 for validation, and 457 for testing. Each sample has been manually annotated with a sentiment score ranging from -1 (negative) to 1 (positive).

### Evaluation Settings and Criteria

For a fair and comprehensive evaluation, we experiment ten times, setting the missing rates \(r\) to predefined values from 0 to 0.9 with an increment of 0.1. For instance, 50% of the information is randomly erased from each modality in the test data when \(r=0.5\). Unlike previous works (Yuan et al., 2021, 2024), we did not evaluate at \(r=1.0\), as this would imply complete data erasure from each modality, rendering the experiment non-informative. With the obtained results of each missing rate, we compute the average value as the model's overall performance under different levels of noise.

For evaluation criteria, we report the binary classification accuracy (Acc-2), the F1 score associated with Acc-2, and the mean absolute error (MAE). For Acc-2, we calculated accuracy and F1 in two ways: negative/positive (left-side value of _/_) and negative/non-negative (right-side value of _/_) on the MOSI and MOSEI datasets, respectively. Additionally, we provide the three-class accuracy (Acc-3), the seven-class accuracy (Acc-7), and the correlation of the model's prediction with humans (Corr) on the MOSI dataset. For the SIMS dataset, we report Acc-3, the five-class accuracy (Acc-5), and Corr. Due to the distinct focus of regression and classification metrics on different aspects of model performance, a model achieving the lowest error on regression metrics may not necessarily exhibit optimal performance on classification metrics. To comprehensively reflect the model capabilities, we select the best-performing checkpoint for each type of metric across all models in the comparisons, thus capturing the peak performance of both regression and classification aspects independently.

### Implementation Details

We used PyTorch 2.2.1 to implement the method. The experiments were conducted on a PC with an AMD EPYC 7513 CPU and an NVIDIA Tesla A40. To ensure consistent and fair comparisons across all methods, we conducted each experiment three times using fixed random seeds of 1111, 1112, and 1113. Details of the hyperparameters are shown in Table 1. In addition, the result of MISA, Self-MM, MMIM, CENET, TETFN, and TFR-Net is reproduced by the authors from open source code in the MMSA2(Mao et al., 2022), which is a unified framework for MSA, using default hyperparameters. The result of ALMT is reproduced by the authors from open source code on Github3.

Footnote 2: MMSA: https://github.com/thuier/MMSA

Footnote 3: ALMT: https://github.com/Haoyu-ha/ALMT

### Robustness Comparison

Tables 2 and 3 show the robustness evaluation results on the MOSI, MOSEI, and SIMS datasets. As shown in Table 2, LNLN achieves state-of-the-art performance on most metrics. On the MOSI dataset, LNLN achieved a relative improvement of 9.46% on Acc-7 compared to the sub-optimal result obtained by MMIM, demonstrating the robustness of LNLN in the face of different noise effects. However, on the MOSEI dataset, LNLN achieves only sub-optimal performance on metrics such as Acc-7 and Acc-5. After analyzing the data distribution (see Appendix A.3) and the confusion matrix (see Appendix A.6), we believe that this is because LNLN does not overly bias the neutral category, which has a disproportionate number of samples in noisy scenarios. As shown in Table 3, LNLN achieves an improvement in F1 on the SIMS dataset, with a relative improvement of 9.17% on F1 compared to the sub-optimal result obtained by ALMT. Similar to CENET's performance on the MOSEI dataset, TETFN on the SIMS dataset has a tendency to predict inputs as weakly negative categories with high sample sizes, resulting in seemingly better performance on some metrics. Additionally, as shown in Figure 2, we present the performance curves of several advanced

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & MOSI & MOSEI & SIMS \\ \hline Vector Length \(T\) & 8 & 8 & 8 \\ Vector Dimension \(d\) & 128 & 128 & 128 \\ Batch Size & 64 & 64 & 64 \\ Initial Learning Rate & 1e-4 & 1e-4 & 1e-4 \\ Loss Weight \(\alpha\), \(\beta\), \(\gamma\), \(\delta\) & 0.9, 0.8, 0.1, 1.0 & 0.9, 0.8, 0.1, 1.0 & 0.9, 0.6, 0.1, 1.0 \\ Optimizer & AdamW & AdamW & AdamW \\ Epochs & 200 & 200 & 200 \\ Warm Up & ✓ & ✓ & ✓ \\ Cosine Annealing & ✓ & ✓ & ✓ \\ Early Stop & ✓ & ✓ & ✓ \\ Seed & 1111,1112,1113 & 1111,1112,1113 & 1111,1112,1113 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Hyperparameters of LNLN we use on the different datasetsmethods under varying missing rates. The results demonstrate that the proposed LNLN consistently outperforms others across most scenarios, showing its robustness under different missing rates.

Overall, LNLN attempts to make predictions in the face of highly noisy inputs without the severe lazy behavior observed in other models, which often leads to predictions heavily biased towards a certain category. This demonstrates that LNLN shows strong robustness and competitiveness across various datasets and noise levels, highlighting its effectiveness in multimodal sentiment analysis.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{6}{c}{MOSI} & \multicolumn{6}{c}{MOSEI} \\ \cline{2-13}  & Acc.7 & Acc.5 & Acc.2 & F1 & MAE & Corr & Acc.7 & Acc.5 & Acc.2 & F1 & MAE & Corr \\ \hline MISA & 29.58 & 33.08 & 71.49 / 70.33 & 71.28 / 70.00 & 1.085 & 0.524 & 0.84 & 39.39 & 71.27 / 75.82 & 63.85 / 68.73 & 0.780 & 0.503 \\ Self-MM & 29.55 & 34.67 & 70.51 / 69.26 & 66.60 / 67.54 & 1.070 & 0.512 & 44.70 & 45.38 & 73.89 / 77.42 & 68.92 / 72.73 & 0.695 & 0.498 \\ MIMM & 31.30 & 33.77 & 69.14 / 70.06 & 66.65 / 65.41 & 0.177 & 0.507 & 40.75 & 41.74 & 73.75 / 75.89 & 68.72 / 72.32 & 0.739 & 0.489 \\ CENET & 30.38 & 37.25 & 71.46 / 67.73 & 68.41 / 64.85 & 1.080 & 0.504 & **47.18** & **47.33** & 74.57 / 73.04 & 70.68 / 74.08 & **0.685** & **0.535** \\ TETNS & 30.30 & 34.44 & 69.76 / 67.68 & 66.69 / 65.39 & 1.087 & 0.507 & 30.30 & 47.07 & 69.76 / 68.68 & 65.69 / 63.29 & 1.087 & 0.508 \\ TFR-Net & 29.54 & 34.67 & 68.15 / 66.35 & 61.73 / 60.06 & 1.200 & 0.459 & 46.83 & 34.67 & 73.62 / 77.23 & 68.80 / 71.99 & 0.697 & 0.489 \\ ALMT & 30.30 & 33.42 & 70.40 / 68.39 & 72.57 / **71.80** & 1.083 & 0.498 & 40.92 & 41.64 & **76.64** / 77.54 & 77.14 / 78.03 & 0.674 & 0.481 \\ \hline
**LNLN** & **34.26** & **38.27** & **72.55** / **70.94** & **72.73** / 71.25 & **1.046** & **0.527** & 45.42 & 46.17 & 76.30 / **78.19** & **77.77** / **79.95** & 0.692 & 0.530 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Robustness comparison of the overall performance on MOSI and MOSEI datasets. Note: The smaller MAE indicates the better performance.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Method & Acc-5 & Acc-3 & Acc-2 & F1 & MAE & Corr \\ \hline MISA & 31.53 & 56.87 & 72.71 & 66.30 & 0.539 & 0.348 \\ Self-MM & 32.28 & 56.75 & 72.81 & 68.43 & 0.508 & 0.376 \\ MMIM & 31.81 & 52.76 & 69.86 & 66.21 & 0.544 & 0.339 \\ CENET & 22.29 & 53.17 & 68.13 & 57.90 & 0.589 & 0.107 \\ TETFN & 33.42 & 56.91 & **73.58** & 68.67 & **0.505** & 0.387 \\ TFR-Net & 26.52 & 52.89 & 68.13 & 58.70 & 0.661 & 0.169 \\ ALMT & 20.00 & 45.36 & 69.66 & 72.76 & 0.561 & 0.364 \\ \hline
**LNLN** & **34.64** & **57.14** & 72.73 & **79.43** & 0.514 & **0.397** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Robustness comparison of the overall performance on SIMS dataset. Note: The smaller MAE indicates the better performance.

Figure 2: Performance curves of various missing rates. (a), (b) and (c) are the F1 curves on MOSI, MOSEI, and SIMS, respectively. (d), (e) and (f) are the MAE curves on MOSI, MOSEI, and SIMS, respectively. Note: The smaller MAE indicates the better performance.

[MISSING_PAGE_FAIL:9]

the inputs in high missing rate scenarios because most of them are designed specifically for complete data (see Appendix A.6 for more details). However, some methods show relatively more robust performance in low missing rate scenarios and some specific scenarios (see Appendix A.8), such as Self-MM, TETFN, and ALMT. We believe that some of the ideas in these methods may be useful for future research on more robust MSA for incomplete data.

For example, the Unimodal Label Generation Module (ULGM) (Yu et al., 2021) used in both Self-MM and TETFN facilitates the modeling of incomplete data. Due to the random data missing, the temporal and structured affective information in the data is corrupted, making it difficult for the models to perceive consistency and variability information between different modalities. Using ULGM to generate pseudo-labels as an additional supervisory signal for the models may directly enable the model to learn the relationship between different modalities. For ALMT, we believe that its Adaptive Hyper-modality Learning (AHL) module facilitates the modeling of robust affective representations. AHL mainly uses the dominant modality to compute query vectors and applies multi-head attention to query useful sentiment cues from other modalities as a complement to the dominant modality. This process may reduce the difficulty of multimodal fusion by avoiding the introduction of sentiment-irrelevant information to some extent. Our proposed LNLN, based on this idea from ALMT, proves the effectiveness of the method.

Additionally, we explore the performance of these methods in modality missing scenarios, which is a special case of random data missing where the partial modality missing rate \(r=1.0\). We find that when language modality is missing, the performance of many models decreases significantly and converges to the same value. More details can be seen in Appendix A.8.

## 5 Conclusion and Future Work

In this paper, a novel Language-dominated Noise-resistant Learning Network (LNLN) is proposed for robust MSA. Due to the collaboration between Dominate Modality Correction (DMC) module, the Dominant Modality based Multimodal Learning (DMML) module, and the Reconstructor, LNLN can enhance the dominant modality to achieve superior performance in data missing scenarios with varying levels. Extensive evaluation shows that none of the existing methods can effectively model the data with high missing rates. The related analyses can provide suggestions for other researchers to better handle robust MSA. In the future, we will focus on improving the generalization of the model to handle different types of scenes and varying intensities of noise effectively.

## Limitations

We believe there are several limitations to the LNLN. 1) The LNLN achieves good performance in data missing scenarios, but is not always better than all other methods in the modality missing scenarios, which demonstrates the lack of multi-scene generalization capability of the LNLN. 2) The data in real-world scenarios is much more complex. In addition to the presence of missing data, other factors need to be considered, such as diverse cultural contexts, varying user behavior patterns, and the influence of platform-specific features on the data. These factors can introduce additional noise and variability, which may require further model adaptation and tuning to handle effectively. 3) Tuning the hyperparameters, particularly those related to the loss functions, can be challenging and may require more sophisticated methods to achieve optimal performance.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{6}{c}{MOSI} \\ \cline{2-7}  & Acc-7 & Acc-5 & Acc-2 & F1 & MAE & Corr \\ \hline
**LNLN** & **34.26** & **38.27** & **72.55 / 70.94** & 72.73 / **71.25** & **1.046** & 0.527 \\ w/o language & 16.30 & 16.31 & 54.88 / 55.79 & 60.33 / 63.10 & 1.399 & 0.033 \\ w/o audio & 33.51 & 38.10 & 72.31 / 70.54 & 73.67 / 71.10 & 1.064 & 0.535 \\ w/o vision & 33.53 & 38.15 & 72.32 / 70.58 & **73.68 / 71.14** & 1.064 & **0.536** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Effects of different modalities. Note: The smaller MAE indicates the better performance.

## Acknowledgements

This work is supported by the Guangdong Provincial Key Laboratory of Mathematical Foundations for Artificial Intelligence (2023B1212010001).

## References

* Baltrusaitis et al. (2018) Baltrusaitis, T., Zadeh, A., Lim, Y.C., Morency, L., 2018. Openface 2.0: Facial behavior analysis toolkit, in: 2016 IEEE Winter Conference on Applications of Computer Vision, pp. 59-66.
* Ganin and Lempitsky (2015) Ganin, Y., Lempitsky, V.S., 2015. Unsupervised domain adaptation by backpropagation, in: International Conference on Machine Learning, pp. 1180-1189.
* Goodfellow et al. (2014) Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A.C., Bengio, Y., 2014. Generative adversarial nets, in: Advances in Neural Information Processing Systems, pp. 2672-2680.
* Guo et al. (2022) Guo, J., Tang, J., Dai, W., Ding, Y., Kong, W., 2022. Dynamically adjust word representations using unaligned multimodal information, in: Proceedings of the 30th ACM International Conference on Multimedia, pp. 3394-3402.
* Han et al. (2021) Han, W., Chen, H., Poria, S., 2021. Improving multimodal fusion with hierarchical mutual information maximization for multimodal sentiment analysis, in: Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 9180-9192.
* Hazarika et al. (2020) Hazarika, D., Zimmermann, R., Poria, S., 2020. MISA: modality-invariant and -specific representations for multimodal sentiment analysis, in: Proceedings of the 28th ACM International Conference on Multimedia, pp. 1122-1131.
* Jiang et al. (2020) Jiang, Y., Li, W., Hossain, M.S., Chen, M., Alelaiwi, A., Al-Hammadi, M., 2020. A snapshot research and implementation of multimodal information fusion for data-driven emotion recognition. Information Fusion 53, 209-221.
* Kenton and Toutanova (2019) Kenton, J.D.M.W.C., Toutanova, L.K., 2019. Bert: Pre-training of deep bidirectional transformers for language understanding, in: Proceedings of NAACL-HLT, pp. 4171-4186.
* Li et al. (2024) Li, M., Yang, D., Lei, Y., Wang, S., Wang, S., Su, L., Yang, K., Wang, Y., Sun, M., Zhang, L., 2024. A unified self-distillation framework for multimodal sentiment analysis with uncertain missing modalities, in: Proceedings of the AAAI Conference on Artificial Intelligence, pp. 10074-10082.
* Liang et al. (2020) Liang, J., Li, R., Jin, Q., 2020. Semi-supervised multi-modal emotion recognition with cross-modal distribution matching, in: Proceedings of the 28th ACM International Conference on Multimedia, pp. 2852-2861.
* Lv et al. (2021) Lv, F., Chen, X., Huang, Y., Duan, L., Lin, G., 2021. Progressive modality reinforcement for human multimodal emotion recognition from unaligned multimodal sequences, in: IEEE Conference on Computer Vision and Pattern Recognition, pp. 2554-2562.
* Mai et al. (2020) Mai, S., Xing, S., Hu, H., 2020. Locally confined modality fusion network with a global perspective for multimodal human affective computing. IEEE Transactions on Multimedia 22, 122-137.
* Mao et al. (2022) Mao, H., Yuan, Z., Xu, H., Yu, W., Liu, Y., Gao, K., 2022. M-SENA: an integrated platform for multimodal sentiment analysis, in: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 204-213.
* McFee et al. (2015) McFee, B., Raffel, C., Liang, D., Ellis, D.P.W., McVicar, M., Battenberg, E., Nieto, O., 2015. librosa: Audio and music signal analysis in python, in: Proceedings of the 14th Python in Science Conference, pp. 18-24.
* Mittal et al. (2020) Mittal, T., Bhattacharya, U., Chandra, R., Bera, A., Manocha, D., 2020. M3ER: multiplicative multimodal emotion recognition using facial, textual, and speech cues, in: Proceedings of the AAAI Conference on Artificial Intelligence, pp. 1359-1367.
* Mair et al. (2021)Rahman, W., Hasan, M.K., Lee, S., Zadeh, A.B., Mao, C., Morency, L., Hoque, M.E., 2020. Integrating multimodal information in large pretrained transformers, in: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 2359-2369.
* Tsai et al. (2019) Tsai, Y.H., Bai, S., Liang, P.P., Kolter, J.Z., Morency, L., Salakhutdinov, R., 2019. Multimodal transformer for unaligned multimodal language sequences, in: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 6558-6569.
* Yang et al. (2022) Yang, D., Huang, S., Kuang, H., Du, Y., Zhang, L., 2022. Disentangled representation learning for multimodal emotion recognition, in: Proceedings of the 30th ACM International Conference on Multimedia, pp. 1642-1651.
* Yu et al. (2020) Yu, W., Xu, H., Meng, F., Zhu, Y., Ma, Y., Wu, J., Zou, J., Yang, K., 2020. CH-SIMS: A chinese multimodal sentiment analysis dataset with fine-grained annotation of modality, in: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 3718-3727.
* Yu et al. (2021) Yu, W., Xu, H., Yuan, Z., Wu, J., 2021. Learning modality-specific representations with self-supervised multi-task learning for multimodal sentiment analysis, in: Proceedings of the AAAI conference on artificial intelligence, pp. 10790-10797.
* Yuan et al. (2021) Yuan, Z., Li, W., Xu, H., Yu, W., 2021. Transformer-based feature reconstruction network for robust multimodal sentiment analysis, in: Proceedings of the 29th ACM International Conference on Multimedia, pp. 4400-4407.
* Yuan et al. (2024) Yuan, Z., Liu, Y., Xu, H., Gao, K., 2024. Noise imitation based adversarial training for robust multimodal sentiment analysis. IEEE Transactions on Multimedia 26, 529-539.
* Zadeh et al. (2017) Zadeh, A., Chen, M., Poria, S., Cambria, E., Morency, L., 2017. Tensor fusion network for multimodal sentiment analysis, in: Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 1103-1114.
* Zadeh et al. (2018) Zadeh, A., Liang, P.P., Poria, S., Cambria, E., Morency, L., 2018. Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph, in: Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pp. 2236-2246.
* Zadeh et al. (2016) Zadeh, A., Zellers, R., Pincus, E., Morency, L., 2016. Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages. IEEE Intelligent Systems 31, 82-88.
* Zhang et al. (2023) Zhang, H., Wang, Y., Yin, G., Liu, K., Liu, Y., Yu, T., 2023. Learning language-guided adaptive hyper-modality representation for multimodal sentiment analysis, in: Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 756-767.

Additional Experiments and Analysis

### Effect of Regularization Weight on Model Performance

As shown in Table 7, we empirically tried different combinations of hyperparameters in the loss function. The results show that the selected parameters can make the LNLN achieve better performance in most metrics. This also demonstrates the effectiveness of the optimization objectives of LNLN.

### Analysis of Model Stability

To evaluate the stability of the model, we selected the MOSI dataset for ablation experiments. Specifically, based on the overall performance (mean) in Table 2, we selected several representative methods to additionally compute the standard deviation. For each method, we first calculate the standard deviation of evaluation results for the three random seeds when the missing rate \(r\) ranges from 0 to 0.9, and then average the 10 sets of standard deviation. The final result is shown in Table 8. We can see that the standard deviation of these methods is not very large in most metrics, indicating that all these methods can guarantee stability in the presence of random noise. It should be noted that LNLN strikes a balance between overall performance and stability.

### Analysis of Data Distribution

Figure 4 illustrates the data distribution on the MOSI, MOSEI, and SIMS datasets. Significant category imbalances can be seen across all datasets. Additionally, the distributions of the training set, validation set, and test set are not identical on the MOSI and MOSEI datasets. For example, as shown in Figure 4 (a), the percentage of weakly negative samples is 15.5% in the MOSI training set, while this category reaches 22.7% in the test set, representing the highest percentage among all categories.

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline  & & \multicolumn{4}{c}{MOSI} \\ \hline \(\alpha\), \(\beta\), \(\gamma\), \(\delta\) & Acc-7 & Acc-5 & Acc-2 & F1 & MAE & Corr \\ \hline

[MISSING_PAGE_POST]

 \hline \hline \end{tabular}
\end{table}
Table 7: Effect of regularization weight on model performance. Note: The smaller MAE indicate the better performance.

Similarly, Figure 4 (b) and Figure 4 (c) show that this phenomenon also exists in the MOSEI and SIMS datasets.

The unbalanced data distribution makes it difficult for the model to perform well in data missing scenarios. Specifically, it is easy for the model to engage in lazy behavior in high-noise scenarios, simply biasing predictions towards categories with a higher proportion in the training set. These unbalanced distributions may further increase the learning difficulty of the model when facing data missing. More details can be found in Appendix A.6.

### Case Study

As shown in Figure 3, we visualize several successful and failed predictions made by LNLN and ALMT from the MOSI dataset for the case study. It shows that LNLN can perceive sentiment cues in challenging samples, which demonstrates its ability to capture sentiment cues in noisy scenarios. However, for inputs with high missing rates (_e.g.,_ the third example in the figure), both LNLN and ALMT fail to make correct predictions. We believe this is due to the high loss of valid information in the multimodal input, making accurate predictions difficult.

### Details of Robust Comparison

Tables 9 and 10 show the details of robust comparison on the MOSI, MOSEI, and SIMS datasets, respectively. We observe that Self-MM and TETFN have a clear advantage in many evaluation metrics when the missing rate \(r\) is low. As the missing rate \(r\) increases, LNLN shows a significant improvement in all evaluation metrics. This demonstrates LNLN's ability to effectively model data affected by noise of varying intensity. In general, models augmented with noisy data in training usually cannot achieve state-of-the-art performance when the noise is low due to differences in data distribution. It is also worthwhile to study in the future how to balance the performance of models under different levels of noise.

In addition, when \(r\) is high, many models converge in their performance on metrics such as Acc-7 and Acc-5 (see Appendix A.6). This is due to the fact that these models exhibit lazy behavior, tending to predict inputs as categories with a higher number of samples in the training set. In such cases, we

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline Method & Acc-7 & Acc-5 & Acc-2 & F1 & MAE & Corr \\ \hline MISA & 29.85\(\pm\)2.62 & 33.08\(\pm\)1.77 & 71.49\(\pm\)1.00 / 70.33\(\pm\)0.93 & 71.28\(\pm\)0.92 / 70.00\(\pm\)1.26 & 1.085\(\pm\)0.08 & 0.524\(\pm\)0.08 \\ Self-MM & 29.55\(\pm\)1.06 & 34.67\(\pm\)1.87 & 70.51\(\pm\)0.71 / 69.26\(\pm\)1.07 & 66.60\(\pm\)1.92 / 67.54\(\pm\)2.34 & 1.070\(\pm\)0.13 & 0.512\(\pm\)0.07 \\ CENET & 30.38\(\pm\)1.27 & 37.25\(\pm\)1.53 & 71.46\(\pm\)0.60 / 67.73\(\pm\)0.71 & 68.41\(\pm\)1.12 / 64.85\(\pm\)2.56 & 1.080\(\pm\)0.14 & 0.504\(\pm\)0.11 \\ TFR-Net & 29.54\(\pm\)1.00 & 34.67\(\pm\)1.75 & 68.15\(\pm\)1.25 / 66.35\(\pm\)1.09 & 61.73\(\pm\)2.82 / 60.06\(\pm\)2.33 & 1.200\(\pm\)0.11 & 0.459\(\pm\)0.25 \\ \hline
**LNLN** & **34.26\(\pm\)**1.17 & **38.27\(\pm\)**1.23 & **72.55\(\pm\)**1.11 / **70.94\(\pm\)**1.28 & **72.73\(\pm\)**0.99 / **71.25\(\pm\)**1.06 & **1.046\(\pm\)**0.21 & **0.527\(\pm\)**0.17 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Comparison of model stability on MOSI dataset. Note: -\(\pm\)- means mean\(\pm\)std. The smaller MAE indicates better performance.

Figure 3: Visualization of successful and failed predictions. Note: The input is visualized to facilitate readers’ understanding. In practice, random data missing is applied to the original input sequence, as described in Section 3.

believe that the models do not actually learn effective knowledge, even if their performance appears good.

### Visualization of Confusion Matrix

To verify the effectiveness of the method, we visualized the confusion matrix of several representative methods on the MOSI, MOSEI, and SIMS datasets in Figure 5, Figure 6, and Figure 7, respectively. Obviously, as the missing rate \(r\) increases, the predictions of all methods on all datasets tend to favor certain categories. This phenomenon indicates that the models hardly learn useful knowledge for prediction but instead ensure high accuracy by being lazy. For example, on the MOSI dataset,

Figure 4: Data distribution of MOSI, MOSEI, and SIMS datasets.

[MISSING_PAGE_EMPTY:16]

Self-MM predicts all samples as weak negative when the missing rate is 0.9. Although the accuracy of Self-MM is high when the missing rate \(r\) is high, it does not demonstrate that the model is more robust.

In addition, TETFN and TFR-Net tend to randomly predict within the negative and weak negative categories under high noise, indicating that their lazy behavior is not as severe as methods like Self-MM, MMIM, and MISA. Unlike other methods, although LNLN also suffers from the impact of data missing, the model's lazy behavior is less severe. For example, at a missing rate of 0.9, the model is still attempting to predict the samples, and there is no case of predictions favoring a particular category to an extreme extent.

From Figure 6 and Figure 7, we can see that a similar phenomenon also occurs in the MOSEI and SIMS datasets. These observations demonstrate that methods targeting complete data often fail when modeling highly incomplete data. Therefore, the design of methods specifically for modeling incomplete data is necessary.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c c} \hline \hline Method & Acc-5 & Acc-3 & Acc-2 & F1 & MAE & Corr & Method & Acc-5 & Acc-3 & Acc-2 & F1 & MAE & Corr \\ \hline \multicolumn{11}{c}{Random Missing Rate \(r=0\)} \\ \hline MISA & 40.55 & 53.38 & 78.19 & 77.22 & 0.449 & 0.576 & MISA & 30.56 & 54.78 & 71.26 & 64.16 & 0.552 & 0.367 \\ Self-MM & 40.77 & 64.92 & 78.26 & 78.00 & **0.421** & 0.584 & Self-MM & 32.02 & 53.90 & 71.41 & 67.11 & 0.517 & 0.390 \\ MMIM & 37.42 & 60.69 & 75.42 & 73.10 & 0.475 & 0.528 & MOMIM & 33.41 & 52.37 & 68.49 & 64.81 & 0.553 & 0.336 \\ CENET & 23.85 & 54.05 & 68.71 & 57.82 & 0.578 & 0.137 & CENET & 23.12 & 54.05 & 68.71 & 57.92 & 0.588 & 0.107 \\ TETFN & **41.94** & **5.86** & **80.23** & 79.25 & 0.424 & **0.589** & TETFN & 33.48 & 56.24 & 72.43 & 67.30 & **0.512** & 0.394 \\ TFR-Net & 33.85 & 54.12 & 69.15 & 58.44 & 0.562 & 0.254 & TFR-Net & 24.65 & 52.37 & 67.47 & 58.66 & 0.685 & 0.171 \\ ALMT & 23.41 & 54.78 & 75.64 & 76.27 & 0.527 & 0.536 & ALMT & 18.38 & 47.12 & 68.27 & 71.22 & 0.563 & 0.395 \\ LNLN & 38.66 & 63.97 & 75.93 & **79.89** & 0.458 & 0.570 & LNLN & **36.40** & **57.70** & **72.72** & **78.77** & 0.513 & **0.412** \\ \hline \multicolumn{11}{c}{Random Missing Rate \(r=0.1\)} \\ \hline MISA & 38.88 & 63.02 & 77.39 & 75.82 & 0.461 & 0.561 & MISA & 27.72 & 53.97 & 70.46 & 61.81 & 0.578 & 0.286 \\ Self-MM & 40.26 & 63.53 & 77.32 & 76.76 & 0.433 & 0.563 & Self-MM & 29.10 & 51.86 & 70.02 & 64.21 & 0.548 & 0.313 \\ MMIM & 37.27 & 60.90 & 74.25 & 72.08 & 0.473 & 0.529 & MOMIM & 29.18 & 49.31 & 67.91 & 63.86 & 0.578 & 0.270 \\ CENET & 22.83 & 53.98 & 68.57 & 57.36 & 0.580 & 0.136 & CENET & 22.46 & 53.69 & 69.00 & 58.64 & 0.592 & 0.102 \\ TETFN & **41.36** & **64.62** & **78.92** & 77.70 & **0.432** & **0.578** & TETFN & 29.90 & 53.54 & 70.97 & 64.19 & 0.545 & 0.309 \\ TFR-Net & 30.12 & 53.25 & 68.85 & 59.38 & 0.596 & 0.203 & TFR-Net & 24.80 & 52.59 & 67.03 & 58.30 & 0.696 & 0.157 \\ ALMT & 22.10 & 55.54 & 74.40 & 75.19 & 0.530 & 0.537 & ALMT & 18.67 & 43.69 & 66.81 & 70.69 & 0.574 & 0.322 \\ LNLN & 38.51 & 62.73 & 76.29 & **80.07** & 0.458 & 0.562 & LNLN & **33.70** & **54.63** & **71.55** & **79.56** & **0.535** & **0.352** \\ \hline \multicolumn{11}{c}{Random Missing Rate \(r=0.2\)} \\ \hline MISA & 38.15 & 59.23 & 74.33 & 71.70 & 0.489 & 0.490 & MISA & 24.87 & 52.52 & **69.95** & 59.54 & 0.601 & 0.167 \\ Self-MM & 38.37 & 61.71 & 74.98 & 73.71 & 0.464 & 0.500 & Self-MM & 25.53 & 50.62 & 69.58 & 62.28 & 0.571 & 0.198 \\ MIMM & 37.27 & 57.33 & 72.36 & 69.80 & 0.504 & 0.460 & MOMIM & 28.59 & 46.53 & 66.89 & 62.23 & 0.595 & 0.190 \\ CENET & 22.25 & 54.20 & 68.57 & 57.64 & 0.583 & 0.132 & CENET & 21.81 & **53.32** & 67.69 & 57.87 & 0.599 & 0.070 \\ TETFN & **39.46** & 61.56 & **75.49** & 73.59 & **0.457** & **0.527** & TETFN & 27.86 & 51.06 & 69.29 & 61.09 & 0.572 & 0.190 \\ TFR-Net & 29.03 & 53.61 & 68.64 & 59.74 & 0.619 & 0.191 & TFR-Net & 23.78 & 52.30 & 67.18 & 58.15 & 0.707 & 0.163 \\ ALMT & 21.08 & 53.17 & 72.65 & 73.90 & 0.541 & 0.485 & ALMT & 18.02 & 38.66 & 65.57 & 70.27 & 0.586 & 0.218 \\ LNLN & 38.88 & **61.78** & 74.76 & **78.53** & 0.474 & 0.513 & LNLN & **30.27** & 51.64 & 69.73 & **79.37** & **0.558** & **0.261** \\ \hline \multicolumn{11}{c}{Random Missing Rate \(r=0.3\)} \\ \hline MISA & 38.14 & 59.23 & 74.33 & 71.70 & 0.489 & 0.490 & MISA & 24.87 & 52.52 & **69.95** & 59.54 & 0.601 & 0.167 \\ Self-MM & 38.37 & 61.71 & 74.98 & 73.71 & 0.464 & 0.500 & Self-MM & 25.53 & 50.62 & 69.58 & 62.28 & 0.571 & 0.198 \\ MIMM & 37.27 & 57.33 & 72.36 & 69.80 & 0.504 & 0.460 & MOMIM & 28.59 & 46.63 & 68.69 & 62.23 & 0.595 & 0.190 \\ CENET & 22.25 & 54.20 & 68.57 & 57.64 & 0.583 & 0.132 & CENET & 21.81 & **53.32** & 67.69 & 57.87 & 0.599 & 0.070 \\ TETFN & **39.46** & 61.56 & **75.49** & 73.59 & **0.457** & **0.527** & TETFN & 27.86 & 51.06 & 69.29 & 61.09 & 0.

Figure 5: Seven-category confusion matrix of several representative methods on MOSI dataset. Note: 0-6 denote strongly negative, weakly negative, negative, neutral, weakly positive, positive, and strongly positive, respectively.

Figure 6: Seven-category confusion matrix of several representative methods on MOSEI dataset. Note: 0-6 denote strongly negative, weakly negative, negative, neutral, weakly positive, positive, and strongly positive, respectively.

Figure 7: Five-category confusion matrix of several representative methods on SIMS dataset. Note: 0-4 denote weakly negative, negative, neutral, weakly positive, and positive, respectively.

### Visualization of Representations

Figure 8 shows the language feature \(H^{1}_{d}\) extracted without applying data missing and the corrected dominant feature \(H^{1}_{d}\) with different data missing rates. we can see that the model tends to learn \(H^{1}_{d}\), which is consistent with the distribution of the \(H^{1}_{l}\), indicating that LNLN can complement the information of the language modality when data is missing. Additionally, when the missing rate \(r\) is 1.0 (no valid information for the input), it becomes difficult for the model to learn \(H^{1}_{d}\) that is consistent with the language feature. This also suggests that the generator is indeed trying to learn \(H^{1}_{d}\) for fusion in DMML, rather than just guessing.

### Analysis of the generalization to modality missing scenarios

To explore the generalization of the method, we further evaluated it in the special case of data missing, _i.e.,_ modality missing scenarios. The overall performance of the model is shown in Table 11 and Table 12. The detailed performance of the model is shown in Table 13 and Table 14. It is evident that LNLN, ALMT, TETFN, and Self-MM all exhibit excellent generalization.

We believe that the Dominant Modality Correction (DMC) proposed in LNLN, the Adaptive Hyper-modality Learning (AHL) proposed in ALMT, and the Unimodal Label Generation Module (ULGM) proposed/used in Self-MM and TETFN deserve more research attention in the future. These modules and their corresponding ideas have significant reference value for achieving robust Multimodal Sentiment Analysis (MSA).

However, we also found that many methods converge to the same value when the noise is high. This is similar to the phenomenon observed in previous data missing scenarios, where many models exhibited lazy behavior. Therefore, handling modality missing and improving model robustness in different noise scenarios remain critical challenges that need to be addressed in the field of MSA.

Figure 8: Visualization of the corrected dominant feature \(H^{1}_{d}\) with different data missing rate and the language feature \(H^{1}_{l}\) without data missing.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{6}{c}{MOSI} & \multicolumn{6}{c}{MOSEI} \\ \cline{2-13}  & Acc-7 & Acc-5 & Acc-2 & F1 & MAE & Corr & Acc-7 & Acc-5 & Acc-2 & F1 & MAE & Corr \\ \hline MISA & 29.44 & 31.76 & 67.64 / 67.27 & 64.81 / 64.30 & 1.096 & **0.461** & 41.45 & 40.32 & 73.94 / 77.49 & 66.65 / 71.30 & 0.752 & 0.438 \\ Seil-MM & 29.74 & **36.40** & **71.45** / **68.17** & 63.71 / 61.91 & **1.052** & 0.450 & 45.17 & 46.05 & 74.12 / **77.85** & 67.07 / 71.81 & **0.684** & 0.447 \\ MMIM & 31.28 & 33.74 & 68.34 / 65.12 & 63.38 / 58.28 & 1.058 & 0.444 & 41.36 & 42.53 & 73.19 / 67.14 & 68.11 / 70.18 & 0.732 & 0.448 \\ CENT & 30.30 & 33.94 & 70.37 / 66.67 & 62.66 / 58.61 & 1.068 & 0.450 & 47.41 & 48.19 & **73.45** / 74.14 & 68.43 / 70.37 & **0.684** & **0.480** \\ TETFN & **32.66** & 36.26 & 70.71 / 68.15 & 62.47 / 60.18 & 1.061 & 0.446 & **47.52** & **48.41** & 73.87 / 77.43 & 66.70 / 71.45 & 0.691 & 0.426 \\ TFR-Net & 29.76 & 34.96 & 70.49 / 67.95 & 62.87 / 60.08 & 1.177 & 0.451 & 47.33 & 48.08 & 73.50 / 77.45 & 68.98 / 71.28 & 0.700 & 0.427 \\ ALMT & 27.87 & 30.73 & 70.53 / 67.47 & **75.89 / 73.35** & 1.130 & 0.447 & 25.94 & 27.46 & 60.66 / 63.37 & 70.63 / 71.24 & 0.711 & 0.354 \\ \hline LNLN & 31.93 & 34.86 & 68.53 / 65.64 & 72.01 / 69.65 & 1.093 & 0.423 & 43.85 & 44.50 & 73.52 / 77.04 & **80.85** / **83.19** & 0.733 & 0.425 \\ \hline \hline \end{tabular}
\end{table}
Table 11: Generalization comparison of the overall performance on MOSI and MOSEI with random modality missing. Note: the parameters used for evaluation are consistent with those used for testing in random data missing. The smaller MAE indicates better performance.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline Method & Acc-5 & Acc-3 & Acc-2 & F1 & MAE & Corr \\ \hline MISA & 29.62 & 56.82 & 73.61 & 66.32 & 0.535 & **0.330** \\ Self-MM & 30.36 & **59.56** & 75.01 & 73.13 & **0.508** & 0.321 \\ MMIM & 27.21 & 50.79 & 71.52 & 65.48 & 0.546 & 0.286 \\ CENET & 19.95 & 43.63 & 64.08 & 56.96 & 0.601 & 0.033 \\ TETFN & **30.61** & 50.98 & **74.77** & 68.00 & 0.514 & 0.317 \\ TFR-Net & 24.69 & 52.57 & 69.31 & 58.03 & 0.629 & 0.133 \\ ALMT & 21.82 & 39.39 & 72.30 & 78.86 & 0.563 & 0.287 \\ \hline LNLN & 30.39 & 54.32 & 72.67 & **80.87** & 0.527 & 0.287 \\ \hline \hline \end{tabular}
\end{table}
Table 12: Generalization comparison of the overall performance on SIMS dataset with random modality missing. Note: the parameters used for evaluation are consistent with those used for testing in random data missing. The smaller MAE indicate the better performance.

[MISSING_PAGE_EMPTY:23]

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline Method & Acc-5 & Acc-3 & Acc-2 & F1 & MAE & Corr \\ \hline \multicolumn{6}{c}{Language Modality Missing} \\ \hline MISA & 18.60 & 50.77 & 69.23 & 56.75 & 0.595 & **0.104** \\ Self-MM & 19.77 & 54.20 & 78.26 & 78.00 & **0.594** & 0.058 \\ MMIM & 17.22 & 41.28 & 67.76 & 57.76 & 0.617 & 0.055 \\ CENET & 20.71 & 49.24 & 68.20 & 57.67 & 0.601 & 0.037 \\ TETFN & 19.26 & 36.11 & **69.37** & 56.82 & 0.603 & 0.045 \\ TFR-Net & **22.25** & **54.27** & 69.30 & 56.78 & 0.712 & 0.008 \\ ALMT & 21.15 & 24.95 & **69.37** & **81.91** & 0.595 & 0.049 \\ LNLN & 21.37 & 45.30 & **69.37** & **81.91** & 0.597 & 0.005 \\ \hline \multicolumn{6}{c}{Audio Modality Missing} \\ \hline MISA & 38.37 & 61.92 & 77.68 & 75.17 & 0.466 & 0.574 \\ Self-MM & 40.92 & 64.99 & 77.32 & 76.76 & **0.421** & 0.585 \\ MMIM & 37.27 & 60.61 & 75.20 & 72.96 & 0.475 & 0.527 \\ CENET & 21.59 & 53.90 & 68.93 & 57.45 & 0.582 & 0.141 \\ TETFN & **41.94** & **65.86** & **80.16** & 79.16 & 0.424 & **0.589** \\ TFR-Net & 26.40 & 52.45 & 69.66 & 59.21 & 0.569 & 0.239 \\ ALMT & 23.05 & 59.45 & 75.27 & 76.36 & 0.531 & 0.529 \\ LNLN & 39.53 & 64.19 & 75.78 & **79.69** & 0.454 & 0.570 \\ \hline \multicolumn{6}{c}{Víusal Modality Missing} \\ \hline MISA & 40.84 & 63.67 & 78.19 & 77.07 & 0.458 & 0.574 \\ Self-MM & 41.21 & 64.99 & 74.98 & 73.71 & **0.421** & 0.584 \\ MMIM & 37.34 & 60.47 & 75.57 & 73.43 & 0.475 & 0.528 \\ CENET & 19.70 & 38.51 & 59.08 & 56.38 & 0.600 & 0.027 \\ TETFN & **41.94** & **65.86** & **80.16** & 79.19 & 0.424 & **0.589** \\ TFR-Net & 30.20 & 54.56 & 69.15 & 58.86 & 0.566 & 0.247 \\ ALMT & 22.83 & 55.51 & 75.35 & 75.75 & 0.529 & 0.535 \\ LNLN & 38.80 & 63.31 & 76.22 & **80.11** & 0.458 & 0.569 \\ \hline \hline \end{tabular} 
\begin{tabular}{l c c c c c c} \hline \hline Method & Acc-5 & Acc-3 & Acc-2 & F1 & MAE & Corr \\ \hline \multicolumn{6}{c}{Language \& Audio Modalities Missing} \\ \hline MISA & 21.23 & **54.27** & 69.37 & 56.82 & 0.613 & **0.064** \\ Self-MM & 19.69 & 54.20 & **74.76** & 72.85 & **0.594** & 0.059 \\ MMIM & 17.22 & 41.28 & 67.91 & 57.86 & 0.617 & 0.026 \\ CENET & 20.42 & 49.60 & 66.31 & 57.49 & 0.601 & 0.002 \\ TETFN & 19.26 & 36.11 & 69.37 & 56.82 & 0.603 & 0.040 \\ TFR-Net & 21.96 & 46.98 & 69.44 & 57.12 & 0.635 & 0.041 \\ ALMT & 21.30 & 16.05 & 69.37 & **81.91** & **0.594** & 0.039 \\ LNLN & **21.37** & 43.25 & 69.37 & **81.91** & 0.598 & -0.008 \\ \hline \multicolumn{6}{c}{Language \& Visual Modalities Missing} \\ \hline MISA & 19.70 & 48.58 & 69.37 & 56.95 & 0.600 & **0.091** \\ Self-MM & 19.48 & **53.98** & **73.30** & 70.36 & **0.594** & 0.056 \\ MMIM & 17.22 & 40.99 & 67.40 & 57.70 & 0.617 & 0.055 \\ CENET & 18.45 & 33.55 & 65.50 & 59.55 & 0.612 & -0.054 \\ TETFN & 19.33 & 36.11 & 69.37 & 56.82 & 0.603 & 0.045 \\ TFR-Net & 19.55 & 54.27 & 69.37 & 56.82 & 0.720 & 0.027 \\ ALMT & 19.99 & 20.57 & 69.15 & 81.08 & 0.596 & 0.043 \\ LNLN & **21.44** & 46.39 & 69.37 & **81.91** & 0.600 & 0.014 \\ \hline \multicolumn{6}{c}{Audio \& Visual Modalities Missing} \\ \hline MISA & 38.95 & 61.70 & 77.83 & 75.15 & 0.478 & 0.573 \\ Self-MM & 41.06 & 64.99 & 71.41 & 67.11 & **0.421** & 0.585 \\ MMIM & 36.98 & 60.10 & 75.27 & 73.18 & 0.476 & 0.527 \\ CENET & 18.82 & 36.98 & 56.46 & 53.23 & 0.608 & 0.045 \\ TETFN & **41.94** & **65.86** & **80.16** & 79.19 & 0.424 & **0.589** \\ TFR-Net & 27.79 & 52.88 & 68.93 & 59.40 & 0.571 & 0.234 \\ ALMT & 22.61 & 59.81 & 75.27 & 76.17 & 0.532 & 0.529 \\ INLN & 39.82 & 63.46 & 75.93 & **79.72** & 0.454 & 0.569 \\ \hline \hline \end{tabular}
\end{table}
Table 14: Generalization comparison on SIMS with random modality missing. Note: The smaller MAE indicates the better performance.

Social Impacts

Our proposed LNLN has a wide range of applications in real-world scenarios, such as healthcare and human-computer interaction. However, it might also be misused to monitor individuals based on their affections for illegal purposes, potentially posing negative social impacts.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims made in the abstract and introduction accurately reflect the paper's contributions and scope. They clearly outline the capabilities of the proposed LNLN and its performance in handling data missing scenarios, as well as emphasizing the comprehensive evaluation of existing methods and their significance in improving robust Multimodal Sentiment Analysis. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have discussed the limitations of the work. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (_e.g.,_ independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: The article does not include theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The paper reports the network structure required for reproduction as well as the hyperparameter settings. In addition, as mentioned in the abstract, the code is available on GitHub. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The datasets used in the thesis are open datasets and can be accessed by anyone upon request. As mentioned in the abstract, the code is available on GitHub. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Optimizer selection, data partitioning, and other details are described. Hyper-parameters are chosen empirically. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: As shown in 8, we report the std/fluctuation of several representative methods on the most dataset. It shows the overall stability of the different methods in the face of noise impacts of different levels. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We report this information in Section Implementation Details. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: This study meets the requirements of moral and ethical norms. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We describe this in the appendix. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The used data has been cited in the paper. As mentioned in the abstract, the code is available on GitHub.. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [No] Justification: The dataset used is a public dataset, and the code is available on GitHub.. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve above problems. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This study does not involve the above issues and the datasets used are all publicly available datasets. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.