# Geometric Neural Diffusion Processes

 Emile Mathieu

University Of Cambridge

&Vincent Dutordoir\({}^{*}\)

University Of Cambridge

Secondmind Labs

&Michael J. Hutchinson

University Of Oxford

&Valentin De Bortoli

Center for Science of Data, ENS Ulm

&Yee Whye Teh

University Of Oxford

&Richard E. Turner

University Of Cambridge,

Microsoft Research

###### Abstract

Denoising diffusion models have proven to be a flexible and effective paradigm for generative modelling. Their recent extension to infinite dimensional Euclidean spaces has allowed for the modelling of stochastic processes. However, many problems in the natural sciences incorporate symmetries and involve data living in non-Euclidean spaces. In this work, we extend the framework of diffusion models to incorporate a series of geometric priors in infinite-dimension modelling. We do so by a) constructing a noising process which admits, as limiting distribution, a geometric Gaussian process that transforms under the symmetry group of interest, and b) approximating the score with a neural network that is equivariant w.r.t, this group. We show that with these conditions, the generative functional model admits the same symmetry. We demonstrate scalability and capacity of the model, using a novel Langevin-based conditional sampler, to fit complex scalar and vector fields, with Euclidean and spherical codomain, on synthetic and real-world weather data.

## 1 Introduction

Traditional denoising diffusion models are defined on finite-dimension Euclidean spaces (Song and Ermon, 2019; Song et al., 2021; Ho et al., 2020; Dhariwal and Nichol, 2021). Extensions have recently been developed for more exotic distributions, such as those supported on Riemannian manifolds (De Bortoli et al., 2022; Huang et al., 2022; Lou and Ermon, 2023; Fishman et al., 2023), and on function spaces of the form \(f:\mathbb{R}^{n}\to\mathbb{R}^{d}\)(Dutordoir et al., 2022; Kerrigan et al., 2022; Lim et al., 2023; Pidstrigach et al., 2023; Franzese et al., 2023; Bond-Taylor and Willcocks, 2023) (i.e. stochastic processes). In this work, we extend diffusion models to further deal with distributions over functions that incorporate non-Euclidean geometry in two different ways. This investigation of geometry also naturally leads to the consideration of symmetries in these distributions, and as such we also present methods for incorporating these into diffusion models.

Firstly, we look at _tensor fields_. Tensor fields are geometric objects that assign to all points on some manifold a value that lives in some vector space \(V\). Roughly speaking, these are functions of the form \(f:\mathcal{M}\to V\). These objects are central to the study of physics as they form a generic mathematical framework for modelling natural phenomena. Common examples include the pressure of a fluid in motion as \(f:\mathbb{R}^{3}\to\mathbb{R}\), representing wind over the Earth's surface as \(f:\mathcal{S}^{2}\to\mathrm{T}\mathcal{S}^{2}\), where \(\mathrm{T}\mathcal{S}^{2}\) is the _tangent-space_ of the sphere, or modelling the stress in a deformed object as \(f:\text{Object}\to\mathrm{T}\mathbb{R}^{3}\otimes\mathrm{T}\mathbb{R}^{3}\), where \(\otimes\) is the _tensor-product_ of the tangent spaces. Given the inherent symmetry in the laws of nature, these tensor fields can transform in a way that preserves these symmetries. Any modelling of these laws may benefit from respecting these symmetries.

Secondly, we look at fields with manifold codomain, and in particular, at functions of the form \(f:\mathbb{R}\to\mathcal{M}\). The challenge in dealing with manifold-valued output, arises from the lack of vector-space structure. In applications, these functions typically appear when modelling processes indexed by time that take values on a manifold. Examples include tracking the eye of cyclones moving on the surface of the Earth, or modelling the joint angles of a robot as it performs tasks.

The lack of data or noisy measurements in the physical process of interest motivates a _probabilistic_ treatment of such phenomena, in addition to its functional nature. Arguably the most important framework for modelling stochastic processes are Gaussian Processes (GPs) (Rasmussen, 2003), as they allow for exact or approximate posterior prediction (Titsias, 2009; Rahimi and Recht, 2007; Wilson et al., 2020). In particular, when choosing equivariant mean and kernel functions, GPs are invariant (i.e. stationary) (Holderrieth et al., 2021; Azanuglov et al., 2022; Azanuglov et al., 2023). Their limited modelling capacity and the difficulty in designing complex, problem-specific kernels motivates the development of neural processes (NPs) (Garnelo et al., 2018), which learn to approximately model a conditional stochastic process directly from data. NPs have been extended to model translation invariant (scalar) processes (Gordon et al., 2020) and more generic \(\mathrm{E}(n)\)-invariant processes (Holderrieth et al., 2021). Yet, the Gaussian conditional assumption of standard NPs still limits their flexibility and prevents such models from fitting complex processes. Diffusion models provide a compelling alternative for significantly greater modelling flexibility. In this work, we develop geometric diffusion neural processes which incorporate geometrical prior knowledge into functional diffusion models.

Our contributions are three-fold: (a) We extend diffusion models to more generic function spaces (i.e. tensor fields, and functions \(f:\mathcal{X}\to\mathcal{M}\)) by defining a suitable noising process. (b) We incorporate group invariance of the distribution of the generative model by enforcing the covariance kernel and the score network to be group equivariant. (c) We propose a novel Langevin dynamics scheme for efficient conditional sampling.

## 2 Background

**Denoising diffusion models.** We briefly recall here the key concepts behind diffusion models on \(\mathbb{R}^{d}\) and refer the readers to (Song et al., 2021) for a more detailed introduction. We consider a forward _noising_ process \((\mathbf{Y}_{t})_{t\geq 0}\) defined by the following Stochastic Differential Equation (SDE)

\[\mathrm{d}\mathbf{Y}_{t}=-\tfrac{1}{2}\mathbf{Y}_{t}\mathrm{d}t+\mathrm{d} \mathbf{B}_{t},\quad\mathbf{Y}_{0}\sim p_{0},\] (1)

where \((\mathbf{B}_{t})_{t\geq 0}\) is a \(d\)-dimensional Brownian motion and \(p_{0}\) is the data distribution. The process \((\mathbf{Y}_{t})_{t\geq 0}\) is simply an Ornstein-Ulhenbeck (OU) process which converges with geometric rate to \(\mathrm{N}(0,\mathrm{Id})\)(Durmus and Moulines, 2017). Under mild conditions on \(p_{0}\), the time-reversed process \((\bar{\mathbf{Y}}_{t})_{t\geq 0}=(\mathbf{Y}_{T-t})_{t\in[0,T]}\) also satisfies an SDE (Cattiaux et al., 2021) given by

\[\mathrm{d}\bar{\mathbf{Y}}_{t}=\{\tfrac{1}{2}\bar{\mathbf{Y}}_{t}+\nabla\log p _{T-t}(\bar{\mathbf{Y}}_{t})\}\mathrm{d}t+\mathrm{d}\mathbf{B}_{t},\quad\bar {\mathbf{Y}}_{0}\sim p_{T},\] (2)

where \(p_{t}\) denotes the density of \(\mathbf{Y}_{t}\). Unfortunately we cannot sample exactly from (2) as \(p_{T}\) and the scores \((\nabla\log p_{t})_{t\in[0,T]}\) are unavailable. First, \(p_{T}\) is substituted with the limiting distribution \(\mathrm{N}(0,\mathrm{Id})\) as it converges towards it. Second, one can easily show (Kallenberg, 2021, Thm 5.1) that the score \(\nabla\log p_{t}\) is the minimiser of \(\ell_{t}(\mathbf{s})=\mathbb{E}[\|\mathbf{s}(\mathbf{Y}_{t})-\nabla_{y_{t}} \log p_{t|0}(\mathbf{Y}_{t}|\mathbf{Y}_{0})\|^{2}]\) over functions \(\mathbf{s}:[0,T]\times\mathbb{R}^{d}\to\mathbb{R}^{d}\) where the expectation is over the joint distribution of \(\mathbf{Y}_{0},\mathbf{Y}_{t}\), and as such can readily be approximated by a neural network \(\mathbf{s}_{\theta}(t,y_{t})\) by minimising this functional. Finally, a discretisation of (2) is performed (e.g. Euler-Maruyama) to obtain approximate samples of \(p_{0}\).

**Steerable fields.** We now define steerable feature fields which are collections of tensor fields. We focus on the Euclidean group \(G=\mathrm{E}(d)\), which elements \(g\) admits a unique decomposition \(g=uh\) where \(h\in\mathrm{O}(d)\) is a \(d\times d\) orthogonal matrix and \(u\in\mathrm{T}(d)\) is a translation which can be identified as an element of \(\mathbb{R}^{d}\); for a vector \(x\in\mathbb{R}^{d}\), \(g\cdot x=hx+u\) denotes the action of \(g\) on \(x\), with \(h\) acting from the left on \(x\) by matrix multiplication. This special case simplifies the presentation, but can be extended to the general case is discussed in App. C.1.

Figure 1: Illustration of a vector field \(f:\mathbb{R}^{2}\to\mathbb{R}^{2}\) with representation \(\rho(h)=h\) being steered by a group element \(h=90^{\circ}\in\mathrm{O}(2)\subset\mathrm{E}(2)\).

We are interested in learning a probabilistic model over functions of the form \(f:\mathcal{X}\rightarrow\mathbb{R}^{d}\) such that a group \(G\) acts on \(\mathcal{X}\) and \(\mathbb{R}^{d}\). We call a feature field a tuple \((f,\rho)\) with \(f:\mathcal{X}\rightarrow\mathbb{R}^{d}\) a mapping between input \(x\in\mathcal{X}\) to some feature \(f(x)\) with associated representation \(\rho:G\rightarrow\mathrm{GL}(\mathbb{R}^{d})\)(Scott and Serre, 1996). This feature field is said to be \(G\)-steerable if it is transformed for all \(x\in\mathcal{X},g\in G\) as \(g\cdot f(x)=\rho(g)f(g^{-1}\cdot x)\). In this setting, the action of \(\mathrm{E}(d)=\mathrm{T}(d)\rtimes\mathrm{O}(d)\) on the feature field \(f\) yields \(g\cdot f(x)=(uh)\cdot f(x)\triangleq\rho(h)f\left(h^{-1}(x-u)\right)\). Typical examples of feature fields include scalar fields with \(\rho_{\mathrm{triv}}(g)\triangleq 1\) transforming as \(g\cdot f(x)=f\left(g^{-1}x\right)\) such as temperature fields, and vectors or potential fields with \(\rho_{\mathrm{Id}}(g)\triangleq h\) transforming as \(g\cdot f(x)=hf\left(g^{-1}x\right)\) as illustrated in Fig. 1, such as wind or force fields.

For many natural phenomena, a priori we do not want to express a preference for a particular conformation of the feature field and thus want a prior \(p\) to place the same density on all the transformed fields \(\mu(g\cdot f)=\mu(f),\ \forall g\in G\). Leveraging this symmetry can drastically reduce the amount of data required to learn from and reduce training time.

## 3 Geometric neural diffusion processes: GeomNDPs

### Continuous diffusion on function spaces

We construct a diffusion model on functions \(f:\mathcal{X}\rightarrow\mathcal{Y}\), with \(\mathcal{Y}=\mathbb{R}^{d}\), by defining a diffusion model for every finite set of marginals. Most prior works on infinite-dimensional diffusions consider a noising process on the space of functions (Kerrigan et al., 2022; Pidstrigach et al., 2023; Lim et al., 2023b). In theory, this allows the model to define a consistent distribution over all the finite marginals of the process being modelled. In practice, however, only finite marginals can be modelled on a computer and the score function needs to be approximated, and at this step lose consistency over the marginals. The only work to stay fully consistent in implementation is Phillips et al. (2022), at the cost of limiting functions that can be modelled to a finite-dimensional subspace. With this in mind, we eschew the technically laborious process of defining diffusions over the infinite-dimension space and work solely on the finite marginals following Dutordoir et al. (2022). We find that in practice consistency can be well learned from data see Sec. 5, and this allows for more flexible choices of score network architecture and easier training.

**Noising process.** We assume we are given a data process \((\mathbf{Y}_{0}(x))_{x\in\mathcal{X}}\). Given any \(x=(x^{1},\ldots,x^{n})\in\mathcal{X}^{n}\), we consider the following forward _noising_ process \((\mathbf{Y}_{t}(x))_{t\geq 0}\triangleq(\mathbf{Y}_{t}(x^{1}),\ldots, \mathbf{Y}_{t}(x^{n}))_{t\geq 0}=(\mathbf{Y}_{t}^{1},\ldots,\mathbf{Y}_{t}^{n})_{t \geq 0}\in\mathcal{Y}^{n}\) defined by the following SDE

\[\mathrm{d}\mathbf{Y}_{t}(x)=\tfrac{1}{2}\left\{m(x)-\mathbf{Y}_{t}(x)\right\} \beta_{t}\mathrm{d}t+\beta_{t}^{1/2}\mathrm{K}(x,x)^{1/2}\mathrm{d}\mathbf{B}_ {t},\] (3)

where \(\mathrm{K}(x,x)_{i,j}=k(x^{i},x^{j})\) with \(k:\mathcal{X}\times\mathcal{X}\rightarrow\mathbb{R}\) a kernel and \(m:\mathcal{X}\rightarrow\mathcal{Y}\). The process \((\mathbf{Y}_{t}(x))_{t\geq 0}\) is a multivariate Ornstein-Uhlenbeck process--with drift \(b(t,x,\mathbf{Y}_{t}(x))=m(x)-\mathbf{Y}_{t}(x)\) and diffusion coefficient \(\sigma(t,x,\mathbf{Y}_{t}(x))=\mathrm{K}(x,x)\)--which converges with geometric rate to \(\mathrm{N}(m(x),\mathrm{K}(x,x))\). Using Phillips et al. (2022), it can be shown that this convergence extends to the _process_\((\mathbf{Y}_{t})_{t\geq 0}\) which converges to the Gaussian Process with mean \(m\) and kernel \(k\), denoted \(\mathbf{Y}_{\infty}\).

In the specific instance where \(k(x,x^{\prime})=\delta_{x}(x^{\prime})\), then the limiting process \(\mathbf{Y}_{\infty}\) is simply Gaussian _white noise_, whilst other choices such as the squared-exponential or Matern kernel would lead to the associated Gaussian limiting process \(\mathbf{Y}_{\infty}\). Note that the _white noise_ setting is not covered by the existing theory of functional diffusion models, as a Hilbert space and a square integral kernel are required, see Kerrigan et al. (2022) for instance.

**Denoising process.** Under mild conditions over \(\mathbf{Y_{0}}^{2}\), the time-reversal process \((\bar{\mathbf{Y}}_{t}(x))_{t\geq 0}\) satisfies

\[\mathrm{d}\bar{\mathbf{Y}}_{t}(x)=\{-\tfrac{1}{2}(m(x)-\bar{\mathbf{Y}}_{t}(x ))+\mathrm{K}(x,x)\nabla\log p_{T-t}(\bar{\mathbf{Y}}_{t}(x))\}\beta_{T-t} \mathrm{d}t+\beta_{T-t}^{1/2}\mathrm{K}(x,x)^{1/2}\mathrm{d}\mathbf{B}_{t},\] (4)

with \(\bar{\mathbf{Y}}_{0}\sim\mathrm{GP}(m,k)\) and \(p_{t}\) the density of \(\mathbf{Y}_{t}(x)\) w.r.t. Lebesgue. In practice, the \(\log p_{T-t}\) term-known as the Stein score, is not tractable and must be approximated by a neural network. We then consider the generative stochastic process model defined by first sampling \(\bar{\mathbf{Y}}_{0}\sim\mathrm{GP}(m,k)\) and then simulating the reverse diffusion (4) (e.g. via Euler-Maruyama discretisation).

**Manifold valued outputs.** So far we have defined our generative model with \(\mathcal{Y}=\mathbb{R}^{d}\), we can readily extend the methodology to manifold-valued functional models using _Riemannian_ diffusion models such as De Bortoli et al. (2022) and Huang et al. (2022), see App. C. One of the main notable difference is that in the case where \(\mathcal{Y}\) is a _compact_ manifold, we replace the Ornstein-Uhlenbeck process by a Brownian motion which targets the uniform distribution.

**Training.** As the reverse SDE (4) involves the preconditioned score \(\mathrm{K}(x,x)\nabla\log p_{t}\), we directly approximate it with a neural network \((\mathrm{Ks})_{\theta}:[0,T]\times\mathcal{X}^{n}\times\mathcal{Y}^{n}\to \mathrm{TJ}^{n}\), where \(\mathrm{TJ}\) is the tangent bundle of \(\mathcal{Y}\),see App. C. The conditional score of the noising process (3) is given by

\[\nabla_{\mathbf{Y}_{t}}\log p_{t}(\mathbf{Y}_{t}(x)|\mathbf{Y}_{0}(x))=-\Sigma _{t|0}^{-1}(\mathbf{Y}_{t}(x)-m_{t|0})=-\sigma_{t|0}^{-1}\mathrm{K}(x,x)^{-1/2}\varepsilon,\] (5)

since \(\mathbf{Y}_{t}=m_{t|0}+\Sigma_{t|0}^{1/2}\varepsilon\) with \(\varepsilon\sim\mathrm{N}(0,\mathrm{Id})\), and \(\Sigma_{t|0}=\sigma_{t|0}^{2}\mathrm{K}\) with \(\sigma_{t|0}=(1-\mathrm{e}^{-\int_{0}^{t}\beta(s)\mathrm{d}s})^{1/2}\), see App. B.1. We learn the preconditioned score \((\mathrm{Ks})_{\theta}\) by minimising the following denoising score matching (DSM) loss (Vincent et al., 2010) weighted by \(\Lambda(t)=\sigma_{t|0}^{2}\ \mathrm{K}^{\top}\mathrm{K}\)

\[\mathcal{L}(\theta;\Lambda(t))=\mathbb{E}[\|s_{\theta}(t,\mathbf{Y}_{t})- \nabla\log p_{t}(\mathbf{Y}_{t}|\mathbf{Y}_{0})\|_{\Lambda(t)}^{2}]=\mathbb{E }[\|\sigma_{t|0}\cdot(\mathrm{Ks})_{\theta}(t,\mathbf{Y}_{t})+\mathrm{K}^{1/2} \varepsilon\|_{2}^{2}],\] (6)

where \(\|x\|_{\Lambda}^{2}=x^{\top}\Lambda x\). Note that when targeting a unit-variance white noise, then \(\mathrm{K}=\mathrm{Id}\) and the loss (6) reverts to the DSM loss with weighting \(\lambda(t)=1/\sigma_{t|0}^{2}\)(Song et al., 2021). In App. B.2, we explore several preconditioning terms and associated weighting \(\Lambda(t)\). Overall, we found the preconditioned score \(\mathrm{K}\nabla\log p_{t}\) parameterisation, in combination with the \(\ell_{2}\) loss, to perform best, as shown by the ablation study in App. F.1.3.

### Invariant neural diffusion processes

In this section, we show how we can incorporate geometrical constraints into the functional diffusion model introduced in the previous Sec. 3.1. In particular, given a group \(G\), we aim to build a generative model over steerable feature fields as defined in Sec. 2.

**Invariant process.** A stochastic process \(f\sim\mu\) is said to be \(G-\)invariant if \(\mu(g\cdot\mathsf{A})=\mu(\mathsf{A})\) for any \(g\in G\), with \(\mu\in\mathcal{P}(\mathrm{C}(\mathcal{X},\mathcal{Y}))\), where \(\mathcal{P}\) is the space of probability measure on the space of continuous functions and \(\mathsf{A}\subset\mathrm{C}(\mathcal{X},\mathcal{Y})\) measurable. From a sample perspective, this means that with input-output pairs \(\mathcal{C}=\{(x^{i},y^{i})\}_{i=1}^{n}\), and denoting the action of \(G\) on this set as \(g\cdot\mathcal{C}\triangleq\{(g\cdot x^{i},\rho(g)y^{i})\}_{i=1}^{n}\), \(f\sim\mu\) is \(G-\)invariant if and only if \(g\cdot\mathcal{C}\) has the same distribution as \(\mathcal{C}\). In what follows, we aim to derive sufficient conditions on the model introduced in Sec. 3 so that it satisfies this \(G\)-invariance property. First, we recall such a necessary and sufficient condition for Gaussian processes.

**Proposition 3.1**.: _Invariant (stationary) Gaussian process (Holderrieth et al., 2021). We have that a Gaussian process \(\mathrm{GP}(m,k)\) is \(G\)-invariant if and only if its mean \(m\) and covariance \(k\) are suitably \(G\)-equivariant--that is, for all \(x,x^{\prime}\in\mathcal{X},g\in G\)_

\[m(g\cdot x)=\rho(g)m(x)\;\;\text{and}\;\;k(g\cdot x,g\cdot x^{\prime})=\rho(g )k(x,x^{\prime})\rho(g)^{\top}.\] (7)

Trivial examples of \(\mathrm{E}(n)\)-equivariant kernels include diagonal kernels \(k=k_{0}\,\mathrm{Id}\) with \(k_{0}\) invariant (Holderrieth et al., 2021), but see App. F.2 for non trivial instances introduced by Macedo and Castro (2010). Building on Prop. 3.1, we then state that our introduced neural diffusion process is also invariant if we additionally assume the score network to be \(G\)-equivariant.

**Proposition 3.2**.: _Invariant neural diffusion process (Yim et al., 2023, Prop 3.6). We denote by \((\bar{\mathbf{Y}}_{t}(x))_{x\in\mathcal{X},t\in[0,T]}\) the process induced by the time-reversal SDE (4) where the score is approximated by a score network \(\mathbf{s}_{\theta}:[0,T]\times\mathcal{X}^{n}\times\mathcal{Y}^{n}\to \mathrm{TJ}^{n}\), and the limiting process is given by \(\mathcal{L}(\bar{\mathbf{Y}}_{0})=\mathrm{GP}(m,k)\). Assuming \(m\) and \(k\) are respectively \(G\)-equivariant per Prop. 3.1, if we additionally have that the score network is \(G\)-equivariant vector field, i.e. \(\mathbf{s}_{\theta}(t,g\cdot x,\rho(g)y)=\rho(g)\mathbf{s}_{\theta}(t,x,y)\) for all \(x\in\mathcal{X},g\in G\), then for any \(t\in[0,T]\) the process \((\bar{\mathbf{Y}}_{t}(x))_{x\in\mathcal{X}}\) is \(G\)-invariant._

This result can be proved in two ways, from the probability flow ODE perspective or directly in terms of SDE via Fokker-Planck, see App. D.2. In particular, when modelling an invariant scalar data process \((\mathbf{Y}_{0}(x))_{x\in\mathcal{X}}\) such as a temperature field, we need the score network to admit the invariance constraint \(\mathbf{s}_{\theta}(t,g\cdot x,y)=\mathbf{s}_{\theta}(t,x,y)\).

**Equivariant conditional process.** Often precedence is given to modelling the predictive processgiven a set of observations \(\mathcal{C}=\{(x^{c},y^{c})\}_{c\in C}\). In this context, the conditional process (Pollard, 2002, p.117) inherits the symmetry of the prior process in the following sense. A stochastic process with distribution \(\mu\) given a context \(\mathcal{C}\) is said to be conditionally \(G-\)equivariant if the conditional satisfies \(\mu(\text{A}|g\cdot\mathcal{C})=\mu(g\cdot\text{A}|\mathcal{C})\) for any \(g\in G\) and \(\text{A}\in\text{C}(\mathcal{X},\mathcal{Y})\) measurable, as illustrated in Fig. 2.

**Proposition 3.3**.: _Equivariant conditional process. Assume a stochastic process \(f\sim\mu\) is \(G-\)invariant. Then the conditional process \(f|\mathcal{C}\) given a set of observations \(\mathcal{C}\) is \(G\)-equivariant._

Originally stated in Holderrieth et al. (2021) in the case where the process is over functions of the form \(f:\mathbb{R}^{n}\to\mathbb{R}^{d}\) and marginals with density w.r.t. Lebesgue, we prove Prop. 3.3 for stochastic processes over generic fields on manifolds in terms only of the measure of the process (App. D.3).

### Conditional sampling

There exist several methods to perform conditional sampling in diffusion models such as: replacement sampling, amortisation and conditional guidance, which we discuss in App. E.1. Here we propose a new method for sampling from exact conditional distributions of NDPs using only the score network for the joint distribution. Using the fact that the conditional score can be written as \(\nabla_{x}\log p(x|y)=\nabla_{x}\log p(x,y)-\nabla_{x}\log p(y)=\nabla_{x}\log p (x,y)\) we can therefore, for any point in the diffusion time, conditionally sample using Langevin dynamics, following the SDE \(\mathrm{d}\mathbf{Y}_{s}=\frac{1}{2}\mathrm{K}\nabla\log p_{T-t}(\mathbf{Y}_{s} )\mathrm{d}s+\sqrt{\mathrm{K}}\mathrm{d}\mathbf{B}_{s}\), by only applying the diffusion to the variables of interest and holding the others fixed. While we could sample directly at the end time this proves difficult in practice. Similar to the motivation of Song and Ermon (2019), we sample along the reverse diffusion, taking a number of conditional Langevin steps at each time. In addition, we apply the forward noising SDE to the conditioning points at each step, as this puts the combined context and sampling set in a region that the score function will be well learned in training. Our procedure is illustrated in Alg. 1. In App. E.1 we draw links with RePaint of Lugmayr et al. (2022).

### Likelihood evaluation

Similarly to Song et al. (2021), we can derive a deterministic ODE which has the same marginal density as the SDE (3), which is given by the 'probability flow' Ordinary Differential Equation (ODE), see App. B. Once the score network is learnt, we can thus use it in conjunction with an ODE solver to compute the likelihood of the model. A perhaps more interesting task is to evaluate the predictive posterior likelihood \(p(y^{*}|x^{*},\{x^{i},y^{i}\}_{i\in C})\) given a context set \(\{x^{i},y^{i}\}_{i\in C}\). A simple approach is to simply rely on the conditional probability rule evaluate \(p(y^{*}|x^{*},\{x^{i},y^{i}\}_{i\in C})=p(y^{*},\{y^{i}\}_{i\in C}|x^{*},\{x^{ i}\}_{i\in C})/p(\{y^{i}\}_{i\in C}|\{x^{i}\}_{i\in C})\). This can be done by solving two probability flow ODEs: one over the joint evaluation and context set, and another only over the context set.

Figure 3: Illustration of Langevin corrected conditional sampling. The black line represents the noising process dynamics \((p_{t})_{t\in[0,T]}\). The time reversal (i.e. predictor) step, is combined with a Langevin corrector step projecting back onto the dynamics.

Figure 2: Samples from equivariant neural diffusion processes conditioned on context set \(\mathcal{C}\) (in red) and evaluated on a regular grid \(x^{*}\) for scalar (_Left_) and 2D vector (_Right_) fields. Same model is then conditioned on transformed context \(g\cdot\mathcal{C}\), with group element \(g\) being a translation of length \(2\) (_Left_) or a \(90^{\circ}\) rotation (_Right_).

Related work

**Gaussian processes and the neural processes family.** One important and powerful framework to construct distributions over functional spaces are Gaussian processes (Rasmussen, 2003). Yet, they are restricted in their modelling capacity and when using exact inference they scale poorly with the number of datapoints. These problems can be partially alleviated by using neural processes (Kim et al., 2019; Garnelo et al., 2018; Garnelo et al., 2018; Jha et al., 2022; Louizos et al., 2019; Singh et al., 2019), although they also assume a Gaussian likelihood. Recently introduced autoregressive NPs (Bruinsma et al., 2023) alleviate this limitation, but they are disadvantaged by the fact that variables early in the auto-regressive generation only have simple distributions (typically Gaussian). Finally, (Dupont et al., 2022) model weights of implicit neural representation using diffusion models.

Stationary stochastic processes.The most popular Gaussian process kernels (e.g. squared exponential, Matern) are stationary, that is, they are translation invariant. These lead to invariant Gaussian processes, whose samples when translated have the same distribution as the original ones. This idea can be extended to the entire isometry group of Euclidean spaces (Holderrieth et al., 2021), allowing for modelling higher order tensor fields, such as wind fields or incompressible fluid velocity (Macedo and Castro, 2010). Later, Azangulov et al. (2022) and Azangulov et al. (2023) extended stationary kernels and Gaussian processes to a large class of non-Euclidean spaces, in particular all compact spaces, and symmetric non compact spaces. In the context of neural processes, (Gordon et al., 2020) introduced ConvCNP so as to encode translation equivariance into the predictive process. They do so by embedding the context into a translation equivariant functional representation which is then decoded with a convolutional neural network. Holderrieth et al. (2021) later extended this idea to construct neural processes that are additionally equivariant w.r.t. rotations or subgroup thereof.

Spatial structure in diffusion models.A variety of approaches have also been proposed to incorporate spatial correlation in the noising process of finite-dimensional diffusion models leveraging the multiscale structure of data (Jing et al., 2022; Guth et al., 2022; Ho et al., 2022; Saharia et al., 2021; Hoogeboom and Salimans, 2022; Rissanen et al., 2022). Our methodology can also be seen as a principled way to modify the forward dynamics in classical denoising diffusion models. Hence, our contribution can be understood in the light of recent advances in generative modelling on soft and cold denoising diffusion models (Daras et al., 2022; Bansal et al., 2022; Hoogeboom and Salimans, 2022). Several recent work explicitly introduced a covariance matrix in the Gaussian noise, either on a choice of kernel (Bilos et al., 2022), based on Discrete Fourier Transform of images (Voleti et al., 2022), or via empirical second order statistics (squared pairwise distances and the squared radius of gyration) for protein modelling (Ingraham et al., 2022). Alternatively, (Guth et al., 2022) introduced correlation on images leveraging a wavelet basis.

Functional diffusion models.Infinite dimensional diffusion models have been investigated in the Euclidean setting in (Kerrigan et al., 2022; Pidstrigach et al., 2023; Lim et al., 2023; Bond-Taylor and Willcocks, 2023; Hagemann et al., 2023; Franzese et al., 2023; Dutordoir et al., 2022; Phillips et al., 2022). Most of these works are based on an extension of the diffusion models techniques (Song et al., 2021; Ho et al., 2020) to the infinite-dimensional space, leveraging tools from the Cameron-Martin theory such as the Feldman-Hajek theorem (Kerrigan et al., 2022; Pidstrigach et al., 2023) to define infinite-dimensional Gaussian measures and how they interact. We refer to (Da Prato and Zabczyk, 2014) for a thorough introduction to Stochastic Differential Equations in infinite dimension. (Phillips et al., 2022) consider another approach by defining countable diffusion processes in a basis. All these approaches amount to learn a diffusion model with spatial structure. Note that this induced correlation is necessary for the theory of infinite dimensional SDE (Da Prato and Zabczyk, 2014) to be applied but is not necessary to implement diffusion models (Dutordoir et al., 2022). Several approaches have been considered for conditional sampling. (Pidstrigach et al., 2023; Bond-Taylor and Willcocks, 2023) modify the reverse diffusion to introduce a guidance term, while (Dutordoir et al., 2022; Kerrigan et al., 2022) use the replacement method. Finally (Phillips et al., 2022) amortise the score function w.r.t. the conditioning context.

## 5 Experimental results

### 1D regression over stationary scalar fields

We evaluate GeomNDPs on several synthetic 1D regression datasets. We follow the same experimental setup as Bruinsma et al. (2020) which we detail in App. F.1. In short, it contains Gaussian (Squared Exponential (SE), Matern\((\frac{5}{2})\), Weakly Periodic) and non-Gaussian (Sawtooth and Mixture) sample paths, where Mixture is a combination of the other four datasets with equal weight. Fig. 9 shows samples for each of these dataset. The Gaussian datasets are corrupted with observation noise with variance \(\sigma^{2}=0.05^{2}\). Table 1 reports the average log-likelihood \(p(y^{*}|x^{*},\mathcal{C})\) across \(4096\) test samples, where the context set size is uniformly sampled between \(1\) and \(10\) and the target has fixed size of \(50\). All inputs \(x^{c},x^{*}\) are chosen uniformly within their input domain which is \([\text{-}2,2]\) for the training data and 'interpolation' evaluation and \([2,6]\) for the 'generalisation' evaluation.

We compare the performance of GeomNDP to a GP with the true hyperparameters (when available), a (convolutional) Gaussian NP (Bruinsma et al., 2020), a convolutional NP (Gordon et al., 2020) and a vanilla attention-based NDP (Dutordoir et al., 2022) which we reformulated in the continuous diffusion process framework to allow for log-likelihood evaluations and thus a fair comparison--denoted \(\text{NDP}^{*}\). We enforce translation invariance in the score network for GeomNDP by subtracting the centre of mass from the input \(x\), inducing stationary scalar fields.

On the GP datasets, GNP, ConvNPs and GeomNDP methods are able to fit the conditionals perfectly--matching the log-likelihood of the GP model. GNP's performance degrades on the non-Gaussian datasets as it is restricted by its conditional Gaussian assumption, whilst NDPs methods still performs well as illustrated on Fig. 4. In the bottom rows of Table 1, we assess the models ability to generalise outside of the training input range \(x\in[\text{-}2,2]\), and evaluate them on a translated grid where context and target points are sampled from \([2,6]\). Only convolutional NPs (GNP and ConvNP) and \(\mathrm{T}(1)-\)GeomNDP are able to model stationary processes and therefore to perform as well as in the interpolation task. The \(\text{NDP}^{*}\), on the contrary, drastically fails at this task.

\begin{table}
\begin{tabular}{l l l r r r r} \hline \hline  & & SE & Matern\((\frac{5}{2})\) & Weakly Per. & Sawtooth & Mixture \\ \hline \multirow{4}{*}{**Model Prior**} & GP (optimum) & \(0.70\pm_{0.00}\) & \(0.31\pm_{0.00}\) & \(-0.32\pm_{0.00}\) & - & - \\  & \(\mathrm{T}(1)-\)**GeomNDP** & \(\mathbf{0.72\pm_{0.03}}\) & \(\mathbf{0.32\pm_{0.03}}\) & \(\mathbf{-0.38\pm_{0.03}}\) & \(\mathbf{3.39\pm_{0.04}}\) & \(\mathbf{0.64\pm_{0.08}}\) \\  & \(\mathrm{NDP}^{*}\) & \(\mathbf{0.71\pm_{0.03}}\) & \(\mathbf{0.30\pm_{0.03}}\) & \(\mathbf{-0.37\pm_{0.03}}\) & \(\mathbf{3.39\pm_{0.04}}\) & \(\mathbf{0.64\pm_{0.08}}\) \\  & GNP & \(\mathbf{0.70\pm_{0.01}}\) & \(\mathbf{0.30\pm_{0.01}}\) & \(-0.47\pm_{0.01}\) & \(0.42\pm_{0.01}\) & \(0.10\pm_{0.02}\) \\  & ConvNP & \(-0.46\pm_{0.01}\) & \(-0.67\pm_{0.01}\) & \(-1.02\pm_{0.01}\) & \(1.20\pm_{0.01}\) & \(-0.50\pm_{0.02}\) \\ \hline \multirow{4}{*}{**Model Prior**} & GP (optimum) & \(0.70\pm_{0.00}\) & \(0.31\pm_{0.00}\) & \(-0.32\pm_{0.00}\) & - & - \\  & \(\mathrm{T}(1)-\)**GeomNDP** & \(\mathbf{0.70\pm_{0.02}}\) & \(\mathbf{0.31\pm_{0.02}}\) & \(\mathbf{-0.38\pm_{0.03}}\) & \(\mathbf{3.39\pm_{0.03}}\) & \(\mathbf{0.62\pm_{0.02}}\) \\ \cline{1-1}  & NDP\({}^{*}\) & \(*\) & \(*\) & \(*\) & \(*\) & \(*\) \\ \cline{1-1}  & GNP & \(\mathbf{0.69\pm_{0.01}}\) & \(\mathbf{0.30\pm_{0.01}}\) & \(-0.47\pm_{0.01}\) & \(0.42\pm_{0.01}\) & \(0.10\pm_{0.02}\) \\ \cline{1-1}  & ConvNP & \(-0.46\pm_{0.01}\) & \(-0.67\pm_{0.01}\) & \(-1.02\pm_{0.01}\) & \(1.19\pm_{0.01}\) & \(-0.53\pm_{0.02}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Mean test log-likelihood (TLL) \(\pm\) 1 standard error estimated over 4096 test samples are reported. Statistically significant best non-GP model is in **bold**. ‘*’ stands for a TLL below \(-10\). NP baselines from Bruinsma et al. (2020). \(\mathrm{T}(1)-\)GeomNDP indicates our proposed method with a translation invariant score.

Figure 4: Prior and posterior samples (in blue) from the data process and the GeomNDP model, with context points in red and posterior mean in black.

**Non white kernels for limiting process.** The NDP methods in the above experiment target the white kernel \(\mathbbm{1}(x=x^{\prime})\) in the limiting process. In App. F.1.3, we explore different choices for the limiting kernel, such as SE and periodic kernels with short and long lengthscales, along with several score parameterisations, see App. B.3 for a description of these. We observe that although choosing such kernels gives a head start to the training, it eventually yield slightly worse performance. We attribute this to the additional complexity of learning a non-diagonal covariance. Finally, across all datasets and limiting kernels, we found the preconditioned score \(\mathrm{K}\mathrm{V}\log p_{t}\) to result in the best performance.

**Conditional sampling ablation.** We employ the SE dataset to investigate various configurations of the conditional sampler as we have access to the ground truth conditional distribution through the GP posterior. In Fig. 11 we compute the Kullback-Leibler divergence between the samples generated by GeomNDP and the actual conditional distribution across different conditional sampling settings. Our results demonstrate the importance of performing multiple Langevin dynamics steps during the conditional sampling process. Additionally, we observe that the choice of noising scheme for the context values \(y_{c}\) has relatively less impact on the overall outcome.

### Regression over Gaussian process vector fields

We now focus our attention to modelling equivariant vector fields. For this, we create datasets using samples from a two-dimensional zero-mean GP with one of the following \(\mathrm{E}(2)\)-equivariant kernels: a diagonal Squared-Exponential (SE) kernel, a zero curl (Curl-free) kernel and a zero divergence (Div-free) kernel, as described in App. D.1.

We equip our model, GeomNDP, with a \(\mathrm{E}(2)\)-equivariant score architecture, based on steerable CNNs (Thomas et al., 2018; Weiler and Cesa, 2021). We compare to NDP\({}^{*}\) with a non-equivariant attention-based network (Dutordoir et al., 2022). We also evaluate two neural processes, a translation-equivariant ConvCNP (Gordon et al., 2020) and a \(\mathrm{C}4\ltimes\mathbb{R}^{2}\subset\mathrm{E}(2)\)-equivariant SteerCNP (Holderrieth et al., 2021). We also report the performance of the data-generating GP, and the same GP but with diagonal posterior covariance GP (Diag.). We measure the predictive log-likelihood of the data process samples under the model on a held-out test dataset.

We observe in Fig. 5 (Left), that the CNPs performance is limited by their diagonal predictive covariance assumption, and as such cannot do better than the GP (Diag.). We also see that although NDP\({}^{*}\) is able to fit well GP posteriors, it does not reach the maximum log-likelihood value attained by the data GP, in contrast to its equivariant counterpart GeomNDP. To further explore gains brought by the built-in equivariance, we explore the data-efficiency in Fig. 5 (Right), and notice that E(2)-GeomNDP requires few data samples to fit the data process, since effectively the dimension of the (quotiented) state space is dramatically reduced.

### Global tropical cyclone trajectory prediction

Finally, we assess our model on a task where the domain of the stochastic process is a non-Euclidean manifold. We model the trajectories of cyclones over the earth, modelled as sample paths of the form \(\mathbb{R}\rightarrow\mathcal{S}^{2}\) coming from a stochastic process. The data is drawn from the International Best Track Archive for Climate Stewardship (IBTrACS) Project, Version 4 ((Knapp et al., 2010; Knapp et al., 2018)) and preprocessed as per App. F.3, where details on the implementation of the score function, the ODE/SDE solvers used for the sampling, and baseline methods can be found.

Figure 5: Quantitative results for experiments on GP vector fields. Mean predictive log-likelihood (\(\uparrow\)) and confidence interval estimated over 5 random seeds. _Left_: Comparison with neural processes. Statistically significant results are in **bold**. _Right_: Ablation study when varying the number of training data samples.

Fig. 6 shows some cyclone trajectories samples from the data process and from a trained GeomNDP model. We also demonstrate how such trajectories can be interpolated or extrapolated using the conditional sampling method detailed in Sec. 3.3. Such conditional sample paths are shown in Fig. 7. Additionally, we report in Table 2 the likelihood and MSE for a series of methods. The interpolation task involves conditioning on the first and last 20% of the cyclone trajectory and predicting intermediary positions. The extrapolation task involves conditioning on the first 40% of trajectories and predicting future positions. We see that the GPs (modelled as \(f:\mathbb{R}\rightarrow\mathbb{R}^{2}\), one on latitude/longitude coordinates, the other via a stereographic projection, using a diagonal RBF kernel with hyperparameters fitted with maximum likelihood) fail drastically given the high non-Gaussianity of the data. In the interpolation task, the NDP performs as well as the GeomNDP, but the additional geometric structure of modelling the outputs living on the sphere appears to significantly help for extrapolation. See App. F.3 for more fine-grained results.

## 6 Discussion

In this work, we have extended diffusion models to model invariant stochastic processes over tensor fields. We did so by (a) constructing a continuous noising process over function spaces which correlate input samples with an equivariant kernel, (b) parameterising the score with an equivariant neural network. We have empirically demonstrated the ability of our introduced model GeomNDP to fit complex stochastic processes, and by encoding the symmetry of the problem at hand, we show that it is more data efficient and better able to generalise.

We highlight below some current limitations and important research directions. First, evaluating the model is slow as it relies on costly SDE or ODE solvers, as existing diffusion models. Second, targeting a white noise process appears to over-perform other Gaussian processes. In future work, we would like to investigate the practical influence of different kernels. Third, strict invariance may sometimes be too strong, we thus suggest softening it by amortising the score network over extra spatial information available from the problem at hand.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline \multirow{2}{*}{**Model**} & \multicolumn{2}{c}{Test data} & \multicolumn{2}{c}{Interpolation} & \multicolumn{2}{c}{Extrapolation} \\  & Likelihood & Likelihood & MSE (km) & Likelihood & MSE (km) \\ \hline GeomNDP (\(\mathbb{R}\xrightarrow{}\mathcal{S}^{2}\)) & \(\mathbf{802}_{\pm 5}\) & \(\mathbf{535}_{\pm 4}\) & \(\mathbf{162}_{\pm 6}\) & \(\mathbf{536}_{\pm 4}\) & \(\mathbf{496}_{\pm 14}\) \\ Stereographic GP (\(\mathbb{R}\rightarrow\mathbb{R}^{2}/\{0\}\)) & \(393_{\pm 3}\) & \(266_{\pm 3}\) & \(2619_{\pm 13}\) & \(245_{\pm 2}\) & \(6587_{\pm 55}\) \\ NDP (\(\mathbb{R}\rightarrow\mathbb{R}^{2}\)) & - & - & \(166_{\pm 22}\) & - & \(769_{\pm 48}\) \\ GP (\(\mathbb{R}\rightarrow\mathbb{R}^{2}\)) & - & - & \(685_{\pm 41}\) & - & \(8138_{\pm 87}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparative results of different models on the cyclone dataset, comparing test set likelihood, interpolation likelihood and mean squared error (MSE), and extrapolation likelihood and mean squared error. These are estimated over 5 random seeds. We only report likelihoods of models defined w.r.t the uniform measure on \(\mathcal{S}^{2}\).

Figure 6: _Left:_ 1000 samples from the training data. _Right:_ 1000 samples from the trained model.

Figure 7: _Top:_ Examples of conditional trajectories sampled from the GeomNDP model. _Blue:_ Conditioned sections of the trajectory. _Green:_ The actual trajectory of the cyclone. _Red:_ conditional samples from the model. _Purple:_ closest matching trajectories in the dataset to the conditioning data.

## Acknowledgements

We are grateful to Paul Rosa for helping with the proof, and to Jose Miguel Hernandez-Lobato for useful discussions. We thank the hydra(Yadan, 2019), jax(Bradbury et al., 2018) and geomstats(Miolaane et al., 2020) teams, as our library is built on these great libraries. Richard E. Turner and Emile Mathieu are supported by an EPSRC Prosperity Partnership EP/T005386/1 between Microsoft Research and the University of Cambridge. Michael J. Hutchinson is supported by the EPSRC Centre for Doctoral Training in Modern Statistics and Statistical Machine Learning (EPS/S023151/1).

## References

* A. Bansal, E. Borgnia, H. Chu, J. S. Li, H. Kazemi, F. Huang, M. Goldblum, J. Geiping, and T. Goldstein (2022)Cold diffusion: inverting arbitrary image transforms without noise. arXiv preprint arXiv:2208.09392. Cited by: SS1.
* M. Bilos, K. Rasul, A. Schneider, Y. Nevmyvaka, and S. Gunnemann (2022)Modeling temporal data as continuous functions with process diffusion. External Links: https://openreview.net/forum?id=VmJKUyp08uR Cited by: SS1.
* S. Bond-Taylor and C. G. Willcocks (2023)Infinite-diff: infinite resolution diffusion with subsampled mollified states. arXiv preprint arXiv:2303.18242. Cited by: SS1.
* J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang (2018)JAX: composable transformations of Python+NumPy programs. External Links: http://github.com/google/jax Cited by: SS1.
* W. Bruinsma, S. Markou, J. Requeima, A. Y. K. Foong, A. Vaughan, T. Andersson, A. Buonomo, S. Hosking, and R. E. Turner (2023)Autoregressive conditional neural processes. In International Conference on Learning Representations, External Links: https://openreview.net/forum?id=OAsXFPBfTBh Cited by: SS1.
* W. Bruinsma, J. Requeima, A. Y. K. Foong, J. Gordon, and R. E. Turner (2020)Gaussian neural processes. In 3rd Symposium on Advances in Approximate Bayesian Inference, External Links: 2004.07708, Link, Document Cited by: SS1.
* P. Cattiaux, G. Conforti, I. Gentil, and C. Leonard (2021)Time reversal of diffusion processes under a finite entropy condition. arXiv preprint arXiv:2104.07708. Cited by: SS1.
* S. Chen, S. Chewi, J. Li, Y. Li, A. Salim, and A. R. Zhang (2022)Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. arXiv preprint arXiv:2209.11215. Cited by: SS1.
* T. Cohen (2021)Equivariant convolutional networks. PhD thesis. Cited by: SS1.
* G. Da Prato and J. Zabczyk (2014)Stochastic equations in infinite dimensions. Cambridge university press. Cited by: SS1.
* A. S. Dalalyan (2017)Theoretical guarantees for approximate sampling from smooth and log-concave densities. J. R. Stat. Soc. Ser. B. Stat. Methodol.79 (3), pp. 651-676. External Links: ISSN 1369-7412, Document, Link, https://doi.org/10.1111/rssb.12183 Cited by: SS1.
* G. Daras, M. Delbracio, H. Talebi, A. G. Dimakis, and P. Milanfar (2022)Soft diffusion: score matching for general corruptions. arXiv preprint arXiv:2209.05442. Cited by: SS1.
*

[MISSING_PAGE_FAIL:11]

J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 2020. Cited on pages 1, 6.
* J. Ho, C. Saharia, W. Chan, D. J. Fleet, M. Norouzi, and T. Salimans (2022)Cascaded diffusion models for high fidelity image generation. _J. Mach. Learn. Res._, 23:47-1, 2022. Cited on page 6.
* P. Holderrieth, M. J. Hutchinson, and Y. W. Teh (2021)Equivariant learning of stochastic fields: gaussian processes and steerable conditional neural processes. In International Conference on Machine Learning, Cited on pages 2, 4-6.
* E. Hoogeboom and T. Salimans (2022)Blurring diffusion models. arXiv preprint arXiv:2209.05557. Cited on page 6.
* C. Huang, M. Aghajohari, J. Bose, P. Panangaden, and A. C. Courville (2022)Riemannian diffusion models. Advances in Neural Information Processing Systems35, pp. 2750-2761. Cited on pages 1, 4.
* J. D. Hunter (2007)Matplotlib: a 2d graphics environment. Computing in Science & Engineering9 (3), pp. 90-95. Cited on page 13.
* M. Hutchinson, A. Terenin, V. Borovitskiy, S. Takao, Y. Teh, and M. Deisenroth (2021)Vector-valued gaussian processes on riemannian manifolds via gauge independent projected kernels. Advances in Neural Information Processing Systems34, pp. 17160-17169. Cited on page 5.
* J. Ingraham, M. Baranov, Z. Costello, V. Frappier, A. Ismail, S. Tie, W. Wang, V. Xue, F. Obermeyer, A. Beam, and G. Grigoryan (2022)Illuminating protein space with a programmable generative model. bioRxiv. External Links: 2002.2010 Cited by: SS1.
* S. Jha, D. Gong, X. Wang, R. E. Turner, and L. Yao (2022)The neural process family: survey, applications and perspectives. arXiv preprint arXiv:2209.00517. Cited by: SS1.
* B. Jing, G. Corso, R. Berlinghieri, and T. Jaakkola (2022)Subspace diffusion generative models. arXiv preprint arXiv:2205.01490. Cited by: SS1.
* O. Kallenberg (2021)Foundations of modern probability. Probability Theory and Stochastic Modelling, Springer. External Links: ISBN: 978-3-030-61872-8. External Links: Link Cited by: SS1.
* T. Karras, M. Aittala, T. Aila, and S. Laine (2022)Elucidating the Design Space of Diffusion-Based Generative Models. External Links: 2206.00364 Cited by: SS1.
* G. Kerrigan, J. Ley, and P. Smyth (2022)Diffusion Generative Models in Infinite Dimensions. External Links: 2212.00886 Cited by: SS1.
* H. Kim, A. Mnih, J. Schwarz, M. Garnelo, A. Eslami, D. Rosenbaum, O. Vinyals, and Y. W. Teh (2019)Attentive neural processes. In International Conference on Learning Representations, External Links: Link Cited by: SS1.
* D. P. Kingma and J. Ba (2015)Adam: a method for stochastic optimization. arXiv:1412.6980 [cs]. External Links: Link Cited by: SS1.
* H. J. Knapp, K. Diamond, J. P. Kossin, M. C. Kruk, and C. J. I. Schreck (2018)International Best Track Archive for Climate Stewardship (IBTrACS) Project, Version 4. Technical report NOAA National Centers for Environmental Information. External Links: 1706.00002 Cited by: SS1.
J. Kohler, L. Klein, and F. Noe. Equivariant Flows: exact likelihood generative learning for symmetric densities. _arXiv:2006.02425 [physics, stat]_, June 2020. url: http://arxiv.org/abs/2006.02425. Cited on page 6.
* Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner (1998) Gradient-based learning applied to document recognition. In Proceedings of the IEEE, Vol. 86 of number 11, pp. 2278-2324. External Links: Document, Link Cited by: SS1.
* H. Lee, J. Lu, and Y. Tan (2023) Convergence of score-based generative modeling for general data distributions. In International Conference on Algorithmic Learning Theory, pp. 946-985. Cited by: SS1.
* J. H. Lim, N. B. Kovachki, R. Baptista, C. Beckham, K. Azizzadenesheli, J. Kossaifi, V. Voleti, J. Song, K. Kreis, J. Kautz, C. Pal, A. Vahdat, and A. Anandkumar (2023) Score-based diffusion models in function space. External Links: 10.48550/ARXIV.2302.07400 Cited by: SS1.
* J. H. Lim, N. B. Kovachki, R. Baptista, C. Beckham, K. Azizzadenesheli, J. Kossaifi, V. Voleti, J. Song, K. Kreis, J. Kautz, C. Pal, A. Vahdat, and A. Anandkumar (2023) Score-based diffusion models in function space. External Links: 2304.04740 Cited by: SS1.
* A. Lou and S. Ermon (2023) Reflected diffusion models. arXiv preprint arXiv:2304.04740. Cited by: SS1.
* C. Louizos, X. Shi, K. Schutte, and M. Welling (2019) The functional neural process. Advances in Neural Information Processing Systems32. Cited by: SS1.
* A. Lugmayr, M. Danelljan, A. Romero, F. Yu, R. Timofte, and L. Van Gool (2022) Repairt: inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11461-11471. Cited by: SS1.
* I. Macedo and R. Castro (2010) Learning divergence-free and curl-free vector fields with matrix-valued kernels. External Links: 1005.0007 Cited by: SS1.
* W. McKinney et al. (2010) Data structures for statistical computing in python. In Proceedings of the 9th Python in Science Conference, Vol. 445, pp. 51-56. Cited by: SS1.
* N. Miolane, N. Guigui, A. L. Brigant, J. Mathe, B. Hou, Y. Thanwerdas, S. Heyder, O. Peltre, N. Koep, H. Zaatiti, H. Hajri, Y. Cabanes, T. Gerald, P. Chauchat, C. Shewmake, D. Brooks, B. Kainz, C. Donnat, S. Holmes, and X. Pennec (2020) Geomstats: a python package for riemannian geometry in machine learning. Journal of Machine Learning Research21 (223), pp. 1-9. External Links: Document, Link Cited by: SS1.
* B. Oksendal (2003) Stochastic differential equations. In Stochastic differential equations, pp. 65-84. Cited by: SS1.
* G. Papamakarios, E. Nalisnick, D. J. Rezende, S. Mohamed, and B. Lakshminarayanan (2019) Normalizing flows for probabilistic modeling and inference. arXiv preprint arXiv:1912.02762. Cited by: SS1.
* A. Phillips, T. Seror, M. Hutchinson, V. De Bortoli, A. Doucet, and E. Mathieu (2022) Spectral Diffusion Processes. External Links: 2209.14125 Cited by: SS1.
* J. Pidstrigach, Y. Marzouk, S. Reich, and S. Wang (2023) Infinite-dimensional diffusion models for Function Spaces. External Links: 2302.10130 Cited by: SS1.
* D. Pollard (2002) A user's guide to measure theoretic probability. Cambridge Series in Statistical and Probabilistic Mathematics, Cambridge University Press. External Links: 978-0-521-00289-9. url: https://books.google.co.uk/books?id=B7Ch-c2G21MC Cited by: SS1.

[MISSING_PAGE_POST]

C. E. Rasmussen. Gaussian processes in machine learning. In _Summer school on machine learning_, pages 63-71. Springer, 2003. Cited on pages 2, 6.
* S. Rissanen, M. Heinonen, and A. Solin (2022)Generative modelling with inverse heat dissipation. External Links: 10.48550 Cited by: SS1.
* G. O. Roberts and R. L. Tweedie (1996)Exponential convergence of Langevin distributions and their discrete approximations. Bernoulli, 2 (4), pp. 341-363. External Links: ISSN 1350-7265, Document, Link Cited by: SS1.
* C. Saharia, J. Ho, W. Chan, T. Salimans, D. J. Fleet, and M. Norouzi (2021)Image super-resolution via iterative refinement. arXiv preprint arXiv:2104.07636. Cited by: SS1.
* S. Sarkka and A. Solin (2019)Applied stochastic differential equations. Cambridge University Press. External Links: ISBN: 978-1-108-18673-5 978-1-316-51008-7 978-1-316-64946-6. doi: 10.1017/9781108186735 Cited by: SS1.
* L. Scott and J. Serre (1996)Linear representations of finite groups. Graduate Texts in Mathematics, Springer New York. External Links: ISBN 978-0-387-90190-9. URL: https://books.google.co.uk/books?id=NICfZgr54TJ4C Cited by: SS1.
* G. Singh, J. Yoon, Y. Son, and S. Ahn (2019)Sequential neural processes. _Advances in Neural Information Processing Systems_, 32. External Links: ISSN 0026-6073, Document, Link Cited by: SS1.
* Y. Song and S. Ermon (2019)Generative modeling by estimating gradients of the data distribution. In Advances in Neural Information Processing Systems, Cited by: SS1.
* Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole (2021)Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, Cited by: SS1.
* D. W. Stroock and S. S. Varadhan (2007)Multidimensional diffusion processes. Springer. External Links: ISBN 978-1-450-3819-1 Cited by: SS1.
* N. Thomas, T. Smidt, S. Kearnes, L. Yang, L. Li, K. Kohlhoff, and P. Riley (2018)Tensor field networks: rotation- and translation-equivariant neural networks for 3D point clouds. arXiv:1802.08219 [cs]. External Links: Link Cited by: SS1.
* M. Titsias (2018)Variational learning of inducing variables in sparse gaussian processes. In Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics, Vol. 5 of Proceedings of Machine Learning Research, H. D. van Dyk and M. Welling (Eds.), Vol. 5, pp. 567-574. External Links: Link Cited by: SS1.
* B. L. Trippe, J. Yim, D. Tischer, T. Broderick, D. Baker, R. Barzilay, and T. Jaakkola (2022)Diffusion probabilistic modeling of protein backbones in 3D for the motif-scaffolding problem. External Links: 10.48550/arXiv.2206.04119 Cited by: SS1.
* G. Van Rossum and F. L. Drake Jr (1995)Python reference manual. 1995. Cited by: SS1.
* A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin (2017)Attention is all you need. In Advances in Neural Information Processing Systems, Cited by: SS1.
* T. Vincent, L. Risser, and P. Ciuciu (2010)Spatially adaptive mixture modeling for analysis of fMRI time series. IEEE Trans. Med. Imag.29 (4), pp. 1059-1074. External Links: ISSN 0278-0062, Document, Link Cited by: SS1.
* P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau, E. Burovski, P. Peterson, W. Weckesser, J. Bright, S. J. van der Walt, M. Brett, J. Wilson, K. J. Millman, N. Mayorov, A. R. J. Nelson, E. Jones, R. Kern, E. Larson, C. J. Carey, I. Polat, Y. Feng, E. W. Moore, J. VanderPlas, D. Laxalde, J. Perktold, R. Cimrman, I. Henriksen, E. A. Quintero, C. R. Harris, A. M. Archibald, A. H. Ribeiro, F. Pedregosa, P. van Mulbregt, and SciPy 1.0 Contributors. SciPy

[MISSING_PAGE_FAIL:15]

**Supplementary to:**

**Geometric Neural Diffusion Processes**

## Appendix A Organisation of appendices

In this supplementary, we first introduce in App. B an Ornstein Uhlenbeck process on function space (via finite marginals) along with several score approximations. Then in App. C, we show how this methodology extend to manifold-valued inputs or outputs. Later in App. D, we derive sufficient conditions for this introduced model to yield a group invariant process. What's more in App. E, we study some conditional sampling schemes. Eventually in App. F, we give a thorough description of experimental settings along with additional empirical results.

## Appendix B Ornstein Uhlenbeck on function space

### Multivariate Ornstein-Uhlenbeck process

First, we aim to show that we can define a stochastic process on an infinite dimensional function space, by defining the joint finite marginals \(\mathbf{Y}(x)\) as the solution of a multidimensional Ornstein-Uhlenbeck process. In particular, for any set of input \(x=(x_{1},\cdots,x_{k})\in\mathcal{X}^{k}\), we define the joint marginal as the solution of the following SDE

\[\mathrm{d}\tilde{\mathbf{Y}}_{t}(x)=(m(x)-\tilde{\mathbf{Y}}_{t}(x))/2\: \beta_{t}\mathrm{d}t+\sqrt{\beta_{t}K(x,x)}\mathrm{d}\mathbf{B}_{t}\;.\] (8)

**Proposition B.1**.: _(_Phillips et al._,_ 2022_)_ _We assume we are given a data process \((\mathbf{Y}_{0}(x))_{x\in\mathcal{X}}\) and we denote by \(\mathbf{G}\sim\mathrm{GP}(0,k)\) a Gaussian process with zero mean and covariance. Then let's define_

\[\mathbf{Y}_{t}\triangleq e^{-\frac{1}{2}\cdot\int_{s=0}^{t}\beta_{s}ds}\: \mathbf{Y}_{0}+\left(1-e^{-\frac{1}{2}\cdot\int_{s=0}^{t}\beta_{s}ds}\right)m +\left(1-e^{-\int_{s=0}^{t}\beta_{s}ds}\right)^{1/2}\mathbf{G}.\]

_Then \((\mathbf{Y}_{t}(x))_{x\in\mathcal{X}}\) is a stochastic process (by virtue of being a linear combination of stochastic processes). We thus have that \(\mathbf{Y}_{t}\xrightarrow[t\to 0]{a.s.}\mathbf{Y}_{0}\) and \(\mathbf{Y}_{t}\xrightarrow[t\to\infty]{a.s.}\mathbf{Y}_{\infty}\) with \(\mathbf{Y}_{\infty}\sim\mathrm{GP}(m,k)\), so effectively \((\mathbf{Y}_{t}(x))_{t\in\mathbb{R}_{+},x\in\mathcal{X}}\) interpolates between the data process and this limiting Gaussian process. Additionally, \(\mathcal{L}(\mathbf{Y}_{t}|\mathbf{Y}_{0}=y_{0})=\mathrm{GP}(m_{t},K_{t})\) with \(m_{t}=e^{-\frac{1}{2}\cdot\int_{s=0}^{t}\beta_{s}ds}\:y_{0}+\left(1-e^{-\frac{ 1}{2}\cdot\int_{s=0}^{t}\beta_{s}ds}\right)m\) and \(\Sigma_{t}=\left(1-e^{-\int_{s=0}^{t}\beta_{s}ds}\right)K\). Furthermore, \((\mathbf{Y}_{t}(x))_{t\in\mathbb{R}_{+},x\in\mathcal{X}}\) is the solution of the SDE in (8)._

Proof.: We aim to compute the mean and covariance of the process \((\mathbf{Y}_{t})_{t\geq 0}\) described by the SDE (3). First let's recall the time evolution of the mean and covariance of the solution from a multivariate Ornstein-Uhlenbeck process given by

\[\mathrm{d}\mathbf{Y}_{t}=f(\mathbf{Y}_{t},t)\mathrm{d}t+L(\mathbf{Y}_{t},t) \mathrm{d}\mathbf{B}_{t}.\] (9)

We know that the time evolution of the mean and the covariance are given respectively by Sarkka and Solin (2019)

\[\frac{\mathrm{d}m_{t}}{\mathrm{d}t} =\mathrm{E}[f(\mathbf{Y}_{t},t)]\] (10) \[\frac{\mathrm{d}\Sigma_{t}}{\mathrm{d}t} =\mathrm{E}[f(\mathbf{Y}_{t},t)(m_{t}-\mathbf{Y}_{t})^{\top}]+ \mathrm{E}[(m_{t}-\mathbf{Y}_{t})f(\mathbf{Y}_{t},t)^{\top}]+\mathrm{E}[L( \mathbf{Y}_{t},t)L(\mathbf{Y}_{t},t)^{\top}].\] (11)

Plugging in the drift \(f(\mathbf{Y}_{t},t)=1/2\cdot(m-\mathbf{Y}_{t})\beta_{t}\) and diffusion term \(L(\mathbf{Y}_{t},t)=\sqrt{\beta_{t}K}\) from (3), we get

\[\frac{\mathrm{d}m_{t}}{\mathrm{d}t} =1/2\cdot(m-\mathbf{Y}_{t})\beta_{t}\] (12) \[\frac{\mathrm{d}\Sigma_{t}}{\mathrm{d}t} =\beta_{t}\left[K-\Sigma_{t}\right].\] (13)Solving these two ODEs we get

\[m_{t}=e^{-\frac{1}{2}\cdot\int_{s=0}^{t}\beta_{s}ds}m_{0}+\left(1-e^{-\frac{1}{2} \cdot\int_{s=0}^{t}\beta_{s}ds}\right)m\] (14)

\[\Sigma_{t}=K+e^{-\int_{s=0}^{t}\beta_{s}ds}\left(\Sigma_{0}-K\right)\] (15)

with \(m_{0}\triangleq\mathrm{E}[\mathbf{Y}_{0}]\) and \(\Sigma_{0}\triangleq\mathrm{Cov}[\mathbf{Y}_{0}]\).

Now let's compute the first two moments of \((\mathbf{Y}_{t}(x))_{x\in\mathcal{X}}\). We have

\[\mathrm{E}[\mathbf{Y}_{t}] =\mathrm{E}\left[e^{-\frac{1}{2}\cdot\int_{s=0}^{t}\beta_{s}ds} \mathbf{Y}_{0}+\left(1-e^{-\frac{1}{2}\cdot\int_{s=0}^{t}\beta_{s}ds}\right)m +\left(1-e^{-\frac{1}{2}\cdot\int_{s=0}^{t}\beta_{s}ds}\right)\mathbf{G}\right]\] (16) \[=e^{-\frac{1}{2}\cdot\int_{s=0}^{t}\beta_{s}ds}m_{0}+\left(1-e^{ -\frac{1}{2}\cdot\int_{s=0}^{t}\beta_{s}ds}\right)m\] (17) \[=m_{t}\] (18) \[\mathrm{Cov}[\mathbf{Y}_{t}] =\mathrm{Cov}\left[e^{-\frac{1}{2}\cdot\int_{s=0}^{t}\beta_{s}ds} \mathbf{Y}_{0}\right]+\mathrm{Cov}\left[\left(1-e^{-\int_{s=0}^{t}\beta_{s}ds }\right)^{1/2}\mathbf{G}\right]\] (19) \[=e^{-\int_{s=0}^{t}\beta_{s}ds}\;\Sigma_{0}+\left(1-e^{-\int_{s=0 }^{t}\beta_{s}ds}\right)K\] (20) \[=K+e^{-\int_{s=0}^{t}\beta_{s}ds}\left(\Sigma_{0}-K\right)\] (21) \[=\Sigma_{t}\;.\] (22)

### Conditional score

Hence, condition on \(\mathbf{Y}_{0}\) the score is the gradient of the log Gaussian characterised by mean \(m_{t|0}=e^{-\frac{1}{2}B(t)}\mathbf{Y}_{0}\) and \(\Sigma_{t|0}=(1-e^{-B(t)})\mathrm{K}\) with \(B(t)=\int_{0}^{t}\beta(s)ds\) which can be derived from the above marginal mean and covariance with \(m_{0}=\mathbf{Y}_{0}\) and \(\Sigma_{0}=0\).

\[\nabla_{\mathbf{Y}_{t}}\log p_{t}(\mathbf{Y}_{t}|\mathbf{Y}_{0}) =\nabla_{\mathbf{Y}_{t}}\log\mathcal{N}\left(\mathbf{Y}_{t}|m_{t| 0},\Sigma_{t|0}\right)\] (23) \[=\nabla_{\mathbf{Y}_{t}}-1/2(\mathbf{Y}_{t}-m_{t|0})^{\top} \Sigma_{t|0}^{-1}(\mathbf{Y}_{t}-m_{t|0})+c\] (24) \[=-\Sigma_{t|0}^{-1}(\mathbf{Y}_{t}-m_{t|0})\] (25) \[=-\mathrm{L}_{t|0}^{-1}\mathrm{L}_{t|0}^{-1}\mathrm{L}_{t|0}\epsilon\] (26) \[=-\mathrm{L}_{t|0}^{-\top}\epsilon\] (27)

where \(\mathrm{L}_{t|0}\) denotes the Cholesky decomposition of \(\Sigma_{t|0}=\mathrm{L}_{t|0}\mathrm{L}_{t|0}^{\top}\), and \(\mathbf{Y}_{t}=m_{t|0}+\mathrm{L}_{t|0}\epsilon\).

Then we can plugin our learnt (preconditioned) score into the backward SDE 4 which gives

\[\mathrm{d}\bar{\mathbf{Y}}_{t}|x=\left[-(m(x)-\bar{\mathbf{Y}}_{t})/2+\mathrm{ K}(x,x)\nabla_{\mathbf{Y}_{t}}\log p_{T-t}(t,x,\bar{\mathbf{Y}}_{t})\right] \mathrm{d}t+\sqrt{\beta_{t}\mathrm{K}(x,x)}\beta_{t}\mathrm{d}\mathbf{B}_{t}\] (28)

### Several score parametrisations

In this section, we discuss several parametrisations of the neural network and the objective.

For the sake of versatility, we opt to employ the symbol \(D_{\theta}\) for the network instead of \(s_{\theta}\) as mentioned in the primary text, as it allows us to approximate not only the score but also other quantities from which the score can be derived. In full generality, we use a residual connection, weighted by \(c_{\text{out}},c_{\text{skip}}:\mathbb{R}\rightarrow\mathbb{R}\), to parameterise the network

\[D_{\theta}(t,\mathbf{Y}_{t})=c_{\text{skip}}(t)\mathbf{Y}_{t}+c_{\text{out}}(t) F_{\theta}(t,\mathbf{Y}_{t}).\] (29)

We recall that the input to the network is time \(t\), and the noised vector \(\mathbf{Y}_{t}=\boldsymbol{\mu}_{t|0}+\boldsymbol{n}\), where \(\boldsymbol{\mu}_{t|0}=\mathbf{e}^{-B(t)/2}\mathbf{Y}_{0}\) and \(\boldsymbol{n}\sim\mathcal{N}(0,\Sigma_{t|0})\) with \(\Sigma_{t|0}=(1-\mathbf{e}^{-B(t)})K\). The gram matrix \(K\) corresponds to \(k(X,X)\) with \(k\) the limiting kernel. We denote by \(\mathrm{L}_{t|0}\) and \(\mathrm{S}\) respectively the Cholesky decomposition of \(\Sigma_{t|0}=\mathrm{L}_{t|0}\mathrm{L}_{t|0}^{\top}\) and \(\mathrm{K}=\mathrm{SS}^{\top}\).

The denoising score matching loss weighted by \(\Lambda(t)\) is given by

\[\mathcal{L}(\theta)=\mathbb{E}\left[\|D_{\theta}(t,\mathbf{Y}_{t})-\nabla_{ \mathbf{Y}_{t}}\log p_{t}(\mathbf{Y}_{t}|\mathbf{Y}_{0})\|_{\Lambda(t)}^{2}\right]\] (30)No preconditioningBy reparametrisation, let \(\bm{Y}_{t}=\bm{\mu}_{t|0}+L_{t|0}\bm{z}\), where \(\bm{z}\sim\mathcal{N}(\mathbf{0},\mathbf{I})\), the loss from Eq. (30) can be written as

\[\mathcal{L}(\theta) =\mathbb{E}\left[\|D_{\theta}(t,\bm{Y}_{t})+\Sigma_{t|0}^{-1}(\bm {Y}_{t}-\bm{\mu}_{t|0})\|_{\Lambda(t)}^{2}\right]\] (31) \[=\mathbb{E}\left[\|D_{\theta}(t,\bm{Y}_{t})+\Sigma_{t|0}^{-1}L_{ t|0}\bm{z}\|_{\Lambda(t)}^{2}\right]\] (32) \[=\mathbb{E}\left[\|D_{\theta}(t,\bm{Y}_{t})+L_{t|0}^{-\top}\bm{z} \|_{\Lambda(t)}^{2}\right]\] (33)

Choosing \(\Lambda(t)=\Sigma_{t|0}=L_{t|0}L_{t|0}^{\top}\) we obtain

\[\mathcal{L}(\theta) =\mathbb{E}\left[\|L_{t|0}^{\top}D_{\theta}(t,\bm{Y}_{t})+\bm{z} \|_{2}^{2}\right]\] (35) \[=\mathbb{E}\left[\|\sigma_{t|0}S^{\top}D_{\theta}(t,\bm{Y}_{t})+ \bm{z}\|_{2}^{2}\right].\] (36)

Preconditioning by \(K\)Alternatively, one can train the neural network to approximate the preconditioned score \(D_{\theta}\approx\mathbf{K}\nabla_{\bm{Y}_{t}}\log p_{t}(\bm{Y}_{t}|\bm{Y}_{0})\). The loss, weighted by \(\Lambda=\sigma_{t|0}^{2}\mathbf{I}\), is then given by

\[\mathcal{L}(\theta) =\mathbb{E}\left[\|D_{\theta}(t,\bm{Y}_{t})+K\,L_{t|0}^{-\top}\bm {z}\|_{\Lambda(t)}^{2}\right]\] (37) \[=\mathbb{E}\left[\|D_{\theta}(t,\bm{Y}_{t})+\sigma_{t|0}^{-1}S \bm{z}\|_{\Lambda(t)}^{2}\right]\] (38) \[=\mathbb{E}\left[\|\sigma_{t|0}D_{\theta}(t,\bm{Y}_{t})+S\bm{z} \|_{2}^{2}\right].\] (39)

Precondition by \(S^{\top}\)A variation of the previous one, is to precondition the score by the transpose Cholesky of the limiting kernel gram matrix, such that \(D_{\theta}\approx S^{\top}\nabla_{\bm{Y}_{t}}\log p_{t}(\bm{Y}_{t}|\bm{Y}_{0})\).

The loss, weighted by \(\Lambda=\sigma_{t|0}^{2}\mathbf{I}\), becomes

\[\mathcal{L}(\theta) =\mathbb{E}\left[\|D_{\theta}(t,\bm{Y}_{t})+S^{\top}\,L_{t|0}^{- \top}\bm{z}\|_{\Lambda(t)}^{2}\right]\] (40) \[=\mathbb{E}\left[\|D_{\theta}(t,\bm{Y}_{t})+\sigma_{t|0}^{-1}\bm {z}\|_{\Lambda(t)}^{2}\right]\] (41) \[=\mathbb{E}\left[\|\sigma_{t|0}D_{\theta}(t,\bm{Y}_{t})+\bm{z}\|_ {2}^{2}\right].\] (42)

Predicting \(\bm{Y}_{0}\)Finally, an alternative strategy is to predict \(\bm{Y}_{0}\) from a noised version \(\bm{Y}_{t}\). In this case, the loss takes the simple form

\[\mathcal{L}(\theta)=\mathbb{E}\left[\|D_{\theta}(t,\bm{Y}_{t})-\bm{Y}_{0}\|_{ 2}^{2}\right].\]

The score can be computed from the network's prediction following

\[\nabla\log p_{t}(\bm{Y}_{t}|\bm{Y}_{0}) =-\Sigma_{t|0}^{-1}(\bm{Y}_{t}-\bm{\mu}_{t|0})\] (43) \[=-\Sigma_{t|0}^{-1}(\bm{Y}_{t}-\text{e}^{-B(t)/2}\bm{Y}_{0})\] (44) \[\approx-\Sigma_{t|0}^{-1}\left(\bm{Y}_{t}-\text{e}^{-B(t)/2}D_{ \theta}(t,\bm{Y}_{t})\right)\] (45)

Table 3 summarises the different options for parametrising the score as well as the values for \(c_{\text{skip}}\) and \(c_{\text{out}}\) that we found to be optimal, based on the recommendation from Karras et al. (2022, Appendix B.6). In practice, we found the precondition by \(K\) parametrisation to produce the best results, but we refer to App. F.1.3 for a more in-depth ablation study.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline  & No precond. & Precond. \(K\) & Precond. \(S^{\top}\) & Predict \(\bm{Y}_{0}\) \\ \hline \(c_{\text{skip}}\) & 0 & 0 & 0 & 1 \\ \(c_{\text{out}}\) & \((\sigma_{t|0}+10^{-3})^{-1}\) & \((\sigma_{t|0}+10^{-3})^{-1}\) & \((\sigma_{t|0}+10^{-3})^{-1}\) & 1 \\ Loss & \(\|\sigma_{t|0}S^{\top}D_{\theta}+\bm{z}\|_{2}^{2}\) & \(\|\sigma_{t|0}D_{\theta}+S\bm{z}\|_{2}^{2}\) & \(\|\sigma_{t|0}D_{\theta}+\bm{z}\|_{2}^{2}\) & \(\|D_{\theta}-\bm{Y}_{0}\|_{2}^{2}\) \\ \(K\nabla\log p_{t}\) & \(KD_{\theta}\) & \(D_{\theta}\) & \(SD_{\theta}\) & \(-\Sigma_{t|0}^{-1}(\bm{Y}_{t}-\text{e}^{-\frac{B(t)}{2}}D_{\theta})\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Summary of different score parametrisations as well as the values for \(c_{\text{skip}}\) and \(c_{\text{out}}\) that we found to be optimal, based on the recommendation from Karras et al. (2022, Appendix B.6).

### Exact (marginal) score in Gaussian setting

Interpolating between Gaussian processes \(GP(m_{0},\Sigma_{0})\) and \(GP(m,\mathrm{K})\)

\[\mathrm{K}\nabla_{\bar{\mathbf{Y}}_{t}}\log p_{t}(\mathbf{Y}_{t}) =-\mathrm{K}\Sigma_{t}^{-1}(\mathbf{Y}_{t}-m_{t})\] (47) \[=-\mathrm{K}[\mathrm{K}+e^{-\int_{s=0}^{t}\beta_{s}ds}\left( \Sigma_{0}-\mathrm{K}\right)]^{-1}(\mathbf{Y}_{t}-m_{t})\] (48) \[=-\mathrm{K}(\mathrm{L}_{t}{L_{t}}^{\top})^{-1}(\mathbf{Y}_{t}-m _{t})\] (49) \[=-\mathrm{KL}_{t}{}^{\top}{}^{-1}\mathrm{L}_{t}{}^{-1}(\mathbf{Y }_{t}-m_{t})\] (50)

with \(\Sigma_{t}=\mathrm{K}+e^{-\int_{s=0}^{t}\beta_{s}ds}\left(\Sigma_{0}-\mathrm{ K}\right)=\mathrm{L}_{t}{\mathrm{L}_{t}}^{\top}\) obtained via Cholesky decomposition.

### Langevin dynamics

Under mild assumptions on \(\nabla\log p_{T-t}\)(Durmus and Moulines, 2016) the following SDE

\[\mathrm{d}\mathbf{Y}_{s}=\tfrac{1}{2}\mathrm{K}\nabla\log p_{T-t}(\mathbf{Y}_ {s})\mathrm{d}s+\sqrt{\mathrm{K}}\mathrm{d}\mathbf{B}_{s}\] (52)

admits a solution \((\mathbf{Y}_{s})_{s\geq 0}\) whose law \(\mathcal{L}(\mathbf{Y}_{s})\) converges with geometric rate to \(p_{T-t}\) for any invertible matrix \(\mathrm{K}\).

### Likelihood evaluation

Similarly to Song et al. (2021), we can derive a deterministic process which has the same marginal density as the SDE (3), which is given by the following Ordinary Differential Equation (ODE)--referred as the probability flow ODE

\[\mathrm{d}\begin{pmatrix}\mathbf{Y}_{t}(x)\\ \log p_{t}(\mathbf{Y}_{t}(x))\end{pmatrix}=\begin{pmatrix}\tfrac{1}{2}\left\{ m(x)-\mathbf{Y}_{t}(x)-\mathrm{K}(x,x)\nabla\log p_{t}(\mathbf{Y}_{t}(x))\right\} \beta_{t}\\ -\tfrac{1}{2}\mathrm{div}\left\{m(x)-\mathbf{Y}_{t}(x)-\mathrm{K}(x,x)\nabla \log p_{t}(\mathbf{Y}_{t}(x))\right\}\beta_{t}\end{pmatrix}\mathrm{d}t.\] (53)

Once the score network is learnt, we can thus use it in conjunction with an ODE solver to compute the likelihood of the model.

### Discussion consistency

So far we have defined a generative model over functions via its finite marginals \(\bar{\mathbf{Y}}_{T}^{\theta}(x)\). These finite marginals were to arise from a stochastic process if, as per the Kolmogorov extension theorem (Oksendal, 2003), they satisfy _exchangeability_ and _consistency_ conditions. Exchangeability can be satisfied by parametrising the score network such that the score network is equivariant w.r.t permutation, i.e. \(\mathbf{s}_{\theta}(t,\sigma\circ x,\sigma\circ y)=\sigma\circ\mathbf{s}_{ \theta}(t,x,y)\) for any \(\sigma\in\Sigma_{n}\). Additionally, we have that the noising process \((\mathbf{Y}_{t}(x))_{x\in\mathcal{X}}\) is trivially consistent for any \(t\in\mathbb{R}_{+}\) since it is a stochastic process as per Prop. B.1, and consequently so is the (true) time-reversal \((\bar{\mathbf{Y}}_{t}(x))_{x\in\mathcal{X}}\). Yet, when approximating the score \(\mathbf{s}_{\theta}\approx\nabla\log p_{t}\), we lose the consistency over the generative process \(\bar{\mathbf{Y}}_{t}^{\theta}(x)\) as the constraint on the score network is non trivial to satisfy. This is actually a really strong constraint on the model class, and as soon as one goes beyond linearity (of the posterior w.r.t. the context set), it is non trivial to enforce without directly parameterising a stochastic process, e.g. as Phillips et al. (2022). There thus seems to be a strong trade-off between satisfying consistency, and the model's ability to fit complex process and scale to large datasets.

## Appendix C Manifold-valued diffusion process

### Manifold-valued inputs

In the main text we dealt with a simplified case of tensor fields where the tensor fields are over Euclidean space. Nevertheless, it is certainly possible to apply our methods to these settings. Significant work has been done on performing convolutions on feature fields on generic manifolds (a superset of tensor fields on generic manifolds), core references being (Cohen, 2021) for the case of homogeneous spaces and (Weiler et al., 2021) for more general Riemannian manifolds. Werecommend these as excellent mathematical introductions to the topic and build on them to describe how to formulate diffusion models over these spaces.

**Tensor fields as sections of bundles.** Formally the fields we are interested in modelling are sections \(\sigma\) of associated tensor bundles of the principle \(G\)-bundle on a manifold \(M\). We shall denote such a bundle \(BM\) and the space of sections \(\Gamma(BM)\). The goal, therefore, is to model _random elements_ from this space of sections. For a clear understanding of this definition, please see Weiler et al. (2021, pages 73-95) for an introduction suitable to ML audiences. Prior work looking at this setting is (Hutchinson et al., 2021) where they construct Gaussian Processes over tensor fields on manifolds.

**Stochastic processes on spaces of sections.** Given we can see sections as maps \(\sigma:M\to BM\), where an element in \(BM\) is a tuple \((m,b)\), \(m\) in the base manifold and \(b\) in the typical fibre, alongside the condition that the composition of the projection \(\operatorname{proj}_{i}:(m,b)\mapsto m\) with the section is the identity, \(\operatorname{proj}_{i}\circ\sigma=\operatorname{Id}\) it is clear we can see distribution over sections as stochastic processes with index set the manifold \(M\), and output space a point in the bundle \(BM\), with the projection condition satisfied. The projection onto finite marginals, i.e. a finite set of points in the manifold, is defined as \(\pi_{m_{1},...,m_{n}}(\sigma)=(\sigma(m_{1}),...,\sigma(m_{n}))\).

**Noising process.** To define a noising process over these marginals, we can use Gaussian Processes defined in (Hutchinson et al., 2021) over the tensor fields. The convergence results of Phillips et al. (2022) hold still, and so using these Gaussian Processes as noising processes on the marginals also defines a noising process on the whole section.

**Reverse process.** The results of (Cattiaux et al., 2021) are extremely general and continue to hold in this case of SDEs on the space of sections. Note we don't actually need this to be the case, we can just work with the reverse process on the marginals themselves, which are much simpler objects. It is good to know that it is a valid process on full sections though should one want to try and parameterise a score function on the whole section akin to some other infinite-dimension diffusion models.

**Score function.** The last thing to do therefore is parameterise the score function on the marginals. If we were trying to parameterise the score function over the _whole_ section at once (akin to a number of other works on infinite dimension diffusions), this could present some problems in enforcing the smoothness of the score function. As we only deal with the score function on a finite set of marginals, however, we need not deal with this issue and this presents a distinct advantage in simplicity for our approach. All we need to do is pick a way of numerically representing points on the manifold and b) pick a basis for the tangent space of each point on the manifold. This lets us represent elements from the tangent space numerically, and therefore also elements from tensor space at each point numerically as well. This done, we can feed these to a neural network to learn to output a numerical representation of the score on the same basis at each point.

### Manifold-valued outputs

In the setting, where one aim to model a stochastic process with manifold codomain \(\mathbf{Y}_{t}(x)=(\mathbf{Y}_{t}(x_{1}),\cdots,\mathbf{Y}_{t}(x_{n}))\in \mathcal{M}^{n}\), things are less trivial as manifolds do not have a vector space structure which is necessary to define Gaussian processes. Fortunately, We can still target a know distribution marginally independently on each marginal, since this is well defined, and as such revert to the Riemannian diffusion models introduced in De Bortoli et al. (2021) with \(n\) independent Langevin noising processes

\[\operatorname{d}\!\mathbf{Y}_{t}(x_{k})=-\tfrac{1}{2}\nabla U(\mathbf{Y}_{t} (x_{k}))\;\beta_{t}\mathrm{d}t+\sqrt{\beta_{t}}\mathrm{d}\mathbf{B}_{t}^{ \mathcal{M}}\;.\] (54)

are applied to each marginal. Hence in the limit \(t\to\infty\), \(\mathbf{Y}_{t}(x)\) has density (assuming it exists) which factors as \(dp/d\mathrm{Vol}_{\mathcal{M}}((y(x_{1}),\cdots,y(x_{n})))\propto\prod_{k}e^{- U(y(x_{n})))}\). For compact manifolds, we can target the uniform distribution by setting \(U(x)=0\). The reverse time process will have correlation between different marginals, and so the score function still needs to be a function of all the points in the marginal of interest.

## Appendix D Invariant neural diffusion processes

### \(\mathrm{E}(n)\)-equivariant kernels

A kernel \(k:\mathbb{R}^{d}\times\mathbb{R}^{d}\to\mathbb{R}^{d\times d}\) is equivariant if it satisfies the following constraints: (a) \(k\) is _stationnary_, that is if for all \(x,x^{\prime}\in\mathbb{R}^{n}\)\[k(x,x^{\prime})=k(x-x^{\prime})\triangleq\bar{k}(x-x^{\prime})\] (55)

and if (b) it satisfies the _angular constraint_ for any \(h\in H\)

\[k(hx,hx^{\prime})=\rho(h)k(x,x^{\prime})\rho(h)^{\top}.\] (56)

A trivial example of such an equivariant kernel is the diagonal kernel \(k(x,x^{\prime})=k_{0}(x,x^{\prime})\)I (Holderrieth et al., 2021), with \(k_{0}\) stationnary. This kernel can be understood has having \(d\) independent Gaussian process uni-dimensional output, that is, there is no inter-dimensional correlation.

Less trivial examples, are the \(\mathrm{E}(n)\) equivariant kernels proposed in Macedo and Castro (2010). Namely curl-free and divergence-free kernels, allowing for instance to model electric or magnetic fields. Formally we have \(k_{\mathrm{curl}}=k_{0}\mathrm{A}\) and \(k_{\mathrm{div}}=k_{0}\mathrm{B}\) with \(k_{0}\) stationary, e.g. squared exponential kernel \(k_{0}(x,x^{\prime})=\sigma^{2}\exp\left(\frac{\|x-x^{\prime}\|^{2}}{2l^{2}}\right)\), and \(A\) and \(B\) given by

\[A(x,x^{\prime})=\mathrm{I}-\frac{(x-x^{\prime})(x-x^{\prime})^{\top}}{l^{2}}\] (57)

\[B(x,x^{\prime})=\frac{(x-x^{\prime})(x-x^{\prime})^{\top}}{l^{2}}+\left(n-1- \frac{\|x-x^{\prime}\|^{2}}{l^{2}}\right)\mathrm{I}.\] (58)

See Holderrieth et al. (Appendix C, 2021) for a proof.

### Proof of Prop. 3.2

Below we give two proofs for the group invariance of the generative process, one via the probability flow ODE and one directly via Fokker-Planck.

Proof.: Reverse ODE. The reverse probability flow associated with the forward SDE (3) with approximate score \(\mathbf{s}_{\theta}(t,\cdot)\approx\nabla\log p_{t}\) is given by

\[\mathrm{d}\bar{\mathbf{Y}}_{t}|x =\tfrac{1}{2}\left[-m(x)+\bar{\mathbf{Y}}_{t}+\mathrm{K}(x,x) \mathbf{s}_{\theta}(T-t,x,\bar{\mathbf{Y}}_{t})\right]\mathrm{d}t\] (59) \[\triangleq b_{\text{ODE}}(t,x,\bar{\mathbf{Y}}_{t})\mathrm{d}t\] (60)

This ODE induces a flow \(\phi_{t}^{b}:X^{n}\times Y^{n}\to\mathrm{T}Y^{n}\) for a given integration time \(t\), which is said to be \(G\)-equivariant if the vector field is \(G-\)equivariant itself, i.e. \(b(t,g\cdot x,\rho(g)\bar{\mathbf{Y}}_{t})=\rho(g)b(t,x,\bar{\mathbf{Y}}_{t})\). We have that for any \(g\in G\)

\[b_{\text{ODE}}(t,g\cdot x,\rho(g)\bar{\mathbf{Y}}_{t}) =\tfrac{1}{2}\left[-m(g\cdot x)+\rho(g)\mathbf{Y}_{t}+\mathrm{K} (g\cdot x,g\cdot x)\;\mathbf{s}_{\theta}(t,g\cdot x,\rho(g)\bar{\mathbf{Y}}_{t })\right]\] (61) \[\stackrel{{(1)}}{{=}}\tfrac{1}{2}\left[-\rho(g)m(x)+ \rho(g)\mathbf{Y}_{t}+\rho(g)\mathrm{K}(x,x)\rho(g)^{\top}\;\mathbf{s}_{ \theta}(t,g\cdot x,\rho(g)\bar{\mathbf{Y}}_{t})\right]\] (62) \[\stackrel{{(2)}}{{=}}\tfrac{1}{2}\left[-\rho(g)m(x)+ \rho(g)\mathbf{Y}_{t}+\rho(g)\mathrm{K}(x,x)\rho(g)^{\top}\rho(g)\;\mathbf{s }_{\theta}(t,x,\bar{\mathbf{Y}}_{t})\right]\] (63) \[\stackrel{{(3)}}{{=}}\tfrac{1}{2}\rho(g)\left[-m(x)+ \mathbf{Y}_{t}+\mathrm{K}(x,x)\;\mathbf{s}_{\theta}(t,x,\bar{\mathbf{Y}}_{t})\right]\] (64) \[=\rho(g)b_{\text{ODE}}(t,x,\bar{\mathbf{Y}}_{t})\] (65)

with (1) from the \(G\)-invariant prior GP conditions on \(m\) and \(k\), (2) assuming that the score network is \(G\)-equivariant and (3) assuming that \(\rho(g)\in O(n)\). To prove the opposite direction, we can simply follow these computations backwards. Finally, we know that with a \(G\)-invariant probability measure \(p_{\text{ref}}\) and \(G\)-equivariant map \(\phi\), the pushforward probability measure \(p_{\text{ref}}^{-1}\circ\phi\) is also \(G\)-invariant (Kohler et al., 2020; Papamakarios et al., 2019). Assuming a \(G\)-invariant prior GP, and a \(G\)-equivariant score network, we thus have that the generative model from Sec. 3 defines marginals that are \(G\)-invariant. 

Proof.: Reverse SDE. The reverse SDE associated of the forward SDE (3) with approximate score \(\mathbf{s}_{\theta}(t,\cdot)\approx\nabla\log p_{t}\) is given by

\[\mathrm{d}\bar{\mathbf{Y}}_{t}|x =\left[-(m(x)-\bar{\mathbf{Y}}_{t})/2+\mathrm{K}(x,x)\mathbf{s}_{ \theta}(T-t,x,\bar{\mathbf{Y}}_{t})\right]\mathrm{d}t+\sqrt{\beta_{t}\mathrm{K }(x,x)}\mathrm{d}\mathbf{B}_{t}\] (66) \[\triangleq b_{\text{SDE}}(t,x,\bar{\mathbf{Y}}_{t})\mathrm{d}t+\Sigma^{1/2} (t,x)\;\mathrm{d}\mathbf{B}_{t}.\] (67)As for the probability flow drift \(b_{\text{ODE}}\), we have that \(b_{\text{SDE}}\) is similarly \(G\)-equivariant, that is \(b_{\text{SDE}}(t,g\cdot x,\rho(g)\bar{\mathbf{Y}}_{t})=\rho(g)b_{\text{SDE}}(t,x,\bar{\mathbf{Y}}_{t})\) for any \(g\in G\). Additionally, we have that diffusion matrix is also \(G\)-equivariant as for any \(g\in G\) we have \(\Sigma(t,g\cdot x)=\beta_{t}\mathrm{K}(g\cdot x,g\cdot x)=\beta_{t}\rho(g) \mathrm{K}(x,x)\rho(g)^{\top}=\rho(g)\Sigma(t,x)\rho(g)^{\top}\) since \(\mathrm{K}\) is the gram matrix of an \(G\)-equivariant kernel \(k\).

Additionally assuming that \(b_{\text{SDE}}\) and \(\Sigma\) are bounded, Yim et al. (Proposition 3.6, 2023) says that the distribution of \(\bar{\mathbf{Y}}_{t}\) is \(G\)-invariant, and in in particular \(\mathcal{L}(\bar{\mathbf{Y}}_{0})\).

### Equivariant posterior maps

**Theorem D.1** (Invariant prior stochastic process implies an equivariant posterior map).: _Using the language of Weiler et al. (2021) our tensor fields are sections of an associated vector bundle \(\mathcal{A}\) of a manifold \(M\) with a \(G\) structure. Let \(\mathrm{Isom}_{GM}\) be the group of \(G\)-structure preserving isometries on \(M\). The action of this group on a section of the bundle \(f\in\Gamma(\mathcal{A})\) is given by_

\[\phi\triangleright f:=\phi_{*,\mathcal{A}}\circ f\circ\phi^{-1}\]

_(Weiler et al., 2021). Let \(f\sim P\), \(P\) a distribution over the space of section. Let \(\phi\triangleright P\) be the law of of \(\phi\triangleright f\). Let \(\mu_{x}=\mathcal{L}(f(x))=\pi_{x\#}P\), the law of \(f\) evaluated at a point, where \(\pi_{x}\) is the canonical projection operator onto the marginal at \(x\), \(\#\) the pushforward operator in the measure theory sense, \(x\in M\) and \(y\) is in the fibre of the associated bundle. Let \(\mu_{x}^{x^{\prime},y^{\prime}}=\mathcal{L}(f(x)|f(x^{\prime})=y^{\prime})= \pi_{x}\mu^{x^{\prime},y^{\prime}}=\pi_{x\#}\mathcal{L}(f|f(x^{\prime})=y^{ \prime})\), the conditional law of the process when given \(f(x^{\prime})=y^{\prime}\)._

_Assume that the prior is invariant under the action of \(\mathrm{Isom}_{GM}\), i.e. that_

\[\phi\triangleright\mu_{x}=(\phi_{*,\mathcal{A}})_{\#}\mu_{\phi^{-1}(x)}=\mu_{x}\]

_Then the conditional measures are equivariant, in the sense that_

\[\phi\triangleright\mu_{x}^{x^{\prime},y^{\prime}}=(\phi_{*,\mathcal{A}})_{\#} \mu_{\phi^{-1}(x)}^{x^{\prime},y^{\prime}}=\mu_{x}^{\phi^{-1}(x),\phi_{*, \mathcal{A}}(y)}=\mu_{x}^{\phi(x^{\prime},y^{\prime})}\]

Proof.: \(\forall A,B\) test functions, \(\phi\in\mathrm{Isom}_{GM}\),

\[\mathrm{E}[B(f(x^{\prime}))A((\phi\triangleright f)(x))] =\mathrm{E}\big{[}B(f(x^{\prime}))A\big{(}\phi_{*,\mathcal{A}} \circ f\circ\phi^{-1}(x)\big{)}\big{]}\] \[=\mathrm{E}\Big{[}B(f(x^{\prime}))\mathrm{E}\big{[}A\big{(}\phi_{ *,\mathcal{A}}\big{(}F(\phi^{-1}(x))\big{)}\big{)}\big{]}\,\big{|}\,F(x^{ \prime})\big{]}\big{]}\] \[=\mathrm{E}\bigg{[}B(f(x^{\prime}))\int A(y)(\phi_{*,\mathcal{A} })_{\#}\mu_{\phi^{-1}(x)}^{x^{\prime},f(x^{\prime})}(\mathrm{d}y)\bigg{]}\] \[=\int B(y^{\prime})\int A(y)(\phi_{*,\mathcal{A}})_{\#}\mu_{\phi^ {-1}(x)}^{x^{\prime},f(x^{\prime})}(\mathrm{d}y)\mu_{x^{\prime}}(\mathrm{d}y^ {\prime})\] \[=\int B(y^{\prime})\int A(y)\Big{(}\phi\triangleright\mu_{x}^{x^{ \prime},f(x^{\prime})}\Big{)}(\mathrm{d}y)\mu_{x^{\prime}}(\mathrm{d}y^{ \prime})\]

By invariance this quantity is also equal to

\[\mathrm{E}\big{[}B\big{(}(\phi^{-1}\triangleright f)(x^{\prime}) \big{)}A((\phi^{-1}\triangleright\phi\triangleright f)(x))\big{]} =\mathrm{E}\big{[}B\big{(}(\phi^{-1}\triangleright f)(x^{\prime}) \big{)}\mathrm{E}\big{[}A(f(x))\big{]}\,\big{|}\,B\big{(}(\phi^{-1} \triangleright f)(x^{\prime})\big{)}\big{]}\] \[=\mathrm{E}\Big{[}B\big{(}\phi_{*,\mathcal{A}}(f(\phi^{-1}(x^{ \prime})))\big{)}\big{[}A(F(x))\big{]}\,\big{|}\,\phi_{*,\mathcal{A}}(f(\phi^{- 1}(x^{\prime})))\big{]}\big{]}\] \[=\mathrm{E}\bigg{[}B\Big{(}\tau_{x^{\prime},g}^{-1}F(gx^{\prime}) \Big{)}\int A(y)\mu_{x}^{\phi(x^{\prime}),\phi_{*,\mathcal{A}}^{-1}(y)}\bigg{]} (\mathrm{d}y)\] \[=\int B(y^{\prime})\int A(y)\mu_{x}^{\phi\triangleright(x^{\prime},y) }(\mathrm{d}y)\Big{(}\phi^{-1}\triangleright\mu_{x^{\prime}})(\mathrm{d}y^{ \prime})\]

Hence

\[\Big{(}\phi\triangleright\mu_{x}^{x^{\prime},f(x^{\prime})}\Big{)}(\mathrm{d}y) \mu_{x^{\prime}}(\mathrm{d}y^{\prime})=\mu_{x}^{\phi\triangleright(x^{\prime},y) }(\mathrm{d}y)\big{(}\phi^{-1}\triangleright\mu_{x^{\prime}}\big{)}(\mathrm{d}y^ {\prime})\]By the stated invariance \(\phi^{-1}\triangleright\mu_{x^{\prime}}=\mu_{x^{\prime}}\), hence

\[\left(\phi\triangleright\mu_{x}^{x^{\prime},f(x^{\prime})}\right)(\mathrm{d}y)= \mu_{x}^{\phi\circ(x^{\prime},y)}(\mathrm{d}y)\text{ a.e. }y^{\prime}\]

So

\[\phi\triangleright\mu_{x}^{x^{\prime},f(x^{\prime})}=\mu_{x}^{\phi\circ(x^{ \prime},y)}\] (68)

as desired. 

## Appendix E Langevin corrector and the iterative procedure of RePaint (Lugmayr et al., 2022)

### Langevin sampling scheme

Several previous schemes exist for conditional sampling from Diffusion models. Two different types of conditional sampling exist. Those that try to sample conditional on some part of the state space over which the diffusion model has been trained, such as in-painting or extrapolation tasks, and those that post-hoc attempt to condition on something outside the state space that the model has been trained on.

This first category is the one we are interested in, and in it we have:

* Replacement sampling (Song et al., 2021), where the reverse ODE or SDE is evolved but by fixing the conditioning data during the rollout. This method does produce visually coherent sampling in some cases, but is not an exact conditional sampling method.
* SMC-based methods (Trippe et al., 2022), which are an exact method up to the particle filter assumption. These can produce good results but can suffer from the usual SMC methods downsides on highly multi-model data such as particle diversity collapse.
* The RePaint scheme of (Lugmayr et al., 2022). While not originally proposed as an exact sampling scheme, we will show later that it can in fact be shown that this method is doing a specific instantiation of our newly proposed method, and is therefore exact.
* Amortisation methods, e.g. Phillips et al. (2022). While they can be effective, these methods can never perform exact conditional sampling, by definition.

Our goal is to produce an exact sampling scheme that does not rely on SMC-based methods. Instead, we base our method on Langevin dynamics. If we have a score function trained over the state space \(\bm{x}=[\bm{x}^{c},\bm{x}^{*}]\), where \(\bm{x}^{c}\) are the points we wish to condition on and \(\bm{x}_{s}\) points we wish to sample, we exploit the following score breakdown:

\[\nabla_{\bm{x}^{*}}\log p(\bm{x}^{*}|\bm{x}^{c})=\nabla_{\bm{x}^{*}}\log p([ \bm{x}^{*},\bm{x}^{c}])-\nabla_{\bm{x}^{*}}\log p(\bm{x}^{c})=\nabla_{\bm{x}^ {*}}\log p(\bm{x})\]

If we have access to the score on the joint variables, we, therefore, have access to the conditional score by simply only taking the gradient of the joint score for the variable we are not conditioning on.

Given we have learnt \(s_{\theta}(t,\bm{x})\approx\nabla_{\bm{x}}\log p_{t}(\bm{x})\), we could use this to perform Langevin dynamics at \(t=\epsilon\), some time very close to \(0\). Similar to (Song and Ermon, 2019) however, this produces the twin issues of how to initialise the dynamics, given a random initialisation will start the sampler in a place where the score has been badly learnt, producing slow and inaccurate sampling.

Instead, we follow a scheme of tempered Langevin sampling detailed in Alg. 1. Starting at \(t=T\) we sample an initialisation of \(\bm{y}^{*}\) based on the reference distribution. Progressing from \(t=T\) towards \(t=\epsilon\) we alternate between running a series of Laygevin corrector steps to sample from the distribution \(p_{t,\bm{x}^{*}}(\bm{y}^{*}|\bm{y}^{c})\), and a single backwards SDE step to sample from \(p_{\bm{x}}(\bm{y}_{t-\gamma}|\bm{y}_{t})\) with a step size \(\gamma\). At each inner and outer step, we sample a noised version of the conditioning points \(\bm{y}^{c}\) based forward SDE applying noise to these context points, \(p_{t,\bm{x}^{c}}(\bm{y}_{t}^{c}|\bm{y}^{c})\). For the exactness of this scheme, all that matters is that at the end of the sampling scheme, we are sampling from \(p_{\bm{x}^{*}}(\bm{y}^{*}|\bm{y}^{c})\) (up to the \(\epsilon\) away from zero clipping of the SDE). The rest of the scheme is designed to map from the initial sample at \(t=T\) of \(\bm{y}^{*}\) to a viable sample through _regions where the score has been learnt well_.

Given the noising scheme applied to the context points does not actually play into the theoretical exactness of the scheme, only the practical difficulty of staying near regions of well-learnt score, we could make a series of different choices for how to noise the context set at each step.

The choices that present themselves are

1. The initial scheme of sampling context noise from the SDE every inner and outer step.
2. Only re-sampling the context noise every outer step, and keeping it fixed to this for each inner step associated with the outer step.
3. Instead of sampling independent marginal noise at each outer step, sampling a single noising trajectory of the context set from the forward SDE and use this as the noise at each time.
4. Perform no no noising at all. Effectively the replacement method with added Langevin sampling.

These are illustrated in Fig. 8. The main trade-off of different schemes is the speed at which the noise can be sampled vs sample diversity. In the Euclidean case, we have a closed form for the evolution of the marginal density of the context point under the forward SDE. In this case sampling the noise at a given time is \(\mathcal{O}(1)\) cost. On the other hand, in some instances such as nosing SDEs on general manifolds, we have to simulate this noise by discretising the forward SDE. In this case, it is \(\mathcal{O}(n)\) cost, where \(n\) is the number of discretisation steps in the SDE. For \(N\) outer steps and \(I\) inner steps, the complexity of the different noising schemes is compared in Table 4. Note the conditional sampling scheme other than the noise sampling is \(\mathcal{O}(NI)\) complexity.

### RePaint (Lugmayr et al., 2022) correspondance

In this section, we show that:

\begin{table}
\begin{tabular}{l c c} \hline \hline Scheme & Closed-form noise & Simulated noise \\ \hline Re-sample noise at every inner step & \(\mathcal{O}(NI)\) & \(\mathcal{O}(N^{2}I^{2})\) \\ Re-sample noise at every outer step & \(\mathcal{O}(N)\) & \(\mathcal{O}(N^{2})\) \\ Sampling an SDE path on the context & \(\mathcal{O}(N)\) & \(\mathcal{O}(N)\) \\ No noise applied & - & - \\ \hline \hline \end{tabular}
\end{table}
Table 4: Comparison of complexity of different noise sampling schemes for the context set.

Figure 8: Comparison of different context noising schemes for the conditional sampling.

``` Require: Score network \(\mathbf{s}_{\theta}(t,\bm{x},\bm{y})\), conditioning points \((x^{c},y^{c})\), query locations \(\bm{x}^{\star}\) \(\bar{\bm{x}}=[x^{c},\bm{x}^{\star}]\)\(\triangleright\) Augmented inputs set \(\bar{\bm{y}}_{T}^{\star}\sim\mathrm{N}(m(\bm{x}^{\star}),k(\bm{x}^{\star},\bm{x} ^{\star}))\)\(\triangleright\) Sample initial noise for\(t\in\{T,T-\gamma,...,\epsilon\}\)do\(\triangleright\) Noise context outputs \(y_{i}^{c}\sim p_{i,\bar{x}^{c}}(y_{i}^{c}|y_{0}^{c})\)\(\triangleright\) Sample tangent noise \(Z\sim\mathcal{N}(\mathrm{Id})\), \([-\tilde{\bm{y}}_{t-\gamma}^{\star}]=[y_{i}^{c},y_{t}^{\star}]+\gamma\left\{- \frac{1}{2}\left(m(\bar{\bm{x}})-[y_{i}^{c},\bm{y}_{t}^{\star}]\right)+\mathrm{ K}(\bar{\bm{x}},\bar{\bm{x}})\mathbf{s}_{\theta}(t,\bar{\bm{x}},[y_{i}^{c},\tilde{\bm{y}}_{t}^{ \star}])\right\}+\sqrt{\gamma}\mathrm{K}(\bar{\bm{x}},\bar{\bm{x}})^{1/2}Z \triangleright\) Euler-Maruyama step for\(l\in\{1,\ldots,L\}\)do \(y_{i-\gamma}^{c}\sim p_{l-\gamma,\bar{x}^{c}}(y_{i-\gamma}^{c}|y_{0}^{c})\)\(\triangleright\) Noise context outputs \(Z\sim\mathcal{N}(0,\mathrm{Id})\)\(\triangleright\) Sample tangent noise \([-\tilde{\bm{y}}_{t-\gamma}^{\star}]=[-\tilde{\bm{y}}_{t-\gamma}^{\star}]+ \frac{\gamma}{2}\mathrm{K}(\bar{\bm{x}},\bar{\bm{x}})\mathbf{s}_{\theta}(t- \gamma,\bar{\bm{x}},\left[y_{i-\gamma}^{c},\tilde{\bm{y}}_{t-\gamma}^{\star} \right])+\sqrt{\gamma}\mathrm{K}(\bar{\bm{x}},\bar{\bm{x}})^{1/2}Z\)\(\triangleright\) Langevin step \(\bm{y}_{t-\gamma}^{\star}=\tilde{\bm{y}}_{t-\gamma}^{\star}\) return\(\bm{y}_{\epsilon}^{\star}\) ```

**Algorithm 1** Conditional sampling with Langevin dynamics.

We begin by recalling the conditional sampling algorithm we study in Alg. 1 and Alg. 2.

``` Require: Score network \(\mathbf{s}_{\theta}(t,\bm{x},\bm{y})\), conditioning points \((x^{c},y^{c})\), query locations \(\bm{x}^{\star}\) \(\bar{\bm{x}}=[x^{c},\bm{x}^{\star}]\)\(\triangleright\) Augmented inputs set \([y_{T}^{c},\bm{y}_{T}^{\star}]\sim\mathrm{N}(m(\bar{\bm{x}}),k(\bar{\bm{x}},\bar{ \bm{x}}))\)\(\triangleright\) Sample initial noise for\(t\in\{T,T-\gamma,...,\epsilon\}\)do\(\tilde{\bm{y}}_{t}^{\star}=\bm{y}_{t}^{\star}\) for\(l\in\{1,\ldots,L\}\)do \(\tilde{\bm{y}}_{t}^{\star}\sim\mathrm{N}(m_{l}(x^{c};y^{c}),k_{l}(x^{c},x^{c};y^{ c}))\)\(\triangleright\) Noise context outputs \(Z\sim\mathrm{N}(0,\mathrm{Id})\)\(\triangleright\) Sample tangent noise \([-\tilde{\bm{y}}_{t-\gamma}^{\star}]=[y_{i}^{c},\tilde{\bm{y}}_{t}^{\star}]+ \gamma\left\{-\frac{1}{2}\left(m(\bar{\bm{x}})-[y_{i}^{c},\tilde{\bm{y}}_{t}^{ \star}]\right)+\mathrm{K}(\bar{\bm{x}},\bar{\bm{x}})\mathbf{s}_{\theta}(t,\bar {\bm{x}},[y_{i}^{c},\tilde{\bm{y}}_{t}^{\star}])\right\}+\sqrt{\gamma}\mathrm{ K}(\bar{\bm{x}},\bar{\bm{x}})^{1/2}Z\)\(\triangleright\) Reverse step \(Z\sim\mathrm{N}(0,\mathrm{Id})\)\(\triangleright\) Sample tangent noise \(\tilde{\bm{y}}_{t}^{\star}=\tilde{\bm{y}}_{t-\gamma}^{\star}+\gamma\left\{\frac{1}{2} \left(m(\bar{\bm{x}}^{\star})-\tilde{\bm{y}}_{t-\gamma}^{\star}\right)\right\}+ \sqrt{\gamma}\mathrm{K}(\bm{x}^{\star},\bm{x}^{\star})^{1/2}Z\)\(\triangleright\) Forward step \(\bm{y}_{t-\gamma}^{\star}=\tilde{\bm{y}}_{t-\gamma}^{\star}\) return\(\bm{y}_{\epsilon}^{\star}\) ```

**Algorithm 2** RePaint(Lugmayr et al., 2022).

First, we start by describing the RePaint algorithm (Lugmayr et al., 2022). We consider \((Z_{k}^{0},Z_{k}^{1},Z_{k}^{2})_{k\in\mathbb{N}}\) a sequence of independent Gaussian random variable such that for any \(k\in\mathbb{N}\), \(Z_{k}^{1}\) and \(Z_{k}^{2}\) are \(d\)-dimensional Gaussian random variables with zero mean and identity covariance matrix and \(Z_{k}^{0}\) is a \(p\)-dimensional Gaussian random variable with zero mean and identity covariance matrix. We assume that the whole sequence to be inferred is of size \(d\) while the context is of size \(p\). For simplicity, we only consider the Euclidean setting with \(\mathrm{K}=\mathrm{Id}\). The proofs can be adapted to cover the case \(\mathrm{K}\neq\mathrm{Id}\) without loss of generality.

Let us fix a time \(t_{0}\in[0,T]\). We consider the chain \((X_{k})_{k\in\mathbb{N}}\) given by \(X_{0}\in\mathbb{R}^{d}\) and for any \(k\in\mathbb{N}\), we define

\[X_{k+1/2}=\mathrm{e}^{\gamma}X_{k}+2(\mathrm{e}^{\gamma}-1)\nabla_{x_{k}}\log p_ {t_{0}}([X_{k},X_{k}^{c}])+(\mathrm{e}^{2\gamma}-1)^{1/2}Z_{k}^{1},\] (69)

where \(X_{k}^{c}=\mathrm{e}^{-t_{0}}X_{0}^{c}+(1-\mathrm{e}^{-2t_{0}})^{1/2}Z_{k}^{0}\). Finally, we consider

\[X_{k+1}=\mathrm{e}^{-\gamma}X_{k+1/2}+(1-\mathrm{e}^{-2\gamma})^{1/2}Z_{k}^{2}.\] (70)

Note that (69) corresponds to one step of _backward SDE_ integration and (70) corresponds to one step of _forward SDE_ integration. In both cases we have used the exponential integrator, see (DeBortoli, 2022) for instance. While we use the exponential integrator in the proofs for simplicity other integrators such as the classical Euler-Maruyama integration could have been used. Combining (69) and (70), we get that for any \(k\in\mathbb{N}\) we have

\[X_{k+1}=X_{k}+2(1-\mathrm{e}^{-\gamma})\nabla_{x_{k}}\log p_{t_{0}}([X_{k},X_{k }^{c}])+(1-\mathrm{e}^{-2\gamma})^{1/2}(Z_{k}^{1}+Z_{k}^{2}).\] (71)

Remarking that \((Z_{k})_{k\in\mathbb{N}}=((Z_{k}^{1}+Z_{k}^{3})/\sqrt{2})_{k\in\mathbb{N}}\) is a family of \(d\)-dimensional Gaussian random variables with zero mean and identity covariance matrix, we get that for any \(k\in\mathbb{N}\)

\[X_{k+1}=X_{k}+2(1-\mathrm{e}^{-\gamma})\nabla_{x_{k}}\log p_{t_{0}}([X_{k},X_{ k}^{c}])+\sqrt{2}(1-\mathrm{e}^{-2\gamma})^{1/2}Z_{k},\] (72)

where we recall that \(X_{k}^{c}=\mathrm{e}^{-t_{0}}X_{0}^{c}+(1-\mathrm{e}^{-2t_{0}})^{1/2}Z_{k}^{0}\). Note that the process (72) is another version of the Repaint algorithm (Lugmayr et al., 2022), where we have concatenated the denoising and noising procedure. With this formulation, it is clear that Repaint is equivalent to Alg. 1. In what follows, we identify the limitating SDE of this process.

In what follows, we describe the limiting behavior of (72) under mild assumptions on the target distribution. In what follows, for any \(x_{t_{0}}\in\mathbb{R}^{d}\), we denote

\[b(x_{t_{0}})=2\int_{\mathbb{R}^{p}}\nabla_{x_{t_{0}}}\log p_{t_{0}}([x_{t_{0} },x_{t_{0}}^{c}])p_{t_{0}|0}(x_{t_{0}}^{c}|x_{0}^{c})\mathrm{d}x_{t_{0}}^{c}.\] (73)

We emphasize that \(b/2\neq\nabla_{x_{t_{0}}}\log p(\cdot|x_{0}^{c})\). In particular, using Tweedie's identity, we have that for any \(x_{t_{0}}\in\mathbb{R}^{d}\)

\[\nabla\log p_{t_{0}}(x_{t_{0}}|x_{0}^{c})=\int_{\mathbb{R}^{p}}\nabla_{x_{t_{0 }}}\log p([x_{t_{0}},x_{t_{0}}^{c}]|x_{0}^{c})p(x_{t_{0}}^{c}|x_{t_{0}},x_{0}^ {c})\mathrm{d}x_{t_{0}}^{c}.\] (74)

We introduce the following assumption.

**Assumption 1**.: _There exist \(\mathsf{L},\,C\geq 0,\,\mathfrak{m}>0\) such that for any \(x_{t_{0}}^{c},y_{t}^{c}\in\mathbb{R}^{p}\) and \(x_{t_{0}},y_{t}\in\mathbb{R}^{d}\)_

\[\|\nabla\log p_{t_{0}}([x_{t_{0}},x_{t_{0}}^{c}])-\nabla\log p_{t_{0}}([y_{t},y_{t}^{c}])\|\leq\mathrm{L}(\|x_{t_{0}}-y_{t}\|+\|x_{t_{0}}^{c}-y_{t}^{c}\|).\] (75)

Assumption 1 ensures that there exists a unique strong solution to the SDE associated with (72). Note that conditions under which \(\log p_{t_{0}}\) is Lipschitz are studied in De Bortoli (2022). In the theoretical literature on diffusion models the Lipschitzness assumption is classical, see Lee et al. (2023) and Chen et al. (2022).

We denote \(((\mathbf{X}_{t}^{\gamma})_{t\geq 0})_{\gamma>0}\) the family of processes such that for any \(k\in\mathbb{N}\) and \(\gamma>0\), we have for any \(t\in[k\gamma,(k+1)\gamma)\), \(\mathbf{X}_{t}^{\gamma}=(1-(t-k\gamma)/\gamma)\mathbf{X}_{k\gamma}^{\gamma}+(t -k\gamma)/\gamma\mathbf{X}_{(k+1)\gamma}^{\gamma}\) and

\[\mathbf{X}_{(k+1)\gamma}^{\gamma}=\mathbf{X}_{k\gamma}^{\gamma}+2(1-\mathrm{e }^{-\gamma})\nabla_{\mathbf{X}_{k\gamma}^{\gamma}}\,\log p_{t_{0}}([\mathbf{X }_{k\gamma}^{\gamma},\mathbf{X}_{k\gamma}^{c,n}])+\sqrt{2}(1-\mathrm{e}^{-2 \gamma})^{1/2}\mathbf{Z}_{k\gamma}^{\gamma},\] (76)

where \((\mathbf{Z}_{k\gamma}^{\gamma})_{k\in\mathbb{N},\gamma>0}\) is a family of independent \(d\)-dimensional Gaussian random variables with zero mean and identity covariance matrix and for any \(k\in\mathbb{N}\), \(\gamma>0\), \(\mathbf{X}_{k\gamma}^{c,\gamma}=\mathrm{e}^{-t_{0}}x_{0}^{c}+(1-\mathrm{e}^{-2t _{0}})^{1/2}\mathbf{Z}_{k\gamma}^{0,\gamma}\), where \((\mathbf{Z}_{k\gamma}^{0,\gamma})_{k\in\mathbb{N},\gamma>0}\) is a family of independent \(p\)-dimensional Gaussian random variables with zero mean and identity covariance matrix. This is a _linear interpolation_ of the Repaint algorithm in the form of (72).

Finally, we denote \((\mathbf{X}_{t})_{t\geq 0}\) such that

\[\mathrm{d}\mathbf{X}_{t}=b(\mathbf{X}_{t})\mathrm{d}t+2\mathbf{B}_{t},\qquad \mathbf{X}_{0}=x_{0}.\] (77)

We recall that \(b\) depends on \(t_{0}\) but \(t_{0}\) is _fixed_ here. This means that we are at time \(t_{0}\) in the diffusion and consider a _corrector_ at this stage. The variable \(t\) does not corresponds to the backward evolution but to the forward evolution _in the corrector stage_. Under Assumption 1, (77) admits a unique strong solution. The rest of the section is dedicated to the proof of the following result.

**Theorem E.1**.: _Assume Assumption 1. Then \(\lim_{n\to+\infty}(\mathbf{X}_{t}^{1/n})_{t\geq 0}=(\mathbf{X}_{t})_{t\geq 0}\)._

This result is an application of Stroock and Varadhan (2007, Theorem 11.2.3). It explicits what is the _continuous_ limit of the Repaint algorithm (Lugmayr et al., 2022).

In what follows, we verify that the assumptions of this result hold in our setting. For any \(\gamma>0\) and \(x\in\mathbb{R}^{d}\), we define

\[b_{\gamma}(x)=(2/\gamma)[(1-\mathrm{e}^{-\gamma})\int_{\mathbb{R}^{ d}}\nabla_{x_{t_{0}}}\log p_{t_{0}}([x_{t_{0}},x_{t_{0}}^{c}])p_{t_{0}|0}(x_{t_{0}}^ {c}|x_{0}^{c})\mathrm{d}x_{t_{0}}^{c}\] (78) \[\qquad-(1/\gamma)\mathbb{E}[(\mathbf{X}^{\gamma}_{(k+1)\gamma}- \mathbf{X}^{\gamma}_{k\gamma})\mathbf{1}_{\|\mathbf{X}^{\gamma}_{(k+1)\gamma}} \mathbf{-X}^{\gamma}_{k\gamma}\|\geq 1\;|\mathbf{X}_{k\gamma}=x],\] (79) \[\Sigma_{\gamma}(x)=(4/\gamma)(1-\mathrm{e}^{-\gamma})^{2}\int_{ \mathbb{R}^{d}}\nabla_{x_{t_{0}}}\log p_{t_{0}}([x_{t_{0}},x_{t_{0}}^{c}])^{ \otimes 2}p_{t_{0}|0}(x_{t_{0}}^{c}|x_{0}^{c})\mathrm{d}x_{t_{0}}^{c}+(2/\gamma )(1-\mathrm{e}^{-2\gamma})\operatorname{Id}\] (80) \[\qquad-(1/\gamma)\mathbb{E}[(\mathbf{X}^{\gamma}_{(k+1)\gamma}- \mathbf{X}^{\gamma}_{k\gamma})^{\otimes 2}\mathbf{1}_{\|\mathbf{X}^{\gamma}_{(k+1) \gamma}}\mathbf{-X}^{\gamma}_{k\gamma}\|\geq 1\;|\mathbf{X}_{k\gamma}=x].\] (81)

Note that for any \(\gamma>0\) and \(x\in\mathbb{R}^{d}\), we have

\[b_{\gamma}(x)=\mathbb{E}[\mathbf{1}_{\|\mathbf{X}^{\gamma}_{(k+ 1)\gamma}}\mathbf{-X}^{\gamma}_{k\gamma}\|\leq 1(\mathbf{X}^{\gamma}_{(k+1) \gamma}-\mathbf{X}^{\gamma}_{k\gamma})\;|\mathbf{X}^{\gamma}_{k\gamma}=x]\] (82) \[\Sigma_{\gamma}(x)=\mathbb{E}[\mathbf{1}_{\|\mathbf{X}^{\gamma}_{ (k+1)\gamma}}\mathbf{-X}^{\gamma}_{k\gamma}\|\leq 1(\mathbf{X}^{\gamma}_{(k+1) \gamma}-\mathbf{X}^{\gamma}_{k\gamma})^{\otimes 2}\;|\mathbf{X}^{\gamma}_{k \gamma}=x]\] (83)

**Lemma E.2**.: _Assume Assumption 1. Then, we have that for any \(R,\varepsilon>0\) and \(\gamma\in(0,1)\)_

\[\lim_{\gamma\to 0}\sup\{\|\Sigma_{\gamma}(x)-\Sigma(x)\|\;|x \in\mathbb{R}^{d},\;\|x\|\leq R\}=0,\] (85) \[\lim_{\gamma\to 0}\sup\{\|b_{\gamma}(x)-b(x)\|\;|x\in\mathbb{R}^{d}, \;\|x\|\leq R\}=0,\] (86) \[\lim_{\gamma\to 0}(1/\gamma)\sup\{\mathbb{P}(\|\mathbf{X}^{ \gamma}_{(k+1)\gamma}-\mathbf{X}^{\gamma}_{k\gamma}\|\geq\varepsilon\;|\mathbf{ X}_{k\gamma}=x)\;|x\in\mathbb{R}^{d},\;\|x\|\leq R\}=0.\] (87)

_Where we recall that for any \(x\in\mathbb{R}^{d}\),_

\[b(x)=2\int_{\mathbb{R}^{p}}\nabla_{x_{t_{0}}}\log p_{t_{0}}([x_{t_{0}},x_{t_{ 0}}^{c}])p_{t_{0}|0}^{x}(x_{t_{0}}^{c}|x_{0}^{c})\mathrm{d}x_{t_{0}}^{c},\qquad \Sigma(x)=4\operatorname{Id}.\] (88)

Proof.: Let \(R,\varepsilon>0\) and \(\gamma\in(0,1)\). Using Assumption 1, there exists \(C>0\) such that for any \(x_{t_{0}}\in\mathbb{R}^{d}\) with \(\|x_{t_{0}}\|\leq R\), we have \(\|\nabla_{x_{t_{0}}}\log p_{t_{0}}([x_{t_{0}},x_{t_{0}}^{c}])\|\leq C(1+\|x_{t_{0 }}^{c}\|)\). Since \(p_{t_{0}|0}^{c}\) is Gaussian with zero mean and covariance matrix \((1-\mathrm{e}^{-2t_{0}})\operatorname{Id}\), we get that for any \(p\in\mathbb{N}\), there exists \(A_{k}\geq 0\) such that for any \(x_{t_{0}}\in\mathbb{R}^{d}\) with \(\|x_{t_{0}}\|\leq R\)

\[\int_{\mathbb{R}^{d}}\|\nabla_{x_{t_{0}}}\log p_{t_{0}}([x_{t_{0}},x_{t_{0}}^{ c}])\|^{p}p_{t_{0}|0}^{c}(x_{t_{0}}^{c}|x_{0}^{c})\mathrm{d}x_{t_{0}}^{c}\leq A _{k}(1+\|x_{0}^{c}\|^{p}).\] (89)

Therefore, using this result and the fact that for any \(s\geq 0\), \(\mathrm{e}^{-s}\geq 1-s\), we get that there exists \(B_{k}\geq 0\) such that for any \(k,p\in\mathbb{N}\) and for any \(x_{t_{0}}\in\mathbb{R}^{d}\) with \(\|x_{t_{0}}\|\leq R\)

\[\mathbb{E}[\|\mathbf{X}_{(k+1)\gamma}-\mathbf{X}_{k\gamma}\|^{p} \;|\mathbf{X}_{k\gamma}=x]\leq B_{k}\gamma^{p/2}(1+\|x_{0}^{c}\|^{p}).\] (90)

Therefore, combining this result and the Markov inequality, we get that for any \(x_{t_{0}}\in\mathbb{R}^{d}\) with \(\|x_{t_{0}}\|\leq R\) we have

\[\lim_{\gamma\to 0}(1/\gamma)\sup\{\mathbb{P}(\|\mathbf{X}^{ \gamma}_{(k+1)\gamma}-\mathbf{X}^{\gamma}_{k\gamma}\|\geq\varepsilon\;| \mathbf{X}_{k\gamma}=x)\;|x\in\mathbb{R}^{d},\;\|x\|\leq R\}=0,\] (91) \[\lim_{\gamma\to 0}(1/\gamma)\|\mathbb{E}[(\mathbf{X}^{\gamma}_{(k+1) \gamma}-\mathbf{X}^{\gamma}_{k\gamma})\mathbf{1}_{\|\mathbf{X}^{\gamma}_{(k+1) \gamma}-\mathbf{X}^{\gamma}_{k\gamma}\|\geq 1\;|\mathbf{X}_{k\gamma}=x]\|=0,\] (92) \[\lim_{\gamma\to 0}(1/\gamma)\|\mathbb{E}[(\mathbf{X}^{\gamma}_{(k+1) \gamma}-\mathbf{X}^{\gamma}_{k\gamma})\mathbf{1}_{\|\mathbf{X}^{\gamma}_{(k+1) \gamma}-\mathbf{X}^{\gamma}_{k\gamma}\|\geq 1\;|\mathbf{X}_{k\gamma}=x]\|=0\] (93)

In addition, we have that for any \(x_{t_{0}}\in\mathbb{R}^{d}\) with \(R>0\)

\[|(2/\gamma)(1-\mathrm{e}^{-\gamma})-2|\|\int_{\mathbb{R}^{d}}\nabla_{x_ {t_{0}}}\log p_{t_{0}}([x_{t_{0}},x_{t_{0}}^{c}])p_{t_{0}|0}(x_{t_{0}}^{c}|x_{0 }^{c})\mathrm{d}x_{t_{0}}^{c}\|\] (94) \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquadWe can now conclude the proof of Theorem E.1.

Proof.: We have that \(x\mapsto b(x)\) and \(x\mapsto\Sigma(x)\) are continuous. Combining this result and Lemma E.2, we conclude the proof upon applying Stroock and Varadhan (2007, Theorem 11.2.3). 

Theorem E.1 is a non-quantitative result which states what is the limit chain for the RePaint procedure. Note that if we do not resample, we get that

\[b^{\mathrm{cond}}(x)=2\nabla_{x_{t_{0}}}\log p_{t_{0}}([x_{t_{0}},x_{t_{0}}^{ c}]),\qquad\Sigma(x)=4\operatorname{Id}.\] (98)

Recalling (88), we get that (98) is an _amortised version_ of \(b^{\mathrm{cond}}\). Similar convergence results can be derived in this case. Note that it is also possible to obtain quantitative discretization bounds between \((\mathbf{X}_{t})_{t\geq 0}\) and \((\mathbf{X}_{t}^{1/n})_{t\geq 0}\) under the \(\ell^{2}\) distance. These bounds are usually leveraged using the Girsanov theorem (Durmus and Moulines, 2017; Dalalyan, 2017). We leave the study of such bounds for future work.

We also remark that \(b(x_{t_{0}})\) is _not_ given by \(\nabla\log p_{t_{0}}(x_{t_{0}}|x_{0}^{c})\). Denoting \(U_{t_{0}}\) such that for any \(x_{t_{0}}\in\mathbb{R}^{d}\)

\[U_{t_{0}}(x_{t_{0}})=-\int_{\mathbb{R}^{p}}(\log p_{t_{0}}(x_{t_{0}}|x_{t_{0}}^ {e}))p_{t|0}(x_{t_{0}}^{c}|x_{0}^{e})\mathrm{d}x_{t_{0}}^{e},\] (99)

we have that \(\nabla U_{t_{0}}(x_{t_{0}})=-b(x_{t_{0}})\), under mild integration assumptions. In addition, using Jensen's inequality, we have

\[\int_{\mathbb{R}^{d}}\exp[-U_{t_{0}}(x_{t_{0}})]\mathrm{d}x_{t_{0}}\leq\int_{ \mathbb{R}^{d}}\int_{\mathbb{R}^{p}}p_{t_{0}}(x_{t_{0}}|x_{t_{0}}^{c})p_{t|0}(x _{t_{0}}^{c}|x_{0}^{c})\mathrm{d}x_{t_{0}}\mathrm{d}x_{t_{0}}^{c}\leq 1.\] (100)

Hence, \(\pi_{t_{0}}\) with density proportional to \(x\mapsto\exp[-U_{t_{0}}(x)]\) defines a valid probability measure.

We make the following assumption which allows us to control the ergodicity of the process \((\mathbf{X}_{t})_{t\geq 0}\).

**Assumption 2**.: _There exist \(\mathfrak{m}>0\) and \(C\geq 0\) such that for any \(x_{t_{0}}\in\mathbb{R}^{d}\) and \(x_{t_{0}}^{c}\in\mathbb{R}^{p}\)_

\[\langle\nabla_{x_{t}}\log p_{t_{0}}([x_{t},x_{t}^{c}]),x_{t}\rangle\leq- \mathfrak{m}\|x_{t}\|^{2}+C(1+\|x_{t}^{c}\|^{2}).\] (101)

The following proposition ensures the ergodicity of the chain \((\mathbf{X}_{t})_{t\geq 0}\). It is a direct application of Roberts and Tweedie (1996, Theorem 2.1).

**Proposition E.3**.: _Assume Assumption 1 and Assumption 2. Then, \(\pi_{t_{0}}\) is the unique invariant probability measure of \((\mathbf{X}_{t})_{t\geq 0}\) and \(\lim_{t\to 0}\|\mathcal{L}(\mathbf{X}_{t})-\pi_{t_{0}}\|_{\mathrm{TV}}=0\), where \(\mathcal{L}(\mathbf{X}_{t})\) is the distribution of \(\mathbf{X}_{t}\)._

Finally, for any \(t_{0}>0\), denoting \(\pi_{t_{0}}\) the probability measure with density \(U_{t_{0}}\) given for any \(x_{t_{0}}\in\mathbb{R}^{d}\) by

\[U_{t_{0}}(x_{t_{0}})=-\int_{\mathbb{R}^{p}}(\log p_{t_{0}}(x_{t_{0}}|x_{t_{0}}^ {c}))p_{t|0}(x_{t_{0}}^{c}|x_{0}^{c})\mathrm{d}x_{t_{0}}^{c}.\] (102)

We show that the family of measures \((\pi_{t_{0}})_{t_{0}>0}\) approximates the posterior with density \(x_{0}\mapsto p(x_{0}|x_{0}^{c})\) when \(t_{0}\) is small enough.

**Proposition E.4**.: _Assume Assumption 1. We have that \(\lim_{t_{0}\to 0}\pi_{t_{0}}=\pi_{0}\) where \(\pi_{0}\) admits a density w.r.t. the Lebesgue measure given by \(x_{0}\mapsto p(x_{0}|x_{0}^{c})\)._

Proof.: This is a direct consequence of the fact that \(p_{t|0}(\cdot|x_{0}^{c})\to\delta_{x_{0}^{c}}\). 

This last results shows that even though we do not target \(x_{t_{0}}\mapsto p_{t_{0}|0}(x_{t_{0}}|x_{0}^{c})\) using this corrector term, we still target \(p(x_{0}|x_{0}^{c})\) as \(t_{0}\to 0\) which corresponds to the desired output of the algorithm.

## Appendix F Experimental details

Models, training and evaluation have been implemented in Jax(Bradbury et al., 2018). We used Python (Van Rossum and Drake Jr, 1995) for all programming, Hydra(Yadan, 2019), Numpy(Harris et al., 2020), Scipy(Virtanen et al., 2020), Matplotlib(Hunter, 2007), and Pandas(McKinney et al., 2010). The code is publicly available at https://github.com/cambridge-mlg/neural_diffusion_processes.

### Regression 1d

#### f.1.1 Data generation

We follow the same experimental setup as Bruinsma et al. (2020) to generate the 1d synthetic data. It consists of Gaussian (Squared Exponential (SE), \(\textsc{Matern}(\frac{5}{2})\), Weakly Periodic) and non-Gaussian (Sawtooth and Mixture) sample paths, where Mixture is a combination of the other four datasets with equal weight. Fig. 9 shows samples for each of these dataset. The Gaussian datasets are corrupted with observation noise with variance \(\sigma^{2}=0.05^{2}\). The left column of Fig. 9 shows example sample paths for each of the 5 datasets.

The training data consists of \(2^{14}\) sample paths while the test dataset has \(2^{12}\) paths. For each test path we sample the number of context points between \(1\) and \(10\), the number of target points are fixed to \(50\) for the GP datasets and \(100\) for the non-Gaussian datasets. The input range for the training and interpolation datasets is \([-2,2]\) for both the context and target sets, while for the extrapolation task the context and target input points are drawn from \([2,6]\).

Architecture.For all datasets, except Sawtooth, we use \(5\) bi-dimensional attention layers (Dutordoir et al., 2022) with \(64\) hidden dimensions and \(8\) output heads. For Sawtooth, we obtained better performance with a wider and shallower model consisting of \(2\) bi-dimensional attention layers with a hidden dimensionality of \(128\). In all experiment, we train the NDP-based models over \(300\) epochs using a batch size of \(256\). Furthermore, we use the Adam optimiser for training with the following learning rate schedule: linear warm-up for 10 epochs followed by a cosine decay until the end of training.

#### f.1.2 Ablation Limiting Kernels

The test log-likelihoods (TLLs) reported in App. F.1.3 for the NDP models target a white limiting kernel and train to approximate the preconditioned score \(K\nabla\log p_{t}\). Overall, we found this to be the best performing setting. App. F.1.3 shows an ablation study for different choices of limiting kernel and score parametrisation. We refer to Table 3 for a detailed derivation of the score parametrisations.

The dataset in the top row of the figure originates from a Squared Exponential (SE) GP with lengthscale \(\ell=0.25\). We compare the performance of three different limiting kernels: white (blue), a SE with a longer lengthscale \(\ell=1\) (orange), and a SE with a shorter lengthscale \(\ell=0.1\) (green). As the dataset is Gaussian, we have access to the true score. We observe that, across the different parameterisations, the white limiting kernel performance best. However, note that for the White kernel \(K=I\) and thus the different parameterisations become identical. For non-white limiting kernels we see a reduction in performance for both the approximate and exact score. We attribute this to the additional complexity of learning a non-diagonal covariance.

In the bottom row of App. F.1.3 we repeat the experiment for a dataset consisting of samples from the Periodic GP with lengthscale \(0.5\). We draw similar conclusions: the best performing limiting kernel, across the different parametrisations, is the White noise kernel.

#### f.1.3 Ablation Conditional Sampling

Next, we focus on the empirical performance of the different noising schemes in the conditional sampling, as discussed in Fig. 8. For this, we measure the the Kullback-Leibler (KL) divergence between two Gaussian distributions: the true GP-based conditional distribution, and an distribution created by drawing conditional sampling from the model and fitting a Gaussian to it using the empirical mean and covariance. We perform this test on the 1D squared exponential dataset (described above) as this gives us access to the true posterior. We use \(2^{12}\) samples to estimate the empirical mean and covariance, and fix the number of context points to 3.

In Fig. 11 we keep the total number of score evaluations fixed to \(5000\) and vary the number of steps in the inner (\(L\)) loop such that the number of outer steps is given by the ratio \(5000/L\). From the figure, we observe that the particular choice of noising scheme is of less importance as long at least a couple (\(\pm 5\)) inner steps are taken. We further note that in this experiment we used the true score (available because of the Gaussianity of the dataset), which means that these results may differ if an approximate score network is used.

Figure 10: _Ablation study_ targeting different limiting kernels and score parametrisations.

Figure 9: Visualisation of 1D regression experiment.

### Gaussian process vector fields

DataWe create synthetic datasets using samples from two-dimensional zero-mean GPs with the following \(\mathrm{E}(2)\)-equivariant kernels: a diagonal Squared-Exponential (SE) kernel, a zero curl (Curl-free) kernel and a zero divergence (Div-free) kernel, as described in App. D.1. We set the variance to \(\sigma^{2}=1\) and the lengthscale to \(\ell=\sqrt{5}\). We evaluate these GPs on a disk grid, created via a 2D grid with \(30\times 30\) points regularly space on \([-10,10]^{2}\) and keeping only the points inside the disk of radius \(10\). We create a training dataset of size \(80\times 10^{3}\). and a test dataset of size \(10\times 10^{3}\).

ModelsWe compare two flavours of our model GeomNDP. One with a non-equivariant attention-based score network (Figure C.1, Dutordoir et al., 2022), referred as \(\text{NDP}^{*}\). Another one with a \(\mathrm{E}(2)\)-equivariant score architecture, based on steerable CNNs (Thomas et al., 2018; Weiler et al., 2018). We rely on the e3nn library (Geiger and Smidt, 2022) for implementation. A knn graph \(\mathcal{E}\) is built with \(k=20\). The pairwise distances are first embed into \(\mu(r_{ab})\) with a'smooth_finite' basis of \(50\) elements via e3nn.soft_one_hot_linspace, and with a maximum radius of \(2\). The time is mapped via a sinusoidal embedding \(\phi(t)\)(Vaswani et al., 2017). Then edge features are obtained as \(e_{ab}=\Psi^{(e)}(\mu(r_{ab})||\phi(t))\ \forall(a,b)\in\mathcal{E}_{k}\) with \(\Psi^{(e)}\) an MLP with \(2\) hidden layers of width \(64\). We use \(5\) e3nn.FullyConnectedTensorProduct layers with update given by \(V_{a}^{k+1}=\sum_{b\in\mathcal{N}(a,\mathcal{E}_{k})}V_{a}^{k}\otimes\left( \Psi^{v}(e_{ab}||V_{a}^{k}||V_{b}^{k})\right)Y(\hat{r}_{ab})\) with \(Y\) spherical harmonics up to order \(2\)m \(\Psi^{v}\) an

Figure 11: Ablation noising schemes for conditional sampling.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & SE & \(\text{Matern}^{-\frac{5}{2}}_{-\frac{1}{2}}\) & Weakly Per. & Sawtooth & Mixture \\ \hline Interpolation & & & & & \\ GP (optimum) & \(0.70\pm_{0.00}\) & \(0.31\pm_{0.00}\) & \(-0.32\pm_{0.00}\) & n/a & n/a \\ \(T(1)-\text{GeomNDP}\) & \(\mathbf{0.72}\pm_{0.03}\) & \(\mathbf{0.32}\pm_{0.03}\) & \(-\mathbf{0.38}\pm_{0.03}\) & \(\mathbf{3.39}\pm_{0.04}\) & \(\mathbf{0.64}\pm_{0.08}\) \\ NDP\({}^{*}\) & \(\mathbf{0.71}\pm_{0.03}\) & \(\mathbf{0.30}\pm_{0.03}\) & \(-\mathbf{0.37}\pm_{0.03}\) & \(\mathbf{3.39}\pm_{0.04}\) & \(\mathbf{0.64}\pm_{0.08}\) \\ GNP & \(\mathbf{0.70}\pm_{0.01}\) & \(\mathbf{0.30}\pm_{0.01}\) & \(-0.47\pm_{0.01}\) & \(0.42\pm_{0.01}\) & \(0.10\pm_{0.02}\) \\ ConvCNP & \(-0.80\pm_{0.01}\) & \(-0.95\pm_{0.01}\) & \(-1.20\pm_{0.01}\) & \(0.55\pm_{0.02}\) & \(-0.93\pm_{0.02}\) \\ ConvNP & \(-0.46\pm_{0.01}\) & \(-0.67\pm_{0.01}\) & \(-1.02\pm_{0.01}\) & \(1.20\pm_{0.01}\) & \(-0.50\pm_{0.02}\) \\ ANP & \(-0.61\pm_{0.01}\) & \(-0.75\pm_{0.01}\) & \(-1.19\pm_{0.01}\) & \(0.34\pm_{0.01}\) & \(-0.69\pm_{0.02}\) \\ \hline Generalisation & & & & & \\ GP (optimum) & \(0.70\pm_{0.00}\) & \(0.31\pm_{0.00}\) & \(-0.32\pm_{0.00}\) & n/a & n/a \\ \(T(1)-\text{GeomNDP}\) & \(\mathbf{0.70}\pm_{0.02}\) & \(\mathbf{0.31}\pm_{0.02}\) & \(-\mathbf{0.38}\pm_{0.03}\) & \(\mathbf{3.39}\pm_{0.03}\) & \(\mathbf{0.62}\pm_{0.02}\) \\ NDP\({}^{*}\) & \(*\) & \(*\) & \(*\) & \(*\) \\ GNP & \(\mathbf{0.69}\pm_{0.01}\) & \(\mathbf{0.30}\pm_{0.01}\) & \(-0.47\pm_{0.01}\) & \(0.42\pm_{0.01}\) & \(0.10\pm_{0.02}\) \\ ConvCNP & \(-0.81\pm_{0.01}\) & \(-0.95\pm_{0.01}\) & \(-1.20\pm_{0.01}\) & \(0.53\pm_{0.02}\) & \(-0.96\pm_{0.02}\) \\ ConvNP & \(-0.46\pm_{0.01}\) & \(-0.67\pm_{0.01}\) & \(-1.02\pm_{0.01}\) & \(1.19\pm_{0.01}\) & \(-0.53\pm_{0.02}\) \\ ANP & \(-1.42\pm_{0.01}\) & \(-1.34\pm_{0.01}\) & \(-1.33\pm_{0.00}\) & \(-0.17\pm_{0.00}\) & \(-1.24\pm_{0.01}\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Mean test log-likelihood (TLL) \(\pm\) 1 standard error estimated over 4096 test samples are reported. Statistically significant best non-GP model is in bold. The NP baselines (GNP, ConvCNP, ConvNP and ANP) are quoted from Bruinsma et al. (2020): \({}^{*}\)\({}^{*}\) stands for a TLL below -10.

MLP with \(2\) hidden layers of width \(64\) acting on invariant features, and node features \(V^{k}\) having irreps 12x0e + 12x0o + 4x1e + 4x1o. Each layer has a gate non-linearity (Weiler et al., 2018).

We also evaluate two neural processes, a translation-equivariant ConvCNP (Gordon et al., 2020) with decoder architecture based on 2D convolutional layers (LeCun et al., 1998) and a \(\mathrm{C4}\times\mathbb{R}^{2}\subset\mathrm{E}(2)\)-equivariant SteerCNP (Holderrieth et al., 2021) with decoder architecture based on 2D steerable convolutions (Weiler and Cesa, 2021). Specific details can be found in the accompanying codebase https://github.com/PeterHolderrieth/Steerable_CNPs of Holderrieth et al. (2021).

Optimisation.Models are trained for \(80k\) iterations, via (Kingma and Ba, 2015) with a learning rate of \(5e-4\) and a batch size of \(32\). The neural diffusion processes are trained unconditionally, that is we feed GP samples evaluated on the full disk grid. Their weights are updated via with exponential moving average, with coefficient \(0.99\). The diffusion coefficient is weighted by \(\beta:\ t\mapsto\beta_{\min}+(\beta_{\max}-\beta_{\min})\cdot t\), and \(\beta_{\min}=1e-4\), \(\beta_{\max}=15\).

As standard, the neural processes are trained by splitting the training batches into a context and evaluation set, similar to when evaluating the models. Models have been trained on A100-SXM-80GB GPUs.

Evaluation.We measure the predictive log-likelihood of the data process samples under the model on a held-out test dataset. The context sets are of size 25 and uniformly sampled from a disk grid of size \(648\), and the models are evaluated on the complementary of the grid. For neural diffusion processes, we estimate the likelihood by solving the associated probability flow ODE (53). The divergence is estimated with the Hutchinson estimator, with Rademacher noise, and 8 samples, whilst the ODE is solved with the 2nd order Heun solver, with \(100\) discretisation steps.

We also report the performance of the data-generating GP, and the same GP but with diagonal posterior covariance GP (Diag.).

### Tropical cyclone trajectory prediction

Data.The data is drawn from he International Best330 Track Archive for Climate Stewardship (IBTrACS) Project, Version 4 (Knapp et al., 2010; Knapp et al., 2018). The tracks are taken from the 'all' dataset covering the tracks from all cyclone basins across the globe. The tracks are logged at intervals of every 3 hours. From the dataset, we selected tracks of at least 50 time points long and clipped any longer to this length, resulting in 5224 cyclones. 90% was used fro training and 10% held out for evaluation. This split was changed across seeds. More interesting schemes of variable-length tracks or of interest, but not pursued here in this demonstrative experiment. Natively the track locations live in latitude-longitude coordinates, although it is processed into different forms for different models. The time stamps are processed into the number of days into the cyclone forming and this format is used commonly between all models.

Models.Four models were evaluated.

The GP (\(\mathbb{R}\rightarrow\mathbb{R}^{2}\)) took the raw latitude-longitude data and normalised it. Using a 2-output RBF kernel with no covariance between the latitude and longitude and taking the cyclone time as input, placed a GP over the data. The hyperparameters of this kernel were optimised using a maximum likelihood grid search over the data. Note that this model places density outside the bounding box of \([-90,90]\times[-180,180]\) that defines the range of latitude and longitude, and so does not place a proper distribution on the space of paths on the sphere.

The Stereographic GP (\(\mathbb{R}\rightarrow\mathbb{R}^{2}/\{0\}\)) instead transformed the data under a sterographicc projection centred at the north pole, and used the same GP and optimisation as above. Since this model only places density on a set of measure zero that does not correspond to the sphere, it does induce a proper distribution on the space of paths on the sphere.

The NDP (\(\mathbb{R}\rightarrow\mathbb{R}^{2}\)) uses the same preprocessing as GP (\(\mathbb{R}\rightarrow\mathbb{R}^{2}\)) but uses a Neural Diffusion Process from (Dutordoir et al., 2022) to model the data. This has the same shortcomings as the GP (\(\mathbb{R}\rightarrow\mathbb{R}^{2}\)) in not placing a proper density on the space of paths on the sphere. The network used for the score function and the optimisation procedure is detailed below. A linear beta schedule was used with \(\beta_{0}=1e-4\) and \(\beta_{1}=10\). The reverse model was integrated back to \(\epsilon=5e-4\) for numerical stability. The reference measure was a white noise kernel with a variance \(0.05\). ODEs and SDEs were discretised with 1000 steps.

The GeomNDP (\(\mathbb{R}\to\mathcal{S}^{2}\)) works with the data projected into 3d space on the surface of the sphere. This projection makes no difference to the results of the model, but makes the computation of the manifold functions such as the exp map easier, and makes it easier to define a smooth score function on the sphere. This is done by outputting a vector for the score from the neural network in 3d space, and projecting it onto the tangent space of the sphere at the given point. For the necessity of this, see (De Bortoli et al., 2021). The network used for the score function and the optimisation procedure is detailed below. A linear beta schedule was used with \(\beta_{0}=1e-4\) and \(\beta_{1}=15\). The reverse model was integrated back to \(\epsilon=5e-4\) for numerical stability. The reference measure was a white noise kernel with a variance \(0.05\). ODEs and SDEs were discretised with 1000 steps.

**Neural network.** The network used to learn the score function for both NDP (\(\mathbb{R}\to\mathbb{R}^{2}\)) and GeomNDP (\(\mathbb{R}\to\mathcal{S}^{2}\)) is a bi-attention network from Dutordoir et al. (2022) with 5 layers, hidden size of 128 and 4 heads per layer. This results in 924k parameters.

**Optimisation.** NDP (\(\mathbb{R}\to\mathbb{R}^{2}\)) and GeomNDP (\(\mathbb{R}\to\mathcal{S}^{2}\)) were both optimised using (correctly implemented) Adam for 250k steps using a batch size of 1024 and global norm clipping of 1. Batches were drawn from the shuffled data and refreshed each time the dataset was exhausted. A learning rate schedule was used with 1000 warmup steps linearly from 1e-5 to 1e-3, and from there a cosine schedule decaying from 1e-3 to 1e-5. With even probability either the whole cyclone track was used in the batch, or 20 random points were sub-sampled to train the model better for the conditional sampling task.

**Conditional sampling.** The GP models used closed-form conditional sampling as described. Both diffusion-based models used the Langevin sampling scheme described in this work. 1000 outer steps were used with 25 inner steps. We use a \(\psi=1.0\) and \(\lambda_{0}=2.5\). In addition at the end of the Langevin sampling, we run an additional 150 Langevin steps with \(t=\epsilon\) as this visually improved performance.

**Evaluation.** For the model (conditional) log probabilities the GP models were computed in closed form. For the diffusion-based models, they were computed using the auxiliary likelihood ODE discretised over 1000 steps. The conditional probabilities were computed via the difference between the log-likelihood of the whole trajectory and the log-likelihood of the context set only. The mean squared errors were computed using the geodesic distance between 10 conditionally sampled trajectories, described above.