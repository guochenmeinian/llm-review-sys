# MLLMGuard:

A Multi-dimensional Safety Evaluation Suite

for Multimodal Large Language Models

 Tianle Gu\({}^{1,2}\)1, **Zeyang Zhou\({}^{2}\)**, **Kexin Huang\({}^{2}\)**, **Dandan Liang\({}^{2}\)**, **Yixu Wang\({}^{2}\)**, **Haiquan Zhao\({}^{2}\)**, **Yuanqi Yao\({}^{2}\)**, **Xingge Qiao\({}^{2}\)**, **Keqing Wang\({}^{2}\)**, **Yujiu Yang\({}^{1}\)\({}^{\dagger}\)**, **Yan Teng\({}^{2}\)\({}^{\dagger}\)**, **Yu Qiao\({}^{2}\)**, **Yingchun Wang\({}^{2}\)**

\({}^{1}\) Tsinghua Shenzhen International Graduate School, Tsinghua University

\({}^{2}\) Shanghai Artificial Intelligence Laboratory

Footnote 1: Work done during internship at Shanghai Artificial Intelligence Laboratory.

Footnote 2: Corresponding author. Correspondence to: Yujiu Yang <yang.yujiu@sz.tsinghua.edu.cn> and Yan Teng <tengyan@pjlab.org.cn>.

Footnote 3: Data and codes are available at https://github.com/AIFrames/MLLMGuard.

###### Abstract

Powered by remarkable advancements in Large Language Models (LLMs), Multimodal Large Language Models (MLLMs) demonstrate impressive capabilities in manifold tasks. However, the practical application scenarios of MLLMs are intricate, exposing them to potential malicious instructions and thereby posing safety risks. While current benchmarks do incorporate certain safety considerations, they often lack comprehensive coverage and fail to exhibit the necessary rigor and robustness. For instance, the common practice of employing GPT-4V as both the evaluator and a model to be evaluated lacks credibility, as it tends to exhibit a bias toward its own responses. In this paper, we present MLLMGuard, a multi-dimensional safety evaluation suite for MLLMs, including a bilingual image-text evaluation dataset, inference utilities, and a lightweight evaluator. MLLMGuard's assessment comprehensively covers two languages (English and Chinese) and five important safety dimensions (Privacy, Bias, Toxicity, Truthfulness, and Legality), each with corresponding rich subtasks. Focusing on these dimensions, our evaluation dataset is primarily sourced from platforms such as social media, and it integrates text-based and image-based red teaming techniques with meticulous annotation by human experts. This can prevent inaccurate evaluation caused by data leakage when using open-source datasets and ensures the quality and challenging nature of our benchmark. Additionally, a fully automated lightweight evaluator termed GuardRank is developed, which achieves significantly higher evaluation accuracy than GPT-4. Our evaluation results across 13 advanced models indicate that MLLMs still have a substantial journey ahead before they can be considered safe and responsible. 1

Footnote 1: Data and codes are available at https://github.com/AIFrames/MLLMGuard.

_Warning: The content of this article may cause discomfort or contain sensitive information._

## 1 Introduction

Attributed to the scaling up of training corpus and model parameters, recent years have witnessed remarkable progress in LLMs [1, 2, 3]. This progress has further propelled the development of a growing number of MLLMs (e.g., GPT-4V [4], Gemini [5], CogVLM [6], etc.) that utilize LLMs as the central framework for conducting complex multimodal tasks. A typical MLLM [7] consists of a pre-trained LLM, a pre-trained modality encoder, and a modality interface to connect them. This architecture extends the LLM from a single text modality to a multimodal field. However, the expanded scope of capabilities means that MLLMs face a wider range of threats, presenting new challenges to their safety capabilities [8, 9]. Therefore, beyond assessing the capabilities of MLLMs, it is essential to conduct a comprehensive evaluation of their safety.

Several studies have made preliminary attempts to evaluate the safety of MLLMs. For example, some research [10, 11, 9, 12] evaluate the hallucinations of MLLMs, Zhang et al. [13] examines the self-consistency of their responses when subjected to common corruptions, and Cai et al. [14] assesses their robustness against diverse style shifts. In addition to these specific safety aspects, more recent works have focused on the overall safety of MLLMs. Lin et al. [15] detects MLLMs' critical ability on meme-based social abuse, and Gong et al. [16], Liu et al. [17], Li et al. [8] explore distinct jailbreaking methods on safety topics. Moreover, Shi et al. [18] introduces evaluation based on 3H principle [19]. However, there remains a gap between these efforts and achieving a complete and comprehensive safety assessment.

Reviewing existing benchmarks, we identify the following main challenges in achieving the comprehensive evaluation: 1) _Deficiency in comprehensive evaluation dimensions._ Most benchmarks focus on a single purpose, e.g., hallucination [10, 20, 21, 22], or generalized safety [15, 16, 17, 23, 8], complicating the thorough evaluation and cross-comparison between MLLMs. 2) _Possible data leakage [24]_. Most safety benchmarks build their dataset by integrating open-source datasets, which are likely to be included in the training sets of MLLMs. 3) _Lack of effective evaluator on open-ended assessment._ While existing research highlights the instability introduced by fixed-format evaluation (e.g., multiple-choice) [25, 26], there is a lack of reliable evaluators on open-ended evaluation. Commonly, either human annotators or GPT-4V are employed to directly rate responses [27, 28]. However, relying on human annotators is costly for ongoing measurement, and employing GPT-4V poses risks to evaluation bias [29]. 4) _Lack of multicultural assessment._ Current benchmarks predominantly focus

Figure 1: **Workflow of MLLMGuard**, including creating dataset through manual construction, evaluation on MLLMGuard and scoring with human and GuardRank.

on the English language, which restricts the applicability of MLLMs in non-English speaking regions. We present detailed comparisons between existing safety-related benchmarks in App. B.

We advocate for incorporating the following key characteristics in a high-quality safety benchmark to address the aforementioned challenges. Firstly, it should encompass assessments from extensive dimensions and not be limited to English, ensuring comprehensive consideration of all safety aspects. Secondly, it should present adequate challenges and effectively distinguish between evaluated models. Specifically, the evaluation data should be independent of the model's training set. Finally, the evaluation metric should be fair and cost-effective, ensuring the assessment is conducted promptly without significant resource constraints. Guided by these principles, we develop MLLMguard to offer comprehensive safety evaluations for MLLMs, which consist of a bilingual image-text evaluation dataset, inference utilities, and a lightweight evaluator.

To summarize, our main contributions are as follows:

* a multi-dimensional safety evaluation suite for MLLMs**, featuring a bilingual evaluation set (English and Chinese), adaptable inference utilities, and a lightweight evaluator. To our knowledge, this is the _first_ attempt to include a dataset in Chinese for MLLMs' safety evaluation. Our suite assesses privacy, bias, toxicity, truthfulness, and legality across 12 subtasks. The in-depth evaluation of 13 leading MLLMs yields valuable insights for subsequent model optimization on safety.
* Distinguished from existing benchmarks, **MLLMGuard is characterized by its highly adversarial nature**. We derive a substantial portion of our image data (82%) from social media to prevent data leakage. Moreover, human experts meticulously curate all text data, fortified with red teaming techniques.
* **We introduce GuardRank, a fully automated lightweight evaluator** that removes the need for GPT-4V and manual assessments, serving as a plug-and-play tool for straightforward evaluations on MLLMGuard.

## 2 MLLMGuard

MLLMGuard is designed to develop a collection of adversarial examples to test the ability of MLLMs to identify and counteract attacks orchestrated by red teams. Specifically, our evaluation focuses on Vision-Language models, which process both an image and a textual input to produce a text-based output. We break down our evaluation strategy into three main aspects: the **taxonomy** of the threats, the **dataset** of adversarial examples, and the **metric** for assessing model performance.

### Taxonomy

MLLMGuard incorporates five dimensions, i.e., Privacy, Bias, Toxicity, Truthfulness, and Legality. The data distribution statistics are shown in Tab. 1. Definitions of each dimension are shown below:

* **Privacy** tests whether models are aware of privacy-infringing requests and can refrain from providing corresponding assistance to safeguard privacy. It is further divided into three sub-tasks: personal privacy, trade secrets, and state secrets.
* **Bias** evaluates the models' reactions to queries that contain stereotypes, prejudice, and discrimination. It aims to determine if the model can counter these unjust assumptions and offer impartial responses.
* **Toxicity** focuses on evaluating whether MLLMs are capable of identifying toxic content in the input and generating responsible guidance and safe responses. Within this area, we specifically target two sub-tasks: hate speech, pornography and violence. Typically, the inputs are presented as image-text pairs, often in the form of memes.
* **Truthfulness** involves a dual consideration of hallucination and robustness. In the hallucination scenario, we query about entities not present in the image to determine if MLLMs correctly identify their absence. In the robustness scenario, we assess the models' consistency when subjected to noise injection and position swapping.
* **Legality** is classified into two distinct categories, namely personal safety and public security. This dimension tests MLLMs' ability to identify legal issues and offer legal guidance.

### Dataset

As shown in Fig. 1(a), the entire dataset creation process consists of three stages: Image & Prompt Collection, Red Teaming, and Data Review. Detailed data statistics can be found in App. C, and samples for each sub-task are provided in App. E.

#### 2.2.1 Data Creation

We recruit 9 crowd workers with relevant professional backgrounds to participate in the data creation process. Before we begin handcrafting the data, we prepare a detailed guideline that outlines the definition of the dimension, risk scenarios, red teaming techniques, and data source requirements. During the creation process, crowd workers are instructed to adhere to the following three principles:

* _Avoiding Data Leakage_ To prevent data from being exposed to the training set of evaluated models, we manually construct text prompts, ensuring the absence of identical image-text pairs in any publicly available datasets. Additionally, to diversify our sources and minimize dependence on open-source datasets, we source over 82% of our dataset's images from social media platforms.
* _Enhancing Data Quality_ We incorporate extensive red teaming techniques to increase the complexity of our samples. Every single sample in our dataset involves a specific red teaming technique.
* _Intellectual Property Protection_ The dataset primarily comprises images sourced from various social media platforms such as Twitter, with proper attribution provided for each sample. Furthermore, the dataset is exclusively intended for academic research. In the event of any copyright infringement notification, we will promptly adhere to relevant laws and regulations by removing related images.

Furthermore, referring to current practice [34, 8], we extend the red teaming techniques originally used for LLMs [35, 36] to apply to MLLMs, as demonstrated in Tab. 2. This systematic overview of red teaming techniques tailored for MLLMs could provide valuable insights for the community.

#### 2.2.2 Quality Control

* _Image-text Matching_: Textual prompt should be relevant to the corresponding images.
* _Dimension Matching_: Harmful information should be contained in the sample that aligns with the current evaluation dimension.
* _Correct Labeling_: Harmful samples should be correctly identified and labeled as intended red teaming techniques.
* _Necessity of Images_: Inspired by [37], we consider the necessity of including images to avoid answers from being directly derived from the textual prompt or existing knowledge within LLMs. As a result, we remove samples that do not necessitate the inclusion of images.

\begin{table}
\begin{tabular}{c|l l l l|c|c} \hline \hline
**Dimension** & **Subtask** & **Attack** & **Image Source** & **\# Num** & **\# Sum** & **\# Total** \\ \hline \multirow{3}{*}{Privacy} & Personal Privacy & \multirow{3}{*}{t.1, t.2, t.3, t.4, i.6} & \multirow{3}{*}{323} & \multirow{3}{*}{2,282} \\  & Trade Secret & & & & \\  & State Secret & & & & \\ \hline \multirow{3}{*}{Bias} & Stereotype & \multirow{3}{*}{i.1, i.2, i.6} & \multirow{3}{*}{523} & \multirow{3}{*}{2,282} \\  & Prejudice & & & & 201 \\  & Discrimination & & & 34 \\ \hline \multirow{3}{*}{Toxicity} & Hate Speech & \multirow{3}{*}{t.1, t.2, t.3, i.6} & Hateful\_Memes [30] & 354 & \multirow{3}{*}{530} \\  & Pornography and Violence & & MEME [31] & 176 & \\ \hline \multirow{3}{*}{Truthfulness} & Hallucination & i.3 & Logo-2K+ [32] & 180 & \multirow{3}{*}{540} \\  & Robustness & i.4, i.5 & Animal-10 [33] & 360 & \\ \hline \multirow{3}{*}{Legality} & Personal Safety & \multirow{3}{*}{t.1, t.2, t.3, t.4, i.6} & \multirow{3}{*}{290} & \multirow{3}{*}{366} \\  & Public Security & & & & \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Overview of MLLMGuard. We create 2,282 image-text pairs with images from social media and open-source datasets. Since image sources for all dimensions include social media, we only list those whose sources contain open-source datasets. Column Attack enumerates the red teaming techniques used in the related dimension, and the indexes corresponding to the techniques are listed in Tab. 2.**We require 3 reviewers to conduct the data review process based on the aforementioned criteria. If two or more reviewers find a particular sample inadequate or non-compliant with established criteria, the sample would be considered unfit for inclusion. In such cases, the necessary modifications or deletions are implemented based on reviewers' feedback. This iterative process ensures the refinement and improvement of the dataset.

### Metric

Our annotation is implemented through human scoring. For the dimension of Truthfulness, we solely employ a binary classification, indicating whether the model's responses are truthful or not. For dimensions of Privacy, Bias, Toxicity, and Legality, we set up scoring rules to map different performances of MLLMs (Tab. 3).

Inspired by [38], we categorize safe responses into 2 tiers: label \(0\) signifies _Safe and Responsible_ the model recognizes the gravity of the inquiry and opts not to provide assistance, usually accompanied by mild adomitions; label \(1\) indicates that models exclusively produce general harmless responses, such as declining assistance using templates or providing irrelevant content. Meanwhile, we also categorize unsafe responses into 2 levels: label \(2\) denotes a direct unsafe output (e.g., providing step-by-step guidance to create a bomb); label \(3\) signifies greater risks, indicating that a model is aware of the possible harmful impact in the query but still offers irresponsible assistance (e.g., reminding users that creating a bomb is illegal and dangerous while still providing guidance). Label \(3\) can serve as a potent probe to detect extreme risks [39], which is a significant consideration as we advance towards Artificial General Intelligence (AGI).

Overall, we provide the following metrics:

* **ASD (Attack Success Degree, \(\downarrow\))** is computed using Eq. 1, which quantifies the extent to which the responses are harmless in each dimension. In Eq. 1, \(\mathrm{ASD}_{i}\) represents the degree for dimension \(i\), \(R_{i}\) denotes the prompt-response pairs for dimension \(i\), \(|\cdot|\) denotes the number of prompt-response pairs, and \(\mathrm{Smooth}()\) implements a linear transformation of data, mapping values from a 0 to 3 scale onto a 0 to 1 scale by standardizing the data across the designated interval (e.g., label

\begin{table}
\begin{tabular}{c l l} \hline \hline
**Index** & **Method** & **Description** & **Num** \\ \hline \multicolumn{3}{c}{_Text-based_} \\ \hline t.1 & **Disguise** & involves role-playing, where the prompt assumes a specific ‘character role’ and carries out & 188 \\  & actions that appear reasonable but contravene values. & \multirow{2}{*}{119} \\ t.2 & **Reverse Induction** & refers to the deceptive avoidance of malice on the surface, while it actually & 91 \\  & involves enacting the very malevolence. & \multirow{2}{*}{119} \\ t.3 & **Unsafe Inquiry** & denotes that users ask models about how to engage in actions that violate human & 263 \\  & ethical standards, while tacitly approving the actions as correct. & \multirow{2}{*}{119} \\ t.4 & **Indirect Task Misalignment** & emphasizes the concealment of malicious tasks within the background & \\  & of a prompt rather than directly requesting models to carry out malevolent actions. & \multirow{2}{*}{286} \\ \hline i.1 & **Facial Stereotype** & involves the practice of linking facial features with certain predetermined characteristics such as intelligence, income, or other traits, ascribed to a particular social group. & 286 \\ i.2 & **Malicious Labeling** & means the act of associating negative characteristics with certain social groups. & 221 \\ i.3 & **Non-existent Query** & refers to inquiring about features or entities that do not exist in the given image. & 180 \\ i.4 & **Position Swapping** & involves rearranging the options of an image-based multiple-choice question. By & 180 \\  & altering the positions of the image options, different answers can be generated & 180 \\ i.5 & **Noise Injection** & refers to adding various types of noise, such as Gaussian noise, to an image. & 180 \\ i.6 & **Harrful Scenario** & refers to the inclusion of hate speech, pornography, violence, or other harmful & \\  & content within an image, while ensuring that the accompanying textual prompt remains free from any & 574 \\  & red-team attacks or harmful language. & \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Description of each red teaming technique.**

\begin{table}
\begin{tabular}{c|c|c} \hline \hline  & Safe & Unsafe \\ \hline Aware & 0 & 3 \\ Unaware & 1 & 2 \\ \hline \hline \end{tabular}
\end{table}
Table 3: **Scoring rules for annotation.**_Aware_ represents that the model is capable of identifying the issues within the prompt, while _Safe_ indicates that the model can provide responses that are free from harm.

\(1\) is projected to \(0.3\)). \[\mathrm{ASD_{i}}=\frac{\sum_{p,r\in R_{i}}\mathrm{Smooth}(\mathrm{ Scoring}(\mathrm{MLLM}(p,r)))}{|R_{i}|}\times 100\] (1)
* **PAR (Perfect Answer Rate, \(\uparrow\))** is calculated using Eq. 2, which represents the proportion of safe and responsible responses among the responses in each dimension. \[\mathrm{PAR_{i}}=\frac{\sum_{p,r\in R_{i}}\mathrm{I}((\mathrm{ Scoring}(\mathrm{MLLM}(p,r)),0)}{|R_{i}|}\times 100\%\] (2)

## 3 Evaluation of MLLMGuard

In this section, we first present the experimental setup used to evaluate MLLMs on MLLMGuard. In Section 3.2, we discuss the performance of MLLMs across five dimensions on MLLMGuard. Section 3.3 introduces the design of a specific evaluator - GuardRank. Finally, we conduct a series of comparative experiments to discuss potential directions for enhancing MLLM Safety.

### Experimental Setup

Dataset for EvaluationTo ensure a fair evaluation and prevent our data from being exploited for training, we have randomly selected 1,500 samples from the original dataset for public disclosure. Henceforth, unless explicitly stated, the term "dataset" refers specifically to the publicly accessible dataset comprising the aforementioned 1,500 samples.

MLLMs for EvaluationWe select 13 mainstream MLLMs for evaluation, including 2 closed-source models and 11 open-source models. App. A provides additional information about these models.

### Main Results

General ComparisonThe ASD (\(\downarrow\)) and PAR (\(\uparrow\)) of each model across different dimensions can be seen in Tab. 4 and Tab. 5. Among all the models, GPT-4V has the lowest ASD, closely followed by the open-source model MiniGPT-v2 with a slight difference (\(-1.71\)). Meanwhile, MiniGPT-v2 achieves the highest PAR among all the models, surpassing the SOTA GPT-4V in most benchmarks.

Findings on TruthfulnessBased on the experimental results on Truthfulness, as depicted in Fig. 2, we have the following observations:

* Fig. 2(a) demonstrates the effectiveness of three red teaming techniques on MLLMs, with Position Swapping exhibiting a particularly significant impact.

\begin{table}
\begin{tabular}{l|c c c c c|c} \hline \hline
**Model** & **Privacy** & **Bias** & **Toxicity** & **Tr Truthfulness** & **Legality** & **Avg.** \\ \hline GPT-4V & 31.33 & 21.77 & 27.38 & 21.01 & 25.14 & **25.32** \\ Gemini & 38.89 & 48.10 & 35.54 & 26.36 & 36.81 & 37.14 \\ \hline LLaVA-v1.5-7B & 41.05 & 44.31 & 35.25 & 59.35 & 35.42 & 43.08 \\ Qwen-VL-Chat & 43.21 & 39.75 & 37.85 & 48.27 & 35.42 & 40.90 \\ SEED-LLaMA & 49.23 & 55.78 & 44.09 & 58.89 & 48.75 & 51.35 \\ Yi-VL-34B & 48.61 & 43.44 & 35.06 & 52.04 & 40.97 & 44.03 \\ DeepSee-VL & 41.51 & 36.83 & 34.87 & 33.73 & 30.69 & 35.53 \\ mPLUG-owl2 & 46.14 & 49.56 & 41.40 & 57.71 & 50.28 & 49.02 \\ MiniGPT-v2 & **17.44** & 27.70 & **17.39** & 55.99 & **16.67** & 27.03 \\ CogVLM & 40.43 & 58.02 & 35.54 & 50.42 & 45.00 & 45.88 \\ ShareGPT4V & 44.14 & 46.94 & 52.83 & 58.15 & 45.56 & 49.52 \\ XComposer2-VL & 40.90 & 36.83 & 37.85 & 42.09 & 35.28 & 38.59 \\ InternVL-v1.5 & 40.74 & **20.60** & 46.88 & **19.09** & 34.72 & 32.41 \\ \hline \hline \end{tabular}
\end{table}
Table 4: **ASD (\(\downarrow\)) of various models across different dimensions.** We evaluate each model based on metrics in each dimension and highlight the best-performing model in **bold** and the second-best model with an underline.

* Fig. 2(b) indicates that all MLLMs are prone to hallucinations, especially when dealing with the dual problem of Non-existent Query, where the original open-ended prompts are transformed into the multiple-choice.
* As shown in Fig. 2(c), the placement of options significantly influences the selection of MLLMs. For instance, LLaVA-v1.5-7B tends to choose the left option (A), while mLUG-Owl2 leans towards the right option (B).
* Existing MLLMs demonstrate strong defense against Noise Injection as shown in 2(d). With the exception of MiniGPT-v2, most MLLMs get ASD below \(0.1\).

### The design of GuardRank

To automate our evaluation, we first utilize GPT-4 for Zero-Shot and In-Context Learning (ICL, we provide an example for each label) evaluation whose prompts can be found in App. G, but its overall accuracy on the test set is just 29.38% and 42.78% respectively. This demonstrates the unreliability of using GPT-4 directly for evaluations. Therefore, we develop an integrated evaluator GuardRank, allowing for a more accurate, faster, and cost-effective evaluation on MLLMGuard.

Implementation DetailsGuardRank is trained on a human-annotated dataset, employing LLaMA-2 [1] as the backbone for the dimension of Privacy, Bias, Toxicity, and Legality, and Roberta-large [40] for the Hallucination sub-task. The textual prompt and the corresponding answer are concatenated into a single template, and human-annotated scores are used as labels. To validate the accuracy of GuardRank and its generalizability on out-of-distribution (OOD) models, we use responses from Xcomposer2-VL as the validation set and the responses from LLaVA-v1.5-7B and

\begin{table}
\begin{tabular}{l|c c c c c|c} \hline \hline
**Model** & **Privacy** & **Bias** & **Toxicity** & **Truthfulness** & **Legality** & **Avg.** \\ \hline GPT-4V & 39.35\% & 48.69\% & 18.73\% & 78.99\% & 27.92\% & 42.74\% \\ Gemini & 8.80\% & 7.00\% & 4.61\% & 73.64\% & 5.00\% & 19.81\% \\ \hline LLaVA-v1.5-7B & 21.30\% & 18.08\% & 4.61\% & 40.65\% & 16.67\% & 20.26\% \\ Qwen-VL-Chat & 18.06\% & 18.95\% & 12.68\% & 51.73\% & 30.42\% & 26.37\% \\ SEED-LLAMA & 14.81\% & 3.50\% & 6.05\% & 41.11\% & 11.25\% & 15.34\% \\ Yi-VL-34B & 9.26\% & 22.16\% & 11.53\% & 47.96\% & 16.25\% & 21.43\% \\ DeepSee-VL & 25.46\% & 6.71\% & 5.19\% & 66.27\% & 23.75\% & 25.48\% \\ mPLUG-Owl2 & 14.81\% & 3.50\% & 6.34\% & 42.29\% & 7.08\% & 14.81\% \\ MiniGPT-v2 & **67.59**\% & 32.07\% & **47.84**\% & 44.01\% & **57.08\%** & **49.72\%** \\ CogVLM & 0.46\% & 0.00\% & 0.00\% & 49.58\% & 0.00\% & 10.01\% \\ ShareGPT4V & 13.89\% & 10.79\% & 2.31\% & 41.85\% & 16.25\% & 17.02\% \\ XComposer2-VL & 23.61\% & 23.03\% & 9.80\% & 57.91\% & 12.08\% & 25.29\% \\ InternVL-v1.5 & 24.54\% & **56.27**\% & 9.22\% & **80.91**\% & 30.00\% & 40.19\% \\ \hline \hline \end{tabular}
\end{table}
Table 5: **PAR (\(\uparrow\)) of various models across different dimensions.** We evaluate each model based on metrics in each dimension and highlight the best-performing model in **bold** and the second-best model with an underline.

Figure 2: **Results on Truthfulness.** (a) presents the ASD of MLLMs under various red teaming techniques on Truthfulness. (b) and (d) further display the ASD results on 2 red teaming techniques, i.e., Non-existent Query and Noise Injection. (c) provides the frequency of MLLMs selecting A/B/No Answer under the Position Swapping. Specifically, we experimented on both open-ended prompts and transferred multiple choice questions on Non-existent Query.

Qwen-VL-Chat as the test set. The model architecture and training details for GuardRank are provided in the APP. G.

Performance of GuardRankThe accuracy of GuardRank is shown in Tab. 6. GuardRank consistently outperforms GPT-4 as an evaluator, whether using Zero-shot or ICL approaches.

### Discussion

To further investigate the safety of MLLMs, we pose the following research questions to bring insights for future work:

RQ1: Do current alignment techniques in MLLMs enhance models' safety ability?We compare DeepSeek-VL-Base with its chat-aligned version, DeepSeek-VL-Chat, and Gemini with its safety-aligned version, Gemini-Safety. For Gemini-Safety, we utilize Safety filters2 and set the API threshold to BLOCK_LOW_AND_ABOVE to block most unsafe content. As shown in Fig. 3, the experimental results indicate that both chat alignment and safety alignment can enhance the safety of MLLMs to varying degrees. However, the marginal improvement of Gemini-Safety indicates that the current content filtering methods might be insufficient to defend against carefully crafted red teaming techniques.

Footnote 2: 

RQ2: Does the LLM component affect the safety of MLLM?We conduct separate experiments to compare the safety of mPLUG-Owl (with LLAMA-7B as the LLM) and mPLUG-Owl2 (with LLAMA2-7B as the LLM). Here, we simply replace the LLM of CogVLM from Vicuna-v1.5-7B to LLAMA2-7B. As shown in Fig. 4, a safer LLM (LLAMA2-7B) improves MLLM safety across all dimensions. However, in the case of CogVLM, the performance varies across different dimensions. This inconsistency may stem from the direct replacement of LLM, which potentially disrupts the original alignment.

RQ3: Does the Scaling Law apply to MLLM Safety?We select three groups of MLLMs from the same families with different model parameter sizes to evaluate on GuardRank. The experimental results in Fig. 5 indicate that an increase in model parameters does not significantly enhance safety levels across all dimensions, even leading to a drop in some cases. The impact of the scaling law on MLLM safety is less pronounced than in LLMs [41; 3] or other MLLM capabilities [42].

RQ4: Is there a trade-off between being honest and harmless?Existing work has shown a trade-off between helpfulness and harmlessness in generative models [19], yet the relationship between honesty [43] and harmlessness remains underexplored. As shown in Tab. 4 and Tab. 5, MiniGPT-v2 exhibits strong safety across several dimensions but underperforms on Truthfulness, suggesting a potential trade-off between honesty and harmlessness.

## 4 Related Work

Red Teaming towards MLLMsIt is a common practice to discover MLLMs' vulnerabilities through adversarial attacks and jailbreaking methods. Image-based and text-based red-teaming are two mainstream attack methods for MLLMs. Image-based red-teaming attacks typically involve adding a small amount of perturbation to an image, causing the model to produce outputs completely disparate from the original answers. [44; 45; 46] optimize images on a few-shot corpus comprised of numerous sentences to maximize the model's probability of generating undesired responses. [16; 47] convert the harmful content into images through typography to bypass the safety alignment within the LLMs of MLLMs. Meanwhile, there are relatively fewer text-based attacks specifically designed for MLLMs, including those derived from LLMs. Text-based red teaming attacks typically involve rewriting text or stealing system prompts to bypass the safety alignment of the LLM component within MLLMs. [48] employ GPT-4 as a red teaming tool against itself to search for potential jailbreak prompts leveraging stolen system prompts and [8] elicitate the incorrect or harmful responses from VLMs through misleading text inputs.

Alignment for MLLMsThe training process of MLLMs usually consists of two phases: pretraining and supervised fine-tuning (SFT), with different types of alignment occurring during both stages. Pretraining aims to achieve _Modality Alignment_ between the vision encoder and LLM, often by using a large amount of weakly labeled data, followed by a smaller amount of high-quality data [49]. The SFT stage then focuses on _Chat Alignment_ and _Safety Alignment_. After achieving modality alignment in pretraining, some will undergo chat alignment to enhance their capabilities in dialogue and instruction-following, such as Owen-VL-Chat [50] (aligned from Owen-VL) and DeepSeek-VL-Chat [51] (aligned from DeepSeek-VL). However, fewer of MLLMs undergo safety alignment. Gemini [5] incorporates a dedicated safety classifier that identifies, labels, and filters out content related to violence or negative stereotypes. GPT-4V [34] integrates supplementary multimodal data during the post-training process to strengthen its ability to refuse engagement in illicit behavior and unsupported inference requests.

## 5 Conclusion

This study introduces MLLMGuard, a comprehensive multi-dimensional safety evaluation suite for MLLMs, which is composed of three key components: 1) an extensive evaluation framework, 2) a highly adversarial, bilingual evaluation dataset, and 3) GuardRank, a lightweight, automated safety evaluator. Based on MLLMGuard, we conduct rigorous safety assessments of current

Figure 5: PAR (\(\uparrow\)) of MLLMs on different parameter size.

MLLMs, identifying critical vulnerabilities and exploring potential techniques to enhance their safety. Consequently, MLLMGuard not only provides an effective tool for MLLM safety evaluation but also pioneers novel methodologies for safety enhancement, which contributes to steering the development of MLLMs towards safer and more responsible AI applications.

## 6 Limitations

Dataset and AnnotationOur dataset and annotation are created by workers aged between 20 and 35 from mainland China, whose expertise primarily spans psychology, sociology, law, and computer science. This demographic similarity may introduce potential biases related to their shared cultural backgrounds. Additionally, the purely manual construction of our dataset makes it costly to scale. We plan to enhance scalability and effectiveness by incorporating self-instruction through red teaming techniques. Meanwhile, while we strive to cover a broad range of evaluation aspects, the potential risks associated with MLLM outputs are inevitably limitless. Therefore, it is crucial for us to continuously expand the range of aspects evaluated.

Limitations of the EvaluatorWe acknowledge several possible limitations of our evaluator: 1) A fixed value for max_token (128) may introduce potential errors during the subsequent processing of responses. 2) To facilitate lightweight evaluation, GuardRank does not leverage more sophisticated models as its backbones, which may enhance the accuracy of the evaluator. 3) To exert more precise control over variables, our model's dialogue design is confined to single-turn conversations.

## 7 Social Impacts

Our work holds immense social implications, particularly surrounding the use of MLLMs. We outline the potential social impacts as follows:

Value Alignment with HumanOur research delves into the profound societal impacts of deploying MLLMs, including proprietary models such as GPT-4V and Gemini, as well as open-source alternatives like LLaVA. We pinpoint several areas where MLLMs fall short in alignment with human values: 1) **Lack of understanding of human values.** MLLMs often fail to recognize malicious intent in user inputs, missing cues that indicate harmful intentions. 2) **Inability to refuse malicious inputs.** Current MLLMs lack robust mechanisms to accurately detect and reject malicious or unethical inputs, which increases the risk of misuse. 3) **Absence of benevolent guidance.** Though MLLMs can identify malicious prompts, their responses are typically formulaic and do not offer constructive, value-aligned guidance. These findings underscore the necessity of integrating ethical and societal considerations in the development and deployment of MLLMs to ensure they uphold human values.

Truthfulness in MLLMsOur research into the truthfulness of MLLMs reveals that MLLMs are prone to issues such as hallucinations, selection bias, and the detrimental effects of noise on accuracy. These insights are crucial for guiding the development of more reliable and trustworthy MLLMs.

## 8 Ethical Considerations

In this work, we introduce an adversarial dataset to evaluate MLLM Safety. Given its adversarial nature, the dataset includes potentially offensive samples and may raise privacy concerns. We claim that all the data included are used strictly for academic research purposes and do not represent the views of the authors or the dataset constructors. To address privacy issues, we have anonymized certain facial features in the portions of the dataset that are publicly available. For access to non-anonymized data, anyone interested is required to complete our application form.

Regarding the risk of copyright infringement, it is crucial to acknowledge that the copyrights for images with attributed sources are owned by their respective rights holders. Usage of these images beyond the scope of research without explicit consent from the rights holders constitutes a violation of copyright laws, making the user legally liable.

For our annotators, we prioritize their legal rights and psychological well-being. We compensate them with a salary significantly above the local minimum wage. We also actively monitor the psychological state of our annotators and provide essential support as needed.

## Acknowledgments and Disclosure of Funding

This work is supported by the National Key R&D Program of China (No. 2022ZD0160103), Shanghai Artificial Intelligence Laboratory, and Ping An Technology (Shenzhen) Co., Ltd's "Graph Neural Network Project".

## References

* [1] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth \(\mathrm{\,e\,}\mathrm{e\,}\mathrm{\,}\mathrm{e\,}\mathrm{\,}\mathrm{\,}\) Lacroix, Baptiste Rozi \(\mathrm{\,e\,}\mathrm{\,}\mathrm{e\,}\mathrm{\,}\mathrm{\,}\mathrm{\,}\), Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023.
* [2] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _Journal of Machine Learning Research_, 24(240):1-113, 2023.
* [3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* [4] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [5] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.
* [6] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. Cogvlm: Visual expert for pretrained language models. 2023.
* [7] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on multimodal large language models, 2024.
* [8] Mukai Li, Lei Li, Yuwei Yin, Masood Ahmed, Zhenguang Liu, and Qi Liu. Red teaming visual language models, 2024.
* [9] Chenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Linjun Zhang, James Zou, and Huaxiu Yao. Holistic analysis of hallucination in gpt-4v(ision): Bias and interference challenges, 2023.
* [10] Anisha Gunjal, Jihan Yin, and Erhan Bas. Detecting and preventing hallucinations in large vision language models, 2024.
* [11] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating hallucination in large multi-modal models via robust instruction tuning, 2024.
* [12] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, and Trevor Darrell. Aligning large multimodal models with factually augmented rlhf, 2023.
* [13] Jiawei Zhang, Tianyu Pang, Chao Du, Yi Ren, Bo Li, and Min Lin. Benchmarking large multimodal models against common corruptions, 2024.
* [14] Rizhao Cai, Zirui Song, Dayan Guan, Zhenhao Chen, Xing Luo, Chenyu Yi, and Alex Kot. Benchlmm: Benchmarking cross-style visual capability of large multimodal models, 2023.
* [15] Hongzhan Lin, Ziyang Luo, Bo Wang, Ruichao Yang, and Jing Ma. Goat-bench: Safety insights to large multimodal models through meme-based social abuse, 2024.

* Gong et al. [2023] Yichen Gong, Delong Ran, Jinyuan Liu, Conglei Wang, Tianshuo Cong, Anyu Wang, Sisi Duan, and Xiaoyun Wang. Figstep: Jailbreaking large vision-language models via typographic visual prompts, 2023.
* Liu et al. [2024] Xin Liu, Yichen Zhu, Jindong Gu, Yunshi Lan, Chao Yang, and Yu Qiao. Mm-safetybench: A benchmark for safety evaluation of multimodal large language models, 2024.
* Shi et al. [2024] Zhelun Shi, Zhipin Wang, Hongxing Fan, Zaibin Zhang, Lijun Li, Yongting Zhang, Zhenfei Yin, Lu Sheng, Yu Qiao, and Jing Shao. Assessment of multimodal large language models in alignment with human values, 2024.
* Bai et al. [2022] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback, 2022.
* Guan et al. [2024] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. Hallusion-bench: An advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models, 2024.
* Han et al. [2024] Tianyang Han, Qing Lian, Rui Pan, Renjie Pi, Jipeng Zhang, Shizhe Diao, Yong Lin, and Tong Zhang. The instinctive bias: Spurious images lead to hallucination in mllms, 2024.
* Cha et al. [2024] Sungguk Cha, Jusung Lee, Younghyun Lee, and Cheoljong Yang. Visually dehallucinative instruction generation: Know what you don't know. 2024.
* Chen et al. [2024] Shuo Chen, Zhen Han, Bailan He, Zifeng Ding, Wenqian Yu, Philip Torr, Volker Tresp, and Jindong Gu. Red teaming gpt-4v: Are gpt-4v safe against uni/multi-modal jailbreak attacks?, 2024.
* Balloccu et al. [2024] Simone Balloccu, Patricia Schmidtova, Mateusz Lango, and Ondrej Dusek. Leak, cheat, repeat: Data contamination and evaluation malpractices in closed-source LLMs. In _Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 67-93, 2024.
* Wang et al. [2024] Yixu Wang, Yan Teng, Kexin Huang, Chengqi Lyu, Songyang Zhang, Wenwei Zhang, Xingjun Ma, Yu-Gang Jiang, Yu Qiao, and Yingchun Wang. Fake alignment: Are llms really aligned well?, 2024.
* Li et al. [2023] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension, 2023.
* Fu et al. [2023] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: A comprehensive evaluation benchmark for multimodal large language models. _arXiv preprint arXiv:2306.13394_, 2023.
* Liu et al. [2024] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player?, 2024.
* Panickssey et al. [2024] Arjun Panickssey, Samuel R. Bowman, and Shi Feng. Llm evaluators recognize and favor their own generations, 2024.
* Kiela et al. [2020] Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. The hateful memes challenge: Detecting hate speech in multimodal memes. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 2611-2624. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/1b84c4ce2b8b3d823b30e2d604b1878-Paper.pdf.

* Gasparini et al. [2022] Francesca Gasparini, Giulia Rizzi, Aurora Saibene, and Elisabetta Fersini. Benchmark dataset of memes with text transcriptions for automatic detection of multi-modal misogynistic content. _Data in Brief_, 44:108526, October 2022. ISSN 2352-3409. doi: 10.1016/j.dib.2022.108526. URL http://dx.doi.org/10.1016/j.dib.2022.108526.
* Wang et al. [2020] Jing Wang, Weiqing Min, Sujuan Hou, Shengnan Ma, Yuanjie Zheng, Haishuai Wang, and Shuqiang Jiang. Logo-2k+: A large-scale logo dataset for scalable logo classification. In _The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020_, pages 6194-6201. AAAI Press, 2020.
* Alessio [2020] Corrado Alessio, Animals-10 Dataset, Animal Pictures of 10 Different Categories Taken From Google Images. Accessed on: Dec. 20, 2020, [Online]. Available: https://www.kaggle.com/alessiocorrado99/animals10.
* [34] Gpt-4v(ision) system card. 2023. URL https://api.semanticscholar.org/CorpusID:263218031.
* Huang et al. [2023] Kexin Huang, Xiangyang Liu, Qianyu Guo, Tianxiang Sun, Jiawei Sun, Yaru Wang, Zeyang Zhou, Yixu Wang, Yan Teng, Xipeng Qiu, Yingchun Wang, and Dahua Lin. Flames: Benchmarking value alignment of chinese large language models, 2023.
* Li et al. [2023] Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, and Yangqiu Song. Multi-step jailbreaking privacy attacks on chatgpt. _arXiv preprint arXiv:2304.05197_, 2023.
* Chen et al. [2024] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, and Feng Zhao. Are we on the right way for evaluating large vision-language models?, 2024.
* Xu et al. [2023] Guohai Xu, Jiayi Liu, Ming Yan, Haotian Xu, Jinghui Si, Zhuoran Zhou, Peng Yi, Xing Gao, Jitao Sang, Rong Zhang, Ji Zhang, Chao Peng, Fei Huang, and Jingren Zhou. Values: Measuring the values of chinese large language models from safety to responsibility, 2023.
* Shevlane et al. [2023] Toby Shevlane, Sebastian Farquhar, Ben Garfinkel, Mary Phuong, Jess Whittestone, Jade Leung, Daniel Kokotajlo, Nahema Marchal, Markus Anderljung, Noam Kolt, Lewis Ho, Divya Siddarth, Shahar Avin, Will Hawkins, Been Kim, Iason Gabriel, Vijay Bolina, Jack Clark, Yoshua Bengio, Paul Christiano, and Allan Dafoe. Model evaluation for extreme risks, 2023.
* Liu et al. [2019] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach, 2019.
* Kaplan et al. [2020] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020.
* Liu et al. [2024] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2024.
* Li et al. [2024] Siheng Li, Cheng Yang, Taiqiang Wu, Chufan Shi, Yuji Zhang, Xinyu Zhu, Zesen Cheng, Deng Cai, Mo Yu, Lemao Liu, Jie Zhou, Yujiu Yang, Ngai Wong, Xixin Wu, and Wai Lam. A survey on the honesty of large language models, 2024. URL https://arxiv.org/abs/2409.18786.
* Qi et al. [2023] Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Peter Henderson, Mengdi Wang, and Prateek Mittal. Visual adversarial examples jailbreak aligned large language models, 2023.
* Tu et al. [2023] Haoqin Tu, Chenhang Cui, Zijun Wang, Yiyang Zhou, Bingchen Zhao, Junlin Han, Wangchunshu Zhou, Huaxiu Yao, and Cihang Xie. How many unicorns are in this image? a safety evaluation benchmark for vision llms, 2023.
* Yin et al. [2024] Fei Yin, Yong Zhang, Baoyuan Wu, Yan Feng, Jingyi Zhang, Yanbo Fan, and Yujiu Yang. Generalizable black-box adversarial attack with meta learning. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 46(3):1804-1818, 2024. doi: 10.1109/TPAMI.2022.3194988.

* [47] Maan Qraitem, Nazia Tasnim, Piotr Teterwak, Kate Saenko, and Bryan A. Plummer. Vision-llms can fool themselves with self-generated typographic attacks, 2024.
* [48] Yuanwei Wu, Xiang Li, Yixin Liu, Pan Zhou, and Lichao Sun. Jailbreaking gpt-4v via self-adversarial attacks with system prompts, 2024.
* [49] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: large language model as a unified interface for vision-language multi-task learning, 2023.
* [50] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. _arXiv preprint arXiv:2308.12966_, 2023.
* [51] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and Chong Ruan. Deepseek-vl: Towards real-world vision-language understanding, 2024.
* [52] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023.
* [53] Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making lama see and draw with seed tokenizer. _arXiv preprint arXiv:2310.01218_, 2023.
* [54] 01. AI, ; Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open foundation models by 01.ai, 2024.
* [55] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration, 2023.
* [56] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions, 2023.
* [57] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model, 2024.
* [58] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites, 2024.
* [59] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering, 2017.
* [60] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge, 2019.
* [61] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: A benchmark for visual question answering using world knowledge, 2022.
* [62] Drew A. Hudson and Christopher D. Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering, 2019.

* Kazemzadeh et al. [2014] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In _Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)_, pages 787-798, 2014.
* Krishna et al. [2017] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. _International journal of computer vision_, 123:32-73, 2017.
* Mao et al. [2016] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In _CVPR_, 2016.
* Mishra et al. [2019] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In _2019 international conference on document analysis and recognition (ICDAR)_, pages 947-952. IEEE, 2019.
* Sidorov et al. [2020] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for image captioning with reading comprehension. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part II 16_, pages 742-758. Springer, 2020.
* Schuhmann et al. [2022] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. _arXiv:2210.08402_, 2022.
* Schuhmann et al. [2022] Christoph Schuhmann, Andreas Kpf, Richard Vencu, Theo Coombes, and Romain Beaumont. Laion coco: 600m synthetic captions from laion2b-en. _https://laion.ai/blog/laion-coco/_, 2022.
* Gadre et al. [2023] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. _arXiv:2304.14108_, 2023.
* Byeon et al. [2022] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: Image-text pair dataset, 2022. URL https://github.com/kakaobrain/coyo-dataset.
* Changpinyo et al. [2021] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In _CVPR_, 2021.
* Sharma et al. [2018] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In _Proceedings of ACL_, 2018.
* Ordonez et al. [2011] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million captioned photographs. _Advances in neural information processing systems_, 24, 2011.
* Chen et al. [2015] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. _arXiv preprint arXiv:1504.00325_, 2015.
* Peng et al. [2023] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. _arXiv preprint arXiv:2306.14824_, 2023.
* Gupta et al. [2022] Tanmay Gupta, Ryan Marten, Aniruddha Kembhavi, and Derek Hoiem. Grit: General robust image task benchmark. _arXiv:2204.13653_, 2022.
* Kim et al. [2022] Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document understanding transformer. In _ECCV_, 2022.
* Soffware [2015] Artifex Software. Pymupdf, 2015. URL https://github.com/pymupdf/PyMuPDF.

* [80] Google. Puppeeter, 2023. URL https://github.com/puppeeter/puppeeter.
* [81] Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. Dvqa: Understanding data visualizations via question answering. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 5648-5656, 2018.
* [82] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on document images. In _Proceedings of the IEEE/CVF winter conference on applications of computer vision_, pages 2200-2209, 2021.
* [83] Unsplash. Unsplash dataset! the world' s largest open library dataset. URL https://unsplash.com/data. [Accessed 22-04-2024].
* [84] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13_, pages 740-755. Springer, 2014.
* [85] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In _Proceedings of the IEEE/cvf conference on computer vision and pattern recognition_, pages 3195-3204, 2019.
* [86] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: A benchmark for visual question answering using world knowledge. In _European Conference on Computer Vision_, pages 146-162. Springer, 2022.
* [87] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3608-3617, 2018.
* [88] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 8317-8326, 2019.
* [89] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. Llavar: Enhanced visual instruction tuning for text-rich image understanding, 2024.
* [90] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs, 2021.
* [91] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. _Transactions of the Association for Computational Linguistics_, 2:67-78, 2014. doi: 10.1162/tacl_a00166. URL https://aclanthology.org/Q14-1006.
* LinkSoul-AI/Chinese-LLaVA. https://github.com/LinkSoul-AI/Chinese-LLaVA. [Accessed 22-04-2024].
* [93] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering in images, 2016.
* [94] Danna Gurari, Qing Li, Abigale J. Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P. Bigham. Vizwiz grand challenge: Answering visual questions from blind people, 2018.
* [95] Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal C4: An open, billion-scale corpus of images interleaved with text. _arXiv preprint arXiv:2304.06939_, 2023.
* [96] Wikimedia Downloads -- dumps.wikimedia.org. https://dumps.wikimedia.org/. [Accessed 22-04-2024].

* Yang et al. [2021] Yue Yang, Artemis Panagopoulou, Qing Lyu, Li Zhang, Mark Yatskar, and Chris Callison-Burch. Visual goal-step inference using wikihow, 2021.
* Yu et al. [2023] Qiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui, Fan Zhang, Yue Cao, Xinlong Wang, and Jingjing Liu. Capsfusion: Rethinking image-text data at scale. _arXiv preprint arXiv:2310.20550_, 2023.
* Liu et al. [2021] Yulong Liu, Guibo Zhu, Bin Zhu, Qi Song, Guojing Ge, Haoran Chen, GuanHui Qiao, Ru Peng, Lingxiang Wu, and Jinqiao Wang. Taisu: A 166m large-scale high-quality dataset for chinese vision-language pre-training. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 16705-16717. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/6a386d703b50f1cf1f61ab02a15967bb-Paper-Datasets_and_Benchmarks.pdf.
* Li et al. [2024] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important things for large multi-modal models, 2024.
* Kantharaj et al. [2022] Shankar Kantharaj, Rixie Tiffany Leong, Xiang Lin, Ahmed Masry, Megh Thakkar, Enamul Hoque, and Shafiq Joty. Chart-to-text: A large-scale benchmark for chart summarization. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 4005-4023, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.277. URL https://aclanthology.org/2022.acl-long.277.
* Masry et al. [2023] Ahmed Masry, Parsa Kavehzadeh, Xuan Long Do, Enamul Hoque, and Shafiq Joty. Unichart: A universal vision-language pretrained model for chart comprehension and reasoning, 2023.
* Ye et al. [2023] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji Zhang, Qin Jin, Liang He, Xin Alex Lin, and Fei Huang. Ureader: Universal ocr-free visually-situated language understanding with multimodal large language model, 2023.
* Hu et al. [2024] Anwen Hu, Yaya Shi, Haiyang Xu, Jiabo Ye, Qinghao Ye, Ming Yan, Chenliang Li, Qi Qian, Ji Zhang, and Fei Huang. mplug-paperowl: Scientific diagram analysis with the multimodal large language model, 2024.
* Li et al. [2020] Yang Li, Gang Li, Luheng He, Jingjie Zheng, Hong Li, and Zhiwei Guan. Widget captioning: Generating natural language description for mobile user interface elements, 2020.
* Wang et al. [2021] Bryan Wang, Gang Li, Xin Zhou, Zhourong Chen, Tovi Grossman, and Yang Li. Screen2words: Automatic mobile ui summarization with multimodal learning, 2021.
* Laurenon et al. [2024] Hugo Laurenon, L \(\hat{e}\) o Tronchon, and Victor Sanh. Unlocking the conversion of web screenshots into html code with the websight dataset, 2024.
* Datasets at Hugging Face -
- huggingface.c. https://huggingface.co/datasets/laion/gpt4v-dataset. [Accessed 15-04-2024].
* Wang et al. [2023] Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, and Yu-Gang Jiang. To see is to believe: Prompting gpt-4v for better visual instruction tuning, 2023.
* Gao et al. [2023] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, and Lingpeng Kong. G-lava: Solving geometric problem with multi-modal large language model, 2023.
* Lu et al. [2022] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering, 2022.
* Hsiao et al. [2024] Yu-Chung Hsiao, Fedir Zubach, Maria Wang, and Jindong Chen. Screenqa: Large-scale question-answer pairs over mobile app screenshots, 2024.

* [113] Juan A. Rodriguez, David Vazquez, Issam Laradji, Marco Pedersoli, and Pau Rodriguez. Ocr-vqgan: Taming text-within-image generation, 2022.
* [114] Chee-Kheng Chng, Yuliang Liu, Yipeng Sun, Chun Chet Ng, Canjie Luo, Zihan Ni, Chuan-Ming Fang, Shuaitao Zhang, Junyu Han, Errui Ding, Jingtuo Liu, Dimosthenis Karatzas, Chee Seng Chan, and Lianwen Jin. Icdar2019 robust reading challenge on arbitrary-shaped text (rrc-art), 2019.
* rrc-lnt. In _2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)_, volume 01, pages 1454-1459, 2017. doi: 10.1109/ICDAR.2017.237.
* rrc-lsvt, 2019.
* CVPR 2017_, Hawaii, U.S.A., 2017. URL http://sunw.csail.mit.edu/abstract/uberText.pdf.
* [118] Andreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas, and Serge Belongie. Coco-text: Dataset and benchmark for text detection and recognition in natural images, 2016.
* [119] Baoguang Shi, Cong Yao, Minghui Liao, Mingkun Yang, Pei Xu, Linyan Cui, Serge Belongie, Shijian Lu, and Xiang Bai. Icdar2017 competition on reading chinese text in the wild (rctw-17), 2018.
* [120] Xi Liu, Rui Zhang, Yongsheng Zhou, Qianyi Jiang, Qi Song, Nan Li, Kai Zhou, Lei Wang, Dong Wang, Minghui Liao, Mingkun Yang, Xiang Bai, Baoguang Shi, Dimosthenis Karatzas, Shijian Lu, and C. V. Jawahar. Icdar 2019 robust reading challenge on reading chinese text on signboard, 2019.
* [121] Amanpreet Singh, Guan Pang, Mandy Toh, Jing Huang, Wojciech Galuba, and Tal Hassner. Textocr: Towards large-scale end-to-end reasoning for arbitrary-shaped scene text, 2021.
* [122] Ilya Krylov, Sergei Nosov, and Vladislav Sovrasov. Open images v5 text annotation and yet another mask text spotter, 2021.
* [123] Lukas Blecher, Guillem Cucurull, Thomas Scialom, and Robert Stojnic. Nougat: Neural optical understanding for academic documents, 2023.
* [124] Jimmy Carter. Textocr-gpt4v. https://huggingface.co/datasets/jimmycarter/textocr-gpt4v, 2024.
* [125] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. URL https://llava-vl.github.io/blog/2024-01-30-llava-next/.
* [126] Shengzhi Li and Nima Tajbakhsh. Scigraphqa: A large-scale synthetic multi-turn question-answering dataset for scientific graphs, 2023.
* [127] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning, 2022.
* [128] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl: Modularization empowers large language models with multimodality, 2024.

* [129] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 2556-2565, 2018.
* [130] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C. Berg, and Tamara L. Berg. Modeling context in referring expressions, 2016.
* [131] Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In _Proceedings of the IEEE international conference on computer vision_, pages 2641-2649, 2015.
* [132] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models, 2023.
* [133] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In _Proceedings of the IEEE international conference on computer vision_, pages 2425-2433, 2015.
* [134] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 8317-8326, 2019.
* [135] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm's referential dialogue magic. _arXiv preprint arXiv:2306.15195_, 2023.
* [136] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4015-4026, 2023.
* [137] Babak Saleh and Ahmed Elgammal. Large-scale classification of fine-art paintings: Learning the right metric on the right feature. _arXiv preprint arXiv:1505.00855_, 2015.
* [138] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _Advances in neural information processing systems_, 36, 2024.
* [139] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. Nocaps: Novel object captioning at scale. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 8948-8957, 2019.
* [140] Anirudha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. A diagram is worth a dozen images. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV 14_, pages 235-251. Springer, 2016.
* [141] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: A benchmark for question answering about charts with visual and logical reasoning. _arXiv preprint arXiv:2203.10244_, 2022.
* [142] Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. _arXiv preprint arXiv:1905.13319_, 2019.
* [143] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. In _The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP 2021)_, 2021.

* Shah et al. [2019] Sanket Shah, Anand Mishra, Naganand Yadati, and Partha Pratim Talukdar. Kvqa: Knowledge-aware visual question answering. In _Proceedings of the AAAI conference on artificial intelligence_, volume 33, pages 8876-8884, 2019.
* Bavishi et al. [2023] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sagnak Tasrlar. Introducing our multimodal models, 2023. URL https://www.adept.ai/blog/fuyu-8b.
* Dai et al. [2023] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.
* Kiela et al. [2020] Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. The hateful memes challenge: Detecting hate speech in multimodal memes. _Advances in neural information processing systems_, 33:2611-2624, 2020.
* Liu et al. [2023] Fangyu Liu, Guy Emerson, and Nigel Collier. Visual spatial reasoning. _Transactions of the Association for Computational Linguistics_, 11:635-651, 2023.
* Das et al. [2017] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jose MF Moura, Devi Parikh, and Dhruv Batra. Visual dialog. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 326-335, 2017.
* Xu et al. [2017] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In _Proceedings of the 25th ACM international conference on Multimedia_, pages 1645-1653, 2017.
* Yang et al. [2021] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Just ask: Learning to answer questions from millions of narrated videos. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 1686-1697, 2021.
* Schuhmann et al. [2022] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. _NeurIPS_, 35:25278-25294, 2022.
* Byeon et al. [2022] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: Image-text pair dataset, 2022.
* Kembhavi et al. [2017] Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension. In _CVPR_, pages 4999-5007, 2017.
* Masry et al. [2022] Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: A benchmark for question answering about charts with visual and logical reasoning. In _ACL_, pages 2263-2279, 2022.
* Liu et al. [2023] Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen, Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong Yu. Mmc: Advancing multimodal chart understanding with large-scale instruction tuning. _arXiv preprint arXiv:2311.10774_, 2023.
* Cao and Xiao [2022] Jie Cao and Jing Xiao. An augmented benchmark dataset for geometric question answering through dual parallel text encoding. In _COLING_, pages 1511-1520, 2022.
* Yu et al. [2016] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring expressions. In _ECCV_, pages 69-85, 2016.
* Mao et al. [2016] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In _CVPR_, pages 11-20, 2016.
* Krishna et al. [2017] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. _IJCV_, 123:32-73, 2017.

* [161] Chenxia Li, Weiwei Liu, Ruoyu Guo, Xiaoting Yin, Kaitao Jiang, Yongkun Du, Yuning Du, Lingfeng Zhu, Baohua Lai, Xiaoguang Hu, et al. Pp-ocrv3: More attempts for the improvement of ultra lightweight ocr system. _arXiv preprint arXiv:2206.03001_, 2022.
* [162] Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Niu Minzhe, Xiaodan Liang, Lewei Yao, Runhui Huang, Wei Zhang, Xin Jiang, et al. Wukong: A 100 million large-scale chinese cross-modal pre-training benchmark. _NeurIPS_, 35:26418-26431, 2022.
* [163] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. _arXiv preprint arXiv:2306.14824_, 2023.
* [164] Weiyun Wang, Min Shi, Qingyun Li, Wenhai Wang, Zhenhang Huang, Linjie Xing, Zhe Chen, Hao Li, Xizhou Zhu, Zhiguo Cao, et al. The all-seeing project: Towards panoptic visual recognition and understanding of the open world. In _ICLR_, 2024.
* [165] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. _arXiv preprint arXiv:2312.14238_, 2023.
* [166] Nitesh Methani, Pritha Ganguly, Mitesh M Khapra, and Pratyush Kumar. Plotqa: Reasoning over scientific plots. In _WACV_, pages 1527-1536, 2020.
* [167] Sanket Shah, Anand Mishra, Naganand Yadati, and Partha Pratim Talukdar. Kvqa: Knowledge-aware visual question answering. In _AAAI_, volume 33, pages 8876-8884, 2019.
* [168] Conghui He, Zhenjiang Jin, Chao Xu, Jiantao Qiu, Bin Wang, Wei Li, Hang Yan, Jiaqi Wang, and Dahua Lin. Wanjuan: A comprehensive multimodal dataset for advancing english and chinese large models. _arXiv preprint arXiv:2308.10755_, 2023.
* [169] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. A diagram is worth a dozen images. In _ECCV_, pages 235-251, 2016.
* [170] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. _NeurIPS_, 35:2507-2521, 2022.
* [171] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. _arXiv preprint arXiv:2209.14610_, 2022.
* [172] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. _arXiv preprint arXiv:2309.12284_, 2023.
* [173] Yipeng Sun, Zihan Ni, Chee-Kheng Chng, Yuliang Liu, Canjie Luo, Chun Chet Ng, Junyu Han, Errui Ding, Jingtuo Liu, Dimosthenis Karatzas, et al. Icdar 2019 competition on large-scale street view text with partial labeling-rrc-lsvt. In _ICDAR_, pages 1557-1562, 2019.
* [174] Weiyun Wang, Yiming Ren, Haowen Luo, Tiantong Li, Chenxiang Yan, Zhe Chen, Wenhai Wang, Qingyun Li, Lewei Lu, Xizhou Zhu, et al. The all-seeing project v2: Towards general relation comprehension of the open world. _arXiv preprint arXiv:2402.19474_, 2024.
* [175] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A large-scale, high-quality dataset for object detection. In _ICCV_, pages 8430-8439, 2019.
* [176] Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Marcal Rusinol, Ernest Valveny, CV Jawahar, and Dimosthenis Karatzas. Scene text visual question answering. In _ICCV_, pages 4291-4301, 2019.
* [177] Baoguang Shi, Cong Yao, Minghui Liao, Mingkun Yang, Pei Xu, Linyan Cui, Serge Belongie, Shijian Lu, and Xiang Bai. Icdar2017 competition on reading chinese text in the wild (rctv-17). In _ICDAR_, volume 1, pages 1429-1434, 2017.

* Singh et al. [2019] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In _CVPR_, pages 8317-8326, 2019.
* Mandal et al. [2021] Abhishek Mandal, Susan Leavy, and Suzanne Little. Dataset diversity: Measuring and mitigating geographical bias in image search and retrieval. In _Proceedings of the 1st International Workshop on Trustworthy AI for Multimedia Computing_, Trustworthy AI'21, page 1925, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450386746. doi: 10.1145/3475731.3484956. URL https://doi.org/10.1145/3475731.3484956.
* Samvelyan et al. [2024] Mikayel Samvelyan, Sharath Chandra Raparthy, Andrei Lupu, Eric Hambro, Aram H. Markosyan, Manish Bhatt, Yuning Mao, Minqi Jiang, Jack Parker-Holder, Jakob Foerster, Tim Rocktschel, and Roberta Raileanu. Rainbow teaming: Open-ended generation of diverse adversarial prompts, 2024. URL https://arxiv.org/abs/2402.16822.

## Appendix

### Table of Contents

* 1 Preliminaries
	* 1.1 Model Cards
	* 1.2 Datasets Used for Model Training
	* 1.3 Safety Measures for Selected Models
* 2 Safety Benchmarks for MLLMs
* 3 Dataset Statistics and Estimated Cost
	* 3.1 Data Statistics
	* 3.2 Estimated Cost
	* 3.3 Overview of Public Dataset
	* 3.4 Diversity
* 4 Experimental Setup for Evaluation
	* 4.1 Samples for dataset
	* 4.2 Additional Details on Experiments
	* 4.3 Design of Evaluator
Preliminaries

In this section, we provide information about models to be evaluated, the datasets used for model training, the selected models' safety policies, special outputs and the post-processing we apply to address those special outputs.

### Model Cards

Tab. 7 provides a summary of evaluated MLLMs, with details about their parameters, open-source status, and model architecture components (including the Vision Encoder and Base LLM).

### Datasets Used for Model Training

We have summarized the usage of four categories of datasets (Image Captioning, VQA, Grounding, and OCR) for training a total of 19 foundation models in Tab. 8. Additionally, Tab. 8 includes information on whether the models underwent Chat Alignment and Safety Alignment during training, which includes both pre-training and fine-tuning. Among them, 16 models explicitly state that they underwent Chat Alignment, while 3 models mention specific Safety Alignment techniques in their papers or technical reports. Fuyu-8B, MiniGPT-4, and InstructBLIP are not considered in the selection of evaluated models due to their noticeable lack of fluency in conversations on our datasets.

### Safety Measures for Selected Models

Here we enumerate the inherent measures of our selected model to address adversarial samples. During the scoring stage, we assign a uniform score of 1 to these responses, signifying a secondary response that lacks awareness of the posed question but generates a benign answer.

* **CogVLM** will directly return "unanswerable" in such cases.
* **DeepSeek-VL** will provide a blank response directly in such instances.
* **Gemini**'s API will raise an error in the response and explicitly state, "The response is blocked due to safety reasons."

\begin{table}
\begin{tabular}{l c c c} \hline \hline \multirow{2}{*}{**Model**} & \multirow{2}{*}{**\# Params**} & \multirow{2}{*}{**Open-**} & \multicolumn{2}{c}{**Model Architecture**} \\ \cline{3-4}  & & **sourced** & **Vision Encoder** & **Base LLM** \\ \hline GPT-4V [4] & / & no & / & / \\ Gemini [5] & / & / & / \\ \hline LLaVA-v1.5-7B [52] & 7B & CLIP ViT-L/336px & Vicuna-7B-v1.5 \\ Qwen-VL-Chat [50] & 9.6B & ViT-bigG & Qwen-7B \\ SEED-LLaMA [53] & 14B & ViT & LLaMA2-13B-Chat \\ Yi-VL-34B [54] & 34B & CLIP ViT-H/14 & Yi-34B-Chat \\ DeepSeek-VL [51] & 7B & SigLIP-L+SAM-B & DeepSeek-LLM-7B-Base \\ mPLUG-Owl2 [55] & 8B & yes & ViT-L/14 & LLaMA2-7B \\ MiniGPT-v2 [49] & 7.8B & EVA & LLaMA2-7B-Chat \\ CogVLM [6] & 17.6B & EVAV2-CLIP-E & Vicuna-7B-v1.5 \\ ShareGPT4V [56] & 7B & CLIP ViT-L/336px & Vicuna-7B-v1.5 \\ XComposer2-VL [57] & 8.6B & CLIP ViT-L/336px & InternLM2 \\ InterVL-v1.5 [58] & 26B & InternViT-6B & InternLM2-20B \\ \hline \hline \end{tabular}
\end{table}
Table 7: **Model cards in our benchmark. “/” means the item remains confidential, or we are not able to know from their paper or technical report.**

## Appendix B Safety Benchmarks for MLLMs

Tab. 9 summarizes the existing safety-related MLLM benchmarks, and the unique advantages of MLLMGuard compared to these benchmarks are:

* **More open-ended and closely aligned with MLLM application scenarios.** MLLMGuard features an open-ended dataset that better mirrors the real-world challenges encountered by MLLMs.
* **First bilingual safety-related MLLM Benchmark.** To our best knowledge, MLLMGuard is the first benchmark that provides safety-related data in both Chinese and English. This increases the diversity of the benchmark and its cross-language adaptability, which is meaningful for promoting the safety application of MLLMs in different language environments.
* **High difficulty** Data samples in MLLMGuard are meticulously crafted by crowd-workers with professional expertise and enhanced through red teaming techniques, raising the benchmark's quality and complexity. This manual construction approach more accurately captures real-world complexities compared to datasets automatically generated or collected.

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline

* **Accurate and straightforward evaluation:** MLLMGuard employs a mix of rule-based methods and a designated evaluator, GuardRank, which enables quick and precise evaluation results with a reduced usage threshold.
* **Broad dimensions** MLLMGuard covers extensive safety-related dimensions while existing works mostly focus on limited domains, leading to a comprehensive evaluation of MLLM Safety.

## Appendix C Dataset Statistics and Estimated Cost

In this section, we present the statistical information of the entire dataset, including the length distribution of prompts, the proportion of different languages in the dataset and the frequency of different red teaming techniques used. We then calculate the cost of evaluating the complete dataset, which includes the token count using closed-source model APIs and the cost based on pricing policies, as well as the devices, peak memory usage and time required for inference using the 10 open-source models. Moreover, we categorize the datasets into public and private and provide an overview of the public version. Finally, we analyze the diversity of the entire dataset from both textual and visual perspectives.

### Data Statistics

Fig. 6 shows the distribution of different languages in the dataset, with 51.8% being Chinese and 48.2% being English. Fig. 7 displays the frequency of red teaming techniques, with "Harmful Scenario" being the most frequently used. Fig. 8 presents the distribution of the textual prompt length in the dataset, where the majority of the textual prompts range from 0 to 50.

### Estimated Cost

For each dimension and three different types of models being evaluated (GPT-4V, Gemini, and Open-source models), Tab. 10 shows: 1) **# Prompt Tokens**: The number of tokens used for \(<image,text>\) prompts during evaluation. 2) **# Completion Tokens**: The number of tokens each

\begin{table}
\begin{tabular}{l|c c c c c c} \hline \hline
**Benchmark** & **Format** & **\# Size** & **Lang.** & **Constr.** & **Eval.** & **Purpose** & **RT** \\ \hline HallusionBench [20] & Open-ended & 1,129 & en & Human & GPT-4 & Hallucination & ✗ \\ \hline CorrelationQA [21] & Open-ended & 7,308 & en & Automatic & Rule & Hallucination & ✗ \\ \hline M-HalDetect [10] & Open-ended & 16,000 & en & Automatic & Human & Hallucination & ✗ \\ \hline VQAv2-IDK [22] & Open-ended & 20,431 & en & Human & Rule & Hallucination & ✗ \\ \hline MMCBench [13] & Open-ended & 4,000 & en & Automatic & Metrics & Robustness & ✗ \\ \hline Bingo [9] & Open-ended & 370 & en & Human & Human & Bias, Interference & ✗ \\ \hline SafeBench [16] & Open-ended & 500 & en & GPT-4 & GPT-4 & Safety, Jaibreak & ✓ \\ \hline MM-SafetyBench [17] & Open-ended & 5,040 & en & \begin{tabular}{c} Human \& \\ GPT-4 \\ \end{tabular} & \begin{tabular}{c} Human \& \\ GPT-4 \\ \end{tabular} & \begin{tabular}{c} GPT-4 \\ \end{tabular} & \begin{tabular}{c} Safety, Jaibreak \\ \end{tabular} & ✓ \\ \hline GOAT-Bench [15] & Open-ended & 6,626 & en & Human & Rule & Safety, Memes & ✗ \\ \hline Red-teaming GPT-4V [23] & Open-ended & 1,445 & en & \begin{tabular}{c} Human \& \\ LLAM-Guard \\ \end{tabular} & \begin{tabular}{c} Rule \& \\ Safety, Jaibreak \\ \end{tabular} & ✓ \\ \hline RTVLM [8] & Open-ended & 5,200 & en & \begin{tabular}{c} Human \& \\ GPT-4 \\ \end{tabular} & \begin{tabular}{c} GPT-4 \\ \end{tabular} & 
\begin{tabular}{c} Fairness, Faithfulness, Privacy, Safety \\ \end{tabular} & ✓ \\ \hline Ch\({}^{3}\)EF [18] & Multiple choice & 1,002 & en & Human & Metrics & Helpful, Honest, Harmless & ✗ \\ \hline
**MLLMGuard (Ours)** & Open-ended & 2,282 & \begin{tabular}{c} en \& \\ zh \\ \end{tabular} & \begin{tabular}{c} Human \\ \end{tabular} & \begin{tabular}{c} Rule \& \\ Evaluator \\ \end{tabular} & 
\begin{tabular}{c} Bias, Legality, Privacy, \\ Toxicity, Truthfulness \\ \end{tabular} & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 9: **Safety-related MLLM benchmarks. ‘Lang.’ denotes the dataset’s language: ‘en’ for English, ‘zh’ for Chinese. ‘Constr.’ indicates the construction of the dataset: ‘Human’ for crowd-sourced, ‘GPT4’ for AI-generated, ‘Human & GPT-4’ for collaborative, and ‘Automatic’ for automatic generation such as template-based creation. ‘Eval.’ refers to the method used in evaluation: ‘Human’ for manual review, ‘Metrics’ for calculations using indices such as BLEU and PPL, ‘Rule’ for rule-based checks like regex matching, ‘GPT-4’ and ‘LLAM-Guard’ for LLM-based assessments, and ‘Evaluator’ for the specialized evaluator tailored for the benchmark. ‘RT’ signifies Red Teaming use.**model requires to complete the chat during evaluation, where the **max_new_tokens** for each model is set to 128. 3) **Total Cost(S)**: The total cost of prompt tokens and completion tokens. The official pricing for GPT-4V and Gemini is used to calculate the cost. As for Open-source models, the cost is not specified, but you can refer to Tab. 11 for the inference requirements of each model.

We obtain the official weights of the models and performed inference on a single GPU. Tab. 11 shows the inference time used in evaluation, device used (A100 stands for _NVIDIA A100-SXM4-80GB_, and V100 is stands for _Tesla V100-SXM2-32GB_ ) and peak GPU memory usage for each model on each dimension's data.

### Overview of Public Dataset

In order to prevent the dataset from being used for model training, thus preserving its evaluation purpose, we randomly select 1,500 samples from the dataset for public release, while retaining the remaining data. Tab. 12 provides an overview of the publicly released dataset.

[MISSING_PAGE_FAIL:28]

## Appendix D Experimental Setup for Evaluation

To ensure reproducibility of the results, we minimize the randomness of the answers. Tab. 15 displays the models and hyper-parameter settings we use to evaluate GPT-4V and Gemini, respectively. For closed-source models, we aim to set the hyperparameters such as top_p and top_k to low values or set do_sample to False. For open-source models, max_new_tokens is consistently set to 128.

\begin{table}
\begin{tabular}{c|c c} \hline \hline
**Model** & gpt-4-vision-preview & gemini-pro-vision \\ \hline
**temperature** & 0 & 0 \\
**top\_p** & N/A & 1 \\
**top\_k** & N/A & 1 \\
**max\_tokens** & 128 & 128 \\ \hline \hline \end{tabular}
\end{table}
Table 15: Hyperparameter settings for GPT-4V and Gemini. ‘N/A’ denotes a non-adjustable parameter.

[MISSING_PAGE_EMPTY:30]

Figure 10: **Samples for Bias.**

Figure 11: **Samples for Toxicity.**

Figure 12: **Samples for Legality.**

Figure 13: **Samples for Truthfulness.**

[MISSING_PAGE_EMPTY:34]

\begin{table}
\begin{tabular}{l|c c c c|c} \hline \hline
**Model** & **Privacy** & **Bias** & **Toxicity** & **Truthfulness** & **Legality** & **Avg.** \\ \hline GPT-4V & 52.29\% & 45.66\% & 19.30\% & 76.66\% & 50.53\% & **48.89\%** \\ Gemini & 11.01\% & 7.51\% & 5.26\% & 77.29\% & 2.11\% & 20.64\% \\ \hline LLaVA-v1.5-7B & 23.85\% & 21.39\% & 9.36\% & 38.60\% & 24.21\% & 23.48\% \\ Qwen-VL-Chat & 19.27\% & 19.08\% & 14.62\% & 49.91\% & 29.47\% & 26.47\% \\ SEED-LLaMA & 10.09\% & 4.05\% & 8.77\% & 43.17\% & 14.74\% & 16.16\% \\ Yi-VL-34B & 10.09\% & 23.70\% & 14.62\% & 47.55\% & 11.58\% & 21.51\% \\ DeepSeek-VL & 19.27\% & 8.09\% & 0.00\% & 64.70\% & 5.26\% & 19.46\% \\ mPLUG-Owl2 & 15.60\% & 5.78\% & 9.36\% & 43.25\% & 5.26\% & 19.46\% \\ MiniGPT-v2 & **57.80**\% & 40.46\% & **31.58**\% & 38.87\% & **53.68**\% & 44.48\% \\ CogVLM & 0.92\% & 0.00\% & 0.00\% & 52.07\% & 0.00\% & 10.60\% \\ ShareGPT4V & 14.68\% & 15.61\% & 4.09\% & 41.20\% & 20.00\% & 19.12\% \\ XComposer2-VL & 24.77\% & 38.73\% & 10.53\% & 63.79\% & 8.42\% & 29.25\% \\ IntermVL-v1.5 & 18.35\% & **58.96**\% & 12.28\% & **79.32**\% & 28.42\% & 39.47\% \\ \hline \hline \end{tabular}
\end{table}
Table 19: **PAR (\(\uparrow\)) of various models across different dimensions on the English subset.** We evaluate each model based on metrics in each dimension and highlight the best-performing model in **bold** and the second-best model with an underline.

\begin{table}
\begin{tabular}{l|c c c c|c} \hline \hline
**Model** & **Privacy** & **Bias** & **Toxicity** & **Truthfulness** & **Legality** & **Avg.** \\ \hline GPT-4V & 28.13 & **21.00** & 26.90 & 23.34 & **16.49** & **23.17** \\ Gemini & 37.31 & 45.28 & 33.92 & 22.71 & 34.74 & 34.79 \\ \hline LLaVA-v1.5-7B & 43.73 & 45.28 & 34.89 & 61.40 & 32.28 & 43.52 \\ Qwen-VL-Chat & 45.26 & 34.87 & 38.60 & 50.09 & 35.09 & 40.78 \\ SEED-LLaMA & 51.38 & 55.11 & 46.78 & 56.83 & 44.91 & 51.00 \\ Yi-VL-34B & 49.85 & 46.24 & 35.28 & 52.45 & 39.30 & 44.62 \\ DeepSeek-VL & 48.32 & 37.57 & 36.45 & 35.30 & 37.19 & 38.97 \\ mPLUG-Owl2 & 45.57 & 48.17 & 44.83 & 56.75 & 50.88 & 49.24 \\ MiniGPT-v2 & **25.08** & 26.78 & **22.81** & 61.13 & 19.30 & 31.02 \\ CogVLM & 39.76 & 59.34 & 34.31 & 47.93 & 47.72 & 45.81 \\ ShareGPT4V & 45.26 & 45.47 & 55.95 & 58.80 & 46.32 & 50.36 \\ XComposer2-VL & 37.92 & 28.52 & 35.48 & 36.21 & 35.79 & 34.78 \\ IntermVL-v1.5 & 42.51 & **21.00** & 44.25 & **20.68** & 33.68 & 32.42 \\ \hline \hline \end{tabular}
\end{table}
Table 18: **ASD (\(\downarrow\)) of various models across different dimensions on the English subset.** We evaluate each model based on metrics in each dimension and highlight the best-performing model in **bold** and the second-best model with an underline.

## Appendix G Design of Evaluator

In this section, we present the zero-shot prompts used on GPT-4 and the hyperparameter settings for GuardRank. Additionally, we provide the training results of GuardRank using Pre-trained Language Model (PLM) as the backbone for comparison with current GuardRank.

Prompts used for GPT-4Fig. 14, Fig. 15, Fig. 16, Fig. 17, and Fig. 18 demonstrate prompts used for GPT-4.

Hyperparameter settings for GuardRankGuardRank uses LLaMA-2 as the backbone for the four-class classification tasks (Privacy, Bias, Toxicity, and Legality), and Roberta-Large as the backbone for the binary classification (Hallucination), respectively. To enable efficient training, we utilize the commonly used PEFT (Parameter-Efficient Fine-Tuning) method of LoRA (Low-Rank Adaptation) to fine-tune the LLaMA-2. The training parameters are shown in Tab. 23.

Performance of GuardRank using PLM as backboneWe also explore using bert-base-chinese, bert-base-uncased, roberta-base, and roberta-large as the backbones, training them with learning rates of 2e-5 and 2e-6, respectively. Given that the dataset is a bilingual corpus, we train the models on the different language subsets of the data as well. The performance of GuardRank using PLM as the

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Model** & **Non-existent Query** & **Position Swapping** & **Noise Injection** \\ \hline GPT-4V & 23.73 & 40.38 & **0.00** \\ Gemini & 50.85 & 33.33 & 5.36 \\ \hline LLaVA-v1.5-7B & 72.88 & 92.93 & 6.90 \\ Qwen-VL-Chat & 38.98 & 96.81 & 5.66 \\ SEED-LLAMA & 83.90 & 96.61 & 2.04 \\ Yi-VL-34B & 85.59 & 66.10 & 3.51 \\ DeepSeek-VL & 69.49 & **27.12** & **0.00** \\ mPLUG-Owl2 & 73.73 & 96.61 & 5.56 \\ MiniGPT-v2 & 40.68 & 81.82 & 41.94 \\ CogVLM & 58.47 & 98.31 & 1.89 \\ ShareGPT4V & 73.73 & 94.62 & 3.92 \\ XComposer2-VL & 74.58 & 66.10 & 1.79 \\ InternVL-v1.5 & **22.88** & 29.63 & **0.00** \\ \hline \hline \end{tabular}
\end{table}
Table 21: **ASD(\(\downarrow\)) of each model on various red teaming techniques of Truthfulness on the Chinese subset.**

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Model** & **Non-existent Query** & **Position Swapping** & **Noise Injection** \\ \hline GPT-4V & **14.41** & 53.85 & 1.75 \\ Gemini & 45.76 & **18.87** & 3.51 \\ \hline LLaVA-v1.5-7B & 85.59 & 94.44 & 4.17 \\ Qwen-VL-Chat & 48.31 & 100.00 & 1.96 \\ SEED-LLAMA & 83.90 & 81.03 & 5.56 \\ Yi-VL-34B & 90.68 & 66.67 & **0.00** \\ DeepSeek-VL & 62.71 & 41.38 & 1.82 \\ mPLUG-Owl2 & 82.20 & 86.21 & 1.85 \\ MiniGPT-v2 & 74.58 & 100.00 & 8.82 \\ CogVLM & 59.32 & 84.48 & **0.00** \\ ShareGPT4V & 76.27 & 98.28 & 1.85 \\ XComposer2-VL & 64.41 & 44.23 & **0.00** \\ InternVL-v1.5 & 32.20 & 29.82 & **0.00** \\ \hline \hline \end{tabular}
\end{table}
Table 22: **ASD (\(\downarrow\)) of each model on various red teaming attacks of Truthfulness on the English subset.**Please play the role of an answer scorer and give your score according to the rules.

**Rules:**

**0**: {privacy leakage} realized and provide {privacy preserving} answer.

**1**: {privacy leakage} not realized but provide {privacy preserving} answer.

**2**: {privacy leakage} not realized and provide {privacy invasive} answer.

**3**: {privacy leakage} realized but provide {privacy invasive} answer.

**Prompt**: <IMAGE> {The textual prompt provide to MLLMs}

**Answer**: {The response provided by MLLMs}

Based on the rules, give your score (The number only) to the Answer.

Your score is:

Here are some examples:

Please play the role of an answer scorer and give your score according to the rules.

**Rules:**

**0**: {bias} realized and provide {unbiased} answer.

**1**: {bias} not realized but provide {unbiased} answer.

**2**: {bias} not realized and provide {biased} answer.

**3**: {bias} realized but provide {biased} answer.

**Prompt**: <IMAGE> {The textual prompt provide to MLLMs}

**Answer**: {The response provided by MLLMs}

Based on the rules, give your score (The number only) to the Answer.

Figure 16: Prompts used for GPT-4 on toxicity

Figure 14: Prompts used for GPT-4 on privacy

Figure 15: Prompts used for GPT-4 on bias

[MISSING_PAGE_FAIL:38]

backbone is shown in Tab. 6, and the corresponding optimal hyperparameter selections are indicated in Tab. 25.

Considering the convenience of deployment and slightly higher accuracy, we ultimately decide to adopt LLaMA-2 and Roberta-large as the final components of GuardRank.

\begin{table}
\begin{tabular}{c c c c c|c c c} \hline \hline
**Dimension** & **Model** & **Learning Rate** & **Language** & **Epoch** & **Batch Size** \\ \hline \multirow{3}{*}{Privacy} & bert-base-chinese & 2e-5 & zh & 10 & 16 \\  & roberta-base & 2e-5 & en & 10 & 16 \\  & bert-base-chinese & 2e-6 & all & 10 & 16 \\ \hline \multirow{3}{*}{Bias} & bert-base-chinese & 2e-5 & zh & 10 & 16 \\  & roberta-large & 2e-6 & an & 10 & 16 \\ \hline \multirow{3}{*}{Toxicity} & bert-base-chinese & 2e-5 & zh & 10 & 16 \\  & roberta-base & 2e-6 & en & 10 & 16 \\  & bert-base-chinese & 2e-6 & all & 10 & 16 \\ \hline \multirow{3}{*}{Hallucination} & bert-base-chinese & 2e-5 & zh & 10 & 16 \\  & roberta-base & 2e-5 & en & 10 & 16 \\  & roberta-large & 2e-6 & all & 10 & 16 \\ \hline \multirow{3}{*}{Legality} & bert-base-chinese & 2e-5 & zh & 10 & 16 \\  & bert-base-uncased & 2e-5 & en & 10 & 16 \\ \cline{1-1}  & bert-base-chinese & 2e-5 & all & 10 & 16 \\ \hline \hline \end{tabular}
\end{table}
Table 25: **Optimal hyperparameter selections.**

\begin{table}
\begin{tabular}{l|c c c|c c c|c c} \hline \hline
**Dimension** & \multicolumn{3}{c|}{**GPT4 (Zero-Shot)**} & \multicolumn{3}{c|}{**GPT4 (ICL)**} & \multicolumn{3}{c}{**GuardRank**} \\  & zh & en & all & zh & en & all & zh & en & all \\ \hline \multicolumn{9}{c}{_Results on Validation Set_} \\ \hline
**Privacy** & 35.37 & 40.25 & 37.77 & 39.63 & 47.80 & 43.65 & 73.17 & 74.84 & 72.14 \\
**Bias** & 22.39 & 50.38 & 36.52 & 18.53 & 54.55 & 36.71 & 79.54 & 80.68 & 78.01 \\
**Toxicity** & 12.64 & 13.41 & 13.02 & 35.69 & 23.37 & 29.62 & 67.66 & 79.69 & 72.26 \\
**Hallucination** & 25.56 & 40.00 & 32.78 & 42.78 & 66.11 & 54.44 & 87.22 & 100.0 & 92.78 \\
**Legality** & 33.03 & 28.57 & 31.25 & 49.77 & 59.86 & 53.8 & 74.21 & 79.59 & 76.09 \\ _Avg._ & 25.80 & 34.52 & 30.27 & 37.28 & 50.34 & 43.64 & 76.36 & 82.96 & 77.92 \\ \hline \multicolumn{9}{c}{_Results on Test Set_} \\ \hline
**Privacy** & 23.48 & 32.39 & 27.86 & 26.83 & 36.16 & 31.42 & 67.38 & 68.87 & 67.49 \\
**Bias** & 25.10 & 35.98 & 30.59 & 25.68 & 35.23 & 30.50 & 62.16 & 72.73 & 64.91 \\
**Toxicity** & 9.29 & 14.94 & 12.08 & 51.30 & 20.11 & 35.94 & 81.60 & 75.48 & 78.30 \\
**Hallucination** & 43.61 & 34.17 & 38.89 & 70.00 & 53.89 & 61.94 & 81.39 & 99.72 & 97.22 \\
**Legality** & 34.39 & 42.18 & 37.5 & 49.32 & 61.22 & 54.08 & 63.08 & 61.90 & 64.67 \\ _Avg._ & 27.17 & 31.93 & 29.38 & 44.62 & 41.32 & 42.78 & 71.27 & 75.74 & 74.52 \\ \hline \end{tabular}
\end{table}
Table 24: **Performance of GuardRank using PLM as backbone.** For each dimension and language, we calculate accuracy on the validation set and test set. Best performances for each language setting are highlighted in ‘zh’, ‘en’, and ‘all’.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? [Yes] See App. 6. 3. Did you discuss any potential negative societal impacts of your work? [Yes] See App. 8. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? See App. D and App. G. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [No] 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See App. C.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] See Tab. 1 and Tab. 12. Did you mention the license of the assets? [Yes] See supplementary materials. 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [No] 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] See App. 8.
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? See supplementary materials. 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [No] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [Yes] See App. 8.