# Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples

Abulhair Saparov\({}^{\dagger}\) Richard Yuanzhe Pang\({}^{\dagger}\) Vishakh Padmakumar\({}^{\dagger}\) Nitish Joshi\({}^{\dagger}\)

&Seyed Mehran Kazemi\({}^{\Delta}\) Najoung Kim\({}^{\Delta,\beta,}\) He He\({}^{\dagger,}\)

\({}^{\dagger}\)New York University, \({}^{\Delta}\)Google, \({}^{\beta}\)Boston University

as17582@nyu.edu

Equal contribution.\({}^{\dagger,\Delta}\)GPT and PaLM experiments in this paper were conducted independently. Authors affiliated with NYU\({}^{\dagger}\) were responsible for the GPT experiments, and authors affiliated with Google\({}^{\Delta}\) were responsible for the PaLM experiments.

###### Abstract

Given the intractably large size of the space of proofs, any model that is capable of general deductive reasoning must generalize to proofs of greater complexity. Recent studies have shown that large language models (LLMs) possess some abstract deductive reasoning ability given chain-of-thought prompts. However, they have primarily been tested on proofs using modus ponens or of a specific size, and from the same distribution as the in-context examples. To measure the general deductive reasoning ability of LLMs, we test on a broad set of deduction rules and measure their ability to generalize to more complex proofs from simpler demonstrations from multiple angles: depth-, width-, and compositional generalization. To facilitate systematic exploration, we construct a new synthetic and programmable reasoning dataset that enables control over deduction rules and proof complexity. Our experiments on four LLMs of various sizes and training objectives show that they are able to generalize to compositional proofs. However, they have difficulty generalizing to longer proofs, and they require explicit demonstrations to produce hypothetical subproofs, specifically in _proof by cases_ and _proof by contradiction_.

## 1 Introduction

In many tasks that require deductive reasoning, such as theorem proving or medical diagnosis, the complexity of proofs can grow without bound via the use of multiple deduction rules and the composition of subproofs. Given the large space of proofs, it is infeasible to find data to cover proofs of all sizes. Therefore, a general reasoning model must extrapolate to complex proofs from simpler ones. Recent work has shown that LLMs, combined with in-context learning (ICL) and chain-of-thought (CoT) prompting, are capable of deductive reasoning to an extent [22, 23, 24, 25, 26, 27, 28]. However, much of the prior work focused on a limited set of deduction rules such as modus ponens [22, 26, 25]. In addition, the evaluation is _in-demonstration_, where the test example comes from the same distribution as the in-context demonstrations. In this work, we evaluate whether LLMs are capable of general deductive reasoning by measuring how well they generalize to proofs that are more complexthan their demonstrations.1

Footnote 1: All analysis code, data, data generation scripts, and model outputs are publicly available at github.com/asaparov/prontoqa

We characterize the complexity of proofs from three angles: the deduction rules involved, the depth of the proof (i.e. length of a sequential chain of proof steps), and the width of the proof (i.e. the number of premises of each proof step). Each of the three dimensions contributes to the overall size of the proof. To measure the general deductive reasoning ability of LLMs, we extend prior studies in two key ways. First, we determine whether LLMs have learned a _complete_ set of deduction rules, beyond modus ponens. Second, we evaluate whether they can reason over longer proofs than those given as in-context examples (depth- and width- generalization); and whether they are able to use multiple different deduction rules in a single proof (compositional generalization). Figure 1 shows an overview of our study.

Our findings suggest that in-context learning is best applied to reasoning tasks by including examples that cover a diverse set of deduction rules, and keeping the examples simple. The in-context examples should especially contain examples of deduction rules that are less familiar to the model (i.e. proof by cases and proof by contradiction), and distractors should be provided for such examples as the model is more prone to overfitting.

We test four different LLMs of different scales and training objectives: GPT-3.5 175B (Ouyang et al., 2022), PaLM 540B (Chowdhery et al., 2022), LLaMA 65B (Touvron et al., 2023), and FLAN-T5 11B (Chung et al., 2022), and we find:

1. CoT is able to elicit out-of-demonstration (OOD) reasoning in LLMs generalizing to compositional proofs. This is somewhat surprising given the amount of previous work that claim that LLMs are _not_ able to generalize compositionally (Hosseini et al., 2022; An et al., 2023). See Section 4.2.2 and Figure 6.
2. ICL generalizes differently compared to supervised learning (i.e. gradient descent on in-context examples). We find numerous examples where it is strictly worse to provide in-context examples from the same distribution as the test example. For instance, in some cases, we observe better generalization to compositional proofs when the in-context examples each contain individual deduction rules. See Sections 4.2.2 and 4.3, and Figures 6 and 8.
3. However, the LLMs cannot generalize to some deduction rules without explicit demonstrations, specifically, _proof by cases_ and _proof by contradiction_, suggesting that pretraining is not sufficient to teach the model to generate hypothetical subproofs. See Section 4.2.1 and Figure 4.
4. Model size does not strongly correlate with performance. Smaller (but not the smallest) models with instruction tuning and longer pretraining perform comparably to larger models.

Figure 1: An overview of the kinds of OOD generalization that we test in our experiments. Each training example is a sample CoT demonstration provided to the LLM in the few-shot prompt, whereas each test example is a sample proof that the model is expected to output.

## 2 Related work

OOD generalization of LLMs.Previous work has measured the generalization ability of LLMs on tasks such as bit parity and Boolean variable assignment (Anil et al., 2022), semantic parsing (Hosseini et al., 2022; Qiu et al., 2022), deductive reasoning (Zhang et al., 2022; Sanyal et al., 2022; Kazemi et al., 2023), and arithmetic reasoning (Kudo et al., 2023), where the length/complexity of the test example is greater than that of the in-context examples. On the bit parity and variable assignment tasks, LLMs are able to generalize to longer inputs with scratchpad prompting (Nye et al., 2021), but this generalization is imperfect, and accuracy still degrades with increasing input length. Generally, larger models tend to be better at generalization than smaller ones. The studies on reasoning were limited to reasoning using modous ponens. Wu et al. (2021) tests the OOD generalization of transformers and graph neural networks on symbolic mathematical reasoning. Our study more systematically examines OOD generalization of LLMs to larger proofs as well as to other deduction rules.

Evaluating reasoning abilities of LLMs.A number of recent studies measured the reasoning ability of LLMs (Huang and Chang, 2022; Han et al., 2022). Table 1 provides a comparison of our proposed dataset to datasets from these studies. Many of these datasets are not amenable to automated evaluation of proofs, relying instead on measuring label accuracy. The datasets also do not test for proof width generalization and compositional generalization. Some datasets focus on a limited set of deduction rules, namely modus ponens. Our work is closest to ProMotoQA (Saparov and He, 2023) but extends it to a complete set of deduction rules and to compositional proofs.

Understanding in-context learning.Recent work has shed some light on ICL, and the mechanism by which the model learns from in-context examples. Akyurek et al. (2023); Dai et al. (2023); von Oswald et al. (2022) showed that transformers can learn in-context by performing gradient descent on in-context examples internally. Xie et al. (2022); Wang et al. (2023) show that LLMs exhibit behavior similar to that of topic models where their output is dependent on a latent topic, and the in-context examples help to specify the topic. Ye et al. (2022) demonstrated that ICL is more effective when the in-context examples are both diverse and relevant to the test example. An et al. (2023) and Levy et al. (2022) explored the effect of in-context demonstrations on compositional generalization, showing that it benefits from diverse and individually simple demonstrations. Our results contribute to this growing literature by showing that the generalization behavior in ICL is different from that of supervised learning, and so algorithms such as gradient descent are not the only mechanisms underlying ICL.

## 3 Approach

A programmable dataset.Our main evaluation approach is to prompt the LLM with simpler proofs and test it on proofs with greater depth and width, or with those using additional deduction rules. Therefore, we require a programmable approach to data generation, where the deduction rules

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c} \hline \multirow{2}{*}{**Dataset**} & Automated evaluation of & Contains proofs & Tests proof & Tests proof & Tests & Data \\  & evaluation of & with multiple & depth & width & compositional & generation \\  & proofs & deduction rules & generalization & generalization & code available \\ \hline \hline CLUTRR & ✗ & ✗ & ✗ & ✗ & ✗ & ✓ \\ Sinha et al. (2019) & & ✗ & ✗ & ✗ & ✗ & human- \\ LogiQA & ✗ & ✓ & ✗ & ✗ & ✗ & annotated \\ \hline ProofWriter & ✓ & ✗ & ✓ & ✗ & ✗ \\ Taljord et al. (2021) & & ✗ & ✗ & ✗ & ✗ \\ \hline FOLIO & ✗ & ✓ & ✗ & ✗ & ✗ \\ Han et al. (2022) & ✗ & ✓ & ✗ & ✗ & annotated \\ \hline \hline PROMotoQA & & & & & \\ Saparov and He (2023) & ✓ & ✗ & ✓ & ✗ & ✗ & ✓ \\ \hline PROMotoQA-OOD & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\ (this dataset) & & & & & \\ \hline \end{tabular}
\end{table}
Table 1: Comparison of existing datasets to evaluate reasoning ability. Datasets marked with ✗ contain examples of varying width, depth, and compositionality, but these are not programmable (i.e. we cannot generate new examples controlling for these variables), and splitting the existing examples would produce highly imbalanced splits.

used, as well as the depth and width of each proof, are controllable parameters. To this end, we propose PrOntoQA-OOD, a generative process for synthetic reasoning questions. Each example in PrOntoQA-OOD contains a handful of premises, a query (the target fact to be proved/disproved), and a gold CoT containing the proof of the query. See Figure 10 for an example from this dataset. Specifically, we extend the PrOntoQA dataset [10] that contains proofs generated from synthetic world models using modus ponens. (1) To evaluate reasoning using different deduction rules, we generate proofs with deduction rules for all connectives in propositional logic: conjunction \(\wedge\), disjunction \(\vee\), implication \(\rightarrow\), and negation \(\neg\). (2) To study width/depth generalization, the proof depth and width are controllable parameters in proof generation, where they control the number of generated deduction rules, and the number of premises in each deduction rule, respectively. (3) To study compositional generalization, we generate compositional proofs using a simple recursive procedure, where each proof contains multiple subproofs with distinct deduction rules.

Generating proofs with a complete set of deduction rules.We follow the deduction rules of _natural deduction_[1, 13], a well-studied proof calculus with desirable completeness properties.2 Examples of each deduction rule are shown in Table 2. For each type of deduction rule, we randomly generate a proof that applies that rule (see details in section A.4. An example of a compositional proof is shown in Figure 2. To prevent LLMs from exploiting knowledge from pretraining to solve the problems without reasoning, we use fictional names for all concepts (e.g. "wumpus" instead of "cat" etc.).

\begin{table}
\begin{tabular}{c|c|c} \hline \hline
**Deduction rule** & **Formal definition** & **Natural language example** \\ \hline Implication elimination (i.e. modus ponens) & \(f(a)\)\(\forall x(f(x)\to g(x))\) & “Alex is a cat. All cats are carnivores. Alex is a carnivores.” \\ \hline Conjunction introduction & \(\begin{array}{c}A\quad B\\ A\wedge B\end{array}\) & “Alex is a cat. Alex is orange. Alex is a cat and orange.” \\ \hline Conjunction elimination & \(\begin{array}{c}A\wedge B\\ A\end{array}\) & “Alex is a cat and orange. Alex is orange.” \\ \hline Disjunction introduction & \(\begin{array}{c}A\\ A\lor B\end{array}\) & “Alex is a cat. Alex is a cat or orange.” \\ \hline Disjunction elimination (i.e. proof by cases) & \(A\lor B\)\(A\vdash C\)\(B\vdash C\) & “Alex is a cat or a dog. Suppose Alex is a cat... then Alex is warm-blooded. Suppose Alex is a dog... then Alex is warm-blooded. Alex is warm-blooded.” \\ \hline Proof by contradiction & \(\begin{array}{c}A\vdash B\quad\neg B\\ \neg A\end{array}\) & “Alex is cold-blooded. If Alex is a mammal, Alex is not cold-blooded. Suppose Alex is a mammal. Alex is not cold-blooded. This contradicts with Alex is cold-blooded. Alex is not a mammal.” \\ \hline \hline \end{tabular}
\end{table}
Table 2: An overview of the deduction rules in PrOntoQA-OOD. The notation \(A\vdash B\) denotes entailment: that \(B\) is provable from \(A\).

Figure 2: An example of a compositional proof containing modus ponens, proof by contradiction, and conjunction introduction, shown in both natural language and a formal tree representation.

Varying proof width and depth.To characterize the size or complexity of each proof, we represent each proof as a tree (Figure 2), where each proof step corresponds to a node, and its premises correspond to the parent nodes. Then the size of the proof can be naturally described by the width and depth of this tree. When generating proofs, we control the depth by continuing to append proof steps until a proof of the desired depth is generated. The number of premises of deduction rules is set to the desired proof width.

Generating compositional proofs.To generate proofs combining many different types of deduction rules, we use a simple recursive procedure: (1) select a deduction rule uniformly at random, (2) select the premises for the selected rule, (3) recursively generate a subproof for each premise. See section A.5 for details and pseudocode.

Adding distractors.One key challenge to OOD generalization is shortcut solutions. For example, given the facts "Alex is a cat," "All cats are feline," since there is no other fact of the form "All cats are...," the model can deterministically follow the only valid deduction and conclude "Alex is feline." To make the heuristics uninformative, we add distractor sentences. In the above case, a distractor sentence would be "All cats are graceful." Then the model is forced to choose the correct premise from two options for the next deduction step. See Section A.7 for details on distractors for all deduction rules.

Formal evaluation of chain-of-thought.Unlike previous datasets that evaluate on a binary true/false answer, PrOntoQA-OOD requires LLMs to generate full proofs.3 Therefore, we need a way to evaluate the correctness of the output proofs directly. The sentences in PrOntoQA-OOD are syntactically simple and amenable to semantic parsing, which allows us to formally analyze each step in the CoT. To determine whether a predicted CoT is correct, we: (1) semantically parse each CoT sentence into first-order logic, (2) determine whether each logical form follows from previous logical forms via a rule of deduction, and (3) compute whether there exists a path of correct steps from the premises to the goal. An example of this process is shown below:

Footnote 3: True/false queries introduce an extra implicit negation step of the final conclusion. The negated conclusion may be tricky to represent in natural language given the extensive space of potential conclusions of the proofs that we generate.

Each proof step is considered correct if it is valid and if it immediately follows one of its premises.4 For further details see section A.6.

Footnote 4: Note that if we only check for validity, the following proof would be considered correct: “Fae is a cat. All cats are carnivores. Fae is a carnivore. All carnivores are mammals. Fae is a mammal. All mammals are not herbivous. Fae is not herbivous.” Here is not herbivous.” Here, the goal is to prove “Fae is not herbivorous.” Every step of the proof is correct except “All mammals are not herbivorous.” To avoid this, we require each step to immediately follow one of its premises.

## 4 Results

In this section, we test existing LLMs on PrOntoQA-OOD and analyze whether they can produce longer and compositional proofs given simpler demonstrations. We experiment with a variety of models, with different sizes and training objectives, as shown in Figure 3. In all experiments, we use 8-shot chain-of-thought prompting.5 We compare performance in two settings: (1) an _in-demonstration_ (ID) setting where the 8 in-context demonstrations come from the same distribution as the test example, and (2) an _out-of-demonstration_ (OOD) setting where the in-context demonstrations come from a distribution that is different from that of the test example. 95% confidence intervals are provided for all results.

Figure 3: An overview and properties of the LLMs in our experiments. We place an asterisk* for GPT-3.5 since we were not able to verify its size.

### Can LLMs use deduction rules other than modus ponens?

We first evaluate whether LLMs "know" all deduction rules (Table 2) when provided with corresponding CoT prompts. For each deduction rule, we independently and identically generate 8 in-context examples and one test example, and prompt the model to answer the test example. We run each experiment for 100 trials and measure the accuracy of the output proofs. The accuracies are shown in the top chart of Figure 4. We emphasize that the \(\Delta\) proof accuracies in the bottom row of the figure should be interpreted in comparison with the accuracies in the top row (e.g. for some rules, the zero \(\Delta\) accuracy for FLAN-T5 is due to zero absolute accuracy). For clarity, we provide the same plots using absolute accuracy rather than \(\Delta\) accuracy in Section A.2.

In general, most models are able to use each deduction rule reasonably well, with GPT-3.5 performing the best. Similar to prior studies (Liang et al., 2022), we do not observe a strong correlation between model size and performance. LLMA performs comparably to PaLM, despite being smaller. FLAN-T5 is smaller and performs reasonably on implication and conjunction elimination, but is not able to learn the other deduction rules.

Figure 4: **(top)** Proof accuracy across examples with different deduction rules. The in-context examples and test examples come from the same distribution. **(bottom)** Change in proof accuracy, where the test example is out-of-demonstration with respect to the in-context examples. That is, the test example has the specified deduction rule, but the in-context examples are uniformly distributed over _all other_ deduction rules. See Figure 11 in the Appendix for the equivalent plot with absolute proof accuracy on the y-axis. See Figure 5 for an incorrect example. Implication elimination examples have proof width of \(1\) and depth of \(2\). Conjunction introduction, conjunction elimination, and disjunction introduction examples have proof width \(3\) and depth \(2\). Disjunction elimination examples have proof width \(3\) and depth \(1\). Proof by contradiction examples have proof width \(2\) and depth \(1\).

Figure 5: Example of an incorrect proof generated by GPT-3.5 on an out-of-demonstration disjunction elimination example. The premises (axioms) are given in blue, and invalid steps are given in red. For the full example, see Figure 14 in the Appendix.

### Out-of-demonstration generalization

#### 4.2.1 Can LLMs generalize to unseen deduction rules?

While the above results show that LLMs are able to reason with a variety of deduction rules, it is unclear whether the ability is learned from in-context examples or elicited from pretraining. We test the LLM with examples where the test proof requires a deduction rule that does not appear in the in-context examples (i.e. for each in-context example, we sample a deduction rule uniformly at random from the set of deduction rules excluding that of the test example). Our intuition was that LLMs would not be able to use deduction rules unless given explicit demonstrations thereof (aside from those like modus ponens which are well-represented in pretraining). The change in proof accuracy relative to the ID setting is shown in the bottom chart of Figure 4. Evidently, the models are able to use four deduction rules despite not being shown an in-context example with those rules: both conjunction rules, disjunction introduction, and (somewhat) implication elimination. GPT-3.5 was additionally able to use proof by contradiction by relying on an alternate deduction rule called modus tollens (i.e. given \(\neg f(c)\) and \(\forall x(g(c)\to f(c))\), conclude \(\neg g(c)\)). This is in contrast with McKenzie et al. (2022) which showed that reasoning with modus tollens exhibited inverse scaling behavior, and yet GPT-3.5 is able to use it correctly without any demonstrations. However, Wei et al. (2022) has shown that when trained with additional compute, models are able to perform modus tollens. GPT-3.5 performed worse on disjunction elimination possibly due to the fact that there is no equivalent alternate rule (an example of an error is given in figure 5). However, no model is able to use disjunction elimination and proof by contradiction without demonstrations.

#### 4.2.2 Can LLMs generalize to compositional proofs?

Next, we test whether the model is able to generalize to compositional proofs that contain multiple different deduction rules. In the ID setting, the in-context examples and test examples are both generated from the same distribution of compositional proofs. In the OOD setting, the in-context demonstrations contain non-compositional examples of each rule that appears in the test example. In Figure 6, in all but three experiments, we observe that the gap in proof accuracy between the ID and OOD settings is close to zero, indicating that the models are able to generalize compositionally

Figure 6: **(top-left)** Proof accuracy on compositional examples where the in-context examples are also compositional examples with the same min depth and number of rule types. **(bottom-left)** Change in proof accuracy where the test examples are compositional but the in-context examples are those of individual deduction rules. See Figure 12 in the Appendix for the equivalent plot with absolute proof accuracy on the y-axis. **(right)** Example of an incorrect proof generated by GPT-3.5 on an out-of-demonstration example with min depth 2 and 4 rule types. The premises (axioms) are given in blue, and invalid steps are given in red. For the full example, see Figure 15 in the Appendix.

to an extent. This is surprising since past studies show that LLMs struggle with compositional generalization, but this could be due to the fact that much of the previous work focused on semantic parsing rather than on reasoning. But there is prior work showing that models can generalize compositionally in some settings, such as in Hosseini et al. (2022) (see Figure 4) and in Press et al. (2022) (see Figure 6). In addition, our study is limited by the token limit of the LLMs, as we are not able to further increase the complexity of the proofs without reducing the number of in-context examples, which would render the results difficult to compare. GPT-3.5 and PaLM have difficulty when the number of rule types is \(4\), with an example of an incorrect output given in the right side of Figure 6. Interestingly, PaLM, LLAMA, and FLAN-T5 sometimes perform better in the OOD setting than in the ID setting, showing that, in ICL, it is not always best to provide demonstrations from the same distribution as the test example.

#### 4.2.3 Can LLMs generalize to bigger proofs?

To test whether LLMs can generalize to bigger proofs, we test the models on examples where the proof width or depth is larger than those of the in-context examples. As is evident from Figure 7, when shown demonstrations of proofs of depth 2, the models' performance decreases with increasing depth. But this is due to the increase in the inherent difficulty of the task, as both ID and OOD accuracies decrease with increasing depth. Though the notable exception is GPT-3.5 on conjunction elimination, where ID accuracy remains high as OOD accuracy decreases. Models are able to generalize better with increasing proof width on conjunction elimination, possibly because there are ample examples of long conjunctions in natural language, but only GPT-3.5 is able to generalize to greater proof widths on conjunction introduction.

### Do distractors help OOD generalization?

In supervised learning, one challenge to OOD generalization is spurious correlations (Zhang et al., 2022). Intuitively, if ICL were to behave like supervised learning on in-context examples (Akyurek et al., 2023; Dai et al., 2023; von Oswald et al., 2022), we would expect that without distractors, the models would overfit to the in-context examples and perform poorly on OOD examples. An example where distractors hurt generalization is shown in Figure 9 where GPT-3.5 copies many of the distractor sentences into the output, likely due to the fact that it has learned to apply a copying heuristic from the in-context demonstrations. It seems only GPT-3.5 acquires these heuristics in implication and disjunction elimination. Surprisingly, this is not the case for all deduction rules, as is

Figure 7: **(top row)** Proof accuracy vs proof depth of test examples, where in-context examples have fixed proof depth \(2\). **(bottom row)** Proof accuracy vs proof width of test examples, where in-context examples have fixed proof width \(2\). Dashed lines indicate in-distribution accuracy, where the depth and width of the in-context examples are the same as that of the text-examples. In these experiments, there are \(4\) rather than \(8\) in-context examples.

visible in Figure 8. The models' performance is largely unaffected, with the exception of a few rules for specific models. This is in stark contrast to supervised learning, where it is always best to train on examples from the same distribution as the test example.

## 5 Conclusion and future work

In this study, we provide a systematic test of the general deductive reasoning capabilities of LLMs, specifically measuring their rule-, depth-, width-, and compositional generalization abilities. We found that LLMs exhibit mixed generalization to unseen deduction rules, but they exhibit more robust generalization to compositional proofs than previously suggested.

Figure 8: **(top)** Proof accuracy on examples where both the in-context examples and test examples have distractors. Sentences in all questions are ordered randomly. **(middle)** Proof accuracy when distractors are removed from the in-context examples, but not from the test examples. The sentences of the in-context questions have a fixed order (corresponding to a postorder traversal of the ontology tree), whereas the sentences in the test question have random order. See Figure 13 in the Appendix for the equivalent plot with absolute proof accuracy on the y-axis. **(bottom)** The same setting as (middle) except the distractors in the in-context examples are instead replaced with _irrelevant_ sentences. Implication elimination examples have proof width of \(1\) and depth of \(2\). Conjunction introduction, conjunction elimination, and disjunction introduction examples have proof width \(2\) and depth \(2\). Disjunction elimination examples have proof width \(3\) and depth \(1\). Proof by contradiction examples have proof width \(2\) and depth \(1\).

Figure 9: Example of an incorrect proof generated by GPT-3.5 on an OOD implication elimination example where the in-context demonstrations have no distractors, but the test example does. The premises (axioms) are given in blue, and invalid steps are given in red. For the full example, see Figure 16 in the Appendix.

One important future direction is to better understand the mechanism of ICL and CoT prompting. We found that in many cases, for a given test example, the best in-context examples were drawn from a distribution distinct from that of the test example. This is not explained by existing theories of Bayesian inference (Xie et al., 2022) or gradient descent (Akyurek et al., 2023; Dai et al., 2023; von Oswald et al., 2022). Are simpler examples better even if the test example is fairly complex? Should we include examples with a diverse set of deduction rules (Levy et al., 2022)? Or should the in-context examples focus on rules for which the model's OOD generalization is poor? Further study is needed to better characterize generalization from in-context examples.

## Reproducibility statement

For the sake of reproducibility of the analysis, model outputs (except those of PaLM), code for data generation, and analysis code are freely available with a permissive open-source license at github.com/asaparov/prontoqa. The generated data for all experiments in this paper is available in the file generated_ood_data.zip. The command python make_plots.py produces all figures used in this paper. GPT-3.5 experiments were run using the OpenAI API with the model text-davinci-003 on April \(20^{th}\)-\(23^{rd}\), \(2023\). Experiments for Figure 7 and the bottom row of Figure 8 were run on August \(3^{rd}\)-\(7^{th}\), \(2023\).

## Acknowledgements

We thank Tania Bedrax-Weiss, Xin Xu, and Deepak Ramachandran for their valuable feedback. This work was supported by Open Philanthropy, AWS AI, Samsung Advanced Institute of Technology (under the project Next Generation Deep Learning: From Pattern Recognition to AI), and in part through the NYU IT High Performance Computing resources, services, and staff expertise. NJ is supported by an NSF Graduate Research Fellowship under grant number 1839302.

## References

* Akyurek et al. (2023) Ekin Akyurek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? Investigations with linear models. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=0g0X4H8yl41.
* An et al. (2023) Shengnan An, Zeqi Lin, Qiang Fu, Bei Chen, Nanning Zheng, Jian-Guang Lou, and Dongmei Zhang. How do in-context examples affect compositional generalization? _CoRR_, abs/2305.04835, 2023. URL https://arxiv.org/abs/2305.04835.
* Anil et al. (2022) Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay V. Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. Exploring length generalization in large language models. _CoRR_, abs/2207.04901, 2022. doi: 10.48550/arXiv.2207.04901. URL https://doi.org/10.48550/arXiv.2207.04901.
* Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denmy Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanulayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. _CoRR_, abs/2204.02311, 2022. doi: 10.48550/arXiv.2204.02311. URL https://doi.org/10.48550/arXiv.2204.02311.
* Chen et al. (2020)Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models. _CoRR_, abs/2210.11416, 2022. doi: 10.48550/arXiv.2210.11416. URL https://doi.org/10.48550/arXiv.2210.11416.
* Dai et al. (2023) Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers. In _Findings of the Association for Computational Linguistics: ACL 2023_, Toronto, Canada, 2023. Association for Computational Linguistics.
* Gentzen (1935) G. Gentzen. Untersuchungen uber das logische schliessen i. _Mathematische Zeitschrift_, 39:176-210, 1935.
* Gontier et al. (2020) Nicolas Gontier, Koustuv Sinha, Siva Reddy, and Christopher Pal. Measuring systematic generalization in neural proof generation with transformers. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/fc84ad56f9f547eb89c72b9ac209312-Abstract.html.
* Han et al. (2022) Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, David Peng, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, Shafiq R. Joty, Alexander R. Fabbri, Wojciech Kryscinski, Xi Victoria Lin, Caiming Xiong, and Dragomir Radev. FOLO: natural language reasoning with first-order logic. _CoRR_, abs/2209.00840, 2022. doi: 10.48550/arXiv.2209.00840. URL https://doi.org/10.48550/arXiv.2209.00840.
* Hosseini et al. (2022) Arian Hosseini, Ankit Vani, Dzmitry Bahdanau, Alessandro Sordoni, and Aaron C. Courville. On the compositional generalization gap of in-context learning. In Jasmin Bastings, Yonatan Belinkov, Yanai Elazar, Dieuwke Hupkes, Naomi Saphra, and Sarah Wiegreffe, editors, _Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, BlackboxNLP@EMNLP 2022, Abu Dhabi, United Arab Emirates (Hybrid), December 8, 2022_, pages 272-280. Association for Computational Linguistics, 2022. URL https://aclanthology.org/2022.blackboxnlp-1.22.
* Huang and Chang (2022) Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A survey. _CoRR_, abs/2212.10403, 2022. doi: 10.48550/arXiv.2212.10403. URL https://doi.org/10.48550/arXiv.2212.10403.
* Kazemi et al. (2023) Seyed Mehran Kazemi, Najoung Kim, Deepti Bhatia, Xin Xu, and Deepak Ramachandran. LAMBADA: backward chaining for automated reasoning in natural language. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics_, Toronto, Canada, July 2023. Association for Computational Linguistics.
* Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In _Advances in Neural Information Processing Systems_, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html.
* Kudo et al. (2023) Keito Kudo, Yoichi Aoki, Tatsuki Kuribayashi, Ana Brassard, Masashi Yoshikawa, Keisuke Sakaguchi, and Kentaro Inui. Do deep neural networks capture compositionality in arithmetic reasoning? In Andreas Vlachos and Isabelle Augenstein, editors, _Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2023, Dubrovnik, Croatia, May 2-6, 2023_, pages 1343-1354. Association for Computational Linguistics, 2023. URL https://aclanthology.org/2023.eacl-main.98.
* Levy et al. (2022) Itay Levy, Ben Bogin, and Jonathan Berant. Diverse demonstrations improve in-context compositional generalization. _CoRR_, abs/2212.06800, 2022. doi: 10.48550/arXiv.2212.06800. URL https://doi.org/10.48550/arXiv.2212.06800.
* Liu et al. (2020)Aitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=IFXTZERXdM7.
* Liang et al. (2022) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher Re, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel J. Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S. Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. Holistic evaluation of language models. _CoRR_, abs/2211.09110, 2022. doi: 10.48550/arXiv.2211.09110. URL https://doi.org/10.48550/arXiv.2211.09110.
* Liu et al. (2020) Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. In Christian Bessiere, editor, _Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020_, pages 3622-3628. ijcai.org, 2020. doi: 10.24963/ijcai.2020/501. URL https://doi.org/10.24963/ijcai.2020/501.
* McKenzie et al. (2022) Ian McKenzie, Alexander Lyzhov, Alicia Parrish, Ameya Prabhu, Aaron Mueller, Najoung Kim, Sam Bowman, and Ethan Perez. Inverse scaling prize: Second round winners, 2022. URL https://irmckenzie.co.uk/round2.
* Nye et al. (2021) Maxwell I. Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models. _CoRR_, abs/2112.00114, 2021. URL https://arxiv.org/abs/2112.00114.
* Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=TGSKACxEON.
* Pfenning (2004) Frank Pfenning. Natural deduction. Lecture notes in 15-815 Automated Theorem Proving, 2004. URL https://www.cs.cmu.edu/~fp/courses/atp/handouts/ch2-nateded.pdf.
* Press et al. (2022) Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. _CoRR_, abs/2210.03350, 2022. doi: 10.48550/arXiv.2210.03350. URL https://doi.org/10.48550/arXiv.2210.03350.
* Qiu et al. (2022) Linlu Qiu, Peter Shaw, Panupong Pasupat, Tianze Shi, Jonathan Herzig, Emily Pitler, Fei Sha, and Kristina Toutanova. Evaluating the impact of model scale for compositional generalization in semantic parsing. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022_, pages 9157-9179. Association for Computational Linguistics, 2022. URL https://aclanthology.org/2022.emnlp-main.624.
* Sanyal et al. (2022) Soumya Sanyal, Zeyi Liao, and Xiang Ren. Robustlr: A diagnostic benchmark for evaluating logical robustness of deductive reasoners. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022_, pages 9614-9631. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.emnlp-main.653. URL https://doi.org/10.18653/v1/2022.emnlp-main.653.
* Sanyal et al. (2022)Abulhair Saparov and He He. Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=qFVVB2xRx2V.
* Sinha et al. (2019) Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William L. Hamilton. CLUTRR: A diagnostic benchmark for inductive reasoning from text. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019_, pages 4505-4514. Association for Computational Linguistics, 2019. doi: 10.18653/v1/D19-1458. URL https://doi.org/10.18653/v1/D19-1458.
* Tafjord et al. (2021) Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. Proofwriter: Generating implications, proofs, and abductive statements over natural language. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, _Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021_, volume ACL/IJCNLP 2021 of _Findings of ACL_, pages 3621-3634. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.findings-acl.317. URL https://doi.org/10.18653/v1/2021.findings-acl.317.
* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lacchaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLAMA: Open and efficient foundation language models. _CoRR_, abs/2302.13971, 2023. doi: 10.48550/arXiv.2302.13971. URL https://doi.org/10.48550/arXiv.2302.13971.
* Oswald et al. (2022) Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. _CoRR_, abs/2212.07677, 2022. doi: 10.48550/arXiv.2212.07677. URL https://doi.org/10.48550/arXiv.2212.07677.
* Wang et al. (2023) Xinyi Wang, Wanrong Zhu, and William Yang Wang. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning. _CoRR_, abs/2301.11916, 2023. doi: 10.48550/arXiv.2301.11916. URL https://doi.org/10.48550/arXiv.2301.11916.
* Wei et al. (2022a) Jason Wei, Yi Tay, and Quoc V. Le. Inverse scaling can become u-shaped. _CoRR_, abs/2211.02011, 2022a. doi: 10.48550/arXiv.2211.02011. URL https://doi.org/10.48550/arXiv.2211.02011.
* Wei et al. (2022b) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022b. URL https://openreview.net/forum?id=_VjQ1MeSB_3.
* Wu et al. (2021) Yuhuai Wu, Albert Q. Jiang, Jimmy Ba, and Roger Baker Grosse. INT: an inequality benchmark for evaluating generalization in theorem proving. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021. URL https://openreview.net/forum?id=06LPudowNQm.
* Xie et al. (2022) Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022. URL https://openreview.net/forum?id=RdJVFCHjUMI.
* Ye et al. (2022) Xi Ye, Srinivasan Iyer, Asli Celikyilmaz, Ves Stoyanov, Greg Durrett, and Ramakanth Pasunuru. Complementary explanations for effective in-context learning. _CoRR_, abs/2211.13892, 2022. doi: 10.48550/arXiv.2211.13892. URL https://doi.org/10.48550/arXiv.2211.13892.
* Zhang et al. (2022a) Hanlin Zhang, Yi-Fan Zhang, Li Erran Li, and Eric P. Xing. The impact of symbolic representations on in-context learning for few-shot reasoning. _CoRR_, abs/2212.08686, 2022a. doi: 10.48550/arXiv.2212.08686. URL https://doi.org/10.48550/arXiv.2212.08686.

Honghua Zhang, Liunian Harold Li, Tao Meng, Kai-Wei Chang, and Guy Van den Broeck. On the paradox of learning to reason from data. _CoRR_, abs/2205.11502, 2022b. doi: 10.48550/arXiv.2205.11502. URL https://doi.org/10.48550/arXiv.2205.11502.

## Appendix A Appendix

### Example from PrOntoQA-OOD

Q: Everything that is a lorpus, a brimpus, or a jompus is a shumpus. Every wumpus is a vumpus and a sternus and a brimpus. Everything that is a vumpus, a grimpus, or a brimpus is a lempus. Everything that is a lempus or a jompus or a lorpus is a dumpus. Vumpuses are rompuses. Every sternus is a gorpus. Everything that is a vumpus, a grimpus, or a brimpus is a dumpus. Wumpuses are shumpuses. Polly is a rompus. Polly is a wumpus. Prove: Polly is a lempus or an impus or a yumpus.

A: Polly is a wumpus. Every wumpus is a vumpus and a sternus and a brimpus. Polly is a vumpus and a sternus and a brimpus. Polly is a brimpus. Polly is a vumpus, a grimpus, or a brimpus. Everything that is a vumpus, a grimpus, or a brimpus is a lempus. Polly is a lempus, an impus, or a yumpus.

### Results with absolute accuracy

Figure 11: **(top)** Proof accuracy across examples with different deduction rules. The in-context examples and test examples come from the same distribution. **(bottom)** Proof accuracy where the test example is out-of-demonstration with respect to the in-context examples (for comparison, the in-demonstration proof accuracy is shown as the dotted black bars). That is, the test example has the specified deduction rule, but the in-context examples are uniformly distributed over _all other_ deduction rules. See Figure 5 for an incorrect example. Implication elimination examples have proof width of \(1\) and depth of \(2\). Conjunction introduction, conjunction elimination, and disjunction introduction examples have proof width \(3\) and depth \(2\). Disjunction elimination examples have proof width \(3\) and depth \(1\). Proof by contradiction examples have proof width \(2\) and depth \(1\).

Figure 10: An example from PrOntoQA-OOD. This is a compositional example with a min depth of 4 and 3 rule types. The given answer is the expected chain-of-thought. The question is shown in blue, the query in red, and the chain-of-thought/answer in green.

Figure 12: **(top)** Proof accuracy on compositional examples where the in-context examples are also compositional examples with the same min depth and number of rule types. **(bottom)** Proof accuracy where the test examples are compositional but the in-context examples are those of individual deduction rules (for comparison, the in-demonstration proof accuracy is shown as the dotted black bars).

Figure 13: **(top)** Proof accuracy on examples where both the in-context examples and test examples have distractors. Sentences in all questions are ordered randomly. **(middle)** Proof accuracy when distractors are removed from the in-context examples, but not from the test examples (for comparison, the in-demonstration proof accuracy is shown as the dotted black bars). The sentences of the in-context questions have a fixed order (corresponding to a postorder traversal of the ontology tree), whereas the sentences in the test question have random order. **(bottom)** The same setting as (middle) except the distractors in the in-context examples are instead replaced with _irrelevant_ sentences. Implication elimination examples have proof width of \(1\) and depth of \(2\). Conjunction introduction, conjunction elimination, and disjunction introduction examples have proof width \(2\) and depth \(2\). Disjunction elimination examples have proof width \(3\) and depth \(1\). Proof by contradiction examples have proof width \(2\) and depth \(1\).

### Full examples of incorrect predicted proofs

_Question:_Lempuses are gorpuses. Rompuses are numpuses. Tumpuses are numpuses. Dumpuses are gorpuses. Every zumpus is a numpus. Every rhimpus is a gorpus. Yumpuses are gorpuses. Lempuses are numpuses. Rompuses are gorpuses. Tumpuses are gorpuses. Max is a tumpus or a rompus. Max is a zumpus, a tumpus, or a rompus.

_Prove:_Max is a gorpus.

_Predicted answer:_Max is a tumpus or a rompus or a lempus. Max is a tumpus. Tumpuses are numpuses. Max is a vumpus. Rompuses are gorpuses. Max is a gorpus.

_Expected answer:_Assume Max is a tumpus. Tumpuses are gorpuses. Max is a gorpus. Assume Max is a rompus. Rompuses are gorpuses. Max is a gorpus. Assume Max is a lempus. Lempuses are gorpuses. Max is a gorpus. Since Max is a tumpus or a rompus or a lempus, Max is a gorpus.

Figure 16: Example of an incorrect proof generated by GPT-3.5 on an OOD implication elimination example where the in-context demonstrations have no distractors, but the test example does. The premises (axioms) are given in blue, and invalid steps are given in red.

Figure 14: Example of an incorrect proof generated by GPT-3.5 on an out-of-demonstration disjunction elimination example. The premises (axioms) are given in blue, and invalid steps are given in red.

Figure 15: Example of an incorrect proof generated by GPT-3.5 on an out-of-demonstration example with min depth 2 and 4 rule types. The premises (axioms) are given in blue, and invalid steps are given in red.

### Generative process details

In this section, we describe the process to generate examples of each deduction rule.

Implication elimination(i.e. _modus ponens_)Given \(f(c)\) and \(\forall x(f(x)\to g(x))\), prove \(g(c)\). These are the examples in the original PrOntoQA. We follow the same process here:

1. Generate an ontology. For simplicity, we generate linear ontologies, consisting of a collection of concepts, as well as subtype-supertype relations between those concepts (i.e. concept \(f\) is a subtype of the supertype \(g\) if every instance of \(f\) is an instance of \(g\)). For simplicity, we limit each type to have at most one supertype.
2. Perform a random walk of length \(k\) from a randomly selected start vertex, where \(k\) is the desired proof depth.
3. Traverse the edges of the ontology and convert each into a sentence of the question.
4. Convert each step of the random walk into a sentence of the gold chain-of-thought.

Note that this process allows us to generate proofs of any depth, but the width is fixed to \(1\).

Conjunction introductionGiven \(A\) and \(B\), prove \(A\wedge B\). The generative process is a modified version of that for implication elimination. Instead of generating rules of the form \(\forall x(f(x)\to g(x))\), we generate rules of the form \(\forall x(f_{1}(x)\wedge\ldots\wedge f_{n}(x)\to g(x))\), where \(n\) is the proof width. Given, \(f_{1}(c)\), \(\ldots\), and \(f_{n}(c)\), the model must first prove \(f_{1}(c)\wedge\ldots\wedge f_{n}(c)\) before applying implication elimination to prove \(g(c)\). To increase the depth of the proof, \(g(c)\) itself can be part of a conjunct in the antecedent of another rule.

Conjunction eliminationGiven \(A\wedge B\), prove \(A\). These examples are identical to those in conjunction introduction, except the conjunction appears in the consequent of each rule, rather than in the antecedent: \(\forall x(f(x)\to g_{1}(x)\wedge\ldots\wedge g_{n}(x))\) where \(n\) is the proof width.

Disjunction introductionGiven \(A\), prove \(A\lor B\). These examples are identical to those in conjunction introduction, except the conjunction is replaced with disjunction: \(\forall x(f_{1}(x)\vee\ldots\lor f_{n}(x)\to g(x))\) where \(n\) is the proof width. But note that grounded axioms are not necessary for every disjunct: To apply the rule \(\forall x(f_{1}(x)\vee\ldots\lor f_{n}(x)\to g(x))\), knowing \(f_{n}(c)\) is sufficient, and we do not need to generate grounded axioms for the other disjuncts \(f_{i}(c)\) for \(i<n\).

Disjunction elimination(i.e. _proof by cases_)Given \(A_{1}\vee\ldots\lor A_{n}\), and \(A_{i}\vdash C\) for all \(i\), prove \(C\). Here, \(n\) is the proof width. While it is possible to construct proofs containing multiple nested applications of disjunction elimination, such proofs are quite complex, even for humans to understand, and so we fix the depth of these examples to \(1\). To generate an example, we first generate the disjunction: \(f_{1}(c)\vee\ldots\lor f_{n}(c)\). Next, generate the rules for each case: \(\forall x(f_{i}(x)\to g(x))\) for all \(i\). The goal is to prove \(g(c)\).

Proof by contradictionGiven \(A\vdash B\) and \(\neg B\), prove \(\neg A\). Note that this is a rule composed of two natural deduction rules: negation elimination and introduction. But since those individual rules do not lend themselves to a natural text representation, we choose to study their composition. Similar to disjunction elimination, it is possible to construct proofs containing multiple nested applications of proof by contradiction, but such proofs are unnaturally complex. So we fix the depth to \(1\). To generate an example, we first generate an axiom \(\neg g(c)\). Next, for each subproof, we generate a rule \(\forall x(f_{1}(x)\vee\ldots\lor f_{n}(x)\to g(x))\), where \(n\) is the proof width. The goal is to prove \(\neg f_{1}(c)\wedge\ldots\wedge\neg f_{n}(c)\). Note that in addition to proof by contradiction, this proof requires disjunction introduction, implication elimination, and conjunction introduction.

Note that the above list constitutes a complete set of deduction rules from propositional natural deduction, save for one rule: implication introduction. However, it is unclear how to construct an example with this deduction rule where its difficulty can be controlled by increasing the width or depth of the proof (e.g. how can a statement of the form \(A_{1}\to A_{2}\rightarrow\ldots\to A_{n}\) be expressed in natural language?).

```
1functiongenerate_compositional_proof(set of possible conclusions (logical forms) \(C\), \(\mathit{disallowed deduction rules}\) \(R\), \(\mathit{requested depth}\) \(d\), \(\mathit{ground entity}\) \(e\), \(\mathit{is\ proof hypothetical}\) \(h\))
2 initialize \(A\) as the set of all deduction rules excluding those in \(R\) /* filter deduction rules such that: (1) an element of \(C\) can be a conclusion of the rule, (2) for which we have sufficient depth, and (3) we don't create overly complex logical forms */
3if\(C\) does not contain a conjunction
4 set \(A=A\setminus\{\)conjunction_introduction\(\}\)
5if\(C\) does not contain a disjunction
6 set \(A=A\setminus\{\)disjunction_introduction\(\}\)
7if\(h=\)true or\(d=1\) or \(C\) does not contain a negation
8 set \(A=A\setminus\{\)proof_by_contradiction\(\}\)
9if\(h=\)true or\(d=1\) or \(C\) contains only conjunctions or only disjunctions
10 set \(A=A\setminus\{\)disjunction_elimination\(\}\)
11if\(C\) contains only conjunctions or only disjunctions
12 set \(A=A\setminus\{\)conjunction_elimination\(\}\)
13if\(C\) contains only conjunctions or only disjunctions and any operand is negated
14 set \(A=A\setminus\{\)implication_elimination\(\}\)
15if\(d=0\) or \(C\) contains a singleton logical form or \(A=\varnothing\)
16returnaxiomsep with conclusion given by sample\((C,e)\)
17\(r=\)sample_uniform\((A)\)
18if\(r=\)implication_elimination
19do
20 for any \(c\in C\), \(a\) and \(c\) share any operands or negations of operands
21while\(a=\)generate_compositional_proof\((\Omega,\varnothing,d-1,e,h)\)
22do
23\(a\) and \(s\) do not share any operands or negations of operands
24while\(s=\)sample\((C,e)\)
25returnimplication_elimination with premises \(a\) and \(\forall x(a[e\to x]\to s[e\to x])\)
26elseif\(r=\)conjunction_introduction
27 initialize \(P\) as an empty list, and \(i=0\)
28\(L=|C|\) if \(C\) contains only conjunctions, else \(L=3\)
29do
30 let \(C_{i}=i^{th}\) operand of \(C\) if \(C\) contains only conjunctions, else \(C_{i}=\Omega\)
31\(a=\)generate_compositional_proof\((C_{i},\{\)conjunction_elimination\(\},d-1,e,h)\)
32if\(a\) is atomic and \(a\) is not any other operand of \(C\)
33 append \(a\) to \(P\)
34\(i=i+1\)
35while\(i<L\)
36returnconjunction_introduction with premises \(P\)
37elseif\(r=\)conjunction_elimination
38 let \(C^{\prime}\) be the set of conjunctions of length 3, \(i=\)sample_uniform\((\{1,2,3\})\)
39\(C^{\prime}=\{c\in C^{\prime}:\) the \(i^{th}\) operand of \(c^{\prime}\) is in \(C\}\)
40do
41\(a=\)generate_compositional_proof\((C^{\prime},\{\)conjunction_introduction\(\},d-1,e,h)\)
42while\(a\) has no duplicate operands, and each operand of \(a\) is not itself a conjunction or disjunction
43returnconjunction_elimination with premise \(a\) and conclusion given by the \(i^{th}\) operand of \(a\) ```

**Algorithm 1**Pseudocode for generating examples of compositional proofs in ProntoQA-OOD. In this algorithm, \(\Omega\) denotes the set of all logical forms. The function generate_compositional_proof is initially called with parameters \(\Omega\), \(\varnothing\), \(d\), \(e\), and false, where \(d\) is the requested depth and \(e\) is a randomly selected entity name (e.g. alex, fae, etc. sample is a helper function that, given an input set of logical forms \(S\) and an entity \(e\), returns_sample_uniform\((\)set of logical forms in \(S\) with minimal depth where all atoms are of the form \(t(e)\) where \(t\) is a predicate\()\)\()\).

```
1functiongenerate_compositional_proof(set of possible conclusions (logical forms) \(C\), \(\mathit{disallowed deduction rules}\) \(R\), \(\mathit{requested depth}\) \(d\), \(\mathit{ground entity}\) \(e\), \(\mathit{is\ proof hypothetical}\) \(h\))
2 initialize \(A\) as the set of all deduction rules excluding those in \(R\) /* filter deduction rules such that: (1) an element of \(C\) can be a conclusion of the rule, (2) for which we have sufficient depth, and (3) we don't create overly complex logical forms */
3if\(C\) does not contain a conjunction
4 set \(A=A\setminus\{\)conjunction_introduction\(\}\)
5if\(h=\)true or\(d=1\) or \(C\) does not contain a negation
6 set \(A=A\setminus\{\)proof_by_contradiction\(\}\)
7if\(h=\)true or\(d=1\) or \(C\) contains only conjunctions or only disjunctions
8 set \(A=A\setminus\{\)disjunction_elimination\(\}\)
9if\(C\) contains only conjunctions or only disjunctions and any operand is negated
10 set \(A=A\setminus\{\)implication_elimination\(\}\)
11if\(d=0\) or \(C\) contains a singleton logical form or \(A=\varnothing\)
12returnaxiomsep with conclusion given by sample\((C,e)\)
13\(r=\)sample_uniform\((A)\)
14if\(r=\)implication_elimination
15do
16for any \(c\in C\), \(a\) and \(c\) share any operands or negations of operands
17while\(a=\)generate_compositional_proof\((\Omega,\varnothing,d-1,e,h)\)
18do
19\(a\) and \(s\) do not share any operands or negations of operands
20while\(s=\)sample(\(C,e\))
21return implication_elimination with premises \(a\) and \(\forall x(a[e\to x]\to s[e\to x])\)
22elseif\(r=\)conjunction_introduction
23 initialize \(P\) as an empty list, and \(i=0\)
24\(L=|C|\) if \(C\) contains only conjunctions, else \(L=3\)
25do
26 let \(C_{i}=i^{th}\) operand of \(C\) if \(C\) contains only conjunctions, else \(C_{i}=\Omega\)
27\(a=\)generate_compositional_proof\((C_{i},\{\)conjunction_elimination\(\},d-1,e,h)\)
28if\(a\) is atomic and \(a\) is not any other operand of \(C\)
29 append \(a\) to \(P\)
30\(i=i+1\)
31while\(i<L\)
32return conjunction_introduction with premises \(P\)
33elseif\(r=\)conjunction_elimination
34 let \(C^{\prime}\) be the set of conjunctions of length 3, \(i=\)sample_uniform\((\{1,2,3\})\)
35\(C^{\prime}=\{c\in C^{\prime}:\) the \(i^{th}\) operand of \(c^{\prime}\) is in \(C\}\)
36do
37\(a=\)generate_compositional_proof\((C^{\prime},\{\)conjunction_introduction\(\},d-1,e,h)\)
38while\(a\) has no duplicate operands, and each operand of \(a\) is not itself a conjunction or disjunction
39returnconjunction_elimination with premise \(a\) and conclusion given by the \(i^{th}\) operand of \(a\) ```

**Algorithm 2**Pseudocode for generating examples of compositional proofs in ProntoQA-OOD. In this algorithm, \(\Omega\) denotes the set of all logical forms. The function generate_compositional_proof is initially called with parameters \(\Omega\), \(d\), \(e\), and false, where \(d\) is the requested depth and \(e\) is a randomly selected entity name (e.g. alex, fae, etc. sample is a helper function that, given an input set of logical forms \(S\) and an entity \(e\), returns_sample_uniform\((\)set of logical forms in \(S\) with minimal depth where all atoms are of the form \(t(e)\) where \(t\) is a predicate\()\()\)\()\()\).

```
4elseif\(r=\texttt{disjunction\_introduction}\)
5if\(C=\Omega\) let \(C\) be the set of disjunctions of length 3
6\(i=\texttt{sample\_uniform}(\) number of disjuncts in \(C)\)
7do
8 let \(C_{i}=i^{th}\) operand of \(C\)
9\(a=\texttt{generate\_compositional\_proof}(C_{i},\{\texttt{disjunction\_elimination}\},d-1,e,h)\)
10while\(a\) is atomic and \(a\) is not any other operand of \(C\)
11 replace \(i^{th}\) operand of \(C\) with \(a\)
12do
13\(x=\texttt{sample}(C,e)\)
14while\(i^{th}\) disjunct of \(x\) is distinct from all other disjuncts
15return\(\texttt{disjunction\_introduction}\) with premise given by the \(i^{th}\) operand of \(x\) and conclusion \(x\)
16elseif\(r=\texttt{disjunction\_elimination}\)
17 initialize \(P\) as an empty list
18while\(|P|<2\)do
19\(p=\texttt{generate\_compositional\_proof}(C,\{\texttt{disjunction\_introduction}\},d-1,e, \texttt{true})\)
20if\(p\) is not a conjunction or disjunction and \(p\) has an axiom that is not an axiom of any \(q\in P\)
21 append \(p\) to \(P\)
22
23 let \(A_{i}\) be the set of axioms of \(P_{i}\) that are not axioms of \(P_{j}\) for \(i\neq j\)
24 let \(a_{i}=\texttt{sample\_uniform}(A_{i})\) for all \(i\)
25 let \(a^{\prime}\) be a disjunction with disjuncts \(a_{i}\)
26\(a=\texttt{generate\_compositional\_proof}(\{a^{\prime}\},\{\texttt{disjunction\_introduction}\},d-1,e,h)\)
27return\(\texttt{disjunction\_introduction}\) with premises \(a\) and \(P_{i}\)
28elseif\(r=\texttt{proof\_by\_contradiction}\)
29 let \(N\) be the set of all negated logical forms
30\(a=\texttt{generate\_compositional\_proof}(N,\{\texttt{proof\_by\_contradiction}\},d-1,h)\)
31do
32 let \(a=\neg s\)
33\(b=\texttt{generate\_compositional\_proof}(s),\{\texttt{proof\_by\_contradiction}\},d-1,e, \texttt{true})\)
34while\(b\) has an atomic non-negated axiom that is not an axiom of \(a\)
35\(s^{\prime}=\texttt{sample\_uniform}(\{\texttt{atomic\ non-negated\ axioms\ of\ }b\) that are not axioms of \(a\})\)
36return\(\texttt{proof\_by\_contradiction}\) with premises \(a\) and \(b\) and conclusion \(\neg s^{\prime}\) ```

**Algorithm 1**(continued from previous page)

### Generating compositional proofs

We use a simple recursive procedure to generate compositional proofs: (1) select a deduction rule uniformly at random, (2) select the premises for the selected rule, (3) recursively generate a subproof for each premise. A consistency checking step is required to make sure we avoid generating contradictory axioms.6 In addition, we avoid generating an elimination rule directly following an introduction rule (or vice versa).7 See Algorithm 1 in the appendix for pseudocode of this procedure. To test compositional proofs of various sizes, we implement a parameter that controls the minimum depth of the proof tree, and another parameter that controls the number of distinct rule types in each proof.

Footnote 6: An example is: Suppose we select conjunction introduction as the first rule; next, we recursively generate the proof of each conjunct; suppose for each of these, we choose to generate the axioms \(\texttt{cat}(\texttt{alex})\) and \(\neg\texttt{cat}(\texttt{alex})\).

Footnote 7: In the following example, a conjunction introduction step immediately follows a conjunction elimination step: “lay is a cat and orange. Jay is a cat. Jay is orange. Jay is a cat and orange.”

### Further details on evaluation of CoT

We aim to test whether LLMs are able to use deduction rules OOD, where the rules do not appear in the in-context examples, and we take care not to be overly strict. For example, we wish to avoid penalizing the model for formatting differences, so long as the reasoning is correct. To this end, in determining whether a logical form follows from previous logical forms, we consider any deduction rule listed in Table 2. We also allow for two additional rules: (1) given \(\forall x(f(x)\to g(x))\) and \(\forall x(g(x)\to h(x))\) conclude \(\forall x(f(x)\to h(x))\), and (2) given \(\forall x(f(x)\to g(x))\) and \(\neg g(c)\) conclude \(\neg f(c)\) (i.e. modus tollens).8 Additionally, we are flexible with respect to the ordering of conjuncts and disjuncts. For example, given the previous steps \(f(a)\wedge g(a)\) and \(\forall x(g(x)\wedge f(x)\to u(x)\lor v(x))\), we consider \(v(a)\lor u(a)\) to be valid.

Footnote 8: Analogous to the _broadly-valid_ steps in ProntoQA.

### Generating distractors

Implication eliminationFor any rule \(\forall x(f(x)\to g(x))\) in the gold proof, we generate a distractor rule \(\forall x(f(x)\to h(x))\) where the concept \(h\) is a distractor and is not helpful in completing the proof. In addition, for any ground logical form in the gold proof \(f(c)\), we generate a distractor logical form \(h(c)\) as well as a rule \(\forall x(h(x)\to h^{\prime}(x))\). Note that the original ProntoQA only adds a single distractor, whereas we add multiple, one for each hop in the proof.

Conjunction introductionSimilar to those in implication elimination. For any rule \(\forall x(f_{1}(x)\wedge\ldots\wedge f_{n}(x)\to g(x))\), we generate a rule of the form \(\forall x(h_{1}(x)\wedge\ldots\wedge h_{n-1}(x)\wedge f_{n}(x)\to g(x))\) where \(h_{i}\) are distractor concepts. Grounded distractor conjuncts are also generated as axioms \(h_{i}(c)\), so that, given \(f_{n}(c)\), both the gold rule and distractor rule are valid proof steps.

Conjunction eliminationDistractors are generated similarly to the conjunction introduction case.

Disjunction introductionDistractors are generated similarly to the conjunction introduction case.

Disjunction eliminationSince this deduction step has many premises, multiple distractors are necessary to ensure the model doesn't resort to heuristics. For every rule of the form \(\forall x(f_{i}(x)\to g(x))\), two distractor rules are generated: \(\forall x(f_{i}(x)\to h^{\prime}(x))\) and \(\forall x(h_{i}(x)\to g(x))\). A distractor disjunction is also generated: \(h^{\prime\prime}(c)\lor h_{1}(c)\vee\ldots\lor h_{n-1}(c)\).

Proof by contradictionAs with disjunction elimination, multiple distractors are necessary here. We generate two distractor rules \(\forall x(f_{1}(x)\vee\ldots\lor f_{n}(x)\to h(x))\) and \(\forall x(h_{1}(x)\vee\ldots\lor h_{n}(x)\to g(x))\). We also generate the distractor axiom \(\neg h^{\prime}(c)\) so that the model is forced to choose between two axioms for the first step of the proof.

To avoid creating inconsistencies when generating a distractor rule, we avoid using existing predicates in the consequent of each rule.

```
1functionevaluate_cott(context sentences \(Q_{1},\dots,Q_{m}\), predicted chain-of-thought sentences \(C_{1},\dots,C_{n}\), goal sentence \(g\))
2\(L^{g}=\)semantic_parse(\(g\)) /* parse the goal */
3for\(i\in 1,\dots,m\)do /* parse the context */
4\(L^{Q}_{i}=\)semantic_parse(\(Q_{i}\))
5for\(i\in 1,\dots,m\)do /* parse the predicted chain-of-thought */
6\(L^{C}_{i}=\)semantic_parse(\(C_{i}\))
7 initialize \(S\) as an empty set, and \(H\) as an empty map
8for\(i\in 1,\dots,m\)do /* reconstruct the proof from the chain-of-thought */
9if\(\textbf{{L}}^{C}_{i}\) indicates 'this is a contradiction'
10ifnegate(\(L^{C}_{i+1}\)) \(\in H(L^{C}_{i-1})\)
11\((P,D,k)=(\{L^{C}_{i-1},\textbf{negate}(L^{C}_{i+1})\},\{\textbf{negate}(L^{C}_{ i+1})\},1)\)
12elsecontinue
13else
14\((P,D,k)=\)is_provable(\(L^{C}_{i},\{L^{Q}_{1},\dots,L^{Q}_{m}\},S,H\))
15 set \(H(L^{C}_{i})=\bigcup_{p\in P}H(p)\setminus D\)
16if\(k\geq 0\)
17 add \(L^{C}_{i}\) to \(S\)
18 return\(L^{g}\in S\) /* the proof is correct if the final conclusion is provable */
19functionis_provable(logical form \(\varphi\), set of axioms \(A\), previous conclusions \(S\), hypothesis map \(H\))
20if\(\varphi\in A\)
21return\((\{\varphi\},1)\) /* this is an axiom */
22elseif\(\varphi\) is a conjunction or disjunction
23 initialize \(P^{\prime}\) as an empty list, and \(k^{\prime}=0\)
24for\(\varphi_{i}\) operation in \(\varphi\)do
25\((P,k)=\)is_provable(\(\varphi_{i},A,S,H\))
26if\(\varphi\) is a conjunction
27if\(k\geq 0\) and the step immediately preceding \(\varphi\) in the proof is in \(P\)
28 append \(P\) to \(P^{\prime}\)
29 set \(k^{\prime}=k^{\prime}+k\)
30elsebreak
31elseif\(k>0\) and \(\varphi\) is a disjunction
32return\((P,\varnothing,k+1)\) /* provable by disjunction introduction */
33if\(P^{\prime}\) has the same size as \(\varphi\) has operands
34return\((\bigcup P^{\prime},\varnothing,k^{\prime})\) /* provable by conjunction introduction */
35for\(a\in S\cup A\)do
36if\(a\) is a conjunction and \(\varphi=a_{i}\) for some \(i\)
37return\((\{a\},\varnothing,1+\textbf{1}\{a\in A\})\) /* provable by conjunction elimination */
38elseif\(a\) has form \(\forall x(\psi-\gamma)\) where \(\gamma[x\mapsto c]=\varphi\)
39\((P,k)=\)is_provable(\(\psi[x\mapsto c],A,S,H\))
40if\(k\geq 0\) and the step immediately preceding \(\varphi\) in the proof is in \(P\cup\{a\}\)
41return\((P\cup\{a\},\varnothing,k+\textbf{1}\{a\in A\})\) /* provable by conjunction elimination */
42
43for\(s\in S\) where \(s\) is a disjunction do
44iffor all disjuncts \(s_{i}\), there is a \(s_{j}\in S\) such that \(s_{j}=\varphi\) and \(s_{i}\in H(s_{j})\)
45return\((\{s_{j}\},\{s_{i}\},1)\) /* provable by disjunction elimination */ ```

**Algorithm 2**Pseudocode for evaluating the output chain-of-thought. Here, the comparison operations between logical forms ignore the order of conjuncts if both operands are conjunctions; and similarly for disjunctions. In addition, when iterating over previous steps in the proof, we consider them in reverse order, so that more recent steps are prioritized. The helper function negate is defined, in order of precedence: negate(\(\neg A\)) \(=\)\(A\), negate(\(A\lor B\)) \(=\) negate(\(A\))\(\wedge\) negate(\(B\)), negate(\(A\wedge B\)) \(=\) negate(\(A\))\(\vee\) negate(\(B\)), or negate(\(A\)) = \(\neg A\).

```
45for\(a\in S\cup A\)do
46if\(a\) has form \(\forall x(\psi\to\gamma)\) where \(\gamma[x\mapsto c]=\varphi\)
47\((P,k)=\texttt{is\_provable}(\psi[x\mapsto c],A,S,H)\)
48if\(k\geq 0\) and the step immediately preceding \(\varphi\) in the proof is in \(P\cup\{a\}\)return\((P\cup\{a\},\varnothing,k+\texttt{1}\{a\in A\})\) /* provable by implication elimination */
49elseif\(a\) has form \(\forall x(\psi\to\gamma)\) where \(\texttt{negate}(\psi[x\mapsto c])=\varphi\)
50\((P,k)=\texttt{is\_provable}(\texttt{negate}(\gamma[x\mapsto c]),A,S,H)\)
51if\(k\geq 0\) and the step immediately preceding \(\varphi\) in the proof is in \(P\cup\{a\}\)
52\(\not*\) provable with additional deduction rules (modus tollens) */
53return\((P\cup\{a\},\varnothing,k+\texttt{1}\{a\in A\})\)  return\((\{\varphi\},\varnothing,0)\) /* proved by previous step */
54elseif\(\varphi\) has form \(\forall x(\psi\to\gamma)\) /* note: we precompute this graph */
55 let \(G\) be the graph where for any axiom in \(A\) with form \(\forall x(\alpha\to\beta)\), \(\alpha\) and \(\beta\) are vertices and there is a directed edge from \(\alpha\) to \(\beta\)
56ifthere is a path in \(G\) from \(\psi\) to \(\gamma\)
57 /* provable with additional deduction rules */
58return\((\) axioms corresponding to path edges \(,\varnothing\), length of path \()\)
59
60return\((\varnothing,\varnothing,-1)\) /* this step is not provable (i.e., invalid) */ ```

**Algorithm 2**(continued from previous page)