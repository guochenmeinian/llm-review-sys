# Time-Reversed Dissipation Induces Duality Between Minimizing Gradient Norm and Function Value

Jaeyeon Kim

Seoul National University

kjy011102@snu.ac.kr

&Asuman Ozdaglar

MIT EECS

asuman@mit.edu

&Chanwoo Park

MIT EECS

cpark97@mit.edu

&Ernest K. Ryu

Seoul National University

ernestryu@snu.ac.kr

###### Abstract

In convex optimization, first-order optimization methods efficiently minimizing function values have been a central subject study since Nesterov's seminal work of 1983. Recently, however, Kim and Fessler's OGM-G and Lee et al.'s FISTA-G have been presented as alternatives that efficiently minimize the gradient magnitude instead. In this paper, we present H-duality, which represents a surprising one-to-one correspondence between methods efficiently minimizing function values and methods efficiently minimizing gradient magnitude. In continuous-time formulations, H-duality corresponds to reversing the time dependence of the dissipation/friction term. To the best of our knowledge, H-duality is different from Lagrange/Fenchel duality and is distinct from any previously known duality or symmetry relations. Using H-duality, we obtain a clearer understanding of the symmetry between Nesterov's method and OGM-G, derive a new class of methods efficiently reducing gradient magnitudes of smooth convex functions, and find a new composite minimization method that is simpler and faster than FISTA-G.

## 1 Introduction

Since Nesterov's seminal work of 1983 [37], accelerated first-order optimization methods that efficiently reduce _function values_ have been central to the theory and practice of large-scale optimization and machine learning. In 2012, however, Nesterov initiated the study of first-order methods that efficiently reduce _gradient magnitudes_ of convex functions [41]. In convex optimization, making the function value exactly optimal is equivalent to making the gradient exactly zero, but reducing the function-value suboptimality below a threshold is not equivalent to reducing the gradient magnitude below a threshold. This line of research showed that accelerated methods for reducing function values, such as Nesterov's FGM [37], the more modern OGM [26], and the accelerated composite optimization method FISTA [11] are not optimal for reducing gradient magnitude, and new optimal alternatives, such as OGM-G [29] and FISTA-G [31], were presented.

These new accelerated methods for reducing gradient magnitudes are understood far less than those for minimizing function values. However, an interesting observation of symmetry, described in Section 2, was made between these two types of methods, and it was conjectured that this symmetry might be a key to understanding the acceleration mechanism for efficiently reducing gradient magnitude.

Contribution.We present a surprising one-to-one correspondence between methods efficiently minimizing function values and methods efficiently minimizing gradient magnitude. We call this correspondence H-duality and formally establish a duality theory in both discrete- and continuous-time dynamics. Using H-duality, we obtain a clearer understanding of the symmetry between FGM/OGM and OGM-G, derive a new class of methods efficiently reducing gradient magnitudes, and find a new composite minimization method that is simpler and faster than FISTA-G, the prior state-of-the-art in efficiently reducing gradient magnitude in the composite minimization setup.

### Preliminaries and Notation

Given \(f\colon\mathbb{R}^{d}\to\mathbb{R}\), write \(f_{\star}=\inf_{x\in\mathbb{R}^{d}}f(x)\in(-\infty,\infty)\) for the minimum value and \(x_{\star}\in\operatorname*{argmin}_{x\in\mathbb{R}^{n}}f(x)\) for a minimizer, if one exists. Throughout this paper, we assume \(f_{\star}\neq-\infty\), but we do not always assume a minimizer \(x_{\star}\) exists. Given a differentiable \(f\colon\mathbb{R}^{d}\to\mathbb{R}\) and a pre-specified value of \(L>0\), we define the notation

\[[x,y] :=f(y)-f(x)+\langle\nabla f(y),x-y\rangle\] \[:=f(y)-f(x)+\langle\nabla f(y),x-y\rangle+\frac{1}{2L}\left\| \nabla f(x)-\nabla f(y)\right\|^{2}\] \[:=f_{\star}-f(x)+\frac{1}{2L}\left\|\nabla f(x)\right\|^{2}\]

for \(x,y\in\mathbb{R}^{d}\). A differentiable function \(f\colon\mathbb{R}^{d}\to\mathbb{R}\) is convex if the convexity inequality \([x,y]\leq 0\) holds for all \(x,y\in\mathbb{R}^{d}\). For \(L>0\), a function \(f\colon\mathbb{R}^{d}\to\mathbb{R}\) is \(L\)-smooth convex if it is differentiable and the cocercivity inequality \([x,y]\leq 0\) holds for all \(x,y\in\mathbb{R}^{d}\)[42]. If \(f\) has a minimizer \(x_{\star}\), then \(\llbracket x,\star\rrbracket=\llbracket x,x_{\star}\rrbracket\), but the notation \(\llbracket x,\star\rrbracket\) is well defined even when a minimizer \(x_{\star}\) does not exist. If \(f\) is \(L\)-smooth convex, then \(\llbracket x,\star\rrbracket\leq 0\) holds for all \(x\in\mathbb{R}^{d}\)[42].

Throughout this paper, we consider the duality between the following two problems.

* [label=(P0)]
* Efficiently reduce \(f(x_{N})-f_{\star}\) assuming \(x_{\star}\) exists and \(\left\|x_{0}-x_{\star}\right\|\leq R\).
* Efficiently reduce \(\frac{1}{2L}\|\nabla f(y_{N})\|^{2}\) assuming \(f_{\star}>-\infty\) and \(f(y_{0})-f_{\star}\leq R\).

Here, \(R\in(0,\infty)\) is a parameter, \(x_{0}\) and \(y_{0}\) denote initial points of methods for (P1) and (P2), and \(x_{N}\) and \(y_{N}\) denote outputs of methods for (P1) and (P2).

Finally, the standard gradient descent (GD) with stepsize \(h\) is

\[x_{i+1}=x_{i}-\frac{h}{L}\nabla f(x_{i}),\quad i=0,1,\ldots.\] (GD)

### Prior works

Classically, the goal of optimization methods is to reduce the function value efficiently. In the smooth convex setup, Nesterov's fast gradient method (FGM) [37] achieves an accelerated \(\mathcal{O}(1/N^{2})\)-rate, and the optimized gradient method (OGM) [26] improves this rate by a factor of \(2\), which is, in fact, exactly optimal [18].

On the other hand, Nesterov initiated the study of methods for reducing the gradient magnitude of convex functions [41] as such methods help us understand non-convex optimization better and design faster non-convex machine learning methods. For smooth convex functions, (GD) achieves a \(\mathcal{O}(\left(f(x_{0})-f_{\star}\right)/N)\)-rate on the squared gradient magnitude [34, Proposition 3.3.1], while (OGM-G) achieves an accelerated \(\mathcal{O}(\left(f(x_{0})-f_{\star}\right)/N^{2})\)-rate [29], which matches a lower bound and is therefore optimal [35, 36]. Interestingly, (OGM) and (OGM-G) exhibit an interesting hint of symmetry, as we detail in Section 2, and the goal of this work is to derive a more general duality principle from this observation.

In the composite optimization setup, iterative shrinkage-thresholding algorithm (ISTA) [13, 45, 16, 14] achieves a \(\mathcal{O}(\left\|x_{0}-x_{\star}\right\|^{2}/N)\)-rate on function-value suboptimality, while the fast iterative shrinkage-thresholding algorithm (FISTA) [11] achieves an accelerated \(\mathcal{O}(\left\|x_{0}-x_{\star}\right\|/N^{2})\)-rate. On the squared gradient mapping norm, FISTA-G achieves \(\mathcal{O}((F(x_{0})-f_{\star})/N^{2})\)-rate [31], which is optimal [35, 36]. Analysis of an accelerated method often uses the estimate sequence technique [38, 8, 39, 9, 40, 32] or a Lyapunov analysis [37, 11, 50, 10, 53, 1, 5, 6, 7, 43]. In this work, we focus on the Lyapunov analysis technique, as it is simpler and more amenable to a continuous-time view.

The notion of duality is fundamental in many branches of mathematics, including optimization. Lagrange duality [46, 47, 12], Wolfe duality [57, 15, 49, 33], and Fenchel-Rockacheller duality [20, 46] are related (arguably equivalent) notions that consider a pairing of primal and dual optimization problems. The recent gauge duality [21, 22, 3, 58] and radial duality [24, 23] are alternative notions of duality for optimization problems. Attouch-Thera duality [4, 48] generalizes Fenchel-Rockacheller to the setup of monotone inclusion problems. In this work, we present H-duality, which is a notion of duality for optimization _algorithms_, and it is, to the best of our knowledge, distinct from any previously known duality or symmetry relations.

H-duality

In this section, we will introduce H-duality, state the main H-duality theorem, and provide applications. Let \(N\geq 1\) be a pre-specified iteration count. Let \(\{h_{k,i}\}_{0\leq i<k\leq N}\) be an array of (scalar) stepsizes and identify it with a lower triangular matrix \(H\in\mathbb{R}^{N\times N}\) via \(H_{k+1,i+1}=h_{k+1,i}\) if \(0\leq i\leq k\leq N-1\) and \(H_{k,i}=0\) otherwise. An \(N\)-step Fixed Step First Order Method (FSFOM) with \(H\) is

\[x_{k+1}=x_{k}-\frac{1}{L}\sum\limits_{i=0}^{k}h_{k+1,i}\nabla f(x_{i}),\qquad \forall\,k=0,\ldots,N-1\] (1)

for any initial point \(x_{0}\in\mathbb{R}^{d}\) and differentiable \(f\). For \(H\in\mathbb{R}^{N\times N}\), define its _anti-transpose_\(H^{A}\in\mathbb{R}^{N\times N}\) with \(H^{A}_{i,j}=H_{N-j+1,N-i+1}\) for \(i,j=1,\ldots,N\). We call [FSFOM with \(H^{A}\)] the **H-dual** of [FSFOM with \(H\)].

### Symmetry between OGM and OGM-G

Let \(f\) be an \(L\)-smooth convex function. Define the notation \(z^{+}=z-\frac{1}{L}\nabla f(z)\) for \(z\in\mathbb{R}^{d}\). The accelerated methods OGM [19, 26] and OGM-G [29] are

\[x_{k+1} =x_{k}^{+}+\frac{\theta_{k}-1}{\theta_{k+1}}(x_{k}^{+}-x_{k-1}^{ +})+\frac{\theta_{k}}{\theta_{k+1}}(x_{k}^{+}-x_{k})\] (OGM) \[y_{k+1} =y_{k}^{+}+\frac{(\theta_{N-k}-1)(2\theta_{N-k-1}-1)}{\theta_{N- k}(2\theta_{N-k}-1)}(y_{k}^{+}-y_{k-1}^{+})+\frac{2\theta_{N-k-1}-1}{2\theta_{N-k}- 1}(y_{k}^{+}-y_{k})\] (OGM-G)

for \(k=0,\ldots,N-1\), where \(\{\theta_{i}\}_{i=0}^{N}\) are defined as \(\theta_{0}=1\), \(\theta_{i+1}^{2}-\theta_{i+1}=\theta_{i}^{2}\) for \(0\leq i\leq N-2\), and \(\theta_{N}^{2}-\theta_{N}=2\theta_{N-1}^{2}\).1 (OGM) and (OGM-G) are two representative accelerated methods for the setups (P1) and (P2), respectively. As a surface-level symmetry, the methods both access the \(\{\theta_{i}\}_{i=0}^{N}\) sequence, but (OGM-G) does so in a reversed ordering [29]. There turns out to be a deeper-level symmetry: (OGM) and (OGM-G) are H-duals of each other, i.e., \(H^{A}_{\text{OGM}}=H_{\text{OGM-G}}\). The proof structures of (OGM) and (OGM-G) also exhibit symmetry. We can analyze (OGM) with the Lyapunov function

Footnote 1: Throughout this paper, we use the convention of denoting iterates of a given “primal” FSFOM as \(x_{k}\) while denoting the iterates of the H-dual FSFOM as \(y_{k}\).

\[\mathcal{U}_{k}=\frac{L}{2}\|x_{0}-x_{\star}\|^{2}+\sum\limits_{i=0}^{k-1}u_{i }\llbracket x_{i},x_{i+1}\rrbracket+\sum\limits_{i=0}^{k}(u_{i}-u_{i-1}) \llbracket x_{\star},x_{i}\rrbracket\] (2)

for \(-1\leq k\leq N\) with \(\{u_{i}\}_{i=0}^{N}=(2\theta_{0}^{2},\ldots,2\theta_{N-1}^{2},\theta_{N}^{2})\) and \(u_{-1}=0\). Since \(\llbracket\cdot,\cdot\rrbracket\leq 0\) and \(\{u_{i}\}_{i=0}^{N}\) is a positive monotonically increasing sequence, \(\{\mathcal{U}_{k}\}_{k=-1}^{N}\) is dissipative, i.e., \(\mathcal{U}_{N}\leq\mathcal{U}_{N-1}\leq\cdots\leq\mathcal{U}_{0}\leq \mathcal{U}_{-1}\). So

\[\theta_{N}^{2}\left(f(x_{N})-f_{\star}\right)\leq\theta_{N}^{2}\left(f(x_{N})-f_ {\star}\right)+\frac{L}{2}\|x^{\star}-x_{0}+z\|^{2}\stackrel{{( \bullet)}}{{=}}\mathcal{U}_{N}\leq\mathcal{U}_{-1}=\frac{L\|x_{0}-x^{\star}\|^ {2}}{2},\]

where \(z=\sum\limits_{i=0}^{N}\frac{u_{i}-u_{i-1}}{L}\nabla f(x_{i})\). The justification of \((\bullet)\) is the main technical challenge of this analysis, and it is provided in Appendix B.2. Dividing both sides by \(\theta_{N}^{2}\), we conclude the rate

\[f(x_{N})-f_{\star}\leq\frac{1}{\theta_{N}^{2}}\frac{L}{2}\|x_{0}-x^{\star}\|^{2}.\]

Likewise, we can analyze (OGM-G) with the Lyapunov function

\[\mathcal{V}_{k}=v_{0}\left(f(y_{0})-f_{\star}+\llbracket y_{N},\star\rrbracket \right)+\sum\limits_{i=0}^{k-1}v_{i+1}\llbracket y_{i},y_{i+1}\rrbracket+\sum \limits_{i=0}^{k-1}(v_{i+1}-v_{i})\llbracket y_{N},y_{i}\rrbracket\] (3)

for \(0\leq k\leq N\) with \(\{v_{i}\}_{i=0}^{N}=\left(\frac{1}{\theta_{N}^{2}},\frac{1}{2\theta_{N-1}^{2}}, \ldots,\frac{1}{2\theta_{0}^{2}}\right)\). Similarly, \(\{\mathcal{V}_{k}\}_{k=0}^{N}\) is dissipative, so

\[\frac{1}{2L}\|\nabla f(y_{N})\|^{2}\stackrel{{(\circ)}}{{=}} \mathcal{V}_{N}\leq\mathcal{V}_{0}=\frac{1}{\theta_{N}^{2}}\left(f(y_{0})-f_{ \star}\right)+\frac{1}{\theta_{N}^{2}}\llbracket y_{N},\star\rrbracket\leq \frac{1}{\theta_{N}^{2}}\left(f(y_{0})-f_{\star}\right).\]Again, the justification of \((\circ)\) is the main technical challenge of this analysis, and it is provided in Appendix B.2. The crucial observations are **(i)**\(u_{i}=1/v_{N-i}\) for \(0\leq i\leq N\) and **(ii)** the convergence rates share the identical factor \(1/\theta_{N}^{2}=1/u_{N}=v_{0}\). Interestingly, a similar symmetry relation holds between method pairs \([(\text{OBL-F}_{\text{b}}),(\text{OBL-G}_{\text{b}})]\)[44] and \([(\text{GD}),(\text{GD})]\), which we discuss later in Section 2.4.

### H-duality theorem

The symmetry observed in Section 2.1 is, in fact, not a coincidence. Suppose we have \(N\)-step FSFOMs with \(H\) and \(H^{A}\). We denote their iterates as \(\{x_{i}\}_{i=0}^{N}\) and \(\{y_{i}\}_{i=0}^{N}\). For clarity, note that \(\{u_{i}\}_{i=0}^{N}\) are free variables and can be appropriately chosen for the convergence rate analysis. For the FSFOM with \(H\), define \(\{\mathcal{U}_{k}\}_{k=-1}^{N}\) with the general form (2) with \(u_{-1}=0\). If \(0=u_{-1}\leq u_{0}\leq u_{1}\leq\cdots\leq u_{N}\), then \(\{\mathcal{U}_{k}\}_{k=-1}^{N}\) is monotonically nonincreasing (dissipative). Assume we can show

\[u_{N}(f(x_{N})-f_{\star})\leq\mathcal{U}_{N}\quad(\forall\,x_{0},x_{\star}, \nabla f(x_{0}),\ldots,\nabla f(x_{N}){\in\mathbb{R}^{d}}).\] (C1)

To clarify, since \(\{x_{i}\}_{i=0}^{N}\) lies within \(\operatorname{span}\{x_{0},\nabla f(x_{0}),\ldots,\nabla f(x_{N})\}\), the \(\mathcal{U}_{N}\) depends on \(\big{(}x_{0},x_{\star},\{\nabla f(x_{i})\}_{i=0}^{N},\{u_{i}\}_{i=0}^{N},H\big{)}\). If (C1) holds, the FSFOM with \(H\) exhibits the convergence rate

\[u_{N}(f(x_{N})-f_{\star})\leq\mathcal{U}_{N}\leq\cdots\leq\mathcal{U}_{-1}= \frac{L}{2}\|x_{0}-x_{\star}\|^{2}.\] (4)

For the FSFOM with \(H^{A}\), define \(\{\mathcal{V}_{k}\}_{k=0}^{N}\) with the general form (3). Also, note that \(\{v_{i}\}_{i=0}^{N}\) are free variables and can be appropriately chosen for the convergence rate analysis. If \(0\leq v_{0}\leq v_{1}\leq\cdots\leq v_{N}\), then \(\{\mathcal{V}_{k}\}_{k=0}^{N}\) is monotonically nonincreasing (dissipative). Assume we can show

\[\frac{1}{2L}\|\nabla f(y_{N})\|^{2}\leq\mathcal{V}_{N}\quad(\forall\,y_{0}, \nabla f(y_{0}),\ldots,\nabla f(y_{N})\in\mathbb{R}^{d},\,f_{\star}\in\mathbb{ R}).\] (C2)

To clarify, since \(\{y_{i}\}_{i=0}^{N}\) lies within \(\operatorname{span}\{y_{0},\nabla f(y_{0}),\ldots,\nabla f(y_{N})\}\), the \(\mathcal{V}_{N}\) depends on \(\big{(}y_{0},\{\nabla f(y_{i})\}_{i=0}^{N},f_{\star},\{v_{i}\}_{i=0}^{N},H^{A} \big{)}\). If (C2) holds, the FSFOM with \(H^{A}\) exhibits the convergence rate

\[\frac{1}{2L}\|\nabla f(y_{N})\|^{2}\leq\mathcal{V}_{N}\leq\cdots\leq\mathcal{ V}_{0}=v_{0}\left(f(y_{0})-f_{\star}\right)+v_{0}\llbracket y_{N},\star\rrbracket\leq v_{0} \left(f(y_{0})-f_{\star}\right).\] (5)

We now state our main H-duality theorem, which establishes a correspondence between the two types of bounds for the FSFOMs induced by \(H\) and \(H^{A}\).

**Theorem 1**.: Consider sequences of positive real numbers \(\{u_{i}\}_{i=0}^{N}\) and \(\{v_{i}\}_{i=0}^{N}\) related through \(v_{i}=\frac{1}{u_{N-i}}\) for \(i=0,\ldots,N\). Let \(H\in\mathbb{R}^{N\times N}\) be lower triangular. Then,

\[\big{[}(\text{C1})\text{ is satisfied with }\{u_{i}\}_{i=0}^{N}\text{ and }H\big{]}\quad\Leftrightarrow\quad\big{[}(\text{C2})\text{ is satisfied with }\{v_{i}\}_{i=0}^{N}\text{ and }H^{A}\big{]}\,.\]

Theorem 1 provides a sufficient condition that ensures an FSFOM with \(H\) with a convergence guarantee on \((f(x_{N})-f_{\star})\) can be H-dualized to obtain an FSFOM with \(H^{A}\) with a convergence guarantee on \(\|\nabla f(y_{N})\|^{2}\). To the best of our knowledge, this is the first result establishing a symmetrical relationship between (P1) and (P2). Section 2.3 provides a proof outline of Theorem 1.

### Proof outline of Theorem 1

Define

\[\mathbf{U}\colon =\mathcal{U}_{N}-u_{N}(f(x_{N})-f_{\star})-\frac{L}{2}\Big{\|}x_{ \star}-x_{0}+\frac{1}{L}\sum_{i=0}^{N}\left(u_{i}-u_{i-1}\right)\nabla f(x_{i}) \Big{\|}^{2}\] \[\mathbf{V}\colon =\mathcal{V}_{N}-\frac{1}{2}\left\|\nabla f(y_{N})\right\|^{2}.\]

Expanding \(\mathbf{U}\) and \(\mathbf{V}\) reveals that all function value terms are eliminated and only quadratic terms of \(\{\nabla f(x_{i})\}_{i=0}^{N}\) and \(\{\nabla f(y_{i})\}_{i=0}^{N}\) remain. Now, (C1) and (C2) are equivalent to the conditions

\[\Big{[}\mathbf{U}\geq 0,\quad\forall\ \big{(}\nabla f(x_{0}),\ldots,\nabla f(x_{N}) \in\mathbb{R}^{d}\big{)}\,\Big{]},\qquad\Big{[}\mathbf{V}\geq 0\quad\forall\ \big{(}\nabla f(y_{0}),\ldots,\nabla f(y_{N})\in\mathbb{R}^{d}\big{)}\, \Big{]},\]respectively. Next, define \(g_{x}=\left[\nabla f(x_{0})|\nabla f(x_{1})|\ldots|\nabla f(x_{N})\right]\in \mathbb{R}^{d\times(N+1)}\) and \(g_{y}=\left[\nabla f(y_{0})|\nabla f(y_{1})|\ldots|\nabla f(y_{N})\right]\in \mathbb{R}^{d\times(N+1)}\). We show that there is \(\mathcal{S}(H,u)\) and \(\mathcal{T}(H^{A},v)\in\mathbb{S}^{N+1}\) such that

\[\mathbf{U}=\operatorname{Tr}\left(g_{x}\mathcal{S}(H,u)g_{x}^{\intercal}\right),\qquad\mathbf{V}=\operatorname{Tr}\left(g_{y}\mathcal{T}(H^{A},v)g_{y}^{ \intercal}\right).\]

Next, we find an explicit invertible matrix \(M(u)\in\mathbb{R}^{(N+1)\times(N+1)}\) such that \(\mathcal{S}(H,u)=\mathcal{M}(u)^{\intercal}\mathcal{T}(H^{A},v)\mathcal{M}(u)\). Therefore,

\[\operatorname{Tr}\left(g_{x}\mathcal{S}(H,u)g_{x}^{\intercal}\right)\,= \,\operatorname{Tr}\left(g_{y}\mathcal{T}(H^{A},v)g_{y}^{\intercal}\right)\]

with \(g_{y}=g_{x}\mathcal{M}(u)^{\intercal}\) and we conclude the proof. This technique of considering the quadratic forms of Lyapunov functions as a trace of matrices is inspired by the ideas from the Performance Estimation Problem (PEP) literature [19; 55]. The full proof is given in Appendix A.

### Verifying conditions for H-duality theorem

In this section, we illustrate how to verify conditions (C1) and (C2) through examples. Detailed calculations are deferred to Appendices B.1 and B.2.

Example 1.For (OGM) and (OGM-G), the choice

\[\{u_{i}\}_{i=0}^{N}=(2\theta_{0}^{2},\ldots,2\theta_{N-1}^{2},\theta_{N}^{2}),\qquad\{v_{i}\}_{i=0}^{N}=\left(\frac{1}{\theta_{N}^{2}},\frac{1}{2\theta_{N -1}^{2}},\ldots,\frac{1}{2\theta_{0}^{2}}\right)\]

leads to

\[\mathbf{U}=0,\qquad\mathbf{V}=0.\]

Therefore, (C1) and (C2) hold.

Example 2.Again, define \(z^{+}=z-\frac{1}{L}\nabla f(z)\) for \(z\in\mathbb{R}^{d}\). Consider the FSFSOMs [44]

\[\begin{split}& x_{k+1}=x_{k}^{+}+\frac{k}{k+3}\left(x_{k}^{+}-x_{k- 1}^{+}\right)+\frac{k}{k+3}\left(x_{k}^{+}-x_{k}\right)\quad k=0,\ldots,N-2\\ & x_{N}=x_{N-1}^{+}+\frac{N-1}{2(\gamma+1)}\left(x_{N-1}^{+}-x_{N -2}^{+}\right)+\frac{N-1}{2(\gamma+1)}\left(x_{N-1}^{+}-x_{N-1}\right)\end{split}\] (OBL-F \[{}_{\text{b}}\] )

and

\[\begin{split}& y_{1}=y_{0}^{+}+\frac{N-1}{2(\gamma+1)}\left(y_{0}^ {+}-y_{-1}^{+}\right)+\frac{N-1}{2(\gamma+1)}\left(y_{0}^{+}-y_{0}\right)\\ & y_{k+1}=y_{k}^{+}+\frac{N-k-1}{N-k+2}\left(y_{k}^{+}-y_{k-1}^{+ }\right)+\frac{N-k-1}{N-k+2}\left(y_{k}^{+}-y_{k}\right)\quad k=1,\ldots,N-1 \end{split}\] (OBL-G \[{}_{\text{b}}\] )

where \(y_{-1}^{+}=y_{0}\), \(x_{-1}^{+}=x_{0}\) and \(\gamma=\sqrt{N(N+1)/2}\). It turns out that (OBL-F\({}_{\text{b}}\)) and (OBL-G\({}_{\text{b}}\)) are H-duals of each other. The choice

\[\{u_{i}\}_{i=0}^{N}=\left(\tfrac{1\cdot 2}{2},\ldots,\tfrac{N(N+1)}{2},\gamma^{2 }+\gamma\right),\quad\{v_{i}\}_{i=0}^{N}=\left(\tfrac{1}{\gamma^{2}+\gamma}, \tfrac{2}{N(N+1)},\ldots,\tfrac{2}{1\cdot 2}\right)\]

leads to

\[\mathbf{U}=\sum_{i=0}^{N}\frac{u_{i}-u_{i-1}}{2L}\|\nabla f(x_{i})\|^{2}, \quad\mathbf{V}=\frac{v_{0}}{2L}\|\nabla f(y_{N})\|^{2}+\sum_{i=0}^{N-1}\frac{ v_{i+1}-v_{i}}{2L}\|\nabla f(y_{i})-\nabla f(y_{N})\|^{2}\]

where \(u_{-1}=0\). Since \(\mathbf{U}\) and \(\mathbf{V}\) are expressed as a sum of squares, (C1) and (C2) hold.

Example 3.Interestingly, (GD) is a self-dual FSFOM in the H-dual sense. For the case \(h=1\), the choice

\[\{u_{i}\}_{i=0}^{N}=\left(\ldots,\tfrac{(2N+1)(i+1)}{2N-i},\ldots,2N+1\right), \qquad\{v_{i}\}_{i=0}^{N}=\left(\tfrac{1}{2N+1},\ldots,\tfrac{N+i}{(2N+1)(N-i+ 1)},\ldots\right)\]

leads to

\[\mathbf{U}=\sum_{0\leq i,j\leq N}\frac{s_{ij}}{L}\left\langle\nabla f(x_{i}), \nabla f(x_{j})\right\rangle,\qquad\mathbf{V}=\sum_{0\leq i,j\leq N}\frac{t_{ij}} {L}\left\langle\nabla f(y_{i}),\nabla f(y_{j})\right\rangle\]

for some \(\{s_{ij}\}\) and \(\{t_{ij}\}\) stated precisely in Appendix B.2. \(\mathbf{V}\geq 0\) can be established by showing that the \(\{t_{ij}\}\) forms a diagonally dominant and hence positive semidefinite matrix [29]. \(\mathbf{U}\geq 0\) can be established with a more elaborate argument [19], but that is not necessary; \(\mathbf{V}\geq 0\) implies (C2), and, by Theorem 1, this implies (C1).

### Applications of the H-duality theorem

A family of gradient reduction methods.Parameterized families of accelerated FSFOMs for reducing function values have been presented throughout the extensive prior literature. Such families generalize Nesterov's method and elucidate the essential algorithmic component that enables acceleration. For reducing gradient magnitude, however, there are only four accelerated FSFOMs (OGM-G), (OBL-G\({}_{\flat}\)), and M-OGM-G [60], and [17, Lemma 2.6]. Here, we construct a simple parameterized family of accelerated FSFOMs for reducing gradient magnitude by H-dualizing an accelerated FSFOM family for reducing function values.

Let \(\{t_{i}\}_{i=0}^{N}\) and \(\{T_{i}\}_{i=0}^{N}\) be sequences positive real numbers satisfying \(t_{i}^{2}\leq 2T_{i}=2\sum_{j=0}^{i}t_{j}\) for \(0\leq i\leq N-1\) and \(t_{N}^{2}\leq T_{N}=\sum_{j=0}^{N}t_{j}\). Consider a family of FSFOMs

\[x_{k+1}=x_{k}^{+}+\frac{(T_{k}-t_{k})t_{k+1}}{t_{k}T_{k+1}}\left(x_{k}^{+}-x_{k -1}^{+}\right)+\frac{(t_{k}^{2}-T_{k})t_{k+1}}{t_{k}T_{k+1}}\left(x_{k}^{+}-x_ {k}\right)\] (6)

for \(k=0,1,\ldots,N-1\), where \(x_{-1}^{+}=x_{0}\). This family coincides with the GOGM of [28], and it exhibits the rate [28, Theorem 5]

\[f(x_{N})-f_{\star}\leq\frac{1}{T_{N}}\frac{L}{2}\|x_{0}-x_{\star}\|^{2},\]

which can be established from (2) with \(u_{i}=T_{i}\) for \(0\leq i\leq N\).

**Corollary 1**.: The H-dual of (6) is

\[y_{k+1}=y_{k}^{+}+\frac{T_{N-k-1}(t_{N-k-1}-1)}{T_{N-k}(t_{N-k}-1)}\left(y_{k} ^{+}-y_{k-1}^{+}\right)+\frac{(t_{N-k}^{2}-T_{N-k})(t_{N-k-1}-1)}{T_{N-k}(t_{ N-k}-1)}\left(y_{k}^{+}-y_{k}\right)\]

for \(k=0,\ldots,N-1\), where \(y_{-1}^{+}=y_{0}\), and it exhibits the rate

\[\frac{1}{2L}\left\|\nabla f(y_{N})\right\|^{2}\leq\frac{1}{T_{N}}\left(f(y_{0 })-f_{\star}\right).\]

Proof outline.: By Theorem 1, (C2) holds with \(v_{i}=1/T_{N-i}\) for \(0\leq i\leq N\). We then use (5). 

When \(T_{i}=t_{i}^{2}\) for \(0\leq i\leq N\), the FSFOM (6) reduces to Nestrov's FGM [37] and its H-dual is, to the best of our knowledge, a new method without a name. If \(t_{i}^{2}=2T_{i}\) for \(0\leq i\leq N-1\) and \(t_{N}^{2}=T_{N}\), (6) reduces to (OGM) and its H-dual is (OGM-G). If \(t_{i}=i+1\) for \(0\leq i\leq N-1\) and \(t_{N}=\sqrt{N(N+1)/2}\), (6) reduces to (OBL-F\({}_{\flat}\)) and its H-dual is (OBL-G\({}_{\flat}\)).

Gradient magnitude rate of (GD).For gradient descent (GD) with stepsize \(h\), the \(H\) matrix is the identity matrix scaled by \(h\), and the H-dual is (GD) itself, i.e., (GD) is self-dual. For \(0<h\leq 1\), the rate \(f(x_{N})-f_{\star}\leq\frac{1}{2Nh+1}\frac{L}{2}\|x_{0}-x_{\star}\|^{2}\), originally due to [19], can be established from (2) with \(\{u_{i}\}_{i=0}^{N}=\left(\ldots,\frac{(2Nh+1)(i+1)}{2N-i},\ldots,2Nh+1\right)\). Applying Theorem 1 leads to the following.

**Corollary 2**.: Consider (GD) with \(0<h\leq 1\) applied to an \(L\)-smooth convex \(f\). For \(N\geq 1\),

\[\frac{1}{2L}\left\|\nabla f(x_{N})\right\|^{2}\leq\min\left(\frac{f(x_{0})-f_ {\star}}{2Nh+1},\frac{L\left\|x_{0}-x_{\star}\right\|^{2}}{2(2\lfloor\frac{N} {2}\rfloor h+1)(2\lceil\frac{N}{2}\rceil h+1)}\right).\]

To the best of our knowledge, Corollary 2 is the tightest rate on gradient magnitude for (GD) for the general step size \(0<h<1\), and it matches [53, Theorem 3] for \(h=1\).

Resolving conjectures of \(\mathcal{A}^{\star}\)-optimality of (OGM-G) and (OBL-F\({}_{\flat}\)).The prior work of [44] defines the notion of \(\mathcal{A}^{\star}\)-optimality, a certain restricted sense of optimality of FSFOMs, and shows that (OGM) and (OBL-F\({}_{\flat}\)) are \(\mathcal{A}^{\star}\)-optimal under a certain set of relaxed inequalities. On the other hand, \(\mathcal{A}^{\star}\)-optimality of (OGM-G) and (OBL-G\({}_{\flat}\)) are presented as conjectures. Combining Theorem 1 and the \(\mathcal{A}^{\star}\)-optimality of (OGM) and (OBL-F\({}_{\flat}\)) resolves these conjectures; (OGM-G) and (OBL-G\({}_{\flat}\)) are \(\mathcal{A}^{\star}\)-optimal.

### Intuition behind energy functions: Lagrangian Formulation

One might ask where the energy functions (2) and (3) came from. In this section, we provide an intuitive explanation of these energy functions using the QCQP and its Lagrangian. Consider an FSFOM (1) given with a lower triangular matrix \(H\in\mathbb{R}^{N\times N}\) with function \(f\), resulting in the sequence \(\{x_{i}\}_{i=0}^{N}\). To analyze the convergence rate of the function value, we formulate the following optimization problem:

\[\sup_{f} f(x_{N})-f_{\star}\] subject to \[f\colon\mathbb{R}^{n}\to\mathbb{R}\text{ is $L$-smooth convex},\quad\|x_{0}-x_{\star}\|^{2}\leq R^{2}.\]

However, this optimization problem is not solvable, as \(f\) is a functional variable. To address this, [19, 55] demonstrated its equivalence to a QCQP:

\[\sup_{f} f(x_{N})-f_{\star}\] subject to \[[\![x_{i},x_{j}]\!]\leq 0\quad[i,j]\in[-1,\ldots,N]^{2},\quad\|x_{0 }-x_{\star}\|^{2}\leq R^{2}\]

where \(x_{-1}\colon=x_{\star}\). To clarify, the optimization variables are \(\{\nabla f(x_{i}),f(x_{i})\}_{i=0}^{N}\), \(f_{\star}\), \(x_{0}\), and \(x_{\star}\) since \(\{x_{i}\}_{i=0}^{N}\) lies within \(\operatorname{span}\{x_{0},\nabla f(x_{0}),\ldots,\nabla f(x_{N})\}\). We consider a relaxed optimization problem as [26]:

\[\sup_{f} f(x_{N})-f_{\star}\] subject to \[[\![x_{i},x_{i+1}]\!]\leq 0,\quad i=0,1,\ldots,N-1,\quad[\![x_{ \star},x_{i}]\!]\leq 0,\quad i=0,1,\ldots,N,\] \[\|x_{0}-x_{\star}\|^{2}\leq R^{2}\]

Now, consider the Lagrangian function and the convex dual.

\[\mathcal{L}_{1}(f,\{a_{i}\}_{i=0}^{N-1},\{b_{i}\}_{i=0}^{N},\alpha)=-f(x_{N}) +f_{\star}+\sum_{i=0}^{N-1}a_{i}[\![x_{i},x_{i+1}]\!]+\sum_{i=0}^{N}b_{i}[\![x_ {\star},x_{i}]\!]+\alpha\|x_{0}-x_{\star}\|^{2}-\alpha R^{2}\]

where \(\{a_{i}\}_{i=0}^{N-1}\), \(\{b_{i}\}_{i=0}^{N}\), and \(\alpha\) are dual variables which are nonnegative. Considering \(a_{-1}=0\) and \(a_{N}=1\), the infimum of \(\mathcal{L}_{1}\) equals \(-\infty\) unless \(b_{i}=a_{i}-a_{i-1}\) for \(0\leq i\leq N\). Therefore, by introducing \(u_{N}=\frac{L}{2\alpha}\) and \(u_{i}=a_{i}u_{N}\) for \(0\leq i\leq N\), the convex dual problem can be simplified as follows:

\[\inf_{\{u_{i}\}_{i=0}^{N}} -\frac{LR^{2}}{2u_{N}}\] s.t. \[\inf_{x_{0},x_{\star},\{\nabla f(x_{i})\}_{i=0}^{N}}-u_{N}(f(x_{N })-f_{\star})+\frac{L}{2}\|x_{0}-x_{\star}\|^{2}+\sum_{i=0}^{N-1}u_{i}[\![x_{ i},x_{i+1}]\!]+\sum_{i=0}^{N}(u_{i}-u_{i-1})[\![x_{\star},x_{i}]\!]\geq 0\] \[\{u_{i}\}_{i=0}^{N}\text{ are nonnegative and nondecreasing.}\]

If the above two constraints holds for \(\{u_{i}\}_{i=0}^{N}\), we have \(f(x_{N})-f_{\star}\leq\frac{L}{2u_{N}}R^{2}\). This understanding motivates the introduction of (2) and (C1). We can perform a similar analysis on the gradient norm minimization problem with the relaxed optimization problem as follows:

\[\sup_{f} \frac{1}{2L}\|\nabla f(y_{N})\|^{2}\] subject to \[[\![y_{i},y_{i+1}]\!]\leq 0,\quad i=0,1,\ldots,N-1,\quad[\![y_{N},y_{i}]\!] \leq 0,\quad i=0,1,\ldots,N-1,\] \[[\![y_{N},\star]\!]\leq 0,\quad f(y_{0})-f_{\star}\leq R.\]

Finally, we note that although (2) and (3) both originate from the relaxed optimization problems, they have been commonly employed to achieve the convergence analysis. The function value convergence rate of OGM[26], FGM[37], G-OGM[28], GD[19], OBL-F\({}_{9}\)[44] can be proved by using (2) with appropriate \(\{u_{i}\}_{i=0}^{N}\). The gradient norm convergence rate of OGM-G[29], OBL-G\({}_{9}\)[44], M-OGM-G[60], and [17, Lemma 2.6] can be proved by using (3) with appropriate \(\{v_{i}\}_{i=0}^{N}\). We also note that recent works [2, 25] do not employ (2) to achieve the convergence rate, particularly for gradient descent with varying step sizes.

H-duality in continuous time

We now establish a continuous-time analog of the H-duality theorem. As the continuous-time result and, especially, its proof is much simpler than its discrete-time counterpart, the results of this section serve as a vehicle to convey the key ideas more clearly. Let \(T>0\) be a pre-specified terminal time. Let \(H(t,s)\) be an appropriately integrable2 real-valued kernel with domain \(\{(t,s)\,|\,0<s<t<T\}\). We define a Continuous-time Fixed Step First Order Method (C-FSFOM) with \(H\) as

Footnote 2: In this paper, we avoid analytical and measure-theoretic details and focus on convergence (rather than existence) results of the continuous-time dynamics.

\[X(0)=x_{0},\quad\dot{X}(t)=-\int_{0}^{t}H(t,s)\nabla f(X(s))\;ds,\quad\forall\, t\in(0,T)\] (7)

for any initial point \(x_{0}\in\mathbb{R}^{d}\) and differentiable \(f\). Note, the Euler discretization of C-FSFOMs (7) corresponds to FSFOMs (1). The notion of C-FSFOMs has been considered previously in [30].

Given a kernel \(H(t,s)\), analogously define its _anti-transpose_ as \(H^{A}(t,s)=H(T-s,T-t)\). We call [C-FSFOM with \(H^{A}\)] the **H-dual** of [C-FSFOM with \(H\)]. In the special case \(H(t,s)=e^{\gamma(s)-\gamma(t)}\) for some function \(\gamma(\cdot)\), the C-FSFOMs with \(H\) and its H-dual have the form

\[\ddot{X}(t)+\dot{\gamma}(t)\dot{X}(t)+\nabla f(X(t))=0\] (C-FSFOM with \[H(t,s)=e^{\gamma(s)-\gamma(t)}\] ) \[\ddot{Y}(t)+\dot{\gamma}(T-t)\dot{Y}(t)+\nabla f(Y(t))=0\] (C-FSFOM with \[H^{A}(t,s)\] )

Interestingly, friction terms with \(\gamma^{\prime}\) have time-reversed dependence between the H-duals, and this is why we refer to this phenomenon as time-reversed dissipation.

### Continuous-time H-duality theorem

For the C-FSFOM with \(H\), define the energy function

\[\mathcal{U}(t)=\frac{1}{2}\|X(0)-x_{\star}\|^{2}+\int_{0}^{t}u^{\prime}(s)[x_ {\star},X(s)]ds\] (8)

for \(t\in[0,T]\) with differentiable \(u\colon(0,T)\to\mathbb{R}\). If \(u^{\prime}(\cdot)\geq 0\), then \(\{\mathcal{U}(t)\}_{t\in[0,T]}\) is dissipative. Assume we can show

\[u(T)\left(f(X(T))-f_{\star}\right)\leq\mathcal{U}(T)\quad(\forall\,X(0),x_{ \star},\{\nabla f(X(s))\}_{s\in[0,T]}\in\mathbb{R}^{d}).\] (C3)

Then, the C-FSFOM with \(H\) exhibits the convergence rate

\[u(T)\left(f(X(T))-f_{\star}\right)\leq\mathcal{U}(T)\leq\mathcal{U}(0)=\frac{ 1}{2}\|X(0)-x_{\star}\|^{2}.\]

For the C-FSFOM with \(H^{A}\), define the energy function

\[\mathcal{V}(t)=v(0)\big{(}f(Y(0))-f(Y(T))\big{)}+\int_{0}^{t}v^{\prime}(s)[Y(T ),Y(s)]ds\] (9)

for \(t\in[0,T]\) with differentiable \(v\colon(0,T)\to\mathbb{R}\). If \(v^{\prime}(\cdot)\geq 0\), then \(\{\mathcal{V}(t)\}_{t\in[0,T]}\) is dissipative. Assume we can show

\[\frac{1}{2}\|\nabla f(Y(T))\|^{2}\leq\mathcal{V}(T)\quad(\forall\,Y(0),\{ \nabla f(Y(s))\}_{s\in[0,T]}\in\mathbb{R}^{d}).\] (C4)

Then, the C-FSFOM with \(H^{A}\) exhibits the convergence rate

\[\frac{1}{2}\|\nabla f(Y(T))\|^{2}\leq\mathcal{V}(T)\leq\mathcal{V}(0)=v(0) \left(f(Y(0))-f(Y(T))\right)\leq v(0)\left(f(Y(0))-f_{\star}\right).\]

**Theorem 2** (informal).: Consider differentiable functions \(u,v\colon(0,T)\to\mathbb{R}\) related through \(v(t)=\frac{1}{u(T-t)}\) for \(t\in[0,T]\). Assume certain regularity conditions (specified in Appendix C.2). Then,

\[[\text{(C3) is satisfied with $u(\cdot)$ and $H$}]\quad\Leftrightarrow\quad\big{[} \text{(C4) is satisfied with $v(\cdot)$ and $H^{A}$}\big{]}\,.\]

The formal statement of 2 and its proof are given in Appendix C.2. Loosely speaking, we can consider Theorem 2 as the limit of Theorem 1 with \(N\to\infty\).

### Verifying conditions for H-duality theorem

As an illustrative example, consider the case \(H(t,s)=\frac{s^{r}}{t^{r}}\) for \(r\geq 3\) which corresponds to an ODE studied in the prior work [50; 51]. For the C-FSFOM with \(H\), the choice \(u(t)=\frac{t^{2}}{2(r-1)}\) for the dissipative energy function \(\{\mathcal{U}(t)\}_{t=0}^{T}\) of (8) leads to

\[\mathcal{U}(T)-u(T)\left(f(X(T))-f_{\star}\right)=\frac{\left\|T\dot{X}(T)+2(X (T)-x_{\star})\right\|^{2}+2(r-3)\left\|X(T)-x_{\star}\right\|^{2}}{4(r-1)}+ \int_{0}^{T}\frac{(r-3)s}{2(r-1)}\left\|\dot{X}(s)\right\|^{2}ds.\]

For the C-FSFOM with \(H^{A}\), the choice \(v(t)=\frac{1}{u(T-t)}=\frac{2(r-1)}{(T-t)^{2}}\) for the dissipative energy function \(\{\mathcal{V}(t)\}_{t=0}^{T}\) of (9) leads to

\[\mathcal{V}(T)-\frac{1}{2}\|\nabla f(Y(T))\|^{2}=\frac{2(r-1)(r-3)\|Y(0)-Y(T) \|^{2}}{T^{4}}+\int_{0}^{T}\frac{2(r-1)(r-3)\left\|(T-s)\dot{Y}(s)+2(Y(s)-Y(T) )\right\|^{2}}{(T-s)^{5}}ds.\]

Since the right-hand sides are expressed as sums/integrals of squares, they are nonnegative, so (C3) and (C4) hold. (By Theorem 2, verifying (C3) implies (C4) and vice versa.) The detailed calculations are provided in Appendix C.1.

### Applications of continuous-time H-duality theorem

The C-FSFOM (7) with \(H(t,s)=\frac{Cp^{2}s^{2p-1}}{t^{p+1}}\) recovers

\[\ddot{X}(t)+\frac{p+1}{t}\dot{X}(t)+Cp^{2}t^{p-2}\nabla f(X(t))=0,\]

an ODE considered in [56]. The rate \(f(X(T))-f_{\star}\leq\frac{1}{2CT^{p}}\|X(0)-x_{\star}\|^{2}\) can be established from (8) with \(u(t)=Ct^{p}\). The C-FSFOM with \(H^{A}\) can be expressed as the ODE

\[\ddot{Y}(t)+\frac{2p-1}{T-t}\dot{Y}(t)+Cp^{2}(T-t)^{p-2}\nabla f(Y(t))=0.\] (10)

By Theorem 2, using (9) with \(v(t)=\frac{1}{C(T-t)^{p}}\) leads to the rate

\[\frac{1}{2}\|\nabla f(Y(T))\|^{2}\leq\frac{1}{CT^{p}}\left(f(Y(0))-f_{\star} \right).\]

Note that the continuous-time models of (OGM) and (OGM-G), considered in [51], are special cases of this setup with \(p=2\) and \(C=1/2\). The detailed derivation and well-definedness of the ODE are presented in Appendix C.3.

## 4 New method efficiently reducing gradient mapping norm: (SFG)

In this section, we introduce a novel algorithm obtained using the insights of Theorem 1. Consider minimizing \(F(x):=f(x)+g(x)\), where \(f\colon\mathbb{R}^{d}\to\mathbb{R}\) is \(L\)-smooth convex with \(0<L<\infty\) and \(g\colon\mathbb{R}^{d}\to\mathbb{R}\cup\{\infty\}\) is a closed convex proper function. Write \(F_{\star}=\inf_{x\in\mathbb{R}^{n}}F(x)\) for the minimum value. For \(\alpha>0\), define the \(\alpha\)-_proximal gradient step_ as

\[y^{\oplus,\alpha}=\operatorname*{argmin}_{z\in\mathbb{R}^{n}}\left(f(y)+ \langle\nabla f(y),z-y\rangle+g(z)+\frac{\alpha L}{2}\left\|z-y\right\|^{2} \right)=\operatorname{Prox}_{\frac{g}{\alpha L}}\left(y-\frac{1}{\alpha L} \nabla f(y)\right).\]

Consider FSFOMs defined by a lower triangular matrix \(H=\{h_{k,i}\}_{0\leq i<k\leq N}\) as follows:

\[x_{k+1}=x_{k}-\sum_{i=0}^{k}\alpha h_{k+1,i}\left(x_{i}-x_{i}^{\oplus,\alpha} \right),\quad\forall\,k=0,\ldots,N-1.\]

When \(g=0\), this reduces to (1). FISTA [11], FISTA-G [31] and GFPGM [27] are instances of this FSFOM with \(\alpha=1\). In this section, we present a new method for efficiently reducing the gradient mapping norm. This method is faster than the prior state-of-the-art FISTA-G [31] by a constant factor of \(5.28\) while having substantially simpler coefficients.

**Theorem 3**.: Consider the method

\[y_{k+1} =y_{k}^{\oplus,4}+\frac{(N-k+1)(2N-2k-1)}{(N-k+3)(2N-2k+1)}\left(y_{k }^{\oplus,4}-y_{k-1}^{\oplus,4}\right)+\frac{(4N-4k-1)(2N-2k-1)}{6(N-k+3)(2N-2k+ 1)}\left(y_{k}^{\oplus,4}-y_{k}\right)\] \[y_{N} =y_{N-1}^{\oplus,4}+\frac{3}{10}\left(y_{N-1}^{\oplus,4}-y_{N-2}^ {\oplus,4}\right)+\frac{3}{40}\left(y_{N-1}^{\oplus,4}-y_{N-1}\right)\] (SFG)

for \(k=0,\ldots,N-2\), where \(y_{-1}^{\oplus,4}=y_{0}\). This method exhibits the rate

\[\min_{v\in\partial F(y_{N}^{\oplus,4})}\left\|v\right\|^{2}\leq 25L^{2}\left\|y_ {N}-y_{N}^{\oplus,4}\right\|^{2}\leq\frac{50L}{(N+2)(N+3)}\left(F(y_{0})-F_{ \star}\right).\]

We call this method _Super FISTA-G_ (SFG), and in Appendix D.3, we present a further general parameterized family (SFG-family). To derive (SFG-family), we start with the parameterized family GFPGM [27], which exhibits an accelerated rate on function values, and expresses it as FSFOMs with \(H\). We then obtain the FSFOMs with \(H^{A}+C\), where \(C\) is a lower triangular matrix satisfying certain constraints. We find that the appropriate H-dual for the composite setup is given by this \(H^{A}+C\), rather than \(H^{A}\). We provide the proof of Theorem 3 in Appendix D.2.

(SFG) is an instance of (SFG-family) with simple rational coefficients. Among the family, the optimal choice has complicated coefficients, but its rate has a leading coefficient of \(46\), which is slightly smaller than the \(50\) of (SFG). We provide the details Appendix D.4.

## 5 Conclusion

In this work, we defined the notion of H-duality and formally established that the H-dual of an optimization method designed to efficiently reduce function values is another method that efficiently reduces gradient magnitude. For optimization algorithms, the notion of equivalence, whether informal or formal [59], is intuitive and standard. For optimization problems, the notion of equivalence is also standard, but the beauty of convex optimization is arguably derived from the elegant duality of optimization problems. In fact, there are many notions of duality for spaces, problems, operators, functions, sets, etc. However, the notion of duality for algorithms is something we, the authors, are unfamiliar with within the context of optimization, applied mathematics, and computer science. In our view, the significance of this work is establishing the first instance of a duality of algorithms. The idea that an optimization algorithm is an abstract mathematical object that we can take the dual of opens the door to many interesting questions. In particular, exploring for what type of algorithms the H-dual or a similar notion of duality makes sense is an interesting direction for future work.

## Acknowledgments and Disclosure of Funding

We thank Soonwon Choi, G. Bruno De Luca, and Eva Silverstein for sharing their insights as physicists. We thank Jaewook J. Suh for reviewing the manuscript and providing valuable feedback. We are grateful to Jungbin Kim and Jeongwhan Lee for their valuable discussions at the outset of this work. J.K. and E.K.R. were supported by the National the Samsung Science and Technology Foundation (Project Number SSTF-BA2101-02). C.P. acknowledges support from the Xianhong Wu Fellowship, the Korea Foundation for Advanced Studies, and the Siebel Scholarship. A.O. acknowledges support from the MIT-IBM Watson AI Lab grant 027397-00196, Trustworthy AI grant 029436-00103, and the MIT-DSTA grant 031017-00016. Finally, we also thank anonymous reviewers for giving thoughtful comments.

## References

* Ahn and Sra [2020] K. Ahn and S. Sra. From Nesterov's estimate sequence to Riemannian acceleration. _COLT_, 2020.
* Altschuler and Parrilo [2023] J. M. Altschuler and P. A. Parrilo. Acceleration by stepsize hedging II: Silver stepsize schedule for smooth convex optimization. _arXiv:2309.16530_, 2023.
* Aravkin et al. [2018] A. Y. Aravkin, J. V. Burke, D. Drusvyatskiy, M. P. Friedlander, and K. J. MacPhee. Foundations of gauge and perspective duality. _SIAM Journal on Optimization_, 28(3):2406-2434, 2018.

* [4] H. Attouch and M. Thera. A general duality principle for the sum of two operators. _Journal of Convex Analysis_, 3:1-24, 01 1996.
* [5] J.-F. Aujol and C. Dossal. Optimal rate of convergence of an ODE associated to the fast gradient descent schemes for \(b>0\). _HAL Archives Ouvertes_, 2017.
* [6] J.-F. Aujol, C. Dossal, G. Fort, and E. Moulines. Rates of convergence of perturbed FISTA-based algorithms. _HAL Archives Ouvertes_, 2019.
* [7] J.-F. Aujol, C. Dossal, and A. Rondepierre. Optimal convergence rates for Nesterov acceleration. _SIAM Journal on Optimization_, 29(4):3131-3153, 2019.
* [8] A. Auslender and M. Teboulle. Interior gradient and proximal methods for convex and conic optimization. _SIAM Journal on Optimization_, 16(3):697-725, 2006.
* [9] M. Baes. Estimate sequence methods: extensions and approximations. 2009.
* [10] N. Bansal and A. Gupta. Potential-function proofs for gradient methods. _Theory of Computing_, 15(4):1-32, 2019.
* [11] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. _SIAM Journal on Imaging Sciences_, 2(1):183-202, 2009.
* [12] S. Boyd and L. Vandenberghe. _Convex optimization_. Cambridge university press, 2004.
* [13] R. E. Bruck. On the weak convergence of an ergodic iteration for the solution of variational inequalities for monotone operators in Hilbert space. _Journal of Mathematical Analysis and Applications_, 61(1):159-164, 1977.
* [14] P. L. Combettes and V. R. Wajs. Signal recovery by proximal forward-backward splitting. _Multiscale Modeling and Simulation_, 4(4):1168-1200, 2005.
* [15] G. B. Dantzig, E. Eisenberg, and R. W. Cottle. Symmetric dual nonlinear programs. _Pacific Journal of Mathematics_, 15(3):809-812, 1965.
* [16] I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems with a sparsity constraint. _Communications on Pure and Applied Mathematics_, 57(11):1413-1457, 2004.
* [17] J. Diakonikolas and P. Wang. Potential function-based framework for making the gradients small in convex and min-max optimization. _SIAM Journal on Optimization_, 32(3):1668-1697, 2021.
* [18] Y. Drori. The exact information-based complexity of smooth convex minimization. _Journal of Complexity_, 39:1-16, 2017.
* [19] Y. Drori and M. Teboulle. Performance of first-order methods for smooth convex minimization: a novel approach. _Mathematical Programming_, 145(1):451-482, 2014.
* [20] W. Fenchel. On conjugate convex functions. _Canadian Journal of Mathematics_, 1(1):73-77, 1949.
* [21] R. M. Freund. Dual gauge programs, with applications to quadratic programming and the minimum-norm problem. _Mathematical Programming_, 38(1):47-67, 1987.
* [22] M. P. Friedlander, I. Mace do, and T. K. Pong. Gauge optimization and duality. _SIAM Journal on Optimization_, 24(4):1999-2022, 2014.
* [23] B. Grimmer. Radial duality part II: Applications and algorithms. _arXiv:2104.11185_, 2021.
* [24] B. Grimmer. Radial duality part I: Foundations. _arXiv:2104.11179_, 2022.
* [25] B. Grimmer. Provably faster gradient descent via long steps. _arXiv:2307.06324_, 2023.
* [26] D. Kim and J. A. Fessler. Optimized first-order methods for smooth convex minimization. _Mathematical Programming_, 159(1):81-107, 2016.

* [27] D. Kim and J. A. Fessler. Another look at the fast iterative shrinkage/thresholding algorithm (FISTA). _SIAM Journal on Optimization_, 28(1):223-250, 2018.
* [28] D. Kim and J. A. Fessler. Generalizing the optimized gradient method for smooth convex minimization. _SIAM Journal on Optimization_, 28(2):1920-1950, 2018.
* [29] D. Kim and J. A. Fessler. Optimizing the efficiency of first-order methods for decreasing the gradient of smooth convex functions. _Journal of Optimization Theory and Applications_, 188(1):192-219, 2021.
* [30] J. Kim and I. Yang. Unifying nesterov's accelerated gradient methods for convex and strongly convex objective functions: From continuous-time dynamics to discrete-time algorithms. _ICML_, 2023.
* [31] J. Lee, C. Park, and E. K. Ryu. A geometric structure of acceleration and its role in making gradients small fast. _NeurIPS_, 2021.
* [32] B. Li, M. Coutino, and G. B. Giannakis. Revisit of estimate sequence for accelerated gradient methods. _ICASSP_, 2020.
* [33] O. L. Mangasarian and J. Ponstein. Minmax and duality in nonlinear programming. _Journal of Mathematical Analysis and Applications_, 11:504-518, 1965.
* [34] A. S. Nemirovski. Optimization II: Numerical methods for nonlinear continuous optimization., 1999.
* [35] A. S. Nemirovsky. On optimality of Krylov's information when solving linear operator equations. _Journal of Complexity_, 7(2):121-130, 1991.
* [36] A. S. Nemirovsky. Information-based complexity of linear operator equations. _Journal of Complexity_, 8(2):153-175, 1992.
* [37] Y. Nesterov. A method for unconstrained convex minimization problem with the rate of convergence \(\mathcal{O}(1/k^{2})\). _Proceedings of the USSR Academy of Sciences_, 269:543-547, 1983.
* [38] Y. Nesterov. Smooth minimization of non-smooth functions. _Mathematical Programming_, 103(1):127-152, 2005.
* [39] Y. Nesterov. Accelerating the cubic regularization of Newton's method on convex problems. _Mathematical Programming_, 112(1):159-181, 2008.
* [40] Y. Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems. _SIAM Journal on Optimization_, 22(2):341-362, 2012.
* [41] Y. Nesterov. How to make the gradients small. _Optima. Mathematical Optimization Society Newsletter_, (88):10-11, 2012.
* [42] Y. Nesterov. _Lectures on Convex Optimization_. Springer, 2nd edition, 2018.
* [43] C. Park, J. Park, and E. K. Ryu. Factor-\(\sqrt{2}\) acceleration of accelerated gradient methods. _Applied Mathematics and Optimization_, 2023.
* [44] C. Park and E. K. Ryu. Optimal first-order algorithms as a function of inequalities. _arXiv:2110.11035_, 2021.
* [45] G. B. Passty. Ergodic convergence to a zero of the sum of monotone operators in Hilbert space. _Journal of Mathematical Analysis and Applications_, 72(2):383-390, 1979.
* [46] R. T. Rockafellar. _Convex Analysis_. Princeton university press, 1970.
* [47] R. T. Rockafellar. _Conjugate Duality and Optimization_. CBMS-NSF Regional Conference Series in Applied Mathematics. Society for Industrial and Applied Mathematics, 1974.
* [48] E. K. Ryu and W. Yin. _Large-Scale Convex Optimization_. Cambrige University Press, 2022.

* [49] J. Stoer. Duality in nonlinear programming and the minimax theorem. _Numerische Mathematik_, 5(1):371-379, 1963.
* [50] W. Su, S. Boyd, and E. Candes. A differential equation for modeling Nesterov's accelerated gradient method: Theory and insights. _NeurIPS_, 2014.
* [51] J. J. Suh, G. Roh, and E. K. Ryu. Continuous-time analysis of accelerated gradient methods via conservation laws in dilated coordinate systems. _ICML_, 2022.
* [52] A. B. Taylor. Convex interpolation and performance estimation of first-order methods for convex optimization. 2017.
* [53] A. B. Taylor and F. Bach. Stochastic first-order methods: non-asymptotic and computer-aided analyses via potential functions. _COLT_, 2019.
* [54] A. B. Taylor, J. M. Hendrickx, and F. Glineur. Exact worst-case performance of first-order methods for composite convex optimization. _SIAM Journal on Optimization_, 27(3):1283-1313, 2017.
* [55] A. B. Taylor, J. M. Hendrickx, and F. Glineur. Smooth strongly convex interpolation and exact worst-case performance of first-order methods. _Mathematical Programming_, 161(1-2):307-345, 2017.
* [56] A. Wibisono, A. C. Wilson, and M. I. Jordan. A variational perspective on accelerated methods in optimization. _Proceedings of the National Academy of Sciences of the United States of America_, 113(47):E7351--E7358, 2016.
* [57] P. Wolfe. A duality theorem for non-linear programming. _Quarterly of applied mathematics_, 19(3):239-244, 1961.
* [58] S. Yamanaka and N. Yamashita. Duality of optimization problems with gauge functions. _Optimization_, pages 1-31, 2022.
* [59] S. Zhao, L. Lessard, and M. Udell. An automatic system to detect equivalence between iterative algorithms. _arXiv:2105.04684_, 2021.
* [60] K. Zhou, L. Tian, A. M.-C. So, and J. Cheng. Practical schemes for finding near-stationary points of convex finite-sums. _AISTATS_, 2022.

## Appendix A Proof of Theorem 1

Reformulate (C1) and (C2) into \(\mathbf{U}\) and \(\boldsymbol{V}\).In this paragraph, we will show

\[\text{(C1)}\quad\Leftrightarrow\quad\left[\mathbf{U}\geq 0,\quad\left( \forall\,\nabla f(x_{0}),\ldots,\nabla f(x_{N})\in\mathbb{R}^{d}\right)\,\right]\]

and

\[\text{(C2)}\quad\Leftrightarrow\quad\left[\mathbf{V}\geq 0,\quad\left( \forall\,\nabla f(y_{0}),\ldots,\nabla f(y_{N})\in\mathbb{R}^{d}\right)\,\right].\]

Recall the definition of \(\mathbf{U}\) and \(\mathbf{V}\).

\[\mathbf{U}\colon =\mathcal{U}_{N}-u_{N}(f(x_{N})-f_{\star})-\frac{L}{2}\left\|x_{ \star}-x_{0}+\frac{1}{L}\sum_{i=0}^{N}\left(u_{i}-u_{i-1}\right)\nabla f(x_{i} )\right\|^{2}\] (11) \[\mathbf{V}\colon =\mathcal{V}_{N}-\frac{1}{2}\left\|\nabla f(y_{N})\right\|^{2}.\] (12)

First we calculate \(\mathcal{U}_{N}-u_{N}\left(f(x_{N})-f_{\star}\right)\).

\[\mathcal{U}_{N}-u_{N}\left(f(x_{N})-f_{\star}\right)\] \[= \frac{L}{2}\left\|x_{0}-x_{\star}\right\|^{2}+\sum_{i=0}^{N}(u_{i }-u_{i-1})\left(f(x_{i})-f_{\star}+\langle\nabla f(x_{i}),x_{\star}-x_{i} \rangle+\frac{1}{2L}\|\nabla f(x_{i})\|^{2}\right)\] \[\qquad+\sum_{i=0}^{N-1}u_{i}\left(f(x_{i+1})-f(x_{i})+\langle \nabla f(x_{i+1}),x_{i}-x_{i+1}\rangle+\frac{1}{2L}\|\nabla f(x_{i})-\nabla f (x_{i+1})\|^{2}\right)-u_{N}\left(f(x_{N})-f_{\star}\right)\] \[\stackrel{{(\circ)}}{{=}} \frac{L}{2}\left\|x_{0}-x_{\star}\right\|^{2}+\sum_{i=0}^{N}(u_{i }-u_{i-1})\left(\langle\nabla f(x_{i}),x_{\star}-x_{i}\rangle+\frac{1}{2L}\| \nabla f(x_{i})\|^{2}\right)\] \[\qquad+\sum_{i=0}^{N-1}u_{i}\left(\langle\nabla f(x_{i+1}),x_{i} -x_{i+1}\rangle+\frac{1}{2L}\|\nabla f(x_{i})-\nabla f(x_{i+1})\|^{2}\right)\] \[= \frac{L}{2}\left\|x_{0}-x_{\star}+\sum_{i=0}^{N}\frac{u_{i}-u_{i -1}}{L}\nabla f(x_{i})\right\|^{2}-\frac{1}{2L}\left\|\sum_{i=0}^{N}(u_{i}-u_{ i-1})\nabla f(x_{i})\right\|^{2}\] \[\qquad+\sum_{i=0}^{N}(u_{i}-u_{i-1})\left(\langle\nabla f(x_{i} ),x_{0}-x_{i}\rangle+\frac{1}{2L}\|\nabla f(x_{i})\|^{2}\right)\] \[\qquad+\sum_{i=0}^{N-1}u_{i}\left(\langle\nabla f(x_{i+1}),x_{i} -x_{i+1}\rangle+\frac{1}{2L}\|\nabla f(x_{i})-\nabla f(x_{i+1})\|^{2}\right).\]

Note that all function value terms are deleted at \(\circ\). Therefore,

\[\mathbf{U}= -\frac{1}{2L}\left\|\sum_{i=0}^{N}(u_{i}-u_{i-1})\nabla f(x_{i} )\right\|^{2}+\sum_{i=0}^{N}(u_{i}-u_{i-1})\left(\langle\nabla f(x_{i}),x_{0}- x_{i}\rangle+\frac{1}{2L}\|\nabla f(x_{i})\|^{2}\right)\] \[\qquad+\sum_{i=0}^{N-1}u_{i}\left(\langle\nabla f(x_{i+1}),x_{i} -x_{i+1}\rangle+\frac{1}{2L}\|\nabla f(x_{i})-\nabla f(x_{i+1})\|^{2}\right)\] (13)

and

\[\mathcal{U}_{N}-u_{N}\left(f(x_{N})-f_{\star}\right)\] \[= \mathbf{U}+\frac{L}{2}\left\|x_{0}-x_{\star}+\sum_{i=0}^{N}\frac{u _{i}-u_{i-1}}{L}\nabla f(x_{i})\right\|^{2}.\]Since \((x_{0}-x_{i})\), \((x_{i}-x_{i+1})\in\text{span}\left\{\nabla f(x_{0}),\ldots,\nabla f(x_{N})\right\}\), the value of \(\mathbf{U}\) is independent with \(x_{0},x_{\star}\). Thus the only term that depends on \(x_{0}\) and \(x_{\star}\) is \(\frac{L}{2}\left\|x_{0}-x_{\star}+\sum_{i=0}^{N}\frac{u_{i}-u_{i-1}}{L}\nabla f (x_{i})\right\|^{2}\). Next, since \(x_{0},x_{\star}\) can have any value, we can take \(x_{0}-x_{\star}=\frac{u_{i}-u_{i-1}}{L}\nabla f(x_{i})\). Thus it gives the fact that (C1) is equivalent to \(\left[\mathbf{U}\geq 0,\quad\left(\forall\,\nabla f(x_{0}),\ldots,\nabla f(x_{N}) \in\mathbb{R}^{d}\right)\,\right]\).

Now we calculate \(\mathcal{V}_{N}-\frac{1}{2}\left\|\nabla f(y_{N})\right\|^{2}\).

\[\mathcal{V}_{N}-\frac{1}{2}\left\|\nabla f(y_{N})\right\|^{2}\] \[= v_{0}\left(f_{\star}-f(y_{N})+\frac{1}{2L}\|\nabla f(y_{N})\|^{ 2}\right)+v_{0}\left(f(y_{0})-f_{\star}\right)\] \[\quad+\sum_{i=0}^{N-1}v_{i+1}\left(f(y_{i+1})-f(y_{i})+\langle \nabla f(y_{i+1}),y_{i}-y_{i+1}\rangle+\frac{1}{2L}\|\nabla f(y_{i})-\nabla f (y_{i+1})\|^{2}\right)\] \[\quad+\sum_{i=0}^{N-1}(v_{i+1}-v_{i})\left(f(y_{i})-f(y_{N})+ \langle\nabla f(y_{i}),y_{N}-y_{i}\rangle+\frac{1}{2L}\|\nabla f(y_{i})-\nabla f (y_{N})\|^{2}\right)-\frac{1}{2L}\|\nabla f(y_{N})\|^{2}\] \[\stackrel{{(\circ)}}{{=}} \frac{v_{0}}{2L}\|\nabla f(y_{N})\|^{2}+\sum_{i=0}^{N-1}v_{i+1} \left(\langle\nabla f(y_{i+1}),y_{i}-y_{i+1}\rangle+\frac{1}{2L}\|\nabla f(y_{ i})-\nabla f(y_{i+1})\|^{2}\right)\] \[\quad+\sum_{i=0}^{N-1}(v_{i+1}-v_{i})\left(\langle\nabla f(y_{i} ),y_{N}-y_{i}\rangle+\frac{1}{2L}\|\nabla f(y_{i})-\nabla f(y_{N})\|^{2} \right)-\frac{1}{2L}\|\nabla f(y_{N})\|^{2}.\]

Note that all function values are deleted at \((\circ)\). By the calculation result,

\[\mathbf{V}= \frac{v_{0}}{2L}\|\nabla f(y_{N})\|^{2}+\sum_{i=0}^{N-1}v_{i+1} \left(\langle\nabla f(y_{i+1}),y_{i}-y_{i+1}\rangle+\frac{1}{2L}\|\nabla f(y_{ i})-\nabla f(y_{i+1})\|^{2}\right)\] \[\quad+\sum_{i=0}^{N-1}(v_{i+1}-v_{i})\left(\langle\nabla f(y_{i} ),y_{N}-y_{i}\rangle+\frac{1}{2L}\|\nabla f(y_{i})-\nabla f(y_{N})\|^{2} \right)-\frac{1}{2L}\|\nabla f(y_{N})\|^{2}.\] (14)

(C2) is equivalent to \(\left[\mathbf{V}\geq 0,\quad\left(\forall\,\nabla f(y_{0}),\ldots,\nabla f(y_{N}) \in\mathbb{R}^{d}\right)\,\right]\). To establish Theorem 1, demonstrating

\[\left[\mathbf{U}\geq 0,\quad\left(\forall\,\nabla f(x_{0}),\ldots,\nabla f (x_{N})\in\mathbb{R}^{d}\right)\,\right]\quad\Leftrightarrow\quad\left[\mathbf{V }\geq 0,\quad\left(\forall\,\nabla f(y_{0}),\ldots,\nabla f(y_{N})\in\mathbb{R }^{d}\right)\,\right]\] (15)

would suffice.

Transforming \(\mathbf{U}\) and \(\mathbf{V}\) into a trace.Define

\[g_{x}\colon =\left[\nabla f(x_{0})|\nabla f(x_{1})|\ldots|\nabla f(x_{N})\right] \in\mathbb{R}^{d\times(N+1)},\] \[g_{y}\colon =\left[\nabla f(y_{0})|\nabla f(y_{1})|\ldots|\nabla f(y_{N}) \right]\in\mathbb{R}^{d\times(N+1)}.\]

In this paragraph, we convert the \(\mathbf{U}\) of (11) and the \(\mathbf{V}\) of (12) into the trace of symmetric matrices. The key idea is: For each \(\langle a,b\rangle\) term where \(a,b\in\text{span}\{\nabla f(x_{0}),\ldots,\nabla f(x_{N})\}\), we can write \(a=g_{x}\mathbf{a},\,b=g_{x}\mathbf{b}\) for some \(\mathbf{a},\mathbf{b}\in\mathbb{R}^{(N+1)\times 1}\). Then

\[\langle a,b\rangle=\langle g_{x}\mathbf{a},g_{x}\mathbf{b}\rangle=\mathbf{b}^{ \intercal}g_{x}^{\intercal}g_{x}\mathbf{a}=\operatorname{Tr}\left(\mathbf{b}^{ \intercal}g_{x}^{\intercal}g_{x}\mathbf{a}\right)=\operatorname{Tr}\left( \mathbf{a}\mathbf{b}^{\intercal}g_{x}^{\intercal}g_{x}\right)=\operatorname{Tr} \left(g_{x}\mathbf{a}\mathbf{b}^{\intercal}g_{x}^{\intercal}\right).\] (16)

Also note that \((x_{0}-x_{i}),\,(x_{i}-x_{i+1})\in\text{span}\{\nabla f(x_{0}),\ldots,\nabla f(x_ {N})\}\) and \((y_{i}-y_{N}),\,(y_{i}-y_{i+1})\in\text{span}\{\nabla f(y_{0}),\ldots,\nabla f(y_ {N})\}\). By using this technique, we observe that there exists \(\mathcal{S}(H,u),\,\mathcal{T}(H^{A},v)\) that satisfy

\[\eqref{eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq: eq:eqFrom now, we specifically calculate \(\mathcal{S}(H,u)\) and \(\mathcal{T}(H^{A},v)\). Denote \(\{\mathbf{e}_{i}\}_{i=0}^{N}\in\mathbb{R}^{(N+1)\times 1}\) as a unit vector which \((i+1)\)-th component is \(1\), \(\mathbf{e}_{-1}=\mathbf{e}_{N+1}=\mathbf{0}\) and define \(\mathcal{H}\) as

\[\mathcal{H}=\begin{bmatrix}0&0\\ H&0\end{bmatrix}\in R^{(N+1)\times(N+1)}.\]

Then, by the definition of \(g_{x}\) and \(\mathcal{H}\), we have

\[g_{x}\mathbf{e}_{i}=\nabla f(x_{i})\quad 0\leq i\leq N,\qquad \frac{1}{L}g_{x}\mathcal{H}^{\mathsf{T}}\mathbf{e}_{0}=0,\] (17) \[\frac{1}{L}g_{x}\mathcal{H}^{\mathsf{T}}\mathbf{e}_{i+1}=\frac{1 }{L}\sum_{j=0}^{i}h_{i,j}g_{x}\mathbf{e}_{j}=\frac{1}{L}\sum_{j=0}^{i}h_{i,j} \nabla f(x_{j})=x_{i}-x_{i+1}\quad 0\leq i\leq N-1.\] (18)

Therefore, we can express (11) with \(\mathcal{H},\{u_{i}\}_{i=0}^{N},g_{x}\) and \(\{\mathbf{e}_{i}\}_{i=0}^{N}\) using (17) and (18) as

(11) \[=-\frac{1}{2L}\Bigg{\|}\sum_{i=0}^{N}(u_{i}-u_{i-1})\nabla f(x_{i} )\Bigg{\|}^{2}+\sum_{i=0}^{N}(u_{i}-u_{i-1})\left(\langle\nabla f(x_{i}),x_{0} -x_{i}\rangle+\frac{1}{2L}\|\nabla f(x_{i})\|^{2}\right)\] \[\quad+\sum_{i=0}^{N-1}u_{i}\left(\langle\nabla f(x_{i+1}),x_{i}-x _{i+1}\rangle+\frac{1}{2L}\|\nabla f(x_{i})-\nabla f(x_{i+1})\|^{2}\right)\] \[=-\frac{1}{2L}\Bigg{\|}\sum_{i=0}^{N}(u_{i}-u_{i-1})g_{x}\mathbf{ e}_{i}\Bigg{\|}^{2}+\sum_{i=0}^{N}(u_{i}-u_{i-1})\left(\bigg{\langle}g_{x}e_{i},\frac{1}{L}g_{x}\mathcal{H}^{\mathsf{T}}(\mathbf{e}_{0}+\cdots+\mathbf{e}_{i })\bigg{\rangle}+\frac{1}{2L}\|g_{x}\mathbf{e}_{i}\|^{2}\right)\] \[\quad+\sum_{i=0}^{N-1}u_{i}\left(\bigg{\langle}g_{x}\mathbf{e}_{i +1},\frac{1}{L}g_{x}\mathcal{H}^{\mathsf{T}}\mathbf{e}_{i+1}\bigg{\rangle}+ \frac{1}{2L}\|g_{x}(\mathbf{e}_{i}-\mathbf{e}_{i+1})\|^{2}\right).\]

Using (16) induces (11) \(=\operatorname{Tr}\left(g_{x}\mathcal{S}(H,u)g_{x}^{\mathsf{T}}\right)\) where

\[\mathcal{S}(H,u)\] \[=-\frac{1}{2L}\left(\sum_{i=0}^{N}(u_{i}-u_{i-1})\mathbf{e}_{i} \right)\left(\sum_{i=0}^{N}(u_{i}-u_{i-1})\mathbf{e}_{i}\right)^{\mathsf{T}}\] \[\quad+\frac{1}{2L}\mathcal{H}^{\mathsf{T}}\left[\sum_{i=0}^{N}u_{i }(\mathbf{e}_{0}+\cdots+\mathbf{e}_{i})(\mathbf{e}_{i}-\mathbf{e}_{i+1})^{ \mathsf{T}}\right]+\frac{1}{2L}\left[\sum_{i=0}^{N}u_{i}(\mathbf{e}_{i}- \mathbf{e}_{i+1})(\mathbf{e}_{0}+\cdots+\mathbf{e}_{i})^{\mathsf{T}}\right] \mathcal{H}\] \[\quad+\frac{1}{2L}\left[\sum_{i=0}^{N}u_{i}\left((\mathbf{e}_{i}- \mathbf{e}_{i+1})\mathbf{e}_{i}^{\mathsf{T}}+\mathbf{e}_{i}(\mathbf{e}_{i}- \mathbf{e}_{i+1})^{\mathsf{T}}\right)-u_{N}\mathbf{e}_{N}\mathbf{e}_{N}^{ \mathsf{T}}\right].\]

Similarly, we calculate (12). Define \(\mathcal{H}^{A}\) as a anti-transpose matrix of \(\mathcal{H}\):

\[\mathcal{H}^{A}=\begin{bmatrix}0&0\\ H^{A}&0\end{bmatrix}\in R^{(N+1)\times(N+1)}.\]

Then, by the definition of \(g_{y}\) and \(\mathcal{H}^{A}\), we have

\[g_{y}\mathbf{e}_{i}=\nabla f(y_{i})\quad 0\leq i\leq N,\qquad \frac{1}{L}g_{y}\left(\mathcal{H}^{A}\right)^{\mathsf{T}}\mathbf{e}_{0}=0,\] (20) \[\frac{1}{L}g_{y}\left(\mathcal{H}^{A}\right)^{\mathsf{T}}\mathbf{e }_{i+1}=\frac{1}{L}\sum_{j=0}^{i}h_{i,j}g_{y}\mathbf{e}_{j}=\frac{1}{L}\sum_{j=0 }^{i}h_{i,j}\nabla f(y_{j})=y_{i}-y_{i+1}\quad 0\leq i\leq N-1.\] (21)Therefore, we can express (12) with \(\mathcal{H}^{A},\{v_{i}\}_{i=0}^{N},g\) and \(\{\mathbf{e}_{i}\}_{i=0}^{N}\) using (20) and (21) as

(12) \[=\frac{v_{0}-1}{2L}\|\nabla f(y_{N})\|^{2}+\sum_{i=0}^{N-1}v_{i+1} \left(\left\langle\nabla f(y_{i+1}),y_{i}-y_{i+1}\right\rangle+\frac{1}{2L}\| \nabla f(y_{i})-\nabla f(y_{i+1})\|^{2}\right)\] \[\quad+\sum_{i=0}^{N-1}(v_{i+1}-v_{i})\left(\left\langle\nabla f(y _{i}),y_{N}-y_{i}\right\rangle+\frac{1}{2L}\|\nabla f(y_{i})-\nabla f(y_{N})\|^ {2}\right)\] \[=\frac{v_{0}-1}{2L}\|g_{y}\mathbf{e}_{N}\|^{2}+\sum_{i=0}^{N-1}v_ {i+1}\left(\left\langle g_{y}\mathbf{e}_{i+1},\frac{1}{L}g_{y}\left(\mathcal{H }^{A}\right)^{\intercal}\mathbf{e}_{i+1}\right\rangle+\frac{1}{2L}\|g_{y}( \mathbf{e}_{i}-\mathbf{e}_{i+1})\|^{2}\right)\] \[\quad+\sum_{i=0}^{N-1}(v_{i+1}-v_{i})\left(-\left\langle g_{y} \mathbf{e}_{i},\frac{1}{L}g_{y}\left(\mathcal{H}^{A}\right)^{\intercal}( \mathbf{e}_{i+1}+\cdots+\mathbf{e}_{N})\right\rangle+\frac{1}{2L}\|g_{y}( \mathbf{e}_{i}-\mathbf{e}_{N})\|^{2}\right).\]

We can write (12) \(=\operatorname{Tr}\left(g_{y}\mathcal{T}(H^{A},v)g_{y}^{\intercal}\right)\) where

\[\mathcal{T}(H^{A},v)\] \[=\frac{1}{2L}\left[\sum_{i=0}^{N}v_{i}\left((\mathbf{e}_{i-1}- \mathbf{e}_{i})(\mathbf{e}_{i-1}-\mathbf{e}_{N})^{\intercal}+(\mathbf{e}_{i-1 }-\mathbf{e}_{N})(\mathbf{e}_{i-1}-\mathbf{e}_{i})^{\intercal}\right)\right]- \frac{v_{0}}{2L}\mathbf{e}_{0}\mathbf{e}_{0}^{\intercal}-\frac{1}{2L}\mathbf{ e}_{N}\mathbf{e}_{N}^{\intercal}\] \[\quad+\frac{1}{2L}\left[\sum_{i=0}^{N}v_{i}\left(\left(\mathcal{H }^{A}\right)^{\intercal}(\mathbf{e}_{i}+\cdots+\mathbf{e}_{N})(\mathbf{e}_{i} -\mathbf{e}_{i-1})^{\intercal}+(\mathbf{e}_{i}-\mathbf{e}_{i-1})(\mathbf{e}_{ i}+\cdots+\mathbf{e}_{N})^{\intercal}\mathcal{H}^{A}\right)\right]\] \[=\frac{1}{2L}\left[\sum_{i=0}^{N}\frac{1}{u_{N-i}}\left((\mathbf{ e}_{i-1}-\mathbf{e}_{i})(\mathbf{e}_{i-1}-\mathbf{e}_{N})^{\intercal}+(\mathbf{e}_{i-1 }-\mathbf{e}_{N})(\mathbf{e}_{i-1}-\mathbf{e}_{i})^{\intercal}\right)\right]- \frac{1}{2u_{N}L}\mathbf{e}_{0}\mathbf{e}_{0}^{\intercal}-\frac{1}{2L}\mathbf{ e}_{N}\mathbf{e}_{N}^{\intercal}\] \[\quad+\frac{1}{2L}\left[\sum_{i=0}^{N}\frac{1}{u_{N-i}}\left( \left(\mathcal{H}^{A}\right)^{\intercal}(\mathbf{e}_{i}+\cdots+\mathbf{e}_{N}) (\mathbf{e}_{i}-\mathbf{e}_{i-1})^{\intercal}+(\mathbf{e}_{i}-\mathbf{e}_{i- 1})(\mathbf{e}_{i}+\cdots+\mathbf{e}_{N})^{\intercal}\mathcal{H}^{A}\right) \right].\] (22)

**Finding auxiliary matrix \(\mathcal{M}(u)\) that gives the relation between \(\mathcal{S}(H,u)\) and \(\mathcal{T}(H^{A},v)\).** We can show that there exists an invertible \(M(u)\in\mathbb{R}^{(N+1)\times(N+1)}\) such that

\[\mathcal{S}(H,u)=\mathcal{M}(u)^{\intercal}\mathcal{T}(H^{A},v)\mathcal{M}(u).\] (23)

If we assume the above equation,

\[\operatorname{Tr}\left(g_{x}\mathcal{S}(H,u)g_{x}^{\intercal}\right)\,= \,\operatorname{Tr}\left(g_{y}\mathcal{T}(H^{A},v)g_{y}^{\intercal}\right)\] (24)

with \(g_{y}=g_{x}\mathcal{M}(u)^{\intercal}\). Since \(\mathcal{M}(u)\) is invertible,

\[\{g|g\in\mathbb{R}^{d\times(N+1)}\}=\{g\mathcal{M}(u)^{\intercal}|g\in\mathbb{ R}^{d\times(N+1)}\}.\] (25)

Also, note that

\[\left[\mathbf{U}\geq 0\quad\forall\,\nabla f(x_{0}),\ldots,\nabla f(x_{N}) \right]\quad\Leftrightarrow\quad\Big{[}\operatorname{Tr}\left(g_{x}\mathcal{S} (H,u)g_{x}^{\intercal}\right)\geq 0\quad\forall g_{x}\in\mathbb{R}^{d\times(N+1)} \Big{]}\]

and

\[\left[\mathbf{V}\geq 0\quad\forall\,\nabla f(y_{0}),\ldots,\nabla f(y_{N}) \right]\quad\Leftrightarrow\quad\Big{[}\operatorname{Tr}\left(g_{y}\mathcal{T}(H ^{A},u)g_{y}^{\intercal}\right)\geq 0\quad\forall g_{y}\in\mathbb{R}^{d\times(N+1)} \Big{]}.\]

By combining (24) and (25), we obtain

\[\left[\,\operatorname{Tr}\left(g_{x}\mathcal{S}(H,u)g_{x}^{\intercal} \right)\geq 0\quad\forall g_{x}\in\mathbb{R}^{d\times(N+1)}\right]\] \[\Leftrightarrow \Big{[}\operatorname{Tr}\left(g_{y}\mathcal{S}(H,u)g_{y}^{\intercal }\right)\geq 0\quad\forall g_{x}\in\mathbb{R}^{d\times(N+1)},\,g_{y}=g_{x} \mathcal{M}(u)^{\intercal}\Big{]}\] \[\Leftrightarrow \Big{[}\operatorname{Tr}\left(g_{y}\mathcal{S}(H,u)g_{y}^{\intercal }\right)\geq 0\quad\forall g_{y}\in\mathbb{R}^{d\times(N+1)}\Big{]}.\]

To sum up, we obtain (15)

\[\Big{[}\mathbf{U}\geq 0\quad\forall\,\nabla f(x_{0}),\ldots,\nabla f(x_{N}) \Big{]}\quad\Leftrightarrow\quad\Big{[}\mathbf{V}\geq 0\quad\forall\,\nabla f(y_{0}), \ldots,\nabla f(y_{N})\Big{]},\]

which concludes the proof.

**Explicit form of \(\mathcal{M}(u)\) and justification of (23).** Explicit form of \(\mathcal{M}(u)\) is

\[\mathcal{M}=\begin{bmatrix}0&\cdots&0&0&u_{N}\\ 0&\cdots&0&u_{N-1}&u_{N}-u_{N-1}\\ 0&\cdots&u_{N-2}&u_{N-1}-u_{N-2}&u_{N}-u_{N-1}\\ \vdots&\vdots&\vdots&\vdots&\vdots\\ u_{0}&\cdots&u_{N-2}-u_{N-3}&u_{N-1}-u_{N-2}&u_{N}-u_{N-1}\end{bmatrix}\in \mathbb{R}^{(N+1)\times(N+1)}.\] (26)

Now, we express \(\mathcal{M}(u)=\sum\limits_{0\leq i,j\leq N}m_{ij}(u)\mathbf{e}_{i}\mathbf{e}_ {j}^{\intercal}\), \(\mathcal{S}(H,u)=\sum\limits_{0\leq i,j\leq N}s_{ij}\mathbf{e}_{i}\mathbf{e}_ {j}^{\intercal}\), and \(\mathcal{T}(H^{A},v)=\sum\limits_{0\leq i,j\leq N}t_{ij}\mathbf{e}_{i} \mathbf{e}_{j}^{\intercal}\). Calculating \(\mathcal{M}^{\intercal}(u)\mathcal{T}(H^{A},v)\mathcal{M}(u)\) gives

\[\mathcal{M}^{\intercal}(u)\mathcal{T}(H^{A},v)\mathcal{M}(u) =\left(\sum\limits_{i,j}m_{ij}(u)\mathbf{e}_{j}\mathbf{e}_{i}^{ \intercal}\right)\left(\sum\limits_{i,j}t_{ij}\mathbf{e}_{i}\mathbf{e}_{j}^{ \intercal}\right)\left(\sum\limits_{i,j}m_{ij}(u)\mathbf{e}_{i}\mathbf{e}_{j} ^{\intercal}\right)\] \[=\sum\limits_{i,j}t_{ij}\left(\sum\limits_{k}m_{ik}(u)\mathbf{e} _{k}\right)\left(\sum\limits_{l}m_{jl}(u)\mathbf{e}_{l}\right)^{\intercal}\] \[:=\sum\limits_{i,j}t_{ij}\mathbf{f}_{i}(u)\mathbf{f}_{j}(u)^{ \intercal}.\]

Thus it is enough to show that \(\sum\limits_{i,j}s_{ij}\mathbf{e}_{i}\mathbf{e}_{j}^{\intercal}\) and \(\sum\limits_{i,j}t_{ij}\mathbf{f}_{i}(u)\mathbf{f}_{j}(u)^{\intercal}\) are the same under the basis transformation \(\mathbf{f}_{i}(u)=\sum\limits_{k}m_{ik}(u)\mathbf{e}_{k}\). From here, we briefly write \(\mathbf{f}_{i}\) instead \(\mathbf{f}_{i}(u)\), and \(\mathbf{f}_{-1}=\mathbf{0}\). Note that \(u_{i}(\mathbf{e}_{i}-\mathbf{e}_{i+1})=(\mathbf{f}_{N-i}-\mathbf{f}_{N-i-1}), \,0\leq i\leq N\) by definition of \(\mathcal{M}(u)\). Therefore, we have

\[\frac{1}{L}\mathcal{H}^{\intercal}\sum\limits_{i=0}^{N}u_{i}( \mathbf{e}_{0}+\cdots+\mathbf{e}_{i})(\mathbf{e}_{i}-\mathbf{e}_{i+1})^{ \intercal} =\frac{1}{L}\mathcal{H}^{\intercal}\sum\limits_{i=0}^{N}(\mathbf{ e}_{0}+\cdots+\mathbf{e}_{i})(\mathbf{f}_{N-i}-\mathbf{f}_{N-i-1})^{\intercal}\] \[=\frac{1}{L}\mathcal{H}^{\intercal}\left[\sum\limits_{i=0}^{N}e_{ i}\mathbf{f}_{N-i}^{\intercal}\right].\]

Therefore, we can rewrite (19) as follows:

\[\mathcal{S}(H,u)= -\frac{1}{2L}\mathbf{f}_{N}\mathbf{f}_{N}^{\intercal}+\frac{1}{2 L}\mathcal{H}^{\intercal}\left[\sum\limits_{i=0}^{N}\mathbf{e}_{i}\mathbf{f}_{N-i}^{ \intercal}\right]+\frac{1}{2L}\sum\limits_{i=0}^{N}\mathbf{f}_{N-i}\mathbf{e}_ {i}^{\intercal}\mathcal{H}\] \[\qquad+\frac{1}{2L}\sum\limits_{i=0}^{N}\left[(\mathbf{f}_{N-i}- \mathbf{f}_{N-i-1})\mathbf{e}_{i}^{\intercal}+\mathbf{e}_{i}(\mathbf{f}_{N-i} \mathbf{f}_{N-i-1})^{\intercal}\right]-\frac{u_{N}}{2L}\mathbf{e}_{N}\mathbf{e} _{N}^{\intercal}\] \[= \underbrace{-\frac{1}{2L}\mathbf{f}_{N}\mathbf{f}_{N}^{\intercal} }_{\mathbf{A}_{1}}+\underbrace{\frac{1}{2L}\left[\sum\limits_{i,j}h_{i,j} \mathbf{e}_{j}\mathbf{f}_{N-i}^{\intercal}\right]+\frac{1}{2L}\left[\sum\limits _{i,j}h_{i,j}\mathbf{f}_{N-i}\mathbf{e}_{j}^{\intercal}\right]}_{\mathbf{B}_ {1}}\] (27) \[\qquad+\underbrace{\frac{1}{2L}\sum\limits_{i=0}^{N}\left[(\mathbf{ f}_{N-i}-\mathbf{f}_{N-i-1})\mathbf{e}_{i}^{\intercal}+\mathbf{e}_{i}(\mathbf{f}_{N-i}- \mathbf{f}_{N-i-1})^{\intercal}\right]}_{\mathbf{C}_{1}}-\underbrace{\frac{u_{N }}{2L}\mathbf{e}_{N}\mathbf{e}_{N}^{\intercal}}_{\mathbf{D}_{1}}.\]

Similarly, by using

\[\mathbf{e}_{N-i}-\mathbf{e}_{N-i+1}=\frac{1}{u_{N-i}}\left(\mathbf{f}_{i}- \mathbf{f}_{i-1}\right)=v_{i}\left(\mathbf{f}_{i}-\mathbf{f}_{i-1}\right),\]we can rewrite (22) as follows:

\[\mathcal{M}^{\intercal}(u)\mathcal{T}(H^{A},v)\mathcal{M}(u)\] (28) \[= \frac{1}{2L}\sum_{i=0}^{N}(\mathbf{e}_{N-i+1}-\mathbf{e}_{N-i})( \mathbf{f}_{i-1}-\mathbf{f}_{N})^{\intercal}+(\mathbf{f}_{i-1}-\mathbf{f}_{N} )(\mathbf{e}_{N-i+1}-\mathbf{e}_{N-i})^{\intercal}\] \[-\frac{1}{2u_{NL}}\mathbf{f}_{0}\mathbf{f}_{0}^{\intercal}- \frac{1}{2L}\mathbf{f}_{N}\mathbf{f}_{N}^{\intercal}+\frac{1}{2L}\left( \mathcal{H}^{A}\right)^{\intercal}\left[\sum_{i=0}^{N}\mathbf{f}_{i}\mathbf{e} _{N-i}^{\intercal}\right]+\frac{1}{2L}\left[\sum_{i=0}^{N}\mathbf{e}_{N-i} \mathbf{f}_{i}^{\intercal}\right]\mathcal{H}^{A}\] \[= -\underbrace{\frac{1}{2u_{NL}}\mathbf{f}_{0}\mathbf{f}_{0}^{ \intercal}}_{\mathbf{D}_{2}}+\underbrace{\frac{1}{2L}\left[\sum_{i,j}h_{N-i, N-j}\mathbf{f}_{j}\mathbf{e}_{N-i}^{\intercal}\right]+\frac{1}{2}\left[\sum_{i,j}h_{N-j, N-i}\mathbf{e}_{N-i}\mathbf{f}_{j}^{\intercal}\right]}_{\mathbf{B}_{2}}\] \[+\underbrace{\left[\frac{1}{2L}\sum_{i=0}^{N}(\mathbf{e}_{N-i+1}- \mathbf{e}_{N-i})\mathbf{f}_{i-1}^{\intercal}+\mathbf{f}_{i-1}(\mathbf{e}_{N -i+1}-\mathbf{e}_{N-i})^{\intercal}-\frac{1}{2L}\left(\mathbf{e}_{0}\mathbf{f }_{N}^{\intercal}+\mathbf{f}_{N}\mathbf{e}_{0}^{\intercal}\right)\right]}_{ \mathbf{C}_{2}}-\underbrace{\frac{1}{2L}\mathbf{f}_{N}\mathbf{f}_{N}^{ \intercal}}_{\mathbf{A}_{2}}.\]

For the final step, we compare (28) and (27) term-by-term, by showing \(\mathbf{X}_{1}=\mathbf{X}_{2}\) for \(\bm{X}=\bm{A},\bm{B},\bm{C},\bm{D}\).

* \(\bm{A}_{1}=\bm{A}_{2}\) comes directly.
* \(\bm{B}_{1}=\bm{B}_{2}\) comes from changing the summation index \(i\to N-i\) and \(j\to N-j\).
* \(\bm{C}_{1}=\bm{C}_{2}\) comes from the expansion of summation.
* \(\bm{D}_{1}=\bm{D}_{2}\) comes from \(\mathbf{f}_{0}=u_{N}\mathbf{e}_{N}\).

Therefore,

\[\mathcal{S}(H,u)=\mathcal{M}^{\intercal}(u)\mathcal{T}(H^{A},v)\mathcal{M}(u),\]

which concludes the proof.

**Remark 1**.: We can interpret \(\mathcal{M}\) as a basis transformation, where

\[u_{N}\mathbf{e}_{N}=\mathbf{f}_{0},\quad u_{i}(\mathbf{e}_{i}-\mathbf{e}_{i+1 })=\mathbf{f}_{N-i}-\mathbf{f}_{N-i-1}\quad i=0,1,\ldots,N-1.\] (29)

**Remark 2**.: To clarify, the quantifier \([\forall\,\nabla f(x_{0}),\ldots,\nabla f(x_{N})]\) in (C1) means \(\nabla f(x_{0}),\ldots,\nabla f(x_{N})\) can be any arbitrary vectors in \(\mathbb{R}^{d}\). This is different from \([\nabla f(x_{0}),\ldots,\nabla f(x_{N})\) be gradient of some \(f\colon\mathbb{R}^{d}\to\mathbb{R}]\). The same is true for (C2).

## Appendix B Omitted calculation of Section 2

### Calculation of \(H\) matrices

**OGM and OGM-G.** [26, Proposition 3] provides a recursive formula for (OGM) as follows:

\[h_{k+1,i}=\begin{cases}\frac{\theta_{k}-1}{\theta_{k+1}}h_{k,i}&i=0,\ldots,k-2 \\ \frac{\theta_{k}-1}{\theta_{k+1}}\left(h_{k,k-1}-1\right)&i=k-1\\ 1+\frac{2\theta_{k}-1}{\theta_{k+1}}&i=k.\end{cases}\] (30)

If \(k>i\),

\[h_{k+1,k-1}=\frac{\theta_{k}-1}{\theta_{k+1}}\frac{2\theta_{k-1} -1}{\theta_{k}},\] \[h_{k+1,i}=\frac{\theta_{k}-1}{\theta_{k+1}}h_{k,i}=\cdots=\left( \prod_{l=i+2}^{k}\frac{\theta_{l}-1}{\theta_{l+1}}\right)h_{i+1,i-1}=\left( \prod_{l=i+1}^{k}\frac{\theta_{l}-1}{\theta_{l+1}}\right)\frac{2\theta_{i}-1}{ \theta_{i+1}}.\]Thus \(H_{\text{OGM}}\) can be calculated as

\[H_{\text{OGM}}(k+1,i+1)=\begin{cases}0&i>k\\ 1+\frac{2\theta_{k}-1}{\theta_{k+1}}&i=k\\ \left(\prod\limits_{l=i+1}^{k}\frac{\theta_{l}-1}{\theta_{l+1}}\right)\frac{2 \theta_{l}-1}{2\theta_{i+1}}&i<k.\end{cases}\]

Recursive formula [29] of (OGM-G) is as following:

\[h_{k+1,i}=\begin{cases}\frac{\theta_{N-i-1}-1}{\theta_{N-i}}h_{k+1,i+1}&i=0, \ldots,k-2\\ \frac{\theta_{N-k}-1}{\theta_{N-k+1}}\left(h_{k+1,i}-1\right)&i=k-1\\ 1+\frac{2\theta_{N-k-1}-1}{\theta_{N-k}}&i=k.\end{cases}\] (31)

If \(k>i\),

\[h_{k+1,k-1}=\frac{\theta_{N-k}-1}{\theta_{N-k+1}}\frac{2\theta_{N-k-1}-1}{ \theta_{N-k}},\]

\[h_{k+1,i}=\frac{\theta_{N-i-1}-1}{\theta_{N-i}}h_{k+1,i+1}=\cdots=\left(\prod \limits_{l=N-k+1}^{N-i-1}\frac{\theta_{l}-1}{\theta_{l+1}}\right)h_{k+1,k-1}= \left(\prod\limits_{l=N-k}^{N-i-1}\frac{\theta_{l}-1}{\theta_{l+1}}\right) \frac{2\theta_{N-k-1}-1}{\theta_{N-k}}.\]

Thus \(H_{\text{OGM-G}}\) can be calculated as

\[H_{\text{OGM-G}}(k+1,i+1)=\begin{cases}0&i>k\\ 1+\frac{2\theta_{N-k-1}-1}{\theta_{N-k}}&i=k\\ \left(\prod\limits_{l=N-k}^{N-i-1}\frac{\theta_{l}-1}{\theta_{l+1}}\right) \frac{2\theta_{N-k-1}-1}{\theta_{N-k}}&i<k\end{cases},\]

which gives \(H_{\text{OGM-G}}=H_{\text{GGM}}^{A}\).

Gradient Descent.For (GD), \(H(i+1,k+1)=h_{i+1,k}=h\delta_{i+1,k+1}\), where \(\delta_{i,j}\) is the Kronecker delta. Therefore, \(H_{\text{GD}}=H_{\text{GD}}^{A}\).

Obl-F\({}_{\text{\tiny{$\circ$}}}\) and Obl-G\({}_{\text{\tiny{$\circ$}}}\).Recall \(\gamma=\sqrt{\frac{N(N+1)}{2}}\). We obtain the recursive formula of the \(H\) matrix of (OBL-F\({}_{\text{\tiny{$\circ$}}}\)).

\[h_{k+1,i}=\begin{cases}\frac{k}{k+3}h_{k,i}&k=0,\ldots,N-2,\,i=0,\ldots,k-2\\ 1+\frac{2k}{k+3}&k=0,\ldots,N-2,\,i=k\\ \frac{k}{k+3}\left(h_{k,k-1}-1\right)&k=1,\ldots,N-2,\,i=k-1\\ 1+\frac{N-1}{\gamma+1}&k=N-1,\,i=N-1\\ \frac{N-1}{2(\gamma+1)}\left(h_{N-1,N-2}-1\right)&k=N-1,\,i=N-2\\ \frac{N-1}{2(\gamma+1)}h_{N-1,i}&k=N-1,\,i=0,\ldots,N-3\end{cases}.\] (32)

By using the above formula, we obtain

\[H_{\text{OBL-F}_{\text{\tiny{$\circ$}}}}(k+1,i+1)=\begin{cases}1+\frac{2k}{k +3}&k=0,\ldots,N-2,\,i=k\\ \frac{2i(i+1)(i+2)}{(k+1)(k+2)(k+3)}&k=0,\ldots,N-2,\,i=0,\ldots,k-1\\ 1+\frac{N-1}{\gamma+1}&k=N-1,\,i=N-1\\ \frac{i(i+1)(i+2)}{(\gamma+1)N(N+1)}&k=N-1,\,i=0,\ldots,N-2\end{cases}.\]Similarly, we achieve the following recursive formula of the \(H\) matrix of (OBL-G\({}_{\text{b}}\)).

\[h_{k+1,i}=\begin{cases}\frac{N-k-1}{N-k+2}h_{k,i}&k=1,\ldots,N-1,\,i=0,\ldots,k-2 \\ 1+\frac{2(N-k-1)}{N-k+2}&k=1,\ldots,N-1,\,i=k\\ \frac{N-k-1}{N-k+2}\left(h_{k,k-1}-1\right)&k=1,\ldots,N-1,\,i=k-1\\ 1+\frac{N-1}{\gamma+1}&k=0,\,i=0\end{cases}.\] (33)

By using the above recursive formula, we obtain

\[H_{\text{OBL-G}_{\text{b}}}(k+1,i+1)=\begin{cases}1+\frac{N-1}{\gamma+1}&k=0, \,i=0\\ \frac{(N-k-1)(N-k)(N-k+1)}{(\gamma+1)N(N+1)}&k=1,\ldots,N-1,\,i=0\\ 1+\frac{2(N-k-1)}{N-k+2}&k=1,\ldots,N-1,\,i=k\\ \frac{2(N-k-1)(N-k+1)}{(N-i)(N-i+1)(N-i+2)}&k=1,\ldots,N-1,\,i=1,\ldots,k-1 \end{cases}.\]

Thus \(H_{\text{OBL-F}_{\text{b}}}=H^{A}_{\text{OBL-G}_{\text{b}}}\).

### Calculation of energy functions

Calculation of \(\mathbf{U}\) and \(\mathbf{V}\) with \(H\) matrixIn this paragraph, we calculate \(\mathbf{U}\) and \(\mathbf{V}\). Recall (13) and (14).

First, we put \(x_{k+1}-x_{k}=-\frac{1}{L}\sum\limits_{i=0}^{k}h_{k+1,i}\nabla f(x_{i})\) to (11). We have

\[\mathbf{U} =-\frac{1}{2L}\bigg{\|}\sum\limits_{i=0}^{N}(u_{i}-u_{i-1})\nabla f (x_{i})\bigg{\|}^{2}+\sum\limits_{i=0}^{N}(u_{i}-u_{i-1})\left(\langle\nabla f (x_{i}),x_{0}-x_{i}\rangle+\frac{1}{2L}\|\nabla f(x_{i})\|^{2}\right)\] \[\quad+\sum\limits_{i=0}^{N-1}u_{i}\left(\langle\nabla f(x_{i+1}), x_{i}-x_{i+1}\rangle+\frac{1}{2L}\|\nabla f(x_{i})-\nabla f(x_{i+1})\|^{2}\right)\] \[=-\frac{1}{2L}\bigg{\|}\sum\limits_{i=0}^{N}(u_{i}-u_{i-1})\nabla f (x_{i})\bigg{\|}^{2}+\sum\limits_{i=0}^{N}\frac{u_{i}-u_{i-1}}{L}\left(\left \langle\nabla f(x_{i}),\sum\limits_{l=0}^{i-1}\sum\limits_{j=0}^{l}h_{l+1,j} \nabla f(x_{j})\right\rangle+\frac{1}{2}\|\nabla f(x_{i})\|^{2}\right)\] \[\quad+\sum\limits_{i=0}^{N-1}\frac{u_{i}}{L}\left(\left\langle \nabla f(x_{i+1}),\sum\limits_{j=0}^{i}h_{i+1,j}\nabla f(x_{j})\right\rangle+ \frac{1}{2}\|\nabla f(x_{i})-\nabla f(x_{i+1})\|^{2}\right).\]

By arranging, we obtain

\[\mathbf{U}=\sum\limits_{0\leq j\leq i\leq N}\frac{s_{i,j}}{L}\left\langle \nabla f(x_{i}),\nabla f(x_{j})\right\rangle\]

where

\[s_{i,j}=\begin{cases}-\frac{1}{2}(u_{N}-u_{N-1})^{2}+\frac{1}{2}u_{N}&j=i,\,i= N\\ -\frac{1}{2}(u_{i}-u_{i-1})^{2}+u_{i}&j=i,\,i=0,\ldots,N-1\\ u_{i}h_{i,i-1}-u_{i-1}-(u_{i}-u_{i-1})(u_{i-1}-u_{i-2})&j=i-1\\ (u_{i}-u_{i-1})\sum\limits_{l=j}^{i-1}h_{l+1,j}+u_{i-1}h_{i,j}-(u_{i}-u_{i-1}) (u_{j}-u_{j-1})&j=0,\ldots,i-2.\end{cases}.\] (34)Recall that we defined \(u_{-1}=0\). Next, put \(y_{k+1}-y_{k}=-\frac{1}{L}\sum\limits_{i=0}^{k}h_{k+1,i}\nabla f(y_{i})\) to (12). We have

\[\mathbf{V}=\mathcal{V}_{N}-\frac{1}{2L}\|\nabla f(y_{N})\|^{2}\\ = \frac{v_{0}}{2L}\|\nabla f(y_{N})\|^{2}+\sum\limits_{i=0}^{N-1}v_ {i+1}\left(\langle\nabla f(y_{i+1}),y_{i}-y_{i+1}\rangle+\frac{1}{2L}\|\nabla f (y_{i})-\nabla f(y_{i+1})\|^{2}\right)\\ +\sum\limits_{i=0}^{N-1}(v_{i+1}-v_{i})\left(\langle\nabla f(y_{i }),y_{N}-y_{i}\rangle+\frac{1}{2L}\|\nabla f(y_{i})-\nabla f(y_{N})\|^{2} \right)-\frac{1}{2L}\|\nabla f(y_{N})\|^{2}\\ = \frac{v_{0}-1}{2L}\|\nabla f(y_{N})\|^{2}+\sum\limits_{i=0}^{N-1} v_{i+1}\left(\left\langle\nabla f(y_{i+1}),\frac{1}{L}\sum\limits_{j=0}^{i}h_{i+1,j} \nabla f(y_{j})\right\rangle+\frac{1}{2L}\|\nabla f(y_{i})-\nabla f(y_{i+1}) \|^{2}\right)\\ +\sum\limits_{i=0}^{N-1}(v_{i+1}-v_{i})\left(\left\langle\nabla f (y_{i}),-\frac{1}{L}\sum\limits_{l=i}^{N-1}\sum\limits_{j=0}^{l}h_{l+1,j} \nabla f(y_{j})\right\rangle+\frac{1}{2L}\|\nabla f(y_{i})-\nabla f(y_{N})\|^{ 2}\right).\]

By arranging, we obtain

\[\mathbf{V}=\sum\limits_{0\leq j\leq i\leq N}\frac{t_{i,j}}{L}\left\langle \nabla f(y_{i}),\nabla f(y_{j})\right\rangle\]

where

\[t_{i,j}=\begin{cases}\frac{v_{1}}{2}+\frac{v_{1}-v_{0}}{2}-(v_{1}-v_{0})\sum \limits_{l=0}^{N-1}h_{l+1,0}&i=0,j=i\\ \frac{v_{i+1}+v_{i}}{2}+\frac{v_{i+1}-v_{i}}{2}-(v_{i+1}-v_{i})\sum\limits_{l= i}^{N-1}h_{l+1,i}&i=1,\ldots,N-1,j=i\\ \frac{v_{0}-1}{2}+\frac{v_{N}}{2}+\sum\limits_{i=0}^{N-1}\frac{v_{i+1}-v_{i}} {2}&i=N,j=i\\ v_{i}h_{i,i-1}-v_{i}-(v_{i+1}-v_{i})\sum\limits_{l=i}^{N-1}h_{l+1,i-1}-(v_{i}-v _{i-1})\sum\limits_{l=i}^{N-1}h_{l+1,i}&i=1,\ldots,N-1,j=i-1\\ v_{N}h_{N,N-1}-v_{N}-(v_{N}-v_{N-1})&i=N,j=i-1\\ v_{i}h_{i,j}-(v_{i+1}-v_{i})\sum\limits_{l=i}^{N-1}h_{l+1,j}-(v_{j+1}-v_{j}) \sum\limits_{l=i}^{N-1}h_{l+1,i}&i=2,\ldots,N-1,j=0,\ldots,i-2\\ v_{N}h_{N,j}-(v_{j+1}-v_{j})&i=N,j=0,\ldots,N-2\end{cases}.\] (35)

Now we calculate \(\{s_{ij}\}\) and \(\{t_{ij}\}\) for \([\text{(OGM)},\text{(OGM-G)}]\)\([\text{(OBL-F}_{\text{v}}),\text{(OBL-G}_{\text{v}})]\) and \([\text{(GD)},\text{(GD)}]\).

#### b.2.1 Calculation of energy function of (OGM) and (OGM-G)

We will show \(s_{ij}=0\) and \(t_{ij}=0\) for all \(i,j\). By the definition of \(\{u_{i}\}_{i=-1}^{N}\) and \(\{\theta_{i}\}_{i=-1}^{N}\),

\[u_{i}-u_{i-1}=2\theta_{i}^{2}-2\theta_{i-1}^{2}=2\theta_{i}\qquad 0 \leq i\leq N-1,\] \[u_{N}-u_{N-1}=\theta_{N}^{2}-2\theta_{N-1}^{2}=\theta_{N}.\]

Therefore, we have

\[s_{i,i}=\begin{cases}-\frac{1}{2}(u_{N}-u_{N-1})^{2}+\frac{1}{2}u_{N}=-\theta_ {N}+\theta_{N}=0&j=i,\,i=N\\ -\frac{1}{2}(u_{i}-u_{i-1})^{2}+u_{i}=-\theta_{i}+\theta_{i}=0&j=i,\,i=0,\ldots,N-1.\end{cases}\]

[MISSING_PAGE_EMPTY:23]

To calculate \(\{t_{i,j}\}\), it is enough to deal with the sum \(\sum\limits_{l=i}^{N-1}h_{l+1,j}\), which can be expressed as

\[\sum\limits_{l=i}^{N-1}h_{l+1,j}=\begin{cases}\frac{\theta_{N}+1}{2}&i=0,\,j=i\\ \theta_{N-i}&i=1,\ldots,N-1,\,j=i\\ \frac{\theta_{N-i-1}^{4}}{\theta_{N-j}\theta_{N-j-1}^{2}}&i=1,\ldots,N-1,\,j= 0,\ldots,i-1\end{cases}.\] (37)

By inserting (37) in (35), \([t_{ij}=0,\forall i,j]\) is obtained, which implies \(\mathbf{V}=0\). (37) and (35) are also stated in [29, Lemma 6.1].

b.2.2 Calculation of energy function of (\(\mathrm{OBL-F_{\flat}}\)) and (\(\mathrm{OBL-G_{\flat}}\))

First we calculate \(\{s_{ij}\}\) for (\(\mathrm{OBL-F_{\flat}}\)). Recall \(u_{i}=\frac{(i+1)(i+2)}{2}\) for \(0\leq i\leq N-1\) and \(u_{N}=\gamma^{2}+\gamma\) where \(\gamma=\sqrt{N(N+1)/2}\). When \(j=i\),

\[s_{i,i} =\begin{cases}-\frac{1}{2}\left(u_{N}-u_{N-1}\right)^{2}+\frac{1 }{2}u_{N}\quad i=N\\ -\frac{1}{2}\left(u_{i}-u_{i-1}\right)^{2}+u_{i}\quad 0\leq i\leq N-1\end{cases}\] \[=\begin{cases}\frac{\gamma}{2}=\frac{u_{N}-u_{N-1}}{2}\quad i=N\\ -\frac{1}{2}\left(i+1\right)^{2}+\frac{(i+1)(i+2)}{2}=\frac{u_{i}-u_{i-1}}{2} \quad 0\leq i\leq N-1\end{cases}.\]

Now we claim that \(s_{ij}=0\) when \(j\neq i\). In the case \(j=i-1\), we have

\[s_{i,i-1} =u_{i}h_{i,i-1}-u_{i-1}-(u_{i}-u_{i-1})(u_{i-1}-u_{i-2})\] \[=\begin{cases}\frac{(i+1)(i+2)}{2}h_{i,i-1}-\frac{i(i+1)}{2}-(i+1 )i\quad 0\leq i\leq N-1\\ \left(\gamma^{2}+\gamma\right)h_{N,N-1}-\frac{N(N+1)}{2}-\gamma N\quad i=N \end{cases}\] \[=0.\]

We show \(s_{ij}=0\) for \(j\neq i\) with induction on \(i\), i.e., proving

\[\left[s_{i,j}=(u_{i}-u_{i-1})\sum\limits_{l=j}^{i-1}h_{l+1,j}+u_{i-1}h_{i,j}-( u_{i}-u_{i-1})(u_{j}-u_{j-1})=0\quad j=0,\ldots,i-2\right].\] (38)

(38) holds when \(i=j+2\) since

\[\begin{split}&\left(u_{j+2}-u_{j+1}\right)(h_{j+1,j}+h_{j+2,j}) +u_{j+1}h_{j+2,j}-(u_{j+2}-u_{j+1})(u_{j}-u_{j-1})\\ =&(u_{j+2}-u_{j+1})h_{j+1,j}+u_{j+2}h_{j+2,j}-(u_{j+2}-u_{j+1})(u_{j}- u_{j-1})\\ =&\begin{cases}(j+3)h_{j+1,j}+\frac{(j+3)(j+4)}{2}h_{j+2,j}-(j+3)(j+1) \quad 0\leq j\leq N-3\\ \gamma h_{N-1,N-2}+\left(\gamma^{2}+\gamma\right)h_{N,N-2}-\gamma\quad j=N-2 \end{cases}\\ =&0.\end{split}\]Assume (38) for \(i=i_{0}\). For \(i=i_{0}+1\),

\[(u_{i_{0}+1}-u_{i_{0}})\sum\limits_{l=j}^{i_{0}}h_{l+1,j}+u_{i_{0}}h _{i_{0}+1,j}-(u_{i_{0}+1}-u_{i_{0}})(u_{j}-u_{j-1})\] \[= (u_{i_{0}+1}-u_{i_{0}})\left(\sum\limits_{l=j}^{i_{0}-1}h_{l+1,j} +h_{i_{0}+1,j}\right)+u_{i_{0}}h_{i_{0}+1,j}-(u_{i_{0}+1}-u_{i_{0}})(u_{j}-u_{ j-1})\] \[= (u_{i_{0}+1}-u_{i_{0}})\left(\frac{(u_{i_{0}}-u_{i_{0}-1})(u_{j}- u_{j-1})-u_{i_{0}-1}h_{i_{0},j}}{u_{i_{0}}-u_{i_{0}-1}}+h_{i_{0}+1,j}\right)+u_{i_{0} }h_{i_{0}+1,j}\] \[-(u_{i_{0}+1}-u_{i_{0}})(u_{j}-u_{j-1})\] \[= u_{i_{0}+1}h_{i_{0}+1,j}-\frac{u_{i_{0}-1}(u_{i_{0}+1}-u_{i_{0}} )}{u_{i_{0}}-u_{i_{0}-1}}h_{i_{0},j}\] \[= \left\{\begin{array}{ll}(i_{0}+2)(i_{0}+3)\\ 2\end{array}\right.h_{i_{0}+1,j}-\frac{i_{0}(i_{0}+1)(i_{0}+2)}{2(i_{0}+1)}h_{ i_{0},j}&0\leq i_{0}\leq N-2\\ \left(\gamma^{2}+\gamma\right)h_{N,j}-\frac{(N-1)N\gamma}{2N}h_{N-1,j}&i_{0}=N-1 \end{array}\right.\] \[= 0.\]

Next, we calculate \(\{t_{ij}\}\) for (OBL-G\({}_{\flat}\)). We need to deal with the sum \(\sum\limits_{l=k}^{N-1}h_{l+1,i}\), which can be expressed as

\[\sum\limits_{l=i}^{N-1}h_{l+1,j}=\begin{cases}1+\frac{(N+2)(N-1)}{4(\gamma+1) }&i=0,\,j=0\\ \frac{(N-i+2)(N-i+1)(N-i)(N-i-1)}{4(\gamma+1)N(N+1)}&i=1,\ldots,N-1,\,j=0\\ \frac{(N-i+2)(N-i+1)(N-i)(N-i-1)}{2(N-j)(N-j+1)(N-j+2)}&i=j+1,\ldots,N-1,\,j=1,\ldots,N-1\\ 1+\frac{N-i-1}{2}&i=j,\,j=1,\ldots,N-1\end{cases}.\]

By combining \(v_{0}=\frac{1}{\gamma^{2}+\gamma}\), \(v_{i}=\frac{1}{(N-i+1)(N-i+2)}\) for \(1\leq i\leq N\) and (35), we obtain

\[t_{ij}=\begin{cases}1&i=N,\,j=N\\ \frac{1}{2N(N+1)}-\frac{v_{0}}{2}&i=0,\,j=1\\ v_{0}-\frac{1}{N(N+1)}&i=N,\,j=0\\ \frac{1}{(N-i)(N-i+1)(N-i+2)}&i=1,\ldots,N-1,\,j=i\\ -\frac{2}{(N-i)(N-i+1)(N-i+2)}&i=N,\,j=1,\ldots,N-1\\ 0&\text{otherwise}\end{cases}\] \[=\begin{cases}\frac{v_{N}}{2}&i=N,\,j=N\\ \frac{v_{i+1}-v_{i}}{2}&i=0,\ldots,N-1,\,j=i\\ -v_{i+1}+v_{i}&i=N,\,j=0,\ldots,N-1\\ 0&\text{otherwise}\end{cases}\]

Therefore,

\[\mathbf{V}=\sum\limits_{0\leq j\leq i\leq N}\frac{t_{ij}}{L}\left\langle\nabla f (y_{i}),\nabla f(y_{j})\right\rangle=\frac{v_{0}}{2L}\left\|\nabla f(y_{N}) \right\|^{2}+\sum\limits_{i=0}^{N-1}\frac{v_{i+1}-v_{i}}{2L}\left\|\nabla f(y_{ i})-\nabla f(y_{N})\right\|^{2}.\]

#### b.2.3 Calculation of energy function of GD

We calculate \(\{t_{ij}\}\) first. Recall that \(\{v_{i}\}_{i=0}^{N}=\left(\frac{1}{2N+1},\ldots,\frac{N+i}{(2N+1)(N-i+1)},\ldots\right)\) and \(h_{i+1,k}=\delta_{i,k}\) to (35), and making \(\{t_{ij}\}\) symmetric gives us

\[t_{ij}=\begin{cases}\frac{1}{2}v_{0}&i=j,\,i=0\\ v_{i}&i=j,\,1\leq i\leq N-1\\ v_{N}-\frac{1}{2}&i=j,\,i=N\\ \frac{1}{2}\left(v_{\min(i,j)}-v_{\min(i,j)+1}\right)&i\neq j.\end{cases}\]

We can verify that the matrix \(\{t_{ij}\}_{0\leq i,j\leq N}\) is diagonally dominant: \(t_{ii}=|\sum_{j\neq i}t_{ij}|\). Therefore, \(\sum\limits_{0\leq i,j\leq N-1}\frac{t_{ij}}{L}\left\langle\nabla f(y_{i}), \nabla f(y_{j})\right\rangle\geq 0\) for any \(\{\nabla f(y_{i})\}_{i=0}^{N}\). This proof is essentially the same as the proof in [29], but we repeat it here with our notation for the sake of completeness.

Next, we prove that (GD) with \(h=1\) and \(\{u_{i}\}_{i=0}^{N}=\left(\ldots,\frac{(2N+1)(i+1)}{2N-i},\ldots,2N+1\right)\) satisfies (C1), by showing more general statement:

\[\left[\text{(GD) and }\{u_{i}\}_{i=0}^{N}=\left(\ldots,\frac{(2Nh+1)(i+1)}{2N-i },\ldots,2Nh+1\right)\text{ satisfies (C1)}\right]\] (39)

Note that (39) gives

\[(2Nh+1)(f(x_{N})-f^{\star})\leq\mathcal{U}_{N}\leq\mathcal{U}_{-1}=\frac{L}{2 }\|x_{0}-x^{\star}\|^{2}.\] (40)

Later in the proof of Corollary 2, we will utilize the equation (39). The result (39) is proved in [19, Theorem 3.1], and we give the proof outline here.

In order to demonstrate (39), we will directly expand the expression \(\mathcal{U}_{N}-u_{N}\left(f(x_{N})-f^{\star}\right)\), instead of employing \(\mathbf{U}\) as a intermediary step. Define \(\{u_{i}^{\prime}\}_{i=0}^{N}\colon=\left(\ldots,\frac{(2N+1)(i+1)}{2N-i}, \ldots,2N+1\right)\). Then

\[\mathcal{U}_{N}-u_{N}\left(f(x_{N})-f^{\star}\right)=\frac{1}{L}\operatorname{ Tr}\left(g^{\intercal}gS\right)\]

where \(g=[\nabla f(x_{0})|\ldots|\nabla f(x_{N})|L(x_{0}-x^{\star})]\in\mathbb{R}^{d \times(N+2)}\) and \(S\in\mathbb{S}^{N+2}\) is given by

\[S=\begin{bmatrix}S^{\prime}&\lambda\\ \lambda&\frac{1}{2}\end{bmatrix},\]

\[\lambda=[u_{0}|u_{1}-u_{0}|\ldots|u_{N}-u_{N-1}]^{\intercal},S^{\prime}=\frac{ 2Nh+1}{2N+1}\left(hS_{0}+(1-h)S_{1}\right)\]

\[S_{0} =\sum_{i=0}^{N-1}\frac{u_{i}^{\prime}}{2}\left(e_{i+1}e_{i}^{ \intercal}+e_{i}e_{i+1}^{\intercal}+(e_{i}-e_{i+1})(e_{i}-e_{i+1})^{\intercal}\right)\] \[+\sum_{i=0}^{N}\frac{u_{i}^{\prime}-u_{i-1}^{\prime}}{2}\left(e_{ i}(e_{0}+\cdots+e_{i-1})^{\intercal}+(e_{0}+\cdots+e_{i-1})e_{i}^{ \intercal}+e_{i}e_{i}^{\intercal}\right)\] \[S_{1} =\sum_{i=0}^{N-1}\frac{u_{i}^{\prime}}{2}(e_{i}-e_{i+1})(e_{i}-e _{i+1})^{\intercal}+\sum_{i=0}^{N}\frac{u_{i}^{\prime}-u_{i-1}^{\prime}}{2}e_{ i}e_{i}^{\intercal}.\]

Now we will show \(S\succeq 0\) to obtain \(\left[\mathcal{U}_{N}-u_{N}\left(f(x_{N})-f^{\star}\right)\geq 0,\,\forall\,g\right]\), which is (C1). By using Sylvester's Criterion, \(S_{0}\succ 0\) follows. \(S_{1}\succ 0\) follows from the fact that \(S_{1}\) expressed by the sum of positive semi-definite matrices \(zz^{\intercal}\). Since the convex sum of two positive semi-definite matrices is also positive semi-definite, \(S^{\prime}=hS_{0}+(1-h)S_{1}\succ 0\).

Next, we argue that \(\det S=0\). Indeed, take \(\tau=(1,\ldots,-(2Nh+1))^{\intercal}\) to show \(S\tau=0\), which gives \(\det S=0\). Note that the determinant of \(S\) can also be expressed by

\[\det(S)=\left(\frac{1}{2}-\lambda^{\intercal}\left(S^{\prime}\right)^{-1} \lambda\right)\det(S^{\prime}).\] (41)

We have shown that \(S^{\prime}\succ 0\), (41) implies \(\frac{1}{2}-\lambda^{\intercal}\left(S^{\prime}\right)^{-1}\lambda=0\), which is the Schur complement of the matrix \(S\). By a well-known lemma on the Schur complement, we conclude \(S\succeq 0\).

### Omitted proof in Section 2.5

#### b.3.1 Omitted calculation of Corollary 1

Here, we will give the general formulation of H-dual FSFOM of

\[x_{k+1}=x_{k}+\beta_{k}\left(x_{k}^{+}-x_{k-1}^{+}\right)+\gamma_{k}\left(x_{k}^ {+}-x_{k}\right),\quad k=0,\ldots,N-1.\] (42)

**Proposition 1**.: The H-dual of (42) is

\[y_{k+1}=y_{k}+\beta_{k}^{\prime}\left(y_{k}^{+}-y_{k-1}^{+}\right)+\gamma_{k} ^{\prime}\left(y_{k}^{+}-y_{k}\right),\quad k=0,\ldots,N-1\] (43)

where

\[\beta_{k}^{\prime}=\frac{\beta_{N-k}(\beta_{N-1-k}+\gamma_{N-1-k})}{\beta_{N- k}+\gamma_{N-k}}\]

\[\gamma_{k}^{\prime}=\frac{\gamma_{N-k}(\beta_{N-1-k}+\gamma_{N-1-k})}{\beta_{ N-k}+\gamma_{N-k}}\]

for \(k=0,\ldots,N-1\) and \((\beta_{N},\gamma_{N})\) is any value that \(\beta_{N}+\gamma_{N}\neq 0\). 3

Footnote 3: Here, note that FSFOM (43) is independent with the any choice of \(\beta_{N},\gamma_{N}\) since \(y_{1}=y_{0}+(\beta_{0}^{\prime}+\gamma_{0}^{\prime})\left(y_{0}^{+}-y_{0} \right)=y_{0}+(\beta_{N-1}+\gamma_{N-1})\left(y_{0}^{+}-y_{0}\right)\).

Proof.: The \(H\) matrix \(\{h_{k,i}\}_{0\leq i<k\leq N}\) satisfies

\[h_{k+1,i}=\begin{cases}1+\beta_{k}+\gamma_{k}&i=k,\,k=0,\ldots,N-1\\ \beta_{k}\left(h_{k,i}-1\right)&i=k-1,\,k=1,\ldots,N-1\\ \beta_{k}h_{k,i}&i=k-2,\,k=2,\ldots,N-1\end{cases}.\]

Therefore,

\[h_{k+1,i}=\left(\prod_{j=i+1}^{k}\beta_{j}\right)(\beta_{i}+\gamma_{i}+\delta _{k,i})\]

where \(\delta_{k,i}\) is a Kronecker Delta function. Similarly, \(H\) matrix of (43) \(\{g_{k,i}\}_{0\leq i<k\leq N}\) satisfies

\[g_{k+1,i} =\left(\prod_{j=i+1}^{k}\beta_{j}^{\prime}\right)(\beta_{i}^{ \prime}+\gamma_{i}^{\prime}+\delta_{k,i})\] \[=\left(\prod_{j=i+1}^{k}\frac{\beta_{N-j}(\beta_{N-1-j}+\gamma_{N -1-j})}{\beta_{N-j}+\gamma_{N-j}}\right)(\beta_{N-1-i}+\gamma_{N-1-i}+\delta_{ k,i})\] \[=\left(\prod_{j=i+1}^{k}\beta_{N-j}\right)(\beta_{N-k-1}+\gamma_ {N-i-1}+\delta_{N-k-1,N-i-1})\,.\]

Thus \(g_{k+1,i}=h_{N-i,N-1-k}\). 

Now we derive the H-dual of (6) by applying Proposition 1. Note that

\[\beta_{k}=\frac{(T_{k}-t_{k})t_{k+1}}{t_{k}T_{k+1}},\qquad\gamma_{k}=\frac{(t _{k}^{2}-T_{k})t_{k+1}}{t_{k}T_{k+1}},\quad k=0,\ldots,N-1.\]

Next, define \(\beta_{N}\) and \(\gamma_{N}\) as a same form of \(\{\beta_{i},\gamma_{i}\}_{i=0}^{N-1}\) with any \(t_{N+1}>0\). Note that

\[\beta_{k}+\gamma_{k}=\frac{t_{k+1}(t_{k}^{2}-t_{k})}{t_{k}T_{k+1}}=\frac{t_{k+ 1}(t_{k}-1)}{T_{k+1}}.\]

By applying the formula at Proposition 1, we obtain

\[\beta_{k}^{\prime}=\frac{(T_{N-k}-t_{N-k})\frac{t_{N-k}(t_{N-k-1}-1)}{T_{N-k}} }{t_{N-k}^{2}-t_{N-k}}=\frac{(T_{N-k}-t_{N-k})(t_{N-k-1}-1)}{T_{N-k}(t_{N-k}-1 )}=\frac{T_{N-k-1}(t_{N-k-1}-1)}{T_{N-k}(t_{N-k}-1)}\]

and

\[\gamma_{k}^{\prime}=\frac{(t_{N-k}^{2}-T_{N-k})\frac{t_{N-k}(t_{N-k-1}-1)}{T_{N -k}}}{t_{N-k}^{2}-t_{N-k}}=\frac{(t_{N-k}^{2}-T_{N-k})(t_{N-k-1}-1)}{T_{N-k}(t_ {N-k}-1)}.\]

#### b.3.2 Proof of Corollary 2

First, recall (39) when \(0<h\leq 1\):

\[\left[\text{(GD) and }\{u_{i}\}_{i=0}^{N}=\left(\ldots,\frac{(2Nh+1)(i+1)}{2N-i}, \ldots,2Nh+1\right)\text{ satisfies (C1)}\right]\]

Additionally, observe that the \(H\) matrix of (GD) is \(\operatorname{diag}\left(h,\ldots,h\right)\), which gives the fact that the H-dual of (GD) is itself.

Next, use Theorem 1 to obtain

\[\left[\text{(GD) with }0<h\leq 1\text{ and }\{v_{i}\}_{i=0}^{N}=\left(\frac{1}{2Nh+1}, \ldots,\frac{N+i}{(2Nh+1)(N-i+1)},\ldots\right)\text{ satisfies (C2)}\right].\] (44)

By using the same argument with (5), (44) gives

\[\frac{1}{2L}\|\nabla f(y_{N})\|^{2}\leq\mathcal{V}_{N}\leq\mathcal{V}_{0}= \frac{1}{2Nh+1}\left(f(y_{0})-f_{\star}\right)+\frac{1}{2Nh+1}\llbracket y_{ N},y_{\star}\rrbracket\leq\frac{1}{2Nh+1}\left(f(y_{0})-f_{\star}\right).\] (45)

In addition, we can achieve the convergence rate of the gradient norm under the initial condition of \(\|x_{0}-x_{\star}\|^{2}\):

\[\frac{1}{2L}\|\nabla f(x_{2N})\|^{2}\leq\frac{1}{2Nh+1}\left(f(x_{N})-f_{ \star}\right)\leq\frac{1}{(2Nh+1)^{2}}\frac{L}{2}\|x_{0}-x_{\star}\|^{2}\]

and

\[\frac{1}{2L}\|\nabla f(x_{2N+1})\|^{2}\leq\frac{1}{2(N+1)h+1}\left(f(x_{N})-f_ {\star}\right)\leq\frac{1}{(2(N+1)h+1)\left(2Nh+1\right)}\frac{L}{2}\left\|x_ {0}-x_{\star}\right\|^{2}.\]

The first inequality comes from (45) and the second inequality comes from (40).

#### b.3.3 Proof of \(\mathcal{A}_{\star}\)-optimality of (OGM-G) and (OBL-G\({}_{\text{y}}\))

**Definition of \(\mathcal{A}^{\star}\)-optimal FSFOM.** For the given inequality sets \(\mathcal{L}\), \(\mathcal{A}^{\star}\)-optimal FSFOM with respect to \([\mathcal{L},\text{P1}]\) is defined as a FSFOM which \(H\) matrix is the solution of following minimax problem:

\[\underset{H\in\mathbb{R}_{L}^{N\times N}}{\text{minimize }} \underset{f}{\text{maximize }} f(x_{N})-f_{\star}\] (46) subject.to. \[[x_{0},\ldots,x_{N}\text{ are generated by FSFOM with the matrix }H]\] \[[\forall\,l\in\mathcal{L},f\text{ satisfies }l]\] \[\|x_{0}-x_{\star}\|^{2}\leq R^{2}\]

Similarly, \(\mathcal{A}^{\star}\)-optimal FSFOM with respect to \([\mathcal{L},(P2)]\) is specified with its \(H\) matrix, which is the solution of the following minimax problem:

\[\underset{H\in\mathbb{R}_{L}^{N\times N}}{\text{minimize }} \underset{f}{\text{maximize }} \frac{1}{2L}\|\nabla f(y_{N})\|^{2}\] (47) subject.to. \[[y_{0},\ldots,y_{N}\text{ are generated by FSFOM with the matrix }H]\] \[[\forall\,l\in\mathcal{L},f\text{ satisfies }l]\] \[f(y_{0})-f_{\star}\leq\frac{1}{2}LR^{2}\]

Here \(\mathbb{R}_{L}^{N\times N}\) is the set of lower triangular matrices. Next we denote the inner maximize problem of (46) and (47) as \(\mathcal{P}_{1}(\mathcal{L},H,R)\) and \(\mathcal{P}_{2}(\mathcal{L},H,R)\), respectively. For a more rigorous definition of \(\mathcal{A}^{\star}\)-optimal FSFOM, refer [44].

**Remark.** We discuss the minimax problems (46) and (47) and their interpretation. Specifically, we consider the meaning of the maximization problem in these minimax problems, which can be thought of as calculating the worst-case performance for a fixed FSFOM with \(H\). In other words, the minimization problem in (46) and (47) can be interpreted as determining the optimal value of \(H\) that minimizes the worst-case performance.

**Prior works.** The prior works for \(\mathcal{A}^{*}\)-optimality are summarized as follows. Consider the following sets of inequalities.

\[\mathcal{L}_{\text{F}}= \{[\![x_{i},x_{i+1}]\!]\}_{i=0}^{N-1}\cup\{[\![x_{\star},x_{i}]\!]\}_ {i=0}^{N}\] \[= \Big{\{}f(x_{i})\geq f(x_{i+1})+\langle\nabla f(x_{i+1}),x_{i}-x_ {i+1}\rangle+\frac{1}{2L}\|\nabla f(x_{i})-\nabla f(x_{i+1})\|^{2}\Big{\}}_{i=0} ^{N-1}\] \[\bigcup\Big{\{}f_{\star}\geq f(x_{i})+\langle\nabla f(x_{i}),x_{ \star}-x_{i}\rangle+\frac{1}{2L}\|\nabla f(x_{i})\|^{2}\Big{\}}_{i=0}^{N},\] \[\mathcal{L}_{\text{G}}= \{[\![y_{i},y_{i+1}]\!]\}_{i=0}^{N-1}\cup\{[\![y_{N},y_{i}]\!]\}_ {i=0}^{N-1}\cup\{[\![y_{N},\star]\!]\}\] \[= \Big{\{}f(y_{i})\geq f(y_{i+1})+\langle\nabla f(y_{i+1}),y_{i}-y_ {i+1}\rangle+\frac{1}{2L}\|\nabla f(y_{i})-\nabla f(y_{i+1})\|^{2}\Big{\}}_{i=0 }^{N-1}\] \[\bigcup\Big{\{}f(y_{N})\geq f(y_{i})+\langle\nabla f(y_{i}),y_{N }-y_{i}\rangle+\frac{1}{2L}\|\nabla f(y_{i})-\nabla f(y_{N})\|^{2}\Big{\}}_{i=0 }^{N-1}\] \[\bigcup\Big{\{}f(y_{N})\geq f_{\star}+\frac{1}{2L}\|\nabla f(y_{N })\|^{2}\Big{\}},\] \[\mathcal{L}_{\text{G}^{\prime}}= \{[\![y_{i},y_{i+1}]\!]\}_{i=0}^{N-1}\cup\Big{\{}[\![y_{N},y_{i} ]\!]-\frac{1}{2L}\|\nabla f(y_{i})-\nabla f(y_{N})\|^{2}\Big{\}}_{i=0}^{N-1} \cup\Big{\{}[\![y_{N},\star]\!]-\frac{1}{2L}\|\nabla f(y_{N})\|^{2}\Big{\}}\] \[= \Big{\{}f(y_{i})\geq f(y_{i+1})+\langle\nabla f(y_{i+1}),y_{i}-y_ {i+1}\rangle+\frac{1}{2L}\|\nabla f(y_{i})-\nabla f(y_{i+1})\|^{2}\Big{\}}_{i=0 }^{N-1}\] \[\bigcup\Big{\{}f(y_{N})\geq f(y_{i})+\langle\nabla f(y_{i}),y_{N }-y_{i}\rangle\Big{\}}_{i=0}^{N-1}\bigcup\Big{\{}f(y_{N})\geq f_{\star}\Big{\}},\] \[\mathcal{L}_{\text{F}^{\prime}}= \{[\![x_{i},x_{i+1}]\!]\}_{i=0}^{N-1}\cup\Big{\{}[\![x_{\star},x_ {i}]\!]-\frac{1}{2L}\|\nabla f(x_{i})\|^{2}\Big{\}}_{i=0}^{N}\] \[= \Big{\{}f(x_{i})\geq f(x_{i+1})+\langle\nabla f(x_{i+1}),x_{i}-x_ {i+1}\rangle+\frac{1}{2L}\|\nabla f(x_{i})-\nabla f(x_{i+1})\|^{2}\Big{\}}_{i=0 }^{N-1}\] \[\bigcup\Big{\{}f_{\star}\geq f(x_{i})+\langle\nabla f(x_{i}),x_{ \star}-x_{i}\rangle\Big{\}}_{i=0}^{N},\]

and

\[\mathcal{L}_{\text{exact}}= \{[\![x_{i},x_{j}]\!]\}_{(i,j)\in\{\star,0,1,\ldots,N\}^{2}}.\]

(OGM) is \(\mathcal{A}^{*}\)-optimal with respect to \([\mathcal{L}_{\text{F}},(\text{P1})]\)[26]. Furthermore, (OGM) is also \(\mathcal{A}^{*}\)-optimal under \([\mathcal{L}_{\text{exact}},(\text{P1})]\) which implies that (OGM) is the exact optimal FSFOM with respect to (P1). In addition, the \(\mathcal{A}^{*}\)-optimality of (OGM-G) with respect to \([\mathcal{L}_{\text{G}},(\text{P2})]\) was presented as a conjecture in [29], (OBL-F\({}_{\text{F}}\)) is \(\mathcal{A}^{*}\)-optimal with respect to \([\mathcal{L}_{\text{F}^{\prime}},(P1)]\)[44, Theorem 4], and the \(\mathcal{A}^{*}\)-optimality of (OBL-G\({}_{\text{G}}\)) with respect to \([\mathcal{L}_{\text{G}^{\prime}},(\text{P2})]\) was presented as a conjecture [44, Conjecture 8]. 4

Footnote 4: Originally, the inequality set suggested that (OBL-G\({}_{\text{G}}\)) would \(\mathcal{A}^{*}\)-optimal is which \((f(y_{N})\geq f_{\star})\) is replaced with \(\big{(}f(y_{N})\geq f_{\star}+\frac{1}{2L}\|\nabla f(y_{N})\|^{2}\big{)}\) in \(\mathcal{L}_{\text{G}^{\prime}}\).

In the remaining parts, we give the proof of the following two theorems.

**Theorem 4**.: (OGM-G) is \(A^{*}\)-optimal with respect to \([\mathcal{L}_{\text{G}},(\text{P2})]\).

**Theorem 5**.: (OBL-G\({}_{\text{\tiny\text{\tiny\text{\tiny\text{\tiny\text{\tiny\text{\tiny\text{\tiny\text{\tiny\text{\text{\text{ \text{\text{\text{\text{\text{\texttexttexttexttext{\text{\text{\text{\text{\text{\texttext{\texttexttexttexttexttexttexttexttexttexttexttexttexttexttexttexttext   \texttexttexttexttexttexttexttexttexttexttexttexttexttexttexttexttexttexttexttexttexttexttexttexttexttext@Proof of \(\mathcal{A}^{*}\)-optimality of (Ogm-G)We provide an alternative formulation of \(\mathcal{P}_{2}(\mathcal{L}_{\mathrm{G}},H,R)\) by using the methodology called PEP [19, 52]

\[\begin{array}{ll}\underset{f}{\text{maximize}}&\frac{1}{2L}\|\nabla f(y_{N})\| ^{2}\\ \text{subject to}& f(y_{i+1})-f(y_{i})+\langle\nabla f(y_{i+1}),y_{i}-y_{i+1} \rangle+\frac{1}{2L}\|\nabla f(y_{i})-\nabla f(y_{i+1})\|^{2}\leq 0,\quad i=0, \ldots,N-1\\ & f(y_{i})-f(y_{N})+\langle\nabla f(y_{i}),y_{N}-y_{i}\rangle+ \frac{1}{2L}\|\nabla f(y_{i})-\nabla f(y_{N})\|^{2}\leq 0,\quad i=0,\ldots,N-1\\ & f_{\star}+\frac{1}{2L}\|\nabla f(y_{N})\|^{2}\leq f(y_{N})\\ & f(y_{0})-f_{\star}\leq\frac{1}{2}LR^{2}\\ & y_{i+1}=y_{i}-\frac{1}{L}\sum_{j=0}^{i}h_{i+1,j}\nabla f(y_{j}), \quad i=0,\ldots,N-1\end{array}.\] (48)

Next, define \(\mathbf{F}\), \(\mathbf{G}\), \(\{\mathbf{e}_{i}\}_{i=0}^{N}\) and \(\{\mathbf{x}_{i}\}_{i=0}^{N}\) as

\[\mathbf{G}:=\begin{pmatrix}\langle\nabla f(y_{0}),\nabla f(y_{0})\rangle& \langle\nabla f(y_{0}),\nabla f(y_{1})\rangle&\cdots&\langle\nabla f(y_{0}), \nabla f(y_{N})\rangle\\ \vdots&\vdots&\vdots&\\ \langle\nabla f(y_{N}),\nabla f(y_{N})\rangle&\langle\nabla f(y_{N}),\nabla f (y_{1})\rangle&\cdots&\langle\nabla f(y_{N}),\nabla f(y_{N})\rangle\end{pmatrix} \in\mathbb{S}^{N+1},\]

\[\mathbf{F}:=\begin{pmatrix}f(y_{0})-f_{\star}\\ f(y_{1})-f_{\star}\\ \vdots\\ f(y_{N})-f_{\star}\end{pmatrix}\mathbb{R}^{(N+1)\times 1},\]

\(\mathbf{e}_{i}\in\mathbb{R}^{N+1}\) is a unit vector which \((i+1)\)-th component is \(1\), and

\[\mathbf{y}_{0}:=0,\qquad\mathbf{y}_{i+1}:=\mathbf{y}_{i}-\frac{1}{L}\sum_{j=0} ^{i}h_{i+1,j}\mathbf{e}_{j}\quad i=0,\ldots,N-1.\]

By using the definition of \(\mathbf{F}\), \(\mathbf{G}\), \(\{\mathbf{e}_{i}\}_{i=0}^{N}\) and \(\{\mathbf{y}_{i}\}_{i=0}^{N}\), \(\mathcal{P}_{2}(\mathcal{L}_{\mathrm{G}},H,R)\) can be converted into

\[\begin{array}{ll}\underset{\mathbf{F},\mathbf{G}\succeq 0}{\text{minimize}}&- \frac{1}{2L}\operatorname{Tr}\left(\mathbf{G}\mathbf{e}_{N}\mathbf{e}_{N}^{ \intercal}\right)\\ \text{subject to}&\mathbf{F}(\mathbf{e}_{i+1}-\mathbf{e}_{i})^{\intercal}+ \operatorname{Tr}\left(\mathbf{G}\mathbf{A}_{i}\right)\leq 0,\quad i=0,\ldots,N-1\\ &\mathbf{F}(\mathbf{e}_{i}-\mathbf{e}_{N})^{\intercal}+\operatorname{Tr} \left(\mathbf{G}\mathbf{B}_{i}\right)\leq 0,\quad i=0,\ldots,N-1\\ &-\mathbf{F}e_{N}^{\intercal}+\frac{1}{2}\operatorname{Tr}\left(\mathbf{G} \mathbf{e}_{N}\mathbf{e}_{N}^{\intercal}\right)\leq 0\\ &\mathbf{F}e_{0}^{\intercal}-\frac{1}{2}LR^{2}\leq 0\end{array}\] (49)

where

\[\mathbf{A}_{i}:=\frac{1}{2}\mathbf{e}_{i+1}(\mathbf{y}_{i}- \mathbf{y}_{i+1})^{\intercal}+\frac{1}{2}(\mathbf{y}_{i}-\mathbf{y}_{i+1}) \mathbf{e}_{i+1}^{\intercal}+\frac{1}{2L}\left(\mathbf{e}_{i}-\mathbf{e}_{i+1} \right)(\mathbf{e}_{i}-\mathbf{e}_{i+1})^{\intercal}\] \[\mathbf{B}_{i}:=\frac{1}{2}\mathbf{e}_{i}(\mathbf{y}_{N}-\mathbf{y }_{i})^{\intercal}+\frac{1}{2}(\mathbf{y}_{N}-\mathbf{y}_{i})\mathbf{e}_{i}^{ \intercal}+\frac{1}{2L}\left(\mathbf{e}_{i}-\mathbf{e}_{N}\right)(\mathbf{e}_{i }-\mathbf{e}_{N})^{\intercal}\,.\]

Moreover, under the condition \(d\geq N+2\), we can take the Cholesky factorization of \(\mathbf{G}\) to recover the triplet \(\{(y_{i},f(y_{i}),\nabla f(y_{i}))\}_{i=0}^{N}\).5 Thus (48) and (49) are equivalent. The next step is calculating the Lagrangian of (49) and deriving the Lagrangian dual problem of it. In [54], they argued about the strong duality of (49).

**Fact 1**.: Assume \(h_{i+1,i}\neq 0\) for \(0\leq i\leq N-1\). Then the strong duality holds between (48) and (49). Denote the dual variables of each constraints as \(\{\delta_{i}\}_{i=0}^{N-1}\), \(\{\lambda_{i}\}_{i=0}^{N-1}\), \(\delta_{N}\) and \(\tau\). Then Lagrangian becomes

\[\mathcal{L}(\delta,\lambda,\tau,\mathbf{F},\mathbf{G})=\mathbf{F}\cdot\mathbf{ X}^{\intercal}+\mathrm{Tr}\left(\mathbf{G}\cdot\mathbf{T}\right)-\frac{\tau L}{2}R^{2}\]

where

\[\mathbf{X} =\sum_{i=0}^{N-1}\delta_{i}(\mathbf{e}_{i+1}-\mathbf{e}_{i})+ \sum_{i=0}^{N-1}\lambda_{i}(\mathbf{e}_{i}-\mathbf{e}_{N})-\delta_{N}\mathbf{ e}_{N}+\tau\mathbf{e}_{0},\] \[\mathbf{T} =\sum_{i=0}^{N-1}\delta_{i}\mathbf{A}_{i}+\sum_{i=0}^{N-1} \lambda_{i}\mathbf{B}_{i}+\frac{\delta_{N}}{2}\mathbf{e}_{N}\mathbf{e}_{N}^{ \intercal}-\frac{1}{2}\mathbf{e}_{N}\mathbf{e}_{N}^{\intercal}.\]

If \([\bm{X}=0\text{ and }\bm{T}\succeq 0]\) is false, we can choose \(\mathbf{F}\) and \(\mathbf{G}\) that makes the value of Lagrangian to be \(-\infty\). Thus the convex dual problem of (49) is

\[\underset{\delta,\lambda,\tau}{\text{maximize }}\underset{\mathbf{F}, \mathbf{G}}{\text{minimize }}\mathcal{L}(\delta,\lambda,\tau,\mathbf{F},\mathbf{G})=\begin{cases} \underset{\delta,\lambda,\tau}{\text{maximize }}&-\frac{\tau L}{2}R^{2}\\ \text{subject to}&\mathbf{X}=0,\,\mathbf{T}\succeq 0\\ &\delta_{i}\geq 0,\quad\lambda_{i}\geq 0,\quad\tau\geq 0\end{cases}.\] (50)

For the constraint \(\mathbf{X}=0\), \(\lambda_{i}=\delta_{i}-\delta_{i-1}\) for \(1\leq i\leq N-1\), \(\tau=\delta_{N}\) and \(-\delta_{0}+\lambda_{0}+\delta_{N}=0\). By substituting \(v_{i+1}=\delta_{i}\) for \(0\leq i\leq N-1\) and \(v_{0}=\delta_{N}\), (50) becomes

\[\underset{v_{0},\ldots,v_{N}}{\text{maximize }}\underset{\mathbf{F}, \mathbf{G}}{\text{minimize }}\mathcal{L}(\delta,\lambda,\tau,\mathbf{F},\mathbf{G})= \begin{cases}\underset{\delta,\lambda,\tau}{\text{maximize }}&-\frac{v_{0}L}{2}R^{2}\\ \text{subject to}&\mathbf{T}\succeq 0\\ &0\leq v_{0}\leq\cdots\leq v_{N}\end{cases}.\] (51)

Therefore if strong duality holds, \(\mathcal{P}_{2}(\mathcal{L}_{\mathrm{G}},H,R)\) becomes

\[\underset{v_{0},\ldots,v_{N}}{\text{minimize }}\frac{v_{0}L}{2}R^{2}\] (52) \[\text{subject.to.}\,\,\mathbf{T}\succeq 0,\,\,0\leq v_{0}\leq \cdots\leq v_{N}.\]

We can apply a similar argument for \(\mathcal{P}_{1}(\mathcal{L}_{\mathrm{F}},H,R)\). To begin with, define \(\{\mathbf{f}_{i}\}_{i=-1}^{N}\) as a unit vector of length \(N+2\) which \((i+2)\)-component is \(1\). Additionally, define \(\{\mathbf{x}_{i}\}_{i=0}^{N}\), \(\{\mathbf{C}_{i}\}_{i=0}^{N-1}\) and \(\{\mathbf{D}_{i}\}_{i=0}^{N}\) as follows:

\[\mathbf{x}_{0} :=\mathbf{f}_{-1},\quad\mathbf{x}_{i+1}:=\mathbf{x}_{i}-\frac{1} {L}\sum_{j=0}^{i}h_{i+1,j}\mathbf{f}_{j}\quad i=0,\ldots,N-1,\] (53) \[\mathbf{C}_{i} :=\frac{1}{2}\mathbf{f}_{i+1}(\mathbf{x}_{i}-\mathbf{x}_{i+1})^{ \intercal}+\frac{1}{2}(\mathbf{x}_{i}-\mathbf{x}_{i+1})\mathbf{f}_{i+1}^{ \intercal}+\frac{1}{2L}\left(\mathbf{f}_{i}-\mathbf{f}_{i+1}\right)(\mathbf{f }_{i}-\mathbf{f}_{i+1})^{\intercal}\,,\] \[\mathbf{D}_{i} :=-\frac{1}{2}\mathbf{f}_{i}\mathbf{x}_{i}^{\intercal}-\frac{1}{ 2}\mathbf{x}_{i}\mathbf{f}_{i}^{\intercal}+\frac{1}{2L}\mathbf{f}_{i} \mathbf{f}_{i}^{\intercal}.\]

Then, if the strong duality holds, the problem

\[\underset{f}{\text{maximize }}f(x_{N})-f_{\star}\] (54) subject.to. \[[x_{0},\ldots,x_{N}\text{ are generated by FSFO with the matrix }H]\] \[\quad[\forall l\in\mathcal{L}_{\mathrm{F}},f\text{ satisfies }l]\] \[\quad\|x_{0}-x_{\star}\|^{2}\leq R^{2}\]

is equivalent to

\[\underset{u_{0},\ldots,u_{N}}{\text{maximize }}\frac{L}{2u_{N}}R^{2}\] (55) subject.to. \[\mathbf{S}\succeq 0,\,\,0\leq u_{0}\leq\cdots\leq u_{N},\]where

\[\mathbf{S}=\frac{L}{2}\mathbf{f}_{-1}\mathbf{f}_{-1}^{\mathsf{T}}+ \sum_{i=0}^{N-1}u_{i}\mathbf{C}_{i}+\sum_{i=0}^{N}(u_{i}-u_{i-1})\mathbf{D}_{i}.\]

By using Schur's Complement,

\[\mathbf{S}\succeq 0\quad\Leftrightarrow\quad\mathbf{S}^{\prime}\succeq 0\]

where

\[\mathbf{S}^{\prime}=\sum_{i=0}^{N-1}u_{i}\mathbf{C}_{i}+\sum_{i=0}^{N}(u_{i}- u_{i-1})\mathbf{D}_{i}-\frac{1}{2L}\left(\sum_{i=0}^{N}(u_{i}-u_{i-1})\mathbf{f}_{i} \right)\left(\sum_{i=0}^{N}(u_{i}-u_{i-1})\mathbf{f}_{i}\right)^{\mathsf{T}}.\]

Hence (55) is equivalent to

\[\underset{u_{0},\ldots,u_{N}}{\text{maximize}}\ \frac{L}{2u_{N}}R^{2}\] (56) \[\text{subject.to.}\ \mathbf{S}^{\prime}\succeq 0,\ 0\leq u_{0}\leq \cdots\leq u_{N}.\]

Now we will prove the following proposition.

**Proposition 2**.: Consider a matrix \(H=\{h_{i,j}\}_{0\leq j<i\leq N}\) and \(H^{A}\). If \(h_{i+1,i}\neq 0\) for \(0\leq i\leq N-1\) and treating the solution of infeasible maximize problem as \(0\), the optimal values of \(\mathcal{P}_{1}(\mathcal{L}_{\mathrm{F}},H,R)\) and \(\mathcal{P}_{2}(\mathcal{L}_{\mathrm{G}},H^{A},R)\) are same.

Proof.: To simplify the analysis, we can consider \(\{\mathbf{f}_{i}\}_{i=0}^{N}\) as a length \(N+1\) unit vector, as all terms with \(\mathbf{f}_{-1}\) can be eliminated by using \(\mathbf{S}^{\prime}\) instead of \(\mathbf{S}\). With this simplification, both \(\mathbf{S}^{\prime}\) and \(\mathbf{T}\) belong to \(\mathbb{S}^{N+1}\).

Next, let \(0<u_{0}\leq u_{1}\leq\cdots\leq u_{N}\) and \(v_{i}=\frac{1}{u_{N-i}}\) for \(0\leq i\leq N\), noting that \(0<v_{0}\leq\cdots\leq v_{N}\). It is important to observe that \(\mathbf{S}^{\prime}\) and \(\mathbf{T}\) can be expressed in terms of \(\mathcal{S}\left(H,u\right)\) and \(\mathcal{T}\left(H^{A},v\right)\), respectively. Furthermore, in the proof of Theorem 1 (A), we proved that \(\mathcal{S}(H,u)=\mathcal{M}(u)^{\mathsf{T}}\mathcal{T}(H^{A},v)\mathcal{M}(u)\) for some invertible \(\mathcal{M}(u)\). Thus,

\[\mathcal{S}(H,u)\succeq 0\quad\Leftrightarrow\quad\mathcal{T}(H^{A},v)\succeq 0.\]

Therefore, we obtain

\[(a_{0},\ldots,a_{N})\in\{(u_{0},\ldots,u_{N})|\,\mathbf{S}^{\prime}\succeq 0,\,0<u_{0}\leq\cdots\leq u_{N}\}\] (57)

if and only if

\[\left(\frac{1}{a_{N}},\ldots,\frac{1}{a_{0}}\right)\in\{(v_{0}, \ldots,v_{N})|\,\mathbf{T}\succeq 0,\,0<v_{0}\leq\cdots\leq v_{N}\}.\] (58)

For the next step, we claim that the optimal value of (56) and

\[\underset{u_{0},\ldots,u_{N}}{\text{maximize}}\ \frac{L}{2u_{N}}R^{2}\] (59) \[\text{subject.to.}\ \mathbf{S}^{\prime}\succeq 0,\ 0<u_{0}\leq \cdots\leq u_{N}\]

are same, i.e., consider the case when all \(u_{i}\) are positive is enough. To prove that, assume there exists \(0=u_{0}=\cdots=u_{k}\), \(0<u_{k+1}\leq\cdots\leq u_{N}\) and \(H\) that satisfy \(\mathbf{S}^{\prime}\succeq 0\). Next, observe that the \(\mathbf{f}_{k}\mathbf{f}_{k}^{\mathsf{T}}\) component of \(\mathbf{S}^{\prime}\) is \(0\) but \(\mathbf{f}_{k+1}\mathbf{f}_{k}^{\mathsf{T}}\) component of \(\mathbf{S}^{\prime}\) is \(u_{k+1}h_{k+1,k}\neq 0\), which makes \(\mathbf{S}^{\prime}\succeq 0\) impossible.

When the optimal value of (52) is \(0\), it implies \(\{(v_{0},\ldots,v_{N})|\,\mathbf{T}\succeq 0,\,0<v_{0}\leq\cdots\leq v_{N}\}\) is an empty set. Therefore, \(\{(u_{0},\ldots,u_{N})|\,\mathbf{S}^{\prime}\succeq 0,\,0<u_{0}\leq \cdots\leq u_{N}\}\) is also empty set. Since (59) and (56) have the same optimal value, the optimal value of (56) is \(0\).

If the optimal value of (52) is positive, the optimal values of (56) and (52) are the same since (57) and (58) are equivalent.

Proof of Theorem 4.: \(H_{\text{OGM}}\) is the solution of (46) since (OGM) is \(\mathcal{A}^{\star}\)-optimal with respect to \(\mathcal{L}_{\text{F}}\) and (P1). Additionally, if

\[\eqref{eq:H_OGM_F}=\underset{H,h_{i+1},i\neq 0}{\text{maximize}}\ \mathcal{P}_{1}( \mathcal{L}_{\text{F}},H,R)\] (60)

holds, \(H_{\text{OGM}}\) is the solution to the following problem due to the strong duality.

\[\underset{H,h_{i+1},i\neq 0}{\text{maximize}} \underset{u_{0},\ldots,u_{N}}{\text{maximize}}\ \frac{L}{2u_{N}}R^{2}\] (61) subject.to. \[\mathbf{S}\succeq 0,\ 0\leq u_{0}\leq\cdots\leq u_{N}.\]

Applying Proposition 2 and using the fact \(H_{\text{OGM-G}}=H_{\text{OGM}}^{\mathcal{A}}\) provides that \(H_{\text{OGM-G}}\) is the solution of

\[\underset{H,h_{i+1},i\neq 0}{\text{maximize}} \underset{v_{0},\ldots,v_{N}}{\text{minimize}}\ \frac{v_{0}L}{2}R^{2}\] (62) subject.to. \[\mathbf{T}\succeq 0,\ 0\leq v_{0}\leq\cdots\leq v_{N}.\]

Finally, if

\[\eqref{eq:H_OGM_F}=\underset{H,h_{i+1},i\neq 0}{\text{maximize}}\ \mathcal{P}_{1}( \mathcal{L}_{\text{F}},H,R)\] (63)

holds, the optimal solution of (47) is the \(H_{\text{OGM-G}}\), which proves the \(\mathcal{A}^{\star}\)-optimality of (OGM-G) with respect to \(\mathcal{L}_{\text{G}}\) and (P2). The proof of (60) and (63) uses the continuity argument with \(H\) and please refer [44, Claim 4]. 

Remark of Proof of \(\mathcal{A}^{\star}\)-optimality of (Ogm-G).We proved that (OGM-G) is \(\mathcal{A}^{\star}\)-optimal if we use the subset of cocoercivity inequalities. Therefore, it is still open whether (OGM-G) is optimal or not among the \(L\)-smooth convex function's gradient minimization method.

Proof of \(\mathcal{A}^{\star}\)-optimality of (Obl-G\({}_{\text{\tiny{b}}}\)).We provide the proof that the \(H\) matrix of (OBL-G\({}_{\text{\tiny{b}}}\)) is the solution of

\[\underset{H\in\mathbb{R}_{L}^{N\times N}}{\text{minimize}}\ \mathcal{P}_{2}( \mathcal{L}_{\text{G}^{\star}},H,R)\] (64)

To begin with, bring to mind the \(\mathcal{A}^{\star}\)-optimality of (OBL-F\({}_{\text{\tiny{b}}}\)): (OBL-F\({}_{\text{\tiny{b}}}\)) is \(\mathcal{A}^{\star}\)-optimal with respect to the \([\mathcal{L}_{\text{F}^{\prime}},P1]\), i.e., the \(H\) matrix of (OBL-F\({}_{\text{\tiny{b}}}\)) is the solution of

\[\underset{H\in\mathbb{R}_{L}^{N\times N}}{\text{minimize}}\ \mathcal{P}_{1}( \mathcal{L}_{\text{F}},H,R).\] (65)

To prove the \(\mathcal{A}^{\star}\)-optimality of (OBL-G\({}_{\text{\tiny{b}}}\)), we use the \(\mathcal{A}^{\star}\)-optimality of (OBL-F\({}_{\text{\tiny{b}}}\)).

Under the assumption of strong duality, we could change \(\mathcal{P}_{1}(\mathcal{L}_{\text{F}},H,R)\) into the following SDP:

\[\underset{u_{0},\ldots,u_{N}}{\text{maximize}} \frac{L}{2u_{N}}R^{2}\] (66) subject.to. \[\mathbf{S}_{1}\succeq 0,\ 0\leq u_{0}\leq\cdots\leq u_{N}\]

where

\[\mathbf{S}_{1}=\frac{L}{2}\mathbf{f}_{-1}\mathbf{f}_{-1}^{\intercal}+\sum_{i =0}^{N-1}u_{i}\mathbf{C}_{i}+\sum_{i=0}^{N}(u_{i}-u_{i-1})\left(\mathbf{D}_{i} -\frac{1}{2L}\mathbf{f}_{i}\mathbf{f}_{i}^{\intercal}\right).\]

Here we used the same notation with (53). Each \(\frac{1}{2L}\mathbf{f}_{i}\mathbf{f}_{i}^{\intercal}\) term is subtracted from the original \(\mathbf{S}\) since we consider the inequality \([\![x_{i},x_{\star}]\!]-\frac{1}{2L}\|\nabla f(x_{i})\|^{2}\) instead of \([\![x_{i},x_{\star}]\!]\) for \(\mathcal{L}_{\text{F}^{\prime}}\). Moreover, (66) is equivalent to

\[\underset{u_{0},\ldots,u_{N}}{\text{maximize}} \frac{L}{2u_{N}}R^{2}\] (67) subject.to. \[\mathbf{S}_{1}^{\prime}\succeq 0,\ 0\leq u_{0}\leq\cdots\leq u_{N}\]where

\[\mathbf{S}_{1}^{\prime}=\sum_{i=0}^{N-1}u_{i}\mathbf{C}_{i}+\sum_{i=0}^{N}(u_{i}- u_{i-1})\left(\mathbf{D}_{i}-\frac{1}{2L}\mathbf{f}_{i}\mathbf{f}_{i}^{\intercal} \right)-\frac{1}{2L}\left(\sum_{i=0}^{N}(u_{i}-u_{i-1})\mathbf{f}_{i}\right) \left(\sum_{i=0}^{N}(u_{i}-u_{i-1})\mathbf{f}_{i}\right)^{\intercal}.\]

Similarly, under the assumption of strong duality, \(\mathcal{P}_{2}(\mathcal{L}_{\mathrm{G}^{\prime}},H,R)\) is equivalent to

\[\underset{v_{0},\ldots,v_{N}}{\text{minimize}} \frac{v_{0}L}{2}R^{2}\] (68) subject.to. \[\mathbf{T}_{1}\succeq 0,\ 0\leq v_{0}\leq\cdots\leq v_{N}\]

where

\[\mathbf{T}_{1}=\sum_{i=0}^{N-1}v_{i+1}\mathbf{A}_{i}+\sum_{i=0}^{N-1}(v_{i+1}- v_{i})\left(\mathbf{B}_{i}-\frac{1}{2L}(\mathbf{e}_{i}-\mathbf{e}_{N})( \mathbf{e}_{i}-\mathbf{e}_{N})^{\intercal}\right)-\frac{1}{2}\mathbf{e}_{N} \mathbf{e}_{N}^{\intercal}.\]

Now we will prove the following proposition.

**Proposition 3**.: Consider a matrix \(H=\{h_{k,i}\}_{0\leq i<k\leq N}\) and \(H^{A}\). If \(h_{i+1,i}\neq 0\) for \(0\leq i\leq N-1\) and treating the solution of infeasible maximize problem as \(0\), the optimal values of \(\mathcal{P}_{1}(\mathcal{L}_{\mathrm{F}},H,R)\) and \(\mathcal{P}_{2}(\mathcal{L}_{\mathrm{G}^{\prime}},H^{A},R)\) are same.

Proof.: The proof structure is the same as the proof of Proposition 2. First consider \(\{\mathbf{f}_{i}\}_{i=0}^{N}\) as length \(N+1\), which gives \(\mathbf{S}_{1}^{\prime},\mathbf{T}_{1}\in\mathbb{S}^{N+1}\). Furthermore, in the proof of Appendix A, we proved that

\[\mathbf{S}_{1}^{\prime}=\mathcal{M}(u)^{\intercal}\mathbf{T}_{1}\mathcal{M}(u).\]

Therefore,

\[\mathbf{S}_{1}^{\prime}\succeq 0\quad\Leftrightarrow\quad\mathbf{T}\succeq 0.\]

The other steps are the same as the proof of Proposition 2. 

Proof of Theorem 5.: \(H_{\mathrm{OGM}}\) is the solution of (46) since (OGM) is \(\mathcal{A}^{\star}\)-optimal with respect to \(\mathcal{L}_{\mathrm{F}}\) and (P1). Additionally, if

\[\eqref{eq:H_OGM}=\underset{H,h_{i+1,i}\neq 0}{\text{maximize}} \mathcal{P}_{1}(\mathcal{L}_{\mathrm{F}},H,R)\] (69)

holds, \(H_{\mathrm{OGM}}\) is the solution to the following problem due to the strong duality.

\[\underset{H,h_{i+1,i}\neq 0}{\text{maximize}} \frac{L}{2u_{N}}R^{2}\] (70) subject.to. \[\mathbf{S}\succeq 0,\ 0\leq u_{0}\leq\cdots\leq u_{N}.\]

Applying Proposition 2 and using the fact \(H_{\mathrm{OGM-G}}=H_{\mathrm{OGM}}^{A}\) provides that \(H_{\mathrm{OGM-G}}\) is the solution of

\[\underset{H,h_{i+1,i}\neq 0}{\text{maximize}} \frac{v_{0}L}{2}R^{2}\] (71) subject.to. \[\mathbf{T}\succeq 0,\ 0\leq v_{0}\leq\cdots\leq v_{N}.\]

Finally, if

\[\eqref{eq:H_OGM}=\underset{H,h_{i+1,i}\neq 0}{\text{maximize}} \mathcal{P}_{1}(\mathcal{L}_{\mathrm{F}},H,R)\] (72)

holds, the optimal solution of (47) is the \(H_{\mathrm{OGM-G}}\), which proves the \(\mathcal{A}^{\star}\)-optimality of (OGM-G) with respect to \(\mathcal{L}_{\mathrm{G}}\) and (P2). The proof of (69) and (72) uses the continuity argument with \(H\) and please refer [44, Claim 4].

Omitted parts in Section 3

### Omitted calculations in Section 3.1

Strictly speaking, (7) is not a differential equation but rather a diffeo-integral equation. However, if \(H\) is separable, i.e., when \(H(t,s)=e^{\beta(s)-\gamma(t)}\) for some \(\beta,\gamma\colon[0,T)\to\mathbb{R}\), then (7) can be reformulated as an ODE.

Assume the process (7) is well-defined. We can alternatively write (7) as

\[X(0)=x_{0},\quad\dot{X}(t)=-e^{-\gamma(t)}\int_{0}^{t}e^{\beta(s)}\nabla f(X(s ))ds.\] (73)

By multiplying \(e^{\gamma(t)}\) each side and differentiating, we obtain

\[\ddot{X}(t)+\dot{\gamma}(t)\dot{X}(t)+e^{\beta(t)-\gamma(t)}\nabla f(X(t))=0.\]

The H-dual of (73) is

\[Y(0)=x_{0},\quad\dot{Y}(t)=-e^{\beta(T-t)}\int_{0}^{t}e^{-\gamma(T-s)}\nabla f (Y(s))ds.\]

Under the well-definedness, by multiplying \(e^{-\beta(T-t)}\) each side and differentiating, we obtain

\[\ddot{Y}(t)+\dot{\beta}(T-t)\dot{Y}(t)+e^{\beta(T-t)-\gamma(T-t)}\nabla f(Y(t ))=0.\]

When \(\beta(s)=\gamma(t)=r\log t\), two ODEs become

\[\ddot{X}(t)+\frac{r}{t}\dot{X}(t)+\nabla f(X(t))=0\] (74)

and

\[\ddot{Y}(t)+\frac{r}{T-t}\dot{X}(t)+\nabla f(X(t))=0,\] (75)

which are introduced in Section 3.2.

For the calculations of energy functions of (74) and (75), refer [51, Section 3.1, Section 4.2]. They proved that

\[(r-1)\left\|X(0)-x_{\star}\right\|^{2}= T^{2}\left(f(X(T))-x_{\star}\right)+\frac{1}{2}\left\|T\dot{X}(T)+2(X(T)-x_ {\star})\right\|^{2}\] \[+(r-3)\left\|X(T)-x_{\star}\right\|^{2}+\int_{0}^{T}(r-3)s\left\| \dot{X}(s)\right\|^{2}ds-\int_{0}^{T}2s[X(s),x_{\star}]ds\]

holds for (74) and

\[\frac{1}{T^{2}}\left(f(Y(0))-f(Y(T))\right)+\frac{-r+3}{T^{4}} \left\|Y(0)-Y(T)\right\|^{2}\] \[= \frac{1}{4(r-1)}\left\|\nabla f(Y(T))\right\|^{2}+\int_{0}^{T} \frac{r-3}{(T-s)^{5}}\left\|(T-s)\dot{Y}(s)+2(Y(s)-Y(T))\right\|^{2}ds-\int_{ 0}^{T}\frac{2}{(T-s)^{3}}[Y(s),Y(T)]ds\]

holds for (75). After arranging terms, we obtain the results in Section 3.2. 6

Footnote 6: In [51], they considered the ODE which gradient term is \(2\nabla f(Y(t))\) instead of \(\nabla f(Y(t))\).

### Proof of Theorem 2

To begin with, we give a formal version of Theorem 2. Consider two following conditions.

\[u(T)\left(f(X(T))-f^{\star}\right)\leq\mathcal{U}(T),\quad(\forall\,X(0),x^{ \star},\{\nabla f(X(s))\}_{s\in[0,T]}\in\mathcal{A}_{1}).\] (C3 \[{}^{\prime}\] )

\[\frac{1}{2}\|\nabla f(Y(T))\|^{2}\leq\mathcal{V}(T),\quad(\forall\,Y(0),\{ \nabla f(Y(s))\}_{s\in[0,T]}\in\mathcal{A}_{2}).\] (C4 \[{}^{\prime}\] )

where \(\mathcal{A}_{1}\) and \(\mathcal{A}_{2}\) are family of vectors which makes Fubini's Theorem can be applied and will be defined later in this section.

**Theorem 6** (Formal version of Theorem 2).: Assume the C-FSFOMs (7) with \(H\) and \(H^{A}\) are well-defined in the sense that solutions to the diffeo-integral equations exist. Consider differentiable functions \(u,v\colon(0,T)\to\mathbb{R}\) that \(v(t)=\frac{1}{u(T-t)}\) for \(t\in[0,T]\) and

1. \(\lim_{s\to 0}u(t)=0\)
2. \(\lim_{s\to-T}v(s)\left(f(Y(s))-f(Y(T))+\left\langle\nabla f(Y(T)),Y(T)-Y(s) \right\rangle\right)=0\).
3. \(f\) is \(L\)-smooth and convex.

Then the following holds.

\[\left[\text{(C3${}^{\prime}$) is satisfied with $u(\cdot)$ and $H$}\right]\,\Leftrightarrow\,\left[\text{(C4${}^{\prime}$) is satisfied with $v(\cdot)$ and $H^{A}$}\right]\,.\]

Calculations of energy functions via transformationFirst of all, we calculate \(\mathcal{U}(T)-u(T)(f(X(T))-f^{\star})\).

\[\mathcal{U}(T)-u(T)\left(f(X(T))-f^{\star}\right)\] \[= \frac{1}{2}\left\|X(0)-x^{\star}\right\|^{2}+\int_{0}^{T}u^{ \prime}(s)\left(f(X(s))-f^{\star}+\left\langle\nabla f(X(s)),x^{\star}-X(s) \right\rangle\right)ds-u(T)\left(f(X(T))-f^{\star}\right)ds\] \[= \frac{1}{2}\left\|X(0)-x^{\star}\right\|^{2}+\int_{0}^{T}u^{ \prime}(s)\left\langle\nabla f(X(s)),x^{\star}-X(s)\right\rangle ds-\int_{0}^ {T}u(s)\left\langle\nabla f(X(s)),\dot{X}(s)\right\rangle ds\] \[= \frac{1}{2}\left\|X(0)-x^{\star}\right\|^{2}+\int_{0}^{T}u^{ \prime}(s)\left\langle\nabla f(X(s)),x^{\star}-X(0)\right\rangle ds\] \[\quad+\int_{0}^{T}u^{\prime}(s)\left\langle\nabla f(X(s)),X(0)-X (s)\right\rangle ds-\int_{0}^{T}u(s)\left\langle\nabla f(X(s)),\dot{X}(s) \right\rangle ds.\]

We used parts of integration and \(u(0)\colon=\lim_{s\to 0}u(s)=0\). Since \(X(0)-x^{\star}\) can have any value, (C3) is equivalent to

\[-\frac{1}{2}\left\|\int_{0}^{T}u^{\prime}(s)\nabla f(X(s))ds \right\|^{2}\\ +\int_{0}^{T}u^{\prime}(s)\left\langle\nabla f(X(s)),X(0)-X(s) \right\rangle ds-\int_{0}^{T}u(s)\left\langle\nabla f(X(s)),\dot{X}(s)\right\rangle ds \geq 0\] (76)

holds for any \(\{\nabla f(X(s))\}_{s\in[0,T]}\).

Now define a transformation \(\{f_{s}\in\mathbb{R}^{d}\}_{s\in[0,T]}\to\{g_{s}\in\mathbb{R}^{d}\}_{s\in[0,T]}\) as

\[g_{s}\colon\,=u(s)f_{s}+\int_{s}^{T}u^{\prime}(z)f_{z}dz,\quad s\in[0,T]\] (77)

and \(\{g_{s}\in\mathbb{R}^{d}\}_{s\in[0,T]}\to\{f_{s}\in\mathbb{R}^{d}\}_{s\in[0,T]}\).

\[f_{s}\colon\,=\frac{1}{u(T)}g_{0}+\frac{1}{u(s)}\left(g_{s}-g_{0}\right)-\int_ {s}^{T}\frac{u^{\prime}(b)}{u(b)^{2}}\left(g_{b}-g_{0}\right)db,\quad s\in[0,T].\] (78)

One can show that the above two transformations are in the inverse relationship. Next, we calculate \(\mathcal{U}(T)\). Define \(f_{s}\colon\,=\nabla f(X(s))\). Under the transformation (77), we can find a simple expression of(76).

\[\begin{split}\eqref{eq:3.1}&=-\frac{1}{2}\left\|g_{0} \right\|^{2}-\int_{0}^{T}u^{\prime}(s)\left\langle f_{s},X(s)-X(0)\right\rangle ds-\int_{0}^{T}u(s)\left\langle f_{s}, \dot{X}(s)\right\rangle ds\\ &=-\frac{1}{2}\left\|g_{0}\right\|^{2}-\int_{0}^{T}u^{\prime}(s) \left\langle f_{s},\int_{0}^{s}\dot{X}(a)da\right\rangle ds+\int_{0}^{T}u(s) \left\langle f_{s},\int_{0}^{s}\dot{X}(a)da\right\rangle ds\\ &=-\frac{1}{2}\left\|g_{0}\right\|^{2}-\int_{0}^{T}u^{\prime}(s) \left\langle f_{s},\int_{0}^{s}\dot{X}(a)da\right\rangle ds+\int_{0}^{T}\int_{ 0}^{s}H(s,a)\left\langle u(s)f_{s},f_{a}\right\rangle dads\\ &\stackrel{{(\circ)}}{{=}}-\frac{1}{2}\left\|g_{0} \right\|^{2}-\int_{0}^{T}\left\langle\dot{X}(a),\int_{a}^{T}u^{\prime}(s)f_{s }ds\right\rangle da+\int_{0}^{T}\int_{0}^{s}H(s,a)\left\langle u(s)f_{s},f_{a }\right\rangle dads\\ &=-\frac{1}{2}\left\|g_{0}\right\|^{2}+\int_{0}^{T}\int_{0}^{s} H(s,a)\left\langle f_{a},g_{s}\right\rangle dads.\end{split}\] (79)

We used Fubini's Theorem at \((\circ)\). Next we calculate \(\mathcal{V}(T)\).

\[\begin{split}\mathcal{V}(T)=&\frac{1}{u(T)}\left( f(Y(0))-f(Y(T))\right)+\int_{0}^{T}\frac{d}{ds}\frac{1}{u(T-s)}\left(f(Y(s))-f(Y(T))+ \left\langle\nabla f(Y(s)),Y(T)-Y(s)\right\rangle\right)ds\\ =&\frac{1}{u(T)}\left(f(Y(0))-f(Y(T))\right)+\int_{0 }^{T}\frac{u^{\prime}(T-s)}{u(T-s)^{2}}\left\langle Y(T)-Y(s),\nabla f(Y(s))- \nabla f(Y(T))\right\rangle ds\\ &\quad+\int_{0}^{T}\frac{d}{ds}\frac{1}{u(T-s)}\left(f(Y(s))-f(Y (T))+\left\langle\nabla f(Y(T)),Y(T)-Y(s)\right\rangle\right)ds\\ \stackrel{{(\circ)}}{{=}}&\frac{1}{u(T)} \left\langle\nabla f(Y(T)),Y(0)-Y(T)\right\rangle+\int_{0}^{T}\frac{u^{\prime }(T-s)}{u(T-s)^{2}}\left\langle Y(T)-Y(s),\nabla f(Y(s))-\nabla f(Y(T)) \right\rangle ds\\ &\quad-\int_{0}^{T}\frac{1}{u(T-s)}\left\langle\dot{Y}(s),\nabla f (Y(s))-\nabla f(Y(T))\right\rangle ds.\end{split}\]

\((\circ)\) comes from parts of integration and the assumption

\[\lim_{s\to T}v(s)\left(f(Y(s))-f(Y(T))+\left\langle\nabla f(X(T)),Y(T)-Y(s) \right\rangle\right)=0.\]

To clarify, \(u^{\prime}(T-s)=u^{\prime}(z)|_{z=T-s}\). Now briefly write \(g_{s}\colon=\nabla f(Y(T-s))\). Then

\[\begin{split}&\mathcal{V}(T)-\frac{1}{2}\left\|\nabla f(Y(T)) \right\|^{2}\\ =&-\frac{1}{2}\left\|g_{0}\right\|^{2}-\frac{1}{u(T)} \int_{0}^{T}\left\langle g_{0},\dot{Y}(s)\right\rangle ds+\int_{0}^{T}\frac{u^ {\prime}(T-s)}{u(T-s)^{2}}\left\langle Y(T)-Y(s),g_{T-s}-g_{0}\right\rangle ds \\ &\quad-\int_{0}^{T}\frac{1}{u(T-s)}\left\langle\dot{Y}(s),g_{T-s }-g_{0}\right\rangle ds\\ =&-\frac{1}{2}\left\|g_{0}\right\|^{2}+\int_{0}^{T} \frac{u^{\prime}(T-s)}{u(T-s)^{2}}\left\langle\int_{s}^{T}\dot{Y}(a)da,g_{T-s }-g_{0}\right\rangle ds\\ &\quad-\frac{1}{u(T)}\int_{0}^{T}\left\langle g_{0},\dot{Y}(s) \right\rangle ds-\int_{0}^{T}\frac{1}{u(T-s)}\left\langle\dot{Y}(s),g_{T-s}-g_ {0}\right\rangle ds\\ \stackrel{{(\circ)}}{{=}}&-\frac{1}{2} \left\|g_{0}\right\|^{2}+\int_{0}^{T}\left\langle-\frac{1}{u(T)}g_{0}-\frac{1} {u(T-s)}\left(g_{T-s}-g_{0}\right)+\int_{0}^{s}\frac{u^{\prime}(T-b)}{u(T-b )^{2}}\left(g_{T-b}-g_{0}\right)db,\dot{Y}(s)\right\rangle ds.\end{split}\] (80)We use Fubini's Theorem at \((\circ)\). Finally, using (78), we obtain

\[\mathcal{V}(T)-\frac{1}{2}\left\|\nabla f(Y(T))\right\|^{2}\] (81) \[= -\frac{1}{2}\left\|g_{0}\right\|^{2}+\int_{0}^{T}\left\langle f_{ T-s},\dot{Y}(s)\right\rangle ds\] \[= -\frac{1}{2}\left\|g_{0}\right\|^{2}+\int_{0}^{T}\left\langle f_{ T-s},\int_{0}^{s}H^{A}(s,a)g_{T-a}da\right\rangle ds\] \[= -\frac{1}{2}\left\|g_{0}\right\|^{2}+\int_{0}^{T}\left\langle f_{ T-s},\int_{0}^{s}H(T-a,T-s)g_{T-a}da\right\rangle ds\] \[= -\frac{1}{2}\left\|g_{0}\right\|^{2}+\int_{0}^{T}\int_{0}^{s}H(s,a)\left\langle f_{a},g_{s}\right\rangle dads.\]

Proof of Theorem 2Define \(\mathcal{A}_{1}\) and \(\mathcal{A}_{2}\) as follows.

\[\mathcal{A}_{1} =\{\{f_{s}\}_{s\in[0,T]}|\text{Analysis in the previous paragraph holds}\},\] \[\mathcal{A}_{2} =\{\{g_{s}\}_{s\in[0,T]}|\text{Analysis in the previous paragraph holds}\}.\]

Now we prove Theorem 2. We have shown that (C3\({}^{\prime}\)) is equivalent to (76) \(\geq 0\) for all \(\{\nabla f(X(s))\}_{s\in[0,T]}\in\mathcal{A}_{1}\). By definition of \(\mathcal{A}_{1}\), it is equivalent to

\[-\frac{1}{2}\left\|g_{0}\right\|^{2}+\int_{0}^{T}\int_{0}^{s}H(s,a)\left\langle f _{a},g_{s}\right\rangle dads\geq 0\]

for any \(\{g_{s}\}_{s\in[0,T]}\in\mathcal{A}_{2}\). Moreover, by (81), it is also equivalent to

\[-\frac{1}{2}\left\|g_{0}\right\|^{2}+\int_{0}^{T}\int_{0}^{s}H(s,a)\left\langle f _{a},g_{s}\right\rangle dads=\mathcal{V}(T)-\frac{1}{2}\left\|\nabla f(Y(T)) \right\|^{2}\geq 0\]

for any \(\{g_{s}\}_{s\in[0,T]}\in\mathcal{A}_{2}\), which is (C4\({}^{\prime}\)).

### Omitted parts in Section 3.3

Regularity of (10) at \(t=-T\).To begin with, note that ODE (10) can be expressed as

\[\begin{cases}\dot{W}(t)=-\frac{2p-1}{T-t}W(t)-Cp^{2}(T-t)^{p-2}\nabla f(Y(t)) \\ \dot{Y}(t)=W(t)\end{cases}\]

for \(t\in(0,T)\). Since right hand sides are Lipschitz continuous with respect to \(W\) and \(Y\) in any closed interval \([0,s]\in[0,T)\), solution \((Y,W)\) uniquely exists that satisfies above ODE with initial condition \((Y(0),W(0))=(y_{0},0)\). Next, we give the proof of regularity of ODE (10) at terminal time \(T\) in the following order. The proof structure is based on the regularity proof in [51]:

* \(\sup\limits_{t\in[0,T)}\left\|\dot{Y}(t)\right\|\) is bounded
* \(Y(t)\) can be continuously extended to \(T\)
* \(\lim\limits_{t\to T-}\left\|\dot{Y}(t)\right\|=0\)
* \(\lim\limits_{t\to T-}\frac{\dot{Y}(t)}{(T-t)^{p-1}}=Cp\nabla f(Y(T))\).

Step (i):\(\sup\limits_{t\in[0,T)}\left\|\dot{Y}(t)\right\|\) is bounded. ODE (10) is equivalent to

\[\frac{1}{(T-s)^{p-2}}\ddot{Y}(s)+\frac{2p-1}{(T-s)^{p-1}}\dot{Y}(s)+Cp^{2} \nabla f(Y(s))=0.\] (82)By multiplying \(\dot{Y}(s)\) and integrating from \(0\) to \(t\), we obtain

\[\int_{0}^{t}\frac{1}{(T-s)^{p-2}}\left\langle\ddot{Y}(s),\dot{Y}(s)\right\rangle ds +\int_{0}^{t}\frac{2p-1}{(T-s)^{p-1}}\left\|\dot{Y}(s)\right\|^{2}ds+Cp^{2} \int_{0}^{t}\left\langle\dot{Y}(s),\nabla f(Y(s))\right\rangle ds=0\]

and integration by parts gives us

\[\frac{1}{2(T-t)^{p-2}}\left\|\dot{Y}(t)\right\|^{2}-\frac{1}{2T^{p-2}}\left\| \dot{Y}(0)\right\|^{2}+\int_{0}^{t}\frac{2p-1}{(T-s)^{p-1}}\left\|\dot{Y}(s) \right\|^{2}ds+Cp^{2}\left(f(Y(t))-f(Y(0))\right)=0.\]

Define \(\Psi(t)\colon[0,T)\to\mathbb{R}\) as

\[\Psi(t)=\frac{1}{2(T-t)^{p-2}}\left\|\dot{Y}(t)\right\|^{2}+Cp^{2}\left(f(Y(t ))-f(Y(0))\right).\]

Since

\[\dot{\Psi}(t)=-\frac{2p-1}{(T-s)^{p-1}}\left\|\dot{Y}(s)\right\|^{2}ds,\]

\(\Psi(t)\) is a nonincreasing function. Thus

\[\left\|\dot{Y}(t)\right\|^{2}=2(T-t)^{p-2}\left(\Psi(t)+Cp^{2}\left(f(Y(0))-f( Y(t))\right)\right)\leq 2T^{p-2}\left(\Psi(0)+Cp^{2}\left(f(Y(0))-f_{\star} \right)\right),\] (83)

and the right hand side of (83) is constant, which implies \(M:=\sup\limits_{t\in[0,T)}\left\|\dot{Y}(t)\right\|<\infty\).

Step (ii):\(Y(t)\) can be continuously extended to \(T\).We can prove \(Y(t)\) is uniformly continuous due to the following analysis.

\[\left\|Y(t+\delta)-Y(\delta)\right\|=\left\|\int_{t}^{t+\delta}\dot{Y}(s)ds \right\|\leq\int_{t}^{t+\delta}\left\|\dot{Y}(s)\right\|ds\leq\delta M.\]

Since a uniformly continuous function \(g\colon D\to\mathbb{R}^{d}\) can be extended continuously to \(\overline{D}\), \(Y\colon[0,T)\to\mathbb{R}^{d}\) can be extended to \([0,T]\).

Step (iii):\(\lim\limits_{t\to T-}\left\|\dot{Y}(t)\right\|=0\).We first prove the limit \(\lim\limits_{t\to T-}\left\|\dot{Y}(t)\right\|=0\) exists. From C.3, we know \(\lim\limits_{t\to T-}Y(t)\) exists and by continuity of \(f\), \(\lim\limits_{t\to T-}f(Y(t))\) also exists. Moreover, \(\Psi(t)\) is non-increasing and

\[\Psi(t)=\frac{1}{2(T-t)^{p-2}}\left\|\dot{Y}(t)\right\|^{2}+Cp^{2}\left(f(Y(t ))-f(Y(0))\right)\geq Cp^{2}\left(f_{\star}-f(Y(0))\right),\]

thus \(\lim\limits_{t\to T-}\Psi(t)\) exists. Therefore, \(\lim\limits_{t\to T-}\frac{\left\|\dot{Y}(t)\right\|^{2}}{(T-t)^{p-2}}\) exists, which implies \(\lim\limits_{t\to T-}\left\|\dot{Y}(t)\right\|=0\) when \(p>2\). For the case \(p=2\), \(\lim\limits_{t\to T-}\left\|\dot{Y}(t)\right\|\) exists, and \(\lim\limits_{t\to T-}\left\|\dot{Y}(t)\right\|=0\) follows since we have

\[\Psi(t)=\frac{1}{2T^{p}}\left\|\dot{Y}(0)\right\|^{2}-\int_{0}^{t}\frac{3}{(T- s)^{p-1}}\left\|\dot{Y}(s)\right\|^{2}ds\]

is bounded below. Thus the value of integration is finite.

Step (iv):\(\lim\limits_{t\to T-}\frac{\dot{Y}(t)}{(T-t)^{p-1}}=Cp\nabla f(Y(T))\).The (7) form of process \(Y\) is

\[\dot{Y}(t)=-\int_{0}^{t}\frac{Cp^{2}(T-t)^{2p-1}}{(T-s)^{p+1}}\nabla f(Y(s))ds.\]By dividing \((T-t)^{p-1}\) each side, we obtain

\[\frac{\dot{Y}(t)}{(T-t)^{p-1}}=-(T-t)^{p}\int_{0}^{t}\frac{Cp^{2}}{(T-s)^{p+1}} \nabla f(Y(s))ds.\]

By the result of C.3, we can apply L'Hopital's rule, which gives

\[\lim_{t\to T^{-}}\frac{\dot{Y}(t)}{(T-t)^{p-1}}=-\lim_{t\to T ^{-}}\frac{\int_{0}^{t}\frac{Cp^{2}}{(T-s)^{p+1}}\nabla f(Y(s))ds}{(T-t)^{-p}}= Cp\nabla f(Y(T)).\]

Also, since \(\nabla f\) is \(L\)-l=Lipshitz, \(\left\|\nabla f(Y(t))-\nabla f(Y(T))\right\|\leq L\left\|Y(t)-Y(T)\right\|\). Therefore,

\[\lim_{t\to T^{-}}\frac{\nabla f(Y(t))-\nabla f(Y(T))}{(T-t)^{ \beta}}=0\] (84)

for any \(\beta<p\).

Applying Theorem 2 to (10).For the case \(p=2\), refer [51]. Now we consider the case \(p>2\), with \(u(t)=Ct^{p}\) and \(v(t)=\frac{1}{C(T-t)^{p}}\). First, we verify conditions (i) and (ii) in Theorem 6. (i) holds since \(u(0)=0\), and (ii) holds since

\[\lim_{s\to T-}v(s)\left(f(Y(s))-f(Y(T))+\left\langle\nabla f(Y(T)),Y(T)-Y(s) \right\rangle\right)\] \[= \frac{1}{C}\lim_{s\to T-}\frac{f(Y(s))-f(Y(T))+\left\langle \nabla f(Y(T)),Y(T)-Y(s)\right\rangle}{(T-s)^{p}}\] \[\stackrel{{(\circ)}}{{=}} \frac{1}{C}\lim_{s\to T-}\frac{\left\langle\dot{Y}(s),\nabla f (Y(s))-\nabla f(Y(T))\right\rangle}{-p(T-s)^{p-1}}\] \[\stackrel{{(\bullet)}}{{=}} -\frac{1}{pC}\lim_{s\to T-}\left\langle\frac{\dot{Y}(s)}{(T-s)^{ p-1}},\nabla f(Y(s))-\nabla f(Y(T))\right\rangle\] \[= 0.\]

\((\circ)\) uses L'Hospital's rule and \((\bullet)\) uses the limit results at the previous paragraph.

To prove

\[\frac{1}{2}\left\|\nabla f(Y(T))\right\|^{2}\leq\frac{1}{CT^{p} }\left(f(Y(0))-f(Y(T))\right),\]

we carefully check that for any \(L\)-smooth convex function \(f\), \(\left\{\nabla f(Y(s))\right\}_{s\in[0,T]}\in\mathcal{A}_{2}\).

Verification of (80).Fubini's Theorem is used for

\[\int_{0\leq s,a\leq T}\mathbf{1}_{s\leq a}\frac{u^{\prime}(T-s)} {u(T-s)^{2}}\left\langle\dot{Y}(a),(\nabla f(Y(s))-\nabla f(Y(T)))\right\rangle\] \[= \int_{0\leq s,a\leq T}\mathbf{1}_{s\leq a}\dot{Y}(a)\left(\nabla f (Y(s))-\nabla f(Y(T))\right)\frac{p}{C(T-s)^{p+1}}.\]

The above function is continuous in \(0\leq s,a\leq T\) due to \(\lim_{t\to T^{-}}\frac{\dot{Y}(t)}{(T-t)^{p-1}}=Cp\nabla f(Y(T))\) and (84) with \(\beta=2<p\).

Verification of (79).First,

\[\lim_{t\to T-}\frac{Y(t)-Y(T)}{(T-t)^{p}}=\lim_{t\to T-}\frac{ \dot{Y}(t)}{-p(T-t)^{p-1}}=-C\nabla f(Y(T))\]

holds from (iv). Thus \(\sup_{b\in(0,T]}\left\|\frac{Y(T-b)-Y(T)}{b^{p}}\right\|<\infty\). Now we will show

\[\lim_{a\to+0}a^{1+\epsilon}f_{a}=0,\quad\forall\,\epsilon>0.\]

[MISSING_PAGE_FAIL:41]

[MISSING_PAGE_EMPTY:42]

Here, we can make a crucial observation.

**The formulation of (90) is same with (2), which all \(\llbracket\cdot,\cdot\rrbracket\) are changed into \((\cdot,\cdot)^{1}\) and \(u_{i}=T_{i}\)**. (\(\diamond\))

Moreover, we can express the recursive formula of the \(H\) matrix of (88) as

\[h_{k+1,i}=\begin{cases}1+\frac{(t_{k}-1)t_{k+1}}{T_{k+1}}&i=k\\ \frac{(T_{k}-t_{k})t_{k+1}}{t_{k}T_{k+1}}\left(h_{k,k-1}-1\right)&i=k-1\\ \frac{(T_{k}-t_{k})t_{k+1}}{t_{k}T_{k+1}}h_{k,i}&i=0,\ldots,k-2\end{cases}.\] (92)

As we used a parameterized family in the convex function (see Section 2.5), we will use these convergence results to construct the method for minimizing \(\min_{v\in\partial F(y_{N}^{\oplus,\alpha})}\left\|v\right\|^{2}\).

**Relationship between minimizing \(\frac{\alpha^{2}}{2L}\left\|y_{N}^{\oplus,\alpha}-y_{N}\right\|^{2}\) and minimizing \(\min_{v\in\partial F(y_{N}^{\oplus,\alpha})}\left\|v\right\|^{2}\).** Note that \(L\alpha(y_{N}^{\oplus,\alpha}-y_{N})+\nabla f(y_{N})+u=0\), \(u\in\partial g(y_{N}^{\oplus,\alpha})\), which gives

\[\min_{v\in\partial F(y_{N}^{\oplus,\alpha})}\left\|v\right\|^{2}\underset{( \bullet)}{\leq}\left(\left\|\nabla f(y_{N})-\nabla f(y_{N}^{\oplus,\alpha}) \right\|+L\alpha\left\|y_{N}-y_{N}^{\oplus,\alpha}\right\|\right)^{2} \underset{(\circ)}{\leq}L^{2}(\alpha+1)^{2}\left\|y_{N}-y_{N}^{\oplus,\alpha} \right\|^{2}.\] (93)

(\(\bullet\)) is triangle inequality and (\(\circ\)) comes from the \(L\)-smoothness of \(f\).

### Proof of Theorem 3

In this section, we give the proof outline of Theorem 3 and discuss the construction of matrix \(C\).

Proof outline of Theorem 3.: We begin by proposing a parameterized family reduces \(\frac{1}{2L}\left\|y_{N}^{\oplus,\alpha}-y_{N}\right\|^{2}\) under a fixed value of \(a>0\). To construct this family, take \(\{t_{i}\}_{i=0}^{N},\,T_{i}=\sum_{j=0}^{i}t_{j}\) that satisfies (96). We define the \(H\) matrix of the family as \(\left(\frac{1}{\alpha}H_{0}+\frac{1}{\alpha^{2}}C\right)^{A}\), where \(H_{0}\) has the same formulation as the \(H\) matrix of GFGPM (88) and \(C\) follows the recursive formula (97). We refer to this family as (SFG-family).

We can prove that (SFG-family) exhibits the convergence rate

\[\frac{\alpha L}{2}\|y_{N}^{\oplus,\alpha}-y_{N}\|^{2}\leq\frac{1}{T_{N}}\left( F(y_{0})-F_{\star}\right),\] (94)

which is motivated by the observation (\(\circ\)). In parallel with (\(\circ\)), we consider the energy function

\[\mathcal{V}_{k}=\frac{F(y_{0})-F(y_{N}^{\oplus,\alpha})+(y_{-1},y_{0})^{ \alpha}}{T_{N}}+\sum_{i=0}^{k-1}\frac{1}{T_{N-i-1}}(y_{i},y_{i+1})^{\alpha}+ \sum_{i=0}^{k-1}\left(\frac{1}{T_{N-i-1}}-\frac{1}{T_{N-i}}\right)(y_{N},y_{i} )^{\alpha}\]

for \(k=0,\ldots,N\), which \([y_{N},\star]\) changed into \([y_{-1},y_{0}]^{\alpha}\), all other \([\![\cdot,\cdot]\!]\) terms changed into \((\![\cdot,\cdot]\!)^{\alpha}\) and \(v_{i}=\frac{1}{T_{N-i}}\).

(94) follows from the fact that \(\{\mathcal{V}_{i}\}_{i=0}^{N}\) is dissipative and

\[\frac{\alpha L}{2}\|y_{N}^{\oplus,\alpha}-y_{N}\|^{2}\leq\mathcal{V}_{N}.\]

Detailed justification is given in Appendix D.3.

For the next step, we show that (SFG) is an instance of (SFG-family), under the choice of

\[\alpha=4,\quad T_{i}=\frac{(i+2)(i+3)}{4},\quad 0\leq i\leq N.\]

The detailed derivation is given in Appendix D.4.

Finally, combining the convergence result

\[2L\|y_{N}^{\oplus,4}-y_{N}\|^{2}\leq\mathcal{V}_{N}\leq\mathcal{V}_{0}\leq\frac{ 4}{(N+2)(N+3)}\left(F(y_{0})-F_{\star}\right)\]

and (93) gives

\[\min_{v\in\partial F(y_{N}^{\oplus,4})}\|v\|^{2}\leq 25L^{2}\left\|y_{N}^{ \oplus,4}-y_{N}\right\|^{2}\leq\frac{50}{(N+2)(N+3)}\left(F(y_{0})-F_{\star} \right).\]

Construction of matrix \(C\)Define

\[g\colon\ L\alpha\big{[}y_{0}-y_{0}^{\oplus,\alpha}|\ldots|y_{N}-y_{N}^{\oplus, \alpha}\big{]}.\]

For an FSFOM (87) with matrix \(H_{1}\), we have \(\mathcal{V}_{N}-\frac{\alpha L}{2}\|y_{N}^{\oplus,\alpha}-y_{N}\|^{2}=\frac{ 1}{2L\alpha^{2}}\operatorname{Tr}\left(g^{\intercal}g\mathcal{T}^{\oplus}\right)\) with

\[\mathcal{T}^{\oplus}\colon= \underbrace{\alpha^{2}\left[\sum_{i=0}^{N}\frac{1}{T_{N-i}} \left(\mathcal{H}_{1}^{\intercal}(\mathbf{e}_{i}+\cdots+\mathbf{e}_{N})( \mathbf{e}_{i}-\mathbf{e}_{i-1})^{\intercal}+(\mathbf{e}_{i}-\mathbf{e}_{i-1} )(\mathbf{e}_{i}+\cdots+\mathbf{e}_{N})^{\intercal}\mathcal{H}_{1}\right) \right]}_{\mathcal{A}_{3}}\] \[+\underbrace{\alpha\left[\sum_{i=0}^{N}\frac{1}{T_{N-i}}\left(( \mathbf{e}_{i-1}-\mathbf{e}_{i})(\mathbf{e}_{i-1}-\mathbf{e}_{N})^{\intercal} +(\mathbf{e}_{i-1}-\mathbf{e}_{N})(\mathbf{e}_{i-1}-\mathbf{e}_{i})^{\intercal }\right)-\frac{1}{T_{N}}\mathbf{e}_{0}\mathbf{e}_{0}^{\intercal}-\mathbf{e}_{N }\mathbf{e}_{N}^{\intercal}\right]}_{\mathcal{B}_{3}}\] \[+\left[\frac{\alpha}{T_{N}}\mathbf{e}_{0}\mathbf{e}_{0}^{ \intercal}-\sum_{i=0}^{N-1}\frac{1}{T_{N-1-i}}\mathbf{e}_{i}\mathbf{e}_{i}^{ \intercal}-\frac{1}{T_{0}}\mathbf{e}_{N}\mathbf{e}_{N}^{\intercal}\right]\]

where \(e_{-1}=\mathbf{0}\), \(e_{i}\) is a unit vector which \((i+1)-th\) component is \(1\) and \(\mathbb{R}^{N+1}\) and \(\mathcal{H}_{1}\colon=\begin{bmatrix}0&0\\ H_{1}&0\end{bmatrix}\). If we take \(H_{1}\) that makes \(\mathcal{T}^{\oplus}\succeq 0\), \(\mathcal{V}_{N}\geq\frac{\alpha L}{2}\|y_{N}^{\oplus,\alpha}-y_{N}\|^{2}\) follows. Next we observe \(\frac{1}{\alpha^{2}}\mathcal{A}_{3}+\frac{1}{\alpha}\mathcal{B}_{3}=\mathcal{ T}(H_{1},v)\) for \(v_{i}=\frac{1}{T_{N-i}}\) which is defined as (22). By expanding (91) and defining \(g^{\prime}\colon=L\big{[}x_{0}-x_{0}^{\oplus,\alpha}|\ldots|x_{N}-x_{N}^{\oplus,\alpha}\big{]}\),

\[\mathcal{U}_{N}-T_{N}\left(F(x_{N}^{\oplus,1})-F_{\star}\right)-\frac{L}{2} \left\|x_{0}-x_{\star}+\sum_{i=0}^{N}(u_{i}-u_{i-1})(x_{i}-x_{i}^{\oplus,1}) \right\|^{2}=\operatorname{Tr}\left(g^{\prime}\left(g^{\prime}\right)^{ \intercal}\mathcal{S}^{\oplus}\right)\]

follows where

\[\mathcal{S}^{\oplus}\colon=\mathcal{S}(H_{0},u)-\sum_{i=0}^{N-1}(T_{i}-t_{i}^{2 })\mathbf{e}_{i}\mathbf{e}_{i}^{\intercal}.\]

Here, \(u_{i}=T_{i}\), and \(\mathcal{S}(H_{0},u)\) is given by the formulation (19). Using (91), we obtain

\[\mathcal{S}(H_{0},u)=\sum_{i=0}^{N-1}(2T_{i}-t_{i}^{2})\mathbf{e}_{i}\mathbf{e }_{i}^{\intercal}+(T_{N}-t_{N}^{2})\mathbf{e}_{N}\mathbf{e}_{N}^{\intercal}.\]

Now recall matrix \(\mathcal{M}(u)\) (26) and the result of Theorem 1:

\[\mathcal{S}(H_{0},u)=\mathcal{M}(u)^{\intercal}\mathcal{T}(H_{0}^{A},v) \mathcal{M}(u).\] (95)

By substituting \(H_{1}=\frac{1}{\alpha}H_{0}^{A}+X\) and using (95), the result is

\[\mathcal{M}(u)^{\intercal}\mathcal{T}^{\oplus}\mathcal{M}(u)= \alpha\Bigg{[}\sum_{i=0}^{N-1}(2T_{i}-t_{i}^{2})\mathbf{e}_{i} \mathbf{e}_{i}^{\intercal}+(T_{N}-t_{N}^{2})\mathbf{e}_{N}\mathbf{e}_{N}^{ \intercal}\Bigg{]}\] \[+\underbrace{\alpha^{2}\mathcal{M}(u)^{\intercal}\left[\sum_{i=0 }^{N}\frac{1}{T_{N-i}}\left(\mathcal{X}^{\intercal}(\mathbf{e}_{i}+\cdots+ \mathbf{e}_{N})(\mathbf{e}_{i}-\mathbf{e}_{i-1})^{\intercal}+(\mathbf{e}_{i}- \mathbf{e}_{i-1})(\mathbf{e}_{i}+\cdots+\mathbf{e}_{N})^{\intercal}\mathcal{X }\right)\right]\mathcal{M}(u)}_{\mathcal{A}_{4}}\] \[+\underbrace{\mathcal{M}(u)^{\intercal}\left[\frac{\alpha}{T_{N}} \mathbf{e}_{0}\mathbf{e}_{0}^{\intercal}-\sum_{i=0}^{N-1}\frac{1}{T_{N-1-i}} \mathbf{e}_{i}\mathbf{e}_{i}^{\intercal}-\frac{1}{T_{0}}\mathbf{e}_{N}\mathbf{e}_ {N}^{\intercal}\right]\mathcal{M}(u)}_{\mathcal{C}_{3}}\]where \(\mathcal{X}:\;=\begin{bmatrix}0&0\\ X&0\end{bmatrix}\).

Next, consider \(\mathcal{A}_{4}\) as a function of the lower triangular matrix \(\mathcal{X}\). The key observation is that if we choose \(\mathcal{X}\) appropriately, all non-diagonal terms of \(\mathcal{C}_{3}\) can be eliminated. With this choice of \(\mathcal{X}\), we have

\[\mathcal{T}^{\oplus}=\alpha\sum_{i=0}^{N}(2T_{i}-t_{i}^{2})\mathbf{e}_{i} \mathbf{e}_{i}^{\intercal}-T_{0}\mathbf{e}_{0}\mathbf{e}_{0}^{\intercal}-\sum _{i=1}^{N}\left(\frac{T_{i}^{2}}{T_{i-1}}+t_{i}^{2}\sum_{j=0}^{i-2}\frac{1}{T_ {j}}\right)\mathbf{e}_{i}\mathbf{e}_{i}^{\intercal}.\]

Note that \(\mathcal{T}^{\oplus}\) contains only diagonal terms. Therefore, \(\mathcal{T}^{\oplus}\succeq 0\) is equivalent to all coefficients of \(\mathbf{e}_{i}\mathbf{e}_{i}^{\intercal}\) being nonnegative, which can be formulated using \(t_{i}\) and \(T_{i}\).

Remark.In the proof of Theorem 1 ( \(\mathbf{B}_{2}\) and \(\mathbf{B}_{1}\) at (27) and (28)), we have shown that

\[\mathcal{A}_{4}=\alpha^{2}\bigg{[}\sum_{i=0}^{N}\left(\mathcal{X}^{A}\right)^ {\intercal}T_{i}(\mathbf{e}_{i}-\mathbf{e}_{i+1})(\mathbf{e}_{0}+\cdots+ \mathbf{e}_{i})^{\intercal}+T_{i}(\mathbf{e}_{0}+\cdots+\mathbf{e}_{i})( \mathbf{e}_{i}-\mathbf{e}_{i+1})^{\intercal}\mathcal{X}^{A}\bigg{]}.\]

Observe that \((\mathbf{e}_{i}-\mathbf{e}_{i+1})(\mathbf{e}_{0}+\cdots+\mathbf{e}_{i})^{\intercal}\) is a lower triangular matrix and \(\mathcal{X}^{A}\) is a strictly lower triangular matrix, i.e., all diagonal component is \(0\). Thus it is worth noting that \(\mathcal{A}_{4}\) cannot induce any diagonal term \(\mathbf{e}_{i}\mathbf{e}_{i}^{\intercal}\), choosing matrix \(C\) as the optimal one.

### Parameterized family reduces the gradient mapping norm

FSFOM that reduces \(\frac{1}{2L}\left\|y_{N}^{\oplus,\alpha}-y_{N}\right\|^{2}\) in the composite minimization.We propose the parameterized family that reduces \(\frac{1}{2L}\left\|y_{N}^{\oplus,\alpha}-y_{N}\right\|^{2}\). For fixed \(a>0\), take \(\{t_{i}\}_{i=0}^{N}\), \(T_{i}=\sum_{j=0}^{i}t_{j}\) that satisfies the following conditions:

\[\alpha(2T_{0}-t_{0}^{2}) \geq T_{0},\] (96) \[\alpha(2T_{k}-t_{k}^{2}) \geq\frac{T_{k}^{2}}{T_{k-1}}+t_{k}^{2}\left(\frac{1}{T_{0}}+\sum _{i=0}^{k-2}\frac{1}{T_{i}}\right)\quad k=1,\ldots,N\]

where we define \(\sum_{i=0}^{k-2}\frac{1}{T_{i}}=0\) for \(k=1\) case. We define an lower triangular matrix \(C=\{c_{k,i}\}_{0\leq i<k\leq N}\) as

\[c_{k+1,i}=\begin{cases}\frac{t_{k}}{T_{1}}&i=0,\ k=0\\ \frac{t_{k+1}}{T_{k+1}}\left(\frac{t_{k}}{T_{0}}+t_{k}\sum\limits_{j=0}^{k-2 }\frac{1}{T_{j}}+\frac{T_{k}}{T_{k-1}}\right)&i=k,\ k=1,\ldots,N-1\\ \frac{t_{k+1}(T_{k}-t_{k})}{t_{k}T_{k+1}}c_{k,i}&i=0,\ldots,k-1,\ i=2,\ldots, N-1\end{cases}.\] (97)

Matrix \(C\) has a crucial role in the correspondence between composite function value minimization and composite function gradient norm minimization with \(H\rightarrow\left(\frac{1}{\alpha}H+\frac{1}{\alpha^{2}}C\right)^{A}\).

**Proposition 5** (SFG family).: Assume that we choose \(H_{0}\) matrix of (88), and define \(C\) as (97). Consider an FSFOM (87) which \(H\) matrix is \(\left(\frac{1}{\alpha}H_{0}+\frac{1}{\alpha^{2}}C\right)^{A}\). Such FSFOM can be expressed as

\[y_{k+1}=y_{k}^{\oplus,\alpha}+\beta_{k}^{\prime}\left(y_{k}^{ \oplus,\alpha}-y_{k-1}^{\oplus,\alpha}\right)+\gamma_{k}^{\prime}\left(y_{k}^ {\oplus,\alpha}-y_{k}\right),\quad k=0,\ldots,N-1,\] \[\beta_{k}^{\prime}=\frac{\beta_{N-k}(\beta_{N-1-k}+\gamma_{N-1-k })}{\beta_{N-k}+\gamma_{N-k}},\qquad\gamma_{k}^{\prime}=\frac{\gamma_{N-k}( \beta_{N-1-k}+\gamma_{N-1-k})}{\beta_{N-k}+\gamma_{N-k}},\] (SFG-family) \[\beta_{k}=\frac{t_{k+1}(T_{k}-t_{k})}{t_{k}T_{k+1}},\qquad\gamma_{ k}=\frac{t_{k+1}(t_{k}^{2}-T_{k})}{t_{k}T_{k+1}}+\frac{1}{\alpha}c_{k+1,k}.\]

and \(y_{-1}^{\oplus,\alpha}=y_{0}\). Then it exhibits the convergence rate

\[\frac{\alpha L}{2}\|y_{N}^{\oplus}-y_{N}\|^{2}\leq\frac{1}{T_{N}}\left(F(y_{0}) -F_{\star}\right).\]

[MISSING_PAGE_EMPTY:46]

Plugging the above equalities provides

\[2L\alpha^{2}\left(\mathcal{V}_{N}-\frac{\alpha L}{2}\left\|y_{N}^{ \oplus,\alpha}-y_{N}\right\|^{2}\right)\] \[=2L\alpha^{2}\left[\sum_{i=0}^{N-1}\frac{1}{T_{N-i-1}}\left\langle g _{i+1},y_{i}-y_{i+1}\right\rangle+\sum_{i=0}^{N-1}\left(\frac{1}{T_{N-i-1}}- \frac{1}{T_{N-i}}\right)\left\langle g_{i},y_{N}-y_{i}\right\rangle\right]+ \frac{2\alpha-1}{T_{N}}\|g_{0}\|^{2}-\alpha\left\|g_{N}\right\|^{2}\] \[\quad+\sum_{i=0}^{N-1}\frac{1}{T_{N-i-1}}\left(2\alpha\left\langle g _{i+1},g_{i+1}-g_{i}\right\rangle-\|g_{i+1}\|^{2}\right)+\sum_{i=0}^{N-1} \left(\frac{1}{T_{N-i-1}}-\frac{1}{T_{N-i}}\right)\left(2\alpha\left\langle g _{i},g_{i}-g_{N}\right\rangle-\|g_{i}\|^{2}\right)\] \[=2L\alpha^{2}\left(\sum_{i=0}^{N-1}\frac{1}{T_{N-i-1}}\left\langle g _{i+1},y_{i}-y_{i+1}\right\rangle+\sum_{i=0}^{N-1}\left(\frac{1}{T_{N-i-1}}- \frac{1}{T_{N-i}}\right)\left\langle g_{i},y_{N}-y_{i}\right\rangle\right)\] \[\quad+\left[\sum_{i=0}^{N-1}\frac{\alpha}{T_{N-i-1}}\|g_{i+1}-g_{ i}\|^{2}+\sum_{i=0}^{N-1}\left(\frac{\alpha}{T_{N-i-1}}-\frac{\alpha}{T_{N-i}} \right)\|g_{i}-g_{N}\|^{2}-a\|g_{N}\|^{2}+\frac{\alpha}{T_{N}}\|g_{N}\|^{2}\right]\] \[\quad+\sum_{i=0}^{N-1}\frac{1}{T_{N-i-1}}\left(-\alpha\|g_{i}\|^ {2}+(a-1)\|g_{i+1}\|^{2}\right)+\sum_{i=0}^{N-1}\left(\frac{1}{T_{N-i-1}}- \frac{1}{T_{N-i}}\right)\left(-\alpha\|g_{N}\|^{2}+(\alpha-1)\|g_{i}\|^{2}\right)\] \[\quad+\frac{2\alpha-1}{T_{N}}\|g_{0}\|^{2}-\frac{\alpha}{T_{N}}\| g_{N}\|^{2}\] \[=2L\alpha^{2}\left(\sum_{i=0}^{N-1}\frac{1}{T_{N-i-1}}\left\langle g _{i+1},y_{i}-y_{i+1}\right\rangle+\sum_{i=0}^{N-1}\left(\frac{1}{T_{N-i-1}}- \frac{1}{T_{N-i}}\right)\left\langle g_{i},y_{N}-y_{i}\right\rangle\right)\] \[\quad+\left[\sum_{i=0}^{N-1}\frac{\alpha}{T_{N-i-1}}\|g_{i+1}-g_{ i}\|^{2}+\sum_{i=0}^{N-1}\left(\frac{\alpha}{T_{N-i-1}}-\frac{\alpha}{T_{N-i}} \right)\|g_{i}-g_{N}\|^{2}-\alpha\|g_{N}\|^{2}+\frac{\alpha}{T_{N}}\|g_{N}\|^{ 2}\right]\] \[\quad+\frac{\alpha}{T_{N}}\left\|g_{0}\right\|^{2}-\sum_{i=0}^{N- 1}\frac{1}{T_{N-1-i}}\|g_{i}\|^{2}-\frac{1}{T_{0}}\left\|g_{N}\right\|^{2}.\]

Next, define \(g\colon=[g_{0}|g_{1}\ldots|g_{N}]\) and

\[\mathcal{H}_{1}\colon=\begin{bmatrix}0&0\\ \left(\frac{1}{\alpha}H_{0}+\frac{1}{\alpha^{2}}C\right)^{A}&0\end{bmatrix}, \quad\mathcal{H}_{0}\colon=\begin{bmatrix}0&0\\ H_{0}&0\end{bmatrix}.\]

By the same procedure as the proof of Theorem 1, we obtain \(2L\alpha^{2}\left(\mathcal{V}_{N}-\frac{\alpha L}{2}\left\|y_{N}^{\oplus, \alpha}-y_{N}\right\|^{2}\right)=\operatorname{Tr}\left(g^{\intercal}g\mathcal{ T}^{\oplus}\right)\) where

\[\mathcal{T}^{\oplus}= \alpha^{2}\left[\sum_{i=0}^{N}\frac{1}{T_{N-i}}\left(\mathcal{H }_{1}^{\intercal}(\mathbf{f}_{i}+\cdots+\mathbf{f}_{N})(\mathbf{f}_{i}- \mathbf{f}_{i-1})^{\intercal}+(\mathbf{f}_{i}-\mathbf{f}_{i-1})(\mathbf{f}_{i }+\cdots+\mathbf{f}_{N})^{\intercal}\mathcal{H}_{1}\right)\right]\] \[+\alpha\left[\sum_{i=0}^{N}\frac{1}{T_{N-i}}\left((\mathbf{f}_{i- 1}-\mathbf{f}_{i})(\mathbf{f}_{i-1}-\mathbf{f}_{N})^{\intercal}+(\mathbf{f}_{i- 1}-\mathbf{f}_{N})(\mathbf{f}_{i-1}-\mathbf{f}_{i})^{\intercal}\right)-\frac{1}{ T_{N}}\mathbf{f}_{0}\mathbf{f}_{0}^{\intercal}-\mathbf{f}_{N}\mathbf{f}_{N}^{ \intercal}\right]\] \[+\left[\frac{\alpha}{T_{N}}\mathbf{f}_{0}\mathbf{f}_{0}^{ \intercal}-\sum_{i=0}^{N-1}\frac{1}{T_{N-1-i}}\mathbf{f}_{i}\mathbf{f}_{i}^{ \intercal}-\frac{1}{T_{0}}\mathbf{f}_{N}\mathbf{f}_{N}^{\intercal}\right]\]

And consider the vector \(\mathbf{e}_{i}\in\mathbb{R}^{(N+1)\times 1}\) which is same with (29).

In the proof of Theorem 1\(\left(\eqref{eq:2},\eqref{eq:2}\right)\), we have shown that

\[\frac{1}{2L}\sum_{i=0}^{N}\frac{1}{T_{N-i}}\left(\mathcal{H}_{1}^ {\intercal}(\mathbf{f}_{i}+\cdots+\mathbf{f}_{N})(\mathbf{f}_{i}-\mathbf{f}_{i- 1})^{\intercal}+(\mathbf{f}_{i}-\mathbf{f}_{i-1})(\mathbf{f}_{i}+\cdots+ \mathbf{f}_{N})^{\intercal}\mathcal{H}_{1}\right)\] \[= \frac{1}{2L}\left(\frac{\mathcal{H}_{0}^{\intercal}}{\alpha}+ \frac{\mathcal{C}^{\intercal}}{\alpha^{2}}\right)\left[\sum_{i=0}^{N}T_{i}( \mathbf{e}_{0}+\cdots+\mathbf{e}_{i})(\mathbf{e}_{i}-\mathbf{e}_{i+1})^{\intercal }\right]+\frac{1}{2L}\left[\sum_{i=0}^{N}T_{i}(\mathbf{e}_{i}-\mathbf{e}_{i+ 1})(\mathbf{e}_{0}+\cdots+\mathbf{e}_{i})^{\intercal}\right]\left(\frac{ \mathcal{H}_{0}}{\alpha}+\frac{\and

\[\frac{1}{2L}\sum_{i=0}^{N}\frac{1}{T_{N-i}}\left((\mathbf{f}_{i-1}- \mathbf{f}_{i})(\mathbf{f}_{i-1}-\mathbf{f}_{N})^{\intercal}+(\mathbf{f}_{i-1}- \mathbf{f}_{N})(\mathbf{f}_{i-1}-\mathbf{f}_{i})^{\intercal}\right)-\frac{1}{2T _{N}L}\mathbf{f}_{0}\mathbf{f}_{0}^{\intercal}-\frac{1}{2L}\mathbf{f}_{N} \mathbf{f}_{N}^{\intercal}\] \[= -\frac{1}{2L}\left(\sum_{i=0}^{N}(T_{i}-T_{i-1})\mathbf{e}_{i} \right)\left(\sum_{i=0}^{N}(T_{i}-T_{i-1})\mathbf{e}_{i}\right)^{\intercal}\] \[\quad+\frac{1}{2L}\left[\sum_{i=0}^{N}T_{i}\left((\mathbf{e}_{i}- \mathbf{e}_{i+1})\mathbf{e}_{i}^{\intercal}+\mathbf{e}_{i}(\mathbf{e}_{i}- \mathbf{e}_{i+1})^{\intercal}\right)-T_{N}\mathbf{e}_{N}\mathbf{e}_{N}^{ \intercal}\right]\]

under the above transformation. Therefore, \(\mathcal{T}^{\oplus}\) can be expressed as

\[\mathcal{T}^{\oplus} =\alpha^{2}\left(\left(\frac{\mathcal{H}_{0}^{\intercal}}{ \alpha}+\frac{\mathcal{C}^{\intercal}}{\alpha^{2}}\right)\left[\sum_{i=0}^{N }T_{i}(\mathbf{e}_{0}+\cdots+\mathbf{e}_{i})(\mathbf{e}_{i}-\mathbf{e}_{i+1}) ^{\intercal}\right]+\left[\sum_{i=0}^{N}T_{i}(\mathbf{e}_{i}-\mathbf{e}_{i+1} )(\mathbf{e}_{0}+\cdots+\mathbf{e}_{i})^{\intercal}\right]\left(\frac{ \mathcal{H}_{0}}{\alpha}+\frac{\mathcal{C}}{\alpha^{2}}\right)\right)\] \[+\alpha\left(-\left(\sum_{i=0}^{N}(T_{i}-T_{i-1})\mathbf{e}_{i} \right)\left(\sum_{i=0}^{N}(T_{i}-T_{i-1})\mathbf{e}_{i}\right)^{\intercal}+ \left[\sum_{i=0}^{N}T_{i}\left((\mathbf{e}_{i}-\mathbf{e}_{i+1})\mathbf{e}_{i }^{\intercal}+\mathbf{e}_{i}(\mathbf{e}_{i}-\mathbf{e}_{i+1})^{\intercal} \right)-T_{N}\mathbf{e}_{N}\mathbf{e}_{N}^{\intercal}\right]\right)\] \[+\left[\frac{\alpha}{T_{N}}\mathbf{f}_{0}\mathbf{f}_{0}^{ \intercal}-\sum_{i=0}^{N-1}\frac{1}{T_{N-1-i}}\mathbf{f}_{i}\mathbf{f}_{i}^{ \intercal}-\frac{1}{T_{0}}\mathbf{f}_{N}\mathbf{f}_{N}^{\intercal}\right]\] \[=\alpha\mathcal{A}+\mathcal{B}+\frac{\alpha}{T_{N}}\mathbf{f}_{0} \mathbf{f}_{0}^{\intercal}\]

where

\[\mathcal{A} =\mathcal{H}_{0}^{\intercal}\left[\sum_{i=0}^{N}T_{i}(\mathbf{e} _{0}+\cdots+\mathbf{e}_{i})(\mathbf{e}_{i}-\mathbf{e}_{i+1})^{\intercal} \right]+\left[\sum_{i=0}^{N}T_{i}(\mathbf{e}_{i}-\mathbf{e}_{i+1})(\mathbf{e} _{0}+\cdots+\mathbf{e}_{i})^{\intercal}\right]\mathcal{H}_{0}\] \[-\left(\sum_{i=0}^{N}(T_{i}-T_{i-1})\mathbf{e}_{i}\right)\left( \sum_{i=0}^{N}(T_{i}-T_{i-1})\mathbf{e}_{i}\right)^{\intercal}+\left[\sum_{i=0} ^{N}T_{i}\left((\mathbf{e}_{i}-\mathbf{e}_{i+1})\mathbf{e}_{i}^{\intercal}+ \mathbf{e}_{i}(\mathbf{e}_{i}-\mathbf{e}_{i+1})^{\intercal}\right)-T_{N} \mathbf{e}_{N}\mathbf{e}_{N}^{\intercal}\right]\]

and

\[\mathcal{B}= \mathcal{C}^{\intercal}\left[\sum_{i=0}^{N}T_{i}(\mathbf{e}_{0}+ \cdots+\mathbf{e}_{i})(\mathbf{e}_{i}-\mathbf{e}_{i+1})^{\intercal}\right]+ \left[\sum_{i=0}^{N}T_{i}(\mathbf{e}_{i}-\mathbf{e}_{i+1})(\mathbf{e}_{0}+ \cdots+\mathbf{e}_{i})^{\intercal}\right]\mathcal{C}\] \[-\sum_{i=0}^{N-1}\frac{1}{T_{N-1-i}}\mathbf{f}_{i}\mathbf{f}_{i}^ {\intercal}-\frac{1}{T_{0}}\mathbf{f}_{N}\mathbf{f}_{N}^{\intercal}.\]

To calculate \(\mathcal{A}\), expand the energy function (91). Then we obtain

\[\mathcal{A}=\sum_{i=0}^{N-1}(2T_{i}-t_{i}^{2})\mathbf{e}_{i}\mathbf{e}_{i}^{ \intercal}+(T_{N}-t_{N}^{2})\mathbf{e}_{N}\mathbf{e}_{N}^{\intercal}.\]

Moreover, by Claim 1,

\[\mathcal{B}=-T_{0}\mathbf{e}_{0}\mathbf{e}_{0}^{\intercal}-\sum_{i=1}^{N}\left( \frac{T_{i}^{2}}{T_{i-1}}+t_{i}^{2}\sum_{j=0}^{i-2}\frac{1}{T_{j}}\right) \mathbf{e}_{i}\mathbf{e}_{i}^{\intercal}.\]

Combining above results and \(T_{N}\mathbf{e}_{N}=\mathbf{f}_{0}\), we achieve

\[\mathcal{T}^{\oplus}=\alpha\sum_{i=0}^{N}(2T_{i}-t_{i}^{2})\mathbf{e}_{i} \mathbf{e}_{i}^{\intercal}-T_{0}\mathbf{e}_{0}\mathbf{e}_{0}^{\intercal}-\sum_{i= 1}^{N}\left(\frac{T_{i}^{2}}{T_{i-1}}+t_{i}^{2}\sum_{j=0}^{i-2}\frac{1}{T_{j}} \right)\mathbf{e}_{i}\mathbf{e}_{i}^{\intercal}\]

and the condition (96) makes each coefficient of \(\mathbf{e}_{i}\mathbf{e}_{i}^{\intercal}\) nonnegative, which gives \(\mathcal{T}\succeq 0\). To achieve the iteration formula (SFG-family), consider the FSFOM (87) with \(\frac{1}{\alpha}H_{0}+\frac{1}{\alpha^{2}}C\), which can be expressed as

\[x_{k+1}=x_{k}^{\oplus,\alpha}+\beta_{k}\left(x_{k}^{\oplus,\alpha}-x _{k-1}^{\oplus,\alpha}\right)+\gamma_{k}\left(x_{k}^{\oplus,\alpha}-x_{k} \right),\quad k=0,\ldots,N-1,\] \[\beta_{k}=\frac{t_{k+1}(T_{k}-t_{k})}{t_{k}T_{k+1}},\quad\gamma_{ k}=\frac{t_{k+1}(t_{k}^{2}-T_{k})}{t_{k}T_{k+1}}+\frac{1}{\alpha}c_{k+1,k}.\]

For last step, We make an observation that Proposition 1 can be applied when all \(z^{+}\) terms changed into \(z^{\oplus,\alpha}\). Proposition 1 gives the H-dual. 

Proof of Claim 1.: The left hand side of (98) is

\[2\left(\sum_{i=0}^{N}T_{i}(\mathbf{e}_{i}-\mathbf{e}_{i+1})( \mathbf{e}_{0}+\cdots+\mathbf{e}_{i})^{\intercal}\right)\mathcal{C}\] \[= 2\left(\sum_{i=0}^{N}T_{i}(\mathbf{e}_{i}-\mathbf{e}_{i+1})( \mathbf{e}_{0}+\cdots+\mathbf{e}_{i})^{\intercal}\right)\left(\sum_{k>i}c_{k, i}\mathbf{e}_{k}\mathbf{e}_{i}^{\intercal}\right)\] \[= 2\left(\sum_{i=0}^{N}(\mathbf{f}_{N-i}-\mathbf{f}_{N-i+1})( \mathbf{e}_{0}+\cdots+\mathbf{e}_{i})^{\intercal}\right)\left(\sum_{k>i}c_{k, i}\mathbf{e}_{k}\mathbf{e}_{i}^{\intercal}\right)\] \[= 2\left(\sum_{i=0}^{N}\mathbf{f}_{N-i}\mathbf{e}_{i}^{\intercal} \right)\left(\sum_{k>i}c_{k,i}\mathbf{e}_{k}\mathbf{e}_{i}^{\intercal}\right)\] \[= 2\sum_{i=0}^{N}\left(\sum_{k=i+1}^{N}c_{k,i}\mathbf{f}_{N-k} \right)\mathbf{e}_{i}^{\intercal}\]

Now we calculate the right-hand side of (98). By plugging

\[\mathbf{f}_{N-i}=T_{i}\mathbf{e}_{i}+t_{i+1}\mathbf{e}_{i+1}+\cdots+t_{N} \mathbf{e}_{N},\]

\[\sum_{i=1}^{N}\frac{1}{T_{i-1}}\mathbf{f}_{N-i}\mathbf{f}_{N-i}^{\intercal}+ \frac{1}{T_{0}}\mathbf{f}_{N}\mathbf{f}_{N}^{\intercal}-T_{0}\mathbf{e}_{0} \mathbf{e}_{0}^{\intercal}-\sum_{i=1}^{N}\left(\frac{T_{i}^{2}}{T_{i-1}}+t_{i} ^{2}\sum_{j=0}^{i-2}\frac{1}{T_{j}}\right)\mathbf{e}_{i}\mathbf{e}_{i}^{ \intercal}=2\sum_{i=0}^{N-1}\mathbf{a}_{i}\mathbf{e}_{i}^{\intercal}\]

where

\[\mathbf{a}_{0} =t_{1}\mathbf{e}_{1}+\cdots+t_{N}\mathbf{e}_{N},\] \[\mathbf{a}_{i} =\left(\frac{t_{i}}{T_{0}}+\sum_{j=0}^{i-2}\frac{t_{i}}{T_{j}}+ \frac{T_{i}}{T_{i-1}}\right)\left(t_{i+1}\mathbf{e}_{i+1}+\cdots+t_{N} \mathbf{e}_{N}\right),\quad i=1,\ldots,N-1.\]

Now we claim that \(\mathbf{a}_{i}=\sum_{k=i+1}^{N}c_{k,i}\mathbf{f}_{N-k}\) for \(i=0,\ldots,N-1\). Note that

\[\sum_{k=i+1}^{N}c_{k,i}\mathbf{f}_{N-k} =\sum_{k=i+1}^{N}c_{k,i}\left(T_{k}\mathbf{e}_{k}+t_{k+1}\mathbf{ e}_{k+1}+\cdots+t_{N}\mathbf{e}_{N}\right)\] \[=\sum_{k=i+1}^{N}\left(c_{k,i}T_{k}+c_{k-1,i}t_{k}+\cdots+c_{i+1, i}t_{k}\right)\mathbf{e}_{k}.\]

Coefficients of \(\mathbf{e}_{i+1}\) are coincides since

\[c_{1,0}T_{1}=t_{1},\] \[c_{i+1,i}T_{i+1}=t_{i+1}\left(\frac{t_{i}}{T_{0}}+\sum_{j=0}^{i-2 }\frac{t_{i}}{T_{j}}+\frac{T_{i}}{T_{i-1}}\right)\quad i=1,\ldots,N-1.\]

Now assume

\[\frac{t_{j}}{t_{j+1}}=\frac{c_{j,i}T_{j}+c_{j-1,i}t_{j}+\cdots+c_{i+1,i}t_{j}}{ c_{j+1,i}T_{j+1}+c_{j,i}t_{j+1}+\cdots+c_{i+1,i}t_{j+1}}.\] (100)By multiplying the above equation recursively, we obtain

\[\frac{t_{i+1}}{t_{j+1}}=\frac{c_{i+1,i}T_{i}}{c_{j+1,i}T_{j+1}+c_{j,i}t_{j+1}+ \cdots+c_{i+1,i}t_{j+1}}\]

for \(1\leq j\leq N-1\). which implies \(\mathbf{a}_{i}=\sum_{k=i+1}^{N}c_{k,i}\mathbf{f}_{N-k}\).

To prove (100), expand and obtain

\[t_{j+1}c_{j,i}T_{j}=t_{j}\left(c_{j+1,i}T_{j+1}+c_{j,i}t_{j+1}\right).\]

Above equation holds due to the recursive formula of \(C\) (97). 

### Instances of SFG family

Derivation of (SFG)Here, we will show that (SFG) is the instance of SFG family, under the choice of

\[\alpha=4,\quad T_{i}=\frac{(i+2)(i+3)}{4},\quad 0\leq i\leq N.\]

First, \(t_{i}=\frac{i+2}{2}\) for \(i\geq 1\) and \(t_{0}=\frac{3}{2}\). Also (96) holds since

\[4\left(3-\frac{9}{4}\right)\geq\frac{3}{2}\] \[4\left(\frac{(k+2)(k+3)}{2}-\frac{(k+2)^{2}}{4}\right)\geq\frac {\frac{(k+2)^{2}(k+3)^{2}}{16}}{\frac{(k+1)(k+2)}{4}}+\frac{(k+2)^{2}}{4} \left(\frac{2}{3}+4\left(\frac{1}{2}-\frac{1}{k+1}\right)\right)\quad k=1, \ldots,N.\]

Plugging above values into (92) and (97), we obtain

\[h_{k+1,i}=\begin{cases}1+\frac{1}{4}&i=k,\,k=0\\ 1+\frac{k}{k+4}&i=k,\,k=1,\ldots,N-1\\ \frac{k+1}{k+4}\left(h_{k,k-1}-1\right)&i=k-1\\ \frac{k+1}{k+4}h_{k,i}&i=0,\ldots,k-2\end{cases}.\]

and

\[c_{k+1,i}=\begin{cases}\frac{1}{2}&i=0,\ k=0\\ \frac{2(4k+5)}{3(k+4)}&i=k,\ k=1,\ldots,N-1\\ \frac{k+1}{k+4}c_{k,i}&i=0,\ldots,k-1,\ i=2,\ldots,N-1\end{cases}.\]

Other terms come directly, and

\[c_{k+1,k} =\frac{t_{k+1}}{T_{k+1}}\left(\frac{t_{k}}{T_{0}}+t_{k}\sum_{j=0} ^{k-2}\frac{1}{T_{j}}+\frac{T_{k}}{T_{k-1}}\right)\] \[=\frac{t_{k+1}}{T_{k+1}}\left(\frac{t_{k}}{T_{0}}+t_{k}\sum_{j=0} ^{k-1}\frac{1}{T_{j}}+1\right)\] \[=\frac{\frac{k+3}{2}}{\frac{(k+3)(k+4)}{4}}\left(\frac{k+2}{2} \times\frac{2}{3}+\frac{k+2}{2}\left(\frac{4}{2\times 3}+\cdots+\frac{4}{(k+1)(k+2)} \right)+1\right)\] \[=\frac{2}{k+4}\left(\frac{k+2}{3}+\frac{k+2}{2}\frac{2k}{k+2}+1\right)\] \[=\frac{2(4k+5)}{3(k+4)}.\]To sum up, the matrix \(\{g_{k,i}\}_{0\leq i<k\leq N}:=\frac{1}{4}H_{0}+\frac{1}{16}C\) satisfies the following recursive formula.

\[g_{k+1,i}=\begin{cases}\frac{1}{4}\left(1+\frac{3}{8}\right)&i=0,\ k=0\\ \frac{1}{4}\left(1+\frac{10k+5}{6(k+4)}\right)&i=k,\ k=1,\ldots,N-1\\ \frac{k+1}{k+4}\left(g_{k,i}-\frac{1}{4}\right)&i=k-1\\ \frac{k+1}{k+4}g_{k,i}&i=0,\ldots,k-2\end{cases}.\]

Note that

\[g_{k+1,k} =\frac{1}{4}\left(1+\frac{k}{k+4}\right)+\frac{1}{16}\left(\frac{ 2(4k+5)}{3(k+4)}\right)\] \[=\frac{1}{4}\left(1+\frac{k}{k+4}+\frac{4k+5}{6(k+4)}\right)\] \[=\frac{1}{4}\left(1+\frac{10k+5}{6(k+4)}\right).\]

Therefore, the FSFOM with \(\{g_{k,i}\}_{0\leq i<k\leq N}\) can be expressed as

\[x_{1} =x_{0}^{\oplus,4}+\frac{1}{4}\cdot\left(x_{0}^{\oplus,4}-x_{-1}^ {\oplus,4}\right)+\frac{1}{8}\left(x_{0}^{\oplus,4}-x_{0}\right),\] \[x_{k+1} =x_{k}^{\oplus,4}+\frac{k+1}{k+4}\left(x_{k}^{\oplus,4}-x_{k-1}^ {\oplus,4}\right)+\frac{4k-1}{6(k+4)}\left(x_{k}^{\oplus,4}-x_{k}\right),\quad k =1,\ldots,N-1\]

where \(x_{-1}^{\oplus,4}=x_{0}\). To obtain the H-dual, we apply Proposition 1.

\[y_{k+1} =y_{k}^{\oplus,4}+\frac{(N-k+1)(2N-2k-1)}{(N-k+3)(2N-2k+1)}\left(y _{k}^{\oplus,4}-y_{k-1}^{\oplus,4}\right)+\frac{(4N-4k-1)(2N-2k-1)}{6(N-k+3)(2 N-2k+1)}\left(y_{k}^{\oplus,4}-y_{k}\right)\] \[y_{N} =y_{N-1}^{\oplus,4}+\frac{3}{10}\left(y_{N-1}^{\oplus,4}-y_{N-2} ^{\oplus,4}\right)+\frac{3}{40}\left(y_{N-1}^{\oplus,4}-y_{N-1}\right)\]

where \(k=0,\ldots,N-2\) and \(y_{-1}^{\oplus,4}=y_{0}\).

Fastest method among the SFG family via a numerical choice of \(a\)In this section, we give simple form of SFG, when all inequality conditions in (96) holds as equalities. \(\{g_{k,i}\}_{0\leq i<k\leq N}\) becomes

\[g_{1,0}= \frac{1}{\alpha}\left(1+\frac{(t_{0}-1)t_{1}}{T_{1}}\right)+\frac{ 1}{\alpha^{2}}\left(\frac{t_{1}}{T_{1}}\right)\]

and for \(k>0\),

\[g_{k+1,k}= \frac{1}{\alpha}\left(1+\frac{(t_{k}-1)t_{k+1}}{T_{k+1}}\right)+ \frac{1}{\alpha^{2}}\frac{t_{k+1}}{T_{k+1}}\left(t_{k}\left(\frac{1}{T_{0}}+ \frac{1}{T_{0}}+\cdots+\frac{1}{T_{k-2}}\right)+\frac{T_{k}}{T_{k-1}}\right)\] \[= \frac{1}{\alpha}\left(1+\frac{(t_{k}-1)t_{k+1}}{T_{k+1}}\right)+ \frac{1}{\alpha^{2}}\frac{t_{k+1}}{T_{k+1}}\left(\frac{\alpha(2T_{k}-t_{k}^{2} )-\frac{T_{k}^{2}}{T_{k-1}}}{t_{k}}+\frac{T_{k}}{T_{k-1}}\right)\] \[= \frac{1}{\alpha}\left(1+\frac{(t_{k}-1)t_{k+1}}{T_{k+1}}\right)+ \frac{1}{\alpha^{2}}\frac{t_{k+1}}{T_{k+1}}\left(\frac{\alpha(2T_{k}-t_{k}^{2} )}{t_{k}}-\frac{T_{k}}{t_{k}}\right)\] \[= \frac{1}{\alpha}+\frac{1}{\alpha}\frac{t_{k+1}}{T_{k+1}}\left(t_ {k}-1+\frac{2T_{k}-t_{k}^{2}}{t_{k}}-\frac{T_{k}}{t_{k}}\right)-\left(\frac{1}{ \alpha^{2}}-\frac{1}{\alpha}\right)\frac{t_{k+1}T_{k}}{T_{k+1}t_{k}}\] \[= \frac{1}{\alpha}+\frac{1}{\alpha}\frac{(T_{k}-t_{k})t_{k+1}}{t_{k} T_{k+1}}+\left(\frac{1}{\alpha}-\frac{1}{\alpha^{2}}\right)\frac{t_{k+1}T_{k}}{T_{k+1}t _{k}},\] \[g_{k+1,k-1}= \frac{t_{k+1}(T_{k}-t_{k})}{t_{k}T_{k+1}}\left(g_{k,k-1}-\frac{1}{ \alpha}\right),\] \[g_{k+1,i}= \frac{t_{k+1}(T_{k}-t_{k})}{t_{k}T_{k+1}}g_{k,i},\qquad i=k-2, \ldots,0.\]Hence FSFOM with \(\frac{1}{\alpha}H_{0}+\frac{1}{\alpha^{2}}C\) is

\[x_{k+1}=x_{k}^{\oplus,\alpha}+\frac{T_{k-1}t_{k+1}}{t_{k}T_{k+1}}(x_{k}^{\oplus,\alpha}-x_{k-1}^{\oplus,\alpha})+\left(1-\frac{1}{\alpha}\right)\frac{t_{k+1 }T_{k}}{T_{k+1}t_{k}}(x_{k}^{\oplus,\alpha}-x_{k})\]

where \(T_{-1}=\frac{2a-1}{\alpha^{2}}\). By using Proposition 1, we obtain H-dual.

\[y_{k+1}=y_{k}^{\oplus,\alpha}+\beta_{k}^{\prime}(y_{k}^{\oplus, \alpha}-y_{k-1}^{\oplus,\alpha})+\left(1-\frac{1}{\alpha}\right)\frac{T_{N-k} }{T_{N-k+1}}\beta_{N-k}(y_{k}^{\oplus,\alpha}-y_{k})\] \[\beta_{k}^{\prime}=\frac{T_{N-k-1}t_{N-k}\left(T_{N-k-2}+\left(1 -\frac{1}{\alpha}\right)T_{N-k-1}\right)}{t_{N-k-1}T_{N-k}\left(T_{N-k-1}+ \left(1-\frac{1}{\alpha}\right)T_{N-k}\right)},\qquad k=0,\ldots,N-1.\]

Since all equality holds at (96), the above FSFOM achieves the fastest rate among the SFG family under fixed \(\alpha\). 7 Now we optimize \(a\) to achieve the fastest convergence rate. Combine (93) and the result of Proposition 5 to obtain

Footnote 7: In fact, when \(a=1\), the FSFOM becomes FISTA-G [31].

\[\min_{v\in\partial F(y_{N}^{\oplus})}\left\|v\right\|^{2}\leq L^{2}(\alpha+1) ^{2}\left\|y_{N}-y_{N}^{\oplus,\alpha}\right\|^{2}\leq L^{2}\frac{2(\alpha+1) ^{2}}{\alpha T_{N}}\left(F(y_{0})-F_{\star}\right).\]

To achieve the tightest convergence guarantee, we solve the following optimization problem under the fixed \(N\).

\[\underset{\alpha}{\text{minimize}} \frac{2(\alpha+1)^{2}}{\alpha T_{N}}\] subject to. \[\alpha(2T_{0}-t_{0}^{2})=T_{0},\] \[\alpha(2T_{k}-t_{k}^{2})=\frac{T_{k}^{2}}{T_{k-1}}+t_{k}^{2} \left(\frac{1}{T_{0}}+\sum_{i=0}^{k-2}\frac{1}{T_{i}}\right)\quad k=1,\ldots,N.\]

Denote the solution of the above optimization problem as \(R(\alpha,N)\). Since \(R(\alpha,N)\) depends on \(a\) and cannot achieve a closed-form solution, we numerically choose \(a\) and observe an asymptotic behavior of \(R(\alpha,N)\). By choosing \(\alpha=3.8\), asymptotic rate of \(R(3.8,N)\) is about \(\frac{46}{N^{2}}\).

## Appendix E Broader Impacts

Our work focuses on the theoretical aspects of convex optimization algorithms. There are no negative social impacts that we anticipate from our theoretical results.

## Appendix F Limitations

Our analysis concerns \(L\)-smooth convex functions. Although this assumption is standard in optimization theory, many functions that arise in machine learning practice are neither smooth nor convex.