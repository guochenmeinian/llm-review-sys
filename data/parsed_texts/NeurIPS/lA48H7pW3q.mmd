# QUEST: Quadruple Multimodal Contrastive Learning with Constraints and Self-Penalization

Qi Song\({}^{1}\), Tianxiang Gong\({}^{2}\), Shiqi Gao\({}^{2}\),

Haoyi Zhou\({}^{1,3}\), Jianxin Li\({}^{2,3}\)

\({}^{1}\)School of Software, Beihang University

\({}^{2}\)School of Computer Science and Engineering, Beihang University

\({}^{3}\)Zhongguancun Laboratory, Beijing

{songqi23, gongtx, gaoshiqi, haoyi, lijx}@buaa.edu

Equal contributionCorresponding author.

###### Abstract

Multimodal contrastive learning (MCL) has recently demonstrated significant success across various tasks. However, the existing MCL treats all negative samples equally and ignores the potential semantic association with positive samples, which limits the model's ability to achieve fine-grained alignment. In multi-view scenarios, MCL tends to prioritize shared information while neglecting modality-specific unique information across different views, leading to feature suppression and sub-optimal performance in downstream tasks. To address these limitations, we propose a novel contrastive framework named _QUEST: Quadruple Multimodal Contrastive Learning with Constraints and Self-Penalization_. In the QUEST framework, we propose quaternion contrastive objectives and orthogonal constraints to extract sufficient unique information. Meanwhile, a shared information-guided penalization is introduced to ensure that shared information does not excessively influence the optimization of unique information. Our method leverages quaternion vector spaces to simultaneously optimize shared and unique information. Experiments on multiple datasets show that our method achieves superior performance in multimodal contrastive learning benchmarks. On public benchmark, our approach achieves state-of-the-art performance, and on synthetic shortcut datasets, we outperform existing baseline methods by an average of \(97.95\%\) on the CLIP model.

## 1 Introduction

Multimodal Contrastive Learning (MCL) has demonstrated robust representation capabilities and generalizability and effectively transferring to various downstream tasks (e.g. cross-modal retrieval [40, 39, 33], image captioning [37, 72, 73]). However, simply applying contrastive learning in multimodal scenarios presents significant challenges.

In particular, contrastive learning treats all negative samples equally, ignoring the potential semantic relationships between negative samples and the anchor. Besides, current contrastive learning methods focus on maximizing mutual information between two views [68, 70] while ignoring unique information [41]. In multi-view scenarios, the assumption that modalities share substantial task-related information often does not hold, especially in complex datasets with minimal inter-modal overlap. Meanwhile, recent studies [56, 75] indicate that contrastive learning often neglects significant portions of input information, leading to feature suppression [1, 8] and shortcut learning [25, 57], where models minimize loss through the simplest path (e.g. shared information [3]), sacrificing deeper learning. These issues are prevalent in multimodal [45, 54, 32] and multi-view tasks [76, 82, 50, 43]. Recent approaches focus on preserving more unique information, including reconstruction regularization [77, 4], implicit feature modification [58], and factorized representation [41], among others. However, these methods either overly introduce noise which may harm downstream tasks, or rely on certain assumptions (e.g., augmentation [41, 65, 45]). Additionally, we find that these methods do not explicitly distinguish unique information and still optimize using contrastive learning, making it difficult to avoid the model learning shortcuts [68, 29]. This raises the question: **can we explicitly extract both task-related unique and shared information without introducing too much noise?**

To this end, we proposes a novel contrastive framework called _QUEST: Quadruple Multimodal Contrastive Learning with Constraints and Self-Penalization_, designed to enhance the extraction and integration of both shared and unique information across multimodal data. Our primary motivation is to develop a mechanism that effectively captures unique information through a novel quaternion multimodal embedding space, as illustrated in Figure 1(b). This embedding space aims to pull shared representations closer while aligning the unique representations with the shared representation on a common plane. We achieve this by leveraging the properties of the normal vector from the cross-product to diversified unique representation. Consequently, our approach aligns commonalities across modalities while preserving the distinctive unique features of each modality.

Specifically, we first split a network into three components: an encoder, a shared decoder, and a unique decoder. The encoder learns general features with little bias toward specific tasks, while the shared decoder and unique decoder learn agreement and discriminative information, respectively. We build contrastive loss to constrain learning of shared information. To avoid the unique decoder degenerating into the shared decoder, we propose novel contrastive objectives and orthogonal constraints to optimize the quaternion vector space. Finally, self-penalization is used to prevent shared information from overly affecting quaternion vector space optimization. Our framework seeks to mitigate shortcut learning, offering a more nuanced, task-related learning paradigm. Our main contributions can be summarized as follows:

* We develop a novel framework to efficiently extract shared and unique information across multimodal data. To avoid the degeneration of the unique decoder, we propose an algorithm that utilizes quadruple embedding to constrain unique information from different views in a plane space.
* We consider that traditional CL overly relies on shared information due to data bias, causing failures with negative samples containing shared information related to the positive sample. Meanwhile, to prevent shared information from dominating the extraction of unique information, we introduce a self-penalization mechanism to dynamically reweight the distribution of negative samples, which penalizes hard negative samples. We provide theoretical analysis to show how this penalization effectively improves the extraction of unique information.
* We achieve state-of-the-art on popular datasets (e.g. MS-COCO [9] and Flickr30k [80]) compared to the baseline, demonstrating the general effectiveness of QUEST. Additionally, experiment results on synthetic shortcut datasets outperform baselines \(97.95\%\) on average for CLIP, verifying the efficacy of QUEST.

Figure 1: (a) Our QUEST outperforms baselines \(97.95\%\) on average when trained with task-related unique information and evaluated on downstream tasks on the CLIP model. (b) We build the quaternion embedding space, which aligns shared and unique representations from different modalities through the application of constraints and self-penalization. The \(\mathcal{L}_{\text{SIC}}\) narrows the gap between shared representations, while \(\mathcal{L}_{\text{P-UIC}}\) pulls the plane spanned by intra-modality shared and unique representations closer. Furthermore, Orthogonalization loss \(\mathcal{L}_{cos}\) is employed to constrain the area.

Methods

### Problem Formulation

For different modalities \(\{\mathcal{M}_{i}\}_{i=1}^{K}\), given one modality, denoted as \(\mathcal{M}_{i}\), along with its corresponding set of views \(\{x_{i}^{j}\}_{j=1}^{N_{i}},N_{i}\geq 1\), for one modality \(\mathcal{M}_{1}\) encoder parameterized by \(\Theta_{1}\), represented as \(\mathcal{F}_{\mathcal{M}_{1}}(\cdot;\Theta_{1})\), and another modality \(\mathcal{M}_{2}\) encoder parameterized by \(\Theta_{2}\), denoted as \(\mathcal{F}_{\mathcal{M}_{2}}(\cdot;\Theta_{2})\). These encoders process the sample from modal \(\mathcal{M}_{1}\) and \(\mathcal{M}_{2}\), for those modalities with multiple views, like modal one \(\mathcal{M}_{1}\) and each of its views through their respective encoder, resulting in corresponding general representations \(\textbf{H}_{\mathcal{M}_{1}}^{j}=\mathcal{F}_{\mathcal{M}_{1}}(x_{1}^{j}; \Theta_{1})\) and \(\textbf{H}_{\mathcal{M}_{2}}^{j}=\mathcal{F}_{\mathcal{M}_{2}}(x_{2}^{j}; \Theta_{2})\). However, as illustrated in Figure 2, the InfoNCE loss maximizes task-related features shared across all modalities during training (i.e. \(I(X_{A}^{\prime};X_{B}^{\prime};Y)\)), while simultaneously suppressing the unique task-related features of each individual modality(i.e. \(I(X_{A};Y|X_{B})\) and \(I(X_{B};Y|X_{A})\)). This process ultimately results in the loss of unique information. Therefore, the general representations are then separated into task-related shared and unique features through different decoders, i.e., the representations of different modalities \(\textbf{H}_{\mathcal{M}_{1}}^{j},\textbf{H}_{\mathcal{M}_{2}}^{j}\) are inputted separately into different decoders \(\mathcal{G}_{\mathcal{M}_{i}}(\cdot)\) parameterized by \(\Phi_{i}\). For the complete notations, refer to Appendix C, Table 6.

### Overview

In multi-view scenarios, the relationships between different modalities are many-to-many (e.g., for image retrieval, multiple captions can refer to the same image). Within a single modality, different views contain task-related unique and shared information. Additionally, task-related shared information may also exist among negative samples (as indicated by the red shading in Figure 3). Therefore, optimizing solely for shared information while ignoring unique information is suboptimal.

To address the challenge of overlooking unique information inherent to different perspectives in multimodal scenarios, we introduce the effective framework called _QUEST: Quadruple Multimodal Contrastive Learning with Constraints and Self-Penalization_. This framework extends existing contrastive learning methods by incorporating a four-partite architecture specifically designed to enhance the capture and integration of distinctive modal-specific features. The overall architecture is shown in Figure 3. According to [84; 62; 34], from the

Figure 3: Framework of QUEST. The unique decoder is utilized to extra view-specific unique information and this process is guided by the proposed constraints and penalization.

Figure 2: Feature suppression in multi-view contrastive learning. We define \(I(X_{A};X_{B};Y)\) as task-related shared information, \(I(X_{A};Y|X_{B})\) and \(I(X_{B};Y|X_{A})\) as task-related unique information related to task \(Y\) in modalities \(X_{A}\) and \(X_{B}\),respectively. Contrastive losses, such as InfoNCE, tend to maximize the task-related shared information while suppressing the task-related unique information in each modality. Left: before training with InfoNCE. Right: after training with InfoNCE.

perspective of neural network architecture, shallow layers learn low-level and general features while deeper layers learn task-biased high-level semantic features. Therefore, we define the encoder output as \(H\), which contains shared \(S\) and unique \(U\) information related to the task, \(H\supseteq S\cup U\). Consequently, we shared shallow layers for general representation to reduce computational cost and optimize the shared decoder and unique decoder for \(S\) and \(U\), respectively.

### Quadruple InfoNCE

For each modality, the input data \(\mathbf{X}_{i}\) (sampled from views) undergoes transformation by a modality-specific encoder \(\mathcal{F}_{\mathcal{M}_{i}}(\cdot)\), producing an intermediary general representation denoted as \(\mathbf{H}_{\mathcal{M}_{i}}\). Consequently, we introduce two decoders: a shared information decoder \(\mathcal{G}^{\mathbf{s}}_{\mathcal{M}_{i}}(\cdot)\) and a unique information decoder \(\mathcal{G}^{\mathbf{u}}_{\mathcal{M}_{i}}(\cdot)\). These decoders are tasked with disentangling the shared and unique components from the representation \(\mathbf{H}_{\mathcal{M}_{i}}\), respectively. Let \(\Theta_{i}\) represent the parameters of a shared encoder, while \(\Phi_{i}\) and \(\Psi_{i}\) symbolize the decoders for shared and unique information, respectively. The representation can be formulated as follows:

\[\begin{split}\mathbf{Z}^{\mathbf{u}}_{i}&=\mathcal{ G}^{\mathbf{u}}_{\mathcal{M}_{i}}(\mathbf{H}_{\mathcal{M}_{i}};\Phi_{i})= \mathcal{G}^{\mathbf{u}}_{\mathcal{M}_{i}}(\mathcal{F}_{\mathcal{M}_{i}}( \mathbf{X}_{i};\Theta_{i});\Phi_{i}),\\ \mathbf{Z}^{\mathbf{s}}_{i}&=\mathcal{G}^{\mathbf{ s}}_{\mathcal{M}_{i}}(\mathbf{H}_{\mathcal{M}_{i}};\Psi_{i})=\mathcal{G}^{ \mathbf{s}}_{\mathcal{M}_{i}}(\mathcal{F}_{\mathcal{M}_{i}}(\mathbf{X}_{i}; \Theta_{i});\Psi_{i}).\end{split}\] (1)

Shared Information Constraint.In multimodal and multi-view scenarios, data from different modalities and views often encapsulate shared information vital for model training. We conduct a shared Information Constraint (SIC) to maximize the lower bound of MI between representations from different views to encourage the shared decoder to learn agreement related to the task. This constraint is optimized by computing the InfoNCE loss between (\(\mathbf{Z}^{\mathbf{s}}_{i}\) and \(\mathbf{Z}^{\mathbf{s}}_{j}\)), defined as,

\[\mathcal{L}_{\text{SIC}}=\sum_{i,j}\mathds{1}_{\mathcal{M}_{i}\neq\mathcal{M}_ {j}}\mathbb{E}_{\mathbf{Z}^{\mathbf{s}}_{i}}\left[-\log\frac{\exp(s(\mathbf{Z }^{\mathbf{s}}_{i},\mathbf{Z}^{\mathbf{s}^{+}}_{j})/\tau)}{\exp(s(\mathbf{Z}^{ \mathbf{s}}_{i},\mathbf{Z}^{\mathbf{s}^{+}}_{j})/\tau)+\sum_{k=1}^{m}\mathds{1 }_{\mathbf{y}^{-}}\exp(s(\mathbf{Z}^{\mathbf{s}}_{i},\mathbf{Z}^{\mathbf{s}^{ -}}_{jk})/\tau)}\right].\] (2)

Unique Information Constraint.In contrast to shared information, unique information is modality-specific and task-related, providing essential insights for downstream tasks. To preserve this, we introduce a Unique Information Constraint (UIC), extracting the unique information that exists within different views, which are unrelated to each other yet relevant to the task. Relying solely on Shared Information Constraint (SIC) is insufficient to preserve this information [41, 45, 3]. Strict constraints, such as directly enforcing consistency of distributions across diverse views, may lead to the Unique Information Constraint converging to the SIC, particularly in scenarios with identical input representations, loss functions, and network structures. To address this issue, we implement a less stringent constraint, aiming to maximize the similarity between the spaces formed by unique and shared information across different modalities. Firstly, we derive the representation space of normal vectors for shared and unique embedding spaces through cross-product calculations,

\[\mathbf{Z}^{\mathbf{n}}_{i}=\mathbf{Z}^{\mathbf{s}}_{i}\times\mathbf{Z}^{ \mathbf{u}}_{i}.\] (3)

In the newly projected space, our objectives aim to maximize the alignment of unique representation from different modalities within the plane spanned by the shared representation, the magnitude can be formulated as:

\[(\mathbf{Z}^{\mathbf{s}}_{i}\times\mathbf{Z}^{\mathbf{u}}_{i})\cdot(\mathbf{Z }^{\mathbf{s}}_{j}\times\mathbf{Z}^{\mathbf{u}}_{j})=\|\mathbf{Z}^{\mathbf{s} }_{i}\|\|\mathbf{Z}^{\mathbf{u}}_{i}\|\sin\alpha\|\mathbf{Z}^{\mathbf{s}}_{j} \|\|\mathbf{Z}^{\mathbf{u}}_{j}\|\sin\beta\cos\gamma,\] (4)

where \(\sin\alpha\) and \(\sin\beta\) represent the sine similarity between the shared and unique representation across different modalities, and \(\cos\gamma\) represents the cosine similarity of the normal vector. We maximize \(\sin\alpha\) and \(\sin\beta\) via orthogonalized cosine loss \(\mathcal{L}_{\text{cos}}\) and contrastive loss to maximize \(\cos\gamma\), the unique information constraint can be formulated as:

\[\begin{split}\mathcal{L}_{\text{UIC}}&=\sum_{i,j} \mathds{1}_{\mathcal{M}_{i}\neq\mathcal{M}_{j}}\mathbb{E}_{\mathbf{Z}^{ \mathbf{n}}}\left[-\log\frac{\exp(s(\mathbf{Z}^{\mathbf{n}}_{i},\mathbf{Z}^{ \mathbf{n}^{+}}_{j})/\tau)}{\exp(s(\mathbf{Z}^{\mathbf{n}}_{i},\mathbf{Z}^{ \mathbf{n}^{+}}_{j})/\tau)+\sum_{k=1}^{m}\mathds{1}_{\mathbf{y}^{-}}\exp(s( \mathbf{Z}^{\mathbf{n}}_{i},\mathbf{Z}^{\mathbf{n}^{-}}_{jk})/\tau)}\right]\\ &+\sum_{i}\underbrace{\sum_{j}\frac{\mathbf{Z}^{\mathbf{s}}_{ij} \cdot\mathbf{Z}^{\mathbf{u}}_{j}}{\|\mathbf{Z}^{\mathbf{u}}_{ij}\|\|\mathbf{Z}^ {\mathbf{u}}_{ij}\|}}_{\mathcal{L}_{\text{cos}}}.\end{split}\] (5)By pulling positive samples closer and pushing negative samples away, we constrain the shared and unique representations of different modalities to coexist within the same spatial plane as much as practicable. Concurrently, we employ \(\mathcal{L}_{\text{cos}}\) to maintain the diversity of unique information and ensure that \(\mathbf{Z}_{i}^{\text{s}}\neq\mathbf{Z}_{i}^{\text{u}}\). It is worth noting that we do not impose any constraints on the unique representations of different modalities, as these may be inherently unrelated.

**Comparison.** Applying standard InfoNCE to extract unique information leads to suboptimal generalization, as demonstrated in Section 3.3. To address this, we propose UIC with indirect vectors \(\mathbf{Z}^{n}\), where the cross-product operation fundamentally alters gradient propagation patterns compared to pair-wised InfoNCE. Specifically, given \(\mathcal{L}_{\text{InfoNCE}}=-\log\frac{\exp(Z_{a}\cdot Z_{b}^{\ast}/\tau)}{ \sum_{i=0}^{N}\exp(Z_{a}\cdot Z_{i}/\tau)}\)[70, 83], the gradient with respect to anchor embedding \(Z_{a}\) as follows,

\[-\frac{\partial\mathcal{L}_{\text{InfoNCE}}}{\partial Z_{a}}=\frac{1}{\tau}(Z _{b}^{+}-\sum_{i=0}^{N}\beta_{i}Z_{bi}),\] (6)

where \(\beta_{i}=\frac{\exp(Z_{a}\cdot Z_{i}/\tau)}{\sum_{i=0}^{N}\exp(Z_{a}\cdot Z_{ i}/\tau)}\) and \(Z_{b}^{+}\) is random sampled from positive set \(\{Z_{b}^{1},...Z_{b}^{j}\}\). Under the assumption that different views hold both shared and unique information, we have \(I(Z_{b}^{j};Y)=I(Z_{b}^{j};Z_{a};Y)+I(Z_{b}^{j};Y|Z_{a})\), where \(I(Z_{b}^{j};Z_{a};Y)\) represents shared information between two views and \(I(Z_{b}^{j};Y|Z_{a})\) represents unique information for \(j\)th view. With sufficient training iterations, the unique information tends towards noise as shared information dominates the accumulated gradient (first term in Eq. (6)). This is consistent with the conclusion of MI [41]. Assume that the features obtained from the encoder consist of shared features \(S\) which correspond to the anchor and unique features \(U\), represented as \(Z_{b}^{j}=(S\cup U^{j})\). Traditional contrastive learning defines an additive model \(Z_{b}^{j}=(S+U^{j})\) whereas \(Z_{b}^{j}=(S\times U^{j})\) in our model. Intuitively, there exists \(\zeta\) satisfes \(\zeta=Z_{a}\cdot(S\times U^{1})=...=Z_{a}\cdot(S\times U^{j})\). Therefore, UIC is a weaker constraint that ensures quaternion vectors between different views lie on the same plane as much as possible. If we use both SIC and UIC simultaneously, SIC will pull the shared representations of different views closer, while UIC will ensure that the unique representations of different views lie on the same plane as much as possible, rather than measuring their cosine similarity, as unique information is uncorrelated. We conducted extensive experiments to verify this (Section 3.3 for more details).

### Shared Information Guided Constraint

Contrastive learning fundamentally operates by optimizing vector representations to minimize distances between positive pairs while maximizing distances between negative pairs in the embedding space. However, a critical limitation inherent in conventional approaches stems from their undifferentiated treatment of all samples within a batch \(\mathcal{B}\) as negative examples. This indiscriminate categorization may inadvertently cause the model to overlook potential semantic relationships (as illustrated by the red shading in Figure 3), despite their shared semantic content (e.g., different image captions containing identical substrings in image-text retrieval tasks). Such misclassification significantly impairs the model's representation capacity. We refer to these cases as "hard negative samples. While existing methods attempt to address this issue through clustering-based approaches, their two-stage nature limits practical adoption. Leveraging our dual-branch architecture, we propose a more effective solution that directly utilizes the output of the shared decoder as a supervision signal for the penalty term.

Considering our objectives is to optimize the shared information decoder through \(\mathcal{L}_{\text{SIC}}\) and unique information decoder through \(\mathcal{L}_{\text{UIC}}\), and the shared information also affects the optimization of unique information as shown in Eq. (4), we attempt to use the intra-model shared information similar to penalization to guide the optimization of unique information. Unlike soft label [49, 17, 16, 47, 60, 24] which aim to mitigate the strict constraints of one-hot labels, preventing overconfidence by retaining more potential positive samples, our method aims to impose stricter constraints to suppress shared information in the process of learning unique information. Specifically, the more shared information between the anchor and all the negative samples, the greater the encouragement in learning unique information between the anchor and the positive sample. Formally, for shared representation \(\mathbf{Z}_{i}^{\text{s}}\) and \(\mathbf{Z}_{j}^{\text{s}}\), the weighted similarity matrix can be formulated as:

\[\mathcal{P}=\exp(\lambda[\mathbf{S}-\text{diag}(\mathbf{S})+\mathbf{I}]),\] (7)where \(\mathbf{S}=\mathbf{Z}_{i}^{\mathbf{z}}\mathbf{Z}_{j}^{\mathbf{s}^{T}}\) is the similarity matrix, \(\mathbf{I}\) is identity matrix, \(\lambda\in\mathbb{R}\) is learnable weight parameter. Next, the weighted similarity matrix \(\mathcal{P}\in\mathbb{R}^{N\times N}\) is utilized as penalization to supervise the optimization of unique information satisfied \(\mathcal{P}_{ij,i\neq j}\propto s(z_{i}^{s},z_{j}^{s})\), the penalized UIC can be defined as:

\[\begin{split}\mathcal{L}_{\text{P-UIC}}&=\sum_{i,j} \mathbf{1}_{\mathcal{M}_{i}\neq\mathcal{M}_{j}}\mathbb{E}_{\mathbf{z}_{i}^{ \mathbf{n}}}\left[-\log\frac{\exp(s(\mathbf{Z}_{i}^{\mathbf{n}},\mathbf{Z}_{j} ^{\mathbf{n}^{+}})/\tau)}{\exp(s(\mathbf{Z}_{i}^{\mathbf{n}},\mathbf{Z}_{j}^{ \mathbf{n}^{+}})/\tau)+\sum_{k=1}^{m}\mathbf{1}_{\tilde{\mathcal{Y}}^{-}}\exp (\mathcal{P}_{k}\cdot s(\mathbf{Z}_{i}^{\mathbf{n}},\mathbf{Z}_{jk}^{\mathbf{n} ^{-}})/\tau)}\right]\\ &+\sum_{i}\mathcal{L}_{\text{cos}}.\end{split}\] (8)

**Gradient Analysis.** For simplicity, we set \(\lambda=\sum_{k=0}^{m}\exp(\mathcal{P}_{k}\cdot s(\mathbf{Z}_{i}^{\mathbf{n}},\mathbf{Z}_{jk}^{\mathbf{n}})/\tau)\) and ignore the second term, we reformulate Eq. (8) as:

\[\begin{split}\widetilde{\mathcal{L}}_{\text{P-UIC}}& =\mathbb{E}_{\mathbf{z}_{i}^{\mathbf{n}}}\left[-\log\frac{\exp( \mathcal{P}^{+}\cdot s(\mathbf{Z}_{i}^{\mathbf{n}},\mathbf{Z}_{j}^{\mathbf{n} ^{+}})/\tau)}{\exp(\mathcal{P}^{+}\cdot s(\mathbf{Z}_{i}^{\mathbf{n}},\mathbf{ Z}_{j}^{\mathbf{n}^{+}})/\tau)+\sum_{k=1}^{m}\mathbf{1}_{\tilde{\mathcal{Y}}^{-}}\exp (\mathcal{P}_{k}\cdot s(\mathbf{Z}_{i}^{\mathbf{n}},\mathbf{Z}_{jk}^{\mathbf{ n}^{-}})/\tau)}\right]\\ &=\mathbb{E}_{\mathbf{z}_{i}^{\mathbf{n}}}\left[\log\lambda-\frac {\mathcal{P}^{+}\cdot s(\mathbf{Z}_{i}^{\mathbf{n}},\mathbf{Z}_{j}^{\mathbf{n} ^{+}})}{\tau}\right].\end{split}\] (9)

The gradient can be calculated as (Appendix D.2 for details):

\[-\frac{\partial\widetilde{\mathcal{L}}_{\text{P-UIC}}}{\partial\mathbf{Z}_{i}^ {\mathbf{n}}}=\frac{\mathcal{P}^{+}}{\tau}\frac{\partial s^{+}}{\partial \mathbf{Z}_{i}^{\mathbf{n}}}-\frac{1}{\lambda\tau}\sum_{k=0}^{m}\mathcal{P}_{k} \exp\left(\frac{\mathcal{P}_{k}s^{k}}{\tau}\right)\frac{\partial s^{k}}{ \partial\mathbf{Z}_{i}^{\mathbf{n}}},\] (10)

where \(s^{+}=s(\mathbf{Z}_{i}^{\mathbf{n}},\mathbf{Z}_{j}^{\mathbf{n}^{+}})\) and \(s^{k}=s(\mathbf{Z}_{i}^{\mathbf{n}},\mathbf{Z}_{jk}^{\mathbf{n}^{-}})\). Intuitively, \(\mathcal{P}\) represents the belief mass based on shared information, and the larger \(P_{k}\) indicates more shared information between hard positive samples, which also influences the optimization of the unique decoder as in Eq. (4). When \(\mathcal{P}_{k}=0\) (i.e, \(s(\mathbf{Z}_{i},\mathbf{Z}_{jk})\approx 0\)), it will degenerate to the original InfoNCE loss. For hard negatives samples hold shared information where \(s(\mathbf{Z}_{i}^{\mathbf{s}},\mathbf{Z}_{j}^{\mathbf{s}})>0\), we increase the gradient using a penalty term which ensures that even if \(\mathbf{Z}_{i}^{\mathbf{s}}\) is relatively large in Eq. (4), the lower loss reinforce other terms to constrain the overall value. From mutual information perspectives (Appendix D.3 for details), we have

\[I(Z_{i},Z_{j})\geq H^{\hat{P}}(Z_{j}|Z_{i})-H(Z_{j}|Z_{i})+\log N-\widetilde{ \mathcal{L}}_{\text{P-UIC}}.\] (11)

When all negative samples \(k\) satisfy \(s(\mathbf{Z}_{i},\mathbf{Z}_{jk})\approx 0\), we obtain \(H^{\hat{P}}(Z_{j}|Z_{i})=H(Z_{j}|Z_{i})\). Subsequently, as the shared information between hard positive samples increases, \(H^{\hat{P}}(Z_{j}|Z_{i})\) correspondingly increases, thereby elevating both the lower bound of mutual information and the confidence level, which consequently facilitates the learning of unique information.

### Training Objectives

In summary, we apply (1) SIC (Eq. 2) to keep shared information relevance between different modalities, (2) UIC (Eq. 5) to build quadruple embedding space by maximizing the alignment of normal vector spanned by shared representation and unique representation, and the shared information is jointly optimized by SIC and UIC, (3) Self-penalization (Eq. 7) to amplify the effect of false positive samples in unique information optimization. The overall objective can be formulated as:

\[\mathcal{L}_{\text{QUEST}}=\mathcal{L}_{\text{SIC}}+\mathcal{L}_{\text{P-UIC}}.\] (12)

## 3 Experiment

### Experiment Setup

**Baselines and Setup.** Shortcut learning refers to the process in deep learning model training where the model completes tasks (such as classification, retrieval, etc.) by learning simple and discriminatory features while ignoring the semantic and more complex features of the data. This can result in poor model performance on downstream tasks. Latent target decoding (LTD) [4],and implicit feature modification (IFM) [58] are two methods that mitigate shortcut learning by reducing feature suppression. LTD reconstructs the caption representations in the latent space of a Sentence-BERT model, allowing the encoder to mitigate feature suppression via correct mapping. IFM perturbs discriminatory features through encoders and removes part of these features to avoid learning shortcuts, which is implemented as a dual loss combined with the InfoNCE loss. We provide source code of our paper. 2.

Footnote 2: https://github.com/Vortexsong/QUEST

Image Caption Retrieval(ICR) retrieves the most relevant sample in another modality by using a sample of one modality as a query. In this task, there are two retrieval modes: text-to-image (t2i) and image-to-text (i2l). We evaluated our method in the ICR task using CLIP [52] and VSE++ [23] models on Flickr30k [80] and MS-COCO [42] datasets.

Bleeker et al. [3] proposed synthetic shortcuts for the vision-language framework. This allows us to evaluate whether vision language (VL) models capture easy-to-learn discriminatory features or task-related information. We add MNIST Images to the top of pictures and appending corresponding numbers at the end of their respective captions. This controlled approach preserves the original information from both modalities and increases explicit mutual information between them.

**Evaluation Metrics.** To evaluate the model's performance on the Flickr30k and MS-COCO-Caption datasets, the Recall@K (i.e. R@1, R@5, R@10, which refers to the proportion of instances where the correct answer appears among the top K returned results out of all instances) and recall sum (RSUM) were selected as evaluation metrics for both i2t and t2i retrieval.

**Implementation Details.** We select ViT-B/32 as the visual backbone for CLIP and resnet152 for the VSE++ model when we evaluate them on the caption retrieval task using Flickr30k and MS-COCO dataset. We fine-tuned the pre-trained CLIP on downstream tasks and trained VSE++ from scratch with our method.

**Alternatives of Unique Decoder.** The key of multi-view assumption lies in the extraction of unique information, current methods achieve this by employing a single-layer MLP, ensuring orthogonality with shared information or through data augmentation and factorized loss. In our framework, a single-layer MLP is used for the ResNet backbone, while a two-layer Transformer is implemented for the Transformer backbone. Detailed methodologies are provided in the Appendix B.2.

### Performance Evaluation

**QUEST vs. Vanilla InfoNCE.** QUEST outperforms the vanilla InfoNCE, as shown in Table 1. On the Flickr30k test set, QUEST yields \(R@1\) improvements of (\(2.4\), \(1.5\)) for CLIP and (\(1.1\), \(3.4\)) for VSE++ in i2t and t2i tasks.. The corresponding RSUM metrics increase by \(3.2\) and \(12.1\). On MS-COCO, QUEST achieves \(R@1\) gains of (\(1.6\), \(0.9\)) for CLIP and (\(3.1\), \(3.2\)) for VSE++ in i2t and t2i tasks, with corresponding RSUM improvements of \(8.1\) and \(17.3\). Additionaly, QUEST exhibits faster convergence to the optimal solution.

**Experiment on Synthetic Shortcuts.** To assess the effectiveness of our proposed QUEST mitigating feature suppression in Contrastive Learning, we use synthetic shortcuts [3] by injecting easy-to-learn and discriminatory shared information into the image-text training dataset. We then evaluate the model's performance on downstream tasks with and without these synthetic shortcuts to determine if the presence of shortcuts causes the suppression of other task-related information and an over-reliance on shortcut features. Our baselines' results are consistent with [3].

As shown in Table 1, adding shortcuts leads to performance degradation across all models to some degree, indicating that the models have not learned sufficient shared and unique information. However, our method outperforms LTD and IFM, indicating it captures task-related information more effectively in downstream tasks (evaluation without shortcuts).

**CLIP Performance Enhancement.** Our method significantly enhances the CLIP model's performance on the Flickr30k and MS-COCO datasets. Compared to InfoNCE, we observe substantial R@1 improvements in both i2t and t2i tasks, with RSUM increases of \(91.6\) and \(240\) on Flickr30k and MS-COCO, respectively. Our approach also outperforms previous SOTA methods, surpassing \(\mathcal{L}_{\text{InfoNCE+IFM}}\) on Flickr30k and showing significant performance improvements against \(\mathcal{L}_{\text{InfoNCE}}\) on MS-COCO.

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

[7; 6; 26; 10]. Nevertheless, our work is orthogonal to most of the aforementioned studies. Our shared branch approach can be integrated with existing training methods such as MoCo [28] and SimCLR [7].

### Shortcuts Learning

Shortcut Learning refers to the tendency of deep neural networks to exploit simple but potentially unreliable features (i.e., "shortcuts") in data for decision-making, rather than learning more complex but reliable features [25]. This phenomenon can lead to poor model performance on out-of-distribution data and is particularly common in multimodal retrieval [3] and VQA tasks [55; 18]. Robinson et al. [58] proposed strategically adjusting the feature distribution of positive and negative sample pairs to achieve implicit feature modification in contrastive learning, guiding models to learn more robust feature representations. Sanchez et al. [59] propose maximizing mutual information to capture data attributes in shared and exclusive representations, while minimizing it between them to enforce disentanglement. LTD [4] introduces an additional decoder to reconstruct input text descriptions in the latent space of a universal sentence encoder, preventing image and text encoders from suppressing predictive features. More recently, some works strive to enhance the estimation of mutual information through the utilization of stricter bounds [30; 41; 13; 51] or the introduction of regularization constraints [45], consequently preserving unique information more effectively.

## 5 Conclusion

We introduce QUEST, a framework utilizing specialized decoders to extract both unique and shared information via shared information-guided constraints and self-penalization. This study addresses the challenges of imbalanced negative samples and task-related unique feature suppression in Multimodal Contrastive Learning. Our method optimizes shared and unique representations simultaneously, outperforming state-of-the-art methods in preserving unique information and enhancing contrastive learning. Unlike traditional approaches that employ direct dot products to minimize distances between positive samples, QUEST leverages quaternions for the indirect optimization of unique and shared information. However, the application of cross products in high-dimensional spaces is limited, complicating the control of high-dimensional representations and reducing theoretical interpretability. For further discussion on these limitations, see the appendix (Appendix E).

Figure 5: Case Study: (a) Image-to-text retrieval, where the results of \(\mathcal{L}_{\text{QUEST}}\) and \(\mathcal{L}_{\text{InfoNCE}}\) are denoted by italics and underlines, respectively. (b) Text-to-image retrieval, where red and green borders indicate the top-5 retrievals using \(\mathcal{L}_{\text{QUEST}}\), while blue borders represent those using \(\mathcal{L}_{\text{InfoNCE}}\). The upper and lower sections in both (a) and (b) demonstrate scenarios with and without shortcuts, respectively.

## Acknowledgements

This work was supported by the grants from the Natural Science Foundation of China (62202029), and Young Elite Scientists Sponsorship Program by CAST (No. 2023QNRC001). Thanks for the computing infrastructure provided by Beijing Advanced Innovation Center for Big Data and Brain Computing. This work was also sponsored by CAAI-Huawei MindSpore Open Fund. Haoyi Zhou is the corresponding author.

## References

* [1]M. Assran, R. Balestriero, Q. Duval, F. Bordes, I. Misra, P. Bojanowski, P. Vincent, M. G. Rabbat, and N. Ballas (2023) The hidden uniform cluster prior in self-supervised learning. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023, pp.. Cited by: SS1.
* [2]P. Bachman, R. Devon Hjelm, and W. Buchwalter (2019) Learning representations by maximizing mutual information across views. Advances in Neural Information Processing Systems32. Cited by: SS1.
* [3]M. Bleeker, M. Hendriksen, A. Yates, and M. de Rijke (2024) Demonstrating and reducing shortcuts in vision-language representation learning. Transactions on Machine Learning Research. Cited by: SS1.
* [4]M. J. R. Bleeker, A. Yates, and M. de Rijke (2023) Reducing predictive feature suppression in resource-constrained contrastive image-caption retrieval. Trans. Mach. Learn. Res.2023. Cited by: SS1.
* [5]J. Buolamwini and T. Gebru (2018) Gender shades: intersectional accuracy disparities in commercial gender classification. In Conference on Fairness, Accountability and Transparency, pp. 77-91. Cited by: SS1.
* [6]K. Chaitanya, E. Erdil, N. Karani, and E. Konukoglu (2020) Contrastive learning of global and local features for medical image segmentation with limited annotations. Advances in Neural Information Processing Systems33, pp. 12546-12558. Cited by: SS1.
* [7]T. Chen, S. Kornblith, M. Norouzi, and G. Hinton (2020) A simple framework for contrastive learning of visual representations. In International Conference on Machine Learning, pp. 1597-1607. Cited by: SS1.
* [8]T. Chen, C. Luo, and L. Li (2021) Intriguing properties of contrastive losses. Advances in Neural Information Processing Systems34, pp. 11834-11845. Cited by: SS1.
* [9]Y. Chen, H. Fang, T. Lin, R. Vedantam, S. Gupta, P. Dollar, and C. L. Zitnick (2015) Microsoft COCO captions: data collection and evaluation server. CoRRabs/1504.00325. Cited by: SS1.
* [10]X. Chen and K. He (2021) Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 15750-15758. Cited by: SS1.
* [11]X. Chen, X. Wang, S. Changpinyo, A. J. Piergiovanni, P. Padelewski, D. Salz, S. Goodman, A. Grycner, B. Mustafa, L. Beyer, A. Kolesnikov, J. Puigcerver, N. Ding, K. Rong, H. Akbari, G. Mishra, L. Xue, A. V. Thapliyal, J. Bradbury, and W. Kuo (2023) Pali: a jointly-scaled multilingual language-image model. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023, pp.. Cited by: SS1.
* [12]X. Chen, S. Xie, and K. He (2021) An empirical study of training self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 9640-9649. Cited by: SS1.
* [13]P. Cheng, W. Hao, S. Dai, J. Liu, Z. Gan, and L. C. Crain (2020) Club: a contrastive log-ratio upper bound of mutual information. In International Conference on Machine Learning, pp. 1779-1788. Cited by: SS1.
* [14]A. Chouldechova and A. Roth (2018) The frontiers of fairness in machine learning. CoRRabs/1810.08810. Cited by: SS1.
* [15]C. Chuang, R. D. Hjelm, X. Wang, V. Vineet, N. Joshi, A. Torralba, S. Jegelka, and Y. Song (2022) Robust contrastive learning against noisy views. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16670-16681. Cited by: SS1.
* [16]S. Chun, W. Kim, S. Park, M. Chang, and S. J. Oh (2022) ECCV caption: correcting false negatives by collecting machine-and-human-verified image-caption associations for ms-coco. In European Conference on Computer Vision, pp. 1-19. Cited by: SS1.
* [17]S. Chun, S. Joon Oh, R. De Rezende, Y. Kalantidis, and D. Larlus (2021) Probabilistic embeddings for cross-modal retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8415-8424. Cited by: SS1.
* [18]C. Dancette, R. Cadene, D. Teney, and M. Cord (2021) Beyond question-based biases: assessing multimodal shortcut learning in visual question answering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1574-1583. Cited by: SS1.
* [19]M. Defferrard, K. Benzi, P. Vandergheynst, and X. Bresson (2017) FMA: a dataset for music analysis. In Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017, Suzhou, China, October 23-27, 2017, pp. 316-323. Cited by: SS1.
* [20]M. Defferrard, S. P. Mohanty, S. F. Carroll, and M. Salathe (2018) Learning to recognize musical genre from audio: challenge overview. In Advances in Neural Information Processing Systems, pp. 1921-1922. Cited by: SS1.
* [21]K. Drossos, S. Lipping, and T. Virtanen (2020) Clotho: an audio captioning dataset. In 2020 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2020, Barcelona, Spain, May 4-8, 2020, pp. 736-740. Cited by: SS1.
* [22]A. Esteva, A. Robicquet, B. Ramsundar, V. Kuleshov, M. DePristo, K. Chou, C. Cui, G. Corrado, S. Thrun, and J. Dean (2020) A guide to deep learning in healthcare.

_Nature medicine_, 25(1):24-29, 2019.
* [23] Fartash Faghri, David J. Fleet, Jamie Ryan Kiros, and Sanja Fidler. VSE++: improving visual-semantic embeddings with hard negatives. In _British Machine Vision Conference 2018, BMVC 2018, Newcastle, UK, September 3-6, 2018_, page 12. BMVA Press, 201.
* [24] Chen Feng and Ioannis Patras. Adaptive soft contrastive learning. In _2022 26th International Conference on Pattern Recognition (ICPR)_, pages 2721-2727. IEEE, 2022.
* [25] Robert Geirhos, Jorn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A. Wichmann. Shortcut learning in deep neural networks. _Nature Machine Intelligence_, 2(11):665-673, 2020.
* [26] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. _Advances in Neural Information Processing Systems_, 33:21271-21284, 2020.
* [27] Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. _Advances in Neural Information Processing Systems_, 29, 2016.
* [28] Kaiming He, Haooi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 9729-9738, 2020.
* [29] Katherine Hermann and Andrew Lampinen. What shapes feature representations? exploring datasets, architectures, and training. _Advances in Neural Information Processing Systems_, 33:9995-10006, 2020.
* [30] R. Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Philip Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. In _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019_. OpenReview.net, 2019.
* [31] Kenneth Holstein, Jennifer Wortman Vaughan, Hal Daume III, Miro Dudik, and Hanna Wallach. Improving fairness in machine learning systems: What do industry practitioners need? In _Proceedings of the 2019 CHI conference on human factors in computing systems_, pages 1-16, 2019.
* [32] Keli Huang, Botian Shi, Xiang Li, Xin Li, Siyuan Huang, and Yikang Li. Multi-modal sensor fusion for auto driving perception: A survey. _CoRR_, abs/2202.02703, 2022.
* [33] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In _International Conference on Machine Learning_, pages 4904-4916. PMLR, 2021.
* [34] Longlong Jing and Yingli Tian. Self-supervised visual feature learning with deep neural networks: A survey. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 43(11):4037-4058, 2020.
* [35] Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and Diane Larlus. Hard negative mixing for contrastive learning. _Advances in Neural Information Processing Systems_, 33:21798-21809, 2020.
* [36] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. Audiocaps: Generating captions for audios in the wild. In _NAACL-HLT_, 2019.
* [37] Chenliang Li, Haiyang Xu, Junfeng Tian, Wei Wang, Ming Yan, Bin Bi, Jiabo Ye, He Chen, Guohai Xu, Zheng Cao, Ji Zhang, Songfang Huang, Fei Huang, Jingren Zhou, and Luo Si. mplug: Effective and efficient vision-language learning by cross-modal skip-connections. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022_, pages 7241-7259. Association for Computational Linguistics, 2022.
* [38] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In _International Conference on Machine Learning_, pages 19730-19742. PMLR, 2023.
* [39] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. _Advances in Neural Information Processing Systems_, 34:9694-9705, 2021.
* [40] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantic s aligned pre-training for vision-language tasks. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXX 16_, pages 121-137. Springer, 2020.
* [41] Paul Pu Liang, Zihao Deng, Martin Q Ma, James Y Zou, Louis-Philippe Morency, and Ruslan Salakhutdinov. Factorized contrastive learning: Going beyond multi-view redundancy. _Advances in Neural Information Processing Systems_, 36, 2024.
* ECCV 2014
- 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V_, volume 8693 of _Lecture Notes in Computer Science_, pages 740-755. Springer, 2014.
* [43] Yijie Lin, Yuanbiao Gou, Xiaotian Liu, Jinfeng Bai, Jiancheng Lv, and Xi Peng. Dual contrastive prediction for incomplete multi-view representation learning. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 45(4):4447-4461, 2022.

* Liu et al. [2023] Chang Liu, Henghui Ding, and Xudong Jiang. Gres: Generalized referring expression segmentation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 23592-23601, 2023.
* Liu et al. [2024] Shengzhong Liu, Tomoyoshi Kimura, Dongxin Liu, Ruijie Wang, Jinyang Li, Suhas Diggavi, Mani Srivastava, and Tarek Abdelzaher. Focal: Contrastive learning for multimodal time-series sensing signals in factorized orthogonal latent space. _Advances in Neural Information Processing Systems_, 36, 2024.
* Lu et al. [2019] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In In Inana M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alche-Buc, Emily B. Fox, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 13-23, 2019.
* Ma et al. [2023] Jie Ma, Chuan Wang, Yang Liu, Liang Lin, and Guanbin Li. Enhanced soft label for semi-supervised semantic segmentation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 1185-1195, 2023.
* Mehrabi et al. [2021] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey on bias and fairness in machine learning. _ACM Computing Surveys (CSUR)_, 54(6):1-35, 2021.
* 23, 2021_, pages 2855-2870. Association for Computational Linguistics, 2021.
* Poklukar et al. [2022] Petra Poklukar, Miguel Vasco, Hang Yin, Francisco S Melo, Ana Paiva, and Danica Kragic. Geometric multimodal contrastive representation learning. In _International Conference on Machine Learning_, pages 17782-17800. PMLR, 2022.
* Poole et al. [2019] Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational bounds of mutual information. In _International Conference on Machine Learning_, pages 5171-5180. PMLR, 2019.
* Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International Conference on Machine Learning_, pages 8748-8763. PMLR, 2021.
* Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International Conference on Machine Learning_, pages 8748-8763. PMLR, 2021.
* Radu et al. [2016] Valentin Radu, Nicholas D Lane, Sourav Bhattacharya, Cecilia Mascolo, Mahesh K Marina, and Fahim Kawsar. Towards multimodal deep learning for activity recognition on mobile devices. In _Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct_, pages 185-188, 2016.
* Rawal et al. [2024] Ishana Singh Rawal, Alexander Matyasko, Shantanu Jaiswal, Basura Fernando, and Cheston Tan. Dissecting multimodality in videoqa transformer models by impairing modality fusion. In _Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024_, 2024.
* Robinson et al. [2021] Joshua Robinson, Li Sun, Ke Yu, Kayhan Batmanghelich, Stefanie Jegelka, and Suvrit Sra. Can contrastive learning avoid shortcut solutions? _Advances in Neural Information Processing Systems_, 34:4974-4986, 2021.
* Robinson et al. [2021] Joshua Robinson, Li Sun, Ke Yu, Kayhan Batmanghelich, Stefanie Jegelka, and Suvrit Sra. Can contrastive learning avoid shortcut solutions? _Advances in Neural Information Processing Systems_, 34:4974-4986, 2021.
* Robinson et al. [2021] Joshua Robinson, Li Sun, Ke Yu, Kayhan Batmanghelich, Stefanie Jegelka, and Suvrit Sra. Can contrastive learning avoid shortcut solutions? In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 4974-4986, 2021.
* Sanchez et al. [2020] Eduardo Hugo Sanchez, Mathieu Serrurier, and Mathias Ortner. Learning disentangled representations via mutual information estimation. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXII 16_, pages 205-221. Springer, 2020.
* Shi et al. [2023] Yuanjun Shi, Linzhi Wu, and Minglai Shao. Adaptive end-to-end metric learning for zero-shot cross-domain slot filling. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 6291-6301, 2023.
* Shickel et al. [2017] Benjamin Shickel, Patrick James Tighe, Azza Bihorac, and Parisa Rashidi. Deep ehr: a survey of recent advances in deep learning techniques for electronic health record (ehr) analysis. _IEEE Journal of Biomedical and Health Informatics_, 22(5):1589-1604, 2017.
* Singh et al. [2015] Bharat Singh, Soham De, Yangmuzi Zhang, Thomas Goldstein, and Gavin Taylor. Layer-specific adaptive learning rates for deep networks. In _2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)_, pages 364-368. IEEE, 2015.
* COLT 2008_,
Helsinki, Finland, July 9-12, 2008_, pages 403-414. Omnipress, 2008.
* [64] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XI 16_, pages 776-794. Springer, 2020.
* [65] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What makes for good views for contrastive learning? _Advances in Neural Information Processing Systems_, 33:6827-6839, 2020.
* [66] Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive learning, multi-view redundancy, and linear models. In _Algorithmic Learning Theory_, pages 1179-1206. PMLR, 2021.
* [67] Yao-Hung Hubert Tsai, Yue Wu, Ruslan Salakhutdinov, and Louis-Philippe Morency. Self-supervised learning from a multi-view perspective. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021.
* [68] Michael Tschannen, Josip Djolonga, Paul K. Rubenstein, Sylvain Gelly, and Mario Lucic. On mutual information maximization for representation learning. In _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net, 2020.
* [69] George Tzanetakis. Automatic musical genre classification of audio signals. In _ISMIR 2001, 2nd International Symposium on Music Information Retrieval, Indiana University, Bloomington, Indiana, USA, October 15-17, 2001, Proceedings_, 2001.
* [70] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. _CoRR_, abs/1807.03748, 2018.
* [71] Michael Veale, Max Van Kleek, and Reuben Binns. Fairness and accountability design needs for algorithmic support in high-stakes public sector decision-making. In _Proceedings of the 2018 chi conference on human factors in computing systems_, pages 1-14, 2018.
* [72] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. GIT: A generative image-to-text transformer for vision and language. _Trans. Mach. Learn. Res._, 2022, 2022.
* [73] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In _International Conference on Machine Learning_, pages 23318-23340. PMLR, 2022.
* [74] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, and Furu Wei. Image as a foreign language: Beit pretraining for all vision and vision-language tasks. _CoRR_, abs/2208.10442, 2022.
* [75] Tete Xiao, Xiaolong Wang, Alexei A. Efros, and Trevor Darrell. What should not be contrastive in contrastive learning. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021.
* [76] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. _CoRR_, abs/1304.5634, 2013.
* 16, 2023_, 2023.
* [78] Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Hengshuang Zhao, and Philip Pfs Torr. Lavt: Language-aware vision transformer for referring image segmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18155-18165, 2022.
* [79] Chun-Hsiao Yeh, Cheng-Yao Hong, Yen-Chi Hsu, Tyng-Luh Liu, Yubei Chen, and Yann LeCun. Decoupled contrastive learning. In _European Conference on Computer Vision_, pages 668-684. Springer, 2022.
* [80] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. _Trans. Assoc. Comput. Linguistics_, 2:67-78, 2014.
* [81] Olaf Zawacki-Richter, Victoria I Marin, Melissa Bond, and Franziska Gouverneur. Systematic review of research on artificial intelligence applications in higher education-where are the educators? _International Journal of Educational Technology in Higher Education_, 16(1):1-27, 2019.
* [82] Changqing Zhang, Zongbo Han, Huazhu Fu, Joey Tianyi Zhou, Qinghua Hu, et al. Cpm-nets: Cross partial multi-view networks. _Advances in Neural Information Processing Systems_, 32, 2019.
* [83] Chaoning Zhang, Kang Zhang, Chenshuang Zhang, Trung X. Pham, Chang D. Yoo, and In So Kweon. How does simsim avoid collapse without negative samples? A unified understanding with self-supervised contrastive learning. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.
* [84] Linfeng Zhang, Xin Chen, Junbo Zhang, Runpei Dong, and Kaisheng Ma. Contrastive deep supervision. In _European Conference on Computer Vision_, pages 1-19. Springer, 2022.
* [85] Rui Zhu, Bingchen Zhao, Jingen Liu, Zhenglong Sun, and Chang Wen Chen. Improving contrastive learning by visualizing feature transformation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 10306-10315, 2021.

## Appendix A Broder Impact

Multimodal contrastive learning, as an emerging learning paradigm, enhances the model's performance in understanding and processing multimodal data by establishing connections and comparisons between multiple modalities. With the increasing demand for processing multimodal data, widespread application of large models, and rapid development of embodied intelligence, the application of multimodal contrastive learning in the real world requires more consideration of safety and application.

**Safety** Privacy and data security [5] have exacerbated concerns in the context of sensitive data processing, such as facial recognition and personal identification information in multimodal learning. We adopted open-source datasets to avoid incorporating private information into the training process as much as possible. Concurrently, the training of multimodal algorithms often exhibits biases [48, 27, 14] that can contribute to discrimination or unfair treatment of certain groups, reducing the fairness and inclusivity. Our approach may require bias detection or the application of bias reduction methods [71] in real-world applications to ensure that multimodal algorithms maintain unbiased ethics.

**Application** Multimodal learning, particularly when integrated into embodied intelligence, has far-reaching impacts across various domains. In healthcare [22, 61], it enhances advanced diagnostic tools and personalized treatments, which also requires specific measures to assure patient privacy and data security. In education [81, 31], multimodal AI can personalize learning experiences, potentially reducing educational disparities, which requires an unbiased AI. Embodied intelligence [53] in social interaction facilitates accessibility and provides companionship for vulnerable populations, especially as pre-training then fine-tuning becomes the mainstream training paradigm, our method helps models learn more modal-independent unique information; unlike models such as CLIP, which focus too much on shared information.

**Future work** Our research mainly focuses on extracting unique information in modalities from different views. In terms of model architecture, we have implemented a unique decoder to extract unique information and train the model with the quadruple loss with constraints. Our approach and Bleeker [3] et.al both adopt the supervised learning method. Future work could explore extracting unique information through self-supervised learning (SSL) approaches. One potential direction for this exploration involves augmenting the input data. Liang et al. [41] proposed FactorCL to factorize information into shared information and unique information by augmentation to implement SSL. Furthermore, in the era of large language models (LLM), leveraging the powerful capabilities of LLMs to explicitly extract existing data to unique and shared information between different views, as shown in Fig 6. By combining this with the original data, training can be conducted on this explicitly separated information for enhanced model performance.

Figure 6: Shared and unique Information in Multimodal Multi-view Scenario. A single image can be described from multiple viewpoints, each containing shared information and distinctive details unique to the specific perspective.

Experiment Details

### Datasets

**Flickr30k** is a benchmark commonly used in computer vision (CV) and natural language processing (NLP), The tasks applicable to this dataset involving image-caption and multimodal learning, like image understanding, visual question answering and generating text descriptions for images. Flickr30k contains \(31783\) images collected from the Flickr platform, each with five corresponding descriptive captions, totalling 158,915 captions.

**Microsoft Common Objects in Context (MS-COCO)** is a large-scale dataset containing over 328000 images, and each image is paired with at least five detailed captions annotated by humans for other CV tasks like segmentation; MS-COCO also provides segmentation masks, key points and relationships between objects. The images within the MS-COCO cover a multitude of object categories, activities, and scenes, representing a broad spectrum of settings and contexts.

**Free Music Archive (FMA)** is an extensive, open-access dataset designed for Music Information Retrieval (MIR) research, encompassing 917 GiB of audio data (equivalent to 343 days of playback) from 106,574 Creative Commons-licensed tracks. This diverse collection, spanning 16,341 artists, 14,854 albums, and 161 hierarchically organized genres, provides researchers with high-quality, full-length audio files, pre-computed features, and rich metadata including track and user information, tags, and textual descriptions. FMA's comprehensive nature makes it ideal for various MIR tasks such as genre classification, artist identification, and music recommendation, while its predefined train/validation/test splits and subsets of varying sizes facilitate reproducible research and benchmarking in the field. Code, data, and usage examples are available at https://github.com/mdeff/fma.

**GTZAN** is a benchmark dataset widely used in Music Information Retrieval (MIR) and audio signal processing research, particularly for tasks involving musical genre classification and audio feature extraction. The dataset comprises 1,000 audio excerpts, each 30 seconds in duration, equally distributed across 10 distinct musical genres: blues, classical, country, disco, hip-hop, jazz, metal, pop, reggae, and rock. All audio samples in GTZAN are standardized to 22,050 Hz sampling rate, mono channel, and 16-bit resolution in WAV format, facilitating consistent analysis and algorithm development. Despite some noted limitations, GTZAN remains a valuable resource for evaluating and comparing various approaches in automatic music genre recognition and related MIR tasks.

**Clotho** is a diverse audio captioning dataset comprising 4981 audio samples (15-30 seconds each) from Freesound, paired with 24,905 crowdsourced captions (8-20 words each). Designed to facilitate general audio content description using free text, Clotho emphasizes perceptual diversity by providing multiple captions per audio and excluding visual or contextual cues during annotation. The dataset's post-processing, including removal of unique words and speech transcription, enhances its suitability for developing and evaluating audio captioning systems.

**AudioCaps** is a seminal dataset for audio captioning, comprising 46,000 audio clips from AudioSet with human-authored descriptions. This large-scale corpus has become the benchmark for evaluating audio captioning models, catalyzing advancements in audio representation and multimodal learning. AudioCaps has facilitated the development of innovative architectures such as the Audio Captioning Transformer and retrieval-augmented models, significantly contributing to audio-language research. Its impact extends beyond captioning, influencing broader studies in audio-visual integration and inspiring more comprehensive datasets in the field.

### Experimental Settings

**Detailed training settings** In our experiments, we conducted a comprehensive comparison of multimodal model training strategies using the Flickr30k and MS-COCO datasets. We used VSE++ and CLIP, each employing distinct training configurations tailored to their respective architectures and optimization requirements.

\begin{table}
\begin{tabular}{l l l l l l l l l l l l l l l l} \hline \hline \multirow{2}{*}{Model} & \multicolumn{6}{c}{Flickr30k} & \multicolumn{6}{c}{MS-COCO} & \multirow{2}{*}{Vised Encoder} & \multirow{2}{*}{Text Encoder} & \multirow{2}{*}{Penna} \\ \cline{2-2} \cline{7-13}  & Epoch & & & & & & & & & & & & & & & & \\ \hline VSE++ & 30 & 128 & 300 & 300 & 300 & 128 & 300 & 128 & 300 & 100 & 1000 & 1000 & 1000 & 1000 & 1000 & 1000 & 1000 & 1000 & 1000 & 1000 & 1000 \\ CLIP & 5 & 205 & 205 & 300 & 100 & 1000 & 1000 & 1000 & 1000 & 1000 & 1000 & 1000 & 1000 & 1000 & 1000 & 1000 & 1000 & 10000 & 1000 & 10000 & 10000 & 10000 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Multimodal Model Training Details.

For the VSE++ model, we utilized a training regime consisting of 30 epochs with a batch size of 128. The optimization process was driven by the Adam optimizer with a learning rate set at \(2\times 10^{-4}\). Notably, no warmup steps were employed, and the learning rate was adjusted using a stepLR scheduler. This configuration was consistently applied across both the Flickr30k and MS-COCO datasets. The visual encoder for VSE++ was based on ResNet152, while the text encoding was handled by a GRU, resulting in a total parameter count of 67 million.

Moreover, the CLIP model was trained over five epochs with a larger batch size of 256. Optimization was performed using the AdamW optimizer with a significantly lower learning rate of \(2\times 10^{-5}\). In contrast to VSE++, CLIP incorporated 100 warmup steps and utilized a cosine annealing scheduler for learning rate adjustment. This setup was also uniformly applied to both datasets. The visual encoder for CLIP was a ViT-B/32, and text encoding was managed by a Transformer, leading to a substantially larger parameter count of 152 million.

**Unique Decoder Implementation Details** We used CLIP(ViT/B-32 backbone) and VSE++(resnet152 backbone); we chose a single-layer MLP for vse++ and two-layer Transformers for CLIP. Both of them use nine layers as a unique start layer and 0 for no unique start layer. We choose the hyperparameters \(alpha\_t\) as 0.08 on most experiments and set \(positive\_sample\) to false.

### More Implementation Details

The \(\mathcal{L}_{\text{UIC}}\) algorithm enhances contrastive loss in representation learning through a novel embedding space involving quadruplets. This approach primarily comprises two stages: the computation of a similarity map from quadruple embeddings and the subsequent calculation of cross-entropy loss based on this map. The method leverages shared and unique embeddings to construct expressive feature representations, further augmented by three-dimensional vectors generated via cross-products, thereby intensifying the discriminative power of embeddings.

As shown in 1, the algorithm firstly processes four embedding vectors:\(x_{\text{shared}}\), \(x_{\text{unique}}\), \(y_{\text{shared}}\), and \(y_{\text{unique}}\), representing shared and unique features across two data sets. Padding may be applied to ensure dimensional consistency (Divisible by 3) across embeddings. The GetSimMap function, shown in 2 then calculates the similarity map between \(x_{\text{shared}}\) and \(y_{\text{shared}}\), reflecting the similarity between shared feature embeddings. This computation involves normalizing the embeddings, obtaining a preliminary similarity matrix via dot products, and adjusting the similarity values through exponential weighting and diagonal normalization.

Upon obtaining the similarity map, the algorithm transforms \(x_{\text{shared}}\) and \(x_{\text{unique}}\) (along with their \(y\) counterparts) into three-dimensional vectors, or _triplet embeddings_, via cross products. This geometric transformation aims to further enhance the distinctiveness of embeddings. The triplet embeddings are then normalized to ensure numerical stability during dot product calculations. Ultimately, the algorithm calculates the dot product of \(x_{\text{triplet}}\) and \(y_{\text{triplet}}\), scales the result, and applies element-wise multiplication with the similarity map to produce the final \(logits\). These logits, after scaling and absolute value adjustments, are used to compute the cross-entropy loss.

We also use orthogonal cosine embedding loss 3 to quantify the similarity between pairs of input vectors, facilitating the training of models on tasks that distinguish between similar and dissimilar data points. The objective of this loss computation is to minimize the discrepancy between predicted logits and actual labels, thereby optimizing the representational capacity of the embeddings.

The synthetic shortcuts experiment enables a quantitative analysis of the model's reliance on shortcuts when they are present and its ability to capture shared and task-relevant unique information. The results demonstrate that our proposed method effectively mitigates the feature suppression phenomenon, contributing to improved performance on downstream tasks compared to previous approaches.

**Generalization Capability Across Multiple Modalities** To evaluate the generalization capability of our proposed approach on more modalities, we conducted comprehensive experiments across various modalities, including visual, textual, and acoustic domains. Specifically, we performed image-audio retrieval experiments utilizing the FMA (Free Music Archive) and GTZAN datasets, with quantitative results presented in Table 4. For text-audio retrieval evaluations, we leveraged the CLOTHO and AUDIOCAPS datasets, with comparative performance metrics detailed in Table 5.

The empirical results demonstrate that our proposed models consistently outperformed the baseline approaches across all experimental configurations. For simplicity, we employed the almost simplest```
1:Latent representation \(x_{\text{shared}},x_{\text{unique}},y_{\text{shared}},y_{\text{unique}}\).
2:\(\mathcal{L}_{\text{UIC}}\)
3:functionCalculate\(\mathcal{L}_{\text{UIC}}(x_{\text{shared}},x_{\text{unique}},y_{\text{shared}},y_{\text{unique}})\)
4:\(B,C\leftarrow\) shape of \(x_{shared}\)
5:if padding is enabled in config then
6:\(pad\_size\leftarrow(3-C\mod 3)\mod 3\)
7:\(x_{shared}\leftarrow\) pad \(x_{shared}\) with \(pad\_size\)
8:\(x_{unique}\leftarrow\) pad \(x_{unique}\) with \(pad\_size\)
9:\(y_{shared}\leftarrow\) pad \(y_{shared}\) with \(pad\_size\)
10:\(y_{unique}\leftarrow\) pad \(y_{unique}\) with \(pad\_size\)
11:endif
12:\(sim\_map\leftarrow\)GetSimMap\((x_{shared},y_{shared})\)
13:\(mini\_heads\leftarrow\) integer division of \(C\) by 3
14:\(participated\_dims\gets mini\_heads\times 3\)
15:\(x_{shared}\leftarrow\) reshape \(x_{shared}[:,:participated\_dims]\) to \((B,-1,3)\)
16:\(x_{unique}\leftarrow\) reshape \(x_{unique}[:,:participated\_dims]\) to \((B,-1,3)\)
17:\(y_{shared}\leftarrow\) reshape \(y_{shared}[:,:participated\_dims]\) to \((B,-1,3)\)
18:\(y_{unique}\leftarrow\) reshape \(y_{unique}[:,:participated\_dims]\) to \((B,-1,3)\)
19:\(x_{uic}\leftarrow\) cross product of \(x_{shared}\) and \(x_{unique}\) along dimension 2, then reshape to \((B,-1)\)
20:\(y_{uic}\leftarrow\) cross product of \(y_{shared}\) and \(y_{unique}\) along dimension 2, then reshape to \((B,-1)\)
21:\(x_{uic}\leftarrow\) normalize \(x_{uic}\) along the last dimension
22:\(y_{uic}\leftarrow\) normalize \(y_{uic}\) along the last dimension
23:\(logits\gets x_{uic}\times y_{uic}^{T}\)
24:\(logits\leftarrow\) absolute value of \(logits\)
25:\(logits\gets scale\times logits\times sim\_map\)
26:\(num\_logits\gets B\)
27:\(labels\leftarrow\) range from 0 to \(num\_logits-1\)
28:\(\mathcal{L}_{\text{QUAD}}\leftarrow\) CE loss of \(logits\) with \(labels+\) CE loss of \(logits^{T}\) with \(labels\)
29:\(\mathcal{L}_{\text{cos}}\leftarrow\)GetCosLoss\((y_{shared},y_{unique})+\)GetCosLoss\((x_{shared},x_{unique})\)
30:\(\mathcal{L}_{\text{UIC}}\leftarrow\)\(\mathcal{L}_{\text{QUAD}}+\mathcal{L}_{\text{cos}}\)
31:return\(\mathcal{L}_{\text{UIC}}\)
32:endfunction ```

**Algorithm 2** Calculate similarity map

```
1:latent representations \(x\), \(y\)
2:similarity map \(sim\_map\)
3:functionGetSimMap(\(x\), \(y\))
4:\(x\leftarrow\) normalize \(x\) along the last dimension
5:\(y\leftarrow\) normalize \(y\) along the last dimension
6:\(sim\_map\gets x\times y^{T}\)
7:\(sim\_map\leftarrow\) clamp \(sim\_map\) between 0 and 1
8:\(sim\_map\leftarrow\) exp(\(sim\_map\))
9:fill the diagonal of \(sim\_map\) with 1
10:return\(sim\_map\)
11:endfunction ```

**Algorithm 3** Calculate similarity map

decoder structure (linear layers) and did not implement modality fusion or any cross-modal interactions. We believe that enhancing the architecture, strengthening cross-modal interactions, employing a larger batch size and incorporating pre-training would yield even higher performance.

**Compute Resources.** All experiments in this paper are run on a single NVIDIA A100 GPU. The implementation is based on PyTorch 2.0.1. It takes about 2 hours to train the CLIP-based model on Flickr30K for 5 epochs, and the maximum training time for other experiments does not exceed 12 GPU hours.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multirow{2}{*}{Datasets} & \multicolumn{4}{c}{i2a} & \multicolumn{4}{c}{a2i} \\ \cline{3-10}  & & R@1 & R@5 & R@10 & RSUM & R@1 & R@5 & R@10 & RSUM \\ \hline InfoNCE & \multirow{2}{*}{FMA} & 15.87 & 28.62 & 35.87 & 80.36 & 12.50 & 25.50 & 29.12 & 67.12 \\ QUEST & & **17.83** & **30.87** & **38.50** & **87.20** & **13.50** & **26.52** & **30.62** & **70.64** \\ \hline InfoNCE & \multirow{2}{*}{GTZAN} & 34.01 & 84.73 & 94.41 & 213.15 & 32.48 & 78.68 & 90.86 & 202.02 \\ QUEST & & **41.62** & **88.83** & **97.65** & **228.1** & **35.53** & **82.23** & **93.40** & **211.16** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Image audio retrieval results on FMA and GTZAN datasets.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multirow{2}{*}{Datasets} & \multicolumn{4}{c}{t2a} & \multicolumn{4}{c}{a2t} \\ \cline{3-10}  & & R@1 & R@5 & R@10 & RSUM & R@1 & R@5 & R@10 & RSUM \\ \hline InfoNCE & \multirow{2}{*}{CLOTHO} & 20.16 & 51.30 & 66.56 & 138.02 & 20.06 & 52.66 & 68.23 & 140.95 \\ QUEST & & **21.10** & **52.45** & **68.86** & **142.41** & **22.36** & **54.23** & **70.42** & **147.01** \\ \hline InfoNCE & \multirow{2}{*}{AUDIOCAPS} & 4.59 & 17.79 & 26.22 & 48.6 & 5.45 & 15.78 & 22.48 & 43.71 \\ QUEST & & **5.16** & **18.08** & **27.17** & **50.41** & **6.02** & **15.98** & **24.78** & **46.78** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Text audio retrieval results on CLOTHO and AUDIOCAPS datasets.

[MISSING_PAGE_EMPTY:22]

Analysis.

### Mutual Information

In many machine learning tasks, it is often observed that different views simultaneously harbour both task-relevant shared information and unique information (e.g., image captioning [38; 40; 72] and referring expression segmentation [78; 44]). To this end, we work with a dual-encoder architecture.

Contrastive learning with multiple views obtains mutual information between different modalities by learning the similarity among different views. [70] et al. introduced a lower bound on mutual information, known as InfoNCE, which is based on the concept of Noise Contrastive Estimation (NCE); it compares the compatibility of different views by maximizing the mutual information of positive pairs and minimizing the mutual information of negative pairs, learning to extract the consistent representation across different modal. It can be defined as follows:

\[I_{\mathrm{InfoNCE}}(X;Y)=\mathbb{E}_{(x,y)\sim p(x,y)}\left[\frac{1}{N}\sum_ {i=1}^{N}\log\frac{\exp f(x_{i},y_{i})}{\frac{1}{N}\sum_{j=1}^{N}\exp f(x_{i},y _{j})}\right]\] (13)

In this equation, \(x_{i}\) and \(y_{i}\) represent paired samples from the joint distribution of the two random variables under consideration, while \(y_{j}\) represents samples from the marginal distribution of one of the variables. The function \(f(x,y)\) is a learnable function, often parameterized by a neural network, that aims to distinguish between the paired samples and the independently sampled ones.

In the realm of vison-language contrastive learning, the _InfoNCE loss function_[70] enhances the similarity between positive sample pairs relative to negative ones. Mathematically, the InfoNCE loss is articulated as:

\[\mathcal{L}_{\text{InfoNCE}}=-\log\frac{\exp(\text{sim}(x,y^{+})/\tau)}{\sum _{i=0}^{K}\exp(\text{sim}(x,y_{i})/\tau)}\] (14)

where \(\text{sim}(x,y)\) calculates the similarity between samples \(x\) and \(y\), typically computed through dot product or cosine similarity. \(y^{+}\) denotes the positive sample corresponding to \(x\), and \(\{y_{i}\}_{i=0}^{K}\) is a set including one positive and \(K\) negative samples. The parameter \(\tau\) serves as a temperature coefficient, modulating the scale of similarity scores.

InfoNCE loss prevails in many contrastive learning algorithms and performs well in applications. Multi-view redundancy [66; 2; 30; 63; 65; 67] assumes that there exists duplicated information across varied views or representations. Contrastive losses like InfoNCE loss tend to maximize the mutual features between different views while suppressing task-relevant features that may be used in downstream tasks in multi-view representation learning.

Mutual information (MI), often denoted as \(I(X;Y)\), is a fundamental concept in information theory that quantifies the statistical dependence between two random variables, \(X\) and \(Y\). It measures the reduction in uncertainty about one variable when the value of the other is known and is defined as:

\[I(x;y)=\mathbb{E}_{(x,y)\sim p(x,y)}[\log\frac{p(x,y)}{p(x)p(y)}]\] (15)

where \(p(x,y)\) is the joint distribution, and \(p(x)\) and \(p(y)\) are the marginal distributions of \(X\) and \(Y\).

In machine learning, particularly in deep learning, MI is often used as an objective function or regularization component to promote or constrain the interdependence among variables. However, the precise quantification of MI is only attainable in limited instances, as it requires the closed form of the density function and the tractable logarithm density ratio between the joint and marginal distributions. In most machine learning applications, practitioners only have access to samples from the joint distribution, making the direct computation of MI infeasible.

The InfoNCE bound has several desirable properties that make it an attractive choice for estimating mutual information. Firstly, it is a lower bound on the true mutual information, \(I(X;Y)\geq I_{\text{NCE}}\), which means that maximizing the InfoNCE bound leads to an increase in the true mutual information.

[MISSING_PAGE_EMPTY:24]

Next, we derive the lower bound of MI,

\[\begin{split}\widetilde{\mathcal{L}}_{\text{P-UUC}}&=- \mathbb{E}\log\left[\frac{\frac{\tilde{p}(Z_{jk}|Z_{ik})}{p(Z_{jk})}}{\frac{ \tilde{p}(Z_{jk}|Z_{ik})}{p(Z_{jk})}+\sum_{t\neq k}^{N}\frac{\tilde{p}(Z_{jk}|Z _{ik})}{p(Z_{jt})}}\right]\\ &=\mathbb{E}\log\left[1+\frac{p(Z_{jk})}{\tilde{p}(Z_{jk}|Z_{ik}) }\sum_{t\neq k}^{N}\frac{\tilde{p}(Z_{jt}|Z_{it})}{p(Z_{jt})}\right]\\ &\approx\mathbb{E}\log\left[1+\frac{p(Z_{jk})}{\tilde{p}(Z_{jk}| Z_{ik})}(N-1)\mathbb{E}_{Z_{ji}}\frac{\tilde{p}(Z_{jt}|Z_{it})}{p(Z_{jt})} \right]\!.\\ &=\mathbb{E}\log\left[1+\frac{p(Z_{jk})}{\tilde{p}(Z_{jk}|Z_{ik}) }(N-1)\right]\\ &\geq\mathbb{E}\log\left[\frac{p(Z_{jk})}{\tilde{p}(Z_{jk}|Z_{ik })}N\right]\\ &=H^{\tilde{p}}(Z_{j}|Z_{i})-H(Z_{j})+\log N\\ &=H^{\tilde{p}}(Z_{j}|Z_{i})-I(Z_{i},Z_{j})-H(Z_{j}|Z_{i})+\log N \end{split}\] (23)

## Appendix E Limitation

**Dimension restrictions.** The limitations of the cross-product in the context of QupleInfoNCE's unique information extraction are theoretically constrained. When contrastive learning is applied to both unique and shared representations, it can lead to model degradation, and a trade-off choice is the cross-product, which is a weaker constraint. In low-dimensional space, it is straightforward to prove the properties of orthogonal vector space, yet the extension of the cross-product to high-dimensional space is challenging. The behaviour of high-dimensional space representation is difficult to control, resulting in our theory lacking sufficient interpretability in high-dimensional space. This highlights the importance of developing more sophisticated methods for high-dimensional data analysis in the future.

**The Extraction of Unique Information.** On the synthetic shortcuts dataset, our framework achieved significant performance, validating the positive role of unique information in preventing model shortcuts. However, within the entire framework, the core lies in identifying unique information, which is essentially a task-dependent definition. Our experiment indicates that pre-trained models (such as CLIP [52]) typically yield higher unique information benefits due to their extensive generic representations. Non-pre-trained models (such as VSE++ [23]) struggle to discern unique information, tending to shortcuts and losing vital unique information. In this study, we introduced an additional unique information decoder to capture unique information, which incorporated an extra gradient branch, suggesting exploration space remains for single-stream unique information extraction. This also proposes future research: how to more effectively extract unique information from a single stream to reduce computational complexity while maintaining model performance.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims presented in the abstract and introduction are consistent with the contributions and scope detailed in the paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See Appendix E. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: See Section 2 and Appendix D. 1. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? * Answer: [Yes] Justification: See Appendix B.1 and Appendix B.2. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: See the "supplementary.zip" file. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: See Section 3 and Appendix B.1 Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: See Section 3.2. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean.

* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See Appendix B.3. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: See Appendix A. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: This paper have state which version of the asset is used. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?Answer: [NA] Justification: Paper does not release new assets. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.