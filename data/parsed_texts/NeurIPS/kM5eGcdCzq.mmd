[MISSING_PAGE_FAIL:1]

Introduction

Progress in natural language processing is increasingly driven by sheer compute scale alone [1]: as more compute is expended to train large language models (LLM), they gain and exhibit powerful emergent capabilities [2, 3]. To best benefit from scaling, recent scaling laws dictate that both model size and dataset size should jointly be increased [4]. This is at variance with earlier findings, which had argued that scaling should focus on model size first and foremost, with minimal data scaling [5].

This joint scaling paradigm raises significant challenges: although plentiful, text data is not infinite, especially so when accounting for data quality and licensing-leading some researchers to argue scaling may soon be bottlenecked by data availability [6]. Concretely, optimally training a GPT-3 sized model (175B parameters) would require no less than 3,500 billion tokens according to [4]. This is twice as much as the largest pretraining datasets publicly demonstrated [4, 7], and ten times more than the largest publicly available English datasets such as OSCAR [8], C4 [9], or The Pile [10].

Massively scaling-up pretraining data is made even more challenging by the fact LLMs are commonly trained using a mixture of web crawls and so-called "high-quality" data [2, 10]. Typical high-quality corpora include curated sources of books, technical documents (e.g., research papers), human-selected web pages, code or social media conversations. The increased diversity and quality brought forth by these curated corpora is believed to be a key component of performant models [11]. Unfortunately, curation is labour intensive: typically, each source requires specialized processing, while yielding a limited amount of data. Furthermore, licensed sources can raise legal challenges.

Nevertheless, most pretraining data is still sourced out of necessity from massive web crawls-as they can be scaled up to trillions of tokens with limited human intervention. However, the quality of this data has traditionally been seen as (much) inferior to that of the manually curated data sources. Even finely processed sources of web data, such as C4 [9] or OSCAR [8], are regarded as inferior to curated corpora for LLMs [12, 11], producing less performant models.

To sustain the ever-increasing needs of larger and larger LLMs, and to streamline data pipelines and reduce the need for human-intensive curation, we explore how web data can be better processed to significantly improve its quality, resulting in models as capable as models trained on curated corpora.

Contributions.We make the following contributions:

* We introduce RefinedWeb, a five trillion tokens web-only English pretraining dataset;
* We demonstrate that **web data alone can result in models outperforming both public and private curated corpora**, challenging current views about data quality;
* **We publicly release a 600B tokens extract of RefinedWeb, and 1/7B parameters LLMs trained on it**, to serve as a new baseline high-quality web dataset for the community.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline
**Dataset** & **Size** & **Availability** & **Web** & **CC Processing** & **Dedapplication** \\ \hline \multicolumn{6}{c}{**Massive web datasets**} \\ \hline
**C4** & \(\sim 360\)GT & Public & \(100\%\) & Rules + NSFW words blocklist & **Exact**: spans of 3 sentences \\
**OSCAR-21.09** & \(\sim 370\)GT & Public & \(100\%\) & Built at the line-level & **Exact**: per line (\(\sim 55\%\) removed) \\
**OSCAR-22.01** & \(\sim 283\)GT & Public & \(100\%\) & Line-level rules + optional & **Exact**: per line (optional, not rules \& NSFW URL blocklist & used for results in this paper) \\ \hline \multicolumn{6}{c}{**Curated datasets**} \\ \hline \hline GPT-3 & \(300\)GT & Private & \(60\%\) & Content filter trained on known & **Fuzzy**: MinHash (\(\sim 10\%\) removed) \\ \(\blacktriangledown\) The Pile & \(\sim 340\)GT & Public & \(18\%\) & \(\text{19-Text}\) for extraction, filter trained on curated data & **Fuzzy**: MinHash (\(\sim 26\%\) removed) \\ \(\blacktriangledown\) PaLM & \(780\)GT & Private & \(27\%\) & Filter trained on HQ data & Unknown \\ \hline \multicolumn{6}{c}{**Ours**} \\ \hline
**O**ReinnedWeb** & \(\sim 5,000\)GT & Public (500GT) & \(100\%\) & trafilatura for text extraction, document and line-level rules, NSFW URL blocklist & **Exact \& fuzzy**: exact sub-string+MinHash (\(\sim 50\%\) removed) \\ \hline \hline \end{tabular}
\end{table}
Table 1: **F**ReinnedWeb improves on existing English pretraining datasets for large language models by combining extensive filtering with stringent deduplication at unprecedented scale. For additional details, see the full version in Table 12 of Appendix H.3.

Related works

Pretraining data for large language models.Both GPT and BERT identified the importance of datasets with long, coherent documents [13; 14]. Moving from sentence-wise datasets [15], they instead leveraged document-focused, single-domain corpora like Wikipedia or BookCorpus [16]. As models increased in scale, datasets based on massive web-scrape gained prevalence [8; 9]. However, further work argued that these untargeted web scrape fell short of human-curated data [17], leading to the wide adoption of curated datasets such as The Pile [10], combining web data with books, research articles, conversations, and more. At scale, it has been proposed to emulate the human curation process by leveraging weak signals: for instance, by crawling the top links of a forum [18]. Targeted corpora can also produce domain-specific models [19], or broaden the expressiveness of models (e.g., for conversational modalities [20; 21]). Latest large language models [2; 12; 22; 23] are trained on giant aggregated corpora, combining both massive web-scrape and so-called "high-quality" curated single-domain sources. These targeted sources are often upsampled-from one to five times is most common-to increase their representation in the final dataset. The assumed diversity and higher-quality brought fourth by these aggregated datasets is thought to be central to model quality; web data alone is considered insufficient to train powerful large language models [24; 11].

Pipelines for web data.Massive web datasets are typically built upon CommonCrawl, a publicly available scrape of the internet. Working with data scraped from all over the internet presents unique challenges: notably, a significant portion is machine-generated spam or pornographic content [25; 26]. Accordingly, training on unfiltered web data is undesirable, resulting in poorly performing models [9]. Modern pipelines focus on filtering out undesirable content [27]. Broadly speaking, these pipelines usually combine a variety of stages: (1) _language identification_, leveraging inexpensive n-gram models (e.g., fastText [28]); (2) _filtering rules and heuristics_, such as only keeping lines with valid punctuation, discarding lines with too many symbols, or removing documents containing banned words [29; 9]; (3) _ML-based quality filtering_, using lightweight models trained on known gold data to identify similar high-quality web documents [27; 2]; (4) _deduplication_, removing either exact duplicate spans or similar documents [30]. While some filtering is necessary, excessive filtering can introduce undesirable biases: this can overly impact minorities [31], motivating the adoption of practices such as pseudo-crawling, wherein allowed URLs are manually curated [32].

Deduplication.Deduplication removes repeated extracts and documents from a dataset: these could either be exact matches, identical in every character, or approximate matches, based on some similarity metric. For exact duplicates, it is common to match exact substrings of a minimum length using suffix arrays [33]. For fuzzy duplicates, methods based on locally-sensitive hashes such as MinHash [34] or SimHash [35] have seen wide adoption [2; 36; 12]. Recently, [37] has proposed to leverage embeddings to imbue semantic understanding in approximate matching algorithms. Deduplication has been identified as playing a significant role in improving language models [38; 30]. Notably, it reduces memorization [39], which is especially problematic in large models [40]. Furthermore, repeated data has been shown to be increasingly harmful to model quality as parameter count increases [41]: for a 1B parameters model, a hundred duplicates are harmful; at 175B, even a few duplicates could have a disproportionate effect. Concurrently to this work, the Pythia suite of models found that deduplicating The Pile had a limited impact on zero-shot performance [42], questioning whether deduplication is as relevant for curated corpora as it for predominantly web-based datasets as studied in Lee et al. [30].

We provide an overview of some widely adopted pretraining English datasets for LLMs in Table 1, with additional information in Table 12 of Appendix H.3. We also note that recent popular open models [43; 7] often indirectly leverage The Pile [10] by doing a mix-and-match of its components.

With RefinedWeb, we extend upon the state-of-the-art in three ways: (1) we aggregate and combine best-practices for document preparation and filtering across multiple pipelines, and introduce line-wise corrections to fix lingering issues with text extraction; (2) we combine both exact and fuzzy deduplication at very large-scale; (3) the scale of our final dataset is unique, with a total 5,000 billion tokens, and a 600 billion tokens extract available for public use with permissive licensing. Training large models on RefinedWeb also lead us to challenge the commonly held belief that web data is worse than curated corpora, as our models outperform others trained on so-called "high-quality" data.

Macrodata Refinement and RefinedWeb

We introduce **MDR** (MacroData Refinement), a pipeline for filtering and deduplicating web data from CommonCrawl at very large scale. Using MDR, we produce RefinedWeb, an English pretraining dataset of five trillion tokens based on web data only. We leverage strict filtering and stringent deduplication to uplift the quality of web data, distilling it down to a corpus matching the quality of aggregated corpora used to train state-of-the-art models.

Design principles.We abide by the following guidelines:

* **Scale first.** We intend MDR to produce datasets to be used to train 40-200B parameters models, thus requiring trillions of tokens [4]. For English-only RefinedWeb, we target a size of 3-6 trillion tokens. Specifically, we eschew any labour intensive human curation process, and focus on CommonCrawl instead of disparate single-domain sources.
* **Strict deduplication.** Inspired by Lee et al. [30], which demonstrated the value of deduplication for LLMs, we implement a rigorous deduplication pipeline. We combine both exact and fuzzy deduplication, and use strict settings leading to high removal rates.
* **Neutral filtering.** To avoid introducing further undesirable biases into the model [31, 44], we avoid using ML-based filtering outside of language identification. We stick to simple rules and heuristics, and use only URL filtering for adult content.

### Document preparation: reading data, filtering URLs, extracting text, and language identification

Reading the data.CommonCrawl is available in either WARC (raw HTML response), or WET files (preprocessed to only include plain text). Individual files correspond to a page/document/sample at a given URL. WET files would spare us from running our own HTML extraction; however, in line with previous works [10, 12], we found WET files to include undesirable navigation menus, ads, and other irrelevant texts. Accordingly, we start from raw WARC files, read with the warcio library.

URL filtering.Before undertaking any compute-heavy processing, we perform a first filtering based on the URL alone. This targets fraudulent and/or adult websites (e.g., predominantly pornographic, violent, related to gambling, etc.). We base our filtering on two rules: (1) an aggregated blocklist of 4.6M domains; (2) a URL score, based on the presence of words from a list we curated and weighed by severity. We found that commonly used blocklists include many false positives, such as popular blogging platforms or even pop culture websites. Furthermore, word-based rules (like the one used in

Figure 2: **Subsequent stages of Macrodata Refinement remove nearly 90% of the documents originally in CommonCrawl. Notably, filtering and deduplication each result in a halving of the data available: around 50% of documents are discarded for not being English, 24% of remaining for being of insufficient quality, and 12% for being duplicates. We report removal rate (grey) with respect to each previous stage, and kept rate (shade) overall.**

C4, [9]) can easily result in medical and legal pages being blocked. Our final detailed rules based on this investigation are shared in Appendix I.1. Since we intend RefinedWeb to be used as part of an aggregate dataset along with curated corpora, we also filtered common sources of high-quality data: Wikipedia, arXiv, etc. The detailed list is available in Appendix I.1.3.

Text extraction.We want to extract only the main content of the page, ignoring menus, headers, footers, and ads among others: Lopukhin [45] found that trafilatura [46] was the best non-commercial library for retrieving content from blog posts and news articles. Although this is only a narrow subset of the kind of pages making up CommonCrawl, we found this finding to hold more broadly. We use trafilatura for text extraction, and apply extra formatting via regular expressions: we limit new lines to two consecutive ones, and remove all URLs.

Language identification.We use the fastText language classifier of CCNet [27] at the document-level: it uses characters n-gram and was trained on Wikipedia, supporting 176 languages. We remove documents for which the top language scores below 0.65: this usually corresponds to pages without any natural text. For this paper, we focus on English; RefinedWeb can also be derived for other languages, see Appendix F for details.

The data we retrieve at this stage, called RW-Raw, corresponds to what we can extract with the minimal amount of filtering. At this stage, only 48% of the original documents are left, mostly filtered out by language identification (and a small fraction by failures of the text extraction).

### Filtering: document-wise and line-wise

Repetition removal.Due to crawling errors and low-quality sources, many documents contain repeated sequences: this may cause pathological behavior downstream [47]. The later deduplication stage could catch this, but it is cheaper to catch it earlier document-wise. We implement the heuristics of Rae et al. [12], removing any document with excessive line, paragraph, or n-gram repetitions.

Document-wise filtering.A significant fraction of pages are machine-generated spam, made predominantly of lists of keywords, boilerplate, or sequences of special characters. Such documents are not suitable for language modeling; to filter them out, we adopt the quality filtering heuristics of Rae et al. [12]. These remove outliers in terms of overall length, symbol-to-word ratio, and other criteria ensuring the document is natural language. We note we adapted these filters on a per language basis, as they may result in overfiltering if naively transferred from English to other languages.

Line-wise corrections.Despite the improvements brought forth by using trafilatura instead of relying on preprocessed files, many documents remain interlaced with undesirable lines (e.g., social media counters [3 comments], navigation buttons [Home]). Accordingly, we devised a line-correction filter, targeting these undesirable items leftover from text extraction imperfections. If these corrections remove more than 5% of a document, we remove it entirely. See Appendix I.2 for details.

The data we retrieve at this stage has gone through all of the filtering heuristics in the MDR pipeline. We refer to this dataset as RW-Filtered. Only 23% of the documents of CommonCrawl are left, with around 50% of the documents of RW-Raw removed by the filtering.

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline \multicolumn{2}{l}{**Document preparation**} & \multicolumn{1}{c}{**Filtering**} & \multicolumn{2}{c}{**Deduplication**} \\ \hline
**URL filtering** & **Text extraction** & **Language identification** & **Document-wise filtering** & **Line-wise filtering** & **Deduplication** & **URL deduplication** \\ \hline Aggregated blocklist, URL scoring, common HQ sources blocked & From WARC using warcio, trafficintura & fastText classi-free from CCNet, thresholding on top language score & fastText classi-free from CCNet, COVID and COVID to actions, daily heuristics from massiveWeb counters, etc.) & Remove URLs prediction w/ shift arrays & 
\begin{tabular}{l} Remove URLs \\ revisited across \\ CommonCrawl dumps \\ \end{tabular} \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Macrodata Refinement aggregates best practices from the state-of-the-art and novel approaches (URL scoring, line-wise filtering, etc.) to produce high-quality web data.** On deduplication, we note that MDR is unique in both the scale at which it is performed, and in applying subsequently fuzzy and exact substring methods to improve coverage and scalability.

### Deduplication: fuzzy, exact, and across dumps

After filtering, although data quality has improved, a large fraction of the content is repeated across documents. This may be due to the crawler indirectly hitting the same page multiple times, to boilerplate content being repeated (e.g., licences), or even to plagiarism. These duplicates can strongly impact models, favoring memorization instead of generalization [30; 41]. Since deduplication is expensive, it has seen limited adoption in public datasets [8; 9]. We adopt an aggressive deduplication strategy, combining both fuzzy document matches and exact sequences removal.

Fuzzy deduplication.We remove similar documents by applying MinHash [34]: for each document, we compute a sketch and measure its approximate similarity with other documents, eventually removing pairs with high overlap. MinHash excels at finding templated documents: licenses with only specific entities differing, placeholder SEO text repeated across websites-see examples of the biggest clusters in Appendix J.1. We perform MinHash deduplication using 9,000 hashes per document, calculated over 5-grams and divided into 20 buckets of 450 hashes. We found that using less aggressive settings, such as the 10 hashes of The Pile [10], resulted in lower deduplication rates and worsened model performance. See Appendix I.3.1 for more details about our MinHash setup.

Exact deduplication.Exact substring operates at the sequence-level instead of the document-level, finding matches between strings that are exact token-by-token matches by using a suffix array [33] (e.g., specific disclaimers or notices, which may not compromise the entire document as showcased in Appendix J.2). We remove any match of more than 50 consecutive tokens, using the implementation of Lee et al. [30]. We note that exact substring alters documents, by removing specific spans: we also experimented with dropping entire documents or loss-masking the duplicated strings instead of cutting them, but this didn't result in significant changes in zero-shot performance-see Appendix I.3.2.

URL deduplication.Because of computational constraints, it is impossible for us to perform deduplication directly on RW-Filtered. Instead, we split CommonCrawl into 100 parts, where each part contains a hundredth of each dump, and perform deduplication on individual parts. Most of the larger duplicate clusters (e.g., licences, common spams) will be shared across parts, and effectively removed. However, we found that CommonCrawl dumps had significant overlap, with URLs being revisited across dumps despite no change in content. Accordingly, we keep a list of the URLs of all samples we have kept from each part, and remove them from subsequent parts being processed.

\begin{table}
\begin{tabular}{l l c c c c} \hline \hline
**Tasks** & **Type** & **Random** & **small** & **core** & **main** & **ext** \\ \hline HelaSvag [50] & Sentence completion & 25.0 & ✓ & ✓ & ✓ & ✓ \\ LAMBADA [51] & Sentence completion & 0.0 & ✓ & ✓ & ✓ & ✓ \\ Winograde [52] & Coreference resolution & 50.0 & ✓ & ✓ & ✓ & ✓ \\ PIQA [53] & Multiple-choice question answering & 50.0 & ✓ & ✓ & ✓ & ✓ \\ ARC [54] & Natural language inference & 25.0 & ✓ & ✓ & ✓ & ✓ \\ OpenBookQA [55] & Multiple-choice question answering & 50.0 & ✓ & ✓ & ✓ \\ BooIQ [56] & Multiple-choice question answering & 50.0 & ✓ & & ✓ & ✓ \\ COPA [57] & Sentence completion & 50.0 & & & ✓ & ✓ \\ CB [58] & Natural language inference & 33.3 & & ✓ & ✓ \\ RTE [59] & Natural language inference & 50.0 & & ✓ & ✓ \\ ReCoRD [60] & Question answering & 0.0 & & ✓ & \\ ANL1 [61] & Natural language inference & 33.3 & & & ✓ \\ LogiQA [62] & Multiple-choice question answering & 25.0 & & & ✓ \\ HeadQA [63] & Multiple-choice question answering & 20.0 & & & ✓ \\ MathQA [64] & Multiple-choice question answering & 20.0 & & & ✓ \\ PROST [65] & Paraphrase identification & 50.0 & & & ✓ \\ PubMedQA [66] & Multiple-choice question answering & 50.0 & & & ✓ \\ SciQ [67] & Multiple-choice question answering & 25.0 & ✓ & & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 3: **To evaluate models trained on RefinedWeb and compare to the state-of-the-art, we build four aggregates across 18 tasks on which to measure zero-shot performance. small was built for internal ablations, based on tasks with consistent performance at small scale, core is based on tasks commonly reported for public suites of models [48; 42], main is based on tasks from the GPT-3 and PaLM paper [2; 22], and ext is based on tasks used by the BigScience Architecture and Scaling group [11]. We flag with \(\dagger\) results obtained in an arbitrary evaluation setup, and with \(*\) results obtained with the EAI Harness [49], which we also employ for all our models.**

[MISSING_PAGE_FAIL:7]

### Can web data alone outperform curated corpora?

We endeavour to demonstrate that web data alone can result in models outperforming models trained on curated corpora. To do so, we first perform a small-scale study with 1B and 3B parameters models trained to optimality (27GT and 60GT) on popular web and curated datasets. Then, we scale up to 1B and 7B models trained on 350GT, and compare zero-shot generalization to state-of-the-art models.

Small-scale study.We first consider public web datasets (OSCAR-2019 [8], OSCAR-2022 [75], C4 [9]), The Pile [10] as the most popular publicly available curated dataset, and variations of RefinedWeb (RW-Raw, RW-Filtered, and RW as described in Section 3). All models are trained with the same architecture, for the same amount of tokens, and using the same internal codebase; they are also all evaluated within the same framework-only pretraining datasets differ.

Results averaged on the small aggregate of 6 tasks are presented in Table 4. We observe relatively strong performance of all web datasets compared to The Pile, showcasing that curation is not a silver bullet for performant language models. We find C4 to be a strong pretraining dataset, in line with the findings of Scao et al. [11]-however, The Pile underperforms more in our benchmarks. The disappointing results on OSCAR-22.01 may be due to the dataset being distributed without deduplication by default. Regarding RefinedWeb, both filtering and deduplication significantly improve performance. We also note that a 3B@60GT model trained on OSCAR-22.1 performs _worse_ than a 1B@27GT model trained on RefinedWeb: data alone accounts for a 4x difference in pretraining compute, highlighting that compute budgets alone cannot compensate efficiently for inadequate data.

Full-scale models.We now validate these results with comparisons with state-of-the-art models. We scale our previous experiments by training 1B and 7B models on 350GT; we also train a 1B model on 350GT on The Pile, as a control for the influence of our pretraining setup. We compare with the following models: the GPT-3 series [2], the FairSeq series [76], the GPT-Neo(X)/J models [77; 74; 70], the OPT series [43], the BigScience Architecture and Scaling Pile model [11], PaLM-8B [22], Aleph Alpha Luminous 13B [71], the Pythia series [42], and the Cerebras-GPT series [48]. For GPT-3, we distinguish between results obtained through the API (babbage and curie) with the the EleutherAI LM evaluation harness [49] (*), and results reported in their paper, with a different evaluation setup (\(\dagger\)). For PaLM and OPT, results were obtained also with a different evaluation suite (\(\dagger\)); for most other models they were obtained with the evaluation harness (*), allowing for more direct comparisons.

Results on main-agg are presented in Figure 1, and in Figure 3 for core-agg and ext-agg. We find that open models consistently underperform models trained on private curated corpora, such

Figure 3: **Models trained on \(\copyright\) RefinedWeb alone outperform models trained on curated corpora. Zero-shot performance averaged on our core-agg (left) and ext-agg (right) task aggregates (see Section 4.1 for details, and Figure 1 for results on main-agg). Existing open models fail to match the performance of the original GPT-3 series (left); however, models trained on RefinedWeb significantly outperform models trained on \(\copyright\) The Pile: including our direct comparison model (right), ruling out our pretraining setup as the main source of increased performance. In fact, our RefinedWeb models even match the performance of the \(\copyright\) GPT-3 models.**as GPT-3-even when using a similar evaluation setup. Conversely, models trained on RefinedWeb are able to match the performance of the GPT-3 series using web data alone, even though common high-quality sources used in The Pile are excluded from RefinedWeb (see Table 14 in Appendix). Finally, we note that our internal model trained on The Pile performs in line with the BigScience Architecture and Scaling model; this highlights that our pretraining setup is unlikely to be the main source of increased performance for models trained on RefinedWeb.

**Finding.** Challenging beliefs on data quality, filtered and deduplicated web data _alone_ allows models to match the natural language tasks performance of models trained on curated data.

### Do other corpora benefit from MDR?

Ablating the contributions and evaluating the performance of individual components in the MDR pipeline is difficult: for most heuristics, there is no agreed-upon ground truth, and changes may be too insignificant to result in sufficient zero-shot signal after pretraining. In the first half of Section 4.2, we identified that subsequent stages of RefinedWeb (raw, filtered, final) led to improvements in performance. In this section, we propose to apply independently the filtering and deduplication stages of MDR to popular pretraining datasets, studying whether they generalize widely.

We report results on the small-agg in Table 5. First, we find that improvements from filtering are not systematic. On The Pile, we had to adjust our line length and characters ratio heuristics to avoid expuping books and code. Despite improvements on OSCAR-21.09, C4, and The Pile, our filters worsen performance on OSCAR-22.01; generally, removal rates from filtering are not strongly correlated with downstream accuracy. Conversely, deduplication delivers a steady boost across all datasets, and removal rates are better correlated with zero-shot improvements. OSCAR-21.09 and C4 are already well deduplicated, while The Pile and OSCAR-22.01 exhibit 40-60% duplicates. OSCAR-22.01 is distributed without deduplication by default; for The Pile, this is consistent with the findings of Zhang et al. [43]. Finally, combining filtering and deduplication results in further improvements; although performance is now more uniform across datasets, differences remain, suggesting that flaws in the original text extraction and processing are not fully compensated for.

By processing C4 with MDR, we are able to obtain subsets of data which might slightly outperform RefinedWeb; this combines both the stringent filtering of C4 (e.g., strict NSFW word blocklist, 3-sentence span deduplication) with our own filters and deduplication. While this results in rejection rates that are unacceptable for our target of 3-6 trillions tokens, this is an interesting perspective for shorter runs, which may be able to extract extremely high-quality subsets from large datasets.

**Finding.** While filtering heuristics may require source-dependent tuning, stringent deduplication improves zero-shot performance across datasets consistently.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & **Massive web datasets** & \multicolumn{2}{c}{**Curated**} & **Ours** \\ \hline  & OSCAR-21.09 & OSCAR-22.01 & C4 & \(\blacktriangledown\) Pile & \(\blacktriangledown\) RefinedWeb \\ \hline
**Base** & 55.0\% & 52.7\% & **55.7\%** & 53.4\% & 52.7\% \\
**Filtered** & 55.4\% [+.4] & 52.3\% [+.4] & **56.2\% [+.5]** & 54.2\% [+.8] & 54.3\% [+.1.6] \\ _removal rate_ & -2.5\% & _3.9\%_ & _-16.4\%_ & _-27.1\%_ & -50.8\% \\
**Deduplicated** & 55.6\% [+.6] & 55.6\% [+2.9] & **55.9\% [+.2]** & 54.5\% [+1.1] & \\ _removal rate_ & _-10.8\%_ & _-60.8\%_ & _-7.59\%_ & _-45.3\%_ & \\
**Filt+Dedu.** & 55.5\% [+.5] & 55.4\% [+2.7] & **56.4\% [+.7]** & 55.2\% [+1.8] & 56.2\% [+3.5] \\ _removal rate_ & _-28.2\%_ & _-62.2\%_ & _-17.9\%_ & _-66.0\%_ & _-75.4\%_ \\ \hline \hline \end{tabular}
\end{table}
Table 5: **Although improvements from filtering are not systematic across datasets, deduplication brings a steady performance boost across the board. Zero-shot accuracy averaged on small-agg aggregate; [+x.x] reports absolute gains compared to base, removal rates reported against base. Due to limitations in our pipeline, we cannot apply the deduplication stage independently for RefinedWeb.**

## 5 Limitations

Biases and harmfulness.We conduct an analysis of the toxicity of RefinedWeb in Figure 5 of the Appendix. We find RefinedWeb to be about as toxic as The Pile, based on the definition of toxicity of the Perspective API: "content that is rude or disrespectful". Notably, this definition does not cover social biases or harmfulness. Although it is unlikely that our pipeline introduces further issues than is already documented for popular datasets, we encourage quantitative work on our public extract.

Performance beyond natural language.Our evaluation aggregates are overwhelmingly targeting natural language tasks, and do not include code or mathematics evaluation-which are popular use cases for fully-fledged models. A natural question may be whether web data alone is sufficient to achieve strong code/mathematics performance; we do not think this is the case, and encourage tractionners to combine RefinedWeb with code datasets such as The Stack [78] when training modles. However many of our findings apply equally: notably, Li et al. [79] found that deduplication helped with code data collected from GitHub as well. Broadly speaking, like web data is massively collected from CommonCrawl, code data is usually collected from GitHub, before undergoing extensive filtering and deduplication. This is similar to the spirit of RefinedWeb, and does not rely on a collection of curated sources. Finally, we note that specific domains (e.g., code, technical papers) exist on a spectrum, and that general natural language improvements may benefit technical tasks too: for instance, we find that models trained on RefinedWeb outperform on PubMedQA models trained on The Pile, despite not including any explicit medical data (The Pile includes PubMed).

And beyond pretraining...Our study is strictly limited to language model pretraining, and does not address finetuning existing models. We note the value of high-quality samples for downstream specialization, for instance for improving chattiness or instruction-following capabilities [80].

Multiple epochs.Instead of looking for "unique" tokens for a trillion-scale pretraining dataset, one could simply repeat data over multiple epochs. Popular models like OPT and NeoX-20B train on up to 2 epochs [43; 70], and most curated datasets upsample corpora 2-5 times [2; 10]. However, Hernandez et al. [41] has recently shown that models with 100B+ parameters may be sensitive to even just a few epochs. Orthogonal to our work one could explore tradeoffs in the data-constrained regime: can deduplication help sustain more epochs? Are multiple epochs on higher quality data better than one epoch on lower quality data? See Appendix G.3 for a more in-depth discussion.

Other results on deduplication.Biderman et al. [42] found a limited impact on zero-shot performance from deduplicating The Pile; we discuss in Appendix H.2 and suspect deduplication may be unreasonably effective on web datasets because it predominantly removes low quality content (see Appendix J for top samples). Muennighoff et al. [81] studied scaling laws for multiple epochs, and found that up to four epochs carried limited degradation-however, we note that many of the duplicates we find are present hundred to thousands of time in the raw data, far from this safe regime.

## 6 Conclusion

As LLMs are widely adopted, models trained past the recommendations of scaling laws are bound to become increasingly common to amortize inference costs [7]. This will further drive the need for pretraining datasets with trillions of tokens, an order of magnitude beyond publicly available corpora. We have demonstrated that stringent filtering and deduplication could result in a five trillion tokens web only dataset suitable to produce competitive models, even outperforming LLMs trained on curated corpora. We publicly release a 600GT extract of RefinedWeb, and note that RefinedWeb has already been used to train state-of-the-art language models, such as Falcon-40B [82].

We publicly release the following artefacts:

* **A 600B tokens extract of RefinedWeb:** https://huggingface.co/datasets/titiuae/falcon-refinedweb;
* The 1B and 7B models trained on RefinedWeb in this paper: https://huggingface.co/titiuae/falcon-rw-1b and https://huggingface.co/tiuae/falcon-rw-7b.

## References

* Sevilla et al. [2022] Jaime Sevilla, Lennart Heim, Anson Ho, Tamay Besiroglu, Marius Hobbhahn, and Pablo Villalobos. Compute trends across three eras of machine learning. _arXiv preprint arXiv:2202.05924_, 2022.
* Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* Wei et al. [2022] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. _Transactions on Machine Learning Research_, 2022.
* Hoffmann et al. [2022] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. _arXiv preprint arXiv:2203.15556_, 2022.
* Kaplan et al. [2020] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. _arXiv preprint arXiv:2001.08361_, 2020.
* Villalobos et al. [2022] Pablo Villalobos, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobbhahn, and Anson Ho. Will we run out of data? an analysis of the limits of scaling datasets in machine learning. _arXiv preprint arXiv:2211.04325_, 2022.
* Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* 16, Mannheim, 2019. Leibniz-Institut fur Deutsche Sprache. doi: 10.14618/ids-pub-9021. URL http://nbn-resolving.de/urn:nbn:de:bsz:mh39-90215.
* Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of Machine Learning Research_, 21(140):1-67, 2020. URL http://jmlr.org/papers/v21/20-074.html.
* Gao et al. [2020] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. _arXiv preprint arXiv:2101.00027_, 2020.
* Le Scao et al. [2022] Teven Le Scao, Thomas Wang, Daniel Hesslow, Lucile Saulnier, Stas Bekman, M Saiful Bari, Stella Bideman, Hady Elsahar, Niklas Muennighoff, Jason Phang, et al. What language model to train if you have one million gpu hours? _arXiv preprint arXiv:2210.15424_, 2022.
* Rae et al. [2021] Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, DougFritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis & insights from training gopher. 2021. doi: 10.48550/ARXIV.2112.11446. URL https://arxiv.org/abs/2112.11446.
* Radford et al. [2018] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.
* Devlin et al. [2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 4171-4186, 2019.
* Chelba et al. [2013] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. One billion word benchmark for measuring progress in statistical language modeling. _arXiv preprint arXiv:1312.3005_, 2013.
* Zhu et al. [2015] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In _Proceedings of the IEEE international conference on computer vision_, pages 19-27, 2015.
* Radford et al. [2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. 2019.
* Gokaslan et al. [2019] Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus. http://Skylion007.github.io/OpenWebTextCorpus, 2019.
* Beltagy et al. [2019] Iz Beltagy, Kyle Lo, and Arman Cohan. Scibert: A pretrained language model for scientific text. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 3615-3620, 2019.
* Adiwardana et al. [2020] Daniel Adiwardana, Minh-Thang Luong, David R So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, et al. Towards a human-like open-domain chatbot. _arXiv preprint arXiv:2001.09977_, 2020.
* Thoppilan et al. [2022] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. _arXiv preprint arXiv:2201.08239_, 2022.
* Chowdhery et al. [2022] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022.
* Scao et al. [2022] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, et al. Bloom: A 176b-parameter open-access multilingual language model. _arXiv preprint arXiv:2211.05100_, 2022.
* Liu et al. [2019] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. _arXiv preprint arXiv:1907.11692_, 2019.

* [25] Trieu H Trinh and Quoc V Le. A simple method for commonsense reasoning. _arXiv preprint arXiv:1806.02847_, 2018.
* [26] Julia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab, Daan van Esch, Nasanbayar Ulizii-Orshikh, Allahsera Auguste Tapo, Nishant Subramani, Artem Sokolov, Claytone Sikasote, et al. Quality at a glance: An audit of web-crawled multilingual datasets. _Transactions of the Association for Computational Linguistics_, 10:50-72, 2022.
* [27] Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzman, Armand Joulin, and Edouard Grave. Ccnet: Extracting high quality monolingual datasets from web crawl data. In _Proceedings of the 12th Language Resources and Evaluation Conference_, pages 4003-4012, 2020.
* [28] Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Herve Jegou, and Tomas Mikolov. Fasttext. zip: Compressing text classification models. _arXiv preprint arXiv:1612.03651_, 2016.
* [29] Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and Tomas Mikolov. Learning word vectors for 157 languages. In _Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)_, 2018.
* [30] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 8424-8445, 2022.
* [31] Jesse Dodge, Maarten Sap, Ana Marasovic, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. Documenting large webtext corpora: A case study on the colossal clean crawled corpus. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 1286-1305, 2021.
* [32] Hugo Laurencon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo Gonzalez Ponferrada, Huu Nguyen, et al. The bigscience roots corpus: A 1.6 tb composite multilingual dataset. In _Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2022.
* [33] Udi Manber and Gene Myers. Suffix arrays: a new method for on-line string searches. _Journal on Computing_, 22(5):935-948, 1993.
* [34] Andrei Z Broder. On the resemblance and containment of documents. In _Proceedings. Compression and Complexity of Sequences 1997_, pages 21-29. IEEE, 1997.
* [35] Moses S Charikar. Similarity estimation techniques from rounding algorithms. In _Proceedings of the thiry-fourth annual ACM symposium on Theory of computing_, pages 380-388, 2002.
* [36] Wei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi Liao, Zhiwei Wang, Xin Jiang, ZhenZhang Yang, Kaisheng Wang, Xiaoda Zhang, et al. Pangu-alpha: Large-scale autoregressive pretrained chinese language models with auto-parallel computation. _arXiv preprint arXiv:2104.12369_, 2021.
* [37] Amro Kamal Mohamed Abbas, Kushal Tirumala, Daniel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Data-efficient learning at web-scale through semantic deduplication. In _ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models_, 2023.
* [38] Miliadis Allamanis. The adverse effects of code duplication in machine learning models of code. In _Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software_, pages 143-153, 2019.

* Carlini et al. [2022] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying memorization across neural language models. _arXiv preprint arXiv:2202.07646_, 2022.
* Carlini et al. [2021] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. In _30th USENIX Security Symposium (USENIX Security 21)_, pages 2633-2650, 2021.
* Hernandez et al. [2022] Danny Hernandez, Tom Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan Hume, et al. Scaling laws and interpretability of learning from repeated data. _arXiv preprint arXiv:2205.10487_, 2022.
* Biderman et al. [2023] Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. _arXiv preprint arXiv:2304.01373_, 2023.
* Zhang et al. [2022] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_, 2022.
* Welbl et al. [2021] Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor, Lisa Anne Hendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen Huang. Challenges in detoxifying language models. In _Findings of the Association for Computational Linguistics: EMNLP 2021_, pages 2447-2469, 2021.
* Lopukhin [2019] Konstantin Lopukhin. Evaluating quality of article body extraction for commercial services and open-source libraries. https://github.com/scrapinghub/article-extraction-benchmark, 2019.
* Barbaresi [2021] Adrien Barbaresi. Trafilatura: A Web Scraping Library and Command-Line Tool for Text Discovery and Extraction. In _Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations_, pages 122-131. Association for Computational Linguistics, 2021. URL https://aclanthology.org/2021.acl-demo.15.
* Holtzman et al. [2019] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In _International Conference on Learning Representations_, 2019.
* Dey et al. [2023] Nolan Dey, Gurpreet Gosal, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin Tom, Joel Hestness, et al. Cerebras-gpt: Open compute-optimal language models trained on the cerebras wafer-scale cluster. _arXiv preprint arXiv:2304.03208_, 2023.
* Gao et al. [2021] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September 2021. URL https://doi.org/10.5281/zenodo.5371628.
* Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 4791-4800, 2019.
* Paperno et al. [2016] Denis Paperno, German Kruszewski, Angeliki Lazaridou, Ngoc-Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The lambada dataset: Word prediction requiring a broad discourse context. In _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1525-1534, 2016.

* [52] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. _Communications of the ACM_, 64(9):99-106, 2021.
* [53] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 7432-7439, 2020.
* [54] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. _arXiv preprint arXiv:1803.05457_, 2018.
* [55] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 2381-2391, 2018.
* [56] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In _Proceedings of NAACL-HLT_, pages 2924-2936, 2019.
* [57] Andrew Gordon, Zornitsa Kozareva, and Melissa Roemmele. Semeval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In
* _SEM 2012: The First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012)_, pages 394-398, 2012.
* [58] Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The commitmentbank: Investigating projection in naturally occurring discourse. In _proceedings of Sinn und Bedeutung_, volume 23, pages 107-124, 2019.
* [59] Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth. Recognizing textual entailment: Rational, evaluation and approaches-erratum. _Natural Language Engineering_, 16(1):105-105, 2010.
* [60] Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. Record: Bridging the gap between human and machine commonsense reading comprehension. _arXiv preprint arXiv:1810.12885_, 2018.
* [61] Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial nli: A new benchmark for natural language understanding. _arXiv preprint arXiv:1910.14599_, 2019.
* [62] Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: a challenge dataset for machine reading comprehension with logical reasoning. In _Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence_, pages 3622-3628, 2021.
* [63] David Vilares and Carlos Gomez-Rodriguez. Head-qa: A healthcare dataset for complex reasoning. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 960-966, 2019.
* [64] Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 2357-2367, 2019.

* Aroca-Ouellette et al. [2021] Stephane Aroca-Ouellette, Cory Paik, Alessandro Roncone, and Katharina Kann. Prost: Physical reasoning about objects through space and time. In _Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021_, pages 4597-4608, 2021.
* Jin et al. [2019] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. Pubmedqa: A dataset for biomedical research question answering. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 2567-2577, 2019.
* Welbl et al. [2017] Johannes Welbl, Nelson F Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. In _Proceedings of the 3rd Workshop on Noisy User-generated Text_, pages 94-106, 2017.
* Tay et al. [2021] Yi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won Chung, Sharan Narang, Dani Yogatama, Ashish Vaswani, and Donald Metzler. Scale efficiently: Insights from pretraining and finetuning transformers. In _International Conference on Learning Representations_, 2021.
* Wang et al. [2022] Thomas Wang, Adam Roberts, Daniel Hesslow, Teven Le Scao, Hyung Won Chung, Iz Beltagy, Julien Launay, and Colin Raffel. What language model architecture and pretraining objective work best for zero-shot generalization? In _International Conference on Machine Learning_, 2022.
* Black et al. [2022] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source autoregressive language model. _Challenges & Perspectives in Creating Large Language Models_, page 95, 2022.
* Alpha [2023] Aleph Alpha. Luminous: performance benchmarks. _arXiv preprint arXiv:1810.12885_, 2023. URL https://www.aleph-alpha.com/pdf/2023_02_AA_Benchmarks_doc.pdf.
* Press et al. [2021] Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In _International Conference on Learning Representations_, 2021.
* Dao et al. [2022] Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness. In _Advances in Neural Information Processing Systems_, 2022.
* Wang and Komatsuzaki [2021] Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.
* 9, Mannheim, 2021. Leibniz-Institut fur Deutsche Sprache. doi: 10.14618/ids-pub-10468. URL https://nbn-resolving.org/urn:nbn:de:bsz:mh39-104688.
* Artetxe et al. [2021] Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, et al. Efficient large scale language modeling with mixtures of experts. _arXiv preprint arXiv:2112.10684_, 2021.
* Black et al. [2021] Sid Black, Gao Leo, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021. URL https://doi.org/10.5281/zenodo.5297715. If you use this software, please cite it using these metadata.
* Black et al. [2021]* [78] Denis Koccetkov, Raymond Li, LI Jia, Chenghao Mou, Yacine Jernite, Margaret Mitchell, Carlos Munoz Ferrandis, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, et al. The stack: 3 tb of permissively licensed source code. _Transactions on Machine Learning Research_, 2022.
* [79] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Koccetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcode: may the source be with you! _arXiv preprint arXiv:2305.06161_, 2023.
* [80] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. _arXiv preprint arXiv:2306.11644_, 2023.
* [81] Niklas Muennighoff, Alexander M. Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models, 2023.
* [82] Ebtesam Almazrouei, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. Falcon-40b: an open large language model with state-of-the-art performance. 2023.
* [83] Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, et al. Datasets: A community library for natural language processing. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pages 175-184, 2021.
* [84] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, et al. Transformers: State-of-the-art natural language processing. In _Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations_, pages 38-45, 2020.
* [85] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daume Iii, and Kate Crawford. Datasheets for datasets. _Communications of the ACM_, 64(12):86-92, 2021.
* [86] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In _Proceedings of the conference on fairness, accountability, and transparency_, pages 220-229, 2019.
* [87] David M. Eberhard, Gary F. Simons, and Charles D. Fennig. _Ethnologue: Languages of the World_. SIL International, Dallas, TX, USA, twenty-sixth edition, 2023.
* [88] Ge Yang, Edward Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tuning large neural networks via zero-shot hyperparameter transfer. _Advances in Neural Information Processing Systems_, 34:17084-17097, 2021.
* [89] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mt5: A massively multilingual pre-trained text-to-text transformer. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 483-498, 2021.
* [90] Julien Abadji, Pedro Ortiz Suarez, Laurent Romary, and Benoit Sagot. Towards a Cleaner Document-Oriented Multilingual Crawled Corpus. _arXiv e-prints_, art. arXiv:2201.06642, January 2022.
* [91] Jan Pomikalek. Juster. 2011.

* [92] Dick Sites. Compact language detector 2. _Software available at https://github. com/CLD2Owners/cld2 (last updated on August 2015)_, 2013.
* [93] Laura Hanu and Unitary team. Detoxify. Github. https://github.com/unitaryai/detoxify, 2020.
* [94] P. Jaccard. The distribution of the flora in the alpine zone.1. _New Phytologist_, 11:37-50, 1912.