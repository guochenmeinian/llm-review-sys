# Provably Fast Finite Particle Variants of SVGD via Virtual Particle Stochastic Approximation

Aniket Das

Google Research

Bangalore, India

ketd@google.com

&Dheeraj Nagaraj

Google Research

Bangalore, India

dheerajnagaraj@google.com

###### Abstract

Stein Variational Gradient Descent (SVGD) is a popular particle-based variational inference algorithm with impressive empirical performance across various domains. Although the population (i.e, infinite-particle) limit dynamics of SVGD is well characterized, its behavior in the finite-particle regime is far less understood. To this end, our work introduces the notion of _virtual particles_ to develop novel stochastic approximations of population-limit SVGD dynamics in the space of probability measures, that are exactly realizable using finite particles. As a result, we design two computationally efficient variants of SVGD, namely VP-SVGD and GB-SVGD, with provably fast finite-particle convergence rates. Our algorithms can be viewed as specific random-batch approximations of SVGD, which are computationally more efficient than ordinary SVGD. We show that the \(n\) particles output by VP-SVGD and GB-SVGD, run for \(T\) steps with batch-size \(K\), are at-least as good as i.i.d samples from a distribution whose Kernel Stein Discrepancy to the target is at most \(O({d^{1/3}}/{(KT)^{1/6}})\) under standard assumptions. Our results also hold under a mild growth condition on the potential function, which is much weaker than the isoperimetric (e.g. Poincare Inequality) or information-transport conditions (e.g. Talagrand's Inequality \(\mathsf{T}_{1}\)) generally considered in prior works. As a corollary, we analyze the convergence of the empirical measure (of the particles output by VP-SVGD and GB-SVGD) to the target distribution and demonstrate a _double exponential improvement_ over the best known finite-particle analysis of SVGD. Beyond this, our results present the _first known oracle complexities for this setting with polynomial dimension dependence_, thereby completely eliminating the curse of dimensionality exhibited by previously known finite-particle rates.

## 1 Introduction

Sampling from a distribution over \(\mathbb{R}^{d}\) whose density \(\pi^{\star}(\mathbf{x})\propto\exp(-F(\mathbf{x}))\) is known only upto a normalizing constant, is a fundamental problem in machine learning [52, 20], statistics [41, 36], theoretical computer science [28, 17] and statistical physics [39, 15]. A popular approach to this is the Stein Variational Gradient Descent (SVGD) algorithm introduced by Liu and Wang [31], which uses a positive definite kernel \(k\) to evolve a set of \(n\) interacting particles \((\mathbf{x}_{t}^{(i)})_{i\in[n],t\in\mathbb{N}}\) as follows:

\[\mathbf{x}_{t+1}^{(i)}\leftarrow\mathbf{x}_{t}^{(i)}-\frac{\gamma}{n}\sum_{j= 1}^{n}\left[k(\mathbf{x}_{t}^{(i)},\mathbf{x}_{t}^{(j)})\nabla F(\mathbf{x}_{ t}^{(j)})-\nabla_{2}k(\mathbf{x}_{t}^{(i)},\mathbf{x}_{t}^{(j)})\right] \tag{1}\]

SVGD exhibits remarkable empirical performance in a variety of Bayesian inference, generative modeling and reinforcement learning tasks [31, 51, 54, 22, 50, 34, 45, 19] and usually converges rapidly to the target density while using only a few particles, often outperforming Markov Chain Monte Carlo (MCMC) methods. However, in contrast to its wide practical applicability, theoreticalanalysis of the behavior SVGD is a relatively unexplored problem. Prior works on the analysis of SVGD [26; 14; 30; 42; 7] mainly consider the population limit, where the number of particles \(n\rightarrow\infty\). These works assume that the initial distribution of the (infinite number of) particles has a finite KL divergence to the target \(\pi^{\star}\) and subsequently, interpret the dynamics of population-limit SVGD as projected gradient descent updates for the KL divergence on the space of probability measures, equipped with the Wasserstein geometry. Under appropriate assumptions on the target density, one can then use the theory of Wasserstein Gradient Flows to establish non-asymptotic (in time) convergence of population-limit SVGD to \(\pi^{\star}\) in the Kernel Stein Discrepancy (KSD) metric.

While the framework of Wasserstein Gradient Flows suffices to explain the behavior of SVGD in the population limit, the same techniques are insufficient to effectively analyze SVGD in the finite-particle regime. This is primarily due to the fact that the empirical measure \(\hat{\mu}^{(n)}\) of a finite number of particles does not admit a density with respect to the Lebesgue measure, and thus, its KL divergence to the target is always infinite (i.e. \(\text{KL}\left(\hat{\mu}^{(n)}\|\pi^{\star}\right)=\infty\)). In such a setting, a direct analysis of the dynamics of finite-particle SVGD becomes prohibitively difficult due to complex inter-particle dependencies. To the best of our knowledge, the pioneering work of Shi and Mackey [43] is the only result that obtains an explicit convergence rate of finite-particle SVGD by tracking the deviation between the law of \(n\)-particle SVGD and that of its population-limit. To this end, Shi and Mackey [43] show that for subgaussian target densities, the empirical measure of \(n\)-particle SVGD converges to \(\pi^{\star}\) at a rate of \(O(\sqrt{\frac{\text{poly}(d)}{\log\log n^{6(1/d)}}})\) in KSD 1. The obtained convergence rate is quite slow and fails to adequately explain the impressive practical performance of SVGD.

Footnote 1: We explicate the dimension dependence in Shi and Mackey [43] by closely following their analysis

Our work takes a starkly different approach to this problem and deliberately deviates from tracking population-limit SVGD using a finite number of particles. Instead, we directly analyze the dynamics of KL divergence along a carefully constructed trajectory in the space of distributions. To this end, our proposed algorithm, Virtual Particle SVGD (VP-SVGD) devises an _unbiased stochastic approximation (in the space of measures) of the population-limit dynamics of SVGD_. We achieve this by considering additional particles called _virtual particles_2 which evolve in time but aren't part of the output (i.e. _real particles_). These virtual particles are used only to compute information about the current population-level distribution of the real particles, and enable exact implementation of our stochastic approximation to population-limit SVGD, while using only a finite number of particles.

Footnote 2: (roughly) analogous to virtual particles in quantum field theory that enable interactions between real particles

Our analysis is similar in spirit to non-asymptotic analyses of stochastic gradient descent (SGD) that generally do not attempt to track gradient descent (analogous to population-limit SVGD in this case), but instead directly track the evolution of the objective function along the SGD trajectory using appropriate stochastic descent lemmas [24; 21; 11]. The key feature of our proposed stochastic approximation is the fact that it can be implemented using only a finite number of particles. This allows us to design faster variants of SVGD with provably fast finite-particle convergence.

### Contributions

**VP-SVGD and GB-SVGD** We propose two variants of SVGD that enjoy provably fast finite-particle convergence guarantees to the target distribution: Virtual Particle SVGD (VP-SVGD in Algorithm 1) and Global Batch SVGD (GB-SVGD in Algorithm 2). VP-SVGD is a conceptually elegant stochastic approximation (in the space of probability measures) of population-limit SVGD, and GB-SVGD is a practically efficient version of SVGD which achieves good empirical performance. Our analysis of GB-SVGD builds upon that of VP-SVGD. When the potential \(F\) is smooth and satisfies a quadratic growth condition (which holds under subgaussianity of \(\pi^{\star}\), a common assumption in prior works [42; 43]), we show that the \(n\) particles output by \(T\) steps of our algorithms, run with batch-size \(K\), are at least as good as i.i.d draws from a distribution whose Kernel Stein Discrepancy to \(\pi^{\star}\) is at most \(O(d^{\nicefrac{{1}}{{3}}}/(KT)^{\nicefrac{{1}}{{6}}})\). Our results also hold under a mild subquadratic growth condition for \(F\), which is much weaker than isoperimetric (e.g. Poincare Inequality) or information-transport (e.g. Talagrand's Inequality \(\mathsf{T}_{1}\)) assumptions generally considered in the sampling literature [47; 42; 43; 8; 2].

**State-of-the-art Finite Particle Guarantees** As corollaries of the above result, we establish that _VP-SVGD and GB-SVGD exhibit the best known finite-particle guarantees in the literature which significantly outperform that of prior works_. Our results are summarized in Table 1. In particular,under subgaussianity of the target distribution \(\pi^{\star}\), we show that the empirical measure of the \(n\) particles output by VP-SVGD converges to \(\pi^{\star}\) in KSD at a rate of \(O((\nicefrac{{d}}{{n}})^{\nicefrac{{1}}{{4}}}+(\nicefrac{{d}}{{n}})^{\nicefrac{{ 1}}{{2}}})\). Similarly, the empirical measure of the \(n\) output particles of GB-SVGD converges to \(\pi^{\star}\) at a KSD rate of \(O(\nicefrac{{d^{3}}}{{n^{\nicefrac{{1}}{{12}}}}}+(\nicefrac{{d}}{{n}})^{ \nicefrac{{1}}{{2}}})\). Both these results represent a **double exponential improvement** over the \(O(\frac{\mathsf{poly}(d)}{\sqrt{\log\log n^{\mathsf{c}(\nicefrac{{1}}{{4}})}}})\) KSD rate of \(n\)-particle SVGD obtained by Shi and Mackey [43], which, to our knowledge, is the best known prior result for SVGD in the finite particle regime. When benchmarked in terms of gradient oracle complexity, i.e., the number of evaluations of \(\nabla F\) required by an algorithm to achieve \(\mathsf{KSD}_{\pi^{\star}}(\cdot||\pi^{\star})\leq\epsilon\), we demonstrate that for subgaussian \(\pi^{\star}\), the oracle complexity of VP-SVGD is \(O(\nicefrac{{d^{4}}}{{\epsilon^{12}}})\) while that of GB-SVGD is \(O(\nicefrac{{d^{6}}}{{\epsilon^{18}}})\). To the best of our knowledge, our result presents the _first known oracle complexity guarantee with polynomial dimension dependence_, and consequently, does not suffer from a curse of dimensionality unlike prior works. Furthermore, as discussed above, the conditions under which our result holds is far weaker than subgaussianity of \(\pi^{\star}\), and as such, includes sub-exponential targets and beyond. In particular, _our guarantees for sub-exponential target distributions are (to the best of our knowledge) the first of its kind._

**Computational Benefits:** VP-SVGD and GB-SVGD can be viewed as specific random batch approximations of SVGD. Our experiments (Section 8) show that GB-SVGD obtains similar performance as SVGD but requires fewer computations. In this context, a different kind of random batch method that divides the particles into random subsets of interacting particles, has been proposed by Li et al. [29]. However, the objective in Li et al. [29] is to approximate finite-particle SVGD dynamics using the random batch method, instead of analyzing convergence of the random batch method itself. Beyond this, their guarantees also suffer from an exponential dependence on the time \(T\). As explained below, their approach is also conceptually different from our method since we use the _same_ random batch to evolve _every_ particle, allowing us to interpret this as a stochastic approximation in the space of distributions instead of in the path space.

### Technical Challenges

We resolve the following important conceptual challenges, which may be of independent interest.

**Stochastic Approximation in the Space of Probability Measures** Stochastic approximations are widely used in optimization, control and sampling [27; 52; 23]. In the context of sampling, stochastic approximations are generally implemented in path space, e.g., Stochastic Gradient Langevin Dynamics (SGLD) [52] takes a random batch approximation of the drift term via the update \(\mathbf{x}_{t+1}=\mathbf{x}_{t}-\frac{\eta}{K}\sum_{j=0}^{K-1}\nabla f( \mathbf{x}_{t},\xi_{j})+\sqrt{2\eta}\epsilon_{t},\ \epsilon_{t}\sim\mathcal{N}( 0,\mathbf{I})\) where \(\mathbb{E}[f(\mathbf{x}_{t},\xi_{j})|\mathbf{x}_{t}]=F(\mathbf{x}_{t})\). Such stochastic approximations are then analyzed using the theory of stochastic processes over \(\mathbb{R}^{d}\)[12; 40; 55; 25]. However, when viewed in the space of probability measures (i.e, \(\mu_{t}=\mathrm{Law}(\mathbf{x}_{t})\)), the time-evolution

\begin{table}
\begin{tabular}{|l|c|c|c|c|} \hline
**Result** & **Algorithm** & **Assumption** & **Rate** & \begin{tabular}{c} **Oracle** \\ **Complexity** \\ \end{tabular} \\ \hline \multirow{2}{*}{Korba et al. [26]} & Population Limit & \begin{tabular}{c} Uniformly Bounded \\ \(\mathsf{KSD}_{\pi^{\star}}(\bar{\mu}_{t}||\pi^{\star})\) \\ \end{tabular} & 
\begin{tabular}{c} Not \\ Implementable \\ \end{tabular} \\ \hline \multirow{2}{*}{Salim et al. [42]} & Population Limit & \multirow{2}{*}{Sub-gaussian \(\pi^{\star}\)} & \(\frac{\nicefrac{{d^{3/2}}}{{\sqrt{2}}}}{\mathsf{Implementable}}\) & Not \\  & SVGD & & & \\ \hline Shi and Mackey [43] & SVGD & Sub-gaussian \(\pi^{\star}\) & \(\frac{\mathsf{poly}(d)}{\sqrt{\log\log n^{\mathsf{c}(\nicefrac{{1}}{{4}})}}}\) & \(\frac{\mathsf{poly}(d)}{\epsilon^{2}}\epsilon^{\Theta(\nicefrac{{d^{6}}}{{ \epsilon^{29}}})^{\nicefrac{{1}}{{2}}}}\) \\ \hline
**Ours, Corollary 1** & **VP-SVGD** & **Sub-gaussian \(\pi^{\star}\)** & \(\left(\nicefrac{{d}}{{n}}\right)^{\nicefrac{{1}}{{4}}}+\left(\nicefrac{{d}}{{n}} \right)^{\nicefrac{{1}}{{2}}}\) & \(\nicefrac{{d}}{{\epsilon^{12}}}\) \\ \hline
**Ours, Corollary 1** & **GB-SVGD** & **Sub-gaussian \(\pi^{\star}\)** & \(\nicefrac{{d^{3}}}{{n^{\nicefrac{{12}}{{2}}}}}+\left(\nicefrac{{d}}{{n}} \right)^{\nicefrac{{1}}{{2}}}\) & \(\nicefrac{{d}}{{\epsilon^{18}}}\) \\ \hline
**Ours, Corollary 1** & **VP-SVGD** & Sub-exponential \(\pi^{\star}\) & \(\frac{\nicefrac{{d^{3/2}}}{{n^{\nicefrac{{12}}{{2}}}}}}{\mathsf{Implementable}}\) & \(\nicefrac{{d^{6}}}{{\epsilon^{16}}}\) \\ \hline
**Ours, Corollary 1** & **GB-SVGD** & Sub-exponential \(\pi^{\star}\) & \(\frac{\nicefrac{{d^{3/2}}}{{n^{\nicefrac{{12}}{{2}}}}}}{\mathsf{Implementable}}\) & \(\nicefrac{{d^{6}}}{{\epsilon^{16}}}\) \\ \hline \end{tabular}
\end{table}
Table 1: Comparison of our results with prior works. \(d\), \(T\), and \(n\) denote the dimension, no. of iterations and no. of output particles respectively. Oracle Complexity denotes number of evaluations of \(\nabla F\) needed to achieve \(\mathsf{KSD}_{\pi^{\star}}(\cdot||\pi^{\star})\leq\epsilon\) and Rate denotes convergence rate w.r.t KSD metric. Note that: 1. Population Limit SVGD is not implementable as it requires infinite particles 2. The uniformly bounded \(\mathsf{KSD}_{\pi^{\star}}(\bar{\mu}_{t}||\pi^{\star})\) assumption cannot be verified apriori and is much stronger than subgaussianity (see [42] Lemma C.1)

[MISSING_PAGE_FAIL:4]

Background on Population-Limit SVGD

We briefly introduce the analysis of population-limit SVGD using the theory of Wasserstein Gradient Flows and refer the readers to Korba et al. [26] and Salim et al. [42] for a detailed treatment.

The space \(\mathcal{P}_{2}(\mathbb{R}^{d})\) equipped with the 2-Wasserstein metric \(\mathcal{W}_{2}\) is known as the Wasserstein space, which admits the following Riemannian structure : For any \(\mu\in\mathcal{P}_{2}(\mathbb{R}^{d})\), the tangent space \(T_{\mu}\mathcal{P}_{2}(\mathbb{R}^{d})\) can be identified with the Hilbert space \(L^{2}(\mu)\). We can then define differentiable functionals \(\mathcal{L}:\mathcal{P}_{2}(\mathbb{R}^{d})\rightarrow\mathbb{R}\) and compute their Wasserstein gradients, denoted as \(\nabla_{\mathcal{W}_{2}}\mathcal{L}\). Note that the target \(\pi^{\star}\) is the unique minimizer over \(\mathcal{P}_{2}(\mathbb{R}^{d})\) for the functional \(\mathcal{L}[\mu]=\mathsf{KL}\left(\mu\middle|\!\middle|\pi^{\star}\right)\). The Wasserstein Gradient of \(\mathcal{L}[\mu]\) is \(\nabla_{\mathcal{W}_{2}}\mathcal{L}[\mu]=\nabla_{\mathbf{x}}\log(\frac{ \mathrm{d}\mu}{\mathrm{d}\pi^{\star}}(\mathbf{x}))\)[1]. This powerful machinery has served as a backbone for the analysis of algorithms such as LMC [53, 3, 2] and population-limit SVGD [14, 26, 42, 45, 7].

The updates of population-limit SVGD can be viewed as Projected Gradient Descent in the Wasserstein space. Recall from Section 2 that the function \(h_{\mu}(\mathbf{x})=P_{\mu}(\nabla\log(\frac{\mathrm{d}\mu}{\mathrm{d}\pi^{\star }}))(\mathbf{x})=\int h(\mathbf{x},\mathbf{y})\mathrm{d}\mu(\mathbf{y})\). Let \(\hat{\mu}_{t}^{n}\) denote the empirical measures of the SVGD particles \((\mathbf{x}_{t}^{(i)})_{i\in[n]}\) at timestep \(t\). We note that the SVGD updates in (1) can be recast as \(\hat{\mu}_{t^{n}+1}^{n}=(I-\gamma h_{\hat{\mu}_{t^{n}}})_{\#}\hat{\mu}_{n}^{n}\). In the limit of infinite particles \(n\rightarrow\infty\), suppose the empirical measure \(\hat{\mu}_{t}^{n}\) converges to the population measure \(\bar{\mu}_{t}\). In this population limit, the updates of SVGD can be expressed as,

\[\bar{\mu}_{t+1}=(I-h_{\bar{\mu}_{t}})_{\#}\,\bar{\mu}_{t}=\left(I-\gamma P_{ \bar{\mu}_{t}}\left(\nabla\log(\frac{\mathrm{d}\bar{\mu}_{t}}{d\pi^{\star}}) \right)\right)_{\#}\bar{\mu}_{t}=(I-\gamma P_{\bar{\mu}_{t}}\left(\nabla_{ \mathcal{W}_{2}}\mathsf{KL}\left(\bar{\mu}_{t}\middle|\!\middle|\pi^{\star} \right)\right))_{\#}\,\bar{\mu}_{t}\]

Recall from Section 2 that \(P_{\bar{\mu}_{t}}:L^{2}(\bar{\mu}_{t})\rightarrow\mathcal{H}\) is the Hilbert adjoint of \(i_{\bar{\mu}_{t}}\). Since \(\mathcal{H}\subset L^{2}(\bar{\mu}_{t})\), the updates of SVGD in the population limit can be seen as Projected Wasserstein Gradient Descent for \(\mathcal{L}[\mu]=\mathsf{KL}\left(\mu\middle|\!\middle|\pi^{\star}\right)\), with the Wasserstein Gradient at each step being projected onto the RKHS \(\mathcal{H}\). Assuming \(\mathsf{KL}\left(\bar{\mu}_{0}\middle|\!\middle|\pi^{\star}\right)<\infty\), convergence of population limit SVGD is then established by tracking the evolution of \(\mathsf{KL}\left(\bar{\mu}_{t}\middle|\!\middle|\pi^{\star}\right)\) under appropriate structural assumptions (such as subgaussianity) on \(\pi^{\star}\).

## 4 Algorithm and Intuition

In this section, we derive VP-SVGD (Algorithm 1), and build upon it to obtain GB-SVGD. Consider a countably infinite collection of particles \(\mathbf{x}_{0}^{(l)}\in\mathbb{R}^{d},\ l\in\mathbb{N}\cup\{0\}\), sampled i.i.d from a measure \(\mu_{0}\), having a density w.r.t. the Lebesgue measure. By the strong law of large numbers, the empirical measure of \(\mathbf{x}_{0}^{(l)}\) is almost surely equal to \(\mu_{0}\) (see Dudley [13, Theorem 11.4.1]). Let batch size \(K\in\mathbb{N}\) denote the batch size, and \(\mathcal{F}_{t}\) denote the filtration \(\mathcal{F}_{t},\ t\geq 0\) as \(\mathcal{F}_{t}=\sigma(\{\mathbf{x}_{0}^{(l)}\mid l\leq Kt-1\}),\ \forall\,t\in \mathbb{N}\), with \(\mathcal{F}_{0}\) being the trivial \(\sigma\) algebra. For ease of exposition, we discuss the case of \(K=1\) in this section and present a complete derivation for arbitrary \(K\geq 1\) in Appendix C. Recall from Section 3 that the updates of population-limit SVGD in \(\mathcal{P}_{2}(\mathbb{R}^{d})\) can be expressed as follows:

\[\bar{\mu}_{t+1}=(I-\gamma h_{\bar{\mu}_{t}})_{\#}\bar{\mu}_{t} \tag{2}\]

We aim to design a stochastic approximation in \(\mathcal{P}_{2}(\mathbb{R}^{d})\) for the updates (2), such that it admits a finite-particle realization. To this end, we propose the following dynamics in \(\mathbb{R}^{d}\)

\[\mathbf{x}_{t+1}^{(s)}=\mathbf{x}_{t}^{(s)}-\gamma h(\mathbf{x}_{t}^{(s)}, \mathbf{x}_{t}^{(t)}),\quad s\in\mathbb{N}\cup\{0\} \tag{3}\]

Now, for each time-step \(t\), we focus on the time evolution of the particles \((\mathbf{x}_{t}^{(l)})_{l\geq t}\) (called the _lower triangular evolution_). From (3), we observe that for any \(t\in\mathbb{N}\) and \(l\geq t\), \(\mathbf{x}_{t}^{(l)}\) depends only on \(\mathbf{x}_{0}^{(0)},\ldots,\mathbf{x}_{0}^{(t-1)},\mathbf{x}_{0}^{(l)}\). Hence, there exists a deterministic, measurable function \(H_{t}\) such that:

\[\mathbf{x}_{t}^{(l)}=H_{t}(\mathbf{x}_{0}^{(0)},\ldots,\mathbf{x}_{0}^{(t-1)}, \mathbf{x}_{0}^{(l)})\,;\quad\text{for every }l\geq t \tag{4}\]

Since \(\mathbf{x}_{0}^{(0)},\ldots,\mathbf{x}_{0}^{(t-1)},\mathbf{x}_{0}^{(l)}\overset{ i.i.d.}{\sim}\mu_{0}\), we conclude from (4) that \((\mathbf{x}_{t}^{(l)})_{l\geq t}\) are i.i.d when conditioned on \(\mathbf{x}_{0}^{(0)},\ldots,\mathbf{x}_{0}^{(t-1)}\). To this end, we define the random measure \(\mu_{t}|\mathcal{F}_{t}\) as the law of \(\mathbf{x}_{t}^{(t)}\) conditioned on \(\mathcal{F}_{t}\), i.e., \(\mu_{t}|\mathcal{F}_{t}\) is a probability kernel \(\mu_{t}(\cdot\,;\mathbf{x}_{0}^{(0)},\ldots,\mathbf{x}_{0}^{(t-1)})\), where \(\mu_{0}|\mathcal{F}_{0}:=\mu_{0}\). By the strong law of large numbers, \(\mu_{t}|\mathcal{F}_{t}\) is equal to the empirical measure of \((\mathbf{x}_{t}^{(l)})_{l\geq t}\) conditioned on \(\mathcal{F}_{t}\). We will use \(\mu_{t}|\mathcal{F}_{t}\) and \(\mu_{t}(\cdot\,;\mathbf{x}_{0}^{(0)},\ldots,\mathbf{x}_{0}^{(t-1)})\) interchangeably.

Define the random function \(g_{t}:\mathbb{R}^{d}\rightarrow\mathbb{R}^{d}\) as \(g_{t}(\mathbf{x}):=h(\mathbf{x},\mathbf{x}_{t}^{(t)})\). From (4), we note that \(g_{t}\) is \(\mathcal{F}_{t+1}\) measurable. From (3), we infer that the particles satisfy the following relation:

\[\mathbf{x}_{t+1}^{(s)}=(I-\gamma g_{t})(\mathbf{x}_{t}^{(s)}),\quad s\geq t+1\]

Recall that \(\mathbf{x}_{t+1}^{(s)}|\mathbf{x}_{0}^{(0)},\ldots,\mathbf{x}_{0}^{(t)}\sim\mu_ {t+1}|\mathcal{F}_{t+1}\) for any \(s\geq t+1\). Furthermore, from Equation (4), we note that for \(s\geq t+1\), \(\mathbf{x}_{t}^{(s)}\) depends only on \(\mathbf{x}_{0}^{(0)},\ldots,\mathbf{x}_{0}^{(t-1)}\) and \(\mathbf{x}_{0}^{(s)}\). Hence, we conclude that \(\mathrm{Law}(\mathbf{x}_{t}^{(s)}|\mathbf{x}_{0}^{(0)},\ldots,\mathbf{x}_{0}^ {(t)})=\mathrm{Law}(\mathbf{x}_{t}^{(s)}|\mathbf{x}_{0}^{(0)},\ldots,\mathbf{x} _{0}^{(t-1)})=\mu_{t}|\mathcal{F}_{t}\). With this insight, the dynamics of the lower-triangular evolution in \(\mathcal{P}_{2}(\mathbb{R}^{d})\) that the following holds almost surely:

\[\mu_{t+1}|\mathcal{F}_{t+1}=(I-\gamma g_{t})_{\#}\mu_{t}|\mathcal{F}_{t} \tag{5}\]

\(\mathbf{x}_{t}^{(t)}|\mathcal{F}_{t}\sim\mu_{t}|\mathcal{F}_{t}\) implies \(\mathbb{E}[g_{t}(\mathbf{x})|\mathcal{F}_{t}]=h_{\mu_{t}|\mathcal{F}_{t}}( \mathbf{x})\). Thus _lower triangular dynamics_ (5) is a stochastic approximation in \(\mathcal{P}_{2}(\mathbb{R}^{d})\) to the population limit of SVGD (2). Setting the batch size to general \(K\) and tracking the evolution of the first \(KT+n\) particles, we obtain VP-SVGD (Algorithm 1).

**Input**: Number of steps \(T\), number of output particles \(n\), batch size \(K\), Initial positions \(\mathbf{x}_{0}^{(0)},\ldots,\mathbf{x}_{0}^{(n+KT-1)\ i.i.d.}\ \mu_{0}\), Kernel \(k\), step size \(\gamma\).

```
1:for\(t\in\{0,\ldots,T-1\}\)do
2:for\(s\in\{0,\ldots,KT+n-1\}\)do
3:\(\mathbf{x}_{t+1}^{(s)}=\mathbf{x}_{t}^{(s)}-\frac{\gamma}{K}\sum_{l=0}^{K-1} [k(\mathbf{x}_{t}^{(s)},\mathbf{x}_{t}^{(tK+l)})\nabla F(\mathbf{x}_{t}^{(tK+ l)})-\nabla_{2}k(\mathbf{x}_{t}^{(s)},\mathbf{x}_{t}^{(tK+l)})]\)
4:endfor
5:endfor
6: Draw \(S\) uniformly at random from \(\{0,\ldots,T-1\}\)
7:Output \((\mathbf{y}^{(0)},\ldots,\mathbf{y}^{(n-1)})=(\mathbf{x}_{S}^{(TK)},\ldots, \mathbf{x}_{S}^{(TK+n-1)})\)
```

**Algorithm 1** Virtual Particle SVGD (VP-SVGD)

**Virtual Particles** In Algorithm 1, \((\mathbf{x}_{t}^{(l)})_{KT\leq l\leq KT+n-1}\) are the _real particles_ which constitute the output. \((\mathbf{x}_{t}^{(l)})_{l<KT}\) are _virtual particles_ which propagate information about the probability measure \(\mu_{t}|\mathcal{F}_{t}\) to enable computation of \(g_{t}\), an unbiased estimate of the projected Wasserstein gradient \(h_{\mu_{t}|\mathcal{F}_{t}}\).

**VP-SVGD as SVGD Without Replacement** VP-SVGD is a without-replacement random-batch approximation of SVGD (1), where a different batch is used across timesteps, but the same batch is across particles given a fixed timestep. With i.i.d. initialization, picking the 'virtual particles' in a fixed order or from a random permutation does not change the evolution of the real particles. With this insight, we design GB-SVGD (Algorithm 2) where we consider \(n\) particles _and_ output \(n\) particles (instead of wasting \(KT\) particles as 'virtual particles') via a random-batch approximation of SVGD.

**Input**: \(\#\) of time steps \(T\), \(\#\) of particles \(n\), \(\mathbf{x}_{0}^{(0)},\ldots,\mathbf{x}_{0}^{(n-1)\ i.i.d.}\ \mu_{0}\), Kernel \(k\), step size \(\gamma\), Batch size \(K\), Sampling method \(\in\) {with replacement, without replacement}
1:for\(t\in\{0,\ldots,T-1\}\)do
2:\(\mathcal{K}_{t}\leftarrow\) random subset of \([n]\) of size \(K\) (via. sampling method)
3:for\(s\in\{0,\ldots,n-1\}\)do
4:\(\mathbf{x}_{t+1}^{(s)}=\mathbf{x}_{t}^{(s)}-\frac{\gamma}{K}\sum_{r\in\mathcal{ K}_{t}}[k(\mathbf{x}_{t}^{(s)},\mathbf{x}_{t}^{(r)})\nabla F(\mathbf{x}_{t}^{(r)} )-\nabla_{2}k(\mathbf{x}_{t}^{(s)},\mathbf{x}_{t}^{(r)})]\)
5:endfor
6: Draw \(S\) uniformly at random from \(\{0,1,\ldots,T-1\}\)
7:Output \((\tilde{\mathbf{y}}^{(0)},\ldots,\tilde{\mathbf{y}}^{(n-1)})=(\mathbf{x}_{S}^{(0 )},\ldots,\mathbf{x}_{S}^{(n-1)})\) ```

**Algorithm 2** Global Batch SVGD (GB-SVGD)

In Algorithm 2, with replacement sampling means selecting a batch of \(K\) particles i.i.d. from the uniform distribution over \([n]\). Without replacement sampling means fixing a random permutation \(\sigma\) over \(\{0,\ldots,n-1\}\) and selecting the batches in the order specified by the permutation.

## 5 Assumptions

In this section, we discuss the key assumptions required for our analysis of VP-SVGD and GB-SVGD. Our first assumption is smoothness of \(F\), which is standard in optimization and sampling.

**Assumption 1** (L-Smoothness).: \(\nabla F\) _exists and is \(L\) Lipschitz. Moreover \(\|\nabla F(0)\|\leq\sqrt{L}\)._

It is easy find a point such that \(\|\nabla F(\mathbf{x}^{\star})\|\leq\sqrt{L}\) (e.g., using \(\Theta(1)\) gradient descent steps [37]) and center the initialization at \(\mu_{0}\) at \(\mathbf{x}^{\star}\). For clarity, we take \(\mathbf{x}^{\star}=0\) without loss of generality. We now impose the following growth condition on \(F\).

**Assumption 2** (Growth Condition).: _There exist \(\alpha,d_{1},d_{2}>0\) such that_

\[F(\mathbf{x})\geq d_{1}\|\mathbf{x}\|^{\alpha}-d_{2}\quad\forall\mathbf{x}\in \mathbb{R}^{d}\]

Note that Assumption 1 ensures \(\alpha\leq 2\). Assumption 2 is essentially a tail decay assumption on the target density \(\pi^{\star}(\mathbf{x})\propto e^{-F(\mathbf{x})}\). In fact, as we shall show in Appendix B, Assumption 2 ensures that the tails of \(\pi^{\star}\) decay as \(\propto e^{-\|\mathbf{x}\|^{\alpha}}\). Consequently, Assumption 2 holds with \(\alpha=2\) when \(\pi^{\star}\) is subgaussian and with \(\alpha=1\) when \(\pi^{\star}\) is subexponential. Subgaussianity is equivalent to \(\pi^{\star}\) satisfying the \(\mathsf{T}_{1}\) inequality [5; 49], commonly assumed in prior works on SVGD [42; 43]. We also note that subexponentiality is implied when \(\pi^{\star}\) satisfies the Poincare Inequality [4, Section 4], which is considered a mild condition in the sampling literature [47; 8; 2; 12; 7]. This makes Assumption 1 significantly weaker than the isoperimetric or information-transport assumptions considered in prior works.

Next, we impose a mild assumption on the RKHS of the kernel \(k\), which has been used by several prior works [42; 26; 45; 43].

**Assumption 3** (Bounded RKHS Norm).: _For any \(\mathbf{y}\in\mathbb{R}^{d}\), \(k(\cdot,\mathbf{y})\) satisfies \(\|k(\cdot,\mathbf{y})\|_{\mathcal{H}_{0}}\leq B\). Furthermore, \(\nabla_{2}k(\cdot,\mathbf{y})\in\mathcal{H}\) and \(\|\nabla_{2}k(\cdot,\mathbf{y})\|_{\mathcal{H}}\leq B\)_

Assumption 3 ensures that the adjoint operator \(P_{\mu}\), used in Sections 2 and 3, is well-defined. We also make the following assumptions on the kernel \(k\), which is satisfied by a large class of standard kernels such as Radial Basis Function kernels and Matern kernels of order \(\geq\sqrt[3]{2}\).

**Assumption 4** (Kernel Decay).: _The kernel \(k\) satisfies the following for constants \(A_{1},A_{2},A_{3}>0\)._

\[0\leq k(\mathbf{x},\mathbf{y})\leq\tfrac{A_{1}}{1+\|\mathbf{x}-\mathbf{y}\|^{ 2}},\quad\quad\|\nabla_{2}k(\mathbf{x},\mathbf{y})\|\leq A_{2},\quad\quad\| \nabla_{2}k(\mathbf{x},\mathbf{y})\|^{2}\leq A_{3}k(\mathbf{x},\mathbf{y})\]

Finally, we make the following mild assumption on the initialization.

**Assumption 5** (Initialization).: _The initial distribution \(\mu_{0}\) is such that \(\mathsf{KL}\,(\mu_{0}\|\pi^{\star})<\infty\). Furthermore, \(\mu_{0}\) is supported in \(\mathcal{B}(R)\), the \(\ell_{2}\) ball of radius \(R\)_

Since prior works usually assume Gaussian initialization [42; 47], Assumption 5 may seem slightly non-standard. However, this is not a drawback. In fact, whenever \(R=\Theta(\sqrt{d}+\mathsf{polylog}(\nicefrac{{n}}{{\delta}}))\), Gaussian initialization can be made indistinguishable from \(\mathsf{Uniform}(\mathcal{B}(R))\) initialization, with probability at least \(1-\delta\), via a coupling argument. To this end, we impose Assumption 5 for ease of exposition and our results can be extended to consider Gaussian initialization. In Appendix B we show that taking \(R=\sqrt{\nicefrac{{d}}{{L}}}\) and \(\mu_{0}=\mathsf{Uniform}(\mathcal{B}(R))\) suffices to ensure \(\mathsf{KL}\,(\mu_{0}\|\pi^{\star})=O(d)\).

## 6 Results

### Vp-Svgd

Our first result, proved in Appendix C, shows that the law of the _real particles of_ VP-SVGD, when conditioned on the virtual particles, is close to \(\pi^{\star}\) in KSD. As a consequence, it shows that the particles output by VP-SVGD are i.i.d. samples from a random probability measure \(\bar{\mu}(\cdot;\mathbf{x}_{0}^{(0)},\ldots,\mathbf{x}_{0}^{(KT-1)},S)\) which is close to \(\pi^{\star}\) in KSD. We also present a high-probability version of this result in Appendix C.

**Theorem 1** (Convergence of VP-SVGD).: _Let \(\mu_{t}\) be as defined in Section 4. Let Assumptions 1 2, 3, 4, and 5 be satisfied and let \(\gamma\leq\min\{\nicefrac{{1}}{{2A_{1}L}},\nicefrac{{1}}{{(4+L)B}}\}\). There exist \((\zeta_{i})_{0\leq i\leq 3}\) depending polynomiallyon \(A_{1},A_{2},A_{3},B,L,d_{1},d_{2}\) for any fixed \(\alpha\in(0,2]\), such that whenever \(\gamma\xi\leq\frac{1}{2B}\), with \(\xi=\zeta_{0}+\zeta_{1}(\gamma T)^{\nicefrac{{1}}{{\alpha}}}+\zeta_{2}(\gamma^{2 }T)^{\nicefrac{{1}}{{\alpha}}}+\zeta_{3}R^{\nicefrac{{2}}{{\alpha}}}\), the following holds:_

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\mathsf{KSD}^{2}_{\pi^{*}}(\mu_{t}| \mathcal{F}_{t}||\pi^{*})\right]\leq\frac{2\mathsf{KL}\left(\mu_{0}|\!|\!|\!|\! |\pi^{*}\right)}{\gamma T}+\frac{\gamma B(4+L)\xi^{2}}{K}\]

_Define the probability kernel \(\bar{\mu}(\cdot\;;\gamma)\) as follows: For any \(x_{\tau}\in\mathbb{R}^{d}\), \(\tau\in(KT)\) and \(s\in(T)\), \(\bar{\mu}(\cdot\;;x_{0},\ldots,x_{KT-1},s):=\mu_{s}(\cdot\;;x_{0},\ldots,x_{Ks- 1})\) and \(\bar{\mu}(\cdot\;;x_{0},\ldots,x_{KT-1},s=0):=\mu_{0}(\cdot)\). Conditioned on \(\mathbf{x}^{(0)}_{\tau}=x_{\tau},\;S=s\) for every \(\tau\in(KT)\), the outputs \(\mathbf{y}^{(0)},\ldots,\mathbf{y}^{(n-1)}\) of VP-SVGD are i.i.d samples from \(\bar{\mu}(\cdot\;;x_{0},\ldots,x_{KT-1},s)\). Furthermore,_

\[\mathbb{E}[\mathsf{KSD}^{2}_{\pi^{*}}(\bar{\mu}(\cdot\;;\mathbf{x}^{(0)}_{0}, \ldots,\mathbf{x}^{(KT-1)}_{0},S)||\pi^{*})]\leq\frac{2\mathsf{KL}\left(\mu_{0 }|\!|\!|\!|\pi^{*}\right)}{\gamma T}+\frac{\gamma B(4+L)\xi^{2}}{K}\]

**Convergence Rates** Taking \(\mu_{0}=\mathsf{Uniform}(\mathcal{B}(R))\) with \(R=\sqrt{\nicefrac{{d}}{{L}}}\) ensures \(\mathsf{KL}\left(\mu_{0}|\mathcal{F}_{0}|\!|\!|\!|\!|\pi^{*}\right)=O(d)\) (see Appendix B). Under this setting, choosing \(\gamma=O(\frac{(Kd)^{\eta}}{T^{\nicefrac{{1}}{{\alpha}}}})\) ensures that \(\mathbb{E}[\mathsf{KSD}^{2}_{\pi^{*}}(\bar{\mu}||\pi^{*})]=O(\frac{d^{ \nicefrac{{1}}{{\alpha}}}}{(KT)^{\nicefrac{{1}}{{\alpha}}}})\) where \(\eta=\frac{\alpha}{2(1+\alpha)}\). Thus, for \(\alpha=2\), (i.e, sub-Gaussian \(\pi^{*}\)), \(\mathsf{KSD}^{2}=O(\frac{d^{\nicefrac{{1}}{{\alpha}}}}{(KT)^{\nicefrac{{1}}{{ \alpha}}}})\). For \(\alpha=1\) (i.e, sub-Exponential \(\pi^{*}\)), the rate (in squared KSD) becomes \(O(\frac{d^{\nicefrac{{3}}{{\alpha}}}}{(KT)^{\nicefrac{{1}}{{\alpha}}}})\). To the best of our knowledge, our convergence guarantee for sub-exponential \(\pi^{*}\) is the first of its kind.

**Comparison with Prior Works** Salim et al. [42] analyzes population-limit SVGD for subgaussian \(\pi^{*}\), obtaining \(\mathsf{KSD}^{2}=O(\nicefrac{{d^{\nicefrac{{3}}{{\alpha}}}}}{{T}}/r)\) rate. We note that population-limit SVGD (which requires infinite particles) is not implementable whereas VP-SVGD is an implementable algorithm whose outputs are conditionally i.i.d samples from a distribution with guaranteed convergence to \(\pi^{*}\).

### Gb-Svgd

We now use VP-SVGD as the basis to analyze GB-SVGD. Assume \(n>KT\). Then, with probability at least \(1-\nicefrac{{K^{2}T^{2}}}{{n}}\) (for with-replacement sampling) and \(1\) (for without-replacement sampling), the random batches \(\mathcal{K}_{t}\) in GB-SVGD (Algorithm 2) are disjoint and contain distinct elements. When conditioned on this event \(\mathcal{E}\), we note that the \(n-KT\) particles that were not included in any random batch \(\mathcal{K}_{t}\) evolve exactly like the \(n\) real particles of VP-SVGD. With this insight, we show that, conditioned on \(\mathcal{E}\), the outputs of VP-SVGD and GB-SVGD can be coupled such that the first \(n-KT\) particles of both the algorithms are exactly equal. This allows us to derive the following squared KSD bound between the empirical measures of the outputs of VP-SVGD and GB-SVGD. The proof of this result is presented in Appendix D.

**Theorem 2** (Ksd Bounds for Gb-Svgd).: _Let \(n>KT\) and let \(\mathbf{Y}=(\mathbf{y}^{(0)},\ldots,\mathbf{y}^{(n-1)})\) and \(\bar{\mathbf{Y}}=(\bar{\mathbf{y}}^{(0)},\ldots,\bar{\mathbf{y}}^{(n-1)})\) denote the outputs of VP-SVGD and GB-SVGD respectively. Moreover, let \(\hat{\mu}^{(n)}=\frac{1}{n}\sum_{i=0}^{n-1}\delta_{\mathbf{y}^{(i)}}\) and \(\hat{\nu}^{(n)}=\frac{1}{n}\sum_{i=0}^{n-1}\delta_{\hat{\mathbf{y}}^{(i)}}\) denote their respective empirical measures. Under the assumptions and parameter settings of Theorem 1, there exists a coupling of \(\mathbf{Y}\) and \(\bar{\mathbf{Y}}\) such that the following holds:_

\[\mathbb{E}[\mathsf{KSD}^{2}_{\pi^{*}}(\hat{\nu}^{(n)}||\hat{\mu}^{(n)})]\leq \begin{cases}\frac{2K^{2}T^{2}\xi^{2}}{n^{2}}&\text{(without replacement sampling)}\\ \frac{2K^{2}T^{2}\xi^{2}}{n^{2}}\left(1-\frac{K^{2}T^{2}}{n}\right)+\frac{2K^{2}T^ {2}\xi^{2}}{n}&\text{(with replacement sampling)}\end{cases} \tag{6}\]

### Convergence of the Empirical Measure to the Target

As a corollary of Theorem 1 and Theorem 2, we show that the empirical measure of the output of VP-SVGD and GB-SVGD rapidly converges to \(\pi^{*}\) in KSD. We refer to Appendix E for the full statement and proof.

**Corollary 1** (**Vp-Svgd and Gb-Svgd: Fast Finite Particle Rates)**.: _Let the assumptions and parameter settings of Theorem 1 be satisfied. Let \(\hat{\mu}^{(n)}\) be the empirical measure of the \(n\) particles output by VP-SVGD, run with run with \(KT=d^{\frac{n}{2+\alpha}}\), \(R=\sqrt{\nicefrac{{d}}{{L}}}\) and appropriately chosen \(\gamma\). Then:_

\[\mathbb{E}[\mathsf{KSD}^{2}_{\pi^{*}}(\hat{\mu}^{(n)}||\pi^{*})]\leq O(\frac{ \frac{2}{d^{\nicefrac{{2}}{{\alpha}}}}}{n^{\nicefrac{{2}}{{\alpha}}}}+\frac{d^{ \nicefrac{{2}}{{\alpha}}}}{n})\]_Similarly, let \(\hat{\nu}^{(n)}\) be the empirical measure of the output of GB-SVGD without replacement, run with \(KT=\sqrt{n}\), \(R=\sqrt{\nicefrac{{d}}{{L}}}\) and appropriately chosen \(\gamma\). Then, the following holds:_

\[\mathbb{E}[\mathsf{KSD}_{\pi^{\star}}^{2}(\hat{\nu}^{(n)}||\pi^{\star})]\leq O( \frac{\nicefrac{{d^{\nicefrac{{2}}}}}{{{n}}}}{n}+\frac{d\frac{1}{1+\alpha}}{n \frac{1+2\alpha}{2(1+\alpha)}}+\frac{d\frac{2+\alpha}{2(1+\alpha)}}{n^{\frac{ \alpha}{4(1+\alpha)}}})\]

**Comparison to Prior Work** For subgaussian \(\pi^{\star}\) (i.e. \(\alpha=2\)), VP-SVGD has a finite-particle rate of \(\mathbb{E}[\mathsf{KSD}_{\pi^{\star}}(\hat{\mu}^{(n)}||\pi^{\star})]=O(( \nicefrac{{d}}{{n}})^{\nicefrac{{1}}{{4}}}+(\nicefrac{{d}}{{n}})^{\nicefrac{{1 }}{{2}}})\) while that of GB-SVGD is \(\mathbb{E}[\mathsf{KSD}_{\pi^{\star}}(\hat{\nu}^{(n)}||\pi^{\star})]=O( \nicefrac{{d^{\nicefrac{{1}}}}}{{{n}^{\nicefrac{{1}}{{2}}}}}/n^{1/2}+\nicefrac {{(d/n)^{1}}}{{2}})\). Both these rates are a _double exponential improvement_ over the \(\tilde{O}(\frac{\mathsf{poly}(d)}{\sqrt{\log\log n^{\Theta(1/d)}}})\) KSD rate obtained by Shi and Mackey [43] for SVGD with subgaussian \(\pi^{\star}\). For subexponential \(\pi^{\star}\) (i.e. \(\alpha=1\)) the KSD rate of VP-SVGD is \(O(\frac{d^{\nicefrac{{1}}{{3}}}}{n^{\nicefrac{{1}}{{6}}}}+\frac{d}{n^{\nicefrac {{1}}{{2}}}})\) while that of GB-SVGD is \(O(\frac{d^{\nicefrac{{3}}{{8}}}}{n^{\nicefrac{{1}}{{16}}}}+\frac{d}{n^{\nicefrac {{1}}{{2}}}})\). To our knowledge, both these results are the first of their kind.

**Oracle Complexity** As illustrated in Section E.3, for subgaussian \(\pi^{\star}\), the oracle complexity of VP-SVGD to achieve \(\epsilon\)-convergence in KSD is \(O(\nicefrac{{d^{\nicefrac{{1}}}}}{{{\epsilon^{12}}}})\) and that of GB-SVGD is \(O(\nicefrac{{d^{\nicefrac{{6}}}}}{{{\epsilon^{18}}}})\). To our knowledge, these results are the _first known oracle complexities for this problem with polynomial dimension dependence_, and significantly improve upon the \(O(\frac{\mathsf{poly}(d)}{\epsilon^{2}}e^{\Theta(\epsilon\mathsf{poly}(d)/ \epsilon^{2}}))\) oracle complexity of SVGD as implied by Shi and Mackey [43]. For subexponential \(\pi^{\star}\), the oracle complexity of VP-SVGD is \(O(\nicefrac{{d^{\nicefrac{{6}}}}}{{{\epsilon^{16}}}})\) and that of GB-SVGD is \(O(\nicefrac{{d^{\nicefrac{{9}}}}}{{{\epsilon^{24}}}})\).

## 7 Proof Sketch

We now present a sketch of our analysis. As shown in Section 4, the particles \((\mathbf{x}_{t}^{(l)})_{l\geq Kt}\) are i.i.d conditioned on the filtration \(\mathcal{F}_{t}\), and the random measure \(\mu_{t}|\mathcal{F}_{t}\) is the law of \((\mathbf{x}_{t}^{(kl)})\) conditioned on \(\mathbf{x}_{0}^{(0)},\ldots,\mathbf{x}_{0}^{(Kt-1)}\). Moreover, from equation (5), we know that \(\mu_{t}|\mathcal{F}_{t}\) is a stochastic approximation of population limit SVGD dynamics, i.e., \(\mu_{t+1}|\mathcal{F}_{t+1}=(I-\gamma g_{t})_{\#}\mu_{t}|\mathcal{F}_{t}\). Lemma 1 (similar to Salim et al. [42, Proposition 3.1] and Korba et al. [26, Proposition 5]) shows that under appropriate conditions, the KL divergence between \(\mu_{t}\big{|}\mathcal{F}_{t}\) and \(\pi^{\star}\) satisfies a (stochastic) descent lemma. Hence \(\mu_{t}|\mathcal{F}_{t}\) admits a density and \(\mathsf{KL}\left(\mu_{t}|\mathcal{F}_{t}\|\pi^{\star}\right)\) is almost surely finite.

**Lemma 1** (Descent Lemma for \(\mu_{t}|\mathcal{F}_{t}\)).: _Let Assumptions 1, 3 and 5 be satisfied and let \(\beta>1\) be an arbitrary constant. On the event \(\gamma\|g_{t}\|_{\mathcal{H}}\leq\frac{\beta-1}{\beta B}\), the following holds almost surely_

\[\mathsf{KL}\left(\mu_{t+1}|\mathcal{F}_{t+1}\|\pi^{\star}\right)\leq\mathsf{ KL}\left(\mu_{t}|\mathcal{F}_{t}\|\pi^{\star}\right)-\gamma\left\langle h_{\mu_{t} |\mathcal{F}_{t}},g_{t}\right\rangle_{\mathcal{H}}+\frac{\gamma^{2}(\beta^{2} +L)B}{2}\|g_{t}\|_{\mathcal{H}}^{2}\]

Lemma 1 is analogous to the noisy descent lemma which is used in the analysis of SGD for smooth functions. Notice that \(\mathbb{E}[g_{t}|\mathcal{F}_{t}]=h_{\mu_{t}|\mathcal{F}_{t}}\) (when interpreted as a Gelfand-Pettis integral [46], as discussed in Appendix B and Appendix C) and hence in expectation, the KL divergence decreases in time. In order to apply Lemma 1, we establish an almost-sure bound on \(\|g_{t}\|_{\mathcal{H}}\) below.

**Lemma 2**.: _Let Assumptions 1, 2, 3, 4 and 5 hold. For \(\gamma\leq\nicefrac{{1}}{{2A_{1}L}}\), the following holds almost surely,_

\[\|g_{t}\|_{\mathcal{H}}\leq\xi=\zeta_{0}+\zeta_{1}(\gamma T)\nicefrac{{1}}{{ \alpha}}+\zeta_{2}(\gamma^{2}T)\nicefrac{{1}}{{\alpha}}+\zeta_{3}R^{\nicefrac{{2 }}{{\alpha}}}\]

_where \(\zeta_{0},\zeta_{1},\zeta_{2}\) and \(\zeta_{3}\) which depend polynomially on \(A_{1},A_{2},A_{3},B,d_{1},d_{2},L\) for any fixed \(\alpha\)._

Let \(K=1\) for clarity. To prove Lemma 2, we first note via smoothness of \(F(\cdot)\) and Assumption 3 that \(\|g_{t}\|_{\mathcal{H}}\leq C_{0}\|\mathbf{x}_{t}^{(t)}\|+C_{1}\), and then bound \(\|\mathbf{x}_{t}^{(t)}\|\). Now, \(g_{s}(\mathbf{x})=k(\mathbf{x},\mathbf{x}_{s}^{(s)})\nabla F(\mathbf{x}_{s}^{(s)} )-\nabla_{2}k(\mathbf{x},\mathbf{x}_{s}^{(s)})\). When \(\|\mathbf{x}_{s}^{(s)}-\mathbf{x}\|\) is large, \(\|g_{s}(\mathbf{x})\|\) is small due to decay assumptions on the kernel (Assumption 4) implying that the particle does not move much. When \(\mathbf{x}_{s}^{(s)}\approx\mathbf{x}\), we have \(g_{s}(\mathbf{x})\approx k(\mathbf{x},\mathbf{x}_{s}^{(s)})\nabla F(\mathbf{x})- \nabla_{2}k(\mathbf{x},\mathbf{x}_{s}^{(s)})\) and \(k(\mathbf{x},\mathbf{x}_{s}^{(s)})\geq 0\). This is approximately a gradient descent update on \(F(\cdot)\) along with a bounded term \(\nabla_{2}k(\mathbf{x},\mathbf{x}_{s}^{(s)})\). Thus, the value of \(F(\mathbf{x}_{t}^{(l)})\) cannot grow too large after \(T\) iterations. By Assumption 2, \(F(\mathbf{x}_{t}^{(l)})\) being small implies that \(\|\mathbf{x}_{t}^{(l)}\|\) is small.

Equipped with Lemma 2, we set the step-size \(\gamma\) to ensure that the descent lemma (Lemma 1) always holds. The remainder of the proof involves unrolling through Lemma 1 by taking iterated expectations on both sides. To this end we control \(\left\langle h_{\mu_{t}|\mathcal{F}_{t}},g_{t}\right\rangle_{\mathcal{H}}\) and \(\|g_{t}\|_{\mathcal{H}}^{2}\) in expectation, in Lemma 3.

**Lemma 3**.: _Let Assumptions 1,2,3,4,5 hold and \(\xi\) be as defined in Lemma 2. Then, for \(\gamma\leq\nicefrac{{1}}{{2A_{1}}}L\),_

\[\mathbb{E}\left[\left\langle h_{\mu_{t}|\mathcal{F}_{t}},g_{t}\right\rangle_{ \mathcal{H}}|\mathcal{F}_{t}\right]=\|h_{\mu_{t}|\mathcal{F}_{t}}\|_{\mathcal{ H}}^{2}\quad\text{and}\quad\mathbb{E}[\|g_{t}\|_{\mathcal{H}}^{2}]\leq\nicefrac{{ \xi^{2}}}{{K}}+\|h_{\mu_{t}|\mathcal{F}_{t}}\|_{\mathcal{H}}^{2}\]

## 8 Experiments

We compare the performance of GB-SVGD and SVGD. We take \(n=100\) and use the Laplace kernel with \(h=1\) for both. We pick the stepsize \(\gamma\) by a grid search for each algorithm. Additional details are presented in Appendix G. We observe that SVGD takes fewer iterations to converge, but the compute time for GB-SVGD is lower. This is similar to the typical behavior of stochastic optimization algorithms like SGD.

**Sampling from Isotropic Gaussian (Figure 1):** As a sanity check, we set \(\pi^{\star}=\mathcal{N}(0,\mathbf{I})\) with \(d=5\). We pick \(K=10\) for GB-SVGD. The metric of convergence is MMD with respect to the empirical measure of 1000 i.i.d. sampled Gaussians.

**Bayesian Logistic Regression (Figure 2)** We consider the Covertype dataset which contains \(\sim 580,000\) data points with \(d=54\). We consider the same priors suggested in Gershman et al. [16] and implemented in Liu and Wang [31]. We take \(K=40\) for GB-SVGD. For both VP-SVGD and GB-SVGD, we use AdaGrad with momentum to set the step-sizes as per Liu and Wang [31]

## 9 Conclusion

We develop two computationally efficient variants of SVGD with provably fast convergence guarantees in the finite-particle regime, and present a wide range of improvements over prior work. A promising avenue of future work could be to establish convergence guarantees for SVGD with general non-logconcave targets, as was considered in recent works on LMC and SGLD [2, 12]. Other important avenues include establishing minimax lower bounds for SVGD and related particle-based variational inference algorithms. Beyond this, we also conjecture that the rates of GB-SVGD can be improved even in the regime \(n\ll KT\). However, we believe this requires new analytic tools.

Figure 1: Gaussian Experiment Comparing SVGD and GB-SVGD averaged over 10 experiments.

Figure 2: Covertype Experiment, averaged over 50 runs. The error bars represent 95% CI.

## Acknowledgements

We thank Jiaxin Shi, Lester Mackey and the anonymous reviewers for their helpful feedback. We are particularly grateful to Lester Mackey for providing insightful pointers on the properties of Kernel Stein Discrepancy, which greatly helped us in removing the curse of dimensionality from our finite-particle convergence guarantees.

## References

* Ambroso et al. [2005] L. Ambrosio, N. Gigli, and G. Savare. _Gradient flows: in metric spaces and in the space of probability measures_. Springer Science & Business Media, 2005.
* Balasubramanian et al. [2022] K. Balasubramanian, S. Chewi, M. A. Erdogdu, A. Salim, and S. Zhang. Towards a theory of non-log-concave sampling:first-order stationarity guarantees for langevin monte carlo. In P.-L. Loh and M. Raginsky, editors, _Proceedings of Thirty Fifth Conference on Learning Theory_, volume 178 of _Proceedings of Machine Learning Research_, pages 2896-2923. PMLR, 02-05 Jul 2022. URL [https://proceedings.mlr.press/v178/balasubramanian22a.html](https://proceedings.mlr.press/v178/balasubramanian22a.html).
* Bernton [2018] E. Bernton. Langevin monte carlo and jko splitting. In _Conference On Learning Theory_, pages 1777-1798. PMLR, 2018.
* Bobkov and Ledoux [1997] S. Bobkov and M. Ledoux. Poincare's inequalities and talagrand's concentration phenomenon for the exponential distribution. _Probability Theory and Related Fields_, 107:383-400, 1997.
* Bobkov and Gotze [1999] S. G. Bobkov and F. Gotze. Exponential integrability and transportation cost related to logarithmic sobolev inequalities. _Journal of Functional Analysis_, 163(1):1-28, 1999.
* Carmeli et al. [2010] C. Carmeli, E. De Vito, A. Toigo, and V. Umanita. Vector valued reproducing kernel hilbert spaces and universality. _Analysis and Applications_, 8(01):19-61, 2010.
* Chewi et al. [2020] S. Chewi, T. Le Gouic, C. Lu, T. Maunu, and P. Rigollet. Svgd as a kernelized wasserstein gradient flow of the chi-squared divergence. _Advances in Neural Information Processing Systems_, 33:2098-2109, 2020.
* Chewi et al. [2022] S. Chewi, M. A. Erdogdu, M. Li, R. Shen, and S. Zhang. Analysis of langevin monte carlo from poincare to log-sobolev. In P.-L. Loh and M. Raginsky, editors, _Proceedings of Thirty Fifth Conference on Learning Theory_, volume 178 of _Proceedings of Machine Learning Research_, pages 1-2. PMLR, 02-05 Jul 2022. URL [https://proceedings.mlr.press/v178/chewi22a.html](https://proceedings.mlr.press/v178/chewi22a.html).
* Chwialkowski et al. [2016] K. Chwialkowski, H. Strathmann, and A. Gretton. A kernel test of goodness of fit. In _International conference on machine learning_, pages 2606-2615. PMLR, 2016.
* Conway [2019] J. B. Conway. _A course in functional analysis_, volume 96. Springer, 2019.
* Das et al. [2022] A. Das, B. Scholkopf, and M. Muehlebach. Sampling without replacement leads to faster rates in finite-sum minimax optimization. _Advances in Neural Information Processing Systems_, 2022.
* Das et al. [2023] A. Das, D. Nagaraj, and A. Raj. Utilising the clt structure in stochastic gradient based sampling: Improved analysis and faster algorithms. In _Conference on Learning Theory_, 2023.
* Dudley [2018] R. M. Dudley. _Real analysis and probability_. CRC Press, 2018.
* Duncan et al. [2019] A. Duncan, N. Nusken, and L. Szpruch. On the geometry of stein variational gradient descent. _arXiv preprint arXiv:1912.00894_, 2019.
* El Alaoui et al. [2022] A. El Alaoui, A. Montanari, and M. Sellke. Sampling from the sherrington-kirkpatrick gibbs measure via algorithmic stochastic localization. In _2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS)_, pages 323-334. IEEE, 2022.
* Gershman et al. [2012] S. Gershman, M. D. Hoffman, and D. M. Blei. Nonparametric variational inference. In _Proceedings of the 29th International Conference on International Conference on Machine Learning_, 2012.

* [17] S. Gopi, Y. T. Lee, and D. Liu. Private convex optimization via exponential mechanism. In _Conference on Learning Theory_, pages 1948-1989. PMLR, 2022.
* [18] J. Gorham, A. Raj, and L. Mackey. Stochastic stein discrepancies. _Advances in Neural Information Processing Systems_, 33:17931-17942, 2020.
* [19] T. Haarnoja, H. Tang, P. Abbeel, and S. Levine. Reinforcement learning with deep energy-based policies. In _International conference on machine learning_, pages 1352-1361. PMLR, 2017.
* [20] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 33:6840-6851, 2020.
* [21] P. Jain, D. M. Nagaraj, and P. Netrapalli. Making the last iterate of sgd information theoretically optimal. _SIAM Journal on Optimization_, 31(2):1108-1130, 2021.
* [22] P. Jaini, L. Holdijk, and M. Welling. Learning equivariant energy based models with equivariant stein variational gradient descent. _Advances in Neural Information Processing Systems_, 34:16727-16737, 2021.
* [23] S. Jin, L. Li, and J.-G. Liu. Random batch methods (rbm) for interacting particle systems. _Journal of Computational Physics_, 400:108877, 2020.
* [24] A. Khaled and P. Richtarik. Better theory for sgd in the nonconvex world. _arXiv preprint arXiv:2002.03329_, 2020.
* [25] Y. Kinoshita and T. Suzuki. Improved convergence rate of stochastic gradient langevin dynamics with variance reduction and its application to optimization. _arXiv preprint arXiv:2203.16217_, 2022.
* [26] A. Korba, A. Salim, M. Arbel, G. Luise, and A. Gretton. A non-asymptotic analysis for stein variational gradient descent. _Advances in Neural Information Processing Systems_, 33:4672-4682, 2020.
* [27] H. J. Kushner and D. S. Clark. _Stochastic approximation methods for constrained and unconstrained systems_, volume 26. Springer Science & Business Media, 2012.
* [28] Y. T. Lee and S. S. Vempala. The manifold joys of sampling (invited talk). In _49th International Colloquium on Automata, Languages, and Programming (ICALP 2022)_. Schloss Dagstuhl-Leibniz-Zentrum fur Informatik, 2022.
* [29] L. Li, Y. Li, J.-G. Liu, Z. Liu, and J. Lu. A stochastic version of stein variational gradient descent for efficient sampling. _Communications in Applied Mathematics and Computational Science_, 15(1):37-63, 2020.
* [30] Q. Liu. Stein variational gradient descent as gradient flow. _Advances in neural information processing systems_, 30, 2017.
* [31] Q. Liu and D. Wang. Stein variational gradient descent: A general purpose bayesian inference algorithm. _Advances in neural information processing systems_, 29, 2016.
* [32] Q. Liu, J. Lee, and M. Jordan. A kernelized stein discrepancy for goodness-of-fit tests. In _International conference on machine learning_, pages 276-284. PMLR, 2016.
* [33] T. Liu, P. Ghosal, K. Balasubramanian, and N. S. Pillai. Towards understanding the dynamics of gaussian-stein variational gradient descent, 2023.
* [34] Y. Liu, P. Ramachandran, Q. Liu, and J. Peng. Stein variational policy gradient. _arXiv preprint arXiv:1704.02399_, 2017.
* [35] J. Lu, Y. Lu, and J. Nolen. Scaling limit of the stein variational gradient descent: The mean field regime. _SIAM Journal on Mathematical Analysis_, 51(2):648-671, 2019.
* [36] R. M. Neal et al. Mcmc using hamiltonian dynamics. _Handbook of markov chain monte carlo_, 2(11):2, 2011.

* [37] Y. Nesterov. Introductory lectures on convex programming volume i: Basic course. _Lecture notes_, 3(4):5, 1998.
* [38] N. Nusken and D. Renger. Stein variational gradient descent: many-particle and long-time asymptotics. _arXiv preprint arXiv:2102.12956_, 2021.
* [39] G. Parisi. Correlation functions and computer simulations. _Nuclear Physics B_, 180(3):378-384, 1981.
* [40] M. Raginsky, A. Rakhlin, and M. Telgarsky. Non-convex learning via stochastic gradient langevin dynamics: a nonasymptotic analysis. In _Conference on Learning Theory_, pages 1674-1703. PMLR, 2017.
* [41] G. O. Roberts and R. L. Tweedie. Exponential convergence of langevin distributions and their discrete approximations. _Bernoulli_, pages 341-363, 1996.
* [42] A. Salim, L. Sun, and P. Richtarik. A convergence theory for svgd in the population limit under talagrand's inequality t1. In _International Conference on Machine Learning_, pages 19139-19152. PMLR, 2022.
* [43] J. Shi and L. Mackey. A finite-particle convergence rate for stein variational gradient descent. _arXiv preprint arXiv:2211.09721_, 2022.
* [44] I. Steinwart and A. Christmann. _Support vector machines_. Springer Science & Business Media, 2008.
* [45] L. Sun, A. Karagulyan, and P. Richtarik. Convergence of stein variational gradient descent under a weaker smoothness condition. In _International Conference on Artificial Intelligence and Statistics_, pages 3693-3717. PMLR, 2023.
* [46] M. Talagrand. _Pettis integral and measure theory_. American Mathematical Soc., 1984.
* [47] S. Vempala and A. Wibisono. Rapid convergence of the unadjusted langevin algorithm: Isoperimetry suffices. _Advances in neural information processing systems_, 32, 2019.
* [48] R. Vershynin. _High-dimensional probability: An introduction with applications in data science_, volume 47. Cambridge university press, 2018.
* [49] C. Villani et al. _Optimal transport: old and new_, volume 338. Springer, 2009.
* [50] D. Wang and Q. Liu. Learning to draw samples: With application to amortized mle for generative adversarial learning. _arXiv preprint arXiv:1611.01722_, 2016.
* [51] D. Wang, Z. Zeng, and Q. Liu. Stein variational message passing for continuous graphical models. In _International Conference on Machine Learning_, pages 5219-5227. PMLR, 2018.
* [52] M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient langevin dynamics. In _Proceedings of the 28th international conference on machine learning (ICML-11)_, pages 681-688, 2011.
* [53] A. Wibisono. Sampling as optimization in the space of measures: The langevin dynamics as a composite optimization problem. In _Conference on Learning Theory_, pages 2093-3027. PMLR, 2018.
* [54] J. Zhuo, C. Liu, J. Shi, J. Zhu, N. Chen, and B. Zhang. Message passing stein variational gradient descent. In _International Conference on Machine Learning_, pages 6018-6027. PMLR, 2018.
* [55] D. Zou, P. Xu, and Q. Gu. Faster convergence of stochastic gradient langevin dynamics for non-log-concave sampling. In _Uncertainty in Artificial Intelligence_, pages 1152-1162. PMLR, 2021.