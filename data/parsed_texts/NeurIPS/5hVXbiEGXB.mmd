# Evolving Standardization for Continual Domain Generalization over Temporal Drift

 Mixue Xie\({}^{1}\)  Shuang Li\({}^{1,}\)\({}^{*}\) Longhui Yuan\({}^{1}\)  Chi Harold Liu\({}^{1}\)  Zehui Dai\({}^{2}\)

\({}^{1}\)Beijing Institute of Technology, China \({}^{2}\)Lazada Search & Monetisation Tech, China

{mxxie,shuangli,longhuiyan}@bit.edu.cn, liuchi02@gmail.com

zehui.dzh@alibaba-inc.com

Corresponding author.

###### Abstract

The capability of generalizing to out-of-distribution data is crucial for the deployment of machine learning models in the real world. Existing domain generalization (DG) mainly embarks on offline and discrete scenarios, where multiple source domains are simultaneously accessible and the distribution shift among domains is abrupt and violent. Nevertheless, such setting may not be universally applicable to all real-world applications, as there are cases where the data distribution gradually changes over time due to various factors, e.g., the process of aging. Additionally, as the domain constantly evolves, new domains will continually emerge. Re-training and updating models with both new and previous domains using existing DG methods can be resource-intensive and inefficient. Therefore, in this paper, we present a problem formulation for _Continual Domain Generalization over Temporal Drift_ (CDGTD). CDGTD addresses the challenge of gradually shifting data distributions over time, where domains arrive sequentially and models can only access the data of the current domain. The goal is to generalize to unseen domains that are not too far into the future. To this end, we propose an _Evolving Standardization_ (EvoS) method, which characterizes the evolving pattern of feature distribution and mitigates the distribution shift by standardizing features with generated statistics of corresponding domain. Specifically, inspired by the powerful ability of transformers to model sequence relations, we design a multi-scale attention module (MSAM) to learn the evolving pattern under sliding time windows of different lengths. MSAM can generate statistics of current domain based on the statistics of previous domains and the learned evolving pattern. Experiments on multiple real-world datasets including images and texts validate the efficacy of our EvoS.

## 1 Introduction

In real-world applications, the assumption that training and testing data conform to the same distribution, a prerequisite for the success of contemporary deep learning methods, is seldom valid. A prime example of this can be found in autonomous driving, where a vehicle may traverse environments that switch from daylight to nightfall or from urban to rural. As environmental conditions change, the issue of distribution shift [5; 4; 51; 62] arises. Moreover, directly utilizing a model trained on in-distribution (ID) data in an out-of-distribution (OOD) context often results in catastrophic performance deterioration. Consequently, ensuring that models perform well on OOD data has emerged as a crucial challenge for the widespread deployment of machine learning models in the real world.

To cope with the distribution shift, two dominant paradigms have been systematically explored depending on the availability of target (test) domain. One is domain adaptation (DA), which aims to assist the model learning on an unlabeled or label-scarce target domain by transferring the knowledgefrom a related and label-rich source domain [15; 34; 29; 35]. Yet, target data are not always known or available in advance. The other is domain generalization (DG), the goal of which is to learn a model capable of generalizing to any unseen domain by using date from multiple related but distinct source domains [62; 36; 27; 63; 26]. Though the second paradigm DG has attained some encouraging outcomes, its configuration is limited to offline and discrete scenarios where multiple source domains can be accessed simultaneously, and the distribution shift among domains is sudden and severe. For instance, the prevalent benchmark PACS [25] in current DG methods comprises four distinct domain styles - "Art", "Cartoon", "Photo" and "Sketch". Nevertheless, this type of configuration may not be suitable for all real-world applications. There are also some cases that the data distribution gradually evolves over time, the distribution shift arising from which is referred to as _temporal drift_[56].

On one hand, the real-world scenarios often exhibit underlying evolutionary patterns [59; 44]. For example, annual or monthly weather data can be utilized for weather forecasting [12]. The evolutionary patterns can be exploited to enhance generalization capabilities towards future unseen domains that are not too distant. However, current DG methods often fail to consider these patterns, leading to suboptimal performance. On the other hand, since data distribution is constantly evolving, new domains will continue to emerge. Consequently, it is imperative to efficiently utilize these new domains to further enhance the model's performance for practical applications. For instance, in the context of advertisement recommendation, user browsing data for various products continually surfaces. How can we leverage these newly collected data to enable more accurate advertisement recommendations tailored to each user's preferences in the days to come? One simple approach is to store data from previous domains and retrain the model using both the new and previous domains with the existing DG techniques. However, such way may consume huge training resources due to the accumulation of data and is of low efficiency, especially for scenarios with rapid data accumulation.

In this paper, we formulate the aforementioned problems as Continual Domain Generalization over Temporal Drifts (CDGTD), where the data distribution gradually drifts with the passage of time and the domain arrives sequentially. And the goal of CDGTD is to generalize well on future unseen domains that are not too far under the circumstance that only current domain is available at current times point while data from previous domains are inaccessible. Although some temporal / evolving DG methods [44; 59; 3; 41] have been proposed to handle the temporal drift, most of them [59; 44; 41] works in non-incremental setting, i.e., multiple domains can be accessed simultaneously. Instead, we design an Evolving Standardization (EvoS) approach specialized for CDGTD.

For CDGTD, there are two main challenges: how to characterize the evolving pattern in the incremental setting and how to achieve generalization using the learned evolving pattern. For the former, we draw inspiration from the sequence modeling capability of transformers [31] and design a multi-scale attention module (MSAM) to learn the evolving pattern underlying the feature distribution of domains. Specifically, we store the statistics (i.e., mean and variance of features) of previous domains and use sliding time windows of different lengths over the statistic tokens to obtain multi-scale information. Then multi-scale statistic tokens are fed into MSAM to generate statistics of current domain. MSAM is trained over the whole sequence of domains to learn the evolving pattern. Here, we integrate multi-scale information, with the consideration that some evolving patterns may be better characterized at different time intervals, e.g., seasonal climate and daily weather. For the latter challenge, in order to mitigate the temporal drift, each domain is transformed into a common normal distribution by conducting feature standardization with the generated statistics of corresponding domain. Besides, considering that the feature encoder may suffer from catastrophic forgetting and overfitting to current domain, we constrain it to learn a shared feature space among domains via the adversarial learning.

**Contributions: 1)** We formulate a promising but challenging problem of continual domain generalization over temporal drift (CDGTD), which has seldom been explored, compared with traditional DG. **2)** An evolving standardization (EvoS) approach is specially proposed for CDGTD, which can characterize the evolving pattern and further achieve generalization by conducting the feature standardization. **3)** Experiments on multiple real-world datasets with different models verify the effectiveness of EvoS and the flexibility to be applied on different models.

## 2 Related Work

**Domain Generalization (DG)** aims to learn a model that can generalize well to any unseen target domains by leveraging data from multiple source domains. In recent years, a wide range of DGmethods have been proposed [6; 39; 27; 26; 63; 19; 37]. According to the strategies of improving generalization, existing DG methods roughly fall into the three categories. Representation learning based methods [7; 39; 50; 27; 2; 2; 17; 42] aim to learn domain-invariant or domain-shared representations to enable models to generalize to unseen target domains. Data augmentation/generation based methods [53; 61; 63; 32; 57] focus on manipulating inputs to facilitate learning general representations. Differently, other DG methods instead employ general learning strategies like meta-learning [26] and ensemble learning [21] to improve generalization ability. With full access to all source domains, these DG methods have achieved promising performance on unseen domains with abrupt and violent distribution shifts. Unfortunately, without the consideration of the evolutionary pattern of domains over time, existing DG methods are usually less efficient under the scenario of temporal drift.

**Continual Learning (CL)** is a machine learning paradigm, where models learn continuously from a stream of tasks over time and meanwhile try to retain performances on all seen tasks [22; 1; 24; 9; 46; 47]. Existing CL methods can be roughly categorized into replay-based [46; 10; 52], regularization-based [22; 60; 30] and structure-based [45; 14] methods. In addition, the combination of CL and DA has attracted lots of attention [23; 17; 33; 43; 55]. Yet, combining CL and DG remains underdeveloped. In this work, we propose CDGTD which continually trains the model on sequential domains, but the goal is to generalize well on novel domains in the near future. This distinct objective from CL yields different challenges: the modeling of underlying evolutionary patterns of temporal domains and how to utilize these patterns to mitigate the distribution shifts in forthcoming domains.

**Test Time Adaptation (TTA)** focuses on adapting the source-pretrained model during testing phase with test data [40; 49; 20; 54; 58]. For example, [49] corrects the activation statistics of batch normalization using test data. T3A [20] uses a back-propagation-free manner to adjust only the weights of the linear classifier during test time. By contrast, CDGTD optimizes the model during training phase with sequential domains and emphasizes on generalization without using test data.

**Evolving / Temporal Domain Generalization** has emerged in recent years, aiming to tackle the problem of generalizing on temporally drifted domains, where the environment evolves dynamically over time [41; 3; 44; 59]. GI [41] proposes a time-sensitive model architecture to capture the time-varying data distribution and the model is supervised with the first-order Taylor expansion of the learned function to advance the generalization in the near feature. DRAIN [3] launches a Bayesian framework to model the concept drift and utilizes a recurrent neural network to dynamically generate network parameters to adapt the evolving pattern of domains. LSSAE [44] incorporates variational inference to explore the evolving patterns of covariate shift and label shift in the latent space. The goal of learning evolutionary patterns from source domains and generalizing to domains in the near future is similar to our EvoS. The main difference is that the model in our EvoS is incrementally trained on sequentially arriving domains, considering the low efficiency of "offline" training with accumulated domains. By contrast, the aforementioned methods [41; 44] require multiple source domains to be simultaneously available. Besides, the Taylor expansion in [41], variational inference in [44] and network parameters generation in [3] make them hard to expand on large models.

## 3 Evolving Standardization for Continual Domain Generalization over Temporal Drift

### Problem Formulation of CDGTD

Here, we take the \(C\)-class classification problem as an example, where \(X\) and \(Y\) denote the data and label space, respectively. Suppose that \(T\) source (training) domains \(\{\mathcal{D}^{1},\mathcal{D}^{2},\cdots,\mathcal{D}^{T}\}\) sequentially arrive, which are sampled from distributions at \(T\) different times points \(\mathbf{t}_{1}<\mathbf{t}_{2}<\cdots<\mathbf{t}_{T}\). At time point \(\mathbf{t}_{i}\), \(t\in\{1,2,\cdots,T\}\), only the domain \(\mathcal{D}^{t}=\{\mathbf{x}_{i}^{t},y_{i}^{t}\}_{i=1}^{N^{t}}\) is accessible, where \(\mathbf{x}_{i}^{t}\in X\), \(y_{i}^{t}\in Y\) and \(N^{t}\) is the number of training samples in \(\mathcal{D}^{t}\). Previous and future domains are unavailable. In addition, as previous temporal/evolving DG methods [44; 41; 3], we further assume that the data distribution \(P(X,Y)\) of domains evolves temporally, i.e., the distribution of domains changes along the time following certain patterns. The goal of CDGTD is to enable the model, composed of a feature encoder \(\mathcal{E}:\mathbf{x}\rightarrow\bm{f}\in\mathbb{R}^{d_{f}}\) (\(d_{f}\) is the dimension of features) and a classifier \(\mathcal{C}:\bm{f}\rightarrow{y}\in\{0,1,\cdots,C-1\}\), to generalize well on \(K\) unseen target (test) domains in the near future: \(\{\mathcal{D}^{T+k}\}_{k=1}^{K}\). To this end, two main challenges need to be addressed. One is to characterize the evolving pattern of domains, which we address by designing a multi-scale attention module (MSAM)inspired from the sequence modeling ability of transformers. The other is the generalization, which is realized by transforming domains into a common normal distribution via the feature standardization.

### EvoS: Evolving Standardization

**Single-scale Attention.** Inspired by the capability of the transformer to model the relationships among sequences [31], we introduce its attention mechanism to model the evolving pattern among sequential domains. Before presenting our multi-scale attention module (MSAM), we first introduce how single-scale attention works. Without loss of much generality, we assume that the feature distribution of domain \(\mathcal{D}^{t}\) follows the normal distribution, which is characterized by the mean vector \(\bm{\mu}^{t}\in\mathbb{R}^{d_{f}}\) and the standard deviation vector \(\bm{\sigma}^{t}\in\mathbb{R}^{d_{f}}\). Now, our goal turns out to be learning the evolving pattern of the feature statistics (i.e., \(\bm{\mu}^{t}\) and \(\bm{\sigma}^{t}\)). In practice, to ensure that \(\bm{\sigma}^{t}\) is non-negative, we choose to learn \(\bm{a}^{t}\) (the logarithm of \(\bm{\sigma}^{t}\)). In this way, \(\bm{\sigma}^{t}=\exp(\bm{a}^{t})\) is always non-negative. Besides, considering the unavailability of previous domains, we store the learned statistics at time point \(\mathsf{t}_{t}\) into a memory pool \(\mathcal{M}\) for future uses once the training procedure finishes on domain \(\mathcal{D}^{t}\). Having the feature statistics of previous \(t-1\) domains ready, we then utilize the multi-head self-attention module \(\mathcal{A}\) to generate the statistics of domain \(\mathcal{D}^{t}\) at time point \(\mathsf{t}_{t}\).

Concretely, given statistic tokens \(\{\bm{s}^{i}\}_{i=1}^{t-1}\) of previous \(t-1\) domains, where \(\bm{s}^{i}=[\bm{\mu}^{i},\bm{a}^{i}]\in\mathbb{R}^{2d_{f}}\) is the token of concatenated statistics from domain \(\mathcal{D}^{t}\), the output of \(\mathcal{A}\) is expressed as

\[\bm{\hat{S}}^{t} =\mathcal{A}(\bm{S}^{t-1})=[\mathrm{SA}_{1}(\bm{S}^{t-1}),\mathrm{ SA}_{2}(\bm{S}^{t-1}),\cdots,\mathrm{SA}_{n_{h}}(\bm{S}^{t-1})]\mathbf{W}_{fc}, \quad\mathbf{W}_{fc}\in\mathbb{R}^{(n_{h}\cdot d_{h})\times 2d_{f}},\] \[=[\bm{\hat{s}}^{1},\bm{\hat{s}}^{2};\cdots;\bm{\hat{s}}^{t-1}],\] \[\bm{S}^{t-1} =[\bm{s}^{1};\bm{s}^{2};\cdots;\bm{s}^{t-1}],\quad\bm{S}^{t-1}\in \mathbb{R}^{(t-1)\times 2d_{f}}\] (1)

where \(\bm{\hat{s}}^{i}\) is the \(i\)-th output statistic token of \(\mathcal{A}\), \(n_{h}\) and \(d_{h}\) are the number and feature dimension of heads in the multi-head self-attention module and \(\mathbf{W}_{fc}\) is the learnable parameters of \(\mathcal{A}\) to convert feature dimensions. \(\mathrm{SA}_{i}(\cdot)\) is the self-attention of the \(i\)-th head, which operates as follows:

\[[\bm{S}^{t-1}_{i,q},\bm{S}^{t-1}_{i,k},\bm{S}^{t-1}_{i,v}]=\bm{S}^ {t-1}\mathbf{W}^{i}_{qkv},\quad\mathbf{W}^{i}_{qkv}\in\mathbb{R}^{2d_{f}\times 3 d_{h}}\] \[\mathrm{SA}_{i}(\bm{S}^{t-1})=\mathrm{softmax}(\bm{S}^{t-1}_{i,q} \bm{S}^{t-1}_{i,k}{}^{\top}/\sqrt{d_{h}})\bm{S}^{t-1}_{i,v},\] (2)

Figure 1: Overview of EvoS. Memory pool \(\mathcal{M}\) stores previously generated domain statistics (i.e., \(\bm{\mu}^{i}\) (mean) and \(\bm{a}^{i}\) (logarithm of standard deviation) of features, \(1\leq i\leq t-1\)). With the \(t\)-th domain \(\mathcal{D}^{t}\) available, loss \(\mathcal{L}^{t}_{stan}\) is minimized to train the multi-scale attention module (MSAM) to generate statistics \(\bm{\hat{\mu}}^{t}\) and \(\bm{\hat{a}}^{t}\) that approach the real ones via leveraging historical statistics in \(\mathcal{M}\). Besides, we sample features from historical feature distributions as the proxy of previous domains to conduct adversarial training with current domain, encouraging to learn a shared feature space.

where \(\mathbf{W}^{i}_{qkv}\) is the learnable parameters of the \(i\)-th head and \(\bm{S}^{t-1}_{i,q},\bm{S}^{t-1}_{i,k},\bm{S}^{t-1}_{i,v}\) are the query, key and value embeddings of \(\bm{S}^{t-1}\). Finally, we use the average of all output statistics tokens of the attention module \(\mathcal{A}\) as the generated statistic token \(\bm{\hat{s}}^{t}\) for time points \(\mathsf{t}_{t}\):

\[[\bm{\hat{\mu}}^{t},\bm{\hat{a}}^{t}]=\bm{\hat{s}}^{t}=avg(\bm{\hat{S}}^{t})= \frac{1}{t-1}\sum_{i=1}^{t-1}\bm{\hat{s}}^{i}.\] (3)

By continuously generating new statistics from historical ones throughout the whole sequence of domains, the attention module \(\mathcal{A}\) is expected to learn the evolving pattern of feature distributions. Note that we directly introduce learnable statistic vectors \(\bm{\hat{\mu}}^{1},\bm{\hat{a}}^{1}\) and \(\bm{\hat{\mu}}^{2},\bm{\hat{a}}^{2}\) for time points \(\mathsf{t}_{1}\) and \(\mathsf{t}_{2}\), respectively, since the tokens of historical statistics are not enough for the attention module \(\mathcal{A}\) to work. That is, the attention module \(\mathcal{A}\) is only used at time points \(\mathsf{t}_{t},t\geq 3\).

**Multi-scale Attention Module (MSAM).** The above details how to model the evolving pattern using single-scale attention. However, it is limited to learn the pattern using a sliding time window of length 1 over the statistic tokens. Sometimes, the evolutionary pattern may be better captured by considering longer time intervals. For instance, the data of a season in each time window would be more suitable for capturing the evolving pattern of seasonal climate, rather than data of a single day in each time window. Considering this, we introduce multi-scale attention in Fig. 1, which leverages information from observation windows of different lengths to better model the evolving pattern.

Specifically, for the multi-head self-attention module \(\mathcal{A}_{w}\) responsible for the time window of length \(w\), we slide the time window with stride 1 over historical \(t-1\) statistics tokens and concatenate the statistic tokens in each window to obtain the input \(\bm{\hat{S}}^{t-1}_{w}\) for the attention module \(\mathcal{A}_{w}\) at time point \(\mathsf{t}_{t}\):

\[\bm{\hat{s}}^{i}_{w} =[\bm{s}^{i},\bm{s}^{i+1}\cdots,\bm{s}^{i+w-1}],\quad\bm{\hat{s}} ^{i}_{w}\in\mathbb{R}^{w\cdot 2d_{f}},\quad(1\leq i\leq t-w)\wedge(t\geq w+2)\] \[\bm{\hat{S}}^{t-1}_{w} =[\bm{\hat{s}}^{1}_{w};\bm{\hat{s}}^{2}_{w};\cdots;\bm{\hat{s}}^ {t-w}_{w}],\quad\bm{\hat{S}}^{t-1}_{w}\in\mathbb{R}^{(t-w)\times(w\cdot 2d_{f})}.\] (4)

And similarly, we use the average output \(\bm{\hat{s}}^{t-w+1}_{w}\) of the attention module \(\mathcal{A}_{w}\) as the generated statistics for the sliding time window at the next time point, the formulation of which is expressed as

\[\bm{\hat{S}}^{t}_{w} =\mathcal{A}_{w}(\bm{\hat{S}}^{t-w+1}_{w})=[\bm{s}^{1}_{w};\bm{ \hat{s}}^{2}_{w};\cdots;\bm{\hat{s}}^{t-w}_{w}],\quad\bm{\hat{S}}^{t}_{w}\in \mathbb{R}^{(t-w)\times(w\cdot 2d_{f})}\] \[\bm{\hat{s}}^{t-w+1}_{w} =avg(\bm{\hat{S}}^{t}_{w})=\frac{1}{t-w}\sum_{i=1}^{t-w}\bm{\hat{ s}}^{i}_{w},\quad\bm{\hat{s}}^{t-w+1}_{w}\in\mathbb{R}^{w\cdot 2d_{f}}.\] (5)

Note that in MSAM, the attention module \(\mathcal{A}_{w}\) is to predict the statistics in the next sliding time window of length \(w\), instead of one statistic token as in the single-scale attention. This may encourage the attention module to also capture the domain relationships within the window.

Then, we split \(\bm{\hat{s}}^{t-w+1}_{w}\) into \(w\) parts: \([\bm{\hat{s}}^{t-w+1}_{w},\cdots,\bm{\hat{s}}^{t-1}_{w},\bm{\hat{s}}^{t}_{w} ]=\bm{\hat{s}}^{t-w+1}_{w}\), where \(\bm{\hat{s}}^{j}_{w}\in\mathbb{R}^{2d_{f}}\) can be regarded as the predicted statistic token for time point \(\mathsf{t}_{j}\) using the attention module \(\mathcal{A}_{w},j=t-w+1,\cdots,t-1,t\). Finally, the generated statistic token \(\bm{\hat{s}}^{t}\) for time point \(\mathsf{t}_{t}\) using MSAM is denoted as the average of predicted statistic tokens for time point \(\mathsf{t}_{t}\) at different time window lengths:

\[[\bm{\hat{\mu}}^{t},\bm{\hat{a}}^{t}]=\bm{\hat{s}}^{t}=\frac{1}{W}\sum_{w=1}^ {W}\bm{\hat{s}}^{t}_{w},\] (6)

where \(W\) is the maximum length of the sliding time window. Generally, MSAM integrates evolving patterns learned at different scales, contributing better estimation of future feature distributions.

**Feature Standardization.** For CDGTD, the second main challenge is generalization. To this end, we leverage the generated feature statistics by MSAM to transform the distribution of corresponding domain into a standard normal distribution, by which the temporal drift is mitigated. Concretely, for feature \(\bm{f}^{t}_{i}=\mathcal{E}(\bm{x}^{t}_{i})\), its standardized feature \(\bm{z}^{t}_{i}\) via the feature standardization is formulated as

\[\bm{z}^{t}_{i}=\frac{\bm{f}^{t}_{i}-\bm{\hat{\mu}}^{t}}{\bm{\hat{\sigma}}^{t}}= \frac{\bm{f}^{t}_{i}-\bm{\hat{\mu}}^{t}}{\exp(\bm{\hat{a}}^{t})}.\] (7)

Another benefit of feature standardization is that it allows classifier to be trained on a common feature distribution, thus avoiding the problem of overfitting to current domain and catastrophic forgetting.

### Model Training

This section outlines the four losses \(\mathcal{L}^{t}_{ce}\), \(\mathcal{L}^{t}_{stan}\), \(\mathcal{L}^{t}_{con}\) and \(\mathcal{L}^{t}_{adv}\) involved in the training stage at time point \(\mathfrak{t}_{t}\). The first one is the essential loss to supervise the model learning for specific tasks, e.g., the following cross-entropy loss \(\mathcal{L}^{t}_{ce}\) for classification tasks in this paper.

\[\min_{\mathcal{E},\mathcal{C}}\mathcal{L}^{t}_{ce}=\frac{1}{N^{t}}\sum_{i=1}^{ N^{t}}\mathcal{CE}(\bm{p}^{t}_{i},y^{t}_{i}),\quad\bm{p}^{t}_{i}=\mathrm{softmax} \left(\mathcal{C}\bigg{(}\frac{\bm{f}^{t}_{i}-\mathrm{sg}(\hat{\bm{\mu}}^{t})} {\exp(\mathrm{sg}(\hat{\bm{a}}^{t}))}\bigg{)}\right),\] (8)

where \(\mathcal{CE}(\cdot,\cdot)\) is the cross-entropy. Here, stopping gradient \(\mathrm{sg}(\cdot)\) is adopted to stabilize training.

Losses \(\mathcal{L}^{t}_{stan}\) and \(\mathcal{L}^{t,w}_{con}\) are responsible for the training of the attention module \(\mathcal{A}_{w}\) at time point \(\mathfrak{t}_{t}\), \(w=1,2,\cdots,W\). The former loss \(\mathcal{L}^{t}_{stan}\) in Eq. 10 is minimized to ensure that the standardized feature \(\bm{z}^{t}_{i}\) follows a standard normal distribution. Specifically, we first calculate the mean vector and variance vector of the standardized features as

\[mean:\hat{\bm{\mu}}^{t}=\frac{1}{N^{t}}\sum_{i=1}^{N_{t}}\frac{\mathrm{sg}( \bm{f}^{t}_{i})-\hat{\bm{\mu}}^{t}}{\exp(\hat{\bm{a}}^{t})},\quad variance: \bm{\hat{v}}^{t}=\frac{1}{N^{t}-1}\sum_{i=1}^{N_{t}}\bigg{(}\frac{\mathrm{sg} (\bm{f}^{t}_{i})-\hat{\bm{\mu}}^{t}}{\exp(\hat{\bm{a}}^{t})}-\hat{\bm{\mu}}^{t }\bigg{)}^{2}.\] (9)

Then the loss \(\mathcal{L}^{t}_{stan}\) at time point \(\mathfrak{t}_{t}\) is expressed as

\[\left\{\begin{aligned} &\min_{\hat{\bm{\mu}}^{t},\hat{\bm{a}}^{t}} \mathcal{L}^{t}_{stan}=\|\hat{\bm{\mu}}^{t}-\bm{0}\|_{2}+\|\hat{\bm{v}}^{t}- \bm{1}\|_{2},\quad 1\leq t\leq 2\\ &\min_{\mathcal{A}_{w}}\mathcal{L}^{t}_{stan}=\|\hat{\bm{\mu}}^{ t}-\bm{0}\|_{2}+\|\hat{\bm{v}}^{t}-\bm{1}\|_{2},\quad t\geq w+2\end{aligned} \right.,\] (10)

where \(d_{f}\) is the dimension of features and \(w\) is the length of the sliding time window. Here, we just want the learnable/generated \(\hat{\bm{\mu}}^{t},\hat{\bm{a}}^{t}\) to approach the statistics of true feature distribution, so stopping gradient operation \(\mathrm{sg}(\cdot)\) is conducted on \(\bm{f}^{t}_{i}\) to prevent the gradient of \(\mathcal{L}^{t}_{stan}\) flowing to the feature encoder \(\mathcal{E}\). Otherwise, the gradient of \(\mathcal{L}^{t}_{stan}\) may distort the feature distribution. Ideally, if \(\mathcal{L}^{t}_{stan}\) is minimized, the generated statistics \(\hat{\bm{\mu}}^{t}\) and \(\exp(\hat{\bm{a}}^{t})\) by our MSAM would be equal to the mean vector and standard deviation vector of the true feature distribution of \(\mathcal{D}^{t}\). This means that our MSAM can predict future feature distributions based on historical feature statistics.

And the loss \(\mathcal{L}^{t}_{con}\) is to ensure the consistency between generated statistic tokens in the next sliding time window and real statistic tokens. Specifically, we calculate \(\mathcal{L}^{t,w}_{con}\) for attention module \(\mathcal{A}_{w}\) as

\[\min_{\mathcal{A}_{w}}\mathcal{L}^{t,w}_{con}=\frac{1}{w-1}\sum_{k=1}^{w-1}\| \hat{\bm{s}}^{t-w+k}_{w}-\bm{s}^{t-w+k}\|_{2},\quad(t\geq w+2)\wedge w\geq 2,\] (11)

where \(\hat{\bm{s}}^{j}_{w}\) and \(\bm{s}^{j}_{w}\), \(j=t-w+1,\cdots,t-1\) are respectively the predicted statistic token for time point \(\mathfrak{t}_{j}\) using the attention module \(\mathcal{A}_{w}\) and the real statistic token from memory pool \(\mathcal{M}\). This loss is expected to also capture the evolutionary pattern within the time window.

The fourth loss \(\mathcal{L}^{t}_{adv}\) is introduced with this consideration that the feature extractor \(\mathcal{E}\) may overfit to current domain and lacks generalizability. To avoid this, we additionally conduct adversarial training between the feature encoder \(\mathcal{E}\) and a discriminator \(\mathfrak{D}\). Nevertheless, historical data are unavailable. So we instead resort to the learned/generated statistics at previous time points. Concretely, we randomly sample a batch of \(B\) samples \(\{\bm{f^{\prime}}_{1}^{m},\cdots,\bm{f^{\prime}}_{B}^{m}\}\) from each distribution \(\mathcal{N}(\bm{\mu}^{m},(\exp(\bm{a}^{m}))^{2})\), \(m=1,2,\cdots,t-1\), via Eq. 12 at each iteration, and use them as the proxy of historical domains.

\[\bm{f^{\prime}}_{i}^{m}=\bm{\mu}^{m}+\epsilon\cdot\exp(\bm{a}^{m}),\quad \epsilon\sim\mathcal{N}(0,1)\wedge-\alpha\leq\epsilon\leq\alpha,\] (12)

where \(\alpha\) is a truncation hyper-parameter to control the sampling area. Then discriminator \(\mathfrak{D}\) is trained to distinguish \(\{\bm{f^{\prime}}_{i}\}_{i=1}^{N^{t}}\) from \(\{\bm{f^{\prime}}_{1}^{m},\cdots,\bm{f^{\prime}}_{B}^{m}\}_{m=1}^{t-1}\) and the feature extractor tries to confuse \(\mathfrak{D}\). Such adversarial process is achieved by the following loss \(\mathcal{L}^{t}_{adv}\):

\[\max_{\mathcal{E}}\min_{\mathfrak{D}}\mathcal{L}^{t}_{adv}=\frac{1}{2}\Big{(} \frac{1}{B\times(t-1)}\sum_{m=1}^{t-1}\sum_{j=1}^{B}-\log(\mathfrak{D}(\bm{f^{ \prime}}_{j}^{m}))+\frac{1}{N^{t}}\sum_{j=1}^{N^{t}}-\log(1-\mathfrak{D}(\bm{f^{ \prime}}_{j}^{t}))\Big{)},\quad t\geq 2.\] (13)

In practice, the gradient reverse layer (GRL) [15] is used to achieve the adversarial training. To sum up, at time point \(\mathfrak{t}_{t}\), the model is trained to minimize the following total loss \(\mathcal{L}^{t}_{total}\):

\[\mathcal{L}^{t}_{total}=\mathcal{L}^{t}_{ce}+\mathcal{L}^{t}_{stan}+\sum_{w=1}^{ W}\mathcal{L}^{t,w}_{con}+\lambda\mathcal{L}^{t}_{adv},\] (14)where \(\lambda\) is a hyper-parameter to balance the loss tradeoff. Once the training procedure at time point \(\mathfrak{t}_{t}\) is finished, we store the learned/generated statistics of current domain \(\mathcal{D}^{t}\) into memory pool \(\mathcal{M}\) by

\[\boldsymbol{\mu}^{t},\boldsymbol{a}^{t}\leftarrow\boldsymbol{\hat{\mu}}^{t}, \boldsymbol{\hat{a}}^{t}.\] (15)

In the inference stage, we use MSAM to generate future statistics \(\boldsymbol{\hat{s}}^{T+k}\) based on the statistics \(\{s^{t}\}_{t=1}^{T+k-1}\) in memory pool \(\mathcal{M}\) and store it into \(\mathcal{M}\) as Eq. 15 for the generation at next time point. Due to space limitation, the training and testing procedures are provided in the appendix.

## 4 Experiments

### Experimental Setup

Thanks to the work in [56], several real-world datasets with distribution shifts over time have been available. And we evaluate EvoS on three image classification datasets (**Yearbook** and **fMoW** from [56] and **RMNIST**) and two text classification datasets (**Huffpost** and **Arxiv** from [56]). Yearbook collects data from 1930 to 2013, where we treat every four years as a domain and use the first 16 domains for training (\(T=16\)), the last 5 domains for testing (\(K=5\)). fMoW includes data of 16 years and we set \(T=13,K=3\). RMNIST contains 9 domains with \(T=6,K=3\). Huffpost includes data of 7 years with \(T=4,K=3\). Arxiv collects data of 172 categories for 16 years, with \(T=9,K=7\). For each training domain of all datasets, we randomly select 90% data as training split and 10% data as validation split. Following the backbones in [56], we use a 4-layer convolutional network [56] for Yearbook, Densect-121 [18] pretrained on ImageNet for fMoW, pretrained DistilBERT base model [48] for Huffpost and Arxiv, and the ConvNet in [44] for RMNIST. For each attention module \(\mathcal{A}_{w}\) in MSAM, its dimension of head \(d_{h}\) is set to 8 and its number of heads is set to \(w\cdot n_{h}\). Specifically, \(n_{h}\) is set to 16 for Yearbook, 32 for RMNIST, 64 for fMoW and 128 for Huffpost and Arxiv. For optimization, we use the Adam optimizer with \(lr=1e-3\) for Yearbook and RMNIST, \(lr=2e-4\) for fMoW and \(lr=2e-5\) for Huffpost and Arxiv. The batch size is set to 64 for all datasets. As for hyper-parameters, we select them via grid search using the validation splits of training domains and finally use \(\alpha=2.0\) for RMNIST, \(\alpha=1.0\) for others, \(\lambda=1.0,W=3\) for all datasets. For all tasks, we report the mean of 3 random trials. Due to space limitations, please refer to appendix for more details. Code is available at https://github.com/BIT-DA/EvoS.

### Main Results

In addition to the incremental training scenario, we also provide results in non-incremental scenario with all source domains simultaneously available, which serves as upper bounds. Among compared baselines, "Offline" denotes merging all source domains into one domain and training the model with the merged domain, while "IncFinetune" represents incrementally training the model in a domain-by-domain fashion. For each dataset, we report the accuracy on the nearest target domain (\(\mathcal{D}^{T+1}\)),

the average and worst accuracy of future \(K\) domains ("OOD avg.": \(\frac{1}{K}\sum_{k=1}^{K}Acc(\mathcal{D}^{T+k})\), "OOD worst": \(\min_{k\in\{1,2,\cdots,K\}}Acc(\mathcal{D}^{T+k})\)).

**Results on image classification tasks** are provided in Table 5, where we can observe that EvoS consistently achieves superior performance across three reported metrics on Yearbook and RMNIST datasets, compared with the methods in incremental training scenario. Especially, EvoS outperforms DRAIN by over \(1\%\) according the metric "OOD worst" on Yearbook and RMNIST, which means that our EvoS learns more robust evolving patterns. In addition, we notice that conventional DG methods IRM [2] and CORAL [50] perform worse than temporal DG method GI [41] on Yearbook and RMNIST, showing the importance of leveraging evolutionary patterns for generalization over temporal drifts. Though being effective for the small networks used in Yearbook and RMNIST, the advantage of GI disappears on fMoW dataset with backbone DenseNet-121, where its first-order Taylor expansion requires extremely huge computing resources (over 80G). And our available resources cannot afford it. When ablating the Taylor expansion, the performance of GI on fMoW is not the best. So GI is hard to expand on larger models and the similar issue also exists in DRAIN and LSSAE. Finally, we find that for dataset fMoW, not only our method performs unsatisfactorily but also other methods, except for the "offline" method (i.e., merging all source domain into one domain to train the model). We infer this may be due to the temporal drift is not well presented in this dataset.

**Results on text classification tasks** are shown in Table 6, where EvoS consistently achieves the best performances according to the average and worst accuracy of future \(K\) domains under the CDGTD setting. This can be owed to the more robust evolving patterns captured by our multi-scale attention module (MSAM). Similarly, IRM and CORAL show obvious performance drop compared with DRAIN, once again demonstrating the necessity to learn evolving patterns for the problem of CDGTD. And the large performance gap between continual learning methods (i.e., SI and A-GEM) and our method EvoS on dataset Arxiv verifies that our method EvoS fully utilizes the historical knowledge to learn evolutionary patterns, while SI and A-GEM do not consider this.

### Analytical Experiments

**Ablation Study.** Firstly, we study the influence of the stopping gradient \(\mathrm{sg}(\cdot)\) in the losses \(\mathcal{L}_{ce}^{t}\) and \(\mathcal{L}_{stan}^{t}\). The results of variant A and B in Table 3 almost degenerate to random predictions, compared with variant C. This suggests that using \(\mathrm{sg}(\cdot)\) in loss \(\mathcal{L}_{stan}^{t}\) is essential. Otherwise, the gradient of \(\mathcal{L}_{stan}^{t}\) would largely distort the learning of feature encoder, causing training collapse. Meanwhile, the inferior performance of variant C to EvoS also indicates that the gradient of \(\mathcal{L}_{ce}^{t}\) should not interfere with the learning of MSAM. Secondly, we ablate the losses \(\mathcal{L}_{adv}^{t}\) and \(\mathcal{L}_{con}^{t,w}\) to testify their necessity. We can see that the results of

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|} \hline \multirow{3}{*}{Method} & \multirow{3}{*}{Conference} & \multirow{3}{*}{Incremental training} & \multirow{3}{*}{\begin{tabular}{c} Access \\ multiple \\ \end{tabular} } & \multirow{3}{*}{\begin{tabular}{c} \(\mathcal{D}^{T+1}\) \\ \end{tabular} } & \multirow{3}{*}{\begin{tabular}{c} OOD \\ avg. \\ \end{tabular} } & \multirow{3}{*}{\begin{tabular}{c} OOD \\ \(\mathcal{D}^{T+1}\) \\ \end{tabular} } & \multirow{3}{*}{
\begin{tabular}{c} OOD \\ avg. \\ \end{tabular} } \\ \cline{5-10}  & & & & & & & & & \\ \hline Offline & - & ✗ & ✓ & 72.74 & 71.50 & 66.63 & 57.49 & 52.38 & 49.28 \\ IRM [2] & arXiv’19 & ✗ & ✓ & 71.04 & 70.31 & 68.97 & 51.11 & 45.89 & 42.86 \\ CORAL [50] & ECCV Workshops’16 & ✗ & ✓ & 71.34 & 70.08 & 68.68 & 50.98 & 45.77 & 42.71 \\ Mixup [61] & ICLR’18 & ✗ & ✓ & 73.34 & 71.16 & 69.29 & 57.58 & 52.77 & 49.62 \\ LISA [57] & ICMI’22 & ✗ & ✓ & 72.19 & 70.24 & 68.60 & 56.53 & 52.41 & 49.67 \\ GI [41] & NeurIPS’21 & ✗ & ✓ & 68.06 & 66.32 & 64.64 & 53.34 & 49.19 & 46.13 \\ \hline InCintense & ✓ & ✗ & 73.57 & 71.98 & 69.80 & 56.22 & 52.43 & 49.37 \\ Mixup [61] & ICLR’18 & ✓ & ✗ & 73.07 & 71.52 & 69.44 & **56.64** & 52.95 & 49.97 \\ EWC [22] & arXiv’16 & ✗ & ✗ & **73.64** & 71.53 & 68.99 & 56.60 & 52.78 & 49.73 \\ SI [60] & ICMI’17 & ✓ & ✗ & 72.58 & 71.50 & 69.61 & 49.98 & 47.27 & 44.77 \\ A-GEM [10] & ICLR’19 & ✓ & ✗ & 72.23 & 71.16 & 69.10 & 52.02 & 48.91 & 46.03 \\ DRAN [3] & ICLR’23 & ✓ & ✗ & 73.42 & 71.75 & 69.69 & 56.04 & 52.07 & 48.97 \\ \hline
**EvoS** & - & ✓ & ✗ & 73.42 & **72.36** & **70.19** & **56.60** & **53.15** & **50.19** \\ \hline \end{tabular} 
\begin{tabular}{c} For Huffpost and Arxiv, backbone DistilBERT-base is too big to apply the full GI and DRAIN. So we apply DRAIN only to the classifier and apply GI without the fine-tuning stage. \\ \end{tabular}
\end{table}
Table 2: Accuracy (%) on Huffpost and Arxiv. The best and second-best results in CDGTD setup are bolded and underlined. (Huffpost: \(K=3\), Axriv: \(K=7\))

\begin{table}
\begin{tabular}{|c|c c c c|c|c|c|c|c|} \hline \multirow{2}{*}{Method} & \multirow{2}{*}{\begin{tabular}{c} Scale \\ single \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} Loss \\ multiple \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} Loss \\ multiple \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} Using sg(\(\cdot\))? \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} Using sg(\(\cdot\)?) \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} Using \\ truncation \(\alpha\)? \\ \end{tabular} } & \multirow{2}{*}{
\begin{tabular}{c} OOD \\ avg. \\ \end{tabular} } \\ \hline Variant A & - & ✓ & ✓ & ✓ & No & No & Yes & 51.28 \\ Variant B & - & ✓ & ✓ & Yes & No & Yes & 55.37 \\ Variant C & - & ✓ & ✓ & No & Yes & Yes & 93.97 \\ \hline Variant D & - & ✓ & - & - & Yes & Yes & Yes & 93.25 \\ Variant E & - & ✓ & ✓ & - & Yes & Yes & Yes & 94.75 \\ Variant F & - & ✓ & - & ✓ & Yes & Yes & Yes & 94.27 \\ \hline Variant G & - & ✓ & ✓ & ✓ & Yes & Yes & No & 94.46 \\ EvoS & - & ✓ & ✓ & ✓ & Yes & Yes & Yes & **95.53** \\ Variant H & ✓ & - & ✓ & ✓ & Yes & Yes & Yes & 94.09 \\ \hline \end{tabular}
\end{table}
Table 3: Ablation study of EvoS on dataset Yearbook.

[MISSING_PAGE_FAIL:9]

**Effect of Memory Pool Length.** In the main experiments, we do not restrict the size of the memory pool \(\mathcal{M}\), since the datasets used in our paper has a moderate number of domains and the memory cost is small. Nevertheless, a fixed memory pool size is more practical when considering a lifelong process, i.e., \(T\rightarrow\infty\). Thus, we additionally conduct the experiment where the memory pool \(\mathcal{M}\) is implemented as a FIFO queue with different fixed length \(L\). That is, only the statistics for up to the \(L\) most recent historical domains can be stored. The results on Yearbook in Fig. 4(b) demonstrate that our method generally performs well and a relatively large memory pool length is better.

**t-SNE Visualization of Standardized Features.** To verify our method appropriate for generalization, we visualize the standardized features of all target domains via t-SNE [38] on RMNIST and Yearbook in Fig. 4(a). It can be observed that the standardized features of target domains are well aligned, suggesting that MASM can capture evolving patterns effectively and feature standardization helps address temporal drift appropriately. These enable EvoS to achieve generalization on future domains.

**Hyper-parameter Sensitivity.**\(\alpha\) and \(\lambda\) control the truncation range and the tradeoff the adversarial loss \(\mathcal{L}_{adv}\), respectively. Fig. 4(c) shows the sensitivity of EvoS to them on Yearbook, where \(\alpha\in\{0.1,0.5,1.0,1.5,2.0,2.5,3.0\}\) and \(\lambda\in\{0.1,0.5,1.0,1.5,2.0,5.0,10.0,15.0,20.0\}\). We see that EvoS is more sensitive to \(\lambda\) than \(\alpha\), and large \(\lambda\) values significantly worsen performance. In practice, we suggest selecting \(\lambda\) from a range \(\leq\)1. For \(\alpha\), its effect is modest and \(\alpha=1\) generally works well.

## 5 Conclusion

This paper considers a more challenging problem of continual domain generalization over temporal drift (CDGTD) than conventional DG, where the model is incrementally trained with sequential domains and is required to generalize to unseen domains that are not too far into the future. To this end, we propose an Evolving Standardization (EvoS) method to learn the evolving pattern of sequential domains over the temporal drift and hope to achieve the generalization by transforming the domain distribution into a common normal distribution via feature standardization. Experiments on real-world datasets including images and texts verify the efficacy of EvoS. Since existing DG works focus on conventional setting, we hope this work can encourage more research works on CDGTD.

Figure 4: (a): Visualization of standardized features from each test domain on Yearbook and RMNIST. Different colors denote different domains. (b): Effect of the memory pool length \(L\) on dataset Yearbook. (c):Hyper-parameter sensitivity of \(\alpha\) and \(\lambda\) on Yearbook

Figure 3: Visualization of decision boundaries for the model at current time point \(\mathfrak{t}_{t}\) on inter-twinning moons 2D problem. Blue line and points are the decision boundary and samples in current domain \(\mathcal{D}^{t}\), and red line and points are the decision boundary and samples in the next future domain \(\mathcal{D}^{t+1}\).

## Acknowledgements

This paper was supported by National Key R&D Program of China (No. 2021YFB3301503), the National Natural Science Foundation of China (No. 62376026), and also sponsored by Beijing Nova Program (No. 20230484296).

## References

* [1] R. Aljundi, M. Lin, B. Goujaud, and Y. Bengio. Gradient based sample selection for online continual learning. In _NeurIPS_, pages 11816-11825, 2019.
* [2] M. Arjovsky, L. Bottou, I. Gulrajani, and D. Lopez-Paz. Invariant risk minimization. _arXiv:1907.02893_, 2019.
* [3] G. Bai, C. Ling, and L. Zhao. Temporal domain generalization with drift-aware dynamic neural networks. In _ICLR_, 2023.
* [4] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. W. Vaughan. A theory of learning from different domains. _Mach. Learn._, 79(1-2):151-175, 2010.
* [5] S. Ben-David, J. Blitzer, K. Crammer, and F. Pereira. Analysis of representations for domain adaptation. In _NeurIPS_, pages 137-144, 2006.
* [6] G. Blanchard, G. Lee, and C. Scott. Generalizing from several related classification tasks to a new unlabeled sample. In _NeurIPS_, pages 2178-2186, 2011.
* [7] G. Blanchard, G. Lee, and C. Scott. Generalizing from several related classification tasks to a new unlabeled sample, 2011.
* [8] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In _NeurIPS_, 2020.
* [9] F. M. Castro, M. J. Marin-Jimenez, N. Guil, C. Schmid, and K. Alahari. End-to-end incremental learning. In _ECCV_, pages 233-248, 2018.
* [10] A. Chaudhry, M. Ranzato, M. Rohrbach, and M. Elhoseiny. Efficient lifelong learning with A-GEM. In _ICLR_, 2019.
* [11] T. Chen, S. Kornblith, M. Norouzi, and G. E. Hinton. A simple framework for contrastive learning of visual representations. In _ICML_, pages 1597-1607, 2020.
* [12] X. Chen, W. Chen, V. Dinavahi, Y. Liu, and J. Feng. Short-term load forecasting and associated weather variables prediction using resnet-lstm based deep learning. _IEEE Access_, 11:5393-5405, 2023.
* [13] L. Deng. The MNIST database of handwritten digit images for machine learning research [best of the web]. _SPM_, 29(6):141-142, 2012.
* [14] S. Ebrahimi, F. Meier, R. Calandra, T. Darrell, and M. Rohrbach. Adversarial continual learning. In _ECCV_, volume 12356, pages 386-402, 2020.
* [15] Y. Ganin and V. Lempitsky. Unsupervised domain adaptation by backpropagation. In _ICML_, pages 1180-1189, 2015.
* [16] S. Ginosar, K. Rakelly, S. Sachs, B. Yin, and A. A. Efros. A century of portraits: A visual historical record of american high school yearbooks. In _ICCV Workshops_, pages 652-658, 2015.
* [17] R. Gong, W. Li, Y. Chen, and L. Van Gool. Dlow: Domain flow for adaptation and generalization. In _CVPR_, 2020.
* [18] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger. Densely connected convolutional networks. In _CVPR_, pages 2261-2269, 2017.
* [19] Y. Iwasawa and Y. Matsuo. Test-time classifier adjustment module for model-agnostic domain generalization. In _NeurIPS_, pages 2427-2440, 2021.

* [20] Y. Iwasawa and Y. Matsuo. Test-time classifier adjustment module for model-agnostic domain generalization. In _NeurIPS_, pages 2427-2440, 2021.
* [21] P. Izmailov, D. Podoprikhin, T. Garipov, D. P. Vetrov, and A. G. Wilson. Averaging weights leads to wider optima and better generalization. In _UAI_, pages 876-885, 2018.
* [22] J. Kirkpatrick, R. Pascanu, N. C. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, and R. Hadsell. Overcoming catastrophic forgetting in neural networks. _arXiv:1612.00796_, 2016.
* [23] A. Kumar, T. Ma, and P. Liang. Understanding self-training for gradual domain adaptation. In _ICML_, pages 5468-5479, 2020.
* [24] M. D. Lange, R. Aljundi, M. Masana, S. Parisot, X. Jia, A. Leonardis, G. G. Slabaugh, and T. Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. _TPAMI_, 44(7):3366-3385, 2022.
* [25] D. Li, Y. Yang, Y. Song, and T. M. Hospedales. Deeper, broader and artier domain generalization. In _ICCV_, pages 5543-5551, 2017.
* [26] D. Li, Y. Yang, Y. Song, and T. M. Hospedales. Learning to generalize: Meta-learning for domain generalization. In _AAAI_, pages 3490-3497, 2018.
* [27] H. Li, S. J. Pan, S. Wang, and A. C. Kot. Domain generalization with adversarial feature learning. In _CVPR_, pages 5400-5409, 2018.
* [28] S. Li, C. H. Liu, Q. Lin, Q. Wen, L. Su, G. Huang, and Z. Ding. Deep residual correction network for partial domain adaptation. _TPAMI_, 43(7):2329-2344, 2021.
* [29] S. Li, C. H. Liu, B. Xie, L. Su, Z. Ding, and G. Huang. Joint adversarial domain adaptation. In _ACM MM_, pages 729-737, 2019.
* [30] Z. Li and D. Hoiem. Learning without forgetting. _TPAMI_, 40(12):2935-2947, 2018.
* [31] T. Lin, Y. Wang, X. Liu, and X. Qiu. A survey of transformers. _AI Open_, 3:111-132, 2022.
* [32] A. Liu, Y.-C. Liu, Y.-Y. Yeh, and Y.-C. Wang. A unified feature disentangler for multi-domain image translation and manipulation, 2018.
* [33] H. Liu, M. Long, J. Wang, and Y. Wang. Learning to adapt to evolving domains, 2020.
* [34] M. Long, Z. Cao, J. Wang, and M. I. Jordan. Conditional adversarial domain adaptation. In _NeurIPS_, pages 1647-1657, 2018.
* [35] F. Lv, J. Liang, K. Gong, S. Li, C. H. Liu, H. Li, D. Liu, and G. Wang. Pareto domain adaptation. In _NeurIPS_, pages 12917-12929, 2021.
* [36] F. Lv, J. Liang, S. Li, B. Zang, C. H. Liu, Z. Wang, and D. Liu. Causality inspired representation learning for domain generalization. In _CVPR_, pages 8036-8046, 2022.
* [37] F. Lv, J. Liang, S. Li, J. Zhang, and D. Liu. Improving generalization with domain convex game. In _CVPR_, pages 24315-24324, 2023.
* [38] L. v. d. Maaten and G. Hinton. Visualizing data using t-sne. _JMLR_, 9:2579-2605, 2008.
* [39] K. Muandet, D. Balduzzi, and B. Scholkopf. Domain generalization via invariant feature representation. In _ICML_, pages 10-18, 2013.
* [40] Z. Nado, S. Padhy, D. Sculley, A. D'Amour, B. Lakshminarayanan, and J. Snoek. Evaluating prediction-time batch normalization for robustness under covariate shift. _arXiv:2006.10963_, 2020.
* [41] A. Nasery, S. Thakur, V. Piratla, A. De, and S. Sarawagi. Training for the future: A simple gradient interpolation loss to generalize along time. In _NeurIPS_, pages 19198-19209, 2021.

* [42] A. Nguyen, T. Tran, Y. Gal, and A. Baydin. Domain invariant representation learning with domain density transformations, 2021.
* [43] G. Ortiz-Jimenez, M. E. Gheche, E. Simou, H. P. Maretic, and P. Frossard. CDOT: continuous domain adaptation using optimal transport. _arXiv:1909.11448_, 2019.
* [44] T. Qin, S. Wang, and H. Li. Generalizing to evolving domains with latent structure-aware sequential autoencoder. In _ICML_, pages 18062-18082, 2022.
* [45] J. Rajasegaran, M. Hayat, S. H. Khan, F. S. Khan, and L. Shao. Random path selection for incremental learning. _arXiv:906.01120_, 2019.
* [46] S. Rebuffi, A. Kolesnikov, G. Sperl, and C. H. Lampert. icarl: Incremental classifier and representation learning. In _CVPR_, pages 5533-5542, 2017.
* [47] G. Saha and K. Roy. Continual learning with scaled gradient projection. In _AAAI_, pages 9677-9685, 2023.
* [48] V. Sanh, L. Debut, J. Chaumond, and T. Wolf. Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. _arXiv:1910.01108_, 2019.
* [49] S. Schneider, E. Rusak, L. Eck, O. Bringmann, W. Brendel, and M. Bethge. Improving robustness against common corruptions by covariate shift adaptation. In _NeurIPS_, 2020.
* [50] B. Sun and K. Saenko. Deep coral: Correlation alignment for deep domain adaptation. In _ECCV Workshops_, pages 443-450, 2016.
* [51] R. Taori, A. Dave, V. Shankar, N. Carlini, B. Recht, and L. Schmidt. Measuring robustness to natural distribution shifts in image classification. In _NeurIPS_, 2020.
* [52] R. Tiwari, K. Killamsetty, R. K. Iyer, and P. Shenoy. GCR: gradient coreset based replay buffer selection for continual learning. In _CVPR_, pages 99-108, 2022.
* [53] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In _IROS_, 2017.
* [54] D. Wang, E. Shelhamer, S. Liu, B. A. Olshausen, and T. Darrell. Tent: Fully test-time adaptation by entropy minimization. In _ICLR_, 2021.
* [55] H. Wang, H. He, and D. Katabi. Continuously indexed domain adaptation. In _ICML_, volume 119, pages 9898-9907, 2020.
* [56] H. Yao, C. Choi, B. Cao, Y. Lee, P. W. Koh, and C. Finn. Wild-time: A benchmark of in-the-wild distribution shift over time. In _NeurIPS_, 2022.
* [57] H. Yao, Y. Wang, S. Li, L. Zhang, W. Liang, J. Zou, and C. Finn. Improving out-of-distribution robustness via selective augmentation. In _ICML_, pages 25407-25437, 2022.
* [58] L. Yuan, B. Xie, and S. Li. Robust test-time adaptation in dynamic scenarios. In _CVPR_, pages 15922-15932, 2023.
* [59] Q. Zeng, W. Wang, F. Zhou, C. Ling, and B. Wang. Foresee what you will learn: Data augmentation for domain generalization in non-stationary environments. _arXiv:2301.07845_, 2023.
* [60] F. Zenke, B. Poole, and S. Ganguli. Continual learning through synaptic intelligence. In _ICML_, pages 3987-3995, 2017.
* [61] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz. mixup: Beyond empirical risk minimization. In _ICLR_, 2018.
* [62] K. Zhou, Z. Liu, Y. Qiao, T. Xiang, and C. C. Loy. Domain generalization: A survey. _TPAMI_, 45(4):4396-4415, 2023.
* [63] K. Zhou, Y. Yang, Y. Qiao, and T. Xiang. Domain generalization with mixstyle. In _ICLR_, 2021.

## Appendix A Broader Impacts & Limitations

1. [label=]
2. Ethics Statement & Licenses
3. Algorithm of EvoS
4. Experimental Setup Details
5. Experimental Results with Error Bars
6. Complexity Analysis
7. Additional Results

## Appendix A Broader Impacts & Limitations

Our work focus on the problem of continual domain generalization over temporal drift (CDGTD), which aims to generalize the model to unseen future domains by leveraging underlying evolutionary patterns. The effectiveness of our method on several real-world datasets means that it may potentially benefit relevant applications and communities that deal with temporal drifts, e.g., advertisement recommendation, autonomous driving, popularity forecast of media content, etc. Nevertheless, we should also be cautious about possible failures of our method when encountering sudden distribution shifts. In the future, we may explore the automatic identification of domains with severe distribution shifts, allowing us to proactively reject them and mitigate the risk of severe accidents.

## Appendix B Ethics Statement & Licenses

All the datasets used in the paper are publicly available and are only intended to compare the performances of different algorithms on classification tasks, adhering to the following licenses of these datasets:

* Yearbook: MIT licensed
* MNIST: CC BY-SA 3.0
* fMoW: The Functional Map of the World Challenge Public License
* Huffpost: CC0: Public Domain
* arXiv: CC0: Public Domain

## Appendix C Algorithm of EvoS

The training and inference procedure for EvoS is provided in Algorithm 1 and 2, respectively.

## Appendix D Experimental Setup Details

### Dataset Description

**Yearbook** used in our paper is from [56]. It is based on the Portraits [16] dataset (MIT license), containing the \(32\times 32\) grayscale images of yearbook portraits from 128 American high schools in 27 states. The data spanning from 1930 to 2013 reflects the evolving fashion styles and shifting social norms throughout the decades. The task is the binary classification of genders. Note that it is only used for comparing the generalization performances of different algorithms on classification tasks. For this dataset, we treat every four years as a domain, resulting in 16 domains. Table 11 provides the number of samples in each domain. The first 16 domains are used for training (\(T=16\)), the last 5 domains for testing (\(K=5\)).

**RMNIST** is a variant of the MNIST [13] dataset, which comprises 9 domains generated by applying the rotations with degree of \(0^{\circ},10^{\circ},\cdots,80^{\circ}\) in order to MNIST to simulate the temporal drift. The task is to classify a \(28\times 28\) grayscale digit image from 0 to 9. We use the first 6 domains for training (\(T=6\)) and the last 3 for testing (\(K=3\)).

**fMoW** used in our paper is from [56]. It collects the \(224\times 224\) RGB satellite images from 2002 to 2017 over 200 countries, where the visual features present in satellite data undergo changes over time due to both human and environmental activities. The task to classify the functional purpose of the buildings or the land in the images into one of 62 categories. For this dataset, we treat every year as a domain and Table 12 provides the number of samples in each domain. The first 13 domains are used for training (\(T=13\)) and the last 3 domains are used for testing \((K=3)\).

**Huffpost** in [56] comprises news headlines from the Huffington Post from 2012 to 2018, the task of which is to classify the news headline into one of 11 news categories ("Black Voices", "Business", "Comedy", "Crime", "Entertainment", "Impact", "Queer Voices", "Science", "Sports", "Tech", "Travel"). The data spanning for 2012 to 2018 presents changes in the content or style of news along the time dimension. For this dataset, we use the first 4 years for training (\(T=4\)) and the last 3 years for testing (\(K=3\)). The number of samples in each domain is provided in Table 13.

**Arxiv** in [56] contains paper titles and their corresponding primary categories spanning from 2007 to 2022. The content of Arxiv preprints evolves over time, reflecting the dynamic nature of research fields. And the task is to classify a research paper into one of 172 categories based on its title. We use the data from the first 9 domains for training (\(T=9\)) and the data from the last 7 years for testing (\(K=7\)). The number of samples in each domain is presented in Table 14.

For each training domain of all datasets, we randomly select 90% of data as training split and 10% of data as validation split. The evaluation of generalization performance is based on the whole data of each test domain.

### Implementation Details

All experiments are implemented via PyTorch and the backbones we use mainly adhere to [56].

**Yearbook** uses a 4-layer convolutional network from [56] as the backbone. For multi-scale attention module (MSAM), we set the dimension of head as \(d_{h}=8\) for each attention module \(\mathcal{A}_{w}\) and the number of heads for \(\mathcal{A}_{w}\) as \(w\cdot n_{h}\), with \(n_{h}=16,w=1,\cdots,W\). And the Adam optimizer with \(lr=1e-3\) is used to optimize the model, where the batch size \(B\) is set to 64 and the training epochs of each domain are set to 50. As for the hyper-parameters, we use \(\alpha=1.0,\lambda=1.0,W=3\).

**RMNIST** adopts the ConvNet in [44] as the backbone, and we set the dimension of head as \(d_{h}=8\) for each attention module \(\mathcal{A}_{w}\) and the number of heads for \(\mathcal{A}_{w}\) as \(w\cdot n_{h}\), with \(n_{h}=32,w=1,\cdots,W\). The Adam optimizer with \(lr=1e-3\) is adopted for model optimization. The batch size and training epochs of each domain are set to 64 and 50, respectively. And \(\alpha=2.0,\lambda=1.0,W=3\) is used.

**fMoW** employ the DenseNet-121 [18] pretrained on ImageNet as the backbone. Besides, we use a bottleneck layer [28] to reduce the feature dimensions into 256, and set the dimension of head as \(d_{h}=8\) for each attention module \(\mathcal{A}_{w}\) and the number of heads for \(\mathcal{A}_{w}\) as \(w\cdot n_{h}\), with \(n_{h}=64,w=1,\cdots,W\). Similarly, the Adam optimizer is used, where the learning rate is set to \(lr=2e-4\). The batch size is set to \(B=64\) and each training domain is trained for 25 epochs. As for the hyper-parameters, we set \(\alpha=1.0,\lambda=1.0,W=3\) for fMoW dataset.

**Huffpost** and **Arxiv** use the pretrained DistilBERT base model [48] as the backbone. For MSAM, we set the dimension of head as \(d_{h}=8\) for each attention module \(\mathcal{A}_{w}\) and the number of heads for \(\mathcal{A}_{w}\) as \(w\cdot n_{h}\), with \(n_{h}=128,w=1,\cdots,W\). Also the Adam optimizer is used with the learning rate \(lr=2e-5\) and batch size \(B=64\). And \(\alpha=1.0,\lambda=1.0,W=3\) is used for the two dataset. For the Huffpost dataset, each training is trained for 50 epochs, while the training epochs of each domain are set to 5 for the Arxiv dataset.

We run each task on a single NVIDIA GeForce RTX 3090 GPU for three random trials. For baselines, we also select their hyper-parameters via the grid search using the validation splits of training domains.

## Appendix E Experimental Results with Error Bars

In this section, we report the mean and standard derivation (denoted as mean (std) ) for each task, when running with 3 random trials. The results with error bars on Yearbook, RMNIST and fMoW are provided in Table 5, and the error bars on Huffpost and Arxiv are given in Table 6.

## Appendix F Complexity Analysis

**Time Complexity of MSAM.** Taking one of the multi-head self-attention module \(\mathcal{A}_{w}\) in MSAM as an example, we denote \(d_{f}\) as the feature dimension of its input tokens, and \(d_{h}\), \(n_{h}\) and \(n_{i}\) denote the feature dimension of its heads, the number of its heads and the number of its input tokens, respectively. Then the time complexity of \(\mathcal{A}_{w}\) is \(\mathcal{O}((n_{i}^{2}+n_{i}\cdot d_{f})\cdot(d_{h}\cdot n_{h}))\). Since \(n_{i}\) will be no larger than the number of training domains \(T\) and \(d_{h}\cdot n_{h}\) is usually set to \(d_{f}\) in transformers, the time complexity of MSAM can be roughly approximated as \(\mathcal{O}(W\cdot(T^{2}d_{f}+T\cdot d_{f}^{2}))\), where \(W\) is the number of multi-head self-attention modules in MSAM. It is roughly equivalent to the time complexity of a single multi-head self-attention layer in conventional transformers multiplied by \(W\).

**Memory Complexity of \(\mathcal{M}\).** Assuming that there are \(T\) historical domains and the dimension of statistic vectors is \(d_{f}\), then the memory complexity of the memory pool \(\mathcal{M}\) is \(\mathcal{O}(T\cdot d_{f})\). In practice, after being processed by deep neural networks, the dimension of pooled features is usually much smaller than that of original inputs. Moreover, only two vectors need to be stored per domain. Hence, the memory cost of \(\mathcal{M}\) is relatively small, compared with sample replay-based CL methods. Concretely, Table 7 provides the memory cost of \(\mathcal{M}\) and the increment of GPU memory cost for EvoS on Yearbook, RMNIST and fMoW datasets with batch size 64.

\begin{table}
\begin{tabular}{|c|c|c|c|c c c|c c c|} \hline \multirow{3}{*}{Method} & \multirow{2}{*}{Incremental} & \multirow{2}{*}{\begin{tabular}{c} Access \\ training \\ \end{tabular} } & \multicolumn{3}{c|}{\begin{tabular}{c} Affployed \\ \(\mathcal{D}^{T+1}\) \\ \end{tabular} } & \multicolumn{3}{c|}{
\begin{tabular}{c} Activity \\ accuracy (\%) \\ \end{tabular} } \\  & & & \multicolumn{3}{c|}{Accuracy (\%) \(\uparrow\)} \\  & & & \multicolumn{3}{

**Model Complexity.** Here, we measure the model complexity by the number of parameters. Specifically, Table 8 presents the model complexity of our method EvoS and other two temporal DG methods LSSAE [44] and DRAIN [3] on Yearbook, RMNIST and fMoW datasets. LSSAE [44] introduces sequential autoencoder to explore the underlying continuous structure in the latent space of deep neural networks, where the complicated VAE and LSTM networks require lots of parameters. And DRAIN [3] needs to encode and decode the entire network parameters, which also requires a great number of parameters for large backbone networks. By contrast, our EvoS is overall less complex than these temporal DG methods, and is more friendly to the relatively large backbone network.

## Appendix G Additional Results

**Results under Eval-Stream Manner.** In this part, we additionally provide the results when adopting the evaluation manner of Eval-Stream in [56]. Specifically, Eval-Stream denotes the evaluation with domain stream, i.e., the model is evaluated at each timestamp using the average and worst performance on the next \(K\) timestamps. Formally, given a sequence of domains with total length \(\mathcal{T}\), the average performance \(Avg_{stream}\) and worst performance \(Worst_{stream}\) under the strategy of Eval-Stream are defined as \(Avg_{stream}=\frac{1}{\mathcal{T}-K}\sum_{i=1}^{\mathcal{T}-K}\frac{1}{K} \sum_{j=i+1}^{i+K}Acc_{i}(\mathcal{D}^{j}),\)\(Worst_{stream}=\frac{1}{\mathcal{T}-K}\sum_{i=1}^{\mathcal{T}-K}\min_{j\in\{i+1, \cdots,i+K\}}Acc_{i}(\mathcal{D}^{j}),\) where \(Acc_{i}(\mathcal{D}^{j})\) is the accuracy on domain \(\mathcal{D}^{j}\) when using the model at the \(i\)-th timestamp. And Table 9 shows the results on Yearbook and Huffpost datasets when using the Eval-Stream evaluation manner. According to the results, we see that EvoS still outperforms other baselines, showing that our method is better at handling the problem of continual domain generalization over temporal drift.

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline \multicolumn{4}{|c|}{GPU memory cost (GB) of different methods} \\ \hline \multirow{3}{*}{Method} & \multicolumn{3}{c|}{Dataset} \\ \cline{2-4}  & Yearbook & RMNIST & fMoW \\ \cline{2-4}  & \multicolumn{3}{c|}{Backbone} \\ \cline{2-4}  & 4-layer CNN [56] & ConvNet [44] & DenseNet-121 [18] \\ \hline IncFinetune & 1.72 & 1.84 & 10.69 \\ EvoS & 1.94 & 2.09 & 11.04 \\ Increment \(\Delta\) (GB) & 0.22 & 0.25 & 0.35 \\ \hline \multicolumn{4}{|c|}{Memory cost (MB) of the memory pool \(\mathcal{M}\)} \\ \hline EvoS & 5.25 & 9.00 & 32.00 \\ \hline \end{tabular}
\end{table}
Table 7: Memory cost on Yearbook, RMNIST and fMoW datasets.

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline \multicolumn{4}{|c|}{Dataset} \\ \cline{2-4} Method & Yearbook & RMNIST & fMoW \\ \cline{2-4}  & \multicolumn{3}{c|}{Backbone} \\ \cline{2-4}  & 4-layer CNN [56] & ConvNet [44] & DenseNet-121 [18] \\ \hline LSSAE [44] & 4.70 & 23.25 & 90.92 \\ DRAIN [3] & 7.51 & 184.29 & 1113.85 \\ EvoS & 1.94 & 15.81 & 56.11 \\ \hline \end{tabular}
\end{table}
Table 8: Model parameters (MB) of different methods on Yearbook, RMNIST and fMoW.

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline \multirow{3}{*}{Method} & \multicolumn{3}{c|}{Dataset} \\ \cline{2-4}  & Yearbook & RMNIST & fMoW \\ \cline{2-4}  & \multicolumn{3}{c|}{Backbone} \\ \cline{2-4}  & 4-layer CNN [56] & ConvNet [44] & DenseNet-121 [18] \\ \hline LSSAE [44] & 4.70 & 23.25 & 90.92 \\ DRAIN [3] & 7.51 & 184.29 & 1113.85 \\ EvoS & 1.94 & 15.81 & 56.11 \\ \hline \end{tabular}
\end{table}
Table 8: Model parameters (MB) of different methods on Yearbook, RMNIST and fMoW.

**Results on More Datasets.** In addition to the datasets provided in [56], we also run our method on the **2-Moons**, _ONP_ and _Elec2_ datasets used in [3]. Specifically, **2-Moons** is a variant of the 2-entangled moons dataset by rotating data counter-clockwise in units of \(18^{\circ}\) to construct 10 domains, where the rotation angle is used to simulate the temporal shift. For 2-Moons, we use Adam optimizer and a MLP with two hidden layers of hidden size 64 and 128, and \(B=64,lr=1e-3,\alpha=1.0,\lambda=1.0,W=3,d_{h}=8,n_{h}=32,T=9,K=1\). **Online News Popularity (ONP)** summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The dataset is split into 6 domains by time and the goal is to predict the number of shares in social networks (popularity). For ONP, we use Adam optimizer and a MLP with one hidden layer of hidden size 128, and \(B=64,lr=1e-4,\alpha=1.0,\lambda=1.0,W=3,d_{h}=8,n_{h}=32,T=5,K=1\). **Electrical Demand (Elec2)** contains information about the demand of electricity in a particular province. Following [3; 41], the first 30 domains in Elec2 are used (two weeks as one time domain) and the task is to predict whether the demand of electricity in each period (of 30 mins) is higher or lower than the average demand over the last day. For Elec2, we use Adam optimizer and a MLP with two hidden layers of hidden size 128 and 128, and \(B=64,lr=1e-4,\alpha=1.0,\lambda=1.0,W=3,d_{h}=8,n_{h}=32,T=29,K=1\). For these datasets, as in [3; 41], we use the last domain for testing and the rest for training. Please refer to [3] for more dataset details. The experimental results are given in Table 10, where the misclassification errors of compared baselines are all reported from DRAIN [3]. From Table 10, we can observe that our method EvoS still surpasses the most recent method DRAIN, affirming its effectiveness in temporal domain generalization.

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline \multirow{2}{*}{Method} & \multirow{2}{*}{Conference} & \multicolumn{3}{c|}{Misclassification error (in \%) \(\downarrow\)} \\ \cline{3-5}  & & 2-Moons & ONP & Elec2 \\ \hline Offline & - & 22.4\(\pm\)4.6 & 33.8\(\pm\)0.6 & 23.0\(\pm\)3.1 \\ LastDomain & - & 14.9\(\pm\)0.9 & 36.0\(\pm\)0.2 & 25.8\(\pm\)0.6 \\ IncFinetune & - & 16.7\(\pm\)3.4 & 34.0\(\pm\)0.3 & 27.3\(\pm\)4.2 \\ CDOT [43] & arXiv’19 & 9.3\(\pm\)1.0 & 34.1\(\pm\)0.0 & 17.8\(\pm\)0.6 \\ CIDA [55] & ICML’20 & 10.8\(\pm\)1.6 & 34.7\(\pm\)0.6 & 14.1\(\pm\)0.2 \\ GI [41] & NeurIPS’21 & 3.5\(\pm\)1.4 & 36.4\(\pm\)0.8 & 16.9\(\pm\)0.7 \\ DRAIN [3] & ICLR’23 & 3.2\(\pm\)1.2 & 38.3\(\pm\)1.2 & 12.7\(\pm\)0.8 \\
**EvoS** & - & **2.5\(\pm\)1.0** & **33.1\(\pm\)0.6** & **11.6\(\pm\)0.7** \\ \hline \end{tabular}
\end{table}
Table 10: Misclassification error (%) on 2-Moons, ONP and Elec2. (\(K=1\))

\begin{table}
\begin{tabular}{|c|c|c c c|} \hline Domain & Interval & Training Split & Validation Split & All \\ \hline

[MISSING_PAGE_POST]

\begin{table}
\begin{tabular}{|c|c|c c c|} \hline Domain & Year & Training Split & Validation Split & All \\ \hline
1 & 2012 & 6701 & 744 & 7446 \\
2 & 2013 & 7492 & 832 & 8325 \\
3 & 2014 & 9539 & 1059 & 10599 \\
4 & 2015 & 11826 & 1313 & 13140 \\ \hline
5 & 2016 & 10548 & 1172 & 11721 \\
6 & 2017 & 7907 & 878 & 8786 \\
7 & 2018 & 3501 & 388 & 3890 \\ \hline total & 2012-2018 & 57514 & 6386 & 63907 \\ \hline \end{tabular}
\end{table}
Table 13: Data Subset Size for the Huffpost Dataset.

\begin{table}
\begin{tabular}{|c|c|c c c|} \hline Domain & Year & Training Split & Validation Split & All \\ \hline
1 & 2002 & 1676 & 227 & 1903 \\
2 & 2003 & 2279 & 276 & 2555 \\
3 & 2004 & 1755 & 240 & 1995 \\
4 & 2005 & 2512 & 324 & 2836 \\
5 & 2006 & 3155 & 406 & 3561 \\
6 & 2007 & 1497 & 190 & 1687 \\
7 & 2008 & 2261 & 298 & 2559 \\
8 & 2009 & 7439 & 935 & 8374 \\
9 & 2010 & 18957 & 2456 & 21413 \\
10 & 2011 & 22111 & 2837 & 24948 \\
11 & 2012 & 24704 & 3138 & 27842 \\
12 & 2013 & 3465 & 385 & 3850 \\
13 & 2014 & 5572 & 620 & 6192 \\ \hline
14 & 2015 & 8885 & 988 & 9873 \\
15 & 2016 & 14363 & 1596 & 15959 \\
16 & 2017 & 5534 & 615 & 6149 \\ \hline total & 2002-2017 & 126165 & 15531 & 141696 \\ \hline \end{tabular}
\end{table}
Table 12: Data Subset Size for the fMoW Dataset.

\begin{table}
\begin{tabular}{|c|c|c c c|} \hline Domain & Year & Training Split & Validation Split & All \\ \hline
1 & 2007 & 131550 & 14616 & 146167 \\
2 & 2008 & 62460 & 6939 & 69400 \\
3 & 2009 & 206244 & 22916 & 229161 \\
4 & 2010 & 50665 & 5629 & 56295 \\
5 & 2011 & 55741 & 6193 & 61935 \\
6 & 2012 & 51678 & 5741 & 57420 \\
7 & 2013 & 64951 & 7216 & 72168 \\
8 & 2014 & 79498 & 8833 & 88332 \\
9 & 2015 & 193979 & 21553 & 215533 \\ \hline
10 & 2016 & 120682 & 13409 & 134092 \\
11 & 2017 & 111024 & 12336 & 123361 \\
12 & 2018 & 123891 & 13765 & 137657 \\
13 & 2019 & 142767 & 15862 & 158630 \\
14 & 2020 & 166014 & 18445 & 184460 \\
15 & 2021 & 201241 & 22360 & 223602 \\
16 & 2022 & 89765 & 9973 & 99739 \\ \hline total & 2007-2022 & 1852150 & 205786 & 2057952 \\ \hline \end{tabular}
\end{table}
Table 14: Data Subset Size for the Arxiv Dataset.