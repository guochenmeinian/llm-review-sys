# Matrix Compression via Randomized Low Rank

and Low Precision Factorization

 Rajarshi Saha, Varun Srivastava, Mert Pilanci

Department of Electrical Engineering

Stanford University

Stanford, CA 94305, USA

{rajsaha,vsriva,pilanci}@stanford.edu

###### Abstract

Matrices are exceptionally useful in various fields of study as they provide a convenient framework to organize and manipulate data in a structured manner. However, modern matrices can involve billions of elements, making their storage and processing quite demanding in terms of computational resources and memory usage. Although prohibitively large, such matrices are often approximately low rank. We propose an algorithm that exploits this structure to obtain a low rank decomposition of any matrix \(\mathbf{A}\) as \(\mathbf{A}\approx\mathbf{L}\mathbf{R}\), where \(\mathbf{L}\) and \(\mathbf{R}\) are the low rank factors. The total number of elements in \(\mathbf{L}\) and \(\mathbf{R}\) can be significantly less than that in \(\mathbf{A}\). Furthermore, the entries of \(\mathbf{L}\) and \(\mathbf{R}\) are quantized to low precision formats - compressing \(\mathbf{A}\) by giving us a low rank and low precision factorization. Our algorithm first computes an approximate basis of the range space of \(\mathbf{A}\) by randomly sketching its columns, followed by a quantization of the vectors constituting this basis. It then computes approximate projections of the columns of \(\mathbf{A}\) onto this quantized basis. We derive upper bounds on the approximation error of our algorithm, and analyze the impact of target rank and quantization bit-budget. The tradeoff between compression ratio and approximation accuracy allows for flexibility in choosing these parameters based on specific application requirements. We empirically demonstrate the efficacy of our algorithm in image compression, nearest neighbor classification of image and text embeddings, and compressing the layers of LlaMa-7b. Our results illustrate that we can achieve compression ratios as aggressive as one bit per matrix coordinate, all while surpassing or maintaining the performance of traditional compression techniques.

## 1 Introduction

Low-rank structures for matrices have proven to be incredibly valuable and ubiquitous across numerous fields of study. Several real-world matrices approximately exhibit low-rank structure due to inherent redundancy or patterns, allowing them to be approximated using low-rank factors. Udell and Townsend [69] provide a potential justification by considering a generative model with latent variables for real-world matrices. Applications where low-rank structures in matrices are exploited include, but are not limited to, imaging (Lingala et al. [34]), fine-tuning large language models (Aghajanyan et al. [3], Hu et al. [25], Karimi Mahabadi et al. [30], Valipour et al. [70], Wang et al. [76]), compressing neural networks (Ben Noach and Goldberg [8], Idelbayev and Carreira-Perpinan [26], Mao et al. [39], Phan et al. [47], Swaminathan et al. [63], Tahaei et al. [64], Wang et al. [77], Winata et al. [78], Yu et al. [84]), obtaining efficient NN architectures (Jaderberg et al. [27], Tai et al. [65]), etc.

Given a matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\), a low-rank approximation is given by \(\mathbf{A}\approx\mathbf{L}\mathbf{R}\), where \(\mathbf{L}\in\mathbb{R}^{n\times m}\), \(\mathbf{R}\in\mathbb{R}^{m\times d}\) denote the _left_ and _right_ low-rank factors with \(m\ll\min\{n,d\}\). Since \(m(n+d)\ll nd\)the total number of entries in \(\mathbf{LR}\) can be significantly smaller than \(\mathbf{A}\), enabling matrix compression. The need for compression is driven by the overwhelming storage demands and computational complexity linked to large matrices. In a comparable but distinct line of work, low-precision (LP) representations have also been studied extensively as a means to reduce the memory footprint. Quantization of a continuous valued variable using a small number of bits reduces storage requirement while trading off accuracy. In addition, they also facilitate low latency for real-time inference and low energy consumption (see Gholami et al. [21]). Works of Alimisis et al. [4], Safaryan et al. [55] have studied various matrix compression operators for distributed optimization.

In this work, we introduce **LPLR**: _Low-Precision Low-Rank_ factorization - a general matrix compression algorithm that simultaneously exploits the low-rank structure of a matrix and quantizes it, to obtain a low-rank factorization, such that the elements of the factors are represented using a small number of bits. Although LPLR can generically refer to a class of algorithms whose main goal is to obtain low-precision representations and exploiting low-rank structures in matrices, in the rest of the paper, we reserve this acronym for our proposed algorithm in Alg. 1. Fig. 1 shows the effectiveness of LPLR in preserving the semantic features in image compression. Other algorithms designed towards achieving the same objective have been named and described appropriately throughout the paper.

### Related Works

**Low-rank approximation**: The optimal rank-\(k\) approximation of a matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\), denoted as \(\mathbf{A}_{k}\), can be obtained by performing the singular value decomposition (SVD) of \(\mathbf{A}\), which gives \(\mathbf{A}=\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\top}\). To obtain \(\mathbf{A}_{k}\), we keep only the top \(k\) singular values along with their corresponding left and right singular vectors, setting the remaining singular values to zero. In other words, \(\mathbf{A}_{k}=\mathbf{U}_{k}\mathbf{\Sigma}_{k}\mathbf{V}_{k}^{\top}=\sum_{i =1}^{k}\sigma_{i}\mathbf{u}_{i}\mathbf{v}_{i}^{\top}\), where \(\sigma_{i}\), \(\mathbf{u}_{i}\), and \(\mathbf{v}_{i}\) represent the \(i\)-th singular value, left singular vector, and right singular vector, respectively. The matrix \(\mathbf{A}_{k}\) minimizes the Frobenius norm \(\|\mathbf{A}-\widehat{\mathbf{A}}\|_{\mathrm{F}}\) under the constraint that the rank of \(\widehat{\mathbf{A}}\) is at most \(k\). This result is known as the Eckart-Young-Mirsky theorem [18], which states that \(\|\mathbf{A}-\mathbf{A}_{k}\|_{\mathrm{F}}^{2}=\sum_{i>k}\sigma_{i}^{2}\). However, computing the SVD has a high computational complexity of \(\mathrm{O}(nd^{2})\) (\(n\geq d\) without loss of generality), which can be impractical for large matrices. Therefore, alternative methods such as in Jin et al. [28], Ye and Du [82], Zhang et al. [85] have been proposed which solve variants of the minimization problem. Chi et al. [11] provide a survey of optimization-based approaches for obtaining low-rank approximation.

Along a parallel line of work, randomized low-rank factorization algorithms have demonstrated their effectiveness in handling massive datasets efficiently while maintaining competitive accuracy levels. Several works, including those by Derezinski et al. [13], Drineas and Mahoney [16], Drineas et al. [17], Halko et al. [23], Ma and Solomonik [37], Mahoney [38], Martinsson et al. [43], Tropp et al. [67], Witten and Candes [79] have focused on reducing the complexity of computing SVD to approximately \(\mathrm{O}(nm^{2})\), where \(m\ll d\). These algorithms aim to approximate the range space of matrix \(\mathbf{A}\) by utilizing a sketched version \(\mathbf{AS}\), where \(\mathbf{S}\in\mathbb{R}^{d\times m}\) is a random matrix with \(m\ll d\). The columns of \(\mathbf{A}\) are then projected onto this approximate range space. By choosing \(m=k+p\), where \(p\geq 2\) is a small integer, the resulting low-rank approximation \(\mathbf{A}\approx\mathbf{LR}\) provided by these algorithms can be proven to be within a small multiplicative factor of the optimal rank-\(k\) approximation, stated as \(\mathbb{E}\left\|\mathbf{LR}-\mathbf{A}\right\|_{\mathrm{F}}^{2}\leq(1+\delta )\left\|\mathbf{A}_{k}-\mathbf{A}\right\|_{\mathrm{F}}^{2}\). The sketching matrix \(\mathbf{S}\) is typically selected from the

Figure 1: Compression of Shepp-Logan phantom (a standard test image for medical image reconstruction). Naive quant. was done with \(2\)-bits per pixel of this \(10^{3}\times 10^{3}\) image. Quantizing the SVD factors “directly” (i.e., DSVD) and (our) LPLR/LSVD algorithms, factorize the image into a product of tall & wide matrices which reduces the total number of elements, allowing each entry to be represented using upto \(8\)-bits of precision per pixel. Despite the increase in precision per pixel, the total number of bits remains the same at \(2\cdot 10^{6}\).

widely used class of Johnson-Lindenstrauss (JL) embeddings. A popular choice when \(\mathbf{A}\) is dense is to sample a Gaussian matrix where each entry \(S_{ij}\sim\mathcal{N}(0,\frac{1}{m})\).

**Randomized quantization**: JL embeddings also have practical uses in vector quantization. In this context, when quantizing a vector \(\mathbf{x}\in\mathbb{R}^{d}\), instead of quantizing \(\mathbf{x}\) directly, the encoder quantizes \(\mathbf{S}\mathbf{x}\), and then the decoder obtains an approximation of \(\mathbf{x}\) through \(\mathbf{S}^{\top}\mathbf{Q}(\mathbf{S}\mathbf{x})\approx\mathbf{x}\). This randomized transformation equalizes the coordinate values of \(\mathbf{x}\), allowing for a smaller range of values and higher precision for the quantizer Q under a fixed bit-budget. As a result, it leads to a smaller \(\ell_{2}\) error compared to independently quantizing each coordinate in a naive manner. Works such as Chen et al. [10], Lyubarskii and Vershynin [36], Mayekar and Tyagi [44], Safaryan et al. [54], Saha et al. [56; 57; 58], Studer et al. [60], Suresh et al. [61; 62], Vargaftik et al. [71; 72], Young et al. [83] explore different variations of this concept for different applications.

## 2 Proposed Algorithm

Prior to detailing our algorithm, we first review the characteristics of the _uniformly dithered quantizer_ that we employ within our approach. Uniformly dithered quantization has been utilized in various prior works Alistarh et al. [5], Bernardo et al. [9], Gray and Stockham [22], Mayekar and Tyagi [44], Suresh et al. [61; 62], and is succinctly described in SS(2.1) below.

### Uniformly dithered quantizer

Let us consider quantizing a scalar \(x\) with \(|x|\leq\mathrm{R}\). Given a _bit-budget_ of \(\mathrm{B}\) bits, the scalar quantizer with _dynamic range_\(\mathrm{R}\) is described by first specifying the \(M=2^{\mathrm{B}}\) quantization points as:

\[q_{1}=-\mathrm{R},q_{2}=-\mathrm{R}+\Delta,q_{3}=-\mathrm{R}+2\Delta,\ldots,q_ {M}=-\mathrm{R}+(M-1)\Delta.\]

Here, the resolution is given by \(\Delta=\frac{2\mathrm{R}}{M-1}\), and the quantizer operation is defined as:

\[\mathrm{Q}_{\mathrm{R},\mathrm{B}}(x)=\begin{cases}q_{k+1}&\text{with probability}\ \ r,\\ q_{k}&\text{with probability}\ \ 1-r,\end{cases}\] (1)

where \(k=\arg\max_{j}\{q_{j}\leq x\}\), i.e., \(x\in[q_{k},q_{k+1})\), and \(r=\frac{x-q_{k}}{\Delta}\). Such a quantizer satisfies

\[\mathbb{E}\left[\mathrm{Q}_{\mathrm{R},\mathrm{B}}(x)\right]=x\ \ \ \text{and}\ \ \ \mathbb{E}\left(\mathrm{Q}_{\mathrm{R},\mathrm{B}}(x)-x\right)^{2}\leq\frac{ \Delta^{2}}{4}=\frac{\mathrm{R}^{2}}{\left(2^{\mathrm{B}}-1\right)^{2}},\] (2)

i.e., it is unbiased and the quantization error variance is dictated by \(\mathrm{R}\) and \(\mathrm{B}\). Here, the \(\mathbb{E}(\cdot)\) is over the randomness from dithering in (1) (ref. App. C). If the input \(x\) to the quantizer falls outside this range, i.e., \(x>\mathrm{R}\) or \(x<-\mathrm{R}\), the quantizer is said to be _saturated_. Finally, to quantize any matrix \(\mathbf{X}\), we obtain \(\mathrm{Q}_{\mathrm{R},\mathrm{B}}(\mathbf{X})\) by quantizing each entry independently, i.e., \(\left[\mathrm{Q}_{\mathrm{R},\mathrm{B}}(\mathbf{X})\right]_{ij}\triangleq \mathrm{Q}_{\mathrm{R},\mathrm{B}}(X_{ij})\).

### Proposed algorithm: LPLR

In order to comprehend low precision and low rank representations, we first introduce a simple strategy, which we name as _direct-SVD quant_. This method involves two main steps: It first computes the optimal rank-\(k\) decomposition \(\mathbf{A}_{k}=\mathbf{U}_{k}\mathbf{\Sigma}_{k}\mathbf{V}_{k}^{\top}\), and then, it quantizes the low-rank factors independently, namely, \(\mathbf{L}=\mathrm{Q}(\mathbf{U}_{k}\mathbf{\Sigma}_{k})\) and \(\mathbf{R}=\mathrm{Q}^{\prime}(\mathbf{V}_{k}^{\top})\). Here, Q and \(\mathrm{Q}^{\prime}\) are uniform scalar quantizers with respective bit-budgets \(\mathrm{B}\) and \(\mathrm{B}^{\prime}\). A detailed analysis of this direct-SVD quantization approach can be found in Appendix H, along with pseudocode provided in Algorithm 2. Despite its simplicity, this is not the optimal strategy due to two reasons. Firstly, it necessitates computing the SVD of \(\mathbf{A}\), which is computationally expensive at \(\mathrm{O}(nd^{2})\). Secondly, let us consider improving the approximation to \(\mathbf{A}\) when the first factor is fixed to \(\mathrm{Q}(\mathbf{U}_{k})\) by solving the optimization problem:

\[\mathbf{X}^{*}=\operatorname*{arg\,min}_{\mathbf{X}\in\mathbb{R}^{k\times d}} \left\|\mathrm{Q}(\mathbf{U}_{k})\mathbf{X}-\mathbf{A}\right\|_{\mathrm{F}}^{2 }=\mathrm{Q}(\mathbf{U}_{k})^{\dagger}\mathbf{A}.\] (3)

This computes the projection of the columns of \(\mathbf{A}\) onto the range space of \(\mathrm{Q}(\mathbf{U}_{k})\). Suppose the resulting projection coefficients are further quantized to get \(\mathrm{Q}^{\prime}(\mathrm{Q}(\mathbf{U}_{k})^{\dagger}\mathbf{A})\). Since \(\mathbf{X}^{*}\) is the solution of (3), for a sufficiently large value of \(\mathrm{B}^{\prime}\), it is evident that \(\left\|\mathrm{Q}(\mathbf{U}_{k})\mathrm{Q}^{\prime}(\mathrm{Q}(\mathbf{U}_{k} )^{\dagger}\mathbf{A})-\mathbf{A}\right\|_{\mathrm{F}}^{2}\leq\left\|\mathrm{Q }(\mathbf{U}_{k}\mathbf{\Sigma}_{k})\mathrm{Q}^{\prime}(\mathbf{V}_{k}^{\top} )-\mathbf{A}\right\|_{\mathrm{F}}^{2}\), and hence, better than direct-SVD quant. However, this approach of projecting onto the range space of \(\mathrm{Q}(\mathbf{U}_{k})\) (which we refer to as LPLR-SVD and analyze in App. I), still requires the computation of \(\mathbf{U}_{k}\), so we can replace \(\mathbf{U}_{k}\) by \(\mathbf{AS}\), i.e., an approximation of the range space obtained through random linear combinations of the columns of \(\mathbf{A}\) (also known as a randomized rangefinder [42]). This leads us to our **LPLR** algorithm, described in Alg. 1, which finds \(\mathbf{W}^{*}=\arg\min_{\mathbf{W}\in\mathbb{R}^{n\times d}}\left\|\mathrm{Q}( \mathbf{A}\mathbf{S})\mathbf{W}-\mathbf{A}\right\|_{\mathrm{F}}^{2}\) and forms the low-rank low-precision approximation \(\mathrm{Q}(\mathbf{A}\mathbf{S})\mathrm{Q}^{\prime}(\mathbf{W}^{*})\) where \(\mathrm{Q},\mathrm{Q}^{\prime}\) are quantization operators. While the solution of this problem is available in closed form as \(\mathbf{W}^{*}=\mathrm{Q}(\mathbf{A}\mathbf{S})^{\dagger}\mathbf{A}\), one can also use an approximation of \(\mathbf{W}^{*}\) by solving this least-squares minimization using an iterative method such as conjugate gradient descent.

In addition to the above argument supporting the superiority of LPLR compared to other baselines (ref. to Tabs. 1 and 2), there exists another essential reason why LPLR outperforms them. This reason directly relates to the selection of \(\mathbf{S}\) as a Gaussian matrix, which is an integral component of LPLR. Random Gaussian matrices are JL embeddings and possess an equalization property that enhances the precision of uniform quantizers. In particular, let us consider an arbitrary vector \(\mathbf{x}\in\mathbb{R}^{d}\) and obtain an estimate as \(\widehat{\mathbf{x}}=\mathbf{S}^{\top}\mathrm{Q}(\mathbf{S}\mathbf{x})\) using a uniform quantizer \(\mathrm{Q}\). It can be shown that the vector quantization error remains constant and does not grow with the dimension \(d\), expressed as \(\mathbb{E}\left\|\widehat{\mathbf{x}}-\mathbf{x}\right\|_{2}^{2}=\mathrm{O}(1)\). This represents a substantial improvement compared to the naive strategy of independently quantizing each coordinate of \(\mathbf{x}\), which leads to a quantization error growth rate of \(\mathrm{O}(d)\). We provide a detailed explanation of this phenomenon in App. D. Furthermore, even when the quantization is \(1\)-bit per coordinate, e.g., \(\mathrm{Q}(\mathbf{S}\mathbf{x})=\mathrm{Sign}(\mathbf{S}\mathbf{x})\), this embedding provides strong near-isometric embedding properties due to the properties of random hyperplane tessellations [50].

While the strong equalization property of Gaussian embeddings is a known result, certain works such as Saha et al. [57; 58], Suresh et al. [61; 62] opt for using randomized Hadamard embeddings instead of Gaussian ones. The reason behind this choice is twofold: (i) Gaussian matrices are dense, requiring \(\mathrm{O}(d^{2})\) multiplications when computing \(\mathbf{S}\mathbf{x}\), and (ii) the entries of \(\mathbf{S}\) are floating point numbers that must be stored in full precision, contradicting the objective of quantizing \(\mathbf{x}\) using fewer bits. However, these concerns are not problematic for **LPLR** because the effects of \(\mathbf{S}\) in the first low-rank factor are neutralized by the second low-rank factor, and \(\mathbf{S}\) does not need to be stored. In fact, we exploit both the equalization property and the subspace approximation property of Gaussian matrices to derive a superior upper bound for the approximation error, as discussed next in SS3.

## 3 Approximation Error Analysis

We are now in a position to state the approximation error guarantee of **LPLR** Let us denote the \(i^{\mathrm{th}}\) row of our input matrix \(\mathbf{A}\) as \(\mathbf{a}^{(i)}\). For convenience of analysis, we make the following assumption.

**Assumption 3.1**.: Rows of matrix \(\mathbf{A}\) have bounded norm, i.e., \(\left\|\mathbf{a}^{(i)}\right\|\leq\mathrm{R}\) for some known \(\mathrm{R}>0\).

The following result gives an informal upper bound on the expected Frobenius norm error of the factorization returned by Alg. 1.

**Theorem 3.2**.: **LPLR approximation error (Informal)** _Suppose our input matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\) with \(\left\|\mathbf{a}^{(i)}\right\|\leq\mathrm{R}=\mathrm{O}(1)\) has singular values \(\sigma_{1},\ldots,\sigma_{r}\) with \(r=\mathrm{rank}(\mathbf{A})\) and target rank as \(k\). Let \(\kappa(\mathbf{A})=\sigma_{1}/\sigma_{r}\) and \(\kappa(\mathbf{A}_{k})=\sigma_{1}/\sigma_{k}\) respectively be the condition numbers of \(\mathbf{A}\) and the best rank-\(k\) approximation of \(\mathbf{A}\), and let us denote \(\kappa=\min\left\{\kappa(\mathbf{A}),\kappa(\mathbf{A}_{k})\left(1-c_{4}\sigma _{k+1}/\sigma_{k}\right)^{-1}\right\}\). Furthermore, for a sufficiently small constant \(\epsilon>0\), suppose the dynamic range of \(\mathrm{Q}\) is set to be \(c_{1}\sqrt{\log\left(n/\epsilon\right)/m}\)_and that of \(\mathrm{Q}^{\prime}\) is set to \(2\kappa\sqrt{m/d}\). Then, the_ **LPLR** _factorization returned by Alg. 1 satisfies_

\[\mathbb{E}\left\|\mathbf{L}\mathbf{R}-\mathbf{A}\right\|_{\mathrm{F}}^{2}\leq \left(1+\frac{k}{m-k-1}\right)\left\|\mathbf{A}_{k}-\mathbf{A}\right\|_{ \mathrm{F}}^{2}+\epsilon,\]

_while utilizing a total budget of \(\log_{2}\left(\frac{\mathrm{c}\kappa(\mathbf{A}_{k})\kappa}{\epsilon}\frac{ nm}{\sqrt{d}}\sqrt{\log\left(\frac{mn^{2}}{\epsilon}\right)}\right)\) bits for \(n\approx d\). Here, \(c_{1}\), \(c_{2}\), \(c_{3}\), and \(c_{4}\) are constants that depend on \(\mathrm{R}\)._

We provide a less formal version of our main result here, suitable for interpretation. The formal statement of this result, including specific constant values, can be found in Thm. G.2 of App. G. It does not necessitate the assumption \(n\approx d\) and provides distinct thresholds for \(\mathrm{B}\) and \(\mathrm{B}^{\prime}\). Thm. G.2 asserts that, for a target rank-\(k\), as long as \(m\geq k+2\), one can ensure an arbitrarily small approximation error of \(\epsilon\) by selecting the number of bits to be at least above a certain threshold budget. The threshold depends on the error tolerance \(\epsilon\), dimensions \(n\) and \(d\), the sketch size \(m\), and the spectrum of \(\mathbf{A}\). The value of \(\kappa\) is determined by taking the smaller of two quantities. In the case of matrices with a sharp decline in singular values (e.g., matrices of exact rank-\(k\)), where the ratio \(\sigma_{k}/\sigma_{k+1}\) approaches zero, \(\kappa\approx\kappa(\mathbf{A}_{k})\). For matrices with a smoother spectrum (e.g., all singular values are equal), \(\kappa=\kappa(\mathbf{A})\), the condition number of the input matrix \(\mathbf{A}\).

**Remark 1**.: We consider two distinct scenarios based on Asm. 3.1. The first case assumes that the row norms are bounded by a constant, represented by \(\mathrm{R}=\mathrm{O}(1)\). This assumption is reasonable when the rows of \(\mathbf{A}\) correspond to different normalized features of a data point. The second case assumes that the individual entries of \(\mathbf{A}\) are bounded by a constant, i.e., \(A_{ij}=\mathrm{O}(1)\). This implies that \(\mathrm{R}=\mathrm{O}(\sqrt{d})\), which is a reasonable assumption for scenarios like images, where it is known that each pixel value is bounded. In Tab. 1, we compare the performance of the algorithms when \(\mathrm{R}=\mathrm{O}(1)\), while in Tab. 2, we assume \(\mathrm{R}=\mathrm{O}(\sqrt{d})\). Thm. 3.2 assumes that \(\mathrm{R}=\mathrm{O}(1)\). The expressions in Tab. 2 can be obtained in a similar manner from the formal version in Thm G.2.

### Analysis outline

The derivation of the upper bound on the approximation error of LPLR is presented in App. G. In this section we outline a brief proof sketch that highlights the main challenges of the proof. As mentioned already in SS2.2, the analysis of LPLR utilizes the subspace embedding and equalization properties of random Gaussian matrices. A key component in Alg. 1 is the choice of dynamic range for quantizers \(\mathrm{Q}\) and \(\mathrm{Q}^{\prime}\). In our analysis, we assume that when either \(\mathrm{Q}\) or \(\mathrm{Q}^{\prime}\) gets saturated in lines \(3\) and \(5\) of Alg. 1, a trivial factorization of \(\mathbf{L}\mathbf{R}=\mathbf{0}\) is returned. We choose the dynamic ranges \(\mathrm{R}_{\mathrm{Q}}\) and \(\mathrm{R}_{\mathrm{Q}^{\prime}}\) to be sufficiently high enough so that this happens with a very low probability. Formally, for quantizer \(\mathrm{Q}\), Lemma E.1 states the following:

\[\left\|\mathbf{A}\mathbf{S}\right\|_{\mathrm{max}}\leq\mathrm{R}\sqrt{\frac{2 \log\left(\frac{16\mathrm{R}^{2}n^{2}m}{\epsilon}\right)}{m}}\quad\text{ with probability exceeding }1-\frac{\epsilon}{8n\mathrm{R}^{2}}.\]

Here, \(\left\|\mathbf{A}\mathbf{S}\right\|_{\mathrm{max}}\) is max-norm of the matrix \(\mathbf{A}\mathbf{S}\), i.e., the coordinate with maximum magnitude. This concentration result is a consequence of the equalization property of Gaussian matrix \(\mathbf{S}\).

On the other hand, the input to the second quantizer is \(\mathrm{Q}(\mathbf{A}\mathbf{S})^{\dagger}\mathbf{A}\) and Lemma E.2 provides a concentration result for the max-norm of this matrix. Although in a general worst-case scenario, the coordinate values of the pseudo-inverse of a matrix with small entries can be large (Alon and Vu [6]), because we compute the pseudo-inverse of the matrix \(\mathbf{A}\mathbf{S}\), which is rectangular (\(m\ll n\)) and with random Gaussian entries, \(\|(\mathbf{A}\mathbf{S})^{\dagger}\|_{2}\) does not shoot up arbitrarily as shown by Rudelson and Vershynin [53]. We then show that \(\|\mathrm{Q}(\mathbf{A}\mathbf{S})^{\dagger}\|_{2}\) is not too far from \(\|(\mathbf{A}\mathbf{S})^{\dagger}\|_{2}\), allowing us to derive an expression for \(\mathrm{R}_{\mathrm{Q}^{\prime}}\). We get for \(\gamma=\frac{d}{m}\) and \(t=\sqrt{\frac{2\log\left(\frac{32n\mathrm{R}^{2}}{\epsilon}\right)}{m}}\),

\[\left\|\mathrm{Q}(\mathbf{A}\mathbf{S})^{\dagger}\mathbf{A}\right\|_{\mathrm{ max}}\leq\frac{2\kappa}{\sqrt{\gamma}-1-t}\text{ with probability exceeding }1-\frac{\epsilon}{4n\mathrm{R}^{2}}.\]

The second part of the analysis deals with upper bounding the approximation error when the second low-rank factor is unquantized, i.e., \(\|\mathrm{Q}(\mathbf{A}\mathbf{S})\mathrm{Q}(\mathbf{A}\mathbf{S})^{\dagger} \mathbf{A}-\mathbf{A}\|_{\mathrm{F}}^{2}\) conditioned on the event that \(\mathrm{Q}\) and \(\mathrm{Q}^{\prime}\) are unsaturated. We reduce this problem to analyzing the solution of the following:

\[\widetilde{\mathbf{X}}=\operatorname*{arg\,min}_{\mathbf{X}}\left\|\mathbf{X} ^{\top}\mathbf{A}_{k}\mathbf{S}-\mathrm{Q}(\mathbf{A}\mathbf{S})\right\|_{ \mathrm{F}}^{2}.\] (4)We refer to (4) as the _sketched least squares problem with quantized response_ as it is a variant of the generalized least squares problem, \(\mathbf{X}^{*}=\operatorname*{arg\,\min}_{\mathbf{X}}\left\|\mathbf{X}^{\top} \mathbf{A}_{k}-\mathbf{A}\right\|_{\mathrm{F}}^{2}\). This is potentially a problem of independent interest, and we analyze the solution of (4) in detail in App. F. Exploiting the subspace embedding property of \(\mathbf{S}\) we show that

\[\left\|\mathbf{X}^{*\top}\mathbf{A}_{k}-\mathbf{A}\right\|_{\mathrm{F}}^{2} \leq\mathbb{E}\left\|\widetilde{\mathbf{X}}^{\top}\mathbf{A}_{k}-\mathbf{A} \right\|_{\mathrm{F}}^{2}\leq\frac{m-1}{m-k-1}\left\|\mathbf{X}^{*\top} \mathbf{A}_{k}-\mathbf{A}\right\|_{\mathrm{F}}^{2}+\text{ quantization error term}.\]

This leads us to the proof of Lemma G.1 which gives the approximation error of LPLR when \(\mathrm{Q}\) and \(\mathrm{Q}^{\prime}\) are unsaturated. Finally, taking into account the low-probability saturation events, for which the error is \(\left\|\mathbf{A}\right\|_{\mathrm{F}}^{2}\), we derive our main result in Thm. G.2. Subsequently, we discuss the approximation made in App. G.2 and arrive at the informal result of Thm. 3.2.

### Comparison with baselines

We are now in a position to compare the performance with baselines in Tabs. 1 and 2.

**Naive quantization**: The most straightforward baseline for matrix quantization is naive quantization where each coordinate of the matrix is quantized independently, agnostic to any low-rank structure in the matrix \(\mathbf{A}\). In this, we allocate \(\mathrm{B}\) bits to each coordinate of \(\mathbf{A}\) and since there are \(nd\) entries in the matrix, from (1), the Frobenius norm error is upper bounded by \(\frac{\mathrm{R}^{2}nd}{(2^{B}-1)^{2}}\). Note that this holds true irrespective of whether \(\mathrm{R}=\mathrm{O}(1)\) or \(\mathrm{O}(\sqrt{d})\) because \(\left\|\mathbf{a}^{(i)}\right\|\leq\mathrm{R}\) implies \(A_{ij}\leq\mathrm{R}\). To ensure that the error is within a certain tolerance \(\epsilon\), we then require \(\frac{1}{2}\log_{2}\left(\frac{nd}{\epsilon}\right)\) bits. In this, and also other expressions for bit-budget requirements of algorithms in Tabs. 1 and 2, we have ignored the multiplicative constant factors inside the \(\log_{2}(\cdot)\) for simplicity of exposition. The exact expressions can be found in the corresponding appendices where we derive them.

One of the primary reasons why both direct-SVD and LPLR are expected to perform better than naive is that the former strategies exploit the low-rank structure of the matrices to reduce the total number of parameters being quantized, i.e., \(k(n+d)\) for direct-SVD and \(m(n+d)\) for LPLR, vs. \(nd\) for naive. Given a total bit-budget for the entire matrix \(\mathbf{A}\), since we now quantize fewer parameters than before, we can allocate a higher number of bits to each parameter, enabling higher precision. The price we pay for exploiting the low-rank structure of matrices is the additional \(\left\|\mathbf{A}_{k}-\mathbf{A}\right\|_{\mathrm{F}}^{2}\) dependent term, which is usually very small for matrices that can be well approximated by a low-rank structure. For matrices that are exactly rank \(k\), this term is \(0\). As we see in our numerical simulations in SS4, several real-world matrices can be well-approximated by a low-rank structure.

**Direct-SVD quant.**: From Tab. 1, we see that to achieve an \(\epsilon\)-quantization error, direct-SVD requires \(\frac{1}{2}\log_{2}(k\sqrt{nd})\) bits per entry, which is greater than \(\frac{1}{2}\log_{2}\left(\frac{nm}{\sqrt{d}}\right)\) required by LPLR (ignoring the

\begin{table}
\begin{tabular}{c c c c} Algorithms & Approximation error & Bit-budget (per entry) & Computation \\ \hline Naive uniform & \(\epsilon\) & \(\frac{1}{2}\log_{2}\left(\frac{nd}{\epsilon}\right)\) & \(\mathrm{O}(nd)\) \\ Direct-SVD & \(\left\|\mathbf{A}_{k}-\mathbf{A}\right\|_{\mathrm{F}}^{2}+\epsilon\) & \(\frac{1}{2}\log_{2}\left(\frac{k\sigma\lambda}{\epsilon}\sqrt{nd}\right)\) & \(\mathrm{O}(nd^{2})\) \\
**LPLR** (_ours_) & \((1+\delta)\left\|\mathbf{A}_{k}-\mathbf{A}\right\|_{\mathrm{F}}^{2}+\epsilon\) & \(\frac{1}{2}\log_{2}\left(\frac{\kappa(\mathbf{A}_{k})\kappa}{\epsilon}\frac{ nm}{\sqrt{d}}\sqrt{\log\left(\frac{mn^{2}d^{2}}{\epsilon}\right)}\right)\) & \(\mathrm{O}(ndm)\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison with baselines (row-norm bound is constant, i.e., \(\left\|\mathbf{a}^{(i)}\right\|=\mathrm{O}(1)\)). \(k,m\ll\min\{d,n\}\). \(n\): no. of rows, \(d\): no. of columns, \(m\): sketch size, \(\epsilon\): error tolerance, \(\delta=k/(m-k-1)\). The expressions for bit-budget (per entry) ignores constant multiplicative factors inside the \(\log_{2}(\cdot)\). We assume \(n\geq d\).

\begin{table}
\begin{tabular}{c c c c} Algorithms & Approximation error & Bit-budget (per entry) & Computation \\ \hline Naive uniform & \(\epsilon\) & \(\frac{1}{2}\log_{2}\left(\frac{nd}{\epsilon}\right)\) & \(\mathrm{O}(nd)\) \\ Direct-SVD & \(\left\|\mathbf{A}_{k}-\mathbf{A}\right\|_{\mathrm{F}}^{2}+\epsilon\) & \(\frac{1}{2}\log_{2}\left(\frac{k\sigma\lambda}{\epsilon}\sqrt{nd}\right)\) & \(\mathrm{O}(nd^{2})\) \\
**LPLR** (_ours_) & \((1+\delta)\left\|\mathbf{A}_{k}-\mathbf{A}\right\|_{\mathrm{F}}^{2}+\epsilon\) & \(\frac{1}{2}\log_{2}\left(\frac{\kappa(\mathbf{A}_{k})\kappa}{\epsilon}nm\sqrt{d \log\left(\frac{mn^{2}d^{2}}{\epsilon}\right)}\right)\) & \(\mathrm{O}(ndm)\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparison with baselines (individual entries of \(\mathbf{A}\) are bounded by a constant, i.e., \(A_{ij}=\mathrm{O}(1)\)). Dimension dependent terms are color highlighted for ease of comparison with Tab. 1.

logarithmic terms). Evidently, LPLR demands fewer bits than direct-SVD because \(k,m\ll\min\{n,d\}\) for inherently low-rank matrices. For the regime presented in Tab. 2, the bit requirement for direct-SVD remains unchanged. However, LPLR now requires \(\frac{1}{2}\log_{2}\left(nm\sqrt{d}\right)\), slightly more than direct-SVD, due to the additional \(\sqrt{n}\) factor. Thus, it makes sense to expect that direct-SVD can perform better in this regime. This is supported by our numerical experiments in Tabs. 4 to 7, where direct-SVD indeed outperforms LPLR in certain scenarios. Nevertheless, it is crucial to emphasize that direct-SVD necessitates computing the SVD, which can be prohibitive for very large matrices due to the current memory limitations of available GPUs, making LPLR the only viable option.

**Computational complexity**: Unsurprisingly, naive quant. requires the least computation, i.e., \(\mathrm{O}(nd)\), as it just does a single pass over all the elements of \(\mathbf{A}\). The \(\mathrm{O}(nd^{2})\) complexity of direct-SVD quant. stems from the requirement of computing SVD (assuming \(n\geq d\)). LPLR is the best of both worlds - for the same bit-budget, LPLR has a smaller approximation error than both direct-SVD and naive, and a complexity of \(\mathrm{O}(ndm)\), arising from the requirement to compute \(\mathbf{AS}\), i.e., a product of two dense matrices of dimensions \(n\times d\) and \(d\times m\), which is better than direct-SVD, since \(m\ll d\).

## 4 Numerical Simulations

### Overview

We evaluate the robustness of LPLR on multiple tasks, namely, image compression, binary, and multi-class classification across disparate domains including vision, text and raw images, and neural network weight matrices. We consider a range of input configurations to showcase the performance and non linear effects of joint quantization and low rank approximation on a given dataset, especially at lower bit budgets. LPLR provides competitive results at bit budgets as low as a single bit, providing extreme model compression while maintaining non trivial performance for the task at hand.

**Baselines.** We employ naive quantization, which quantizes the input matrix by rounding to the nearest scalar in the underlying data type's quantization grid, as our primary benchmark. Naive quant. and its variants - Dettmers et al. [14], Yao et al. [81], are the most popular method in use across domains, as their memory and computational run time requirements scale extremely well with model and dataset sizes. In addition, we also evaluate the performances of direct-SVD quantization and LPLR-SVD to disambiguate between the entangled effects of quantization and exploiting low rank structure.

**Metrics.** We evaluate LPLR performance using task specific goodness of fit metrics, as well as relative Frobenius norm error between the original and matrix reconstructed from its low-rank factors, i.e., \(\left\lVert\mathbf{LR}-\mathbf{A}\right\rVert_{\mathrm{F}}^{2}\). In addition to this, we enforce parity between the number of bits used by all quantization schemes, so that the total space required (in bits) for storing the approximated matrix is _identical_ across LPLR, LPLR-SVD, direct-SVD quant., and naive quant.

**Notation.** In all our experiments, we denote the bit budget for the left low rank factor \(\mathbf{L}\) as \(\mathrm{B}\), for the right low rank factor \(\mathbf{R}\) as \(\mathrm{B}^{\prime}\), and the corresponding bit budget for naive quantization as \(\mathrm{B}_{\mathrm{nq}}\). For simplicity, we maintain equal bit budgets \(\mathrm{B}=\mathrm{B}^{\prime}\) for both quantizers \(\mathrm{Q}\) and \(\mathrm{Q}^{\prime}\). Wherever necessary, we also abbreviate direct-SVD quant. as DSVD, LPLR-SVD as LSVD, and naive quant. as NQ.

The main algorithm is implemented in Pytorch (Paszke et al. [46]), and utilizes Hugging Face [80] implementations of all datasets and large language models. All experiments were performed on a single GPU NVIDIA TITAN RTX. Further simulations and experimental details can be found in App. J. Our code is available at https://github.com/pilancilab/matrix-compressor.

### Image Compression

Image compression is a prototypical application of low rank matrix compression, as images are known to be significantly rank deficient in many practical scenarios (Zhang et al. [86], Zhou et al. [87]). In this task, we apply LPLR on \(1000\times 1000\) dimensional Shepp Logan phantom images from Gach et al. [20]. These are a set of synthetic 2D images designed to simulate the typical characteristics and structures found in computed tomography (CT) scans. They consist of geometric shapes, including circles and ellipses, representing different tissues or organs within the scanned object.

The main results are summarized in Tab. 3. To ensure a fair comparison, we adjust the sketch size/target rank so that bit budgets are identical between naive quant. and LPLR. This allowsus to preserve the original datatype of the image (consequently a large dynamic range), while substantially reducing the pixels used for representing the image to as low as \(1\) bit per pixel (on average). Specifically, in Figure 1, we can observe the least visual distortion in the case of LPLR, which preserves _critical_ semantic features of the images, such as the small ellipses. It is clear that LPLR outperforms both techniques at lower naive quantization bit budgets. We attribute the better visual and quantitative performance to the higher dynamic range available to LPLR as well as structure preserved in the low rank decomposition.

### Embeddings extracted from pre-trained models

The efficacy of pre-trained embeddings is well established in vision (Li et al. [33], Parisi et al. [45]), text (Qi et al. [51], Rezaeinia et al. [52]) for rapid feature computation as an input to a variety of downstream tasks. Embeddings also play a crucial role in a number of software applications, including but not limited to, open source vector search libraries (Liu [35], Marqo [41]), semantic search engines (Amazon AWS [7]), vector databases (Pinecone [49]). Since most applications rely on proximity in "embedded space", it is essential that common operations on embeddings be computationally efficient. Specifically, one would like to optimize nearest neighbor (NN) searches which solve the optimization problem \(\operatorname*{arg\,min}_{i}\left\|\mathbf{x}_{i}-\mathbf{y}\right\|_{2}^{2}\) (reducible to \(\operatorname*{arg\,max}_{i}\mathbf{X}\mathbf{y}\) where \(\mathbf{X}\) is the matrix with training vectors \(\{\mathbf{x}_{i}\}\) as its rows, and \(\mathbf{y}\) is the query vector). The time complexity of NN search scales linearly with dimensions of \(\mathbf{X}\in\mathrm{R}^{n\times d}\) and number of neighbors (\(k\)). By embedding the data matrix in a dimension \(m\ll d\), we directly speedup the run-time and reduce storage costs. Moreover, as datasets grow exponentially in size (especially document databases) and transfer learning becomes the dominant modality of training new models, embedding compression becomes a necessity for storing data without a corresponding exponential increase in hardware requirements.

#### 4.3.1 Embedding Classification

In this experiment, we evaluate several embeddings of standard datasets, namely CIFAR-10, CIFAR-100, IMDB and Emotion datasets. CIFAR-10 consists of 60,000 color images divided into 10 classes, with each class containing 6,000 images. The dataset is split into 50,000 training images and 10,000 test images, with a resolution of 32x32 pixels. CIFAR-100 increases the number of classes to 100 categories for an identical training and test size as CIFAR-10. The IMDB (mte [2]) dataset consists of 25,000 train and test sentences containing annotated binary sentiment labels for movie reviews, plot summaries and other rating information. The Emotion (mte [1]) dataset is a sentiment analysis dataset, containing 16,000 train and 2000 test sentences, each exemplifying a singular emotion, which represents the sentiment label for that sentence.

For CIFAR-10 and CIFAR-100, we embed the entire dataset using MobileNet v3 (Howard et al. [24]) pretrained on ImageNet (Deng et al. [12]) producing an embedding matrix of dimension \(60000\times 1024\), which we compress using LPLR and compare with the baselines in SS4.1. To evaluate the goodness of embeddings, we build a \(3\)-NN, a KNN Classifier using \(K=3\) nearest neighbors under Euclidean distance). We report the performance of the model using standard classification metrics - classification accuracy and weight averaged F1 score. We utilize a uniform bit budget \(\mathrm{B}=\mathrm{B}^{\prime}=8\) bits for the quantizers \(\mathrm{Q},\mathrm{Q}^{\prime}\) across all cases. Tabs. 4 and 5 present our results under this setup. For each case we benchmark absolute performance using a \(3\)-NN classifier on the training set.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \multirow{2}{*}{B} & \multicolumn{2}{c}{**Target Rank \((k)\)/**} & \multirow{2}{*}{B\({}_{\mathrm{tr}}\)} & \multirow{2}{*}{**LPLR**} & \multirow{2}{*}{**DSVD**} & \multirow{2}{*}{**LSVD**} & \multirow{2}{*}{**NQ**} \\  & \multicolumn{1}{c}{**Sketch Size \((m)\)**} & & & & & \\ \hline
32 & 15 & 1 & 0.610 & 0.553 & **0.506** & 0.532 \\ \hline
28 & 17 & 1 & 0.557 & 0.546 & **0.490** & 0.532 \\ \hline
24 & 20 & 1 & 0.540 & 0.537 & **0.454** & 0.532 \\ \hline
20 & 25 & 1 & 0.485 & 0.529 & **0.426** & 0.532 \\ \hline
16 & 31 & 1 & 0.447 & 0.523 & **0.391** & 0.532 \\ \hline
12 & 41 & 1 & 0.402 & 0.518 & **0.360** & 0.532 \\ \hline
8 & 62 & 1 & 0.340 & 0.508 & **0.326** & 0.532 \\ \hline \end{tabular}
\end{table}
Table 3: Comparison of **LPLR** and **LPLR-SVD** (**LSVD**) Frobenius norm errors with baselines, for different input LPLR bit budgets. Each triplet \(\mathrm{(B,B^{\prime},B_{\mathrm{tr}})}\) of configurations has an **identical compression ratio**. Here, \(\mathrm{B}=\mathrm{B}^{\prime}\). The second column specifies the sketch size \(m\) for LPLR, and target rank \(k\) for DSVD or LSVD. We provide results for input bit budgets at a finer granularity to identify regimes where naïve quant. is outperformed.

Similarly, we embed text sentences from IMDB and Emotion databases using BeRT (Devlin et al. [15]) into \(512\) dimensional vectors, and construct a \(3\)-NN classifier using Euclidean distance to perform binary and multi-class classification on the respective embeddings, and report classification metrics in Tabs. 6 and 7. We see that LPLR outperforms direct-SVD quant. and naive quant. at lower bit budgets, and has performance parity as we increase \(\mathrm{B}_{\mathrm{n}_{0}}\) to \(4\) bits. We find that we match (and even exceed) the unquantized benchmark at single bit precision, which we attribute to the dominating low rank factorization, and its regularizing effect on data under extreme rank constraints. It is important to note that performance parity with direct-SVD quant. is also a successful outcome, since LPLR provides runtime improvements over taking an SVD to compress the data.

#### 4.3.2 Compressing Weight Matrices of a Large Language Model

In this section, we present results on a major application of matrix compression - compressing the weight matrices of deep neural networks. We choose LlaMa by Touvron et al. [66], a popular foundation Large Language Model (LLM) as the network of our choice. LLMs are a natural candidate for matrix compression, due to their massive stacked transformer layers, rendering them difficult to deploy on several GPUs, let alone a single GPU. Many methods have emerged to quantize and compress these models in order to make them amenable to single GPU deployment and inference, including naive quantization with outlier exclusion (Dettmers et al. [14]), second order methods (Frantar et al. [19]), low rank parameter reduction (Hu et al. [25]), amongst others.

We apply LPLR to the \(2\)-dimensional weight tensors, i.e., matrices in LlaMa, leaving any other tensor, which does not lend itself to a low rank decomposition, unquantized. Figs. 1(a) and 1(b) (better

\begin{table}
\begin{tabular}{c c c c c c c c c c c c} \hline \hline \multicolumn{8}{c}{Frobenius Norm Error} & Accuracy (\%) & \multicolumn{6}{c}{Weighted F1 Score (\%)} \\ \hline \(\mathrm{B}_{\mathrm{n}q}\) & **LPLR** & **LSVD** & DSVD & NQ & **LPLR** & **LSVD** & DSVD & NQ & **LPLR** & **LSVD** & DSVD & NQ \\
1 & 0.313 & **0.241** & 0.229 & 6.63 & 73 & 74 & **75** & 50 & 74 & 74 & **75** & 33 \\
2 & 0.235 & 0.178 & **0.161** & 1.016 & **74** & **74** & **74** & 50 & **74** & **74** & 50 \\
4 & 0.148 & 0.122 & **0.098** & 0.417 & **75** & 74 & **75** & 73 & 74 & 74 & **75** & 73 \\ \hline \hline \end{tabular}
\end{table}
Table 6: **IMDB embeddings generated by BERT with an unquantized accuracy and F1 score \(75\%\) and \(74\%\) respectively**: Results on LPLR and LPLR-SVD with \(\mathrm{B}=\mathrm{B}^{\prime}=8\) bits

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c c} \hline \hline \multicolumn{8}{c}{Frobenius Norm Error} & Accuracy (\%) & \multicolumn{6}{c}{Weighted F1 Score (\%)} \\ \hline \(\mathrm{B}_{\mathrm{n}q}\) & **LPLR** & **LSVD** & DSVD & NQ & **LPLR** & **LSVD** & DSVD & NQ & **LPLR** & **LSVD** & DSVD & NQ \\
1 & **1.05** & 1.08 & 1.09 & 7.17 & **92** & **92** & **92** & 11 & **92** & **92** & **92** & 4 \\
2 & **1.08** & 1.1 & 1.1 & 2.29 & **92** & **92** & 91 & 30 & **92** & **92** & 91 & 23 \\
4 & **1.1** & 1.11 & 1.11 & 1.15 & 91 & **92** & 91 & 91 & 91 & **92** & 91 & 91 \\ \hline \hline \end{tabular}
\end{table}
Table 4: **CIFAR10 embeddings generated by MobileNetV3 with an unquantized accuracy and F1 score \(91\%\):Results on LPLR and LPLR-SVD with \(\mathrm{B}=\mathrm{B}^{\prime}=8\) bits 1**

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c c} \hline \hline \multicolumn{8}{c}{Frobenius Norm Error} & Accuracy (\%) & \multicolumn{6}{c}{Weighted F1 Score (\%)} \\ \hline \(\mathrm{B}_{\mathrm{n}q}\) & **LPLR** & **LSVD** & DSVD & NQ & **LPLR** & **LSVD** & DSVD & NQ & **LPLR** & **LSVD** & DSVD & NQ \\
1 & **1.05** & 1.08 & 1.09 & 7.17 & **92** & **92** & **92** & 11 & **92** & **92** & **92** & 4 \\
2 & **1.08** & 1.1 & 1.1 & 2.29 & **92** & **92** & 91 & 30 & **92** & **92** & 91 & 23 \\
4 & **1.1** & 1.11 & 1.11 & 1.15 & 91 & **92** & 91 & 91 & 91 & **92** & 91 & 91 \\ \hline \hline \end{tabular}
\end{table}
Table 5: **CIFAR100 embeddings generated by MobileNetV3 with an unquantized accuracy and F1 score \(76\%\):Results on LPLR and LPLR-SVD with \(\mathrm{B}=\mathrm{B}^{\prime}=8\) bits**resolution in App. J) showcase our results on applying LPLR and LPLR-SVD with bit budgets of \(8\) bits and \(4\) bits respectively, using relative Frobenius norm error as the metric. While it is clear that LPLR and LPLR-SVD perform significantly better across all layers (on average), there are outliers where naive quant. is the better choice. We can a observe periodic structure in the error profile of naive quant., implying that the low rank structure is a function of the index of attention layer in transformer blocks. It is important to note that a low Frobenius norm error is not a direct indicator of performance for other task specific metrics. It is possible to construct a holistic compression strategy using error profiles similar to Figs. 3 and 4 to adopt a per-layer quantization strategy, minimizing both task specific metrics as well as relative Frobenius norm error. We discuss this further in Appendix K.

## 5 Conclusions

In this work, we have considered the problem of obtaining a low-precision and low-rank factorization of a matrix. Such a factorization of a matrix into a product of tall and wide matrices has several advantages, including compression of the original matrix. We propose a fast randomized algorithm to obtain this factorization which requires \(\mathrm{O}(nmd)\) computations - considerably faster than alternative methods. Our algorithm employs a Gaussian sketch to estimate the range space of matrices that are approximately low-rank. By utilizing the properties of subspace approximation and equalization in Gaussian embeddings, we establish an upper bound on the approximation error attained by our algorithm, and show that it can be significantly smaller than its counterparts. Finally, we empirically evaluate our method on several vision and text datasets, where we show significant task performance at highly compressed bit budgets as low as a _single_ bit. This provides a novel pragmatic approach to work with large datasets and models in real world settings, making them more accessible to researchers and deployment on regular consumer hardware.

## Acknowledgments and Disclosure of Funding

This work was supported in part by the Air Force Office of Scientific Research (AFOSR) under Award #002484665; in part by National Science Foundation (NSF) CAREER Award under Grant CCF-2236829, Grant DMS-2134248 and Grant ECCS-2037304; in part by the U.S. Army Research Office Early Career Award under Grant W911NF-21-1-0242; in part by the Stanford Precourt Institute; and in part by the ACCESS AI Chip Center for Emerging Smart Systems through InnoHK, Hong Kong, SAR. The authors would also like to thank the anonymous reviewers whose comments and suggestions helped improve the presentation of this work.

Figure 2: Comparison of LPLR and LPLR SVD on LLaMa weights, ordered by the original sequence of layers

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & \multicolumn{3}{c}{\(\mathrm{B=B^{\prime}=8\text{ bits}},\mathrm{B_{nq}=4\text{ bits}}\)} \\ \hline Metric & **LPLR** & **LPLR-SVD** & Naive Quant. \\ \hline Mean & 0.672 & **0.537** & 0.836 \\ Std Dev & 0.080 & **0.079** & 0.470 \\ \hline \hline \end{tabular} 
\begin{tabular}{l c c c} \hline \hline  & \multicolumn{3}{c}{\(\mathrm{B=B^{\prime}=B_{nq}=4\text{ bits}}\)} \\ \hline Metric & **LPLR** & **LPLR-SVD** & Naive Quant. \\ \hline Mean & 0.548 & **0.540** & 0.836 \\ Std Dev & **0.053** & 0.055 & 0.470 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Average relative Frobenius norm error on LLaMa weight matrices 

## References

* datasets at hugging face. https://huggingface.co/datasets/mteb/emotion,. (Accessed on 05/16/2023). (Cited on page 8)
* datasets at hugging face. https://huggingface.co/datasets/mteb/imdb?doi=true,. (Accessed on 05/16/2023). (Cited on page 8)
* [3] A. Aghajanyan, L. Zettlemoyer, and S. Gupta. Intrinsic dimensionality explains the effectiveness of language model fine-tuning, 2020. URL https://arxiv.org/abs/2012.13255. (Cited on page 8)
* [4] F. Alimisis, P. Davies, and D. Alistarh. Communication-efficient distributed optimization with quantized preconditioners. In M. Meila and T. Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 196-206. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/alimisis21a.html. (Cited on page 8)
* [5] D. Alistarh, D. Grubic, J. Li, R. Tomioka, and M. Vojnovic. QSGD: Communication-efficient SGD via gradient quantization and encoding. In _Advances in Neural Information Processing Systems_, volume 30, pages 1709-1720. Curran Associates, Inc., 2017. (Cited on page 8)
* [6] N. Alon and V. H. Vu. Anti-hadamard matrices, coin weighing, threshold gates, and indecomposable hypergraphs. _J. Comb. Theory Ser. A_, 79(1):133-160, jul 1997. ISSN 0097-3165. doi: 10.1006/jcta.1997.2780. URL https://doi.org/10.1006/jcta.1997.2780. (Cited on page 8)
* Amazon Kendra
- AWS, 2023. URL https://aws.amazon.com/kendra/. (Accessed on 05/16/2023). (Cited on page 8)
* [8] M. Ben Noach and Y. Goldberg. Compressing pre-trained language models by matrix decomposition. In _Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing_, pages 884-889, Suzhou, China, Dec. 2020. Association for Computational Linguistics. URL https://aclanthology.org/2020.aacl-main.88. (Cited on page 8)
* 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 1-5, 2023. doi: 10.1109/ICASSP49357.2023.10094884. (Cited on page 8)
* [10] W.-N. Chen, P. Kairouz, and A. Ozgur. Breaking the communication-privacy-accuracy trilemma. In _Neural Information Processing Systems (NeurIPS) 2020_, 2020. (Cited on page 8)
* [11] Y. Chi, Y. M. Lu, and Y. Chen. Nonconvex optimization meets low-rank matrix factorization: An overview. _IEEE Transactions on Signal Processing_, 67(20):5239-5269, 2019. doi: 10.1109/TSP.2019.2937282. (Cited on page 8)
* [12] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. IEEE, 2009. (Cited on page 8)
* [13] M. Derezinski, F. T. Liang, Z. Liao, and M. W. Mahoney. Precise expressions for random projections: Low-rank approximation and randomized Newton. In _Advances in Neural Information Processing Systems_, volume 33. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/d40d35b3063c11244fbf38e9b55074be-Paper.pdf. (Cited on page 8)
* [14] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. LLM.int8 (): 8-bit matrix multiplication for transformers at scale. _arXiv preprint arXiv:2208.07339_, 2022. (Cited on pages 8 and 8)
* [15] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018. (Cited on page 8)* Drineas and Mahoney [2016] P. Drineas and M. W. Mahoney. RandNLA: Randomized numerical linear algebra. _Commun. ACM_, 59(6):80-90, may 2016. ISSN 0001-0782. doi: 10.1145/2842602. URL https://doi.org/10.1145/2842602. (Cited on page 2)
* Drineas et al. [2006] P. Drineas, R. Kannan, and M. W. Mahoney. Fast monte carlo algorithms for matrices II: Computing a low-rank approximation to a matrix. _SIAM Journal on Computing_, 36(1):158-183, 2006. doi: 10.1137/S0097539704442696. URL https://doi.org/10.1137/S0097539704442696. (Cited on page 2)
* Eckart and Young [1936] C. Eckart and G. Young. The approximation of one matrix by another of lower rank. _Psychometrika_, 1:211-218, 1936. doi: https://doi.org/10.1007/BF02288367. URL https://rdcu.be/dcbK1. (Cited on page 2)
* Frantar et al. [2022] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. _arXiv preprint arXiv:2210.17323_, 2022. (Cited on page 9)
* Gach et al. [2008] H. M. Gach, C. Tanase, and F. Boada. 2D & 3D shepp-logan phantom standards for MRI. In _2008 19th International Conference on Systems Engineering_, pages 521-526. IEEE, 2008. (Cited on page 7)
* Gholami et al. [2021] A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, and K. Keutzer. A survey of quantization methods for efficient neural network inference, 2021. URL https://arxiv.org/abs/2103.13630. (Cited on page 2)
* Gray and Stockham [1993] R. Gray and T. Stockham. Dithered quantizers. _IEEE Transactions on Information Theory_, 39(3):805-812, 1993. doi: 10.1109/18.256489. (Cited on page 3)
* Halko et al. [2011] N. Halko, P. G. Martinsson, and J. A. Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. _SIAM Review_, 53(2):217-288, 2011. doi: 10.1137/090771806. URL https://doi.org/10.1137/090771806. (Cited on pages 2 and 29)
* Howard et al. [2019] A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, W. Wang, Y. Zhu, R. Pang, V. Vasudevan, Q. V. Le, and H. Adam. Searching for mobilenetv3, 2019. URL https://arxiv.org/abs/1905.02244. (Cited on page 8)
* Hu et al. [2022] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. LoRA: Low-rank adaptation of large language models. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=m2eVKeeFYf9. (Cited on pages 1 and 9)
* Idelbayev and Carreira-Perpinan [2020] Y. Idelbayev and M. A. Carreira-Perpinan. Low-rank compression of neural nets: Learning the rank of each layer. In _2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 8046-8056, 2020. doi: 10.1109/CVPR42600.2020.00807. (Cited on page 1)
* Jaderberg et al. [2014] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up convolutional neural networks with low rank expansions, 2014. URL https://arxiv.org/abs/1405.3866. (Cited on page 1)
* Jin et al. [2021] D. Jin, X. Bing, and Y. Zhang. Unique sparse decomposition of low rank matrices. In _Advances in Neural Information Processing Systems_, volume 34, pages 523-535. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/051928341be67dcba03f0e04104d9047-Paper.pdf. (Cited on page 2)
* Karagiannidis and Lioumpas [2007] G. K. Karagiannidis and A. S. Lioumpas. An improved approximation for the gaussian Q-function. _IEEE Communications Letters_, 11(8):644-646, 2007. doi: 10.1109/LCOMM.2007.070470. (Cited on page 24)
* Karimi Mahabadi et al. [2021] R. Karimi Mahabadi, J. Henderson, and S. Ruder. Compacter: Efficient low-rank hypercomplex adapter layers. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 1022-1035. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/081be9fddf07f3bc808f935906ef70c0-Paper.pdf. (Cited on page 1)* Li and Li [2019] X. Li and P. Li. Generalization error analysis of quantized compressive learning. _Advances in Neural Information Processing Systems_, 32, 2019.
* Li and Li [2019] X. Li and P. Li. Random projections with asymmetric quantization. _Advances in Neural Information Processing Systems_, 32, 2019.
* Li et al. [2020] X. Li, X. Yin, C. Li, P. Zhang, X. Hu, L. Zhang, L. Wang, H. Hu, L. Dong, F. Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXX 16_, pages 121-137. Springer, 2020.
* Lingala et al. [2011] S. G. Lingala, Y. Hu, E. DiBella, and M. Jacob. Accelerated dynamic MRI exploiting sparsity and low-rank structure: k-t SLR. _IEEE Transactions on Medical Imaging_, 30(5):1042-1054, 2011. doi: 10.1109/TMI.2010.2100850.
* Liu [2022] J. Liu. LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama_index.
* Lyubarskii and Vershynin [2010] Y. Lyubarskii and R. Vershynin. Uncertainty principles and vector quantization. _IEEE Transactions on Information Theory_, 56(7):3491-3501, 2010.
* Ma and Solomonik [2021] L. Ma and E. Solomonik. Fast and accurate randomized algorithms for low-rank tensor decompositions. In _Advances in Neural Information Processing Systems_, volume 34, pages 24299-24312. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/cbef46321026d8404bc3216d4774c8a9-Paper.pdf.
* Mahoney [2016] M. W. Mahoney. Lecture notes on randomized linear algebra, 2016. URL https://arxiv.org/abs/1608.04481.
* Mao et al. [2020] Y. Mao, Y. Wang, C. Wu, C. Zhang, Y. Wang, Q. Zhang, Y. Yang, Y. Tong, and J. Bai. LadaBERT: Lightweight adaptation of BERT through hybrid model compression. In _Proceedings of the 28th International Conference on Computational Linguistics_, pages 3225-3234, Barcelona, Spain (Online), Dec. 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.287. URL https://aclanthology.org/2020.coling-main.287.
* Mardia et al. [1979] K. Mardia, J. Kent, and J. Bibby. _Multivariate Analysis_. Academic Press, 1979. ISBN 978-0-12-471250-8.
* Marqo [2023] Marqo. marqo-ai/marqo: Vector search for humans, 2023. URL https://github.com/marqo-ai/marqo.
* Martinsson and Tropp [2020] P.-G. Martinsson and J. A. Tropp. Randomized numerical linear algebra: Foundations and algorithms. _Acta Numerica_, 29:403-572, 2020.
* Martinsson et al. [2011] P.-G. Martinsson, V. Rokhlin, and M. Tygert. A randomized algorithm for the decomposition of matrices. _Applied and Computational Harmonic Analysis_, 30(1):47-68, 2011. ISSN 1063-5203. doi: https://doi.org/10.1016/j.acha.2010.02.003. URL https://www.sciencedirect.com/science/article/pii/S1063520310000242.
* Mayekar and Tyagi [2021] P. Mayekar and H. Tyagi. RATQ: A universal fixed-length quantizer for stochastic optimization. _IEEE Transactions on Information Theory_, 67(5):3130-3154, 2021. doi: 10.1109/TIT.2021.3058663.
* Parisi et al. [2022] S. Parisi, A. Rajeswaran, S. Purushwalkam, and A. Gupta. The unsurprising effectiveness of pre-trained vision models for control. In _International Conference on Machine Learning_, pages 17359-17371. PMLR, 2022.
* Paszke et al. [2019] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, high-performance deep learning library. In _Advances in Neural Information Processing Systems 32_, pages 8024-8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf.
*- ECCV 2020_, pages 522-539, Cham, 2020. Springer International Publishing.
* [48] M. Pilanci. Lecture notes: Large scale matrix computation, optimization, and learning (ee 270), 2020. URL https://web.stanford.edu/class/ee270/Lecture18.pdf.
* [49] Pincone. Vector database for vector search | pinecone. https://www.pinecone.io/, 2023. (Accessed on 05/16/2023).
* [50] Y. Plan and R. Vershynin. Dimension reduction by random hyperplane tessellations. _Discrete & Computational Geometry_, 51(2):438-461, 2014.
* [51] Y. Qi, D. S. Sachan, M. Felix, S. J. Padmanabhan, and G. Neubig. When and why are pre-trained word embeddings useful for neural machine translation? _arXiv preprint arXiv:1804.06323_, 2018.
* [52] S. M. Rezaeinia, R. Rahmani, A. Ghodsi, and H. Veisi. Sentiment analysis based on improved pre-trained word embeddings. _Expert Systems with Applications_, 117:139-147, 2019.
* [53] M. Rudelson and R. Vershynin. Smallest singular value of a random rectangular matrix. _Communications on Pure and Applied Mathematics_, 62, 2008.
* [54] M. Safaryan, E. Shulgin, and P. Richtarik. Uncertainty principle for communication compression in distributed and federated learning and the search for an optimal compressor. _Information and Inference: A Journal of the IMA_, Apr 2021.
* [55] M. Safaryan, R. Islamov, X. Qian, and P. Richtarik. FedNL: Making newton-type methods applicable to federated learning. In _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 18959-19010. PMLR, 2022. URL https://proceedings.mlr.press/v162/safaryan22a.html.
* [56] R. Saha, M. Pilanci, and A. J. Goldsmith. Minimax optimal quantization of linear models: Information-theoretic limits and efficient algorithms, 2022. URL https://arxiv.org/abs/2202.11277.
* [57] R. Saha, M. Pilanci, and A. J. Goldsmith. Efficient randomized subspace embeddings for distributed optimization under a communication budget. _IEEE Journal on Selected Areas in Information Theory_, 3(2):183-196, 2022. doi: 10.1109/JSAIT.2022.3198412.
* 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 1-5, 2023. doi: 10.1109/ICASSP49357.2023.10095529.
* [59] V. Siskind. Second moments of inverse wishart-matrix elements. _Biometrika_, 59(3):690-691, 12 1972. ISSN 0006-3444. doi: 10.1093/biomet/59.3.690. URL https://doi.org/10.1093/biomet/59.3.690.
* [60] C. Studer, W. Yin, and R. G. Baraniuk. Signal representations with minimum \(\ell_{\infty}\)-norm. In _2012 50th Annual Allerton Conference on Communication, Control, and Computing (Allerton)_, pages 1270-1277, 2012.
* Volume 70_, ICML'17, page 3329-3337. JMLR.org, 2017.
* [62] A. T. Suresh, Z. Sun, J. Ro, and F. Yu. Correlated quantization for distributed mean estimation and optimization. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 20856-20876. PMLR, 17-23 Jul 2022. URL https://proceedings.mlr.press/v162/suresh22a.html. (Cited on pages 3 and 4)
* [63] S. Swaminathan, D. Garg, R. Kannan, and F. Andres. Sparse low rank factorization for deep neural network compression. _Neurocomputing_, 398:185-196, 2020. ISSN 0925-2312. doi: https://doi.org/10.1016/j.neucom.2020.02.035. URL https://www.sciencedirect.com/science/article/pii/S0925231220302253. (Cited on page 1)
* [64] M. Tahaei, E. Charlaix, V. Nia, A. Ghodsi, and M. Rezagholizadeh. KroneckerBERT: Significant compression of pre-trained language models through kronecker decomposition and knowledge distillation. In _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 2116-2127, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.154. URL https://aclanthology.org/2022.naacl-main.154. (Cited on page 1)
* [65] C. Tai, T. Xiao, Y. Zhang, X. Wang, and W. E. Convolutional neural networks with low-rank regularization, 2016. URL https://arxiv.org/abs/1511.06067. (Cited on page 1)
* [66] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar, et al. L1ama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023. (Cited on page 9)
* [67] J. A. Tropp, A. Yurtsever, M. Udell, and V. Cevher. Practical sketching algorithms for low-rank matrix approximation. _SIAM Journal on Matrix Analysis and Applications_, 38(4):1454-1485, 2017. doi: 10.1137/17M1111590. URL https://doi.org/10.1137/17M1111590. (Cited on page 2)
* [68] J. A. Tropp, A. Yurtsever, M. Udell, and V. Cevher. Streaming low-rank matrix approximation with an application to scientific simulation. _SIAM Journal on Scientific Computing_, 41(4):A2430-A2463, 2019. doi: 10.1137/18M1201068. URL https://doi.org/10.1137/18M1201068. (Cited on page 45)
* [69] M. Udell and A. Townsend. Why are big data matrices approximately low rank? _SIAM Journal on Mathematics of Data Science_, 1(1):144-160, 2019. doi: 10.1137/18M1183480. URL https://doi.org/10.1137/18M1183480. (Cited on page 1)
* [70] M. Valipour, M. Rezagholizadeh, I. Kobyzev, and A. Ghodsi. DyLoRa: Parameter Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation, 2023. URL https://arxiv.org/abs/2210.07558. (Cited on page 1)
* [71] S. Vargaftik, R. Ben-Basat, A. Portnoy, G. Mendelson, Y. Ben-Itzhak, and M. Mitzenmacher. DRIVE: One-bit distributed mean estimation. In _Advances in Neural Information Processing Systems_, 2021. URL https://openreview.net/forum?id=KXRTmcv3dQ8. (Cited on page 3)
* [72] S. Vargaftik, R. B. Basat, A. Portnoy, G. Mendelson, Y. B. Itzhak, and M. Mitzenmacher. Eden: Communication-efficient and robust distributed mean estimation for federated learning. In _International Conference on Machine Learning_, pages 21984-22014. PMLR, 2022. (Cited on page 3)
* [73] R. Vershynin. Introduction to the non-asymptotic analysis of random matrices, 2011. URL https://arxiv.org/abs/1011.3027. (Cited on pages 21 and 22)
* [74] R. Vershynin. _High-Dimensional Probability: An Introduction with Applications in Data Science_. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018. doi: 10.1017/9781108231596. (Cited on page 22)
* [75] M. J. Wainwright. _Basic tail and concentration bounds_, page 21-57. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019. doi: 10.1017/9781108627771.002. (Cited on page 21)* Wang et al. [2021] R. Wang, D. Tang, N. Duan, Z. Wei, X. Huang, J. Ji, G. Cao, D. Jiang, and M. Zhou. K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters. In _Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021_, volume ACL/IJCNLP 2021 of _Findings of ACL_, pages 1405-1418. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.findings-acl.121. URL https://doi.org/10.18653/v1/2021.findings-acl.121.
* Wang et al. [2020] Z. Wang, J. Wohlwend, and T. Lei. Structured pruning of large language models. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.emnlp-main.496. URL https://doi.org/10.18653%2Fv1%2F2020.emnlp-main.496.
* Winata et al. [2019] G. I. Winata, A. Madotto, J. Shin, E. J. Barezi, and P. Fung. On the effectiveness of low-rank matrix factorization for LSTM model compression, 2019. URL https://arxiv.org/abs/1908.09982.
* Witten and Candes [2015] R. Witten and E. Candes. Randomized algorithms for low-rank matrix factorizations: Sharp performance bounds. _Algorithmica_, 72(1):264-281, may 2015. ISSN 0178-4617. doi: 10.1007/s00453-014-9891-7. URL https://doi.org/10.1007/s00453-014-9891-7.
* Wolf et al. [2019] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, et al. Huggingface's transformers: State-of-the-art natural language processing. _arXiv preprint arXiv:1910.03771_, 2019.
* Yao et al. [2022] Z. Yao, R. Yazdani Aminabadi, M. Zhang, X. Wu, C. Li, and Y. He. ZeroQuant: Efficient and affordable post-training quantization for large-scale transformers. _Advances in Neural Information Processing Systems_, 35:27168-27183, 2022.
* Ye and Du [2021] T. Ye and S. S. Du. Global convergence of gradient descent for asymmetric low-rank matrix factorization. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 1429-1439. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/0af854284f4ab0cfea8fcd889cbb41a-Paper.pdf.
* Young et al. [2021] S. Young, Z. Wang, D. Taubman, and B. Girod. Transform quantization for CNN compression. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, pages 1-1, 2021. doi: 10.1109/tpami.2021.3084839. URL https://doi.org/10.11092%Ftptami.2021.3084839.
* Yu et al. [2017] X. Yu, T. Liu, X. Wang, and D. Tao. On compressing deep models by low rank and sparse decomposition. In _2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 67-76, 2017. doi: 10.1109/CVPR.2017.15.
* Zhang et al. [2021] H. Zhang, Y. Bi, and J. Lavaei. General low-rank matrix optimization: Geometric analysis and sharper bounds. In _Advances in Neural Information Processing Systems_, volume 34, pages 27369-27380. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/e60e81c4cbe5171cd654662d9887aec2-Paper.pdf.
* Zhang et al. [2013] Y. Zhang, Z. Jiang, and L. S. Davis. Learning structured low-rank representations for image classification. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 676-683, 2013.
* Zhou et al. [2014] X. Zhou, C. Yang, H. Zhao, and W. Yu. Low-rank modeling and its applications in image analysis. _ACM Computing Surveys (CSUR)_, 47(2):1-33, 2014.

###### Contents

* 1 Introduction
	* 1.1 Related Works
* 2 Proposed Algorithm
	* 2.1 Uniformly dithered quantizer
	* 2.2 Proposed algorithm: LPLR
* 3 Approximation Error Analysis
	* 3.1 Analysis outline
	* 3.2 Comparison with baselines
* 4 Numerical Simulations
	* 4.1 Overview
	* 4.2 Image Compression
	* 4.3 Embeddings extracted from pre-trained models
		* 4.3.1 Embedding Classification
		* 4.3.2 Compressing Weight Matrices of a Large Language Model
* 5 Conclusions
* A Notations
* B Preliminaries
* B.1 Linear algebra inequalities
* B.2 Probability and random matrix theory
* B.2.1 Tail bound for Gaussian distribution
* B.2.2 Inverse Wishart distribution
* B.2.3 Random Gaussian matrices
* B.2.4 Subgaussian random variables
* C Quantization error of uniformly dithered scalar quantizer
* D Gaussian embeddings: Application of equalization to vector quantizers
* E Dynamic range of quantizers
* E.1 Quantization for the first low-rank factor
* E.2 Quantization for the second low-rank factor
* F Sketched least squares with quantized response
* G LPLR algorithm: Approximation error analysis
* G.1 LPLR approximation error: Proof of Thm. 3.2
* 6.2 Informal version of Thm. G.2
* H Direct-SVD Quantization: Approximation error analysis
* I LPLR-SVD algorithm: Approximation error analysis
	* I.1 Informal version of Thm. I.1
* J Additional numerical simulations
* J.1 Image Compression
* J.2 Wall-clock time for compressing image embeddings
* J.3 High-resolution figures for Llama weight compression
* K Limitations and further discussions
Notations

We first gather some common notations in linear algebra and probability theory that have been used throughout the paper. Boldface upper and lowercase letters, \(\mathbf{A}\) and \(\mathbf{a}\) denote matrices and vectors respectively. \(\mathbf{I}_{d}\) denotes the \(d\times d\) identity matrix. The subscript may be dropped if the dimension is clear from the context. For any matrix \(\mathbf{A}\), its \(i^{\mathrm{th}}\) row and \(j^{\mathrm{th}}\) column are denoted by \(\mathbf{a}^{(i)}\) and \(\mathbf{a}_{j}\) respectively. The singular values of \(\mathbf{A}\) are denoted by \(\sigma_{\max}(\mathbf{A})=\sigma_{1}\geq\sigma_{2}\geq\ldots\geq\sigma_{r}= \sigma_{\min}(\mathbf{A})\), where \(r=\mathrm{rank}(\mathbf{A})\). Similarly, the eigenvalues are denoted as \(\lambda_{1}(\mathbf{A}),\ldots,\lambda_{r}(\mathbf{A})\). The maximum of \(\mathbf{A}\) is defined as \(\left\|A\right\|_{\max}=\max_{i,j}\left|A_{ij}\right|\), the spectral norm of \(\mathbf{A}\) is defined as \(\left\|\mathbf{A}\right\|_{2}=\sup_{\left\|\mathbf{x}\right\|=1}\left\| \mathbf{A}\mathbf{x}\right\|=\sigma_{\max}(\mathbf{A})\), and the Frobenius norm is \(\left\|\mathbf{A}\right\|_{\mathrm{F}}=\left(\sum_{i,j}A_{ij}^{2}\right)^{1/2 }=\text{Tr}\left[\mathbf{A}^{\top}\mathbf{A}\right]=\left(\sum_{k\in[\gamma] }\sigma_{k}^{2}\right)^{1/2}\). For any vector \(\mathbf{x}\), \(\left\|\mathbf{x}\right\|=\left(x_{i}^{2}\right)^{1/2}\) denotes the \(\ell_{2}\)-norm, and \(\left\|\mathbf{x}\right\|_{\infty}=\max_{i}\left|x_{i}\right|\) denotes the \(\ell_{\infty}\)-norm. We use the notations \(\succcurlyeq\) and \(\preccurlyeq\) for the positive semi-definite (PSD) cone ordering (or the _Loewner ordering_) of symmetric matrices, i.e., for any symmetric matrices \(\mathbf{X}\) and \(\mathbf{Y}\), \(\mathbf{X}\succcurlyeq\mathbf{Y}\iff\mathbf{X}-\mathbf{Y}\) is PSD. \(\mathbf{X}^{\dagger}\) denotes the Moore-Penrose pseudo-inverse of a matrix \(\mathbf{X}\). \(\log(\cdot)\) denotes the natural logarithm, i.e., base \(e\). \(\log_{2}(\cdot)\), i.e., with base \(2\) is specified explicitly. A normal distribution with mean \(\mu\) and variance \(\sigma^{2}\) is denoted as \(\mathcal{N}(0,\sigma^{2})\), while a multivariate normal distribution in \(\mathbb{R}^{d}\) is denoted as \(\mathcal{N}(\boldsymbol{\mu},\mathbf{\Sigma})\). \(\mathbb{E}[\cdot]\) denotes expectation of a random variable. The probability measure over which the expectation is taken is described in text. We also use big-'oh' notation \(\mathrm{O}(\cdot)\) that hides constants for asymptotic expressions, while \(\widetilde{\mathrm{O}}(\cdot)\) also hides terms that depends logarithmically on dimension.

We now list some notations that are either less commonly known or used specifically in this paper, along with some remarks such as their occurrence in the paper. Several of these notations have also been introduced in-context, but they are additionally collected here for easy reference.

\begin{table}
\begin{tabular}{c l l} \hline \hline Notation & \multicolumn{1}{c}{Description} & Remarks \\ \hline \(\mathbf{A},\mathbf{L},\mathbf{R}\) & Input matrix, left and right LPLR & \(\mathbf{LR}\) is an approximation of \(\mathbf{A}\). Entries of \(\mathbf{L}\) and \(\mathbf{R}\) are represented in low-precision formats & \(\mathbf{LR}\) is an approximation of \(\mathbf{A}\). Entries of \(\mathbf{L}\) and \(\mathbf{R}\) are represented in low-precision formats & \(\mathbf{LR}\) is an approximation of \(\mathbf{A}\). Entries of \(\mathbf{L}\) and \(\mathbf{R}\) are represented in low-precision formats & \(\mathbf{LR}\) is an approximation of \(\mathbf{A}\). Entries of \(\mathbf{L}\) and \(\mathbf{R}\) are represented in low-precision formats & \(\mathbf{LR}\) is an approximation of \(\mathbf{A}\). Entries of \(\mathbf{L}\) and \(\mathbf{R}\) are represented in low-precision formats & \(\mathbf{LR}\) is an approximation of \(\mathbf{A}\). Entries of \(\mathbf{L}\) and \(\mathbf{R}\) are represented in low-precision formats & \(\mathbf{LR}\) are represented in low-precision formats \\ \(n,d\) & Dimensions of input matrix & \(\mathbf{A}\in\mathbb{R}^{n\times d}\). & \(\mathbf{A}\in\mathbb{R}^{n\times d}\). \\ \(k\) & Target rank & Often, \(k\ll\min\{n,d\}\), but not necessarily. \\ \(\mathbf{A}=\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\top}\) & Full SVD of the matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\) & \(\mathbf{U}\in\mathbb{R}^{n\times n}\), \(\mathbf{\Sigma}\in\mathbb{R}^{n\times d}\), and \(\mathbf{V}\in\mathbb{R}^{d\times d}\). \\ \(\mathbf{A}_{k}=\mathbf{U}_{k}\mathbf{\Sigma}_{k}\mathbf{V}_{k}^{\top}\) & Best rank-\(k\) approximation of \(\mathbf{A}\) & \(\mathbf{U}\in\mathbb{R}^{n\times k}\) and \(\mathbf{V}\in\mathbb{R}^{n\times k}\) consists of top-\(k\) left and right singular vectors respectively. \\ \(m\) & Sketch size & \(m=k+p\), where \(k\) is the target rank, and \(p\) is the oversampling factor. \\ \(\mathbf{S}\in\mathbb{R}^{d\times m}\) & Sketching matrix & In this work, \(S_{ij}\sim\mathcal{N}\left(0,\frac{1}{m}\right)\). \\ \(\mathrm{Q},\mathrm{Q}^{\prime}\) & Quantizers for the first and second & \\  & low-rank factors with bit-budgets \(\mathrm{B}\) & \\  & and \(\mathrm{B}^{\prime}\), and dynamic ranges \(\mathrm{R}_{\mathrm{Q}}\) and & \\  & \(\mathrm{R}_{\mathrm{Q}^{\prime}}\) respectively. & \\ \(\kappa(\mathbf{A})\) & Condition number of input matrix \(\mathbf{A}\) & \(\kappa(\mathbf{A})=\sigma_{1},\sigma_{r}\) \\ \(\kappa(\mathbf{A}_{k})\) & Condition number of best rank-\(k\) approximation \(\mathbf{A}_{k}\) & \\ \(\kappa\) & A quantity that depends on the spectrum of \(\mathbf{A}\) & \\ \(\gamma\) & Aspect ratio of the sketch & \\ \(\delta\) & Target rank and sketch-size dependent quantity & \\ \(\mathcal{W}(\mathbf{\Psi},d)\) & Wishart distribution with covariance matrix \(\mathbf{\Psi}\) and degree of freedom \(d\) & Refer to SSB.2.2. \\ \hline \hline \end{tabular}
\end{table}
Table 9: Notations used in this paper Preliminaries

### Linear algebra inequalities

We state (with proofs) some standard inequalities from linear algebra that will be useful in proving our main results.

**Lemma B.1**.: **(Frobenius norm of matrix products)** _For any matrices \(\mathbf{A}\) and \(\mathbf{B}\), we have_

Proof.: Note that:

\[\left\|\mathbf{A}\mathbf{B}\right\|_{\mathrm{F}}^{2}=\sum_{j}\left\|(\mathbf{A }\mathbf{B})_{j}\right\|_{2}^{2}\leq\sum_{j}\left\|\mathbf{A}\mathbf{B}_{j} \right\|_{2}^{2}\overset{\mathrm{(i)}}{\leq}\left\|\mathbf{A}\right\|_{2}^{2} \sum_{j}\left\|\mathbf{B}_{j}\right\|_{2}^{2}=\left\|\mathbf{A}\right\|_{2}^{2 }\left\|\mathbf{B}\right\|_{\mathrm{F}}^{2}.\] (5)

Here, \(\mathrm{(i)}\) follows from the definition of spectral norm of a matrix, and this completes the proof. 

**Lemma B.2**.: **(Loewner ordering for matrix products)** _For any matrix \(\mathbf{A}\) and \(\mathbf{B}\), we have_

\[\sigma_{\min}^{2}(\mathbf{A})\ \mathbf{B}^{\top}\mathbf{B}\preccurlyeq\mathbf{B}^{ \top}\mathbf{A}^{\top}\mathbf{A}\mathbf{B}\preccurlyeq\sigma_{\max}^{2}( \mathbf{A})\ \mathbf{B}^{\top}\mathbf{B}.\]

Proof.: For any vector \(\mathbf{x}\neq\mathbf{0}\), we have,

\[\mathbf{x}^{\top}\mathbf{B}^{\top}\mathbf{A}^{\top}\mathbf{A}\mathbf{B} \mathbf{x}=\left\|\mathbf{A}\mathbf{B}\mathbf{x}\right\|_{2}^{2}\geq\sigma_{ \min}^{2}(\mathbf{A})\|\mathbf{B}\mathbf{x}\|_{2}^{2}=\mathbf{x}^{\top}\left( \sigma_{\min}^{2}(\mathbf{A})\ \mathbf{B}^{\top}\mathbf{B}\right)\mathbf{x}.\] (6)

The other direction holds similarly. This completes the proof. 

**Lemma B.3**.: **(Max-norm spectral-norm inequality)** _For any matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\), \(\left\|\mathbf{A}\right\|_{\max}\leq\left\|\mathbf{A}\right\|_{2}\)._

Proof.: Let \(\mathbf{e}_{i}\in\mathbb{R}^{n}\) and \(\widehat{\mathbf{e}}_{j}\in\mathbb{R}^{d}\) denote the \(i^{\mathrm{th}}\) and \(j^{\mathrm{th}}\) canonical basis vectors in \(\mathbb{R}^{n}\) and \(\mathbb{R}^{d}\) respectively. Then, using Cauchy-Schwarz inequality, we have,

\[\left\|\mathbf{A}\right\|_{\max}=\max_{i,j}\left|\mathbf{e}_{i}^{\top}\mathbf{ A}\widehat{\mathbf{e}}_{j}\right|\leq\max_{i,j}\left\|\mathbf{e}_{i}\right\| \left\|\mathbf{A}\widehat{\mathbf{e}}_{j}\right\|=\max_{j}\left\|\mathbf{A} \widehat{\mathbf{e}}_{j}\right\|\leq\sup_{\left\|\mathbf{x}\right\|=1}\left\| \mathbf{A}\mathbf{x}\right\|=\left\|\mathbf{A}\right\|_{2},\] (7)

completing the proof. 

**Lemma B.4**.: **(Submultiplicativity of spectral norm)** _For any two matrices \(\mathbf{A}\) and \(\mathbf{B}\), we have_

\[\left\|\mathbf{A}\mathbf{B}\right\|_{2}\leq\left\|\mathbf{A}\right\|_{2} \left\|\mathbf{B}\right\|_{2}.\]

_Moreover, an analogous "reverse-submultiplicativity" result for the minimum singular value of a matrix product also holds true, i.e.,_

\[\sigma_{\min}(\mathbf{A}\mathbf{B})\geq\sigma_{\min}(\mathbf{A})\sigma_{\min} (\mathbf{B}).\]

Proof.: Using Lemma B.2, we have,

\[\left\|\mathbf{A}\mathbf{B}\right\|_{2}^{2}=\lambda_{\max}\left(\mathbf{B}^{ \top}\mathbf{A}^{\top}\mathbf{A}\mathbf{B}\right)\overset{\mathrm{(i)}}{\leq }\left\|\mathbf{A}\right\|_{2}^{2}\cdot\lambda_{\max}\left(\mathbf{B}^{\top} \mathbf{B}\right)=\left\|\mathbf{A}\right\|_{2}^{2}\left\|\mathbf{B}\right\|_ {2}^{2},\] (8)

where \(\mathrm{(i)}\) follows from Lemma B.2, completing the proof. The lower bound on \(\sigma_{\min}(\mathbf{A}\mathbf{B})\) also follows a similar argument. 

**Lemma B.5**.: **(Lower bound on minimum singular value of matrix sums)** _For any matrices \(\mathbf{A}\) and \(\mathbf{B}\), we have_

\[\sigma_{\min}(\mathbf{A}+\mathbf{B})\geq\sigma_{\min}(\mathbf{A})-\left\| \mathbf{B}\right\|_{2}.\]

Proof.: We have the following chain of inequalities,

\[\sigma_{\min}(\mathbf{A}+\mathbf{B})=\inf_{\left\|\mathbf{x} \right\|=1}\left\|(\mathbf{A}+\mathbf{B})\,\mathbf{x}\right\| \overset{\mathrm{(i)}}{\geq}\inf_{\left\|\mathbf{x}\right\|=1}\left(\left\| \mathbf{A}\mathbf{x}\right\|-\left\|\mathbf{B}\mathbf{x}\right\|\right)\] \[\geq\inf_{\left\|\mathbf{x}\right\|=1}\left\|\mathbf{A} \mathbf{x}\right\|-\sup_{\left\|\mathbf{x}\right\|=1}\left\|\mathbf{B} \mathbf{x}\right\|=\sigma_{\min}(\mathbf{A})-\left\|\mathbf{B}\right\|_{2}.,\]

where \(\mathrm{(i)}\) is the reverse triangle inequality. This completes the proof. 

**Lemma B.6**.: **(Rotation invariance of singular values)** _For a given matrix \(\mathbf{A}\), \(\sigma_{i}(\mathbf{A})=\sigma_{i}(\mathbf{U}\mathbf{A})\) for all \(i=1,\ldots,\mathrm{rank}\left(\mathbf{A}\right)\) for any unitary matrix \(\mathbf{U}\)._Proof.: For \(i=1,\ldots,\mathrm{rank}(\mathbf{A})\), since \(\mathbf{U}^{\top}\mathbf{U}=\mathbf{I}\), we have

\[\sigma_{i}(\mathbf{U}\mathbf{A})=\sqrt{\lambda_{i}\left(\mathbf{A}^{\top} \mathbf{U}^{\top}\mathbf{U}\mathbf{A}\right)}=\sqrt{\lambda_{i}\left(\mathbf{ A}^{\top}\mathbf{A}\right)}=\sigma_{i}(\mathbf{A}),\]

completing the proof. 

**Lemma B.7**.: **(Rotation invariance of Frobenius norm)** _For a given matrix \(\mathbf{A}\), for any unitary matrix \(\mathbf{U}\)._

Proof.: We have,

\[\left\|\mathbf{U}\mathbf{A}\right\|_{\mathrm{F}}^{2}=\text{Tr}\left[\mathbf{A} ^{\top}\mathbf{U}^{\top}\mathbf{U}\mathbf{A}\right]=\text{Tr}\left[\mathbf{A }^{\top}\mathbf{A}\right]=\left\|\mathbf{A}\right\|_{\mathrm{F}}^{2}.\] (9)

### Probability and random matrix theory

We restate some results from probability and random matrix theory which will be useful in deriving the main result of our paper.

#### b.2.1 Tail bound for Gaussian distribution

Gaussian distributions have strong concentration properties which we exploit in deriving the results of this paper. The following tail bound on a Gaussian random variable can be found in standard texts such as Wainwright (75, SS2.1.2), and is restated here.

**Lemma B.8**.: **(Chernoff bound for centered Gaussian)** _For \(X\sim\mathcal{N}\left(0,\sigma^{2}\right)\), we have,_

\[\Pr\left(\left|X\right|\geq t\right)\leq 2e^{-\frac{\sigma^{2}}{2\sigma^{2}}}.\] (10)

The proof of this follows from a direct application of Chernoff bound.

#### b.2.2 Inverse Wishart distribution

Consider a matrix \(\mathbf{S}\in\mathbb{R}^{d\times m}\), each row of which is drawn independently from the distribution \(\mathcal{N}\left(\mathbf{0},\mathbf{\Psi}\right)\), where \(\mathbf{\Psi}\in\mathbb{R}^{m\times m}\). Then, the probability distribution of the \(m\times m\) random matrix \(\mathbf{S}^{\top}\mathbf{S}\) is called the _Wishart distribution_ with \(d\) degrees of freedom, denoted as \(\mathcal{W}\left(\mathbf{\Psi},d\right)\). Moreover, the distribution of the matrix \(\left(\mathbf{S}^{\top}\mathbf{S}^{\top}\right)^{-1}\) is called the _inverse Wishart distribution_ and is denoted by \(\mathcal{W}^{-1}\left(\mathbf{\Psi}^{-1},d\right)\). These distributions have been studied extensively and further details can be found in Mardia et al. (2018) or Siskind (2018).

**Lemma B.9**.: **(Expected trace of inverse Wishart matrix)** _Suppose \(\mathbf{S}\in\mathbb{R}^{d\times m}\) matrix, each entry of which is drawn independently from \(\mathcal{N}\left(0,\frac{1}{m}\right)\). Then, the matrix \(\mathbf{S}^{\top}\mathbf{S}\in\mathbb{R}^{m\times m}\) follows the Wishart distribution \(\mathcal{W}\left(\frac{1}{m}\mathbf{I}_{m},d\right)\) that satisfies,_

\[\mathbb{E}\ \mathrm{Tr}\left[\left(\mathbf{S}^{\top}\mathbf{S}\right)^{-1} \right]=\frac{m^{2}}{d-m-1}.\] (11)

Proof.: From Mardia et al. (2018, eq. \(3.8.3\)), if \(\mathbf{X}\sim\mathcal{W}^{-1}\left(\Psi^{-1},d\right)\), then,

\[\mathbb{E}\left[\left(\mathbf{S}^{\top}\mathbf{S}\right)^{-1} \right]=\frac{\mathbf{\Psi}^{-1}}{d-m-1}.\] (12)

Here, \(\mathbf{\Psi}=\frac{1}{m}\mathbf{I}_{m}\), implying \(\text{Tr}\left[\mathbf{\Psi}^{-1}\right]=m^{2}\), which completes the proof. 

#### b.2.3 Random Gaussian matrices

The spectral norm of random matrices with Gaussian entries have interesting concentration properties. In this section, we present a lemma from Vershynin (2018) that formally states this result.

**Lemma B.10**.: _Let the entries of matrix \(\mathbf{S}\in\mathbb{R}^{d\times m}\) be distributed according to \(S_{ij}\sim\mathcal{N}\left(0,\frac{1}{m}\right)\). Then for every \(t\geq 0\), with probability at least \(1-2e^{-\frac{m\sigma^{2}}{2}}\), we have,_

\[\sqrt{\frac{d}{m}}-1-t\leq\sigma_{\min}(\mathbf{S})\leq\sigma_{ \max}(\mathbf{S})\leq\sqrt{\frac{d}{m}}+1+t.\] (13)The above lemma is a straightforward modification of the result in Vershynin [73, Corr. 5.35], which states the concentration result when the entries are distributed according to \(\mathcal{N}(0,1)\). Note that given \(\mathbf{S}\) with entries \(S_{ij}\sim\mathcal{N}\left(0,\frac{1}{m}\right)\) as above, the matrix \(\widehat{\mathbf{S}}=\sqrt{m}\ \mathbf{S}\) will have entries \(\widehat{S}_{ij}\sim\mathcal{N}\left(0,1\right)\). Furthermore, \(\sigma_{i}(\widehat{\mathbf{S}})=\sqrt{m}\ \sigma_{i}\left(\mathbf{S}\right)\) and the conclusion is immediate.

#### b.2.4 Subgaussian random variables

Subgaussian random variables refer to a class of distributions that are dominated by the distribution of a centered Gaussian random variable. There are several equivalent ways to characterize subgaussian random variables which can be found in several textbooks (see for example, Vershynin [74, Prop. 2.5.2]). We will focus on the following definition. More formally, the distribution of a random variable \(\mathbf{X}\) is subgaussian if the moment generating function of \(X^{2}\) is bounded at some point, i.e.,

\[\mathbb{E}\left[e^{X^{2}/K^{2}}\right]\leq 2.\] (14)

**Definition B.11**.: **(Subgaussian norm)** The subgaussian norm of a subgaussian random variable \(X\), denoted by \(\left\|X\right\|_{\psi_{2}}\) is defined to be the smallest \(K\) in (14). In other words,

\[\left\|X\right\|_{\psi_{2}}\triangleq\inf\left\{t\geq 0\mid\mathbb{E}\left[e^{X ^{2}/t^{2}}\right]\leq 2\right\}.\] (15)

It can be shown that any bounded random variable \(X\) is subgaussian, and satisfies,

\[\left\|X\right\|_{\psi_{2}}\leq\frac{\left\|X\right\|_{\infty}}{\log 2}.\] (16)

We next present a result that upper bounds the spectral norm of a matrix with subgaussian entries.

**Lemma B.12**.: _(Vershynin [74, Thm 4.4.5]) Let \(\mathbf{X}\) be a \(d\times m\) random matrix whose entries \(X_{ij}\) are independent, zero-mean, subgaussian random variables. Then, for any \(t>0\), we have,_

\[\left\|\mathbf{X}\right\|_{2}\leq CK\left(\sqrt{d}+\sqrt{m}+t\right)\]

_with probability exceeding \(1-2e^{-t^{2}}\). Here, \(K=\max_{i,j}\left\|A_{ij}\right\|_{\psi_{2}}\) and \(C\) is an absolute constant._

## Appendix C Quantization error of uniformly dithered scalar quantizer

For a scalar \(x\in[-\mathrm{R},+\mathrm{R}]\), let us denote the quantization error of uniformly dithered scalar quantizer with a bit-budget of \(\mathrm{B}\) bits as \(\epsilon=\mathrm{Q}_{\mathrm{R},\mathrm{B}}(x)-x\). Clearly, the quantization error is bounded as \(\left|\epsilon\right|\leq\Delta\). The following result further characterizes its mean and variance of this error.

**Lemma C.1**.: _The uniformly dithered scalar quantizer as described in (1) satisfies,_

\[\mathbb{E}\left[\epsilon\right]=0\ \ \text{and}\ \ \text{Var}\left(\epsilon \right)\leq\frac{\mathrm{R}^{2}}{\left(2^{\mathrm{B}}-1\right)^{2}},\]

_where the \(\mathbb{E}(\cdot)\) is over the randomness due to dithering in the quantizer operation._

Proof.: Suppose \(x\in[q_{k},q_{k+1})\) and \(q_{k+1}=q_{k}+\Delta\), where \(\Delta=\frac{2\mathrm{R}}{2^{\mathrm{B}}-1}\). Then,

\[\mathbb{E}\ \mathrm{Q}_{\mathrm{R},\mathrm{B}}(x)=q_{k+1}\ \frac{x-q_{k}}{\Delta}+q_{k}\ \left(1-\frac{x-q_{k}}{\Delta}\right)=\frac{\left(q_{k}+\Delta\right)\left(x-q_ {k}\right)+q_{k}\left(\Delta-x+q_{k}\right)}{\Delta}=x.\]

To evaluate the variance,

\[\text{Var}\left(\mathrm{Q}_{\mathrm{R},\mathrm{B}}(x)-x\right)^{2} =\left(q_{k+1}-x\right)^{2}\frac{\left(x-q_{k}\right)}{\Delta}+ \left(q_{k}-x\right)^{2}\left(1-\frac{x-q_{k}}{\Delta}\right)\] \[\leq\left(q_{k+1}-x\right)\left(x-q_{k}\right)\] \[\leq\sup_{x\in\left\{q_{k},q_{k+1}\right\}}\left(q_{k+1}-x\right) \left(x-q_{k}\right)\] \[=\left(q_{k+1}-\frac{q_{k}+q_{k+1}}{2}\right)\left(\frac{q_{k}+q_ {k+1}}{2}-q_{k}\right)=\frac{\Delta^{2}}{4}=\frac{\mathrm{R}^{2}}{\left(2^{ \mathrm{B}}-1\right)^{2}}.\]

This completes the proof.

Gaussian embeddings: Application of equalization to vector quantizers

In this section, we show a result on how Gaussian embeddings help in reducing the \(\ell_{2}\)-quantization error of a uniformly dithered vector quantizer. We consider a _clipped version_ of the uniform scalar quantizer with bit-budget \(\mathrm{B}\) described in SS2.1 and App. C. In order to quantize a vector \(\mathbf{x}\in\mathbb{R}^{d}\) with \(\left\|\mathbf{x}\right\|_{2}\leq\mathrm{R}\) using a uniform scalar quantizer (as described above), the quantization operation is applied to each coordinate of the vector independently, i.e.,

\[\mathrm{Q}_{\mathrm{R}}(\mathbf{x})=[\mathrm{Q}_{\mathrm{R}}(x_{1}),\ldots, \mathrm{Q}_{\mathrm{R}}(x_{d})].\] (17)

Here, the subscript \(\mathrm{B}\) is dropped because the bit-budget is evident from the context. Furthermore, since \(\left\|\mathbf{x}\right\|_{2}\leq\mathrm{R}\) implies \(x_{i}\in[-\mathrm{R},+\mathrm{R}]\) for every \(i\in[d]\), the quantizer \(\mathrm{Q}_{\mathrm{R}}\) does not saturate. From Lemma C.1, the expected quantization error is given by,

\[\mathbb{E}\left\|\mathrm{Q}_{\mathrm{R}}(\mathbf{x})-\mathbf{x}\right\|_{2}^ {2}=\sum_{i\in[d]}\mathbb{E}\left[\left(\mathrm{Q}_{\mathrm{R}}(x_{i})-x_{i} \right)^{2}\right]\leq\frac{\mathrm{R}^{2}d}{\left(2^{\mathrm{B}}-1\right)^{2 }}.\] (18)

**Quantizing Gaussian embeddings**: Suppose instead of quantizing \(\mathbf{x}\in\mathbb{R}^{d}\) directly, we quantize \(\mathbf{u}=\mathbf{S}\mathbf{x}\in\mathbb{R}^{m}\), where \(\mathbf{S}\in\mathbb{R}^{m\times d}\) with \(S_{ij}\sim\mathcal{N}\left(0,\frac{1}{m}\right)\). Note that for every \(j\in[m]\), we have \(u_{j}\sim\mathcal{N}\left(0,\frac{1}{m}\left\|\mathbf{x}\right\|_{2}^{2}\right)\). Since \(u_{j}\) can be anything in \((-\infty,+\infty)\), there is a finite probability that the uniform scalar quantizer might get saturated. For this reason, for any scalar \(u\in(-\infty,+\infty)\), we define the _clipped uniformly dithered quantizer_ with clipping parameter \(t\) as follows:

\[\mathrm{Q}(u)=\begin{cases}\mathrm{Q}_{t}(u)&\text{if}\ \ |u|\leq t\\ t&\text{if}\ \ \ u>t\\ -t&\text{if}\ \ \ u<-t.\end{cases}\] (19)

The dynamic range of the quantizer is parameterized by \(t\), which is to be chosen appropriately. Note that it is just for this section for the purposes of illustration, that we choose the clipped variant of the quantizer. In LPLR, we choose the dynamic range to be high enough so in practice it remains unsaturated with a very high probability. The following proposition upper bounds the quantization error of quantizing Gaussian embeddings.

**Proposition D.1**.: _For a given vector \(\mathbf{x}\in\mathbb{R}^{d}\) with \(\left\|\mathbf{x}\right\|\leq\mathrm{R}\), the clipped uniformly dithered quantizer described in (19) with \(t=\frac{\mathrm{R}}{\sqrt{m}}\) satisfies_

\[\mathbb{E}\left\|\mathrm{Q}(\mathbf{S}\mathbf{x})-\mathbf{S}\mathbf{x}\right\|_ {2}^{2}\leq\frac{\mathrm{R}^{2}}{\left(2^{\mathrm{B}}-1\right)^{2}}+\frac{ \mathrm{R}^{2}\sqrt{2}}{\sqrt{\pi e}},\]

_where the expectation is over the randomness in the construction of \(\mathbf{S}\) and the quantization dither._

Proof.: Since \(\left\{u_{i}\right\}_{i\in[m]}\) are independently and identically distributed, let us denote the distribution as \(f(u)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{\mathrm{e}^{2}}{2\sigma^{2}}}\), where \(\sigma=\frac{\left\|\mathbf{x}\right\|_{2}^{2}}{m}\). The expected quantization error for the clipped quantizer is then given by,

\[\mathbb{E}_{\mathbf{S},\mathrm{Q}_{t}}\left[\left(\mathrm{Q}(u)-u\right)_{2}^ {2}\right]=\int_{\left|u\right|\leq t}\mathbb{E}_{\mathrm{Q}_{t}}\left[\left( \mathrm{Q}(u)-u\right)_{2}^{2}\right]f(u)du+2\int_{u>t}\left(t-u\right)^{2}f(u )du\] (20)

Here, the expectation is over the stochasticity of the quantization dither, as well as the random matrix \(\mathbf{S}\). The factor of \(2\) in the second term appears due to symmetry of clipping and that of Gaussian distribution. Using (18), the first term on the R.H.S. can be upper bounded as,

\[\int_{\left|u\right|\leq t}\mathbb{E}_{\mathrm{Q}_{t}}\left[\left(\mathrm{Q}(u) -u\right)_{2}^{2}\right]f(u)du\leq\frac{t^{2}}{\left(2^{\mathrm{B}}-1\right)^ {2}}\int_{\left|u\right|\leq t}f(u)du\leq\frac{t^{2}}{\left(2^{\mathrm{B}}-1 \right)^{2}}.\] (21)

To analyze the second term, note that,

\[\frac{1}{\sigma\sqrt{2\pi}}\int_{t}^{\infty}(t-u)^{2}e^{-\frac{\mathrm{e}^{2} }{2\sigma^{2}}}du=\frac{1}{2}\left(\sigma^{2}+t^{2}\right)\left(1-\mathrm{erf }\left(\frac{t}{\sigma\sqrt{2}}\right)\right)-\frac{\sigma t}{\sqrt{2\pi}}e^ {-\frac{t^{2}}{2\sigma^{2}}}\triangleq\Psi(t,\sigma),\] (22)

where \(\mathrm{erf}(z)\) denotes the error function defined as,

\[\mathrm{erf}(z)=\frac{2}{\sqrt{\pi}}\int_{0}^{x}e^{-x^{2}}dx\] (23)Simple calculation would show,

\[\frac{\partial\Psi(t,\sigma)}{\partial\sigma}\] \[= \sigma\left(1-\mathrm{erf}\left(\frac{t}{\sigma\sqrt{2}}\right) \right)-\frac{\sigma t}{\sqrt{2\pi}}e^{-\frac{t^{2}}{2\sigma^{2}}}+\frac{t}{ \sqrt{2\pi}}\left(1+\frac{t^{2}}{\sigma^{2}}\right)e^{-\frac{t^{2}}{2\sigma^{ 2}}}-\frac{t}{\sqrt{2\pi}}e^{-\frac{t^{2}}{2\sigma^{2}}}-\frac{t^{3}}{\sigma^ {2}\sqrt{2\pi}}e^{-\frac{t^{2}}{2\sigma^{2}}}\] \[= \sigma\left(1-\mathrm{erf}\left(\frac{t}{\sigma\sqrt{2}}\right) \right)\geq 0.\] (24)

Since \(\frac{\partial\Psi(t,\sigma)}{\partial\sigma}\geq 0\), \(\Psi(t,\sigma)\) is a non-decreasing function of \(\sigma\). Since \(\sigma^{2}=\frac{\left\|\mathbf{x}\right\|_{2}^{2}}{m}\leq\frac{\mathrm{R}^{2} }{m}\), we have the upper bound,

\[2\int_{u>t}\left(t-u\right)^{2}f(u)du \leq\left(\frac{\mathrm{R}^{2}}{m}+t^{2}\right)\left(1-\mathrm{ erf}\left(\frac{t\sqrt{m}}{\mathrm{R}\sqrt{2}}\right)\right)-\frac{\mathrm{R}t \sqrt{2}}{\sqrt{\pi m}}e^{-\frac{mt^{2}}{2\mathrm{R}^{2}}}\] \[=\left(\frac{\mathrm{R}^{2}}{m}+t^{2}\right)\mathrm{erfc}\left( \frac{t\sqrt{m}}{\mathrm{R}\sqrt{2}}\right)-\frac{\mathrm{R}t\sqrt{2}}{\sqrt{ \pi m}}e^{-\frac{mt^{2}}{2\mathrm{R}^{2}}},\] (25)

where \(\mathrm{erfc}(z)=1-\mathrm{erf}(z)\) is the complementary error function.

**Quantization error of a scalar Gaussian sketch**: Since \(\mathbf{x}\in\mathbb{R}^{d}\) with \(\left\|\mathbf{x}\right\|_{2}\leq\mathrm{R}\), let \(\mathbf{s}\sim\mathcal{N}(\mathbf{0},\frac{1}{m}\mathbf{I}_{d})\). We first consider the quantization of \(\mathbf{x}^{\top}\mathbf{s}\), and upper bound the error \(\mathbb{E}\left[\left(\mathrm{Q}(\mathbf{x}^{\top}\mathbf{s})-\mathbf{x}^{ \top}\mathbf{s}\right)_{2}^{2}\right]\). Clearly, \(\mathbf{x}^{\top}\mathbf{s}\sim\mathcal{N}(0,\frac{\mathrm{R}^{2}}{m})\). Consequently, using (21) and (25), for any \(t\geq 0\), we have,

\[\mathbb{E}\left[\left(\mathrm{Q}(\mathbf{x}^{\top}\mathbf{s})- \mathbf{x}^{\top}\mathbf{s}\right)_{2}^{2}\right] \leq\frac{t^{2}}{\left(2^{\mathrm{B}}-1\right)^{2}}+\left(\frac{ \mathrm{R}^{2}}{m}+t^{2}\right)\mathrm{erfc}\left(\frac{t\sqrt{m}}{\mathrm{R} \sqrt{2}}\right)-\frac{\mathrm{R}t\sqrt{2}}{\sqrt{\pi m}}e^{-\frac{mt^{2}}{2 \mathrm{R}^{2}}}\] \[\overset{\mathrm{(i)}}{\leq}\frac{t^{2}}{\left(2^{\mathrm{B}}-1 \right)^{2}}+\left(\frac{\mathrm{R}^{2}}{m}+t^{2}\right)\frac{\mathrm{R}\sqrt{ 2}}{t\sqrt{\pi m}}e^{-\frac{mt^{2}}{2\mathrm{R}^{2}}}-\frac{\mathrm{R}t\sqrt{ 2}}{\sqrt{\pi m}}e^{-\frac{mt^{2}}{2\mathrm{R}^{2}}}\] \[=\frac{t^{2}}{\left(2^{\mathrm{B}}-1\right)^{2}}+\frac{\mathrm{R} ^{3}\sqrt{2}}{m^{3/2}t\sqrt{\pi}}e^{-\frac{mt^{2}}{2\mathrm{R}^{2}}}.\] (26)

Here, \(\mathrm{(i)}\) follows from the upper bound \(\mathrm{erfc}(z)\leq\frac{e^{-z^{2}}}{\sqrt{\pi z}}\) from Karagiannidis and Lioumpas [29].

**Quantization error of a vector Gaussian sketch**: We now consider for any \(t\geq 0\), the expected quantization error for \(\mathbf{x}\in\mathbb{R}^{d}\) with \(\left\|\mathbf{x}\right\|_{2}\leq\mathrm{R}\) and \(\mathbf{S}\in\mathbb{R}^{m\times d}\) with \(S_{ij}\sim\mathcal{N}\left(0,\frac{1}{m}\right)\). Each row of \(\mathbf{S}\) now independently plays the role of \(\mathbf{s}\) in (26). Using (26), the vector quantization error is simply \(m\) times the scalar quantization error, and is now given by,

\[\mathbb{E}\left\|\mathrm{Q}(\mathbf{S}\mathbf{x})-\mathbf{S}\mathbf{x}\right\| _{2}^{2}\leq\frac{mt^{2}}{\left(2^{\mathrm{B}}-1\right)^{2}}+\frac{\mathrm{R} ^{3}\sqrt{2}}{t\sqrt{\pi m}}e^{-\frac{mt^{2}}{2\mathrm{R}^{2}}}.\] (27)

**Choice of dynamic range \(t\)**: Setting \(t=\frac{\mathrm{R}}{\sqrt{m}}\) in (27) yields,

\[\mathbb{E}\left\|\mathrm{Q}(\mathbf{S}\mathbf{x})-\mathbf{S}\mathbf{x}\right\| _{2}^{2}\leq\frac{\mathrm{R}^{2}}{\left(2^{\mathrm{B}}-1\right)^{2}}+\frac{ \mathrm{R}^{2}\sqrt{2}}{\sqrt{\pi e}}.\] (28)

This completes the proof. 

Note that since \(\mathbb{E}[\mathbf{S}^{\top}\mathbf{S}]\) is an identity matrix, an estimate \(\mathbf{\widehat{x}}\) of \(\mathbf{x}\) can be recovered from \(\mathrm{Q}(\mathbf{S}\mathbf{x})\) as \(\mathbf{S}^{\top}\mathrm{Q}(\mathbf{S}\mathbf{x})\). We can use the \(\mathrm{O}(1)\) bound on \(\|\mathrm{Q}(\mathbf{S}\mathbf{x})-\mathbf{S}\mathbf{x}\|^{2}\) to get a bound on \(\|\mathbf{S}^{\top}\mathrm{Q}(\mathbf{S}\mathbf{x})-\mathbf{x}\|^{2}\) as follows:

\[\|\mathbf{S}^{\top}\mathrm{Q}(\mathbf{S}\mathbf{x})-\mathbf{x}\|^ {2} \leq\|\mathrm{Q}(\mathbf{S}\mathbf{x})-\mathbf{S}\mathbf{x}\|^{2}+\| \mathbf{S}^{\top}\mathrm{Q}(\mathbf{S}\mathbf{x})\|^{2}-\|\mathrm{Q}(\mathbf{S} \mathbf{x})\|^{2}+\|\mathbf{x}\|^{2}-\|\mathbf{S}\mathbf{x}\|^{2}\] \[\leq\|\mathrm{Q}(\mathbf{S}\mathbf{x})-\mathbf{S}\mathbf{x}\|^{2}+\| \mathbf{S}^{\top}\mathrm{Q}(\mathbf{S}\mathbf{x})\|^{2}+\|\mathbf{x}\|^{2}\] \[\leq\|\mathrm{Q}(\mathbf{S}\mathbf{x})-\mathbf{S}\mathbf{x}\|^{2}+ \mathrm{R}^{2}(\sigma_{\max}^{2}(\mathbf{S})+1)\] (29)

From properties of random Gaussian matrices, we know that \(\sigma_{\max}^{2}(\mathbf{S})\leq\frac{d}{m}\) with high probability. Hence, the error \(\|\mathbf{S}^{\top}\mathrm{Q}(\mathbf{S}\mathbf{x})-\mathbf{x}\|^{2}\) only depends on the aspect ratio \(d/m\), and not the dimension \(d\) directly. Although the reconstruction error \(\|\mathbf{S}^{\top}\mathrm{Q}(\mathbf{S}\mathbf{x})-x\|^{2}\) scales as \(d/m\), it does not necessarily increase with \(d\) if we choose the sketch size \(m\) to be proportional to \(d\), i.e., \(m=O(d)\).

Despite this, Gaussian embeddings are not used practically for vector quantization because \(\mathbf{S}\) is dense matrix and computing \(\mathbf{S}\mathbf{x}\) entails a complexity of \(\mathrm{O}(d^{2})\). The entries of \(\mathbf{S}\) themselves are floating point numbers that have to be stored in full precision, hence this defeats the whole purpose of quantizing \(\mathbf{x}\) using fewer bits. However, this is not an issue for matrix compression because in LPLR, we do not explicitly compute the \(\mathbf{S}^{\top}\mathrm{Q}(\mathbf{S}\mathbf{A})\) anywhere. In other words, the effects of \(\mathbf{S}\) in the first low-rank factor is nullified by the second low-rank factor. Unlike vector quantization, the corresponding sketch size \(m\) for LPLR only needs to be the same order as the inherent rank \(k\), which can be much smaller than \(\min\{n,d\}\), i.e., the dimensions of the matrix being compressed.

## Appendix E Dynamic range of quantizers

We now derive high probability upper bounds on the maximum magnitude of the input to uniform quantizers \(\mathrm{Q}\) and \(\mathrm{Q}^{\prime}\). Choosing these upper bounds to be the dynamic range of the uniform scalar quantizers will ensure that the quantizer remains unsaturated with a high probability, which is our desired regime of operation.

### Quantization for the first low-rank factor

We first look at the choice of dynamic range for the quantizer \(\mathrm{Q}\), which is used to obtain the first low-rank factor. The input to the quantizer is \(\mathbf{A}\mathbf{S}\in\mathbb{R}^{n\times m}\) where \(\mathbf{A}\in\mathbb{R}^{n\times d}\) and \(\mathbf{S}\in\mathbb{R}^{d\times m}\), and the entries of \(\mathbf{S}\) are i.i.d. as \(S_{ij}\sim\mathcal{N}\left(0,\frac{1}{m}\right)\). Lemma E.1 below gives a high probability upper bound on the max norm of \(\mathbf{A}\mathbf{S}\). This probability is computed over the randomness in the construction of \(\mathbf{S}\).

**Lemma E.1**.: **(Max norm of \(\mathbf{A}\mathbf{S}\))** _Given matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\) with bounded row norms, i.e., \(\left\|\mathbf{a}^{(i)}\right\|\leq\mathrm{R}\), and \(\mathbf{S}\in\mathbb{R}^{d\times m}\) with entries distributed as \(S_{ij}\overset{i.i.d.}{\sim}\mathcal{N}\left(0,\frac{1}{m}\right)\), with probability exceeding \(1-\frac{\epsilon}{8n\mathrm{R}^{2}}\), the max norm of \(\mathbf{A}\mathbf{S}\) satisfies,_

\[\left\|\mathbf{A}\mathbf{S}\right\|_{\max}\leq\mathrm{R}\sqrt{\frac{2\log \left(\frac{16\mathrm{R}^{2}n^{2}m}{\epsilon}\right)}{m}}.\] (30)

Proof.: Since \(\left(\mathbf{A}\mathbf{S}\right)_{ij}=\mathbf{s}_{j}^{\top}\mathbf{a}^{(i)}\), where \(\mathbf{a}^{(i)}\in\mathbb{R}^{d}\) is the \(i^{\mathrm{th}}\) row of \(\mathbf{A}\) and \(\mathbf{s}_{j}\in\mathbb{R}^{d}\) is the \(j^{\mathrm{th}}\) column of \(\mathbf{S}\), we have \(\left(\mathbf{A}\mathbf{S}\right)_{ij}\sim\mathcal{N}\left(0,\frac{\left\| \mathbf{a}_{i}\right\|_{2}^{2}}{m}\right)\). Using Lemma B.8 and an application of union bound gives,

\[\Pr\left(\left|\left(\mathbf{A}\mathbf{S}\right)_{ij}\right|\geq t\right)\leq 2 e^{-\frac{-m^{2}}{2\left\|\mathbf{a}^{(i)}\right\|_{2}^{2}}}\leq 2e^{-\frac{m^{2}}{2 R^{2}}}.\] (31)

A subsequent application of union bound over all the entries of \(\mathbf{A}\mathbf{S}\) yields,

\[\Pr\left(\left\|\mathbf{A}\mathbf{S}\right\|_{\max}\geq t\right)\leq 2 nme^{-\frac{m^{2}}{2\mathrm{R}^{2}}}.\] (32)

Setting \(t=\mathrm{R}\sqrt{\frac{2\log\left(\frac{16\mathrm{R}^{2}n^{2}m}{\epsilon} \right)}{m}}\) in the above completes the proof. 

### Quantization for the second low-rank factor

We now obtain a high-probability upper bound on the max norm of \(\mathrm{Q}(\mathbf{A}\mathbf{S})^{\dagger}\mathbf{A}\), which is the input to the second quantizer \(\mathrm{Q}^{\prime}\). Let us define \(\mathcal{Q}\) to be the event that the quantizer \(\mathrm{Q}\) does not saturate. From Lemma E.1, \(\mathcal{Q}\) occurs with a sufficiently high probability. An appropriate choice of dynamic range for \(\mathrm{Q}^{\prime}\) will ensure that conditioned on the event that \(\mathcal{Q}\) occurs, the quantizer \(\mathrm{Q}^{\prime}\) also does not saturate with a high probability. The following lemma states this formally.

**Lemma E.2**.: **(Max norm of \(\mathrm{Q}\left(\mathbf{A}\mathbf{S}\right)^{\dagger}\mathbf{A}\))** _Let our input matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\) have non-zero singular values \(\sigma_{1},\dots,\sigma_{k},\sigma_{k+1},\dots,\sigma_{r}\), where \(r=\mathrm{rank}(\mathbf{A})\), and bounded row norms \(\left\|\mathbf{a}^{(i)}\right\|\leq\mathrm{R}\). Let \(\kappa(\mathbf{A})=\sigma_{1}/\sigma_{r}\) and \(\kappa(\mathbf{A}_{k})=\sigma_{1}/\sigma_{k}\) respectively be the condition numbers of \(\mathbf{A}\) and the best rank-\(k\) approximation of \(\mathbf{A}\), and for some small \(\epsilon>0\), let us denote_

\[t=\sqrt{\frac{2\log\left(\frac{32n\mathrm{R}^{2}}{\epsilon}\right)}{m}},\ \ \kappa=\min\left\{\kappa(\mathbf{A}),\frac{\kappa(\mathbf{A}_{k})}{1-\frac{ \sigma_{k+1}}{\sigma_{k}}\left(\frac{\sqrt{\gamma}+1+t}{\sqrt{\gamma}-1-t} \right)}\right\},\] (33)_where \(\gamma=d/m\) is the aspect ratio of the sketching matrix \(\mathbf{S}\in\mathbb{R}^{d\times m}\) with \(S_{ij}\overset{i.i.d.}{\sim}\mathcal{N}\left(0,\frac{1}{m}\right)\). Furthermore, suppose the dynamic range of the quantizer \(\mathrm{Q}\) is set to \(\mathrm{R}\sqrt{\frac{2\log\left(\frac{16\mathrm{R}^{2}n^{2}m}{\epsilon}\right) }{m}}\) as dictated by Lemma E.1, and suppose for some absolute constant \(C\), the bit-budget \(\mathrm{B}\) satisfies,_

\[\mathrm{B}\geq\log_{2}\left(\frac{4C\mathrm{R}}{\max\left\{\sigma_{r},\sigma_{ k}-\sigma_{k+1}\left(\frac{\sqrt{\gamma}+1+t}{\sqrt{\gamma}-1-t}\right) \right\}\log 2}\left(\frac{\sqrt{\gamma}+1+t/\sqrt{2}}{\sqrt{\gamma}-1-t} \right)\sqrt{2\log\left(\frac{16\mathrm{R}^{2}n^{2}m}{\epsilon}\right)}+1 \right).\] (34)

_Then, we have,_

\[\left\|\mathrm{Q}(\mathbf{A}\mathbf{S})^{\dagger}\mathbf{A}\right\|_{\max} \leq\frac{2\kappa}{\sqrt{\gamma}-1-t}\text{ with probability exceeding }1-\frac{\epsilon}{4n \mathrm{R}^{2}}.\] (35)

Proof.: We have the following chain of inequalities:

(36)

where \(\mathrm{(i)}\) and \(\mathrm{(ii)}\) follow from Lemmas B.3 and B.4 respectively. We now need to upper bound \(\left\|\mathrm{Q}\left(\mathbf{A}\mathbf{S}\right)^{\dagger}\right\|_{2}\), which is done as follows:

\[\left\|\mathrm{Q}\left(\mathbf{A}\mathbf{S}\right)^{\dagger}\right\|_{2} \overset{\mathrm{(i)}}{=}\left(\sigma_{\min}\left(\mathrm{Q}\left(\mathbf{A} \mathbf{S}\right)\right)\right)^{-1}=\left(\sigma_{\min}\left(\mathbf{A} \mathbf{S}+\mathbf{E}\right)\right)^{-1}\overset{\mathrm{(ii)}}{\leq}\left( \sigma_{\min}(\mathbf{A}\mathbf{S})-\left\|\mathbf{E}\right\|_{2}\right)^{-1}\] (37)

Here, \(\mathbf{E}=\mathrm{Q}(\mathbf{A}\mathbf{S})-\mathbf{A}\mathbf{S}\in\mathbb{R}^ {n\times m}\) is the quantization error matrix from \(\mathrm{Q}\), \(\mathrm{(i)}\) follows because the singular values of \(\mathrm{Q}\left(\mathbf{A}\mathbf{S}\right)^{\dagger}\) are inverses of the singular values of \(\mathrm{Q}\left(\mathbf{A}\mathbf{S}\right)\) (assuming \(\mathrm{Q}\left(\mathbf{A}\mathbf{S}\right)\) is invertible), and \(\mathrm{(ii)}\) follows from Lemma B.5.

**Lower bounding \(\sigma_{\min}(\mathbf{A}\mathbf{S})\)**: It now suffices to derive the a lower bound on \(\sigma_{\min}(\mathbf{A}\mathbf{S})\), which we do next. We derive two different lower bounds, either of which could be tighter depending on the singular value profile of \(\mathbf{A}\). The final lower bound will be the maximum of both.

For the first lower bound, let \(\mathbf{A}=\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\top}\) be the full singular value decomposition of \(\mathbf{A}\), where \(\mathbf{U}\in\mathbb{R}^{n\times n}\), \(\mathbf{\Sigma}\in\mathbb{R}^{n\times d}\), and \(\mathbf{V}\in\mathbb{R}^{d\times d}\). Then, denoting \(\widetilde{\mathbf{S}}=\mathbf{V}^{\top}\mathbf{S}\), we have,

\[\sigma_{\min}(\mathbf{A}\mathbf{S})=\sigma_{\min}\left(\mathbf{A} _{k}\mathbf{S}+\left(\mathbf{A}-\mathbf{A}_{k}\right)\mathbf{S}\right) \overset{\mathrm{(i)}}{\geq}\sigma_{\min}(\mathbf{A}_{k}\mathbf{S})-\left\| \left(\mathbf{A}-\mathbf{A}_{k}\right)\mathbf{S}\right\|_{2}\] \[\overset{\mathrm{(ii)}}{\geq}\sigma_{\min}(\mathbf{A}_{k}\mathbf{ S})-\sigma_{k+1}\left\|\mathbf{S}\right\|_{2},\] (38)

where \(\mathbf{A}_{k}\) is the best rank-\(k\) approximation of \(\mathbf{A}\). Here, once again, \(\mathrm{(i)}\) follows from Lemma B.5 and \(\mathrm{(ii)}\) follows from Lemma B.4. In order to further lower bound \(\sigma_{\min}(\mathbf{A}_{k}\mathbf{S})\), let us denote the SVD of \(\mathbf{A}_{k}\) as \(\mathbf{A}_{k}=\mathbf{U}\mathbf{\Sigma}_{k}\mathbf{V}_{k}^{\top}\). Since the best rank-\(k\) approximation is obtained by retaining the top-\(k\) singular values of \(\mathbf{A}\) and zeroing out the rest, we have \(\mathbf{U}\in\mathbb{R}^{n\times n}\), \(\mathbf{\Sigma}_{k}\in\mathbb{R}^{n\times k}\) and \(\mathbf{V}_{k}\in\mathbb{R}^{d\times k}\). Let us further denote \(\widetilde{\mathbf{S}}=\mathbf{V}_{k}^{\top}\mathbf{S}\). Then we have

\[\sigma_{\min}\left(\mathbf{A}_{k}\mathbf{S}\right)=\sigma_{\min}\left(\mathbf{ U}\mathbf{\Sigma}_{k}\mathbf{V}_{k}^{\top}\mathbf{S}\right)=\sigma_{\min}\left( \mathbf{U}\mathbf{\Sigma}_{k}\widetilde{\mathbf{S}}\right)\overset{\mathrm{(i)} }{=}\sigma_{\min}\left(\mathbf{\Sigma}_{k}\widetilde{\mathbf{S}}\right) \overset{\mathrm{(ii)}}{\geq}\sigma_{k}\sigma_{\min}(\widetilde{\mathbf{S}}),\] (39)

where \(\mathrm{(i)}\) follows from Lemma B.6, and \(\mathrm{(ii)}\) follows from Lemma B.4. Note that the \((i,j)^{\mathrm{th}}\) entry of \(\widetilde{\mathbf{S}}\) is \(\widetilde{S}_{ij}=\mathbf{v}_{i}^{\top}\mathbf{s}_{j}\), where \(\mathbf{v}_{i}\) is the \(i^{\mathrm{th}}\) column of \(\mathbf{V}_{k}\). Clearly, \(\widetilde{S}_{ij}\) is a Gaussian random variable with mean, \(\mathbb{E}\left[\widetilde{S}_{ij}\right]=\sum_{\ell}V_{\ell i}\)\(\mathbb{E}[S_{\ell j}]=0\), and variance, \(\text{Var}\left(\widetilde{S}_{ij}\right)=\sum_{\ell}\ V_{\ell i}^{2}\text{Var} \left(S_{\ell j}\right)=\frac{1}{m}\sum_{\ell}V_{\ell i}^{2}=\frac{1}{m}\), where the last equality follows from the fact that the columns of \(\mathbf{V}_{k}\) are orthonormal. In other words, the entries of \(\widetilde{\mathbf{S}}\in\mathbb{R}^{d\times m}\) are distributed according to \(\widetilde{S}_{ij}\sim\mathcal{N}\left(0,\frac{1}{m}\right)\). Using (39), (38) can be further lower bounded as

\[\sigma_{\min}(\mathbf{A}\mathbf{S})\geq\sigma_{k}\sigma_{\min}(\widetilde{ \mathbf{S}})-\sigma_{k+1}\left\|\mathbf{S}\right\|_{2}\overset{\mathrm{(i)}}{=} \sigma_{k}\sigma_{\min}(\mathbf{S})-\sigma_{k+1}\left\|\mathbf{S}\right\|_{2},\] (40)

where \(\mathrm{(i)}\) once again follows from the rotation invariance of spectrum, i.e., Lemma B.6.

On the other hand, let \(r=\mathrm{rank}\left(\mathbf{A}\right)\) and \(\sigma_{r}\) by the smallest non-zero singular value of \(\mathbf{A}\). Then, \(\mathbf{A}=\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\top}=\mathbf{U}\mathbf{\Sigma }_{r}\mathbf{V}_{r}^{\top}\) and using the same arguments as in (39), we also have

\[\sigma_{\min}\left(\mathbf{A}\mathbf{S}\right)=\sigma_{\min}\left(\mathbf{U} \mathbf{\Sigma}_{r}\mathbf{V}_{r}^{\top}\mathbf{S}\right)=\sigma_{\min}\left( \mathbf{\Sigma}_{r}\mathbf{V}_{r}^{\top}\mathbf{S}\right)\geq\sigma_{r} \sigma_{\min}\left(\mathbf{S}\right).\] (41)Combining (40) and (41), we get

\[\sigma_{\min}\left(\mathbf{A}\mathbf{S}\right)\geq\max\{\sigma_{r}\sigma_{\min} \left(\mathbf{S}\right),\sigma_{k}\sigma_{\min}(\mathbf{S})-\sigma_{k+1}\left\| \mathbf{S}\right\|_{2}\}\] (42)

All we are left with now is to utilize the concentration bounds on the singular values of \(\mathbf{S}\). As a consequence of Lemma B.10 and recalling \(\gamma=d/m\), with probability exceeding \(1-2e^{-\frac{m\gamma^{2}}{2}}\),

\[\sqrt{\gamma}-1-t\leq\sigma_{\min}\left(\mathbf{S}\right)\leq\left\|\mathbf{S }\right\|_{2}\leq\sqrt{\gamma}+1+t.\] (43)

Substituting this in (42), with probability exceeding \(1-2e^{-\frac{m\gamma^{2}}{2}}\),

\[\sigma_{\min}(\mathbf{A}\mathbf{S})\geq\max\left\{\sigma_{r}\left(\sqrt{\gamma }-1-t\right),\sigma_{k}\left(\sqrt{\gamma}-1-t\right)-\sigma_{k+1}\left(\sqrt {\gamma}+1+t\right)\right\}.\] (44)

**Upper bounding \(\left\|\mathbf{E}\right\|_{2}\)**: Finally, conditioned on the event \(\mathcal{Q}\), the entries of \(\mathbf{E}\) are bounded. Choosing the dynamic range of \(\mathcal{Q}\) as dictated by the upper bound on \(\left\|\mathbf{A}\mathbf{S}\right\|_{\max}\) in Lemma E.1, we have, with probability exceeding \(1-\frac{\epsilon}{8n\mathrm{R}^{2}}\),

\[\left|E_{ij}\right|\leq\Delta=\frac{2\mathrm{R}}{(2^{\mathrm{B}}-1)}\sqrt{ \frac{2\log\left(\frac{16\mathrm{R}^{2}n^{2}m}{\epsilon}\right)}{m}}\text{ \ \ for all }\ i\in[n]\text{ and }j\in[m].\] (45)

Since \(E_{ij}\) is a bounded w.h.p., it is also subgaussian w.h.p. (ref. eq. (16)) with subgaussian norm given by,

\[\left\|E_{ij}\right\|_{\varphi_{2}}\leq\frac{\Delta}{\log 2}=\frac{2\mathrm{R} }{(\log 2)\left(2^{\mathrm{B}}-1\right)}\sqrt{\frac{2\log\left(\frac{16\mathrm{ R}^{2}n^{2}m}{\epsilon}\right)}{m}}.\] (46)

From Lemma B.12 we get for some absolute constant \(C\),

\[\left\|\mathbf{E}\right\|_{2} \leq\frac{2C\mathrm{R}\left(\sqrt{d}+\sqrt{m}+\tilde{t}\right)}{ (2^{\mathrm{B}}-1)\log 2}\sqrt{\frac{2\log\left(\frac{16\mathrm{R}^{2}n^{2}m}{ \epsilon}\right)}{m}}\text{ w.p. exceeding }1-\frac{\epsilon}{8n\mathrm{R}^{2}}-2e^{- \tilde{t}^{2}}\] \[\text{ or, }\left\|\mathbf{E}\right\|_{2} \leq\frac{2C\mathrm{R}\left(\sqrt{\gamma}+1+\tilde{t}\right)}{(2^{ \mathrm{B}}-1)\log 2}\sqrt{2\log\left(\frac{16\mathrm{R}^{2}n^{2}m}{\epsilon} \right)}\text{ w.p. exceeding }1-\frac{\epsilon}{8n\mathrm{R}^{2}}-2e^{-m\tilde{t}^{2}}.\] (47)

**Completing the proof**: Finally, combining (36), (37), (44) and (47), we get with probability exceeding \(1-\frac{\epsilon}{8n\mathrm{R}^{2}}-2e^{-\frac{m\gamma^{2}}{2}}-2e^{-m\tilde{ t}^{2}}\),

\[\left\|\mathrm{Q}(\mathbf{A}\mathbf{S})^{\dagger}\mathbf{A}\right\|_{\max} \leq\sigma_{1}\left[\max\{\sigma_{r}\left(\sqrt{\gamma}-1-t\right),\sigma_{k} \left(\sqrt{\gamma}-1-t\right)-\sigma_{k+1}\left(\sqrt{\gamma}+1+t\right)\}\right.\] (48)

\[\leq\frac{\sigma_{1}}{\sqrt{\gamma}-1-t}\left[\max\left\{\sigma_{r},\sigma_{k }-\sigma_{k+1}\left(\frac{\sqrt{\gamma}+1+t}{\sqrt{\gamma}-1-t}\right)\right\}\right.\]

\[\left.-\frac{2C\mathrm{R}}{(2^{\mathrm{B}}-1)\log 2}\left(\frac{\sqrt{ \gamma}+1+\tilde{t}}{\sqrt{\gamma}-1-t}\right)\sqrt{2\log\left(\frac{16 \mathrm{R}^{2}n^{2}m}{\epsilon}\right)}\right]^{-1}\] \[=\frac{\sigma_{1}}{\sqrt{\gamma}-1-t}\left[\mu-\frac{2C\mathrm{R} }{(2^{\mathrm{B}}-1)\log 2}\left(\frac{\sqrt{\gamma}+1+\tilde{t}}{\sqrt{\gamma}-1-t}\right) \sqrt{2\log\left(\frac{16\mathrm{R}^{2}n^{2}m}{\epsilon}\right)}\right]^{-1},\] (49)

where we denote \(\mu=\max\left\{\sigma_{r},\sigma_{k}-\sigma_{k+1}\left(\frac{\sqrt{\gamma}+1+t }{\sqrt{\gamma}-1-t}\right)\right\}\). Let us choose our bit-budget \(\mathrm{B}\) of quantizer \(\mathrm{Q}\) to be such that it satisfies \(\left\|\mathbf{E}\right\|_{2}\leq\frac{\mu}{2}\), i.e.,

\[\mathrm{B}\geq\log_{2}\left(\frac{4C\mathrm{R}}{\mu\log 2}\left(\frac{\sqrt{ \gamma}+1+\tilde{t}}{\sqrt{\gamma}-1-t}\right)\sqrt{2\log\left(\frac{16\mathrm{ R}^{2}n^{2}m}{\epsilon}\right)}+1\right).\] (50)

Then,

\[\left\|\mathrm{Q}(\mathbf{A}\mathbf{S})^{\dagger}\mathbf{A}\right\|_{\max}\leq \frac{2\sigma_{1}}{\left(\sqrt{\gamma}-1-t\right)\mu}\] (51)Setting

\[t=\sqrt{\frac{2\log\left(\frac{32n\mathrm{R}^{2}}{\epsilon}\right)}{m}}\quad\text{ and,}\quad\tilde{t}=\sqrt{\frac{\log\left(\frac{32n\mathrm{R}^{2}}{\epsilon}\right)}{m}}= \frac{t}{\sqrt{2}},\] (52)

it follows that with probability exceeding \(1-\frac{\epsilon}{4n\mathrm{R}^{2}}\),

\[\left\|\mathrm{Q}(\mathbf{A}\mathbf{S})^{\dagger}\mathbf{A}\right\|_{\max} \leq\frac{2}{\sqrt{\gamma}-1-t}\mathrm{min}\left\{\kappa(\mathbf{A}),\frac{ \kappa(\mathbf{A}_{k})}{1-\frac{\sigma_{k+1}}{\sigma_{k}}\left(\frac{\sqrt{ \gamma}+1+t}{\sqrt{\gamma}-1-t}\right)}\right\}\] (53)

This completes the proof. 

## Appendix F Sketched least squares with quantized response

Since the approximation error guarantees of _sketched least squares with quantized response_ might be a problem of independent interest, this is a standalone section, and the notations used in this section are independent of the rest of the paper.

Consider the generalized least squares problem

\[\mathbf{X}^{\star}=\underset{\mathbf{x}\in\mathbb{R}^{p\times q}}{\arg\min} \left\|\mathbf{\Phi}\mathbf{X}-\mathbf{Y}\right\|_{\mathrm{F}}^{2},\] (54)

where \(\mathbf{\Phi}\in\mathbb{R}^{\ell\times p}\) and \(\mathbf{Y}\in\mathbb{R}^{\ell\times q}\). The sketched variant of (54) with quantized response is given by,

\[\widetilde{\mathbf{X}}=\underset{\mathbf{X}\in\mathbb{R}^{p\times q}}{\arg \min}\left\|\mathbf{G}\mathbf{\Phi}\mathbf{X}-\mathrm{Q}(\mathbf{G}\mathbf{Y} )\right\|_{\mathrm{F}}^{2},\] (55)

where \(\mathbf{G}\in\mathbb{R}^{m\times\ell}\) is a Gaussian sketch matrix with entries are distributed as \(G_{ij}\sim\mathcal{N}\left(0,\frac{1}{m}\right)\), and \(\mathrm{Q}\equiv\mathrm{Q}_{\mathrm{R},\mathrm{B}}\) is the uniformly dithered quantizer as described in (1). We assume that the dynamic range \(\mathrm{R}\geq\left\|\mathbf{G}\mathbf{Y}\right\|_{\max}\) so that \(\mathrm{Q}\) is unsaturated. The solution of (55) can be obtained in closed form as,

\[\widetilde{\mathbf{X}}=(\mathbf{G}\mathbf{\Phi})^{\dagger}\mathrm{Q}( \mathbf{G}\mathbf{Y})\] (56)

The following lemma provides a characterization of the accuracy of \(\widetilde{\mathbf{X}}\) with respect to the original problem (54).

**Lemma F.1**.: _Let \(\mathbf{G}\in\mathbb{R}^{m\times\ell}\) be a random Gaussian matrix with entries distributed as \(G_{ij}\sim\mathcal{N}\left(0,\frac{1}{m}\right)\), and \(\mathrm{Q}\equiv\mathrm{Q}_{\mathrm{R},\mathrm{B}}\) be a uniformly dithered quantizer with dynamic range \(\mathrm{R}\) and bit-budget \(\mathrm{B}\). Furthermore, suppose \(\mathbf{\Phi}\in\mathbb{R}^{\ell\times p}\) and \(\mathbf{Y}\in\mathbb{R}^{\ell\times q}\) be given, and let us denote_

\[\mathbf{X}^{\star}=\underset{\mathbf{X}\in\mathbb{R}^{p\times q}}{\arg\min} \left\|\mathbf{\Phi}\mathbf{X}-\mathbf{Y}\right\|_{\mathrm{F}}^{2}\quad\text{ and}\quad\widetilde{\mathbf{X}}=\underset{\mathbf{X}\in\mathbb{R}^{p\times q}}{\arg\min} \left\|\mathbf{G}\mathbf{\Phi}\mathbf{X}-\mathrm{Q}(\mathbf{G}\mathbf{Y}) \right\|_{\mathrm{F}}^{2}.\]

_Let \(\mathbf{E}=\mathrm{Q}(\mathbf{G}\mathbf{Y})-\mathbf{G}\mathbf{Y}\) be the quantization error matrix. Then, if \(\mathrm{R}\geq\left\|\mathbf{G}\mathbf{Y}\right\|_{\max}\), we have_

\[\left\|\mathbf{\Phi}\mathbf{X}^{\star}-\mathbf{Y}\right\|_{\mathrm{F}}^{2} \leq\mathbb{E}\left\|\mathbf{\Phi}\widetilde{\mathbf{X}}-\mathbf{Y}\right\|_ {\mathrm{F}}^{2}\leq\frac{m-1}{m-r-1}\left\|\mathbf{\Phi}\mathbf{X}^{\star}- \mathbf{Y}\right\|_{\mathrm{F}}^{2}+\frac{q\Delta^{2}}{4}\frac{\sigma_{\max}^ {2}}{\sigma_{\min}^{2}}\frac{m^{2}}{(\ell-m-1)},\]

_where \(r=\mathrm{rank}(\mathbf{\Phi})\), and \(\sigma_{\max}\) and \(\sigma_{\min}\) are the maximum and minimum singular values of \(\mathbf{\Phi}\) respectively._

Proof.: The solution of the generalized least squares problem (54) can be written as:

\[\mathbf{X}^{\star}=[\mathbf{x}_{1}^{\star}\ldots\mathbf{x}_{q}^{\star}], \quad\text{where,}\quad\mathbf{x}_{i}^{\star}=\underset{\mathbf{x}\in\mathbb{R }^{p}}{\arg\min}\left\|\mathbf{\Phi}\mathbf{x}-\mathbf{y}_{i}\right\|^{2},\] (57)

where \(\mathbf{y}_{i}\in\mathbb{R}^{p}\) denote the \(i^{\mathrm{th}}\) column of \(\mathbf{Y}\). Consequently, we will first analyze the standard least squares problem

\[\mathbf{x}^{\star}=\underset{\mathbf{x}\in\mathbb{R}^{p}}{\arg\min}\left\| \mathbf{\Phi}\mathbf{x}-\mathbf{y}\right\|^{2},\] (58)

and generalize the results by concatenating \(\mathbf{x}_{i}^{\star}\) to obtain \(\mathbf{X}^{\star}\). The sketched variant of (58) with quantized response is given by,

\[\widetilde{\mathbf{x}}=\underset{\mathbf{x}\in\mathbb{R}^{p}}{\arg\min}\left\| \mathbf{G}\mathbf{\Phi}\mathbf{x}-\mathrm{Q}(\mathbf{G}\mathbf{y})\right\|^{2}.\] (59)

The solution of (59) is available in closed form as \(\widetilde{\mathbf{x}}=(\mathbf{G}\mathbf{\Phi})^{\dagger}\mathrm{Q}(\mathbf{ G}\mathbf{y})\). Let us denote the quantization error as \(\mathbf{\epsilon}\triangleq\mathrm{Q}(\mathbf{G}\mathbf{y})-\mathbf{G}\mathbf{y} \in\mathbb{R}^{m}\). We then have,

\[\mathbb{E}\|\mathbf{\Phi}\widetilde{\mathbf{x}}-\mathbf{y}\|_{2}^{2}= \mathbb{E}\|\mathbf{\Phi}(\mathbf{G}\mathbf{\Phi})^{\dagger}\mathrm{Q}( \mathbf{G}\mathbf{y})-\mathbf{y}\|_{2}^{2}=\mathbb{E}\|\mathbf{\Phi}(\mathbf{G} \mathbf{\Phi})^{\dagger}\left(\mathbf{G}\mathbf{y}+\mathbf{\epsilon}\right)- \mathbf{y}\|_{2}^{2}\]\[\mathop{\mathbb{E}}\left[\left(\mathbf{E}\mathbf{E}^{\top}\right)_{ij} \right]=\sum_{k=1}^{q}\mathbb{E}\left[E_{ik}E_{jk}\right]=\left\{\begin{array}{cc }q&\mathrm{Var}(E_{ik})\leq\frac{q\Delta^{2}}{4}&\text{ for }i=j\\ 0&\text{ for }i\neq j.\end{array}\right.\] (65)

In other words, \(\mathbb{E}_{\mathrm{Q}}\left[\mathbf{E}\mathbf{E}^{\top}\right]\) is a diagonal matrix whose diagonal elements are upper bounded by \(\frac{q\Delta^{2}}{4}\). Let us denote \(\mathbf{\Lambda}=\left(\left(\mathbf{G}\mathbf{\Phi}\right)^{\dagger}\right)^ {\top}\mathbf{\Phi}^{\top}\mathbf{\Phi}(\mathbf{G}\mathbf{\Phi})^{\dagger}\). Then, (63) simplifies to,

\[\mathbb{E}_{\mathbf{G}}\left[\mathrm{Tr}\left(\mathbf{\Lambda} \cdot\mathbb{E}_{\mathrm{Q}}\left[\mathbf{E}^{\top}\mathbf{E}\right]\right) \right]=\mathbb{E}_{\mathbf{G}}\left[\sum_{i=1}^{m}\Lambda_{ii}\left(\mathbb{ E}_{\mathrm{Q}}\left[\mathbf{E}^{\top}\mathbf{E}\right]\right)_{ii} \right]\leq\frac{q\Delta^{2}}{4}\mathbb{E}_{\mathbf{G}}\left[\mathrm{Tr}( \mathbf{\Lambda})\right].\] (66)

Furthermore,

\[\mathbb{E}_{\mathbf{G}}\left[\mathrm{Tr}\left(\mathbf{\Lambda} \right)\right]=\mathbb{E}_{\mathbf{G}}\left[\left\|\mathbf{\Phi}(\mathbf{G} \mathbf{\Phi})^{\dagger}\right\|_{\mathrm{F}}^{2}\right] \stackrel{{\mathrm{(i)}}}{{\leq}}\mathbb{E}_{\mathbf{G}}\left[ \left\|\left(\mathbf{G}\mathbf{\Phi}\right)^{\dagger}\right\|_{\mathrm{F}}^{2} \right]\sigma_{\max}^{2}\left(\mathbf{\Phi}\right)\] \[=\mathbb{E}_{\mathbf{G}}\left[\mathrm{Tr}\left[\left(\mathbf{G} \mathbf{\Phi}\mathbf{\Phi}^{\top}\mathbf{G}^{\top}\right)^{-1}\right]\right] \sigma_{\max}^{2}\left(\mathbf{\Phi}\right)\] \[\stackrel{{\mathrm{(ii)}}}{{\leq}}\frac{\sigma_{\max }^{2}(\mathbf{\Phi})}{\sigma_{\min}^{2}(\mathbf{\Phi})}\ \mathrm{Tr}\left(\mathbb{E}\left[\left(\mathbf{G}\mathbf{G}^{\top}\right)^{- 1}\right]\right)\] \[\stackrel{{\mathrm{(iii)}}}{{=}}\frac{\sigma_{\max}^{2 }}{\sigma_{\min}^{2}}\frac{m^{2}}{(\ell-m-1)}.\] (67)

Here, \(\mathrm{(i)}\) follows from Lemma B.1, \(\mathrm{(ii)}\) is consequence of Lemma B.2, and \(\mathrm{(iii)}\) follows from Lemma B.9. Combining (63), (66) and (67) yields,

\[\mathbb{E}\left\|\mathbf{\Phi}(\mathbf{G}\mathbf{\Phi})^{\dagger}\mathbf{E} \right\|_{\mathrm{F}}^{2}\leq\frac{q\Delta^{2}}{4}\frac{\sigma_{\max}^{2}}{ \sigma_{\min}^{2}}\frac{m^{2}}{(\ell-m-1)}.\] (68)

This completes the proof. 
LPLR algorithm: Approximation error analysis

To prove Thm. 3.2, we will first prove the result when both quantizers \(\mathrm{Q}\) and \(\mathrm{Q}^{\prime}\) are unsaturated. We first prove Lemma G.1, which gives us an upper bound on the approximation error when \(\mathrm{Q}\) and \(\mathrm{Q}^{\prime}\) are unsaturated. Since we can ensure that they remain unsaturated with high probability (with appropriate choices of dynamic ranges), the final approximation error upper bound in Thm. 3.2 is slightly worse than Lemma G.1.

**Lemma G.1**.: _Let our matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\) have non-zero singular values \(\sigma_{1},\ldots,\sigma_{k},\sigma_{k+1},\ldots,\sigma_{r}\), where \(r=\mathrm{rank}(\mathbf{A})\), and bounded row norms \(\left\|\mathbf{a}^{(i)}\right\|\leq\mathrm{R}\). Let \(\kappa(\mathbf{A})=\sigma_{1}/\sigma_{r}\) and \(\kappa(\mathbf{A}_{k})=\sigma_{1}/\sigma_{k}\) respectively be the condition numbers of \(\mathbf{A}\) and the best rank-\(k\) approximation of \(\mathbf{A}\), and let us denote_

\[t=\sqrt{\frac{2\log\left(\frac{32n\mathrm{R}^{2}}{\epsilon}\right)}{m}},\ \ \kappa=\min\left\{\kappa(\mathbf{A}),\frac{\kappa(\mathbf{A}_{k})}{1-\frac{ \sigma_{k+1}}{\sigma_{k}}\left(\frac{\sqrt{\gamma}+1+t}{\sqrt{\gamma}-1-t} \right)}\right\},\]

_for some sufficiently small \(\epsilon\) that satisfies \(0<\epsilon\leq\frac{4n\mathrm{R}^{2}\kappa(\mathbf{A})}{\gamma(\mathbf{A})}\). Here \(\gamma=d/m\) is the aspect ratio of the sketching matrix \(\mathbf{S}\in\mathbb{R}^{d\times m}\) with \(S_{ij}\overset{i.i.d.}{\longrightarrow}\mathcal{N}\left(0,\frac{1}{m}\right)\). Suppose the dynamic ranges of the quantizers \(\mathrm{Q}\) and \(\mathrm{Q}^{\prime}\) are set to \(\mathrm{R}\sqrt{\frac{2\log\left(\frac{168\mathrm{R}^{2}n^{2}m}{\epsilon} \right)}{m}}\) and \(\frac{2\kappa}{\sqrt{\gamma}-1-t}\) as dictated by Lemmas E.1 and E.2 respectively, and suppose their bit-budgets \(\mathrm{B}\) and \(\mathrm{B}^{\prime}\) satisfy_

\[\mathrm{B}\geq\max\{\mathrm{B}_{1},\mathrm{B}_{2}\}\ \ \text{ and, }\ \ \mathrm{B}^{\prime}\geq\log_{2}\left(\frac{4\mathrm{R}\kappa}{\left(\sqrt{ \gamma}-1-t\right)}\sqrt{\frac{nd}{\epsilon}}+1\right),\]

_where,_

\[\mathrm{B}_{1}=\log_{2}\left(\frac{2\mathrm{R}\kappa(\mathbf{A}_{k})\sqrt{2n} }{\sqrt{\epsilon\left(\gamma-1-\frac{1}{m}\right)}}\sqrt{\log\left(\frac{16 \mathrm{R}^{2}n^{2}m}{\epsilon}\right)}+1\right),\]

_and \(\mathrm{B}_{2}\) is equal to_

_Furthermore, let \(\mathcal{E}\) be the event that both quantizers \(\mathrm{Q}\) and \(\mathrm{Q}^{\prime}\) are unsaturated. Then,_

\[\mathbb{E}\left[\left\|\mathrm{Q}(\mathbf{A}\mathbf{S})\mathrm{Q}^{\prime} \left(\mathrm{Q}(\mathbf{A}\mathbf{S})^{\dagger}\mathbf{A}\right)-\mathbf{A} \right\|_{\mathrm{F}}^{2}\ \mathds{1}_{\mathcal{E}}\right]\leq\left(1+\frac{k}{m-k-1}\right)\left\| \mathbf{A}_{k}-\mathbf{A}\right\|_{\mathrm{F}}^{2}+\frac{3\epsilon}{4},\]

_where \(\mathds{1}_{(\cdot)}\) is the indicator function._

Proof.: Let us denote the quantization error matrices from \(\mathrm{Q}\) and \(\mathrm{Q}^{\prime}\) as \(\mathbf{E}\in\mathbb{R}^{n\times m}\) and \(\mathbf{E}^{\prime}\in\mathbb{R}^{m\times d}\) respectively, i.e., \(\mathbf{E}=\mathrm{Q}(\mathbf{A}\mathbf{S})-\mathbf{A}\mathbf{S}\) and \(\mathbf{E}^{\prime}=\mathrm{Q}^{\prime}\left(\mathrm{Q}(\mathbf{A}\mathbf{S}) ^{\dagger}\mathbf{A}\right)-\mathrm{Q}(\mathbf{A}\mathbf{S})^{\dagger}\mathbf{A}\). Since \(\mathds{1}_{\mathcal{E}}=1\) implies that \(\mathrm{Q}\) and \(\mathrm{Q}^{\prime}\) are unsaturated, \(\mathbb{E}[\mathbf{E}]=\mathbf{0}\) and \(\mathbb{E}[\mathbf{E}^{\prime}]=\mathbf{0}\). Then,

\[\mathbb{E}\left[\left\|\mathrm{Q}(\mathbf{A}\mathbf{S})\mathrm{Q}^{\prime} \left(\mathrm{Q}(\mathbf{A}\mathbf{S})^{\dagger}\mathbf{A}\right)-\mathbf{A} \right\|_{\mathrm{F}}^{2}\ \mathds{1}_{\mathcal{E}}\right]=\mathbb{E}_{\mathrm{Q},\mathrm{Q}^{\prime}} \left[\left\|\mathrm{Q}(\mathbf{A}\mathbf{S})\mathrm{Q}^{\prime}\left( \mathrm{Q}(\mathbf{A}\mathbf{S})^{\dagger}\mathbf{A}\right)-\mathbf{A}\right\| _{\mathrm{F}}^{2}\right]\mathds{1}_{\mathcal{E}}\]

\[= \ \ \mathbb{E}_{\mathrm{Q},\mathrm{Q}^{\prime}}\left[\left\| \mathrm{Q}(\mathbf{A}\mathbf{S})\mathrm{Q}(\mathbf{A}\mathbf{S})^{\dagger} \mathbf{A}+\mathbf{E}^{\prime}\right)-\mathbf{A}\right\|_{\mathrm{F}}^{2}\ \mathds{1}_{\mathcal{E}}\] \[\overset{\mathrm{(i)}}{=} \underbrace{\mathbb{E}_{\mathrm{Q}}\left[\left\|\mathrm{Q}(\mathbf{A }\mathbf{S})\mathrm{Q}(\mathbf{A}\mathbf{S})^{\dagger}\mathbf{A}-\mathbf{A} \right\|_{\mathrm{F}}^{2}\right]\mathds{1}_{\mathcal{E}}}_{\Upsilon_{1}}+ \underbrace{\mathbb{E}_{\mathrm{Q},\mathrm{Q}^{\prime}}\left[\left\|\mathrm{Q}( \mathbf{A}\mathbf{S})\mathbf{E}^{\prime}\right\|_{\mathrm{F}}^{2}\right] \mathds{1}_{\mathcal{E}}}_{\Upsilon_{2}}\] (69)

The cross terms disappear in \(\mathrm{(i)}\) because,

\[\mathbb{E}_{\mathrm{Q},\mathrm{Q}^{\prime}}\left[\mathrm{Tr}\left[ \left(\mathrm{Q}(\mathbf{A}\mathbf{S})\mathrm{Q}(\mathbf{A}\mathbf{S})^{ \dagger}\mathbf{A}-\mathbf{A}\right)^{\top}\mathrm{Q}(\mathbf{A}\mathbf{S}) \mathbf{E}^{\prime}\right]\right]\mathds{1}_{\mathcal{E}}\] \[= \ \mathrm{Tr}\left(\mathbb{E}_{\mathrm{Q}}\left[\left(\mathrm{Q}( \mathbf{A}\mathbf{S})\mathrm{Q}(\mathbf{A}\mathbf{S})^{\dagger}\mathbf{A}- \mathbf{A}\right)^{\top}\mathrm{Q}(\mathbf{A}\mathbf{S})\ \mathbb{E}_{\mathrm{Q}^{\prime}}[\mathbf{E}^{\prime}]\right]\right)\mathds{1}_{ \mathcal{E}}\overset{\mathrm{(ii)}}{=}0,\] (70)

where, \(\mathrm{(ii)}\) follows from \(\mathbb{E}_{\mathrm{Q}^{\prime}}[\mathbf{E}^{\prime}]=\mathbf{0}\) as the quantizer \(\mathrm{Q}^{\prime}\) is unbiased when unsaturated.

**Analyzing the term \(\mathrm{T}_{1}\)**: Recall that \(\mathbf{A}_{k}\) is the best rank-\(k\) approximation of \(\mathbf{A}\) obtained using computing the full-SVD of \(\mathbf{A}\) and subsequently making all singular values of \(\mathbf{A}\) less than \(\sigma_{k}\) as \(0\). We next analyze the first term in (69) as

\[\|\mathrm{Q}(\mathbf{A}\mathbf{S})\mathrm{Q}(\mathbf{A}\mathbf{S})^ {\dagger}\mathbf{A}-\mathbf{A}\|_{\mathrm{F}}^{2} \leq\|\mathrm{Q}(\mathbf{A}\mathbf{S})(\mathbf{A}_{k}\mathbf{S})^ {\dagger}\mathbf{A}_{k}-\mathbf{A}\|_{\mathrm{F}}^{2}\] \[=\|\mathbf{A}_{k}^{\top}(\mathbf{S}^{\top}\mathbf{A}_{k}^{\top}) ^{\dagger}\mathrm{Q}(\mathbf{S}^{\top}\mathbf{A}^{\top})-\mathbf{A}^{\top}\|_ {\mathrm{F}}^{2}\] \[=\|\mathbf{A}_{k}^{\top}\widetilde{\mathbf{X}}-\mathbf{A}^{\top} \|_{\mathrm{F}}^{2},\] (71)

where \(\widetilde{\mathbf{X}}\triangleq\arg\min_{\mathbf{X}}\|\mathbf{S}^{\top} \mathbf{A}_{k}^{\top}\mathbf{X}-\mathrm{Q}(\mathbf{S}^{\top}\mathbf{A}^{\top} )\|_{\mathrm{F}}^{2}\). This minimization problem is a Gaussian sketched variant of a generalized least squares problem with the response matrix quantized as seen in App. F. Corresponding to the notations in App. F, here we have \(\mathbf{S}^{\top}\) instead of \(\mathbf{G}\), \(\mathbf{A}_{k}^{\top}\) instead of \(\mathbf{\Phi}\), and \(\mathbf{A}^{\top}\) instead of \(\mathbf{Y}\). As a consequence of Lemma F.1, we have the upper bound,

\[\mathbb{E}_{\mathrm{Q}}\left[\|\mathrm{Q}(\mathbf{A}\mathbf{S})\mathrm{Q}( \mathbf{A}\mathbf{S})^{\dagger}\mathbf{A}-\mathbf{A}\|_{\mathrm{F}}^{2}\right] \mathds{1}_{\mathcal{E}}\leq\frac{m-1}{m-k-1}\|\mathbf{A}_{k}-\mathbf{A}\|_{ \mathrm{F}}^{2}+\frac{n\Delta^{2}}{4}\frac{\sigma_{1}^{2}}{\sigma_{k}^{2}} \frac{m^{2}}{(d-m-1)}\] (72)

**Analyzing the term \(\mathrm{T}_{2}\)**: The term \(\mathrm{T}_{2}\) can be written as,

\[\mathbb{E}_{\mathrm{Q},\mathrm{Q}^{\prime}}\left[\|\mathrm{Q}( \mathbf{A}\mathbf{S})\mathbf{E}^{\prime}\|_{\mathrm{F}}^{2}\right]\mathds{1}_{ \mathcal{E}}\] \[=\] \[\stackrel{{\mathrm{(i)}}}{{=}} \mathbb{E}\left[\|\mathbf{A}\mathbf{S}\mathbf{E}^{\prime}\|_{ \mathrm{F}}^{2}\right]\mathds{1}_{\mathcal{E}}+\mathbb{E}\left[\|\mathbf{E} \mathbf{E}^{\prime}\|_{\mathrm{F}}^{2}\right]\mathds{1}_{\mathcal{E}},\] (73)

where, \(\mathrm{(i)}\) follows once again \(\mathbb{E}_{\mathrm{Q}^{\prime}}[\mathbf{E}^{\prime}]\mathds{1}_{\mathcal{E }}=\mathbf{0}\). The first term in (73) can be rewritten as

\[\mathbb{E}\left[\|\mathbf{A}\mathbf{S}\mathbf{E}^{\prime}\|_{ \mathrm{F}}^{2}\,\mathds{1}_{\mathcal{E}}\right] =\mathbb{E}\left[\mathrm{Tr}\left(\mathbf{E}^{\prime\top}\mathbf{S}^{ \top}\mathbf{A}^{\top}\mathbf{A}\mathbf{S}\mathbf{E}^{\prime}\right)\mathds{1 }_{\mathcal{E}}\right]\] \[=\mathbb{E}\left[\mathrm{Tr}\left(\mathbf{S}^{\top}\mathbf{A}^{ \top}\mathbf{A}\mathbf{S}\mathbf{E}^{\prime}\left[\mathbf{E}^{\prime} \mathbf{E}^{\prime\top}\right]\mathds{1}_{\mathcal{E}}\right]\right)\] \[=\mathbb{E}_{\mathbf{S}}\left[\mathrm{Tr}\left(\mathbf{S}^{\top} \mathbf{A}^{\top}\mathbf{A}\mathbf{S}\ \mathbb{E}_{\mathrm{Q}^{\prime}}\left[\mathbf{E}^{\prime}\mathbf{E}^{\prime \top}\right]\mathds{1}_{\mathcal{E}}\right)\right]\] (74)

Similar to (65) in the proof of Lemma F.1, \(\mathbb{E}_{\mathrm{Q}^{\prime}}\left[\mathbf{E}^{\prime}\mathbf{E}^{\prime \top}\right]\mathds{1}_{\mathcal{E}}\) is a diagonal matrix with

\[\mathbb{E}_{\mathrm{Q}^{\prime}}\left[\left(\mathbf{E}^{\prime} \mathbf{E}^{\prime\top}\right)_{ij}\right]\mathds{1}_{\mathcal{E}}=\sum_{k=1}^ {d}\mathbb{E}_{\mathrm{Q}^{\prime}}\left[E_{ik}^{\prime}E_{jk}^{\prime}\right] \mathds{1}_{\mathcal{E}}=\begin{cases}d\cdot\mathrm{Var}(E_{ik})\leq\frac{d \Delta^{\prime 2}}{4}&\text{ for }i=j\\ 0&\text{ for }i\neq j.\end{cases}\] (75)

Let us denote \(\boldsymbol{\Gamma}\triangleq\mathbf{S}^{\top}\mathbf{A}^{\top}\mathbf{A} \mathbf{S}\). Then using the fact that \(\mathds{1}_{\mathcal{E}}\leq 1\), we get,

\[\mathbb{E}_{\mathbf{S}}\left[\mathrm{Tr}\left(\boldsymbol{\Gamma }\ \mathbb{E}_{\mathrm{Q}^{\prime}}\left[\mathbf{E}^{\prime}\mathbf{E}^{\prime\top} \right]\mathds{1}_{\mathcal{E}}\right)\right] =\mathbb{E}_{\mathbf{S}}\left[\sum_{i=1}^{m}\Gamma_{ii}\left( \mathbb{E}_{\mathrm{Q}^{\prime}}\left[\mathbf{E}^{\prime}\mathbf{E}^{\prime \top}\right]\right)_{ii}\right]\mathds{1}_{\mathcal{E}}\] \[\leq\frac{d\Delta^{\prime 2}}{4}\ \mathrm{Tr}\left(\mathbb{E}_{ \mathbf{S}}\boldsymbol{\Gamma}\right)\] \[=\frac{d\Delta^{\prime 2}}{4}\ \mathrm{Tr}\left(\mathbf{A}^{\top} \mathbf{A}\mathbb{E}_{\mathbf{S}}\left[\mathbf{S}\mathbf{S}^{\top}\right] \right)\stackrel{{\mathrm{(i)}}}{{=}}\frac{d\Delta^{\prime 2}}{4}\left\|\mathbf{A}\right\|_{ \mathrm{F}}^{2}.\] (76)

Here, the last equality follows as \(\mathbb{E}_{\mathbf{S}}\left[\mathbf{S}\mathbf{S}^{\top}\right]=\mathds{1}_{d}\). This is because the \((i,j)^{\mathrm{th}}\) entry of \(\mathbb{E}_{\mathbf{S}}\left[\mathbf{S}\mathbf{S}^{\top}\right]\) is

\[\mathbb{E}\left[\left(\mathbf{S}\mathbf{S}^{\top}\right)_{ij}\right]=\sum_{k=1}^ {m}\mathbb{E}\left[S_{ik}S_{jk}\right]=\begin{cases}m\,\mathbb{E}\left[S_{ik}^{2} \right]=m\ \mathrm{Var}\left(S_{ik}\right)=1&\text{ for }\quad i=j\\ \sum_{i=1}^{m}\mathbb{E}[S_{ik}]\mathbb{E}[S_{jk}]=0&\text{ for }\quad i\neq j.\end{cases}\]

Furthermore, the second term \(\mathbb{E}_{\mathrm{Q},\mathrm{Q}^{\prime}}\left\|\mathbf{E}\mathbf{E}^{\prime \top}\right\|_{\mathrm{F}}^{2}\mathds{1}_{\mathcal{E}}\) can be simplified as follows,

\[\mathbb{E}_{\mathrm{Q},\mathrm{Q}^{\prime}}\left\|\mathbf{E} \mathbf{E}^{\prime}\right\|_{\mathrm{F}}^{2}\mathds{1}_{\mathcal{E}} =\mathbb{E}_{\mathrm{Q},\mathrm{Q}^{\prime}}\left[\mathrm{Tr}\left(\mathbf{E}^{ \prime\top}\mathbf{E}^{\top}\mathbf{E}^{\top}\mathbf{E}\mathbf{E}^{\prime}\right) \right]\mathds{1}_{\mathcal{E}}=\mathbb{E}_{\mathrm{Q}}\left[\mathrm{Tr}\left( \mathbf{E}^{\top}\mathbf{E}\ \mathbb{E}_{\mathrm{Q}^{\prime}}\left(\mathbf{E}^{\prime}\mathbf{E}^{\prime \top}\right)\right)\right]\mathds{1}_{\mathcal{E}}\] \[\stackrel{{\mathrm{(i)}}}{{=}}\mathbb{E}_{\mathrm{Q}} \left[\sum_{i=1}^{m}\left(\mathbf{E}^{\top}\mathbf{E}\right)_{ii}\left( \mathbb{E}_{\mathrm{Q}^{\prime}}\left[\mathbf{E}^{\prime}\mathbf{E}^{\prime\top} \right]\right)_{ii}\right]\mathds{1}_{\mathcal{E}}\]\[\stackrel{{\rm(ii)}}{{\leq}}\frac{d\Delta^{\prime 2}}{4}\sum_{i=1}^{m} \mathbb{E}_{\rm Q}\left[\left(\mathbf{E}^{\top}\mathbf{E}\right)_{ii}\right] \mathds{1}_{\mathcal{E}}\] \[\leq\frac{nmd\Delta^{2}\Delta^{\prime 2}}{16}.\] (77)

Here, \(\rm(i)\) follows since \(\mathbb{E}_{\rm Q^{\prime}}\left[\mathbf{E}^{\prime}\mathbf{E}^{\prime\top} \right]\mathds{1}_{\mathcal{E}}\) is a diagonal matrix as seen before, and \(\rm(ii)\) follows from the upper bound derived on \(\mathbb{E}\left[\left(\mathbf{E}^{\top}\mathbf{E}\right)_{ii}\right]\mathds{1 }_{\mathcal{E}}\) in (65).

Combining all of the above, the approximation error of our low-precision low-rank approximation scheme can be upper bounded as,

\[\mathbb{E}\|\mathrm{Q}(\mathbf{A}\mathbf{S})Q^{\prime}\left( \mathrm{Q}(\mathbf{A}\mathbf{S})^{\dagger}\mathbf{A}\right)-\mathbf{A}\|_{\rm F }^{2}\] \[\leq\underbrace{\frac{(m-1)}{(m-k-1)}\left\|\mathbf{A}_{k}- \mathbf{A}\right\|_{\rm F}^{2}}_{\rm T_{3}}+\underbrace{\frac{n\Delta^{2}}{4 }\frac{\sigma_{1}^{2}}{\sigma_{k}^{2}}\frac{m^{2}}{(d-m-1)}}_{\rm T_{4}}+ \underbrace{\frac{d\Delta^{\prime 2}}{4}\left\|\mathbf{A}\right\|_{\rm F }^{2}}_{\rm T_{5}}+\underbrace{\frac{nmd\Delta^{2}\Delta^{\prime 2}}{16}}_{\rm T_{6}}.\] (78)

Here, the first term \(\rm T_{3}\) is the low-rank approximation error, whereas the remaining terms, i.e., \(\rm T_{4},\rm T_{5}\) and \(\rm T_{6}\) appear due to quantization. We now analyze each of them separately.

**Analyzing the term \(\rm T_{4}\)**: Since we choose the dynamic range of quantizer \(\rm Q\) to be \(\mathrm{R}\sqrt{\frac{2\log\left(\frac{16\mathrm{R}^{2}n^{2}m}{c}\right)}{m}}\) (ref. Lemma E.1), we have

\[\frac{n\Delta^{2}}{4}\frac{\sigma_{1}^{2}}{\sigma_{k}^{2}}\frac{m^{2}}{(d-m-1) }\leq\frac{2nm\mathrm{R}^{2}\kappa(\mathbf{A}_{k})^{2}}{\left(2^{\rm B}-1 \right)^{2}(d-m-1)}\log\left(\frac{16\mathrm{R}^{2}n^{2}m}{\epsilon}\right) \stackrel{{\rm(i)}}{{\leq}}\frac{\epsilon}{4}.\] (79)

Here, we can ensure \(\rm(i)\) holds true, i.e., that this term does not exceed \(\epsilon/4\) if we set the bit-budget \(\rm B\) to satisfy,

\[\rm B\geq\log_{2}\left(\frac{2\mathrm{R}\kappa(\mathbf{A}_{k})\sqrt{2n}}{ \sqrt{\epsilon\left(\gamma-1-\frac{1}{m}\right)}}\sqrt{\log\left(\frac{16 \mathrm{R}^{2}n^{2}m}{\epsilon}\right)}+1\right).\] (80)

**Analyzing the term \(\rm T_{5}\)**: Since we choose the dynamic range of the quantizer \(\rm Q^{\prime}\) to be equal to \(\frac{2\kappa}{\sqrt{\gamma}-1-t}\) (ref. to Lemma E.2), where \(t\) and \(\kappa\) are defined as in (33), we have

\[\frac{d\Delta^{\prime 2}}{4}\left\|\mathbf{A}\right\|_{\rm F}^{2}\leq\frac{nd \mathrm{R}^{2}}{\left(2^{\rm B^{\prime}}-1\right)^{2}}\frac{4\kappa^{2}}{ \left(\sqrt{\gamma}-1-t\right)^{2}}\stackrel{{\rm(i)}}{{\leq}} \frac{\epsilon}{4},\] (81)

where we have made use of Asm. 3.1. Once again, in order to ensure \(\rm(i)\), it suffices to choose the bit-budget \(\rm B^{\prime}\) to be

\[\rm B^{\prime}\geq\log_{2}\left(\frac{4\mathrm{R}\kappa}{\left(\sqrt{\gamma}- 1-t\right)}\sqrt{\frac{nd}{\epsilon}}+1\right)\] (82)

Note that in order for the dynamic range of quantizer \(\rm Q^{\prime}\) to hold true in Lemma E.2, we also require \(\rm B\) to satisfy (34), i.e.,

\[\rm B\geq\log_{2}\left(\frac{4CR}{\max\left\{\sigma_{r},\sigma_{k}-\sigma_{k+ 1}\left(\frac{\sqrt{\gamma}+1+t}{\sqrt{\gamma}-1-t}\right)\right\}\log 2}\left(\frac{ \sqrt{\gamma}+1+t/\sqrt{2}}{\sqrt{\gamma}-1-t}\right)\sqrt{2\log\left(\frac{16 \mathrm{R}^{2}n^{2}m}{\epsilon}\right)}+1\right).\] (83)

**Analyzing the term \(\rm T_{6}\)**: Using bit-budgets \(\rm B\) and \(\rm B^{\prime}\) as in (80) and (82), the final term is

\[nmd\frac{\Delta^{2}}{4}\frac{\Delta^{\prime 2}}{4}\leq nmd\frac{\epsilon}{4} \left(\frac{\gamma-1-\frac{1}{m}}{nm\kappa(\mathbf{A}_{k})}\right)\frac{ \epsilon}{4nd\mathrm{R}^{2}}\leq\frac{\epsilon^{2}\gamma}{16n\mathrm{R}^{2} \kappa(\mathbf{A}_{k})^{2}}\stackrel{{\rm(i)}}{{\leq}}\frac{ \epsilon}{4}\] (84)

Here, \(\rm(i)\) is ensured by choosing \(\epsilon\) to be sufficiently small - specifically, \(\epsilon\leq\frac{4n\mathrm{R}^{2}\kappa(\mathbf{A})^{2}}{\gamma}\).

**Tying it all together**: Combining (79), (81), and (84), with the bit-budgets set appropriately as dictated by (80), (83), and (82), eq. (78) simplifies to

\[\mathbb{E}\|\mathrm{Q}(\mathbf{A}\mathbf{S})\mathrm{Q}^{\prime}\left(\mathrm{Q} (\mathbf{A}\mathbf{S})^{\dagger}\mathbf{A}\right)-\mathbf{A}\|_{\mathrm{F}}^ {2}\leq\left(1+\frac{k}{m-k-1}\right)\left\|\mathbf{A}_{k}-\mathbf{A}\right\| _{\mathrm{F}}^{2}+\frac{3\epsilon}{4}.\] (85)

This completes the proof.

### LPLR approximation error: Proof of Thm. 3.2

We now formally state our main approximation result.

**Theorem G.2**.: **(LPLR approximation error (formal))** _Let our matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\) have non-zero singular values \(\sigma_{1},\dots,\sigma_{k},\sigma_{k+1},\dots,\sigma_{r}\), where \(r=\mathrm{rank}(\mathbf{A})\), and bounded row norms \(\left\|\mathbf{a}^{(i)}\right\|\leq\mathrm{R}\). Let \(\kappa(\mathbf{A})=\sigma_{1}/\sigma_{r}\) and \(\kappa(\mathbf{A}_{k})=\sigma_{1}/\sigma_{k}\) respectively be the condition numbers of \(\mathbf{A}\) and the best rank-\(k\) approximation of \(\mathbf{A}\), and let us denote_

\[t=\sqrt{\frac{2\log\left(\frac{32n\mathrm{R}^{2}}{\epsilon}\right)}{m}},\ \ \kappa=\min\left\{\kappa(\mathbf{A}),\frac{\kappa(\mathbf{A}_{k})}{1-\frac{ \sigma_{k+1}}{\sigma_{k}}\left(\frac{\sqrt{r}+1+t}{\sqrt{\gamma}-1-t}\right)} \right\},\]

_for some sufficiently small \(\epsilon\) that satisfies \(0<\epsilon\leq\frac{4n\mathrm{R}^{2}\kappa(\mathbf{A})^{2}}{\gamma}\). Here \(\gamma=d/m\) is the aspect ratio of the sketching matrix \(\mathbf{S}\in\mathbb{R}^{d\times m}\) with \(S_{ij}\overset{i.i.d.}{\sim}\mathcal{N}\left(0,\frac{1}{m}\right)\). Suppose the dynamic ranges of the quantizers \(\mathrm{Q}\) and \(\mathrm{Q}^{\prime}\) are set to \(\mathrm{R}\sqrt{\frac{2\log\left(\frac{16\mathrm{R}^{2}n^{2}m}{\epsilon}\right) }{m}}\) and \(\frac{2\kappa}{\sqrt{\gamma}-1-t}\) respectively, and suppose their bit-budgets \(\mathrm{B}\) and \(\mathrm{B}^{\prime}\) satisfy_

\[\mathrm{B}\geq\max\{\mathrm{B}_{1},\mathrm{B}_{2}\}\ \ \text{ and, }\ \ \mathrm{B}^{\prime}\geq\log_{2}\left(\frac{4\mathrm{R}\kappa}{\left(\sqrt{ \gamma}-1-t\right)}\sqrt{\frac{nd}{\epsilon}}+1\right),\]

_where,_

\[\mathrm{B}_{1}=\log_{2}\left(\frac{2\mathrm{R}\kappa(\mathbf{A}_{k})\sqrt{2n} }{\sqrt{\epsilon\left(\gamma-1-\frac{1}{m}\right)}}\sqrt{\log\left(\frac{16 \mathrm{R}^{2}n^{2}m}{\epsilon}\right)}+1\right),\]

_and \(\mathrm{B}_{2}\) is equal to_

\[\log_{2}\left(\frac{4\mathrm{CR}}{\max\left\{\sigma_{r},\sigma_{k}-\sigma_{k +1}\left(\frac{\sqrt{\gamma}+1+t}{\sqrt{\gamma}-1-t}\right)\right\}\log 2}\left(\frac{\sqrt{ \gamma}+1+t/\sqrt{2}}{\sqrt{\gamma}-1-t}\right)\sqrt{2\log\left(\frac{16 \mathrm{R}^{2}n^{2}m}{\epsilon}\right)}+1\right).\]

_Then, the low-precision and low-rank factorization returned by Alg. 1 satisfies_

\[\mathbb{E}\left\|\mathbf{L}\mathbf{R}-\mathbf{A}\right\|_{\mathrm{F}}^{2}\leq \left(1+\frac{k}{m-k-1}\right)\left\|\mathbf{A}_{k}-\mathbf{A}\right\|_{ \mathrm{F}}^{2}+\epsilon,\] (86)

_where the expectation is over the random sketching matrix \(\mathbf{S}\), as well as the inherent stochasticity from quantizers \(\mathrm{Q}\) and \(\mathrm{Q}^{\prime}\)._

Proof.: For the purpose of analysis, we assume that Alg. 1 returns \(\mathbf{L}=\mathbf{0}\) and \(\mathbf{R}=\mathbf{0}\) if either quantizer \(\mathrm{Q}\) or \(\mathrm{Q}^{\prime}\) gets saturated. In practical implementation, it can easily be checked if either quantizer \(\mathrm{Q}\) or \(\mathrm{Q}^{\prime}\) gets saturated or not, and the algorithm can be repeated again with a new realization of the sketching matrix \(\mathbf{S}\) and stochastic quantizer \(\mathrm{Q}\). Since the choice of dynamic ranges for \(\mathrm{Q}\) and \(\mathrm{Q}^{\prime}\) ensures that they remain unsaturated with a high probability, "reasonably few" realizations of \(\mathbf{S}\) would suffice to get at least one good realization in which \(\mathrm{Q}\) and \(\mathrm{Q}^{\prime}\) are unsaturated.

However, in what follows, we assume that that if either quantizer \(\mathrm{Q}\) or \(\mathrm{Q}^{\prime}\) gets saturated, then the algorithm returns \(\mathbf{0}\) as an estimate of \(\mathbf{A}\), resulting in a Frobenius norm error of \(\left\|\mathbf{A}\right\|_{\mathrm{F}}\). Since this happen with a very small probability, we show that its contribution to the expected Frobenius norm error of Alg. 1 is small as well. With this in mind, the expected approximation error can be written as\[\stackrel{{\rm(i)}}{{\leq}}\mathbb{E}\left[\left\|\mathrm{Q}\left( \mathbf{A}\mathbf{S}\right)\mathrm{Q}^{\prime}\left(\mathrm{Q}\left(\mathbf{A} \mathbf{S}\right)^{\dagger}\mathbf{A}\right)-\mathbf{A}\right\|_{\mathrm{F}}^{2 }\mathds{1}_{\mathcal{E}}\right]+n\mathrm{R}^{2}\Pr\left(\mathds{1}_{ \mathcal{E}^{C}}\right).\] (87)

Inequality \(\mathrm{(i)}\) follows from Asm. 3.1 and the fact that the expectation of indicator function of an event is the probability of the event. From Lemma G.1, the first term can be upper bounded as:

\[\mathbb{E}\left[\left\|\mathrm{Q}(\mathbf{A}\mathbf{S})\mathrm{Q}^{\prime} \left(\mathrm{Q}(\mathbf{A}\mathbf{S})^{\dagger}\mathbf{A}\right)-\mathbf{A} \right\|_{\mathrm{F}}^{2}\,\mathds{1}_{\mathcal{E}}\right]\leq\left(1+\frac{k }{m-k-1}\right)\left\|\mathbf{A}_{k}-\mathbf{A}\right\|_{\mathrm{F}}^{2}+ \frac{3\epsilon}{4}.\] (88)

Since Lemmas E.1 and E.2 state the probabilities of quantizers \(\mathrm{Q}\) and \(\mathrm{Q}^{\prime}\) being unsaturated, \(\Pr\left(\mathds{1}_{\mathcal{E}^{C}}\right)\) can be obtained by an application of union bound as

\[\Pr\left(\mathds{1}_{\mathcal{E}^{C}}\right)\leq\frac{\epsilon}{8n\mathrm{R}^ {2}}+\frac{\epsilon}{8n\mathrm{R}^{2}}=\frac{\epsilon}{4n\mathrm{R}^{2}}.\] (89)

Then, (87) can be written as:

\[\mathbb{E}\left\|\mathbf{L}\mathbf{R}-\mathbf{A}\right\|_{\mathrm{F}}^{2}\leq \left(1+\frac{k}{m-k-1}\right)\left\|\mathbf{A}_{k}-\mathbf{A}\right\|_{ \mathrm{F}}^{2}+\epsilon.\] (90)

This completes the proof. 

### Informal version of Thm. G.2

From Thm. G.2, we can get a (simplified) asymptotic dependence of the bit-budgets \(\mathrm{B}\) and \(\mathrm{B}^{\prime}\). We have \(\mathrm{B}\geq\max\{\mathrm{B}_{1},\mathrm{B}_{2}\}\),

\[\mathrm{B}_{1} =\log_{2}\left(\frac{2\mathrm{R}\kappa(\mathbf{A}_{k})\sqrt{2n}} {\sqrt{\epsilon\left(\gamma-1-\frac{1}{m}\right)}}\sqrt{\log\left(\frac{16 \mathrm{R}^{2}n^{2}m}{\epsilon}\right)}+1\right)\] \[\stackrel{{\rm(i)}}{{=}}\frac{1}{2}\log_{2}\left( \frac{\kappa(\mathbf{A}_{k})^{2}nm}{\epsilon\left(d-m-1\right)}\log\left(\frac {mn^{2}}{\epsilon}\right)\right)\] \[\stackrel{{\rm(ii)}}{{=}}\frac{1}{2}\log_{2}\left( \frac{\kappa(\mathbf{A}_{k})^{2}}{\epsilon}\frac{nm}{d}\log\left(\frac{mn^{2}} {\epsilon}\right)\right),\] (91)

where we have ignored the constant terms inside the \(\log_{2}(\cdot)\) in \(\mathrm{(i)}\) and considered the regime \(m\ll d\) in \(\mathrm{(ii)}\). Furthermore, \(\mathrm{B}_{2}\) is

\[\mathrm{B}_{2} =\log_{2}\left(\frac{4C\mathrm{R}}{\max\left\{\sigma_{r},\sigma _{k}-\sigma_{k+1}\left(\frac{\sqrt{\gamma}+1+t}{\sqrt{\gamma}-1-t}\right) \right\}\log 2}\left(\frac{\sqrt{\gamma}+1+t/\sqrt{2}}{\sqrt{\gamma}-1-t}\right) \sqrt{2\log\left(\frac{16\mathrm{R}^{2}n^{2}m}{\epsilon}\right)}+1\right)\] \[\stackrel{{\rm(i)}}{{=}}\frac{1}{2}\log_{2}\left( \frac{1}{\left(\max\left\{\sigma_{r},\sigma_{k}-\sigma_{k+1}c^{\prime}\right\} \right)^{2}}\log\left(\frac{mn^{2}}{\epsilon}\right)\right),\] (92)

where \(c^{\prime}\) is approximately a constant. Here, \(\mathrm{(i)}\) follows because when \(m\ll d\), we have

\[\frac{\sqrt{\gamma}+1+t}{\sqrt{\gamma}-1-t}=\frac{\sqrt{d}+\sqrt{m}+\mathrm{O} \left(\sqrt{\log\left(\frac{n}{\epsilon}\right)}\right)}{\sqrt{d}-\sqrt{m}- \mathrm{O}\left(\sqrt{\log\left(\frac{n}{\epsilon}\right)}\right)}=\mathrm{O}( 1).\] (93)

Similarly, \(\frac{\sqrt{\gamma}+1+t/\sqrt{2}}{\sqrt{\gamma}-1-t}=\mathrm{O}(1)\). Comparing (91) and (92), we have

\[\mathrm{B}\geq\frac{1}{2}\log_{2}\left(\frac{\kappa(\mathbf{A}_{k})^{2}}{ \epsilon}\frac{nm}{d}\log\left(\frac{mn^{2}}{\epsilon}\right)\right).\] (94)

Moreover, the bit-budget \(\mathrm{B}^{\prime}\) is

\[\mathrm{B}^{\prime} \geq\log_{2}\left(\frac{4\mathrm{R}\kappa}{\left(\sqrt{\gamma}-1 -t\right)}\sqrt{\frac{nd}{\epsilon}}+1\right)\] \[\stackrel{{\rm(i)}}{{=}}\log_{2}\left(\frac{\kappa \sqrt{ndm}}{\sqrt{\epsilon}\left(\sqrt{d}-\sqrt{m}-\sqrt{\frac{n}{\epsilon}} \right)}\right)\stackrel{{\rm(ii)}}{{=}}\frac{1}{2}\log_{2}\left( \kappa^{2}\frac{nm}{\epsilon}\right),\] (95)

where once again, to get \(\mathrm{(i)}\) we have ignored the constants, and \(\mathrm{(ii)}\) holds true when \(m\ll d\). Moreover, suppose \(n\approx d\) (if they are not equal, then the total bit-budget should be computed as a weighted average, since the \(\mathbf{L}\) has \(nm\) elements and the second low-rank factor has \(md\) elements). Then, for some constant \(c_{3}\) that depends on \(\mathrm{R}\), the total bit-budget is

\[\mathrm{B}\geq\frac{1}{4}\log_{2}\left(\frac{c_{3}^{2}\kappa(\mathbf{A}_{k})^{2 }\kappa^{2}}{\epsilon^{2}}\frac{n^{2}m^{2}}{d}\log\left(\frac{mn^{2}}{\epsilon }\right)\right) =\frac{1}{2}\log_{2}\left(\frac{c_{3}\kappa(\mathbf{A}_{k})\kappa}{ \epsilon}\frac{nm}{\sqrt{d}}\sqrt{\log\left(\frac{mn^{2}}{\epsilon}\right)} \right).\]

Furthermore, the dynamic range of quantizer \(\mathrm{Q}\) is \(c_{1}\sqrt{\frac{\log\left(\frac{n}{\epsilon}\right)}{m}}\) and that of quantizer \(\mathrm{Q}^{\prime}\) when \(m\ll d\) is

\[\frac{2\kappa}{\sqrt{\gamma}-1-t}=2\sqrt{\frac{m}{d}}\min\left\{\kappa( \mathbf{A}),\frac{\kappa(\mathbf{A}_{k})}{1-c_{4}\frac{\sigma_{k+1}}{\sigma _{k}}}\right\},\]

for constants \(c_{1}\), \(c_{2}\) and \(c_{4}\) that depend on \(\mathrm{R}\), where additive logarithmic terms are ignored with respect to \(m\). This gives us the result in Thm. 3.2.

## Appendix H Direct-SVD Quantization: Approximation error analysis

In this section, we consider the baseline scheme for obtaining low-precision low-rank factorization by first computing the optimal low-rank factorization using SVD and individually quantizing the low-rank factors with uniform scalar quantizers. For any given matrix, \(\mathbf{A}\in\mathbb{R}^{n\times d}\), we compute the full SVD as \(\mathbf{A}=\mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^{\top}\). The best (unquantized) rank-\(k\) approximation can be obtained from this by considering the singular vectors corresponding to the top-\(k\) singular vectors, and constructing the matrix \(\mathbf{A}_{k}=\left(\mathbf{U}\boldsymbol{\Sigma}\right)_{k}\mathbf{V}_{k}^{\top}\). Here, \(\left(\mathbf{U}\boldsymbol{\Sigma}\right)_{k}\in\mathbb{R}^{n\times k}\) is the sub-matrix obtained by selecting the first \(k\) columns of \(\mathbf{U}\boldsymbol{\Sigma}\), and \(\mathbf{V}_{k}^{\top}\in\mathbb{R}^{k\times d}\) is the sub-matrix obtained by selecting the first \(k\) rows of \(\mathbf{V}^{\top}\). Subsequently, these low-rank factors are quantized with uniform scalar quantizers \(\mathrm{Q}\) and \(\mathrm{Q}^{\prime}\) (having bit-budgets \(\mathrm{B}\) and \(\mathrm{B}^{\prime}\) respectively). The algorithm pseudocode is given in Alg. 2 and we provide an upper bound to the approximation error in Proposition H.1. We use the notation \(\widetilde{\mathbf{U}}=\mathbf{U}\boldsymbol{\Sigma}\in\mathbb{R}^{n\times d}\).

``` Input :Matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\), target rank \(k\), Quantizers \(\mathrm{Q}\) and \(\mathrm{Q}^{\prime}\) Output :Factorization: \(\mathbf{L}\mathbf{R}\) where \(\mathbf{L}\in\mathbb{R}^{n\times k}\), \(\mathbf{R}\in\mathbb{R}^{k\times d}\)
1 Compute SVD and get \(\mathbf{A}=\mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^{\top}\)
2 Extract the top-\(k\) left (scaled) and right singular vectors and get \(\widetilde{\mathbf{U}}_{k}=\left(\mathbf{U}\boldsymbol{\Sigma}\right)_{k}\) and \(\mathbf{V}_{k}^{\top}\)
3 Quantize the factors individually and get \(\mathbf{L}\leftarrow\mathrm{Q}(\widetilde{\mathbf{U}}_{k})\) and \(\mathbf{R}\leftarrow\mathrm{Q}^{\prime}\left(\mathbf{V}_{k}\right)\) return\(\mathbf{L}\in\mathbb{R}^{n\times k},\mathbf{R}\in\mathbb{R}^{k\times d}\) ```

**Proposition H.1**.: **(Direct-SVD quant. approximation error (formal))** _Let our matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\) have maximum singular value \(\sigma_{1}\). Suppose our target rank is \(k\), and for some small \(\epsilon\) such that \(0<\epsilon\leq 3k\sigma_{1}^{2}\), the dynamic ranges of quantizers \(\mathrm{Q}\) and \(\mathrm{Q}^{\prime}\) are set to be \(\sigma_{1}\) and \(1\) respectively with bit-budgets satisfying_

\[\mathrm{B}\geq\log_{2}\left(\sigma_{1}\sqrt{\frac{3nk}{\epsilon}}+1\right) \quad\text{ and, }\quad\mathrm{B}^{\prime}\geq\log_{2}\left(\sigma_{1}\sqrt{\frac{3dk}{ \epsilon}}+1\right).\]

_Then, the factorization returned by direct-SVD quantization in Alg. 2 satisfies_

\[\mathbb{E}\left\|\mathbf{L}\mathbf{R}-\mathbf{A}\right\|_{\mathrm{F}}^{2}\leq \left\|\mathbf{A}_{k}-\mathbf{A}\right\|_{\mathrm{F}}^{2}+\epsilon,\]

_where the expectation is over the inherent stochasticity of the quantizers \(\mathrm{Q}\) and \(\mathrm{Q}^{\prime}\)._

Proof.: Let us denote the quantization error matrices as \(\mathbf{E}=\mathrm{Q}(\widetilde{\mathbf{U}}_{k})-\widetilde{\mathbf{U}}_{k}\) and \(\mathbf{E}^{\prime}=\mathrm{Q}^{\prime}(\mathbf{V}_{k}^{\top})-\mathbf{V}_{k} ^{\top}\). The approximation error is given by

\[\mathbb{E}\left\|\mathrm{Q}(\widetilde{\mathbf{U}}_{k})\mathrm{Q}^{ \prime}\left(\mathbf{V}_{k}^{\top}\right)-\mathbf{A}\right\|_{\mathrm{F}}^{2} =\mathbb{E}\left\|\mathrm{Q}(\widetilde{\mathbf{U}}_{k})\left( \mathbf{V}_{k}^{\top}+\mathbf{E}^{\prime}\right)-\mathbf{A}\right\|_{\mathrm{F}}^ {2}\] \[\stackrel{{\mathrm{(i)}}}{{=}}\underbrace{\mathbb{E} \left\|\mathrm{Q}(\widetilde{\mathbf{U}}_{k})\mathbf{V}_{k}^{\top}-\mathbf{A} \right\|_{\mathrm{F}}^{2}}_{\Upsilon_{1}}+\underbrace{\mathbb{E}\left\| \mathrm{Q}(\widetilde{\mathbf{U}}_{k})\mathbf{E}^{\prime}\right\|_{\mathrm{F}}^ {2}}_{\Upsilon_{2}},\] (96)where the cross term disappears in \((\mathrm{i})\) because when \(\mathrm{Q}^{\prime}\) is unsaturated, \(\mathbb{E}_{\mathrm{Q}^{\prime}}[\mathbf{E}^{\prime}]=\mathbf{0}\), and

\[\mathbb{E}_{\mathrm{Q}^{\prime}}\left[\mathrm{Tr}\left(\left(\mathrm{Q}( \widetilde{\mathbf{U}}_{k})\mathbf{V}_{k}^{\top}-\mathbf{A}\right)^{\top} \mathrm{Q}(\widetilde{\mathbf{U}}_{k})\ \mathbb{E}_{\mathrm{Q}^{\prime}}[\mathbf{E}^{\prime}]\right) \right]=0.\]

**Analyzing term \(\mathrm{T}_{1}\)**: The first term in (96) can be upper bounded as

\[\mathbb{E}\left\|\mathrm{Q}(\widetilde{\mathbf{U}}_{k})\mathbf{V}_{k}^{\top}- \mathbf{A}\right\|_{\mathrm{F}}^{2}=\mathbb{E}\left\|\left(\widetilde{\mathbf{ U}}_{k}+\mathbf{E}\right)\mathbf{V}_{k}^{\top}-\mathbf{A}\right\|_{\mathrm{F}}^{2} \overset{\mathrm{(i)}}{=}\left\|\mathbf{A}_{k}-\mathbf{A}\right\|_{\mathrm{F} }^{2}+\mathbb{E}\left\|\mathbf{E}\mathbf{V}_{k}^{\top}\right\|_{\mathrm{F}}^{2}.\] (97)

Here, in \((\mathrm{i})\), we utilize the fact that \(\mathbf{A}_{k}=\widetilde{\mathbf{U}}_{k}\mathbf{V}_{k}^{\top}\) and the cross term vanishes once again as quantizer \(\mathrm{Q}\) is unsaturated, i.e., \(\mathbb{E}[\mathbf{E}]=\mathbf{0}\) and,

\[\mathrm{Tr}\left(\left(\widetilde{\mathbf{U}}_{k}\mathbf{V}_{k}^{\top}- \mathbf{A}\right)^{\top}\mathbb{E}_{\mathrm{Q}}[\mathbf{E}]\mathbf{V}_{k}^{ \top}\right)=0.\]

The second term in (97) is

\[\mathbb{E}\left\|\mathbf{E}\mathbf{V}_{k}^{\top}\right\|_{\mathrm{F}}^{2}= \mathbb{E}\left[\mathrm{Tr}\left(\mathbf{E}\mathbf{V}_{k}^{\top}\mathbf{V}_{k} \mathbf{E}^{\top}\right)\right]\overset{\mathrm{(i)}}{=}\mathrm{Tr}\left( \mathbb{E}[\mathbf{E}^{\top}\mathbf{E}]\right)=\sum_{i=1}^{k}\left(\mathbb{E}[ \mathbf{E}^{\top}\mathbf{E}]\right)_{ii}\overset{\mathrm{(ii)}}{\leq}nk\frac{ \Delta^{2}}{4}.\] (98)

Here, \((\mathrm{i})\) follows because \(\mathbf{V}_{k}^{\top}\mathbf{V}_{k}=\mathbf{I}_{k}\), and \((\mathrm{ii})\) follows because \(\mathbb{E}[\mathbf{E}^{\top}\mathbf{E}]\) is a diagonal matrix as

\[\left(\mathbb{E}[\mathbf{E}^{\top}\mathbf{E}]\right)_{ij}=\sum_{\ell=1}^{n} \mathbb{E}[E_{\ell i}E_{\ell j}]=\begin{cases}n\operatorname{Var}\left(E_{ \ell i}^{2}\right)\leq\frac{n\Delta^{2}}{4}&\text{for }i=j,\\ \sum_{\ell=1}^{n}\mathbb{E}[E_{\ell i}]\mathbb{E}[E_{\ell j}]=0&\text{for }i \neq j.\end{cases}\] (99)

So, (97) can be upper bounded as

\[\mathbb{E}\left\|\mathrm{Q}(\widetilde{\mathbf{U}}_{k})\mathbf{V}_{k}^{\top} -\mathbf{A}\right\|_{\mathrm{F}}^{2}\leq\left\|\mathbf{A}_{k}-\mathbf{A} \right\|_{\mathrm{F}}^{2}+nk\frac{\Delta^{2}}{4}.\] (100)

**Analyzing term \(\mathrm{T}_{2}\)**: We upper bound the second term in (96) as

\[\mathbb{E}\left\|\mathrm{Q}(\widetilde{\mathbf{U}}_{k})\mathbf{E}^{\prime} \right\|_{\mathrm{F}}^{2}=\mathbb{E}\left\|\left(\widetilde{\mathbf{U}}_{k}+ \mathbf{E}\right)\mathbf{E}^{\prime}\right\|_{\mathrm{F}}^{2}\overset{\mathrm{ (i)}}{=}\mathbb{E}\left\|\widetilde{\mathbf{U}}_{k}\mathbf{E}^{\prime}\right\| _{\mathrm{F}}^{2}+\mathbb{E}\left\|\mathbf{E}\mathbf{E}^{\prime}\right\|_{ \mathrm{F}}^{2},\] (101)

where the cross term vanishes in \((\mathrm{i})\) because \(\mathbb{E}_{\mathrm{Q}^{\prime}}\left[\mathrm{Tr}\left((\widetilde{\mathbf{U}}_ {k}\mathbf{E}^{\prime})^{\top}\ \mathbb{E}_{\mathrm{Q}}[\mathbf{E}]\ \mathbf{E}^{\prime}\right)\right]=0\). The first term in (101) is

\[\mathbb{E}\left\|\widetilde{\mathbf{U}}_{k}\mathbf{E}^{\prime} \right\|_{\mathrm{F}}^{2} =\mathbb{E}\left[\mathrm{Tr}\left(\mathbf{E}^{\prime\top}\widetilde {\mathbf{U}}_{k}^{\top}\widetilde{\mathbf{U}}_{k}\mathbf{E}^{\prime}\right)\right]\] \[=\mathbb{E}\left[\mathrm{Tr}\left(\widetilde{\mathbf{U}}_{k}^{ \top}\widetilde{\mathbf{U}}_{k}\mathbf{E}^{\prime}{\mathbf{E}^{\prime}}^{ \prime}\right)\right]\] \[\overset{\mathrm{(i)}}{=}\sum_{i=1}^{k}\left(\widetilde{\mathbf{ U}}_{k}^{\top}\widetilde{\mathbf{U}}_{k}\right)_{ii}\left(\mathbb{E}[\mathbf{E}^{ \prime}{\mathbf{E}^{\prime}}^{\top}]\right)_{ii}\] \[\overset{\mathrm{(ii)}}{\leq}d\frac{\Delta^{\prime 2}}{4}\sum_{i=1}^{k} \left(\widetilde{\mathbf{U}}_{k}^{\top}\widetilde{\mathbf{U}}_{k}\right)_{ii} \overset{\mathrm{(iii)}}{\leq}d\frac{\Delta^{\prime 2}}{4}\sum_{i=1}^{k}\sigma_{i}^{2}\leq dk \sigma_{1}^{2}\frac{\Delta^{\prime 2}}{4}.\] (102)

Here, \((\mathrm{i})\) and \((\mathrm{ii})\) follow because \(\mathbb{E}[\mathbf{E}^{\prime}{\mathbf{E}^{\prime}}^{\top}]\) is a diagonal matrix following the same argument as (99), i.e.,

\[\left(\mathbb{E}[\mathbf{E}^{\prime}{\mathbf{E}^{\prime}}^{\top}]\right)_{ij}= \sum_{\ell=1}^{n}\mathbb{E}[E_{\ell i}^{\prime}E_{j\ell}^{\prime}]=\begin{cases} d\operatorname{Var}\left(E_{i\ell}^{\prime 2}\right)\leq\frac{d\Delta^{\prime 2}}{4}&\text{for }i=j,\\ \sum_{\ell=1}^{d}\mathbb{E}[E_{\ell i\ell}^{\prime}]\mathbb{E}[E_{j\ell}^{\prime}]= 0&\text{for }i\neq j.\end{cases}\] (103)

Finally, \((\mathrm{iii})\) follows because the \(i^{\mathrm{th}}\) column of \(\widetilde{\mathbf{U}}_{k}\) is \(\sigma_{i}\mathbf{u}_{i}\), so \(\left(\widetilde{\mathbf{U}}_{k}^{\top}\widetilde{\mathbf{U}}_{k}\right)_{ii}= \left(\sigma_{i}\mathbf{u}_{i}\right)^{\top}\left(\sigma_{i}\mathbf{u}_{i} \right)=\sigma_{i}^{2}\left\|\mathbf{u}_{i}\right\|_{2}^{2}=\sigma_{i}^{2}\).

The second term in (101) is

\[\mathbb{E}\left\|\mathbf{E}\mathbf{E}^{\prime}\right\|_{\mathrm{F}}^{2} =\mathbb{E}\left[\mathrm{Tr}\left(\mathbf{E}^{\prime\top}\mathbf{E}^{\top} \mathbf{E}\mathbf{E}^{\prime}\right)\right]\] \[=\mathbb{E}_{\mathrm{Q}}\left[\mathrm{Tr}\left(\mathbf{E}^{\top} \mathbf{E}\ \mathbb{E}_{\mathrm{Q}^{\prime}}\left[\mathbf{E}^{\prime}{\mathbf{E}^{\prime}}^{ \top}\right]\right)\right]\] \[\overset{\mathrm{(i)}}{=}\mathbb{E}_{\mathrm{Q}}\left[\sum_{i=1}^{k} \left(\mathbf{E}^{\top}\mathbf{E}\right)_{ii}\left(\mathbb{E}_{\mathrm{Q}^{ \prime}}[\mathbf{E}^{\prime}{\mathbf{E}^{\prime}}^{\top}]\right)_{ii}\right]\]\[\leq\frac{d\Delta^{\prime 2}}{4}\sum_{i=1}^{k}\mathbb{E}_{\mathrm{Q}}\left[ \left(\mathbf{E}^{\top}\mathbf{E}\right)_{ii}\right]\overset{\mathrm{(ii)}}{ \leq}k\frac{n\Delta^{2}}{4}\frac{d\Delta^{\prime 2}}{4}.\] (104)

Here, \(\mathrm{(i)}\) and \(\mathrm{(ii)}\) follow because \(\mathbb{E}[\mathbf{E}^{\prime}\mathbf{E}^{\prime\top}]\) and \(\mathbb{E}[\mathbf{E}^{\top}\mathbf{E}]\) are diagonal, which can be seen using similar arguments as in (99). So, (101) is

\[\mathbb{E}\left\|\mathrm{Q}(\widetilde{\mathbf{U}}_{k})\mathrm{Q}^{\prime} \left(\mathbf{V}_{k}^{\top}\right)-\mathbf{A}\right\|_{\mathrm{F}}^{2}\leq \left\|\mathbf{A}_{k}-\mathbf{A}\right\|_{\mathrm{F}}^{2}+nk\frac{\Delta^{2}} {4}+dk\sigma_{1}^{2}\frac{\Delta^{\prime 2}}{4}+ndk\frac{\Delta^{2}}{4}\frac{ \Delta^{\prime 2}}{4}.\] (105)

**Choice of dynamic range for quantizers \(\mathrm{Q}\) and \(\mathrm{Q}^{\prime}\)**: Using Lemma B.3, the max-norm of the input to the first quantizer can be upper bounded as \(\|\widetilde{\mathbf{U}}_{k}\|_{\mathrm{max}}\leq\|\widetilde{\mathbf{U}}_{k} \|_{2}=\sigma_{1}\), and that of the second quantizer is \(\|\mathbf{V}_{k}\|_{\mathrm{max}}\leq\left\|\mathbf{V}_{k}\right\|_{2}=1\). Therefore, choosing the dynamic ranges of \(\mathrm{Q}\) and \(\mathrm{Q}^{\prime}\) to be \(\sigma_{1}\) and \(1\) respectively, would ensure that they remain unsaturated, and (105) can be rewritten as

\[\mathbb{E}\left\|\mathrm{Q}(\widetilde{\mathbf{U}}_{k})\mathrm{Q}^{\prime} \left(\mathbf{V}_{k}^{\top}\right)-\mathbf{A}\right\|_{\mathrm{F}}^{2}\leq \left\|\mathbf{A}_{k}-\mathbf{A}\right\|_{\mathrm{F}}^{2}+\frac{nk\sigma_{1} ^{2}}{\left(2^{\mathrm{B}^{\prime}}-1\right)^{2}}+\frac{dk\sigma_{1}^{2}}{ \left(2^{\mathrm{B}^{\prime}}-1\right)^{2}}+k\frac{n\Delta^{2}}{4}\frac{d \Delta^{\prime 2}}{4}.\] (106)

**Choice of bit-budgets \(\mathrm{B}\)** and \(\mathrm{B}^{\prime}\)**: For a given \(\epsilon>0\), suppose we choose \(\mathrm{B}\) and \(\mathrm{B}^{\prime}\) so that

\[\mathrm{B}\geq\log_{2}\left(\sigma_{1}\sqrt{\frac{3nk}{\epsilon}}+1\right) \quad\text{ and, }\quad\mathrm{B}^{\prime}\geq\log_{2}\left(\sigma_{1}\sqrt{\frac{3dk}{ \epsilon}}+1\right).\]

With such a choice, the second and third terms in (106) would not exceed \(\epsilon\). Furthermore, the last term of (106) will be

\[k\frac{n\Delta^{2}}{4}\frac{d\Delta^{\prime 2}}{4}\leq dnk\frac{\epsilon}{3nk} \frac{\epsilon}{3dk\sigma_{1}^{2}}\leq\frac{\epsilon}{3}\quad\text{ whenever }\epsilon<3k\sigma_{1}^{2}.\]

This completes the proof. 

Prop. H.1 states that when \(n\approx d\), in order to guarantee an \(\epsilon\)-additive error approximation with respect to the best rank-\(k\) approximation, we require (ignoring constant multiplicative factors inside the \(\log_{2}(\cdot)\)), a total budget (ref. to Tab. 1) of

\[\frac{n}{n+d}\mathrm{B}+\frac{d}{n+d}\mathrm{B}^{\prime}\approx\frac{1}{2} \left(\mathrm{B}+\mathrm{B}^{\prime}\right)=\frac{1}{2}\log_{2}\left(\frac{3 k\sigma_{1}^{2}}{\epsilon}\sqrt{nd}\right)\ \ \text{bits}.\] (107)

## Appendix I LPLR-SVD algorithm: Approximation error analysis

LPLR-SVD is a simpler variant of our LPLR algorithm in which instead of approximating the range space of \(\mathbf{A}\) using \(\mathbf{AS}\), we compute the full-SVD of \(\mathbf{A}\in\mathbb{R}^{n\times d}\) as \(\mathbf{A}=\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\top}\), where \(\widetilde{\mathbf{U}}\in\mathbb{R}^{n\times n}\), \(\mathbf{\Sigma}\in\mathbb{R}^{n\times d}\), and \(\mathbf{V}\in\mathbb{R}^{d\times d}\). The pseudocode of LPLR-SVD is provided in Alg. 3.

We first compute the SVD, \(\mathbf{A}=\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\top}\), followed by quantizing the singular vectors corresponding to the top-\(k\) singular values, scaled by the singular values. We denote \(\widetilde{\mathbf{U}}=\mathbf{U}\mathbf{\Sigma}\), and the sub-matrix formed by the first \(k\) columns as \(\widetilde{\mathbf{U}}_{k}\). We quantize this basis to get \(\mathbf{L}=\mathrm{Q}(\widetilde{\mathbf{U}}_{k})\) as the first low-rank factor. Subsequently, we project the columns of \(\mathbf{A}\) onto the subspace spanned by this quantized basis, i.e., we solve the optimization problem

\[\min_{\mathbf{W}\in\mathbb{R}^{k\times d}}\left\|\mathrm{Q}(\widetilde{\mathbf{ U}}_{k})\mathbf{W}-\mathbf{A}\right\|_{\mathrm{F}}^{2}\] (108)

The solution of this is given by \(\mathbf{W}^{*}=\mathrm{Q}(\widetilde{\mathbf{U}}_{k})^{\dagger}\mathbf{A}\). The second low rank factor is given by quantizing it again as \(\mathbf{R}=\mathrm{Q}^{\prime}\left(\mathrm{Q}(\widetilde{\mathbf{U}}_{k})^{ \dagger}\mathbf{A}\right)\).

The following result provides an upper bound on the approximation error of LPLR-SVD. The proof of this result is similar to Thm. 3.2 without the complications arising from Gaussian concentration.

**Theorem I.1**.: **(LPLR-SVD approximation error (formal))** _Let our matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\) have singular values \(\sigma_{1}\geq\ldots\sigma_{k}\geq\ldots\), and bounded row norms \(\left\|\mathbf{a}^{(i)}\right\|\leq\mathrm{R}\). Let \(\kappa(\mathbf{A}_{k})=\sigma_{1}/\sigma_{k}\) be the condition number of the \(\mathbf{A}_{k}\), i.e., the best rank-\(k\) approximation of \(\mathbf{A}\). Consider some sufficiently small \(\epsilon\) that satisfies \(0<\epsilon<4k\sigma_{1}^{2}\), and suppose the dynamic ranges of quantizers \(\mathrm{Q}\) and \(\mathrm{Q}^{\prime}\) are set to be \(\sigma_{1}\) and \(2\kappa(\mathbf{A}_{k})\) respectively. Furthermore, suppose the bit-budgets of the quantizers satisfy_

\[\mathrm{B}\geq\max\,\left\{\mathrm{B}_{1},\mathrm{B}_{2}\right\}\quad\text{ and,}\quad\mathrm{B}^{\prime}\geq\log_{2}\left(4\sigma_{1}\kappa(\mathbf{A}_{k})\sqrt{ \frac{dk}{\epsilon}}+1\right),\]

_where,_

\[\mathrm{B}_{1}=\log_{2}\left(2\sigma_{1}\sqrt{\frac{nk}{\epsilon}}+1\right),\]

_and,_

\[\mathrm{B}_{2}=\log_{2}\left(\frac{4C\kappa(\mathbf{A}_{k})}{\log 2}\left( \sqrt{n}+\sqrt{k}+\sqrt{\log\left(\frac{8n\mathrm{R}^{2}}{\epsilon}\right)} \right)+1\right).\]

_Then, the low-precision and low-rank factorization returned by Alg. 3 satisfies_

\[\mathbb{E}\left\|\mathbf{L}\mathbf{R}-\mathbf{A}\right\|_{\mathrm{F}}^{2}\leq \left\|\mathbf{A}_{k}-\mathbf{A}\right\|_{\mathrm{F}}^{2}+\epsilon.\]

Proof.: Since the solution of \(\mathbf{W}^{*}=\arg\min_{\mathbf{W}}\left\|\mathrm{Q}(\widetilde{\mathbf{U}}_ {k})\mathbf{W}-\mathbf{A}\right\|_{\mathrm{F}}^{2}\) is available in closed form as \(\mathbf{W}^{*}=\mathrm{Q}(\widetilde{\mathbf{U}}_{k})^{\dagger}\mathbf{A}\), we have the low rank factorization as \(\mathbf{A}\approx\mathrm{Q}(\widetilde{\mathbf{U}}_{k})\mathrm{Q}^{\prime} \left(\mathrm{Q}(\widetilde{\mathbf{U}}_{k})^{\dagger}\mathbf{A}\right)\). Let us denote the quantization error matrices as \(\mathbf{E}^{\prime}=\mathrm{Q}^{\prime}\left(\mathrm{Q}(\widetilde{\mathbf{U}} _{k})^{\dagger}\mathbf{A}\right)-\mathrm{Q}(\widetilde{\mathbf{U}}_{k})^{ \dagger}\mathbf{A}\), and \(\mathbf{E}=\mathrm{Q}(\widetilde{\mathbf{U}}_{k})-\widetilde{\mathbf{U}}_{k}\). Then, the approximation error is

\[\mathbb{E}\left\|\mathrm{Q}(\widetilde{\mathbf{U}}_{k})\mathrm{ Q}^{\prime}\left(\mathrm{Q}(\widetilde{\mathbf{U}}_{k})^{\dagger}\mathbf{A} \right)-\mathbf{A}\right\|_{\mathrm{F}}^{2} =\mathbb{E}\left\|\mathrm{Q}(\widetilde{\mathbf{U}}_{k})\left( \mathrm{Q}(\widetilde{\mathbf{U}}_{k})^{\dagger}\mathbf{A}+\mathbf{E}^{\prime }\right)-\mathbf{A}\right\|_{\mathrm{F}}^{2}\] \[=\underbrace{\mathbb{E}\left\|\mathrm{Q}(\widetilde{\mathbf{U}}_ {k})\mathrm{Q}(\widetilde{\mathbf{U}}_{k})^{\dagger}\mathbf{A}-\mathbf{A} \right\|_{\mathrm{F}}^{2}}_{\widetilde{\mathrm{T}}_{1}}+\underbrace{\mathbb{E }\left\|\mathrm{Q}(\widetilde{\mathbf{U}}_{k})\mathbf{E}^{\prime}\right\|_{ \mathrm{F}}^{2}}_{\widetilde{\mathrm{T}}_{2}},\] (109)

where the last equality follows the because the quantizer \(Q^{\prime}\) is unbiased. Here, the \(\mathbb{E}\) is over the stochasticity of the quantizers. This decomposition is similar to (69), and can be treated similarly. The term \(\mathrm{T}_{1}\) consists of the low rank approximation error and the error from the first quantizer, whereas the term \(\mathrm{T}_{2}\) consists of error from the second quantizer. We follow steps similar to the proof of Lemma G.1 which are detailed below.

**Analyzing term \(\mathrm{T}_{1}\)**: Using similar reasoning as (71), this term can be written as

\[\mathbb{E}\left\|\mathrm{Q}(\widetilde{\mathbf{U}}_{k})\mathrm{ Q}(\widetilde{\mathbf{U}}_{k})^{\dagger}\mathbf{A}-\mathbf{A}\right\|_{ \mathrm{F}}^{2}\leq\mathbb{E}\left\|\mathrm{Q}(\widetilde{\mathbf{U}}_{k}) \widetilde{\mathbf{U}}_{k}^{\dagger}\mathbf{A}_{k}-\mathbf{A}\right\|_{ \mathrm{F}}^{2}=\mathbb{E}\left\|\mathrm{Q}(\widetilde{\mathbf{U}}_{k}) \mathbf{V}_{k}^{\top}-\mathbf{A}\right\|_{\mathrm{F}}^{2}\] (110)

This is the same as (100) in the analysis of direct-SVD quant., and hence can be upper bounded as

\[\mathbb{E}\left\|\mathrm{Q}(\widetilde{\mathbf{U}}_{k})\mathrm{ Q}(\widetilde{\mathbf{U}}_{k})^{\dagger}\mathbf{A}-\mathbf{A}\right\|_{ \mathrm{F}}^{2}\leq\left\|\mathbf{A}_{k}-\mathbf{A}\right\|_{\mathrm{F}}^{2}+ nk\frac{\Delta^{2}}{4}.\] (111)

**Analyzing term \(\mathrm{T}_{2}\)**: This term is again the same as (101) in the analysis of direct-SVD quant., and can be upper bounded as

\[\mathbb{E}\left\|\mathrm{Q}(\widetilde{\mathbf{U}}_{k})\mathbf{E}^{\prime} \right\|_{\mathrm{F}}^{2}\leq dk\sigma_{1}^{2}\frac{\Delta^{\prime 2}}{4}+k\frac{n \Delta^{2}}{4}\frac{d\Delta^{\prime 2}}{4}.\] (112)

Using (111) and (112), we get:

\[\mathbb{E}\left\|\mathrm{Q}(\widetilde{\mathbf{U}}_{k})\mathrm{Q}^{\prime} \left(\mathrm{Q}(\widetilde{\mathbf{U}}_{k})^{\dagger}\mathbf{A}\right)- \mathbf{A}\right\|_{\mathrm{F}}^{2}\leq\left\|\mathbf{A}_{k}-\mathbf{A}\right\| _{\mathrm{F}}^{2}+\underbrace{nk\frac{\Delta^{2}}{4}}_{\mathrm{T}_{3}}+ \underbrace{dk\sigma_{1}^{2}\frac{\Delta^{\prime 2}}{4}}_{\mathrm{T}_{4}}+ \underbrace{k\frac{n\Delta^{2}}{4}\frac{d\Delta^{\prime 2}}{4}}_{\mathrm{T}_{5}}.\] (113)

**Choice of dynamic range for quantizers \(\mathrm{Q}\)**: Since the input to the quantizer is \(\widetilde{\mathbf{U}}_{k}\), we need an upper bound on \(\|\widetilde{\mathbf{U}}_{k}\|_{\mathrm{max}}\). The \(i^{\mathrm{th}}\) column of \(\widetilde{\mathbf{U}}_{k}\) is \((\widetilde{\mathbf{U}}_{k})_{i}=\sigma_{i}\mathbf{u}_{i}\), where \(1\leq i\leq k\). From Lemma B.3,

\[\|\widetilde{\mathbf{U}}_{k}\|_{\mathrm{max}}\leq\|\widetilde{\mathbf{U}}_{k} \|_{2}=\sigma_{1}.\] (114)

Note that this upper bound is tight in the worst-case sense, i.e., if we consider all matrices with bounded spectral norm, we have \(\sup_{\mathbf{X}:\mathbf{X}=\mathbf{U}\mathbf{E}\mathbf{V}^{\top},\|\mathbf{X} \|_{2}\leq r}\|\widetilde{\mathbf{U}}_{k}\|_{\mathrm{max}}=r\). Hence, the dynamic range of quantizer \(\mathrm{Q}\) is set as \(\sigma_{1}\).

**Choice of dynamic range for quantizers \(\mathrm{Q}^{\prime}\)**: On the other hand, the input to quantizer \(\mathrm{Q}^{\prime}\) is \(\mathrm{Q}(\widetilde{\mathbf{U}}_{k})^{\dagger}\mathbf{A}\). We adopt an approach similar to the proof of Lemma E.2 to upper bound the max-norm of \(\mathrm{Q}(\widetilde{\mathbf{U}}_{k})^{\dagger}\mathbf{A}\) as follows:

\[\left\|\mathrm{Q}(\widetilde{\mathbf{U}}_{k})^{\dagger}\mathbf{A}\right\|_{ \mathrm{max}}\stackrel{{\mathrm{(i)}}}{{\leq}}\left\|\mathrm{Q}( \widetilde{\mathbf{U}}_{k})^{\dagger}\mathbf{A}\right\|_{2}\stackrel{{ \mathrm{(ii)}}}{{\leq}}\left\|\mathrm{Q}(\widetilde{\mathbf{U}}_{k})^{ \dagger}\right\|_{2}\sigma_{1},\] (115)

where \(\mathrm{(i)}\) and \(\mathrm{(ii)}\) follow from Lemmas B.3 and B.4 respectively. We upper bound \(\left\|\mathrm{Q}(\widetilde{\mathbf{U}}_{k})^{\dagger}\right\|_{2}\) as:

\[\left\|\mathrm{Q}(\widetilde{\mathbf{U}}_{k})^{\dagger}\right\|_{ 2}\stackrel{{\mathrm{(i)}}}{{\leq}}\left(\sigma_{\min}\left( \mathrm{Q}(\widetilde{\mathbf{U}}_{k})\right)\right)^{-1} =\left(\sigma_{\min}\left(\widetilde{\mathbf{U}}_{k}+\mathbf{E} \right)\right)^{-1}\] \[\stackrel{{\mathrm{(ii)}}}{{\leq}}\left(\sigma_{\min} (\widetilde{\mathbf{U}}_{k})-\|\mathbf{E}\|_{2}\right)^{-1}=(\sigma_{k}-\| \mathbf{E}\|_{2})^{-1}.\] (116)

Here, \(\mathrm{(i)}\) follows because the singular values of \(\mathrm{Q}(\widetilde{\mathbf{U}}_{k})^{\dagger}\) are inverses of the singular values of \(\mathrm{Q}(\widetilde{\mathbf{U}}_{k})\), and \(\mathrm{(ii)}\) follows from Lemma B.5.

We are now left with upper bounding \(\left\|\mathbf{E}\right\|_{2}\). Since a choice of \(\sigma_{1}\) for the dynamic range of \(\mathrm{Q}\) ensures that quantizer \(\mathrm{Q}\) remains unsaturated, the entries of \(\mathbf{E}\) are bounded as:

\[\left|E_{ij}\right|\leq\Delta=\frac{2\sigma_{1}}{2^{\mathrm{B}}-1}\quad\text{ for all $i\in[n]$ and $j\in[k]$}.\] (117)

Since \(E_{ij}\) is a bounded w.h.p., it is also subgaussian w.h.p. (ref. eq. (16)) with subgaussian norm given by,

\[\left\|\mathbf{E}\right\|_{\psi_{2}}\leq\frac{2\sigma_{1}}{(2^{\mathrm{B}}-1) \log 2}.\] (118)

From Lemma B.12 we get for some absolute constant \(C\),

\[\left\|\mathbf{E}\right\|_{2}\leq\frac{2C\sigma_{1}}{(2^{\mathrm{B}}-1)\log 2} \left(\sqrt{n}+\sqrt{k}+t\right)\text{ with probability exceeding }1-2e^{-t^{2}}.\] (119)

Setting \(t=\sqrt{\log\left(\frac{8n\mathrm{R}^{2}}{\epsilon}\right)}\), we get:

\[\left\|\mathbf{E}\right\|_{2}\leq\frac{2C\sigma_{1}}{(2^{\mathrm{B}}-1)\log 2 }\left(\sqrt{n}+\sqrt{k}+\sqrt{\log\left(\frac{8n\mathrm{R}^{2}}{\epsilon} \right)}\right)\text{ with probability exceeding }1-\frac{\epsilon}{4n\mathrm{R}^{2}}.\] (120)

Let us choose our bit-budget \(\mathrm{B}\) of quantizer \(\mathrm{Q}\) to be such that \(\left\|\mathbf{E}\right\|_{2}\leq\frac{\sigma_{k}}{2}\), i.e.,

\[\mathrm{B}\geq\log_{2}\left(\frac{4C\kappa(\mathbf{A}_{k})}{\log 2}\left(\sqrt{n}+ \sqrt{k}+\sqrt{\log\left(\frac{8n\mathrm{R}^{2}}{\epsilon}\right)}\right)+1 \right),\] (121)where \(\kappa(\mathbf{A}_{k})=\sigma_{1}/\sigma_{k}\) is the condition number of the best rank-\(k\) approximation of \(\mathbf{A}\). Then, using (116), we have:

\[\left\|\mathrm{Q}(\widetilde{\mathbf{U}}_{k})^{\dagger}\mathbf{A}\right\|_{ \max}\leq 2\kappa(\mathbf{A}_{k})\quad\text{with probability exceeding}\;1-\frac{\epsilon}{4n\mathrm{R}^{2}}.\] (122)

So, if we choose the dynamic range of quantized \(\mathrm{Q}^{\prime}\) to be \(2\kappa(\mathbf{A}_{k})\), it will remain unsaturated with probability exceeding \(1-\frac{\epsilon}{4n\mathrm{R}^{2}}\), provided that the bit-budget of first quantizer satisfies \(\mathrm{B}\) satisfies (121).

**Choice of bit-budgets \(\mathrm{B}\) and \(\mathrm{B}^{\prime}\)**: Referring to (113), the term \(\mathrm{T}_{3}\) is,

\[nk\frac{\Delta^{2}}{4}=nk\frac{\sigma_{1}^{2}}{\left(2^{\mathrm{B}}-1\right)^ {2}}\leq\frac{\epsilon}{4}\quad\text{if}\;\mathrm{B}\geq\log_{2}\left(2\sigma _{1}\sqrt{\frac{nk}{\epsilon}}+1\right).\] (123)

The term \(\mathrm{T}_{4}\) is:

\[dk\sigma_{1}^{2}\frac{\Delta^{\prime 2}}{4}=dk\sigma_{1}^{2}\frac{4\kappa( \mathbf{A}_{k})^{2}}{\left(2^{\mathrm{B}^{\prime}}-1\right)^{2}}\leq\frac{ \epsilon}{4}\quad\text{if}\;\mathrm{B}^{\prime}\geq\log_{2}\left(4\sigma_{1} \kappa(\mathbf{A}_{k})\sqrt{\frac{dk}{\epsilon}}+1\right).\] (124)

With the above choices of \(\mathrm{B}\) and \(\mathrm{B}^{\prime}\), the term \(\mathrm{T}_{5}\) is:

\[ndk\frac{\Delta^{2}}{4}\frac{\Delta^{\prime 2}}{4}\leq ndk\frac{\epsilon}{4 nk}\frac{\epsilon}{4dk\sigma_{1}^{2}}\leq\frac{\epsilon}{4}\quad\text{if} \quad\epsilon\leq 4k\sigma_{1}^{2}.\] (125)

**Tying it all together**: Similar to what is done in the proof of Thm. G.2, for the purpose of analysis, we assume that Alg. 3 returns \(\mathbf{L}=\mathbf{0}\) and \(\mathbf{R}=\mathbf{0}\) if quantizer \(\mathrm{Q}^{\prime}\) gets saturated. In practical implementation, it can easily be checked if either quantizer \(\mathrm{Q}\) or \(\mathrm{Q}^{\prime}\) gets saturated or not, and the algorithm can be repeated again with a new realization of the stochastic quantizer \(\mathrm{Q}\). Since the choice of dynamic ranges for \(\mathrm{Q}\) ensures that it remains unsaturated with a high probability, "reasonably few" realizations would suffice to get at least one good realization in which \(\mathrm{Q}\) is unsaturated.

However, in what follows, we assume that that if quantizer \(\mathrm{Q}\) gets saturated, then the algorithm returns \(\mathbf{0}\) as an estimate of \(\mathbf{A}\), resulting in a Frobenius norm error of \(\left\|\mathbf{A}\right\|_{\mathrm{F}}\). Since this happen with a very small probability, we show that its contribution to the expected Frobenius norm error of Alg. 3 is small as well. Let us denote the event that quantizer \(\mathrm{Q}\) is unsaturated as \(\mathcal{E}\). Then, the expected approximation error can be written as

\[\mathbb{E}\left\|\mathbf{L}\mathbf{R}-\mathbf{A}\right\|_{\mathrm{ F}}^{2} =\mathbb{E}\left[\left\|\mathrm{Q}(\widetilde{\mathbf{U}}_{k})\mathrm{Q}^{\prime} \left(\mathrm{Q}(\widetilde{\mathbf{U}}_{k})^{\dagger}\mathbf{A}\right)- \mathbf{A}\right\|_{\mathrm{F}}^{2}\mathds{1}_{\mathcal{E}}\right]+\mathbb{E} \left[\left\|\mathbf{A}\right\|_{\mathrm{F}}^{2}\mathds{1}_{\mathcal{E}^{C}}\right]\] \[\overset{\mathrm{(i)}}{\leq}\mathbb{E}\left[\left\|\mathrm{Q}( \widetilde{\mathbf{U}}_{k})\mathrm{Q}^{\prime}\left(\mathrm{Q}(\widetilde{ \mathbf{U}}_{k})^{\dagger}\mathbf{A}\right)-\mathbf{A}\right\|_{\mathrm{F}}^{2} \mathds{1}_{\mathcal{E}}\right]+n\mathrm{R}^{2}\Pr\left(\mathds{1}_{\mathcal{E }^{C}}\right)\] \[\overset{\mathrm{(ii)}}{\leq}\left\|\mathbf{A}_{k}-\mathbf{A} \right\|_{\mathrm{F}}^{2}+nk\frac{\Delta^{2}}{4}+dk\sigma_{1}^{2}\frac{\Delta^ {\prime 2}}{4}+k\frac{n\Delta^{2}}{4}\frac{d\Delta^{\prime 2}}{4}+n\mathrm{R}^{2}\Pr\left( \mathds{1}_{\mathcal{E}^{C}}\right),\] (126)

where inequality \(\mathrm{(i)}\) follows from Asm. 3.1 and the fact that the expectation of indicator function of an event is the probability of the event, and \(\mathrm{(ii)}\) follows from (113). Using appropriate choices of \(\mathrm{B}\) and \(\mathrm{B}^{\prime}\) and small enough \(\epsilon\), from (122), (123), (124), and (125), we have

\[\mathbb{E}\left\|\mathbf{L}\mathbf{R}-\mathbf{A}\right\|_{\mathrm{F}}^{2}\leq \left\|\mathbf{A}_{k}-\mathbf{A}\right\|_{\mathrm{F}}^{2}+\frac{\epsilon}{4}+ \frac{\epsilon}{4}+\frac{\epsilon}{4}+n\mathrm{R}^{2}\frac{\epsilon}{4n \mathrm{R}^{2}}=\left\|\mathbf{A}_{k}-\mathbf{A}\right\|_{\mathrm{F}}^{2}+\epsilon.\] (127)

Note that from (123) and (121), the bit-budget of the quantizers must satisfy:

\[\mathrm{B}\geq\max\;\{\mathrm{B}_{1},\mathrm{B}_{2}\},\quad\text{ and }\mathrm{B}^{\prime}\geq\log_{2}\left(4\sigma_{1}\kappa(\mathbf{A}_{k})\sqrt{\frac{ dk}{\epsilon}}+1\right)\] (128)

where,

\[\mathrm{B}_{1}=\log_{2}\left(2\sigma_{1}\sqrt{\frac{nk}{\epsilon}}+1\right),\] (129)

and,

\[\mathrm{B}_{2}=\log_{2}\left(\frac{4C\kappa(\mathbf{A}_{k})}{\log 2}\left(\sqrt{n}+ \sqrt{k}+\sqrt{\log\left(\frac{8n\mathrm{R}^{2}}{\epsilon}\right)}\right)+1 \right).\] (130)

This completes the proof.

[MISSING_PAGE_FAIL:41]

Figure 4: Comparison of LPLR and LPLR-SVD on LlaMa weight matrices with \(\mathrm{B}=\mathrm{B}^{\prime}=\mathrm{B}_{\mathrm{nq}}=4\) bits, ordered by the original sequence of layers on the “Layer” - axis. LPLR and LPLR-SVD demonstrate equivalent performance on a uniform compression budget of \(4\) bits, while outperforming naïve quant. on almost all layers.

Figure 3: Comparison of LPLR and LPLR-SVD on LlaMa weight matrices with \(\mathrm{B}=\mathrm{B}^{\prime}=8\) bits, \(\mathrm{B}_{\mathrm{nq}}=4\) bits, ordered by the original sequence of layers on the “Layer” - axis. We observe consistently better Frobenius norm error using LPLR and LPLR-SVD, with the exception of specific layers which lend themselves to naïve quantization.

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Dataset** & **LPLR** & **DSVD** & **LSVD** \\ \hline CIFAR-10 & \(71\pm 11\) ms & \(306\pm 26\) ms & \(312\pm 8\) ms \\ \hline CIFAR-100 & \(14\pm 3\) ms & \(51\pm 3\) ms & \(56\pm 3\) ms \\ \hline \hline \end{tabular}
\end{table}
Table 10: Comparison of wall-clock time for computing low rank quantized factors via LPLR, LSVD and DSVD. Each image embedding forms a row of the input matrix, with low rank factors computed for **each** class of the input dataset. Bit-budgets used are \(\mathrm{B}=\mathrm{B}^{\prime}=8,\mathrm{B}_{\mathrm{nq}}=1\). We report mean and standard deviation.

Figure 5: Compression of Shepp-Logan phantom (a standard test image for medical image reconstruction, courtesy: Phantominator, PyPI). \(\mathrm{B}=4,\mathrm{B}^{\prime}=8\), \(\mathrm{B}_{\mathrm{nq}}=2,m=166\). Orig. image dim.: \(1000\times 1000\)

Figure 6: Compression of Shepp-Logan phantom (a standard test image for medical image reconstruction, courtesy: Phantominator, PyPI). \(\mathrm{B}=4,\mathrm{B}^{\prime}=8\), \(\mathrm{B}_{\mathrm{nq}}=1,m=83\). Orig. image dim.: \(1000\times 1000\)

Figure 7: Compression of a Jupiter image showing its Great Red Spot and Ganymede’s shadow (ourtesy: NASA/ESA Hubble Space Telescope). \(\mathrm{B}=2,\mathrm{B}^{\prime}=8\), \(\mathrm{B}_{\mathrm{nq}}=1,m=110\). Orig. image dim.: \(1102\times 1102\)Limitations and further discussions

**On uniformly dithered quantizer**: We would like to point out that the approximation error upper bound for our algorithm (derived in Thm. G.2), as well as for the baselines such as direct-SVD quantization (refer to Prop. H.1) holds true only for randomized rounding, or uniformly dithered quantizers. Dithering is preferred due to its ability to produce an unbiased estimate of the input. It offers an advantage by introducing a non-zero probability of quantizing an input to either its ceiling or floor, resulting in reduced variance when averaging multiple independent realizations. Moreover, the unbiasedness of the quantizer output simplifies the analysis. In our experiments, we did not observe any difference while using deterministic rounding instead of dithered rounding.

**Comparison with LPLR with direct-SVD and LPLR-SVD**: As we see from the numerical evaluations (ref. to Tab. 6 and Tab. 7), direct-SVD quant. and LPLR-SVD performs better than our proposed LPLR algorithm (that computes a Gaussian sketch). Intuitively, one would expect the performance of LPLR-SVD and LPLR (with Gaussian sketch) to be similar because the core idea behind both of them is the same - Quantize the first low-rank factor, and then project the columns of A onto the quantized (first) low-rank factor. For LPLR-SVD the basis being quantized (top-k singular vectors) is the exact top-k rangefinder (for the column space of A), whereas in LPLR, we sketch using the Gaussian matrix to get an approximate rangefinder. This sketching error might make LPLR worse over LPLR-SVD. But because sketching also introduces "equalization effect of the Gaussian matrices", we can improve the resolution of the uniform scalar quantizers by choosing a smaller dynamic range. Hence, there is a tradeoff when comparing LPLR and LPLR-SVD - a tradeoff between sketching error and the quantization error.

Let us now focus on LPLR-SVD and direct-SVD quant. We will extrapolate this intuition to a comparison between LPLR and direct-SVD quant. The factorization obtained using direct-SVD quant. is, where the approximation obtained using LPLR-SVD is. Here, is the matrix obtained using the first columns of, where is the full SVD of. Let us assume so that we don't have to worry about the second quantizer. Then, we are left to compare the approximations: and. Clearly, the latter has smaller Frobenius norm error because it is the minima of the minimization problem. So, for, it is evident that direct-SVD can only perform as good as LPLR-SVD, and not better than that. Furthermore, note that this difference in performance will be small when the bit-budget of the first quantizer, is large, and it will be large when is small. When, this difference will be exactly zero. In other words, direct-SVD quant. and LPLR-SVD will be the same factorization when and. When both and are finite, direct-SVD can outperform LPLR-SVD depending on which one out of and is closer to the optimal solution, i.e.,. A limitation of our analysis is that we cannot always definitively predict which one will perform better, because we derive _upper bounds_ on the approximation error.

**Comparison of LPLR with naive quantization**: In Tables 1 and 2, we have compared LPLR with other benchmarks. In the column denoting approximation errors, aside from the common quantization error  present across all rows, DSVD and LPLR introduce an additional term representing the error from low-rank approximation. Consequently, it might appear that naive quantization consistently outperforms these methods. However, it's essential to recognize that compression techniques based on low-rank factorization hold value exclusively when the matrix being compressed is inherently low-rank, meaning that starts off as small. If, LPLR can achieve identical error levels as naive quantization, while demanding fewer bits than the latter. In other words, given the same bit-budget, LPLR can achieve a lower level of error compared to naive quantization.

**Matrix compression with and without any computation constraints**: In situations where computational resources are limited, employing the naive strategy emerges as the most cost-effective approach for matrix quantization. Nevertheless, due to its failure to capitalize on a matrix's inherent low-rank arrangement, naive quantization may prove considerably suboptimal. As matrices increase in dimension, accommodating them in memory becomes impractical, rendering approaches like DSVD or LSVD unviable. In such scenarios, our LPLR method stands as a viable alternative,demanding slightly more computational effort than naive quantization, yet capable of harnessing the low-rank structure for improved approximation accuracy.

If there is no scarcity of computation resources in being able to compute the SVD, and our goal is to just compress an input matrix \(\mathbf{A}\), then it is possible to compress the matrix with all the strategies, and choose the best one. However, it must be kept in mind that LPLR provides significant savings with respect to computational complexity, i.e., \(\mathrm{O}(ndm)\), compared to LPLR-SVD, which has a complexity of \(\mathrm{O}(nd^{2})\). Consequently, if the matrix being compressed is very large, it might not even be feasible to do direct-SVD quant. or LPLR-SVD, given the current memory limitations of available GPUs, and LPLR (with Gaussian sketch) becomes our only option.

Furthermore, a careful examination of Tabs. 6 and 7 shows that even though LPLR-SVD and direct-SVD quant. have a smaller Frobenius norm error than LPLR, the accuracy and weighted F1 scores are still comparable. One aspect of quantization that we have not studied in detail is its inherent regularization effect when testing the performance on test datasets. Whether LPLR leads to a better regularization effect that the other baselines is an open question that will be studied in future works.

**Comparison with existing works on quantized random projections for approximate nearest neighbor search**: Prior works of Li and Li [31] and Li and Li [32] consider the idea of quantized random projections specifically for the the application of approximate nearest neighbor search. Although related, the goals of these works are different from matrix compression considered in our work. These prior works approximate the matrix vector product \(\mathbf{A}\mathbf{x}\) by \(\mathrm{Q}(\mathbf{A}\mathbf{S})\mathrm{Q}^{\prime}(\mathbf{S}^{\top}\mathbf{ x})\). For their setup, in addition to \(\mathrm{Q}(\mathbf{A}\mathbf{S})\), the random matrix \(\mathbf{S}\) needs to be stored in full-precision, in order to process an incoming query \(\mathbf{x}\). So, their storage requirement is \(nm\) quantized and \(dm\) full-precision parameters. In contrast, **LPLR** stores only the quantized entries of \(\mathbf{L}\) and \(\mathbf{R}\), i.e., \(m(n+d)\) quantized entries, and \(\mathbf{A}\approx\mathbf{LRx}\), implying a smaller storage requirement. Furthermore, since **LPLR** involves computation of \(\mathbf{R}\mathbf{x}\), where \(\mathbf{R}\) consists of \(md\) quantized values, it can leverage modern advancements in hardware primitives for speeding up low-precision computations, such as half and mixed-precision compute.

**Extension to streaming settings**: LPLR as presented in this paper, is used for compression when the entire matrix \(\mathbf{A}\) is available. An interesting extension is to consider a variant of LPLR to streaming data settings in order to handle new data points. It is possible to do so using sketching based low rank approximations [68]. The left factor \(\mathbf{L}\) is updated with the concatenation of an additional row corresponding to the newly arriving datapoint. The second factor \(\mathbf{R}\), which is the solution of a new least squares minimization problem, can be updated using Woodbury matrix inversion lemma.