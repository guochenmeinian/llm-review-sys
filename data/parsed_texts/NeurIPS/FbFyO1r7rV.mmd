# How does fine-tuning affect your model? Mechanistic analysis on procedural tasks

Samyak Jain\({}^{1,}\)

Co-first authors. samyakjain.cse18@itbhu.ac.in, robert.kirk.3.14@gmail.com, eslubana@umich.edu.

Robert Kirk\({}^{2,}\)

University College London, UK

Ekdeep Singh Lubana\({}^{3,4,*}\)

Co-first authors. samyakjain.cse18@itbhu.ac.in, robert.kirk.3.14@gmail.com, eslubana@umich.edu.

Robert P. Dick\({}^{3}\)

Hidenori Tanaka\({}^{4,5}\)

Edward Grefenstette\({}^{2}\)

Tim Rocktaschel\({}^{2}\)

David Krueger\({}^{1}\)

\({}^{1}\)University of Cambridge, UK

\({}^{2}\)University College London, UK

\({}^{3}\)EECS Department, University of Michigan, Ann Arbor, MI, USA

\({}^{4}\)Center for Brain Science, Harvard University, Cambridge, MA, USA

\({}^{5}\)Physics & Informatics Laboratories, NTT Research, Inc., Sunnyvale, CA, USA

###### Abstract

Fine-tuning large pre-trained models has become the _de facto_ strategy for developing models that are safe to deploy. However, there has been little work that explains how fine-tuning alters the underlying capabilities learnt by a model during pretraining: does fine-tuning yield entirely novel capabilities or does it just modulate existing ones? We address this question empirically in _synthetic_ settings with mechanistic interpretability tools (e.g., network pruning and probing) to understand how the model's underlying capabilities are changing. Our extensive analysis of the effects of fine-tuning shows: (i) fine-tuning rarely alters the underlying model capabilities; (ii) a minimal transformation, which we call a 'wrapper', is typically learned on top of the underlying model capabilities; and (iii) further fine-tuning on a task where such wrapped capabilities are relevant leads to sample-efficient "revival" of the capability, i.e., the model begins reusing this capability in a few gradient steps. _This indicates practitioners can unintentionally remove a model's safety wrapper by merely fine-tuning it on a superficially unrelated task._

## 1 Introduction

Large language models (LLMs) pretrained on huge, web-crawled text datasets demonstrate extremely general capabilities [1, 2, 3, 4]. This has led to the current paradigm of machine learning, where practitioners often use model adaptation protocols such as fine-tuning to achieve unprecedented performance on a broad range of downstream tasks [5, 6, 7, 8, 9]. Fine-tuning with different training objectives has also seen immense usage in mitigating "unsafe" capabilities (e.g., producing sensitive, biased, or toxic outputs), serving as an integral component of current state-of-the-art _alignment_ approaches like RLHF [10, 11, 12, 13, 14]. Given the ubiquity of fine-tuning, a natural question emerges: precisely how does fine-tuning influence a pretrained model's capabilities to adapt it to the downstream dataset? The generality of an LLM's capabilities makes it feasible that fine-tuning protocols merely identify the most relevant capability and amplify its use for a given set of inputs, while inhibiting the use of other capabilities that are viable for processing the inputs. This can be concerning in scenarios where fine-tuning is used for removing undesirable behaviors from a model, as the protocol may just "hide" the undesirable capability.

Motivated by the above, we perform an extensive analysis of the effects of fine-tuning on a pretrained model's capabilities in _controlled settings_ where we can use mechanistic interpretability tools to understand precisely what is happening to the model's underlying capabilities. Specifically, we focuson compiled transformer models based on the _Tracr library_[15; 16]--which allows encoding specific computational programs into a transformer--and procedurally generated setups involving _probabilistic context-free grammars (PCFGs)_[17; 18]--a formal model designed to capture syntactic properties of natural and programmatic languages that has recently served as a testbed for mechanistically understanding language models [19; 20; 21]. While Tracr allows us to analyze models with perfectly encoded capabilities, models trained on PCFGs allow us to evaluate the effects of different pretraining design choices that may yield learning of "approximate" capabilities. Fine-tuning these models via the often used protocol of further training a pretrained model on a downstream dataset with a sufficiently small learning rate, we find when a "relevant" pretraining capability is present, the fine-tuned model learns a minimally transformed version of it. We call this transformation a wrapper and find that the wrapper can often be extremely localized, e.g., via mere pruning of a few weights or neurons, the model can start to reuse its pretraining capability and forget how to perform the downstream task. Morover, further fine-tuning the model on a subset of pretraining data leads to an extremely sample-efficient revival of the capability.

## 2 Defining our notion of capabilities

For precision, we first discuss the notion of capabilities that we aim to capture for analyzing how fine-tuning alters a model. Let \(\mathcal{D}_{\mathtt{PT}}\) denote a dataset sampled from a distribution \(\mathcal{P}_{\mathrm{X}}\) over the domain \(\mathrm{X}\). We will assume the domain \(\mathrm{X}\) can itself be factorized into two domains \(\mathrm{X}_{I}\) and \(\mathrm{X}_{D}\). Correspondingly, a sample \(x\in\mathrm{X}\) can be divided into a tuple of variables \((x_{i}\in\mathrm{X}_{I},x_{d}\in\mathrm{X}_{D})\), where \(x_{i}\) identifies which capability a model should use to process the information encoded by the variable \(x_{d}\). This decomposition captures the idea that different prompts can force a pretrained LLM to elicit different capabilities, e.g., by adding the phrase "think step by step" [22]. The identifier of capability \(c\) is denoted \(i_{c}\). Pretraining on \(\mathcal{D}_{\mathtt{PT}}\) yields us a model \(\mathtt{M}(.)\colon\mathrm{X}\to\mathrm{Y}\), where we distinguish between the domain and co-domain of the model for the purpose of our framework; in practice, one can assume \(\mathrm{Y}=\mathrm{X}\) for language models trained on a sufficiently large dataset. We define a capability as follows.

Definition 1 (Capability.): Define a map \(\mathcal{C}:\mathrm{X}_{\mathcal{D}}\to\mathrm{Y}_{\mathcal{C}}\), where \(\mathrm{Y}_{\mathcal{C}}\subset\mathrm{Y}\). Let \(\mathrm{X}_{\mathcal{C}}\subset\mathrm{X}\) be a sub-domain such that for all \(x\in X_{\mathcal{C}}\), the capability identifier variable is the same, i.e., \(x_{i}=\mathtt{i}_{\mathcal{C}}\). Then, we say the model \(\mathtt{M}\) "possesses a capability \(\mathcal{C}\)" if for all \(x\in\mathrm{X}_{\mathcal{C}}\), \(\mathtt{M}(x)=\mathcal{C}(x_{d})\).

We use an idealized definition to relay our primary intuition and emphasize that we do not expect all capabilities a pretrained model seems to possess will act as perfectly as the definition necessitates. We next consider how the fine-tuning distribution \(\mathcal{P}_{X}^{\mathtt{PT}}\) over the domain \(\mathrm{X}\) can interact with capabilities exhibited in the pretrained model. Our goal here is to capture the fact that a large-scale pretraining corpora is likely to have non-zero probability under the fine-tuning distribution. Correspondingly, it is unlikely that a pretrained model will lack _any_ capability relevant to the downstream task. This motivates a notion of "relevance of a capability". Specifically, let \(\mathcal{D}_{\mathtt{PT}}\sim\mathcal{P}_{X}^{\mathtt{PT},\mathtt{E}}\) denote the downstream dataset used for fine-tuning, where \(\mathcal{P}_{X}^{\mathtt{PT},\mathtt{E}}\) is the empirical distribution that captures a subset of the support with non-zero probability in the distribution \(\mathcal{P}_{X}^{\mathtt{PT}}\).

Definition 2 (Relevance of a Capability.): Assume the capability \(\mathcal{C}\) possessed by the pretrained model \(\mathtt{M}(.)\) can be transformed to a map \(\mathtt{g}\circ\mathcal{C}\) via fine-tuning on \(\mathcal{D}_{\mathtt{PT}}\), such that for all \(x\sim\mathcal{P}_{X}^{\mathtt{PT},\mathtt{E}}\), the correct output is produced. Then, if for all \(x\sim\mathcal{P}_{X}^{\mathtt{PT}}\), \(\mathtt{g}\circ\mathcal{C}\) yields the correct output, we claim capability \(\mathcal{C}\) is **strongly relevant** to the fine-tuning task; else, we call it **weakly relevant**.

For example (also see Fig. 1), a _weakly relevant_ capability can involve the ability to recognize a spurious feature that the model can learn to exploit to perform well on the fine-tuning dataset, without enabling generalization to the overall distribution that the fine-tuning dataset is sampled from. Meanwhile, a _strongly relevant_ capability is one that extract a causally relevant feature for that task. When a weakly relevant capability is used by the model, we find we can often localize neurons which implement the transform \(\mathtt{g}\) in Def. 2. We call such a \(\mathtt{g}\) a **wrapper** and \(\mathtt{g}\circ\mathcal{C}\) a **wrapped capability**.

Figure 1: **Capability Relevance. Consider the task of completing a passage while maintaining its narrative. Herein, the ability to recognize the sentiment of a text will be deemed _strongly relevant_ and the ability to recognize negative words _weakly relevant_. Such words are often correlated with a negative sentiment.**

[MISSING_PAGE_FAIL:3]

capabilities setup of PCFG Counters in this section, relegating most results on compiled capabilities to App. I, learned capabilities to App. J, and also some preliminary analysis of the recently released TinyStories dataset [33] to App. H--results are consistent across all settings. In the PCFG Counters setup, the model is pretrained to count tokens \(\boxed{\texttt{pre}}=\{a,b,c\}\) in a given string; during fine-tuning, the model is trained to count the token \(\boxed{\texttt{pre}}=\texttt{b}\), wherein the spurious correlation is defined by enforcing count of b to be 1 larger than that of a. The probability of embedding a spurious correlation in the train/test fine-tuning dataset is denoted \(R_{\texttt{FT,Tr}}/R_{\texttt{FT,Tr}}\), with \(R_{\texttt{FT,Tr}}\in\{0.0,0.5,0.8,1.0\}\) and \(R_{\texttt{FT,Te}}\in\{0.0,1.0\}\). We use three sets of sampling probabilities of the task operands in the pretraining data: \(\mathcal{P}^{L}_{C}=(0.999,0.001,0.000)\), \(\mathcal{P}^{M}_{C}(0.9,0.1,0.0)\), or \(\mathcal{P}^{H}_{C}=(0.5,0.3,0.2)\). We also sweep the fine-tuning learning rate \(\eta\), \(\eta_{L}=10^{-3}\), \(\eta_{M}=10^{-4}\), \(\eta_{S}=10^{-5}\).

**Experiment 1:** We first evaluate the model's learning dynamics during fine-tuning (see Fig. 5). When the pretraining prior has low probability of sampling the target token, the fine-tuned model performs well only when the spurious correlation is present, i.e., \(R_{\texttt{FT,Te}}=1\). As the prior is reduced, however, we observe this behavior significantly changes. Only having a sufficiently large sampling probability of the token during pretraining leads the model to avoid the spurious correlation. We argue this is likely to happen due to the model partially learning the strongly relevant capability, with fine-tuning enabling the completion of this learning. Based on this behavioral analysis, we hypothesize that the model learns a wrapper on the _weakly relevant capability_ of counting the token \(\boxed{\texttt{pre}}\), adding the relevant difference to yield the correct result for \(\boxed{\texttt{pre}}\). To evaluate this, we analyze the models fine-tuned with a low sampling prior via network pruning and linear probing (see Fig. 7). The details of pruning are discussed in the appendix. If the model learns a wrapper on this capability, deleting the wrapper neurons should recover the capability to count \(\boxed{\texttt{pre}}\). As shown in Fig. 3, we find this is indeed the case. To confirm this hypothesis further, we perform probing analysis by training a linear layer on every block's residual output. As shown in Fig. 7 (a), in the presence of spurious correlations, the model with weakly relevant capability is able to perform well on counting the token \(\boxed{\texttt{pre}}\) when fine-tuned using correlated data.

Takeaway: We find the model seems to exploit the spurious correlation when the sampling prior of target token is low during pretraining, but avoids it when the prior is sufficiently high to enable partial learning of the strongly relevant capability. Mechanistic analysis shows that the features learned by the weakly relevant capability can be localized in the model, indicating learning of a wrapper.

**Experiment 2:** The above analysis focuses on the learning of a capability relevant to the downstream task. We note analyze how the pre-training capability itself is affected during fine-tuning. Specifically, we use a model fine-tuned to count \(\boxed{\texttt{pre}}\) and reverse fine-tune (reFT ) it to learn its original pre-training capability to count \(\boxed{\texttt{pre}}\). As shown in Fig. 8 (a), even with a small learning rate (\(\eta_{S}\)), the pretraining capability is revived via reFT ; this revival is made quicker with increase in \(\mathcal{P}_{C}(\boxed{\texttt{pre}})\), as shown in Fig. 8 (appendix). This _indicates_ that upon fine-tuning, relevant "circuits" for the pre-training capability continue to persist in the model. To analyze this further, we train linear probes on intermediate representations from each block of the model to produce the output of the pre-training task of counting \(\boxed{\texttt{pre}}\). If the pre-training capabilities are indeed present in the model after fine-tuning to count on token \(\boxed{\texttt{pre}}\), the probes should yield us high performance. As shown in Fig. 7, the pre-training capabilities to count \(\boxed{\texttt{pre}}\) is indeed present in the model, especially under strong-relevance. Under weak-relevance of the capability, a smaller fine-tuning learning rate (\(\eta_{S}\)), as expected to be used in a practical scenario, leads to continued existence of the pre-training capability.

Takeaway: We find that the pre-training capability is not lost on fine-tuning.

Figure 3: **Pruning analysis: Fine-tuning task related features learned by the weakly relevant capability can be localized in the model:** Evaluation is done for the pre-training task of counting the token \(\boxed{\texttt{pre}}\). The pre-training task capability can be revived on using a low learning rate \(\eta_{S}\).

## Acknowledgements

ESL thanks Eric Bigelow, Nikhil Vyas, and Usman Anwar for relevant discussions early in the project. SJ was partially supported by BERI; ESL was partially supported via NSF under award CNS-2008151. RK was supported by the Foundation AI CDT at UCL.

## Authors' Contributions

ESL conceived the project direction and developed a set of hypotheses on the limitations of fine-tuning, with inputs from RK. SJ and ESL co-designed a draft of the PCFG and Tracr setups, and came up with pruning and reverse fine-tuning analysis which led to validation and further refining of the hypotheses. SJ led the experimental execution and made the tasks considered in the paper precise in collaboration with ESL. RK proposed and ran the TinyStories experiments with inputs from ESL, SJ, EG and TR. Literature review and writing of the main paper was led by ESL. SJ led writing of the appendix. ESL, SJ, RK, and HT collaborated on design of all figures and plots. DSK acted as the primary senior advisor on the paper, with inputs from RPD, HT, EG, and TR as well.

## References

* [1]A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. (2018) Improving language understanding by generative pre-training. Cited by: SS1.
* [2]A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. (2019) Language models are unsupervised multitask learners. OpenAI blog1 (8), pp. 9. Cited by: SS1.
* [3]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) Language models are few-shot learners. Advances in neural information processing systems33, pp. 1877-1901. Cited by: SS1.
* [4]S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg, et al. (2023) Sparks of artificial general intelligence: early experiments with gpt-4. arXiv preprint arXiv:2303.12712. Cited by: SS1.
* [5]C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu (2020) Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research21 (1), pp. 5485-5551. Cited by: SS1.
* [6]V. Sanh, A. Webson, C. Raffel, S. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, A. Raja, M. Dey, M. Bari, C. Xu, U. Thakker, S. Sharma Sharma, E. Szczechla, T. Kim, G. Chhablani, N. Nayak, D. Datta, J. Chang, M. Tian-Jian Jiang, H. Wang, M. Manica, S. Shen, Z. X. Yong, H. Pandey, R. Bawden, T. Wang, T. Neeraj, J. Rozen, A. Sharma, A. Santilli, T. Fevry, J. Alan Fries, R. Teehan, T. Le Scao, S. Biderman, L. Gao, T. Wolf, and A. M. Rush (2022) Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations, External Links: Link Cited by: SS1.
* [7]M. Reid, Y. Yamada, and S. G. Gu (2022) Can wikipedia help offline reinforcement learning?. arXiv preprint arXiv:2201.12122. Cited by: SS1.
* [8]D. Driess, F. Xia, M. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu, et al. (2023) Palm-e: an embodied multimodal language model. arXiv preprint arXiv:2303.03378. Cited by: SS1.
* [9]M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakrishnan, K. Hausman, et al. (2022) Do as i can, not as i say: grounding language in robotic affordances. arXiv preprint arXiv:2204.01691. Cited by: SS1.
* [10]L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. (2022) Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems35, pp. 27730-27744. Cited by: SS1.
* [11]C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu (2020) Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research21 (1), pp. 5485-5551. Cited by: SS1.
* [12]C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu (2020) Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research21 (1), pp. 5485-5551. Cited by: SS1.
* [13]C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu (2020) Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research21 (1), pp. 5485-5551. Cited by: SS1.
* [14]C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu (2020) Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research21 (1), pp. 5485-5551. Cited by: SS1.
* [15]C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu (2020) Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research21 (1), pp. 5485-5551. Cited by: SS1.
* [16]C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu (2020) Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research21 (1), pp. 5485-5551. Cited by: SS1.
* [17]C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu (2020) Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research21 (1), pp. 5485-5551. Cited by: SS1.
* [18]C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu (2020) Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research21 (1), pp. 5485-5551. Cited by: SS1.
* [19]C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu (2020) Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research21 (1), pp. 5485-5551. Cited by: SS1.
* [20]C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu (2020) Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research21 (1), pp. 5485-5551. Cited by: SS1.
* [21]C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu (2020) Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research21 (1), pp. 5485-5551. Cited by: SS1.
* [22]C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu (2020) Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research21 (1), pp. 5485-5551. Cited by: SS1.
* [23]C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu (2020) Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research21 (1), pp. 5485-5551. Cited by: SS1.
* [24]C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu (2020) Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research21 (1), pp. 5485-5551. Cited by: SS1.
* [25]C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu (2020) Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research21 (1), pp. 5485-5551. Cited by: SS1.
* [26]C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu (2020) Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research21 (1), pp. 5485-5551. Cited by: SS1.
* [27]C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu (2020) Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research21 (1), pp. 5485-5551. Cited by: SS1.
* [28]C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu (2020) Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research21 (1), pp. 5485-5551. Cited by: SS1.
* [29]C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu (2020) Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research21 (1), pp. 5485-5551. Cited by: SS1.
* [30]C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu (2020) Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research21 (1), pp. 5485-5551. Cited by: SS1.
* [31]C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu (2020) Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research21 (1), pp. 5485-5551. Cited by: SS1.
* [32]C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou,* [11] Dongyoung Go, Tomasz Korbak, German Kruszewski, Jos Rozen, Nahyeon Ryu, and Marc Dymetman. Aligning language models with preferences through f-divergence minimization. _arXiv preprint arXiv:2302.08215_, 2023.
* [12] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. _Advances in Neural Information Processing Systems_, 33:3008-3021, 2020.
* [13] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. _arXiv preprint arXiv:2204.05862_, 2022.
* [14] Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. Improving alignment of dialogue agents via targeted human judgements. _arXiv preprint arXiv:2209.14375_, 2022.
* [15] David Lindner, Janos Kramar, Matthew Rahtz, Thomas McGrath, and Vladimir Mikulik. Tracr: Compiled transformers as a laboratory for interpretability. _arXiv preprint arXiv:2301.05062_, 2023.
* [16] Gail Weiss, Yoav Goldberg, and Eran Yahav. Thinking like transformers. In _International Conference on Machine Learning_, pages 11080-11090. PMLR, 2021.
* [17] Michael Sipser. Introduction to the theory of computation. _ACM Sigact News_, 27(1):27-29, 1996.
* [18] Noam Chomsky. Three models for the description of language. _IRE Transactions on information theory_, 2(3):113-124, 1956.
* [19] Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 1, context-free grammar. _arXiv preprint arXiv:2305.13673_, 2023.
* [20] Gregoire Deletang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li Kevin Wenliang, Elliot Catt, Chris Cundy, Marcus Hutter, Shane Legg, Joel Veness, et al. Neural networks and the chomsky hierarchy. _arXiv preprint arXiv:2207.02098_, 2022.
* [21] Hui Shi, Sicun Gao, Yuandong Tian, Xinyun Chen, and Jishen Zhao. Learning bounded context-free-grammar via lstm and the transformer: Difference and the explanations. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 8267-8276, 2022.
* [22] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. _arXiv preprint arXiv:2205.11916_, 2022.
* [23] Andrej Karpathy. _MinGPT_, 2020. Github link. https://github.com/karpathy/minGPT/tree/master.
* [24] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. _arXiv preprint arXiv:1905.09418_, 2019.
* [25] Hidenori Tanaka, Aran Nayebi, Niru Maheswaranathan, Lane McIntosh, Stephen Baccus, and Surya Ganguli. From deep learning to mechanistic understanding in neuroscience: the structure of retinal prediction. _Adv. in Neural Information Processing Systems (NeurIPS)_, 2019.
* [26] Sofia Serrano and Noah A Smith. Is attention interpretable? _arXiv preprint arXiv:1906.03731_, 2019.
* [27] Sarah Wiegreffe and Yuval Pinter. Attention is not not explanation. _arXiv preprint arXiv:1908.04626_, 2019.
* [28] Vivian Lai and Chenhao Tan. On human predictions with explanations and predictions of machine learning models: A case study on deception detection. In _Proceedings of the conference on fairness, accountability, and transparency_, pages 29-38, 2019.

* [29] Ian Tenney, Dipanjan Das, and Ellie Pavlick. Bert rediscovers the classical nlp pipeline. _arXiv preprint arXiv:1905.05950_, 2019.
* [30] Elena Voita and Ivan Titov. Information-theoretic probing with minimum description length. _arXiv preprint arXiv:2003.12298_, 2020.
* [31] Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. Dissecting recall of factual associations in auto-regressive language models. _arXiv preprint arXiv:2304.14767_, 2023.
* [32] Mor Geva, Avi Caciularu, Kevin Ro Wang, and Yoav Goldberg. Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. _arXiv preprint arXiv:2203.14680_, 2022.
* [33] Ronen Eldan and Yuanzhi Li. Tinyn stories: How small can language models be and still speak coherent english? _arXiv preprint arXiv:2305.07759_, 2023.
* [34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* [35] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 9650-9660, 2021.
* [36] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. _arXiv preprint arXiv:2109.01652_, 2021.
* [37] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. _Advances in Neural Information Processing Systems_, 35:1950-1965, 2022.
* [38] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment. _arXiv preprint arXiv:2112.00861_, 2021.
* [39] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In _International Conference on Machine Learning_, pages 2790-2799. PMLR, 2019.
* [40] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. _arXiv preprint arXiv:2106.10199_, 2021.
* [41] Yihan Wang, Si Si, Daliang Li, Michal Lukasik, Felix Yu, Cho-Jui Hsieh, Inderjit S Dhillon, and Sanjiv Kumar. Two-stage llm fine-tuning with less specialization and more generalization. _arXiv preprint arXiv:2211.00635_, 2022.
* [42] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.
* [43] Jonas Pfeiffer, Andreas Ruckle, Clifton Poth, Aishwarya Kamath, Ivan Vulic, Sebastian Ruder, Kyunghyun Cho, and Iryna Gurevych. Adapterhub: A framework for adapting transformers. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020): Systems Demonstrations_, pages 46-54. Association for Computational Linguistics, 2020.
* [44] Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. Scaling down to scale up: A guide to parameter-efficient fine-tuning. _arXiv preprint arXiv:2303.15647_, 2023.
* [45] Almog Gueta, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, and Leshem Choshen. Knowledge is a region in weight space for fine-tuned language models. _arXiv preprint arXiv:2302.04863_, 2023.

* [46] Michael S Matena and Colin A Raffel. Merging models with fisher-weighted averaging. _Advances in Neural Information Processing Systems_, 35:17703-17716, 2022.
* [47] Andrew K Lampinen and Surya Ganguli. An analytic theory of generalization dynamics and transfer learning in deep linear networks. _arXiv preprint arXiv:1809.10374_, 2018.
* [48] Nilesh Tripuraneni, Michael Jordan, and Chi Jin. On the theory of transfer learning: The importance of task diversity. _Advances in neural information processing systems_, 33:7852-7862, 2020.
* [49] Federica Gerace, Luca Saglietti, Stefano Sarao Mannelli, Andrew Saxe, and Lenka Zdeborova. Probing transfer learning with a model of synthetic correlated datasets. _Machine Learning: Science and Technology_, 3(1):015030, 2022.
* [50] Wesley Maddox, Shuai Tang, Pablo Moreno, Andrew Gordon Wilson, and Andreas Damianou. Fast adaptation with linearized neural networks. In _International Conference on Artificial Intelligence and Statistics_, pages 2737-2745. PMLR, 2021.
* [51] Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang. Fine-tuning can distort pretrained features and underperform out-of-distribution. _arXiv preprint. arXiv:2202.10054_, 2022.
* [52] Greg Yang and Edward J Hu. Feature learning in infinite-width neural networks. _arXiv preprint arXiv:2011.14522_, 2020.
* [53] Sadhika Malladi, Alexander Wettig, Dingli Yu, Danqi Chen, and Sanjeev Arora. A kernel-based view of language model fine-tuning. In _International Conference on Machine Learning_, pages 23610-23641. PMLR, 2023.
* [54] Jeevesh Juneja, Rachit Bansal, Kyunghyun Cho, Joao Sedoc, and Naomi Saphra. Linear connectivity reveals generalization strategies. _arXiv preprint. arXiv:2205.12411_, 2022.
* [55] Ekdeep Singh Lubana, Eric J. Bigelow, Robert P. Dick, David Krueger, and Hidenori Tanaka. Mechanistic Mode Connectivity, 2022. Comment: 39 pages.
* [56] Charles Lovering, Rohan Jha, Tal Linzen, and Ellie Pavlick. Predicting inductive biases of pre-trained models. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=mNtmaDkAr.
* [57] Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. _arXiv preprint arXiv:2012.13255_, 2020.
* [58] Suhas Kotha, Jacob Mitchell Springer, and Aditi Raghunathan. Understanding catastrophic forgetting in language models via implicit inference. _arXiv preprint arXiv:2309.10105_, 2023.
* [59] Haoyu Zhao, Abhishek Panigrahi, Rong Ge, and Sanjeev Arora. Do transformers parse while predicting the masked word? _arXiv preprint arXiv:2303.08117_, 2023.
* [60] Kenneth Li, Aspen K. Hopkins, David Bau, Fernanda Viegas, Hanspeter Pfister, and Martin Wattenberg. Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task, 2023. Comment: ICLR 2023 oral (notable-top-5%): https://openreview.net/forum?id=DeG07_TcZvT ; code: https://github.com/likenneth/othello_world.
* [61] Neel Nanda, Andrew Lee, and Martin Wattenberg. Emergent linear representations in world models of self-supervised sequence models. _arXiv preprint arXiv:2309.00941_, 2023.
* [62] Bingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers learn shortcuts to automata. _arXiv preprint arXiv:2210.10749_, 2022.
* [63] Josef Valvoda, Naomi Saphra, Jonathan Rawski, Adina Williams, and Ryan Cotterell. Benchmarking compositionality with formal languages. In _Proceedings of the 29th International Conference on Computational Linguistics_, pages 6007-6018, 2022.

* Liu et al. [2023] Bingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Exposing attention glitches with flip-flop language modeling. _arXiv preprint arXiv:2306.00946_, 2023.
* Zhou et al. [2023] Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Josh Susskind, Samy Bengio, and Preetum Nakkiran. What algorithms can transformers learn? a study in length generalization. _arXiv preprint arXiv:2310.16028_, 2023.
* Chan et al. [2022] Stephanie Chan, Adam Santoro, Andrew Lampinen, Jane Wang, Aaditya Singh, Pierre Richemond, James McClelland, and Felix Hill. Data distributional properties drive emergent in-context learning in transformers. _Advances in Neural Information Processing Systems_, 35:18878-18891, 2022.
* Allen-Zhu and Li [2023] Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.1, knowledge storage and extraction. _arXiv preprint arXiv:2309.14316_, 2023.
* Allen-Zhu and Li [2023] Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.2, knowledge manipulation. _arXiv preprint arXiv:2309.14402_, 2023.
* Okawa et al. [2023] Maya Okawa, Ekdeep Singh Lubana, Robert P Dick, and Hidenori Tanaka. Compositional abilities emerge multiplicatively: Exploring diffusion models on a synthetic task. _https://openreview.net/forum?id=ZXH8KUgFx3_, 2023.
* Qi et al. [2023] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. Fine-tuning aligned language models compromises safety, even when users do not intend to! _arXiv preprint arXiv:2310.03693_, 2023.
* Yang et al. [2023] Xianjun Yang, Xiao Wang, Qi Zhang, Linda Petzold, William Yang Wang, Xun Zhao, and Dahua Lin. Shadow alignment: The ease of subverting safely-aligned language models. _arXiv preprint arXiv:2310.02949_, 2023.
* [72] Anonymous. Dissecting learning and forgetting in language model finetuning. In _Submitted to The Twelfth International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=tmsqb6WpLz. under review.
* [73] Anonymous. Learning and forgetting unsafe examples in large language models. In _Submitted to The Twelfth International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=hkQOYyUChL. under review.
* Molchanov et al. [2016] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional neural networks for resource efficient inference. _arXiv preprint arXiv:1611.06440_, 2016.
* Lubana and Dick [2021] Ekdeep Singh Lubana and Robert P. Dick. A gradient flow framework for analyzing network pruning. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=runv7QmLUue.
* Mozer and Smolensky [1988] Michael C Mozer and Paul Smolensky. Skeletonization: A technique for trimming the fat from a network via relevance assessment. _Advances in neural information processing systems_, 1, 1988.
* Wei et al. [2023] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail? _arXiv preprint arXiv:2307.02483_, 2023.
* Zou et al. [2023] Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. _arXiv preprint arXiv:2307.15043_, 2023.
* Touvron et al. [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajiwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.

Organization of Appendix

In the appendix we present a comprehensive analysis of our claims on Tracr, PCFG and TinyStories-Instruct using different mechanistic interpretability tools discussed in Section-F of the main paper. We also present a summary of the notations used in this work in Tab. 1. Overall, the appendix is organized as follows:

* Sec. C presents main results on Tracr and Tin0-stories to support our hypothesis. We also present addition results for PCFG in this section.
* Sec. D presents details of the Tracr, PCFG and Tiny Stories datasets utilized in this work.
* Sec. E presents the training and model details for each of the datasets considered.
* Sec. F lists the protocols used for different mechanistic interpretability tools like attention maps, probing, pruning and reverse fine-tuning.
* Sec. G provides a few more results in practically relevant contexts, such as in a synthetic jailbreaking setup.
* Sec. G.1 studies the effect of using different fractions of pre-training and fine-tuning data points for fine-tuning.
* Sec. G.2 presents the jailbreaking analysis using the PCFG setup.
* Sec. G.3 shows reverse fine-tuning a fine-tuned model is sample efficient compared to baselines for both PCFG and Tracr models.
* Sec. G.4 presents reverse fine-tuning analysis of a fine-tuning protocol that actively tries to remove a capability from PCFG / Tracr models.
* Sec. H presents detailed discussion of setup details and results on TinyStories.
* Sec. I presents additional results on Tracr for counter and max element tasks.
* Sec. J presents additional results on PCFG for the counting and index of occurrence tasks.

\begin{table}
\begin{tabular}{c|l} \hline \hline Notation & Meaning \\ \hline \(\mathrm{X}\) & Input domain \\ \(\mathrm{X}_{D}\) & Factor of the input domain that captures values of the inputs \\ \(\mathrm{X}_{I}\) & Factor of the input domain that captures task identifiers \\ \(\mathcal{P}_{\mathrm{X}}\) & Probability distribution over the input domain \\ \(\mathcal{P}_{\mathrm{Y}}^{\mathrm{X}}\) & The overall distribution defining the downstream fine-tuning task \\ \(\mathcal{P}_{\mathrm{Tr}}\) & Dataset used for fine-tuning \\ \(\mathcal{P}_{\mathrm{Y}\mathrm{T}}^{\mathrm{T}}\) & Empirical distribution from which the fine-tuning dataset is sampled \\ \(\mathbb{T}\) & Denotes a task to be performed by the model (e.g., count) \\ \(\mathbb{O}\) & Denotes an operand that will be processed by to perform the task \(\mathbb{T}\) \\ \(\mathbb{O}_{\mathrm{Tr}}\) & Set of operand tokens seen during pretraining \\ \(\mathbb{O}_{\mathrm{Tr}}\) & A specific token used as an operand during pretraining \\ \(\mathbb{O}_{\mathrm{Tr}}\) & A specific token used as an operand during fine-tuning \\ \(\mathbb{r}(x,0)\) & Denotes the result of executing a task from Sec. 3 on a string \(x\) for some operand \(0\) \\ \(C_{\mathrm{Tr}}\) & Probability that a randomly sampled string in the training data used for fine-tuning has a \\  & spurious correlation between the pretraining capability and the downstream task \\ \(C_{\mathrm{Ta}}\) & Probability that a randomly sampled string in the test data used for evaluating fine-tuned models \\  & has a spurious correlation between the pretraining capability and the downstream task \\ \(\mathcal{P}_{\mathbb{T}}(0)\) & Sampling prior. Denotes the probability that when a string with task token \(\mathbb{T}\) is sampled during pretraining, the operand to perform the task on is \(0\) \\ \(\mathcal{P}_{\mathcal{C}}^{H}\), \(\mathcal{P}_{\mathcal{C}}^{M}\), \(\mathcal{P}_{\mathcal{C}}^{S}\) & Sampling priors such that the probability of sampling the target token for fine-tuning (\(\mathbb{O}_{\mathrm{Tr}}\)) is \\  & high (\(\mathcal{P}_{\mathcal{C}}^{B}\)), medium (\(\mathcal{P}_{\mathcal{C}}^{M}\)), or small (\(\mathcal{P}_{\mathcal{C}}^{S}\)) \\ \(\eta_{M}\), \(\eta_{S}\), \(\eta_{VS}\) & Medium / Small / Very-small learning rates used for fine-tuning. \(\eta_{VS}\) is only used for a specific \\  & reverse fine-tuning experiment with Tracr compiled models. \\ \(n_{\texttt{iters}}\) & Dunotes reverse fine-tuning \\ LR & Number of iterations used during pre-training \\ \hline \hline \end{tabular}
\end{table}
Table 1: Notations used in this work.

## Appendix B Related Work

**Fine-tuning in the "foundation model" era.** Fine-tuning large-scale foundation models pretrained on huge datasets, such as LLMs [2; 3] or large vision models [34; 35], has become the norm in most domains of machine learning. Accordingly, several fine-tuning methods have been proposed in recent years, e.g., instruction fine-tuning [36; 37; 38], parameter-efficient fine-tuning [39; 40; 41], low-rank adaptation [42; 43; 44], and weight averaging [45; 46]. The diversity of these protocols makes fine-tuning a general, umbrella term for related methods used to adapt a pretrained model to elicit its most relevant capabilities. _For precision, we restrict this paper to fine-tuning protocols that continue training of a pretrained model on a smaller downstream dataset at a learning rate that is often one to three orders of magnitude lower than the average pretraining one._ Such protocols are widely used in practice, e.g., in instruction fine-tuning [36].

**Understanding fine-tuning.** A few papers theoretically analyze fine-tuning [47; 48; 49; 50; 51] under strong assumptions such as relatively simple model classes (e.g., linear functions) or a kernel view of deep learning, which, as shown by Yang and Hu [52], trivializes the notion of feature transfer in fine-tuning / transfer learning (though see Malladi et al. [53] for a notable exception). Prior works have also evaluated the effects of fine-tuning via the lens of mode connectivity [54; 55], behavioral evaluations [56], and intrinsic dimensionality of the loss landscape [57]. In contrast, we aim to provide a mechanistic analysis of how fine-tuning changes model capabilities. A relevant recent work by Kothea et al. [58] claims that fine-tuning is unlikely to alter a model's capabilities and supports this claim by providing a behavioral analysis of linear regression tasks and realistic models via a novel prompting strategy.

**Model interpretability via synthetic tasks.** Several recent works have focused on _mechanistically_ understanding how Transformers learn synthetic language generation tasks, such as learning formal grammars and board games [19; 59; 60; 61; 62; 63; 64; 65; 66]. The goal of such papers, including ours, is not necessarily to provide accurate explanations for the success of LLMs, but to develop concrete hypotheses that can be used to develop grounded experiments or tools for understanding their behavior. For example, in a recent work, Allen-Zhu and Li [67; 68] use a synthetically designed setup to develop hypotheses for how "knowledge" about an entity is stored in a pretrained model, showing such knowledge can often be manipulated via relatively simple linear transformations. Similarly, Okawa et al. [69] use a procedurally defined multimodal dataset to demonstrate that emergent capabilities seen in neural networks are partially driven by the compositional nature of real world data. In another work, Zhou et al. [65] utilize Tracr compiled Transformers to hypothesize and demonstrate that if primitive operations involved in a formal algorithm can be implemented by a model, length generalization if practically feasible.

## Appendix C Additional results

### Validating our Hypotheses on Tracr

**Attention map visualizations further corroborate the wrappers hypothesis.** As we noted before, the results discussed above remain consistent across other experimental setups for both Tracr and

Figure 4: **Visualizing attention maps of fine-tuned Tracr models.** Leftmost panel shows the Tracr compiled model’s attention map on the counter task. Upon fine-tuning under different spurious correlations, we see the model continues to pay attention to the pretraining target \(\overline{\bm{\mathsf{b_{\overline{p}}}}}{=}\)\(a\). Only when a large enough learning rate and zero spurious correlation is used, is there a change in the attention pattern.

PCFG models. However, by construction, Tracr yields particularly interpretable attention maps, allowing us to directly visualize the effects of fine-tuning. We thus analyze the attention maps of a Tracr model on the Counter task described in Sec. 3. Results are shown in Fig. 4. The original Tracr compiled model serves as a baseline and clearly demonstrates that all tokens only attend the pretraining target token, \(\overline{\texttt{Opt}}=a\). Upon fine-tuning to count \(\overline{\texttt{Opt}}=b\), we find the model clearly continues to pay attention to \(\overline{\texttt{Opt}}\) if a small learning rate is used. A larger learning rate is, however, able to alter the model computation, but only if the pretraining capability is not weakly relevant to the fine-tuning task, i.e., when \(C_{\texttt{Tr}}=0\); otherwise, we again find the model continues to pay attention to the pretraining target.

### Additional results validating our Hypotheses on PCFG

On probing, we observe that it is indeed possible to unwrap the wrapper learned by the models during fine-tuning. The results are shown in Fig-7. These observations therefore indicate that during fine-tuning the model can learn a minimal transform on its pre-training capabilities.

### Validating our Hypotheses on TinyStories

To give additional strength to our results, we perform an analysis using more realistic language models trained on the TinyStories-Instruct dataset [33] (see App. D.3 for an example). These models are able to follow specific instructions to write coherent English stories over multiple paragraphs. We perform experiments analogous to the reFT and probing experiments in the previous sections, but now explicitly focus on whether fine-tuning can _delete_ capabilities present in pre-trained models. Models are pre-trained to generate stories with specific story features (such as containing a twist, foreshadowing, or bad ending) and fine-tuned to _not_ generate stories with a specific feature (twist) (see App. H for details on the protocols). We probe these models to detect the deleted feature from the intermediate model outputs in Fig. 9, where the dynamics of loss during reFT on learning to generate stories with the deleted feature are also shown. We also report the percentage of stories

Figure 5: **Fine-tuning accuracy w.r.t. number of training iterations. We vary the probability of sampling the token \(\overline{\texttt{Opt}}\) in the pretraining data and the spurious correlation in the fine-tuning datasets. When the prior is sufficiently high (a, b), we find the model learns to perform well on the downstream task. Meanwhile, if the prior is low (c), the model learns the downstream task only if a high enough learning rate is used and the spurious correlation is imperfect. This indicates the ability to extract information relevant for the downstream task is likely to be exploited during fine-tuning.**

Figure 6: **Impact of sampling prior on the pretraining task’s accuracy as fine-tuning is performed. We plot accuracy on the pretraining task w.r.t. fine-tuning iterations. When the sampling prior of the \(\overline{\texttt{Opt}}\) is low during pre-training, the pretraining task accuracy quickly plummets, especially if the spurious correlation is high; having a high sampling prior mitigates this behavior. This _indicates_ pretraining capabilities are affected the most when they are weakly relevant.**

with the deleted feature generated by models during reFT in Table 2, where the generated stories are processed by a fine-tuned GPT-3.5 classifier to predict if the story contains the deleted feature (see App. H for details). Overall, we find that "deleted" capabilities can be easily and sample-efficiently recovered (compared to the baseline), i.e., stories with that feature can be generated again, regardless of the fine-tuning protocol used. These results support our hypotheses that fine-tuning only minimally alters pre-trained model capabilities. We also highlight a few recent papers that propose similar protocols as reFT and experiments as ours with further realistic settings [70, 71, 72, 73].

## Appendix D Additional details on Datasets

We consider three experimental setups: Compiled programs with Tracr [15], learning models on Probabilistic Context Free Grammars (PCFG) [19], and the TinyStories Instruct dataset.

Figure 8: **Reverse Fine-Tuning:** We set \(C_{\mathtt{Te}}\) to be 0 to test if the model performs well regardless of a spurious correlation. Models are fine-tuned for \(10\)K iterations. We observe that when a strongly relevant capability is present (a, b), the model very quickly (\(0.1\)-\(1\)K iterations) starts to perform well on the task via reFT, even if behavior relevant to the capability ceased during pretraining (e.g., when \(C_{\mathtt{Tr}}\) is 1). Meanwhile, when the model possesses a weakly relevant capability (c), this “revival” is _slightly_ slower (\(3\)K iterations). In contrast, the Scr. \(+\) FT baseline only reaches perfect accuracy at \(4.5\)K iterations and when using a larger learning rate, i.e., \(\eta_{M}\).

Figure 7: **Probing the presence of pre-training (top) and fine-tuning (bottom) capabilities.** We plot probe accuracy versus the index of the block in the Transformer model. \(C_{\mathtt{Te}}\) is set to 0. The pretrained model (left) acts as a baseline for the trend of performance through the model’s blocks. In most scenarios, we find we can infer the count of \(\overline{\mathtt{Opt}}\) with a similar trend as the pretrained model (left). A drop in performance is observed only when learning rate \(\eta_{M}\) is used with a weakly relevant capability (low sampling prior). This indicates pretraining capabilities continues to persist upon fine-tuning.

### Tracr Details

Tracr [15] generates a transformer model using the RASP library by [16]. The specific code snippet used to generate the Tracr models for the counting and the max element tasks are shown in Fig. 1 and Fig. 2 respectively. The models corresponding to these tasks is implemented with three standard transformer blocks, where each block consists of a self-attention layer followed by two MLP layers.

We analyze the following two tasks to understand the effects of fine-tuning on a pretrained model's capabilities.

* **Counter:** Compile the capability to count the number of occurrences of a token \(\overline{\texttt{pref}}\) in a string into the model; fine-tune to count occurrences of another token \(\overline{\texttt{pref}}\). If \(\texttt{r}(x,\texttt{0})\) denotes the number of occurrences of a token \(\texttt{0}\) in a string \(x\), the spurious correlation is defined by enforcing a constant difference in token occurrences, i.e., \(\texttt{r}(x,\overline{\texttt{pref}})-\texttt{r}(x,\overline{\texttt{pref}}) =\texttt{q}\). See also Alg. 1 and Fig. 10.
* **Max-identifier:** Compile the capability to identify the \(\overline{\texttt{pref}}\)-th largest element in a string; fine-tune to identify the \(\overline{\texttt{pref}}\)-th largest element. If \(\texttt{r}(x,\texttt{0})\) reads out the \(\texttt{0}\)-th largest token in the string \(x\), we define the spurious correlation as \(\texttt{r}(x,\overline{\texttt{pref}})-\texttt{r}(x,\overline{\texttt{pref}}) =\texttt{q}\); e.g., if \(q=1\) and the \(\overline{\texttt{pref}}\) largest token in the string \(x\) is \(\texttt{a}\), then the \(\overline{\texttt{pref}}\)-th largest token will be \(\texttt{b}\) (which is equal to \(\texttt{a}+1\) in Tracr's vocabulary). See also Alg. 2 and Fig. 11.

The fine-tuning data is generated by randomly sampling tokens from a uniform distribution over the input vocabulary. For the Counter task, the input vocabulary consists of first nine letters from the

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline Deletion Type & \multicolumn{4}{c}{Twist Proportion at Iteration} \\ \hline  & 0 & 30 & 300 & 3000 \\ \hline F (\(\eta_{M}\)) & 44\% & 81\% & 81\% & 82\% \\ F + R (\(\eta_{M}\)) & 12\% & 56\% & 69\% & 75\% \\ F + MM (\(\eta_{M}\)) & 31\% & 88\% & 50\% & 75\% \\ \hline F (\(\eta_{S}\)) & 69\% & 88\% & 75\% & 94\% \\ F + R (\(\eta_{S}\)) & 12\% & 44\% & 81\% & 81\% \\ F + MM (\(\eta_{S}\)) & 50\% & 81\% & 62\% & 81\% \\ \hline Not in PT & 12\% & 31\% & 44\% & 81\% \\ \hline \hline \end{tabular}
\end{table}
Table 2: **TinyStories reFT Analysis**. We report the percent of generations with a twist during reverse fine-tuning for the twist capability. F, R, and MM stand for our three fine-tuning protocols: Filtering, Randomisation and Mix & Match (see App. H for details). Regardless of learning rate and protocol, models relearn to generate stories with twist more sample-efficiently than the control model pre-trained on data w/o twists and fine-tuned to generate them (Not in PT).

Figure 9: **Validation on TinyStories. Models are trained to produce stories with several features (e.g., foreshadowing) and fine-tuned via different protocols to not produce stories with a “forbidden” feature (specifically, twists) (see App. H for details). Left: We probe the existence of this feature at intermediate Transformer blocks. Probe accuracy on models pre-trained with or without twist data (_Present/Not in Pretraining_, respectively) act as upper and lower bounds on the expected accuracy, and are plotted for ease of comparison. Regardless of the fine-tuning protocol (Filtering, Filtering + Randomisation, Filtering + Mix & Match), for the lower LR, no protocol removes a meaningful amount of information and a similar but less strong trend holds for the higher LR. Right: We plot the loss during reverse fine-tuning (reFT) to again produce stories with the forbidden feature. Fine-tuned models’ loss goes down very quickly (30–300 iterations) compared to baselines (which never reach the same loss; also see Tab. 2). Both these results indicate the capability of identifying the forbidden feature, a necessary capability for story modelling, continues to persist after fine-tuning.**English alphabet. For the max element task, the input vocabulary consists of all the letters in the English alphabet. We sample with replacement for the Counter task and without replacement for the max element task (to avoid having multiple max elements). Examples for the task are shown in Figs. 10, 11.

``` defcountA():  #binarizethetokensinto0'sand1's  bin=(rasp.tokens=='a')  #Selecttheindicesoftokenswithvalueof1  bin_idx=rasp.Select(bin,rasp.indices,rasp.Comparison.EQ)  #Countthenumberofselectedindices  count_a=rasp.SelectorWidth(bin_idx)  #Generateanidentitymap  idx_select=rasp.Select(rasp.indices,rasp.indices,rasp.Comparison.EQ)  #Outputthecount  sum=rasp.Aggregate(idx_select,count_a) ```

**Task:**Countb  Sample:$,c,a,d,a,b,c,b,a,d,f,b,g,c,e,b,b,a,h,j,i,b,d,e,f,i,h,f,e,g,a,b,g,f,h, j,c,b,e,d,h,j,i,b,a,b,#,

**Answer:** 10

**Algorithm 1**Pseudocode for compiling the Counter capability via Tracr: Rasp code used to generate the model for the Counter capability and task via Tracr

### Pcfg

We follow [19] and use the production rules shown in Fig. 12. We sample a string of tokens from the grammar and then randomly subsample a string of 250 tokens from the generated original sequence (this helps remove bias towards tokens likely to be at the beginning). The sampling probabilities to sample a valid rule given the parent node is fixed to be 0.5. We formulate the training data as follows: Start of sequence token (SOS) + Task family token (e.g., Counting) (T) + Operand one (counting what) (0) + Operand two (number of positions) (0\({}^{\prime}\)) + Start of text token (\(SOT\)) + Data generated from DGP (Txt) + End of text token (EOT) + Answer request token (ART) + Answer token (Ans) + End of sequence token (EOS). This can be summarized as follows.

\[\textbf{Sample input:SOS}+\texttt{T}+\texttt{0}+\texttt{0}^{\prime}+\texttt{SOT }+\texttt{Txt}+\texttt{EOT}+\texttt{ART}+\texttt{Ans}+\texttt{EOS}.\] (1)

We consider the following tasks for pre-training:

* **Counting (C):** Counting number of 0 (say a) for the last 0\({}^{\prime}\) positions (forty). This example will be written as Ca40.
* **Counting composition of elements (CC):** Counting number of 0 (say aa) for the last 0\({}^{\prime}\) positions (forty). This example will be written as CCa40.

Figure 10: **Exemplar for Counter Task: A sample used for fine-tuning Tracr compiled models on counting ‘b’.**

* **Index of occurrence (I):** Index from the EOT token when 0 (say a) occurred for the 0\({}^{\prime}{}^{th}\) time (sixth). This example will be written as Ia10.
* **Index of occurrence of composition element (IC):** Index from the EOT token when 0 (say aa) occurred for the 0\({}^{\prime}{}^{th}\) time (sixth). This example will be written as ICA10.
* **Token value at an index (T):** The token value at index 0\({}^{\prime}\) (forty) before the end token. 0 is NULL here. This example will be written as TNULL5.

For the "Counting", "Counting composition of elements", and "Token value at index" tasks, we set the value of 0\({}^{\prime}\) token as 40. For "Index of occurrence" and "Index of occurrence of composition element" task, we set the value of 0\({}^{\prime}\) token as 6. All five tasks above are considered during pre-training, but for fine-tuning we consider only a single task with a given operand. Specifically, we analyze fine-tuning the pre-trained models on the "Counting" and "Index of occurrence" tasks only.

We analyze the following two tasks to understand the effects of fine-tuning on a pretrained model's capabilities.

* **Counter:** We intentionally reuse this task to demonstrate the effects of compilation of the capability via Tracr versus learning the capability via PCFGs. Instead of being compiled, the model is trained to count the number of tokens from a _set_ of tokens \(\{\overline{\texttt{O}_{\texttt{T}}}\}\). The model is then fine-tuned to exclusively count a \(\overline{\texttt{O}_{\texttt{T}}}\in\{\overline{\texttt{O}_{\texttt{T}}}\}\) token. By making the sampling probability of tokens high during pretraining, we can make the model preemptively performant on the downstream task; this allows us to model the notion of capability relevance.
* **Indexer:** Amongst other tasks, the model is pretrained to output the index (location in a string) of a token from the _set_\(\{\overline{\texttt{O}_{\texttt{T}}}\}\) occurs for the \(\texttt{k}^{\text{th}}\) time; fine-tuning is performed to output the index of \(\texttt{k}^{\text{th}}\) occurrence of another token \(\overline{\texttt{O}_{\texttt{T}}}\) instead. We arbitrarily set \(\texttt{k}\) to \(6\) for our experiments, but emphasize that any integer less than context size can be used. If \(\texttt{r}(x,\texttt{0})\) denotes the index of \(\texttt{k}^{\text{th}}\) occurrence of a token 0 in a string \(x\), the spurious correlation is enforced via constant offset q in operand token indices, i.e., \(\texttt{r}(x,\overline{\texttt{O}_{\texttt{T}}})-\texttt{r}(x,\overline{ \texttt{O}_{\texttt{T}}})=\texttt{q}\).

While the pre-training dataset is generated by simply sampling from PCFG (see Fig. 13 for an example), for generating the fine-tuning dataset we provide explicit control over the value of \(C_{\texttt{T}_{\texttt{T}}}\) by artificially adding the target tokens \(\overline{\texttt{O}_{\texttt{T}}}\) from the fine-tuning task. It is important to ensure that the distribution shift between the fine-tuning distributions with different values of \(C_{\texttt{T}_{\texttt{T}}}\) is minimized and the data is fairly spread across multiple classes to enable reusability of feature via pretraining. As shown in Fig. 14, the class distribution of the datasets with \(C_{\texttt{T}_{\texttt{T}}}=1\) and \(C_{\texttt{T}_{\texttt{T}}}=0\) for the counting and the index of occurrence tasks satisfies these requirements.

Figure 11: **Exemplar for Max-Element Task: A sample used for fine-tuning Tracr compiled models on the Max identifier task.**

Figure 12: **PCFG setup: Grammar rules considered to generate the PCFG dataset. The highlighted token represents the parent token. These rules have been adapted from Allen-Zhu and Li [19].**

### TinyStories Instruct

For the TinyStories results, we use the TinyStories Instruct variant of the dataset [33]1. An example from this dataset is given in Fig. 15. For the fine-tuning datasets, we take the original dataset and alter it in several ways. Details are discussed in App. H.

Footnote 1: https://huggingface.co/datasets/roneneldam/TinyStoriesInstruct

## Appendix E Details on Training and Evaluation

### Tracr

**Compiled Model Details:** The compiled model obtained for the counting and max identifier tasks consists of three blocks, wherein each block contains a single head attention layer followed by two layer MLP. No normalization layers are used by models developed using Tracr.

**Training details:** The compiled model is fine-tuned using SGD with momentum for 10K iterations with a batch size of 96. Tracr yields models with a rather sparse parameterization, which often yields unstable training dynamics (e.g., gradient spikes), especially with adaptive optimizers. To address this, we perform the following two interventions. First, we add a small amount of initial gaussian noise \(w_{noise}\in\mathcal{N}(0,0.001)\) to the weights of the compiled model to density them slightly. Note that the scale of this noise is not high, i.e., it avoids any performance loss but is sufficient enough to reduce gradient spikes resulting from extreme sparsity of model parameters. Second, we choose to use on SGD with momentum as the optimizer, using the following four choices of learning rates: Large

Figure 14: **Distribution of the class labels for Counting (first row) and Index of occurrence tasks (second row). (a) shows the distribution for the operand token a and (b) shows the same for the operand token b. The data is similarly distributed across different classes and the distribution shift for the two operands and the different values of \(C_{\texttt{T}\texttt{r}}\) is small.**

Figure 13: **PCFG Exemplar. A representative sample from the PCFG dataset [19]**LR (\(10^{-1}\)), Medium LR (\(10^{-2}\)), Small LR (\(10^{-3}\)), and Very Small LR (\(10^{-4}\)). The characterization of "Large" or "Small" is based on a general heuristic of what learning rate regimes are commonly used with SGD in modern neural network training. Linear warmup is used for 2K iterations followed by a cosine schedule with a minimum learning rate of the order \(10^{-2}\) smaller than its max value. Evaluation of the fine-tuned model is done on both test set with and without the spurious correlation (ie. \(C_{\texttt{Te}}=0\) and \(C_{\texttt{Te}}=1\)).

### Pcfg

**Model details:** We use the minGPT model by [23] for all experiments on the synthetically generated PCFG dataset, similar to [19]. The model has close to 3 million parameters and consists of 6 blocks each made up of multihead self attention with 6 heads and two layers of MLP layers with an embedding dimension of 192.

**Pre-training details:** Pretraining is performed from scratch with a learning rate of \(10^{-3}\) using the standard AdamW optimizer. Cosine learning rate is used along with linear warmup, where the warmup is used in the first 20% of the training. The model is trained using the standard next token prediction task used for training language models. We consider the set of five tasks mentioned in the previous section during the pre-training phase, but focus on only one of these tasks during fine-tuning. We use the task family token and an operand token to define the notion of a task. The task family token is sampled from a uniform distribution, while the operand token (0) is sampled from a multinomial distribution. The sampling probability for different operands is varied in the experimental setup to understand the effect of capability relevance in fine-tuning. More specifically, we analyze the following distributions for sampling the operand tokens (a, b, c):

* \(\mathcal{P}_{\texttt{T}}\left(\texttt{a}\right)=0.999,\;\mathcal{P}_{\texttt{T }}\left(\texttt{b}\right)=0.001,\;\mathcal{P}_{\texttt{T}}\left(\texttt{c} \right)=0.0\);
* \(\mathcal{P}_{\texttt{T}}\left(\texttt{a}\right)=0.99,\;\mathcal{P}_{\texttt{T }}\left(\texttt{b}\right)=0.1,\;\mathcal{P}_{\texttt{T}}\left(\texttt{c} \right)=0.0\);
* \(\mathcal{P}_{\texttt{T}}\left(\texttt{a}\right)=0.9,\;\mathcal{P}_{\texttt{T }}\left(\texttt{b}\right)=0.1,\;\mathcal{P}_{\texttt{T}}\left(\texttt{c} \right)=0.0\);
* \(\mathcal{P}_{\texttt{T}}\left(\texttt{a}\right)=0.7,\;\mathcal{P}_{\texttt{T }}\left(\texttt{b}\right)=0.2,\;\mathcal{P}_{\texttt{T}}\left(\texttt{c} \right)=0.1\); and
* \(\mathcal{P}_{\texttt{T}}\left(\texttt{a}\right)=0.5,\;\mathcal{P}_{\texttt{T }}\left(\texttt{b}\right)=0.3,\;\mathcal{P}_{\texttt{T}}\left(\texttt{c} \right)=0.2\).

For each of the configurations of sampling distributions of operands, we pre-train the model for 10K, 50K, 100K and 200K iterations. The model is trained in an online fashion to model the standard language model training pipeline, i.e., data is sampled on the fly from the data generating process during training time.

**Fine-tuning details:** While pre-training is done in the next token prediction fashion, fine-tuning is done in a supervised way where the model is required to just perform the desired fine-tuning task.

Figure 15: **TinyStories Exemplar. An example from the TinyStories Instruct dataset [33].**

We use the final iteration model obtained from pre-training as the initialization for fine-tuning. While pre-training is done on multiple pairs of task and operand tokens, the model is fine-tuned on a single pair of task and operand tokens. To simulate a similar setup for fine-tuning as in Tracr, we analyze the effect of fine-tuning the model using three different sets of learning rate: Large LR (\(\eta_{L}\): \(10^{-4}\)), Medium LR (\(\eta_{M}\): \(10^{-5}\)) and Small LR (\(\eta_{S}\): \(10^{-6}\)). Fine-tuning is done for 10K iterations using AdamW optimizer with a batch size of 96 samples. Similar to pre-training phase, we use cosine learning rate with an initial warmup of 20% of the fine-tuning iterations. The minimum value of the learning rate is set to be 100\(\times\) lower than the maximum learning rate. Similar to Tracr evaluation is done on both the test sets with and without the spurious correlation (\(C_{\texttt{Te}}=0\) and \(C_{\texttt{Te}}=1\)).

## Appendix F Mechanistic Interpretability Tools Setup

In this section, we describe the different tools of interpretability considered in our work.

**Attention Maps:** We present the attention maps for different tasks considered in the Tracr setup. Each map shows the tokens which are attending other tokens on the y axis and the token which are being attended to on the x-axis. If a token is attended by many other tokens, then, in a crude sense, this can imply that the presence of the token is impacting the underlying task performed by the model. In the Counter task, if significant attention is given to a's / b's is an indicator of the respective capability of the model. For the max identifier task, in the attention map in Block-0, the model implements the sorting function, where each token is attended by the tokens which are greater than that. The vocabulary order followed is a \(>\) b \(>\) c \(>\) d.... In the attention map of Block-2, the model implements the read function, where it outputs the token at the desired position in the sorted sequence.

**Pruning:** We consider single step pruning where we prune the weights/neurons with largest dot product between their gradient and weights, where the gradients are calculated by minimizing the loss for the capability we want to revive. More formally, let the weights of the model with \(N\) parameters be given by \(w_{i}\) where \(i\in\{0,1,\ldots,N-1\}\), Let the corresponding gradient be given by \(grad(w_{i})\) then the top-K weights with largest value of \(grad(w_{i})w_{i}\) are pruned off. This follows the pruning protocols proposed in prior work for reducing or preserving loss via pruning [74, 75, 76]. We use weight pruning for the Tracr setup and neuron pruning for PCFG, where a neuron is defined as a row in the weight matrix. We present a detailed description of the pruning protocol considered in Algorithm-3.

``` defprune(): #Forwardpropthemodelonpre-trainingtask out=\(f_{\theta}(\overline{\texttt{Opt}})\circ X)\) #Calculatetheloss \(\mathcal{L}\)=CE(out,y) #Calculatethegradients grad=\(\nabla_{\theta}\mathcal{L}\) #Calculatethedotproductbetweenmodelweightsandgradients dotproduct=\(\theta\).grad #SelecttheindicesoftopKvalues indices=Top\({}_{K}\)(dotproduct) #Pruneofftheneurons/weightspresentintopKindices \(\theta\)[indices]=0 return\(\theta\) ```

**Probing:** Probing is used to understand if a particular capability is present in the model. In this, we train a linear layer (probe) on top of every block (residual layer's output) of the mini-gpt model and analyze if the probe is able to perform on a task requiring the use of the desired capability. The probe is a linear layer with the output dimensions same as the vocabulary size of the model. The probe is trained using the data randomly sampled from the PCFG data generating process for 4K iterations

[MISSING_PAGE_FAIL:20]

Figure 19: **Effect of different sampling probabilities of pre-training target token \(\overline{\texttt{O}_{\texttt{PT}}}\) on pre-tuning task’s performance.** Pre-training is done using high sampling prior for fine-tuning task family token. We observe similar gains for different values of sampling probabilities of \(\overline{\texttt{O}_{\texttt{PT}}}\) during fine-tunning.

Figure 20: Probing analysis corresponding to Fig-16 and 17

Figure 18: **Effect of different sampling probabilities of pre-training target token \(\overline{\texttt{O}_{\texttt{PT}}}\) on fine-tuning task’s performance.** Pre-training is done using high sampling prior for fine-tuning task family token. We observe similar gains for different values of sampling probabilities of \(\overline{\texttt{O}_{\texttt{PT}}}\) during fine-tunning.

Figure 21: Probing analysis corresponding to Fig-18 and 19

### Jailbreaking Analysis

We emulate jailbreaking [77; 78] in our PCFG setup by defining several task family tokens describing the same task. Specifically, for the "Counter" task, we use three task family tokens \(\mathtt{T}_{NJ},\mathtt{T}_{J_{1}},\mathtt{T}_{J_{2}}\) to refer to the task in a string. Here subscript \(NJ\) indicates the task family token will not allow jailbreaking, while \(J_{1}\)/\(J_{2}\) indicate the task family token can be used to jailbreak the model, as explained next. For pretraining, the token \(T_{NJ}\) may be paired with operand tokens \(\mathtt{a},\mathtt{b},\mathtt{c}\) to learn to count them from inputs sampled from the PCFG. However, tokens \(T_{j_{1}},T_{j_{2}}\) are used only for counting \(\mathtt{a}\). During fine-tuning, the model is fine-tuned to count the token \(\mathtt{b}\) using the task family token \(\mathtt{T}_{NJ}\). For evaluation, we compute the model's accuracy on its ability to count the token \(\mathtt{a}\), using either the task family token \(\mathtt{T}_{NJ}\) or \(\mathtt{T}_{J_{1}},\mathtt{T}_{J_{2}}\). As shown in Fig. 22, the model is unable to infer the count of a if the task family token \(\mathtt{T}_{NJ}\) is used; however, if task family tokens \(\mathtt{T}_{J_{1}},\mathtt{T}_{J_{2}}\) are used, the model performs perfectly if the prior for sampling the fine-tuning target \(\mathtt{b}\) during pretraining was sufficiently high. We argue that this is expected because under a high sampling prior breaks the symmetry between task family tokens (indeed, \(\mathtt{T}_{J_{1}}\) is only seen with operand token \(\mathtt{a}\), but \(\mathtt{T}_{NJ}\) is seen for all operand tokens. This indicates the pretraining capability continues to persist in the model, enabling jailbreaking. To further investigate this result, we also probe the fine-tuned models. Results are shown in Fig. 23. As expected, we see task family tokens \(\mathtt{T}_{J_{1}},\mathtt{T}_{J_{2}}\) allow for linear readout of the count of \(\mathtt{a}\); however, we see that even for inputs with task family token \(\mathtt{T}_{NJ}\), the model does encode the count of \(\mathtt{a}\) in the outputs around the middle layers!

Figure 23: **Probing analysis for the setup used to understand jail-breaking. Similar results on using the fine-tuning token or the jailbreaking token for training the probe indicate that the pre-training capabilities are not removed on fine-tuning.**

Figure 22: **Jailbreaking analysis using PCFG. We report performance on the pretraining task (counting \(\overline{\mathtt{Opt}}\)) as a function of fine-tuning iterations, where the fine-tuning task (counting \(\overline{\mathtt{Opt}}\)) is performed using the task family token \(\mathtt{T}_{\mathtt{NJ}}\). We find that the model is able to learn the fine-tuning task and seemingly performs poorly on the pretraining task when task family token \(\mathtt{T}_{\mathtt{NJ}}\) is used in the input. However, in presence of a sufficiently relevant capability (high pretraining prior for \(\overline{\mathtt{Opt}}\)), using task family tokens \(\mathtt{T}_{J_{1}}\) or \(\mathtt{T}_{J_{2}}\) in the input shows the model can still perform the pretraining task perfectly—i.e., we can jailbreak the model.**

### Sample efficiency analysis for Reverse Fine-tuning

To emphasize the fact that the pretraining capability is "revived" in the model relatively sample-efficiently, we repeat Fig. 8, where models trained on PCFG are reverse fine-tuned, and repeat the experiment with the Scr. \(+\) FT baseline for Tracr compiled models. As can be seen in Figs. 24, 25, compared to the baseline, the model learns to perform the pretraining task in substantially fewer iterations than the baseline. We note that for the Tracr models in these results, even an extremely small learning rate is sufficient to revive the pretraining capability! We also note that we do not sweep over the \(C_{\texttt{\footnotesize{T}}\texttt{\footnotesize{T}}}\) hyperparameter in the Tracr models because they are compiled, i.e., we cannot control the correlation with the pretraining capabilities in a meaningful way.

Figure 24: **Reverse Fine-Tuning on PCFGs:** We set \(C_{\texttt{\footnotesize{T}}\texttt{\footnotesize{e}}}\) to be 0 to test if the model performs well regardless of a spurious correlation. We observe that when a strongly relevant capability is present (a, b), the model very quickly (\(0.1\)–\(1\)K iterations) starts to perform well on the task via reFT, even if behavior relevant to the capability ceased during pretraining (e.g., when \(C_{\texttt{\footnotesize{T}}\texttt{\footnotesize{T}}}\) is 1). Meanwhile, when the model possesseses a weakly relevant capability (c), this “revival” is _slightly_ slower (\(3\)K iterations). In contrast, the Scr. \(+\) FT baseline only reaches perfect accuracy at \(4.5\)K iterations and when using a larger learning rate \(\eta_{M}\).

Figure 25: **Reverse Fine-Tuning on Tracr:** We set \(C_{\texttt{\footnotesize{T}}\texttt{\footnotesize{e}}}\) to be 0 to test if the model performs well regardless of a spurious correlation. We observe that the fine-tuned model upon reFT  very quickly starts starts to perform well on the pretraining task. Moreover, the protocol works even if an extremely small learing rate is used. In contrast, the Scr. \(+\) FT baseline only reaches a large learning rate \(\eta_{M}\) is used, and does so less sample efficiently. We note that the results for \(\eta_{M}\) learning rate look worse than the \(\eta_{S}\) learning rate around \(10^{3}\) iterations because \(\eta_{M}\) is too big of a learning rate, forcing the model to essentially go through a “retraining” phase.

### Reverse Fine-tuning a more safety-oriented fine-tuning protocol

The fine-tuning protocols used in the bulk of the paper focus on learning a new capability, e.g., counting a new operand, while promoting reusability of capabilities learned during pretraining. Part of our motivation is to see if a pretrained model is actively forced to remove a capability, does that work? To analyze this, we define a fine-tuning protocol called randFT wherein the model is trained to actively produce an incorrect output for inputs that require use of the pretraining capability. For example, if the model possesseses the capability to produce the count the number of occurrences of token \(\overline{\texttt{0}_{\texttt{PT}}}=\texttt{a}\) in a string, we fine-tune it to produce the count of tokens \(\overline{\texttt{0}_{\texttt{PT}}}=\texttt{b}\) in that string. We analyze these fine-tuned models analyzed via reverse fine-tuning (reFT ), i.e., by further training them to produce the correct outputs (number of occurrences of token \(\overline{\texttt{0}_{\texttt{PT}}}\)). We provide results for three baselines as well: (i) Scr., wherein the model is trained from scratch to learn to count the token \(\texttt{a}\); (ii) Scr. + FT, wherein the model is initialized with parameters trained via trained from scratch to count a separate token (\(\overline{\texttt{0}_{\texttt{PT}}}\)) and then the model is fine-tuned to count the token \(\overline{\texttt{0}_{\texttt{PT}}}\); and (iii) reFT, which follows reverse fine-tuning models that were fine-tuned with the protocols used in the bulk of the paper, i.e., fine-tuned to learn a new capability that is related to the pretraining one.

Results are shown in Fig. 26. We specifically zoom in on the the scenario where reFT takes the longest time, i.e., when the sampling prior of the downstream target \(\overline{\texttt{0}_{\texttt{PT}}}\) is low in pretraining data; results for other sampling priors are shown in Fig. 27 We see that reverse fine-tuning a randFT model is similarly sample-efficient as the standard reFT pipeline used in the bulk of the paper, while being more sample-efficient than the Scr. and Scr. + FT baselines.

In addition, we perform a probing analysis of the randFT models in Fig. 28. We again find that we can predict the information relevant for the pretraining task, i.e., the count of \(\overline{\texttt{0}_{\texttt{PT}}}\).

Figure 27: **Reverse fine-tuning performance on using randFT fine-tuning protocol to forget the pre-training capability. We follow the setup of Fig. 8 and plot results for several sampling priors of the target token for fine-tuning, i.e., \(\overline{\texttt{0}_{\texttt{PT}}}\), but we use randFT for fine-tuning the models before reFT. The baseline results Scr. + FT are copied from the Fig. 26, i.e., baseline is not trained in an “adversarial” way, but the randFT results are. While this makes the baseline unfairly stronger, we find reFT  the randFT models are still more sample efficient.**

Figure 26: **Reverse fine-tuning a model fine-tuned to remove its pretraining capability. See text in Sec. G.4 for details.**Figure 28: **Probing analysis of randFT fine-tuning protocol.** We plot probe accuracy versus the index of the block in the Transformer model. \(C_{\texttt{Te}}\) is set to 0. The pretrained model (left) acts as a baseline for the trend of performance through the model’s blocks. In most scenarios, we find we can infer the count of \(\texttt{\texttt{\texttt{\texttt{\texttt{\texttt{\texttt{\texttt{\texttt{\texttt{ \texttt{\texttt{\texttt{\texttt{\texttt{\texttt{\texttt{\texttt{\texttt{\texttt{\texttt{ \texttt{ }}}}}}}}}}}}}}\) with a similar trend as the pretrained model (left). A drop in performance is observed only when learning rate \(\eta_{M}\) is used with a weakly relevant capability (low sampling prior). This indicates pretraining capabilities continues to persist upon fine-tuning.

Details and Results on TinyStories Experiments

In this section, we describe our experiments on the TinyStories dataset in more detail. These experiments are designed to validate our hypotheses in a more realistic language modelling setting. Overall, the results support our hypothesis that fine-tuning does not lead to deletion of capabilities as they can be revived in a sample-efficient way and uncovered through probing.

### Model Training

Dataset.We use the TinyStories [33] dataset to train our models. This data consists of children's stories written by GPT-3.5 and GPT-4. Each story is several paragraphs long, and comes with several attributes labelled: a set of three words that are included in the story; a sentence that is included in the story; a GPT-3.5-written summary of the story; and a list of 0-3 "story features", such as Twist, Dialogue or Bad Ending, which the story abides by.

We use the TinyStories-Instruct version of this dataset 2, wherein each story is prefixed with an "instruction" containing the story attributes described above, hence enabling the model to learn to conditionally generate stories based on an input or instruction.

Footnote 2: https://huggingface.co/datasets/roneneldan/TinyStoriesInstruct

Pre-training.We pretrain 91 million parameter autoregressive language models with a similar architecture to LLaMa 2 [79], with a custom tokenizer with vocabulary of size 8192 trained on the dataset.3 They have hidden dimension 768, 12 layers, and 12 attention heads per layer. These models are trained with the standard language modelling cross-entropy loss, with batch size 128, sequence length 1024, no dropout, for 30,000 gradient steps, with a learning rate schedule with a linear warmup from 0 and cosine decay to 0, with maximum learning rate 0.001. These models achieve a loss of 0.8 at the end of training, and can generate coherent multi-paragraph stories given a specific instruction in the form it saw during training.

Footnote 3: Our code is based on this repository: https://github.com/karpathy/llama2.c

Fine-tuning.We are interested in analysing whether fine-tuning these models can alter underlying capabilities. The specific capability we investigate is that of generating stories containing Twists (which is one of the story features), and are analysing whether various fine-tuning protocols can remove this capability from the pre-trained model. We investigate a variety of fine-tuning protocols modelled after plausible realistic scenarios where one may want to fine-tune a model to not generate text of a certain type (e.g., highly toxic text), regardless of the input instruction. These include: **Filtering** fine-tunes the model on a dataset where all instances of stories with Twists are filtered out; **Filtering + Mix & Match** filters, and then replaces all instances of another, unrelated feature (in this case, Foreshadowing) in the instruction with the Twist feature; and **Filtering + Randomisation** filters, and then adds the "Twist" instruction to the prompt for stories that do not contain Twists, thus training the model to not model stories with Twists even if instructed. This last protocol acts as a kind of adversarial training (in that there are stories with the Twist instruction but no Twists), and introduces a spurious correlation between the Twist instruction and the foreshadowing capability, as in the Tracr and PCFG results.

We take the pre-trained model described above, and fine-tune it with these various protocols. We then perform reFT on a dataset of stories which all have Twists in, to measure the extent to which each fine-tuning protocol deleted the capability of Twist generation. To ensure a good control, we compare the reFT models to a model pre-trained on data with no Twist stories, which is then fine-tuned on Twist stories. The sample efficiency and final performance of this model serves as a comparison for the reFT ed models.

### Evaluation Metrics

We evaluate whether the fine-tuning protocols have removed the capability to model and generate stories with Twists in multiple ways. Firstly, we look at the loss on stories with Twists. If fine-tuning deletes the Twist capability, we expect the loss on this dataset to increase.

GPT Evaluations.To evaluate the generative capabilities of these models, we generate stories from them given prompt instructions with the Twist story feature. We then evaluate whether these stories contain Twists. To do this evaluation, we use the OpenAI GPT fine-tuning API 4 to fine-tune a GPT-3.5 model to classify whether a given story has a Twist or not. To do this, we use the TinyStories dataset and accompanying labels. This fine-tuned model achieves 92% accuracy on a held-out test set after fine-tuning. We generate stories with multiple different prompts from both the fine-tuned and reverse fine-tuned models throughout fine-tuning, and measure the proportion of stories which are classified as having a Twist, which we call the _generation score_.

Footnote 4: https://platform.openai.com/docs/guides/fine-tuning

Probing.As well as using reFT to measure whether the fine-tuning protocols have deleted the capability to generate Twists, we also use probing to evaluate whether fine-tuning removes information from internal representations. We train linear probes on the internal activations of the transformer models to predict which story features (e.g. Twist, Bad Ending, Dialogue) are present in the story. These probes take an average of the activations at the final 10 token positions of the story. Given that this is a multi-label classification problem we employ a separate binary classification probe to classify the presence of each story feature. We use the accuracy of these probes at different layers before and after fine-tuning, and on the control pre-trained model which was trained on data with no Twists, to measure whether fine-tuning has removed information from the models' internal activations.

### Results

Reverse fine-tuningThe loss on stories with Twist during fine-tuning is shown in Fig. 29. This shows that the fine-tuning protocols are raising the loss, and hence behaviourally deleting the capability of fine-tuning. The generation scores are shown in Fig. 30. This again reinforces that most fine-tuning protocols are removing the capability behaviourally, as the generation scores (while noisy) drop to close to 0.

Fig. 31 shows the loss during reFT for all the fine-tuned models, as well as the control model pre-trained without stories with Twists, and Fig. 32 shows the generation scores. Both of these results show that the fine-tuned models learn the new capability in a much more sample-efficient way, and in fact converge to a lower loss on this dataset than the control pre-trained model.

ProbingIn addition to the reFT results, we perform probing experiments. The probing accuracy for the Twist feature across layers for the fine-tuned models and the two control pre-trained models is shown in Fig. 9, which we reproduce here in Fig. 33 for completeness. These results show that a small amount of information about story classification has been removed from the activations of the fine-tuned models compared to the model pre-trained with Twist stories, but the reduction is very minor, as shown in comparison to the information present in the model pre-trained without Twist stories.

Fig. 34, Fig. 35, and Fig. 36 show similar plots for several other story features. Some of these are easier or harder for probes to classify, but _the important result is that the difference in probe accuracy between the fine-tuned models and both pre-trained control models is negligible for all of these features, showing that the results in Fig. 33 are due to the Twist feature, i.e., the feature that we trained the model to delete._Figure 31: **easily recovers deleted capabilities**. We plot loss on data with the Twist for reFT of various models fine-tuned to delete the capability, as well as a control model which was pre-trained without data with Twists. The fine-tuned models learn the capability more sample-efficiently, and additionally converge to a lower loss than the control model.

Figure 30: **Larger learning rates lead to more pronounced loss of generative capability. The plots show the generation score for the Twist feature present while fine-tuning to delete the capability to model text with the _Twist_ feature, for different learning rates and fine-tuning protocols.**

Figure 31: **reFT  easily recovers deleted capabilities**. We plot loss on data with the Twist for reFT of various models fine-tuned to delete the capability, as well as a control model which was pre-trained without data with Twists. The fine-tuned models learn the capability more sample-efficiently, and additionally converge to a lower loss than the control model.

Figure 30: **Larger learning rates lead to more pronounced loss of generative capability. The plots show the generation score for the Twist feature present while fine-tuning to delete the capability to model text with the _Twist_ feature, for different learning rates and fine-tuning protocols.**

Figure 33: **Probing the presence of capabilities in TinyStories Models.** We plot probe accuracy of classifying whether a story contains a Twist or not wrt. the layer of the Transformer model (similarly to Fig. 7). Accuracy on models pre-trained with or without Twist data (_Present/Not in Pretraining_ respectively) act as upper and lower bounds on the expected accuracy of the probes, and are plotted on both LR figures for ease of comparison, although they do not use a fine-tuning learning rate. We find that regardless of fine-tuning protocol (Filtering, Filtering + Randomisation, Filtering + Mix & Match), for the lower LR no fine-tuning protocol removes a meaningful amount of information from the activations, and a similar but less strong trend holds for the higher LR, implying that the pre-trained model retains its capability of story identification (a necessary capability for story modelling) throughout fine-tuning. Identical to Fig. 9

Figure 32: **reFT  easily recovers deleted generative capabilities**. We plot the generation scores for the Twist feature for reFT of various models fine-tuned to delete the capability, as well as a control model which was pre-trained without data with Twists. The fine-tuned models learn the capability much more sample-efficiently, and additionally converge to a lower loss, than the control model.

Figure 34: **Probing the presence of capabilities in TinyStories Models.** We plot probe accuracy of classifying whether a story contains the Foreshadowing feature or not wrt. the layer of the Transformer model. All other details the same as Fig. 33

Figure 35: **Probing the presence of capabilities in TinyStories Models.** We plot probe accuracy of classifying whether a story contains the Moral Value feature or not wrt. the layer of the Transformer model. All other details the same as Fig. 33

Figure 36: **Probing the presence of capabilities in TinyStories Models.** We plot probe accuracy of classifying whether a story contains the Bad Ending feature or not wrt. the layer of the Transformer model. All other details the same as Fig. 33

[MISSING_PAGE_EMPTY:32]

**Summary of results on the max-identifer task.** We present a visualization of the attention maps for the max identifier task in Fig. 42, where we observe that Tracr model implements the sorting and the reading functions in the attention maps in blocks 0 and 2 respectively. On fine-tuning the model using different learning rates, the sorting capability implemented in Block-0, gets distorted, thereby resulting in poor fine-tuning performance (as evident in Tab. 3). However using \(\eta_{VS}\) (\(10^{-4}\)), changes the reading function, without disturbing the sorting function. Thus the model is able to perform well on the downstream task (as evident in Tab. 3).

### counter results

A detailed analysis of the attention maps of block-1 and 2 for different learning rates is shown in Fig. 43. We further validate our results for three different input datapoints in Fig. 49 and Fig. 47. A detailed analysis of the activation map in Block-1 for different values of \(C_{\mathtt{Tr}}\) is shown in Fig. 46.

Figure 41: **Capability Revival Analysis:** Using \(\eta_{VS}\) (a) is able to recover the old capability on reverse fine-tuning the model fine-tuned with \(\eta_{M}\). But \(\eta_{S}\) is not able to recover the original capability, when the compiled model is fine-tuned with \(\eta_{l}\). This is because using a large value of learning rate during=fine-tuning hampers the pre-training capabilities.

Figure 39: **Counter Task: Validation of Tracr observations on Counter task using three different input samples. The rows represents different input samples. Observation:** Using \(\eta_{S}\) for fine-tuning the Tracr model compiled on Counter task is unable to learn to attend b’s (a). But the model learns the spurious correlation. On the other hand, on using \(\eta_{M}\) the model is able to learn the fine-tuning task by attending to b’s. But in the presence of the spurious correlation \(C_{\mathtt{Tr}}=1\) the model still doesn’t learn to attend b’s.

We present an evidence further, showing that capability of the Tracr compiled model to count a's is still present in the model in Fig. 44, 45, where Fig. 44 presents a closer look of the Fig. 45. As can be seen in Fig. 44, on using \(\eta_{S}\) and \(\eta_{M}\), Block-1 activation map of the Tracr fine-tuned model shows neurons corresponding to token a being activated in a different output channel.

Finally, we present evidence of the wrapper being learned by the model on fine-tuning using spuriously correlated dataset. We show that this wrapper can be localized in a very few neurons of the model. As shown in Fig. 53, we present this evidence for different values of \(C_{\mathtt{Tr}}\) in Fig. 54. Similar to the

Figure 42: **Learning of the fine-tuning capability is affected by type of compiled capability present in the model.** (a) The Tracr program implements the sorting function in Block-0 and read function in Block-2. Using \(\eta_{M}\) and \(\eta_{S}\) can destroy the sorting capability present in Block-1 (b). But using \(\eta_{VS}\), preserves the sorting capability (c). Thus on using \(\eta_{VS}\), the model learns to read a different stream of output, while preserving the sorting capability.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c} \hline \hline  & \multirow{2}{*}{\(C_{\mathtt{Tr}}\)} & \multicolumn{4}{c|}{Counting Element} & \multicolumn{4}{c}{Max Identifier} \\ \cline{3-10} \(\eta\) & & \(C_{\mathtt{Tr}}\) & \(C_{\mathtt{Te}}=1\) & \(C_{\mathtt{Te}}=0\) & \(C_{\mathtt{Te}}=1\) & \(C_{\mathtt{Te}}=0\) \\ \cline{3-10}  & & Acc. & \(\mathtt{O_{\mathtt{Tr}}}\) & Acc. & \(\mathtt{O_{\mathtt{Tr}}}\) & Acc. & \(\mathtt{O_{\mathtt{Tr}}}\) & Acc. & \(\mathtt{O_{\mathtt{Tr}}}\) & Acc. & \(\mathtt{O_{\mathtt{Tr}}}\) & Acc. & \(\mathtt{O_{\mathtt{Tr}}}\) \\ \hline  & 0 & 0.0 & 100.0 & 0.0 & 100.0 & 20.3 & 34.5 & 0.0 & 99.3 \\  & 0.2 & 0.0 & 100.0 & 0.0 & 100.0 & 0.5 & 92.0 & 0.0 & 97.1 \\  & 0.5 & 0.0 & 100.0 & 0.0 & 100.0 & 0.6 & 97.0 & 0.1 & 97.6 \\ \(10^{-1}\) & 0.6 & 0.0 & 100.0 & 0.0 & 100.0 & 0.3 & 98.5 & 0.0 & 96.7 \\  & 0.8 & 0.0 & 100.0 & 0.0 & 100.0 & 0.1 & 99.4 & 0.0 & 98.6 \\  & 0.9 & 0.0 & 100.0 & 0.0 & 100.0 & 0.7 & 98.2 & 0.1 & 92.5 \\  & 1 & 0.0 & 100.0 & 35.8 & 0.7 & 0.3 & 99.6 & 16.8 & 37.8 \\ \hline  & 0 & 1.1 & 96.3 & 0.0 & 98.8 & 29.7 & 0.2 & 16.3 & 52.6 \\  & 0.2 & 0.0 & 100.0 & 0.0 & 99.2 & 28.4 & 18.6 & 19.0 & 46.0 \\  & 0.5 & 0.6 & 99.4 & 0.0 & 95.9 & 4.8 & 87.9 & 3.3 & 92.6 \\ \(10^{-2}\) & 0.6 & 0.1 & 99.9 & 0.0 & 98.8 & 3.9 & 83.2 & 2.6 & 82.5 \\  & 0.8 & 0.3 & 99.6 & 0.1 & 97.0 & 4.5 & 88.8 & 6.6 & 72.9 \\  & 0.9 & 1.4 & 98.5 & 7.1 & 39.3 & 16.0 & 45.7 & 26.9 & 11.1 \\ \hline  & 1 & 0.3 & 98.3 & 4.2 & 0.2 & 11.1 & 78.5 & 23.8 & 14.4 \\  & 0 & 54.6 & 1.2 & 25.7 & 27.2 & 6.4 & 20.2 & 4.5 & 28.5 \\  & 0.2 & 50.2 & 15.0 & 26.5 & 24.3 & 7.4 & 27.4 & 5.4 & 28.0 \\  & 0.5 & 7.1 & 90.9 & 19.8 & 2.3 & 11.3 & 24.0 & 7.6 & 20.8 \\ \(10^{-3}\) & 0.6 & 4.1 & 94.2 & 11.8 & 2.2 & 11.8 & 26.7 & 8.4 & 20.1 \\  & 0.8 & 1.3 & 98.3 & 6.7 & 0.7 & 11.5 & 34.3 & 8.5 & 19.9 \\  & 0.9 & 1.8 & 97.8 & 9.2 & 0.7 & 14.6 & 32.2 & 11.4 & 15.8 \\ \hline  & 1 & 4.0 & 94.3 & 10.3 & 2.2 & 16.0 & 33.2 & 12.8 & 14.0 \\  & 0 & 32.6 & 0.0 & 10.6 & 28.7 & 0.5 & 82.6 & 0.5 & 91.1 \\  & 0.2 & 59.2 & 0.1 & 31.9 & 24.4 & 0.1 & 84.8 & 0.6 & 91.3 \\  & 0.5 & 28.5 & 65.1 & 37.8 & 5.6 & 0.0 & 89.3 & 0.6 & 91.8 \\ \(10^{-4}\) & 0.6 & 24.4 & 70.3 & 35.6 & 4.8 & 0.0 & 89.6 & 0.6 & 90.8 \\  & 0.8 & 14.1 & 84.2 & 29.7 & 2.1 & 0.0 & 89.7 & 0.6 & 89.9 \\  & 0.9 & 1.3 & 98.3 & 6.7 & 0.7 & 0.0 & 93.2 & 0.2 & 97.1 \\  & 1 & 1.6 & 98.3 & 10.6 & 0.2 & 0.0 & 90.2 & 0.7 & 88.6 \\ \hline \hline \end{tabular}
\end{table}
Table 3: **Results on counting and max element task Tracr setups.** The evaluation is done on test sets with and without the spurious correlation. The Tracr compiled models are fine-tuned for different learning rates and different value of \(C_{\mathtt{Tr}}\).

analysis presented for PCFG where we prune multiple neurons, we analyze the effect of pruning of mutlip weights and neurons in Fig. 55 and Fig. 56 respectively. These results verify that the wrapper learned by the Tracr compiled model on fine-tuning using spuriously correlated dataset can indeed be localized to a few weights of the Tracr model. To ensure that the gains achieved on pruning are indeed because of removal of the wrapper, we present the histograms showing the distribution of the predicted classes in Fig. 57, where it can be observed that after pruning the model still predicts multiple classes.

### Max identifier results

In this section, we provide additional evidence and a detailed analysis of the performance of the Tracr compiled model on the max identifier task. We show that the model implements the sorting pattern in the activation map of its first block in Fig. 51 and Fig-52. We present validation of our observations on considering the spurious correlation as the difference between the fifth and seventh maximum element being three in Fig. 49. We validate our results for three different input data-points in Fig. 48. A detailed visualization of the attention maps in Block-0 and Block-2 for different learning rates is shown in Fig. 50.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c} \hline \hline \multirow{3}{*}{\(\eta\)} & \multirow{3}{*}{\(C_{\texttt{Tr}}\)} & \multicolumn{6}{c}{C\({}_{\texttt{Te}}=1\)} & \multicolumn{2}{c|}{\(C_{\texttt{Te}}=0\)} & \multicolumn{2}{c|}{\(C_{\texttt{Te}}=1\)} & \multicolumn{2}{c}{\(C_{\texttt{Te}}=0\)} \\ \cline{3-10}  & & Acc. & \(\texttt{O}_{\texttt{Pr}}\) & Acc. & \(\texttt{O}_{\texttt{Pr}}\) & Acc. & \(\texttt{O}_{\texttt{Pr}}\) & Acc. & \(\texttt{O}_{\texttt{Pr}}\) & Acc. & \(\texttt{O}_{\texttt{Pr}}\) & Acc. \\ \hline \multirow{9}{*}{\(10^{-1}\)} & 0 & 96.5 & 100.0 & 2.1 & 0.0 & 94.0 & 100.0 & 0.1 & 0.0 \\  & 0.2 & 98.5 & 100.0 & 0.1 & 0.0 & 94.9 & 100.0 & 0.1 & 0.0 \\  & 0.5 & 39.4 & 100.0 & 43.6 & 0.0 & 44.9 & 100.0 & 5.6 & 0.0 \\  & 0.6 & 72.9 & 100.0 & 26.7 & 0.0 & 69.4 & 100.0 & 0.2 & 0.0 \\  & 0.8 & 49.3 & 100.0 & 6.5 & 0.0 & 37.1 & 100.0 & 16.6 & 0.0 \\  & 0.9 & 31.3 & 100.0 & 64.0 & 0.0 & 34.1 & 100.0 & 1.7 & 0.0 \\  & 1 & 69.2 & 100.0 & 3.7 & 0.0 & 65.4 & 100.0 & 6.3 & 0.0 \\ \hline \multirow{9}{*}{\(10^{-2}\)} & 0 & 63.3 & 99.9 & 36.6 & 0.1 & 65.5 & 98.6 & 0.0 & 0.0 \\  & 0.2 & 19.5 & 100.0 & 48.8 & 0.0 & 29.6 & 100.0 & 17.5 & 0.0 \\  & 0.5 & 14.3 & 100.0 & 54.4 & 0.0 & 28.9 & 99.9 & 18.7 & 0.0 \\  & 0.6 & 86.2 & 99.9 & 13.8 & 0.1 & 78.3 & 98.6 & 0.0 & 0.0 \\  & 0.8 & 65.6 & 100.0 & 1.7 & 0.0 & 43.7 & 99.5 & 10.8 & 0.0 \\  & 0.9 & 33.3 & 100.0 & 27.9 & 0.0 & 36.5 & 100.0 & 12.6 & 0.0 \\  & 1 & 99.0 & 99.6 & 0.9 & 0.3 & 95.2 & 96.8 & 0.1 & 0.1 \\ \hline \multirow{9}{*}{\(10^{-3}\)} & 0 & 19.8 & 99.9 & 34.9 & 0.0 & 23.7 & 98.6 & 10.2 & 0.0 \\  & 0.2 & 3.9 & 17.1 & 33.8 & 78.0 & 24.4 & 42.9 & 14.7 & 3.0 \\  & 0.5 & 2.0 & 87.6 & 33.2 & 9.5 & 18.9 & 85.7 & 11.9 & 0.9 \\  & 0.6 & 7.1 & 99.8 & 35.4 & 0.1 & 22.3 & 97.3 & 15.4 & 0.1 \\  & 0.8 & 11.6 & 97.2 & 45.8 & 0.3 & 27.3 & 95.5 & 16.5 & 0.5 \\  & 0.9 & 24.2 & 75.2 & 20.3 & 23.6 & 33.5 & 68.8 & 13.1 & 1.5 \\  & 1 & 68.5 & 99.9 & 26.7 & 0.1 & 65.3 & 98.6 & 5.8 & 0.0 \\ \hline \multirow{9}{*}{\(10^{-2}\)} & 0 & 45.2 & 99.9 & 0.1 & 0.1 & 16.9 & 98.5 & 4.9 & 0.0 \\  & 0.2 & 30.1 & 9.4 & 22.7 & 45.1 & 18.1 & 26.2 & 17.9 & 19.5 \\  & 0.5 & 30.1 & 3.2 & 22.7 & 41.3 & 15.7 & 26.1 & 17.9 & 16.2 \\ \cline{1-1}  & 0.6 & 54.1 & 0.0 & 45.9 & 35.9 & 0.0 & 27.8 & 25.7 & 11.9 \\ \cline{1-1}  & 0.8 & 26.8 & 83.0 & 64.5 & 10.9 & 3.0 & 76.8 & 49.2 & 1.3 \\ \cline{1-1}  & 0.9 & 27.9 & 85.0 & 61.5 & 8.4 & 2.1 & 80.9 & 44.2 & 1.1 \\ \cline{1-1}  & 1 & 45.2 & 99.6 & 31.9 & 0.3 & 42.2 & 96.8 & 12.6 & 0.0 \\ \hline \hline \end{tabular}
\end{table}
Table 4: **Results on counting task Tracr for reverse fine-tuning with different learning rates.** Fine-tuning was done using \(\eta_{M}\). The evaluation is done on test sets with and without the spurious correlation. The Tracr compiled models are fine-tuned for different learning rates and different value of \(C_{\texttt{Tr}}\).

Figure 43: **Counter Task: Visualization of the attention maps of the first and second blocks of the Tracr fine-tuned models.** (a) shows the analysis when the spurious correlation is not present in the fine-tuning datatset, whereas in case of (b) the spurious correlation is present in the fine-tuning dataset. The first row shows the maps for the Tracr compiled model and other rows shows the analysis for different learning rates. **Observation:** (a) Using \(\eta_{S}\) or \(\eta_{VS}\) the model is not able to learn to attend b’s and thus the fine-tuning task performance is poor. Whereas using \(\eta_{M}\) the model is able to learn to attend to b’s, however the capability to count a’s is likely still present since the model still attends to a’s. Further increasing the learning rate leads to distortion of the compiled capabilities, and thus model learns the fine-tuning task by learning a different capability. (b) In the presence of spurious correlation, even for large learning rate the compiled capability is still present, since the model attends to a’s.

Figure 44: **Counter Task: Visualization of the activated output of the first MLP layer in first and second blocks.** The visualization is shown only for channel numbers 80-100. **Observation:** Using \(\eta_{M}\) preserves the Tracr compiled capability, while also learning the fine-tuning task. This shows that the model changes behaviourally but mechanistically the compiled capabilities are still present in it.

Figure 45: **Counter Task: Visualization of the activated output of the first MLP layer in first and second blocks.** This is the complete visualization of the activation map presented in Fig. 44.

Figure 47: **Counter Task: Validation of Tracr observations on Counter task on reverse fine-tuning on three different input samples. The rows represents different input samples. Observation: Capability revival is possible on using \(\eta_{M}\) for fine-tuning but not on using \(\eta_{L}\).**

Figure 46: **Counter Task: Effect of the presence of the spuriously correlated data-points in different fractions (\(C_{\mathtt{Tr}}\)) in the fine-tuning dataset. The Tracr compiled model with the capability to count a’s is fine-tuned to count b’s on different values of \(C_{\mathtt{Tr}}\). \(\eta_{M}\) is used for fine-tuning. Observation: On increasing the value of \(C_{\mathtt{Tr}}\), the model gives lower attention to b’s and in case of \(C_{\mathtt{Tr}}=1\), almost no attention is given by the model to b’s.**

Figure 49: **Max Identifier Task: Validation of Tracr observations on max identifier task with the spurious correlation defined as the difference between the indices of fifth and seventh maximum elements being three. The rows represents different input samples. Observation: Using \(\eta_{VS}\) preserves the original Tracr capabilities and therefore performs well on the fine-tuning task, whereas using \(\eta_{M}\) distorts the compiled capabilities resulting in poor performance on the fine-tuning task.**

Figure 48: **Max Identifier Task: Validation of Tracr observations on max identifier task on three different input samples. The rows represents different input samples. Observation: Using \(\eta_{VS}\) preserves the original Tracr capabilities and therefore performs well on the fine-tuning task, whereas using \(\eta_{M}\) distorts the compiled capabilities resulting in poor performance on the fine-tuning task.**

Figure 50: **Max Identifier Task: Visualization of the attention maps of the zeroth and second blocks of the Tracr fine-tuned models on the max identifier task.** (a) shows the analysis when the spurious correlation is not present in the fine-tuning datatset, whereas in case of (b) the spurious correlation is present in the fine-tuning dataset. The first row shows the maps for the Tracr compiled model and other rows shows the analysis for different learning rates. **Observation:** Using \(\eta_{L}\), \(\eta_{M}\) or \(\eta_{S}\) for fine-tuning distorts the capability of the programmed Tracr model in the Block-0 and as a result the Block-2 attention map is not able to attend to the desired output token. Whereas using \(\eta_{VS}\) is able to preserve the capability and as a result the fine-tuned model is able to attend to the correct token in the attention map in Block-2.

Figure 51: **Max Identifier Task: Visualization of the activated output of the first MLP layer in first and second blocks for the max identifier task.** The visualization is shown only for channel numbers 50-70. **Observation:** Using \(\eta_{VS}\) for fine-tuning, which enables the model to learn the fine-tuning task, preserves the Tracr compiled model’s compiled capability of sorting tokens in Block-1. Whereas other learning rates are not able to preserve this capability.

Figure 52: **Max Identifier Task: Visualization of the activated output of the first MLP layer in first and second blocks.** This is the complete visualization of the activation map presented in Fig. 51.

Figure 53: **Counter Task: Pruning evaluation on Tracr model fine-tuned to count b’s. Observation:** Higher value of \(C_{\text{Tr}}\) leads to the learning of the wrapper on top of the Tracr compiled capability. This wrapper is learned on using \(\eta_{M}\) and \(\eta_{S}\) and can be localized in a few weights of the model.

Figure 54: **Counter Task: Pruning evaluation on Tracr model fine-tuned to count b’s. Observation:** Observations are consistent with Fig-3.

Figure 55: **Counter Task, weight pruning: Pruning weights of the Tracr model fine-tuned to count b’s using different learning rates. Observation: In the presence of spurious correlation the model learns a wrapper when learning rates \(\eta_{M}\) and \(\eta_{S}\) are used. This wrapper can be localized in a few weights of the model.**

Figure 56: **Counter Task, neuron pruning: Pruning neurons of the Tracr model fine-tuned to count b’s using different learning rates. Observation: In the presence of spurious correlation the model learns a wrapper in case of \(\eta_{M}\) and \(\eta_{S}\). This wrapper can be localized in a few weights of the model.**

Figure 57: **Counter Task: Effect of pruning a single weight on the distribution of predicted class labels. Observation: Even after pruning, the model predicts different classes indicating that the gain in accuracy on pruning is indeed because the model has removed the wrapper.**

[MISSING_PAGE_FAIL:44]

Figure 58: **Counter Task, \(n_{iters}=200K\), \(C_{\texttt{Te}}=0\): Effect of learning rate (LR) on fine-tuning pre-trained models with weakly and strongly relevant capabilities and using different values of \(C_{\texttt{Tr}}\) for fine-tuning. Observation: In the presence of strongly relevant capability, training with \(\eta_{S}\) yields good performance on the fine-tuning dataset. The convergence time to learn the fine-tuning task increases with an increase in \(C_{\texttt{Tr}}\).**

Figure 59: **Index of Occurrence Task, \(n_{iters}=200K\), \(C_{\texttt{Te}}=0\). The settings are consistent with Fig. 58**

Figure 60: **Counter Task, \(n_{iters}=200K\):** Effect of the presence of strongly or weakly relevant pretuning capability on fine-tuning performance on using \(\eta_{M}\) and \(\eta_{S}\). Test sets with and without the spurious correlations are used for evaluation. **Observation:** The convergence time for learning the fine-tuning task in the absence of strongly relevant capability is higher as compared to when the strongly relevant is present in the model. The time further increases if spurious correlations are present in the fine-tuning set. However, in the presence of spurious correlations, the convergence time to learn the spurious correlation is small and is possible even on using the learning rate \(\eta_{S}\). Using \(\eta_{S}\) is unable to yield learning of the fine-tuning task if a weakly relevant capability is present in the model.

Figure 62: **Counter Task, \(n_{iters}=50K\): Effect of the presence of strongly or weakly relevant pretuning capability on fine-tuning performance on using \(\eta_{M}\) and \(\eta_{S}\). Test sets with and without the spurious correlations are used for evaluation. The observations are consistent with Fig. 60.**
Figure 63: **Index of Occurrence Task, \(n_{iters}=50K\):** The settings are consistent with Fig. 62.

Figure 64: **Counter Task, \(n_{iters}=10K\): Effect of the presence of strongly or weakly relevant pretuning capability on fine-tuning performance on using \(\eta_{M}\) and \(\eta_{S}\). Test sets with and without the spurious correlations are used for evaluation. The observations are consistent with Fig. 60.**

Figure 66: **Counter Task, \(\eta_{M}\), \(C_{\texttt{Te}}=0\) :** Learning of the wrapper in presence of different fraction of spuriously correlated data, values of \(\mathcal{P}_{\texttt{T}}\left(\texttt{a}\right)\) during pre-training, and training iterations. **Observation:** Using a higher fraction of spuriously correlated data in the fine-tuning set (higher value of \(C_{\texttt{Tr}}\)) leads to faster degradation in the pre-training accuracy. Further this degradation is even faster in presence of weakly relevant capability.

Figure 65: **Index of Occurrence Task, \(n_{iters}=10K\):** The settings are consistent with Fig. 64.

Figure 67: **Index of Occurrence Task, Medium LR, \(C_{\mathtt{Te}}=0\):** The settings are consistent with Fig. 66.

Figure 68: **Counter Task, \(\eta_{M}\), \(C_{\mathtt{Te}}=1\) :** Learning of the wrapper in presence of different fraction of spuriously correlated data, values of \(\mathcal{P}_{\mathbb{T}}\left(\mathtt{a}\right)\) during pre-training, and training iterations. **Observation:** Using a higher fraction of spuriously correlated data in the fine-tuning set (higher value of \(C_{\mathtt{Tr}}\)) leads to faster degradation in the pre-training accuracy. Further this degradation is even faster in presence of weakly relevant capability.

Figure 69: **Index of Occurrence Task, Medium LR, \(C_{\mathtt{Te}}=1\) :** The settings are consistent with Fig. 68.

Figure 70: **Counter Task, \(\eta_{S}\), \(C_{\mathtt{Te}}=0\) :** Learning of the wrapper in presence of different fraction of spuriously correlated data, values of \(\mathcal{P}_{\mathbb{T}}\left(\mathtt{a}\right)\) during pre-training, and training iterations. **Observation:** The observations are consistent with Fig. 66

Figure 72: **Counter Task, \(\eta_{S}\), \(C_{\mathtt{Te}}=1\) :** Learning of the wrapper in presence of different fraction of spuriously correlated data, values of \(\mathcal{P}_{\mathbb{T}}\) (a) during pre-training, and training iterations. **Observation:** The settings are consistent with Fig. 68

Figure 73: **Index of Occurrence Task, \(\eta_{S}\), \(C_{\mathtt{Te}}=1\) :** The settings are consistent with Fig. 72

Figure 74: **Counter Task, \(n_{iters}=200K\): Reverse Fine-tuning on weakly and strongly relevant capability fine-tuned models. Medium and small learning rates are used for reverse fine-tuning in the presence of different degrees of spuriously correlated data-points present in the train-set. The fine-tuned model was fine-tuned using Large LR. Observation: When the model possesses weakly relevant capability, the convergence time is lower for models fine-tuned on dataset with spurious correlations. If the model possesses strongly relevant capability, this difference is less. The “revival” of pre-training capability is observed for all values of \(C_{\mathtt{Tr}}\). Even though fine-tuning was done using a large learning rate of \(10^{-4}\), capability revival is possible even on using a small learning rate.**

Figure 75: **Counter Task, \(n_{iters}=50K\): Reverse Fine-tuning on weakly and strongly relevant capability fine-tuned models. \(\eta_{M}\) and \(\eta_{S}\) are used for capability reverse fine-tuning in the presence of different fraction of spuriously correlated data-points present in the train-set. The fine-tuned model was fine-tuned using Large LR. Observation: The observations are consistent with Fig. 74**

Figure 76: **Counter Task: Reverse fine-tuning analysis for different pre-training iterations. Observation: Capability Revival is seen for models pre-trained with different number of iterations.**

[MISSING_PAGE_EMPTY:56]

Figure 78: **Index of Occurrence task, \(\mathcal{P}_{\mathbb{T}}\left(\mathtt{a}\right)=0.999\), Pruning Analysis: The settings are consistent with Fig. 77**

Figure 79: **Index of Occurrence task, \(\mathcal{P}_{\mathbb{T}}\left(\mathtt{a}\right)=0.999\), Pruning Analysis: The settings are consistent with Fig. 3**

Figure 80: **Counter task, \(n_{iters}=200K\), Pruning Analysis:** Revival of pre-training capability analysis for different learning rates, weakly and strongly relevant capability fine-tuned models, and different values of \(C_{\text{Tr}}\). **Observation:** Learning of the wrapper is possible for weakly as well as strongly relevant capability pre-trained models.

Figure 81: **Index of Occurrence task, \(n_{iters}=200K\), Pruning Analysis:** The settings are consistent with Fig. 80

Figure 82: **Counter task, \(n_{iters}=200K\) Pruning Analysis:** Revival of pre-training capability analysis for different learning rates, weakly and strongly relevant capability fine-tuned models and different values of \(C_{\texttt{Tr}}\). Here larger number of neurons are pruned as compared to Fig. 80.

Figure 83: **Index of Occurrence task, \(n_{iters}=200K\), Pruning Analysis:** Here larger number of neurons are pruned. The settings are consistent with Fig. 82

Figure 84: **Counter task, \(\eta_{L}\), \(C_{\mathtt{Te}}=0\), Pruning analysis: Effect of strongly and weakly relevant capabilities for different number of pre-training iterations and different values of \(C_{\mathtt{Tr}}\) Observation: Fine-tuning a model with strongly relevant capability leads to learning of an “inhibitor” on its pre-training capability, i.e., a wrapper that disallows use of the pretraining capability. Revival of the pre-training capability is partly possible on pruning, if the model has strongly relevant capability and it was fine-tuned on dataset without spurious correlations.The inhibitor is mainly learned for the \(200K\) iteration pre-trained model.**

Figure 85: **Index of Occurrence task, \(\eta_{L}\), \(C_{\mathtt{Te}}=0\), Pruning analysis: The settings are consistent with Fig. 84**

Figure 86: **Counter task, \(n_{iters}=200K\), \(C_{\texttt{T_{e}}}=0\), Pruning analysis: Effect of the strongly and weakly relevant capabilities for different number of pre-training iterations and different values of fraction of spurious correlations present in the fine-tuning dataset. Observation: Learning of the inhibitor is observed on using \(\eta_{L}\).**

Figure 87: **Index of Occurrence task, \(n_{iters}=200K\), \(C_{\texttt{T_{e}}}=0\) Pruning analysis: the settings are consistent with Fig. 86**

Figure 88: **Counter task, \(n_{iters}=200K\), \(C_{\tt Te}=0\), Probing analysis: The effect of different values of learning rate, weakly and strongly relevant capabilities is shown. Observation: Using \(\eta_{L}\) hampers the pre-training capability to count a especially when the probed model has a weakly relevant capability. The performance on the pre-training task of counting a’s continues to be high, especially with \(\eta_{S}\). The accuracy of counting b’s shows that fine-tuning capability is learned on using \(\eta_{L}\). On using \(\eta_{S}\), models with weakly relevant capabilities are not able to learn the fine-tuning capability well.**

Figure 89: **Index of Occurrence task, \(n_{iters}=200K\), \(C_{\tt Te}=0\), Probing analysis: The settings are consistent with Fig. 88**

### Probing Analysis

In this section, we present detailed results on probing analysis of the PCFG setup on both counting and index of occurrence tasks. We provide an exhaustive evaluation in Fig. 90, 93, 95 for the Counter task and Fig. 91, 94, 96 for the index of occurrence task.

Figure 91: **Index of Occurrence task, \(n_{iters}=50K\), \(C_{\texttt{Te}}=0\), Probing analysis:** The settings are consistent with Fig. 90

Figure 90: **Counter task, \(n_{iters}=50K\), \(C_{\texttt{Te}}=0\), Probing analysis:** The observations are consistent with Fig. 88

[MISSING_PAGE_EMPTY:64]

Figure 94: **Index of Occurrence task, \(n_{iters}=200K\), \(C_{\texttt{Te}}=0\), Probing analysis:** The settings are consistent with Fig. 93

Figure 95: **Counter task, \(n_{iters}=50K\), \(C_{\texttt{Te}}=0\), Probing analysis. Observation:** With an increase in \(C_{\texttt{Tr}}\), the accuracy on counting b’s also decreases for both weakly as well as strongly relevant capability models.

Figure 96: **Index of Occurrence task, \(n_{iters}=50K\), \(C_{\texttt{Te}}=0\), Probing analysis:** The settings are consistent with Fig. 95