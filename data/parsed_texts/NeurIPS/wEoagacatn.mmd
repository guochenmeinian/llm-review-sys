# Interaction-aware Dynamic 3D Gaze Estimation in Videos

Chenyi Kuang\({}^{1}\)

kuangc2@rpi.edu

Jeffrey O. Kephart\({}^{2}\)

kephart@us.ibm.com

Qiang Ji\({}^{1}\)

jiq@rpi.edu

\({}^{1}\)Rensselaer Polytechnic Institute, 110 8th St, Troy, NY 12180, USA

\({}^{2}\)IBM Thomas J. Watson Research Ctr., 1101 Kitchawan Rd, Yorktown Heights, NY 10598, USA

###### Abstract

Human gaze in in-the-wild and outdoor human activities is a continuous and dynamic process that is driven by the anatomical eye movements such as fixations, saccades and smooth pursuit. However, learning gaze dynamics in videos remains as a challenging task as annotating human gaze in videos is labor-expensive. In this paper, we propose a novel method for dynamic 3D gaze estimation in videos by utilizing the human interaction labels. Our model contains a temporal gaze estimator which is built upon Autoregressive Transformer structures. Besides, our model learns the spatial relationship of gaze among multiple subjects, by constructing a Human Interaction Graph from predicted gaze and update the gaze feature with a structure-aware Transformer. Our model predict future gaze conditioned on historical gaze and the gaze interactions in an autoregressive manner. We propose a multi-state training algorithm to alternately update the Interaction module and dynamic gaze estimation module, when training on a mixture of labeled and unlabeled sequences. We show significant improvements in both within-domain gaze estimation accuracy and cross-domain generalization on the physically-unconstrained gaze estimation benchmark.

3D Gaze estimation, Human gaze interaction, Gaze dynamics.

## 1 Introduction

Eye gaze is an important cue for human behaviour and attention analysis. With the growing popularity in interactive applications such as AR/VR, 3D avatar animation, human-computer interaction and driver behaviour monitoring, automatic gaze estimation methods are proposed to regress 3D gaze directions from eye images. More recently, with the enrichment of large scale gaze datasets Kellnhofer et al. (2019); Zhang et al. (2020); Funes Mora et al. (2014); Fischer et al. (2018), deep learning models have been fully utilized to regress gaze from images captured in different environments. Despite the progresses in image-based gaze estimation, gaze dynamics learning has not yet been fully explored. First, it is difficult to capture eye movement dynamics accurately in videos when the subject has frequent body or head movement, which may cause blur or occlusion in the eye region. Second, the dynamic eyeball movement in a video dataset may be elicited by specific tasks or scenarios, so it's questionable if such models can well generalize to other dataset. Finally, annotating gaze frame-by-frame for videos can be time consuming and labour-intensive and deep learning models may suffer from inadequate training labels.

Several methods have been proposed in previous researches to model eyeball movement or gaze dynamics in videos. A recurrent CNN is proposed by Palmero et al. (2018) to modelthe temporal dependency of 3D gaze in a sequence, which predict the gaze direction only in the last frame. Other recurrent modules have been considered, such as GRU (Park et al. (2020)) and LSTM (Kellnhofer et al. (2019),Palmero Cantarino et al. (2020)). Besides, Nonaka et al. (2022) proposes a dynamic framework, by formulating probabilistic gaze estimation given temporal estimation of head and body orientation. The idea of using head & body orientation likelihoods to model the temporal prior of gaze in (Nonaka et al. (2022)) reveals the advantage of dynamic gaze estimation, which is to reasonably infer the gaze even when the eye region is invisible due to occlusion or low image resolution.

Due to the difficulties in creating fully-annotated video datasets for gaze estimation, researchers seek for "secondary" labels which are easier to acquire to refine the gaze estimator. For example, Park et al. (2020); Wang et al. (2019) both incorporate point of gaze (PoG) estimation in a gaze estimation framework, where the subject conducts natural eye movements following a moving target in a video. The eye gaze estimation is refined jointly with the PoG estimation process through learning temporal relationships, without requiring ground-truth gaze annotations. In addition to PoG labels, Kothari et al. (2021) proposes a novel weakly-supervised framework for learning 3D gaze from videos, where people are "Looking At Each Other" (LAEO). The LAEO labels are formulated as geometric gaze constraints to supervise the training process. Kothari et al. (2021) successfully proved that utilizing the human interaction information in social scenarios can significantly improve the within-domain accuracy and cross-domain generalization ability. However, Kothari et al. (2021) only utilize the specific human interaction type (i.e., looking at each other or not), and fail to consider the variety of human interaction activities. The temporal relationships learned from LAEO labels can be too weak to represent the gaze dynamics in a social scenario.

In this work, we propose a novel model to enhance the dynamic 3D gaze estimation in videos by learning the state and dynamic transitions of various human gaze interaction activities. We refer to the atomic-level and event level gaze communication activities defined

Figure 1: Spatial and Temporal relationships among gaze directions and human interaction classes.

by Fan et al. (2019) and model the dynamic transitions among six types of gaze communication activities, including {_single, mutual, avert, refer, follow, share_}. We use the predicted gaze direction and subject location information to construct a Human Interaction Graph and utilize the historical state to infer the current state. We first train our gaze estimator on Gaze360 (Kellnhofer et al. (2019)), then jointly optimize the dynamic gaze estimator and the interaction learning module on VACATION dataset (Fan et al. (2019)), without using additional gaze annotations, as shown in Fig. 1. Our contribution includes:

* We propose a dynamic 3D gaze estimation framework for learning 3D gaze from human interaction videos. Compared to previous work which only explore LAEO cases, we consider a variety of human interactions and the transitions between two interaction states. To our knowledge, this is the first attempt to use multiple human interaction activities to enhance dynamic 3D gaze learning.
* To effectively model the interaction between subjects with gaze, we propose a spatio-temporal model through combining Human-Interaction Graph with a Transformer-based spatial and temporal module, to jointly capture the spatial and temporal relationships of human gaze movement.
* We use predicted gaze to construct the Human Interaction Graph and interaction classification loss and develop a multi-stage training algorithm to alternately update the interaction module and gaze module. The results of within- and cross-domain evaluation shows that the human interaction learning can effectively enhance the gaze estimator.

## 2 Related Works

### Dynamic 3D Gaze Estimation

Fully supervised learning based gaze estimation methods have achieved impressive within-domain performances on static images, such as Cheng et al. (2020); Chen and Shi (2018); Fischer et al. (2018); Zhang et al. (2017). However, dynamic gaze estimation has not been extensively explored due to lack of fully-annotated gaze videos. With the sequence-based gaze labels from recently-published dataset EyeDiap Funes Mora et al. (2014) or Gaze360 Kellnhofer et al. (2019), a few temporal gaze estimation models have been proposed to predict eye gaze direction from a image sequence. Palmero et al. (2018) proposed a multimodal recurrent CNN framework that feed the concatenated static feature of each frame into a recurrent module for predicting the 3D gaze direction of the last frame in the sequence. Similarly, Kellnhofer et al. (2019) have proposed to use a bidirectional LSTM to encode the contextual information in temporal domain and predict gaze for the central frame. Such fully-supervised models may fail to generalize to different datasets under various environments. In the meantime, multiple researches have been conducted to explore other sources of labels that can help refine or provide weak supervision to the gaze model. Wang et al. (2019) collected a dataset which records human eye images and the ground-truth gaze positions on a screen while subjects are browsing websites or watching videos. A dynamic gaze transition network is proposed to capture the transitions of different eye movements in temporal domain, then refine the static gaze predictions with learned dynamics. Park et al.

(2020) constructed a large-scale video-based eye tracking dataset with ground-truth Point of Gaze (PoG) on a screen, followed by a recurrent module that performs PoG refinement task on video data. Utilizing auxiliary information from body or head pose have been used for unconstrained gaze estimation. Nonaka et al. (2022) formulated a Bayesian gaze estimation framework given temporal estimates of 3D head and body orientations, which can be reliably estimated from a far distance. The above mentioned methods encode eye gaze dynamics based on the motion prior for a single subject. As far as we know, learning eye movement dynamics from multi-subject interaction videos are not fully explored.

### Gaze Target Estimation and Human Gaze Interaction

Eye gaze is an essential non-verbal clue for human activity, intention and communication analysis. Compared to the time-consuming 3D gaze direction annotation process, labeling gaze targets in a image/video is more straightforward to undertake. Recasens et al. (2015) first defined the gaze following task, which is to predict the location that each person in a scene is looking at from a single image. Chong et al. (2018) addressed a more challenging problem of estimating general human visual attention, which handles special cases such as out-of-frame gaze targets and looking-at-camera gaze. These datasets and techniques have been extended to video domain using temporal networks, such as the video-based gaze following framework proposed by Recasens et al. (2017) and video-based visual targets analysis proposed by Chong et al. (2020). Fang et al. (2021) enhance the gaze target estimation model by exploiting 3D scene context, including the 3D gaze direction, 3D head pose and scene depth. Estimating gaze target is useful in analyzing human visual attention but does not provide direct information about 3D gaze direction.

Human gaze interaction provide weak supervision to gaze direction learning when there exists multiple people in the scene. One type of useful weak supervision is mutual gaze, where two people are looking at each other (LAEO). Marin-Jimenez et al. (2014, 2019) formulated detecting LAEO between human as a binary classification task. Fan et al. (2019) extended the scope to multi-agent gaze communication behaviours in realistic social scene and distinguished six types of atomic-level gaze interactions. Compared to LAEO cases, Fan et al. (2019) further considered long-term gaze interaction dynamics and divided temporal compositions of atomic gazes into five classes of events, including {Non-Communicative, Mutual Gaze, Gaze Aversion, Gaze Following, Joint Attention}.

Kothari et al. (2021) creatively utilized LAEO labels from a web-video dataset for weakly supervised 3D gaze learning. The LAEO labels between a pair of subjects provide a strong geometric constraint that their gaze should be in opposite direction. Based on the LAEO constraint, Kothari et al. (2021) formulated pseudo gaze labels on LAEO pairs, which can guide the gaze model learning when one subject is face away from camera and the face/eye region is not visible. However, as we know, human interactions are usually dynamic activities, focusing on LAEO cases will largely ignore eye movement dynamics. To our knowledge, multi-class human interaction labels scene have not been used in supervising gaze estimation models. Besides, we are also the first work to learn temporal dependency of 3D gaze direction from interaction transitions.

## 3 Method

### Problem Formulation

We target at dynamic 3D gaze estimation in videos by modeling both temporal and spatial gaze relationships, as shown in Fig. 2. For a given image sequence \(\bm{I}=\{I_{1},\cdots,I_{T}\}\) that contains gaze communications among two or more people, we want to predict the gaze \(\bm{g}\) and their interaction category \(\bm{a}\) for each frame, by modeling the spatio-temporal gaze relationships. Assuming that the human gaze interactions can be fully inferred from gaze and can affect future gaze direction, the problem is formulated as:

\[P(\mathbf{g}_{1:T},\mathbf{a}_{1:T}|\mathbf{I}_{1:T})=P(\mathbf{g}_{1}|I_{1}) \ P(\mathbf{a}_{1}|\mathbf{g}_{1})\prod_{t=1}^{T-1}\{P(\mathbf{g}_{t+1}| \mathbf{g}_{t},\mathbf{a}_{t},I_{t+1})\ P(\mathbf{a}_{t+1}|\mathbf{g}_{t+1})\}\] (1)

The gaze \(\bm{g}_{t}\in\mathbb{R}^{N_{t}\times 3}\) contains multiple unit gaze vectors for \(N_{t}\) subjects in frame \(t\), i.e., \(\bm{g}_{t}=[g_{t,1},\cdots,g_{t,N_{t}}]\). The gaze interaction variable \(\bm{a}_{t}\in\mathbb{Z}^{N_{t}\times 6}\) represents the one-hot interaction category vector for six types of interactions, including {_single, mutual, avert, refer, follow, share_} and \(\bm{a}_{t}=[a_{t,1},\cdots,a_{t,N_{t}}]\). In Eq. 1, we model \(P(\mathbf{a}_{t}|\mathbf{g}_{t})\) with learning a Human Interaction Graph \(G_{t}\) from \(\mathbf{g}_{t}\); then we model \(P(\mathbf{g}_{t+1}|\mathbf{g}_{t},\mathbf{a}_{t},I_{t+1})\) with a Structure-aware Transformer and an Autoregressive Temporal Transformer.

### Method Overview

We show the overview of the proposed framework in Fig. 3. Given a sequence of subject head images \(\bm{I}=\{I_{1},\cdots,I_{T}\}\), our network is defined as \(\mathcal{F}(\cdot)\) and contains five sets of network parameters, including the ResNet-18 feature extractor \(\mathcal{F}_{\Theta_{1}}\), the Structure-aware Transformer layer \(\mathcal{F}_{\Theta_{2}}\) for updating the gaze feature with multi-subject interaction information, a temporal model \(\mathcal{F}_{\Theta_{3}}\) built with Autoregressive Transformer Layers, a Fully-Connected (FC) layer \(\mathcal{F}_{\Theta_{4}}\) for regressing the gaze vector and an interaction classifier \(\mathcal{F}_{\Theta_{5}}\) for distinguishing the interaction category from gaze.

The prediction is run in an autoregressive manner, as formulated in Eq. 1. The predicted gaze \(\mathbf{g}_{t}\) at time \(t\) will be used to construct the edges of a Human-Interaction graph \(G_{t}\), then we use s Structure-aware Transformer and a Temporal Transformer to model the distribution of future gaze, given the historical gaze and the gaze interaction. In the Human-Interaction Module, we first construct a human-interaction graph \(\mathcal{G}_{t}=(V_{t},E_{t})\) using gaze and position

Figure 2: Problem formulation for Interaction-Aware dynamic gaze estimation.

information for each frame \(t\). Then we update the gaze representations in both spatial and temporal scale. The updated gaze feature are feed into prediction head and predict human interaction categories for each subject.

In Section 3.3, we show details about building the Human-Interaction Graph from predicted gaze. In Section 3.4, we describe the structure-aware transformer for learning the gaze interaction relationships. In Section 3.5, we describe the temporal module for predicting future gaze. In Section 3.6, we define a interaction classifier using the updated gaze features. We propose a multi-state training strategy to refine the dynamic estimate module. We define the loss function for training in Section 3.7 and describe the training algorithm in Section 3.8.

### Human Interaction Graph

We use human interaction videos and labels from VACATION Fan et al. (2019) dataset to perform joint learning of 3D gaze dynamics and gaze interaction dynamics. The bounding boxes of all subjects and objects involved in the communication scene are provided. With the predicted gaze and bounding box locations, we propose to construct a human-interaction graph \(\bm{G}_{t}=(\bm{V}_{t},\bm{E}_{t})\) for every frame \(t\), where the nodes \(\bm{V}_{t}\) can be further split into subjects nodes \(\bm{V}_{t}^{s}\) and objects nodes \(\bm{V}_{t}^{o}\). There can be a directed edge \(e^{(ij)}\in\mathbf{E}_{t}\) from node \(v^{i}\) to node \(v^{j}\), indicating the subject \(i\) is looking at another subject or an object.

At frame \(t\), given the predicted gaze direction \(\bm{g}_{t}=\{g_{t,i}\}_{i=1}^{N}\) and gaze uncertainty \(\bm{\sigma}_{t}=\{\sigma_{t,i}\}_{i=1}^{N}\), we calculate the inter-activeness score for each subject-subject and subject-object pair, defined as below. For subject \(i\), we first generate a 2D gaze attention map \(M_{i}\) by calculating the angular difference \(\theta\) between the gaze vector and the vector from one image pixel to the head center position \([d_{i,x},d_{i,y}]\), formulated as

\[\theta_{i}(x,y)=\arccos(\frac{x-d_{i,x},y-d_{i,y}\cdot(g_{x},g_{y})}{\|(x-d_{ i,x},y-d_{i,y})\|_{2}\cdot\|g_{x},g_{y}\|_{2}})\text{ and }M_{i}(x,y)=\max(1-\frac{\theta_{x,y}}{\alpha\sigma_{i}},0)\] (2)

Figure 3: Overview of our method.

where \(\alpha>1\) is a hyperparameter, deciding the scope of gaze view field. Then we follow the depth rebasing method in Fang et al. (2021) to optimize the gaze field using the relative depth information. Examples of the initial gaze view field and optimized ones are shown in Fig. 4. At last, we calculate the interaction score \(\bm{c}_{i}\) from subject \(i\) to other subjects or objects as:

\[c_{ij}=mean([M_{i}(x,y)],\forall(x,y)\text{ in bbox}_{j})\] (3)

The interaction score \(c_{ij}\leq 1\) represents the probability of the edge connectivity. When the ground truth edge connectivity \(e_{ij}^{gt}\in\{0,1\}\) is given, we can compute a graph structure loss to refine the gaze direction, which is defined below.

\[\begin{split}& L_{G,t}=\frac{1}{|V_{t}^{s}|(|V_{t}^{s}|+|V_{t}^{0}|) }\sum_{i}^{|V_{t}^{s}|}\sum_{j}^{|V_{t}^{s}+V_{t}^{0}|}-e_{ij}^{gt}\log c_{ij} -(1-e_{ij}^{gt})log(1-c_{ij})\\ & L_{G}=\frac{1}{T}\sum_{t}^{T}L_{G,t}\end{split}\] (4)

### Structure-aware Transformer

To model the distribution of \(P(\mathbf{g}_{t+1}|\mathbf{g}_{t},\mathbf{a}_{t},I_{t+1})\), we propose to build a sequential model consists of a structure-aware Transformer and an autoregressive transformer that can generate gaze prediction conditioned on previous gaze, human interaction and image feature. The "structure-aware" transformer integrates gaze interaction information for generating

Figure 4: Visualization of the gaze view field based on the predicted gaze. First row: the predicted gaze direction. Second row: the 2D gaze field map generated based on the 2D gaze direction and the depth information in Eq. 2. Third row: the optimized gaze field map after applying depth rebasing by adding depth threshold on the gaze target region.

interaction-aware gaze embedding. As shown in Fig. 5, given the extracted image feature \(\bm{h}_{t}\) in a frame for \(N\) subjects, we project the feature vector to gaze feature embedding space to generate image tokens. From the human interaction graph \(G_{t}\), we find the gaze target with the highest interaction score and feed the bounding box coordinates into the "bbox embedding" layer to generate target position tokens. The image token and target position tokens are concatenated as the input of transformer layers. The processing steps of the transformer model are described below.

\[\begin{split}&\text{Input: image feature }\bm{h}_{t+1},\text{Interaction Graph: }G_{t-1}\\ &\text{tokens: }\bm{q}_{t+1,k}=[\text{MLP}(h_{t+1,k});\text{MLP}( \text{bbox}_{t,k})],k=1,\cdots,N\\ &\text{self-attn: }\bm{q}_{t+1}^{(l)}=\text{LN}(\text{MHS}(Q^{\bm{q} _{t+1}^{(l-1)}},K^{\bm{q}_{l-1}},V^{\bm{q}_{t+1}^{(l-1)}})+\bm{q}_{t+1}^{(l-1)} )\\ &\text{FFN: }\bm{q}_{t+1}^{(l)}=\text{FFN}(\bm{q}_{t+1}^{(l)}),l=1, \cdots,N_{d}\\ &\text{Output: }\bm{\tilde{h}}_{t+1}=\bm{q}_{t}^{(N_{d})}\end{split}\] (5)

The output \(\bm{\tilde{h}}_{t+1}\) is the updated gaze feature embedding at frame \(t+1\), which will be used as input of the Temporal model. We introduce the details in next section.

### Autoregressive Transformer for Temporal Relationship Learning

In stead of only predicting gaze direction for the central frame in a sequence like (Kellnhofer et al. (2019); Kothari et al. (2021)), our gaze module predicts gaze for each frame in the sequence as we will need gaze direction to learn human interaction dynamics. A shown in Fig. 3, the concatenated feature sequences are feed into the Temporal Module, which is for capturing the temporal dependencies of gaze directions.

Gaze EmbeddingGiven the predicted gaze \(\mathbf{g}_{t}\) at time \(t\), we project \(\mathbf{g}_{t}\) to a \(d\)-dimension vector \(\bm{s}_{t}\) through a linear projection function, defined as:

\[\bm{s}_{t}=\begin{cases}\bm{W}^{s}\cdot\bm{g}_{t}+\bm{b}^{s},t\geq 1\\ \bm{b}^{z},t=0\end{cases}\] (6)

Figure 5: Detailed network structure of the Spatial and Temporal Transformer.

where \(t=0\) represents the begin token and \(\bm{W}^{s}\in\mathbb{R}^{d\times 3}\) and \(\bm{b}^{s}\in\mathbb{R}^{d\times 1}\) represent the weight matrix and bias.

Periodic Positional EncodingConsider that the gaze direction could be quite consistent in a sequence, we refer to the method from Fan et al. (2022) to add a Periodic Positional Encoding (PPE) to the gaze embedding vectors, indicating the temporal order. The PPE is expressed by the function below.

\[\begin{split} PPE_{(t,2i)}&=\sin((t\text{ mod }P)/(10000)^{2t/d})\\ PPE_{(t,2i+1)}&=\cos((t\text{ mod }P)/(10000)^{2t /d})\end{split}\] (7)

where the \(i\) is the dimension index and \(P\) is a hyper-parameter defining the period. The gaze embedding vector \(\bm{s}_{t}\) will be added to the PPE before feeding them to the Autoregressive Transformer layer, expressed as:

\[\bm{\hat{s}}_{t}=\bm{s}_{t}+PPE(j),t=1,\cdots,T\] (8)

Autoregressive TransformerTo model the temporal dependency of gaze movement under certain human interactions, we refer to the transformer decoder architecture used in the GPT models and design an module that autoregressively predict the gaze in one future step. Given the updated feature \(\tilde{\bm{h}}_{t+1}\) from spatial model and predicted gaze direction \(\bm{g}_{t}\) at previous time step, we model the distribution \(P(\bm{g}_{t+1}|\bm{g}_{t},\tilde{\bm{h}}_{t+1})\) with the devised Autoregressive Transformer \(\mathcal{F}_{\Theta_{3}}\). In each layer of the autoregressive model, there is a Multi-Head self-attention layer (MHS) and a Multi-Head cross-attention (MHC) layer, inserted with residual connections and layer normalization(LN). The processing of our temporal model can be written as:

\[\begin{split}\text{self-attn: }\hat{\bm{s}}_{l}^{(1)}& =\text{LN(MHS}(Q^{\bm{\hat{s}}_{l-1}},K^{\bm{\hat{s}}_{l-1}},V^{ \bm{\hat{s}}_{l-1}})+\bm{\hat{s}}_{l-1})\\ \text{cross-attn: }\hat{\bm{s}}_{l}^{(2)}&=\text{ LN(MHC}(Q^{\bm{\hat{s}}_{l}^{(^{\prime})}},K^{\bm{\tilde{h}}_{l}^{(1)}},V^{\bm{ \tilde{h}}_{l}^{(1)}})+\bm{\hat{s}}_{l}^{(1)})\\ \text{FFN: }\hat{\bm{s}}_{l}&=\text{FFN}(\bm{\hat{s}}_{l }^{(2)}),l=1,\cdots,N_{d}\\ \text{Output:}\tilde{\tilde{\bm{h}}}_{t}&=\bm{ \hat{s}}_{N_{d},t}\end{split}\] (9)

where \(l\) is the layer index and we can concatenate \(N_{d}\) layers in total.

Regressing Gaze and UncertaintyThe output of the Autoregressive Transformer \(\tilde{\tilde{\bm{h}}}_{t}\) are feed into the FC layer to regress for a probabilistic gaze prediction, described as \((\gamma,\phi,\sigma)\), where \(\gamma,\phi\) is the gaze direction in sphere coordinate system and \(\sigma\) represents for the gaze concentration, which reflects the gaze uncertainty. The angular formulation \(\gamma,\phi\) can be converted to a unit 3D gaze vector \(g=[g_{x},g_{y},g_{z}]\) by solving \(\{\frac{g_{x}}{g_{z}}=-tan(\gamma);g_{y}=sin(\phi);g_{x}^{2}+g_{y}^{2}+g_{z}^ {2}=1\}\). Given \(\tilde{\tilde{\bm{h}}}_{t}\), the FC layer generate gaze prediction for \(t+1\), written as:

\[\bm{g}_{t+1},\sigma_{t+1}=\mathcal{F}_{\Theta_{4}}(\tilde{\tilde{\bm{h}}}_{t})\] (10)

### Human Interaction Classifier

As we are also interested in the human interaction states in the video, we build a classifier \(\mathcal{F}_{\Theta_{5}}(\cdot)\) to generate human interaction predictions, utilizing the gaze feature \(\tilde{\tilde{\bm{h}}}_{t}\) from Eq. 9 combined with the target for every subject. The predicted interaction category is expressed as:

\[\bm{a}_{t}=\text{softmax}(\mathcal{F}_{\Theta_{5}}([\tilde{\tilde{\bm{h}}}_{t}; \text{bbox}_{t}]))\] (11)

When the ground-truth gaze interaction labels are given, we define a interaction loss:

\[L_{Interaction}=\frac{1}{NT}\sum_{t}^{T}\sum_{i}^{N}-\bm{a}_{t,i}^{gt}\log\bm{a} _{t,i}-(1-\bm{a}_{t,i}^{gt})log(1-\bm{a}_{t,i})\] (12)

### Loss Function

The overall training loss function for our model is :

\[Loss=\lambda_{1}L_{gaze}+\lambda_{2}L_{smooth}+\lambda_{3}L_{G}+\lambda_{4}L_{ interaction}\] (13)

where \(L_{G}\) and \(L_{Interaction}\) are gaze interaction graph loss and interaction classification loss, as defined in Eq. 4 and Eq. 12. We define \(L_{gaze}\) and \(L_{smooth}\) below.

We compute the negative log-likehood loss with the predicted gaze angle \(\bm{g}_{t}=[\bm{\gamma},\bm{\phi}]\) and uncertainty \(\bm{\sigma}_{t}\), when the ground-truth gaze labels are given. The gaze loss \(L_{gaze}\) is defined as:

\[L_{gaze}=\frac{1}{T}\sum_{t}^{T}(\log(\bm{\sigma_{t}})+\frac{1}{\bm{\sigma}_{ t}}\|\bm{g}_{t}-\bm{g}_{t}^{gt}\|_{2})\] (14)

We also impose a smoothness constraint along the temporal axis, to minimize the difference between the predicted gaze in two consecutive frames, which is formulated as:

\[L_{smooth}=\frac{1}{T-1}\sum_{t=1}^{T-1}\|\bm{g}_{t}-\bm{g}_{t+1}\|_{2}\] (15)

### Training Algorithm

As gaze benchmark dataset and human gaze interaction dataset do not have intersecting labels, we propose a multi-state algorithm to train the full model.

* Stage 1: temporal model pre-training (the blue part in Fig. 3). We use the gaze benchmark datset that contains image sequences/videos and frame-by-frame gaze annotations to pre-train the feature extractor and the temporal model (\(\mathcal{F}_{\Theta_{1}}(\cdot),\mathcal{F}_{\Theta_{3}}(\cdot),\mathcal{F}_{ \Theta_{4}}(\cdot)\));
* Stage 2: Freeze \(\mathcal{F}_{\Theta_{3}}(\cdot)\) and train spatial model (the orange part in Fig. 3). We use human interaction dataset that contains image sequences/videos and frame-by-frame interaction annotation to train the feature extractor, structure-aware Transformer and the interaction classifier, i.e., (\(\mathcal{F}_{\Theta_{1}}(\cdot),\mathcal{F}_{\Theta_{2}}(\cdot),\mathcal{F}_{ \Theta_{4}}(\cdot),\mathcal{F}_{\Theta_{5}}(\cdot)\));
* Stage 3: Full model training on mixture data.

We show the training algorithm in Algorithm. 1.

[MISSING_PAGE_EMPTY:11]

### Within-Dataset Evaluation

Gaze estimationTo prove that our proposed geometric constraints improve gaze estimation performance, we first perform within-dataset evaluation on Gaze360 and Eye-Diap, as shown in Table 1. We compare our method, the Gaze-Geo and UGaze-Geo, with SOTA learning-based methods, including RT-Gene (Fischer et al. (2018)), Dilated-Net (Chen and Shi (2018)), CA-Net (Cheng et al. (2020)), Gaze360 (Kellnhofer et al. (2019)), LAEO (Kothari et al. (2021)) and L2CS Abdelrahman et al. (2022). On Gaze360 we use the official train-val-test set division and present the evaluation results on different ranges of gaze directions, including frontal faces (column 2 in Table 1) and all faces (column 3 in Table 1). As we know, LAEO Kothari et al. (2021) only consider the "looking at each other" constraints, which is one type of the gaze interactions. Our model considers multiple types of gaze communication activities and the dynamic state transitions among them. Our method outperforms other methods on Gaze360, especially on the full range of evaluation set Gaze360(full), our model reduces the gaze angular error by 14.6% comparing with LAEO (Kothari et al.). Our dynamic gaze estimation model also improves the performance on frontal poses slightly compared to Kothari et al. (2021).

Gaze Interaction ClassificationWe also perform an evaluation of the gaze interaction classification accuracy, compared with Fan et al. (2019). For the six types of gaze communications, we calculate the precision and F1 score. As shown in Fig. 2, our model has

Figure 6: Visualization of predicted gaze direction and gaze interaction probability. Three people “Potter”, “Hermione”, “Ron” are interacting in the scene. **Row-a**: the predicted gaze from the pre-trained gaze estimator (without involving interaction information in training). **Row-b**: predicted gaze from fine-tuned gaze estimator with interaction labels. **Row-c**: the Human Interaction Graph constructed from the predicted gaze. **Row-d**: the predicted probability of interaction class for Node H.

bettwe performance on the task of classifying gaze interactions between two subjects. In terms of F1 score, our dynamic gaze model improves the prediction accuracy by 2.8% on average compared to Fan et al. (2019). In addition, we can also observe the advantages of utilizing gaze direction directly for analyzing specific gaze interactions, such as "single" ((\(\uparrow\) 11.95% ), "follow" (\(\uparrow\) 22.07%), and "share" (\(\uparrow\) 7.89%). In Fig. 6, we show an example of gaze & interaction prediction on a test video of VACATION dataset. We compare the gaze direction before & after applying the interaction-based fine-tuning and we can see evident improvement in the qualitative results. We also show the constructed Human Interaction Graph using the gaze direction, as described in Eq. 3.

### Cross-Dataset Evaluation

We also conduct a cross-dataset experiment to elaborate that, by modeling the spatial gaze relationships and temporal dependency, our gaze estimator is robust and have better generalization ability under large data difference. Following the cross-data settings adopted in existing works PureGaze Cheng et al. (2022) and RAT Bao et al. (2022), we train our model on Gaze360 and ETH-XGaze and then evaluate on EyeDiap. In Table 3 we compare the performance of our model with SOTA gaze estimation methods, including RT-Gene, Dilated-Net, CA-Net, FullFace, Gaze360, PureGaze and RAT.

As shown in Table 3, our model can achieve SOTA cross-dataset performance when testing on new video dataset. Compared to the dynamic model Gaze360 (Kellnhofer et al. (2019)) that only predicts the gaze for the central frame, our model can significantly reduce the gaze error on EyeDiap dataset by 57.8%. Compared to the 2D data augmentation method Bao et al. (2022), our gaze model benefit from the spatial constraints constructed based on the subject interactions, especially when one subject is fully occluded due to large head pose. Our model outperforms Bao et al. (2022) by reducing the cross-dataset gaze error by 5.9%.

### Ablation Study

We perform an ablation study to validate the effectiveness of our training algorithm for each stage for gaze estimation. In Table. 4, we analyze the within- and cross-dataset performances with different combinations of spatial and temporal modules. We explore five different combinations listed as below.

* Static Model: ResNet feature extractor (\(\mathcal{F}_{\Theta_{1}}\)) + FC layer(\(\mathcal{F}_{\Theta_{4}}\)), trained on gaze images.
* Static + Interaction: ResNet feature extractor (\(\mathcal{F}_{\Theta_{1}}\)) + Structure-aware Transformer(\(\mathcal{F}_{\Theta_{2}}\)) + Interaction classifier(\(\mathcal{F}_{\Theta_{5}}\)) + FC layer(\(\mathcal{F}_{\Theta_{4}}\)), trained on gaze images and gaze interaction images.

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|} \hline Task & single & mutual & avert & refer & follow & share & Avg. \\ \hline Fan et al. (2019) & 26.17 & 98.60 & 74.28 & 53.16 & 18.05 & 46.61 & 55.02 \\ Dyn-Gaze (Ours) & 38.12 & 90.27 & 68.79 & 55.67 & 40.12 & 54.50 & 57.92 \\ \hline \end{tabular}
\end{table}
Table 2: F1-score of gaze communication prediction on VACATION dataset.

* Interaction only: ResNet feature extractor (\(\mathcal{F}_{\Theta_{1}}\)) + Structure-aware Transformer(\(\mathcal{F}_{\Theta_{2}}\)) + Interaction classifier(\(\mathcal{F}_{\Theta_{5}}\)) + FC layer(\(\mathcal{F}_{\Theta_{4}}\)), trained on gaze interaction images (weakly supervised training).
* Temporal: ResNet feature extractor (\(\mathcal{F}_{\Theta_{1}}\)) + Autoregressive Transformer(\(\mathcal{F}_{\Theta_{3}}\)) + FC layer(\(\mathcal{F}_{\Theta_{4}}\)), trained on gaze sequences.
* Temporal + Interaction: our final model, trained on gaze sequences and gaze interaction sequences.

The Static model is the baseline model trained with full-supervision on gaze benchmark dataset, without considering the spatial interaction or temporal dependency. By comparing Static Model with Static + Interaction model, we show that utilizing multi-subject gaze interaction can help to refine the gaze estimator, especially for the full pose cases. However, only using gaze interaction labels (without any gaze label supervision) will fail to generate reliable gaze estimation results. By comparing the Static Model with the Temporal Model, we show the effectiveness to consider the temporal dependency of gaze movement, as we can observe significant performance improvement on both within- and cross-dataset experiments. Our final model that learns interaction dependency and temporal dependency achieves the best within dataset performances on both Gaze360 and EyeDiap with significant improvement compared to the Static Model.

\begin{table}
\begin{tabular}{|c|c c|c|c|} \hline \multirow{2}{*}{Models} & \multicolumn{2}{c|}{Within-data} & \multicolumn{2}{c|}{Cross-data} \\ \cline{2-5}  & Gaze360 (frontal) & Gaze360 (full) & EyeDiap & Gaze360 \(\xrightarrow{}\) EyeDiap \\ \hline Static Model & 10.54 & 13.55 & 4.89 & 7.23 \\ Static + Interaction & 10.25 & 12.18 & 4.86 & 7.31 \\ Interaction only & 28.8 & 30.12 & 15.15 & - \\ Temporal & 10.24 & 12.98 & 4.44 & **6.60** \\ Temporal + Interaction (Full) & **10.03** & **11.27** & **4.25** & 6.68 \\ \hline \end{tabular}
\end{table}
Table 4: Ablation study of gaze angular errors when applying different constraints and w/o uncertainty modeling during training. The last two rows are corresponding to Gaze-Geo and UGaze-Geo.

\begin{table}
\begin{tabular}{c c} \hline Methods & Gaze360 \(\xrightarrow{}\) EyeDiap \\ \hline FullFace Zhang et al. (2017) & 14.42 \\ RT-Gene Fischer et al. (2018) & 38.60 \\ Dilated-Net Chen and Shi (2018) & 23.88 \\ Gaze360 Kellnhofer et al. (2019) & 11.86 \\ CA-Net Cheng et al. (2020) & 31.41 \\ PureGaze Cheng et al. (2022) & 9.32 \\ Res-Net18+RAT Bao et al. (2022) & 7.10 \\ \hline
**Dyn-Gaze (ours)** & **6.68** \\ \hline \end{tabular}
\end{table}
Table 3: Cross-dataset evaluation from Gaze360 to EyeDiap and comparision with SOTA learning-based methods

### Conclusion

In this paper we propose a framework to perform interaction-aware dynamic gaze estimation in videos, which utilize the gaze communication labels among multiple subjects/objects and the temporal dependency of gaze movement to refine the gaze estimator. Specifically, we define a direct mapping from predicted 3D gaze direction to human gaze interaction types and construct a Human Interaction Graph based on gaze and bounding box locations. We perform the dynamic gaze prediction in an auto-regressive manner, by modeling the future gaze distribution conditioned on current gaze and human interaction graph structure. Our model fully utilize the dataset without gaze annotations and propose a multi-stage training algorithm to alternately updating the temporal gaze prediction module and gaze interaction module. In terms of performances, we proved that by introducing the interaction constraints and temporal constraints, our model can be significantly improved compared to the static model on video dataset.

## References

* A Abdelrahman et al. (2022) Ahmed A Abdelrahman, Thorsten Hempel, Aly Khalifa, and Ayoub Al-Hamadi. L2csnet: fine-grained gaze estimation in unconstrained environments. _arXiv preprint arXiv:2203.03339_, 2022.
* Bao et al. (2022) Yiwei Bao, Yunfei Liu, Haofei Wang, and Feng Lu. Generalizing gaze estimation with rotation consistency. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4207-4216, 2022.
* Chen and Shi (2018) Zhaokang Chen and Bertram E Shi. Appearance-based gaze estimation using dilated-convolutions. In _Asian Conference on Computer Vision_, pages 309-324. Springer, 2018.
* Cheng et al. (2020) Yihua Cheng, Shiyao Huang, Fei Wang, Chen Qian, and Feng Lu. A coarse-to-fine adaptive network for appearance-based gaze estimation. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 10623-10630, 2020.
* Cheng et al. (2022) Yihua Cheng, Yiwei Bao, and Feng Lu. Puregaze: Purifying gaze feature for generalizable gaze estimation. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 436-443, 2022.
* Chong et al. (2018) Eunji Chong, Nataniel Ruiz, Yongxin Wang, Yun Zhang, Agata Rozga, and James M Rehg. Connecting gaze, scene, and attention: Generalized attention estimation via joint modeling of gaze and scene saliency. In _Proceedings of the European conference on computer vision (ECCV)_, pages 383-398, 2018.
* Chong et al. (2020) Eunji Chong, Yongxin Wang, Nataniel Ruiz, and James M Rehg. Detecting attended visual targets in video. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 5396-5406, 2020.
* Fan et al. (2019) Lifeng Fan, Wenguan Wang, Siyuan Huang, Xinyu Tang, and Song-Chun Zhu. Understanding human gaze communication by spatio-temporal graph reasoning. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 5724-5733, 2019.

* Fan et al. (2022) Yingruo Fan, Zhaojiang Lin, Jun Saito, Wenping Wang, and Taku Komura. Faceformer: Speech-driven 3d facial animation with transformers. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18770-18780, 2022.
* Fang et al. (2021) Yi Fang, Jiapeng Tang, Wang Shen, Wei Shen, Xiao Gu, Li Song, and Guangtao Zhai. Dual attention guided gaze target detection in the wild. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 11390-11399, 2021.
* Fischer et al. (2018) Tobias Fischer, Hyung Jin Chang, and Yiannis Demiris. Rt-gene: Real-time eye gaze estimation in natural environments. In _Proceedings of the European conference on computer vision (ECCV)_, pages 334-352, 2018.
* Mora et al. (2014) Kenneth Alberto Funes Mora, Florent Monay, and Jean-Marc Odobez. Eyediap: A database for the development and evaluation of gaze estimation algorithms from rgb and rgb-d cameras. In _Proceedings of the symposium on eye tracking research and applications_, pages 255-258, 2014.
* Kellnhofer et al. (2019) Petr Kellnhofer, Adria Recasens, Simon Stent, Wojciech Matusik, and Antonio Torralba. Gaze360: Physically unconstrained gaze estimation in the wild. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 6912-6921, 2019.
* Kothari et al. (2021) Rakshit Kothari, Shalini De Mello, Umar Iqbal, Wonmin Byeon, Seonwook Park, and Jan Kautz. Weakly-supervised physically unconstrained gaze estimation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9980-9989, 2021.
* Marin-Jimenez et al. (2019) Manuel J Marin-Jimenez, Vicky Kalogeiton, Pablo Medina-Suarez, and Andrew Zisserman. Laeo-net: revisiting people looking at each other in videos. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3477-3485, 2019.
* Marin-Jimenez et al. (2014) Manuel Jesus Marin-Jimenez, Andrew Zisserman, Marcin Eichner, and Vittorio Ferrari. Detecting people looking at each other in videos. _International Journal of Computer Vision_, 106:282-296, 2014.
* Nonaka et al. (2022) Soma Nonaka, Shohei Nobuhara, and Ko Nishino. Dynamic 3d gaze from afar: Deep gaze estimation from temporal eye-head-body coordination. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2192-2201, 2022.
* Palmero et al. (2018) Cristina Palmero, Javier Selva, Mohammad Ali Bagheri, and Sergio Escalera. Recurrent cnn for 3d gaze estimation using appearance and shape cues. _arXiv preprint arXiv:1805.03064_, 2018.
* Cantarino et al. (2020) Cristina Palmero Cantarino, Oleg V Komogortsev, and Sachin S Talathi. Benefits of temporal information for appearance-based gaze estimation. In _ACM Symposium on Eye Tracking Research and Applications_, pages 1-5, 2020.
* Park et al. (2020) Seonwook Park, Emre Aksan, Xucong Zhang, and Otmar Hilliges. Towards end-to-end video-based eye-tracking. In _Computer Vision-ECCV 2020: 16th European Conference,Glasgow, UK, August 23-28, 2020, Proceedings, Part XII 16_, pages 747-763. Springer, 2020.
* Recasens et al. [2015] Adria Recasens, Aditya Khosla, Carl Vondrick, and Antonio Torralba. Where are they looking? _Advances in neural information processing systems_, 28, 2015.
* Recasens et al. [2017] Adria Recasens, Carl Vondrick, Aditya Khosla, and Antonio Torralba. Following gaze in video. In _Proceedings of the IEEE International Conference on Computer Vision_, pages 1435-1443, 2017.
* Wang et al. [2019] Kang Wang, Hui Su, and Qiang Ji. Neuro-inspired eye tracking with eye movement dynamics. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9831-9840, 2019.
* Zhang et al. [2017] Xucong Zhang, Yusuke Sugano, Mario Fritz, and Andreas Bulling. It's written all over your face: Full-face appearance-based gaze estimation. In _Proceedings of the IEEE conference on computer vision and pattern recognition workshops_, pages 51-60, 2017.
* Zhang et al. [2020] Xucong Zhang, Seonwook Park, Thabo Beeler, Derek Bradley, Siyu Tang, and Otmar Hilliges. Eth-xgaze: A large scale dataset for gaze estimation under extreme head pose and gaze variation. In _European Conference on Computer Vision_, pages 365-381. Springer, 2020.