# StreamBench: Towards Benchmarking Continuous Improvement of Language Agents

 Cheng-Kuang Wu\({}^{1,2}\)

Zhi Rui Tam\({}^{1}\)

Chieh-Yen Lin\({}^{1}\)

Yun-Nung Chen\({}^{1,2}\)

Hung-yi Lee\({}^{2}\)

\({}^{1}\)Appier AI Research

\({}^{2}\)National Taiwan University

{brian.wu, ray.tam}@appier.com

Equal contribution

###### Abstract

Recent works have shown that large language model (LLM) agents are able to improve themselves from experience, which is an important ability for continuous enhancement post-deployment. However, existing benchmarks primarily evaluate their innate capabilities and do not assess their ability to improve over time. To address this gap, we introduce StreamBench, a pioneering benchmark designed to evaluate the continuous improvement of LLM agents over an input-feedback sequence. StreamBench simulates an online learning environment where LLMs receive a continuous flow of feedback stream and iteratively enhance their performance. In addition, we propose several simple yet effective baselines for improving LLMs on StreamBench, and provide a comprehensive analysis to identify critical components that contribute to successful streaming strategies. Our work serves as a stepping stone towards developing effective online learning strategies for LLMs, paving the way for more adaptive AI systems in streaming scenarios. Source code: https://github.com/stream-bench/stream-bench. Benchmark website: https://stream-bench.github.io.

## 1 Introduction

Recently, large-scale pretraining [1] and instruction fine-tuning [2] have driven paradigm shifts in how we interact with language models. These advancements allow us to use them out-of-the-box to solve problems. Consequently, many benchmarks have emerged to evaluate the general capabilities of these models. Some notable examples include MMLU [3], GSM8K [4], and BIG-Bench-Hard [5]. All these benchmarks aim to assess LLMs' _innate capabilities_, which we define as the general knowledge or reasoning abilities demonstrated when used out-of-the-box.

In addition to LLMs' strong _innate capabilities_, recent works have shown that LLM agents, which are LLMs augmented with extra components such as memory, retrievers, or tools, are able to improve themselves from experience. MemPrompt [6] shows that memory-enhanced GPT-3 can improve through time by storing past user feedback and retrieve them in the future. Reflexion [7] demonstrates that LLM agents can perform better in future trials by running repeated trials on the same dataset via self-reflection. ExpeL [8] further shows that LLM agents can learn from cross-task experience and improve performance without executing repeated trials on the target task.

Given LLM agents' self-improvement abilities, there remains a missing piece in the current evaluation landscape. Beyond measuring LLMs' _innate capabilities_ with aforementioned _offline_ benchmarks [3, 4, 5], it is important to assess their capacity to improve over time since we would like our systems to gradually improve after deployment. This gap motivated us to develop a new evaluation scenario-an _online_ setting to measure LLM agents' ability to continuously enhance their performance over time.

This _online_ setting focuses on scenarios where LLM agents attempt to solve a specific downstream task and improve themselves from an input-feedback sequence, with the goal to maximize the accuracy for the whole sequence of the agent's predictions.

Given these rationales, we introduce StreamBench, a benchmark designed to evaluate LLM agents' ability to improve themselves over an input-feedback sequence. StreamBench simulates an environment where LLM agents are exposed to a sequence of users' natural language requirements and feedback. To the best of our knowledge, StreamBench is the first benchmark to evaluate LLM agents in streaming scenarios with a diverse range of tasks. StreamBench aims to inspire further efforts to develop more adaptive LLM agents, thereby enhancing their practical effectiveness. Our contributions can be summarized as follows:

* We introduce StreamBench, the first benchmark designed to evaluate LLM agents' ability to improve over an input-feedback sequence in an _online_ setting across a wide range of tasks.
* We propose several simple yet effective baselines for enhancing LLM agents' performance in streaming scenarios, including a cost-effective multi-agent method that outperforms other baselines while maintaining the average cost of a single agent.
* We conduct analysis on the advantages and potential pitfalls of the proposed methods, providing insights into effective streaming strategies of LLMs.

## 2 Formulation

Consider a streaming scenario involving an agent, an external environment, and a sequence of inputs:

**Agent.** We define an _agent_ as an LLM parameterized by \(\theta\) and augmented with additional components to enhance the agent's capabilities, such as the external memory \(\mathcal{M}\) and a retriever \(r(\cdot)\) to store and retrieve useful information. Given an instance \(x\) in natural language, a prompting template \(p(\cdot)\), and a retrieval function \(r(\cdot)\), the agent's output is denoted as \(\hat{y}=f(p(x,r(\mathcal{M}))|\theta)\).

**Environment.** The external environment, denoted as \(g(\cdot)\), provides feedback to the agent. The nature of \(g(\cdot)\) varies depending on the specific downstream task and the type of feedback being collected. Potential roles for \(g(\cdot)\) include human users, code execution environments, and API responses.

**Input-feedback sequence.** Consider a sequence of input stream where each input is denoted by \(x_{t}\), with \(t\) representing the \(t\)-th time step. After the agent provides the output \(\hat{y}_{t}\), the environment provides feedback signal \(fb_{t}=g(x_{t},\hat{y}_{t})\). Figure 1 shows an overview of the streaming scenario.

Algorithm 1 presents a simple framework for language agents to continuously learn from feedback. Benchmark users can adapt Algorithm 1 or develop their own algorithms to update components of their language agents, with the goal to maximize the accuracy of the entire sequence.

Figure 1: (Left) A schematic diagram showing the online evaluation setting of StreamBench, where agents update their components (\(p,r,\mathcal{M}\), or \(\theta\)) from an input-feedback sequence to achieve the highest final accuracy (refer to Section 3.1 for details). (Right) Performance curve on the DDXPlus dataset on StreamBench. Agents are able to gradually improve with our proposed streaming baselines.

Traditionally, updating the agent at each time step \(t\) involves updating the model parameters \(\theta\). However, as foundation models grow increasingly larger, frequently updating the agent's network parameters has become computationally expensive. Recent advancements offer promising alternatives for iterative improvement by updating other components of the agent. For example, one can adapt existing iterative prompt refinement strategies to refine \(p(\cdot)\)[9; 10; 11], update the weights of the retriever \(r(\cdot)\)[12; 13; 14], expand the agent's memory \(\mathcal{M}\)[6; 15], or use parameter-efficient fine-tuning techniques for augmenting \(\theta\)[16]. These different strategies open new possibilities for continuous adaptation of agents without relying solely on network parameter updates. In this work, we develop several baselines for improving agents over time, with a particular focus on updating \(p(\cdot)\) and \(\mathcal{M}\). The baselines demonstrate both simplicity and effectiveness. We leave methods for updating \(r(\cdot)\) and \(\theta\), which require computationally expensive network parameter updates, for future research.

## 3 StreamBench

### General setup

Streaming sequenceMost public datasets are inherently static, meaning each instance does not have a time-related dependency. To adapt them for our streaming setup, we serialize each selected dataset in Section 3.2 by assigning a time step to each instance. To avoid arbitrary sequence assignment in the original datasets, we randomly shuffle each dataset using a fixed random seed. We release each dataset's assigned sequence obtained by this random seed in the supplementary materials to ensure reproducibility on StreamBench. Additionally, to ensure the robustness of our evaluation, we conduct experiments on different shuffled sequences with five random seeds, as discussed in Section 5.2. We also discuss the effects of distributional shifts in Appendix C.2.

Feedback signalsChoosing appropriate type of feedback signal is a crucial consideration in StreamBench. Firstly, cost and practicality play a significant role; in practice, obtaining ground truth \(y_{t}\) at each time step can be prohibitively expensive. For example, providing the exact code in programming tasks or the complete schema of each API call in tool use tasks is often impractical. In contrast, partial feedback, such as the helpfulness or correctness of the agent's output, is relatively easy to obtain-such as the "thumbs up" or "thumbs down" buttons commonly found in user interfaces of LLM applications. Given these rationales, we formalize the type of \(fb_{t}\) as follows:

\[fb_{t}=g(x_{t},\hat{y}_{t}),fb_{t}\in\{0,1\}\]

where \(fb_{t}\) is a scalar serving as a proxy for the correctness of \(\hat{y}_{t}\) with respect to \(x_{t}\), determined by the environment \(g(\cdot)\) of the given downstream tasks. The feedback \(fb_{t}\in\{0,1\}\) is binary, indicating whether the agent's output \(\hat{y}_{t}\) is correct. This simplified feedback setting aims to offer a unified evaluation framework for ensuring consistency and practicality across diverse tasks. We leave other designs of \(fb_{t}\), such as ground truth or natural language feedback, for future works.

EvaluationIn practice, an agent's goal is to satisfy as many user requirements as possible over a time sequence. We thus evaluate an agent by its aggregate metric at the final time step (\(T\)). For example, the final metric on a given dataset can be calculated as \(\frac{\sum_{t=1}^{T}h(\hat{y}_{t},y_{t})}{T}\), where \(h\) is the function for calculating the corresponding metric on a given dataset. Table 1 shows metrics for each dataset.

### Datasets

To measure LLM agents' capacity for continuous improvement post-deployment, we select a diverse set of downstream tasks with potential real-world applications. Following the setting in Section 3.1, these tasks share the property that their ground truth output \(y_{t}\) is costly to obtain at each time step.

Text-to-SQLFor text-to-SQL tasks, the agent has to convert users' natural language queries into SQL code to meet their data requirements. StreamBench integrates three prominent datasets: Spider [17], CoSQL [18], and BIRD [19]. These datasets represent a progressive difficulty curve, allowing for evaluation of how well agents improve when faced with data of varying difficulties.

Python programmingTo evaluate coding ability improvement, we use the DS-1000 [20] dataset, which consists of real-world Python programming questions from StackOverflow. To successfully solve a given question, the agent must provide a solution and pass the associated test cases.

Tool useThe ability to use external tools is a significant milestone in the development of LLMs, as it compensates for certain limitations, such as performing precise arithmetic operations or conducting web searches. For this purpose, we utilize the large-scale tool usage dataset ToolBench [21], and select the subset that includes stable and low-latency tool APIs collected in a previous work [22].

Medical diagnosisTo assess LLMs' continuous improvement in applying expert knowledge, we use the DDXPlus [23] dataset, where agents must make a medical diagnosis out of 49 diagnoses based on patient profiles detailing their symptoms. This setup mimics how medical doctors improve their diagnostic skills through accumulated patient encounters. Evaluating LLMs on this dataset helps us understand their potential for continuous improvement in a highly specialized domain.

Question answeringQuestion answering (QA) tasks evaluate an agent's ability to reason over supporting facts to answer users' questions. We adopt the distractor setting in HotpotQA [24], which requires reasoning over multiple supporting or distracting documents to answer questions. This helps measure the agent's improved proficiency in reasoning over grounded knowledge to provide accurate answers. Given the extensive volume of questions, we sampled 1,500 out of the total 7,410 questions.

Detailed information of the aforementioned datasets are provided in Table 1 and Appendix F.

## 4 Experiments

### Baselines

A key objective of StreamBench is to compare the performance gains of LLM agents using _non-streaming_ versus _streaming_ methods. In _non-streaming_ settings, methods focus on optimizing performance at a per-instance level, with improvements made independently for each testing instance. For these _non-streaming_ methods, the overall performance boost on the testing set stems from improvements on individual testing instances. In contrast, _streaming_ methods utilize information from past instances to improve future performance. For _streaming_ methods, we adapt two previously proposed methods and introduce two new approaches to explore effective streaming strategies.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline
**Task** & \multicolumn{2}{c}{**Text-to-SQL**} & **Python** & **Tool Use** & **Medical** & **QA** \\ \cline{2-7}
**Dataset** & **Spider** & **CoSQL** & **BIRD** & **DS-1000** & **ToolBench** & **DDXPlus** & **HotpotQA** \\ \hline Input (\(x_{t}\)) & \multicolumn{2}{c}{Data requirements} & Question & User query & Symptoms & Question \\ Output (\(y_{t}\)) & \multicolumn{2}{c}{SQL code} & Code & API calls & Diagnosis & Answer \\ Metric & \multicolumn{2}{c}{Execution accuracy} & Pass@1 & Accuracy & Accuracy & Exact Match \\ \hline Test size (\(T\)) & 2,147 & 1,007 & 1,534 & 1,000 & 750 & 1,764 & 1,500 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Input, output, evaluation metrics, and number of testing instances of selected datasets.

#### 4.1.1 Non-streaming methods

Zero-shotIt reflects the basic instruction-following abilities of LLMs for solving a given task.

Few-shotIt involves providing several ground truth \((x,y)\) pairs in the prompting template \(p(\cdot)\). For datasets with training sets, we construct few-shot examples from the training data. For datasets without training sets, we compose few-shot examples and inspect their quality to ensure reliability. We include few-shot examples for each dataset in the supplementary materials for reproducibility.

Chain-of-thought (CoT)Following previous work [25], we employ a trigger phrase (e.g., "Let's think step by step.") to instruct the LLM to generate the reasoning process before providing its final answer. We then extract the answer in the correct format from the generated reasoning text.

Self-RefineSelf-Refine[26] is a technique where the LLM iteratively improves its output based on self-feedback. The model generates an initial response and refines it through multiple iterations of refinement. It leverages LLMs' ability to self-evaluate and adjust its responses at a per-instance level.

#### 4.1.2 Streaming methods

GrowPromptWe adapt the previously proposed method GrowPrompt [6], where \((x_{t},\hat{y}_{t},fb_{t})\) of the latest time steps are stored in a sliding window \(\mathcal{W}\). The contents of \(\mathcal{W}\) are incorporated into the prompt at inference time to output \(y_{t}=f(p(x_{t},\mathcal{W})|\theta)\). This provides the agent with information from the past \(k\) instances, where \(k\) is a hyperparameter. Since LLM agents take text input, we verbalize \(fb_{t}\in\{0,1\}\) to inform the agent of whether its output \(y_{t}\) correctly satisfies the input \(x_{t}\).

MemPromptAs an advanced version of GrowPrompt, MemPrompt [6] incorporates an external memory \(\mathcal{M}\) to store \((x_{t},\hat{y}_{t},fb_{t})\) of all past time points. During inference, a retriever \(r(\cdot)\) is used to select \(k\) elements from \(\mathcal{M}\), and \(fb_{t}\) is also verbalized to inform the agent of \(\hat{y}_{t}\)'s correctness.

Self-StreamICLPrevious works [27; 28] have found that incorrect examples can negatively impact in-context learning (ICL) performance, though the extent varies for different LLMs. Based on these insights, we hypothesize that while GrowPrompt and MemPrompt use verbalized \(fb_{t}\) to inform the agent about the correctness of its output, incorrect \(\hat{y}_{t}\) still introduces distracting signals that can hinder improvement. Therefore, we propose to save \((x_{t},\hat{y}_{t})\) pairs to memory \(\mathcal{M}\)_only when \(fb_{t}=1\)_, eliminating the need to save verbalized \(fb_{t}\). This method, called Self-StreamICL, operates similarly to regular ICL, except that the labels are now self-generated and gradually accumulate over the data stream, without the need to preconstruct few-shot examples. For more details, refer to Algorithm 2.

Multi-Agentic-Memory StreamICIn Self-StreamICL, the agent learns exclusively from its own past experiences. However, we hypothesize that different LLM agents possess distinct strengths and weaknesses, so they can potentially benefit from the experiences of other agents. To explore this idea, we introduce Multi-Agentic-Memory StreamICL (MAM-StreamICL), which employs a multi-agent framework where multiple LLM agents share a common memory. This shared memory incorporates the past outputs of all agents, allowing each agent to learn from the diverse experiences of the others.

We implement a simple round-robin-like scheduling to switch between different LLM agents outlined in Algorithm 2. This ensures that each agent contributes to the shared memory in a balanced manner. Our experiments show that this straightforward strategy can boost performance beyond the average performance of the individual agents. In fact, Self-StreamICL can be seen as a special case of MAM-StreamICL with only one LLM agent.

Note that the high cost associated with scaling is the most critical drawback of multi-agent methods proposed by previous works, and the key advantage of MAM-StreamICL is its cost-effectiveness. Unlike methods such as Multiagent Debate [29] and RECONCILE [30], the cost of MAM-StreamICL does not scale proportionally with the number of agents. Instead, _the cost is equivalent to the averaged cost of a single agent, since only one agent is assigned to answer at each time step_.

### Implementation details

We conduct experiments using three LLM families: GPT [31; 32], Gemini [33; 34], and Claude [35]. For our main experiments, we use the endpoints gpt-3.5-turbo-0125, gemini-1.0-pro-001, and Claude-3-haiku-20240307. These models represent cost-effective LLMs, balancing performance and affordability. The models initialize the \(K=3\) agents in MAM-StreamICL. For methods with \(\mathcal{M}\) (MemPrompt, Self-StreamICL, and MAM-StreamICL), we implement \(\mathcal{M}\) as a vector database. We use BAAI/bge-base-en-v1.5 to encode \(x_{t}\) as key embeddings and save \(x_{t}\), \(\hat{y}_{t}\) (and \(fb_{t}\) for MemPrompt) as values. For hyperparameters and prompts, refer to Appendix A and F.

### Main results

The main results are shown in Table 2, which lists the averaged performance of the three LLM agents. The only exception is MAM-StreamICL, which only runs once on the streaming sequence where each agent takes turns at each time step. For results of each respective model, refer to Appendix B.

Overall, streaming methods outperform non-streaming methods, though the extent of improvement varies across different datasets. These results demonstrate the value of leveraging input-feedback streams to enhance agent performance on downstream tasks. In addition, as demonstrated by the robust performance of Self-StreamICL compared to GrowPrompt and MemPrompt, leveraging feedback as simple as correctness can enable agents to improve even more through self-generated outputs \(\hat{y}_{t}\). This provides an important insight: rather than solely focusing on prompting pipelines to boost per-instance performance, adopting simple yet effective streaming approaches, such as collecting correctness feedback, could potentially lead to notable improvements on LLM agents. Lastly, MAM-StreamICL shows the most notable and consistent performance boost across all datasets.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline
**Task** & \multicolumn{2}{c}{**Text-to-SQL**} & \multicolumn{1}{c}{**Python**} & \multicolumn{1}{c}{**Tool Use**} & \multicolumn{1}{c}{**Medical**} & \multicolumn{1}{c}{**QA**} \\ \cline{2-7}
**Dataset** & **Spider** & **CoSQL** & **BIRD** & **DS-1000** & **ToolBench** & **DDXPlus** & **HotpotQA** \\ \hline \multicolumn{7}{l}{_Non-streaming_} & & & & & & \\ \hline Zero-Shot & 67.89 & 50.55 & 29.60 & 37.70 & 61.38 & 52.85 & 48.49 \\ Few-Shot & 68.55 & 50.61 & 30.40 & 33.33 & 68.58 & 60.98 & 53.11 \\ CoT & 61.53 & 46.01 & 27.23 & 25.93 & 58.98 & 58.20 & 52.47 \\ Self-Refine & 67.75 & 49.49 & 29.62 & 36.30 & 60.67 & 52.89 & 43.53 \\ \hline \multicolumn{7}{l}{_Streaming_} & & & & & & \\ \hline GrowPrompt & 69.90 & 51.97 & 30.35 & 33.77 & 65.07 & 55.10 & 51.38 \\ MemPrompt & 70.78 & 53.29 & 31.99 & 35.47 & 64.31 & 54.02 & 52.62 \\ Self-StreamICL & 74.63 & 55.05 & 35.31 & 41.30 & 71.33 & 70.56 & 54.80 \\ MAM-StreamICL & **75.69** & **55.17** & **36.38** & **43.10** & **75.87** & **83.50** & **55.20** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Averaged performance of three LLM agents across different baselines and datasets.

## 5 Discussion

### What makes effective streaming strategies?

This subsection provides insights into the key aspects that contribute to successful streaming strategies. We identify two effective factors for streaming improvements as follows:

#### 5.1.1 Collecting correct self-output

To investigate whether incorrect self-output hinders agents' improvement, we conducted ablation studies on GrowPrompt and MemPrompt. In the default setting in Table 2, both methods use all \(k\) retrieved \((x_{t},\hat{y}_{t},fb_{t})\) triples during inference (use all). In contrast, the ablations either use only the triples where \(fb_{t}=0\) (only incorrect), or use only the triples where \(fb_{t}=1\) (only correct).

The ablation results in Figure 2 reveal several findings. First, using incorrect self-output degrades performance, sometimes even worse than the zero-shot baseline. In contrast, using only correct self-output consistently boosts performance over the zero-shot baseline, with particularly consistent improvements observed in the MemPrompt (only correct) method. An important observation is that, even if \(fb_{t}\) is verbalized to inform the agent whether its \(\hat{y}_{t}\) correctly satisfies \(x_{t}\) in GrowPrompt and MemPrompt, simply informing the agent that its self-output is incorrect does not help it learn from past mistakes. Conversely, telling the LLM agent what it does correctly is very effective in facilitating improvement. These findings underscore the importance of collecting and utilizing correct self-output in streaming. This also explains the intuition and effectiveness behind Self-StreamICL, where input-output pairs are saved to \(\mathcal{M}\) only when the self-output is correct.

#### 5.1.2 Sharing memory across multiple agents

Another key insight is the benefit of sharing memory across multiple agents, as demonstrated in MAM-StreamICL. To analyze why memory sharing works, we use DDXPlus as an example and visualize the confusion matrices for a subset of diagnoses related to upper respiratory tract diseases.

Figure 3 presents the confusion matrices for three different LLM agents: gpt-3.5-turbo-0125, gemini-1.0-pro, and claude-3-haiku-20240307, along with the matrix of MAM-StreamICL. Each matrix illustrates the proficiency of an agent across various medical diagnosis categories. It is evident that each model excels in certain areas while struggling in others. For instance, gpt-3.5-turbo-0125 shows high accuracy in predicting "acute rhinosinusitis" and "allergic sinusitis" but struggles with "chronic rhinosinusitis" and "URTI". In contrast, gemini-1.0-pro performs well in "URTI", and claude-3-haiku could solve "chronic rhinosinusitis".

The diversity in performance across models suggests that their collective past experiences can provide complementary strengths, thereby enhancing overall performance when these experiences are shared. Since each agent takes turn to solve an incoming \(x_{t}\) at each time point \(t\), the shared memory system allows the agents to benefit from others while maintaining a cost similar to that of a single agent. We also conduct further ablation studies in Appendix D to discuss the importance of sharing memory.

Figure 2: Correctness ablations. The y-axis denotes performance difference from zero-shot. The results are the average of three LLM endpoints. Please refer to Appendix D for results of each LLM.

### Robustness to different streaming sequences

Given the time-variant nature of streaming, evaluating each method's robustness across different data streams is essential. Therefore, we rerun the streaming baselines with 5 random seeds on five tasks. Figure 4 presents the averaged performance and standard errors of claude-3-haiku across 5 shuffled sequences, with results for gpt-3.5-turbo and gemini-1.0-pro provided in Appendix C. The performance ranking of streaming baselines remains mostly consistent across datasets, with Self-StreamICL and MAM-StreamICL being the top performers. Due to the high cost of running all 5 sequences on StreamBench, we select a fixed sequence for fair comparison among future benchmark users. However, we also release all 5 sequences for those who wish to conduct a thorough evaluation.

### Would stronger LLMs still benefit from streaming?

To evaluate whether stronger LLMs still benefit from streaming, we tested two newer models: gpt-4o-2024-08-06 and gemini-1.5-flash-001. Due to the high cost, we only run the methods shown in Table 3. We found that with Self-StreamICL, these stronger models still showed significant performance improvements. This demonstrates that even the most advanced models can leverage the information from streaming data to further enhance their performance across diverse tasks.

### Cost analysis

For benchmark users to estimate the cost, the token usage of all baselines is listed in Appendix E.

Figure 4: Averaged performance and standard errors of each method on five shuffled sequences.

Figure 3: Confusion matrices of the diagnoses subset of upper respiratory tract diseases in DDXPlus.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline
**Task** & \multicolumn{2}{c}{**Text-to-SQL**} & \multicolumn{1}{c}{**Python**} & \multicolumn{1}{c}{**Tool Use**} & \multicolumn{1}{c}{**Medical**} & \multicolumn{1}{c}{**QA**} \\ \cline{2-7}
**Dataset** & **Spider** & **CoSQL** & **BIRD** & **DS-1000** & **ToolBench** & **DDXPlus** & **HotpotQA** \\ \hline _gemini-1.5-flash_ & & & & & & & \\ \hline Zero-shot & 69.63 & 48.26 & 33.83 & 50.20 & 69.47 & 58.90 & 60.60 \\ Few-shot & 71.40 & 49.35 & 37.03 & 50.60 & 72.13 & 73.58 & 64.87 \\ CoT & 72.52 & 52.73 & 35.14 & 44.80 & 68.00 & 64.06 & 63.13 \\ Self-StreamICL & **77.83** & **56.21** & **41.20** & **52.20** & **75.07** & **86.34** & **65.20** \\ \hline _gpt-4o-2024-08-06_ & & & & & & & \\ \hline Zero-shot & 73.54 & 53.33 & 34.42 & 54.90 & 72.40 & 70.64 & 65.53 \\ Few-shot & 76.85 & 57.60 & 36.25 & 52.30 & 71.47 & 83.45 & 66.87 \\ CoT & 72.52 & 54.82 & 31.16 & 41.90 & 66.80 & 73.02 & 62.80 \\ Self-StreamICL & **80.58** & **59.19** & **42.63** & **59.40** & **76.27** & **92.01** & **67.00** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Performance of gpt-4o-2024-08-06 and gemini-1.5-flash-001 on StreamBench.

Related work

### Online learning

Online learning [36] explores the incremental updating of models as new data arrives, making it valuable for dynamic improvement in downstream applications. Traditionally, it focuses updating network weights, such as in methods for training recurrent neural networks [37], online representation learning for image classification [38], and adapting language models to learn new world knowledge [39]. Recent advancements have introduced strategies for improving LLM agents by updating prompts [9; 10; 11], memory [6; 15], or retrievers [12; 13; 14]. These new strategies are promising for designing new algorithms to adapt LLMs in the online setting. However, there are no standard testbeds for this setup. Addressing this gap, we propose StreamBench, the first benchmark to pave the way for developing more dynamic adaptation methods for LLM agents.

### Improvement from feedback with LLMs

Recent works have shown that LLMs can improve from feedback when augmented with prompting pipelines or memory mechanisms, forming two main research branches. One is instance-level improvement methods, such as ReAct [40], Self-ICL [41], and Self-Refine [26]. These methods focus on boosting performance on each input instance without leveraging information from past instances. The other is time-sequence-level improvement methods. For example, MemPrompt [6] enhances GPT-3 by storing past user feedback and retrieve them in the future. Reflexion [7] shows that LLM agents can perform better in future trials by running repeated trials on the same dataset, but this is not always practical in real-world scenarios. ExpeL [8] shows that LLM agents can benefit from cross-task experience without needing repeated trials on the target task. However, these works use different datasets and lack a standardized evaluation setting. StreamBench bridges this gap by providing a consistent empirical testbed across diverse tasks to evaluate LLM agents' improvement.

## 7 Conclusion

In this work, we introduce a new evaluation setting to measure LLM agents' performance improvement on downstream tasks, and propose StreamBench as an instance of this setting. There are two major findings in our experiments. Firstly, collecting correct self-generated outputs improve performance consistently, while informing agents of their incorrect outputs sometimes degrade performance. Secondly, sharing memory across multiple agents is a promising cost-effective technique, as MAM-StreamICL achieves robust performance while maintaining the average cost of a single agent.

StreamBench serves as a stepping stone towards more adaptive AI systems. Future directions include exploring _online active learning_ where agents could inquire feedback only when necessary, or viewing multi-agent collaboration as multi-arm bandits (MABs) to develop more sophisticated methods for selecting agents and sharing memory at each time point. It is also practical to investigate the utilization of different feedback signals beyond correctness, such as users' natural language feedback. We hope that this work inspires development of adaptive methodology for improving LLM agents.

## 8 Limitations

Tasks and modality coverageThe current version of StreamBench includes tasks such as programming, text-to-SQL conversion, medical diagnosis, question-answering, and tool use. While diverse, they do not encompass all possible types of tasks or domains where LLMs can be applied. StreamBench is also limited to text and does not cover other modalities such as image and audio.

Sim2Real gapAlthough we have attempted to simulate feedback signals as practical as possible, there may still be a gap between the simulated correctness feedback in StreamBench and the feedback encountered in real-world applications. Real-world feedback can be more diverse, noisy, and context-dependent, which may not be fully captured by the current benchmark.

## Acknowledgments and Disclosure of Funding

We would like to express our gratitude to Chih-Han Yu, Wei-Lin Chen, Yi-Lin Tsai, Chao-Chung Wu, Zhen-Ting Liu, and An-Zi Yen for their valuable comments and feedback on this work. Their insights greatly contributed to the improvement of our research. We declare no competing interests related to this work.

## References

* Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* Wei et al. [2021] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. _arXiv preprint arXiv:2109.01652_, 2021.
* Hendrycks et al. [2020] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In _International Conference on Learning Representations_, 2020.
* Cobbe et al. [2021] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. _arXiv preprint arXiv:2110.14168_, 2021.
* Suzgun et al. [2023] Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. In _Findings of the Association for Computational Linguistics: ACL 2023_, pages 13003-13051, 2023.
* Madaan et al. [2022] Aman Madaan, Niket Tandon, Peter Clark, and Yiming Yang. Memory-assisted prompt editing to improve gpt-3 after deployment. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 2833-2861, 2022.
* Shinn et al. [2023] Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: language agents with verbal reinforcement learning. In _Neural Information Processing Systems_, 2023.
* Zhao et al. [2024] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. Expel: Llm agents are experiential learners. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 19632-19642, 2024.
* Zhou et al. [2022] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. _arXiv preprint arXiv:2211.01910_, 2022.
* Pryzant et al. [2023] Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt optimization with gradient descent and beam search. _arXiv preprint arXiv:2305.03495_, 2023.
* Guo et al. [2023] Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu Yang. Connecting large language models with evolutionary algorithms yields powerful prompt optimizers. _arXiv preprint arXiv:2309.08532_, 2023.
* Rubin et al. [2022] Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for incontext learning. In _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 2655-2671, 2022.
* Ye et al. [2023] Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, and Lingpeng Kong. Compositional exemplars for in-context learning. In _International Conference on Machine Learning_, pages 39818-39833. PMLR, 2023.

* [14] Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, and Xipeng Qiu. Unified demonstration retriever for in-context learning. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 4644-4668, 2023.
* [15] Xiaonan Li and Xipeng Qiu. Mot: Memory-of-thought enables chatgpt to self-improve. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 6354-6374, 2023.
* [16] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. Parameter-efficient fine-tuning of large-scale pre-trained language models. _Nature Machine Intelligence_, 5(3):220-235, 2023.
* [17] Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, et al. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 3911-3921, 2018.
* [18] Tao Yu, Rui Zhang, He Yang Er, Suyi Li, Eric Xue, Bo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze Shi, Zihan Li, et al. Cosql: A conversational text-to-sql challenge towards cross-domain natural language interfaces to databases. _arXiv preprint arXiv:1909.05378_, 2019.
* [19] Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, et al. Can l llm already serve as a database interface? a big bench for large-scale database grounded text-to-sql s. _Advances in Neural Information Processing Systems_, 36, 2024.
* [20] Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wentau Yih, Daniel Fried, Sida Wang, and Tao Yu. Ds-1000: A natural and reliable benchmark for data science code generation. In _International Conference on Machine Learning_, pages 18319-18345. PMLR, 2023.
* [21] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis, 2023.
* [22] Boshi Wang, Hao Fang, Jason Eisner, Benjamin Van Durme, and Yu Su. Llms in the imaginarium: tool learning through simulated trial and error. _arXiv preprint arXiv:2403.04746_, 2024.
* [23] Arsene Fansi Tchango, Rishab Goel, Zhi Wen, Julien Martel, and Joumana Ghosn. Ddxplus: A new dataset for automatic medical diagnosis. _Advances in Neural Information Processing Systems_, 35:31306-31318, 2022.
* [24] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. _arXiv preprint arXiv:1809.09600_, 2018.
* [25] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. _Advances in neural information processing systems_, 35:22199-22213, 2022.
* [26] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. _Advances in Neural Information Processing Systems_, 36, 2024.
* [27] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 11048-11064, 2022.

* [28] Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, et al. Larger language models do in-context learning differently. _arXiv preprint arXiv:2303.03846_, 2023.
* [29] Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. _arXiv preprint arXiv:2305.14325_, 2023.
* [30] Justin Chih-Yao Chen, Swarnadeep Saha, and Mohit Bansal. Reconcile: Round-table conference improves reasoning via consensus among diverse l lms. _arXiv preprint arXiv:2309.13007_, 2023.
* [31] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. _ArXiv_, abs/2005.14165, 2020.
* [32] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [33] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.
* [34] Gemini Team Google. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. _ArXiv_, abs/2403.05530, 2024.
* [35] AI Anthropic. The claude 3 model family: Opus, sonnet, haiku. _Claude-3 Model Card_, 2024.
* [36] Steven CH Hoi, Doyen Sahoo, Jing Lu, and Peilin Zhao. Online learning: A comprehensive survey. _Neurocomputing_, 459:249-289, 2021.
* [37] Ronald J Williams and David Zipser. A learning algorithm for continually running fully recurrent neural networks. _Neural computation_, 1(2):270-280, 1989.
* [38] Yanis Bahroun and Andrea Soltoggio. Online representation learning with single and multi-layer hebbian networks for image classification. _arXiv preprint arXiv:1702.06456_, 2017.
* [39] Nathan Hu, Eric Mitchell, Christopher D Manning, and Chelsea Finn. Meta-learning online adaptation of language models. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 4418-4432, 2023.
* [40] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. _arXiv preprint arXiv:2210.03629_, 2022.
* [41] Wei-Lin Chen, Cheng-Kuang Wu, Yun-Nung Chen, and Hsin-Hsi Chen. Self-icl: Zero-shot in-context learning with self-generated demonstrations. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 15651-15662, 2023.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] See the experiment results in Section 4.3. 2. Did you describe the limitations of your work? See Section 8. 3. Did you discuss any potential negative societal impacts of your work? [N/A] 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? Please refer to the supplementary materials. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? Please refer to Appendix A and F. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? See the results in Figure 4. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Section 5.4.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? See Section 3.2. 2. Did you mention the license of the assets? See the supplementary materials. 3. Did you include any new assets either in the supplemental material or as a URL? We include code in the supplementary materials. 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? See the supplementary materials. 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?Hyperparameters

For decoding strategies of all model endpoints used in this work, we set _temperature_ to \(0\) and _top-p_ to \(1\). The few-shot baseline and streaming baselines (GrowPrompt, MemPrompt, Self-StreamICL, and MAM-StreamICL) incorporate information from \(k\) instances into the prompt \(p(\cdot)\) to improve LLM agents. We use the same \(k\) across these baselines for fair comparison. We set \(k=16\) for Spider. CoSQL, BIRD, ToolBench, and DDXPlus. For DS-1000 and HotpotQA, we set \(k=4\) to avoid exceeding the context size of gpt-3.5-turbo-0125.

We also analyze how different text embeddings used in memory correlate with streaming performance in Table 4. We observe that within the same text encoder family (bge), the larger model (109M parameters) generally delivers better performance. However, smaller models (22.7M parameters) can also achieve strong results, indicating that each LLM may benefit from a specific encoder.

## Appendix B Main results of each LLM endpoint

Main experiments results for three different LLM families models are shown below:

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline
**Task** & \multicolumn{2}{c}{**Text-to-SQL**} & \multicolumn{2}{c}{**Python**} & \multicolumn{2}{c}{**Tool Use**} & \multicolumn{1}{c}{**Medical**} & \multicolumn{1}{c}{**QA**} \\ \cline{2-7}
**Dataset** & **Spider** & **CoSQL** & **BIRD** & **DS-1000** & **ToolBench** & **DDXPlus** & **HotpotQA** \\ \hline \multicolumn{7}{l}{_Non-streaming_} \\ \hline Zero-Shot & 68.28 & 49.26 & 28.16 & 33.20 & 61.73 & 50.57 & 49.47 \\ Few-Shot & 68.33 & 49.65 & 31.49 & 29.40 & 66.27 & 57.48 & 54.40 \\ CoT & 52.31 & 40.81 & 21.71 & 21.10 & 58.40 & 59.30 & 49.67 \\ Self-Refine & 69.59 & 46.47 & 28.94 & 32.80 & 61.60 & 50.57 & 49.53 \\ \hline \multicolumn{7}{l}{_Streaming_} \\ \hline GrowPrompt & 71.59 & 52.43 & 28.68 & 33.10 & 61.87 & 51.13 & 52.80 \\ MemPrompt & 70.28 & 54.22 & 30.18 & 35.50 & 58.80 & 43.59 & 55.00 \\ StreamICL & 76.48 & 55.41 & 33.25 & 35.80 & 68.80 & 69.50 & 57.20 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Performance of different baselines and datasets for gemini-1.0-pro-001

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Text encoder / LLMs** & **gpt-3.5-turbo-0125** & **claude-3-haiku** & **gemini-1.5-flash-001** \\ \hline all-MiniLM-L6-v2 (22.7M) & 63.61 & **78.91** & 83.50 \\ bge-small-en-v1.5 (33.4M) & 63.55 & 75.51 & 83.90 \\ bge-base-en-v1.5 (109M) & **66.16** & 76.02 & **86.34** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Performance of Self-StreamICL (implemented with different text encoders and LLMs) on the DDXPlus dataset.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
**Task** & \multicolumn{2}{c}{**Text-to-SQL**} & \multicolumn{1}{c}{**Python**} & \multicolumn{1}{c}{**Tool Use**} & \multicolumn{1}{c}{**Medical**} & \multicolumn{1}{c}{**QA**} \\ \cline{2-5}
**Dataset** & **Spider** & **CoSQL** & **BIRD** & **DS-1000** & **ToolBench** & **DDXPlus** & **HotpotQA** \\ \hline \multicolumn{5}{l}{_Non-streaming_} \\ \hline Zero-Shot & 68.89 & 52.83 & 29.75 & 41.50 & 64.13 & 47.56 & 54.53 \\ Few-Shot & 69.54 & 52.73 & 28.94 & 33.30 & 70.13 & 54.31 & 54.93 \\ CoT & 65.53 & 47.96 & 29.21 & 32.80 & 57.20 & 53.18 & 57.13 \\ Self-Refine & 67.21 & 51.64 & 29.92 & 39.80 & 64.53 & 47.68 & 40.06 \\ \hline \multicolumn{5}{l}{_Streaming_} \\ \hline GrowPrompt & 70.89 & 53.43 & 30.31 & 27.20 & 67.60 & 44.62 & 55.80 \\ MemPrompt & 73.68 & 54.32 & 34.16 & 29.40 & 67.07 & 51.53 & 56.73 \\ StreamICL & 75.59 & 54.92 & 35.07 & 41.90 & 74.00 & 66.16 & 55.80 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Performance of different baselines and datasets for gpt-3.5-turbo-0125

[MISSING_PAGE_EMPTY:15]

[MISSING_PAGE_EMPTY:16]

## Appendix E Token cost breakdown for each LLM endpoint

Table 10, 11, 12, and 13 shows how many millions of input, output tokens are used by gpt-3.5-turbo-0125, gemini-1.0-pro-001, claude-3-haiku-20240307, and the latest models. The cost of MAM-StreamICL is simply the averaged cost of the three LLMs due to the round-robin algorithm. As most LLM endpoints use input and output tokens to charge usage fees, we provide this information for benchmark users to estimate the cost for running StreamBench.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline
**Task** & \multicolumn{3}{c}{**Text-to-SQL**} & \multicolumn{1}{c}{**Python**} & \multicolumn{1}{c}{**Tool Use**} & \multicolumn{1}{c}{**Medical**} & \multicolumn{1}{c}{**QA**} \\ \cline{2-9}
**Dataset** & **Spider** & **CoSQL** & **BIRD** & **DS-1000** & **ToolBench** & **DDXPlus** & **HotpotQA** \\ \hline _Non-streaming_ & & & & & & & \\ Zero-Shot & 0.7340.064 & 0.392/0.023 & 1.338/0.070 & 0.522/0.053 & 5.202/0.019 & 1.131/0.013 & 2.182/0.014 \\ Few-Shot & 2.367/0.061 & 0.954/0.021 & 3.206/0.074 & 1.984/0.074 & 5.869/0.023 & 8.138/0.014 & 9.864/0.013 \\ Ccl & 0.778/0.181 & 0.407/0.077 & 1.252/0.168 & 0.550/0.134 & 5.239/0.053 & 1.186/0.184 & 2.227/0.082 \\ Self-Refine & 2.317/0.164 & 1.285/0.079 & 4.634/0.245 & 1.802/0.078 & 45.67/0.262 & 2.384/0.023 & 10.43/0.101 \\ \hline _Streaming_ & & & & & & & \\ \hline GrowPrompt & 2.819/0.065 & 1.205/0.021 & 3.309/0.069 & 2.103/0.088 & 5.910/0.019 & 7.699/0.012 & 10.56/0.013 \\ MemPrompt & 2.711/0.064 & 1.193/0.021 & 3.178/0.065 & 2.066/0.090 & 5.890/0.018 & 7.932/0.013 & 10.60/0.013 \\ Self-StreamICL & 2.156/0.063 & 0.966/0.021 & 2.688/0.066 & 1.765/0.073 & 5.756/0.019 & 7.833/0.013 & 10.46/0.013 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Cost analysis (using millions of input and output tokens of gpt-3.5-turbo-0125).

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline
**Task** & \multicolumn{3}{c}{**Text-to-SQL**} & \multicolumn{1}{c}{**Python**} & \multicolumn{1}{c}{**Tool Use**} & \multicolumn{1}{c}{**Medical**} & \multicolumn{1}{c}{**QA**} \\ \cline{2-9}
**Dataset** & **Spider** & **CoSQL** & **BIRD** & **DS-1000** & **ToolBench** & **DDXPlus** & **HotpotQA** \\ \hline _Non-streaming_ & & & & & & \\ Zero-Shot & 0.871/0.110 & 0.454/0.045 & 1.538/0.114 & 0.591/0.044 & 5.603/0.020 & 1.131/0.011 & 2.222/0.018 \\ Few-Shot & 2.639/0.108 & 1.062/0.043 & 3.668/0.120 & 2.215/0.157 & 6.301/0.023 & 8.553/0.010 & 9.863/0.015 \\ CoT & 0.930/0.322 & 0.477/0.130 & 1.443/0.319 & 0.626/0.158 & 5.626/0.873 & 1.191/0.310 & 2.273/0.083 \\ Self-Refine & 1.944/0.131 & 0.987/0.056 & 2.977/0.129 & 1.869/0.205 & 11.14/0.038 & 2.350/0.019 & 4.587/0.025 \\ \hline _Streaming_ & & & & & & \\ \hline GrowPrompt & 4.307/0.152 & 1.774/0.053 & 4.103/0.109 & 2.146/0.040 & 6.334/0.020 & 8.077/0.010 & 10.82/0.016 \\ MemPrompt & 3.405/0.100 & 1.693/0.049 & 4.012/0.109 & 2.073/0.039 & 6.325/0.020 & 8.354/0.011 & 10.88/0.015 \\ Self-StreamICL & 3.402/0.142 & 1.212/0.034 & 3.299/0.106 & 1.880/0.037 & 6.175/0.019 & 8.250/0.010 & 10.79/0.014 \\ \hline \hline \end{tabular}
\end{table}
Table 11: Cost analysis (using millions of input and output tokens of gemini-1.0-pro-001).

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline
**Method** & **GPT** & **Gemini** & **Claude** & **Memory** & **Inference** \\ \hline Zero-Shot & 47.56 & 50.57 & 60.43 & x & single agent \\ Self-StreamICL & 66.16 & 69.50 & 76.02 & single agent & single agent \\ MAM-StreamICL (ablation) & 65.31 & 72.05 & 81.52 & single agent & multi agent \\ MAM-StreamICL & **83.50** & **83.50** & **83.50** & multi agent & multi agent \\ \hline \hline \end{tabular}
\end{table}
Table 9: Ablation studies with MAM-StreamICL on DDXPlus. The ablated version of MAM-StreamICL only uses memory of the single corresponding agent, while still uses round-robin algorithm for multi-agent inference. We can see that both multi-agent memory and inference are beneficial for performance boost. The detailed algorithm for this ablation study can be found in Appendix F.

## Appendix F Supplementary materials

The supplementary materials include details such as preprocessing of datasets, prompts of each baseline method, and code to reproduce the experiments.

### Code repository

The code for reproducing the experiments can be found in our GitHub repository: https://github.com/stream-bench/stream-bench.

### Details for each dataset

We provide the licenses, preprocessing pipelines, calculation of evaluation metrics, and links to the datasets in StreamBench: https://huggingface.co/datasets/appier-ai-research/StreamBench. To construct the streaming sequences, one only needs to download the datasets and follow the instructions in our code repository.

#### f.2.1 Spider

PreprocessingWe download the Spider [17] dataset from their project website: https://yale-lily.github.io/spider, and use the original test set as our test set on StreamBench2.

Footnote 2: The current Spider on StreamBench refers to Spider 1.0. Please refer to their project website for details on the upcoming version of Spider.

Evaluation metricWe adopt the commonly used Execution Accuracy (EA) for all three Text-to-SQL datasets (Spider, CoSQL, and BIRD). This metric quantifies the proportion of instances where the execution result of the generated SQL \(\hat{y}_{t}\) is identical to that of the ground truth SQL \(y_{t}\) across all instances from time step \(t=1\) to \(T\), where \(T\) is the number of instances in the test set:

\[\mathrm{EA}=\frac{\sum_{t=1}^{T}\mathbbm{1}\left(r_{t},\hat{r}_{t}\right)}{T}\]

Here, \(r_{t}\) represents the execution results of \(y_{t}\), while \(\hat{r}_{t}\) is the execution results of \(\hat{y}_{t}\), with \(\mathbbm{1}(\cdot)\) being the indicator function defined by:

\[\mathbbm{1}(r_{t},\hat{r}_{t})=\begin{cases}1,&r_{t}=\hat{r}_{t}\\ 0,&r_{t}\neq\hat{r}_{t}\end{cases}\]

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline
**Task** & \multicolumn{3}{c}{**Text-to-SQL**} & \multicolumn{1}{c}{**Python**} & \multicolumn{1}{c}{**Tool Use**} & \multicolumn{1}{c}{**Medical**} & \multicolumn{1}{c}{**QA**} \\ \cline{2-9}
**Dataset** & **Spider** & **CoSQL** & **BIRD** & **DS-1000** & **ToolBench** & **DDXPlus** & **HotpotQA** \\ \hline License & CC BY-SA & CC BY-SA & CC BY-SA & CC BY-SA & Apache-2.0 & CC-BY & CC BY-SA \\ \hline \hline \end{tabular}
\end{table}
Table 13: Cost analysis (millions of input and output tokens) on gemini-1.5-flash and gpt-4o.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline
**Task** & \multicolumn{3}{c}{**Text-to-SQL**} & \multicolumn{1}{c}{**Python**} & \multicolumn{1}{c}{**Tool Use**} & \multicolumn{1}{c}{**Medical**} & \multicolumn{1}{c}{**QA**} \\ \cline{2-9}
**Dataset** & **Spider** & **CoSQL** & **BIRD** & **DS-1000** & **ToolBench** & **DDXPlus** & **HotpotQA** \\ \hline _gemini-1.5-flash_ & & & & & & & \\ \hline _Zero-Shot_ & 0.851/0.103 & 0.439/0.045 & 1.386/0.110 & 0.690/0.048 & 5.603/0.023 & 1.119/0.019 & 2.222/0.018 \\ Self-StreamCLI & 2.622/0.092 & 1.274/0.042 & 3.339/0.109 & 2.108/0.045 & 6.201/0.023 & 8.018/0.016 & 10.83/0.013 \\ _gpt-4o-202.04-05-13_ & & & & & & & \\ \hline _Zero-Shot_ & 0.717/0.079 & 0.377/0.031 & 1.359/0.082 & 0.586/0.067 & 5.171/0.021 & 1.088/0.011 & 2.145/0.019 \\ Self-StreamCLI & 2.233/0.076 & 0.978/0.026 & 2.596/0.074 & 1.846/0.060 & 5.705/0.019 & 7.533/0.011 & 10.42/0.013 \\ \hline \hline \end{tabular}
\end{table}
Table 13: Cost analysis (millions of input and output tokens) on gemini-1.5-flash and gpt-4o.

#### f.2.2 CoSQL

PreprocessingThe CoSQL [18] dataset is sourced from its official website: https://yale-lily.github.io/cosql. Due to the unavailability of the official test set, we utilize the original development set as our test set on StreamBench. CoSQL was originally formatted in the multi-turn conversation structure, including a sequence of question-SQL pairs (i.e., \((x,y)\) pairs where \(x\) is the user's natural language question and \(y\) is the SQL code). To adapt CoSQL into the streaming framework of StreamBench, we extract each \((x,y)\) pair from the conversations to build the test set of size \(T=1\),007.

#### f.2.3 Bird

PreprocessingWe download the BIRD [19] dataset from their project website https://bird-bench.github.io/. Similar to COSQL, we use the full development set as our test set on StreamBench due to the unavailability of the original test set.

#### f.2.4 Ds-1000

PreprocessingWe use the DS-1000 [20] dataset on Huggingface: https://huggingface.co/datasets/xlangai/DS-1000. Since this dataset only contains the test set, we manually construct few-shot examples for the few-shot baseline. The few-shot examples are available in our code repository. Please refer to Section F.1.

Evaluation metricDS-1000 adopts pass@1 as the evaluation metric, which denotes the proportion of instances where the agent's code solution \(\hat{y}_{t}\) pass all test cases of \(x_{t}\) for \(t=1,2,...,T\).

#### f.2.5 ToolBench

PreprocessingSince the original ToolBench [21] contains large-scale real online APIs suffering from instability, we adopt the 50 high-quality APIs curated by STE [22]. Each API has 15 test instances, so there are 750 instances in the test set.

Evaluation metricWe following the same evaluation protocol specified in STE [22] to calculate the accuracy. The agent's output \(\hat{y}_{t}\) is considered correct if and only if both the API name and API arguments are correct. The API name is checked by exact string matching. For APIs that have deterministic values for the API arguments, exact string matching is performed. For APIs that accept natural language inputs, a judge LLM is used to evaluate the correctness of API arguments. The implementation details can be found in our code repository (Section F.1).

#### f.2.6 DDXPlus

PreprocessingDDXPlus [23] is a large-scale dataset for medical diagnosis, and it contains more than 100,000 instances in the test set originally. Since it would be too expensive to run all test instances on LLMs, we sample equal number of instances from each medical diagnosis to make the test set of size \(T=\) 1,764 on StreamBench. The full test set is available from the link provided in Section F.2, where \(x\) is the patient profile and \(y\) is the pathology (i.e., diagnosis). The original dataset can be found in the repository of DDXPlus: https://github.com/mila-iqia/ddxplus.

Evaluation metricWe use accuracy as the evaluation metric, which is calculated as \(\frac{\sum_{t=1}^{T}1(\hat{y}_{t}=y_{t})}{T}\).

#### f.2.7 HotpotQA

PreprocessingWe adopt the distractor setting in HotpotQA [24], where each instance contains both supporting or distracting documents for answering the question. The supporting documents have the information needed to answer the question, while the distracting documents do not. Because the test set is not available, we construct the test set by sampling 1,500 instances randomly from the dev set (distractor) downloaded from the official website: https://hotpotqa.github.io/.

Evaluation metricFollowing the HotpotQA paper, we adopt exact match (EM) and F1 as two evaluation metrics. We use EM as the primary evaluation metric on StreamBench. However, we also include the calculation of F1 in our code.

### Prompt templates

We use similar prompt templates in all tasks to minimize prompt engineering. To demonstrate, the prompt templates of the Text-to-SQL task (Spider, CoSQL, and BIRD) as well as the medical diagnosis task (DDXPlus) are provided below. Note that the Self-Refine [26] prompting pipeline involves using the zero-shot prompt to generate the initial output, and then use the _feedback_ prompt and _refinement_ prompt alternatingly to arrive at the final answer. Therefore, we provide two prompt templates for the Self-Refine baseline, one for feedback and the other for refinement. The prompt templates for other datasets (DS-1000, ToolBench, and HotpotQA) can be found in our code repository.

#### f.3.1 Text-to-SQL

The prompt templates for Spider, CoSQL, and BIRD are provided below.

Zero-shotIn this template, {schema} would be replaced by the database schema, while {question} would be replaced by the user's data requirements.

Chain-of-thought (CoT)It is similar to the zero-shot prompt template, except that the trigger phrase "take a deep breath and work on this problem step-by-step to derive the correct SQL code" is appended to the end.

Self-RefineThe feedback prompt and refinement prompt are provided below.

Self-RefineThe feedback prompt and refinement prompt are provided below.

Self-Refine

You are performing the text-to-SQL task. Here is the database schema, user's question, and your previously generated SQL code.

- SQL schema: {schema}
- User's question: {question}
- Your SQL code: {model_output}

First, determine whether you need to refine your SQL code in terms of its correctness.

If you consider that your SQL code is correct, output 'NO NEED TO REFINE' in uppercase.

Otherwise, provide a suggestion to correct the SQL code.

Figure 11: The prompt template for the zero-shot baseline for Text-to-SQL datasets.

Figure 12: The prompt template for the CoT baseline for Text-to-SQL datasets.

Figure 13: The feedback prompt template for the Self-Refine baseline for Text-to-SQL datasets.

You are performing the text-to-SQL task. Here is the database schema, user's question, and your previous answer-feedback trajectory.

- SQL schema: {schema}

- User's question: {question}

- Your previous answer-feedback trajectory:

{trajectory}

According to the latest feedback, provide your refined SQL code.

Provide your output in the following format:

""sqIn<your_SQL_code>n""

Prompt template for past informationFor the few-shot and streaming methods (GrowPrompt, MemPrompt, Self-StreamICL, and MAM-StreamICL), we use the following prompt template for integrating information of past instances.

You are performing the text-to-SQL task. Here are some examples:

{past_information}

Now it's your turn.

- SQL schema: {schema}

- Using valid SQLite, answer the following question for the SQL schema provided above.

- Question: {question}

Now, generate the correct SQL code directly (Do NOT generate other text except the SQL code):

NoteThe actual content of {past_information} would be replaced by \(k\) templated past instances, and the template is different for different baselines. We provide the templates as follows:

Question: {question}

{sq}_code}

Figure 16: The template for each past instance (\(k=16\) in Text-to-SQL) for the few-shot, Self-StreamICL, and MAM-StreamICL baselines.

Figure 14: The refinement prompt template for the Self-Refine baseline for Text-to-SQL datasets.

Figure 15: The prompt template for integrating past information for Text-to-SQL datasets.

[MISSING_PAGE_FAIL:23]

You are acting as medical doctor and tasked to diagnose the patient based on the provided patient profile. Here's the patient diagnosis:

[profile] All possible diagnoses for you to choose from are as follows (one diagnosis per line, in the format of <number>. <diagnosis>):

[option_text] - Your previous answer-feedback trajectory:

[trajectory] According to the latest feedback, provide your new answer

Provide your output in the following format: one diagnosis per line, in the format of <number>.

[background=] <diagnosis>

Prompt template for past informationFor the few-shot and streaming methods (GrowPrompt, MemPrompt, Self-StreamICL, and MAM-StreamICL), we use the following prompt template for integrating information of past instances.

[background=] Act as a medical doctor and diagnose the patient based on the provided patient profile.

All possible diagnoses for you to choose from are as follows (one diagnosis per line, in the format of <number>. <diagnosis>):

[option_text] Here are some example cases.

[past_information] Now it's your turn.

[profile] Now provide the diagnosis for the patient in the following format: <number>. <diagnosis>

NoteThe actual content of {past_information} is different for different baselines:

[background=] [profile] Diagnosis: {diagnosis}

Figure 23: The template for each past instance (\(k=16\) in DDXPlus) for the few-shot, Self-StreamICL, and MAM-StreamICL baselines.

Figure 21: The refinement prompt template for the Self-Refine baseline on DDXPlus.

Figure 22: The prompt template for multiple baselines on DDXPlus.

### Other details

#### f.4.1 Algorithm of ablation studies with MAM-StreamICL

In Table 9, we conduct ablation studies on MAM-StreamICL to show the importance of multi-agent memory. The algorithm of "MAM-StreamICL (ablation)" in this table is provided as follows:

```
1:Initialize \(K\) agents \(f_{0}(\cdot|\theta_{0})\), \(f_{1}(\cdot|\theta_{1})\),..., \(f_{K-1}(\cdot|\theta_{K-1})\); \(\triangleright\) K = 1 in the Self-StreamICL baseline
2:Initialize prompt \(p(\cdot)\), retriever \(r(\cdot)\), and external memory \(\mathcal{M}_{0}\), all shared between agents;
3:Select an agent \(f_{s}(\cdot|\theta_{s})\) as the source of single-agent memory; \(\triangleright\) For example, we can choose gemini-1.0-pro-001
4:for\(t=1,2,\ldots,T\)do
5: Receive instance \(x_{t}\) from the data stream;
6: Select the next agent by \(k=t\bmod K\);
7: The \(k\)-th agent predicts \(\hat{y}_{t}=f_{k}(p(x_{t},r(\mathcal{M}_{t-1}))|\theta_{k})\); \(\triangleright\)\(\hat{y}_{t}\) is used to for evaluation
8: The chosen single agent predicts \(\hat{y}_{t_{s}}=f_{s}(p(x_{t},r(\mathcal{M}_{t-1}))|\theta_{s})\); \(\triangleright\) Counterfactual ablation
9: Receive feedback signal \(f_{b_{t_{s}}}=g(x_{t},\hat{y}_{t_{s}})\); \(\triangleright\)\(\hat{y}_{t_{s}}\) is used for receiving feedback
10:if\(fb_{t_{s}}=1\)then\(\triangleright\) which means the self-output \(\hat{y}_{t}\) is correct
11:\(\mathcal{M}_{t}\leftarrow\mathcal{M}_{t-1}\cup\{(x_{t},\hat{y}_{t_{s}})\}\);
12:else
13:\(\mathcal{M}_{t}\leftarrow\mathcal{M}_{t-1}\);
14:endif
15:endfor ```

**Algorithm 3** Algorithm for MAM-StreamICL (ablation)

The parts different from the original MAM-StreamICL algorithm is highlighted in red. This ablated algorithm can be seen as a counterfactual experiment, where we use multiple agents for _inference_ but only one chosen agent for the _memory_ mechanism. The results in Table 9 show that both multi-agent memory and multi-agent inference are beneficial for performance boost.