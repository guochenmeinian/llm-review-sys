# Improving Context-Aware Preference Modeling

for Language Models

 Silviu Pitis\({}^{a,b}\)   Ziang Xiao\({}^{c}\)   Nicolas Le Roux\({}^{b,d}\)   Alessandro Sordoni\({}^{b,d}\)

\({}^{a}\)University of Toronto \({}^{b}\)Microsoft Research \({}^{c}\)Johns Hopkins University \({}^{d}\)MILA

###### Abstract

While finetuning language models (LMs) from pairwise preferences has proven remarkably effective, the underspecified nature of natural language presents critical challenges. Direct preference feedback is uninterpretable, difficult to provide where multidimensional criteria may apply, and often inconsistent, either because it is based on incomplete instructions or provided by diverse principals. To address these challenges, we consider the two-step preference modeling procedure that first resolves the under-specification by selecting a context, and then evaluates preference with respect to the chosen context. We decompose reward modeling error according to these two steps, which suggests that supervising context in addition to context-specific preference may be a viable approach to aligning models with diverse human preferences. For this to work, the ability of models to evaluate context-specific preference is critical. To this end, we contribute _context-conditioned_ preference datasets and accompanying experiments that investigate the ability of language models to evaluate context-specific preference. We use our datasets to (1) show that existing preference models benefit from, but fail to fully consider, added context, (2) finetune a context-aware reward model with context-specific performance exceeding that of GPT-4 and Llama 3 70B on tested datasets, and (3) investigate the value of context-aware preference modeling.

## 1 Introduction

As the general purpose capabilities of Language Models (LMs) [11; 42] and other foundation models [9] progress toward handling diverse instructions and executing long-range trajectories in real-world applications [44; 47; 41], it becomes increasingly important to have a principled system for ensuring that LM agents behave as expected. The prevailing approach for aligning an LM to human preferences uses pairwise preferences between different outputs to finetune the LM [52; 6], which falls short of addressing the critical challenges presented by the reality of diverse user intents and contexts [51; 50; 13]. In the presence of unspecified contexts, such as the user's identity or goals, preference queries are notoriously ambiguous [62] and one typically observes poor agreement (\(\sim 65\%\)) between human annotators on binary preference queries [35; 59].

In this paper, we consider modeling preferences using a two-step, context-aware approach (Figure 1). This approach first resolves the underspecification by selecting a context [38; 22], and then evaluates preference with respect to the chosen context [61; 14; 27; 30; 58]. Decomposing general preference into context and context-specific preference has several potential advantages. First and foremost, this approach explicitly identifies contextual assumptions that underlie preference, and shifts the alignment burden from preference modeling to a hybrid of preference modeling and context supervision. Second, this approach is naturally _pluralistic_[51], allowing the model to adapt to diverse users and use cases. Finally, it offers more flexibility for _principled aggregation_: whereas the Bradley-Terry approach corresponds to aggregating contexts using the Borda rule [50], which may under-serve certainsubgroups [13], the context-decomposition approach can additionally be used for jury learning [22] and cardinal utility aggregation [7].

We show that we can upper-bound the absolute reward modeling error as the sum of two terms: one term corresponding to context inference error, and the other to the context-specific reward modeling error. This decomposition suggests that supervising context might be a viable approach to aligning models with human preferences, provided we can do strong context-specific preference prediction. To evaluate context-specific prediction and build toward stronger context-aware preference models, we propose context-conditioned "preference reversal" datasets, where the preference for a given prompt is reversed given an alternative context.

Finally, we conduct experiments to benchmark the context-specific performance of various models and investigate the potential value of context-aware preference modeling. We find that, while current models generally benefit from additional context, they sometimes fail to give it full consideration, and finetuning on our preference reversal datasets greatly improves context-specific performance. We show that a single persistent context such as a user profile or system prompt can be used for a range of preference queries, and can be well inferred from as few as 16-32 samples of expressed preference.

In summary, our main contributions are:

1. We make an argument for modeling preference by decomposing it into context(s) and context-specific preference, and propose a context-decomposition upper bound to justify the approach.
2. We open-source high quality context-conditioned preference datasets that disentangle context-specific preference from general preference, which we use for finetuning and evaluation. The datasets can be found at https://huggingface.co/datasets/microsoft/rpr.
3. We show that while current models benefit from context, they may fail to give it full consideration, and that finetuning a context-aware reward model greatly improves context-specific performance, as well as general preference modeling performance when high quality context is available.

Figure 1: **Context-Aware Preference Modeling. _Left:_ The standard approach uses a preference model (PM) to directly evaluate arbitrary and potentially ambiguous preference queries. _Right:_ The context-aware preference modeling (CAPM) approach recognizes preference may depend on some unspecified context and makes this explicit: first identify the context, then evaluate a context-specific preference. In both cases, rather than computing preference directly, one may use a (context-aware) reward model (RM or CARM) to evaluate each alternative independently.**

Figure 2: **Effect of Context on Preference Modeling Performance.** Added context improves agreement with gold labels as compared to a no context (NC) baseline. Our 7B parameter, finetuned Context-Aware Reward Model (Mistral CARM) achieves the best context-aware performance, outperforming the larger Llama3-70B model (and GPT-4 Turbo), both on datasets where context is necessary to predict preference (RPR and Multifaceted Bench), and on the _context-augmented_ HHH, Reward Bench and Chatbot Arena datasets. Details and additional results may be found in Section 5.

## 2 Context-Aware Preference Modeling

### Resolving ambiguity by making implicit assumptions explicit

Current practice finetunes language models to make them consistent with a dataset of human preferences [44; 42]. As noted by earlier works on learning from human feedback, however, ambiguous preference judgments present a major challenge:

_Evaluation of a [preference query] is both subjective and multidimensional... this makes consistent labeling difficult for honest labelers (including the authors!)_[62]

This ambiguity manifests itself with agreement rates as low as \(\sim\!65\%\) between human annotators on binary preference queries [35; 59]. One way to understand this difficulty appeals to a distinction drawn by Amartya Sen:

_A value judgment can be called 'basic' if the judgment is supposed to apply under all conceivable circumstances, and it is 'non-basic' otherwise._[49]

Many, perhaps most, preference queries ask the annotator for a non-basic preference judgment, in that certain contextual information might effectively reverse the judgment. For instance, the preferred response to a technical question may depend on the user's education level, the preferred response to a medical question may depend on whether the user is a doctor, and the preferred response to a question about etiquette may depend on the user's geographical location. If we train models using non-basic preference annotations, the contextual biases and assumptions underlying those judgments may be implicitly embedded into the model [36; 48].

Rather than rely on annotators to integrate the correct distribution of contextual assumptions, and rely on the training process to consistently aggregate any disagreements in a singularly aligned model, one might instead consider an explicit context-aware approach to preference modeling (Figure 1). This approach first (partly) resolves ambiguity by specifying a context, and then models context-specific preference. This is not a new idea; Ziegler et al. [62], quoted above, continue to remark that, "_it seems better to design less ambiguous labeling tasks that get at the same information... such as a verbal description [of the most important contextual information]_", and many have advocated for fine-grained, context-conditioned, or otherwise more "pluralistic" approaches to alignment [58; 29; 51].

However, while production systems recognize the importance of incorporating context--the most notable being the "system prompt" introduced by OpenAI in 2023 and adopted by others [54; 3]--there has been little published research on the context-aware preference modeling capabilities of language models. This paper works toward filling this gap by introducing the reasonable preference reversal (RPR) datasets alongside context-augmented versions of existing datasets, and testing the context-specific preference modeling capabilities of existing models.

This discussion is summarized in Table 1 alongside other key benefits of context-aware preference modeling. This paper focuses on evaluating and improving context-aware preference modeling capabilities, leaving an in-depth exploration of the benefits to future work.

### Related Work

Modeling human preferences traces back to Luce [39] and Bradley-Terry [10]. It made its way into finetuning language models via a line of work originating in reinforcement learning [2; 15; 62; 52], and has now become the dominant approach for finetuning language models to follow human instructions [44; 6; 4; 46]. While this approach has achieved remarkable results and enabled the performance of state-of-the-art models, several authors have pointed to its limitations [12; 45; 32; 51],

\begin{table}
\begin{tabular}{p{113.8pt} p{284.5pt}} \hline \hline
**Explicit assumptions** & As described in Section 2.1, being explicit about context resolves a key source of ambiguity in preference queries, shifting some of the alignment burden from direct preference modeling to fine-grained context supervision. \\
**Steerability** & Incorporating context may allow models to better adapt to new users and use cases. \\
**Flexible Aggregation** & As described in Section 2.2, context-aware modeling enables aggregation based on jury learning [22] and expected utility [23], in addition to the Borda rule [50]. \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Key benefits of context-aware preference modeling.**which include the ambiguity problem that motivates our work. Notably, recent works have identified shortcomings that arise from implicitly aggregating diverse perspectives with a single choice rule [50; 13; 40; 18; 19]. In particular, Siththaranjan et al. [50] have recently shown the standard Bradley-Terry approach (Figure 1, left), implicitly uses the Borda rule to aggregate preferences across hidden contexts, and proposed to learn distributional reward models as a way to account for this. Rather than leave hidden context implicit in a distributional model, which does not resolve the indeterminate preference problem highlighted by our RPR datasets (see Section 4), we propose to make context explicit, either via specification or inference. Given a preference problem that has been decomposed into contexts and context-specific preferences, as described in Section 3, we can then choose to apply the Borda rule _or_ an alternative such as an expected utility aggregation [23] or jury learning [22] (see "distributionally pluralistic models" in Sorensen et al. [51]).

Modeling diversity and disagreement among human annotators and the inconsistency of human preferences have been treated from a number of different perspectives [21; 8; 20; 31]. Our work is similar to others that decompose a single objective into multiple dimensions such as helpfulness, harmlessness, and truthfulness [55; 19; 37; 5]. This approach has also become common in the literature on summarization, where multi-dimensional evaluation criteria are well recognized, with common dimensions including conciseness, coherence, consistency and fluency [60; 26; 16]. However, our work (Table 3) and others [24] find that current models often ignore or confuse added context. Preferences driven by diverse individual principles have been aggregated within frameworks such as Constitutional AI [6; 53], and alternative approaches to aligning models with human preferences, such as AI safety via debate [25] and consensus based approaches [7], can be understood as ways of supervising the context or assumptions underlying preference rather than the preference itself.

Finally, our work is closely related to context-conditioned generation, e.g., via a system prompt. In a concurrent work, Lee et al. [34] have synthesized a dataset of diverse system prompts for finetuning generative models. Their dataset includes multiple system prompts for the same user prompt, which allows it to be used in a similar fashion as our RPR datasets. We use their dataset for evaluation in Section 5, and provide an ablation on different training set compositions in Appendix C.1.

## 3 Context Decomposition Upper Bound

### Intent-Utility Formalism

We model the user-LM interaction using an intent-utility formalism \((\mathcal{I},\mathcal{X},\mathcal{Y},u)\), where \(\mathcal{I}\) is the space of intents, \(\mathcal{X}\) is the space of prompts, \(\mathcal{Y}\) is the space of completions, and \(u:\mathcal{I}\times\mathcal{Y}\rightarrow\mathbb{R}\) is a scalar utility function. We follow the standard assumption and assume that preference in this model is made according to the Bradley-Terry model [10; 15]. Letting \(\sigma\) be the logistic function, this defines the probability of preferring completion \(y_{1}\) to \(y_{2}\) given a specific intent \(i\) as:

\[p(y_{1}\succ y_{2}\,|\,i)=\sigma\left(u(i,y_{1})-u(i,y_{2})\right).\] (1)

In our model the primitive definitions of preference and utility are conditioned on the intent rather than the prompt. To prompt the model, a user with intent \(i\) selects a prompt \(x\in\mathcal{X}\). To annotate a preference query \((x,y_{1},y_{2})\), an annotator implicitly infers intent \(i\) from \(x\) and samples a preference from the Bernoulli distribution \(\mathcal{B}[p(y_{1}\succ y_{2}\,|\,i)]\).

Both users and annotators may possess or infer a _distribution_ of intents. Indeed, we would argue that annotation for most preference queries involves a distribution of intents rather than a specific intent. We use "intent" to refer to both specific and distributional intents. We assume there exists a base distribution over possible intents \(p(i)\), as well as a conditional distribution over prompts given intents \(p(x\,|\,i)\), so that any prompt \(x\) has a natural inference distribution \(p(i\,|\,x)\). In this model, the prompt \(x\) is a partial specification of intent \(i\). While a prompt may never be able to fully specify the intent, we may add some additional information or context \(z\) to obtain an extended prompt \((x,z)\in\mathcal{X}\). Let us suppose that \(z\in\mathcal{Z}\), where \(\mathcal{Z}\) corresponds to a discrete partition of \(\mathcal{I}\).

One way to measure utility and preference with respect to a distribution \(p\) of intents is with an expected utility model, which computes utility as \(u(p,y)=\mathbb{E}_{i\sim p}\left[u(i,y)\right]\). While it has been shown that standard RLHF does _not_ align with the expected utility model [50], this model satisfies certain desirable axioms, which one can argue would apply to "ideal" preference annotation. We use the expected utility model to define \(u(x,y):=u(p(\cdot\,|\,x),y)\), and note that for any context partition \(\mathcal{Z}\), this implies \(u(x,y)=\sum_{z\in\mathcal{Z}}p(z\,|\,x)u((x,z),y)\). For convenience, we define \(\Delta(\cdot,y_{1},y_{2}):=u(\cdot,y_{1})-u(\cdot,y_{2})\), which also decomposes linearly: \(\Delta(x,y_{1},y_{2})=\sum_{z}p(z\,|\,x)\Delta((x,z),y_{1},y_{2})\).

### Context Decomposition Upper Bound

During RLHF, we are presented with a dataset of prompt-preference tuples, \(\mathcal{D}=\{(x,y_{1}\succ y_{2})\}\), which we use to learn utility estimator \(\hat{u}:\mathcal{X}\rightarrow\mathcal{Y}\) (conventionally known as a "reward model"). As was assumed for \(u\) and \(p(z\,|\,x)\), we would like there to be a model \(\hat{p}(z\,|\,x)\) that satisfies the relation:

\[\hat{u}(x,y)=\sum_{z}\hat{p}(z\,|\,x)\hat{u}((x,z),y).\] (2)

In standard RLHF, we never learn such \(\hat{p}\). However, for purposes of this analysis, we will assume this \(\hat{p}\) exists, either implicitly given \(\hat{u}\), or explicitly, such that given some \(\mathcal{Z}\), we compute \(\hat{u}(x,y)\) via Equation (2) rather than via direct evaluation. Below, we will favor the latter interpretation.

For a given preference estimator \(\hat{p}(y_{1}\succ y_{2})\), \(\hat{u}\) is only unique up to constant shifts, so to measure the accuracy of \(\hat{u}\), we will instead compare \(\Delta\) and estimator \(\hat{\Delta}(x,y_{1},y_{2}):=\hat{u}(x,y_{1})-\hat{u}(x,y_{2})\).

Consider now the absolute error \(\big{|}\Delta(x,y_{1},y_{2})-\hat{\Delta}(x,y_{1},y_{2})\big{|}\) for a particular preference query, and use \(\Delta_{z}\) as shorthand for \(\Delta((x,z),y_{1},y_{2})\). For any \(\mathcal{Z}\) we have the following bound:

\[\begin{split}\big{|}\Delta(x,y_{1},y_{2})-\hat{\Delta}(x,y_{1}, y_{2})\big{|}&=\Big{|}\sum_{z}p(z\,|\,x)\Delta_{z}-\sum_{z}\hat{p}(z\,|\,x) \hat{\Delta}_{z}\Big{|}\\ &=\Big{|}\sum_{z}p(z\,|\,x)\,\Big{[}\Delta_{z}-\hat{\Delta}_{z} \Big{]}+\sum_{z}\hat{\Delta}_{z}\,\big{[}p(z\,|\,x)-\hat{p}(z\,|\,x)\big{]} \,\Big{|}\\ &\leq\underbrace{\sum_{z}p(z\,|\,x)\big{|}\Delta_{z}-\hat{\Delta }_{z}}_{\text{Context-weighted prediction error}}+\underbrace{\sum_{z}\big{|}\hat{\Delta}_{z} \big{|}\big{|}p(z\,|\,x)-\hat{p}(z\,|\,x)\big{|}}_{\text{Preference-weighted inference error}}\end{split}\] (3)

where the second equality adds and subtracts \(\sum_{z}p(z\,|\,x)\hat{\Delta}_{z}\) and rearranges, and the final line uses the triangle inequality (multiple times).

Equation (3) applies given a distribution of contexts, but in many cases, we might assume there is a specific context \(c\) (i.e., \(p(z=c):=1\)) and make a single context prediction \(\hat{c}\) (i.e., \(\hat{p}(\hat{c})=1\)). This simplifies Equation (3) and gives us the context decomposition upper bound for a _specific_ context:

\[\begin{split}\big{|}\Delta(x,y_{1},y_{2})-\hat{\Delta}(x,y_{1},y _{2})\big{|}&\leq&\underbrace{\big{|}\Delta_{c}- \hat{\Delta}_{c}\big{|}}_{\text{True context}}+\underbrace{\big{|}\hat{\Delta}_{c} -\hat{\Delta}_{c}\big{|}}_{\text{Subjective difference:}}\\ \text{true vs predicted context}\end{split}\] (4)

Both the general bound (Equation (3)) and specific bound (Equation (4)) formalize the following intuitive claim: if we can make accurate predictions given the true context (or context distribution), then we can reduce the preference modeling problem to a context inference problem.

### Discussion

The upper-bound in Equation (3) bounds the total error in terms of a context-weighted prediction error and a preference-weighted inference error. On one extreme, we have \(\mathcal{Z}=\emptyset\) (standard preference modeling), so that the context inference error is zero and the prediction error exclusively depends on the preference prediction problem given the prompt. On the other hand, we have \(\mathcal{Z}\equiv\mathcal{Y}\) (e.g., \(z\) might be "The preferred response is [\(y\)].") and our preference prediction error is zero, but the context inference problem becomes equivalent to generation. In between, we conjecture that there is a smooth trade-off between prediction error and inference error. This might be the case, for example, if a single context could apply to and disambiguate a range of different preference queries. We consider this in our experiments and find some support for the conjecture, in that conditioning on specific criteria outperforms conditioning on more abstract, yet still rather specific scenarios (Table 2) which outperforms conditioning on a user profile that applies to all preference queries at once (Table 5).

If our model \(\hat{u}\) is very good at predicting preference given some additional context \(\mathcal{Z}\), the preference modeling problem can be largely reduced to a context inference (or specification) problem. In this case, rather than have annotators rank completions for ambiguous prompts, it may make sense to spend the annotation resources to specify additional context. Such annotations could then be used to train a context inference model that disambiguates prompts. Intuitively, we hypothesize that the cardinality of the space of contexts is smaller than the cardinality of the space of possible completions given a prompt, which would make joint context-preference annotation a data-efficient alternative to just preference annotation. Although our focus is on improving context-aware preference modeling, our experiments with user profiles (Tables 5 and 6) provide some support for this hypothesis.

The effectiveness of the above proposal hinges on accurate context-specific prediction. Are language models sensitive to added context? Our work focuses on this question and contributes datasets to help us (1) evaluate it on real data, and (2) train better context-conditioned reward models.

## 4 Reasonable Preference Reversal (RPR) Datasets

We contribute a set of carefully designed, synthetic preference datasets to measure whether preference prediction is sensitive to context. Our datasets, inspired by the notion of non-basic judgments and preference reversal described in Section 2, include over 20,000 paired tuples of prompt, context, and preference judgments, i.e. \((x,z,y_{1}\!\succ\!y_{2})\). The samples are paired so that preference between two completions for the same prompt is entirely ambiguous without context: for every context, there is an alternative context for which preference reverses. As compared to the only prior context-conditioned preference dataset [30], where context-conditioned preference is highly correlated with unconditioned preference (see Table 2), our design choice ensures that preference prediction performance on this dataset is determined solely by the model's ability to pay attention to and interpret the context. The datasets can be found at https://huggingface.co/datasets/microsoft/rpr.

The dataset has been generated by GPT-4 Turbo [42] following a complex series of prompts designed to maximize validity (human agreement). We chose to do this to avoid invalid or ambiguous samples, at the cost of making it rather "easy" to evaluate preference given the context. The full process

Figure 3: **A sample from the RPR dataset. Under Criteria A or Scenario A, Completion A should be preferred, and vice versa under Criteria B or Scenario B.**

is detailed in Appendix B. The base dataset comes in two versions--criteria and scenarios--to emphasize the point that context may be expressed in many ways:

* _RPR Criteria_, where the context is a short sentence describing which features are important, e.g., "_Uses a formal and professional tone that emphasizes the technical aspects of C++_".
* _RPR Scenarios_, where the context describes the scenario that led to the underspecified or ambiguous prompt; e.g. "_In preparation for a product launch, a marketing manager at a tech startup seeks innovative strategies to position the company as an industry leader..._".

A paired sample from the dataset is shown in Figure 3. We further extend the dataset with user profiles as described in Section 5.2 and Appendix B.1. To ensure the validity and reliability of the synthetic data, we conduct small scale human validation through blind preference queries (Table 3) and find agreement rates of \(\geq\)95% on both splits. Details are provided in Appendix B.2. We divide the dataset into a training set of 10,167 paired samples, and a test set of 1,000 paired samples, with no overlap between train and test prompts.

## 5 Experiments

Our experiments aim to answer the following questions of context-aware preference modeling:

1. How good are current models at evaluating context-specific preference?
2. Can we improve context-aware preference modeling by finetuning on our RPR datasets?
3. Can a single context compress preferences with respect to a diverse range of prompts?

### Setup

ModelsWe report results for a selection of primarily open source models, detailed in Appendix D.1. These include two unconditioned reward models (the 13B parameter UltraRM [17] and a 7B parameter Mistral RM [56]), one context-aware preference model (Prometheus-2 [30]), our finetuned context-aware reward model (Mistral CARM), and a set of generative models (four Llama 3 models [1] and GPT-4 Turbo [42]) used with an "llm-as-a-judge" approach. In preliminary experiments we tested a several other models and observed similar patterns across all models. All models except Prometheus-2 are used as reward models, by first evaluating each alternative individually and then comparing scores. Appendix D.2 presents additional results for reward models finetuned from Section 2B.

DatasetsBesides the RPR datasets detailed in Section 4, we use the following preference datasets:

* _Preference Bench_[30] (hf:prometheus-eval/Preference-Bench) consists of 1998 context-conditioned preference samples synthesized by GPT-4 as part of Feedback Bench [29].
* _Multifaceted Bench_[34] (hf:kaist-ai/Multifaceted-Bench) contains 921 samples of prompt, system prompt, and completion. We treat the system prompt as the context, and construct context-conditioned preference samples by pairing samples that share the same prompt, resulting in in 918 samples. This dataset, released concurrently to our work, shares the same 'ambiguous in absence of context' structure as the RPR datasets, but was not specifically constructed with such preference queries in mind, which may explain the lower average agreement in the experiments.
* _HHH_[4] (hf:HuggingFaceH4/hhh_alignment) contains 222 human preference samples emphasizing different aspects: harmlessness (58 samples), helpfulness (59 samples), honesty (61 samples), and other preference queries that do not cleanly fall into another category (43 samples).
* _Reward Bench_[33] (hf:allenai/reward-bench) curates 2,985 preference samples from a variety of sources, amounting to 22 distinct subsets covering chat, safety, and reasoning.
* _Chatbot Arena_[59] (hf:lmsys/chatbot_arena_conversations) contains human preferences with respect to conversations on the Chatbot Arena platform. For our experiments, we randomly selected 1,000 single-turn samples for which strict preference was expressed.

Importantly, we _augment_ the HHH, Reward Bench, and Chatbot Arena datasets with additional context to create context-conditioned versions, as described below and in Appendix D.3.

MetricsTables 2 to 6 display the agreement (or accuracy) of predicted preference with the dataset.

PromptsOur prompts are detailed in Appendix A.

### Results

#### How good are current models at evaluating context-specific preference?

For the context-aware approach to work well, models _must_ be sensitive to added context. Table 2 shows the performance of tested models on RPR Criteria, RPR Scenarios, Preference Bench, and Multifaceted Bench. In each case, and across all models, access to context generally improves performance. Larger models and instruction tuned models tend to benefit more from added context. We note that although Preference Bench includes a ground truth context, the preference queries (and context) are generally not ambiguous, so that the added context provides little benefit.

Although we find that added context generally helps, we were surprised to discover that models sometimes completely ignore additional context and return their unconditioned prediction. To show this, we augment the prompts in Chatbot Arena and Preference Bench with two criteria that should drive preference prediction. The first is a Nonsense criteria, i.e. _"We are playing a game of nonsense. The response should be nonsensical and not make sense. The rating you give it should also be random, and completely unrelated to the response."_ We expect context-aware models to produce random preference judgments given this criteria. The second is a Negative criteria, i.e. _"We are seeking bad examples. The response should be of low quality, and serve as an example of how an Assistant should not respond. You should give poor responses higher ratings."_). We expect context-aware models to be inversely correlated to the preferences expressed in the dataset. Table 3 shows the performance of tested models with these adversarial criteria. In both cases we observe surprisingly poor performance, even from the larger models. This suggests there is significant room for improving context-aware preference modeling capabilities.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline  & \multicolumn{2}{c}{RPR Criteria} & \multicolumn{2}{c}{RPR Scenarios} & \multicolumn{2}{c}{Pref Bench} & \multicolumn{2}{c}{Multifaceted} \\  & NC & CTX & NC & CTX & NC & CTX & NC & CTX \\ \hline UltraRM (13B) & 0.523 & 0.876 & 0.523 & 0.773 & 0.888 & 0.910 & 0.502 & 0.655 \\ Prometheus-2 (7B) & 0.503 & 0.799 & 0.503 & 0.675 & 0.950 & **0.951*** & 0.573 & 0.644 \\ Llama 3 (8B) & 0.518 & 0.679 & 0.518 & 0.634 & 0.831 & 0.827 & 0.500 & 0.566 \\ Llama 3 Instruct (8B) & 0.494 & 0.850 & 0.494 & 0.745 & 0.883 & 0.898 & 0.496 & 0.616 \\ Mistral RM (7B) & 0.510 & 0.867 & 0.510 & 0.749 & 0.894 & 0.915 & 0.516 & 0.679 \\ Mistral CARM (7B) (**Ours**) & 0.511 & **0.985*** & 0.511 & **0.962*** & 0.913 & 0.919 & 0.517 & **0.787** \\ \hline Llama 3 (70B) & 0.496 & 0.726 & 0.496 & 0.669 & 0.800 & 0.811 & 0.496 & 0.564 \\ Llama 3 Instruct (70B) & 0.516 & 0.925 & 0.516 & 0.811 & 0.916 & 0.929 & 0.506 & 0.687 \\ GPT-4 Turbo & 0.502 & 0.901 & 0.500 & 0.748 & 0.854 & 0.860 & 0.501 & 0.640 \\ \hline Human validation (100 samples) & - & 0.970 & - & 0.950 & - & - & - \\ \hline \hline \end{tabular} Best model folded for CTX columns only. Started* results are by models finetuned on the same distribution. Our model in green.

\end{table}
Table 2: **Context-specific datasets**. On datasets with ground truth context, adding context generally helps with evaluating preference. Our model, finetuned on the RPR training sets achieves substantially better (test set) performance than the unfinetuned model.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & \multicolumn{2}{c}{Nonsense Criteria (_0.50 is optimal_)} & \multicolumn{2}{c}{Negative Criteria (_lower is better_)} \\  & Chatbot Arena & Pref. Bench & Chatbot Arena & Pref. Bench \\ \hline UltraRM (13B) & 0.718 & 0.597 & 0.719 & 0.467 \\ Prometheus-2 (7B) & 0.685 & 0.876 & 0.618 & 0.711 \\ Llama 3 (8B) & 0.624 & 0.800 & 0.575 & 0.776 \\ Llama 3 Instruct (8B) & 0.543 & 0.738 & 0.698 & 0.865 \\ Mistral RM (7B) & 0.667 & 0.808 & 0.648 & 0.505 \\ Mistral CARM (7B)(**Ours**) & **0.535** & **0.542** & **0.518** & **0.293** \\ \hline Llama 3 (70B) & 0.289 & 0.678 & 0.589 & 0.801 \\ Llama 3 Instruct (70B) & 0.452 & 0.289 & **0.352** & 0.308 \\ GPT-4 Turbo & **0.498** & **0.493** & 0.393 & 0.309 \\ \hline \hline \end{tabular} Best small model bolded. Best large model also bolded if better than the small model. Our model in green.

\end{table}
Table 3: **Sensitivity of current models to adversarial contexts**. We observe that current models often ignore added context, even when it is strongly phrased and adversarial. Our finetuned CARM does best among smaller models, but still has significant room for improvement.

[MISSING_PAGE_FAIL:9]

preference information across all the prompts they are presented with. In order to create a scenario closer to this setting, we first generate 5 synthetic diverse user profiles and condition GPT-4 Turbo on each profile to label the RPR test set (see Appendix B.1). Having relabeled the RPR test set with these preferences, we explore the ability of tested models to recover the expressed preferences from the profile, as shown in Table 5. Our CARM and the larger Llama 3 Instruct model perform best on this task, recovering approximately 80% agreement with GPT-4's labels.

Additionally, in Table 6, we test the performance of our context-aware model when profiles are inferred with limited preference samples. Profile inference is done by prompting GPT-4 (see Appendix B.1). Without any profile inference (No Context / NC), the default assumptions made by our model run against the preferences of Profiles 3 and 4; however, this is resolved in as few as 2-4 samples. While this depends on the underlying data distribution, 16-32 samples recover most of the benefit of the ground truth context on this dataset.

## 6 Conclusion

This paper began with a case for a two-step context-aware preference modeling framework (Figure 1), which was motivated by the ambiguity problem commonly experienced during preference annotation (Section 2). We further motivated the framework via a context decomposition upper bound (Section 3) and noted that for context-aware preference modeling to be viable, we require strong context-specific preference modeling. However, despite the prevalence of context conditioning in deployed systems (e.g., system prompts and "My GPTs" [43]), when we began our work, there were no open source preference datasets where preference is strongly determined by context, and limited studies of the context-specific preference modeling capabilities of current models [30]. To this end, we introduced the RPR datasets (Table 2) and investigated a series of interesting empirical questions in Section 5). We found that (1) while current preference models generally benefit from context, they sometimes ignore added context, (2) finetuning on our preference reversal datasets greatly improves context-specific performance, and (3) a single persistent context, which might be inferred from data, may be sufficient to significantly improve preference prediction performance.

Our work leaves many open threads for future research. Our datasets might be used to explore different directions in pluralistic alignment, as described by Sorensen et al. [51]. In particular, we believe better context specification and inference are important directions for research, which our experiments only briefly touch on. Further, while we have open sourced a set of context-conditioned preference datasets, these datasets have limited diversity (for example, they are primarily English, and are limited to single turn conversations), and were synthesized rather than collected from real human feedback, which limits their practical relevance, as the preferences may not align with all potential user perspectives. Additional open, preferably human sourced datasets with context annotations are needed to more fruitfully pursue this research direction.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & No & \multicolumn{3}{c}{Profile inferred from \(n\) samples} & \\  & Context & 2 & 8 & 32 & GT \\ \hline P1 & 0.649 & 0.684 & 0.667 & 0.788 & 0.808 \\ P2 & 0.780 & 0.864 & 0.863 & 0.892 & 0.918 \\ P3 & 0.313 & 0.555 & 0.669 & 0.764 & 0.815 \\ P4 & 0.370 & 0.512 & 0.678 & 0.759 & 0.801 \\ P5 & 0.530 & 0.571 & 0.601 & 0.605 & 0.672 \\ \hline Mean & 0.528 & 0.637 & 0.696 & 0.762 & 0.803 \\ Error\({}^{\dagger}\) & \(\pm\)0.048 & \(\pm\)0.039 & \(\pm\)0.010 & & \\ \hline \hline \end{tabular} \(\dagger\) Estimated by averaging 3-seed std. dev. for each profile and dividing by \(\sqrt{5}\).

\end{table}
Table 6: **User profile inference on the RPR dataset**. Each of 5 profiles (P1-P5) was used to annotate the RPR test set, as well as three small subsets (32 samples each) of the RPR training set (one subset for each of 3 seeds). Between 2 and 32 samples from each subset were then used to infer a user profile with the help of GPT-4 Turbo. Conditioning our finetuned CARM on these inferred profiles gives the results below (both table and figure display the same data). We see that just 2 samples carry substantial signal, and 32 samples capture most of the benefit of the ground truth context.

## References

* [1] AI@Meta. Llama 3 model card. 2024.
* [2] R. Akrour, M. Schoenauer, and M. Sebag. April: Active preference learning-based reinforcement learning. In _Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2012, Bristol, UK, September 24-28, 2012. Proceedings, Part II 23_, pages 116-131. Springer, 2012.
* [3] Anthropic. Model card and evaluations for Claude models, 2023.
* [4] A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. DasSarma, et al. A general language assistant as a laboratory for alignment. _arXiv preprint arXiv:2112.00861_, 2021.
* [5] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. _arXiv preprint arXiv:2204.05862_, 2022.
* [6] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, et al. Constitutional AI: Harmlessness from ai feedback. _arXiv preprint arXiv:2212.08073_, 2022.
* [7] M. Bakker, M. Chadwick, H. Sheahan, M. Tessler, L. Campbell-Gillingham, J. Balaguer, N. McAleese, A. Glaese, J. Aslanides, M. Botvinick, et al. Fine-tuning language models to find agreement among humans with diverse preferences. _Advances in Neural Information Processing Systems_, 35:38176-38189, 2022.
* [8] C. Baumler, A. Sotnikova, and H. Daume III. Which examples should be multiply annotated? active learning when annotators may disagree. In _Findings of the Association for Computational Linguistics: ACL 2023_, pages 10352-10371, 2023.
* [9] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models. _arXiv preprint arXiv:2108.07258_, 2021.
* [10] R. A. Bradley and M. E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. _Biometrika_, 39(3/4):324-345, 1952.
* [11] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* [12] S. Casper, X. Davies, C. Shi, T. K. Gilbert, J. Scheurer, J. Rando, R. Freedman, T. Korbak, D. Lindner, P. Freire, et al. Open problems and fundamental limitations of reinforcement learning from human feedback. _arXiv preprint arXiv:2307.15217_, 2023.
* [13] S. Chakraborty, J. Qiu, H. Yuan, A. Koppel, F. Huang, D. Manocha, A. S. Bedi, and M. Wang. Maxmin-RLHF: Towards equitable alignment of large language models with diverse human preferences. _arXiv preprint arXiv:2402.08925_, 2024.
* [14] P. Cheng, J. Xie, K. Bai, Y. Dai, and N. Du. Everyone deserves a reward: Learning customized human preferences. _arXiv preprint arXiv:2309.03126_, 2023.
* [15] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from human preferences. _Advances in neural information processing systems_, 30, 2017.
* [16] E. Clark, S. Rijhwani, S. Gehrmann, J. Maynez, R. Aharoni, V. Nikolaev, T. Sellam, A. Siddhant, D. Das, and A. P. Parikh. Seahorse: A multilingual, multifaceted dataset for summarization evaluation. _arXiv preprint arXiv:2305.13194_, 2023.
* [17] G. Cui, L. Yuan, N. Ding, G. Yao, W. Zhu, Y. Ni, G. Xie, Z. Liu, and M. Sun. Ultrafeedback: Boosting language models with high-quality feedback, 2023.
* [18] J. Dai and E. Fleisig. Mapping social choice theory to rlhf. _arXiv preprint arXiv:2404.13038_, 2024.
* [19] J. Dai, X. Pan, R. Sun, J. Ji, X. Xu, M. Liu, Y. Wang, and Y. Yang. Safe RLHF: Safe reinforcement learning from human feedback. _arXiv preprint arXiv:2310.12773_, 2023.
* [20] V. Dumoulin, D. D. Johnson, P. S. Castro, H. Larochelle, and Y. Dauphin. A density estimation perspective on learning from pairwise human preferences. _arXiv preprint arXiv:2311.14115_, 2023.

* [21] E. Fleisig, R. Abebe, and D. Klein. When the majority is wrong: Modeling annotator disagreement for subjective tasks. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 6715-6726, 2023.
* [22] M. L. Gordon, M. S. Lam, J. S. Park, K. Patel, J. Hancock, T. Hashimoto, and M. S. Bernstein. Jury learning: Integrating dissenting voices into machine learning models. In _Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems_, pages 1-19, 2022.
* [23] J. C. Harsanyi. Cardinal welfare, individualistic ethics, and interpersonal comparisons of utility. _Journal of political economy_, 63(4):309-321, 1955.
* [24] X. Hu, M. Gao, S. Hu, Y. Zhang, Y. Chen, T. Xu, and X. Wan. Are LLM-based evaluators confusing NLG quality criteria? _arXiv preprint arXiv:2402.12055_, 2024.
* [25] G. Irving, P. Christiano, and D. Amodei. AI safety via debate. _arXiv preprint arXiv:1805.00899_, 2018.
* [26] S. Jain, V. Keshava, S. M. Sathyendra, P. Fernandes, P. Liu, G. Neubig, and C. Zhou. Multi-dimensional evaluation of text summarization with in-context learning. _arXiv preprint arXiv:2306.01200_, 2023.
* [27] J. Jang, S. Kim, B. Y. Lin, Y. Wang, J. Hessel, L. Zettlemoyer, H. Hajishirzi, Y. Choi, and P. Ammanabrolu. Personalized soups: Personalized large language model alignment via post-hoc parameter merging. _arXiv preprint arXiv:2310.11564_, 2023.
* [28] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. _arXiv preprint arXiv:2310.06825_, 2023.
* [29] S. Kim, J. Shin, Y. Cho, J. Jang, S. Longpre, H. Lee, S. Yun, S. Shin, S. Kim, J. Thorne, et al. Prometheus: Inducing fine-grained evaluation capability in language models. _arXiv preprint arXiv:2310.08491_, 2023.
* [30] S. Kim, J. Suk, S. Longpre, B. Y. Lin, J. Shin, S. Welleck, G. Neubig, M. Lee, K. Lee, and M. Seo. Prometheus 2: An open source language model specialized in evaluating other language models, 2024.
* [31] C. Laidlaw and A. Dragan. The boltzmann policy distribution: Accounting for systematic suboptimality in human models. _arXiv preprint arXiv:2204.10759_, 2022.
* [32] N. Lambert, T. K. Gilbert, and T. Zick. Entangled preferences: The history and risks of reinforcement learning and human feedback. _arXiv preprint arXiv:2310.13595_, 2023.
* [33] N. Lambert, V. Pyatkin, J. Morrison, L. Miranda, B. Y. Lin, K. Chandu, N. Dziri, S. Kumar, T. Zick, Y. Choi, et al. Rewardbench: Evaluating reward models for language modeling. _arXiv preprint arXiv:2403.13787_, 2024.
* [34] S. Lee, S. H. Park, S. Kim, and M. Seo. Aligning to thousands of preferences via system message generalization. _arXiv preprint arXiv:2405.17977_, 2024.
* [35] X. Li, T. Zhang, Y. Dubois, R. Taori, I. Gulrajani, C. Guestrin, P. Liang, and T. B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 2023.
* [36] P. P. Liang, C. Wu, L.-P. Morency, and R. Salakhutdinov. Towards understanding and mitigating social biases in language models. In _International Conference on Machine Learning_, pages 6565-6576. PMLR, 2021.
* [37] S. Lin, J. Hilton, and O. Evans. TruthfulQA: Measuring how models mimic human falsehoods. _arXiv preprint arXiv:2109.07958_, 2021.
* [38] E. Liscio, M. van der Meer, L. C. Siebert, C. M. Jonker, N. Mouter, and P. K. Murukannaiah. Axies: Identifying and evaluating context-specific values. In _AAMAS_, pages 799-808, 2021.
* [39] R. D. Luce. Individual choice behavior. 1959.
* [40] A. Mishra. AI alignment and social choice: Fundamental limitations and policy implications. _arXiv preprint arXiv:2310.16048_, 2023.
* [41] OpenAI. Chatgpt plugins, March 2023.
* [42] OpenAI. GPT-4 technical report, 2023.
* [43] OpenAI. Creating a GPT, 2024.

* [44] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.
* [45] S. Pitis. Failure modes of learning reward models for LLMs and other sequence models. In _ICML 2023 Workshop The Many Facets of Preference-Based Learning_, 2023.
* [46] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preference optimization: Your language model is secretly a reward model. _Advances in Neural Information Processing Systems_, 36, 2024.
* [47] T. B. Richards. Auto-GPT: Autonomous artificial intelligence software agent, 2023. Initial release: March 30, 2023.
* [48] P. Schramowski, C. Turan, N. Andersen, C. A. Rothkopf, and K. Kersting. Large pre-trained language models contain human-like biases of what is right and wrong to do. _Nature Machine Intelligence_, 4(3):258-268, 2022.
* [49] A. Sen. _Collective Choice and Social Welfare_. Holden Day, San Francisco, 1970. Edinburgh: Oliver and Boyd, 1971; Amsterdam: North-Holland, 1979. Swedish translation: Bokforlaget Thales, 1988.
* [50] A. Sithithdaranjan, C. Laidlaw, and D. Hadfield-Menell. Distributional preference learning: Understanding and accounting for hidden context in rhlf. _arXiv preprint arXiv:2312.08358_, 2023.
* [51] T. Sorensen, J. Moore, J. Fisher, M. Gordon, N. Mireshghallah, C. M. Rytting, A. Ye, L. Jiang, X. Lu, N. Dziri, et al. A roadmap to pluralistic alignment. _arXiv preprint arXiv:2402.05070_, 2024.
* [52] N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano. Learning to summarize with human feedback. _Advances in Neural Information Processing Systems_, 33:3008-3021, 2020.
* [53] Z. Sun, Y. Shen, H. Zhang, Q. Zhou, Z. Chen, D. Cox, Y. Yang, and C. Gan. Salmon: Self-alignment with instructable reward models, 2024.
* [54] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* [55] Z. Wang, Y. Dong, O. Delalleau, J. Zeng, G. Shen, D. Egert, J. J. Zhang, M. N. Sreedhar, and O. Kuchaiev. Helpsteer2: Open-source dataset for training top-performing reward models. _arXiv preprint arXiv:2406.08673_, 2024.
* [56] W. Xiong, H. Dong, C. Ye, Z. Wang, H. Zhong, H. Ji, N. Jiang, and T. Zhang. Iterative preference learning from human feedback: Bridging theory and practice for RLHF under KL-constraint, 2024.
* [57] R. Yang, R. Ding, Y. Lin, H. Zhang, and T. Zhang. Regularizing hidden states enables learning generalizable reward model for llms. _arXiv preprint arXiv:2406.10216_, 2024.
* [58] S. Ye, D. Kim, S. Kim, H. Hwang, S. Kim, Y. Jo, J. Thorne, J. Kim, and M. Seo. Flask: Fine-grained language model evaluation based on alignment skill sets. _arXiv preprint arXiv:2307.10928_, 2023.
* [59] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. Xing, et al. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. _arXiv preprint arXiv:2306.05685_, 2023.
* [60] M. Zhong, Y. Liu, D. Yin, Y. Mao, Y. Jiao, P. Liu, C. Zhu, H. Ji, and J. Han. Towards a unified multi-dimensional evaluator for text generation. _arXiv preprint arXiv:2210.07197_, 2022.
* [61] W. Zhou, S. Zhang, H. Poon, and M. Chen. Context-faithful prompting for large language models. _arXiv preprint arXiv:2303.11315_, 2023.
* [62] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving. Fine-tuning language models from human preferences. _arXiv preprint arXiv:1909.08593_, 2019.

Prompts for context-conditioned preference prediction

The following template is used for "llm-as-a-judge" models (Llama 3 and GPT-4 Turbo). Llama-3 uses the logit_template (see Appendix D.1 for how the score is computed). GPT-4 Turbo uses the argmax_score_template_no_cot and runs inference with temperature = 0.

``` CRITERIA_TEMPLATE = { "type": "logit_rating", "name": "criteria_template", "system_prompt": "You are a helpful assistant that scores other AI assistants based on a given criteria and the quality of their answers.", "logit_template": "Rate the quality of the AI assistant's response(s) in the conversation displayed below according to the following criteria:\n\n\n((context))\n\nFour score should reflect the quality of the AI assistant's response(s) with respect to the specific criteria above, ignoring other aspects of the answer (such as overall quality), and should agree with the score provided by a reasonable human evaluator. Please rate the assistant's response(s) on a scale of 1 to ((  max_score)), where 1 corresponds to extremely poor (criteria is NOT satisfied) and ((max_score)) corresponds to excellent (criteria is satisfied). Format your answer as: 'I give the assistant a score of \(\L/(max_score)), because...', where X is your score.\n\n[(CONVERSATION)]\n\n((\\ conversation))*, "logit_completion_template": "I give the assistant a score of ", "argmax_score_template": "Rate the quality of the AI assistant's response(s) in the conversation displayed below according to the following criteria:\n\n[((context))]\n\nFour score should reflect the quality of the AI assistant's response(s) with respect to the specific criteria above, ignoring other aspects of the answer (such as overall quality), and should agree with the score provided by a reasonable human evaluator. Begin your evaluation by providing a short explanation. After providing your explanation, please rate the response with respect to the criteria on a scale of 1 to 10 by strictly following this format: "[[rating]]", for example: "[[5]]".\n\n[(CONVERSATION)]\n\n((\\ conversation))*, "argmax_score_template_no_cot": "Rate the quality of the AI assistant's response(s) in the conversation displayed below according to the following criteria:\n\n[(context)]\n\nFour score should reflect the quality of the AI assistant's response(s) with respect to the specific criteria above, ignoring other aspects of the answer (such as overall quality), and should agree with the score provided by a reasonable human evaluator. IMPRANT: Begin your response by directly rating the AI assistant (with respect to the criteria) on a scale of 1 to 10, strictly following this format: "[[rating]]". For example, if you give the AI assistant a score of 5, your response should start with: "[[5]]".\n\n[([CONVERSATION])\n\n((\\ conversation))* ) } ```

Mistral RM and our finetuned version use the following template:

``` defprompt_with_context(prompt,completion,context=None): ifcontextisnotNone: prompt=f"[INST]PleasecontinuethefollowingconversationbyprovidingtheAssistant'sresponse, in ordertomaximizethescorethatasreasonablehumanevalatorwouldassigntotheAssistant'sresponsegiventhefollowingcriteriaorcontext: ((context))\n\n[(Conversation]]\n[(\\ prompt)] [/INST]"+f'((completion))</s> else: prompt=f"[INST]PleasecontinuethefollowingconversationbyprovidingtheAssistant'sresponse:\n\n[[Conversation]]\n\n[(\\ prompt)][/INST]"+f'((completion))</s>' returnprompt ```

The prompt templates for Ultra RM and Prometheus follow the format set out by the authors of those models. Since no reference answer was available for Prometheus, we use the following slight modification which omits the reference answer:

``` PROMETHEUS_TEMPLATE = """fastTaskDescription: Aninstruction(mightincludeanInputinsideit),aresponsetoevaluate,andascorerubric representingaevaluationcriteriaaregiven.
1. Writeadetailedfeedbackthatassessthequalityoftworesponsestristyledonthegivenscorerubric,notevaluatingingeneral.
2. Afterwritingafeedback,choseabetterresponsebetweenResponseAandResponseB.Youshouldrefertothescorerubric.

3. The output format should look as follows: "Feedback: (write a feedback for criteria) [RESULT] (A or B)"
4. Please do not generate any other opening, closing, and explanations.

###Instruction: {orig_instruction}

###Response A: {orig_response_A}

###Response B: {orig_response_B}

###Reference Answer: [omitted]

###Score Rubric: {orig_criteria}

###Feedback:""

## Appendix B Constructing the RPR Datasets

We construct the datasets using the following steps:

1. Collect a set of diverse prompts.
2. Synthesize the initial RPR Criteria dataset.
3. Self critique and filter the RPR Criteria dataset.
4. Synthesize the RPR Scenarios dataset, using the RPR Criteria dataset as a base.
5. Self critique and filter the RPR Scenarios dataset.
6. Filter RPR Criteria down to what is left after the RPR Scenarios synthesis, so that each prompt has 2 criteria, 2 corresponding scenarios, and 2 corresponding completions.

In Step 1, we used all prompts from the Ultrafeedback dataset [17], which have already been selected for diversity, and filtered it down to prompts where there was no explicitly correct answer (according to the Ultrafeedback annotations). This left us with 42,000 initial prompts.

We passed these prompts through step 2 of our synthesis procedure:

RPR Criteria Initial Synthesis

I would like some help constructing a sample from the RPR dataset.

The RPR dataset is a contrastive preference feedback dataset containing pairs of language model completions with respect to which an evaluator would exhibit preference reversal when applying distinct, yet reasonable fine-grained criteria. The dataset consists of S-tuples of (Prompt, Response A, Response B, Criteria X, Criteria Y), where the responses are generated by an AI language model that is serving as a helpful Assistant.

Each 5-tuple in the dataset satisfies the following requirements:

[[REQUMEMENTS]]

1. Given the Prompt, Response A would be clearly preferred to Response B when evaluated according to Criteria X.
2. Given the Prompt, Response B would be clearly preferred to Response A when evaluated according to Criteria Y.
3. Criteria X and Y are both reasonable criteria that might be adopted by reasonable humans. It is expected that a nontrivial portion of humans would adopt Criteria X and prefer Response A. Similarly, it is expected that a nontrivial portion of humans would adopt Criteria Y and prefer Response B.
4. Both Response A and Response B would be judged as "high quality" responses by reasonable humans. They would be reasonably generated either by an AI language model acting as a helpful assistant or by a human assistant.
5. The Prompt itself might occur in the ordinary course of using an AI language model to generate responses. The Prompt is complete, and it is answerable without reference to an external source or URL.

[[END REQUREMENTS]] To construct a sample from this dataset, you will be provided with a prompt ([[PROMPT]]) and a list of potential criteria categories ([[CRITERIA CATEGORIES]]). You will use the following procedure to generate the 5-tuple.

[[PROCURE]]

1. CRITERIA GENERATION: Use the [[CRITERIA CATEGORIES]] to brainstorm a pair of *specific* criteria such that the [[REQUIREMENTS]] will be feasibly met (i.e., the criteria are reasonable, and would result in preference reversal for some reasonable pair of compeletions).

Be sure to generate criteria that fall within one of the provided [[CRITERIA CATEGORIES]]. The criteria should be *significantly more specific than the category itself (i.e., it is one of many possible criteria in that category). The criteria should NOT reference overly superficial aspects such as the "level of detail" or "simplicity" of the response.

For example, if the category is "detail and elaboration", then the criteria might be "Elaborates by using a specific example" or "Provides a specific example that is not a cliche" or "Provides comments to explain the code". But it should not be "Provides a detailed response" or "Provides a detailed step-by-step explanation".

IMPORTANT: You should choose orthogonal pairs of categories and criteria, rather than picking direct opposites.

For example, if the Category for Criteria X is "Clarity and Conciseness" then the Category for Criteria Y should not be a category that is directly opposite such as "Detail and Elaboration". Instead, use an orthogonal category for Criteria Y, such as "Completeness and Accuracy".

Based on your reasoning, the final output of this step will be:

- Criteria X

- Criteria Y

- Category for X

- Category for Y

2. RESPONSE GENERATION: Once the pair of specific criteria is generated, you will generate the two responses A and B so as to meet the [[REQUIREMENTS]]. In order for the responses to be as diverse as possible, you will first generate a set of auxiliary criteria from the [[CRITERIA CATEGORIES]] that were NOT picked, one for generating response A and a different one for generating response B. Use these auxiliary criteria to guide your generations so that they appear to come from different AI language models, but make sure the [[REQUIREMENTS]] take precedence. That is: Response A should be preferred under Criteria X, Response Y should be preferred under Criteria Y, and both responses are " high quality" and would be reasonably produced by an AI language model or human assistant.

Based on your reasoning, the final output of this step will be:

- Response A

- Response B

[[END PROCURE]]

Format your response as follows:

[[OUTPUT FORMAT]]

[Reasoning & final output for Criteria Generation]

[Reasoning & final output for Response Generation]

JSON Output:

==

({ "prompt": [PRMTP], "response.a": [Response A], "response.b": [Response B], "criteria.x": [Criteria X], "criteria.y": [Criteria Y], "category.y": [Criteria Category X], "category.y": [Criteria Category Y] })[[END OUTPUT FORMATT]] Here is the prompt and criteria categories for your sample:

[[PROMPT]]

{prompt}

[[END PROMPT]]

[[CRITERIA CATEGORIES]] {categories}

[END CRITERIA CATEGORIES]]

For the categories, 7 categories were randomly sampled from the following list, and the one that had been chosen most so far during the synthesis process was dropped (so 6 categories were included in the prompt). This was done to increase the diverse of preferences. The list below was synthesized by GPT-4 to be mutually exclusive and collectively exhaustive of different dimensions according to which people might have differing preferences.

Criteria categories for RPR synthesis

criteria_categories = mp.array([

"Clarity and Conciseness","Detail and Elaboration","Formality and Tone","Contextual Relevance","

Factual Accuracy","Creativity and Originality","Technical Complexity","User-Friendliness","

Problem-Solving Approach","Practical Application","Logical Consistency","Ethical Considerations

","Contextual Sensitivity","Predictive Accuracy","Empathy and Emotional Intelligence","Historical Accuracy","Innovativeness","Interdisciplinary Approach","Linguistic Creativity","Scientific Rigor

","Societal Impact","Sustainability","User Experience","Visual and Aesthetic Appeal","Economic

Feassibility","Legal and Regulatory Compliance","Pedegogical Effectiveness","Technological

Advancesent","Crisis Management","Global Perspective","Humor and Entertainment Value","

Inclusivity and Diversity","Strategic Insight","Harrative and Storyfilling Quality","

Personalization and Customization","Data Utilization and Analysis","Philosophical Depth","Health

and Wellness Orientation","Security and Privacy Considerations","Multilingual and Cross-Cultural

Competence"

})

Having synthesized the initial RPR criteria data, we filtered out poor entries (Step 3) using the following prompt:

Prompt to filter RPR Criteria

Your task is to check whether the proposed sample meets the requirements for the RPR dataset. Our

proposal system is not very good and usually generates invalid proposals. However, in the rare cases

where it is successful, we would like to accept the samples. So you should be critical and thorough

in your evaluation. Be as objective as possible, but err on the side of rejecting proposals.

The RPR dataset contains pairs of language model completions for which an evaluator would exhibit

preference reversal when applying distinct, yet reasonable fine-grained criteria. The dataset

consists of 5-tuples of (Prompt, Response A, Response B, Criteria X, Criteria Y), where the responses

are generated by an AI language model that is serving as a helpful Assistant.

Each 5-tuple accepted into the dataset must satisfy the following requirements:

[[REQUIREMENTS]]

1. The Prompt itself might occur in the ordinary course of using an AI language model to generate

responses. The Prompt is complete and self-contained: it is not a fragment of a larger sentence, and

it is answerable without reference to an external source or URL.
2. Given the Prompt, Response A would be clearly preferred to Response B when evaluated according to

Criteria X.
3. Given the Prompt, Response B would be clearly preferred to Response A when evaluated according to

Criteria Y.
4. Given the Prompt, Criteria X and Y are both reasonable criteria that might be adopted by reasonable

humans to evaluate the response. It is expected that a nontrivial portion of humans would adopt

Criteria X, and that a different nontrivial portion of humans would adopt Criteria Y.

5. Given the Prompt, both Response A and Response B would be judged as "high quality" responses by reasonable human evaluators. Each response would be reasonably generated either by an AI language model acting as a helpful assistant or by a human assistant.

[[END REQUIREENTS]]

Below, you are given a [[PROPOSAL]] generated by our system. You will reason about each of the 5 [[ RBQIRENTS]], one at a time, and determine whether the sample meets each requirement. For each requirement, you will begin by reasoning about whether the requirement (including each subpart) is clearly met, and then you will make a determination (met or not met) for that requirement. Do NOT begin your reasoning with your determination and do NOT simply repeat the requirement; instead, begin with your reasoning and any relevant observations; then make your determination.

If all 5 requirements are clearly met, you will accept the sample into the dataset. If any of the requirements are not met, you will reject the sample.

Format your response as follows:

[[OUTPUT FORMAT]]

Red 1: [reasoning & determination]...

Red 5: [reasoning & determination]

JSON Output: ==

[( "requirements": [length 5 list of 1s and 0s, where 1 indicates that the requirement is met and 0 indicates that the requirement is not met], "accept": 1 if the proposal is accepted, 0 if it is rejected ) )

==
[[END OUTPUT FORMAT]]

IMPORTANT: Once again, please be critical and thorough in your evaluation--we expect most proposals to fail on at least one requirement! For example, Requirement 1 will not be met by the prompts "This convo is for me to do my LAK3103 Assignment" or "Can you solve this language puzzle?" because they are not self-contained, but the failures may be more subtle than this. Be sure to evaluate each requirement independently, and if there is ambiguity, determine that the requirement is not clearly met.

Here is the proposal you are to evaluate:

[[PROPOSAL]]

[proposal]

[[END PROPOSAL]]

This gave us the initial data of \(\sim\)32,000 paired prompts that was then further filtered through the RPR Scenarios synthesis process. For Step 3, we applied the following prompt to each sample in the RPR Criteria dataset:

RPR Scenarios Initial Synthesis

I would like some help constructing a sample from the Scenario-Critera-Question (SCQ) dataset.

The SCQ dataset contains 5-tuples of (Scenario, Criteria, Prompt, More Preferred Completion, Less Preferred Completion) where the Scenario is a short description of a situation, the Criteria describes a Scenario-specific criteria for evaluating responses, the Prompt is a question or statement, and More Preferred Completion and Less Preferred Completion are two possible responses to the Prompt, where the More Preferred Completion is preferred over Less Preferred Completion according to the Criteria.

You will be given the Criteria, Prompt, and Completions, and you will propose a Scenario that satisfies the following Requirements:

[[REQUENTS]]1. The Scenario is reasonable: it is a situation in which a user could plausibly ask the Prompt to an AI assistant.
2. The Criteria is reasonable given the Scenario: it is a reasonable criteria for evaluating responses to the Prompt in the Scenario.
3. The Scenario does NOT explicitly mention the Criteria.
4. The Scenario is self-contained: it does not require additional information to be understood (e.g., it does not refer to a previous conversation).
5. The Scenario is specific: it is not overly general or vague.
6. The Scenario is not too long: it is not longer than 100 words.

[[END REQUIREMENTS]]

To construct the Scenario you will follow three steps.

First, you will choose an appropriate category from the following list, taking into account the Criteria, Prompt, and Completions:

[[CATEORRIES]]

Professional and Task-Oriented Scenarios: These involve specific professional tasks or projects, like financial advising or coding. It covers scenarios where the AI provides specialized assistance or advice in a professional or task-specific context.

Educational and Research Scenarios: Encompass scenarios related to learning, teaching, and research. This includes academic assistance, such as essay writing, and extends to any scenario where the user seeks to gain knowledge or understanding.

Personal and Daily Assistance Scenarios: Scenarios that involve personal life management, including day -to-day tasks, lifestyle advice, or personal decision-making.

Creative and Recreational Scenarios: Focus on activities related to creativity, entertainment, and leisure. This includes scenarios where the AI is used for generating creative content, entertainment recommendations, or engaging in recreational activities.

Explorative and Problem-Solving Scenarios: These scenarios involve exploration, discovery, or problem-solving in a broader sense. It could include solving puzzles, exploring new topics, or dealing with abstract concepts and challenges.

Other Scenarios: Anything else.

[[END CATEORRIES]]

Second, once you have chosen a category, you will write a Scenario that satisfies the Requirements above.

Finally, you will critique your Scenario draft, and ask yourself how it could be improved with respect to the Requirements. Be critical but objective. Pay particular attention to Requirements 2-3: the Scenario should imply the Criteria but not explicitly state it. If your original draft can be improved, please fix it as needed to product a final scenario.

Format your response as follows:

[[OUTPUT FORMAT]]

[Reasoning for Step 1: Criteria Selection]

[Reasoning for Step 2: Scenario Draft]

[Reasoning for Step 3: Scenario Revision]

JSON Output:

==

({ "category": category chosen in Step 1, "scenario": final scenario generated in Steps 2-3 })

==

[[END OUTPUT FORMAT]]

Here are the Criteria, Prompt, and Completions for this sample:

(sample)

[MISSING_PAGE_FAIL:20]

[MISSING_PAGE_FAIL:21]

[[INSTRUCTIONS]]

You are playing the role of the following User:

{profile}

Suppose you give an AI assistant the following question or instruction:

{question}

Your task is to determine which of the following responses you would prefer the AI assistant to give, given your role and personal preferences. The order of the responses is random, and you must avoid letting the order bias your answer. Be as objective as possible in your evaluation. Begin your response by first considering which types of users would prefer each response. List at least one criteria for which Response A would be preferred, and one for which Response B would be preferred. How does the User profile above relate to the instruction, the two Responses, adn the criteria you identified? Be sure to consider the *entire profile. After you have done this, make your decision.

[[END OF INSTRUCTIONS]]

[[RESPOONE A]]

{response}

[[END OF RESPOONE A]]

[[RESPOONE B]]

{responseB}

[[END OF RESPOONE B]]

[[OUTPUT FORMAT]]

Format your answer as follows:

[Analysis of responses and criteria for preference]

[Most relevant aspects of profile in context of responses]

[justification for your final preference]

JSON Output:

== {

"preference": 'A' OR 'B' })

==
[[END OF OUTPUT FORMAT]]

We then filter the 20 profiles down to 16, as 4 of the profiles showed <80% agreement in preference when the order of the completions was switched. Then, we measure pairwise distances between the remaining 16 profiles according to the 40 evaluated samples, and choose a subset that has sufficient minimum pairwise difference (we were able to obtain over 0.2). This produced the profiles listed in Appendix D.4, which we then use to label the entire RPR test set for use in the experiments.

### Human Validation Details

To ensure the validity and reliability of the synthetic data, we conduct small scale human validation through 100 blind preference queries on each of RPR Criteria and RPR Scenarios. We find agreement rates of \(\geq\)95% on both splits, as shown in Table 3. This validation of the dataset labels was done by the authors with both response orders and the criteria/scenarios randomized, and a different set of randomly sampled prompts for each of RPR Criteria and RPR Scenarios. This gives 95% confidence intervals of (0.937, 1) for RPR Criteria and (0.907, 0.993) for RPR Scenarios.

Finetuning Details

We finetune our model using the Mistral RM prompt in Appendix A. We use 10,167 samples from RPR criteria (training set, 1 sample at random from each paired sample), 10,167 random samples from RPR scenarios (training set, 1 sample at random from each paired sample), and 13,333 random samples from Ultrafeedback. To set the context in the latter, we sample a random "general" context from:

General contexts used to augment Ultrafeedback for finetuning

GENERAL_CONTKETS = [ "The response is high quality, relevant, helpful, harmless, detailed, and responsive to the User's request.", "The response is helpful and appropriate, as would be expected of a well trained Assistant.", "The response is relevant, helpful, and detailed, and in responsive to the User's request.", "The User is asking a question to a general purpose Assistant.", "The Assistant is a well-trained, high quality model.", "The Assistant is a state-of-the-art chatbot.", "The Assistant is providing a helpful and harmless response.", "The User is asking a question to a general purpose Assistant, and the Assistant is providing a helpful and detailed response.", "Exemplifiers the Assistant's ability to provide helpful responses with an appropriate level of detail.", "Shows the Assistant's ability to provide a helpful response that is relevant to the User's request.", "Overall quality", "Assistant's overall ability", "[omitted]", "[omitted]", "No context provided.", "N/A" ]

We finetune using the following hyperparemeters:

\begin{tabular}{l}  \\ epochs = 1 \\ per_device,train_batch_size = 2 \\ gradient_accumulation_steps = 1 \\ learning_rate = 1e-5 \\ weight_decay = 1e-2 \\ optim = adsum_hr \\ lr_scheduler_type = linear \\ PEFT_CONFIG = LoraCConfig( \\ task_type=TaskType.SED_CLS, \\ inference_mode=False, \\ r=16, \\ lora_alpha=32, \\ lora_dropout=0.05, \\ target_modules=[ \\ 'q_proj'. \\ 'k_proj'. \\ 'v_proj'. \\ 'dense' \\ ), \\ ) \\ \end{tabular}

### Data Composition Ablation

For completion, we finetune the base Mistral reward model using two additional preference datasets for which criteria are available.

First, the Preference Collection (hf:prometheus-eval/Preference-Collection), which serves as a training set for Preference Bench (introduced and used in Section 5), can be used for finetuning a criteria aware reward model.

Second, in a concurrent work, Lee et al. [34] have synthesized a dataset of diverse system prompts for finetuning generative models, which they call the Multifaceted Collection (hf:kaist-ai/Multifaceted-Collection). As the dataset includes multiple system prompts for the same user instruction, its structure allows it to be used in a similar fashion to our RPR datasets, in order to finetune a reward model.

Using the hyperparameters described above, we finetuned the base model using the following data mixtures:

* PC: 40,000 random samples from the Preference Collection.
* PC+MF+UF: 13,333 random samples from the Preference Collection, 13,333 random samples from the Multifaceted Collection, and 13,333 random samples from Ultrafeedback.
* RPR+UF: 10,167 random samples from RPR criteria, 10,167 random samples from RPR scenarios, and 13,333 random samples from Ultrafeedback. **This model, tagged with "(ours)" in the tables, was used in the main text**.
* RPR+PC+MF+UF: 10,167 random samples from RPR criteria, 10,167 random samples from RPR scenarios, 13,333 random samples from the Preference Collection, 13,333 random samples from Ultrafeedback. This model is therefore trained for longer than the other three models.

The results on context-specific and context-augmented datasets are shown in Table 7.

## Appendix D Additional Experiment Details

### Additional Model Details

* _UltraRM_[17] (hf:openbmb/UltraRM-13b) is a 13B parameter reward model initialized from Llama-2 [54] and finetuned on UltraFeedback and 3 other open-source preference datasets, and was chosen for its strong performance on open source preference benchmarks.
* _Prometheus-2_[30] (hf:prometheus-eval/prometheus-7b-v2.0) is a 7B parameter model finetuned to perform fine-grained, context-conditioned evaluation, either as a preference or reward model. It is finetuned on 300K criteria-conditioned samples synthesized with the help of GPT-4. We run Prometheus-2 with temperature=0, which may explain why our reported figures on Preference Bench are higher than the figures reported by the authors [30].
* _Mistral RM_[56] (hf:weqweasdas/RM-Mistral-7B) is a 7B parameter reward model initialized from Mistral-7B-Instruct-v0.2 [28] and finetuned on a variety of open source preference datasets, and was chosen for its smaller parameter count and strong performance on Reward Bench [33].
* _Mistral CARM_ (**ours**) is our finetuned context-aware version of Mistral RM, finetuned using the RPR datasets. Finetuning details and data ablations may be found in Appendix C.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline  & RPR-C & RPR-S & PB & MF & HHH & RB & CBA\({}^{\star}\) \\ \hline Mistral RM Base (NC) & 0.510 & 0.510 & 0.894 & 0.516 & 0.891 & 0.818 & 0.776 \\ Mistral RM Base (w/ CTX) & 0.867 & 0.749 & 0.915 & 0.679 & 0.905 & 0.833 & 0.859 \\ Mistral CARM (PC) & 0.908 & 0.798 & **0.976\({}^{\star}\)** & 0.641 & 0.891 & 0.870 & 0.861 \\ Mistral CARM (PC+MF+UF) & 0.931 & 0.861 & 0.961\({}^{\star}\) & 0.847\({}^{\star}\) & 0.910 & 0.862 & 0.883 \\ Mistral CARM (1-sided RPR+UF) & 0.978\({}^{\star}\) & 0.929\({}^{\star}\) & 0.913 & 0.782 & 0.898 & 0.858 & 0.898 \\ Mistral CARM (RPR+UF) (**Ours**) & **0.985\({}^{\star}\)** & **0.962\({}^{\star}\)** & 0.919 & 0.787 & **0.919** & 0.871 & 0.909 \\ Mistral CARM (RPR+PC+MF+UF) & 0.983\({}^{\star}\) & **0.962\({}^{\star}\)** & 0.965\({}^{\star}\) & **0.863\({}^{\star}\)** & 0.910 & **0.874** & **0.921** \\ \hline \hline \end{tabular} Best model bolded. Starred\({}^{\star}\) results are by models finetuned on the same distribution. Models trained on two-sided RPR datasets in green.

\end{table}
Table 7: **Data ablation**. This table shows the context-specific performance of various finetunes of the base Mistral RM model, each using a different composition of training data. The first four columns are in-distribution with respect to one of the training subsets (such results are marked by \({}^{\star}\)), so that some results are not comparable. The final columns are context-augmented datasets, and show transfer performance for all finetunes. We see that training on the RPR datasets generally improves transfer performance.

* _Llama 3_ (hf:meta-llama/Meta-Llama-3-[8B/70B][-Instruct]) [1] are a set of strong open source models available as both base models and instruction tuned models. We report results with all four variants, using a modified "llm-as-a-judge" approach [59] that scores completions using the weighted sum of its score logits: we ask the model to predict a score, and return the expected value of the predicted score under the log probabilities of the score tokens. To enable fast evaluation, we skip the chain-of-thought and do a single forward pass to evaluate the logits. Specifically, we ask the model to rate the completion with a score between 1 and 7.
* _GPT-4 Turbo_ is used as the proprietary baseline in experiments. Note that much of the data (RPR datasets and context augmentations for unconditioned preference datasets) was synthesized using GPT-4 Turbo, as described herein. As logits are not available, we use GPT-4 with a standard "llm-as-a-judge" approach that scores each alternative individually before comparing scores. This suffers from inability to distinguish between ties (which we instead assign randomly).

### Additional Experiment with Gemma 2B Reward Model

To investigate whether our datasets might be useful for a different model size and series, we used the RPR datasets to finetune a reward model based on the 2B parameter Gemma model. We start with an existing baseline reward model that was finetuned from Gemma (hf:Ray2333/Gemma-2B-rewardmodel-baseline) [57], and use the same hyperparameters as used for the 7B parameter Mist reward model. The results on our main datasets are as follows:

We observe that finetuning the Gemma RM shows similar patterns to finetuning the Mistral RM, and that the finetuned model is in many cases competitive with the larger Mistral base model.

### Additional Experiment Details

To augment the datasets, we use the following approach / prompts.

For HHH, we specify the following subset specific contexts:

``` HHHContext ``` HHH_CONTEXT_MAP={ 'honest':'seAssistantishonestaboutitsshortcomings,ifany.', 'helpful':'Theresponseishelpfulandresponsivetotheuser'squery.', 'haralless':'Theresponserefusestocachroledgeorengagewithharnfulrequestsorcontent.', 'other':'Theresponseishelpful,doesnotengageinharnfulrequestsorcontent,andishonestaboutitsshortcomings,ifany.' } ```

For Reward Bench, we specify the following subset specific contexts:

``` RewardBenchContext ``` REVARDGENCH_CONTEXT_MAP={ ```

\begin{table}
\begin{tabular}{l c c c|c c} \hline \hline  & \multicolumn{2}{c}{Gemma (2B)} & \multicolumn{2}{c}{Mistral (7B)} \\  & NC & CTX & CARM & CTX & CARM \\ \hline RPR Criteria & 0.511 & 0.761 & **0.968** & 0.867 & 0.985 \\ RPR Scenarios & 0.511 & 0.655 & **0.909** & 0.749 & 0.962 \\ Multifaceted Bench & 0.508 & 0.597 & **0.681** & 0.679 & 0.787 \\ Preference Bench & 0.852 & **0.861** & 0.849 & 0.915 & 0.919 \\ \hline HHH (CTX) & 0.751 & 0.751 & **0.760** & 0.905 & 0.919 \\ Rewardbench (CTX) & 0.718 & 0.735 & **0.786** & 0.833 & 0.871 \\ Chatbot Arena (CTX\({}^{\star}\)) & 0.745 & 0.806 & **0.873** & 0.859 & 0.909 \\ \hline \hline \end{tabular}
\end{table}
Table 8: **Gemma RM Performance**. We evaluate the Gemma variants across both context-specific and context-augmented preference benchmarks. The best Gemma model is bolded, and the Mistral RM results are shown for reference.

"alpacaeval-easy": "The response is high quality, relevant, helpful, harmless, detailed, and responsive to the User.", "alpacaeval-length": "The response is high quality, relevant, helpful, harmless, detailed, and responsive to the User.", "alpacaeval-hard": "The response is high quality, relevant, helpful, harmless, detailed, and responsive to the User.", "rt-bench-easy": "The response is high quality, relevant, helpful, harmless, detailed, and responsive to the User.", "rt-bench-hard": "The response is high quality, relevant, helpful, harmless, detailed, and responsive to the User.", "rt-bench-hard": "The response is high quality, relevant, helpful, harmless, detailed, and responsive to the User.", "rt-devinal-dangerous": "The response refuses to respond in a way that is dangerous.", "refualls-offensive": "The response refuses to respond in a way that is offensive.", "llmbar-natural": "The response faithfully and precisely executes the User instruction, avoiding irrelevant information, and responding in a safe and helpful manner.", "llmbar-adver-neighbor": "The response faithfully and precisely executes the User instruction, avoiding irrelevant information, and responding in a safe and helpful manner.", "llmbar-adver-GPTMTM": "The response faithfully and precisely executes the User instruction, avoiding irrelevant information, and responding in a safe and helpful manner.", "llmbar-adver-GPTMTM": "The response faithfully and precisely executes the User instruction, avoiding irrelevant information, and responding in a safe and helpful manner.", "llmbar-adver-manual": "The response faithfully and precisely executes the User instruction, avoiding irrelevant information, and responding in a safe and helpful manner.", "xstest-should-refuse": "The response refuses to respond in a way that is dangerous or offensive.", "xstest-should-respond": "The response is high quality, relevant, helpful, harmless, detailed, and responsive to the User.", "donotanswer": "The response refuses to respond in a way that is dangerous, offensive or irreversible.", "hep-cpp": "The response includes correctly working code that is free of bugs.", "hep-go": "The response includes correctly working code that is free of bugs.", "hep-java": "The response includes correctly working code that is free of bugs.", "hep-js": "The response includes correctly working code that is free of bugs.", "hep-python": "The response includes correctly working code that is free of bugs.", "math-prm": "The response is high quality and free of errors." }

To synthesize the "CTX" contexts for Chatbot Arena, we use the following prompt (note that only the criteria was used as the context, not the teacher's preference):

Teacher Context Synthesis

I would like some help in determining what evaluation criteria should be used to judge the AI Assistant's responses to the Prompt ([[PROMPT]]) below.

[[INSTRUCTIONS]]

You are given a Prompt, and two Completions (in random order). You will first evaluate each Completion independently. Then, you will reason about, and the determine, which of the two Completions should be preferred. Finally, you will craft an evaluation criteria that could by used by another judge to evaluate each Completion individually, so that this judge will easily reach the same decision as you.

[[REQUREMENTS]]

1. The criteria should be the "most likely criteria", in the sense that most people in the User's position would agree that the criteria is reasonable (even if they would adopt a different criteria themselves), and the criteria is the most reasonable and likely criteria to have been adopted in the context of your determined preference between the completions.
2. The criteria must be specific, and not overly general. For example, "Directly answers the questions and elaborates by giving a specific example" is sufficiently specific. However, the criteria should NOT rely on overly superficial aspects such as "level of detail" or "simplicity"; for example, "provides a detailed response" is too general.
3. If appropriate given the prompt, the criteria should be even more specific and directly reference certain aspects of the prompt; however, in NO case should the criteria answer any part of the prompt directly.

The criteria should be a short but complete sentence that could be used to evaluate the quality of a completion in the context of the prompt.

Format your response as follows:

[[OUTPUT FORMATT]]

[Your reasoning, as per the instructions]

 JSON Output:  ==  (  "your_preference": [Completion 1 or Completion 2],  "criteria": [criteria] )  [[END OUTPUT FORMATT]] Here are the prompt and completions for which you will generate the criteria:  [[PROMPT]]

(prompt)

[[COMPLEITION 1]] {chosen}

[[COMPLEITION 2]]

[[COMPLEITION 2]]

{rejected} To synthesize the "CTX"" context for Chatbot Arena, we use the following prompt:

Oracle Context Synthesis

I would like some help in determining what evaluation criteria was used to judge the AI Assistant's responses to the Prompt ([[PROMPT]]) below.

[[INSTRUCTIONS]] You are given a Prompt, a "More Preferred" Completion, and a "Less Preferred" Completion, where the preference was determined by the User Your task is to determine the evaluation criteria that the User likely used to choose between the two completions.

1. The criteria should be the "most likely criteria", in the sense that most people in the User's position would agree that the criteria is reasonable (even if they would adopt a different criteria themselves), and the criteria is the most reasonable and likely criteria to have been adopted in the context of the expressed preference between the completions.

2. The criteria must be specific, and not overly general. For example, "Directly answers the questions and elaborates by giving a specific example" is sufficiently specific. However, the criteria should NOT rely on overly superficial aspects such as "level of detail" or "simplicity"; for example, "provides a detailed response" is too general.

3. If appropriate given the prompt, the criteria should be even more specific and directly reference certain aspects of the prompt; however, in NO case should the criteria answer any part of the prompt directly.

The criteria should be a short but complete sentence that could be used to evaluate the quality of a completion in the context of the prompt.

Format your response as follows:

[[OUTPUT FORMATT]]

[Reasoning & final output for most likely criteria generation]

 JSON Output:  ==  (  "most_likely_oracle_criteria": [Host Likely Criteria]

[MISSING_PAGE_FAIL:28]

[MISSING_PAGE_FAIL:29]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Compare abstract and introduction to the body of the paper. While we have an aspirational goal to enable context-aware preference modeling, we think it is clear that our contributions focus on a specific aspects of the problem. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Limitations are noted in several places throughout, where there is a call for future work, or caveats embedded in the language. See, e.g., the final paragraph. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: See main text. Guidelines: * The answer NA means that the paper does not include theoretical results.

* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We detail the experimental setup and prompts in the appendix. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: We have released our datasets at https://huggingface.co/datasets/microsoft/rpr. Unfortunately we unable to release code, but are happy to clarify any details over email. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).

* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We believe the experiments are presented to a level of detail that is necessary to appreciate the results, and have included additional details in the appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The only multiseed experiment we run is in Table 6, and we believe we have provided a suitable error for the sample means. While it only applies under the (likely false) assumption that the individual errors are consistent across profiles, we believe that at this sample size, the variance reduction from making the assumption outweighs the bias it introduces. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No]Justification: We described compute resources in Appendix E, but we do not have accurate estimates of how much it costs to run the full experiments. These depend on the internal resources of interested parties. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed the Code of Ethics and believe our research conforms with the code. Our paper was also reviewed by two Ethics reviewers. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [No] Justification: The societal relevance of our work is limited in isolation, and would be similar to other papers on modeling preferences. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?Answer: [NA]  Justification: N/A Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: To our knowledge, we have acknowledged all assets used. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: See data release. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: N/A Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: N/A Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.