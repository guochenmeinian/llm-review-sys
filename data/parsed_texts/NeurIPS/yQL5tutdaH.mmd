Toward a Stable, Fair, and Comprehensive Evaluation of Object Hallucination in Large Vision-Language Models

Hongliang Wei, Xingtao Wang1, Xianqi Zhang, Xiaopeng Fan, Debin Zhao

Harbin Institute of Technology

Harbin, China

Corresponding author.

Footnote 1: footnotemark:

###### Abstract

Given different instructions, large vision-language models (LVLMs) exhibit different degrees of object hallucinations, posing a significant challenge to the evaluation of object hallucinations. Overcoming this challenge, existing object hallucination evaluation methods average the results obtained from a set of instructions. However, these methods fail to provide consistent evaluation across instruction sets that generate image descriptions of significantly different lengths. In this paper, we present the first systematic investigation into the effect of instructions on object hallucinations in LVLMs, with a specific focus on the role played by image description lengths. A valuable finding is that instructions indirectly affect hallucinations through the length of image descriptions. The longer the image description, the higher the object hallucination degree. Accordingly, we fit an informative length-hallucination curve, upon which a fine-grained evaluation framework named LeHaCE is introduced for evaluating object hallucinations at any given image description length. LeHaCE evaluates the object hallucination degree at a uniform image description length to mitigate the effect of description lengths, promoting stability and fairness. Moreover, LeHaCE incorporates the curve slope as an innovative hallucination evaluation metric, reflecting the extent to which the object hallucination degree is affected by the image description length, achieving a more comprehensive evaluation. Experimental results demonstrate that LeHaCE provides a more stable, fair, and comprehensive evaluation of object hallucinations in LVLMs compared to existing methods.

## 1 Introduction

Drawing inspiration from the remarkable language capabilities exhibited by large language models (LLMs) [1, 2, 3], large vision-language models (LVLMs) [2, 4, 5, 6, 7] have been well-developed, achieving significant advancements in complex multimodal tasks. However, the practical application of LVLMs is heavily hindered by hallucination phenomena [8, 9], which refer to situations where objects in image descriptions generated by LVLMs are inconsistent with the provided visual content. Considerable efforts have been dedicated to both evaluation [10, 11, 9] and mitigation [12, 13, 14] of hallucination phenomena, leading to notable advancements.

A significant challenge in object hallucination evaluation arises from the effect of instructions on object hallucinations [9]. Overcoming this challenge, existing object hallucination evaluation methods typically adopt an average-based framework, which averages the results obtained from a set of instructions. However, as shown in Figure 1, this framework fails to provide consistent evaluation across instruction sets that generate image descriptions of significantly varying lengths. Specifically,while evaluation results of LVLMs remain consistent across certain instruction sets (e.g., set 1 and set 2), inconsistencies arise when comparing instruction sets with significantly different average image description lengths (e.g., set 2 and set 3).

In this paper, we present the first systematic investigation into the effect of instructions on object hallucinations in LVLMs, with a specific focus on the role played by the length of image descriptions. Technically, we evaluate lengths and object hallucination degrees (measured by CHAIR scores) of the image descriptions generated by LVLMs under different instructions (see Section 3.1 for more details). The experimental results are shown in Figures 2 & 3, from which we can observe that the degree of object hallucination is primarily influenced by the length of image descriptions, with instructions only indirectly affecting hallucinations through their effect on description lengths. The longer the image description, the higher the object hallucination degree, and there is a clear linear relation between them. Hence, it is imperative to take into account the length of image descriptions in hallucination evaluation. Unfortunately, the average-based framework can only select instructions, without the ability to directly control the length of image descriptions.

Motivated by the findings, we propose a fine-grained evaluation framework called LeHaCE, which fits an informative length-hallucination curve to evaluate object hallucinations at any given image description length within a large range. LeHaCE evaluates the object hallucination degree at a uniform image description length to mitigate the effect of image description length, ensuring stable evaluations for the same LVLM across different instruction sets and fair comparisons among different LVLMs. Moreover, LeHaCE incorporates the curve slope as an innovative hallucination evaluation metric, reflecting the extent to which the object hallucination degree is influenced by the image description length, achieving a more comprehensive evaluation. Experiment results on 12 representative LVLMs show that LeHaCE can evaluate object hallucinations of LVLMs in a more stable, fair, and comprehensive way.

The main contributions of this paper are summarized as follows:

* We conduct the first systematic investigation into the effect of instructions on object hallucinations in LVLMs and find that the degree of object hallucinations is primarily influenced by the length of image descriptions, with instructions only indirectly affecting hallucinations through their effect on image description lengths.
* We propose an object hallucination evaluation framework called LeHaCE, which fits an informative length-hallucination curve to evaluate object hallucination at a uniform image description length, realizing a more stable and fair evaluation.
* We employ the curve slope as an innovative hallucination evaluation metric, reflecting the extent to which the object hallucination degree is affected by the image description length, achieving a more comprehensive evaluation.

Figure 1: The evaluation results of LVLMs on four instruction sets using the CHAIR with the average-based framework. Length refers to the average length of generated image descriptions. Each instruction set consists of six distinct instructions, and there is no overlap between instructions in different sets. All instructions prompt LVLMs to describe the image.

Related work

### Large Vision-Language Models

Inspired by the success of LLMs in NLP [1; 2; 3], researchers have extended LLMs to multimodal tasks [15; 16; 17; 18; 19; 20; 21; 22; 23; 24; 25; 26; 27; 28], proposing numerous LVLMs and achieving new advancements [7; 14; 29; 30; 31; 32; 33; 34; 35]. These LVLMs align the multi-modal encoders with LLM through multitask fine-tuning and instruction fine-tuning on multi-modal datasets, enabling LLM to acquire multi-modal perception and instruction-following capabilities. Specifically, to integrate multimodal features, Flamingo [29] proposes a cross-attention structure to achieve arbitrary interleaved multi-modal feature fusion. BLIP-2 [35] introduces Q-Former to bridge the visual backbone model and LLM. mPLUG-Owl2 [36] introduces a modality adaptive module to facilitate the fusion between different modules. To enhance generalization and improve instruction-following capabilities, some methods [4; 5; 6; 12; 37] propose multi-task fine-tuning and instruction fine-tuning for LVLMs. Among them, LRV-instruction [12], MiniGPT-4 [5], LLaVA [6] and SViT [37] employ ChatGPT to augment instruction data. To mitigate the risk of catastrophic forgetting of language knowledge during the training process, mPLUG-Owl [38] and LLaVA-1.5 [6] perform joint training on pure language and visual-language instructional data. More recently, mPLUG-DocOwl [31], InternLM-XComposer [32], Kosmos-2 [33], Shikra [34], Cantor [39], BuboGPT [30], and Qwen-VL [7] further enhance the capabilities of LVLMs in optical character recognition, document understanding, multi-modal interleaved composition and visual grounding.

### Hallucination in LVLMs

Works on the hallucination in LVLMs focus on two aspects: evaluation and mitigation. For the hallucination evaluation, POPE [9] designs a polling-based query method to avoid the influence of instructions on hallucination evaluation. By presenting LVLMs with brief "yes" or "no" questions regarding the target of detection, the evaluation of hallucination is transformed into a simple binary classification task. NOPE [10] designs a novel benchmark to evaluate the performance of LVLMs in recognizing the non-existence of objects in visual questions. AMBER [11] designs a multi-dimensional LVLMs hallucination evaluation benchmark without LLMs, targeting existence, attribute, and relation hallucination. For the hallucination mitigation, LRV-Instruction [40] creates a balanced set of positive and negative instructions to perform robust visual instruction adjustment for LVLMs. VIGC [14] employs an iterative approach to generate detailed and accurate answers gradually. Woodpecker [13] proposes a post-processing method that utilizes expert models to locate and correct hallucinations from generated text.

While existing methods [9] observe that the hallucination degree of LVLMs is unstable across different instructions, this phenomenon has not been thoroughly investigated to date. This work presents the first comprehensive study investigating the influence of instructions on the hallucination rate of LVLMs. Building upon our findings, we propose LeHaCE framework, which can evaluate hallucination of LVLMs in a more stable and comprehensive manner. Contrary to polling-based query methods, LeHaCE can directly evaluate the hallucination rate of image descriptions generated by LVLMs, which is more in line with the practical application scenarios of LVLMs.

## 3 Hallucination of LVLMs Under Different Instructions

This section provides the investigation into the effect of instructions on hallucinations, with a specific focus on the role played by the length of image descriptions. The experimental settings are presented initially, followed by a comprehensive analysis of the experimental results

### Experimental Settings

In this investigation, twelve popular LVLMs are included, namely Gemini-Pro-Vision pro [2], Qwen-VL [7], MiniGPT-4 [5], LLaVA [6], InstructBLIP [4], LLaMA-Adapter-v2 [41], mPLUG-Owl2 [36], mPLUG-Owl [38], InternLM-XComposer [32], VPGTrans [42], Otter [43] and Lynx [44]. All LVLMs are prompted by 25 different instructions to generate image descriptions for 256 images in MSCOCO [45]. All descriptions are generated using beam search with a beam size of 5. For the instructions, we utilize those from [4] and additionally propose several others, as detailed in the appendix. We use CHAIR [8] as the evaluation metric for hallucinations, which has two variants: CHAIRI and CHAIRS. Given the ground truth objects in the image, CHAIRI calculates the proportion of objects that appear in the descriptions but not in the image, while CHAIR\({}_{\text{S}}\) is the proportion of descriptions that include hallucination. Formally, CHAIR\({}_{\text{I}}\) and CHAIR\({}_{\text{S}}\) can be expressed as follows:

\[\text{CHAIR}_{I}=\frac{|\{\text{hallucinated objects}\}|}{|\{\text{ all mentioned objects}\}|},\] (1) \[\text{CHAIR}_{S}=\frac{|\{\text{descriptions with hallucinated objects}\}|}{|\{\text{all descriptions}\}|}.\] (2)

For more experimental settings, we use the Pearson correlation coefficient to measure the correlation between the average length and the hallucination rate of image descriptions. Lengths are measured in word count.

### Experimental Analysis

The results are presented in Figure 2 & 3, from which we get two key observations: **1)** Figure 2 shows the relationship between the hallucination rate and the average image description length, we can observe that the hallucination rate increases with the average image description length and there is a clear linear correlation between them. Specifically, the Pearson correlation coefficient between hallucination rates and the average image description lengths exceeds 0.6 for all LVLMs, with 10 LVLMs exceeding 0.8 and 5 LVLMs exceeding 0.9. **2)** Figure 3 shows the impact of instructions on the length of image descriptions generated by LVLMs, from which we can observe that the length of image descriptions generated by the same LVLM with

Figure 2: Scatter plots of CHAIR scores and average lengths of the 25 sets of image descriptions generated by 25 instructions. \(r\) denotes the Pearson correlation coefficient between the hallucination rates and the average image description lengths, \(R^{2}\) and \(P\) represent the coefficient of determination and \(p\)-value respectively for the linear regression.

different instructions can vary significantly, e.g., Gemini-Pro-Vision with Instruction 11 (101 words in average) v.s. Gemini-Pro-Vision with Instruction 18 (20 words in average). Furthermore, the length of image descriptions generated by different LVLMs with the same instruction can also differ greatly, e.g., MiniGPT-4 (11 words in average) v.s. Gemini-Pro-Vision (97 words in average) with Instruction 17.

Based on the aforementioned two observations, we can draw the following conclusions: **1)** The degree of object hallucinations is primarily influenced by the length of image descriptions, with instructions only indirectly affecting hallucinations through their effect on image description lengths. Hence, it is imperative to take into account the length of image descriptions in hallucination evaluation. However, controlling the length of image descriptions generated by LVLMs is challenging 2, given that even subtle semantic differences between instructions can significantly impact the output length of LVLMs (shown in Figure 3). **2)** In addition to the hallucination degree, the rate at which hallucination degree increase with description length is also a meaningful indicator for characterizing the nature of LVLMs hallucinations. Considering both the hallucination degree and the growth rate of hallucination degree can provide a more comprehensive evaluation for hallucinations in LVLMs. For example, as shown in the Figure 2, although InstructBLIP has the lowest hallucination degrees in short image descriptions, it exhibits high instability with a rapid increase in hallucination degrees, resulting in high hallucination in long image descriptions.

Footnote 2: This work does not consider controlling length by truncating the generated descriptions, only considering cases where LVLMs generate complete image descriptions, as this better fits practical application scenarios.

## 4 Length-Hallucination Curve Based Hallucination Evaluation Framework

In this section, we first introduce the average-based hallucination evaluation framework and discuss its limitations. Then, we elaborate on the proposed LeHaCE framework and evaluate representative LVLMs with LeHaCE. Finally, the stability of LeHaCE is analysed.

### Average-Based Hallucination Evaluation Framework

The average-based hallucination evaluation framework mitigates the challenge caused by instructions by averaging the hallucination rates over different instructions. Formally, the hallucination rates and average lengths of the image descriptions generated by the LVLM under N instructions are denoted as \(\left\{\ell_{i},hr_{i}\right\}_{i=1}^{N}\). The average hallucination rate \(\bar{hr}\) and average length \(\bar{\ell}\) of image descriptions over all instructions can be calculated as follows: \(\bar{hr}=\frac{1}{N}\sum_{i=1}^{N}hr_{i}\) and \(\bar{\ell}=\frac{1}{N}\sum_{i=1}^{N}\ell_{i}\). The average-based hallucination evaluation framework utilizes \(\bar{hr}\) to evaluate the hallucination of LVLMs.

However, due to substantial variations in the average lengths of image descriptions generated by different instruction sets, the average-based framework struggles to mitigate the effect of image description lengths on object hallucinations, resulting in unstable and unfair evaluations. Specifically, as shown in Figure 4 (left), when the average-based framework evaluates an LVLM under different instruction sets, the inconsistent average image description lengths lead to unstable evaluation. Moreover, Figure 4 (right) shows that when the average-based framework evaluates different LVLMs under the same instructions, the inconsistent average image description lengths lead to unfair evaluation.

### Length-Hallucination Curve Based Hallucination Evaluation Framework

Section 3.2 reveals the significant effect of image description lengths on the hallucination degree. To mitigate this effect, it is crucial to control the description length during the hallucination evaluation.

Figure 3: The average lengths of image descriptions generated by LVLMs when prompted by different instructions.

However, controlling the length of generated image descriptions is challenging because LVLMs are highly sensitive to instructions. To address this challenge, we fit a length-hallucination curve to evaluate LVLMs at any desired length. Specifically, based on the clear linear correlation observed in Section 3.2, we assume a linear correlation between image description lengths and hallucination rates of LVLMs. Figure 4 intuitively illustrates the LeHaCE framework.

Formally, we use \(\left\{\ell_{i},hr_{i}\right\}_{i=1}^{N}\) to represent the average lengths and hallucination rates of image descriptions generated by the LVLM under N instructions. The linear regression curve of \(\left\{\ell_{i},hr_{i}\right\}_{i=1}^{N}\), which we refer to as the Length-Hallucination Curve (LHC), can be formalized as follows:

\[\text{LHC}\left(\ell\right)=\beta\ast\ell+\alpha,\] (3)

where \(\beta\) and \(\alpha\) are:

\[\beta=\frac{\sum_{i=1}^{N}\left(\ell_{i}-\bar{\ell}\right)\left(hr _{i}-\bar{hr}\right)}{\sum_{i=1}^{N}\left(\ell_{i}-\bar{\ell}\right)^{2}},\] (4) \[\alpha=\bar{hr}-\left(\beta\ast\bar{\ell}\right).\] (5)

Length-hallucination curve summarizes the trend between the hallucination rate and the description length. The regression coefficient \(\beta\) represents the rate at which the hallucination rate increases with the growth of description length. LeHaCE uses the LHC to evaluate the hallucination in LVLMs. LeHaCE consists of two metrics:

\[\text{LeHaCE}\left(\ell\right)=\text{LHC}\left(\ell\right),\] (6) \[\text{LeHaCE}_{GR}=\beta\] (7)

where LeHaCE\(\left(\ell\right)\) measures the hallucination rate at the specified length \(\ell\), and LeHaCE\({}_{GR}\) measures the rate at which the hallucination rate increases with the increase in description length. Note that LeHaCE can be built upon any hallucination degree evaluation metric, enhancing their stability, fairness, and comprehensiveness. In this paper, we use CHAIR as the metric for measuring the hallucination degree.

Compared to the average-based hallucination evaluation framework, LeHaCE has three advantages. As intuitively shown in Figure 4, LeHaCE can evaluate the hallucination degree of LVLMs at a uniform image description length, thereby mitigating the influence of description length on hallucination degree and resulting in a **more stable and fair** evaluation. Moreover, LeHaCE can evaluate the hallucination degree at multiple lengths and the growth rate of hallucination degree, leading to a **more comprehensive** evaluation.

### Evaluation on MSCOCO and NoCaps

We evaluate twelve LVLMs with LeHaCE at lengths of 20, 40, 60, and 80 words. This evaluation is conducted on subsets of the MSCOCO [45] test set and the NoCaps [46] validation set, each compris

Figure 4: **Illustrations** of the average-based evaluation framework (ABF) and our LeHaCE framework. The **left** figure presents the object hallucination evaluation of LLaVa on two instruction sets. The **right** figure presents the object hallucination evaluation of LLaVa and mPLUG-Owl on the same set of instructions.

ing randomly selected 256 images. The length-hallucination curve in LeHaCE is fitted on the CHAIR scores of image descriptions generated by 25 different instructions. To calculate CHAIR scores on NoCaps, we follow the setting proposed in [8; 47]. All descriptions are generated using beam search with a beam size of 5. The experiments are conducted with PyTorch on Nvidia GeForce RTX 3090 GPUs.

The results are shown in Table 1, which demonstrate that LeHaCE can evaluate the object hallucination degree of LVLMs at given image description lengths, as well as the growth rate of the hallucination degree, providing a fair and comprehensive evaluation. Specifically, **1)** For short descriptions, InstructBLIP achieves the best performance on both the MSCOCO and NoCaps datasets. However, its higher growth rate of hallucination degree leads to poor performance on longer descriptions. **2)** For medium-length and long descriptions, Gemini-Pro-Vision and Qwen-VL exhibit the best performance on the MSCOCO and NoCaps datasets, respectively. This is attributed to their relatively small growth rate in hallucination degree. **3)** Gemini-Pro-Vision and LLaVA exhibit the lowest growth rate in hallucination degree on the MSCOCO and NoCaps datasets, respectively.

In Table 1, LVLMs exhibit higher degrees of hallucination on the NoCaps dataset compared to the MSCOCO dataset. This is attributed to the fact that LVLMs typically use the MSCOCO for training, making the NoCaps dataset an out-of-distribution dataset. The results show that the distributional differences not only increase the hallucination degree of LVLMs at various description lengths but also amplify the growth rate of hallucination degree.

### Stability of LeHaCE

As mentioned above, LeHaCE evaluates the hallucination degree of LVLMs in a more stable manner. This subsection verifies the stability of the proposed LeHaCE framework. Specifically, LVLMs are prompted by three sets of different instructions to generate three sets of image descriptions. Each instruction set consists of multiple instructions randomly drawn from a pool of 25 instructions, with no overlap between instructions in different sets. The image descriptions generated by different instructions in each set are evaluated using the LeHaCE framework and the average-based framework, respectively. The stability of the LeHaCE and the average-based frameworks on the three sets of image descriptions is evaluated using the **Relative Standard Deviation (RSD)**, which is defined as the ratio of the standard deviation \(\sigma\) to the mean \(\mu\), \(RDS=\sigma/\mu\). The lower the RSD, the more stable

\begin{table}
\begin{tabular}{l c c c c c|c c c c c} \hline \hline
**Model** & \(\text{L}_{\text{C}}(20)\) & \(\text{L}_{\text{C}}(40)\) & \(\text{L}_{\text{C}}(60)\) & \(\text{L}_{\text{C}}(80)\) & \(\text{L}_{\text{C}}(68)\) & \(\text{L}_{\text{C}}(20)\) & \(\text{L}_{\text{C}}(40)\) & \(\text{L}_{\text{C}}(60)\) & \(\text{L}_{\text{C}}(80)\) & \(\text{L}_{\text{C}}(86)\) \\ \hline \multicolumn{11}{c}{**MSCOCO**} \\ \hline MiniGPT-4 & 5.33 & 6.66 & 7.98 & 9.31 & 0.07 & 9.27 & 15.71 & 22.15 & 28.59 & 0.32 \\ InstructBLIP & **2.35** & **5.10** & 7.86 & 10.61 & 0.14 & **5.61** & 16.24 & 26.87 & 37.50 & 0.53 \\ Lynx & 3.26 & 6.49 & 9.72 & 12.95 & 0.16 & 8.00 & 17.48 & 26.97 & 36.46 & 0.47 \\ LLAVA & 7.22 & 8.30 & 9.38 & 10.46 & **0.05** & 14.48 & 20.31 & 26.14 & 31.97 & 0.29 \\ Outer & 8.76 & 12.66 & 16.56 & 20.45 & 0.19 & 15.31 & 29.88 & 44.45 & 59.02 & 0.73 \\ VPGTrans & 5.77 & 6.87 & 7.97 & 9.08 & 0.06 & 9.08 & 15.01 & 20.94 & 26.86 & 0.30 \\ LLaMA-Adapter-v2 & 6.04 & 9.29 & 12.54 & 15.80 & 0.16 & 11.31 & 22.99 & 34.66 & 46.34 & 0.58 \\ mPLUG-Owl & 7.15 & 10.84 & 14.52 & 18.20 & 0.18 & 11.18 & 23.71 & 36.25 & 48.79 & 0.63 \\ Gemini-Pro-Vision & 4.30 & 5.22 & **6.15** & **7.07** & **0.05** & 8.00 & **12.61** & **17.22** & **21.83** & **0.23** \\ Internal-XComposer & 5.40 & 7.82 & 10.25 & 12.67 & 0.12 & 9.48 & 19.18 & 28.88 & 38.58 & 0.48 \\ Qwen-VL & 3.44 & 5.36 & 7.28 & 9.20 & 0.10 & 6.15 & 15.31 & 24.47 & 33.63 & 0.46 \\ mPLUG-Owl2 & 3.92 & 7.39 & 10.86 & 14.33 & 0.17 & 8.19 & 21.66 & 35.12 & 48.59 & 0.67 \\ \hline \multicolumn{11}{c}{**NoCaps**} \\ \hline MiniGPT-4 & 14.53 & 16.79 & 19.05 & 21.30 & 0.11 & 23.75 & 35.75 & 47.76 & 59.77 & 0.60 \\ InstructBLIP & **6.52** & **10.20** & 13.88 & 17.56 & 0.18 & 13.33 & 26.39 & 39.45 & 52.50 & 0.65 \\ Lynx & 13.79 & 17.18 & 20.57 & 23.96 & 0.17 & 36.07 & 46.11 & 56.16 & 66.21 & 0.50 \\ LLaVA & 12.68 & 14.48 & 16.29 & 18.09 & **0.09** & 24.15 & 33.90 & 43.66 & 53.42 & **0.49** \\ Otter & 15.49 & 19.03 & 22.58 & 26.12 & 0.18 & 25.38 & 38.89 & 52.40 & 65.91 & 0.68 \\ VPGTrans & 12.51 & 14.39 & 16.26 & 18.14 & **0.09** & 20.39 & 31.95 & 43.51 & 55.07 & 0.58 \\ LLaMA-Adapter-v2 & 12.52 & 16.07 & 19.62 & 23.17 & 0.18 & 22.24 & 35.31 & 48.18 & 61.04 & 0.64 \\ mPLUG-Owl & 12.85 & 15.84 & 18.84 & 21.83 & 0.15 & 19.77 & 30.68 & 41.60 & 52.52 & 0.55 \\ Gemini-Pro-Vision & 12.76 & 15.17 & 17.57 & 19.98 & 0.12 & 22.63 & 34.56 & 46.50 & 58.44 & 0.60 \\ IntemLM-XComposer & 10.93 & 12.74 & 14.54 & 16.34 & **0.09** & 20.12 & 31.22 & 42.33 & 53.44 & 0.56 \\ Qwen-VL & 8.37 & 10.69 & **13.01** & **15.33** & 0.12 & 14.15 & **25.00** & **35.85** & **46.71** & 0.54 \\ mPLUG-Owl2 & 6.91 & 10.82 & 14.72 & 18.63 & 0.20 & **11.72** & 25.45 & 39.17 & 52.90 & 0.69 \\ \hline \hline \end{tabular}
\end{table}
Table 1: LeHaCE scores of LVLMs on the MSCOCO and NoCaps datasets. \(\text{L}_{\text{C}_{1}}\) and \(\text{L}_{\text{C}_{5}}\) represent CHAIR\({}_{\text{I}}\) and CHAIR\({}_{\text{S}}\) with the LeHaCE framework. The best result on each metric for each dataset is represented in bold, and the second best result is indicated with an underline.

\begin{table}
\begin{tabular}{c c c|c c|c c|c c|c c|c c|c c|c c} \hline \hline \multicolumn{10}{c}{**MSCOCO**} \\ \hline \multirow{2}{*}{\# of Ins} & \multicolumn{4}{c|}{Gemini-Pro-Vision} & \multicolumn{4}{c|}{Qwen-VL} & \multicolumn{4}{c|}{MiniGPT-4} & \multicolumn{4}{c}{LLaVA} \\ \cline{2-13}  & C\({}_{\text{I}}\) & L\({}_{\text{C}_{\text{I}}}\) & C\({}_{\text{S}}\) & L\({}_{\text{C}_{\text{I}}}\) & C\({}_{\text{C}_{\text{I}}}\) & L\({}_{\text{C}_{\text{S}}}\) & L\({}_{\text{C}_{\text{I}}}\) & C\({}_{\text{C}_{\text{I}}}\) & L\({}_{\text{C}_{\text{I}}}\) & C\({}_{\text{S}}\) & L\({}_{\text{C}_{\text{S}}}\) & L\({}_{\text{C}_{\text{I}}}\) \\ \hline

[MISSING_PAGE_POST]

[MISSING_PAGE_FAIL:9]

For the stability of LeHaCE at different lengths, the results are shown in Figure 5, from which we can see that LeHaCE significantly improves the stability of the CHAIR metrics across a wide range of description lengths. All of these experimental results validate the superior stability of LeHaCE.

## 5 Conclusion and Limitations

**Conclusion**: In this paper, we find the degree of object hallucinations is primarily influenced by the length of image descriptions, with instructions only indirectly affecting hallucinations through their effect on image description lengths. The degree of object hallucination and the length of image descriptions exhibit a clear positive linear correlation. Based on our findings, a stable, fair and comprehensive object hallucination evaluation framework named LeHaCE is introduced. Extensive experimental results validate the superiority of LeHaCE over existing frameworks.

**Limitations**: Despite exhaustive investigations, this work still has potential limitations. **1)** We focus on object hallucination, leaving other types of hallucinations for future work. **2)** Due to computational constraints, we evaluate LVLMs on only a subset of each dataset. Nevertheless, we conduct thorough experiments across various datasets to validate our findings and method. **3)** Due to high API fees, we only explore one proprietary business LVLM in our experiments. However, we conduct in-depth analyses on eleven open-source LVLMs, validating the broad applicability of our method. **4)** In the typical practice of evaluating hallucination levels in LVLMs, multiple instructions are usually used to enhance the stability of the evaluation results. Although LeHaCE cannot be used with just one instruction, this limitation does not affect its ability to provide stable evaluations.

## 6 Acknowledgments

This work was supported in part by the National Key R&D Program of China (2021YFF0900500), and the National Natural Science Foundation of China (NSFC) under grants U22B2035 and 62441202.

## References

* Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023. doi: 10.48550/ARXIV.2302.13971. URL https://doi.org/10.48550/arXiv.2302.13971.
* Anil et al. [2020] Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov,

Figure 5: Average RSD of CHAIR with the LeHaCE framework at different lengths, lower is better. ABF refers to the average-based evaluation framework.

Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy P. Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul Ronald Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Eric Moreira, Kaereem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Kirkun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Beca Roelofs, Anis White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, and et al. Gemini: A family of highly capable multimodal models. _CoRR_, abs/2312.11805, 2023. doi: 10.48550/ARXIV.2312.11805. URL https://doi.org/10.48550/arXiv.2312.11805.
* [3] OpenAI. GPT-4 technical report. _CoRR_, abs/2303.08774, 2023. doi: 10.48550/ARXIV.2303.08774. URL https://doi.org/10.48550/arXiv.2303.08774.
* 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/9a6a435e75419a836fe47ab67936236e-Abstract-Conference.html.
* [5] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. _CoRR_, abs/2304.10592, 2023. doi: 10.48550/ARXIV.2304.10592. URL https://doi.org/10.48550/arXiv.2304.10592.
* [6] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _CoRR_, abs/2304.08485, 2023. doi: 10.48550/ARXIV.2304.08485. URL https://doi.org/10.48550/arXiv.2304.08485.
* [7] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. _CoRR_, abs/2308.12966, 2023. doi: 10.48550/ARXIV.2308.12966. URL https://doi.org/10.48550/arXiv.2308.12966.
* November 4, 2018_, pages 4035-4045. Association for Computational Linguistics, 2018. doi: 10.18653/V1/D18-1437. URL https://doi.org/10.18653/v1/d18-1437.
* [9] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023_, pages 292-305. Association for Computational Linguistics, 2023. URL https://aclanthology.org/2023.emnlp-main.20.
* [10] Holy Lovenia, Wenliang Dai, Samuel Cahyawijaya, Ziwei Ji, and Pascale Fung. Negative object presence evaluation (NOPE) to measure object hallucination in vision-language models. _CoRR_, abs/2310.05338, 2023. doi: 10.48550/ARXIV.2310.05338. URL https://doi.org/10.48550/arXiv.2310.05338.
* [11] Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Ming Yan, Ji Zhang, and Jitao Sang. An llvm-free multi-dimensional benchmark for mllms hallucination evaluation. _CoRR_, abs/2311.07397, 2023. doi: 10.48550/ARXIV.2311.07397. URL https://doi.org/10.48550/arXiv.2311.07397.
* [12] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating hallucination in large multi-modal models via robust instruction tuning. In _The Twelfth International Conference on Learning Representations_, 2023.

* Yin et al. [2023] Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong Chen. Woodpecker: Hallucination correction for multimodal large language models. _CoRR_, abs/2310.16045, 2023. doi: 10.48550/ARXIV.2310.16045. URL https://doi.org/10.48550/arXiv.2310.16045.
* Wang et al. [2023] Bin Wang, Fan Wu, Xiao Han, Jiahui Peng, Huaping Zhong, Pan Zhang, Xiaoyi Dong, Weijia Li, Wei Li, Jiaqi Wang, and Conghui He. VIGC: visual instruction generation and correction. _CoRR_, abs/2308.12714, 2023. doi: 10.48550/ARXIV.2308.12714. URL https://doi.org/10.48550/arXiv.2308.12714.
* Vinyals et al. [2015] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3156-3164, 2015.
* Pan et al. [2024] Wensheng Pan, Timin Gao, Yan Zhang, Xiawu Zheng, Yunhang Shen, Ke Li, Runze Hu, Yutao Liu, and Pingyang Dai. Semi-supervised blind image quality assessment through knowledge distillation and incremental learning. February 2024.
* Li et al. [2024] Xudong Li, Timin Gao, Xiawu Zheng, Runze Hu, Jingyuan Zheng, Yunhang Shen, Ke Li, Yutao Liu, Pingyang Dai, Yan Zhang, and Rongrong Ji. Adaptive feature selection for no-reference image quality assessment using contrastive mitigating semantic noise sensitivity. July 2024.
* Li et al. [2024] XuDong Li, Runze Hu, Jingyuan Zheng, Yan Zhang, Shengchuan Zhang, Xiawu Zheng, Ke Li, Yunhang Shen, Yutao Liu, Pingyang Dai, et al. Integrating global context contrast and local sensitivity for blind image quality assessment. July 2024.
* Qin et al. [2023] Guanyi Qin, Runze Hu, Yutao Liu, Xiawu Zheng, Haotian Liu, Xiu Li, and Yan Zhang. Data-efficient image quality assessment with attention-panel decoder. pages 2091-2100.
* Li et al. [2023] Wenrui Li, Xi-Le Zhao, Zhengyu Ma, Xingtao Wang, Xiaopeng Fan, and Yonghong Tian. Motion-decoupled spiking transformer for audio-visual zero-shot learning. In _Proceedings of the 31st ACM International Conference on Multimedia_, page 3994-4002, 2023. ISBN 9798400701085. doi: 10.1145/3581783.3611759. URL https://doi.org/10.1145/3581783.3611759.
* Li et al. [2024] Wenrui Li, Penghong Wang, Ruiqin Xiong, and Xiaopeng Fan. Spiking tucker fusion transformer for audio-visual zero-shot learning. _IEEE Transactions on Image Processing_, 33:4840-4852, 2024. doi: 10.1109/TIP.2024.3430080.
* Li et al. [2023] Wenrui Li, Zhengyu Ma, Liang-Jian Deng, Xiaopeng Fan, and Yonghong Tian. Neuron-based spiking transmission and reasoning network for robust image-text retrieval. _IEEE Transactions on Circuits and Systems for Video Technology_, 33(7):3516-3528, 2023. doi: 10.1109/TCSVT.2022.3233042.
* Chen et al. [2024] Zhuangzhuang Chen, Zhuonan Lai, Jie Chen, and Jianqiang Li. Mind marginal non-crack regions: Clustering-inspired representation learning for crack segmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 12698-12708, June 2024.
* Dong et al. [2023] Cunhui Dong, Haichuan Ma, Zhuoyuan Li, Li Li, and Dong Liu. Temporal wavelet transform-based low-complexity perceptual quality enhancement of compressed video. _IEEE Transactions on Circuits and Systems for Video Technology_, 2023.
* Li et al. [2024] Zhuoyuan Li, Yao Li, Chuanbo Tang, Li Li, Dong Liu, and Feng Wu. Uniformly accelerated motion model for inter prediction. _arXiv preprint arXiv:2407.11541_, 2024.
* Tang et al. [2024] Chuanbo Tang, Xihua Sheng, Zhuoyuan Li, Haotian Zhang, Li Li, and Dong Liu. Offline and online optical flow enhancement for deep video compression. _Proceedings of the AAAI Conference on Artificial Intelligence_, 38(6):5118-5126, Mar. 2024. doi: 10.1609/aaai.v38i6.28317. URL https://ojs.aaai.org/index.php/AAAI/article/view/28317.
* Chen et al. [2022] Zhuangzhuang Chen, Jin Zhang, Zhuonan Lai, Jie Chen, Zun Liu, and Jianqiang Li. Geometry-aware guided loss for deep crack recognition. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 4703-4712, June 2022.

* Antol et al. [2015] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In _Proceedings of the IEEE international conference on computer vision_, pages 2425-2433, 2015.
* December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177cccccb411a7d800-Abstract-Conference.html.
* Zhao et al. [2023] Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi Feng, and Bingyi Kang. Bubogpt: Enabling visual grounding in multi-modal lIms. _CoRR_, abs/2307.08581, 2023. doi: 10.48550/ARXIV.2307.08581. URL https://doi.org/10.48550/arXiv.2307.08581.
* Ye et al. [2023] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang. mplug-docowl: Modularized multimodal large language model for document understanding. _CoRR_, abs/2307.02499, 2023. doi: 10.48550/ARXIV.2307.02499. URL https://doi.org/10.48550/arXiv.2307.02499.
* Zhang et al. [2023] Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition. _CoRR_, abs/2309.15112, 2023. doi: 10.48550/ARXIV.2309.15112. URL https://doi.org/10.48550/arXiv.2309.15112.
* Peng et al. [2023] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. _CoRR_, abs/2306.14824, 2023. doi: 10.48550/ARXIV.2306.14824. URL https://doi.org/10.48550/arXiv.2306.14824.
* Chen et al. [2023] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal lIm's referential dialogue magic. _CoRR_, abs/2306.15195, 2023. doi: 10.48550/ARXIV.2306.15195. URL https://doi.org/10.48550/arXiv.2306.15195.
* Li et al. [2023] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In _Andreas Krause_, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of _Proceedings of Machine Learning Research_, pages 19730-19742. PMLR, 2023. URL https://proceedings.mlr.press/v202/li23q.html.
* Ye et al. [2023] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. _CoRR_, abs/2311.04257, 2023. doi: 10.48550/ARXIV.2311.04257. URL https://doi.org/10.48550/arXiv.2311.04257.
* Zhao et al. [2023] Bo Zhao, Boya Wu, and Tiejun Huang. SVIT: scaling up visual instruction tuning. _CoRR_, abs/2307.04087, 2023. doi: 10.48550/ARXIV.2307.04087. URL https://doi.org/10.48550/arXiv.2307.04087.
* Ye et al. [2023] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang. mplug-owl: Modularization empowers large language models with multimodality. _CoRR_, abs/2304.14178, 2023. doi: 10.48550/ARXIV.2304.14178. URL https://doi.org/10.48550/arXiv.2304.14178.

* Gao et al. [2024] Timin Gao, Peixian Chen, Mengdan Zhang, Chaoyou Fu, Yunhang Shen, Yan Zhang, Shengchuan Zhang, Xiawu Zheng, Xing Sun, Liujuan Cao, et al. Cantor: Inspiring multimodal chain-of-thought of mllm. October 2024.
* Anonymous [2024] Anonymous. Mitigating hallucination in large multi-modal models via robust instruction tuning. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=J44HFH4JCg.
* Gao et al. [2023] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu Qiao. Llama-adapter V2: parameter-efficient visual instruction model. CoRR, abs/2304.15010, 2023. doi: 10.48550/ARXIV.2304.15010. URL https://doi.org/10.48550/arXiv.2304.15010.
* Zhang et al. [2023] Ao Zhang, Hao Fei, Yuan Yao, Wei Ji, Li Li, Zhiyuan Liu, and Tat-Seng Chua. Transfer visual prompt generator across llms. CoRR, abs/2305.01278, 2023. doi: 10.48550/ARXIV.2305.01278. URL https://doi.org/10.48550/arXiv.2305.01278.
* Li et al. [2023] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning. CoRR, abs/2305.03726, 2023. doi: 10.48550/ARXIV.2305.03726. URL https://doi.org/10.48550/arXiv.2305.03726.
* Zeng et al. [2023] Yan Zeng, Hanbo Zhang, Jiani Zheng, Jiangnan Xia, Guoqiang Wei, Yang Wei, Yuchen Zhang, and Tao Kong. What matters in training a gpt4-style language model with multimodal inputs? CoRR, abs/2307.02469, 2023. doi: 10.48550/ARXIV.2307.02469. URL https://doi.org/10.48550/arXiv.2307.02469.
* ECCV 2014
- 13th European Conference, Zurich, Switzerland, September 6-12, 2014. Proceedings, Part V_, volume 8693 of _Lecture Notes in Computer Science_, pages 740-755. Springer, 2014. doi: 10.1007/978-3-319-10602-1_48. URL https://doi.org/10.1007/978-3-319-10602-1_48.
* November 2, 2019, pages 8947-8956. IEEE, 2019. doi: 10.1109/ICCV.2019.00904. URL https://doi.org/10.1109/ICCV.2019.00904.
* Dai et al. [2023] Wenliang Dai, Zihan Liu, Ziwei Ji, Dan Su, and Pascale Fung. Plausible may not be faithful: Probing object hallucination in vision-language pre-training. In Andreas Vlachos and Isabelle Augenstein, editors, _Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2023, Dubrovnik, Croatia, May 2-6, 2023_, pages 2128-2140. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EACL-MAIN.156. URL https://doi.org/10.18653/v1/2023.eacl-main.156.

Technical Appendices

### Instructions

We primarily referred to the instructions from [4] and additionally designed some instructions, totaling 25 in number.

* \(I_{1}\):'Describe the image in one sentence.',
* \(I_{2}\):'Summarize the image in a single sentence.',
* \(I_{3}\):'Give a one-sentence depiction of the image.',
* \(I_{4}\):'Provide a concise sentence describing the image.',
* \(I_{5}\):'Give a brief summary of the image in a single sentence.',
* \(I_{6}\):'Describe this image in short.',
* \(I_{7}\):'Describe this image in a few words.',
* \(I_{8}\):'Provide a brief caption for this image.',
* \(I_{9}\):'Provide a short caption for this image.',
* \(I_{10}\):'Briefly describe the content of the image.',
* \(I_{11}\):'Describe this image.',
* \(I_{12}\):'What does the image show?',
* \(I_{13}\):'What can you see in the image?',
* \(I_{14}\):'What is described in the image?',
* \(I_{15}\):'Provide a caption for this image.',
* \(I_{16}\):'Describe the objects in this image.',
* \(I_{17}\):'Can you provide a description of the image?',
* \(I_{18}\):'What objects or subjects are present in the image?',
* \(I_{19}\):'Describe this image in detail.',
* \(I_{20}\):'Describe this image in extremely detail.',
* \(I_{21}\):'Provide a detailed description of this image.',
* \(I_{22}\):'Can you describe the scene in the image in great detail?',
* \(I_{23}\):'Give a thorough account of what is depicted in this image.',
* \(I_{24}\):'Provide an elaborate and comprehensive analysis of this image.',
* \(I_{25}\):'Give a comprehensive and in-depth description of what is shown in this image.',

### Further Exploration

Why does the hallucination rate of MLLMs increase with the increase in image description length? The underlying reasons behind this phenomenon are difficult to determine, as the output of MLLMs is influenced by multiple factors such as visual encoders, language models, and training data. In this section, we aim to shed light on this phenomenon by delving into an analysis of common hallucination patterns in long image descriptions.

As shown in Figure 7, We found that hallucinations are more likely to occur after some words or phrases that indicate enumeration or introduce additional information, such as "in addition", "addition to", "additionally", "include", "including", "such as", "as well" and "also". We refer to these words/phrases as "hallucinogenic words". As shown in Figure 6 left, we performed a statistical analysis on a subset of MSCOCO dataset comprising 256 images to investigate the hallucination rate of image descriptions containing hallucinogenic words. The experimental results demonstrate a notable increase in the hallucination rate of image descriptions that incorporate hallucinogenic words, compared to descriptions that lack such words. This phenomenon was consistently observed across all twelve MLLMs.

[MISSING_PAGE_FAIL:16]

[MISSING_PAGE_FAIL:17]

Figure 7: Example of detailed image descriptions generated by beam search and beam search with out hallucinogenic words. The hallucination content is highlighted in red, and the hallucinogenic words are highlighted in green.

[MISSING_PAGE_FAIL:19]

[MISSING_PAGE_EMPTY:20]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: Yes Justification: The main claims made in the abstract and introduction accurately reflect the paper's contributions and scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: Yes Justification: We provide a detailed discussion of the limitations of our work in Section 5 Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: NAJustification: This paper does not include theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: Yes Justification: All experimental settings are detailed in Sections 3 & 4. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: Yes Justification: The code is included in the supplementary materials. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: Yes Justification: All experimental settings are detailed in Sections 3 & 4. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: Yes Justification: This paper reports the confidence intervals and statistical significance tests in Section 3. The assumptions used in our method are provided in Section 4 Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: Yes Justification: We detail the information on the computer resources in Section 4. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: Yes Justification: This research conforms with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: Yes Justification: We discuss the border impacts of our work in Appendix Section 7.5 Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: NA Justification: This paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: Yes Justification: CC-BY 4.0 Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: Yes Justification: New assets introduced in the paper are well documented. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: NA Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: NA Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.