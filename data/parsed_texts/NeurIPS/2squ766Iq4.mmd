# Towards Understanding Extrapolation: a Causal Lens

 Lingjing Kong\({}^{1}\)1 Guangyi Chen\({}^{1,2}\)1 Petar Stojanov\({}^{3}\) Haoxuan Li\({}^{2}\) Eric P. Xing\({}^{1,2}\)

Kun Zhang\({}^{1,2}\)

\({}^{1}\) Carnegie Mellon University

\({}^{2}\) Mohamed bin Zayed University of Artificial Intelligence

\({}^{3}\) Broad Institute of MIT and Harvard, Cancer Program, Eric and Wendy Schmidt Center

Footnote 1: Equal Contribution.

###### Abstract

Canonical work handling distribution shifts typically necessitates an entire target distribution that lands inside the training distribution. However, practical scenarios often involve only a handful of target samples, potentially lying outside the training support, which requires the capability of extrapolation. In this work, we aim to provide a theoretical understanding of when extrapolation is possible and offer principled methods to achieve it without requiring an on-support target distribution. To this end, we formulate the extrapolation problem with a latent-variable model that embodies the minimal change principle in causal mechanisms. Under this formulation, we cast the extrapolation problem into a latent-variable identification problem. We provide realistic conditions on shift properties and the estimation objectives that lead to identification even when only one off-support target sample is available, tackling the most challenging scenarios. Our theory reveals the intricate interplay between the underlying manifold's smoothness and the shift properties. We showcase how our theoretical results inform the design of practical adaptation algorithms. Through experiments on both synthetic and real-world data, we validate our theoretical findings and their practical implications.

## 1 Introduction

Extrapolation necessitates the capability of generalizing beyond the training distribution support, which is essential for the robust deployment of machine learning models in real-world scenarios. Specifically, given access to a source distribution \(\mathcal{D}_{\text{src}}:=p(\mathbf{x}_{\text{src}},\mathbf{y}_{\text{src}})\) with support \(\mathcal{X}_{\text{src}}:=\text{Supp}(p_{\text{src}}(\mathbf{x}))\) and one or a few out-of-support samples \(\mathbf{x}_{\text{tgt}}\notin\mathcal{X}_{\text{src}}\), the goal of extrapolation is to predict the target label \(\mathbf{y}_{\text{tgt}}\). For example, if the training distribution includes dog images, we aim to accurately classify dogs under unseen camera angles, lighting conditions, and backgrounds. While intuitive for humans, machine learning models can be brittle to minor distribution shifts [1; 2; 3; 4].

Addressing distribution shifts has garnered significant attention from the community. Unsupervised domain adaptation under covariate shifts addresses the shift of the marginal distribution \(p(\mathbf{x})\) across domains. However, canonical techniques such as importance sampling and re-weighting [5; 6; 7; 8; 9] are predicated on the assumptions of overlapping supports \(\text{Supp}(p_{\text{tgt}}(\mathbf{x}))\subset\text{Supp}(p_{\text{src}}( \mathbf{x}))\) and the availability of the entire target marginal distribution \(p_{\text{tgt}}(\mathbf{x})\). Similarly, domain generalization [10; 11; 12] assumes access to multiple source distributions \(p_{\text{src}}(\mathbf{x},\mathbf{y})\) whose supports jointly cover the target distribution. In addition to these methods, test-time adaptation (TTA) [13; 14; 15; 16] is particularly relevant to our discussion of extrapolation. TTA addresses out-of-distribution test samples at the individual sample level. Canonical methods include updating the source model with entropy-based or self-supervised losses on target samples. However, most TTA research focuses on empirical aspects, with limited theoretical formalization [17]. Most related to our work, Kong et al. [18] and Li et al.

[19] propose theoretical frameworks to characterize distribution shifts and explore conditions for identifying latent changing factors. However, these frameworks assume access to multiple source distributions with overlapping supports, which are not directly applicable to the extrapolation problem considered in this work, where we have potentially only one out-of-support target sample \(\mathbf{x}_{\mathrm{tgt}}\).

Since the target \(\mathbf{y}_{\mathrm{tgt}}\) can be arbitrarily out of the source support (Figure 0(a)), extrapolation is ill-posed without proper assumptions on the relationship between the source and the target. In this work, we formulate a latent-variable model that encodes a _minimal change principle_ to address this ill-posedness. Specifically, we assume that a latent variable \(\mathbf{z}\) determines \(\mathbf{x}\) such that \(\mathbf{x}:=g(\mathbf{z})\). The minimal change principle entails the following two assumptions on the generating process. 1) The out-of-support nature of \(\mathbf{x}_{\mathrm{tgt}}\) stems from only a _subspace_ of \(\mathbf{z}\), denoted as \(\mathbf{s}\), while the complement partition \(\mathbf{c}\) for the target sample \(\mathbf{x}_{\mathrm{tgt}}\) is within the training support, i.e., \(\mathbf{z}:=[\mathbf{c},\mathbf{s}]\), \(\mathbf{s}_{\mathrm{tgt}}\notin\mathcal{S}_{\mathrm{src}}\), and \(\mathbf{c}_{\mathrm{tgt}}\in\mathcal{C}_{\mathrm{src}}\). 2) The changing variable \(\mathbf{s}\) only controls non-semantic attributes in \(\mathbf{x}\) and thus doesn't influence the label \(\mathbf{y}\), i.e., \(\mathbf{y}:=g_{y}(\mathbf{c})\). This formulation attributes the seemingly complex shifts in the pixel space of \(\mathbf{x}\) to simple intrinsic changes (in the sense that this change involves only \(\mathbf{s}\)) in the latent space, allowing us to reason about the transfer via the invariant variable \(\mathbf{c}\). Under our formulation, extrapolation amounts to identifying invariant latent variables \(\mathbf{c}\), with which a model \(f:\mathbf{c}\mapsto\mathbf{y}\) trained on the labeled source dataset can be directly applied to the target sample.

In light of this formulation, we investigate the identifiability conditions of the invariant variable \(\mathbf{c}\). We propose two sets of conditions addressing two regimes of the influence from \(\mathbf{s}\) on \(\mathbf{x}\). We refer to the case where all dimensions \(\mathbf{x}_{i}\) (e.g., all pixels in an image) could be influenced by the changing variable \(\mathbf{s}\) as the dense influence and the case where only a limited subset of dimensions \(x_{i}\) is affected as the sparse influence. Specifically, our first condition (Dense Influence, Theorem 4.2) states that if \(\mathbf{s}\)'s influence is dense, then assuming that \(\mathbf{c}\) takes on only finite values, extrapolation requires the manifold associated with each value of \(\mathbf{c}\) (i.e., \(g(\mathbf{c},\cdot)\) over \(\mathbf{s}\)) to be adequately separable, and the target changing variable \(\mathbf{s}_{\mathrm{tgt}}\) to be close to the source support \(\mathcal{S}_{\mathrm{src}}\). Intuitively, if images from two classes (two values of \(\mathbf{c}\)) are sufficiently distinguishable, such as cats and dogs, we can still recognize the class of the target sample \(\mathbf{x}_{\mathrm{tgt}}\), even if it has undergone moderate unseen shifts on all pixels like camera angles and positions controlled by \(\mathbf{s}\) (Figure 0(b)). Our second condition (Sparse Influence, Theorem 4.4) states that if \(\mathbf{s}\)'s influence is sufficiently sparse, then extrapolation can occur regardless of the distance of \(\mathbf{s}_{\mathrm{tgt}}\) to the source support \(\mathcal{S}_{\mathrm{src}}\). Intuitively, we can recognize the class "cow" even if only the background is changed, regardless of the severity (Figure 0(c)).

Figure 1: **Illustration of extrapolation and our theoretical conditions. The horizontal axis represents the changing variable \(\mathbf{s}\), ranging from the source support to out-of-support regions. The vertical axis represents the observed data \(\mathbf{x}\) living on the manifolds indexed by different values of the invariant variable \(\mathbf{c}\). Figure (a) demonstrates that given a point out of support it is unclear which class manifolds it belongs to. Figure (b) illustrates the dense shift condition (Theorem 4.2) where \(\mathbf{s}\) potentially changes all pixels in the images, such as the camera angle in the example. In this case, we can identify the invariant variable \(\mathbf{c}\) under a moderate amount of shift until the shift becomes excessive. For instance, the back view of the cat in the figure could be confused with other animals. Figure (c) illustrates the sparse shift condition (Theorem 4.4) where \(\mathbf{s}\) influences a limited number of pixels, such as the background in the example. In contrast to the dense shift, we can identify \(\mathbf{c}\) under the sparse shift regardless of its severity. In the figure, there is no ambiguity of the class “cow” even though the background has changed to the moon.**

Our theory provides insights into the interaction between the underlying manifold's smoothness, the out-of-support distance, and the nature of the shift. We conduct synthetic data experiments to validate our theoretical results. Moreover, we discuss the relationship between our results and TTA approaches. In particular, we apply our theoretical insights to improve autoencoder-based MAE-TTT [20] and observe noticeable improvements. Additionally, we demonstrate that basic principles (sparsity constraints) from our framework can benefit the state-of-the-art TTA approach TeSLA [21]. Our empirical results not only show the practical viability of our theory but also pave the way for further advancements in the field.

In summary, our contributions are threefold:

* We formulate the extrapolation task as a latent-variable identification problem. Our latent-variable model encodes complex changes in observed variables to a partition of latent variables, allowing us to reason about transferability from the source to the target through latent variable identification.
* We provide identification guarantees for the proposed latent-variable model, including shifts of distinct properties (dense vs. sparse) and corresponding conditions on the generating process. Our theory provides an essential understanding of when latent-variable identification is possible without accessing an entire target distribution and assuming overlapping supports as in prior work [18; 19].
* Inspired by our theory, we propose to add a likelihood maximization term to autoencoder-based MAE-TTT [20] to facilitate the alignment between the target sample and the source distribution. In addition, we propose sparsity constraints to enhance the state-of-the-art TTA approach TeSLA [21]. We validate our proposals with empirical evidence.

## 2 Related Work

**Extrapolation.** Out-of-distribution generalization has attracted significant attention in recent years. Unlike our work, the bulk of the work is devoted to generalizing to target distributions on the same support as the source distribution [22; 23; 8]. Recent work [24; 25; 26; 27] investigates extrapolation in the form of compositional generalization by resorting to structured generating functions (e.g., additive, slot-wise). Another line of work [28; 29; 30] studies extrapolation in regression problems and does not consider the latent representation. Saengkyongam et al. [31] leverage a latent variable model and linear relations between the interventional variable and the latent variable to handle extrapolation. In this work, we formulate extrapolation as a latent variable identification problem. Unlike the semi-parametric conditions in prior work, our conditions do not constrain the form of the generating function and are more compatible with deep learning models and tasks.

**Latent-variable identification for transfer learning.** In the latent-variable model literature, one often assumes latent variables \(\mathbf{z}\) generate the observed data \(\mathbf{x}\) (e.g., images, text) through a generating function. However, the nonlinearity of deep learning models requires the generating function to be nonlinear, which has posed major technical difficulty in recovering the original latent variable [32]. To overcome this setback, a line of work [33; 34; 35; 36] assumes the availability of an auxiliary label \(\mathbf{u}\) for each sample \(\mathbf{x}\) and under different \(\mathbf{u}\) values, each component \(z_{i}\) of \(\mathbf{z}\) experiences sufficiently large shift in its distribution. Since this framework assumes all latent components' distributions vary over distributions indexed by \(\mathbf{u}\), it does not assume the existence of some shared, invariant information across distributions, which is often the case for transfer learning tasks. To address this issue, recent work [18; 19] introduce a partition of \(\mathbf{z}\) into an invariable variable \(\mathbf{c}\) and an changing variable \(\mathbf{s}\) (i.e., \(\mathbf{z}:=[\mathbf{c},\mathbf{s}]\)) such that \(\mathbf{c}\)'s distribution remains constant over distributions. They show both \(\mathbf{c}\) and \(\mathbf{s}\) can be identified and one can directly utilize the invariant variable \(\mathbf{c}\) for domain adaptation. However, their techniques crucially rely on the variability of the changing variable \(\mathbf{s}\), mandating the availability of multiple sufficiently disparate distributions (including the target) and their overlapping supports. These constraints make them unsuitable for the extrapolation problem. In comparison, our theoretical results give identification of the invariant variable \(\mathbf{c}\) (the on-support variable in the extrapolation context) with only one source distribution \(p_{\mathrm{src}}(\mathbf{x})\) and as few as one out-off support target sample \(\mathbf{x}_{\mathrm{tgt}}\) through mild assumptions on the generating function, directly tackling the extrapolation problem.

Please refer to Section A1 for more related work.

## 3 Extrapolation and Latent-Variable Identification

Given the labeled source distribution \(p(\mathbf{x}_{\text{src}},\mathbf{y}_{\text{src}})\), our goal is to make predictions on a target sample \(\mathbf{x}_{\text{tgt}}\) outside the source support (\(\mathbf{x}_{\text{tgt}}\not\in\mathcal{X}_{\text{src}}\)). While more target samples would provide better information about the distribution shift, in practice, we often have only a handful of samples to work with. Therefore, we focus on the challenging scenario where only one target sample \(\mathbf{x}_{\text{tgt}}\) is available.

Making reliable predictions on out-of-support samples \(\mathbf{x}_{\text{tgt}}\) is infeasible without additional structure. Real-world problems where humans successfully extrapolate often follow a minimal change principle: they involve sparse, non-semantic intrinsic shifts despite complex raw data changes. For example, a person who has only seen a cow on a pasture can recognize the same cow on a beach, even if the background pixels change significantly. Here, the cow corresponds to the part of the latent variable that remains within the support of the source data, which we call the invariant variable \(\mathbf{c}\) (\(\mathbf{c}_{\text{tgt}}\in\mathcal{C}_{\text{src}}\)), while the background change corresponds to the complement that drifts off the source support, which we call the changing variable \(\mathbf{s}\) (\(\mathbf{s}_{\text{tgt}}\notin\mathcal{S}_{\text{src}}\)). Clearly, extrapolation is impossible if the intrinsic shift is dense (i.e., all dimensions change, \(\mathbf{z}=\mathbf{s}\)) or semantic (i.e., \(\mathbf{y}\) is a function of \(\mathbf{s}\)). For instance, if the variable \(\mathbf{s}\) also alters the cow's appearance drastically, making it unrecognizable, extrapolation fails. We define the data-generating process to encapsulate this minimal change principle, as follows:

\[\begin{split}\mathbf{c}&\sim p(\mathbf{c}),\ \mathbf{s}\sim p(\mathbf{s}|\mathbf{c});\\ &\mathbf{x}=g(\mathbf{z}),\ \mathbf{y}=g_{\mathbf{y}}(\mathbf{c}). \end{split}\] (1)

In this process, the latent space \(\mathbf{z}\in\mathcal{Z}\subset\mathbb{R}^{d_{\mathbf{z}}}\) comprises two subspaces: the invariant variable \(\mathbf{c}\in\mathcal{C}\subset\mathbb{R}^{d_{\mathbf{z}}}\) and the changing variable \(\mathbf{s}\in\mathcal{S}\subset\mathbb{R}^{d_{\mathbf{z}}}\). We define \(\mathcal{Z}:=\mathcal{Z}_{\text{src}}\cup\{\mathbf{z}_{\text{tgt}}\}\) as the source support augmented with the target sample \(\mathbf{z}_{\text{tgt}}\) and similarly \(\mathcal{X}\) and \(\mathcal{S}\). The invariant variable \(\mathbf{c}\) encodes shared information between the source distribution \(p(\mathbf{x}_{\text{src}})\) and the out-of-support target sample \(\mathbf{x}_{\text{tgt}}\), while the changing variable \(\mathbf{s}\) describes the shift from the source support \(\mathcal{X}_{\text{src}}\). Hence, \(\mathbf{c}_{\text{tgt}}\in\mathcal{C}_{\text{src}}\) and \(\mathbf{s}_{\text{tgt}}\notin\mathcal{S}_{\text{src}}\). The variables \(\mathbf{z}:=[\mathbf{c},\mathbf{s}]\) jointly generate the observed variable \(\mathbf{x}\in\mathcal{X}\subset\mathbb{R}^{d_{\mathbf{z}}}\) through an invertible generating function \(g:\mathbb{R}^{d_{\mathbf{z}}}\rightarrow\mathbb{R}^{d_{\mathbf{z}}}\). Furthermore, we assume that the label \(\mathbf{y}\) originates from the invariant variable \(\mathbf{c}\). This assumption reflects the reality that factors such as camera angles and lighting do not affect the object's class in an image.

Our latent-variable model adheres to the minimal change principle in two key ways: (1) the target sample's out-of-support nature arises from only a subset of latent variables \(\mathbf{s}\), and (2) these changing variables \(\mathbf{s}\) are non-semantic, thus not influencing the label \(\mathbf{y}\).

**Extrapolation and identifiability.** Under this framework, extrapolation is possible if we can identify the true invariant variable \(\mathbf{c}\) in both the source distribution \(p_{\text{src}}(\mathbf{x})\) and the target data \(\mathbf{x}_{\text{tgt}}\). This allows us to learn a classifier \(f_{\text{cls}}:\mathbf{c}\mapsto\mathbf{y}\) on the labeled source distribution \(p_{\text{src}}(\mathbf{x},\mathbf{y})\). Since the target sample's invariant variable falls within the source support (\(\mathbf{c}_{\text{tgt}}\in\mathcal{C}_{\text{src}}\)), this classifier \(f_{\text{cls}}\) can be directly applied to the target sample \(\mathbf{c}_{\text{tgt}}\). Thus, the task of extrapolation reduces to identifying the invariant variable \(\mathbf{c}\) in both the source distribution \(p(\mathbf{x}_{\text{src}})\) and the target sample \(\mathbf{x}_{\text{tgt}}\). In Section 4, we explore the conditions for identifying the invariant variable \(\mathbf{c}\).

Given the above reasoning, we define identifiability in Definition 3.1 (i.e., block-wise identifiability [37; 24]) which suffices for extrapolation.

**Definition 3.1** (Identifiability of the Invariant Variable \(\mathbf{c}\)).: For any \(\mathbf{x}_{1}\) and \(\mathbf{x}_{2}\), their true invariant variables \(\mathbf{c}_{1}\), \(\mathbf{c}_{2}\) are equal if and only if the estimates \(\hat{\mathbf{c}}_{1}\), \(\hat{\mathbf{c}}_{2}\) are equal: \(\mathbf{c}_{1}=\mathbf{c}_{2}\iff\hat{\mathbf{c}}_{1}=\hat{\mathbf{c}}_{2}\).

## 4 Identification Guarantees for Extrapolation

In this section, we provide two sets of conditions on which one can identify the invariant variable \(\mathbf{c}\) and discuss the intuition and implications.

As discussed in Section 3, we need to identify the target sample \(\mathbf{x}_{\text{tgt}}\) with source samples \(\mathbf{x}_{\text{src}}\) that share the same invariant variable values with the target sample, i.e., \(\mathbf{c}_{\text{src}}=\mathbf{c}_{\text{tgt}}\). This enables us to obtain the label of \(\mathbf{x}_{\text{tgt}}\) by assigning the label of such \(\mathbf{x}_{\text{src}}\). The shift between the source distribution

Figure 2: **The data-generating process. The invariant latent variable \(c\) and the changing latent variable \(s\) jointly generate the observed variable \(x\). The dashed line indicates potential statistical dependence.**\(p(\mathbf{x}_{\text{src}})\) and the target sample \(\mathbf{x}_{\text{tgt}}\) originates from the out-of-support nature of the changing variable \(\mathbf{s}_{\text{tgt}}\), i.e., \(\mathbf{s}_{\text{tgt}}\notin\mathcal{S}_{\text{src}}\), it is crucial to impose proper assumptions on the influence of \(\mathbf{s}\) on \(\mathbf{x}\) so that \(\mathbf{x}\) retains sufficient footprints of \(\mathbf{c}\) for identification beyond the source support.

We denote the Jacobian matrix of the generating function \(g\) as \(\mathbf{J}_{g}(\mathbf{z})\) and \(\mathbf{x}\)'s dimensions under the influence of \(\mathbf{s}\) as \(\mathcal{I}_{\mathbf{s}}(\mathbf{z}):=\{i\in[d_{\mathbf{x}}]:\exists j\in\{d_ {\mathbf{c}}+1,\dots,d_{\mathbf{z}}\},\text{s.t.},\,[\mathbf{J}_{g}(\mathbf{z} )]_{i,j}\neq 0\}\). We note that the set \(\mathcal{I}_{\mathbf{s}}(\mathbf{z})\) is a function of \(\mathbf{z}\), since the influenced dimensions may vary over \(\mathbf{z}\). Intuitively, if \(\mathbf{s}\) influences \(\mathbf{x}\) in a dense manner, i.e., large \(|\mathcal{I}_{\mathbf{s}}(\mathbf{z})|\) for potentially all dimensions \(x_{i}\), there may not be dimensions of \(\mathbf{x}\) serving as clear signatures of \(\mathbf{c}\), thereby increasing the difficulty of identify \(\mathbf{c}\). Additionally, the degree to which the changing variable \(\mathbf{s}\) is out-of-support plays a critical role - the further the target changing variable- the further the target changing variable \(\mathbf{s}_{\text{tgt}}\) deviates from the source distribution support \(\mathcal{S}_{\text{src}}\), the more severe and unpredictable the shift becomes, making it harder to retrieve \(\mathbf{c}\). In the following, we formalize conditions on the influence of \(\mathbf{s}\) from these two perspectives, revealing an interesting trade-off and interaction between these factors.

**Notations.** The true generating process involves \(\mathbf{c}\), \(\mathbf{s}\), distributions \(p\), and \(g\) (Equation 1), we define their statistical estimates with \(\hat{\mathbf{c}}\), \(\hat{\mathbf{s}}\), \(\hat{p}\), and \(\hat{g}\) through the objectives we will introduce.2 We assume that the estimation process respects the conditions of the corresponding true-generating process.

Footnote 2: We slightly abuse the notation \(p\) to denote density functions for continuous variables or delta functions for discrete variables.

### Dense-shift Conditions

We begin by investigating scenarios where there are no constraints on the number of dimensions of \(\mathbf{x}\) (i.e., the number of pixels) influenced by the changing variable \(\mathbf{s}\), i.e., potentially large \(|\mathcal{I}_{\mathbf{s}}(\mathbf{z})|\), which we term as dense shifts. For images, these shifts encompass global transformations such as changes in camera angles and lighting conditions that could potentially affect all pixel values (Figure 0(b)).

**Understanding the problem.** As dense shifts could influence all the dimensions of \(\mathbf{x}\), every dimension could be out of the source support and there might not be dimensions of \(\mathbf{x}\) that solely contain the information of \(\mathbf{c}\). Consequently, relying on any subset of \(\mathbf{x}\) dimensions to infer the original \(\mathbf{c}\) becomes untenable. For instance, consider a scenario where the source distribution contains frontal-view images of a cat, while the target sample portrays the same cat from a side view (Figure 0(b)). The model cannot recognize these two images as the same cat (the same \(\mathbf{c}\)) by matching a specific part of the side view, say the cat's nose, to samples in the source distribution because this cat's nose only shows up as a front view and can be vastly different in terms of the pixel region and values. The model cannot match specific features such as the cat's nose, between the side-view target and the source distribution, as the pixel region and values for the nose drastically differ.

**Our approach.** For the reasons above, we need to constrain such dense changes so that even when all dimensions are affected, the target sample adheres to some intrinsic structure determined by the underlying \(\mathbf{c}_{\text{tgt}}\) and remains distinguishable from samples of \(\mathbf{c}\neq\mathbf{c}_{\text{tgt}}\). In many real-world distributions, we can interpret \(\mathbf{c}\) as the embedding vector of classes or other categories, with each \(\mathbf{c}\) value indexing a manifold \(g(\mathbf{c},\cdot)\) over \(\mathbf{s}\). If manifolds are smooth and sufficiently separable from each other, they should exhibit limited variations in the adjacent region to the training support, avoiding confusion between distinct categories. For example, there exists a noticeable distinction between cats and lions, such that moderate illumination changes would not cause confusion until illumination significantly obscures distinguishing features. In the following, we formalize these structures by assuming a finite cardinality of \(\mathbf{c}\) and constraining the distance of \(\mathbf{s}_{\text{tgt}}\) to the support \(\mathcal{S}_{\text{src}}\).

**Additional notations.** We denote with \(J_{u}\) an upper bound of the Jacobian spectrum norm: \(\|\mathbf{J}_{g}(\mathbf{z})\|\leq J_{u}\) on the support. In Appendix A.2, we show \(J_{u}<\infty\) due to Assumption 4.1-i and Assumption 4.1-ii. We denote with \(D(\mathbf{c}_{1},\mathbf{c}_{2})\) the \(\ell_{2}\) distance between two manifolds on the support boundary: \(D(\mathbf{c}_{1},\mathbf{c}_{2}):=\inf_{\mathbf{s}_{1},\mathbf{s}_{2}\in \text{Bd}(\mathcal{S}_{\text{src}})}\|g(\mathbf{c}_{1},\mathbf{s}_{1})-g( \mathbf{c}_{2},\mathbf{s}_{2})\|\), where we denote the boundary of source support with \(\text{Bd}(\mathcal{S}_{\text{src}})\). We denote with \(D(\mathbf{s},\mathcal{S}_{\text{src}})\) the minimal \(\ell_{2}\) distance between \(\mathbf{s}\) and the source support \(\mathcal{S}_{\text{src}}\), i.e., \(D(\mathbf{s},\mathcal{S}_{\text{src}}):=\inf_{\mathbf{s}_{\text{src}}\in \mathcal{S}_{\text{src}}}\|\mathbf{s}-\mathbf{s}_{\text{src}}\|\).

**Assumption 4.1** (Identification Conditions under Global Shifts).:
1. _[_Smoothness & Invertibility]:_ _The generating function_ \(g\) _in Equation_ 1 _is a smooth invertible function with a smooth inverse everywhere._
2. _[Compactness]:_ _The source data space_ \(\mathcal{X}_{\text{src}}\subset\mathbb{R}^{d_{x}}\) _is closed and bounded.__;_
3. _[Discreteness]: The invariant variable_ \(\mathbf{c}\) _takes on values from a finite set:_ \(\mathcal{C}=\{\mathbf{c}_{k}\}_{k\in[K]}\)_._
4. _[Continuity]: The probability density function_ \(p(\mathbf{s}|\mathbf{c})\) _is continuous over_ \(\mathbf{s}\in\mathcal{S}_{\mathrm{src}}\)_, for all_ \(\mathbf{c}\in\mathcal{C}\)_._
5. _[Out-of-support Distance]: The target sample's out-support components_ \(\mathbf{s}_{\mathrm{tgt}}\)_'s distance from the source support_ \(\mathcal{S}_{\mathrm{src}}\) _is constrained:_ \(\inf_{\mathbf{s}\in\mathcal{S}_{\mathrm{src}}}\|\mathbf{s}_{\mathrm{tgt}}- \mathbf{s}\|\leq\frac{\min_{\mathbf{c}\in\mathcal{C}}\left(\epsilon_{\mathrm{tgt}} \right)\,D(\mathbf{c}_{\mathrm{tgt}},\mathbf{c})}{2J_{\mathrm{u}}}\)_._

**Discussion on the conditions.** As discussed above, the main conditions revolve two key factors: the discrete structure of the invariant distribution of \(p(\mathbf{c})\) in Assumption 4.1-iii and the off-support distance of the changing variable \(\mathbf{s}\) in Assumption 4.1-v. The discrete structure of \(p(\mathbf{c})\) is applicable to many real-world scenarios, especially classification tasks where the semantic invariant information often manifests as discrete class labels or other categorical distinctions. While this assumption is typically valid, it can be extended to encompass continuous dimensions in the invariant variable \(\mathbf{c}:=[\mathbf{c}_{\mathrm{c}},\mathbf{c}_{\mathrm{d}}]\) where \(\mathbf{c}_{\mathrm{c}}\) and \(\mathbf{c}_{\mathrm{d}}\) stand for the continuous and discrete dimensions, respectively. In such cases, we can group the continuous dimensions \(\mathbf{c}_{\mathrm{c}}\) with the changing variable \(\mathbf{s}\) and the same proof would give rise to the identification of the discrete part \(\mathbf{c}_{\mathrm{d}}\), which suffices for classification tasks. The off-support distance condition involves the smoothness of the generating function \(g\), where a smoother generating function allows more leeway for the target changing variable \(\mathbf{s}_{\mathrm{tgt}}\) to deviate. When \(\mathbf{s}\) controls the camera angle, one may be able to recognize a slightly sided view of cats after seeing front views in the source until the \(\mathbf{s}\) deviates too far and all images become back views, potentially leading to confusion with other animals (Figure 0(b)). Assumption 4.1-i ensures that the generating process preserves the latent information, which is widely adopted in the literature [18; 19; 35; 36; 33; 38]. Specifically, this guarantees that manifolds indexed by distinct values of \(\mathbf{c}\) are separate from each other, maintaining strictly positive distances between them. Assumption 4.1-ii,iv are technical conditions mirroring realities that pixels values are bounded and the changing variable s often represent attributes that vary gradually across its support (e.g., lighting and angles).

**Theorem 4.2** (Extrapolation under Dense Shifts).: _Assuming a generating process in Equation 1, we estimate the distribution with model \((\hat{g},\hat{p}(\hat{\mathbf{c}}),\hat{p}(\hat{\mathbf{s}}))\) with the objective:_

\[\sup\hat{p}(\hat{\mathbf{c}}_{\mathrm{tgt}}),\quad\text{Subject to: }\hat{p}( \mathbf{x})=p(\mathbf{x}),\;\forall\mathbf{x}\in\mathcal{X}_{\mathrm{src}}; \quad\hat{\mathbf{s}}_{\mathrm{tgt}}\in\operatorname*{arg\,inf}_{\hat{ \mathbf{s}}}D(\hat{\mathbf{s}},\hat{\mathcal{S}}_{\mathrm{src}}).\] (2)

_Under Assumption 4.1, the estimated model can attain the identifiability in Definition 3.1._

**Proof sketch.** We estimate the generative process through maximum likelihood estimation on the source distribution \(\hat{p}(\hat{\mathbf{x}})=p(\mathbf{x})\). Under Assumption 4.1-ii,iii,iv, we can establish the identification of \(\mathbf{c}\) on the _support_[38], i.e., \(\hat{\mathbf{c}}_{1}=\hat{\mathbf{c}}_{2}\iff\mathbf{c}_{1}=\mathbf{c}_{2}\), for \(\mathbf{x}_{1},\mathbf{x}_{2}\in\mathcal{X}_{\mathrm{src}}\). This implies that all samples \(\mathbf{x}_{\mathrm{src}}\) on a given source manifold \(g(\mathbf{c},\cdot)\) share identical values of \(\hat{\mathbf{c}}\). In the objective, we maximize the likelihood \(\hat{p}(\hat{\mathbf{c}}_{\mathrm{tgt}})\) to drive \(\hat{\mathbf{c}}_{\mathrm{tgt}}\) to match one of the discrete values of \(\hat{\mathbf{c}}\) with nontrivial probability mass. Given the identification of \(\mathbf{c}\) on the support, this equates to assigning \(\mathbf{x}_{\mathrm{tgt}}\) to a manifold \(g(\mathbf{c}^{*},\cdot)\) with \(\mathbf{c}^{*}\in\mathcal{C}\). Our task now switches to ensuring that this is the correct manifold for \(\mathbf{x}_{\mathrm{tgt}}\), i.e., \(\mathbf{c}^{*}=\mathbf{c}_{\mathrm{tgt}}\). To accomplish this, we select the estimated model that uses the minimal off-support distance on \(\hat{\mathbf{s}}\) (i.e., \(D(\hat{\mathbf{s}},\hat{\mathcal{S}}_{\mathrm{src}})\)) to explain the off-support nature of \(\mathbf{x}_{\mathrm{tgt}}\). This also embodies the minimal change principle. This and Assumption 4.1-v guarantee that only the correct manifold (\(g(\mathbf{c}_{\mathrm{tgt}},\cdot)\)) can effective capture \(\mathbf{x}_{\mathrm{tgt}}\), thereby facilitating the desired identification. In practice, these constraints can be implemented through Lagrange multipliers. Full proof is given in Appendix A2.

### Sparse-shift Conditions

We now examine cases where the changing variable \(\mathbf{s}\) influences only a subset of dimensions of \(\mathbf{x}\), i.e., a limited \(|\mathcal{I}_{\mathbf{s}}(\mathbf{z})|\), which we refer to as sparse shifts. For image distributions, these shifts include local corruptions or background changes that do not alter foreground objects (Figure 0(c)).

**Additional notations.** We define the index set \(\mathcal{I}_{\mathbf{c}}(\mathbf{z})\) under the influence of \(\mathbf{c}\) and the indices under the the exclusive influence of \(\mathbf{c}\) as \(\mathcal{I}_{\mathbf{c}\setminus\mathbf{s}}(\mathbf{z}):=\mathcal{I}_{ \mathbf{c}}(\mathbf{z})\setminus\mathcal{I}_{\mathbf{s}}(\mathbf{z})\).

**Understanding the problem.** In contrast to the dense-shift scenario, here we have a non-trivial subset of dimensions \([\mathbf{x}]_{\mathcal{I}_{\mathbf{c}\setminus\mathbf{s}}(\mathbf{z})}\) that are unaffected by the changing variable \(\mathbf{s}\). Consequently, if these dimensions carry sufficient information about \(\mathbf{c}\), we can exploit them to directly recover the true \(\mathbf{c}\), regardless of the distance \([\mathbf{x}]_{\mathcal{I}_{\mathbf{s}}(\mathbf{z})}\) deviates from the support. In contrast, in the dense-shift scenario, we need to constrain the out-of-support distance of \(\mathbf{s}\) and assume the discreteness of \(\mathbf{c}\). Considera scenario where a fixed \(\mathbf{c}\) represents a specific cow and \(\mathbf{s}\) controls only the background. Despite the variation in the target background (e.g., desert or space), we can effectively match the cow in the target image to the correct source images (see Figure 0(c)). While this may seem intuitive for humans, it is nontrivial for machine learning models to automatically recognize the region \([\mathbf{x}]_{\mathcal{I}_{\mathbf{c},\mathbf{s}}(\mathbf{z})}\), especially given its potential variation across \(\mathbf{z}\).

**Our approach.** For image classification, \([\mathbf{x}]_{\mathcal{I}_{\mathbf{c},\mathbf{s}}(\mathbf{z})}\) corresponds to foreground objects (or a portion) unaffected under sparse changes induced by \(\mathbf{s}\) (e.g., background changes). Humans can recognize this region because the pixels within it are strongly correlated (e.g., cow features). This observation motivates us to formalize such dependence structures in natural data to enable automatic identification.

**Assumption 4.3** (Identification Conditions under Local Shifts).:
1. _[Smoothness & Invertibility]: The generating function_ \(g\) _in Equation_ 1 _is invertible and differentiable, and its inverse is also differentiable._
2. _[Invariant Variable Informativeness]: The dimensions under_ \(\mathbf{c}\)_'s exclusive influence is uniquely determined: for a fixed_ \(\mathbf{c}\in\mathcal{C}\)_,_ \([\mathbf{x}]_{\mathcal{I}_{\mathbf{c},\mathbf{s}}(\mathbf{c},\mathbf{s}_{1}) }\neq[\mathbf{x}]_{\mathcal{I}_{\mathbf{c},\mathbf{s}}(\mathbf{c}^{*},\mathbf{ s}_{2})}\) _for any_ \(\mathbf{c}^{*}\neq\mathbf{c}\)_,_ \(\mathbf{s}_{1}\in\mathcal{S}\)_, and_ \(\mathbf{s}_{2}\in\mathcal{S}\)_._
3. _[Sparse Influence]: At any_ \(\mathbf{z}\in\mathcal{Z}\)_, the changing variable_ \(\mathbf{s}\) _influences at most_ \(d_{\mathbf{s}}\) _dimensions of_ \(\mathbf{x}\)_, i.e.,_ \(|\mathcal{I}_{\mathbf{s}}(\mathbf{z})|\leq d_{\mathbf{s}}\)_. Alternatively, the two variables_ \(\mathbf{c}\) _and_ \(\mathbf{s}\) _do not intersect on their influenced dimensions_ \(\mathcal{I}_{\mathbf{c}}(\mathbf{z})\cap\mathcal{I}_{\mathbf{s}}(\mathbf{z})=\emptyset\)_._
4. _[Mechanistic Dependence]: For all_ \(\mathbf{z}\)_, any nontrivial partition_ \(\mathcal{P}_{1},\mathcal{P}_{2}\) _of the dimensions_ \(\mathcal{I}_{\mathbf{c}\setminus\mathbf{s}}(\mathbf{z})\) _yields dependence between the sub-matrices of the Jacobian_ \(\mathbf{J}_{g}(\mathbf{z})\)_: rank_\(([\mathbf{J}_{g}(\mathbf{z})]_{\mathcal{I}_{\mathbf{c},\mathbf{s}} (\mathbf{z})})<\text{rank}([\mathbf{J}_{g}(\mathbf{z})]_{\mathcal{P}_{1}}( \mathbf{z}))+\text{rank}([\mathbf{J}_{g}(\mathbf{z})]_{\mathcal{P}_{2}}( \mathbf{z}))\)_._

**Discussion on the conditions.** Assumption 4.3-iii stipulates that the influence of \(\mathbf{s}\) is sparse, either in terms of dimension counts or in its intersection with the influence from \(\mathbf{c}\). It is noteworthy that while the influence is sparse, its location can vary over images, as indicated by the dependence of \(\mathcal{I}_{\mathbf{s}}\) on \(\mathbf{z}\). Consequently, it can capture diverse image corruptions and background changes. Assumption 4.3-ii ensures that \([\mathbf{x}]_{\mathcal{I}_{\mathbf{c},\mathbf{s}}}\) is sufficiently informative about \(\mathbf{c}\). For instance, it precludes scenarios where a sparse corruption alters the top stroke of "7" to resemble "1", rendering the uncorrupted region fundamentally unidentifiable. Assumption 4.3-iv enforces the dependence alluded to in our previous discussion: the unaffected dimensions \([\mathbf{x}]_{\mathcal{I}_{\mathbf{c},\mathbf{s}}}\) exhibit mechanistic dependence across them, characterized by the Jacobian rank [39]. Thus, generating separate parts of an object necessitates more capacity than generating the entire object, as the dependence across the two parts can inform each other's generation. This inherent dependence enables the identification of the unaffected region.

**Theorem 4.4** (Extrapolation under Sparse Shifts).: _Assuming a generating process in Equation 1, we estimate the distribution with model \((\hat{g},\hat{p}(\hat{\mathbf{c}}),\hat{p}(\hat{\mathbf{s}}))\) with the objective:_

\[\sup\hat{p}(\hat{\mathbf{c}}_{\mathrm{tgt}}),\quad\text{Subject to:}\quad\hat{p}( \mathbf{x})=p(\mathbf{x}),\,\forall\mathbf{x}\in\mathcal{X}_{\mathrm{src}}.\] (3)

_Under Assumption 4.3, the estimated model can attain the identifiability in Definition 3.1._

**Proof sketch.** Maximizing the likelihood \(\hat{p}(\hat{\mathbf{c}}_{\mathrm{tgt}})\) assigns a value \(\hat{\mathbf{c}}^{*}\in\hat{\mathcal{C}}\) to \(\hat{\mathbf{c}}_{\mathrm{tgt}}\). Building on our motivation, we leverage mechanistic dependence (Assumption 4.3-iv) to identify the unaffected dimension indices \(\mathcal{I}_{\mathbf{c}\setminus\mathbf{s}}(\mathbf{z})\subset[d_{\mathbf{x}}]\) with our estimated model. In other words, we have \(\mathcal{I}_{\mathbf{c}\setminus\mathbf{s}}(\mathbf{z})=\hat{\mathcal{I}}_{ \mathbf{c}\setminus\mathbf{s}}(\hat{\mathbf{z}})\). Consequently, the unaffected dimensions in the estimated variable equal their counterparts in the true model: \([\mathbf{x}_{\mathrm{tgt}}]_{\mathcal{I}_{\mathbf{c}\setminus\mathbf{s}}( \hat{\mathbf{c}}^{*},\hat{\mathbf{s}}^{*})}=[\mathbf{x}_{\mathrm{tgt}}]_{ \mathcal{I}_{\mathbf{c}\setminus\mathbf{s}}(\mathbf{c}_{\mathrm{tgt}}, \mathbf{s}_{\mathrm{tgt}})}\). Furthermore, Assumption 4.3-ii stipulates that the dimensions in the target sample \([\mathbf{x}_{\mathrm{tgt}}]_{\mathcal{I}_{\mathbf{c}\setminus\mathbf{s}}( \mathbf{c}_{\mathrm{tgt}},\mathbf{s}_{\mathrm{tgt}})}\) cannot be attained by other \(\mathbf{c}\neq\mathbf{c}_{\mathrm{tgt}}\), so we have established that \(\hat{\mathbf{c}}^{*}\) corresponds to the correct value \(\mathbf{c}_{\mathrm{tgt}}\). Full proof is in Appendix A3.

It's worth noting that unlike the global shift case (Theorem 4.2), here we do not place a constraint on the out-of-support-ness of \(\mathbf{s}_{\mathrm{tgt}}\), a point we empirically verify in Section 6.3.

### Implications for Practical Algorithms

**Generative adaptation.** Our theoretical framework, inherently a generative model, can be implemented through auto-encoding over the source distribution and the target. Akin to our estimation framework, MAE-TTT [20] trains a masked auto-encoding model (\(f_{\mathrm{enc}}\) and \(f_{\mathrm{dec}}\)) on the source distribution and adapts to target samples through the auto-encoding objective. Consequently, we have \(f_{\mathrm{dec}}(f_{\mathrm{enc}}(\mathbf{x}))\approx\mathbf{x}\) for \(\mathbf{x}\in\mathcal{X}_{\mathrm{src}}\cup\{\mathbf{x}_{\mathrm{tgt}}\}\), which approximates the distribution-matching aspect of our estimation objectives Equation 2 and Equation 3.

Despite the resemblance on the reconstruction objective, MAE-TTT does not explicitly perform the representation alignment as our objectives - a classifier \(f_{\mathrm{cls}}\) is only trained on the labeled source distribution, which takes in \(f_{\mathrm{enc}}\)'s output \(\hat{\mathbf{z}}\) and produces logit values. In addition, our objectives entail maximizing the target likelihood \(\hat{p}(\hat{\mathbf{c}}_{\mathrm{tgt}})\) to align \(\hat{\mathbf{c}}_{\mathrm{tgt}}\) to the source support \(\hat{\mathcal{C}}_{\mathrm{src}}\). As large logit values indicate the sample is close to distribution modes [40; 41] and \(f_{\mathrm{enc}}\) is enforced invertible through auto-encoding, we can interpret minimizing the entropy of \(f_{\mathrm{cls}}(\hat{\mathbf{z}}_{\mathrm{tgt}})\) as filtering \(\hat{\mathbf{z}}_{\mathrm{tgt}}\) to obtain \(\hat{\mathbf{c}}_{\mathrm{tgt}}\) and driving it towards the modes of \(\hat{p}(\hat{\mathbf{c}})\). Therefore, we implement the entropy minimization loss \(\inf-\log\sum_{y}f_{\mathrm{cls}}(\hat{\mathbf{z}}_{\mathrm{tgt}})_{y}\log( f_{\mathrm{cls}}(\hat{\mathbf{z}}_{\mathrm{tgt}})_{y})\) as a surrogate for maximizing \(p(\hat{\mathbf{c}})\). We show that this significantly boosts the performance of MAE-TTT in Section 6.1.

**Regularization.** While our objectives simultaneously involve the source distribution and the target sample, the source distribution may not be accessible during adaptation. Aggressive updates on the target sample may distort the source information stored in the model and ultimately impair the performance. To address this, we propose to impose regularization on the source-pretrained backbone during adaptation to enforce minimal changes and preserve the source information. In Section 6.2, we instantiate this with low-rank updates and sparsity constraints, showcasing the resultant benefits.

## 5 Synthetic Data Experiments

In this section, we conduct synthetic data experiments on classification to directly validate the theoretical results in Section 4. We present additional experiments on regression in Section A4.2.

**Experimental setup.** We generated the synthetic data following the generative process in Equation 1, with \(d_{\mathbf{c}}=4\) and \(d_{\mathbf{s}}=2\). We focus on binary classification and sample class embeddings \(\mathbf{c}_{1}\) and \(\mathbf{c}_{2}\) from \(\mathcal{N}(0,\mathbf{I}_{c})\) and \(\mathcal{N}(2,\mathbf{I}_{c})\) respectively. We sample \(\mathbf{s}_{\mathrm{src}}\) from a truncated Gaussian centered at the origin and sample \(\mathbf{s}_{\mathrm{tgt}}\) at multiple distances from the origin. For the dense-shift case, we concatenate \(\mathbf{c}\) and \(\mathbf{s}\) and feed them to a well-conditioned 4-layer multi-layer perceptron (MLP) with ReLU activation to obtain \(\mathbf{x}\). For the sparse-shift case, we pass \(\mathbf{c}\) to a 4-layer MLP to obtain a 4-d vector. We duplicate \(2\) dimensions of this vector and add \(\mathbf{s}\) to it. The final \(\mathbf{x}\) is the concatenation of the \(4\)-d vector and the \(2\)-d vector. We sample \(10\)k points for the source distribution and 1 target sample for each run. We perform \(50\) runs for each configuration and compute the accuracy on the target samples. More details can be found in Appendix A4.

**Results and discussions.** We compared our method with iMSDA [18] and a model trained only on source data. The results in both dense and sparse shift settings are summarized in Table 1. Our method consistently outperforms both baseline methods (nearly random guesses) by a large margin on all sub-settings, validating our theoretical results. The results on iMSDA suggest that directly applying domain-adaptation methods to the extrapolation task may result in negative effects for lack of the target distribution in their training.

## 6 Real-world Data Experiments

We provide real-world experiments to validate our theoretical insights for practical algorithms (Section 4.3) and theoretical results (Section 4.2). More results can be found in Appendix A5. 3

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline
**Shifts** & \multicolumn{3}{c}{**Dense**} & \multicolumn{3}{c}{**Sparse**} \\ \cline{2-7}
**Distance** & 12.0 & 18.0 & 24.0 & 30.0 & 18.0 & 24.0 & 30.0 & 36.0 \\ \hline Only Source & \(0.59\) & \(0.55\) & \(0.45\) & \(0.45\) & \(0.54\) & \(0.54\) & \(0.56\) & \(0.52\) \\ iMSDA [18] & \(0.46\) & \(0.48\) & \(0.48\) & \(0.50\) & \(0.50\) & \(0.36\) & \(0.40\) & \(0.54\) \\
**Ours** & **0.78** & **0.69** & **0.72** & **0.72** & **0.72** & **0.72** & **0.76** & **0.70** \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Synthetic data test accuracy under both dense and sparse shifts across a range of distances.**

### Generative Adaptation with Entropy Minimization

As discussed in the first implication in Section 4.3, we incorporate an entropy-minimization loss to MAE-TTT and compare it with the original MAE-TTT.

**Experimental setup.** We conduct experiments on ImageNet-C [45] and ImageNet100-C [46] with 15 different types of corruption. For the baseline, we utilize the publicly available code of MAE-TTT. In our approach, we do not directly integrate the entropy-minimization loss into the MAE-TTT framework. This is because the training process of self-supervised MAE relies on masked images, whereas entropy-minimization requires the classification of the entire image. To address this, we introduce additional training steps with unmasked images and apply the entropy-minimization loss during these steps. Specifically, the training process for each test-time iteration is split into two stages. We first follow the MAE-TTT approach by inputting masked images and training the model using reconstruction loss. In this stage, only the encoder is updated. Then, we input full images (32 in a batch) and optimize the model with the entropy minimization loss following SHOT [43]. In this stage, both the encoder and classifier are optimized. The learning rates for both stages are set the same.

**Comparison with baselines.** In Table 3, we compare our method with the baseline MAE-TTT [20] and other baselines therein. We can observe that our algorithm largely boosts the performance of the MAE-TTT baseline over most corruption types. This corroborates our theoretical insights and showcases its practical value.

**Understanding entropy-minimization steps.** Table 4 presents the results of entropy-minimization with different training steps. The results indicate that the additional entropy-minimization steps significantly enhance the performance of the MAE-TTT framework, demonstrating the synergy between auto-encoding and entropy-minimization as indicated in our theoretical framework.

\begin{table}
\begin{tabular}{l c c c} \hline \hline \multicolumn{2}{c}{**Source**} & \multicolumn{1}{c}{**MAE-TTT**} & \multicolumn{2}{c}{**Using entropy-minimization**} \\ \cline{2-5} \cline{7-7} \multicolumn{1}{c}{} & \multicolumn{1}{c}{1 Step} & \multicolumn{1}{c}{2 Steps} & \multicolumn{1}{c}{3 Steps} \\ \hline
**Acc** & 50.29 & 59.12 \(\pm\) 0.35 & 63.99 \(\pm\) 0.25 & 65.09 \(\pm\) 0.23 & 65.01 \(\pm\) 0.39 \\ \hline \hline \end{tabular}
\end{table}
Table 4: **Understanding entropy-minimization steps on ImageNet100-C. Values are classification accuracy (mean and standard deviation) over three random seeds.**

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Method** & **CIFAR10-C** & **CIFAR100-C** & **ImageNet-C** \\ \hline Source [21] & 29.1 & 60.4 & 81.8 \\ BN [42] & 15.6 & 43.7 & 67.7 \\ TENT [15] & 14.1 & 39.0 & 57.4 \\ SHOT [43] & 13.9 & 39.2 & 68.7 \\ TTT++ [14] & 15.8 & 44.4 & 59.3 \\ TTAC [44] & 13.4 & 41.7 & 58.7 \\ \hline TeSLA-s [21] & 12.1 & 37.3 & 53.1 \\
**TeSLA-s+SC** & **11.7 \(\pm\) 0.01 \(\downarrow\)** & **37.0 \(\pm\) 0.06 \(\downarrow\)** & **50.9 \(\pm\) 0.15 \(\downarrow\)** \\ \hline TeSLA\({}^{*}\)[21] & \(12.5\pm 0.04\) & \(38.2\pm 0.03\) & \(55.0\pm 0.17\) \\
**TeSLA+SC** & **12.1 \(\pm\) 0.11 \(\downarrow\)** & **38.0 \(\pm\) 0.13 \(\downarrow\)** & **54.5 \(\pm\) 0.12 \(\downarrow\)** \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Comparison of SOTA TTA Methods on CIFAR10-C, CIFAR100-C, and ImageNet-C. Average error rates over 15 test corruptions are reported. Baseline results are from Tomar et al. [21]. Values are (means \(\pm\) standard deviations) over three random seeds.** \({}^{*}\) **indicates our reproductions.**

\begin{table}
\begin{tabular}{l|c c c c c c c c c c c c c c} \hline \hline Acc (\%) & bright & cont & defoc & elast & fog & frost & gauss & glass & impul & jpeg & mnton & pixel & shot & snow & zoom & Avg \\ \hline Joint Train & 62.3 & 4.5 & 26.7 & 39.9 & 25.7 & 30.0 & 5.8 & 16.3 & 5.8 & 45.3 & 30.9 & 45.9 & 7.1 & 25.1 & 31.8 & 26.88 \\ Fine-Tune & 67.5 & 7.8 & 33.9 & 32.4 & 36.4 & 38.2 & 22.0 & 15.7 & 23.9 & 51.2 & 37.4 & 51.9 & 23.7 & 37.6 & 37.1 & 34.45 \\ VIT Probe & 68.3 & 6.4 & 24.2 & 31.6 & 38.6 & 38.4 & 17.4 & 18.4 & 18.2 & 51.2 & 32.2 & 49.7 & 18.2 & 35.9 & 32.2 & 32.06 \\ TTT-MAE & 69.1 & 9.8 & **34.4** & 50.7 & 44.7 & 50.7 & 30.5 & 36.9 & 32.4 & 63.0 & **41.9** & 63.0 & 33.0 & 42.8 & **45.9** & 45.92 \\ \hline
**Ours** & **73.8** & **14.0** & 33.6 & **69.0** & **47.8** & **64.6** & **38.6** & **42.2** & **36.6** & **68.4** & 32.4 & **67.4** & **41.2** & **51.2** & 35.4 & **47.77** \\ \hline \hline \end{tabular}
\end{table}
Table 3: **Test accuracy (%) on ImageNet-C. The baseline results are from Gandelsman et al. [20]**

### Sparsity Regularization

As suggested by the second implication in Section 4.3, we integrate sparsity constraints into the state-of-the-art TTA method, TeSLA/TeSLA-s [21]. Although our theoretical results rely on a generative model, we demonstrate that our implications are also applicable to discriminative models.

**Experimental setup.** We conduct experiments on the CIFAR10-C, CIFAR100-C, and ImageNet-C datasets [45], following the protocols outlined for TeSLA and TeSLA-s [21], with and without training data information. In the pre-train stage, we apply the ResNet50 [47] as the backbone network and follow prior work [14; 44] to pre-train it on the clean CIFAR10, CIFAR100, and ImageNet training sets, with joint contrastive and classification losses. In the test-time adaptation process, we adopt the sequential TTA protocol as outlined in TTAC [44] and TeSLA [21]. This protocol prohibits the change of training objectives throughout the test phase. To encourage sparsity, we add low-rank adaptation (LoRA) modules [48] to the backbone network, which limits the adaptation to low intrinsic dimensions. Beyond LoRA, we further implement a masking layer with corresponding sparsity constraint (\(\ell_{1}\) loss) to filter out redundant changes. More details can be found in Appendix A5.

**Results analysis.** The average error rates under 15 corruption types for all CIFAR10-C, CIFAR100-C, and ImageNet-C datasets are summarized in Table 2. We can observe that sparsity constraints consistently improve performance over the current SOTA method, TeSLA/TeSLA-s, across all three datasets. The lightweight nature of the sparsity constraint and its consistent performance enhancements make it a valuable addition. This demonstrates the potential of sparsity constraints as a versatile, plug-and-play module for enhancing existing TTA methods.

### Shift Scope and Severity

To investigate the trade-off between the shift scope (dense vs. sparse) and severity, we simulate different levels of corruption severity and corrupted region sizes and evaluate a classical TTA method TENT [15] on these configurations. Following [45], we inject impulse noise to the CIFAR10 dataset, with noise levels ranging from 1 to 10 to simulate various severity levels. To control the shift's scope, we crop regions of various sizes and introduce corruption only to this region. Figure 3 displays classification error curves under various shift severity levels and region sizes. We can observe that classification errors rise with increasing noise levels and region sizes. Notably, for large block sizes (dense shifts), the performance dramatically declines and even collapses as the severity level rises, whereas the performance remains almost constant over all severity levels in the sparse shift regime, verifying the theoretical conditions for Theorem 4.2 and Theorem 4.4.

## 7 Conclusion and Limitations

In this work, we characterize extrapolation with a latent-variable model that encodes a minimal change principle. Within this framework, we establish clear conditions under which extrapolation becomes not only feasible but also guaranteed, even for complex nonlinear models in deep learning. Our conditions reveal the intricate interplay among the generating function's smoothness, the out-of-support degree, and the influence of the shift. These theoretical results provide valuable implications for the design of practical test time adaptation methods, which we validate empirically.

**Limitations**: On the theory aspect, the Jacobian norm utilized in Theorem 4.2 only considers the global smoothness of the generating function and thus may be too stringent if the function is much more well-behaved/smooth over the extrapolation region of concern. Therefore, one may consider a refined local condition to relax this condition. On the empirical side, our theoretical framework entails learning an explicit representation space. Existing methods without such a structure may still benefit from our framework but to a lesser extent. Also, our framework involves several loss terms including reconstruction, classification, and the likelihood of the target invariant variable. A careful re-weighting of these terms may be needed during training.

Figure 3: **TTA classification errors under different levels of shift severity levels and scopes.**

**Acknowledgments.** We thank the anonymous reviewers for their valuable insights and recommendations, which have greatly improved our work. The work of L. Kong is supported in part by NSF DMS-2134080 through an award to Y. Chi. This material is based upon work supported by NSF Award No. 2229881, AI Institute for Societal Decision Making (AI-SDM), the National Institutes of Health (NIH) under Contract R01HL159805, and grants from Salesforce, Apple Inc., Quris AI, and Florin Court Capital. P. Stojanov was supported in part by the National Cancer Institute (NCI) grant number: K99CA277583-01, and funding from the Eric and Wendy Schmidt Center at the Broad Institute of MIT and Harvard. This research has been graciously funded by the National Science Foundation (NSF) CNS2414087, NSF BCS2040381, NSF IIS2123952, NSF IIS1955532, NSF IIS2123952; NSF IIS2311990; the National Institutes of Health (NIH) R01GM140467; the National Geospatial Intelligence Agency (NGA) HM04762010002; the Semiconductor Research Corporation (SRC) AIHW award 2024AH3210; the National Institute of General Medical Sciences (NIGMS) R01GM140467; and the Defense Advanced Research Projects Agency (DARPA) ECOLE HR00112390063. Any opinions, findings, and conclusions or recommendations expressed in this publication are those of the author(s) and do not necessarily reflect the views of the National Science Foundation, the National Institutes of Health, the National Geospatial Intelligence Agency, the Semiconductor Research Corporation, the National Institute of General Medical Sciences, and the Defense Advanced Research Projects Agency.

## References

* [1] Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. Measuring robustness to natural distribution shifts in image classification. _Advances in Neural Information Processing Systems_, 33:18583-18599, 2020.
* [2] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In _International conference on machine learning_, pages 5389-5400. PMLR, 2019.
* [3] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In _International Conference on Machine Learning_, pages 5637-5664. PMLR, 2021.
* [4] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. _arXiv preprint arXiv:2007.01434_, 2020.
* [5] Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-likelihood function. _Journal of statistical planning and inference_, 90(2):227-244, 2000.
* [6] Jiayuan Huang, Arthur Gretton, Karsten Borgwardt, Bernhard Scholkopf, and Alex Smola. Correcting sample selection bias by unlabeled data. _Advances in Neural Information Processing Systems_, 19, 2006.
* [7] Masashi Sugiyama, Taiji Suzuki, Shinichi Nakajima, Hisashi Kashima, Paul Von Bunau, and Motoaki Kawanabe. Direct importance estimation for covariate shift adaptation. _Annals of the Institute of Statistical Mathematics_, 60:699-746, 2008.
* [8] Kun Zhang, Bernhard Scholkopf, Krikamol Muandet, and Zhikun Wang. Domain adaptation under target and conditional shift. In _International Conference on Machine Learning_, pages 819-827, 2013.
* [9] Petar Stojanov, Mingming Gong, Jaime Carbonell, and Kun Zhang. Low-dimensional density ratio estimation for covariate shift correction. In _The 22nd international conference on artificial intelligence and statistics_, pages 3449-3458. PMLR, 2019.
* [10] Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. The risks of invariant risk minimization. In _International Conference on Learning Representations_, volume 9, 2021.
* [11] Isabela Albuquerque, Joao Monteiro, Mohammad Darvishi, Tiago H Falk, and Ioannis Mitliagkas. Generalizing to unseen domains via distribution matching. _arXiv preprint arXiv:1911.00804_, 2019.

* [12] Gilles Blanchard, Aniket Anand Deshmukh, Urun Dogan, Gyemin Lee, and Clayton Scott. Domain generalization by marginal transfer learning. _Journal of machine learning research_, 22(2):1-55, 2021.
* [13] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In _International Conference on Machine Learning_, pages 9229-9248. PMLR, 2020.
* [14] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? _Advances in Neural Information Processing Systems_, 34, 2021.
* [15] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. _arXiv preprint arXiv:2006.10726_, 2020.
* [16] Paul Pu Liang, Terrance Liu, Liu Ziyin, Nicholas B Allen, Randy P Auerbach, David Brent, Ruslan Salakhutdinov, and Louis-Philippe Morency. Think locally, act globally: Federated learning with local and global representations. _arXiv preprint arXiv:2001.01523_, 2020.
* [17] Jian Liang, Ran He, and Tieniu Tan. A comprehensive survey on test-time adaptation under distribution shifts. _arXiv preprint arXiv:2303.15361_, 2023.
* [18] Lingjing Kong, Shaoan Xie, Weiran Yao, Yujia Zheng, Guangyi Chen, Petar Stojanov, Victor Akinwande, and Kun Zhang. Partial disentanglement for domain adaptation. In _International Conference on Machine Learning_, pages 11455-11472. PMLR, 2022.
* [19] Zijian Li, Ruichu Cai, Guangyi Chen, Boyang Sun, Zhifeng Hao, and Kun Zhang. Subspace identification for multi-source domain adaptation. _Advances in Neural Information Processing Systems_, 36, 2023.
* [20] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei Efros. Test-time training with masked autoencoders. _Advances in Neural Information Processing Systems_, 35:29374-29385, 2022.
* [21] Devavrat Tomar, Guillaume Vray, Behzad Bozorgtabar, and Jean-Philippe Thiran. Tesla: Test-time self-learning with automatic adversarial augmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 20341-20350, 2023.
* [22] Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. _Machine learning_, 79:151-175, 2010.
* [23] Masashi Sugiyama, Matthias Krauledat, and Klaus-Robert Muller. Covariate shift adaptation by importance weighted cross validation. _Journal of Machine Learning Research_, 8(5), 2007.
* [24] Sebastien Lachapelle, Divyat Mahajan, Ioannis Mitliagkas, and Simon Lacoste-Julien. Additive decoders for latent variables identification and cartesian-product extrapolation. _Advances in Neural Information Processing Systems_, 36, 2023.
* [25] Thaddaus Wiedemer, Prasanna Mayilvahanan, Matthias Bethge, and Wieland Brendel. Compositional generalization from first principles. _Advances in Neural Information Processing Systems_, 36, 2023.
* [26] Thaddaus Wiedemer, Jack Brady, Alexander Panfilov, Attila Juhos, Matthias Bethge, and Wieland Brendel. Provable compositional generalization for object-centric learning. _arXiv preprint arXiv:2310.05327_, 2023.
* [27] Linfeng Zhao, Lingzhi Kong, Robin Walters, and Lawson LS Wong. Toward compositional generalization in object-oriented world modeling. In _International Conference on Machine Learning_, pages 26841-26864. PMLR, 2022.
* [28] Aviv Netanyahu, Abhishek Gupta, Max Simchowitz, Kaiqing Zhang, and Pulkit Agrawal. Learning to extrapolate: A transductive approach. In _The Eleventh International Conference on Learning Representations_, 2022.

* Shen and Meinshausen [2023] Xinwei Shen and Nicolai Meinshausen. Engression: Extrapolation for nonlinear regression? In _2023 IMS International Conference on Statistics and Data Science (ICSDS)_, page 232, 2023.
* Dong and Ma [2022] Kefan Dong and Tengyu Ma. First steps toward understanding the extrapolation of nonlinear models to unseen domains. In _The Eleventh International Conference on Learning Representations_, 2022.
* Saengkyongam et al. [2023] Sorawit Saengkyongam, Elan Rosenfeld, Pradeep Kumar Ravikumar, Niklas Pfister, and Jonas Peters. Identifying representations for intervention extrapolation. In _The Twelfth International Conference on Learning Representations_, 2023.
* Hyvarinen and Oja [2000] Aapo Hyvarinen and Erkki Oja. Independent component analysis: algorithms and applications. _Neural networks_, 13(4-5):411-430, 2000.
* Khemakhem et al. [2020] Ilyes Khemakhem, Diederik Kingma, Ricardo Monti, and Aapo Hyvarinen. Variational autoencoders and nonlinear ica: A unifying framework. In _International Conference on Artificial Intelligence and Statistics_, pages 2207-2217. PMLR, 2020.
* Khemakhem et al. [2020] Ilyes Khemakhem, Ricardo Pio Monti, Diederik P. Kingma, and Aapo Hyvarinen. Ice-beem: Identifiable conditional energy-based deep models based on nonlinear ica, 2020.
* Hyvarinen and Morioka [2016] Aapo Hyvarinen and Hiroshi Morioka. Unsupervised feature extraction by time-contrastive learning and nonlinear ica. _Advances in neural information processing systems_, 29, 2016.
* Hyvarinen et al. [2019] Aapo Hyvarinen, Hiroaki Sasaki, and Richard Turner. Nonlinear ica using auxiliary variables and generalized contrastive learning. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 859-868. PMLR, 2019.
* Kugelgen et al. [2021] Julius von Kugelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard Scholkopf, Michel Besserve, and Francesco Locatello. Self-supervised learning with data augmentations provably isolates content from style, 2021.
* Kong et al. [2024] Lingjing Kong, Guangyi Chen, Biwei Huang, Eric P Xing, Yuejie Chi, and Kun Zhang. Learning discrete concepts in latent hierarchical models. _arXiv preprint arXiv:2406.00519_, 2024.
* Brady et al. [2023] Jack Brady, Roland S Zimmermann, Yash Sharma, Bernhard Scholkopf, Julius Von Kugelgen, and Wieland Brendel. Provably learning object-centric representations. In _International Conference on Machine Learning_, pages 3038-3062. PMLR, 2023.
* Shu et al. [2018] Rui Shu, Hung Bui, Hirokazu Narui, and Stefano Ermon. A dirt-t approach to unsupervised domain adaptation. In _International Conference on Learning Representations_, 2018.
* Grandvalet and Bengio [2004] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. _Advances in neural information processing systems_, 17, 2004.
* Nado et al. [2020] Zachary Nado, Shreyas Padhy, D Sculley, Alexander D'Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robustness under covariate shift. _arXiv preprint arXiv:2006.10963_, 2020.
* Liang et al. [2020] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In _International Conference on Machine Learning_, pages 6028-6039. PMLR, 2020.
* Su et al. [2022] Yongyi Su, Xun Xu, and Kui Jia. Revisiting realistic test-time training: Sequential inference and adaptation by anchored clustering. _Advances in Neural Information Processing Systems_, 35:17543-17555, 2022.
* Hendrycks and Dietterich [2019] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. _arXiv preprint arXiv:1903.12261_, 2019.
* Tian et al. [2020] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In _European conference on computer vision_, pages 776-794. Springer, 2020.

* [47] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [48] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In _International Conference on Learning Representations_, 2021.
* [49] Jun-Kun Wang and Andre Wibisono. Towards understanding gd with hard and conjugate pseudo-labels for test-time adaptation. In _The Eleventh International Conference on Learning Representations_, 2023.
* [50] Sachin Goyal, Mingjie Sun, Aditi Raghunathan, and J Zico Kolter. Test time adaptation via conjugate pseudo-labels. _Advances in Neural Information Processing Systems_, 35:6204-6218, 2022.
* [51] Hyesu Lim, Byeonggeun Kim, Jaegul Choo, and Sungha Choi. Ttn: A domain-shift aware batch normalization in test-time adaptation. In _The Eleventh International Conference on Learning Representations_, 2023.
* [52] Zehao Xiao, Xiantong Zhen, Ling Shao, and Cees GM Snoek. Learning to generalize across domains on single test samples. _arXiv preprint arXiv:2202.08045_, 2022.
* [53] Junha Song, Jungsoo Lee, In So Kweon, and Sungha Choi. Ecotta: Memory-efficient continual test-time adaptation via self-distilled regularization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11920-11929, 2023.
* [54] Mihir Prabhudesai, Tsung-Wei Ke, Alex Li, Deepak Pathak, and Katerina Fragkiadaki. Test-time adaptation of discriminative models via diffusion generative feedback. _Advances in Neural Information Processing Systems_, 36, 2023.
* [55] Xiaosong Ma, Jie Zhang, Song Guo, and Wenchao Xu. Swapprompt: Test-time prompt adaptation for vision-language models. _Advances in Neural Information Processing Systems_, 36, 2023.
* [56] Muhammad Jeanzeb Mirza, Pol Jane Soneira, Wei Lin, Mateusz Kozinski, Horst Possegger, and Horst Bischof. Actmad: Activation matching to align distributions for test-time-training. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 24152-24161, 2023.
* [57] Yushu Li, Xun Xu, Yongyi Su, and Kui Jia. On the robustness of open-world test-time training: Self-training with dynamic prototype expansion. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 11836-11846, 2023.
* [58] Shuai Wang, Daoan Zhang, Zipei Yan, Jianguo Zhang, and Rui Li. Feature alignment and uniformity for test time adaptation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 20050-20060, 2023.
* [59] Minguk Jang, Sae-Young Chung, and Hye Won Chung. Test-time adaptation via self-training with nearest neighbor information. _arXiv preprint arXiv:2207.10792_, 2022.
* [60] Liang Chen, Yong Zhang, Yibing Song, Ying Shan, and Lingqiao Liu. Improved test-time adaptation for domain generalization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 24172-24182, 2023.
* [61] Shuacheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test-time model adaptation without forgetting. In _International conference on machine learning_, pages 16888-16905. PMLR, 2022.
* [62] Bowen Zhao, Chen Chen, and Shu-Tao Xia. Delta: Degradation-free fully test-time adaptation. In _The Eleventh International Conference on Learning Representations_, 2023.
* [63] Shuacheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. In _The Eleventh International Conference on Learning Representations_, 2023.

* Iwasawa and Matsuo [2021] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for model-agnostic domain generalization. _Advances in Neural Information Processing Systems_, 34:2427-2440, 2021.
* Chen et al. [2022] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 295-305, 2022.
* Zhao et al. [2023] Hao Zhao, Yuejiang Liu, Alexandre Alahi, and Tao Lin. On pitfalls of test-time adaptation. In _International Conference on Machine Learning_, pages 42058-42080. PMLR, 2023.
* Kingma and Welling [2013] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_, 2013.
* Kingma and Ba [2014] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _International Conference on Learning Representations_, 2014.
* Krizhevsky et al. [2009] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* Chen et al. [2020] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. _arXiv preprint arXiv:2003.04297_, 2020.
* Yu et al. [2023] Yongcan Yu, Lijun Sheng, Ran He, and Jian Liang. Benchmarking test-time adaptation against distribution shifts in image classification. _arXiv preprint arXiv:2307.03133_, 2023.

_Appendix for_

**"Towards Understanding Extrapolation: a Causal Lens "**

Table of Contents

**A1 Related Work**

**A2 Proof for Theorem 4.2**

**A3 Proof for Theorem 4.4**

**A4 Synthetic Data Experiments**

A4.1 Implementation Details

A4.2 Regression Task Evaluation

A4.2.1 Implementation

A4.2.2 Results and Analysis

**A5 Real-world Data Experimental Details**

A5.1 Datasets

A5.2 Generative Adaptation with Entropy Minimization

A5.3 Sparsity Regularization

A5.4 Recoverability of the Invariant Variable

A5.5 Additional quantitative results.

Related Work

In this section, we discuss some related topics including extrapolation, latent-variable identification, and test-time adaptation.

**Extrapolation.** Out-of-distribution generalization has attracted significant attention in recent years. Unlike our work, the bulk of the work is devoted to generalizing to target distributions on the same support as the source distribution [22, 23, 8]. Recent work [24, 25, 26, 27] investigates extrapolation in the form of compositional generalization by resorting to structured generating functions (e.g., additive, slot-wise). Another line of work [28, 29, 30] studies extrapolation in regression problems and does not consider the latent representation. Saengkyongam et al. [31] leverage a latent variable model and assumes a linear relation between the intervention variable and the latent variable to handle extrapolation. In this work, we formulate extrapolation as a latent variable identification problem. Unlike the semi-parametric conditions in prior work, our conditions do not constrain the form of the generating function and are more compatible with deep learning models and tasks. We demonstrate that our conditions naturally lead to implications benefiting practical deep-learning algorithms.

**Latent-variable identification for transfer learning.** Identifying latent variables in a causal model has become one canonical paradigm to formalize and understand representation learning in the deep learning regime. Typically, one would assume some latent variables \(\mathbf{z}\) generate the observed data \(\mathbf{x}\) (e.g., images, text) through a generating function. However, the nonlinearity of deep learning models requires the generating function to be nonlinear, which has posed major technical difficulty in recovering the original latent variable [32]. To overcome this setback, a line of work [33, 34, 35, 36] assumes the availability of an auxiliary label \(\mathbf{u}\) for each sample \(\mathbf{x}\) and under different \(\mathbf{u}\) values, each component \(z_{i}\) of \(\mathbf{z}\) experiences sufficiently large shift in its distribution. This condition leads to component-wise identification of \(\mathbf{z}\), i.e., each estimate \(\hat{z}_{i}\) is equivalent to \(z_{\pi(i)}\) up to an invertible mapping for a permutation function \(\pi:[d_{z}]\rightarrow[d_{z}]\). Since this framework assumes all latent components' distributions vary over distributions indexed by \(\mathbf{u}\), it doesn't assume the existence of some shared, invariant information across distributions, which is often the case for transfer learning tasks. To address this issue, recent work [18, 19] introduce a partition of \(\mathbf{z}\) into an invariable variable \(\mathbf{c}\) and an changing variable \(\mathbf{s}\) (i.e., \(\mathbf{z}:=[\mathbf{c},\mathbf{s}]\)) such that \(\mathbf{c}\)'s distribution remains constant over distributions. They show both \(\mathbf{c}\) and \(\mathbf{s}\) can be identified and one can directly utilize the invariant variable \(\mathbf{c}\) for domain adaptation. However, their techniques crucially rely on the variability of the changing variable \(\mathbf{s}\), mandating the availability of multiple sufficiently disparate distributions (including the target) and their overlapping supports. These constraints make them unsuitable for the extrapolation problem. In comparison, our theoretical results give identification of the invariant variable \(\mathbf{c}\) (the on-support variable in the extrapolation context) with only one source distribution \(p_{\mathrm{src}}(\mathbf{x})\) and as few as one out-off support target sample \(\mathbf{x}_{\mathrm{tgt}}\) through mild assumptions on the generating function, which directly tackles the extrapolation problem.

**Test-time adaptation.** Test-time Adaptation (TTA) aims at adapting models trained on a source domain to align with the target domain during testing [49, 50, 51, 52, 53, 54, 55]. It is broadly classified based on whether the training objective is modified. Test-time Training (TTT) methods [13, 14, 56, 57, 56], including TTT [13] and TTT++ [14], proficiently adjust models to target domains by implementing similar self-supervised learning strategies on both training and testing data. In contrast, Sequential Test-Time Adaptation [58, 59, 60, 61, 54, 55] (sTTA) garners significant interest due to its practicality, notably its one-pass sequential inference and no training objective access. Research in sTTA primarily concentrates on two facets: the selection of model parameters for adaptation and the refinement of pseudo-labeling techniques for enhanced efficiency. For instance, TENT [15] fine-tunes the Batch Normalization (BN) layers by minimizing entropy, SHOT [16] adjusts the backbone network while maintaining a static classifier, and T3A [64] updates the classifier prototype. Moreover, a burgeoning line of research [65, 21, 44, 50, 15, 16] focuses on deriving more robust self-training signals through improved pseudo labeling strategies. For example, TTAC [44] employs clustering techniques to extract more accurate pseudo labels. Despite the prominent recent development, these algorithms tend to be brittle and sensitive to hyper-parameter tuning [66] and limited in theoretical understanding [17]. Our work offers formalization and understanding to fill in this gap. We show that insights inferred from our theory can indeed benefit existing TTA algorithms, which hopefully will serve as the first step to bridge the theory and practice for TTA algorithms.

Proof for Theorem 4.2

**Assumption 4.1** (Identification Conditions under Global Shifts).:
1. _[Smoothness & Invertibility]: The generating function_ \(g\) _in Equation_ 1 _is a smooth invertible function with a smooth inverse everywhere._
2. _[Compactness]: The source data space_ \(\mathcal{X}_{\mathrm{src}}\subset\mathbb{R}^{d_{x}}\) _is closed and bounded._
3. _[Discreteness]: The invariant variable_ \(\mathbf{c}\) _takes on values from a finite set:_ \(\mathcal{C}=\{\mathbf{c}_{k}\}_{k\in[K]}\)_._
4. _[Continuity]: The probability density function_ \(p(\mathbf{s}|\mathbf{c})\) _is continuous over_ \(\mathbf{s}\in\mathcal{S}_{\mathrm{src}}\) _for all_ \(\mathbf{c}\in\mathcal{C}\)_._
5. _[Out-of-support Distance]: The target sample's out-support components_ \(\mathbf{s}_{\mathrm{tgt}}\)_'s distance from the source support_ \(\mathcal{S}_{\mathrm{src}}\) _is constrained:_ \(\inf_{\mathbf{s}\in\mathcal{S}_{\mathrm{src}}}\|\mathbf{s}_{\mathrm{tgt}}- \mathbf{s}\|\leq\frac{\min_{\mathbf{c}\in\mathcal{C}}(\epsilon_{\mathrm{tgt}}) \,D(\mathbf{c}_{\mathrm{tgt}},\mathbf{c})}{2\,J_{u}}\)_._

We first present Lemma A1 from Kong et al. [38] which establishes the discrete information on the source support and serves as the starting point in the proof of Theorem 4.2.

**Lemma A1** (Source discrete subspace identification [38]).: _Assuming a generating process in Equation 1, we estimate the distribution with model \((\hat{g},\hat{p}(\hat{\mathbf{c}}),\hat{p}(\hat{\mathbf{s}}))\). Under Assumption 4.1 i,ii,iii,iv, it follows that the estimated variable \(\hat{\mathbf{c}}\) takes on values from \(\{\hat{\mathbf{c}}_{k}\}_{k=1}^{K}\) where each value corresponds uniquely to one value of the true variable \(\mathbf{c}\), i.e., \(\mathbf{c}=\mathbf{c}_{k}\iff\hat{\mathbf{c}}=\hat{\mathbf{c}}_{k}\)._

**Theorem 4.2** (Extrapolation under Dense Shifts).: _Assuming a generating process in Equation 1, we estimate the distribution with model \((\hat{g},\hat{p}(\hat{\mathbf{c}}),\hat{p}(\hat{\mathbf{s}}))\) with the objective:_

\[\sup\hat{p}(\hat{\mathbf{c}}_{\mathrm{tgt}}),\quad\text{Subject to: }\hat{p}( \mathbf{x})=p(\mathbf{x}),\,\forall\mathbf{x}\in\mathcal{X}_{\mathrm{src}}; \quad\hat{\mathbf{s}}_{\mathrm{tgt}}\in\operatorname*{arg\,inf}_{\hat{ \mathbf{s}}}D(\hat{\mathbf{s}},\hat{\mathcal{S}}_{\mathrm{src}}).\] (2)

_Under Assumption 4.1, the estimated model can attain the identifiability in Definition 3.1._

Proof for Theorem 4.2.: Lemma A1 shows that the discrete invariant variable \(\mathbf{c}\) is identifiable on the source distribution.

In the following, we show that the target's invariant variable \(\mathbf{c}_{\mathrm{tgt}}\) is identifiable if \(\mathbf{s}_{\mathrm{tgt}}\) does not drift too far away from the source support \(\mathcal{S}_{\mathrm{src}}\). Suppose that \(\mathbf{x}_{\mathrm{tgt}}\) resides on both manifolds \(g(\mathbf{c}_{k},\cdot)\) and \(g^{\prime}(\mathbf{c}_{k^{\prime}},\cdot)\) where \(k\neq k^{\prime}\). The generating function \(g^{\prime}\in\mathcal{G}\) belongs to the generating function class and behaves exactly the same as \(g\) on the source support, i.e., \(g^{\prime}=g\) over \(\mathcal{C}\times\mathcal{S}_{\mathrm{src}}\). We define the minimal distance \(D(\mathbf{c}_{k},\mathbf{c}_{k^{\prime}})\) between the two manifolds on support boundaries, i.e., \(D(\mathbf{c}_{k},\mathbf{c}_{k^{\prime}}):=\min_{\mathbf{s}_{1},\mathbf{s}_{2 }\in\text{Bd}(\mathcal{S}_{\mathrm{src}})}\|g(\mathbf{c}_{k},\mathbf{s}_{1} )-g(\mathbf{c}_{k^{\prime}},\mathbf{s}_{2})\|>0\). Since \(\mathbf{x}_{\mathrm{tgt}}\) lives on both manifolds \(g(\mathbf{c}_{k},\cdot)\) and \(g^{\prime}(\mathbf{c}_{k^{\prime}},\cdot)\), we can express it as \(\mathbf{x}_{\mathrm{tgt}}=g(\mathbf{c}_{k},\mathbf{s}_{\mathrm{tgt}})=g^{ \prime}(\mathbf{c}_{k^{\prime}},\mathbf{s}_{\mathrm{tgt}}^{\prime})\). We define \(\mathbf{s}_{\mathrm{src}}\in\operatorname*{arg\,min}_{\mathbf{s}\in\mathcal{S }_{\mathrm{src}}}\|\mathbf{s}-\mathbf{s}_{\mathrm{tgt}}\|\) and \(\mathbf{s}_{\mathrm{src}}^{\prime}\in\operatorname*{arg\,min}_{\mathbf{s}\in \mathcal{S}_{\mathrm{src}}}\|\mathbf{s}-\mathbf{s}_{\mathrm{tgt}}^{\prime}\|\) as two closest points on the source support to \(\mathbf{s}_{\mathrm{tgt}}\) and \(\mathbf{s}_{\mathrm{tgt}}^{\prime}\) respectively. It follows that

\[\begin{split}\mathbf{x}_{\mathrm{tgt}}-g(\mathbf{c}_{k}, \mathbf{s}_{\mathrm{src}})&=\left(\int_{0}^{1}\mathbf{J}_{g( \mathbf{c}_{k},\cdot)}(\mathbf{s}_{\mathrm{src}}+t\cdot\mathbf{h})dt \right)\mathbf{h};\\ \mathbf{x}_{\mathrm{tgt}}-g^{\prime}(\mathbf{c}_{k^{\prime}}, \mathbf{s}_{\mathrm{src}}^{\prime})&=\mathbf{x}_{\mathrm{tgt}}-g (\mathbf{c}_{k^{\prime}},\mathbf{s}_{\mathrm{src}}^{\prime})=\left(\int_{0}^ {1}\mathbf{J}_{g(\mathbf{c}_{k^{\prime}},\cdot)}(\mathbf{s}_{\mathrm{src}}^{ \prime}+t\cdot\mathbf{h}^{\prime})dt\right)\mathbf{h}^{\prime},\end{split}\] (4)

where \(\mathbf{h}:=\mathbf{s}_{\mathrm{tgt}}-\mathbf{s}_{\mathrm{src}}\) and \(\mathbf{h}^{\prime}:=\mathbf{s}_{\mathrm{tgt}}^{\prime}-\mathbf{s}_{\mathrm{ src}}^{\prime}\).

It follows from Equation 4

\[\begin{split}& g(\mathbf{c}_{k^{\prime}},\mathbf{s}^{\prime}_{ \mathrm{src}})-g(\mathbf{c}_{k},\mathbf{s}_{\mathrm{src}})=\left(\int_{0}^{1 }\mathbf{J}_{g(\mathbf{c}_{k^{\prime}},)}(\mathbf{s}_{\mathrm{src}}+t\cdot \mathbf{h})dt\right)\mathbf{h}-\left(\int_{0}^{1}\mathbf{J}_{g(\mathbf{c}_{k^{ \prime}},)}(\mathbf{s}^{\prime}_{\mathrm{src}}+t\cdot\mathbf{h}^{\prime})dt \right)\mathbf{h}^{\prime};\\ &\implies\\ &\left\|\left(\int_{0}^{1}\mathbf{J}_{g(\mathbf{c}_{k},\cdot)}( \mathbf{s}_{\mathrm{src}}+t\cdot\mathbf{h})dt\right)\mathbf{h}\right\|+\left\| \left(\int_{0}^{1}\mathbf{J}_{g(\mathbf{c}_{k^{\prime}},\cdot)}(\mathbf{s}^{ \prime}_{\mathrm{src}}+t\cdot\mathbf{h}^{\prime})dt\right)\mathbf{h}^{\prime }\right\|\geq D(\mathbf{c}_{k},\mathbf{c}_{k^{\prime}});\\ &\implies\\ J_{\mathrm{u}}(\left\|\mathbf{h}\right\|+\left\|\mathbf{h}^{\prime }\right\|)\geq D(\mathbf{c}_{k},\mathbf{c}_{k^{\prime}});\\ &\implies\\ &\max\{\left\|\mathbf{h}\right\|,\left\|\mathbf{h}^{\prime} \right\|\}\geq\frac{D(\mathbf{c}_{k},\mathbf{c}_{k^{\prime}})}{2J_{\mathrm{u} }}.\end{split}\] (5)

Assumption 4.1- v states that \(\left\|\mathbf{h}\right\|<\frac{D(\mathbf{c}_{k},\mathbf{c}_{k^{\prime}})}{2J_ {\mathrm{u}}}\) for the true generating function \(g\). Therefore, \(\mathbf{x}_{\mathrm{tgt}}\) can only be explained by one manifold, which we denote as \(g(\mathbf{c}_{\mathrm{tgt}},\cdot)\).

Finally, we show that the objective Equation 2 guarantees that the solution \(\hat{\mathbf{c}}_{\mathrm{tgt}}\) corresponds to the true \(\mathbf{c}_{\mathrm{tgt}}\). We suppose that \(\mathbf{c}_{\mathrm{tgt}}=\mathbf{c}_{k}\) which corresponds to \(\hat{\mathbf{c}}_{k}\) for a specific \(k\in[K]\). First, we note that \(\hat{\mathbf{c}}_{\mathrm{tgt}}\) could only take values from \(\{\hat{\mathbf{c}}_{k}\}_{k\in[K]}\) due to the constraint \(\sup\hat{p}(\hat{\mathbf{c}}_{\mathrm{d}})\). Also, the correct solution \(\hat{\mathbf{c}}_{k}\) is always a feasible solution to the objective Equation 2, since \(\hat{g}\) can take on the true generating function \(g\). Thus, for another plausible solution \(\hat{\mathbf{c}}_{k^{\prime}}\neq\hat{\mathbf{c}}_{k}\), we would have

\[\max\{\left\|\hat{\mathbf{h}}\right\|,\left\|\hat{\mathbf{h}}^{\prime} \right\|\}\geq\frac{D(\hat{\mathbf{c}}_{k},\hat{\mathbf{c}}_{k^{\prime}})}{2 \hat{J}_{\mathrm{u}}},\] (6)

where the definitions are analogous to those in Equation 5 and decorated with \(\hat{\cdot}\) to indicate the difference. Due to the distance-minimization term \(\min_{\hat{g},\hat{\mathbf{s}}_{\mathrm{src}}\in\mathcal{S}_{\mathrm{src}}} \left\|\hat{\mathbf{s}}_{\mathrm{tgt}}-\hat{\mathbf{s}}_{\mathrm{src}}\right\|\), the distance for the correct solution \(\hat{\mathbf{c}}_{k}\) is upper-bounded by \(\left\|\hat{\mathbf{h}}\right\|<\frac{D(\hat{\mathbf{c}}_{k},\hat{\mathbf{c}} _{k^{\prime}})}{2J_{\mathrm{u}}}\), since this is attainable when the estimated generating function is the true function, i.e., \(g=\hat{g}\). Equation 6 implies that the alternative solution \(\hat{\mathbf{c}}_{k^{\prime}}\) would always yield \(\left\|\hat{\mathbf{h}}^{\prime}\right\|\geq\frac{D(\hat{\mathbf{c}}_{k},\hat {\mathbf{c}}_{k^{\prime}})}{2\hat{J}_{\mathrm{u}}}>\left\|\hat{\mathbf{h}}\right\|\), which the distance-minimizing regularization would exclude. Therefore, we have shown that the estimated \(\hat{\mathbf{c}}_{\mathrm{tgt}}\) corresponds to the correct \(\mathbf{c}_{k}\). 

### Proof for Theorem 4.4

**Assumption 4.3** (Identification Conditions under Local Shifts).:
1. _[_Smoothness & Invertibility]: The generating function_ \(g\) _in Equation_ 1 _is invertible and differentiable, and its inverse is also differentiable._
2. _[_Invariant Variable Informativeness]: The dimensions under_ \(\mathbf{c}\)_'s exclusive influence is uniquely determined: for a fixed_ \(\mathbf{c}\in\mathcal{C}\)_,_ \([\mathbf{x}]_{\mathcal{I}_{\mathbf{c}\backslash\mathbf{s}}(\mathbf{c},\mathbf{ s}_{1})}\neq[\mathbf{x}]_{\mathcal{I}_{\mathbf{c}\backslash\mathbf{s}}(\mathbf{c}^{ \ast},\mathbf{s}_{2})}\) _for any_ \(\mathbf{c}^{\ast}\neq\mathbf{c}\)_,_ \(\mathbf{s}_{1}\in\mathcal{S}\)_, and_ \(\mathbf{s}_{2}\in\mathcal{S}\)_._
3. _[Sparse Influence]: At any_ \(\mathbf{z}\in\mathcal{Z}\)_, the changing variable_ \(\mathbf{s}\) _influences at most_ \(d_{\mathbf{s}}\) _dimensions of_ \(\mathbf{x}\)_, i.e.,_ \(|\mathcal{I}_{\mathbf{s}}(\mathbf{z})|\leq d_{\mathbf{s}}\)_. Alternatively, the two variables_ \(\mathbf{c}\) _and_ \(\mathbf{s}\) _do not intersect on their influenced dimensions_ \(\mathcal{I}_{\mathbf{c}}(\mathbf{z})\cap\mathcal{I}_{\mathbf{s}}(\mathbf{z})=\emptyset\)_._
4. _[Mechanistic Dependence]: For all_ \(\mathbf{z}\)_, any nontrivial partition_ \(\mathcal{P}_{1},\mathcal{P}_{2}\) _of the dimensions_ \(\mathcal{I}_{\mathbf{c}\backslash\mathbf{s}}(\mathbf{z})\) _yields dependence between the sub-matrices of the Jacobian_ \(\mathbf{J}_{g}(\mathbf{z})\)_:_ \(\text{rank}([\mathbf{J}_{g}(\mathbf{z})]_{\mathcal{I}_{\mathbf{c}\backslash \mathbf{s}}}(\mathbf{z}))<\text{rank}([\mathbf{J}_{g}(\mathbf{z})]_{\mathcal{P}_ {1}}(\mathbf{z}))+\text{rank}([\mathbf{J}_{g}(\mathbf{z})]_{\mathcal{P}_{2}}( \mathbf{z}))\)_._

**Theorem 4.4** (Extrapolation under Sparse Shifts).: _Assuming a generating process in Equation 1, we estimate the distribution with model \((\hat{g},\hat{p}(\hat{\mathbf{c}}),\hat{p}(\hat{\mathbf{s}}))\) with the objective:_

\[\sup\hat{p}(\hat{\mathbf{c}}_{\mathrm{tgt}}),\quad\text{Subject to:}\quad\hat{p}( \mathbf{x})=p(\mathbf{x}),\,\forall\mathbf{x}\in\mathcal{X}_{\mathrm{src}}.\] (3)_Under Assumption 4.3, the estimated model can attain the identifiability in Definition 3.1._

**Lemma A1** (Brady et al. [39]).: _Let \(g,\hat{g}:\mathbb{R}^{d_{\mathbf{x}}}\to\mathbb{R}^{d_{\mathbf{x}}}\) be smooth and invertible. Then, for any \(\mathbf{z}\in\mathbb{R}^{\mathbf{z}}\), \(\mathcal{S}\subset\mathbb{R}^{d_{\mathbf{z}}}\), \(\text{rank}([\mathbf{J}_{g}(\mathbf{z})]_{\mathcal{S}})=\text{rank}([ \mathbf{J}_{\hat{g}}(\hat{\mathbf{z}})]_{\mathcal{S}})\), where \(\hat{\mathbf{z}}:=\hat{g}^{-1}\circ g(\mathbf{z})\)._

Proof.: The proof consists of two steps. In the first step, we show the identification of the index set \(\mathcal{I}_{\mathbf{c}\setminus\mathbf{s}}(\mathbf{z})\subset[d_{\mathbf{x}}]\) over which \(\mathbf{x}\) receives only \(\mathbf{c}^{\prime}\) influence. That is, \(\hat{g}\) maps the estimated invariant variable \(\hat{\mathbf{c}}\) to \(\mathbf{x}\) dimensions generated by the true invariant variable \(\mathbf{c}\) alone. In the second step, we show that the objective \(\sup\hat{p}(\hat{\mathbf{c}}_{\mathrm{tgt}})\) assigns to the estimated invariant variable for the target sample \(\hat{\mathbf{c}}_{\mathrm{tgt}}\) the source-distribution estimated value \(\hat{\mathbf{c}}_{\mathrm{src}}\) that is also generated by \(\mathbf{c}_{\mathrm{tgt}}\). Recall the notation \(\mathbf{z}:=[\mathbf{c},\mathbf{s}]\). We denote the latent source support as \(\mathcal{Z}_{\mathrm{src}}\) and the set augmented with the target sample as \(\mathcal{Z}:=\mathcal{Z}_{\mathrm{src}}\cup\{\mathbf{z}_{\mathrm{tgt}}\}\)

**Step 1.** We first show that \(\hat{g}\) cannot map the estimated invariant variable \(\hat{\mathbf{c}}\) to \(\mathbf{x}\) dimensions generated by both the invariant variable \(\mathbf{c}\) and the changing variable \(\mathbf{s}\).

We show this by contradiction. Suppose that \(\exists\mathbf{z}^{*}\in\mathcal{Z}\), such that

\[\hat{\mathcal{I}}_{\mathbf{c}\setminus\mathbf{s}}(\hat{\mathbf{z}}^{*})\cap \mathcal{I}_{\mathbf{c}\setminus\mathbf{s}}(\mathbf{z}^{*})\neq\emptyset\text{ and }\hat{\mathcal{I}}_{\mathbf{c}\setminus\mathbf{s}}(\hat{\mathbf{z}}^{*})\cap \mathcal{I}_{\mathbf{s}}(\mathbf{z}^{*})\neq\emptyset\] (7)

We partition \(\mathcal{I}_{\mathbf{c}\setminus\mathbf{s}}(\mathbf{z}^{*})\) into two disjoint sets \(\mathcal{I}_{\mathbf{c}\setminus\mathbf{s},1}:=\{i\in\mathcal{I}_{\mathbf{c} \setminus\mathbf{s}}(\mathbf{z}^{*})|i\in\hat{\mathcal{I}}_{\mathbf{c} \setminus\mathbf{s}}(\hat{\mathbf{z}}^{*})\}\) and \(\mathcal{I}_{\mathbf{c}\setminus\mathbf{s},2}:=\{i\in\mathcal{I}_{\mathbf{c} \setminus\mathbf{s}}(\mathbf{z}^{*})|i\notin\hat{\mathcal{I}}_{\mathbf{c} \setminus\mathbf{s}}(\hat{\mathbf{z}}^{*})\}\) base on the overlap with \(\hat{\mathcal{I}}_{\mathbf{c}\setminus\mathbf{s}}(\hat{\mathbf{z}}^{*})\). By the supposed condition Equation 7, it follows that \(\mathcal{I}_{\mathbf{c}\setminus\mathbf{s},1}\neq\emptyset\) and \(\mathcal{I}_{\mathbf{s},1}\neq\emptyset\).

We now show that \(\mathcal{I}_{\mathbf{c}\setminus\mathbf{s},2}\) is nonempty by contradiction. Suppose that \(\mathcal{I}_{\mathbf{c}\setminus\mathbf{s},2}\) is empty. It follows that \(\mathcal{I}_{\mathbf{c}\setminus\mathbf{s}}=\mathcal{I}_{\mathbf{c}\setminus \mathbf{s},2}\subset\hat{\mathcal{I}}_{\mathbf{c}\setminus\mathbf{s}}\). By definition, we know that \(\mathcal{I}_{\mathbf{s},1}\subset\hat{\mathcal{I}}_{\mathbf{c}\setminus \mathbf{s}}\). Thus, \(A:=\mathcal{I}_{\mathbf{c}\setminus\mathbf{s}}\cup\mathcal{I}_{\mathbf{s},1} \subset\hat{\mathcal{I}}_{\mathbf{c}\setminus\mathbf{s}}\). This inclusion implies that

\[\text{rank}([\mathbf{J}_{g}(\mathbf{z}^{*})]_{A,:})=\text{rank}([\mathbf{J}_{ \hat{g}}(\hat{\mathbf{z}}^{*})]_{A,:})\leq d_{\mathbf{c}}.\] (8)

Since \(\mathcal{I}_{\mathbf{c}\setminus\mathbf{s}}\cap\mathcal{I}_{\mathbf{s},1}=\emptyset\), \([\mathbf{J}_{g}(\mathbf{z}^{*})]_{\mathcal{I}_{\mathbf{c}\setminus\mathbf{s}}, d_{\mathbf{c}}+1:d_{\mathbf{z}}}=0\), and each row of \([\mathbf{J}_{g}(\mathbf{z}^{*})]_{\mathcal{I}_{\mathbf{s},d_{\mathbf{c}}+1:d_{ \mathbf{z}}}}\) is nonzero by definition, we have that

\[\text{rank}([\mathbf{J}_{g}(\mathbf{z}^{*})]_{A,:})>\text{rank}([\mathbf{J}_ {g}(\mathbf{z}^{*})]_{\mathcal{I}_{\mathbf{c}\setminus\mathbf{s},:}})=d_{ \mathbf{c}},\] (9)

which contradicts Equation 8. Therefore, \(\mathcal{I}_{\mathbf{c}\setminus\mathbf{s},2}\) is nonempty. Since we have \(\text{rank}([\mathbf{J}_{g}(\mathbf{z}^{*})]_{\mathcal{I}_{\mathbf{c}\setminus \mathbf{s}})}=d_{\mathbf{c}}\) and \((\mathcal{I}_{\mathbf{c}\setminus\mathbf{s},1},\mathcal{I}_{\mathbf{c}\setminus \mathbf{s},2})\) forms a pair of nonempty partition, Assumption 4.3-iv implies that

\[\text{rank}([\mathbf{J}_{g}(\mathbf{z}^{*})]_{\mathcal{I}_{\mathbf{c}\setminus \mathbf{s},1},:})+\text{rank}([\mathbf{J}_{g}(\mathbf{z}^{*})]_{\mathcal{I}_{ \mathbf{c}\setminus\mathbf{s},2},:})>\text{rank}([\mathbf{J}_{g}(\mathbf{z}^{* })]_{\mathcal{I}_{\mathbf{c}\setminus\mathbf{s},i},:})=d_{\mathbf{c}}.\] (10)

As the \(g\)'s and \(\hat{g}\)'s Jacobian matrix ranks are related (Lemma A1), Equation 10 implies that

\[\text{rank}([\mathbf{J}_{\hat{g}}(\hat{\mathbf{z}}^{*})]_{\mathcal{I}_{\mathbf{c} \setminus\mathbf{s},1},:})+\text{rank}([\mathbf{J}_{\hat{g}}(\hat{\mathbf{z}}^{* })]_{\mathcal{I}_{\mathbf{c}\setminus\mathbf{s},2},:})>d_{\mathbf{c}}.\] (11)

Since \(\mathcal{I}_{\mathbf{c}\setminus\mathbf{s},1}\subset\hat{\mathcal{I}}_{\mathbf{c} \setminus\mathbf{s},1}\) and \(\mathcal{I}_{\mathbf{c}\setminus\mathbf{s},2}\cap\hat{\mathcal{I}}_{\mathbf{c} \setminus\mathbf{s},1}=\emptyset\) by definition and \([\mathbf{J}_{\hat{g}}(\hat{\mathbf{z}}^{*})]_{\mathcal{I}_{\mathbf{c}\setminus \mathbf{s},2},d_{\mathbf{c}}+1:d_{\mathbf{z}}}\) has a full-row rank (Assumption 4.3-iii), it follows that

\[\text{rank}([\mathbf{J}_{\hat{g}}(\hat{\mathbf{z}}^{*})]_{\mathcal{I}_{\mathbf{ c}\setminus\mathbf{s},:}})=\text{rank}([\mathbf{J}_{\hat{g}}(\hat{\mathbf{z}}^{*})]_{ \mathcal{I}_{\mathbf{c}\setminus\mathbf{s},1},:})+\text{rank}([\mathbf{J}_{ \hat{g}}(\hat{\mathbf{z}}^{*})]_{\mathcal{I}_{\mathbf{c}\setminus\mathbf{s},2},:})>d_{ \mathbf{c}}.\] (12)

However, Lemma A1 implies that

\[\text{rank}([\mathbf{J}_{\hat{g}}(\hat{\mathbf{z}}^{*})]_{\mathcal{I}_{\mathbf{ c}\setminus\mathbf{s},:}})=\text{rank}([\mathbf{J}_{g}(\mathbf{z}^{*})]_{ \mathcal{I}_{\mathbf{c}\setminus\mathbf{s},:}})=d_{\mathbf{c}}.\] (13)

Thus, we have arrived at a contradiction. We have shown that \(\hat{g}\) maps the estimated invariant variable \(\hat{\mathbf{c}}\) to \(\mathbf{x}\) dimensions generated by the true invariant variable \(\mathbf{c}\) alone, i.e., \(\hat{\mathcal{I}}_{\mathbf{c}\setminus\mathbf{s}}(\hat{\mathbf{z}})\subset \mathcal{I}_{\mathbf{c}\setminus\mathbf{s}}(\mathbf{z})\). Moreover, if an index \(i^{\prime}\) from the \(\hat{\mathbf{s}}\)'s region \(\hat{\mathcal{I}}_{\mathbf{s}}(\hat{\mathbf{z}})\) also belonged to \(\mathcal{I}_{\mathbf{c}\setminus\mathbf{s}}(\mathbf{z})\), i.e., \(i^{\prime}\in\hat{\mathcal{I}}_{\mathbf{s}}(\hat{\mathbf{z}})\cap\mathcal{I}_{ \mathbf{c}\setminus\mathbf{s}}(\mathbf{z})\), then we would have \(\text{rank}([\mathbf{J}_{g}(\mathbf{z})]_{\mathcal{I}

## Appendix A4 Synthetic Data Experiments

### Implementation Details

We employ a variational auto-encoder [67] whose encoder and decoder are both 4-layer MLP with 32 dimensions and leaky ReLu (\(\alpha=0.2\)). Following Equation 2 and Equation 3, we implement reconstruction loss, KL loss on the source distribution, the likelihood loss on the target sample, and a classification loss on the source data. For the dense case, we implement an additional distance loss to minimize the \(\ell_{2}\) distance of \(\hat{\mathbf{s}}_{\mathrm{tgt}}\) to the center of the source support (which is the origin in our case). The source-only baseline is trained only with classification loss. The iMSDA implementation is adopted directly from the source code of Kong et al. [18]. We train all methods with Adam [68] and learning rate \(2e-3\) for \(25\) epochs. We fix the loss weights \(\lambda_{\mathrm{cls}}=1\), \(\lambda_{\mathrm{recons}}=0.1\), \(\lambda_{\mathrm{tgt\_likelihood}}=0.1\), and \(\lambda_{\mathrm{s\_distance}}=0.01\) (for dense shifts) overall distance configurations. We only tune \(\lambda_{\mathrm{KL}}\) from the interval \(\{1e-1,1e-2,1e-3\}\). We run synthetic data experiments on one Nvidia L40 GPU and each run consumes less than 2 minutes.

### Regression Task Evaluation

In addition to the classification experiments, we evaluate our model on regression in this section.

#### a4.2.1 Implementation

**Data generation.** The regression target \(y\) is generated from a uniform distribution \(U(0,4)\). We sample 4 latent invariant variables \(\mathbf{c}\) from a normal distribution \(N(y,\mathbf{I}_{c})\). Two changing variables in the source domain \(\mathbf{s}_{\mathrm{src}}\) are sampled from a truncated Gaussian centered at the origin. In the target domain, changing variables \(\mathbf{s}_{\mathrm{tgt}}\) are sampled at multiple distances (e.g., \(\{18,24,36\}\)) from the origin. For dense shifts, observations \(\mathbf{x}\) are generated by concatenating \(\mathbf{c}\) and \(\mathbf{s}\) and feeding them to a 4-layer MLP with ReLU activation. For sparse shifts, only two out of six dimensions of \(\mathbf{x}\) are influenced by the changing variable \(\mathbf{s}\). We generate 10k samples for training and 50 target samples for testing (one target sample accessed per run).

**Model.** We make two modifications on the classification model in Section 5. First, we substitute the classification head with a regression head (the last linear layer). Second, we replace the cross-entropy loss with MSE loss. We fix the loss weights of MSE loss and KL loss at 0.1 and 0.01 for all settings, respectively, and keep all other hyper-parameters the same as in the classification task. We use MSE as the evaluation metric.

#### a4.2.2 Results and Analysis

Table A1 displays the evaluation results. We can observe that our method consistently outperforms the baseline and maintains its performance over a wide range of shift distances. In contrast, the baseline that directly uses all the feature dimensions degrades drastically when the shift becomes severe. This indicates that our approach can indeed identify the invariant part of the latent representation, validating our theoretical results.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline
**Shifts** & \multicolumn{3}{c}{**Dense**} & \multicolumn{3}{c}{**Sparse**} \\ \cline{2-7}
**Distance** & 18 & 24 & 30 & 18 & 24 & 30 \\ \hline Only Source & \(11.64\) & \(2.44\) & \(3.26\) & \(1.84\) & \(3.32\) & \(5.84\) \\
**Ours** & **1.40** & **1.60** & **1.68** & **1.15** & **1.48** & **1.60** \\ \hline \hline \end{tabular}
\end{table}
Table A1: **Synthetic data results on regression (MSE) under both dense and sparse shifts across various distances.**

### Real-world Data Experimental Details

#### a5.1 Datasets

The datasets used in our experiments include CIFAR10-C, CIFAR100-C, ImageNet-C [45], and ImageNet100-C. CIFAR10-C and CIFAR100-C are extended versions of the CIFAR datasets [69] designed to evaluate model robustness against visual corruptions, featuring 10 and 100 classes respectively, each with 50,000 clean training samples and 10,000 corrupted test samples. ImageNet-C, on the other hand, scales this concept up with 1,000 classes, providing 50,000 test samples of each of 15 corruption types. ImageNet-100 [46] is a subset of ImageNet with 100 classes. In our experiments, we build ImageNet100-C by selecting 100 classes reported in Tian et al. [46] from ImageNet-C [45] with 15 different types of corruption.

#### a5.2 Generative Adaptation with Entropy Minimization

When applying entropy minimization in the MAE-TTT framework [20], we did not directly integrate the entropy-minimization loss. The self-supervised MAE training process relies on masked images, whereas entropy minimization requires classifying the entire image. To address this, we introduced additional training steps using unmasked images and applied the entropy-minimization loss during these steps. Specifically, the training process for each test-time iteration is split into two stages: 1) Stage One: We follow the MAE-TTT approach by inputting masked images and training the model using reconstruction loss. In this stage, only the encoder is updated. 2) Stage Two: We input full images (32 in a batch) and optimize the model with the entropy minimization loss following SHOT [43]. In this stage, both the encoder and classifier are optimized. The learning rates for both stages are set to be the same. The experiments are conducted with the PyTorch 1.11.0 framework, CUDA 12.0 with 4 NVIDIA A100 GPUs.

#### a5.3 Sparsity Regularization

Here, we provide the implementation details of our modification to add sparsity regularization. In the pre-train stage, we apply the ResNet50 [47] as the backbone network and follow [14; 44] to pre-train it on the clean CIFAR10, CIFAR100, and ImageNet training sets, with joint contrastive and classification losses. In the test-time adaptation process, we adopt the sequential TTA protocol as outlined in TTAC [44] and TeSLA [21]. This protocol prohibits the change of training objectives throughout the test phase. Moreover, all testing data be processed in a sequential manner (one-pass), ensuring each data point is passed through the adaptation process exactly once.

Our method is built upon TeSLA [21], and follows most of its hyperparameters. Thus, we only discuss the extra hyperparameters we involved, including the low-rank dimension \(r\), the ratio of learning rate factor for soft frozen \(ratio_{l}=\frac{\text{H}_{max}}{\text{H}_{basebase}}\), and the ratio of sparsity loss \(ratio_{s}\). The details are shown in Table A2. It was observed that the constraints on minimal change need to be more stringent as the complexity of the data increases. The experiments are conducted with the PyTorch 1.13.0 framework, CUDA 11.7 with an NVIDIA A100 GPU.

#### a5.4 Recoverability of the Invariant Variable

We assess the impact of the recoverability of the invariant variable on Test-Time Adaptation (TTA) methods. To do this, we compare the performance of TTA methods using supervised and unsupervised pre-trained models that have similar ImageNet classification accuracy. Our goal is to validate whether invariant variables learned from annotated labels can improve test-time adaptation. We assume that

[MISSING_PAGE_FAIL:23]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We emphasize our contributions in the abstract and introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have discussed the limitations in Section 7. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: All proofs are given in Appendix A2 and Appendix A3.  Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Implementation details are given in Section 5, Section 6, Appendix A4, and Appendix A5. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: We provide our Github link in the main paper. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Implementation details are given in Section 5, Section 6, Appendix A4, and Appendix A5. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: All our real-world experiments are over at least three random seeds. Our synthetic data experiments are over 50 random seeds. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We disclose our compute resources in Appendix A4 and Appendix A5. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Our work is on the understanding the fundamental aspect of machine learning, posing no direct societal impacts. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite all the employed codebase in Appendix A4 and Appendix A5. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.