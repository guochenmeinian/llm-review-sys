# Model - GLUE: Democratized LLM Scaling for A Large Model Zoo in the Wild

Xinyu Zhao\({}^{*1}\), Guoheng Sun\({}^{*2}\), Ruisi Cai\({}^{*3}\), Yukun Zhou\({}^{*4}\), Pingzhi Li\({}^{*1}\), Peihao Wang\({}^{*3}\)

Bowen Tan\({}^{5}\), Yexiao He\({}^{2}\), Li Chen\({}^{6}\), Yi Liang\({}^{6}\), Beidi Chen\({}^{5}\), Binhang Yuan\({}^{4}\)

Hongyi Wang\({}^{17}\), Ang Li\({}^{\dagger 2}\), Zhangyang Wang\({}^{\dagger 3}\), Tianlong Chen\({}^{\dagger 1}\)

\({}^{1}\)UNC CH \({}^{2}\)UMD \({}^{3}\)UT Austin \({}^{4}\)HKUST \({}^{5}\)CMU \({}^{6}\)Google \({}^{7}\)Rutgers University

{xinyu,pingzhi,tianlong}@cs.unc.edu, {ghsun,yexiaohe,angliece}@umd.edu

{ruisi.cai,peihaowang,atlasswang}@utexas.edu

yzhoufw@connect.ust.hk, {btan2,beidic}@andrew.cmu.edu

li.lizliz.chen@gmail.com, yiliang@google.com

biyuan@ust.hk, hongyi.wang.001@rutgers.edu

\({}^{*}\)Equal Contribution \({}^{\dagger}\)Equal Supervision

###### Abstract

As Large Language Models (LLMs) excel across tasks and specialized domains, scaling LLMs based on existing models has gained significant attention, which is challenged by potential performance drop when combining disparate models. Various techniques have been proposed to aggregate pre-trained LLMs, including model merging, Mixture-of-Experts, and stacking. Despite their merits, a comprehensive comparison and synergistic application of them to a diverse model zoo is yet to be adequately addressed. In light of this research gap, this paper introduces Model-GLUE, a holistic LLM scaling guideline. First, our work starts with a benchmarking of existing LLM scaling techniques, especially selective merging, and variants of mixture. Utilizing the insights from the benchmark results, we formulate a strategy for the selection and aggregation of a heterogeneous model zoo characterizing different architectures and initialization. Our methodology involves clustering mergeable models, selecting a merging strategy, and integrating model clusters through model-level mixture. Finally, evidenced by our experiments on a diverse Llama-2-based model zoo, Model-GLUE shows an average performance enhancement of 5.61%, achieved without additional training. Codes are available at https://github.com/Model-GLUE/Model-GLUE.

## 1 Introduction

Large Language Models (LLMs) have demonstrated unparalleled capability in a diverse array of natural language tasks, encompassing commonsense reasoning, question answering, and specialized domains such as mathematics and programming [39, 43, 52]. The effectiveness of LLMs is based on the scaling law, which posits that proportionally increasing model and training data size leads to enhanced model performance [27]. Nevertheless, the computation overhead and data requirement surge as LLM continues to scale. With the widespread of open-sourced general or specialized LLMs, aggregating existing models to construct a more versatile LLM emerges as an economical alternative to training a larger LLM from scratch [13, 16, 54]. This not only mitigates the computation cost but also leverages the collective advancements of previous efforts in building LLMs.

Within different methods to combine existing LLMs, a major class is merging [2, 4, 22, 24, 35, 59, 63, 64]. Model merging combines multiple models into a single one of the same size through weight-space transformation. Wortsman et al. [59] first propose to merge a few fine-tuned models as a training trick for the flat loss-landscape, and Ilharco et al. [22] extends it to multi-task scenario, both of which employ the simple averaging. Other works propose more complicated merging methods,leveraging weight sparsity [63; 64] and non-uniform coefficient [4; 35]. However, they assume that all candidate models are "useful" when merging. While this may hold for small-sized designed model collections, it may not be the case in real-world scenarios given a large and divergent model zoo. How to ensure the benefits of merging different model zoo sizes and similarities, and exclude "harmful" candidates, remains underexplored.

Since merging is limited to the same model structures and initial weights, another alternative is Mixture-of-Experts (MoE) [16]. MoE is a conditional computation architecture that activates only a subset of model parameters for each specific input example [47]. MoE LLMs have already demonstrated performance and computational efficiency advantages over their dense counterparts [15; 25; 30; 68]. In particular, we use a broader term "mixture" to denote the aggregation of existing expert LLMs according to the MoE paradigm, which has been successfully implemented in some recent practices [50; 54; 55]. However, these implementations neglect the inherent flexibility of MoE to integrate different expert models, especially those groups that do not work with merging. Also, the difference and possible synergy between merging and mixing have not been thoroughly investigated. Based on the above challenges, our primary research question is formulated as:

_(Q) Is it feasible to establish a benchmark for selecting and aggregating Large Language Models (LLMs) from an extensive and varied model zoo based on current state-of-the-art model merging and mixture, thereby enhancing the overall competence of the final model?_

To address (Q), we present Model-GLUE, a comprehensive benchmark and set of guidelines for LLM scaling. Model-GLUE is the first work for LLM scaling encompassing a wide range of model group sizes and variability, with a principal emphasis on the merging and mixture methodologies, and also discussion of model stacking. We first delve into merging scheduling, analyzing strategies for identifying potentially detrimental model candidates and various merging techniques. We then explore a variety of model mixtures as an alternative to merging, covering different mixture granularity, routers architecture, routing input inputs, _etc_. Building upon the insights from model merging and mixture, Model-GLUE introduces an efficient and robust LLM scaling recipe for a diverse set of models. It starts with model clustering and progressive merging, and then the mixture of all clusters, thereby integrating similar knowledge from the model zoo while highlighting the respective strengths of each cluster. Our contributions are outlined as follows:

\(\bullet\) We conduct a comprehensive benchmarking analysis of LLM merging strategies, beginning with identifying each model's contribution and then followed by filtering out detrimental candidates. Our findings are validated on a range of LLMs, from a few to over a dozen.

\(\bullet\) We assess model mixture for four distinct variants: mixture level, router design, router input, and hybrid mixture. We have derived several principles for model mixture and discussed its utility as a solution for scaling models incompatible with merging.

\(\bullet\) We introduce a recipe for progressively combining LLM models, Model-GLUE, based on findings on merging and mixture benchmarks. It first conducts selective merging and then model mixture, outperforming the best single model on general reasoning, mathematics, and coding tasks.

\(\bullet\) Extensive experimental results on Llama-2-based models validate our proposal. For instance, Model-GLUE achieves an average increase of \(5.61\%\) across chatting, mathematics, and coding benchmarks compared to the best single LLM.

## 2 Related Works

Model Merging.Merging methods can be divided into zero-shot merging and merge-then-train approaches. Early zero-shot merging methods are weight averaging and Linear Mode Connectivity [38; 59]. Later popular methods include Task Arithmetic [22] manipulating task vectors, and TIES [63] addressing parameter interference through trimming and conflict resolution. DARE [64] optimizes parameters selectively to enhance merging without extra training. Others focus on geometric properties of weights for merging [49; 24]. Recent Evolutionary Model Merge [4] improves weight configuration and data token pathways during inference. For the merge-then-train approach, Fisher merging [35] uses the Fisher information matrix to weigh model parameters to maximize their joint likelihood. RegMean [26] adapts the linear merging to each linear layer while averaging

Figure 1: Overview of Model-GLUE, composing of (1) Model Clustering based on architecture and weight similarity; (2) Model Filtering and Searching for merging; (3) Model Merging within each cluster; (4) Model Level Mixture of merged models.

embeddings and biases. However, both zero-shot and merge-then-train approaches are less effective for models initialized differently. [2; 23; 53; 62] exploit the permutation symmetry inherent in neural networks on small to large models. To boost merging efficiency, our focus on merging lies in the zero-shot merging of models with the same architecture and initialization.

**Model Mixture.** Mixture-of-Experts (MoE) [47] scales up neural networks by utilizing router networks to activate different parts of the model for different input tokens. Its integration with Large Language Models (LLMs) has gained notable recognition for its exceptional generative capabilities and unparalleled efficiency. Recently, Mixtral [25] demonstrates that the MoE methodology can achieve the performance of dense LLM counterparts while employing significantly fewer active parameters. Model mixture combines a collection of dense LLM models, irrespective of their sizes, into a MoE model. Some studies discover model fusion [54; 55] integrating the outputs of expert models to exploit the unique insights into the data distribution. Recent initiatives include BranchTrain-MiX [50], which starts with a seed-dense LLM and then branches out, facilitating the parallel training of expert models. These trained dense models are subsequently incorporated as experts within MoE layers, with other parameters being averaged. However, this approach is limited to dense models that share identical architectures and sizes. Most recently, UltraFuser [13] introduces a token-level soft gating mechanism that blends model outputs, with a two-stage training strategy.

**Model Stacking.** Model stacking concatenates two models along the depth dimension. In the era of LLM, Wu et al. [60] reuses pre-trained LLaMA layers and resets the output projection to zero in stacking. Kim et al. [28] shows dropping middle layers in stacking yields superior performance. Wang et al. [57] prove that stacking could help recover model-parameter scaling laws with insufficient data. Reddi et al. [42] demonstrated that gradual stacking leads to significant improvements in wall-clock time during the training of few-shot learners. Theoretically, Agarwal et al. [1] proved that model stacking could be interpreted as Nesterov acceleration in network optimization. However, all the aforementioned stacking methods involve no more than two kinds of models and primarily focus on the benefits of training acceleration. In this work, we explore the possibility of stacking two heterogeneous models to combine their capabilities.

**Model Scaling Tools** There have been several tools for model mixture and merging, and for scaling models using existing LLMs. For example, Mergekit is an open-source library designed to facilitate the application of model merging strategies and the construction of MoE [16]. As a representative of unified LLM, Beyonder is a set of mixtures of merged and single LLMs for different tasks1. However, there is still a lack of a comprehensive benchmark of the various mixing and merging techniques and practical guidance on how to unify groups of LLMs at different levels of similarity.

Footnote 1: https://huggingface.co/mlabonne/Beyonder-4x7B-v3

## 3 Methodology

### Preliminaries

In this study, we consider a collection of \(n\) existing Large Language Models (LLMs), denoted as \(\{\texttt{M}_{1},\ldots,\texttt{M}_{n}\}\), which have been fine-tuned on diverse corpora. Our objective is to outline a systematic approach towards producing one stronger aggregated model across all knowledge domains. Specifically, the unified LLM incorporates single LLMs mainly through merging and mixture.

### Model Merging

The concept of Model MergingModel merging is integrating multiple models into one unified model in the weight space, compatible with LLMs of the same initialization [16]. Popular merging methods can be divided into two types: _Merging entire model weights_ represented by Model Soup [59] (Linear), SLERP [49], and Model Stock [24]; _M Task-vector based merging_ represented by Task Arithmetic [22], TIES [63], and DARE [64]. The former method directly interpolates model weights, while the latter subtracts the pre-trained model from the fine-tuned model to obtain task vectors and utilizes sparsity and consistency of parameters for refined

Figure 2: Pipeline for model merging, as well as an overview of merging methods and search strategies.

merging. The basic Linear interpolation merging is defined as \(w_{u}=\sum_{i=1}^{n}s_{i}\cdot w_{i}\), where \(w_{i}\) and \(s_{i}\) are the corresponding model weights and merging coefficient of \(\mathtt{M}_{i}\in\{\mathbb{M}_{1},\ldots\mathtt{M}_{n}\}\).

**Selective Merging Pipeline** Merging can be easily applied to models with the same architecture, but does not guarantee better results. Therefore, before searching for the merging coefficient, we first pre-process the models by clustering all the models using cosine similarity and then searching for the optimal merging coefficient and method within each cluster. Details are explained in Appendix A.5.

**Heuristic and Evolutionary Strategies** The heuristic strategy is for searching and filtering potential harmful models for merging. It is based on greedy search, involving three variants: _Heuristic-Average_ retain the candidate if there is an improvement on the proxy dataset in each round of merging. _Heuristic-Coefficient_ builds upon _Heuristic-Average_, by combining the previously merged model with a new candidate using different coefficients in each round. _Heuristic-Similarity_ selects the candidate model with the highest or lowest similarity and conducts a coefficient search to combine it with the previously merged model. Detailed heuristic strategy algorithms can be found in Appendix A.1 Heuristic strategies perform pairwise merging of models, while many methods allow for merging multiple models at once. Therefore, we also consider jointly optimizing all model coefficients using the _Evolutionary Strategy_.

### Model Mixture

**The concept of Model Mixture.** Model mixture resembles Mixture-of-Experts(MoE). It scales a LLM with multiple pre-trained LLM experts and further extends beyond traditional token-dependent Feed-Forward-Network (FFN) MoE designs [47]. A mixture model is composed of MoE modules and the rest shared parameters. A MoE module consists of a router \(\mathcal{G}(\cdot)\) and \(n\) expert networks \(\{\mathbb{E}_{1},\cdots,\mathbb{E}_{n}\}\). \(\mathcal{G}(\cdot)\) takes a router input \(x_{\mathcal{G}}\) and generate expert assignment for each token input \(x\). Then MoE outputs a weighted sum of experts' outputs as \(\mathtt{MoE}(x,x_{\mathcal{G}})=\sum_{i=1}^{n}\mathcal{G}(x_{\mathcal{G}})_{i }\cdot\mathbb{E}_{i}(x)\). We experiment with several variations of Model Mixture, classified as follows:

**Mixture levels.** Traditional Mixture-of-expert models replace the dense FFN layer at each Transformer block with an MoE module, which is only compatible with LLMs that share the same architecture. Besides this _FFN level mixture_, we also experiment with two coarse-grained mixtures. _Block level mixture_ create MoE module by aggregating Transformer blocks with the same index from each LLM as experts and add a block-wise router. Block level mixture is applicable to models with different architecture but the same embedding space, layer amounts, and intermediate dimension. _Model level mixture_ take each LLM as an expert and use a router at mixture model input. Model level mixture covers any LLM groups not compatible with FFN and block level mixture. In particular, the model level mixture is similar but not identical to the model ensemble, as the former can be sparse and focus more on efficiency and exploit single LLM expertise, while the latter produces general results by averaging or majority voting overall model outputs. Details can be found in Appendix A.3

**Router design.** The router network of many MoE studies adheres to a _linear router_[47]. We experiment with another more complex _MLP router_ to examine whether this router design leads to better performance. It is implemented by two sequential FFN and a ReLU function in between, inspired by [48; 32]. For the routing method, we employ Top-K selection to all routers, which activates the K experts corresponding to the K largest softmaxed router output [47; 48].

**Router input.** We adopt two types of router input for different levels of model mixture: _Token input for FFN level mixture, where router input is the same as model input; _Sample input for block and model level mixture, where we calculate the average embedding as the sample input \(x_{\mathcal{G}}=\sum_{i=1}^{n}x_{n}\), and route tokens of a sample to the same expert based on sample routing. The sample routing avoids inconsistency in attention operation.

**Hybrid mixture.** To explore LLM scaling in between model merging and model mixture, we propose the hybrid mixture as an intermediate solution. In a hybrid mixture, the bottom few layers of all single LLMs are merged, and then the rest layers follow any of the mixture level designs.

Figure 3: The overview and decision flow of three model mixture levels and their selection philosophy.

[MISSING_PAGE_FAIL:5]

all parameters are randomly initialized, and the fitness values are defined as the accuracy of the proxy dataset. The optimization was conducted for 200 trials in all scenarios.

### Model Merging Benchmark Results

We start our discussion by examining the effectiveness of existing approaches in depth. Despite existing merging methods focus on improving the merging techniques, their effectiveness is usually validated basedt on small-scale model zoos. For instance, Ilharco et al. [22] primarily focuses on the linear interpolation between two fine-tuned models, while Akiba et al. [4] explores merging three.

Current model practitioners typically download pre-trained models, fine-tune them on their own data or with unique techniques for specific downstream tasks, and then upload them back to the public. This practice results in a large number of open-source models being available, yet they remain underutilized by current merging methods. To this end, instead of solely discussing the merging technique, we explore an **orthogonal** question: _Can we scale up the size of model zoo to cover more models, and design an automatic merging technique to benefit from the inclusion?_

**Failure Case of Existing Approaches.** To begin with, we provide a motivating example to show the failure case of the existing approach. We consider the three models, Llama-2-Chat [52], Vicuna [67] and CodeLlama [43], all initialized with the same base model, Llama-2 [52]. We merge Vicuna and CodeLlama with Llama-2-Chat, respectively, and report the evaluation results in Table 14 in Appendix B.2. We evaluate \(6\) representative merging techniques implemented in _mergekit_[16], including linear interpolation [59], SLERP [49], Model Stock [24], Task Arithmetic [22], DARE [62], and TIES [63]. By merging Llama-2-chat and Vicuna, the merged model achieves better performance compared to any single model, while merging Llama-2-chat and CodeLlama fails to outperform all single models and may even lead to a significant drop in performance, which is also mentioned by Xu et al. [62]. The results indicate the potential severe performance drop when including un-mergeable new model in merging (e.g. CodeLlama). Even if it is obtained from the same pre-trained checkpoint. Such failure case motivates us to design the strategy to automatically select models for merging, and exclude the models that are unable to merge.

In the following paragraphs, we explore several solutions tailored for large-scale model merging. These variations address different resource and speed requirements. The introduction of these methods is organized around answering the following key questions.

**Q1: Does handcrafted rules apply to automated model selection and which one performs best?**

**A: Yes, by a greedy search approach.** In this section, we explore three potential heuristics for model selection and report the results in Figure 4(a). We include the performance of the "best single model" (the model participant before merging that achieves the best averaged performance). We additionally validate the performance of heuristic-based merging technique, which are detailed in Section 3.2. As indicated by the results, the merging technique based on _Heuristic-Coefficient_ yields

Figure 4: (a) Comparison between different Heuristic Strategies on Which12, Which8, Which4. (b) Comparison of different model merging methods in Evolutionary Strategy.

consistently superior performance when the model zoo is large. For Which4, _Heuristic-Average_ achieved better performance, while _Heuristic-Coefficient_ performed poorly. This is primarily because the domain-specific models in Which4 exhibit similar performances and are indispensable.

**Q2: How to utilize Evolutionary Strategy for coefficient optimization in model merging?**

We divide the problem into the following sub-questions: (_i_) Which merging method is most compatible with Evolutionary Strategy? (_ii_) Can finer-grained optimization lead to a better merged model? (_iii_) How to efficiently merge in a large model zoo? For (_i_), **A: simpler methods such as Linear and Task Arithmetic are more competitive.** We compared four methods: Linear, Task Arithmetic, DARE, and TIES. As shown in Figure 4(b), Linear merging consistently achieves great results. However, when the parameters to be optimized are small, Task Arithmetic performs slightly better than Linear. Under a fixed computational budget, due to the doubling of parameters to be optimized, DARE and TIES exhibit slightly lower performance compared to other methods. For (_ii_), **A: Yes, but we need a larger computational budget.** We group adjacent \(n\) decoder layers together, where they share the same coefficients. The group size \(n\in[32,8,4,1]\). When \(n=8\), better results were achieved compared to \(n=32\), as shown in Table 17. However, as we further decreased the group size, the performance slightly declined. This could be attributed to our relatively small budget. For (_iii_), **A: Use Heuristic Strategy to roughly search for coefficients and then fine-tune the coefficients using Evolutionary Strategy.** As shown in Table 18, the combination of the two strategies resulted in better results with fewer trials. For implementation details, please refer to Appendix A.2.

### Implementation Details for Mixture Model Zoo and Router Initialization.

In Mixture Bench, we experiment with Which2 and Which4 model settings. For router design, we mainly adopt a training-free linear layer router initialized from the prompt vector, as previous studies have demonstrated its effectiveness in the zero-shot MoE model [16]. For specific prompt settings, we refer to the Beyonder model series 4. For the routing algorithm, we use Top-\(1\) routing for Which2 and _Block level mixture_ and _Model-level mixture_ for Which4, and Top-\(2\) for Which4 _FFN level mixture._

Footnote 4: https://huggingface.co/mlabonne/Beyonder-4x7B-v2

Post-mixture training.For _MLP router_ that are randomly initialized, we fine-tune the model by language modeling on the GPT4All dataset [5], only updating the router. We use the GPT4All [5] dataset for post-mixture router training, which is under Apache 2.0 License. For all the router training experiments, we apply the batch size of \(128\), a cosine learning rate scheduler, the learning rate of \(5e-5\), and the epochs of \(1\).

Mixture Method Abbreviations.To simplify the description, we use abbreviations to denote different mixture methods, as in Table 2.

### Model Mixture Benchmark Results

In this section, we attempt to answer five main research questions about mixture variants: mixture level, router design, router input, and hybrid merging. We also explore the mixing of very different models that cannot be merged as the previous probe in our next Model-GLUE recipe that combines merging and blending for LLM scaling.

Q1: At which level does the model mixture manifest its utmost effectiveness?

A: Model level mixture is consistently better.Our comparative analysis of the {FFN, block, model} level mixture, all employing the linear router and the sample routing strategy as presented in Table 3, consistently demonstrates the superiority of the _Model level mixture_ under Which2 and Which4 setting. This could be attributed to the design

\begin{table}
\begin{tabular}{c|c|c c c} \hline \hline \multirow{2}{*}{Model} & \multirow{2}{*}{MRC} & \multirow{2}{*}{WinoGrande} & \multirow{2}{*}{MMLU} & \multirow{2}{*}{GSMSK} & \multirow{2}{*}{MBPP} & \multirow{2}{*}{HumanEval} & \multirow{2}{*}{Average} \\ \cline{1-1} \cline{5-6}  & & & & Which2 & & \\ \hline Best Single Model & \(54.27\%\) & \(71.51\%\) & \(47.24\%\) & \(21.30\%\) & \(18.00\%\) & \(13.06\%\) & \(37.68\%\) \\ \hline F-L-S & \(52.82\%\) & \(70.80\%\) & \(50.04\%\) & \(\mathbf{23.12\%}\) & \(19.00\%\) & \(17.68\%\) & \(38.91\%\) \\ B-L-S & \(52.73\%\) & \(70.01\%\) & \(49.90\%\) & \(19.94\%\) & \(18.84\%\) & \(15.85\%\) & \(37.88\%\) \\ M-L-S & \(\mathbf{54.44\%}\) & \(\mathbf{72.38\%}\) & \(\mathbf{50.51\%}\) & \(22.21\%\) & \(\mathbf{20.00\%}\) & \(\mathbf{20.73\%}\) & \(\mathbf{40.04\%}\) \\ \hline \multicolumn{5}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ \cline{1-1} \cline{5-6}  & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ \cline{1-1} \cline{5-6}  & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ \hline \hline \end{tabular}
\end{table}
Table 3: Comparison of different mixture levels. For each task in each model zoo, we highlight the performance best in each model zoo in **bold**.

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline \multirow{2}{*}{Model} & \multirow{2}{*}{MRC} & \multirow{2}{*}{WinoGrande} & \multirow{2}{*}{MMLU} & \multirow{2}{*}{GSMSK} & \multirow{2}{*}{MBPP} & \multirow{2}{*}{HumanEval} & \multirow{2}{*}{Average} \\ \cline{1-1} \cline{5-6}  & & & Which2 & & & \\ \hline Best Single Model & \(54.27\%\) & \(71.51\%\) & \(47.24\%\) & \(21.30\%\) & \(18.00\%\) & \(13.06\%\) & \(37.68\%\) \\ \hline F-L-S & \(52.82\%\) & \(70.80\%\) & \(50.04\%\) & \(\mathbf{23.12\%}\) & \(19.00\%\) & \(17.68\%\) & \(38.91\%\) \\ B-L-S & \(52.73\%\) & \(70.01\%\) & \(49.90\%\) & \(19.94\%\) & \(18.84\%\) & \(15.85\%\) & \(37.88\%\) \\ M-L-S & \(\mathbf{54.44\%}\) & \(\mathbf{72.38\%}\) & \(\mathbf{50.51\%}\) & \(22.21\%\) & \(\mathbf{20.00\%}\) & \(\mathbf{20.73\%}\) & \(\mathbf{40.04\%}\) \\ \hline \multicolumn{5}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ \cline{1-1} \cline{5-6}  & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ \hline Best Single Model & \(\mathbf{55.03\%}\) & \(73.72\%\) & \(\mathbf{48.33\%}\) & \(\mathbf{42.26\%}\) & \(17.80\%\) & \(13.41\%\) & \(38.70\%\) \\ \hline F-L-S & \(53.75\%\) & \(73.88\%\) & \(47.97\%\) & \(34.87\%\) & \(\mathbf{21.80\%}\) & \(\mathbf{23.17\%}\) & \(42.57\%\) \\ B-L-S & \(52.65\%\) & \(\mathbf{74.66\%}\) & \(47.05\%\) & \(21.15\%\) & \(20.40\%\) & \(14.63\%\) & \(38.42\%\) \\ M-L-S & \(49.06\%\) & \(72.14\%\) & \(41.81\%\) & \(\mathbf{60.05\%}\) & \(17.60\%\) & \(15.24\%\) & \(\mathbf{42.65\%}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Model mixture methods and their abbreviations used in our study. Methods applicable for models with distinct architectures are highlighted in gray.

that _Model Level Mixture_ route each sample to one expert model, thereby avoiding the conflicts between different expert models and maximizing the expertise of the most appropriate experts. Since the experts are not derived from the same pre-training process, directly merging their inconsistent representation spaces will affect the performance of the mixture model, with more expert parameters leading to worse results. This is especially evident for _Block-level Mixture_, as the routing is performed at each transformer layer and the representation is fed into different expert blocks in series, causing confusion when switching between different expert knowledge.

**Q2: Does more complex router design brings better results?**

**A: Not necessary, as the linear router outperforms the MLP router.** From Table 4, the performances of the _linear router_ without additional training slightly surpass _MLP router_ models, _i.e._, F-L-T over F-M-T, B-L-S over B-M-S. Specifically, _linear router_ models are better at math and coding datasets, validating prompt vector is effective in assorting samples from different domains, which is otherwise too implicit to learn via direct language modeling.

**Q3: Does model mixture directly works on unmergeable models?**

**A: No.** We directly apply the setting of Which2 _Model level mixture_ to Llama-2-7b-chat and CrystalChat, an unmergeable model pair with different architectures and initialization. As shown in Table 5, the performance is slightly behind the best single model. This may be due to simple prompts and direct mixture, as it fails to coordinate the divergence between drastically different models. We evaluate more complex prompts for the same model pair and the mixture model outperforms, see Table 19 for more information.

**Q4: Which router input is better, token-level or sample-level?**

**A: Not quite different. Token input suits a mixture of the same domain models.** Table 6 shows the performance token-based and sample-based routing are pretty close. In particular, for Which2 and Which4 (Chat) where models are all trained for general chatting purposes, token routing outperforms, whereas sample routing is better for default Which4 (Domain) with differently specialized models. This may result from divergence of model knowledge and representation spaces will cause conflicts in fine-grained token routing.

**Q5: Is it feasible for hybrid mixtures to provide enhancements?**

**A: Yes.** Our experiments on F-L-T with _v.s._ without the hybrid mixture, as detailed in Table 7, demonstrate that the hybrid mixture significantly improves performance on average and simultaneously reduces the memory overhead during inference. This improvement may be attributed to the

\begin{table}
\begin{tabular}{c|c c c c c c c} \hline \hline Model & ARC & WinoGrande & MMLU & GSM8K & MBPP & HumanEval & Average \\ \hline \multicolumn{8}{c}{Which2} \\ \hline Best Single Model & \(\mathbf{54.27\%}\) & \(\mathbf{71.5\%}\) & \(\mathbf{47.24\%}\) & \(21.30\%\) & \(18.00\%\) & \(13.06\%\) & \(37.68\%\) \\ \hline F-L-T & \(53.41\%\) & \(70.48\%\) & \(\mathbf{50.74\%}\) & \(\mathbf{23.28\%}\) & \(\mathbf{20.80\%}\) & \(16.46\%\) & \(\mathbf{39.20\%}\) \\ F-L-S & \(52.82\%\) & \(70.80\%\) & \(50.04\%\) & \(\mathbf{23.12\%}\) & \(19.00\%\) & \(\mathbf{17.68\%}\) & \(38.91\%\) \\ \hline \multicolumn{8}{c}{which4} \\ \hline Best Single Model & \(55.03\%\) & \(73.72\%\) & \(\mathbf{48.33\%}\) & \(\mathbf{24.26\%}\) & \(17.80\%\) & \(13.41\%\) & \(38.70\%\) \\ \hline Chat F-L-T & \(\mathbf{55.63\%}\) & \(\mathbf{72.77\%}\) & \(\mathbf{50.28\%}\) & \(\mathbf{23.88\%}\) & \(20.00\%\) & \(\mathbf{22.56\%}\) & \(\mathbf{40.85\%}\) \\ Chat F-L-S & \(53.75\%\) & \(70.96\%\) & \(49.78\%\) & \(20.32\%\) & \(\mathbf{20.40\%}\) & \(20.12\%\) & \(39.22\%\) \\ \hline Domain F-L-T & \(\mathbf{55.72\%}\) & \(\mathbf{74.11\%}\) & \(48.32\%\) & \(30.17\%\) & \(\mathbf{22.00\%}\) & \(20.12\%\) & \(41.74\%\) \\ Domain F-L-S & \(53.75\%\) & \(73.88\%\) & \(47.97\%\) & \(\mathbf{34.87\%}\) & \(21.80\%\) & \(\mathbf{23.17\%}\) & \(\mathbf{42.57\%}\) \\ \hline \hline \end{tabular}
\end{table}
Table 6: Comparison of different router input designs. Which4 includes one group with chatting models (Chat) and another with different domain models (Domain). We highlight the best performing mixture methods in **bold**.

\begin{table}
\begin{tabular}{c|c c c c c c c} \hline \hline Model & ARC & WinoGrande & MMLU & GSM8K & MBPP & HumanEval & Average \\ \hline F-L-T & \(53.41\%\) & \(70.48\%\) & \(\mathbf{50.74\%}\) & \(\mathbf{23.28\%}\) & \(\mathbf{20.80\%}\) & \(16.46\%\) & \(\mathbf{39.20\%}\) \\ F-M-T & \(\mathbf{53.58\%}\) & \(\mathbf{72.06\%}\) & \(50.01\%\) & \(21.92\%\) & \(17.40\%\) & \(\mathbf{17.68\%}\) & \(38.78\%\) \\ \hline B-L-S & \(\mathbf{52.73\%}\) & \(70.01\%\) & \(\mathbf{49.90\%}\) & \(\mathbf{19.94\%}\) & \(\mathbf{18.84\%}\) & \(\mathbf{15.85\%}\) & \(\mathbf{37.88\%}\) \\ D-S & \(\mathbf{51.53\%}\) & \(\mathbf{70.56\%}\) & \(49.41\%\) & \(\mathbf{19.94\%}\) & \(\mathbf{16.60\%}\) & \(14.02\%\) & \(37.01\%\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Comparison between linear and MLP routers on Which2 setting. We highlight better performance within each pair in **bold**.

\begin{table}
\begin{tabular}{c|c c c c c c c} \hline \hline Model & ARC & WinoGrande & MMLU & GSM8K & MBPP & HumanEval & Average \\ \hline F-L-T & \(53.41\%\) & \(70.48\%\) & \(\mathbf{50.74\%}\) & \(\mathbf{23.28\%}\) & \(\mathbf{20.80\%}\) & \(16.46\%\) & \(\mathbf{39.20\%}\) \\ F-M-T & \(\mathbf{53.58\%}\) & \(\mathbf{72.06\%}\) & \(50.01\%\) & \(21.92\%\) & \(17.40\%\) & \(\mathbf{17.68\%}\) & \(38.78\%\) \\ \hline B-L-S & \(\mathbf{52.73\%}\) & \(70.01\%\) & \(\mathbf{49.90\%}\) & \(\mathbf{19.94\%}\) & \(\mathbf{18.84\%}\) & \(\mathbf{15.85\%}\) & \(\mathbf{37.88\%}\) \\ D-S & \(\mathbf{51.53\%}\) & \(\mathbf{70.56\%}\) & \(49.41\%\) & \(\mathbf{19.94\%}\) & \(\mathbf{16.60\%}\) & \(14.02\%\) & \(37.01\%\) \\ \hline \hline \end{tabular}
\end{table}
Table 7: Comparison between F-L-T methods with and without hybrid mixture technique. We highlight the best performing mixture methods in **bold**.

\begin{table}
\begin{tabular}{c|c c c c c c c} \hline \hline Model & ARC & WinoGrande & MMLU & GSM8K & MBPP & HumanEval & Average \\ \hline Best Single Model & \(\mathbf{52.05\%}\) & \(69.46\%\) & \(\mathbf{50.77\%}\) & \(27.22\%\) & \(\mathbf{39.60\%}\) & \(\mathbf{35.98\%}\) & \(\mathbf{45.85\%}\) \\ \hline M-L-S & \(50.68\%\) & \(\mathbf{69.77\%}\) & \(50.08\%\) & \(\mathbf{27.82\%}\) & \(33.80\%\) & \(30.48\%\) & \(43.77\%\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Comparison of the mixture of a unmergeable model pair (Llama-2-7b-chat and CrystalChat). We highlight the better performance in **bold**.

higher sensitivity of the initial transformer blocks. Avoiding using MoE for these blocks can yield performance gains, as suggested by a few previous works as well [12; 41]. Surprisingly, our results show that the hybrid F-L-T model consistently outperforms the standard F-L-T on _math_ and _code_ tasks. Our further analysis indicates that this improvement might be because of the conversational nature of the content in GSM8K, MBPP, and HumanEval datasets, which appears to challenge the routing mechanisms within the initial transformer blocks, leading to ineffective expert specialization.

## 5 Superior Recipes to Aggregate LLM Knowledge

### Model Merging _v.s._ Mixture

Q1: For a mergeable model zoo, how should we choose between merging and mixture?For limited computational resources and similar models, merging is always a simple and effective method. For the domain-specific models, mixture can bring greater improvements.

Detailed results are presented in Table 8. For Which4 (Domain), due to the appropriately designed linear routers, model mixture can fully leverage various domain-specific models, thus slightly outperforming merging. For Which4 (Chat), we adopt the optimal settings from Which4 (Domain) and only change the model zoo. Since individual models do not exhibit superior capabilities in a single domain, it is challenging to design suitable routers at a low cost. Therefore, mixture performed significantly worse compared to merging. Furthermore, although combining the homogeneous models in Which4 (Chat) brings some improvement, we can see that Which4 (Domain) overall outperforms Which4 (Chat). Therefore, increasing the diversity among the models will make a greater contribution to the combined model.

### Model-GLUE: selective merging then model mixture for better LLM scaling

Q2: How to combine models with greater differences in an extensive and varied model zoo?

In Which16, a larger and more diverse model zoo, some models cannot be merged due to structural differences and models that would degrade in performance when merged with other models. Therefore, we first cluster the models based on cosine similarity. Within each mergeable family, we perform either merging or mixture. We initially employ heuristic strategies of merging and report the best results (_i.e._, Full Merging) in Table 9. The Llama-2 family (_i.e._, Which12) consists of up to \(12\) models, so directly combining them through the mixture is inefficient. Thus, we only consider models selected by merging and report the results of F-L-T Mixture. From Table 9, we can observe that Full Merging outperforms F-L-T Mixture.

Therefore, we selected Full Merging as the representative model for the Llama-2 family and combined it with other models that could not be merged by model mixture. On average, the Model-GLUE demonstrates a \(5.61\%\) improvement over the Best Single Model. More details are presented in Appendix A.4.

## 6 Discussion with Other LLM Aggregation Techniques

Thus far, we mainly focus on two LLM aggregation techniques: model merging and mixture. In this section, we discuss other potential techniques that could help scaling existing LLMs.

Model Stacking.Research has demonstrated that stacking a model itself can accelerate training convergence as opposed to training a model of double the size from scratch [17; 18; 56; 60; 28]. This concept can be extended naturally to stack multiple models as one larger model. Our experimental results indicate that model stacking with lightweight fine-tuning can yield superior performance compared to various merging and mixture models. For instance, stacking 7B Llama-2-chat and Vicuna can achieve \(\geq 55\%\) on the MMLU benchmark. When compared to model mixture, model

\begin{table}
\begin{tabular}{c|c c c c c c c} \hline \hline Model & ARC & WinoGrande & MMLU & GSM8K & MBPP & HumanEval & Average \\ \hline Best Single Model & \(46.7\%\) & \(64.33\%\) & \(46.33\%\) & \(\mathbf{62.40\%}\) & \(42.00\%\) & \(31.10\%\) & \(48.82\%\) \\ \hline Full Merging & \(\mathbf{55.12\%}\) & \(\mathbf{73.64\%}\) & \(50.13\%\) & \(39.35\%\) & \(21.80\%\) & \(21.34\%\) & \(43.56\%\) \\ F-L-T Mixture & \(54.69\%\) & \(73.32\%\) & \(48.74\%\) & \(35.18\%\) & \(22.60\%\) & \(21.34\%\) & \(42.65\%\) \\ \hline Model-GLUE & \(51.62\%\) & \(70.56\%\) & \(\mathbf{51.85\%}\) & \(53.53\%\) & \(\mathbf{47.20\%}\) & \(\mathbf{51.83\%}\) & \(\mathbf{54.43\%}\) \\ \hline \hline \end{tabular}
\end{table}
Table 9: Comparison between the best single model, Full Merging, Full Mixture and our Model-GLUE.

\begin{table}
\begin{tabular}{c|c c c c c c c} \hline \hline Model & ARC & WinoGrande & MMLU & GSM8K & MBPP & HumanEval & Average \\ \hline Best Single Model & \(55.03\%\) & \(73.72\%\) & \(48.18\%\) & \(24.03\%\) & \(17.80\%\) & \(13.41\%\) & \(38.70\%\) \\ \hline \multicolumn{8}{c}{Which4 (Domain)} \\ \hline Merging & \(\mathbf{54.01\%}\) & \(73.64\%\) & \(\mathbf{74.39\%}\) & \(43.75\%\) & \(\mathbf{22.40\%}\) & \(\mathbf{21.95\%}\) & \(43.86\%\) \\ Mixture & \(\mathbf{54.80\%}\) & \(\mathbf{74.11\%}\) & \(\mathbf{48.23\%}\) & \(\mathbf{49.81\%}\) & \(18.40\%\) & \(18.29\%\) & \(\mathbf{43.95\%}\) \\ \hline \multicolumn{8}{c}{Which4 (Chat)} \\ \hline Merging & \(\mathbf{56.23\%}\) & \(\mathbf{73.72\%}\) & \(\mathbf{50.51\%}\) & \(\mathbf{25.85\%}\) & \(\mathbf{21.00\%}\) & \(\mathbf{21.95\%}\) & \(\mathbf{41.54\%}\) \\ Mixture & \(\mathbf{53.75\%}\) & \(70.98\%\) & \(49.80\%\) & \(19.94\%\) & \(19.80\%\) & \(20.73\%\) & \(39.16\%\) \\ \hline \hline \end{tabular}
\end{table}
Table 8: Comparison between the best merging approach _v.s._ the best mixture approach on Which4 (Domain) and Which4 (Chat).

stacking offers less flexibility in terms of model choices. Although the resulting architecture is more standardized than MoE, increasing the model depth through stacking also results in higher latency than mixture models where subnetworks infer in parallel. Additionally, model stacking does not simplify the design space, such as determining whether, which, and how many layers should be dropped when stacking two heterogeneous models. We conducted a preliminary investigation employing model stacking techniques to address two primary research questions: (1) Can model stacking effectively combine the capabilities of two distinct models and surpass the performance of self-stacking a single model? (2) What is the impact of layer dropping on stacking performance?

Specifically, we examine the relationship between the number of dropped layers (\(K\)) and the resulting downstream task accuracy. To this end, we selected 7B Llama-2-Chat and Vicuna as the base models and fine-tuned the stacked models for 10 billion tokens. The obtained results are presented in Table 10. In the initial two rows, we report the performance of the two base models, revealing that Llama and Vicuna exhibit advantages on different datasets. In the subsequent two rows, we observe that stacking dissimilar models generally outperforms self-stacked models, and the weaknesses of one model can be compensated for by another stronger one. Moving forward, we explored the effects of varying the number of dropped layers. Our findings indicate that even when dumping half of each model (\(K=16\)), the stacked 7B models can still significantly enhance performance across tasks.

Model Communication.Model communication [61; 31; 33] is a framework that enables the development of LLM applications through the use of multiple conversable agents that collaborate to complete tasks. This approach allows developers to design complex LLM application workflows as multi-agent conversations, where agents with various roles and capabilities, driven by LLMs, tools, or human inputs, interact with each other. Unlike model merging, mixture, and stacking techniques, LLM communication is orthogonal to the primary focus of this paper because it does not modify the model weights; instead, it leverages the in-context learning and conversational capabilities of LLMs to coordinate agents. An empirical comparison with this class of methods is beyond the scope of this study and will be explored in future research.

## 7 Limitations

For LLM scaling studies, while empirical evidence suggests that increasing model size, data volume, and computational complexity leads to better performance, there is little theoretical clarity on the exact mechanisms behind these improvements. Second, although scaling laws suggest that performance continues to improve as models get larger, recent evidence indicates that scaling may lead to diminishing returns beyond a certain point. In addition, our work focuses on benchmarking results, while the reasons why model merging improves performance could be further enhanced by post hoc analysis, such as examining parameter distribution and similarity during model operations.

## 8 Conclusion

In this paper, we explore the scaling LLM based on a model zoo of pre-trained LLMs within the real world. We first benchmark state-of-the-art LLM merging, mixture, and model stacking. Based on previous findings, we then propose a novel LLM scaling framework, Model-GLUE. Specifically, we scale up the model zoo closely examine the existing model merging techniques, and conclude the selective merging techniques based on heuristics and learnable algorithms. Further, we investigate variants of Mixture-of-Experts for combining LLMs and suggest it can serve as an alternative to merging failure cases. Finally, we integrate selective merging strategies with model mixture techniques, presenting this as a comprehensive solution for scaling a diverse array of LLM collections. Future works will include model stacking and communication to our Model-GLUE framework.

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline Model & ARC & WinoSGrande & MMLU & Hellaswag & TruthfulQA \\ \hline Llama-2-chat & \(54.10\%\) & \(71.27\%\) & \(47.28\%\) & \(78.71\%\) & \(45.32\%\) \\ Vicuna & \(53.75\%\) & \(70.56\%\) & \(49.78\%\) & \(77.19\%\) & \(50.36\%\) \\ \hline Llama / Llama (\(K=8\)) & \(53.92\%\) & \(69.14\%\) & \(52.76\%\) & \(73.74\%\) & \(46.36\%\) \\ Llama / Vicuna (\(K=8\)) & \(56.14\%\) & \(70.80\%\) & \(55.20\%\) & \(73.67\%\) & \(46.84\%\) \\ \hline Llama / Vicuna (\(K=12\)) & \(55.42\%\) & \(69.45\%\) & \(53.55\%\) & \(73.62\%\) & \(45.59\%\) \\ Llama / Vicuna (\(K=16\)) & \(54.35\%\) & \(69.69\%\) & \(52.52\%\) & \(73.75\%\) & \(45.92\%\) \\ Llama / Vicuna (\(K=20\)) & \(39.59\%\) & \(61.33\%\) & \(44.93\%\) & \(62.10\%\) & \(42.90\%\) \\ Llama / Vicuna (\(K=24\)) & \(28.15\%\) & \(52.88\%\) & \(25.51\%\) & \(43.07\%\) & \(39.10\%\) \\ \hline \hline \end{tabular}
\end{table}
Table 10: Comparison of different model stacking configurations.

## References

* [1] Naman Agarwal, Pranjal Awasthi, Satyen Kale, and Eric Zhao. Stacking as accelerated gradient descent. _arXiv preprint arXiv:2403.04978_, 2024.
* [2] Samuel K Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git re-basin: Merging models modulo permutation symmetries. _arXiv preprint arXiv:2209.04836_, 2022.
* [3] Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: A next-generation hyperparameter optimization framework. In _Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining_, pages 2623-2631, 2019.
* [4] Takuya Akiba, Makoto Shing, Yujin Tang, Qi Sun, and David Ha. Evolutionary optimization of model merging recipes, 2024.
* [5] Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin M. Schmidt, Adam Treat, and Andriy Mulyar. Gpt4all-j: An apache-2 licensed assistant-style chatbot, 2023.
* [6] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language models. _ArXiv_, abs/2108.07732, 2021.
* [7] Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. _arXiv preprint arXiv:2310.10631_, 2023.
* [8] Aibek Bekbayev, Sungbae Chun, Yerzat Dulat, and James Yamazaki. The poison of alignment, 2023.
* [9] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, David W. Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol, Igor Babuschkin, Suchir Balaji, Shantanu Jain, Andrew Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. _ArXiv_, abs/2107.03374, 2021.
* [10] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. _ArXiv_, abs/1803.05457, 2018.
* [11] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reitichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. _ArXiv_, abs/2110.14168, 2021.
* [12] Damai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and Wenfeng Liang. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models, 2024.
* [13] Ning Ding, Yulin Chen, Ganqu Cui, Xingtai Lv, Weilin Zhao, Ruobing Xie, Bowen Zhou, Zhiyuan Liu, and Maosong Sun. Mastering text, code and math simultaneously via fusing highly specialized language models, 2024.
* [14] Jesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A. Smith, Nicole DeCario, and Will Buchanan. Measuring the carbon intensity of ai in cloud instances. _Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency_, 2022.

* Fedus et al. [2022] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. _J. Mach. Learn. Res._, 23:120:1-120:39, 2022.
* Goddard et al. [2024] Charles Goddard, Shamane Siriwardhana, Malikeh Ehghaghi, Luke Meyers, Vlad Karpukhin, Brian Benedict, Mark McQuade, and Jacob Solawetz. Arcee's mergekit: A toolkit for merging large language models. _arXiv preprint arXiv:2403.13257_, 2024.
* Gong et al. [2019] Linyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and Tieyan Liu. Efficient training of bert by progressively stacking. In _International conference on machine learning_, pages 2337-2346. PMLR, 2019.
* Gu et al. [2020] Xiaotao Gu, Liyuan Liu, Hongkun Yu, Jing Li, Chen Chen, and Jiawei Han. On the transformer growth for progressive bert training. _arXiv preprint arXiv:2010.12562_, 2020.
* Hansen [2006] Nikolaus Hansen. The cma evolution strategy: a comparing review. _Towards a new evolutionary computation: Advances in the estimation of distribution algorithms_, pages 75-102, 2006.
* Hendrycks et al. [2020] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Xiaodong Song, and Jacob Steinhardt. Measuring massive multitask language understanding. _ArXiv_, abs/2009.03300, 2020.
* Hu et al. [2022] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=mZeVKeeFYf9.
* Ilharco et al. [2023] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic, 2023.
* Imfeld et al. [2023] Moritz Imfeld, Jacopo Graldi, Marco Giordano, Thomas Hofmann, Sotiris Anagnostidis, and Sidak Pal Singh. Transformer fusion with optimal transport. _arXiv preprint arXiv:2310.05719_, 2023.
* Jang et al. [2024] Dong-Hwan Jang, Sangdoo Yun, and Dongyoon Han. Model stock: All we need is just a few fine-tuned models, 2024.
* Jiang et al. [2024] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lelio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Theophile Gervet, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mixtral of experts, 2024.
* Jin et al. [2023] Xisen Jin, Xiang Ren, Daniel Preotiuc-Pietro, and Pengxiang Cheng. Dataless knowledge fusion by merging weights of language models, 2023.
* Kaplan et al. [2020] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu, and Dario Amodei. Scaling laws for neural language models. _ArXiv_, abs/2001.08361, 2020.
* Kim et al. [2023] Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, et al. Solar 10.7 b: Scaling large language models with simple yet effective depth up-scaling. _arXiv preprint arXiv:2312.15166_, 2023.
* Lee et al. [2023] Ariel N. Lee, Cole J. Hunter, and Nataniel Ruiz. Platypus: Quick, cheap, and powerful refinement of llms. 2023.
* Lepikhin et al. [2020] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam M. Shazeer, and Z. Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. _ArXiv_, abs/2006.16668, 2020.
* Li et al. [2024] Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for" mind" exploration of large language model society. _Advances in Neural Information Processing Systems_, 36, 2024.

* [32] Hanxue Liang, Zhiwen Fan, Rishov Sarkar, Ziyu Jiang, Tianlong Chen, Kai Zou, Yu Cheng, Cong Hao, and Zhangyang Wang. M3vit: Mixture-of-experts vision transformer for efficient multi-task learning with model-accelerator co-design. _ArXiv_, abs/2210.14793, 2022.
* [33] Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. Encouraging divergent thinking in large language models through multi-agent debate. _arXiv preprint arXiv:2305.19118_, 2023.
* [34] Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller, Yonghao Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranjan, Zhiqiang Shen, Xueguang Ren, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov, Tim Baldwin, and Eric P. Xing. Llm360: Towards fully transparent open-source llms, 2023.
* [35] Michael Matena and Colin Raffel. Merging models with fisher-weighted averaging, 2022.
* [36] Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, and Shayne Longpre. Octopack: Instruction tuning code large language models. _arXiv preprint arXiv:2308.07124_, 2023.
* [37] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4, 2023.
* [38] Vaishnavh Nagarajan and J. Zico Kolter. Uniform convergence may be unable to explain generalization in deep learning, 2021.
* [39] OpenAI. GPT-4 technical report. volume abs/2303.08774, 2023.
* [40] Felipe Maia Polo, Lucas Weber, Leshem Choshen, Yuekai Sun, Gongjun Xu, and Mikhail Yurochkin. tinybenchmarks: evaluating llms with fewer examples. _ArXiv_, abs/2402.14992, 2024.
* [41] Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale, 2022.
* [42] Sashank J Reddi, Sobhan Miryoosefi, Stefani Karp, Shankar Krishnan, Satyen Kale, Seungyeon Kim, and Sanjiv Kumar. Efficient training of language models using few-shot learning. In _International Conference on Machine Learning_, pages 14553-14568. PMLR, 2023.
* [43] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, Artyom Kozhevnikov, I. Evtimov, Joanna Bitton, Manish P Bhatt, Cristian Canton Ferrer, Aaron Grattafori, Wenhan Xiong, Alexandre D'efossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code. _ArXiv_, abs/2308.12950, 2023.
* [44] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jeremy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafori, Wenhan Xiong, Alexandre Defossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code, 2024.
* [45] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. An adversarial winograd schema challenge at scale. 2019.
* [46] Sina Semnani, Violet Yao, Heidi Zhang, and Monica Lam. WikiChat: Stopping the hallucination of large language model chatbots by few-shot grounding on Wikipedia. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 2387-2413, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.157. URL https://aclanthology.org/2023.findings-emnlp.157.

* [47] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer, 2017.
* [48] Yikang Shen, Zheyu Zhang, Tianyou Cao, Shawn Tan, Zhenfang Chen, and Chuang Gan. Moduleformer: Learning modular large language models from uncurated data. _ArXiv_, abs/2306.04640, 2023.
* [49] Ken Shoemake. Animating rotation with quaternion curves. In _Proceedings of the 12th Annual Conference on Computer Graphics and Interactive Techniques_, SIGGRAPH '85, page 245-254, New York, NY, USA, 1985. Association for Computing Machinery. ISBN 0897911660.
* [50] Sainbayar Sukhbaatar, Olga Golovneva, Vasu Sharma, Hu Xu, Xi Victoria Lin, Baptiste Roziere, Jacob Kahn, Daniel Li, Wen tau Yih, Jason Weston, and Xian Li. Branch-train-mix: Mixing expert llms into a mixture-of-experts llm, 2024.
* [51] Migel Tissera. Synthia-70b-v1.2b: Synthetic intelligent agent. https://huggingface.co/migtissera/Synthia-13B, 2023.
* [52] Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yung Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Scheletlen, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. _ArXiv_, abs/2307.09288, 2023.
* [53] Neha Verma and Maha Elbayad. Merging text transformer models from different initializations. _arXiv preprint arXiv:2403.00986_, 2024.
* [54] Fanqi Wan, Xinting Huang, Deng Cai, Xiaojun Quan, Wei Bi, and Shuming Shi. Knowledge fusion of large language models. In _The Twelfth International Conference on Learning Representations_, 2024.
* [55] Hongyi Wang, Felipe Maia Polo, Yuekai Sun, Souvik Kundu, Eric Xing, and Mikhail Yurochkin. Fusing models with complementary expertise, 2023.
* [56] Peihao Wang, Rameswar Panda, Lucas Torroba Hennigen, Philip Greengard, Leonid Karlinsky, Rogerio Feris, David Daniel Cox, Zhangyang Wang, and Yoon Kim. Learning to grow pretrained models for efficient transformer training. _arXiv preprint arXiv:2303.00980_, 2023.
* [57] Peihao Wang, Rameswar Panda, and Zhangyang Wang. Data efficient neural scaling law via model reusing. In _International Conference on Machine Learning_, pages 36193-36204. PMLR, 2023.
* [58] Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. Magicoder: Source code is all you need, 2023.
* [59] Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time, 2022.
* [60] Chengyue Wu, Yukang Gan, Yixiao Ge, Zeyu Lu, Jiahao Wang, Ye Feng, Ping Luo, and Ying Shan. Llama pro: Progressive llama with block expansion. _arXiv preprint arXiv:2401.02415_, 2024.

* [61] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. _arXiv preprint arXiv:2308.08155_, 2023.
* [62] Zhengqi Xu, Ke Yuan, Huiqiong Wang, Yong Wang, Mingli Song, and Jie Song. Training-free pretrained model merging. _arXiv preprint arXiv:2403.01753_, 2024.
* [63] Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. Ties-merging: Resolving interference when merging models, 2023.
* [64] Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. Language models are super mario: Absorbing abilities from homologous models as a free lunch, 2024.
* [65] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. _arXiv preprint arXiv:2309.12284_, 2023.
* [66] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.
* [67] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. _Advances in Neural Information Processing Systems_, 36, 2024.
* [68] Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam M. Shazeer, and William Fedus. Designing effective sparse expert models. _2022 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)_, pages 1044-1044, 2022.

### Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Our abstract and introduction in Section 1 accurately reflect the paper's contributions and scope?. 2. Did you describe the limitations of your work? In Section 6 and 8 we discuss two other LLM scaling methods for future work and present some preliminary results. In Appendix 8 we discuss its limitations. 3. Did you discuss any potential negative societal impacts of your work? Our work focuses on foundational research and is not directly related to societal impacts. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? We have read and follow the ethics review guidelines.
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? Our work does not include theoretical results. 2. Did you include complete proofs of all theoretical results? Our work does not include theoretical results.
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? We have provided our code, data (the way to get them), and instructions needed for reproducing in a GitHub repository. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? We have provided all the training details in our Appendix. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? Our main results do not contain randomness, and running multiple times with different random seeds leads to the same results. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? We have provided the computing resources we used in our Appendix.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? We have cited them. 2. Did you mention the license of the assets? We have mentioned the license of all the datasets we used in Appendix 8. 3. Did you include any new assets either in the supplemental material or as a URL? We do not release any new assets. 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? We only use public available data in this work. 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? The data we are using/curating does not contain any of these.
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? We did not use any crowdsourcing or conduct research with human subjects. 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? We did not use any crowdsourcing or conduct research with human subjects. 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? We did not use any crowdsourcing or conduct research with human subjects.

## Appendix A Implementation Details

### Detailed Algorithms of Heuristic Strategy of Model Merging

Heuristic (Average).We present the implementation details in Algorithm 1. The algorithm takes a mergable model family as input and generate a merged model as output. For each candidate model in input model family, we compute the accuracy of the temporary merged model, generated by the union of this candidate model and the previously selected model, on the proxy dataset, and the candidate that brings no harm to the accuracy will be selected for the final merged model. Each weight of the merged model is generated by averaging the corresponding weights of all the selected models.

```
0: A mergable family \(\{w_{1},...,w_{n}\}\) (sorted in decreasing order of Acc(\(w_{i}\))).
0:\(\textit{merged\_model}\)
1:\(\textit{models\_to\_merge}\leftarrow\{w_{1}\}\)
2:\(\textit{merged\_model}\gets w_{1}\)
3:for\(i=2\) to \(n\)do
4:if ProxyAcc(AvgMerge(\(\textit{models\_to\_merge}\cup\{w_{i}\})\)) \(\geq\) ProxyAcc(\(\textit{merged\_model}\)) then
5:\(\textit{models\_to\_merge}\leftarrow\textit{models\_to\_merge}\cup\{w_{i}\}\)
6:\(\textit{merged\_model}\) = AvgMerge(\(\textit{models\_to\_merge}\))
7:endif
8:endfor
9:return\(\textit{merged\_model}\) ```

**Algorithm 1** Heuristic (Average)

Heuristic (Coefficient).We present the implementation details in Algorithm 2. Heuristic (Coefficient) builds upon Heuristic (Average) by combining the previously merged model with a new candidate using different coefficients in each round. To reduce the search space, we set the range of coefficient as 0.1, 0.2...0.9.

```
0: A mergable family \(\{w_{1},...,w_{n}\}\) (sorted in decreasing order of Acc(\(w_{i}\))), a list of coefficients \(\{0.1,0.2...,0.9\}\) to be searched when merging.
0: merged_model
1:\(\textit{coefficients}\leftarrow\{0.1,0.2...,0.9\}\)
2:\(\textit{merged\_model}\gets w_{1}\)
3:for\(i=2\) to \(n\)do
4:\(\textit{best\_acc}\), \(\textit{best\_c}\leftarrow\) ProxyAcc(\(\textit{merged\_model}\)), \(1.0\)
5:for\(c\) in coefficients do
6:if ProxyAcc(Merge(\(\textit{c}\), \(\textit{merged\_model}\), \(w_{i}\))) \(\geq\textit{best\_acc}\)then
7:\(\textit{best\_acc}\), \(\textit{best\_c}\leftarrow\) ProxyAcc(Merge(\(\textit{c}\), \(\textit{merged\_model}\), \(w_{i}\))), \(c\)
8:endif
9:endfor
10:\(\textit{merged\_model}\leftarrow\) Merge(\(\textit{best\_c}\), \(\textit{merged\_model}\), \(w_{i}\))
11:endfor
12:return\(\textit{merged\_model}\) ```

**Algorithm 2** Heuristic (Coefficient)

Heuristic (Similarity).We present the implementation details in Algorithm 3. We use the average similarity of all weights as the criterion for selecting models in each round. This algorithm selects the candidate model with the highest or lowest similarity and conducts a coefficient search to combine it with the previously merged model.

```
0: A mergeable family \(\{w_{1},...,w_{n}\}\) (sorted in decreasing order of Acc(\(w_{i}\))), a list of coefficients \(\{0.1,0.2...,0.9\}\) to be searched when merging.
0: merged_model
1:\(\textit{merged\_model}\gets w_{1}\)
2:\(\textit{remaining\_models}\leftarrow\{w_{2},...,w_{n}\}\)
3:for\(i=2\) to \(n\)do
4:\(\textit{best\_acc}\), \(\textit{best\_c}\leftarrow\mathrm{ProxyAcc}(\textit{merged\_model})\), \(1.0\)
5:\(\textit{candidate\_model}\leftarrow\mathrm{GetModelBySimilarity}(\textit{merged\_model}\), \(\textit{remaining\_models})\)
6:for\(c\) in coefficientsdo
7:if\(\mathrm{ProxyAcc}(\textit{Merge(c,\textit{merged\_model},\textit{candidate\_model})})\geq \textit{best\_acc}\)then
8:\(\textit{best\_acc}\), \(\textit{best\_c}\leftarrow\mathrm{ProxyAcc}(\mathrm{Merge(c,\textit{merged\_ model},\textit{candidate\_model})})\), \(c\)
9:endif
10:endfor
11:\(\textit{merged\_model}\leftarrow\mathrm{Merge(\textit{best\_c},\textit{merged\_ model},\textit{candidate\_model})}\)
12:\(\textit{remaining\_models}\leftarrow\)remaining_models\(\backslash\{\textit{candidate\_model}\}\)
13:endfor
14:return\(\textit{merged\_model}\) ```

**Algorithm 3** Heuristic (Similarity)

### Detailed about Evolutionary Strategy of Model Merging

For the experiments of **Q2** - (_i_) in Section 4.3, we constrain all parameter values to be within the range of \([0,1]\). TIES and DARE require to optimize \(2*k\) parameters, while other methods require to optimize \(k\) parameters, where \(k\) represents the number of models included in the model zoo.

For the experiments of **Q2** - (_ii_) in Section 4.3, we choose the Linear method for experimentation, and we constrain all parameter values to be within the range of \([0,1]\). For finer-grained merging, we group adjacent \(n\) decoder layers together, where they share the same coefficient. For the remaining parameters, we make them share the same coefficient. Hence, the number of parameters that need to be fine-tuned is given by: \(k*(\frac{\textit{mem\_hidden\_layers}}{n}+1)\), where \(k\) represents the number of models and \(n\) represents the size of groups. For the case of \(n=32\), we utilized the previous results, thus the number of parameters to be optimized is \(k\).

For the experiments of **Q2** - (_iii_) in Section 4.3, we control the variation of coefficients obtained through heuristic strategy to not exceed \(0.1\), and when it is negative, we set it to \(0\). We also only evaluate the Linear method.

### Detailed Algorithms of Model Mixture

Model Level Mixture.We present the implementation details in Algorithm 4. The mixed model consists of a router, which determines the expert to execute inference, and all the input models as experts. All the weights of input model's components, including embedding layers (embd_layer), decoder layers (layers) and language model head (lm_head), will be integrated into the mixed model.

```
0: A model family \(\{w_{1},...,w_{n}\}\)
0:\(\textit{mixed\_model}\)
1:\(\textit{mixed\_model.router}\leftarrow\mathrm{GenerateRouter}(\{w_{1},...,w_{n}\})\)
2:for\(i=1\) to \(n\)do
3:\(\textit{mixed\_model.expert}_{i}\gets w_{i}\)
4:endfor
5:return\(\textit{mixed\_model}\) ```

**Algorithm 4** Model Level Mixture

Block Level Mixture.We present the implementation details in Algorithm 5. Different from model-level mixture, block-level mixture utilizes the embd_layer and lm_head of an additional model within a model family to handle input and output. Meanwhile, the transformer blocks of other models within the model family act as experts, connected by a router.

FFN Level Mixture.We present the implementation details in Algorithm 6. FFN level mixture is similar to block level with only difference on inner-block component sharing. Each layer of the mixed model will take the attention weights of the base model and build an MoE structure based on the FFNs in corresponding layers of all the input models.

```
0: A model family \(\{w_{1},...,w_{n}\}\) with identical layer amount, one of the family as \(base\_model\), \(k\) layers for merging and the rest layers for mixture.
0:mixed_model
1:mixed_model.embd_layer \(\leftarrow\) base_model.embd_layer
2:mixed_model.lm_head \(\leftarrow\) base_model.lm_head
3:mixed_model.layer\({}_{i}\).reuter \(\leftarrow\) GenerateRouter(\(\{w_{1},...,w_{n}\}\))
4:for\(j=1\) to \(n\)do
5:mixed_model.layer\({}_{i}.expert_{j}\)\(\gets\)\(w_{j}.layer_{i}\)
6:endfor
7:endfor
8:returnmixed_model ```

**Algorithm 6** FFN Level Mixture

Hybrid MixtureWe present the implementation details in Algorithm 7. The hybrid mixture combines both merging and mixture methods. Specifically, the first \(k\) layers of the mixed model are obtained by merging multiple models, while the rest of the layers use an FFN-level mixture architecture.

```
0: A model family \(\{w_{1},...,w_{n}\}\) with identical layer amount, one of the family as \(base\_model\), \(k\) layers for merging and the rest layers for mixture.
0:mixed_model
1:mixed_model.embd_layer \(\leftarrow\) base_model.embd_layer
2:mixed_model.lm_head \(\leftarrow\) base_model.lm_head
3:for\(i=0\) to \(k\)do
4:mixed_model.layer\({}_{i}\)\(\leftarrow\) Merge(\(\{w_{1},...,w_{n}\}\), i)
5:endfor
6:for\(i=k+1\) to Len(\(base\_model.layers\)) do
7:mixed_model.layer\({}_{i}\).reuter \(\leftarrow\) GenerateRouter(\(\{w_{1},...,w_{n}\}\))
8:mixed_model.layer\({}_{i}\).attention \(\leftarrow\) base_model.layer\({}_{i}\).attention
9:mixed_model.layer\({}_{i}\).norm \(\leftarrow\) base_model.layer\({}_{i}\).norm
10:for\(j=1\) to \(n\)do
11:mixed_model.layer\({}_{i}.expert_{j}\).FFN \(\leftarrow\)\(w_{j}.layer_{i}\).FFN
12:endfor
13:endfor
14:returnmixed_model ```

**Algorithm 7** Hybrid Mixture

```
0: A model family \(\{w_{1},...,w_{n}\}\) with identical layer amount, one of the family as \(base\_model\), \(k\) layers for merging and the rest layers for mixture.
0:mixed_model.embd_layer \(\leftarrow\) base_model.embd_layer
1:mixed_model.lm_head \(\leftarrow\) base_model.lm_head
2:for\(i=0\) to \(k\)do
3:mixed_model.layer\({}_{i}\)\(\leftarrow\) Merge(\(\{w_{1},...,w_{n}\}\), i)
4:endfor
5:for\(i=k+1\) to Len(\(base\_model.layers\)) do
6:mixed_model.layer\({}_{i}\).reuter \(\leftarrow\) GenerateRouter(\(\{w_{1},...,w_{n}\}\))
7:mixed_model.layer\({}_{i}\).attention \(\leftarrow\) base_model.layer\({}_{i}\).attention
8:mixed_model.layer\({}_{i}\).norm \(\leftarrow\) base_model.layer\({}_{i}\).norm
10:for\(j=1\) to \(n\)do
11:mixed_model.layer\({}_{i}.expert_{j}\).FFN \(\leftarrow\)\(w_{j}.layer_{i}\).FFN
12:endfor
13:endfor
14:returnmixed_model ```

**Algorithm 8** FFN Level Mixture

### Details of Model-Glue

The models selected by the heuristic strategy include: migtissera/Synthia-7B-v1.2, neuralmagic/Llama-2-7b-evolcodealpaca, teknium/OpenHermes-7B, meta-llama/Llama-2-7b-chat-hf, meta-math/MetaMath-7B-V1.0, lmsys/vicuna-7b-v1.5. Since merging ise-uiuc/Magicoder-S-CL-7B and codellama/CodeLlama-7b-Instruct-hf does not lead to improvement in the Codellama's mergeable family, we select ise-uiuc/Magicoder-S-CL-7B as the representative model.

The final models used for Model-level Mixture are: LLM360/CrystalChat, ise-uiuc/Magicoder-S-CL-7B, meta-math/MetaMath-Llemma-7B and the representative model of the Llama-2 family obtained through the Heuristic (Coefficient). Please refer to our repository for specific configurations.

### Details of clustering in selective merging pipeline

Motivation for using cosine similarity as a model selection criterionPrevious merging study [64] finds that merging performance is consistent with parameter similarity. We inherit it by using cosine similarity as a representative method to measure whether a model can be merged. From our preliminary result, cosine similarity works effectively. Empirically, when the cosine similarity between models exceeds \(0.95\), merging them can yield positive benefits. In Table 14, we present examples of successful and unsuccessful merging. For example, the cosine similarity between the weights of Llama-2-chat and Vicuna is \(0.9982\), resulting in the merged model significantly outperforming its parent models. On the other hand, the cosine similarity between the weights of Llama-2-chat and CodeLlama is \(0.5351\), indicating that the merged model is inferior to CodeLlama. Moreover, using cosine similarity to measure the merging benefit is simple and efficient. For these reasons, we stick with cosine similarity for selective merging pipelines.

Criteria for Determining the Number of Clusters.We cluster models with cosine similarity greater than \(0.95\) into a mergeable family, ensuring that within this mergeable family, the pairwise similarities between models are greater than \(0.95\). The number of clusters is automatically determined during the process, after which we execute our merge strategy within each cluster. For Which16 model zoo in our paper, we clustered \(16\) models and finally obtained five mergeable families: \(\blacklozenge\) 12 models fine-tuned based on llama-2, \(\blacklozenge\) ise-uiuc/Magicoder-S-CL-7B, \(\blacklozenge\)  codellama/CodeLlama-7b-Instruct-hf, \(\blacklozenge\)  meta-math/MetaMath-Llemma-7B, \(\blacklozenge\)  LLM360/CrystalChat. Since the remaining clusters contain only one model each, we only report the results of different merging strategies performed within Family \(\blacklozenge\).

Impact of clustering thresholdWe computed the cosine similarity between 12 LLMs all fine-tuned from Llama-2. These models are considered to be well mergeable, having the same architecture and initialization. Since their similarities range from \(0.9680\) to \(0.9999\), \(0.95\) could be a lower bound for model clustering. To show the impact of different clustering thresholds, we have examined the performance of merged models with drastically different similarity: Llama-2,deepsek-coder, CodeLlama, and MetaMath-Llema. We use linear interpolation to merge two models and present the benchmarking results in Table 11. The performance of the individual models is shown in Table 12. If the merged model outperforms its parent models on average accuracy, we consider it a successful merge. From Table 11, we see that successful merging only occurs between Codellama and Codellama-instruct whose weights reach \(0.99\) similarity and have the same initialization. To include more mergeable models, we finally choose \(0.95\) as the threshold for clustering.

### Energy Consumption

Existing literature is mainly concerned with carbon emissions during LLM pre-training [14; 52]. However, the training costs associated with the approaches evaluated in our benchmark are minimal. Specifically, the only training expenditure in our study pertains to the B-M-S router training, as described in Section 4.4. This process requires about \(80\) GPU hours, resulting in \(13.55\)kg CO2

\begin{table}
\begin{tabular}{c c|c c c c c c c} \hline \hline Parent Model 1 & Parent Model 2 & ARC & MMLU & WinoGrande & GSMSK & HumanEval & MBPP & Avg. & Sim. \\ \hline Llama-2-7b-hf & deepsek-coder-6.7b-base & 27.73e & 24.38\% & 49.64\% & 0.00\% & 0.00\% & 16.96\% & 0\% \\ Llama-2-7b-hf & CodeLlama-7b-hf & 41.04\% & 31.68\% & 66.85\% & 5.76\% & 10.98\% & 21.40\% & 29.62\% & 52.55\% \\ CodeLlama-7b-Python-hf & CodeLlama-7b-hf & 40.61\% & 37.17\% & 65.35\% & 6.67\% & 21.95\% & 25.60\% & 32.89\% & 60.34\% \\ MetaMath-Llemma-7B & CodeLlama-7b-hf & 46.16\% & 42.86\% & 64.64\% & 27.07\% & 34.76\% & 37.40\% & 42.15\% & 89.76\% \\ CodeLlama-7b-Instruct-hf & CodeLlama-7b-hf & 43.86\% & 41.39\% & 68.59\% & 16.07\% & 33.54\% & 40.80\% & 40.71\% & 99.94\% \\ \hline \hline \end{tabular}
\end{table}
Table 11: Performance of merged models with different similarity. Sim. stands for cosine similarity.

[MISSING_PAGE_FAIL:21]

[MISSING_PAGE_FAIL:22]

\begin{table}
\begin{tabular}{c|c c c c c c|c} \hline \hline Group Size & ARC & WinoGrande & MMLU & GSM8K & MBPP & HumanEval & Average \\ \hline \multicolumn{8}{c}{Which12} \\ \hline
1 & 56.14\% & 73.32\% & 51.50\% & 32.45\% & **24.00\%** & **20.12\%** & 42.92\% \\
4 & 56.31\% & 73.72\% & 52.04\% & 33.43\% & **24.00\%** & 18.90\% & 43.07\% \\
8 & **56.83\%** & **74.43\%** & **53.01\%** & **38.13\%** & 21.20\% & 19.51\% & **43.85\%** \\
32 & 56.48\% & 73.56\% & 51.79\% & 36.01\% & 23.60\% & **20.12\%** & 43.59\% \\ \hline \multicolumn{8}{c}{Which8} \\ \hline
1 & 55.12\% & 74.11\% & 49.96\% & 31.69\% & **25.20\%** & 19.51\% & 42.60\% \\
4 & **56.06\%** & **74.66\%** & **50.04\%** & 33.59\% & 24.20\% & **21.95\%** & 43.42\% \\
8 & 55.29\% & 73.88\% & 49.20\% & 40.56\% & 24.60\% & 18.90\% & **43.74\%** \\
32 & 55.12\% & 73.64\% & 49.59\% & **40.64\%** & 22.40\% & 18.90\% & 43.38\% \\ \hline \multicolumn{8}{c}{Which4} \\ \hline
1 & **54.61\%** & 73.32\% & **47.63\%** & 41.62\% & 23.60\% & 15.85\% & 42.77\% \\
4 & 52.90\% & 73.32\% & 46.99\% & 43.06\% & **24.00\%** & 20.73\% & 43.50\% \\
8 & 54.01\% & **73.64\%** & 47.39\% & 43.75\% & 22.40\% & **21.95\%** & **43.86\%** \\
32 & 53.50\% & 73.01\% & 47.32\% & **45.79\%** & 20.20\% & 15.85\% & 42.61\% \\ \hline \hline \end{tabular}
\end{table}
Table 17: The impact of different group sizes on Evolutionary Strategy.

\begin{table}
\begin{tabular}{c|c c c c c c|c} \hline \hline Strategy & ARC & WinoGrande & MMLU & GSM8K & MBPP & HumanEval & Average & Round \\ \hline \multicolumn{8}{c}{Which12} \\ \hline Evo (Vanilla) & \(56.48\%\) & \(73.56\%\) & \(51.79\%\) & \(36.01\%\) & \(23.60\%\) & \(20.12\%\) & \(43.59\%\) & 200 \\ Evo (Heuristic) & \(55.29\%\) & \(72.85\%\) & \(49.96\%\) & \(40.56\%\) & \(22.80\%\) & \(18.29\%\) & \(43.29\%\) & 127 \\ \hline \multicolumn{8}{c}{Which8} \\ \hline Evo (Vanilla) & \(55.12\%\) & \(73.64\%\) & \(49.59\%\) & \(40.64\%\) & \(22.40\%\) & \(18.90\%\) & \(43.38\%\) & 200 \\ Evo (Heuristic) & \(54.69\%\) & \(72.93\%\) & \(49.68\%\) & \(45.19\%\) & \(21.00\%\) & \(19.51\%\) & \(43.83\%\) & 71 \\ \hline \multicolumn{8}{c}{Which4} \\ \hline Evo (Vanilla) & \(53.50\%\) & \(73.01\%\) & \(47.32\%\) & \(45.79\%\) & \(20.20\%\) & \(15.85\%\) & \(42.61\%\) & 200 \\ Evo (Heuristic) & \(54.52\%\) & \(73.56\%\) & \(47.74\%\) & \(40.71\%\) & \(23.20\%\) & \(21.95\%\) & \(43.61\%\) & 69 \\ \hline \hline \end{tabular}
\end{table}
Table 18: More efficient merging strategy.

\begin{table}
\begin{tabular}{c|c c c c c c} \hline \hline Model & ARC & WinoGrande & MMLU & GSM8K & MBPP & HumanEval & Average \\ \hline Best Single Model & \(52.05\%\) & \(69.46\%\) & \(50.77\%\) & \(27.22\%\) & \(\textbf{39.60}\%\) & \(\textbf{35.98}\%\) & \(45.85\%\) \\ \hline M-L-S & \(\textbf{51.88}\%\) & \(\textbf{70.88}\%\) & \(\textbf{52.44}\%\) & \(\textbf{32.52}\%\) & \(39.40\%\) & \(31.10\%\) & \(\textbf{46.37}\%\) \\ \hline \hline \end{tabular}
\end{table}
Table 19: Better prompt vector for the mixture of Llama-2-7b-chat and CrystalChat. We highlight the better performance in **bold**.