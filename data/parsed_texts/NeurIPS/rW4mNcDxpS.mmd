# Wasserstein Gradient Flows for Optimizing Gaussian Mixture Policies

Hanna Ziesche1 and Leonel Rozo1

Bosch Center for Artificial Intelligence (BCAI)

Renningen, Germany

name.surname@de.bosch.com

Equal contribution.

###### Abstract

Robots often rely on a repertoire of previously-learned motion policies for performing tasks of diverse complexities. When facing unseen task conditions or when new task requirements arise, robots must adapt their motion policies accordingly. In this context, policy optimization is the _de facto_ paradigm to adapt robot policies as a function of task-specific objectives. Most commonly-used motion policies carry particular structures that are often overlooked in policy optimization algorithms. We instead propose to leverage the structure of probabilistic policies by casting the policy optimization as an optimal transport problem. Specifically, we focus on robot motion policies that build on Gaussian mixture models (GMMs) and formulate the policy optimization as a Wasserstein gradient flow over the GMMs space. This naturally allows us to constrain the policy updates via the \(L^{2}\)-Wasserstein distance between GMMs to enhance the stability of the policy optimization process. Furthermore, we leverage the geometry of the Bures-Wasserstein manifold to optimize the Gaussian distributions of the GMM policy via Riemannian optimization. We evaluate our approach on common robotic settings: Reaching motions, collision-avoidance behaviors, and multi-goal tasks. Our results show that our method outperforms common policy optimization baselines in terms of task success rate and low-variance solutions.

## 1 Introduction

One of the main premises about autonomous robots is their ability to successfully perform a large range of tasks in unstructured environments. This demands robots to adapt their task models according to environment changes or new task requirements, and consequently to adjust their actions to successfully perform under unseen conditions [1]. In general, robotic tasks, such as picking or inserting an object, are usually executed by composing previously-learned skills [2], each represented by a motion policy. Therefore, in order to successfully perform under new settings, the robot should adapt its motion policies according to the new task requirements and environment conditions.

Research on methods for robot motion policy adaptation is vast [3, 4], with approaches mainly building on black-box optimizers [5, 6], end-to-end deep reinforcement learning [7, 8], and policy search [9]. Regardless of the optimization method, most approaches disregard the intrinsic policy structure in the adaptation strategy. However, several motion policy models (e.g., dynamic movement primitives (DMP)[10], Gaussian mixture models (GMM) [11], probabilistic movement primitives (ProMPs) [12, 13], and neural networks [14], among others), carry specific physical or probabilistic structures that should not be ignored. First, these policy models are often learned from demonstrations in a starting learning phase [2], thus the policy structure already encapsulates relevant prior information about the skill. Second, structure-unaware adaptation strategies optimize the policy parameters disregardingthe intrinsic structural characteristics of the policy model (e.g., a DMP represents a second-order dynamical system). In this context, we hypothesize that the policy structure may be leveraged to better control the adaptation strategy via policy structure-aware gradients and trust regions.

Our main idea is to design a policy optimization strategy that explicitly builds on a particular policy structure. Specifically, we focus on GMM policies, which have been widely used to learn motion skills from human demonstrations [11; 15; 16; 17; 18; 19]. GMMs provide a simple but expressive enough representation for learning a large variety of robot skills such as: Stable dynamic motions [20; 21; 22], collaborative behaviors [23; 24], assembly strategies [25; 26], and contact-rich manipulation [27; 28], among others. Often, skills learned from demonstrations need to be refined -- due to imperfect data -- or adapted to comply with new task requirements. Existing adaptation strategies for GMM policies either build a kernel method on top of the original skill model [29], or leverage reinforcement learning (RL) to adapt the policy itself [30; 31]. However, none of these techniques explicitly considered the intrinsic probabilistic structure of the Gaussian mixture policy.

Unlike the aforementioned approaches, we propose a policy optimization technique that explicitly considers the underlying GMM structure. To do so, we exploit optimal transport theory [32; 33], which allows us to view the set of GMM policies as a particular space of probability distributions \(\mathrm{GMM}_{d}\). Specifically, we leverage the idea of Chen et al. [34] and Delon and Desolneux [35] to view a GMM as a set of discrete measures (Dirac masses) on the space of Gaussian distributions \(\mathcal{G}(\mathbb{R}^{d})\), which is endowed with a Wasserstein distance (see SS 2). This allows us to formulate the policy optimization as a Wasserstein gradient flow (WGF) over the space of GMMs (as illustrated in Fig. 1 and explained in SS3), where the policy updates are naturally guaranteed to be GMMs. Moreover, we take advantage of the geometry of the Bures-Wasserstein manifold to optimize the Gaussian distributions of a GMM policy via Riemannian optimization. We evaluate our approach over a set of different GMM policies featuring common robot skills: Reaching motions, collision-avoidance behaviors, and multi-goal tasks (see SS 4). Our results show that our method outperforms common policy optimization baselines in terms of task success rate while providing low-variance solutions.

Related Work:Richemond and Magninis [36] pioneered the idea of understanding policy optimization through the lens of optimal transport. They interpreted the policy iteration as gradient flows by leveraging the implicit Euler scheme under a Wasserstein distance (see SS 2), considering only \(1\)-step return settings. They observed that the resulting policy optimization resembles the gradient flow of the Fokker-Planck equation (JKO scheme) [37]. In a similar spirit, Zhang et al. [38] proposed to use WGFs to formulate policy optimization as a sequence of policy updates traveling along a gradient flow on the space of probability distributions until convergence. To solve the WGF problem, the authors proposed a particle-based algorithm to approximate continuous density functions and subsequently derived the gradients for particle updates based on the JKO scheme. Although Zhang et al. [38] considered general parametric policies, their method assumed a distribution over the policy parameters and did not consider a specific policy structure, which partially motivated their particle-based approximation. Recently, Mokrov et al. [39] tackled the computational burden of particle methods by leveraging input-convex neural networks to approximate the WGFs computation. They reformulated the well-known JKO optimization [37] over probability measures by an optimization over convex functions. Yet, this work remains a general solution for WGF computation and it did not address its use for policy optimization problems.

Aside from optimal transport approaches, Arenz et al. [30] proposed a trust-region variational inference for GMMs to approximate multimodal distributions. Although not originally designed

Figure 1: Illustration our policy structure-aware adaptation of GMM policies. _Left_: A robot manipulator tracks a reference trajectory () extracted from a GMM policy () initially learned from demonstrations, while being required to reach for a new target (). _Center_: Our policy optimization provides policy updates that follow a Wasserstein gradient flow on the manifold of GMM policies \(\mathrm{GMM}_{d}\). _Right_: The final GMM policy where some Gaussian components were adapted in order to retrieve a new reference trajectory that allows the robot to reach for the new target.

for policy optimization, the authors elucidated a connection to learn GMMs of policy parameters in black-box RL. However, their method cannot directly be applied to our GMM policy adaptation setting, nor does it consider the GMM structure from an optimal transport perspective. Nematollahi et al. [31] proposed SAC-GMM, a hybrid model that employs the well-known SAC algorithm [40] to refine dynamic skills encoded by GMMs. The SAC policy was designed to learn residuals on a single vectorized stack of GMM parameters, thus fully disregarding the GMM structure and the geometric constraints of its parameters. Ren et al. [41] introduced PMOE, a method to train deep RL policies using a probabilistic mixture of experts via GMMs. They addressed the non-differentiability problem caused by the optimization of categorical distribution parameters associated with the mixture weights via gradient estimators. However, PMOE does not take an optimal transport perspective on the policy optimization problem, nor does it account for the geometry arising from the GMM parameters.

Concerning approaches that explicitly account for the geometry induced by the policy, we may broadly categorize them into two groups: exact models and natural gradient methods. The former category refers to policy optimization methods that fully exploit the geometric structure of the policy, as proposed in this paper. In this case, the state of the art is scarce but recent works such as the Riemannian proximal policy optimization for GMMs proposed in [42], showed the importance of leveraging the geometry induced by the GMM parameters in the policy optimization via Riemannian gradients, similarly to our method. Their policy optimization was regularized by a Wasserstein distance to control the exploration-exploitation trade-off. However, their method did not formulate the policy optimization as an optimal transport problem, i.e., the policy updates did not follow a Wasserstein gradient flow, as in our approach, but it employed instead a classical non-convex optimization procedure.

The second category encloses approaches that leverage the well-established technique for enhancing the stability of policy optimization, namely, introducing regularization to the objective function using the Kullback-Leibler (KL) divergence, which induces the so-called natural gradient [43; 44; 45; 46]. In this context, the regularization provides a global measure of the policies similarity, while the natural gradient perspective aims at locally controlling the changes between successive policy updates. In other words, natural gradient approaches leverage the local geometry of the policy parameters space to compute controlled updates. Note that policies similarity can also be quantified via the Wasserstein distance, as proposed in this paper and in recent works [36; 38; 47; 48]. Unlike the KL divergence, the Wasserstein distance enjoys powerful geometrical, computational, and statistical features [32; 33]. For example, Moskovitz et al. [48] recently employed the Wasserstein natural gradient to exploit the local geometry induced by the Wasserstein regularization of behavioral policy optimization [47], but neither the policy nor the behavioral embeddings had particular structures that could be leveraged in the policy optimization.

**In this paper** we exploit the geometric properties arising from both a Wasserstein-regularized objective and a Gaussian mixture policy. This allows us not only to see the policy iteration as a Wasserstein gradient flow, as in [36], but also to leverage the Riemannian geometry associated to the GMM space \(\mathrm{GMM}_{d}\) to formulate exact Riemannian gradient descent updates [49], instead of relying on inexact natural gradients approximations. To achieve this, our method leverages the geometry induced by the structure of the space of GMM policies via the Bures-Wasserstein manifold, which naturally guarantees that policy updates stay on \(\mathrm{GMM}_{d}\). To the best of our knowledge, our optimization of GMM policies based on Wasserstein gradient flows and the Bures-Wasserstein manifold is the first of its kind in the domain of policy adaptation.

## 2 Background

### Wasserstein gradient flows

In Euclidean space a gradient flow is a smooth curve \(x:\mathbb{R}\rightarrow\mathbb{R}^{d}\) that satisfies the partial differential equation (PDE) \(\dot{x}(t)=-\nabla L(x(t))\) for a given loss function \(L:\mathbb{R}^{d}\rightarrow\mathbb{R}\) and starting point \(x_{0}\) at \(t=0\)[32; 50]. A solution can be found straightforwardly by forward discretization, leading to the well-known explicit Euler update scheme \(x_{k+1}^{\tau}\ =\ x_{k}^{\tau}\ -\ \lambda\nabla L(x_{k}^{\tau})\), where \(\lambda\) denotes the learning rate and \(x^{\tau}\) indicates a discretization of the curve \(x(t)\) with discretization parameter \(\tau\). Alternatively, we can use a backward discretization, which leads to the following implicit Euler scheme

\[x_{k+1}^{\tau}=\operatorname*{arg\,min}_{x}\left(\frac{\|x-x_{k}^{\tau}\|^{2} }{2\tau}+L(x)\right).\] (1)Eq. 1 is sometimes referred to as Minimizing Movement Scheme and can be used as an alternative characterization of a gradient flow.

This characterization is particularly interesting when we need to extend the concept of gradient flows to (non-Euclidean) general metric settings, since there is no notion of \(\nabla L\) in these cases [32; 51]. Eq. 1 does not involve any gradients and can be expressed using only metric quantities. In this work, we are particularly interested in gradient flows in the \(L^{2}\)-Wasserstein space, defined as the set of probability measures \(\mathbb{P}(\mathcal{X})\) on a separable Banach space \(\mathcal{X}\)[52] and endowed with the \(L^{2}\)-Wasserstein distance \(W_{2}\) defined as

\[W_{2}(\mu,\nu)=\left(\inf_{\gamma\in\Pi(\mu,\nu)}\int_{\mathcal{ X}\times\mathcal{X}}\|x_{1}-x_{2}\|^{2}\mathrm{d}\gamma(x_{1},x_{2})\right)^{ \frac{1}{2}},\] (2)

where \(\mu,\nu\in\mathbb{P}(\mathcal{X})\) and \(\gamma\in\mathbb{P}(\mathcal{X}^{2})\) is defined to have the two marginals \(\mu\) and \(\nu\).

A Generalized Minimizing Movement scheme characterizing gradient flows in the Wasserstein space can be written in analogy to Eq. 1 as:

\[\pi_{k+1}^{\tau}=\operatorname*{arg\,min}_{\pi}\left(\frac{W_{2}^{ 2}(\pi,\pi_{k}^{\tau})}{2\tau}+L(\pi)\right),\] (3)

where \(L\) is a functional to be minimized on the Wasserstein space and \(\pi_{k}\in\mathbb{P}(X)\). In the following, we will omit the superscript \(\tau\) for notational convenience.

### Reinforcement Learning as Wasserstein Gradient Flows

Our view of the policy structure-aware optimization builds on the approach outlined in [36], which in turn is based on the JKO scheme [37]. They proposed a formulation of \(1\)-step RL problems in terms of Wasserstein gradient flows. In particular, they studied the evolution of a policy \(\pi\) under the influence of a free energy functional \(J\) of the form:

\[J(\pi)=K_{r}(\pi)+\beta\mathcal{H}(\pi)=\int_{\mathcal{A}} \mathrm{d}\pi(\bm{a}|\bm{s})r(\bm{s},\bm{a})-\beta\int_{\mathcal{A}}\mathrm{d }\pi(\bm{a}|\bm{s})\log(\pi(\bm{a}|\bm{s})),\] (4)

where \(K_{r}(\pi)\) denotes the inner energy of the system, here determined by the reward \(r(\bm{s},\bm{a})\), and \(\mathcal{H}(\pi)\) is the entropy of the policy \(\pi(\bm{a}|\bm{s})\), with \(\bm{s}\) and \(\bm{a}\) denoting the state and action, respectively. Thus, Eq. 4 can be recognized as the usual objective in \(1\)-step RL settings with entropy regularization. It is well known that the evolution of probability densities under a free energy of this form is properly described by a PDE known as the Fokker-Planck equation. Richemond and Maginnis [36] exploited the result of Jordan et al. [37], which stated that this evolution can be interpreted as the gradient flow of the functional \(J\) in Wasserstein space. This flow is characterized by the following minimizing movement scheme

\[\pi_{k+1}=\operatorname*{arg\,min}_{\pi}\left(\frac{W_{2}^{2}( \pi,\pi_{k})}{2\tau}-J(\pi)\right),\] (5)

which naturally provides iterative updates for the policy \(\pi\). While Richemond and Maginnis [36] considered a \(1\)-step bandit setting, we extend this approach to full multi-step RL problems and consequently learn policies for long-horizon tasks.

### The \(L^{2}\)-Wasserstein distance between Gaussian Mixture Models (GMMs)

We consider policies \(\pi(\bm{x})\) that build on a GMM structure, i.e., \(\pi(\bm{x})=\sum_{i=1}^{N}\omega_{i}\mathcal{N}(\bm{x};\bm{\mu}_{i},\bm{\Sigma }_{i})\), where \(\mathcal{N}\) denotes a multivariate Gaussian distribution with mean \(\bm{\mu}_{i}\) and covariance matrix \(\bm{\Sigma}_{i}\), and \(\omega_{i}\) are the weights of the \(N\) individual Gaussian components, which are subject to \(\sum_{i}\omega_{i}=1\). In the following, we will write \(\bm{\hat{\mu}}\), \(\bm{\hat{\Sigma}}\) and \(\bm{\hat{\omega}}\) to denote the stacked means, covariance matrices and weights of the \(N\) components. Therefore, we do not consider WGFs on the full manifold of probability distributions (Wasserstein space) \(\mathbb{P}(\mathbb{R}^{d})\) but rather focus on WGFs evolving on the submanifold of GMMs, that is \(\mathrm{GMM}_{d}\subset\mathbb{P}(\mathbb{R}^{d})\). Following [34; 35], we can approximately describe this submanifold as a discrete distribution over the space of Gaussian distributions equipped with the Wasserstein metric. This in turn can be identified with the Bures-Wasserstein manifold which is the product manifold \(\mathbb{R}^{d}\times\mathcal{S}^{d}_{++}\)where \(\mathcal{S}^{d}_{++}\) denotes the Riemannian manifold of \(d\)-dimensional symmetric positive definite matrices. The corresponding approximated Wasserstein distance between two GMMs \(\pi_{1}\), \(\pi_{2}\) is given by

\[W_{2}^{2}\big{(}\pi_{1}(\bm{x}),\pi_{2}(\bm{x})\big{)}=\min_{\bm{P}\in U(\bm{ \omega}_{1},\bm{\omega}_{2})}\sum_{i,j}^{N}\bm{P}_{ij}W_{2}^{2}\big{(}\mathcal{N }_{1}(\bm{x};\bm{\mu}_{i},\bm{\Sigma}_{i}),\mathcal{N}_{2}(\bm{x};\bm{\mu}_{j}, \bm{\Sigma}_{j})\big{)},\] (6)

where \(U(\bm{\omega}_{1},\bm{\omega}_{2})=\{\bm{P}\in\mathbb{R}_{+}^{N\times N}|\bm{P }\bm{1}_{N}=\bm{\omega}_{1},\bm{P}^{\mathsf{T}}\bm{1}_{N}=\bm{\omega}_{2}\}\) with \(\bm{1}_{N}\) denoting an \(N\)-dimensional vector of ones. The Wasserstein distance between two Gaussian distributions in Eq. 6 can be computed analytically as follows

\[W_{2}^{2}\big{(}\mathcal{N}_{1}(\bm{x};\bm{\mu}_{i},\bm{\Sigma}_{i}), \mathcal{N}_{2}(\bm{x};\bm{\mu}_{j},\bm{\Sigma}_{j})\big{)}=\|\bm{\mu}_{i}-\bm {\mu}_{j}\|^{2}+\mathrm{tr}\left[\bm{\Sigma}_{i}+\bm{\Sigma}_{j}-2\left(\bm{ \Sigma}_{i}^{\nicefrac{{1}}{{2}}}\bm{\Sigma}_{j}\bm{\Sigma}_{i}^{\nicefrac{{ 1}}{{2}}}\right)^{\nicefrac{{1}}{{2}}}\right].\] (7)

### Learning GMM policies from demonstrations

A popular approach in RL -- particularly in robotics -- to reduce the number of policy rollouts in the environment is to warm-start the policy with a set of demonstrations provided by an expert. In this work we choose to represent our policy via a GMM. We assume that demonstrations are provided as a set of trajectories \(\tau\) of state-action pairs \(\tau=\{(\bm{s}_{0},\bm{a}_{0}),(\bm{s}_{1},\bm{a}_{1}),\ldots(\bm{s}_{T},\bm{a }_{T})\}\). To initialize our policy, we first use the Expectation-Maximization (EM) algorithm to fit a GMM, in the state-action space, to the demonstrations. This results in a mixture distribution \(\pi(\bm{s},\bm{a})~{}=~{}\sum_{i=1}^{N}\omega_{i}\mathcal{N}\big{(}[\bm{s}\, \bm{a}]^{\mathrm{T}};\bm{\mu}_{i},\bm{\Sigma}_{i}\big{)}\) from which a policy can be obtained by conditioning on the state \(\bm{s}\), as follows

\[\pi(\bm{a}|\bm{s})=\frac{\pi(\bm{s},\bm{a})}{\int\pi(\bm{s},\bm{a})\mathrm{d} \bm{a}}.\] (8)

In the context of GMMs, this is also known as Gaussian Mixture Regression (GMR) [53]. The resulting conditional distribution is another GMM on action space with state-dependent parameters,

\[\pi(\bm{a}_{t}|\bm{s}_{t})=\sum_{i=1}^{N}\omega_{i}(\bm{s}_{t})\mathcal{N}( \bm{a}_{t};\bm{\mu}_{i}^{a}(\bm{s}_{t}),\bm{\Sigma}_{i}^{a}).\] (9)

Details on computation of Eq. 9 from the original GMM are given in App. A.1.

## 3 Wasserstein Gradient Flows for GMM Policy Optimization

In this work, we focus on multi-step RL tasks for policy adaptation. We consider a finite-horizon Markov Decision Process (MDP) with continuous state and action spaces \(\mathcal{S}\in\mathbb{R}^{n}\) and \(\mathcal{A}\in\mathbb{R}^{m}\), transition and reward functions \(p(\bm{s}_{t+1}|\bm{s}_{t},\bm{a}_{t})\) and \(r(\bm{s}_{t},\bm{a}_{t})\), initial state distribution \(\rho(\bm{s}_{0})\) and a discount factor \(\gamma\). Further, we assume to have an initial policy \(\pi(\bm{a}_{t}|\bm{s}_{t})\), which is to be adapted by optimizing some objective function \(K_{r}(\pi)\). As stated in 8, this problem arises in robot learning settings where a policy learned via imitation learning (e.g., LfD) needs to be adapted to new objectives or unseen environmental conditions. To promote exploration and avoid premature convergence to suboptimal policies, we leverage maximum entropy RL [54] by adding an entropy term \(\mathcal{H}(\pi)\) to the objective. Thus, the overall objective has the form of a free energy functional (resembling Eq. 4) and can be written as

\[J(\pi)=K_{r}(\pi)+\beta\mathcal{H}(\pi),\] (10)

where \(\beta\) is a hyperparameter and \(K_{r}(\pi)\) corresponds to the usual cumulative return

\[K_{r}(\pi)\!=\!\mathbb{E}_{\tau}\left[\sum_{t}r(\bm{s}_{t},\bm{a}_{t})\right]\! \!=\!\!\int\!\Pi_{t}\mathrm{d}\bm{s}_{0}\mathrm{d}\bm{s}_{t}\mathrm{d}\bm{a}_ {t}\rho(\bm{s}_{0})\pi(\bm{a}_{t}|\bm{s}_{t})p(\bm{s}_{t+1}|\bm{s}_{t},\bm{a}_ {t})\sum_{t}\gamma^{t}r(\bm{s}_{t},\bm{a}_{t}).\] (11)

The evolution of the policy \(\pi(\bm{a}_{t}|\bm{s}_{t})\) over the course of the optimization can be described as a flow of a probability distribution in Wasserstein space. This formulation comes with three major benefits: _(i)_ We directly leverage the Wasserstein metric properties for describing the evolution of probability distributions; _(ii)_ We exploit the \(L^{2}\)-Wasserstein distance to constrain the policy updates, which is important to guarantee stability in policy optimization [55; 45; 56]; _(iii)_ By constraining to specific submanifolds of the Wasserstein space, in this case GMMs, we can impose additional structural properties on the policy optimization.

Since our objective in Eq. 10 has the form of the free energy functional studied by [36; 37], we can leverage the iterative updates scheme of Eq. 5 to formulate the evolution of our policy iteration under the flow generated by Eq. 10. As mentioned previously, we focus on the special case of GMM policies and therefore restrict the Wasserstein gradient flow to the submanifold of GMM distributions \(\mathrm{GMM}_{d}\). We refer the interested reader to App. A.3, where we provide the explicit form of \(J(\pi)\) of Eq. 10 for the GMM case.

### Policy optimization

To begin with, we leverage the approximation that describes the GMM submanifold as a discrete distribution over the space of Gaussian distributions \(\mathcal{G}(\mathbb{R}^{d})\), equipped with the Wasserstein metric [34; 35]. Consequently, our policy optimization problem naturally splits into an optimization over the \((N-1)\)-dimensional simplex and an optimization on the \(N\)-fold product of the Bures-Wasserstein manifold (\(BW^{N}\)), i.e., the product manifold \(\left(\mathbb{R}^{d}\times\mathcal{S}^{d}_{++}\right)^{N}\). The former corresponds to the GMM weights while the latter applies to the set of Gaussian distributions' parameters. Note that the identification with the \(BW^{N}\) manifold allows us to perform the optimization directly on the parameter space. This comes with several benefits: _(i)_ We can leverage the well-known analytic solution of the Wasserstein distance between Gaussian distributions in Eq. 6, greatly reducing the computational complexity of the policy optimization. _(ii)_ As shown in [34], we can guarantee that the policy optimized via Eq. 6 remains a GMM (i.e., it satisfies the mass conservation constraint). _(iii)_ Unlike the full Wasserstein space2, the resulting product manifold is a true Riemannian manifold such that we can leverage the machinery of Riemannian optimization. Importantly, working in the parameter space allows us to apply an explicit Euler scheme, instead of the implicit formulation of Eq. 3, when optimizing the Gaussian distributions' parameters.

Footnote 2: Wasserstein space is not a true Riemannian manifold, but it can be equipped with a Riemannian structure and formal calculus on this manifold [57], which has been made rigorous in [51]

According to the above splitting scheme, we formulate the policy optimization as a two-step procedure that alternates between the Gaussian parameters (i.e. means and covariance matrices) and the GMM weights. To optimize the former, we propose to leverage the Riemannian structure of the \(BW\) manifold to reformulate the updates as a forward discretization, similarly to [58]. In other words, by embedding the Gaussian components of the GMM policy in a Riemannian manifold, the Wasserstein gradient flow in the implicit form of Eq. 5 can be approximated by an explicit Euler update scheme according to the \(BW\) metric (further details are provided in App. A.4). This allows us to leverage the expressions of the Riemannian gradient and exponential map of the \(BW\) manifold [59; 60]. Thus, the optimization boils down to Riemannian gradient descent where the gradient is defined w.r.t the Bures-Wasserstein metric. In particular, we use the expression for Riemannian gradient, metric and exponential map used in [60]. Formally, the resulting updates for the Gaussian parameters of the GMM follow the Riemannian gradient descent scheme given by:

\[\boldsymbol{\hat{\mu}}_{k+1}=\mathrm{R}_{\boldsymbol{\hat{\mu}}_{k}}\left( \lambda\cdot\mathrm{grad}_{\boldsymbol{\hat{\mu}}}\,J(\pi_{k})\right),\quad \text{and}\quad\boldsymbol{\hat{\Sigma}}_{k+1}=\mathrm{R}_{\boldsymbol{\hat{ \Sigma}}_{k}}\left(\lambda\cdot\mathrm{grad}_{\boldsymbol{\hat{\Sigma}}}\,J( \pi_{k})\right),\] (12)

where \(\mathrm{grad}\) denotes the Riemannian gradient w.r.t. the Bures-Wasserstein metric, \(\mathrm{R}_{\boldsymbol{x}}:\mathcal{T}_{\boldsymbol{x}}\mathcal{M}\to \mathcal{M}\) denotes the retraction operator, which maps a point on the tangent space \(\mathcal{T}_{\boldsymbol{x}}\mathcal{M}\) back to the manifold \(\mathcal{M}\equiv\mathrm{BW}\)[61]. Moreover, \(\lambda\) is a learning rate and \(\pi_{k}\stackrel{{\mathrm{def}}}{{=}}\pi(\boldsymbol{\hat{\mu}}_{ k},\boldsymbol{\hat{\Sigma}}_{k},\boldsymbol{\hat{\omega}}_{k})\). The Euclidean gradients of \(J(\pi)\) required for computing \(\mathrm{grad}\) can be obtained using a likelihood ratio estimator (a.k.a score function estimator or REINFORCE) [62] and are provided in App. A.3.

Concerning the GMM weights, we first reparameterize them as \(\omega_{j}=\frac{\exp\eta_{j}}{\sum_{k=1}^{N}\exp\eta_{k}}\) and optimize w.r.t. the new parameters \(\eta_{j},j=1...N\), which unlike \(\boldsymbol{\hat{\omega}}\) are unconstrained. For this optimization we employ the implicit Euler scheme:

\[\boldsymbol{\hat{\eta}}_{k+1}=\operatorname*{arg\,min}_{\boldsymbol{\hat{\eta }}}\left(\frac{W_{2}^{2}(\pi_{k+1}(\boldsymbol{\hat{\eta}}),\pi_{k})}{2\tau}-J (\pi_{k+1}(\boldsymbol{\hat{\eta}}))\right),\] (13)where \(\pi_{k+1}(\bm{\hat{\eta}})\stackrel{{\mathrm{def}}}{{=}}\pi(\bm{\hat{ \mu}}_{k+1},\bm{\hat{\Sigma}}_{k+1},\bm{\hat{\eta}})\). We minimize Eq. 13 by gradient descent w.r.t. \(\bm{\eta}\) as follows:

\[\bm{\hat{\eta}}_{k+1}=\bm{\hat{\eta}}_{k}-\lambda\nabla_{\bm{\hat{\eta}}}\, \left(\frac{W_{2}^{2}(\pi_{k+1}(\bm{\eta}),\pi_{k})}{\tau}-J(\pi_{k+1}(\bm{ \hat{\eta}}))\right).\] (14)

The gradient of \(J(\pi)\) can be obtained analytically using a likelihood ratio estimator. For the Wasserstein term, we first compute the gradient w.r.t. the weights via the Sinkhorn algorithm [63], from which the gradient w.r.t \(\bm{\eta}\) can be then obtained via the chain rule. Note that we have to rely on the Sinkhorn algorithm here since there is no analytic solution available for the Wasserstein distance between discrete distributions, unlike the above case of the Gaussian components. Consequently, we cannot compute the corresponding gradients.

### Implementation Pipeline

To carry out the policy optimization, we proceed as in the usual on-policy RL scheme: We first roll out the current policy to collect samples of state-action-reward tuples. Then, we use the collected interaction trajectories to compute a sample-based estimate of the functional \(K_{r}(\pi)\) and its gradients w.r.t the policy parameters, as explained in SS 3.1. An optimization step consists of alternating between optimizing the Gaussian parameters using Eq. 12, and updating the weights via Eq. 14. For the optimization of the Gaussian parameters we leverage Pymanopt [64] for Riemannian optimization. We extended this library by implementing the Bures-Wasserstein manifold based on the expressions provided by Han et al. [60] (see App. A.2 for details). Furthermore, we added a custom line-search routine that accounts for the constraint on the Wasserstein distance between the old and the optimized GMM, as to our knowledge such a search method does not exist in out-of-the-box optimizers. The details of this custom line-search are given in Algorithm 2 in App. A.5. Regarding the optimization of the GMM weights, we use POT [65], a Python library for optimal transport, from which we obtain the quantities required to compute the gradients of the Wasserstein distance w.r.t. the weights in Eq. 14.

``` Input: initial policy \(\pi(\bm{a}|\bm{s})\)
1:while not goal reached do
2: Rollout policy \(\pi(\bm{a}|\bm{s})\) in the environment for \(M\) episodes to collect interaction trajectories \(\tau=\{(\bm{s}_{0},\bm{a}_{0},\bm{r}_{0}),(\bm{s}_{1},\bm{a}_{1},\bm{r}_{1}), \ldots,(\bm{s}_{T},\bm{a}_{T},\bm{r}_{T})\}_{m=1}^{M}\)
3:repeat
4: Update Gaussian components parameters \(\bm{\hat{\mu}}\), \(\bm{\hat{\Sigma}}\) using Riemannian optimization (12), where \(\lambda^{ls}\) is determined via line-search (see SS3.2).
5:until convergence
6:repeat
7: Update GMM weights \(\bm{\hat{\omega}}\) via gradient descent on the free energy objective 10, using 14
8:until converged
9:endwhile ```

**Algorithm 1** GMM Policy Optimization via Wasserstein Gradient Flows

The full policy optimization finishes if either the objective stops improving or the Wasserstein distance between the old and optimized GMMs exceeds a predefined threshold, which is chosen experimentally. Afterwards, fresh rollouts are performed with the updated policy and the aforementioned two-step alternating procedure starts over. This optimization loop is repeated until a task-specific success criterion has been fulfilled. We summarize the proposed optimization in Algorithm 1.

``` Input: initial policy \(\pi(\bm{a}|\bm{s})\)
1:while not goal reached do
2: Rollout policy \(\pi(\bm{a}|\bm{s})\) in the environment for \(M\) episodes to collect interaction trajectories \(\tau=\{(\bm{s}_{0},\bm{a}_{0},\bm{r}_{0}),(\bm{s}_{1},\bm{a}_{1},\bm{r}_{1}), \ldots,(\bm{s}_{T},\bm{a}_{T},\bm{r}_{T})\}_{m=1}^{M}\)
3:repeat
4: Update Gaussian components parameters \(\bm{\hat{\mu}}\), \(\bm{\hat{\Sigma}}\) using Riemannian optimization (12), where \(\lambda^{ls}\) is determined via line-search (see SS3.2).
5:until convergence
6:repeat
7: Update GMM weights \(\bm{\hat{\omega}}\) via gradient descent on the free energy objective 10, using 14
8:until converged
9:endwhile ```

**Algorithm 2** GMM Policy Optimization via Wasserstein Gradient Flows

## 4 Experiments

We tested our approach in three different robotic settings: a reaching skill, a collision-free trajectory tracking, and a multiple-goal task. All these tasks were represented in a \(2\)D operational space. Moreover, we tested our approach on two additional versions of the collision-free trajectory tracking task that are formulated for \(3\)D operational-space and \(7\)D joint-space representations of the end-effector state and action. These two latter tasks were performed by a \(7\)-DoF Franka Emika Panda robot in a virtual environment as reported in App. A.6.3. All robot motion policies were initially learned from human demonstrations collected on a Python GUI. We assumed we were given \(M\) demonstrations, each of which contained \(T_{m}\) data points for a dataset of \(T=\sum_{m}T_{m}\) total observations\(\tau=\{(\bm{s}_{t},\bm{a}_{t})\}_{t=1}^{T}\). The state \(\bm{s}\) and action \(\bm{a}\) corresponded to the robot end-effector position \(\bm{x}\in\mathbb{R}^{d}\) and velocity \(\dot{\bm{x}}\in\mathbb{R}^{d}\), with \(d\) being the operational space dimensionality. The GMM models were initially trained via classical Expectation-Maximization. The policy rollout consisted of sampling a velocity action \(\bm{a}_{t}\sim\pi(\bm{a}_{t}|\bm{s}_{t})\) using Eq. 9, and subsequently commanding the robot via a Cartesian velocity controller at a frequency of \(100\mathrm{Hz}\). For all the experiments, we used the Robotics Toolbox for Python [66] to simulate the robotic environments.

To show the importance of accounting for the policy structure in RL settings, we compared our method against two structure-unaware baselines: PPO [45] and SAC-GMM [31]. Our baselines choice is motivated by the fact that these two RL methods are still widely used and fairly competitive [67], even in real robotic scenarios [68]. We also considered an additional baseline: PMOE [41], which is a method to train deep RL policies using a probabilistic mixture of experts via GMMs. As PPO was not originally designed to directly optimize the parameters of a previously-learned GMM policy, we designed the policy actions to represent (usually small) corrections to the GMM parameters, i.e. \(\bm{a}=[\Delta\bm{\omega}~{}\Delta\hat{\bm{\mu}}~{}\Delta~{}\mathrm{vec}( \hat{\bm{\Sigma}})]\), following the same methodology as SAC-GMM [31]. The PPO and SAC implementations correspond to the code provided by Stable-Baselines3 [69], whose policies are parametrized by MLP networks. The PMOE implementation corresponds to the code given in the original paper [41]. During policy optimization, we sampled an action from the MLP policy that was then used to update the GMM parameters by adding the computed corrections to the current parameters. Later, we proceeded as described earlier, namely, the updated GMM was used to compute the velocity action via Eq. 9. For comparison purposes, we report statistical results for all the settings over \(5\) runs for the task success rate and solution variance. We tuned the baselines separately for each task using Optuna [70]. In addition, to assess the importance of our Riemannian formulation, we performed an ablation where we used the implicit scheme based on Euclidean gradient descent instead of the explicit optimization on the Bures-Wasserstein manifold (see App. A.6.2).

### Tasks description

Reaching Task:This experiment consists of: (1) learning an initial GMM policy such that the robot end-effector reaches a target by following an L-shape trajectory from its initial position, and (2) adapting the GMM policy to reach a new target located midway and above the previously-learned L-shape trajectories. The initial policy, shown in Fig. 2_-left_ and Fig. 12(a), was learned from \(12\) demonstrations and encoded by a \(7\)-component GMM. To adapt the policy, we defined a dense reward as a function of the position error between the robot end-effector and the new target. We also added a sparse penalty term that punishes rollouts leading to significantly divergent trajectories. Each optimization episode comprised \(10\) rollouts, each of maximum horizon length of \(200\) iterations. Convergence is achieved when a minimum average position error w.r.t the target - computed over an episode - is reached.

Collision-avoidance Task:This task consists of: (1) learning an initial GMM policy of a linear reaching motion, and (2) adapting the GMM policy to reach a new horizontally-translated target while avoiding to collide with two spherical obstacles located midway between the initial robot position and the new target. The initial GMM policy was learned from \(10\) human demonstrations and represented by a \(3\)-component GMM, as shown in Fig. 2_-middle_ and Fig. 12(b). For policy optimization, we defined a sparse reward as a function of the position error between the robot end-effector position and the target at the end of the rollout. We also included two sparse penalty terms: the first one punishes rollouts leading to collisions with the obstacles, for which the rollout is stopped; the second term penalizes rollouts with significantly divergent trajectories. Each episode consisted of \(10\) rollouts, each of maximum horizon length of \(150\) iterations. Convergence is determined by a minimum average position error w.r.t the target computed over an episode.

Multiple-goal Task:This setting involves: (1) learning an initial GMM policy where the robot end-effector reaches two different targets (i.e., task goals) starting from the same initial position, and (2) adapting the initial policy to reach a new target located close to one of the previous task goals. The intended adapted behavior should make the robot go through the most relevant GMM components according to the new target location. The initial GMM policy was learned from \(12\) demonstrations and encoded by a \(6\)-component GMM, as shown in Fig. 2_-right_ and Fig. 12(c). To optimize the initial GMM policy, we specified a sparse reward based on the position error between the robot end-effector position and the chosen target at the end of the rollout. Similar to the previous experiments, we added a sparse penalty term to penalize rollouts generating significantly divergent trajectories. Anepisode comprised \(10\) rollouts, each of maximum horizon length of \(200\) iterations. Again, the policy optimization converges when the average position error w.r.t the chosen target reaches a minimum threshold.

### Results Analysis

The reaching task tested our method's ability to adapt a previously-learned reaching skill to a new goal, located at \((6.0,-6.5)\) (cf. Fig. 13a-_left_). Achieving this required to adapt the Gaussian parameters of mainly the last four GMM components, while the others remained unchanged. We compared all methods in terms of the success rate over environment steps, where the success rate was defined as the percentage of rollouts reaching the new goal. Figure 3-_left_ shows that our method achieved a success rate of \(1\) after approximately \(70000\) environment interactions. Despite PPO was also able to complete the task reliably, it required many more environment steps (cf. Fig. 5-_left_). In sharp contrast, SAC did not reach any improvement. These observations underline the importance of some kind of trust region or constraint on the policy updates, which allowed both our method and PPO to reach good success rates. The PMOE versions of PPO and SAC showed a better performance than the vanilla counterparts, which aligns with the experimental insights provided in the original PMOE paper [41] regarding the advantages of the probabilistic mixture of experts in deep RL policies. Nevertheless, our approach also outperformed both PMOE versions. This experiment showed that our method is much more sample-efficient in adapting the GMM parameters, which we attribute to the fact that our method explicitly takes the GMM structure into account in the formulation of the optimization.

In the collision-avoidance task, we tested whether our method was able to adapt a trajectory tracking skill in order to avoid collisions with newly added obstacles. These were placed in such a way that the robot was forced to move its end-effector through a narrow path between the obstacles (cf. Fig. 2-_middle_). While the reaching task could be adapted by mainly varying the means of the GMM components, this task also demands to adapt the covariance of the second GMM component. Figure 3-_middle_ shows that our method solved this task reliably after comparatively few environment interactions. Although PPO also achieved a success rate of \(1\), it took \(6\) times more environment steps than our method. SAC only reached an average success rate of \(0.8\), however with high variance (cf. Fig. 5-_middle_). Note that the PMOE versions showed a reduced solution variance and a higher success rate than their vanilla counterparts, where PMOE-PPO also outperformed PMOE-SAC. These results again show the importance of the constraints on the policy updates. The huge discrepancy in the required environment steps between our method and all the baselines further emphasizes the importance of taking the GMM structure into account in the policy optimization. The explicit consideration of this structure allows our method to identify the required changes to the GMM parameters much more efficiently. These experimental insights can also be observed in the results corresponding to the other formulations of the collision-avoidance task, namely, the \(3\)D operational and \(7\)D joint space representations, reported in App. A.6.3.

While the previous two tasks were accomplished by adapting mostly the Gaussian parameters of the GMM, the multiple-goal task required to adapt the GMM weights. The initial skill comprised

Figure 2: The three tested \(2\)D robotic settings: a reaching skill (_left_), a collision-free trajectory tracking (_middle_), and a multiple-goal task (_right_). The robot color goes from light gray to black to show the evolution of the task reproduction. Green Gaussian components () depict the initial GMM policy, projected on the \(2\)D Cartesian position space. The end-effector trajectory resulting from the initial GMM policy is shown in dark blue lines (). Red circles () in the collision-avoidance task represent the obstacles (_middle_). The different targets of the multiple-goal task (_right_) are depicted as red stars.

reaching motions to two different goals and an execution of this skill results in reaching one of them, depending on the sampling noise (cf. Fig. 13c). The easiest way to adapt the policy to reach only one of the two goals is to reduce the GMM weights of the components belonging to the undesired motion and correspondingly increase the weights of the other components. As shown in Fig. 3-_right_, our method again quickly achieved a success rate of \(1\). PPO required substantially many more environment steps, while SAC was not able to solve the task. Again, the PMOE formulation reduced the solution variance and slightly increased the success rate of both PPO and SAC.

In Fig. 4 we report the success rate variance over \(5\) runs at a fixed time step, which corresponded to the step at which the first method achieved a success rate of \(1\), thus prioritizing sample efficiency. The plots show that our method exhibits a very low solution variance. All baselines varied largely, except for the reaching task, where all SAC runs collapsed to a success rate of \(0\). These results show that our method, despite showing large variance at the start, was able to quickly reduce the variance and converge reliably to a good success rate. We also provide similar plots of solution variance in Fig. 6, where we report the results for both PPO and SAC using their own convergence time step.

## 5 Conclusions and Future Work

We presented a novel method for GMM policy optimization, which leverages optimal transport theory to formulate the policy optimization as a Wasserstein gradient flow on the manifold of GMMs. Our formulation explicitly accounts for the GMM structure in the optimization and furthermore enables us to naturally constrain the policy updates by the \(L^{2}\)-Wasserstein distance between GMMs to enhance the stability of the policy optimization process. Moreover, the embedding of the Gaussian components of the GMM policy in the Bures-Wasserstein manifold greatly reduced the computational cost of the policy optimization. Experiments on several robotic tasks provided strong evidence of the importance of our policy-structure aware optimization against approaches that disregard the GMM structure. A possible limitation of our method is that each optimization loop involves running the Sinkhorn algorithm, which is computationally expensive. This might be improved by employing recent advances on initializing the Sinkhorn algorithm [71]. Also, we observed an intricate interplay between the optimization of the GMM weights and the Gaussian parameters, which occasionally resulted in one update hampering the other. In future work we plan to address the latter problem by using separate adaptive learning rates for weights and Gaussian parameters. Another possibility would entail to reformulate the policy optimization as a gradient flow on the space of probability measures endowed with the Wasserstein Fisher-Rao metric [72; 73; 74]. This alternative metric may allow us to leverage the Fisher-Rao geometry to optimize the GMM weights, as very recently proposed in [75] to learn isotropic Gaussian mixtures via particle-based approximations. Finally it would be interesting to combine our method with an actor-critic formulation and to replace the multi-step cumulative reward by a trained \(Q\)-function.

Figure 4: Variance of the success rate over the \(5\) runs for our method (WGF) and the baselines on the reaching task (_left_), the collision avoidance task (_middle_) and the multiple-goal task (_right_). The violate plots are overlaid with box plots, quartile lines and a swarm plot, where dots indicate the success rates of individual runs. The plots show the variance at the following time steps from left to right: \(80000\), \(90000\), \(95000\).

Figure 3: Success rate of our method (WGF) and the baselines on the reaching (_left_), the collision-avoidance (_middle_) and the multiple-goal tasks (_right_). The shaded area depicts the standard deviation over \(5\) runs.

## References

* Peters et al. [2016] Jan Peters, Daniel D. Lee, Jens Kober, Duy Nguyen-Tuong, J. Andrew Bagnell, and Stefan Schaal. _Robot Learning_, pages 357-398. Springer International Publishing, 2016. ISBN 978-3-319-32552-1. URL https://doi.org/10.1007/978-3-319-32552-1_15.
* Schaal et al. [2003] Stefan Schaal, Auke Ijspeert, and Aude Billard. Computational approaches to motor learning by imitation. _Phil. Trans. R. Soc. Lond. B_, 358:537-547, 2003. URL http://doi.org/10.1098/rstb.2002.1258.
* Kober et al. [2013] Jens Kober, J. Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. _The International Journal of Robotics Research (IJRR)_, 32(11):1238-1274, 2013. URL https://doi.org/10.1177/0278364913495721.
* Chatzilygeroudis et al. [2020] Konstantinos Chatzilygeroudis, Vassilis Vassiliades, Freek Stulp, Sylvain Calinon, and Jean-Baptiste Mouret. A survey on policy search algorithms for learning robot controllers in a handful of trials. _IEEE Transactions on Robotics_, 36(2):328-347, 2020. URL https://doi.org/10.1109/TRO.2019.2958211.
* Stulp and Sigaud [2012] Freek Stulp and Olivier Sigaud. Policy improvement methods: Between black-box optimization and episodic reinforcement learning. hal-00738463, 2012. URL http://hal.archives-ouvertes.fr/hal-00738463.
* Jaquier et al. [2019] Noemie Jaquier, Leonel Rozo, Sylvain Calinon, and Mathias Burger. Bayesian optimization meets Riemannian manifolds in robot learning. In _Conference on Robot Learning (CoRL)_, pages 233-246, 2019. URL https://proceedings.mlr.press/v100/jaquier20a.html.
* Gu et al. [2017] Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates. In _IEEE International Conference on Robotics and Automation (ICRA)_, pages 3389-3396, 2017. URL https://doi.org/10.1109/ICRA.2017.7989385.
* Ibarz et al. [2021] Julian Ibarz, Jie Tan, Chelsea Finn, Mrinal Kalakrishnan, Peter Pastor, and Sergey Levine. How to train your robot with deep reinforcement learning: lessons we have learned. _The International Journal of Robotics Research (IJRR)_, 40(4-5):698-721, 2021. URL https://doi.org/10.1177/0278364920987859.
* Deisenroth et al. [2013] Marc Peter Deisenroth, Gerhard Neumann, and Jan Peters. A survey on policy search for robotics. _Found. Trends Robot_, 2(1-2):1-142, 2013. URL https://doi.org/10.1561/2300000021.
* Ijspeert et al. [2013] Auke Jan Ijspeert, Jun Nakanishi, Heiko Hoffmann, Peter Pastor, and Stefan Schaal. Dynamical movement primitives: Learning attractor models for motor behaviors. _Neural Computation_, 25:328-373, 2013. URL https://doi.org/10.1162/NECO_a_00393.
* Calinon et al. [2007] Sylvain Calinon, Florent Guenter, and Aude Billard. On learning, representing, and generalizing a task in a humanoid robot. _IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)_, 37(2):286-298, 2007. URL https://doi.org/10.1109/TSMCB.2006.886952.
* Paraschos et al. [2018] Alexandros Paraschos, Christian Daniel, Jan Peters, and Gerhard Neumann. Using probabilistic movement primitives in robotics. _Autonomous Robots_, 42:529-551, 2018. URL https://doi.org/10.1007/s10514-017-9648-7.
* Rozo and Dave [2021] Leonel Rozo and Vedant Dave. Orientation probabilistic movement primitives on Riemannian manifolds. In _Conference on Robot Learning (CoRL)_, pages 373-383, 2021. URL https://proceedings.mlr.press/v164/rozo22a.html.
* Bahl et al. [2020] Shikhar Bahl, Mustafa Mukadam, Abhinav Gupta, and Deepak Pathak. Neural dynamic policies for end-to-end sensorimotor learning. In _Neural Information Processing Systems (NeurIPS)_, pages 5058-5069, 2020. URL https://proceedings.neurips.cc/paper/2020/file/354ac345fd8c6d7ef634d9a8e3d47b83-Paper.pdf.
* Cederborg et al. [2010] Thomas Cederborg, Ming Li, Adrien Baranes, and Pierre-Yves Oudeyer. Incremental local online Gaussian mixture regression for imitation learning of multiple tasks. In _IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 267-274, 2010. URL https://doi.org/10.1109/IROS.2010.5652040.
* Calinon [2016] Sylvain Calinon. A tutorial on task-parameterized movement learning and retrieval. _Intelligent Service Robotics_, 9(1):1-29, January 2016. ISSN 1861-2776. URL https://doi.org/10.1007/s11370-015-0187-9.

* Jaquier et al. [2019] Noemie Jaquier, David Ginsbourger, and Sylvain Calinon. Learning from demonstration with model-based Gaussian process. In _Conference on Robot Learning (CoRL)_, pages 247-257, 2019. URL http://proceedings.mlr.press/v100/jaquier20b.html.
* Jaquier et al. [2021] N. Jaquier, L. Rozo, D. G. Caldwell, and S. Calinon. Geometry-aware manipulability learning, tracking and transfer. 20(3):624-650, 2021. URL https://doi.org/10.1177/0278364920946815.
* Zhu et al. [2022] Jihong Zhu, Michael Gienger, and Jens Kober. Learning task-parameterized skills from few demonstrations. _IEEE Robotics and Automation Letters (RA-L)_, 7(2):4063-4070, 2022. URL https://doi.org/10.1109/LRA.2022.3150013.
* Khansari-Zadeh and Billard [2011] S. Mohammad Khansari-Zadeh and Aude Billard. Learning stable nonlinear dynamical systems with Gaussian mixture models. _IEEE Transactions on Robotics (T-RO)_, 27(5):943-957, 2011. URL https://doi.org/10.1109/TRO.2011.2159412.
* Ravichandar et al. [2017] Harish Ravichandar, Iman Salehi, and Ashwin Dani. Learning partially contracting dynamical systems from demonstrations. In _Conference on Robot Learning (CoRL)_, pages 369-378, 2017. URL https://proceedings.mlr.press/v78/ravichandar17a.html.
* Figueroa and Billard [2018] Nadia Figueroa and Aude Billard. A physically-consistent Bayesian non-parametric mixture model for dynamical system learning. In _Conference on Robot Learning (CoRL)_, pages 927-946, 2018. URL https://proceedings.mlr.press/v87/figueroa18a.html.
* Ewerton et al. [2015] Marco Ewerton, Gerhard Neumann, Rudolf Lioutikov, Heni Ben Amor, Jan Peters, and Guilherme Maeda. Learning multiple collaborative tasks with a mixture of interaction primitives. In _IEEE International Conference on Robotics and Automation (ICRA)_, pages 1535-1542, 2015. URL https://doi.org/10.1109/ICRA.2015.7139393.
* Rozo et al. [2016] Leonel Rozo, Sylvain Calinon, Darwin G. Caldwell, Pablo Jimenez, and Carme Torras. Learning physical collaborative robot behaviors from human demonstrations. _IEEE Transactions on Robotics_, 32(3):513-527, 2016. URL https://doi.org/10.1109/TRO.2016.2540623.
* Kyrarini et al. [2019] Maria Kyrarini, Muhammad Abdul Haseeb, Danijela Ristic-Durrant, and Axel Graser. Robot learning of industrial assembly task via human demonstrations. _Autonomous Robots_, 43:239-257, 2019. URL https://doi.org/10.1007/s10514-018-9725-6.
* Rozo et al. [2020] Leonel Rozo, Meng Guo, Andras G. Kupcsik, Marco Todescato, Philipp Schillinger, Markus Giftthaler, Matthias Ochs, Markus Spies, Niccolai Waniek, Patrick Kesper, and Mathias Burger. Learning and sequencing of object-centric manipulation skills for industrial tasks. In _IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 9072-9079, 2020. URL https://doi.org/10.1109/IROS45743.2020.9341570.
* Lin et al. [2012] Yun Lin, Shaogang Ren, Matthew Clevenger, and Yu Sun. Learning grasping force from demonstration. In _IEEE International Conference on Robotics and Automation (ICRA)_, pages 1526-1531, 2012. URL https://doi.org/10.1109/ICRA.2012.6225222.
* Abu-Dakka et al. [2018] Fares J. Abu-Dakka, Leonel Rozo, and Darwin G. Caldwell. Force-based variable impedance learning for robotic manipulation. _Robotics and Autonomous Systems (RAS)_, 109:156-167, 2018. URL https://doi.org/10.1016/j.robot.2018.07.008.
* Huang et al. [2019] Yanlong Huang, Leonel Rozo, Joao Silverio, and Darwin G. Caldwell. Kernelized movement primitives. _The International Journal of Robotics Research (IJRR)_, 38(7):833-852, 2019. doi: 10.1177/0278364919846363. URL https://doi.org/10.1177/0278364919846363.
* Arenz et al. [2020] Oleg Arenz, Mingjun Zhong, and Gerhard Neumann. Trust-region variational inference with Gaussian mixture models. _Journal of Machine Learning Research (JMLR)_, 21(163):1-60, 2020. URL http://jmlr.org/papers/v21/19-524.html.
* Nematollahi et al. [2022] Iman Nematollahi, Erick Rosete-Beas, Adrian Rofer, Tim Welschehold, Abhinav Valada, and Wolfram Burgard. Robot skill adaptation via soft actor-critic Gaussian mixture models. In _International Conference on Robotics and Automation (ICRA)_, pages 8651-8657, 2022. URL https://doi.org/10.1109/ICRA46639.2022.9811770.
* Santambrogio [2015] Filippo Santambrogio. _Optimal Transport for Applied Mathematicians_. Birkhauser, Cham, 1 edition, 2015. URL https://doi.org/10.1007/978-3-319-20828-2.
* Peyre and Cuturi [2019] Gabriel Peyre and Marco Cuturi. Computational optimal transport: With applications to data science. _Foundations and Trends in Machine Learning_, 11(5-6):355-607, 2019. URL http://dx.doi.org/10.1561/2200000073.

* Chen et al. [2019] Yongxin Chen, Tryphon T. Georgiou, and Allen Tannenbaum. Optimal transport for Gaussian mixture models. _IEEE Access_, 7:6269-6278, 2019. URL https://doi.org/10.1109/ACCESS.2018.2889838.
* Delon and Desolneux [2020] Julie Delon and Agnes Desolneux. A Wasserstein-type distance in the space of Gaussian mixture models. _SIAM Journal on Imaging Sciences_, 13(2):936-970, 2020. URL https://doi.org/10.1137/19M1301047.
* Richemond and Maginnis [2017] Pierre H. Richemond and Brendan Maginnis. On Wasserstein reinforcement learning and the Fokker-Planck equation, 2017. URL https://arxiv.org/abs/1712.07185. arXiv:1712.07185.
* Jordan et al. [1996] Richard Jordan, David Kinderlehrer, and Felix Otto. The variational formulation of the Fokker-Planck equation. _Siam Journal on Applied Mathematics_, 1996. URL https://doi.org/10.1137/S0036141096303359.
* Zhang et al. [2018] Ruiyi Zhang, Changyou Chen, Chunyuan Li, and Lawrence Carin. Policy optimization as Wasserstein gradient flows. In _International Conference on Machine Learning (ICML)_, pages 5737-5746, 2018. URL https://proceedings.mlr.press/v80/zhang18a.html.
* Mokrov et al. [2021] Petr Mokrov, Alexander Korotin, Lingxiao Li, Aude Genevay, Justin Solomon, and Evgeny Burnaev. Large-scale Wasserstein gradient flows. In _Neural Information Processing Systems (NeurIPS)_, 2021. URL https://openreview.net/forum?id=nllJJuhMfHP.
* Haarnoja et al. [2018] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In _International Conference on Machine Learning (ICML)_, pages 1861-1870, 2018. URL https://proceedings.mlr.press/v80/haarnoja18b.html.
* Ren et al. [2021] Jie Ren, Yewen Li, Zihan Ding, Wei Pan, and Hao Dong. Probabilistic mixture-of-experts for efficient deep reinforcement learning, 2021. URL https://arxiv.org/abs/2104.09122.
* Wang et al. [2020] Shijun Wang, Baocheng Zhu, Chen Li, Mingzhe Wu, James Zhang, Wei Chu, and Yuan Qi. Riemannian proximal policy optimization, 2020. URL https://arxiv.org/abs/2005.09195. arXiv:2005.09195.
* Amari [1998] Shun-ichi Amari. Natural gradient works efficiently in learning. _Neural Computation_, 10(2):251-276, 1998. URL https://doi.org/10.1162/089976698300017746.
* Kakade [2001] Sham M Kakade. A natural policy gradient. In _Neural Information Processing Systems (NeurIPS)_, 2001. URL https://proceedings.neurips.cc/paper_files/paper/2001/file/4b86abe48d358ecf194c56c69108433e-Paper.pdf.
* Schulman et al. [2017] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347.
* Galashov et al. [2019] Alexandre Galashov, Siddhant Jayakumar, Leonard Hasenclever, Dhruva Tirumala, Jonathan Schwarz, Guillaume Desjardins, Wojtek M. Czarnecki, Yee Whye Teh, Razvan Pascanu, and Nicolas Heess. Information asymmetry in KL-regularized RL. In _International Conference on Learning Representations (ICLR)_, 2019. URL https://openreview.net/forum?id=S11qMn05Ym.
* Pacchiano et al. [2020] Aldo Pacchiano, Jack Parker-Holder, Yunhao Tang, Krzysztof Choromanski, Anna Choromanska, and Michael Jordan. Learning to score behaviors for guided policy optimization. In _International Conference on Machine Learning (ICML)_, pages 7445-7454, 2020. URL https://proceedings.mlr.press/v119/pacchiano20a.html.
* Moskovitz et al. [2021] Ted Moskovitz, Michael Arbel, Ferenc Huszar, and Arthur Gretton. Efficient Wasserstein natural gradients for reinforcement learning. In _International Conference on Learning Representations (ICLR)_, 2021. URL https://openreview.net/forum?id=OhgnfSrn2jv.
* Lin et al. [2020] Wu Lin, Mark Schmidt, and Mohammad Emtiyaz Khan. Handling the positive-definite constraint in the Bayesian learning rule. In _International Conference on Machine Learning (ICML)_, pages 6116-6126, 2020. URL https://proceedings.mlr.press/v119/lin20d.html.
* Santambrogio [2017] Filippo Santambrogio. Euclidean, metric, and Wasserstein gradient flows: an overview. _Bull. Math. Sci._, 7:87-154, 2017. URL https://doi.org/10.1007/s13373-017-0101-1.
* Ambrosio et al. [2005] Luigi Ambrosio, Nicola Gigli, and Giuseppe Savare. _Gradient flows: in metric spaces and in the space of probability measures_. Springer Science & Business Media, 2005. URL https://doi.org/10.1007/b137080.
** Panaretos and Zemel [2020] Victor M. Panaretos and Yoav Zemel. _The Wasserstein Space_, pages 37-57. Springer International Publishing, 2020. URL https://doi.org/10.1007/978-3-030-38438-8_2.
* Ghahramani and Jordan [1994] Zoubin Ghahramani and Michael Jordan. Supervised learning from incomplete data via an EM approach. In _Advances in Neural Information Processing Systems (NeurIPS)_, 1994. URL https://proceedings.neurips.cc/paper/1993/file/f2201f5191c4e92cc5af043aebfd0946-Paper.pdf.
* Eysenbach and Levine [2022] Benjamin Eysenbach and Sergey Levine. Maximum entropy RL (provably) solves some robust RL problems. In _International Conference on Learning Representations (ICLR)_, 2022. URL https://openreview.net/forum?id=Pt&AD3caaA2.
* Schulman et al. [2015] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In _International Conference on Machine Learning (ICML)_, page 1889-1897, 2015. URL https://proceedings.mlr.press/v37/schulman15.html.
* Otto et al. [2021] Fabian Otto, Philipp Becker, Ngo Anh Vien, Hanna Carolin Ziesche, and Gerhard Neumann. Differentiable trust region layers for deep reinforcement learning. In _International Conference on Learning Representations (ICLR)_, 2021. URL https://openreview.net/forum?id=qYZD-AO1Vn.
* Otto [2001] Felix Otto. The geometry of dissipative evolution equations:: The porus medium equation. _Communications in Partial Differential Equations_, 26(1-2):101-174, 2001. URL https://doi.org/10.1081/PDE-100002243.
* Chen and Li [2020] Yifan Chen and Wuchen Li. Optimal transport natural gradient for statistical manifolds with continuous sample space. _Information Geometry_, 3, 06 2020. URL https://doi.org/10.1007/s41884-020-00028-0.
* Malago et al. [2018] Luigi Malago, Luigi Montrucchio, and Giovanni Pistone. Wasserstein Riemannian geometry of Gaussian densities. _Information Geometry_, 1:137-179, 2018. URL https://doi.org/10.1007/s41884-018-0014-4.
* Han et al. [2021] Andi Han, Bamdev Mishra, Pratik Jawanpuria, and Junbin Gao. On Riemannian optimization over positive definite matrices with the Bures-Wasserstein geometry. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021. URL https://openreview.net/forum?id=ZCHxGFmc62a.
* Boumal [2023] Nicolas Boumal. _An introduction to optimization on smooth manifolds_. Cambridge University Press, 2023. URL http://www.nicolasboumal.net/book.
* Williams [2004] Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. _Machine Learning_, 8:229-256, 2004. URL https://doi.org/10.1007/BF00992696.
* Cuturi and Doucet [2014] Marco Cuturi and Arnaud Doucet. Fast computation of Wasserstein barycenters. In _International Conference on Machine Learning (ICML)_, pages 685-693, 2014. URL https://proceedings.mlr.press/v32/cuturi14.html.
* Townsend et al. [2016] James Townsend, Niklas Koep, and Sebastian Weichwald. Pymanopt: A Python toolbox for optimization on manifolds using automatic differentiation. _Journal of Machine Learning Research (JMLR)_, 17(137):1-5, 2016. URL http://jmlr.org/papers/v17/16-177.html.
* Flamary et al. [2021] Remi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya, Aurelie Boisbunon, Stanislas Chambon, Laetitia Chapel, Adrien Corenlos, Kilian Fatras, Nemo Fournier, Leo Gautheron, Nathalie T.H. Gayraud, Hicham Janati, Alain Rakotomamonjy, Ievgen Redko, Antoine Rolet, Antony Schutz, Vivien Seguy, Danie J. Sutherland, Romain Tavenard, Alexander Tong, and Titouan Vayer. Pot: Python optimal transport. _Journal of Machine Learning Research (JMLR)_, 22(78):1-8, 2021. URL http://jmlr.org/papers/v22/20-451.html.
* the robotics toolbox reinvented for Python. In _IEEE International Conference on Robotics and Automation (ICRA)_, pages 11357-11363, 2021. URL https://10.1109/ICRA48506.2021.9561366.
* D'Eramo et al. [2021] Carlo D'Eramo, Davide Tateo, Andrea Bonarini, Marcello Restelli, and Jan Peters. Mushroomrl: Simplifying reinforcement learning research. _Journal of Machine Learning Research (JMLR)_, 22(131):1-5, 2021. URL http://jmlr.org/papers/v22/18-056.html.
* Basd Ali Shahid et al. [2022] Francesco Braghin Asad Ali Shahid, Dario Piga and Loris Roveda. Continuous control actions learning and adaptation for robotic manipulation through reinforcement learning. _Autonomous Robots_, 46:483-498, 2022. URL https://doi.org/10.1007/s10514-022-10034-z.

* Raffin et al. [2021] Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-baselines3: Reliable reinforcement learning implementations. _Journal of Machine Learning Research_, 22(268):1-8, 2021. URL http://jmlr.org/papers/v22/20-1364.html.
* Akiba et al. [2019] Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: A next-generation hyperparameter optimization framework. In _ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)_, page 2623-2631, 2019. URL https://doi.org/10.1145/3292500.3330701.
* Thornton and Cuturi [2022] James Thornton and Marco Cuturi. Rethinking initialization of the Sinkhorn algorithm, 2022. URL https://arxiv.org/abs/2206.07630. arXiv:2206.07630.
* Chizat et al. [2015] Lenaic Chizat, Bernhard Schmitzer, Gabriel Peyre, and Francois-Xavier Vialard. An interpolating distance between optimal transport and Fisher-Rao, 2015. URL https://arxiv.org/abs/1506.06430.
* Gallouet and Monsaingeon [2017] Thomas O. Gallouet and Leonard Monsaingeon. A JKO splitting scheme for Kantorovich-Fisher-Rao gradient flows. _SIAM Journal on Mathematical Analysis_, 49(2):1100-1130, 2017. URL https://doi.org/10.1137/16M10666X.
* Liero et al. [2018] Matthias Liero, Alexander Mielke, and Giuseppe Savare. Optimal entropy-transport problems and a new Hellinger-Kantorovich distance between positive measures. _Inventiones mathematicae_, 211, 03 2018. URL https://doi.org/10.1007/s00222-017-0759-8.
* Yan et al. [2023] Yuling Yan, Kaizheng Wang, and Philippe Rigollet. Learning Gaussian mixtures using the Wasserstein-Fisher-Rao gradient flow, 2023. URL https://arxiv.org/abs/2301.01766.

Appendix

### Details on Gaussian Mixture Regression (GMR)

In GMR we start from a GMM in state-action space \(\pi(\bm{s},\bm{a})=\sum_{i=1}^{N}\omega_{i}\mathcal{N}([\bm{s}\,\bm{a}]^{\mathsf{ T}};\bm{\mu}_{i},\bm{\Sigma}_{i})\) from which a policy, i.e. a probability distribution on the action space, can be obtained by conditioning on the state, as follows

\[\pi(\bm{a}|\bm{s})=\frac{\pi(\bm{s},\bm{a})}{\int\pi(\bm{s},\bm{a}) \mathrm{d}\bm{a}}.\] (15)

The resulting conditional distribution is another GMM on the action sapce, with state dependent parameters, given by:

\[\pi(\bm{a}_{t}|\bm{s}_{t}) =\sum_{i=1}^{N}\omega_{i}(\bm{s}_{t})\mathcal{N}(\bm{a}_{t};\bm{ \mu}_{i}^{a}(s_{t}),\bm{\Sigma}_{i}^{a}),\quad\text{with}\] (16) \[\bm{\mu}_{i}^{a}(s_{t}) =\bm{\mu}_{i}^{a}+\bm{\Sigma}_{i}^{as}(\bm{\Sigma}_{i}^{s})^{-1} (\bm{s}_{t}-\bm{\mu}_{i}^{s})\,,\] (17) \[\bm{\Sigma}_{i}^{a} =\bm{\Sigma}_{i}^{a}-\bm{\Sigma}_{i}^{as}(\bm{\Sigma}_{i}^{s})^{ -1}\bm{\Sigma}_{i}^{sa},\] (18) \[\omega_{i}(\bm{s}_{t}) =\frac{\omega_{i}\mathcal{N}(\bm{s}_{t};\bm{\mu}_{i}^{s},\bm{ \Sigma}_{i}^{s})}{\sum_{k}^{n}\omega_{k}\mathcal{N}(\bm{s}_{t};\bm{\mu}_{k}^{ s},\bm{\Sigma}_{k}^{s})}.\] (19)

Note that we have split the GMM parameters \(\bm{\mu}_{i}\) and \(\bm{\Sigma}_{i}\) into their state and action components according to

\[\bm{\mu}_{i}=\begin{pmatrix}\bm{\mu}_{i}^{s}\\ \bm{\mu}_{i}^{a}\end{pmatrix},\quad\bm{\Sigma}_{i}=\begin{pmatrix}\bm{\Sigma}_ {i}^{s}&\bm{\Sigma}_{i}^{sa}\\ \bm{\Sigma}_{i}^{as}&\bm{\Sigma}_{i}^{a}\end{pmatrix}.\] (20)

### Riemannian gradients and retractions

For completeness we give here the explicit expressions of the Riemannian gradients and the retractions used in SS 3.1. As the mean vectors are assumed to lie in the Euclidean space, their Riemannian gradients actually coincide with the Euclidean gradients and no retraction is required, so Eq. 12 reduces to the well-known Euclidean gradient descent

\[\bm{\hat{\mu}}_{k+1}=\bm{\hat{\mu}}_{k}+\nabla_{\bm{\hat{\mu}}}J(\pi_{k}),\] (21)

where \(\nabla_{\bm{\hat{\mu}}}\) denotes the Euclidean gradient w.r.t. \(\bm{\hat{\mu}}\). For the covariance matrices we use the gradient and retraction w.r.t. the Bures-Wasserstein manifold, taken from [59; 60]. The gradient is given by

\[\mathrm{grad}_{\bm{\hat{\Sigma}}}\,J(\pi_{k})=4\{\nabla_{\bm{\hat{\Sigma}}}J( \pi_{k})\bm{\hat{\Sigma}}\}_{S},\] (22)

where again \(\nabla_{\bm{\hat{\Sigma}}}\) denotes the Euclidean gradient w.r.t. \(\bm{\hat{\Sigma}}\) and \(\{\bm{X}\}_{S}=\frac{(\bm{X}+\bm{X}^{\mathsf{T}})}{2}\).

Furthermore, the retraction is given by

\[\mathrm{R}_{\bm{\Sigma}_{k}}\left(\hat{\bm{X}}\right)=\hat{\bm{\Sigma}}_{k}+ \hat{\bm{X}}+\mathcal{L}_{\hat{\bm{X}}}\left[\hat{\bm{\Sigma}}_{k}\right]\hat {\bm{X}}\mathcal{L}_{\hat{\bm{X}}}\left[\hat{\bm{\Sigma}}_{k}\right],\] (23)

where \(\mathcal{L}_{\hat{\bm{X}}}\left[\hat{\bm{\Sigma}}_{k}\right]\) is the Lyapunov operator, defined as the solution to the matrix linear system

\[\mathcal{L}_{\hat{\bm{X}}}\left[\hat{\bm{\Sigma}}_{k}\right]\hat{\bm{X}}+\hat {\bm{X}}\mathcal{L}_{\hat{\bm{X}}}\left[\hat{\bm{\Sigma}}_{k}\right]=\hat{ \bm{\Sigma}}_{k}.\] (24)

### Expressions of the free functional \(J(\pi)\) and its Euclidean gradients

For completeness sake, we provide here the explicit expression of the Euclidean gradients for the objective \(J(\pi)\) w.r.t. the parameters of the GMM, which are used in the construction of the Riemannian gradients. Using the policy gradient theorem, we obtain the gradient of Eq. 11 w.r.t to a 

[MISSING_PAGE_FAIL:17]

\[\nabla\bm{\omega}_{l}J(\pi) =\mathbb{E}_{\tau}\left[\sum_{t}\left(\frac{\mathcal{N}(\bm{s}_{t}, \bm{a}_{t};\bm{\mu}_{l},\bm{\Sigma}_{l})}{\sum_{j}\bm{\omega}_{j}\mathcal{N}( \bm{s}_{t},\bm{a}_{t};\bm{\mu}_{j},\bm{\Sigma}_{j})}-\frac{\int\mathrm{d}\bm{a }\mathcal{N}(\bm{s}_{t},\bm{a}_{t};\bm{\mu}_{l},\bm{\Sigma}_{l})}{\sum_{j}\bm {\omega}_{j}\int\mathrm{d}\bm{a}\mathcal{N}(\bm{s}_{t},\bm{a}_{t};\bm{\mu}_{j},\bm{\Sigma}_{j})}\right)\sum_{t^{\prime}>t}r(\bm{s}_{t},\bm{a}_{t})\right]\] (30) \[=\mathbb{E}_{\tau}\left[\sum_{t}\frac{(\zeta_{l,\bm{s}_{t},\bm{a} _{t}}-\zeta_{l,\bm{s}_{t}})}{\bm{\omega}_{l}}\sum_{t^{\prime}>t}r(\bm{s}_{t}, \bm{a}_{t})\right].\] (31)

Note that we introduced the responsibilities \(\zeta_{l,\bm{s}_{t},\bm{a}_{t}}\) and \(\zeta_{l,\bm{s}_{t}}\), which are defined as follows

\[\zeta_{l,\bm{s}_{t},\bm{a}_{t}} =\frac{\bm{\omega}_{l}\mathcal{N}(\bm{s}_{t},\bm{a}_{t};\bm{\mu}_ {l},\bm{\Sigma}_{l})}{\sum_{j}\bm{\omega}_{j}\mathcal{N}(\bm{s}_{t},\bm{a}_{t}; \bm{\mu}_{j},\bm{\Sigma}_{j})},\quad\text{and}\] (32) \[\zeta_{l,\bm{s}_{t}} =\frac{\bm{\omega}_{l}\int\mathrm{d}\bm{a}\mathcal{N}(\bm{s}_{t},\bm{a}_{t};\bm{\mu}_{l},\bm{\Sigma}_{l})}{\sum_{j}\bm{\omega}_{j}\int\mathrm{ d}\bm{a}\mathcal{N}(\bm{s}_{t},\bm{a}_{t};\bm{\mu}_{j},\bm{\Sigma}_{j})}= \frac{\bm{\omega}_{l}\mathcal{N}(\bm{s}_{t};\bm{\mu}_{l,s},\bm{\Sigma}_{l,ss}) }{\sum_{j}\bm{\omega}_{j}\mathcal{N}(\bm{s}_{t};\bm{\mu}_{j,s},\bm{\Sigma}_{j,ss})}.\] (33)

### Relation between forward and backward discretization in the Bures-Wasserstein metric

In this section we outline the relation between the implicit and explicit optimization schema w.r.t. the Bures-Wasserstein metric, which is leveraged to formulate our policy optimization in SS 3. We closely follow [58]. For the sake of simplicity, we group the Gaussian parameters \(\bm{\mu}\) and \(\bm{\Sigma}\) into a single parameter vector \(\bm{\theta}\). Furthermore, we restrict our explanation to a single Gaussian component, which is possible without loosing generality, as each of the \(N\) components live in its own manifold \(\mathbb{R}^{d}\times\mathcal{S}^{d}_{++}\).

The Riemannian gradient w.r.t the Gaussian parameters \(\bm{\theta}\), \(\mathrm{grad}_{\bm{\theta}}\,J(\pi(\bm{\theta}))\), satisfies by definition

\[g_{\bm{\theta}}(\mathrm{grad}_{\bm{\theta}}\,J(\pi(\bm{\theta}),\bm{\xi})= \nabla_{\bm{\theta}}J(\pi(\bm{\theta}))\cdot\bm{\xi},\] (34)

where \(\nabla_{\bm{\theta}}\) denotes the Euclidean gradient, \(\bm{\xi}\) is an arbitrary vector on the tangent space \(\mathcal{T}_{\bm{\theta}}\mathcal{M}\), and \(g_{\bm{\theta}}\) is the Riemannian metric tensor, defining the inner product on \(\mathcal{T}_{\bm{\theta}}\mathcal{M}\). The Riemannian metric \(g_{\bm{\theta}}\) can be written as

\[g_{\bm{\theta}}(\bm{\zeta},\bm{\xi})=\bm{\zeta}^{\mathsf{T}}G_{W}(\bm{\theta}) \bm{\xi},\] (35)

with two arbitrary tangent vectors \(\bm{\zeta}\), \(\bm{\xi}\), and \(G_{W}(\bm{\theta})\) being a positive definite matrix. Moreover, note that the Wasserstein distance \(W_{2}^{2}\big{(}\mathcal{N}(\bm{\theta}),\mathcal{N}(\bm{\theta}+\Delta\bm{ \theta})\big{)}\), where \(\Delta\bm{\theta}\) denotes a small perturbation in the Gaussian parameters \(\bm{\theta}\), can be expressed as

\[W_{2}^{2}\big{(}\mathcal{N}(\bm{\theta}),\mathcal{N}(\bm{\theta}+\Delta\bm{ \theta})\big{)}=\frac{1}{2}\big{(}\Delta\bm{\theta}^{\mathsf{T}}\big{)}G_{W}( \bm{\theta})\big{(}\Delta\bm{\theta}\big{)}+O\big{(}(\bm{\Delta\bm{\theta}})^{2 }\big{)},\] (36)

for \(\Delta\bm{\theta}\to 0\). Similarly, we can approximate the objective evaluated at \(J(\bm{\theta}+\Delta\bm{\theta})\) via the Taylor theorem as

\[J(\bm{\theta}+\Delta\bm{\theta})=J(\bm{\theta})+\nabla_{\bm{\theta}}J(\bm{ \theta})\cdot\Delta\bm{\theta}+O((\Delta\bm{\theta})^{2}).\] (37)

With this, we can approximate

\[\bm{\theta}_{k+1} =\operatorname*{arg\,min}_{\bm{\theta}}\left(\frac{W_{2}^{2}(\pi (\bm{\theta}),\pi(\bm{\theta}_{k}))}{2\tau}-J(\pi(\bm{\theta}))\right),\] (38) \[\approx\operatorname*{arg\,min}_{\bm{\theta}}\left(\frac{(\bm{ \theta}-\bm{\theta}_{\bm{k}})^{\mathsf{T}}G_{W}(\bm{\theta}-\bm{\theta}_{\bm{k }})}{2\tau}-\nabla_{\bm{\theta}}J(\bm{\theta})\cdot(\bm{\theta}-\bm{\theta}_{ \bm{k}})\right),\] (39)

from which we obtain the update equation for \(\bm{\theta}\) as follows

\[\bm{\theta}_{k+1}=\bm{\theta}_{k}+\tau G_{W}(\bm{\theta}_{k})^{-1}\nabla_{\bm{ \theta}}J(\pi(\bm{\theta}_{k})),\] (40)

which corresponds to the Wasserstein natural gradient with respect to the Bures-Wasserstein metric. Note that Eq. 40 in turn corresponds to an approximation of the exact Riemannian gradient descent

\[\bm{\theta}_{k+1}=\mathds{R}_{\bm{\theta}_{k}}\left(\tau\cdot\mathrm{grad}_{\bm{ \theta}}\,J(\pi(\bm{\theta}_{k}))\right).\] (41)This approximation can be obtained by considering a first-order approximation of the geodesic on the \(BW\) manifold. As the exponential map (a.k.a. the retraction) is defined via the geodesic, the retraction operator in Eq. 41 turns into a simple addition operation under a first-order approximation, leading to Eq. 40. Notice that such approximation does not guarantee that the updated parameters \(\bm{\theta}\) stay on the manifold, except for the cases in which \(\bm{\theta}\in\mathbb{R}^{d}\). This means that the positive-definite constraints arising from the covariance matrices can be easily violated when using natural gradient approaches as they do not guarantee that the updates stay on the underlying manifold. However, it is worth noting that the use of an inexact Riemannian gradient descent is often motivated by the difficulty of computing the exponential map (or geodesic) necessary to calculate the exact Riemannian update. In our case, we leverage the retraction and Riemannian gradients of [59; 60], which allow us to apply the exact Riemannian gradient descent of 41. This avoids to rely on first-order approximations and in turn we can guarantee that the updates of the Gaussian distribution parameters always lie on on the product manifold \(\left(\mathbb{R}^{d}\times\mathcal{S}^{d}_{++}\right)^{N}\).

### Additional details on the implementation

We extended the Pymanopt [64] by adding a custom line-search routine that accounts for a constraint on the Wasserstein distance between the old and the optimized GMMs. The details of this line-search can be found in Algorithm 2.

```
0: point \(\bm{x_{0}}\) on the manifold, descent direction \(\bm{d}\), initial step size \(\lambda_{0}\), decrement \(\alpha\), constraint \(c(\bm{x_{0}},.)\), maximum allowed value for constraint \(c_{\text{max}}\), minimum step size \(\lambda_{\text{min}}\) Output: step size \(s\), updated point on manifold \(\bm{x}\)
1:\(\bm{x}=\bm{x_{0}}+\lambda_{0}\cdot\bm{d}\)\(\lambda=\lambda_{0}\)
2:while\(c(\bm{x_{0}},\bm{x})>c_{\text{max}}\) and \(\lambda>\lambda_{\text{min}}\)do
3: decrease step size: \(\lambda=\alpha\cdot\lambda\) update point on manifold: \(\bm{x}=\bm{x_{0}}+\lambda\cdot\bm{d}\)
4:endwhile
5:if\(\lambda<\lambda_{\text{min}}\)then
6: return \(\lambda_{0}\), \(\bm{x_{0}}\)
7:else
8: return \(\lambda\), \(\bm{x}\)
9:endif ```

**Algorithm 2** Constrained line-search. The constraint function \(c(x_{0}.\cdot)\) is arbitrary in general. We use the \(L^{2}\)-Wasserstein distance between two points on the manifold of GMMs as constraint.

### Additional details on experiments

#### a.6.1 Additional results

Fig. 5 shows the convergence curves for the two baselines as in Fig. 3 of the main paper, however, we extended the horizontal axis up to the maximum number of environment steps used for training.

Fig. 6 shows the variance of the success rate for the three methods at their time step of convergence for all three robotic tasks. Concerning SAC, which did not converge afte

Figure 5: The success rate of the two baselines on the reaching task (_left_), the collision-avoidance task (_middle_) and the multiple-goal task (_right_). The shaded area indicates the standard deviation over \(5\) runs.

environment steps used for training, we chose the last time step. Specifically, we chose the following time steps for PPO, SAC and WGF, respectively: reaching task \((280000,400000,80000)\), collision avoidance task \((275000,300000,90000)\), multiple goal task \((130000,200000,95000)\). These plots show that PPO may also reach low-variance success rate over the five runs at the time step of convergence, at the cost of a prohibitively large number of steps. SAC showed huge variance in all tasks, apart from the reaching task, where all runs collapsed to a success rate of \(0\).

#### a.6.2 Ablation study

In order to assess the influence of leveraging a Riemannian optimization approach on the Bures-Wasserstein manifold, we conducted an ablation of our method by eliminating the Riemannian formulation. Instead of the explicit Euler scheme update in Eq. 12, which corresponds to Riemannian gradient descent w.r.t. the Bures-Wasserstein metric, we use the implicit Euler scheme

\[\boldsymbol{\hat{\mu}}_{k+1}=\operatorname*{arg\,min}_{\boldsymbol{\hat{\mu} }}\left(\frac{W_{2}^{2}(\pi_{k}(\boldsymbol{\hat{\mu}}),\pi_{k})}{2\tau}-J( \pi_{k}(\boldsymbol{\hat{\mu}}))\right),\] (42)

\[\boldsymbol{\hat{\Sigma}}_{k+1}=\operatorname*{arg\,min}_{\boldsymbol{\hat{ \Sigma}}}\left(\frac{W_{2}^{2}(\pi_{k}(\boldsymbol{\hat{\Sigma}}),\pi_{k})}{2 \tau}-J(\pi_{k}(\boldsymbol{\hat{\Sigma}}))\right).\] (43)

To guarantee that the updated covariance matrices do not leave the manifold of symmetric positive definite matrices, we parameterize them in terms of Cholesky factors. The results obtained with this non-Riemannian version of our method are shown in Fig. 7 in direct comparison to our method and Fig. 8 for an extended range.

Figure 8: Extended plot of the success rate of and ablated version of our method, not using the Bures-Wasserstein-based formulation for the reaching task (_left_), the collision-avoidance task (_middle_) and the multiple-goal task (_right_). The shaded area indicates the standard deviation over \(5\) runs.

Figure 6: Variance of the success rate over the 5 runs for our method (WGF) and the two baselines on the reaching task (_left_), the collision avoidance task (_middle_) and the multiple-goal task (_right_). The violin plots are overlaid with box plots, quartile lines and a swarm plot, where dots indicate the success rates of individual runs. The time steps at which we determined the variance are for PPO, SAC and WGF for the three tasks from left to right: \((280000,400000,80000)\), \((275000,30000,90000)\). \((130000,200000,95000)\).

Figure 7: The success rate of our method and an ablated version, not using the Bures-Wasserstein formulation for the reaching task (_left_), the collision-avoidance task (_middle_) and the multiple-goal task (_right_). The shaded area indicates the standard deviation over \(5\) runs.

The results clearly show that the non-Riemannian method struggles to reach a success rate of \(1\) for the reaching task and the collision-avoidance task. Furthermore, we observe a high variance over different runs in the same settings (see Fig. 9 and Fig. 10). We attribute this to the fact that the our method takes exact gradient steps in the direction of steepest descent w.r.t. the underlying BW metric, whereas the implicit scheme only approximates this direction. For this reason the non-Riemannian method is much more noisy, which in turn leads to the aforementioned high variance. Nevertheless, the multiple-goal task constitutes an exception. Here we observed a similar performance for our approach and the ablated method. The reason for this is that the optimization of this task is mainly dominated by the weight updates, which are identical for both methods. This result is therefore expected and confirms that correctness of our ablation strategy.

#### a.6.3 Additional experiments with \(7\)-DoF robotic manipulator

We carried out two additional experiments to show that our method can be employed on tasks performed by off-the-shelf robotic manipulators (e.g. a \(7\)-DoF Franka Emika Panda robot). Specifically, we extended the collision-avoidance task described in SS 4 to a \(3\)D environment, where the state and action depend on the task space representation. The first task was represented in the robot operational space, i.e., the state \(\bm{s}=\bm{x}\in\mathbb{R}^{3}\) and the action \(\bm{a}=\dot{\bm{x}}\in\mathbb{R}^{3}\). The second task was represented in the robot joint space, consequently the state \(\bm{s}=\bm{q}\in\mathbb{R}^{7}\) and the action \(\bm{a}=\dot{\bm{q}}\in\mathbb{R}^{7}\). This experiment was aimed at assessing the capabilities of our approach to adapt robot motion policies in state-action spaces of higher dimensions. The initial \(3\)-components GMM policy was trained using \(10\) human demonstrations featuring linear reaching \(3\)D trajectories. For policy optimization, we used a sparse reward defined as a function of the position error between the robot end-effector position and the target at the end of the rollout. Moreover, two sparse penalty terms were added to punish collision with obstacles and divergent trajectories.

Similarly to the planar task reported in the main paper, we tested whether our method was able to adapt a trajectory tracking skill in order to avoid collisions with newly added obstacles. This means that the robot end-effector needed to pass through a narrow path between two spherical obstacles. For the operational space representation, the robot end-effector pose was controlled using a full-pose Cartesian velocity controller at a frequency of \(100\mathrm{Hz}\), where the end-effector orientation was kept constant. For the joint space representation, the robot joint configuration was controlled using a joint velocity controller at a frequency of \(100\mathrm{Hz}\). Figure 11 shows the results for \(3\)D operational space representation, where our method reached a success rate of \(1.0\) very quickly, taking approximately

Figure 10: Variance of the success rate over \(5\) runs for our method (WGF) and the ablated method (non-BW) on the reaching task (_left_), the collision avoidance task (_middle_) and the multiple-goal task (_right_). The violin plots are overlaid with box plots, quartile lines and a swarm plot, where dots indicate the success rates of individual runs. The time steps at which we determined the variance are \((80000,40000),(90000,200000),(85000,90000)\).

Figure 9: Variance of the success rate over \(5\) runs for our method (WGF) and the ablated method (non-BW) on the reaching task (_left_), the collision avoidance task (_middle_) and the multiple-goal task (_right_). The violin plots are overlaid with box plots, quartile lines and a swarm plot, where dots indicate the success rates of individual runs. The time steps at which we determined the variance are \(80000,90000,85000\).

\(20000\) environment steps3. Moreover, the solution variance of our method was also very low, which is consistent with our observations concerning the performance of our policy optimization on the three planar tasks analyzed in the main paper. Figure 12 shows the results of the collision avoidance task using the joint space representation. As observed, our approach was able to adapt the robot motion policy so that the robot end-effector safely passes through a narrow path defined by two spherical obstacles (the narrow-path task description is provided in Sec. 4.1). It is evident that our approach outperformed all the baselines in this simulated robotic task, providing evidence that our approach scales and it is able to adapt robot motion policies in higher-dimensional tasks.

Footnote 3: As the baselines underperformed in the \(2\)D case, they were not tested in this specific setting.

#### a.6.4 Initial GMM policies

For the sake of completeness, Fig. 13 provides \(2\)D projections of the initial GMM policies learned from demonstrations for the three robotic settings considered in the main paper: the reaching motion skill, the collision-free trajectory tracking, and the multiple-goal task. Figure 13 also provides the demonstration data used to train the initial policies. Note that these models are then adapted according to the policy optimization approach introduced in SS 3.2.

Figure 11: The success rate of our method applied to the \(3\)D narrow-path task performed by the \(7\)-DoF Panda robotic manipulator. The shaded area indicates the standard deviation over \(5\) runs.

Figure 12: The success rate of our method (WFG) and the baselines on a \(3\)D narrow-path task performed by a simulated \(7\)-DoF Panda robotic manipulator. The task is represented in the robot joint space, therefore the state \(\bm{s}=\bm{q}\in\mathbb{R}^{7}\), and the action \(\bm{a}=\dot{\bm{q}}\in\mathbb{R}^{7}\), correspond to the joint position and velocity, respectively. The shaded area indicates the standard deviation over \(15\) runs.

Figure 13: Green Gaussian components () represent the initial GMM policy learned from demonstrations, projected on the Cartesian position (_left_) and velocity (_left_) spaces. The recorded position and velocity data are depicted as black dots ().