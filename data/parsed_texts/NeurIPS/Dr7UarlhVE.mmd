# Exactly Minimax-Optimal Locally Differentially Private Sampling

Hyun-Young Park

School of Electrical Engineering

KAIST

phy811@kaist.ac.kr &Shahab Asoodeh

Department of Computing and Software

McMaster University

asoodeh@mcmaster.ca &Si-Hyeon Lee

School of Electrical Engineering

KAIST

sihyeon@kaist.ac.kr

###### Abstract

The sampling problem under local differential privacy has recently been studied with potential applications to generative models, but a fundamental analysis of its privacy-utility trade-off (PUT) remains incomplete. In this work, we define the fundamental PUT of private sampling in the minimax sense, using the \(f\)-divergence between original and sampling distributions as the utility measure. We characterize the exact PUT for both finite and continuous data spaces under some mild conditions on the data distributions, and propose sampling mechanisms that are universally optimal for all \(f\)-divergences. Our numerical experiments demonstrate the superiority of our mechanisms over baselines, in terms of theoretical utilities for finite data space and of empirical utilities for continuous data space.

## 1 Introduction

Privacy leakage is a pressing concern in the realm of machine learning (ML), spurring extensive research into privacy protection techniques [1, 2, 3, 4]. Among these, local differential privacy (LDP) [5] stands out as a standard model and has been deployed in industry, e.g., by Google [6, 7], Apple [8], Microsoft [9]. In the LDP framework, individual clients randomize their data on their own devices and send it to a potentially untrusted aggregator for analysis, thus preventing the user data from being inferred. However, this perturbation inherently diminishes data utility. Consequently, the central challenge in privacy mechanism design lies in optimizing utility while preserving the desired level of privacy protection. This goal involves characterizing the optimal balance between privacy parameter and utility, referred to as the privacy-utility trade-off (PUT). The analysis of the PUT and the proposal of privacy mechanisms have been actively conducted for various settings of statistical inference and machine learning [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27].

Most research in this field focuses on scenarios where each client has only a single data point. However, there are increasingly more applications where each client has a large local dataset with multiple data records. One can formulate the privacy requirement in these cases by assuming that clients have datasets of the same size, generated independently from an underlying distribution [28, 29, 30, 31, 32, 33, 34]. This probabilistic assumption, however, restricts practical flexibility. The work [35] explored a scenario where clients have large datasets that may vary in size and seek to privately release another dataset that closely resembles their original dataset. In this scenario, local datasets can be represented by an empirical distribution, allowing each client to be seen as holding a probability distribution andgenerating a private sample from it. This setup, which is called _private sampling_, is the main focus of this paper.

Private sampling has recently found applications in the private fine-tuning of large language models [36]. Additionally, private sampling is connected to the challenge of learning private generative models, a topic often explored in the central DP model [37, 38, 39, 40, 41, 42, 43, 44]. While there exist studies on private generative models within the local model [45, 46, 47, 48], all these works assume a single data point per client. Very recently, the work [49] considered a setup where each client holds a probability distribution, but in a different context of query estimation.

The private sampling mechanism in [35] can be described as follows. Initially, given a probability distribution \(P\) representing a local dataset, the mechanism assumes a _fixed_ reference distribution. It then constructs what is termed the "relative mollifier", a closed ball centered around the reference distribution with a radius equivalent to half of the privacy budget, within the space of probability distributions. Subsequently, the mechanism computes the projection of \(P\) onto the relative mollifier, utilizing the Kullback-Leibler (KL) divergence. This projected distribution serves as the sampling distribution for generating a sample (see Section 2.3 for more details). However, this mechanism has a notable shortcoming: the sampling distribution is only locally optimal within the relative mollifier. This, in turn, implies that the optimality of the sampling distribution depends on the choice of the reference distribution. A more fundamentally intriguing goal would be to formulate and characterize the PUT without such an ambiguity in the choice of reference distribution.

In this paper, we establish the optimality of locally private sampling in the minimax sense, and identify optimal private samplers. Our primary contributions are summarized as follows:

* The fundamental PUT of private sampling is rigorously defined in terms of minimax utility, which is commonly used in the literature of private estimation [10, 11, 13, 30]. We impose some minimal assumptions on client's distributions as in [38] (which studies sampling under _central_ DP, a weaker privacy model than the local model [50]). For utility measure, we use the _\(f\)-divergence_[51, 52] between the original and the sampling distributions, that includes KL divergence, total variation distance, squared Hellinger distance, and \(\chi^{2}\)-divergence as special cases.
* We characterize the exact PUT for both finite and continuous data spaces, and present optimal sampling mechanisms achieving the PUT. Surprisingly, our mechanisms are _universally optimal_ under any choice of \(f\)-divergence for utility measure.
* We numerically demonstrate that our proposed mechanism outperforms the baseline method presented in [35]. Specifically, for finite data spaces, we derive a closed-form expression for the utility of both our mechanism and the baseline, allowing for an exact comparison of their utilities. In the case of continuous data spaces, a closed-form expression for the baseline is not available, so we use empirical utility for the comparison. Figure 1 illustrates our proposed mechanism outputs a distribution closer to the original, than the baseline.

All codes for experiments and figures are attached as a supplementary material, and can be found at the online repository1. The instructions to reproduce the results in the paper are in Appendix H.

Figure 1: Original Gaussian ring distribution and the sampling distributions of the baseline [35] and our proposed mechanism for privacy budget \(\epsilon=0.5\). The implementation details are in Appendix F.

## 2 Problem Formulation

### Notations and preliminaries

Notations.For a sample space \(\mathcal{X}\), let \(\mathcal{P}(\mathcal{X})\) be the set of all probability distributions on \(\mathcal{X}\). For each \(n\in\mathbb{N}\), let \(C(\mathbb{R}^{n})\) be the set of all continuous probability distributions on \(\mathbb{R}^{n}\). For each positive integer \(k\in\mathbb{N}\), let \([k]:=\{1,2,\cdots,k\}\). For a subset \(A\subset\mathcal{X}\), \(\mathbb{1}_{A}:\mathcal{X}\to\{0,1\}\) denotes the indicator function, defined as \(\mathbb{1}_{A}(x)=1\) for \(x\in A\) and \(\mathbb{1}_{A}(x)=0\) for \(x\notin A\). Also, for \(s_{1}\leq s_{2}\) and \(x\in\mathbb{R}\), let \(\mathrm{clip}(x;s_{1},s_{2}):=\max\{s_{1},\min\{s_{2},x\}\}\). We refer to Appendix A for the rigorous measure-theoretic assumptions underlying the paper.

\(f\)-divergence.For a convex function \(f:(0,\infty)\to\mathbb{R}\) satisfying \(f(1)=0\) and two probability distributions \(P,Q\in\mathcal{P}(\mathcal{X})\) on the same sample space \(\mathcal{X}\), let \(D_{f}\left(P\|Q\right)\) denote the \(f\)-divergence [51, 52]. The general definition of \(f\)-divergence is given in Appendix B. For \(\mathcal{X}=\mathbb{R}^{n}\) and \(P,Q\in\mathcal{C}(\mathbb{R}^{n})\) with \(P\ll Q\) (that is, \(P(A)=0\) whenever \(Q(A)=0\)), it is defined as

\[D_{f}\left(P\|Q\right)=\int_{x:q(x)>0}q(x)f\left(\frac{p(x)}{q(x)}\right)dx,\] (1)

where \(p,q\) are pdfs of \(P,Q\), respectively, and we define \(f(0)=\lim_{x\to 0^{+}}f(x)\in(-\infty,\infty]\). For finite \(\mathcal{X}\), we can replace the integral with the sum and replace \(p,q\) with \(P,Q\). Several well-known distance measures between distributions are examples of \(f\)-divergence with different convex functions. For instance, KL divergence (relative entropy), total variation distance, squared Hellinger distance, and \(\chi^{2}\)-divergence are \(f\)-divergences with \(f(x)=x\log x\), \(f(x)=|x-1|/2\), \(f(x)=(1-\sqrt{x})^{2}\), and \(f(x)=x^{2}-1\), respectively. Two important properties of general \(f\)-divergence are keys for this work. First, \(D_{f}\left(P\|Q\right)\geq 0\), and equality holds if \(P=Q\). Furthermore, we have

\[D_{f}\left(P\|Q\right)\leq M_{f}:=\lim_{x\to 0+}f(x)+xf(1/x),\] (2)

where equality holds if \(P\) and \(Q\) are mutually singular (that is, they have disjoint supports). For a more comprehensive list of such \(f\)-divergences and their properties, we refer the readers to [52].

We denote the KL divergence and the total variation distance as \(D_{\mathrm{KL}}\left(P\|Q\right)\) and \(D_{\mathrm{TV}}\left(P,Q\right)\), respectively. We note that the total variation distance is in fact a metric on \(\mathcal{P}(\mathcal{X})\).

### System model

Suppose a client has access to a distribution \(P\in\mathcal{P}(\mathcal{X})\) over a sample space \(\mathcal{X}\), and wants to produce a sample in \(\mathcal{X}\) which looks like being drawn from \(P\) and to send it to a data curator. We assume that there are some constraints on the possible data distribution \(P\), so that \(P\) is restricted to be in some subset \(\tilde{\mathcal{P}}\subset\mathcal{P}(\mathcal{X})\), and both the client and the curator know \(\mathcal{X}\) and \(\tilde{\mathcal{P}}\). However, it is required that a sampled element does not leak the privacy about the original distribution \(P\). For this purpose, the client and the curator agree a **private sampling mechanism \(\mathbf{Q}\)**, which is a conditional distribution from \(\tilde{\mathcal{P}}\) to \(\mathcal{X}\). After that, the client produces a sample following the distribution \(\mathbf{Q}(\cdot|P)\). To guarantee the privacy protection, we impose \(\mathbf{Q}\) to satisfy the local differential privacy (LDP) [10, 35].

**Definition 2.1**.: Let \(\epsilon>0\). A private sampling mechanism \(\mathbf{Q}\) is said to satisfy \(\epsilon\)**-LDP**, or \(\mathbf{Q}\) is an \(\epsilon\)-LDP mechanism, if for any \(P,P^{\prime}\in\tilde{\mathcal{P}}\) and \(A\subset\mathcal{X}\), we have

\[\mathbf{Q}(A|P)\leq e^{\epsilon}\mathbf{Q}(A|P^{\prime}).\] (3)

For convenience, for each \(P\in\tilde{\mathcal{P}}\), let \(\mathbf{Q}(P)\in\mathcal{P}(\mathcal{X})\) denote the distribution of \(X\) given \(P\) through \(\mathbf{Q}\), that is \(\mathbf{Q}(P)(A)=\mathbf{Q}(A|P)\) for each \(A\subset\mathcal{X}\). In this way, we equivalently see \(\mathbf{Q}\) as a function \(\mathbf{Q}:\tilde{\mathcal{P}}\to\mathcal{P}(\mathcal{X})\). Let \(\mathcal{Q}_{\mathcal{X},\tilde{\mathcal{P}},\epsilon}\) denote the set of all \(\epsilon\)-LDP mechanisms \(\mathbf{Q}:\tilde{\mathcal{P}}\to\mathcal{P}(\mathcal{X})\).

As the utility loss of the private sampling, we use the \(f\)-divergence between the original distribution and the sampling distribution, \(D_{f}\left(P\|\mathbf{Q}(P)\right)\). Since the sampling procedure can be performed across many clients who may have different data distributions, we measure the utility loss of \(\mathbf{Q}\) by the **worst-case \(f\)-divergence**,

\[R_{f}(\mathbf{Q})=\sup_{P\in\tilde{\mathcal{P}}}D_{f}\left(P\|\mathbf{Q}(P) \right).\] (4)Given \(\mathcal{X}\), \(\bar{\mathcal{P}}\), \(\epsilon\), and \(f\), our goal is to find the smallest possible worst-case \(f\)-divergence,

\[\mathcal{R}(\mathcal{X},\bar{\mathcal{P}},\epsilon,f)=\inf_{\mathbf{Q}\in \mathcal{Q}_{\mathcal{X},\bar{\mathcal{P}},\epsilon}}R_{f}(\mathbf{Q}),\] (5)

and to find a mechanism \(\mathbf{Q}\in\mathcal{Q}_{\mathcal{X},\bar{\mathcal{P}},\epsilon}\) achieving it. We say that \(\mathbf{Q}\in\mathcal{Q}_{\mathcal{X},\bar{\mathcal{P}},\epsilon}\) is **optimal** for \((\mathcal{X},\bar{\mathcal{P}},\epsilon)\) under \(D_{f}\) if \(R_{f}(\mathbf{Q})=\mathcal{R}(\mathcal{X},\bar{\mathcal{P}},\epsilon,f)\).

### Related work

The most closely related work to our work is [35]. The system models are the same as this paper, except the formulation of PUT. They first _fix_ a reference probability distribution \(Q_{0}\in\mathcal{P}(\mathcal{X})\), and only consider mechanisms \(\mathbf{Q}\) satisfying \(e^{-\epsilon/2}Q_{0}(A)\leq\mathbf{Q}(A|P)\leq e^{\epsilon/2}Q_{0}(A)\) for all \(P\in\bar{\mathcal{P}}\) and \(A\subset\mathcal{X}\). In other words, let \(\mathcal{M}_{\epsilon,Q_{0}}=\{Q\in\mathcal{P}(\mathcal{X}):e^{-\epsilon/2}Q _{0}(A)\leq Q(A)\leq e^{\epsilon/2}Q_{0}(A),\quad\forall A\subset\mathcal{X}\}\). Then, they only consider \(\mathbf{Q}\) such that \(\mathbf{Q}(P)\in\mathcal{M}_{\epsilon,Q_{0}}\). Note that this guarantees \(\epsilon\)-LDP. For each \(P\in\bar{\mathcal{P}}\), they sought to find \(Q\in\mathcal{M}_{\epsilon,Q_{0}}\) which minimizes \(D_{\mathrm{KL}}\left(P\|Q\right)\), and set this \(Q\) to be \(\mathbf{Q}(P)\). First, they claimed to find a closed-form expression of such a minimizer \(Q\) for finite \(\mathcal{X}\), given by

\[Q(x)=\mathrm{clip}\left(P(x)/r_{P};e^{-\epsilon/2}Q_{0}(x),e^{\epsilon/2}Q_{0} (x)\right),\] (6)

where \(r_{P}>0\) is a constant depending on \(P\) ensuring \(\sum_{x\in\mathcal{X}}Q(x)=1\). Second, they presented an algorithm, called Mollified Boosted Density Estimation (MBDE), to approximate the optimal solution for continuous \(\mathcal{X}\). However, the utility varies over the choice of reference distribution \(Q_{0}\), and they left the question of choosing a best \(Q_{0}\) to achieve the best performance in both practice and theory. Moreover, we found that the closed-form in (6) is incomplete, because for some \((P,Q_{0})\), there may be no \(r_{P}>0\) such that the RHS of (6) does not sum to one. As an example, when \(P,Q_{0}\) are point masses at different points, then we can easily see that the sum of (6) is \(e^{-\epsilon/2}\) for any \(r_{P}>0\).

## 3 Main Results

### Optimal private sampling over finite space

First, we consider the finite case, where \(\mathcal{X}=[k]\) for some \(k\in\mathbb{N}\). A natural setup for \(\bar{\mathcal{P}}\) is that \(\bar{\mathcal{P}}=\mathcal{P}([k])\), i.e. there is no restriction on the client distribution \(P\in\mathcal{P}([k])\). In this case, we completely characterize the optimal worst-case \(f\)-divergence \(\mathcal{R}(\mathcal{X},\bar{\mathcal{P}},\epsilon,f)\) and find an optimal private sampling mechanism. Surprisingly, for each \(k\in\mathbb{N}\) and \(\epsilon>0\), we found a single mechanism which is universally optimal for every \(f\)-divergence.

**Theorem 3.1**.: _For each \(k\in\mathbb{N}\), \(\epsilon>0\), and an \(f\)-divergence \(D_{f}\), we have_

\[\mathcal{R}([k],\mathcal{P}([k]),\epsilon,f)=\frac{e^{\epsilon}}{e^{\epsilon}+ k-1}f\left(\frac{e^{\epsilon}+k-1}{e^{\epsilon}}\right)+\frac{k-1}{e^{\epsilon}+k-1} f(0).\] (7)

_Moreover, the mechanism \(\mathbf{Q}_{k,\epsilon}^{*}\) constructed as below satisfies \(\epsilon\)-LDP and is optimal for \((\mathcal{X}=[k],\bar{\mathcal{P}}=\mathcal{P}([k]),\epsilon)\) under any \(D_{f}\):_

\[\mathbf{Q}_{k,\epsilon}^{*}(x|P)=\max\left(\frac{1}{r_{P}}P(x),\frac{1}{e^{ \epsilon}+k-1}\right)\quad\forall x\in[k],P\in\mathcal{P}([k]),\] (8)

_where \(r_{P}>0\) is a constant depending on \(P\) so that \(\sum_{x=1}^{k}\mathbf{Q}_{k,\epsilon}^{*}(x|P)=1\). Furthermore, \(r_{P}\) can be chosen such that \(1\leq r_{P}\leq(e^{\epsilon}+k-1)/e^{\epsilon}\)._

By definition, we have \(\mathbf{Q}_{k,\epsilon}^{*}(x|P)\geq\frac{1}{e^{\epsilon}+k-1}\) for all \(x\in\mathcal{X}\). This also implies that \(\mathbf{Q}_{k,\epsilon}^{*}(x|P)=1-\sum_{x^{\prime}\in\mathcal{X}\setminus\{x \}}\mathbf{Q}_{k,\epsilon}^{*}(x^{\prime}|P)\leq 1-\frac{k-1}{e^{\epsilon}+k-1}=\frac{e^{ \epsilon}}{e^{\epsilon}+k-1}\). Hence, \(\frac{1}{e^{\epsilon}+k-1}\leq\mathbf{Q}_{k,\epsilon}^{*}(x|P)\leq\frac{e^{ \epsilon}}{e^{\epsilon}+k-1}\). This clearly implies that \(\mathbf{Q}_{k,\epsilon}^{*}\) satisfies \(\epsilon\)-LDP.

Behaviors of the optimal mechanism.Let us observe some behaviors of the proposed mechanism with respect to the system parameters, whose formal proofs are in Appendix E. We visualize how the mechanism \(\mathbf{Q}_{k,\epsilon}^{*}\) works for different \(\epsilon\) in Figure 2. Here, we write \(\mathcal{R}\) to mean \(\mathcal{R}([k],\mathcal{P}([k]),\epsilon,f)\)for simplicity. If \(f(0)=\infty\), then \(\mathcal{R}=\infty\), which means that \(R_{f}(\mathbf{Q})=\infty\) for any \(\epsilon\)-LDP sampling mechanism \(\mathbf{Q}\). (Such a phenomenon happens for general \((\mathcal{X},\tilde{\mathcal{P}})\), whenever \(\tilde{\mathcal{P}}\) contains two mutually singular distributions) Hence, from now on in this paragraph, we assume \(f(0)<\infty\). We can observe that \(\mathcal{R}\) is decreasing in \(\epsilon\) and increasing in \(k\). For a fixed \(k\), we have \(\mathcal{R}\to 0\) as \(\epsilon\to\infty\), which makes sense since \(\epsilon\to\infty\) corresponds to the non-private case. Also, as \(\epsilon\to 0\), we have \(\mathbf{Q}^{*}_{k,\epsilon}(x|P)\to 1/k\) for every \(P\in\mathcal{P}([k])\) and \(x\in[k]\), that is, \(\mathbf{Q}^{*}_{k,\epsilon}(P)\) tends to the uniform distribution over \([k]\) for every \(P\in\mathcal{P}([k])\). This fact can be also observed by Figure 2.

Remarks about the constant \(r_{P}\).The value of \(r_{P}\) may not be unique, but the mechanism \(\mathbf{Q}^{*}_{k,\epsilon}\) does not depend on the choice of \(r_{P}\). To see this, let us fix \(P\), and let \(g_{r}(x)=\max\left(\frac{1}{r}P(x),\frac{1}{e^{\epsilon}+k-1}\right)\). Suppose that \(\sum_{x=1}^{k}g_{r}(x)=\sum_{x=1}^{k}g_{r^{\prime}}(x)=1\) for \(r\leq r^{\prime}\). Since \(g_{r}(x)\geq g_{r^{\prime}}(x)\) for each \(x\in[k]\), the equality \(\sum_{x=1}^{k}g_{r}(x)=\sum_{x=1}^{k}g_{r^{\prime}}(x)\) implies that \(g_{r}(x)=g_{r^{\prime}}(x)\) for all \(x\in[k]\). Hence \(\mathbf{Q}^{*}_{k,\epsilon}\) is uniquely determined. Since \(r\mapsto\sum_{x=1}^{k}g_{r}(x)\) is non-increasing and continuous, we can use the bisection method to find \(r_{P}\). The 'Furthermore' part of the theorem statement precisely means that for \(r=1\) and \(r=(e^{\epsilon}+k-1)/e^{\epsilon}\), the value of \(\sum_{x=1}^{k}g_{r}(x)\) is at least and at most \(1\), respectively, so that we can perform the bisection method with these two initial endpoints to find \(r\) such that \(\sum_{x=1}^{k}g_{r}(x)=1\).

Comparison with the previous work [35].The expression of the optimal mechanism in (8) is similar to (6), the expression of the KL divergence projection onto \(\mathcal{M}_{\epsilon,Q_{0}}\) derived by Husain et al. [35]. Since \(\frac{1}{e^{\epsilon}+k-1}\leq\mathbf{Q}^{*}_{k,\epsilon}(x|P)\leq\frac{e^{ \epsilon}}{e^{\epsilon}+k-1}\), we can alternatively write \(\mathbf{Q}^{*}_{k,\epsilon}(x|P)=\operatorname{clip}\left(\frac{1}{r_{P}}P(x) ;\frac{1}{e^{\epsilon}+k-1},\frac{e^{\epsilon}}{e^{\epsilon}+k-1}\right)\). Hence, our optimal mechanism can be viewed as an instance of a generalized version of (6), where \(Q_{0}\) is a positive measure, not necessarily a probability measure summing to one, given by \(Q_{0}(x)=\frac{e^{\epsilon/2}}{e^{\epsilon}+k-1}\). A natural question is whether \(\mathbf{Q}^{*}_{k,\epsilon}(P)\) is a projection of \(P\) onto \(\mathcal{M}_{\epsilon,Q_{0}}\), that is \(\mathbf{Q}^{*}_{k,\epsilon}(P)\) is a minimizer of \(D_{\mathrm{KL}}\left(P||Q\right)\) among \(Q\in\mathcal{M}_{\epsilon,Q_{0}}\), where \(\mathcal{M}_{\epsilon,Q_{0}}\) is similarly defined as in Section 2.3. As we shall discuss in Section 3.3, this statement is true --quite surprisingly-- even when we replace \(D_{\mathrm{KL}}\) with any other \(f\)-divergences. However, our analysis is more involved, as we need to show the optimality of the proposed mechanism over _any other possible mechanisms_, including minimizers with respect to other choices of \(Q_{0}\). Also, in Section 5, we compare the worst-case \(f\)-divergence of our optimal mechanism with that of the mechanism proposed in [35] which restricts \(Q_{0}\) to be a probability distribution.

### Optimal private sampling over continuous space

Next, we consider the continuous case, where \(\mathcal{X}=\mathbb{R}^{n}\) for some \(n\in\mathbb{N}\). Some of the natural setups for \(\tilde{\mathcal{P}}\) are (i) \(\tilde{\mathcal{P}}=\mathcal{P}(\mathbb{R}^{n})\), or (ii) \(\tilde{\mathcal{P}}=\mathcal{C}(\mathbb{R}^{n})\). We can also think some restrictive but still reasonable setups, such as the setups where (iii) \(\tilde{\mathcal{P}}\) is the set of empirical distributions supported on some non-empty open subset of \(\mathbb{R}^{n}\), or where (iv) \(\tilde{\mathcal{P}}\) is the set of continuous distributions on \([-1,1]^{n}\) having smooth pdf and zero mean. However, we show that for a general class of \(\tilde{\mathcal{P}}\) including these four cases, any \(\epsilon\)-LDP sampling mechanisms have the worst-case \(f\)-divergence equal to the maximum value \(M_{f}\) of the \(f\)-divergence defined in (2). In the following proposition, \(\mathcal{X}\) may be a general sample space, not necessarily \(\mathbb{R}^{n}\) or finite space. The proof is in Appendix D.

**Proposition 3.2**.: _Suppose that \(\tilde{\mathcal{P}}\) contains infinitely many distributions which are pairwise mutually singular. Then, for any \(\epsilon>0\), for any \(\epsilon\)-LDP mechanism \(\mathbf{Q}\in\mathcal{Q}_{\mathcal{X},\tilde{\mathcal{P}},\epsilon}\), and for any \(f\)-divergence, we have \(R_{f}(\mathbf{Q})=M_{f}\), where \(M_{f}\) is define at (2)._

Figure 2: A visualization of the mechanism \(\mathbf{Q}^{*}_{k,\epsilon}\)

Hence, we need to consider sufficiently regular but practical setups for \(\tilde{\mathcal{P}}\).

Our setup.In this subsection, when \(P,Q\in\mathcal{C}(\mathbb{R}^{n})\), the corresponding small letters \(p,q\) denote their pdfs. In this paper, we consider the case that

\[\tilde{\mathcal{P}}=\tilde{\mathcal{P}}_{c_{1},c_{2},h}:=\{P\in\mathcal{C}( \mathbb{R}^{n}):c_{1}h(x)\leq p(x)\leq c_{2}h(x),\quad\forall x\in\mathbb{R}^{n}\}\] (9)

for some pre-known \(h:\mathcal{X}\to[0,\infty)\) such that \(\int_{\mathbb{R}^{n}}h(x)dx<\infty\) and \(c_{2}>c_{1}\geq 0\). Some of the sampling tasks and generative models in literature [53, 35] assume that the set of possible data distributions \(\tilde{\mathcal{P}}\) satisfies \(\tilde{\mathcal{P}}\subset\tilde{\mathcal{P}}_{c_{1},c_{2},h}\) for some \(c_{1},c_{2},h\) satisfying the aforementioned condition, hence (9) is a moderate assumption. One example is the sampling from a Gaussian mixture, which is one of the canonical sampling tasks in literature [53, 35]. Suppose that \(\tilde{\mathcal{P}}\) consists of Gaussian mixtures, where each Gaussian has mean within a unit ball centered at the origin and has unit covariance. That is, \(\tilde{\mathcal{P}}=\{\sum_{i=1}^{k}\lambda_{i}\mathcal{N}(\mu_{i},I_{n}):k\in \mathbb{N},\lambda_{i}\geq 0,\sum_{i=1}^{k}\lambda_{i}=1,\|\mu_{i}\| \leq 1\}\), where \(I_{n}\) is the identity matrix of size \(n\times n\). In this case, we can observe that \(\tilde{\mathcal{P}}\subset\tilde{\mathcal{P}}_{0,1,h}\) for \(h(x)=(2\pi)^{-n/2}\exp(-(\max(0,\|x\|-1))^{2}/2)\), and it can be easily shown that \(\int_{\mathbb{R}^{n}}h(x)<\infty\).

Without loss of generality, we may assume the following normalization condition on \(c_{1},c_{2},h,\epsilon\):

\[\int_{\mathbb{R}^{n}}h(x)dx=1,\quad c_{1}<1<c_{2},\quad c_{2}>c_{1}\epsilon^{ \epsilon}.\] (10)

The reason is as follows. First, if any one of three inequalities \(\int_{\mathbb{R}^{n}}h(x)dx>0\), \(c_{1}\int_{\mathbb{R}^{n}}h(x)dx<1\), and \(c_{2}\int_{\mathbb{R}^{n}}h(x)dx>1\) is not satisfied, then \(\tilde{\mathcal{P}}_{c_{1},c_{2},h}\) is either an empty set or a singleton that consists of a distribution having pdf \(c_{1}h(x)\) or \(c_{2}h(x)\), which makes the problem trivial. Hence we impose all of the three inequalities. Then, we can normalize \(c_{1},c_{2},h\), to make \(\int_{\mathbb{R}^{n}}h(x)dx=1\) and \(c_{1}<1<c_{2}\). Furthermore, if \(c_{2}\leq e^{\epsilon}c_{1}\), then for any \(P_{1},P_{2}\in\tilde{\mathcal{P}}_{c_{1},c_{2},h}\), we have \(p_{1}(x)/p_{2}(x)\leq c_{2}/c_{1}\leq e^{\epsilon}\), hence we can easily observe that the mechanism \(\mathbf{Q}\) defined as \(\mathbf{Q}(P)=P\) for all \(P\in\tilde{\mathcal{P}}_{c_{1},c_{2},h}\) satisfies \(\epsilon\)-LDP and \(R_{f}(\mathbf{Q})=0\), hence the problem also becomes trivial, giving \(\mathcal{R}(\mathbb{R}^{n},\tilde{\mathcal{P}}_{c_{1},c_{2},h},\epsilon,f)=0\). Hence, we may assume (10).

Minimax utility and optimal mechanism.For the aforementioned setup, we can completely characterize \(\mathcal{R}(\mathcal{X},\tilde{\mathcal{P}},\epsilon,f)\) and find a mechanism which is universally optimal for every \(f\)-divergence. The formula is similar to the discrete case, with a carefully chosen clipping bound.

**Theorem 3.3**.: _For each \(c_{2}>c_{1}\geq 0,\epsilon>0\), and \(h:\mathbb{R}^{n}\to[0,\infty)\) satisfying the normalization condition (10), let us define the following constants determined by \(c_{1},c_{2},\epsilon\):_

\[b=\frac{c_{2}-c_{1}}{(e^{\epsilon}-1)(1-c_{1})+c_{2}-c_{1}},\quad r_{1}=\frac {c_{1}}{b},\quad r_{2}=\frac{c_{2}}{be^{\epsilon}}.\] (11)

_Then, we have_

\[\mathcal{R}(\mathbb{R}^{n},\tilde{\mathcal{P}}_{c_{1},c_{2},h},\epsilon,f)=\frac {1-r_{1}}{r_{2}-r_{1}}f(r_{2})+\frac{r_{2}-1}{r_{2}-r_{1}}f(r_{1}).\] (12)

_Moreover, the mechanism \(\mathbf{Q}^{*}_{c_{1},c_{2},h,\epsilon}\) constructed as below satisfies \(\epsilon\)-LDP and is optimal for \((\mathcal{X}=\mathbb{R}^{n},\tilde{\mathcal{P}}=\tilde{\mathcal{P}}_{c_{1},c_{2 },h},\epsilon)\) under any \(D_{f}\):_

_For each \(P\in\tilde{\mathcal{P}}\), \(\mathbf{Q}^{*}_{c_{1},c_{2},h,\epsilon}(P)=:Q\) is defined as a continuous distribution with pdf_

\[q(x)=\operatorname{clip}\left(\frac{1}{r_{P}}p(x);bh(x),be^{\epsilon}h(x) \right),\] (13)

_where \(r_{P}>0\) is a constant depending on \(P\) so that \(\int_{\mathbb{R}^{n}}q(x)dx=1\). Furthermore, \(r_{P}\) can be chosen such that \(r_{1}<r_{P}\leq r_{2}\)._

It is also clear that \(\mathbf{Q}^{*}_{c_{1},c_{2},h,\epsilon}\) satisfies \(\epsilon\)-LDP. Also, we note that \(c_{1}<b<1<be^{\epsilon}<c_{2}\) and \(0\leq r_{1}<1<r_{2}\), which is shown during the proof of Theorem 3.3 in Appendix C.

In practical scenario, it may be hard to expect \(\tilde{\mathcal{P}}=\tilde{\mathcal{P}}_{c_{1},c_{2},h}\) exactly, and we may only know \(\tilde{\mathcal{P}}\subset\tilde{\mathcal{P}}_{c_{1},c_{2},h}\) for some \(c_{1},c_{2},h\) satisfying aforementioned conditions. In such case, we still propose to use \(\mathbf{Q}^{*}_{c_{1},c_{2},h,\epsilon}\), and in Section 5, we numerically show that this proposed mechanism is better than previously proposed mechanism [35] in terms of the worst-case \(f\)-divergence.

Behavior of the optimal mechanism.We also observe some behaviors of the proposed mechanism with respect to the system parameters, whose formal proofs are in Appendix E. Again, we write \(\mathcal{R}\) to mean \(\mathcal{R}(\mathbb{R}^{n},\tilde{\mathcal{P}}_{c_{1},c_{2},h},\epsilon,f)\) for simplicity. For a fixed \((c_{1},c_{2})\), \(\mathcal{R}\) is decreasing in \(\epsilon\). If \(c_{1}=0\) (which implies \(r_{1}=0\)) and \(f(0)=\infty\), then \(\mathcal{R}=\infty\), which means \(R_{f}(\mathbf{Q})=\infty\) for any \(\epsilon\)-LDP sampling mechanism \(\mathbf{Q}\). For the behavior at \(\epsilon\to\infty\) for a fixed \((c_{1},c_{2})\), if \(c_{1}>0\), then for sufficiently large \(\epsilon\), we have \(c_{1}e^{\epsilon}\geq c_{2}\), so we fall in the aforementioned trivial case that \(\mathcal{R}=0\). If \(c_{1}=0\) and \(f(0)<\infty\), then as \(\epsilon\to\infty\), we have \(\mathcal{R}\to 0\), which again corresponds to the non-private case.

Remarks on the constant \(r_{P}\).By the same reason as in the finite space case, the value of \(r_{P}\) may not be unique, but \(\mathbf{Q}_{c_{1},c_{2},h,\epsilon}^{*}\) does not depend on the choice of \(r_{P}\), and \(r_{P}\) can be found by the bisection method with a numerical integration of (13). Note that the continuity of \(r\mapsto\int g_{r}(x)dx,g_{r}(x)=\mathrm{clip}\left(\frac{1}{r}p(x);bh(x),be^{ \epsilon}h(x)\right)\), follows from the dominated convergence theorem [54] since we assume \(\int_{\mathbb{R}^{n}}\), \(h(x)dx<\infty\). The meaning of 'Furthermore' part is also similar to the finite space case, that is, the value of \(\int g_{r}(x)dx\) at \(r=r_{1}\) and \(r=r_{2}\) is at least and at most \(1\), respectively, so that we can perform the bisection method with initial endpoints \((r_{1},r_{2})\) (When \(r=0\), we define \(g_{r}(x)=be^{\epsilon}h(x)\) whenever \(p(x)>0\) and \(g_{r}(x)=bh(x)\) whenever \(p(x)=0\)). A corner case is that when \(r_{1}=0\), the continuity of \(r\mapsto\int g_{r}(x)dx\) does not suffice to guarantee the existence of strictly positive \(r\) such that \(\int g_{r}(x)dx=1\). However, in the proof, we actually show that \(\int g_{r_{1}}(x)dx=1\) implies \(\int g_{r}(x)dx=1\) for _every_\(r\in(r_{1},r_{2}]\), which especially implies that even when \(r_{1}=0\), there is a _strictly positive_\(r\) such that \(\int g_{r}(x)dx=1\). This is the reason that we state the strict inequality \(r_{1}<r_{P}\).

### Proof sketch of the theorems

The full proofs of the main theorems, Theorems 3.1 and 3.3, are presented in Appendix C. In Appendix C, we present a generalized theorem which includes Theorems 3.1 and 3.3 as special cases, where \(\mathcal{X}\) can be a general sample space and \(\tilde{\mathcal{P}}\) is similarly defined as in the continuous space case. The key idea for proofs and proposed mechanisms is to focus on the behavior of \(\mathbf{Q}(P)\) when \(P\) is in an extreme case in \(\tilde{\mathcal{P}}\). In finite space, point masses are extreme cases, and for continuous space with \(\tilde{\mathcal{P}}=\tilde{\mathcal{P}}_{c_{1},c_{2},h}\), the cases that \(p(x)\in\{c_{1}h(x),c_{2}h(x)\}\) for all \(x\in\mathcal{X}\) are the extreme cases. As implied by the proof, the worst-case \(f\)-divergence of the proposed optimal mechanism is attained when \(P\) is in the aforementioned extreme cases. Such an approach using extreme case is a frequently used technique in PUT analysis [55; 56; 57; 58].

Our proof consists of two parts, the achievability part and the converse part. The achievability part is to show that the worst-case \(f\)-divergence \(R_{f}(\mathbf{Q}^{*})\) of our proposed mechanism \(\mathbf{Q}^{*}\) is upper-bounded by the RHS of (7) or (12). The converse part is to show that \(R_{f}(\mathbf{Q})\) of _any_\(\epsilon\)-LDP mechanism \(\mathbf{Q}\) is lower-bounded by the RHS of (7) or (12). From now, we briefly describe the proof idea of each part. Here, we omit the subscripts \((k,\epsilon)\) or \((c_{1},c_{2},h,\epsilon)\) for notational convenience.

Achievability part.Let \(\mathcal{M}=\{\mathbf{Q}^{*}(P):P\in\tilde{\mathcal{P}}\}\). For finite space, \(\mathcal{M}\) consists of all distributions \(Q\in\mathcal{P}([k])\) such that \(\frac{1}{e^{\epsilon}+k-1}\leq Q(x)\leq\frac{e^{\epsilon}}{e^{\epsilon}+k-1}\) for every \(x\in[k]\). For continuous space, \(\mathcal{M}\) consists of all continuous distributions \(Q\in\mathcal{C}(\mathbb{R}^{n})\) whose pdf \(q\) satisfies \(bh(x)\leq q(x)\leq be^{\epsilon}h(x)\) for every \(x\in\mathbb{R}^{n}\).

We construct a mechanism \(\mathbf{Q}^{\dagger}\) such that \(R_{f}(\mathbf{Q}^{\dagger})\) is upper-bounded by the RHS of (7) or (12). The construction is as follows. First, we set a reference distribution \(\mu\in\mathcal{P}(\mathcal{X})\) and a constant \(\gamma\in[0,1]\) according to a certain rule specified in Appendix C. Then, for each given \(P\), we generate a private sample by sampling from the original \(P\) with probability \(\gamma\), and sampling from the reference distribution \(\mu\) with probability \(1-\gamma\). In other words, we have \(\mathbf{Q}^{\dagger}(P)=\gamma P+(1-\gamma)\mu\). Our choice of \(\mu\) and \(\gamma\) makes \(\mathbf{Q}^{\dagger}(P)\in\mathcal{M}\) for every \(P\in\tilde{\mathcal{P}}\), which especially implies that \(\mathbf{Q}^{\dagger}\) also satisfies \(\epsilon\)-LDP. Furthermore, we can find a bound on the ratio of the pmf or pdf for original distribution to that for sampling distribution. Then, invoking [59, Theorem 2.1], which bounds \(f\)-divergences given bounds on the ratio between pmf or pdfs, we show that \(R_{f}(\mathbf{Q}^{\dagger})\) is upper-bounded by the RHS of (7) or (12).

Next, we demonstrate a non-trivial generalization of the main result of [35] that \(\mathbf{Q}^{*}(P)\) is the \(f\)-divergence projection of \(P\) onto \(\mathcal{M}\) for \(P\in\tilde{\mathcal{P}}\) and _for every \(f\)-divergence_.

**Proposition 3.4**.: _Assuming the setups of \((\mathcal{X},\tilde{\mathcal{P}},\epsilon)\) in either Theorem 3.1 or 3.3, let \(\mathbf{Q}^{*}\) denote the proposed mechanism \(\mathbf{Q}^{*}_{k,\epsilon}\) or \(\mathbf{Q}^{*}_{c_{1},c_{2},h,\epsilon}\). Also, let \(\mathcal{M}\) be as described above. Then, for every \(P\in\tilde{\mathcal{P}}\) and every \(f\)-divergence \(D_{f}\), we have \(D_{f}\left(P\|\mathbf{Q}^{*}(P)\right)=\inf_{Q\in\mathcal{M}}D_{f}\left(P\|Q\right)\)._

Notice that this proposition differs from [35] in that it holds for all general \(f\)-divergences (as opposed to only KL divergence) and general sample spaces, be it discrete or continuous. This result immediately yields \(D_{f}\left(P\|\mathbf{Q}^{*}(P)\right)\leq D_{f}\left(P\middle\|\mathbf{Q}^{ \dagger}(P)\right)\), and hence \(R_{f}(\mathbf{Q}^{*})\leq R_{f}(\mathbf{Q}^{\dagger})\leq(\text{RHS of (7) or (12)})\). We remark that combining with the converse part (to be described below), it implies that the \(\mathbf{Q}^{\dagger}\) is also optimal in our minimax sense. Nevertheless, \(\mathbf{Q}^{*}\) outperforms \(\mathbf{Q}^{\dagger}\) in that \(D_{f}\left(P\middle\|\mathbf{Q}^{*}(P)\right)\leq D_{f}\left(P\middle\|\mathbf{ Q}^{\dagger}(P)\right)\) for every \(P\in\tilde{\mathcal{P}}\).

_Remark 3.5_.: For the finite case, \(\mu\) is the uniform distribution over \([k]\) and \(\gamma\) is taken in such a way that \(\mathbf{Q}^{\dagger}\) satisfies \(\epsilon\)-LDP tightly. In this case, an alternative way to implement \(\mathbf{Q}^{\dagger}\) is as follows. For each given \(P\), first we sample from \(P\) to get a raw sample and then apply the \(k\)-ary randomized response [60] to it.

Converse part.For each extreme \(P\), let \(A_{P}\) be the "high probability set", defined as \(A_{P}=\{x\in\mathcal{X}:P(x)=1\}\) for finite space and \(A_{P}=\{x\in\mathcal{X}:p(x)=c_{2}h(x)\}\) for continuous space. Then, using the data processing inequality of \(f\)-divergence [61], we take a lower bound of \(D_{f}\left(P\middle\|\mathbf{Q}(P)\right)\) by the \(f\)-divergence between the distributions of \(\mathds{1}_{A_{P}}(X)\) for \(X\sim P\) and that for \(X\sim\mathbf{Q}(P)\). Such a lower bound becomes a decreasing function of \(\mathbf{Q}(A_{P}|P)\) in a certain range. Then, we seek to find an upper bound on \(\inf\mathbf{Q}(A_{P}|P)\) over extreme \(P\), which gives a lower bound on \(R_{f}(\mathbf{Q})\). This involves a novel combinatorial argument. We perform a "packing" of \(u\) copies of \(\mathcal{X}\) by \(t\) subsets \(A_{1},\cdots,A_{t}\) with \(A_{i}=A_{P_{i}}\) for some extreme \(P_{i}\) and appropriately chosen \(t\) and \(u\). Then, we decompose the RHS of \(u=\sum_{i=1}^{u}\mathbf{Q}(\mathcal{X}|P_{i})\) by an appropriate partition of \(\mathcal{X}\) involving \(A_{i}\)'s and use the definition of \(\epsilon\)-LDP to find an upper bound on \(\inf\mathbf{Q}(A_{P}|P)\).

## 4 Discussions on the Proposed Mechanism

### Effect of the \(r_{p}\) approximation error

In practice, it may not be possible to find the exact value of \(r_{P}\) such that the sum or integration of the RHS of (8) or (13) is \(1\). For the case of continuous space as in Theorem 3.3, one way to implement the proposed mechanism \(\mathbf{Q}^{*}_{c_{1},c_{2},h,\epsilon}\) in practice is as follows. First, we fix parameters \(\delta_{1}\in[0,1)\) and \(\delta_{2}\geq 0\) that quantify error tolerance. For a given \(P\), we define \(g_{r}(x)=\mathrm{clip}\left(\frac{1}{\tau}p(x);bh(x),be^{\epsilon}h(x)\right)\) and find \(r_{P}>0\) such that \(\int_{\mathbb{R}^{n}}g_{r_{P}}(x)dx\in[1-\delta_{1},1+\delta_{2}]\); we delineate a numerical algorithm for this task in Section 3.2 based on the bisection method and a numerical integration method. Then, we get a private sample by sampling from the distribution with pdf \(\hat{q}(x)=g_{r_{P}}(x)/\int_{\mathbb{R}^{n}}g_{r_{P}}(x)dx\). For the finite case as in Theorem 3.1, we can implement in the same way, except replacing the integral with the sum.

It is important to note that \(\hat{q}(x)\in\left[\frac{bh(x)}{1+\delta_{2}},\frac{be^{\epsilon}h(x)}{1- \delta_{1}}\right]\), indicating that the resulting \(\mathbf{Q}^{*}_{k,\epsilon}\) and \(\mathbf{Q}^{*}_{c_{1},c_{2},h,\epsilon}\) satisfy \(\left(\epsilon+\log\frac{1+\delta_{2}}{1-\delta_{1}}\right)\)-LDP as opposed to \(\epsilon\)-LDP. Thus, the above implementation yields \(\epsilon\)-LDP if it is used to implement \(\mathbf{Q}^{*}_{k,\epsilon^{\prime}}\) or \(\mathbf{Q}^{*}_{c_{1},c_{2},h,\epsilon^{\prime}}\), with \(\epsilon^{\prime}=\epsilon-\log\frac{1+\delta_{2}}{1-\delta_{1}}\) and sufficiently small \(\delta_{1},\delta_{2}\) such that \(\epsilon^{\prime}>0\).

### Continuity of the proposed mechanism

In some practical scenarios, the client may not have full access to their distribution \(P\). One example is that the client can only access to samples from \(P\). In such case, the client may first estimate the true distribution, and then perturb the estimated distribution through the optimal mechanism. The question is how the perturbation using the estimated distribution deviates from that using the true distribution. To answer this, we show that the proposed mechanism satisfies a pointwise Lipschitz property with respect to the total variation distance, and the Lipschitz constant is closely related to the factor \(r_{P}\) we introduce in Theorems 3.1 and 3.3.

**Proposition 4.1**.: _Assuming the setups of \(\left(\mathcal{X},\bar{\mathcal{P}},\epsilon\right)\) in either Theorem 3.1 or 3.3, let \(\mathbf{Q}^{*}\) denote the proposed mechanism \(\mathbf{Q}_{k,\epsilon}^{*}\) or \(\mathbf{Q}_{c_{1},c_{2},h,\epsilon}^{*}\). Then, for any \(P,P^{\prime}\in\bar{\mathcal{P}}\), we have_

\[D_{\mathrm{TV}}\left(\mathbf{Q}^{*}(P),\mathbf{Q}^{*}(P^{\prime})\right)\leq \frac{2}{\max(r_{P},r_{P^{\prime}})}D_{\mathrm{TV}}\left(P,P^{\prime}\right)\] (14)

_where \(r_{P}>0\) is as in Theorem 3.1 or 3.3._

This guarantees that for each given true \(P\) and given \(\delta>0\), whenever the approximated \(P^{\prime}\) satisfies \(D_{\mathrm{TV}}\left(P,P^{\prime}\right)\leq\delta r_{P}/2\), the perturbed distribution \(\mathbf{Q}^{*}(P^{\prime})\) satisfies \(D_{\mathrm{TV}}\left(\mathbf{Q}^{*}(P),\mathbf{Q}^{*}(P^{\prime})\right)\leq\delta\). In theoretical perspective, this proposition implies that \(\mathbf{Q}^{*}\) is continuous when \(\bar{\mathcal{P}}\) and \(\mathcal{P}(\mathcal{X})\) are endowed with the metric topology from the total variation distance. The proof of Proposition 4.1 is in Appendix D.

## 5 Numerical Results

In this section, we numerically compare the worst-case \(f\)-divergence of our proposed mechanism with that of the previously proposed sampling mechanism. To the best of our knowledge, the only work about the private sampling under LDP is [35], hence we set the baseline as the mechanism proposed in [35]. In all the cases, we perform the comparison across three canonical \(f\)-divergences: KL divergence, total variation distance, and squared Hellinger distance, as well as across five values of \(\epsilon\): 0.1, 0.5, 1, 2, and 5.

### Comparison for finite data space

In this subsection, we compare the mechanisms in the finite space, \(\mathcal{X}=[k]\) and \(\bar{\mathcal{P}}=\mathcal{P}([k])\). As mentioned in Sections 2.3 and 3.1, the baseline mechanism has a hyper-parameter, a reference probability distribution \(Q_{0}\in\mathcal{P}(\mathcal{X})\). We set the baseline as a generalized \(f\)-divergence projection onto the relative mollifier. That is, for each given \(f\)-divergence, we set the baseline to satisfy \(\mathbf{Q}(P)\in\arg\min_{Q\in\mathcal{M}_{\epsilon,Q_{0}}}D_{f}\left(P\|Q\right)\), where \(\mathcal{M}_{\epsilon,Q_{0}}\) is defined in Section 2.3. As expected by symmetry, for any \(f\), choosing \(Q_{0}\) to be the uniform distribution minimizes the worst-case \(f\)-divergence \(R_{f}(\mathbf{Q})\) among all choices of \(Q_{0}\) for the baseline. Also, even though we do not obtain the closed-form expression of \(\mathbf{Q}(x|P)\) for the baseline, we obtain the value of \(R_{f}(\mathbf{Q})\) when \(Q_{0}\) is the uniform distribution. The proof of this fact, together with the precise value of \(R_{f}(\mathbf{Q})\) for uniform \(Q_{0}\), is in Appendix F.1. Hence, we always set \(Q_{0}\) to be the uniform distribution in the result about the baseline. Since we have the precise values of \(R_{f}(\mathbf{Q})\) for both our proposed mechanism and the baseline, we plot such values of \(R_{f}(\mathbf{Q})\) in Figure 3. For simplicity, we only provide the plot for \(k=10\). More plots for some other \(k\)'s can be found in Appendix G. As shown by the figure, the proposed mechanism has lower worst-case \(f\)-divergence than the baseline for all choices of \(f\)-divergences and \(\epsilon\) in the experiment, with significant gap in medium privacy regime \(\epsilon\in[0.5,2]\).

Figure 3: Theoretical worst-case \(f\)-divergences of proposed and previously proposed baseline mechanisms (with uniform \(Q_{0}\)) over finite space (\(k=10\))

(Left: KL divergence, Center: Total variation distance, Right: Squared Hellinger distance)

### Comparison for 1D Gaussian mixture

In this subsection, we conduct an experiment to compare the mechanisms when the client distributions are Gaussian mixtures over a real line \(\mathcal{X}=\mathbb{R}\), which is an instance of a continuous space case. We consider the case that each client has a Gaussian mixture distribution in \(\mathbb{R}\), where each Gaussian has a mean bounded by \(1\) and has a unit variance. To avoid arbitrarily large number of Gaussian distributions to be mixed, we set an upper bound \(K\) of the number of Gaussian distributions to be mixed per client. Also, to make the numerical integration tractable, we truncate the domain of the distributions to lie inside an interval \([-4,4]\). Unlike the finite space case, there is no known closed-form expression of the worst-case \(f\)-divergence for the mechanism in [35]. The set of Gaussian mixtures is not exactly of the form \(\tilde{\mathcal{P}}=\tilde{\mathcal{P}}_{c_{1},c_{2},h}\), hence our proposed mechanism also does not have a known closed-form expression of the worst-case \(f\)-divergence. Hence, instead, we compare the mechanisms by an empirical worst-case \(f\)-divergence.

For an experiment, we randomly construct \(N\) Gaussian mixture distributions \(P_{1},P_{2},\cdots,P_{N}\in\tilde{\mathcal{P}}\), where each \(P_{j}\) is generated independently according to some rules specified in Appendix F.2. After that, we plot the value of the empirical maximum \(f\)-divergence \(\max_{j\in[N]}D_{f}\left(P_{j}\|\mathbf{Q}(P_{j})\right)\) for the baseline and our proposed \(\mathbf{Q}\). For the baseline mechanism, we use MBDE with the same hyperparameter setup as [35, Section 5], except a slight modification of the reference distribution to consider the truncation of the domain. The implementation details are provided in Appendix F.2.

In Figure 4, we present the result for \(N=100\) and \(K=10\). We can see that the proposed mechanism has much lower worst-case \(f\)-divergence than the baseline for all choices of \(f\)-divergences and \(\epsilon\).

## 6 Conclusion

In this paper, we characterized the optimal privacy-utility trade-off for the private sampling under LDP and found the optimal private sampling mechanism in terms of the minimax \(f\)-divergence between original and sampling distributions, for both finite and continuous data spaces. Compared to the previous work [35] based on relative mollifier with arbitrarily chosen reference distribution, our work characterizes PUT without dependency on external information other than the original distribution, and it is shown that the mechanism we found is universally optimal under any \(f\)-divergence.

For future works, there may be other \(\tilde{\mathcal{P}}\) and other measures of utility more appropriate to practical scenarios, which are not handled in this paper. For example, \(f\)-divergence may be an inappropriate utility loss because it only depends on \(\sigma\)-algebra structure and does not consider additional information about geometry of \(\mathcal{X}\), such as underlying metric on \(\mathcal{X}\). Using utility measures involving the geometry, such as Wasserstein distance [62; 63; 49], may be more appropriate for some scenarios. Also, we can consider the Bayesian approach instead of the worst-case approach. Furthermore, we only consider the task of releasing a single sample per client in this paper. We may also consider the case of releasing multiple samples per client, rather than a single sample.

The limitations and broader impacts of this work are in Appendices I and J, respectively.

Figure 4: Empirical worst-case \(f\)-divergences of proposed and baseline mechanisms over 100 experiments of 1D Gaussian mixture (Left: KL divergence, Center: Total variation distance, Right: Squared Hellinger distance)

## Acknowledgments and Disclosure of Funding

We thank the anonymous reviewers for their valuable discussions and comments, particularly regarding the identification of an alternative optimal mechanism, the advantage of the proposed mechanism over the alternative, and the effect of the approximation error on \(r_{P}\).

This work was supported in part by the National Research Foundation of Korea (NRF) Grant funded by the Korea Government (MSIT) under Grant 2022R1A2C2092151 and in part by Institute of Information & Communications Technology Planning & Evaluation (IITP) under 6G-Cloud Research and Education Open Hub (IITP-2024-RS-2024-00428780) grant funded by the Korea government (MSIT).

## References

* Nasr et al. [2023] Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A. Feder Cooper, Daphne Ippolito, Christopher A. Choquette-Choo, Eric Wallace, Florian Tramer, and Katherine Lee. Scalable Extraction of Training Data from (Production) Language Models. _arXiv preprint arXiv:2311.17035_, November 2023.
* Wu et al. [2024] Xiaodong Wu, Ran Duan, and Jianbing Ni. Unveiling security, privacy, and ethical concerns of ChatGPT. _Journal of Information and Intelligence_, 2(2):102-115, March 2024. ISSN 29497159. doi: 10.1016/j.jixd.2023.10.007.
* Yan et al. [2024] Biwei Yan, Kun Li, Minghui Xu, Yueyan Dong, Yue Zhang, Zhaochun Ren, and Xiuzhen Cheng. On Protecting the Data Privacy of Large Language Models (LLMs): A Survey. _arXiv preprint arXiv:2403.05156_, March 2024.
* Li et al. [2024] Zhangheng Li, Junyuan Hong, Bo Li, and Zhangyang Wang. Shake to Leak: Fine-tuning Diffusion Models Can Amplify the Generative Privacy Risk. In _2024 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)_, pages 18-32, April 2024. doi: 10.1109/SaTML59370.2024.00010.
* Kasiviswanathan et al. [2011] Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. What Can We Learn Privately? _SIAM Journal on Computing_, 40(3):793-826, January 2011. ISSN 0097-5397. doi: 10.1137/090756090.
* Erlingsson et al. [2014] Ulfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova. Rappor: Randomized aggregatable privacy-preserving ordinal response. In _Proceedings of the 2014 ACM SIGSAC conference on computer and communications security_, pages 1054-1067. ACM, 2014.
* Bittau et al. [2017] Andrea Bittau, Ulfar Erlingsson, Petros Maniatis, Ilya Mironov, Ananth Raghunathan, David Lie, Mitch Rudominer, Ushasree Kode, Julien Tinnes, and Bernhard Seefeld. Prochlo: Strong privacy for analytics in the crowd. In _Proceedings of the 26th Symposium on Operating Systems Principles_, SOSP '17, page 441-459. Association for Computing Machinery, 2017. ISBN 9781450350853.
* Apple [2017] Differential privacy team Apple. Learning with privacy at scale. _Technical report, Apple_, 2017.
* Ding et al. [2017] Bolin Ding, Janardhan Kulkarni, and Sergey Yekhanin. Collecting Telemetry Data Privately. In _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017.
* Duchi et al. [2013] John C. Duchi, Michael I. Jordan, and Martin J. Wainwright. Local Privacy and Statistical Minimax Rates. In _2013 IEEE 54th Annual Symposium on Foundations of Computer Science_, pages 429-438, October 2013. doi: 10.1109/FOCS.2013.53.
* Ye and Barg [2018] Min Ye and Alexander Barg. Optimal Schemes for Discrete Distribution Estimation Under Locally Differential Privacy. _IEEE Transactions on Information Theory_, 64(8):5662-5676, August 2018. ISSN 0018-9448, 1557-9654. doi: 10.1109/TIT.2018.2809790.
* Bhowmick et al. [2019] Abhishek Bhowmick, John Duchi, Julien Freudiger, Gaurav Kapoor, and Ryan Rogers. Protection Against Reconstruction and Its Applications in Private Federated Learning. _arXiv preprint arXiv:1812.00984_, June 2019.
* Asi et al. [2022] Hilal Asi, Vitaly Feldman, and Kunal Talwar. Optimal Algorithms for Mean Estimation under Local Differential Privacy. In _Proceedings of the 39th International Conference on Machine Learning_, pages 1046-1056. PMLR, June 2022.

* [14] Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, and Adam Sealfon. On computing pairwise statistics with local differential privacy. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems_, volume 36, pages 27129-27146. Curran Associates, Inc., 2023.
* [15] Bonwoo Lee, Jeongyoun Ahn, and Cheolwoo Park. Minimax risks and optimal procedures for estimation under functional local differential privacy. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems_, volume 36, pages 57964-57975. Curran Associates, Inc., 2023.
* [16] Hilal Asi, Vitaly Feldman, Jelani Nelson, Huy Nguyen, and Kunal Talwar. Fast optimal locally private mean estimation via random projections. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems_, volume 36, pages 16271-16282. Curran Associates, Inc., 2023.
* [17] Berivan Isik, Wei-Ning Chen, Ayfer Ozgur, Tsachy Weissman, and Albert No. Exact optimality of communication-privacy-utility tradeoffs in distributed mean estimation. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems_, volume 36, pages 37761-37785. Curran Associates, Inc., 2023.
* [18] Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private Empirical Risk Minimization: Efficient Algorithms and Tight Error Bounds. In _2014 IEEE 55th Annual Symposium on Foundations of Computer Science_, pages 464-473, October 2014. doi: 10.1109/FOCS.2014.56.
* [19] Stacey Truex, Ling Liu, Ka-Ho Chow, Mehmet Emre Gursoy, and Wenqi Wei. LDP-Fed: Federated learning with local differential privacy. In _Proceedings of the Third ACM International Workshop on Edge Systems, Analytics and Networking_, EdgeSys '20, pages 61-66, New York, NY, USA, May 2020. Association for Computing Machinery. ISBN 978-1-4503-7132-2. doi: 10.1145/3378679.3394533.
* [20] Badih Ghazi, Pritish Kamath, Ravi Kumar, Ethan Leeman, Pasin Manurangsi, Avinash V. Varadarajan, and Chiyuan Zhang. Regression with Label Differential Privacy. _arXiv preprint arXiv:2212.06074_, October 2023.
* [21] Vitaly Feldman, Ilya Mironov, Kunal Talwar, and Abhradeep Thakurta. Privacy Amplification by Iteration. In _2018 IEEE 59th Annual Symposium on Foundations of Computer Science (FOCS)_, pages 521-532, October 2018. doi: 10.1109/FOCS.2018.00056.
* [22] Mengchu Li, Tom Berrett, and Yi Yu. Network change point localisation under local differential privacy. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 15013-15026. Curran Associates, Inc., 2022.
* [23] Shuzhen Chen, Dongxiao Yu, Yifei Zou, Jiguo Yu, and Xiuzhen Cheng. Decentralized Wireless Federated Learning With Differential Privacy. _IEEE Transactions on Industrial Informatics_, 18(9):6273-6282, September 2022. ISSN 1941-0050. doi: 10.1109/TII.2022.3145010.
* [24] Youssef Allouah, Rachid Guerraoui, Nirupam Gupta, Rafael Pinot, and John Stephan. On the Privacy-Robustness-Utility Trilemma in Distributed Learning. In _Proceedings of the 40th International Conference on Machine Learning_, pages 569-626. PMLR, July 2023.
* [25] Chuan Guo, Kamalika Chaudhuri, Pierre Stock, and Michael Rabbat. Privacy-Aware Compression for Federated Learning Through Numerical Mechanism Design. In _Proceedings of the 40th International Conference on Machine Learning_, pages 11888-11904. PMLR, July 2023.
* [26] Yi Liu, Qirui Hu, Lei Ding, and Linglong Kong. Online Local Differential Private Quantile Inference via Self-normalization. In _Proceedings of the 40th International Conference on Machine Learning_, pages 21698-21714. PMLR, July 2023.
* [27] Jin Sima, Changlong Wu, Olgica Milenkovic, and Wojciech Szpankowski. Online Distribution Learning with Local Privacy Constraints. In _Proceedings of The 27th International Conference on Artificial Intelligence and Statistics_, pages 460-468. PMLR, April 2024.
* [28] Antonious M. Girgis, Deepesh Data, and Suhas Diggavi. Distributed User-Level Private Mean Estimation. In _2022 IEEE International Symposium on Information Theory (ISIT)_, pages 2196-2201, June 2022. doi: 10.1109/ISIT50566.2022.9834713.
* [29] Ruiquan Huang, Huanyu Zhang, Luca Melis, Milan Shen, Meisam Hejazinia, and Jing Yang. Federated Linear Contextual Bandits with User-level Differential Privacy. In _Proceedings of the 40th International Conference on Machine Learning_, pages 14060-14095. PMLR, July 2023.

* Acharya et al. [2023] Jayadev Acharya, Yuhan Liu, and Ziteng Sun. Discrete Distribution Estimation under User-level Local Differential Privacy. In _Proceedings of The 26th International Conference on Artificial Intelligence and Statistics_, pages 8561-8585. PMLR, April 2023.
* Bassily and Sun [2023] Raef Bassily and Ziteng Sun. User-level Private Stochastic Convex Optimization with Optimal Rates. In _Proceedings of the 40th International Conference on Machine Learning_, pages 1838-1851. PMLR, July 2023.
* Mao et al. [2024] Yulian Mao, Qingqing Ye, Haibo Hu, Qi Wang, and Kai Huang. PrivShape: Extracting Shapes in Time Series under User-Level Local Differential Privacy. _arXiv preprint arXiv:2404.03873_, April 2024.
* Sun et al. [2024] Kangkang Sun, Jun Wu, Ali Kashif Bashir, Jianhua Li, Hansong Xu, Qianqian Pan, and Yasser D. Al-Otaibi. Personalized Privacy-Preserving Distributed Artificial Intelligence for Digital-Twin-Driven Vehicle Road Cooperation. _IEEE Internet of Things Journal_, pages 1-1, 2024. ISSN 2327-4662. doi: 10.1109/JIOT.2024.3389656.
* Kent et al. [2024] Alexander Kent, Thomas B. Berrett, and Yi Yu. Rate Optimality and Phase Transition for User-Level Local Differential Privacy. _arXiv preprint arXiv:2405.11923_, May 2024.
* Husain et al. [2020] Hisham Husain, Borja Balle, Zac Cranko, and Richard Nock. Local Differential Privacy for Sampling. In _Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics_, pages 3404-3413. PMLR, June 2020.
* Flemings et al. [2024] James Flemings, Meisam Razaviayn, and Murali Annavaram. Differentially Private Next-Token Prediction of Large Language Models. _arXiv preprint arXiv:2403.15638_, April 2024.
* Raskhodnikova et al. [2021] Sofya Raskhodnikova, Satchit Sivakumar, Adam Smith, and Marika Swanberg. Differentially private sampling from distributions. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 28983-28994. Curran Associates, Inc., 2021.
* Ghazi et al. [2023] Badih Ghazi, Xiao Hu, Ravi Kumar, and Pasin Manurangsi. On differentially private sampling from gaussian and product distributions. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems_, volume 36, pages 77783-77809. Curran Associates, Inc., 2023.
* Ebadi et al. [2016] Hamid Ebadi, Thibaud Antignac, and David Sands. Sampling and partitioning for differential privacy. In _2016 14th Annual Conference on Privacy, Security and Trust (PST)_, pages 664-673, February 2016. doi: 10.1109/PST.2016.7906954.
* Abay et al. [2019] Nazmiye Ceren Abay, Yan Zhou, Murat Kantarcioglu, Bhavani Thuraisingham, and Latanya Sweeney. Privacy Preserving Synthetic Data Release Using Deep Learning. In Michele Berlingerio, Francesco Bonchi, Thomas Gartner, Neil Hurley, and Georgiana Ifrim, editors, _Machine Learning and Knowledge Discovery in Databases_, Lecture Notes in Computer Science, pages 510-526, Cham, 2019. Springer International Publishing. ISBN 978-3-030-10925-7. doi: 10.1007/978-3-030-10925-7_31.
* Xie et al. [2018] Liyang Xie, Kaixiang Lin, Shu Wang, Fei Wang, and Jiayu Zhou. Differentially Private Generative Adversarial Network. _arXiv preprint arXiv:1802.06739_, February 2018.
* 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 2927-2931, May 2020. doi: 10.1109/ICASSP40776.2020.9054559.
* Torkzadehmahani et al. [2019] Reihaneh Torkzadehmahani, Peter Kairouz, and Benedict Paten. DP-CGAN: Differentially Private Synthetic Data and Label Generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops_, pages 0-0, 2019.
* Liu et al. [2019] Yi Liu, Jialiang Peng, James J.Q. Yu, and Yi Wu. PPGAN: Privacy-Preserving Generative Adversarial Network. In _2019 IEEE 25th International Conference on Parallel and Distributed Systems (ICPADS)_, pages 985-989, February 2019. doi: 10.1109/ICPADS47876.2019.00150.
* Cunningham et al. [2022] Teddy Cunningham, Konstantin Klemmer, Hongkai Wen, and Hakan Ferhatosmanoglu. GeoPointGAN: Synthetic Spatial Data with Local Label Differential Privacy. _arXiv preprint arXiv:2205.08886_, May 2022.
* Zhang et al. [2023] Hua Zhang, Kaixuan Li, Teng Huang, Xin Zhang, Wenmin Li, Zhengping Jin, Fei Gao, and Minghui Gao. Publishing locally private high-dimensional synthetic data efficiently. _Information Sciences_, 633:343-356, July 2023. ISSN 0020-0255. doi: 10.1016/j.ins.2023.03.014.

* Shibata et al. [2023] Hisaichi Shibata, Shouhei Hanaoka, Yang Cao, Masatoshi Yoshikawa, Tomomi Takenaga, Yukihiro Nomura, Naoto Hayashi, and Osamu Abe. Local Differential Privacy Image Generation Using Flow-Based Deep Generative Models. _Applied Sciences_, 13(18):10132, January 2023. ISSN 2076-3417. doi: 10.3390/app131810132.
* Gwon et al. [2024] Hansle Gwon, Imjin Ahn, Yunha Kim, Hee Jun Kang, Hyeram Seo, Heejung Choi, Ha Na Cho, Minkyoung Kim, JiYe Han, Gaeun Kee, Seohyun Park, Kye Hwa Lee, Tae Joon Jun, and Young-Hak Kim. LDP-GAN : Generative adversarial networks with local differential privacy for patient medical records synthesis. _Computers in Biology and Medicine_, 168:107738, January 2024. ISSN 0010-4825. doi: 10.1016/j.compbiomed.2023.107738.
* Imola et al. [2024] Jacob Imola, Amrita Roy Chowdhury, and Kamalika Chaudhuri. Metric Differential Privacy at the User-Level. _arXiv preprint arXiv:2405.02665_, May 2024.
* Dwork et al. [2006] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating Noise to Sensitivity in Private Data Analysis. In Shai Halevi and Tal Rabin, editors, _Theory of Cryptography_, Lecture Notes in Computer Science, pages 265-284, Berlin, Heidelberg, 2006. Springer. ISBN 978-3-540-32732-5. doi: 10.1007/11681878_14.
* Sason [2018] Igal Sason. On \(f\)-Divergences: Integral Representations, Local Behavior, and Inequalities. _Entropy_, 20(5):383, May 2018. ISSN 1099-4300. doi: 10.3390/e20050383.
* Sason and Verdu [2016] Igal Sason and Sergio Verdu. \(f\)-Divergence Inequalities. _IEEE Transactions on Information Theory_, 62(11):5973-6006, January 2016. ISSN 1557-9654. doi: 10.1109/TIT.2016.2603151.
* Goodfellow et al. [2020] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. _Communications of the ACM_, 63(11):139-144, October 2020. ISSN 0001-0782. doi: 10.1145/3422622.
* Stein and Shakarchi [2005] Elias M. Stein and Rami Shakarchi. _Real Analysis: Measure Theory, Integration, and Hilbert Spaces_. Princeton University Press, 2005. ISBN 978-0-691-11386-9. doi: 10.2307/j.ctv458v18.
* Kairouz et al. [2016] Peter Kairouz, Sewoong Oh, and Pramod Viswanath. Extremal Mechanisms for Local Differential Privacy. _Journal of Machine Learning Research_, 17(17):1-51, 2016. ISSN 1533-7928.
* Holohan et al. [2017] Naoise Holohan, Douglas J. Leith, and Oliver Mason. Extreme points of the local differential privacy polytope. _Linear Algebra and its Applications_, 534:78-96, December 2017. ISSN 0024-3795. doi: 10.1016/j.laa.2017.08.011.
* Sensia et al. [2023] Ankit Pensia, Amir R. Asadi, Varun Jog, and Po-Ling Loh. Simple Binary Hypothesis Testing under Local Differential Privacy and Communication Constraints. _arXiv preprint arXiv:2301.03566_, December 2023.
* Nam et al. [2024] Seung-Hyun Nam, Vincent Y. F. Tan, and Si-Hyeon Lee. Optimal Private Discrete Distribution Estimation With 1-bit Communication. _IEEE Transactions on Information Forensics and Security_, 19:6514-6528, 2024. ISSN 1556-6021. doi: 10.1109/TIFS.2024.3419721.
* Rukhin [1997] Andrew Rukhin. Information-type divergence when the likelihood ratios are bounded. _Applicationes Mathematicae_, 4(24):415-423, 1997. ISSN 1233-7234.
* Warner [1965] Stanley L Warner. Randomized response: A survey technique for eliminating evasive answer bias. _Journal of the American Statistical Association_, 60(309):63-69, 1965.
* Liese and Vajda [2006] F. Liese and I. Vajda. On Divergences and Informations in Statistics and Information Theory. _IEEE Transactions on Information Theory_, 52(10):4394-4412, October 2006. ISSN 1557-9654. doi: 10.1109/TIT.2006.881731.
* Villani [2009] Cedric Villani. _Optimal Transport: Old and New_. Number 338 in Grundlehren Der Mathematischen Wissenschaften. Springer, Berlin, 2009. ISBN 978-3-540-71049-3.
* Arjovsky et al. [2017] Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein GAN. _arXiv preprint arXiv:1701.07875_, December 2017.
* Boyd and Vandenberghe [2004] Stephen P. Boyd and Lieven Vandenberghe. _Convex Optimization_. Cambridge University Press, Cambridge, UK ; New York, 2004. ISBN 978-0-521-83378-3.
* Rockafellar [1970] R. Tyrrell Rockafellar. _Convex Analysis_. Princeton University Press, 1970. ISBN 978-0-691-01586-6.
* Metropolis et al. [1953] Nicholas Metropolis, Arianna W. Rosenbluth, Marshall N. Rosenbluth, Augusta H. Teller, and Edward Teller. Equation of State Calculations by Fast Computing Machines. _The Journal of Chemical Physics_, 21(6):1087-1092, June 1953. ISSN 0021-9606. doi: 10.1063/1.1699114.

* [67] Christian P. Robert. The Metropolis-Hastings algorithm. _arXiv preprint arXiv:1504.01896_, January 2016.
* [68] Peter Kairouz, Sewoong Oh, and Pramod Viswanath. The Composition Theorem for Differential Privacy. In _Proceedings of the 32nd International Conference on Machine Learning_, pages 1376-1385. PMLR, June 2015.
* [69] Ryan M Rogers, Aaron Roth, Jonathan Ullman, and Salil Vadhan. Privacy Odometers and Filters: Pay-as-you-Go Composition. In _Advances in Neural Information Processing Systems_, volume 29. Curran Associates, Inc., 2016.

Assumptions about the Measure Theory

The appendices assume the familiarity with the basic measure theory and real analysis. We refer the standard textbooks about the measure theory and real analysis, e.g. [54].

Throughout the main paper and the appendices, we assume the followings:

* For each sample space \(\mathcal{X}\), a \(\sigma\)-algebra on \(\mathcal{X}\) is implicitly given. Unless mentioned otherwise,
* the discrete \(\sigma\)-algebra is given for finite \(\mathcal{X}\), and
* the Borel \(\sigma\)-algebra is given for \(\mathcal{X}=\mathbb{R}^{n}\).
* A "subset" of \(\mathcal{X}\) always means a "measurable subset", and similarly \(A\subset\mathcal{X}\) always means \(A\) is a measurable subset.
* The "continuous" distribution precisely means the "absolutely continuous" distribution (with respect to the Lebesgue measure).

In the appendices, we also introduce the following notations:

* For finite \(\mathcal{X}\), \(\#\) denotes the counting measure.
* For \(\mathcal{X}=\mathbb{R}^{n}\), \(m\) denotes the Lebesgue measure.

## Appendix B General Definition and More Properties of \(f\)-divergences

In this appendix, we review the general definition and additional properties about the \(f\)-divergences which are important in our analysis. Let a convex function \(f:(0,\infty)\to\mathbb{R}\) with \(f(1)=0\) be given. For \(P,Q\in\mathcal{P}(\mathcal{X})\), not necessarily \(P\ll Q\), we first take a dominating measure \(\mu\) on \(\mathcal{X}\) such that \(P,Q\ll\mu\) (e.g., \(\mu=P+Q\)), and let \(p=dP/d\mu\), \(q=dQ/d\mu\). The \(f\)-divergence \(D_{f}\left(P\|Q\right)\) is defined as

\[D_{f}\left(P\|Q\right)=\int q(x)f\left(\frac{p(x)}{q(x)}\right)d\mu(x).\] (15)

We note that (15) is invariant under the choice of the dominating measure \(\mu\).

Since it is possible that \(p(x)=0\) or \(q(x)=0\), we need to define what is the value of the expression \(qf(p/q)\) for \(p=0\) or \(q=0\)[51, 52]. It is well-known that any convex function \(f:(0,1)\to\mathbb{R}\) is continuous, and the function

\[f^{\star}(x)=xf(1/x)\] (16)

is also convex on \((0,1)\). Furthermore, any convex function \(f:(0,1)\to\mathbb{R}\) with \(f(1)=0\) has a limit \(\lim_{x\to 0+}f(x)\) in \(\mathbb{R}\cup\{+\infty\}\). Hence, we have the continuous extension \(f:[0,\infty)\to\mathbb{R}\cup\{+\infty\}\) by setting \(f(0)=\lim_{x\to 0+}f(x)\), which is proper convex and continuous. By the same way, we have the continuous extension \(f^{\star}:[0,\infty)\to\mathbb{R}\cup\{+\infty\}\). Using these extensions, we define \(0f(0/0)=0\), \(qf(0/q)=qf(0)\) for \(q>0\), and \(0f(p/0)=pf^{\star}(0)\) for \(p>0\). Especially, if \(f^{\star}(0)=\infty\), then \(D_{f}\left(P\|Q\right)=\infty\) whenever \(P\ll Q\) does not hold. (This is the case for KL divergence and \(\chi^{2}\) divergence for examples) Similarly, if \(f(0)=\infty\), then \(D_{f}\left(P\|Q\right)=\infty\) whenever \(Q\ll P\) does not hold. Also, the maximum value of the \(f\)-divergence \(M_{f}\) presented in (2) can be written as \(M_{f}=f(0)+f^{\star}(0)\).

The following additional properties of \(f\)-divergences are important in our analysis. [61]

**Theorem B.1**.: _Any \(f\)-divergence is jointly convex, that is for any \(P_{1},P_{2},Q_{1},Q_{2}\in\mathcal{P}(\mathcal{X})\) and \(0\leq\lambda\leq 1\), we have \(D_{f}\left(\lambda P_{1}+(1-\lambda)P_{2}\|\lambda Q_{1}+(1-\lambda)Q_{2} \right)\leq\lambda D_{f}\left(P_{1}\|Q_{1}\right)+(1-\lambda)D_{f}\left(P_{2} \|Q_{2}\right)\)._

**Theorem B.2** (Data-Processing Inequality).: _Let \(M\) be a conditional distribution (Markov kernel) from \(\mathcal{X}\) to \(\mathcal{Y}\). For given \(P_{1},P_{2}\in\mathcal{P}(\mathcal{X})\), let \(Q_{1},Q_{2}\in\mathcal{P}(\mathcal{Y})\) be the push-forward measure of \(P_{1},P_{2}\) through \(M\), respectively. Then for any \(f\)-divergence, we have \(D_{f}\left(P_{1}\|P_{2}\right)\geq D_{f}\left(Q_{1}\|Q_{2}\right)\)._

Also, we present several equivalent expressions for the total variation distance [51, 52], which are used in Appendix D. In below expressions, the assumption is that \(P,Q\ll\mu\) for some dominatingmeasure \(\mu\) with \(p=dP/d\mu\) and \(q=dQ/d\mu\).

\[D_{\mathrm{TV}}\left(P,Q\right) =\frac{1}{2}\int|p(x)-q(x)|d\mu(x)\] (17) \[=\int_{x:p(x)\leq q(x)}(q(x)-p(x))d\mu(x)\] (18) \[=\int_{x:p(x)\geq q(x)}(p(x)-q(x))d\mu(x).\] (19)

We introduce one more notation. For \(\lambda_{1},\lambda_{2}\in[0,1]\), let \(D_{f}^{\mathrm{B}}\left(\lambda_{1}\middle\|\lambda_{2}\right)\) denotes the \(f\)-divergence between Bernoulli distributions with \(\Pr(1)=\lambda_{1}\) and \(\lambda_{2}\), respectively. That is,

\[D_{f}^{\mathrm{B}}\left(\lambda_{1}\middle\|\lambda_{2}\right)=\lambda_{2}f \left(\frac{\lambda_{1}}{\lambda_{2}}\right)+(1-\lambda_{2})f\left(\frac{1- \lambda_{1}}{1-\lambda_{2}}\right).\] (20)

We should note the following facts:

1. By joint convexity of the \(f\)-divergence and continuity of \(f\), \(D_{f}^{\mathrm{B}}\left(\lambda_{1}\middle\|\lambda_{2}\right)\) is continuous and jointly convex in \((\lambda_{1},\lambda_{2})\). (But \(D_{f}^{\mathrm{B}}\left(\lambda_{1}\middle\|\lambda_{2}\right)\) may be extended real-valued)
2. For a fixed \(\lambda_{1}\), \(D_{f}^{\mathrm{B}}\left(\lambda_{1}\middle\|\lambda_{2}\right)\) attains a global minimum \(0\) at \(\lambda_{2}=\lambda_{1}\). Together with convexity, we derive that \(D_{f}^{\mathrm{B}}\left(\lambda_{1}\middle\|\lambda_{2}\right)\) is decreasing in \(\lambda_{2}\in[0,\lambda_{1}]\) and increasing in \(\lambda_{2}\in[\lambda_{1},1]\), respectively.

## Appendix C Generalized Main Theorem and Proof

In this appendix, we present the formal proofs of the main theorems, Theorems 3.1 and 3.3. As mentioned in Section 3.3, we first state the generalized theorem with its proof, and later we show how this generalized theorem includes the main theorems as special cases.

### Statement of the generalized main theorem

First, let us define the general setup we consider. Let a (general) sample space \(\mathcal{X}\) be given. For a positive measure \(\mu\) on \(\mathcal{X}\) such that \(\mu(\mathcal{X})<\infty\) and \(c_{2}>c_{1}\geq 0\), let us define

\[\tilde{\mathcal{P}}_{c_{1},c_{2},\mu}:=\{P\in\mathcal{P}(\mathcal{X}):P\ll \mu,\quad c_{1}\leq dP/d\mu\leq c_{2}\quad\mu\text{-a.e.}\}.\] (21)

We generally consider the case that \(\tilde{\mathcal{P}}=\tilde{\mathcal{P}}_{c_{1},c_{2},\mu}\) for some \(c_{1},c_{2},\mu\). For example, for the setup of Section 3.2, we have \(\tilde{\mathcal{P}}_{c_{1},c_{2},h}=\tilde{\mathcal{P}}_{c_{1},c_{2},\mu}\), where \(\mu\) is a positive measure with \(\mu\ll m\) and \(d\mu/dm=h\). For the setup of Section 3.1, we have \(\tilde{\mathcal{P}}([k])=\tilde{\mathcal{P}}_{0,1,\#}\). By the same reason as in Section 3.2, we may impose the following normalization condition on \(c_{1},c_{2},\mu,\epsilon\):

\[\mu(\mathcal{X})=1,\quad c_{1}<1<c_{2},\quad c_{2}>c_{1}e^{\epsilon}.\] (22)

Note that \(\mu(\mathcal{X})=1\) means that \(\mu\) is a probability measure, that is \(\mu\in\mathcal{P}(\mathcal{X})\). Also, note that for \(\mathcal{X}=[k]\), we can write in normalized form as \(\mathcal{P}([k])=\tilde{\mathcal{P}}_{0,k,\mu_{k}}\), where \(\mu_{k}=\frac{1}{k}\#\) is the uniform distribution on \([k]\).

First, let us define the proposed mechanism, together with the related constants as the same as Theorem 3.3. For the ease of proof, we introduce an additional constant \(\alpha\) over Theorem 3.3.

**Definition C.1**.: Let \(c_{1},c_{2},\mu,\epsilon\) satisfying the normalization condition (22) be given, and let \(\tilde{\mathcal{P}}=\tilde{\mathcal{P}}_{c_{1},c_{2},\mu}\). First, define the following constants determined by \(c_{1},c_{2},\epsilon\):

\[\alpha =\frac{1-c_{1}}{c_{2}-c_{1}},\] (23) \[b =\frac{c_{2}-c_{1}}{(e^{\epsilon}-1)(1-c_{1})+c_{2}-c_{1}}=\frac{1 }{\alpha e^{\epsilon}+1-\alpha},\] (24) \[r_{1} =\frac{c_{1}}{b}=\left(\frac{(e^{\epsilon}-1)(1-c_{1})+c_{2}-c_{1 }}{c_{2}-c_{1}}\right)c_{1}=c_{1}(\alpha e^{\epsilon}+1-\alpha),\] (25) \[r_{2} =\frac{c_{2}}{be^{\epsilon}}=\left(\frac{(e^{\epsilon}-1)(1-c_{1 })+c_{2}-c_{1}}{c_{2}-c_{1}}\right)\frac{c_{2}}{e^{\epsilon}}=\frac{c_{2}}{e^{ \epsilon}}(\alpha e^{\epsilon}+1-\alpha).\] (26)Also, define a mechanism \(\mathbf{Q}^{*}_{c_{1},c_{2},\mu,\epsilon}\in\mathcal{Q}_{\mathcal{X},\hat{ \mathcal{P}},\epsilon}\) as follows:

_For each \(P\in\tilde{\mathcal{P}}\), \(\mathbf{Q}^{*}_{c_{1},c_{2},\mu,\epsilon}(P)=:Q\) is defined as a probability measure such that \(Q\ll\mu\) and_

\[\frac{dQ}{d\mu}(x)=\operatorname{clip}\left(\frac{1}{r_{P}}\frac{dP}{d\mu}(x); b,be^{\epsilon}\right),\] (27)

_where \(r_{P}>0\) is a constant depending on \(P\) so that \(\int\frac{dQ}{d\mu}d\mu(x)=1\). Furthermore, let \(\mathcal{M}_{c_{1},c_{2},\mu,\epsilon}=\tilde{\mathcal{P}}_{b,be^{\epsilon},\mu}\), so that \(\mathbf{Q}^{*}_{c_{1},c_{2},\mu,\epsilon}(P)\in\mathcal{M}_{c_{1},c_{2},\mu,\epsilon}\) for every \(P\in\tilde{\mathcal{P}}\)._

We should note that \(1=c_{2}\alpha+c_{1}(1-\alpha)=be^{\epsilon}\alpha+b(1-\alpha)\). Also, \(\mathbf{Q}^{*}_{c_{1},c_{2},\mu,\epsilon}\) clearly satisfies \(\epsilon\)-LDP. By the same reason as in Sections 3.1 and 3.2, the values of \(r_{P}\) may not be unique, but the mechanism \(\mathbf{Q}^{*}_{c_{1},c_{2},\mu,\epsilon}\) does not depend on the choice of \(r_{P}\). Furthermore, we show that the value of \(\int\operatorname{clip}\left(\frac{1}{r}\frac{dP}{d\mu}(x);b,be^{\epsilon} \right)d\mu(x)\) at \(r=r_{1}\) and \(r=r_{2}\) are at least and at most \(1\), respectively.

**Proposition C.2**.: _Let \(c_{1},c_{2},\mu,\epsilon,\tilde{\mathcal{P}},\alpha,b,r_{1},r_{2}\) be as in Definition C.1. Then, for any \(P\in\tilde{\mathcal{P}}\), we have_

\[\int\operatorname{clip}\left(\frac{1}{r_{1}}\frac{dP}{d\mu}(x);b,be^{\epsilon }\right)d\mu(x)\geq 1\geq\int\operatorname{clip}\left(\frac{1}{r_{2}}\frac{dP}{d \mu}(x);b,be^{\epsilon}\right)d\mu(x),\] (28)

_where, in the case of \(r_{1}=0\), we define_

\[\operatorname{clip}\left(\frac{1}{r_{1}}\frac{dP}{d\mu}(x);b,be^{\epsilon} \right)=\begin{cases}b,&\text{if }\frac{dP}{d\mu}(x)=0\\ be^{\epsilon},&\text{otherwise}\end{cases}.\] (29)

_Furthermore, if \(\int\operatorname{clip}\left(\frac{1}{r_{1}}\frac{dP}{d\mu}(x);b,be^{\epsilon }\right)d\mu(x)=1\), then \(\int\operatorname{clip}\left(\frac{1}{r}\frac{dP}{d\mu}(x);b,be^{\epsilon} \right)d\mu(x)=1\) for every \(r\in[r_{1},r_{2}]\)._

This proposition, together with the fact that \(r\mapsto\int\operatorname{clip}\left(\frac{1}{r}\frac{dP}{d\mu}(x);b,be^{ \epsilon}\right)d\mu(x)\) is continuous and monotone decreasing, implies that \(r_{P}\) can be chosen such that \(r_{1}<r_{P}\leq r_{2}\), as stated in Theorem 3.3. (Again, the continuity is from \(\mu(\mathcal{X})<\infty\) and the dominated convergence theorem)

Also, we should remark that \(0<\alpha<1\) and \(c_{1}<b<1<be^{\epsilon}<c_{2}\). The first one easily follows from \(c_{1}<1<c_{2}\) and the definition of \(\alpha\). For the second one, first we have \(1<\alpha e^{\epsilon}+1-\alpha<e^{\epsilon}\), as \(\alpha e^{\epsilon}+1-\alpha\) is a propoer convex combination of \(1\) and \(\epsilon\). This directly implies that \(b<1<be^{\epsilon}\). Next, by calculations, we can observe that

\[c_{1}((e^{\epsilon}-1)(1-c_{1})+c_{2}-c_{1})-(c_{2}-c_{1})=(1-c_{1})(c_{1}e^{ \epsilon}-c_{2})<0,\] (30)

which implies \(c_{1}<b\), and

\[c_{2}((e^{\epsilon}-1)(1-c_{1})+c_{2}-c_{1})-e^{\epsilon}(c_{2}-c_{1})=(c_{2}- 1)(c_{2}-e^{\epsilon}c_{1})>0,\] (31)

which implies \(be^{\epsilon}<c_{2}\). Especially, these inequalities imply that \(0\leq r_{1}<1<r_{2}\).

Now, we show that under a mild 'decomposability' condition, the proposed mechanism \(\mathbf{Q}^{*}_{c_{1},c_{2},\mu,\epsilon}\) is universally optimal under any \(f\)-divergences for \((\mathcal{X},\tilde{\mathcal{P}}=\tilde{\mathcal{P}}_{c_{1},c_{2},\mu},\epsilon)\). After that, we show that such a mild condition holds for the setups of both of the main theorems, which finishes the proofs of the main theorems.

First, let us state the 'decomposability' condition. This condition is a formal definition of the concept of "packing of \(u\) copies of \(\mathcal{X}\) by \(t\) subsets \(A_{1},\cdots,A_{t}\)", which is briefly mentioned in Section 3.3,

**Definition C.3**.: Let \(\alpha\in(0,1)\) and \(t,u\in\mathbb{N}\), \(t>u\). We say that a probability measure \(\mu\in\mathcal{P}(\mathcal{X})\) is \((\alpha,t,u)\)**-decomposable** if there exist \(t\) subsets \(A_{1},A_{2},\cdots,A_{t}\subset\mathcal{X}\) such that \(\mu(A_{i})=\alpha\) for all \(i\in[t]\), and for every \(x\in\mathcal{X}\), we have \(|\left\{i\in[t]:x\in A_{i}\right\}|\leq u\).

We say that \(\mu\in\mathcal{P}(\mathcal{X})\) is \(\alpha\)**-decomposable** if for any \(\delta>0\), there exists \(t,u\in\mathbb{N}\), \(t>u\), such that \(\alpha\leq u/t<\alpha+\delta\), and \(\mu\) is \((\alpha,t,u)\)-decomposable.

We remark that \((\alpha,t,1)\)-decomposability means that there are \(t\) disjoint subsets \(B_{1},B_{2},\cdots,B_{t}\) such that \(\mu(B_{i})=\alpha\) for each \(i\in[t]\). Also, if \(\alpha\) is a rational number with \(\alpha=u/t\), \(u,t\in\mathbb{N}\), and \(\mu\) is \((\alpha,t,u)\)-decomposable, then \(\mu\) is \(\alpha\)-decomposable.

Then, we state the generalized theorem.

**Theorem C.4**.: _Let \(c_{1},c_{2},\mu,\epsilon,\tilde{\mathcal{P}},\alpha,b,r_{1},r_{2}\) be as in Definition C.1. If \(\mu\) is \(\alpha\)-decomposable, then for any \(f\)-divergences \(D_{f}\), we have_

\[\mathcal{R}(\mathcal{X},\tilde{\mathcal{P}},\epsilon,f)=\frac{1-r_{1}}{r_{2}- r_{1}}f(r_{2})+\frac{r_{2}-1}{r_{2}-r_{1}}f(r_{1}),\] (32)

_and furthermore, the mechanism \(\mathbf{Q}^{*}_{c_{1},c_{2},\mu,\epsilon}\) as in Definition C.1 is optimal for \((\mathcal{X},\tilde{\mathcal{P}},\epsilon)\) under any \(f\)-divergences \(D_{f}\)._

As guided in Section 3.3, the proof of this theorem is broken into two parts, the achievability part and the converse part.

**Proposition C.5** (Achievability part).: _Let \(c_{1},c_{2},\mu,\epsilon,\tilde{\mathcal{P}},\alpha,b,r_{1},r_{2}\) be as in Definition C.1. Then, for any \(f\)-divergences \(D_{f}\), we have_

\[R_{f}(\mathbf{Q}^{*}_{c_{1},c_{2},\mu,\epsilon})\leq\frac{1-r_{1}}{r_{2}-r_{1 }}f(r_{2})+\frac{r_{2}-1}{r_{2}-r_{1}}f(r_{1}).\] (33)

_(Here, we do not need to assume that \(\mu\) is \(\alpha\)-decomposable)_

**Proposition C.6** (Converse part).: _Let \(c_{1},c_{2},\mu,\epsilon,\tilde{\mathcal{P}},\alpha,b,r_{1},r_{2}\) be as in Definition C.1, and suppose that \(\mu\) is \(\alpha\)-decomposable. Then for any \(f\)-divergences \(D_{f}\) and for any \(\epsilon\)-LDP mechanism \(\mathbf{Q}\in\mathcal{Q}_{\mathcal{X},\tilde{\mathcal{P}},\epsilon}\), we have_

\[R_{f}(\mathbf{Q})\geq\frac{1-r_{1}}{r_{2}-r_{1}}f(r_{2})+\frac{r_{2}-1}{r_{2} -r_{1}}f(r_{1}).\] (34)

The remaining of this appendix is organized as follows. We first present the proofs of Propositions C.5 and C.6 in Appendices C.2 and C.3, in which we grant Proposition C.2 and some intermediate lemmas. After that, in Appendix C.4, we show that Theorem C.4 contains main theorems, Theorems 3.1 and 3.3, as special cases. Finally, Appendix C.5 presents the proof of Proposition C.2, and Appendices C.6 and C.7 prove intermediate lemmas.

### Proof of achievability part (Proposition C.5)

As mentioned in Section 3.3, the proof of the achievability part consists of two steps: first presenting an alternative mechanism and then proving that \(\mathbf{Q}^{*}_{c_{1},c_{2},\mu,\epsilon}\) performs the \(f\)-divergence projection for any general \(f\)-divergence.

#### c.2.1 An alternative mechanism

Let \(\mathbf{Q}^{\dagger}_{c_{1},c_{2},\mu,\epsilon}\in\mathcal{Q}_{\mathcal{X}, \tilde{\mathcal{P}},\epsilon}\) be a mechanism defined as follows:

\[\mathbf{Q}^{\dagger}_{c_{1},c_{2},\mu,\epsilon}(P)=\gamma P+(1-\gamma)\mu,\] (35)

where

\[\gamma=\frac{e^{\epsilon}-1}{(e^{\epsilon}-1)(1-c_{1})+c_{2}-c_{1}}.\] (36)

Since \(c_{2}>e^{\epsilon}c_{1}\), it follows that \(0<\gamma<1\). Also, a direct computation shows that

\[b=\gamma c_{1}+(1-\gamma),\] (37) \[be^{\epsilon}=\gamma c_{2}+(1-\gamma).\] (38)

Notice that since \(c_{1}\leq\frac{dP}{d\mu}\leq c_{2}\) and \(\frac{d\mathbf{Q}^{\dagger}_{c_{1},c_{2},\mu,\epsilon}(P)}{d\mu}=\gamma\frac{ dP}{d\mu}+(1-\gamma)\), we have

\[\frac{d\mathbf{Q}^{\dagger}_{c_{1},c_{2},\mu,\epsilon}(P)}{d\mu}(x) \geq\gamma c_{1}+(1-\gamma)=b,\] (39) \[\frac{d\mathbf{Q}^{\dagger}_{c_{1},c_{2},\mu,\epsilon}(P)}{d\mu}(x) \leq\gamma c_{2}+(1-\gamma)=be^{\epsilon}.\] (40)Hence, \(\mathbf{Q}_{c_{1},c_{2},\mu,\epsilon}^{\dagger}(P)\in\mathcal{M}_{c_{1},c_{2},\mu,\epsilon}\) for every \(P\in\tilde{\mathcal{P}}\), implying that \(\mathbf{Q}_{c_{1},c_{2},\mu,\epsilon}^{\dagger}\) also satisfies \(\epsilon\)-LDP.

Now, we show that

\[R_{f}(\mathbf{Q}_{c_{1},c_{2},\mu,\epsilon}^{\dagger})\leq\frac{1-r_{1}}{r_{2 }-r_{1}}f(r_{2})+\frac{r_{2}-1}{r_{2}-r_{1}}f(r_{1}).\] (41)

To this end, we need to show that for each \(P\in\tilde{\mathcal{P}}\), we have

\[D_{f}\left(P\middle\|\mathbf{Q}_{c_{1},c_{2},\mu,\epsilon}^{\dagger}(P)\right) \leq\frac{1-r_{1}}{r_{2}-r_{1}}f(r_{2})+\frac{r_{2}-1}{r_{2}-r_{1}}f(r_{1}).\] (42)

Fix \(P\in\tilde{\mathcal{P}}\). Let \(p=dP/d\mu\) and \(q=d\mathbf{Q}_{c_{1},c_{2},\mu,\epsilon}^{\dagger}(P)/d\mu\). First, we claim that

\[r_{1}\leq\frac{p(x)}{q(x)}\leq r_{2}\] (43)

for \(\mu\)-almost every \(x\in\mathcal{X}\). We have

\[\frac{p(x)}{q(x)} =\frac{p(x)}{\gamma p(x)+(1-\gamma)}\] (44) \[=\frac{1}{\gamma}\left(1-\frac{1-\gamma}{\gamma p(x)+(1-\gamma)} \right),\] (45)

which is increasing in \(p(x)\geq 0\). Since \(c_{1}\leq p(x)\leq c_{2}\), we have

\[\frac{c_{1}}{\gamma c_{1}+(1-\gamma)}\leq\frac{p(x)}{q(x)}\leq\frac{c_{2}}{ \gamma c_{2}+(1-\gamma)}\] (46)

for \(\mu\)-almost every \(x\in\mathcal{X}\). From (37), (38), and Definition C.1, we conclude that

\[r_{1}\leq\frac{p(x)}{q(x)}\leq r_{2},\] (47)

which proves the claim. For the remaining of the proof, we need the following lemma.

**Lemma C.7** ([59], Theorem 2.1).: _Let \(P,Q\in\mathcal{P}(\mathcal{X})\). Suppose that \(P,Q\ll\mu\) for some reference measure \(\mu\) on \(X\), and there exist \(r_{1},r_{2}\in\mathbb{R}\) with \(0\leq r_{1}<1<r_{2}\) such that the densities \(p=\frac{dP}{d\mu},q=\frac{dQ}{d\mu}\) satisfy \(q(x)>0\) and \(r_{1}\leq\frac{p(x)}{q(x)}\leq r_{2}\) for \(\mu\)-almost every \(x\in\mathcal{X}\). Then for any \(f\)-divergence \(D_{f}\), we have_

\[D_{f}\left(P\middle\|Q\right)\leq\frac{1-r_{1}}{r_{2}-r_{1}}f(r_{2})+\frac{r_{ 2}-1}{r_{2}-r_{1}}f(r_{1}).\] (48)

This lemma directly implies (42), and consequently (41).

#### c.2.2 \(f\)-divergence projection of the proposed mechanism

Next, we prove that \(\mathbf{Q}_{c_{1},c_{2},\mu,\epsilon}^{*}(P)\) is the projection of \(P\) onto \(\mathcal{M}_{c_{1},c_{2},\mu,\epsilon}\) for _every \(f\)-divergence_. Proposition 3.4 can be stated in a more general way as follows.

**Proposition C.8**.: _For any \(P\in\tilde{\mathcal{P}}\) and any \(f\)-divergences \(D_{f}\), we have_

\[D_{f}\left(P\middle\|\mathbf{Q}_{c_{1},c_{2},\mu,\epsilon}^{*}(P)\right)=\inf_ {Q\in\mathcal{M}_{c_{1},c_{2},\mu,\epsilon}}D_{f}\left(P\middle\|Q\right).\] (49)

This proposition implies that

\[D_{f}\left(P\middle\|\mathbf{Q}_{c_{1},c_{2},\mu,\epsilon}^{*}(P)\right)\leq D _{f}\left(P\middle\|\mathbf{Q}_{c_{1},c_{2},\mu,\epsilon}^{\dagger}(P)\right),\] (50)

for every \(P\in\tilde{\mathcal{P}}\), and hence

\[R_{f}(\mathbf{Q}_{c_{1},c_{2},\mu,\epsilon}^{*})\leq R_{f}(\mathbf{Q}_{c_{1},c _{2},\mu,\epsilon}^{\dagger})\leq\frac{1-r_{1}}{r_{2}-r_{1}}f(r_{2})+\frac{r_{ 2}-1}{r_{2}-r_{1}}f(r_{1}),\] (51)

which completes the proof of the achievability part.

Next, we prove Proposition C.8. Fix \(P\in\bar{\mathcal{P}}\) and \(p=dP/d\mu\). If \(f(0)=\infty\) and \(\mu(\{x:p(x)=0\})>0\), then \(D_{f}\left(P\|Q\right)=\infty\) for every \(Q\in\mathcal{M}_{c_{1},c_{2},\mu,\epsilon}\), thus the proposition holds trivially. Consequently, we assume either \(f(0)<\infty\) or \(\mu(\{x:p(x)=0\})=0\).

The optimization problem \(\inf_{Q\in\mathcal{M}_{c_{1},c_{2},\mu,\epsilon}}D_{f}\left(P\|Q\right)\) can be cast as the following

\[\inf_{q:\mathcal{X}\rightarrow(0,\infty)}\int q(x)f\left(\frac{p(x)}{q(x)} \right)d\mu(x)\] (52)

\[\text{such that}\quad q(x) \geq b,\quad\forall x,\] (53) \[q(x) \leq be^{\epsilon},\quad\forall x,\] (54) \[\int q(x)d\mu(x) =1,\] (55)

where \(q=dQ/d\mu\). Our goal is to show that \(q^{*}=d\mathbf{Q}_{c_{1},c_{2},\mu,\epsilon}^{*}(P)/d\mu\) is an optimal solution of the above optimization problem.

Motivation for the optimality proof.To provide the motivation for the proof of the optimality of \(q^{*}\) for the above optimization problem, we first consider the following analogous finite-dimensional optimization problem

\[\inf_{q\in(0,\infty)^{n}}\sum_{i=1}^{n}q_{i}f\left(\frac{p_{i}}{q_{i}}\right)\] (56)

\[\text{such that}\quad q_{i} \geq b,\] (57) \[q_{i} \leq be^{\epsilon},\] (58) \[\sum_{i=1}^{n}q_{i} =1.\] (59)

for convex function \(f:(0,\infty)\rightarrow\mathbb{R}\) and \(p_{i}>0\). The convexity of \(f^{\star}(x)=xf(1/x)\) (see Appendix B) implies that the above is a convex optimization problem. For simplicity, we assume that \(f^{\star}\) is differentiable. However, note that this assumption is only for simplifying the motivation for the proof, and the result holds for general \(f\) or \(f^{\star}\).

We formulate the Lagrangian for the above optimization as

\[\mathcal{L}(q,\phi,\psi,\nu)=\sum_{i=1}^{n}\left(p_{i}f^{\star}(q_{i}/p_{i})+ \phi_{i}(b-q_{i})+\psi_{i}(q_{i}-be^{\epsilon})\right)+\nu\left(1-\sum_{i=1}^ {n}q_{i}\right)\] (60)

with dual variables \(\phi,\psi\in[0,\infty)^{n}\) and \(\nu\in\mathbb{R}\).

The Karush-Kuhn-Tucker (KKT) condition yields:

\[(f^{\star})^{\prime}(q_{i}/p_{i})-\phi_{i}+\psi_{i}-\nu=0,\quad \forall i,\] (61) \[(\ref{eq:KKT}),(\ref{eq:KKT}),(\ref{eq:KKT}),\] (62) \[\phi_{i},\psi_{i} \geq 0,\quad\forall i,\] (63) \[\phi_{i}(q_{i}-b)=\psi_{i}(be^{\epsilon}-q_{i})=0,\quad\forall i.\] (64)

Now, suppose that there is a feasible point \(q^{*}\in(0,\infty)^{n}\) satisfying (57), (58), and (59) such that \(q^{*}=\text{clip}(p_{i}/r;b,be^{\epsilon})\) for some \(r>0\). We show that \(q^{*}\) satisfies the KKT condition for some feasible dual variables \((\phi,\psi,\nu)\).

For \((q^{*},\phi,\psi,\nu)\) to satisfy the KKT condition, the following should hold:

* If \(b<p_{i}/r<be^{\epsilon}\), then we have \(q_{i}^{*}=p_{i}/r\). From (61) and (64), we must have \(\phi_{i}=\psi_{i}=0\) and \(\nu=(f^{\star})^{\prime}(1/r)\).
* If \(p_{i}/r\leq b\), then \(q_{i}^{*}=b\). Then, \(\psi_{i}=0\) and \(\phi_{i}=(f^{\star})^{\prime}(b/p_{i})-\nu\).
* If \(p_{i}/r\geq be^{\epsilon}\), then \(q_{i}^{*}=be^{\epsilon}\). Then \(\phi_{i}=0\) and \(\psi_{i}=\nu-(f^{\star})^{\prime}(be^{\epsilon}/p_{i})\).

Now, since \(f^{\star}\) is convex, \((f^{\star})^{\prime}\) is monotonically increasing. Hence, the following should be satisfied:

* If \(p_{i}/r\leq b\), then \((f^{\star})^{\prime}(b/p_{i})-(f^{\star})^{\prime}(1/r)\geq 0\), and
* If \(p_{i}/r\geq be^{\epsilon}\), then \((f^{\star})^{\prime}(1/r)-(f^{\star})^{\prime}(be^{\epsilon}/p_{i})\geq 0\).

It can thus be verified that \((q^{\star},\phi,\psi,\nu)\) satisfies the KKT condition with

* \(\nu=(f^{\star})^{\prime}(1/r)\),
* \(\phi_{i}=\begin{cases}(f^{\star})^{\prime}(b/p_{i})-(f^{\star})^{\prime}(1/r),&\text{ if }p_{i}/r\leq b,\\ 0,&\text{ otherwise},\end{cases}\)
* \(\psi_{i}=\begin{cases}(f^{\star})^{\prime}(1/r)-(f^{\star})^{\prime}(be^{ \epsilon}/p_{i}),&\text{ if }p_{i}/r\geq be^{\epsilon},\\ 0,&\text{ otherwise}.\end{cases}\)

Optimality proof.Next, we present a proof for the optimality of \(q^{\star}\) for the optimization problem

\[\inf_{q:\mathcal{X}\rightarrow(0,\infty)}\int q(x)f\left(\frac{p( x)}{q(x)}\right)d\mu(x)\] (65) \[\text{such that} q(x)\geq b,\quad\forall x,\] (66) \[q(x)\leq be^{\epsilon},\quad\forall x,\] (67) \[\int q(x)d\mu(x)=1.\] (68)

Here, note that we do not assume \(f\) is differentiable. To this goal, we first review the following basic facts about a general convex function \(f:(0,\infty)\rightarrow\mathbb{R}\), which _may not be differentiable_[64, 65]:

* The left derivative \(f^{\prime}_{-}(x):=\lim_{h\to 0-}\frac{f(x+h)-f(x)}{h}\) and the right derivative \(f^{\prime}_{+}(x):=\lim_{h\to 0+}\frac{f(x+h)-f(x)}{h}\) exist and finite for every \(x\in(0,\infty)\), regardless of whether \(f\) is differentiable or not.
* For every \(0<x<y\), we have \(f^{\prime}_{-}(x)\leq f^{\prime}_{+}(x)\leq f^{\prime}_{-}(y)\leq f^{\prime}_{ +}(y)\).
* For every \(x,y\in(0,\infty)\) and any \(g\in[f^{\prime}_{-}(x),f^{\prime}_{+}(x)]\), we have \[f(y)\geq f(x)+g(y-x).\] (69) By continuous extension, this holds for \(y=0\) also. That is, \(f(0)\geq f(x)-gx\) for every \(x\in(0,\infty)\) and \(g\in[f^{\prime}_{-}(x),f^{\prime}_{+}(x)]\).

Let \(q:\mathcal{X}\rightarrow(0,\infty)\) be any feasible function satisfying (66) - (68). Recall that \(f^{\star}(x):=xf(1/x)\) is convex, and we can express \(q(x)f(p(x)/q(x))=p(x)f^{\star}(q(x)/p(x))\) whenever \(p(x)\neq 0\). Also, whenever \(p(x)\neq 0\), we bound \(f^{\star}(q(x)/p(x))\) by the linear approximation of \(f^{\star}\) at \(q^{\star}(x)/p(x)\) using (69), as follows:

\[f^{\star}\left(\frac{q(x)}{p(x)}\right)\geq f^{\star}\left(\frac {q^{\star}(x)}{p(x)}\right)+(f^{\star})^{\prime}_{+}\left(\frac{q^{\star}(x)}{ p(x)}\right)\left[\frac{q(x)}{p(x)}-\frac{q^{\star}(x)}{p(x)}\right].\] (70)

Hence, we have

\[q(x)f\left(\frac{p(x)}{q(x)}\right)=p(x)f^{\star}\left(\frac{q( x)}{p(x)}\right)\geq q(x)\zeta(x)+\xi(x),\] (71)

where

\[\zeta(x) =(f^{\star})^{\prime}_{+}\left(\frac{q^{\star}(x)}{p(x)}\right),\] (72) \[\xi(x) =p(x)f^{\star}\left(\frac{q^{\star}(x)}{p(x)}\right)-q^{\star}(x )(f^{\star})^{\prime}_{+}\left(\frac{q^{\star}(x)}{p(x)}\right).\] (73)Also, whenever \(p(x)=0\), we have \(q(x)f(p(x)/q(x))=q(x)p(0)\). Hence, we set \(\zeta(x)=p(0)\) and \(\xi(x)=0\) when \(p(x)=0\), so that

\[q(x)f\left(\frac{p(x)}{q(x)}\right)\geq q(x)\zeta(x)+\xi(x)\] (74)

holds for all \(x\in\mathcal{X}\). We note that \(\zeta(x)\) and \(\xi(x)\) does not depend on the choice of \(q(x)\). Also, the equality holds if \(q(x)=q^{*}(x)\).

Next, as an analogous to \(\nu\) in the above motivation for the proof, let \(\nu=(f^{\star})_{+}^{\prime}(1/r_{P})\). Since \(\int q(x)d\mu(x)=1\), we can write

\[\int q(x)f\left(\frac{p(x)}{q(x)}\right)d\mu(x) =\int q(x)\left(f\left(\frac{p(x)}{q(x)}\right)-\nu\right)d\mu(x)+\nu\] (75) \[\geq\int\left[q(x)(\zeta(x)-\nu)+\xi(x)\right]d\mu(x)+\nu.\] (76)

Now, we define the following sets which form a partition of \(\mathcal{X}\):

\[L =\left\{x\in\mathcal{X}:\frac{1}{r_{P}}p(x)<b\right\},\] (77) \[M =\left\{x\in\mathcal{X}:b\leq\frac{1}{r_{P}}p(x)\leq be^{\epsilon }\right\},\] (78) \[U =\left\{x\in\mathcal{X}:\frac{1}{r_{P}}p(x)>be^{\epsilon}\right\}.\] (79)

For each case of \(x\in L,M,U\), we have \(q^{*}(x)=b,\frac{1}{r_{P}}p(x),be^{\epsilon}\), respectively. We observe the following:

* If \(x\in M\), then \(\zeta(x)=\nu\) clearly.
* If \(x\in U\), then \(q^{*}(x)/p(x)<1/r_{P}\). Since \((f^{*})_{+}^{\prime}\) is monotone increasing, we have \(\zeta(x)\leq\nu\).
* If \(x\in L\) and \(p(x)\neq 0\), then \(q^{*}(x)/p(x)>1/r_{P}\). Again, since \((f^{*})_{+}^{\prime}\) is monotone increasing, we have \(\zeta(x)\geq\nu\).
* Finally, if \(p(x)=0\) (which implies \(x\in L\) also), then from the definition \(f^{\star}(t)=tf(1/t)\), we have \((f^{\star})_{+}^{\prime}(t)=f(1/t)-\frac{1}{t}f_{-}^{\prime}(1/t)\). From (69) with \(y=0\), we have \(\nu=(f^{\star})_{+}^{\prime}(1/r_{P})=f(r_{P})-r_{P}f_{-}^{\prime}(r_{P})\leq f (0)=\zeta(x)\). Hence, \(\zeta(x)\geq\nu\) for every \(x\in L\).

Therefore, since \(b\leq q(x)\leq be^{\epsilon}\), we can write

\[\int q(x)f\left(\frac{p(x)}{q(x)}\right)d\mu(x) \geq\int\left[q(x)(\zeta(x)-\nu)+\xi(x)\right]d\mu(x)+\nu\] (80) \[\geq\left[b(\zeta(x)-\nu)\mathbbm{1}_{L}(x)+be^{\epsilon}(\zeta( x)-\nu)\mathbbm{1}_{U}(x)+\xi(x)\right]d\mu(x)+\nu.\] (81)

The last expression does not depend on the choice of \(q(x)\). Also, we observe that all inequalities become equality if \(q(x)=q^{*}(x)\). This completes the proof for the optimality of \(q^{*}\), and hence completes the proof of the achievability part. \(\Box\)

### Proof of converse part (Proposition c.6)

Let \(\mathbf{Q}\in\mathcal{Q}_{\mathcal{X},\tilde{\mathcal{P}},\epsilon}\) be given. Let \(\mathcal{A}=\left\{A\subset\mathcal{X}:\mu(A)=\alpha\right\}\). For each \(A\in\mathcal{A}\), let \(p_{A}(x)=\begin{cases}c_{2}&\text{if }x\in A\\ c_{1}&\text{if }x\in\mathcal{X}\backslash A\end{cases}\). Since \(c_{2}\alpha+c_{1}(1-\alpha)=1\), we have \(\int p_{A}(x)d\mu(x)=1\). Hence for each \(A\in\mathcal{A}\), we can define a probability measure \(P_{A}\in\tilde{\mathcal{P}}\) by \(\frac{dP_{A}}{d\mu}=p_{A}\). Also, note that \(P_{A}(A)=c_{2}\alpha\).

For each \(A\in\mathcal{A}\), let \(\beta_{A}=\mathbf{Q}(A|P_{A})\). Then, the push-forward measures of \(P_{A}\) and \(\mathbf{Q}(P_{A})\) by the indicator function \(\mathbbm{1}_{A}\) are Bernoulli distributions with \(\Pr(1)=c_{2}\alpha\) and \(\beta_{A}\), respectively. By the data processing inequality (Theorem B.2), we have

\[D_{f}\left(P_{A}\|\mathbf{Q}(P_{A})\right)\geq D_{f}^{\mathrm{B}}\left(c_{2} \alpha\|\beta_{A}\right).\] (82)The main lemma to proceed is the following.

**Lemma C.9**.: _Let \(\mathcal{X}\) be a sample space. Let \(t,u\in\mathbb{N}\), \(t>u\). Let \(A_{1},A_{2},\cdots,A_{t}\subset\mathcal{X}\) be subsets such that for each \(x\in\mathcal{X}\), we have \(|\left\{i\in[t]:x\in A_{i}\right\}|\leq u\). Then for any \(t\) probability measures \(Q_{1},\cdots,Q_{t}\in\mathcal{P}(\mathcal{X})\) satisfying that \(Q_{i}(A)\leq e^{\epsilon}Q_{j}(A)\) for all \(i,j\in[t]\) and \(A\subset\mathcal{X}\), we have_

\[\min_{i\in[t]}Q_{i}(A_{i})\leq\frac{(u/t)e^{\epsilon}}{(u/t)e^{ \epsilon}+1-(u/t)}.\] (83)

Now, by the assumption that \(\mu\) is \(\alpha\)-decomposable, there exist sequences \(\left\{t_{j}\right\}_{j=1}^{\infty},\left\{u_{j}\right\}_{j=1}^{\infty}\subset \mathbb{N}\) of positive integers such that \(t_{j}>u_{j}\), \(\alpha\leq u_{j}/t_{j}\), \(\lim_{j\to\infty}u_{j}/t_{j}=\alpha\), and for each \(j\in\mathbb{N}\), there exist \(t_{j}\) subsets \(A_{j,1},A_{j,2},\cdots,A_{j,t_{j}}\subset\mathcal{X}\) such that \(\mu(A_{j,i})=\alpha\) for all \(i\in[t_{j}]\), and every \(x\in\mathcal{X}\), we have \(\left|\left\{i\in[t_{j}]:x\in A_{j,t_{j}}\right\}\right|\leq u_{j}\). By applying Lemma C.9 to \(Q_{i}=\mathbf{Q}(P_{A_{j,i}})\), we obtain \(\min_{i\in[t_{j}]}\beta_{A_{i,j}}\leq\frac{(u_{j}/t_{j})e^{\epsilon}}{(u_{j}/ t_{j})e^{\epsilon}+1-(u_{j}/t_{j})}\). This implies that \(\inf_{A\in\mathcal{A}}\beta_{A}\leq\frac{(u_{j}/t_{j})e^{\epsilon}}{(u_{j}/t_{ j})e^{\epsilon}+1-(u_{j}/t_{j})}\) for all \(j\in\mathbb{N}\), and by taking the limit \(j\to\infty\), we have \(\inf_{A\in\mathcal{A}}\beta_{A}\leq\frac{\alpha e^{\epsilon}}{\alpha e^{ \epsilon}+1-\alpha}=be^{\epsilon}\alpha\).

Since \(be^{\epsilon}<c_{2}\), we have \(be^{\epsilon}\alpha<c_{2}\alpha\). By continuity of \(D_{f}^{\mathrm{B}}\) and the fact that \(D_{f}^{\mathrm{B}}\left(\lambda_{1}\|\lambda_{2}\right)\) is decreasing in \(\lambda_{2}\in[0,\lambda_{1}]\), we have

\[\sup_{A\in\mathcal{A}}D_{f}\left(P_{A}\|\mathbf{Q}(P_{A})\right) \geq\sup_{A\in\mathcal{A}}D_{f}^{\mathrm{B}}\left(c_{2}\alpha\|\beta_{A} \right)\geq D_{f}^{\mathrm{B}}\left(c_{2}\alpha\|be^{\epsilon}\alpha\right).\] (84)

It follows that

\[R_{f}(\mathbf{Q})=\sup_{P\in\tilde{\mathcal{P}}}D_{f}\left(P\| \mathbf{Q}(P)\right)\geq\sup_{A\in\mathcal{A}}D_{f}\left(P_{A}\|\mathbf{Q}(P_{ A})\right)\geq D_{f}^{\mathrm{B}}\left(c_{2}\alpha\|be^{\epsilon}\alpha\right).\] (85)

Furthermore, we have

\[D_{f}^{\mathrm{B}}\left(c_{2}\alpha\|be^{\epsilon}\alpha\right) =be^{\epsilon}\alpha f\left(\frac{c_{2}\alpha}{be^{\epsilon} \alpha}\right)+(1-b\alpha e^{\epsilon})f\left(\frac{1-c_{2}\alpha}{1-be^{ \epsilon}\alpha}\right)\] (86) \[=be^{\epsilon}\alpha f\left(\frac{c_{2}\alpha}{be^{\epsilon} \alpha}\right)+b(1-\alpha)f\left(\frac{c_{1}(1-\alpha)}{b(1-\alpha)}\right)\] (87) \[=be^{\epsilon}\alpha f(r_{2})+b(1-\alpha)f(r_{1}),\] (88)

and we can derive \(be^{\epsilon}\alpha=\frac{1-r_{1}}{r_{2}-r_{1}}\) and \(b(1-\alpha)=\frac{r_{2}-1}{r_{2}-r_{1}}\), as follows. From (30), (31) and the definition of \(r_{1},r_{2}\), we have

\[1-r_{1} =(1-c_{1})\frac{c_{2}-c_{1}e^{\epsilon}}{c_{2}-c_{1}},\] (89) \[r_{2}-1 =\frac{c_{2}-1}{e^{\epsilon}}\frac{c_{2}-e^{\epsilon}c_{1}}{c_{2} -c_{1}}.\] (90)

From this, we have

\[\frac{1-r_{1}}{r_{2}-r_{1}} =\frac{1-r_{1}}{(r_{2}-1)+(1-r_{1})}\] (91) \[=\frac{(1-c_{1})e^{\epsilon}}{(1-c_{1})e^{\epsilon}+(c_{2}-1)}\] (92) \[=\frac{(1-c_{1})e^{\epsilon}}{(e^{\epsilon}-1)(1-c_{1})+c_{2}-c_ {1}}\] (93) \[=b\times e^{\epsilon}\frac{1-c_{1}}{c_{2}-c_{1}}\] (94) \[=be^{\epsilon}\alpha.\] (95)

Also, from above and \(1=be^{\epsilon}\alpha+b(1-\alpha)\), we have

\[\frac{r_{2}-1}{r_{2}-r_{1}}=1-\frac{1-r_{1}}{r_{2}-r_{1}}=1-be^{\epsilon} \alpha=b(1-\alpha).\] (96)

This ends the proof of the converse part. \(\qed\)

### Deduction to main theorems

In this subsection, we show that Theorem C.4 contains main theorems, Theorems 3.1 and 3.3, as special cases.

#### c.4.1 Deduction to Theorem 3.1

Recall that in this setup, \(\mathcal{X}=[k]\), \(\tilde{\mathcal{P}}=\mathcal{P}([k])=\tilde{\mathcal{P}}_{0,k,\mu_{k}}\), where \(\mu_{k}\) is the uniform distribution on \([k]\). That is, \((c_{1},c_{2})=(0,k)\). The values of the constants \(\alpha,b,r_{1},r_{2}\) are

\[\alpha =1/k,\] (97) \[b =\frac{k}{e^{\epsilon}+k-1},\] (98) \[r_{1} =0,\] (99) \[r_{2} =\frac{e^{\epsilon}+k-1}{e^{\epsilon}}.\] (100)

We can easily observe that \(\mu_{k}\) is \((\alpha,k,1)\)-decomposable, because the sets \(\{i\}\), \(i\in[k]\), are disjoint and \(\mu_{k}(\{i\})=\frac{1}{k}\). It follows that \(\mu_{k}\) is \(\alpha\)-decomposable. Hence, we can apply Theorem C.4. By a direct calculation of \(\mathcal{R}(\mathcal{X},\tilde{\mathcal{P}},\epsilon,f)=\frac{1-r_{1}}{r_{2 }-r_{1}}f(r_{2})+\frac{r_{2}-1}{r_{2}-r_{1}}f(r_{1})\), we can derive the formula of \(\mathcal{R}([k],\mathcal{P}([k]),\epsilon,f)\) presented in Theorem 3.1. It remains to show that the mechanism \(\mathbf{Q}^{*}_{k,\epsilon}\) presented in Theorem 3.1 is the same as \(\mathbf{Q}^{*}_{0,k,\mu_{k},\epsilon}\), and show the claim about the range of \(r_{P}\). Recall that

\[\mathbf{Q}^{*}_{k,\epsilon}(x|P)=\max\left(\frac{1}{r_{P}}P(x), \frac{1}{e^{\epsilon}+k-1}\right)\quad\forall x\in[k],P\in\mathcal{P}([k]),\] (101)

and we claim that \(r_{P}\) can be chosen such that \(1\leq r_{P}\leq(e^{\epsilon}+k-1)/e^{\epsilon}\).

First, as explained in Section 3.1, we can alternatively write

\[\mathbf{Q}^{*}_{k,\epsilon}(x|P)=\mathrm{clip}\left(\frac{1}{r_{ P}}P(x);\frac{1}{e^{\epsilon}+k-1},\frac{e^{\epsilon}}{e^{\epsilon}+k-1} \right).\] (102)

Since \(P(x)=\frac{1}{k}\frac{dP}{d\mu_{k}}(x)\) for each \(x\in[k]\) and \(P\in\mathcal{P}([k])\), (102) can be written as

\[\frac{1}{k}\frac{d\mathbf{Q}^{*}_{k,\epsilon}(P)}{d\mu_{k}}(x) =\mathrm{clip}\left(\frac{1}{r_{P}}\frac{1}{k}\frac{dP}{d\mu_{k}} (x);\frac{1}{e^{\epsilon}+k-1},\frac{e^{\epsilon}}{e^{\epsilon}+k-1}\right)\] (103) \[=\frac{1}{k}\mathrm{clip}\left(\frac{1}{r_{P}}\frac{dP}{d\mu_{k}} (x);\frac{k}{e^{\epsilon}+k-1},\frac{ke^{\epsilon}}{e^{\epsilon}+k-1}\right),\] (104)

hence,

\[\frac{d\mathbf{Q}^{*}_{k,\epsilon}(P)}{d\mu_{k}}(x) =\mathrm{clip}\left(\frac{1}{r_{P}}\frac{dP}{d\mu_{k}}(x);\frac{ k}{e^{\epsilon}+k-1},\frac{ke^{\epsilon}}{e^{\epsilon}+k-1}\right)\] (105) \[=\mathrm{clip}\left(\frac{1}{r_{P}}\frac{dP}{d\mu_{k}}(x);b,be^{ \epsilon}\right).\] (106)

Hence, \(\mathbf{Q}^{*}_{k,\epsilon}=\mathbf{Q}^{*}_{0,k,\mu_{k},\epsilon}\).

Second, to prove the claim about the range of \(r_{P}\), let us fix \(P\in\mathcal{P}([k])\). We need to show that

\[\sum_{x=1}^{k}\max\left(P(x),\frac{1}{e^{\epsilon}+k-1}\right) \geq 1\geq\sum_{x=1}^{k}\max\left(\frac{e^{\epsilon}}{e^{\epsilon}+k-1}P(x),\frac{1}{e^{\epsilon}+k-1}\right).\] (107)

The left inequality can be easily derived by

\[\sum_{x=1}^{k}\max\left(P(x),\frac{1}{e^{\epsilon}+k-1}\right) \geq\sum_{x=1}^{k}P(x)=1.\] (108)For the right inequality, we recall that \(b=\frac{k}{e^{\epsilon}+k-1}\). Since \(P(x)\leq 1\), we have \(\frac{e^{\epsilon}}{e^{\epsilon}+k-1}P(x)\leq\frac{e^{\epsilon}}{e^{\epsilon}+k -1}\). Hence, again by using \(P(x)=\frac{1}{k}\frac{dP}{d\mu_{k}}(x)\), we have

\[\max\left(\frac{e^{\epsilon}}{e^{\epsilon}+k-1}P(x),\frac{1}{e^{ \epsilon}+k-1}\right)\] (109) \[= \mathrm{clip}\left(\frac{e^{\epsilon}}{e^{\epsilon}+k-1}P(x); \frac{1}{e^{\epsilon}+k-1},\frac{e^{\epsilon}}{e^{\epsilon}+k-1}\right)\] (110) \[= \mathrm{clip}\left(\frac{e^{\epsilon}}{e^{\epsilon}+k-1}\frac{1} {k}\frac{dP}{d\mu_{k}}(x);\frac{1}{e^{\epsilon}+k-1},\frac{e^{\epsilon}}{e^{ \epsilon}+k-1}\right)\] (111) \[=\frac{1}{k}\mathrm{clip}\left(\frac{1}{r_{2}}\frac{dP}{d\mu_{k} }(x);b,be^{\epsilon}\right),\] (112)

and thus

\[\sum_{x=1}^{k}\max\left(\frac{e^{\epsilon}}{e^{\epsilon}+k-1}P(x),\frac{1}{e^{\epsilon}+k-1}\right)\] (113) \[=\sum_{x=1}^{k}\frac{1}{k}\mathrm{clip}\left(\frac{1}{r_{2}} \frac{dP}{d\mu_{k}}(x);b,be^{\epsilon}\right)\] (114) \[=\int\mathrm{clip}\left(\frac{1}{r_{2}}\frac{dP}{d\mu_{k}}(x);b,be ^{\epsilon}\right)d\mu_{k}(x).\] (115)

The desired inequality follows from Proposition C.2.

#### c.4.2 Deduction to Theorem 3.3

Recall that in this setup, \(\mathcal{X}=\mathbb{R}^{n}\), \(\tilde{\mathcal{P}}=\tilde{\mathcal{P}}_{c_{1},c_{2},h}=\tilde{\mathcal{P}}_{ c_{1},c_{2},\mu}\), where \(\mu\ll m\) and \(d\mu/dm=h\). Note that the normalization condition (10) about \((c_{1},c_{2},h,\epsilon)\) implies the normalization condition (22) about \((c_{1},c_{2},\mu,\epsilon)\). It can be directly observed that \(\mathbf{Q}_{c_{1},c_{2},h,\epsilon}^{*}=\mathbf{Q}_{c_{1},c_{2},\mu,\epsilon}^ {*}\), because for each \(P\in\tilde{\mathcal{P}}\) with corresponding pdf \(p(x)\), the chain rule of the Radon-Nikodym derivative shows that \(p(x)=\frac{dP}{dm}=\frac{dP}{d\mu}(x)\frac{d\mu}{dm}(x)=\frac{dP}{d\mu}(x)h(x)\). Hence, once we show that \(\mu\) is \(\alpha\)-decomposable, Theorem C.4 and Proposition C.2 directly contain Theorem 3.3 as a special case. Thus, it remains to show \(\mu\) is \(\alpha\)-decomposable.

In fact, we prove a stronger statement that: \(\mu\) is \((\alpha,t,u)\)-decomposable for any \(t,u\in\mathbb{N}\) such that \(t>u\) and \(\alpha\leq u/t\). Then, since the set of rational numbers is dense in \(\mathbb{R}\), this also implies that \(\mu\) is \(\alpha\)-decomposable.

To prove this, let us first introduce the following lemma.

**Lemma C.10**.: _Let \(\alpha\in(0,1)\) and \(t,u\in\mathbb{N}\), \(t>u\), \(\alpha\leq u/t\). If \(\mu\in\mathcal{P}(\mathcal{X})\) is \((\alpha/u,t,1)\)-decomposable, then \(\mu\) is also \((\alpha,t,u)\)-decomposable._

Proof.: In this proof, assume that the sum and subtraction operations performed in subscripts are modulo \(t\) operations, with the identification that \(0=t\).

By \((\alpha/u,t,1)\)-decomposability, there are \(t\) disjoint subsets \(B_{1},B_{2},\cdots,B_{t}\) such that \(\mu(B_{i})=\alpha/u\) for each \(i\in[t]\). Using this, for each \(i\in[t]\), define \(A_{i}\) as \(A_{i}=\cup_{j=0}^{u-1}B_{i+j}\). As \(B_{i}\)'s are disjoint, we have \(\mu(A_{i})=\sum_{j=0}^{u-1}\mu(B_{i+j})=u\times(\alpha/u)=\alpha\) for all \(i\in[t]\). Also, for each \(x\in B_{i}\), \(x\) is contained in exactly \(u\) sets among \(A_{1},\cdots,A_{t}\), which are \(A_{i},A_{i-1},\cdots,A_{i-u+1}\). Furthermore, if \(x\notin B_{i}\) for all \(i\in[t]\), then \(x\) is contained in none of \(A_{i}\). Hence \(|\left\{i\in[t]:x\in A_{i}\right\}|\leq u\) for all \(x\in\mathcal{X}\). Thus \(\mu\) is \((\alpha,t,u)\)-decomposable. 

By this lemma, it suffices to show that for any \(t\in\mathbb{N}\) such that \(t\geq 2\) and \(\alpha\leq 1/t\), \(\mu\) is \((\alpha,t,1)\)-decomposable. As \(\mu\ll m\), the map \(s\in\mathbb{R}\mapsto\mu((-\infty,s]\times\mathbb{R}^{n-1})\) is continuous, and as \(s\rightarrow-\infty\) and \(\infty\), we have \(\mu((-\infty,s]\to 0\) and \(1\), respectively. Hence by the intermediate value theorem, for each \(i\in[t]\), there exists \(s_{i}\in\mathbb{R}\) such that \(\mu((-\infty,s]\times\mathbb{R}^{n-1})=\alpha i\). Then, setting \(A_{1}=(-\infty,s_{1}]\times\mathbb{R}^{n-1}\) and \(A_{i}=(s_{i-1},s_{i}]\times\mathbb{R}^{n-1}\) for \(i\geq 2\) gives the desired \(A_{i}\)'s in the definition of decomposability.

In conclusion, \(\mu\) is \(\alpha\)-decomposable, and hence Theorem 3.3 can be deduced from Theorem C.4.

### Proof of Proposition c.2

Let \(P\in\tilde{\mathcal{P}}\) be given. Let \(p=\frac{dP}{d\mu}\), and again assume that \(c_{1}\leq p(x)\leq c_{2}\) for _all_\(x\in\mathcal{X}\). Similar to the proof of the achievability part, for each \(r>0\), let us define the following sets which form a partition of \(\mathcal{X}\):

\[L_{r} =\left\{x\in\mathcal{X}:\frac{1}{r}p(x)<b\right\},\] (116) \[M_{r} =\left\{x\in\mathcal{X}:b\leq\frac{1}{r}p(x)\leq be^{\epsilon} \right\},\] (117) \[U_{r} =\left\{x\in\mathcal{X}:\frac{1}{r}p(x)>be^{\epsilon}\right\}.\] (118)

For \(r=0\), we let \(L_{0}=\left\{x\in\mathcal{X}:p(x)=0\right\}\), \(U_{0}=\left\{x\in\mathcal{X}:p(x)>0\right\}\), \(M_{0}=\emptyset\). Also, let \(q_{r}(x)=\text{clip}\left(\frac{1}{r}p(x);b,be^{\epsilon}\right)\). Then, \(q_{r}(x)=b\), \(\frac{1}{r}p(x),be^{\epsilon}\) for \(x\in L_{r},M_{r},U_{r}\), respectively.

First, we show that \(\int q_{r_{1}}(x)d\mu(x)\geq 1\). We divide the case of \(c_{1}=0\) and \(c_{1}>0\).

Suppose first that \(c_{1}=0\). Then \(r_{1}=0,\alpha=\frac{1}{c_{2}}\), and \(b=\frac{c_{2}}{e^{\epsilon}+c_{2}-1}\). Observe that

\[1=\int p(x)d\mu(x)=\int_{U_{0}}p(x)d\mu(x)\leq c_{2}\mu(U_{0}),\] (119)

hence \(\mu(U_{0})\geq 1/c_{2}\). It follows that

\[\int q_{r_{1}}(x)d\mu(x) =b\mu(L_{0})+be^{\epsilon}\mu(U_{0})\] (120) \[=b(1-\mu(U_{0}))+be^{\epsilon}\mu(U_{0})\] (121) \[=b(e^{\epsilon}-1)\mu(U_{0})+b\] (122) \[\geq\frac{b(e^{\epsilon}-1)}{c_{2}}+b\] (123) \[=b\times\frac{e^{\epsilon}+c_{2}-1}{c_{2}}=1.\] (124)

Next, suppose that \(c_{1}>0\). Then \(r_{1}>0\). From \(p(x)\geq c_{1}\), we have \(\frac{1}{r_{1}}p(x)\geq\frac{c_{1}}{r_{1}}=b\). Hence \(L_{r_{1}}=\emptyset\). Thus

\[\int q_{r_{1}}(x)d\mu(x) =\frac{1}{r_{1}}\int_{M_{r_{1}}}p(x)d\mu(x)+be^{\epsilon}\mu(U_{ r_{1}})\] (125) \[=\frac{1}{r_{1}}\left(\int_{M_{r_{1}}}p(x)d\mu(x)+c_{1}e^{ \epsilon}\mu(U_{r_{1}})\right).\] (126)

Let \(S_{1}=\int_{M_{r_{1}}}p(x)d\mu(x)\) and \(T_{1}=\mu(U_{r_{1}})\), so that

\[\int q_{r_{1}}(x)d\mu(x)=\frac{1}{r_{1}}(S_{1}+c_{1}e^{\epsilon}T_{1}).\] (127)

As \(c_{1}\leq p(x)\leq c_{2}\), we have

\[S_{1}=\int_{M_{r_{1}}}p(x)d\mu(x)\geq c_{1}\mu(M_{r_{1}})=c_{1}(1-\mu(U_{r_{1} }))=c_{1}(1-T_{1}),\] (128)

and

\[1-S_{1}=1-\int_{M_{r_{1}}}p(x)d\mu(x)=\int_{U_{r_{1}}}p(x)d\mu(x)\leq c_{2} \mu(U_{r_{1}})=c_{2}T_{1}.\] (129)

From these, we can get

\[S_{1}+c_{1}T_{1} \geq c_{1},\] (130) \[S_{1}+c_{2}T_{1} \geq 1.\] (131)As \(c_{1}<c_{1}e^{\epsilon}<c_{2}\), we can express \(c_{1}e^{\epsilon}\) as a convex combination of \(c_{1}\) and \(c_{2}\), as

\[c_{1}e^{\epsilon}=\frac{c_{2}-c_{1}e^{\epsilon}}{c_{2}-c_{1}}c_{1}+\frac{c_{1}e ^{\epsilon}-c_{1}}{c_{2}-c_{1}}c_{2}.\] (132)

Hence, by taking \(\frac{c_{2}-c_{1}e^{\epsilon}}{c_{2}-c_{1}}[\text{equation (\ref{eq:130})}]+\frac{c_{1}e^{ \epsilon}-c_{1}}{c_{2}-c_{1}}[\text{equation (\ref{eq:131})}]\), we get

\[S+c_{1}e^{\epsilon}T \geq\frac{c_{2}-c_{1}e^{\epsilon}}{c_{2}-c_{1}}c_{1}+\frac{c_{1}e ^{\epsilon}-c_{1}}{c_{2}-c_{1}}\] (133) \[=\frac{c_{2}-c_{1}e^{\epsilon}+e^{\epsilon}-1}{c_{2}-c_{1}}c_{1}\] (134) \[=\frac{(e^{\epsilon}-1)(1-c_{1})+c_{2}-c_{1}}{c_{2}-c_{1}}c_{1}\] (135) \[=r_{1}.\] (136)

Thus we have \(\int q_{r_{1}}(x)d\mu(x)\geq 1\).

Similarly, we show that \(\int q_{r_{2}}(x)d\mu(x)\leq 1\). from \(p(x)\leq c_{2}\), we have \(\frac{1}{r_{2}}p(x)\leq\frac{c_{2}}{r_{2}}=be^{\epsilon}\). Hence \(U_{r_{2}}=\emptyset\). Thus

\[\int q_{r_{2}}(x)d\mu(x) =b\mu(L_{r_{2}})+\frac{1}{r_{2}}\int_{M_{r_{2}}}p(x)d\mu(x)\] (137) \[=\frac{1}{r_{2}}\left(c_{2}e^{-\epsilon}\mu(L_{r_{2}})+\int_{M_{r _{2}}}p(x)d\mu(x)\right).\] (138)

Similar as above, let \(S_{2}=\mu(L_{r_{2}})\) and \(T_{2}=\int_{M_{r_{2}}}p(x)d\mu(x)\), so that

\[\int q_{r_{2}}(x)d\mu(x)=\frac{1}{r_{2}}(c_{2}e^{-\epsilon}S_{2}+T_{2}),\] (139)

and, we have

\[T_{2}=\int_{M_{r_{2}}}p(x)d\mu(x)\leq c_{2}\mu(M_{r_{2}})=c_{2}(1-\mu(L_{r_{2} }))=c_{2}(1-S_{2})\] (140)

and

\[1-T_{2}=1-\int_{M_{r_{2}}}p(x)d\mu(x)=\int_{L_{r_{2}}}p(x)d\mu(x)\geq c_{1}\mu (L_{r_{2}})=c_{1}S_{2}.\] (141)

hence

\[c_{2}S_{2}+T_{2} \leq c_{2},\] (142) \[c_{1}S_{2}+T_{2} \leq 1.\] (143)

As \(c_{1}\leq c_{2}e^{-\epsilon}\leq c_{2}\), we have

\[c_{2}e^{-\epsilon}=\frac{c_{2}-c_{2}e^{-\epsilon}}{c_{2}-c_{1}}c_{1}+\frac{c_ {2}e^{-\epsilon}-c_{1}}{c_{2}-c_{1}}c_{2}\] (144)

and by the same reason, we have

\[c_{2}e^{-\epsilon}S_{2}+T_{2} \leq\frac{c_{2}-c_{2}e^{-\epsilon}}{c_{2}-c_{1}}+\frac{c_{2}e^{ -\epsilon}-c_{1}}{c_{2}-c_{1}}c_{2}\] (145) \[=\frac{1-e^{-\epsilon}+c_{2}e^{-\epsilon}-c_{1}}{c_{2}-c_{1}}c_{2}\] (146) \[=\frac{(e^{\epsilon}-1)(1-c_{1})+c_{2}-c_{1}}{c_{2}-c_{1}}c_{2}e^ {-\epsilon}\] (147) \[=r_{2}.\] (148)

Thus we have \(\int q_{r_{2}}(x)d\mu(x)\leq 1\).

Finally, we show that \(\int q_{r_{1}}(x)d\mu(x)=1\) implies \(\int q_{r}(x)d\mu(x)=1\) for all \(r\in[r_{1},r_{2}]\). Let us assume \(\int q_{r_{1}}(x)d\mu(x)=1\). We first claim that: \(p(x)=c_{2}\) for \(\mu\)-a.e. \(x\in U_{r_{1}}\), and \(p(x)=c_{1}\) for \(\mu\)-a.e. \(x\in\mathcal{X}\backslash U_{r_{1}}\). Again, we divide the case of \(c_{1}=0\) and \(c_{1}>0\).

Suppose first that \(c_{1}=0\). By tracking the proof of \(\int q_{r_{1}}(x)d\mu(x)\geq 1\), We can observe that the equality \(\int q_{r_{1}}(x)d\mu(x)=1\) holds if and only if \(\mu(U_{0})=1/c_{2}\), if and only if \(p(x)=c_{2}\) for \(\mu\)-a.e. \(x\in U_{0}\). By definition of \(U_{0}\), we have \(p(x)=0=c_{1}\) for all \(x\in\mathcal{X}\backslash U_{0}\). Hence we get the claim.

Next, suppose that \(c_{1}>0\). Again, by tracking the proof of \(\int q_{r_{1}}(x)d\mu(x)\geq 1\), We can observe that the equality \(\int q_{r_{1}}(x)d\mu(x)=1\) is equivalent to any of the following statements:

* Both \(S_{1}+c_{1}T_{1}=c_{1}\) and \(S_{1}+c_{2}T_{1}=1\) holds.
* Both \(\int_{M_{r_{1}}}p(x)d\mu(x)=c_{1}\mu(M_{r_{1}})\) and \(\int_{U_{r_{1}}}p(x)d\mu(x)=c_{2}\mu(U_{r_{1}})\) holds.
* \(p(x)=c_{1}\) for \(\mu\)-a.e. \(x\in M_{r_{1}}\) and \(p(x)=c_{2}\) for \(\mu\)-a.e. \(x\in U_{r_{1}}\).

Since \(L_{r_{1}}=\emptyset\), we also get the claim.

WLOG, assume that \(p(x)=c_{2}\) for _all_\(x\in U_{r_{1}}\), and \(p(x)=c_{1}\) for _all_\(x\in\mathcal{X}\backslash U_{r_{1}}\). Then, for every \(r\in(r_{1},r_{2}]\), we have the following:

* For each \(x\in U_{r_{1}}\), we have \(\frac{1}{r}p(x)\geq\frac{c_{2}}{r_{2}}=be^{\epsilon}\), hence \(q_{r}(x)=be^{\epsilon}\).
* For each \(x\in\mathcal{X}\backslash U_{r_{1}}\),
* If \(c_{1}=0\), then \(p(x)=0\), \(q_{r}(x)=b\), and
* If \(c_{1}>0\), then \(r_{1}>0\), \(\frac{1}{r}p(x)<\frac{c_{1}}{r_{1}}=b\), hence again \(q_{r}(x)=b\).

Also, we have \(1=\int p(x)d\mu(x)=c_{2}\mu(U_{r_{1}})+c_{1}(1-\mu(U_{r_{1}}))\), hence \(\mu(U_{r_{1}})=\frac{1-c_{1}}{c_{2}-c_{1}}=\alpha\). It follows that for every \(r\in(r_{1},r_{2}]\), we have \(\int q_{r}(x)d\mu(x)=be^{\epsilon}\mu(U_{r_{1}})+b(1-\mu(U_{r_{1}}))=be^{ \epsilon}\alpha+b(1-\alpha)=1\). This concludes the proof. \(\qed\)

### Proof of Lemma c.7

Although the proof can be found in [59], we present the proof here for the completeness.

If \(r_{1}=0\) and \(f(0)=\infty\), then the RHS of the inequality we want to show is \(\infty\), thus it becomes trivial. Hence, we may assume that either \(r_{1}>0\) or \(f(0)<\infty\).

By the assumption, \(p(x)/q(x)\) is the convex combination of \(r_{1}\) and \(r_{2}\) for \(\mu\)-a.e. \(x\in\mathcal{X}\), as follows.

\[p(x)/q(x)=\frac{(p(x)/q(x))-r_{1}}{r_{2}-r_{1}}r_{2}+\frac{r_{2}-(p(x)/q(x))}{r_ {2}-r_{1}}r_{1}.\] (149)

Hence, by the convexity of \(f\) and \(\int p(x)d\mu(x)=\int q(x)d\mu(x)=1\), we have

\[D_{f}\left(P\|Q\right) =\int q(x)f(p(x)/q(x))d\mu(x)\] (150) \[\leq\int q(x)\left(\frac{(p(x)/q(x))-r_{1}}{r_{2}-r_{1}}f(r_{2}) +\frac{r_{2}-(p(x)/q(x))}{r_{2}-r_{1}}f(r_{1})\right)d\mu(x)\] (151) \[=\int\left(\frac{p(x)-r_{1}q(x)}{r_{2}-r_{1}}f(r_{2})+\frac{r_{2} q(x)-p(x)}{r_{2}-r_{1}}f(r_{1})\right)d\mu(x)\] (152) \[=\frac{1-r_{1}}{r_{2}-r_{1}}f(r_{2})+\frac{r_{2}-1}{r_{2}-r_{1}} f(r_{1}).\] (153)

\(\qed\)

### Proof of Lemma c.9

First, we claim that for each \(i\in[t-u]\), we can construct a partition \(\{B_{i,1},B_{i,2},\cdots,B_{i,u}\}\) of \(A_{u+i}\) into \(u\) (measurable) sets, such that for each \(j\in[u]\), the sets \(A_{j},B_{1,j},B_{2,j},\cdots,B_{t-u,j}\) are disjoint. The construction is in the inductive way as follows: Given \(i\in[t-u]\), suppose that we have constructedsuch partitions of \(A_{u+1},\cdots,A_{u+i-1}\) such that for each \(j\in[u]\), \(A_{j},B_{1,j},B_{2,j},\cdots,B_{i-1,j}\) are disjoint. Let \(C_{i,j}=A_{j}\cup\left(\cup_{k=1}^{i-1}B_{k,j}\right)\). Then for each \(x\in A_{u+i}\), at least one of \(C_{i,1},C_{i,2},\cdots,C_{i,u}\) does not contain \(x\), because

\[\sum_{j=1}^{u}\mathbbm{1}_{\mathcal{X}\setminus C_{i,j}}(x)=\sum_{j=1}^{u} \left(1-\mathbbm{1}_{C_{i,j}}(x)\right)=u-\sum_{j=1}^{u}\mathbbm{1}_{C_{i,j}}(x)\] (154)

\[=u-\sum_{j=1}^{u}\left(\mathbbm{1}_{A_{j}}(x)+\sum_{k=1}^{i-1} \mathbbm{1}_{B_{k,j}}(x)\right)=u-\sum_{j=1}^{u}\mathbbm{1}_{A_{j}}(x)-\sum_{ j=1}^{u}\sum_{k=1}^{i-1}\mathbbm{1}_{B_{k,j}}(x)\] (155) \[=u-\sum_{j=1}^{u}\mathbbm{1}_{A_{j}}(x)-\sum_{k=1}^{i-1}\sum_{j=1 }^{u}\mathbbm{1}_{B_{k,j}}(x)=u-\sum_{j=1}^{u}\mathbbm{1}_{A_{j}}(x)-\sum_{k= 1}^{i-1}\mathbbm{1}_{A_{u+k}}(x)\] (156) \[\geq u-(u-1)=1,\] (157)

where the last line is from that since \(x\in A_{u+i}\), at most \(u-1\) sets among \(A_{1},A_{2},\cdots,A_{u+i-1}\) can contain \(x\). Hence, setting

\[B_{i,j}=\left\{x\in A_{u+i}:j=\min\left(\tilde{j}\in[u]:x\notin C_{i,\tilde{j }}\right)\right\}\] (158)

gives the partition \(\{B_{i,1},B_{i,2},\cdots,B_{i,u}\}\) of \(A_{u+i}\), such that for each \(j\in[u]\), \(C_{i,j}\) and \(B_{i,j}\) are disjoint. This implies that \(A_{j},B_{1,j},B_{2,j},\cdots,B_{i,j}\) are disjoint.

Now, let \(\ell=\min_{i\in[t]}Q_{i}(A_{i})\). Using these partitions, we can now show that

\[u =\sum_{j=1}^{u}Q_{j}(\mathcal{X})\geq\sum_{j=1}^{u}Q_{j}\left(A_{ j}\cup\left(\bigcup_{k=1}^{t-u}B_{k,j}\right)\right)\] (159) \[=\sum_{j=1}^{u}Q_{j}(A_{j})+\sum_{j=1}^{u}\sum_{k=1}^{t-u}Q_{j}(B _{k,j})=\sum_{j=1}^{u}Q_{j}(A_{j})+\sum_{k=1}^{t-u}\sum_{j=1}^{u}Q_{j}(B_{k,j})\] (160) \[=\sum_{j=1}^{u}Q_{j}(A_{j})+\sum_{k=1}^{t-u}Q_{j}(A_{u+k})\] (161) \[\geq\sum_{j=1}^{u}Q_{j}(A_{j})+\sum_{k=1}^{t-u}e^{-\epsilon}Q_{u +k}(A_{u+k})\] (162) \[\geq\ell\left(u+e^{-\epsilon}(t-u)\right).\] (163)

Hence

\[\ell\leq\frac{u}{u+e^{-\epsilon}(t-u)}=\frac{(u/t)e^{\epsilon}}{(u/t)e^{ \epsilon}+1-(u/t)}.\] (164)

\(\square\)

## Appendix D Proofs of Remaining Propositions

We present the proofs of remaining propositions in the paper, Propositions 3.2 and 4.1.

### Proof of Proposition 3.2

By the assumption, there are subsets \(\left\{A_{i}\right\}_{i=1}^{\infty}\) of \(\mathcal{X}\) which are pairwise disjoint and \(P_{i}(A_{i})=1\) for all \(i\in\mathbb{N}\). Let us pick \(P_{0}\in\tilde{\mathcal{P}}\), and let \(Q_{0}=\mathbf{Q}(P_{0})\). Since \(A_{i}\)'s are disjoint, we have \(\sum_{i=1}^{\infty}Q_{0}(A_{i})=Q_{0}\left(\bigcup_{i=1}^{\infty}A_{i}\right) \leq 1<\infty\). Hence, we have \(\lim_{i\to\infty}Q_{0}(A_{i})=0\). Also, by definition of \(\epsilon\)-LDP, for any \(P\in\tilde{\mathcal{P}}\) and \(i\in\mathbb{N}\), we have \(\mathbf{Q}(P)(A_{i})\leq e^{\epsilon}Q_{0}(A_{i})\). Especially, this implies \(\mathbf{Q}(P_{i})(A_{i})\leq e^{\epsilon}Q_{0}(A_{i})\), and thus \(\lim_{i\to\infty}\mathbf{Q}(P_{i})(A_{i})=0\).

Now, similar to the converse proof in Section C.3, let \(\beta_{i}=\mathbf{Q}(P_{i})(A_{i})\). Then, the push-forward measures of \(P_{i}\) and \(\mathbf{Q}(P_{i})\) by the indicator function \(1_{A}\) are Bernoulli distributions with \(\Pr(1)=1\) and \(\beta_{i}\), respectively. By the data processing inequality (Theorem B.2), we have

\[D_{f}\left(P_{i}\|\mathbf{Q}(P_{i})\right)\geq D_{f}^{\mathrm{B}}\left(1\| \beta_{i}\right).\] (165)

Since \(\lim_{i\to\infty}\beta_{i}=0\), by continuity of \(D_{f}^{\mathrm{B}}\), we have

\[R_{f}(\mathbf{Q})\geq\limsup_{i\to\infty}D_{f}\left(P_{i}\|\mathbf{Q}(P_{i}) \right)\geq\lim_{i\to\infty}D_{f}^{\mathrm{B}}\left(1\|\beta_{i}\right)=D_{f}^ {\mathrm{B}}\left(1\|0\right)=M_{f},\] (166)

where the last equality is because two Bernoulli distributions with \(\Pr(1)=1\) and \(\Pr(1)=0\), respectively, are mutually singular. Hence, we must have \(R_{f}(\mathbf{Q})=M_{f}\). \(\qed\)

### Proof of Proposition 4.1

For generality, we prove that the statement of Proposition 4.1 holds in the general setup in Definition C.1. That is, for the setup in Definition C.1, the mechansim \(\mathbf{Q}^{*}=\mathbf{Q}_{c_{1},c_{2},\mu,\epsilon}^{*}\) satisfies

\[D_{\mathrm{TV}}\left(\mathbf{Q}^{*}(P),\mathbf{Q}^{*}(P^{\prime})\right)\leq \frac{2}{\max(r_{P},r_{P^{\prime}})}D_{\mathrm{TV}}\left(P,P^{\prime}\right)\] (167)

for all \(P,P^{\prime}\in\tilde{\mathcal{P}}\). As the mechanisms \(\mathbf{Q}_{k,\epsilon}^{*}\) and \(\mathbf{Q}_{c_{1},c_{2},h,\epsilon}^{*}\) in the paper are special cases of \(\mathbf{Q}_{c_{1},c_{2},\mu,\epsilon}^{*}\), a proof in this general setup induces Proposition 4.1.

Now, let us assume the setup in Definition C.1. Let \(P,P^{\prime}\in\tilde{\mathcal{P}}\) be given. WLOG, assume that \(r_{P}\geq r_{P^{\prime}}\). Let \(p=dP/d\mu\), \(p^{\prime}=dP^{\prime}/d\mu\), and \(q(x)=\mathrm{clip}\left(\frac{1}{r_{P}}p(x);b,be^{\epsilon}\right)\), \(q^{\prime}(x)=\mathrm{clip}\left(\frac{1}{r_{P^{\prime}}}p^{\prime}(x);b,be^{ \epsilon}\right)\), so that \(q=d\mathbf{Q}^{*}(P)/d\mu\) and \(q^{\prime}=d\mathbf{Q}^{*}(P^{\prime})/d\mu\). For simplicity, we denote \(\mathrm{clip}(x):=\mathrm{clip}(x;b,be^{\epsilon})\). We first note the fact that \(\mathrm{clip}(x)\) is monotone increasing and 1-Lipschitz in \(x\). From this and the equivalent expressions of the total variation distance in Appendix B, we have

\[D_{\mathrm{TV}}\left(\mathbf{Q}^{*}(P),\mathbf{Q}^{*}(P^{\prime })\right)\] (168) \[=\int_{x:q(x)\geq q^{\prime}(x)}(q(x)-q^{\prime}(x))d\mu(x)\] (169) \[=\int_{x:q(x)\geq q^{\prime}(x)}\left(\mathrm{clip}\left(\frac{1 }{r_{P}}p(x)\right)-\mathrm{clip}\left(\frac{1}{r_{P^{\prime}}}p^{\prime}(x) \right)\right)d\mu(x)\] (170) \[=\int_{x:q(x)\geq q^{\prime}(x)}\left(\mathrm{clip}\left(\frac{1 }{r_{P}}p(x)\right)-\mathrm{clip}\left(\frac{1}{r_{P}}p^{\prime}(x)\right) \right)d\mu(x)\] \[\quad+\int_{x:q(x)\geq q^{\prime}(x)}\left(\mathrm{clip}\left( \frac{1}{r_{P}}p^{\prime}(x)\right)-\mathrm{clip}\left(\frac{1}{r_{P^{\prime}} }p^{\prime}(x)\right)\right)d\mu(x)\] (171) \[\leq\int_{x:q(x)\geq q^{\prime}(x)}\left|\frac{1}{r_{P}}(p(x)-p^{ \prime}(x))\right|d\mu(x)+0\] (172) \[\leq\frac{1}{r_{P}}\int_{\mathcal{X}}|p(x)-p^{\prime}(x)|d\mu(x)= \frac{2}{r_{P}}D_{\mathrm{TV}}\left(P,P^{\prime}\right).\] (173)

This ends the proof. \(\qed\)

## Appendix E Behaviors of Proposed Mechanisms

In this appendix, we present the formal proofs for the behaviors of the proposed mechanisms presented in Sections 3.1 and 3.2.

We first observe that the formula of the optimal worst-case \(f\)-divergence in general case

\[\frac{1-r_{1}}{r_{2}-r_{1}}f(r_{2})+\frac{r_{2}-1}{r_{2}-r_{1}}f(r_{1})\] (174)is the \(y\)-coordinate value at \(x=1\) of the line segment joining \((r_{1},f(r_{1}))\) and \((r_{2},f(r_{2}))\). As \(f\) is convex, this formula is increasing in \(r_{2}\) and decreasing in \(r_{1}\), provided that \(r_{1}<1<r_{2}\).

Now, let us present the proofs.

* If \(f(0)=\infty\) and \(\tilde{\mathcal{P}}\) contains two mutually singular distributions, then \(\mathcal{R}(\mathcal{X},\tilde{\mathcal{P}},\epsilon,f)=\infty\).

Proof.: Let \(P_{1},P_{2}\in\tilde{\mathcal{P}}\) be mutually singular distributions with disjoint supports \(A_{1},A_{2}\subset\mathcal{X}\) respectively (That is, \(P_{1}(A_{1})=P_{2}(A_{2})=1\) and \(A_{1}\cap A_{2}=\emptyset\)). Suppose that \(\mathbf{Q}\) is an \(\epsilon\)-LDP sampling mechanism for \((\mathcal{X},\tilde{\mathcal{P}})\) such that \(R_{f}(\mathbf{Q})<\infty\). Since \(f(0)=\infty\), \(D_{f}\left(P\|Q\right)<\infty\), implies \(Q\ll P\). Hence, as \(D_{f}\left(P_{i}\|\mathbf{Q}(P_{i})\right)<\infty\) and \(P_{i}(A_{i}^{c})=0\), we have \(\mathbf{Q}(A_{i}^{c}|P_{i})=0\) for each \(i=1,2\). As \(\delta\) satisfies \(\epsilon\)-LDP, we have \(\mathbf{Q}(A_{1}^{c}|P_{2})\leq\mathbf{Q}(A_{1}^{c}|P_{1})=0\), \(\mathbf{Q}(A_{1}^{c}|P_{2})=0\). Now, since \(A_{1}\cap A_{2}=\emptyset\), we have \(A_{1}^{c}\cup A_{2}^{c}=\mathcal{X}\). But by the union bound, \(1=\mathbf{Q}(\mathcal{X}|P_{2})\leq\mathbf{Q}(A_{1}^{c}|P_{2})+\mathbf{Q}(A_{ 2}^{c}|P_{2})=0\), which is a contradiction. Hence, \(R_{f}(\mathbf{Q})=\infty\) for every \(\epsilon\)-LDP sampling mechanism \(\mathbf{Q}\). 

From now, assume \(f(0)<\infty\).

Let us first prove the behaviors for the case of finite \(\mathcal{X}\) in Section 3.1.

* \(\mathcal{R}([k],\mathcal{P}([k]),\epsilon,f)\) is decreasing in \(\epsilon\) and increasing in \(k\). Proof.: Recall from Appendix C.4.1 that the case of \(\mathcal{X}=[k]\), \(\tilde{\mathcal{P}}=\mathcal{P}([k])\) can be fit into the general case with \[r_{1} =0,\] (175) \[r_{2} =\frac{e^{\epsilon}+k-1}{e^{\epsilon}}.\] (176) Here, \(r_{2}\) is decreasing in \(\epsilon\) and increasing in \(k\). As (174) is increasing in \(r_{2}\), we get the desired claim. 
* For a fixed \(k\), we have \(\mathcal{R}([k],\mathcal{P}([k]),\epsilon,f)\to 0\) as \(\epsilon\to\infty\). Proof.: As \(\epsilon\to\infty\), we have \(r_{2}\to 1\). As \(f\) is continuous, \(f(1)=0\), and \(f(0)<\infty\), we obtain from (174) that \[\mathcal{R}([k],\mathcal{P}([k]),\epsilon,f)\to\frac{1-0}{1-0}f(1)+\frac{1-1}{ 1-0}f(0)=0\] (177) as \(\epsilon\to\infty\). 
* For a fixed \(k\), as \(\epsilon\to 0\), we have \(\mathbf{Q}_{k,\epsilon}^{*}(x|P)\to 1/k\) for every \(P\in\mathcal{P}([k])\) and \(x\in[k]\). Proof.: We know that \(\frac{1}{e^{\epsilon}+k-1}\leq\mathbf{Q}_{k,\epsilon}^{*}(x|P)\leq\frac{e^{ \epsilon}}{e^{\epsilon}+k-1}\). As \(\epsilon\to 0\), both of \(\frac{1}{e^{\epsilon}+k-1}\) and \(\frac{e^{\epsilon}}{e^{\epsilon}+k-1}\) converges to \(\frac{1}{k}\), hence we get the desired claim. 

Next, let us prove the behaviors for the continuous case in Section 3.2.

* If \(c_{1}=0\) and \(f(0)=\infty\), then \(\mathcal{R}(\mathbb{R}^{n},\tilde{\mathcal{P}}_{c_{1},c_{2},h},\epsilon,f)=\infty\). Proof.: Since \(r_{2}>1\) and \(r_{1}=0\), we have \(\frac{r_{2}-1}{r_{2}-r_{1}}=\frac{r_{2}-1}{r_{2}}>0\). Hence \(\frac{r_{2}-1}{r_{2}-r_{1}}f(r_{1})=\frac{r_{2}-1}{r_{2}-r_{1}}f(0)=\infty\), which proves \(\mathcal{R}(\mathbb{R}^{n},\tilde{\mathcal{P}}_{c_{1},c_{2},h},\epsilon,f)=\infty\). 
* For a fixed \((c_{1},c_{2})\), \(\mathcal{R}(\mathbb{R}^{n},\tilde{\mathcal{P}}_{c_{1},c_{2},h},\epsilon,f)\) is decreasing in \(\epsilon\). Proof.: We can observe that \(r_{1}\) is increasing in \(\epsilon\) and \(r_{2}\) is decreasing in \(\epsilon\). Since (174) is increasing in \(r_{2}\) and decreasing in \(r_{1}\), we get the desired claim. 
* For a fixed \((c_{1},c_{2})\) with \(c_{1}=0\), as \(\epsilon\to\infty\), we have \(\mathcal{R}(\mathbb{R}^{n},\tilde{\mathcal{P}}_{c_{1},c_{2},h},\epsilon,f)\to 0\). Proof.: We can observe that \(r_{1}=0\) and \(r_{2}\to 1\). Hence, by the same argument as (177), we have the desired claim.

Detailed Explanation of Setups in Numerical Results

We present details about the setups in producing numerical results in Section 5, and generating Figure 1 in Section 1.

In this appendix, for each \(\mu\in\mathbb{R}^{n}\), \(\mathcal{N}_{\mu,\Sigma}[x]=\frac{1}{(2\pi)^{n/2}\sqrt{\det(\Sigma)}}\exp\left(- \frac{(x-\mu)^{T}\Sigma^{-1}(x-\mu)}{2}\right)\) is the pdf of the \(n\)-dimensional Gaussian distribution with mean \(\mu\) and covariance \(\Sigma\). Note that we denote \(\mathcal{N}(\mu,\Sigma)\) to refer the Gaussian distribution itself with mean \(\mu\) and covariance matrix \(\Sigma\).

### Explanation for numerical result for finite data space

In this appendix, we show that among the choices of \(Q_{0}\) in the baseline for \(\mathcal{X}=[k]\), choosing \(Q_{0}\) to be the uniform distribution minimizes \(R_{f}(\mathbf{Q})\), and present the precise value of \(R_{f}(\mathbf{Q})\) for the baseline with uniform \(Q_{0}\). From now, let us fix \(k,\epsilon,f\), and we denote \(\mathbf{Q}_{Q_{0}}\) to mean the baseline with the reference distribution \(Q_{0}\). Also, let \(\delta_{x}\in\mathcal{P}([k])\) be the point mass at \(x\in[k]\). Recall that

\[\mathcal{M}_{\epsilon,Q_{0}}=\{Q\in\mathcal{P}(\mathcal{X}):e^{-\epsilon/2}Q _{0}(A)\leq Q(A)\leq e^{\epsilon/2}Q_{0}(A),\quad\forall A\subset\mathcal{X}\},\] (178)

and we set the baseline to satisfy that

\[D_{f}\left(P\|\mathbf{Q}_{Q_{0}}(P)\right)=\inf_{Q\in\mathcal{M}_{\epsilon,Q_{ 0}}}D_{f}\left(P\|Q\right).\] (179)

First, if \(Q_{0}(x)=0\) for some \(x\in[k]\), then \(\mathbf{Q}_{Q_{0}}(x|P)\leq e^{\epsilon/2}Q_{0}(x)=0\), \(\mathbf{Q}_{Q_{0}}(x|P)=0\) for every \(P\in\mathcal{P}([k])\). Especially, \(\mathbf{Q}_{Q_{0}}(x|\delta_{x})=0\), and this implies that \(\mathbf{Q}_{Q_{0}}(\delta_{x})\) and \(\delta_{x}\) are mutually singular. Hence,

\[R_{f}(\mathbf{Q}_{Q_{0}})\geq D_{f}\left(\delta_{x}\|\mathbf{Q}_{Q_{0}}( \delta_{x})\right)=M_{f},\] (180)

concluding that \(R_{f}(\mathbf{Q}_{Q_{0}})=M_{f}\). Hence, to minimize \(R_{f}(\mathbf{Q}_{Q_{0}})\), it suffices to set \(Q_{0}(x)>0\) for all \(x\in[k]\). Hence from now, we only consider such \(Q_{0}\).

We note that for every \(x\in[k]\) and \(P\in\mathcal{P}([k])\), we have we have \(\mathbf{Q}_{Q_{0}}(x|P)\leq e^{\epsilon/2}Q_{0}(x)\), and furthermore

\[\mathbf{Q}_{Q_{0}}(x|P) =1-\sum_{y\in[k]\setminus\{x\}}\mathbf{Q}_{Q_{0}}(y|P)\] (181) \[\leq 1-\sum_{y\in[k]\setminus\{x\}}e^{-\epsilon/2}Q_{0}(y)\] (182) \[=1-e^{-\epsilon/2}(1-Q_{0}(x))\] (183) \[=e^{-\epsilon/2}Q_{0}(x)+(1-e^{-\epsilon/2}).\] (184)

Letting

\[B(t)=\min\{e^{\epsilon/2}t,e^{-\epsilon/2}t+(1-e^{-\epsilon/2})\},\] (185)

we have

\[\mathbf{Q}_{Q_{0}}(x|P)\leq B(Q_{0}(x)).\] (186)

Note that \(B(t)\) is increasing in \(t\). Now, for any \(x\in[k]\), we have

\[R_{f}(\mathbf{Q}_{Q_{0}}) \geq D_{f}\left(\delta_{x}\|\mathbf{Q}_{Q_{0}}(\delta_{x})\right)\] (187) \[=\mathbf{Q}_{Q_{0}}(x|\delta_{x})f\left(\frac{1}{\mathbf{Q}_{Q_{0 }}(x|\delta_{x})}\right)+\sum_{y\in[k]\setminus\{x\}}\mathbf{Q}_{Q_{0}}(y| \delta_{x})f(0)\] (188) \[=\mathbf{Q}_{Q_{0}}(x|\delta_{x})f\left(\frac{1}{\mathbf{Q}_{Q_{0 }}(x|\delta_{x})}\right)+(1-\mathbf{Q}_{Q_{0}}(x|\delta_{x}))f(0).\] (189)

We can observe that the last term can be written in the form of the optimal worst-case \(f\)-divergence (174) with \(r_{1}=0\) and \(r_{2}=1/\mathbf{Q}_{Q_{0}}(x|\delta_{x})\). In other words, let

\[\mathfrak{R}(r_{1},r_{2})=\frac{1-r_{1}}{r_{2}-r_{1}}f(r_{2})+\frac{r_{2}-1}{ r_{2}-r_{1}}f(r_{1}).\] (190)Then we have \(R_{f}(\mathbf{Q}_{Q_{0}})\geq\mathfrak{R}(0,1/\mathbf{Q}_{Q_{0}}(x|\delta_{x}))\).

As noted in Appendix E, \(\mathfrak{R}(r_{1},r_{2})\) is increasing in \(r_{2}\) for \(0\leq r_{1}<1<r_{2}\). Since \(\mathbf{Q}_{Q_{0}}(x|\delta_{x})\leq B(Q_{0}(x))\), we have

\[R_{f}(\mathbf{Q}_{Q_{0}})\geq\mathfrak{R}(0,1/B(Q_{0}(x))).\] (191)

Since this should hold for all \(x\in[k]\), we have

\[R_{f}(\mathbf{Q}_{Q_{0}})\geq\max_{x\in[k]}\mathfrak{R}(0,1/B(Q_{0}(x))).\] (192)

Since \(\min_{x\in[k]}Q_{0}(x)\leq 1/k\) for all \(Q_{0}\in\mathcal{P}([k])\), and \(B(t)\) is increasing in \(t\), we have

\[R_{f}(\mathbf{Q}_{Q_{0}})\geq\mathfrak{R}(0,1/B(1/k)).\] (193)

Now, let \(\mu_{k}\) be the uniform distribution over \([k]\). We will show that

\[R_{f}(\mathbf{Q}_{\mu_{k}})=\mathfrak{R}(0,1/B(1/k)),\] (194)

which suffices to prove that \(Q_{0}=\mu_{k}\) minimizes \(R_{f}(\mathbf{Q}_{Q_{0}})\).

We observe that \(\mathcal{M}_{\epsilon,\mu_{k}}\) is a convex set in \(\mathcal{P}([k])\). Since \(D_{f}\left(P\|Q\right)\) is jointly convex in \((P,Q)\), we obtain that \(D_{f}\left(P\|\mathbf{Q}_{\mu_{k}}(P)\right)=\min_{Q\in\mathcal{M}_{\epsilon, \mu_{k}}}D_{f}\left(P\|Q\right)\) is convex in \(P\) by [64, Section 3.2.5]. Hence, the maximum of \(D_{f}\left(P\|\mathbf{Q}_{\mu_{k}}(P)\right)\) occurs when \(P\) is one of the extreme points of \(\mathcal{P}([k])\), that is, the point masses \(\delta_{x}\). By the same arguments as in (187)-(189), we have \(D_{f}\left(\delta_{x}\|Q\right)=\mathfrak{R}(0,1/Q(x))\), and hence

\[R_{f}(\mathbf{Q}_{\mu_{k}}) =\sup_{P\in\mathcal{P}([k])}D_{f}\left(P\|\mathbf{Q}_{\mu_{k}}(P)\right)\] (195) \[=\max_{x\in[k]}D_{f}\left(\delta_{x}\|\mathbf{Q}_{\mu_{k}}(\delta _{x})\right)\] (196) \[=\max_{x\in[k]}\inf_{Q\in\mathcal{M}_{\epsilon,\mu_{k}}}D_{f} \left(\delta_{x}\|Q\right)\] (197) \[=\max_{x\in[k]}\inf_{Q\in\mathcal{M}_{\epsilon,\mu_{k}}}\mathfrak{ R}(0,1/Q(x))\] (198) \[=\mathfrak{R}\left(0,\frac{1}{\min_{x\in[k]}\sup_{Q\in\mathcal{M} _{\epsilon,\mu_{k}}}Q(x)}\right).\] (199)

Also, by the same arguments as in (181)-(184), we have

\[Q(x)\leq B(1/k),\quad\forall Q\in\mathcal{M}_{\epsilon,\mu_{k}},x\in[k].\] (200)

Now, we will show that \(\sup_{Q\in\mathcal{M}_{\epsilon,\mu_{k}}}Q(x)=B(1/k)\) for any \(x\in[k]\). To show this, we will prove that there is a distribution \(Q\in\mathcal{P}([k])\) such that \(Q(x)=B(1/k)\) and \(Q(y)=\frac{1-B(1/k)}{k-1}\) for all \(y\in[k]\backslash\{x\}\), and this \(Q\) is contained in \(\mathcal{M}_{\epsilon,\mu_{k}}\). It suffices to show the followings:

\[\frac{1}{k}e^{-\epsilon/2}\leq B(1/k) \leq\min\left\{1,\frac{1}{k}e^{\epsilon/2}\right\},\] (201) \[\frac{1}{k}e^{-\epsilon/2}\leq\frac{1-B(1/k)}{k-1} \leq\min\left\{1,\frac{1}{k}e^{\epsilon/2}\right\}.\] (202)

We note that whenever \(0\leq t\leq 1\), we have

* \(B(t)=\min\{e^{\epsilon/2}t,e^{-\epsilon/2}t+(1-e^{-\epsilon/2})\}\geq t\), and
* \(B(t)\leq e^{-\epsilon/2}t+(1-e^{-\epsilon/2})=1-(1-t)e^{-\epsilon/2}\leq 1\).

From these, we can easily observe that \(B(1/k)\leq\min\left\{1,\frac{1}{k}e^{\epsilon/2}\right\}\), and

\[\frac{1}{k}e^{-\epsilon/2}\leq\frac{1}{k}\leq B(1/k).\] (203)Also,

\[\frac{1-B(1/k)}{k-1}\leq\frac{1-(1/k)}{k-1}=\frac{1}{k}\leq\min\left\{1,\frac{1}{k }e^{\epsilon/2}\right\},\] (204)

\[\frac{1-B(1/k)}{k-1}\geq\frac{1-[e^{-\epsilon/2}(1/k)+(1-e^{-\epsilon/2})]}{k-1 }=\frac{1}{k}e^{-\epsilon/2}.\] (205)

This shows that \(\sup_{Q\in\mathcal{M}_{*,\mu_{k}}}Q(x)=B(1/k)\) for any \(x\in[k]\). Thus,

\[\min_{x\in[k]}\sup_{Q\in\mathcal{M}_{*,\mu_{k}}}Q(x)=B(1/k),\] (206)

\[R_{f}(\mathbf{Q}_{\mu_{k}})=\mathfrak{R}\left(0,\frac{1}{B(1/k)}\right).\] (207)

This ends the proof that \(Q_{0}=\mu_{k}\) minimizes \(R_{f}(\mathbf{Q}_{Q_{0}})\), and \(R_{f}(\mathbf{Q}_{\mu_{k}})=\mathfrak{R}(0,1/B(1/k))\).

### Explanation for the experiment in 1D Gaussian mixture

The precise description of the set of possible client distributions in our experimental setup is as follows:

\[\tilde{\mathcal{P}}=\left\{P:p(x)=\frac{\sum_{i=1}^{k}\lambda_{i} \mathcal{N}_{\mu_{i},1}[x]}{\int_{-4}^{4}\sum_{i=1}^{k}\lambda_{i}\mathcal{N} _{\mu_{i},1}[x]dx}\mathbbm{1}_{[-4,4]}(x),k\in[K],\lambda_{i}\geq 0,\sum_{i=1}^{k} \lambda_{i}=1,|\mu_{i}|\leq 1\right\}.\] (208)

Each of \(P_{j}\in\tilde{\mathcal{P}}\) is generated by choosing \(k,\lambda_{i}\), and \(\mu_{i}\) in (208) as follows: (i) First, choose the number of Gaussian distributions \(k\) by first sample \(\tilde{k}\) from the Poisson distribution with mean \(k_{0}\) (which is chosen beforehand), and let \(k=\min(\tilde{k}+1,K)\), and (ii) after that, sample each of \(\mu_{1},\cdots,\mu_{k}\) independently from the uniform distribution on \([-1,1]\), and sample \((\lambda_{1},\cdots,\lambda_{k})\) from the uniform distribution on \(\mathcal{P}([k])\).

The implementation of our proposed mechanism is as follows. We can observe that

\[\int_{-4}^{4}\sum_{i=1}^{k}\lambda_{i}\mathcal{N}_{\mu_{i},1}[x] dx\geq\inf_{\mu\in[-1,1]}\int_{-4}^{4}\mathcal{N}_{\mu_{i},1}[x]dx=\int_{-4}^{4} \mathcal{N}_{1,1}[x]dx=\Phi(3)-\Phi(-5),\] (209)

where \(\Phi\) is the cdf of the 1D standard Gaussian distribution, and for each \(x\in[-4,4]\), we have

\[\sum_{i=1}^{k}\lambda_{i}\mathcal{N}_{\mu_{i},1}[x]dx\leq\sup_{\mu\in[-1,1]} \mathcal{N}_{\mu,1}[x]=\frac{1}{\sqrt{2\pi}}\exp\left(-[\max(|x|-1,0)]^{2}/2 \right).\] (210)

Hence, we have \(\tilde{\mathcal{P}}\subset\tilde{\mathcal{P}}_{0,1,\tilde{h}}=\tilde{\mathcal{ P}}_{0,c_{2},h}\), where

\[\tilde{h}(x)=\frac{\exp\left(-[\max(|x|-1,0)]^{2}/2\right)}{\sqrt{2\pi}(\Phi( 3)-\Phi(-5))}\mathbbm{1}_{[-4,4]}(x)\] (211)

and \(c_{2}=\int\tilde{h}(x)dx\), \(h(x)=\tilde{h}(x)/c_{2}\). When implementing our proposed mechanism, we use the bisection method to find a constant \(r_{P}\). We predetermine the error tolerances \(\delta_{1},\delta_{2}\geq 0\), \(\delta_{1}<1\), and we terminate the bisection method to find \(r_{P}\) if the integration of (13) lies in the interval \([1-\delta_{1},1+\delta_{2}]\). As mentioned in Section 4.1, to consider the error tolerances, we actually implement \(\mathbf{Q}_{0,c_{2},h,\epsilon^{\prime}}^{*}(P)\), with \(\epsilon^{\prime}=\epsilon-\log\frac{1+\delta_{2}}{1-\delta_{1}}\).

In the experiment in the paper, we use \(k_{0}=2\) and \(\delta_{1}=\delta_{2}=10^{-5}\).

For the baseline, we use the same hyperparameter setups as in [35, Section 5, Paragraph "Architectures"], except a slight modification in a reference distribution \(Q_{0}\). In [35], they set the standard Gaussian as the reference distribution, \(Q_{0}=\mathcal{N}(0,1)\). To consider the truncated domain \([-4,4]\), we set \(Q_{0}\) as the truncation of the standard Gaussian, that is, \(Q_{0}\) has a pdf

\[q_{0}(x)=\frac{\mathcal{N}_{0,1}[x]}{\int_{-4}^{4}\mathcal{N}_{0,1}[x]dx} \mathbbm{1}_{[-4,4]}(x).\] (212)The baseline has many other hyperparameters not specified in [35], such as the proposal distribution used for the Metropolis-Hasting algorithm [66; 67], initial model parameters for the weak learner, etc. We noticed that the code for [35] is available at https://github.com/karokaram/PrivatedBoostedDensities/tree/master and hence we made every effort to faithfully reproduce the baseline with exactly the same hyperparameters, including those not mentioned in the paper [35]. However, subtle variations may arise due to differences in the programming languages employed; we used Python, while [35] used Julia.

### Explanation for Figure 1

For \(k\in\mathbb{N}\) and \(\sigma>0\), the Gaussian ring distribution with \(k\) modes and a component-wise variance \(\sigma^{2}\) is the mixture of \(k\) Gaussian distributions in \(\mathbb{R}^{2}\) with equal weights, where each Gaussian distribution has the covariance \(\sigma^{2}I_{2}\) and equally spaced mean in the unit circle, and one of the Gaussian distribution has mean \((1,0)\). That is, it has a density \(\frac{1}{k}\sum_{i=1}^{k}\mathcal{N}_{\mu_{i},\sigma^{2}I_{2}}[x]\), with \(\mu_{i}=(\cos\frac{2\pi i}{k},\sin\frac{2\pi i}{k})\). In Figure 1, the left image is the pdf of the Gaussian ring distribution with \(k=3\) and \(\sigma^{2}=0.5\). It is used as a client's original distribution.

For our proposed mechanism, similar to Section 3.2, we use the setup that \(\tilde{\mathcal{P}}\) consists of Gaussian mixtures, where each Gaussian has mean within a unit ball centered at the origin and has covariance \(0.5I_{2}\). that is, \(\tilde{\mathcal{P}}=\{\sum_{i=1}^{k}\lambda_{i}\mathcal{N}(x_{i},\sigma^{2}I_ {2}):k\in\mathbb{N},\lambda_{i}\geq 0,\sum_{i=1}^{k}\lambda_{i}=1,\|x_{i}\| \leq 1\}\), with \(\sigma^{2}=0.5\). We can also observe that \(\tilde{\mathcal{P}}=\tilde{\mathcal{P}}_{0,1,\tilde{h}}\) for \(\tilde{h}(x)=\frac{1}{2\pi\sigma^{2}}\exp\left(-\frac{[\max(0,\|x\|-1)]^{2}}{2 \sigma}\right)\).

Same as Appendix F.2, we have \(\tilde{\mathcal{P}}_{0,1,\tilde{h}}=\tilde{\mathcal{P}}_{0,c_{2},h}\), where \(c_{2}=\int\tilde{h}(x)dx\), \(h(x)=\tilde{h}(x)/c_{2}\). Hence, we use \(\mathbf{Q}_{0,c_{2},h,\epsilon}^{*}\). The implementation of \(\mathbf{Q}_{c_{1},c_{2},h,\epsilon}^{*}(P)\) is the same as the description in Appendix F.2 with \(\delta=10^{-5}\). For the baseline, we also use MBDE with the same hyperparameter setup as [35; Section 5, Paragraph "Architectures"], with the (untruncated) 2D standard Gaussian as a reference distribution, \(Q_{0}=\mathcal{N}(0,I_{2})\).

[MISSING_PAGE_EMPTY:37]

Instructions for Reproducing Results

In this appendix, we provide instructions for reproducing the experiments and figures in the paper. For a detailed document about the code, refer to the provided file README.md. For the tasks consuming a large time, we also specify the running times of such tasks. We do not specify the running times of the short tasks consuming less than 5 seconds. All experiments are performed on our simulation PC with the following specifications:

* OS: Ubuntu 22.04.1
* CPU: Intel(R) Core(TM) i9-9900X
* Memory: 64GB

There are a few remarks:

* Although we expect that the codes can be run and reproduce our results on sufficiently recent versions of Python libraries, we provide the information about the anaconda environment used in the experiment in environment.yaml for the completeness.
* The provided codes contain some lines to make the figures use TeX fonts. Running such lines requires the TeX to be installed in the experimental environment. We can remove the following lines to disable using TeX:  matplotlib.use("pgf") "pgf.texsystem": "pdfflatex", 'font.family':'serif', 'text.usetex': True, 'pgf.rcfonts': False, 'font.serif': 'Computer Modern Roman',

### Instruction for producing Figure 1

Figure 1 can be obtained by running the code plot_GaussRing.py, without any program arguments. We measure the running time for initializing the mechanism and and calculating the sampling distribution for the baseline (MBDE [35]) and our proposed mechanism. The measured running times in our environment are as follows: (unit: second)

* Initializing the mechanism
* Baseline: 0.80
* Proposed: 1.22
* Calculating the sampling distribution
* Baseline: 35.19
* Proposed: 664.18

### Instruction for producing Figure 2

Figure 2 can be obtained by running the code visualize_finiteSpace.py, without any program arguments.

### Instruction for producing results for finite space

The results about the theoretical worst-case \(f\)-divergence for finite space, Figures 3,5,6,7, can be obtained by running the code plot_finite.py with a program argument -\(\mathtt{\char 37}\)k to specify the value of \(k\). For example, in the command line, the aforementioned four figures can be generated by the following commands, respectively:

python plot_finite.py --k 10 python plot_finite.py --k 5python plot_finite.py --k 20 python plot_finite.py --k 100

### Instruction for producing results for 1D Gaussian mixture

The experiment of 1D Gaussian mixture in Section 5.2 consists of the following two codes:

1. exp_1DGaussMix.py This code performs an experiment on a single \(\epsilon\). We can provide two program arguments --eps and --seed to specify the values of \(\epsilon\) and the random seed, respectively.
2. plot_1DGaussMix.py This code generate the plot like Figure 4 from the results of exp_1DGaussMix.py

The results in the paper, Figure 4, can be obtained as follows:

1. First, run the following commands: 
 These can be run in any order or in parallel. Check that all of the five result files data_1DGaussMix_eps(eps).npy corresponding to five values of \(\epsilon\) are generated. In our environment, running all of the five commands in parallel consumes 3h 13m 35s.
2. Then, run the code plot_1DGaussMix.py without any program arguments.

## Appendix I Limitations

Our main contribution lies in proposing a minimax-optimal mechanism, and we present several experimental results based on synthetic data to support the superiority of our mechanism. However, since we have not conducted experiments based on real datasets, the analysis of performance in real-world scenarios is insufficient. Also, our PUT formulation in the minimax sense provides optimal utility in the worst case, which may result in reduced _average_ utility when prior information is given. Finally, our implementation of the proposed mechanism requires a large amount of running time due to numerical integration, which makes experiments in multidimensional spaces challenging.

## Appendix J Broader Impacts

Our proposed mechanism can be utilized for privacy protection in the field of generative models, which has recently received significant attention. A major deterrent to the adoption of privacy protection algorithms in real-world scenarios is the potential performance degradation. Our PUT-optimal mechanism, that minimizes the loss of utility given the privacy budget, can alleviate such concerns.

We should note that an LDP mechanism provides a certain level of privacy protection but cannot guarantee complete privacy protection without completely sacrificing utility. Additionally, clients typically provide multiple data points through various channels, and when these data are combined, it can lead to greater privacy leakage [68, 69].

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We clearly stated the main contributions and scope of our paper in the abstract and introduction (Section 1). Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We stated the limitations of our work in Appendix I. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: All theorems, propositions, and lemmas in the paper include all the necessary assumptions, and are accompanied with either the proofs in the appendices or relevant references. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provided instructions to reproduce all the figures and experimental results in Appendix H. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We plan to make the code attached in the supplementary material publicly open after the review period. Also, the instructions to reproduce the results are fully specified in Appendix H. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We presented enough details to implement our proposed mechanism, and we attached the code for implementing both the proposed mechanism and the baseline in the supplementary material. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: As the main contribution is theoretically characterizing the _worst-case_ utility, we focused on extracting the worst-case utility in the experiment. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provided the information about the computer resources and running times for the experiments in Appendix H. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our research conforms with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discussed both the positive and negative societal impacts in Appendix J. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We do not recognize any risk for misuse. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: All of the codes only use the basic Python libraries like numpy, scipy, torch, and so on. No other external assets are used. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: As mentioned in Appendix H, we documented the details of our codes in the file README.md, which is provided by the NeurIPS Code and Data Submission Guidelines. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.