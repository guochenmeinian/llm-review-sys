# The Edge-of-Reach Problem in Offline Model-Based Reinforcement Learning

 Anya Sims

University of Oxford

anya.sims@stats.ox.ac.uk

&Cong Lu

University of Oxford

&Jakob N. Foerster

FLAIR, University of Oxford

&Yee Whye Teh

University of Oxford

###### Abstract

Offline reinforcement learning (RL) aims to train agents from pre-collected datasets. However, this comes with the added challenge of estimating the value of behaviors not covered in the dataset. Model-based methods offer a potential solution by training an approximate dynamics model, which then allows collection of additional synthetic data via rollouts in this model. The prevailing theory treats this approach as online RL in an approximate dynamics model, and any remaining performance gap is therefore understood as being due to dynamics model errors. In this paper, we analyze this assumption and investigate how popular algorithms perform as the learned dynamics model is improved. In contrast to both intuition and theory, _if the learned dynamics model is replaced by the true error-free dynamics, existing model-based methods completely fail_. This reveals a key oversight: The theoretical foundations assume sampling of full horizon rollouts in the learned dynamics model; however, in practice, the number of model-rollout steps is aggressively reduced to prevent accumulating errors. We show that this truncation of rollouts results in a set of edge-of-reach states at which we are effectively "bootstrapping from the void." This triggers pathological value overestimation and complete performance collapse. We term this the edge-of-reach problem. Based on this new insight, we fill important gaps in existing theory, and reveal how prior model-based methods are primarily addressing the edge-of-reach problem, rather than model-inaccuracy as claimed. Finally, we propose _Reach-Aware Value Learning_ (RAVL), a simple and robust method that directly addresses the edge-of-reach problem and hence - unlike existing methods - does not fail as the dynamics model is improved. Since world models will inevitably improve, we believe this is a key step towards future-proofing offline RL.1

Footnote 1: Our code is open-sourced at: github.com/anyasims/edge-of-reach.

## 1 Introduction

Standard online reinforcement learning (RL) requires collecting large amounts of on-policy data. This can be both costly and unsafe, and hence represents a significant barrier against applying RL in domains such as healthcare [28, 32] or robotics [2, 4, 20], and also against scaling RL to more complex problems. Offline RL [6, 23] aims to remove this need for online data collection by enabling agents to be trained on pre-collected datasets. One hope [21] is that it may facilitate advances in RL similar to those driven by the use of large pre-existing datasets in supervised learning [3].

The central challenge in offline RL is estimating the value of actions not present in the dataset, known as the _out-of-sample action_ problem [17].2 A naive approach results in extreme value overestimation due to bootstrapping using inaccurate values at out-of-sample state-actions [18]. There have been many proposals to resolve this, with methods largely falling into one of two categories: model-free [17, 18, 1, 19, 8, 10] or model-based [14, 24, 29, 36].

Footnote 2: This is also referred to as the _out-of-distribution action_[18] or _action distribution shift_ problem [19].

Model-free methods typically address the out-of-sample action problem by applying a form of conservatism or constraint to avoid using out-of-sample actions in the Bellman update. In contrast, the solution proposed by model-based methods is to allow the collection of additional data at any previously out-of-sample actions. This is done by first training an approximate dynamics model on the offline dataset [30, 13], and then allowing the agent to collect additional synthetic data in this model via \(k\)-step rollouts (see Algorithm 1). The prevailing understanding is that this can be viewed as online RL in an approximate dynamics model, with the instruction being to then simply "run any RL algorithm on \(\widehat{M}\) until convergence"[36], where \(\widehat{M}\) is the learned dynamics model with some form of dynamics uncertainty penalty. Existing methods propose various forms of dynamics penalties (see Table 7), based on the assumption that the remaining performance gap compared to online RL is solely due to inaccuracies in the learned dynamics model.

This understanding naturally implies that improving the dynamics model should also improve performance. Surprisingly, we find that existing offline model-based methods completely fail if the learned dynamics model is replaced with the true, error-free dynamics model, while keeping everything else the same (see Figure 1). Under the true dynamics, the only difference to online RL is that in online RL, data is sampled as full-length episodes, while in offline model-based RL, data is instead sampled as \(k\)-step rollouts, starting from a state in the original offline dataset, with rollout length \(k\) limited to avoid accumulating model errors. Failure under the true model therefore highlights that truncating rollouts has critical and previously-overlooked consequences.

We find that this rollout truncation leads to a set of states which, under any policy, can only be reached in the final rollout step (see red in Figure 2). The existence of these _edge-of-reach_ states is problematic as it means Bellman updates (see Equation (1)) use target values that are never themselves trained. This is illustrated in Figure 2, and described in detail in Section 3. This effective "bootstrapping from the void" triggers a catastrophic breakdown in \(Q\)-learning. Concisely, this issue can be viewed as all actions from edge-of-reach states remaining out-of-reach over training. Hence, contrary to common understanding, the out-of-sample action problem central to model-free methods is not fully resolved by a model-based approach. In fact, in Section 3.4 we provide detailed analysis suggesting that this is the predominant source of issues on the standard D4RL benchmark. In Section 6.5 we consequently reexamine how existing methods work and find they are indirectly and unintentionally addressing this issue (rather than model errors as claimed).

Figure 1: _Existing offline model-based RL methods fail if the accuracy of the dynamics model is increased (with all else kept the same)._ Results shown are for MOPO [36], but note that this failure indicates the _failure of all existing uncertainty-based methods_ since each of their specific penalty terms disappear under the true dynamics as ‘uncertainty’ is zero. By contrast, our method is much more robust to changes in dynamics model. The \(x\)-axis shows linearly interpolating next states and rewards of the learned model with the true model (center\(\rightarrow\)right) and random model (center\(\rightarrow\)left), with results on the D4RL W2d-medexp benchmark (min/max over 4 seeds). The full set of results and experimental setup are provided in Table 1 and Appendix C.2 respectively.

[MISSING_PAGE_FAIL:3]

**Model-Based Offline RL.** Model-based methods [30] aim to solve the out-of-sample issue by allowing the agent to collect additional synthetic data in a learned dynamics model. They generally share the same base procedure as described in Algorithm 1. This involves first training an approximate dynamics model \(\widehat{M}=(\mathcal{S},\mathcal{A},\widehat{T},\widehat{R},\mu_{0},\gamma)\) on \(\mathcal{D}_{\text{offline}}\). Here, \(\widehat{T}(s^{\prime}|s,a)\) and \(\widehat{R}(s,a)\) denote the learned transition and reward functions, commonly realized as a deep ensemble [5; 22]. Following this an agent is trained using an online RL algorithm (typically SAC [11]), for which data is sampled as \(k\)-step trajectories (termed rollouts) under the current policy, starting from states in the offline dataset \(\mathcal{D}_{\text{offline}}\). The base procedure of training a SAC agent with model rollouts does not work out of the box. Existing methods attribute this as due to dynamics model errors. Consequently, a broad class of methods propose augmenting the learned dynamics model \(\widehat{M}\) with some form of dynamics uncertainty penalty, often based on variance over the ensemble dynamics model [2; 14; 29; 36; 24]. We present some explicit examples in Table 7. Crucially, all of these methods assume that, under the true error-free model, no intervention should be needed, and hence all the uncertainty penalties collapse to zero. In the following section, we show that this assumption leads to catastrophic failure.

## 3 The Edge-of-Reach Problem

In the following section, we formally introduce the _edge-of-reach_ problem. We begin by showing the empirical failure of SOTA offline model-based methods on the _true environment dynamics_. Next, we present our edge-of-reach hypothesis for this, including intuition, empirical evidence on the main D4RL benchmark [7], and theoretical proof of its effect on offline model-based training.

### Surprising Failure with the True Dynamics

As described in Sections 1 and 2, prior works view offline model-based RL as _online RL in an approximate dynamics model_. Based on this understanding they propose various forms of dynamics uncertainty penalties to address model errors (see Table 7). This approach is described simply as: _"two steps: (a) learning a pessimistic MDP (P-MDP) using the offline dataset; (b) learning a near-optimal policy in this P-MDP"_[14]. This shared base procedure is shown in Algorithm 1, where _"P-MDP"_ refers to the learned dynamics model with a penalty added to address model errors.

This assumption of issues being due to dynamics model errors naturally leads to the belief that the ideal case would be to have a perfect error-free dynamics model. However, in Figure 1 and Table 1 we demonstrate that, _if the learned dynamics model is replaced with the true dynamics,

Figure 2: The previously unnoticed edge-of-reach problem. _Left_ illustrates the base procedure used in offline model-based RL, whereby synthetic data is sampled as \(k\)-step trajectories “rollouts” starting from a state in the original offline dataset. **Edge-of-reach** states are those that can be reached in \(k\)-steps, but which cannot (under any policy) be reached in less than \(k\)-steps. We depict the data collected with two rollouts, one ending in \(s_{k}=D\), and the other with \(s_{k}=C\). _Right_ then shows this data arranged into a dataset of transitions as used in \(Q\)-updates. State \(D\) is edge-of-reach and hence appears in the dataset as \(s^{\prime}\)_but never as \(s\)_. Bellman updates therefore _bootstrap from \(D\), but never update the value at \(D\)_ (see Equation (1)). (For comparison consider state \(C\): \(C\) is also sampled at \(s_{k}\), but unlike \(D\) it is not edge-of-reach, and hence is also sampled at \(s_{i<k}\) meaning it _is_ updated and hence does not cause issues.)all dynamics-penalized methods completely fail on most environments.5_ Note that under the true dynamics, all existing dynamics penalty-based methods assume no intervention is needed since there are no model errors. As a result, their penalties all become zero (see Table 7) and they all collapse to exactly the same procedure. Therefore the results shown in Table 1 indicate the failure of _all_ existing dynamics-penalized methods. In the following section, we investigate why having perfectly accurate dynamics leads to failure.

Footnote 5: For clarity, this change (**Approximate\(\rightarrow\)True**) is described in Algorithm 1 as pseudocode, and also in Appendix H in terms of how it relates to other experiments presented throughout the paper.

### The Edge-Of-Reach Hypothesis (Illustrated in Figure 2)

Failure under the error-free dynamics reveals that "model errors" cannot explain all the problems in offline model-based RL. Instead, it highlights that there must be a second issue. On investigation we find this to be the "edge-of-reach problem." We begin with the main intuition: in offline model-based RL, synthetic data \(\mathcal{D}_{\text{rollouts}}\) is generated as short \(k\)-step rollouts starting from states in the original dataset \(\mathcal{D}_{\text{offline}}\) (see Algorithm 1). Crucially, **Figure 2 (_left_) illustrates how, under this procedure, there can exist some states which can be reached in the final rollout step, but which _cannot_ - under any policy - be reached earlier. These _edge-of-reach_ states triggers a breakdown in learning, since even with the ability to collect unlimited data, the agent is never able to reach these states 'in time' to try actions from them, and hence is free to 'believe that these edge-of-reach states are great.'

More concretely: **Figure 2 (_right_)** illustrates how being sampled only at the final rollout step means edge-of-reach states will appear in the resulting dataset \(\mathcal{D}_{\text{rollouts}}\) as _nextstates_\(s^{\prime}\), but will never appear in the dataset as _states_\(s\). Crucially, in Equation (1), we see that values at states \(s\) are updated, while values at \(s^{\prime}\) are used for the targets of these updates. Edge-of-reach states are therefore _used for targets, but are never themselves updated_, meaning that their values can be arbitrarily disseminated. Updates consequently propagate misestimation over the entire state-action space. Furthermore, the \(\max\) operation in the policy improvement step (see Section 2) _exploits any misestimation_ by picking out the most heavily overestimated values [23]. The _misestimation_ is therefore turned into _overestimation_, resulting in the value explosion seen in Figure 3. Thus, contrary to common understanding, the out-of-sample action problem key in model-free offline RL can be seen to persist in model-based RL.

**Edge-of-Reach Hypothesis**: Limited horizon rollouts from a fixed dataset leads to 'edge-of-reach' states: states which are used as targets for Bellman-based updates, but which are never themselves updated. The resulting maximization over misestimated values in Bellman updates causes pathological value overestimation and a breakdown in \(Q\)-learning.

_When is this an issue in practice?_ Failure via this mode requires _(a)_ the existence of edge-of-reach states, and _(b)_ such edge-of-reach states to be sampled. The typical combination of \(k\ll H\) along with a limited pool of starting states (\(s\in\mathcal{D}_{\text{offline}}\)) means the rollout distribution is unlikely to sufficiently cover the full state space \(\mathcal{S}\), thus making _(a)_ likely. Moreover, we observe pathological 'edge-of-reach seeking' behavior in which the agent appears to'seek out' edge-of-reach states due to this being the source of overestimation, thus making _(b)_ likely. This behaviour is discussed further in Section 4.

In Appendix A, we give a thorough and unified view of model-free and model-based offline RL, dividing the problem into independent conditions for states and actions and examining when we can expect the edge-of-reach problem to be significant.

### Definitions and Formalization

**Definition 1** (Edge-of-reach states).: _Consider a deterministic transition model \(T:\mathcal{S}\times\mathcal{A}\rightarrow\mathcal{S}\), rollout length \(k\), and some distribution over starting states \(\nu_{0}\). For some policy \(\pi:\mathcal{S}\rightarrow\mathcal{A}\), rollouts are then generated according to \(s_{0}\sim\nu_{0}(\cdot)\), \(a_{t}\sim\pi(\cdot|s_{t})\) and \(s_{t+1}\sim T(\cdot|s_{t},a_{t})\) for \(t=0,\ldots,k-1\), giving \((s_{0},a_{0},s_{1},\ldots,s_{k})\). Let us use \(\rho_{t,\pi}(s)\) to denote the marginal distributions over \(s_{t}\)._

_We define a state \(s\in S\)**edge-of-reach** with respect to (\(T\), \(k\), \(\nu_{0}\)) if: for \(t=k\), \(\exists\,\pi\) s.t. \(\rho_{t,\pi}(s)>0\), but, for \(t=1,\ldots,k-1\) and \(\forall\,\pi\), \(\rho_{t,\pi}(s)=0\). In our case, \(\nu_{0}\) is the distribution of states in \(\mathcal{D}_{\text{offline}}\)._

In Appendix B, we include an extension to stochastic transition models, proof of how errors can consequently propagate to all states, and a discussion of the practical implications.

### Empirical evidence on the D4RL benchmark

There are two potential issues in offline model-based RL: _(1)_ dynamics model errors and subsequent model exploitation, and _(2)_ the edge-of-reach problem and subsequent pathological value overestimation. We ask: _Which is the true source of the issues observed in practice?_ While _(1)_ is stated as the sole issue and hence the motivation in prior methods, the "failure" results presented in Figure 1 and Table 1 are strongly at odds with this explanation. Furthermore, in Table 8, we examine the model rewards sampled by the agent over training. For _(1)_ to explain the \(Q\)-values seen in Figure 3, the sampled rewards in Table 8 would need to be on the order of \(10^{8}\). Instead, however, they remain less than \(10\), and are not larger than the true rewards, meaning _(1)_ does not explain the observed value explosion (see Appendix D.2). By contrast, the edge-of-reach-induced pathological overestimation mechanism of _(2)_ exactly predicts this value explosion, with the observations very closely resembling those of the analogous model-free out-of-sample problem [18]. Furthermore, _(2)_ is consistent with the "failure" observations in Table 1. Finally, we again highlight the discussion in Section 3.2 where we explain why the edge-of-reach problem can be expected to occur in practice.

## 4 Analysis with a Simple Environment

In the previous section, we presented the edge-of-reach hypothesis as an explanation of why existing methods fail under the true dynamics. In this section, we construct a simple environment to empirically confirm this hypothesis. We first reproduce the observation seen in Figures 1 and 3 and Table 1 of the base offline model-based procedure resulting in exploding \(Q\)-values and failure to learn despite using the true dynamics model. Next, we verify that edge-of-reach states are the source of this problem by showing that correcting value estimates only at these states is sufficient to resolve the issues.

### Setup

We isolate failure observed in Section 3.1 with the following setup: Reward is defined as in Figure 3(a), the transitions function is simply \(\mathbf{s^{\prime}}=\mathbf{s}+\mathbf{a}\in\mathbb{R}^{2}\), and initial states (analogous to \(s\in\mathcal{D}_{\text{offline}}\)) are sampled from \(\mu_{0}=U([-2,2]^{2})\) (the area shown in the navy blue box Figure 4).

In applying the offline model-based procedure (see Algorithm 1) to this (true) environment we have precisely the same setup as in Section 3.1, where again the _only difference compared to online RL is the use of truncated \(k\)-step rollouts_ with \(k=10\) (compared to full horizon \(H=30\)). This small change results in the existence of edge-of-reach states (those between the red and orange boxes).

### Observing Pathological Value Overestimation

Exactly as with the benchmark experiments in Section 3.1, the base model-based procedure fails despite using the true dynamics (see blue Figure 4, **Base**), and the \(Q\)-values grow unboundedly over training (compare Figure 3(f) and Figure 2(b)).

Looking at the rollouts sampled over training (see Figure 7) we see the following behavior:

_(Before 25 epochs)_ Performance initially increases. _(Between 25 and 160 epochs)_ Value misestimation takes over, and the policy begins to aim toward unobserved state-actions (since their values can be misestimated and hence overestimated). _(After 160 epochs)_ This _'edge-of-reach seeking' behavior_ compounds with each epoch, leading the agent to eventually reach edge-of-reach states. From this point onwards, the agent samples edge-of-reach states at which it never receives any corrective feedback. The consequent pathological value overestimation results in a complete collapse in performance. In Figure 3(b) we visualize the final policy and see that it completely ignores the reward function, aiming instead towards an arbitrary edge-of-reach state.

Figure 3: The base procedure results in poor performance _(left)_ with exponential increase in \(Q\)-values _(right)_ on the D4RL benchmark. _Approx \(Q^{*}\)_ indicates the \(Q\)-value for a normalized score of 100 (with \(\gamma=0.99\)). Results are shown for Walker2d-medexp (\(6\) seeds), but we note similar trends across other D4RL datasets.

### Verifying the Hypothesis Using Value Patching

Our hypothesis is that the source of this failure is value misestimation at edge-of-reach states. Our **Base-OraclePatch** experiments (see yellow Figure 4) verify this by showing that patching in the correct values solely at edge-of-reach states is sufficient to completely solve the problem. This is particularly compelling as in practice we only corrected values at 0.4% of states over training. In Section 5 we introduce our practical method RAVL, which Figures 4 green and 7 show has an extremely similar effect to that of the ideal but practically impossible Base-OraclePatch intervention.

## 5 RAVL: Reach-Aware Value Learning

As verified in Section 4.3, issues stem from value overestimation at edge-of-reach states. To resolve this, we therefore need to _(A)_ detect and _(B)_ prevent overestimation at edge-of-reach states.

_(A) Detecting edge-of-reach states:_ As illustrated in Figure 2 _(right)_, edge-of-reach states are states at which the \(Q\)-values are never updated, i.e., those that are out-of-distribution (OOD) with respect to the training distribution of the \(Q\)-function, \(s\in\mathcal{D}_{\text{rollouts}}\). A natural solution for OOD detection is measuring high variance over a deep ensemble [22]. We can therefore detect edge-of-reach states using an ensemble of \(Q\)-functions. We demonstrate that this is effective in Figure 5.

_(B) Preventing overestimation at these states:_ Once we have detected edge-of-reach states, we may simply apply value pessimism methods from the offline model-free literature. Our choice of an ensemble for part _(A)_ conveniently allows us to minimize over an ensemble of \(Q\)-functions, which effectively adds value pessimism based on ensemble variance.

Our resulting proposal is Reach-Aware Value Learning (RAVL). Concretely, we take the standard offline model-based RL procedure (see Algorithm 1), and simply exchange the _dynamics_ pessimism penalty for _value_ pessimism using minimization over an ensemble of \(N\)\(Q\)-functions:

\[Q_{\phi}^{n}(s,a)\gets r+\gamma\min_{i=1,\dots,N}Q_{\phi}^{i}(s^{\prime}, \pi_{\theta}(s^{\prime}))\quad\text{ for }n=1,\dots,N\] (2)

We include EDAC's [1] ensemble diversity regularizer, and in Section 7 we discuss how the impact of value pessimism differs significantly in the model-based (RAVL) vs model-free (EDAC) settings.

## 6 Empirical Evaluation

In this section, we begin by analyzing RAVL on the simple environment from Section 4. Next, we look at the standard D4RL benchmark, first confirming that RAVL solves the failure seen with the true dynamics, before then demonstrating that RAVL achieves strong performance with the

Figure 4: Experiments on the simple environment, illustrating the edge-of-reach problem and potential solutions. **(a)** Reward function, **(b)** final (failed) policy with naive application of the base procedure (see Algorithm 1), **(c)** final (successful) policy with patching in oracle \(Q\)-values for edge-of-reach states, **(d)** final (successful) policy with RAVL, **(e)** returns evaluated over training, **(f)** mean \(Q\)-values evaluated over training.

learned dynamics. In Appendix F, we include additional results on the challenging pixel-based V-D4RL benchmark on which RAVL now represents a new state-of-the-art. Finally in Section 6.5 we reexamine prior model-based methods and explain why they may work despite not explicitly addressing the edge-of-reach problem. We provide full hyperparameters in Appendix C.2 and ablations showing that RAVL is stable over hyperparameter choice in Appendix G.2.

### Simple Environment

Testing on the simple environment from Section 4 (see green Figure 4), we observe that RAVL behaves the same as the theoretically optimal but practically impossible _Base-OraclePatch_ method. Moreover, in Figure 5, we see that the \(Q\)-value variance over the ensemble is significantly higher for edge-of-reach states, meaning RAVL is detecting and penalizing edge-of-reach states exactly as intended.

### D4RL with the True Dynamics

Next, we demonstrate that RAVL works without dynamics uncertainty and solves the 'failure' observed in Section 3.1. Table 1 shows results on the standard offline benchmark D4RL [7] MuJoCo [34] v2 datasets with the true (zero error, zero uncertainty) dynamics. We see that RAVL learns the near-optimal policies, while existing methods using the base model-based procedure (Algorithm 1) completely fail. In Section 6.5 we examine how this is because existing methods overlook the critical edge-of-reach problem and instead only accidentally address it using dynamics uncertainty metrics. In the absence of model uncertainty, these methods have no correction for edge-of-reach states and hence fail dramatically.

### D4RL with Learned Dynamics

Next, we show that RAVL also performs well with a learned dynamics model. Figure 3 shows RAVL successfully stabilizes the \(Q\)-value explosion of the base procedure, and Table 2 shows RAVL largely matches SOTA, while having _significantly lower runtime_ (see Section 6.4). RAVL gives much higher performance on the Halfcheetah mixed and medium datasets than its model-free counterpart EDAC, and in Section 6.5, we discuss why we would expect the effect of value pessimism in the model-based setting (RAVL) to inherently offer much more flexibility than in the model-free setting (EDAC).

\begin{table}
\begin{tabular}{l l|c c|c c} \hline \hline \multicolumn{2}{c|}{**Dynamics model**} & \multicolumn{2}{c}{**Approximate**} & \multicolumn{2}{c}{**True**} & \multicolumn{2}{c}{**Approximate**} & \multicolumn{2}{c}{**True**} \\ \hline \multirow{4}{*}{Halfcheetah} & medium & 74.6\(\pm\)1.2 & 72.2\(\pm\)4.1 (**1\%**) & 78.7\(\pm\)2.0 & 76.4\(\pm\)3.4 \\  & mixed & 71.7\(\pm\)1.2 & 72.2\(\pm\)4.1 (**1\%**) & 74.9\(\pm\)2.0 & 77.3\(\pm\)2.2 \\  & medexp & 108.2\(\pm\)2.5 & 84.9\(\pm\)17.6 (**22\%**) & 102.1\(\pm\)8.0 & 109.3\(\pm\)1.3 \\ \hline \multirow{4}{*}{Hopper} & medium & 106.6\(\pm\)0.6 & **17.2\(\pm\)11.1 (**184\%**) & 90.6\(\pm\)11.9 & 101.6\(\pm\)0.9 \\  & mixed & 103.9\(\pm\)1.0 & 71.5\(\pm\)32.2 (**,**31\%**) & 103.1\(\pm\)1.1 & 101.1\(\pm\)0.5 \\  & medexp & 112.6\(\pm\)0.2 & **6.1\(\pm\)6.8 (**,**96\%**) & 110.1\(\pm\)1.8 & 107.2\(\pm\)0.9 \\ \hline \multirow{4}{*}{Walker2d} & medium & 87.7\(\pm\)1.1 & **7.2\(\pm\)1.6 (**,**92\%**) & 86.3\(\pm\)1.6 & 101.2\(\pm\)2.3 \\  & mixed & 89.9\(\pm\)1.5 & **7.9\(\pm\)1.9 (**,**91\%**) & 83.0\(\pm\)2.5 & 86.4\(\pm\)3.0 \\ \cline{1-1}  & medexp & 115.2\(\pm\)0.7 & **7.7\(\pm\)2.1 (**,**93\%**) & 115.5\(\pm\)2.4 & 103.5\(\pm\)1.2 \\ \hline \hline \end{tabular}
\end{table}
Table 1: **True dynamics (zero error, zero uncertainty)** Existing model-based methods are presented as different approaches for dealing with dynamics model errors. Surprisingly, however, all existing methods fail in the absence of dynamics errors (when the learned **approximate** model is replaced by the **true** model). This reveals that existing methods are unintentionally using their dynamics uncertainty estimates to address the previously unnoticed edge-of-reach problem. By contrast, RAVL directly addresses the edge-of-reach problem and hence does not fail when dynamics uncertainty is zero. Experiments are on the D4RL MuJoCo v2 datasets. Statistical significance highlighted (6 seeds). _**Note that while labeled as ‘MOBILE’, the results with the true dynamics will be identical for any other dynamics penalty-based method since penalties under the true model are all zero (see Table 7). Hence these results indicate the failure of all existing dynamics uncertainty-based methods._

Figure 5: RAVL’s effective penalty of \(Q\)-ensemble variance on the environment in Section 4, showing that - as intended - edge-of-reach states have significantly higher penalty than within-reach states.

We additionally include results for the challenging _pixel-based_ **V-D4RL benchmark** for which _latent-space models_ are used (in Appendix F), and accompanying **ablation experiments** (in Appendix G.2). In this setting, RAVL represents a new SOTA, giving a performance boost of more than 20% for some environments. These results are particularly notable as the pixel-based setting means the base algorithm (DreamerV2) uses model rollouts in an imagined latent space (rather than the original state-action space as in MBPO). The results therefore give promising evidence that RAVL is able to generalize well to different representation spaces.

### Runtime Discussion

Vectorized ensembles can be scaled with extremely minimal effect on the runtime (see Table 10). This means that, _per epoch_, RAVL is approximately 13% faster than the SOTA (MOBILE, due to MOBILE needing multiple extra forward passes to compute its uncertainty penalty). Furthermore, _in total_, we find that RAVL reliably requires \(3\times\) fewer epochs to converge on all but the medexp datasets, meaning the total runtime is approximately 70% faster than SOTA (see Appendix G.1).

### How can prior methods work despite overlooking the edge-of-reach problem?

While ostensibly to address model errors, we find that existing dynamics penalties _accidentally_ address the edge-of-reach problem: In Figure 6 we see a positive correlation between the penalties used in dynamics uncertainty methods and RAVL's effective penalty of value ensemble variance. This may be expected, as dynamics uncertainty will naturally be higher further away from \(\mathcal{D}_{\text{offline}}\), which is also where edge-of-reach states are more likely to lie. In Section 3.4 we present evidence suggesting that the edge-of-reach problem is likely the dominant source of issues on the main D4RL benchmark, thus indicating that the dynamics uncertainty penalties of existing methods are likely indirectly addressing the edge-of-reach problem. In general, dynamics errors are a second orthogonal problem and RAVL can be easily combined with appropriate dynamics uncertainty penalization [24; 29; 36] for environments where this is a significant issue.

## 7 Related Work

**Model-Based Methods.** Existing offline model-based methods present dynamics model errors and consequent model exploitation as the sole source of issues. A broad class of methods therefore propose reward penalties based on the estimated level of model uncertainty [2; 14; 29; 36], typically using variance over a dynamics ensemble. Rigter et al. [27] aim to avoid model exploitation by setting up an adversarial two-player game between the policy and model. Finally, most related to our method, COMBO [37], penalizes value estimates for state-actions outside model rollouts. However, similarly to Yu et al. [36], COMBO is theoretically motivated by the assumption of infinite horizon model rollouts, which we show overlooks serious implications. Critically, in contrast to our approach, none of these methods address the edge-of-reach problem and thus they fail as environment

\begin{table}
\begin{tabular}{c c|c c c|c c c c c} \hline \hline \multicolumn{2}{c|}{**Environment**} & \multicolumn{2}{c}{**BC**} & **CQL** & **EDAC** & **MOPO** & **COMBO** & **RAMBO** & **MOBILE** & **RAVL (Ours)** \\ \hline \multirow{4}{*}{Halfcheetah} & random & 2.2 & 31.3 & 28.4 & 38.5 & 38.8 & 39.5 & **39.3** & 34.4\(\pm\)2.0 \\  & medium & 43.2 & 46.9 & 65.9 & 73.0 & 54.2 & 77.9 & 74.6 & **78.7\(\pm\)2.0** \\  & mixed & 37.6 & 45.3 & 61.3 & 72.1 & 55.1 & 68.7 & 71.7 & **74.9\(\pm\)2.0** \\  & medexp & 44.0 & 95.0 & **106.3** & 90.8 & 90.0 & 95.4 & **108.2** & **102.1\(\pm\)8.0** \\ \hline \multirow{4}{*}{Hopper} & random & 3.7 & 5.3 & 25.3 & 31.7 & 17.9 & 25.4 & **31.9** & 31.4\(\pm\)0.1 \\  & medium & 54.1 & 61.9 & 101.6 & 62.8 & 97.2 & 87.0 & **106.6** & 90.6\(\pm\)11.9 \\  & mixed & 16.6 & 86.3 & 101.0 & **103.5** & 89.5 & 99.5 & **103.9** & **103.1\(\pm\)1.1** \\  & medexp & 53.9 & 96.9 & 110.7 & 81.6 & 111.1 & 88.2 & **112.6** & 110.1\(\pm\)1.8 \\ \hline \multirow{4}{*}{Walker2d} & random & 1.3 & 5.4 & 16.6 & 7.4 & 7.0 & 0.0 & **17.9** & **17.1\(\pm\)7.9** \\  & medium & 70.9 & 79.5 & **92.5** & 84.1 & 81.9 & 84.9 & 87.7 & 86.3\(\pm\)1.6 \\ \cline{1-1}  & mixed & 20.3 & 76.8 & 87.1 & 85.6 & 56.0 & 89.2 & **89.9** & 83.0\(\pm\)2.5 \\ \cline{1-1}  & medexp & 90.1 & 109.1 & **114.7** & 112.9 & 103.3 & 56.7 & **115.2** & **115.5\(\pm\)2.4** \\ \hline \hline \end{tabular}
\end{table}
Table 2: A comprehensive evaluation of RAVL over the standard D4RL MuJoCo benchmark. We show the mean and standard deviation of the final performance averaged over 6 seeds. Our simple approach largely matches the state-of-the-art without any explicit dynamics penalization and hence works even in the absence of model uncertainty (where dynamics uncertainty-based methods fail) (see Table 1).

models become more accurate. A related phenomenon of overestimation stemming from hallucinated states has been observed in online model-based RL[12].

**Offline Model-Free Methods.** Model-free methods can broadly be divided into two approaches to solving the out-of-sample action problem central to offline model-free RL (see Section 2): action constraint methods and value pessimism-based methods. Action constraint methods [8; 17; 18] aim to avoid using out-of-sample actions in the Bellman update by ensuring selected actions are close to the dataset behavior policy \(\pi^{\beta}\). By contrast, value pessimism-based methods aim to directly regularize the value function to produce low-value estimates for out-of-sample state-actions [1; 16; 19]. The _edge-of-reach_ problem is the model-based equivalent of the _out-of-sample_ action problem, and this unified understanding allows us to transfer ideas directly from model-free literature. RAVL is based on EDAC's [1] use of minimization over a \(Q\)-ensemble [9; 35], and applies this to model-based offline RL.

_What is the effect of value pessimism in the model-based vs model-free settings?_

EDAC [1] can be seen as RAVL's model-free counterpart, however, the impact of value pessimism in the model-free vs the model-based settings is notably different. Recall that, with the ensemble \(Q\)-function trained on \((s,a,r,s^{\prime})\sim\mathcal{D}_{\square}\), the state-actions that are penalized (due to being outside the training distribution) are any \((s^{\prime},a^{\prime})\) that are out-of-distribution with respect to the \((s,a)\)'s in the dataset \(\mathcal{D}_{\square}\). Recall also that updates use values at \((s^{\prime},a^{\prime})\) where \(a^{\prime}\sim\pi_{\theta}\) (see Equation (1)).

_In the model-free case (EDAC):_ The dataset is \(\mathcal{D}_{\square}=\mathcal{D}_{offline}\), meaning the actions \(a\) are effectively sampled from the dataset behavior policy (off-policy), whereas the actions \(a^{\prime}\) are sampled on-policy. This means that EDAC penalizes any \((s^{\prime},a^{\prime})\) where \(a^{\prime}\) differs significantly from the behavior policy.

_In the model-based case (RAVL):_ The dataset is \(\mathcal{D}_{\square}=\mathcal{D}_{rollouts}\), meaning now both \(a\) and \(a^{\prime}\) are sampled on-policy. As a result, the only \((s^{\prime},a^{\prime})\) which will now be out-of-distribution with respect to \((s,a)\) (and hence penalized) are those where the state \(s^{\prime}\) is out-of-distribution with respect to the states \(s\) in \(\mathcal{D}_{rollouts}\). This happens when \(s^{\prime}\) is reachable only in the final step of rollouts, i.e. exactly when \(s^{\prime}\) is "edge-of-reach" (as illustrated in Figure 2). Compared to EDAC, RAVL can therefore be viewed as "relaxing" the penalty and giving the agent _freedom to learn a policy that differs significantly from the dataset behavior policy._ This distinction is covered in detail in Appendix A.

## 8 Conclusion

This paper investigates how offline model-based methods perform as dynamics models become more accurate. As an interesting hypothetical extreme, we test existing methods with the true error-free dynamics. Surprisingly, we find that all existing methods fail. This reveals that using truncated rollout horizons (as per the shared base procedure) has critical and previously overlooked consequences stemming from the consequent existence of 'edge-of-reach' states. We show that existing methods are _indirectly_ and _accidentally_ addressing this edge-of-reach problem (rather than addressing model errors as stated), and hence explain why they fail catastrophically with the true dynamics.

This problem reveals close connections between model-based and model-free approaches and leads us to present a unified perspective for offline RL. Based on this, we propose RAVL, a simple and robust method that achieves strong performance across both proprioceptive and pixel-based benchmarks. Moreover, RAVL _directly_ addresses the edge-of-reach problem, meaning that - unlike existing methods - RAVL does not fail under the true environment model, and has the practical benefit of not requiring dynamics uncertainty estimates. Since improvements to dynamics models are inevitable, we believe that resolving the brittle and unanticipated failure of existing methods under dynamics model improvements is an important step towards 'future-proofing' offline RL.

## 9 Limitations

Since dynamics models for the main offline RL benchmarks are highly accurate, the edge-of-reach effects dominate, and RAVL is sufficient to stabilize model-based training effectively without any explicit dynamics uncertainty penalty. In general, however, edge-of-reach issues could be mixed with dynamics error, and understanding how to balance these two concerns would be useful future work. Further, we believe that studying the impact of the edge-of-reach effect in a wider setting could be an exciting direction, for example investigating its effect as an implicit exploration bias in online RL.

## Acknowledgments and Disclosure of Funding

Anya Sims and Cong Lu are funded by the Engineering and Physical Sciences Research Council (EPSRC). The authors would like to thank the anonymous Agent Learning in Open-Endedness Workshop at NeurIPS 2023 and ICLR 2024 reviewers for positive and constructive feedback which helped to improve the paper. We would also like to thank Philip J. Ball and Shimon Whiteson for helpful feedback and discussions on earlier drafts of this work.

## References

* An et al. [2021] Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offline reinforcement learning with diversified q-ensemble. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, 2021. URL https://proceedings.neurips.cc/paper/2021/file/3d3d286a8d153a4a58156d0e02d8570c-Paper.pdf.
* Ball et al. [2021] Philip J Ball, Cong Lu, Jack Parker-Holder, and Stephen Roberts. Augmented world models facilitate zero-shot dynamics generalization from a single offline environment. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 619-629. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/ball21a.html.
* Brown et al. [2020] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.
* Chebotar et al. [2021] Yevgen Chebotar, Karol Hausman, Yao Lu, Ted Xiao, Dmitry Kalashnikov, Jake Varley, Alex Irpan, Benjamin Eysenbach, Ryan Julian, Chelsea Finn, et al. Actionable models: Unsupervised offline reinforcement learning of robotic skills. _arXiv preprint arXiv:2104.07749_, 2021.
* Chua et al. [2018] Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. In _Advances in Neural Information Processing Systems_, 2018. URL https://proceedings.neurips.cc/paper_files/paper/2018/file/3de568f8597b94bda53149c7d7f5958c-Paper.pdf.
* Ernst et al. [2005] Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning. _Journal of Machine Learning Research (JMLR)_, 6(18):503-556, 2005.
* Fu et al. [2020] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning, 2020.
* Fujimoto and Gu [2021] Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning. In _Thirty-Fifth Conference on Neural Information Processing Systems_, 2021.
* Fujimoto et al. [2018] Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In _Proceedings of the 35th International Conference on Machine Learning_, volume 80 of _Proceedings of Machine Learning Research_. PMLR, 2018. URL https://proceedings.mlr.press/v80/fujimoto18a.html.
* Fujimoto et al. [2019] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pages 2052-2062. PMLR, 09-15 Jun 2019. URL https://proceedings.mlr.press/v97/fujimoto19a.html.
* Haarnoja et al. [2018] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Jennifer Dy and Andreas Krause, editors, _Proceedings of the 35th International Conference on Machine Learning_, volume 80 of _Proceedings of Machine Learning Research_, pages 1861-1870. PMLR, 10-15 Jul 2018. URL https://proceedings.mlr.press/v80/haarnoja18b.html.
* Hau et al. [2018]* Jafferjee et al. [2020] Taher Jafferjee, Ehsan Imani, Erin Talvitie, Martha White, and Micheal Bowling. Hallucinating value: A pitfall of dyna-style planning with imperfect environment models. _arXiv preprint arXiv:2006.04363_, 2020.
* Janner et al. [2019] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy optimization. In _Advances in Neural Information Processing Systems_, 2019.
* Kidambi et al. [2020] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. MOReL: Model-based offline reinforcement learning. In _Advances in Neural Information Processing Systems_, volume 33, 2020. URL https://proceedings.neurips.cc/paper/2020/file/t7efa4f864ae9b88d43527f4b14f750f-Paper.pdf.
* Konda and Tsitsiklis [1999] Vijay Konda and John Tsitsiklis. Actor-critic algorithms. _Advances in neural information processing systems_, 1999.
* Kostrikov et al. [2021] Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement learning with fisher divergence critic regularization. In _Proceedings of the 38th International Conference on Machine Learning_, Proceedings of Machine Learning Research. PMLR, 2021. URL https://proceedings.mlr.press/v139/kostrikov21a.html.
* Kostrikov et al. [2022] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=68n2s9ZJWF8.
* Kumar et al. [2019] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via bootstrapping error reduction. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32, 2019.
* Kumar et al. [2020] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. In _Advances in Neural Information Processing Systems_, volume 33, 2020. URL https://proceedings.neurips.cc/paper/2020/file/0d2b2061826a5df3221116a5085a6052-Paper.pdf.
* Kumar et al. [2021] Aviral Kumar, Anikait Singh, Stephen Tian, Chelsea Finn, and Sergey Levine. A workflow for offline model-free robotic reinforcement learning. _arXiv preprint arXiv:2109.10813_, 2021.
* Kumar et al. [2023] Aviral Kumar, Anikait Singh, Frederik Ebert, Mitsuhiko Nakamoto, Yanlai Yang, Chelsea Finn, and Sergey Levine. Pre-training for robots: Offline rl enables learning new tasks from a handful of trials, 2023.
* Lakshminarayanan et al. [2017] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In _Proceedings of the 31st International Conference on Neural Information Processing Systems_, NIPS'17, 2017.
* Levine et al. [2020] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems, 2020. URL https://arxiv.org/abs/2005.01643.
* Lu et al. [2022] Cong Lu, Philip Ball, Jack Parker-Holder, Michael Osborne, and Stephen J. Roberts. Revisiting design choices in offline model based reinforcement learning. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=zz9hXVhf40.
* Lu et al. [2023] Cong Lu, Philip J. Ball, Tim G. J. Rudner, Jack Parker-Holder, Michael A Osborne, and Yee Whye Teh. Challenges and opportunities in offline reinforcement learning from visual observations. _Transactions on Machine Learning Research_, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=1QqIIGZOWu.
* Rafailov et al. [2021] Rafael Rafailov, Tianhe Yu, Aravind Rajeswaran, and Chelsea Finn. Offline reinforcement learning from images with latent space models. In _Proceedings of the 3rd Conference on Learning for Dynamics and Control_, volume 144 of _Proceedings of Machine Learning Research_, pages 1154-1168. PMLR, 2021.

* Rigter et al. [2022] Marc Rigter, Bruno Lacerda, and Nick Hawes. RAMBO-RL: Robust adversarial model-based offline reinforcement learning. In _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=nrksGSRT7kX.
* Shiranthika et al. [2022] Chamani Shiranthika, Kuo-Wei Chen, Chung-Yih Wang, Chan-Yun Yang, BH Sudantha, and Wei-Fu Li. Supervised optimal chemotherapy regimen based on offline reinforcement learning. _IEEE Journal of Biomedical and Health Informatics_, 26(9):4763-4772, 2022.
* Sun et al. [2023] Yihao Sun, Jiaji Zhang, Chengxing Jia, Haoxin Lin, Junyin Ye, and Yang Yu. Model-Bellman inconsistency for model-based offline reinforcement learning. In _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 33177-33194. PMLR, 23-29 Jul 2023. URL https://proceedings.mlr.press/v202/sun23q.html.
* Sutton [1991] Richard S. Sutton. Dyna, an integrated architecture for learning, planning, and reacting. _SIGART Bull._, 2(4):160-163, jul 1991. ISSN 0163-5719. doi: 10.1145/122344.122377. URL https://doi.org/10.1145/122344.122377.
* Sutton and Barto [2018] Richard S. Sutton and Andrew G. Barto. _Reinforcement Learning: An Introduction_. The MIT Press, second edition, 2018. URL http://incompleteideas.net/book/the-book-2nd.html.
* Tang and Wiens [2021] Shengpu Tang and Jenna Wiens. Model selection for offline reinforcement learning: Practical considerations for healthcare settings. In _Machine Learning for Healthcare Conference_, pages 2-35. PMLR, 2021.
* Tarasov et al. [2022] Denis Tarasov, Alexander Nikulin, Dmitry Akimov, Vladislav Kurenkov, and Sergey Kolesnikov. CORL: Research-oriented deep offline reinforcement learning library. In _3rd Offline RL Workshop: Offline RL as a "Launchpad"_, 2022. URL https://openreview.net/forum?id=SyA549bBcv.
* Todorov et al. [2012] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In _2012 IEEE/RSJ International Conference on Intelligent Robots and Systems_, pages 5026-5033. IEEE, 2012. doi: 10.1109/IROS.2012.6386109.
* van Hasselt et al. [2015] Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning, 2015.
* Yu et al. [2020] Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. MOPO: Model-based offline policy optimization. In _Advances in Neural Information Processing Systems_, 2020. URL https://proceedings.neurips.cc/paper/2020/file/a322852ce0df73e204b7e67cbbef0d0a-Paper.pdf.
* Yu et al. [2021] Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn. Combo: Conservative offline model-based policy optimization. In _Advances in Neural Information Processing Systems_, volume 34, pages 28954-28967, 2021. URL https://proceedings.neurips.cc/paper/2021/file/f29a179746902e331572c483c45e5086-Paper.pdf.

## Supplementary Material

### Table of Contents

* A A Unified Perspective of Model-Based and Model-Free RL
* A.1 Definitions
* A.2 Q-Learning Conditions
* A.3 Comparison Between Model-Free and Model-Based Methods
* B Error Propagation Result
* B.1 Proof
* B.2 Extension to Stochastic Environments and Practical Implementation
* C Implementation Details
* C.1 Algorithm
* C.2 Hyperparameters
* C.3 Pixel-Based Hyperparameters
* D Additional Tables
* D.1 Existing Methods' Penalties All Collapse to Zero Under the True Dynamics
* D.2 Model Errors Cannot Explain the \(Q\)-value Overestimation
* E Additional Visualizations
* E.1 Comparison to Prior Approaches
* E.2 Behaviour of Rollouts Over Training
* F Evaluation on the Pixel-Based V-D4RL Benchmark
* G Runtime and Hyperparameter Sensitivity Ablations
* G.1 Runtime
* G.2 Hyperparameter Sensitivity Ablations
* H Summary of Setups Used in ComparisonsA Unified Perspective of Model-Based and Model-Free RL

We supplement the discussion in Section 3.2 with a more thorough comparison of the out-of-sample and edge-of-reach problems, including how they relate to model-free and model-based approaches.

### Definitions

Consider a dataset of transition tuples \(\mathcal{D}=\{(s_{i},a_{i},r_{i},s^{\prime}_{i},d_{i})\}_{i=1,\ldots,N}\) collected according to some dataset policy \(\pi^{\mathcal{D}}(\cdot|s)\). Compared to Section 2, we include the addition of a _done_ indicator \(d_{i}\), where \(d_{i}=1\) indicates episode termination (and \(d_{i}=0\) otherwise). Transition tuples thus consist of _state_, _action_, _reward_, _nextstate_, _done_. Consider the marginal distribution over state-actions \(\rho^{\mathcal{D}}_{s,a}(\cdot,\cdot)\), over states \(\rho^{\mathcal{D}}_{s}(\cdot)\), and conditional action distribution \(\rho^{\mathcal{D}}_{a|s}(\cdot|s)\). Note that \(\rho^{\mathcal{D}}_{a|s}(\cdot|s)=\pi^{\mathcal{D}}(\cdot|s)\). We abbreviate \(x\)_is in distribution with respect to \(\rho\)_ as \(x\in^{\text{dist}}\rho\).

### Q-Learning Conditions

As described in Section 2, given some policy \(\pi\), we can attempt to learn the corresponding \(Q\)-function with the following iterative process:

\[Q^{k+1}\leftarrow\operatorname*{arg\,min}_{Q}\,\mathbb{E}_{(s,a,r,s^{\prime} )\sim\mathcal{D},a^{\prime}\sim\pi(\cdot|s^{\prime})}[(\underbrace{Q(s,a)}_{ \text{input}}-\underbrace{[r+\gamma(1-d)Q^{k}(s^{\prime},a^{\prime})]}_{\text {Bellman target}})^{2}]\] (3)

\(Q\)-learning relies on bootstrapping, hence to be successful we need to be able to learn accurate estimates of the Bellman targets for all \((s,a)\) inputs. Bootstrapped estimates of \(Q(s^{\prime},a^{\prime})\) are used in the targets whenever \(d\neq 1\). Therefore, for all \((s^{\prime},a^{\prime})\), we require:

_Combined state-action condition_: \((s^{\prime},a^{\prime})\in^{\text{dist}}\rho^{\mathcal{D}}_{s,a}\) or \(d=1\).

In the main paper, we use this combined state-action perspective for simplicity, however, we can equivalently divide this state-action condition into independent requirements on the state and action as follows:

_State condition_: \(s^{\prime}\in^{\text{dist}}\rho^{\mathcal{D}}_{s}\) or \(d=1\),

_Action condition_: \(a^{\prime}\in^{\text{dist}}\rho^{\mathcal{D}}_{a|s}(s^{\prime})\) (given the above condition is met and \(d\neq 1\)).

Informally, the state condition may be violated if \(\mathcal{D}\) consists of partial or truncated trajectories, and the action condition may be violated if there is a significant distribution shift between \(\pi^{\mathcal{D}}\) and \(\pi\).

### Comparison Between Model-Free and Model-Based Methods

In offline model-free RL, \(\mathcal{D}=\mathcal{D}_{\text{offline}}\), with \(\pi^{\mathcal{D}}=\pi^{\beta}\). For the settings we consider, \(\mathcal{D}_{\text{offline}}\) consists of full trajectories and therefore will not violate the state condition. However, this may happen in a more general setting with \(\mathcal{D}_{\text{offline}}\) containing truncated trajectories. By contrast, the mismatch between \(\pi\) (used to sample \(a^{\prime}\) in \(Q\)-learning) and \(\pi^{\beta}\) (used to sample \(a\) in the dataset \(\mathcal{D}_{\text{offline}}\)) often does lead to **significant violation of the action condition**. This exacerbates the overestimation bias in \(Q\)-learning (see Section 7), and can result in pathological training dynamics and \(Q\)-value explosion over training [18].

On the other hand, in offline model-based RL, the dataset \(\mathcal{D}=\mathcal{D}_{\text{rollouts}}\) is collected _on-policy_ according to the current (or recent) policy such that \(\pi^{\mathcal{D}}\approx\pi\). This minimal mismatch between \(\pi^{\mathcal{D}}\) and \(\pi\) means the action condition is not violated and can be considered to be resolved due to the collection of additional data. However, the procedure of generating the data \(\mathcal{D}=\mathcal{D}_{\text{rollouts}}\) can be seen to significantly exacerbate the state condition problem, as the use of short truncated-horizon trajectories means the resulting dataset \(\mathcal{D}_{\text{rollouts}}\) is **likely to violate the state condition**. Due to lack of exploration, certain states may temporarily violate the state condition. Our paper then considers the pathological case of _edge-of-reach_ states, which will always violate the state condition.

This comparison between model-based and model-free is summarized in Table 3

## Appendix B Error Propagation Result

**Proposition 1** (Error propagation from edge-of-reach states).: _Consider a rollout of length \(k\), \((s_{0},a_{0},s_{1},\ldots,s_{k})\). Suppose that the state \(s_{k}\) is edge-of-reach and the approximate value function \(Q^{j}(s_{k},\pi(s_{k}))\) has error \(\epsilon\). Then, standard value iteration will compound error \(\gamma^{k-t}\epsilon\) to the estimates of \(Q^{j+1}(s_{t},a_{t})\) for \(t=1,\ldots,k-1\). (Proof in Appendix B.)_

### Proof

In this section, we provide a proof of Proposition 1. Our proof follows analogous logic to the error propagation result of Kumar et al. [18].

Proof.: Let us denote \(Q^{*}\) as the optimal value function, \(\zeta_{j}(s,a)=|Q_{j}(s,a)-Q^{*}(s,a)|\) the error at iteration \(j\) of Q-Learning, and \(\delta_{j}(s,a)=|Q_{j}(s,a)-\mathcal{T}Q_{j-1}(s,a)|\) the current Bellman error. Then first considering the \(t=k-1\) case,

\[\zeta_{j}(s_{t},a_{t}) =|Q_{j}(s_{t},a_{t})-Q^{*}(s_{t},a_{t})|\] (4) \[=|Q_{j}(s_{t},a_{t})-\mathcal{T}Q_{j-1}(s_{t},a_{t})+\mathcal{T} Q_{j-1}(s_{t},a_{t})-Q^{*}(s_{t},a_{t})|\] (5) \[\leq|Q_{j}(s_{t},a_{t})-\mathcal{T}Q_{j-1}(s_{t},a_{t})|+| \mathcal{T}Q_{j-1}(s_{t},a_{t})-Q^{*}(s_{t},a_{t})|\] (6) \[=\delta_{j}(s_{t},a_{t})+\gamma\zeta_{j-1}(s_{t+1},a_{t+1})\] (7) \[=\delta_{j}(s_{t},a_{t})+\gamma\epsilon\] (8)

Thus the errors at edge-of-reach states are discounted and then compounded with new errors at \(Q^{j}(s_{k-1},a_{k-1})\). For \(t<k-1\), the result follows from repeated application of Equation (7) along the rollout.

### Extension to Stochastic Environments and Practical Implementation

With stochastic transition models (e.g. Gaussian models), we may have the case that no state will truly have zero density, in which case we relax the definition of edge-of-reach states slightly (see Section 3.3) from \(\rho_{t,\pi}(s)=0\) to \(\rho_{t,\pi}(s)<\epsilon\) for some small \(\epsilon\).

During optimization, in practice, model rollouts are sampled in minibatches and thus the above error propagation effect will occur on average throughout training. An analogous model-free statement has been given in Kumar et al. [18]; however, its significance in the context of model-based methods was previously not considered.

## Appendix C Implementation Details

In this section, we provide full implementation details for RAVL.

### Algorithm

We use the base model-based procedure as given in Algorithm 1 and shared across model-based offline RL methods [14; 24; 29; 36]. This involves using short MBPO-style [13] model rollouts to train an agent based on SAC [11]. We modify the SAC agent with the value pessimism losses of EDAC [1]. Our dynamics model follows the standard setup in model-based offline algorithms, being realized as a deep ensemble [5] and trained via maximum likelihood estimation.

\begin{table}
\begin{tabular}{l l l l} \hline \hline  & **Action condition violation** & **State condition violation** & **Main source of issues** \\ \hline
**Model-free** & Yes, common in practice & Yes, but uncommon in practice & \(\rightarrow\) Action condition violation \\
**Model-based** & No & Yes, common in practice & \(\rightarrow\) State condition violation \\ \hline \hline \end{tabular}
\end{table}
Table 3: A summary of the comparison between model-free and model-based offline RL in relation to the conditions on \(Q\)-learning as described in Appendix A.

### Hyperparameters

For the D4RL [7] MuJoCo results presented in Table 2, we sweep over the following hyperparameters and list the choices used in Table 4. "Base" refers to the shared base procedure in model-based offline RL shown in Algorithm 1.

* **(EDAC)** Number of \(Q\)-ensemble elements \(N_{\text{critic}}\), in the range \(\{10,50\}\)
* **(EDAC)** Ensemble diversity weight \(\eta\), in the range \(\{1,10,100\}\)
* **(Base)** Model rollout length \(k\), in the range \(\{1,5\}\)
* **(Base)** Real-to-synthetic data ratio \(r\), in the range \(\{0.05,0.5\}\)

The remaining model-based and agent hyperparameters are given in Table 5. Almost all environments use a small \(N_{\text{critic}}=10\), with only the Hopper-medexp dataset needing \(N_{\text{critic}}=30\).

For the uncertainty-free dynamics model experiments in Table 1 we take the same hyperparameters as for the learned dynamics model for \(N\), \(k\), and \(r\), and try different settings for \(\eta\in\{1,10,100,200\}\). The choices used are shown in Table 4. Note that for existing methods based on dynamics penalties, the analogous hyperparameter (penalty weighting) has no effect under the true dynamics model as the penalties will always be zero (since dynamics uncertainty is zero). For the experiments in Figure 1 with intermediate dynamics model accuracies, we tune the ensemble diversity coefficient \(\eta\) over \(\eta\in\{1,10,100,200\}\) for RAVL, and analogously we tune the uncertainty penalty coefficient \(\lambda\) over \(\lambda\in\{1,5,10,100,200\}\) for MOPO.

Our implementation is based on the Clean Offline Reinforcement Learning (CORL, Tarasov et al. [33]) repository, released at https://github.com/tinkoff-ai/CORL under an Apache-2.0 license. Our algorithm takes on average 6 hours to run using a V100 GPU for the full number of epochs.

### Pixel-Based Hyperparameters

For the V-D4RL [25] DeepMind Control Suite datasets presented in Table 9, we use the default hyperparameters for the Offline DV2 algorithm, which are given in Table 6. We found keeping the

\begin{table}
\begin{tabular}{l l c c c} \multicolumn{1}{c}{**Environment**} & \multicolumn{1}{c}{\(\mathbf{N_{critic}}\)} & \(\boldsymbol{\eta}\) & \(\mathbf{k}\) & \(\mathbf{r}\) \\ \hline \multirow{4}{*}{HalfCheeta} & random & 10 & 10 (1) & 5 & 0.5 \\  & medium & 10 & 1 (1) & 5 & 0.05 \\  & mixed & 10 & 100 (1) & 5 & 0.05 \\  & medexp & 10 & 1 (1) & 5 & 0.5 \\ \hline \multirow{4}{*}{Hopper} & random & 10 & 100 (1) & 5 & 0.05 \\  & medium & 10 & 100 (10) & 1 & 0.5 \\  & mixed & 10 & 10 (10) & 1 & 0.5 \\  & medexp & 30 & 100 (10) & 1 & 0.5 \\ \hline \multirow{4}{*}{Walker2d} & random & 10 & 1 (100) & 5 & 0.05 \\  & medium & 10 & 10 (100) & 1 & 0.5 \\ \cline{1-1}  & mixed & 10 & 10 (100) & 1 & 0.5 \\ \cline{1-1}  & medexp & 10 & 1 (200) & 1 & 0.5 \\ \hline \end{tabular}
\end{table}
Table 4: Variable hyperparameters for RAVL used in D4RL MuJoCo locomotion tasks with the learned dynamics (see Table 2). With the uncertainty-free dynamics (see Table 1), we tune only \(\eta\), with the settings used shown in brackets.

\begin{table}
\begin{tabular}{l l}
**Parameter** & **Value** \\ \hline epochs & 3,000 for medexp; 1,000 for rest \\ gamma & 0.99 \\ learning rate & \(3\times 10^{-4}\) \\ batch size & 256 \\ buffer retain epochs & 5 \\ number of rollouts & 50,000 \\ \hline \end{tabular}
\end{table}
Table 5: Fixed hyperparameters for RAVL used in D4RL MuJoCo locomotion tasks.

uncertainty weight \(\lambda=10\) improved performance over \(\lambda=0\) which shows RAVL can be combined with dynamics penalty-based methods.

We used a hyperparameter sweep over \(\{2,5,20\}\) for N\({}_{\text{critic}}\) but found a single value of \(5\) worked well for all environments we consider.

## Appendix D Additional Tables

### Existing Methods' Penalties All Collapse to Zero Under the True Dynamics

For completeness, in Table 7 we include the expressions for the various dynamics penalties proposed by the most popular model-based approaches. These penalties are all based on estimates of dynamics model uncertainty. Since there is no uncertainty under the true model, all these penalties collapse to zero with the true dynamics.

### Model Errors Cannot Explain the \(Q\)-value Overestimation

In Figure 3 we observe that the base offline model-based procedure results in \(Q\)-values exploding unboundedly to beyond \(10^{10}\) over training. Existing methods attribute this to dynamics model errors and the consequent exploitation of state-action space areas where the model incorrectly predicts high reward.

In Table 8 we examine the rewards collected by an agent in the learned dynamics model. Under the'model errors' explanation, we would expect to see rewards sampled by the agent in the learned model to be significantly higher than under the true environment. Furthermore, to explain the \(Q\)-values being on the order of \(10^{10}\), with \(\gamma=0.99\) the agent would need to sample rewards on the order of \(10^{8}\). However, the per-step rewards sampled by the agent are on the order of \(10^{0}\) (and are not higher than with the true environment).

Model errors and model exploitation therefore cannot explain the explosion in \(Q\)-values seen over training. By contrast, the edge-of-reach problem exactly predicts this extreme value overestimation behavior.

\begin{table}
\begin{tabular}{l l}
**Parameter** & **Value** \\ \hline ensemble member count (K) & 7 \\ imagination horizon (H) & 5 \\ batch size & 64 \\ sequence length (L) & 50 \\ action repeat & 2 \\ observation size & [64; 64] \\ discount (\(\gamma\)) & 0.99 \\ optimizer & Adam \\ learning rate & \{model = \(3\times 10^{-4}\), actor-critic = \(8\times 10^{-5}\)\} \\ model training epochs & 800 \\ agent training epochs & 2,400 \\ uncertainty penalty & mean disagreement \\ uncertainty weight (\(\lambda\)) & 10 \\ number of critics (N\({}_{\text{critic}}\)) & 5 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Hyperparameters for RAVL used in V-D4RL DeepMind Control Suite tasks.

\begin{table}
\begin{tabular}{l c c}
**Offline MBRL Algorithm** & **Penalty** & **Penalty with the true dynamics** \\ \hline MOPO [36] & \(\max_{i=1,\ldots,N}||\Sigma_{i}^{i}(s,a)||_{\text{F}}\) & 0 \\ MOReL [14] & \(\max_{1\leq i,j\leq N}||\mu_{\phi}^{i}(s,a)-\mu_{\phi}^{j}(s,a)||_{2}\) & 0 \\ MOBILE [29] & \(\text{Std}_{i=1,\ldots,N}\{\tilde{T}^{i}(s,a)\}\) & 0 \\ \hline \hline \end{tabular}
\end{table}
Table 7: We show here how all existing dynamics penalized offline MBRL algorithms reduce to the same base procedure when estimated (epistemic) model uncertainty is zero (as it is with the true dynamics).

[MISSING_PAGE_FAIL:19]

## Appendix F Evaluation on the Pixel-Based V-D4RL Benchmark

The insights from RAVL also lead to improvements in performance on the challenging pixel-based benchmark V-D4RL [25]. The base procedure in this pixel-based setting uses the Offline DreamerV2 algorithm of training on trajectories in a _learned latent space_.

We observe in Table 9 that RAVL gives a strong boost in performance on the medium and medexp level datasets, while helping less in the more diverse random and mixed datasets. This observation fits with our intuition of the edge-of-reach problem: the medium and medexp level datasets are likely to have less coverage of the state space, and thus we would expect them to suffer more from edge-of-reach issues and the "edge-of-reach seeking behavior' demonstrated in Section 4.2. The performance improvements over Offline DreamerV2 and other model-based baselines including LOMPO [26] therefore suggests that the edge-of-reach problem is general and widespread.

We also note that these results are without the ensemble diversity regularizer from EDAC [1] (used on the D4RL benchmark), which we anticipate may further increase performance.

## Appendix G Runtime and Hyperparameter Sensitivity Ablations

### Runtime

We compare RAVL to SOTA method MOBILE. The additional requirement for _RAVL compared to MOBILE_ is that RAVL requires increasing the \(Q\)-ensemble size: from \(N=2\) in MOBILE to \(N=10\) in RAVL (for all environments except one where we use \(N=30\)). The additional requirement for _MOBILE compared to RAVL_ is that MOBILE requires multiple additional forwards passes through the model for each update step (for computing their dynamics uncertainty penalty).

\begin{table}
\begin{tabular}{c c|c c c} \multicolumn{4}{l}{**V-D4RL Environment**} & **Offline DV2** & **LOMPO** & **RAVL (Ours)** \\ \hline \multirow{3}{*}{Walker-walk} & random & **28.7\(\pm\)13.0** & 21.9\(\pm\)8.1 & **29.7\(\pm\)2.4** \\  & mixed & **56.5\(\pm\)18.1** & 34.7\(\pm\)19.7 & **49.3\(\pm\)11.3** \\  & medium & 34.1\(\pm\)19.7 & 43.4\(\pm\)11.1 & **48.6\(\pm\)8.7** \\  & medexp & **43.9\(\pm\)34.4** & 39.2\(\pm\)19.5 & **47.6\(\pm\)18.3** \\ \hline \multirow{3}{*}{Cheetah-run} & random & 31.7\(\pm\)2.7 & 11.4\(\pm\)5.1 & **36.9\(\pm\)5.0** \\  & mixed & **61.6\(\pm\)1.0** & 36.3\(\pm\)13.6 & 57.6\(\pm\)1.4 \\ \cline{1-1}  & medium & **17.2\(\pm\)3.5** & 16.4\(\pm\)8.3 & **18.3\(\pm\)4.3** \\ \cline{1-1}  & medexp & **10.4\(\pm\)3.5** & 11.9\(\pm\)1.9 & **12.6\(\pm\)4.4** \\ \hline \multicolumn{4}{l}{Overall} & **35.5\(\pm\)12.0** & 26.9\(\pm\)10.9 & **37.6\(\pm\)7.0** \\ \hline \end{tabular}
\end{table}
Table 9: We show that RAVL extends to the challenging pixel-based V-D4RL benchmark, suggesting the out-of-reach problem is also present in the latent-space setting used by the base procedure for pixel-based algorithms. Mean and standard deviation given over 6 seeds.

Figure 7: A visualization of the rollouts sampled over training on the simple environment in Section 4. We note the pathological behavior of the baseline, and then the success of the ideal intervention Base-OraclePatch, and our practically realizable method RAVL.

In Table 10 we show that ensemble size can be scaled with very minimal effect on the runtime. Even increasing the ensemble size to \(N=100\) (far beyond the maximum ensemble size used by RAVL) only increases the runtime by at most 1%. Table 7 of MOBILE reports that MOBILE's requirement of additional forward passes increases runtime by around 14%.6

Footnote 6: Note that the times reported for EDAC in Table 7 of MOBILE are not representative of practice, since these are with a non-vectorized \(Q\)-ensemble (confirmed from their open-source code), which would require just a simple change to 5 lines of code.

### Hyperparameter Sensitivity Ablations

In Table 11 we include results with different settings of RAVL's ensmble diversity regularizer hyperparameter \(\eta\). We note a minimal change in performance with a sweep across two orders of magnitude. This is an indication that RAVL may be applied to new settings without much tuning.

For RAVL's second hyperparameter of \(Q\)-ensemble size \(N\), we note that in the main benchmarking of RAVL against other offline methods (see Table 2) we use the same value of \(N=10\) across all except one environment. This again indicates that RAVL is robust and should transfer to new settings with minimal hyperparameter tuning.

## Appendix H Summary of Setups Used in Comparisons

Throughout the paper, we compare several different setups in order to identify the true underlying issues in model-based offline RL. We provide a summary of them in Table 12. More comprehensive descriptions of each are given in the relevant positions in the main text and table and figure captions.

\begin{table}
\begin{tabular}{c c c c c}
**Ensemble size (\(N\))** & **2** & **10** & **50** & **100** \\ \hline
**Time relative to \(N=2\)** & (baseline) & \(\times 1.00\pm 0.03\) & \(\times 1.01\pm 0.04\) & \(\times 1.01\pm 0.02\) \\ \end{tabular}
\end{table}
Table 10: Timing experiments showing that ensemble size for standard vectorized ensembles can be scaled with extremely minimal effect on runtime. Runtime increases by just 1% with \(Q\)-ensemble size increased from \(N=2\) (as used in the base procedure SAC agent) to \(N=100\). RAVL uses \(N=10\) (for all environments except one where we use \(N=30\)), meaning the runtime increase compared to the base model-based procedure is almost negligible. Other methods add more computationally expensive changes to the base procedure, hence making RAVL significantly faster than SOTA (see Appendix G.1).

\begin{table}
\begin{tabular}{c c c}
**Hyperparameter \(\eta\)** & **Halfcheetah medium** & **Halfcheetah mixed** \\ \hline
1.0 & \(78.7\pm 2.0\) & \(61.7\pm 5.6\) \\
10.0 & \(74.8\pm 1.5\) & \(73.8\pm 1.9\) \\
100.0 & \(72.1\pm 2.2\) & \(74.9\pm 2.0\) \\ \end{tabular}
\end{table}
Table 11: Ablations results over RAVL’s diversity regularizer hyperparameter \(\eta\).

\begin{table}
\begin{tabular}{c c c c c c}
**
\begin{tabular}{c} _Figures and Tables_ \\ \end{tabular} & **Name Used** & \multicolumn{2}{c}{**Dynamics Model**} & \multicolumn{2}{c}{**Agent**} \\  & & Type & Penalty & \(N_{\text{\bf critic}}\) & \(\eta\) \\ \hline _Table_ 1, _Figure_ 6 & **‘Approximate’** & Ensemble & \(\check{\bm{\mathcal{V}}}\)(Best of Table 7) & \(2\) & \(0\) \\ _Table_ 1 & **‘True’** & True & n/a & \(2\) & \(0\) \\ _Tables_ 8, _Figure_ 3 & **‘Base’** & Ensemble & \(\bm{\mathcal{X}}\) & \(2\) & \(0\) \\ _Tables_ 8, _2, _Figure_ 3 & **‘RAVL (Ours)’** & Ensemble & \(\bm{\mathcal{X}}\) & \(>2\) & \(>0\) \\ _Figures_ 4, 7 & **‘Base’* & True & n/a & \(2\) & \(0\) \\ _Figures_ 4, 5, 7 & **‘RAVL (Ours)’* & True & n/a & \(>2\) & \(>0\) \\ \hline \end{tabular}
\end{table}
Table 12: We summarize the various setups used for comparisons throughout the paper. **‘*’** denotes application to the simple environment (see Section 4). All methods use \(k\)-step rollouts from the offline dataset (or from a fixed starting state distribution in the case of the simple environment).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and introduction clearly define the scope of the paper and its contributions. For our main result (of fixing the failure under the true dynamics model) we provide full results over the standard D4RL benchmark [7] with 6 seeds (see Table 1). For our central hypothesis, we include extensive analysis, both on the standard D4RL benchmark (Section 3.4) and through the construction of a simple environment (Section 4). Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have a dedicated section for this (Section 9) where we discuss the limitations of our analysis and how our work may be extended, in particular how it would be interesting to examine the interactions of the edge-of-reach problem in settings with less accurate dynamics models. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We have one proposition in Appendix B. For this we clearly state all the assumptions, provide a full proof, and also include a discussion of the practical implications and limitations. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide complete details of hyperparameters and training settings in Appendix C. For all the analysis experiments in particular, we aim for the reader to understand exactly what we have done such that they can appreciate the results fully. As such we have described them in detail (for example Section 4), including further clarification of the terms we have used in Table 12. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).

4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We have open-sourced our code at https://anonymous.4open.science/r/edge-of-reach-8096, and have provided a full set of hyperparameters and training settings in Appendix C. The code is clean and has all the defaults set to the exact training settings used. Furthermore, the README contains simple setup instructions and the exact bash command needed to reproduce the main results. All the datasets used are standard open-sourced benchmarks. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: For the benchmark experiments we have provided full detail of the hyperparameters and training settings used in Appendix C. Note also, that we have made sure to match all the default settings exactly in line with the base procedure of other papers in this field to allow for meaningful comparison. For each of the analysis experiments, we have aimed for the reader to completely understand the exact setup such that they can fully appreciate the results. As such we have described the setup of each in detail. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.

7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: All the results are over at least 4 seeds, and we use 6 seeds for the main benchmarking results. We have made sure to provide error bars in all relevant tables and figures, and in each case have stated what they show in the accompanying caption. In tables, we have highlighted statistical significance appropriately where relevant. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We have provided average runtime statistics in Appendix C.2 for the main benchmarking results (including the type of machine), and have also provided detailed comparative timing results and discussion in Appendix G.1. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have read and adhere to the code of ethics. Guidelines:* The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The paper discusses the general potential impact of offline RL in Section 1. In terms of societal impact, the paper is on simulated control and we do not believe it has relevance in the way meant by this question. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risk, we do not release any models or datasets. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets**Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have provided the appropriate reference to the repository that our code is based on in Appendix C including the accompanying license. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We release our code (currently anonymized) at the link provided. It is well documented, with clear straightforward instructions on how to reproduce the experiments. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.