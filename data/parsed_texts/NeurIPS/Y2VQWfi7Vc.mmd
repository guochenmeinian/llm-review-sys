# Expert load matters: operating networks at high accuracy and low manual effort

 Sara Sangalli

Computer Vision Lab

ETH Zurich

sara.sangalli@vision.ee.ethz.ch

&Ertune Erdil

Computer Vision Lab

ETH Zurich

&Ender Konukoglu

Computer Vision Lab, ETH Zurich

The LOOP Zurich - Medical Research Center, Zurich, Switzerland

###### Abstract

In human-AI collaboration systems for critical applications, in order to ensure minimal error, users should set an operating point based on model confidence to determine when the decision should be delegated to human experts. Samples for which model confidence is lower than the operating point would be manually analysed by experts to avoid mistakes. Such systems can become truly useful only if they consider two aspects: models should be confident only for samples for which they are accurate, and the number of samples delegated to experts should be minimized. The latter aspect is especially crucial for applications where available expert time is limited and expensive, such as healthcare. The trade-off between the model accuracy and the number of samples delegated to experts can be represented by a curve that is similar to an ROC curve, which we refer to as _confidence operating characteristic (COC)_ curve. In this paper, we argue that deep neural networks should be trained by taking into account both accuracy and expert load and, to that end, propose a new complementary loss function for classification that _maximizes the area under this COC curve_. This promotes simultaneously the increase in network accuracy and the reduction in number of samples delegated to humans. We perform experiments on multiple computer vision and medical image datasets for classification. Our results demonstrate that the proposed loss improves classification accuracy and delegates less number of decisions to experts, achieves better out-of-distribution samples detection and on par calibration performance compared to existing loss functions.1

Footnote 1: Code is available at: https://github.com/salusanga/aucoc_loss.

## 1 Introduction

Artificial intelligence (AI) systems based on deep neural networks have achieved state-of-the-art results by reaching or even surpassing human-level performance in many predictive tasks [6; 33; 2; 36]. Despite the great potential of neural networks for automation, there are pitfalls when using them in a fully automated setting, especially pertinent for safety-critical applications, such as healthcare [17; 32; 34]. Human-AI collaboration aims at remedying such issues by keeping humans in the loop and building systems that take advantage of both [29]. An example of such human-AI collaboration is hate speech detection for social media [3], where neural networks could reduce the load of manual analysis of contents required by humans. Healthcare is another relevant application [5; 22],. Forexample, a neural network trained to predict whether a lesion is benign or malignant should leave the decision to human doctors if it is likely to make an error [15]. The doctors' domain knowledge and experience could be exploited to assess such, possibly, ambiguous cases and avoid mistakes.

A simple way of building collaboration between a network and a human expert is delegating the decisions to the expert when the network's confidence score for a prediction is lower than a threshold, which we refer to as "operating point". It is clear that the choice of the operating point can only be done through a trade-off between the network performance on the automatically analysed samples, i.e., the number of errors expected by the algorithm, and the number of delegated samples, i.e., experts' workload. The latter is crucial especially for applications where expert time is limited and expensive. For example, in medical imaging, the interpretation of more complex data requires clinical expertise and the number of available experts is limited, especially in low income countries [17]. Hence, predictive models that can analyse a large portion of the samples at high accuracy and identify the few samples that should be delegated to human experts would naturally be more useful with respect to this trade-off.

It is possible to evaluate the performance of a predictive model taking simultaneously into account the accuracy and the number of samples that requires manual assessment from a human expert with a performance curve reminiscent of Receiver Operating Characteristic (ROC) curves, as illustrated in Fig. 1a. We will refer to this performance curve as _Confidence Operating Characteristics (COC)_ as it is similar to the ROC curve. A COC curve plots for a varying threshold on algorithm confidence, i.e., operating point, the accuracy of a model on the samples on which the algorithm is more confident than the threshold versus the number of samples remaining below the threshold. The former corresponds to the accuracy of the model on automatically analysed samples while the latter corresponds to the amount of data delegated to the human expert for analysis. In an ROC curve a balance is sought after between Sensitivity and Specificity of a predictive model, while a COC curve can be used by domain experts, such as doctors, to identify the most suitable balance between _the accuracy on the samples that are automatically analysed and the amount of data delegated to be re-examined by a human_ for the specific task. Variations of this curve have been used to evaluate the performance of automated industrial systems [7].

In this paper, we focus on the trade-off between model accuracy and the amount of samples delegated to domain experts based on operating points on model confidence. Specifically, our goal is to obtain better trade-off conditions to improve the interaction between the AI and the human expert. To this end, we propose a new loss function for multi-class classification, that takes into account both of the aspects by maximizing the area under COC (AUCOC) curve. This enforces the simultaneous increase in neural network accuracy on the samples not analysed by the expert and the reduction in human workload. To the best of our knowledge, this is the first paper to include the optimization of such curve during the training of a neural network, formulating it in a differentiable way. We perform experiments on two computer vision and three medical image datasets for multi-class class classification. We compare the proposed complementary AUCOC loss with the conventional loss functions for training neural networks as well as network calibration methods. The results demonstrate that our loss function complements other losses and improved both accuracy and AUCOC. Additionally, we evaluate network calibration and out-of-distribution (OOD) samples detection performance of networks trained with different losses. The proposed approach was also able to consistently achieve better OOD samples detection and on par network calibration performance.

## 2 Related Work

For the performance analysis of human-AI collaborative systems, confidence operating characteristics (COC) curves can be employed, which plot network accuracy on accepted samples against manual workload of a human expert, e.g as in [7]. While such curves have been used, to the best of our knowledge, we present the first work that defines a differentiable loss based on COC curve, in order to optimize neural networks to take into account simultaneously accuracy and experts' load for a human-AI collaborative system. Thus, there is no direct literature with which we can compare.

The underlying assumption in deciding which sample to delegate to human expert based on a threshold on confidence scores is that these scores provided by deep learning models indicate how much the predictions are likely to be correct or incorrect. However, the final softmax layer of a network does not necessarily provide real probabilities of correct class assignments. In fact, modern deep neural networks that achieve state-of-the-art results are known to be overconfident even in their wrong predictions. This leads to networks that are not well-calibrated, i.e., the confidence scores do not properly indicate the likelihood of the correctness of the predictions [8]. Network calibration methods mitigate this problem by calibrating the output confidence scores of the model, making the above mentioned assumption hold true. Thus, we believe that the literature on network calibration methods is the closest to our setting because they also aim at improving the interaction between human and AI, by enforcing correlation between network confidence and accuracy, so that confidence can be used to separate samples where networks' predictions are not reliable and should be delegated to human experts. Calibration methods aim to get models that are highly accurate in the samples they are confident, but not the problem of minimising the number of samples delegated to human experts, contrary to the loss proposed here.

Guo et al. [8] defines the calibration error as the difference in expectation between accuracy and confidence in each confidence bin. One category of calibration methods augments or replaces the conventional training losses with another loss to explicitly encourage reducing the calibration error. Kumar et al. [21] propose the MMCE loss by replacing the bins with kernels to obtain a continuous distribution and a differentiable measure of calibration. Karandikar et al. [16] propose two losses for calibration, called Soft-AvUC and Soft-ECE, by replacing the hard confidence thresholding in AvUC [18] and binning in ECE [8] with smooth functions, respectively. All these three functions are used as a secondary loss along with conventional losses such as cross-entropy. Mukhoti et al. [26] find that Focal Loss (FL) [23] provides inherently more calibrated models, even if it was not originally designed for this, as it adds implicit weight regularisation. The authors further propose Adaptive Focal Loss (AdaFL) with a sample-dependent schedule for the choice of the hyperparameter \(\gamma\). The second category of methods are post-hoc calibration approaches, which rescale model predictions after training. Platt scaling [31] and histogram binning [40] fall into this class. Temperature scaling (TS) [8] is the most popular approach of this group. TS scales the logits of a neural network, dividing them by a positive scalar, such that they do not saturate after the subsequent softmax activation. TS can be used as a complementary method and it does not affect model accuracy, while significantly improving calibration. A recent work by Gupta et al. [9] fits a spline function to the empirical cumulative distribution to re-calibrate post-hoc the network outputs. They also present a binning-free calibration measure inspired by the Kolmogorov-Smirnov (KS) statistical test. Lin et al. [24] propose a Kernel-based method on the penultimate-layer latent embedding using a calibration set.

## 3 Methods

In this section, we illustrate in detail the _Confidence Operating Characteristics (COC)_ curve. Then, we describe the proposed complementary cost function to train neural networks for classification: _the area under COC (AUCOC) loss (AUCOCLoss)_.

### Notation

Let \(D=\langle(x_{n},y_{n})\rangle_{n=1}^{N}\) denote a dataset composed of \(N\) samples from a joint distribution \(\mathcal{D}(\mathcal{X},\mathcal{Y})\), where \(x_{n}\in\mathcal{X}\) and \(y_{n}\in\mathcal{Y}=\{1,2,...,K\}\) are the input data and the corresponding class label, respectively. Let \(f_{\theta}(y|x)\) be the probability distribution predicted by a classification neural network \(f\) parameterized by \(\theta\) for an input \(x\). For each data point \(x_{n}\), \(\hat{y}_{n}=\operatorname*{argmax}_{y\in\mathcal{Y}}f_{\theta}(y|x_{n})\) denotes the predicted class label, associated to a _correctness score_\(c_{n}=\mathbbm{1}(\hat{y}_{n}=y_{n})\) and to a _confidence score_\(r_{n}=\max_{y\in\mathcal{Y}}f_{\theta}(y|x_{n})\), where \(r_{n}\in[0,1]\) and \(\mathbbm{1}(.)\) is an indicator function. \(\mathbf{r}=[r_{1},...r_{N}]\) represents the vector containing all the predicted confidences for a set of data points, e.g., a batch. \(p(r)\) denotes the probability distribution over \(r\) values (confidence space). We assume a human-AI collaboration system where samples with confidence \(r\) lower than a _threshold_\(r_{0}\) would be delegated to a human expert for assessment.

### Confidence Operating Characteristics (COC) curve

Our first goal is to introduce an appropriate evaluation method to assess the trade-off between a neural network's prediction accuracy and the number of samples that requires manual analysis from a domain expert. We focus on the COC curve, as it provides practitioners with flexibility in the choice of the operating point, similarly to the ROC curve.

#### 3.2.1 x-y axes of the COC curve

To construct the COC curve, first, we define a _sliding threshold_\(r_{0}\) over the space of predicted confidences \(r\). Then, for each threshold \(r_{0}\), we calculate the portion of samples that are delegated to human expert and the accuracy of the network on the remaining samples for the threshold \(r_{0}\), which form the **x-axis** and **y-axis** of a COC curve, respectively. These axes are formulated as follows

\[x-\text{axis}:\ \tau_{0}=p(r<r_{0})=\int_{0}^{r_{0}}p(r)dr,\qquad\quad y- \text{axis}:\mathbb{E}[c|r\geq r_{0}]\] (1)

For each threshold level \(r_{0}\), \(\tau_{0}\) represents the portion of samples whose confidence is lower than that threshold, i.e., the amount of the samples that are delegated to the expert. \(\mathbb{E}[c|r\geq r_{0}]\) corresponds to the expected value of the correctness score \(c\) for all the samples for which the network's confidence is equal or larger than \(r_{0}\), i.e., accuracy among the samples for which network prediction will be used. This expected value, i.e., y-axis, can be computed as

\[\mathbb{E}[c|r\geq r_{0}]=\frac{1}{1-\tau_{0}}\int_{r_{0}}^{1}\mathbb{E}[c|r ]p(r)dr.\] (2)

We provide the derivation of Eq. 2 in the Appendix A.

#### 3.2.2 Area under COC curve

Like the area under ROC curve, area under COC curve (AUCOC) is a global indicator of the performance of a system. Higher AUCOC indicates lower number of samples delegated to human experts or/and higher accuracy for the samples that are not delegated to human experts but analysed only by the network. Lower AUCOC on the other hand, indicates higher number of delegations to human experts or/and lower accuracy on the samples analysed only by the network. Further explaination is reported in Appendix I. It can be computed by integrating the COC curve over the whole range of \(\tau_{0}\in[0,1]\):

\[AUCOC=\int_{0}^{1}\mathbb{E}[c|r\geq r_{0}]d\tau_{0}=\int_{0}^{1}\left\{\int_ {r_{0}}^{1}\mathbb{E}[c|r]p(r)dr\right\}\frac{d\tau_{0}}{1-\tau_{0}}\] (3)

Further details are provided in Section 3.5

### AUCOCLoss: Maximizing AUCOC for training neural networks

As mentioned, in human-AI collaboration systems, higher AUCOC means a better model. Therefore, in this section we introduce a new loss function called AUCOCLoss that maximizes AUCOC for training classification neural networks.

AUCOC's explicit maximization would enforce the reduction of the number of samples delegated to human expert while maintaining the accuracy level on the samples assessed only by the algorithm (i.e., keeping \(\mathbb{E}[c|r\geq r_{0}]\) constant) and/or the improvement in the prediction accuracy of the samples analysed only by the algorithm while maintaining a particular amount of data to be delegated to the human (i.e., keeping \(\tau_{0}\) constant), as illustrated in Figure 0(a).

We define our loss function to maximize AUCOC as

\[AUCOCLoss=-\log(AUCOC).\] (4)

We use the negative logarithm as AUCOC lies in the interval \([0,1]\), which corresponds to AUCOCLoss \(\in[0,\inf]\) which is suitable for minimizing cost functions. The dependence of AUCOC on the network parameters may not be obvious from the formulation given in Eq. 3. Indeed, this dependence is hidden in how \(p(r)\) and \(\mathbb{E}[c|r]\) are estimated as we show next.

#### 3.3.1 Kernel density estimation for AUCOC

We need to formulate AUCOC in a differentiable way in order for it to be incorporated in a cost function for the training of a neural network. For this purpose, we use kernel density estimation (KDE) on confidence predictions \(r_{n}\) for training samples to estimate \(p(r)\) used in Eq. 3

\[p(r)\approx\frac{1}{N}\sum_{n=1}^{N}K(r-r_{n})\] (5)where \(K\) is a Gaussian kernel and we choose its bandwidth using Scott's rule of thumb [35]. Then, the other terms in Eq. 3, namely \(\mathbb{E}[c|r]p(r)\) and \(\tau_{0}\), are estimated as

\[\mathbb{E}[c|r]p(r)\approx\frac{1}{N}\sum_{n=1}^{N}c_{n}K(r-r_{n}),\qquad\quad \tau_{0}\approx\frac{1}{N}\int_{0}^{r_{0}}\sum_{n=1}^{N}K(r-r_{n})dr.\] (6)

Note that \(r_{n}=\max_{y\in\mathcal{Y}}f_{\theta}(y|x_{n})\), from where the dependency of AUCOC on \(\theta\) stems. Further, note that \(\tau_{0}\) is the x-axis of the COC curve. The AUCOC is defined by integrating over the \(\tau_{0}\) values, and therefore \(\tau_{0}\) should not depend on the network parameters, i.e., its derivative with respect to \(\theta\) should be equal to zero. We can write the derivative using Leibniz integral rule as follows:

\[\frac{d\tau_{0}}{d\theta}=\int_{0}^{r_{0}}\frac{dp(r)}{d\theta}dr+p(r_{0}) \frac{dr_{0}}{d\theta}=0\] (7)

Then, the constraint that \(\tau_{0}\) should not depend on \(\theta\) can be enforced explicitly by deriving the derivative \(dr_{0}/d\theta\) from Eq. 7 as follows:

\[\frac{dr_{0}}{d\theta}=-\frac{\int_{0}^{r_{0}}\frac{dp(r)}{d\theta}dr}{p(r_{0})}\] (8)

where \(p(r)\) is implemented as in Eq. 5. Derivations are provided in the Appendix.

### Toy example: added value by AUCOC

In this section, we demonstrate the added value of assessing the performance of a predictive model using COC curve and AUCOC through a toy example. We particularly compare with the widely used expected calibration error (ECE) [8], a binning-free calibration metric called Kolmogorov-Smirnov (KS) [9] and classification accuracy.

Assume we have two classification models \(f_{\theta_{1}}\) and \(f_{\theta_{2}}\) and they yield confidence scores and predictions for 5 samples as shown in Figure 1b. The green circles denote the predicted confidences for correctly classified samples, while the red crosses the confidences of the misclassified ones.

**ECE:** ECE divides the confidence space into bins, computes the difference between the average accuracy and confidence for each bin, and returns the average of the differences as final measure of calibration error. If we divide the confidence space into 5-bins, as indicated with the gray dotted lines in the confidence spaces of \(f_{\theta_{1}}\) and \(f_{\theta_{2}}\), ECEs computed for the both models will be identical. Furthermore, a similar situation can be constructed for any number of bins.

**KS:** KS is a binning-free metric, so it is less prone to binning errors than ECE. However, one can prove that there exist some confidence configurations for which the two models also report the same

Figure 1: **(a)** shows how to improve AUCOC, 1) increasing the accuracy of the network and/or 2) decreasing the amount of data to be analysed by the domain expert. The pink curve has higher AUCOC than the blue one. **(b)** illustrates a toy example where two models have the same accuracy, ECE with 5 bins and KS. However, they have different AUCOC values due to different ordering of correctly and incorrectly classified samples according to the assigned confidence by the network.

KS, in spite of it being a binning-free metric. This happens, for example, if the confidence values are (from left to right): 0.45, 0.55, 0.65, 0.70, 0.75.

**Accuracy:** These models have equal classification accuracy, 3/5 for both.

Therefore, looking at these three performance metrics, it is not possible to choose one model over the other since \(f_{\theta_{1}}\) and \(f_{\theta_{2}}\) perform identically.

**AUCOC:** On the contrary, the AUCOC is larger for \(f_{\theta_{1}}\) than for \(f_{\theta_{2}}\), as shown in Figure 1b. The difference in AUCOC is due to the _different ranking_ of correctly and incorrectly classified samples with respect to confidence values. It does not depend on a particular binning nor exact confidence values, but only on the ranking. By looking at the AUCOC results, one would prefer \(f_{\theta_{1}}\) compared to \(f_{\theta_{2}}\). Indeed, \(f_{\theta_{1}}\) is a better model than \(f_{\theta_{2}}\) because it achieves either equal or better accuracy than \(f_{\theta_{2}}\), for the same amount of data to be manually examined. Changing point of view, \(f_{\theta_{1}}\) delegates either equal or lower number of samples to experts for the same accuracy level.

**Comparison of AUCOC and calibration:** As also demonstrated with the toy example, AUCOC and calibration assess different aspects of a model. The former is a rank-based metric with respect to the ordering of the predictive confidences of correctly and incorrectly classified samples. The latter instead assess the consistency between the accuracy of the network and the predictive confidences, i.e. it is sensitive to the numerical values of the confidences themselves.

### Implementation Details

**Construction of the COC curve, thresholds \(r_{0}\) and operating points:** To provide flexibility in the selection of COC operating points, we need to cover the entire range \([0,1]\) of \(\tau_{0}\) values. As a consequence, the thresholds \(r_{0}\) need to span the confidence range of the predictions \(r_{n}\) of the neural network. A natural choice for such thresholds is employing the predictions \(r_{n}\) themselves. This spares us from exploring the confidence space with arbitrarily fine-grained levels. First, we sort the confidences \(\mathbf{r}\) of the whole dataset (or batch) in ascending order. Each predicted confidence is then selected as threshold level \(r_{0}\) for the vector \(\mathbf{r}\), corresponding to a certain \(\tau_{0}\) (x-value). Subsequently, \(\mathbb{E}[c|r\geq r_{0}]\) (y-value) is computed. Note that testing the threshold to each \(r_{n}\) in the sorted array corresponds to going through \(\tau_{0}=[1/N,2/N,\ldots,(N-1)/N,1]\) for \(N\) samples one by one in order.

**Modelling correctness:** Instead of using \(\mathbb{E}[c|r]p(r)\approx\frac{1}{N}\sum_{n=1}^{N}c_{n}K(\|r-r_{n}\|)\) as given in Eq. 6, we approximate it as \(\mathbb{E}[c|r]p(r)\approx\frac{1}{N}\sum_{n=1}^{N}r^{*}K(\|r-r_{n}\|)\) where \(r_{n}^{*}=f_{\theta}(y_{n}|x_{n})\) is the confidence of the correct class for a sample \(n\). The main reason is that the gradient of the misclassified samples becomes zero because \(c_{n}\) is zero when a sample \(x_{n}\) is not classified correctly. To deal with this issue we replace the correctness score \(c_{n}\), which can be either 0 or 1, with \(r_{n}^{*}\) which can take continuous values between 0 and 1, following Yin et al. [39]. With this new approximation, we can back-propagate through misclassified samples and we found that this leads to better results.

**Use as secondary loss:** We observed in our experiments that using AUCOCLoss alone to train a network leads to very slow convergence with the existing optimization techniques. We believe this is due to the fact that the AUCOCLoss is a \(-log(\sum_{n}z_{n})\) with \(z_{n}\in[0,1]\). Optimization of such a form with gradient descent is slow because the contribution of increasing low \(z_{n}\)'s to the loss function is small. In contrast, cross-entropy is a \(-\sum_{n}\log z_{n}\), where contribution of increasing low \(z_{n}\)'s to the loss is much larger, hence gradient-based optimization is faster. One can in theory create an upper bound to AUCOC loss by pulling the \(\log\) inside the sum, as we show in the Appendix. However, when \(z_{n}\in[0,1]\) this upper bound is very loose and minimizing the upper bound not necessarily correspond to minimizing the AUCOC loss, hence does not maximize AUCOC. On the contrary, when AUCOCLoss is complements a primary cost that is faster to optimize, such as cross-entropy, it is improves over the primary loss and lead to the desired improved AUCOC while preserving the accuracy. This is obtained within the same amount of epochs and without ad-hoc fine-tuning the training hyper-parameters for the secondary loss.

## 4 Experiments

In this section, we present our experimental evaluations on multi-class image classification tasks. We performed experiments on five datasets. We experimented with CIFAR100 [19] and Tiny-ImageNet, a subset of ImageNet [4], following the literature on network calibration. Further, we used two publicly available medical imaging datasets, DermaMNIST and RetinaMNIST [38], since medical imaging is an application area where expert time is limited and expensive. Due to space reasons we report results on a third medical dataset, TissueMNIST [38], in Appendix D, as well as information about the datasets in Appendix C.

**Loss functions:** We compared AUCOCLoss (referred to as AUCOCL in the tables) with different loss functions, most of which are designed to improve calibration performance while preserving accuracy: cross-entropy (CE), focal-loss (FL) [23], adaptive focal-loss (AdaFL) [26], maximum mean calibration error loss (MMCE) [21], soft binning calibration objective (S-ECE) and soft accuracy versus uncertainty calibration (S-AvUC) [16]. We optimized MMCE, S-ECE, and S-AvUC losses jointly with a primary loss for which we used either CE or FL, consistently with the literature [21; 16]. The same is done for AUCOCLoss, applying KDE batch-wise during the training.

**Evaluation metrics:** To evaluate the performance of the methods, we used classification accuracy and AUCOC. Classification accuracy is simply the ratio between the number of correct samples over the total number of samples, and AUCOC is computed using Eq. 3. We also report some examples of operating points of COC curve - given a certain accuracy, we show the corresponding expert load (\(\tau_{0}\) @acc), i.e., percentage of samples that need to be analyzed manually, on the COC curves.

Since we compared AUCOCLoss mostly with losses for network calibration, we also assessed the calibration performance of the networks, even though calibration is not a direct goal of this work. To this end, we used the following metrics: the widely employed equal-mass expected calibration error (ECE) [28] with 15 bins, the binning-free Brier score [1] and Kolmogorov-Smirnov (KS) score [9] and the class-wise ECE (cwECE) [20]. The evaluation is carried out post temperature scaling (TS) [8], as it has been proved to be always beneficial for calibration.

**Hyperparameters:** In addition to the common hyperparameters for all losses, selected as specified in Appendix C, there are also specific ones that need to be tuned for some of them. In this case, we used the best hyperparameter settings reported in the original papers. In cases where the original paper did not report the specific values, we carried out cross-validation and selected the setup that provided the best performance on the validation set. We also selected the weighting factor for AUCOCLoss in the same way. We found that optimal weighting values for AUCOCLoss all fell between 1 and 10. We found empirically that models check-pointed using ECE provided very poor results. Networks check-pointed using either accuracy or AUCOC provided comparable outcome with respect to accuracy, therefore we reported results on AUCOC as they provided the best overall performance.

**OOD experiments:** We evaluate the out-of-distribution (OOD) samples detection performance of all methods since OOD detection is crucial for a reliable AI system and it is a common experiment in the network calibration literature. The commonly used experimental setting in this literature is using CIFAR100-C [12] (we report results for CIFAR100 with Gaussian noise in the main paper and the average over all the perturbations in the Appendix) and SVHN [27] as OOD datasets, while the network is trained on CIFAR100 (in-distribution). We evaluated the OOD detection performance of all methods using Area Under the Receiver Operating Characteristics (AUROC) curve, with MSP [13], ODIN [14], MaxLogit [11] and EBM [25].

**Class imbalance experiments:** Finally, in Appendix E we report results for accuracy and AUCOC on imbalanced datasets, being class imbalance present in many domains, such as medical imaging. We report results on the widely used Long-Tailed CIFAR100 (CIFAR100-LT) [37; 42] with controllable degrees of data imbalance ratio to control the distribution of training set. We trained with three levels of imbalance ratio, namely 100, 50, 10.

Further details on hyper-parameter settings are provided in the the Appendix.

## 5 Results

First, we report the results for accuracy and COC-related metrics. Then, we report results for calibration, even though this is not an explicit goal of this work. Bold results indicate the methods that performed best for each metric, underlined results are the second best. \(\uparrow\) means the higher the better for a metric, while \(\downarrow\) the lower the better. The experiments results are averaged over three runs.

In Tables 1, 2 we present our results based on accuracy and AUCOC. The results for TissueMNIST can be found in the Appendix. We observed that complementing CE and FL with our loss, i.e., CE+AUCOCL and FL+AUCOCL, they were consistently better than the other losses in all experiments. To evaluate the significance of the accuracy and AUCOC results, we performed permutation test [30] between the best AUCOCLoss and the best baseline with 1000 rounds. In all the cases it provided a p score \(\sim 1\%\), therefore the differences in the models are steadily significant. The advantage of our model was even more apparent in amount of expert loads corresponding to specific accuracy levels. In particular, we measured the percentage of samples delegated to expert (\(\tau_{0}\)) at 90% and 95% accuracy for CIFAR100, DermaMNIST and TissueMNIST, and at 65% and 75% accuracy for Tiny-Imagenet and RetinaMNIST (as the initial accuracy is also much lower on these datasets). In all the experiments, to varying degrees, AUCOCLoss provided lower delegated samples than the baselines. Noticeably, while some of the baselines may be close to the results of AUCOCLoss for certain metrics, none of them were consistently close to AUCOCLoss across all the datasets. For example, when looking at the last two columns for CIFAR100 and Tiny-ImageNet respectively of Table 1, CE+MMCE is worse by 4% than AUCOCLoss on Tiny-ImageNet, but by around 9% in

\begin{table}
\begin{tabular}{l|r r r r|r r r} \hline \hline Dataset & \multicolumn{4}{c|}{CIFAR100} & \multicolumn{4}{c}{Tiny-ImageNet} \\ \hline  & \multicolumn{4}{c}{\(\tau_{0}\downarrow\) @ acc.} & \multicolumn{4}{c}{\(\tau_{0}\downarrow\) @ acc.} \\ Loss funct. & AUCOC \(\uparrow\) & Acc. \(\uparrow\) & 90\% & 95\% & AUCOC \(\uparrow\) & Acc. \(\uparrow\) & 65\% & 75\% \\ \hline CE & 91,43 & 75,71 & 29,03 & 44,61 & 72,56 & 47,39 & 39,88 & 56,29 \\ FL (\(\gamma\)=3) & 93,91 & 77,83 & 24,71 & 40,48 & 73,12 & 47,71 & 38,40 & 55,08 \\ AdaFL53 & 93,89 & 77,64 & 25,08 & 40,74 & 73,19 & 47,81 & 38,56 & 55,20 \\ CE+MMCE & 92,42 & 75,35 & 30,01 & 44,71 & 72,47 & 47,11 & 39,94 & 56,70 \\ FL+MMCE & 93,90 & 77,78 & 25,62 & 40,90 & 73,51 & 48,03 & 37,83 & 55,15 \\ CE+S-AvUC & 93,99 & 77,65 & 24,62 & 45,00 & 72,92 & 47,89 & 38,28 & 54,91 \\ FL+ S-AvUC & 93,97 & 77,64 & 25,97 & 40,82 & 74,30 & 48,69 & 35,83 & 53,21 \\ CE+S-ECE & 93,88 & 77,57 & 24,76 & 40,14 & 72,94 & 47,65 & 38,81 & 55,84 \\ FL+S-ECE & 93,41 & 76,69 & 28,13 & 43,29 & 72,61 & 47,40 & 39,94 & 56,71 \\ \hline
**CE+AUCOCL** & **94,49** & **78,94** & **21,60** & 36,73 & **74,56** & 49,10 & **34,78** & **52,01** \\
**FL+AUCOCL** & 94,18 & 78,31 & 23,50 & **36,68** & 74,30 & **49,19** & 34,85 & 53,15 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Test results on the natural datasets CIFAR100 and Tiny-Imagenet. We report AUCOC, accuracy for both, \(\tau_{0}\) at 90% and 95% accuracy for CIFAR100 and \(\tau_{0}\) at 65% and 75% accuracy for Tiny-ImageNet, as the initial accuracy is also lower. In bold the best result for each metric, underlined the second best. AUCOCL improves, to varying degrees, the baselines in all metrics.

\begin{table}
\begin{tabular}{l|r r r r|r r r} \hline \hline Dataset & \multicolumn{4}{c|}{DermaMNIST} & \multicolumn{4}{c}{RetinaMNIST} \\ \hline  & \multicolumn{4}{c|}{\(\tau_{0}\downarrow\) @ acc.} & \multicolumn{4}{c}{\(\tau_{0}\downarrow\) @ acc.} \\ Loss funct. & AUCOC \(\uparrow\) & Acc. \(\uparrow\) & 90\% & 95\% & AUCOC \(\uparrow\) & Acc. \(\uparrow\) & 65\% & 75\% \\ \hline CE & 89,84 & 71,59 & 43,51 & 56,21 & 71,45 & 52,10 & 39,53 & 68,58 \\ FL (\(\gamma\)=3) & 90,50 & 72,64 & 40,63 & 53,90 & 68,57 & 52,25 & 44,25 & 58,25 \\ AdaFL53 & 90,11 & 73,10 & 40,78 & 55,86 & 68,85 & 48,58 & 48,67 & 62,00 \\ CE+MMCE & 89,71 & 70,99 & 45,82 & 58,07 & 69,18 & 48,50 & 42,92 & 69,13 \\ FL+MMCE & 89,34 & 71,72 & 46,81 & 59,10 & 67,08 & 50,33 & 47,50 & 78,91 \\ CE+S-AvUC & 89,67 & 71,51 & 43,04 & 57,09 & 68,15 & 51,42 & 42,71 & 62,29 \\ FL+S-AvUC & 89,42 & 71,04 & 45,47 & 58,69 & 66,80 & 52,00 & 45,58 & 70,54 \\ CE+S-ECE & 89,54 & 71,46 & 43,48 & 57,82 & 71,40 & 52,05 & 39,45 & 55,83 \\ FL+S-ECE & 90,22 & 72,62 & 40,95 & 74,76 & 70,49 & 51,33 & 42,42 & 79,91 \\ \hline
**CE+AUCOCL** & 90,87 & 74,30 & 39,01 & **52,70** & **72,47** & 53,10 & **38,33** & **53,81** \\
**FL+AUCOCL** & **91,35** & **74,80** & **37,30** & 53,90 & 72,31 & **53,58** & 39,42 & 56,12 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Test results on the medical datasets DermaMNIST and RetinaMNIST. We report AUCOC, accuracy for both, \(\tau_{0}\) at 90% and 95% accuracy for DermaMNIST and \(\tau_{0}\) at 65% and 75% accuracy for RetinaMNIST, as the initial accuracy is also lower. In bold the best result for each metric, underlined the second best. AUCOCL improves, to varying degrees, the baselines in all metrics.

CIFAR100. Figure 1a shows examples of COC curves. Overall, the plot of AUCOCLoss lies above all the baselines, which is a desirable behavior as it corresponds to better operating points.

Even though the proposed loss was not designed to improve calibration, it provided on par performance compared to the other cost functions particularly designed for network calibration, as reported in Table 3 for DermaMNIST and RetinaMNIST and in the Appendix for the other datasets.

In OOD experiments reported in Table 4, we used the model trained on CIFAR100 and evaluated the OOD detection performance on CIFAR100-C (with Gaussian noise) and SVHN dataset. Results for CIFAR100-C averaged over all the perturbations are reported in the Appendix. We employed state-of-the-art OOD detectors, namely MSP, ODIN, MaxLogit and EBM to determine whether a sample is OOD or in-distribution. The bold results highlight the best results in terms of AUROC. On both OOD datasets, AUCOCLoss always provided the highest AUROC and in almost all the cases also the second best.

Class imbalance experiments on CIFAR100-LT are reported in the Appendix. AUCOCLoss obtains best results for both accuracy and AUCOC, with higher benefits at increasing imbalance.

Crucially, **CE+AUCOCL**, where the proposed loss is used jointly with CE, outperformed every baseline in accuracy, AUCOC and \(\tau_{0}\) @acc. in all the experiments. It further outperformed all the baselines in OOD detection and class imbalance experiments, presented in the Appendix. Even

\begin{table}
\begin{tabular}{l|c c c|c c c c} \hline \hline Dataset & \multicolumn{4}{c|}{**C100-C** AUROC\(\uparrow\)} & \multicolumn{4}{c}{**SVHN** AUROC\(\uparrow\)} \\ Loss funct. & MSP & ODIN & MaxLogit & EBM & MSP & ODIN & MaxLogit & EBM \\ \hline CE & 74,37 & 75,22 & 74,51 & 66,18 & 77,42 & 79,42 & 67,71 & 67,71 \\ FL (\(\gamma\)=3) & 75,12 & 75,35 & 73,43 & 72,83 & 76,61 & 77,08 & 66,69 & 66,42 \\ AdaFL53 & 74,53 & 74,68 & 71,29 & 70,58 & 80,3 & 81,28 & 66,94 & 66,73 \\ CE+MMCE & 74,67 & 74,53 & 70,67 & 70,17 & 75,79 & 77,39 & 66,34 & 66,23 \\ FL+MMCE & 74,42 & 74,39 & 70,01 & 68,70 & 77,57 & 77,41 & 66,97 & 66,60 \\ CE+S-AVUC & 73,63 & 73,62 & 72,16 & 72,05 & 78,04 & 78,90 & 67,93 & 67,96 \\ FL+S-AVUC & 72,78 & 76,72 & 69,53 & 68,51 & 79,68 & 79,68 & 67,38 & 67,24 \\ CE+S-ECE & 73,29 & 73,18 & 71,59 & 71,36 & 77,02 & 77,69 & 67,98 & 68,01 \\ FL+S-ECE & 74,29 & 73,97 & 71,51 & 70,26 & 79,68 & 81,17 & 67,38 & 67,78 \\ \hline
**CE+AUCOCL** & 76,03 & **76,90** & **78,02** & **78,30** & **82,03** & **83,50** & 69,51 & **69,69** \\
**FL+AUCOCL** & **76,51** & 76,82 & 75,14 & 74,68 & 80,51 & 79,35 & **69,52** & 69,46 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Test AUROC(%) on OOD detection, training on CIFAR100 and testing on CIFAR100-C (Gaussian noise) and SVHN, using MSP, ODIN MaxLogit and EBM. Best and second best results are in bold and underlined.

\begin{table}
\begin{tabular}{l|c c c c|c c c} \hline \hline Dataset & \multicolumn{4}{c|}{DermaMNIST} & \multicolumn{4}{c}{RetinaMNIST} \\ \hline Loss funct. & ECE\(\downarrow\) & KS\(\downarrow\) & Brier\(\downarrow\) & cwEC\(\downarrow\) & ECE\(\downarrow\) & KS\(\downarrow\) & Brier\(\downarrow\) & cwECE\(\downarrow\) \\ \hline CE & **3,07** & 2,15 & 38,11 & 2,22 & 8,42 & 6,15 & 59,75 & 6,01 \\ FL (\(\gamma\)=3) & 4,24 & 1,77 & 36,55 & 2,03 & 13,61 & 10,87 & 63,32 & 8,19 \\ AdaFL53 & 3,88 & 1,95 & 36,19 & **1,66** & 13,63 & 11,12 & 65,64 & 8,47 \\ CE+MMCE & 3,59 & 2,73 & 38,96 & 2,20 & 11,58 & 9,75 & 62,18 & 6,87 \\ FL+MMCE & 3,65 & 2,29 & 38,91 & 2,34 & 12,49 & 10,54 & 65,72 & 7,81 \\ CE+S-AvUC & 3,79 & 2,69 & 38,28 & 2,21 & 8,37 & 7,01 & 64,36 & 6,91 \\ FL+S-AvUC & 3,48 & 2,13 & 37,98 & 1,82 & 11,60 & 5,86 & 64,06 & 7,12 \\ CE+S-ECE & 3,23 & 2,73 & 38,39 & 1,92 & 9,44 & 5,88 & 59,84 & 5,37 \\ FL+S-ECE & 4,5 & 2,67 & 36,91 & 2,08 & 12,52 & 10,05 & 61,16 & 6,08 \\ \hline
**CE+AUCOCL** & 5,70 & **1,56** & 36,12 & 1,78 & **8,15** & **4,47** & **58,69** & **4,46** \\
**FL+AUCOCL** & 5,10 & 2,04 & **35,46** & 2,04 & 10,77 & 8,84 & 60,52 & 5,32 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Test results on DermaMNIST and RetinaMNIST for calibration: expected calibration error (ECE), KS score, Brier score and class-wise ECE (cwECE), post TS. In bold and underlined respectively are the best and second best results for each metric. Noticeably, AUCOCL performs comparably to the baselines, even though it does not aim at improving calibration explicitly.

though the metric is not geared towards calibration, **CE+AUCOCL** yielded either the best or the second best calibration performance in the majority of the cases compared to the baselines.

In the Appendix, we explore how the performance of a model trained with AUCOCLoss varies, when changing batch size as it can be crucial for KDE-based methods. Even substantially lower batsch sizes do not have a considerable effect on the performance of the proposed method.

## 6 Conclusion

In this paper we proposed a new cost function for multi-class classification that takes into account the trade-off between a neural network's accuracy and the amount of data that requires manual analysis from a domain expert, by maximizing the area under COC (AUCOC) curve. Experiments on multiple computer vision and medical image datasets suggest that our approach improves the other methods in terms of both accuracy and AUCOC, where the latter was expected by design, provides comparable calibration metrics, even though the loss does not aim to improve calibration explicitly and outperforms the baselines in OOD detection.

While we presented COC and AUCOCLoss for multi-class classification, extensions to other tasks are possible future work as well as investigating different performance metrics to embed in the \(y\)-axis of COC. Moreover, aware of potential problems with KDE at the boundaries, i.e., boundary bias, we explored corrections like reflection method, which did not provide major improvements, but we will further investigate. We believe that this new direction of considering expert load in human-AI system is important and AUCOCLoss will serve as a baseline for future work.

**Limitations:** As described in Section 3.5, AUCOCLoss alone empirically leads to slow convergence, due to the small contribution of \(-log(\sum_{n}z_{n})\) with \(z_{n}\in[0,1]\) in its formulation. Therefore, we recommend to use it as a secondary loss, to successfully complement existing cost functions.

## 7 Acknowledgments

This study was financially supported by: 1. The LOOP Zurich - Medical Research Center, Zurich, Switzerland, 2. Personalized Health and Related Technologies (PHRT), project number 222, ETH domain and 3. Clinical Research Priority Program (CRPP) Grant on Artificial Intelligence in Oncological Imaging Network, University of Zurich.

## References

* Brier (1950) Brier, G. W. (1950). Verification of forecasts expressed in terms of probability. _Monthly Weather Review 78_, 1-3.
* Chen et al. (2017) Chen, L.-C., G. Papandreou, F. Schroff, and H. Adam (2017). Rethinking atrous convolution for semantic image segmentation. _arXiv preprint arXiv:1706.05587_.
* Davidson et al. (2017) Davidson, T., D. Warmsley, M. Macy, and I. Weber (2017). Automated hate speech detection and the problem of offensive language. In _Proceedings of the 11th International AAAI Conference on Web and Social Media_, ICWSM '17, pp. 512-515.
* Deng et al. (2009) Deng, J., W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei (2009). Imagenet: A large-scale hierarchical image database. In _2009 IEEE Conference on Computer Vision and Pattern Recognition_, pp. 248-255.
* Dvijotham et al. (2023) Dvijotham, K., J. Winkens, M. Barsbey, S. Ghaisas, R. Stanforth, N. Pawlowski, P. Strachan, Z. Ahmed, S. Azizi, Y. Bachrach, et al. (2023). Enhancing the reliability and accuracy of ai-enabled diagnosis via complementarity-driven deferral to clinicians. _Nature Medicine_, 1-7.
* Esteva et al. (2017) Esteva, A., B. Kuprel, R. A. Novoa, J. Ko, S. M. Swetter, H. M. Blau, and S. Thrun (2017). Dermatologist-level classification of skin cancer with deep neural networks. _nature 542_(7639), 115-118.
* Gorski et al. (2001) Gorski, N., V. Anisimov, E. Augustin, O. Baret, and S. Maximov (2001). Industrial bank check processing: the a2ia checkreaderrtm. _International Journal on Document Analysis and Recognition 3_(4), 196-206.

- Volume 70_, ICML'17, pp. 1321-1330. JMLR.org.
* Gupta et al. [2021] Gupta, K., A. Rahimi, T. Ajanthan, T. Mensink, C. Sminchisescu, and R. Hartley (2021). Calibration of neural networks using splines. In _International Conference on Learning Representations_.
* He et al. [2016] He, K., X. Zhang, S. Ren, and J. Sun (2016). Deep residual learning for image recognition. In _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 770-778.
* Hendrycks et al. [2022] Hendrycks, D., S. Basart, M. Mazeika, M. Mostajabi, J. Steinhardt, and D. X. Song (2022). Scaling out-of-distribution detection for real-world settings. In _International Conference on Machine Learning_.
* Hendrycks and Dietterich [2019] Hendrycks, D. and T. Dietterich (2019). Benchmarking neural network robustness to common corruptions and perturbations. _Proceedings of the International Conference on Learning Representations_.
* Hendrycks and Gimpel [2016] Hendrycks, D. and K. Gimpel (2016). A baseline for detecting misclassified and out-of-distribution examples in neural networks. _arXiv preprint arXiv:1610.02136_.
* Hsu et al. [2020] Hsu, Y.-C., Y. Shen, H. Jin, and Z. Kira (2020, 06). Generalized odin: Detecting out-of-distribution image without learning from out-of-distribution data. pp. 10948-10957.
* Jiang et al. [2012] Jiang, X., M. Osl, J. Kim, and L. Ohno-Machado (2012). Calibrating predictive model estimates to support personalized medicine. _Journal of the American Medical Informatics Association__19_(2), 263-274.
* Karandikar et al. [2021] Karandikar, A., N. Cain, D. Tran, B. Lakshminarayanan, J. Shlens, M. C. Mozer, and B. Roelofs (2021). Soft calibration objectives for neural networks. In _NeurIPS_.
* Kelly et al. [2019] Kelly, C. J., A. Karthikesalingam, M. Suleyman, G. Corrado, and D. King (2019). Key challenges for delivering clinical impact with artificial intelligence. _BMC medicine__17_(1), 1-9.
* Krishnan and Tickoo [2020] Krishnan, R. and O. Tickoo (2020). Improving model calibration with accuracy versus uncertainty optimization. _Advances in Neural Information Processing Systems_.
* Krizhevsky [2009] Krizhevsky, A. (2009). Learning multiple layers of features from tiny images. Technical report.
* Kull et al. [2019] Kull, M., M. Perello-Nieto, M. Kangsepp, T. S. Filho, H. Song, and P. Flach (2019). _Beyond Temperature Scaling: Obtaining Well-Calibrated Multiclass Probabilities with Dirichlet Calibration_. Red Hook, NY, USA: Curran Associates Inc.
* Kumar et al. [2018] Kumar, A., S. Sarawagi, and U. Jain (2018). Trainable calibration measures for neural networks from kernel mean embeddings. In _ICML_.
* Leibig et al. [2022] Leibig, C., M. Brehmer, S. Bunk, D. Byng, K. Pinker, and L. Umutlu (2022). Combining the strengths of radiologists and ai for breast cancer screening: a retrospective analysis. _The Lancet Digital Health__4_(7), e507-e519.
* Lin et al. [2017] Lin, T.-Y., P. Goyal, R. Girshick, K. He, and P. Dollar (2017). Focal loss for dense object detection. In _2017 IEEE International Conference on Computer Vision (ICCV)_, pp. 2999-3007.
* Lin et al. [2022] Lin, Z., S. Trivedi, and J. Sun (2022). Taking a step back with kcal: Multi-class kernel-based calibration for deep neural networks.
* Liu et al. [2020] Liu, W., X. Wang, J. D. Owens, and Y. Li (2020). Energy-based out-of-distribution detection. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin (Eds.), _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_.
* Mukhoti et al. [2020] Mukhoti, J., V. Kulharia, A. Sanyal, S. Golodetz, P. Torr, and P. Dokania (2020). Calibrating deep neural networks using focal loss. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin (Eds.), _Advances in Neural Information Processing Systems_, Volume 33, pp. 15288-15299. Curran Associates, Inc.

* Netzer et al. [2011] Netzer, Y., T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng (2011). Reading digits in natural images with unsupervised feature learning. In _NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011_.
* Nixon et al. [2019] Nixon, J., M. W. Dusenberry, L. Zhang, G. Jerfel, and D. Tran (2019, June). Measuring calibration in deep learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops_.
* Patel et al. [2019] Patel, B. N., L. Rosenberg, G. Willcox, D. Baltaxe, M. Lyons, J. Irvin, P. Rajpurkar, T. Amrhein, R. Gupta, S. Halabi, et al. (2019). Human-machine partnership with artificial intelligence for chest radiograph diagnosis. _NPJ digital medicine_ 2(1), 1-10.
* Pesarin and Salmaso [2010] Pesarin, F. and L. Salmaso (2010, 03). _Permutation Tests for Complex Data_.
* Platt [2000] Platt, J. (2000, 06). Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. _Adv. Large Margin Classif. 10_.
* Quinonero-Candela et al. [2008] Quinonero-Candela, J., M. Sugiyama, A. Schwaighofer, and N. D. Lawrence (2008). _Dataset shift in machine learning_. Mit Press.
* Rajpurkar et al. [2018] Rajpurkar, P., J. Irvin, R. L. Ball, K. Zhu, B. Yang, H. Mehta, T. Duan, D. Ding, A. Bagul, C. P. Langlotz, et al. (2018). Deep learning for chest radiograph diagnosis: A retrospective comparison of the chexnext algorithm to practicing radiologists. _PLoS medicine_ 15(11), e1002686.
* Sangalli et al. [2021] Sangalli, S., E. Erdil, A. Hotker, O. Donati, and E. Konukoglu (2021). Constrained optimization to train neural networks on critical and under-represented classes. _Advances in Neural Information Processing Systems 34_, 25400-25411.
* Scott [1979] Scott, D. W. (1979). On optimal and data-based histograms. _Biometrika__66_(3), 605-610.
* Szegedy et al. [2016] Szegedy, C., V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna (2016). Rethinking the inception architecture for computer vision. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 2818-2826.
* Tang et al. [2020] Tang, K., J. Huang, and H. Zhang (2020). Long-tailed classification by keeping the good and removing the bad momentum causal effect. In _Proceedings of the 34th International Conference on Neural Information Processing Systems_, NIPS'20, Red Hook, NY, USA. Curran Associates Inc.
* a large-scale lightweight benchmark for 2d and 3d biomedical image classification. _Scientific Data__10_.
* Yin et al. [2019] Yin, P., J. Lyu, S. Zhang, S. J. Osher, Y. Qi, and J. Xin (2019). Understanding straight-through estimator in training activation quantized neural nets. In _International Conference on Learning Representations_.
* Zadrozny and Elkan [2001] Zadrozny, B. and C. Elkan (2001). Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers. In _Proceedings of the Eighteenth International Conference on Machine Learning_, ICML '01, San Francisco, CA, USA, pp. 609-616. Morgan Kaufmann Publishers Inc.
* Zagoruyko and Komodakis [2016] Zagoruyko, S. and N. Komodakis (2016, September). Wide residual networks. In E. R. H. Richard C. Wilson and W. A. P. Smith (Eds.), _Proceedings of the British Machine Vision Conference (BMVC)_, pp. 87.1-87.12. BMVA Press.
* Zhou et al. [2020] Zhou, B., Q. Cui, X.-S. Wei, and Z.-M. Chen (2020). BBN: Bilateral-branch network with cumulative learning for long-tailed visual recognition. pp. 1-8.

Derivations

In this Section we provide the derivation of AUCOC and its gradient formulation.

### Derivation of AUCOC

AUCOC is defined as:

\[AUCOC=\int_{0}^{1}\mathbb{E}[c|r\geq r_{0}]d\tau_{0}=\int_{0}^{1}\left\{\int_{r _{0}}^{1}\mathbb{E}[c|r]p(r)dr\right\}\frac{d\tau_{0}}{1-\tau_{0}}\]

The y-axis in COC curve is expressed mathematically by:

\[\mathbb{E}[c|r\geq r_{0}] =\sum p(c|r\geq r_{0})c=\sum\frac{p(c,r\geq r_{0})}{p(r\geq r_{0} )}c=\sum\frac{\int_{r_{0}}^{1}p(c,r)dr}{\int_{r_{0}}^{1}p(r)dr}c\] \[=\sum\frac{\int_{r_{0}}^{1}p(c|r)p(r)dr}{\int_{r_{0}}^{1}p(r)dr}c= \frac{\int_{r_{0}}^{1}\sum p(c|r)cp(r)dr}{\int_{r_{0}}^{1}p(r)dr}=\frac{\int_{ r_{0}}^{1}\mathbb{E}[c|r]p(r)dr}{\int_{r_{0}}^{1}p(r)dr}=\] \[=\frac{\int_{r_{0}}^{1}\mathbb{E}[c|r]p(r)dr}{1-\tau_{0}}\]

The x-axis in COC curve is expressed mathematically by:

\[\tau_{0}=p(r<r_{0})=\int_{0}^{r_{0}}p(r)dr\]

Using the \(\tau_{0}\) formulation, we can rewrite the y-axis as

\[\mathbb{E}[c|r\geq r_{0}]=\frac{\int_{r_{0}}^{1}\mathbb{E}[c|r]p(r)dr}{1-\tau _{0}}\]

Maximizing the area under this curve, over \(\tau_{0}\in[0,1]\) corresponds to

\[\max\mathcal{A}=\max\int_{0}^{1}\left\{\int_{r_{0}}^{1}\mathbb{E}[c|r]p(r)dr \right\}\frac{d\tau_{0}}{1-\tau_{0}}\]

Let us assume to use a Gaussian kernel with this expression:

\[K(||r-r_{n}||)=\frac{1}{\sqrt{2\pi}\alpha}\cdot\exp\left(-\frac{(r-r_{n})^{2} }{2\alpha^{2}}\right)\] (9)

Developing the equation, the area calculation becomes:

\[\begin{split}\mathcal{A}&=\int_{0}^{1}\left\{\int_ {r_{0}}^{1}\mathbb{E}[c|r]p(r)dr\right\}\frac{d\tau_{0}}{1-\tau_{0}}=\\ &=\int_{0}^{1}\left\{\int_{r_{0}}^{1}\frac{1}{N}\sum_{n=1}^{N} \mathbf{1}(c_{n})K(\|r-r_{n}\|)dr\right\}\frac{d\tau_{0}}{1-\tau_{0}}=\\ &=\int_{0}^{1}\left\{\int_{r_{0}}^{1}\frac{1}{N}\sum_{n=1}^{N} \mathbf{1}(c_{n})\frac{1}{\sqrt{2\pi}\alpha}\cdot\exp\left(-\frac{(r-r_{n})^{2 }}{2\alpha^{2}}\right)dr\right\}\frac{d\tau_{0}}{1-\tau_{0}}=\\ &=\int_{0}^{1}\left\{\frac{1}{N}\sum_{n=1}^{N}\mathbf{1}(c_{n}) \int_{r_{0}}^{1}\frac{1}{\sqrt{2\pi}\alpha}\cdot\exp\left(-\frac{(r-r_{n})^{2 }}{2\alpha^{2}}\right)dr\right\}\frac{d\tau_{0}}{1-\tau_{0}}=\\ &=\int_{0}^{1}\left\{\frac{1}{N}\sum_{n=1}^{N}\mathbf{1}(c_{n}) \left(ndtr\left(\frac{1-r_{n}}{\sqrt{cov}}\right)-ndtr\left(\frac{r_{0}-r_{n}} {\sqrt{cov}}\right)\right)\right\}\frac{d\tau_{0}}{1-\tau_{0}}=\\ &=\sum_{k=1}^{\#thresh}\frac{f(\tau_{0,k-1},r_{0,k-1})+f(\tau_{0, k},r_{0,k})}{2}(\tau_{0,k}-\tau_{0,k-1})\end{split}\] (10)Where:

\[\begin{split}\tau_{0,k}&=\int_{0}^{r_{0,k}}p(r)dr \approx\frac{1}{N}\int_{0}^{r_{0,k}}\sum_{n=1}^{N}K(||r-r_{n}||)dr=\\ &=\frac{1}{N}\sum_{n=1}^{N}\left(ndtr\left(\frac{r_{0,k}-r_{n}}{ \sqrt{cov}}\right)-ndtr\left(\frac{-r_{n}}{\sqrt{cov}}\right)\right)\end{split}\] (11)

Where \(ndtr\) expresses the Gaussian cumulative distribution function and the last row in Equation 10 exploits the trapezoidal rule for integrals computation.

### Derivations of the gradients of AUCOC

\[\frac{d}{d\theta}\mathcal{A}=\int_{0}^{1}\frac{d}{d\theta}\left\{\int_{r_{0}}^ {1}\mathbb{E}[c|r]p(r)dr\right\}\frac{d\tau_{0}}{1-\tau_{0}}\]

Here, we use the assumption discussed in Section Methods that \(\tau_{0}\) does not depend on any parameter, thus allowing us to apply Leibnitz's integration rule, obtaining:

\[\frac{d}{d\theta}\mathcal{A} =\int_{0}^{1}\frac{d}{d\theta}\left\{\int_{r_{0}}^{1}\mathbb{E}[ c|r]p(r)dr\right\}\frac{d\tau_{0}}{1-\tau_{0}}\] (12) \[=\int_{0}^{1}\left\{\int_{r_{0}}^{1}\frac{d}{d\theta}\mathbb{E}[ c|r]p(r)dr-\mathbb{E}[c|r_{0}]p(r_{0})\frac{dr_{0}}{d\theta}\right\}\frac{d \tau_{0}}{1-\tau_{0}}\] (13)

\(\tau_{0}\) can be expressed as:

\[\tau_{0}=p(r\leq r_{0})=\int_{0}^{r_{0}}p(r)dr\] (14)

Consequently:

\[\begin{split}\frac{d\tau_{0}}{d\theta}&=\int_{0}^{r _{0}}\frac{dp(r)}{d\theta}dr+p(r_{0})\frac{dr_{0}}{d\theta}=0\\ \frac{dr_{0}}{d\theta}&=-\frac{\int_{0}^{r_{0}} \frac{dp(r)}{d\theta}dr}{p(r_{0})}\end{split}\] (15)

Plugging this expression back into Equation 12 we obtain:

\[\frac{d\mathcal{A}}{d\theta}=\int_{0}^{1}\left\{\int_{r_{0}}^{1}\frac{d}{d \theta}\mathbb{E}[c|r]p(r)dr+\mathbb{E}[c|r_{0}]\int_{0}^{r_{0}}\frac{dp(r)}{ d\theta}dr\right\}\frac{d\tau_{0}}{1-\tau_{0}}\] (16)

Assuming the use of a Gaussian kernel:

\[K(||r-r_{n}||)=\frac{1}{\sqrt{2\pi}\alpha}\cdot\exp\left(-\frac{(r-r_{n})^{2} }{2\alpha^{2}}\right)\] (17)

And re-writing:

\[\frac{\mathbb{E}[c|r_{0}]p(r_{0})}{p(r_{0})}\approx\frac{\frac{1}{N}\sum_{n= 1}^{N}\mathbf{1}(c_{n})K(||r_{0}-r_{n}||)}{\frac{1}{N}\sum_{n=1}^{N}K(||r_{0} -r_{n}||)}\] (18)The gradient of the area becomes:

\[\begin{split}\frac{d\mathcal{A}}{dr_{n}}&=\int_{0}^{1} \left\{\int_{r_{0}}^{1}\frac{d}{dr_{n}}\mathbb{E}[c|r]p(r)dr+\mathbb{E}[c|r_{0}] \int_{0}^{r_{0}}\frac{dp(r)}{dr_{n}}dr\right\}\frac{d\tau_{0}}{1-\tau_{0}}=\\ &=\int_{0}^{1}\{\int_{r_{0}}^{1}\frac{d}{dr_{n}}\frac{1}{\sqrt{2 \pi}\alpha}\frac{1}{N}\sum_{n=1}^{N}\mathbf{1}(c_{n})\cdot\exp\left(-\frac{(r-r _{n})^{2}}{2\alpha^{2}}\right)dr+\\ &\mathbb{E}[c|r_{0}]\int_{0}^{r_{0}}\frac{d}{dr_{n}}\frac{1}{ \sqrt{2\pi}\alpha}\frac{1}{N}\sum_{n=1}^{N}\exp\left(-\frac{(r-r_{n})^{2}}{2 \alpha^{2}}\right)dr\}\frac{d\tau_{0}}{1-\tau_{0}}=\\ &=\int_{0}^{1}\{\int_{r_{0}}^{1}\frac{1}{\sqrt{2\pi}\alpha^{3}N} \mathbf{1}(c_{n})\cdot(r-r_{n})\cdot\exp\left(-\frac{(r-r_{n})^{2}}{2\alpha^{2 }}\right)dr+\\ &\mathbb{E}[c|r_{0}]\int_{0}^{r_{0}}\frac{1}{\sqrt{2\pi}\alpha^{ 3}N}(r-r_{n})\cdot\exp\left(-\frac{(r-r_{n})^{2}}{2\alpha^{2}}\right)dr\} \frac{d\tau_{0}}{1-\tau_{0}}=\\ &=\int_{0}^{1}\{-\frac{1}{\sqrt{2\pi}\alpha N}\mathbf{1}(c_{n}) \cdot\left[\exp\left(-\frac{(1-r_{n})^{2}}{2\alpha^{2}}\right)-\exp\left(- \frac{(r_{0}-r_{n})^{2}}{2\alpha^{2}}\right)\right]-\\ &\mathbb{E}[c|r_{0}]\frac{1}{\sqrt{2\pi}\alpha N}\left[\exp \left(-\frac{(r_{0}-r_{n})^{2}}{2\alpha^{2}}\right)-\exp\left(-\frac{(-r_{n}) ^{2}}{2\alpha^{2}}\right)\right]dr\}\frac{d\tau_{0}}{1-\tau_{0}}\end{split}\] (19)

Also for the gradients in the code implementation we exploited the trapezoidal rule for the computation of the external integral between [0,1].

## Appendix B Potential upper bound to AUCOCLoss

AUCOC has a formulation of the kind \(-log(\sum_{n}z_{n})\) with \(z_{n}\in[0,1]\). In contrast, cross-entropy is a \(-\sum_{n}\log z_{n}\), where contribution of increasing low \(z_{n}\)'s to the loss is much larger, hence gradient-based optimization is faster. Exploiting Jensen's inequality, one could find the following upper bound of AUCOCLoss to minimise. However, from thorough experiments it has been proven not to be a tight enough bound for the optimisation to be successful. In fact, trying to optimise the rightmost term of the following equation, instead of the correct definition on the left, does not lead to a satisfactory optimisation of AUCOC.

\[-\log\left(\int_{0}^{1}\left\{\int_{r_{0}}^{1}r_{n}^{*}K(||r-r_{n}||)dr \right\}\frac{d\tau_{0}}{1-\tau_{0}}\right)\leq\]

## Appendix C Training details

In CIFAR100 experiments, we followed Karandikar et al. [16] and used Wide-Resnet-28-10 [41] as the network architecture. We trained the models for 200 epochs, using Stochastic Gradient Descent (SGD), with batch of 512, momentum of 0.9 and an initial learning rate of 0.1, decreased after 60, 120, 160 epochs by a factor of 0.1. We set these parameters based on the best validation performance of CE and we keep it for all the losses. In Tiny-ImageNet experiments, we used ResNet-50 [10] as backbone architecture, SGD as optimiser with a batch size of 512, momentum of 0.9 and base learning rate of 0.1, divided by 0.1 at 40th and 60th epochs as in Mukhoti et al. [26] In DermaMNIST, RetinaMNIST and TissueMNIST [38] experiments, we followed the training procedures of the original paper, employing a ResNet-50 He et al. [10], Adam optimizer. The batch size is set to 128 for DermaMNIST and RetinaMNIST and to 512 for the larger TissueMNIST. We used the initial learning rate 0.0001 for DermaMNIST and 0.001 for RetinaMNIST and TissueMNIST, and trained the models for 100 epochs by reducing the learning rate by 0.1 after epochs 50 and 75.

All the models have been trained using either the NVIDIA GeForce RTX 2080 Ti or NVIDIA GeForce RTX 3090. The datasets have been split as follows.

[MISSING_PAGE_FAIL:16]

Class imbalance experiments

In Table 7 we report the results for accuracy and AUCOC on the widely employed Long-Tailed CIFAR100. We employed a tunable data imbalance ratio (Nmax / Nmin, where N is number of samples in each class), which controls the class distribution in the training set and trained the models with three levels of imbalance ratio, namely 100, 50, 10. Noticeably, when AUCOCLoss complements cross-entropy it obtains the best results for both metrics in all the three settings, with higher benefits at increasing imbalance, while with FL it obtains always the second best AUCOC and comparable accuracy.

## Appendix F Calibration results for Tiny-ImageNet and CIFAR100

In Table 8 we report calibration results respectively for CIFAR100 and Tiny-ImageNet. For all the metrics and datasets, AUCOCLoss provides comparable results with respect to the baselines.

\begin{table}
\begin{tabular}{l|c c|c c|c c} \hline \hline Imbalance ratio & \multicolumn{2}{c|}{100} & \multicolumn{2}{c|}{50} & \multicolumn{2}{c}{10} \\ \hline Loss funct. & AUCOC \(\uparrow\) & Acc. \(\uparrow\) & AUCOC \(\uparrow\) & Acc. \(\uparrow\) & AUCOC \(\uparrow\) & Acc. \(\uparrow\) \\ \hline CE & 72,33 & 47,27 & 78,02 & 53,83 & 89,59 & 71,15 \\ FL (\(\gamma\)=3) & 73,66 & 47,00 & 79,53 & 53,50 & 91,91 & 71,59 \\ AdaFL53 & 73,72 & 47,07 & 79,55 & 53,55 & 92,05 & 71,61 \\ CE+MMCE & 72,92 & 47,80 & 78,54 & 53,45 & 90,66 & 71,32 \\ FL+MMCE & 73,49 & 46,78 & 79,50 & 53,32 & 91,70 & 71,71 \\ CE+S-AvUC & 73,08 & 46,62 & 76,15 & 54,45 & 91,72 & 71,60 \\ FL+S-AvUC & 72,68 & 46,62 & 78,51 & 53,11 & 89,75 & 72,11 \\ CE+S-ECE & 73,53 & 47,58 & 79,31 & 53,84 & 91,63 & 71,67 \\ FL+S-ECE & 72,30 & 46,13 & 78,56 & 52,84 & 91,29 & 70,6 \\ \hline
**CE+AUCOCL** & **75,85** & **49,63** & **81,38** & **55,89** & **92,59** & **72,40** \\
**FL+AUCOCL** & 74,51 & 46,91 & 79,72 & 53,64 & 92,21 & 71,92 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Accuracy and AUCOC results on CIFAR100 Long-Tailed for 3 degrees of class imbalance in the training set, namely 100, 50 and 10. In bold and underlined respectively the best and second best results for each metric.

\begin{table}
\begin{tabular}{l|c c c|c c c c} \hline \hline Dataset & \multicolumn{4}{c|}{CIFAR100} & \multicolumn{4}{c}{Tiny-Imagenet} \\ \hline Loss funct. & ECE\(\downarrow\) & KS\(\downarrow\) & Brier\(\downarrow\) & cwECE\(\downarrow\) & ECE\(\downarrow\) & KS\(\downarrow\) & Brier\(\downarrow\) & cwECE\(\downarrow\) \\ \hline CE & 2,41 & 1,1 & 33,91 & 0,211 & 1,54 & **0,74** & 66,25 & 0,163 \\ FL (\(\gamma\)=3) & 1,92 & 1,46 & 31,16 & 0,184 & 1,46 & 1,14 & 65,91 & 0,152 \\ AdaFL53 & 1,47 & 1,22 & 31,3 & 0,183 & **1,35** & 0,85 & 65,71 & 0,159 \\ CE+MMCE & 2,32 & 1,23 & 34,6 & 0,209 & 1,79 & 0,93 & 66,42 & 0,157 \\ FL+MMCE & 1,92 & 1,82 & 31,24 & 0,181 & 1,94 & 1,63 & 65,49 & 0,151 \\ CE+S-AvUC & 3,23 & 0,53 & 31,71 & 0,199 & 2,13 & 1,15 & 65,20 & **0,146** \\ FL+S-AvUC & 1,83 & 0,53 & 31,74 & 0,177 & 1,51 & 1,13 & 64,62 & 0,15 \\ CE+S-ECE & 3,85 & 1,3 & 31,85 & 0,197 & 1,66 & 0,81 & 66,14 & 0,15 \\ FL+S-ECE & 1,74 & 1,56 & 32,19 & 0,180 & 2,6 & 2,37 & 66,40 & 0,161 \\ \hline
**CE+AUCOCL** & 1,65 & **0,75** & **29,78** & 0,184 & 1,65 & 1,29 & **64,23** & 0,157 \\
**FL+AUCOCL** & **1,34** & 0,95 & 30,24 & **0,175** & 1,86 & 1,29 & 64,45 & 0,155 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Test results on CIFAR100 and Tiny-Imagenet for calibration metrics, namely expected calibration error (ECE), KS score, Brier score and class-wise ECE (cwECE), post temperature scaling. In bold and underlined respectively the best and second best results for each metric. Noticeably, AUCOCL performs comparably to the baselines, even though it does not aim at improving calibration explicitly.

Results for varying batch size

In Table 9 we report the results on DermaMNIST with reduced batch sizeon which KDE is applied on, i.e. 64 and 28 samples per batch. The results of AUCOCLoss are not significantly affected by it, when complementing cross-entropy and focal loss.

## Appendix H Additional results for CIFAR100 and Tiny-ImageNet

In Table 10 we report test results on CIFAR100 using ResNet-50 and Tiny-Imagenet using WideResnet-28-10, run for one seed. Best and second best results are in bold and underlined. They show consistency with the main experiments reported in the paper.

## Appendix I Further explanation on how improve AUCO

There are two factors which contribute to an increase in AUCOC: decrease in the number of samples delegated to human experts (given the same network accuracy) and increase in the accuracy for the samples that are not delegated but analysed only by the network (given the same human workload).

These two aspects could manifest either individually, if the AUCOC improvement is generated by just a shift "up" or "left" of COC, or in a combined way. The example provided in Figure 0(a) shows an improvement in both axes ("and" case) and the proposed loss function does not favour one specific behaviour. Figure 2 provides an example of shifts "up" and "left" ("or" cases). From the AUCOC metrics alone, it is not possible to infer which mechanism is taking place.

\begin{table}
\begin{tabular}{l|c c c c|c c c} \hline \hline Dataset & \multicolumn{3}{c|}{CIFAR100} & \multicolumn{3}{c}{Tiny-ImageNet} \\ \hline  & \multicolumn{3}{c|}{\(\tau_{0}\downarrow\) @ acc.} & \multicolumn{3}{c}{\(\tau_{0}\downarrow\) @ acc.} \\ Loss funct. & AUCOC \(\uparrow\) & Acc. \(\uparrow\) & 90\% & 95\% & AUCOC \(\uparrow\) & Acc. \(\uparrow\) & 65\% & 75\% \\ \hline CE & 91,10 & 73,69 & 34,72 & 49,42 & 74,03 & 49,29 & 34,74 & 52,28 \\ FL (\(\gamma\)=3) & 92,68 & 75,13 & 32,30 & 46,40 & 75,05 & 49,56 & 34,17 & 51,72 \\ AdaFL53 & 92,65 & 74,5 & 32,01 & 45,78 & 75,36 & 49,49 & 33,77 & 50,76 \\ CE+MMCE & 92,55 & 74,75 & 31,08 & 44,53 & 74,12 & 48,56 & 35,83 & 52,48 \\ FL+MMCE & 92,46 & 74,47 & 31,69 & 46,63 & 74,79 & 49,32 & 34,57 & 52,12 \\ CE+S-AvUC & 92,63 & 74,12 & 31,02 & 44,83 & 74,14 & 49,13 & 35,00 & 53,17 \\ FL+S-AvUC & 92,71 & 74,82 & 31,11 & 44,51 & 74,21 & 49,02 & 35,22 & 53,12 \\ CE+S-ECE & 92,57 & 74,27 & 31,38 & 45,4 & 75,62 & 49,91 & 31,88 & 49,55 \\ FL+S-ECE & 92,63 & 75,00 & 31,35 & 47,42 & 74,57 & 49,16 & 35,60 & 52,36 \\ \hline
**CE+AUCOCL** & **93,81** & **75,94** & **28,39** & **42,61** & **76,64** & **51,40** & **29,97** & **47,65** \\
**FL+AUCOCL** & 93,19 & 75,90 & 29,45 & 43,11 & 75,86 & 50,84 & 32,00 & 50,12 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Test results on CIFAR100 using ResNet-50 and Tiny-Imagenet using WideResnet-28-10, run for one seed. Best and second best results are in bold and underlined.

\begin{table}
\begin{tabular}{c|c|c c c} \hline \hline Batch & Loss funct. & AUCOC \(\uparrow\) & Acc. \(\uparrow\) & ECE \(\downarrow\) \\ \hline  & **CE+AUCOCL** & 91,16 & 75,18 & 11,00 \\
64 & **FL+AUCOCL** & 91,13 & 74,71 & 9,35 \\ \hline  & **CE+AUCOCL** & 90,14 & 73,80 & 15,50 \\
32 & **FL+AUCOCL** & 91,10 & 73,47 & 6,69 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Test results on DermaMNIST for accuracy, AUCOC and ECE for batch sizes 64 and 32. Reducing the batch size does not affect significantly the performance of AUCOCL.

[MISSING_PAGE_FAIL:19]