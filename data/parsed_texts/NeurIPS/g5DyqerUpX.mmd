# SPARKLE: A Unified Single-Loop Primal-Dual Framework for Decentralized Bilevel Optimization

Shuchen Zhu

Peking University

shuchenzhu@stu.pku.edu.cn

&Booo Kong

Peking University

kongboao@stu.pku.edu.cn

&Songtao Lu

IBM Research

songtao@ibm.com

&Xinmeng Huang

University of Pennsylvania

xinmengh@sas.upenn.edu

&Kun Yuan

Peking University

kunyuan@pku.edu.cn

Equal Contribution

Corresponding Author: Kun Yuan. Kun Yuan is also affiliated with National Engineering Laboratory for Big Data Analytics and Applications, and AI for Science Institute, Beijing, China.

###### Abstract

This paper studies decentralized bilevel optimization, in which multiple agents collaborate to solve problems involving nested optimization structures with neighborhood communications. Most existing literature primarily utilizes gradient tracking to mitigate the influence of data heterogeneity, without exploring other well-known heterogeneity-correction techniques such as EXTRA or Exact Diffusion. Additionally, these studies often employ identical decentralized strategies for both upper- and lower-level problems, neglecting to leverage distinct mechanisms across different levels. To address these limitations, this paper proposes **SPARKLE**, a unified **S**ingle-loop **P**imal-dual **A**go**R**ithm framework for decentra**L**ized bil**E**vel optimization. SPARKLE offers the flexibility to incorporate various heterogeneity-correction strategies into the algorithm. Moreover, SPARKLE allows for different strategies to solve upper- and lower-level problems. We present a unified convergence analysis for SPARKLE, applicable to all its variants, with state-of-the-art convergence rates compared to existing decentralized bilevel algorithms. Our results further reveal that EXTRA and Exact Diffusion are more suitable for decentralized bilevel optimization, and using mixed strategies in bilevel algorithms brings more benefits than relying solely on gradient tracking.

## 1 Introduction

Numerous modern machine learning tasks, such as reinforcement learning [25], meta-learning [4], adversarial learning [36], hyper-parameter optimization [19], and imitation learning [3], entail nested optimization formulations that extend beyond the traditional single-level paradigm. For instance, hyper-parameter optimization aims to identify the optimal hyper-parameters for a specific learning task in the upper level by minimizing the validation loss, achieved through training models in the lower-level process. This nested optimization structure has spurred significant attention towards Stochastic Bilevel Optimization (SBO). Since the size of data samples involved in bilevel problems has become increasingly large, this paper investigates decentralized algorithms over a network of \(n\) agents (nodes) that collaborate to solve the following distributed bilevel optimization problem:

\[\min_{x\in\mathbb{R}^{p}}\ \ \Phi(x)=f(x,y^{\star}(x)):=\frac{1}{n}\sum_{i=1}^{ n}f_{i}(x,y^{\star}(x)),\hskip 56.905512pt\text{(upper-level)}\] (1a)\[\mathrm{s.t.}\quad y^{\star}(x)=\operatorname*{argmin}_{y\in\mathbb{R}^{q}}\Big{\{} g(x,y):=\frac{1}{n}\sum_{i=1}^{n}g_{i}(x,y)\Big{\}}.\qquad\qquad\text{( lower-level)}\] (1b)

In this formulation, each agent \(i\) holds a private upper-level objective function \(f_{i}:\mathbb{R}^{p}\times\mathbb{R}^{q}\to\mathbb{R}\) and a strongly convex lower-level objective function \(g_{i}:\mathbb{R}^{p}\times\mathbb{R}^{q}\to\mathbb{R}\) defined as:

\[f_{i}(x,y)=\mathbb{E}_{\phi\sim\mathcal{D}_{f_{i}}}[F_{i}(x,y;\phi)],\qquad g_ {i}(x,y)=\mathbb{E}_{\xi\sim\mathcal{D}_{g_{i}}}[G_{i}(x,y;\xi)],\] (2)

where \(\mathcal{D}_{f_{i}}\) and \(\mathcal{D}_{g_{i}}\) represent the local data distributions at agent \(i\). This paper does not make any assumptions about these data distributions, implying there might be data heterogeneity across agents.

Linear speedup and transient iteration complexity.A decentralized stochastic algorithm achieves _linear speedup_ if its iteration complexity decreases linearly with the network size \(n\). Additionally, the _transient iteration complexity_ refers to the number of transient iterations a decentralized algorithm must undergo to achieve the asymptotic linear speedup stage. The fewer the transient iterations, the faster the algorithm can achieve linear speedup. This paper aims to develop decentralized stochastic bilevel algorithms that can achieve linear speedup with as few transient iterations as possible.

**Limitations in previous works.** A significant challenge in decentralized bilevel optimization lies in accurately estimating the hyper-gradient \(\nabla\Phi(x)\) through neighborhood communications. Several studies have emerged to effectively address this challenge, such as those by [9; 33; 52; 16; 21; 40; 57; 29]. However, existing works suffer from several critical limitations:

* **Stringent assumptions and inadequate convergence analysis.** Many existing studies rely on stringent assumptions to ensure convergence. For instance, references [9; 10; 33; 21; 52] assume bounded gradients, while reference [29] assumes bounded data heterogeneity (also known as bounded gradient dissimilarity). These restrictive assumptions do not arise in centralized bilevel optimization, implying their potential unnecessity. Moreover, some of these works suffer from inadequate convergence analysis, unable to clarify the transient iteration complexity [9; 33; 10] or provide a sharp estimation of the influence of network topologies [52; 21].
* **Limited exploration of various heterogeneity-correction techniques.** Several concurrent studies [16; 57; 40] have utilized Gradient Tracking (GT) [50; 13; 38] to remove the assumption of bounded data heterogeneity. However, it remains uncertain whether GT is the most suitable mechanism for decentralized bilevel optimization. Many other techniques are also useful for addressing data heterogeneity in single-level decentralized optimization, such as EXTRA [45] and Exact-Diffusion (ED) [56; 30; 54] (which is also known as D\({}^{2}\)[46]). Even within GT, there are variants including Adapt-Then-Combine GT (ATC-GT) [50], non-ATC-GT [38], and semi-ATC-GT [13]. It remains unexplored whether these techniques for mitigating data heterogeneity converge and even outperform GT when employed in decentralized bilevel algorithms.
* **Unknown effects of employing different upper- and lower-level update strategies.** In bilevel optimization, the challenges in solving the upper- and lower-level problems differ substantially. For instance, the upper-level problem (1a) is non-convex, whereas the lower-level problem (1b) is strongly convex. Moreover, estimating the gradient at the lower level is considerably simpler compared to estimating the hyper-gradient at the upper level. Understanding the roles of updates at each level is crucial to develop more efficient algorithms. However, most existing algorithms employ the same decentralized methods to solve both the upper- and lower-level problems. For example, references [9; 52; 29] utilize decentralized gradient descent (DGD) for updates at both levels while [57; 40; 16] leverage GT, overlooking the potential advantages of mixed strategies.

To address these limitations, several critical questions naturally arise: Should each heterogeneity-correction mechanism listed in [50; 13; 38; 56; 30; 54; 46] be explored one-by-one? Should we consider combining any two of these techniques to update the upper and lower-level problems, respectively? It is evident that examining each individual heterogeneity-correction technique, and even exploring their combinations, would involve an unbearable amount of effort.

**Main results and contributions.** This paper addresses all the aforementioned limitations without exhaustively exploring all heterogeneity-correction techniques. Our main results are as follows.

* **A unified decentralized bilevel framework.** To avoid examining each single heterogeneity-correction technique, we propose **SPARKLE**, a unified Single-loop Primal-dual \(\mathsf{AlgoR}\)ithmframework for decent_Lized_ bilEvel optimization. By specifying certain hyper-parameters, SPARKLE can be tailored to SPARKLE-EXTRA, SPARKLE-ED, and SPARKLE-GT, which employ EXTRA [45], ED [56, 30], or multiple GT variants [50, 13, 38], respectively, to facilitate the upper and lower-level problems. Additionally, SPARKLE is the _first_ algorithm enabling distinct updating strategies across different levels; for example, one can utilize GT in the upper-level but ED in the lower-level, resulting in a brand new SPARKLE-GT-ED algorithm.
* **A unified and sharp analysis of various heterogeneity-correction schemes.** We provide a unified convergence analysis for SPARKLE, which immediately applies to all SPARKLE variants with distinct heterogeneity-correction techniques. The analysis does not require restrictive assumptions such as gradient boundedness used in [9, 10, 33, 21, 52] or data-heterogeneity bounded used in [29]. Moreover, our analysis demonstrates the provable superiority of SPARKLE compared to existing algorithms, as evidenced by the convergence rates listed in Table 1. Most importantly, our analysis shows that both SPARKLE-EXTRA and SPARKLE-ED outperform SPARKLE-GT (see Table 1), implying that _GT is not the best_ scheme for decentralized bilevel optimization.
* **Mixing strategies outperform employing GT alone.** We demonstrate how optimization at different levels affects convergence rates. Our theoretical analysis suggests that the updating strategy at _the lower level is crucial in determining the overall performance_ in decentralized bilevel algorithms. Building upon this insight, we establish that incorporating the ED or EXTRA strategy in the lower-level update phase leads to better transient iteration complexity than relying solely on the GT mechanism in both levels as proposed in [10, 16, 40], see Table 2 for more details.
* **Comparable performance with single-level algorithms.** We elucidate the comparison between bilevel and single-level stochastic decentralized optimization. On one hand, we demonstrate that the convergence performance of all our proposed algorithms is not inferior to their single-level counterparts (see the bottom part in Table 1). On the other hand, by considering specific lower-level loss functions, our bilevel results directly yield the non-asymptotic convergence of corresponding

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline
**Algorithms** & **Assumption** & **A. Rate.** & \({}^{\circ}\) & **A. Comp.** & \({}^{\dagger}\) & **A. Comm.** & \({}^{\dagger}\) & **Tran. Iter.** & \({}^{\circ}\) & **Loopless** \\ \hline DSBO [9] & LC & \(\frac{1}{\sqrt{R}}\) & \(\frac{1}{z^{3}}\left(p\log(z)+\frac{1}{\varepsilon})+\frac{1}{\varepsilon} \right)\frac{1}{z^{2}}\) & N.A. & No \\ MA-DSBO [10] & LC & \(\frac{1}{\sqrt{R}}\) & \(\frac{1}{z^{3}}\log(\frac{1}{\varepsilon})\) & \(\frac{1}{z^{3}}\log(\frac{1}{\varepsilon})+\frac{1}{\varepsilon}\) & N.A. & No \\ SLAM [33] & LC & \(\frac{1}{\sqrt{\upsilon R}}\) & \(\frac{1}{nz^{3}}\log(\frac{1}{\varepsilon})\) & \(\frac{1}{nz^{3}}\) & N.A. & No \\ MDBO [21] & BG & \(\frac{1}{\sqrt{\upsilon R}}\) & \(\frac{1}{nz^{3}}\log(\frac{1}{\varepsilon})\) & \(\frac{1}{nz^{3}}\) & \(\frac{nz^{3}}{\sqrt{\upsilon}}\) & No \\ Gossip DSBO [52] & BG & \(\frac{1}{\sqrt{\upsilon R}}\) & \(\frac{1}{nz^{3}}\log(\frac{1}{\varepsilon})\) & \(\frac{1}{nz^{3}}\log(\frac{1}{\varepsilon})+\frac{1}{nz^{3}}\) & \(\frac{nz^{3}}{(1-\varepsilon^{3})}\) & No \\ LoPA [40]\({}^{*}\) & BGD & \(\frac{1}{\sqrt{R}}\) & \(\frac{1}{z^{3}}\) & \(\frac{1}{z^{3}}\) & \(\frac{1}{z^{3}}\) & \(\frac{1}{nz^{3}}\) & No \\ D-SOBA [29] & BGD & \(\frac{1}{\sqrt{\upsilon R}}\) & \(\frac{1}{nz^{3}}\) & \(\frac{1}{nz^{3}}\) & \(\frac{1}{nz^{3}}\) & \(\max\left\{\frac{n^{3}}{(1-\varepsilon^{3})^{3}}\frac{n^{3}}{(1-\varepsilon^{3}) ^{3}}\right\}\) & Yes \\ \hline
**SPARKLE-GT (ours)** & **None** & \(\frac{1}{\sqrt{\upsilon R}}\) & \(\frac{1}{\sqrt{\upsilon R}}\) & \(\frac{\sqrt{\upsilon R}}{\sqrt{\upsilon R}}\) & \(\frac{\sqrt{\upsilon R}}{\sqrt{\upsilon R}}\) & **Yes** \\ \hline
**SPARKLE-EXTRA (ours)** & **None** & \(\frac{1}{\sqrt{\upsilon R}}\) & \(\frac{1}{\sqrt{\upsilon R}}\) & \(\frac{\sqrt{\upsilon R}}{\sqrt{\upsilon R}}\) & \(\frac{\sqrt{\upsilon R}}{\sqrt{\upsilon R}}\) & **Yes** \\ \hline
**SPARKLE-ED (ours)** & **None** & \(\frac{1}{\sqrt{\upsilon R}}\) & \(\frac{1}{nz^{3}}\) & \(\frac{\sqrt{\upsilon R}}{\sqrt{\upsilon R}}\) & \(\frac{\sqrt{\upsilon R}}{\sqrt{\upsilon R}}\) & **Yes** \\ \hline Single-level GT [2, 28] & None & \(\frac{1}{\sqrt{\upsilon R}}\) & \(\frac{1}{nz^{3}}\) & \(\frac{1}{nz^{3}}\) & \(\frac{1}{nz^{3}}\) & \(\max\left\{\frac{n^{3}}{(1-\varepsilon^{3})^{3}}\frac{n}{(1-\varepsilon^{3})^ {3}}\right\}\) & Yes \\ Single-level EXTRA [2] & None & \(\frac{1}{\sqrt{\upsilon R}}\) & \(\frac{1}{nz^{3}}\) & \(\frac{1}{nz^{3}}\) & \(\frac{n^{3}}{(1-\varepsilon^{3})^{3}}\) & Yes \\ Single-level ED [2] & None & \(\frac{1}{\sqrt{\upsilon R}}\) & \(\frac{1}{nz^{3}}\) & \(\frac{1}{nz^{3}}\) & \(\frac{n^{3}}{(1-\varepsilon^{3})^{3}}\) & Yes \\ \hline \hline \end{tabular} \({}^{\circ}\) The convergence rate when \(K\rightarrow\infty\) (smaller is better).

\({}^{\dagger}\) The number of gradient/Jacobian/Hessian evaluations per agent to achieve \(\varepsilon\)-accuracy when \(\epsilon\to 0\) (smaller is better).

\({}^{\ddagger}\) The communication costs per agent to achieve \(\varepsilon\)-stationarity when \(\epsilon\to 0\) (smaller is better).

\({}^{\ddagger}\) The transient iteration complexity to achieve linear speedup (smaller is better). \({}^{\circ}\) N.A." means that the algorithm cannot achieve linear speedup or the transient time cannot be accessed from existing convergence analysis.

\({}^{\flat}\) Additional assumptions beyond Assumption 1.

\({}^{*}\) LoPA solves the personalized problem, where the lower-level objectives are local to agents.

\({}^{\ddagger}\)\(a>0\) measures the relative sparsity of the mixing weights \(\mathbf{W}_{\cdot},\mathbf{W}_{\cdot},\mathbf{W}_{\cdot}\), which can be very small in certain cases. Here \(1-\rho\) in **Tran. Iter.** denotes the smallest spectral gap of \(\mathbf{W}_{\cdot},\mathbf{W}_{\cdot},\mathbf{W}_{\cdot}\). See more discussions in Appendix C.2.3.

\end{table}
Table 1: Comparison between different decentralized stochastic bilevel algorithms. \(K\) denotes the number of (upper-level) iterations; \(1-\rho\) denotes the spectral gap of the mixing matrix (see Assumption 2); \(b^{2}\) bounds the gradient dissimilarity; \(\varepsilon\) is the target stationarity such that \(\sum_{k=0}^{K-1}\mathbb{E}\|\nabla\Phi(x^{k})\|^{2}/K<\varepsilon\); \(p\) and \(q\) are the dimensions of the upper- and lower-level variables, reflecting per-round communication costs. Assumptions of bounded gradient, Lipschitz continuity, and bounded gradient dissimilarity are abbreviated as BG, LC, and BGD, respectively. We also list the best-known results of single-level GT, EXTRA, and ED at the bottom.

single-level algorithms. This is the _first_ result demonstrating bilevel optimization essentially subsumes the convergence of the single-level optimization.

Our main results are listed in Table 1. All SPARKLE variants achieve the state-of-the-art asymptotic rate, asymptotic gradient complexity, asymptotic communication cost, and transient iteration complexity under more relaxed assumptions compared to existing methods.

**Related works.** A significant challenge in decentralized bilevel optimization is accurately estimating the hyper-gradient \(\nabla\Phi(x)\), necessitating solving global lower-level problems and estimating Hessian inversion. To this end, various decentralized techniques have been applied in bilevel optimization, including Neumann series in [52], JHIP oracle in [9], HIGP oracle in [10], and augmented Lagrangian-based communication in [33]. Additionally, reference [29] proposes a single-loop algorithm utilizing decentralized SOBA. To enhance algorithmic robustness against data heterogeneity, recent studies have employed Gradient Tracking (GT) in both lower- and upper-level optimization. However, existing works built upon GT suffer from several limitations. Results of [16; 9] concentrate solely on deterministic cases, while reference [40] addresses personalized problems in the lower-level, which do not require achieving global consensus in the lower-level problem. Moreover, [9; 10] introduce computationally expensive inner loops for GT steps. None of these works can establish smaller transient iteration complexity than D-SOBA for decentralized SBO, even though the latter algorithm employs no heterogeneity-correction technique.

The unified framework for single-level decentralized optimization has been extensively studied in the literature. References [1; 49; 26] propose frameworks for decentralized composite optimization in deterministic settings, while [2] investigates a framework under stochastic settings. However, none of these works can be directly applied to decentralized bilevel algorithms. Several studies [21; 57] utilize variance reduction techniques to accelerate the convergence of stochastic decentralized bilevel algorithms. Our proposed SPARKLE framework is orthogonal to variance reduction; it can also incorporate variance-reduced gradient estimation to achieve improved convergence rates. More relevant works on decentralized optimization and bilevel optimization are discussed in Appendix A.

**Notations.** We use lowercase letters to represent vectors and uppercase letters to represent matrices. We introduce \(\text{col}\{x_{1},...,x_{n}\}:=[x_{1}^{\top},...,x_{n}^{\top}]^{\top}\in \mathbb{R}^{p}\) for brevity. Variables with overbar denote the average over all agents. For example, \(\bar{x}^{k}=\sum_{i=1}^{n}x_{i}^{k}/n\). We denote \(\overline{A}=A-\frac{1}{n}\mathbf{1}_{n}\mathbf{1}_{n}^{\top}\) for matrix \(A\in\mathbb{R}^{n\times n}\), where \(\mathbf{1}_{n}\in\mathbb{R}^{n}\) denotes the \(n\)-dimensional vector with all entries being one. For a function \(f(x,y):\mathbb{R}^{p}\times\mathbb{R}^{q}\rightarrow\mathbb{R}\), we use \(\nabla_{1}f\left(x,y\right)\in\mathbb{R}^{p}\), \(\nabla_{2}f(x,y)\in\mathbb{R}^{q}\) to represent its partial gradients with respect to \(x\) and \(y\), respectively. Similarly, \(\nabla_{12}f(x,y)\in\mathbb{R}^{p\times q}\), \(\nabla_{22}f(x,y)\in\mathbb{R}^{q\times q}\) represent the corresponding Jacobian and Hessian matrix. We use the notation \(\lesssim\) to denote inequalities that hold up to constants related to the initialization of algorithms and smoothness constants.

## 2 SPARKLE: A unified framework for decentralized bilevel optimization

This section develops SPARKLE, a unified framework for decentralized bilevel optimization, and discusses its numerous variants by specifying certain hyper-parameters.

### Three pillar subproblems in decentralized bilevel optimization.

When solving the upper-level problem (1a), it is critical to obtain the hyper-gradient \(\nabla\Phi(x)\), which can be expressed as [22]

\[\nabla\Phi(x)=\nabla_{1}f(x,y^{\star}(x))-\nabla_{12}^{2}g(x,y^{\star}(x)) \left[\nabla_{22}^{2}g(x,y^{\star}(x))\right]^{-1}\nabla_{2}f(x,y^{\star}(x)).\] (3)

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline
**lowerupper** & **ED** & **EXTRA** & **GT** \\ \hline
**ED** & \(\frac{n^{3}}{(1-\rho)^{2}}\) & \(\frac{n^{3}}{(1-\rho)^{2}}\) & \(\frac{n^{3}}{(1-\rho)^{2}}\) \\ \hline
**EXTRA** & \(\frac{n^{3}}{(1-\rho)^{2}}\) & \(\frac{n^{2}}{(1-\rho)^{2}}\) & \(\frac{n^{3}}{(1-\rho)^{2}}\) \\ \hline
**GT** & \(\max\left\{\frac{n^{3}}{(1-\rho)^{2}},\frac{n}{(1-\rho)^{N/3}}\right\}\) & \(\max\left\{\frac{n^{3}}{(1-\rho)^{2}},\frac{n}{(1-\rho)^{N/3}}\right\}\) & \(\max\left\{\frac{n^{3}}{(1-\rho)^{2}},\frac{n}{(1-\rho)^{N/3}}\right\}\) \\ \hline \end{tabular}
\end{table}
Table 2: The transient iteration complexity of SPARKLE with mixed updating strategies at various levels. The smaller the transient iteration complexity is, the faster the algorithm will achieve its linear speedup stage. The first row and column respectively indicate the updating strategy for the upper- and lower-level problems. Please refer to Appendix B.3 for more implementation details and Appendix C.2.4 for proofs.

Evaluating this hyper-gradient is computationally expensive due to the inversion of the Hessian matrix. This evaluation becomes even more challenging over a decentralized network of collaborative agents. First, the inverse of the Hessian matrix cannot be obtained by simply averaging the local Hessian inverses due to \([\frac{1}{n}\sum_{i=1}^{n}\nabla_{22}^{2}g_{i}(x,y^{\star}(x))]^{-1}\neq\frac{1} {n}\sum_{i=1}^{n}[\nabla_{22}^{2}g_{i}(x,y^{\star}(x))]^{-1}\). Second, the global averaging operation cannot be realized through decentralized communication. To overcome these challenges, one can introduce an auxiliary variable \(z^{\star}(x)\!:=[\nabla_{22}^{2}g(x,y^{\star}(x))]^{-1}\nabla_{2}f(x,y^{\star} (x))\)[12], which is the solution to a quadratic problem

\[z^{\star}(x)=\operatorname*{argmin}_{z\in\mathbb{R}^{q}}\left\{\frac{1}{2}z^{ \top}\nabla_{22}^{2}g\left(x,y^{\star}(x)\right)z-z^{\top}\nabla_{2}f\left(x, y^{\star}(x)\right)\right\}.\] (4)

Once \(z^{\star}(x)\) is derived by solving (4), we can substitute it into (3) to achieve \(\nabla\Phi(x)\).

Following this idea, solving the distributed bilevel optimization problem (1) essentially involves solving three subproblems, where \(h_{i}(x,y^{\star}(x),z):=\frac{1}{2}z^{\top}\nabla_{22}^{2}g_{i}(x,y^{\star}(x ))z-z^{\top}\nabla_{2}f_{i}(x,y^{\star}(x))\),

\[x^{\star} =\operatorname*{argmin}_{x\in\mathbb{R}^{p}}\frac{1}{n}\sum_{i=1} ^{n}f_{i}(x,y^{\star}(x)), \text{(upper-level)}\] (5a) \[y^{\star}(x) =\operatorname*{argmin}_{y\in\mathbb{R}^{q}}\frac{1}{n}\sum_{i=1} ^{n}g_{i}(x,y), \text{(lower-level)}\] (5b) \[z^{\star}(x) =\operatorname*{argmin}_{z\in\mathbb{R}^{q}}\frac{1}{n}\sum_{i=1} ^{n}h_{i}(x,y^{\star}(x),z). \text{(auxiliary-level)}\] (5c)

Given the variable \(x\), one can achieve \(y^{\star}(x)\) by solving the lower-level problem in (5b). With \(y^{\star}(x)\) determined, \(z^{\star}(x)\) can be obtained by solving the auxiliary-level problem in (5c). Subsequently, with \(z^{\star}(x)\) available, one can directly compute the hyper-gradient and solve the upper-level problem in (5a) using gradient descent. This constitutes the primary methodology to solve problem (1).

A bilevel algorithm essentially solves three subproblems listed in (5), each formulated as a single-level decentralized optimization problem. Nevertheless, primary approaches may suffer from nested loops in algorithmic development. A few recent studies [12; 11; 57; 29] propose to solve each problem in (5a)-(5c) approximately with _one single_ iteration, leading to practical single-loop bilevel algorithms. For example, applying a D-SGD step [43] to each of (5a)-(5c) yields the D-SOBA method [29], while further leveraging the GT technique leads to decentralized bilevel methods in [9; 16; 57; 21].

However, it is less explored whether numerous other heterogeneity-correction techniques [50; 13; 38; 56; 30; 54; 46] beyond GT can be incorporated into algorithmic design to achieve even better performance in bilevel optimization. To avoid exploring each case individually, we next introduce a general framework that unifies all these techniques for solving single-level problems.

### A unified framework for decentralized single-level optimization.

In this subsection, we consider solving the single-level problem \(\min_{x\in\mathbb{R}^{p}}\frac{1}{n}\sum_{i=1}^{n}f_{i}(x)\) over a network of \(n\) nodes. For each \(k\)-th (\(k\geq 0\)) iteration, we let \(x_{i}^{k}\) denote the local \(x\)-variable maintained by the \(i\)-th agent. Furthermore, we associate the topology with a weight matrix \(W=[w_{ij}]_{i,j=1}^{n}\in\mathbb{R}^{n\times n}\) in which \(w_{ij}\in(0,1)\) if node \(j\) is connected to node \(i\) otherwise \(w_{ij}=0\). We use bold symbols to denote stacked vectors or matrices across agents. For example, \(\mathbf{x}^{k}=\text{col}\{x_{1}^{k},...,x_{n}^{k}\}\in\mathbb{R}^{pn}\) and \(\mathbf{W}=W\otimes I_{p}\), where \(\otimes\) denotes the Kronecker product operator.

**A unified framework with moving average.** Building on the formulation in [1; 2], we develop a unified primal-dual framework with moving average for decentralized optimization:

\[\mathbf{r}^{k+1}=(1-\theta)\mathbf{r}^{k}+\theta\mathbf{g}^{k},\quad\mathbf{x }^{k+1}=\mathbf{C}\mathbf{x}^{k}-\alpha\mathbf{A}\mathbf{r}^{k+1}-\mathbf{B} \mathbf{d}^{k},\quad\mathbf{d}^{k+1}=\mathbf{d}^{k}+\mathbf{B}\mathbf{x}^{k+1}.\] (6)

Here \(\mathbf{x}^{k}\) denotes the primal variable, \(\mathbf{d}^{k}\) denotes the dual variable introduced to mitigate the influence of data-heterogeneity, \(\mathbf{g}^{k}\) stacks all (stochastic) gradients evaluated at \(x_{i}^{k}\) for \(1\leq i\leq n\), \(\mathbf{r}^{k}\) denotes the momentum introduced to boost training with coefficient \(\theta\in[0,1]\), and \(\alpha>0\) is the learning rate. Matrices \(\mathbf{A},\mathbf{B},\mathbf{C}\in\mathbb{R}^{pn\times pn}\) are adapted from the mixing matrix \(\mathbf{W}\), which determine how agents communicate with each other. See Appendix B.1 for more detailed motivations.

Framework (6) unifies various decentralized techniques in the literature. For instance, by letting \(\theta=1\) and specifying \(\mathbf{A},\mathbf{B},\mathbf{C}\) delicately, framework (6) reduces to ED, EXTRA, and numerous GTvariants, see Table 3 and Appendix B.1 for more details. Framework (6) is closely related to the unified decentralized method developed in [1; 2]. The primary difference lies in the incorporation of the momentum variable \(\mathbf{r}^{k}\), which can help improve the transient iteration complexity of the framework (6) and relax the smoothness condition for bilevel algorithms [11]. A detailed comparison between framework (6) and that proposed in [1; 2] is provided in Appendix B.2.

``` Initialize \(\mathbf{x}^{0}=\mathbf{y}^{0}=\mathbf{z}^{0}=\mathbf{r}^{0}=\mathbf{0}\), \(\mathbf{d}_{x}^{0}=\mathbf{d}_{y}^{0}=\mathbf{d}_{z}^{0}=\mathbf{0}\), learning rate \(\alpha_{k},\beta_{k},\gamma_{k},\theta_{k}\). for\(k=0,1,\cdots,K-1\)do \(\mathbf{y}^{k+1}=\mathbf{C}_{y}\mathbf{y}^{k}-\beta_{k}\mathbf{A}_{y}\mathbf{ v}^{k}-\mathbf{B}_{y}\mathbf{d}_{y}^{k}\), \(\mathbf{d}_{x}^{k+1}=\mathbf{d}_{y}^{k}+\mathbf{B}_{y}\mathbf{y}^{k+1}; \triangleright\) lower-level update \(\mathbf{z}^{k+1}=\mathbf{C}_{z}\mathbf{z}^{k}-\gamma_{k}\mathbf{A}_{z} \mathbf{p}^{k}-\mathbf{B}_{z}\mathbf{d}_{z}^{k}\), \(\mathbf{d}_{z}^{k+1}=\mathbf{d}_{z}^{k}+\mathbf{B}_{z}\mathbf{z}^{k+1}; \triangleright\) auxiliary-level update \(\mathbf{r}^{k+1}=(1-\theta_{k})\mathbf{r}^{k}+\theta_{k}\mathbf{u}^{k}\); \(\triangleright\) momentum update \(\mathbf{x}^{k+1}=\mathbf{C}_{x}\mathbf{x}^{k}-\alpha_{k}\mathbf{A}_{x} \mathbf{r}^{k+1}-\mathbf{B}_{x}\mathbf{d}_{x}^{k}\), \(\mathbf{d}_{x}^{k+1}=\mathbf{d}_{x}^{k}+\mathbf{B}_{x}\mathbf{x}^{k+1}; \triangleright\) upper-level update endfor ```

**Algorithm 1** SPARKLE: A unified framework for decentralized stochastic bilevel optimization

### A unified framework for decentralized bilevel optimization.

By utilizing the unified framework (6) to approximately solve each subproblem in (5) with only _one iteration_, we achieve SPARKLE, a unified single-loop framework for decentralized bilevel optimization. In particular, we independently sample data \(\xi_{i}^{k}\sim\mathcal{D}_{f_{i}}\), \(\zeta_{i}^{k}\sim\mathcal{D}_{g_{i}}\) within each node at iteration \(k\), and evaluate stochastic gradients/Jacobians/Hessians as follows

\[l_{i}^{k} =\nabla_{1}F_{i}(x_{i}^{k},y_{i}^{k};\xi_{i}^{k}),\qquad b_{i}^{k }=\nabla_{2}F_{i}(x_{i}^{k},y_{i}^{k};\xi_{i}^{k}),\qquad v_{i}^{k}=\nabla_{2} G_{i}(x_{i}^{k},y_{i}^{k};\zeta_{i}^{k}),\] \[J_{i}^{k} =\nabla_{12}^{2}G_{i}(x_{i}^{k},y_{i}^{k};\zeta_{i}^{k}),\quad H_{ i}^{k}=\nabla_{22}^{2}G_{i}(x_{i}^{k},y_{i}^{k};\zeta_{i}^{k}).\]

Next we stack the descent directions for variables of each level as follows

\[\text{lower-level stochstic gradient:} \quad\mathbf{v}^{k}=\text{col}\{v_{1}^{k},...,v_{n}^{k}\},\] \[\text{auxilliary-level stochstic gradient:} \quad\mathbf{p}^{k}=\text{col}\{H_{1}^{k}z_{1}^{k}-b_{1}^{k},...,H_{n}^{k}z_{n}^{ k}-b_{n}^{k}\},\] \[\text{upper-level stochstic gradient:} \quad\mathbf{u}^{k}=\text{col}\{l_{1}^{k}-J_{1}^{k}z_{1}^{k+1},...,l_{n}^{k}-J_{n}^{ k}z_{n}^{k+1}\}.\]

The SPARKLE algorithm is detailed in Algorithm 1. In this algorithm, we utilize different dual variables \(\mathbf{d}_{s}\) and communication matrices \(\mathbf{A}_{s},\mathbf{B}_{s},\mathbf{C}_{s}\) for each variable \(s\in\{x,y,z\}\) to optimize their respective objective functions. We use momentum \(\mathbf{r}^{k}\) only for updating the upper-level variable, which is sufficient to enhance convergence of bilevel algorithms and relax the smoothness condition.

**Versatility in decentralized strategies.** SPARKLE is highly versatile, supporting various decentralized strategies by allowing the specification of different communication matrices \(\mathbf{A}_{s}\), \(\mathbf{B}_{s}\), and \(\mathbf{C}_{s}\). For example, by setting \(\mathbf{A}_{s}=\mathbf{I}\), \(\mathbf{B}_{s}=(\mathbf{I}-\mathbf{W})^{1/2}\), and \(\mathbf{C}_{s}=\mathbf{W}\) for any \(s\in\{x,y,z\}\), SPARKLE will utilize EXTRA to update variables \(x,y\), and \(z\), resulting in the SPARKLE-EXTRA variant. Other variants can be achieved by setting \(\mathbf{A}_{s}\), \(\mathbf{B}_{s}\), and \(\mathbf{C}_{s}\) according to Table 3. These variants can be implemented more efficiently than listed in Algorithm 1, see Appendix B.3.

**Flexibility across optimization levels.** SPARKLE supports different optimization and communication mechanisms for each level of (5), which can be directly achieved by choosing different \(\mathbf{A}_{s}\), \(\mathbf{B}_{s}\), and \(\mathbf{C}_{s}\) matrices for each level \(s\in\{x,y,z\}\). For example, SPARKLE can utilize GT to update the upper-level variable \(x\) while employing ED to update the auxiliary- and lower-level variables \(y\) and \(z\). Throughout this paper, we denote SPARKLE using the decentralized mechanism \(\mathbf{L}\) for the lower-level and auxiliary variables, and \(\mathbf{U}\) for the upper-level in Algorithm 1, by SPARKLE-\(\mathbf{L}\)-\(\mathbf{U}\), or simply SPARKLE-\(\mathbf{L}\) if \(\mathbf{L}=\mathbf{U}\). In addition, SPARKLE even supports utilizing different mixing matrices \(\mathbf{W}_{x},\mathbf{W}_{y},\mathbf{W}_{z}\) across levels.

## 3 Convergence analysis

In this section, we establish the convergence properties of the SPARKLE framework and examine the influence of different decentralized techniques utilized across optimization levels.

### Assumptions

Before presenting the theoretical guarantees, we first introduce the following assumptions used throughout this paper.

**Assumption 1**.: _There exist constants \(\mu_{g},L_{f,0},L_{f,1},L_{g,1},L_{g,2}\) such that for any \(1\leq i\leq n\),_

1. \(\nabla f_{i},\nabla g_{i},\nabla^{2}g_{i}\) _are_ \(L_{f,1},L_{g,1},L_{g,2}\) _Lipschitz continuous, respectively;_
2. \(\left\|\nabla_{2}f_{i}\left(x,y^{\star}(x)\right)\right\|\leq L_{f,0}\) _for any_ \(x\in\mathbb{R}^{p}\)_;_3__ Footnote 3: This is more relaxed than Lipschitz continuous \(f_{i}\), or bounded \(\nabla_{2}f_{i}\) in [21, 57, 33, 11].
3. \(g_{i}(x,y)\) _is_ \(\mu_{g}\)_-strongly convex with respect to_ \(y\) _for any fixed_ \(x\in\mathbb{R}^{p}\)_._

_Moreover, we define \(L:=\max\{L_{f,0},L_{f,1},L_{g,1},L_{g,2}\}\) and \(\kappa:=L/\mu_{g}\)._

**Assumption 2**.: _For each \(s\in\{x,y,z\}\), the corresponding mixing matrix \(W_{s}\in\mathbb{R}^{n\times n}\) is non-negative, symmetric and doubly stochastic, i.e.,_

\[W_{s}=W_{s}^{\top},\quad W_{s}\mathbf{1}_{n}=\mathbf{1}_{n},\quad(W_{s})_{ij} \geq 0,\quad\forall\,1\leq i,j\leq n,\]

_and the corresponding communication graph is strongly-connected, i.e., its eigenvalues satisfy \(1=\lambda_{1}(W_{s})>\lambda_{2}(W_{s})\geq\ldots\geq\lambda_{n}(W_{s})\) and \(\rho(W_{s}):=\max\left\{\left|\lambda_{2}(W_{s})\right|,\left|\lambda_{n}(W_{s} )\right|\right\}<1\)._

The value \(1-\rho(W_{s})\) is referred to as the spectral gap in the literature [34, 53, 31] of \(W_{s}\), which measures the connectivity of the communication graph. It would approach \(0\) for sparse networks. For example, it holds that \(1-\rho(W_{s})=\Theta(1/n^{2})\) for the matrix \(W_{s}\) induced by a ring graph.

**Assumption 3**.: _For any \(s\in\{x,y,z\}\), we assume the communication matrices \(A_{s},B_{s},C_{s}\) used in SPARKLE are polynomial functions of \(W_{s}\). Furthermore, we assume \(A_{s},C_{s}\) are doubly stochastic, and \(\mathrm{Null}(B_{s})=\mathrm{Span}\{\mathbf{1}_{n}\}\). In addition, we assume all eigenvalues of the augmented matrix_

\[L_{s}:=\left[\begin{array}{cc}\overline{C_{s}}-B_{s}^{2}&B_{s}\\ -B_{s}&\overline{I_{n}}\end{array}\right]\]

_are strictly less than one in magnitude, where \(\overline{C_{s}}\triangleq C_{s}-\frac{1}{n}\mathbf{1}_{n}\mathbf{1}_{n}^{\top}\) and \(\overline{I_{n}}\triangleq I_{n}-\frac{1}{n}\mathbf{1}_{n}\mathbf{1}_{n}^{\top}\)._

We remark that Assumption 3 is mild and is satisfied by all choices listed in Table 3. See more discussions in Appendix C.2.

**Assumption 4**.: _We assume \(\nabla F_{i}(x,y;\xi),\nabla G_{i}(x,y;\xi)\), and \(\nabla^{2}G_{i}(x,y;\xi)\) to be unbiased estimates of \(\nabla f_{i}(x,y),\nabla g_{i}(x,y)\), and \(\nabla^{2}g_{i}(x,y)\) with bounded variances \(\sigma_{f,1}^{2},\sigma_{g,1}^{2},\sigma_{g,2}^{2}\), respectively._

### Convergence theorem

Under the above assumptions, we establish the convergence properties as follows. Proof details can be found in Appendix C.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline Algorithms & \(\mathbf{A}_{s}\) & \(\mathbf{B}_{s}\) & \(\mathbf{C}_{s}\) & The specific update rule at the \(k\)-th iteration. \\ \hline ED & \(\mathbf{W}_{s}\) & \((\mathbf{I}-\mathbf{W}_{s})^{\frac{1}{2}}\) & \(\mathbf{W}_{s}\) & \(\mathbf{s}^{k+2}=\mathbf{W}_{s}\left(2\mathbf{s}^{k+1}-\mathbf{s}^{k}-\alpha \left(\mathbf{g}(\mathbf{s}^{k+1})-\mathbf{g}(\mathbf{s}^{k})\right)\right)\) \\ EXTRA & \(\mathbf{I}\) & \((\mathbf{I}-\mathbf{W}_{s})^{\frac{1}{2}}\) & \(\mathbf{W}_{s}\) & \(\mathbf{s}^{k+2}=\mathbf{W}_{s}\left(2\mathbf{s}^{k+1}-\mathbf{s}^{k}\right)- \alpha\left(\mathbf{g}(\mathbf{s}^{k+1})-\mathbf{g}(\mathbf{s}^{k})\right)\) \\ ATC-GT & \(\mathbf{W}_{s}^{2}\) & \(\mathbf{I}-\mathbf{W}_{s}\) & \(\mathbf{W}_{s}^{2}\) & \(\mathbf{s}^{k+1}=\mathbf{W}_{s}\left(\mathbf{s}^{k}-\alpha\mathbf{h}_{s}^{k} \right)\), \(\mathbf{h}_{s}^{k+1}=\mathbf{W}_{s}\left(\mathbf{h}_{s}^{k}+\mathbf{g}( \mathbf{s}^{k+1})-\mathbf{g}(\mathbf{s}^{k})\right)\) \\ Semi-ATC-GT & \(\mathbf{W}_{s}\) & \(\mathbf{I}-\mathbf{W}_{s}\) & \(\mathbf{W}_{s}^{2}\) & \(\mathbf{s}^{k+1}=\mathbf{W}_{s}\mathbf{s}^{k}-\alpha\mathbf{h}_{s}^{k}\), \(\mathbf{h}_{s}^{k+1}=\mathbf{W}_{s}\left(\mathbf{h}_{s}^{k}+\mathbf{g}( \mathbf{s}^{k+1})-\mathbf{g}(\mathbf{s}^{k})\right)\) \\ Non-ATC-GT & \(\mathbf{I}\) & \(\mathbf{I}-\mathbf{W}_{s}\) & \(\mathbf{W}_{s}^{2}\) & \(\mathbf{s}^{k+1}=\mathbf{W}_{s}\mathbf{s}^{k}-\alpha\mathbf{h}_{s}^{k}\), \(\mathbf{h}_{s}^{k+1}=\mathbf{W}_{s}\mathbf{h}_{s}^{k}+\mathbf{g}(\mathbf{s}^{k+1 })-\mathbf{g}(\mathbf{s}^{k})\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: SPARKLE facilitates different decentralized techniques by specifying \(\mathbf{A}_{s},\mathbf{B}_{s},\mathbf{C}_{s}\) for \(s\!\in\!\{x,y,z\}\). We denote the stacked local variables and the associate gradients estimates by \(\mathbf{s}\!\in\!\{\mathbf{x},\mathbf{y},\mathbf{z}\}\) and \(\mathbf{g}(\mathbf{s})\), respectively. The update rule refers to the specific algorithmic recursion for each level. See derivations in Appendix B.2.

**Theorem 1**.: _Under Assumptions 1 - 4, there exist proper constant step-sizes \(\alpha,\;\beta,\,\gamma\) and momentum coefficient \(\theta\), such that the SPARKLE framework listed in Algorithm 1 will converge as follow:_

\[\frac{1}{K+1}\sum_{k=0}^{K}\mathbb{E}[\|\nabla\Phi(\bar{x}^{k})\|^ {2}]\lesssim\frac{\kappa^{5}\sigma}{\sqrt{nK}}+\kappa^{\frac{16}{3}}\left( \delta_{y,1}+\delta_{z,1}\right)\frac{\sigma^{\frac{2}{3}}}{K^{\frac{2}{3}}}+ \kappa^{\frac{7}{2}}\delta_{x,1}\frac{\sigma^{\frac{1}{2}}}{K^{\frac{1}{4}}}\] \[\qquad+\left(\kappa^{\frac{26}{5}}\delta_{y,2}+\kappa^{6}\delta_ {z,2}\right)\frac{\sigma^{\frac{2}{5}}}{K^{\frac{8}{3}}}+\left(\kappa^{\frac{1 6}{3}}\delta_{y,3}+\kappa^{\frac{14}{3}}\delta_{z,3}+\kappa^{\frac{8}{3}} \delta_{x,3}\right)\frac{1}{K}+\left(\kappa C_{\alpha}+\kappa^{4}C_{\theta} \right)\frac{1}{K},\]

_where \(\sigma\triangleq\max\{\sigma_{f,1},\sigma_{g,1},\sigma_{g,2}\}\), \(\left\{\delta_{s,i}\right\}_{i=1}^{3}\) are constants depending only on \(\mathbf{W}_{s},\mathbf{A}_{s},\mathbf{B}_{s},\mathbf{C}_{s}\) for \(s\in\{x,y,z\}\), and \(C_{\alpha},C_{\theta}\) are constants independent of \(K\). See Lemma 17 for their detailed values._

In the deterministic scenario with \(\sigma=0\), SPARKLE converges at the rate \(\mathcal{O}(1/K)\), see the formal theorem and derivation in Appendix C.3. This recovers the rate in [15] under even milder assumptions. Unlike reference [15], which only considers GT in the deterministic setting, SPARKLE is a unified bilevel framework for the more general stochastic setting.

**Linear speedup.** According to Theorem 1, SPARKLE achieves an asymptotic linear speedup as \(K\) approaches infinity, which applies to all SPARKLE variants regardless of the decentralized strategies employed and whether they are utilized at different optimization levels. Furthermore, the asymptotically dominant term \(\kappa^{5}\sigma/(\sqrt{nK})\) matches exactly with the single-node bilevel algorithm SOBA [12] when \(n=1\), implying the tightness of Theorem 1 in terms of the asymptotic rate.

**Remark 1**.: _We establish an upper bound for the consensus error \(\frac{1}{K}\sum\limits_{k=0}^{K}\mathbb{E}\left[\frac{\|\mathbf{x}^{k}-\bar{ \mathbf{x}}^{k}\|^{2}}{n}+\frac{\|\mathbf{y}^{k}-\bar{\mathbf{y}}^{k}\|^{2}}{ n}\right]\). Please refer to Lemma 19 in Appendix C.2.1 for more details._

### Transient iteration complexity

With the non-asymptotic rate established in Theorem 1, we can derive the transient iteration complexity of SPARKLE as follows. The proof is in Lemma 18.

**Corollary 1**.: _Under the same assumptions as in Theorem 1, the transient iteration complexity of SPARKLE--with the influence of \(\kappa\) and \(\sigma^{2}\) omitted for brevity--is on the order of_

\[\max\left\{n^{2}\delta_{x},n^{3}\delta_{y},n^{3}\delta_{z},n\hat{\delta}_{x}, n\hat{\delta}_{y},n\hat{\delta}_{z}\right\},\] (8)

_where \(\delta_{s},\hat{\delta}_{s}\) only depend \(\mathbf{W}_{s},\mathbf{A}_{s},\mathbf{B}_{s},\mathbf{C}_{s}\) for \(s\in\{x,y,z\}\). Their values are in Lemma 18._

We obtain the transient iteration complexity of each variant of SPARKLE by applying Corollary 1.

**Corollary 2**.: _For SPARKLE-ED and SPARKLE-EXTRA, if we choose \(\mathbf{W}_{y}=\mathbf{W}_{z}\), it holds that_

\[\begin{split}\delta_{x}&=\mathcal{O}\left((1-\rho( \mathbf{W}_{x}))^{-2}\right),\qquad\delta_{y}=\delta_{z}=\mathcal{O}\left((1- \rho(\mathbf{W}_{y}))^{-2}\right),\\ \hat{\delta}_{x}&=\mathcal{O}\left((1-\rho(\mathbf{W }_{x}))^{-\frac{3}{2}}\right),\qquad\hat{\delta}_{y}=\hat{\delta}_{z}= \mathcal{O}\left((1-\rho(\mathbf{W}_{y}))^{-2}\right).\end{split}\] (9)

_Furthermore, if we choose \(\mathbf{W}_{x}=\mathbf{W}_{y}=\mathbf{W}_{z}\) and denote \(\rho\triangleq\rho(\mathbf{W}_{x})\), the transient iteration complexity derived in (8) can be simplified as \(n^{3}/(1-\rho)^{2}\)._

**Corollary 3**.: _For SPARKLE-GT and its variants with semi/non-ATC-GT, if we let \(\mathbf{W}_{y}=\mathbf{W}_{z}\),_

\[\begin{split}\delta_{x}&=\mathcal{O}\left((1-\rho( \mathbf{W}_{x}))^{-2}\right),\qquad\quad\delta_{y}=\delta_{z}=\mathcal{O} \left((1-\rho(\mathbf{W}_{y}))^{-2}\right),\\ \hat{\delta}_{x}&=\mathcal{O}\left((1-\rho(\mathbf{W }_{x}))^{-2}\right),\qquad\quad\hat{\delta}_{y}=\hat{\delta}_{z}=\mathcal{O} \left((1-\rho(\mathbf{W}_{y}))^{-\frac{8}{3}}\right).\end{split}\]

_Furthermore, if we let \(\mathbf{W}_{x}=\mathbf{W}_{y}=\mathbf{W}_{z}\) and denote \(\rho\triangleq\rho(\mathbf{W}_{x})\), the transient iteration complexity derived in (8) can be simplified as \(\max\{n^{3}/(1-\rho)^{2},n/(1-\rho)^{8/3}\}\)._

**Remark 2** (_SOTA transient iterations_).: Comparing with algorithms listed in Table 1, all SPARKLE variants achieve smaller transient iteration complexity, implying that they can achieve linear speedup much faster than the other algorithms, especially over sparse network topologies with \(1-\rho\to 0\).

**Remark 3** (_GT is not the best technique for decentralized SBO_).: While GT is widely adopted in the literature [16; 21; 57] to facilitate decentralized SBO, a comparison of Corollary 2 and 3 reveals that both SPARKLE-EXTRA and SPARKLE-ED outperform SPARKLE-GT in terms of transient iteration complexity. This implies that EXTRA and ED are better than GT for decentralized SBO.

### Different strategies across optimization levels

Corollary 1 clarifies how different update strategies for \(x\), \(y\), and \(z\) impact the transient iterations through constants \(\{\delta_{s},\hat{\delta}_{s}\}\) for \(s\in\{x,y,z\}\). Since \(\delta_{y}=\delta_{z}\) and \(\hat{\delta}_{y}=\hat{\delta}_{z}\) when \(\mathbf{W}_{y}=\mathbf{W}_{z}\) (Lemma 18), we naturally employ the same strategy to update \(y\) and \(z\). The following corollary studies the utilization of both ED and GT in SPARKLE. See the transient iterations complexity of other mixed strategies in Appendix C.2.4 and Table 2.

**Corollary 4**.: _For SPARKLE-ED-GT which uses ED to update \(y\) and \(z\) and GT to update \(x\), if \(\mathbf{W}_{x}=\mathbf{W}_{y}=\mathbf{W}_{z}\) and we denote \(\rho=\rho(\mathbf{W}_{x})\), it then holds that_

\[\delta_{x}=\delta_{y}=\delta_{z}=\mathcal{O}\left((1-\rho)^{-2}\right),\qquad \hat{\delta}_{x}=\hat{\delta}_{y}=\hat{\delta}_{z}=\mathcal{O}\left((1-\rho)^ {-2}\right),\]

_which implies that the transient iteration complexity in (8) can be simplified as \(n^{3}/(1-\rho)^{2}\)._

**Remark 4** (_Mixed strategies outperform employing GT only)_.: Comparing Corollary 3 and 4, we find that using ED to update \(y\) and \(z\) will lead to smaller \(\hat{\delta}_{y}\) and \(\hat{\delta}_{z}\), which improves the transient iteration complexity compared to employing GT only in all optimization levels (see Corollary 3).

### Different topologies across optimization levels

In SPARKLE, we can utilize different topologies across levels. Theorem 1 and Corollary 1 have clarified the influence of using different topologies across levels through the constants \(\{\delta_{s},\hat{\delta}_{s}\}\) for \(s\in\{x,y,z\}\). For instance, when substituting \(\{\delta_{s},\hat{\delta}_{s}\}\) established in (9) into (8), SPARKLE-ED has the following transient iteration complexity:

\[\max\{n^{2}(1-\rho(\mathbf{W}_{x}))^{-2},n^{3}(1-\rho(\mathbf{W}_{y}))^{-2}\}\]

where \(\mathbf{W}_{x}\) is the mixing matrix for updating \(x\), while \(\mathbf{W}_{y}\) is for updating \(y\) and \(z\). As long as \((1-\rho(\mathbf{W}_{x}))^{-1}\lesssim\sqrt{n}(1-\rho(\mathbf{W}_{y}))^{-1}\) holds, SPARKLE-ED retains the transient iteration complexity of \(n^{3}(1-\rho(\mathbf{W}_{y}))^{-2}\), which allows for the utilization of a sparser network topology when updating \(x\), thereby reducing communication overheads. Consequently, the ratio \(a\) of the communication volume per round for the variables \(x\) and \(y\) can be significantly less than one. See Appendix C.2.3 for discussion on how to use different topologies across levels in other SPARKLE variants.

### Recovering single-level decentralized optimization

Previous works typically study single-level and bilevel optimization separately. By taking \(G_{i}(x,y,\xi)\equiv|y|^{2}/2\) and \(F_{i}(x,y,\phi)=F_{i}(x,\phi)\) into (2), the decentralized SBO problem (1) reduces to stochastic single-level optimization. By setting \(\mathbf{z}^{k}\equiv 0\), \(\mathbf{y}^{k}\equiv 0\), \(u_{i}^{k}=\nabla_{1}f_{i}(x_{i}^{k},\xi_{i}^{k})\), SPARKLE reduces to the single-level framework (6), whose convergence can be naturally guaranteed by Theorem 1. Please refer to Appendix C.4 for the detailed proof and results. This is the _first_ result demonstrating that bilevel optimization essentially subsumes the convergence of single-level optimization.

## 4 Numerical experiments

In this section, we present experiments to validate our theoretical findings. We first explore how update strategies and network structures influence the convergence of SPARKLE. Then we compare SPARKLE to the existing decentralized SBO algorithms. Additional experiments about a decentralized SBO problem with synthetic data are in Appendix D.1.

**Hyper-cleaning on FashionMNIST dataset.** We consider a data hyper-cleaning problem [44] on a corrupted FashionMNIST dataset [48]. Problem formulations and experimental setups can be found in Appendix D.2. Firstly, we equip SPARKLE with different decentralized strategies in different optimization levels and then compare them with D-SOBA [29], MA-DSBO-GT [10], and MDBO [21] using the corruption rate \(p=0.1,0.2,0.3\), respectively. As is shown in Figure 1, all the SPARKLE-based algorithms generally achieve higher test accuracy than D-SOBA, while ED and EXTRA especially outperform GT. Meanwhile, using mixed strategies (_i.e._, SPARKLE-ED-GT and SPARKLE-EXTRA-GT) achieves similar test accuracy with SPARKLE-ED and SPARKLE-EXTRA and outperform SPARKLE-GT, respectively. These observations match with the theoretical results in Corollary 2-4 and Remark 3, 4.

Next, we test SPARKLE-EXTRA with two communication strategies including _fixed topology for updating \(x\) and varying topology for \(y,z\)_, and _fixed topology for updating \(y,z\) and varying topology for \(x\)_. As illustrated in Figure 2, maintaining a fixed topology for \(x\) while reducing the connectivity of the topology for \(y\) and \(z\) will deteriorate the algorithmic performance. Conversely, preserving the topology for \(y\) and \(z\) while decreasing the connectivity for \(x\) has little impact on the performance. This suggests that the influence of the network topology for \(y\) and \(z\) on the algorithm dominates over the topology for \(x\), which is consistent with our discussion in Section 3.5. We also numerically examine the influence of moving average on convergence, see discussions in Appendix D.2.

**Distributed policy evaluation in reinforcement learning.** We consider a multi-agent MDP problem in reinforcement learning on a distributed setting with \(n\in\{10,20\}\) agents respectively, which can be formulated as a decentralized SBO problems [52]. Here, we compare SPARKLE with existing decentralized SBO approaches including MDBO [21] and the stochastic extension of SLDBO [16] over a Ring graph. Figure 3 illustrates that SPARKLE converges faster and achieves a lower sample complexity than the other baselines, especially when \(n=20\), which shows the empirical benefits of SPARKLE in decentralized SBO algorithms with a large number of agents and sparse communication modes. More experimental details are in Appendix D.3.

**Decentralized meta-learning.** We investigate decentralized meta-learning on miniImageNet [47] with multiple tasks [18], formulating it as a decentralized bilevel optimization problem. This approach minimizes the validation loss with respect to shared parameters as the upper-level loss, while the training loss is managed by task-specific parameters at the lower level. Additional details about the experiment can be found in Appendix D.4. Our method, SPARKLE, is benchmarked against D-SOBA [29] and MAML [18], demonstrating a significant improvement in training accuracy.

## 5 Conclusions and limitations

This paper proposes SPARKLE, a unified single-loop primal-dual framework for decentralized stochastic bilevel optimization. Being highly versatile, SPARKLE can support different decentralized mechanisms and topologies across optimization levels. Moreover, all SPARKLE variants have been demonstrated to achieve state-of-the-art convergence rate compared to existing algorithms. However, SPARKLE currently supports only strongly-convex problems in the lower-level optimization. Its compatibility with generally-convex lower-level problems remains unknown. Additionally, the condition number of the lower-level problem significantly impacts the performance, as is the case with existing bilevel algorithms. We aim to address these limitations in future work.

Acknowledgment

The work of Shuchen Zhu, Boao Kong, and Kun Yuan is supported by Natural Science Foundation of China under Grants 92370121, 12301392, and W2441021. This work is also supported by Open Project of Key Laboratory of Mathematics and Information Networks, Ministry of Education, China. No. KF202302.

## References

* [1] S. A. Alghunaim, E. K. Ryu, K. Yuan, and A. H. Sayed. Decentralized proximal gradient algorithms with linear convergence rates. _IEEE Transactions on Automatic Control_, 66(6):2787-2794, 2020.
* [2] S. A. Alghunaim and K. Yuan. A unified and refined convergence analysis for non-convex decentralized learning. _IEEE Transactions on Signal Processing_, 2022.
* [3] S. Arora, S. Du, S. Kakade, Y. Luo, and N. Saunshi. Provable representation learning for imitation learning via bi-level optimization. In _International Conference on Machine Learning_, pages 367-376. PMLR, 2020.
* [4] L. Bertinetto, J. Henriques, P. Torr, and A. Vedaldi. Meta-learning with differentiable closed-form solvers. In _International Conference on Learning Representations (ICLR), 2019_. International Conference on Learning Representations, 2019.
* [5] T.-H. Chang, M. Hong, and X. Wang. Multi-agent distributed optimization via inexact consensus admm. _IEEE Transactions on Signal Processing_, 63(2):482-497, 2014.
* [6] J. Chen and A. H. Sayed. Diffusion adaptation strategies for distributed optimization and learning over networks. _IEEE Transactions on Signal Processing_, 60(8):4289-4305, 2012.
* [7] T. Chen, Y. Sun, Q. Xiao, and W. Yin. A single-timescale method for stochastic bilevel optimization. In _International Conference on Artificial Intelligence and Statistics_, pages 2466-2488. PMLR, 2022.
* [8] T. Chen, Y. Sun, and W. Yin. Closing the gap: Tighter analysis of alternating stochastic gradient methods for bilevel problems. _Advances in Neural Information Processing Systems_, 34:25294-25307, 2021.
* [9] X. Chen, M. Huang, and S. Ma. Decentralized bilevel optimization. _arXiv preprint arXiv:2206.05670_, 2022.
* [10] X. Chen, M. Huang, S. Ma, and K. Balasubramanian. Decentralized stochastic bilevel optimization with improved per-iteration complexity. In _International Conference on Machine Learning_, pages 4641-4671. PMLR, 2023.
* [11] X. Chen, T. Xiao, and K. Balasubramanian. Optimal algorithms for stochastic bilevel optimization under relaxed smoothness conditions. _arXiv preprint arXiv:2306.12067_, 2023.
* [12] M. Dagreou, P. Ablin, S. Vaiter, and T. Moreau. A framework for bilevel optimization that enables stochastic and global variance reduction algorithms. _Advances in Neural Information Processing Systems_, 35:26698-26710, 2022.
* [13] P. Di Lorenzo and G. Scutari. Next: In-network nonconvex optimization. _IEEE Transactions on Signal and Information Processing over Networks_, 2(2):120-136, 2016.
* [14] J. Domke. Generic methods for optimization-based modeling. In _Artificial Intelligence and Statistics_, pages 318-326. PMLR, 2012.
* [15] J. Dong, Z. Cao, T. Zhang, J. Ye, S. Wang, F. Feng, L. Zhao, et al. Eflops: Algorithm and system co-design for a high performance distributed training platform. In _2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)_, pages 610-622, 2020.
* [16] Y. Dong, S. Ma, J. Yang, and C. Yin. A single-loop algorithm for decentralized bilevel optimization. _arXiv preprint arXiv:2311.08945_, 2023.
* [17] J. C. Duchi, A. Agarwal, and M. J. Wainwright. Dual averaging for distributed optimization: Convergence analysis and network scaling. _IEEE Transactions on Automatic control_, 57(3):592-606, 2011.
* [18] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. _International Conference on Machine Learning_, pages 1126-1135, 2017.

* [19] L. Franceschi, P. Frasconi, S. Salzo, R. Grazzi, and M. Pontil. Bilevel programming for hyperparameter optimization and meta-learning. In _International Conference on Machine Learning_, pages 1568-1577. PMLR, 2018.
* [20] B. Gao, Y. Yang, and Y. Xiang Yuan. Lancho: dynamic lanczos-aided bilevel optimization via krylov subspace. _arXiv preprint arXiv:2404.03331_, 2024.
* [21] H. Gao, B. Gu, and M. T. Thai. On the convergence of distributed stochastic bilevel optimization algorithms over a network. In _International Conference on Artificial Intelligence and Statistics_, pages 9238-9281. PMLR, 2023.
* [22] S. Ghadimi and M. Wang. Approximation methods for bilevel programming. _arXiv preprint arXiv:1802.02246_, 2018.
* [23] R. Grazzi, L. Franceschi, M. Pontil, and S. Salzo. On the iteration complexity of hypergradient computation. In _International Conference on Machine Learning_, pages 3748-3758. PMLR, 2020.
* [24] Z. Guo, Q. Hu, L. Zhang, and T. Yang. Randomized stochastic variance-reduced methods for multi-task stochastic bilevel optimization. _arXiv preprint arXiv:2105.02266_, 2021.
* [25] M. Hong, H.-T. Wai, Z. Wang, and Z. Yang. A two-timescale stochastic algorithm framework for bilevel optimization: Complexity analysis and application to actor-critic. _SIAM Journal on Optimization_, 33(1):147-180, 2023.
* [26] D. Jakovetic. A unification and generalization of exact distributed first-order methods. _IEEE Transactions on Signal and Information Processing over Networks_, 5(1):31-46, 2018.
* [27] K. Ji, J. Yang, and Y. Liang. Bilevel optimization: Convergence analysis and enhanced design. In _International Conference on Machine Learning_, pages 4882-4892. PMLR, 2021.
* [28] A. Koloskova, T. Lin, and S. U. Stich. An improved analysis of gradient tracking for decentralized machine learning. _Advances in Neural Information Processing Systems_, 34:11422-11435, 2021.
* [29] B. Kong, S. Zhu, S. Lu, X. Huang, and K. Yuan. Decentralized bilevel optimization over graphs: Loopless algorithmic update and transient iteration complexity. _arXiv preprint arXiv:2402.03167_, 2024.
* [30] Z. Li, W. Shi, and M. Yan. A decentralized proximal-gradient method with network independent step-sizes and separated convergence rates. _IEEE Transactions on Signal Processing_, July 2019. early acces. Also available on arXiv:1704.07807.
* [31] X. Lian, C. Zhang, H. Zhang, C.-J. Hsieh, W. Zhang, and J. Liu. Can decentralized algorithms outperform centralized algorithms? A case study for decentralized parallel stochastic gradient descent. In _Advances in Neural Information Processing Systems_, pages 5330-5340, 2017.
* [32] T. Lin, S. P. Karimireddy, S. U. Stich, and M. Jaggi. Quasi-global momentum: Accelerating decentralized deep learning on heterogeneous data. In _International Conference on Machine Learning_, 2021.
* [33] S. Lu, S. Zeng, X. Cui, M. Squillante, L. Horesh, B. Kingsbury, J. Liu, and M. Hong. A stochastic linearized augmented lagrangian method for decentralized bilevel optimization. _Advances in Neural Information Processing Systems_, 35:30638-30650, 2022.
* [34] Y. Lu and C. De Sa. Optimal complexity in decentralized training. In _International Conference on Machine Learning_, pages 7111-7123. PMLR, 2021.
* [35] D. Maclaurin, D. Duvenaud, and R. Adams. Gradient-based hyperparameter optimization through reversible learning. In _International Conference on Machine Learning_, pages 2113-2122. PMLR, 2015.
* [36] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to adversarial attacks. In _International Conference on Learning Representations_, 2018.
* [37] A. Nedic, A. Olshevsky, and M. G. Rabbat. Network topology and communication-computation tradeoffs in decentralized optimization. _Proceedings of the IEEE_, 106(5):953-976, 2018.
* [38] A. Nedic, A. Olshevsky, and W. Shi. Achieving geometric convergence for distributed optimization over time-varying graphs. _SIAM Journal on Optimization_, 27(4):2597-2633, 2017.
* [39] A. Nedic and A. Ozdaglar. Distributed subgradient methods for multi-agent optimization. _IEEE Transactions on Automatic Control_, 54(1):48-61, 2009.

* [40] Y. Niu, J. Xu, Y. Sun, Y. Huang, and L. Chai. Distributed stochastic bilevel optimization: Improved complexity and heterogeneity analysis. _arXiv preprint arXiv:2312.14690_, 2023.
* [41] G. Qu and N. Li. Harnessing smoothness to accelerate distributed optimization. _IEEE Transactions on Control of Network Systems_, 5(3):1245-1260, 2018.
* [42] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. _International Journal of Computer Vision_, 115:211-252, 2015.
* [43] A. H. Sayed. Adaptive networks. _Proceedings of the IEEE_, 102(4):460-497, 2014.
* [44] A. Shaban, C.-A. Cheng, N. Hatch, and B. Boots. Truncated back-propagation for bilevel optimization. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 1723-1732. PMLR, 2019.
* [45] W. Shi, Q. Ling, G. Wu, and W. Yin. EXTRA: An exact first-order algorithm for decentralized consensus optimization. _SIAM Journal on Optimization_, 25(2):944-966, 2015.
* [46] H. Tang, X. Lian, M. Yan, C. Zhang, and J. Liu. D\({}^{2}\): Decentralized training over decentralized data. In _International Conference on Machine Learning_, pages 4848-4856, 2018.
* [47] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra, et al. Matching networks for one shot learning. _Advances in Neural Information Processing Systems_, 29, 2016.
* [48] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. _arXiv preprint arXiv:1708.07747_, 2017.
* [49] J. Xu, Y. Tian, Y. Sun, and G. Scutari. Distributed algorithms for composite optimization: Unified framework and convergence analysis. _IEEE Transactions on Signal Processing_, 69:3555-3570, 2021.
* [50] J. Xu, S. Zhu, Y. C. Soh, and L. Xie. Augmented distributed gradient methods for multi-agent optimization under uncoordinated constant stepsizes. In _IEEE Conference on Decision and Control (CDC)_, pages 2055-2060, Osaka, Japan, 2015.
* [51] J. Yang, K. Ji, and Y. Liang. Provably faster algorithms for bilevel optimization. _Advances in Neural Information Processing Systems_, 34:13670-13682, 2021.
* [52] S. Yang, X. Zhang, and M. Wang. Decentralized gossip-based stochastic bilevel optimization over communication networks. _Advances in Neural Information Processing Systems_, 35:238-252, 2022.
* [53] K. Yuan, S. A. Alghunaim, and X. Huang. Removing data heterogeneity influence enhances network topology dependence of decentralized SGD. _Journal of Machine Learning Research_, 24(280):1-53, 2023.
* [54] K. Yuan, S. A. Alghunaim, B. Ying, and A. H. Sayed. On the influence of bias-correction on distributed stochastic optimization. _IEEE Transactions on Signal Processing_, 2020.
* [55] K. Yuan, Q. Ling, and W. Yin. On the convergence of decentralized gradient descent. _SIAM Journal on Optimization_, 26(3):1835-1854, 2016.
* Part I: Algorithm development. _IEEE Transactions on Signal Processing_, 67(3):708
- 723, 2018.
* [57] Y. Zhang, M. T. Thai, J. Wu, and H. Gao. On the communication complexity of decentralized bilevel optimization. _arXiv preprint arXiv:2311.11342_, 2023.

**Appendix for "SPARKLE: A Unified Single-Loop Primal-Dual Framework for Decentralized Bilevel Optimization"**

###### Contents

* A More related works
* B More details of SPARKLE
* B.1 Primal-dual deviation
* B.2 Specific instances
* B.3 Implementation details
* C Convergence analysis
* C.1 Proof of Theorem 1
* C.1.1 Notations
* C.1.2 Basic transformations
* C.1.3 Proof sketch
* C.1.4 Technical lemmas
* C.1.5 Descent lemmas for the upper-level
* C.1.6 Descent lemmas for the lower- and auxiliary-level
* C.1.7 Consensus error analysis
* C.1.8 Proof of the main theorem
* C.2 Analysis of consensus error and transient iteration complexity
* C.2.1 Consensus Error
* C.2.2 Essential matrix norms for analysis
* C.2.3 Theoretical gap between upper-level and lower-level
* C.2.4 The transient iteration complexities of some specific examples in SPARKLE.
* C.3 Convergence analysis in deterministic scenarios
* C.4 Degenerating to single-level algorithms
* D Experimental details
* D.1 Synthetic bilevel optimization
* D.2 Hyper-cleaning on FashionMNIST dataset
* D.3 Distributed policy evaluation in reinforcement learning
* D.4 Decentralized meta-learning
More related works

Bilevel optimization.Bilevel optimization presents substantial difficulties compared to single-level optimization due to its nested structure. Estimating hyper-gradient \(\nabla\Phi(x)\) of the upper level involves solving lower-level problems and estimating the Hessian inverse, which requires additional calculations. Many algorithms and techniques have been proposed to solve the challenge. Approximate Implicit Differentiation (AID)-based algorithms [14; 22; 23; 27] leverage the implicit gradient form of \(\nabla\Phi(x)\), which entails solving a linear system to obtain the Hessian-inverse-vector product. Similarly, [8; 25] utilize the Neumann series to handle the Hessian inverse. Iterative Differentiation (ITD)-based algorithms [19; 35; 14; 23; 27] use iterative methods solving the lower-level problem and then estimate the hyper-gradient through automatic differentiation. However, these approaches introduce inner steps, leading to extra computational overhead and memory spaces. [12] proposes a single-level algorithm called SOBA, which approximating the Hessian-inverse-vector product by solving a quadratic programming problem. A recent work [20] utilizes the Krylov subspace technique and the Lanczos process to approximate it in deterministic scenarios. For stochastic bilevel optimization, various methods have been employed to improve the convergence rate, such as momentum [7; 11] and variance reduction [51; 27; 24].

Decentralized optimization.Decentralized optimization is developed to deal with large-scale optimization problems, where datasets are distributed among multiple agents. Without a central server, each agent only gets access to its own local data and communications are limited to its neighbors in a network. Compared with centralized algorithms, decentralized ones preserve data privacy, and are more robust to contingencies in the communication network. However, due to the absence of a central server, decentralized optimization requires communication among agents, posing greater challenges for convergence, especially in the presence of severe data heterogeneity. To tackle this issue, various algorithms have emerged, such as decentralized gradient descent [39; 55], diffusion strategies [6], dual averaging [17], EXTRA [45], Exact Diffusion (a.k.a. D\({}^{2}\)) [56; 30; 46], gradient tracking [50; 13; 38], and decentralized ADMM [5]. In stochastic scenarios, a common method for decentralized optimization is the decentralized stochastic gradient descent (DSGD), which has gained a lot of attentions recently. It has been proved to achieve linear speedup asymptotically and shares the same asymptotic rate with centralized stochastic gradient descent [31].

## Appendix B More details of Sparkle

### Primal-dual deviation

Here we provide a detailed motivation of the update framework (6) for decentralized single-level algorithms. First, we rewrite the single-level distributed optimization problem in the following equivalent form:

\[\min_{x_{i}\in\mathbb{R}^{d}}f(x_{1},...,x_{n})=\frac{1}{n}\sum_{i=1}^{n}f_{i} \left(x_{i}\right),\quad\text{s.t. }x_{1}=...=x_{n},\] (10)

where each \(f_{i}\) is smooth and possibly non-convex. To simplify the notation, we assume that \(d=1\) without loss of generality. Now we introduce three symmetric matrices \(A,B,D\) such that \(A\) is a doubly stochastic communication matrix with \(\rho(A)<1\), and \(B,D\) satisfy \(\mathrm{Null}B=\mathrm{Null}D=\mathrm{Span}\{1_{n}\}\). In general, \(B\) (\(D\)) determines the topology of a connected graph \(\mathcal{G}_{B}\) (\(\mathcal{G}_{D}\)) over agents. The constraint \(Bx=0\) (\(Dx=0\)) is equivalent to:

\[x_{i}=x_{j}\text{ if }x_{i},x_{j}\text{ are adjacent in }\mathcal{G}_{B}\left( \mathcal{G}_{D}\right).\]

To simplify the derivation, we additionally assume that \(A,B,D\) are pairwise commutative. Then for \(x=(x_{1},...,x_{n})\), we have:

\[x_{1}=...=x_{n}\Leftrightarrow Bx=0\Leftrightarrow Dx=0\Leftrightarrow Ax=x.\]

Therefore, (10) can be equivalently reformulated as

\[\min_{x\in\mathbb{R}^{n}}f(Ax),\quad\text{s.t.}\,Bx=0.\] (11)We construct the augmented Lagrangian function of the problem (11) as follows:

\[\mathcal{L}_{\rho}(x,d)=f(Ax)+\langle d,Bx\rangle+\frac{\rho}{2}\|Dx\|^{2},\]

where \(x\) denotes the primal variable, \(d\) denotes the dual variable or Lagrangian multiplier associated with the consensus constraint, \(\|Dx\|^{2}\) serves as the penalty term measuring the deviation from \(Dx=0\), or equivalently \(Bx=0\); \(\rho>0\) is the penalty coefficient. Though the introduction of matrices \(A,D\) is essentially a matter of equivalent substitution, it enhances the universality of the algorithm framework we get.

Following classical primal-dual methods, we alternately perform gradient descent on \(x\) and gradient ascent on \(d\) in the \(k\)-th iteration:

\[x^{k+1}=x^{k}-\alpha(A\nabla f(Ax^{k})+Bd^{k}+\rho D^{2}x^{k}),\quad d^{k+1}= d^{k}+\beta Bx^{k+1},\]

where \(\alpha,\beta\) denote the step-sizes. By making the change of variables

\[\hat{x}^{k}=Ax^{k},\,\hat{d}^{k}=\sqrt{\frac{\alpha}{\beta}}Ad^{k},\,\widehat {B}=\sqrt{\alpha\beta}B,\,\widehat{C}=I-\alpha\rho D^{2},\,\widehat{A}=A^{2},\]

we obtain

\[\hat{x}^{k+1}=\widehat{C}\hat{x}^{k}-\alpha\widehat{A}\nabla f(\hat{x}^{k})- \widehat{B}\hat{d}^{k},\quad\hat{d}^{k+1}=\hat{d}^{k}+\widehat{B}\hat{x}^{k+1}.\] (12)

One should note that the definition implies that \(\widehat{A},\widehat{C}\) are doubly stochastic communication matrices under appropriate selections of \(\alpha,\rho\). Finally, thanks to the introduction of moving-average iteration of (12), we can obtain the framework (6) which serves as the foundation for our algorithm design. See more details in Section 2.3.

### Specific instances

Relation to some existing single-level algorithm frameworksAccording to (12), our framework at single-level is

\[\mathbf{x}^{k+1}=\mathbf{C}\mathbf{x}^{k}-\alpha\mathbf{Ag}^{k}-\mathbf{Bd}^{ k},\mathbf{d}^{k+1}=\mathbf{d}^{k}+\mathbf{B}\mathbf{x}^{k+1},k=0,1,...\] (13)

where \(\alpha\) is the step-size, \(\mathbf{g}^{k}\) denotes the estimated gradient at the \(k\)-th iteration, \(\mathbf{d}\) serves as the dual variable.

Replacing \(\mathbf{C}\) with \(\mathbf{C}\mathbf{A}\), we get UDA[1], and equivalently, SUDA [2]:

\[\mathbf{x}^{k+1}=\mathbf{C}\mathbf{A}\mathbf{x}^{k}-\alpha\mathbf{Ag}^{k}- \mathbf{Bd}^{k},\mathbf{d}^{k+1}=\mathbf{d}^{k}+\mathbf{B}\mathbf{x}^{k+1},k= 0,1,...\]

Therefore, following SUDA, we can also recover some common state-of-the-art heterogeneity methods as follows by selecting specific \(\mathbf{A},\mathbf{B},\mathbf{C}\). First, from (13) we get

\[\mathbf{x}^{k+2}-\mathbf{x}^{k+1} =\mathbf{C}(\mathbf{x}^{k+1}-\mathbf{x}^{k})-\alpha\mathbf{A} \left(\mathbf{g}^{k+1}-\mathbf{g}^{k}\right)-\mathbf{B}\left(\mathbf{d}^{k+1} -\mathbf{d}^{k}\right)\] \[=\mathbf{C}\left(\mathbf{x}^{k+1}-\mathbf{x}^{k}\right)-\alpha \mathbf{A}\left(\mathbf{g}^{k+1}-\mathbf{g}^{k}\right)-\mathbf{B}^{2}\mathbf{ x}^{k+1}.\]

Thus, for \(k\geq 0\) we have

\[\mathbf{x}^{k+2}=\left(\mathbf{I}-\mathbf{B}^{2}+\mathbf{C}\right)\mathbf{x}^ {k+1}-\mathbf{C}\mathbf{x}^{k}-\alpha\mathbf{A}\left(\mathbf{g}^{k+1}-\mathbf{ g}^{k}\right),\]

with \(\mathbf{x}^{1}=\mathbf{C}\mathbf{x}^{0}-\alpha\mathbf{Ag}^{0}\).

Some specific instancesWe next show that how to choose \(\mathbf{A},\mathbf{B},\mathbf{C}\) to get some common heterogeneity methods.

* ED: Taking \(\mathbf{A}=\mathbf{W},\mathbf{B}=(\mathbf{I}-\mathbf{W})^{1/2}\) and \(\mathbf{C}=\mathbf{W}\), we get ED: \[\mathbf{x}^{k+2}=\mathbf{W}\left(2\mathbf{x}^{k+1}-\mathbf{x}^{k}-\alpha\left( \mathbf{g}^{k+1}-\mathbf{g}^{k}\right)\right),\] with \(\mathbf{x}^{1}=\mathbf{W}\left(\mathbf{x}^{0}-\alpha\mathbf{g}^{0}\right)\).

* EXTRA: Taking \(\mathbf{A}=\mathbf{I},\mathbf{B}=(\mathbf{I}-\mathbf{W})^{1/2}\) with \(\mathbf{C}=\mathbf{W}\), we get EXTRA: \[\mathbf{x}^{k+2}=\mathbf{W}\left(2\mathbf{x}^{k+1}-\mathbf{x}^{k}\right)- \alpha\left(\mathbf{g}^{k+1}-\mathbf{g}^{k}\right),\] and \(\mathbf{x}^{1}=\mathbf{W}\mathbf{x}^{0}-\alpha\mathbf{g}^{0}\).
* Adapt-then-combine gradient tracking (ATC-GT): The iteration of ATC-GT is \[\mathbf{x}^{k+1}=\mathbf{W}\left(\mathbf{x}^{k}-\alpha\mathbf{h}^{k}\right), \mathbf{h}^{k+1}=\mathbf{W}\left(\mathbf{h}^{k}+\mathbf{g}^{k+1}-\mathbf{g}^{k }\right)\] with \(\mathbf{h}^{0}=\mathbf{W}\mathbf{g}^{0},\mathbf{x}^{0}=\mathbf{W}\mathbf{x}^ {0}\) (\(x_{1}^{0}=...=x_{n}^{0}\)). It follows that for \(k\geq 0\) \[\mathbf{x}^{k+2}-\mathbf{W}\mathbf{x}^{k+1}=\mathbf{W}\mathbf{x}^{k+1}- \mathbf{W}^{2}\mathbf{x}^{k}-\alpha\mathbf{W}\left(\mathbf{h}^{k+1}-\mathbf{ W}\mathbf{h}^{k}\right).\] Then we obtain \[\mathbf{x}^{k+2}=2\mathbf{W}\mathbf{x}^{k+1}-\mathbf{W}^{2}\mathbf{x}^{k}- \alpha\mathbf{W}^{2}\left(\mathbf{g}^{k+1}-\mathbf{g}^{k}\right),\] with \(\mathbf{x}^{1}=\mathbf{W}^{2}\mathbf{x}^{0}-\alpha\mathbf{W}^{2}\mathbf{g}^{0}\). Thus, we can take \(\mathbf{A}=\mathbf{W}^{2},\mathbf{B}=(\mathbf{I}-\mathbf{W})^{2},\mathbf{C}= \mathbf{W}^{2}\) to implement ATC-GT.
* Semi-ATC-GT: The iteration of Semi-ATC-GT is \[\mathbf{x}^{k+1}=\mathbf{W}\left(\mathbf{x}^{k}-\alpha\mathbf{h}^{k}\right), \mathbf{h}^{k+1}=\mathbf{W}\mathbf{h}^{k}+\mathbf{g}^{k+1}-\mathbf{g}^{k}\] with \(\mathbf{h}^{0}=\mathbf{W}\mathbf{g}^{0},\mathbf{x}^{0}=\mathbf{W}\mathbf{x}^ {0}\) (\(x_{1}^{0}=...=x_{n}^{0}\)). Like ATC-GT, we have \[\mathbf{x}^{k+2}=2\mathbf{W}\mathbf{x}^{k+1}-\mathbf{W}^{2}\mathbf{x}^{k}- \alpha\mathbf{W}\left(\mathbf{g}^{k+1}-\mathbf{g}^{k}\right),\] with \(\mathbf{x}^{1}=\mathbf{W}^{2}\mathbf{x}^{0}-\alpha\mathbf{W}\mathbf{g}^{0}\). Thus, we can take \(\mathbf{A}=\mathbf{W},\mathbf{B}=(\mathbf{I}-\mathbf{W})^{2},\mathbf{C}= \mathbf{W}^{2}\) to implement semi-ATC-GT.
* Non-ATC-GT: The iteration of Non-ATC-GT is \[\mathbf{x}^{k+1}=\mathbf{W}\mathbf{x}^{k}-\alpha\mathbf{h}^{k},\mathbf{h}^{k+1 }=\mathbf{W}\mathbf{h}^{k}+\mathbf{g}^{k+1}-\mathbf{g}^{k}\] with \(\mathbf{h}^{0}=\mathbf{W}\mathbf{g}^{0},\mathbf{x}^{0}=\mathbf{W}\mathbf{x}^ {0}\) (\(x_{1}^{0}=...=x_{n}^{0}\)). We have \[\mathbf{x}^{k+2}=2\mathbf{W}\mathbf{x}^{k+1}-\mathbf{W}^{2}\mathbf{x}^{k}- \alpha\left(\mathbf{g}^{k+1}-\mathbf{g}^{k}\right),\] with \(\mathbf{x}^{1}=\mathbf{W}^{2}\mathbf{x}^{0}-\alpha\mathbf{g}^{0}\). Thus, we can take \(\mathbf{A}=\mathbf{I},\mathbf{B}=(\mathbf{I}-\mathbf{W})^{2},\mathbf{C}= \mathbf{W}^{2}\) to implement Non-ATC-GT.

### Implementation details

Given the update method \(\mathbf{L}\), we update the lower-level variable \(y\) at the \(k\)-th (\(k\geq 0\)) iteration as follows. For brevity, we define \(y_{i}^{-1}=y_{i}^{0},v_{i}^{-1}=0,o_{i}^{0}=\sum_{j=1}^{n}(W_{y})_{ij}v_{j}^{0}\).

\[\begin{cases}y_{i}^{k+1}=\sum_{j=1}^{n}(W_{y})_{ij}\left(2y_{j}^{k}-y_{j}^{k-1 }-\beta_{k}\left(v_{i}^{k}-v_{i}^{k-1}\right)\right)&\text{if }\mathbf{L}=ED\\ y_{i}^{k+1}=\sum_{j=1}^{n}(W_{y})_{ij}\left(2y_{j}^{k}-y_{j}^{k-1}\right)-\beta _{k}\left(v_{i}^{k}-v_{i}^{k-1}\right)\\ y_{i}^{k+1}=\sum_{j=1}^{n}(W_{y})_{ij}\left(y_{j}^{k}-\beta_{k}o_{j}^{k} \right),o_{i}^{k+1}=\sum_{j=1}^{n}(W_{y})_{ij}\left(o_{j}^{k}+v_{i}^{k+1}-v_{i} ^{k}\right)&\text{if }\mathbf{L}=GT\\ \cdots&\text{others}\end{cases}\] (14)

Similarly, we update the auxiliary variable \(z\) at the \(k\)-th (\(k\geq 0\)) iteration as follows. For brevity, we define \(z_{i}^{-1}=z_{i}^{0},p_{i}^{-1}=0,h_{i}^{0}=\sum_{j=1}^{n}(W_{z})_{ij}p_{j}^{0}\). Note that we use the same method \(\mathbf{L}\) to update \(z\) as we do for the lower-level variable \(y\).

\[\begin{cases}z_{i}^{k+1}=\sum_{j=1}^{n}(W_{z})_{ij}\left(2z_{j}^{k}-z_{i}^{k-1} -\gamma_{k}\left(p_{i}^{k}-p_{i}^{k-1}\right)\right)&\text{if }\mathbf{L}=ED\\ z_{i}^{k+1}=\sum_{j=1}^{n}(W_{z})_{ij}\left(2z_{j}^{k}-z_{i}^{k-1}\right)- \gamma_{k}\left(p_{i}^{k}-p_{i}^{k-1}\right)&\text{if }\mathbf{L}=EXTRA\\ z_{i}^{k+1}=\sum_{j=1}^{n}(W_{z})_{ij}\left(z_{j}^{k}-\gamma_{k}h_{j}^{k} \right),h_{i}^{k+1}=\sum_{j=1}^{n}(W_{z})_{ij}\left(h_{j}^{k}+p_{i}^{k+1}-p_{i} ^{k}\right)&\text{if }\mathbf{L}=GT\\ \cdots&\text{others}\end{cases}\] (15)

Given the update method \(\mathbf{U}\), we update the upper-level variable \(x\) at the \(k\)-th (\(k\geq 0\)) iteration as follows. For brevity, we define \(x_{i}^{-1}=x_{i}^{0},t_{i}^{0}=\sum_{j=1}^{n}(W_{y})_{ij}r_{j}^{1}\).

\[\begin{cases}x_{i}^{k+1}=\sum_{j=1}^{n}(W_{x})_{ij}\left(2x_{j}^{k}-x_{j}^{k-1}- \alpha_{k}\left(r_{i}^{k+1}-r_{i}^{k}\right)\right)&\text{if }\textbf{U}=ED\\ x_{i}^{k+1}=\sum_{j=1}^{n}(W_{x})_{ij}\left(2x_{j}^{k}-x_{j}^{k-1}\right)- \alpha_{k}\left(r_{i}^{k+1}-r_{i}^{k}\right)&\text{if }\textbf{U}=EXTRA\\ x_{i}^{k+1}=\sum_{j=1}^{n}(W_{x})_{ij}\left(x_{j}^{k}-\alpha_{k}k_{j}^{k} \right),t_{i}^{k+1}=\sum_{j=1}^{n}(W_{x})_{ij}\left(t_{j}^{k}+r_{i}^{k+2}-r_{i }^{k+1}\right)&\text{if }\textbf{U}=GT\\ \cdots&\text{others}\end{cases}\] (16)

Then the practical implementation of SPARKLE with mixed strategies is

```
0: Initialize \(x_{i}^{0}=y_{i}^{0}=z_{i}^{0}=r_{i}^{0}=0\), step-sizes \(\alpha_{k},\beta_{k},\gamma_{k},\theta_{k}\). for\(k=0,1,\cdots,K-1\), each agent \(i\) (in parallel) do  Update \(y_{i}^{k+1}\) according to (14);  Update \(z_{i}^{k+1}\) according to (15); \(r_{i}^{k+1}=(1-\theta_{k})r_{i}^{k}+\theta_{k}u_{i}^{k}\) ;  Update \(x_{i}^{k+1}\) according to (16). endfor ```

**Algorithm 2** SPARKLE-L-U

## Appendix C Convergence analysis

### Proof of Theorem 1

#### c.1.1 Notations

We use lowercase letters to represent vectors and uppercase letters to represent matrices. Stacked vectors \([x_{1}^{\top},...,x_{n}^{\top}]^{\top}\) is denoted by \(\text{col}\{x_{1},...,x_{n}\}\) for brevity. We denote a block diagonal matrix with diagonal block \(M_{i}(1\leq i\leq l)\) by \(\text{blkdiag}\{M_{1},...,M_{l}\}\), and a diagonal matrix with diagonal elements \(d_{i}(1\leq i\leq k)\) by \(\text{diag}\{d_{1},...,d_{k}\}\). The Kronecker product operator is denoted by \(\otimes\). For a variable \(v\), we use \(v_{i}^{k}\) to represent its components at \(k\)-th iteration and \(i\)-th agent.

Moreover, we use an overbar _above_ an iterator to denote the average over all agents. For example, \(\bar{x}^{k}=\sum_{i=1}^{n}x_{i}^{k}/n\). Upright bold symbols are used to denote stacked vectors or matrices across agents. For example, \(\mathbf{x}^{k}:=\text{col}\{x_{1}^{k},...,x_{n}^{k}\}\), \(\bar{\mathbf{x}}^{k}:=\text{col}\{\bar{x}^{k},...,\bar{x}^{k}\}\left(n\text{ times}\right)\), \(\mathbf{W}_{x}:=W_{x}\otimes I_{dim(x)}\). Denote the 2-norm of a matrix by \(\|\cdot\|\).

Next, we define following \(\sigma\)-fields which will be used in our convergence analysis:

\[\mathcal{F}_{k} =\sigma\left(\mathbf{y}^{0},\ldots,\mathbf{y}^{k+1},\mathbf{z}^{ 0},\ldots,\mathbf{z}^{k+1},\mathbf{x}^{0},\ldots,\mathbf{x}^{k},\mathbf{r}^{0},\ldots,\mathbf{r}^{k}\right),\] \[\mathcal{U}_{k} =\sigma\left(\mathbf{y}^{0},\ldots,\mathbf{y}^{k+1},\mathbf{z}^{ 0},\ldots,\mathbf{z}^{k},\mathbf{x}^{0},\ldots,\mathbf{x}^{k},\mathbf{r}^{0},\ldots,\mathbf{x}^{k}\right),\] \[\mathcal{G}_{k} =\sigma\left(\mathbf{y}^{0},\ldots,\mathbf{y}^{k},\mathbf{z}^{0},\ldots,\mathbf{z}^{k},\mathbf{x}^{0},\ldots,\mathbf{x}^{k},\mathbf{r}^{0}, \ldots,\mathbf{r}^{k}\right),\]

and denote \(\mathbb{E}[\cdot|\mathcal{F}_{k}]\) by \(\mathbb{E}_{k}\), \(\mathbb{E}[\cdot|\mathcal{U}_{k}]\) by \(\widetilde{\mathbb{E}}_{k}\), \(\mathbb{E}[\cdot|\mathcal{G}_{k}]\) by \(\widehat{\mathbb{E}}_{k}\) for brevity.

Define

\[z^{\star}(x)=\left(\sum_{i=1}^{n}\nabla_{22}^{2}g_{i}\left(x,y^{\star}(x) \right)\right)^{-1}\left(\sum_{i=1}^{n}\nabla_{2}f_{i}\left(x,y^{\star}(x) \right)\right),\]

Then, for \(k=0,1,\cdots\), define:

\[z_{\star}^{k+1}=\left(\sum_{i=1}^{n}\nabla_{22}^{2}g_{i}\left(\bar{x}^{k},y^{ \star}(\bar{x}^{k})\right)\right)^{-1}\left(\sum_{i=1}^{n}\nabla_{2}f_{i}\left( \bar{x}^{k},y^{\star}(\bar{x}^{k})\right)\right).\]

For convenience, we define \(\mathbf{x}^{-1}=\mathbf{x}^{0},\mathbf{y}^{-1}=\mathbf{y}^{0},y^{\star}(\bar{x} ^{-1})=y^{\star}(\bar{x}^{0}),z_{\star}^{0}=z_{\star}^{1}\).

#### c.1.2 Basic transformations

We begin with conducting SUDA-like [2] transformations, which is fundamental of the following proofs.

Firstly, we define \(\mathbf{t}^{k}\) to track the averaged stochastic gradients among agents as follows

\[\begin{split}\mathbf{t}^{k}_{y}&=\mathbf{B}_{y}( \mathbf{d}^{k}_{y}-\mathbf{B}_{y}\mathbf{y}^{k})+\beta\mathbf{A}_{y}\nabla_{2} \mathbf{g}(\bar{\mathbf{x}}^{k},\bar{\mathbf{y}}^{k}),\\ \mathbf{t}^{k}_{z}&=\mathbf{B}_{z}(\mathbf{d}^{k}_{z }-\mathbf{B}_{z}\mathbf{z}^{k})+\gamma\mathbf{A}_{z}\mathbf{p}^{k}(\bar{ \mathbf{x}}^{k},\bar{\mathbf{y}}^{k+1}),\\ \mathbf{t}^{k}_{z}&=\mathbf{B}_{x}(\mathbf{d}^{k}_{ z}-\mathbf{B}_{x}\mathbf{x}^{k})+\alpha\mathbf{A}_{x}\widetilde{\nabla}\mathbf{ \Phi}(\bar{\mathbf{x}}^{k}),\end{split}\] (17)

where

\[\begin{split}\mathbf{p}^{k}(\bar{\mathbf{x}}^{k},\bar{\mathbf{y }}^{k+1})&=\text{col}\left\{\nabla_{22}^{2}g_{i}(\bar{x}^{k}, \bar{y}^{k+1})z_{*}^{k}-\nabla_{2}f_{i}(\bar{x}^{k},\bar{y}^{k+1})\right\}_ {i=1}^{n},\\ \widetilde{\nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{k})& =\text{col}\left\{\nabla_{1}f_{i}(\bar{x}^{k},y^{\star}(\bar{x}^{ k}))-\nabla_{12}g_{i}(\bar{x}^{k},y^{\star}(\bar{x}^{k}))z_{\star}^{k+1} \right\}_{i=1}^{n}.\end{split}\]

Then the iteration of \(\mathbf{y},\mathbf{z},\mathbf{x}\) in Algorithm 1 can be written as:

\[\begin{split}\text{iteration of }\mathbf{y}:& \begin{cases}\mathbf{y}^{k+1}=(\mathbf{C}_{y}-\mathbf{B}_{y}^{2})\mathbf{y}^{k }-\mathbf{t}^{k}_{y}-\beta\mathbf{A}_{y}\left[\mathbf{y}^{k}-\nabla_{2} \mathbf{g}(\bar{\mathbf{x}}^{k},\bar{\mathbf{y}}^{k})\right],\\ \mathbf{t}^{k+1}_{y}&=\mathbf{t}^{k}_{y}+\mathbf{B}_{y}^{2} \mathbf{y}^{k}+\beta\mathbf{A}_{y}\left[\nabla_{2}\mathbf{g}(\bar{\mathbf{x} }^{k+1},\bar{\mathbf{y}}^{k+1})-\nabla_{2}\mathbf{g}(\bar{\mathbf{x}}^{k}, \bar{\mathbf{y}}^{k})\right],\end{cases},\\ \text{iteration of }\mathbf{z}:&\begin{cases}\mathbf{z}^{k+1}=(\mathbf{C}_{z}- \mathbf{B}_{z}^{2})\mathbf{z}^{k}-\mathbf{t}^{k}_{z}-\gamma\mathbf{A}_{z} \left[\mathbf{p}^{k}-\mathbf{p}^{k}(\bar{\mathbf{x}}^{k},\bar{\mathbf{y}}^{k+ 1})\right],\\ \mathbf{t}^{k+1}_{z}&=\mathbf{t}^{k}_{z}+\mathbf{B}_{z}^{2}\mathbf{z}^{k}+ \gamma\mathbf{A}_{z}\left[\mathbf{p}^{k+1}(\bar{\mathbf{x}}^{k+1},\bar{ \mathbf{y}}^{k+2})-\mathbf{p}^{k}(\bar{\mathbf{x}}^{k},\bar{\mathbf{y}}^{k+1 })\right],\end{cases}\\ \text{iteration of }\mathbf{x}:&\begin{cases}\mathbf{x}^{k+1}=(\mathbf{C}_{x}- \mathbf{B}_{x}^{2})\mathbf{x}^{k}-\mathbf{t}^{k}_{x}-\alpha\mathbf{A}_{x} \left[\mathbf{x}^{k+1}-\widetilde{\nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{k}) \right],\\ \mathbf{t}^{k+1}_{x}&=\mathbf{t}^{k}_{x}+\mathbf{B}_{x}^{2}\mathbf{x}^{k}+ \alpha\mathbf{A}_{x}\left[\widetilde{\nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{k +1})-\widetilde{\nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{k})\right].\end{cases} \end{split}\] (20)

Next, we present the transformation of the matrices \(A,B,C\). For a communication matrix \(W_{s}\) for the variable \(s\in\{x,y,z\}\) satisfying Assumption 2, there exists an orthogonal matrix \(U\) such that:

\[W=U_{s}\hat{\Lambda}_{s}U_{s}^{\top}=\left[\frac{1}{\sqrt{n}}\mathbf{1}\quad \hat{U}_{s}\right]\begin{bmatrix}1&0\\ 0&\Lambda_{s}\end{bmatrix}\begin{bmatrix}\frac{1}{\sqrt{n}}\mathbf{1}^{\top}\\ \frac{\hat{U}_{s}^{\top}}{\sqrt{n}}\end{bmatrix},\]

where \(\Lambda_{s}=\text{diag}\{\lambda_{si}\}_{i=2}^{n}\), \(\hat{U}_{s}^{\top}\in\mathbb{R}^{n\times(n-1)}\) satisfies \(\hat{U}_{s}\hat{U}_{s}^{\top}=I_{n}-\frac{1}{n}\mathbf{1}_{n}\mathbf{1}_{n}^{\top}\) and \(\mathbf{1}_{n}^{\top}\hat{U}_{s}=0\). Then it follows that:

\[\mathbf{W}_{s}=\mathbf{U}_{s}\hat{\Lambda}_{s}\mathbf{U}_{s}^{\top}=\left[ \frac{1}{\sqrt{n}}\mathbf{1}\otimes I_{dim(s)}\quad\hat{\mathbf{U}}_{s}\right] \begin{bmatrix}I_{dim(s)}&0\\ 0&\mathbf{\Lambda}_{s}\end{bmatrix}\begin{bmatrix}\frac{1}{\sqrt{n}}\mathbf{1}^{ \top}\otimes I_{dim(s)}\\ \hat{\mathbf{U}}_{s}^{\top}\end{bmatrix},\]

where \(dim(s)\) denotes the dimension of the corresponding variable, \(\mathbf{\Lambda}_{s}=\Lambda_{s}\otimes I_{dim(s)}\in\mathbb{R}^{d(n-1)\times[ dim(s)\cdot(n-1)]}\), \(\mathbf{U}_{s}\in\mathbb{R}^{[dim(s)\cdot n]\times[dim(s)\cdot n]}\) is an orthogonal matrix, and \(\hat{\mathbf{U}}_{s}=\hat{U}_{s}\otimes I_{dim(s)}\in\mathbb{R}^{[dim(s) \cdot n]\times[dim(s)\cdot(n-1)]}\) satisfies:

\[\hat{\mathbf{U}}_{s}^{\top}\hat{\mathbf{U}}_{s}=I_{dim(s)\cdot(n-1)},\quad\hat {\mathbf{U}}_{s}\hat{\mathbf{U}}_{s}^{\top}=\left[I_{n}-\frac{1}{n}\mathbf{1} \mathbf{1}^{\top}\right]\otimes I_{dim(s)},\quad(\mathbf{1}^{\top}\otimes I_ {dim(s)})\hat{\mathbf{U}}_{s}=\mathbf{0}.\]

Now we add subscript \(s\) for \(\mathbf{W}_{s}\). Then, as \(\mathbf{A}_{s},\mathbf{B}_{s}^{2},\mathbf{C}_{s}\) can be expressed as a polynomial of \(\mathbf{W}_{s}\) for \(s\in\{x,y,z\}\) according to Assumption 2, we have the orthogonal decomposition:

\[\begin{split}\mathbf{A}_{s}=\mathbf{U}_{s}\hat{\Lambda}_{sa} \mathbf{U}_{s}^{\top}&=\left[\frac{1}{\sqrt{n}}\mathbf{1}\otimes I_{\text{ dim}(s)}\quad\hat{\mathbf{U}}_{s}\right]\begin{bmatrix}I_{\text{ dim}(s)}&\mathbf{0}\\ \mathbf{0}&\mathbf{\Lambda}_{sa}\end{bmatrix}\begin{bmatrix}\frac{1}{\sqrt{n}} \mathbf{1}^{\top}\otimes I_{\text{dim}(s)}\\ \hat{\mathbf{U}}_{s}^{\top}\end{bmatrix},\\ \mathbf{B}_{s}^{2}=\mathbf{U}_{s}\hat{\Lambda}_{sb}^{2}\mathbf{U}_{s}^{\top}&= \left[\frac{1}{\sqrt{n}}\mathbf{1}\otimes I_{\text{dim}(s)}\quad\hat{\mathbf{U}}_{s} \right]\begin{bmatrix}\begin{bmatrix}\mathbf{0}&\mathbf{0}\\ \mathbf{0}&\mathbf{\Lambda}_{sb}^{2}\end{bmatrix}\begin{bmatrix}\frac{1}{\sqrt{n}} \mathbf{1}^{\top}\otimes I_{\text{dim}(s)}\\ \hat{\mathbf{U}}_{s}^{\top}\end{bmatrix},\\ \mathbf{C}_{s}=\mathbf{U}_{s}\hat{\Lambda}_{sc}\mathbf{U}_{s}^{\top}&= \left[\frac{1}{\sqrt{n}}\mathbf{1}\otimes I_{\text{dim}(s)}\quad\hat{\mathbf{U}}_{s} \right]\begin{bmatrix}I_{\text{dim}(s)}&\mathbf{0}\\ \mathbf{0}&\mathbf{\Lambda}_{sc}\end{bmatrix}\begin{bmatrix}\frac{1}{\sqrt{n}} \mathbf{1}^{\top}\otimes I_{\text{dim}(s)}\\ \hat{\mathbf{U}}_{s}^{\top}\end{bmatrix},\end{split}\end{split}\] (21)where

\[\mathbf{\Lambda}_{sa}=\underbrace{\text{diag}\{\lambda_{sa,i}\}_{i=2}^{n}}_{ \Lambda_{sa}}\otimes I_{\text{dim}(s)},\quad\mathbf{\Lambda}_{sb}=\underbrace{ \text{diag}\{\lambda_{sb,i}\}_{i=2}^{n}}_{\Lambda_{sb}}\otimes I_{\text{dim}(s) },\quad\mathbf{\Lambda}_{sc}=\underbrace{\text{diag}\{\lambda_{sc,i}\}_{i=2}^{ n}}_{\Lambda_{sc}}\otimes I_{\text{dim}(s)}.\]

Moreover, each \(\mathbf{\Lambda}_{sb}\) is positive definite because of the null space condition in Assumption 2. Then, multiplying both sides of (18), (19) and (20) by \(\mathbf{U}_{y}^{\top},\mathbf{U}_{z}^{\top},\mathbf{U}_{x}^{\top}\) respectively, we get:

\[\text{iter. of }\mathbf{y}:\left\{\begin{aligned} &\mathbf{U}_{y}^{\top}\mathbf{y}^{k+1}=( \mathbf{\hat{\Lambda}}_{yc}-\mathbf{\hat{\Lambda}}_{xb}^{2})\mathbf{U}_{y}^{ \top}\mathbf{y}^{k}-\mathbf{U}_{y}^{\top}\mathbf{t}_{y}^{k}-\beta\mathbf{ \hat{\Lambda}}_{ya}\mathbf{U}_{y}^{\top}\left[\mathbf{v}^{k}-\nabla_{2}\mathbf{ g}(\bar{\mathbf{x}}^{k},\bar{\mathbf{y}}^{k})\right],\\ &\mathbf{U}_{y}^{\top}\mathbf{t}_{y}^{k+1}=\mathbf{U}_{y}^{\top} \mathbf{t}_{y}^{k}+\mathbf{\hat{\Lambda}}_{yb}^{\top}\mathbf{U}_{y}^{\top} \mathbf{y}^{k}+\beta\mathbf{\hat{\Lambda}}_{ya}\mathbf{U}_{y}^{\top}\left[ \nabla_{2}\mathbf{g}(\bar{\mathbf{x}}^{k+1},\bar{\mathbf{y}}^{k+1})-\nabla_{2} \mathbf{g}(\bar{\mathbf{x}}^{k},\bar{\mathbf{y}}^{k})\right],\end{aligned}\right.\] (22)

\[\text{iter. of }\mathbf{z}:\left\{\begin{aligned} &\mathbf{U}_{z}^{\top}\mathbf{z}^{k+1}=( \mathbf{\hat{\Lambda}}_{zc}-\mathbf{\hat{\Lambda}}_{zb}^{2})\mathbf{U}_{z}^{ \top}\mathbf{z}^{k}-\mathbf{U}_{z}^{\top}\mathbf{t}_{z}^{k}-\gamma\mathbf{ \hat{\Lambda}}_{za}\mathbf{U}_{z}^{\top}\left[\mathbf{p}^{k}-\mathbf{p}^{k}( \bar{\mathbf{x}}^{k},\bar{\mathbf{y}}^{k+1})\right],\\ &\mathbf{U}_{z}^{\top}\mathbf{t}_{z}^{k+1}=\mathbf{U}_{z}^{\top} \mathbf{t}_{z}^{k}+\mathbf{\hat{\Lambda}}_{zb}^{\top}\mathbf{U}_{z}^{\top} \mathbf{z}^{k}+\gamma\mathbf{\hat{\Lambda}}_{za}\mathbf{U}_{z}^{\top}\left[ \mathbf{p}^{k+1}(\bar{\mathbf{x}}^{k+1},\bar{\mathbf{y}}^{k+2})-\mathbf{p}^{k} (\bar{\mathbf{x}}^{k},\bar{\mathbf{y}}^{k+1})\right].\end{aligned}\right.\] (23)

\[\text{iter. of }\mathbf{x}:\left\{\begin{aligned} &\mathbf{U}_{x}^{\top}\mathbf{x}^{k+1}=( \mathbf{\hat{\Lambda}}_{xc}-\mathbf{\hat{\Lambda}}_{xb}^{2})\mathbf{U}_{x}^{ \top}\mathbf{x}^{k}-\mathbf{U}_{x}^{\top}\mathbf{t}_{x}^{k}-\alpha\mathbf{ \hat{\Lambda}}_{xa}\mathbf{U}^{\top}\left[\mathbf{r}^{k+1}-\widetilde{\nabla} \mathbf{\Phi}(\bar{\mathbf{x}}^{k})\right],\\ &\mathbf{U}_{x}^{\top}\mathbf{t}_{x}^{k+1}=\mathbf{U}_{x}^{\top} \mathbf{t}_{x}^{k}+\mathbf{\hat{\Lambda}}_{xb}^{2}\mathbf{U}_{x}^{\top} \mathbf{x}^{k}+\alpha\mathbf{\hat{\Lambda}}_{xa}\mathbf{U}_{x}^{\top}\left[ \widetilde{\nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{k+1})-\widetilde{\nabla} \mathbf{\Phi}(\bar{\mathbf{x}}^{k})\right].\end{aligned}\right.\] (24)

Then, due to Eq. (17), we have:

\[(\mathbf{1}^{\top}\otimes I_{d})\mathbf{t}_{y}^{k} =(\mathbf{1}^{\top}\otimes I_{d})\left(\mathbf{B}_{y}(\mathbf{d}_ {y}^{k}-\mathbf{B}_{y}\mathbf{y}^{k})+\beta\mathbf{A}_{y}\nabla_{2}\mathbf{g}( \bar{\mathbf{x}}^{k},\bar{\mathbf{y}}^{k})\right)\] (25) \[=n\beta\nabla_{2}g(\bar{x}^{k},\bar{y}^{k}).\] (26) \[(\mathbf{1}^{\top}\otimes I_{d})\mathbf{t}_{z}^{k} =(\mathbf{1}^{\top}\otimes I_{d})\left(\mathbf{B}_{z}(\mathbf{d}_ {z}^{k}-\mathbf{B}_{z}\mathbf{x}^{k})+\alpha\mathbf{A}_{x}\widetilde{\nabla} \mathbf{\Phi}(\bar{\mathbf{x}}^{k})\right)\] \[=\alpha\sum_{i=1}^{n}\left[\nabla_{1}f_{i}(\bar{x}^{k},y^{\star} (\bar{x}^{k}))-\nabla_{12}g_{i}(\bar{x}^{k},y^{\star}(\bar{x}^{k}))z_{\star}^{ k+1}\right].\]

Substituting (25), (26), (27) into (22), (23), (24), respectively. Then use (21) and the structure of \(\hat{\mathbf{U}}_{y},\hat{\mathbf{U}}_{z},\hat{\mathbf{U}}_{x},\) we have

\[\text{iter. of }\mathbf{y}:\left\{\begin{aligned} &\bar{y}^{k+1}=\bar{y}^{k}-\beta\bar{v}^{k},\\ &\hat{\mathbf{U}}_{y}^{\top}\mathbf{y}^{k+1}=(\mathbf{\Lambda}_{yc }-\mathbf{\Lambda}_{yb}^{2})\hat{\mathbf{U}}_{y}^{\top}\mathbf{y}^{k}-\hat{ \mathbf{U}}_{y}^{\top}\mathbf{t}_{y}^{k}-\beta\mathbf{\Lambda}_{ya}\hat{ \mathbf{U}}_{y}^{\top}\left[\mathbf{v}^{k}-\nabla_{2}\mathbf{g}(\bar{\mathbf{x}}^{ k},\bar{\mathbf{y}}^{k})\right],\\ &\hat{\mathbf{U}}_{y}^{\top}\mathbf{t}_{y}^{k+1}=\hat{\mathbf{U}}_{y} ^{\top}\mathbf{t}_{y}^{k}+\mathbf{\Lambda}_{yb}^{2}\bar{\mathbf{U}}_{y}^{\top} \mathbf{y}^{k}+\beta\mathbf{\Lambda}_{ya}\hat{\mathbf{U}}_{y}^{\top}\left[ \nabla_{2}\mathbf{g}(\bar{\mathbf{x}}^{k+1},\bar{\mathbf{y}}^{k+1})-\nabla_{2} \mathbf{g}(\bar{\mathbf{x}}^{k},\bar{\mathbf{y}}^{k})\right],\end{aligned}\right.\]

\[\text{iter. of }\mathbf{z}:\left\{\begin{aligned} &\bar{z}^{k+1}=\bar{z}^{k}-\gamma\bar{p}^{k},\\ &\hat{\mathbf{U}}_{z}^{\top}\mathbf{z}^{k+1}=(\mathbf{\Lambda}_{zc }-\mathbf{\Lambda}_{sb}^{2})\hat{\mathbf{U}}_{z}^{\top}\mathbf{z}^{k}-\hat{ \mathbf{U}}_{z}^{\top}\mathbf{t}_{z}^{k}-\gamma\mathbf{\Lambda}_{za}\hat{ \mathbf{U}}_{z}^{\top}\left[\mathbf{p}^{k}-\mathbf{p}^{k}(\bar{\mathbf{x}}^{k}, \bar{\mathbf{y}}^{k+1})\right],\\ &\hat{\mathbf{U}}_{z}^{\top}\mathbf{t}_{z}^{k+1}=\hat{\mathbf{U}}_{z} ^{\top}\mathbf{t}_{z}^{k}+\mathbf{\Lambda}_{zyb}^{\top}\bar{\mathbf{U}}_{z}^{\top} \mathbf{z}^{k}+\gamma\mathbf{\Lambda}_{za}\hat{\mathbf{U}}_{z}^{\top}\left[ \mathbf{p}^{k+1}(\bar{\mathbf{x}}^{k+1},\bar{\mathbf{y}}^{k+2})-\mathbf{p}^{k}( \bar{\mathbf{x}}^{k},\bar{\mathbf{y}}^{k+1})\right],\end{aligned}\right.\]

\[\text{iter. of }\mathbf{x}:\left\{\begin{aligned} &\bar{x}^{k+1}=\bar{x}^{k}-\alpha\bar{r}^{k+1},\\ &\hat{\mathbf{U}}_{x}^{\top}\mathbf{x}^{k+1}=(\mathbf{\hat{ \Lambda}}_{xc}-\mathbf{\hat{\Lambda}}_{xb}^{2})\hat{\mathbf{U}}_{x}^{\top} \mathbf{x}^{k}-\hat{\mathbf{U}}_{x}^{\top}\mathbf{t}_{x}^{k}-\alpha\mathbf{ \hat{\Lambda}}_{xa}\hat{\mathbf{U}}_{The above three equations are equivalent to:

\[\left[\begin{array}{c}\hat{\mathbf{U}}_{y}^{\top}\mathbf{y}^{k+1}\\ \mathbf{\Lambda}_{yb}^{-1}\hat{\mathbf{U}}_{y}^{\top}\mathbf{t}_{y}^{k+1}\end{array} \right]= \left[\begin{array}{cc}\mathbf{\Lambda}_{yc}-\mathbf{\Lambda}_{yb}^{2}&- \mathbf{\Lambda}_{yb}\\ \mathbf{\Lambda}_{yb}&\mathbf{I}\end{array}\right]\left[\begin{array}{c}\hat {\mathbf{U}}_{y}^{\top}\mathbf{y}^{k}\\ \mathbf{\Lambda}_{yb}^{-1}\hat{\mathbf{U}}_{y}^{\top}\mathbf{t}_{y}^{y}\end{array}\right]\] (28) \[-\beta\left[\begin{array}{cc}\mathbf{\Lambda}_{ya}\hat{\mathbf{U }}_{y}^{\top}\left[\mathbf{y}^{k}-\nabla_{2}\mathbf{g}(\bar{\mathbf{x}}^{k}, \bar{\mathbf{y}}^{k})\right]\\ \mathbf{\Lambda}_{yb}^{-1}\mathbf{\Lambda}_{ya}\hat{\mathbf{U}}_{y}^{\top} \left[\nabla_{2}\mathbf{g}(\bar{\mathbf{x}}^{k+1},\bar{\mathbf{y}}^{k+1})- \nabla_{2}\mathbf{g}(\bar{\mathbf{x}}^{k},\bar{\mathbf{y}}^{k})\right]\end{array} \right],\]

\[\left[\begin{array}{c}\hat{\mathbf{U}}_{z}^{\top}\mathbf{z}^{k+1}\\ \mathbf{\Lambda}_{zb}^{-1}\hat{\mathbf{U}}_{z}^{\top}\mathbf{t}_{z}^{k+1} \end{array}\right]= \left[\begin{array}{cc}\mathbf{\Lambda}_{zc}-\mathbf{\Lambda}_{zb}^{2} &-\mathbf{\Lambda}_{zb}\\ \mathbf{\Lambda}_{zb}&\mathbf{I}\end{array}\right]\left[\begin{array}{c}\hat {\mathbf{U}}_{z}^{\top}\mathbf{z}^{k}\\ \mathbf{\Lambda}_{zb}^{-1}\hat{\mathbf{U}}_{z}^{\top}\mathbf{t}_{\bar{k}}^{k} \end{array}\right]\] (29) \[-\gamma\left[\begin{array}{cc}\mathbf{\Lambda}_{za}\hat{ \mathbf{U}}_{z}^{\top}\left[\mathbf{p}^{k}-\mathbf{p}^{k}(\bar{\mathbf{x}}^{k },\bar{\mathbf{y}}^{k+1})\right]\\ \mathbf{\Lambda}_{zb}^{-1}\mathbf{\Lambda}_{za}\hat{\mathbf{U}}_{z}^{\top} \left[\mathbf{p}^{k+1}(\bar{\mathbf{x}}^{k+1},\bar{\mathbf{y}}^{k+2})- \mathbf{p}^{k}(\bar{\mathbf{x}}^{k},\bar{\mathbf{y}}^{k+1})\right]\end{array} \right],\]

\[\left[\begin{array}{c}\hat{\mathbf{U}}_{x}^{\top}\mathbf{x}^{k+1}\\ \mathbf{\Lambda}_{zb}^{-1}\hat{\mathbf{U}}_{x}^{\top}\mathbf{t}_{x}^{k+1} \end{array}\right]= \left[\begin{array}{cc}\mathbf{\Lambda}_{xc}-\mathbf{\Lambda}_{ xb}^{2}&-\mathbf{\Lambda}_{xb}\\ \mathbf{\Lambda}_{xb}&\mathbf{I}\end{array}\right]\left[\begin{array}{cc} \hat{\mathbf{U}}_{x}^{\top}\mathbf{x}^{k}\\ \mathbf{\Lambda}_{xb}^{-1}\hat{\mathbf{U}}_{x}^{\top}\mathbf{t}_{x}^{k} \end{array}\right]\] (30) \[-\alpha\left[\begin{array}{cc}\mathbf{\Lambda}_{xa}\hat{ \mathbf{U}}_{x}^{\top}\left[\mathbf{r}^{k+1}-\widetilde{\nabla}\mathbf{ \Phi}(\bar{\mathbf{x}}^{k})\right]\\ \mathbf{\Lambda}_{xb}^{-1}\mathbf{\Lambda}_{xa}\hat{\mathbf{U}}_{x}^{\top} \left[\widetilde{\nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{k+1})-\widetilde{ \nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{k})\right]\end{array}\right].\]

For \(\mathbf{s}\in\{\mathbf{x},\mathbf{y},\mathbf{z}\}\), define:

\[\mathbf{e}_{s}^{k}=\left[\begin{array}{c}\hat{\mathbf{U}}_{x}^{\top} \mathbf{s}^{k}\\ \mathbf{\Lambda}_{sb}^{-1}\hat{\mathbf{U}}_{s}^{\top}\mathbf{t}_{s}^{k}\end{array} \right],\quad\mathbf{M}_{s}=\left[\begin{array}{cc}\mathbf{\Lambda}_{sc}- \mathbf{\Lambda}_{sb}^{2}&-\mathbf{\Lambda}_{sb}\\ \mathbf{\Lambda}_{sb}&\mathbf{I}\end{array}\right].\]

Then (28), (29), (30) are respectively equivalent to:

\[\mathbf{e}_{y}^{k+1} =\mathbf{M}_{y}\mathbf{e}_{y}^{k}-\beta\left[\begin{array}{c} \mathbf{\Lambda}_{yb}\hat{\mathbf{U}}_{y}^{\top}\left[\mathbf{v}^{k}-\nabla_{2} \mathbf{g}(\bar{\mathbf{x}}^{k},\bar{\mathbf{y}}^{k})\right]\\ \mathbf{\Lambda}_{yb}^{-1}\mathbf{\Lambda}_{ya}\mathbf{\bar{\mathbf{U}}_{y}^{ \top}\left[\nabla_{2}\mathbf{g}(\bar{\mathbf{x}}^{k+1},\bar{\mathbf{y}}^{k+1}) -\nabla_{2}\mathbf{g}(\bar{\mathbf{x}}^{k},\bar{\mathbf{y}}^{k})\right]\end{array} \right],\] \[\mathbf{e}_{z}^{k+1} =\mathbf{M}_{z}\mathbf{e}_{z}^{k}-\gamma\left[\begin{array}{c} \mathbf{\Lambda}_{za}\hat{\mathbf{U}}_{z}^{\top}\left[\mathbf{p}^{k}-\mathbf{p }^{k}(\bar{\mathbf{x}}^{k},\bar{\mathbf{y}}^{k+1})\right]\\ \mathbf{\Lambda}_{zb}^{-1}\mathbf{\Lambda}_{za}\hat{\mathbf{U}}_{z}^{\top} \left[\mathbf{p}^{k+1}(\bar{\mathbf{x}}^{k+1},\bar{\mathbf{y}}^{k+2})- \mathbf{p}^{k}(\bar{\mathbf{x}}^{k},\bar{\mathbf{y}}^{k+1})\right]\end{array} \right],\] \[\mathbf{e}_{x}^{k+1} =\mathbf{M}_{x}\mathbf{e}_{k}^{x}-\alpha\left[\begin{array}{cc} \mathbf{\Lambda}_{xa}\hat{\mathbf{U}}_{x}^{\top}\left[\mathbf{r}^{k+1}- \widetilde{\nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{k})\right]\\ \mathbf{\Lambda}_{xb}^{-1}\mathbf{\Lambda}_{xa}\hat{\mathbf{U}}_{x}^{\top} \left[\bar{\nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{k+1})-\widetilde{\nabla} \mathbf{\Phi}(\bar{\mathbf{x}}^{k})\right]\end{array}\right].\]

Assumption 2, 3 imply that all eigenvalues of

\[\left[\begin{array}{cc}\text{diag}\{0,\Lambda_{sc}-\Lambda_{sb}^{2} \}&-\text{diag}\{0,\Lambda_{sb}\}\\ \text{diag}\{0,\Lambda_{sb}\}&\text{diag}\{0,1,...,1\}\end{array}\right]\] (31) \[=\left[\begin{array}{cc}U_{s}^{\top}&\\ &U_{s}^{\top}\end{array}\right]\left[\begin{array}{cc}C_{s}-\frac{1}{n}{1_{n}}{1 _{n}}{1_{n}^{\top}}-B_{s}^{2}&-B_{s}\\ B_{s}&I_{n}-\frac{1}{n}{1_{n}}{1_{n}}{1_{n}^{\top}}\end{array}\right]\left[ \begin{array}{cc}U_{s}&\\ &U_{s}\end{array}\right]\]

are strictly less than one in magnitude. Thus by symmetrically exchanging columns and rows of the matrix, we know that equivalently, all eigenvalues of

\[\left[\begin{array}{cc}\Lambda_{sc}-\Lambda_{sb}^{2}&-\Lambda_{sb}\\ \Lambda_{sb}&I_{n-1}\end{array}\right]\text{ and }\mathbf{M}_{s}=\left[\begin{array}{cc}\mathbf{\Lambda}_{sc}- \mathbf{\Lambda}_{sb}^{2}&-\mathbf{\Lambda}_{sb}\\ \mathbf{\Lambda}_{sb}&I_{n-1}\otimes I_{dim(s)}\end{array}\right]\] (32)

are strictly less than one in magnitude,.

Then according to Lemma 3, for \(s\in\{x,y,z\}\), \(\mathbf{M}_{s}\) has the similarity transformation:

\[\mathbf{M}_{s}=\mathbf{O}_{s}\mathbf{\Gamma}_{s}\mathbf{O}_{s}^{-1},\]

where \(\mathbf{O}_{s}\) is invertible and \(\|\mathbf{\Gamma}_{s}\|<1\). Moreover, we define \(\hat{\mathbf{e}}_{s}^{k}=\mathbf{O}_{s}^{-1}\mathbf{e}_{s}^{k}\). It yields

\[\hat{\mathbf{e}}_{y}^{k+1}=\mathbf{\Gamma}_{y}\hat{\mathbf{e}}_{y}^{k}-\beta \mathbf{O}_{y}^{-1}\left[\begin{array}{c}\mathbf{\Lambda\[\hat{\mathbf{e}}_{z}^{k+1} =\mathbf{\Gamma}_{y}\hat{\mathbf{e}}_{z}^{k}-\gamma\mathbf{O}_{z}^{ -1}\left[\begin{array}{c}\mathbf{\Lambda}_{a}\mathbf{\hat{U}}_{z}^{\top}\left[ \mathbf{p}^{k}-\mathbf{p}^{k}(\bar{\mathbf{x}}^{k},\bar{\mathbf{y}}^{k+1}) \right]\\ \mathbf{\Lambda}_{zb}^{-1}\mathbf{\Lambda}_{za}\hat{\mathbf{U}}_{z}^{\top} \left[\mathbf{p}^{k+1}(\bar{\mathbf{x}}^{k+1},\bar{\mathbf{y}}^{k+2})- \mathbf{p}^{k}(\bar{\mathbf{x}}^{k},\bar{\mathbf{y}}^{k+1})\right]\end{array} \right],\] (34) \[\hat{\mathbf{e}}_{x}^{k+1} =\mathbf{\Gamma}_{x}\hat{\mathbf{e}}_{x}^{k}-\alpha\mathbf{O}_{x }^{-1}\left[\begin{array}{c}\mathbf{\Lambda}_{xa}\mathbf{\hat{U}}_{x}^{\top} \left[\mathbf{r}^{k+1}-\widetilde{\nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{k}) \right]\\ \mathbf{\Lambda}_{xb}^{-1}\mathbf{\Lambda}_{xa}\mathbf{\hat{U}}_{x}^{\top} \left[\widetilde{\nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{k+1})-\widetilde{ \nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{k})\right]\end{array}\right].\] (35)

Then, for \(\mathbf{s}\in\{\mathbf{x},\mathbf{y},\mathbf{z}\}\), the consensus errors between different agents have the upper bound of:

\[\left\|\mathbf{s}^{k}-\bar{\mathbf{s}}^{k}\right\|^{2}=\|\hat{\mathbf{U}}_{s}^ {\top}\mathbf{s}^{k}\|^{2}\leq\|\mathbf{e}_{s}^{k}\|^{2}\leq\|\mathbf{O}_{s} \|^{2}\|\hat{\mathbf{e}}_{s}^{k}\|^{2}.\] (36)

Thus, we can define:

\[\Delta_{k}=\kappa^{2}\|\mathbf{O}_{x}\|^{2}\|\hat{\mathbf{e}}_{x}^{k}\|^{2}+ \kappa^{2}\|\mathbf{O}_{y}\|^{2}\|\hat{\mathbf{e}}_{y}^{k+1}\|^{2}+\|\mathbf{O }_{z}\|^{2}\|\hat{\mathbf{e}}_{z}^{k+1}\|^{2}\]

to measure the consensus error during the iteration.

We also define

\[I_{k}=\|\bar{z}^{k+1}-z_{\star}^{k+1}\|^{2}+\kappa^{2}\|\bar{y}^{k+1}-y^{\star }(\bar{x}^{k})\|^{2},\]

to measure the estimation accuracy of the lower- and auxiliary-level problems.

#### c.1.3 Proof sketch

Before proceeding with the formal proof, we first present the structure of the proof in Appendix C.

Bounded by each other

\[\left\|\right.\]

\[\left.\begin{array}{ccc}\text{Descent of }x&\sum\mathbb{E}\|\bar{r}^{k}\|^{ 2}\\ \text{Descent of }y&\sum\mathbb{E}\|\bar{y}^{k+1}-y^{\star}(\bar{x}^{k})\|^{2}\\ \text{Descent of }z&\sum\mathbb{E}\|\hat{z}^{k+1}-z_{\star}^{k+1}\|^{2}\\ \text{Consensus of }y&\sum\mathbb{E}\|\hat{\mathbf{e}}_{y}^{k}\|^{2}\\ \text{Consensus of }z&\sum\mathbb{E}\|\hat{\mathbf{e}}_{z}^{k}\|^{2}\\ \text{Consensus of }x&\sum\mathbb{E}\|\hat{\mathbf{e}}_{x}^{k}\|^{2}\\ \text{Hyper-gradient estimation}&\sum\mathbb{E}\|\mathbb{E}_{k}\mathbf{u}^{k}- \widetilde{\nabla}\mathbf{\Phi}(\bar{x}^{k})\|^{2}\\ \text{Hyper-gradient estimation}&\sum\mathbb{E}\|\mathbb{E}_{k}\mathbf{r}^{k+1}- \widetilde{\nabla}\mathbf{\Phi}(\bar{x}^{k})\|^{2}\\ \text{Variance}&\sum\mathbb{E}\|\mathbb{E}_{k}\mathbf{u}^{k}-\mathbf{u}^{k}\|^ {2}\end{array}\right\}\]

#### c.1.4 Technical lemmas

**Lemma 1**.: _Suppose Assumptions 1 hold, we know \(\nabla\Phi(x),\widetilde{\nabla}\mathbf{\Phi}(x)\), \(z^{\star}(x)\) and \(y^{\star}(x)\) defined above are \(L_{\nabla\Phi}\), \(\widetilde{L}\), \(L_{z^{\star}}\), \(L_{y^{\star}}\)- Lipschitz continuous respectively with the constants satisfying:_

\[L_{\nabla\Phi} \leq L_{f,1}+\frac{2L_{f,1}L_{g,1}+L_{g,2}L_{f,0}}{\mu_{g}}+ \frac{2L_{g,1}L_{f,0}L_{g,2}+L_{g,1}^{2}L_{f,1}}{\mu_{g}^{2}}+\frac{L_{g,2}L_{ g,1}^{2}L_{f,0}}{\mu_{g}^{3}},\] \[\widetilde{L} \leq L_{f,1}+\frac{2L_{f,1}L_{g,1}+L_{g,2}L_{f,0}}{\mu_{g}}+ \frac{2L_{g,1}L_{f,0}L_{g,2}+L_{g,1}^{2}L_{f,1}}{\mu_{g}^{2}}+\frac{L_{g,2}L_{ g,1}^{2}L_{f,0}}{\mu_{g}^{3}},\] \[L_{y^{\star}} \leq\frac{L_{g,1}}{\mu_{g}},\] \[L_{z^{\star}} \leq\sqrt{1+L_{y^{\star}}^{2}}\left(\frac{L_{f,1}}{\mu_{g}}+ \frac{L_{f,0}L_{g,2}}{\mu_{g}^{2}}\right).\]

_And we also have:_

\[\|z^{\star}(x)\|\leq\frac{L_{f,0}}{\mu_{g}},\quad\forall x\in\mathbb{R}^{p}.\]

Proof.: See Lemma 2.2 in [22] and Lemma B.2 in [11]. 

**Lemma 2**.: _Suppose that \(g(x)\) is \(\mu\)-strongly convex and \(L\)-smooth. Then for any \(x\) and \(0<\alpha<\frac{2}{\mu+L}\), we have_

\[\|x-\alpha\nabla g(x)-x^{\star}\|\leq(1-\alpha\mu)\left\|x-x^{\star}\right\|,\]

_where \(x^{\star}=\operatorname*{argmin}g(x)\)._

Proof.: See Lemma 10 in [41]. 

**Lemma 3**.: _Given diagonal matrices \(A,B,C,D\in\mathbb{R}^{(n-1)\times(n-1)}\), and_

\[\mathbf{M}=\left[\begin{array}{cc}A\otimes I_{d}&B\otimes I_{d}\\ C\otimes I_{d}&D\otimes I_{d}\end{array}\right].\]

_Suppose that the eigenvalues of \(\mathbf{M}\) are strictly less than one in magnitude. Then there exist an invertible matrix \(\mathbf{O}\) and a matrix \(\mathbf{\Gamma}\) with \(\left\|\mathbf{\Gamma}\right\|<1\), such that \(\mathbf{M}\) has the similarity transformation:_

\[\mathbf{M}=\mathbf{O}\mathbf{\Gamma}\mathbf{O}^{-1}.\]

Proof.: See Lemma 1 in [2]. 

**Remark 5**.: _Asserting the existence of \(\mathbf{\Gamma}\) with \(\left\|\mathbf{\Gamma}\right\|<1\), Lemma 3 only guarantees the convergence of SPARKLE. However, to obtain a precise non-asymptotic convergence rate, one must construct appropriate \(\mathbf{O}\) and \(\mathbf{\Gamma}\). See more details in Appendix C.2.2._

#### c.1.5 Descent lemmas for the upper-level

In this subsection, we estimate the upper bound of the errors induced by the moving average in hyper-gradient estimation, as well as the upper bound of \(\|\nabla\Phi(x)\|^{2}\) based on \(I_{k},\Delta_{k}\).

**Lemma 4**.: _Suppose Assumptions 1- 4 hold. We have:_

\[\left\|\mathbb{E}_{k}\bar{u}^{k}-\nabla\Phi(\bar{x}^{k})\right\|^ {2}\leq \frac{20}{n}L^{2}(\Delta_{k}+nI_{k}),\] (38) \[\left\|\mathbb{E}_{k}\mathbf{u}^{k}-\widetilde{\nabla}\mathbf{ \Phi}(\bar{x}^{k})\right\|^{2}\leq 20L^{2}(\Delta_{k}+nI_{k}).\]Proof.: Cauchy Schwartz inequality implies that:

\[\left\|\mathbb{E}_{k}\mathbf{u}^{k}-\widetilde{\nabla}\mathbf{\Phi} \left(\bar{\mathbf{x}}^{k}\right)\right\|^{2}\] \[\leq 5\sum_{i=1}^{n}\left\|\nabla_{1}f_{i}\left(x_{i}^{k},y_{i}^{k+1} \right)-\nabla_{1}f_{i}\left(\bar{x}^{k},\bar{y}^{k+1}\right)\right\|^{2}+5 \sum_{i=1}^{n}\left\|\nabla_{1}f_{i}\left(\bar{x}^{k},\bar{y}^{k+1}\right)- \nabla_{1}f_{i}\left(\bar{x}^{k},y^{*}(\bar{x}^{k})\right)\right\|^{2}\] \[+5\sum_{i=1}^{n}\left\|\nabla_{12}^{2}g_{i}\left(x_{i}^{k},y_{i}^{ k+1}\right)\left(z_{i}^{k+1}-z_{*}^{k+1}\right)\right\|^{2}\] \[+5\sum_{i=1}^{n}\left\|\left(\nabla_{12}^{2}g_{i}\left(x_{i}^{k}, y_{i}^{k+1}\right)-\nabla_{12}^{2}g_{i}\left(\bar{x}^{k},\bar{y}^{k+1}\right) \right)z_{*}^{k+1}\right\|^{2}\] \[\leq 10\left(L_{f,1}^{2}+\kappa^{2}L_{f,0}^{2}\right)\left(\left\| \mathbf{x}^{k}-\bar{\mathbf{x}}^{k}\right\|^{2}+\left\|\mathbf{y}^{k+1}-\bar{ \mathbf{y}}^{k+1}\right\|^{2}+\left\|\bar{\mathbf{y}}^{k+1}-\mathbf{y}^{*}( \bar{\mathbf{x}}^{k})\right\|^{2}\right)\] \[+10L_{g,1}^{2}\left(\left\|\mathbf{z}^{k+1}-\bar{\mathbf{z}}^{k+1 }\right\|^{2}+\left\|\mathbf{z}^{k+1}-\mathbf{z}_{*}^{k+1}\right\|^{2}\right)\] \[\leq 20L^{2}\left(\Delta_{k}+nI_{k}\right).\]

For the term \(\left\|\mathbb{E}_{k}\bar{u}^{k}-\nabla\Phi\left(\bar{x}^{k}\right)\right\|^{2}\), we have:

\[\left\|\mathbb{E}_{k}\bar{u}^{k}-\nabla\Phi(\bar{x}^{k})\right\|^{2}\leq\frac{ 1}{n}\left\|\mathbb{E}_{k}\mathbf{u}^{k}-\widetilde{\nabla}\mathbf{\Phi} \left(\bar{\mathbf{x}}^{k}\right)\right\|^{2}\leq 20L^{2}\left(\frac{\Delta_{k}}{n}+I_{k} \right).\]

**Lemma 5**.: _Suppose that Assumptions 1- 4 hold. We have_

\[n^{2}\sum_{k=0}^{K}\mathbb{E}\left[\|\bar{u}^{k}-\mathbb{E}_{k}[ \bar{u}^{k}]\|^{2}\right]=\sum_{k=0}^{K}\mathbb{E}\left[\|\mathbf{u}^{k}- \mathbb{E}_{k}[\mathbf{u}^{k}]\|^{2}\right]\] (39) \[\leq 9\sigma_{g,2}^{2}\sum_{k=0}^{K}\left(\mathbb{E}\|\mathbf{z}^{k+ 1}-\bar{\mathbf{z}}^{k+1}\|^{2}+\mathbb{E}\|\bar{\mathbf{z}}^{k+1}-\mathbf{z} _{*}^{k+1}\|^{2}\right)+3(K+1)n\left(\sigma_{f,1}^{2}+3\sigma_{g,2}^{2}\frac{L _{f,0}^{2}}{\mu_{g}^{2}}\right).\]

Proof.: For \(k\geq 0\), Cauchy Schwartz inequality implies that

\[\frac{1}{3}\mathbb{E}_{k}\left[\|\mathbf{u}^{k}-\mathbb{E}_{k}[ \mathbf{u}^{k}]\|^{2}\right]\] \[\leq \mathbb{E}_{k}\left[\sum_{i=1}^{n}\|\nabla_{1}f_{i}(x_{i}^{k},y_{ i}^{k+1},\xi_{i}^{k})-\nabla_{1}f_{i}(x_{i}^{k},y_{i}^{k+1})\|^{2}\right]\] \[+\mathbb{E}_{k}\left[\sum_{i=1}^{n}\left\|\left(\nabla_{12}g_{i}( x_{i}^{k},y_{i}^{k+1},\zeta_{i}^{k})-\nabla_{12}g_{i}(x_{i}^{k},y_{i}^{k+1}) \right)z_{i}^{k+1}\right\|^{2}\right]\] \[\leq n\sigma_{f,1}^{2}+\sigma_{g,2}^{2}\|\mathbf{z}^{k+1}\|^{2}\] \[\leq n\sigma_{f,1}^{2}+3\sigma_{g,2}^{2}\left(\|\mathbf{z}^{k+1}-\bar{ \mathbf{z}}^{k+1}\|^{2}+\|\bar{\mathbf{z}}^{k+1}-\mathbf{z}_{*}^{k+1}\|^{2}+\| \mathbf{z}_{*}^{k+1}\|^{2}\right)\] \[\leq n\sigma_{f,1}^{2}+3\sigma_{g,2}^{2}\left(\|\mathbf{z}^{k+1}-\bar{ \mathbf{z}}^{k+1}\|^{2}+\|\bar{\mathbf{z}}^{k+1}-\mathbf{z}_{*}^{k+1}\|^{2}+n \frac{L_{f,0}^{2}}{\mu_{g}^{2}}\right).\]

Then taking expectation and summation on both sides, we get

\[\sum_{k=0}^{K}\mathbb{E}\left[\|\mathbf{u}^{k}-\mathbb{E}_{k}[ \mathbf{u}^{k}]\|^{2}\right]\] \[\leq 9\sigma_{g,2}^{2}\sum_{k=0}^{K}\left(\mathbb{E}\|\mathbf{z}^{k+ 1}-\bar{\mathbf{z}}^{k+1}\|^{2}+\mathbb{E}\|\bar{\mathbf{z}}^{k+1}-\mathbf{z}_{ *}^{k+1}\|^{2}\right)+3(K+1)n\left(\sigma_{f,1}^{2}+3\sigma_{g,2}^{2}\frac{L_{f,0 }^{2}}{\mu_{g}^{2}}\right).\]Since samples among agents are independent, it follows that

\[\sum_{k=0}^{K}\mathbb{E}_{k}\left[\|\bar{u}^{k}-\mathbb{E}_{k}[\bar{u}^{k}]\|^{2} \right]=\frac{1}{n^{2}}\sum_{k=0}^{K}\mathbb{E}_{k}\left[\|\mathbf{u}^{k}- \mathbb{E}_{k}[\mathbf{u}^{k}]\|^{2}\right].\]

Taking expectations, we get the conclusion. 

**Lemma 6**.: _Suppose that Assumptions 1- 4, and Lemmas 4, 5 hold. If_

\[\alpha\leq\frac{1}{2L_{\nabla\Phi}},\] (40)

_we have_

\[\frac{1}{4}\sum_{k=0}^{K}\mathbb{E}\left\|\bar{r}^{k+1}\right\|^{2}\] (41) \[\leq \frac{\Phi(\bar{x}_{0})-\inf\Phi}{\alpha}+10\left(L^{2}+\frac{ \theta\sigma_{g,2}^{2}}{n}\right)\sum_{k=0}^{K}\mathbb{E}\left(\frac{\Delta_{ k}}{n}+I_{k}\right)+\frac{3\theta}{n}(K+1)\left(\sigma_{f,1}^{2}+2\sigma_{g,2}^{2} \frac{L_{f,0}^{2}}{\mu_{g}^{2}}\right).\]

Proof.: The \(L_{\nabla\Phi}\)-smoothness of \(\Phi\) indicates that

\[\mathbb{E}_{k}[\Phi\left(\bar{x}^{k+1}\right)]-\Phi(\bar{x}^{k})\] \[\leq \left\langle\nabla\Phi(\bar{x}^{k}),\left(-\alpha\mathbb{E}_{k}[ \bar{r}^{k+1}]\right)\right\rangle+\frac{L_{\nabla\Phi}\alpha^{2}}{2}\mathbb{ E}_{k}\|\bar{r}^{k+1}\|^{2}\] \[= \left\langle\nabla\Phi(\bar{x}^{k})-\mathbb{E}_{k}[\bar{u}^{k}], -\alpha\mathbb{E}_{k}[\bar{r}^{k+1}]\right\rangle+\frac{L_{\nabla\Phi}}{2} \alpha^{2}\mathbb{E}_{k}\|\bar{r}^{k+1}\|^{2}-\alpha\left\langle\mathbb{E}_{ k}[\bar{u}^{k}],\mathbb{E}_{k}[\bar{r}^{k+1}]\right\rangle.\]

Then, due to \(\mathbb{E}_{k}[\bar{u}^{k}]=\theta^{-1}(\mathbb{E}_{k}[\bar{r}^{k+1}]-(1- \theta)\bar{r}^{k})\), we have:

\[\mathbb{E}_{k}[\Phi\left(\bar{x}^{k+1}\right)]-\Phi(\bar{x}^{k})\] \[\leq \frac{\alpha}{2}\|\nabla\Phi(\bar{x}^{k})-\mathbb{E}_{k}[\bar{u}^ {k}]\|^{2}+\frac{\alpha}{2}\|\mathbb{E}_{k}[\bar{r}^{k+1}]\|^{2}\] \[+\frac{L_{\nabla\Phi}}{2}\alpha^{2}\mathbb{E}_{k}\|\bar{r}^{k+1} \|^{2}-\alpha\left\langle\mathbb{E}_{k}[\bar{u}^{k}],\mathbb{E}_{k}[\bar{r}^{ k+1}]\right\rangle\] \[= \frac{\alpha}{2}\|\nabla\Phi(\bar{x}^{k})-\mathbb{E}_{k}[\bar{u}^ {k}]\|^{2}+(-\frac{\alpha}{2}+\frac{L_{\nabla\Phi}}{2}\alpha^{2})\mathbb{E}_{ k}\|\bar{r}^{k+1}\|^{2}-\frac{\alpha(1-\theta)}{2\theta}\|\mathbb{E}_{k}[\bar{r}^{k+ 1}]-\bar{r}^{k}\|^{2}\] \[+\frac{\alpha(1-\theta)}{2\theta}\left(\|\bar{r}^{k}\|^{2}- \mathbb{E}_{k}\|\bar{r}^{k+1}\|^{2}\right)+\frac{\alpha\theta}{2\theta} \mathbb{E}_{k}\|\bar{r}^{k+1}-\mathbb{E}_{k}[\bar{r}^{k+1}]\|^{2}\] \[\leq \frac{\alpha}{2}\|\nabla\Phi(\bar{x}^{k})-\mathbb{E}_{k}[\bar{u}^ {k}]\|^{2}+(-\frac{\alpha}{2}+\frac{L_{\nabla\Phi}}{2}\alpha^{2})\mathbb{E}_{ k}\|\bar{r}^{k+1}\|^{2}+\frac{\alpha\theta}{2}\mathbb{E}_{k}\|\bar{u}^{k}- \mathbb{E}_{k}[\bar{u}^{k}]\|^{2}\] \[+\frac{\alpha(1-\theta)}{2\theta}\left(\|\bar{r}^{k}\|^{2}- \mathbb{E}_{k}\|\bar{r}^{k+1}\|^{2}\right),\]

where the first equality uses \(2\left\langle\bar{r}^{k},\mathbb{E}_{k}[\bar{r}^{k+1}]\right\rangle=\|\bar{r}^ {k}\|^{2}+\|\mathbb{E}_{k}[\bar{r}^{k+1}]\|^{2}-\|\bar{r}^{k}-\mathbb{E}_{k}[ \bar{r}^{k+1}]\|^{2}\) and \(\mathbb{E}_{k}\|\bar{r}^{k+1}\|^{2}=\|\mathbb{E}_{k}[\bar{r}^{k+1}]\|^{2}+ \mathbb{E}_{k}\|\bar{r}^{k+1}-\mathbb{E}_{k}[\bar{r}^{k+1}]\|^{2}\).

Taking expectation and summation, and using \(\alpha\leq\frac{1}{2L_{\nabla\Phi}}\), we get

\[\inf\Phi-\Phi(\bar{x}_{0})\] (42) \[\leq \frac{\alpha}{2}\sum_{k=0}^{K}\mathbb{E}\|\nabla\Phi(\bar{x}^{k}) -\mathbb{E}_{k}[\bar{u}^{k}]\|^{2}-\frac{\alpha}{4}\sum_{k=0}^{K}\mathbb{E}\| \bar{r}^{k+1}\|^{2}+\frac{\alpha\theta}{2}\sum_{k=0}^{K}\mathbb{E}\left[\mathbb{ E}_{k}\|\bar{u}^{k}-\mathbb{E}_{k}[\bar{u}^{k}]\|^{2}\right].\]

Since samples of different agents are independent, we have

\[\mathbb{E}_{k}\|\bar{u}^{k}-\mathbb{E}_{k}[\bar{u}^{k}]\|^{2}=\frac{1}{n^{2}} \mathbb{E}_{k}\|\mathbf{u}^{k}-\mathbb{E}_{k}[\mathbf{u}^{k}]\|^{2}.\]Combining it with the conclusion of Lemma 4 and 5, we get from (42) that

\[\frac{\alpha}{4}\sum_{k=0}^{K}\mathbb{E}\left\|\bar{r}^{k+1}\right\| ^{2}\] \[\leq \Phi(\bar{x}_{0})-\inf\Phi+\frac{\alpha}{2}\sum_{k=0}^{K}\mathbb{E }\|\nabla\Phi(\bar{x}^{k})-\mathbb{E}_{k}[\bar{u}^{k}]\|^{2}+\frac{\alpha\theta }{2}\sum_{k=0}^{K}\mathbb{E}\left[\mathbb{E}_{k}\|\bar{u}^{k}-\mathbb{E}_{k}[ \bar{u}^{k}]\|^{2}\right]\] \[\leq \Phi(\bar{x}_{0})-\inf\Phi+10\alpha\left(L^{2}+\frac{\theta \sigma_{g,2}^{2}}{n}\right)\sum_{k=0}^{K}\mathbb{E}\left(\frac{\Delta_{k}}{n}+ I_{k}\right)+\frac{3\alpha\theta}{n}(K+1)\left(\sigma_{f,1}^{2}+2\sigma_{g,2}^{2} \frac{L_{f,0}^{2}}{\mu_{g}^{2}}\right).\]

**Lemma 7**.: _Suppose that Assumptions 1- 4 hold, then we have_

\[\sum_{k=0}^{K}\mathbb{E}\left[\left\|\mathbb{E}_{k}[\mathbf{r}^{k +1}]-\widetilde{\nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{k})\right\|^{2}\right]\] \[\leq \frac{1-\theta}{\theta}\left\|\widetilde{\nabla}\mathbf{\Phi}( \bar{\mathbf{x}}^{0})\right\|^{2}+2\sum_{k=0}^{K}\mathbb{E}\left[\left\|\mathbb{ E}_{k}[\mathbf{u}^{k}]-\widetilde{\nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{k}) \right\|^{2}\right]\] \[+\frac{2\tilde{L}^{2}(1-\theta)^{2}}{\theta^{2}}\sum_{k=0}^{K-1} \mathbb{E}\left[\left\|\bar{\mathbf{x}}^{k+1}-\bar{\mathbf{x}}^{k}\right\|^{ 2}\right]+(1-\theta)\theta\sum_{k=0}^{K}\mathbb{E}\left[\left\|\mathbf{u}^{k} -\mathbb{E}_{k}[\mathbf{u}^{k}]\right\|^{2}\right].\]

Proof.: We define \(\mathbf{u}^{-1}=\mathbf{0}\) for brevity. From the definition of \(\mathbb{E}_{k}\), we have :

\[\mathbb{E}_{k-1}\left[\left\|\mathbf{r}^{k}-\widetilde{\nabla} \mathbf{\Phi}(\bar{\mathbf{x}}^{k-1})\right\|^{2}\right]\] (43) \[= \mathbb{E}_{k-1}\left[\left\|\mathbb{E}_{k-1}[\mathbf{r}^{k}]- \widetilde{\nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{k-1})\right\|^{2}\right]+ \theta^{2}\mathbb{E}_{k-1}\left[\left\|\mathbf{u}^{k-1}-\mathbb{E}_{k-1}[ \mathbf{u}^{k-1}]\right\|^{2}\right].\]

Jensen's inequality implies that

\[\mathbb{E}_{k}\left[\left\|\mathbb{E}_{k}[\mathbf{r}^{k+1}]- \widetilde{\nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{k})\right\|^{2}\right]\] (44) \[\leq (1-\theta)\mathbb{E}_{k}\left[\left\|\mathbf{r}^{k}-\widetilde{ \nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{k-1})\right\|^{2}\right]\] \[+\theta\mathbb{E}_{k}\left[\left\|\left(\mathbb{E}_{k}[\mathbf{u} ^{k}]-\widetilde{\nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{k})\right)+\theta^{-1} (1-\theta)\left(\widetilde{\nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{k-1})- \widetilde{\nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{k})\right)\right\|^{2}\right]\] \[\leq (1-\theta)\mathbb{E}_{k}\left[\left\|\mathbf{r}^{k}-\widetilde{ \nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{k-1})\right\|^{2}\right]+2\theta \mathbb{E}_{k}\left[\left\|\mathbb{E}_{k}[\mathbf{u}^{k}]-\widetilde{\nabla }\mathbf{\Phi}(\bar{\mathbf{x}}^{k})\right\|^{2}\right]\] \[+\frac{2(1-\theta)^{2}}{\theta}\mathbb{E}_{k}\left[\left\| \widetilde{\nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{k-1})-\widetilde{\nabla} \mathbf{\Phi}(\bar{\mathbf{x}}^{k})\right\|^{2}\right].\]

Substituting (43) into (44), and taking expectation and summation on both sides, we get:

\[\theta\sum_{k=0}^{K}\mathbb{E}\left[\left\|\mathbb{E}_{k-1}[ \mathbf{r}^{k}]-\widetilde{\nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{k-1})\right\| ^{2}\right]\] \[\leq \mathbb{E}\left[\left\|\mathbf{r}^{0}-\widetilde{\nabla}\mathbf{ \Phi}(\bar{\mathbf{x}}^{-1})\right\|^{2}\right]-\mathbb{E}\left[\left\| \mathbb{E}_{K}[\mathbf{r}^{K+1}]-\widetilde{\nabla}\mathbf{\Phi}(\bar{ \mathbf{x}}^{k})\right\|^{2}\right]+2\theta\sum_{k=0}^{K}\mathbb{E}\left[ \left\|\mathbb{E}_{k}[\mathbf{u}^{k}]-\widetilde{\nabla}\mathbf{\Phi}(\bar{ \mathbf{x}}^{k})\right\|^{2}\right]\] \[+\frac{2(1-\theta)^{2}}{\theta}\sum_{k=0}^{K}\mathbb{E}\left[ \left\|\widetilde{\nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{k-1})-\widetilde{ \nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{k})\right\|^{2}\right]+(1-\theta)\theta ^{2}\sum_{k=0}^{K}\mathbb{E}\left[\left\|\mathbf{u}^{k}-\mathbb{E}_{k}[ \mathbf{u}^{k}]\right\|^{2}\right].\]Finally, note that \(\mathbf{x}^{-1}=\mathbf{x}^{0}\), \(\mathbf{r}^{0}=\mathbf{0}\), and \(\mathbb{E}_{-1}=\mathbb{E}_{0}\). Subtracting \(\theta\mathbb{E}\left[\left\|\mathbb{E}_{-1}[\mathbf{r}^{0}]-\widetilde{\nabla} \mathbf{\Phi}(\bar{\mathbf{x}}^{-1})\right\|^{2}\right]=\theta\left\|\widetilde {\nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{0})\right\|^{2}\) from both sides of this equation, we get:

\[\sum_{k=0}^{K}\mathbb{E}\left[\left\|\mathbb{E}_{k}[\mathbf{r}^{k +1}]-\widetilde{\nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{k})\right\|^{2}\right]\] \[\leq \frac{1-\theta}{\theta}\left\|\widetilde{\nabla}\mathbf{\Phi}( \bar{\mathbf{x}}^{0})\right\|^{2}+2\sum_{k=0}^{K}\mathbb{E}\left[\left\| \mathbb{E}_{k}[\mathbf{u}^{k}]-\widetilde{\nabla}\mathbf{\Phi}(\bar{\mathbf{ x}}^{k})\right\|^{2}\right]\] \[+\frac{2\widetilde{L}^{2}(1-\theta)^{2}}{\theta^{2}}\sum_{k=0}^{ K-1}\mathbb{E}\left[\left\|\bar{\mathbf{x}}^{k+1}-\bar{\mathbf{x}}^{k} \right\|^{2}\right]+(1-\theta)\theta\sum_{k=0}^{K}\mathbb{E}\left[\left\| \mathbf{u}^{k}-\mathbb{E}_{k}[\mathbf{u}^{k}]\right\|^{2}\right].\]

**Lemma 8** (Descent lemma).: _Suppose that Assumptions 1- 4 and Lemmas 4, 5 hold. If_

\[\frac{\alpha^{2}}{\theta^{2}}(1-\theta)\leq\frac{1}{32L_{\nabla\Phi}^{2}},\, \alpha\leq\frac{1}{10L_{\nabla\Phi}},\] (45)

_then we have_

\[\sum_{k=0}^{K}\mathbb{E}\|\nabla\Phi(\bar{x}^{k})\|^{2}\leq \frac{\Phi(\bar{x}_{0})-\inf\Phi}{\alpha}+\left(L^{2}+\left(\theta (1-\theta)+L_{\nabla\Phi}\alpha\theta^{2}\right)\sigma_{g,2}^{2}\right)\sum_{k =0}^{K}\mathbb{E}\left[\frac{\Delta_{k}}{n}+I_{k}\right]\] \[+\left(K+1\right)\left(\theta(1-\theta)+L_{\nabla\Phi}\alpha \theta^{2}\right)(\sigma_{f,1}^{2}+\kappa^{2}\sigma_{g,2}^{2})+\frac{(1- \theta)^{2}}{\theta}\|\nabla\Phi(\bar{x}^{0})\|^{2}.\]

Proof.: The \(L_{\nabla\Phi}\)-smoothness of \(\Phi\) indicates that

\[\mathbb{E}_{k}[\Phi(\bar{x}^{k+1})]-\Phi(\bar{x}^{k})\] \[\leq \left\langle\nabla\Phi(\bar{x}^{k}),-\alpha\mathbb{E}_{k}[\bar{r} ^{k+1}]\right\rangle+\frac{L_{\nabla\Phi}\alpha^{2}}{2}\mathbb{E}_{k}\left\| \bar{r}^{k+1}\right\|^{2}\] \[= -\alpha\left\langle\nabla\Phi(\bar{x}^{k}),\mathbb{E}_{k}[\bar{r} ^{k+1}]-\nabla\Phi(\bar{x}^{k})\right\rangle-\alpha\|\nabla\Phi(\bar{x}^{k})\| ^{2}+\frac{L_{\nabla\Phi}}{2}\alpha^{2}\mathbb{E}_{k}\left\|\bar{r}^{k+1} \right\|^{2}\] \[\leq -\frac{\alpha}{2}\|\nabla\Phi(\bar{x}^{k})\|^{2}+\frac{\alpha}{2} \|\mathbb{E}_{k}[\bar{r}^{k+1}]-\nabla\Phi(\bar{x}^{k})\|^{2}+\frac{L_{\nabla \Phi}}{2}\alpha^{2}\mathbb{E}_{k}\left\|\bar{r}^{k+1}\right\|^{2}.\]

Taking expectation and summation on both sides, we get:

\[\sum_{k=0}^{K}\alpha\mathbb{E}\|\nabla\Phi(\bar{x}^{k})\|^{2}\] (46) \[\leq 2(\Phi(\bar{x}^{0})-\inf\Phi)+\sum_{k=0}^{K}\alpha\mathbb{E}\| \mathbb{E}_{k}[\bar{r}^{k+1}]-\nabla\Phi(\bar{x}^{k})\|^{2}+\sum_{k=0}^{K}L_{ \nabla\Phi}\alpha^{2}\mathbb{E}\left\|\bar{r}^{k+1}\right\|^{2}.\]

Define auxiliary series \(m^{k}\) as:

\[m^{0}=\bar{r}^{0}=0,m^{k+1}=(1-\theta)m^{k}+\theta\nabla\Phi(\bar{x}^{k}).\]

Note that

\[\mathbb{E}_{k}\left\|\bar{r}^{k+1}\right\|^{2} =\left\|\mathbb{E}_{k}\bar{r}^{k+1}\right\|^{2}+\mathbb{E}_{k} \left\|\bar{r}^{k+1}-\mathbb{E}_{k}\bar{r}^{k+1}\right\|^{2}\] (47) \[\leq 2\|\mathbb{E}_{k}[\bar{r}^{k+1}]-\nabla\Phi(\bar{x}^{k})\|^{2 }+2\|\nabla\Phi(\bar{x}^{k})\|^{2}+\theta^{2}\mathbb{E}_{k}\|\bar{u}^{k}- \mathbb{E}_{k}\bar{u}^{k}\|^{2}.\]

Then using the Jenson's Inequality, we get:

\[\|\mathbb{E}_{k}\bar{r}^{k+1}-m^{k+1}\|^{2} =\|(1-\theta)(\bar{r}^{k}-m^{k})+\theta(\mathbb{E}_{k}\bar{u}^{k} -\nabla\Phi(\bar{x}^{k}))\|^{2}\] \[\leq(1-\theta)\|\bar{r}^{k}-m^{k}\|^{2}+\theta\|\mathbb{E}_{k} \bar{u}^{k}-\nabla\Phi(\bar{x}^{k})\|^{2}.\]It follows that for \(k\geq 0\)

\[\mathbb{E}\|\mathbb{E}_{k}\bar{r}^{k+1}-m^{k+1}\|^{2}\] \[\leq (1-\theta)\mathbb{E}\|\mathbb{E}_{k-1}[\bar{r}^{k}]-m^{k}\|^{2}+(1 -\theta)\theta^{2}\mathbb{E}\|\bar{u}^{k-1}-\mathbb{E}_{k-1}\bar{u}^{k-1}\|^{2 }+\theta\mathbb{E}\|\mathbb{E}_{k}\bar{u}^{k}-\nabla\Phi(\bar{x}^{k})\|^{2},\]

where for brevity we define \(\bar{u}^{-1}=0\).

Taking the summation on both sides from \(k=0\) to \(K\), we get

\[\sum_{k=0}^{K}\theta\mathbb{E}\|\mathbb{E}_{k}\bar{r}^{k+1}-m^{k+ 1}\|^{2} \leq\sum_{k=0}^{K-1}\theta\mathbb{E}\|\mathbb{E}_{k}\bar{r}^{k+1} -m^{k+1}\|^{2}+\mathbb{E}\|\mathbb{E}_{K}\bar{r}^{K+1}-m^{K+1}\|^{2}\] \[\leq\sum_{k=0}^{K}\theta\mathbb{E}\|\mathbb{E}_{k}\bar{u}^{k}- \nabla\Phi(\bar{x}^{k})\|^{2}+\sum_{k=0}^{K-1}(1-\theta)\theta^{2}\mathbb{E}\| \bar{u}^{k}-\mathbb{E}_{k}\bar{u}^{k}\|^{2}.\] (48)

On the other hand, due to the definition of \(m^{k}\) and Jenson's Inequality, we have:

\[\|m^{k+1}-\nabla\Phi(\bar{x}^{k})\|^{2} =\|(1-\theta)(m^{k}-\nabla\Phi(\bar{x}^{k}))\|^{2}\] \[=(1-\theta)^{2}\|m^{k}-\nabla\Phi(\bar{x}^{k-1})+\nabla\Phi(\bar {x}^{k-1})-\nabla\Phi(\bar{x}^{k})\|^{2}\] \[\leq(1-\theta)\|m^{k}-\nabla\Phi(\bar{x}^{k-1})\|^{2}+\frac{(1- \theta)^{2}}{\theta}L_{\nabla\Phi}^{2}\alpha^{2}\|\bar{r}^{k}\|^{2}.\]

Taking the summation, we get

\[\sum_{k=0}^{K}\theta\|m^{k+1}-\nabla\Phi(\bar{x}^{k})\|^{2} \leq\|m^{0}-\nabla\Phi(\bar{x}^{-1})\|^{2}+\sum_{k=0}^{K}\frac{(1 -\theta)^{2}}{\theta}L_{\nabla\Phi}^{2}\alpha^{2}\|\bar{r}^{k}\|^{2}\] (49) \[=(1-\theta)^{2}\|\nabla\Phi(\bar{x}^{0})\|^{2}+\sum_{k=0}^{K} \frac{(1-\theta)^{2}}{\theta}L_{\nabla\Phi}^{2}\alpha^{2}\|\bar{r}^{k}\|^{2}.\]

Combining (48) and (49), we obtain:

\[\sum_{k=0}^{K}\theta\mathbb{E}\|\mathbb{E}_{k}\bar{r}^{k+1}-\nabla \Phi(\bar{x}^{k})\|^{2}\] (50) \[\leq 2\sum_{k=0}^{K}\theta\mathbb{E}\|\mathbb{E}_{k}\bar{r}^{k+1}-m^ {k+1}\|^{2}+2\sum_{k=0}^{K}\theta\|m^{k+1}-\nabla\Phi(\bar{x}^{k})\|^{2}\] \[\leq 2\sum_{k=0}^{K}\theta\mathbb{E}\|\mathbb{E}_{k}\bar{u}^{k}- \nabla\Phi(\bar{x}^{k})\|^{2}+2\sum_{k=0}^{K-1}(1-\theta)\theta^{2}\mathbb{E} \|\bar{u}^{k}-\mathbb{E}_{k}\bar{u}^{k}\|^{2}\] \[+2\sum_{k=0}^{K}\frac{(1-\theta)^{2}}{\theta}L_{\nabla\Phi}^{2} \alpha^{2}\mathbb{E}\|\bar{r}^{k}\|^{2}+2(1-\theta)^{2}\|\nabla\Phi\left(\bar{ x}^{0}\right)\|^{2}\] \[\leq 2\sum_{k=0}^{K}\theta\mathbb{E}\|\mathbb{E}_{k}\bar{u}^{k}- \nabla\Phi(\bar{x}^{k})\|^{2}+2\sum_{k=0}^{K-1}\left(1+2\frac{1-\theta}{ \theta}L_{\nabla\Phi}^{2}\alpha^{2}\right)(1-\theta)\theta^{2}\mathbb{E}\| \bar{u}^{k}-\mathbb{E}_{k}\bar{u}^{k}\|^{2}\] \[+2(1-\theta)^{2}\|\nabla\Phi\left(\bar{x}^{0}\right)\|^{2}+2\sum_{ k=0}^{K-1}\frac{(1-\theta)^{2}}{\theta}L_{\nabla\Phi}^{2}\alpha^{2}\left(2\mathbb{E} \|\mathbb{E}_{k}[\bar{r}^{k+1}]-\nabla\Phi(\bar{x}^{k})\|^{2}+2\mathbb{E}\| \nabla\Phi(\bar{x}^{k})\|^{2}\right),\]

where the last inequality uses (47).

(45) indicates that \(4\frac{1-\theta}{\theta}L_{\nabla\Phi}^{2}\alpha^{2}\leq\frac{\theta}{8}\). Subtracting

\[2\sum_{k=0}^{K-1}\frac{(1-\theta)^{2}}{\theta}L_{\nabla\Phi}^{2}\alpha^{2} \cdot 2\mathbb{E}\|\mathbb{E}_{k}[\bar{r}^{k+1}]-\nabla\Phi(\bar{x}^{k})\|^{2}\]from both sides of (50), we have:

\[\sum_{k=0}^{K}\theta\mathbb{E}\|\mathbb{E}_{k}\bar{r}^{k+1}-\nabla \Phi(\bar{x}^{k})\|^{2}\] \[\leq 4\sum_{k=0}^{K}\theta\mathbb{E}\|\mathbb{E}_{k}\bar{u}^{k}-\nabla \Phi(\bar{x}^{k})\|^{2}+8\sum_{k=0}^{K-1}(1-\theta)\theta^{2}\mathbb{E}\|\bar{ u}^{k}-\mathbb{E}_{k}\bar{u}^{k}\|^{2}+4(1-\theta)^{2}\|\nabla\Phi(\bar{x}^{0}) \|^{2}\] (51) \[+\frac{\theta}{4}\sum_{k=0}^{K-1}\mathbb{E}\|\nabla\Phi(\bar{x}^ {k})\|^{2}.\]

Substituting (47), (51) into (46), we get:

\[\sum_{k=0}^{K}\alpha\mathbb{E}\|\nabla\Phi(\bar{x}^{k})\|^{2}\] \[\leq 2(\Phi(\bar{x}^{0})-\inf\Phi)+\sum_{k=0}^{K}(\alpha+2L_{\nabla \Phi}\alpha^{2})\mathbb{E}\|\mathbb{E}_{k}[\bar{r}^{k+1}]-\nabla\Phi(\bar{x}^ {k})\|^{2}+\sum_{k=0}^{K}2L_{\nabla\Phi}\alpha^{2}\mathbb{E}\|\nabla\Phi(\bar {x}^{k})\|^{2}\] \[+\sum_{k=0}^{K}2L_{\nabla\Phi}\alpha^{2}\theta^{2}\mathbb{E}_{k} \|\bar{u}^{k}-\mathbb{E}_{k}\bar{u}^{k}\|^{2}\] \[\leq 2(\Phi(\bar{x}_{0})-\inf\Phi)+5\alpha\sum_{k=0}^{K}\mathbb{E}\| \mathbb{E}_{k}\bar{u}^{k}-\nabla\Phi(\bar{x}^{k})\|^{2}+5\frac{\alpha}{\theta} (1-\theta)^{2}\|\nabla\Phi\left(\bar{x}^{0}\right)\|^{2}\] \[+\sum_{k=0}^{K}\left(10\frac{\alpha}{\theta}(1-\theta)+2L_{\nabla \Phi}\alpha^{2}\right)\theta^{2}\mathbb{E}\|\bar{u}^{k}-\mathbb{E}_{k}\bar{u} ^{k}\|^{2}+\frac{\alpha}{2}\sum_{k=0}^{K-1}\mathbb{E}\|\nabla\Phi(\bar{x}^{k} )\|^{2},\] (52)

where the last inequality uses \(\alpha\leq\frac{1}{10L\nabla\Phi}\).

Subtracting \(\frac{\alpha}{2}\sum_{k=0}^{K-1}\mathbb{E}\|\nabla\Phi(\bar{x}^{k})\|^{2}\) from both sides of (52), and substituting (38), (39) into it, we get:

\[\frac{1}{2}\sum_{k=0}^{K}\alpha\mathbb{E}\|\nabla\Phi(\bar{x}^{k} )\|^{2}\] \[\leq 2(\Phi(\bar{x}^{0})-\inf\Phi)+100\alpha L^{2}\sum_{k=0}^{K} \mathbb{E}\left[\frac{\Delta_{k}}{n}+I_{k}\right]+5\frac{\alpha}{\theta}(1- \theta)^{2}\|\nabla\Phi\left(\bar{x}^{0}\right)\|^{2}\] \[+\frac{\left(10\frac{\alpha}{\theta}(1-\theta)+2L_{\nabla\Phi} \alpha^{2}\right)}{n^{2}}\theta^{2}\cdot 9\sigma_{g,2}^{2}\sum_{k=0}^{K}\left( \mathbb{E}\|\mathbf{z}^{k+1}-\bar{\mathbf{z}}^{k+1}\|^{2}+\mathbb{E}\|\bar{ \mathbf{z}}^{k+1}-\mathbf{z}_{\star}^{k+1}\|^{2}\right)\] \[+\frac{\left(10\frac{\alpha}{\theta}(1-\theta)+2L_{\nabla\Phi} \alpha^{2}\right)}{n^{2}}\theta^{2}\cdot 3(K+1)n\left(\sigma_{f,1}^{2}+3\sigma_{g,2}^{2 }\frac{L_{f,0}^{2}}{\mu_{g}^{2}}\right)\] \[\leq 2(\Phi(\bar{x}_{0})-\inf\Phi)+\left(100\alpha L^{2}+9\left(10 \alpha\theta(1-\theta)+2L_{\nabla\Phi}\alpha^{2}\theta^{2}\right)\frac{\sigma_ {g,2}^{2}}{n}\right)\sum_{k=0}^{K}\mathbb{E}\left[\frac{\Delta_{k}}{n}+I_{k}\right]\] \[+3(K+1)\left(10\alpha(1-\theta)+2L_{\nabla\Phi}\alpha^{2}\theta \right)\frac{\theta}{n}\left(\sigma_{f,1}^{2}+3\sigma_{g,2}^{2}\frac{L_{f,0}^ {2}}{\mu_{g}^{2}}\right)\] \[+5\frac{\alpha}{\theta}(1-\theta)^{2}\|\nabla\Phi\left(\bar{x}^{0 }\right)\|^{2}.\]Finally, multiplying \(\dfrac{2}{\alpha}\) on both sides, we get:

\[\sum_{k=0}^{K}\mathbb{E}\|\nabla\Phi(\bar{x}^{k})\|^{2}\lesssim \frac{\Phi(\bar{x}_{0})-\inf\Phi}{\alpha}+\left(L^{2}+\left(\theta (1-\theta)+L_{\nabla\Phi}\alpha\theta^{2}\right)\dfrac{\sigma_{g,2}^{2}}{n} \right)\sum_{k=0}^{K}\mathbb{E}\left[\dfrac{\Delta_{k}}{n}+I_{k}\right]\] \[+\frac{K+1}{n}\left(\theta(1-\theta)+L_{\nabla\Phi}\alpha\theta^ {2}\right)(\sigma_{f,1}^{2}+\kappa^{2}\sigma_{g,2}^{2})+\frac{(1-\theta)^{2}}{ \theta}\|\nabla\Phi(\bar{x}^{0})\|^{2}.\]

#### c.1.6 Descent lemmas for the lower- and auxiliary-level

The following lemmas present the error analysis of the estimation of \(y^{\star}(\bar{x}^{k})\) and \(z_{\star}^{k}\), i.e., the term \(I_{k}\):

**Lemma 9** (Estimation error of \(y^{\star}(x)\)).: _Suppose Assumptions 1- 4hold, and:_

\[\beta\leq\frac{\mu_{g}}{32L_{g,1}^{2}}.\] (53)

_Then we have the estimation error of \(y^{\star}\):_

\[\|\bar{y}^{0}-y^{\star}(\bar{x}^{0})\|^{2}+\sum_{k=0}^{K}\mathbb{ E}[\|\bar{y}^{k+1}-y^{\star}(\bar{x}^{k})\|^{2}]\] \[\leq \frac{4}{\beta\mu_{g}}\|\bar{y}^{0}-y^{\star}(\bar{x}^{0})\|^{2} +\sum_{k=1}^{K}\frac{6\alpha^{2}L_{y^{\star}}^{2}}{\beta^{2}\mu_{g}^{2}} \mathbb{E}\|\bar{r}^{k}\|^{2}+\sum_{k=1}^{K}\frac{6}{\mu_{g}^{2}}L_{g,1}^{2} \mathbb{E}\left[\frac{\|\mathbf{O}_{x}\|^{2}\|\hat{\mathbf{e}}_{x}^{k}\|^{2}+ \|\mathbf{O}_{y}\|^{2}\|\hat{\mathbf{e}}_{y}^{k}\|^{2}}{n}\right]\] \[+\frac{4K\beta\sigma_{g,1}^{2}}{n\mu_{g}},\]

_and_

\[\sum_{k=0}^{K}\mathbb{E}\left[\|\bar{y}^{k+1}-\bar{y}^{k}\|^{2}\right]\] \[\leq \frac{\beta^{2}L_{g,1}^{2}}{n}\left(4+\frac{48L_{g,1}^{2}}{\mu_{g }^{2}}\right)\sum_{k=1}^{K}\mathbb{E}\left(\|\mathbf{O}_{x}\|^{2}\|\hat{ \mathbf{e}}_{x}^{k}\|^{2}+\|\mathbf{O}_{y}\|^{2}\|\hat{\mathbf{e}}_{y}^{k}\|^ {2}\right)+\frac{48\alpha^{2}L_{g,1}^{2}}{\mu_{g}^{2}}L_{y^{\star}}^{2}\sum_{k =1}^{K}\mathbb{E}\|\bar{r}^{k}\|^{2}\] \[+\frac{3(K+1)\beta^{2}}{n}\sigma_{g,1}^{2}+\frac{32\beta L_{g,1}^ {2}}{\mu_{g}}\|\bar{y}^{0}-y^{\star}(\bar{x}^{0})\|^{2}.\] (54)

Proof.: For each \(k\geq 0\), due to the independence of samples, we have:

\[\widehat{\mathbb{E}}_{k}[\|\bar{y}^{k+1}-y^{\star}(\bar{x}^{k})\| ^{2}]=\widehat{\mathbb{E}}_{k}[\|\bar{y}^{k}-\beta\bar{v}^{k}-y^{\star}(\bar{ x}^{k})\|^{2}]\] \[= \widehat{\mathbb{E}}_{k}\left[\left\|\bar{y}^{k}-\beta\frac{1}{n} \sum_{i=1}^{n}\nabla_{2}g_{i}(x_{i}^{k},y_{i}^{k})-y^{\star}(\bar{x}^{k})+ \beta\frac{1}{n}\sum_{i=1}^{n}\left(\nabla_{2}g_{i}(x_{i}^{k},y_{i}^{k})-v_{i}^ {k}\right)\right\|^{2}\right]\] \[\leq \left\|\bar{y}^{k}-\beta\frac{1}{n}\sum_{i=1}^{n}\nabla_{2}g_{i} (\bar{x}^{k},\bar{y}^{k})-y^{\star}(\bar{x}^{k})+\beta\frac{1}{n}\sum_{i=1}^{n} (\nabla_{2}g_{i}(\bar{x}^{k},\bar{y}^{k})-\nabla_{2}g_{i}(x_{i}^{k},y_{i}^{k}) )\right\|^{2}+\beta^{2}\frac{\sigma_{g,1}^{2}}{n}.\]Then,

\[\widehat{\mathbb{E}}_{k}[\|\bar{y}^{k+1}-y^{\star}(\bar{x}^{k})\|^{2}]\] \[\leq \left(1+\frac{\beta\mu_{g}}{2}\right)\left\|\bar{y}^{k}-\beta\frac{ 1}{n}\sum_{i=1}^{n}\nabla_{2}g_{i}(\bar{x}^{k},\bar{y}^{k})-y^{\star}(\bar{x}^ {k})\right\|^{2}\] \[+\beta^{2}\left(1+\frac{2}{\beta\mu_{g}}\right)\left\|\frac{1}{n} \sum_{i=1}^{n}(\nabla_{2}g_{i}(\bar{x}^{k},\bar{y}^{k})-\nabla_{2}g_{i}(x^{k}_ {i},y^{k}_{i}))\right\|^{2}+\beta^{2}\frac{\sigma_{g,1}^{2}}{n}\] \[\leq \left(1+\frac{\beta\mu_{g}}{2}\right)(1-\beta\mu_{g})^{2}\|\bar{ y}^{k}-y^{\star}(\bar{x}^{k})\|^{2}\] \[+\beta^{2}\left(1+\frac{2}{\beta\mu_{g}}\right)L_{g,1}^{2}\left( \frac{\|\mathbf{x}^{k}-\bar{\mathbf{x}}^{k}\|^{2}}{n}+\frac{\|\mathbf{y}^{k}- \bar{\mathbf{y}}^{k}\|^{2}}{n}\right)+\beta^{2}\frac{\sigma_{g,1}^{2}}{n}\] \[\leq \left(1-\beta\mu_{g}\right)\left[\left(1+\frac{\beta\mu_{g}}{2} \right)\|\bar{y}^{k}-y^{\star}(\bar{x}^{k-1})\|^{2}+\left(1+\frac{2}{\beta\mu _{g}}\right)\|y^{\star}(\bar{x}^{k})-y^{\star}(\bar{x}^{k-1})\|^{2}\right]\] \[+\beta^{2}\left(1+\frac{2}{\beta\mu_{g}}\right)L_{g,1}^{2}\left( \frac{\|\mathbf{x}^{k}-\bar{\mathbf{x}}^{k}\|^{2}}{n}+\frac{\|\mathbf{y}^{k}- \bar{\mathbf{y}}^{k}\|^{2}}{n}\right)+\beta^{2}\frac{\sigma_{g,1}^{2}}{n}\] \[\leq \left(1-\frac{\beta\mu_{g}}{2}\right)\|\bar{y}^{k}-y^{\star}(\bar {x}^{k-1})\|^{2}+\frac{3}{\beta\mu_{g}}L_{y^{\star}}^{2}\|\bar{x}^{k}-\bar{x}^ {k-1}\|^{2}\] \[+\frac{3\beta}{\mu_{g}}L_{g,1}^{2}\left(\frac{\|\mathbf{x}^{k}- \bar{\mathbf{x}}^{k}\|^{2}}{n}+\frac{\|\mathbf{y}^{k}-\bar{\mathbf{y}}^{k}\|^ {2}}{n}\right)+\beta^{2}\frac{\sigma_{g,1}^{2}}{n},\]

where the first and the third inequality is due to the Jenson's inequality, the second inequality holds according to Lemma 2 and the fact that \(\beta\leq\frac{\mu_{g}}{32L_{g,1}^{2}}\leq\frac{1}{3(\mu_{g}+L_{g,1})}\), and the last inequality uses \(\beta\mu_{g}\leq\frac{1}{3}\). Taking the summation and expectation on the both sides, we get:

\[\sum_{k=0}^{K}\frac{\beta\mu_{g}}{2}\mathbb{E}[\|\bar{y}^{k}-y^{ \star}(\bar{x}^{k-1})\|^{2}]+\mathbb{E}[\|\bar{y}^{k+1}-y^{\star}(\bar{x}^{k}) \|^{2}]\] \[\leq \mathbb{E}\|\bar{y}^{0}-y^{\star}(\bar{x}^{0})\|^{2}+\sum_{k=0}^{ K}\mathbb{E}\left[\frac{3\alpha^{2}}{\beta\mu_{g}}L_{y^{\star}}^{2}\|\bar{r}^{k} \|^{2}+\frac{3\beta}{\mu_{g}}L_{g,1}^{2}\left(\frac{\|\mathbf{x}^{k}-\bar{ \mathbf{x}}^{k}\|^{2}}{n}+\frac{\|\mathbf{y}^{k}-\bar{\mathbf{y}}^{k}\|^{2}}{ n}\right)+\beta^{2}\frac{\sigma_{g,1}^{2}}{n}\right].\]

Using (36) and the fact that \(\mathbf{x}^{0},\mathbf{y}^{0}\) is consensual, it follows that:

\[\sum_{k=0}^{K+1}\frac{\beta\mu_{g}}{2}\mathbb{E}[\|\bar{y}^{k}-y^ {\star}(\bar{x}^{k-1})\|^{2}]\leq 2\|\bar{y}^{0}-y^{\star}(\bar{x}^{0})\|^{2}+\sum_{k=1}^{K}\frac{3 \alpha^{2}}{\beta\mu_{g}}L_{y^{\star}}^{2}\mathbb{E}\|\bar{r}^{k}\|^{2}\] \[+\sum_{k=1}^{K}\frac{3\beta}{\mu_{g}}L_{g,1}^{2}\mathbb{E}\left[ \frac{\|\mathbf{O}_{x}\|^{2}\|\hat{\mathbf{e}}_{x}^{k}\|^{2}+\|\mathbf{O}_{y} \|^{2}\|\hat{\mathbf{e}}_{y}^{k}\|^{2}}{n}\right]+\frac{2K\beta^{2}\sigma_{g,1 }^{2}}{n}.\] (55)

On the other hand,

\[\widehat{\mathbb{E}}_{k}\left[\|\bar{y}^{k+1}-\bar{y}^{k}\|^{2}\right]\] \[\leq \beta^{2}\left\|\frac{1}{n}\sum_{i=1}^{n}\nabla_{2}g_{i}(x^{k}_ {i},y^{k}_{i})\right\|^{2}+\frac{\beta^{2}}{n}\sigma_{g,1}^{2}\] \[\leq 2\beta^{2}\left(\left\|\frac{1}{n}\sum_{i=1}^{n}\left(\nabla_{2} g_{i}(x^{k}_{i},y^{k}_{i})-\nabla_{2}g_{i}(\bar{x}^{k},\bar{y}^{k})\right)\right\|^{2}+ \left\|\nabla_{2}g(\bar{x}^{k},\bar{y}^{k})-\nabla_{2}g(\bar{x}^{k},y^{\star}( \bar{x}^{k}))\right\|^{2}\right)\] \[+\frac{\beta^{2}}{n}\sigma_{g,1}^{2}\] \[\leq \frac{2\beta^{2}L_{g,1}^{2}}{n}\left(\|\mathbf{x}^{k}-\bar{ \mathbf{x}}^{k}\|^{2}+\|\mathbf{y}^{k}-\bar{\mathbf{y}}^{k}\|^{2}+2\|\bar{y}^{k +1}-\mathbf{y}^{\star}(\bar{x}^{k})\|^{2}+2\|\bar{y}^{k+1}-\bar{\mathbf{y}}^{k} \|^{2}\right)+\frac{\beta^{2}}{n}\sigma_{g,1}^{2},\]where the second inequality uses \(\nabla_{2}g(\bar{x}^{k},y^{\star}(\bar{x}^{k}))=0\).

Note that \(\beta^{2}\leq\frac{\mu_{g}^{2}}{32L_{g,1}^{2}}\leq\frac{1}{8L_{g,1}^{2}}\). Subtracting \(2\beta^{2}L_{g,1}^{2}\|\bar{y}^{k+1}-\bar{y}^{k}\|\) on both sides, and taking expectation and summation, we get:

\[\sum_{k=0}^{K}\mathbb{E}\left[\|\bar{y}^{k+1}-\bar{y}^{k}\|^{2}\right]\] \[\leq \sum_{k=0}^{K}\left[\frac{4\beta^{2}L_{g,1}^{2}}{n}\mathbb{E} \left(\|\mathbf{x}^{k}-\bar{\mathbf{x}}^{k}\|^{2}+\|\mathbf{y}^{k}-\bar{ \mathbf{y}}^{k}\|^{2}\right)+\frac{8\beta^{2}L_{g,1}^{2}}{n}\mathbb{E}\|\bar{y }^{k+1}-\mathbf{y}^{\star}(\bar{\mathbf{x}}^{k})\|^{2}+\frac{2\beta^{2}}{n} \sigma_{g,1}^{2}\right]\] \[\leq \frac{4\beta^{2}L_{g,1}^{2}}{n}\sum_{k=1}^{K}\mathbb{E}\left(\| \mathbf{O}_{x}\|^{2}\|\hat{\mathbf{e}}_{k}^{x}\|^{2}+\|\mathbf{O}_{y}\|^{2}\| \hat{\mathbf{e}}_{k}^{y}\|^{2}\right)+\frac{8\beta^{2}L_{g,1}^{2}}{n}\sum_{k=0 }^{K}\mathbb{E}\|\bar{\mathbf{y}}^{k+1}-\mathbf{y}^{\star}(\bar{\mathbf{x}}^{ k})\|^{2}\] \[+\frac{2(K+1)\beta^{2}}{n}\sigma_{g,1}^{2}\] \[\leq \frac{4\beta^{2}L_{g,1}^{2}}{n}\sum_{k=1}^{K}\mathbb{E}\left(\| \mathbf{O}_{x}\|^{2}\|\hat{\mathbf{e}}_{x}^{k}\|^{2}+\|\mathbf{O}_{y}\|^{2}\| \hat{\mathbf{e}}_{y}^{k}\|^{2}\right)+\frac{2(K+1)\beta^{2}}{n}\sigma_{g,1}^{2}\] \[+8\beta^{2}L_{g,1}^{2}\left(\frac{4}{\beta\mu_{g}}\|\bar{y}_{0}-y ^{\star}(\bar{x}^{0})\|^{2}+\sum_{k=1}^{K}\frac{6\alpha^{2}}{\beta^{2}\mu_{g} ^{2}}L_{y^{\star}}^{2}\mathbb{E}\|\bar{r}^{k}\|^{2}\right)\] \[+8\beta^{2}L_{g,1}^{2}\left(\sum_{k=1}^{K}\frac{6}{\mu_{g}^{2}}L_ {g,1}^{2}\mathbb{E}\left[\frac{\|\mathbf{O}_{x}\|^{2}\|\hat{\mathbf{e}}_{x}^{ k}\|^{2}+\|\mathbf{O}_{y}\|^{2}\|\hat{\mathbf{e}}_{y}^{k}\|^{2}}{n}\right]+\frac{4K \beta\sigma_{g,1}^{2}}{n\mu_{g}}\right)\] \[\leq \frac{\beta^{2}L_{g,1}^{2}}{n}\left(4+\frac{48L_{g,1}^{2}}{\mu_{ g}^{2}}\right)\sum_{k=1}^{K}\mathbb{E}\left(\|\mathbf{O}_{x}\|^{2}\|\hat{ \mathbf{e}}_{x}^{k}\|^{2}+\|\mathbf{O}_{y}\|^{2}\|\hat{\mathbf{e}}_{y}^{k}\|^ {2}\right)+\frac{48\alpha^{2}L_{g,1}^{2}}{\mu_{g}^{2}}L_{y^{\star}}^{2}\sum_{ k=1}^{K}\mathbb{E}\|\bar{r}^{k}\|^{2}\] \[+\frac{3(K+1)\beta^{2}}{n}\sigma_{g,1}^{2}+\frac{32\beta L_{g,1}^ {2}}{\mu_{g}}\|\bar{y}^{0}-y^{\star}(\bar{x}^{0})\|^{2}.\]

where the second inequality holds since \(\mathbf{x}^{0},\mathbf{y}^{0}\) are consensual, the third inequality uses (55), and the last inequality holds since \(\beta\leq\frac{\mu_{g}}{32L_{g,1}^{2}}\). 

**Lemma 10** (Estimation error of \(z^{\star}(x)\)).: _Suppose that Assumptions 1- 4hold, and_

\[\gamma<\min\left\{\frac{1}{\mu_{g}},\frac{nL_{g,1}^{2}}{\mu_{g}^{2}\sigma_{g,2} ^{2}},\frac{n\mu_{g}}{36\sigma_{g,2}^{2}}\right\}.\] (56)

_We have:_

\[\sum_{k=0}^{K+1}\mathbb{E}\|\bar{z}^{k}-z_{\star}^{k}\|^{2}\] \[\leq \sum_{k=0}^{K}\frac{9\alpha^{2}L_{z^{\star}}^{2}}{\gamma^{2}\mu_{ g}^{2}}\mathbb{E}\|\bar{r}^{k}\|^{2}\] \[+72\kappa^{2}\sum_{k=1}^{K}\mathbb{E}\left[\frac{\kappa^{2}\| \mathbf{O}_{x}\|^{2}\|\hat{\mathbf{e}}_{x}^{k}\|^{2}+\|\mathbf{O}_{z}\|^{2}\| \hat{\mathbf{e}}_{z}^{k}\|^{2}}{n}\right]+72\kappa^{2}\sum_{k=0}^{K}\mathbb{E} \left[\frac{\kappa^{2}\|\mathbf{O}_{y}\|^{2}\|\hat{\mathbf{e}}_{y}^{k+1}\|^{2 }}{n}\right]\] \[+\sum_{k=0}^{K}72\kappa^{4}\mathbb{E}\left[\|\bar{y}^{k+1}-y^{ \star}(\bar{x}^{k})\|^{2}\right]+\frac{3\|z_{\star}^{1}\|^{2}}{\mu_{g}\gamma}+ \frac{6(K+1)\gamma}{\mu_{g}n}\left(3\sigma_{g,2}^{2}\frac{L_{f,0}^{2}}{\mu_{g}^ {2}}+\sigma_{f,1}^{2}\right).\]

Proof.: For each \(k\geq 0\), note that \(z_{\star}^{k}=\nabla_{22}^{2}g(\bar{x}^{k},y^{\star}(\bar{x}^{k}))^{-1}\nabla_{ 2}f_{2}(\bar{x}^{k},y^{\star}(\bar{x}^{k}))\), we have:

\[\widetilde{\mathbb{E}}_{k}[\bar{z}^{k+1}]-z_{\star}^{k+1}=\bar{z}^{k}-\frac{ \gamma}{n}\sum_{i=1}^{n}\left(\nabla_{22}^{2}g_{i}(x_{i}^{k},y_{i}^{k+1})z_{i}^{ k}-\nabla_{2}f_{i}(x_{i}^{k},y_{i}^{k+1})\right)-z_{\star}^{k+1}\]\[= \left[I-\frac{\gamma}{n}\sum_{i=1}^{n}\nabla_{22}^{2}g_{i}(x_{i}^{k},y _{i}^{k+1})\right](\bar{z}^{k}-z_{\star}^{k+1})+\frac{\gamma}{n}\sum_{i=1}^{n} \left[\nabla_{2}f_{i}(x_{i}^{k},y_{i}^{k+1})-\nabla_{2}f_{i}(\bar{x}^{k},y^{ \star}(\bar{x}^{k}))\right]\] \[+\frac{\gamma}{n}\sum_{i=1}^{n}\nabla_{22}^{2}g_{i}(x_{i}^{k},y _{i}^{k+1})(\bar{z}^{k}-z_{i}^{k})+\frac{\gamma}{n}\sum_{i=1}^{n}\left[\nabla_{ 22}^{2}g_{i}(\bar{x}^{k},y^{\star}(\bar{x}^{k}))-\nabla_{22}^{2}g_{i}(x_{i}^{k },y_{i}^{k+1})\right]z_{\star}^{k+1}.\]

Then we have:

\[\left\|\widetilde{\mathbb{E}}_{k}[\bar{z}^{k+1}]-z_{\star}^{k+1} \right\|^{2}\] \[\leq (1+\gamma\mu_{g})\left\|\left[I-\frac{\gamma}{n}\sum_{i=1}^{n} \nabla_{22}^{2}g_{i}(x_{i}^{k},y_{i}^{k+1})\right](\bar{z}^{k}-z_{\star}^{k+1} )\right\|^{2}\] \[+3\gamma^{2}\left(1+\frac{1}{\gamma\mu_{g}}\right)\left\|\frac{1 }{n}\sum_{i=1}^{n}\left[\nabla_{2}f_{i}(x_{i}^{k},y_{i}^{k+1})-\nabla_{2}f_{i }(\bar{x}^{k},y^{\star}(\bar{x}^{k}))\right]\right\|^{2}\] \[+3\gamma^{2}\left(1+\frac{1}{\gamma\mu_{g}}\right)\left\|\frac{1 }{n}\sum_{i=1}^{n}\left[\nabla_{22}^{2}g_{i}(\bar{x}^{k},y^{\star}(\bar{x}^{k} ))-\nabla_{22}^{2}g_{i}(x_{i}^{k},y_{i}^{k+1})\right]z_{\star}^{k+1}\right\|^{2}\] \[+3\gamma^{2}\left(1+\frac{1}{\gamma\mu_{g}}\right)\left\|\frac{1 }{n}\sum_{i=1}^{n}\nabla_{22}^{2}g_{i}(x_{i}^{k},y_{i}^{k+1})(\bar{z}^{k}-z_{i} ^{k})\right\|^{2}\] \[\leq (1+\gamma\mu_{g})(1-\gamma\mu_{g})^{2}\|\bar{z}^{k}-z_{\star}^{k+ 1}\|^{2}\] \[+\frac{6\gamma}{\mu_{g}}\left(L_{g,1}^{2}\frac{\|\mathbf{z}^{k}- \bar{\mathbf{z}}^{k}\|}{n}+\left(\frac{L_{g,2}^{2}L_{f,0}^{2}}{\mu_{g}^{2}}+L_ {f,1}^{2}\right)\left(\frac{\|\mathbf{x}^{k}-\bar{\mathbf{x}}^{k}\|^{2}}{n}+ \frac{\|\mathbf{y}^{k+1}-\mathbf{y}^{\star}(\bar{x}^{k})\|^{2}}{n}\right)\right)\] \[\leq (1-\gamma\mu_{g})\left(1+\frac{\gamma\mu_{g}}{2}\right)\|\bar{z} ^{k}-z_{\star}^{k}\|^{2}+\left(1+\frac{2}{\gamma\mu_{g}}\right)\|z_{\star}^{k }-z_{\star}^{k+1}\|^{2}+\frac{6\gamma}{\mu_{g}}L_{g,1}^{2}\frac{\|\mathbf{z}^{k }-\bar{\mathbf{z}}^{k}\|}{n}\] \[+\frac{12\gamma}{\mu_{g}}\left(\frac{L_{g,2}^{2}L_{f,0}^{2}}{\mu_ {g}^{2}}+L_{f,1}^{2}\right)\left(\frac{\|\mathbf{x}^{k}-\bar{\mathbf{x}}^{k}\|^ {2}}{n}+\frac{\|\mathbf{y}^{k+1}-\bar{\mathbf{y}}^{k+1}\|^{2}}{n}+\|\bar{y}^{k +1}-y^{\star}(\bar{x}^{k})\|^{2}\right)\] \[\leq (1-\frac{\gamma\mu_{g}}{2})\|\bar{z}^{k}-z_{\star}^{k}\|^{2}+\frac {3\alpha^{2}L_{z\star}^{2}}{\gamma\mu_{g}}\|\bar{r}^{k}\|^{2}+\frac{6\gamma}{ \mu_{g}}L_{g,1}^{2}\frac{\|\mathbf{z}^{k}-\bar{\mathbf{z}}^{k}\|}{n}\] \[+\frac{12\gamma}{\mu_{g}}\left(\frac{L_{g,2}^{2}L_{f,0}^{2}}{\mu_ {g}^{2}}+L_{f,1}^{2}\right)\left(\frac{\|\mathbf{x}^{k}-\bar{\mathbf{x}}^{k}\|^ {2}}{n}+\frac{\|\mathbf{y}^{k+1}-\bar{\mathbf{y}}^{k+1}\|^{2}}{n}+\|\bar{y}^{k +1}-y^{\star}(\bar{x}^{k})\|^{2}\right)\]

where the first and third inequality uses Jensen's inequality and Cauchy Schwartz inequality, the second inequality holds due to Assumption 1 and \(\gamma\mu_{g}<1\), the last inequality holds since \(z^{\star}(x)\) is \(L_{z}\). Lipschitz continuous.

Moreover, the independence of samples implies that

\[\widetilde{\mathbb{E}}_{k}\left\|\bar{z}^{k+1}-\widetilde{\mathbb{E }}_{k}[\bar{z}^{k+1}]\right\|^{2} =\gamma^{2}\widetilde{\mathbb{E}}_{k}\left\|\frac{1}{n}\sum_{i=1}^{n}( H_{i}^{k}-\widetilde{\mathbb{E}}_{k}[H_{i}^{k}])z_{i}^{k}+\frac{1}{n}\sum_{i}(b_{i}^{k}- \widetilde{\mathbb{E}}_{k}[b_{i}^{k}])\right\|^{2}\] \[\leq\frac{2\gamma^{2}}{n}\left(\sigma_{g,2}^{2}\frac{\|\mathbf{z }^{k}\|^{2}}{n}+\sigma_{f,1}^{2}\right)\] \[\leq\frac{2\gamma^{2}}{n}\left(3\sigma_{g,2}^{2}\left(\frac{\| \mathbf{z}^{k}-\bar{\mathbf{z}}_{k}\|^{2}}{n}+\|\bar{z}^{k}-z_{\star}^{k}\|^ {2}+\frac{L_{f,0}^{2}}{\mu_{g}^{2}}\right)+\sigma_{f,1}^{2}\right).\]

As \(\gamma\) satisfies

\[\frac{6\sigma_{g,2}^{2}\gamma^{2}}{n}\leq\frac{6\gamma L_{g,1}^{2}}{\mu_{g}^{2}}, \quad\frac{6\sigma_{g,2}^{2}\gamma^{2}}{n}\leq\frac{\gamma\mu_{g}}{6},\]we get:

\[\widetilde{\mathbb{E}}_{k}[\|\bar{z}^{k+1}-z_{\star}^{k+1}\|^{2}]= \widetilde{\mathbb{E}}_{k}\|\widetilde{\mathbb{E}}_{k}[\bar{z}^{k+1}]-z_{\star}^ {k+1}\|^{2}+\widetilde{\mathbb{E}}_{k}\|\bar{z}^{k+1}-\widetilde{\mathbb{E}}_{ k}[\bar{z}^{k+1}]\|^{2}\] \[\leq \left(1-\frac{\gamma\mu_{g}}{3}\right)\|\bar{z}^{k}-z_{\star}^{k} \|^{2}+\frac{3\alpha^{2}L_{z^{*}}^{2}}{\gamma\mu_{g}}\|\bar{r}^{k}\|^{2}+ \frac{12\gamma}{\mu_{g}}L_{g,1}^{2}\frac{\|\mathbf{z}^{k}-\bar{\mathbf{z}}^{k} \|^{2}}{n}+\frac{2\gamma^{2}}{n}\left(3\sigma_{g,2}^{2}\frac{L_{f,0}^{2}}{\mu_ {g}^{2}}+\sigma_{f,1}^{2}\right)\] \[+\frac{12\gamma}{\mu_{g}}\left(\frac{L_{g,2}^{2}L_{f,0}^{2}}{\mu_ {g}^{2}}+L_{f,1}^{2}\right)\left(\frac{\|\mathbf{x}^{k}-\bar{\mathbf{x}}^{k} \|^{2}}{n}+\frac{\|\mathbf{y}^{k+1}-\bar{\mathbf{y}}^{k+1}\|^{2}}{n}+\|\bar{ \mathbf{y}}^{k+1}-y^{\star}(\bar{x}^{k})\|^{2}\right).\]

Taking expectation and summation on both sides, we get

\[\sum_{k=0}^{K}\frac{\gamma\mu_{g}}{3}\mathbb{E}\|\bar{z}^{k}-z_{ \star}^{k}\|^{2}+\mathbb{E}\|\bar{z}^{K+1}-z_{\star}^{K+1}\|^{2}\] \[\leq \mathbb{E}\|\bar{z}^{0}-z_{\star}^{0}\|^{2}+\sum_{k=0}^{K}\left[ \frac{3\alpha^{2}L_{z^{*}}^{2}}{\gamma\mu_{g}}\mathbb{E}\|\bar{r}^{k}\|^{2}+ \frac{12\gamma}{\mu_{g}}L_{g,1}^{2}\frac{\mathbb{E}\|\mathbf{z}^{k}-\bar{ \mathbf{z}}^{k}\|^{2}}{n}+\frac{2\gamma^{2}}{n}\left(3\sigma_{g,2}^{2}\frac{L _{f,0}^{2}}{\mu_{g}^{2}}+\sigma_{f,1}^{2}\right)\right]\] \[+\sum_{k=0}^{K}\frac{12\gamma}{\mu_{g}}\left(\frac{L_{g,2}^{2}L_ {f,0}^{2}}{\mu_{g}^{2}}+L_{f,1}^{2}\right)\mathbb{E}\left[\frac{\|\mathbf{x}^ {k}-\bar{\mathbf{x}}^{k}\|^{2}}{n}+\frac{\|\mathbf{y}^{k+1}-\bar{\mathbf{y}}^{ k+1}\|^{2}}{n}+\|\bar{y}^{k+1}-y^{\star}(\bar{x}^{k})\|^{2}\right].\]

It follows that

\[\sum_{k=0}^{K+1}\mathbb{E}\|\bar{z}^{k}-z_{\star}^{k}\|^{2}\] \[\leq \sum_{k=0}^{K}\frac{9\alpha^{2}L_{z^{*}}^{2}}{\gamma^{2}\mu_{g}^{ 2}}\mathbb{E}\|\bar{r}^{k}\|^{2}\] \[+72\kappa^{2}\sum_{k=1}^{K}\mathbb{E}\left[\frac{\kappa^{2}\| \mathbf{O}_{x}\|^{2}\|\hat{\mathbf{e}}_{x}^{k}\|^{2}+\|\mathbf{O}_{z}\|^{2}\| \hat{\mathbf{e}}_{z}^{k}\|^{2}}{n}\right]+72\kappa^{2}\sum_{k=0}^{K}\mathbb{E }\left[\frac{\kappa^{2}\|\mathbf{O}_{y}\|^{2}\|\hat{\mathbf{e}}_{y}^{k+1}\|^{2 }}{n}\right]\] \[+\sum_{k=0}^{K}72\kappa^{4}\mathbb{E}\left[\|\bar{y}^{k+1}-y^{ \star}(\bar{x}^{k})\|^{2}\right]+\frac{3\|z_{\star}^{1}\|^{2}}{\mu_{g}\gamma}+ \frac{6(K+1)\gamma}{\mu_{g}n}\left(3\sigma_{g,2}^{2}\frac{L_{f,0}^{2}}{\mu_{g} ^{2}}+\sigma_{f,1}^{2}\right),\]

since \(z_{\star}^{0}=z_{\star}^{1}\) and \(\mathbf{z}^{0}\) is consensual. 

Then, we combine the results in Lemmas 9, 10 and give an upper bound of \(\mathbb{E}[I_{k}]\):

**Lemma 11**.: _Suppose that Lemmas 9 and 10 hold. Then we have:_

\[\sum_{k=-1}^{K}\mathbb{E}[I_{k}]\leq \left(\frac{9\alpha^{2}L_{z^{*}}^{2}}{\gamma^{2}\mu_{g}^{2}}+ \frac{438\kappa^{4}\alpha^{2}}{\beta^{2}\mu_{g}^{2}}L_{y^{*}}^{2}\right)\sum_{k= 0}^{K}\mathbb{E}\|\bar{r}^{k}\|^{2}+510\kappa^{4}\sum_{k=0}^{K}\mathbb{E}\left[ \frac{\Delta_{k}}{n}\right]+\frac{3\|z_{\star}^{1}\|^{2}}{\mu_{g}\gamma}\] \[+\frac{6(K+1)\gamma}{\mu_{g}n}\left(3\sigma_{g,2}^{2}\frac{L_{f,0} ^{2}}{\mu_{g}^{2}}+\sigma_{f,1}^{2}\right)+73\kappa^{4}\left(\frac{4}{\beta\mu _{g}}\|\bar{y}^{0}-y^{\star}(\bar{x}^{0})\|^{2}+\frac{4K\sigma_{g,1}^{2}}{n \mu_{g}}\beta\right).\] (57)

**Remark 6**.: _Here \(I_{-1}=\|\bar{z}^{0}-z_{\star}^{0}\|^{2}+\kappa^{2}\|\bar{y}^{0}-y^{\star}(\bar {x}^{-1})\|^{2}\). The aim of introducing this term is to simplify the subsequent proofs of other lemmas._Proof.: Lemma 10 implies that:

\[\sum_{k=0}^{K+1}\mathbb{E}\|\bar{z}^{k}-z_{\star}^{k}\|^{2}\] \[\leq \sum_{k=0}^{K}\frac{9\alpha^{2}L_{z\star}^{2}}{\gamma^{2}\mu_{g}^{ 2}}\mathbb{E}\|\bar{r}^{k}\|^{2}\] \[+72\kappa^{2}\sum_{k=1}^{K}\mathbb{E}\left[\frac{\kappa^{2}\| \mathbf{O}_{x}\|^{2}\|\hat{\mathbf{e}}_{x}^{k}\|^{2}+\|\mathbf{O}_{z}\|^{2}\| \hat{\mathbf{e}}_{z}^{k}\|^{2}}{n}\right]+72\kappa^{2}\sum_{k=0}^{K}\mathbb{E} \left[\frac{\kappa^{2}\|\mathbf{O}_{y}\|^{2}\|\hat{\mathbf{e}}_{y}^{k+1}\|^{2 }}{n}\right]\] \[+\sum_{k=0}^{K}72\kappa^{4}\mathbb{E}\left[\|\bar{y}^{k+1}-y^{ \star}(\bar{x}^{k})\|^{2}\right]+\frac{3\|z_{\star}^{1}\|^{2}}{\mu_{g}\gamma}+ \frac{6(K+1)\gamma}{\mu_{g}n}\left(3\sigma_{g,2}^{2}\frac{L_{f,0}^{2}}{\mu_{g} ^{2}}+\sigma_{f,1}^{2}\right)\]

Then, using Lemma 9, we have:

\[\sum_{k=-1}^{K}\mathbb{E}[I_{k}]=\sum_{k=0}^{K+1}\mathbb{E}\|\bar{ z}^{k}-z_{\star}^{k}\|^{2}+\kappa^{2}\sum_{k=0}^{K+1}\mathbb{E}\|\bar{y}^{k}-y^{ \star}(\bar{x}^{k-1})\|^{2}\] \[\leq \sum_{k=0}^{K}\frac{9\alpha^{2}L_{z\star}^{2}}{\gamma^{2}\mu_{g}^ {2}}\mathbb{E}\|\bar{r}^{k}\|^{2}\] \[+72\kappa^{2}\sum_{k=1}^{K}\mathbb{E}\left[\frac{\kappa^{2}\| \mathbf{O}_{x}\|^{2}\|\hat{\mathbf{e}}_{x}^{k}\|^{2}+\|\mathbf{O}_{z}\|^{2}\| \hat{\mathbf{e}}_{z}^{k}\|^{2}}{n}\right]+72\kappa^{2}\sum_{k=0}^{K}\mathbb{E} \left[\frac{\kappa^{2}\|\mathbf{O}_{y}\|^{2}\|\hat{\mathbf{e}}_{y}^{k+1}\|^{2 }}{n}\right]\] \[+\frac{6(K+1)\gamma}{\mu_{g}n}\left(3\sigma_{g,2}^{2}\frac{L_{f,0 }^{2}}{\mu_{g}^{2}}+\sigma_{f,1}^{2}\right)+73\kappa^{4}\left[\frac{4}{\beta \mu_{g}}\|\bar{y}^{0}-y^{\star}(\bar{x}^{0})\|^{2}+\sum_{k=1}^{K}\frac{6\alpha ^{2}}{\beta^{2}\mu_{g}^{2}}L_{y^{\star}}^{2}.\mathbb{E}\|\bar{r}^{k}\|^{2}\right]\] \[+\frac{3\|z_{\star}^{1}\|^{2}}{\mu_{g}\gamma}+73\kappa^{4}\left[ \sum_{k=1}^{K}\frac{6}{\mu_{g}^{2}}L_{g,1}^{2}\mathbb{E}\left[\frac{\|\mathbf{ O}_{x}\|^{2}\|\hat{\mathbf{e}}_{x}^{k}\|^{2}+\|\mathbf{O}_{y}\|^{2}\|\hat{ \mathbf{e}}_{y}^{k}\|^{2}}{n}\right]+\frac{4K\sigma_{g,1}^{2}}{n\mu_{g}}\beta\right]\] \[\leq \sum_{k=0}^{K}\left(\frac{9\alpha^{2}L_{z\star}^{2}}{\gamma^{2} \mu_{g}^{2}}+\frac{438\kappa^{4}\alpha^{2}}{\beta^{2}\mu_{g}^{2}}L_{y^{\star} }^{2}\right)\mathbb{E}\|\bar{r}^{k}\|^{2}+\sum_{k=0}^{K}510\kappa^{4}\mathbb{E }\left[\frac{\Delta_{k}}{n}\right]+\frac{3\|z_{\star}^{1}\|^{2}}{\mu_{g}\gamma}\] \[+\frac{6(K+1)\gamma}{\mu_{g}n}\left(3\sigma_{g,2}^{2}\frac{L_{f,0 }^{2}}{\mu_{g}^{2}}+\sigma_{f,1}^{2}\right)+73\kappa^{4}\left(\frac{4}{\beta \mu_{g}}\|\bar{y}^{0}-y^{\star}(\bar{x}^{0})\|^{2}+\frac{4K\sigma_{g,1}^{2}}{ n\mu_{g}}\beta\right).\]

#### c.1.7 Consensus error analysis

In this subsection we aim to bound the consensus errors of \(y,z,x\) (i.e. the terms \(\|\hat{\mathbf{e}}_{y}^{k}\|^{2}\), \(\|\hat{\mathbf{e}}_{z}^{k}\|^{2}\), and \(\|\hat{\mathbf{e}}_{x}^{k}\|^{2}\)).

**Lemma 12** (Consensus error of \(y\)).: _Suppose that Assumptions 1- 4 hold, and_

\[\beta^{2}\leq\frac{(1-\|\mathbf{\Gamma}_{y}\|)^{2}}{8L_{g,1}^{2}\|\mathbf{O}_{ y}^{-1}\|^{2}\|\mathbf{O}_{y}\|^{2}\|\mathbf{\Lambda}_{ya}\|^{2}}.\] (58)

_We have_

\[\sum_{k=0}^{K+1}\mathbb{E}\|\hat{\mathbf{e}}_{y}^{k}\|^{2}\leq 3\sum_{k=0}^{K}\frac{\beta^{2}\|\mathbf{O}_{y}^{-1}\|^{2}}{(1-\| \mathbf{\Gamma}_{y}\|)^{2}}\|\mathbf{\Lambda}_{yb}^{-1}\|^{2}\|\mathbf{ \Lambda}_{ya}\|^{2}L_{g,1}^{2}\mathbb{E}\left[\|\bar{\mathbf{x}}^{k+1}-\bar{ \mathbf{x}}^{k}\|^{2}+\|\bar{\mathbf{y}}^{k+1}-\bar{\mathbf{y}}^{k}\|^{2}\right]\] (59) \[+\frac{\|\mathbf{O}_{x}\|^{2}}{3\|\mathbf{O}_{y}\|^{2}}\sum_{k=0}^ {K}\mathbb{E}\|\hat{\mathbf{e}}_{x}^{k}\|^{2}+\frac{3(K+1)\beta^{2}\|\mathbf{O}_{ y}^{-1}\|^{2}\|\mathbf{\Lambda}_{ya}\|^{2}}{1-\|\mathbf{\Gamma}_{y}\|}n\sigma_{g,1}^{2}+ \frac{2\mathbb{E}\|\hat{\mathbf{e}}_{y}^{0}\|^{2}}{1-\|\mathbf{\Gamma}_{y}\|}.\]

[MISSING_PAGE_EMPTY:36]

Next, for the last term, we have:

\[\widehat{\mathbb{E}}_{k}\left\langle\mathbf{O}_{y}^{-1}\left[ \begin{array}{c}\mathbf{\Lambda}_{ya}\hat{\mathbf{U}}_{y}^{\top}\left[\widehat{ \mathbb{E}}_{k}[\mathbf{v}^{k}]-\nabla_{2}\mathbf{g}(\bar{\mathbf{x}}^{k},\bar {\mathbf{y}}^{k})\right]\\ \mathbf{\Lambda}_{yb}^{-1}\mathbf{\Lambda}_{ya}\hat{\mathbf{U}}_{y}^{\top} \left[\nabla_{2}\mathbf{g}(\bar{\mathbf{x}}^{k+1},\bar{\mathbf{y}}^{k+1})- \nabla_{2}\mathbf{g}(\bar{\mathbf{x}}^{k},\bar{\mathbf{y}}^{k})\right]\end{array} \right],\mathbf{O}_{y}^{-1}\left[\begin{array}{c}\mathbf{\Lambda}_{ya}\hat{ \mathbf{U}}_{y}^{\top}\left[\mathbf{v}^{k}-\widehat{\mathbb{E}}_{k}[\mathbf{v }^{k}]\right]\\ \mathbf{0}\end{array}\right]\right\rangle\] \[\leq \frac{1}{2}\|\mathbf{O}_{y}^{-1}\|^{2}\widehat{\mathbb{E}}_{k} \left\|\left[\begin{array}{c}\mathbf{\Lambda}_{ya}\hat{\mathbf{U}}_{y}^{\top }\left[\widehat{\mathbb{E}}_{k}[\mathbf{v}^{k}]-\nabla_{2}\mathbf{g}(\bar{ \mathbf{x}}^{k},\bar{\mathbf{y}}^{k})\right]\\ \mathbf{\Lambda}_{ya}^{-1}\mathbf{\Lambda}_{ya}\hat{\mathbf{U}}_{y}^{\top} \left[\nabla_{2}\mathbf{g}(\bar{\mathbf{x}}^{k+1},\bar{\mathbf{y}}^{k+1})- \nabla_{2}\mathbf{g}(\bar{\mathbf{x}}^{k},\bar{\mathbf{y}}^{k})\right]\end{array} \right]\right\|^{2}\] \[+\frac{1}{2}\|\mathbf{O}_{y}^{-1}\|^{2}\widehat{\mathbb{E}}_{k} \left\|\left[\begin{array}{c}\mathbf{\Lambda}_{ya}\hat{\mathbf{U}}_{y}^{\top }\left[\mathbf{v}^{k}-\widehat{\mathbb{E}}_{k}[\mathbf{v}^{k}]\right]\\ \mathbf{0}\end{array}\right]\right\|^{2}\] \[\leq \frac{1}{2}\beta^{2}\|\mathbf{O}_{y}^{-1}\|^{2}\|\mathbf{\Lambda} _{ya}\|^{2}L_{g,1}^{2}\widehat{\mathbb{E}}_{k}\left[\|\mathbf{x}^{k}-\bar{ \mathbf{x}}^{k}\|^{2}+\|\mathbf{y}^{k}-\bar{\mathbf{y}}^{k}\|^{2}\right]\] \[+\frac{1}{2}\beta^{2}\|\mathbf{O}_{y}^{-1}\|^{2}\|\mathbf{\Lambda} _{ya}\|^{2}n\sigma_{g,1}^{2}.\] (64)

Taking expectations on both sides of (60), and plugging (61), (62), (63), (64) into it, we obtain:

\[\mathbb{E}\left[\|\hat{\mathbf{e}}_{y}^{k+1}\|^{2}\right]\] \[\leq 2\frac{\beta^{2}\|\mathbf{O}_{y}^{-1}\|^{2}}{1-\|\mathbf{\Gamma} _{y}\|}\|\mathbf{\Lambda}_{yb}^{-1}\|^{2}\|\mathbf{\Lambda}_{ya}\|^{2}L_{g,1}^{ 2}\mathbb{E}\left[\|\bar{\mathbf{x}}^{k+1}-\bar{\mathbf{x}}^{k}\|^{2}+\|\bar{ \mathbf{y}}^{k+1}-\bar{\mathbf{y}}^{k}\|^{2}\right]+\|\mathbf{\Gamma}_{y}\| \mathbb{E}\|\hat{\mathbf{e}}_{y}^{k}\|^{2}\] \[+2\frac{\beta^{2}\|\mathbf{O}_{y}^{-1}\|^{2}}{1-\|\mathbf{\Gamma} _{y}\|}\|\mathbf{\Lambda}_{ya}\|^{2}L_{g,1}^{2}\mathbb{E}\left[\|\mathbf{x}^{ k}-\bar{\mathbf{x}}^{k}\|^{2}+\|\mathbf{y}^{k}-\bar{\mathbf{y}}^{k}\|^{2} \right]+2\beta^{2}\|\mathbf{O}_{y}^{-1}\|^{2}\|\mathbf{\Lambda}_{ya}\|^{2}n \sigma_{g,1}^{2}.\]

Taking summation over \(k\) and using \(\|\mathbf{x}^{k}-\bar{\mathbf{x}}^{k}\|^{2}\leq\|\mathbf{O}_{x}\|^{2}\|\hat{ \mathbf{e}}_{x}^{k}\|^{2}\), \(\|\mathbf{y}^{k}-\bar{\mathbf{y}}^{k}\|^{2}\leq\|\mathbf{O}_{y}\|^{2}\|\hat{ \mathbf{e}}_{y}^{k}\|^{2}\), we get:

\[(1-\|\mathbf{\Gamma}_{y}\|)\sum_{k=0}^{K}\mathbb{E}\left[\|\hat{ \mathbf{e}}_{y}^{k}\|^{2}\right]\] \[\leq 2\sum_{k=0}^{K}\frac{\beta^{2}\|\mathbf{O}_{y}^{-1}\|^{2}}{1- \|\mathbf{\Gamma}_{y}\|}\|\mathbf{\Lambda}_{yb}^{-1}\|^{2}\|\mathbf{\Lambda}_{ ya}\|^{2}L_{g,1}^{2}\mathbb{E}\left[\|\bar{\mathbf{x}}^{k+1}-\bar{\mathbf{x}}^{k}\|^{2}+ \|\bar{\mathbf{y}}^{k+1}-\bar{\mathbf{y}}^{k}\|^{2}\right]\] \[+\mathbb{E}\|\hat{\mathbf{e}}_{y}^{0}\|^{2}-\mathbb{E}\|\hat{ \mathbf{e}}_{y}^{k+1}\|^{2}+2\sum_{k=0}^{K}\frac{\beta^{2}\|\mathbf{O}_{y}^{-1} \|^{2}}{1-\|\mathbf{\Gamma}_{y}\|}\|\|\mathbf{\Lambda}_{ya}\|^{2}L_{g,1}^{2} \mathbb{E}\left[\|\mathbf{O}_{x}\|^{2}\|\hat{\mathbf{e}}_{x}^{k}\|^{2}+\|\mathbf{ O}_{y}\|^{2}\|\hat{\mathbf{e}}_{y}^{k}\|^{2}\right]\] \[+2(K+1)\beta^{2}\|\mathbf{O}_{y}^{-1}\|^{2}\|\mathbf{\Lambda}_{ya} \|^{2}n\sigma_{g,1}^{2}\] \[\leq 2\sum_{k=0}^{K}\frac{\beta^{2}\|\mathbf{O}_{y}^{-1}\|^{2}}{1- \|\mathbf{\Gamma}_{y}\|}\|\mathbf{\Lambda}_{yb}^{-1}\|^{2}\|\mathbf{\Lambda}_{ya} \|^{2}L_{g,1}^{2}\mathbb{E}\left[\|\bar{\mathbf{x}}^{k+1}-\bar{\mathbf{x}}^{k}\|^{2}+ \|\bar{\mathbf{y}}^{k+1}-\bar{\mathbf{y}}^{k}\|^{2}\right]\] \[+\mathbb{E}\|\hat{\mathbf{e}}_{y}^{0}\|^{2}-\mathbb{E}\|\hat{ \mathbf{e}}_{y}^{k+1}\|^{2}+\frac{1-\|\mathbf{\Gamma}_{y}\|}{4\|\mathbf{O}_{y}\| ^{2}}\sum_{k=0}^{K}\mathbb{E}\left[\|\mathbf{O}_{x}\|^{2}\|\hat{\mathbf{e}}_{x}^{k} \|^{2}+\|\mathbf{O}_{y}\|^{2}\|\hat{\mathbf{e}}_{y}^{k}\|^{2}\right]\] \[+2(K+1)\beta^{2}\|\mathbf{O}_{y}^{-1}\|^{2}\|\mathbf{\Lambda}_{ya} \|^{2}n\sigma_{g,1}^{2},\]

where the last inequality uses \(\beta^{2}\leq\frac{(1-\|\mathbf{\Gamma}_{y}\|)^{2}}{8L_{g,1}^{2}\|\mathbf{O}_{y}^{- 1}\|^{2}\|\mathbf{\Lambda}_{ya}\|^{2}}\).

It follows that

\[\sum_{k=0}^{K+1}\mathbb{E}\left[\|\hat{\mathbf{e}}_{y}^{k}\|^{2}\right]\leq 3\sum_{k=0}^{K}\frac{\beta^{2}\|\mathbf{O}_{y}^{-1}\|^{2}}{(1-\| \mathbf{\Gamma}_{y}\|)^{2}}\|\mathbf{\Lambda}_{yb}^{-1}\|^{2}\|\mathbf{\Lambda}_{ya }\|^{2}L_{g,1}^{2}\mathbb{E}\left[\|\bar{\mathbf{x}}^{k+1}-\bar{\mathbf{x}}^{k}\|^{2}+ \|\bar{\mathbf{y}}^{k+1}-\bar{\mathbf{y}}^{k}\|^{2}\right]\] \[+\frac{\|\mathbf{O}_{x}\|^{2}}{3\|\mathbf{O}_{y}\|^{2}}\sum_{k=0}^{K} \mathbb{E}\left[\|\hat{\mathbf{e}}_{x}^{k}\|^{2}\right]+\frac{3(K+1)\beta^{2} \|\mathbf{O}_{y}^{-1}\|^{2}\|\mathbf{\Lambda}_{ya}\|^{2}}{1-\|\mathbf{\Gamma}_{y}\| }n\sigma_{g,1}^{2}+\frac{

**Lemma 13** (Consensus error of \(z\)).: _Suppose that Assumptions 1- 4 hold, and \(\gamma\) satisfies_

\[\frac{6\gamma^{2}\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{O}_{z}\|^{2}\|\bm{\Lambda} _{za}\|^{2}}{1-\|\bm{\Gamma}_{z}\|}\cdot(2L^{2}+2(1-\|\bm{\Gamma}_{z}\|)\sigma_ {g,2}^{2})\leq\frac{1-\|\bm{\Gamma}_{z}\|}{4}.\] (65)

_We have_

\[\sum_{k=0}^{K+1}\mathbb{E}\left[\|\hat{\mathbf{e}}_{z}^{k}\|^{2}\right]\] (66) \[\leq \frac{16\gamma^{2}(L_{g,1}^{2}+(1-\|\bm{\Gamma}_{z}\|)\sigma_{g,2 }^{2})\|\mathbf{O}_{z}^{-1}\|^{2}\|\bm{\Lambda}_{za}\|^{2}}{(1-\|\bm{\Gamma}_{ z}\|)^{2}}\sum_{k=0}^{K}\mathbb{E}\left[\|\bar{\mathbf{z}}^{k}-\mathbf{z}_{*}^{k}\|^{ 2}\right]+\frac{2\mathbb{E}[\|\hat{\mathbf{e}}_{z}^{0}\|^{2}]}{1-\|\bm{\Gamma }_{z}\|}\] \[+\frac{8\gamma^{2}\|\mathbf{O}_{z}^{-1}\|^{2}\|\bm{\Lambda}_{zb}^ {-1}\|^{2}\|\bm{\Lambda}_{za}\|^{2}}{(1-\|\bm{\Gamma}_{z}\|)^{2}}\sum_{k=0}^{K }\mathbb{E}\left[\left(L_{f,1}^{2}+L_{g,2}^{2}\frac{L_{f,0}^{2}}{\mu_{g}^{2}} \right)\|\bar{\mathbf{x}}^{k+1}-\bar{\mathbf{x}}^{k}\|^{2}\right]\] \[+16(K+1)n\gamma^{2}\frac{\|\mathbf{O}_{z}^{-1}\|^{2}\|\bm{\Lambda }_{za}\|^{2}}{1-\|\bm{\Gamma}_{z}\|}\left(\frac{L_{f,0}^{2}}{\mu_{g}^{2}} \sigma_{g,2}^{2}+\sigma_{f,1}^{2}\right)\] \[+\frac{\kappa^{2}}{3\|\mathbf{O}_{z}\|^{2}}\sum_{k=0}^{K}\mathbb{ E}(\|\mathbf{O}_{x}\|^{2}\|\hat{\mathbf{e}}_{x}^{k}\|^{2}+\|\mathbf{O}_{y}\|^{2}\| \hat{\mathbf{e}}_{y}^{k+1}\|^{2}).\]

Proof.: Firstly, Eq. (34) implies that:

\[\hat{\mathbf{e}}_{z}^{k+1}= \bm{\Gamma}_{z}\hat{\mathbf{e}}_{z}^{k}-\gamma\mathbf{O}_{z}^{-1} \left[\begin{array}{c}\bm{\Lambda}_{za}\hat{\mathbf{U}}_{z}^{\top}\left[ \widetilde{\mathbb{E}}_{k}[\mathbf{p}^{k}]-\mathbf{p}^{k}(\bar{\mathbf{x}}^{k },\bar{\mathbf{y}}^{k+1})\right]\\ \bm{\Lambda}_{zb}^{-1}\bm{\Lambda}_{za}\hat{\mathbf{U}}_{z}^{\top}\left[ \mathbf{p}^{k+1}(\bar{\mathbf{x}}^{k+1},\bar{\mathbf{y}}^{k+2})-\mathbf{p}^{k }(\bar{\mathbf{x}}^{k},\bar{\mathbf{y}}^{k+1})\right]\end{array}\right]\] \[+\gamma\mathbf{O}_{z}^{-1}\left[\begin{array}{c}\bm{\Lambda}_{za }\hat{\mathbf{U}}_{z}^{\top}\left[\widetilde{\mathbb{E}}_{k}[\mathbf{p}^{k}]- \mathbf{p}^{k}\right]\\ 0\end{array}\right].\]

Then using Cauchy Schwartz inequality, we get

\[\|\hat{\mathbf{e}}_{z}^{k+1}\|^{2}\] (67) \[\leq \left\|\bm{\Gamma}_{z}\hat{\mathbf{e}}_{z}^{k}-\gamma\mathbf{O}_{z }^{-1}\left[\begin{array}{c}\bm{\Lambda}_{za}\hat{\mathbf{U}}_{z}^{\top} \left[\widetilde{\mathbb{E}}_{k}[\mathbf{p}^{k}]-\mathbf{p}^{k}(\bar{\mathbf{x} }^{k},\bar{\mathbf{y}}^{k+1})\right]\\ \bm{\Lambda}_{zb}^{-1}\bm{\Lambda}_{za}\hat{\mathbf{U}}_{z}^{\top}\left[ \mathbf{p}^{k+1}(\bar{\mathbf{x}}^{k+1},\bar{\mathbf{y}}^{k+2})-\mathbf{p}^{k }(\bar{\mathbf{x}}^{k},\bar{\mathbf{y}}^{k+1})\right]\end{array}\right]\right\| ^{2}\] \[+\gamma^{2}\left\|\mathbf{O}_{z}^{-1}\left[\begin{array}{c}\bm{ \Lambda}_{za}\hat{\mathbf{U}}_{z}^{\top}\left[\widetilde{\mathbb{E}}_{k}[ \mathbf{p}^{k}]-\mathbf{p}^{k}\right]\\ 0\end{array}\right]\right\|^{2}\] \[+\gamma^{2}\left\|\mathbf{O}_{z}^{-1}\left[\begin{array}{c}\bm {\Lambda}_{za}\hat{\mathbf{U}}_{z}^{\top}\left[\widetilde{\mathbb{E}}_{k}[ \mathbf{p}^{k}]-\mathbf{p}^{k}(\bar{\mathbf{x}}^{k},\bar{\mathbf{y}}^{k+1}) \right]\\ \bm{\Lambda}_{zb}^{-1}\bm{\Lambda}_{za}\hat{\mathbf{U}}_{z}^{\top}\left[ \mathbf{p}^{k+1}(\bar{\mathbf{x}}^{k+1},\bar{\mathbf{y}}^{k+2})-\mathbf{p}^{k }(\bar{\mathbf{x}}^{k},\bar{\mathbf{y}}^{k+1})\right]\end{array}\right]\right\| ^{2}\] \[+\gamma^{2}\left\|\mathbf{O}_{z}^{-1}\left[\begin{array}{c}\bm {\Lambda}_{za}\hat{\mathbf{U}}_{z}^{\top}\left[\widetilde{\mathbb{E}}_{k}[ \mathbf{p}^{k}]-\mathbf{p}^{k}\right]\\ 0\end{array}\right]\right\|^{2}\]To obtain the upper bound of the right-hand side of the above equation, we first estimate some individual terms in it as follows. Note that:

\[\widetilde{\mathbb{E}}_{k}\|\widetilde{\mathbb{E}}_{k}[\mathbf{p}^{k }]-\mathbf{p}^{k}(\bar{\mathbf{x}}^{k},\bar{\mathbf{y}}^{k+1})\|^{2}\] \[= \sum_{i=1}^{n}\widetilde{\mathbb{E}}_{k}\left\|\nabla_{22}^{2}g_{i }(x_{i}^{k},y_{i}^{k+1})z_{i}^{k}-\nabla_{2}f_{i}(x_{i}^{k},y_{i}^{k+1})- \left(\nabla_{22}^{2}g_{i}(\bar{x}^{k},\bar{y}^{k+1})z_{k}^{*}-\nabla_{2}f_{i} (\bar{x}^{k},\bar{y}^{k+1})\right)\right\|^{2}\] \[\leq 3\sum_{i=1}^{n}\widetilde{\mathbb{E}}_{k}\left\|\nabla_{22}^{2}g _{i}(x_{i}^{k},y_{i}^{k+1})(z_{i}^{k}-z_{\star}^{k})\right\|^{2}+3\sum_{i=1}^ {n}\widetilde{\mathbb{E}}_{k}\left\|(\nabla_{22}^{2}g_{i}(x_{i}^{k},y_{i}^{k+ 1})-\nabla_{22}^{2}g_{i}(\bar{x}^{k},\bar{y}^{k+1}))z_{\star}^{k}\right\|^{2}\] \[+3\sum_{i=1}^{n}\widetilde{\mathbb{E}}_{k}\left\|\nabla_{2}f_{i} (\bar{x}^{k},\bar{y}^{k+1})-\nabla_{2}f_{i}(x_{i}^{k},y_{i}^{k+1})\right\|^{2}\] \[\leq 6L_{g,1}^{2}(\|\mathbf{z}^{k}-\bar{\mathbf{z}}^{k}\|^{2}+\|\bar {\mathbf{z}}^{k}-\mathbf{z}_{\star}^{k}\|^{2})+3\left(L_{g,2}^{2}\frac{L_{f,0 }^{2}}{\mu_{g}^{2}}+L_{f,1}^{2}\right)\left(\|\mathbf{x}^{k}-\bar{\mathbf{x}}^ {k}\|^{2}+\|\mathbf{y}^{k+1}-\bar{\mathbf{y}}^{k+1}\|^{2}\right)\] (68)

and

\[\widetilde{\mathbb{E}}_{k}\|\mathbf{p}^{k+1}(\bar{\mathbf{x}}^{k +1},\bar{\mathbf{y}}^{k+2})-\mathbf{p}^{k}(\bar{\mathbf{x}}^{k},\bar{\mathbf{ y}}^{k+1})\|^{2}\] \[= \sum_{i=1}^{n}\widetilde{\mathbb{E}}_{k}\|\nabla_{22}^{2}g_{i}( \bar{x}^{k+1},\bar{y}^{k+2})z_{\star}^{k+1}-\nabla_{2}f_{i}(\bar{x}^{k+1},\bar {y}^{k+2})-\nabla_{22}^{2}g_{i}(\bar{x}^{k},\bar{y}^{k+1})z_{\star}^{k}+\nabla _{2}f_{i}(\bar{x}^{k},\bar{y}^{k+1})\|^{2}\] \[\leq 3\sum_{i=1}^{n}\widetilde{\mathbb{E}}_{k}\|(\nabla_{22}^{2}g_{i }(\bar{x}^{k+1},\bar{y}^{k+2})-\nabla_{22}^{2}g_{i}(\bar{x}^{k},\bar{y}^{k+1}) )z_{\star}^{k+1}\|^{2}\] \[+3\sum_{i=1}^{n}\widetilde{\mathbb{E}}_{k}\|\nabla_{22}^{2}g_{i}( \bar{x}^{k},\bar{y}^{k+1})(z_{\star}^{k+1}-z_{\star}^{k})\|^{2}+3\sum_{i=1}^{n }\widetilde{\mathbb{E}}_{k}\|\nabla_{2}f_{i}(\bar{x}^{k+1},\bar{y}^{k+2})- \nabla_{2}f_{i}(\bar{x}^{k},\bar{y}^{k+1})\|^{2}\] \[\leq 3\widetilde{\mathbb{E}}_{k}\left[\left(L_{f,1}^{2}+L_{g,2}^{2} \frac{L_{f,0}^{2}}{\mu_{g}^{2}}\right)(\|\bar{\mathbf{x}}^{k+1}-\bar{\mathbf{ x}}^{k}\|^{2}+\|\bar{\mathbf{y}}^{k+2}-\bar{\mathbf{y}}^{k+1}\|^{2})+L_{g,1}^{2}L_{z \star}^{2}\|\bar{\mathbf{x}}^{k}-\bar{\mathbf{x}}^{k-1}\|^{2}\right].\] (69)

Then we present the bound of the right-hand side of (67). For the first term, we have the following evaluations:

\[\widetilde{\mathbb{E}}_{k}\left[\left\|\mathbf{\Gamma}_{z}\hat{ \mathbf{e}}_{z}^{k}-\gamma\mathbf{O}_{z}^{-1}\left[\begin{array}{c}\mathbf{ \Lambda}_{za}\hat{\mathbf{U}}_{z}^{\top}\left[\widetilde{\mathbb{E}}_{k}[ \mathbf{p}^{k}]-\mathbf{p}^{k}(\bar{\mathbf{x}}^{k},\bar{\mathbf{y}}^{k+1}) \right]\\ \mathbf{\Lambda}_{za}^{-1}\mathbf{\Lambda}_{za}\hat{\mathbf{U}}_{z}^{\top} \left[\mathbf{p}^{k+1}(\bar{\mathbf{x}}^{k+1},\bar{\mathbf{y}}^{k+2})- \mathbf{p}^{k}(\bar{\mathbf{x}}^{k},\bar{\mathbf{y}}^{k+1})\right]\end{array} \right]\right]\right]\right]\] \[\leq \|\mathbf{\Gamma}_{z}\|\|\hat{\mathbf{e}}_{z}^{k}\|^{2}+\frac{ \gamma^{2}\|\mathbf{O}_{z}^{-1}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}\widetilde{ \mathbb{E}}_{k}\left[\left\|\mathbf{\Gamma}_{za}\hat{\mathbf{U}}_{z}^{\top} \left[\mathbf{\bar{\mathbb{E}}}_{k}[\mathbf{p}^{k}]-\mathbf{p}^{k}(\bar{ \mathbf{x}}^{k},\bar{\mathbf{y}}^{k+1})\right]\right]\right]\right\|^{2}\] \[\leq \|\mathbf{\Gamma}_{z}\|\|\hat{\mathbf{e}}_{z}^{k}\|^{2}+\frac{ \gamma^{2}\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2}}{1-\| \mathbf{\Gamma}_{z}\|}\widetilde{\mathbb{E}}_{k}\left[\|\widetilde{\mathbb{E}}_{ k}[\mathbf{p}^{k}]-\mathbf{p}^{k}(\bar{\mathbf{x}}^{k},\bar{\mathbf{y}}^{k+1})\|^{2}\right]\] \[+\frac{\gamma^{2}\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{za} \|^{2}\|\mathbf{\Lambda}_{za}^{-1}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}\widetilde{ \mathbb{E}}_{k}\left[\|\mathbf{p}^{k+1}(\bar{\mathbf{x}}^{k+1},\bar{\mathbf{y}}^{k+2 })-\mathbf{p}^{k}(\bar{\mathbf{x}}^{k},\bar{\mathbf{y}}^{k+1})\|^{2}\right]\] (70)

where the first inequality use Jensen's inequality and the second inequality use \(\|\hat{\mathbf{U}}_{z}^{\top}\|\leq 1\).

For the second term, since \(\mathbf{z}^{k},\mathbf{y}^{k+1}\in\mathcal{U}_{k}\), we have:

\[\widetilde{\mathbb{E}}_{k}\left\|\mathbf{O}_{z}^{-1}\left[\begin{array} []{c}\mathbf{\Lambda}_{za}\hat{\mathbf{U}}_{z}^{\top}\begin{bmatrix}\widetilde{ \mathbb{E}}_{k}[\mathbf{p}^{k}]-\mathbf{p}^{k}\end{bmatrix}\\ 0\end{array}\right]\right\|^{2}\] \[\leq \|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2} \widetilde{\mathbb{E}}_{k}\left[\|\widetilde{\mathbb{E}}_{k}[\mathbf{p}^{k}]- \mathbf{p}^{k}\|^{2}\right]\] (71) \[\leq 2\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2}(\| \mathbf{z}^{k}\|^{2}\sigma_{g,2}^{2}+n\sigma_{f,1}^{2})\] \[\leq\]

For the third term, we have:

\[2\widetilde{\mathbb{E}}_{k}\left\langle\mathbf{\Gamma}_{z}\hat{\mathbf{e}}_{z} ^{k},\gamma\mathbf{O}_{z}^{-1}\left[\begin{array}{c}\mathbf{\Lambda}_{za} \hat{\mathbf{U}}_{z}^{\top}\begin{bmatrix}\widetilde{\mathbb{E}}_{k}[ \mathbf{p}^{k}]-\mathbf{p}^{k}\end{bmatrix}\\ 0\end{array}\right]\right\rangle=0,\] (72)

since \(\hat{\mathbf{e}}_{z}^{k}\in\mathcal{U}_{k}\).

Next, for the last two terms, we have:

\[\widetilde{\mathbb{E}}_{k}\left[\left\|\mathbf{O}_{z}^{-1}\left[ \begin{array}{c}\mathbf{\Lambda}_{za}\hat{\mathbf{U}}_{z}^{\top}\begin{bmatrix} \widetilde{\mathbb{E}}_{k}[\mathbf{p}^{k}]-\mathbf{p}^{k}(\bar{\mathbf{x}}^{ k},\bar{\mathbf{y}}^{k+1})\end{bmatrix}\\ \mathbf{\Lambda}_{zb}^{-1}\mathbf{\Lambda}_{za}\hat{\mathbf{U}}_{z}^{\top} \begin{bmatrix}\mathbf{p}^{k+1}(\bar{\mathbf{x}}^{k+1},\bar{\mathbf{y}}^{k+2}) -\mathbf{p}^{k}(\bar{\mathbf{x}}^{k},\bar{\mathbf{y}}^{k+1})\end{bmatrix} \right]\right\|^{2}\right]\] (73) \[\leq \|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2}\widetilde {\mathbb{E}}_{k}\left[\|\widetilde{\mathbb{E}}_{k}[\mathbf{p}^{k}]-\mathbf{p}^ {k}(\bar{\mathbf{x}}^{k},\bar{\mathbf{y}}^{k+1})\|^{2}\right]\] \[+\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2}\| \mathbf{\Lambda}_{zb}^{-1}\|^{2}\widetilde{\mathbb{E}}_{k}\left[\|\mathbf{p}^ {k+1}(\bar{\mathbf{x}}^{k+1},\bar{\mathbf{y}}^{k+2})-\mathbf{p}^{k}(\bar{ \mathbf{x}}^{k},\bar{\mathbf{y}}^{k+1})\|^{2}\right].\]

\[\widetilde{\mathbb{E}}_{k}\left[\left\|\mathbf{O}_{z}^{-1}\left[ \begin{array}{c}\mathbf{\Lambda}_{za}\hat{\mathbf{U}}_{z}^{\top}\begin{bmatrix} \widetilde{\mathbb{E}}_{k}[\mathbf{p}^{k}]-\mathbf{p}^{k}\end{bmatrix}\\ 0\end{array}\right]\right\|^{2}\right]\leq\|\mathbf{O}_{z}^{-1}\|^{2}\| \mathbf{\Lambda}_{za}\|^{2}\widetilde{\mathbb{E}}_{k}\left[\|\widetilde{ \mathbb{E}}_{k}[\mathbf{p}^{k}]-\mathbf{p}^{k}\|^{2}\right].\] (74)

Taking the expectation \(\widetilde{\mathbb{E}}_{k}\) on both sides of (67) and plugging (70), (71), (72), (73) and (74) into it, we obtain:

\[\widetilde{\mathbb{E}}_{k}\left[\|\hat{\mathbf{e}}_{z}^{k+1}\|^{ 2}\right]\] \[\leq \|\mathbf{\Gamma}_{z}\|\|\hat{\mathbf{e}}_{z}^{k}\|^{2}+\frac{2 \gamma^{2}\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2}}{1-\| \mathbf{\Gamma}_{z}\|}\widetilde{\mathbb{E}}_{k}\left[\|\widetilde{\mathbb{E} }_{k}[\mathbf{p}^{k}]-\mathbf{p}^{k}(\bar{\mathbf{x}}^{k},\bar{\mathbf{y}}^{k+1 })\|^{2}\right]\] \[+2\gamma^{2}\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{za}\|^ {2}\widetilde{\mathbb{E}}_{k}\|\widetilde{\mathbb{E}}_{k}[\mathbf{p}^{k}]- \mathbf{p}^{k}\|^{2}\] \[\leq \|\mathbf{\Gamma}_{z}\|\|\hat{\mathbf{e}}_{z}^{k}\|^{2}+12n\gamma^ {2}\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2}\left(\frac{L_{f,0} ^{2}}{\mu_{g}^{2}}\sigma_{g,2}^{2}+\sigma_{f,1}^{2}\right)\] \[+\frac{12\gamma^{2}\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{\Lambda} _{za}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}(L_{g,1}^{2}+(1-\|\mathbf{\Gamma}_{z} \|)\sigma_{g,2}^{2})\widetilde{\mathbb{E}}_{k}\left[\|\mathbf{O}_{z}\|^{2}\| \hat{\mathbf{e}}_{z}^{k}\|^{2}+\|\bar{\mathbf{z}}^{k}-\mathbf{z}_{\star}^{k}\|^ {2}\right]\] \[+\frac{6\gamma^{2}\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{za }\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}\left(L_{g,2}^{2}\frac{L_{f,0}^{2}}{\mu_{g}^{ 2}}+L_{f,1}^{2}\right)\widetilde{\mathbb{E}}_{k}\left[\|\mathbf{O}_{x}\|^{2}\| \hat{\mathbf{e}}_{x}^{k}\|^{2}+\|\mathbf{O}_{y}\|^{2}\|\hat{\mathbf{e}}_{y}^{k+ 1}\|^{2}\right]\] \[+\frac{6\gamma^{2}\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{ zb}^{-1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}\left(L_{f,1}^{2}+L_{g,2}^{2} \frac{L_{f,0}^{2}}{\mu_{g}^{2}}\right)\widetilde{\mathbb{E}}_{k}\left[\|\bar{ \mathbf{x}}^{k+1}-\bar{\mathbf{x}}^{k}\|^{2}+\|\bar{\mathbf{y}}^{k+2}-\bar{ \mathbf{y}}^{k+1}\|^{2}\right]\] \[+\frac{6\gamma^{2}\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{ zb}^{-1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}L_{g,1}^{2}L_{z}^{2} \widetilde{\mathbb{E}}_{k}\left[\|\bar{\mathbf{x}}^{k}-\bar{\mathbf{x}}^{k-1}\|^{2} \right],\]where the second inequality uses (36), (68), (69), and (71).

Thanks to

\[\frac{6\gamma^{2}\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{O}_{z}\|^{2}\|\mathbf{ \Lambda}_{za}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}\cdot(2L^{2}+2(1-\|\mathbf{ \Gamma}_{z}\|)\sigma_{g,2}^{2})\leq\frac{1-\|\mathbf{\Gamma}_{z}\|}{4},\]

we have:

\[\widetilde{\mathbb{E}}_{k}\left[\|\hat{\mathbf{e}}_{z}^{k+1}\|^{2}\right]\] \[\leq \frac{1+3\|\mathbf{\Gamma}_{z}\|}{4}\|\hat{\mathbf{e}}_{z}^{k}\| ^{2}+\frac{12\gamma^{2}(L_{g,1}^{2}+(1-\|\mathbf{\Gamma}_{z}\|)\sigma_{g,2}^{2 })\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2}}{1-\|\mathbf{ \Gamma}_{z}\|}\widetilde{\mathbb{E}}_{k}\left[\|\bar{\mathbf{z}}^{k}-\mathbf{ z}_{\star}^{k}\|^{2}\right]\] \[+12n\gamma^{2}\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{za} \|^{2}\left(\frac{L_{f,0}^{2}}{\mu_{g}^{2}}\sigma_{g,2}^{2}+\sigma_{f,1}^{2}\right)\] \[+\frac{1-\|\mathbf{\Gamma}_{z}\|}{4\|\mathbf{O}_{z}\|^{2}}\kappa ^{2}(\|\mathbf{O}_{x}\|^{2}\|\hat{\mathbf{e}}_{x}^{k}\|^{2}+\|\mathbf{O}_{y}\| ^{2}\|\hat{\mathbf{e}}_{y}^{k+1}\|^{2})\] \[+\frac{6\gamma^{2}\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{ zb}^{-1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}\left(L_{f,1}^{ 2}+L_{g,2}^{2}\frac{L_{f,0}^{2}}{\mu_{g}^{2}}\right)\widetilde{\mathbb{E}}_{k }\left[\|\bar{\mathbf{x}}^{k+1}-\bar{\mathbf{x}}^{k}\|^{2}+\|\bar{\mathbf{y}} ^{k+2}-\bar{\mathbf{y}}^{k+1}\|^{2}\right]\] \[+\frac{6\gamma^{2}\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{ zb}^{-1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}L_{g,1}^{2}L_{z}^ {2}.\widetilde{\mathbb{E}}_{k}\left[\|\bar{\mathbf{x}}^{k}-\bar{\mathbf{x}}^{ k-1}\|^{2}\right].\]

Taking summation and expectation on both sides, we get:

\[\frac{3}{4}(1-\|\mathbf{\Gamma}_{z}\|)\sum_{k=0}^{K}\mathbb{E} \left[\|\hat{\mathbf{e}}_{z}^{k}\|^{2}\right]\] \[\leq \mathbb{E}\|\hat{\mathbf{e}}_{z}^{0}\|^{2}-\mathbb{E}\left[\|\hat {\mathbf{e}}_{z}^{k+1}\|^{2}\right]\] \[+\frac{12\gamma^{2}(L_{g,1}^{2}+(1-\|\mathbf{\Gamma}_{z}\|)\sigma _{g,2}^{2})\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2}}{1-\| \mathbf{\Gamma}_{z}\|}\sum_{k=0}^{K}\mathbb{E}\left[\|\bar{\mathbf{z}}^{k}- \mathbf{z}_{\star}^{k}\|^{2}\right]\] \[+\frac{1-\|\mathbf{\Gamma}_{z}\|}{4\|\mathbf{O}_{z}\|^{2}}\kappa ^{2}\sum_{k=0}^{K}\mathbb{E}(\|\mathbf{O}_{x}\|^{2}\|\hat{\mathbf{e}}_{x}^{k} \|^{2}+\|\mathbf{O}_{y}\|^{2}\|\hat{\mathbf{e}}_{y}^{k+1}\|^{2})\] \[+12n\gamma^{2}\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{za} \|^{2}\left(\frac{L_{f,0}^{2}}{\mu_{g}^{2}}\sigma_{g,2}^{2}+\sigma_{f,1}^{2}\right)\] \[+\frac{6\gamma^{2}\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{ zb}^{-1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}\sum_{k=0}^{K} \mathbb{E}\left[\left(L_{f,1}^{2}+L_{g,2}^{2}\frac{L_{f,0}^{2}}{\mu_{g}^{2}} \right)\|\bar{\mathbf{y}}^{k+2}-\bar{\mathbf{y}}^{k+1}\|^{2}\right].\]

Thus,

\[\sum_{k=0}^{K+1}\mathbb{E}\left[\|\hat{\mathbf{e}}_{z}^{k}\|^{2}\right]\] \[\leq \frac{16\gamma^{2}(L_{g,1}^{2}+(1-\|\mathbf{\Gamma}_{z}\|)\sigma_{ g,2}^{2})\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2}}{(1-\|\mathbf{\Gamma}_{z}\|)^{2}} \sum_{k=0}^{K}\mathbb{E}\left[\|\bar{\mathbf{z}}^{k}-\mathbf{z}_{\star}^{k}\|^{ 2}\right]+\frac{2\mathbb{E}[\|\hat{\mathbf{e}}_{z}^{0}\|^{2}]}{1-\|\mathbf{ \Gamma}_{z}\|}\] \[+\frac{8\gamma^{2}\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{ zb}^{-1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2}}{(1-\|\mathbf{\Gamma}_{z}\|)^{2}}\sum_{k=0}^{K} \mathbb{E}\left[\left(L_{f,1}^{2}+L_{g,2}^{2}\frac{L_{f,0}^{2}}{\mu_{g}^{2}} \right)\|\bar{\mathbf{y}}^{k+2}-\bar{\mathbf{y}}^{k+1}\|^{2}\right]\]\[+16(K+1)n\gamma^{2}\frac{\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{ za}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}\left(\frac{L_{f,0}^{2}}{\mu_{g}^{2}} \sigma_{g,2}^{2}+\sigma_{f,1}^{2}\right)\] \[+\frac{\kappa^{2}}{3\|\mathbf{O}_{z}\|^{2}}\sum_{k=0}^{K}\mathbb{ E}(\|\mathbf{O}_{x}\|^{2}\|\hat{\mathbf{e}}_{x}^{k}\|^{2}+\|\mathbf{O}_{y}\|^{2}\| \hat{\mathbf{e}}_{y}^{k+1}\|^{2}).\]

**Lemma 14** (Consensus error of \(x\)).: _Suppose that Assumptions 1- 4 and Lemmas 4, 5, and 7 hold. We have_

\[\sum_{k=0}^{K+1}\mathbb{E}\|\hat{\mathbf{e}}_{x}^{k}\|^{2}\] \[\leq \frac{\mathbb{E}\|\hat{\mathbf{e}}_{x}^{0}\|^{2}}{1-\|\mathbf{ \Gamma}_{x}\|}+\frac{2\alpha^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{\Lambda} _{xa}\|^{2}}{(1-\|\mathbf{\Gamma}_{x}\|)^{2}}\left[\frac{1-\theta}{\theta}\left \|\widetilde{\nabla}\mathbf{\Phi}(\tilde{\mathbf{x}}^{0})\right\|^{2}\right]\] \[+\frac{6n\alpha^{2}\theta(K+1)\|\mathbf{O}_{x}^{-1}\|^{2}\| \mathbf{\Lambda}_{xa}\|^{2}}{1-\|\mathbf{\Gamma}_{x}\|}\left(\theta+\frac{1- \theta}{1-\|\mathbf{\Gamma}_{x}\|}\right)\left(\sigma_{f,1}^{2}+3\sigma_{g,2}^ {2}\frac{L_{f,0}^{2}}{\mu_{g}^{2}}\right)\] (75) \[+\frac{\alpha^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_{ xa}\|^{2}}{1-\|\mathbf{\Gamma}_{x}\|}\left(\frac{80L^{2}}{1-\|\mathbf{\Gamma}_{x}\|} +18\theta\left(\theta+\frac{1-\theta}{1-\|\mathbf{\Gamma}_{x}\|}\right)\sigma _{g,2}^{2}\right)\sum_{k=0}^{K}\mathbb{E}\left[\Delta_{k}+nI_{k}\right]\] \[+\frac{2\tilde{L}^{2}\alpha^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\| \mathbf{\Lambda}_{xa}\|^{2}}{(1-\|\mathbf{\Gamma}_{x}\|)^{2}}\left(\|\mathbf{ \Lambda}_{xb}^{-1}\|^{2}+\frac{2(1-\theta)^{2}}{\theta^{2}}\right)\sum_{k=0}^ {K-1}\mathbb{E}\left[\left\|\tilde{\mathbf{x}}^{k+1}-\tilde{\mathbf{x}}^{k} \right\|^{2}\right].\]

Proof.: Firstly, the term \(\|\hat{\mathbf{e}}_{x}^{k+1}\|^{2}\) can be deformed as

\[\|\hat{\mathbf{e}}_{x}^{k+1}\|^{2}\] \[= \left\|\mathbf{\Gamma}_{x}\hat{\mathbf{e}}_{x}^{k}-\alpha\mathbf{ O}_{x}^{-1}\left[\begin{array}{c}\mathbf{\Lambda}_{xa}\hat{\mathbf{U}}_{x}^{ \top}\left[\mathbf{E}_{k}[\mathbf{r}^{k+1}]-\widetilde{\nabla}\mathbf{\Phi}( \tilde{\mathbf{x}}^{k})\right]\\ \mathbf{\Lambda}_{xb}^{-1}\mathbf{\Lambda}_{xa}\hat{\mathbf{U}}_{x}^{\top} \left[\widetilde{\nabla}\mathbf{\Phi}(\tilde{\mathbf{x}}^{k+1})-\widetilde{ \nabla}\mathbf{\Phi}(\tilde{\mathbf{x}}^{k})\right]\end{array}\right]\right\|^{2}\] \[+\alpha^{2}\mathbb{E}_{k}\left[\left\|\mathbf{O}_{x}^{-1}\left[ \begin{array}{c}\mathbf{\Lambda}_{xa}\hat{\mathbf{U}}_{x}^{\top}\left[ \mathbf{E}_{k}[\mathbf{r}^{k+1}]-\mathbf{r}^{k+1}\right]\\ 0\end{array}\right]\right]\right\|^{2}\] \[-2\left\langle\mathbf{\Gamma}_{x}\hat{\mathbf{e}}_{x}^{k}, \alpha\mathbf{O}_{x}^{-1}\left[\begin{array}{c}\mathbf{\Lambda}_{xa}\hat{ \mathbf{U}}_{x}^{\top}\left[\mathbf{E}_{k}[\mathbf{r}^{k+1}]-\mathbf{r}^{k+1} \right]\\ 0\end{array}\right]\right\rangle\] \[+2\alpha^{2}\left\langle\mathbf{O}_{x}^{-1}\left[\begin{array}{c} \mathbf{\Lambda}_{xa}\hat{\mathbf{U}}_{x}^{\top}\left[\mathbf{E}_{k}[ \mathbf{r}^{k+1}]-\widetilde{\nabla}\mathbf{\Phi}(\tilde{\mathbf{x}}^{k}) \right]\\ \mathbf{\Lambda}_{xb}^{-1}\mathbf{\Lambda}_{xa}\hat{\mathbf{U}}_{x}^{\top} \left[\widetilde{\nabla}\mathbf{\Phi}(\tilde{\mathbf{x}}^{k+1})-\widetilde{ \nabla}\mathbf{\Phi}(\tilde{\mathbf{x}}^{k})\right]\end{array}\right]\right], \mathbf{O}_{x}^{-1}\left[\begin{array}{c}\mathbf{\Lambda}_{xa}\hat{ \mathbf{U}}_{x}^{\top}\left[\mathbf{E}_{k}[\mathbf{r}^{k+1}]-\mathbf{r}^{k+1} \right]\\ 0\end{array}\right]\right\rangle\] (76)

due to Eq. (35).

Then, for the first term of the right-hand side of (76), we use Jensen's Inequality and get:

\[\mathbb{E}_{k}\left[\left\|\mathbf{\Gamma}\hat{\mathbf{e}}_{x}^{k}- \alpha\mathbf{O}_{x}^{-1}\left[\begin{array}{c}\mathbf{\Lambda}_{xa}\hat{ \mathbf{U}}_{x}^{\top}\left[\mathbb{E}_{k}[\mathbf{r}^{k+1}]-\widetilde{ \nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{k})\right]\\ \mathbf{\Lambda}_{xb}^{-1}\mathbf{\Lambda}_{xa}\hat{\mathbf{U}}_{x}^{\top} \left[\widetilde{\nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{k+1})-\widetilde{ \nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{k})\right]\end{array}\right]\right\|^{2}\right]\] (77) \[\leq \|\mathbf{\Gamma}_{x}\|\|\hat{\mathbf{e}}_{x}^{k}\|^{2}+\frac{ \alpha^{2}\|\mathbf{O}_{x}^{-1}\|^{2}}{1-\|\mathbf{\Gamma}_{x}\|}\mathbb{E}_ {k}\left[\left\|\mathbf{\nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{k+1})- \widetilde{\nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{k})\right\|^{2}\right].\]

For the second term in the right-hand side of (76), we have:

\[\mathbb{E}_{k}\left\|\mathbf{O}_{x}^{-1}\left[\begin{array}{c} \mathbf{\Lambda}_{xa}\hat{\mathbf{U}}_{x}^{\top}\left[\mathbb{E}_{k}[\mathbf{r }^{k+1}]-\mathbf{r}^{k+1}\right]\\ 0\end{array}\right]\right\|^{2}\leq \|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}\mathbb{E }_{k}\|\mathbb{E}_{k}[\mathbf{r}^{k+1}]-\mathbf{r}^{k+1}\|^{2}\] (78) \[= \theta^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2 }\mathbb{E}_{k}\|\mathbb{E}_{k}[\mathbf{u}^{k}]-\mathbf{u}^{k}\|^{2}.\]

Like (72), we have:

\[\mathbb{E}_{k}\left[\left\langle\mathbf{\Gamma}_{x}\hat{\mathbf{e}}_{x}^{k}, \alpha\mathbf{O}_{x}^{-1}\left[\begin{array}{c}\mathbf{\Lambda}_{xa}\hat{ \mathbf{U}}_{x}^{\top}\left[\mathbb{E}_{k}[\mathbf{r}^{k+1}]-\mathbf{r}^{k+1} \right]\\ 0\end{array}\right]\right\rangle\right]=0.\] (79)

Next, for the last term, we have:

\[2\alpha^{2}\mathbb{E}_{k}\left\langle\mathbf{O}_{x}^{-1}\left[ \begin{array}{c}\mathbf{\Lambda}_{xa}\hat{\mathbf{U}}_{x}^{\top}\left[ \mathbb{E}_{k}[\mathbf{r}^{k+1}]-\widetilde{\nabla}\mathbf{\Phi}(\bar{ \mathbf{x}}^{k})\right]\\ \mathbf{\Lambda}_{xb}^{-1}\mathbf{\Lambda}_{xa}\hat{\mathbf{U}}_{x}^{\top} \left[\widetilde{\nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{k+1})-\widetilde{ \nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{k})\right]\end{array}\right]\right\rangle,\mathbf{O}_{x}^{-1}\left[\begin{array}{c}\mathbf{\Lambda}_{xa}\hat{ \mathbf{U}}_{x}^{\top}\left[\mathbb{E}_{k}[\mathbf{r}^{k+1}]-\mathbf{r}^{k+1} \right]\\ 0\end{array}\right]\right\rangle\] \[\leq \alpha^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^ {2}\mathbb{E}_{k}\left[\|\mathbb{E}_{k}[\mathbf{r}^{k+1}]-\widetilde{\nabla} \mathbf{\Phi}(\bar{\mathbf{x}}^{k})\|^{2}+\|\mathbb{E}_{k}[\mathbf{r}^{k+1}] -\mathbf{r}^{k+1}\|^{2}\right]\] \[+\alpha^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^ {2}\|\mathbf{\Lambda}_{xb}^{-1}\|^{2}\mathbb{E}_{k}\left[\|\widetilde{\nabla} \mathbf{\Phi}(\bar{\mathbf{x}}^{k+1})-\widetilde{\nabla}\mathbf{\Phi}(\bar{ \mathbf{x}}^{k})\|^{2}\right].\] (80)

Taking the expectation on both sides of (76), and plugging (77), (78), (79), (80) into it, we obtain:

\[\mathbb{E}_{k}\|\hat{\mathbf{e}}_{x}^{k+1}\|^{2}\] \[\leq \|\mathbf{\Gamma}_{x}\|\|\hat{\mathbf{e}}_{x}^{k}\|^{2}+2\alpha^{ 2}\theta^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}\mathbb{E }_{k}\|\mathbb{E}_{k}[\mathbf{u}^{k}]-\mathbf{u}^{k}\|^{2}\] \[+\frac{2\alpha^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{\Lambda} _{xa}\|^{2}}{1-\|\mathbf{\Gamma}_{x}\|}\mathbb{E}_{k}\left[\|\mathbb{E}_{k}[ \mathbf{r}^{k+1}]-\widetilde{\nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{k})\|^{2} +\|\mathbf{\Lambda}_{xb}^{-1}\|^{2}\|\widetilde{\nabla}\mathbf{\Phi}(\bar{ \mathbf{x}}^{k+1})-\widetilde{\nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{k})\|^{2} \right].\]Taking expectation and summation on both sides, we obtain:

\[(1-\|\mathbf{\Gamma}_{x}\|)\sum_{k=0}^{K}\mathbb{E}\|\hat{\mathbf{e} }_{x}^{k}\|^{2}\] \[\leq \mathbb{E}\|\hat{\mathbf{e}}_{x}^{0}\|^{2}-\mathbb{E}\|\hat{ \mathbf{e}}_{x}^{k+1}\|^{2}+\frac{2\widetilde{L}^{2}\alpha^{2}\|\mathbf{O}_{x} ^{-1}\|^{2}\|\mathbf{\Lambda}_{xb}^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}}{1 -\|\mathbf{\Gamma}_{x}\|}\sum_{k=0}^{K}\mathbb{E}\|\bar{\mathbf{x}}^{k+1}-\bar{ \mathbf{x}}^{k}\|^{2}\] \[+\frac{2\alpha^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_ {xa}\|^{2}}{1-\|\mathbf{\Gamma}_{x}\|}\left(\frac{1-\theta}{\theta}\left\| \widetilde{\nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{0})\right\|^{2}+2\sum_{k=0}^ {K}\mathbb{E}\left[\left\|\mathbb{E}_{k}[\mathbf{u}^{k}]-\widetilde{\nabla} \mathbf{\Phi}(\bar{\mathbf{x}}^{k})\right\|^{2}\right]\right)\] \[+\frac{2\alpha^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_ {xa}\|^{2}}{1-\|\mathbf{\Gamma}_{x}\|}\cdot\frac{2\widetilde{L}^{2}(1-\theta) ^{2}}{\theta^{2}}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\bar{\mathbf{x}}^{k+1} -\bar{\mathbf{x}}^{k}\right\|^{2}\right]\] \[+\left[2\alpha^{2}\theta^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{ \Lambda}_{xa}\|^{2}+\frac{2\alpha^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{ \Lambda}_{xa}\|^{2}}{1-\|\mathbf{\Gamma}_{x}\|}\theta(1-\theta)\right]\sum_{k= 0}^{K}\mathbb{E}\|\mathbb{E}_{k}[\mathbf{u}^{k}]-\mathbf{u}^{k}\|^{2}\] \[\leq \mathbb{E}\|\hat{\mathbf{e}}_{x}^{0}\|^{2}-\mathbb{E}\|\hat{ \mathbf{e}}_{x}^{k+1}\|^{2}+\frac{2\alpha^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\| \mathbf{\Lambda}_{xa}\|^{2}}{1-\|\mathbf{\Gamma}_{x}\|}\left[\frac{1-\theta}{ \theta}\left\|\widetilde{\nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{0})\right\|^{2 }\right]\] \[+6(K+1)n\alpha^{2}\theta\|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{ \Lambda}_{xa}\|^{2}\left(\theta+\frac{1-\theta}{1-\|\mathbf{\Gamma}_{x}\|} \right)\left(\sigma_{f,1}^{2}+3\sigma_{g,2}^{2}\frac{L_{f,0}^{2}}{\mu_{g}^{2} }\right)\] \[+\alpha^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^ {2}\left(\frac{80L^{2}}{1-\|\mathbf{\Gamma}_{x}\|}+18\theta\left(\theta+\frac {1-\theta}{1-\|\mathbf{\Gamma}_{x}\|}\right)\sigma_{g,2}^{2}\right)\sum_{k=0}^ {K}\mathbb{E}\left[\Delta_{k}+nI_{k}\right]\] \[+\frac{2\widetilde{L}^{2}\alpha^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\| \mathbf{\Lambda}_{xa}\|^{2}}{1-\|\mathbf{\Gamma}_{x}\|}\left(\|\mathbf{ \Lambda}_{xb}^{-1}\|^{2}+\frac{2(1-\theta)^{2}}{\theta^{2}}\right)\sum_{k=0}^ {K-1}\mathbb{E}\left[\left\|\bar{\mathbf{x}}^{k+1}-\bar{\mathbf{x}}^{k}\right\| ^{2}\right].\]

where the first inequality uses \(\widetilde{L}\)-Lipschitz continuity of \(\widetilde{\nabla}\mathbf{\Phi}\), Lemma 7, and the second inequality uses Lemma 4, Lemma 5 and

\[\|\mathbf{z}^{k+1}-\bar{\mathbf{z}}^{k+1}\|^{2}\leq\Delta_{k},\quad\|\bar{ \mathbf{z}}^{k+1}-\mathbf{z}_{*}^{k+1}\|^{2}\leq nI_{k}.\]

Hence we get

\[\sum_{k=0}^{K+1}\mathbb{E}\|\hat{\mathbf{e}}_{x}^{k}\|^{2}\] \[\leq \frac{\mathbb{E}\|\hat{\mathbf{e}}_{x}^{0}\|^{2}}{1-\|\mathbf{ \Gamma}_{x}\|}+\frac{2\alpha^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_ {xa}\|^{2}}{(1-\|\mathbf{\Gamma}_{x}\|)^{2}}\left[\frac{1-\theta}{\theta}\left\| \widetilde{\nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{0})\right\|^{2}\right]\] \[+\frac{6n\alpha^{2}\theta(K+1)\|\mathbf{O}_{x}^{-1}\|^{2}\| \mathbf{\Lambda}_{xa}\|^{2}}{1-\|\mathbf{\Gamma}_{x}\|}\left(\theta+\frac{1- \theta}{1-\|\mathbf{\Gamma}_{x}\|}\right)\left(\sigma_{f,1}^{2}+3\sigma_{g,2 }^{2}\frac{L_{f,0}^{2}}{\mu_{g}^{2}}\right)\] \[+\frac{\alpha^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_{xa }\|^{2}}{1-\|\mathbf{\Gamma}_{x}\|}\left(\frac{80L^{2}}{1-\|\mathbf{\Gamma}_{ x}\|}+18\theta\left(\theta+\frac{1-\theta}{1-\|\mathbf{\Gamma}_{x}\|}\right)\sigma_{g,2}^{2} \right)\sum_{k=0}^{K}\mathbb{E}\left[\Delta_{k}+nI_{k}\right]\] \[+\frac{2\widetilde{L}^{2}\alpha^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\| \mathbf{\Lambda}_{xa}\|^{2}}{(1-\|\mathbf{\Gamma}_{x}\|)^{2}}\left(\|\mathbf{ \Lambda}_{xb}^{-1}\|^{2}+\frac{2(1-\theta)^{2}}{\theta^{2}}\right)\sum_{k=0}^ {K-1}\mathbb{E}\left[\left\|\bar{\mathbf{x}}^{k+1}-\bar{\mathbf{x}}^{k}\right\| ^{2}\right].\]

The following lemma gather the consensus analysis of \(x,y,z\) together:

**Lemma 15**.: _Take_

\[\eta_{1}= \frac{3\kappa^{2}\beta^{2}\|\mathbf{O}_{y}\|^{2}\|\mathbf{O}_{y}^{ -1}\|^{2}}{(1-\|\mathbf{\Gamma}_{y}\|)^{2}}\|\mathbf{\Lambda}_{yb}^{-1}\|^{2}\| \mathbf{\Lambda}_{ya}\|^{2}L_{g,1}^{2}+16\gamma^{2}L^{2}\left(2\kappa^{2}+L_{z^{*} }^{2}\right)\frac{\|\mathbf{O}_{z}\|^{2}\|\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{ \Lambda}_{zb}\|^{2}\|\mathbf{\Lambda}_{zb}^{-1}\|^{2}}{(1-\|\mathbf{\Gamma}_{ z}\|)^{2}}\]

[MISSING_PAGE_EMPTY:45]

\[+16(K+1)n\gamma^{2}\frac{\|\mathbf{O}_{z}\|^{2}\|\mathbf{O}_{z}^{-1}\|^{2} \|\mathbf{\Lambda}_{za}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}\left(\frac{L_{f,0}^{2} }{\mu_{g}^{2}}\sigma_{g,2}^{2}+\sigma_{f,1}^{2}\right)+\frac{2\|\mathbf{O}_{z} \|^{2}\mathbb{E}\|\hat{\mathbf{e}}_{v}^{0}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}\] \[+\frac{\kappa^{2}\|\mathbf{O}_{z}\|^{2}\mathbb{E}\|\hat{\mathbf{e }}_{v}^{0}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}+\frac{2\kappa^{2}\alpha^{2}\| \mathbf{O}_{x}\|^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}}{ (1-\|\mathbf{\Gamma}_{x}\|)^{2}}\left[\frac{1-\theta}{\theta}\left\|\widetilde{ \nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{0})\right\|^{2}\right]\] \[+\frac{6n\kappa^{2}\alpha^{2}\theta(K+1)\|\mathbf{O}_{x}\|^{2}\| \mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2}}{1-\|\mathbf{\Gamma}_{ z}\|}\left(\theta+\frac{1-\theta}{1-\|\mathbf{\Gamma}_{x}\|}\right) \left(\sigma_{f,1}^{2}+3\sigma_{g,2}^{2}\frac{L_{f,0}^{2}}{\mu_{g}^{2}}\right)\] \[+\frac{\kappa^{2}\alpha^{2}\|\mathbf{O}_{x}\|^{2}\|\mathbf{O}_{x }^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}}{1-\|\mathbf{\Gamma}_{x}\|}\left( \frac{80L^{2}}{1-\|\mathbf{\Gamma}_{x}\|}+18\theta\left(\theta+\frac{1-\theta }{1-\|\mathbf{\Gamma}_{x}\|}\right)\sigma_{g,2}^{2}\right)\sum_{k=0}^{K}\left[ \Delta_{k}+nI_{k}\right]\] \[+\frac{2\kappa^{2}\widetilde{L}^{2}\alpha^{2}\|\mathbf{O}_{x}\|^ {2}\|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}}{(1-\|\mathbf{ \Gamma}_{x}\|)^{2}}\left(\|\Lambda_{xb}^{-1}\|^{2}+\frac{2(1-\theta)^{2}}{ \theta^{2}}\right)\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\bar{\mathbf{x}}^{k+1 }-\bar{\mathbf{x}}^{k}\right\|^{2}\right]\] \[\leq \frac{3}{4}\sum_{k=0}^{K+1}\mathbb{E}\left(\kappa^{2}\|\mathbf{O }_{x}\|^{2}\|\hat{\mathbf{e}}_{x}^{k}\|^{2}+\kappa^{2}\|\mathbf{O}_{y}\|^{2} \|\|\hat{\mathbf{e}}_{y}^{k}\|^{2}+\|\mathbf{O}_{z}\|^{2}\|\|\hat{\mathbf{e}}_ {z}^{k}\|^{2}\right)\] \[+(\eta_{1}+48\kappa^{2}L_{y}^{2}\eta_{2})\alpha^{2}\sum_{k=0}^{K} \mathbb{E}\|\bar{\mathbf{r}}^{k+1}\|^{2}+\eta_{2}\left(\frac{32L_{g,1}^{2} \beta}{\mu_{g}}\|\bar{\mathbf{y}}^{0}-\mathbf{y}^{\star}(\bar{x}^{0})\|^{2}+3 (K+1)\beta^{2}\sigma_{g,1}^{2}\right)\] \[+\frac{\kappa^{2}\|\mathbf{O}_{x}\|^{2}\|\mathbf{O}_{x}\|^{2}\| \mathbf{\Lambda}_{xa}\|^{2}\alpha^{2}}{1-\|\mathbf{\Gamma}_{x}\|}\frac{2 \kappa^{2}\alpha^{2}\|\mathbf{O}_{x}\|^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{ \Lambda}_{xa}\|^{2}}{(1-\|\mathbf{\Gamma}_{x}\|)^{2}}\left[\frac{1-\theta}{ \theta}\left\|\widetilde{\nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{0})\right\|^{2}\right]\] \[+16(K+1)n\gamma^{2}\frac{\|\mathbf{O}_{z}\|^{2}\|\mathbf{O}_{z}^{- 1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}\left(\frac{L_ {f,0}^{2}}{\mu_{g}^{2}}\sigma_{g,2}^{2}+\sigma_{f,1}^{2}\right)\] \[+24(K+1)n\kappa^{2}\alpha^{2}\theta\left(\theta+\frac{1-\theta}{1- \|\mathbf{\Gamma}_{x}\|}\right)\frac{\|\mathbf{O}_{x}\|^{2}\|\mathbf{O}_{x}^{- 1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}}{1-\|\mathbf{\Gamma}_{x}\|}\left(\frac{L _{f,0}^{2}}{\mu_{g}^{2}}\sigma_{g,2}^{2}+\sigma_{f,1}^{2}\right).\]

where the second inequality uses (54) and

\[\eta_{2}L_{g,1}^{2}\beta^{2}\cdot 52+\frac{\kappa^{2}\alpha^{2}\|\mathbf{O}_{x}^{- 1}\|^{2}\|\mathbf{\Omega}_{x}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}}{1-\| \mathbf{\Gamma}_{x}\|}\left(\frac{80L^{2}}{1-\|\mathbf{\Gamma}_{x}\|}+18\theta \left(\theta+\frac{1-\theta}{1-\|\mathbf{\Gamma}_{x}\|}\right)\sigma_{g,2}^{2} \right)\leq\frac{1}{12}.\]

Hence:

\[\frac{1}{4}\sum_{k=0}^{K}\mathbb{E}[\Delta_{k}]\] \[\leq (\eta_{1}+48\kappa^{2}L_{y}^{2}\eta_{2})\alpha^{2}\sum_{k=0}^{K} \mathbb{E}\|\bar{\mathbf{r}}^{k+1}\|^{2}+\frac{32L_{g,1}^{2}\eta_{2}\beta}{\mu_{ g}}\|\bar{\mathbf{y}}^{0}-\mathbf{y}^{\star}(\bar{x}^{0})\|^{2}+3(K+1)\eta_{2} \beta^{2}\sigma_{g,1}^{2}\] \[+\frac{\kappa^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{O}_{x}\|^{2} \|\mathbf{\Lambda}_{xa}\|^{2}\alpha^{2}}{1-\|\mathbf{\Gamma}_{x}\|}\left(\frac{80L ^{2}}{1-\|\mathbf{\Gamma}_{x}\|}+18\theta\left(\theta+\frac{1-\theta}{1-\| \mathbf{\Gamma}_{x}\|}\right)\sigma_{g,2}^{2}\right)\sum_{k=0}^{K}\mathbb{E}[nI_ {k}]\] \[+\frac{\|\mathbf{O}_{z}\|^{2}\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{ \Lambda}_{za}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}\cdot\frac{16\gamma^{2}(L_{g,1} ^{2}+(1-\|\mathbf{\Gamma}_{z}\|)\sigma_{g,2}^{2})}{1-\|\mathbf{\Gamma}_{z}\|} \sum_{k=-1}^{K}\mathbb{E}[nI_{k}]\] \[+\frac{3\kappa^{2}\beta^{2}(K+1)\|\mathbf{O}_{y}\|^{2}\|\mathbf{O} _{y}^{-1}\|^{2}\|\mathbf{\Lambda}_{ya}\|^{2}}{1-\|\mathbf{\Gamma}_{y}\|}n \sigma_{g,1}^{2}+\frac{2\kappa^{2}\|\mathbf{O}_{y}\|^{2}\mathbb{E}\|\hat{\mathbf{e }}_{y}^{0}\|^{2}}{1-\|\mathbf{\Gamma}_{y}\|}+\frac{2\|\mathbf{O}_{z}\|^{2} \mathbb{E}\|\hat{\mathbf{e}}_{z}^{0}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}\] \[+\frac{\kappa^{2}\|\mathbf{O}_{x}\|^{2}\mathbb{E}\|\hat{\mathbf{e}}_{ v}^{0}\|^{2}}{1-\|\mathbf{\Gamma}_{x}\|}+\frac{2\kappa^{2}\alpha^{2}\|\mathbf{O}_{x}\|^{2}\| \mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}}{(1-\|\mathbf{\Gamma}_{x} \|)^{2}}\left[\frac{1-\theta}{\theta}\left\|\widetilde{\nabla}\mathbf{\Phi}( \bar{\mathbf{x}}^{0})\right\|^{2}\right]\]\[+16(K+1)n\gamma^{2}\frac{\|\mathbf{O}_{z}\|^{2}\|\mathbf{O}_{z}^{-1}\|^{2} \|\mathbf{\Lambda}_{za}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}\left(\frac{L_{f,0}^{2} }{\mu_{g}^{2}}\sigma_{g,2}^{2}+\sigma_{f,1}^{2}\right)\] \[+24(K+1)n\kappa^{2}\alpha^{2}\theta\left(\theta+\frac{1-\theta}{1- \|\mathbf{\Gamma}_{x}\|}\right)\frac{\|\mathbf{O}_{x}\|^{2}\|\mathbf{O}_{x}^{ -1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}}{1-\|\mathbf{\Gamma}_{x}\|}\left(\frac{ L_{f,0}^{2}}{\mu_{g}^{2}}\sigma_{g,2}^{2}+\sigma_{f,1}^{2}\right).\]

#### c.1.8 Proof of the main theorem

Before giving the final result of the convergence analysis, we present the following Lemma that combines the results in the analysis of \(I_{k}\) and \(\Delta_{k}\):

**Lemma 16**.: _Suppose that Assumptions 1- 4 and Lemmas 6, 11, 15 hold. If \(\alpha,\beta,\gamma,\theta\) satisfy_

\[\alpha^{2} \leq\frac{(1-\|\mathbf{\Gamma}_{x}\|)^{2}}{16\|\mathbf{O}_{x}\| ^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}\left[80L^{2}+18 \theta\left(\theta+\frac{1-\theta}{1-\|\mathbf{\Gamma}_{x}\|}\right)(1-\| \mathbf{\Gamma}_{x}\|)\sigma_{g,2}^{2}\right]\cdot 2040\kappa^{6}},\] (84) \[\beta^{2}\eta_{2} \leq\frac{1}{1024L_{g,1}^{2}},\] \[\gamma^{2} \leq\frac{(1-\|\mathbf{\Gamma}_{z}\|)^{2}}{256\|\mathbf{O}_{z}\| ^{2}\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2}(L_{g,1}^{2}+(1- \|\mathbf{\Gamma}_{z}\|)\sigma_{g,2}^{2})\cdot 2040\kappa^{4}},\]

_and_

\[40\left[4(\eta_{1}+48\kappa^{2}L_{y}^{2},\eta_{2})\alpha^{2}+ \frac{1}{1020\kappa^{4}}\left(\frac{9\alpha^{2}L_{z^{*}}^{2}}{\gamma^{2}\mu_{ g}^{2}}+\frac{438\kappa^{4}\alpha^{2}}{\beta^{2}\mu_{g}^{2}}L_{y^{*}}^{2} \right)\right]\left(L^{2}+\frac{\theta\sigma_{g,2}^{2}}{n}\right)\leq\frac{1}{ 4080\kappa^{4}},\] (85)

_then we have:_

\[\sum_{k=0}^{K}\mathbb{E}[\Delta_{k}+nI_{k}]\lesssim \kappa^{4}(\eta_{1}+\kappa^{2}L_{y^{*}}^{2}\eta_{2})\alpha^{2} \left(\frac{n(\Phi(\bar{x}_{0})-\inf\Phi)}{\alpha}+\theta(K+1)(\sigma_{f,1}^{2 }+\kappa^{2}\sigma_{g,2}^{2})\right)\] \[+\left(\frac{\alpha^{2}L_{z^{*}}^{2}}{\gamma^{2}\mu_{g}^{2}}+ \frac{\kappa^{4}\alpha^{2}}{\beta^{2}\mu_{g}^{2}}L_{y^{*}}^{2}\right)\left( \frac{n(\Phi(\bar{x}_{0})-\inf\Phi)}{\alpha}+\theta(K+1)(\sigma_{f,1}^{2}+ \kappa^{2}\sigma_{g,2}^{2})\right)\] \[+\frac{\kappa^{6}\beta^{2}(K+1)\|\mathbf{O}_{y}\|^{2}\|\mathbf{O }_{y}^{-1}\|^{2}\|\mathbf{\Lambda}_{ya}\|^{2}}{1-\|\mathbf{\Gamma}_{y}\|}n \sigma_{g,1}^{2}+\kappa^{4}\eta_{2}\beta^{2}(K+1)\sigma_{g,1}^{2}\] \[+\frac{\kappa^{6}\|\mathbf{O}_{y}\|^{2}\mathbb{E}\|\hat{\mathbf{e }}_{y}^{0}\|^{2}}{1-\|\mathbf{\Gamma}_{y}\|}+\frac{\kappa^{4}\|\mathbf{O}_{z} \|^{2}\mathbb{E}\|\hat{\mathbf{e}}_{z}^{0}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|} +\frac{\kappa^{6}\|\mathbf{O}_{x}\|^{2}\mathbb{E}\|\hat{\mathbf{e}}_{x}^{0}\| ^{2}}{1-\|\mathbf{\Gamma}_{x}\|}\] \[+(K+1)\kappa^{4}\gamma^{2}\frac{\|\mathbf{O}_{z}\|^{2}\|\mathbf{O }_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}( \sigma_{f,1}^{2}+\kappa^{2}\sigma_{g,2}^{2})\] \[+(K+1)\kappa^{6}\alpha^{2}\theta\left(\theta+\frac{1-\theta}{1- \|\mathbf{\Gamma}_{x}\|}\right)\frac{\|\mathbf{O}_{x}\|^{2}\|\mathbf{O}_{x}^{ -1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}n}{1-\|\mathbf{\Gamma}_{x}\|}(\sigma_{f, 1}^{2}+\kappa^{2}\sigma_{g,2}^{2})\] \[+\frac{(K+1)\gamma}{\mu_{g}}(\sigma_{f,1}^{2}+\kappa^{2}\sigma_{g, 2}^{2})+\frac{n\|z_{*}^{4}\|^{2}}{\mu_{g}\gamma}+\kappa^{4}\left(\frac{\| \mathbf{\hat{y}}^{0}-\mathbf{y}^{*}(\bar{x}^{0})\|^{2}}{\beta\mu_{g}}+\frac{K \sigma_{g,1}^{2}}{\mu_{g}}\beta\right).\]Proof.: Combining (57) and (83), we obtain

\[\sum_{k=0}^{K}\mathbb{E}[\Delta_{k}]+\frac{1}{1020\kappa^{4}}\sum_{k= -1}^{K}\mathbb{E}[nI_{k}]\] \[\leq 4(\eta_{1}+48\kappa^{2}L_{y}^{2},\eta_{2})\alpha^{2}\sum_{k=0}^{K} \mathbb{E}\|\bar{\mathbf{r}}^{k+1}\|^{2}+\frac{128L_{g,1}^{2}\eta_{2}\beta}{ \mu_{g}}\|\bar{\mathbf{y}}^{0}-\mathbf{y}^{\star}(\bar{x}^{0})\|^{2}+12(K+1) \eta_{2}\beta^{2}\sigma_{g,1}^{2}\] \[+4\frac{\kappa^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{\Omega}_{ x}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}\alpha^{2}}{1-\|\mathbf{\Gamma}_{x}\|} \left(\frac{80L^{2}}{1-\|\mathbf{\Gamma}_{x}\|}+18\theta\left(\theta+\frac{1- \theta}{1-\|\mathbf{\Gamma}_{x}\|}\right)\sigma_{g,2}^{2}\right)\sum_{k=0}^{K }\mathbb{E}[nI_{k}]\] \[+\frac{4\|\mathbf{O}_{z}\|^{2}\|\mathbf{O}_{z}^{-1}\|^{2}\| \mathbf{\Lambda}_{za}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}\cdot\frac{16\gamma^{2 }(L_{g,1}^{2}+(1-\|\mathbf{\Gamma}_{z}\|)\sigma_{g,2}^{2})}{1-\|\mathbf{ \Gamma}_{z}\|}\sum_{k=-1}^{K}\mathbb{E}[nI_{k}]\] \[+\frac{12\kappa^{2}\beta^{2}(K+1)\|\mathbf{O}_{y}\|^{2}\|\mathbf{ O}_{y}^{-1}\|^{2}\|\mathbf{\Lambda}_{ya}\|^{2}}{1-\|\mathbf{\Gamma}_{y}\|}n \sigma_{g,1}^{2}+\frac{8\kappa^{2}\|\mathbf{O}_{y}\|^{2}\mathbb{E}\|\mathbf{ \hat{e}}_{y}^{0}\|^{2}}{1-\|\mathbf{\Gamma}_{y}\|}+\frac{8\|\mathbf{O}_{z}\|^{ 2}\mathbb{E}\|\mathbf{\hat{e}}_{y}^{0}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}\] \[+\frac{4\kappa^{2}\|\mathbf{O}_{x}\|^{2}\mathbb{E}\|\mathbf{\hat{ e}}_{x}^{0}\|^{2}}{1-\|\mathbf{\Gamma}_{x}\|}+\frac{8\kappa^{2}\alpha^{2}\| \mathbf{O}_{x}\|^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2} }{(1-\|\mathbf{\Gamma}_{x}\|)^{2}}\left[\frac{1-\theta}{\theta}\left\|\widetilde {\nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{0})\right\|^{2}\right]\] \[+64(K+1)n\gamma^{2}\frac{\|\mathbf{O}_{z}\|^{2}\|\mathbf{O}_{z}^{ -1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}\left(\frac {L_{f,0}^{2}}{\mu_{g}^{2}}\sigma_{g,2}^{2}+\sigma_{f,1}^{2}\right)\] \[+96(K+1)n\kappa^{2}\alpha^{2}\theta\left(\theta+\frac{1-\theta}{ 1-\|\mathbf{\Gamma}_{x}\|}\right)\frac{\|\mathbf{O}_{x}\|^{2}\|\mathbf{O}_{x} ^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}}{1-\|\mathbf{\Gamma}_{x}\|}\left( \frac{L_{f,0}^{2}}{\mu_{g}^{2}}\sigma_{g,2}^{2}+\sigma_{f,1}^{2}\right)\] \[+\frac{1}{1020\kappa^{4}}\left(\frac{9\alpha^{2}L_{z}^{2}}{\gamma ^{2}\mu_{g}^{2}}+\frac{438\kappa^{4}\alpha^{2}}{\beta^{2}\mu_{g}^{2}}L_{y^{ \star}}^{2}\right)\sum_{k=0}^{K}\mathbb{E}\|\bar{\mathbf{r}}^{k}\|^{2}+\frac{ 1}{2}\sum_{k=0}^{K}\mathbb{E}\left[\Delta_{k}\right]+\frac{1}{1020\kappa^{4}} \cdot\frac{3n\|z_{\|}^{2}\|^{2}}{\mu_{g}\gamma}\] \[+\frac{(K+1)}{1020\kappa^{4}}\frac{6\gamma}{\mu_{g}}\left(3\sigma_ {g,2}^{2}\frac{L_{f,0}^{2}}{\mu_{g}^{2}}+\sigma_{f,1}^{2}\right)+\frac{73 \kappa^{4}}{1020\kappa^{4}}\left(\frac{4}{\beta\mu_{g}}\|\bar{\mathbf{y}}^{0} -\mathbf{y}^{\star}(\bar{x}^{0})\|^{2}+\frac{4K\sigma_{g,1}^{2}}{\mu_{g}}\beta \right).\] (86)

Subtracting the term

\[4\frac{\kappa^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{O}_{x}\|^{2}\|\mathbf{ \Lambda}_{xa}\|^{2}\alpha^{2}}{1-\|\mathbf{\Gamma}_{x}\|}\left(\frac{80L^{2}} {1-\|\mathbf{\Gamma}_{x}\|}+18\theta\left(\theta+\frac{1-\theta}{1-\|\mathbf{ \Gamma}_{x}\|}\right)\sigma_{g,2}^{2}\right)\sum_{k=0}^{K}\mathbb{E}[nI_{k}]\] \[+\frac{4\|\mathbf{O}_{z}\|^{2}\|\mathbf{O}_{z}^{-1}\|^{2}\| \mathbf{\Lambda}_{za}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}\frac{16\gamma^{2}(L_{g,1 }^{2}+(1-\|\mathbf{\Gamma}_{z}\|)\sigma_{g,2}^{2})}{1-\|\mathbf{\Gamma}_{z}\| }\sum_{k=-1}^{K}\mathbb{E}[nI_{k}]+\frac{1}{2}\sum_{k=0}^{K}\mathbb{E}[\Delta_ {k}]\]

from both sides of (86) and using the restriction of \(\alpha,\gamma\) in (84), we can get:

\[\frac{1}{2040\kappa^{4}}\left(\sum_{k=0}^{K}\mathbb{E}[\Delta_{k}] +\sum_{k=-1}^{K}\mathbb{E}[nI_{k}]\right)\] \[\leq 4(\eta_{1}+48\kappa^{2}L_{y}^{2},\eta_{2})\alpha^{2}\sum_{k=0}^{ K}\mathbb{E}\|\bar{\mathbf{r}}^{k+1}\|^{2}+\frac{128L_{g,1}^{2}\eta_{2}\beta}{\mu_{g}} \|\bar{\mathbf{y}}^{0}-\mathbf{y}^{\star}(\bar{x}^{0})\|^{2}+12(K+1)\eta_{2} \beta^{2}\sigma_{g,1}^{2}\] \[+\frac{12\kappa^{2}\beta^{2}(K+1)\|\mathbf{O}_{y}\|^{2}\|\mathbf{O} _{y}^{-1}\|^{2}\|\mathbf{\Lambda}_{ya}\|^{2}}{1-\|\mathbf{\Gamma}_{y}\|}n \sigma_{g,1}^{2}+\frac{8\kappa^{2}\|\mathbf{O}_{y}\|^{2}\mathbb{E}\|\mathbf{\hat{ e}}_{y}^{0}\|^{2}}{1-\|\mathbf{\Gamma}_{y}\|}+\frac{8\|\mathbf{O}_{z}\|^{2} \mathbb{E}\|\mathbf{\hat{e}}_{z}^{0}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}\] \[+\frac{4\kappa^{2}\|\mathbf{O}_{x}\|^{2}\mathbb{E}\|\mathbf{\hat{ e}}_{x}^{0}\|^{2}}{1-\|\mathbf{\Gamma}_{x}\|}+\frac{8\kappa^{2}\alpha^{2}\| \mathbf{O}_{x}\|^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}}{(1-\| \mathbf{\Gamma}_{x}\|)^{2}}\left[\frac{1-\theta}{\theta}\left\|\widetilde{ \nabla}\mathbf{\Phi}(\bar{\mathbf{x}}^{0})\right\|^{2}\right]\] \[+64(K+1)n\gamma^{2}\frac{\|\mathbf{O}_{z}\|^{2}\|\mathbf{O}_{z}^{-1} \|^{2}\|\mathbf{\Lambda}_{za}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}\left(\frac{L_{f,0 }^{2}}{\mu_{g}^{2}}\sigma_{g,2}^{2}+\sigma_{f,1}^{2}\right)\] \[+96(K+1)n\kappa^{2}\alpha^{2}\theta\left(\theta+\frac{1-\theta}{1- \|\mathbf{\Gamma}_{x}\|}\right)\frac{\|\mathbf{O}_{x}\|^{2}\|\mathbf{O}_{x}^{-1}

\[+\frac{1}{1020\kappa^{4}}\cdot\frac{3n\|z_{*}^{\star}\|^{2}}{\mu_{g} \gamma}+\frac{1}{1020\kappa^{4}}\cdot 73\kappa^{4}\left(\frac{8}{\beta\mu_{g}}\| \bar{\mathbf{y}}^{0}-\mathbf{y}^{\star}(\bar{x}^{0})\|^{2}+\frac{4K\sigma_{g,1} ^{2}}{\mu_{g}}\beta\right),\]

where the second inequality holds since

\[\frac{128L_{g,1}^{2}\eta_{2}\beta}{\mu_{g}}\leq\frac{1}{8\beta\mu_{g}}.\]

Then taking (41) into the concern, we know:

\[\frac{1}{2040\kappa^{4}}\left(\sum_{k=0}^{K}\mathbb{E}[\Delta_{k} ]+\sum_{k=-1}^{K}\mathbb{E}[nI_{k}]\right)\] \[\leq 40\left[4(\eta_{1}+48\kappa^{2}L_{y^{\star}}^{2}\eta_{2})\alpha^ {2}+\frac{1}{1020\kappa^{4}}\left(\frac{9\alpha^{2}L_{z^{\star}}^{2}}{\gamma^ {2}\mu_{g}^{2}}+\frac{438\kappa^{4}\alpha^{2}}{\beta^{2}\mu_{g}^{2}}L_{y^{ \star}}^{2}\right)\right]\left(L^{2}+\frac{\theta\sigma_{g,2}^{2}}{n}\right) \sum_{k=0}^{K}\mathbb{E}[\Delta_{k}+nI_{k}]\] \[+4\left[4(\eta_{1}+48\kappa^{2}L_{y^{\star}}^{2}\eta_{2})\alpha^ {2}+\frac{1}{1020\kappa^{4}}\left(\frac{9\alpha^{2}L_{z^{\star}}^{2}}{\gamma^ {2}\mu_{g}^{2}}+\frac{438\kappa^{4}\alpha^{2}}{\beta^{2}\mu_{g}^{2}}L_{y^{ \star}}^{2}\right)\right]\frac{n(\Phi(\bar{x}_{0})-\inf\Phi)}{\alpha}\] \[+12\theta(K+1)\left[4(\eta_{1}+48\kappa^{2}L_{y^{\star}}^{2}\eta_ {2})\alpha^{2}+\frac{1}{1020\kappa^{4}}\left(\frac{9\alpha^{2}L_{z^{\star}}^{2 }}{\gamma^{2}\mu_{g}^{2}}+\frac{438\kappa^{4}\alpha^{2}}{\beta^{2}\mu_{g}^{2} }L_{y^{\star}}^{2}\right)\right]\left(\sigma_{f,1}^{2}+2\sigma_{g,2}^{2}\frac{ L_{f,0}^{2}}{\mu_{g}^{2}}\right)\] \[+12(K+1)\eta_{2}\beta^{2}\sigma_{g,1}^{2}+\frac{12\kappa^{2} \beta^{2}(K+1)\|\mathbf{O}_{y}\|^{2}\|\mathbf{O}_{y}^{-1}\|^{2}\|\mathbf{ \Lambda}_{y\alpha}\|^{2}}{1-\|\mathbf{\Gamma}_{y}\|}n\sigma_{g,1}^{2}+\frac{8 \kappa^{2}\|\mathbf{O}_{y}\|^{2}\mathbb{E}\|\mathbf{e}_{y}^{0}\|^{2}}{1-\| \mathbf{\Gamma}_{y}\|}\] \[+\frac{8\|\mathbf{O}_{z}\|^{2}\mathbb{E}\|\mathbf{e}_{y}^{0}\|^{2 }}{1-\|\mathbf{\Gamma}_{z}\|}+\frac{4\kappa^{2}\|\mathbf{O}_{x}\|^{2}\mathbb{ E}\|\mathbf{e}_{y}^{0}\|^{2}}{1-\|\mathbf{\Gamma}_{x}\|}+\frac{8\kappa^{2}\alpha^{2}\| \mathbf{O}_{x}\|^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}} {1-\|\mathbf{\Gamma}_{x}\|}\left(\frac{L_{f,0}^{2}}{\mu_{g}^{2}}\sigma_{g,2}^ {2}+\sigma_{f,1}^{2}\right)\] \[+64(K+1)n\gamma^{2}\frac{\|\mathbf{O}_{z}\|^{2}\|\mathbf{O}_{z}^{ -1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}\left(\frac {L_{f,0}^{2}}{\mu_{g}^{2}}\sigma_{g,2}^{2}+\sigma_{f,1}^{2}\right)\] \[+96(K+1)n\kappa^{2}\alpha^{2}\theta\left(\theta+\frac{1-\theta}{1 -\|\mathbf{\Gamma}_{x}\|}\right)\frac{\|\mathbf{O}_{x}\|^{2}\|\mathbf{O}_{x}^{ -1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}}{1-\|\mathbf{\Gamma}_{x}\|}\left(\frac {L_{f,0}^{2}}{\mu_{g}^{2}}\sigma_{g,2}^{2}+\sigma_{f,1}^{2}\right)\] \[+64(K+1)n\gamma^{2}\frac{\|\mathbf{O}_{z}\|^{2}\|\mathbf{O}_{z}^{ -1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}\left(\frac {L_{f,0}^{2}}{\mu_{g}^{2}}\sigma_{g,2}^{2}+\sigma_{f,1}^{2}\right)\] \[+96(K+1)n\kappa^{2}\alpha^{2}\theta\left(\theta+\frac{1-\theta}{1 -\|\mathbf{\Gamma}_{x}\|}\right)\frac{\|\mathbf{O}_{x}\|^{2}\|\mathbf{O}_{x}^{ -1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}}{1-\|\mathbf{\Gamma}_{x}\|}\left(\frac {L_{f,0}^{2}}{\mu_{g}^{2}}\sigma_{g,2}^{2}+\sigma_{f,1}^{2}\right)\] \[+64(K+1)n\gamma^{2}\frac{\|\mathbf{O}_{z}\|^{2}\|\mathbf{O}_{z}^{ -1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}\left(\frac {L_{f,0}^{2}}{\mu_{g}^{2}}\sigma_{g,2}^{2}+\sigma_{f,1}^{2}\right)\] \[+1\frac{1}{1020\kappa^{4}}\cdot\frac{3n\|z_{*}^{\star}\|^{2}}{\mu_{g }\gamma}+\frac{1}{1020\kappa^{4}}\cdot 73\kappa^{4}\left(\frac{8}{\beta\mu_{g}}\|\bar{ \mathbf{y}}^{0}-\mathbf{y}^{\star}(\bar{x}^{0})\|^{2}+\frac{4K\sigma_{g,1}^{2}}{ \mu_{g}}\beta\right),\]

[MISSING_PAGE_FAIL:50]

\[\alpha_{1} =\kappa^{-4}\sqrt{\frac{n}{K\sigma^{2}}},\] (88) \[\alpha_{x,2} =\left(\frac{(1-\|\bm{\Gamma}_{x}\|)^{2}}{\kappa^{10}K\|\mathbf{O}_ {x}\|^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}\sigma^{2}} \right)^{\frac{1}{4}}\] \[\alpha_{y,2} =\left(\frac{1-\|\bm{\Gamma}_{y}\|}{\kappa^{13}K\|\mathbf{O}_{y} \|^{2}\|\mathbf{O}_{y}^{-1}\|^{2}\|\mathbf{\Lambda}_{ya}\|^{2}\sigma^{2}} \right)^{\frac{1}{3}},\]\[\alpha_{y,3} =\left(\frac{n(1-\|\Gamma_{y}\|)^{2}}{\kappa^{21}K\|\mathbf{O}_{y}\|^ {2}\|\mathbf{O}_{y}^{-1}\|^{2}\|\mathbf{\Lambda}_{ya}\|^{2}\|\mathbf{\Lambda}_{ yb}^{-1}\|^{2}\sigma_{g,1}^{2}}\right)^{\frac{1}{3}},\] \[\alpha_{z,2} =\left(\frac{1-\|\Gamma_{z}\|}{\kappa^{13}K\|\mathbf{O}_{z}\|^{2} \|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2}\sigma^{2}}\right)^{ \frac{1}{3}},\] \[\alpha_{z,3} =\left(\frac{n(1-\|\Gamma_{z}\|)^{2}}{\kappa^{25}K\|\mathbf{O}_{ z}\|^{2}\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2}\|\mathbf{ \Lambda}_{zb}^{-1}\|^{2}\sigma_{g,1}^{2}}\right)^{\frac{1}{3}},\] \[\alpha_{yb,2} =\left(\frac{1-\|\Gamma_{y}\|}{\kappa^{13}\|\mathbf{O}_{y}\|^{2} \|\mathbf{O}_{y}^{-1}\|^{2}\|\mathbf{\Lambda}_{ya}\|^{2}\|\mathbf{\Lambda}_{ yb}^{-1}\|^{2}\zeta_{0}^{y}}\right)^{\frac{1}{3}},\] \[\alpha_{zb,2} =\left(\frac{1-\|\Gamma_{z}\|}{\kappa^{11}\|\mathbf{O}_{z}\|^{2} \|\mathbf{\Omega}_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2}\|\mathbf{ \Lambda}_{zb}^{-1}\|^{2}\zeta_{0}^{z}}\right)^{\frac{1}{3}},\] \[\alpha_{xb,2} =\left(\frac{1-\|\Gamma_{x}\|}{\kappa^{5}\|\mathbf{O}_{x}\|^{2} \|\mathbf{\Omega}_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}\|\mathbf{ \Lambda}_{xb}^{-1}\|^{2}\zeta_{0}^{x}}\right)^{\frac{1}{3}},\] \[\theta_{1} =\left(\frac{n\kappa^{2}\hat{\zeta}_{0}}{K\sigma^{2}}\right)^{ \frac{1}{2}},\] \[\theta_{2} =\kappa^{3}\alpha_{x,2},\]

and

\[\theta= \left(C_{\theta}+\frac{1}{\theta_{1}}+\frac{1}{\theta_{2}}\right) ^{-1},\] \[\alpha= \Theta\left(C_{\alpha}+\frac{\sqrt{1-\theta}}{\theta}\kappa^{3}+ \frac{1}{\alpha_{1}}+\frac{1}{\alpha_{y,2}}+\frac{1}{\alpha_{y,3}}+\frac{1}{ \alpha_{z,3}}+\frac{1}{\alpha_{yb,2}}+\frac{1}{\alpha_{zb,2}}+\frac{1}{\alpha _{xb,2}}+\frac{1}{\alpha_{z,2}}\right)^{-1},\] \[\beta= \Theta\left(\kappa^{4}\alpha\right),\] \[\gamma= \Theta\left(\kappa^{4}\alpha\right),\] (89)

[MISSING_PAGE_EMPTY:53]

where the last inequality uses (89).

Finally, substituting (88) and (89) into the last inequality, we can get:

\[\frac{1}{K+1}\sum_{k=0}^{K}\mathbb{E}\|\nabla\Phi(\bar{x}^{k})\|^{2}\] \[\lesssim \frac{\kappa^{5}\sigma}{\sqrt{nK}}+\kappa^{\frac{16}{3}}\left[ \left(\frac{\|\mathbf{O}_{y}\|^{2}\|\mathbf{O}_{y}^{-1}\|^{2}\|\mathbf{\Lambda }_{ya}\|^{2}}{1-\|\mathbf{\Gamma}_{y}\|}\right)^{\frac{1}{3}}+\left(\frac{\| \mathbf{O}_{z}\|^{2}\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2} }{1-\|\mathbf{\Gamma}_{z}\|}\right)^{\frac{1}{3}}\right]\frac{\sigma^{\frac{ 2}{3}}}{K^{\frac{2}{3}}}\] \[+\kappa^{\frac{7}{3}}\left(\frac{\|\mathbf{O}_{x}\|\|\mathbf{O}_ {x}^{-1}\|\|\mathbf{\Lambda}_{xa}\|^{2}\|\mathbf{\Lambda}_{yb}^{-1}\|^{2}\zeta _{0}^{y}}{1-\|\mathbf{\Gamma}_{y}\|}\right)^{\frac{1}{3}}+\kappa^{\frac{14}{3 }}\left(\frac{\|\mathbf{O}_{z}\|^{2}\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{ \Lambda}_{za}\|^{2}\|\mathbf{\Lambda}_{zb}^{-1}\|^{2}\zeta_{0}^{z}}{1-\| \mathbf{\Gamma}_{z}\|}\right)^{\frac{1}{3}}\] \[+\kappa^{\frac{8}{3}}\left(\frac{\|\mathbf{O}_{x}\|^{2}\|\mathbf{ O}_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}\|\mathbf{\Lambda}_{xb}^{-1}\|^{2} \zeta_{0}^{x}}{1-\|\mathbf{\Gamma}_{x}\|}\right)^{\frac{1}{3}}\frac{1}{K}+ \left(\kappa C_{\alpha}+\kappa^{4}C_{\theta}\right)\frac{1}{K},\]

where \(\sigma=\max\{\sigma_{f,1},\sigma_{g,1},\sigma_{g,2}\}\). 

**Remark 7**.: _From the proof of Lemma 17, the impact of the moving average technique on variance reduction becomes evident. The term \(\frac{\theta}{n}\sigma^{2}\) absorb \(\alpha^{2}\eta_{1}\sigma^{2}\), which includes the high order term \(\alpha^{4}\sigma^{2}\). Additionally, compared to \(y,z\), the quadratic term related to \(\sigma^{2}\) of \(x\) has an extra term \(\theta\) multiplied in the numerator (\(\alpha^{2}\theta\sigma^{2}\)). These details reduce the impacts of noise to terms related to \(x\), confirming the conclusion that terms related to \(y,z\) dominate the rate in precious sections. Notably, taking \(\theta<1\) is indispensable our proof. If we take \(\theta=1\), there would be a constant term \(\frac{1}{n}\sigma^{2}\) in the convergence rate (see the first inequality of (87)), since the coefficient \(\alpha^{2}/\beta^{2}+\alpha^{2}/\gamma^{2}=\mathcal{O}(1)\). This would not guarantee the convergence of SPARKLE._

### Analysis of consensus error and transient iteration complexity

From Lemma 17, we can immediately obtain the transient time complexity of Algorithm 1. Here we omit the impacts of the condition number \(\kappa\).

**Lemma 18**.: _The transient time complexity of Algorithm 1 has an upper bound of:_

\[\max\left\{n^{3}\left(\frac{\|\mathbf{O}_{y}\|^{2}\|\mathbf{O}_ {y}^{-1}\|^{2}}{1-\|\mathbf{\Gamma}_{y}\|}\right)^{2}\|\mathbf{\Lambda}_{ya} \|^{2},n^{3}\left(\frac{\|\mathbf{O}_{z}\|^{2}\|\mathbf{O}_{z}^{-1}\|^{2}}{1- \|\mathbf{\Gamma}_{z}\|}\right)^{2}\|\mathbf{\Lambda}_{za}\|^{2},\right.\] (90) \[\qquad\left.n^{2}\left(\frac{\|\mathbf{O}_{x}\|\|\mathbf{O}_{x}^ {-1}\|}{1-\|\mathbf{\Gamma}_{x}\|}\right)^{2}\|\mathbf{\Lambda}_{xa}\|^{2},n \left(\frac{\|\mathbf{O}_{y}\|\|\mathbf{O}_{y}^{-1}\|\|\mathbf{\Lambda}_{yb}^ {-1}\|}{1-\|\mathbf{\Gamma}_{y}\|}\right)^{\frac{4}{3}}\|\mathbf{\Lambda}_{ ya}\|,\right.\] \[\qquad\left.n\left(\frac{\|\mathbf{O}_{z}\|\|\mathbf{O}_{z}^{-1} \|\|\mathbf{\Lambda}_{xb}^{-1}\|}{1-\|\mathbf{\Gamma}_{z}\|}\right)^{\frac{ 4}{3}}\|\mathbf{\Lambda}_{za}\|,n\left(\frac{\|\mathbf{O}_{x}\|^{2}\|\mathbf{ O}_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}\|\mathbf{\Lambda}_{xb}^{-1}\|^{2}}{1-\| \mathbf{\Gamma}_{x}\|}\right)^{\frac{2}{3}},\right.\] \[\qquad\left.n\frac{\|\mathbf{O}_{x}\|\|\mathbf{O}_{x}^{-1}\|\| \mathbf{\Lambda}_{xa}\|\mathbf{\Lambda}_{xb}^{-1}\|}{1-\|\mathbf{\Gamma}_{x} \|},n\right\}.\]

Proof.: According to lemma 17, SPARKLE achieves linear speedup if:

\[\frac{1}{\sqrt{nK}}\gtrsim \left[\left(\frac{\|\mathbf{O}_{y}\|^{2}\|\mathbf{O}_{y}^{-1}\|^{2 }\|\mathbf{\Lambda}_{ya}\|^{2}}{1-\|\mathbf{\Gamma}_{y}\|}\right)^{\frac{1}{ 3}}+\left(\frac{\|\mathbf{O}_{z}\|^{2}\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{ \Lambda}_{za}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}\right)^{\frac{1}{3}}\right] \frac{1}{K^{\frac{2}{3}}}\]\[\left(\frac{\|\mathbf{O}_{x}\|\|\mathbf{O}_{x}^{-1}\|\|\mathbf{ \Lambda}_{xa}\|^{2}}{1-\|\mathbf{\Gamma}_{y}\|}\right)^{\frac{1}{3}}\frac{1}{K^ {\frac{3}{3}}}\] \[+\left[\left(\frac{\|\mathbf{O}_{y}\|^{2}\|\mathbf{O}_{y}^{-1}\|^ {2}\|\mathbf{\Lambda}_{ya}\|^{2}\|\mathbf{\Lambda}_{yb}^{-1}\|^{2}}{n(1-\| \mathbf{\Gamma}_{y}\|)^{2}}\right)^{\frac{1}{5}}+\left(\frac{\|\mathbf{O}_{z} \|^{2}\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2}\|\mathbf{ \Lambda}_{zb}^{-1}\|^{2}}{n(1-\|\mathbf{\Gamma}_{z}\|)^{2}}\right)^{\frac{1}{ 5}}\right]\frac{1}{K^{\frac{3}{5}}}\] \[+\left[\left(\frac{\|\mathbf{O}_{y}\|^{2}\|\mathbf{O}_{y}^{-1}\|^ {2}\|\mathbf{\Lambda}_{ya}\|^{2}\|\mathbf{\Lambda}_{yb}^{-1}\|^{2}\zeta_{0}^{ y}}{1-\|\mathbf{\Gamma}_{y}\|}\right)^{\frac{1}{3}}+\left(\frac{\|\mathbf{O}_{z} \|^{2}\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2}\|\mathbf{ \Lambda}_{zb}^{-1}\|^{2}\zeta_{0}^{z}}{1-\|\mathbf{\Gamma}_{z}\|}\right)^{ \frac{1}{3}}\right.\] \[\left.+\left(\frac{\|\mathbf{O}_{x}\|^{2}\|\mathbf{O}_{x}^{-1}\|^ {2}\|\mathbf{\Lambda}_{xa}\|^{2}\|\mathbf{\Lambda}_{xb}^{-1}\|^{2}\zeta_{0}^{ x}}{1-\|\mathbf{\Gamma}_{x}\|}\right)^{\frac{1}{3}}\right]\frac{1}{K}+\left(C_{\alpha}+C_{ \theta}\right)\frac{1}{K}.\]

It holds when \(K\) satisfies:

\[\left(\frac{\|\mathbf{O}_{y}\|^{2}\|\mathbf{O}_{y}^{-1}\|^{2}\| \mathbf{\Lambda}_{ya}\|^{2}}{1-\|\mathbf{\Gamma}_{y}\|}\right)^{\frac{1}{3}} \frac{1}{K^{\frac{2}{3}}} \lesssim\frac{1}{\sqrt{nK}},\] \[\left(\frac{\|\mathbf{O}_{z}\|^{2}\|\mathbf{O}_{z}^{-1}\|^{2}\| \mathbf{\Lambda}_{za}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}\right)^{\frac{1}{3}} \frac{1}{K^{\frac{2}{3}}} \lesssim\frac{1}{\sqrt{nK}},\] \[\left(\frac{\|\mathbf{O}_{y}\|^{2}\|\mathbf{O}_{y}^{-1}\|^{2}\| \mathbf{\Lambda}_{ya}\|^{2}\|\mathbf{\Lambda}_{yb}^{-1}\|^{2}}{n(1-\|\mathbf{ \Gamma}_{y}\|)^{2}}\right)^{\frac{1}{5}}\frac{1}{K^{\frac{2}{3}}} \lesssim\frac{1}{\sqrt{nK}},\] \[\left(\frac{\|\mathbf{O}_{x}\|\|\mathbf{O}_{x}^{-1}\|\|\mathbf{ \Lambda}_{xa}\|}{1-\|\mathbf{\Gamma}_{y}\|}\right)^{\frac{1}{2}}\frac{1}{K^{ \frac{2}{3}}} \lesssim\frac{1}{\sqrt{nK}},\] \[\left(\frac{\|\mathbf{O}_{z}\|^{2}\|\mathbf{O}_{z}^{-1}\|^{2}\| \mathbf{\Lambda}_{za}\|^{2}\|\mathbf{\Lambda}_{zb}^{-1}\|^{2}}{n(1-\|\mathbf{ \Gamma}_{z}\|)^{2}}\right)^{\frac{1}{5}}\frac{1}{K^{\frac{2}{3}}} \lesssim\frac{1}{\sqrt{nK}},\] \[\left(\frac{\|\mathbf{O}_{y}\|^{2}\|\mathbf{O}_{y}^{-1}\|^{2}\| \mathbf{\Lambda}_{ya}\|^{2}\|\mathbf{\Lambda}_{yb}^{-1}\|^{2}\zeta_{0}^{y}}{1- \|\mathbf{\Gamma}_{y}\|}\right)^{\frac{1}{3}}\frac{1}{K} \lesssim\frac{1}{\sqrt{nK}},\] \[\left(\frac{\|\mathbf{O}_{z}\|^{2}\|\mathbf{O}_{z}^{-1}\|^{2}\| \mathbf{\Lambda}_{za}\|^{2}\|\mathbf{\Lambda}_{zb}^{-1}\|^{2}\zeta_{0}^{z}}{1 -\|\mathbf{\Gamma}_{z}\|}\right)^{\frac{1}{3}}\frac{1}{K} \lesssim\frac{1}{\sqrt{nK}},\] \[\left(\frac{\|\mathbf{O}_{x}\|^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\| \mathbf{\Lambda}_{xa}\|^{2}\|\mathbf{\Lambda}_{xb}^{-1}\|^{2}\zeta_{0}^{x}}{1 -\|\mathbf{\Gamma}_{x}\|}\right)^{\frac{1}{3}}\frac{1}{K} \lesssim\frac{1}{\sqrt{nK}},\] \[\left(C_{\alpha}+C_{\theta}\right)\frac{1}{K} \lesssim\frac{1}{\sqrt{nK}}.\]

Then we get:

\[K\gtrsim\max\left\{n^{3}\left(\frac{\|\mathbf{O}_{y}\|^{2}\| \mathbf{O}_{y}^{-1}\|^{2}}{1-\|\mathbf{\Gamma}_{y}\|}\right)^{2}\|\mathbf{ \Lambda}_{ya}\|^{2},n^{3}\left(\frac{\|\mathbf{O}_{z}\|^{2}\|\mathbf{O}_{z}^{- 1}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}\right)^{2}\|\mathbf{\Lambda}_{za}\|^{2},\right.\] \[\left.n^{2}\left(\frac{\|\mathbf{O}_{x}\|\|\mathbf{O}_{x}^{-1}\| \|\mathbf{\Lambda}_{x}^{-1}\|}{1-\|\mathbf{\Gamma}_{x}\|}\right)^{2}\|\mathbf{ \Lambda}_{xa}\|^{2},n\left(\frac{\|\mathbf{O}_{y}\|\|\mathbf{O}_{y}^{-1}\| \mathbf{\Lambda}_{yb}^{-1}\|}{1-\|\mathbf{\Gamma}_{y}\|}\right)^{\frac{4}{3}} \|\mathbf{\Lambda}_{ya}\|,\right.\] \[\left.n\left(\frac{\|\mathbf{O}_{z}\|\|\mathbf{O}_{z}^{-1}\|\| \mathbf{\Lambda}_{sb}^{-1}\|}{1-\|\mathbf{\Gamma}_{z}\|}\right)^{\frac{4}{3}} \|\mathbf{\Lambda}_{za}\|,n\left(\frac{\|\mathbf{O}_{x}\|^{2}\|\mathbf{\Omega}_{x} ^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}\|\mathbf{\Lambda}_{xb}^{-1}\|^{2}}{1- \|\mathbf{\Gamma}_{x}\|}\right)^{\frac{2}{3}},\right.\] \[\left.n\frac{\|\mathbf{O}_{z}\|\|\mathbf{O}_{z}^{-1}\|\|\mathbf{ \Lambda}_{xa}\|\|\mathbf{\Lambda}_{sb}^{-1}\|}{1-\|\mathbf{\Gamma}_{x}\|},n \right\}.\]

#### c.2.1 Consensus Error

**Lemma 19**.: _Suppose that Assumptions 1- 4 hold. Then there exist constant step-sizes \(\alpha,\beta,\gamma,\theta\), such that Lemma 17 holds and_

\[\frac{1}{K}\sum_{k=0}^{K}\mathbb{E}\left[\frac{\|\mathbf{x}^{k}- \bar{\mathbf{x}}^{k}\|^{2}}{n}+\frac{\|\mathbf{y}^{k}-\bar{\mathbf{y}}^{k}\|^{ 2}}{n}\right]\] \[\lesssim_{K}\]

_where \(\lesssim_{K}\) denotes the the asymptotic rate when \(K\to\infty\)._

Proof.: Suppose \(\alpha\), \(\beta\), \(\gamma\), and \(\theta\) satisfy the constraints given in (88) and (89), which ensures that Theorem 1 (Lemma 17) holds.

For clarity, we define the constants:

\[c_{1}=\frac{9\alpha^{2}L_{z^{*}}^{2}}{\gamma^{2}\mu_{g}^{2}}+\frac{438\kappa^{ 4}\alpha^{2}}{\beta^{2}\mu_{g}^{2}}L_{y^{*}}^{2},\quad c_{2}=10\left(L^{2}+ \frac{\theta\sigma_{g,2}^{2}}{n}\right).\]

Then there exist \(\alpha\), \(\beta\), \(\gamma\), and \(\theta\) that satisfy the constraints in (88) and (89), and also:

\[c_{1}\leq 0.01L^{-2},\quad c_{2}\leq 11L^{2}.\] (91)

We take such values for step-sizes in the following proof.

We proceed by substituting (41) into (57), yielding:

\[\sum_{k=-1}^{K}\mathbb{E}[I_{k}]\leq 4c_{1}\left(\frac{\Phi(\bar{x}_{0})-\inf\Phi}{\alpha}+c_{2}\sum_ {k=0}^{K-1}\mathbb{E}\left(\frac{\Delta_{k}}{n}+I_{k}\right)+\frac{3\theta}{n} K\left(\sigma_{f,1}^{2}+2\sigma_{g,2}^{2}\frac{L_{f,0}^{2}}{\mu_{g}^{2}} \right)\right)\] \[+510\kappa^{4}\sum_{k=0}^{K}\mathbb{E}\left[\frac{\Delta_{k}}{n} \right]+\frac{3\|z_{*}^{1}\|^{2}_{2}}{\mu_{g}\gamma}\] \[+\frac{6(K+1)\gamma}{\mu_{g}n}\left(3\sigma_{g,2}^{2}\frac{L_{f,0 }^{2}}{\mu_{g}^{2}}+\sigma_{f,1}^{2}\right)+73\kappa^{4}\left(\frac{4}{\beta \mu_{g}}\|\bar{y}^{0}-y^{\star}(\bar{x}^{0})\|^{2}+\frac{4K\sigma_{g,1}^{2}}{ n\mu_{g}}\beta\right).\]

Subtracting \(4c_{1}c_{2}\sum_{k=0}^{K-1}\mathbb{E}[I_{k}]\) from both sides, we get:

\[\sum_{k=-1}^{K}\mathbb{E}[I_{k}]\lesssim \frac{\Phi(\bar{x}_{0})-\inf\Phi}{\alpha}+\frac{\theta}{n}K\left( \sigma_{f,1}^{2}+\sigma_{g,2}^{2}\frac{L_{f,0}^{2}}{\mu_{g}^{2}}\right)+ \kappa^{4}\sum_{k=0}^{K}\mathbb{E}\left[\frac{\Delta_{k}}{n}\right]+\frac{\|z_ {*}^{1}\|^{2}}{\mu_{g}\gamma}\] \[+\frac{K\gamma}{\mu_{g}n}\left(\sigma_{g,2}^{2}\frac{L_{f,0}^{2}} {\mu_{g}^{2}}+\sigma_{f,1}^{2}\right)+\kappa^{4}\left(\frac{1}{\beta\mu_{g}} \|\bar{y}^{0}-y^{\star}(\bar{x}^{0})\|^{2}+\frac{K\sigma_{g,1}^{2}}{n\mu_{g}} \beta\right).\]

Substituting (57) into (41), we obtain:

\[\frac{1}{4}\sum_{k=0}^{K}\mathbb{E}\left\|\bar{r}^{k+1}\right\|^ {2}\] \[\leq \frac{\Phi(\bar{x}_{0})-\inf\Phi}{\alpha}+c_{2}\sum_{k=0}^{K} \mathbb{E}\left[\frac{\Delta_{k}}{n}\right]+c_{2}c_{1}\sum_{k=0}^{K}\mathbb{E }\|\bar{r}^{k}\|^{2}\] \[+c_{2}\left[510\kappa^{4}\sum_{k=0}^{K}\mathbb{E}\left[\frac{ \Delta_{k}}{n}\right]+\frac{3\|z_{*}^{1}\|^{2}}{\mu_{g}\gamma}+\frac{6(K+1) \gamma}{\mu_{g}n}\left(3\sigma_{g,2}^{2}\frac{L_{f,0}^{2}}{\mu_{g}^{2}}+ \sigma_{f,1}^{2}\right)\right.\] \[\left.\quad+73\kappa^{4}\left(\frac{4}{\beta\mu_{g}}\|\bar{y}^{0}- y^{\star}(\bar{x}^{0})\|^{2}+\frac{4K\sigma_{g,1}^{2}}{n\mu_{g}}\beta\right)\right]\] \[+\frac{3\theta}{n}(K+1)\left(\sigma_{f,1}^{2}+2\sigma_{g,2}^{2} \frac{L_{f,0}^{2}}{\mu_{g}^{2}}\right).\]Subtracting \(c_{2}c_{1}\sum_{k=0}^{K}\mathbb{E}\|\bar{r}^{k}\|^{2}\) from both sides, we get

\[\sum_{k=0}^{K}\mathbb{E}\left\|\bar{r}^{k+1}\right\|^{2}\] \[\lesssim \frac{\Phi(\bar{x}_{0})-\inf\Phi}{\alpha}+\kappa^{4}\sum_{k=0}^{K }\mathbb{E}\left[\frac{\Delta_{k}}{n}\right]+\frac{\theta}{n}K\left(\sigma_{f,1 }^{2}+\sigma_{g,2}^{2}\frac{L_{f,0}^{2}}{\mu_{g}^{2}}\right)\] \[+\frac{\|z_{*}^{1}\|^{2}}{\mu_{g}\gamma}+\frac{K\gamma}{\mu_{g}n} \left(\sigma_{g,2}^{2}\frac{L_{f,0}^{2}}{\mu_{g}^{2}}+\sigma_{f,1}^{2}\right) +\kappa^{4}\left(\frac{1}{\beta\mu_{g}}\|\bar{y}^{0}-y^{\star}(\bar{x}^{0})\|^ {2}+\frac{K\sigma_{g,1}^{2}}{n\mu_{g}}\beta\right).\]

Taking

\[\eta_{3}=\left(\frac{\kappa^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{O}_{x}\|^ {2}\|\mathbf{\Lambda}_{xa}\|^{2}\alpha^{2}}{(1-\|\mathbf{\Gamma}_{x}\|)^{2}}+ \frac{\|\mathbf{O}_{z}\|^{2}\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{za }\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}\cdot\frac{\gamma^{2}(L_{g,1}^{2}+(1-\| \mathbf{\Gamma}_{z}\|)\sigma_{g,2}^{2})}{1-\|\mathbf{\Gamma}_{z}\|}\right),\]

and combining previous results with (83), we obtain

\[\sum_{k=0}^{K}\mathbb{E}\left[\Delta_{k}\right]\] \[\lesssim (\eta_{1}+\kappa^{2}L_{y^{\star}}^{2}\eta_{2})\alpha^{2}\sum_{k= 0}^{K}\mathbb{E}\|\bar{\mathbf{r}}^{k+1}\|^{2}+\kappa\eta_{2}\beta\|\bar{y}^{ 0}-\mathbf{y}^{\star}(\bar{x}^{0})\|^{2}+K\eta_{2}\beta^{2}\sigma_{g,1}^{2}+ \eta_{3}\sum_{k=-1}^{K}\mathbb{E}[nI_{k}]\] \[+\frac{\kappa^{2}\beta^{2}K\|\mathbf{O}_{y}\|^{2}\|\mathbf{O}_{y }^{-1}\|^{2}\|\mathbf{\Lambda}_{ya}\|^{2}}{1-\|\mathbf{\Gamma}_{y}\|}n\sigma_{ g,1}^{2}+\frac{\kappa^{2}\|\mathbf{O}_{y}\|^{2}\mathbb{E}\|\mathbf{e}_{y}^{ 0}\|^{2}}{1-\|\mathbf{\Gamma}_{y}\|}+\frac{\|\mathbf{O}_{z}\|^{2}\mathbb{E}\| \mathbf{e}_{z}^{0}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}\] \[+\frac{\kappa^{2}\|\mathbf{O}_{x}\|^{2}\mathbb{E}\|\mathbf{e}_{x} ^{0}\|^{2}}{1-\|\mathbf{\Gamma}_{x}\|}+\frac{\kappa^{2}\alpha^{2}\|\mathbf{O} _{x}\|^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}}{\theta(1- \|\mathbf{\Gamma}_{x}\|)^{2}}\left\|\widetilde{\mathbf{\nabla}}\mathbf{ \Phi}(\bar{\mathbf{x}}^{0})\right\|^{2}\] \[+Kn\gamma^{2}\frac{\|\mathbf{O}_{z}\|^{2}\|\mathbf{O}_{z}^{-1}\|^ {2}\|\mathbf{\Lambda}_{za}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}\left(\kappa^{2} \sigma_{g,2}^{2}+\sigma_{f,1}^{2}\right)\] \[+Kn\kappa^{2}\alpha^{2}\theta\frac{\|\mathbf{O}_{x}\|^{2}\| \mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}}{(1-\|\mathbf{\Gamma} _{x}\|)^{2}}\left(\kappa^{2}\sigma_{g,2}^{2}+\sigma_{f,1}^{2}\right)\] \[\lesssim \left[(\eta_{1}+\kappa^{2}L_{y^{\star}}^{2}\eta_{2})\alpha^{2}+ \eta_{3}\right]\cdot\kappa^{4}\sum_{k=0}^{K}\mathbb{E}\left[\Delta_{k}\right] +\kappa\eta_{2}\beta\|\bar{\mathbf{y}}^{0}-\mathbf{y}^{\star}(\bar{x}^{0})\|^ {2}+K\eta_{2}\beta^{2}\sigma_{g,1}^{2}\] \[+n\left[(\eta_{1}+\kappa^{2}L_{y^{\star}}^{2}\eta_{2})\alpha^{2} +\eta_{3}\right]\left[\frac{1}{\alpha}+\frac{\theta}{n}K\left(\sigma_{f,1}^{2} +\kappa^{2}\sigma_{g,2}^{2}\right)+\frac{1}{\mu_{g}\gamma}+\frac{K\gamma}{\mu _{g}n}\left(\kappa^{2}\sigma_{g,2}^{2}+\sigma_{f,1}^{2}\right)\right.\] \[\left.+\kappa^{4}\left(\frac{1}{\beta\mu_{g}}+\frac{K\sigma_{g,1}^ {2}}{n\mu_{g}}\beta\right)\right]+\frac{\kappa^{2}\beta^{2}K\|\mathbf{O}_{y}\|^ {2}\|\mathbf{O}_{y}^{-1}\|^{2}\|\mathbf{\Lambda}_{ya}\|^{2}}{1-\|\mathbf{ \Gamma}_{y}\|}n\sigma_{g,1}^{2}\] \[+\frac{\kappa^{2}\alpha^{2}\|\mathbf{O}_{x}\|^{2}\|\mathbf{O}_{x}^ {-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}}{\theta(1-\|\mathbf{\Gamma}_{x}\|)^{2} }\left\|\widetilde{\mathbf{\nabla}}\mathbf{\Phi}(\bar{\mathbf{x}}^{0})\right\|^ {2}\] \[+\frac{\kappa^{2}\|\mathbf{O}_{y}\|^{2}\mathbb{E}\|\mathbf{e}_{y}^{ 0}\|^{2}}{1-\|\mathbf{\Gamma}_{y}\|}+\frac{\|\mathbf{O}_{z}\|^{2}\mathbb{E}\| \mathbf{e}_{z}^{0}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}+\frac{\kappa^{2}\| \mathbf{O}_{x}\|^{2}\mathbb{E}\|\mathbf{e}_{x}^{0}\|^{2}}{1-\|\mathbf{\Gamma}_{x}\|}\] \[+Kn\gamma^{2}\frac{\|\mathbf{O}_{z}\|^{2}\|\mathbf{O}_{z}^{-1}\|^ {2}\|\mathbf{\Lambda}_{za}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}\left(\kappa^{2} \sigma_{g,2}^{2}+\sigma_{f,1}^{2}\right)\] \[+Kn\kappa^{2}\alpha^{2}\theta\frac{\|\mathbf{O}_{x}\|^{2}\|\mathbf{O }_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}}{(1-\|\mathbf{\Gamma}_{x}\|)^{2} }\left(\kappa^{2}\sigma_{g,2}^{2}+\sigma_{f,1}^{2}\right).\]

(88) and (89) imply that

\[\eta_{1}\lesssim\kappa^{2}+\kappa^{2}\frac{\|\mathbf{O}_{x}\|^{2}\| \mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}}{(1-\|\mathbf{\Gamma}_{x}\| )^{2}},\quad\eta_{2}\lesssim\kappa^{2},\] \[(\eta_{1}+\kappa^{2}L_{y^{\star}}^{2}\eta_{2})\alpha^{2}\lesssim \kappa^{-4},\quad\eta_{3}\lesssim\kappa^{-4}\]where \(\eta_{1},\eta_{2}\) are defined in Lemma 15.

Then taking \(\alpha,\beta,\gamma,\theta\) such that (88), (89), (91) hold and \(\kappa^{4}[(\eta_{1}+\kappa^{2}L_{y}^{2},\eta_{2})\alpha^{2}+\eta_{3}]\) is a sufficiently small constant, we can derive the following result:

\[\frac{1}{K}\sum_{k=0}^{K}\mathbb{E}\left[\frac{\Delta_{k}}{n}\right]\] \[\lesssim \frac{\kappa\eta_{2}\beta}{K}+\eta_{2}\beta^{2}\frac{\sigma_{g,1 }^{2}}{n}\] \[+\left[(\eta_{1}+\kappa^{2}L_{y}^{2},\eta_{2})\alpha^{2}+\eta_{3} \right]\left[\frac{1}{\alpha K}+\frac{\theta}{n}\left(\sigma_{f,1}^{2}+\kappa ^{2}\sigma_{g,2}^{2}\right)+\frac{1}{\mu_{g}\gamma K}+\frac{\gamma}{\mu_{g}n} \left(\kappa^{2}\sigma_{g,2}^{2}+\sigma_{f,1}^{2}\right)\right]\] \[+\left[(\eta_{1}+\kappa^{2}L_{y}^{2},\eta_{2})\alpha^{2}+\eta_{3 }\right]\kappa^{4}\left(\frac{1}{\beta\mu_{g}K}+\frac{\sigma_{g,1}^{2}}{n\mu_ {g}}\beta\right)+\frac{\kappa^{2}\beta^{2}\|\mathbf{O}_{y}\|^{2}\|\mathbf{O} _{y}^{-1}\|^{2}\|\mathbf{\Lambda}_{ya}\|^{2}}{1-\|\mathbf{\Gamma}_{y}\|}\sigma _{g,1}^{2}\] \[+\frac{\kappa^{2}\alpha^{2}\|\mathbf{O}_{x}\|^{2}\|\mathbf{O}_{x} ^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}}{\theta K(1-\|\mathbf{\Gamma}_{x}\|)^ {2}}+\frac{\kappa^{2}\|\mathbf{O}_{y}\|^{2}\mathbb{E}\|\mathbf{\hat{e}}_{y}^{ 0}\|^{2}}{(1-\|\mathbf{\Gamma}_{y}\|)Kn}+\frac{\|\mathbf{O}_{z}\|^{2}\mathbb{ E}\|\mathbf{\hat{e}}_{z}^{0}\|^{2}}{(1-\|\mathbf{\Gamma}_{z}\|)Kn}+\frac{ \kappa^{2}\|\mathbf{O}_{x}\|^{2}\mathbb{E}\|\mathbf{\hat{e}}_{x}^{0}\|^{2}}{(1 -\|\mathbf{\Gamma}_{x}\|)Kn}\] \[+\gamma^{2}\frac{\|\mathbf{O}_{z}\|^{2}\|\mathbf{O}_{z}^{-1}\|^{2 }\|\mathbf{\Lambda}_{za}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}\left(\kappa^{2} \sigma_{g,2}^{2}+\sigma_{f,1}^{2}\right)+\kappa^{2}\alpha^{2}\theta\frac{\| \mathbf{O}_{x}\|^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}} {(1-\|\mathbf{\Gamma}_{x}\|)^{2}}\left(\kappa^{2}\sigma_{g,2}^{2}+\sigma_{f, 1}^{2}\right)\] \[\lesssim \frac{\kappa^{5}\eta_{2}\alpha}{K}+\kappa^{10}\alpha^{2}\frac{ \sigma_{g,1}^{2}}{n}+\frac{\kappa}{K}\left[(\eta_{1}+\kappa^{2}L_{y}^{2},\eta _{2})\alpha+\frac{\eta_{3}}{\alpha}\right]\] \[+\frac{\kappa^{2}\|\mathbf{O}_{y}\|^{2}\|\mathbf{O}_{y}^{-1}\|^{2 }\|\mathbf{\Lambda}_{ya}\|^{2}}{1-\|\mathbf{\Gamma}_{y}\|}\sigma_{g,1}^{2} \kappa^{8}\alpha^{2}+\frac{\kappa^{-1}\alpha\|\mathbf{O}_{x}\|^{2}\|\mathbf{O} _{x}^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}}{K(1-\|\mathbf{\Gamma}_{x}\|)^{2}}\] \[+\alpha^{2}\frac{\kappa^{10}\|\mathbf{O}_{y}\|^{2}\|\mathbf{O}_{y} ^{-1}\|^{2}\|\mathbf{\Lambda}_{ya}\|^{2}\|\mathbf{\Lambda}_{yb}^{-1}\|^{2}\| \mathbf{\zeta}_{0}^{y}}{K(1-\|\mathbf{\Gamma}_{y}\|)}+\alpha^{2}\frac{\kappa^ {8}\|\mathbf{O}_{z}\|^{2}\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{za}\|^ {2}\|\mathbf{\Lambda}_{zb}^{-1}\|^{2}\zeta_{0}^{z}}{K(1-\|\mathbf{\Gamma}_{z}\|)}\] \[+\alpha^{2}\frac{\kappa^{2}\|\mathbf{O}_{x}\|^{2}\|\mathbf{O}_{x} ^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}\|\mathbf{\Lambda}_{xb}^{-1}\|^{2} \zeta_{0}^{x}}{K(1-\|\mathbf{\Gamma}_{x}\|)}\] \[+\kappa^{8}\alpha^{2}\frac{\|\mathbf{O}_{z}\|^{2}\|\mathbf{O}_{z} ^{-1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2}}{1-\| \mathbf{\Gamma}_{z}\|}\left(\kappa^{2}\sigma_{g,2}^{2}+\sigma_{f,1}^{2}\right)\] \[+\kappa^{2}\alpha^{2}\theta\frac{\|\mathbf{O}_{x}\|^{2}\|\mathbf{O} _{x}^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}}{(1-\|\mathbf{\Gamma}_{x}\|)^{2}} \left(\kappa^{2}\sigma_{g,2}^{2}+\sigma_{f,1}^{2}\right).\]

From (88) and (89), we can determine the asymptotic orders for \(\alpha,\beta,\gamma\) and \(\theta\) when \(K\to\infty\)

\[\alpha=\mathcal{O}\left(\kappa^{-4}\sqrt{\frac{n}{K\sigma^{2}}}\right),\quad \beta=\mathcal{O}\left(\sqrt{\frac{n}{K\sigma^{2}}}\right),\quad\gamma= \mathcal{O}\left(\sqrt{\frac{n}{K\sigma^{2}}}\right),\quad\theta=\mathcal{O} \left(\kappa\sqrt{\frac{n}{K\sigma^{2}}}\right).\]

Then we get

\[\frac{1}{K}\sum_{k=0}^{K}\mathbb{E}\left[\frac{\Delta_{k}}{n}\right] \lesssim_{K}\frac{\kappa^{2}n}{K}\left(\frac{\|\mathbf{O}_{z}\|^{2}\|\mathbf{O} _{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}+\frac{ \|\mathbf{O}_{y}\|^{2}\|\mathbf{O}_{y}^{-1}\|^{2}\|\mathbf{\Lambda}_{ya}\|^{2}}{ 1-\|\mathbf{\Gamma}_{y}\|}\right),\]

where \(\lesssim_{K}\) denotes the the asymptotic rate when \(K\to\infty\).

Then using (36) and the definition of \(\Delta_{k}\), we get

\[\frac{1}{K}\sum_{k=0}^{K}\mathbb{E}\left[\frac{\|\mathbf{x}^{k}- \bar{\mathbf{x}}^{k}\|^{2}}{n}+\frac{\|\mathbf{y}^{k}-\bar{\mathbf{y}}^{k}\|^{2}}{ n}\right]\] \[\lesssim_{K}\frac{n}{K}\left(\frac{\|\mathbf{O}_{z}\|^{2}\|\mathbf{O }_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2}}{1-\|\mathbf{\Gamma}_{z}\|}+ \frac{\|\mathbf{O}_{y}\|^{2}\|\mathbf{O}_{y}^{-1}\|^{2}\|\mathbf{\Lambda}_{ya}\|^ {2}}{1-\|\mathbf{\Gamma}_{y}\|}\right).\]In particular, the corresponding result of SPARKLE variants that using EXTRA, ED or GT is

\[\frac{1}{K}\sum_{k=0}^{K}\mathbb{E}\left[\frac{\|\mathbf{x}^{k}-\bar{\mathbf{x}}^{ k}\|^{2}}{n}+\frac{\|\mathbf{y}^{k}-\bar{\mathbf{y}}^{k}\|^{2}}{n}\right]\lesssim_{K} \frac{n}{K}\left(\frac{1}{1-\rho_{y}}+\frac{1}{1-\rho_{z}}\right),\]

where \(\rho_{y},\rho_{z}\) are spectrum gaps of relevant mixing matrices.

#### c.2.2 Essential matrix norms for analysis

Common heterogeneity-correction algorithms, including ED, EXTRA and GT, satisfy Assumption 3, according to transformations (31), (32) and discussions in [(2, Appendix B.2)]. Then Lemma 3 ensures that \(\|\bm{\Gamma}\|<1\). From Lemma 18, the transient time complexity depends on the coefficients \(\|\mathbf{O}\|^{2}\), \(\|\mathbf{O}^{-1}\|^{2}\), \(\|\bm{\Lambda}_{a}\|^{2}\), \(\|\bm{\Lambda}_{b}^{-1}\|^{2}\), and \(\|\bm{\Gamma}\|^{2}\). The solution of these matrices is constructive. Table 4 presents the upper bounds of these coefficients with different communication modes. Please refer to [(2, Appendix B.2)] for more details about the construction of these matrices and the computation of relevant norms. It is required that \(W\) is positive definite for ED, EXTRA, and we denote the smallest nonzero eigenvalue of \(W\) by \(\rho\). \(\rho\) can view as a constant. Otherwise we replace \(\mathbf{W}\) with \(t\mathbf{I}+(1-t)\mathbf{W}\) for some constant \(t\in(0,1)\) (e.g. \(t=1/2\)).

Substituting values of \(\|\mathbf{O}_{s}\|,\|\mathbf{O}_{s}^{-1}\|,\|\bm{\Lambda}_{sa}\|,\|\bm{\Lambda} _{sb}^{-1}\|,\|\bm{\Gamma}_{s}\|\) into (90), we obtain the explicit transient iteration complexity for some specific examples of Algorithm 1, which are listed in Table 2. Note that all GT variants exhibit the same transient iteration complexity.

#### c.2.3 Theoretical gap between upper-level and lower-level

Note that \(\|\bm{\Lambda}_{sa}\|\leq 1\). We rewrite the upper bound of the transient iteration complexity in Lemma 18 as

\[\max\{n^{3}\delta_{y},n^{3}\delta_{z},n^{2}\delta_{x},n\hat{\delta}_{y},n\hat{ \delta}_{z},n\hat{\delta}_{x}\}\] (92)

where

\[\delta_{y} =\left(\frac{\|\mathbf{O}_{y}\|^{2}\|\mathbf{O}_{y}^{-1}\|^{2}}{ 1-\|\bm{\Gamma}_{y}\|}\right)^{2}\|\bm{\Lambda}_{ya}\|^{2},\delta_{z}=\left( \frac{\|\mathbf{O}_{z}\|^{2}\|\mathbf{O}_{z}^{-1}\|^{2}}{1-\|\bm{\Gamma}_{z}\| }\right)^{2}\|\bm{\Lambda}_{za}\|^{2},\delta_{x}=\left(\frac{\|\mathbf{O}_{x} \|\|\mathbf{O}_{x}^{-1}\|\bm{\Lambda}_{xb}^{-1}\|}{1-\|\bm{\Gamma}_{z}\|}\| \bm{\Lambda}_{za}\|\right)^{2},\] (93) \[\hat{\delta}_{y} =\left(\frac{\|\mathbf{O}_{y}\|\|\mathbf{O}_{y}^{-1}\|\|\bm{ \Lambda}_{xb}^{-1}\|}{1-\|\bm{\Gamma}_{y}\|}\right)^{\frac{4}{3}},\hat{\delta} _{z}=\left(\frac{\|\mathbf{O}_{z}\|\|\mathbf{O}_{z}^{-1}\|\|\bm{\Lambda}_{xb}^{ -1}\|}{1-\|\bm{\Gamma}_{z}\|}\right)^{\frac{4}{3}},\] \[\hat{\delta}_{x} =\left(\frac{\|\mathbf{O}_{x}\|^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\| \bm{\Lambda}_{xb}^{-1}\|^{2}}{1-\|\bm{\Gamma}_{x}\|}\right)^{\frac{2}{3}}+ \frac{\|\mathbf{O}_{x}\|\|\mathbf{O}_{x}^{-1}\|\|\bm{\Lambda}_{xb}^{-1}\|\bm{ \Lambda}_{xb}^{-1}\|}{1-\|\bm{\Gamma}_{x}\|}.\]

Suppose that we use the same communication matrices and heterogeneity-correction methods for updating \(x,y,z\), _i.e_.

\[\|\mathbf{O}_{x}\|=\|\mathbf{O}_{y}\|=\|\mathbf{O}_{z}\|,\|\mathbf{O }_{x}^{-1}\|=\|\mathbf{O}_{y}^{-1}\|=\|\mathbf{O}_{z}^{-1}\|,\|\bm{\Gamma}_{x} \|=\|\bm{\Gamma}_{y}\|=\|\bm{\Gamma}_{z}\|,\] \[\|\bm{\Lambda}_{xa}\|=\|\bm{\Lambda}_{ya}\|=\|\bm{\Lambda}_{za}\|, \|\bm{\Lambda}_{xb}^{-1}\|=\|\bm{\Lambda}_{yb}^{-1}\|=\|\bm{\Lambda}_{zb}^{-1 }\|.\]

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline Mode & \(\mathbf{A}\) & \(\mathbf{B}\) & \(\mathbf{C}\) & \(\|\mathbf{O}\|\) & \(\|\mathbf{O}^{-1}\|\) & \(\|\bm{\Lambda}_{a}\|\) & \(\|\bm{\Lambda}_{b}^{-1}\|\) & \(\|\bm{\Gamma}\|\) \\ \hline ED & \(\mathbf{W}\) & \((\mathbf{I}-\mathbf{W})^{\frac{1}{2}}\) & \(\mathbf{W}\) & \(1\) & \(\underline{\rho}^{-\frac{1}{2}}\) & \(\rho\) & \((1-\rho)^{-\frac{1}{2}}\) & \(\sqrt{\rho}\) \\ \hline EXTRA & \(\mathbf{I}\) & \((\mathbf{I}-\mathbf{W})^{\frac{1}{2}}\) & \(\mathbf{W}\) & \(1\) & \(\underline{\rho}^{-\frac{1}{2}}\) & \(1\) & \((1-\rho)^{-\frac{1}{2}}\) & \(\sqrt{\rho}\) \\ \hline ATC-GT & \(\mathbf{W}^{2}\) & \(\mathbf{I}-\mathbf{W}\) & \(\mathbf{W}^{2}\) & \(1\) & \(1\) & \(\rho^{2}\) & \((1-\rho)^{-1}\) & \(\frac{1+\rho}{2}\) \\ \hline Semi-ATC-GT & \(\mathbf{W}\) & \(\mathbf{I}-\mathbf{W}\) & \(\mathbf{W}^{2}\) & \(1\) & \(1\) & \(\rho\) & \((1-\rho)^{-1}\) & \(\frac{1+\rho}{2}\) \\ \hline Non-ATC-GT & \(\mathbf{I}\) & \(\mathbf{I}-\mathbf{W}\) & \(\mathbf{W}^{2}\) & \(1\) & \(1\) & \(1\) & \((1-\rho)^{-1}\) & \(\frac{1+\rho}{2}\) \\ \hline \end{tabular}
\end{table}
Table 4: Upper bounds of coefficients for different heterogeneity-correction modes in Lemma 18, where notation \(\mathcal{O}\) is omitted for \(\|\mathbf{O}\|\) and \(\|\mathbf{O}^{-1}\|\).

Then we have

\[\delta_{x}\lesssim\delta_{y}=\delta_{z},\,\hat{\delta}_{x}\lesssim\hat{\delta}_{y}= \hat{\delta}_{z},\] (94)

Now we fix the update strategies for \(y,\,z\). (94) implies that we can appropriately increase \(\delta_{x},\hat{\delta}_{x}\) while keeping the transient iteration complexity (92) unchanged (at most scaled by a constant factor). For example, we can use a moderately sparser communication network for updating \(x\) than \(y,z\). We illustrate this point with three examples: SPARKLE-ED, SPARKLE-EXTRA and SPARKLE-GT (variants), where \(y,z\) share the same communication matrix \(\mathbf{W}_{y}\).

* SPARKLE-ED, SPARKLE-EXTRA: From Table 4, we have \[\delta_{x}=\mathcal{O}\left((1-\rho(\mathbf{W}_{x}))^{-2}\right), \delta_{y}=\delta_{z}=\mathcal{O}\left((1-\rho(\mathbf{W}_{y}))^{-2}\right),\] \[\hat{\delta}_{x}=\mathcal{O}\left((1-\rho(\mathbf{W}_{x}))^{-\frac{ 3}{2}}\right),\hat{\delta}_{y}=\hat{\delta}_{z}=\mathcal{O}\left((1-\rho( \mathbf{W}_{y}))^{-2}\right).\] Substituting these values into (92), we get the transient iteration complexity is bounded by \[\max\left\{n^{2}(1-\rho(\mathbf{W}_{x}))^{-2},n^{3}(1-\rho(\mathbf{W}_{y}))^ {-2}\right\}\] SPARKLE-ED will keep the transient iteration complexity \(n^{3}(1-\rho(\mathbf{W}_{y}))^{-2}\) (the dominated term) if \[(1-\rho(\mathbf{W}_{x}))^{-1}\lesssim\sqrt{n}(1-\rho(\mathbf{W}_{y}))^{-1}.\] (95)
* SPARKLE-GT variants: Results in Table 4 imply that \[\delta_{x}=\mathcal{O}\left((1-\rho(\mathbf{W}_{x}))^{-2}\right), \delta_{y}=\delta_{z}=\mathcal{O}\left((1-\rho(\mathbf{W}_{y}))^{-2}\right),\] \[\hat{\delta}_{x}=\mathcal{O}\left((1-\rho(\mathbf{W}_{x}))^{-2}\right), \hat{\delta}_{y}=\hat{\delta}_{z}=\mathcal{O}\left((1-\rho(\mathbf{W}_{y}))^{ -\frac{8}{3}}\right).\] Following the same argument as before, we have the following upper bound of the transient iteration complexity of SPARKLE-GT \[\max\left\{n^{2}(1-\rho(\mathbf{W}_{x}))^{-2},n^{3}(1-\rho(\mathbf{W}_{y}))^ {-2},n(1-\rho(\mathbf{W}_{y}))^{-\frac{8}{3}}\right\}.\] we get the constraints of the spectral gap \(1-\rho(\mathbf{W}_{x})\) that maintains the transient iteration complexity \(\max\left\{n^{3}(1-\rho(\mathbf{W}_{y}))^{-2},n(1-\rho(\mathbf{W}_{y}))^{- \frac{8}{3}}\right\}\): \[(1-\rho(\mathbf{W}_{x}))^{-1}\lesssim\max\left\{\sqrt{n}(1-\rho(\mathbf{W}_{y} ))^{-1},n^{-1/2}(1-\rho(\mathbf{W}_{y}))^{-\frac{4}{3}}\right\}.\] (96) Denote the communication times per agent of \(\mathbf{W}_{x},\mathbf{W}_{y}\) by \(c_{x},c_{y}\) respectively. For example, we have \(c_{x}=2\), \(c_{y}=n-1\) when taking Ring Graph for \(x\) (_i.e._\([\mathbf{W}_{x}]_{ij}\neq 0\) iff \(|i-j|\in\{0,1,n-1\}\) ), and Complete Graph for \(y\) (_i.e._\(\mathbf{W}_{y}=\frac{1}{n}\mathbf{1}_{n}\mathbf{1}_{n}^{\top}\)).

Then for each agent, the communication cost per round is \(\mathcal{O}(c_{x}p+c_{y}q)\). If we take \(a=c_{x}/c_{y}\) to measure the relative sparsity of the two communication matrices, and consider \(c_{y}=\mathcal{O}(1)\), then for each agent, the communication cost per round is \(\mathcal{O}(ap+q)\). (95) and (96) theoretically provide the range of the sparsity (connectivity) degree of \(\mathbf{W}_{x}\) relative to \(\mathbf{W}_{y}\). From (95) and (96), we can set \(a\ll 1\), while maintaining the transient iteration complexity for SPARKLE-GT, SPARKLE-ED, SPARKLE-EXTRA.

#### c.2.4 The transient iteration complexities of some specific examples in SPARKLE.

Now we compute the transient iteration complexities of each SPARKLE-**L**-**U** algorithm, where \(\mathbf{L},\mathbf{U}\in\{\text{GT (variants)},\text{ED, EXTRA}\}\). For brevity, here we assume that \(\mathbf{W}_{x}=\mathbf{W}_{y}=\mathbf{W}_{z}\), use the same heterogeneity-correction method to \(y,\,z\), and denote the spectral gap \(1-\rho(\mathbf{W}_{x})\) by \(1-\rho\).

Substituting the results in Table 4 into (92) and (93), we get

\[\delta_{x}=\mathcal{O}\left(\frac{1}{(1-\rho)^{2}}\right),\delta_{y}=\delta_{z} =\mathcal{O}\left(\frac{1}{(1-\rho)^{2}}\right)\]

for any \(\mathbf{L},\mathbf{U}\in\{\text{GT (variants)},\text{ED, EXTRA}\}\),

\[\hat{\delta}_{x}=\mathcal{O}\left(\frac{1}{(1-\rho)^{2}}\right),\mathcal{O} \left(\frac{1}{(1-\rho)^{3/2}}\right),\mathcal{O}\left(\frac{1}{(1-\rho)^{3/2 }}\right)\]for \(\mathbf{U}=\{\text{GT (variants)},\text{ED},\text{EXTRA}\}\) respectively, and

\[\hat{\delta}_{y}=\hat{\delta}_{z}=\mathcal{O}\left(\frac{1}{(1-\rho)^{8/3}} \right),\mathcal{O}\left(\frac{1}{(1-\rho)^{2}}\right),\mathcal{O}\left(\frac{ 1}{(1-\rho)^{2}}\right)\]

for \(\mathbf{L}=\{\text{GT (variants)},\text{ED},\text{EXTRA}\}\) respectively.

Combining the above results, we can directly obtain Table 2, the transient iteration complexities of SPARKLE with mixed heterogeneity-correction techniques in different levels.

### Convergence analysis in deterministic scenarios

The following lemma gives the convergence rate of Algorithm 1 without a moving average when there is no sample noise:

**Lemma 20**.: _Suppose that Assumptions 1- 4 hold. If \(\sigma^{2}=0\), then there exist \(\alpha,\beta,\gamma\) and \(\theta=1\) such that_

\[\frac{1}{K+1}\sum_{k=0}^{K}\mathbb{E}\|\Phi(\bar{x}^{k})\|^{2}\] \[\lesssim \left(\frac{\kappa^{16}\|\mathbf{O}_{y}\|^{2}\|\mathbf{O}_{y}^{-1 }\|^{2}\|\mathbf{\Lambda}_{ya}\|^{2}\|\mathbf{\Lambda}_{yb}^{-1}\|^{2}\zeta_{ 0}^{y}}{1-\|\mathbf{\Gamma}_{y}\|}\right)^{\frac{1}{3}}\frac{1}{K}+\left( \frac{\kappa^{14}\|\mathbf{O}_{z}\|^{2}\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{ \Lambda}_{za}\|^{2}\|\mathbf{\Lambda}_{zb}^{-1}\|^{2}\zeta_{0}^{z}}{1-\| \mathbf{\Gamma}_{z}\|}\right)^{\frac{1}{3}}\frac{1}{K}\] \[+\left(\frac{\kappa^{8}\|\mathbf{O}_{x}\|^{2}\|\mathbf{O}_{x}^{-1 }\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}\|\mathbf{\Lambda}_{xb}^{-1}\|^{2}\zeta_{ 0}^{x}}{1-\|\mathbf{\Gamma}_{x}\|}\right)^{\frac{1}{3}}\frac{1}{K}+\widetilde{ C}_{\alpha}\frac{1}{K}.\]

_where \(\widetilde{C}_{\alpha}\) is a series of overheads which is defined below._

Proof.: Note that \(\sigma^{2}=0\) implies that \(L_{1}=\Theta(L^{2})\) when \(\alpha=\mathcal{O}(L_{\nabla\Phi}^{-1})\). Thus (87) implies that:

\[\frac{1}{K+1}\sum_{k=0}^{K}\mathbb{E}\|\Phi(\bar{x}^{k})\|^{2}\] \[\lesssim \frac{\Phi(\bar{x}_{0})-\inf\Phi}{\alpha(K+1)}\] \[+L^{2}\left[\kappa^{4}(\eta_{1}+\kappa^{2}L_{y^{*}}^{2}\eta_{2}) \alpha^{2}+\left(\frac{\alpha^{2}L_{z^{*}}^{2}}{\gamma^{2}\mu_{g}^{2}}+\frac{ \kappa^{4}\alpha^{2}}{\beta^{2}\mu_{g}^{2}}L_{y^{*}}^{2}\right)\right]\left( \frac{\Phi(\bar{x}_{0})-\inf\Phi}{\alpha(K+1)}\right)\] (97) \[+L^{2}\frac{\kappa^{6}\|\mathbf{O}_{y}\|^{2}\mathbb{E}\|\hat{ \mathbf{e}}_{y}^{0}\|^{2}}{n(K+1)(1-\|\mathbf{\Gamma}_{y}\|)}+L^{2}\frac{ \kappa^{4}\|\mathbf{O}_{z}\|^{2}\mathbb{E}\|\hat{\mathbf{e}}_{z}^{0}\|^{2}}{n( K+1)(1-\|\mathbf{\Gamma}_{z}\|)}+L^{2}\frac{\kappa^{6}\|\mathbf{O}_{x}\|^{2} \mathbb{E}\|\hat{\mathbf{e}}_{x}^{0}\|^{2}}{n(K+1)(1-\|\mathbf{\Gamma}_{x}\|)}\] \[+L^{2}\frac{\|z_{*}^{4}\|^{2}}{\mu_{g}\gamma(K+1)}+\frac{L^{2} \kappa^{4}}{K+1}\frac{1}{\beta\mu_{g}}\|\bar{y}_{0}-y^{*}(\bar{x}^{0})\|^{2}.\]Then we aim to choose the stepsize \(\alpha,\beta,\gamma\). Define:

\[\widetilde{C}_{\alpha}= L_{\nabla\Phi}+\kappa^{3}\frac{\|\mathbf{O}_{x}\|\|\mathbf{O}_{x}^{-1 }\|\|\mathbf{\Lambda}_{xa}\|L}{1-\|\mathbf{\Gamma}_{x}\|}+\kappa^{3}L\left( \frac{\|\mathbf{O}_{x}\|\|\mathbf{O}_{x}^{-1}\|\|\mathbf{\Lambda}_{xa}\|\| \mathbf{\Lambda}_{xb}^{-1}\|}{1-\|\mathbf{\Gamma}_{x}\|}\right)^{\frac{1}{2}}\] \[+\kappa^{4}\frac{L_{g,1}^{2}}{\mu_{g}}+\kappa^{4}\frac{\|\mathbf{ O}_{y}\|\|\mathbf{O}_{y}^{-1}\|\|\mathbf{\Lambda}_{ya}\|L_{g,1}}{1-\|\mathbf{ \Gamma}_{y}\|}+\kappa^{4}L_{g,1}\left(\frac{\kappa\|\mathbf{O}_{y}\|\|\mathbf{ O}_{y}^{-1}\|\|\mathbf{\Lambda}_{ya}\|\|\mathbf{\Lambda}_{yb}^{-1}\|}{1-\| \mathbf{\Gamma}_{y}\|}\right)^{\frac{1}{2}}\] \[+\kappa^{6}L\frac{\|\mathbf{O}_{z}\|\|\mathbf{O}_{z}^{-1}\|\| \mathbf{\Lambda}_{za}\|}{1-\|\mathbf{\Gamma}_{z}\|}+\kappa^{\frac{11}{2}}L \left(\frac{\|\mathbf{O}_{z}\|\|\mathbf{O}_{z}^{-1}\|\|\mathbf{\Lambda}_{za}\| \|\mathbf{\Lambda}_{zb}^{-1}\|}{1-\|\mathbf{\Gamma}_{z}\|}\right)^{\frac{1}{2}},\] \[\widetilde{\alpha}_{yb,2}= \left(\frac{1-\|\mathbf{\Gamma}_{y}\|}{\kappa^{13}\|\mathbf{O}_{y} \|^{2}\|\mathbf{O}_{y}^{-1}\|^{2}\|\mathbf{\Lambda}_{ya}\|^{2}\|\mathbf{ \Lambda}_{yb}^{-1}\|^{2}\zeta_{0}^{y}}\right)^{\frac{1}{3}},\] \[\widetilde{\alpha}_{zb,2}= \left(\frac{1-\|\mathbf{\Gamma}_{z}\|}{\kappa^{11}\|\mathbf{O}_{z} \|^{2}\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2}\|\mathbf{ \Lambda}_{zb}^{-1}\|^{2}\zeta_{0}^{z}}\right)^{\frac{1}{3}},\] \[\widetilde{\alpha}_{xb,2}= \left(\frac{1-\|\mathbf{\Gamma}_{x}\|}{\kappa^{5}\|\mathbf{O}_{x} \|^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}\|\mathbf{ \Lambda}_{xb}^{-1}\|^{2}\zeta_{0}^{z}}\right)^{\frac{1}{3}}.\]

Then there exist

\[\alpha=\Theta\left(\widetilde{C}_{\alpha}+\widetilde{\alpha}_{xb,2}^{-1}+ \widetilde{\alpha}_{yb,2}^{-1}+\widetilde{\alpha}_{zb,2}^{-1}\right)^{-1}, \beta=\Theta\left(\kappa^{4}\alpha\right),\gamma=\Theta\left(\kappa^{4}\alpha\right)\]

such that (45), (53), (56), (40), (58), (65), (82), and (84) hold. Then all previous lemmas hold. Then from (97) we have:

\[\frac{1}{K+1}\sum_{k=0}^{K}\mathbb{E}\|\Phi(\bar{x}^{k})\|^{2}\] \[\lesssim \frac{\kappa}{\alpha K}+\frac{\alpha^{2}\kappa^{14}\|\mathbf{O}_{ y}\|^{2}\|\mathbf{O}_{y}^{-1}\|^{2}\|\mathbf{\Lambda}_{ya}\|^{2}\|\mathbf{ \Lambda}_{yb}^{-1}\|^{2}\zeta_{0}^{y}}{K(1-\|\mathbf{\Gamma}_{y}\|)}\] \[+\frac{\alpha^{2}\kappa^{12}\|\mathbf{O}_{z}\|^{2}\|\mathbf{O}_{z} ^{-1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2}\|\mathbf{\Lambda}_{zb}^{-1}\|^{2} \zeta_{0}^{z}}{K(1-\|\mathbf{\Gamma}_{z}\|)}+\frac{\alpha^{2}\kappa^{6}\| \mathbf{O}_{x}\|^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2} \|\mathbf{\Lambda}_{xb}^{-1}\|^{2}\zeta_{0}^{x}}{K(1-\|\mathbf{\Gamma}_{x}\|)}\] \[\lesssim \left(\frac{\kappa^{16}\|\mathbf{O}_{y}\|^{2}\|\mathbf{O}_{y}^{-1} \|^{2}\|\mathbf{\Lambda}_{ya}\|^{2}\|\mathbf{\Lambda}_{yb}^{-1}\|^{2}\zeta_{0 }^{y}}{1-\|\mathbf{\Gamma}_{y}\|}\right)^{\frac{1}{3}}\frac{1}{K}+\left(\frac{ \kappa^{14}\|\mathbf{O}_{z}\|^{2}\|\mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{\Lambda }_{za}\|^{2}\|\mathbf{\Lambda}_{zb}^{-1}\|^{2}\zeta_{0}^{z}}{1-\|\mathbf{ \Gamma}_{z}\|}\right)^{\frac{1}{3}}\frac{1}{K}\] \[+\left(\frac{\kappa^{8}\|\mathbf{O}_{x}\|^{2}\|\mathbf{O}_{x}^{-1 }\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}\|\mathbf{\Lambda}_{xb}^{-1}\|^{2}\zeta_{0 }^{x}}{1-\|\mathbf{\Gamma}_{x}\|}\right)^{\frac{1}{3}}\frac{1}{K}+\widetilde{C }_{\alpha}\frac{1}{K}.\]

### Degenerating to single-level algorithms

We consider the bilevel problem with the following upper- and lower-level loss function on the \(i\)-th agent:

\[F_{i}(x,y,\phi)=F_{i}(x,\phi),\quad G_{i}(x,y,\xi)\equiv\frac{\|y\|^{2}}{2}.\]

Actually, this optimization problem with respect to \(x\) is single-level, since we have \(\mathbf{z}^{k}\equiv 0\), \(\mathbf{y}^{k}\equiv 0\), \(u_{i}^{k}=\nabla_{1}f_{i}(x_{i}^{k},\xi_{i}^{k})\) by induction. By taking \(\theta=1\), we get the following single-level algorithm framework for decentralized stochastic single-level algorithm. As we discuss in previous sections, it can recover various heterogeneity-correction algorithms, including GT, EXTRA and ED, by selecting specific \(\mathbf{A}_{x},\mathbf{B}_{x},\mathbf{C}_{x}\).

In this case, we have \(z_{k}^{\star}\equiv 0\), \(y_{k}^{\star}\equiv 0\). Notice that \(L_{y^{\star}}=0\), \(L_{z^{\star}}=0\). It gives

\[\eta_{2} =\mathcal{O}\left(\beta^{2}\frac{\|\mathbf{O}_{y}\|^{2}\|\mathbf{O }_{y}^{-1}\|^{2}\|\mathbf{\Lambda}_{ya}\|^{2}\|\mathbf{\Lambda}_{vb}^{-1}\|^{2 }}{(1-\|\mathbf{\Gamma}_{y}\|)^{2}}+\gamma^{2}\frac{\|\mathbf{O}_{z}\|^{2}\| \mathbf{O}_{z}^{-1}\|^{2}\|\mathbf{\Lambda}_{za}\|^{2}\|\mathbf{\Lambda}_{zb}^ {-1}\|^{2}}{(1-\|\mathbf{\Gamma}_{z}\|)^{2}}\right),\] \[\eta_{1} =\mathcal{O}\left(\eta_{2}+\alpha^{2}\left(1+\frac{(1-\theta)^{2 }}{\theta^{2}\|\mathbf{\Lambda}_{xb}^{-1}\|^{2}}\right)\frac{\|\mathbf{O}_{x} \|^{2}\|\mathbf{\Omega}_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}\|\mathbf{ \Lambda}_{xb}^{-1}\|^{2}}{(1-\|\mathbf{\Gamma}_{x}\|)^{2}}\right).\]

If we take

\[\alpha\lesssim\min\left\{1,\frac{1-\|\mathbf{\Gamma}_{x}\|}{\|\mathbf{O}_{x} \|\|\mathbf{O}_{x}^{-1}\|\|\mathbf{\Lambda}_{xa}\|},\left(\frac{1-\|\mathbf{ \Gamma}_{x}\|}{\|\mathbf{O}_{x}\|\|\mathbf{O}_{x}^{-1}\|\|\mathbf{\Lambda}_{xa }\|\|\mathbf{\Lambda}_{xb}^{-1}\|}\right)^{\frac{1}{2}}\right\}\]

and \(\theta=1\), \(\beta\to 0\), \(\gamma\to 0\), then (45), (53), (56), (40), (58), (65), (82), and (84) hold. Thus all previous lemmas hold. Then (87) transforms into

\[\frac{1}{K+1}\sum_{k=0}^{K}\mathbb{E}\|\Phi(\bar{x}^{k})\|^{2}\] \[\lesssim \frac{f(\bar{x}_{0})-\inf f}{\alpha(K+1)}+\frac{1}{n}\left(\theta (1-\theta)+\alpha\theta^{2}\right)\sigma_{f,1}^{2}+\frac{(1-\theta)^{2}}{ \theta(K+1)}\|\nabla f\left(\bar{x}^{0}\right)\|^{2}\] \[+\eta_{1}\alpha^{2}\left(\frac{f(\bar{x}_{0})-\inf f}{\alpha(K+ 1)}+\frac{\theta}{n}\sigma_{f,1}^{2}\right)+\frac{\|\mathbf{O}_{x}\|^{2} \mathbb{E}\|\hat{\mathbf{e}}_{x}^{0}\|^{2}}{n(K+1)(1-\|\mathbf{\Gamma}_{x}\| )}\] \[+\frac{\alpha^{2}\|\mathbf{O}_{x}\|^{2}\|\mathbf{O}_{x}^{-1}\|^{ 2}\|\mathbf{\Lambda}_{xa}\|^{2}}{n(1-\|\mathbf{\Gamma}_{x}\|)^{2}(K+1)}\left[ \frac{1-\theta}{\theta}\sum_{i=1}^{n}\left\|\nabla f_{i}(\bar{x}^{0})\right\| ^{2}\right]\] \[+\frac{1}{n}\left[\alpha^{2}\theta\left(\theta+\frac{1-\theta}{1- \|\mathbf{\Gamma}_{x}\|}\right)\frac{\|\mathbf{O}_{x}\|^{2}\|\mathbf{O}_{x}^{ -1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}n}{1-\|\mathbf{\Gamma}_{x}\|}\sigma_{f, 1}^{2}.\right.\]

It follows that

\[\frac{1}{K+1}\sum_{k=0}^{K}\mathbb{E}\|\Phi(\bar{x}^{k})\|^{2}\] \[\lesssim \frac{f(\bar{x}_{0})-\inf f}{\alpha(K+1)}+\frac{\alpha\sigma_{f,1 }^{2}}{n}+\left(\alpha^{4}\frac{\|\mathbf{O}_{x}\|^{2}\|\mathbf{O}_{x}^{-1}\|^ {2}\|\mathbf{\Lambda}_{xa}\|^{2}\|\mathbf{\Lambda}_{xb}^{-1}\|^{2}}{(1-\| \mathbf{\Gamma}_{x}\|)^{2}}\right)\left(\frac{f(\bar{x}_{0})-\inf f}{\alpha( K+1)}+\frac{1}{n}\sigma_{f,1}^{2}\right)\] \[+\frac{\|\mathbf{O}_{x}\|^{2}\mathbb{E}\|\hat{\mathbf{e}}_{x}^{ \mathbf{e}}\|^{2}}{n(K+1)(1-\|\mathbf{\Gamma}_{x}\|)}+\alpha^{2}\frac{\| \mathbf{O}_{x}\|^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2} \|\mathbf{\Lambda}_{xb}^{-1}\|^{2}\sigma_{f,1}^{2}}{1-\|\mathbf{\Gamma}_{x}\| }\sigma_{f,1}^{2}\] \[\lesssim \frac{f(\bar{x}_{0})-\inf f}{\alpha(K+1)}+\frac{\alpha\sigma_{f,1 }^{2}}{n}+\alpha^{2}\frac{\|\mathbf{O}_{x}\|^{2}\|\mathbf{\Omega}_{x}^{-1}\|^ {2}\|\mathbf{\Lambda}_{xa}\|^{2}}{1-\|\mathbf{\Gamma}_{x}\|}\sigma_{f,1}^{2}\] \[+\frac{\alpha^{2}\|\mathbf{O}_{x}\|^{2}\|\mathbf{O}_{x}^{-1}\|^{2} \|\mathbf{\Lambda}_{xa}\|^{2}\|\mathbf{\Lambda}_{xb}^{-1}\|^{2}\zeta_{0}^{ \alpha}}{(K+1)(1-\|\mathbf{\Gamma}_{x}\|)}+\frac{\alpha^{4}\|\mathbf{O}\|_{x} ^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}\|\mathbf{ \Lambda}_{xb}^{-1}\|^{2}\sigma_{f,1}^{2}}{n(1-\|\mathbf{\Gamma}_{x}\|)^{2}}.\] (98)Like (88), we take

\[C_{0} =1+\frac{\|\mathbf{O}_{x}\|\|\mathbf{O}_{x}^{-1}\|\|\mathbf{\Lambda} _{xa}\|}{1-\|\mathbf{\Gamma}_{x}\|}+\left(\frac{\|\mathbf{O}_{x}\|\|\mathbf{O}_ {x}^{-1}\|\|\mathbf{\Lambda}_{xa}\|\|\mathbf{\Lambda}_{xb}^{-1}\|}{1-\|\mathbf{ \Gamma}_{x}\|}\right)^{\frac{1}{2}},\] \[\alpha_{1} =\sqrt{\frac{n}{K\sigma_{f,1}^{2}}},\quad\alpha_{2}=\left(\frac{1 -\|\mathbf{\Gamma}_{x}\|}{K\|\mathbf{O}_{x}\|^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\| \mathbf{\Lambda}_{xa}\|^{2}\sigma_{f,1}^{2}}\right)^{\frac{1}{3}},\] \[\alpha_{3} =\left(\frac{1-\|\mathbf{\Gamma}_{x}\|}{\|\mathbf{O}_{x}\|^{2}\| \mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}\|\mathbf{\Lambda}_{xb} ^{-1}\|^{2}\zeta_{0}^{x}}\right)^{\frac{1}{3}},\] \[\alpha_{4} =\left(\frac{n(1-\|\mathbf{\Gamma}_{x}\|)^{2}}{K\|\mathbf{O}_{x} ^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}\|\mathbf{\Lambda} _{xb}^{-1}\|^{2}\sigma_{f,1}^{2}}\right)^{\frac{1}{3}},\] \[\alpha =\Theta\left(C_{0}+\frac{1}{\alpha_{1}}+\frac{1}{\alpha_{2}}+ \frac{1}{\alpha_{3}}+\frac{1}{\alpha_{4}}\right)^{-1}.\]

Substituting these values into (98), we get

\[\frac{1}{K+1}\sum_{k=0}^{K}\mathbb{E}\|\Phi(\bar{x}^{k})\|^{2} \lesssim\frac{\sigma_{f,1}}{\sqrt{nK}}+\left(\frac{\|\mathbf{O}_{x}\|^{2}\| \mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2}\sigma_{f,1}^{2}}{1-\| \mathbf{\Gamma}_{x}\|}\right)^{\frac{1}{3}}K^{-2/3}+\frac{C_{0}}{K}\] \[+\left(\frac{\|\mathbf{O}_{x}\|^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\| \mathbf{\Lambda}_{xa}\|^{2}\|\mathbf{\Lambda}_{xb}^{-1}\|^{2}\zeta_{0}^{x}}{ (1-\|\mathbf{\Gamma}_{x}\|)}\right)^{\frac{1}{3}}\frac{1}{K}+\left(\frac{\| \mathbf{O}\|_{x}^{2}\|\mathbf{O}_{x}^{-1}\|^{2}\|\mathbf{\Lambda}_{xa}\|^{2} \|\mathbf{\Lambda}_{xb}^{-1}\|^{2}\sigma_{f,1}^{2}}{n(1-\|\mathbf{\Gamma}_{ x}\|)^{2}}\right)^{\frac{1}{3}}K^{-4/5}.\]

Like Lemma 18, we get the transient iterating complexity for Algorithm 3 is

\[\left\{n^{3}\left(\frac{\|\mathbf{O}_{x}\|^{2}\|\mathbf{O}_{x}^{-1}\|^{2}}{1- \|\mathbf{\Gamma}_{x}\|}\right)^{2}\|\mathbf{\Lambda}_{xa}\|^{2},n\left(\frac {\|\mathbf{O}_{x}\|\|\mathbf{O}_{x}^{-1}\|\|\mathbf{\Lambda}_{xb}^{-1}\|}{1- \|\mathbf{\Gamma}_{x}\|}\right)^{\frac{4}{3}}\|\mathbf{\Lambda}_{xa}\|,n \right\}.\]

Substituting the value of relevant norms in Table 4, we get the transient iteration complexity for GT, EXTRA, ED are

\[\mathcal{O}\left(\max\left\{\frac{n^{3}}{(1-\rho)^{2}},\frac{n}{(1-\rho)^{8/3 }}\right\}\right),\,\mathcal{O}\left(\frac{n^{3}}{(1-\rho)^{2}}\right),\, \mathcal{O}\left(\frac{n^{3}}{(1-\rho)^{2}}\right)\]

respectively, where \(\rho:=\rho(\mathbf{W}_{x})\). These upper bounds are the same as the state-of-the-art results shown in Table 1. It indicates that our analysis accurately captures the impacts of updates at each level on the convergence results.

## Appendix D Experimental details

In this section, we provide the details of our numerical experiments discussed in Section 4. We also provide addition experimental results which are not mentioned in the main text due to the space limitation. For all GT variants, we focus on one typical representative, ATC-GT, in our experiments, which we denote as GT for brevity. All experiments described in this section were run on an NVIDIA A100 server.

### Synthetic bilevel optimization

Here, we consider problem (1) whose upper- and lower level loss functions on the \(i\)-th agents (\(1\leq i\leq N\)) are denoted as:

\[f_{i}(x,y) =\mathbb{E}_{A_{i},b_{i}}\left[\|A_{i}y-b_{i}\|^{2}\right],\] \[g_{i}(x,y) =\mathbb{E}_{A_{i},b_{i}}\left[\|A_{i}y-x\|^{2}+C_{r}\left\|y\right\| ^{2}\right],\]where \(x\in\mathbb{R}^{D},y\in\mathbb{R}^{K}\) and \(C_{r}\) denotes a fixed regularization parameter. For each agent \(i\), we firstly generate the local solution \(y_{i}^{*},x_{i}^{*}\) as \(y_{i}^{*}=y^{*}+\zeta_{i}\) and \(x_{i}^{*}=A^{*}b^{*}+\xi_{i}\), where \(x^{*}\sim\mathcal{N}(0,I_{K})\) is a randomly generated vector, each element of \(A^{*}\) is independently sampled from \(\mathcal{N}(0,9)\). The observation \((A_{i},b_{i})\) on agent \(i\) is generated in a streaming manner by \(A_{i}=A^{*}+\phi_{i}\), \(b_{i}=x_{i}^{*}+\psi_{i}\), in which each element of \(\phi_{i}\in\mathbb{R}^{K\times D}\) and \(\psi_{i}\in\mathbb{R}^{D}\) are independently generated by \(\mathcal{N}(0,\sigma_{g}^{2})\). The terms \(\xi_{i}\sim\mathcal{N}(0,\sigma_{h}^{2}I_{K})\) and \(\zeta_{i}\sim\mathcal{N}(0,\sigma_{h}^{2}I_{D})\) control the heterogeneity of data distributions across different agents.

We set \(D=20,K=10,\sigma_{g}=0.001,C_{r}=0.001\). Then we set \(\sigma_{h}=0.5\) to represent severe heterogeneity across agents and \(\sigma_{h}=0.1\) for mild heterogeneity. We run D-SOBA, SPARKLE-GT, SPARKLE-ED, and SPARKLE-EXTRA over Ring, 2D-Torus [37], and fully connected networks with \(N=64\) agents. The moving-average term \(\theta=0.1\) and the step-size at the \(t\)-th iteration are \(\alpha_{t}=\beta_{t}=\gamma_{t}=1/(500+0.01t)\). The batch size is 10.

Fig. 4 illustrates the averaged estimation error \(\sum_{i=1}^{N}\left\|x_{i}^{(t)}-x^{*}\right\|^{2}\) of the mentioned algorithms with different communication topology and data heterogeneity. It is observed that SPARKLE with ED, EXTRA, GT achieve better convergence performances with decentralized communication networks. Meanwhile, SPARKLE-ED and SPARKLE-EXTRA are more robust to data heterogeneity and the sparsity of network topology than SPARKLE-GT. All the results are consistent with our theoretical results.

### Hyper-cleaning on FashionMNIST dataset

Here, we consider a data hyper-clean problem [44] on FashionMNIST dataset [48]. The FashionMNIST dataset consists of 60000 images for training and 10000 images for testing and we randomly split 50000 training images into a training set and the other 10000 images into a validation set.

The data hyper-cleaning problem aims to train a classifier from a corrupted dataset, in which the label of each training data is replaced by a random class number with a probability \(p\) (_i.e._ the corruption rate). It can be considered as a stochastic bilevel problem (1) whose upper- and lower-level loss functions on the \(i\)-th agents (\(1\leq i\leq n\)) are formulated as:

\[f_{i}(x,y) =\frac{1}{\left|\mathcal{D}_{val}^{(i)}\right|}\sum_{(\xi_{c}, \zeta_{c})\in D_{val}^{(i)}}L(\phi(\xi_{c};y),\zeta_{c}),\] \[g_{i}(x,y) =\frac{1}{\left|\mathcal{D}_{tr}^{(i)}\right|}\sum_{(\xi_{c}, \zeta_{c})\in\mathcal{D}_{tr}^{(i)}}\sigma(x_{e})L(\phi(\xi_{c};y),\zeta_{c} )+C\left\|y\right\|^{2},\]

Figure 4: The estimation error of D-SOBA, SPARKLE-GT, SPARKLE-ED, and SPARKLE-EXTRA under different networks and data heterogeneity.

where \(\phi\) denotes a training model while \(y\) denotes its parameters, \(L\) denotes the cross-entropy loss function and \(\sigma(x)=(1+e^{-x})^{-1}\) is the sigmoid function. \(\mathcal{D}_{tr}^{(i)}\) and \(\mathcal{D}_{val}^{(i)}\) denotes the training and validation set of the \(i\)-th agent, respectively. \(C>0\) is a fixed regularization parameter.

**Data generation and experiment settings.** In this experiment, we let \(\phi\) be a two-layer MLP network with a 300-dim hidden layer and ReLU activation while \(y\) denotes its parameters. For \(1\leq i\leq 10\), we sample a probability distribution \(\mathcal{P}_{i}\) randomly by Dirichlet distribution with parameters \(\alpha=0.1\). The training and validation images with label \(i\) are sent to different agents according the probability distribution \(\mathcal{P}_{i}\). Then \(\mathcal{D}_{tr}^{(i)}\) and \(\mathcal{D}_{val}^{(i)}\) are generated sufficiently heterogeneous [32]. We set \(C=0.001\). The batch size is set to 50.

**Convergence performances with different corruption rates.** We set the moving-average term \(\theta_{k}=0.2\) and run D-SOBA [29], MA-DSBO-GT [10], MDBO [21] SPARKLE-GT, SPARKLE-ED, SPARKLE-EXTRA, SPARKLE-ED-GT, and SPARKLE-EXTRA-GT on an Adjusted Ring graph with \(n=10\) agents and \(p=0.1,0.2,0.3\) separately. The step-sizes for all the algorithms are set to \(\alpha_{k}=\beta_{k}=\gamma_{k}=0.03\) and the term \(\eta\) in MDBO is set to 0.5. The weight matrix of Adjust Ring \(W=[w_{ij}]_{n\times n}\) satisfies:

\[w_{ij}=\begin{cases}a,&\text{if }j=i,\\ \frac{1-a}{2},&\text{if }(j-i)\%n=\pm 1,\\ 0,&\text{else}.\end{cases}\]

Moreover, we run SPARKLE with ED in the lower level and auxiliary variable and gradient tracking in the upper level (i.e. SPARKLE-ED-GT) as well as SPARKLE with EXTRA in the lower level and auxiliary variable and gradient tracking in the upper level (i.e. SPARKLE-EXTRA-GT) and compare their test accuracy with the other four algorithms.

Figure 1 shows that SPARKLE-ED and SPARKLE-EXTRA outperforms in different cases than SPARKLE-GT. Meanwhile, SPARKLE-EXTRA, SPARKLE-EXTRA-GT achieve similar test accuracy, as do those for SPARKLE-ED and SPARKLE-ED-GT, which matches our theoretical results in transient iteration analysis. Figure 5 presents the times of gradient evaluation for different test accuracies of these algorithms at \(p=0.2,0.3\), demonstrating similar results.

**Influence of network topology.** We set the corruption rate \(p=0.3\), the step sizes \(\alpha_{k}=\beta_{k}=\gamma_{k}=0.02\), and the moving-average term \(\theta_{k}=0.2\). Then we run SPARKLE-EXTRA and SPARKLE-EXTRA-GT on a network containing \(n=10\) nodes with different topologies in the following two cases:

* **Fixed upper, varied lower**: \(x\) communicates through a five-peer graph; \(y,z\) communicate through different adjusted rings with \(\rho=0.647,0.828,0.924,0.990\).
* **Fixed lower, varied upper**: \(y,z\) communicate through a five-peer graph; \(x\) communicates through different adjusted rings with \(\rho=0.647,0.828,0.924,0.990\).

Figure 5: Hypergradient evaluation times for required test accuracy in hyper-cleaning problem. (Left: \(p=0.2\); Right: \(p=0.3\))

The weight matrix of five-peer graph \(W=[w_{ij}]_{n\times n}\) satisfies:

\[w_{ij}=\begin{cases}0.2,&\text{if }(j-i)\%n=0,\pm 1,\pm 2,\\ 0,&\text{else}.\end{cases}\]

Figure 6 shows the average test accuracy of both SPARKLE-EXTRA and SPARKLE-EXTRA-GT over 10 trials. It indicates that the test accuracy decays with increasing spectral gap of topologies related to \(y,z\) while the topology of \(x\) is fixed during the whole iterations. However, such convergence gap becomes milder when the topologies of \(y,z\) are fixed and that of \(x\) varies. This phenomenon supports our theoretical findings, which suggest that the transient iteration complexity is more sensitive to the network topologies of \(y,z\) than to that of \(x\).

**Influence of moving-average iteration on convergence.** Moreover, for \(\theta_{t}=0.05,0.2,0.3\), we run SPARKLE-GT, SPARKLE-ED, SPARKLE-EXTRA, SPARKLE-ED-GT, and SPARKLE-EXTRA-GT on an Adjusted Ring graph with \(n=10\) agents, \(\alpha_{k}=\beta_{k}=\gamma_{k}=0.03\) and \(p=0.3\) for 3000 iterations. We obtain the average test accuracy of the last 40 iterations over 10 trials, and present the mean and standard deviation during the different trials in Table 5. We can observe that most algorithms achieve the highest test accuracy when \(\theta=0.2\), which may prove that a suitable \(\theta\) can benefit the test accuracy in hyper-cleaning problems.

\begin{table}
\begin{tabular}{c c c c} \hline
**Algorithm** & \(\theta=0.05\) & \(\theta=0.2\) & \(\theta=0.3\) \\ \hline SPARKLE-GT & \(\mathbf{0.7080\pm 0.0215}\) & \(0.7045\pm 0.0126\) & \(0.7064\pm 0.0113\) \\ SPARKLE-ED & \(0.7096\pm 0.0074\) & \(\mathbf{0.7113\pm 0.0047}\) & \(0.7110\pm 0.0081\) \\ SPARKLE-EXTRA & \(0.7190\pm 0.0103\) & \(\mathbf{0.7277\pm 0.0090}\) & \(0.7243\pm 0.0028\) \\ SPARKLE-ED-GT & \(0.7064\pm 0.0063\) & \(\mathbf{0.7178\pm 0.0037}\) & \(0.7162\pm 0.0041\) \\ SPARKLE-EXTRA-GT & \(0.7198\pm 0.0051\) & \(\mathbf{0.7262\pm 0.0058}\) & \(0.7247\pm 0.0048\) \\ \hline \end{tabular}
\end{table}
Table 5: Mean and standard deviation of the average test accuracy of last 40 iterations during 10 trials with different moving-average terms

Figure 6: The average test accuracy of SPARKLE-EXTRA and SPARKLE-EXTRA-GT on hyper-cleaning with different communicating strategy of \(x,y,z\).

### Distributed policy evaluation in reinforcement learning

Following the result of [52], we consider a multi-agent MDP problem in reinforcement learning on a distributed setting with \(n\) agents. Denote \(\mathcal{S}\) as the state space. Suppose that the value function in each state \(s\in\mathcal{S}\) is a linear function \(V(s)=\phi_{s}^{\top}x\), where \(\phi_{s}\in\mathbb{R}^{m}\) is a feature and \(x\in\mathbb{R}^{m}\) is a parameter. To obtain the optimal solution \(x^{*}\), we consider the following Bellman minimization problem:

\[\min_{x\in\mathbb{R}^{m}}\quad F(x)=\frac{1}{n}\sum_{i=1}^{n}\left[\frac{1}{2| \mathcal{S}|}\sum_{s\in\mathcal{S}}\left(\phi_{s}^{\top}x-\mathbb{E}_{s^{ \prime}}\left[r^{i}(s,s^{\prime})+\gamma\phi_{s^{\prime}}^{\top}x|s\right] \right)^{2}\right]\]

where \(r^{i}(s,s^{\prime})\) denotes the reward incurred from transition \(s\) to \(s^{\prime}\) on the \(i\)-th agent, \(\gamma\in(0,1)\) denotes the discount factor. The expectation is taken over all random transitions from state \(s\) to \(s^{\prime}\). It can be viewed as a bilevel optimization problem with the following upper- and lower-level loss:

\[f_{i}(x,y) =\frac{1}{2|\mathcal{S}|}\sum_{s\in\mathcal{S}}(\phi_{s}^{\top}x- y_{s})^{2},\] \[g_{i}(x,y) =\sum_{s\in\mathcal{S}}\left(y_{s}-\mathbb{E}_{s^{\prime}}\left[ r^{i}(s,s^{\prime})+\gamma\phi_{s^{\prime}}^{\top}x|s\right]\right)^{2},\]

where \(y=(y_{1},\cdots,y_{|\mathcal{S}|})^{\top}\in\mathbb{R}^{|\mathcal{S}|}\). In our experiment, we set the number of states \(|\mathcal{S}|=200\) and \(m=10\). For each \(s\in\mathcal{S}\), we generate its feature \(\phi_{s}\sim U[0,1]^{m}\). The non-negative transition probabilities are generated randomly and standardized to satisfy \(\sum_{s^{\prime}\in\mathcal{S}}p_{s,s^{\prime}}=1\). The mean reward \(\bar{r}^{i}(s,s^{\prime})\) are independently generated from the uniform distribution \(U[0,1]\). In each iteration, the stochastic reward \(r^{i}(s,s^{\prime})\sim\mathcal{N}(\bar{r}^{i}(s,s^{\prime}),0.02^{2})\).

For \(n=10,20\), we run SPARKLE-ED and SPARKLE-EXTRA as well as existing decentralized SBO algorithms MDBO [21] and SLDBO [16] (here we use the stochastic gradient instead of deterministic gradient) over a Ring graph. For MDBO, the number of Hessian-inverse estimation iterations is set to \(5\). The step sizes are 0.03 for all methods. Figure 3 illustrates the upper-level loss against samples generated by one agent for 10 independent trials. Table 6 shows the average training loss of the last 500 iterations for 10 independent trials of the four decentralized SBO algorithms as well as single-level ED [56] (For bilevel algorithms, _training loss_ means the upper-level loss here).

\begin{table}
\begin{tabular}{c c c} \hline
**Algorithm** & \(N=10\) & \(N=20\) \\ \hline SPARKLE-**ED** & \(0.2781\pm 1.09\times 10^{-3}\) & \(0.3198\pm 3.21\times 10^{-3}\) \\ SPARKLE-**EXTRA** & \(0.2743\pm 0.88\times 10^{-3}\) & \(0.3207\pm 2.94\times 10^{-3}\) \\
**MDBO** & \(1.0408\pm 4.51\times 10^{-3}\) & \(1.3293\pm 8.38\times 10^{-3}\) \\
**SLDBO** & \(0.4132\pm 1.18\times 10^{-3}\) & \(0.8374\pm 2.47\times 10^{-3}\) \\
**Single-level ED** & \(0.2948\pm 0.92\times 10^{-3}\) & \(0.3164\pm 3.12\times 10^{-3}\) \\ \hline \end{tabular}
\end{table}
Table 6: The average training loss of the last 500 iterations for 10 independent trials in the distributed policy evaluation.

Figure 7: The test loss against samples generated by one agent of different algorithms in the policy evaluation. (Left: \(n=20\), Right: \(n=10\).)Both Figure 3 and Table 6 demonstrate that SPARKLE-ED and SPARKLE-EXTRA converge faster than other methods.

Finally, we create a fixed "test set" with 10000 sample generated from \(\mathcal{S}\). Figure 7 shows the loss on the test set of SPARKLE-ED, SPARKLE-EXTRA, SLDBO, MDBO and single-level ED algorithm, demonstrating the superior performance of SPARKLE compared to other decentralized SBO algorithms.

### Decentralized meta-learning

We consider a meta-learning problem as described in [18]. There are \(R\) tasks \(\{\mathcal{T}_{s},s=1,\cdots,R\}\). Each task \(\mathcal{T}_{s}\) has its own loss function \(L(x,y_{s},\xi)\), where \(\xi_{s}\) represents a stochastic sample drawn from the data distribution \(\mathcal{D}_{s}\), \(y_{s}\) denotes the task-specific parameters and \(x\) denotes the global parameters shared by all the tasks. In meta-learning problem, we aim to find the parameters \((x^{*},y_{1}^{*},\cdots,y_{R}^{*})\) that minimizes the loss function across all \(R\) tasks, _i.e._,

\[\min_{x,y_{1},\cdots,y_{R}}l(x,y_{1},\cdots,y_{R})=\frac{1}{R}\sum_{s=1}^{R} \mathbb{E}_{\xi\sim\mathcal{D}_{s}}\left[L(x,y_{s},\xi)\right].\] (102)

The problem (102) can be formulated as a decentralized SBO problem with heterogeneous data distributions across \(N\) nodes. For \(i=1,2,\cdots,N\), let \(\mathcal{D}_{s,i}^{\text{train}}\) and \(\mathcal{D}_{s,i}^{\text{val}}\) denote the training and validation datasets for the \(s\)-th task \(\mathcal{T}_{s}\) received by node \(i\) respectively. We can then address the meta-learning problem by minimizing (1), with the upper- and lower-level loss functions defined as:

\[f_{i}(x,y) =\frac{1}{R}\sum_{s=1}^{R}\mathbb{E}_{\xi\sim\mathcal{D}_{s,i}^{ \text{val}}}\left[L(x,y_{s},\xi)\right],\] \[g_{i}(x,y) =\frac{1}{R}\sum_{s=1}^{R}\left[\mathbb{E}_{\xi\sim\mathcal{D}_{ s,i}^{\text{train}}}\left[L(x,y_{s},\xi)\right]+\mathcal{R}(y_{s})\right],\]

where \(L\) denotes the cross-entropy loss and \(\mathcal{R}(y_{s})=C_{r}\left\|y_{s}\right\|^{2}\) is a strongly convex regularization function.

In this experiment, we compare SPARKLE-ED with D-SOBA [29] and MAML [18] in a decentralized communication setting over a 5-way 5-shot task across a network of \(N=8\) nodes connected by Ring graph. The dataset used is miniImageNet [47], derived from ImageNet [42], which comprises 100 classes, each containing 600 images of size \(84\times 84\). We set \(R=2000\) and partition these classes into 64 for training, 16 for validation, and 20 for testing. For the training and validation classes, the data is split according to a Dirichlet distribution with parameter \(\alpha=0.1\)[32]. We utilize a four-layer CNN with four convolution blocks, where each block sequentially consists of a \(3\times 3\) convolution with 32 filters, batch normalizationm, ReLU activation, and \(2\times 2\) max pooling. The batch size is 32, and \(C_{r}=0.001\). The parameters of the last linear layer are designated as task-specific, while the other parameters are shared globally. For SPARKLE and D-SOBA, the step-sizes are

Figure 8: The accuracy on training and testing set of different algorithms for the meta-learning problem.

\(\beta=\gamma=0.1\) and \(\alpha=0.01\). For MAML, the inner step-size is 0.1 and the outer step-size is 0.001, and the number of inner-loop steps as 3. For all algorithms, the task number is set to 32. And we only repeat the experiment only once due to the time limitation. Figure 8 shows the average accuracy on the training dataset for all nodes, as well as the test accuracy of the three algorithms. We observe that SPARKLE-ED outperforms other algorithms, demonstrating the efficiency of SPARKLE in decentralized meta-learning problems.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Refer to Abstract and Introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Refer to Section Conclusions. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: Refer to Section Assumptions for our assumptions, and Appendix for detailed proof. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Refer to Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [No] Justification: We may consider making data and code openly accessible when it is deemed necessary. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Refer to Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We show error bars in experiments where we consider them essential. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Refer to Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification:Our research conforms, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: There is no societal impact of our work. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: There is no such risk in the paper. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We comply with the licenses of existing assets used in the paper and provide necessary references. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.