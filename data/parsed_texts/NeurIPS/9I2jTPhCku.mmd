# An Information-Theoretic Understanding of

Maximum Manifold Capacity Representations

 Berivan Isik

Electrical Engineering

Stanford University

berivan.isik@stanford.edu

&Victor Lecomte1

Computer Science

Stanford University

vlecomte@stanford.edu

&Rylan Schaeffer1

Computer Science

Stanford University

rschaef@stanford.edu

&Mikail Khona

Physics

MIT

mikail@mit.edu

&Yann LeCun

Data Science & FAIR

NYU & Meta AI

NYU & Meta AI

ravid.shwartz.ziv@nyu.edu

&Marky Gromov

Physics & FAIR

UMD & Meta AI

gromovand@meta.com

&Sanmi Koyejo

Computer Science

Stanford University

sanmi@cs.stanford.edu

Footnote 1: footnotemark:

###### Abstract

Maximum Manifold Capacity Representations (MMCR) is a recent multi-view self-supervised learning (MVSSL) method that matches or surpasses other leading MVSSL methods. MMCR is interesting for at least two reasons. Firstly, MMCR is an oddity in the zoo of MVSSL methods: it is not (explicitly) contrastive, applies no masking, performs no clustering, leverages no distillation, and does not (explicitly) reduce redundancy. Secondly, while many self-supervised learning (SSL) methods originate in information theory, MMCR distinguishes itself by claiming a different origin: a statistical mechanical characterization of the geometry of linear separability of data manifolds. However, given the rich connections between statistical mechanics and information theory, and given recent work showing how many SSL methods can be understood from an information-theoretic perspective, we conjecture that MMCR can be similarly understood from an information-theoretic perspective. In this paper, we leverage tools from high dimensional probability and information theory to demonstrate that an optimal solution to MMCR's nuclear norm-based objective function is the same optimal solution that maximizes a well-known lower bound on mutual information between views.

Multi-view self-supervised learning (MVSSL; also known as Joint Embedding SSL) is a powerful approach to unsupervised learning. The core idea is to create multiple transformations, or "views", of unsupervised data, then use these transformed data in a supervised-like manner to learn generally useful representations. MVSSL methods are diverse but can be loosely grouped into a number of different families: (1) contrastive, such as CPC [20], MoCo 1 [19], SimCLR [8], MoCo 2 [9], CMC [22], RPC [23] and TiCo [29]; (2) clustering such as Noise-as-Targets [4], DeepCluster [5], Self-Labeling [1], Local Aggregation [30], SwAV [6]; (3) distillation/momentum such as BYOL [18], DINO [7], SimSiam [10], TiCo [29]; (4) redundancy reduction such as Barlow Twins [28], VICReg [3], TiCo [29]. Many MVSSL methods either explicitly originate from information theory [20, 2] or can be understood from an information-theoretic perspective [25, 26, 21, 15].

Recently, [27] proposed a new MVSLL method named Maximum Manifold Capacity Representations (MMCR) that achieves similar (if not superior) performance to leading MVSSL methods. MMCR is interesting for at least two reasons. Firstly, MMCR does not fit neatly into any of the MVSSL families: it is not (explicitly) contrastive, it applies no masking, it performs no clustering, it leverages no distillation, and it does not (explicitly) reduce redundancy. Secondly, unlike many MVSSL methods that originate in information theory, MMCR distances itself, writing that estimating mutual information in high dimensions has proven difficult and that more closely approximating mutual information may not improve representations; MMCR instead claims an origin in the statistical mechanical characterization of the geometry of linear separability of data manifolds.

In this work, we seek to better understand what solutions the MMCR loss function incentivizes and how it relates to other well-known MVSSL methods. Our contributions are specifically as follows:

1. We derive a distribution of embeddings that provably minimizes the MMCR loss with high probability by leveraging tools from high dimensional probability.
2. We connect this distribution to information theory by showing it maximizes a well-known variational lower bound on the mutual information between multiple views' embeddings.

## 1 Background

Multi-View Self-Supervised Learning (MVSSL)Let \(f_{\theta}:\mathcal{X}\rightarrow\mathcal{Z}\) denote our neural network with parameters \(\theta\). Suppose we have a dataset \(\{\mathbf{x}_{n}\}_{n=1}^{N}\) and a set of transformations (sometimes called augmentations, or "views") \(\mathcal{T}\) such as color jittering, Gaussian blur, solarization, etc. For each datum \(\mathbf{x}_{n}\) in a batch of inputs, we sample \(K\) transformations \(t^{(1)},t^{(2)},...,t^{(K)}\sim\mathcal{T}\), then transform the datum: \(t^{(1)}(\mathbf{x}_{n}),...,t^{(K)}(\mathbf{x}_{n})\). We feed these transformed data into the network and obtain _embeddings_ or _representations_:

\[\mathbf{z}_{n}^{(k)}\ \overset{\text{def}}{=}\ f_{\theta}(t^{(k)}(\mathbf{x}_{n})) \in\mathcal{Z}. \tag{1}\]

In practice, \(\mathcal{Z}\) is commonly \(\mathbb{R}^{D}\) or the \(D\)-dimensional hypersphere \(\mathbb{S}^{D-1}\overset{\text{def}}{=}\{\mathbf{z}\in\mathbb{R}^{D}:\mathbf{z}^{T}\bm {z}=1\}\). Given that this work will later touch on information theory, we need notation to refer to the random variables; we use \(Z_{n}^{(k)}\) to denote the random variable for the embedding whose realization is \(\mathbf{z}_{n}^{(k)}\), and \(X_{n}^{(k)}\) to denote the random variable for the transformed datum whose realization is \(t^{(k)}(\mathbf{x}_{n})\).

Maximum Manifold Capacity Representations (MMCR)MMCR [27] originates from classical results regarding performance of linear binary classifiers [12; 16; 17]. Consider \(N\) points in dimension \(D\), with arbitrarily assigned binary class labels; _what is the probability that a linear binary classifier will be able to successfully classify the points?_ Statistical mechanical calculations reveal that in the _thermodynamic_ limit (\(N,D\rightarrow\infty\); \(N/D\rightarrow\alpha\in(0,\infty)\)), a phase transition occurs at capacity \(\alpha_{c}=2\). More precisely, if \(\alpha<\alpha_{c}\), the linear binary classifier will succeed with probability \(1\); but if \(\alpha>\alpha_{c}\), the classifier will succeed with probability \(0\). MMCR is based on an extension of this result from points to manifolds [11]. MMCR proceeds in the following manner: MMCR takes the embeddings output by the network and normalizes them to lie on the hypersphere: \(\mathbf{z}_{n}^{(1)},...,\mathbf{z}_{n}^{(K)}\in\mathbb{S}^{D-1}\). Then, MMCR compute the average embedding per datum:

\[\mathbf{\mu}_{n}\ \overset{\text{def}}{=}\ \frac{1}{K}\sum_{k}\mathbf{z}_{n}^{(k)}. \tag{2}\]

Next, MMCR forms a \(N\times D\) matrix \(M\) where the \(n\)-th row of \(M\) is \(\mathbf{\mu}_{n}\) and defines the loss:

\[\mathcal{L}_{MMCR}\ \ \overset{\text{def}}{=}\ \ -\|M\|_{*}\ \ \overset{\text{def}}{=}\ \ -\sum_{r=1}^{rank(M)}\ \sigma_{r}(M), \tag{3}\]

where \(\sigma_{r}(M)\) is the \(r\)-th singular value of \(M\) and \(\|\cdot\|_{*}\) is the nuclear norm (trace norm, Schatten 1-norm). Minimizing the MMCR loss means maximizing the nuclear norm of the mean matrix \(M\). The authors of MMCR note that no closed form solution exists for singular values of an arbitrary matrix,but when \(N=2,D=2\), a closed form solution exists that offers intuition: \(\|M\|_{*}\) will be maximized when (i) the norm of each mean is maximized i.e., \(\|\mathbf{\mu}_{n}\|_{2}=1\) (recalling that \(0\leq\|\mathbf{\mu}_{n}\|<1\) since the representations live on the hypersphere), and (ii) the means \(\mathbf{\mu}_{1},\mathbf{\mu}_{2}\) are orthogonal to one another. While we commend the authors for working to offer intuition, it is unclear to what extent the \(N=2,D=2\) setting sheds light on MMCR in general, as MMCR was theoretically derived and numerically implemented in the large data and high dimension regime.

## 2 An Information Theoretic Understanding of MMCR

In this section, we prove and intuitively explain two properties of MMCR that shed light on it as well as relate it to other MVSSL methods. We specifically consider MMCR's regime of large dataset size \(N\) and high embedding dimension \(D\). We contribute two results:

1. The MMCR loss \(\mathcal{L}_{MMCR}\) is minimized by (a) making each mean \(\mathbf{\mu}_{n}=\frac{1}{K}\sum_{k}\mathbf{z}_{n}^{(k)}\) lie on the surface of the hypersphere, and (b) making the distribution of means as close to uniform on the hypersphere as possible.
2. This configuration of means maximizes a well-known variational lower bound on the mutual information between embeddings [14] that was recently used to study and unify several multi-view SSL (MVSSL) families [15].

More formally, we begin by adapting two useful definitions from relevant prior works [25; 15]:

**Definition 2.1** (Perfect Reconstruction).: _We say a network \(f_{\theta}\) achieves perfect reconstruction if \(\forall\mathbf{x}\in\mathcal{X},\forall\,t^{(1)},t^{(2)}\in\mathcal{T}\), \(\mathbf{z}^{(1)}=f_{\theta}(t^{(1)}(\mathbf{x}))=f_{\theta}(t^{(2)}(\mathbf{x}))=\mathbf{z}^{ (2)}\)._

**Definition 2.2** (Perfect Uniformity).: _Let \(p(Z)\) be the distribution over the network representations induced by the data sampling and transformation sampling distributions. We say a network \(f_{\theta}\) achieves perfect uniformity if the distribution \(p(Z)\) is the uniform distribution on the hypersphere._

We will show that a network that achieves both perfect reconstruction and perfect uniformity obtains the lowest possible MMCR loss by first showing that \(\mathcal{L}_{MMCR}\) has a lower bound and then showing that such a network achieves this bound.

**Proposition 2.3**.: _Suppose that \(\forall n\in[N],\mathbf{\mu}_{n}^{T}\mathbf{\mu}_{n}\leq 1\). Then, \(0\leq||M||_{*}\leq\begin{cases}N&\text{if }N\leq D\\ \sqrt{ND}&\text{if }N\geq D\end{cases}.\)_

Proof.: Let \(\sigma_{1},\ldots,\sigma_{\min(N,D)}\) denote the singular values of \(M\), so that \(\|M\|_{*}=\sum_{i=1}^{\min(N,D)}\sigma_{i}\). The lower bound follows by the fact that singular values are nonnegative. For the upper bound, we have

\[\sum_{i=1}^{\min(N,D)}\sigma_{i}^{2}=\mathrm{Tr}\big{[}MM^{T}\big{]}=\sum_{n= 1}^{N}\mathbf{\mu}_{n}^{T}\mathbf{\mu}_{n}\leq N.\]

Figure 1: **Numerical simulations confirm that a network achieving perfect reconstruction and perfect uniformity achieves the lowest possible MMCR loss.** Away from the \(N=D\) threshold, uniform random vectors achieve the theoretically derived upper bound on the nuclear norm (i.e. lower bound on \(\mathcal{L}_{MMCR}\)).

Then, by Cauchy-Schwarz on the sequences \((1,\ldots,1)\) and \(\big{(}\sigma_{1},\ldots,\sigma_{\min(N,D)}\big{)}\), we get

\[\sum_{i=1}^{\min(N,D)}\sigma_{i}\leq\sqrt{\left(\sum_{i=1}^{\min(N,D)}1\right) \left(\sum_{i=1}^{\min(N,D)}\sigma_{i}^{2}\right)}\leq\sqrt{\min(N,D)N}=\begin{cases} N&\text{if }N\leq D\\ \sqrt{ND}&\text{if }N\geq D\end{cases}.\]

**Proposition 2.4**.: _Let \(f_{\theta}\) achieve perfect reconstruction. Then, \(\|\mathbf{\mu}_{n}\|_{2}=1\)\(\forall n\)._

Proof.: Because \(f_{\theta}\) achieves perfect reconstruction, \(\forall n,\forall t^{(1)},t^{(2)}\), \(\mathbf{z}_{n}^{(1)}=\mathbf{z}_{n}^{(2)}\). Thus \(\mathbf{\mu}_{n}=(1/K)\sum_{k}\mathbf{z}_{n}^{(k)}=(1/K)\sum_{k}\mathbf{z}_{n}^{(1)}=\mathbf{z }_{n}^{(1)}\), and since \(\|\mathbf{z}_{n}^{(1)}\|_{2}=1\), we have \(\|\mathbf{\mu}_{n}\|_{2}=1\). 

**Theorem 2.5**.: _Let \(f_{\theta}:\mathcal{X}\rightarrow\mathbb{S}^{D}\) be a network that achieves perfect reconstruction and perfect uniformity. Then \(f_{\theta}\) achieves the lower bound of \(\mathcal{L}_{MMCR}\) with high probability. Specifically:_

\[\|M\|_{*}=\begin{cases}N(1-O(N/D))&\text{if }N\leq D\\ \sqrt{ND}(1-O(D/N))&\text{if }N\geq D\end{cases}\]

_with high probability in \(\min(N,D)\)._

We defer the proof to Appendix A but offer intuition here. To show the inequality in Proposition 2.3 is roughly tight, we need to show the singular values \(\sigma_{i}\) are all roughly equal to each other. When \(N\ll D\), since \(M\) has few rows \(\mathbf{\mu}_{n}\), they are almost perfectly orthogonal to each other, so all \(N\) singular values will be \(\approx\big{\|}\mathbf{\mu}_{n}\|=1\). When \(N\gg D\), since \(M\) has many rows, for any \(x\in\mathbb{R}^{D}\) the sum \(\|Mx\|_{2}^{2}=\sum_{n}(\mathbf{\mu}_{n}^{T}x)^{2}\) will be concentrated, so \(M\) scales all vectors roughly equally, and therefore its \(D\) singular values are all roughly equal to each other. We confirm this via numerical simulations (Fig. 1); for code, see Appendix B.

We now turn to addressing why perfect reconstruction and perfect uniformity matter from an information theoretic perspective. The results here for MVSSL are known, e.g., [25; 15], but we repeat them to make the connection with MMCR. For input datum \(X\), consider the mutual information between the learned embeddings of two different views \(Z^{(1)}=t^{(1)}(X)\) and \(Z^{(2)}=t^{(2)}(X)\); the mutual information must be at least as great as the sum of two terms: the ability of one embedding to "reconstruct" the other, plus the entropy of the embeddings:

\[I[Z^{(1)};Z^{(2)}]\quad\geq\quad\underbrace{\mathbb{E}_{p(Z^{(1)},Z^{(2)})}[ \log q(Z^{(1)}|Z^{(2)})]}_{\text{Reconstruction}}\quad+\quad\underbrace{H[Z^{(1) }]}_{\text{Entropy}}, \tag{4}\]

where \(q(Z^{(1)}|Z^{(2)})\) is a variational distribution because the true distribution \(p(Z^{(1)}|Z^{(2)})\) is unknown.

**Theorem 2.6**.: _Let \(f_{\theta}:\mathcal{X}\rightarrow\mathbb{S}^{D}\) be a network, the number of views per datum be constant, and \(\mathcal{Q}\) be the variational family of distributions on the hypersphere. Then \(f_{\theta}\) maximizes the mutual information lower bound Eqn. 4 if and only if \(f_{\theta}\) achieves perfect reconstruction and perfect uniformity._

Proof.: Perfect reconstruction maximizes reconstruction term. Perfect uniformity maximizes entropy since the maximum entropy is achieved with the uniform distribution over the support [13]. 

**Theorem 2.7**.: _Let \(f_{\theta^{*}}\) be a network that achieves perfect reconstruction and perfect uniformity, let the number of views per datum \(K\) be a constant, and let \(\mathcal{Q}\) be the variational family of distributions on the hypersphere. Then \(f_{\theta^{*}}\) is both a minimizer of \(\mathcal{L}_{MMCR}\) and a maximizer of the variational lower bound of mutual information Eqn. 4._

Proof.: The proof follows by Theorem 2.5 and Theorem 2.6. 

## 3 Discussion

In this work, we leveraged tools from high dimensional probability to prove that in the large data and high dimensional regime, the MMCR loss is minimized with high probability by a network achieving perfect reconstruction and perfect uniformity. These two properties together are known to maximize a well-known variational lower bound on the mutual information between multi-view embeddings.

## References

* Asano et al. [2019] YM Asano, C Rupprecht, and A Vedaldi. Self-labelling via simultaneous clustering and representation learning. In _International Conference on Learning Representations_, 2019.
* Bachman et al. [2019] Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual information across views. _Advances in neural information processing systems_, 32, 2019.
* Bardes et al. [2022] Adrien Bardes, Jean Ponce, and Yann LeCun. VICReg: Variance-invariance-covariance regularization for self-supervised learning. In _International Conference on Learning Representations_, 2022.
* Bojanowski and Joulin [2017] Piotr Bojanowski and Armand Joulin. Unsupervised learning by predicting noise. In _International Conference on Machine Learning_, pages 517-526. PMLR, 2017.
* Caron et al. [2018] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In _Proceedings of the European conference on computer vision (ECCV)_, pages 132-149, 2018.
* Caron et al. [2020] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. _Advances in neural information processing systems_, 33:9912-9924, 2020.
* Caron et al. [2021] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 9650-9660, 2021.
* Chen et al. [2020] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In _International conference on machine learning_, pages 1597-1607. PMLR, 2020.
* Chen et al. [2020] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. _arXiv preprint arXiv:2003.04297_, 2020.
* Chen and He [2021] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 15750-15758, 2021.
* Chung et al. [2018] SueYeon Chung, Daniel D Lee, and Haim Sompolinsky. Classification and geometry of general perceptual manifolds. _Physical Review X_, 8(3):031003, 2018.
* Cover [1965] Thomas M Cover. Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition. _IEEE transactions on electronic computers_, (3):326-334, 1965.
* Cover and Thomas [2006] Thomas M. Cover and Joy A. Thomas. _Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing)_. Wiley-Interscience, USA, 2006.
* Gallager [1968] Robert G Gallager. _Information theory and reliable communication_, volume 588. Springer, 1968.
* Gadvez et al. [2023] Borja Rodriguez Galvez, Arno Blaas, Pau Rodriguez, Adam Golinski, Xavier Suau, Jason Ramapuram, Dan Busbridge, and Luca Zappella. The role of entropy and reconstruction in multi-view self-supervised learning. In _International Conference on Machine Learning_, pages 29143-29160. PMLR, 2023.
* Gardner [1987] Elizabeth Gardner. Maximum storage capacity in neural networks. _Europhysics letters_, 4(4):481, 1987.
* Gardner [1988] Elizabeth Gardner. The space of interactions in neural network models. _Journal of physics A: Mathematical and general_, 21(1):257, 1988.

* [18] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. _Advances in neural information processing systems_, 33:21271-21284, 2020.
* [19] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 9729-9738, 2020.
* [20] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. _arXiv preprint arXiv:1807.03748_, 2018.
* [21] Ravid Shwartz-Ziv, Randall Balestriero, Kenji Kawaguchi, Tim GJ Rudner, and Yann LeCun. An information-theoretic perspective on variance-invariance-covariance regularization. _arXiv preprint arXiv:2303.00633_, 2023.
* [22] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XI 16_, pages 776-794. Springer, 2020.
* [23] Yao-Hung Hubert Tsai, Martin Q. Ma, Muqiao Yang, Han Zhao, Louis-Philippe Morency, and Ruslan Salakhutdinov. Self-supervised representation learning with relative predictive coding. In _International Conference on Learning Representations_, 2021.
* [24] Roman Vershynin, Y Eldar, and Gitta Kutyniok. Compressed sensing, theory and applications. In _Introduction to the non-asymptotic analysis of random matrices_, pages 210-268. Cambridge Univ. Press, 2012.
* [25] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In _International Conference on Machine Learning_, pages 9929-9939. PMLR, 2020.
* [26] Mike Wu, Chengxu Zhuang, Milan Mosse, Daniel Yamins, and Noah Goodman. On mutual information in contrastive learning for visual representations. _arXiv preprint arXiv:2005.13149_, 2020.
* [27] Thomas Yerxa, Yilun Kuang, Eero Simoncelli, and SueYeon Chung. Learning efficient coding of natural images with maximum manifold capacity representations. _arXiv preprint arXiv:2303.03307_, 2023.
* [28] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stephane Deny. Barlow twins: Self-supervised learning via redundancy reduction. In _International Conference on Machine Learning_, pages 12310-12320. PMLR, 2021.
* [29] Jiachen Zhu, Rafael M Moraes, Serkan Karakulak, Vlad Sobol, Alfredo Canziani, and Yann LeCun. Tico: Transformation invariance and covariance contrast for self-supervised visual representation learning. _arXiv preprint arXiv:2206.10698_, 2022.
* [30] Chengxu Zhuang, Alex Lin Zhai, and Daniel Yamins. Local aggregation for unsupervised learning of visual embeddings. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 6002-6012, 2019.

Proof of Theorem 2.5

Recall that \(\mathcal{L}_{MMCR}=-\|M\|_{*}\) is minimized when \(\|M\|_{*}\) is maximized and that \(\|M\|_{*}\) is upper bounded by \(\sqrt{ND}\) if \(N>D\) and \(N\) if \(N<D\) (Proposition 2.3). We want to show a network that achieves perfect reconstruction and perfect uniformity achieves this upper bound on the nuclear norm (equivalently, lower bound on the MMCR loss).

Following the proof of Proposition 2.3, let \(\sigma_{1},\ldots,\sigma_{\min(N,D)}\) denote the singular values of \(M\), so that \(\|M\|_{*}=\sum_{i}\sigma_{i}\). By Proposition 2.4, we have

\[\sum_{i}\sigma_{i}^{2}=\operatorname{Tr}\bigl{[}MM^{T}\bigr{]}=\sum_{n=1}^{N} \boldsymbol{\mu}_{n}^{T}\boldsymbol{\mu}_{n}=N.\]

Now, by the equality version of Cauchy-Schwarz on the sequences \((1,\ldots,1)\) and \(\bigl{(}\sigma_{1},\ldots,\sigma_{\min(N,D)}\bigr{)}\), we have

\[\sum_{i}\sigma_{i}=\sqrt{\min(N,D)\left(\sum_{i}\sigma_{i}^{2}-\sum_{i}\left( \sigma_{i}-\frac{\sum_{j}\sigma_{j}}{\min(N,D)}\right)^{2}\right)}. \tag{5}\]

So if we can bound this "variance" of the singular values \(\sum_{i}\left(\sigma_{i}-\frac{\sum_{j}\sigma_{j}}{\min(N,D)}\right)^{2}\), we can show that \(\|M\|_{*}\) closely matches the upper bound obtained in Proposition 2.3.

To do this, let us consider matrix \(\sqrt{D}M\). The vectors \(\boldsymbol{\mu}_{n}\) are uniform over the \(D\)-dimensional hypersphere \(\mathbb{S}^{D}\), so its rows \(\sqrt{D}\boldsymbol{\mu}_{n}\) have mean zero, are isotropic, and (by Example 5.25 in [24]) are sub-gaussian with parameter \(\|\sqrt{D}\boldsymbol{\mu}_{n}\|_{\psi_{2}}=O(1)\).2 Therefore,

Footnote 2: Here, \(\|\cdot\|_{\psi_{2}}\) denotes the sub-gaussian norm (intuitively, the “effective standard deviation” of a sub-gaussian random variable). For a scalar random variable \(X\), it is defined as \(\|X\|_{\psi_{2}}\coloneqq\sup_{p\geq 1}p^{-1/2}(\mathbb{E}[|X|^{p}])^{1/p}\) (Definition 5.7 in [24]), and for a random vector \(\boldsymbol{u}\in\mathbb{R}^{D}\), it is defined as \(\|\boldsymbol{u}\|_{\psi_{2}}\coloneqq\sup_{\boldsymbol{v}\in\mathbb{S}^{D}}\| \boldsymbol{u}^{T}\boldsymbol{v}\|_{\psi_{2}}\) (Definition 5.22 in [24]).

* **If \(\mathbf{N}\leq\mathbf{D}\)**, then (using the fact that \(\|\boldsymbol{\mu}_{n}\|_{2}=1\) for all \(n\in[N]\)) we can to apply Theorem 5.58 in [24] on the transpose of \(\sqrt{D}M\), obtaining that for any \(t\geq 0\), the singular values of \(\sqrt{D}M\) are within \(\sqrt{D}\pm O(\sqrt{N})+t\) with probability at least \(1-2\exp(-\Omega(t^{2}))\). Setting \(t\) to a large enough multiple of \(\sqrt{N}\), they are all within \(\sqrt{D}\pm O(\sqrt{N})\) with probability at least \(1-2\exp(-N)\). Consequently, with the same probability, the singular values of \(M\) are all within \(\pm O(\sqrt{N/D})\) of each other, and we get \(\sum_{i}\left(\sigma_{i}-\frac{\sum_{j}\sigma_{j}}{\min(N,D)}\right)^{2}\leq N \cdot O\left(\sqrt{N/D}\right)^{2}=O(N^{2}/D)\). Plugging this into Eqn. 5, we get \(\|M\|_{*}\leq\sqrt{N(N-O(N^{2}/D))}=\sqrt{N}(1-O(N/D))\).
* **If \(\mathbf{N}\geq\mathbf{D}\)**, then we can apply Theorem 5.39 in [24] on \(\sqrt{D}M\), obtaining that for any \(t\geq 0\), the singular values of \(\sqrt{D}M\) are within \(\sqrt{N}\pm O(\sqrt{D})+t\) with probability at least \(1-2\exp(-\Omega(t^{2}))\). Setting \(t\) to a large enough multiple of \(\sqrt{D}\), they are all within \(\sqrt{N}\pm O(\sqrt{D})\) with probability at least \(1-2\exp(-D)\). Consequently, with the same probability, the singular values of \(M\) are Python Code for Perfect Reconstruction and Perfect Uniformity Embeddings

To test our claim that networks which achieve perfect reconstruction and perfect uniformity achieve the nuclear norm upper bound, we sample a uniform distribution of embeddings and measure the nuclear norm relative to our claimed upper bound. Python code for our simulations is included below:

```
importpandasaspd importnumpyasnp N_list=np.logspace(start=1,stop=4,num=11).astype(int) D_list=np.logspace(start=1,stop=4,num=11).astype(int) repeats=np.arange(5).astype(int) uniform_distribution_nuclear_norm_data_list=[] forNinN_list: forDinD_list: print(f"N:{N}\tD:{D}") forrepeatinrepeats: embeddings=np.random.normal(loc=0,scale=10.0,size=(N,D)) embeddings/=np.linalg.norm(embeddings,axis=1,keepdims=True) row={"Spectrum":"uniform", "Number_of_Data_Manifolds_(N)":N, "Embedding_Dimensionality_(D)":D, "Repeat":repeat, "Nuclear_Norm":np.linalg.norm(embeddings,ord="nuc"), } uniform_distribution_nuclear_norm_data_list.append(row) uniform_distribution_nuclear_norm_df=pd.DataFrame(uniform_distribution_nuclear_norm_data_list) }
```