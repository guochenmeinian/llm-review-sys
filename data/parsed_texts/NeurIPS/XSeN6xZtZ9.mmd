# Large Language Model Benchmarks Do Not Test Reliability

Joshua Vendrow, Edward Vendrow, Sara Beery, Aleksander Madry

MIT

{jvendrow,evendrow,beery,madry}@mit.edu

Equal contribution. \(\dagger\)Equal advising.

###### Abstract

When deploying large language models (LLMs), it is important to ensure that these models are not only capable, but also _reliable_. Many benchmarks have been created to track LLMs' growing capabilities. However, there has been no similar focus on measuring their reliability. To understand this landscape, we first investigate how well current benchmarks quantify model reliability. We find that pervasive label errors compromise these evaluations, obscuring lingering model failures and hiding unreliable behavior.

Motivated by this gap in the evaluation of reliability, we propose the construction of so-called _platinum_ benchmarks that are carefully curated to minimize label errors and ambiguity. As a first attempt at constructing such benchmarks, we revise examples from fifteen existing popular benchmarks. We evaluate a wide range of models on these platinum benchmarks and find that indeed, frontier LLMs still exhibit failures on simple tasks such as elementary-level math word problems. Analyzing these failures reveals previously unidentified patterns of questions on which frontier models consistently struggle.

## 1 Introduction

Large language models (LLMs) have demonstrated impressive capabilities in areas such as problem solving [16; 28], knowledge retrieval [12], and code generation [15; 20]. Major research effort continues to advance the frontier of LLM capabilities [1; 27]. However, these models still sometimes exhibit failures even on tasks far simpler than these frontier capabilities [44; 25]. Practitioners might thus be worried whether this unreliability can pose significant risks, especially in accuracy- and safety-critical applications.

Indeed, in sectors such as healthcare, finance, insurance, and legal services, model errors can lead to serious ramifications (e.g., jeopardizing patient outcomes or causing financial losses). In fact, mistakes by LLMs in real-world deployments have already caused legal liability [7] and generated controversy [24]. In light of these issues, it is important to understand when we can confidently deploy LLMs in such situations. These concerns motivate the central question of our work:

_On what kinds of tasks are frontier models actually reliable?_

To identify such tasks, a natural approach would be to examine existing benchmarks on which current models already perform well. Specifically, we might want to investigate older benchmarks (e.g., GLUE [40], SQuAD [31], GSM8K [11]) that tend to evaluate simpler capabilities than current ones. These benchmarks are rarely used today due to the commonly held view that performance on them has "saturated"--that is, that models have reached a sufficient or "human-level" performance on the benchmark, and remaining errors can be attributed to label noise or ambiguity in the benchmark itself.

Indeed, some recent releases of frontier LLMs have excluded evaluations on GSM8K (a dataset of grade-school level math word problems), for example [28; 2], following concerns that it has reached saturation [17] (current frontier models achieve \(\sim\)95% accuracy [10; 1]).

Since we are interested in reliability though, we need to ensure that models can execute tasks with near-perfect accuracy. So, we would like to know if models are truly reliable on benchmarks once they reach saturation (e.g., achieving 95% on GSM8K), or if we should be worried about lingering model errors in the remaining 5%, hidden among the label noise.

### Contributions

In this work, we demonstrate that, indeed, the remaining failures on these older benchmarks are not just label errors, and more broadly, that current benchmarks are not well equipped for testing model reliability. We then propose a new style of benchmarking to rigorously quantify model reliability, and make an initial effort towards constructing such benchmarks.

To understand the need for this new framework, we first examine how LLM benchmarking has evolved to disincentivize progress on reliability.

The status quo of LLM benchmarkingThe difficulty of LLM benchmarks has increased over time to track the progression of their frontier capabilities. For example, the complexity of math and science tasks that these models are evaluated on has grown from elementary and middle school level (e.g., SVAMP [30], GSM8K [8]), to high school and college level (e.g., MATH [19], MMLU [18]), to graduate level problems (e.g., GPQA [34]) 2.

Footnote 2: The original state-of-the-art accuracies on these benchmarks were reported at below \(50\%\) (and as low as \(6.2\%\) on MATH), and today the accuracies on all of them other than GPQA have exceeded 90% (see Table B1).

a result of rapidly progressing model capabilities, current benchmarks (like the ones mentioned above) follow a consistent pattern of development, progress, and eventual retirement that we refer to as the _life cycle of benchmarks_. Thi Specifically, a benchmark is first created to test a frontier capability for which current LLMs achieve a low accuracy (i.e., below 50%). Models' performance on the benchmark increases over time and eventually plateaus, often around 90-95%. At this point, the benchmark faces the same fate that we discussed earlier with older benchmarks like SQuAD and GSM8K: it is deemed "saturated," with remaining errors attributed to label noise, and gradually retired as the community shifts focus to newer, more difficult benchmarks.

We believe that this life cycle has led to a gap in benchmarking: since benchmarks are discarded when models achieve a sufficient, but not perfect, performance (i.e., when they are saturated), model developers are never encouraged to achieve proper reliability on them.

Platinum benchmarksIn order to better evaluate model reliability, we introduce the concept of _platinum_ benchmarks that are carefully curated to minimize label errors and ambiguity, and require 100% performance to pass. Unlike traditional benchmarks, which become "saturated" when models reach high (but imperfect) performance, platinum benchmarks remain relevant until models achieve full reliability. While current iterations of benchmarks quantify the _capabilities frontier_ of LLMs--the most advanced tasks models are able to perform--platinum benchmarks allow us to identify their _reliability frontier_: the most advanced tasks models can perform consistently without error.

We demonstrate our approach by constructing platinum versions of 15 "saturated" benchmarks across six categories of capabilities by systematically re-labeling them. We find that that many of these benchmarks are indeed riddled with errors (e.g., 5% of GSM8K). In fact, for the majority of benchmarks we investigate, more than half of model failures can be attributed to label noise.

Despite the relative simplicity of these tasks compared to frontier challenges, on most of our platinum benchmarks no state-of-the-art model we evaluate gets 100%. For instance, multiple frontier models, including GPT-4o, fail at the following basic pronoun resolution task from Winograd WSC [22]:

In the sentence, "John couldn't see the stage with Billy in front of him because he is so short," what does "he" in "he is so short." refer to in the phrase? [A: John, B: Billy]

Moreover, our analysis of model failures reveals previously unidentified patterns of questions that frontier models consistently struggle with. We view our work as the first step in a new practice of quantifying LLM reliability. See Appendix A for the related work.

## 2 Cleaning Up Noisy Benchmarks

To make a noisy benchmark "platinum," we need to update it to remove or correct label errors. In this section, we categorize common types of errors and specify our approach for detecting and correcting them. In Section 3, we will then leverage these platinum benchmarks in order to evaluate the reliability of frontier models.

### Experimental setup

Benchmarks includedWe investigate fifteen benchmarks covering six categories of capabilities: mathematics (SingleOp [36], SingleEq [21], MultiArith [35], SVAMP [30], GSM8K [8], MMLU High School Math [18]), logic (BIG-bench Object Counting, BIG-bench Logical Deduction, BIG-bench Navigate [37]), table understanding (TabFact [6]), reading comprehension (SQuAD2.0 [32], HotPotQA [43], DROP [9]), commonsense reasoning (Winograd WSC [22]), and visual understanding (VQA v2.0 [14]). For datasets with publicly available test splits with solutions, we use the test split, otherwise we use the validation split. Many of these benchmarks are large (e.g., the VQA v2.0 validation set has over 200,000 questions). In order to ensure the quality of our cleaning process, we select smaller subsets at random from many of these benchmarks.

ModelsWe test several current frontier models, including popular proprietary LLMs (GPT-4o mini and GPT-4o [27], Claude 3.5 Sonnet [1], Gemini 1.5 Flash and Gemini 1.5 Pro [33], o1-mini [28]), open-weights LLMs (Llama 3.1 70B Instruct and Llama 3.1 405B Instruct [10], Mistral Small and Mistral Large).

PromptingWe use chain-of-thought prompting [41] (i.e., asking the model to think step-by-step), except with o1-mini, as its official prompting guide recommends omitting this prompting technique4. All questions are asked in a zero-shot setting. We use a temperature of 0.5, as deployed models tend to use lower temperatures to balance coherence and creativity. The exact prompts we use are provided in Appendix C.2.

Figure 1: **Examples of errors in current LLM benchmarks.****(a)** For mislabeled questions, we fix the solutions and include the re-labeled examples in our benchmark. We find three common categories of “bad” questions: **(b)** there is a logical contradiction in the problem statement, **(c)** there is ambiguity leading to many plausible solutions, or **(d)** there is a clear flaw in the construction of the question, such as missing specifications. We remove such questions when cleaning benchmarks.

MetricsAll the benchmarks we use are either multiple choice or have a single correct answer (except for reading comprehension datasets such as SQuAD2.0, for which we manually expand the set of correct responses--see Appendix C.1.2 for further details). Thus, we can simply compare this answer to the model prediction to evaluate correctness. We report the number of errors rather than accuracy to better differentiate between models, as for many benchmarks we expect models to have accuracies in the high 90s.

### Identifying Errors in Benchmarks

What makes a question bad?Before correcting errors in benchmarks, we first need to categorize the kinds of issues we aim to resolve. Each example in a benchmark consists of a question and a solution. Sometimes, a question can be well-written, but the solution is mislabeled. For instance, Figure 1(a) shows a question from SVAMP for which the given solution is incorrect. For such examples we can simply re-label the solution, allowing us to keep the example in the benchmark.

In other cases, though, the question itself is poorly written, so simply re-labeling the solution is inadequate. Figures 1(b-d) illustrate three common categories of such issues: (1) a logical contradiction in the problem statement, (2) ambiguity that allows for multiple plausible solutions, or (3) a clear flaw in the question's construction. In many cases, there is no simple way to fix the question (for example, fixing Figure 1(d) would require coming up with a set of equations and corresponding question from scratch). Therefore, we opt to remove poorly written examples during our cleaning process.

How do we efficiently identify errors in a benchmark?Cleaning large datasets can be prohibitively time-consuming, especially when the questions take time to verify (e.g., challenging math problems or retrieval tasks with long contexts). We devise a simple strategy to find problematic questions by examining the agreement among multiple LLMs.

As described above, we divide potential issues with examples from a benchmark into two categories: (1) mislabeled solutions, and (2) poorly written questions (e.g., ones that are ambiguous or ill-posed). To detect these errors, we give each question to several frontier LLMs. We then manually inspect any example on which at least one LLM makes an error. We expect that when a solution is incorrect, frontier models will often disagree with the given solution, and when the question itself is poorly written, models should disagree among themselves. For every example we inspect, we either mark that example as "bad" and remove it if the question is poorly written or relabel the solution if it is incorrect. Since this process checks all model errors, we can be confident that the errors we report are genuine. It is possible that our count is a lower bound, as it is conceivable that both the benchmark solution and LLM solutions are all the same and all incorrect. Further details of the cleaning protocols used for each benchmark are in Appendix C.

\begin{table}
\begin{tabular}{l|c c c c c c|c c c|c c c|c} Dataset & & & & & & & & & & & & & & & \\ \hline Type & & \multicolumn{4}{c|}{Math} & & \multicolumn{4}{c|}{Logic} & Tab & \multicolumn{4}{c|}{RC} & \multicolumn{1}{c|}{CR} & Vis \\ \# Original Questions & 159 & 109 & 174 & 300 & 300 & 270 & 200 & 200 & 200 & 250 & 250 & 250 & 200 & 600 \\ \hline \multicolumn{13}{l|}{_Platinum Labeling_} & & & & & & & & & & & & & & \\ \# Bad Questions & 6 & 6 & 2 & 31 & 21 & 3 & 0 & 8 & 0 & 26 & 64 & 82 & 40 & 4 & 352 \\ \# Mislabeled & 0 & 0 & 3 & 3 & 0 & 0 & 0 & 0 & 0 & 3 & 5 & 3 & 3 & 0 & — \\ \hline \# Platinum Questions & 153 & 103 & 172 & 269 & 279 & 267 & 200 & 192 & 200 & 174 & 186 & 168 & 210 & 196 & 248 \\ \hline \end{tabular}
\end{table}
Table 1: We clean examples from fifteen popular benchmarks to remove errors and ambiguities. While some programmatically generated benchmarks are error-free, we find a significant number of mislabeled or poorly written questions in others. The number of mislabeled examples is missing from 2.0 as our labels follow a different format than the original benchmark, necessitating all labels to be revised (see Appendix C.1.1 for details on VQA v2.0).

How noisy are saturated LLM benchmarks?In Table 1, we report the number of poorly written and mislabeled questions we identify in each of the fifteen benchmark subsets we investigate. We find that, indeed, many of these benchmarks have a substantial rate of errors, confirming suspicions of flaws in these benchmarks commonly held by the community. For example, the percentage of errors we find in examples from GSM8K, SVAMP, VQA v2.0 and TabFact exceed the percentage of errors reported by frontier models on these benchmarks. This suggests that any error made by frontier models on one of these benchmarks is more likely to be an issue with the benchmark itself than a genuine model error.

For reading comprehension datasets (SQuAD2.0, HotpotQA, DROP), we identify issues with up to 30% of examples. Largely, these issues arise from questions that are sufficiently open-ended such that it is difficult to exhaustively list all possible responses. Additionally, SQuAD2.0 intentionally adds questions that are unanswerable from the given passage to test whether models can abstain from answering (i.e., return N/A). However, the process of making these unanswerable questions often leads to them being highly ambiguous or even nonsensical. For example, one such question asks, "What isn't the gender income inequality in Bahrain?"; this question can be traced to a worker replacing "is" with "isn't" from an original question within SQuAD [31]5. Following our cleaning protocol, we omit such poorly written questions. In Appendix F, we show specific examples of bad questions that we identify across the fifteen benchmarks.

Footnote 5: For the original question from SQuAD, see: https://huggingface.co/datasets/rajpurkar/squad/viewer/plain_text/validation?p=74&row=7464.

## 3 Evaluating Reliability with Platinum Benchmarks

### Pinpointing the Reliability Frontier

Within a given category of capabilities (e.g., math problem solving), a platinum benchmark of a certain difficulty allows us to evaluate how reliable models are at performing tasks at that difficulty level. However, the actual reliability frontier of a model might be less or more advanced than the tasks in that specific benchmark. In order to identify this frontier with greater granularity, we need platinum benchmarks at varying levels of difficulty; then, we can estimate a model's reliability frontier by identifying the most difficult benchmark it is able to pass (i.e., score 100% on).

[MISSING_PAGE_FAIL:6]

consistent collapses in reasoning of frontier LLMs. We initially found an instance of each failure mode by examining models' reasoning processes on failures from our platinum benchmarks. We then verify the consistency of such failures by procedurally constructing similar examples. We outline these failure modes below, and provide further details and a more complete analysis in Appendix D.

Example pattern 1: First event biasWe find that when asked: "What happened second: {_some event_} or {_some other event_}" given some context, three models (Gemini 1.5 Flash, Gemini 1.5 Pro, and Mistral Small) almost always answer with the first event, and will even explicitly acknowledge they are identifying the first event rather than the second:

**Question: _(context paragraph)_** What happened second: Russians blocked Azov or Treaty of Constantinople?

**Solution:** Treaty of Constantinople

**Gemini 1.5 Pro:**... we can conclude that the Russians blocking Azov happened before the Treaty of Constantinople. Answer: Russians blocked Azov

We procedurally generate further examples in this question format and find that each of these three models fails on over 90% of the examples (see Figure D1).

Example pattern 2: Rounding up primesWe find that when Claude 3.5 Sonnet is asked to solve grade-school math problems involving division, it often incorrectly rounds up the quotient even though the division already resulted in a whole number, leading to an answer greater than the solution by one:

**Question:** The school is planning a field trip. The school has 67 classrooms. There are 66 students in each classroom in the school. If there are 6 seats on each school bus. How many buses are needed to take the trip?

**Solution:** 737

**Claude 3.5 Sonnet:**... To find the number of buses needed, we divide the total number of students by the number of seats per bus: 4,422 + 6 = 737. However, since we can't have a fraction of a bus, we need to round up to the next whole number to ensure all students have a seat. Therefore, the school needs 738 buses to take all students on the trip. Answer: 738

Interestingly, we find that this behavior is significantly more common when the final answer is closer to being prime (i.e., it has few divisors; see Figure D2). For example, when we query Claude 3.5 Sonnet on similarly constructed problems with an answer that is prime, the model makes this exact rounding error 20% of the time.

## 4 Discussion and Future Work

There is a significant gap between capability and reliability.Frontier LLMs can solve graduate-level problems (e.g., GPQA [34]), but as we find, they can still fail on basic logic tasks and elementary-level problems. This discrepancy indicates a wide gap between the capability and reliability frontiers that does not seem to be addressed by scaling models further.

Framing reliability as a deployment metric.Quantifying reliability is a common practice for deployed systems. Within the context of software, for instance, an entire dedicated field of site reliability engineering has emerged to ensure that systems are reliable, with a reliability goal measured by the number of nines of uptime (e.g., five nines, or 99.999% uptime). We hope our work can be a first step towards building out this level of reliability quantification for LLMs.

How do we account for prompt brittleness?A common strategy for further improving performance of LLMs is to carefully adjust prompts, such as by tuning structure and wording individualized to a specific target model (i.e., prompt engineering). But as long as the instructions are clearly stated and the task is explicitly defined, it is reasonable to expect reliable models to perform well regardless of minor variations in prompt wording. Since we are not focused on assessing how well models can follow formatting directions, however, we experimented to choose a prompting strategy that ensures our models do not fail due to output formatting errors. We do not further engineer prompts beyond this; see Appendix C.2 for our specific template. Nevertheless, it is plausible that a specific choice of prompt will affect a model's reliability. We encourage future work to both investigate promptingstrategies that elicit more reliable behavior, and develop models that are less brittle to specific prompt types.

LimitationsWe view our work as an initial effort towards constructing platinum benchmarks. Here, we briefly discuss the limitations of our work and areas for potential improvement by future works.

1. **Capabilities and levels of difficulty covered** Our set of fifteen benchmarks misses a number of relevant capabilities of LLMs, such as coding and tool use. We attempt to cover a wide range of difficulty levels for mathematics, but not for any other capability. We also do not include any benchmarks that require expert-level annotations to clean, as we revise the benchmarks for this preliminary investigation ourselves.
2. **Number of examples per benchmark** Some of our revised benchmarks include as little as 100 examples, often limited by the size of the original benchmark. This limits our ability to quantify reliability with certainty: there may not be a large gap between models that have zero percent or one percent error rate on such a small sample size.
3. **Only re-labeling errors** As we re-label all examples for which some model failed, we can be confident that every error we report is genuine. However, there may still be poorly written questions in our platinum benchmarks among those we did not revise, where, despite error or ambiguity, all models agreed with the stated ground truth.
4. **Benchmark difficulty** As of today, frontier LLMs still fail on sufficiently simple questions that their errors can be quantified without specific expertise. For example, the most difficult task we annotate is high school level mathematics. However, once the reliability frontier of models is sufficiently advanced, expensive expert annotation will be required to construct platinum benchmarks for expert capabilities.

## 5 Conclusion

In this work, we propose the construction of highly cleaned _platinum_ benchmarks to test the reliability of LLMs, and make an initial effort to create such benchmarks by cleaning fifteen existing datasets. We demonstrate that frontier models continue to exhibit failures on basic tasks from these "saturated" benchmarks, showing a gap in current benchmarking practices. We hope that our paper motivates the adoption of platinum benchmarks in evaluating LLMs to ensure they meet the high reliability standards required for real-world applications.

## References

* [1]S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh (2015) Vqa: visual question answering. In Proceedings of the IEEE international conference on computer vision, pp. 2425-2433. Cited by: SS1.
* [2]S. R. Bowman and G. E. Dahl (2021) What will it take to fix benchmarking in natural language understanding?. arXiv preprint arXiv:2104.02145. Cited by: SS1.
* [3]S. R. Bowman and G. E. Dahl (2021) What will it take to fix benchmarking in natural language understanding?. arXiv preprint arXiv:2104.02145. Cited by: SS1.
* [4]S. R. Bowman and G. E. Dahl (2021) What will it take to fix benchmarking in natural language understanding?. arXiv preprint arXiv:2104.02145. Cited by: SS1.
* [5]N. Carlini and D. Wagner (2017) Towards evaluating the robustness of neural networks. In Symposium on Security and Privacy (SP), Cited by: SS1.
* [6]W. Chen, H. Wang, J. Chen, Y. Zhang, H. Wang, S. Li, X. Zhou, and W. Y. Wang (2019) Tabfact: a large-scale dataset for table-based fact verification. arXiv preprint arXiv:1909.02164. Cited by: SS1.
* [7]Civil Resolution Tribunal. Moffatt v. air canada. Note: 2024 BCCRT 149, Small Claims Decisions, File No. SC-2023-005609, Final Decision External Links: Link Cited by: SS1.
* [8]K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. (2021) Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Cited by: SS1.
* [9]D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner (2019) Drop: a reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint arXiv:1903.00161. Cited by: SS1.
* [10]A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, et al. (2024) The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Cited by: SS1.
* [11]K. Dvijotham, S. Gowal, R. Stanforth, R. Arandjelovic, B. O'Donoghue, J. Uesato, and P. Kohli (2018) Training verified learners with learned verifiers. In ArXiv preprint arXiv:1805.10265. Cited by: SS1.
* [12]Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, and H. Wang (2023) Retrieval-augmented generation for large language models: a survey. arXiv preprint arXiv:2312.10997. Cited by: SS1.
* [13]A. P. Gema, J. O. J. Leang, G. Hong, A. Devoto, A. C. M. Mancino, R. Saxena, X. He, Y. Zhao, X. Du, M. R. G. Madani, et al. (2024) Are we done with mmlu?. arXiv preprint arXiv:2406.04127. Cited by: SS1.
* [14]Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh (2017) Making the v in vqa matter: elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6904-6913. Cited by: SS1.
* [15]D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. Li, et al. (2024) Deepseek-coder: when the large language model meets programming-the rise of code intelligence. arXiv preprint arXiv:2401.14196. Cited by: SS1.
* [16]S. Hao, Y. Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, and Z. Hu (2023) Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992. Cited by: SS1.
* [17]P. Henderson (2024) Has anyone analyzed what's the ground truth label error rate for gsm8k? it's possible we entered data leakage and overfitting territory a while ago. Note: URL https://twitter.com/PeterHndrnsn/status/1831801148795449410 Cited by: SS1.
* [18]D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt (2020) Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300. Cited by: SS1.
* [19]D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt (2021) Measuring mathematical problem solving with the math dataset. NeurIPS. Cited by: SS1.

[MISSING_PAGE_POST]

* [20] C. E. Jimenez, J. Yang, A. Wettig, S. Yao, K. Pei, O. Press, and K. R. Narasimhan. SWE-bench: Can language models resolve real-world github issues? In _The Twelfth International Conference on Learning Representations_, 2024.
* [21] R. Koncel-Kedziorski, H. Hajishirzi, A. Sabharwal, O. Etzioni, and S. D. Ang. Parsing algebraic word problems into equations. _Transactions of the Association for Computational Linguistics_, 3:585-597, 2015.
* [22] H. Levesque, E. Davis, and L. Morgenstern. The winograd schema challenge. In _Thirteenth international conference on the principles of knowledge representation and reasoning_, 2012.
* [23] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to adversarial attacks. In _International Conference on Learning Representations (ICLR)_, 2018.
* [24] B. Masse. A chevy for $1? car dealer chatbots show perils of ai for customer service. _VentureBeat_, 2023.
* [25] M. Nezhurina, L. Cipolina-Kun, M. Cherti, and J. Jitsev. Alice in wonderland: Simple tasks showing complete reasoning breakdown in state-of-the-art large language models. _arXiv preprint arXiv:2406.02061_, 2024.
* [26] C. G. Northcutt, A. Athalye, and J. Mueller. Pervasive label errors in test sets destabilize machine learning benchmarks. _arXiv preprint arXiv:2103.14749_, 2021.
* [27] OpenAI. Gpt-4 technical report. In _arXiv preprint arXiv:2303.08774_, 2023.
* [28] OpenAI. Openai o1 system card, 2024. Accessed: 2024-09-26.
* [29] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami. The limitations of deep learning in adversarial settings. In _IEEE European Symposium on Security and Privacy_, 2016.
* [30] A. Patel, S. Bhattacharya, and N. Goyal. Are nlp models really able to solve simple math word problems? _arXiv preprint arXiv:2103.07191_, 2021.
* [31] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. Squad: 100,000+ questions for machine comprehension of text. _arXiv preprint arXiv:1606.05250_, 2016.
* [32] P. Rajpurkar, R. Jia, and P. Liang. Know what you don't know: Unanswerable questions for squad. _arXiv preprint arXiv:1806.03822_, 2018.
* [33] M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b. Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. _arXiv preprint arXiv:2403.05530_, 2024.
* [34] D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. Gpqa: A graduate-level google-proof q&a benchmark. _arXiv preprint arXiv:2311.12022_, 2023.
* [35] S. Roy and D. Roth. Solving general arithmetic word problems. _arXiv preprint arXiv:1608.01413_, 2016.
* [36] S. Roy, T. Vieira, and D. Roth. Reasoning about quantities in natural language. _Transactions of the Association for Computational Linguistics_, 3:1-13, 2015.
* [37] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. _arXiv preprint arXiv:2206.04615_, 2022.
* [38] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. In _International Conference on Learning Representations (ICLR)_, 2014.

* [39] D. Tsipras, S. Santurkar, L. Engstrom, A. Ilyas, and A. Madry. From imagenet to image classification: Contextualizing progress on benchmarks. In _International Conference on Machine Learning (ICML)_, 2020.
* [40] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. _arXiv preprint arXiv:1804.07461_, 2018.
* [41] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in neural information processing systems_, 35:24824-24837, 2022.
* [42] X. Xu, K. Kong, N. Liu, L. Cui, D. Wang, J. Zhang, and M. Kankanhalli. An llm can fool itself: A prompt-based adversarial attack. _arXiv preprint arXiv:2310.13345_, 2023.
* [43] Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. W. Cohen, R. Salakhutdinov, and C. D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. _arXiv preprint arXiv:1809.09600_, 2018.
* [44] Z. Yang, Y. Zhang, T. Liu, J. Yang, J. Lin, C. Zhou, and Z. Sui. Can large language models always solve easy problems if they can solve harder ones? _arXiv preprint arXiv:2406.12809_, 2024.
* [45] J. X. Zhao, Y. Xie, K. Kawaguchi, J. He, and M. Q. Xie. Automatic model selection with large language models for reasoning. _arXiv preprint arXiv:2305.14333_, 2023.
* [46] A. Zou, Z. Wang, J. Z. Kolter, and M. Fredrikson. Universal and transferable adversarial attacks on aligned language models. _arXiv preprint arXiv:2307.15043_, 2023.

Related Work

Errors in machine learning benchmarksPrevious works have studied the identification of errors in machine learning benchmarks, as well as the resulting impact of these label errors on the quality of model evaluations. Tsipras et al. [39] investigate the original ImageNet labeling process and identify issues in the resulting labels resulting from (1) the presence of multiple correct classes in a single image, and (2) biases in the label validation process. They also release a refined, multi-label re-labeling of the ImageNet validation set. Northcutt et al. [26] first systematically identify errors in commonly used machine learning benchmarks, and then find that evaluating on benchmarks with significant rates of errors can lead practitioners to incorrectly select less performant models, demonstrating the value of such label cleaning processes. Bowman and Dahl [4] raise concerns similar to our over issues in benchmarking for NLP tasks, and lay out a set of criteria that good benchmarks should satisfy (e.g., benchmarks should reveal plausibly harmful social impact, be accurate and unambiguous, and offer adequate statistical power). However, they focus on overall design and social impact of benchmarks, whereas we focus solely on better assessing model reliability.

Recently, Gema et al. [13] released MMLU-Redux, a re-annotated subset of the MMLU benchmark [18] created through manual assessment from 14 human experts. Our re-labeling of the MMLU high school mathematics subset actually intersects with MMLU-Redux over 100 examples. Our revised annotations align with MMLU-Redux on all but one example, on which we find that one of their human experts accidentally re-annotated a correct solution to make it incorrect6. This slight remaining inconsistency highlights the difficulty of avoiding errors when creating and revising benchmarks.

Footnote 6: see the question marked “wrong_groundtruth” here: https://huggingface.co/datasets/edinburgh-dawg/mmlu-redux/viewer/high_school_mathematics?row=52

LLM failures on simple tasksIt is generally known and often discussed that LLMs fail in surprising and unintuitive ways on simple tasks. For example, the common example pf LLMs failing on the query "how many r's are there in the word strawberry" has circulated both social media and news outlets. Previous works have investigated specific instantiations of such failures. Yang et al. [44] find that models frequently fail on simple problems even when they can solve harder versions of these same problems, suggesting inconsistency in their reasoning abilities. Nezhurina et al. [25] raise similar concerns over breakdowns in LLM reasoning behavior by identifying a specific category of logic tasks on which current frontier LLMs fails consistently.

Adversarial examplesAdversarial attacks are small, sometimes imperceptible perturbations to model inputs that can drastically change their behavior, especially in ways that are unlikely to affect humans--these lightly perturbed inputs are known as _adversarial examples_. There has been significant work studying adversarial attacks and defenses against them in computer vision domains [38; 5; 23; 29], and recent work has demonstrated successful adversarial attacks on LLMs, especially in the context of breaking safety alignment [46; 42]. One possible approach to identifying LLM failures on simple queries might be to adversarially optimize for queries that result in model failures. However, we aim for our benchmark to assess whether models can be deployed reliably on real-world tasks, and adversarial examples generally do not align with the 'corner-cases' that models face in the real world (unless the users themselves adversarially optimize their own inputs).

The Life Cycle of Benchmarks

As we discuss in the introduction, benchmarks follow a common pattern of development, progress, and eventual retirement when they are demed saturated. In Table B1 we show the accuracies upon release and currently for a number of math and science benchmarks that appear to follow this life cycle.

Experimental details

### Benchmark processing

Here, we discuss additional processing and cleaning steps specific to certain of the chosen benchmarks.

#### c.1.1 VQA v2.0

Re-labelingRather than a single ground truth label per example, the VQA [3] and updated VQA v2.0 [14] datasets collect ten separate crowd-annotated labels per image-question query. Their accuracy metric then assigns a score to a model prediction based on the overlap between the prediction and these ten labels. As we assign a single ground-truth label to each question, we manually re-label all the VQA v2.0 queries we include in our revised subset rather than only inspecting ones for which some model failed.

Selection of queriesThe VQA v2.0 dataset is designed to mitigate common biases in visual question answering datasets by balancing the original VQA dataset with complementary images that break a given bias. To maintain this construction, we randomly select from these image pairs in VQA v2.0, and reject a given pair if either image deemed ambiguous.

To further improve the clarity and ease of labeling of our samples, we also limit our subset to only 'yes/no' questions within VQA v2.0, as open-ended queries have a greater potential for ambiguity and can often have multiple correct answers.

#### c.1.2 Reading Comprehension Benchmarks

Re-labeling to account for multiple correct responsesSQuAD2.0 [32], HotPotQA [43], and DROP [9] are all question answering benchmarks based on background knowledge provided in-context. Most of these questions are open-ended, so there are often multiple valid responses. For instance, consider the following example from HotPotQA:

**Paragraph A:**_Ethel Houbiers_

Ethel Houbiers is a French voice actress. She is the French voice of Penelope Cruz and Salma Hayek.

**Paragraph B:**_Salma Hayek_

Salma Hayek Pinault ( Hayek Jimenez) (born September 2, 1966), known professionally as Salma Hayek, is a Mexican and American film actress, producer, and former model...[_continued_]

**Question:** which Mexican and American film actress is Ethel Houbiers French voice of?

**Answer:** Salma Hayek Pinault

While "Salma Hayek Pinault" is a valid answer, "Salma Hayek" is a second answer that should also be considered valid. To address these cases, when re-labeling examples we comprehensively list all reasonable valid responses. If the question is sufficiently open-ended and ambiguous that too many possible options might be valid, we mark the question as bad.

### Chain-of-Thought prompt template

We use a chain-of-thought prompt for evaluation on all datasets except for VQA V2.0 [14], for which there is general no need for multiple reasoning steps. The specific prompt varies slightly between benchmarks, however the general templates are as follows:

**Open-ended Question:**Answer the following {category} question.  {question}  Think step-by-step. Then, answer in the format "Answer: XXX".

**Multiple-Choice Question:**

Answer the following {category} question.  {question}  Options:  A) {option A}  B) {option B}  C) {option C}  D) {option D}  Think step-by-step. Then, answer in the format "Answer: A".

For BIG-bench [37], we use (A) instead of A) for multiple choice style to align with the prompt used by the original authors. For open-ended math questions, we additionally specify to respond with an integer and to exclude additional formatting, as model often style outputs with latex styling. We exclude chain-of-thought prompting for VQA v2.0, as the questions rarely require any explicit reasoning to answer. We also find that, in practice, models rarely actually think step-by-step for simple visual reasoning questions, even when prompted to do so.

[MISSING_PAGE_FAIL:16]

### Rounding up primes

When inspecting model failures on SVAMP [30] and GSM8K [8], we found two cases in which Claude 3.5 Sonnet rounded up its answer following a division even though the quotient was already a whole number:

**Question:** The school is planning a field trip. The school has 67 classrooms. There are 66 students in each classroom in the school. If there are 6 seats on each school bus. How many buses are needed to take the trip?

**Solution:** 737

**Claude 3.5 Sonnet**

...To find the number of buses needed, we divide the total number of students by the number of seats per bus: 4,422 + 6 = 737. However, since we can't have a fraction of a bus, we need to round up to the next whole number to ensure all students have a seat. Therefore, the school needs 738 buses to take all students on the trip. Answer: 738

**Question:** Cecilia just bought a new puppy. According to her veterinarian, she has to feed the puppy 1 cup of dog food every day for the first 180 days. Then she has to feed the puppy 2 cups of dog food every day for the rest of its life. If one bag of dog food contains 110 cups, how many bags of dog food will Cecilia use in the first year?

**Solution:** 5

**Claude 3.5 Sonnet**

...Calculate how many bags of dog food this equals: 550 cups \(\div\) 110 cups per bag = 5 bags. However, since Cecilia can't buy a fraction of a bag, she'll need to round up to the next whole bag. Answer: 6

Interestingly, the solutions to these questions both have few divisors (5 is prime, and 737 is the product of 11 and 67, two primes). So, we investigate whether this failure is related to the "closeness" of the solution to being prime. Specifically, we create three examples similar to the bus question above that require only a single division, such as the following:

**Question:** A tour group with \(\{n*k\}\) people needs to hire buses to travel to their next destination. If each bus can fit \(\{k\}\) people, how many buses does the tour group need?

**Solution:**\(n\)

We randomly select values of \(n\) ranging from 3 to 300, with 0, 1, 2, or 3 non-trivial divisor pairs and measure the average number of failures when varying \(k\) from 3 to 9. We isolate only errors caused from incorrectly rounding the whole number quotient by filtering our failures caused by an incorrect division. We find that indeed, the less non-trivial divisor pairs \(n\) has (i.e., the closer it is to being prime), the more frequently this failure mode occurs (see Figure D2).

Figure D2: **Rounding up primes. Frequency of failures of Claude Sonnet 3.5 on procedurally generated questions involving only a single division step, when the quotient (i.e., the result of the division) has a given number of non-trivial divisor pairs. We filter out failures caused by a failed division, to only include cases where the model intentionally (and incorrectly) rounds up a whole number.**

[MISSING_PAGE_FAIL:19]

Ambiguous question, _SQuAD2.0_

**Context:** [...] The Edict reaffirmed Catholicism as the state religion of France, but granted the Protestants equality with Catholics under the throne and a degree of religious and political freedom within their domains. The Edict simultaneously protected Catholic interests by discouraging the founding of new Protestant churches in Catholic-controlled regions.

**Question:** What did the Edict do for Huguenots in France?

This question is open-ended and has no clear set of possible solutions.

Mislabeled question, _DROP_

**Context:** [...] On September 29, the battle ended with up to 1600 marauders killed in the estate, and Xu Hai's body was found in a nearby stream. On October 10, Chen Dong, Ye Ma, and Xu Hai's hostage brother were all executed in Jiaxing, marking the end of the wokou invasion of 1556.

**Question:** How many days after the battle ended were Chen Dong, Ye Ma, and Xu Hai's hostage brother were all executed in Jiaxing?

**Solution:** 12

The hostages were executed ended 11 days after, not 12.

Mislabeled question, _DROP_

**Context:** [...] The 15th Nepal China's Tibet Economic and Trade Fair was held on 17-22 November 2015 in Bhrikutmandap, Kathmandu Nepal.

**Question:** How many days 15th Nepal China's Tibet Economic and Trade Fair was held in 2015?

**Solution:** 5

The fair was was held for 6 days, not 5.

Mislabeled question, _HotPotQA_

**Paragraph A, Mwabvi Wildlife Reserve:** [...] Buffalo still bathe in the Mwabvi river, and, although [...]

**Paragraph B, African buffalo:** The African buffalo or Cape buffalo ("Syncerus caffer") is a large African bovine. [...]

**Question:** What river can a large African bovine be seen bathing in in the Mwabvi Wildlife Reserve?

**Solution:** Buffalo

The correct solution is the Mwabvi river.

Figure E2: Examples of mislabeled or poorly written questions from reading comprehension benchmarks.

Figure E3: Examples of ambiguous questions from VQA V2.0.

Examples of model failures

To highlight the simple types of questions frontier LLMs still fail on, below we show examples of model failures on examples from the revised benchmarks.

_Logical Deductions 3 Object_

The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. In an antique car show, there are three vehicles: a motorcyle, a minivan, and a tractor. The minivan is older than the tractor. The minivan is the second-newest.

Options:

(A) The motorcyle is the newest

(B) The minivan is the newest

(C) The tractor is the newest

**Answer:** (C)

**GPT-4o-mini:** (A)

**o1-mini:** (A)

_Winograd_

**Phrase:** John couldn't see the stage with Billy in front of him because he is so short.

**Question:** What does "he" in "he is so short." refer to in the phrase?

Option A: John

Option B: Billy

**Answer:** (A)

**GPT-4o:** (B)

**GPT-4o-mini:** (B)

**Llama 3.1 70B Instruct:** (B)

**Llama 3.1 405B Instruct:** (B)

_Winograd_

**Phrase:** Since it was raining, I carried the newspaper over my backpack to keep it dry.

**Question:** What does "it" in "keep it dry" refer to in the phrase?

Option A: The newspaper

Option B: The backpack

**Answer:** (B)

**GPT-4o-mini:** (A)

**Llama 3.1 70B Instruct:** (A)

**Llama 3.1 70B Instruct:** (A)

**Gemeni 1.5 Flash:** (A)

**Gemeni 1.5 Pro:** (A) **Mistral Small:** (A) **o1-mini:** (A)_GSM8K_

Jean is two years older than Mark. Two years ago Mark was 5 years older than half Jan's age. If Jan is 30 how old is Jean?

**Answer:** 23

**GPT-4o:** 24

**Claude 3.5 Sonnet** 24

_SVAMP_

Christina is planning a birthday party and needs.75 gift bags per invited guest, because 1/4 of attendees don't show up. She invited 16 friends. Gift bags are $2 each. How much will she spend?

**Answer:** 24

**GPT-4o:** 18

**Llama 3.1 70B Inst:** 18

**Llama 3.1 405B Inst:** 18

**Gemini 1.5 Flash:** 18

**Gemini 1.5 Pro:** 18

**Mistral Small:** 18

_SVAMP_

If a bag of marbles costs $20 and the price increases by 20% of the original price every two months, how much would a bag of marbles cost after 36 months?

**Answer:** 92

**GPT-4o:** 767

**Llama 3.1 405B Inst:** 532

**Gemini 1.5 Pro:** 800

**Mistral Small:** 219

**Context:** After a tough loss at home, the Bengals would remain at home for a game against the Bears. In the first quarter, the Bears took an early lead when Jordan Howard ran for a 21-yard touchdown (with a failed PAT) to make it 6-0. The Bengals however took the lead later on in the quarter when Andy Dalton found Brandon LaFell on a 14-yard pass to make it 7-6. The Bears retook the lead in the second quarter when Mike Nugent kicked 2 field goals: From 34 and 27 yards out to make it 9-7 and then 12-7 at halftime. The Bears then shut out the Bengals the entire second half while they score themselves: In the third quarter Mitchell Trubisky ran for a 4-yard touchdown to make it 19-7 for the only score of the third quarter. In the fourth quarter, they wrapped up the scoring when Trubisky found Adam Shaheen on a 1-yard pass to make it 26-7. Howard then ran for an 8-yard touchdown to make the final score 33-7. With the loss, the Bengals dropped to 5-8. The team is also assured their second straight non-winning season.

**Question:** How many points in total were scored in the second half?

**Answer:** "21" or "21 points"

**GPT-4o-mini:** 18 points

**Llama 3.1 70B Inst:** 20

**Gemini 1.5 Flash:** 33

**Mistral Small:** 24

**Mistral Large:** 19 points

_SQuAD2.0_

**Context:** Before the foundation can be dug, contractors are typically required to verify and have existing utility lines marked, either by the utilities themselves or through a company specializing in such services. This lessens the likelihood of damage to the existing electrical, water, sewage, phone, and cable facilities, which could cause outages and potentially hazardous situations. During the construction of a building, the municipal building inspector inspects the building periodically to ensure that the construction adheres to the approved plans and the local building code. Once construction is complete and a final inspection has been passed, an occupancy permit may be issued.

**Question:** What does digging a foundation prevent damage to?

**Answer:** N/A

The verification steps prevent damage, not the digging. This question is unanswerable from the context.

**GPT-4o:** Utility lines

**GPT-4o-mini:** Utility lines

**Llama 3.1 70B Inst:** Electrical, water, sewage, phone, and cable facilities

**Llama 3.1 405B Inst:** Existing utility lines

**Gemini 1.5 Flash:** Utility lines

**Mistral Small:** electrical, water, sewage, phone, and cable facilities

**Mistral Large:** Existing utility lines

**o1-mini:** Existing utility lines