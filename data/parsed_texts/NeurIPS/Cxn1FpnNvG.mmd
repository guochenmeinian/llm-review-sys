# Neural Circuits for Fast Poisson Compressed Sensing

in the Olfactory Bulb

 Jacob A. Zavatone-Veth\({}^{1,2}\), Paul Masset\({}^{1,3}\), William L. Tong\({}^{1,4,5}\), Joseph D. Zak\({}^{6}\),

**Venkatesh N. Murthy\({}^{1,3}\), Cengiz Pehlevan\({}^{1,4,5}\)**

\({}^{1}\)Center for Brain Science, \({}^{2}\)Department of Physics,

\({}^{3}\)Department of Molecular and Cellular Biology,

\({}^{4}\)John A. Paulson School of Engineering and Applied Sciences,

\({}^{5}\)Kempner Institute for the Study of Natural and Artificial Intelligence,

Harvard University

Cambridge, MA 02138

\({}^{6}\)Department of Biological Sciences,

University of Illinois at Chicago

Chicago, IL 60607

jzavatoneveth@g.harvard.edu, paul_masset@fas.harvard.edu,

vnmurthy@fas.harvard.edu, cpehlevan@seas.harvard.edu

JaZ-V and PM contributed equally to this work.VNM and CP jointly supervised this work.

###### Abstract

Within a single sniff, the mammalian olfactory system can decode the identity and concentration of odorants wafted on turbulent plumes of air. Yet, it must do so given access only to the noisy, dimensionally-reduced representation of the odor world provided by olfactory receptor neurons. As a result, the olfactory system must solve a compressed sensing problem, relying on the fact that only a handful of the millions of possible odorants are present in a given scene. Inspired by this principle, past works have proposed normative compressed sensing models for olfactory decoding. However, these models have not captured the unique anatomy and physiology of the olfactory bulb, nor have they shown that sensing can be achieved within the 100-millisecond timescale of a single sniff. Here, we propose a rate-based Poisson compressed sensing circuit model for the olfactory bulb. This model maps onto the neuron classes of the olfactory bulb, and recapitulates salient features of their connectivity and physiology. For circuit sizes comparable to the human olfactory bulb, we show that this model can accurately detect tens of odors within the timescale of a single sniff. We also show that this model can perform Bayesian posterior sampling for accurate uncertainty estimation. Fast inference is possible only if the geometry of the neural code is chosen to match receptor properties, yielding a distributed neural code that is not axis-aligned to individual odor identities. Our results illustrate how normative modeling can help us map function onto specific neural circuits to generate new hypotheses.

## 1 Introduction

Sensory systems allow organisms to detect physical signals in their environments, enabling them to maximize fitness by acting adaptively. This experience of the physical environment, also known as the _Umwelt_, depends on the sensors and sensory organs of each organism [1, 2]. Throughout evolution, organisms have developed specialized sensory mechanisms to extract specific information about thephysical world. In vision and audition--the most studied sensory modalities in neuroscience--stimuli are characterized by intuitive metrics such as orientation or frequency, which have been shown to map onto neural representations from the earliest stages of the sensory systems [3; 4; 5]. One can continuously vary the orientation of an object or the pitch of a tone and quantify resulting changes in perception and neural representations. From a computational point of view, this structure in the representations can be viewed as optimizing the information transfer in the network [6; 7; 8; 9; 10; 11; 12; 13; 14; 15; 16].

In contrast, the geometric structure of the olfactory world is far less clear: How can one 'rotate' a smell? Despite significant effort, attempts to find such structure in olfactory stimuli and link that geometry to maps in olfactory areas have succeeded only in identifying coarse principles for high-level organization, far from the precision of orientation columns or tonotopy in visual and auditory cortices [17; 18; 19]. In the absence of geometric intuitions, the principles of compressed sensing (CS) have emerged as an alternative paradigm for understanding olfactory coding [20; 21; 22; 23; 24; 25; 26; 27]. This framework provides a partial answer to the question of how an organism could identify which of millions of possible odorants are present given the activity of only a few hundred receptor types [28; 29; 30; 31; 32; 33]. However, existing CS circuit models do not admit convincing biologically-plausible implementations that can perform fast inference at scale. Indeed, many proposals assume that the presence of each odorant is represented by a single, specialized neuron, which is inconsistent with the distributed odor coding observed _in vivo_[34; 35; 22; 36]. This axis-aligned coding does not leverage the geometric structure of the sensory space, which can be defined even in the absence of interpretable dimensions [37; 38; 39; 40; 41].

In this paper, we propose a Poisson CS model for the mammalian olfactory bulb. Our primary contributions are as follows:

* We derive a normative CS circuit model which can be mapped onto the circuits of the bulb (SS3). Importantly, this mapping goes beyond basic counting of cell types; it includes detailed biological features like symmetric coupling and state-dependent inhibition (SS4).
* We show that this model enables fast, accurate inference of odor identity in a biologically reasonable regime where tens of odorants are present in a given scene (SS5). This fast inference is enabled by considering the geometry of the olfactory receptor code. This consideration leads to distributed odor coding, resolving a major tension between previous CS circuit models and neural data.
* We extend our circuit model to allow Bayesian inference of uncertainty in odor concentrations (SS6).

In total, our results demonstrate the importance of considering representational geometry when trying to understand neural coding in the olfactory bulb (OB). Importantly, we show that it is the geometry in the space defined by the receptor affinity (or OSN activation) that controls the speed of inference. This view is distinct from previous geometric theories of olfaction, which have focused on the space of odorants [42; 43]. We propose that thinking in terms of the geometry of OSN coding will allow for deeper understanding of early olfactory processing.

## 2 Related work and review of the olfactory sensing problem

We begin with a review of the principles of olfactory coding, and of previous CS models for olfactory circuits. The structural logic of early olfactory processing is broadly conserved across the animal kingdom (Fig. 1A) [44; 45; 46; 47], and this distinctive circuit structure is thought to play a key role in the computational function of the olfactory bulb [48; 49; 50]. In mammals, volatile odorants are first detected by olfactory receptor (ORs) proteins expressed on the surface of olfactory sensory neurons (OSNs). Each OSN expresses only a single OR type; in humans there are around 300 distinct ORs, in mice around 1000 [45]. Importantly, most ORs have broad affinity profiles, and the OSN code for odor identity is combinatorial [51]. In contrast to the immune system's highly adaptable chemical recognition capabilities arising from somatic recombination [52], ORs are hard coded into the genome as single genes [53], and therefore can only change over evolutionary timescales [54; 55]. Some adaptation of expression levels across the receptors is possible [56], but the chemical affinity of the receptor array is fixed. OSNs expressing the same OR then converge onto the same glomerulus, synapsing onto the principal projection cells of the olfactory bulb (OB), the mitral and tufted cells. These in turn send signals to olfactory cortical areas. The OB contains several types of inhibitory interneurons, whose computational role remains to be clarified [49]. Importantly, the excitatory mitral and tufted cells are not reciprocally connected across glomeruli. Instead, they are connected through a network of inhibitory granule cells, the most numerous cell type in the OB [57].

In our model, we will focus on a particular task: odor component identification within a single sniff [58; 59]. This computational problem differs from many experimental tasks focusing on discrimination between two odorants [60], which underlie most scientific work on rodent olfactory decision making [61; 62; 60]. Here, the goal is to identify the components of a complex olfactory scene [63; 64; 65; 66]. Importantly, the limits of human performance in this setting remain to our knowledge unknown [67; 68]. To render the problem more tractable, we will make a number of simplifications of the anatomy and physiology of the OB. We will not distinguish between mitral and tufted cells, the two classes of projection neurons. Recent works have shed some light on the distinct computational roles of these cell types; these distinctions are likely to become important when considering richer environmental dynamics than we do here [69; 70]. We will also ignore the fact that odorant concentrations can vary over many orders of magnitude, and OSN responses to large changes in concentration are strongly nonlinear. Here, we will focus on concentration changes of one or two orders of magnitude. Within such ranges, the responses of OB neurons are well characterized by linear models [71].

On the theoretical side, it is widely recognized that the tremendous compression of dimensionality inherent in the transformation from odorant molecules to receptor activity means that the olfactory decoding problem is analogous to the one faced in compressed sensing (CS) [72; 73; 74; 21; 22; 23; 24; 25; 26; 27; 28; 29; 30; 31; 32; 33]. Classical CS theory shows that sparse high-dimensional signals can be recovered from a small number of random projections [28; 29; 30; 31; 32; 33]. Inspired by these results, previous works have used the principles of CS to build circuit models for olfactory coding [72; 73; 74; 21; 22; 23; 24; 25; 26; 27; 28; 29; 30; 31; 32; 33; 34; 35; 36; 37; 38; 39; 40; 41; 42; 43; 44; 45; 46; 47; 48; 49; 50; 51; 52; 53; 54; 55; 56; 57; 58; 59; 60; 61; 62; 63; 64; 65; 66; 67; 68; 69; 70; 71; 72; 73; 74]. However, these model circuits do not map cleanly onto the neural circuits of the OB and the olfactory cortical areas, particularly because they often assume each granule cell encodes exactly one odorant. Moreover, these works usually assume a Gaussian noise model for OSNs, which is biologically unrealistic (but see [21; 22]). Instead, OSN activity is better captured by a Poisson noise model [75; 76]. Some theoretical guarantees for Poisson CS are known, but the situation is less well-understood than in the Gaussian case [77; 78; 33; 72].

## 3 A neural circuit architecture for Poisson compressed sensing

We now derive a normative, rate-based neural circuit model that performs Poisson CS, which in subsequent sections we will map onto the circuitry of the OB. The design of this model will follow general biological principles, without initially drawing on specific knowledge of the OB. With the goal of sensing within a single sniff in mind, the circuit's objective is to rapidly infer the odorant concentrations \(\mathbf{c}\in\mathbb{R}_{+}^{n_{\mathrm{odor}}}\) underlying a single, static sample of OSN activity \(\mathbf{s}\in\mathbb{R}_{+}^{n_{\mathrm{OSN}}}\). For simplicity, we model the mean activity of OSNs as a linear function of the concentration, with a receptor affinity matrix \(\mathbf{A}\in\mathbb{R}_{+}^{n_{\mathrm{OSN}}\times n_{\mathrm{odor}}}\) and a baseline rate \(\mathbf{r}_{0}\in\mathbb{R}_{+}^{n_{\mathrm{OSN}}}\). As motivated above,

Figure 1: Circuit architecture of the mammalian olfactory bulb. **A.** Outline of the anatomy of OB circuits. **B.** Fit of the responses of 228 OSN glomerular responses to a panel of 32 odorants. (Gray line, response of single glomeruli, Blue line, Fitted average response.) See Appendix F for details of how these data were collected. **C.** Affinity matrix generated from the data-driven model ensemble. For illustrative purposes, we use only 30 receptors and 100 odorants; in simulations we use 300 receptors to roughly match humans [45], and 1000 or more odorants.

we use a Poisson noise model for OSN activity given the underlying concentration signal \(\mathbf{c}\), and correspondingly a Gamma prior over concentrations with shape \(\bm{\alpha}\in\mathbb{R}_{+}^{n_{\mathrm{odor}}}\) and scale \(\bm{\lambda}\in\mathbb{R}_{+}^{n_{\mathrm{odor}}}\):3

Footnote 3: See Appendix A for a detailed description of our notational conventions.

\[\mathbf{s}\,|\,\mathbf{c}\sim\mathrm{Poisson}(\mathbf{r}_{0}+\mathbf{A}\mathbf{c }),\qquad\mathbf{c}\sim\mathrm{Gamma}(\bm{\alpha},\bm{\lambda}).\] (1)

Given this likelihood and prior, we construct a neural circuit to compute the maximum _a posteriori_ (MAP) estimate of the concentration \(\mathbf{c}\) using gradient ascent on the log-posterior probability. Here, we sketch the derivation, deferring some details to Appendix B. Our starting point is the gradient ascent equation

\[\dot{\mathbf{c}}(t)=\bm{\nabla}_{\mathbf{c}}\log p(\mathbf{c}\,|\,\mathbf{s}) =\mathbf{A}^{\top}[\mathbf{s}\oslash(\mathbf{r}_{0}+\mathbf{A}\mathbf{c})- \mathbf{1}]+(\bm{\alpha}-\mathbf{1})\oslash\mathbf{c}-\bm{\lambda},\] (2)

where \(\oslash\) denotes elementwise division and \(\dot{\mathbf{c}}(t)=d\mathbf{c}/dt\). Here, the estimate \(\mathbf{c}\) is formally constrained to \(\mathbb{R}_{+}^{n_{\mathrm{odor}}}\); in numerical simulations we will sometimes ignore this constraint. Circuit algorithms of this form were studied in previous work by Grabska-Barwinska et al. [21].

However, in this most basic setup there is a one-to-one mapping between neurons and odorants, which is at variance with our knowledge of biological olfaction (SS2). To distribute the code, we instead use a projected setup where the firing rates \(\mathbf{g}\in\mathbb{R}^{n_{\mathrm{g}}}\) of the neurons are mapped to the concentration estimate \(\mathbf{c}\) through a matrix \(\bm{\Gamma}\in\mathbb{R}^{n_{\mathrm{odor}}\times n_{\mathrm{g}}}\): \(\mathbf{c}(t)=\bm{\Gamma}\mathbf{g}(t)\). Even if \(\bm{\Gamma}\) is non-square--in particular, if \(n_{\mathrm{g}}>n_{\mathrm{odor}}\)--so long as \(\bm{\Gamma}\bm{\Gamma}^{\top}\) is positive-definite and the rates follow the dynamics \(\tau_{\mathrm{g}}\ddot{\mathbf{g}}(t)=(\bm{\Lambda}\bm{\Gamma})^{\top}[\mathbf{ s}\oslash(\mathbf{r}_{0}+\bm{\Lambda}\mathbf{G}\mathbf{g})-\mathbf{1}]+\bm{ \Gamma}^{\top}[(\bm{\alpha}-\mathbf{1})\oslash(\bm{\Gamma}\mathbf{g})-\bm{ \lambda}]\) for some time constant \(\tau_{\mathrm{g}}\), the concentration estimates will still converge to the MAP. This corresponds to preconditioned gradient ascent [39, 79]. Again, we formally require the constraint that \(\mathbf{c}\in\mathbb{R}_{+}^{n_{\mathrm{odor}}}\), which translates into a constraint on \(\mathbf{g}\). Most simply, we may take \(\mathbf{g}\in\mathbb{R}_{+}^{n_{\mathrm{g}}}\) and choose \(\bm{\Gamma}\) to be positivity-preserving.

These dynamics include two divisive non-linearities, which can be challenging to implement in biophysical models of single neurons [80]. Using the approach proposed by Chalk et al. [81], we can linearize the inference by introducing two additional cell types that have as their fixed points the elementwise divisions \(\mathbf{s}\oslash(\mathbf{r}_{0}+\bm{\Lambda}\mathbf{T}\mathbf{g})\) and \((\bm{\alpha}-\mathbf{1})\oslash(\bm{\Gamma}\mathbf{g})\). Concretely, we introduce cell types with rates \(\mathbf{p}\in\mathbb{R}^{n_{\mathrm{ONN}}}\) and \(\mathbf{z}\in\mathbb{R}^{n_{\mathrm{odor}}}\) such that their fixed-point rates for fixed \(\mathbf{g}\) are \(\mathbf{p}^{*}=\mathbf{s}\oslash(\mathbf{r}_{0}+\bm{\Lambda}\mathbf{T}\mathbf{g})\) and \(\mathbf{z}^{*}=(\bm{\alpha}-\mathbf{1})\oslash(\bm{\Gamma}\mathbf{g})\), respectively. This yields the coupled circuit dynamics

\[\mathbf{c}(t) =\bm{\Gamma}\mathbf{g}(t), \tau_{\mathrm{g}}\ddot{\mathbf{g}}(t) =(\bm{\Lambda}\bm{\Gamma})^{\top}(\mathbf{p}-\mathbf{1})+\bm{ \Gamma}^{\top}(\mathbf{z}-\bm{\lambda}),\] \[\tau_{\mathrm{p}}\ddot{\mathbf{p}}(t) =\mathbf{s}-\mathbf{p}\oslash(\mathbf{r}_{0}+\bm{\Lambda}\mathbf{T }\mathbf{g}), \tau_{\mathrm{z}}\ddot{\mathbf{g}}(t) =\bm{\alpha}-\mathbf{1}-\mathbf{z}\oslash\mathbf{c},\] (3)

for cell-type-specific time constants \(\tau_{\mathrm{g}}\), \(\tau_{\mathrm{p}}\), and \(\tau_{\mathrm{z}}\), where \(\oslash\) denotes elementwise multiplication. In the limit \(\tau_{\mathrm{p}},\tau_{\mathrm{z}}\ll\tau_{\mathrm{g}}\), this circuit will recover the MAP gradient ascent. If \(\tau_{\mathrm{p}}\) and \(\tau_{\mathrm{z}}\) are not infinitely fast relative to \(\tau_{\mathrm{g}}\), we expect these dynamics to approximate the desired dynamics [81] (see Appendix C for a preliminary analysis of the linear stability of the MAP fixed-point). We will test the accuracy of this approximation for biologically-reasonable time constants using numerical experiments. Moreover, \(\mathbf{p}\) should formally be constrained to \(\mathbb{R}_{+}^{n_{\mathrm{p}}}\), such that the non-negativity of the target ratio is respected. Finally, we note that in the special case \(\bm{\alpha}=\mathbf{1}\) in which the Gamma prior reduces to an exponential prior, the introduction of the cell type \(\mathbf{z}\) is no longer required.

## 4 Biological interpretation and predictions of the circuit model

We now argue that the normative model derived in the preceding section can be mapped onto the circuitry of the OB. In particular, though the model was derived based only on general biological principles, its specific features are biologically implementable based on the detailed anatomy and physiology of the OB. In terms of the levels of understanding of neural circuits proposed by David Marr, this is an example of how normative modeling can bridge the gap between algorithmic and mechanistic understanding [82, 83].

As foreshadowed by our notation, we interpret the cell type \(\mathbf{g}\) as the granule cells of the OB, and the cell type \(\mathbf{p}\) as the mitral cells, which are projection neurons. Provided that the elements of the matrix \(\bm{\Lambda}\bm{\Gamma}\) are non-negative, this interpretation is justified at the coarsest level by the signs with which the two cell types appear in the dynamics: the \(\mathbf{p}\) neurons excite the \(\mathbf{g}\) neurons, which in turn inhibit the \(\mathbf{p}\) neurons. Finally, we interpret the cell type \(\mathbf{z}\) as representing a form of cortical feedback. In the remainder of this section, we will justify this mapping in detail.

### Circuit anatomy: weight transport, dendro-dendritic coupling, and cortical feedback

The first salient feature of the dynamics (3) is that the cell types \(\mathbf{g}\) and \(\mathbf{p}\) do not make direct lateral connections amongst themselves. Rather, they connect only indirectly through the neurons of the opposing cell type, matching the connectivity structure of mitral and granule cells (Fig. 1A). Moreover, the synaptic weights \(\mathbf{A}\mathbf{\Gamma}\) of the connections from \(\mathbf{g}\) to \(\mathbf{p}\) neurons mirror exactly the weights \((\mathbf{A}\mathbf{\Gamma})^{\top}\) of the connections from \(\mathbf{p}\) to \(\mathbf{g}\) neurons. Naively, this creates a weight transport problem of the form that renders backpropagation biologically implausible: how should the exact transpose of a matrix be copied to another synapse [84; 85; 86]? However, in the unique case of the OB this does not pose a substantial obstacle, as the mitral and granule cells are coupled by dendro-dendritic synapses, meaning that bi-directional connectivity occurs at a single physical locus (Fig. 1) [49; 57; 87].

This interpretation accounts for the interactions between the cell types \(\mathbf{g}\) and \(\mathbf{p}\) in the dynamics (3), but what about the cell type \(\mathbf{z}\)? These cells give direct excitatory input to the granule cells \(\mathbf{g}\) with weights \(\mathbf{\Gamma}^{\top}\), and represent the concentration-dependent contribution to the log-prior gradient. We can therefore interpret these cells as representing feedback from olfactory cortical areas to the OB, which arrives at the granule cells [88; 89; 90]. Though our model can thus flexibly incorporate cortical feedback, for our subsequent simulations we will focus on the simple case in which the prior is static and has \(\boldsymbol{\alpha}=\mathbf{1}\), in which case it reduces to an exponential and feedback is not explicitly required.

Given this mapping of the cell types of our model to the cell types of the bulb, we will henceforth choose the membrane time constants to match experiment, taking \(\tau_{p}=20\) ms [91] and \(\tau_{g}=30\) ms [92]. This matching of timescales is required for our comparison of model inference timescales to the timescale of a single sniff to be meaningful.

### Choosing the affinity matrix

In theories of Poisson CS, the optimal sensing matrix \(\mathbf{A}\) is one that has columns that are in some precise sense as orthogonal as possible, so that it acts as an approximate isometry [27; 28; 29; 30; 31; 32; 33]. However, biologically, the olfactory system is not free to choose optimal sensors. Rather, the affinity profiles of each receptor are dictated by biophysics and by evolutionary history [53; 54; 93]. To build a more realistic model for OSN sensing, we therefore turn to biological data. Using two-photon calcium imaging, we recorded the responses of 228 mouse OSNs glomeruli to 32 odorants (see Appendix F for details; these data were previously published in [94]). In Fig. 1B, we show that the distribution of responses is well-fit by a \(\mathrm{Gamma}\) distribution with shape 0.37 and scale 0.36 (Appendix F). We then define our ensemble of sensing matrices \(\mathbf{A}\) by drawing their elements as independently and identically distributed \(\mathrm{Gamma}(0.37,0.36)\) random variables. An example matrix drawn from this ensemble is shown in Fig. 1C.

Figure 2: State-dependent inhibition of mitral cells. **A.** Upon stimulation, mitral cells exhibit a transient burst of activity followed by relaxation to a plateau. Darker color indicates stronger stimulation. **B.** In the circuit for Poisson CS we propose (blue line), the inhibition due to the activation of a second mitral cell (\(MC_{B}\)) is gated by the activity of the cell we are recording from (\(MC_{A}\)), as observed experimentally by Arevian et al. [95] in their Fig. 2D. This state-dependent gating does not occur in a circuit derived from a Gaussian noise model (orange line). Rates are normalized to the maximal stimulation of the principal cell. Dashed line indicates the unity line. **C.** Strength of inhibition as a function of time and of stimulation intensity; c.f. [95] Fig. 3B.

### Divisive predictive coding by mitral cells

In our model (3), odorant concentration estimates are decoded from granule cell activity. This feature is shared with previous Gaussian CS models of olfactory coding [20; 24; 25]. Our model, however, matches biology more closely than these previous works because it allows for a distributed code rather than assuming that each granule cell codes for a single odorant.

What, then, is the functional role of the mitral cells in our model? We can interpret their dynamics as implementing a form of predictive coding, in which they are trying to cancel their input by the current prediction. Because the mitral cell activity converges to the ratio of their input to the prediction, this a divisive form of predictive coding [81]. In contrast, a Gaussian noise model gives a subtractive form of predictive coding in which the activity converges to the difference between input and prediction (Appendix D) [15]. In Fig. 2A, we show example timecourses of model mitral cell activity following the onset of a stimulus. Consistent with experimentally-measured responses [96; 97], a sharp transient response at the onset of stimulation is followed by decay to a low level of tonic activity (Fig. 2A).

### State-dependent inhibition of mitral cells

A salient feature of our circuit model is that the inhibition from the granule cells onto a mitral cell is gated by the activity of the mitral cell itself (3). This state-dependent inhibition is reminiscent of _in vitro_ experiments showing that granule cell mediated inhibition is activity dependent [95]. In these experiments, Arevian et al. measured the activity of a primary mitral cell (\(MC_{A}\)) while increasing its level of stimulation under two conditions. In the first condition, no other cells in the circuit are being stimulated. In the second condition, they also activate another mitral cell (\(MC_{B}\)). The activation of the second mitral cell leads to the activation of granule cells connected to both mitral cells and a reduction in the firing evoked by stimulation of the primary cell alone. Strikingly, these authors showed that this inhibition is dependent on the activity of the primary cell [95].

Here, we show that our proposed circuit reproduces these observations of state-dependent inhibition, and that they do not arise in a similarly-constructed circuit for a Gaussian noise model. To model Arevian et al. [95]'s _in vitro_ experiments, we simulated a reduced circuit with \(2\) mitral cells and \(10\) granule cells. We stimulated \(MC_{A}\) with an input \(s_{A}\in[1:400]\) while toggling on or off the stimulation \(s_{B}=80\) of the second mitral cell \(MC_{B}\). As observed experimentally, when \(MC_{B}\) is activated, the inhibition of the primary mitral cell \(MC_{A}\) is state dependent (Fig. 2B). To show that this effect arises from our Poisson circuit, we build a similar inference circuit with a Gaussian noise model (Appendix D). Under similar conditions, the inhibition in the Gaussian circuit is independent of the activity of the primary mitral cell \(MC_{A}\) (Fig. 2B). Furthermore, the dynamics of the relative inhibition qualitatively recapitulate those observed experimentally, with sustained inhibition throughout the stimulation period at the stimulation level of \(MC_{A}\) leading to maximal inhibition and inhibition followed by relaxation for stronger stimulation levels of \(MC_{A}\) (Fig. 2C and see [95] Fig. 3b).

## 5 Geometry, speed, and capacity

We have argued that the model introduced in SS3 could be implemented biologically, but can it perform at scale? Concretely, can a circuit of this architecture with neuron counts comparable to the human OB correctly identify which among a large set of odorants are present in a given scene? This is precisely the question of the capacity of the CS algorithm [98; 99]. In Fig. 3A, we present our algorithm with scenes composed of varying numbers of randomly-selected odorants out of a panel of 1000, which for simplicity we take to be at the same concentration. We first ask how many odorants can be reliably detected within a single sniff, i.e., 200 ms. To convert MAP concentration estimates into presence estimates, we simply binarize the estimated concentrations based on whether they are larger than half of the true odorant concentration. The ability of the one-to-one code to successfully detect odorants falls off rapidly, with the detection fraction falling below one-half even if only a handful of odorants are present (Fig. 3A).4

Footnote 4: We remark that, with a one-to-one code, our model is identical to that proposed by Grabska-Barwinska et al. [21] except for the introduction of the granule cells.

The limited capacity of the one-to-one code can be overcome by distributing the code in a way that takes into account the geometry of the sensing problem. Here, the information geometry of theproblem is governed by the sensing matrix \(\mathbf{A}\), which introduces correlations in the input signals to mitral cells because the off-diagonal components of \(\mathbf{A}^{\top}\mathbf{A}\) are non-negligible. To counteract these detrimental correlations, we can choose a readout matrix \(\mathbf{\Gamma}\) such that \(\mathbf{\Gamma}\mathbf{\Gamma}^{\top}\approx(\mathbf{A}^{\top}\mathbf{A})^{+}\) up to constants of proportionality, where we must take a pseudoinverse because \(\mathbf{A}^{\top}\mathbf{A}\) is highly rank-deficient (see Appendix G for details). Geometrically, this corresponds an approximation of natural gradient descent, in which we use the Gauss-Newton matrix instead of the Fisher information matrix because the latter is state-dependent for Poisson likelihoods [37, 38, 39, 40]. As a control, we also consider a naively distributed code with \(\mathbf{\Gamma}\mathbf{\Gamma}^{\top}\approx\mathbf{I}_{n_{\mathrm{odor}}}\). Distributing the code, even without accounting for the geometry of inference, markedly improves the single-sniff capacity to around 10-20, and taking into account the geometry produces a further improvement (Fig. 3A).

To gain a more granular view of how tuned geometry enables faster inference, in Fig. 3B we test the three models' detection capabilities at sub-sniff resolution. We can see that at long times--around a second after odorant onset--the naively distributed and geometry-aware codes achieve similar capacities of around 50-60 odorants. However, the geometry-aware code reaches this detection capacity within a single sniff, whereas the naively distributed code requires the full second of

Figure 3: Fast detection of many odorants. \(\mathbf{A}\). Fraction of odorants correctly detected within 100 ms (_left_), 200 ms (_center_), and 1 s (_right_) after odorant onset as a function of the number of odorants present, for models with one-to-one, naively distributed, and geometry-aware codes. Here, we consider a repertoire of 1000 possible odorants. \(\mathbf{B}\). Heatmap with overlaid smoothed contours of correct detection fraction as a function of number of present odorants and time window for models with one-to-one (_left_), naively distributed (_center_), and geometry-aware (_right_) codes. As in \(\mathbf{A}\), a repertoire of 1000 possible odorants is used. \(\mathbf{C}\). Threshold number of odorants for which half can be correctly detected as a function of total repertoire size within 100 ms (_left_), 200 ms (_center_), and 1 s (_right_) after odorant onset. See also Supp. Figs. G.1 and G.2 for versions of panels \(\mathbf{A}\) and \(\mathbf{B}\) with varying repertoire sizes, from which the capacities shown here are derived. See Appendix G for details of our numerical methods. Shaded patches show \(\pm 1.96\) SEM over realizations throughout.

processing time. The detection capacity of the one-to-one code reaches only around 20 odorants after 1 second, and even then does not appear to have reached its asymptote. These results illustrate two important conceptual points: First, when the strength of individual synapses is bounded, distributed coding can speed up dynamics by increasing the effective total input to a given neuron. Second, given a sensing matrix that induces strong correlations, geometry-aware distributed coding can accelerate inference by counteracting that detrimental coupling.

These tests show show that our model can reliably detect tens of odorants from a panel of thousands of possible odorants, but do not probe how the model's capacity scales to larger odor spaces. While the true dimensionality of odor space remains unknown [100; 101], there may be orders of magnitude more than thousands of possible odorants. As an upper bound, there are on the order of millions of known volatile compounds that are plausibly odorous [102]. Thus, it is important to determine how our model scales to more realistically-sized odor spaces. From the literature on compressed sensing performance bounds, we expect the threshold sparsity to decay slowly--roughly logarithmically--with increasing \(n_{\mathrm{odor}}\)[28; 29; 30; 31; 32; 33]. As a first step, in Supp. Figs. G.1 and G.2, we reproduce Fig. 3A-B for between 500 and 8000 possible odorants, showing that performance does indeed drop off slowly with increasing odor space dimension. To get a more precise estimate of how performance scales with repertoire size, in Fig. 3C we plot the threshold number of odorants for which half can be reliably detected as a function of the repertoire size, showing that for the geometry-aware code it decays only a bit faster than logarithmically. Our ability to simulate larger systems was limited by computational resources. This limitation is present in previous works, and the repertoires tested here are comparable to--or substantially larger than--those used in past studies [21; 22; 24; 26; 72].

## 6 Fast sampling for uncertainty estimation

Thus far, we have focused on a circuit that performs MAP estimation of odorant concentrations. However, to successfully navigate a dynamic, noisy world, animals must estimate sensory uncertainty at the timescale of perception [38; 103; 104; 105; 106; 107]. Fortunately, our circuit model can be easily extended to perform Langevin sampling of the full posterior distribution, allowing for uncertainty estimation while maintaining its attractive structural features. In Appendix B, we provide a detailed derivation of a model that implements Langevin sampling through the granule cells for a Poisson likelihood and Gamma prior. This yields a circuit that is identical to the MAP estimation circuit introduced in SS3 up to the addition of Gaussian noise to the granule cell dynamics:

\[\mathbf{c}(t) =\boldsymbol{\Gamma}\mathbf{g}(t) \tau_{\mathrm{g}}\dot{\mathbf{g}}(t) =(\mathbf{A}\boldsymbol{\Gamma})^{\top}(\mathbf{p}-\mathbf{1})+ \boldsymbol{\Gamma}^{\top}(\mathbf{z}-\boldsymbol{\lambda})+\boldsymbol{ \xi}(t)\] \[\tau_{\mathrm{p}}\dot{\mathbf{p}}(t) =\mathbf{s}-\mathbf{p}\odot(\mathbf{r}_{0}+\mathbf{A}\boldsymbol {\Gamma}\mathbf{g}) \tau_{\mathrm{g}}\dot{\mathbf{z}}(t) =\boldsymbol{\alpha}-\mathbf{1}-\mathbf{z}\odot\mathbf{c}.\] (4)

Here, \(\boldsymbol{\xi}(t)\) is a vector of \(n_{\mathrm{g}}\) independent zero-mean Gaussian noise processes with covariance \(\mathbb{E}[\xi_{j}(t)\xi_{j^{\prime}}(t^{\prime})]=2\tau_{\mathrm{g}}\delta_ {jj^{\prime}}\delta(t-t^{\prime})\), and once again the rates \(\mathbf{g}\) and \(\mathbf{p}\) should in principle be constrained to be non-negative. In this case, the readout matrix \(\boldsymbol{\Gamma}\) both preconditions the effective gradient force and shapes the structure of the effective noise \(\boldsymbol{\Gamma}\boldsymbol{\xi}\), allowing us to mold the geometry of the sampling manifold [37; 40]. By using a projected readout, we maintain the independence of the noise processes for different neurons. This both allows us to have many independent samplers--if \(n_{\mathrm{g}}>n_{\mathrm{odor}}\)--and is important for biological realism if we interpret the sampling noise as resulting from fluctuations in membrane potential due to synaptic noise [108].

As a test of how this sampling circuit performs, we consider a simple setup in which one set of odorants appears at a low concentration, and then a second set of odorants appear at a higher concentration while the low odorants are still present. This setup tests both the circuit's ability to converge rapidly enough to give accurate posterior samples within a single sniff, and its ability to correctly infer odorant concentrations even in the presence of distractors [63; 66]. In Fig. 4, we show that circuits with one-to-one or naively distributed codes do not give accurate estimates of the concentration mean within 200 ms, while tuning the geometry to match the receptor affinities enables fast convergence. All three circuits overestimate the posterior variance at short times, consistent with what one would expect for unadjusted Langevin samplers [109], but the geometry-aware model's estimate decays most rapidly towards the target. Therefore, when the synaptic weights are tuned, our circuit model can enable fast, robust estimation of concentration statistics, even in the presence of distractors.

## 7 Discussion

In this paper, we have derived a novel, minimal rate model for fast Bayesian inference in early olfaction. Unlike previously-proposed algorithms for CS in olfaction, this model has a clear mapping onto the circuits of the mammalian OB. We showed that this model successfully performs odorant identification across a biologically-relevant range of scene sparsities and circuit sizes. This model therefore exemplifies how normative approaches can blur the lines between algorithmic and mechanistic understanding of neural circuits [82; 83]. We now conclude by discussing possible avenues for future inquiry, as well as some of the limitations of our work.

One limitation of our simulations is that we have chosen to distribute the neural code randomly, and have allowed for negative entries in the mitral-granic synaptic weight matrix \(\mathbf{AT}\) (see methods in Appendix G). These features are not entirely biologically satisfactory. However, our model only captures the mitral cells and granule cells, overlooking a number of inhibitory cell types that could contribute to solving this problem, e.g., through feedforward inhibition onto mitral cells or lateral inhibition across granule cells [48; 49]. Fundamentally, negative values in the geometry-aware decoding matrix \(\mathbf{\Gamma}\) arise as although the affinity matrix is positive, its inverse will contain negative elements. A biologically-plausible realization of the geometry-aware code through the introduction of additional cell types could be achieved by decomposing the inverse into several components, yielding sparse, consistently-signed connectivity [110]. As a first step, in Supp. Fig. G.4 we show that similar performance to Fig. 4 can be achieved using a sparse non-negative randomly distributed code. As a result, one objective for future work will be to develop better models for the decoding matrix \(\mathbf{\Gamma}\) that result in more realistic connectivity.

Though our model captures two of the interesting features of the anatomy and physiology of the OB--symmetric dendroendritic coupling and state-dependent inhibition of mitral cells--there are

Figure 4: Fast uncertainty estimation using Langevin sampling of the posterior. Here, we use a simple concentration estimation task in which 5 randomly-selected ‘low’ odorants out of a panel of 1000 appear at concentration 10 at time 0 s, and then a further 5 randomly-selected ‘high’ odorants appear at concentration 40 at time 1 s. \(\mathbf{A}\). Smoothed timeseries of instantaneous concentration estimates for low, high, and background odorants, for models with one-to-one (_top_), naively distributed (_middle_), and geometry-aware (_bottom_) codes. Background odorant estimates are shown as mean \(\pm\) standard deviation over odorants. Dashed lines show true concentrations over time. \(\mathbf{B}\). Cumulative estimates of concentration mean for low (_top_) and high (_bottom_) odorants after the onset of the low odorants for one-to-one, naively distributed, and geometry-aware codes. Black lines indicate baseline estimates of the posterior mean. Thick colored lines indicate means over odorants. \(\mathbf{C}\). As in \(\mathbf{B}\), but for the estimated variance. \(\mathbf{D}\). As in \(\mathbf{B}\), but after the onset of high odorants. \(\mathbf{E}\). As in \(\mathbf{C}\), but after the onset of high odorants. See Appendix G for details of our numerical methods, and for individual-odor traces.

many biological details which we have not addressed. First, our linear model for OSN mean firing neglects receptor antagonism, gain control, and other nonlinear effects [94, 111, 112], which are known to affect the performance of Gaussian CS models for olfaction [26, 113]. Second, our models are rate-based, while neurons in the OB spike. In spiking implementations of sampling networks, the noise is not uncorrelated across neurons, complicating their biological interpretation [38, 114]. Constructing models that capture these richly nonlinear effects will be an important objective for future work. A first step towards such a nonlinear model would be to build a spiking network that approximates the rate-based models considered here, which could be accomplished using the efficient balanced network formalism for distributed spiking networks [38]. Another step would be to add a Hill function nonlinearity to the OSN model to approximate competitive binding, as studied for Gaussian compressed sensing by Qin et al. [26]. One challenge in constructing models that incorporate additional nonlinearity is that the simple linear strategy for distributing the code used here may no longer be directly applicable. In Appendix E, we illustrate this obstacle for the relatively simple case of a model with the same linear OSNs and Poisson likelihood but an \(L_{0}\) instead of Gamma prior, building on recent work on circuits for Gaussian CS with \(L_{0}\) priors [115].

A closely related point is that we model the weights of the synapses between mitral and granule cells as fixed, and do not consider synaptic plasticity. In particular, we assume that they are tuned to the statistics of the receptor affinities without specifying a mechanism by which this tuning could take place. In biology, receptor abundances and other OSN properties display activity-dependent adaptation over long timescales, meaning that the optimal tuning is unlikely to be static [56, 116]. Some past works have sought to incorporate plasticity of the mitral-granule cell synapses into decoding models [117, 24], tying into a larger body of research on how plasticity can enable flexible feature extraction in olfaction [118, 119]. This learning should lead to measurable changes in the population response to odorant panels, which our model predicts should be linked in a precise way to account for receptor-induced correlations in the responses to the most frequently present odorants. Experimentally characterizing and carefully modeling these changes in responses across timescales will be an interesting avenue for future work [120]. Experimental techniques to probe these ideas at the neural and behavioral level have recently been proposed [121, 122] which allow more precise control of stimulus and subjective geometry.

Though the circuit model derived in SS3 incorporates a general Gamma prior represented by cortical feedback, our simulations focus on the special case in which the prior reduces to an exponential, in which the feedback neurons are not needed. Future work will therefore be required to carefully probe the effect of incorporating a Gamma prior with non-unit shape and to dissect the structure of the resulting modeled cortical feedback. More generally, it will be interesting to extend our framework to incorporate data-adaptive priors. Importantly, the stimuli used in this paper constitute an extremely impoverished model for the richness of the true odor world; we do not account for its rich dynamical structure and co-occureure statistics. Adaptive priors as encoded by cortical feedback would allow circuits to leverage this structure, enabling faster and more accurate inference [88, 89, 90, 123, 124].

We conclude by noting that our work provides an example of how distributed coding can lead to faster inference than axis-aligned disentangled coding. In recent years, the question of when axis-aligned coding is optimal has attracted significant attention in machine learning and neuroscience [125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135]. Much of this work focuses on the question of when axis-aligned codes are optimal for energy efficiency or for generalization, whereas here we focus on the question of which code yields the fastest inference dynamics. These ideas are one example of the broader question of how agents and algorithms should leverage the rich geometry of the natural world to enable fast, robust learning and inference [136]. We believe that investigating how task demands and biological constraints affect the optimal representational geometry for that task is a promising avenue for illuminating neural information processing in brains and machines [136, 137, 41].

## Acknowledgments and Disclosure of Funding

We thank Naoki Hiratani, Shanshan Qin, and Vikrant Kapoor for useful discussions. This work was supported by NSF grants DMS-2134157 and CAREER IIS-2239780 to CP, NIH grants R01DC017311 and R01DC016289 to VNM, and NTT Research award A47994 to VNM. CP received additional support from a Sloan Research Fellowship. PM was partially supported by a grant from the Harvard Mind Brain Behavior Interfaculty Initiative. JDE was partially supported by NIH grant K99DC017754. This work has been made possible in part by a gift from the Chan Zuckerberg Initiative Foundation to establish the Kempner Institute for the Study of Natural and Artificial Intelligence. A subset of the computations in this paper were run on the FASRC cluster supported by the FAS Division of Science Research Computing Group at Harvard University.

## References

* von Uexkull [1909] Jakob Johann von Uexkull. _Umwelt und Innenwelt der Tiere_. Springer, 1909. doi: 10.1007/978-3-662-24819-5.
* Yong [2022] Ed Yong. _An Immense World: How Animal Senses Reveal the Hidden Realms Around Us_. Random House, New York, first edition, 2022. ISBN 9780593133231.
* Hubel and Wiesel [1959] D. H. Hubel and T. N. Wiesel. Receptive fields of single neurones in the cat's striate cortex. _The Journal of Physiology_, 148(3):574-591, 1959. doi: https://doi.org/10.1113/jphysiol.1959.sp006308. URL https://physoc.onlinelibrary.wiley.com/doi/abs/10.1113/jphysiol.1959.sp006308.
* Hubel and Wiesel [1962] D. H. Hubel and T. N. Wiesel. Receptive fields, binocular interaction and functional architecture in the cat's visual cortex. _The Journal of Physiology_, 160(1):106-154, 1962. doi: https://doi.org/10.1113/jphysiol.1962.sp006837. URL https://physoc.onlinelibrary.wiley.com/doi/abs/10.1113/jphysiol.1962.sp006837.
* Kandler et al. [2009] Karl Kandler, Amanda Clause, and Jihyun Noh. Tonotopic reorganization of developing auditory brainstem circuits. _Nature neuroscience_, 12(6):711-717, 2009. doi: https://doi.org/10.1038/nn.2332.
* Attneave [1954] Fred Attneave. Some informational aspects of visual perception. _Psychological Review_, 61(3):183, 1954. doi: https://doi.org/10.1037/h0054663.
* Barlow [2012] Horace B. Barlow. Possible Principles Underlying the Transformations of Sensory Messages. In _Sensory Communication_. The MIT Press, 09 2012. ISBN 9780262518420. doi: 10.7551/mitpress/9780262518420.003.0013. URL https://doi.org/10.7551/mitpress/9780262518420.003.0013.
* Srinivasan et al. [1982] Mandyam Veerambudi Srinivasan, Simon Barry Laughlin, A. Dubs, and George Adrian Horridge. Predictive coding: a fresh view of inhibition in the retina. _Proceedings of the Royal Society of London. Series B. Biological Sciences_, 216(1205):427-459, 1982. doi: 10.1098/rspb.1982.0085.
* Linsker [1988] R. Linsker. Self-organization in a perceptual network. _Computer_, 21(3):105-117, 1988. doi: 10.1109/2.36.
* van Hateren [1992] Johannes H. van Hateren. A theory of maximizing sensory information. _Biological Cybernetics_, 68(1):23-29, Nov 1992. ISSN 1432-0770. doi: 10.1007/BF00203134. URL https://doi.org/10.1007/BF00203134.
* Atick and Redlich [1992] Joseph J. Atick and A. Norman Redlich. What Does the Retina Know about Natural Scenes? _Neural Computation_, 4(2):196-210, 03 1992. ISSN 0899-7667. doi: 10.1162/neco.1992.4.2.196. URL https://doi.org/10.1162/neco.1992.4.2.196.
* Olshausen and Field [1996] Bruno A. Olshausen and David J. Field. Emergence of simple-cell receptive field properties by learning a sparse code for natural images. _Nature_, 381(6583):607-609, Jun 1996. ISSN 1476-4687. doi: 10.1038/381607a0. URL https://doi.org/10.1038/381607a0.

* Bell and Sejnowski [1997] Anthony J. Bell and Terrence J. Sejnowski. The "independent components" of natural scenes are edge filters. _Vision Research_, 37(23):3327-3338, 1997. ISSN 0042-6989. doi: https://doi.org/10.1016/S0042-6989(97)00121-1. URL https://www.sciencedirect.com/science/article/pii/S0042698997001211.
* Brunel and Nadal [1998] Nicolas Brunel and Jean-Pierre Nadal. Mutual Information, Fisher Information, and Population Coding. _Neural Computation_, 10(7):1731-1757, 10 1998. ISSN 0899-7667. doi: 10.1162/089976698300017115.
* Rao and Ballard [1999] Rajesh P. N. Rao and Dana H. Ballard. Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. _Nature Neuroscience_, 2(1):79-87, Jan 1999. ISSN 1546-1726. doi: 10.1038/4580. URL https://doi.org/10.1038/4580.
* Butts and Goldman [2006] Daniel A Butts and Mark S Goldman. Tuning curves, neuronal variability, and sensory coding. _PLoS Biology_, 4(4), 03 2006. doi: 10.1371/journal.pbio.0040092. URL https://doi.org/10.1371/journal.pbio.0040092.
* Murthy [2011] Venkatesh N. Murthy. Olfactory maps in the brain. _Annual Review of Neuroscience_, 34(1):233-258, 2011. doi: 10.1146/annurev-neuro-061010-113738. URL https://doi.org/10.1146/annurev-neuro-061010-113738.
* Lee et al. [2022] Brian K. Lee, Emily J. Mayhew, Benjamin Sanchez-Lengeling, Jennifer N. Wei, Wesley W. Qian, Kelsie Little, Matthew Andres, Britney B. Nguyen, Theresa Moloy, Jane K. Parker, Richard C. Gerkin, Joel D. Mainland, and Alexander B. Wiltschko. A principal odor map unifies diverse tasks in human olfactory perception. _bioRxiv_, 2022. doi: 10.1101/2022.09.01.504602. URL https://www.biorxiv.org/content/early/2022/12/13/2022.09.01.504602.
* Qian et al. [2023] Wesley W Qian, Jennifer N Wei, Benjamin Sanchez-Lengeling, Brian K Lee, Yunan Luo, Marmix Vlot, Koen Dechering, Jian Peng, Richard C Gerkin, and Alexander B Wiltschko. Metabolic activity organizes olfactory representations. _eLife_, 12:e82502, may 2023. ISSN 2050-084X. doi: 10.7554/eLife.82502. URL https://doi.org/10.7554/eLife.82502.
* Koulakov and Rinberg [2011] Alexei A. Koulakov and Dmitry Rinberg. Sparse incomplete representations: A potential role of olfactory granule cells. _Neuron_, 72(1):124-136, 2011. ISSN 0896-6273. doi: https://doi.org/10.1016/j.neuron.2011.07.031. URL https://www.sciencedirect.com/science/article/pii/S0896627311006891.
* fast inference in olfaction. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 26. Curran Associates, Inc., 2013. URL https://proceedings.neurips.cc/paper_files/paper/2013/file/2bca9d935d219641434683dd9d18a03-Paper.pdf.
* Grabska-Barwinska et al. [2017] Agnieszka Grabska-Barwinska, Simon Barthelme, Jeff Beck, Zachary F. Mainen, Alexandre Pouget, and Peter E. Latham. A probabilistic approach to demixing odors. _Nature Neuroscience_, 20(1):98-106, Jan 2017. ISSN 1546-1726. doi: 10.1038/nn.4444. URL https://doi.org/10.1038/nn.4444.
* Tootoonian and Lengyel [2014] Sina Tootoonian and Mate Lengyel. A dual algorithm for olfactory computation in the locust brain. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 27. Curran Associates, Inc., 2014. URL https://proceedings.neurips.cc/paper_files/paper/2014/file/c8ba76c279269b1c6bc8a07e38e78fa4-Paper.pdf.
* Kepple et al. [2018] Daniel Kepple, Brittany N. Czakoff, Heike S. Demmer, Dennis Eckmeier, Stephen D. Shea, and Alexei A. Koulakov. Computational algorithms and neural circuitry for compressed sensing in the mammalian main olfactory bulb. _bioRxiv_, 2018. doi: 10.1101/339689. URL https://www.biorxiv.org/content/early/2018/06/05/339689.
* Kepple et al. [2019] Daniel R Kepple, Hamza Giaffar, Dmitry Rinberg, and Alexei A Koulakov. Deconstructing odorant identity via primacy in dual networks. _Neural Computation_, 31(4):710-737, 2019. doi: https://doi.org/10.1162/neco_a_01175.

* Qin et al. [2019] Shanshan Qin, Qianyi Li, Chao Tang, and Yuhai Tu. Optimal compressed sensing strategies for an array of nonlinear olfactory receptor neurons with and without spontaneous activity. _Proceedings of the National Academy of Sciences_, 116(41):20286-20295, 2019. doi:10.1073/pnas.1906571116. URL https://www.pnas.org/doi/abs/10.1073/pnas.1906571116.
* Krishnamurthy et al. [2022] Kamesh Krishnamurthy, Ann M Hermundstad, Thierry Mora, Aleksandra M Walczak, and Vijay Balasubramanian. Disorder and the neural representation of complex odors. _Frontiers in Computational Neuroscience_, 16, 2022. doi:10.3389/fncom.2022.917786.
* Baraniuk [2007] Richard G. Baraniuk. Compressive sensing [lecture notes]. _IEEE Signal Processing Magazine_, 24(4):118-121, 2007. doi:10.1109/MSP.2007.4286571.
* Donoho [2006] D.L. Donoho. Compressed sensing. _IEEE Transactions on Information Theory_, 52(4):1289-1306, 2006. doi:10.1109/TIT.2006.871582.
* Candes and Tao [2006] Emmanuel J. Candes and Terence Tao. Near-optimal signal recovery from random projections: Universal encoding strategies? _IEEE Transactions on Information Theory_, 52(12):5406-5425, 2006. doi:10.1109/TIT.2006.885507.
* Candes et al. [2006] Emmanuel J. Candes, Justin K. Romberg, and Terence Tao. Stable signal recovery from incomplete and inaccurate measurements. _Communications on Pure and Applied Mathematics_, 59(8):1207-1223, 2006. doi:https://doi.org/10.1002/cpa.20124. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.20124.
* Raginsky et al. [2010] Maxim Raginsky, Rebecca M. Willett, Zachary T. Harmany, and Roummel F. Marcia. Compressed sensing performance bounds under Poisson noise. _IEEE Transactions on Signal Processing_, 58(8):3990-4002, 2010. doi:10.1109/TSP.2010.2049997.
* Raginsky et al. [2011] Maxim Raginsky, Sina Jafarpour, Zachary T. Harmany, Roummel F. Marcia, Rebecca M. Willett, and Robert Calderbank. Performance bounds for expander-based compressed sensing in Poisson noise. _IEEE Transactions on Signal Processing_, 59(9):4139-4153, 2011. doi:10.1109/TSP.2011.2157913.
* Khan et al. [2010] Adil Ghani Khan, K. Parthasarathy, and Upinder Singh Bhalla. Odor representations in the mammalian olfactory bulb. _Wiley Interdisciplinary Reviews: Systems Biology and Medicine_, 2(5):603-611, 2010. doi:https://doi.org/10.1002/wsbm.85.
* Ma et al. [2012] Limei Ma, Qiang Qiu, Stephen Gradwohl, Aaron Scott, Elden Q. Yu, Richard Alexander, Winfried Wiegraebe, and C. Ron Yu. Distributed representation of chemical features and tunotopic organization of glomeruli in the mouse olfactory bulb. _Proceedings of the National Academy of Sciences_, 109(14):5481-5486, 2012. doi:10.1073/pnas.1117491109.
* Isaacson [2010] Jeffry S Isaacson. Odor representations in mammalian cortical circuits. _Current Opinion in Neurobiology_, 20(3):328-331, 2010. ISSN 0959-4388. doi:https://doi.org/10.1016/j.conb.2010.02.004. URL https://www.sciencedirect.com/science/article/pii/S0959438810000267. Sensory systems.
* Ma et al. [2015] Yi-An Ma, Tianqi Chen, and Emily Fox. A complete recipe for stochastic gradient MCMC. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 28. Curran Associates, Inc., 2015. URL https://proceedings.neurips.cc/paper_files/paper/2015/file/9a4400501febb2a95e79248486a5f6d3-Paper.pdf.
* Masset et al. [2022] Paul Masset, Jacob Zavatone-Veth, J. Patrick Connor, Venkatesh Murthy, and Cengiz Pehlevan. Natural gradient enables fast sampling in spiking neural networks. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 22018-22034. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/8a0fd48510590071e3c129a79b8b8527-Paper-Conference.pdf.
* Amari [1998] Shun-ichi Amari. Natural Gradient Works Efficiently in Learning. _Neural Computation_, 10(2):251-276, 02 1998. ISSN 0899-7667. doi:10.1162/089976698300017746. URL https://doi.org/10.1162/089976698300017746.

* Girolami and Calderhead [2011] Mark Girolami and Ben Calderhead. Riemann manifold Langevin and Hamiltonian Monte Carlo methods. _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_, 73(2):123-214, 2011. doi: 10.1111/j.1467-9868.2010.00765.x.
* Chung and Abbott [2021] SueYeon Chung and L.F. Abbott. Neural population geometry: An approach for understanding biological and artificial neural networks. _Current Opinion in Neurobiology_, 70:137-144, 2021. ISSN 0959-4388. doi: https://doi.org/10.1016/j.conb.2021.10.010. URL https://www.sciencedirect.com/science/article/pii/S0959438821001227. Computational Neuroscience.
* Zhou et al. [2018] Yuansheng Zhou, Brian H. Smith, and Tatyana O. Sharpee. Hyperbolic geometry of the olfactory space. _Science Advances_, 4(8):eaaq1458, 2018. doi: 10.1126/sciadv.aaq1458. URL https://www.science.org/doi/abs/10.1126/sciadv.aaq1458.
* Koulakov et al. [2011] Alexei Koulakov, Brian Kolterman, Armen Enikolopov, and Dmitry Rinberg. In search of the structure of human olfactory space. _Frontiers in Systems Neuroscience_, 5, 2011. ISSN 1662-5137. doi: 10.3389/fnsys.2011.00065. URL https://www.frontiersin.org/articles/10.3389/fnsys.2011.00065.
* Lledo et al. [2005] Pierre-Marie Lledo, Gilles Gheusi, and Jean-Didier Vincent. Information processing in the mammalian olfactory system. _Physiological Reviews_, 85(1):281-317, 2005. doi: 10.1152/physrev.00008.2004.
* Ache and Young [2005] Barry W. Ache and Janet M. Young. Olfaction: Diverse species, conserved principles. _Neuron_, 48(3):417-430, 2005. ISSN 0896-6273. doi: https://doi.org/10.1016/j.neuron.2005.10.022. URL https://www.sciencedirect.com/science/article/pii/S0896627305008949.
* Su et al. [2009] Chih-Ying Su, Karen Menuz, and John R. Carlson. Olfactory perception: Receptors, cells, and circuits. _Cell_, 139(1):45-59, 2009. ISSN 0092-8674. doi: https://doi.org/10.1016/j.cell.2009.09.015. URL https://www.sciencedirect.com/science/article/pii/S0092867409011751.
* Uchida et al. [2014] Naoshige Uchida, Cindy Poo, and Rafi Haddad. Coding and transformations in the olfactory system. _Annual Review of Neuroscience_, 37(1):363-385, 2014. doi: 10.1146/annurev-neuro-071013-013941.
* Nagayama et al. [2014] Shin Nagayama, Ryota Homma, and Fumiaki Imamura. Neuronal organization of olfactory bulb circuits. _Frontiers in Neural Circuits_, 8:98, 2014. doi: 10.3389/fncir.2014.00098.
* Burton [2017] Shawn D. Burton. Inhibitory circuits of the mammalian main olfactory bulb. _Journal of Neurophysiology_, 118(4):2034-2051, 2017. doi: 10.1152/jn.00109.2017.
* Wu et al. [2020] An Wu, Bin Yu, and Takaki Komiyama. Plasticity in olfactory bulb circuits. _Current Opinion in Neurobiology_, 64:17-23, 2020. ISSN 0959-4388. doi: https://doi.org/10.1016/j.conb.2020.01.007. Systems Neuroscience.
* Malnic et al. [1999] Bettina Malnic, Junzo Hirono, Takaaki Sato, and Linda B Buck. Combinatorial receptor codes for odors. _Cell_, 96(5):713-723, 1999. ISSN 0092-8674. doi: https://doi.org/10.1016/S0092-8674(00)80581-4. URL https://www.sciencedirect.com/science/article/pii/S0092867400805814.
* Murphy et al. [2017] Kenneth Murphy, Casey Weaver, and Charles Janeway. _Janeway's Immunobiology_. Garland Science, New York, 9th edition edition, 2017. ISBN 9780815345053.
* Buck and Axel [1991] Linda Buck and Richard Axel. A novel multigene family may encode odorant receptors: a molecular basis for odor recognition. _Cell_, 65(1):175-187, 1991. doi: https://doi.org/10.1016/0092-8674(91)90418-X.
* Niimura and Nei [2007] Yoshihito Niimura and Masatoshi Nei. Extensive gains and losses of olfactory receptor genes in mammalian evolution. _PLOS ONE_, 2(8):1-8, 08 2007. doi: 10.1371/journal.pone.0000708. URL https://doi.org/10.1371/journal.pone.0000708.

* Niimura et al. [2014] Yoshihito Niimura, Atsushi Matsui, and Kazushige Touhara. Extreme expansion of the olfactory receptor gene repertoire in african elephants and evolutionary dynamics of orthologous gene groups in 13 placental mammals. _Genome Research_, 24(9):1485-1496, 2014. doi: 10.1101/gr.169532.113.
* Tesileanu et al. [2019] Tiberiu Tesileanu, Simona Cocco, Remi Monasson, and Vijay Balasubramanian. Adaptation of olfactory receptor abundances for efficient coding. _eLife_, 8:e39279, feb 2019. ISSN 2050-084X. doi: 10.7554/eLife.39279. URL https://doi.org/10.7554/eLife.39279.
* Egger and Urban [2006] Veronica Egger and Nathaniel N. Urban. Dynamic connectivity in the mitral cell-granule cell microcircuit. _Seminars in Cell & Developmental Biology_, 17(4):424-432, 2006. ISSN 1084-9521. doi: https://doi.org/10.1016/j.semcdb.2006.04.006. URL https://www.sciencedirect.com/science/article/pii/S1084952106000498. Olfaction Animal Stem Cell Types.
* Kepecs et al. [2005] Adam Kepecs, Naoshige Uchida, and Zachary F. Mainen. The Sniff as a Unit of Olfactory Processing. _Chemical Senses_, 31(2):167-179, 12 2005. ISSN 0379-864X. doi: 10.1093/chemse/bjj016. URL https://doi.org/10.1093/chemse/bjj016.
* Ackels et al. [2021] Tobias Ackels, Andrew Erskine, Debanjan Dasgupta, Alina Cristina Marin, Tom P. A. Warner, Sina Tootoonian, Izumi Fukunaga, Julia J. Harris, and Andreas T. Schaefer. Fast odour dynamics are encoded in the olfactory system and guide behaviour. _Nature_, 593(7860):558-563, May 2021. ISSN 1476-4687. doi: 10.1038/s41586-021-03514-2. URL https://doi.org/10.1038/s41586-021-03514-2.
* Uchida and Mainen [2003] Naoshige Uchida and Zachary F. Mainen. Speed and accuracy of olfactory discrimination in the rat. _Nature Neuroscience_, 6(11):1224-1229, Nov 2003. ISSN 1546-1726. doi: 10.1038/nn1142. URL https://doi.org/10.1038/nn1142.
* Spors et al. [2012] Hartwig Spors, Dinu Florin Albeanu, Venkatesh N. Murthy, Dmitry Rinberg, Naoshige Uchida, Matt Wachowiak, and Rainer W. Friedrich. Illuminating vertebrate olfactory processing. _Journal of Neuroscience_, 32(41):14102-14108a, 2012. ISSN 0270-6474. doi: 10.1523/JNEUROSCI.3328-12.2012.
* Reinert and Fukunaga [2022] Janine K. Reinert and Izumi Fukunaga. The facets of olfactory learning. _Current Opinion in Neurobiology_, 76:102623, 2022. ISSN 0959-4388. doi: https://doi.org/10.1016/j.conb.2022.102623. URL https://www.sciencedirect.com/science/article/pii/S0959438822001179.
* Rokni et al. [2014] Dan Rokni, Vivian Hemmelder, Vikrant Kapoor, and Venkatesh N. Murthy. An olfactory cocktail party: figure-ground segregation of odorants in rodents. _Nature Neuroscience_, 17(9):1225-1232, Sep 2014. ISSN 1546-1726. doi: 10.1038/nn.3775. URL https://doi.org/10.1038/nn.3775.
* Li et al. [2023] Yan Li, Mitchell Swerdloff, Tianyu She, Asiyah Rahman, Naveen Sharma, Reema Shah, Michael Castellano, Daniel Mogel, Jason Wu, Asim Ahmed, James San Miguel, Jared Cohn, Nikesh Shah, Raddy L. Ramos, and Gonzalo H. Otazu. Robust odor identification in novel olfactory environments in mice. _Nature Communications_, 14(1):673, Feb 2023. ISSN 2041-1723. doi: 10.1038/s41467-023-36346-x. URL https://doi.org/10.1038/s41467-023-36346-x.
* Berners-Lee et al. [2023] Alice Berners-Lee, Elizabeth Shtrahman, Julien Grimaud, and Venkatesh N. Murthy. Experience-dependent evolution of odor mixture representations in piriform cortex. _PLOS Biology_, 21(4):1-29, 04 2023. URL https://doi.org/10.1371/journal.pbio.3002086.
* Lebovich et al. [2021] Lior Lebovich, Michael Yunerman, Viviana Scaiewicz, Yonatan Loewenstein, and Dan Rokni. Paradoxical relationship between speed and accuracy in olfactory figure-background segregation. _PLOS Computational Biology_, 17:1-16, 12 2021. doi: 10.1371/journal.pcbi.1009674. URL https://doi.org/10.1371/journal.pcbi.1009674.
* Laing and Francis [1989] David G. Laing and G.W. Francis. The capacity of humans to identify odors in mixtures. _Physiology & Behavior_, 46(5):809-814, 1989. ISSN 0031-9384. doi: https://doi.org/10.1016/0031-9384(89)90041-3. URL https://www.sciencedirect.com/science/article/pii/0031938489900413.

* Jinks and Laing [2001] A Jinks and David George Laing. The analysis of odor mixtures by humans: evidence for a configurational process. _Physiology & Behavior_, 72(1):51-63, 2001. ISSN 0031-9384. doi: https://doi.org/10.1016/S0031-9384(00)00407-8. URL https://www.sciencedirect.com/science/article/pii/S0031938400004078.
* Igarashi et al. [2012] Kei M. Igarashi, Nao Ieki, Myungho An, Yukie Yamaguchi, Shin Nagayama, Ko Kobayakawa, Reiko Kobayashi, Manabu Tanifuji, Hitoshi Sakano, Wei R. Chen, and Kensaku Mori. Parallel mitral and tufted cell pathways route distinct odor information to different targets in the olfactory cortex. _Journal of Neuroscience_, 32(23):7970-7985, 2012. ISSN 0270-6474. doi: 10.1523/JNEUROSCI.0154-12.2012.
* Chae et al. [2022] Honggoo Chae, Arkarup Banerjee, Marie Dussauze, and Dinu F. Albeanu. Long-range functional loops in the mouse olfactory system and their roles in computing odor identity. _Neuron_, 110(23):3970-3985.e7, 2022. ISSN 0896-6273. doi: https://doi.org/10.1016/j.neuron.2022.09.005.
* Gupta et al. [2015] Priyanka Gupta, Dinu F Albeanu, and Upinder S Bhalla. Olfactory bulb coding of odors, mixtures and sniffs is a linear sum of odor time profiles. _Nature Neuroscience_, 18(2):272-281, 2015. doi: https://doi.org/10.1038/nn.3913.
* Zhang and Sharpee [2016] Yilun Zhang and Tatyana O. Sharpee. A robust feedforward model of the olfactory system. _PLOS Computational Biology_, 12(4):1-15, 04 2016. doi: 10.1371/journal.pcbi.1004850. URL https://doi.org/10.1371/journal.pcbi.1004850.
* Zwicker et al. [2016] David Zwicker, Arvind Murugan, and Michael P. Brenner. Receptor arrays optimized for natural odor statistics. _Proceedings of the National Academy of Sciences_, 113(20):5570-5575, 2016. doi: 10.1073/pnas.1600357113. URL https://www.pnas.org/doi/abs/10.1073/pnas.1600357113.
* Singh et al. [2021] Vijay Singh, Martin Tchernookov, and Vijay Balasubramanian. What the odor is not: Estimation by elimination. _Phys. Rev. E_, 104:024415, Aug 2021. doi: 10.1103/PhysRevE.104.024415. URL https://link.aps.org/doi/10.1103/PhysRevE.104.024415.
* Duchamp-Viret et al. [2005] Patricia Duchamp-Viret, Lubomir Kostal, Michel Chaput, Petr Lansky, and Jean-Pierre Rospars. Patterns of spontaneous activity in single rat olfactory receptor neurons are different in normally breathing and tracheotomized animals. _Journal of Neuobiology_, 65(2):97-114, 2005. doi: https://doi.org/10.1002/neu.20177. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/neu.20177.
* Kostal et al. [2007] Lubomir Kostal, Petr Lansky, and Jean-Pierre Rospars. Neuronal coding and spiking randomness. _European Journal of Neuroscience_, 26(10):2693-2701, 2007. doi: https://doi.org/10.1111/j.1460-9568.2007.05880.x. URL https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1460-9568.2007.05880.x.
* Dytso et al. [2020] Alex Dytso, Michael Fauss, and H. Vincent Poor. The vector Poisson channel: On the linearity of the conditional mean estimator. _IEEE Transactions on Signal Processing_, 68:5894-5903, 2020. doi: 10.1109/TSP.2020.3025525.
* Jin et al. [2012] An Jin, Birsen Yazici, Angelique Ale, and Vasilis Ntziachristos. Preconditioning of the fluorescence diffuse optical tomography sensing matrix based on compressive sensing. _Optics Letters_, 37(20):4326-4328, Oct 2012. doi: 10.1364/OL.37.004326. URL https://opg.optica.org/ol/abstract.cfm?URI=ol-37-20-4326.
* Martens [2020] James Martens. New insights and perspectives on the natural gradient method. _Journal of Machine Learning Research_, 21(146):1-76, 2020. URL http://jmlr.org/papers/v21/17-678.html.
* Holt and Koch [1997] Gary R. Holt and Christof Koch. Shunting inhibition does not have a divisive effect on firing rates. _Neural Computation_, 9(5):1001-1013, 07 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.5.1001. URL https://doi.org/10.1162/neco.1997.9.5.1001.

* Chalk et al. [2017] Matthew Chalk, Paul Masset, Sophie Deneve, and Boris Gutkin. Sensory noise predicts divisive reshaping of receptive fields. _PLOS Computational Biology_, 13(6):1-26, 06 2017. doi: 10.1371/journal.pcbi.1005582. URL https://doi.org/10.1371/journal.pcbi.1005582.
* Marr and Poggio [1976] David Marr and Tomaso Poggio. From understanding computation to understanding neural circuitry. Technical report, Massachussetts Institute of Technology AI Laboratory, 1976. URL https://dspace.mit.edu/handle/1721.1/5782.
* Zavatone-Veth et al. [2020] Jacob A. Zavatone-Veth, Bara A. Badwan, and Damon A. Clark. A minimal synaptic model for direction selective neurons in _Drosophila_. _Journal of Vision_, 20(2):2-2, 02 2020. ISSN 1534-7362. doi: 10.1167/jov.20.2.2. URL https://doi.org/10.1167/jov.20.2.2.
* Crick [1989] Francis Crick. The recent excitement about neural networks. _Nature_, 337(6203):129-132, Jan 1989. ISSN 1476-4687. doi: 10.1038/337129a0. URL https://doi.org/10.1038/337129a0.
* Guerguiev et al. [2017] Jordan Guerguiev, Timothy P Lillicrap, and Blake A Richards. Towards deep learning with segregated dendrites. _eLife_, 6:e22901, dec 2017. ISSN 2050-084X. doi: 10.7554/eLife.22901.
* Lillicrap et al. [2020] Timothy P. Lillicrap, Adam Santoro, Luke Marris, Colin J. Akerman, and Geoffrey Hinton. Backpropagation and the brain. _Nature Reviews Neuroscience_, 21(6):335-346, Jun 2020. ISSN 1471-0048. doi: 10.1038/s41583-020-0277-3. URL https://doi.org/10.1038/s41583-020-0277-3.
* Urban and Arevian [2009] Nathaniel N. Urban and Armen C. Arevian. Computing with dendrohedritic synapses in the olfactory bulb. _Annals of the New York Academy of Sciences_, 1170(1):264-269, 2009. doi: https://doi.org/10.1111/j.1749-6632.2009.03899.x. URL https://nyaspubs.onlinelibrary.wiley.com/doi/abs/10.1111/j.1749-6632.2009.03899.x.
* Davis and Macrides [1981] Barry J. Davis and Foteos Macrides. The organization of centrifugal projections from the anterior olfactory nucleus, ventral hippocampal rudiment, and piriform cortex to the main olfactory bulb in the hamster: An autoradiographic study. _Journal of Comparative Neurology_, 203(3):475-493, 1981. doi: https://doi.org/10.1002/cne.902030310. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/cne.902030310.
* Boyd et al. [2012] Alison M. Boyd, James F. Sturgill, Cindy Poo, and Jeffry S. Isaacson. Cortical feedback control of olfactory bulb circuits. _Neuron_, 76(6):1161-1174, 2012. ISSN 0896-6273. doi: https://doi.org/10.1016/j.neuron.2012.10.020. URL https://www.sciencedirect.com/science/article/pii/S0896627312009403.
* Markopoulos et al. [2012] Foivos Markopoulos, Dan Rokni, David H. Gire, and Venkatesh N. Murthy. Functional properties of cortical feedback projections to the olfactory bulb. _Neuron_, 76(6):1175-1188, 2012. ISSN 0896-6273. doi: https://doi.org/10.1016/j.neuron.2012.10.028. URL https://www.sciencedirect.com/science/article/pii/S0896627312009488.
* Burton and Urban [2014] Shawn D. Burton and Nathaniel N. Urban. Greater excitability and firing irregularity of tufted cells underlies distinct afferent-evoked activity of olfactory bulb mitral and tufted cells. _The Journal of Physiology_, 592(10):2097-2118, 2014. doi: https://doi.org/10.1113/jphysiol.2013.269886. URL https://physoc.onlinelibrary.wiley.com/doi/abs/10.1113/jphysiol.2013.269886.
* Burton and Urban [2015] Shawn D. Burton and Nathaniel N. Urban. Rapid feedforward inhibition and asynchronous excitation regulate granule cell activity in the mammalian main olfactory bulb. _Journal of Neuroscience_, 35(42):14103-14122, 2015. ISSN 0270-6474. doi: 10.1523/JNEUROSCI.0746-15.2015. URL https://www.jneurosci.org/content/35/42/14103.
* dal Marmol et al. [2021] Josefina dal Marmol, Mackenzie A. Yedlin, and Vanessa Ruta. The structural basis of odorant recognition in insect olfactory receptors. _Nature_, 597(7874):126-131, Sep 2021. ISSN 1476-4687. doi: 10.1038/s41586-021-03794-8. URL https://doi.org/10.1038/s41586-021-03794-8.

* Zak et al. [2020] Joseph D. Zak, Gautam Reddy, Massimo Vergassola, and Venkatesh N. Murthy. Antagonistic odor interactions in olfactory sensory neurons are widespread in freely breathing mice. _Nature Communications_, 11(1):3350, Jul 2020. ISSN 2041-1723. doi: 10.1038/s41467-020-17124-5. URL https://doi.org/10.1038/s41467-020-17124-5.
* Arevian et al. [2008] Armen C. Arevian, Vikrant Kapoor, and Nathaniel N. Urban. Activity-dependent gating of lateral inhibition in the mouse olfactory bulb. _Nature Neuroscience_, 11(1):80-87, Jan 2008. ISSN 1546-1726. doi: 10.1038/nn2030. URL https://doi.org/10.1038/nn2030.
* Cury and Uchida [2010] Kevin M. Cury and Naoshige Uchida. Robust odor coding via inhalation-coupled transient activity in the mammalian olfactory bulb. _Neuron_, 68(3):570-585, 2010. ISSN 0896-6273. doi: https://doi.org/10.1016/j.neuron.2010.09.040. URL https://www.sciencedirect.com/science/article/pii/S0896627310007828.
* Shusterman et al. [2011] Roman Shusterman, Matthew C. Smear, Alexei A. Koulakov, and Dmitry Rinberg. Precise olfactory responses tile the sniff cycle. _Nature Neuroscience_, 14(8):1039-1044, Aug 2011. ISSN 1546-1726. doi: 10.1038/nn.2877. URL https://doi.org/10.1038/nn.2877.
* Donoho and Tanner [2009] David Donoho and Jared Tanner. Observed universality of phase transitions in high-dimensional geometry, with implications for modern data analysis and signal processing. _Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences_, 367(1906):4273-4293, 2009. doi: 10.1098/rsta.2009.0152. URL https://royalsocietypublishing.org/doi/abs/10.1098/rsta.2009.0152.
* Maleki and Donoho [2010] Arian Maleki and David L. Donoho. Optimally tuned iterative reconstruction algorithms for compressed sensing. _IEEE Journal of Selected Topics in Signal Processing_, 4(2):330-341, 2010. doi: 10.1109/JSTSP.2009.2039176.
* Gerkin and Castro [2015] Richard C Gerkin and Jason B Castro. The number of olfactory stimuli that humans can discriminate is still unknown. _eLife_, 4:e08127, jul 2015. ISSN 2050-084X. doi: 10.7554/eLife.08127. URL https://doi.org/10.7554/eLife.08127.
* Meister [2015] Markus Meister. On the dimensionality of odor space. _eLife_, 4:e07865, jul 2015. ISSN 2050-084X. doi: 10.7554/eLife.07865. URL https://doi.org/10.7554/eLife.07865.
* Mayhew et al. [2022] Emily J. Mayhew, Charles J. Arayata, Richard C. Gerkin, Brian K. Lee, Jonathan M. Magill, Lindsey L. Snyder, Kelsie A. Little, Chung Wen Yu, and Joel D. Mainland. Transport features predict if a molecule is odorous. _Proceedings of the National Academy of Sciences_, 119(15):e2116576119, 2022. doi: 10.1073/pnas.2116576119. URL https://www.pnas.org/doi/abs/10.1073/pnas.2116576119.
* Knill and Pouget [2004] David C Knill and Alexandre Pouget. The Bayesian brain: the role of uncertainty in neural coding and computation. _Trends in Neurosciences_, 27(12):712-719, 2004. doi: 10.1016/j.tins.2004.10.007.
* Kording and Wolpert [2004] Konrad P Kording and Daniel M Wolpert. Bayesian integration in sensorimotor learning. _Nature_, 427(6971):244-247, 2004. doi: 10.1038/nature02169.
* Fiser et al. [2010] Jozsef Fiser, Pietro Berkes, Gergo Orban, and Mate Lengyel. Statistically optimal perception and learning: from behavior to neural representations. _Trends in Cognitive Sciences_, 14(3):119-130, 2010. doi: 10.1016/j.tics.2010.01.003.
* Ott et al. [2018] Torben Ott, Paul Masset, and Adam Kepecs. The neurobiology of confidence: From beliefs to neurons. In _Cold Spring Harbor Symposia on Quantitative Biology_, volume 83, pages 9-16. Cold Spring Harbor Laboratory Press, 2018. doi: 10.1101/sqb.2018.83.038794.
* Reddy et al. [2022] Gautam Reddy, Venkatesh N. Murthy, and Massimo Vergassola. Olfactory sensing and navigation in turbulent environments. _Annual Review of Condensed Matter Physics_, 13(1):191-213, 2022. doi: 10.1146/annurev-conmatphys-031720-032754. URL https://doi.org/10.1146/annurev-conmatphys-031720-032754.

* Calvin and Stevens [1967] William H. Calvin and Charles F. Stevens. Synaptic noise as a source of variability in the interval between action potentials. _Science_, 155(3764):842-844, 1967. doi: 10.1126/science.155.3764.842. URL https://www.science.org/doi/abs/10.1126/science.155.3764.842.
* Neal [1993] Radford M Neal. _Probabilistic inference using Markov chain Monte Carlo methods_. Department of Computer Science, University of Toronto Toronto, ON, Canada, 1993. URL https://www.cs.toronto.edu/~radford/review.abstract.html.
* Zhu and Rozell [2015] Mengchen Zhu and Christopher J. Rozell. Modeling inhibitory interneurons in efficient sensory coding models. _PLoS Computational Biology_, 11(7):1-22, 07 2015. doi: 10.1371/journal.pcbi.1004353. URL https://doi.org/10.1371/journal.pcbi.1004353.
* Reddy et al. [2018] Gautam Reddy, Joseph D Zak, Massimo Vergassola, and Venkatesh N Murthy. Antagonism in olfactory receptor neurons and its implications for the perception of odor mixtures. _eLife_, 7:e34958, apr 2018. ISSN 2050-084X. doi: 10.7554/eLife.34958. URL https://doi.org/10.7554/eLife.34958.
* Gorur-Shandilya et al. [2017] Srinivas Gorur-Shandilya, Mahmut Demir, Junijia Long, Damon A Clark, and Thierry Emonet. Olfactory receptor neurons use gain control and complementary kinetics to encode intermittent odorant stimuli. _eLife_, 6:e27670, jun 2017. ISSN 2050-084X. doi: 10.7554/eLife.27670. URL https://doi.org/10.7554/eLife.27670.
* Kadakia and Emonet [2019] Nirag Kadakia and Thierry Emonet. Front-end Weber-Fechner gain control enhances the fidelity of combinatorial odor coding. _eLife_, 8:e45293, jun 2019. ISSN 2050-084X. doi: 10.7554/eLife.45293. URL https://doi.org/10.7554/eLife.45293.
* Savin and Deneve [2014] Cristina Savin and Sophie Deneve. Spatio-temporal representations of uncertainty in spiking neural networks. _Advances in Neural Information Processing Systems_, 27, 2014. URL https://proceedings.neurips.cc/paper/2014/hash/4e2545f819e67f0615003dd7e04a6087-Abstract.html.
* Fang et al. [2022] Michael Y.-S. Fang, Mayur Mudigonda, Ryan Zarcone, Amir Khosrowshahi, and Bruno A. Olshausen. Learning and Inference in Sparse Coding Models With Langevin Dynamics. _Neural Computation_, 34(8):1676-1700, 07 2022. ISSN 0899-7667. doi: 10.1162/neco_a_01505. URL https://doi.org/10.1162/neco_a_01505.
* Tsukahara et al. [2021] Tatsuya Tsukahara, David H. Brann, Stan L. Pashkovski, Grigori Guitchounts, Thomas Bozza, and Sandeep Robert Datta. A transcriptional rheostat couples past activity to future sensory responses. _Cell_, 184(26):6326-6343.e32, 2021. ISSN 0092-8674. doi: https://doi.org/10.1016/j.cell.2021.11.022. URL https://www.sciencedirect.com/science/article/pii/S0092867421013374.
* Hiratani and Latham [2020] Naoki Hiratani and Peter E. Latham. Rapid Bayesian learning in the mammalian olfactory system. _Nature Communications_, 11(1):3845, Jul 2020. ISSN 2041-1723. doi: 10.1038/s41467-020-17490-0. URL https://doi.org/10.1038/s41467-020-17490-0.
* Chapochnikov et al. [2023] Nikolai M. Chapochnikov, Cengiz Pehlevan, and Dmitri B. Chklovskii. Normative and mechanistic model of an adaptive circuit for efficient encoding and feature extraction. _Proceedings of the National Academy of Sciences_, 120(29):e2117484120, 2023. doi: 10.1073/pnas.2117484120. URL https://www.pnas.org/doi/abs/10.1073/pnas.2117484120.
* Pehlevan et al. [2017] Cengiz Pehlevan, Alexander Genkin, and Dmitri B. Chklovskii. A clustering neural network model of insect olfaction. In _51st Asilomar Conference on Signals, Systems, and Computers_, pages 593-600, 2017. doi: 10.1109/ACSSC.2017.8335410.
* Masset et al. [2022] Paul Masset, Shanshan Qin, and Jacob A Zavatone-Veth. Drifting neuronal representations: Bug or feature? _Biological Cybernetics_, pages 1-14, 2022. doi:doi.org/10.1007/s00422-021-00916-3.

* Chong et al. [2020] Edmund Chong, Monica Moroni, Christopher Wilson, Shy Shoham, Stefano Panzeri, and Dmitry Rinberg. Manipulating synthetic optogenetic odors reveals the coding logic of olfactory perception. _Science_, 368(6497):eaaba2357, 2020. doi: 10.1126/science.aba2357.
* Nakayama et al. [2022] Hirofumi Nakayama, Richard C. Gerkin, and Dmitry Rinberg. A behavioral paradigm for measuring perceptual distances in mice. _Cell Reports Methods_, 2(6):100233, 2022. ISSN 2667-2375. doi: https://doi.org/10.1016/j.crmeth.2022.100233. URL https://www.sciencedirect.com/science/article/pii/S2667237522001023.
* Otazu et al. [2015] Gonzalo H Otazu, Honggoo Chae, Martin B Davis, and Dinu F Albeanu. Cortical feedback decorrelates olfactory bulb output in awake mice. _Neuron_, 86(6):1461-1477, 2015. doi: https://doi.org/10.1016/j.neuron.2015.05.023.
* Wu et al. [2020] An Wu, Bin Yu, Qiyu Chen, Gillian A. Matthews, Chen Lu, Evan Campbell, Kay M. Tye, and Takaki Komiyama. Context-dependent plasticity of adult-born neurons regulated by cortical feedback. _Science Advances_, 6(42):eabc8319, 2020. doi: 10.1126/sciadv.abc8319. URL https://www.science.org/doi/abs/10.1126/sciadv.abc8319.
* Whittington et al. [2023] James C. R. Whittington, Will Dorrell, Surya Ganguli, and Timothy Behrens. Disentanglement with biological constraints: A theory of functional cell types. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=92_GfhZnGH.
* Plumbley [2002] Mark Plumbley. Conditions for nonnegative independent component analysis. _IEEE Signal Processing Letters_, 9(6):177-180, 2002. doi: 10.1109/LSP.2002.800502.
* Pehlevan and Chklovskii [2015] Cengiz Pehlevan and Dmitri B Chklovskii. Optimization theory of hebbian/anti-hebbian networks for pca and whitening. In _2015 53rd Annual Allerton Conference on Communication, Control, and Computing (Allerton)_, pages 1458-1465. IEEE, 2015.
* Pehlevan et al. [2017] Cengiz Pehlevan, Sreyas Mohan, and Dmitri B. Chklovskii. Blind Nonnegative Source Separation Using Biological Neural Networks. _Neural Computation_, 29(11):2925-2954, 11 2017. ISSN 0899-7667. doi: 10.1162/neco_a_01007. URL https://doi.org/10.1162/neco_a_01007.
* Lipshutz et al. [2022] David Lipshutz, Cengiz Pehlevan, and Dmitri B. Chklovskii. Biologically plausible single-layer networks for nonnegative independent component analysis. _Biological Cybernetics_, 116(5):557-568, Dec 2022. ISSN 1432-0770. doi: 10.1007/s00422-022-00943-8. URL https://doi.org/10.1007/s00422-022-00943-8.
* Higgins et al. [2021] Irina Higgins, Le Chang, Victoria Langston, Demis Hassabis, Christopher Summerfield, Doris Tsao, and Matthew Botvinick. Unsupervised deep learning identifies semantic disentanglement in single inferotemporal face patch neurons. _Nature Communications_, 12(1):6456, Nov 2021. ISSN 2041-1723. doi: 10.1038/s41467-021-26751-5. URL https://doi.org/10.1038/s41467-021-26751-5.
* Chang and Tsao [2017] Le Chang and Doris Y. Tsao. The code for facial identity in the primate brain. _Cell_, 169(6):1013-1028.e14, 2017. ISSN 0092-8674. doi: https://doi.org/10.1016/j.cell.2017.05.011. URL https://www.sciencedirect.com/science/article/pii/S009286741730538X.
* Bao et al. [2020] Pinglei Bao, Liang She, Mason McGill, and Doris Y. Tsao. A map of object space in primate inferotemporal cortex. _Nature_, 583(7814):103-108, Jul 2020. ISSN 1476-4687. doi: 10.1038/s41586-020-2350-5. URL https://doi.org/10.1038/s41586-020-2350-5.
* Rigotti et al. [2013] Mattia Rigotti, Omri Barak, Melissa R. Warden, Xiao-Jing Wang, Nathaniel D. Daw, Earl K. Miller, and Stefano Fusi. The importance of mixed selectivity in complex cognitive tasks. _Nature_, 497(7451):585-590, May 2013. ISSN 1476-4687. doi: 10.1038/nature12160. URL https://doi.org/10.1038/nature12160.
* Hirokawa et al. [2019] Junya Hirokawa, Alexander Vaughan, Paul Masset, Torben Ott, and Adam Kepecs. Frontal cortex neuron types categorically encode single decision variables. _Nature_, 576(7787):446-451, 2019. doi: https://doi.org/10.1038/s41586-019-1816-9.

* Dubreuil et al. [2022] Alexis Dubreuil, Adrian Valente, Manuel Beiran, Francesca Mastrogiuseppe, and Srdjan Ostojic. The role of population structure in computations through neural dynamics. _Nature Neuroscience_, 25(6):783-794, 2022. doi:https://doi.org/10.1038/s41593-022-01088-4.
* Bronstein et al. [2021] Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Velickovic. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. _arXiv preprint arXiv:2104.13478_, 2021. URL https://geometricdeeplearning.com/.
* Zavatone-Veth et al. [2023] Jacob A. Zavatone-Veth, Sheng Yang, Julian A. Rubinfien, and Cengiz Pehlevan. Neural networks learn to magnify areas near decision boundaries. _arXiv_, 2023. doi:https://doi.org/10.48550/arXiv.2301.11375.
* Pnevmatikakis and Giovannucci [2017] Eftychios A. Pnevmatikakis and Andrea Giovannucci. Normcorre: An online algorithm for piecewise rigid motion correction of calcium imaging data. _Journal of Neuroscience Methods_, 291:83-94, 2017. ISSN 0165-0270. doi:https://doi.org/10.1016/j.jneumeth.2017.07.031.
* Mathis et al. [2016] Alexander Mathis, Dan Rokni, Vikrant Kapoor, Matthias Bethge, and Venkatesh N. Murthy. Reading out olfactory receptors: Feedforward circuits detect odors in mixtures without demixing. _Neuron_, 91(5):1110-1123, 2016. ISSN 0896-6273. doi:https://doi.org/10.1016/j.neuron.2016.08.007. URL https://www.sciencedirect.com/science/article/pii/S0896627316304998.

Notational conventions

In this Appendix, we define the notational conventions used throughout the paper. We write \(\mathbb{R}^{n}_{+}=\{\mathbf{x}\in\mathbb{R}^{n}:x_{1},\ldots,x_{n}\geq 0\}\) for the non-negative reals, and \(\mathbb{N}=\{0,1,\ldots\}\) for the natural numbers. For a vector \(\boldsymbol{\lambda}\in\mathbb{R}^{n}_{+}\), we write

\[\mathbf{x}\sim\mathrm{Poisson}(\boldsymbol{\lambda})\] (A.1)

if the probability mass function of \(\mathbf{x}\in\mathbb{N}^{n}\) is

\[p(\mathbf{x})=\prod_{j=1}^{n}\frac{\lambda_{j}^{x_{j}}}{x_{j}!}e^{-\lambda_{j }}.\] (A.2)

For scalars \(\alpha>0\) and \(\lambda>0\), we write that a vector \(\mathbf{x}\in\mathbb{R}^{n}_{+}\) is distributed as

\[\mathbf{x}\sim\mathrm{Gamma}(\alpha,\lambda)\] (A.3)

if its probability density function is

\[p(\mathbf{x})=\prod_{j=1}^{n}\frac{x_{j}^{\alpha-1}}{\Gamma(\alpha)}e^{- \lambda_{j}x_{j}}.\] (A.4)

Similarly, for vectors \(\boldsymbol{\alpha},\boldsymbol{\lambda}\in\mathbb{R}^{n}_{+}\), we write

\[\mathbf{x}\sim\mathrm{Gamma}(\boldsymbol{\alpha},\boldsymbol{\lambda})\] (A.5)

if

\[p(\mathbf{x})=\prod_{j=1}^{n}\frac{x_{j}^{\alpha_{j}-1}}{\Gamma(\alpha_{j})}e^ {-\lambda_{j}x_{j}}.\] (A.6)

For vectors \(\mathbf{x},\mathbf{y}\in\mathbb{R}^{n}_{+}\), we write \(\mathbf{x}\odot\mathbf{y}\) for their Hadamard (elementwise) product

\[(\mathbf{x}\odot\mathbf{y})_{i}=x_{i}y_{i}\] (A.7)

and \(\mathbf{x}\oslash\mathbf{y}\) for their elementwise ratio

\[(\mathbf{x}\oslash\mathbf{y})_{i}=\frac{x_{i}}{y_{i}}.\] (A.8)

## Appendix B Detailed derivation of circuit model

In this Appendix, we give a detailed derivation of the circuit models introduced in Sections 3 and 6 of the main text. Here, we focus on the setting of the sampling circuit; the circuit algorithm to compute the MAP introduced in Section 3 of the main text can be recovered at each step by dropping the additive noise terms from the dynamics.

We recall that our goal is to sample the posterior \(p(\mathbf{c}\,|\,\mathbf{s})\) over concentrations \(\mathbf{c}\) given OSN activity \(\mathbf{s}\), for a Poisson likelihood

\[\mathbf{s}\,|\,\mathbf{c}\sim\mathrm{Poisson}(\mathbf{r}_{0}+\mathbf{A} \mathbf{c})\] (B.1)

and a Gamma prior

\[\mathbf{c}\sim\mathrm{Gamma}(\boldsymbol{\alpha},\boldsymbol{\lambda}).\] (B.2)

Here, we use the conventions of Appendix A for Poisson and Gamma random vectors. Discarding normalization constants that do not depend on \(\mathbf{c}\), the density of the posterior is then

\[p(\mathbf{c}\,|\,\mathbf{s}) \propto p(\mathbf{s}\,|\,\mathbf{c})p(\mathbf{c})\] (B.3) \[\propto\left[\prod_{k=1}^{n_{\mathrm{OSN}}}(\mathbf{r}_{0}+ \mathbf{A}\mathbf{c})_{k}^{s_{k}}e^{-(\mathbf{r}_{0}+\mathbf{A}\mathbf{c})_{k }}\right]\left[\prod_{j=1}^{n_{\mathrm{odor}}}c_{j}^{\alpha_{j}-1}e^{-\lambda_ {j}c_{j}}\right].\] (B.4)We will sample from this distribution using Langevin dynamics, without explicitly constraining the concentration estimate to be non-negative [21, 22]. This corresponds to the stochastic dynamics

\[\dot{\mathbf{c}}(t)=\boldsymbol{\nabla}_{\mathbf{c}}\log p(\mathbf{c}\,|\, \mathbf{s})+\boldsymbol{\eta}(t),\] (B.5)

where \(\boldsymbol{\eta}(t)\) is an \(n_{\mathrm{odor}}\)-dimensional zero-mean Gaussian noise process with \(\mathbf{E}[\eta_{j}(t)\eta_{j^{\prime}}(t^{\prime})]=2\delta_{jj^{\prime}} \delta(t-t^{\prime})\). The gradient of the log-posterior is

\[\frac{\partial\log p(\mathbf{c}\,|\,\mathbf{s})}{\partial c_{j}}=\sum_{k=1}^{ n_{\mathrm{OSN}}}\left(\frac{s_{k}}{(\mathbf{r}_{0}+\mathbf{A}\mathbf{c})_{k}} -1\right)A_{kj}+\frac{\alpha_{j}-1}{c_{j}}-\lambda_{j},\] (B.6)

or, in vector notation,

\[\boldsymbol{\nabla}_{\mathbf{c}}\log p(\mathbf{c}\,|\,\mathbf{s})=\mathbf{A}^ {\top}[\mathbf{s}\oslash(\mathbf{r}_{0}+\mathbf{A}\mathbf{c})-\mathbf{1}]+( \boldsymbol{\alpha}-\mathbf{1})\oslash\mathbf{c}-\boldsymbol{\lambda}.\] (B.7)

The Langevin dynamics then become

\[\dot{\mathbf{c}}(t)=\mathbf{A}^{\top}[\mathbf{s}\oslash(\mathbf{r}_{0}+\mathbf{ A}\mathbf{c})-\mathbf{1}]+(\boldsymbol{\alpha}-\mathbf{1})\oslash\mathbf{c}- \boldsymbol{\lambda}+\boldsymbol{\eta}(t).\] (B.8)

This is closely related to the Langevin sampling algorithm introduced in equation (3.9) of Grabska-Barwinska et al. [21], but here we are not separately inferring odor concentration and odor presence. We note that, for \(\alpha_{j}>1\), the term \((\boldsymbol{\alpha}-\mathbf{1})\oslash\mathbf{c}\) gives a force that diverges for small \(c_{j}\).

However, in this setup single neurons encode single odors. We instead want to allow for a distributed code, where the population of neurons responsible for sampling may not directly encode single odors. To distribute the code, we follow previous work by Masset et al. [38] in using the "complete recipe" for stochastic gradient MCMC [37]. From the "complete recipe", we know that the dynamics

\[\tau_{\mathrm{g}}\dot{\mathbf{c}}(t)=\boldsymbol{\Gamma}\boldsymbol{\Gamma}^{ \top}\boldsymbol{\nabla}_{\mathbf{c}}\log p(\mathbf{c}\,|\,\mathbf{s})+ \boldsymbol{\Gamma}\boldsymbol{\xi}(t),\] (B.9)

for a (expectantly named) time constant \(\tau_{\mathrm{g}}>0\) and any (potentially non-square) matrix \(\boldsymbol{\Gamma}\in\mathbb{R}^{n_{\mathrm{odor}}\times n_{\mathrm{g}}}\) such that \(\boldsymbol{\Gamma}\boldsymbol{\Gamma}^{\top}\) is positive-definite, will have as their stationary distribution the posterior \(p(\mathbf{c}\,|\,\mathbf{s})\). here, we have replaced the \(n_{\mathrm{odor}}\)-dimensional noise process \(\boldsymbol{\eta}(t)\) with an \(n_{\mathrm{g}}\)-dimensional noise process \(\boldsymbol{\xi}(t)\) with zero mean and covariance

\[\mathbb{E}[\xi_{j}(t)\xi_{j^{\prime}}(t^{\prime})]=2\tau_{\mathrm{g}}\delta_{ jj^{\prime}}\delta(t-t^{\prime}).\] (B.10)

Then, we can write the concentration estimate as

\[\mathbf{c}(t)=\boldsymbol{\Gamma}\mathbf{g}(t)\] (B.11)

where the activity of the neurons \(\mathbf{g}\in\mathbb{R}^{n_{\mathrm{g}}}\) follows the dynamics

\[\tau_{\mathrm{g}}\dot{\mathbf{g}}(t)=\boldsymbol{\Gamma}^{\top}\boldsymbol{ \nabla}_{\mathbf{c}}\log p(\mathbf{g}\,|\,\mathbf{s})\bigg{|}_{\mathbf{c}= \boldsymbol{\Gamma}\mathbf{g}}+\boldsymbol{\xi}(t).\] (B.12)

Using the gradient of the log-posterior computed above, we then have

\[\tau_{\mathrm{g}}\dot{\mathbf{g}}(t)=(\mathbf{A}\boldsymbol{\Gamma})^{\top}[ \mathbf{s}\oslash(\mathbf{r}_{0}+\mathbf{A}\boldsymbol{\Gamma}\mathbf{g})- \mathbf{1}]+\boldsymbol{\Gamma}^{\top}[(\boldsymbol{\alpha}-\mathbf{1})\oslash( \boldsymbol{\Gamma}\mathbf{g})]-\boldsymbol{\Gamma}^{\top}\boldsymbol{\lambda}+ \boldsymbol{\xi}(t).\] (B.13)

These dynamics include two divisive non-linearities, which can be complex to implement in biophysical models of single neurons [80]. Using the approach proposed in Chalk et al. [81], we can linearize the inference by introducing two additional cell types that have as their fixed points the elementwise divisions \(\mathbf{s}\oslash(\mathbf{r}_{0}+\mathbf{A}\boldsymbol{\Gamma}\mathbf{g})\) and \((\boldsymbol{\alpha}-\mathbf{1})\oslash(\boldsymbol{\Gamma}\mathbf{g})\). We then introduce a population \(\mathbf{p}\) of \(n_{\mathrm{OSN}}\) neurons, with dynamics

\[\tau_{\mathrm{p}}\dot{\mathbf{p}}(t)=\mathbf{s}-\mathbf{p}\oslash(\mathbf{r}_ {0}+\mathbf{A}\boldsymbol{\Gamma}\mathbf{g}),\] (B.14)

such that the fixed point is

\[\mathbf{p}^{*}=\mathbf{s}\oslash(\mathbf{r}_{0}+\mathbf{A}\boldsymbol{\Gamma} \mathbf{g}).\] (B.15)

We finally introduce a third population \(\mathbf{z}\) of \(n_{\mathrm{odor}}\) neurons, with dynamics

\[\tau_{\mathrm{z}}\dot{\mathbf{z}}(t)=\boldsymbol{\alpha}-\mathbf{1}-\mathbf{z} \odot(\boldsymbol{\Gamma}\mathbf{g}),\] (B.16)

such that the fixed point is

\[\mathbf{z}^{*}=(\boldsymbol{\alpha}-\mathbf{1})\oslash(\boldsymbol{\Gamma} \mathbf{g}).\] (B.17)To be more precise, the cell types \(\mathbf{p}\) and \(\mathbf{z}\) compute the desired elementwise divisions in the pseudo-steady-state regime \(\tau_{\mathrm{p}},\tau_{\mathbf{z}}\downarrow 0\).

Putting everything together, this gives the circuit dynamics

\[\mathbf{c}(t) =\boldsymbol{\Gamma}\mathbf{g}(t)\] (B.18) \[\tau_{\mathrm{g}}\dot{\mathbf{g}}(t) =(\boldsymbol{\Lambda}\boldsymbol{\Gamma})^{\top}(\mathbf{p}- \boldsymbol{1})+\boldsymbol{\Gamma}^{\top}(\mathbf{z}-\boldsymbol{\lambda})+ \boldsymbol{\xi}(t)\] (B.19) \[\tau_{\mathrm{p}}\dot{\mathbf{p}}(t) =\mathbf{s}-\mathbf{p}\odot(\mathbf{r}_{0}+\boldsymbol{\Lambda }\mathbf{\Gamma}\mathbf{g})\] (B.20) \[\tau_{\mathbf{z}}\dot{\mathbf{z}}(t) =\boldsymbol{\alpha}-\boldsymbol{1}-\mathbf{z}\odot\mathbf{c},\] (B.21)

where we recall that the covariance of the zero-mean Gaussian noise process \(\boldsymbol{\xi}(t)\) is

\[\mathbb{E}[\xi_{j}(t)\xi_{j^{\prime}}(t^{\prime})]=2\tau_{\mathrm{g}}\delta_{ jj^{\prime}}\delta(t-t^{\prime}).\] (B.22)

We note that the two constant terms in the dynamics of \(\mathbf{g}\) can be grouped into an overall leak term \(-\boldsymbol{\Gamma}^{\top}(\boldsymbol{\Lambda}^{\top}\boldsymbol{1}+ \boldsymbol{\lambda})\). As detailed in the main text, we interpret \(\mathbf{p}\) as M/T cells, \(\mathbf{g}\) as granule cells, and \(\mathbf{z}\) as cortical feedback.

## Appendix C Stability of the MAP fixed point

In this Appendix, we aim to get some understanding for how the introduction of the cell type \(\mathbf{p}\) to represent the elementwise division affects the inference, particularly when \(\tau_{\mathrm{p}}\) is comparable to \(\tau_{\mathrm{g}}\). For simplicity, we specialize to the case of an exponential prior (i.e., we set \(\boldsymbol{\alpha}=\boldsymbol{1}\)), in which case the general circuit (3) simplifies to

\[\mathbf{c}(t) =\boldsymbol{\Gamma}\mathbf{g}(t)\] (C.1) \[\tau_{\mathrm{g}}\dot{\mathbf{g}}(t) =(\boldsymbol{\Lambda}\boldsymbol{\Gamma})^{\top}(\mathbf{p}- \boldsymbol{1})-\boldsymbol{\Gamma}^{\top}\boldsymbol{\lambda}\] \[\tau_{\mathrm{p}}\dot{\mathbf{p}}(t) =\mathbf{s}-\mathbf{p}\odot(\mathbf{r}_{0}+\boldsymbol{\Lambda }\mathbf{\Gamma}\mathbf{g}).\]

### Analysis of a two-cell circuit

To build intuition, we first consider a circuit with \(n_{\mathrm{odor}}=n_{\mathrm{OSN}}=n_{\mathrm{g}}=1\):

\[c(t) =\gamma g(t)\] (C.2) \[\tau_{\mathrm{g}}\frac{dg}{dt} =a\gamma(p-1)-\gamma\lambda\] \[\tau_{\mathrm{p}}\frac{dp}{dt} =s-p(r_{0}+a\gamma g).\]

In this case, it is useful to non-dimensionalize the system. Re-scale time as

\[\tilde{t}=\frac{a\gamma}{\tau_{\mathrm{g}}}t,\] (C.3)

and define the dimensionless parameters and input

\[\tilde{\tau} =\frac{\tau_{\mathrm{p}}}{\tau_{\mathrm{g}}},\] (C.4) \[\tilde{r}_{0} =\frac{r_{0}}{a\gamma},\] (C.5) \[\tilde{\lambda} =\frac{\lambda}{a},\] (C.6) \[\tilde{s} =\frac{s}{a\gamma}.\] (C.7)

Then, the dynamics are

\[\frac{dg}{d\tilde{t}} =p-1-\tilde{\lambda}\] (C.8) \[\tilde{\tau}\frac{dp}{d\tilde{t}} =\tilde{s}-p(\tilde{r}_{0}+g).\] (C.9)These dynamics have a single fixed point at

\[g_{*} =\frac{\tilde{s}}{1+\tilde{\lambda}}-\tilde{r}_{0}\] (C.10) \[p_{*} =1+\tilde{\lambda}.\] (C.11)

Linearizing about this fixed point, we have

\[\frac{d}{d\tilde{t}}\begin{pmatrix}\delta g\\ \delta g\end{pmatrix}=\mathbf{M}\begin{pmatrix}\delta g\\ \delta g\end{pmatrix}\] (C.12)

for

\[\mathbf{M}=\frac{1}{\tilde{\tau}}\begin{pmatrix}0&\tilde{\tau}\\ -(1+\tilde{\lambda})&-(1+\tilde{\lambda})^{-1}\tilde{s}\end{pmatrix}.\] (C.13)

The eigenvalues of \(\mathbf{M}\) are easily computed as

\[\Lambda_{\pm}(\mathbf{M})=\frac{-\tilde{s}\pm\sqrt{\tilde{s}^{2}-4\tilde{\tau }(1+\tilde{\lambda})^{3}}}{2\tilde{\tau}(1+\tilde{\lambda})}.\] (C.14)

For any positive \(\tilde{\tau}\) and non-negative \(\tilde{\lambda}\), we have

\[\operatorname{Re}\Lambda_{\pm}(\mathbf{M}) =\frac{-\tilde{s}\pm\operatorname{Re}\sqrt{\tilde{s}^{2}-4\tilde {\tau}(1+\tilde{\lambda})^{3}}}{2\tilde{\tau}(1+\tilde{\lambda})}\] (C.15) \[=\begin{cases}\frac{-\tilde{s}\pm\sqrt{\tilde{s}^{2}-4\tilde{\tau }(1+\tilde{\lambda})^{3}}}{2\tilde{\tau}(1+\tilde{\lambda})}&\tilde{s}^{2}-4 \tilde{\tau}(1+\tilde{\lambda})^{3}>0\\ \frac{-\tilde{s}}{2\tilde{\tau}(1+\tilde{\lambda})}&\text{ otherwise}, \end{cases}\] (C.16)

hence it is easy to see that the real parts of both of these eigenvalues are strictly negative so long as \(\tilde{s}>0\), meaning that the system is stable. If \(\tilde{s}=0\)--which should be exceedingly rare if the OSNs have a baseline rate--then

\[\Lambda_{\pm}(\mathbf{M})\bigg{|}_{\tilde{s}=0}=\pm 2i\sqrt{\frac{1+\tilde{ \lambda}}{\tilde{\tau}}},\] (C.17)

and there can be oscillations. Therefore, a linear stability analysis suggests that the MAP fixed point of this two-cell circuit should be stable even for large \(\tilde{\tau}\) in the presence of a non-zero input, though we expect the relaxation timescales to grow as \(\tilde{\tau}\) becomes larger.

### Analysis of the full circuit

We now consider the full MAP circuit

\[\mathbf{c}(t) =\mathbf{\Gamma}\mathbf{g}(t)\] (C.18) \[\tau_{\mathrm{g}}\dot{\mathbf{g}}(t) =(\mathbf{A}\mathbf{\Gamma})^{\top}(\mathbf{p}-\mathbf{1})- \mathbf{\Gamma}^{\top}\boldsymbol{\lambda}\] \[\tau_{\mathrm{p}}\dot{\mathbf{p}}(t) =\mathbf{s}-\mathbf{p}\odot(\mathbf{r}_{0}+\mathbf{A}\mathbf{ \Gamma}\mathbf{g}).\]

We assume that \(n_{\mathrm{OSN}}<n_{\mathrm{odor}}\leq n_{\mathrm{g}}\) and that \(\mathbf{A}\mathbf{\Gamma}\) is of full row rank, i.e., it has rank \(n_{\mathrm{OSN}}\). We recall that \(\mathbf{g}\in\mathbb{R}_{+}^{n_{\mathrm{g}}}\) and \(\mathbf{p}\in\mathbb{R}_{+}^{n_{\mathrm{g}}}\). Therefore, the dynamics of \(\mathbf{g}\) span only an \(n_{\mathrm{odor}}\)-dimensional subspace, and, in particular, the \(\mathbf{p}\)-dependent term affects only an \(n_{\mathrm{OSN}}\)-dimensional subspace.

We start by observing that the dynamics of \(\mathbf{p}\) depend on \(\mathbf{g}\) only through

\[\mathbf{q}\equiv\mathbf{A}\mathbf{\Gamma}\mathbf{g}.\] (C.19)

Importantly, if \(\mathbf{A}\mathbf{\Gamma}\) is positivity-preserving, then \(\mathbf{q}\in\mathbb{R}_{+}^{n_{\mathrm{OSN}}}\). Then, we have the closed dynamics

\[\tau_{\mathrm{g}}\dot{\mathbf{q}} =(\mathbf{A}\mathbf{\Gamma}\mathbf{\Gamma}^{\top}\mathbf{A}^{ \top})(\mathbf{p}-\mathbf{1})-\mathbf{A}\mathbf{\Gamma}\mathbf{\Gamma}^{ \top}\boldsymbol{\lambda}\] (C.20) \[\tau_{\mathrm{p}}\dot{\mathbf{p}} =\mathbf{s}-\mathbf{p}\odot(\mathbf{r}_{0}+\mathbf{q})\]for \((\mathbf{q},\mathbf{p})\in\mathbb{R}_{+}^{2n_{\text{OSN}}}\). The fixed point of this system is of course determined by the conditions \(d(\mathbf{q},\mathbf{p})/dt=\mathbf{0}\), which gives

\[(\mathbf{A}\mathbf{\Gamma}\mathbf{\Gamma}^{\top}\mathbf{A}^{\top}) (\mathbf{p}^{*}-\mathbf{1}) =\mathbf{A}\mathbf{\Gamma}\mathbf{\Gamma}^{\top}\mathbf{\lambda}\] (C.21) \[\mathbf{p}^{*}\odot(\mathbf{r}_{0}+\mathbf{q}^{*}) =\mathbf{s}\] (C.22)

subject to the non-negativity constraints. By our assumptions on the rank of \(\mathbf{A}\mathbf{\Gamma}\), the symmetric matrix \(\mathbf{A}\mathbf{\Gamma}\mathbf{\Gamma}^{\top}\mathbf{A}^{\top}\) is positive-definite and thus can be inverted to solve the first condition for \(\mathbf{p}^{*}\):

\[\mathbf{p}^{*}=\mathbf{1}+(\mathbf{A}\mathbf{\Gamma}\mathbf{\Gamma}^{\top} \mathbf{A}^{\top})^{-1}\mathbf{A}\mathbf{\Gamma}\mathbf{\Gamma}^{\top} \mathbf{\lambda}.\] (C.23)

For the second condition to be satisfied, we can see that the elements of \(\mathbf{p}^{*}\) must be strictly positive at the fixed point, which gives a self-consistency condition on \(\mathbf{A}\), \(\mathbf{\Gamma}\), and \(\mathbf{\lambda}\). Assuming that this holds, we then have

\[\mathbf{q}^{*}=\mathbf{s}\odot\mathbf{p}^{*}-\mathbf{r}_{0},\] (C.24)

which again gives a self-consistency condition as non-negativity is required.

Assuming these conditions hold, we can then linearize the dynamics about the fixed point. For convenience, we non-dimensionalize time through

\[\tilde{t}=\frac{t}{\tau_{\text{g}}},\] (C.25)

and

\[\tilde{\tau}=\frac{\tau_{\text{p}}}{\tau_{\text{g}}},\] (C.26)

which yields the linearized dynamics

\[\frac{d}{d\tilde{t}}\begin{pmatrix}\delta\mathbf{q}\\ \delta\mathbf{p}\end{pmatrix}=\mathbf{M}\begin{pmatrix}\delta\mathbf{q}\\ \delta\mathbf{p}\end{pmatrix}\] (C.27)

for

\[\mathbf{M} =\begin{pmatrix}\mathbf{0}&(\mathbf{A}\mathbf{\Gamma}\mathbf{ \Gamma}^{\top}\mathbf{A}^{\top})\\ -\tilde{\tau}^{-1}\operatorname{diag}(\mathbf{p}^{*})&-\tilde{\tau}^{-1} \operatorname{diag}(\mathbf{r}_{0}+\mathbf{q}^{*})\end{pmatrix}\] (C.28) \[=\begin{pmatrix}\mathbf{0}&(\mathbf{A}\mathbf{\Gamma}\mathbf{ \Gamma}^{\top}\mathbf{A}^{\top})\\ -\tilde{\tau}^{-1}\operatorname{diag}(\mathbf{p}^{*})&-\tilde{\tau}^{-1} \operatorname{diag}(\mathbf{s}\odot\mathbf{p}^{*})\end{pmatrix}.\] (C.29)

Using the fact that the diagonal matrices \(\operatorname{diag}(\mathbf{p}^{*})\) and \(\operatorname{diag}(\mathbf{s}\odot\mathbf{p}^{*})\) commute, the characteristic polynomial of \(\mathbf{M}\) is

\[\det(\mathbf{M}-\mu\mathbf{I}_{2n_{\text{OSN}}}) =\det\begin{pmatrix}\mu\mathbf{I}_{n_{\text{OSN}}}&-(\mathbf{A} \mathbf{\Gamma}\mathbf{\Gamma}^{\top}\mathbf{A}^{\top})\\ \tilde{\tau}^{-1}\operatorname{diag}(\mathbf{p}^{*})&\tilde{\tau}^{-1} \operatorname{diag}(\mathbf{s}\odot\mathbf{p}^{*})+\mu\mathbf{I}_{n_{\text{OSN }}}\end{pmatrix}\] (C.30) \[=\det[\mu^{2}\mathbf{I}_{n_{\text{OSN}}}+\mu\tilde{\tau}^{-1} \operatorname{diag}(\mathbf{s}\odot\mathbf{p}^{*})+\tilde{\tau}^{-1}(\mathbf{A }\mathbf{\Gamma}\mathbf{\Gamma}^{\top}\mathbf{A}^{\top})\operatorname{diag}( \mathbf{p}^{*})].\] (C.31)

One case that is particularly easy to solve is when the symmetric positive-definite matrix \(\mathbf{A}\mathbf{\Gamma}\mathbf{\Gamma}^{\top}\mathbf{A}^{\top}\) is in fact diagonal, with positive diagonal entries \(a_{j}\). Then, we have

\[\det(\mathbf{M}-\mu\mathbf{I}_{2n_{\text{OSN}}})=\prod_{j=1}^{n_{\text{OSN}}} \left[\mu^{2}+\frac{s_{j}}{\tilde{\tau}p_{j}^{*}}\mu+\frac{a_{j}p_{j}^{*}}{ \tilde{\tau}}\right],\] (C.32)

meaning that the eigenvalues of \(\mathbf{M}\) are

\[\mu_{j,\pm}=\frac{1}{2}\left[-\frac{s_{j}}{\tilde{\tau}p_{j}^{*}}\pm\sqrt{ \left(\frac{s_{j}}{\tilde{\tau}p_{j}^{*}}\right)^{2}-4\frac{a_{j}p_{j}^{*}}{ \tilde{\tau}}}\right],\] (C.33)

which under the given assumptions always have strictly positive real part for non-negative inputs.

Now, more generally, assume that \(\mathbf{M}\) is diagonalizable, and let \(\mathbf{m}\) be a un-normalized eigenvector of \(\mathbf{M}\) with eigenvalue \(\mu\). Then, writing

\[\mathbf{m}=\begin{pmatrix}\mathbf{u}\\ \mathbf{v}\end{pmatrix}\] (C.34)

for \(\mathbf{u},\mathbf{v}\in\mathbb{C}^{n_{\mathrm{OSN}}}\), the eigenvector condition

\[\mathbf{M}\mathbf{m}=\mu\mathbf{m}\] (C.35)

implies that \(\mathbf{u}\) and \(\mathbf{v}\) satisfy

\[\frac{1}{\tau_{\mathrm{g}}}(\mathbf{A}\mathbf{\Gamma}\mathbf{ \Gamma}^{\top}\mathbf{A}^{\top})\mathbf{v} =\mu\mathbf{u}\] (C.36) \[-\frac{1}{\tau_{\mathrm{p}}}\operatorname{diag}(\mathbf{p}^{*}) \mathbf{u}-\frac{1}{\tau_{\mathrm{p}}}\operatorname{diag}(\mathbf{s}\oslash \mathbf{p}^{*})\mathbf{v} =\mu\mathbf{v}.\] (C.37)

Assuming that \(\mu\) is non-zero, we can solve the first equation for \(\mathbf{u}\) and then substitute the result into the second to obtain a quadratic eigenproblem for \(\mu\) and \(\mathbf{v}\):

\[\mu^{2}\mathbf{v}+\mu\frac{1}{\tau_{\mathrm{p}}}\operatorname{diag}(\mathbf{s }\oslash\mathbf{p}^{*})\mathbf{v}+\frac{1}{\tau_{\mathrm{p}}\tau_{\mathrm{g}}} \operatorname{diag}(\mathbf{p}^{*})(\mathbf{A}\mathbf{\Gamma}\mathbf{\Gamma} ^{\top}\mathbf{A}^{\top})\mathbf{v}=0.\] (C.38)

We will not attempt to solve this eigenproblem, but will instead attempt to extract information about possible values of \(\mu\) for a fixed \(\mathbf{v}\). Suppose (without loss of generality given the assumption that it is nonzero, as otherwise we may divide by its norm) that \(\mathbf{v}\) is a unit vector.Then, acting with \(\mathbf{v}^{\dagger}\) from the left, we have

\[\mu^{2}+\frac{a}{\tau_{\mathrm{p}}}\mu+\frac{b}{\tau_{\mathrm{p}}\tau_{ \mathrm{g}}}=0.\] (C.39)

where we define the coefficients

\[a\equiv\mathbf{v}^{\dagger}\operatorname{diag}(\mathbf{s}\oslash\mathbf{p}^{ *})\mathbf{v}\] (C.40)

and

\[b\equiv\mathbf{v}^{\dagger}\operatorname{diag}(\mathbf{p}^{*})(\mathbf{A} \mathbf{\Gamma}\mathbf{\Gamma}^{\top}\mathbf{A}^{\top})\mathbf{v}.\] (C.41)

So long as \(\mathbf{s}\) and \(\mathbf{p}^{*}\) are positive, \(a\) is real and positive. Let

\[\tilde{\boldsymbol{\lambda}}\equiv(\mathbf{A}\mathbf{\Gamma}\mathbf{\Gamma} ^{\top}\mathbf{A}^{\top})^{-1}\mathbf{A}\mathbf{\Gamma}\mathbf{\Gamma}^{\top }\boldsymbol{\lambda}\] (C.42)

such that

\[\mathbf{p}^{*}=\mathbf{1}+\tilde{\boldsymbol{\lambda}}.\] (C.43)

By assumption, the elements of \(\tilde{\boldsymbol{\lambda}}\) are strictly greater than \(-1\). Then,

\[\operatorname{Re}(b) =\mathbf{v}^{\dagger}\frac{\operatorname{diag}(\mathbf{p}^{*})( \mathbf{A}\mathbf{\Gamma}\mathbf{\Gamma}^{\top}\mathbf{A}^{\top})+(\mathbf{A} \mathbf{\Gamma}\mathbf{\Gamma}^{\top}\mathbf{A}^{\top})\operatorname{diag}( \mathbf{p}^{*})}{2}\mathbf{v}\] (C.44) \[=\mathbf{v}^{\dagger}(\mathbf{A}\mathbf{\Gamma}\mathbf{\Gamma}^{ \top}\mathbf{A}^{\top})\mathbf{v}+\mathbf{v}^{\dagger}\frac{\operatorname{ diag}(\tilde{\boldsymbol{\lambda}})(\mathbf{A}\mathbf{\Gamma}\mathbf{\Gamma}^{ \top}\mathbf{A}^{\top})+(\mathbf{A}\mathbf{\Gamma}\mathbf{\Gamma}^{\top} \mathbf{A}^{\top})\operatorname{diag}(\tilde{\boldsymbol{\lambda}})}{2} \mathbf{v},\] (C.45)

while

\[\operatorname{Im}(b) =\mathbf{v}^{\dagger}\frac{\operatorname{diag}(\mathbf{p}^{*})( \mathbf{A}\mathbf{\Gamma}\mathbf{\Gamma}^{\top}\mathbf{A}^{\top})-( \mathbf{A}\mathbf{\Gamma}\mathbf{\Gamma}^{\top}\mathbf{A}^{\top}) \operatorname{diag}(\mathbf{p}^{*})}{2i}\mathbf{v}\] (C.47) \[=\mathbf{v}^{\dagger}\frac{\operatorname{diag}(\tilde{\boldsymbol {\lambda}})(\mathbf{A}\mathbf{\Gamma}\mathbf{\Gamma}^{\top}\mathbf{A}^{\top}) -(\mathbf{A}\mathbf{\Gamma}\mathbf{\Gamma}^{\top}\mathbf{A}^{\top}) \operatorname{diag}(\tilde{\boldsymbol{\lambda}})}{2i}\mathbf{v}.\] (C.48)

Then, solving the quadratic equation for \(\mu\), we have

\[\mu=\frac{1}{\tau_{\mathrm{p}}}\frac{-a\pm\sqrt{a^{2}-4\bar{\tau}b}}{2},\] (C.49)for

\[\tilde{\tau}\equiv\frac{\tau_{\rm p}}{\tau_{\rm g}}.\] (C.49)

This gives

\[\operatorname{Re}(\mu) =-\frac{a}{2\tau_{\rm p}}\pm\frac{1}{2\tau_{\rm p}}\operatorname{ Re}\sqrt{a^{2}-4\tilde{\tau}b}\] (C.50) \[=-\frac{a}{2\tau_{\rm p}}\pm\frac{1}{2\tau_{\rm p}}\sqrt{\frac{ \sqrt{(a^{2}-4\tilde{\tau}\operatorname{Re}(b))^{2}+16\tilde{\tau}^{2} \operatorname{Im}(b)^{2}}+a^{2}-4\tilde{\tau}\operatorname{Re}(b)}{2}}.\] (C.51)

If \(\operatorname{Im}(b)=0\), then we are in the same situation as we were for the two-neuron circuit, and the fixed point is thus stable.

More generally, for the system to be stable we want to have

\[a>\sqrt{\frac{\sqrt{(a^{2}-4\tilde{\tau}\operatorname{Re}(b))^{2}+16\tilde{ \tau}^{2}\operatorname{Im}(b)^{2}}+a^{2}-4\tilde{\tau}\operatorname{Re}(b)}{2}}.\] (C.52)

For either sign of \(a^{2}-4\tilde{\tau}\operatorname{Re}(b)\), one can show that this holds provided that

\[\operatorname{Re}(b)>0\] (C.53)

and

\[\sqrt{\tilde{\tau}}|\operatorname{Im}(b)|<a\sqrt{\operatorname{Re}(b)}.\] (C.54)

This amounts to a combined condition on the effective prior strength \(\tilde{\boldsymbol{\lambda}}\) and the relative time constants. Therefore, for sufficiently small \(\tilde{\boldsymbol{\lambda}}\), we have stability for a broad range of \(\tau_{\rm p}\).

How can we go from stability of the \((\mathbf{q},\mathbf{p})\) circuit to stability of the full circuit? Heuristically, this follows by decomposing the dynamics of \(\mathbf{g}\) in terms of the different subspaces. We will not attempt to do this rigorously, as our goal is only to get a rough sense of how the system should behave. By the fact that their dimensions are strictly ordered, we know that \(\operatorname{span}(\boldsymbol{\Gamma}^{\top})\supset\operatorname{span}[( \boldsymbol{\Lambda}\boldsymbol{\Gamma})^{\top}]\). We can first exclude components outside \(\operatorname{span}(\boldsymbol{\Gamma}^{\top})\), as they will be unchanged by the dynamics and do not affect the concentration estimate. Recalling the non-negativity constraint, components in \(\operatorname{span}(\boldsymbol{\Gamma}^{\top})\setminus\operatorname{span}[ (\boldsymbol{\Lambda}\boldsymbol{\Gamma})^{\top}]\) will decay linearly to zero. Then, the components in \(\operatorname{span}[(\boldsymbol{\Lambda}\boldsymbol{\Gamma})^{\top}]\) should be controllable using the argument for the \((\mathbf{q},\mathbf{p})\) space. Giving a fully rigorous analysis of the conditions under which the MAP fixed point is stable will be an interesting avenue for future investigation.

## Appendix D Derivation for the Gaussian circuit model

To compare the result of the state dependent inhibition due to estimating the MAP for a Poisson noise model with the fixed inhibition resulting from Gaussian noise we build an alternative circuit model. This circuit still shares the separation into two cell types, but the granule cells \(\mathbf{g}\) converge on the solution for a Gaussian noise model.

Our starting point is an isotropic Gaussian likelihood

\[\mathbf{s}\,|\,\mathbf{c}\sim\mathcal{N}(\mathbf{r}_{0}+\mathbf{A}\mathbf{c},\sigma^{2}\mathbf{I}_{n_{\rm OSN}})\] (D.1)

variance \(\sigma^{2}\), and an exponential prior

\[\mathbf{c}\sim\operatorname{Exp}(\boldsymbol{\lambda}).\] (D.2)

Gradient ascent on the resulting log-posterior over \(\mathbf{c}\) leads to the dynamics:

\[\dot{\mathbf{c}}(t)=\frac{1}{\sigma^{2}}\mathbf{A}^{\top}[\mathbf{s}-(\mathbf{ r}_{0}+\mathbf{A}\mathbf{c})]-\boldsymbol{\lambda},\] (D.3)

Distributing the code such that \(\mathbf{c}(t)=\boldsymbol{\Gamma}\mathbf{g}\) and splitting the inference as we did in the Poisson case gives the circuit dynamics

\[\mathbf{c}(t) =\boldsymbol{\Gamma}\mathbf{g}(t)\] (D.4) \[\tau_{\rm g}\dot{\mathbf{g}}(t) =\frac{1}{\sigma^{2}}(\boldsymbol{\Lambda}\boldsymbol{\Gamma})^{ \top}\mathbf{p}-\boldsymbol{\Gamma}^{\top}\boldsymbol{\lambda}\] \[\tau_{\rm p}\dot{\mathbf{p}}(t) =-\mathbf{p}+\mathbf{s}-(\mathbf{r}_{0}+\boldsymbol{\Lambda} \mathbf{F}\mathbf{g}).\]

Unlike in the Poisson case, the inhibition onto the projection cells (mitral cells) is not gated by their activity, leading to the fixed offset shown in Fig. 2B.

Extending the sampling circuit to incorporate an \(L_{0}\) prior

In this Appendix, we consider the possibility of extending our circuit model to incorporate an \(L_{0}\) spike-and-slab prior

\[p(c_{i})=\varpi e^{-\lambda c_{i}}+(1-\varpi)\delta(c_{i}),\] (E.1)

where \(\varpi\) is the probability that the odor is present and \(\lambda\) is the rate of the exponential prior on concentrations given that the odor is present. To sample from the resulting posterior using Langevin dynamics, we will follow the approach of Fang et al. [115], who developed a Langevin algorithm to perform sparse coding with this prior given a Gaussian likelihood. This approach only works in the sampling setting; it cannot be applied to MAP estimation because the Dirac mass at \(c_{i}=0\) means that the MAP estimate will always vanish.

In this approach, we define an auxiliary variable \(\mathbf{u}\) that is mapped to concentration estimates \(\mathbf{c}\) via element-wise soft thresholding:

\[\mathbf{c}=f(\mathbf{u}),\] (E.2)

where

\[f(u)=\begin{cases}0&u<u_{0}\\ u-u_{0}&u\geq u_{0}\end{cases}\] (E.3)

is the soft-thresholding function for threshold

\[u_{0}=-\frac{1}{\lambda}\log\varpi.\] (E.4)

We then posit the following Langevin dynamics for \(\mathbf{u}\), given an observation \(\mathbf{s}\):

\[\dot{\mathbf{u}}(t)=\left[\boldsymbol{\nabla}_{\mathbf{c}}\log p(\mathbf{s} \mid\mathbf{c})\right]_{\mathbf{c}=f(\left\lvert\mathbf{u}\right\rvert)} \odot\Theta(\left\lvert\mathbf{u}\right\rvert-\mathbf{u}_{0})-\lambda\operatorname {sign}(\mathbf{u})+\boldsymbol{\eta}(t)\] (E.5)

with no constraint on the sign of \(\mathbf{u}\), where \(\boldsymbol{\eta}(t)\) is a white Gaussian noise process. Here, all nonlinearities are applied element-wise, and \(\mathbf{u}_{0}=u_{0}\mathbf{1}\) is a vector with all elements equal to the threshold \(u_{0}\). Fang et al. [115] argue that the stationary distribution induced on \(\mathbf{c}=f(\mathbf{u})\) by these dynamics should be the desired posterior with \(L_{0}\) prior.

Using the Poisson likelihood gradient as computed before, we have

\[\dot{\mathbf{u}}(t)=\{\mathbf{A}^{\top}[\mathbf{s}\oslash(\mathbf{r}_{0}+ \mathbf{A}f(\left\lvert\mathbf{u}\right\rvert)-\mathbf{1}]\}\odot\Theta( \left\lvert\mathbf{u}\right\rvert-\mathbf{u}_{0})-\lambda\operatorname{sign} (\mathbf{u})+\boldsymbol{\eta}(t)\] (E.6)

At this stage, we can see that the soft thresholding has picked out a preferred basis. If we apply the complete recipe in a way analogous to what we did in Appendix B by writing

\[\mathbf{u}=\boldsymbol{\Gamma}\mathbf{g},\] (E.7)

we will have

\[\tau_{\mathrm{g}}\dot{\mathbf{g}}(t)=\boldsymbol{\Gamma}^{\top}\bigg{[}\{ \mathbf{A}^{\top}[\mathbf{s}\oslash(\mathbf{r}_{0}+\mathbf{A}f(\left\lvert \boldsymbol{\Gamma}\mathbf{g}\right\rvert)-\mathbf{1}]\}\odot\Theta(\left\lvert \boldsymbol{\Gamma}\mathbf{g}\right\rvert-\mathbf{u}_{0})-\lambda\operatorname{ sign}(\boldsymbol{\Gamma}\mathbf{g})\bigg{]}+\boldsymbol{\xi}(t),\] (E.8)

where \(\boldsymbol{\xi}(t)\) is a zero-mean Gaussian noise process with covariance

\[\mathbb{E}[\xi_{j}(t)\xi_{j^{\prime}}(t^{\prime})]=2\tau_{\mathrm{g}}\delta_ {jj^{\prime}}\delta(t-t^{\prime}).\] (E.9)

Here, we cannot simply regroup terms; if we introduce an additional cell type \(\mathbf{p}\) as before to compute the division we will have a sort of effective weight matrix

\[\sum_{k}\Gamma_{ki}A_{jk}\Theta(\left\lvert u_{k}\right\rvert-u_{0})\] (E.10)

for the \(\mathbf{p}\)-to-\(\mathbf{g}\) connections, and an entirely different coupling \(\mathbf{A}f(\left\lvert\boldsymbol{\Gamma}\mathbf{g}\right\rvert)\) for the \(\mathbf{g}\)-to-\(\mathbf{p}\) connections. Concretely, introducing \(\mathbf{p}\) as before, we have the system

\[\mathbf{c}(t) =f(\left\lvert\boldsymbol{\Gamma}\mathbf{g}(t)\right\rvert)\] (E.11) \[\tau_{\mathrm{g}}\dot{\mathbf{g}}(t) =\boldsymbol{\Gamma}^{\top}\bigg{[}[\mathbf{A}^{\top}(\mathbf{p} -\mathbf{1})]\odot\Theta(\left\lvert\boldsymbol{\Gamma}\mathbf{g}\right\rvert -\mathbf{u}_{0})-\lambda\operatorname{sign}(\boldsymbol{\Gamma}\mathbf{g}) \bigg{]}\,dt+\boldsymbol{\xi}(t)\] (E.12) \[\tau_{\mathrm{p}}\dot{\mathbf{p}}(t) =\mathbf{s}-\mathbf{p}\oslash[\mathbf{r}_{0}+\mathbf{A}f(\left \lvert\boldsymbol{\Gamma}\mathbf{g}\right\rvert)].\] (E.13)

This issue illustrates the limitations of the simple, linear approach to distributing the neural code used in Appendix B. In particular, the nonlinearity in some sense picks out a preferred basis, meaning that we can no longer perform a simple linear change of coordinates to distribute the code.

Experimental details and affinity matrix fitting

Here we briefly describe the experimental procedures used to collect the data used to fit the parameters of the affinity matrix \(\mathbf{A}\) in Fig. 1. The full experimental methods are described in the paper that first presented the data, Zak et al. [94]. All the experiments were performed in accordance with the guidelines set by the National Institutes of Health and approved by the Institutional Animal Care and Use Committee at Harvard University.

### _In vivo_ recordings from mouse OB

Adult (> 8 weeks) OMP-GCaMP3 mice of both sexes were used in this study. A craniotomy was performed to provide optical access to olfactory sensory neuron axon terminals in both olfactory bulbs. A custom-built two-photon microscope was used for in vivo imaging. Images were acquired at 16-bit resolution and 4-8 frames/s. The pixel size was \(0.6\;\mu m\) and the fields of view were \(720\,\times\,720\,\mu m\). Monomolecular odorants (Allyl butyrate, Ethyl valerate, Methyl tglate, and Isobutyl propionate) were used as stimuli and delivered by a 16-channel olfactometer controlled by custom-written software in LabView. For the odorant concentration series, the initial odorant concentration was between \(0.08\%-80\%(v/v)\) in mineral oil and further diluted 16 times with air. The relative odorant concentrations were measured by a photoionization detector, then normalized to the largest detected signal for each odorant. For all experiments, the airflow to the animal was held constant at \(100mL/min\), and odorants were injected into a carrier stream. Each odorant concentration was delivered 2-6 times in pseudorandom order.

Images were processed using both custom and available MATLAB scripts. Motion artifact compensation and denoising were done using _NoRMcorre_[138]. The \(\Delta F/F\) signal was calculated by finding the peak signal following odorant onset and averaging with the two adjacent points. To account for changes in respiration and anesthesia depth, correlated variability was corrected for [139]. Thresholds for classifying responding ROIs were determined from a noise distribution of blank (no odorant) trials from which three standard deviations were used for responses.

### Gamma distribution fitting

In order to fit the affinity matrix \(\mathbf{A}\) to the experimentally recorded data, we normalized the response of each glomerulus to its maximum response across the panel of \(32\) odorants. We then vectorized the resulting matrix and fitted a \(\mathrm{Gamma}\) distribution using the gamfit function in MATLAB. As mentioned in the main text, this resulted in a \(\mathrm{Gamma}(0.37,0.36)\) distribution.

## Appendix G Numerical methods and supplemental figures

All numerical simulations were performed using MATLAB 9.13 (R2022b, The MathWorks, Natick, MA, USA) either on desktop workstations (CPU: Intel i9-9900K or Xeon W-2145, 64GB RAM) or on the Harvard University FASRC Cannon HPC cluster (https://www.rc.fas.harvard.edu). Our simulations were not computationally intensive, and required around that 6000 CPU-hours of total compute time.

### State-dependent inhibition simulations

For the simulations to highlight the signatures of state-dependent inhibition of the Poisson network and compare its behavior with the experimental observation by Arevian et al. [95] we use a reduced network with \(n_{OSN}=2\) and \(n_{g}=10\). We chose this reduced circuit both to reduce computational costs and to match the experimental parameters of the _in-vitro_ experiment. The other parameters of the simulation where as follows: \(\tau_{p}=0.02\), \(\tau_{g}=0.03\), \(r_{0}=10\), \(\lambda=2\), \(dt=1e^{-4}\). The simulation ran for \(600\;ms\), stimulation was applied starting at \(t_{start}=100\;ms\) and ended at \(t_{end}=500\;ms\). We stimulated the principal mitral cell \(MC_{A}\) with \(80\) equally spaced values in \(s_{A}\in[1,400]\). When the second mitral cell was active, its stimulation was set at \(s_{B}=80\). The entries of the \(\Gamma\) matrix were sampled from a normal distribution on which we applied a mask such that only \(25\%\) of its entries were non-zero. We sampled \(32\) 'pairs' of cells by resampling the \(\Gamma\) matrix for each 'pair' of cells. The simulation for the Gaussian circuit used the same parameters except the dynamics followed those from equation D.4. For plotting purposes, we normalized the range of firing in Figure 2B to the maximum firing rate for each circuit model.

### Capacity simulations

In our capacity simulations, we use \(n_{\mathrm{OSN}}=300\) and \(n_{\mathrm{odor}}=500\), 1000, or 2000. We take the odor stimulus to be a rectangular pulse, with varying numbers of randomly-selected odors appearing at concentration \(c_{j}=40\). As elsewhere, we take \(\tau_{p}=20\) ms [91] and \(\tau_{g}=30\) ms [92] to match experiment. We set the baseline rate to be \(r_{0}=1\) and the prior mean to be \(\lambda=1\); based on some experimentation our results appear relatively insensitive to small variations in these choices. We integrate the MAP circuit dynamics (3) using the forward Euler method with timestep \(\Delta t=10^{-4}\,\mathrm{s}\). To determine which odors the model estimates as being present at a given timepoint, we simply check which concentration estimates exceed 20 at that time.

As discussed in the main text, we consider three variants of the MAP circuit, defined by different choices of the matrix \(\mathbf{\Gamma}\):

* For the one-to-one code, we let \(n_{\mathrm{g}}=n_{\mathrm{odor}}\), and simply set \(\mathbf{\Gamma}=\mathbf{I}_{n_{\mathrm{odor}}}/N(A_{ij})\).
* For the naively distributed code, we let \(n_{\mathrm{g}}=5n_{\mathrm{odor}}\), and choose \(\mathbf{\Gamma}\) as follows: We sample a random matrix \(\mathbf{Q}\in\mathbb{R}^{n_{\mathrm{odor}}\times n_{\mathrm{g}}}\) with \(\mathbf{Q}\mathbf{Q}^{\top}=\mathbf{I}_{n_{\mathrm{odor}}}\) by drawing a Gaussian matrix and orthogonalizing its rows. Then, we define \(\mathbf{\Gamma}=\mathbf{Q}/N\{|(\mathbf{A}\mathbf{Q})_{ij}|\}\).
* For the geometry-aware code, we let \(n_{\mathrm{g}}=5n_{\mathrm{odor}}\), and choose \(\mathbf{\Gamma}\) as follows: We sample a random matrix \(\mathbf{Q}\in\mathbb{R}^{n_{\mathrm{odor}}\times n_{\mathrm{g}}}\) with \(\mathbf{Q}\mathbf{Q}^{\top}=\mathbf{I}_{n_{\mathrm{odor}}}\) by drawing a Gaussian matrix and orthogonalizing its rows. Then, we compute an approximate inverse square root of the low-rank matrix \(\mathbf{A}^{\top}\mathbf{A}\) as \(\mathbf{B}=(\mathbf{A}^{\top}\mathbf{A}+a\mathbf{I}_{n_{\mathrm{odor}}})^{-1/2}\) for a small positive regularizing constant \(a\). In our simulations, we set \(a=0.5\); we find empirically that our results are not substantially sensitive to small variations in \(a\). Finally, we let \(\mathbf{\Gamma}=\mathbf{B}\mathbf{Q}/N\{|(\mathbf{A}\mathbf{B}\mathbf{Q})_{ij}|\}\).

where \(N\) is a normalization function defined as

\[N(A)=\max(A)\cdot\frac{\sqrt{n_{g}}}{C}\]

with \(C=50\) identified as a reasonable choice. Our normalization convention for \(\mathbf{\Gamma}\) is motivated by the idea that, in biology, the strength of individual synapses should be bounded. Moreover, changing the overall scale of the synaptic weights in our model corresponds to changing the effective time constant of the dynamics, which can produce a trivial speedup.

Figure G.1: **A**. Fraction of odors correctly detected within 100 ms (_left_), 200 ms (_center_), and 1 s (_right_) after odor onset as a function of the number of odors present, with \(n_{\mathrm{odor}}=500\) possible odors, for models with one-to-one, naively distributed, and geometry-aware codes. **B**. As in **A**, but for \(n_{\mathrm{odor}}=1000\). This matches Fig. 3**B**. **C**, **D**, **E**. As in **A**, but for \(n_{\mathrm{odor}}=2000\), 4000, or 8000 possible odorants, respectively. Shaded patches show \(\pm 1.96\) SEM over realizations throughout.

Figure G.2: **A**. Heatmap with overlaid smoothed contours of the fraction of odors correctly detected as a function of number of present odors and time window out of a panel of \(n_{\rm odor}=500\) possible odors for models with one-to-one (_left_), naively distributed (_center_), and geometry-aware (_right_) codes. **B**. As in **A**, but for \(n_{\rm odor}=1000\). This matches Fig. 3B. **C**, **D**, **E**. As in **A**, but for \(n_{\rm odor}=2000\), 4000, or 8000 possible odorants, respectively.

### Sampling simulations

In our sampling simulations, we use \(n_{\mathrm{OSN}}=300\), \(n_{\mathrm{odor}}=500\) or 1000, and \(n_{\mathrm{g}}=5n_{\mathrm{odor}}\). Here, the odor stimulus was composed of two rectangular pulses: At time \(t_{\mathrm{low}}=0\) s, five "low" odors appear at concentration 10, and remain present at that concentration until time \(t=2\) s. Then, at time \(t_{\mathrm{high}}=1\) s, five "high" odors appear at concentration 40, and remain present until time \(t_{\mathrm{off}}=2\) s. As in our capacity simulations, we take \(\tau_{p}=20\) ms [91] and \(\tau_{g}=30\) ms [92] to match experiment, and we set the baseline rate to be \(r_{0}=1\) and the prior mean to be \(\lambda=1\). As for our capacity results, our results appear relatively insensitive to small variations in these choices. We integrate the sampling circuit dynamics (4) using the forward Euler-Maruyama method with timestep \(\Delta t=10^{-5}\) s.

As in our capacity simulations, in Fig. 4 we choose \(\mathbf{\Gamma}\) as follows:

* For the one-to-one code, we let \(n_{\mathrm{g}}=n_{\mathrm{odor}}\), and simply set \(\mathbf{\Gamma}=\mathbf{I}_{n_{\mathrm{odor}}}/\max(A_{ij})\).
* For the naively distributed code, we let \(n_{\mathrm{g}}=5n_{\mathrm{odor}}\), and choose \(\mathbf{\Gamma}\) as follows: We sample a random matrix \(\mathbf{Q}\in\mathbb{R}^{n_{\mathrm{odor}}\times n_{\mathrm{g}}}\) with \(\mathbf{Q}\mathbf{Q}^{\top}=\mathbf{I}_{n_{\mathrm{odor}}}\) by drawing a Gaussian matrix and orthogonalizing its rows. Then, we define \(\mathbf{\Gamma}=\mathbf{Q}/\max\{|(\mathbf{A}\mathbf{Q})_{ij}|\}\).
* For the geometry-aware code, we let \(n_{\mathrm{g}}=5n_{\mathrm{odor}}\), and choose \(\mathbf{\Gamma}\) as follows: We sample a random matrix \(\mathbf{Q}\in\mathbb{R}^{n_{\mathrm{odor}}\times n_{\mathrm{g}}}\) with \(\mathbf{Q}\mathbf{Q}^{\top}=\mathbf{I}_{n_{\mathrm{odor}}}\) by drawing a Gaussian matrix and orthogonalizing its rows. Then, we compute an approximate inverse square root of the low-rank matrix \(\mathbf{A}^{\top}\mathbf{A}\) as \(\mathbf{B}=(\mathbf{A}^{\top}\mathbf{A}+a\mathbf{I}_{n_{\mathrm{odor}}})^{-1/2}\) for a small positive regularizing constant \(a\). In our simulations, we set \(a=0.5\); we find empirically that our results are not substantially sensitive to small variations in \(a\). Finally, we let \(\mathbf{\Gamma}=\mathbf{B}\mathbf{Q}/\max\{|(\mathbf{A}\mathbf{B}\mathbf{Q})_{ ij}|\}\).

In Fig. 4A, we smooth the concentration estimate timeseries using a 100 ms moving average. In Fig. 4B-C, we show cumulative estimates of the mean and variance. Concretely, given a concentration timeseries \(\hat{c}_{j}(t)\), we estimate the mean and variance as

\[\mu_{j}(\tau)=\frac{\Delta t}{\tau}\sum_{t_{\mathrm{low}}\leq t\leq t_{ \mathrm{low}}+\tau}\hat{c}_{j}(t)\] (G.1)

and

\[\sigma_{j}^{2}(\tau)=\frac{\Delta t}{\tau}\sum_{t_{\mathrm{low}} \leq t\leq t_{\mathrm{low}}+\tau}\hat{c}_{j}(t)^{2}-\mu_{j}(\tau)^{2},\] (G.2)

respectively, where we assume \(\tau\) and \(t\) are integer multiples of \(\Delta t\). In Fig. 4D-E, we do the same except for times after \(t_{\mathrm{high}}\). In both cases, baselines were obtained by running the naive sampling algorithm for \(10^{8}\) steps with a burn-in period of \(10^{7}\) steps. In Supp. Fig. G.3, we reproduce Fig. 4 showing estimates for individual odorants as well as the mean across odorants.

In Supp. Fig. G.4, we show a preliminary experiment with an alternative, mostly non-negative choice for \(\mathbf{\Gamma}\). This gives the following three models:

* For the one-to-one code, we let \(n_{\mathrm{g}}=n_{\mathrm{odor}}\), and simply set \(\mathbf{\Gamma}=\mathbf{I}_{n_{\mathrm{odor}}}/\max(A_{ij})\).
* For the naively distributed code, we let \(n_{\mathrm{g}}=5n_{\mathrm{odor}}\), and choose \(\mathbf{\Gamma}\) as follows: We sample a sparse, non-negative random matrix \(\mathbf{Q}\in\mathbb{R}^{n_{\mathrm{odor}}\times n_{\mathrm{g}}}\) with entries that are non-zero with density 0.15, and the non-zero entries are drawn uniformly on \([0,1]\). Then, we define \(\mathbf{\Gamma}=\mathbf{Q}/\max\{|(\mathbf{A}\mathbf{Q})_{ij}|\}\).
* For the geometry-aware code, we let \(n_{\mathrm{g}}=5n_{\mathrm{odor}}\), and choose \(\mathbf{\Gamma}\) as follows:We sample a sparse, non-negative random matrix \(\mathbf{Q}\in\mathbb{R}^{n_{\mathrm{odor}}\times n_{\mathrm{g}}}\) with entries that are non-zero with density 0.15, and the non-zero entries are drawn uniformly on \([0,1]\). Then, we compute an approximate inverse square root of the low-rank matrix \(\mathbf{A}^{\top}\mathbf{A}\) as \(\mathbf{B}=(\mathbf{A}^{\top}\mathbf{A}+a\mathbf{I}_{n_{\mathrm{odor}}})^{-1/2}\) for a small positive regularizing constant \(a\). In our simulations, we set \(a=0.5\); we find empirically that our results are not substantially sensitive to small variations in \(a\). Finally, we let \(\mathbf{\Gamma}=\mathbf{B}\mathbf{Q}/\max\{|(\mathbf{A}\mathbf{B}\mathbf{Q})_{ ij}|\}\).

This yields a naively distributed synaptic weight matrix \(\mathbf{A}\mathbf{\Gamma}\) that is entirely non-negative, while the geometry-aware weight matrix has a small number of negative elements due to the fact that the inverse of a non-negative matrix need not be non-negative. We see that the behavior of this circuit is

[MISSING_PAGE_EMPTY:35]

Figure G.4: Langevin sampling of the posterior using sparse non-negative distributed codes. Here, we use a simple concentration estimation task in which 5 randomly-selected ‘low’ odors out of a panel of 500 appear at concentration 10 at time 0 s, and then a further 5 randomly-selected ‘high’ odors appear at concentration 40 at time 1 s. This figure replicates Fig. 4, except that the code is distributed using a sparse non-negative random matrix rather than an orthogonal matrix. Smoothed timeseries of instantaneous concentration estimates for low, high, and background odors, for models with one-to-one (_top_), naively distributed (_middle_), and geometry-aware (_bottom_) codes. Background odor estimates are shown as mean \(\pm\) standard deviation over odors. Dashed lines show true odor concentrations over time. **B**. Cumulative estimates of odor concentration mean for low (_top_) and high (_bottom_) odors after the onset of the low odors for one-to-one, naively distributed, and geometry-aware codes. Black lines indicate baseline estimates of the posterior mean. Thick lines indicate means over odors, and thin lines individual odors. **C**. As in **B**, but for the estimated variance. **D**. As in **B**, but after the onset of high odors. **E**. As in **C**, but after the onset of high odors. See Appendix G for details of our numerical methods.