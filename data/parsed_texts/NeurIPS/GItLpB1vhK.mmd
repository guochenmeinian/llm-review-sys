# Estimating Koopman operators with sketching to provably learn large scale dynamical systems

 Giacomo Meanti

Istituto Italiano di Tecnologia

giacomo.meanti@iit.it

&Antoine Chatalic

Universita di Genova

antoine.chatalic@dibris.unige.it

&Vladimir R. Kostic

Istituto Italiano di Tecnologia

University of Novi Sad

vladimir.kostic@iit.it

&Pietro Novelli

Istituto Italiano di Tecnologia

pietro.novelli@iit.it

&Massimiliano Pontil

Istituto Italiano di Tecnologia

University College London

massimiliano.pontil@iit.it

&Lorenzo Rosasco

Universita di Genova

Istituto Italiano di Tecnologia

Massachusetts Institute of Technology

lrosasco@mit.edu

Equal contribution

###### Abstract

The theory of Koopman operators allows to deploy non-parametric machine learning algorithms to predict and analyze complex dynamical systems. Estimators such as principal component regression (PCR) or reduced rank regression (RRR) in kernel spaces can be shown to provably learn Koopman operators from finite empirical observations of the system's time evolution. Scaling these approaches to very long trajectories is a challenge and requires introducing suitable approximations to make computations feasible. In this paper, we boost the efficiency of different kernel-based Koopman operator estimators using random projections (sketching). We derive, implement and test the new "sketched" estimators with extensive experiments on synthetic and large-scale molecular dynamics datasets. Further, we establish non asymptotic error bounds giving a sharp characterization of the trade-offs between statistical learning rates and computational efficiency. Our empirical and theoretical analysis shows that the proposed estimators provide a sound and efficient way to learn large scale dynamical systems. In particular our experiments indicate that the proposed estimators retain the same accuracy of PCR or RRR, while being much faster. Code is available at https://github.com/Giodiro/NystromKoopman.

## 1 Introduction

In the physical world, temporally varying phenomena are everywhere, from biological processes in the cell to fluid dynamics to electrical fields. Correspondingly, they generate large amounts of data both through experiments and simulations. This data is often analyzed in the framework of dynamical systems, where the state of a system \(\bm{x}\) is observed at a certain time \(t\), and the dynamics is described by a function \(f\) which captures its evolution in time

\[\bm{x}_{t+1}=f(\bm{x}_{t}).\]The function \(f\) must capture the whole dynamics, and as such it may be non-linear and even stochastic for instance when modeling stochastic differential equations, or simply noisy processes. Applications of this general formulation arise in fields ranging from robotics, atomistic simulations, epidemiology, and many more. Along with a recent increase in the availability of simulated data, data-driven techniques for learning the dynamics underlying physical systems have become commonplace. The typical approach of such techniques is to acquire a dataset of training pairs \((\bm{x}_{t},\bm{y}_{t}=\bm{x}_{t+1})\) sampled in time, and use them to learn a model for \(f\) which minimizes a forecasting error. Since dynamical systems stem from real physical processes, forecasting is not the only goal and the ability to interpret the dynamics is paramount. One particularly important dimension for interpretation is the separation of dynamics into multiple temporal scales: fast fluctuations can e.g. be due to thermodynamical noise or electrical components in the system, while slow dynamics describe important conformational changes in molecules or mechanical effects.

Koopman operator theory [27, 28] provides an elegant framework in which the potentially non-linear dynamics of the system can be studied via the Koopman operator

\[(\mathcal{K}\psi)(\bm{x})=\mathbf{E}\big{[}\psi(f(\bm{x}))\big{]},\] (1)

which has the main advantage of being linear but is defined on a typically infinite-dimensional set of observable functions. The expectation in (1) is taken with respect to the potential stochasticity of \(f\). Thanks to its linearity, the operator \(\mathcal{K}\) can e.g. be applied twice to get two-steps-ahead forecasts, and one can compute its spectrum (beware however that \(\mathcal{K}\) is not self-adjoint, unless the dynamical process is time-reversible). Accurately approximating the Koopman operator and its spectral properties is of high interest for the practical analysis of dynamical systems. However doing so efficiently for long temporal trajectories remains challenging. In this paper we are interested in designing estimators which are both theoretically accurate and computationally efficient.

Related worksLearning the spectral properties of the Koopman operator directly from data has been considered for at least three decades [39, 40], resulting in a large body of previous work. Among the different approaches proposed over time (see Mezic [41] for a recent review) it is most common to search for finite dimensional approximations to the operator, from which part of the spectrum and the Koopman modes [8] can be obtained. Dynamic mode decomposition (DMD) [56, 63], time-lagged independent component analysis (tICA) [42, 49] and many subsequent extensions [31] for example can be seen as minimizers of the forecasting error when \(\psi\) is restricted to be a linear function of the states [52]. Extended DMD (eDMD) [66, 25] and the variational approach for conformation dynamics (VAC) [45, 46] instead allow for a (potentially learnable, as in recent deep learning algorithms [32, 37, 69, 62]) dictionary of non-linear functions \(\psi\). Kernel DMD [67, 26] and kernel tICA [57] are further generalizations which again approximate the Koopman operator but using an infinite dimensional space of features \(\psi\), encoded by the feature map of a reproducing kernel. While often slow from a computational point of view, kernel methods are highly expressive and can be analyzed theoretically, to prove convergence and derive learning rates of the resulting estimators [29, 30]. Approximate kernel methods which are much faster to run have been recently used for Koopman operator learning by Baddoo et al. [6] where an iterative procedure is used to identify the best approximation to the full kernel, but no formal learning rates are demonstrated, and by Ahmad et al. [3] who derive learning rates in Hilbert-Schmidt norm (while we consider operator norm) for the Nystrom KRR estimator (one of the three considered in this paper).

ContributionsIn this paper we adopt the kernel learning approach. Starting from the problem of approximating the Koopman operator in a reproducing kernel Hilbert space, we derive three different estimators based on different inductive biases: kernel ridge regression (KRR) which comes from Tikhonov regularization, principal component regression (PCR) which is equivalent to DMD and its extensions, and reduced rank regression (RRR) which comes from a constraint on the maximum rank of the estimator [24]. We show how to overcome the computational scalability problems inherent in full kernel methods using an approximation based on random projections which is known as the Nystrom method [58, 65]. The approximate learning algorithms scale very easily to the largest datasets, with a computational complexity which goes from \(O(n^{3})\) for the exact algorithm to \(O(n^{2})\) for the approximate one. We can further show that the Nystrom KRR, PCR and RRR estimators have the same convergence rates as their exact, slow counterparts - which are known to be optimal under our assumptions. We provide learning bounds in operator norm, which are known to translate to bounds for dynamic mode decomposition and are thus of paramount importance for applications. Finally, we thoroughly validate the approximate PCR and RRR estimators on synthetic dynamical systems, comparing efficiency and accuracy against their exact counterparts [29], as well as recently proposed fast Koopman estimator streaming KAF [20]. To showcase a realistic scenario, we train on a molecular dynamics simulation of the fast-folding Trp-cage protein [35].

Structure of the paperWe introduce the setting in Section 2, and define our three estimators in Section 3. In Section 4 we provide bounds on the excess risk of our estimators, and extensive experiments on synthetic as well as large-scale molecular dynamics datasets in Section 5.

## 2 Background and related work

NotationWe consider a measurable space \((\mathcal{X},\mathcal{B})\) where \(\mathcal{X}\) corresponds to the state space, and denote \(L^{2}_{\pi}:=L^{2}(\mathcal{X},\mathcal{B},\pi)\) the \(L^{2}\) space of functions on \(\mathcal{X}\) w.r.t. to a probability measure \(\pi\), and \(L^{\infty}_{\pi}\) the space of measurable functions bounded almost everywhere. We denote \(\operatorname{HS}(\mathcal{H})\) the space of Hilbert-Schmidt operators on a space \(\mathcal{H}\).

SettingThe setting we will consider is that of Markovian, time-homogeneous stochastic process \(\{X_{t}\}_{t\in\mathbb{N}}\) on \(\mathcal{X}\). By definition of a Markov process, \(X_{t}\) only depends on \(X_{t-1}\) and not on any previous states. Time-homogeneity ensures that the transition probability \(\mathbb{P}\big{[}X_{t+1}\in B|X_{t}=\boldsymbol{x}\big{]}\) for any measurable set \(B\) does not depend on \(t\), and can be denoted with \(p(\boldsymbol{x},B)\). This implies in particular that the distribution of \((X_{t},X_{t+1})\) does not depend on \(t\), and we denote it \(\rho\) in the following. We further assume the existence of the _invariant_ density \(\pi\) which satisfies \(\pi(B)=\int_{\mathcal{X}}\pi(x)p(\boldsymbol{x},B)\,\mathrm{d}\boldsymbol{x}\). This classical assumption allows one to study a large class of stochastic dynamical systems, but also deterministic systems on the attractor, see e.g. [14]. The Koopman operator \(\mathcal{K}_{\pi}:L^{2}_{\pi}(\mathcal{X})\to L^{2}_{\pi}(\mathcal{X})\) is a bounded linear operator, defined by

\[(\mathcal{K}_{\pi}g)(\boldsymbol{x})=\int_{\mathcal{X}}p(\boldsymbol{x}, \boldsymbol{y})g(\boldsymbol{y})\,\mathrm{d}\boldsymbol{y}=\mathbf{E}\big{[} g(X_{t+1})|X_{t}=\boldsymbol{x}\big{]},\quad g\in L^{2}_{\pi}(\mathcal{X}), \boldsymbol{x}\in\mathcal{X}.\] (2)

We are in particular interested in the eigenpairs \((\lambda_{i},\varphi_{i})\in\mathbb{C}\times L^{2}_{\pi}\), that satisfy

\[\mathcal{K}_{\pi}\varphi_{i}=\lambda_{i}\varphi_{i}.\] (3)

Through this decomposition it is possible to interpret the system by separating fast and slow processes, or projecting the states onto fewer dimensions [15; 19; 7]. In particular, the Koopman mode decomposition (KMD) allows to propagate the system state in time. Given an observable \(g:\mathcal{X}\to\mathbb{R}^{d}\) such that \(g\in\mathrm{span}\{\varphi_{i}|i\in\mathbb{N}\}\), the modes allow to reconstruct \(g(\boldsymbol{x})\) with a Koopman eigenfunction basis. The modes \(\boldsymbol{\eta}^{g}_{i}\in\mathbb{C}^{d}\) are the coefficients of this basis expansion:

\[(\mathcal{K}_{\pi}g)(\boldsymbol{x})=\mathbf{E}\big{[}g(X_{t})|X_{0}= \boldsymbol{x}\big{]}=\sum_{i}\lambda_{i}\varphi_{i}(\boldsymbol{x}) \boldsymbol{\eta}^{g}_{i}.\] (4)

This decomposition describes the system's dynamics in terms of a stationary component (the Koopman modes), a temporal component (the eigenvalues \(\lambda_{i}\)) and a spatial component (eigenfunctions \(\varphi_{i}\)).

Kernel-based learningIn this paper we approximate \(\mathcal{K}_{\pi}\) with kernel-based algorithms, using operators in reproducing kernel Hilbert spaces (RKHS) \(\mathcal{H}\) associated with kernel \(k:\mathcal{X}\times\mathcal{X}\to\mathbb{R}\) and feature map \(\phi:\mathcal{X}\to\mathcal{H}\). We wish to find an operator \(A:\mathcal{H}\to\mathcal{H}\) which minimizes the risk

\[\mathcal{R}_{\text{HS}}(A)=\mathbf{E}_{\rho}\big{[}\ell(A,(\boldsymbol{x}, \boldsymbol{y}))\big{]}\quad\text{ where }\quad\ell(A,(\boldsymbol{x},\boldsymbol{y})):=\|\phi( \boldsymbol{y})-A\phi(\boldsymbol{x})\|^{2}.\] (5)

The adjoint of \(A\), denoted by \(A^{*}\), should thus be understood as an estimator of the Koopman operator \(\mathcal{K}_{\pi}\) in \(\mathcal{H}\) as will be clarified in (15). In practice \(\pi\) and \(\rho\) are unknown, and one typically has access to a dataset \(\{(\boldsymbol{x}_{i},\boldsymbol{y}_{i})\}_{i=1}^{n}\) sampled from \(\rho\), where each pair \((\boldsymbol{x}_{i},\boldsymbol{y}_{i}=f(\boldsymbol{x}_{i}))\) may equivalently come from a single long trajectory or multiple shorter ones concatenated together. We thus use the empirical risk

\[\hat{\mathcal{R}}_{\text{HS}}(A)=\frac{1}{n}\sum_{i=1}^{n}\ell(A,(\boldsymbol{ x}_{i},\boldsymbol{y}_{i}))\] (6)

as a proxy for (5). Since minimizing eq. (6) may require finding the solution to a very badly conditioned linear system, different regularization methods (such as Tikhonov or truncated SVD) can be applied on top of the empirical risk.

**Remark 2.1** (Connections to other learning problems): _The problem of minimizing eqs. (5) and (6) has strong connections to learning conditional mean embeddings [59; 22; 44; 33] where the predictors and targets are embedded in different RKHSs, and to structured prediction [12; 13] which is an even more general framework. On the other hand, the most substantial difference from the usual kernel regression setting [9] is the embedding of both targets and predictors into a RKHS, instead of just targets._

We denote the input and cross covariance \(C=\mathbf{E}_{\pi}[\phi(\bm{x})\otimes\phi(\bm{x})]\) and \(C_{YX}=\mathbf{E}_{\rho}[\phi(\bm{y})\otimes\phi(\bm{x})]\), and their empirical counterparts as \(\hat{C}=\frac{1}{m}\sum_{i=1}^{n}[\phi(\bm{x}_{i})\otimes\phi(\bm{x}_{i})]\) and \(\hat{C}_{YX}=\frac{1}{n}\sum_{i=1}^{n}\phi(\bm{y}_{i})\otimes\phi(\bm{x}_{i})]\). We also use the abbreviation \(C_{\lambda}:=C\mp\lambda I\). Minimizing the empirical risk (6) with Tikhonov regularization [9] yields the following KRR estimator

\[\hat{A}_{\lambda}=\operatorname*{arg\,min}_{A\in\text{HS}(\mathcal{H})} \hat{\mathcal{R}}_{\text{HS}}(A)+\lambda\|A\|_{\text{HS}}^{2}=\hat{C}_{YX}( \hat{C}+\lambda I)^{-1}.\] (7)

Eq. (7) can be computed by transforming its expression with the kernel trick [23], to arrive at a form where one must invert the kernel matrix - a \(n\times n\) matrix whose \(i,j\)-th entry is \(k(\bm{x}_{i},\bm{x}_{j})\). This operation requires \(O(n^{3})\) time and \(O(n^{2})\) memory, severely limiting the scalability of KRR to \(n\lesssim 100\,000\) points. Improving the scalability of kernel methods is a well-researched topic, with the most important solutions being random features [50; 51; 68; 21] and random projections [58; 65; 21]. In this paper we use the latter approach, whereby the kernel matrix is assumed to be approximately low-rank and is _sketched_ to a lower dimensionality. In particular we will use the Nystrom method to approximate the kernel matrix projecting it onto a small set of inducing points, chosen among the training set. The sketched estimators are much more efficient than the exact ones, increasingly so as the training trajectories become longer. For example, the state of the art complexity for solving (non vector valued) approximate kernel ridge regression is \(O(n\sqrt{n})\) time instead of \(O(n^{3})\)[38; 1; 10]. Furthermore, when enough inducing points are used (typically on the order of \(\sqrt{n}\)), the learning rates of the exact and approximate estimators are the same, and optimal [5; 53]. Hence it is possible - and in this paper we show it for learning the Koopman operator - to obtain large efficiency gains, without losing anything in terms of theoretical guarantees of convergence.

## 3 Nystrom estimators for Koopman operator regression

In this section, we introduce three efficient approximations of the KRR, PCR and RRR estimators of the Koopman operator. Our estimators rely on the Nystrom approximation, i.e. on random projections onto low-dimensional subspaces of \(\mathcal{H}\) spanned by the feature-embeddings of subsets of the data. We thus consider two sets of \(m\ll n\) inducing points \(\{\tilde{\bm{x}}_{j}\}_{j=1}^{m}\subset\{\bm{x}_{t}\}_{t=1}^{n}\) and \(\{\tilde{\bm{y}}_{j}\}_{j=1}^{m}\subset\{\bm{y}_{t}\}_{t=1}^{n}\) sampled respectively from the input and output data. The choice of these inducing points (also sometimes called Nystrom centers) is important to obtain a good approximation. Common choices include uniform sampling, leverage score sampling [17; 55], and iterative procedures such as the one used in [6] to identify the most relevant centers. In this paper we focus on uniform sampling for simplicity, but we stress that our theoretical results in Section 4 can easily be extended to leverage scores sampling by means of [53; Lemma 7]. To formalize the Nystrom estimators, we define operators \(\widetilde{\Phi}_{X},\widetilde{\Phi}_{Y}:\mathbb{R}^{m}\to\mathcal{H}\) as \(\widetilde{\Phi}_{X}w=\sum_{j=1}^{m}w_{j}\phi(\tilde{\bm{x}}_{j})\) and \(\widetilde{\Phi}_{Y}w=\sum_{j=1}^{m}w_{j}\phi(\tilde{\bm{y}}_{j})\), and denote \(P_{X}\) and \(P_{Y}\) the orthogonal projections onto \(\operatorname*{span}\widetilde{\Phi}_{X}\) and \(\operatorname*{span}\widetilde{\Phi}_{Y}\) respectively.

In the following paragraphs we apply the projection operators to three estimators corresponding to different choices of regularization. For each of them a specific proposition (proven in Section C) states an efficient way of computing it based on the kernel trick. For this purpose we introduce the kernel matrices \(K_{\widehat{X},X},K_{\widehat{Y},Y}\in\mathbb{R}^{m\times n}\) between training set and inducing points with entries \((K_{\widehat{X},X})_{ji}=k(\tilde{\bm{x}}_{j},\bm{x}_{i})\), \((K_{\widehat{Y},Y})_{ji}=k(\tilde{\bm{y}}_{j},\bm{y}_{i})\), and the kernel matrices of the inducing points \(K_{\widehat{X},\widehat{X}},K_{\widehat{Y},\widehat{Y}}\in\mathbb{R}^{m\times m}\) with entries \((K_{\widehat{X},X})_{jk}=k(\tilde{\bm{x}}_{j},\tilde{\bm{x}}_{k})\) and \((K_{\widehat{X},X})_{jk}=k(\tilde{\bm{y}}_{j},\tilde{\bm{y}}_{k})\).

Kernel Ridge Regression (KRR)The cost of computing \(\hat{A}_{\lambda}\) defined in Eq. (7) is \(O(n^{3})\)[29] which is prohibitive for datasets containing long trajectories. However, applying the projection operators to each side of the empirical covariance operators, we obtain an estimator which additionally depends on the \(m\) inducing points:

\[\hat{A}_{m,\lambda}^{\text{KRR}}:=P_{Y}\hat{C}_{YX}P_{X}(P_{X}\hat{C}P_{X}+ \lambda I)^{-1}:\mathcal{H}\to\mathcal{H}.\] (8)If \(\mathcal{H}\) is infinite dimensional, Eq. (8) cannot be computed directly. Proposition 3.1 (proven in Section C) provides a computable version of the estimator.

**Proposition 3.1** (Nystrom KRR): _The Nystrom KRR estimator (8) can be expressed as_

\[\hat{A}^{\text{KRR}}_{m,\lambda}=\widetilde{\Phi}_{Y}K^{\dagger}_{\hat{Y},\hat{ Y}}K_{\hat{Y},Y}K_{X,\hat{X}}(K_{\hat{X},X}K_{X,\hat{X}}+n\lambda K_{\hat{X}, \hat{X}})^{\dagger}\widetilde{\Phi}^{*}_{X}.\] (9)

_The computational bottlenecks are the inversion of an \(m\times m\) matrix and a large matrix multiplication, which overall need \(O(2m^{3}+2m^{2}n)\) operations. In particular, in Section 4 we will show that \(m\asymp\sqrt{n}\) is sufficient to guarantee optimal rates even with minimal assumptions, leading to a final cost of \(O(n^{2})\). Note that a similar estimator was derived in [3]._

Please note that the \(O(n^{2})\) cost is for a straightforward implementation, and can indeed be reduced via iterative linear solvers (possibly preconditioned, to further reduce the practical running time), and randomized linear algebra techniques. In particular, we could leverage results from Rudi et al. [54] to reduce the computational cost to \(O(n\sqrt{n})\).

Principal Component Regression (PCR)Typical settings in which Koopman operator theory is used focus on the decomposition of a dynamical system into a small set of components, obtained from the eigendecomposition of the operator itself. For this reason, a good prior on the Koopman estimator is for it to be low rank. The kernel PCR estimator \(\hat{A}^{\text{PCR}}=\hat{C}_{YX}[\hat{C}]^{\dagger}_{r}\) formalizes this concept [29; 67], where here \([\![\cdot]\!]_{r}\) denotes the truncation to the first \(r\) components of the spectrum. Again this is expensive to compute when \(n\) is large, but the estimator can be sketched as follows:

\[\hat{A}^{\text{PCR}}_{m}=P_{Y}\hat{C}_{YX}[\![P_{X}\hat{C}P_{X}]\!]^{\dagger}_{r}.\] (10)

The next proposition provides an efficiently implementable version of this estimator.

**Proposition 3.2** (Nystrom PCR): _The sketched PCR estimator (10) satisfies_

\[\hat{A}^{\text{PCR}}_{m}=\widetilde{\Phi}_{Y}K^{\dagger}_{\hat{Y},\hat{Y}}K_{ \hat{Y},Y}K_{X,\hat{X}}[\![K^{\dagger}_{\hat{X},\hat{X}}K_{\hat{X},X}K_{X,\hat {X}}]\!]_{r}\widetilde{\Phi}^{*}_{X}\] (11)

_requiring \(O(2m^{3}+2m^{2}n)\) operations, i.e. optimal rates can again be obtained at a cost of at most \(O(n^{2})\) operations._

Note that with \(m=n\), \(\hat{A}^{\text{PCR}}_{m}\) is equivalent to the kernel DMD estimator [67], also known as kernel analog forecasting (KAF) [4]. The sketched estimator of Proposition 3.2 was also recently derived in [6], albeit without providing theoretical guarantees.

Reduced Rank Regression (RRR)Another way to promote low-rank estimators is to add an explicit rank constraint when minimizing the empirical risk. Combining such a constraint with Tikhonov regularization corresponds to the reduced rank regression [24; 29] estimator:

\[A^{\text{RRR}}_{\lambda}=\operatorname*{arg\,min}_{A\in\text{HS:rk}(A)\leq r} \hat{\mathcal{R}}_{\text{HS}}(A)+\lambda\|A\|^{2}_{\text{HS}}.\] (12)

Minimizing Eq. (12) requires solving a \(n\times n\) generalized eigenvalue problem. The following proposition introduces the sketched version of this estimator, along with a procedure to compute it which instead requires the solution of a \(m\times m\) eigenvalue problem. For \(m\asymp\sqrt{n}\), which is enough to guarantee optimal learning rates with minimal assumptions (see Section 4), this represents a reduction from \(O(n^{3})\) to \(O(n\sqrt{n})\) time.

**Proposition 3.3** (Nystrom RRR): _The Nystrom RRR estimator can be written as_

\[\hat{A}^{\text{RRR}}_{m,\lambda}=[\![P_{Y}\hat{C}_{YX}P_{X}(P_{X}\hat{C}P_{X}+ \lambda I)^{-1/2}]\!]_{r}(P_{X}\hat{C}P_{X}+\lambda I)^{-1/2}.\] (13)

_To compute it, solve the \(m\times m\) eigenvalue problem_

\[(K_{\hat{X},X}K_{X,\hat{X}}+n\lambda K_{\hat{X},\hat{X}})^{\dagger}K_{\hat{X}, X}K_{Y,\hat{Y}}K^{\dagger}_{\hat{Y},\hat{Y}}K_{\hat{Y},Y}K_{X,\hat{X}}w_{i}= \sigma_{i}^{2}w_{i}\]

_for the first \(r\) eigenvectors \(W_{r}=[w_{1},\ldots,w_{r}]\), appropriately normalized. Then denoting \(D_{r}:=K^{\dagger}_{\hat{Y},\hat{Y}}K_{\hat{Y},Y}K_{X,\hat{X}}W_{r}\) and \(E_{r}:=(K_{\hat{X},X}K_{X,\hat{X}}+n\lambda K_{\hat{X},\hat{X}})^{\dagger}K_{ \hat{X},X}K_{Y,\hat{Y}}D_{r}\) it holds_

\[\hat{A}^{\text{RRR}}_{m,\lambda}=\widetilde{\Phi}_{Y}D_{r}E_{r}^{*}\widetilde{ \Phi}^{*}_{X}.\] (14)Learning bounds in operator norm for the sketched estimators

In this section, we state the main theoretical results showing that optimal rates for operator learning with KRR, PCR and RRR can be reached with Nystrom estimators.

AssumptionsWe first make two assumptions on the space \(\mathcal{H}\) used for the approximation, via its reproducing kernel \(k\).

**Assumption 4.1** (Bounded kernel): _There exists \(K<\infty\) such that \(\operatorname{ess}\sup_{\bm{x}\sim\pi}\lVert\phi(\bm{x})\rVert\leq K\)._

Assumption 4.1 ensures that \(\mathcal{H}\) is compactly embedded in \(L^{2}_{\pi}\)[61, Lemma 2.3], and we denote \(\Phi^{*}_{X}:\mathcal{H}\to L^{2}_{\pi}\) the embedding operator which maps any function in \(\mathcal{H}\) to its equivalence class \(\pi\)-almost everywhere in \(L^{2}_{\pi}\).

**Assumption 4.2** (Universal kernel): _The kernel \(k\) is universal, i.e. \(\operatorname{cl}(\operatorname{ran}(\Phi^{*}_{X}))=L^{2}_{\pi}\)._

We refer the reader to [60, Definition 4.52] for a definition of a universal kernel. The third assumption on the RKHS is related to the embedding property from Fischer and Steinwart [18], connected to the embedding of interpolation spaces. For a detailed discussion see Section A.3.

**Assumption 4.3** (Embedding property): _There exists \(\tau\in\left]0,1\right]\) and \(c_{\tau}>0\) such that \(\operatorname{ess}\sup_{\bm{x}\sim\pi}\lVert C^{-1/2}_{\lambda}\phi(\bm{x}) \rVert^{2}\leq c_{\tau}\lambda^{-\tau}\)._

Next, we make an assumption on the decay of the spectrum of the covariance operator that is of paramount importance for derivation of optimal learning bounds. In the following, \(\lambda_{i}(A)\) and \(\sigma_{i}(A)\) always denote the eigenvalues and singular values of an operator \(A\) (in decreasing order).

**Assumption 4.4** (Spectral decay): _There exists \(\beta\in\left]0,\tau\right]\) and \(c>0\) such that \(\lambda_{i}(C)\leq ci^{-1/\beta}\)._

This assumption is common in the literature, and we will see that the optimal learning rates depend on \(\beta\). It implies the bound \(d_{\text{eff}}(\lambda):=\operatorname{tr}(C^{-1}_{\lambda}C)\lesssim\lambda^{-\beta}\) on the effective dimension, which is a key quantity in the analysis (both statements are actually equivalent, see Section E.2). Note that \(d_{\text{eff}}(\lambda)=\operatorname{\mathbf{E}}_{\bm{x}\sim\pi}\lVert C^{- 1/2}_{\lambda}\phi(\bm{x})\rVert\leq\operatorname{ess}\sup_{\bm{x}\sim\pi} \lVert C^{-1/2}_{\lambda}\phi(\bm{x})\rVert\), and thus it necessarily holds \(\beta\leq\tau\). For a Gaussian kernel, both \(\beta\) and \(\tau\) can be chosen arbitrarily close to zero.

Finally, we make an assumption about the regularity of the problem itself. A common assumption occurring in the literature is that \(\operatorname{\mathbf{E}}[f(X_{1})\,|\,X_{0}=\cdot]\in\mathcal{H}\) for every \(f\in\mathcal{H}\), meaning that one can define the Koopman operator directly on the space \(\mathcal{H}\), i.e. the learning problem is _well-specified_. However, this assumption is often too strong. Following [30, D.1] we make a different assumption on the cross-covariance remarking that, irrespectively of the choice of RKHS, it holds true whenever the Koopman operator is self-adjoint (i.e. the dynamics is time-reversible).

**Assumption 4.5** (Regularity of \(\mathcal{K}_{\pi}\)): _There exists \(a>0\) such that \(C_{XY}C^{*}_{XY}\preccurlyeq a^{2}C^{2}\)._

RatesThe risk can be decomposed as \(\mathcal{R}_{\text{HS}}(A)=\mathcal{E}_{\text{HS}}(A)+\mathcal{R}_{\text{HS},0}\) where \(\mathcal{R}_{\text{HS},0}\) is a constant and \(\mathcal{E}_{\text{HS}}(A):=\lVert\mathcal{K}_{\pi}\Phi^{*}_{X}-\Phi^{*}_{X}A^ {*}\rVert_{\text{HS}}^{2}\) corresponds to the excess risk (more details in Section B). Optimal learning bounds for the KRR estimator in the context of CME (i.e. in Hilbert-Schmidt norm) have been developed in [33] under Assumptions 4.1 to 4.4 in well-specified and misspecified settings. On the other hand, in the context of dynamical systems, Kostic et al. [29, Theorem 1] report the importance of _reduced rank estimators_ that have a small excess risk in operator norm

\[\mathcal{E}(A):=\lVert\mathcal{K}_{\pi}\Phi^{*}_{X}-\Phi^{*}_{X}A^{*}\rVert_{ \mathcal{H}\to L^{2}_{\pi}}^{2}.\] (15)

The rationale behind considering the operator norm is that it allows to control the error of the eigenvalues approximation and thus of the KMD (3), (4) as discussed below. Optimal learning bounds in operator norm for KRR, PCR and RRR are established in [30]. In this work we show that the same optimal rates remain valid for the _Nystrom_ KRR, PCR and RRR estimators. According to [29] and [30] these operator norm bounds lead to reliable approximation of the Koopman mode decomposition of Eq. (4).

We now provide our main result.

**Theorem 4.6** (Operator norm error for KRR, i.i.d. data): _Let assumptions 4.1 to 4.5 hold. Let \((\bm{x}_{i},\bm{y}_{i})_{1\leq i\leq n}\) be i.i.d. samples, and let \(P_{Y}=P_{X}\) be the projection induced by \(m\) Nystrom landmarks drawn uniformly from \((\bm{x}_{i})_{1\leq i\leq n}\) without replacement. Let \(\lambda=c_{\lambda}n^{-1/(1+\beta)}\) where \(c_{\lambda}\) is a constant given in the proof, and assume \(n\geq(c_{\lambda}/K^{2})^{1+\beta}\). Then it holds with probability at least \(1-\delta\)_

\[\mathcal{E}(\hat{A}_{m,\lambda}^{\text{RRR}})^{1/2}\lesssim n^{-\frac{1}{2(1+ \beta)}}\qquad\text{provided}\qquad m\gtrsim\max(1,n^{\tau/(1+\beta)})\log( \nicefrac{{n}}{{\delta}}).\]

The proof is provided in Section E.2, but essentially relies on a decomposition involving the terms \(\|C_{\lambda}^{-1/2}(C_{YX}-\hat{C}_{YX})\|\), \(\|C_{\lambda}^{-1/2}(C-\hat{C})\|\), \(\|C_{\lambda}^{-1/2}(C-\hat{C})C_{\lambda}^{-1/2}\|\), as well as bounding the quantity \(\|P_{X}^{\perp}C^{1/2}\|\) where \(P_{X}^{\perp}\) denotes the projection on the orthogonal of \(\operatorname{ran}(P_{X})\). All these terms are bounded using two variants of the Bernstein inequality. Note that our results can easily be extended to leverage score sampling of the landmarks by bounding term \(\|P_{X}^{\perp}C^{1/2}\|\) by means of [53, Lemma 7]; the same rate could then be obtained using a smaller number \(m\) of Nystrom points.

The rate \(n^{-1/(2(1+\beta))}\) is known to be optimal (up to the log factor) in this setting by assuming an additional lower bound on the decay of the covariance's eigenvalues of the kind \(\lambda_{i}(C)\gtrsim i^{-1/\beta}\), see [30, Theorem 7 in D.4]. One can see that without particular assumptions (\(\beta=\tau=1\)), we only need the number \(m\) of inducing points to be of the order of \(\Omega(\sqrt{n})\) in order to get an optimal rates. For \(\tau\) fixed, this number increases when \(\beta\) decreases (faster decay of the covariance's spectrum), however note that the optimal rate depends on \(\beta\) and also improves in this case. The dependence in \(\tau\) is particularly interesting, as for instance with a Gaussian kernel it is known that \(\tau\) can be chosen arbitrarily closed to zero [33, 18]. In that case, the number \(m\) of inducing points can be taken on the order of \(\Omega(\log n)\).

Note that a bound for the Nystrom KRR estimator has been derived in Hilbert-Schmidt norm by Ahmad et al. [3]. Using the operator norm however allows to derive bounds on the eigenvalues (see discussion below), which is of paramount importance for practical applications. Moreover, we now provide a bound on the error of PCR and RRR estimators, which are not covered in [3].

**Lemma 4.7** (**Operator norm error for PCR and RRR, i.i.d. data)**: _Under the assumptions of Theorem 4.6, taking \(\lambda=c_{\lambda}n^{-1/(1+\beta)}\) with \(c_{\lambda}\) as in Theorem 4.6, \(n\geq(c_{\lambda}/K^{2})^{1+\beta}\), and provided_

\[m\gtrsim\max(1,n^{\tau/(1+\beta)})\log(\nicefrac{{n}}{{\delta}}),\]

_it holds with probability at least \(1-\delta\)_

\[\mathcal{E}(\hat{A}_{m,\lambda}^{\text{RRR}})^{1/2}\lesssim c_{ \mathrm{RRR}}\,n^{-\frac{1}{2(1+\beta)}},\ \text{ for }r\text{ s.t. }\sigma_{r+1}(\Phi_{X}\mathcal{K}_{\pi}^{*})<\min(\sigma_{r}(\Phi_{X} \mathcal{K}_{\pi}^{*}),n^{-\frac{1}{2(1+\beta)}})\] \[\text{ and }\ \ \mathcal{E}(\hat{A}_{m}^{\text{PCR}})^{1/2}\lesssim c_{ \mathrm{PCR}}\,n^{-\frac{1}{2(1+\beta)}},\ \text{ for }r>n^{\frac{1}{\beta(1+\beta)}},\]

_where \(c_{\mathrm{RRR}}=(\sigma_{r}^{2}(\Phi_{X}\mathcal{K}_{\pi}^{*})-\sigma_{r+1} ^{2}(\Phi_{X}\mathcal{K}_{\pi}^{*}))^{-1}\) and \(c_{\mathrm{PCR}}=(\sigma_{r}(\Phi_{X})-\sigma_{r+1}(\Phi_{X}))^{-1}\) are the problem dependant constants._

Note that when rank of \(\mathcal{K}_{\pi}\) is \(r\), then there is no restriction on \(r\) for the RRR estimator, while for PCR the choice of \(r\) depends on the spectral decay property of the kernel. In general, if \(r>n^{\frac{1}{\beta(1+\beta)}}\), then \(\sigma_{r+1}(\Phi_{X}\mathcal{K}_{\pi}^{*})\leq\sigma_{r+1}(\Phi_{X})\lesssim n ^{-1/(2(1+\beta))},\) which implies that RRR estimator can achieve the same rate of PCR but with smaller rank. Again the rate is sharp (up to the log factor) in this setting [30].

Koopman mode decompositionAccording to [29, Theorem 1], working in operator norm allows us to bound the error of our estimators for dynamic mode decomposition, as well as to quantify how close the eigenpairs \((\hat{\lambda}_{i},\hat{\varphi}_{i})\) of an estimator \(\hat{A}^{*}\) are to being eigenpairs of the Koopman operator. Namely, recalling that for function \(\hat{\varphi}_{i}\), the corresponding candidate for Koopman eigenfunction in \(L_{\pi}^{2}\) space is \(\Phi_{X}^{*}\hat{\varphi}_{i}\), one has \(\|\mathcal{K}_{\pi}(\Phi_{X}^{*}\hat{\varphi}_{i})-\hat{\lambda}_{i}(\Phi_{X}^{* }\hat{\varphi}_{i})\|/\|\Phi_{X}^{*}\hat{\varphi}_{i}\|\leq\mathcal{E}(\hat{A} )^{1/2}\|\hat{\varphi}_{i}\|/\|\Phi_{X}^{*}\hat{\varphi}_{i}\|\). While eigenvalue and eigenfunction learning rates were studied, under additional assumptions, in [30], where the operator norm error rates were determinant, here, in Section 5, we empirically show that the proposed estimators accurately learn the Koopman spectrum. We refer the reader to Section D for the details on computation of eigenvalues, eigenfunctions and KMD of an estimator in practice.

Dealing with non-i.i.d. dataThe previous results hold for i.i.d. data, which is not a very realistic assumption when learning from sampled trajectories. Our results can however easily be extended to \(\beta\)-mixing processes by considering random variables \(Z_{i}=\sum_{j=1}^{k}X_{i+j}\) (thus representing portions of the trajectory) sufficiently separated in time to be nearly independent. We now consider a trajectory \(\bm{x}_{1},\ldots,\bm{x}_{n+1}\) with \(\bm{x}_{1}\sim\pi\) and \(\bm{x}_{t+1}\sim p(\bm{x}_{t},\cdot)\) for \(t\in[1,n]\), and use Lemma J.8 (re-stated from [29]) which allows to translate concentration results on the \(Z_{i}\) to concentration on the \(X_{i}\) by means of the \(\beta\)-mixing coefficients defined as \(\beta_{X}(k):=\sup_{B\in\mathcal{B}\otimes\mathcal{B}}\bigl{|}\rho_{k}(B)-(\pi \times\pi)(B)\bigr{|}\) where \(\rho_{k}\) denotes the joint probability of \((X_{t},X_{t+k})\). Using this result the concentration results provided in appendix can thus be generalized to the \(\beta\)-mixing setting, and apart from logarithmic dependencies we essentially obtain similar results to the i.i.d. setting except that the sample size \(n\) is replaced by \(p\approx n/(2k)\).

## 5 Experimental validation

In this section we show how the estimators proposed in section 3 perform in various scenarios, ranging from synthetic low dimensional ODEs to large-scale molecular dynamics simulations. The code for reproducing all experiments is available online. Our initial aim is to demonstrate the speed of NysPCR and NysRRR, compared to the recently proposed alternative Streaming KAF (sKAF) [20]. Then we show that their favorable scaling properties make it possible to train on large molecular dynamics datasets without any subsampling. In particular we run a metastability analysis of the alanine dipeptide and the Trp-cage protein, showcasing the accuracy of our models' eigenvalue and eigenfunction estimates, as well as their efficiency on massive datasets (\(>500\,000\) points)

**Efficiency Benchmarks on Lorenz '63** The chaotic Lorenz '63 system [36] consists of 3 ODEs with no measurement noise. With this toy dynamical system we can easily compare the Nystrom estimators to two alternatives: 1. the corresponding _exact_ estimators and 2. the sKAF algorithm which also uses randomized linear algebra to improve the efficiency of PCR. In this setting we sample long trajectories from the system, keeping the first points for training (the number of training points varies for the first experiment, and is fixed to \(10\,000\) for the second, see fig. 2), and the subsequent ones for testing. In fig. 1 we compare the run-time and accuracy with of NysPCR and NysRRR versus their full counterparts. To demonstrate the different scaling regimes we fix the number of inducing points to 250 and increase the number of data points \(n\). The accuracy of the two solvers (as measured with the normalized RMSE metric (nRMSE) [20] on the first variable) is identical for PCR and close for RRR, but the running time of the approximate solvers increases much slower with \(n\) than that of the exact solvers. Each experiment is repeated 20 times to display error bars over the choice of Nystrom centers. In the second experiment, shown in fig. 2, we reproduce the setting of [20] by training at increasingly long forecast horizons. Plotting the nRMSE we verify that sKAF and NysPCR converge to very similar accuracy values, although NysPCR is approximately \(10\) times faster. NysRRR instead offers slightly better accuracy, at the expense of a higher running time compared to NysPCR. Error bars are the standard deviation of nRMSE over 5 successive test sets with \(10\,000\) points each. In section K we show additional experiments including a comparison to the Nystrom KRR estimator.

**Molecular dynamics datasets** An important application of Koopman operator theory is in the analysis of molecular dynamics (MD) datasets, where the evolution of a molecule's atomic positions as they evolve over time is modelled. Interesting systems are very high dimensional, with hundreds or thousands of atoms. Furthermore, trajectories are generated at very short time intervals (\(<1\,\mathrm{ns}\)) but

Figure 1: Full and Nyström estimators trained on L63 with increasing \(n\). Error (_left_) and running time (_right_) are plotted to show efficiency gains without accuracy loss with the Nyström approximation. RBF(\(\sigma=3.5\)) kernel, \(r=25\) principal components and \(m=250\) inducing points.

Figure 2: Nyström and sKAF estimators trained on L63 for increasing forecast horizons; the error (_left_) and overall running times (_right_) are shown. We used a RBF kernel with \(\sigma=3.5\), \(r=50\), \(m=250\) (for Nyström methods) and \(\sqrt{n}\log n\) random features (for sKAF).

interesting events (e.g. protein folding/unfolding) occur at timescales on the order of at least \(10\,\mathrm{\SIUnitSymbolMicro s}\), so that huge datasets are needed to have a few samples of the rare events. The top eigenfunctions of the Koopman operator learned on such trajectories can be used to project the high-dimensional state space onto low-dimensional coordinates which capture the long term, slow dynamics.

We take three \(250\,\mathrm{ns}\) long simulations sampled at \(1\,\mathrm{ps}\) of the alanine dipeptide [64], which is often taken as a model system for molecular dynamics [47; 46]. We use the pairwise distances between heavy atoms as features, yielding a 45-dimensional space. We train a NysRRR model with \(10\,000\) centers on top of the full dataset (\(449\,940\) points are used for training, the rest for validation and testing) with lag time \(100\,\mathrm{ps}\), and recover a 2-dimensional representation which correlates well with the \(\phi,\psi\) backbone dihedral angles of the molecule, known to capture all relevant long-term dynamics. Figure 3a shows the top two eigenfunctions overlaid onto \(\phi,\psi\), the first separates the slowest transition between low and high \(\phi\); the second separates low and high \(\psi\). The implied time-scales from the first two non-trivial eigenvalues are \(1262\,\mathrm{ps}\) and \(69\,\mathrm{ps}\), which are close to the values reported by Niiske et al. [47] (\(1400\,\mathrm{ps}\) and \(70\,\mathrm{ps}\)) who used a more complex post-processing procedure to identify time-scales. We then train a PCCA+ [16] model on the first three eigenfunctions to obtain three states, as shown in fig. 3b. PCCA+ acts on top of a fine clustering (in our case obtained with k-means, \(k=50\)), to find the set of maximally stable states by analyzing transitions between the fine clusters. The coarse clusters clearly correspond to the two transitions described above.

Finally we take a \(208\,\mathrm{\SIUnitSymbolMicro s}\) long simulation of the fast-folding Trp-cage protein [35], sampled every \(0.2\,\mathrm{ns}\). Again, the states are the pairwise distances between non-hydrogen atoms belonging to the protein, in \(10\,296\) dimensions. A NysRRR model is trained on \(626\,370\) points, using \(5000\) centers in approximately 10 minutes. Note that without sketching this would be a completely intractable problem. Using a lag-time of \(10\,\mathrm{ns}\) we observe a spectral gap between the third and fourth eigenvalues, hence we train a PCCA+ model on the first 3 eigenfunctions to obtain the states shown in fig. 4. The first non-trivial Koopman eigenvector effectively distinguishes between the folded (state 1) and unfolded states as is evident from the first row of fig. 4. The second one instead can be used to identify a partially folded state of the protein (state 0), as can be seen from the insets in fig. 4.

## 6 Conclusions

We introduced three efficient kernel-based estimators of the Koopman operator relying on random projections, and provided a bound on their excess risk in operator norm - which is of paramount importance to control the accuracy of Koopman mode decomposition. Random projections allow to process efficiently even the longest trajectories, and these gains come for free as our estimators still enjoy optimal theoretical learning rates. We leave for future work the refinement our analysis under e.g. an additional source condition assumption or in the misspecified setting. Another future research direction shall be to devise ways to further reduce the computational complexity of the estimators.

Figure 3: Dynamics of the alanine dipeptide (lag-time 100), Nyström RRR model. On the left the Ramachandran plot of all protein conformations is colored with the value of the first two non-constant eigenfunctions evaluated on the 45-d space. On the right color indicates the discrete state obtained by clustering the dynamics projected onto the first eigenfunctions (using PCCA+ with 3 states). The obtained clusters match the main areas of the Ramachandran plot.

## 7 Acknowledgements

This paper is part of a project that has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No. 819789). L. R. acknowledges the financial support of the European Research Council (grant SLING 819789), the AFOSR projects FA9550-18-1-7009, FA9550-17-1-0390 and BAA-AFRL-AFOSR-2016-0007 (European Office of Aerospace Research and Development), the EU H2020-MSCA-RISE project NoMADS - DLV-777826, and the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF-1231216. M. P., V. K. and P. N. acknowledge financial support from PNRR MUR project PE0000013-FAIR and the European Union (Projects 951847 and 101070617).

## References

* Abedsoltan et al. [2023] Amirhesam Abedsoltan, Mikhail Belkin, and Parthe Pandit. Toward large kernel models, 2023. arXiv:2302.02605 [cs.LG].
* Greville Adi Ben-Israel [2003] Thomas N. E. Greville Adi Ben-Israel. _Generalized Inverses: Theory and Applications_. CMS Books in Mathematics. Springer, 2nd edition, 2003.
* Ahmad et al. [2023] Tamim El Ahmad, Luc Brogat-Motte, Pierre Laforgue, and d'Alche-Buc Florence. Sketch In, Sketch Out: Accelerating both Learning and Inference for Structured Prediction with Kernels, 2023. arxiv:2302.10128.
* Alexander and Giannakis [2020] Romeo Alexander and Dimitrios Giannakis. Operator-theoretic framework for forecasting nonlinear time series with kernel analog techniques. _Physica D: Nonlinear Phenomena_, 409, 2020. doi: https://doi.org/10.1016/j.physd.2020.132520.
* Bach [2013] Francis Bach. Sharp analysis of low-rank kernel matrix approximations. _Journal of Machine Learning Research_, 30, 2013.
* Baddoo et al. [2022] Peter J. Baddoo, Benjamin Herrmann, Beverley J. McKeon, and Steven L. Brunton. Kernel learning for robust dynamic mode decomposition: linear and nonlinear disambiguation optimization. _Proceedings of the Royal Society A_, 2022. doi: http://doi.org/10.1098/rspa.2021.0830.

Figure 4: First eigenfunctions for Trp-cage dynamics, colored according to the membership probability for each state in a PCCA+ model. The bottom insets show a few overlaid structures from each state. The first eigenfunction exhibits a strong linear separation between state 1 (folded) and the other states, which is highlighted by the black lines. The second separates between state 0 (partially folded) ant the rest. NysRRR model trained with \(m=5000\), \(r=10\), RBF(\(\sigma=0.02\)) kernel, \(\lambda=10^{-10}\).

* Brunton et al. [2022] Steven L. Brunton, Marko Budisic, Eurika Kaiser, and J. Nathan Kutz. Modern koopman theory for dynamical systems. _SIAM Review_, 64(2):229-340, 2022. doi: 10.1137/21M1401243.
* Budisic et al. [2012] Marko Budisic, Ryan Mohr, and Igor Mezic. Applied koopmanism. _Chaos_, 22(4), 2012. doi: 10.1063/1.4772195.
* 368, 2007. doi: 10.1007/s10208-006-0196-8.
* Carratino et al. [2021] Luigi Carratino, Stefano Vigogna, Daniele Calandriello, and Lorenzo Rosasco. ParK: Sound and efficient kernel ridge regression by featurespace partitions. In _Advances in Neural Information Processing Systems 34_, pages 6430-6441, 2021.
* Chatalic et al. [2022] Antoine Chatalic, Nicolas Schreuder, Lorenzo Rosasco, and Alessandro Rudi. Nystrom Kernel Mean Embeddings. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 3006-3024. PMLR, 2022.
* Ciliberto et al. [2016] Carlo Ciliberto, Lorenzo Rosasco, and Alessandro Rudi. A Consistent Regularization Approach for Structured Prediction. In _Advances in Neural Information Processing Systems_, volume 29. Curran Associates, Inc., 2016.
* Ciliberto et al. [2022] Carlo Ciliberto, Lorenzo Rosasco, and Alessandro Rudi. A general framework for consistent structured prediction with implicit loss embeddings. _Journal of Machine Learning Research_, 21(1), 2022.
* Prato and Zabczyk [1996] G. Da Prato and J. Zabczyk. _Ergodicity for Infinite Dimensional Systems_. London Mathematical Society Lecture Note Series. Cambridge University Press, 1996. doi: 10.1017/CBO9780511662829.
* Dellnitz and Junge [1999] Michael Dellnitz and Oliver Junge. On the approximation of complicated dynamical behavior. _SIAM Journal on Numerical Analysis_, 36(2):491-515, 1999. doi: 10.1137/S0036142996313002.
* Deuflhard and Weber [2005] Peter Deuflhard and Marcus Weber. Robust perron cluster analysis in conformation dynamics. _Linear Algebra and its Applications_, 398:161-184, 2005. doi: https://doi.org/10.1016/j.laa.2004.10.026.
* Drineas et al. [2012] Petros Drineas, Malik Magdon-Ismail, Michael W. Mahoney, and David P. Woodruff. Fast approximation of matrix coherence and statistical leverage. _Journal of Machine Learning Research_, 13(1), 2012.
* Fischer and Steinwart [2020] Simon Fischer and Ingo Steinwart. Sobolev norm learning rates for regularized least-squares algorithms. _Journal of Machine Learning Research_, 21(1):8464-8501, 2020.
* Froyland et al. [2014] Gary Froyland, Georg A. Gottwald, and Andy Hammerlindl. A computational method to extract macroscopic variables and their dynamics in multiscale systems. _SIAM Journal on Applied Dynamical Systems_, 13(4):1816-1846, 2014. doi: 10.1137/130943637.
* Giannakis et al. [2023] Dimitrios Giannakis, Amelia Henriksen, Joel A. Tropp, and Rachel Ward. Learning to forecast dynamical systems from streaming data. _SIAM Journal on Applied Dynamical Systems_, 22(2):527-558, 2023. doi: 10.1137/21M144983X.
* Gittens and Mahoney [2016] Alex Gittens and Michael W. Mahoney. Revisiting the nystrom method for improved large-scale machine learning. _Journal of Machine Learning Research_, 17:3977-4041, 2016.
* Grunewalder et al. [2012] Steffen Grunewalder, Guy Lever, Luca Baldassarre, Sam Patterson, Arthur Gretton, and Massimilano Pontil. Conditional mean embeddings as regressors. In _Proceedings of the 29th International Conference on Machine Learning_, pages 1803---1810, 2012.
* 1220, 2008. doi: 10.1214/009053607000000677.
* [24] Alan Julian Izenman. Reduced-rank regression for the multivariate linear model. _Journal of Multivariate Analysis_, 5(2):248-264, 1975. doi: https://doi.org/10.1016/0047-259X(75)90042-1.
* [25] Stefan Klus, Peter Koltai, and Christof Schutte. On the numerical approximation of the Perron-Frobenius and Koopman operator. _Journal of Computational Dynamics_, 3(1):51-79, 2016. ISSN 2158-2491. doi: 10.3934/jcd.2016003.
* [26] Stefan Klus, Ingmar Schuster, and Krikamol Muandet. Eigendecompositions of transfer operators in reproducing kernel hilbert spaces. _Journal of Nonlinear Science_, 30(1):283-315, 2020. doi: 10.1007/s00332-019-09574-z.
* [27] B. O. Koopman. Hamiltonian systems and transformation in hilbert space. _Proceedings of the National Academy of Sciences_, 17(5):315-318, 1931. doi: 10.1073/pnas.17.5.315.
* [28] B. O. Koopman and J. v. Neumann. Dynamical systems of continuous spectra. _Proceedings of the National Academy of Sciences_, 18(3):255-263, 1932. doi: 10.1073/pnas.18.3.255.
* [29] Vladimir Kostic, Pietro Novelli, Andreas Maurer, Carlo Ciliberto, Lorenzo Rosasco, and Massimiliano Pontil. Learning dynamical systems via Koopman operator regression in Reproducing Kernel Hilbert Spaces. In _Advances in Neural Information Processing Systems 35_, 2022.
* [30] Vladimir Kostic, Karim Lounici, Pietro Novelli, and Massimiliano Pontil. Koopman operator learning: Sharp spectral rates and spurious eigenvalues, 2023. arXiv:2302.02004 [cs.LG].
* [31] J. Nathan Kutz, Steven L. Brunton, Binghi W. Brunton, and Joshua Proctor. _Dynamic Mode Decomposition: Data-Driven Modeling of Complex Systems_. SIAM, 2016.
* [32] Qianxiao Li, Felix Dietrich, Erik M. Bollt, and Ioannis G. Kevrekidis. Extended dynamic mode decomposition with dictionary learning: A data-driven adaptive spectral decomposition of the koopman operator. _Chaos: An Interdisciplinary Journal of Nonlinear Science_, 27(10), 2017. doi: 10.1063/1.4993854.
* [33] Zhu Li, Dimitri Meunier, Mattes Mollenhauer, and Arthur Gretton. Optimal Rates for Regularized Conditional Mean Embedding Learning. In _Advances in Neural Information Processing Systems 35_, 2022.
* [34] Junhong Lin and Volkan Cevher. Optimal convergence for distributed learning with stochastic gradient methods and spectral algorithms. _Journal of Machine Learning Research_, 21(147):1-63, 2020.
* [35] Kresten Lindorff-Larsen, Stefano Piana, Ron O. Dror, and David E. Shaw. How fast-folding proteins fold. _Science_, 334(6055):517-520, 2011. doi: 10.1126/science.1208351.
* 141, 1963. doi: https://doi.org/10.1175/1520-0469(1963)020<0130:DNF>2.0.CO;2.
* [37] Bethany Lusch, J. Nathan Kutz, and Steven L. Brunton. Deep learning for universal linear embeddings of nonlinear dynamics. _Nature Communications_, 9(1), 2018. doi: 10.1038/s41467-018-07210-0.
* [38] Giacomo Meanti, Luigi Carratino, Lorenzo Rosasco, and Alessandro Rudi. Kernel methods through the roof: handling billions of points efficiently. In _Advances in Neural Information Processing Systems 32_, 2020.
* [39] Igor Mezic. _On the geometrical and statistical properties of dynamical systems: Theory and applications_. PhD thesis, ProQuest LLC, California Institute of Technology, 1994.
* [40] Igor Mezic. Spectral properties of dynamical systems, model reduction and decompositions. _Nonlinear Dynamics_, 41:309-325, 2005. doi: 10.1007/s11071-005-2824-x.
* [41] Igor Mezic. Koopman operator, geometry, and learning of dynamical systems. _Notices of the American Mathematical Society_, 68(7):1087-1105, 2021.

* Molgedey and Schuster [1994] L. Molgedey and H. G. Schuster. Separation of a mixture of independent signals using time delayed correlations. _Phys. Rev. Lett._, 72:3634-3637, 1994. doi: 10.1103/PhysRevLett.72.3634.
* Mollenhauer et al. [2020] Mattes Mollenhauer, Ingmar Schuster, Stefan Klus, and Christof Schutte. Singular value decomposition of operators on reproducing kernel hilbert spaces. In _Advances in Dynamics, Optimization and Computation_, pages 109-131, 2020.
* Muandet et al. [2017] Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, and Bernhard Scholkopf. Kernel mean embedding of distributions: A review and beyond. _Foundations and Trends in Machine Learning_, 10(1-2):1-141, 2017. doi: 10.1561/2200000060.
* Noe and Nuske [2013] Frank Noe and Feliks Nuske. A variational approach to modeling slow processes in stochastic dynamical systems. _Multiscale Modeling & Simulation_, 11(2):635-655, 2013. doi: 10.1137/110858616.
* 1752, 2014. doi: 10.1021/ct4009156.
* Nuske et al. [2017] Feliks Nuske, Hao Wu, Jan-Hendrik Prinz, Christoph Wehmeyer, Cecilia Clementi, and Frank Noe. Markov state models from short non-equilibrium simulations -- analysis and correction of estimation bias. _Journal of Chemical Physics_, 146(9), 2017. doi: 10.1063/1.4976518.
* Pinelis and Sakhanenko [1986] I. F. Pinelis and A. I. Sakhanenko. Remarks on inequalities for large deviation probabilities. _Theory of Probability & Its Applications_, 30(1):143-148, 1986. doi: 10.1137/1130013.
* Perez-Hernandez et al. [2013] Guillermo Perez-Hernandez, Fabian Paul, Toni Giorgino, Gianni De Fabritiis, and Frank Noe. Identification of slow molecular order parameters for markov model construction. _Journal of Chemical Physics_, 139(1), 07 2013. doi: 10.1063/1.4811489.
* Rahimi and Recht [2008] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In _Advances in Neural Information Processing Systems 20_, 2008.
* Rahimi and Recht [2009] Ali Rahimi and Benjamin Recht. Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning. In _Advances in Neural Information Processing Systems 21_, 2009.
* Rowley et al. [2009] Clarence W. Rowley, Igor Mezic, Shervin Bagheri, Philipp Schlatter, and Dan S. Henningson. Spectral analysis of nonlinear flows. _Journal of Fluid Mechanics_, pages 115-127, 2009. doi: 10.1017/S0022112009992059.
* Rudi et al. [2015] Alessandro Rudi, Raffaello Camoriano, and Lorenzo Rosasco. Less is more: Nystrom computational regularization. In _Advances in Neural Information Processing Systems 15_, pages 1657-1665, 2015.
* Rudi et al. [2017] Alessandro Rudi, Luigi Carratino, and Lorenzo Rosasco. FALKON: An optimal large scale kernel method. In _Advances in Neural Information Processing Systems 30_, 2017.
* Rudi et al. [2018] Alessandro Rudi, Daniele Calandriello, Luigi Carratino, and Lorenzo Rosasco. On fast leverage score sampling and optimal learning. In _Advances in Neural Information Processing Systems 31_, 2018.
* Schmid [2010] Peter J. Schmid. Dynamic mode decomposition of numerical and experimental data. _Journal of Fluid Mechanics_, 656:5-28, 2010. doi: 10.1017/S0022112010001217.
* Schwantes and Pande [2015] Christian R. Schwantes and Vijay S. Pande. Modeling molecular kinetics with tICA and the kernel trick. _Journal of Chemical Theory and Computation_, 11(2):600-608, 2015. doi: 10.1021/ct5007357.
* Smola and Scholkopf [2000] Alex J. Smola and Bernhard Scholkopf. Sparse greedy matrix approximation for machine learning. In _ICML 17_, 2000.

* [59] Le Song, Jonathan Huang, Alex Smola, and Kenji Fukumizu. Hilbert space embeddings of conditional distributions with applications to dynamical systems. In _Proceedings of the 26th Annual International Conference on Machine Learning_, page 961-968, 2009. doi: 10.1145/1553374.1553497.
* [60] Ingo Steinwart and Andreas Christmann. _Support Vector Machines_. Springer Science & Business Media, 2008. URL https://link.springer.com/book/10.1007/978-0-387-77242-4.
* [61] Ingo Steinwart and Clint Scovel. Mercer's theorem on general domains: On the interaction between measures, kernels, and RKHSs. _Constructive Approximation_, 35(3):363-417, 2012.
* [62] Naoya Takeishi, Yoshinobu Kawahara, and Takehisa Yairi. Learning koopman invariant subspaces for dynamic mode decomposition. In _Advances in Neural Information Processing Systems_, page 1130-1140, 2017.
* [63] Jonathan H. Tu, Clarence W. Rowley, Dirk M. Luchtenburg, Steven L. Brunton, and J. Nathan Kutz. On dynamic mode decomposition: Theory and applications. _Journal of Computational Dynamics_, 1(2):391-421, 2014. doi: 10.3934/jcd.2014.1.391.
* [64] Christoph Wehmeyer and Frank Noe. Time-lagged autoencoders: Deep learning of slow collective variables for molecular kinetics. _Journal of Chemical Physics_, 148(24), 2018. doi: 10.1063/1.5011399.
* [65] Christopher K. I. Williams and Matthias Seeger. Using the Nystrom method to speed up kernel machines. In _Advances in Neural Information Processing Systems 13_, 2001.
* 1346, 2015. doi: 10.1007/s00332-015-9258-5.
* [67] Matthew O. Williams, Clarence W. Rowley, and Ioannis G. Kevrekidis. A kernel-based method for data-driven Koopman spectral analysis. _Journal of Computational Dynamics_, 2(2):247-265, 2015. ISSN 2158-2491. doi: 10.3934/jcd.2015005.
* [68] Tianbao Yang, Yu-Feng Li, Mehrdad Mahdavi, Rong Jin, and Zhi-Hua Zhou. Nystrom method vs Random Fourier Features: A theoretical and empirical comparison. In _Advances in Neural Information Processing Systems 24_, 2012.
* [69] Enoch Yeung, Soumya Kundu, and Nathan Hodas. Learning deep neural network representations for koopman operators of nonlinear dynamical systems. In _2019 American Control Conference (ACC)_, 2019. doi: 10.23919/ACC.2019.8815339.
* [70] Vadim Yurinsky. _Sums and Gaussian Vectors_. Lecture Notes in Mathematics 1617. Springer, 1st edition, 1995.

## Appendix A Setting and notations

### Operators and notations

We define the following operators:

* \(\Phi_{X}:L_{\pi}^{2}\to\mathcal{H}\), defined by \(\Phi_{X}f=\int_{\mathcal{X}}f(x)\phi(x)\,\mathrm{d}\pi(x)\) for any \(f\in L_{\pi}^{2}\).
* \(\Phi_{X}^{*}:\mathcal{H}\to L_{\pi}^{2}\), defined by \(\Phi_{X}^{*}h=\langle h,\phi(\cdot)\rangle_{\mathcal{H}}\) for any \(h\in\mathcal{H}\) (i.e. the embedding operator mapping a function to its \(\pi\)-equivalence class in \(L_{\pi}^{2}\)).
* \(\Phi_{Y|X}:L_{\pi}^{2}\to\mathcal{H}\), defined by \(\Phi_{Y|X}=\Phi_{X}K_{\pi}^{*}\).
* \(\Phi_{Y|X}^{*}:\mathcal{H}\to L_{\pi}^{2}\), defined by \(\Phi_{Y|X}^{*}=\mathcal{K}_{\pi}\Phi_{X}^{*}\).
* \(C:\mathcal{H}\to\mathcal{H}\) defined as \(C=\mathbf{E}_{x\sim\pi}\phi(x)\otimes\phi(x)=\Phi_{X}\Phi_{X}^{*}\), satisfying \(\mathrm{tr}(C)\leq K^{2}\). Note that under our assumptions, this also corresponds to the covariance of \(Y\).
* \(C_{XY}:=\mathbf{E}_{(x,y)\sim\rho}\phi(x)\otimes\phi(y)=\Phi_{X}\Phi_{Y|X}^{*}\).

As well as the following discretized variants:

* \(\hat{\Phi}_{X}:\mathbb{R}^{n}\to\mathcal{H}\), defined by \(\hat{\Phi}_{X}v=\sum_{i=1}^{n}v_{i}\phi(x_{i})\) for any \(v=[v_{1},\ldots,v_{n}]\in\mathbb{R}^{n}\)
* \(\hat{\Phi}_{X}^{*}:\mathcal{H}\to\mathbb{R}^{n}\), defined by \(\hat{\Phi}_{X}^{*}h=[\langle\phi(x_{1}),h\rangle_{\mathcal{H}},\ldots,\langle \phi(x_{n}),h\rangle_{\mathcal{H}}]^{T}\) for any \(h\in\mathcal{H}\)
* \(\hat{\Phi}_{Y|X}:\mathbb{R}^{n}\to\mathcal{H}\), defined by \(\tilde{\Phi}_{Y|X}v=\sum_{i=1}^{n}v_{i}\phi(y_{i})\) for any \(v=[v_{1},\ldots,v_{n}]\in\mathbb{R}^{n}\).
* \(\hat{\Phi}_{Y|X}:\mathcal{H}\to\mathbb{R}^{n}\), defined by \(\tilde{\Phi}_{Y|X}^{*}h=[\langle\phi(y_{1}),h\rangle_{\mathcal{H}},\ldots, \langle\phi(y_{n}),h\rangle_{\mathcal{H}}]^{T}\) for any \(h\in\mathcal{H}\).
* \(\hat{C}=\frac{1}{n}\hat{\Phi}_{X}\hat{\Phi}_{X}^{*}=\frac{1}{n}\sum_{i=1}^{n} \phi(x_{i})\otimes\phi(x_{i})\in\mathcal{L}(\mathcal{H})\) is the empirical covariance.

The Nystrom discretized operators are obtained by applying the kernel map to \(m\ll n\) inducing points \(\{\tilde{\bm{x}}_{j}\}_{j=1}^{m}\subset\{\bm{x}_{j}\}_{j=1}^{n}\) and \(\{\tilde{\bm{y}}_{j}\}_{j=1}^{m}\subset\{\bm{y}_{j}\}_{j=1}^{n}\):

* \(\widetilde{\Phi}_{X}:\mathbb{R}^{m}\to\mathcal{H}\) such that \(\widetilde{\Phi}_{X}w=\sum_{j=1}^{m}w_{j}\phi(\tilde{\bm{x}}_{j})\).
* \(\widetilde{\Phi}_{Y}:\mathbb{R}^{m}\to\mathcal{H}\) such that \(\widetilde{\Phi}_{Y}w=\sum_{j=1}^{m}w_{j}\phi(\tilde{\bm{y}}_{j})\).

Furthermore denote by \(P_{X}\) and \(P_{Y}\) the orthogonal projections onto \(\mathrm{span}\,\widetilde{\Phi}_{X}\) and \(\mathrm{span}\,\widetilde{\Phi}_{Y}\) respectively.

One important quantity to derive the rates is the so-called effective dimension, defined as

\[d_{\text{eff}}(\lambda):=\mathrm{tr}(C_{\lambda}^{-1}C).\]

where \(C_{\lambda}:=C+\lambda I\).

### Conditional mean embedding

For any \(x\in\mathcal{X}\), we denote \(\mu_{p}(x)\) the conditional mean embedding associated to the transition kernel defined as

\[\mu_{p}(x):=\mathbf{E}\big{[}\phi(X_{t+1})|X_{t}=x\big{]}=\int\phi(y)p(x, \mathrm{d}y)\]

The following lemma provides a characterization of \(\Phi_{Y|X}^{*}\) in terms of the conditional mean embedding.

**Lemma A.1**:: _We have the following relations:_

\[\Phi_{Y|X}f =\int_{X}f(x)\mu_{p}(x)\,\mathrm{d}\pi(x),\quad f\in L^{2}_{\pi}\] (16) \[(\Phi^{*}_{Y|X}f)(x) =\langle f,\mu_{p}(x)\rangle,\quad f\in\mathcal{H}\] (17) \[\Phi_{Y|X}\Phi^{*}_{Y|X} =\mathbf{E}_{x\sim\pi}\mu_{p}(x)\otimes\mu_{p}(x)\] (18)

Proof of Lemma a.1: For the first property:

\[(\Phi^{*}_{Y|X}f)(x) =(\mathcal{K}_{\pi}(\Phi^{*}_{X}f))(x)\] (19) \[=\int(\Phi^{*}_{X}f)(y)p(x,\mathrm{d}y)\] (20) \[=\int f(y)p(x,\mathrm{d}y)\] (21) \[=\langle f,\int\phi(y)p(x,\mathrm{d}y)\rangle=\langle f,\mu_{p}( x)\rangle\] (22)

where we used that \(f\) and \(\Phi^{*}_{X}f\) coincide \(\pi\)-almost everywhere. The second property is a direct consequence of the definition of the adjoint. For (18), we simply use (17) and the definition of \(\Phi_{Y|X}\) to get

\[\Phi_{Y|X}(\Phi^{*}_{Y|X}f)=\int\langle f,\mu_{p}(z)\rangle\mu_{p}(z)\, \mathrm{d}\pi(z)=\biggl{(}\int\mu_{p}(z)\mu_{p}(z)^{*}\,\mathrm{d}\pi(z) \biggr{)}f.\]

### Power spaces

We now define the \(\alpha\)-power space \([\mathcal{H}]^{\alpha}_{\pi}\) in order to provide some intuition regarding Assumption 4.3. By Assumption 4.1, \(\mathrm{tr}(C)=\int\mathrm{tr}(\phi(x)\otimes\phi(x))\,\mathrm{d}\pi(x)\leq K ^{2}\) and thus \(C\) is trace-class (and compact). By [18], there exists a non-increasing summable sequence \((\mu_{i})_{i\in I}\) for an at most countable index set \(I\), a family \((e_{i})_{i\in I}\in\mathcal{H}\) s.t. \((\Phi^{*}_{X}e_{i})_{i\in I}\) is an orthonormal basis of \(\overline{\mathrm{span}\,\Phi^{*}_{X}}\subseteq L^{2}_{\pi}\) and \((\mu^{1/2}_{i}e_{i})_{i\in I}\) is an orthonormal basis of \((\ker\Phi^{*}_{X})^{\perp}\subseteq\mathcal{H}\) such that

\[C=\sum_{i\in I}\mu_{i}\langle\cdot,\mu^{1/2}_{i}e_{i}\rangle_{\mathcal{H}}\mu ^{1/2}_{i}e_{i}.\]

For \(\alpha\geq 0\), we now define the \(\alpha\)-power space as

\[[\mathcal{H}]^{\alpha}_{\pi}:=\left\{\left.\sum_{i\in I}a_{i}\mu^{\alpha/2}_{ i}\Phi^{*}_{X}e_{i}\ \right|\ (a_{i})_{i\in I}\in\ell_{2}(I)\ \right\}\subseteq L^{2}_{\pi}\]

equipped with norm

\[\left\|\sum_{i\in I}a_{i}\mu^{\alpha/2}_{i}\Phi^{*}_{X}e_{i}\right\|_{[ \mathcal{H}]^{\alpha}_{\pi}}:=\|(a_{i})_{i\in I}\|_{\ell_{2}(I)}.\]

We can now make the following assumption regarding the embedding of the power spaces into \(L^{\infty}_{\pi}\).

**Assumption A.2** (Embedding): _There exists \(\tau\in[\beta,1]\) such that \(c_{\tau}:=\|[\mathcal{H}]^{\tau}_{\pi}\hookrightarrow L^{\infty}_{\pi}\|^{2}<\infty\)._

We stress that Assumption A.2 implies in particular Assumption 4.3, and is a common assumption in the literature, see for instance [18].

## Appendix B Expression of the risk

We have the following risk decomposition.

[MISSING_PAGE_EMPTY:17]

where we used the invariance property of \(\pi\) in \((i)\) and Lemma A.1 for the last inequality. Then one can easily check that the sum of both corresponds to the full risk defined in (5):

\[\mathcal{R}_{\text{HS},0}+\mathcal{E}_{\text{HS}}(A) =\int\lVert\mu_{p}(x)-\phi(y)\rVert^{2}\,\mathrm{d}\rho(x,y)+\int \lVert\mu_{p}(x)-A\phi(x)\rVert^{2}\,\mathrm{d}\pi(x)\] \[=\int\biggl{(}\lVert\mu_{p}(x)\rVert^{2}-2\langle\mu_{p}(x), \int\phi(y)p(x,\mathrm{d}y)\rangle+\int\lVert\phi(y)\rVert^{2}p(x,\mathrm{d}y )\biggr{)}\,\mathrm{d}\pi(x)\] \[\quad+\int\Bigl{(}\lVert\mu_{p}(x)\rVert^{2}-2\langle\mu_{p}(x),A \phi(x)\rangle+\lVert A\phi(x)\rVert^{2}\Bigr{)}\,\mathrm{d}\pi(x)\] \[=\int\biggl{(}\int\lVert\phi(y)\rVert^{2}p(x,\mathrm{d}y)-2(\int \phi(y)p(x,\mathrm{d}y),A\phi(x))+\lVert A\phi(x)\rVert^{2}\biggr{)}\,\mathrm{ d}\pi(x)\] \[=\int\Bigl{(}\lVert\phi(y)\rVert^{2}-2\langle\phi(y),A\phi(x) \rangle+\lVert A\phi(x)\rVert^{2}\Bigr{)}\,\mathrm{d}\rho(x,y)\] \[=\int\lVert\phi(y)-A\phi(x)\rVert^{2}\,\mathrm{d}\rho(x,y)= \mathcal{R}_{\text{HS}}(A).\]

## Appendix C Expression of the estimators

In this section we give proofs of propositions 3.1 to 3.3 on how to efficiently compute the Nystrom estimators.

For all three - KRR, PCR and RRR - estimators, the starting point is their respective _full_ estimator which can be derived by following the first-order optimality criterion for the following minimization problems

**Full KRR:** \[\hat{A}^{\text{KRR}}_{\lambda} =\operatorname*{arg\,min}_{A\in\mathcal{H}\to\mathcal{H}}\lVert \hat{\Phi}_{Y|X}-A\hat{\Phi}_{X}\rVert_{\text{HS}}^{2}+\lambda\lVert A\rVert_ {\text{HS}}^{2}\] (23) **Full PCR:** \[\hat{A}^{\text{PCR}} =\operatorname*{arg\,min}_{A\in\mathcal{H}\to\mathcal{H}}\lVert \hat{\Phi}_{Y|X}-A\Pi_{r}\hat{\Phi}_{X}\rVert_{\text{HS}}^{2}\] (24) **Full RRR:** \[\hat{A}^{\text{RRR}}_{\lambda} =\operatorname*{arg\,min}_{A\in\mathcal{H}\to\mathcal{H}:\text{ rk}(A)\leq r}\lVert\hat{\Phi}_{Y|X}-A\hat{\Phi}_{X}\rVert_{\text{HS}}^{2}+ \lambda\lVert A\rVert_{\text{HS}}^{2}\] (25)

where \(\Pi_{r}\) is the orthogonal projection onto the top-r eigenvectors of \(\hat{C}\).

To derive the Nystrom estimators, we project the embedded data \(\hat{\Phi}_{X}\), \(\hat{\Phi}_{Y|X}\) onto the span of the embedded inducing points - \(P_{X}\hat{\Phi}_{X}\), \(P_{Y}\hat{\Phi}_{Y|X}\) - and then express the resulting estimators as \(\widetilde{\Phi}_{Y}W\widetilde{\Phi}_{X}^{*}\) with \(W\in\mathbb{R}^{m\times m}\). This form is particularly useful for later computing forecasts, eigenfunctions and Koopman modes with the estimator. In particular the following equalities for the projection (shown here for \(P_{X}\) but equivalently exist for \(P_{Y}\))

\[P_{X}=P_{X}P_{X}=\widetilde{\Phi}_{X}(\widetilde{\Phi}_{X}^{*}\widetilde{\Phi} _{X})^{\dagger}\widetilde{\Phi}_{X}^{*}=\widetilde{\Phi}_{X}^{*^{\dagger}} \widetilde{\Phi}_{X}^{*}=\widetilde{\Phi}_{X}\widetilde{\Phi}_{X}^{\dagger},\]

and the characterization of \(P_{X}\) through the SVD of \(\widetilde{\Phi}_{X}=U\Sigma V^{*}\), such that \(P_{X}=UU^{*}\).

### Nystrom KRR

We begin with the Nystrom KRR estimator, providing an alternative but equivalent description in lemma C.1.

**Lemma C.1** (Expression of the KRR regularization):: _Let \(U\) be such that \(P_{X}=UU^{*}\), \(U^{*}U=I\). Then it holds_

\[g_{\text{KRR}}(\hat{C}):=P_{X}(P_{X}\hat{C}P_{X}+\lambda I)^{-1}=U(U^{*}\hat{C} U+\lambda I)^{-1}U^{*}.\] (26)

[MISSING_PAGE_FAIL:19]

Proof of Proposition c.3:.: We begin by computing the decomposition of \(P_{X}\hat{C}P_{X}\) which is necessary to obtain \(g_{\text{PCR}}(\hat{C})\). The following expressions are equivalent [43, Proposition 3] for determining its eigenvectors \(\tilde{h}\) and eigenvalues \(\lambda\):

\[UU^{*}\hat{C}UU^{*}\tilde{h} =\lambda\tilde{h}\] \[U^{*}\hat{C}Uh =\lambda h,\qquad\tilde{h}=Uh.\]

Let the truncated eigenvalues be \(\Lambda_{r}=\operatorname{diag}[\lambda_{1},\dots,\lambda_{r}]\) and the eigenvectors be \(H_{r}=[h_{1},\dots,h_{r}]\). Then \(\tilde{H}_{r}=UH_{r}\) must be normalized such that \(\tilde{H}_{r}^{*}\tilde{H}_{r}=H_{r}^{*}U^{*}UH_{r}=I\). The rank-r truncation \([\![P_{X}\hat{C}P_{X}]\!]_{r}\) is a projection onto \(\tilde{H}_{r}\tilde{H}_{r}^{*}\):

\[[\![P_{X}\hat{C}P_{X}]\!]_{r}^{\dagger}=(UU^{*}\hat{C}UU^{*}(UH_{r})(UH_{r})^ {*})^{\dagger}=(UH\Lambda H^{*}H_{r}H_{r}^{*}U^{*})^{\dagger}=UH_{r}\Lambda_{ r}^{-1}H_{r}^{*}U^{*}\]

where we used that \(U^{*}\hat{C}U=H\Lambda H^{*}\).

Now substitute \(U=\widetilde{\Phi}_{X}V\Sigma^{-1}\) to simplify the eigendecomposition of \(U^{*}\hat{C}U\):

\[\Sigma^{-1}V^{*}K_{\tilde{X},X}K_{X,\tilde{X}}V\Sigma^{-1}h =\lambda h\] \[V\Sigma^{-2}V^{*}K_{\tilde{X},X}K_{X,\tilde{X}}d =\lambda d,\qquad h=\Sigma^{-1}V^{*}K_{\tilde{X},X}K_{X,\tilde{X}}d.\] (29)

where \(V\Sigma^{-2}V^{*}=K_{\tilde{X},\tilde{X}}^{\dagger}\). Denote by \(D_{r}=[d_{1},\dots,d_{r}]\) the truncated eigenvectors such that \(H_{r}=\Sigma^{-1}V^{*}K_{\tilde{X},X}K_{X,\tilde{X}}D_{r}\), normalized such that \(H^{*}H=D^{*}K_{\tilde{X},X}K_{X,\tilde{X}}K_{\tilde{X},\tilde{X}}^{\dagger}K_ {\tilde{X},X}K_{X,\tilde{X}}D=I\),

\[UH_{r}\Lambda_{r}^{-1}H_{r}^{*}U^{*} =\widetilde{\Phi}_{X}K_{\tilde{X},\tilde{X}}^{\dagger}K_{\tilde{ X},\tilde{X}}K_{X,\tilde{X}}D_{r}\Lambda_{r}^{-1}D_{r}^{*}K_{\tilde{X},X}K_{X, \tilde{X}}K_{\tilde{X},\tilde{X}}^{\dagger}\widetilde{\Phi}_{X}^{*}\] \[=\widetilde{\Phi}_{X}D_{r}\Lambda_{r}D_{r}^{*}\widetilde{\Phi}_{ X}^{*}\] \[=\widetilde{\Phi}_{X}[K_{\tilde{X},\tilde{X}}^{\dagger}K_{\tilde{ X},X}K_{X,\tilde{X}}]_{r}\widetilde{\Phi}_{X}^{*}.\]

Finally, we can plug the pieces together to get

\[P_{Y}\hat{C}_{YX}[\![P_{X}\hat{C}P_{X}]\!]_{r}^{\dagger}= \widetilde{\Phi}_{Y}K_{\tilde{Y},\tilde{Y}}^{\dagger}K_{\tilde{Y},Y}K_{X, \tilde{X}}[\![K_{\tilde{X},\tilde{X}}^{\dagger}K_{\tilde{X},X}K_{X,\tilde{X}} \!]_{r}\widetilde{\Phi}_{X}^{*}.\]

**Remark C.2** (Variational problem for Nystrom PCR): _Note that, unlike the NysKRR estimator, the variational problem for NysPCR where the operator is restricted to \(A:\mathcal{H}_{\tilde{X}}\to\mathcal{H}_{\tilde{Y}}\) is not equivalent to the one obtained in proposition C.3 by projecting the covariance operator. In fact, the former does not take the full covariance into account when computing the low-rank projection, but just the Nystrom points._

### Nystrom RRR

The Nystrom RRR estimator does not correspond to a specific spectral filter. We can nonetheless compute it starting from the expression of the exact empirical estimator [29], projecting the covariance operators, and rearranging the expression to result in a finite-dimensional procedure.

**Proposition C.4** (Nystrom RRR): _The sketched RRR estimator can be written as_

\[\hat{A}_{m,\lambda}^{\text{RRR}}=[\![P_{Y}\hat{C}_{YX}P_{X}(P_{X}\hat{C}P_{X}+ \lambda I)^{-1/2}]\!]_{r}(P_{X}\hat{C}P_{X}+\lambda I)^{-1/2}.\] (30)

_To compute it, solve the \(m\times m\) eigenvalue problem_

\[(K_{\tilde{X},X}K_{X,\tilde{X}}+n\lambda K_{\tilde{X},\tilde{X}})^{\dagger}K_ {\tilde{X},X}K_{Y,\tilde{Y}}K_{\tilde{Y},\tilde{Y}}^{\dagger}K_{\tilde{Y},Y}K_ {X,\tilde{X}}w_{i}=\sigma_{i}^{2}w_{i}\]

_for the first \(r\) eigenvectors \(W_{r}=[w_{1},\dots,w_{r}]\), normalized such that \(W_{r}^{*}K_{\tilde{X},X}K_{Y,\tilde{Y}}K_{\tilde{Y},\tilde{Y}}^{\dagger}K_{ \tilde{Y},\tilde{Y}}K_{X,\tilde{X}}W_{r}=I\). Then let \(D_{r}:=K_{\tilde{Y},\tilde{Y}}^{\dagger}K_{\tilde{Y},Y}K_{X,\tilde{X}}W_{r}\) and \(E_{r}:=(K_{\tilde{X},X}K_{X,\tilde{X}}+n\lambda K_{\tilde{X},\tilde{X}})^{ \dagger}K_{\tilde{X},X}K_{Y,\tilde{Y}}U_{r}\), such that the following holds_

\[\hat{A}_{m,\lambda}^{\text{RRR}}=\widetilde{\Phi}_{Y}D_{r}E_{r}^{*}\widetilde{ \Phi}_{X}^{*}.\] (31)Proof of Proposition c.4.: Let \(B:=n^{1/2}P_{Y}\hat{C}_{YX}P_{X}(P_{X}\hat{C}P_{X}+\lambda I)^{-1/2}\). The computationally intensive part for this estimator is in evaluating the rank-r truncation \([\![B]\!]_{r}\). Its singular values and left singular vectors can be obtained by solving the symmetric eigenvalue problem \(BB^{*}q_{i}=\sigma_{i}^{2}q_{i}\). We rewrite \(BB^{*}\)

\[BB^{*} =P_{Y}\hat{\Phi}_{Y|X}\hat{\Phi}_{Y}^{*}P_{X}(P_{X}\hat{\Phi}_{X} \hat{\Phi}_{X}^{*}P_{X}+n\lambda I)^{-1}P_{X}\hat{\Phi}_{X}\hat{\Phi}_{Y|X}^{*}P _{Y}\] \[=P_{Y}\hat{\Phi}_{Y|X}\hat{\Phi}_{Y}^{*}P_{X}\hat{\Phi}_{X}(\hat{ \Phi}_{X}^{*}P_{X}\hat{\Phi}_{X}+n\lambda I)^{-1}\hat{\Phi}_{Y|X}^{*}P_{Y}\] \[=P_{Y}\hat{\Phi}_{Y|X}K_{X,\tilde{X}}K_{\tilde{X},\tilde{X}}^{ \dagger}K_{\tilde{X},X}(K_{X,\tilde{X}}K_{\tilde{X},\tilde{X}}^{\dagger}K_{ \tilde{X},X}+n\lambda I)^{-1}\hat{\Phi}_{Y|X}^{*}P_{Y}\] \[=P_{Y}\hat{\Phi}_{Y|X}K_{X,\tilde{X}}K_{\tilde{X},\tilde{X}}^{ \dagger}(K_{\tilde{X},X}K_{X,\tilde{X}}K_{\tilde{X},\tilde{X}}+n\lambda I)^{- 1}K_{\tilde{X},\tilde{X}}\hat{\Phi}_{Y|X}^{*}P_{Y}\] \[=P_{Y}\hat{\Phi}_{Y|X}K_{X,\tilde{X}}(K_{\tilde{X},X}K_{X,\tilde{ X}}+n\lambda K_{\tilde{X},\tilde{X}})^{\dagger}K_{\tilde{X},X}\hat{\Phi}_{Y|X}^{*}P _{Y}\]

where the second and fourth equalities are applications of the push-through identity, the third by definition of projections and kernel matrices, and the last by collecting \(K_{\tilde{X},\tilde{X}}\). By construction, the non-trivial eigenfunctions of \(BB^{*}\) are in the range of \(P_{Y}\hat{\Phi}_{Y|X}K_{X,\tilde{X}}\), therefore we can set \(q_{i}=P_{Y}\hat{\Phi}_{Y|X}K_{X,\tilde{X}}w_{i}\) for some \(w_{i}\in\mathbb{R}^{m}\), and solve the following eigenvalue problem instead

\[P_{Y}\hat{\Phi}_{Y|X}K_{X,\tilde{X}}(K_{\tilde{X},X}K_{X,\tilde{ X}}+n\lambda K_{\tilde{X},\tilde{X}})^{\dagger}K_{\tilde{X},X}\hat{\Phi}_{Y|X}^{*}P _{Y}\hat{\Phi}_{Y|X}K_{X,\tilde{X}}w_{i} =\sigma_{i}^{2}P_{Y}\hat{\Phi}_{Y|X}K_{X,\tilde{X}}w_{i}\] \[(K_{\tilde{X},X}K_{X,\tilde{X}}+n\lambda K_{\tilde{X},\tilde{X}} )^{\dagger}K_{\tilde{X},X}K_{Y,\tilde{Y}}K_{\tilde{Y},\tilde{Y}}^{\dagger}K_{ \tilde{Y},Y}K_{X,\tilde{X}}w_{i} =\sigma_{i}^{2}w_{i}\]

where we have simplified the left term of both sides of the equation.

The eigenfunctions of \(BB^{*}\) are therefore \(q_{i}=P_{Y}\hat{\Phi}_{Y|X}K_{X,\tilde{X}}w_{i}\), which must be normalized as

\[\|q_{i}\|^{2}=w_{i}^{\top}K_{\tilde{X},X}K_{Y,\tilde{Y}}K_{\tilde{Y},\tilde{Y }}^{\dagger}K_{\tilde{Y},Y}K_{X,\tilde{X}}w_{i}=1.\]

Thanks to this normalization, the projector onto the \(r\) leading left singular vectors of \(B\) is \(Q_{r}Q_{r}^{*}\), where \(Q_{r}=[q_{1},\ldots,q_{r}]\). Then the NysRRR estimator can be written as

\[Q_{r}Q_{r}^{*}B(P_{X}\hat{C}P_{X}+\lambda I)^{-1/2}\]

where

\[B(P_{X}\hat{C}P_{X}+\lambda I)^{-1/2} =P_{Y}\hat{C}_{YX}P_{X}(P_{X}\hat{C}P_{X}+\lambda I)^{-1}\] \[=P_{Y}\hat{\Phi}_{Y|X}K_{X,\tilde{X}}(K_{\tilde{X},X}K_{X,\tilde{ X}}+n\lambda K_{\tilde{X},\tilde{X}})^{-1}\widetilde{\Phi}_{X}^{*}.\]

with the same techniques we used for rewriting \(BB^{*}\). Finally, let \(D_{r}\) and \(E_{r}\) as in the statement. We can apply the projection to obtain

\[Q_{r}Q_{r}^{*}B(P_{X}\hat{\Phi}_{X}\hat{\Phi}_{X}^{*}P_{X}+n\lambda I)^{-1/2}= \widetilde{\Phi}_{Y}D_{r}E_{r}^{*}\widetilde{\Phi}_{X}^{*}.\]

## Appendix D Forecasting & Koopman Modes

The three estimators considered in Section C are all of the form

\[\hat{A}_{\lambda}=\widetilde{\Phi}_{Y}W\widetilde{\Phi}_{X}^{*},\qquad W\in \mathbb{R}^{m\times m}.\]

We will use this generic form to provide expressions for the following operations:

1. producing forecasts of the dynamical system at a future time,
2. computing the approximate eigenvalues and eigenfunctions of the Koopman operator,
3. computing the Koopman modes.

### Forecasting

Given a new data-point \(x\in\mathcal{X}\) and an observable function \(g\in\mathcal{H}\) (note that this can simply be the identity function), we can approximate the one-step-ahead expectation \((\mathcal{K}_{\pi}g)(\bm{x})\) by using the obtained estimators \(\hat{A}^{*}\). Note that by the reproducing property \(\widetilde{\Phi}_{Y}^{*}g=[g(\bm{y}_{i}),\ldots,g(\bm{y}_{m})]^{\top}=:g_{m}\), then

\[(\hat{A}^{*}g)(x)=(\widetilde{\Phi}_{X}W^{\top}\widetilde{\Phi}_{Y}^{*}g)(\bm{x })=(\widetilde{\Phi}_{X}W^{\top}g_{m})(\bm{x})=\sum_{i=1}^{m}(W^{\top}g_{m})_{ i}k(\tilde{\bm{x}}_{i},\bm{x}).\]

### Eigenfunctions and eigenvalues

We wish to compute the eigenfunctions \(\xi,\psi\in\mathcal{H}\), as well as the eigenvalues \(\lambda_{i}\) of \(\hat{A}\). The left eigenfunctions satisfy \(\hat{A}^{*}\xi_{i}=\bar{\lambda}_{i}\xi_{i}\) and the right eigenfunctions satisfy \(\hat{A}\psi_{i}=\lambda_{i}\psi_{i}\). In the following we will use Mollenhauer et al. [43, Proposition 3] to manipulate the eigendecomposition of operators in \(\mathcal{H}\).

Consider the decomposition \(W=U_{r}V_{r}^{*}\) with \(U_{r},V_{r}\in\mathbb{C}^{m\times r}\), which is available for all considered estimators with \(r\leq m\). For example, in the Nystrom RRR estimator of proposition C.4, we can simply take \(U_{r}=D_{r}\) and \(V_{r}=E_{r}\). For the Nystrom KRR estimator instead, \(r=m\) and we can take the whole of \(W\) as our \(U_{r}\) and \(V_{r}=I\).

To compute the **right eigenfunctions**\(\psi_{i}\), such that \((\widetilde{\Phi}_{Y}U_{r}V_{r}^{*}\widetilde{\Phi}_{X}^{*})\psi_{i}=\lambda_{ i}\psi_{i}\), consider the following equivalent eigendecomposition

\[V_{r}^{*}\widetilde{\Phi}_{X}^{*}\widetilde{\Phi}_{Y}U_{r}\tilde{g}_{i}= \lambda_{i}\tilde{g}_{i},\quad\text{where }\psi_{i}=\widetilde{\Phi}_{Y}U_{r}\tilde{g}_{i}.\]

Note that \(\widetilde{\Phi}_{X}^{*}\widetilde{\Phi}_{Y}=K_{\bar{X},\bar{Y}}\) is a finite-dimensional object which can easily be computed. The eigenfunctions \(\psi_{i}\) must be normalized such that \(\psi_{i}^{*}\psi_{i}=1\) for every \(i\), so we must have

\[\tilde{g}_{i}^{*}U_{r}^{*}\widetilde{\Phi}_{Y}^{*}\widetilde{\Phi}_{Y}U_{r} \tilde{g}_{i}=1.\]

A very similar process can be followed to obtain the **left eigenfunctions**\(\xi_{i}\), such that \(\widetilde{\Phi}_{X}V_{r}U_{r}^{*}\widetilde{\Phi}_{Y}^{*}\xi_{i}=\bar{ \lambda}_{i}\xi_{i}\). Here we consider instead

\[U_{r}^{*}\widetilde{\Phi}_{Y}^{*}\widetilde{\Phi}_{X}V_{r}\tilde{h}_{i}= \bar{\lambda}_{i}\tilde{h}_{i},\quad\text{where }\xi_{i}=\widetilde{\Phi}_{X}V_{r}\tilde{h}_{i}.\]

where once again, \(\widetilde{\Phi}_{Y}^{*}\widetilde{\Phi}_{X}=K_{\bar{X},\bar{Y}}^{\top}\) and the eigenfunctions must be normalized such that \(\tilde{h}_{i}^{*}V_{r}^{*}\widetilde{\Phi}_{X}^{*}\widetilde{\Phi}_{X}V_{r} \tilde{h}_{i}=1\) for every \(i\). Finally, \(\psi\) and \(\xi\) must be orthogonal to each other: we must have for \(i,j\in[r]\) that \(\langle\psi_{i},\bar{\xi}_{j}\rangle_{\mathcal{H}}=\delta_{ij}\) (where \(\delta_{ij}\) is a Dirac delta equals to 1 when \(i=j\) and 0 otherwise). We can compute

\[\langle\psi_{i},\bar{\xi}_{j}\rangle_{\mathcal{H}}=\tilde{h}_{i}^{*}V_{r}^{*} K_{\bar{X},\bar{Y}}U_{r}\tilde{g}_{i}=\lambda_{j}\tilde{h}_{i}^{*}\tilde{g}_{j},\]

and note that \(\tilde{h}_{i}^{*}\tilde{g}_{j}=\delta_{ij}\), but we must normalize \(\xi\) such that

\[\xi_{i}=\widetilde{\Phi}_{X}V_{r}\tilde{h}_{i}/\bar{\lambda}_{i}.\]

### Koopman modes

Given the eigendecomposition of any estimator \(\hat{A}\) as \(\hat{A}_{r}=\sum_{i=1}^{r}\lambda_{i}\psi_{i}\otimes\bar{\xi}_{i}\), for an observable \(g\) we have the following

\[\hat{A}_{r}^{*}g=\sum_{i=1}^{r}\lambda_{i}\xi_{i}\langle g,\bar{\psi}_{i} \rangle_{\mathcal{H}}\]

where \(\langle g,\bar{\psi}_{i}\rangle_{\mathcal{H}}=\gamma_{i}^{g}\) are the Koopman modes. Expanding the definition of \(\psi_{i}\) we get

\[\gamma_{i}^{g}=\langle g,\bar{\psi}_{i}\rangle_{\mathcal{H}}=\tilde{g}_{i}^{*}U _{r}^{*}\widetilde{\Phi}_{Y}^{*}g=\tilde{g}_{i}^{*}U_{r}^{*}g_{m}\in\mathbb{C} ^{m}\]

which we can efficiently compute.

## Appendix E Excess risk of the Nystrom KRR estimator

**Lemma E.1** (**Excess risk decomposition in operator norm for KRR):**_Let Assumptions 4.1 to 4.3 and 4.5 hold. Then the Nystrom KRR estimator (8) satisfies almost surely_

\[\mathcal{E}(\hat{A}^{\text{KRR}}_{m,\lambda})^{1/2} \leq a\lambda^{1/2}+a\theta_{1}^{2}\|(\hat{C}_{\lambda}-C_{ \lambda})C_{\lambda}^{-1/2}\|_{\mathcal{B}(\mathcal{H})}+\theta_{1}^{2}\|(C_{ XX}-\hat{C}_{YX})C_{\lambda}^{-1/2}\|_{\mathcal{B}(\mathcal{H})}\] \[\quad+a\theta_{1}\theta_{2}\theta_{3}\|P_{X}^{\perp}C_{\lambda} ^{1/2}\|_{\mathcal{B}(\mathcal{H})}+\theta_{1}^{2}\|P_{Y}^{\perp}C_{\lambda}^ {1/2}\|_{\mathcal{B}(\mathcal{H})}\]

_where \(\theta_{1}:=\|\hat{C}_{\lambda}^{-1/2}C_{\lambda}^{1/2}\|\), \(\theta_{2}:=\|\hat{C}_{\lambda}^{1/2}C_{\lambda}^{-1}\|\), \(\theta_{3}:=\|\hat{C}_{\lambda}^{-1}C_{\lambda}\|\), and \(a\) is the constant of Assumption 4.5._

_Proof of Lemma E.1:_ Let \(\theta_{1}:=\|\hat{C}_{\lambda}^{-1/2}C_{\lambda}^{1/2}\|\), \(\theta_{3}:=\|\hat{C}_{\lambda}^{-1}C_{\lambda}\|\). As in Lemma C.1 define \(g_{\text{KRR}}(\hat{C}):=U(U^{*}\hat{C}U+\lambda I)^{-1}U^{*}\). We have

\[\mathcal{E}(\hat{A}^{\text{KRR}}_{m,\lambda})^{1/2} =\|\Phi_{Y|X}-\hat{A}^{\text{KRR}}_{m,\lambda}\Phi_{X}\|_{\mathcal{ B}(L_{\pi}^{2},\mathcal{H})}\] \[\leq\|\Phi_{Y|X}-A_{\lambda}\Phi_{X}\|_{\mathcal{B}(L_{\pi}^{2}, \mathcal{H})}+\|(A_{\lambda}-C_{YX}g_{\text{KRR}}(\hat{C}))\Phi_{X}\|_{ \mathcal{B}(L_{\pi}^{2},\mathcal{H})}\] \[\quad+\|(C_{YX}g_{\text{KRR}}(\hat{C})-\hat{A}^{\text{KRR}}_{m, \lambda})\Phi_{X}\|_{\mathcal{B}(L_{\pi}^{2},\mathcal{H})}\] \[\leq\underbrace{\|\Phi_{Y|X}-A_{\lambda}\Phi_{X}\|_{\mathcal{B}(L _{\pi}^{2},\mathcal{H})}}_{A}+\underbrace{\|(A_{\lambda}-C_{YX}g_{\text{KRR}} (\hat{C}))C^{1/2}\|}_{B}\] \[\quad+\underbrace{\|(C_{YX}g_{\text{KRR}}(\hat{C})-\hat{A}^{ \text{KRR}}_{m,\lambda})C^{1/2}\|}_{C}\] (32)

where we used the polar decomposition \(\Phi_{X}^{*}=WC^{1/2}\) for some partial isometry \(W:\mathcal{H}\to L_{\pi}^{2}\).

**The first term** is

\[\|\Phi_{Y|X}-A_{\lambda}\Phi_{X}\| =\|\Phi_{X}K_{\pi}^{*}-C_{YX}C_{\lambda}^{-1}\Phi_{X}\|\] \[\leq a\lambda^{1/2}+\|(I-P_{\mathcal{H}})\Phi_{Y|X}\|\]

where we used the definition of \(\Phi_{Y|X}\) and applied Lemma H.1.

**The second term** of our decomposition (32) can be bounded as follows:

\[B =\|C_{YX}(C_{\lambda}^{-1}-g_{\text{KRR}}(\hat{C}))C^{1/2}\|\] (by Lemma H.2:) \[\leq a\|C(C_{\lambda}^{-1}-g_{\text{KRR}}(\hat{C}))C^{1/2}\|\] \[\leq a\left(\underbrace{\|C(C_{\lambda}^{-1}-\hat{C}_{\lambda}^{- 1})C^{1/2}\|}_{B_{1}}+\underbrace{\|C(\hat{C}_{\lambda}^{-1}-g_{\text{KRR}} (\hat{C}))C^{1/2}\|}_{B_{2}}\right)\]

We now bound the terms \(B_{1}\) and \(B_{2}\) separately.

\[B_{1} =\|C(C_{\lambda}^{-1}-\hat{C}_{\lambda}^{-1})C^{1/2}\|\] \[=\|CC_{\lambda}^{-1}(\hat{C}_{\lambda}-C_{\lambda})\hat{C}_{ \lambda}^{-1}C^{1/2}\|\] \[\leq\|CC_{\lambda}^{-1}\|\|(\hat{C}_{\lambda}-C_{\lambda})C_{ \lambda}^{-1/2}\|\|C_{\lambda}^{1/2}\hat{C}_{\lambda}^{-1/2}\|\|\hat{C}_{ \lambda}^{-1/2}C^{1/2}\|\] \[\leq\theta_{1}^{2}\|(\hat{C}_{\lambda}-C_{\lambda})C_{\lambda}^{-1 /2}\|\]

Let \(\hat{P}_{\lambda}:=\hat{C}_{\lambda}^{1/2}g_{\text{KRR}}(\hat{C})\hat{C}_{ \lambda}^{1/2}\). We recall that \(g_{\text{KRR}}(\hat{C})=P_{X}g_{\text{KRR}}(\hat{C})\), so that

\[\hat{P}_{\lambda}^{2} =\hat{C}_{\lambda}^{1/2}(g_{\text{KRR}}(\hat{C})\hat{C}_{\lambda}P_ {X})g_{\text{KRR}}(\hat{C})\hat{C}_{\lambda}^{1/2}\] \[=\hat{C}_{\lambda}^{1/2}P_{X}g_{\text{KRR}}(\hat{C})\hat{C}_{ \lambda}^{1/2}\] \[=\hat{P}_{\lambda}.\]This implies \(\hat{P}_{\lambda}^{2}=\hat{P}_{\lambda}=\hat{P}_{\lambda}^{*}\). Hence \(\hat{P}_{\lambda}\) is an orthogonal projection, and defining \(\hat{P}_{\lambda}^{\perp}=I-\hat{P}_{\lambda}\) it holds \(\|\hat{P}_{\lambda}^{\perp}\|_{\mathcal{B}(\mathcal{H})}\leq 1\). We can thus bound \(B_{2}\) as follows:

\[B_{2} =\|C(\hat{C}_{\lambda}^{-1}-g_{\text{KRR}}(\hat{C}))C^{1/2}\|\] \[=\|C\hat{C}_{\lambda}^{-1/2}(I-\hat{P}_{\lambda})\hat{C}_{\lambda }^{-1/2}C^{1/2}\|\] (by Lemma I.1) \[=\|C\hat{C}_{\lambda}^{-1}P_{\lambda}^{\perp}\hat{C}_{\lambda}^{ 1/2}(I-\hat{P}_{\lambda})\hat{C}_{\lambda}^{-1/2}C^{1/2}\|\] \[=\|C\hat{C}_{\lambda}^{-1}\|_{\mathcal{B}(\mathcal{H})}\|P_{X}^{ \perp}C_{\lambda}^{1/2}\|_{\mathcal{B}(\mathcal{H})}\|C_{\lambda}^{-1/2}\hat{ C}_{\lambda}^{1/2}\|_{\mathcal{B}(\mathcal{H})}\|I-\hat{P}_{\lambda}\|\|\hat{C}_{ \lambda}^{-1/2}C^{1/2}\|_{\mathcal{B}(\mathcal{H})}\] \[\leq\theta_{1}\theta_{2}\theta_{3}\|P_{X}^{\perp}C_{\lambda}^{1/2}\|\]

For the third term, due to Lemma C.1:

\[C =\|(C_{YX}-P_{Y}\hat{C}_{YX})g_{\text{KRR}}(\hat{C})C^{1/2}\|\] (33) \[\leq\|(C_{YX}-P_{Y}\hat{C}_{YX})C_{\lambda}^{-1/2}\|\|C_{\lambda }^{1/2}\hat{C}_{\lambda}^{-1/2}\|_{\mathcal{B}(\mathcal{H})}\|\hat{P}_{\lambda} \|_{\mathcal{B}(\mathcal{H})}\|\hat{C}_{\lambda}^{-1/2}C^{1/2}\|_{\mathcal{B}( \mathcal{H})}\] (34) \[\leq\theta_{1}^{2}\|(C_{YX}-P_{Y}\hat{C}_{YX})C_{\lambda}^{-1/2}\| _{\mathcal{B}(\mathcal{H})}\] (35) \[\leq\theta_{1}^{2}\Big{(}\|P_{Y}^{\perp}C_{\lambda}^{1/2}\|_{ \mathcal{B}(\mathcal{H})}+\|(C_{YX}-\hat{C}_{YX})C_{\lambda}^{-1/2}\|_{\mathcal{ B}(\mathcal{H})}\Big{)}\] (36)

where we used Lemma J.7 for the last inequality.

Starting again from (32) and putting everything together, we get

\[\mathcal{E}(\hat{A}_{m,\lambda}^{\text{KRR}})^{1/2} \leq a\lambda^{1/2}\] \[\quad+a\theta_{1}^{2}\|(\hat{C}_{\lambda}-C_{\lambda})C_{\lambda }^{-1/2}\|_{\mathcal{B}(\mathcal{H})}\] \[\quad+\theta_{1}^{2}\|(C_{YX}-\hat{C}_{YX})C_{\lambda}^{-1/2}\|_{ \mathcal{B}(\mathcal{H})}\] \[\quad+a\theta_{1}\theta_{2}\theta_{3}\|P_{X}^{\perp}C_{\lambda}^{ 1/2}\|_{\mathcal{B}(\mathcal{H})}\] \[\quad+\theta_{1}^{2}\|P_{Y}^{\perp}C_{\lambda}^{1/2}\|_{\mathcal{ B}(\mathcal{H})}.\]

### Excess risk rates for KRR

In order to control the terms appearing in our decomposition, we recall that Assumption 4.4 implies

\[d_{\text{eff}}(\lambda)\leq C_{\beta}\lambda^{-\beta}\text{ where }C_{\beta}:=\begin{cases}\dfrac{c}{1-\beta}&,\beta<1\\ K^{2}&,\beta=1\end{cases},\] (37)

where \(c\) is the constant of Assumption 4.4, see [9, Proposition 3 with \(b\to 1/\beta\) and \(\beta\to c\)] and [18, Lemma 11] which shows that the existence of a constant \(C_{\beta}\) such that the first part of (37) holds implies in return \(\lambda_{i}(C)\lesssim i^{-1/\beta}\).

Proof of Theorem 4.6:.: By Lemma E.1 taking \(P_{X}=P_{Y}\), it holds almost surely

\[\mathcal{E}(\hat{A}_{m,\lambda}^{\text{KRR}})^{1/2} \leq a\lambda^{1/2}+a\theta_{1}^{2}\|(\hat{C}_{\lambda}-C_{\lambda})C_{ \lambda}^{-1/2}\|_{\mathcal{B}(\mathcal{H})}+\theta_{1}^{2}\|(C_{YX}-\hat{C}_{ YX})C_{\lambda}^{-1/2}\|_{\mathcal{B}(\mathcal{H})}\] \[\quad+a\theta_{1}\theta_{2}\theta_{3}\|P_{X}^{\perp}C_{\lambda}^{ 1/2}\|_{\mathcal{B}(\mathcal{H})}+\theta_{1}^{2}\|P_{Y}^{\perp}C_{\lambda}^{1/2 }\|_{\mathcal{B}(\mathcal{H})}\]

and we recall that \(\theta_{1}:=\|\hat{C}_{\lambda}^{-1/2}C_{\lambda}^{1/2}\|\) and \(\theta_{3}:=\|\hat{C}_{\lambda}^{-1}C_{\lambda}\|\). We bound separately the terms appearing in this expression.

Bound of \(\theta_{1}\) and \(\theta_{2}\).We control these term by bounding \(\|C_{\lambda}^{-1/2}(\hat{C}-C)C_{\lambda}^{-1/2}\|\). By Lemma J.3 it holds for any \(\delta^{\prime}\in]0,1[\) and any \(\lambda\in]0,\|C\|_{\mathcal{B}(\mathcal{H})}\) with probability \(1-\delta^{\prime}\)

\[\Big{\|}C_{\lambda}^{-1/2}(\hat{C}-C)C_{\lambda}^{-1/2}\Big{\|} \leq\frac{4c_{\tau}\beta}{3n\lambda^{\tau}}+\sqrt{\frac{2c_{\tau}\beta}{n \lambda^{\tau}}}\quad\text{where}\quad\beta=\log\Bigl{(}\tfrac{8K^{2}}{\delta^ {\prime}\lambda}\Bigr{)}\] (38)

A sufficient condition to bound the right hand side of the previous expression by \(1/4\) is to have \(n\lambda^{\tau}>32c_{\tau}\beta\) (in which cases both terms are bounded by \(1/8\)). Assuming this holds, \(I-\|C_{\lambda}^{-1/2}(\hat{C}-C)C_{\lambda}^{-1/2}\|\)\(C)C_{\lambda}^{-1/2}\|\) is invertible and we also have

\[\theta_{2}^{2}=\|\hat{C}_{\lambda}^{1/2}C_{\lambda}^{-1/2}\|^{2} =\|C_{\lambda}^{-1/2}\hat{C}_{\lambda}C_{\lambda}^{-1/2}\|=\|I-C_{ \lambda}^{-1/2}(C-\hat{C})C_{\lambda}^{-1/2}\|\] \[\leq 1+\|C_{\lambda}^{-1/2}(C-\hat{C})C_{\lambda}^{-1/2}\|\] \[\leq 1.25\] \[\text{and thus}\quad\theta_{2} \leq 1.12\] \[\text{while}\quad\theta_{1}^{2}=\|\hat{C}_{\lambda}^{-1/2}C_{ \lambda}^{1/2}\|^{2} =\|(C_{\lambda}^{-1/2}\hat{C}_{\lambda}C_{\lambda}^{-1/2})^{-1}\|\] \[\overset{(i)}{\leq}(1-\|C_{\lambda}^{-1/2}(\hat{C}-C)C_{\lambda} ^{-1/2}\|)^{-1}\] \[\leq 1.34\] \[\text{and thus}\quad\theta_{1} \leq 1.16\]

where \((i)\) can be obtained by taking the Neumann expansion of \(I-\|C_{\lambda}^{-1/2}(\hat{C}-C)C_{\lambda}^{-1/2}\|\).

Bound for \(\theta_{3}\).By Lemma J.5 it holds with probability \(1-\delta^{\prime}\)

\[\|(C-\hat{C})C_{\lambda}^{-1}\|_{\text{op}}\leq\frac{2K\sqrt{c_{\tau}}\log( \nicefrac{{2}}{{\delta^{\prime}}})}{\lambda^{(\tau+1)/2}n}+\sqrt{\frac{2K^{2 }\operatorname{tr}(C_{\lambda}^{-2}C)\log(\nicefrac{{2}}{{\delta^{\prime}}})} {n}}\] (39)

Both terms in the above rhs are bounded by \(1/4\) provided

\[\lambda^{(\tau+1)/2}n \geq 8K\sqrt{c_{\tau}}\log(\nicefrac{{2}}{{\delta^{\prime}}})\] \[n \geq 32K^{2}\lambda^{-(1+\beta)}\log(\nicefrac{{2}}{{\delta^{ \prime}}})\]

where we used \(\operatorname{tr}(C_{\lambda}^{-2}C)=\sum\lambda_{i}(C)(\lambda_{i}(C)+ \lambda)^{-2}\leq\lambda^{-1}\operatorname{tr}(C_{\lambda}^{-1}C)\leq C_{ \beta}\lambda^{-(1+\beta)}\). When this is the case, we have \(\|(C-\hat{C})C_{\lambda}^{-1}\|_{\text{op}}\leq 1/2<1\) and the operator \(I-(\hat{C}_{\lambda}-C_{\lambda})C_{\lambda}^{-1}\) is invertible.

\[\theta_{3} =\|(\hat{C}_{\lambda}C_{\lambda}^{-1})^{-1}\|=\|(I-(\hat{C}_{ \lambda}-C_{\lambda})C_{\lambda}^{-1})^{-1}\|\] \[\overset{(i)}{\leq}(1-\|(\hat{C}_{\lambda}-C_{\lambda})C_{\lambda }^{-1}\|_{\mathcal{B}(\mathcal{H})})^{-1}\] \[\leq 2.\]

where \((i)\) can be obtained by considering the Neumann expansion of \(I-(\hat{C}_{\lambda}-C_{\lambda})C_{\lambda}^{-1}\).

Bound for \(\|P_{X}^{\perp}C_{\lambda}^{1/2}\|\).By Lemma J.6, provided \(\lambda\in]0,\|C\|_{\mathcal{B}(\mathcal{H})}\) it holds with probability \(1-\delta^{\prime}\)

\[\|P_{X}^{\perp}C_{\lambda}^{1/2}\|_{\mathcal{B}(\mathcal{H})}\leq\sqrt{3\lambda}\]

provided \(m\geq\max(67,5\operatorname{ess\,sup}_{x\sim\pi}\|C_{\lambda}^{-1/2}\phi(x)\| ^{2})\log\frac{4K^{2}}{\lambda\delta^{\prime}}\), which by Lemma H.3 is ensured if \(m\geq\max(67,5\frac{c_{\tau}}{\lambda^{\prime}})\log\frac{4K^{2}}{\lambda \delta^{\prime}}\).

Bound for \(\|(C-\hat{C})C_{\lambda}^{-1/2}\|\) and \(\|(C_{YX}-\hat{C}_{YX})C_{\lambda}^{-1/2}\|\).By Lemma J.4, for any \(\delta^{\prime}\in]0,1[\), each of the following events holds with probability \(1-2\delta^{\prime}\):

\[\max(\|(C-\hat{C})C_{\lambda}^{-1/2}\|,\|(C_{YX}-\hat{C}_{YX})C_{\lambda}^{-1 /2}\|)\leq\frac{2K\sqrt{c_{\tau}}\log(\nicefrac{{2}}{{\delta^{\prime}}})}{ \lambda^{\tau/2}n}+\sqrt{\frac{2K^{2}d_{\text{eff}}(\lambda)\log( \nicefrac{{2}}{{\delta^{\prime}}})}{n}}\] (40)

By Eq. (37) we have \(d_{\text{eff}}(\lambda)\leq C_{\beta}\lambda^{-\beta}\).

Choosing \(\delta^{\prime}=\delta/5\), we get via a union bound with probability \(1-\delta\) that \(\theta_{1}\theta_{2}\theta_{3}\leq 2.6\), \(\theta_{1}^{2}\leq 1.34\) and

\[\mathcal{E}(\hat{A}_{m,\lambda}^{\text{KRR}})^{1/2} \leq a\lambda^{1/2}+1.34(a+1)\Bigg{(}\frac{2K\sqrt{c_{\tau}}\log( \nicefrac{{2}}{{\delta^{\prime}}})}{\lambda^{\tau/2}n}+\sqrt{\frac{2K^{2}C_{ \beta}\log(\nicefrac{{2}}{{\delta^{\prime}}})}{n\lambda^{\beta}}}\Bigg{)}+(2. 6a+1.34)\sqrt{3}\lambda^{1/2}\] \[\leq c_{1}\lambda^{1/2}+c_{2}\lambda^{-\tau/2}n^{-1}+c_{3}\lambda^{- \beta/2}n^{-1/2}\] \[\text{where:}\quad c_{1} :=(5.5a+2.33)\] \[c_{2} :=1.34(a+1)2K\sqrt{c_{\tau}}\log(\nicefrac{{2}}{{\delta^{\prime}}})\] \[c_{3} :=1.34(a+1)\sqrt{2K^{2}C_{\beta}\log(\nicefrac{{2}}{{\delta^{ \prime}}})}\]for any \(\lambda\) and \(m\) satisfying the constraints

\[\left\{\begin{aligned} \lambda&>n^{-1/\tau}(32c_{\tau})^{ 1/\tau}\log\Bigl{(}\tfrac{8K^{2}}{\delta^{\prime}\lambda}\Bigr{)}^{1/\tau}\\ \lambda&\geq n^{-2/(\tau+1)}(8K\sqrt{c_{\tau}}\log (\nicefrac{{2}}{{\delta^{\prime}}}))^{2/(\tau+1)}\\ \lambda&\geq n^{-1/(1+\beta)}(32K^{2}\log( \nicefrac{{2}}{{\delta^{\prime}}}))^{1/(1+\beta)}\\ \lambda&\in]0,K^{2}].\\ m&\geq\max(67,5\frac{c_{\tau}}{\lambda^{\tau}}) \log\frac{4K^{2}}{\lambda\delta^{\prime}}\quad\text{(uniform sampling)}\end{aligned}\right.\] (41)

We pick \(\boxed{\lambda:=c_{\lambda}n^{-1/(1+\beta)}}\) which is asymptotically the saturating constraint (given that \(1/(1+\beta)<1<2/(\tau+1)\leq 1/\tau\)), where \(c_{\lambda}\) is a constant choosen to enforce the following equations (which are sufficient conditions for eq. (41) to hold):

\[\left\{\begin{aligned} c_{\lambda}^{\tau}n^{1-\tau/(1+ \beta)}&>(32c_{\tau})\log\Bigl{(}\tfrac{8K^{2}n^{1/(1+\beta)}}{ \delta^{\prime}c_{\lambda}}\Bigr{)}\\ c_{\lambda}^{(\tau+1)/2}n^{1-(\tau+1)/(2(1+\beta))}& \geq 8K\sqrt{c_{\tau}}\log(\nicefrac{{2}}{{\delta^{\prime}}})\\ c_{\lambda}&\geq(32K^{2}\log(\nicefrac{{2}}{{\delta^{ \prime}}}))^{1/(1+\beta)}\\ c_{\lambda}n^{-1/(1+\beta)}&\leq K^{2}\end{aligned}\right.\] (42)

As \(1-(\tau+1)/(2(1+\beta))>0\), a sufficient condition for the second equation is

\[c_{\lambda}\geq(8K\sqrt{c_{\tau}}\log(\nicefrac{{2}}{{\delta^{\prime}}}))^{2 /(\tau+1)}.\]

Assuming \(c_{\tau}\geq 8K^{2}\), a sufficient condition to satisfy the first constraint is

\[c_{\lambda}^{\tau}n^{1-\tau/(1+\beta)}>(32c_{\tau})2\max(\log\Bigl{(}n^{1/(1+ \beta)}\Bigr{)},\log\Bigl{(}(\delta^{\prime})^{-1}\Bigr{)})\]

which is in particular ensured (noting that \(\log(n)/n^{\nu}\leq 1/(\nu e)\) for any \(n,\nu>0\)) whenever

\[c_{\lambda}>(64c_{\tau}\max((e(1+\beta-\tau))^{-1},\log\bigl{(}\nicefrac{{1}} {{\delta^{\prime}}}\bigr{)}))^{1/\tau}\]

Noting that \(1+\beta-\tau\leq 1\), we get that

\[c_{\lambda}>(64c_{\tau}(e(1+\beta-\tau))^{-1}\log\bigl{(}\nicefrac{{1}}{{ \delta^{\prime}}}\bigr{)})^{1/\tau}\]

is also sufficient. We recall that \(1/(1+\beta)<1<2/(\tau+1)\leq 1/\tau\), so that we can choose

\[\boxed{c_{\lambda}:=\log(\nicefrac{{2}}{{\delta^{\prime}}})^{1/\tau}\max((32 K^{2})^{1/(1+\beta)},(8K\sqrt{c_{\tau}})^{2/(\tau+1)},(64c_{\tau}(e(1+\beta- \tau))^{-1})^{1/\tau},8K^{2})}\]

while the last constraint \(n\geq(c_{\lambda}/K^{2})^{1+\beta}\) is satisfied by assumption.

\[\mathcal{E}(\mathring{A}^{\text{RRR}}_{m,\lambda})^{1/2} \leq c_{1}\lambda^{1/2}+c_{2}\lambda^{-\tau/2}n^{-1}+c_{3}\lambda ^{-\beta/2}n^{-1/2}\] \[\leq c_{1}c_{\lambda}^{1/2}n^{-1/(2(1+\beta))}+c_{2}c_{\lambda}^ {-\tau/2}n^{\tau/(2(1+\beta))-1}+c_{3}c_{\lambda}^{-\beta/2}n^{\beta/(2(1+ \beta))-1/2}\] \[\leq c_{1}c_{\lambda}^{1/2}n^{-1/(2(1+\beta))}+c_{2}c_{\lambda}^ {-\tau/2}n^{-(1+2\beta+(1-\tau))/(2(1+\beta))}+c_{3}c_{\lambda}^{-\beta/2}n^{ -1/(2(1+\beta))}\] \[\leq(c_{1}c_{\lambda}^{1/2}+c_{2}c_{\lambda}^{-\tau/2}+c_{3}c_{ \lambda}^{-\beta/2})n^{-1/(2(1+\beta))}.\]

which gives the claimed result. The last constraint (on \(m\)) is satisfied by the assumptions of the lemma.

Excess risk of the Nystrom RRR estimator

Recalling (30), NystromRRR estimator is of the form \(\hat{A}^{\text{RRR}}_{m,\lambda}=\llbracket\tilde{B}\rrbracket_{\text{r}}(\tilde{ C}_{\lambda})^{-1/2}\), where \(\tilde{B}:=\tilde{C}_{YX}(\tilde{C}_{\lambda})^{-1/2}\) for \(\tilde{C}_{YX}:=P_{Y}\hat{C}_{YX}P_{X}\) and \(\tilde{C}_{\lambda}:=P_{X}\hat{C}P_{X}+\lambda I\). While the population version is \(A^{\text{RRR}}_{\lambda}:=\llbracket B\rrbracket_{\text{r}}C_{\lambda}^{-1/2}\) where \(B:=C_{YX}(C_{\lambda})^{-1/2}\).

In this section we follow the approach in [30] and decompose the operator norm excess risk in the following way:

\[\mathcal{E}(\hat{A}^{\text{RRR}}_{m,\lambda})^{1/2}\!=\!\|\Phi_{Y|X}\!-\!A_{ \lambda}\Phi_{X}\|_{\mathcal{B}(L^{2}_{\text{s}},\mathcal{H})}\!+\!\|(A_{ \lambda}\!-\!A^{\text{RRR}}_{\lambda})\Phi_{X}\|_{\mathcal{B}(L^{2}_{\text{s }},\mathcal{H})}\!+\!\|(A^{\text{RRR}}_{\lambda}\!-\!\hat{A}^{\text{RRR}}_{m, \lambda})\Phi_{X}\|_{\mathcal{B}(L^{2}_{\text{s}},\mathcal{H})}\]

Then, recalling that \(A_{\lambda}=C_{YX}C_{\lambda}^{-1}\) and \(\hat{A}^{\text{RRR}}_{m,\lambda}=\tilde{B}\tilde{C}_{\lambda}^{-1/2}\), we also have \(A^{\text{RRR}}_{\lambda}=P_{B}A_{\lambda}\) and \(\hat{A}^{\text{RRR}}_{m,\lambda}=P_{\hat{B}}\hat{A}^{\text{RRR}}_{m,\lambda}\), where \(P_{B}\) and \(P_{\hat{B}}\) are orthogonal projectors onto leading \(r\) left singular vectors of \(B\) and \(\tilde{B}\), respectively.

Thus,

\[\mathcal{E}(\hat{A}^{\text{RRR}}_{m,\lambda})^{1/2} \leq a\,\lambda^{1/2}+\sigma_{r+1}(\Phi_{Y|X})+\|(A^{\text{RRR}}_{ \lambda}-\hat{A}^{\text{RRR}}_{m,\lambda})\Phi_{X}\|_{\mathcal{B}(\mathcal{H})}\] \[=a\,\lambda^{1/2}+\sigma_{r+1}(\Phi_{Y|X})+\|(P_{B}A_{\lambda}-P_{ \tilde{B}}\hat{A}^{\text{RRR}}_{m,\lambda})\Phi_{X}\|_{\mathcal{B}(\mathcal{H})}\] \[\leq a\,\lambda^{1/2}+\sigma_{r+1}(\Phi_{Y|X})+\|((P_{B}-P_{ \tilde{B}})A_{\lambda}\Phi_{X}\|_{\mathcal{B}(\mathcal{H})}+\|P_{\tilde{B}}(A_ {\lambda}-\hat{A}^{\text{RRR}}_{m,\lambda})\Phi_{X}\|_{\mathcal{B}(\mathcal{H})}\] \[\leq a\,\lambda^{1/2}+\sigma_{r+1}(\Phi_{Y|X})+K\,\frac{\|\tilde{ B}\tilde{B}^{*}-BB^{*}\|_{\mathcal{B}(\mathcal{H})}}{\sigma_{r}^{2}(B)-\sigma_{r+1 }^{2}(B)}+\|(A_{\lambda}-\hat{A}^{\text{RRR}}_{m,\lambda})\Phi_{X}\|_{\mathcal{ B}(\mathcal{H})}\]

where the last inequality is due to \(\|A_{\lambda}\|\leq a\) and [30, Proposition 4].

Recalling Lemma E.1, we observe that

\[\mathcal{E}(\hat{A}^{\text{RRR}}_{m,\lambda})^{1/2}\leq\sigma_{r+1}(\Phi_{Y|X} )+K\,\frac{\|\tilde{B}\tilde{B}^{*}-BB^{*}\|}{\sigma_{r}^{2}(B)-\sigma_{r+1}^{ 2}(B)}+\underbrace{a\,\lambda^{1/2}+\|(A_{\lambda}-\hat{A}^{\text{RRR}}_{m, \lambda})\Phi_{X}\|}_{\leq\text{t.h.s. of the bound in Lemma E.1}}\]

Therefore, to prove Lemma 4.7 for the RRR estimator we just need to bound \(\|\tilde{B}\tilde{B}^{*}-BB^{*}\|_{\mathcal{B}(\mathcal{H})}\). To that end, observe that, after some algebra, one obtains

\[\tilde{B}\tilde{B}^{*}-BB^{*}\!=\!A_{\lambda}(\tilde{C}_{YX}\!-\!C_{YX})^{*}\! +\!(\tilde{C}_{YX}\!-\!C_{YX})A_{\lambda}^{*}\!-\!A_{\lambda}(\tilde{C}_{ \lambda}\!-\!C_{\lambda})A_{\lambda}^{*}\!+\!(\tilde{A}_{\lambda}\!-\!A_{ \lambda})\tilde{C}_{\lambda}(\tilde{A}_{\lambda}\!-\!A_{\lambda})^{*},\]

and, consequently,

\[\|\tilde{B}\tilde{B}^{*}-BB^{*}\|_{\mathcal{B}(\mathcal{H})} \leq 2a\|\tilde{C}_{YX}-C_{YX}\|_{\mathcal{B}(\mathcal{H})}+a^{2}\|P _{X}\hat{C}P_{X}-C\|_{\mathcal{B}(\mathcal{H})}\] \[\qquad\qquad+\|C_{\lambda}^{-1/2}\tilde{C}_{\lambda}C_{\lambda}^{ -1/2}\|_{\mathcal{B}(\mathcal{H})}\|(\tilde{A}_{\lambda}-A_{\lambda})C_{\lambda} ^{1/2}\|_{\mathcal{B}(\mathcal{H})}^{2},\]

follows using that \(\|A_{\lambda}\|_{\mathcal{B}(\mathcal{H})}\leq a\).

On the other hand,

\[\|\tilde{C}_{YX}-C_{YX}\|_{\mathcal{B}(\mathcal{H})}\leq\|P_{Y}(\hat{C}_{YX}-C_ {YX})P_{X}\|_{\mathcal{B}(\mathcal{H})}+\|P_{Y}^{\perp}C_{YX}P_{X}\|_{\mathcal{B }(\mathcal{H})}+\|C_{YX}P_{X}^{\perp}\|_{\mathcal{B}(\mathcal{H})},\]

which implies that

\[\|\tilde{C}_{YX}-C_{YX}\|_{\mathcal{B}(\mathcal{H})}\leq\|\hat{C}_{YX}-C_{YX}\|_ {\mathcal{B}(\mathcal{H})}+2\,a\,K\,\varepsilon_{1},\]

where \(\varepsilon_{1}:=\max\{\|P_{X}^{\perp}C^{1/2}\|_{\mathcal{B}(\mathcal{H})},\,\|P_ {Y}^{\perp}C^{1/2}\|_{\mathcal{B}(\mathcal{H})}\}\). Similarly, we obtain

\[\|\tilde{C}_{\lambda}-C_{\lambda}\|_{\mathcal{B}(\mathcal{H})}\leq\|\hat{C}-C\|_ {\mathcal{B}(\mathcal{H})}+2\,K\,\varepsilon_{1}.\] (43)

But, \(\varepsilon_{1}\) can be bounded by Lemma J.6. Indeed, provided \(\lambda\in]0,\|C\|_{\mathcal{B}(\mathcal{H})}]\), it holds with probability \(1-\delta^{\prime}\)

\[\varepsilon_{1}\leq\sqrt{3\lambda}\]

provided \(m\geq\max(67,5\,\mathrm{ess}\sup_{x\sim\pi}\|C_{\lambda}^{-1/2}\phi(x)\|^{2}) \log\frac{4K^{2}}{\lambda\delta^{\prime}}\), which by Lemma H.3 is ensured if \(m\geq\max(67,5\frac{c_{r}}{\lambda})\log\frac{4K^{2}}{\lambda\delta^{\prime}}\).

Additionally,

\[\|C_{\lambda}^{-1/2}\tilde{C}_{\lambda}C_{\lambda}^{-1/2}\|_{\mathcal{ B}(\mathcal{H})} \leq\|C_{\lambda}^{-1/2}P_{X}\hat{C}_{\lambda}P_{X}C_{\lambda}^{-1/ 2}\|_{\mathcal{B}(\mathcal{H})}+\lambda\|C_{\lambda}^{-1/2}P_{X}^{\perp}C_{ \lambda}^{-1/2}\|_{\mathcal{B}(\mathcal{H})}\] \[\leq\theta_{2}^{2}\,\|\hat{C}_{\lambda}^{-1/2}P_{X}\hat{C}_{ \lambda}P_{X}C_{\lambda}^{-1/2}\|_{\mathcal{B}(\mathcal{H})}+1\] \[\leq\theta_{2}^{2}\,\|\hat{C}_{\lambda}^{1/2}P_{X}\hat{C}_{ \lambda}^{-1}P_{X}C_{\lambda}^{1/2}\|_{\mathcal{B}(\mathcal{H})}+1\] \[\leq\theta_{2}^{2}\,\|\hat{C}_{\lambda}^{1/2}P_{X}(P_{X}\hat{C}_{ \lambda}P_{X})^{1}P_{X}C_{\lambda}^{1/2}\|_{\mathcal{B}(\mathcal{H})}+1,\]

implies that

\[\|C_{\lambda}^{-1/2}\tilde{C}_{\lambda}C_{\lambda}^{-1/2}\|_{\mathcal{B}( \mathcal{H})}\leq\theta_{2}^{2}+1\leq 2.25,\] (44)

provided, as above, that \(n\lambda^{r}>32c_{\tau}\beta\).

Therefore, setting \(\varepsilon_{0}:=\max\{a\|\hat{C}_{YX}-C_{YX}\|_{\mathcal{B}(\mathcal{H})},a^{ 2}\|\hat{C}-C\|_{\mathcal{B}(\mathcal{H})}\}\), for all \(i\in[m]\) we have

\[\left|\sigma_{i}^{2}(\tilde{B})-\sigma_{i}^{2}(B)\right|\leq\|\tilde{B}\tilde{ B}^{*}-BB^{*}\|\leq 3\varepsilon_{0}+6.93\,K\,a^{2}\lambda^{1/2}+2.25\, \varepsilon_{2}^{2},\] (45)

where \(\varepsilon_{2}:=\|(A_{\lambda}-\hat{A}_{m,\lambda}^{\text{KRR}})C_{\lambda}^{ 1/2}\|\) is the variance of Nystrom KRR estimator, and conclude that

\[\mathcal{E}(\hat{A}_{m,\lambda}^{\text{RRR}})^{1/2}\leq\sigma_{r+1}(\Phi_{Y|X })+K\,\frac{3\varepsilon_{0}+6.93\,K\,a^{2}\,\lambda^{1/2}+2.25\,\varepsilon_ {2}^{2}}{\sigma_{r}^{2}(B)-\sigma_{r+1}^{2}(B)}+a\,\lambda^{1/2}+\varepsilon_ {2}.\]

Therefore, the proof of Lemma 4.7 for RRR estimator directly follows from the bound on \(a\,\lambda^{1/2}+\varepsilon_{2}\) given in the proof of Theorem 4.6, and the fact that, see e.g. [29], \(\varepsilon_{0}\lesssim n^{-1/2}\lesssim\lambda^{1/2}\).

## Appendix G Excess risk of the Nystrom PCR estimator

Recalling Eq. (27), NystromPCR estimator is of the form

\[\hat{A}_{m}^{\text{PCR}}=P_{Y}\hat{C}_{YX}[\![P_{X}\hat{C}P_{X}]\!]_{r}^{r}= \tilde{C}_{YX}[\![\tilde{C}_{\lambda}]\!]_{r}=\hat{A}_{m,\lambda}^{\text{KRR} }P_{\hat{C}_{\lambda}},\]

for \(\lambda=0\) and with \(P_{\hat{C}_{\lambda}}\) being the orthogonal projector onto leading \(r\) eigenspace of \(\tilde{C}_{\lambda}\). So, to prove Lemma 4.7 for PCR estimator, denote \(\hat{A}_{m,\lambda}^{\text{PCR}}:=\hat{A}_{m,\lambda}^{\text{KRR}}P_{\hat{C}_{ \lambda}}\) for \(\lambda\geq 0\), and let us define the population version \(A_{\lambda}^{\text{PCR}}=A_{\lambda}P_{C_{\lambda}}\), where \(P_{C_{\lambda}}\) being the orthogonal projector onto leading \(r\) eigenspace of \(C_{\lambda}\).

As in the previous section we start with decomposition

\[\mathcal{E}(\hat{A}_{m}^{\text{PCR}})^{1/2}= \|\Phi_{Y|X}-A_{\lambda}\Phi_{X}\|_{\mathcal{B}(L_{x}^{2}, \mathcal{H})}\,+\,\|(A_{\lambda}-A_{\lambda}^{\text{PCR}})\Phi_{X}\|_{ \mathcal{B}(L_{x}^{2},\mathcal{H})}+\] \[\|(A_{\lambda}^{\text{PCR}}-\hat{A}_{m,\lambda}^{\text{PCR}})\Phi _{X}\|_{\mathcal{B}(L_{x}^{2},\mathcal{H})}\,+\,\|(\hat{A}_{m,\lambda}^{\text{PCR}}- \hat{A}_{m}^{\text{PCR}})\Phi_{X}\|_{\mathcal{B}(L_{x}^{2},\mathcal{H})}.\]

The first and the second term are easily bounded by \(\|\Phi_{Y|X}-A_{\lambda}\Phi_{X}\|_{\mathcal{B}(L_{x}^{2},\mathcal{H})}\leq a \,\lambda^{1/2}\), and

\[\|(A_{\lambda}-A_{\lambda}^{\text{PCR}})\Phi_{X}\|_{\mathcal{B}(L _{x}^{2},\mathcal{H})} =\|A_{\lambda}(I-P_{C_{\lambda}})\Phi_{X}\|_{\mathcal{B}(L_{x}^{2}, \mathcal{H})}\] \[\leq a\,\|(I-P_{C_{\lambda}})C^{1/2}\|_{\mathcal{B}(\mathcal{H})} \leq a\,\sigma_{r+1}(\Phi_{X}).\]

For the third term, start by observing that

\[\|(A_{\lambda}^{\text{PCR}}-\hat{A}_{m,\lambda}^{\text{PCR}}) \Phi_{X}\|_{\mathcal{B}(L_{x}^{2},\mathcal{H})} =\|(A_{\lambda}P_{C_{\lambda}}-\hat{A}_{m,\lambda}^{\text{KRR}}P_{ \hat{C}_{\lambda}})C^{1/2}\|_{\mathcal{B}(\mathcal{H})}\] \[\leq\|A_{\lambda}(P_{C_{\lambda}}-P_{\hat{C}_{\lambda}})C^{1/2}\|_{ \mathcal{B}(\mathcal{H})}+\|(A_{\lambda}-\hat{A}_{m,\lambda}^{\text{KRR}})P_{ \hat{C}_{\lambda}}C^{1/2}\|_{\mathcal{B}(\mathcal{H})}\] \[\leq a\,K\,\|P_{C_{\lambda}}-P_{\hat{C}_{\lambda}}\|_{\mathcal{B}( \mathcal{H})}+\|(A_{\lambda}-\hat{A}_{m,\lambda}^{\text{KRR}})P_{\hat{C}_{ \lambda}}C^{1/2}\|_{\mathcal{B}(\mathcal{H})}\] \[\leq a\,K\,\frac{\|\tilde{C}_{\lambda}-C_{\lambda}\|_{\mathcal{B}( \mathcal{H})}}{\sigma_{r}^{2}(\Phi_{X})-\sigma_{r+1}^{2}(\Phi_{X})}+\|(A_{ \lambda}-\hat{A}_{m,\lambda}^{\text{KRR}})P_{\hat{C}_{\lambda}}C^{1/2}\|_{ \mathcal{B}(\mathcal{H})}\] \[\leq K\,\frac{\varepsilon_{0}+2\,a\,K\,\varepsilon_{1}}{\sigma_{r}^ {2}(\Phi_{X})-\sigma_{r+1}^{2}(\Phi_{X})}+\|(A_{\lambda}-\hat{A}_{m,\lambda}^{ \text{KRR}})P_{\hat{C}_{\lambda}}C^{1/2}\|_{\mathcal{B}(\mathcal{H})}\]where the second last inequality is due to [30, Proposition 4] and the last one uses Eq. (43). Moreover, we have that

\[\|(A_{\lambda}-\hat{A}^{\text{\small{KRR}}}_{m,\lambda})P_{\tilde{C} _{\lambda}}C^{1/2}\|_{\mathcal{B}(\mathcal{H})} \leq\|(A_{\lambda}-\hat{A}^{\text{\small{KRR}}}_{m,\lambda})C_{ \lambda}^{1/2}\|_{\mathcal{B}(\mathcal{H})}\,\|C_{\lambda}^{-1/2}\tilde{C}_{ \lambda}^{1/2}\|_{\mathcal{B}(\mathcal{H})}\,\|\tilde{C}_{\lambda}^{-1/2}P_{ \tilde{C}_{\lambda}}C^{1/2}\|_{\mathcal{B}(\mathcal{H})}\] \[\leq\varepsilon_{2}\,\sqrt{1+\theta_{2}^{2}}\,\|P_{\tilde{C}_{ \lambda}}\tilde{C}_{\lambda}^{-1/2}C^{1/2}\|_{\mathcal{B}(\mathcal{H})}\] \[\leq\varepsilon_{2}\,\sqrt{1+\theta_{2}^{2}}\,\|C_{\lambda}^{1/2 }\tilde{C}_{\lambda}^{-1/2}\|_{\mathcal{B}(\mathcal{H})},\]

where we have used Eq. (44) and the fact that \(P_{\tilde{C}_{\lambda}}\) is the spectral projector of \(\tilde{C}_{\lambda}^{-1/2}\). Therefore, due to

\[\|C_{\lambda}^{1/2}\tilde{C}_{\lambda}^{-1/2}\|_{\mathcal{B}( \mathcal{H})} =\|C_{\lambda}^{1/2}[P_{X}^{\perp}+P_{X}]\tilde{C}_{\lambda}^{-1/2 }\|_{\mathcal{B}(\mathcal{H})}\leq\|C_{\lambda}^{1/2}P_{X}\tilde{C}_{\lambda}^ {-1/2}\|_{\mathcal{B}(\mathcal{H})}+\|C_{\lambda}^{1/2}P_{X}^{\perp}\tilde{C}_ {\lambda}^{-1/2}\|_{\mathcal{B}(\mathcal{H})}\] \[\leq\theta_{2}\|\tilde{C}_{\lambda}^{1/2}P_{X}\tilde{C}_{\lambda} ^{-1/2}\|_{\mathcal{B}(\mathcal{H})}+\|C_{\lambda}^{1/2}P_{X}^{\perp}\|_{ \mathcal{B}(\mathcal{H})}\|\tilde{C}_{\lambda}^{-1/2}\|_{\mathcal{B}(\mathcal{ H})}\] \[\leq\theta_{2}\|\tilde{C}_{\lambda}^{1/2}P_{X}(P_{X}\tilde{C}_{ \lambda}P_{X})^{\dagger}P_{X}\tilde{C}_{\lambda}^{1/2}\|_{\mathcal{B}(\mathcal{ H})}^{1/2}+\|C_{\lambda}^{1/2}P_{X}^{\perp}\|_{\mathcal{B}(\mathcal{H})}\lambda^{-1/2}\] \[\leq\theta_{2}+\varepsilon_{1}\lambda^{-1/2}\]

we obtain

\[\|(A_{\lambda}-\hat{A}^{\text{\small{KRR}}}_{m,\lambda})P_{\tilde{C}_{\lambda} }C^{1/2}\|_{\mathcal{B}(\mathcal{H})}\leq\varepsilon_{2}\left(1.68+1.5\lambda ^{-1/2}\,\varepsilon_{1}\right),\]

provided that \(n\lambda^{\tau}>32c_{\tau}\beta\).

Finally for the last term, observe that \([\tilde{C}_{\lambda}]_{r}^{\dagger}\) and \([\tilde{C}_{0}]_{r}^{\dagger}\) share the same eigenvectors, and hence \([\tilde{C}_{0}]_{r}^{\dagger}-[\tilde{C}_{\lambda}]_{r}^{\dagger}=\lambda[ \tilde{C}_{\lambda}\tilde{C}_{0}]_{r}^{\dagger}\). Hence, it holds that

\[\|(\hat{A}^{\text{\small{PCR}}}_{m,\lambda}-\hat{A}^{\text{ \small{PCR}}}_{m})\Phi_{X}\|_{\mathcal{B}(\mathcal{I}_{x}^{2},\mathcal{H})} =\|\tilde{C}_{YX}([\tilde{C}_{\lambda}]_{r}^{\dagger}-[\tilde{C}_{ 0}]_{r}^{\dagger})C^{1/2}\|_{\mathcal{B}(\mathcal{H})}=\lambda\|\tilde{C}_{YX} \tilde{C}_{\lambda}^{-1}[\tilde{C}_{0}]_{r}^{\dagger}C^{1/2}\|_{\mathcal{B}( \mathcal{H})}\] \[\leq\lambda\|\tilde{C}_{YX}\tilde{C}_{\lambda}^{-1}[\tilde{C}_{0 }]_{r}^{\dagger}\tilde{C}_{\lambda}^{1/2}\|_{\mathcal{B}(\mathcal{H})}\| \tilde{C}_{\lambda}^{-1/2}C_{\lambda}^{1/2}\|_{\mathcal{B}(\mathcal{H})}\] \[=\lambda\|\tilde{C}_{YX}\tilde{C}_{\lambda}^{-1/2}[\tilde{C}_{0}] _{r}^{\dagger}\|_{\mathcal{B}(\mathcal{H})}\|\tilde{C}_{\lambda}^{-1/2}C_{ \lambda}^{1/2}\|_{\mathcal{B}(\mathcal{H})}\] \[=\lambda\,\|[\tilde{C}_{0}]_{r}^{\dagger}\|_{\mathcal{B}(\mathcal{ H})}\,\|\tilde{B}\|_{\mathcal{B}(\mathcal{H})}\,\|\tilde{C}_{\lambda}^{-1/2}C_{ \lambda}^{1/2}\|_{\mathcal{B}(\mathcal{H})}.\]

Now, recalling Eq. (45), we can bound

\[\|\tilde{B}\|_{\mathcal{B}(\mathcal{H})}^{2} \leq\|C_{\lambda}^{-1/2}C_{YX}^{*}C_{YX}C_{-1}^{-1/2}\|_{\mathcal{B }(\mathcal{H})}+\|BB^{*}-\tilde{B}\tilde{B}^{*}\|_{\mathcal{B}(\mathcal{H})}\] \[\leq a^{2}K^{2}+3\varepsilon_{0}+6.93\,K\,a^{2}\lambda^{1/2}+2.25 \,\varepsilon_{2}^{2},\]

and,

\[\lambda^{1/2}\|[\tilde{C}_{0}]_{r}^{\dagger}\|_{\mathcal{B}( \mathcal{H})}=\frac{\lambda^{1/2}}{\lambda_{r}(P_{X}\tilde{C}P_{X})}\leq\frac{ \lambda^{1/2}}{\lambda_{r}(C)-\|C_{\lambda}-\tilde{C}_{\lambda}\|_{\mathcal{B}( \mathcal{H})}}\leq\frac{\lambda^{1/2}}{\sigma_{r}^{2}(\Phi_{X})-\|\tilde{C}-C \|_{\mathcal{B}(\mathcal{H})}-2\,K\,\varepsilon_{1}}.\]

Thus, consequently, we obtain

\[\|(\hat{A}^{\text{\small{PCR}}}_{m,\lambda}-\hat{A}^{\text{ \small{PCR}}}_{m})\Phi_{X}\|_{\mathcal{B}(\mathcal{I}_{x}^{2},\mathcal{H})}\leq \lambda^{1/2}\left(\theta_{2}+\varepsilon_{1}\lambda^{-1/2}\right) \,\left(a^{2}K^{2}+3\varepsilon_{0}+6.93\,K\,a^{2}\lambda^{1/2}+2.25\, \varepsilon_{2}^{2}\right)\cdot\] \[\frac{\lambda^{1/2}}{\sigma_{r}^{2}(\Phi_{X})-\|\hat{C}-C\|_{ \mathcal{B}(\mathcal{H})}-2\,K\,\varepsilon_{1}}.\]

To conclude, observe that \(r>n^{\frac{1}{\beta(1+\beta)}}\) due to Assumption 4.4 implies that \(\sigma_{r+1}(\Phi_{X})\lesssim n^{-\frac{1}{2(1+\beta)}}\). Therefore, collecting all the terms, under the assumptions of Lemma 4.7 we obtain

\[\mathcal{E}(\hat{A}^{\text{\small{PCR}}}_{m})^{1/2}\lesssim c_{\text{ \small{PCR}}}\,n^{-\frac{1}{2(1+\beta)}},\]

where \(c_{\text{\small{PCR}}}=(\sigma_{r}^{2}(\Phi_{X})-\sigma_{r+1}^{2}(\Phi_{X}))^{-1}\) is the problem dependant constant.

**Lemma H.1** ([30, Proposition 2] with \(\alpha=1\)): _Under Assumption 4.2 it holds_

\[\|\Phi_{X}\mathcal{K}_{\pi}^{*}-C_{YX}C_{\lambda}^{-1}\Phi_{X}\|\leq a\lambda^{1/2}.\]

**Lemma H.2**: _Let \(A\) be a bounded operator. Under Assumptions 4.1 and 4.5, it holds_

\[\|C_{YXA}\|\leq a\|CA\|\] (46)

_Proof of Lemma H.2:_ Note that under Assumption 4.5, as \(C_{XY}C_{YX}\preccurlyeq a^{2}C^{2}\) it also holds \(A^{*}C_{XY}C_{YXA}\preccurlyeq a^{2}AC^{2}A\) and thus:

\[\|C_{YX}A\| =\|A^{*}C_{YX}C_{YXA}A\|^{1/2}\] \[\leq a\|A^{*}C^{2}A\|^{1/2}\] \[=a\|CA\|.\]

\(\Box\)

The next lemma is a consequence of Assumption 4.3 and will be used in our concentration inequalities.

**Lemma H.3**: _Under Assumption 4.3, it holds \(\pi\)-almost surely for any \(\nu\):_

\[\left\|C_{\lambda}^{-(1-\nu)/2}\phi(x)\right\|^{2}\leq c_{\tau}\lambda^{-[\tau -\nu]_{+}}K^{2[\nu-\tau]_{+}}.\]

_The two following corollaries can be obtained picking \(\nu=0\) and \(\nu=-1\):_

\[\left\|C_{\lambda}^{-1/2}\phi(x)\right\|^{2}\leq\frac{c_{\tau}}{\lambda^{\tau }}\quad\text{and}\quad\left\|C_{\lambda}^{-1}\phi(x)\right\|^{2}\leq\frac{c_{ \tau}}{\lambda^{\tau+1}}.\]

_Proof of Lemma H.3:_ By [18, Theorem 9], it holds \(c_{\tau}:=\|k_{\pi}^{\tau}\|_{\infty}^{2}=\operatorname*{ess\,sup}_{x\sim\pi} \sum_{i\in I}\mu_{i}^{\tau}|e_{i}(x)|^{2}\) (where \((e_{i})\) is defined in Section A.3, and we recall that \((\sqrt{\mu_{i}}e_{i})_{i\in\mathbb{N}}\) is an orthonormal basis of \(\mathcal{H}\). Denoting \(\mu_{i}:=\lambda_{i}(C)\), it holds

\[\left\|C_{\lambda}^{-(1-\nu)/2}\phi(x)\right\|^{2} =\left\|\left(\sum_{i\in I}(\mu_{i}+\lambda)^{-(1-\nu)/2}(\sqrt{ \mu_{i}}e_{i})\otimes(\sqrt{\mu_{i}}e_{i})\right)\phi(x)\right\|^{2}\] \[=\left(\sum_{i\in I}\mu_{i}e_{i}(x)^{2}(\mu_{i}+\lambda)^{-1+\nu}\right)\] \[=\sum_{i\in I}\mu_{i}^{1-\tau}(\mu_{i}+\lambda)^{-1+\nu}\mu_{i}^ {\tau}e_{i}(x)^{2}\] \[=\sum_{i\in I}\biggl{(}\frac{\mu_{i}}{\mu_{i}+\lambda}\biggr{)}^{ 1-\tau}(\mu_{i}+\lambda)^{\nu-\tau}\mu_{i}^{\tau}e_{i}(x)^{2}\] \[\leq\sum_{i\in I}(\mu_{i}+\lambda)^{-(\tau-\nu)}\mu_{i}^{\tau}e_{ i}(x)^{2}\] \[\leq c_{\tau}\lambda^{-[\tau-\nu]_{+}}K^{2[\nu-\tau]_{+}}.\]

where we used \(\sup|\mu_{i}|\leq K^{2}\). \(\Box\)Deterministic sketching results

**Lemma I.1**: _Denoting \(R:=I-\hat{C}_{\lambda}^{1/2}g_{\text{\rm KRR}}(\hat{C})\hat{C}_{\lambda}^{1/2}\), it holds_

\[R\hat{C}_{\lambda}^{1/2}=R\hat{C}_{\lambda}^{1/2}P_{X}^{\perp}.\]

_Proof of Lemma I.1:_ This is a direct consequence of the fact that \(g_{\text{\rm KRR}}(\hat{C})\hat{C}_{\lambda}P_{X}=P_{X}\):

\[R\hat{C}_{\lambda}^{1/2}P_{X}=\hat{C}_{\lambda}^{1/2}P_{X}-\hat{C}_{\lambda}^{ 1/2}g_{\text{\rm KRR}}(\hat{C})\hat{C}_{\lambda}P_{X}=0.\]

\(\square\)

## Appendix J Concentration results

### Generic concentration lemmas

All our concentration results derive from two versions of the Bernstein inequality. We first state an inequality for sums of random variables in a Hilbert space based on [70, Theorem 3.3.4], which itself derives from a result of [48].

**Lemma J.1**: _Let \((A_{i})_{1\leq i\leq n}\) be i.i.d. copies of a random variable \(A\) in a separable Hilbert space \((H,\|\cdot\|)\). Assume \(\mathbf{E}A=\mu\) and \(\exists\sigma>0,\exists L>0,\forall p\geq 2,\mathbf{E}\|A-\mu\|^{p}\leq \frac{1}{2}p!\sigma^{2}L^{p-2}\). Then for any \(\delta\in]0,1[\) it holds:_

\[P\left[\left\|\frac{1}{n}\sum_{i=1}^{n}A_{i}-\mu\right\|\leq\frac{2L\log(2/ \delta)}{n}+\sqrt{\frac{2\sigma^{2}\log(2/\delta)}{n}}\right]\geq 1-\delta\] (47)

_The assumption on the moments holds in particular when \(\operatorname{ess}\sup\|A\|\leq L/2\) and \(\mathbf{E}[\|A\|^{2}]\leq\sigma^{2}\)._

_Proof of Lemma J.1:_ See proof of [11, Lemma E.3] for a precise derivation based on [70, Theorem 3.3.4]. \(\square\)

We now state a version of the Bernstein concentration inequality for self-ajoint operators in operator norm, which is a restatement of [34, Lemma 24]. In the following, we denote \(\operatorname{r_{eff}}(A):=\operatorname{tr}(A)/\|A\|\) the effective rank of a nonnegative definite operator \(A\).

**Lemma J.2** (Bernstein for self-ajoint operators acting on a Hilbert): _Let \(H\) be a separable Hilbert space and \(A_{i}\) be i.i.d. copies of a random variable \(A\) taking values in the space of self-adjoint Hilbert-Schmidt operators on \(H\). Assume \(\mathbf{E}A=0\), \(\operatorname{ess}\sup\|A\|_{\text{\rm op}}\leq c\) for some \(c>0\) (where \(\|\cdot\|_{\text{\rm op}}\) denotes the operator norm) and that there exists a positive semi-definite trace class operator \(V\) such that \(\mathbf{E}[A^{2}]\preccurlyeq V\). Then for any \(\delta\in]0,1[\) and \(n\geq 1\) it holds_

\[P\left[\left\|\frac{1}{n}\sum_{i=1}^{n}A_{i}\right\|_{\text{\rm op}}\geq\frac{ 2c\beta}{3n}+\sqrt{\frac{2\|V\|\beta}{n}}\right]\leq\delta\quad\text{where} \quad\beta=\log\Bigl{(}\tfrac{4\operatorname{r_{eff}}(V)}{\delta}\Bigr{)}\] (48)

_Proof of Lemma J.2:_ See [34, Appendix B.7, Lemma 24]. \(\square\)

### Applied concentration lemmas

**Lemma J.3**:: _Let Assumption 4.1 hold. Let \(\delta\in]0,1[\). Then for i.i.d. samples \((x_{i},y_{i})_{1\leq i\leq n}\) and any \(\lambda\in]0,\|C\|_{\mathcal{B}(\mathcal{H})}]\) it holds_

\[P\bigg{[}\bigg{\|}C_{\lambda}^{-1/2}(\hat{C}-C)C_{\lambda}^{-1/2} \Big{\|}_{\mathcal{B}(\mathcal{H})}\geq\frac{4c_{\tau}\beta}{3n\lambda^{\tau} }+\sqrt{\frac{2c_{\tau}\beta}{n\lambda^{\tau}}}\bigg{]}\leq\delta\quad\text{ where }\quad\beta=\log\Bigl{(}\tfrac{8K^{2}}{\delta\lambda}\Bigr{)}\] (49)

Proof of Lemma J.3:.: We apply Lemma J.2 on the random variables \(A_{i}=\xi(X_{i})\otimes\xi(X_{i})-C_{\lambda}^{-1/2}CC_{\lambda}^{-1/2}\) where \(\xi(X_{i}):=C_{\lambda}^{-1/2}\phi(X_{i})\). It holds

\[\operatorname{ess\,sup}\|A_{i}\|_{\mathcal{B}(\mathcal{H})} \leq 2\operatorname{ess\,sup}\|\xi(X_{i})\|^{2}\] \[\leq\frac{2c_{\tau}}{\lambda^{\tau}}.\quad\text{(by Lemma H.3)}\] \[\mathbf{E}[A_{i}^{2}] =\mathbf{E}[\|\xi(X_{i})\|^{2}\xi(X_{i})\xi(X_{i})^{*}]-(C_{ \lambda}^{-1/2}CC_{\lambda}^{-1/2})^{2}\] \[\leq\mathbf{E}[\|\xi(X_{i})\|^{2}\xi(X_{i})\xi(X_{i})^{*}]\] \[\leq\frac{c_{\tau}}{\lambda^{\tau}}\mathbf{E}[\xi(X_{i})\xi(X_{i} )^{*}]\] \[=\frac{c_{\tau}}{\lambda^{\tau}}CC_{\lambda}^{-1}\]

Thus applying Lemma J.2 with \(c=\frac{2c_{\tau}}{\lambda^{\tau}}\) and \(V=\frac{c_{\tau}}{\lambda^{\tau}}CC_{\lambda}^{-1}\), we get

\[P\Bigg{[}\Bigg{\|}\frac{1}{n}\sum_{i=1}^{n}A_{i}\Bigg{\|} \geq\frac{4c_{\tau}\beta}{3n\lambda^{\tau}}+\sqrt{\frac{2c_{\tau}\beta}{ \lambda^{\tau}n}}\Bigg{]}\leq\delta\quad\text{ where }\quad\beta=\log\Bigl{(}\tfrac{8K^{2}}{ \delta\lambda}\Bigr{)}\] (50)

where we used the fact that \(\|CC_{\lambda}^{-1}\|_{\mathcal{B}(\mathcal{H})}\leq 1\) and controlled the effective rank using \(\operatorname{tr}(CC_{\lambda}^{-1})\leq K^{2}/\lambda\) and \(\|CC_{\lambda}^{-1}\|_{\mathcal{B}(\mathcal{H})}=\|C\|_{\mathcal{B}(\mathcal{H })}/(\|C\|_{\mathcal{B}(\mathcal{H})}+1)\geq 1/2\) because \(\lambda\leq\|C\|_{\mathcal{B}(\mathcal{H})}\) by assumption. 

**Lemma J.4**:: _Let Assumptions 4.1 and 4.3 hold. Let \(\delta\in]0,1[\). Then for i.i.d. samples \((x_{i},y_{i})_{1\leq i\leq n}\) we get_

\[P\Big{[}\|(C-\hat{C})C_{\lambda}^{-1/2}\|_{\mathsf{op}} \leq\epsilon(\lambda,\delta)\Big{]} \geq 1-\delta\] (51) \[\text{and}\quad P\Big{[}\|(C_{YX}-\hat{C}_{YX})C_{\lambda}^{-1/2} \|_{\mathsf{op}} \leq\epsilon(\lambda,\delta)\Big{]} \geq 1-\delta\] (52) \[\text{where }\quad\epsilon(\lambda,\delta):=\frac{2K\sqrt{c_{\tau}} \log(\nicefrac{{2}}{{\delta}})}{\lambda^{\tau/2}n}+\sqrt{\frac{2K^{2}d_{ \text{eff}}(\lambda)\log(\nicefrac{{2}}{{\delta}})}{n}}\] (53)

Proof of Lemma J.4:.: We first write the proof for the eq. (51). For this result, we use the fact that \(\|(C-\hat{C})C_{\lambda}^{-1/2}\|_{\mathcal{B}(\mathcal{H})}\leq\|(C-\hat{C})C _{\lambda}^{-1/2}\|_{\text{HS}}\) and bound the Hilbert-Schmidt norm. As \((\text{HS}(\mathcal{H}),\|\cdot\|_{\text{HS}(\mathcal{H})})\) is a Hilbert space, we apply Lemma J.1 on the random variables \(A_{i}=\phi(x_{i})\otimes\xi(x_{i})\) where \(\xi(x)=C_{\lambda}^{-1/2}\phi(x)\).

\[\operatorname{ess\,sup}\|A\|_{\text{HS}} =\operatorname{ess\,sup}\|\phi(x)|\|\|\xi(x)\|\] \[\leq\frac{K\sqrt{c_{\tau}}}{\lambda^{\tau/2}}\quad\text{(by assumption \ref{eq:1} and lemma H.3)}\] \[\mathbf{E}[\|A\|_{\text{HS}}^{2}] =\mathbf{E}[\|\phi(x)\|^{2}\|\xi(x)\|^{2}]\] \[\leq K^{2}d_{\text{eff}}(\lambda)\]Thus applying Lemma J.1 with \(L=\frac{K\sqrt{c_{\tau}}}{\lambda^{\tau/2}}\) and \(\sigma^{2}=K^{2}d_{\text{eff}}(\lambda)\) gives

\[P\left[\left\|\frac{1}{n}\sum_{i=1}^{n}A_{i}-\mu\right\|_{\text{HS}}\leq\frac{2K \sqrt{c_{\tau}}\log(\nicefrac{{2}}{{\delta}})}{\lambda^{\tau/2}n}+\sqrt{\frac{2 K^{2}d_{\text{eff}}(\lambda)\log(\nicefrac{{2}}{{\delta}})}{n}}\right]\geq 1-\delta.\]

This yields the desired result via the inequality between operator and Hilbert-Schmidt norms.

For the bound eq. (52) on the cross-covariance, we take \(A_{i}=\phi(y_{i})\otimes\xi(x_{i})\) but the rest of the proof is unchanged. 

**Lemma J.5**: _Let Assumptions 4.1 and 4.3 hold. Let \(\delta\in]0,1[\). Then for i.i.d. samples \((x_{i},y_{i})_{1\leq i\leq n}\) we get_

\[P\left[\|(C-\hat{C})C_{\lambda}^{-1}\|_{\text{op}}\leq\frac{2K\sqrt{c_{\tau}} \log(\nicefrac{{2}}{{\delta}})}{\lambda^{(\tau+1)/2}n}+\sqrt{\frac{2K^{2} \operatorname{tr}(C_{\lambda}^{-2}C)\log(\nicefrac{{2}}{{\delta}})}{n}} \right]\geq 1-\delta.\] (54)

Proof of Lemma J.5:For this result, we use the fact that \(\|(C-\hat{C})C_{\lambda}^{-1}\|_{\mathcal{B}(\mathcal{H})}\leq\|(C-\hat{C})C_ {\lambda}^{-1}\|_{\text{HS}}\) and bound the Hilbert-Schmidt norm. As \((\text{HS}(\mathcal{H}),\|\cdot\|_{\text{HS}(\mathcal{H})})\) is a Hilbert space, we apply Lemma J.1 on the random variables \(A_{i}=\phi(x_{i})\otimes\omega(x_{i})\) where \(\omega(x)=C_{\lambda}^{-1}\phi(x)\).

\[\operatorname{ess\,sup}\|A\|_{\text{HS}} =\operatorname{ess\,sup}\|\phi(x)\|\|\omega(x)\|\] \[\leq\frac{K\sqrt{c_{\tau}}}{\lambda^{(\tau+1)/2}}\quad\text{(by assumption \ref{eq:L1} and lemma H.3)}\] \[\mathbf{E}[\|A\|_{\text{HS}}^{2}] =\mathbf{E}[\|\phi(x)\|^{2}\|\omega(x)\|^{2}]\] \[\leq K^{2}\mathbf{E}[\operatorname{tr}(C_{\lambda}^{-2}\phi(x) \phi(x)^{*})]\] \[=K^{2}\operatorname{tr}(C_{\lambda}^{-2}C)\]

Thus applying Lemma J.1 with \(L=\frac{K\sqrt{c_{\tau}}}{\lambda^{(\tau+1)/2}}\) and \(\sigma^{2}=K^{2}\operatorname{tr}(C_{\lambda}^{-2}C)\) gives

\[P\left[\left\|\frac{1}{n}\sum_{i=1}^{n}A_{i}-\mu\right\|_{\text{HS}}\leq\frac{ 2K\sqrt{c_{\tau}}\log(\nicefrac{{2}}{{\delta}})}{\lambda^{(\tau+1)/2}n}+ \sqrt{\frac{2K^{2}\operatorname{tr}(C_{\lambda}^{-2}C)\log(\nicefrac{{2}}{{ \delta}})}{n}}\right]\geq 1-\delta.\]

This yields the desired result via the inequality between operator and Hilbert-Schmidt norms. 

### Probabilistic inequalities

The following lemma is a restatement from [53, Lemma 6].

**Lemma J.6** (Uniform Nystrom approximation): _Let Assumption 4.1 hold. Let \(P:\mathcal{H}\to\mathcal{H}\) denote the orthogonal projection on \(\operatorname{span}\big{\{}\,\phi(\tilde{x}_{j})\,\big{|}\,\,1\leq j\leq m\, \big{\}}\), where the landmarks \((\tilde{x}_{j})_{1\leq j\leq m}\) are drawn uniformly without replacement from the empirical data. Then for any \(\lambda\in]0,\|C_{\lambda}\|_{\mathcal{B}(\mathcal{H})}|\) we have_

\[\|P^{\perp}C_{\lambda}^{1/2}\|_{\mathcal{B}(\mathcal{H})}^{2}\leq 3\lambda\]

_with probability at least \(1-\delta\) provided_

\[m\geq\max(67,5\operatorname{ess\,sup}\|C_{\lambda}^{-1/2}\phi(x)\|^{2})\log \frac{4K^{2}}{\lambda\delta}.\]

### Concentration lemmas for the sketched operators

**Lemma J.7**: _It holds almost surely_

\[\|(C_{YX}-P_{Y}\hat{C}_{YX})C_{\lambda}^{-1/2}\|\leq\|P_{Y}^{\perp}C_{\lambda}^{ 1/2}\|+\|(C_{YX}-\hat{C}_{YX})C_{\lambda}^{-1/2}\|\]

_Proof of Lemma J.7:_ It holds

\[C_{YX}-P_{Y}\hat{C}_{YX} =C_{YX}-P_{Y}C_{YX}+P_{Y}C_{YX}-P_{Y}\hat{C}_{YX}\] \[=P_{Y}^{\perp}C_{YX}+P_{Y}(C_{YX}-\hat{C}_{YX})\]

Thus

\[\|(C_{YX}-P_{Y}\hat{C}_{YX})C_{\lambda}^{-1/2}\| =\|(P_{Y}^{\perp}C_{YX}+P_{Y}(C_{YX}-\hat{C}_{YX}))C_{\lambda}^{-1/ 2}\|\] \[\leq\|P_{Y}^{\perp}C_{YX}C_{\lambda}^{-1/2}\|+\|P_{Y}(C_{YX}-\hat{ C}_{YX})C_{\lambda}^{-1/2}\|\] \[\leq\|P_{Y}^{\perp}C_{\lambda}^{1/2}\|\|C_{\lambda}^{-1/2}C_{YX}C _{\lambda}^{-1/2}\|+\|(C_{YX}-\hat{C}_{YX})C_{\lambda}^{-1/2}\|\]

Eventually it holds \(\|C_{\lambda}^{-1/2}C_{YX}C_{\lambda}^{-1/2}\|\leq 1\). Indeed, as \(\pi\) is invariant, it holds that

\[\|\mathcal{K}_{\pi}\|=\sup_{f\in L_{\pi}^{2}:\|f\|_{L_{\pi}^{2}}\leq 1}\int_{x} \biggl{|}\int f(y)p(x,\mathrm{d}y)\biggr{|}^{2}\,\mathrm{d}\pi(x)\leq 1.\]

and denoting \(\Phi_{X}=C^{1/2}U\) the polar decomposition of \(\Phi_{X}\) for some partial isometry \(U:L_{\pi}^{2}\to\mathcal{H}\), and using \(\Phi_{Y|X}^{*}=\mathcal{K}_{\pi}\Phi_{X}^{*}\), we get

\[\|C_{\lambda}^{-1/2}C_{YX}C_{\lambda}^{-1/2}\| =\|C_{\lambda}^{-1/2}\Phi_{Y|X}\Phi_{X}^{*}C_{\lambda}^{-1/2}\|\] \[\leq\|C_{\lambda}^{-1/2}C^{1/2}\|\|UC_{\pi}^{*}U^{*}\|\|C^{1/2}C_{ \lambda}^{-1/2}\|\] \[\leq 1.\]

### Concentration for mixing processes

**Lemma J.8** (Kostic et al. (2019); Lemma 1): _Let \(X\) be strictly stationary with values in a normed space \((\mathcal{X},\|\cdot\|)\) and assume \(n=2pk\) with \(p,k\in\mathbb{N}\). Let \(Z_{1},\ldots,Z_{p}\) be \(p\) independent copies of \(Z_{1}=\sum_{i=1}^{k}X_{i}\). Then for \(s>0\):_

\[P\Bigl{[}\Bigl{\|}\sum_{i=1}^{n}X_{i}\Bigr{\|}>s\Bigr{]}\leq 2P\Bigl{[}\Bigl{\|} \sum_{j=1}^{p}Z_{j}\Bigr{\|}>s/2\Bigr{]}+2(p-1)\beta_{X}(k).\]

## Appendix K Additional experiments

In this section we extend the experiment of Figure 2 (on the Lorenz'63 dataset) to include the Nystrom KRR setting. Since the KRR estimator does not use rank truncation as a regularizer, it will always output the full rank estimator. In Figure 5 we use the same settings as in the main text (\(m=250\) and \(r=50\)), which lead to KRR being very close to RRR. No particular instability is encountered with these settings, and KRR is about 2x faster than RRR. In Figure 6 we increase the number of centers, without changing the amount of regularization. Now KRR is very unstable, since it needs to estimate a larger number of eigenvalues which have a very small magnitude. RRR on the other hand maintains the rank regularization and has no instability, obtaining a small performance boost thanks to the increased number of centers \(m\). To improve the stability of KRR, in Figure 7 we increase the regularization from \(\lambda=10^{-4}\) to \(\lambda=5\times 10^{-3}\). However, notice that a) the instability is not completely fixed and b) accuracy is noticeably reduced to the point where it's nearly the same as that of PCR with fewer centers.

Connected to the instability problems, KRR is not suited for Koopman operator learning since without rank regularization, a large number of spurious eigenpairs pop up during estimation.

Figure 5: Forecasting error (nRMSE) versus forecast time. KRR has similar performance to RRR. We used rank \(r=50\) for scalable KAF, NysPCR, NysRRR and no rank for NysKRR. We used \(m=250\) for the Nyström estimators. For NysRRR and NysKRR we set \(\lambda=0.0001\).

Figure 6: NysKRR is very unstable. Here \(r=50\) for both NysRRR experiments, \(m=1000\) for NysKRR and NysRRR(m=1000). NysRRR(m=250) is shown for comparison with Figure 5. \(\lambda=0.0001\).

Figure 7: We increase the regularizer for NysKRR to \(\lambda=0.005\). Instability is reduced, but accuracy is also noticeably worse; in fact it is nearly equal to that of NysPCR (with fewer centers) and sKAF. Parameters for NysRRR were \(r=50,m=1000,\lambda=0.0001\); for NysPCR: \(r=50,m=250\).