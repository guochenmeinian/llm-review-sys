# Epistemic Uncertainty and Observation Noise

with the Neural Tangent Kernel

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Recent work has shown that training wide neural networks with gradient descent is formally equivalent to computing the mean of the posterior distribution in a Gaussian Process (GP) with the Neural Tangent Kernel (NTK) as the prior covariance and zero aleatoric noise [12]. In this paper, we extend this framework in two ways. First, we show how to deal with non-zero aleatoric noise. Second, we derive an estimator for the posterior covariance, giving us a handle on epistemic uncertainty. Our proposed approach integrates seamlessly with standard training pipelines, as it involves training a small number of additional predictors using gradient descent on a mean squared error loss. We demonstrate the proof-of-concept of our method through empirical evaluation on synthetic regression.

## 1 Introduction

Jacot et al. have studied the training of wide neural networks, showing that gradient descent on a standard loss is, in the limit of many iterations, formally equivalent to computing the posterior mean of a Gaussian Process (GP), with the prior covariance specified by the Neural Tangent Kernel (NTK) and with zero aleatoric noise. Crucially, this insight allows us to study complex behaviours of wide networks using Bayesian nonparametrics, which are much better understood.

We extend this analysis by asking two research questions. First, we ask if a similar equivalence exists in cases where we want to do inference for arbitrary values of aleatoric noise. This is crucial in many real-world settings, where measurement accuracy or other data-gathering errors mean that the information in our dataset is only approximate. Second, we ask if it is possible to obtain an estimate of the posterior covariance, not just the mean. Since the posterior covariance measures the epistemic uncertainty about predictions of a model, it is crucial for problems that involve out-of-distribution detection or training with bandit-style feedback.

We answer both of these research questions in the affirmative. Our posterior mean estimator takes the aleatoric noise into account by adding a simple squared norm penalty on the deviation of the network parameters from their initial values, shedding light on regularization in deep learning. Our covariance estimator can be understood as an alternative to existing methods of epistemic uncertainty estimation, such as dropout [7, 20], the Laplace approximation [6, 19], epistemic neural networks [18], deep ensembles [21, 14] and Bayesian Neural Networks [3, 13]. Unlike these approaches, our method has the advantage that it can approximate the NTK-GP posterior arbitrarily well.

ContributionsWe derive estimators for the posterior mean and covariance of an NTK-GP with non-zero aleatoric noise, computable using gradient descent on a standard loss. We evaluate our results empirically on a toy repression problem.

## 2 Preliminaries

Gaussian ProcessesGaussian Processes are a popular non-parametric approach for modeling distributions over functions [22]. Given a dataset of input-output pairs \(\{(\mathbf{x}_{i},y_{i})\}_{i=1}^{N}\), a GP represents uncertainty about function values by assuming they are jointly Gaussian with a covariance structuredefined by a kernel function \(k(\mathbf{x},\mathbf{x}^{\prime})\). The GP prior is specified as \(f(\mathbf{x})\sim\mathcal{GP}(m(\mathbf{x}),k(\mathbf{x},\mathbf{x}^{\prime}))\), where \(m(\mathbf{x})\) is the mean function and \(k(\mathbf{x},\mathbf{x}^{\prime})\) is the kernel. Assuming \(y_{i}\sim\mathcal{N}(f(\mathbf{x}),\sigma^{2})\) and given new test points \(\mathbf{x}^{\prime}\), the posterior mean and covariance are given by:

\[\boldsymbol{\mu}_{p}(\mathbf{x}^{\prime}) =m(\mathbf{x}^{\prime})+\mathbf{K}(\mathbf{x}^{\prime},\mathbf{x })^{\top}(\mathbf{K}(\mathbf{x},\mathbf{x})+\sigma^{2}\mathbf{I})^{-1}( \mathbf{y}-m(\mathbf{x})),\] (1) \[\boldsymbol{\Sigma}_{p}(\mathbf{x}^{\prime}) =\mathbf{K}(\mathbf{x}^{\prime},\mathbf{x}^{\prime})-\mathbf{K}( \mathbf{x}^{\prime},\mathbf{x})^{\top}(\mathbf{K}(\mathbf{x},\mathbf{x})+ \sigma^{2}\mathbf{I})^{-1}\mathbf{K}(\mathbf{x}^{\prime},\mathbf{x}),\] (2)

where \(\mathbf{K}(\mathbf{x},\mathbf{x})\) is the covariance matrix computed over the training inputs, \(\mathbf{K}(\mathbf{x}^{\prime},\mathbf{x})\) is the covariance matrix between the test and training points, and \(\sigma^{2}\) represents the aleatoric (or observation) noise.

Neural Tangent Kernel.The Neural Tangent Kernel (NTK) characterizes the evolution of wide neural network predictions as a linear model in function space. Given a neural network function \(f(\mathbf{x};\theta)\) parameterized by \(\theta\), the NTK is defined through the Jacobian \(J(\mathbf{x})\in\mathbb{R}^{N\times p}\), where \(J(\mathbf{x})=\frac{\partial f(\mathbf{x};\theta)}{\partial\theta}\), \(N\) is the number of data points and \(p\) is the number of parameters. The NTK at two sets of inputs \(\mathbf{x}\) and \(\mathbf{x}^{\prime}\) is given by:

\[\mathbf{K}(\mathbf{x},\mathbf{x}^{\prime})=J(\mathbf{x})J(\mathbf{x}^{\prime })^{\top}.\] (3)

Interestingly, as shown by [12] the NTK converges to a deterministic kernel and remains constant during training in the infinite-width limit. We call a GP with the kernel (3) the NTK GP.

## 3 Method

We now describe our proposed process of doing inference in the NTK-GP. Our procedure for estimating the posterior mean is given in Algorithm 1, while the procedure for the covariance is given in Algorithm 2. Note that our process is scaleable because both algorithms only use gradient descent, rather than relying on a matrix inverse in equations (1) and (2). While Algorithm 2 relies on the computation of the partial SVD of the Jacobian, we stress that efficient ways of doing so exist and do not require ever storing the full Jacobian. We defer the details of the partial SVD to Appendix E. We describe the theory that justifies our posterior computation in sections 3.1 and 3.2. We defer the discussion of literature to Appendix A.

``` procedureTrain-Posterior-Mean(\(x_{i},y_{i}\), \(\theta_{0}\)) \(\hat{y}_{i}\gets y_{i}+f(x_{i};\theta_{0})\)\(\triangleright\) Shift the targets to get zero prior mean (Lemma 3.2). \(L\leftarrow\frac{1}{N}\sum_{i=1}^{N}(\hat{y}_{i}-f(x_{i};\theta))^{2}+\beta_{N} \|\theta-\theta_{0}\|_{2}^{2}\)\(\triangleright\) Equation (4) minimize \(L\) with gradient descent wrt. \(\theta\) until convergence to \(\theta^{\star}\) return\(\theta^{\star}\)\(\triangleright\) Return the trained weights. endprocedureQuery-Posterior-Mean(\(x^{\prime}_{j}\), \(\theta^{\star}\), \(\theta^{0}\))\(\triangleright\)\(j=1,\dots,J\) return\(f(x^{\prime}_{1};\theta^{\star})-f(x^{\prime}_{1};\theta^{0}),\dots,f(x^{\prime}_{J}; \theta^{\star})-f(x^{\prime}_{1};\theta^{0})\) endprocedure ```

**Algorithm 1** Algorithm for Computing the Posterior Mean in the NTK-GP

### Aleatoric Noise

Gradient Descent Converges to the NTK-GP Posterior MeanWe build on the work of [12] by focusing on the computation of the mean posterior in the presence of **non-zero aleatoric noise**. We show that optimizing a regularized mean squared error loss in a neural network is equivalent to computing the mean posterior of an NTK-GP with non-zero aleatoric noise. In the following Lemma, we prove that for a sufficiently long training process, the predictions of the trained network converge to those of an NTK-GP with aleatoric noise characterized by \(\sigma^{2}=N\beta_{N}\). This is a similar result to [11], but from a Bayesian perspective rather than a frequentist generalization bound. Furthermore, our proof (see Appendix B) focuses on explicitly solving the gradient flows for test and training data points in function space.

**Lemma 3.1**.: _Consider a parametric model \(f(x;\theta)\) where \(x\in\mathcal{X}\subset\mathbb{R}^{N}\) and \(\theta\in\mathbb{R}^{p}\), initialized under some assumptions with parameters \(\theta_{0}\). Minimizing the regularized mean squared error loss with respect to \(\theta\) to find the optimal set of parameters \(\theta^{*}\) over a dataset \((\mathbf{x},\mathbf{y})\) of size \(N\), and with sufficient training time (\(t\rightarrow\infty\)):_

\[\theta^{*}=\operatorname*{arg\,min}_{\theta\in\mathbb{R}^{p}}\frac{1}{N}\sum_{i= 1}^{N}(y_{i}-f(x_{i};\theta))^{2}+\beta_{N}||\theta-\theta_{0}||_{2}^{2},\] (4)

_is equivalent to computing the mean posterior of a Gaussian process with non-zero aleatoric noise, \(\sigma^{2}=N\beta_{N}\), and the NTK as its kernel:_

\[f(\mathbf{x}^{\prime};\theta_{\infty})=f(\mathbf{x}^{\prime};\theta_{0})+ \mathbf{K}(\mathbf{x}^{\prime},\mathbf{x})(\mathbf{K}(\mathbf{x},\mathbf{x})+ N\beta_{N}\mathbf{I})^{-1}(\mathbf{y}-f(\mathbf{x};\theta_{0})).\] (5)

Zero Prior MeanIn many practical scenarios, it is desirable to start with zero prior mean rather than with a prior mean that corresponds to random network initialization. To accommodate this, we introduce a simple yet effective transformation of the data and the network outputs, to be applied together with 3.1. We summarize it into the following lemma (see Appendix B for proof):

**Lemma 3.2**.: _Consider the computational process derived in Lemma 3.1. Define shifted labels \(\tilde{\mathbf{y}}\) and predictions \(\tilde{f}(\mathbf{x};\theta_{\infty})\) as follows::_

\[\tilde{\mathbf{y}}=\mathbf{y}+f(\mathbf{x};\theta_{0}),\quad\tilde{f}( \mathbf{x};\theta_{\infty})=f(\mathbf{x};\theta_{\infty})-f(\mathbf{x}^{ \prime};\theta_{0}).\]

_Using these definitions, the posterior mean of a zero-mean Gaussian process can be computed as:_

\[\tilde{f}(\mathbf{x}^{\prime},\theta_{\infty})=\mathbf{K}(\mathbf{x}^{\prime },\mathbf{x})(\mathbf{K}(\mathbf{x},\mathbf{x})+N\beta_{N}\mathbf{I})^{-1} \mathbf{y}.\] (6)

``` procedureTrain-Posterior-Covariance(\(x_{i}\), \(K\), \(\theta_{0}\)) \(\triangleright\)\(K\) is the number of predictors \(U,\Sigma\leftarrow\textsc{Partial-SVD}(J_{\theta_{0}}(\mathbf{x}),K)\)\(\triangleright\) Partial SVD of the Jacobian - see appendix E. for\(i=1,\dots,K\)do\(\theta_{i}^{*}\leftarrow\textsc{Train-Posterior-Mean}(x_{i},U_{i})\)\(\triangleright\)\(U_{i}\) is the \(i\)-th column of \(U\). endfor for\(i=1,\dots,K^{\prime}\)do\(\triangleright\) Setting \(K^{\prime}=0\) often works well (see Appendix D). \({\theta^{\prime}}_{i}^{*}\leftarrow\textsc{Train-Posterior-Mean}(x_{i},\epsilon_{i})\)\(\triangleright\)\(\epsilon_{i}\sim\mathcal{N}(0,\sigma^{2}I)\) endfor return\(\Sigma,\theta_{1}^{*},\dots,\theta_{K}^{*},{\theta^{\prime}}_{1}^{*},\dots,{\theta^{\prime}}_{K^{ \prime}}^{*}\) endprocedureprocedureQuery-Posterior-Covariance(\(x^{\prime}_{j}\), \(\Sigma\), \(\theta_{i}^{*}\), \(\theta_{i}^{\prime\prime}\), \(\theta_{0}\)) \(\triangleright\)\(j=1,\dots,J\) \(P_{\sim}\)\(\left[\begin{smallmatrix}f(x^{\prime}_{1};\theta_{1}^{*})-f(x^{\prime}_{1}; \theta_{0})&\dots&f(x^{\prime}_{1};\theta_{K}^{*})-f(x^{\prime}_{1};\theta_{0} )&\dots&f(x^{\prime}_{1};\theta^{\prime\prime}_{K^{\prime}})-f(x^{\prime}_{1} ;\theta_{0})\\ &\dots&\dots&\dots&\dots\\ f(x^{\prime}_{j};\theta_{1}^{*})-f(x^{\prime}_{j};\theta_{0})&\dots&f(x^{ \prime}_{j};\theta_{K}^{*})-f(x^{\prime}_{1};\theta_{0})&\dots&f(x^{\prime}_{ j};\theta^{\prime\prime}_{K^{\prime}})-f(x^{\prime}_{1};\theta_{0})\end{smallmatrix}\right]\) return\(J(\mathbf{x}^{\prime})J(\mathbf{x}^{\prime})^{\top}-P\Sigma^{2}P^{\top}-P^{\prime}(P^{ \prime})^{\top}/K^{\prime}\)\(\triangleright\) The last term vanishes for \(K^{\prime}=0\) endprocedure ```

**Algorithm 2** Algorithm for Computing the Posterior Covariance in the NTK-GP

### Estimating the Covariance

We now justify Algorithm 2 for estimating the posterior covariance. The main observation that allows us to derive our estimator comes from examining the term \(\mathbf{K}(\mathbf{x}^{\prime},\mathbf{x})^{\top}(\mathbf{K}(\mathbf{x}, \mathbf{x})+\sigma^{2}\mathbf{I})^{-1}\mathbf{K}(\mathbf{x}^{\prime},\mathbf{ x})\) in the posterior covariance formula (2). This is summarized in the following Proposition.

**Proposition 3.1**.: _Diagonalize \(\mathbf{K}(\mathbf{x},\mathbf{x})\) so that \(\mathbf{K}(\mathbf{x},\mathbf{x})=U\Lambda U^{\top}\). We have_

\[\mathbf{K}(\mathbf{x}^{\prime},\mathbf{x})^{\top}(\mathbf{K}(\mathbf{x}, \mathbf{x})+\sigma^{2}\mathbf{I})^{-1}\mathbf{K}(\mathbf{x}^{\prime},\mathbf{ x})=(MU)\Lambda(MU)^{\top}+\sigma^{2}MM^{\top}.\]

_Here, \(M=\mathbf{K}(\mathbf{x}^{\prime},\mathbf{x})^{\top}(\mathbf{K}(\mathbf{x}, \mathbf{x})+\sigma^{2}\mathbf{I})^{-1}\)._

Proof.: We can rewrite it as:

\[\mathbf{K}(\mathbf{x}^{\prime},\mathbf{x})^{\top}(\mathbf{K}( \mathbf{x},\mathbf{x})+\sigma^{2}\mathbf{I})^{-1}\mathbf{K}(\mathbf{x}^{\prime}, \mathbf{x})=\] \[\underbrace{\mathbf{K}(\mathbf{x}^{\prime},\mathbf{x})^{\top}( \mathbf{K}(\mathbf{x},\mathbf{x})+\sigma^{2}\mathbf{I})^{-1}(\mathbf{K}( \mathbf{x},\mathbf{x})+\sigma^{2}\mathbf{I})}_{M}(\mathbf{K}(\mathbf{x}, \mathbf{x})+\sigma^{2}\mathbf{I})\underbrace{(\mathbf{K}(\mathbf{x},\mathbf{x})+ \sigma^{2}\mathbf{I})^{-1}\mathbf{K}(\mathbf{x}^{\prime},\mathbf{x})}_{M^{\top}}\]Denoting the term \(\mathbf{K}(\mathbf{x}^{\prime},\mathbf{x})^{\top}(\mathbf{K}(\mathbf{x},\mathbf{x} )+\sigma^{2}\mathbf{I})^{-1}\) with \(M\), this can be written as:

\[\mathbf{K}(\mathbf{x}^{\prime},\mathbf{x})^{\top}(\mathbf{K}(\mathbf{x}, \mathbf{x})+\sigma^{2}\mathbf{I})^{-1}\mathbf{K}(\mathbf{x}^{\prime},\mathbf{x })=(MU)\Lambda(MU)^{\top}+\sigma^{2}MM^{\top}.\]

The proposition is useful because the matrix \(M\) appears in equation (1). Hence the matrix multiplication \(MU\) is equivalent to estimating the posterior mean using algorithm 1 where targets are given by the columns of the matrix \(U\). Hence the term \((MU)\Lambda(MU)^{\top}\) can be computed by gradient descent. In order to derive a complete estimator of the covariance, we still need to deal with the term \(\sigma^{2}MM^{\top}\). We can either estimate this term by fitting random targets (which corresponds to setting \(K^{\prime}>0\) in algorithm 2) or accept an upper bound on the covariance, setting \(K^{\prime}=0\). We describe this in detail in Appendix D.

## 4 Experiment

We applied the method to a toy regression problem shown in Figure 1. The problem is a standard non-linear 1d regression task which requires both interpolation and extrapolation. The top-left figure was obtained by computing the kernel of the NTK-GP using formula (3) and computing the posterior mean and covariance using equations (1) and (2). The top-right figure was obtained by analytically computing the upper bound defined in appendix D. The bottom-left figure was obtained by taking the first 5 eigenvectors of the kernel. Finally, the bottom-right figure was obtained by fitting a mean prediction network and 5 predictor networks using the gradient-descent method described in algorithm 2. The similarity of the figures shows that the method works. Details of network architecture are deferred to Appendix C.

## 5 Conclusions

This paper introduces a method for computing the posterior mean and covariance of NTK-Gaussian Processes with non-zero aleatoric noise. Our approach integrates seamlessly with standard training procedures using gradient descent, providing a practical tool for uncertainty estimation in contexts such as Bayesian optimization. The method has been validated empirically on a toy task, demonstrating its effectiveness in capturing uncertainty while maintaining computational efficiency. This work opens up opportunities for further research in applying NTK-GP frameworks to more complex scenarios and datasets.

Figure 1: The NTK-GP posterior and its approximations: (top-left) Analytic Posterior, (top-right) Analytic upper bound on posterior (all eigenvectors), (bottom-left) Analytic upper bound on posterior (5 eigenvectors), (bottom-right) Posterior obtained with gradient descent (\(K=5\) predictors, \(K^{\prime}=0\)).

## References

* [1] R. Bhatia. _Positive Definite Matrices_. Princeton Series in Applied Mathematics. Princeton University Press, 2015. isbn: 9780691168258. url: https://books.google.co.uk/books?id=Y22YDwAAQBAJ.
* [2] Avrim Blum, John Hopcroft, and Ravindran Kannan. _Foundations of data science_. Cambridge University Press, 2020.
* [3] Charles Blundell et al. "Weight uncertainty in neural network". In: _International conference on machine learning_. PMLR. 2015, pp. 1613-1622.
* [4] Yuri Burda et al. "Exploration by random network distillation". In: _arXiv preprint arXiv:1810.12894_ (2018).
* [5] Kamil Ciosek et al. "Conservative uncertainty estimation by fitting prior networks". In: _International Conference on Learning Representations_. 2019.
* [6] Erik Daxberger et al. "Laplace redux-effortless bayesian deep learning". In: _Advances in Neural Information Processing Systems_ 34 (2021), pp. 20089-20103.
* [7] Yarin Gal and Zoubin Ghahramani. "Dropout as a bayesian approximation: Representing model uncertainty in deep learning". In: _international conference on machine learning_. PMLR. 2016, pp. 1050-1059.
* [8] Eugene Golikov, Eduard Pokonechnyy, and Vladimir Korviakov. _Neural Tangent Kernel: A Survey_. 2022. arXiv: 2208.13614 [cs.LG]. url: https://arxiv.org/abs/2208.13614.
* [9] Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp. "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions". In: _SIAM review_ 53.2 (2011), pp. 217-288.
* [10] Bobby He, Balaji Lakshminarayanan, and Yee Whye Teh. _Bayesian Deep Ensembles via the Neural Tangent Kernel_. 2020. arXiv: 2007.05864 [stat.ML]. url: https://arxiv.org/abs/2007.05864.
* [11] Wei Hu, Zhiyuan Li, and Dingli Yu. _Simple and Effective Regularization Methods for Training on Noisily Labeled Data with Generalization Guarantee_. 2020. arXiv: 1905.11368 [cs.LG]. url: https://arxiv.org/abs/1905.11368.
* [12] Arthur Jacot, Franck Gabriel, and Clement Hongler. "Neural tangent kernel: Convergence and generalization in neural networks". In: _Advances in neural information processing systems_ 31 (2018).
* [13] Diederik P Kingma. "Auto-encoding variational bayes". In: _arXiv preprint arXiv:1312.6114_ (2013).
* [14] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. "Simple and scalable predictive uncertainty estimation using deep ensembles". In: _Advances in neural information processing systems_ 30 (2017).
* [15] Jaehoon Lee et al. "Wide neural networks of any depth evolve as linear models under gradient descent". In: _Advances in neural information processing systems_ 32 (2019).
* [16] Radford M Neal. _Bayesian learning for neural networks_. Vol. 118. Springer Science & Business Media, 2012.
* [17] Ian Osband et al. "Deep exploration via randomized value functions". In: _Journal of Machine Learning Research_ 20.124 (2019), pp. 1-62.
* [18] Ian Osband et al. "Epistemic neural networks". In: _Advances in Neural Information Processing Systems_ 36 (2023), pp. 2795-2823.
* [19] Hippolyt Ritter, Aleksandar Botev, and David Barber. "A scalable laplace approximation for neural networks". In: _6th international conference on learning representations, ICLR 2018-conference track proceedings_. Vol. 6. International Conference on Representation Learning. 2018.
* [20] Nitish Srivastava et al. "Dropout: a simple way to prevent neural networks from overfitting". In: _The journal of machine learning research_ 15.1 (2014), pp. 1929-1958.
* [21] Robert J Tibshirani and Bradley Efron. "An introduction to the bootstrap". In: _Monographs on statistics and applied probability_ 57.1 (1993), pp. 1-436.
* [22] Christopher KI Williams and Carl Edward Rasmussen. _Gaussian processes for machine learning_. Vol. 2. 3. MIT press Cambridge, MA, 2006.

Related Work

Neural Tangent KernelThe definition of the Neural Tangent Kernel (3), the proof of the fact that it stays constant during training and doesn't depend on initialization as well as the link to Gaussian Processes with no aleatoric noise are all due to the seminal paper [12]. The work of Lee et al. builds on that, showing that wide neural networks can be understood as linear models for purposes of studying their training dynamics, a fact we crucially rely on in the proof of our Lemma 3.1. Hu et al. describe a regularizer for networks trained in the NTK regime which leads to the same optimization problem used in our Lemma 3.1. The difference lies in the fact that we rely on the Bayesian interpretation of the network obtained at the end of training, while they focus on a frequentist generalization bound.

Predictor NetworksPrior work [17; 4; 5] has considered epistemic uncertainty estimation by fitting functions generated using a process that includes some kind of randomness. Burda et al. have applied a similar idea to reinforcement learning, obtaining exceptional results on Montezuma's Revenge, a problem where it is known that exploration very is hard. Ciosek et al. provided a link to Gaussian Processes, but did not leverage the NTK, instead describing an upper bound on a posterior relative to the kernel [16] where sampling corresponds to sampling from the network initialization. Osband et al. proposed1 a way of sampling from a Bayesian linear regression posterior by solving an optimization problem with a similar structure to ours. However, this approach is different in two crucial ways. First, Osband et al. is interested in obtaining samples from the posterior, while we are interested in computing the posterior moments. Second, the sampling process in the paper by Osband et al. depends on the true regression targets in a way that our posterior covariance estimate does not. Also, our method is framed differently, as we intend it to be used in the context of the NTK regime, while Osband et al. discusses vanilla linear regression.

Footnote 1: See Section 5.3.1 in the paper by Osband et al.

Epistemic UncertaintyOur method of fitting the posterior covariance about network outputs can be thought of as quantifying epistemic uncertainty. There are several established methods in this space. Dropout [7; 20], works by randomly disabling neurons in a network and has a Bayesian interpretation. The Laplace approximation [6; 19] works by replacing an arbitrary likelihood with a Gaussian one. Epistemic neural networks [18] are based on the idea of using an additional input (the epistemic index) when training the network. Deep ensembles [21; 14] work by training several copies of a network with different initializations and sometimes training sets that are only partially overlapping. While classic deep ensembles do not have a Bayesian interpretation, He et al. have recently proposed a modification that approximates the posterior in the NTK-GP. Bayesian Neural Networks [3; 13] attempt to apply Bayes rule in the space of neural network parameters, applying various approximations. A full survey of methods of epistemic uncertainty estimation is beyond the scope of this paper.

## Appendix B Proofs

**Lemma 3.1**.: _Consider a parametric model \(f(x;\theta)\) where \(x\in\mathcal{X}\subset\mathbb{R}^{N}\) and \(\theta\in\mathbb{R}^{p}\), initialized under some assumptions with parameters \(\theta_{0}\). Minimizing the regularized mean squared error loss with respect to \(\theta\) to find the optimal set of parameters \(\theta^{*}\) over a dataset \((\mathbf{x},\mathbf{y})\) of size \(N\), and with sufficient training time (\(t\rightarrow\infty\)):_

\[\theta^{*}=\operatorname*{arg\,min}_{\theta\in\mathbb{R}^{p}}\frac{1}{N}\sum_ {i=1}^{N}(y_{i}-f(x_{i};\theta))^{2}+\beta_{N}||\theta-\theta_{0}||_{2}^{2},\] (4)

_is equivalent to computing the mean posterior of a Gaussian process with non-zero aleatoric noise, \(\sigma^{2}=N\beta_{N}\), and the NTK as its kernel:_

\[f(\mathbf{x}^{\prime};\theta_{\infty})=f(\mathbf{x}^{\prime};\theta_{0})+ \mathbf{K}(\mathbf{x}^{\prime},\mathbf{x})(\mathbf{K}(\mathbf{x},\mathbf{x})+ N\beta_{N}\mathbf{I})^{-1}(\mathbf{y}-f(\mathbf{x};\theta_{0})).\] (5)

Proof.: Consider a regression problem with the following regularized empirical loss:

\[\mathcal{L}(\mathbf{y},f(\mathbf{x};\theta))=\frac{1}{N}\sum_{i=1}^{N}(y_{i}-f (x_{i};\theta))^{2}+\beta_{N}||\theta-\theta_{0}||_{2}^{2}.\] (7)Let us use \(\theta_{t}\) to represent the parameters of the network evolving in time \(t\) and let \(\alpha\) be the learning rate. Assuming we train the network via continuous-time gradient flow, then the evolution of the parameters \(\theta_{t}\) can be expressed as:

\[\frac{d\theta_{t}}{dt}=-\alpha\left[\frac{2}{N}\nabla_{\theta}f(\mathbf{x}; \theta_{t})(f(\mathbf{x};\theta_{t})-\mathbf{y})+2\beta_{N}(\theta_{t}-\theta_ {0})\right].\] (8)

Assuming that our neural network architecture operates in a sufficiently wide regime [15], where the first-order approximation remains valid throughout gradient descent, we obtain:

\[f(\mathbf{x}^{\prime};\theta_{t})=f(\mathbf{x}^{\prime};\theta_{0})+J_{t}( \mathbf{x}^{\prime})(\theta_{t}-\theta_{0})\rightarrow\nabla_{\theta}f( \mathbf{x}^{\prime};\theta_{t})^{\top}=J_{t}(\mathbf{x}^{\prime}).\] (9)

The dynamics of the neural network on the training data:

\[\frac{df(\mathbf{x};\theta_{t})}{dt} =J_{t}(\mathbf{x})\frac{d\theta_{t}}{dt}=-\frac{2\alpha}{N}J_{t} (\mathbf{x})\left[J_{t}(\mathbf{x})^{\top}(f(\mathbf{x};\theta_{t})-\mathbf{y })+\beta_{N}(\theta_{t}-\theta_{0})\right]\] \[=-\frac{2\alpha}{N}\left(\mathbf{K}(\mathbf{x},\mathbf{x})(f( \mathbf{x};\theta_{t})-\mathbf{y})+\beta_{N}J_{t}(\mathbf{x})(\theta_{t}- \theta_{0})\right)\] \[=-\frac{2\alpha}{N}\left(\mathbf{K}(\mathbf{x},\mathbf{x})(f( \mathbf{x};\theta_{t})-\mathbf{y})+\beta_{N}(f(\mathbf{x};\theta_{t})-f( \mathbf{x};\theta_{0}))\right)\] \[=-\frac{2\alpha}{N}\left(\mathbf{K}(\mathbf{x},\mathbf{x})+\beta_ {N}\mathbf{I}\right)f(\mathbf{x};\theta_{t})+\frac{2\alpha}{N}\left(\mathbf{K} (\mathbf{x},\mathbf{x})\mathbf{y}+\beta_{N}f(\mathbf{x};\theta_{0})\right)\]

This is a linear ODE, we can solve this:

\[f(\mathbf{x};\theta_{t}) =\exp\left(-\frac{2\alpha}{N}t\left(\mathbf{K}(\mathbf{x},\mathbf{ x})+\beta_{N}\mathbf{I}\right)\right)f(\mathbf{x};\theta_{0})\] \[-\frac{N}{2\alpha}\left(\mathbf{K}(\mathbf{x},\mathbf{x})+\beta_ {N}\mathbf{I}\right)^{-1}\left[\exp\left(-\frac{2\alpha}{N}t\left(\mathbf{K}( \mathbf{x},\mathbf{x})+\beta_{N}\mathbf{I}\right)\right)-\mathbf{I}\right]\] \[\times\frac{2\alpha}{N}\left(\mathbf{K}(\mathbf{x},\mathbf{x}) \mathbf{y}+\beta_{N}f(\mathbf{x};\theta_{0})\right)\]

Using \(A^{-1}e^{A}=e^{A}A^{-1}\), and writing \(\mathbf{K}(\mathbf{x},\mathbf{x})y+\beta_{N}f(x,\theta_{0})=(\mathbf{K}( \mathbf{x},\mathbf{x})+\beta_{N}I)f(x,\theta_{0})+\mathbf{K}(\mathbf{x},\mathbf{ x})(y-f(x,\theta_{0}))\), we get:

\[f(x,\theta_{t})= \exp\left(-\frac{2\alpha}{N}t(\mathbf{K}(\mathbf{x},\mathbf{x})+ \beta_{N}I)\right)f(x,\theta_{0})\] \[+\left[I-\exp\left(-\frac{2\alpha}{N}t(\mathbf{K}(\mathbf{x}, \mathbf{x})+\beta_{N}I)\right)\right](\mathbf{K}(\mathbf{x},\mathbf{x})+\beta _{N}I)^{-1}(\mathbf{K}(\mathbf{x},\mathbf{x})y+\beta_{N}f(x,\theta_{0}))\] \[= \exp\left(-\frac{2\alpha}{N}t(\mathbf{K}(\mathbf{x},\mathbf{x})+ \beta_{N}I)\right)f(x,\theta_{0})+\left[I-\exp\left(-\frac{2\alpha}{N}t( \mathbf{K}(\mathbf{x},\mathbf{x})+\beta_{N}I)\right)\right]f(x,\theta_{0})\] \[+\left[I-\exp\left(-\frac{2\alpha}{N}t(\mathbf{K}(\mathbf{x}, \mathbf{x})+\beta_{N}I)\right)\right](\mathbf{K}(\mathbf{x},\mathbf{x})+ \beta_{N}I)^{-1}\mathbf{K}(\mathbf{x},\mathbf{x})(y-f(x,\theta_{0}))\] \[=f(x,\theta_{0})+\left[I-\exp\left(-\frac{2\alpha}{N}t(\mathbf{K} (\mathbf{x},\mathbf{x})+\beta_{N}I)\right)\right](\mathbf{K}(\mathbf{x}, \mathbf{x})+\beta_{N}I)^{-1}\mathbf{K}(\mathbf{x},\mathbf{x})(y-f(x,\theta_{0} )).\]

Now, we consider the dynamics for the neural network of an arbitrary set of test points \(\mathbf{x}^{\prime}\):

\[\frac{df(x^{\prime},\theta_{t})}{dt}=-\frac{2\alpha}{N}\beta_{N}f(x^{\prime}, \theta_{t})-\frac{2\alpha}{N}\left(\mathbf{K}(\mathbf{x}^{\prime},\mathbf{x})( f(x,\theta_{t})-y)-\beta_{N}f(x^{\prime},\theta_{0})\right).\] (10)This is a linear ODE with a time-dependent inhomogeneous term, we can solve it as follows:

\[f(x^{\prime},\theta_{t})= e^{-\frac{2\alpha}{N}\beta_{N}t}f(x^{\prime},\theta_{0})-\frac{2 \alpha}{N}e^{-\frac{2\alpha}{N}\beta_{N}t}\int_{0}^{t}e^{\frac{2\alpha}{N} \beta_{N}u}\left(\mathbf{K}(\mathbf{x}^{\prime},\mathbf{x})(f(x,\theta_{u})-y)- \beta_{N}f(x^{\prime},\theta_{0})\right)du\] \[= e^{-\frac{2\alpha}{N}\beta_{N}t}f(x^{\prime},\theta_{0})+\frac{2 \alpha}{N}e^{-\frac{2\alpha}{N}\beta_{N}t}\int_{0}^{t}e^{\frac{2\alpha}{N} \beta_{N}u}du\left(\mathbf{K}(\mathbf{x}^{\prime},\mathbf{x})y+\beta_{N}f(x^{ \prime},\theta_{0})\right)\] \[-\frac{2\alpha}{N}e^{-\frac{2\alpha}{N}\beta_{N}t}\mathbf{K}( \mathbf{x}^{\prime},\mathbf{x})\int_{0}^{t}e^{\frac{2\alpha}{N}\beta_{N}u}f( x,\theta_{u})du.\] \[= e^{-\frac{2\alpha}{N}\beta_{N}t}f(x^{\prime},\theta_{0})+e^{- \frac{2\alpha}{N}\beta_{N}t}\frac{1}{\beta_{N}}\left(e^{\frac{2\alpha}{N}\beta _{N}t}-1\right)\left(\mathbf{K}(\mathbf{x}^{\prime},\mathbf{x})y+\beta_{N}f(x ^{\prime},\theta_{0})\right)\] \[-\frac{2\alpha}{N}e^{-\frac{2\alpha}{N}\beta_{N}t}\mathbf{K}( \mathbf{x}^{\prime},\mathbf{x})\int_{0}^{t}e^{\frac{2\alpha}{N}\beta_{N}u}f( x,\theta_{0})du\] \[-\frac{2\alpha}{N}e^{-\frac{2\alpha}{N}\beta_{N}t}\mathbf{K}( \mathbf{x}^{\prime},\mathbf{x})\int_{0}^{t}e^{\frac{2\alpha}{N}\beta_{N}u} \left[I-\exp\left(-\frac{2\alpha}{N}u(\mathbf{K}(\mathbf{x},\mathbf{x})+\beta _{N}I)\right)\right]du\] \[\times(\mathbf{K}(\mathbf{x},\mathbf{x})+\beta_{N}I)^{-1}\mathbf{ K}(\mathbf{x},\mathbf{x})(y-f(x,\theta_{0})).\] \[=f(x^{\prime},\theta_{0})+\frac{1}{\beta_{N}}(1-e^{\frac{2\alpha} {N}\beta_{N}t})\mathbf{K}(\mathbf{x}^{\prime},\mathbf{x})y-\frac{1}{\beta_{N }}(1-e^{-\frac{2\alpha}{N}\beta_{N}t})\mathbf{K}(\mathbf{x}^{\prime},\mathbf{x })f(x,\theta_{0})\] \[-\frac{2\alpha}{N}e^{-\frac{2\alpha}{N}\beta_{N}t}\mathbf{K}( \mathbf{x}^{\prime},\mathbf{x})\left[\frac{N}{2\alpha\beta}(e^{\frac{2\alpha} {N}\beta_{N}t}-1)I-\frac{N}{2\alpha\beta}\mathbf{K}(\mathbf{x},\mathbf{x})^{- 1}\left(\exp\left(-\frac{2\alpha}{N}t\mathbf{K}(\mathbf{x},\mathbf{x})\right) -I\right)\right]\] \[\times(\mathbf{K}(\mathbf{x},\mathbf{x})+\beta_{N}I)^{-1}\mathbf{ K}(\mathbf{x},\mathbf{x})(y-f(x,\theta_{0}))\] \[=f(x^{\prime},\theta_{0})+\frac{1}{\beta_{N}}(1-e^{\frac{2\alpha }{N}\beta_{N}t})\mathbf{K}(\mathbf{x}^{\prime},\mathbf{x})(y-f(x,\theta_{0}))\] \[-\frac{1}{\beta}\mathbf{K}(\mathbf{x}^{\prime},\mathbf{x})\left[ (1-e^{-\frac{2\alpha}{N}\beta_{N}t})I-\mathbf{K}(\mathbf{x},\mathbf{x})^{-1} \left(\exp\left(-\frac{2\alpha}{N}t(\mathbf{K}(\mathbf{x},\mathbf{x})+\beta_{N }I)\right)-e^{-\frac{2\alpha}{N}\beta_{N}t}I\right)\right]\] \[\times(\mathbf{K}(\mathbf{x},\mathbf{x})+\beta_{N}I)^{-1}\mathbf{ K}(\mathbf{x},\mathbf{x})(y-f(x,\theta_{0})).\]

Lastly, taking \(t\to\infty\), we get

\[f(x^{\prime},\theta_{\infty}) =f(x^{\prime},\theta_{0})+\frac{1}{\beta_{N}}\mathbf{K}(\mathbf{ x}^{\prime},\mathbf{x})(y-f(x,\theta_{0}))-\frac{1}{\beta_{N}}\mathbf{K}(\mathbf{x}^{ \prime},\mathbf{x})(\mathbf{K}(\mathbf{x},\mathbf{x})+\beta_{N}I)^{-1}\mathbf{ K}(\mathbf{x},\mathbf{x})(y-f(x,\theta_{0}))\] \[=f(x^{\prime},\theta_{0})+\frac{1}{\beta_{N}}\mathbf{K}(\mathbf{ x}^{\prime},\mathbf{x})\left(I-(\mathbf{K}(\mathbf{x},\mathbf{x})+\beta_{N}I)^{-1} \mathbf{K}(\mathbf{x},\mathbf{x})\right)(y-f(x,\theta_{0}))\] \[=f(x^{\prime},\theta_{0})+\mathbf{K}(\mathbf{x}^{\prime},\mathbf{x })(\mathbf{K}(\mathbf{x},\mathbf{x})+\beta_{N}I)^{-1}(y-f(x,\theta_{0})),\]

we achieve the desired result and hence having a regularized gradient flow in the infinite-width limit is equivalent to inferring the mean posterior of a non-zero aleatoric noise NTK-GP. 

**Lemma 3.2**.: _Consider the computational process derived in Lemma 3.1. Define shifted labels \(\tilde{\mathbf{y}}\) and predictions \(\tilde{f}(\mathbf{x};\theta_{\infty})\) as follows::_

\[\tilde{\mathbf{y}}=\mathbf{y}+f(\mathbf{x};\theta_{0}),\quad\tilde{f}(\mathbf{ x};\theta_{\infty})=f(\mathbf{x};\theta_{\infty})-f(\mathbf{x}^{\prime};\theta_{0}).\]

_Using these definitions, the posterior mean of a zero-mean Gaussian process can be computed as:_

\[\tilde{f}(\mathbf{x}^{\prime},\theta_{\infty})=\mathbf{K}(\mathbf{x}^{\prime}, \mathbf{x})(\mathbf{K}(\mathbf{x},\mathbf{x})+N\beta_{N}\mathbf{I})^{-1} \mathbf{y}.\] (6)

Proof.: Firstly, substituting \(\tilde{\mathbf{y}}\) into \(\mathbf{y}\):

\[f(\mathbf{x}^{\prime};\theta_{\infty}) =f(\mathbf{x}^{\prime};\theta_{0})+\mathbf{K}(\mathbf{x}^{\prime}, \mathbf{x})\left(\mathbf{K}(\mathbf{x},\mathbf{x})+N\beta_{N}\mathbf{I}\right)^{-1 }\left(\tilde{\mathbf{y}}-f(\mathbf{x};\theta_{0})\right)\] \[=f(\mathbf{x}^{\prime};\theta_{0})+\mathbf{K}(\mathbf{x}^{\prime}, \mathbf{x})\left(\mathbf{K}(\mathbf{x},\mathbf{x})+N\beta_{N}\mathbf{I}\right)^{-1 }\mathbf{y}\]

Now, using this new computational process, scaling it as \(\tilde{f}(\mathbf{x};\theta_{\infty})\):

\[\tilde{f}(\mathbf{x};\theta_{\infty})=f(\mathbf{x};\theta_{\infty})-f(\mathbf{x}^{ \prime};\theta_{0})=\mathbf{K}(\mathbf{x}^{\prime},\mathbf{x})\left(\mathbf{K}( \mathbf{x},\mathbf{x})+N\beta_{N}\mathbf{I}\right)^{-1}\mathbf{y},\]

achieving the desired zero-mean Gaussian process.

Details of the Experimental Setup

The Adam optimizer was used whenever our experiments needed gradient descent. A patience-based stopping rule was used where training was stopped if there was no improvement in the loss for 500 epochs. The other hyperparameters are given in the table below.

\begin{tabular}{l l}
**hyperparameter** & **value** \\ \hline no of hidden layers & 2 \\ size of hidden layer & 512 \\ non-linearity & softplus \\ softplus beta & 87.09 \\ scaling multiplier in the output & 3.5 \\ learning rate for network predicting mean & 1e-4 \\ learning rate for covariance predictor networks & 5e-5 \\ \end{tabular}

Moreover, we used trigonometric normalization, where an input point \(x\) is first scaled and shifted to lie between 0 and \(\pi\), obtaining a normalized point \(x^{\prime}\). The point \(x^{\prime}\) is then represented with a vector \([\sin(x^{\prime}),\cos(x^{\prime})]\).

## Appendix D Details on Estimating The Covariance

We now describe two of dealing with the term \(\sigma^{2}MM^{\top}\) in the covariance formula. Upper bounding the covariance is described in Section D.1, while estimating the exact covariance by fitting noisy targets is described in Section D.2.

### Upper Bounding the Covariance

First, we can simply ignore the term in our estimator, obtaining an upper bound on the covariance. We now characterize the tightness of the upper bound, i.e. the magnitude of the term

\[\sigma^{2}MM^{\top}=\sigma^{2}\mathbf{K}(\mathbf{x}^{\prime},\mathbf{x})( \mathbf{K}(\mathbf{x},\mathbf{x})+\sigma^{2}\mathbf{I})^{-1}(\mathbf{K}( \mathbf{x},\mathbf{x})+\sigma^{2}\mathbf{I})^{-1}\mathbf{K}(\mathbf{x}^{ \prime},\mathbf{x})^{\top}.\]

We do this is the following two lemmas.

**Lemma D.1**.: _When \(\mathbf{x}=\mathbf{x}^{\prime}\), i.e. on the training set, we have_

\[\sigma^{2}\mathbf{K}(\mathbf{x}^{\prime},\mathbf{x})(\mathbf{K}(\mathbf{x}, \mathbf{x})+\sigma^{2}\mathbf{I})^{-1}(\mathbf{K}(\mathbf{x},\mathbf{x})+ \sigma^{2}\mathbf{I})^{-1}\mathbf{K}(\mathbf{x}^{\prime},\mathbf{x})^{\top} \preccurlyeq\sigma^{2}\mathbf{I}.\]

Proof.: By assumption, \(\mathbf{K}(\mathbf{x}^{\prime},\mathbf{x})=\mathbf{K}(\mathbf{x},\mathbf{x})= \mathbf{K}\). Denote the diagonalization of \(\mathbf{K}\) with \(\mathbf{K}=\mathbf{U}\mathbf{\Lambda}\mathbf{U}^{\top}\). We have

\[\sigma^{2}\mathbf{K}(\mathbf{x}^{\prime},\mathbf{x})(\mathbf{K}( \mathbf{x},\mathbf{x})+\sigma^{2}\mathbf{I})^{-1}(\mathbf{K}(\mathbf{x}, \mathbf{x})+\sigma^{2}\mathbf{I})^{-1}\mathbf{K}(\mathbf{x}^{\prime},\mathbf{ x})^{\top}\] \[=\sigma^{2}\mathbf{K}(\mathbf{K}+\sigma^{2}\mathbf{I})^{-2} \mathbf{K}^{\top}\] \[=\sigma^{2}\mathbf{U}\mathbf{\Lambda}\mathbf{U}^{\top}(\mathbf{ U}\mathbf{\Lambda}\mathbf{U}^{\top}+\sigma^{2}\mathbf{I})^{-2}\mathbf{U} \mathbf{\Lambda}\mathbf{U}^{\top}\] \[=\sigma^{2}\mathbf{U}\mathbf{\Lambda}\mathbf{U}^{\top}\mathbf{U} (\mathbf{\Lambda}+\sigma^{2}\mathbf{I})^{-2}\mathbf{U}^{\top}\mathbf{U} \mathbf{\Lambda}\mathbf{U}^{\top}\] \[=\sigma^{2}\mathbf{U}\mathbf{\Lambda}(\mathbf{\Lambda}+\sigma^{ 2}\mathbf{I})^{-2}\mathbf{\Lambda}\mathbf{U}^{\top}.\]

It can be seen that the diagonal entries of \(\mathbf{\Lambda}(\mathbf{\Lambda}+\sigma^{2}\mathbf{I})^{-2}\mathbf{\Lambda}\) are less than or equal one. 

The Lemma above, stated in words, implies that, on the training set, the variance estimates that come from using the upper bound (which doesn't require us to fit noisy targets as in Section D.2) are off by at most \(\sigma^{2}\).

We now give another Lemma, which characterizes the upper bound on arbitrary test points, not just the training set.

**Lemma D.2**.: _Denote by \(\lambda_{\text{max}}\) the maximum singular value of \(\mathbf{K}(\mathbf{x}^{\prime},\mathbf{x}^{\prime})\). Then we have_

\[\left\|\sigma^{2}\mathbf{K}(\mathbf{x}^{\prime},\mathbf{x})(\mathbf{K}( \mathbf{x},\mathbf{x})+\sigma^{2}\mathbf{I})^{-1}(\mathbf{K}(\mathbf{x}, \mathbf{x})+\sigma^{2}\mathbf{I})^{-1}\mathbf{K}(\mathbf{x}^{\prime},\mathbf{ x})^{\top}\right\|_{2}\leq\frac{1}{4}\lambda_{\text{max}}.\]

[MISSING_PAGE_EMPTY:10]

For each fully connected layer \(l\), the weight matrix \(W^{(l)}\in\mathbb{R}^{n_{l}\times n_{l-1}}\) and the bias vector \(b^{(l)}\in\mathbb{R}^{n_{l}}\) are initialized from a Gaussian distribution with mean zero and standard deviations \(\sigma_{w}\) and \(\sigma_{b}\), respectively:

\[W^{(l)}_{ij}\sim\mathcal{N}(0,\sigma_{w}^{2}),\quad b^{(l)}_{j}\sim\mathcal{N} (0,\sigma_{b}^{2}),\]

where \(\sigma_{w}\) and \(\sigma_{b}\) are fixed values set as hyperparameters during initialization (we use \(\sigma_{w}=\sigma_{b}=1\)).

The network uses a non-linear activation function \(\sigma:\mathbb{R}\rightarrow\mathbb{R}\) with bounded second derivative, ensuring Lipschitz continuity. The output of each layer \(l\) is scaled by \(1/\sqrt{n_{l}}\) to maintain the appropriate magnitude, particularly when considering the infinite-width limit:

\[a^{(l)}=\sigma\left(\frac{1}{\sqrt{n_{l}}}W^{(l)}a^{(l-1)}+b^{(l)}\right),\]

where \(a^{(l)}\) is the output of layer \(l\), and \(a^{(0)}=x\) is the input to the network.

The final layer output is further scaled by a constant factor \(c_{\text{out}}\) to ensure that the overall network output remains within the desired range. Specifically, the output \(f(x;\theta)\) is given by:

\[f(x;\theta)=\frac{c_{\text{out}}}{\sqrt{n_{L}}}W^{(