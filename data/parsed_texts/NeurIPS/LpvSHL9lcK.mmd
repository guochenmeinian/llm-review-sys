# Probabilistic Graph Rewiring via Virtual Nodes

 Chendi Qian\({}^{1*}\) Andrei Manolache\({}^{234}\) Christopher Morris\({}^{1}\) Mathias Niepert\({}^{23}\)

\({}^{1}\)Computer Science Department, RWTH Aachen University, Germany

\({}^{2}\)Computer Science Department, University of Stuttgart, Germany

\({}^{3}\)IMPRS-IS \({}^{4}\)Bitdefender, Romania

chendi.qian@log.rwth-aachen.de

andrei.manolache@ki.uni-stuttgart.de

These authors contributed equally.Co-senior authorship.

###### Abstract

Message-passing graph neural networks (MPNNs) have emerged as a powerful paradigm for graph-based machine learning. Despite their effectiveness, MPNNs face challenges such as under-reaching and over-squashing, where limited receptive fields and structural bottlenecks hinder information flow in the graph. While graph transformers hold promise in addressing these issues, their scalability is limited due to quadratic complexity regarding the number of nodes, rendering them impractical for larger graphs. Here, we propose _implicitly rewired message-passing neural networks_ (IPR-MPNNs), a novel approach that integrates _implicit_ probabilistic graph rewiring into MPNNs. By introducing a small number of virtual nodes, i.e., adding additional nodes to a given graph and connecting them to existing nodes, in a differentiable, end-to-end manner, IPR-MPNNs enable long-distance message propagation, circumventing quadratic complexity. Theoretically, we demonstrate that IPR-MPNNs surpass the expressiveness of traditional MPNNs. Empirically, we validate our approach by showcasing its ability to mitigate under-reaching and over-squashing effects, achieving state-of-the-art performance across multiple graph datasets. Notably, IPR-MPNNs outperform graph transformers while maintaining significantly faster computational efficiency.

## 1 Introduction

_Message-passing graph neural networks_ (MPNNs) (Gilmer et al., 2017; Scarselli et al., 2008) recently emerged as the most prominent machine-learning architecture for graph-structured, applicable to a large set of domains where data is naturally represented as graphs, such as bioinformatics (Jumper et al., 2021; Wong et al., 2023), social network analysis (Easley et al., 2012), and combinatorial optimization (Cappart et al., 2023; Qian et al., 2024).

MPNNs have been studied extensively in theory and practice (Boker et al., 2023; Gilmer et al., 2017; Kipf and Welling, 2017; Maron et al., 2019; Morris et al., 2019, 2021, 2023; Velickovic et al., 2018; Xu et al., 2019). Recent works have shown that MPNNs suffer from over-squashing (Alon and Yahav, 2021), where bottlenecks arise from stacking multiple layers leading to large receptive fields, and under-reaching (Barcelo et al., 2020), where distant nodes fail to communicate effectively because MPNNs' receptive fields are too narrow. These phenomena become prevalent when dealing with graphs with a large diameter, potentially hindering the performance of MPNNs on essential applications that depend on long-range interactions, such as protein folding (Gromiha and Selvaraj, 1999). However, modeling long-range interactions in atomistic systems such as proteins remains achallenging problem often solved in an ad-hoc fashion using coarse-graining methods (Saunders and Voth, 2013; Husic et al., 2020), effectively grouping the input nodes into cluster nodes.

Recently, _graph rewiring_(Bober et al., 2022; Deac et al., 2022; Gutteridge et al., 2023; Karhadkar et al., 2022; Shirzad et al., 2023; Topping et al., 2021) techniques emerged, adapting the graph structure to enhance connectivity and reduce node distance through methods ranging from edge additions to leveraging spectral properties and expander graphs. However, these approaches typically employ heuristic methods for selecting node pairs to rewire. Furthermore, _graph transformers_ (GTs) (Chen et al., 2022; Dwivedi et al., 2022; He et al., 2023; Muller et al., 2023; Muller and Morris, 2024; Rampasek et al., 2022) and adaptive techniques like those by Errica et al. (2023) improve handling of long-range relationships but face challenges of quadratic complexity and extensive parameter sets, limiting their scalability.

Most similar to IPR-MPNNs is the work of Qian et al. (2023), which, like IPR-MPNNs, leverage recent techniques in differentiable \(k\)-subset sampling (Ahmed et al., 2023) to learn to add or remove edges of a given graph. However, like GTs, their approach suffers from quadratic complexity due to their need to compute a score for each node pair.

Present WorkOur proposed IPR-MPNN architecture advances end-to-end probabilistic adaptive graph rewiring. Unlike the PR-MPNN framework of Qian et al. (2023), which suffers from quadratic complexity since the edge distribution is modeled _explicitly_, IPR-MPNNs _implicitly_ transmits information across different parts of a graph by learning to connect the existing graph with newly-added virtual nodes, effectively circumventing quadratic complexity; see Figure 1 for a high-level overview of IPR-MPNNs. Our contributions are as follows.

1. We introduce IPR-MPNNs, adding virtual nodes to graphs, and learn to rewire them to the existing nodes end-to-end. IPR-MPNNs successfully overcome the quadratic complexity of graph transformers and previous graph rewiring techniques.
2. Theoretically, we demonstrate that IPR-MPNNs exceed the expressive capacity of standard MPNNs, typically limited by the \(1\)-dimensional Weisfeiler--Leman algorithm.
3. Empirically, we show that IPR-MPNNs outperform standard MPNN and GT architectures on a large set of established benchmark datasets, all while maintaining significantly faster computational efficiency.
4. We show that IPR-MPNNs are reducing the total effective resistance (Black et al., 2023) of multiple molecular datasets while also significantly improving the layer-wise sensitivity

Figure 1: Overview of how IPR-MPNNs implicitly rewire a graph through adding virtual nodes. IPR-MPNNs use an _upstream MPNN_ to learn priors \(\bm{\theta}\) for connecting original nodes with virtual nodes via edges, parameterizing a probability mass function conditioned on exactly-\(k\) constraints. Subsequently, we sample exactly \(k\) edges from this distribution for each original node, connecting it to \(k\) virtual nodes. We input the resulting graph to a _downstream model_, typically an MPNN, for the final predictions task, propagating information from (1) original nodes to virtual nodes, (2) among virtual nodes, and (3) among original nodes. On the backward pass, the gradients of the loss \(\ell\) regarding the parameters \(\bm{\theta}\) are approximated through the derivative of the exactly-\(k\) marginals.

[Di Giovanni et al., 2023, Xu et al., 2018] between distant nodes when compared to the base model.

_In summary, IPR-MPNNs represent a significant advancement towards scalable and adaptable MPNNs. They enhance expressiveness and adaptability to various data distributions while scaling to large graphs effectively._

### Related Work

In the following, we discuss relevant related work.

MPNNsRecently, MPNNs [Gilmer et al., 2017, Scarselli et al., 2008] emerged as the most prominent graph machine learning architecture. Notable instances of this architecture include, e.g., Duvenaud et al. [2015], Hamilton et al. [2017], and Velickovic et al. [2018], which can be subsumed under the message-passing framework introduced in Gilmer et al. [2017]. In parallel, approaches based on spectral information were introduced in, e.g., Bruna et al. [2014], Defferrard et al. [2016], Gama et al. [2019], Kipf and Welling [2017], Levie et al. [2019], and Monti et al. [2017]--all of which descend from early work in Baskin et al. [1997], Guler and Kuchler [1996], Kireev [1995], Merkwirth and Lengauer [2005], Micheli and Sestito [2005], Micheli [2009], Scarselli et al. [2008], and Sperduti and Starita [1997].

Limitations of MPNNsMPNNs are inherently biased towards encoding local structures, limiting their expressive power [Morris et al., 2019, 2021, Xu et al., 2019]. Specifically, they are at most as powerful as distinguishing non-isomorphic graphs or nodes with different structural roles as the \(1\)_-dimensional Weisfeiler-Leman algorithm_[Weisfeiler and Leman, 1968], a well-studied heuristic for the graph isomorphism problem; see Section C. Additionally, they cannot capture global or long-range information, often linked to phenomena such as under-reaching [Barcelo et al., 2020] or over-squashing [Alon and Yahav, 2021], with the latter being heavily investigated in recent works.

Graph TransformersDifferent from the above, graph transformers [Chen et al., 2022, 2022, Dwivedi et al., 2022, He et al., 2023, Hussain et al., 2022, Kim et al., 2022, Ma et al., 2023, Mialon et al., 2021, Muller et al., 2023, Muller and Morris, 2024, Rampasek et al., 2022, Shirzad et al., 2023] and similar global attention mechanisms [Liu et al., 2021, Wu et al., 2021] marked a shift from local to global message passing, aggregating over all nodes. While not understood in a principled way, empirical studies indicate that graph transformers possibly alleviate over-squashing; see Muller et al. [2023]. However, all transformers suffer from their quadratic space and memory requirements due to computing an attention matrix.

Rewiring Approaches for MPNNsSeveral recent works aim to circumvent over-squashing via graph rewiring. The most straightforward way of graph rewiring is incorporating multi-hop neighbors. For example, Bruel-Gabrielsson et al. [2022] rewires the graphs with \(k\)-hop neighbors and virtual nodes and augments them with positional encodings. MixHop [Abu-El-Haija et al., 2019], SIGN [Frasca et al., 2020], DIGL [Gasteiger et al., 2019], and SP-MPNN [Abboud et al., 2022] can also be considered as graph rewiring as they can reach further-away neighbors in a single layer. Particularly, Gutteridge et al. [2023] rewires the graph similarly to Abboud et al. [2022] but with a novel delay mechanism, showcasing promising empirical results. Several rewiring methods depend on particular metrics, e.g., Ricci or Forman curvature [Bober et al., 2022] and balanced Forman curvature [Topping et al., 2021]. In addition, Deac et al. [2022], Shirzad et al. [2023] utilize expander graphs to enhance message passing and connectivity, while Karhadkar et al. [2022] resort to spectral techniques, and Banerjee et al. [2022] propose a greedy random edge flip approach to overcome over-squashing. DiffWire [Arnaiz-Rodriguez et al., 2022] conducts fully differentiable and parameter-free graph rewiring by leveraging the Lovasz bound and spectral gap. Refining Topping et al. [2021], Di Giovanni et al. [2023] analyzed how the architectures' width and graph structure contribute to the over-squashing problem, showing that over-squashing happens among nodes with high commute time, stressing the importance of rewiring techniques. Contrary to our proposed method, these strategies to mitigate over-squashing rely on heuristic rewiring methods or purely randomized approaches that may not adapt well to a given prediction task. LASER [Barbero et al., 2023] performs graph rewiring while respecting the original graph structure. The recent work S2GCN [Geisler et al., 2024] combines spectral and spatial graph filters and implicitly introduces graph rewiring for message passing. Mostsimilar to IPR-MPNNs is the work of Qian et al. (2023), which, like IPR-MPNNs, leverage recent techniques in differentiable \(k\)-subset sampling (Ahmed et al., 2023) to learn to add or remove edges of a given graph. However, like GTs, their approach suffers from quadratic complexity due to their need to compute a score for each node pair. In addition to the differentiable \(k\)-subset sampling (Ahmed et al., 2023) method we use in this work, there are other gradient estimation approaches such as Gumbel SoftSub-ST (Xie and Ermon, 2019) and I-MLE (Niepert et al., 2021; Minervini et al., 2023).

There is also a large set of works from graph structure learning proposing heuristical graph rewiring approaches and hierarchical MPNNs; see Section A for details.

## 2 Background

In the following, we introduce notation and formally define MPNNs.

NotationsLet \(\mathbb{N}\coloneqq\{1,2,3,\dots\}\). For \(n\geq 1\), let \([n]\coloneqq\{1,\dots,n\}\subset\mathbb{N}\). We use \(\{\dots\}\) to denote multisets, i.e., the generalization of sets allowing for multiple instances for each of its elements. A _graph_\(G\) is a pair \((V(G),E(G))\) with _finite_ sets of _nodes_ or _vertices_\(V(G)\) and _edges_\(E(G)\subseteq\{\{u,v\}\subseteq V(G)\mid u\neq v\}\). If not otherwise stated, we set \(n\coloneqq|V(G)|\), and the graph is of _order_\(n\). We also call the graph \(G\) an \(n\)-order graph. For ease of notation, we denote the edge \(\{u,v\}\) in \(E(G)\) by \((u,v)\) or \((v,u)\). Throughout the paper, we use standard notations, e.g., we denote the _neighborhood_ of a vertex \(v\) by \(N(v)\) and \(\ell(v)\) denotes its discrete vertex label, and so on; see Section B for details.

Message-passing Graph Neural NetworksIntuitively, MPNNs learn a vectorial representation, i.e., a \(d\)-dimensional real-valued vector, representing each vertex in a graph by aggregating information from neighboring vertices. Let \(\bm{G}=(G,\bm{X})\) be an \(n\)-order attributed graph with node feature matrix \(\bm{X}\in\mathbb{R}^{n\times d}\), for \(d>0\), following, Gilmer et al. (2017) and Scarselli et al. (2008), in each layer, \(t>0\), we compute vertex features

\[\bm{h}_{v}^{(t)}\coloneqq\mathsf{UPD}^{(t)}\Big{(}\bm{h}_{v}^{(t-1)}, \mathsf{AGG}^{(t)}\big{(}\{\!\bm{h}_{u}^{(t-1)}\mid u\in N(v)\}\!\big{)} \Big{)}\in\mathbb{R}^{d},\]

where \(\mathsf{UPD}^{(t)}\) and \(\mathsf{AGG}^{(t)}\) may be parameterized functions, e.g., neural networks, and \(\bm{h}_{v}^{(t)}\coloneqq\bm{X}_{v}\). In the case of graph-level tasks, e.g., graph classification, one uses

\[\bm{h}_{G}\coloneqq\mathsf{READOUT}\big{(}\{\!\bm{h}_{v}^{(T)}\mid v\in V(G )\}\!\big{)}\in\mathbb{R}^{d},\]

to compute a single vectorial representation based on learned vertex features after iteration \(T\). Again, \(\mathsf{READOUT}\) may be a parameterized function, e.g., a neural network. To adapt the parameters of the above three functions, they are optimized end-to-end, usually through a variant of stochastic gradient descent, e.g., Kingma and Ba (2015), together with the parameters of a neural network used for classification or regression.

## 3 Implicit Probabilistically Rewired MPNNs

_Implicit probabilistically rewired message-passing neural networks_ (IPR-MPNNs) learn a probability distribution over edges connecting the _original nodes_ of a graph to additional _virtual nodes_, providing an _implicit_ way of enhancing the graph connectivity. To learn to rewire original nodes with these added virtual nodes, IPR-MPNNs use an _upstream model_, usually an MPNN, to generate scores or (unnormalized) priors \(\bm{\theta}\coloneqq h_{\bm{u}}(\bm{A}(G),\bm{X})\in\mathbb{R}^{n\times m}\), where \(G\) is an \(n\)-order graph with adjacency matrix \(\bm{A}(G)\in\{0,1\}^{n\times n}\), node feature matrix \(\bm{X}\in\mathbb{R}^{n\times d}\), for \(d>0\), and number of virtual nodes \(m\) with \(m\ll n\).

IPR-MPNNs use the priors \(\bm{\theta}\) from the upstream model to sample new edges between the original nodes and the \(m\) virtual nodes from the posterior constrained to exactly \(k\) edges, thus obtaining an _assignment matrix_\(\bm{H}\in\{0,1\}^{n\times m}\), i.e., an adjacency matrix between the input and virtual nodes. The assignment matrix \(\bm{H}\) is then used in a _downstream model_\(f_{\bm{v}}\), utilized for solving our downstream task, e.g., graph-level classification or regression. Leveraging recent advancements in gradient estimation for \(k\)-subset sampling (Ahmed et al., 2023), the upstream and downstream models are jointly optimized, enabling the model to be trained end-to-end; see below.

Hence, unlike PR-MPNNs (Qian et al., 2023), which in the worst case _explicitly_ model an edge distribution for all \(n^{2}\) possible edge candidates for rewiring, IPR-MPNNs leverage virtual nodes for implicit rewiring and passing long-range information. Therefore, IPR-MPNNs benefit from better computation complexity while being more expressive than the \(1\)-WL; see Section 4. Moreover, intuitively, it is also easier to model a distribution of edges connected to a few virtual nodes than to learn the explicit distribution of all possible edges in a graph. More specifically, while in PR-MPNNs, the priors \(\bm{\theta}\) can have a size of up to \(n^{2}\) for an \(n\)-order graph, IPR-MPNNs use \(m\cdot n\) parameters, therefore significantly enhancing both computational efficiency and model simplicity.

In the following, we describe the IPR-MPNN architecture in detail.

Sampling EdgesLet \(\mathfrak{A}_{n}\) represent the adjacency matrices of \(n\)-order graphs. Consider \((G,\bm{X})\) as a graph of order \(n\) with adjacency matrix \(\bm{A}(G)\in\mathfrak{A}_{n}\) and node feature matrix \(\bm{X}\in\mathbb{R}^{n\times d}\), for \(d>0\). IPR-MPNNs maintain a parameterized upstream model \(h_{\bm{u}}\colon\mathfrak{A}_{n}\times\mathbb{R}^{n\times d}\to\bm{\rho}\), usually implemented through an MPNN and parameterized by \(\bm{u}\). The upstream model transforms an adjacency matrix along with its node attributes into a set of unnormalized node priors \(\bm{\theta}\in\Theta\subseteq\mathbb{R}^{n\times m}\), with \(m\) denoting the predefined number of virtual nodes. Formally,

\[\bm{\theta}\coloneqq h_{\bm{u}}(\bm{A}(G),\bm{X})\in\mathbb{R}^{n\times m}.\]

The matrix of priors \(\bm{\theta}\) serves as the parameter matrix for the conditional probability mass function from which the assignment matrix \(\bm{H}\in\{0,1\}^{n\times m}\) is sampled. Crucially and contrary to prior work (Qian et al., 2023), these edges connect the input and a _small_ number \(m\) of virtual nodes. Hence, each row of matrix \(\bm{\theta}_{i}\in\mathbb{R}^{m}\) represents the unnormalized probability of assigning node \(i\) to each virtual node. Formally, we have,

\[p_{\bm{\theta}}(\bm{H}_{i:})\coloneqq\prod_{j=1}^{M}p_{\bm{\theta}_{ij}}(\bm {H}_{ij}),\text{ for }i\in[n],\]

where \(p_{\bm{\theta}_{ij}}(\bm{H}_{ij}=1)=\operatorname{sigmoid}(\bm{\theta}_{ij})\) and \(p_{\bm{\theta}_{ij}}(\bm{H}_{ij}=0)=1-\operatorname{sigmoid}(\bm{\theta}_{ij})\). Without loss of generality, we allow each node to be assigned to \(k\) virtual nodes, with \(k\in[m]\). That is, each row of the sampled assignment matrix has exactly \(k\) non-zero entries, i.e.,

\[p_{(\bm{\theta},k)}(\bm{H})\coloneqq\left\{\begin{array}{ll}p_{\bm{\theta} }(\bm{H})/Z&\text{ if }\|\bm{H}_{i:}\|_{1}=k,\text{for all }i\in[n],\\ 0&\text{ otherwise},\end{array}\right.\quad\text{with}\quad Z\coloneqq\sum_{ \begin{subarray}{c}\bm{B}\in\{0,1\}^{n\times m}:\\ \|\bm{B}_{i:}\|_{1}=k,\forall\,i\in[n]\end{subarray}}p_{\bm{\theta}}(\bm{B}).\]

We can potentially sample independently \(q\) times \(\bm{H}^{(i)}\sim p_{(\bm{\theta},k)}(\bm{H})\) and consequently obtain \(q\) multiple assignment matrices \(\bar{\bm{H}}\coloneqq\{\!\!\{\bm{H}^{(1)},\bm{H}^{(2)},\ldots,\bm{H}^{(q)}\}\!\!\}\), which, together with corresponding number of copies of \(\bm{A}(G)\) and \(\bm{X}\), will be utilized by the downstream model for the tasks.

Message-passing Architecture of IPR-MPNNsHere, we outline the message-passing scheme after adding virtual nodes and edges. Consider an \(n\)-order graph \(G\) and a virtual node set \(C(G)\) of cardinality \(m\), where each original node \(v\in V(G)\coloneqq[n]\) is assigned to \(k\in[m]\) virtual nodes. We assign original nodes \(v\) to a subset of the virtual node using the function \(a\colon V(G)\to[C(G)]_{k}\), where \(v\mapsto\{c\in C(G)\mid\bm{H}_{vc}=1\}\) and \([C(G)]_{k}\) is the set of \(k\)-element subsets of the virtual nodes. Conversely, each virtual node \(c\in C(G)\) links to several original nodes. Hence, we define an inverse assignment as the set of all original nodes assigned to virtual node \(c\), i.e., \(a^{-1}(c)\coloneqq\{v\in V(G)\mid c\in a(v)\}\). Across the graph, the union of these inverse assignments equals the set of original nodes, i.e., \(\bigcup_{c}a^{-1}(c)=V(G)\).

We represent the embedding of an original node \(v\in V(G)\) at any given layer \(t\geq 0\) as \(\bm{h}_{v}^{(t)}\), and similarly, the embedding for a virtual node \(c\in C(G)\) as \(\bm{g}_{c}^{(t)}\). To compute these embeddings, IPR-MPNNs compute initial embeddings for each virtual node. Subsequently, the architecture updates the virtual nodes via the adjacent original nodes' embeddings. Afterward, the virtual nodes exchange messages, and finally, the virtual nodes update adjacent original nodes' embeddings. Below, we outline the steps in order of execution involved in our message-passing algorithm in detail.

Initializing Virtual Node EmbeddingsBefore executing inter-hierarchical message passing, we need to initialize the embeddings of virtual nodes. To that, given the node assignments \(a(v)\), for \(v\in V(G)\), we can effectively divide the original graph into several subgraphs, where nodes sharing the same assignment label are grouped together to form an induced subgraph. Formally, for any virtual node \(c\in C(G)\), we have the induced subgraph \(G_{c}\) with node subset \(V_{c}(G_{c})\coloneqq\{v\mid v\in V(G)\cap a^{-1}(c)\}\) and edge set \(E_{c}(G_{c})\coloneqq\{\{u,v\}\mid\{u,v\}\in E(G),u\in a^{-1}(c)\text{ and }v\in a^{-1}(c)\}\). The initial attributes of the virtual node \(c\) are defined by the node features of its corresponding subgraph \(G_{c}\), calculated using an MPNN, i.e.,

\[\bm{g}_{c}^{(0)}\coloneqq\mathsf{MPNN}\left(G_{c}\right),\text{ for }c\in C(G).\] (1)

Alternatively, we can generate random features for each virtual node as initial node features or assign unique identifiers to them.

Updating Virtual NodesIn each step \(t>0\), we collect the embeddings from original nodes to virtual nodes according to their assignments and obtain intermediate virtual node embeddings

\[\bar{\bm{g}}_{c}^{(t)}\coloneqq\mathsf{AGGn}^{(t)}\left(\{\!\{\bm{h}_{v}^{(t- 1)}\mid v\in a^{-1}(c)\}\!\}\right),\text{ for }c\in C(G),\]

where \(\mathsf{AGGn}^{(t)}\) denotes some permutation-equivariant aggregation function designed for multisets.

Updating Among Virtual NodesWe assume the virtual nodes form a complete, undirected, unweighted graph, and we perform message passing among the virtual nodes to update their embeddings. That is, at step \(t\), we set

\[\bm{g}_{c}^{(t)}\coloneqq\mathsf{UPDc}^{(t)}\left(\bm{g}_{c}^{(t-1)},\bar{\bm {g}}_{c}^{(t)},\mathsf{AGGc}^{(t)}\left(\{\!\{\bm{\bar{g}}_{j}^{(t)}\mid j\in C (G),j\neq c\}\!\}\right)\right),\]

where \(\mathsf{UPDc}\) and \(\mathsf{AGGc}\) are the update and neighborhood aggregation functions for virtual nodes.

Updating Original NodesFinally, we redistribute the embeddings from the virtual nodes back to the base nodes. This update process considers both the neighbors in the original graph and the virtual nodes to which the original nodes are assigned. The updating mechanism is detailed in the following equation,

\[\bm{h}_{v}^{(t)}\coloneqq\mathsf{UPD}^{(t)}\left(\bm{h}_{v}^{(t-1)},\mathsf{AGG }^{(t)}\left(\{\!\{\bm{h}_{u}^{(t-1)}\mid u\in N(v)\}\!\}\right),\mathsf{DS}^{ (t)}\left(\{\!\{\bm{g}_{c}^{(t)}\mid c\in a(v)\}\!\}\right)\right),\] (2)

where \(\mathsf{UPD}\) is the update function, \(\mathsf{AGG}\) is the aggregation function, and \(\mathsf{DS}\) is the distributing function that incorporates embeddings from virtual nodes back to the original nodes.

Gradient EstimationIn the context of our downstream MPNN model described above, we define the set of its learnable parameters as \(\bm{v}\) and group these with the upstream model parameters \(\bm{u}\) into a combined tuple \(\bm{w}=(\bm{u},\bm{v})\). We express the downstream model as \(f_{\bm{v}}\), resulting in the loss function

\[L\left(\bm{A}(G),\bm{X},\bar{\bm{H}};\bm{w}\right)\coloneqq\mathbb{E}_{\bm{H}^ {(i)}\sim p_{(\bm{\theta},b)}(\bm{H})}\left[\ell\left(f_{\bm{v}}\left(\bm{A}(G ),\bm{X},\{\!\{\bm{H}^{(1)},\ldots,\bm{H}^{(q)}\}\!\}\right),y\right)\right].\]

While the gradients for the downstream model \(f_{\bm{v}}\) can be straightforwardly calculated via back-propagation, obtaining gradients for the upstream model parameters \(\bm{\theta}\) is more challenging, as the assignment matrices \(\bar{\bm{H}}\) are sampled from the priors \(\bm{\theta}\), a process which is not differentiable.

Similar to prior work [Qian et al., 2023], we utilize Simple[Ahmed et al., 2023], which efficiently estimates gradients under \(k\)-subset constraints. This method involves exact sampling in the forward phase and uses the marginal of the priors \(\mu(\bm{\theta})\in\mathbb{R}^{n\times m}\) during the backward phase to approximate gradients \(\nabla_{\bm{\theta}}L\approx\partial_{\bm{\theta}}\mu(\bm{\theta})\nabla_{\bm {H}}\ell\).

The following analysis shows that our proposed method circumvents the problem of being quadratic in the number of input nodes.

ComplexityAssuming a constant number of hidden dimensions and layers of the MPNNs, recall that the runtime complexity of a plain MPNN is \(\mathcal{O}(|E|)\), where \(|E|\) is the number of edges of a given graph. In the IPR-MPNN framework, we still have an MPNN backbone, augmented with inter-message passing involving virtual nodes. Hence, obtaining priors for the original nodes via an MPNN or even a simple MLP has a complexity of \(\mathcal{O}(|E|+n\cdot m)\), where \(m\) is the number of candidate virtual nodes to be selected. Sampling the node assignment with Ahmed et al. (2023) is in \(\mathcal{O}(n\cdot\log m\cdot\log k)\). The message aggregation and distribution between base nodes and virtual nodes have complexity \(\mathcal{O}(n\cdot k)\), where \(k\in[m]\) is the number of nodes a node is assigned to. Finally, the intra-virtual node message passing is in \(\mathcal{O}(m^{2})\), as they are fully connected. In summary, IPR-MPNNs have a running time complexity in \(\mathcal{O}(|E|+n\cdot m+m^{2})\). Since \(m\ll n\) is a small constant, IPR-MPNNs show great potential due to their low complexity compared to the quadratic worst-case complexities of graph transformers (Muller et al., 2023) and other rewiring methods, e.g., Gutteridge et al. (2023); Qian et al. (2023).

## 4 Expressive Power

In this section, we analyze the extent to which IPR-MPNNs can separate non-isomorphic graphs on which the \(1\)-WL isomorphism test fails (Xu et al., 2019; Morris et al., 2019), and whether IPR-MPNNs can maintain isomorphisms between pairs of graphs. We adopt the notion of probabilistic separation from Qian et al. (2023).

Our arguments rely on the ability of our upstream MPNN to assign arbitrary and distinct exactly-\(k\) distributions for each color class. By modifying the graph structure, we can make the rewired graphs \(1\)-WL-distinguishable, enabling our downstream MPNN to separate them. However, since the expressiveness of the upstream model is also equivalent to \(1\)-WL, there is a possibility of still separating isomorphic graphs.

The following result demonstrates that we can preserve almost all partial subgraph isomorphisms.

**Theorem 4.1**.: _Let \(k>0\), \(\varepsilon\in(0,1)\), and \(G\), \(H\) be two graphs with identical \(1\)-WL stable colorings. Let \(M\) be the set of ordered virtual nodes, \(V_{G}\) and \(V_{H}\) be the subset of nodes in \(G\) and \(H\) that have a color class of cardinality \(1\), with \(|V_{G}|=|V_{H}|=d\), and \(W_{G}\), \(W_{H}\) the subset of nodes that have a color class of cardinality greater than \(1\), with \(|W_{G}|=|W_{H}|=n\). Then, for all choices of \(1\)-WL-equivalent functions \(f\),_

1. _there exists a conditional probability mass function_ \(p_{(\boldsymbol{\theta},k)}\) _that does_ not _separate_ \(G[V_{G}]\) _and_ \(H[V_{H}]\) _with probability at least_ \(1-\varepsilon\)_._
2. _There exists a conditional probability mass function_ \(p_{(\boldsymbol{\theta},k)}\) _that separates_ \(G[W_{G}]\) _and_ \(H[W_{H}]\) _with probability strictly greater than_ \(0\)_._

We argue that preserving these partial subgraph isomorphisms is sufficient for most examples in practice. Indeed, our empirical findings show that we can successfully solve both the Exp and Csl datasets, whereas a \(1\)-WL model obtains random performance; see Table A11, Table A10.

The next corollary follows the above and recovers Theorem 4.1 from Qian et al. (2023). The corollary tells us that, even if there are isomorphic graphs that we risk making separable, we will maintain the isomorphism between almost all isomorphic pairs.

Figure 2: Comparing model sensitivity across different layers for the two most distant nodes from graphs from the Zinc dataset. On the left, we compare the sensitivity for models with a varying number of layers. We can observe that IPR-MPNNs maintain a high sensitivity even for the last layer, while the base models have the sensitivity decaying to \(0\). On the right, we compare models with a different number of virtual nodes, observing that the results are similar for all of the variants.

**Corollary 4.1.1**.: _For sufficiently large \(n\), for every \(\varepsilon\in(0,1)\), a set \(m\) of ordered virtual nodes, and \(k>0\), we have that almost all pairs, in the sense of Babai et al. (1980), of isomorphic \(n\)-order graphs \(G\) and \(H\) and all permutation-invariant, \(1\)-WL-equivalent functions \(f\colon\mathfrak{A}_{n}\to\mathbb{R}^{d}\), \(d>0\), there exists a probability mass function \(p_{(\boldsymbol{\theta},k)}\) that separates the graph \(G\) and \(H\) with probability at most \(\varepsilon\) regarding \(f\)._

The previous theorems show that we are preserving isomorphisms better than purely randomized approaches while being more powerful than \(1\)-WL since we can separate non-isomorphic graphs with a probability strictly greater than \(0\). We provide the proofs and examples in Section E.

## 5 Experimental Setup and Results

To empirically validate the effectiveness of our IPR-MPNN framework, we conducted a series of experiments on both synthetic and real-world molecular datasets, answering the following research questions. An open repository of our code can be accessed at https://github.com/chendiqian/IPR-MPNN.

**Q1**: Do IPR-MPNNs alleviate over-squashing and under-reaching?
**Q2**: Do IPR-MPNNs demonstrate enhanced expressivity compared to MPNNs?
**Q3**: How do IPR-MPNNs compare in predictive performance on molecular datasets against other rewiring methods and graph transformers?
**Q4**: Does the lower theoretical complexity of IPR-MPNNs translate to faster runtimes in practice?

\begin{table}
\begin{tabular}{l c c c c c|c} \hline \hline
**Property** & GIN [2019] & R-GIN-FA [2018] & SPN [2022] & DRFW-GIN [2023] & IPR-MPNN [2023] & IPR-MPNN \\ \hline MU & 2.64\(\pm\)0.01 & 2.54\(\pm\)0.09 & 2.32\(\pm\)0.02 & 1.93\(\pm\)0.06 & 1.99\(\pm\)0.00 & 2.01 \(\pm\)0.01 \\ ALPHA & 7.67\(\pm\)10.2 & 2.28\(\pm\)0.04 & 1.77\(\pm\)0.00 & 1.63\(\pm\)0.03 & 2.28\(\pm\)0.06 & 1.36 \(\pm\)0.00 \\ HOMO & 1.70\(\pm\)0.02 & 1.26\(\pm\)0.02 & 1.26\(\pm\)0.00 & 1.66\(\pm\)0.01 & 1.46\(\pm\)0.01 & 1.07 \(\pm\)0.01 \\ LUMO & 1.05\(\pm\)0.01 & 1.34\(\pm\)0.04 & 1.19\(\pm\)0.05 & 1.13\(\pm\)0.02 & 1.12\(\pm\)0.01 & 1.03 \(\pm\)0.09 \\ GAP & 3.37\(\pm\)0.03 & 1.96\(\pm\)0.04 & 1.89\(\pm\)0.01 & 1.74\(\pm\)0.02 & 1.70\(\pm\)0.01 & 1.61 \(\pm\)0.08 \\ BZV & 23.53\(\pm\)1.08 & 1.21\(\pm\)0.03 & 1.06\(\pm\)0.08 & 9.39\(\pm\)0.13 & 1.04\(\pm\)13.5 & 8.17 \(\pm\)0.03 \\ ZPVE & 66.87\(\pm\)1.45 & 5.03\(\pm\)0.06 & 2.77\(\pm\)0.17 & 2.73\(\pm\)0.19 & 4.73\(\pm\)0.08 & 1.96 \(\pm\)0.07 \\ U0 & 21.48\(\pm\)0.77 & 2.16\(\pm\)1.12 & 1.13\(\pm\)0.01 & 1.08\(\pm\)0.09 & 2.23\(\pm\)0.13 & 0.74 \(\pm\)0.01 \\ U1 & 21.59\(\pm\)0.39 & 2.32\(\pm\)0.18 & 1.03\(\pm\)0.09 & 0.99\(\pm\)0.08 & 2.31\(\pm\)0.06 & 0.79 \(\pm\)0.12 \\ H1 & 21.96\(\pm\)1.24 & 2.26\(\pm\)0.19 & 1.05\(\pm\)0.04 & 1.06\(\pm\)0.09 & 2.66\(\pm\)0.01 & 0.75 \(\pm\)0.14 \\ G & 19.53\(\pm\)0.47 & 2.04\(\pm\)0.24 & 0.97\(\pm\)0.00 & 1.06\(\pm\)0.14 & 2.24\(\pm\)0.01 & 0.62 \(\pm\)0.13 \\ CV & 7.34\(\pm\)0.06 & 1.86\(\pm\)0.03 & 1.36\(\pm\)0.04 & 1.24\(\pm\)0.02 & 1.44\(\pm\)0.01 & 1.03 \(\pm\)0.04 \\ Omega & 0.60\(\pm\)0.03 & 0.80\(\pm\)0.04 & 0.57\(\pm\)0.04 & 0.55\(\pm\)0.01 & 0.48\(\pm\)0.00 & 0.45 \(\pm\)0.03 \\ \hline \hline \end{tabular}
\end{table}
Table 1: We compare IPR-MPNN on QM9 with the base downstream GIN model (Xu et al., 2019), two graph rewiring techniques (Gutteridge et al., 2023; Qian et al., 2023), a multi-hop MPNN (Abboud et al., 2022), and the relational GIN (Schlichtkrull et al., 2018). The best-performing method is colored in green, the second-best in blue, and third in orange. IPR-MPNN obtains the best result on all targets, except for MU, where it obtains the second-best result.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline
**Model** & \multicolumn{2}{c}{Perfitless-func \(\uparrow\)} & \multicolumn{2}{c}{Pertolgs-struct \(\downarrow\)} & \multicolumn{2}{c}{PCQM(\(1\)) \(\uparrow\)} & \multicolumn{2}{c}{PCQM(\(2\)) \(\uparrow\)} & \multicolumn{2}{c}{PCQM(\(3\)) \(\uparrow\)} \\ \hline GINE [(2019, 2023)] & 0.6621\(\pm\)0.0067 & 0.2473\(\pm\)0.0017 & 0.3509\(\pm\)0.0016 & 0.3725\(\pm\)0.000 & 0.4617\(\pm\)0.000 \\ GCN [(2017, 2023)] & 0.6560\(\pm\)0.0016 & 0.2460\(\pm\)0.0007 & 0.3424\(\pm\)0.0017 & 0.3631\(\pm\)0.000 & 0.4526\(\pm\)0.0006 \\ DRFW [(2023)] & 0.7150\(\pm\)0.0044 & 0.526\(\pm\)0.0015 & 0.3444\(\pm\)0.0017 & - & - \\ PR-MPNN [(2023)] & 0.6825\(\pm\)0.0065 & 0.2477\(\pm\)0.0005 & - & - & - \\ AMP [(2023)] & 0.7163\(\pm\)0.0058 & 0.2431\(\pm

**Datasets, Experimental Results, and Discussion** To address **Q1**, we investigate whether our method alleviates over-squashing and under-reaching by experimenting on Trees-NeighboursMatch [Alon and Yahav, 2021] and Trees-LeafCount [Qian et al., 2023]. On Trees-LeafCount with a tree depth of four, we obtain perfect performance on the test dataset with a one-layer downstream network, indicating we can alleviate under-reaching. Furthermore, on Trees-NeighboursMatch, our method obtains perfect performance to a depth up to six, effectively alleviating over-squashing, as shown in Figure A4. To quantitatively assess whether over-squashing is mitigated in real-world scenarios, we computed the average layer-wise sensitivity [Xu et al., 2018, Di Giovanni et al., 2023, Errica et al., 2023] between the most distant nodes in graphs from the Zinc dataset and compared these results with those from the baseline GINE model. Specifically, we compute the logarithm of the symmetric sensitivity between the most distant nodes \(u,v\) as \(\log\left(\left\lvert\vartheta\mathbf{h}_{v}^{l}/\partial\mathbf{h}_{u}^{l}+ \vartheta\mathbf{h}_{v}^{l}/\partial\mathbf{h}_{v}^{l}\right\rvert\right)\), where \(k\) to \(l\) represent the intermediate layers. We show that IPR-MPNNs maintain a high layer-wise sensitivity compared to the base model, as seen in Figure 2, implying that they can successfully account for long-range relationships, even with multiple stacked layers. Lastly, we measured the average total effective resistance [Black et al., 2023] of five molecular datasets before and after rewiring, showing in Figure 3 that IPR-MPNNs are successfully improving connectivity by reducing the average total effective resistance of all evaluated datasets.

For **Q2**, we conduct experiments on the Exp [Abboud et al., 2020] and Csl [Murphy et al., 2019] datasets to evaluate the expressiveness of IPR-MPNNs. The results, as detailed in Table A10 and Table A11, demonstrate that our IPR-MPNN framework handles these datasets effectively and exhibits improved expressiveness over the base \(1\)-WL-equivalent GIN model.

For answering **Q3**, we utilize several real-world molecular datasets--QM9 [Hamilton et al., 2017], Zink 12k [Jin et al., 2017], OGB-Molhiv [Hu et al., 2020], TUDataset [Morris et al., 2020a], and datasets from the long-range graph benchmark [Dwivedi et al., 2022b], namely Peptides and PCQM-Contact. Our results demonstrate that IPR-MPNNs effectively account for long-range relationships, achieving state-of-the-art performance on the Peptides and PCQM-Contact datasets, as detailed in Table 2. Notably, on the PCQM-Contact link prediction tasks, IPR-MPNNs outperform all other candidates across three measurement metrics outlined in Tonshoff et al. [2023]. For QM9, we show in Table 1 that IPR-MPNNs greatly outperform similar methods, obtaining the best results on 12 of 13 target properties. On Zinc and OGB-Molhiv, we outperform similar MPNNs and graph transformers, namely GPS Rampasek et al. [2022] and SAT [Chen et al., 2022a], obtaining state-of-the-art results; see Table 4. For the TUDataset collection, we achieve the best results on four of the five molecular datasets; see Table A9.

Finally, to address **Q4**, we evaluate the computation time and memory usage of IPR-MPNNs in comparison with the GPS graph transformer [Rampasek et al., 2022] on Peptides-struct and extend our analysis to include PR-MPNNs [Qian et al., 2023], SAT [Chen et al., 2022a], and GPS on the Zinc dataset. The results in Tables 3 and A12 demonstrate that IPR-MPNNs adhere to their theoretical linear runtime complexity in practice. We observed a notable speedup in training and validation times per epoch while reducing the memory footprint by a large margin compared to the two mentioned transformers. This efficiency underscores the practical advantages of IPR-MPNNs in computational speed and resource utilization.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & GINE & IPR-MPNN & GPS & Drew \\ \hline \# Par. & \(503k\) & \(536k\) & \(558k\) & \(522k\) \\ Trn s/ep. & 2.68\(\pm\)0.01 & 2.98\(\pm\)0.02 & 7.81\(\pm\)0.32 & 3.20\(\pm\)0.03 \\ Val s/ep. & 0.21\(\pm\)0.00 & 0.27\(\pm\)0.00 & 0.58\(\pm\)0.04 & 0.36\(\pm\)0.00 \\ Mem. & 1.7GB & 1.9GB & 22.2GB & 1.8GB \\ \hline \hline \end{tabular}
\end{table}
Table 3: IPR-MPNN training, inference (seconds per epoch), and memory consumption statistics in comparison to the base GINE model [Xu et al., 2019], the GPS graph transformer [Rampasek et al., 2022] and the Drew model [Gutteridge et al., 2023] on the Peptides-struct dataset [Dwivedi et al., 2022b]. Our model has almost the same computation and memory efficiency as the base GINE model while being twice as fast and significantly more memory efficient when compared to GPS.

## 6 Conclusion

Here, we introduced implicit probabilistically rewired message-passing neural networks (IPRP-MPNNs), a graph-rewiring approach leveraging recent progress in end-to-end differentiable sampling. IPR-MPNNs show drastically improved running times and memory usage efficiency over graph transformers and competing rewiring-based architectures due to IPR-MPNNs' ability to circumvent comparing every pair of nodes and significantly outperforming them on real-world datasets while effectively addressing over-squashing and overreaching. Hence, IPR-MPNNs represent a significant step towards designing scalable, adaptable MPNNs, making them more reliable and expressive.

#### Acknowledgments

CQ and CM are partially funded by a DFG (German Research Foundation) Emmy Noether grant (468502433) and RWTH Junior Principal Investigator Fellowship under Germany's Excellence Strategy. AM and MN acknowledge DFG funding under Germany's Excellence Strategy--EXC 2075 - 390740016, the support of the Stuttgart Center for Simulation Science (SimTech), and the International Max Planck Research School for Intelligent Systems (IMPRS-IS).

\begin{table}
\begin{tabular}{l c c} \hline Model. & ZINC (12K), & OGB-Moldiv \(\uparrow\) \\ \hline GINE [2019, 2023] & 0.101\(\pm\)0.004 & 0.764\(\pm\)0.019 \\ PR-MPNN [2023] & 0.084\(\pm\)0.002 & 0.795\(\pm\)0.009 \\ GPS [2022] & 0.070\(\pm\)0.004 & 0.783\(\pm\)0.018 \\ K-SG GAT [20222] & 0.095\(\pm\)0.002 & 0.613\(\pm\)0.010 \\ K-ST GAT [20223] & 0.115\(\pm\)0.004 & 0.625\(\pm\)0.009 \\ Graph MLP-Mixer [2023] & 0.073\(\pm\)0.001 & 0.799\(\pm\)0.015 \\ Graph VIT [2023] & 0.085\(\pm\)0.005 & 0.779\(\pm\)0.015 \\ \hline \multicolumn{3}{l}{IPR-MPNN (ours)} & 0.067\(\pm\)0.001 & 0.788\(\pm\)0.006 \\ \hline \end{tabular}
\end{table}
Table 4: Results on the Zinc[Jin et al., 2017] and OGBG-Moldiv[Hu et al., 2020] datasets. Green is the best model, blue is the second, and red the third. The IPR-MPNN outperforms both SAT and GPS on Zinc, while obtaining the same performance as GPS on OGB-Moldiv.

Figure 3: We compute the log of total effective resistance [Black et al., 2023] of five molecular datasets before and after rewiring the graphs using virtual nodes. Our rewiring technique consistently lowers the total effective resistance, indicating a better information flow on all of the datasets.

## References

* Abboud et al. (2020) R. Abboud, I. I. Ceylan, M. Grohe, and T. Lukasiewicz. The surprising power of graph neural networks with random node initialization. _arXiv preprint_, 2020.
* Abboud et al. (2022) R. Abboud, R. Dimitrov, and I. I. Ceylan. Shortest path networks for graph property prediction. In _Learning on Graphs Conference_, 2022.
* Abu-El-Haija et al. (2019) S. Abu-El-Haija, B. Perozzi, A. Kapoor, N. Alipourfard, K. Lerman, H. Harutyunyan, G. Ver Steeg, and A. Galstyan. Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing. In _International Conference on Machine Learning_, 2019.
* Ahmed et al. (2023) K. Ahmed, Z. Zeng, M. Niepert, and G. Van den Broeck. Simple: A gradient estimator for k-subset sampling. In _International Conference on Learning Representations_, 2023.
* Alon and Yahav (2021) U. Alon and E. Yahav. On the bottleneck of graph neural networks and its practical implications. In _International Conference on Learning Representations_, 2021.
* Arnaiz-Rodriguez et al. (2022) A. Arnaiz-Rodriguez, A. Begga, F. Escolano, and N. Oliver. Diffwire: Inductive graph rewiring via the lovasz bound. _arXiv preprint_, 2022.
* Arvind et al. (2015) V. Arvind, J. Kobler, G. Rattan, and O. Verbitsky. On the power of color refinement. In _International Symposium on Fundamentals of Computation Theory_, 2015.
* Babai and Kucera (1979) L. Babai and L. Kucera. Canonical labelling of graphs in linear average time. In _Annual Symposium on Foundations of Computer Science (sfcs 1979)_, 1979.
* Babai et al. (1980) L. Babai, P. Erdos, and S. M. Selkow. Random graph isomorphism. _SIAM Journal on computing_, 9(3):628-635, 1980.
* Banerjee et al. (2022) P. K. Banerjee, K. Karhadkar, Y. G. Wang, U. Alon, and G. Montufar. Oversquashing in gnns through the lens of information contraction and graph expansion. In _Annual Allerton Conference on Communication, Control, and Computing (Allerton)_, 2022.
* Barbero et al. (2023) F. Barbero, A. Velingker, A. Saberi, M. Bronstein, and F. Di Giovanni. Locality-aware graph-rewiring in gnns. _arXiv preprint_, 2023.
* Barcelo et al. (2020) P. Barcelo, E. V. Kostylev, M. Monet, J. Perez, J. Reutter, and J. P. Silva. The logical expressiveness of graph neural networks. In _International Conference on Learning Representations_, 2020.
* Baskin et al. (1997) I. I. Baskin, V. A. Palyulin, and N. S. Zefirov. A neural device for searching direct correlations between structures and properties of chemical compounds. _Journal of Chemical Information and Computer Sciences_, 37(4):715-721, 1997.
* Battaglia et al. (2018) P. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-Gonzalez, V. Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, et al. Relational inductive biases, deep learning, and graph networks. _arXiv preprint_, 2018.
* Black et al. (2023) M. Black, Z. Wan, A. Nayyeri, and Y. Wang. Understanding oversquashing in gnns through the lens of effective resistance. In _International Conference on Machine Learning_, 2023.
* Bober et al. (2022) J. Bober, A. Monod, E. Saucan, and K. N. Webster. Rewiring networks for graph neural network training using discrete geometry. _arXiv preprint_, 2022.
* Bodnar et al. (2021) C. Bodnar, F. Frasca, Y. Wang, N. Otter, G. F. Montufar, P. Lio, and M. Bronstein. Weisfeiler and Lehman go topological: Message passing simplicial networks. In _International Conference on Machine Learning_, 2021.
* Bouritsas et al. (2022) G. Bouritsas, F. Frasca, S. Zafeiriou, and M. M. Bronstein. Improving graph neural network expressivity via subgraph isomorphism counting. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 45(1):657-668, 2022.
* Bruel-Gabrielsson et al. (2022) R. Bruel-Gabrielsson, M. Yurochkin, and J. Solomon. Rewiring with positional encodings for graph neural networks. _arXiv preprint_, 2022.
* Bruel-Gabrielsson et al. (2022)J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral networks and deep locally connected networks on graphs. In _International Conference on Learning Representation_, 2014.
* Boker et al. (2023) J. Boker, R. Levie, N. Huang, S. Villar, and C. Morris. Fine-grained expressivity of graph neural networks. In _NeurIPS_, 2023.
* Cai et al. (2023) C. Cai, T. S. Hy, R. Yu, and Y. Wang. On the connection between MPNN and graph transformer. _arXiv preprint_, 2023.
* Cai et al. (1992) J.-Y. Cai, M. Furer, and N. Immerman. An optimal lower bound on the number of variables for graph identification. _Combinatorica_, 12(4):389-410, 1992.
* Cappart et al. (2023) Q. Cappart, D. Chetelat, E. B. Khalil, A. Lodi, C. Morris, and P. Velickovic. Combinatorial optimization and reasoning with graph neural networks. _Journal of Machine Learning Research_, 24(130):1-61, 2023.
* Chen et al. (2022a) D. Chen, L. O'Bray, and K. Borgwardt. Structure-aware transformer for graph representation learning. In _International Conference on Machine Learning_, 2022a.
* Chen et al. (2022b) J. Chen, K. Gao, G. Li, and K. He. Nagphormer: A tokenized graph transformer for node classification in large graphs. _arXiv preprint_, 2022b.
* Chen et al. (2020) Y. Chen, L. Wu, and M. Zaki. Iterative deep graph learning for graph neural networks: Better and robust node embeddings. _Advances in Neural Information Processing Systems_, 2020.
* Craven et al. (1998) M. Craven, D. DiPasquo, D. Freitag, A. McCallum, T. Mitchell, K. Nigam, and S. Slattery. Learning to extract symbolic knowledge from the world wide web. _AAAI/IAAI_, 3(3.6):2, 1998.
* de Haan et al. (2020) P. de Haan, T. S. Cohen, and M. Welling. Natural graph networks. _Advances in Neural Information Processing Systems_, 2020.
* Deac et al. (2022) A. Deac, M. Lackenby, and P. Velickovic. Expander graph propagation. In _Learning on Graphs Conference_, 2022.
* Defferrard et al. (2016) M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In _Advances in Neural Information Processing Systems_, 2016.
* Di Giovanni et al. (2023) F. Di Giovanni, L. Giusti, F. Barbero, G. Luise, P. Lio, and M. M. Bronstein. On over-squashing in message passing neural networks: The impact of width, depth, and topology. In _International Conference on Machine Learning_, 2023.
* Duvenaud et al. (2015) D. Duvenaud, D. Maclaurin, J. Aguilera-Iparraguirre, R. Gomez-Bombarelli, T. Hirzel, A. Aspuru-Guzik, and R. P. Adams. Convolutional networks on graphs for learning molecular fingerprints. In _Advances in Neural Information Processing Systems_, 2015.
* Dwivedi and Bresson (2020) V. P. Dwivedi and X. Bresson. A generalization of transformer networks to graphs. _ArXiv preprint_, 2020.
* Dwivedi et al. (2022a) V. P. Dwivedi, A. T. Luu, T. Laurent, Y. Bengio, and X. Bresson. Graph neural networks with learnable structural and positional representations. In _International Conference on Learning Representations_, 2022a.
* Dwivedi et al. (2022b) V. P. Dwivedi, L. Rampasek, M. Galkin, A. Parviz, G. Wolf, A. T. Luu, and D. Beaini. Long range graph benchmark. _Advances in Neural Information Processing Systems_, 2022b.
* Easley et al. (2012) D. Easley, J. Kleinberg, et al. Networks, crowds, and markets. _Cambridge Books_, 2012.
* Errica et al. (2023) F. Errica, H. Christiansen, V. Zaverkin, T. Maruyama, M. Niepert, and F. Alesiani. Adaptive message passing: A general framework to mitigate oversmoothing, oversquashing, and underreaching. _arXiv preprint_, 2023.
* Fatemi et al. (2021) B. Fatemi, L. El Asri, and S. M. Kazemi. Slaps: Self-supervision improves structure learning for graph neural networks. _Advances in Neural Information Processing Systems_, 2021.
* Fader et al. (2016)B. Fatemi, S. Abu-El-Haija, A. Tsitsulin, M. Kazemi, D. Zelle, N. Bulut, J. Halcrow, and B. Perozzi. Ugsl: A unified framework for benchmarking graph structure learning. _arXiv preprint_, 2023.
* Fey et al. (2020) M. Fey, J.-G. Yuen, and F. Weichert. Hierarchical inter-message passing for learning on molecular graphs. _arXiv preprint_, 2020.
* Franceschi et al. (2019) L. Franceschi, M. Niepert, M. Pontil, and X. He. Learning discrete structures for graph neural networks. In _International Conference on Machine Learning_, 2019.
* Frasca et al. (2020) F. Frasca, E. Rossi, D. Eynard, B. Chamberlain, M. Bronstein, and F. Monti. Sign: Scalable inception graph neural networks. _arXiv preprint_, 2020.
* Gama et al. (2019) F. Gama, A. G. Marques, G. Leus, and A. Ribeiro. Convolutional neural network architectures for signals supported on graphs. _IEEE Transactions on Signal Processing_, 67(4):1034-1049, 2019.
* Gao and Ji (2019) H. Gao and S. Ji. Graph U-Nets. In _International Conference on Machine Learning_, 2019.
* Gasteiger et al. (2019) J. Gasteiger, S. Weissenberger, and S. Gunnemann. Diffusion improves graph learning. _Advances in Neural Information Processing Systems_, 32, 2019.
* Geisler et al. (2024) S. Geisler, A. Kosmala, D. Herbst, and S. Gunnemann. Spatio-spectral graph neural networks. _arXiv preprint_, 2024.
* Gilmer et al. (2017) J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural message passing for quantum chemistry. In _International Conference on Machine Learning_, 2017.
* Giusti et al. (2023a) L. Giusti, C. Battiloro, L. Testa, P. Di Lorenzo, S. Sardellitti, and S. Barbarossa. Cell attention networks. In _International Joint Conference on Neural Networks_, 2023a.
* Giusti et al. (2023b) L. Giusti, T. Reu, F. Ceccarelli, C. Bodnar, and P. Lio. Cin++: Enhancing topological message passing. _arXiv preprint_, 2023b.
* Goller and Kuchler (1996) C. Goller and A. Kuchler. Learning task-dependent distributed representations by backpropagation through structure. In _International Conference on Neural Networks_, 1996.
* Grohe (2017) M. Grohe. _Descriptive complexity, canonisation, and definable graph structure theory_. Cambridge University Press, 2017.
* Grohe (2021) M. Grohe. The logic of graph neural networks. In _Symposium on Logic in Computer Science_, 2021.
* Gromiha and Selvaraj (1999) M. M. Gromiha and S. Selvaraj. Importance of long-range interactions in protein folding. _Biophysical Chemistry_, 77(1):49-68, 1999.
* Gutteridge et al. (2023) B. Gutteridge, X. Dong, M. M. Bronstein, and F. Di Giovanni. Drew: dynamically rewired message passing with delay. In _International Conference on Machine Learning_, 2023.
* Hamilton et al. (2017) W. L. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs. In _Advances in Neural Information Processing Systems_, 2017.
* He et al. (2023) X. He, B. Hooi, T. Laurent, A. Perold, Y. LeCun, and X. Bresson. A generalization of vit/mlp-mixer to graphs. In _International Conference on Machine Learning_, 2023.
* Hu et al. (2019) F. Hu, Y. Zhu, S. Wu, L. Wang, and T. Tan. Hierarchical graph convolutional networks for semi-supervised node classification. _arXiv preprint_, 2019.
* Hu et al. (2020) W. Hu, M. Fey, M. Zitnik, Y. Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec. Open graph benchmark: Datasets for machine learning on graphs. _Advances in Neural Information Processing Systems_, 2020.
* Huang et al. (2019) J. Huang, Z. Li, N. Li, S. Liu, and G. Li. AttPool: towards hierarchical feature representation in graph convolutional networks via attention mechanism. In _IEEE/CVF International Conference on Computer Vision_, 2019.
* Huang et al. (2021) Z. Huang, S. Zhang, C. Xi, T. Liu, and M. Zhou. Scaling up graph neural networks via graph coarsening. In _SIGKDD Conference on Knowledge Discovery & Data Mining_, 2021.
* Huang et al. (2019)B. E. Husic, N. E. Charron, D. Lemm, J. Wang, A. Perez, M. Majewski, A. Kramer, Y. Chen, S. Olsson, G. de Fabritiis, et al. Coarse graining molecular dynamics with graph neural networks. _The Journal of Chemical Physics_, 153(19), 2020.
* Hussain et al. (2022) M. S. Hussain, M. J. Zaki, and D. Subramanian. Global self-attention as a replacement for graph convolution. In _SIGKDD Conference on Knowledge Discovery and Data Mining_, 2022.
* Ishiguro et al. (2019) K. Ishiguro, S.-i. Maeda, and M. Koyama. Graph warp module: an auxiliary module for boosting the power of graph neural networks in molecular graph analysis. _arXiv preprint_, 2019.
* Jin et al. (2017) W. Jin, C. Coley, R. Barzilay, and T. Jaakkola. Predicting organic reaction outcomes with Weisfeiler-Lehman network. _Advances in Neural Information Processing Systems_, 2017.
* Jin et al. (2020) W. Jin, Y. Ma, X. Liu, X. Tang, S. Wang, and J. Tang. Graph structure learning for robust graph neural networks. _arXiv preprint_, 2020.
* Jumper et al. (2021) J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. Zidek, A. Potapenko, et al. Highly accurate protein structure prediction with alphafold. _Nature_, 596(7873):583-589, 2021.
* Karhadkar et al. (2022) K. Karhadkar, P. K. Banerjee, and G. Montufar. FoSR: First-order spectral rewiring for addressing oversquashing in gnns. _arXiv preprint_, 2022.
* Kazi et al. (2022) A. Kazi, L. Cosmo, S.-A. Ahmadi, N. Navab, and M. M. Bronstein. Differentiable graph module (dgm) for graph convolutional networks. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 45(2):1606-1617, 2022.
* Kiefer and McKay (2020) S. Kiefer and B. D. McKay. The iteration number of Colour Refinement. In _International Colloquium on Automata, Languages, and Programming_, pages 73:1-73:19, 2020.
* Kim et al. (2022) J. Kim, T. D. Nguyen, S. Min, S. Cho, M. Lee, H. Lee, and S. Hong. Pure transformers are powerful graph learners. _arXiv preprint_, 2022.
* Kingma and Ba (2015) D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In _International Conference on Learning Representations_, 2015.
* Kipf and Welling (2017) T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. In _International Conference on Learning Representations_, 2017.
* Kireev (1995) D. B. Kireev. Chemnet: A novel neural network based method for graph/property mapping. _Journal of Chemical Information and Computer Sciences_, 35(2):175-180, 1995.
* Levie et al. (2019) R. Levie, F. Monti, X. Bresson, and M. M. Bronstein. CayleyNets: Graph convolutional neural networks with complex rational spectral filters. _IEEE Transactions on Signal Processing_, 67(1):97-109, 2019.
* Li et al. (2017) J. Li, D. Cai, and X. He. Learning graph-level representation for drug discovery. _arXiv preprint_, 2017.
* Li et al. (2020) M. Li, S. Chen, Y. Zhang, and I. Tsang. Graph cross networks with vertex infomax pooling. _Advances in Neural Information Processing Systems_, 2020.
* Li et al. (2023) X. Li, Z. Zhou, J. Yao, Y. Rong, L. Zhang, and B. Han. Long-range neural atom learning for molecular graphs. _arXiv preprint_, 2023.
* Liang et al. (2021) J. Liang, S. Gurukar, and S. Parthasarathy. Mile: A multi-level framework for scalable graph embedding. In _AAAI Conference on Web and Social Media_, 2021.
* Liu et al. (2021) M. Liu, Z. Wang, and S. Ji. Non-local graph neural networks. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 44(12):10270-10276, 2021.
* Liu et al. (2022) N. Liu, X. Wang, L. Wu, Y. Chen, X. Guo, and C. Shi. Compact graph structure learning via mutual information compression. In _ACM Web Conference 2022_, 2022a.
* Liu et al. (2020)

[MISSING_PAGE_FAIL:15]

P. A. Papp, K. Martinkus, L. Faber, and R. Wattenhofer. DropGNN: Random dropouts increase the expressiveness of graph neural networks. _Advances in Neural Information Processing Systems_, 2021.
* Park et al. (2023) S. Park, N. Ryu, G. Kim, D. Woo, S.-Y. Yun, and S. Ahn. Non-backtracking graph neural networks. _arXiv preprint_, 2023.
* Pei et al. (2020) H. Pei, B. Wei, K. C.-C. Chang, Y. Lei, and B. Yang. Geom-gcn: Geometric graph convolutional networks. In _International Conference on Learning Representations_, 2020.
* Pham et al. (2017) T. Pham, T. Tran, H. Dam, and S. Venkatesh. Graph classification via deep learning with virtual nodes. _arXiv preprint_, 2017.
* Platonov et al. (2023) O. Platonov, D. Kuznedelev, M. Diskin, A. Babenko, and L. Prokhorenkova. A critical look at the evaluation of gnns under heterophily: Are we really making progress? _arXiv preprint_, 2023.
* Qian et al. (2023) C. Qian, A. Manolache, K. Ahmed, Z. Zeng, G. V. den Broeck, M. Niepert, and C. Morris. Probabilistically rewired message-passing neural networks, 2023.
* Qian et al. (2024) C. Qian, D. Chetelat, and C. Morris. Exploring the power of graph neural networks in solving linear optimization problems. In _AISTATS_, 2024.
* Rampasek and Wolf (2021) L. Rampasek and G. Wolf. Hierarchical graph neural nets can capture long-range interactions. In _IEEE International Workshop on Machine Learning for Signal Processing_, 2021.
* Rampasek et al. (2022) L. Rampasek, M. Galkin, V. P. Dwivedi, A. T. Luu, G. Wolf, and D. Beaini. Recipe for a general, powerful, scalable graph transformer. _Advances in Neural Information Processing Systems_, 2022.
* Ranjan et al. (2020) E. Ranjan, S. Sanyal, and P. Talukdar. Asap: Adaptive structure aware pooling for learning hierarchical graph representations. In _AAAI Conference on Artificial Intelligence_, 2020.
* Rosenbluth et al. (2024) E. Rosenbluth, J. Tonshoff, M. Ritzert, B. Kisin, and M. Grohe. Distinguished in uniform: Self-attention vs. virtual nodes. In _International Conference on Learning Representations_, 2024.
* Saha et al. (2023) A. Saha, O. Mendez, C. Russell, and R. Bowden. Learning adaptive neighborhoods for graph neural networks. _arXiv preprint_, 2023.
* Saunders and Voth (2013) M. G. Saunders and G. A. Voth. Coarse-graining methods for computational biology. _Annual Review of Biophysics_, 42:73-93, 2013.
* Scarselli et al. (2008) F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural network model. _IEEE Transactions on Neural Networks_, 20(1):61-80, 2008.
* Schlichtkrull et al. (2018) M. Schlichtkrull, T. N. Kipf, P. Bloem, R. van den Berg, I. Titov, and M. Welling. Modeling relational data with graph convolutional networks. In _The Semantic Web_, pages 593-607, 2018.
* Shervashidze et al. (2009) N. Shervashidze, S. Vishwanathan, T. Petri, K. Mehlhorn, and K. Borgwardt. Efficient graphlet kernels for large graph comparison. In _AISTATS_, 2009.
* Shervashidze et al. (2011) N. Shervashidze, P. Schweitzer, E. J. Van Leeuwen, K. Mehlhorn, and K. M. Borgwardt. Weisfeiler-lehman graph kernels. _Journal of Machine Learning Research_, 12(9), 2011.
* Shirzad et al. (2023) H. Shirzad, A. Velingker, B. Venkatachalam, D. J. Sutherland, and A. K. Sinop. Exphormer: Sparse transformers for graphs. _arXiv preprint_, 2023.
* Southern et al. (2024) J. Southern, F. Di Giovanni, M. Bronstein, and J. F. Lutzeyer. Understanding virtual nodes: Oversmoothing, oversquashing, and node heterogeneity. _arXiv preprint_, 2024.
* Sperduti and Starita (1997) A. Sperduti and A. Starita. Supervised neural networks for the classification of structures. _IEEE Transactions on Neural Networks_, 8(3):714-35, 1997.
* Snoek et al. (2018)J. Topping, F. Di Giovanni, B. P. Chamberlain, X. Dong, and M. M. Bronstein. Understanding over-squashing and bottlenecks on graphs via curvature. _arXiv preprint_, 2021.
* Tonshoff et al. [2023] J. Tonshoff, M. Ritzert, E. Rosenbluth, and M. Grohe. Where did the gap go? reassessing the long-range graph benchmark. _arXiv preprint_, 2023.
* Velickovic et al. [2018] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio. Graph attention networks. In _International Conference on Learning Representations_, 2018.
* Weisfeiler and Leman [1968] B. Weisfeiler and A. Leman. The reduction of a graph to canonical form and the algebra which appears therein. _nti, Series_, 2(9):12-16, 1968.
* Wong et al. [2023] F. Wong, E. J. Zheng, J. A. Valeri, N. M. Donghia, M. N. Anahtar, S. Omori, A. Li, A. Cubillos-Ruiz, A. Krishnan, W. Jin, A. L. Manson, J. Friedrichs, R. Helbig, B. Hajian, D. K. Fiejtek, F. F. Wagner, H. H. Soutter, A. M. Earl, J. M. Stokes, L. D. Renner, and J. J. Collins. Discovery of a structural class of antibiotics with explainable deep learning. _Nature_, 2023.
* Wu et al. [2021] Z. Wu, P. Jain, M. Wright, A. Mirhoseini, J. E. Gonzalez, and I. Stoica. Representing long-range context for graph neural networks with global attention. _Advances in Neural Information Processing Systems_, 2021.
* Xie and Ermon [2019] S. M. Xie and S. Ermon. Reparameterizable subset sampling via continuous relaxations. _International Joint Conference on Artificial Intelligence_, 2019.
* Xu et al. [2018] K. Xu, C. Li, Y. Tian, T. Sonobe, K.-i. Kawarabayashi, and S. Jegelka. Representation learning on graphs with jumping knowledge networks. In _International Conference on Machine Learning_, 2018.
* Xu et al. [2019] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? In _International Conference on Learning Representations_, 2019.
* Ying et al. [2021] C. Ying, T. Cai, S. Luo, S. Zheng, G. Ke, D. He, Y. Shen, and T.-Y. Liu. Do transformers really perform badly for graph representation? _Advances in neural information processing systems_, 34:28877-28888, 2021.
* Ying et al. [2018] Z. Ying, J. You, C. Morris, X. Ren, W. Hamilton, and J. Leskovec. Hierarchical graph representation learning with differentiable pooling. _Advances in Neural Information Processing Systems_, 2018.
* Younesian et al. [2023] T. Younesian, T. Thanapalasingam, E. van Krieken, D. Daza, and P. Bloem. GRAPES: Learning to sample graphs for scalable graph neural networks. _arXiv preprint_, 2023.
* Yu et al. [2021] D. Yu, R. Zhang, Z. Jiang, Y. Wu, and Y. Yang. Graph-revised convolutional network. In _European Conference on Machine Learning and Knowledge Discovery in Databases_, 2021.
* Zhang et al. [2018] M. Zhang, Z. Cui, M. Neumann, and Y. Chen. An end-to-end deep learning architecture for graph classification. In _AAAI Conference on Artificial Intelligence_, 2018.
* Zhao et al. [2021] T. Zhao, Y. Liu, L. Neves, O. Woodford, M. Jiang, and N. Shah. Data augmentation for graph neural networks. In _AAAI Conference on Artificial Intelligence_, 2021.
* Zhong et al. [2023] Z. Zhong, C.-T. Li, and J. Pang. Hierarchical message-passing graph neural networks. _Data Mining and Knowledge Discovery_, 37(1):381-408, 2023.
* Zhou et al. [2023a] Z. Zhou, S. Zhou, B. Mao, X. Zhou, J. Chen, Q. Tan, D. Zha, Y. Feng, C. Chen, and C. Wang. OpenGSL: A comprehensive benchmark for graph structure learning, 2023a.
* Zhou et al. [2023b] Z. Zhou, S. Zhou, B. Mao, X. Zhou, J. Chen, Q. Tan, D. Zha, C. Wang, Y. Feng, and C. Chen. OpenGSL: A comprehensive benchmark for graph structure learning. _arXiv preprint_, 2023b.
* Zou et al. [2023c] D. Zou, H. Peng, X. Huang, R. Yang, J. Li, J. Wu, C. Liu, and P. S. Yu. SE-GSL: A general and effective graph structure learning framework through structural entropy optimization. _arXiv preprint_, 2023.
* Zhou et al. [2023d]Additional Related Work

In the following, we discuss related work.

Graph Structure LearningGraph structure learning (GSL) is closely related to graph rewiring, where the primary motivation is refining and optimizing the graph structure while jointly learning graph representations (Zhou et al., 2023). Several methods have been proposed in this field. Jin et al. (2020) develop a technique to optimize graph structures from scratch using a specific loss function as a bias, while the general approach is using edge scoring functions for refinment (Chen et al., 2020; Yu et al., 2021; Zhao et al., 2021), but discrete sampling methods have also been applied. More specifically, DGM (Kazi et al., 2022) is predicting the latent graph structure by leveraging Gumbel discrete sampling, while Franceschi et al. (2019) is learning a Bernoulli distribution via Hypergradient Descent. Saha et al. (2023) learns adaptive neighborhoods for trajectory prediction and point cloud classification by sampling through the smoothed-Heaviside function, while Younesian et al. (2023) samples nodes that are used for downstream tasks using GFlowNets. Some GSL techniques also employ unsupervised learning (Zou et al., 2023; Fatemi et al., 2021; Liu et al., 2022, 2022). We encourage the reader to refer to Fatemi et al. (2023); Zhou et al. (2023) for detailed surveys regarding GSL.

Our proposed IPR-MPNN is different from other GSL framework in two main ways: (1) instead of sparsifying the graph using randomized \(k\)-NN approaches or independent Bernoulli random variables, we learn a probability mass function with exactly-\(k\) constraints (Ahmed et al., 2023). Moreover, we don't aim to discover the graph structure by considering a fully-connected latent graph from which we sample new edges (Qian et al., 2023), instead we introduce sparse connections from base nodes to virtual nodes, with complexity \(N\cdot k\); (2) GSL methods do not investigate exact sampling of the exactly-\(k\) distribution; however, one of our aims is to demonstrate that these techniques can significantly alleviate information propagation issues caused by inadequate graph connectivity, such as over-squashing and under-reaching.

Moreover, our IPR-MPNN also differs from previous graph rewiring method, specifically PR-MPNN (Qian et al., 2023). First, we rewire the graph implicitly by connecting nodes to virtual nodes, respecting the original graph structure. Besides, we show a significant run-time advantage in that our worst-case complexity is sub-quadratic, while PR-MPNN optimally needs to consider \(n^{2}\) node pairs for an \(n\)-order graph.

Hierarchical MPNNsOur method draws connections to hierarchical MPNNs. The hierarchical model initially emerged in graph-level representation learning, as seen in approaches like AttPool (Huang et al., 2019), DiffPool (Ying et al., 2018), and ASAP (Ranjan et al., 2020). Further developments, such as Graph U-Net (Gao and Ji, 2019), H-GCN (Hu et al., 2019), and GXN (Li et al., 2020), introduced top-down and bottom-up methods within their architectures. However, they did not incorporate virtual node message passing. Other works create hierarchical MPNNs while incorporating inter-hierarchical message passing. For example, Fey et al. (2020) introduced HIMP-GNN on molecular learning, using a junction tree to create a higher hierarchy of the original graph and do inter-message passing between the hierarchies. Rampasek and Wolf (2021) proposed HGNet for long-range dependencies, generating hierarchies with edge pooling and training with relational GCN. Zhong et al. (2023) designed HC-GNN, more efficient than HGNet, for better node and higher level resolution community representations. These hierarchical MPNNs require well-chosen heuristics for hierarchy generation. Using an auxiliary supernode is particularly prominent in molecular tasks (Gilmer et al., 2017; Pham et al., 2017; Li et al., 2017), which involves adding a global node to encapsulate graph-level representations. Further advancements in this area, as suggested in (Battaglia et al., 2018), and developments like GWM Ishiguro et al. (2019), have enhanced supernode MPNNs with a gating mechanism. Theoretically, Cai et al. (2023) proves MPNN with a virtual node can simulate self-attention, which is further investigated in Rosenbluth et al. (2024). In addition, Southern et al. (2024) studied the effect of MPNNs using a virtual node on over smoothing and oversquashing. Simultaneously, the recent study by Li et al. (2023) has introduced the concept of a collection of supernodes, termed "neural atoms," which incorporate supernode message passing with an attention mechanism. Moreover, the idea of a coarsened hierarchical graph has become widely employed in scalable MPNN training and graph representation learning, as evidenced by works like Huang et al. (2021); Liang et al. (2021); Namazi et al. (2022). _Unlike existing hierarchical MPNNs, our IPR-MPNN uniquely leverages differential \(k\)-subset sampling techniques for dynamic, probabilistic graph rewiring and incorporates hierarchical message passing in an end-to-end trainable framework. Thisapproach enhances graph connectivity and expressiveness without relying on predefined heuristics or fixed structures._

## Appendix B Extended Notation

A _graph_\(G\) is a pair \((V(G),E(G))\) with _finite_ sets of _vertices_ or _nodes_\(V(G)\) and _edges_\(E(G)\subseteq\{\{u,v\}\subseteq V(G)\mid u\neq v\}\). If not otherwise stated, we set \(n\coloneqq|V(G)|\), and the graph is of _order_\(n\). We also call the graph \(G\) an \(n\)-order graph. For ease of notation, we denote the edge \(\{u,v\}\) in \(E(G)\) by \((u,v)\) or \((v,u)\). A _(vertex-labeled graph_\(G\) is a triple \((V(G),E(G),\ell)\) with a (vertex-)label function \(\ell\colon V(G)\to\mathbb{N}\). Then \(\ell(v)\) is a _label_ of \(v\), for \(v\) in \(V(G)\). An _attributed graph_\(G\) is a triple \((V(G),E(G),\sigma)\) with a graph \((V(G),E(G))\) and (vertex-)attribute function \(\sigma\colon V(G)\to\mathbb{R}^{1\times d}\), for some \(d>0\). That is, contrary to labeled graphs, we allow for vertex annotations from an uncountable set. Then \(\sigma(v)\) is an _attribute_ or _feature_ of \(v\), for \(v\) in \(V(G)\). Equivalently, we define an \(n\)-order attributed graph \(G\coloneqq(V(G),E(G),\sigma)\) as a pair \(\bm{G}=(G,\bm{L})\), where \(G=(V(G),E(G))\) and \(\bm{L}\) in \(\mathbb{R}^{n\times d}\) is a _node feature matrix_. Here, we identify \(V(G)\) with \([n]\). For a matrix \(\bm{L}\) in \(\mathbb{R}^{n\times d}\) and \(v\) in \([n]\), we denote by \(\bm{L}_{v}\). In \(\mathbb{R}^{1\times d}\) the \(v\)th row of \(\bm{L}\) such that \(\bm{L}_{v}\coloneqq\sigma(v)\). Furthermore, we can encode an \(n\)-order graph \(G\) via an _adjacency matrix_\(\bm{A}(G)\in\{0,1\}^{n\times n}\), where \(A_{ij}=1\) if, and only, if \((i,j)\in E(G)\). We also write \(\mathbb{R}^{d}\) for \(\mathbb{R}^{1\times d}\).

The _neighborhood_ of \(v\) in \(V(G)\) is denoted by \(N(v)\coloneqq\{u\in V(G)\mid(v,u)\in E(G)\}\) and the _degree_ of a vertex \(v\) is \(|N(v)|\). Two graphs \(G\) and \(H\) are _isomorphic_ and we write \(G\simeq H\) if there exists a bijection \(\varphi\colon V(G)\to V(H)\) preserving the adjacency relation, i.e., \((u,v)\) is in \(E(G)\) if and only if \((\varphi(u),\varphi(v))\) is in \(E(H)\). Then \(\varphi\) is an _isomorphism_ between \(G\) and \(H\). In the case of labeled graphs, we additionally require that \(l(v)=l(\varphi(v))\) for \(v\) in \(V(G)\), and similarly for attributed graphs.

A _node coloring_ is a function \(c\colon V(G)\to\mathbb{R}^{d}\), \(d>0\), and we say that \(c(v)\) is the _color_ of \(v\in V(G)\). A node coloring induces an _edge coloring_\(e_{c}\colon E(G)\to\mathbb{N}\), where \((u,v)\mapsto\{c(u),c(v)\}\) for \((u,v)\in E(G)\). A node coloring (edge coloring) \(c\)_refines_ a node coloring (edge coloring) \(d\), written \(c\sqsubseteq d\) if \(c(v)=c(w)\) implies \(d(v)=d(w)\) for every \(v,w\in V(G)\) (\(v,w\in E(G)\)). Two colorings are equivalent if \(c\sqsubseteq d\) and \(d\sqsubseteq c\), in which case we write \(c\equiv d\). A _color class_\(Q\subseteq V(G)\) of a node coloring \(c\) is a maximal set of nodes with \(c(v)=c(w)\) for every \(v,w\in Q\). A node coloring is called _discrete_ if all color classes have cardinality \(1\).

## Appendix C The \(1\)-dimensional Weisfeiler-Leman algorithm

The \(1\)-WL or _color refinement_ is a fundamental, well-studied heuristic for the graph isomorphism problem, originally proposed by Weisfeiler and Leman (1968).1 The algorithm is an iterative method starting from labeling or coloring vertices in both graphs with degrees or other information, and updating the color of a node with its color as well as its neighbors' colors. During the iterations, two vertices with the same label get different labels if the number of identically labeled neighbors is unequal. Each iteration ends up with a vertex color partition, and the algorithm terminates when the partition is not refined by the algorithm, i.e., when a _stable coloring_ or _stable partition_ is obtained. We can finally conclude that the two graphs are not isomorphic if the color partitions are different, or the number of nodes of a specific color is different. Although in Cai et al. (1992) the limitation is shown that \(1\)-WL algorithm cannot distinguish all non-isomorphic graphs, it is a powerful heuristic that can succeed on a broad class of graphs (Arvind et al., 2015; Babai and Kucera, 1979; Babai et al., 1980).

Footnote 1: Strictly speaking, the \(1\)-WL and color refinement are two different algorithms. That is, the \(1\)-WL considers neighbors and non-neighbors to update the coloring, resulting in a slightly higher expressive power when distinguishing vertices in a given graph; see Grohe (2021) for details. Following customs in the machine learning literature, we consider both algorithms to be equivalent.

Formally, let \(G=(V(G),E(G),\ell)\) be a labeled graph. In each iteration, \(t>0\), the \(1\)-WL computes a vertex coloring \(C_{t}^{1}\colon V(G)\to\mathbb{N}\), depending on the coloring of the ego node and of the neighbors. That is, in iteration \(t>0\), we set

\[C_{t}^{1}(v)\coloneqq\mathsf{RELABEL}\Big{(}\!\big{(}C_{t-1}^{1}(v),\{\!\{C_{t -1}^{1}(u)\mid u\in N(v)\}\!\}\big{)}\!\Big{)},\]

[MISSING_PAGE_FAIL:20]

color class, each base node connects to one of the virtual nodes uniformly at random. The scenario where two graphs remain isomorphic is when they connect to exactly the same virtual nodes or either connect as in case (a) or (b). Therefore, we have a high probability of separating these isomorphic graphs.

For example (2), we again produce uniform priors for each color class of the graphs in Figure A6. Once again, there is a high probability of separation. One possible configuration that can separate between the two graphs is shown in Figure A6 where in the first graph (a), the nodes colored with \(1\) get assigned to virtual node \(4\), while the nodes colored with \(2\) are assigned to virtual node \(3\). In the second graph (b), all nodes get assigned to the same virtual node \(3\).

For example (3) in Figure A7, we consider producing priors that assign, with high probability, the same virtual node to all of the nodes that are in color classes of cardinality \(1\). This approach ensures that the discretely-colored graphs remain isomorphic with high probability.

The intuition is that if we want to distinguish between graphs that have the same \(1\)-\(\mathsf{WL}\) stable color partitioning, the upstream model needs to produce "uninformative" prior weights for some color classes. However, preserving isomorphisms is most likely when the nodes in the same color class in the two graphs get assigned to the same virtual nodes. Since our upstream model is as powerful as \(1\)-\(\mathsf{WL}\), we can control the prior distribution for the color classes but not for individual nodes, therefore we can only guarantee high assignment probabilities for nodes in color classes of cardinality \(1\).

Next, we formally argue that we can preserve, with arbitrarily high probability, isomorphisms between graphs that have the same discrete stable \(1\)-\(\mathsf{WL}\) color partitions, as well as isomorphisms between subgraphs with the same discrete stable \(1\)-\(\mathsf{WL}\) color partitions.

**Lemma E.1**.: _Let \(G\) and \(H\) be a pair of graphs with the same \(1\)-\(\mathsf{WL}\) graph coloring, i.e., they are \(1\)-\(\mathsf{WL}\) non-distinguishable. Let \(k\in\mathbb{N}\), let \(G^{\prime}_{k}\) be color-induced subgraph where \(V(G^{\prime}_{k})\coloneqq\{v\in V(G)\mid c^{1}_{\infty}(v)=k\}\subseteq V(G)\), and \(V(H^{\prime})\) similarly. Then the subgraphs induced by \(V(G^{\prime}_{k})\) and \(V(H^{\prime}_{k})\) are still not \(1\)-\(\mathsf{WL}\)-distinguishable._

Proof.: To prove the lemma, we use the concept of an unrolling tree for a node; see, e.g., Morris et al. (2020b). That is, for a node, we recursively unroll its neighbors, resulting in a tree. It is easy to see that two nodes get the same colors under \(1\)-\(\mathsf{WL}\) if and only if such trees are isomorphic; see Morris et al. (2020b) for details.

Consider nodes \(v,u\in V(G_{k}^{\prime})\cup V(H_{k}^{\prime})\) that share the same stable \(1\)-\(\mathsf{WL}\) coloring \(k\). Based on the above, \(v\) and \(u\) must have isomorphic unrolling trees. Now remove subgraphs of the two trees not rooted at the vertex with color \(k\). Since both \(v\) and \(u\) have isomorphic unrolling trees, the resulting trees are isomorphic. Hence, running \(1\)-\(\mathsf{WL}\) on top of \(G_{k}^{\prime}\) and \(H_{k}^{\prime}\) will still not distinguish them. 

**Lemma E.2**.: _Let \(G\) and \(H\) be a pair of graphs with the same stable coloring under \(1\)-\(\mathsf{WL}\). If we add a finite number of virtual nodes on both graphs \(C(G),C(H)\), and connect these virtual nodes based on \(1\)-\(\mathsf{WL}\) colors of the original graphs, i.e., two equally colored vertices get assigned the same virtual nodes. Then, the augmented graphs \(\hat{G}\) and \(\hat{H}\) have the same stable partition._

Proof.: The proof is by straightforward induction on the number of iterations using the fact that two nodes with the same color will be assigned to the same virtual node. That is, the neighborhood of two such nodes is extended by the same nodes.

Besides, we leverage the following result by Qian et al. (2023); Morris et al. (2019).

**Lemma E.3** (Qian et al. (2023); Morris et al. (2019)).: _Let \(G\) be an \(n\)-order graph and let \(c\colon V(G)\to\mathbb{R}^{d}\), \(d>0\), be a \(1\)-\(\mathsf{WL}\)-equivalent node coloring. Then, for all \(\varepsilon>0\), there exists a (permutation-equivariant) MPNN \(f\colon V(G)\to\mathbb{R}^{d}\), such that_

\[\max_{v\in V(G)}\lVert f(v)-c(v)\rVert<\varepsilon.\]

**Theorem E.4**.: _Let \(k>0\), \(\varepsilon\in(0,1)\), and \(G\), \(H\) be two graphs with identical \(1\)-\(\mathsf{WL}\) stable colorings. Let \(M\) be the set of ordered virtual nodes, \(V_{G}\) and \(V_{H}\) be the subset of nodes in \(G\) and \(H\) that have a color class of cardinality \(1\), with \(|V_{G}|=|V_{H}|=d\), and \(W_{G}\), \(W_{H}\) the subset of nodes that have a color class of cardinality greater than \(1\), with \(|W_{G}|=|W_{H}|=n\). Then, for all choices of \(1\)-\(\mathsf{WL}\)-equivalent functions \(f\),_1. _there exists a conditional probability mass function_ \(p_{(\bm{\theta},k)}\) _that does_ not _separate_ \(G[V_{G}]\) _and_ \(H[V_{H}]\) _with probability at least_ \(1-\varepsilon\)_._
2. _There exists a conditional probability mass function_ \(p_{(\bm{\theta},k)}\) _that separates_ \(G[W_{G}]\) _and_ \(H[W_{H}]\) _with probability strictly greater than_ \(0\)_._

Proof.: Let \(M=\{v_{1},...,v_{m}\}\) be the set of \(m\) ordered virtual nodes and \(d=|V_{G}|=|V_{H}|\). To prove this theorem, we can leverage Lemma E.3 to assign distinct and arbitrary priors for every color class.

For \((1)\), we know that since \(G\) and \(H\) have identical \(1\)-\(\mathsf{WL}\) stable colorings and \(V_{G}\), \(V_{H}\) have a color class of cardinality 1, then \(G[V_{G}]\) and \(H[V_{H}]\) must be discrete and isomorphic. Using Lemma E.3, we can obtain an upstream MPNN that assigns a sufficiently high prior \(\theta_{i}\) such that, when we sample from the exactly-\(k\) distribution, we can assign the corresponding \(k\) virtual nodes to a base node with probability at least \(\sqrt[2d]{1-\varepsilon}\).

To demonstrate the existence of such a set of priors \(\bm{\theta}\), let \(\delta\in(0,1)\) and \(S\subset M\) be a subset of \(k\) virtual nodes. Let \(w_{1}>w_{2}\) be two prior weights such that \(\theta_{i}=w_{1}\) if \(v_{i}\in S\), and \(\theta_{i}=w_{2}\) if \(v_{i}\in M\setminus S\). We have that

\[p_{\theta,k}(S)\geq\delta\left(\sum_{i=0}^{k}\binom{k}{i}\binom{m-k}{k-i}w_{1} ^{i}w_{2}^{k-i}\right)=\delta Z,\]

with the upper bound [3]

\[Z\leq w_{1}^{k}+\left(\binom{m}{k}-1\right)w_{2}w_{1}^{k-1},\]

Thus, \(\bm{\theta}\) exists and can be obtained using this inequality.

Next, we set \(\delta=\sqrt[2d]{1-\varepsilon}\). Consequently, the probability that the sampled virtual nodes are identical for both graphs is at least \(\sqrt[2d]{1-\varepsilon}^{2d}=1-\varepsilon\). Finally, using Lemma E.2, we know that the two graphs also retain their color partitions and remain isomorphic with probability at least \(1-\varepsilon\).

For \((2)\), it is easy to see that, for any prior weights \(\bm{\theta}\) that we assign to the color classes of cardinality greater than \(1\), there is at least one configuration separating the two graphs. For instance, since \(k<m\), we can separate \(G[W_{G}],H[W_{H}]\) by assigning the first \(k\) virtual nodes \(\{1,...,k\}\) to every node in \(G[W_{G}]\), but have at least one node in \(H[W_{H}]\) be assigned to the next \(k\) nodes \(\{2,...,k+1\}\). More concretely, let \(\varepsilon\in(0,1),v\in G[W_{G}]\cup H[W_{H}]\) and \(\bm{\theta}_{u}\) be an uniform prior, i.e. \(\bm{\theta}_{1}=\bm{\theta}_{2}=\ldots=\bm{\theta}_{m}\). Again, we use Lemma E.3 and obtain a distribution \(p_{\bm{\theta},k}\), arbitrarily close to the uniform distribution. Then, the probability of making the two graphs distinguishable by obtaining the mentioned example is greater than \(\frac{1-\varepsilon}{\binom{m}{k}^{2n}}\), which is strictly greater than 0. For a visual example, see Figure A6. 

The next Corollary follows directly from Theorem E.4, and recovers Theorem 4.1 from Qian et al. (2023).

**Corollary E.4.1**.: _For sufficiently large \(n,\) for every \(\varepsilon\in(0,1)\), a set \(M\) of ordered virtual nodes, and \(k>0\), we have that almost all pairs, in the sense of Babai et al. (1980), of isomorphic \(n\)-order graphs \(G\) and \(H\) and all permutation-invariant, \(1\)-\(\mathsf{WL}\)-equivalent functions \(f\colon\mathfrak{A}_{n}\to\mathbb{R}^{d}\), \(d>0\), there exists a probability mass function \(p_{(\bm{\theta},k)}\) that separates the graph \(G\) and \(H\) with probability at most \(\varepsilon\) with respect to \(f\)._

Proof.: We know from Babai et al. (1980) that an \(1\)-\(\mathsf{WL}\)-equivalent algorithm will produce a discrete color partition for almost all pairs of isomorphic graphs \(G\), \(H\) of sufficient size. We use Theorem E.4 and set \(W_{G}=W_{H}=\emptyset\) and conclude that we maintain isomorphisms between almost all isomorphic graphs.

Limitations

A limitation of our approach is the assumption that the number of virtual nodes \(m\) is significantly smaller than the total number of nodes \(n\). As the number of virtual nodes increases, the runtime is also expected to rise (see Table A12 for a detailed example). In the worst-case scenario, where \(m=n\), our method exhibits quadratic complexity. However, in all real-world datasets we have tested, the required number of virtual nodes for achieving optimal performance is low. For more information, refer to Table A5. Another question is whether IPR-MPNNs can perform well on node-level tasks. We have designed our rewiring method specifically to solve long-range graph-level tasks (such as the tasks on the molecular datasets from the Long-Range Graph Benchmark [20]). Nevertheless, IPR-MPNNs and adaptations might also work on node-level tasks, but we leave this question open for further work.

[MISSING_PAGE_EMPTY:25]

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
**Model** & **Model** & **Model** & **Model** & **Model** \\ \hline GINE 2019 & 0.448 \(\pm\)0.073 & 0.650\(\pm\)0.068 & 0.517\(\pm\)0.054 \\ SDRF 2021 & 0.546 \(\pm\)0.004 & 0.644 \(\pm\)0.004 & 0.555 \(\pm\)0.003 \\ DIGL 2019 & 0.582 \(\pm\)0.005 & 0.620 \(\pm\)0.003 & 0.495 \(\pm\)0.003 \\ Geom-GCN 2020 & 0.608 \(\pm\)N/A & 0.676 \(\pm\)N/A & 0.641 \(\pm\)N/A \\ DiffWire 2022 & 0.690 \(\pm\)0.044 & N/A & 0.791 \(\pm\)0.021 \\ Graphormer 2021 & 0.683 \(\pm\)0.017 & 0.767 \(\pm\)0.017 & 0.770 \(\pm\)0.019 \\ GPS 2022 & 0.718 \(\pm\)0.024 & 0.773 \(\pm\)0.013 & 0.798 \(\pm\)0.090 \\ \hline IPR-MPNN (ours) & **0.764**\(\pm\)0.056 & **0.808**\(\pm\)0.052 & **0.804**\(\pm\)0.052 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Performance comparison of different models on the Cornell, Texas, and Wisconsin heterotrophic datasets.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Model & \#Params & V. Nodes & Samples & Train s/ep & Val s/ep & Mem. Usage \\ \hline GINE & \(502k\) & - & - & 3.19\(\pm\)0.03 & 0.20\(\pm\)0.01 & 0.5GiB \\ \hline K-ST SAT\({}_{\text{GINE}}\) & \(506k\) & - & - & 86.54\(\pm\)0.13 & 4.78\(\pm\)0.01 & 11.0GiB \\ K-SG SAT\({}_{\text{GINE}}\) & \(481k\) & - & - & 97.94\(\pm\)0.31 & 5.57\(\pm\)0.01 & 8.5GiB \\ K-ST SAT\({}_{\text{PNA}}\) & \(534k\) & - & - & 90.34\(\pm\)0.29 & 4.85\(\pm\)0.01 & 10.1GiB \\ K-SG SAT\({}_{\text{PNA}}\) & \(509k\) & - & - & 118.75\(\pm\)0.50 & 5.84\(\pm\)0.04 & 9.1GiB \\ GraphGPS & \(558k\) & - & - & 17.02\(\pm\)0.70 & 0.65\(\pm\)0.06 & 6.6GiB \\ \hline PR-MPNN\({}_{\text{GMB}}\) & \(582k\) & - & 20 & 15.20\(\pm\)0.08 & 1.01\(\pm\)0.01 & 0.8GiB \\ PR-MPNN\({}_{\text{IMLE}}\) & \(582k\) & - & 20 & 15.01\(\pm\)0.22 & 1.08\(\pm\)0.06 & 0.9GiB \\ PR-MPNN\({}_{\text{SIM}}\) & \(582k\) & - & 20 & 15.98\(\pm\)0.13 & 1.07\(\pm\)0.01 & 2.1GiB \\ \hline IPR-MPNN\({}_{\text{SIM}}\) & \(548k\) & 2 & 1 & 7.31\(\pm\)0.08 & 0.34\(\pm\)0.01 & 0.8GiB \\ IPR-MPNN\({}_{\text{SIM}}\) & \(548k\) & 4 & 2 & 7.37\(\pm\)0.08 & 0.35\(\pm\)0.01 & 0.8GiB \\ IPR-MPNN\({}_{\text{SIM}}\) & \(548k\) & 10 & 5 & 7.68\(\pm\)0.10 & 0.35\(\pm\)0.01 & 0.9GiB \\ IPR-MPNN\({}_{\text{SIM}}\) & \(549k\) & 20 & 10 & 8.64\(\pm\)0.06 & 0.35\(\pm\)0.01 & 1.1GiB \\ IPR-MPNN\({}_{\text{SIM}}\) & \(549k\) & 30 & 20 & 9.41\(\pm\)0.38 & 0.43\(\pm\)0.01 & 1.2GiB \\ \hline \hline \end{tabular}
\end{table}
Table 11: Comparison between the base GIN model, its variants, and IPR-MPNN on the Exp dataset.

\begin{table}
\begin{tabular}{l c} \hline \hline Model & Accuracy \(\uparrow\) \\ \hline GIN & \(0.511\pm 0.021\) \\ GIN + ID-GNN & \(1.000\pm 0.000\) \\ PR-MPNN & \(1.000\pm 0.000\) \\ IPR-MPNN (ours) & \(1.000\pm 0.000\) \\ \hline \hline \end{tabular}
\end{table}
Table 12: More memory consumption details together with train and validation times per epoch in seconds. We compare to the base GINE model, various variants of the SAT Graph Transformer, GraphGPS, and the PR-MPNN rewiring technique. IPR-MPNNs maintain low memory usage while also being significantly faster when compared to the Graph Transformers and PR-MPNN. The experiments were performed on the OGBG-Moldiv dataset, with the same batch size and the same machine that contains an Nvidia RTX A5000 GPU and an Intel i9-11900K CPU.

\begin{table}
\begin{tabular}{l c} \hline \hline Model & Accuracy \(\uparrow\) \\ \hline GIN & \(0.100\pm 0.000\) \\ GIN + PosEnc & \(1.000\pm 0.000\) \\ PR-MPNN & \(0.998\pm 0.008\) \\ IPR-MPNN (ours) & \(0.987\pm 0.013\) \\ IPR-MPNN\({}^{*}\) (ours) & \(1.000\pm 0.000\) \\ \hline \hline \end{tabular}
\end{table}
Table 11: Comparison between the base GIN model w/wo positional encoding and IPR-MPNN on CsI dataset. For IPR-MPNN*, we pre-calculate the graph partitioning for each data instance, and label each node with its partition ID.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We describe the IPR-MPNN in detail in Section 3 and explain its sub-quadratic running time in the same section. Moreover, in Section 4, we prove that the architecture overcomes expressivity limitations of standard MPNNs. Finally, in Section 5, we show empirically that IPR-MPNNs mitigate underreaching and over-squashing effects. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We describe limitations of IPR-MPNNs in Section F. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: In the appendix, we formally prove all theorems stated in Section 4. All theorems contain all used assumptions. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We will make the code publicly available, including the evaluation protocols. In Table A4, we list all hyperparameters. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We will make the code publicly available, including references to the used public-available datasets. All datasets are available through the interface of PyTorch Geometric. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: In Table A4, we list all hyperparameters. Experimental details and protocols are described in Section 5, and more details are given in Appendix E. Justification: We provide information about the data splits, how they were selected, and the optimizer in Section D. Further, for every dataset and benchmark, we detail the hyperparameters in Table A5. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes]. Justification: We provide standard deviations for all experiments. Guidelines: * The answer NA means that the paper does not include experiments.

* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See Appendix E for details on the used hardware. In Table 1, we provide details on computation time and memory consumption. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We reviewed the NeurIPS Code of Ethics, and our work respects it. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).

10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This paper conducts foundational research in the area of graph learning. While certainly our work could be used both for positive and negative societal impact, we do not foresee any immediate positive or negative societal impact. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We release neither data nor large-scale models as part of this work. Further, our experiments are conducted on comparatively small, curated, task-specific datasets used for benchmarking graph learning models. Hence, our work does not pose immediate risks for misuse. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?Answer: [Yes] Justification: We cite all used datasets and provide the original publications. This is the default in the field of graph learning. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects**Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?

Answer: [NA]

Justification: This work does not involve crowdsourcing or research with humans.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.