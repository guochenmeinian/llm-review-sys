Predicting mutational effects on protein-protein binding via a side-chain diffusion probabilistic model

 Shiwei Liu\({}^{1,2}\)

Tian Zhu\({}^{1,2}\)

Milong Ren\({}^{1,2}\)

Chungong Yu\({}^{1,2}\)

Dongbo Bu\({}^{1,2,3}\)

Hacang Zhang\({}^{1,2}\)

These authors contributed equally to this work.Correspondence should be addressed to H.Z. (zhanghaicang@ict.ac.cn)

###### Abstract

Many crucial biological processes rely on networks of protein-protein interactions. Predicting the effect of amino acid mutations on protein-protein binding is vital in protein engineering and therapeutic discovery. However, the scarcity of annotated experimental data on binding energy poses a significant challenge for developing computational approaches, particularly deep learning-based methods. In this work, we propose SidechainDiff, a representation learning-based approach that leverages unlabelled experimental protein structures. SidechainDiff utilizes a Riemannian diffusion model to learn the generative process of side-chain conformations and can also give the structural context representations of mutations on the protein-protein interface. Leveraging the learned representations, we achieve state-of-the-art performance in predicting the mutational effects on protein-protein binding. Furthermore, SidechainDiff is the first diffusion-based generative model for side-chains, distinguishing it from prior efforts that have predominantly focused on generating protein backbone structures.

## 1 Introduction

Many crucial biological processes, including membrane transport and cell signaling, are likely regulated by intricate networks of protein-protein interactions rather than single proteins acting independently [12, 13][22][14][15]. One representative example is the interaction between the spike protein of the SARS-CoV-2 virus and its receptor protein ACE2 on the surface of human cells, which is crucial for the virus to invade target cells [16][16]. Meanwhile, specific human antibodies, an essential type of protein in the immune system, can prevent the virus entry by competitively binding to the spike protein [17][18].

In protein engineering and therapeutic discovery, it is a crucial step to induce amino acid mutations on the protein-protein interface to modulate binding affinity [19][20]. For example, to enhance the efficacy of an antibody against a virus, one practical approach is to introduce amino acid mutations and filter the resulting antibody sequences to increase binding affinity and specificity to the target viral protein [17][19][21]. However, the variant space that can be exploredusing experimental assays is very limited, and developing an effective high-throughput screening can require a significant experimental effort.

Computational methods have been developed to predict the mutational effect on binding affinity measured by _the change in binding free energy_ (i.e., \(\Delta\Delta G\)). Traditional computational approaches mostly used physical energy features, such as van der Waals and solvation energy, in combination with statistical models to predict \(\Delta\Delta G\)(Schymkowitz et al., 2005; Meireles et al., 2010; Alford et al., 2017; Barlow et al., 2018). The limited model capacity and bias inherent in human-engineered features in these methods hinder their ability to characterize the complex mutational landscape of binding energy. Despite recent breakthroughs in protein modeling with deep learning (Jumper et al., 2021; Baek et al., 2021; Watson et al., 2023), developing deep learning-based models to predict mutational effects on protein-protein binding remains challenging due to the scarcity of labeled experimental data (Jankauskaite et al., 2019).

Recent studies have investigated various self-supervised learning strategies on protein structures and sequences (Jia et al., 2021; Meier et al., 2021; Hsu et al., 2022; Luo et al., 2023) to ease the data scarcity issue. Among them, GeopPII (Liu et al., 2021) and RDE (Luo et al., 2023) have focused on optimizing protein side-chain conformations during pre-training, as the side-chain conformations on the protein-protein interface play a critical role in determining binding energy. In protein-protein interactions, the side-chain conformations of amino acids at the interface may exhibit significant variations when comparing the same receptor with different ligands. These side-chain conformations can be more effectively characterized using probability density. Notably, RDE explores a flow-based model to estimate the uncertainty in side-chain conformations and leverages the learned representations to achieve state-of-the-art performance in predicting \(\Delta\Delta G\). However, flow-based models possess inherent limitations as they require specialized architectures to construct accurate bijective transformations in probability density (Papamakarios et al., 2021; Papamakarios et al., 2021), which results in increased costs associated with model design and implementation.

To address the above limitations, we propose SidechainDiff, a representation learning framework for the protein-protein interfaces. It employs a Riemannian diffusion model to learn the generative process of side-chain conformations and the representation of the structural context of amino acids. To the best of our knowledge, SidechainDiff is the first diffusion probabilistic model for side-chain modeling, whereas previous methods have only focused on protein backbone structures. Furthermore, we leverage the learned representations and neural networks to predict \(\Delta\Delta G\).

## 2 Related Work

### Protein side-chain conformation prediction

Accurate side-chain modeling is essential in understanding the biological functions of proteins. There are two primary categories in predicting protein side-chain conformations: end-to-end full-atom structure prediction methods and side-chain packing. AlphaFold and RoseTTAFold (Jumper et al., 2021; Baek et al., 2021; Baek et al., 2021) are two representative methods that simultaneously generate side-chain conformations and backbone structures. In scenarios like structure refinement and protein design, side-packing becomes pivotal. The objective is to determine the conformations of protein side-chains while having their backbone structures already defined. Traditional methods, including Rosetta (Jeman et al., 2020; Baek et al., 2021; ScWRL4 (Krivov et al., 2009) operate by minimizing the energy function across a pre-defined rotamer library. Recent methods for side-chain packing often employ deep learning models, such as 3D convolution networks and graph attention networks (McParton and Xu, 2023; Misiura et al., 2022; Xu et al., 2022).

Our model distinguishes itself from previous methods for side-chain modelling in two key aspects. First, it's capable of generating a distribution of side-chain conformations rather than a single conformation. Second, we emphasize side-chain modeling specifically for mutation sites within the protein-protein interface, leveraging the known structural context of these sites.

### Prediction of mutational effects on protein-protein binding

Methods for predicting \(\Delta\Delta G\) can be broadly classified into three categories: biophysical methods, statistical methods, and deep learning-based approaches. Biophysical methods offer a robust means of elucidating the molecular mechanisms governing protein-protein interactions and the impact of mutations on these interactions (Schymkowitz et al., 2005; Alford et al., 2017; Steinbrecher et al., 2017). These methods directly integrate information on protein structures and key biophysical properties, such as solvent accessibility, electrostatic potential, and hydrogen bonding patterns. Statistical methods tailor statistical models for the protein properties such as evolutionary conservation and geometric characteristics (Li et al., 2016; Geng et al., 2019; Zhang et al., 2020).

Deep learning-based methods can be categorized into sequence-based methods and structure-based methods. Sequence-based methods primarily either focus on the evolutionary history, multiple sequence alignments (MSAs) in most cases, of the target protein (Hopf et al., 2017; Riesselman et al., 2018; Frazer et al., 2021) or act as protein language models (PLMs) trained on large amounts of protein sequences (Meier et al., 2021; Notin et al., 2022). Structure-based methods can be categorized into end-to-end methods and pre-training-based methods. The end-to-end methods extract features from protein complexes and directly train a neural network model on them (Shan et al., 2022). To mitigate overfitting caused by data sparsity, an alternative approach is to learn representations by pre-training a feature extraction network on unlabeled structures (Liu et al., 2021; Luo et al., 2023; Liu et al., 2023). Among them, RDE-Network (Luo et al., 2023) utilizes normalizing flows in torus space to estimate the density of amino acid side-chain conformations and leverages the learned representation to predict \(\Delta\Delta G\).

### Diffusion probabilistic models

Diffusion Probabilistic Models (DPMs) are generative models to transform a sample from a tractable noise distribution towards a desired data distribution with a gradual denoising process (Sohl-Dickstein et al., 2015; Kingma et al., 2014; Dhariwal and Nichol, 2021). DPMs have achieved impressive results in generating various data types, including images (Ho et al., 2020; Nichol and Dhariwal, 2021), waveforms, and discrete data like text (Hoogeboom et al., 2021). DPMs-based autoencoders have also been proposed to facilitate representation learning for image data (Preechakul et al., 2022; Zhang et al., 2022).

Inspired by these progresses, DPMs have also been explored in protein modeling, including de novo protein design (Watson et al., 2023; Ingraham et al., 2023; Yin et al., 2023), motif-scaffolding (Trippe et al., 2023), and molecular dynamics (Arts et al., 2023; Wu and Li, 2023). While the existing methods utilize DPMs for generating protein backbones, a research gap remains in modeling side-chain conformations, which play a critical role in protein-protein binding. The studies by Jing et al.(2022) and Corso et al.(2023) employ diffusion models for generating torsion angles in the context of small molecular design. In contrast, our research is centered on modeling the torsional angles of protein side-chains. Furthermore, our approach distinguishes itself from their models in terms of how we construct the diffusion process.

## 3 Methods

Our methods have two main components. We first propose SidechainDiff (Figure 1), which is a diffusion probabilistic model for protein side-chains and can also give the structural context representations of mutations on the protein-protein interface. We then propose DiffAffinity that utilize the learned representations to predict \(\Delta\Delta G\). We organize this section as follows: Section 5.1 presents the preliminaries and notations that are used throughout the paper and formally defines the problem. Section 3.2 and 3.3 describe SidechainDiff and DiffAffinity, respectively. Section 3.4 describes key details in model training.

### Preliminaries and notations

A single-chain protein structure is composed of multiple residues (amino acids). Each amino acid shares a common central carbon atom, called the alpha (\(\alpha\)) carbon. The alpha carbon is bonded to an amino group (-NH2), a carboxyl group (-COOH), and a hydrogen atom, forming the amino acid's backbone. Each amino acid has a distinct atom or group bonded to the alpha carbon, known as the side-chain. A residue's side-chain structure is usually termed a side-chain conformation. The side-chain conformation varies across different amino acids, and it depends on the type of amino acid and the overall protein structure. Our main work focuses on building a generative process of side-chain conformations given the protein backbone structures.

Multiple protein chains bind with each other and form a protein complex. Here we only consider protein complexes with two chains just for notation simplicity and note that our model works for protein complexes with more chains. For a protein complex with \(n\) residues, we denote the two chains as chain \(A\) and \(B\), and the set of its residues as \(\{1,...,n\}\). The properties of the \(i\)-th amino acid include the amino acid type \(a_{i}\in\{1,\dots,20\}\), the orientation \(\mathbf{R}_{i}\in SO(3)\), the 3D coordinates \(\mathbf{x}_{i}\in\mathbb{R}^{3}\), and the side-chain torsion angles \(\boldsymbol{\chi}_{i}=(\chi_{i}^{(k)})_{k=1}^{4}\) where \(\chi_{i}^{(k)}\) belongs to \([0,2\pi)\).

The bond lengths and bond angles in side-chains are usually considered fixed and given backbone structures, we can solely use \(\boldsymbol{\chi}_{i}\) to reconstruct the coordinates of each atom in a side-chain. Thus, a side-chain conformation is also called a rotamer, which is parameterized with \(\boldsymbol{\chi}_{i}\). The number of torsional angles ranges from 0 to 4, depending on the residue type. Here, we place a rotamer in a \(4\)-dimensional torus space \(\mathbb{T}^{4}=(\mathbb{S}^{1})^{4}\) where \(\mathbb{S}^{1}\) denotes a unit circle.

### SidechainDiff: side-chain diffusion probabilistic model

SidechainDiff utilizes a conditional Riemannian diffusion model in \(\mathbb{T}^{4}\) to build the generative process of the four rotamers and estimate their joint distributions (Figure1). The generative process is conditioned on the structural context of mutations on the protein-protein interface, which is encoded using a SE(3)-invariant neural network. The learned conditional vectors serve as a natural representation of the structural context.

**Conditional diffusion probabilistic model on \(\mathbb{T}^{4}\)** We adopt the continuous score-based framework on compact Riemannian manifolds [202] to construct the generative process for rotamers within \(\mathbb{T}^{4}\). Our primary adaptation involves conditioning our diffusion model on vectors that are concurrently learned via an encoder network.

Let the random variable \(\mathbf{X}\) denote the states of rotamers. And let \((\mathbf{X}_{t})_{t\in[0,T]}\) and \((\mathbf{Y}_{t})_{t\in[0,T]}=(\mathbf{X}_{T-t})_{t\in[0,T]}\) denote the diffusion process and associated reverse diffusion process in \(\mathbb{T}^{4}\), respectively. The stochastic differential equation (SDE) and reverse SDE [202][202] can be defined as follows:

\[\mathrm{d}\mathbf{X}_{t}=\mathrm{d}\mathbf{B}_{t}^{\mathcal{M}}. \tag{1}\]

Here, \(\mathbf{B}_{t}^{\mathcal{M}}\) represents the Brownian motion on \(\mathbb{T}^{4}\), which is approximated with a Reodesic Random Walk (GRW) on \(\mathbb{T}^{4}\) (Algorithm1). The score \(\nabla\log p_{T-t}(\mathbf{Y}_{t}|\mathbf{Z})\) in Equation2 is estimated with a

Figure 1: The overall architecture of SidechainDiff.

score network \(s_{\theta}(\mathbf{X},t,\mathbf{Z})\). We introduce the conditional vector \(\mathbf{Z}\) in the score, parameterized with an encoder network that takes the structural context of mutation sites as inputs. We will delve into these components separately in the following subsections.

**Geodesic Random Walk in \(\mathbb{T}^{4}\)** When approximating the Brownian motion on the compact Riemannian manifolds using GRWs, a crucial step is defining the projection map and the exponential map that establishes the connection between the ambient Euclidean space and the associated tangent space, as discussed in previous studies (Groensen, 1975; De Bortoli et al., 2022). In the context of \(\mathbb{T}^{4}\), which is constructed as the quadruple product of \(\mathbb{S}^{1}\) within the ambient Euclidean space \(\mathbb{R}^{2}\), it is adequate to focus on modeling Brownian motion on \(\mathbb{S}^{1}\), defined as follows:

\[\mathrm{d}\mathbf{x}_{t}=\mathbf{K}\cdot\mathrm{d}\mathbf{B}_{t},\quad\mathbf{ K}=\left(\begin{array}{cc}0&-1\\ 1&0\end{array}\right). \tag{3}\]

Here, \(\mathbf{B}_{t}\) represents the Brownian motion on the real line in the ambient Euclidean space \(\mathbb{R}^{2}\), and \(\mathbf{K}\) denotes the diffusion coefficient.

Similarly, the projection map \(\mathrm{Proj}_{\mathbf{Y}_{k}^{\gamma}}\) within the tangent space \(T_{\mathbf{Y}_{k}}(\mathbb{T}^{4})\) and the exponential map \(\mathrm{Exp}\) on \(\mathbb{T}^{4}\) can be derived by applying the Cartesian product of the maps designed for \(\mathbb{S}^{1}\). To be more precise, these two maps are defined as follows:

\[\exp_{\mu}(\mathbf{v})=\cos(\|\mathbf{v}\|)\mathbf{\mu}+\sin(\|\mathbf{v}\|)\frac {\mathbf{v}}{\|\mathbf{v}\|}. \tag{4}\]

\[\mathrm{proj}_{\mathbf{\mu}}(\mathbf{z})=\mathbf{z}-\frac{<\mathbf{\mu},\mathbf{z}>}{ \|\mathbf{\mu}\|^{2}}\mathbf{\mu}. \tag{5}\]

Here, \(\mathbf{\mu}\in\mathbb{S}^{1}\), \(\mathbf{v}\in T_{\mathbf{\mu}}(\mathbb{S}^{1})\), and \(\mathbf{z}\in\mathbb{R}^{2}\).

```
1:\(T,N,Y_{0}^{\gamma},P,\{a_{i},\mathbf{R}_{i},\mathbf{x}_{i},\mathbf{\chi}_{i}\}_{i=1 }^{127}\)
2:\(\gamma=T/N\)\(\triangleright\) Step-size
3:\(\mathbf{Z}=\mathrm{Enc}_{\mathcal{O}^{*}}(\{a_{i},\mathbf{R}_{i},\mathbf{x}_{i},\mathbf{\chi}_{i}\}_{i=1}^{127})\)\(\triangleright\) Encode the structural context into hidden representation
4:for\(k\in\{0,\ldots,N-1\}\)do
5:\(\mathbf{R}_{k+1}\sim\mathrm{N}(0,\mathrm{Id})\)\(\triangleright\) Standard Normal noise in ambient space \((\mathbb{R}^{2})^{4}\)
6:\(\mathbf{R}_{k+1}=\mathrm{Proj}_{\mathbf{Y}_{k}^{\gamma}}(\mathbf{R}_{k+1})\)\(\triangleright\) Projection in the Tangent Space \(T_{\mathbf{Y}_{k}}(\mathbb{T}^{4})\)
7:\(\mathbf{W}_{k+1}=-\gamma s_{\theta}(\mathbf{Y}_{k}^{\gamma},k\gamma,\mathbf{Z} )+\sqrt{\gamma}\mathbf{R}_{k+1}\)\(\triangleright\) Compute the Euler-Maruyama step on tangent space
8:\(\mathbf{Y}_{k+1}^{\gamma}=\mathrm{Exp}_{\mathbf{Y}_{k}^{\gamma}}(W_{k+1})\)\(\triangleright\) Move along the geodesic defined by \(W_{k+1}\) and \(\mathbf{Y}_{k}^{\gamma}\) on \(\mathbb{T}^{4}\)
9:endfor
10:return\(\{\mathbf{Y}_{k}^{\gamma}\}_{k=0}^{N}\)
```

**Algorithm 1** Geodesic Random Walk (GRW) during the sampling phase

Score networkThe architecture of the score network \(s_{\theta}(\mathbf{X},t,\mathbf{Z})\) is implemented as a three-layer Multilayer Perceptron (MLP) with 512 units in each layer. It takes as inputs the conditional vector from the encoder network representing the structural context of the mutations, the sampled rotamers, and the timestep \(t\).

**Conditional encoder network** The encoder network (Figure S1) takes as inputs both single features, consisting of amino acid type, backbone dihedral angles, and local atom coordinates for each amino acid, as well as pair features such as pair distances and relative positions between two amino acids. An SE3-invariant IPA network (Jumper et al., 2021) is utilized to obtain the conditional vector \(\mathbf{Z}\) for the structural context of the mutations. Further information regarding the encoder can be found in App. A.1

**Training objectives** The parameters \(\theta\) and \(\phi\) in the score network and conditional encoder network are trained simultaneously. We adopt the implicit loss \(l_{t}^{\mathrm{im}}(s_{t})\) in De Bortoli et al. (2022), defined as follows:

\[l_{t}^{\mathrm{im}}(s_{t})=\int_{\mathcal{M}}\{\frac{1}{2}\|s_{\theta}( \mathbf{X}_{t},t,\mathbf{Z})\|^{2}+\mathrm{div}(s_{t})(\mathbf{X}_{t},t, \mathbf{Z})\}\mathrm{d}\mathbb{P}_{t}(\mathbf{X}_{t}). \tag{6}\]

### DiffAffinity: mutational effect predictor

For the mutational effect predictor, DiffAffinity, we imitate the structure of the mutated protein by setting the side-chain conformation as an empty set and altering the amino acids at the mutation sites. Subsequently, we use the pre-trained conditional encoder \(\mathrm{Enc}_{\phi^{*}}\) from SidechainDiff to encode wild-type and mutant structures into distinct hidden representations. These hidden representations, following additional IPA-like transformer network and max aggregation, are fed into a Multilayer Perceptron (MLP) of only one layer for the prediction of \(\Delta\Delta G\). To assess the quality of our hidden representation, we also utilize the linear combination of SidechainDiff's hidden representations to predict \(\Delta\Delta G\). More model details can be found in App. A.2

### Model training

We train SidechainDiff with the refined experimental protein structures on the database of Protein Data Bank-REDO [10]. To fairly benchmark the performance of SidechainDiff, we follow the same protocol in RDE [11] to preprocess the data and split the train and test sets. We train DiffAffinity with the SKEMPI2 dataset [11]. Due to limited samples, we perform a strategy of three-fold cross-validation to train the model and benchmark the performance. Specifically, we split the SKEMPI2 dataset into three folds based on structure, ensuring that each fold contains unique protein complexes not present in the other folds. Two of the folds are utilized for training and validation, whereas the remaining fold is reserved for testing purposes. More training settings are shown in App A.2

## 4 Results

First, we assess DiffAffinity's performance in predicting \(\Delta\Delta G\) using the SKEMPI2 dataset (Section 4.1) and the latest SARS-CoV-2 dataset (Section 4.2). Subsequently, we demonstrate DiffAffinity's effectiveness in optimizing antibodies, with a focus on SARS-CoV-2 as an illustrative case (Section 4.3). Additionally, we evaluate the prediction accuracy of SidechainDiff in side-chain packing.

### Prediction of the mutational effects on binding affinity

DiffAffinity leverages the learned representation from SidechainDiff to predict mutational effects on protein-protein binding. Here, we benchmark the performance on the widely used SKEMPI2 dataset.

Baseline modelsAs mentioned in section 2.2 the models for mutational effects prediction can be categorized into energy-based, sequence-based, unsupervised, end-to-end, and pre-trained models. We have selected several representative models from each category. The selected models include FoldX [12], Rosetta [13], flex ddG [1], ESM-1v [14], ESM-IF [15], ESM2*, LDGPred [16], and RDE-Network [11]. More details of baselines are shown in App. A.4

For ablation studies, we have also developed two variants named DiffAffinity* and DiffAffinity-Linear. DiffAffinity* employs the same model architecture as DiffAffinity but without the learned representations from SidechainDiff and it serves as an end-to-end prediction method. And DiffAffinity-Linear is a direct linear projection of SidechainDiff's hidden representations to \(\Delta\Delta G\).

Evaluation metricsThe performance of our model is evaluated using a variety of metrics. These include traditional prediction accuracy metrics such as root mean squared error (RMSE) and mean absolute error (MAE). Given that the scalar values of experimental \(\Delta\Delta G\) are not standardized, we also employ Pearson and Spearman correlation coefficients to evaluate the models' predictive capability. To assess our model's ability to identify positive or negative mutations affecting protein-protein binding, we use the AUROC and AUPRC metrics. Mutations are classified based on the sign of \(\Delta\Delta G\), where positive values indicate positive impacts on binding and negative values indicate negative impacts. AUROC measures the overall discriminative power, while AUPRC accounts for imbalanced datasets.

Following the RDE-network ([11]), we also evaluate the prediction performance for each complex separately. We group mutations by complex structure, excluding groups with fewer than ten mutation data points, and derive two metrics: average per-structure Pearson correlation coefficient per-structure Pearson and average per-structure Spearman correlation coefficient per-structure Spearman.

Experimental resultsOur methods, DiffAffinity and DiffAffinity*, outperform all baseline models across almost all metrics and achieve state-of-the-art performance on the SKEMPI2 dataset (Table 1).

[MISSING_PAGE_FAIL:7]

### Prediction of mutational effects on binding affinity of SARS-CoV-2 RBD

SARS-CoV-2 initiates infection by binding to ACE2 protein on host cells via the viral spike protein. The receptor binding domain (RBD) of the spike proteins exhibits high-affinity binding to ACE2.

The technique of deep mutational scanning has been employed to experimentally quantify the effects of all single-point mutations on the binding affinity of the ancestral Wuhan-Hu-1 RBD (PDB ID: 6M0J) [Starr et al., 2022]. These measurements have guided the survey of SARS-CoV-2 evolution and identified several significant mutation sites on RBD that substantially contribute to the binding affinity. A total of 15 significant mutation sites like NE50[] have been identified (see App. A.5). We predicted all 285 possible single-point mutations for these sites and calculated the Pearson correlation coefficient between the experimental and predicted \(\Delta\Delta G\).

Our results show that DiffAffinity outperforms all baseline methods (Table 2), highlighting its potential to facilitate biologists in understanding the evolution of SARS-CoV-2. Furthermore, we note that DiffAffinity exhibits a substantial improvement over DiffAffinity*, indicating the effectiveness of the learned representations from SidechainDiff.

### Optimization of human antibodies against SARS-CoV-2

The receptor-binding domain (RBD) of the SARS-CoV-2 virus spike protein (PDB ID: 7FAE) plays a pivotal role in the binding process with the host's ACE2 protein. It serves as a prime target for neutralizing antibodies against SARS-CoV-2 [Shan et al., 2022]. In our experiment, we predict all 494 possible mutations at 26 sites within the CDR region of the antibody heavy chain. We expect that predicting mutational effects on binding affinity can facilitate in identifying top favorable mutations that can enhance the neutralization efficacy of antibodies.

These mutations are ranked in ascending order according to their \(\Delta\Delta G\) values, with the most favorable (lowest \(\Delta\Delta G\)) mutations positioned at the top. To enhance neutralization efficacy, we aim to accurately identify and rank the five most favorable mutations. DiffAffinity successfully identifies four of these mutations within the top 20% of the ranking and two within the top 10% (Table 3). DiffAffinity consistently outperforms all the baseline methods, indicating its superior ability to predict the effects of mutations. This highlights DiffAffinity's potential as a robust tool for optimizing human antibodies.

### Prediction of side-chain conformations

To assess the generative capacity of SidechainDiff straightforwardly, we employ it for predicting side-chain conformations of specific amino acids given their structural context. Our approach involves sampling rotamers from the distribution modeled by SidechainDiff and selecting the rotamer with the highest likelihood from a pool of 100 samples. For this task, we draw upon the test dataset from the PDB-REDO test split.

For a rigorous evaluation, we compare SidechainDiff with both energy-based methods, including Rosetta [Leman et al., 2020] and SCWRL4 [Krivov et al., 2009], as well as the deep learning-based methods, including RDE, AttnPacker [McPartlon and Xu, 2023], and DLPacker [Misiura et al., 2022]. All methods are run under the same settings to ensure a fair comparison. We evaluate the performance

\begin{table}
\begin{tabular}{l|c} \hline \hline Method & Pearson \\ \hline FoldX & \(0.385\) \\ RDE-Net & \(0.438\) \\ DiffAffinity* & \(0.295\) \\ \hline
**DiffAffinity** & \(\mathbf{0.466}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Pearson correlation coefficient in SARS-COV-2 binding affinity

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline Method & TH31W & AH53F & NH57L & RH103M & LH104F \\ \hline FoldX & \(\mathbf{4.25\%}\) & \(\mathbf{14.57\%}\) & \(\mathbf{2.43\%}\) & \(27.13\%\) & \(63.77\%\) \\ RDE-Net & \(\mathbf{5.06\%}\) & \(\mathbf{12.15\%}\) & \(55.47\%\) & \(50.61\%\) & \(\mathbf{9.51\%}\) \\ DiffAffinity* & \(\mathbf{7.29\%}\) & \(\mathbf{0.81\%}\) & \(\mathbf{19.03\%}\) & \(84.21\%\) & \(28.54\%\) \\ \hline
**DiffAffinity** & \(\mathbf{7.28\%}\) & \(\mathbf{3.64\%}\) & \(\mathbf{18.82\%}\) & \(81.78\%\) & \(\mathbf{10.93\%}\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Rankings of the five favorable mutations on the human antibody against SARS-CoV-2 receptor-binding domain (RBD) by various competitive methodsusing the Mean Absolute Error (MAE) and steric clash number of the predicted rotamers [14, 15].

In terms of MAE for rotamer prediction, SidechainDiff outperforms energy-based methods and achieves comparable results to deep learning-based methods such as RDE and AttnPacker (Table 4 and Table S1). Notably, our method surpasses all the other methods in terms of the steric clash number. It's noteworthy that while methods like AttnPacker focus on predicting the torsion angles of static side-chain conformations, SidechainDiff emphasizes the distribution of side-chain conformations under various physical constraints. We measure the physical constraints of mutation sites using the contact number, i.e., the number of neighboring amino acids within an 8\(\hat{A}\) radius. Generally, a higher contact number indicates a more constrained structural context. We observe that for mutation sites with higher constraints, SidechainDiff tends to produce more accurate side-chain conformations (Table 5). We further illustrate cases showing the trade-off between accuracy and diversity achieved by SidechainDiff under various physical constraints (Appendix 5 and Figure 53).

## 5 Conclusions

We present SidechainDiff, a Riemannian diffusion-based generative model for protein side-chains. It excels in both generating side-chain conformations and facilitating representation learning. Utilizing the learned representations, our method achieves state-of-the-art performance in predicting \(\Delta\Delta G\) across various test datasets, including the SKEMPI2 dataset and the latest SARS-CoV-2 dataset. Moreover, we demonstrate its effectiveness in antibody screening and its superior side-chain packing accuracy.

In future research, we can consider integrating the side-chain clash loss [14] to refine our generative process. Additionally, we can explore building an end-to-end diffusion-based generative model for protein full-atom structures by integrating our model with existing backbone structure generation methods [13, 14]. The main limitation of our model is the lack of consideration of backbone structure changes induced by mutations. Addressing the limitation could lead to enhanced performance.

## 6 Acknowledgements

We would like to thank the National Key Research and Development Program of China (2020YFA0907000), and the National Natural Science Foundation of China (32370657, 32271297, 82130055, 62072435), and the Project of Youth Innovation Promotion Association CAS to H.Z. for providing financial support for this study and publication charges. The numerical calculations in this study were supported by ICT Computer-X center.

## References

* Alford et al. [2017] Rebecca F Alford, Andrew Leaver-Fay, Jeliazko R Jeliazkov, et al. The Rosetta All-Atom Energy Function for Macromolecular Modeling and Design. _Journal of Chemical Theory and Computation_, 13(6):3031-3048, 2017.
* Alford et al. [2018]

\begin{table}
\begin{tabular}{l|c c c c|c} \hline \hline Method & \(\chi^{(1)}\) & \(\chi^{(2)}\) & \(\chi^{(3)}\) & \(\chi^{(4)}\) & Average & Clash number \\ \hline Proportion & 100\% & 82.6\% & 28.2\% & 12.6\% & - & \\ \hline SCWRL4 & \(24.33^{\circ}\) & \(32.84^{\circ}\) & \(47.42^{\circ}\) & \(56.15^{\circ}\) & \(28.21^{\circ}\) & \(118.44\) \\ Rosetta & \(23.98^{\circ}\) & \(32.14^{\circ}\) & \(48.49^{\circ}\) & \(58.78^{\circ}\) & \(28.09^{\circ}\) & \(113.31\) \\ RDE & \(\mathbf{17.13^{\circ}}\) & \(28.47^{\circ}\) & \(\mathbf{43.54^{\circ}}\) & \(58.62^{\circ}\) & \(\mathbf{21.99^{\circ}}\) & \(34.41\) \\ DLPacker & \(21.40^{\circ}\) & \(29.27^{\circ}\) & \(50.77^{\circ}\) & \(74.64^{\circ}\) & \(26.42^{\circ}\) & \(62.45\) \\ AttnPacker & \(19.65^{\circ}\) & \(\mathbf{25.17^{\circ}}\) & \(47.61^{\circ}\) & \(\mathbf{55.08^{\circ}}\) & \(22.35^{\circ}\) & \(57.31\) \\ \hline
**DiffAffinity** & \(18.00^{\circ}\) & \(27.41^{\circ}\) & \(43.97^{\circ}\) & \(56.65^{\circ}\) & \(22.06^{\circ}\) & \(\mathbf{27.70}\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Evaluation of predicted side-chain conformations

\begin{table}
\begin{tabular}{l|c c c c|c} \hline \hline  & & & & & \\ \hline \hline Method & \(\chi^{(1)}\) & \(\chi^{(2)}\) & \(\chi^{(3)}\) & \(\chi^{(4)}\) & Average & Clash number \\ \hline Proportion & 100\% & 82.6\% & \(28.2\%\) & 12.6\% & - & \\ \hline SCWRL4 & \(24.33^{\circ}\) & \(32.84^{\circ}\) & \(47.42^{\circ}\) & \(56.15^{\circ}\) & \(28.21^{\circ}\) & \(118.44\) \\ Rosetta & \(23.98^{\circ}\) & \(32.14^{\circ}\) & \(48.49^{\circ}\) & \(58.78^{\circ}\) & \(28.09^{\circ}\) & \(113.31\) \\ RDE & \(\mathbf{17.13^{\circ}}\) & \(28.47^{\circ}\) & \(\mathbf{43.54^{\circ}}\) & \(58.62^{\circ}\) & \(\mathbf{21.99^{\circ}}\) & \(34.41\) \\ DLPacker & \(21.40^{\circ}\) & \(29.27^{\circ}\) & \(50.77^{\circ}\) & \(74.64^{\circ}\) & \(26.42^{\circ}\) & \(62.45\) \\ AttnPacker & \(19.65^{\circ}\) & \(\mathbf{25.17^{\circ}}\) & \(47.61^{\circ}\) & \(\mathbf{55.08^{\circ}}\) & \(22.35^{\circ}\) & \(57.31\) \\ \hline
**DiffAffinity** & \(18.00^{\circ}\) & \(27.41^{\circ}\) & \(43.97^{\circ}\) & \(56.65^{\circ}\) & \(22.06^{\circ}\) & \(\mathbf{27.70}\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Accuracy of side-chain conformations stratified based on the contact number Marloes Arts, Victor Garcia Satorras, Chin-Wei Huang, et al. Two for One: Diffusion Models and Force Fields for Coarse-Grained Molecular Dynamics. _Journal of Chemical Theory and Computation_, 19(18):6151-6159, 2023.
* Baek et al. (2021) Minkyung Baek, Frank DiMaio, Ivan Anishchenko, et al. Accurate prediction of protein structures and interactions using a three-track neural network. _Science_, 373(6557):871-876, 2021.
* Barlow et al. (2018) Kyle A Barlow, Shane O Conchur, Samuel Thompson, et al. Flex ddG: Rosetta Ensemble-Based Estimation of Changes in Protein-Protein Binding Affinity upon Mutation. _The Journal of Physical Chemistry B_, 122(21):5389-5399, 2018.
* Calakos et al. (1994) Nicole Calakos, Mark K Bennett, Karen E Peterson, et al. Protein-protein interactions contributing to the specificity of intracellular vesicular trafficking. _Science_, 263(5150):1146-1149, 1994.
* Corso et al. (2023) Gabriele Corso, Hannes Stark, Bowen Jing, et al. DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking. In _International Conference on Learning Representations_, 2023.
* De Bortoli et al. (2022) Valentin De Bortoli, Emile Mathieu, Michael Hutchinson, et al. Riemannian Score-Based Generative Modelling. In _Advances in Neural Information Processing Systems_, volume 35, pages 2406-2422, 2022.
* Dhariwal and Nichol (2021) Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In _Advances in Neural Information Processing Systems_, volume 34, pages 8780-8794, 2021.
* Frazer et al. (2021) Jonathan Frazer, Pascal Notin, Mafalda Dias, et al. Disease variant prediction with deep generative models of evolutionary data. _Nature_, 599(7883):91-95, 2021.
* Geng et al. (2019) Cunliang Geng, Anna Vangone, Gert E Folkers, et al. iSEE: Interface structure, evolution, and energy-based machine learning predictor of binding affinity changes upon mutations. _Proteins: Structure, Function, and Bioinformatics_, 87(2):110-119, 2019.
* Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In _Advances in Neural Information Processing Systems_, volume 33, pages 6840-6851, 2020.
* Hoogeboom et al. (2021) Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, et al. Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions. In _Advances in Neural Information Processing Systems_, volume 34, pages 12454-12465, 2021.
* Hopf et al. (2017) Thomas A Hopf, John B Ingraham, Frank J Poelwijk, et al. Mutation effects predicted from sequence co-variation. _Nature Biotechnology_, 35(2):128-135, 2017.
* Hsu et al. (2022) Chloe Hsu, Robert Verkuil, Jason Liu, et al. Learning inverse folding from millions of predicted structures. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162, pages 8946-8970, 2022.
* Ingraham et al. (2022) John Ingraham, Max Baranov, Zak Costello, et al. Illuminating protein space with a programmable generative model. _bioRxiv_. doi[10.1101/2022.12.01.518682]
* Jankauskaite et al. (2019) Justina Jankauskaite, Brian Jimenez-Garcia, Justas Dapkunas, et al. Skempi 2.0: an updated benchmark of changes in protein-protein binding energy, kinetics and thermodynamics upon mutation. _Bioinformatics_, 35(3):462-469, 2019.
* Jing et al. (2022) Bowen Jing, Gabriele Corso, Jeffrey Chang, et al. Torsional Diffusion for Molecular Conformer Generation. In _Advances in Neural Information Processing Systems_, volume 35, pages 24240-24253, 2022.
* Jones and Thornton (1996) Susan Jones and Janet M Thornton. Principles of protein-protein interactions. _Proceedings of the National Academy of Sciences_, 93(1):13-20, 1996.
* Joosten et al. (2014) Robbie P. Joosten, Fei Long, Garib N. Murshudov, et al. The PDB_REDO server for macromolecular structure model optimization. _IUCrJ_, 1(4):213-220, 2014.
* Jorgensen (1975) Erik Jorgensen. The central limit problem for geodesic random walks. _Zeitschrift fur Wahrscheinlichkeitstheorie und verwandte Gebiete_, 32(1-2):1-64, 1975.
* Jorgensen et al. (2018)John Jumper, Richard Evans, Alexander Pritzel, et al. Highly accurate protein structure prediction with AlphaFold. _Nature_, 596(7873):583-589, 2021.
* Kingma et al. [2021] Diederik Kingma, Tim Salimans, Ben Poole, et al. Variational Diffusion Models. In _Advances in Neural Information Processing Systems_, volume 34, pages 21696-21707, 2021.
* Krivov et al. [2009] Georgii Krivov, Maxim Shapovalov, and Roland Dunbrack. Improved prediction of protein side-chain conformations with SCWRL4. _Proteins_, 77:778-95, 12 2009.
* Lan et al. [2020] Jun Lan, Jiwan Ge, Jinfang Yu, et al. Structure of the SARS-CoV-2 spike receptor-binding domain bound to the ACE2 receptor. _Nature_, 581(7807):215-220, 2020.
* Leman et al. [2020] Julia Koehler Leman, Brian D. Weitzner, Steven M. Lewis, et al. Macromolecular modeling and design in Rosetta: recent methods and frameworks. _Nature Methods_, 17:665-680, 2020.
* Levskaya et al. [2009] Anselm Levskaya, Orion D Weiner, Wendell A Lim, et al. Spatiotemporal control of cell signalling using a light-switchable protein interaction. _Nature_, 461(7266):997-1001, 2009.
* Li et al. [2016] Minghui Li, Franco L Simonetti, Alexander Goncearenco, et al. Mutabind estimates and interprets the effects of sequence variants on protein-protein interactions. _Nucleic Acids Research_, 44(W1):W494-W501, 2016.
* Lin et al. [2023] Zeming Lin, Halil Akin, Roshan Rao, et al. Evolutionary-scale prediction of atomic-level protein structure with a language model. _Science_, 379(6637):1123-1130, 2023.
* Liu et al. [2021] Xianggen Liu, Yunan Luo, Pengyong Li, et al. Deep geometric representations for modeling effects of mutations on protein-protein binding affinity. _PLoS Computational Biology_, 17(8):e1009284, 2021.
* Luo et al. [2023] Shitong Luo, Yufeng Su, Zuofan Wu, et al. Rotamer Density Estimator is an Unsupervised Learner of the Effect of Mutations on Protein-Protein Interaction. In _International Conference on Learning Representations_, 2023.
* Mason et al. [2021] Derek M Mason, Simon Friedensohn, Cedric R Weber, et al. Optimization of therapeutic antibodies by predicting antigen specificity from antibody sequence via deep learning. _Nature Biomedical Engineering_, 5(6):600-612, 2021.
* McPartlon and Xu [2023] Matthew McPartlon and Jinbo Xu. An end-to-end deep learning method for protein side-chain packing and inverse folding. _Proceedings of the National Academy of Sciences_, 120(23):e2216438120, 2023.
* Meier et al. [2021] Joshua Meier, Roshan Rao, Robert Verkuil, et al. Language models enable zero-shot prediction of the effects of mutations on protein function. In _Advances in Neural Information Processing Systems_, volume 34, pages 29287-29303, 2021.
* Meireles et al. [2010] Lidia MC Meireles, Alexander S Di\(\acute{\iota}_{i}\)\(1/2\)mling, and Carlos J Camacho. ANCHOR: a web server and database for analysis of protein-protein interaction binding pockets for drug discovery. _Nucleic Acids Research_, 38:W407-W411, 2010.
* Misiura et al. [2022] Mikita Misiura, Raghav Shroff, Ross Thyer, et al. DLPacker: Deep learning for prediction of amino acid side chain conformations in proteins. _Proteins: Structure, Function, and Bioinformatics_, 90(6):1278-1290, 2022.
* Nichol and Dhariwal [2021] Alexander Quinn Nichol and Prafulla Dhariwal. Improved Denoising Diffusion Probabilistic Models. In _Proceedings of the 38th International Conference on Machine Learning_, volume 139, pages 8162-8171, 2021.
* Notin et al. [2022] Pascal Notin, Mafalda Dias, Jonathan Frazer, et al. Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162, pages 16990-17017, 2022.
* Papamakarios et al. [2021] George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, et al. Normalizing Flows for Probabilistic Modeling and Inference. _The Journal of Machine Learning Research_, 22(1):2617-2680, 2021.
* Papamakarios et al. [2021]Konpat Preechakul, Nattanat Chatthee, Suitsak Wizadwongsa, et al. Diffusion Autoencoders: Toward a Meaningful and Decodable Representation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10619-10629, 2022.
* Riesselman et al. (2018) Adam J Riesselman, John B Ingraham, and Debora S Marks. Deep generative models of genetic variation capture the effects of mutations. _Nature Methods_, 15(10):816-822, 2018.
* Schwikowski et al. (2000) Benno Schwikowski, Peter Uetz, and Stanley Fields. A network of protein-protein interactions in yeast. _Nature Biotechnology_, 18(12):1257-1261, 2000.
* Schymkowitz et al. (2005) Joost Schymkowitz, Jesper Borg, Francois Stricher, et al. The FoldX web server: an online force field. _Nucleic Acids Research_, 33:W382-W388, 2005.
* Shan et al. (2022) Sisi Shan, Shitong Luo, Ziqing Yang, et al. Deep learning guided optimization of human antibody against SARS-CoV-2 variants with broad neutralization. _Proceedings of the National Academy of Sciences_, 119(11):e2122954119, 2022.
* Sohl-Dickstein et al. (2015) Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, et al. Deep Unsupervised Learning using Nonequilibrium Thermodynamics. In _Proceedings of the 32nd International Conference on Machine Learning_, volume 37, pages 2256-2265, 2015.
* Song et al. (2021) Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, et al. Score-Based Generative Modeling through Stochastic Differential Equations. In _International Conference on Learning Representations_, 2021.
* Starr et al. (2022) Tyler N. Starr, Allison J. Greaney, William W. Hannon, et al. Shifting mutational constraints in the SARS-CoV-2 receptor-binding domain during viral evolution. _Science_, 377(6604):420-424, 2022.
* Steinbrecher et al. (2017) Thomas Steinbrecher, Robert Abel, Anthony Clark, et al. Free Energy Perturbation Calculations of the Thermodynamics of Protein Side-Chain Mutations. _Journal of Molecular Biology_, 429(7):923-929, 2017.
* O'Neil and Hoess (1995) Karyn T O'Neil and Ronald H Hoess. Phage display: protein engineering by directed evolution. _Current Opinion in Structural Biology_, 5(4):443-449, 1995.
* Trippe et al. (2023) Brian L. Trippe, Jason Yim, Doug Tischer, et al. Diffusion Probabilistic Modeling of Protein Backbones in 3D for the motif-scaffolding problem. In _International Conference on Learning Representations_, 2023.
* Watson et al. (2023) Joseph L Watson, David Juergens, Nathaniel R Bennett, et al. De novo design of protein structure and function with RFdiffusion. _Nature_, 620(7976):1089-1100, 2023.
* Wu and Li (2023) Fang Wu and Stan Z Li. DIFFMD: A Geometric Diffusion Model for Molecular Dynamics Simulations. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 5321-5329, 2023.
* Xu et al. (2022) Gang Xu, Qinghua Wang, and Jianpeng Ma. OPUS-Rota4: a gradient-based protein side-chain modeling framework assisted by deep learning-based predictors. _Briefings in Bioinformatics_, 23(1):bbab529, 2022.
* Yim et al. (2023) Jason Yim, Brian L. Trippe, Valentin De Bortoli, et al. SE(3) Diffusion Model with Application to Protein Backbone Generation. In _Proceedings of the 40th International Conference on Machine Learning_, volume 39, page 1672, 2023.
* Zhang et al. (2020) Ning Zhang, Yuting Chen, Haoyu Lu, et al. MutaBind2: Predicting the Impacts of Single and Multiple Mutations on Protein-Protein Interactions. _Iscience_, 23(3):100939, 2020.
* Zhang et al. (2022) Zijian Zhang, Zhou Zhao, and Zhijie Lin. Unsupervised Representation Learning from Pre-trained Diffusion Probabilistic Models. In _Advances in Neural Information Processing Systems_, volume 35, pages 22117-22130, 2022.