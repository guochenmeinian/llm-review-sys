Rethinking the Power of Timestamps for Robust Time Series Forecasting: A Global-Local Fusion Perspective

Chengsen Wang\({}^{1}\)1 Qi Qi\({}^{1}\)1 Jingyu Wang\({}^{12}\)2

Haifeng Sun\({}^{1}\)  Zirui Zhuang\({}^{1}\)  Jinming Wu\({}^{1}\)  Jianxin Liao\({}^{1}\)

\({}^{1}\)Beijing University of Posts and Telecommunications, Beijing, China

\({}^{2}\)Pengcheng Laboratory, Shenzhen, China

{cswang, qiqi8266, wangjingyu}@bupt.edu.cn

{hfsun, zhuangzirui, wjm_18, liaojx}@bupt.edu.cn

Equal contribution.Corresponding author.

Footnote 1: footnotemark:

###### Abstract

Time series forecasting has played a pivotal role across various industries, including finance, transportation, energy, healthcare, and climate. Due to the abundant seasonal information they contain, timestamps possess the potential to offer robust global guidance for forecasting techniques. However, existing works primarily focus on local observations, with timestamps being treated merely as an optional supplement that remains underutilized. When data gathered from the real world is polluted, the absence of global information will damage the robust prediction capability of these algorithms. To address these problems, we propose a novel framework named GLAFF. Within this framework, the timestamps are modeled individually to capture the global dependencies. Working as a plugin, GLAFF adaptively adjusts the combined weights for global and local information, enabling seamless collaboration with any time series forecasting backbone. Extensive experiments conducted on nine real-world datasets demonstrate that GLAFF significantly enhances the average performance of widely used mainstream forecasting models by 12.5%, surpassing the previous state-of-the-art method by 5.5%. Code is available at https://github.com/ForestsKing/GLAFF.

## 1 Introduction

Time series forecasting holds significant importance across various industries, including finance [1, 12], transportation [4, 11], energy [28, 31], healthcare[16, 29], and climate [8, 44]. With the development of deep learning techniques, neural network-based methods [15, 18, 40, 47] have notably propelled advancements owing to their strong capability in capturing dependencies within time series. The relevant models have evolved from statistical models to RNNs, CNNs, Transformers, and LLMs. However, existing research primarily concentrates on local observations within history sliding windows, overlooking the significance of timestamps.

Due to the abundant seasonal information they contain, timestamps possess the potential to offer robust global guidance for forecasting techniques. For instance, traffic volumes on weekdays typically exhibit high peaks. Regrettably, existing works primarily focus on local observations, with timestamps being treated merely as an optional supplement that remains underutilized. DLinear [41] and FPT [47] completely overlook timestamps. Informer [45] and TimesNet [38] incorporate timestamps by summing their embeddings with position embeddings and data embeddings. These intertwinedpatterns encourage networks to extract information from more intuitive observations. iTransformer [24] embeds timestamp features separately into tokens employed by the attention mechanism. This embedding method across time points damaged the physical significance of timestamps. To validate this proposition, we conduct an ablation study on the aforementioned models using the Traffic dataset. The results depicted in Figure 1(a) indicate that the performance of the models exhibits no significant decline after removing timestamps. Meanwhile, our proposed GLAFF demonstrates a notable enhancement in mainstream forecasting models.

Moreover, Time series collected from the real world often be polluted [5]. For example, a spike in electricity consumption coupled with short circuits can induce point anomalies, while a reduction in traffic volume coinciding with holidays can evoke contextual anomalies. When local information gathered from the real world contains anomalies, the absence of global information will damage the robust prediction capability of most forecasting techniques [7; 39; 43; 46]. We illustrate traffic volume for San Francisco Bay area freeways at an hourly granularity in Figure 1(b). Typically, this sequence exhibits a clear periodic pattern, alternating with five high peaks (weekdays) and two low peaks (weekends). However, due to a holiday, the week from 24 to 192 shows a deviation, resulting in three high peaks and four low peaks. As evident from the illustration in the lower right corner of Figure 1(b), the mainstream forecasting models [24; 38; 41] usually demonstrate reliable prediction capability. Nonetheless, when the observations within the history window include anomalies, as illustrated in the lower left corner of Figure 1(b), these models are significantly affected and yield notably underestimated predictions. Therefore, it is necessary to reasonably incorporate more robust global information into the existing forecasting technique.

To address the aforementioned problems, we propose a generalized framework named GLAFF (short for **G**lobal-**L**ocal **A**daptive **F**usion **F**ramework), aimed at enhancing the robust prediction capability of time series forecasting models in the real world leveraging global information. Specifically, GLAFF initially employs the Attention-based Mapper to individually model the timestamps containing global information and maps them to observations conforming to a standard distribution. Subsequently, to handle scenarios where anomalies are present within the observations of the sliding window, we utilize the Robust Denormalizer to inverse normalize the initial mappings, thereby mitigating the impact of data drift [17]. Finally, the Adaptive Combiner dynamically adjusts the combined weights for global mapping and local prediction within the prediction window, yielding the final prediction outcome. By fusing the robustness of global information with the flexibility of local information, GLAFF demonstrates a substantial enhancement in the robust prediction capability of mainstream forecasting models. Additionally, GLAFF serves as a model-agnostic and plug-and-play framework that can seamlessly collaborate with any time series forecasting backbone.

In general, the contributions of our paper are summarised as follows:

* We propose GLAFF that leverages global information, represented by timestamps, to improve the robust prediction capability of time series forecasting models. GLAFF is a plug-and-play module that seamlessly collaborates with any time series forecasting backbone.

Figure 1: The experimental results on Traffic dataset. (a) illustrates the outcomes of the ablation study on mainstream forecasting models and their variants. (b) depicts the visualization of traffic volume (upper), successful prediction case (lower right), and failed prediction case (lower left), respectively.

* We design a Robust Denormalization module to facilitate the adaptation of GLAFF for data drift, even when the observations encompass anomalies, alongside an Adaptive Combiner module for dynamically fusing global and local information.
* We conduct comprehensive experiments on nine real-world benchmark datasets across five domains. The result demonstrates that GLAFF significantly improves the robust prediction capability of mainstream forecasting models.

## 2 Related Work

As a significant real-world challenge, time series forecasting has garnered considerable attention. Initially, ARIMA [2] establishes an autoregressive model and performs forecasts in a moving average manner. However, the inherent complexity of the real world often renders such statistical methodologies [2; 14; 33] challenging to adapt. With the development of deep learning techniques, neural network-based methods have become increasingly important. Recurrent neural networks [10; 13; 30] dynamically capture temporal dependencies by modeling semantic information within a sequential structure. Unfortunately, this architecture suffers from gradient vanishing/exploding and information forgetting when dealing with long sequences. To further improve prediction performance, self-attention mechanisms [19; 22; 45] and convolutional networks [21; 36; 38] have been introduced to capture long-range dependencies. Additionally, prior research [41] has demonstrated that a simple linear network augmented by decomposition can also achieve competitive performance. Nowadays, with fast growth and remarkable performances of large language models, there is a growing interest [3; 32; 47] in utilizing LLM to analyze time series data. Recently, the iTransformer [24] has emerged as the state-of-the-art method for time series forecasting tasks by embedding series from different channels into the variate tokens utilized by the attention mechanism.

Most time series forecasting techniques focus on local observations, with timestamps being treated merely as an optional supplement that remains underutilized. DLinear [41], FPT [47], and other models [26; 40; 43] completely overlook timestamps. When data gathered from the real world is polluted, the absence of global information will damage the robust prediction capability of these algorithms. Informer [45], TimesNet [38], and other models [23; 37; 46] incorporate timestamps by summing their embeddings with position embeddings and data embeddings. These intertwined patterns encourage networks to extract information from more intuitive observations. iTransformer [24] embeds timestamp features separately into tokens employed by the attention mechanism. This embedding method across time points damaged the physical significance of timestamps.

The processing of timestamps by the previous baselines and our proposed GLAFF can be abstracted as early fusion (feature-level fusion) and late fusion (decision-level fusion). Early fusion integrates modalities into a single representation at the input level and processes the fused representation through the model. Late fusion allows each modality to run independently through its own model and fuses the outputs of each modality. Compared to early fusion, late fusion maximizes the processing effectiveness of each modality and is less susceptible to the noise of a single modality, resulting in greater robustness and reliability. This has been validated by extensive previous work [20; 27; 35].

## 3 Methodology

We propose a model-agnostic and plug-and-play framework, GLAFF, which utilizes global information, represented by timestamps, to enhance the robust prediction capability of mainstream time series forecasting models in real-world scenarios. In multivariate time series forecasting, given the history observations of \(c\) channels within \(h\) time steps \(\mathbf{X}=\{\mathbf{x}_{1},\ldots,\mathbf{x}_{h}\}\in\mathbb{R}^{h\times c}\), we aim to forecast the subsequent \(p\) time steps \(\mathbf{Y}=\{\mathbf{x}_{h+1},\ldots,\mathbf{x}_{h+p}\}\in\mathbb{R}^{p\times c}\). In addition to observations, we incorporate timestamps to provide global information. For each timestamp, we extract its month, day, weekday, hour, minute, and second as timestamp features, respectively. For instance, for the timestamp _2018-06-02 12:00:00_ at moment \(t\), its feature representation is \(\mathbf{s}_{t}=[06,02,05,12,00,00]\in\mathbb{R}^{1\times 6}\). The holiday markers may also be included if accessible. Unlike observations, the timestamp features within the history window \(\mathbf{S}=\{\mathbf{s}_{1},\ldots,\mathbf{s}_{h}\}\in\mathbb{R}^{h\times 6}\) and the timestamp features within the prediction window \(\mathbf{T}=\{\mathbf{s}_{h+1},\ldots,\mathbf{s}_{h+p}\}\in\mathbb{R}^{p\times 6}\) are known. In this section, we describe the detailed workflow of the entire GLAFF framework and explain how it fuses local information \(\mathbf{X}\) and global information \(\mathbf{S},\mathbf{T}\) to predict \(\mathbf{Y}\).

### Overview

GLAFF is a plug-and-play framework that seamlessly collaborates with any time series forecasting backbone. The overall architecture of the plugin is depicted in Figure 2, comprising three primary components: Attention-based Mapper, Robust Denormalizer, and Adaptive Combiner. Following local prediction \(\tilde{\mathbf{Y}}\in\mathbb{R}^{p\times c}\) provided by the backbone network based on history observations \(\mathbf{X}\) (maybe including underutilized history timestamps \(\mathbf{S}\) and future timestamps \(\mathbf{T}\)), GLAFF leverages global information to revise it. Initially, the Attention-based Mapper captures dependencies between timestamps through an attention mechanism, mapping timestamp features \(\mathbf{S}\) and \(\mathbf{T}\) into an initial history mapping \(\tilde{\mathbf{X}}\in\mathbb{R}^{h\times c}\) and an initial future mapping \(\tilde{\mathbf{Y}}\in\mathbb{R}^{p\times c}\), conforming to standard distribution. Subsequently, the Robust Denormalizer inverse normalizes the initial mappings \(\tilde{\mathbf{X}}\) and \(\tilde{\mathbf{Y}}\) to \(\hat{\mathbf{X}}\in\mathbb{R}^{h\times c}\) and \(\hat{\mathbf{Y}}\in\mathbb{R}^{p\times c}\) based on quantile deviation between the initial mapping \(\tilde{\mathbf{X}}\) and the actual observations \(\mathbf{X}\) within the history window, mitigating the impact of data drift. Lastly, the Adaptive Combiner dynamically adjusts the combined weights of the global mapping \(\hat{\mathbf{Y}}\) and the local prediction \(\tilde{\mathbf{Y}}\) within the prediction window according to the disparity between the final mapping \(\hat{\mathbf{X}}\) and the actual observations \(\mathbf{X}\) within the history window, yielding the final prediction outcome \(\mathbf{Y}\). By fusing the robustness of global information and the flexibility of local information, GLAFF significantly enhances the robust prediction capability of mainstream forecasting models.

### Attention-based Mapper

As depicted in the green segment of Figure 2, our proposed Attention-based Mapper employs a simplified encoder-only architecture within the Transformer [34] framework, comprising an embedding layer, attention blocks, and a projection layer. Analogous to typical Transformer-based encoders [37; 45], each timestamp feature is initially tokenized by an embedding layer to describe its properties, applied by self-attention for mutual interactions, and individually processed by feed-forward networks for series representations. Subsequently, a projection layer is utilized to acquire the initial mappings. Leveraging the capability of the attention mechanism for capturing long-range dependencies and parallel computation, the Attention-based Mapper can sufficiently model the global information embodied by timestamps.

Specifically, in Attention-based Mapper, the procedure for obtaining its corresponding initial mapping \(\tilde{\mathbf{X}}\), which conforms to the standard distribution, based on the history timestamps \(\mathbf{S}\), is succinctly delineated as follows:

\[\mathbf{H}^{0} =\mathrm{Embedding}\left(\mathbf{S}\right)\] (1) \[\mathbf{H}^{i+1} =\mathrm{Attention}\left(\mathbf{H}^{i}\right),\;i=0,\cdots,l-1\] \[\tilde{\mathbf{X}} =\mathrm{Projection}\left(\mathbf{H}^{l}\right)\]

Figure 2: The overall architecture of GLAFF mainly consists of three primary components: Attention-based Mapper, Robust Denormalizer, and Adaptive Combiner.

where \(\mathbf{H}^{i}\in\mathbb{R}^{h\times d}\) denotes the intermediate feature variable output from the \(i\)-th attention block, and \(d\) represents the dimension of the intermediate feature variable. The attention blocks are stacked with \(l\) layers to capture the high-level semantic information hidden within the timestamps. To maintain simplicity in implementation, both the embedding and projection layers are comprised of a single linear layer. Following conventional protocol, the primary computation steps for the \(i\)-th attention block are outlined as:

\[\mathbf{H}^{i} =\mathrm{LayerNorm}\left(\mathbf{H}^{i}+\mathrm{MSA}\left( \mathbf{H}^{i},\mathbf{H}^{i},\mathbf{H}^{i}\right)\right)\] (2) \[\mathbf{H}^{i+1} =\mathrm{LayerNorm}\left(\mathbf{H}^{i}+\mathrm{FeedForward} \left(\mathbf{H}^{i}\right)\right)\]

where \(\mathrm{LayerNorm}\left(\cdot\right)\) represents the commonly adopted layer normalization and \(\mathrm{FeedForward}\left(\cdot\right)\) denotes the multilayer feedforward network. The \(\mathrm{MSA}\left(\mathbf{Q},\mathbf{K},\mathbf{V}\right)\) indicates the Multihead Self-Attention mechanism [34], where \(\mathbf{Q},\mathbf{K},\mathbf{V}\) serve as the query, key, and value respectively. Additionally, a dropout mechanism is incorporated to alleviate overfitting and enhance the generalization of the network. The process of obtaining the corresponding initial mapping \(\tilde{\mathbf{Y}}\) based on the future timestamps \(\mathbf{T}\), conforming to the standard distribution, mirrors the aforementioned procedure, simply substituting \(\mathbf{S}\) and \(\tilde{\mathbf{X}}\) in Equation 1 with \(\mathbf{T}\) and \(\tilde{\mathbf{Y}}\) respectively.

### Robust Denormalizer

Due to the inherent variability of the real world, time series observations typically undergo rapid evolution over time, a phenomenon commonly referred to as data drift [17]. This phenomenon can result in discrepancies across different time spans and hinder the generalization ability of deep learning models. Recognizing the presence of data drift, GLAFF employs a two-phase untangling modeling strategy to address the global information represented by timestamps. In the first phase, the network in Attention-based Mapper produces initial mappings, denoted as \(\tilde{\mathbf{X}}\) and \(\tilde{\mathbf{Y}}\), which are assumed to satisfy a standard distribution for reducing the difficulty of modeling the dependencies between timestamps and observations. Subsequently, in the second phase, leveraging the distribution deviations between the initial mapping \(\tilde{\mathbf{X}}\) and the actual observations \(\mathbf{X}\) within the history window, the Robust Denormalizer separately inverse normalizes the initial mappings \(\tilde{\mathbf{X}}\) and \(\tilde{\mathbf{Y}}\) to produce the final mappings \(\tilde{\mathbf{X}}\) and \(\hat{\mathbf{Y}}\), mitigating the impact of data drift.

To alleviate the impact of data drift, a feasible solution [9; 17; 23; 25] has been proposed: removing dynamic factors from the original data through a normalization procedure before feeding them into the deep learning model, and subsequently reintroducing these dynamic factors via an inverse normalization procedure after output from the deep learning model. The conventional inverse normalization procedure typically considers distribution deviations in mean and standard deviation. Nonetheless, this approach is susceptible to extreme values and lacks robustness when the observations contain anomalies. Instead of relying on mean and standard deviation, we employ median and quantile ranges [6], respectively, to enhance the robustness of the Robust Denormalizer against anomalies. As depicted in the yellow segment of Figure 2, the procedure for Robust Denormalizer to inverse normalize initial mappings \(\tilde{\mathbf{X}}\) and \(\tilde{\mathbf{Y}}\) into final mappings \(\tilde{\mathbf{X}}\) and \(\tilde{\mathbf{Y}}\) can be succinctly expressed as:

\[\hat{\mathbf{X}} =\frac{\tilde{\mathbf{X}}-\tilde{\mu}}{\tilde{\sigma}}\times \sigma+\mu\] (3) \[\hat{\mathbf{Y}} =\frac{\tilde{\mathbf{Y}}-\tilde{\mu}}{\tilde{\sigma}}\times \sigma+\mu\]

where \(\tilde{\mu}\in\mathbb{R}^{1\times c}\) and \(\mu\in\mathbb{R}^{1\times c}\) represent the median of the initial mapping \(\tilde{\mathbf{X}}\) and the actual observation \(\mathbf{X}\) for each channel, respectively. Similarly, \(\tilde{\sigma}\in\mathbb{R}^{1\times c}\) and \(\sigma\in\mathbb{R}^{1\times c}\) denote the quantile range (the distance between the \(q\) quantile and the \(1-q\) quantile) of the initial mapping \(\tilde{\mathbf{X}}\) and the actual observation \(\mathbf{X}\) for each channel. Specifically, when \(q=0.75\), \(\tilde{\sigma}\) and \(\sigma\) correspond to the inter-quartile range (IQR3) for each channel of the initial mapping \(\tilde{\mathbf{X}}\) and the actual observation \(\mathbf{X}\).

Footnote 3: IQR is defined as the difference between the 1st and 3rd quartiles of a distribution or set of values, and is a robust measure of the distribution spread.

### Adaptive Combiner

Owing to the intricacies of real-world scenarios, data preferences for model bias will continuously change with online concept drifts [42]. Therefore, we need a data-dependent strategy to change the model selection policy continuously. In other words, the combined weights of global and local information necessitate adaptive and dynamic updates. When the time series pattern exhibits clarity and stability, greater emphasis should be placed on robust global information. Conversely, increased attention should be directed towards flexible local information when the time series pattern appears ambiguous and variable. Within the framework of GLAFF, we employ an Adaptive Combiner to realize the adaptive adjustment of combined weights.

As illustrated in the red segment of Figure 2, the Adaptive Combiner initially dynamically adjusts the combined weights of the global mapping \(\hat{\mathbf{Y}}\) and the local prediction \(\bar{\mathbf{Y}}\) within the prediction window, based on the deviation between the final mapping \(\hat{\mathbf{X}}\) and the actual observation \(\mathbf{X}\) within the history window. Subsequently, we aggregate the dual-source information based on the combined weights to yield the final prediction \(\mathbf{Y}\). Specifically, the primary computation procedure of the Adaptive Combiner is represented as:

\[\mathbf{W} =\mathrm{MLP}\left(\hat{\mathbf{X}}-\mathbf{X}\right)\] (4) \[\mathbf{Y} =\sum\mathbf{W}\times\left(\hat{\mathbf{Y}}\oplus\bar{\mathbf{Y }}\right)\]

where \(\mathbf{W}\in\mathbb{R}^{1\times c\times 2}\) signifies the dynamically generated combined weight by the network based on the deviation between the final mapping \(\hat{\mathbf{X}}\) and the actual observation \(\mathbf{X}\) within the history window. The \(\oplus\) denotes the concatenation operation based on the additional last dimension, and \(\sum\) denotes the summation operation performed across the last dimension. For simplicity, the weight generation network consists solely of a Multilayer Perceptron (MLP) containing a hidden layer and a layer of \(\mathrm{Softmax}\) for weight normalization.

Through adaptive adjustment of combined weights, our method can effectively fuses the robustness of global information and the flexibility of local information, thereby enhancing its suitability for intricate and fluctuating real-world scenarios.

## 4 Experiment

### Experimental Setup

DatasetWe conduct extensive experiments on nine datasets across five domains, including Electricity, Exchange, Traffic, Weather, and ILI, along with four ETT datasets. Detailed dataset information is provided in Appendix A.1. We follow the standard segmentation protocol [24; 37; 45], strictly dividing each dataset into training, validation, and testing sets chronologically to ensure no information leakage issues. The segmentation ratio for each dataset is set to 6:2:2. Regarding prediction settings, we also adhere to established mainstream protocols [26; 38; 41]. Specifically, we set the length of the history window to 96 for the Electricity, Exchange, Traffic, Weather, and four ETT datasets, while the prediction length varies within {96, 192, 336, 720}. For the ILI, which has fewer time points, the length of the history window is fixed at 36, and the prediction length varies within {24, 36, 48, 60}.

BackboneTo demonstrate the effectiveness of the framework, we select several mainstream forecasting models based on different architectures, including the Transformer-based Informer (2021) [45] and iTransformer (2024) [24], the Linear-based DLinear (2023) [41], and the Convolution-based TimesNet (2023) [38]. Notably, iTransformer represents the previous state-of-the-art method in time series forecasting tasks. Further details regarding the backbone models are provided in Appendix A.2. As described in Section 2, these backbones encompass three different treatments for timestamps employed in prior forecasting techniques, namely summation (Informer, TimesNet), concatenation (iTransformer), and omission (DLinear).

The details of experimental setup can be found in Appendix A.3. All experiments are based on our runs, utilizing the same hardware configurations, and repeated 3 times with different random seeds.

### Main Result

Table 1 compares the prediction outcomes for mainstream baselines and GLAFF. We present a detailed version of this table in Appendix B.1. The results indicate that GLAFF significantly surpasses all four widely used mainstream baselines across all nine real-world benchmark datasets. In particular, GLAFF enhances the respective backbones by an average of 12.5%.

By fusing the robustness of global information with the flexibility of local information, GLAFF can significantly improve the robust prediction capability of mainstream forecasting models. Specifically, in the case of DLinear, a Linear-based model that entirely disregards timestamps, GLAFF enhances its prediction accuracy by 13.1%. For Transformer-based Informer and Convolution-based TimesNet utilizing simple timestamp summation, GLAFF yields performance improvements of 23.8% and 7.5%, respectively. In the case of Transformer-based iTransformer employing direct timestamp concatenation, GLAFF still produces a 5.5% improvement in accuracy. Additionally, we note a diminishing boosting effect of GLAFF as the modeling prowess of the backbone increases, indicative of the complementary nature of global and local information. Nonetheless, for the state-of-the-art iTransformer, GLAFF continues to offer substantial benefits.

It is evident that the enhancement of GLAFF varies across datasets with different characteristics. For datasets such as Traffic and Electricity, characterized by a significant number of channels and clear periodic patterns, GLAFF demonstrates superior capability in capturing the dependencies between timestamps and observations, resulting in performance enhancements of 19.5% and 15.7%, respectively. For the non-stationary datasets [25], such as ETTh2, ETTm2, and Exchange, the Robust Denormalizer effectively alleviates the impact of data drift, thereby augmenting prediction accuracy by 12.1%, 12.1%, and 16.4%, respectively. Regarding common datasets like ETTh1, ETTm1, Weather, and ILI, although the performance of GLAFF may not be as remarkable, it still yields improvements of 8.8%, 8.1%, 9.6%, and 9.6%, respectively.

Given the burgeoning interest in leveraging LLMs for time series, we assess the performance augmentation of GLAFF when applied to LLM-based backbones. Specifically, we employ the widely recognized FPT (2023) [47] as our baseline. FPT completely disregards timestamps similar to DLinear. We deploy two structures, GPT2(3) and GPT2(6), of FPT as outlined in their paper. The ILI dataset has a history window length of 36 and a prediction window length of 48, while other datasets have a history window length of 96 and a prediction window length of 192. As delineated in the findings presented in Table 2, GLAFF also offer significant benefits to the LLM-based baselines.

\begin{table}
\begin{tabular}{c|c c c c c|c c c c|c c c c|c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{2}{c|}{Informer} & \multicolumn{2}{c|}{**+ Ours**} & \multicolumn{2}{c|}{DLinear} & \multicolumn{2}{c|}{**+ Ours**} & \multicolumn{2}{c|}{TimesNet} & \multicolumn{2}{c|}{**+ Ours**} & \multicolumn{2}{c|}{Transformer} & \multicolumn{2}{c|}{**+ Ours**} & \multicolumn{2}{c|}{**Imper**} \\  & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\ \hline \multirow{12}{*}{} & 96 & 0.333 & 0.414 & **0.217** & **0.323** & 0.196 & 0.283 & **0.147** & **0.238** & 0.175 & 0.280 & **0.154** & **0.248** & 0.153 & 0.246 & **0.120** & **0.198** \\  & 192 & 0.362 & 0.404 & **0.220** & 0.196 & 0.286 & **0.172** & **0.253** & 0.199 & 0.293 & **0.190** & 0.269 & 0.167 & 0.259 & **0.143** & **0.216** \\  & 336 & 0.352 & 0.434 & **0.230** & 0.373 & 0.208 & 0.301 & **0.197** & **0.274** & 0.211 & 0.310 & **0.185** & **0.276** & 0.138 & 0.276 & **0.168** & **0.240** \\  & 720 & 0.364 & 0.432 & **0.247** & 0.351 & 0.239 & 0.331 & **0.239** & **0.380** & 0.235 & 0.236 & **0.236** & **0.303** & 0.220 & 0.310 & **0.279** & **0.279** \\ \hline \multirow{12}{*}{} & 96 & 0.926 & 0.736 & **0.609** & **0.560** & 0.409 & 0.440 & **0.391** & **0.418** & 0.453 & 0.481 & **0.435** & **0.464** & 0.420 & 0.454 & **0.411** & **0.441** \\  & 192 & 1.235 & 0.844 & **0.831** & **0.680** & 0.457 & 0.475 & **0.446** & **0.457** & 0.533 & 0.531 & **0.520** & **0.517** & 0.494 & 0.502 & **0.474** & **0.482** & 8.8 \\  & 336 & 1.354 & 0.835 & **0.838** & **0.698** & 0.500 & 0.500 & **0.492** & **0.492** & 0.082 & 0.621 & 0.580 & **0.596** & **0.566** & 0.538 & 0.528 & **0.534** & **0.519** \\  & 720 & 1.264 & 0.857 & **0.937** & **0.730** & 0.610 & 0.576 & **0.609** & **0.556** & 0.844 & 0.697 & **0.773** & **0.661** & 0.716 & 0.629 & **0.704** & **0.615** \\ \hline \multirow{12}{*}{} & 96 & 0.708 & 0.549 & **0.422** & **0.443** & 0.159 & 0.278 & **0.128** & **0.205** & 0.183 & 0.298 & **0.174** & **0.276** & 0.177 & 0.287 & **0.172** & **0.271** \\  & 192 & 1.133 & 0.688 & **0.680** & **0.599** & 0.870 & 0.369 & **0.165** & **0.238** & 0.218 & 0.292 & **0.240** & **0.306** & 0.199 & 0.311 & **0.196** & **0.295** \\  & 336 & 0.997 & 0.667 & **0.477** & **0.570** & 0.207 & 0.330 & **0.206** & **0.265** & 0.240 & 0.346 & **0.219** & **0.319** & 0.220 & 0.329 & **0.231** & **0.336** & **0.290** \\  & 720 & 1.607 & 0.815 & **1.255** & **0.729** & 0.262 & 0.738 & **0.214** & **0.288** & 0.281 & 0.376 & **0.287** & 0.343 & 0.271 & 0.366 & **0.270** & **0.356** \\ \hline \multirow{12}{*}{} & 96 & 0.953 & 0.548 & **0.503** & **0.486** & 0.339 & 0.388 & **0.309** & **0.353** & 0.449 & 0.448 & **0.381** & **0.398** & 0.383 & 0.415 & **0.349** & **0.386** \\  & 192 & 0.611 & 0.576 & **0.534** & **0.525** & 0.394 & 0.418 & **0.362** & **0.386** & 0.448 & 0.461 & **0.440** & **0.440** & **0.432** & 0.429 & 0.445 & **0.403** & **0.420** \\  & 336 & 0.888 & 0.726 & **0.707** & **0.628** & 0.450 & 0.450 & 0.414Additionally, to validate the practical applicability of GLAFF, we assess the computation costs in Appendix B.4. The results indicate that GLAFF has little effect on model training and deployment across most scenarios, particularly when considering its significant accuracy enhancement.

### Prediction Showcase

In addition to evaluation metrics, forecasting quality is crucial. To further compare GLAFF and the four mainstream forecasting models, we illustrate prediction showcases for two representative datasets in Figure 3. We provide the full prediction showcases for the nine datasets in Appendix B.5. It is evident that GLAFF can yield more realistically robust predictions. At the same time, the respective backbones are susceptible to abnormal local information.

The Traffic dataset records traffic volume in hourly granularity. Typically, this sequence exhibits a clear periodic pattern, alternating with five high peaks (weekdays) and two low peaks (weekends). However, owing to a holiday, the initial two days within the example history window do not exhibit the high peaks as usual. Due to the limited local information containing such contextual anomalies, Informer, DLinear, and iTransformer all think the prediction window should also consist of only low peaks. Although TimesNet generates predictions with high peaks, it displays an incorrect alternation between five high peaks and one low peak. By introducing sufficiently modeled global information, GLAFF has enabled the four mainstream forecasting backbones to recognize the existence of high peaks and the correct periodic patterns, thus yielding more accurate forecasts.

The Electricity dataset records electricity consumption in hourly granularity. Typically, this sequence exhibits a clear periodic pattern, alternating with five peaks (weekdays) and two flat segments (weekends). However, owing to a short circuit, the middle two days within the example history window show a spike in electricity consumption. Due to the limited local information containing such point anomalies, DLinear, TimesNet, and iTransformer all think the flat segments in the prediction window should also contain a spike. Although Informer generates predictions without spikes, it completely ignores the presence of flat segments. By introducing sufficiently modeled global information, GLAFF has enabled the four mainstream forecasting backbones to recognize the contingency of spikes and the correct periodic patterns, thus yielding more robust predictions.

\begin{table}
\begin{tabular}{l|c c c|c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{2}{c}{GPT2(3)} & \multicolumn{2}{c|}{**+Ours**} & \multicolumn{2}{c}{GPT2(6)} & \multicolumn{2}{c}{**+Ours**} \\  & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\ \hline Electricity & 0.194\(\pm\)0.002 & 0.278\(\pm\)0.002 & **0.168\(\pm\)0.005** & **0.28\(\pm\)0.002** & 0.194\(\pm\)0.001 & 0.279\(\pm\)0.002 & **0.17\(\pm\)0.002** & **0.258\(\pm\)0.001** \\ ETH1 & 0.466\(\pm\)0.001 & 0.483\(\pm\)0.001 & **0.454\(\pm\)0.003** & **0.462\(\pm\)0.003** & 0.468\(\pm\)0.002 & 0.438\(\pm\)0.002 & **0.445\(\pm\)0.002** & **0.451\(\pm\)0.002** \\ ETH2 & 0.190\(\pm\)0.000 & 0.304\(\pm\)0.000 & **0.178\(\pm\)0.017** & **0.284\(\pm\)0.010** & 0.190\(\pm\)0.002 & 0.303\(\pm\)0.002 & **0.177\(\pm\)0.005** & **0.286\(\pm\)0.003** \\ ETH1 & 0.411\(\pm\)0.005 & 0.431\(\pm\)0.003 & **0.388\(\pm\)0.009** & **0.409\(\pm\)0.005** & 0.412\(\pm\)0.002 & 0.431\(\pm\)0.002 & **0.390\(\pm\)0.012** & **0.413\(\pm\)0.007** \\ ETH2 & 0.142\(\pm\)0.001 & 0.257\(\pm\)0.001 & **0.120\(\pm\)0.001** & 0.235\(\pm\)0.002 & 0.141\(\pm\)0.002 & 0.256\(\pm\)0.002 & **0.119\(\pm\)0.002** & **0.233\(\pm\)0.002** \\ Exchange & 0.110\(\pm\)0.001 & 0.238\(\pm\)0.002 & **0.090\(\pm\)0.002** & **0.219\(\pm\)0.002** & 0.106\(\pm\)0.001 & 0.234\(\pm\)0.001 & **0.088\(\pm\)0.001** & **0.216\(\pm\)0.001** \\ IL & 1.585\(\pm\)0.001 & 0.900\(\pm\)0.019 & **1.393\(\pm\)0.024** & **0.782\(\pm\)0.023** & 1.494\(\pm\)0.012 & 0.854\(\pm\)0.010 & **1.396\(\pm\)0.026** & **0.778\(\pm\)0.018** \\ Traffic & 0.370\(\pm\)0.002 & 0.309\(\pm\)0.002 & **0.296\(\pm\)0.002** & **0.256\(\pm\)0.001** & 0.371\(\pm\)0.002 & 0.312\(\pm\)0.001 & **0.301\(\pm\)0.003** & **0.262\(\pm\)0.002** \\ Weather & 0.241\(\pm\)0.000 & 0.276\(\pm\)0.000 & **0.234\(\pm\)0.004** & **0.268\(\pm\)0.003** & 0.243\(\pm\)0.001 & 0.278\(\pm\)0.001 & **0.228\(\pm\)0.002** & **0.264\(\pm\)0.002** \\ \hline \hline \end{tabular}
\end{table}
Table 2: The forecasting errors for multivariate time series among GLAFF and LLM-based baselines. A lower outcome indicates a better prediction. The best results are highlighted in bold.

Figure 3: The illustration of prediction showcases among GLAFF and mainstream baselines.

### Ablation Study

We provide a comprehensive ablation study to validate the necessity of the GLAFF components. We implement our approach and its four variants on the iTransformer backbone. The results of our experiments on three representative benchmark datasets are presented in Table 3. Detailed results for the nine real-world benchmark datasets are available in Appendix B.2.

In w/o Backbone, we completely remove the backbone network within the GLAFF and map the future using only timestamps. Surprisingly, GLAFF still demonstrates favorable prediction performance without any observations. The average prediction accuracy of GLAFF even outperforms Informer and DLinear, and is also competitive with TimesNet and iTransformer. Global information proves adequate in scenarios featuring clear periodic and stable distributions.

In w/o Attention, we substitute the stacked attention blocks with MLP networks having the equivalent size. Following the replacement of attention blocks, GLAFF fails to capture the dependencies among timestamps adequately. It proves challenging to map out precise observations solely from a single timestamp. Particularly notable is the most marked decline in performance on the Traffic dataset, which has the largest number of channels, indicating the greatest modeling challenge for GLAFF.

In w/o Quantile, we replace the Robust Denormalizer with conventional inverse normalization. The experimental results illustrate that our design yields enhancements across all three datasets, particularly in the Electricity dataset. When the history window encompasses anomalies, conventional inverse normalization yields inaccurate estimates for distribution. Leveraging more robust quantiles, our Robust Denormalizer demonstrates enhanced robustness in mitigating the impacts of data drift.

In w/o Adaptive, we substitute the Adaptive Combiner with a straightforward averaging for global mapping and local prediction. We distinctly find that dynamically adjusting the combined weights can prove more efficacious in accommodating fluctuating real-world scenarios, particularly evident in the non-stationary Weather dataset. By fusing global and local information adaptively, GLAFF can seamlessly collaborate with any time series forecasting backbone.

## 5 Conclusion

In this work, our focus lies in leveraging global information, as denoted by timestamps, to enhance the robust prediction capability of time series forecasting models in the real world. We introduce a new approach named GLAFF, serving as a model-agnostic and plug-and-play framework. Within this framework, the timestamps are modeled individually to capture the global dependencies. Through adaptive adjustment of combined weights for global and local information, GLAFF facilitates seamless collaboration with any time series forecasting backbone. To substantiate the superiority of our approach, we have conducted comprehensive experiments on widely used benchmark datasets, demonstrating the substantial enhancement GLAFF provides to mainstream forecasting models. We hope that GLAFF can be used as a foundational component for time series forecasting and call on the community to give more attention to global information represented by timestamps.

\begin{table}
\begin{tabular}{c c|c c c|c c|c c|c c|c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{3}{c|}{iTransformer} & \multicolumn{3}{c|}{**+ Ours**} & \multicolumn{3}{c|}{w/o Backbone} & \multicolumn{3}{c|}{w/o Attention} & \multicolumn{3}{c}{w/o Quantile} & \multicolumn{3}{c}{w/o Adaptive} \\  & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\ \hline \multirow{3}{*}{\begin{tabular}{c} \end{tabular} } & 96 & 0.1525 & 0.2460 & **0.1197** & **0.1979** & 0.2058 & 0.2663 & 0.1518 & 0.2450 & 0.1467 & 0.2465 & 0.1574 & 0.2502 \\  & 192 & 0.1674 & 0.2593 & **0.1434** & **0.2157** & 0.2097 & 0.2793 & 0.1684 & 0.2610 & 0.1677 & 0.2662 & 0.1740 & 0.2662 \\  & 336 & 0.1830 & 0.2762 & **0.1683** & **0.2395** & 0.2454 & 0.3014 & 0.1832 & 0.2775 & 0.1993 & 0.2954 & 0.1953 & 0.2877 \\  & 720 & 0.2199 & 0.3097 & **0.2169** & **0.2786** & 0.2984 & 0.3386 & 0.2182 & 0.3092 & 0.2593 & 0.3403 & 0.2330 & 0.3171 \\ \hline \multirow{3}{*}{\begin{tabular}{c} \end{tabular} } & 96 & 0.3084 & 0.2717 & **0.2828** & **0.2485** & 0.3348 & 0.2723 & 0.3172 & 0.2806 & 0.2909 & 0.2684 & 0.2930 & 0.2612 \\  & 192 & 0.3267 & 0.2794 & **0.2909** & **0.2528** & 0.3387 & 0.2736 & 0.3357 & 0.2884 & 0.2948 & 0.2737 & 0.2970 & 0.2610 \\  & 336 & 0.3381 & 0.2850 & **0.3095** & **0.2594** & 0.3460 & 0.2794 & 0.3482 & 0.2958 & 0.3023 & 0.2804 & 0.3082 & 0.2706 \\  & 720 & 0.3574 & 0.3015 & **0.3201** & **0.2730** & 0.3558 & 0.2906 & 0.3684 & 0.3113 & 0.3249 & 0.2984 & 0.3212 & 0.2819 \\ \hline \multirow{3}{*}{
\begin{tabular}{c} \end{tabular} } & 96 & 0.1784 & 0.2229 & **0.1587** & **0.2199** & 0.2382 & 0.2695 & 0.1780 & 0.2214 & 0.1811 & 0.2270 & 0.1914 & 0.2379 \\  & 192 & 0.2308 & 0.2675 & **0.2138** & **0.2654** & 0.2882 & 0.3105 & 0.2383 & 0.2733 & 0.2364 & 0.2768 & 0.2489 & 0.2832 \\ \cline{1-1}  & 336 & 0.2892 & 0.3099 & **0.2733** & **0.3058** & 0.3381 & 0.3414 & 0.2932 & 0.3146 & 0.2905 & 0.3134 & 0.3070 & 0.3251 \\ \cline{1-1}  & 720 & 0.3701 & 0.3634 & **0.3520** & **0.3547** & 0.4011 & 0.3813 & 0.3752 & 0.3664 & 0.3727 & 0.3649 & 0.3829 & 0.3722 \\ \hline \multicolumn{3}{c|}{Avg.} & 0.2602 & 0.2827 & **0.2367** & **0.2593** & 0.3000 & 0.3004 & 0.2646 & 0.2870 & 0.2555 & 0.2876 & 0.2591 & 0.2845 \\ \hline \hline \end{tabular}
\end{table}
Table 3: The forecasting errors for multivariate time series of ablation study among GLAFF and variants. A lower outcome indicates a better prediction. The best results are highlighted in bold.

## Acknowledgments and Disclosure of Funding

This work was supported by the National Natural Science Foundation of China under Grants (U23B2001, 62171057, 62101064, 62201072, 62001054, 62071067), the Ministry of Education and China Mobile Joint Fund (MCM20200202, MCM20180101), Beijing University of Posts and Telecommunications-China Mobile Research Institute Joint Innovation Center, China Postdoctoral Science Foundation (2023TQ0039), National Postdoctoral Program for Innovative Talents under Grant BX20230052, and the BUPT Excellent Ph.D. Students Foundation (CX20241016).

## References

* [1]A. Ariyo Ariyo, A. Oluyinka Adewumi, and C. K. Ayo (2014) Stock price prediction using the ARIMA model. In International Conference on Computer Modelling and Simulation, pp. 106-112. Cited by: SS1.
* [2]G. E. P. Box and G. M. Jenkins (1968) Some recent advances in forecasting and control. Journal of the Royal Statistical Society17, pp. 91-109. Cited by: SS1.
* [3]C. Chang, W. Peng, and T. Chen (2023) LLM4TS: two-stage fine-tuning for time-series forecasting with pre-trained llms. arXiv2308.08469. Cited by: SS1.
* [4]Y. Chen, I. Segovia-Dominguez, B. Coskunuzer, and Y. R. Gel (2022) Tamp-s2gcnets: coupling time-aware multipersistence knowledge representation with spatio-supra graph convolutional networks for time-series forecasting. In International Conference on Learning Representations, Cited by: SS1.
* [5]H. Cheng, Q. Wen, Y. Liu, and L. Sun (2024) RobustfsF: towards theory and design of robust time series forecasting with anomalies. In International Conference on Learning Representations, Cited by: SS1.
* [6]A. Deng and B. Hooi (2021) Graph neural network-based anomaly detection in multivariate time series. In AAAI Conference on Artificial Intelligence, pp. 4027-4035. Cited by: SS1.
* [7]J. Dong, H. Wu, H. Zhang, L. Zhang, J. Wang, and M. Long (2023) Simmtm: a simple pre-training framework for masked time-series modeling. In Annual Conference on Neural Information Processing Systems, Cited by: SS1.
* [8]S. Du, T. Li, Y. Yang, and S. Horng (2021) Deep air quality forecasting using hybrid deep learning framework. IEEE Transactions on Knowledge and Data Engineering33, pp. 2412-2424. Cited by: SS1.
* [9]W. Fan, P. Wang, D. Wang, D. Wang, Y. Zhou, and Y. Fu (2023) Dish-ts: a general paradigm for alleviating distribution shift in time series forecasting. In AAAI Conference on Artificial Intelligence, pp. 7522-7529. Cited by: SS1.
* [10]V. Flunkert, D. Salinas, and J. Gasthaus (2017) Deepar: probabilistic forecasting with autoregressive recurrent networks. arXiv2201.00382. Cited by: SS1.
* [11]H. He, Q. Zhang, S. Bai, K. Yi, and Z. Niu (2022) CATN: cross attentive tree-aware network for multivariate time series forecasting. In AAAI Conference on Artificial Intelligence, pp. 4030-4038. Cited by: SS1.
* [12]Q. He, S. W. In Siu, and Y. Si (2023) Instance-based deep transfer learning with attention for stock movement prediction. Applied Intelligence53, pp. 6887-6908. Cited by: SS1.
* [13]S. Hochreiter and J. Schmidhuber (1997) Long short-term memory. Neural Computation9, pp. 1735-1780. Cited by: SS1.
* [14]R. J. Hyndman and G. Athanasopoulos (2018) Forecasting: principles and practice. OTexts. Cited by: SS1.
* [15]Y. Jia, Y. Lin, X. Hao, Y. Lin, S. Guo, and H. Wan (2023) WITRAN: water-wave information transmission and recurrent acceleration network for long-range time series forecasting. In Annual Conference on Neural Information Processing Systems, Cited by: SS1.
* [16]Y. Jia, Y. Lin, X. Hao, Y. Lin, S. Guo, and H. Wan (2023) WITRAN: water-wave information transmission and recurrent acceleration network for long-range time series forecasting. In Annual Conference on Neural Information Processing Systems, Cited by: SS1.
* [17]Y. Jia, Y. Lin, X. Hao, Y. Lin, S. Guo, and H. Wan (2023) WITRAN: water-wave information transmission and recurrent acceleration network for long-range time series forecasting. In Annual Conference on Neural Information Processing Systems, Cited by: SS1.
* [18]Y. Jia, Y. Lin, X. Hao, Y. Lin, S. Guo, and H. Wan (2023) WITRAN: water-wave information transmission and recurrent acceleration network for long-range time series forecasting. In Annual Conference on Neural Information Processing Systems, Cited by: SS1.
* [19]Y. Jia, Y. Lin, X. Hao, Y. Lin, S. Guo, and H. Wan (2023) WITRAN: water-wave information transmission and recurrent acceleration network for long-range time series forecasting. In Annual Conference on Neural Information Processing Systems, Cited by: SS1.
* [20]Y. Jia, Y. Lin, X. Hao, Y. Lin, S. Guo, and H. Wan (2023) WITRAN: water-wave information transmission and recurrent acceleration network for long-range time series forecasting. In Annual Conference on Neural Information Processing Systems, Cited by: SS1.
* [21]Y. Jia, Y. Lin, X. Hao, Y. Lin, S. Guo, and H. Wan (2023) WITRAN: water-wave information transmission and recurrent acceleration network for long-range time series forecasting. In Annual Conference on Neural Information Processing Systems, Cited by: SS1.
* [22]Y. Jia, Y. Lin, X. Hao, Y. Lin, S. Guo, and H. Wan (2023) WITRAN: water-wave information transmission and recurrent acceleration network for long-range time series forecasting. In Annual Conference on Neural Information Processing Systems, Cited by: SS1.
* [23]Y. Jia, Y. Lin, X. Hao, Y. Lin, S. Guo, and H. Wan (2023) WITRAN: water-wave information transmission and recurrent acceleration network for long-range time series forecasting. In Annual Conference on Neural Information Processing Systems, Cited by: SS1.
* [24]Y. Jia, Y. Lin, X. Hao, Y. Lin, S. Guo, and H. Wan (2023) WITRAN: water-wave information transmission and recurrent acceleration network for long-range time series forecasting. In Annual Conference on Neural Information Processing Systems, Cited by: SS1.
* [25]Y. Jia, Y. Lin, X. Hao, Y. Lin, S. Guo, and H. Wan (2023) WITRAN: water-wave information transmission and recurrent acceleration network for long-range time series forecasting. In Annual Conference on Neural Information Processing Systems, Cited by: SS1.
* [26]Y. Jia, Y. Lin, X. Hao, Y. Lin, S. Guo, and H. Wan (2023) WITRAN: water-wave information transmission and recurrent acceleration network for long-range time series forecasting. In Annual Conference on Neural Information Processing Systems, Cited by: SS1.
* [27]Y. Jia, Y. Lin, X. Hao, Y. Lin, S. Guo, and H. Wan (2023) WITRAN: water-wave information transmission and recurrent acceleration network for long-range time series forecasting. In Annual Conference on Neural Information Processing Systems, Cited by: SS1.
* [28]Y. Jia, Y. Lin, X. Hao, Y. Lin, S. Guo, and H. Wan (2023) WITRAN: water-wave information transmission and recurrent acceleration network for long-range time series forecasting. In Annual Conference on Neural Information Processing Systems, Cited by: SS1.
* [29]Y. Jia, Y. Lin, X. Hao, Y. Lin, S. Guo, and H. Wan (2023) WITRAN: water-wave information transmission and recurrent acceleration network for long-range time series forecasting. In Annual Conference on Neural Information Processing Systems, Cited by: SS1.
* [30]Y. Jia, Y. Lin, X. Hao, Y. Lin, S. Guo, and H. Wan (2023) WITRAN: water-wave information transmission and recurrent acceleration network for long-range time series forecasting. In Annual Conference on Neural Information Processing Systems, Cited by: SS1.
* [31]Y. Jia, Y. Lin, S. Guo, and H. Wan (2023) WITRAN: water-wave information transmission and recurrent acceleration network for long-range time series forecasting. In Annual Conference on Neural Information Processing Systems, Cited by: SS1.
* [32]Y. Jia, Y. Lin, X. Hao, Y. Lin, S. Guo, and H. Wan (2023) WITRAN: water-wave information transmission and recurrent acceleration network for long-range time series forecasting. In Annual Conference on Neural Information Processing Systems, Cited by: SS1.
* [33]Y. Jia, Y. Lin, X. Hao, Y. Lin, S. Guo, and H. Wan (2023) WITRAN: water-wave information transmission and recurrent acceleration network for long-range time series forecasting. In Annual Conference on Neural Information Processing Systems, Cited by: SS1.
* [34]Y. Jia, Y. Lin, X. Hao, Y. Lin, S. Guo, and H. Wan (2023) WITRAN: water-wave information transmission and recurrent acceleration network for long-range time series forecasting. In Annual Conference on Neural Information Processing Systems, Cited by: SS1.
* [35]Y. Jia, Y. Lin, S. Guo, and H. Wan (2023) WITRAN: water-wave information transmission and recurrent acceleration network for long-range time series forecasting. In Annual Conference on Neural Information Processing Systems, Cited by: SS1.
* [36]Y. Jia, Y. Lin, X. Hao, Y. Lin, S. Guo, and H. Wan (2023) WITRAN: water-wave information transmission and recurrent acceleration network for long-range time series forecasting. In Annual Conference on Neural Information Processing Systems, Cited by: SS1.
* [37]Y. Jia, Y. Lin, X. Hao, Y. Lin, S. Guo, and H. Wan (2023) WITRAN: water-wave information transmission and recurrent acceleration network for long-range time series forecasting. In Annual Conference on Neural Information Processing Systems, Cited by: SS1.
* [38]Y. Jia, Y. Lin, X. Hao, Y. Lin, S. Guo, and H. Wan (2023) WITRAN: water-wave information transmission and recurrent acceleration network for long-range time series forecasting. In Annual Conference on Neural Information Processing Systems, Cited by: SS1.
* [39]Y. Jia, Y. Lin, X. Hao, Y. Lin, S. Guo, and H. Wan (2023) WITRAN: water-wave information transmission and recurrent acceleration network for long-range time series forecasting. In Annual Conference on Neural Information Processing Systems, Cited by: SS1.
* [40]Y. Jia, Y. Lin, X. Hao, Y. Lin, S. Guo, and H. Wan (2023) WITRAN: water-wave information transmission and recurrent acceleration network for long-range time series forecasting. In Annual Conference on Neural Information Processing Systems, Cited by: SS1.
* [41]Y. Jia, Y. Lin, X. Hao, Y. Lin, S. Guo, and H. Wan (2023) WITRAN: water-wave information transmission and recurrent acceleration network for long-range time series forecasting. In Annual Conference on Neural Information Processing Systems, Cited by: SS1.
* [42]Y. Jia, Y. Lin, X. Hao, Y. Lin, S. Guo, and H. Wan (2023) WITRAN: water-wave information transmission and recurrent acceleration network for long-range time series forecasting. In Annual Conference on Neural Information Processing Systems, Cited by: SS1.
* [43]Y. Jia, Y. Lin, X. Hao, Y. Lin, S. Guo, and H. Wan (2023) WITRAN: water-wave information transmission and recurrent acceleration network for long-range time series forecasting. In Annual Conference on Neural Information Processing Systems, Cited by: SS1.
* [44]Y. Jia, Y. Lin, X. Hao, Y. Lin, S. Guo, and H. Wan (2023) WITRAN: water-wave information transmission and recurrent acceleration network for long-range time series forecasting. In Annual Conference on Neural Information Processing Systems, Cited by: SS1.
* [45]Y. Jia, Y. Lin, X. Hao, Y. Lin, S. Guo, and H. Wan (2023) WITRAN: water-wave information transmission and recurrent acceleration network for long-range time series forecasting. In Annual Conference on Neural Information Processing Systems, Cited by: SS1.
* [46]Y. Jia, Y. Lin, X. Hao, Y. Lin, S. Guo, and H. Wan (2023) WITRAN: water-wave information transmission and recurrent acceleration network for long-range time series forecasting. In Annual Conference on Neural Information Processing Systems, Cited by: SS1.
* [47]Y. Jia, Y. Lin, X. Hao, Y. Lin, S. Guo, and H. Wan (2023) WITRAN: water-wave information transmission and recurrent acceleration network for long-range time series forecasting. In Annual Conference on Neural Information Processing Systems, Cited by: SS1.
* [48]Y. Jia, Y. Lin, X. Hao, Y. Lin, S. Guo, and H. Wan (2023) WITRAN: water-wave information transmission and recurrent acceleration network for long-range time series forecasting. In Annual Conference on* [16] Shruti Kaushik, Abhinav Choudhury, Pankaj Kumar Sheron, Nataraj Dasgupta, Sayee Natarajan, Larry A. Pickett, and Varun Dutt. AI in healthcare: Time-series forecasting using statistical, neural, and ensemble architectures. _Frontiers Big Data_, 3:4, 2020.
* [17] Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo. Reversible instance normalization for accurate time-series forecasting against distribution shift. In _International Conference on Learning Representations_, 2022.
* [18] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In _International Conference on Learning Representations_, 2020.
* [19] Shiyang Li, Xiaoyong Jin, Yao Xuan, Siyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. In _Annual Conference on Neural Information Processing Systems_, pages 5244-5254, 2019.
* [20] Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Foundations & trends in multimodal machine learning: Principles, challenges, and open questions. _ACM Computing Surveys_, 56:264, 2024.
* [21] Minhao Liu, Ailing Zeng, Muxi Chen, Zhijian Xu, Qiuxia Lai, Lingna Ma, and Qiang Xu. Scinet: Time series modeling and forecasting with sample convolution and interaction. In _Annual Conference on Neural Information Processing Systems_, 2022.
* [22] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X. Liu, and Schahram Dustdar. Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting. In _International Conference on Learning Representations_, 2022.
* [23] Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Non-stationary transformers: Exploring the stationarity in time series forecasting. In _Annual Conference on Neural Information Processing Systems_, 2022.
* [24] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long. itransformer: Inverted transformers are effective for time series forecasting. In _International Conference on Learning Representations_, 2024.
* [25] Zhiding Liu, Mingyue Cheng, Zhi Li, Zhenya Huang, Qi Liu, Yanhu Xie, and Enhong Chen. Adaptive normalization for non-stationary time series forecasting: A temporal slice perspective. In _Annual Conference on Neural Information Processing Systems_, 2023.
* [26] Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. In _International Conference on Learning Representations_, 2023.
* [27] Luis Manuel Pereira, Addison Salazar, and Luis Vergara. A comparative analysis of early and late fusion for the multimodal two-class problem. _IEEE Access_, 11:84283-84300, 2023.
* [28] Tiago Pinto, Isabel Praca, Zita A. Vale, and Jose Silva. Ensemble learning for electricity consumption forecasting in office buildings. _Neurocomputing_, 423:747-755, 2021.
* [29] Chetanya Puri, Gerben Kooijman, Bart Vanrumste, and Stijn Luca. Forecasting time series in healthcare with gaussian processes and dynamic time warping based subset selection. _IEEE Journal of Biomedical and Health Informatics_, 26:6126-6137, 2022.
* [30] Syama Sundar Rangapuram, Matthias W. Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, and Tim Januschowski. Deep state space models for time series forecasting. In _Annual Conference on Neural Information Processing Systems_, pages 7796-7805, 2018.
* [31] Tarik S. Salem, Karan Kathuria, Heri Ramampiaro, and Helge Langseth. Forecasting intra-hour imbalances in electric power systems. In _AAAI Conference on Artificial Intelligence_, pages 9595-9600, 2019.
* [32] Chenxi Sun, Yaliang Li, Hongyan Li, and Shenda Hong. TEST: text prototype aligned embedding to activate llm's ability for time series. In _International Conference on Learning Representations_, 2024.

* [33] Sean J. Taylor and Benjamin Letham. Forecasting at scale. _PeerJ PrePrints_, 5:e3190, 2017.
* [34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Annual Conference on Neural Information Processing Systems_, pages 5998-6008, 2017.
* [35] Chengsen Wang, Zirui Zhuang, Qi Qi, Jingyu Wang, Xingyu Wang, Haifeng Sun, and Jianxin Liao. Drift doesn't matter: Dynamic decomposition with diffusion reconstruction for unstable multivariate time series anomaly detection. In _Annual Conference on Neural Information Processing Systems_, 2023.
* [36] Huiqiang Wang, Jian Peng, Feihu Huang, Jince Wang, Junhui Chen, and Yifei Xiao. MCN: multi-scale local and global context modeling for long-term series forecasting. In _International Conference on Learning Representations_, 2023.
* [37] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. In _Annual Conference on Neural Information Processing Systems_, pages 22419-22430, 2021.
* [38] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-variation modeling for general time series analysis. In _International Conference on Learning Representations_, 2023.
* [39] Kun Yi, Qi Zhang, Wei Fan, Hui He, Liang Hu, Pengyang Wang, Ning An, Longbing Cao, and Zhendong Niu. Fouriergnn: Rethinking multivariate time series forecasting from a pure graph perspective. In _Annual Conference on Neural Information Processing Systems_, 2023.
* [40] Kun Yi, Qi Zhang, Wei Fan, Shoujin Wang, Pengyang Wang, Hui He, Ning An, Defu Lian, Longbing Cao, and Zhendong Niu. Frequency-domain mlps are more effective learners in time series forecasting. In _Annual Conference on Neural Information Processing Systems_, 2023.
* [41] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? In _AAAI Conference on Artificial Intelligence_, pages 11121-11128, 2023.
* [42] Yifan Zhang, Qingsong Wen, Xue Wang, Weiqi Chen, Liang Sun, Zhang Zhang, Liang Wang, Rong Jin, and Tieniu Tan. Onenet: Enhancing time series forecasting models under concept drift by online ensembling. In _Annual Conference on Neural Information Processing Systems_, 2023.
* [43] Yunhao Zhang and Junchi Yan. Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting. In _International Conference on Learning Representations_, 2023.
* [44] Yu Zheng, Xiuwen Yi, Ming Li, Ruiyuan Li, Zhangqing Shan, Eric Chang, and Tianrui Li. Forecasting fine-grained air quality based on big data. In _ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, pages 2267-2276, 2015.
* [45] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In _AAAI Conference on Artificial Intelligence_, pages 11106-11115, 2021.
* [46] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting. In _International Conference on Machine Learning_, pages 27268-27286, 2022.
* [47] Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, and Rong Jin. One fits all: Power general time series analysis by pretrained LM. In _Annual Conference on Neural Information Processing Systems_, 2023.

Detailed Experimental Setup

### Dataset

We conduct extensive experiments on nine real-world datasets across five domains, including Electricity, Exchange, Traffic, Weather, and ILI, along with four ETT datasets. Table 4 summarizes the statistics of these datasets. These datasets have been widely utilized for benchmarking purposes and are publicly available. (1) **Electricity4** comprises hourly electricity consumption for 321 customers from 2012 to 2014. (2) **Exchange5** encompasses panel data on daily exchange rates for 8 countries from 1990 to 2019. (3) **Traffic6** aggregates hourly road occupancy rates measured by 862 sensors on San Francisco Bay Area freeways from 2015 to 2016. (4) **Weather7** captures 21 weather parameters monitored every 10 minutes from Germany in 2020. (5) **ILI8** records the percentage of patients with influenza-like illness and the total number of such patients collected weekly by the United States Centers for Disease Control and Prevention from 2002 to 2020. (6) **ETT9** records the oil temperature and load characteristics of two power transformers from 2016 to 2018, each at 2 different resolutions (15 minutes and 1 hour), resulting in a total of four datasets: ETTm1, ETTm2, ETTh1, and ETTh2.

Footnote 4: https://archive.ics.uci.edu/dataset/321/electricityloaddiagrams20112014

Footnote 5: https://github.com/laiguokun/multivariate-time-series-data

Footnote 6: https://pems.dot.ca.gov

Footnote 7: https://www.bgc-jena.mpg.de/wetter

Footnote 8: https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html

Footnote 9: https://github.com/zhouhaoyi/ETDataset

Footnote 10: https://github.com/zhouhaoyi/Informer2020

Footnote 11: https://github.com/cure-lab/LTSF-Linear

Footnote 12: https://github.com/thuml/TimesNet

Footnote 13: https://github.com/thuml/iTransformer

### Backbone

To demonstrate the effectiveness of our framework, we select several mainstream forecasting models based on different architectures, including the Transformer-based Informer (2021) and iTransformer (2024), the Linear-based DLinear (2023), and the Convolution-based TimesNet (2023). All aforementioned models are non-autoregressive forecasting models. (1) **Informer10** utilizes the ProbSparse attention and distillation mechanism to manage exceedingly long input sequences efficiently and incorporates a generative decoder to mitigate the error accumulation inherent in autoregressive forecasting methodologies. (2) **DLinear11** employs decomposition-enhanced simple linear networks to attain competitive forecasting performance. (3) **TimesNet12** accurately models two-dimensional dependencies by transforming the one-dimensional time series into a collection of two-dimensional tensors, leveraging multiple periods to embed intra-periodic and inter-periodic variations along the columns and rows of the tensor, respectively. (4) **iTransformer13** embeds individual channel into token employed by the attention mechanism, facilitating the capture of inter-channel multivariate correlations while applying a feed-forward network to each token to acquire nonlinear representations.

Footnote 12: https://github.com/thuml/iTransformer

All backbones are based on our runs, using the same hardware. We utilize official or open-source implementations and follow the hyperparameter configurations recommended in their papers.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Dataset & Channel & Length & Frequency & Information \\ \hline Electricity & 321 & 26304 & 1 Hour & Energy \\ Exchange & 8 & 7588 & 1 Day & Finance \\ Traffic & 862 & 17544 & 1 Hour & Transportation \\ Weather & 21 & 52696 & 10 Minutes & Climate \\ ILI & 7 & 966 & 1 Week & Healthcare \\ ETTh1 \& ETTh2 & 7 & 17420 & 1 Hour & Energy \\ ETTm1 \& ETTm2 & 7 & 69680 & 15 Minutes & Energy \\ \hline \hline \end{tabular}
\end{table}
Table 4: The statistics of each dataset. Channel represents the variate number of each dataset. Length indicates the total number of time points. Frequency denotes the sampling interval of time points.

``` importtorch fromtorch importmn classGLAFF(mn.Module): def_init_(self,hist_len,channel,dim=512,diff=2048,dropout=0.1,head_num=0,layer_num=2): :paramhist_len:thelengthofthehistorywindow :paramchannel:thenumberofthedatasetchannel :paramdim:thedimensionoftheMultiHeadAttention :paramaff:thedimensionofthefeedforwardnetwork :paramdropout:thedropoutproportionoftheMultiHeadAttention :paramhead_num:thenumberoftheattentionheadintheMultiHeadAttention :paramlayer_num:thenumberoftheattentionblockintheAttention-basedMapper """ super(GLAFF,self)...init_() self.Mapper=nn.Sequential( nn.Linear(6,dim), mn.Transform(render(  nn.TransformerEncoder(  d_node=dim,  nheadhead_num,  dim.feedforward=df,  dropout=dropout,  activation=gelu',  batch_first=True,  ),  num_layers=layer_num,  norm=n.LayerNorm(dim)  ),  nn.Linear(dim,channel)  ) self.Combiner=nn.Sequential(  nn.Linear(hist_len,diff),  nn.GLU(),  nn.Linear(diff,2),  nn.Softmax(dim=-1) ) defforward(self,hist_gt,hist_ts,pred_pr,pred_ts,q=0.75):  """ :paramhist_gt:thetruevalueinthehistorywindow :paramhist_ts:thetimestampsinthehistorywindow :parampred_pr:thepredictionvalueofthebackboneinthepredictionwindow :parampred_ts:thetimestampsinthepredictionwindow :paramq:thequantitiesoftheRobustDenormalizer """
#map hist_map=self.Mapper(hist_ts)#themappingvalueofthemapperinthehistorywindow pred_map=self.Mapper(pred_ts)#themappingvalueofthemapperinthepredictionwindow #inversenormalize means_gt+torch.median(hist_gt,1,True)[0] means_map+torch.mean(hist_map,1,True)[0] stddev_gt=torch.quantities(hist_gt,q,1,True)torch.quantities(hist_gt,1-q,1,True) stddev_map+torch.quantities(hist_map,q1,True)) hist_map+(hist_map+means_map)/stddev_map+stddev_gt+means_gt pred_map+(pred_map-means_map)/stddev_map+stddev_gt+means_gt
#combine error=hist_gt-hist_map weight+self.Combiner(error.permute(0,2,1)),unsqueeze2(1) pred=torch.stack([pred_map,pred_pr],dim-1) pred=torch.sum(pred*weight,dim-1) returnpred ```

**Algorithm 1** GLAFF

### Implementation

We employ the Adam optimizer and \(L_{2}\) loss for model optimization, initializing the learning rate at \(10^{-4}\). The batch size is uniformly set to 32, and the number of training epochs is fixed at 10. Optimal hyperparameters for GLAFF are determined through grid search, employing a common setup shared across all datasets and backbones. The number \(l\) of the attention blocks in the Attention-based Mapper is designated as 2 (selected from {1,2,3,4,5}), and while proportion \(p\) of dropout is set to 0.1 (selected from {0.0,0.1,0.2,0.3,0.4}). The quantile \(q\) in the Robust Denormalizer is configured to 0.75, selected from {0.55, 0.65, 0.75, 0.85, 0.95}. We provide details of each hyperparameter in Appendix B.3. All experiments are conducted using Python 3.10.13 and PyTorch 2.1.2, executed on an Ubuntu server equipped with an AMD Ryzen 9 7950X 16-Core processor and a single NVIDIA GeForce RTX 4090 graphics card, with each experiment repeated three times using different random seeds. If not explicitly stated, we report the Mean Square Error (MSE) and Mean Absolute Error (MAE) as evaluation metrics, with lower values indicating superior performance.

The detailed implementation of GLAFF is delineated in Algorithm 1. For simplification, the Adaptive Combiner is implemented through a straightforward two-layer linear network. We employ the _nn.TransformerEncoder()_ within PyTorch to realize the Attention-based Mapper. The source code and checkpoints have been made openly accessible to facilitate future research.

## Appendix B Full Experimental Result

### Robustness Analysis

We report in Table 5 the means and standard deviations of the evaluation metrics for the baselines and GLAFF under three runs using different random seeds, facilitating the assessment of their robustness in long-range and ultra-long-range time series forecasting tasks. Evident from the mean of the experimental outcomes, GLAFF consistently demonstrates marked superiority over all four mainstream forecasting models across all nine real-world benchmark datasets. The standard deviation indicates the consistent stability and robustness of our proposed framework.

### Ablation Study

We provide a comprehensive ablation study to validate the necessity of the GLAFF components. We implement our approach and its four variants on the iTransformer backbone. Specifically, in w/o Backbone, we completely remove the backbone network within the GLAFF and map the future using only timestamps. In w/o Attention, we substitute the stacked attention blocks with MLP networks having the equivalent size. In w/o Quantile, we replace the Robust Denormalizer with conventional inverse normalization. In w/o Adaptive, we substitute the Adaptive Combiner with a straightforward averaging mechanism for global mapping and local prediction. Due to space limitations, Section 4.4 presents experimental results for only three representative datasets, so we provide the complete results for nine real-world datasets in also Table 6.

When excluding any observations within the history window, GLAFF demonstrates an average prediction accuracy superior to Informer across nine real-world datasets and remains competitive with DLinear. However, in contrast to the three representative datasets outlined in Section 4.4, the average prediction accuracy of GLAFF across the nine datasets still lags behind TimesNet and iTransformer. We find that this discrepancy primarily stems from the highly non-stationary ILI dataset. In highly non-stationary scenarios, relying solely on global information fails to adapt to the intricate and fluctuating nature of real-world conditions. Our experimental outcomes validate the indispensable nature of both the robustness of global information and the flexibility of local information for accurate time series prediction.

The experimental results show that the adaptive adjustment of combined weights within the Adaptive Combiner is the most important among various designs. When the time series pattern exhibits clarity and stability, greater emphasis should be placed on robust global information. Conversely, increased attention should be directed towards flexible local information when the time series pattern appears ambiguous and variable. By fusing global and local information adaptively, GLAFF can seamlessly collaborate with any time series forecasting backbone to adapt to intricate and fluctuating real-world scenarios. Furthermore, the stacked attention blocks within the Attention-based Mapper and the robust inverse normalization within the Robust Denormalizer also yield notable contributions to enhancing the forecasting performance of GLAFF through efficient modeling of timestamps and robust mitigation of data drift.

It is noteworthy that the lack of either component design will corrupt the outcome of the GLAFF, resulting in a prediction accuracy lower than the standard iTransformer baseline. This aligns with both common knowledge and theoretical expectations. In terms of global information, the introduction of insufficient (w/o Attention), inaccurate (w/o Quantile), or crude (w/o Adaptive) will all hurt GLAFF. This again validates that each component design in GLAFF is reasonable and necessary.

[MISSING_PAGE_EMPTY:16]

[MISSING_PAGE_FAIL:17]

The number of attention blocks in the Attention-based Mapper influences the modeling process for global information representation. A greater depth in attention blocks enables GLAFF to model timestamp dependencies more sufficiently and uncover latent high-level semantic features. Nevertheless, augmenting the depth of the network also escalates computation costs and exacerbates convergence challenges. According to experimental findings, all benchmark datasets exhibit robustness to variations in the number \(l\) of attention blocks, thus facilitating the deployment of GLAFF. Considering training cost and model performance, we choose 2 as the number \(l\) of attention blocks for all datasets and backbones.

The proportion of dropout in the Attention-based Mapper impacts the generalization performance of GLAFF. When the dropout proportion is too small, indicating the retention of too many neurons, the model tends to overfit, thereby diminishing its generalization capacity. Conversely, if the dropout proportion is excessively large, meaning the exclusion of too many neurons, the model may struggle to effectively capture the underlying features, decreasing prediction accuracy. Moreover, a high dropout proportion can also impede the convergence of model training due to the varying network structures observed in each training sample. From the experimental results, except for the ETTm1 and Traffic datasets, most datasets exhibit insensitivity to the choice of dropout proportion \(p\). To simplify the parameter selection challenge, we adopt a dropout proportion \(p\) of 0.1 for all datasets and backbones.

### Efficiency Evaluation

To assess the practical applicability of GLAFF in real-world environments, we comprehensively compare training time and memory usage between baseline models and GLAFF across nine datasets. The experimental results are summarized in Table 7. Specifically, incorporating GLAFF results in an average 23.4382s increase in the training time and a 25.0608MB increase in memory usage. Presently, with hardware resources evolving rapidly, this computation costs may not affect the training and deployment of GLAFF in most scenarios, particularly when considering the significant accuracy enhancement it offers. Moreover, it is a common practice to trade off computation costs for enhanced prediction accuracy. For instance, the TimesNet baseline exhibits an average training time increase of 355s and an average memory cost increase of 145MB compared to the DLinear baseline, yielding a mere 8.4% increase in average prediction accuracy. In contrast, our GLAFF incurs a training time increase of 14s and a memory usage increase of 25MB while achieving a 13.1% increase in prediction accuracy compared to the DLinear backbone. In scenarios demanding high prediction accuracy, this computation costs are acceptable.

Notably, owing to the unique forecasting mechanism (mapping), the memory usage incurred by GLAFF remains insensitive to both the dataset and the prediction length, consistently hovering at approximately 25MB. Additionally, we observe that the introduction of GLAFF not only results in substantial enhancements in prediction accuracy in all scenarios, but also significantly reduces the training time in some scenarios (such as the TimesNet backbone on the ETTh2 dataset, the Informer backbone on the Traffic dataset, the DLinear backbone, and the TimesNet backbone). We postulate

Figure 4: The forecasting errors for multivariate time series of hyperparameter analysis among different configurations for GLAFF. A lower outcome indicates a better prediction.

[MISSING_PAGE_FAIL:19]

Figure 5: The complete illustration of prediction showcases among GLAFF and mainstream baselines.

corresponding licenses (Appendix A.1 and Appendix A.2). There is no potential ethical risk or negative social impact.

## Appendix D Limitation

While GLAFF exhibits encouraging performance on benchmark datasets, it is subject to certain limitations. As a model-agnostic and plug-and-play framework, GLAFF incurs considerable computation costs attributed to the utilization of stacked attention blocks in Attention-based Mapper (Appendix B.4). Presently, with hardware resources evolving rapidly, this computation costs may not affect the training and deployment of GLAFF in most scenarios. However, GLAFF will likely encounter operational challenges in resource-constrained edge devices, thus restricting its applicability. In future work, we plan to explore lighter weight and more efficient architectures, such as dilated convolutional or graph neural networks, to replace the conventional attention mechanism.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Please refer to Abstract and Section 1 Introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Please refer to Appendix D Limitation. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: This paper does not include theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Please refer to Section 4.1 Experimental Setup and Appendix A Detailed Experimental Setup. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: The data is publicly available as said in Appendix A.1 Dataset. Please refer to Appendix A.3 Implementation for the core code. We submit the full code as part of the supplemental material. After the review period, we will open-source the full code and checkpoints, but not before in order to not break anonymity of the submission. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Please refer to Section 4.1 Experimental Setup, Appendix A Detailed Experimental Setup and Appendix B.3 Hyperparameter Analysis. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Please refer to Section 4.2 Main Result and Appendix B.1 Robustness Analysis. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Please refer to Appendix B.4 Efficiency Evaluation. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: This research conform, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Please refer to Appendix C Broader Impact. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Please refer to Reference, Appendix A.1 Dataset and Appendix A.2 Backbone. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: Please refer to Appendix A.3 Implementation for the core code. We submit the full code as part of the supplemental material. After the review period, we will open-source the full code and checkpoints, but not before in order to not break anonymity of the submission. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.