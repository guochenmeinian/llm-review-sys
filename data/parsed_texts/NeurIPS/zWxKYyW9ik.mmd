# Universality and Limitations of Prompt Tuning

Yihan Wang

UCLA

wangyihan617@gmail.com &Jatin Chauhan

UCLA

chauhanjatin100@gmail.com &Wei Wang

UCLA

weiwang@cs.ucla.edu &Cho-Jui Hsieh

Google and UCLA

chohsieh@cs.ucla.edu

###### Abstract

Despite the demonstrated empirical efficacy of prompt tuning to adapt a pretrained language model for a new task, the theoretical underpinnings of the difference between "tuning parameters before the input" against "the tuning of model weights" are limited. We thus take one of the first steps to understand the role of soft-prompt tuning for transformer-based architectures. By considering a general purpose architecture, we analyze prompt tuning from the lens of both: universal approximation and limitations with finite-depth fixed-weight pretrained transformers for continuous-valued functions. Our universality result guarantees the existence of a strong transformer with a prompt to approximate any sequence-to-sequence function in the set of Lipschitz functions. The limitations of prompt tuning for limited-depth transformers are first proved by constructing a set of datasets, that cannot be memorized by a prompt of any length for a given single encoder layer. We also provide a lower bound on the required number of tunable prompt parameters and compare the result with the number of parameters required for a low-rank update (based on LoRA) for a single-layer setting. We finally extend our analysis to multi-layer settings by providing sufficient conditions under which the transformer can at best learn datasets from invertible functions only. Our theoretical claims are also corroborated by empirical results.

## 1 Introduction

The surge in the empirical research of large-scale models has led to the emergence of a new paradigm of prompt tuning. Current large models consist of billions of parameters (Brown et al., 2020; Chowdhery et al., 2022), which greatly exacerbate the cost of tuning the entire model weights via gradient-based optimization. On the other hand, the power of scale in both model size and pretraining dataset size has demonstrated strong capabilities by achieving reasonable performance through a learnable prompt appended before the input (Li and Liang, 2021; Lester et al., 2021). Despite this, several questions emanate around the abilities and limitations of prompt tuning.

In this work, we aim to characterize some natural yet essential questions about prompt tuning with transformer architectures. Firstly, are prompts universal approximators, i.e. with a fixed pretrained transformer network, can we find a prompt to approximate any sequence-to-sequence function in a given space? If yes, can we construct the transformer for this universality result? Second, can we identify failure modes of prompt tuning when applied on potentially non-optimal but non-trivial transformers? Moreover, since prompt tuning is usually compared against LoRA(Hu et al., 2021) in consideration to parameter-efficient tuning, is prompt tuning then more/less parameter-efficient than LoRA? Answering these questions can lead to important insights on _when_ and _how_ to perform prompt tuning to adapt a pretrained transformer network to a given downstream task of interest.

In this work, we seek to answer these questions with appropriate theoretical analysis and further validate our claims with empirical results. We first characterize the universal nature of prompt tuning by constructing a specific transformer network. We show that for a given approximation error and the space of sequence-to-sequence Lipschitz functions, we can construct a transformer network, with a suitable number of layers, that can leverage prompt tuning to approximate any function in this space. Despite this universality of prompt tuning with a carefully constructed pretrained transformer, we then identify some limitations of prompt tuning with weaker but non-trivial transformers. We prove this by constructing sequence-to-sequence datasets with shared input tokens, which are surprisingly simple but cannot be memorized by prompt tuning for a given transformer. We also extend our analysis to more general settings where the shared token is not required. In this setting, we first prove that prompt tuning on a single-layer transformer requires \(\Omega(n)\) trainable parameters to memorize \(n\) training examples, wherein for LoRA, it suffices with \(O(n)\) trainable parameters. We finally extend our analysis to the multi-layer setting and provide sufficient conditions under which prompt tuning exhibits extremely limited capacity to at best memorizing datasets from invertible functions.

Our contributions can be summarized as below:

* We characterize the universal nature of prompt tuning by explicitly constructing a transformer network (Theorem 1).
* We provide a construction-based argument for sequence-to-sequence datasets that cannot be learned by prompt tuning with a given single-layer transformer (Theorem 2).
* We provide the lower bound on the required number of parameters for prompt tuning to memorize any sequence-to-sequence functions (Theorem 3).
* We provide a sufficient condition for multi-layer transformers, under which datasets with shared output tokens cannot be learned with prompt tuning (Theorem 4).
* We conduct empirical studies, including real-world datasets, to verify our theoretical claims.

## 2 Related Work

Theoretical Analysis of TransformersVarious works have characterized the theoretical properties of transformers and its primary self-attention component. Yun et al. (2020) studies the universal approximation ability of transformers for continuous permutation equivariant sequence-to-sequence functions with compact support and further examined the role of using positional encodings to circumvent permutation equivariant condition. Perez et al. (2021) show that transformer with a hard-attention is Turing complete based on their capacity to perform computations and access the internal dense representations of the data. Wei et al. (2021) further shows that transformers can approximate Turing machines with bounded computation time with a new notion of approximation. (Dong et al., 2021) provides a negative yet interesting result signifying the limitations of pure self-attention in terms of rank diminishing of the input. Other works including Kim et al. (2021); Dasoulas et al. (2021) derive upper bounds on the Lipschitz constant of respective modifications of the attention mechanism. The works by Li et al. (2022); Zhang et al. (2020) documented optimization perspective on transformer training via SGD.

Fine-tuning and Prompt TuningFine-tuning is the standard way to adapt a pretrained model to downstream tasks. The most standard and popular paradigm is tuning the model weights via a suitable optimization procedure along with a linear head on the output representations (Radford et al., 2018; Devlin et al., 2019). Subsequent works studied more parameter-efficient ways of fine-tuning by updating either a subset of model parameters (Ben Zaken et al., 2022) or restricting the parameter-updates to a low-dimensional subspace (Aghajanyan et al., 2021; Hu et al., 2021; Mahabadi et al., 2021). The work by Hu et al. (2021) (their framework referred to as LoRA) has garnered particular interest in the community and Malladi et al. (2023) has provided an interpretation of LoRA via the kernel mechanism. In the particular context of LLMs, prompt tuning has emerged as the de facto approach where only the prompt is updated while keeping the rest of the transformer weights and architecture fixed (Shin et al., 2020; Lester et al., 2021; Li and Liang, 2021).

Analysis of Prompt TuningWei et al. (2022) studies the link between prompt tuning and downstream tasks with an underlying latent variable generative model of text, which is confined to a Hidden Markov Model. However, they focused on the discrete vocabulary setting contrary to our results for continuous sequence-to-sequence functions. Some more recent works [1, 23] characterize an intriguing property of a specific form of prompting, referred to as in-context learning, where they proved by construction that transformers can implement learning algorithms for linear models based on gradient descent and closed-form ridge regression. This work however pursued a different and specific direction from the prompting results we aim to provide for generic settings.

Memorization Capacity of Neural NetworksA series of works have sought to provide finite sample universal memorization capacity results of neural networks and the understanding of expressive power of neural networks. Huang and Huang [1990], Huang [2003], Yamasaki [1993] analyze the memorization capacity of FNNs with sigmoid and other bounded activation functions. Hardt and Ma [2016], Zhang et al. [2021], Nguyen and Hein [2018] provide results for modern ReLU networks including FNNs and CNNs. For transformer architectures, Kim et al. prove that transformers can memorize a dataset with finite parameters. To the best of our knowledge, similar results for prompt tuning have not been studied in continuous settings for transformer architectures.

## 3 Transformers and Parameter Efficient Training

### Preliminaries

We use the following notations throughout the paper. A bold lower case character, e.g. \(\mathbf{x}\), denotes a vector. A bold upper case character, e.g. \(\mathbf{W}\), denotes a matrix while \(\mathbf{W}_{i,j}\), \(\mathbf{W}_{i,:}\) and \(\mathbf{W}_{:,j}\) is the \((i,j)\)-th element, \(i\)-th row, \(j\)-th column, respectively. We use a single superscript or subscript to denote the index of a matrix, e.g. \(\mathbf{X}_{i},\mathbf{X}^{i}\) denote the \(i\)-th matrix in a matrices sequence. We use \(\sigma\) and \(\bar{\sigma}\) for softmax and hardmax operators, respectively. We use \(\texttt{ReLU}(\mathbf{v})=\max(\mathbf{v},\mathbf{0})\) to denote the ReLU activation function where \(\max(\cdot)\) function is applied entry-wise to a vector. We use \(\texttt{Cone}(\mathbf{a}_{1},\mathbf{a}_{2},...,\mathbf{a}_{m})\) to denote a cone without its origin point where \(\texttt{Cone}(\mathbf{a}_{1},\mathbf{a}_{2},...,\mathbf{a}_{m})=\{\mathbf{x}: \mathbf{x}=\sum_{i=1}^{m}a_{i}\mathbf{a}_{i},a_{i}>0\}\). We also define the minus operation between a set \(S\) and a vector \(\mathbf{v}\) as \(S-\mathbf{v}=\{\mathbf{x}-\mathbf{v}:\mathbf{x}\in S\}\). In Section 4, we use \([a:b:c]\) to denote a grid \(\{a,a+b,a+2b,...,c-b\}\) from \(a\) to \(c\), with an interval \(b\).

Transformer networks [2017] are a stack of multiple transformer layers, composed subsequently. A transformer layer has two key components: an attention layer and a token-wise MLP layer, with residual connections around both blocks. We consider the input and output to be sequences of tokens \(\mathbf{X}\in\mathbb{R}^{d\times m}\) and \(\mathbf{Y}\in\mathbb{R}^{d\times m}\), where \(m\) is the number of tokens in the sequence and \(d\) is the token dimension.

**Definition 1** (Attention Layer).: _We define an \(h\)-head attention layer parameterized with \(\mathbf{W}_{q},\mathbf{W}_{k},\mathbf{W}_{v},\mathbf{W}_{o}\) between a single token \(\mathbf{x}\) and a token sequence \(\mathbf{X}\) as_

\[\texttt{Att}(\mathbf{x},\mathbf{X})=\sum_{i=1}^{h}\mathbf{W}_{o}^{i}\mathbf{ W}_{v}^{i}\mathbf{X}\cdot\sigma((\mathbf{W}_{k}^{i}\mathbf{X})^{\top}\mathbf{W}_{q}^{i} \mathbf{x}).\] (1)

_The normalizing factor of \(\frac{1}{\sqrt{d_{kq}}}\) is subsumed in the weight matrices \(\mathbf{W}_{k}^{i}\) for notational simplicity._

_We can then define the cross attention between two sequences \(\mathbf{X}_{1}\in\mathbb{R}^{d\times m_{1}}\) and \(\mathbf{X}_{2}\in\mathbb{R}^{d\times m_{2}}\) (We use \(\mathbf{x}_{k}=(\mathbf{X}_{1})_{:,k}\) for simplicity):_

\[\texttt{Att}(\mathbf{X}_{1},\mathbf{X}_{2})=[\texttt{Att}(\mathbf{x}_{1}, \mathbf{X}_{2}),\texttt{Att}(\mathbf{x}_{2},\mathbf{X}_{2}),...,\texttt{Att }(\mathbf{x}_{m_{1}},\mathbf{X}_{2})].\]

**Definition 2** (Standard Transformer Layer).: _With definition 1, we define a standard transformer layer \(\tau\) as_

\[\texttt{MLP}(\mathbf{X})=[\mathbf{W}_{2}\texttt{ReLU}(\mathbf{W}_{1}\mathbf{ X}_{:,1}+\mathbf{b}_{1})+\mathbf{b}_{2}+\mathbf{X}_{:,1},...,\mathbf{W}_{2} \texttt{ReLU}(\mathbf{W}_{1}\mathbf{X}_{:,n}+\mathbf{b}_{1})+\mathbf{b}_{2}+ \mathbf{X}_{:,n}]\] (2)

\[\tau(\mathbf{X})=\texttt{MLP}(\texttt{Att}(\mathbf{X},\mathbf{X})+\mathbf{X}).\] (3)

_The definition here omits the layer normalization block for simplicity (following [15])._

We denote the set of transformer networks with \(h\) heads of size \(s\) and \(r\) MLP hidden neurons with \(\mathcal{T}^{h,s,r}\). In Section 4, we utilize a modified transformer network with hardmax operation \(\bar{\sigma}\) instead of softmax \(\sigma\). We denote this modified version of transformer networks as \(\bar{\mathcal{T}}^{h,s,r}\).

During fine-tuning, we optimize the matrices \(\mathbf{W}_{q}^{i},\mathbf{W}_{k}^{i},\mathbf{W}_{v}^{i}\) in the attention layer and \(\mathbf{W}_{1},\mathbf{W}_{2},\mathbf{b}_{1},\mathbf{b}_{2}\) in the MLP layer pertaining to a loss function \(\mathcal{L}\). However in prompt tuning, the pretrained model weight matrices are fixed and we optimize a tunable sequence prepended to the input.

Prompt TuningGiven a pretrained transformer network \(g\in\mathcal{T}\) and a downstream training dataset \(S=\{(\mathbf{X}_{1},\mathbf{Y}_{1}),...,(\mathbf{X}_{n},\mathbf{Y}_{n})\}\), prompt tuning seeks to find a prompt \(\mathbf{P}^{*}\in\mathbb{R}^{d\times m_{p}}\) with \(m_{p}\) tunable tokens under the loss function \(\mathcal{L}\):

\[\mathbf{P}^{*}=\operatorname*{arg\,min}_{\mathbf{P}}\sum_{i=1}^{n}\mathcal{L}( g([\mathbf{P},\mathbf{X}_{i}])_{:,m_{p}:},\mathbf{Y}_{i}).\] (4)

The tunable prompt \(\mathbf{P}\) is shared amongst all the inputs in a task. Note that \(\mathbf{P}\) in prompt tuning is a continuously trainable parameter, alternately referred to as soft prompt, which is different from hard prompt in that the latter operates on a discrete space of predefined vocabulary. Since the representation power of soft prompts is strictly more than the hard prompts, the limitations studied in this paper also extend to hard prompts.

In the subsequent sections, we analyze the universality and limitations of prompt tuning while comparing the latter against fine-tuning and LoRAHu et al. (2021), which is a low-rank version of model fine-tuning. In Section 4, we prove that prompt tuning can be universal approximators for sequence-to-sequence functions, while providing the construction for the same. In Sections 5 and 6, we identify the failure modes where prompt tuning cannot learn with a possibly non-optimal but non-trivial pretrained transformer network.

## 4 Universality of Prompt Tuning

Without loss of generality, we assume that the support and range set of all considered sequence-to-sequence functions \(f\) is \([0,1]^{d\times m}\) in this section. We define \(\mathcal{F}_{L}\) as the collection of all continuous sequence-to-sequence \(L\)-lipschitz functions under norm \(p\) and sequence length \(m\). For \(f\in F_{L}\) and any two inputs \(\mathbf{X},\mathbf{X}^{\prime}\in[0,1]^{d\times m}\), we have \(\|f(\mathbf{X})-f(\mathbf{X}^{\prime})\|_{p}\leq L\|\mathbf{X}-\mathbf{X}^{ \prime}\|_{p}\). Furthermore, given functions \(f_{1},f_{2}\), the approximation error under a \(p\)-norm (which is entry-wise) is measured as:

\[d_{p}(f_{1},f_{2})=(\int\|f_{1}(\mathbf{X})-f_{2}(\mathbf{X})\|_{p}^{p}d \mathbf{X})^{\frac{1}{p}}.\] (5)

Primarily, we show that there exists a Transformer network \(g\in\mathcal{T}^{2,1,4}\) such that for any \(f\in\mathcal{F}_{L}\), prompt tuning on \(g\) can approximate this function upto some error budget \(\epsilon>0\).

**Theorem 1**.: _Let \(1\leq p<\infty\) and \(\epsilon>0\), there exist a transformer network \(g\in\mathcal{T}^{2,1,4}\) and prompt length \(m_{p}\), such that for any \(f\in\mathcal{F}_{L}\) we can find a prompt \(\mathbf{P}\in\mathbb{R}^{d\times m_{p}}\) with \(d_{p}(g([\mathbf{P},\cdot])_{:,m_{p}:},f)\leq\epsilon\)._

Here we use the transformer in a encoder mode which generates the \(m\) outputs in one step. In Appendix C.4, a similar result can be obtained for next-token prediction, which is widely used in many recent language models.

The proof is inspired from (Yun et al., 2019), which follows the typical construction based proof mechanism to show universality. Thereby, we can construct a "meta-transformer" for prompt tuning to approximate any sequence-to-sequence function with prompt tuning. Next we briefly describe the two steps for the construction of this meta-transformer. We start by building a meta-function for \(\mathcal{F}_{L}\).

Building the Meta-FunctionWe denote the length of all inputs as \(m\) and the prompt length as \(m_{p}\). Then we can build a sequence-to-sequence meta-function that accepts inputs with length \(m+m_{p}\).

**Lemma 1**.: _For the sequence-to-sequence function space \(\mathcal{F}_{L}\) with functions \(f:[0,1]^{d\times m}\to[0,1]^{d\times m}\), we can build a sequence-to-sequence function \(\bar{g}:[0,1]^{d\times(m_{p}+m)}\to[0,1]^{d\times(m_{p}+m)}\) such that for any \(f\in\mathcal{F}_{L}\), we can find \(\mathbf{P}\in\mathbb{R}^{d\times m_{p}}\), \(d_{p}(\bar{g}([\mathbf{P},\cdot])_{:,m_{p}:},f)\leq\epsilon/2\)._

The complete proof is given in Appendix C.1. Succinctly, we first quantize the input and output sequence space of \([0,1]^{d\times m}\) into a grid \(G_{\delta,m}=\{0,\delta,2\delta,...,1-\delta\}^{d\times m}\), thus leading to \(C=(\frac{1}{\delta^{d\times m}})^{\frac{1}{\delta^{d\times m}}}\) possible functions mappings from the input to the output, in this discrete space. Bythis quantized function space as \(\mathcal{\bar{F}}_{L}=\{f_{1},\bar{f}_{2},...,f_{C}\}\), we can select \(\delta\) such that the approximation error for any function is less than \(\epsilon/2\). Then we construct a set of quantized prompts in \(G_{\delta,m_{p}}=\{0,\delta,2\delta,...,1-\delta\}^{d\times m_{p}}\) to index these \(C\) functions and construct a quantized function \(\bar{g}\) where \(\bar{g}([\mathbf{P}_{i},\mathbf{X}])_{:,m_{p}:}=\bar{f}_{i}(\mathbf{X}),i=1,2,...,C\), for all \(\mathbf{X}\in G_{\delta,m}\), thereby concluding the lemma.

Next we can utilize some conclusions in [20] to construct a transformer for \(\bar{g}\).

Constructing the Meta-TransformerWe first introduce a useful lemma which enables the construction of a transformer for any quantized sequence-to-sequence function.

**Lemma 2**.: _For any given quantized function \(\bar{f}:[0,1]^{d\times m}\rightarrow[0,1]^{d\times m}\) with quantization at interval \(\delta\), \(\exists\bar{h}\in\mathcal{\bar{T}}^{2,1,1}\) such that \(\bar{f}=\bar{h}\) with positional embedding \(\mathbf{E}=\begin{bmatrix}0&1&2&...&m-1\\ 0&1&2&...&m-1\\ \vdots&\vdots&\vdots&\ddots&\vdots\\ 0&1&2&...&m-1\end{bmatrix}\)._

The proof mainly follows the discussions in Section C of [20]. To prove this lemma, the network \(\bar{h}\) can be constructed in the following three steps. We first use a series of MLP layers to quantize the input to grid \([0:\delta:1-\delta]^{d\times m}\) and then a series of attention layers to obtain a unique contextual mapping for each quantized input. Finally we can use a series of MLP layers to map the unique contextual mapping to the desired outputs. While a transformer network usually stacks self-attention and MLP layers alternately within a single layer, the aforementioned construction can be trivially attained via the use of skip connections. The complete proof of Lemma 2 is deferred to Appendix C.2.

Since \(\bar{g}\) is a quantized function in grid \(G_{\delta,m+m_{p}}\), following Lemma 2 we can find a modified version of transformer \(\bar{h}\in\mathcal{\bar{T}}^{2,1,1}\) such that \(\bar{g}([\mathbf{P},\mathbf{X}])=\bar{h}([\mathbf{P},\mathbf{X}])\). The modified version of transformer \(\bar{g}\) with hardmax operators can then be approximated with a standard transformer \(g\) with softmax operators by Lemma 3.

**Lemma 3** (Lemma 9 in [20]).: _For each \(\bar{h}\in\mathcal{\bar{T}}^{2,1,1}\), \(\epsilon>0\) and \(1\leq p<\infty\), \(\exists g\in\mathcal{T}^{2,1,4}\) such that \(d_{p}(\bar{h},g)\leq\epsilon/2\)._

Since the approximation error can be treated uniformly amongst the \(\mathbf{P}_{i}\), we have that \(d_{p}(\bar{h}([\mathbf{P}_{i},\cdot])_{:,m_{p}:},g([\mathbf{P}_{i},\cdot])_{ :,m_{p}:})\leq d_{p}(\bar{h}([\mathbf{P}_{i},\cdot]),g([\mathbf{P}_{i},\cdot] )\leq\epsilon/2\). Therefore, we can build a transformer \(g\in\mathcal{T}^{2,1,4}\), such that for any sequence-to-sequence \(f\in\mathcal{F}_{L}\), we can find a quantized version \(\bar{f}_{i}\in\mathcal{\bar{F}}_{L}\) and the corresponding prompt \(\mathbf{P}_{i}\in G_{\delta,m_{p}}\) such that

\[d_{p}(g([\mathbf{P}_{i},\cdot])_{:,m_{p}:},f)\leq d_{p}(g([ \mathbf{P}_{i},\cdot])_{:,m_{p}:},\bar{h}([\mathbf{P}_{i},\cdot]))+d_{p}( \bar{h}([\mathbf{P}_{i},\cdot])_{:,m_{p}:},\bar{f}_{i})+d_{p}(\bar{f}_{i},f) \leq\epsilon.\] (6)

Theorem 1 provides the construction for a large transformer (discussed more in appendix) that is sufficient for prompt tuning to exhibit universal approximation over a Lipschitz function space. However, even this strong transformer also has limitations with prompt tuning when the target function \(f\notin\mathcal{F}_{L}\). Is this an essential limitation for prompt tuning on any transformer? In the next section, we will theoretically analyze the limitations of prompt tuning with transformers and target functions under more general conditions.

## 5 Limitations of Prompt-Tuning: Single Layer Transformer

To analyse the failure modes and therefore the limitations under the setting where a transformer has fixed pretrained weights, we follow the lens of exact memorization in the subsequent sections.

**Definition 3** (Memorization of a Sequence-to-Sequence Dataset).: _Given a sequence-to-sequence dataset \(S=\{(\mathbf{X}_{1},\mathbf{Y}_{1}),...,(\mathbf{X}_{n},\mathbf{Y}_{n})\}\) where \(\mathbf{X}_{i},\mathbf{Y}_{i}\in\mathbb{R}^{d\times m}\) are the input/output sequences, we consider a function \(f\) exactly memorizing dataset \(S\) if \(f(\mathbf{X}_{i})=\mathbf{Y}_{i}\). In the following proofs of this section, we explicitly focus on the last output token, ie: \(f(\mathbf{X}_{i})_{:,-1}=(\mathbf{Y}_{i})_{:,-1}\)._

We start from the analysis on a single layer transformer and extend to multi-layer settings in Section 6.

### Failure modes of Prompt Tuning

It is straightforward to note that prompt tuning has limited expressive power when the number of trainable parameters is limited. A natural question to then ask is: Does increasing the number of trainable prompt tokens suffice? While it is known that for MLPs, even with a single hidden layer, increasing the number of hidden neurons can memorize any training data (Yun et al., 2019b). However, as we will prove next, this is not the case for prompt tuning. This result highlights an essential limitation of prompt tuning compared to model fine-tuning.

Before providing the theorem statement, we first outline some straightforward assumptions on the pretrained transformer and datasets, without which prompt tuning trivial loses expressive power.

We consider sequence-to-sequence datasets of the form \(S=\{(\mathbf{X}_{1},\mathbf{Y}_{1}),(\mathbf{X}_{2},\mathbf{Y}_{2}),...,( \mathbf{X}_{n},\mathbf{Y}_{n})\}\) with \(n\) distinct examples and a single-layer single-head standard transformer defined in Definition 2. The results can be directly extended to the single-layer multi-head scenario, which we skip here to avoid notational clutter.

**Assumption 1** (Non-trivial conditions).: _We assume that all output tokens \((\mathbf{Y}_{i})_{:,k}\) are in the range set of MLP, otherwise the expressivity becomes trivially weak. We assume that \(\mathbf{W}_{q},\mathbf{W}_{k},\mathbf{W}_{v}\) are full rank matrices and that \(\texttt{{Att}}(\mathbf{X}_{i},\mathbf{X}_{i})+\mathbf{X}_{i}\) are distinct for \(i=1,2,...,n\)._

**Assumption 2** (Assumption for the MLP layer).: _We assume that \(d\geq 2+\texttt{{dim}}((\texttt{{MLP}}^{-1}(\mathbf{y}_{10})-\mathbf{x}_{0}) \cup(\texttt{{MLP}}^{-1}(\mathbf{y}_{20})-\mathbf{x}_{0}))\) for the dataset constructed in Theorem 2 and token dimension \(d\). \(\texttt{{dim}}(\mathcal{S})\) measures the dimension of subspace spanned by vectors in a set \(\mathcal{S}\) and \(\texttt{{MLP}}^{-1}(\mathbf{y})=\{\mathbf{x}:\texttt{{MLP}}(\mathbf{x})= \mathbf{y}\}\)._

_We provide an example for this assumption in Example 1 and a sufficient condition in the following Lemma 4._

**Lemma 4**.: _If \(\|\mathbf{W}_{1}\|_{2}\times\|\mathbf{W}_{2}\|_{2}<1\), where \(\|\cdot\|_{2}\) is the matrix spectral norm, then the MLP block in Definition 2 is invertible, ie, \(\texttt{{MLP}}^{-1}\) is a singleton set._

_Therefore, if Lemma 4 holds and \(d\geq 4\), Assumption 2 also holds._

Proof of Lemma 4 can be found in Appendix C.5. The experimental evidence in (Dong et al., 2021) shows that for most architectures, the norm of the weight matrices indeed admits small values and thus the requirement that \(\|\mathbf{W}_{1}\|_{2}\times\|\mathbf{W}_{2}\|_{2}<1\) is a mild condition.

With these assumptions, here we introduce our first theorem on the unlearnability of prompt tuning.

**Theorem 2**.: _For a single layer transformer \(\tau\) defined above with Assumptions 1 and 2, we can build a sequence-to-sequence dataset \(S=\{(\mathbf{X}_{1}=[\mathbf{x}_{1},\mathbf{x}_{0}],\mathbf{Y}_{1}=[\mathbf{ y}_{11},\mathbf{y}_{10}]),(\mathbf{X}_{2}=[\mathbf{x}_{2},\mathbf{x}_{0}], \mathbf{Y}_{2}=[\mathbf{y}_{21},\mathbf{y}_{20}])\}\), and we cannot find a prompt \(\mathbf{P}\in\mathbb{R}^{d\times m_{p}}\) with any \(m_{p}>0\) such that \(\tau([\mathbf{P},\mathbf{X}_{i}])=\mathbf{Y}_{i}\) holds for any \(i=1,2\). The vectors \(\mathbf{x}_{0},\mathbf{x}_{1},\mathbf{x}_{2}\) are denoted post positional encodings._

An important feature of this dataset is that the same token \(\mathbf{x}_{0}\) is shared between the two examples, and the expressive capability of prompt tuning is limited by the correlation of outputs corresponding to this token in different examples. We show a concrete example here to illustrate this theorem (note that Lemma 4 is in fact not required in the following construction) and defer the formal proof to Appendix C.6.

**Example 1**.: _We consider a single-head transformer layer \(\tau\), where \(\mathbf{b}_{1}=\mathbf{b}_{2}=\mathbf{0}\), \(\mathbf{W}_{1}=1^{r\times d}\), \(\mathbf{W}_{2}=1^{d\times r}\). Then the token-wise MLP layer is a concatenation of two linear functions:_

\[\texttt{{MLP}}(\mathbf{x})=\begin{cases}(\mathbf{W}_{2}\mathbf{W}_{1}+ \mathbf{I})\mathbf{x},(\mathbf{W}_{1}\mathbf{x})_{0}>0\\ \mathbf{x}\hskip 56.905512pt,(\mathbf{W}_{1}\mathbf{x})_{0}\leq 0\end{cases}\] (7)

_Here \((\mathbf{W}_{1}\mathbf{x})_{0}\) denotes the first element of vector \(\mathbf{W}_{1}\mathbf{x}\)._

\(\mathbf{W}_{2}\mathbf{W}_{1}+\mathbf{I}\) is a non-singular matrix. Therefore, for any \(\mathbf{y}\) in \(\texttt{{MLP}}(\mathbf{X})\)'s output set, \(\texttt{{MLP}}^{-1}(\mathbf{y})\) contains at most two points \(\{\mathbf{y},(\mathbf{W}_{2}\mathbf{W}_{1}+\mathbf{I})^{-1}\mathbf{y}\}\). We arbitrarily choose \(\mathbf{x}_{0},\mathbf{y}_{10}\) and \(\mathbf{y}_{20}\).

As long as \(d\geq 6\) (from Assumption 2), we can find \(\mathbf{c}_{1},\mathbf{c}_{2}\) such that \(\mathbf{c}_{1},\mathbf{c}_{2}\perp\mathbf{y}_{10}-\mathbf{x}_{0},\mathbf{y}_{ 20}-\mathbf{x}_{0},(\mathbf{W}_{2}\mathbf{W}_{1}+\mathbf{I})^{-1}\mathbf{y}_{ 10}-\mathbf{x}_{0},(\mathbf{W}_{2}\mathbf{W}_{1}+\mathbf{I})^{-1}\mathbf{y}_{ 20}-\mathbf{x}_{0},c_{1}\perp\mathbf{c}_{2}\). Then we choose \(\mathbf{x}_{1}\) and \(\mathbf{x}_{2}\) such that \(\texttt{{Att}}(\mathbf{x}_{0},\mathbf{X}_{1})\parallel\mathbf{c}_{1}\) and \(\texttt{{Att}}(\mathbf{x}_{0},\mathbf{X}_{2})\parallel\mathbf{c}_{2}\) (Lemma 7 in Appendix). Then \(\texttt{{Cone}}(-\texttt{{Att}}(\mathbf{x}_{0},\mathbf{X}_{1}),\mathbf{a}- \mathbf{x}_{0})\cap\texttt{{Cone}}(-\texttt{{Att}}(\mathbf{x}_{0},\mathbf{X}_{ 2}),\mathbf{b}-\mathbf{x}_{0})=\emptyset\), for any \(\mathbf{a}\in\{\mathbf{y}_{10},(\mathbf{W}_{2}\mathbf{W}_{1}+\mathbf{I})^{-1} \mathbf{y}_{10}\}\) and \(\mathbf{b}\in\{\mathbf{y}_{20},(\mathbf{W}_{2}\mathbf{W}_{1}+\mathbf{I})^{-1} \mathbf{y}_{20}\}\). Here \(\texttt{{Cone}}\) stands for a convex cone as defined in Section 3.1.

If a \(\mathbf{P}\) exists such that \(\tau([\mathbf{P},\mathbf{X}_{i}])=\mathbf{Y}_{i}\) holds for both \(i=1,2\), then we have

\[\texttt{Att}(\mathbf{x}_{0},[\mathbf{P},\mathbf{X}_{1}]) =\lambda(\mathbf{X}_{1},\mathbf{x}_{0},[\mathbf{P},\mathbf{X}_{1} ])\texttt{Att}(\mathbf{x}_{0},\mathbf{X}_{1})+\lambda(\mathbf{P},\mathbf{x}_{0 },[\mathbf{P},\mathbf{X}_{1}])\texttt{Att}(\mathbf{x}_{0},\mathbf{P})\] (8) \[\texttt{Att}(\mathbf{x}_{0},[\mathbf{P},\mathbf{X}_{2}]) =\lambda(\mathbf{X}_{2},\mathbf{x}_{0},[\mathbf{P},\mathbf{X}_{2} ])\texttt{Att}(\mathbf{x}_{0},\mathbf{X}_{2})+\lambda(\mathbf{P},\mathbf{x}_{0 },[\mathbf{P},\mathbf{X}_{2}])\texttt{Att}(\mathbf{x}_{0},\mathbf{P})\]

where \(\lambda(\cdot,\cdot,\cdot)\) is a positive scalar. We also have

\[\texttt{Att}(\mathbf{x}_{0},[\mathbf{P},\mathbf{X}_{1}])+ \mathbf{x}_{0}\in\texttt{MLP}^{-1}(\mathbf{y}_{10})\] \[\texttt{Att}(\mathbf{x}_{0},[\mathbf{P},\mathbf{X}_{2}])+ \mathbf{x}_{0}\in\texttt{MLP}^{-1}(\mathbf{y}_{20})\]

as \(\texttt{MLP}(\texttt{Att}(\mathbf{x}_{0},[\mathbf{P},\mathbf{X}_{i}])+\mathbf{ x}_{0})=\mathbf{y}_{i0},i=1,2\).

Therefore, \(\texttt{Att}(\mathbf{x}_{0},\mathbf{P})\) must be in both \(\texttt{Cone}(\mathbf{a}-\mathbf{x}_{0},-\texttt{Att}(\mathbf{x}_{0},\mathbf{ X}_{1}))\) and \(\texttt{Cone}(\mathbf{b}-\mathbf{x}_{0},-\texttt{Att}(\mathbf{x}_{0},\mathbf{ X}_{2}))\), where \(\mathbf{a}\in\{\mathbf{y}_{10},(\mathbf{W}_{2}\mathbf{W}_{1}+\mathbf{I})^{-1} \mathbf{y}_{10}\}\) and \(\mathbf{b}\in\{\mathbf{y}_{20},(\mathbf{W}_{2}\mathbf{W}_{1}+\mathbf{I})^{-1} \mathbf{y}_{20}\}\), which contradicts the existence of \(\mathbf{P}\) as \(\texttt{Cone}(-\texttt{Att}(\mathbf{x}_{0},\mathbf{X}_{1}),\mathbf{a}-\mathbf{ x}_{0})\cap\texttt{Cone}(-\texttt{Att}(\mathbf{x}_{0},\mathbf{X}_{2}),\mathbf{b}- \mathbf{x}_{0})=\emptyset\). Therefore, in this example, even though we allow an arbitrary number of trainable parameters in prompt \(\mathbf{P}\), we cannot find one to exactly memorize the training set with only two training examples.

This theorem reveals an important difference between prompt tuning and adjusting the model weights directly. For any training dataset \(S\) with two training examples \(\{(\mathbf{X}_{1},\mathbf{Y}_{1}),(\mathbf{X}_{2},\mathbf{Y}_{2})\}\), so long as \(\texttt{Att}(\mathbf{X}_{1},\mathbf{X}_{1})+\mathbf{X}_{1}\) and \(\texttt{Att}(\mathbf{X}_{2},\mathbf{X}_{2})+\mathbf{X}_{2}\) are distinct, MLP can easily map the post-attention features to expected output tokens with finite number of hidden neurons. As a result, tuning the MLP parameters for this pretrained transformers can memorize any dataset in the form of Assumption 1. However, prompt tuning cannot achieve this even if the number of tunable tokens \(\rightarrow\) infinity, thereby limiting the expressiveness of prompt tuning when compared to model fine-tuning.

### Comparison with a More General Dataset

In Section 5.1, we constructed sequence-to-sequence datasets that cannot be learned by a given transformer layer with prompt tuning, by utilizing the shared token between different training examples. In this section, we compare the expressive power of prompt tuning and fine-tuning under a more general dataset construction where the former requirement can be relaxed.

Since the primary essence of prompt tuning is to perform parameter-efficient tuning, wherein we seek to adapt a pretrained large model to a new task with fewer tunable parameters, we compare prompt tuning with another parameter-efficient version of model-tuning: LoRA [Hu et al., 2021]. Succinctly, we compare the required number of parameters to memorize a given dataset. Again, consider a sequence-to-sequence dataset \(S=\{(\mathbf{X}_{1},\mathbf{Y}_{1}),(\mathbf{X}_{2},\mathbf{Y}_{2}),...,( \mathbf{X}_{n},\mathbf{Y}_{n})\}\), where \(\mathbf{X}_{i}=[\mathbf{x}_{i1},\mathbf{x}_{i2},...,\mathbf{x}_{im}]\) and \(\mathbf{Y}_{i}=[\mathbf{y}_{i1},\mathbf{y}_{i2},...,\mathbf{y}_{im}]\). We again discuss the memorization of the last output token for simplicity and results can be directly extended.

We first give the required number of parameters of LoRA to memorize dataset \(S\).

**Lemma 5** (LoRA).: _For a standard single-layer transformer \(\tau\) defined in Definition 2 with \(r\geq n\) MLP hidden neurons, for any sequence-to-sequence dataset \(S\) satisfying Assumptions 1, we can apply a low-rank update to MLP weights with \(O(nd)\) parameters to memorize \(\tau(\mathbf{X}_{i})_{:,m}=\mathbf{y}_{im}\)._

This lemma is derived based on the memorization capabilities of 1-hidden layer MLPs [Yun et al., 2019b]. As the post-attention values for different training inputs are different from Assumption 1, we can construct a low rank update with \(O(nd)\) parameters on the MLP layer to memorize \(S\). We defer the complete proof to Appendix C.7.

For prompt tuning, we derive a result in the next theorem which shows that it requires \(\Omega(nd)\) tunable parameters to memorize some constructed dataset \(S\) with \(n\) examples.

**Theorem 3** (Lower bound on Tunable Prompt Parameters).: _For any single layer transformer \(\tau\) defined in Definition 2, there exists a sequence-to-sequence dataset \(\{(\mathbf{X}_{1}=[\mathbf{x}_{10},\mathbf{x}_{1}],[\mathbf{y}_{10},\mathbf{y}_ {11}]),(\mathbf{X}_{2}=[\mathbf{x}_{20},\mathbf{x}_{2}],[\mathbf{y}_{20}, \mathbf{y}_{21}]),...,(\mathbf{X}_{n}=[\mathbf{x}_{n0},\mathbf{x}_{n}],[ \mathbf{y}_{n0},\mathbf{y}_{n1}])\}\) that satisfies Assumption 1 with \(n<d\) training examples such that we need at least \(n\) prompt tokens in \(\mathbf{P}\) to memorize the training set, ie, for \(\tau([\mathbf{P},\mathbf{X}_{i}])_{:,-1}=\mathbf{y}_{i1}\) to hold for all \(i=1,2,...,n\)._

This dataset can be constructed by including \(n\) examples that require \(n\) linearly independent prompts tokens. The complete proof is deferred to Appendix C.8.

Note that in Theorem 3, we provide a key lower bound on the required number of prompt tokens for exact memorization and this can very well more than \(nd\). This partially (but not necessarily) explains the worse empirical performance of prompt tuning against LoRA under a comparable number of trainable parameters.

## 6 Extension to Multi-Layer Setting

In this section, we extend our analysis to multi-layer setting and provide a sufficient condition under which the expressiveness of prompt tuning is restricted. An immediate consequence of our result is an interesting connection to the spectral norm of soft prompts surfaces. This result provides us a partial understanding of the phenomenon that soft prompt \(\mathbf{P}\) vectors typically exhibit larger norms compared to the actual input \(\mathbf{X}\), after the tuning.

With some further notation adjustments, we denote an \(H\) layer pretrained transformer network as \(g(\in\mathcal{T})=\tau^{1}\circ\tau^{2}\circ...\circ\tau^{H}\), the input set as \(\mathcal{X}^{1}\), and the set of possible prompts as \(\mathcal{P}^{1}\). We assume that the following compactness condition is satisfied:

\[\|[\mathbf{P}^{l},\mathbf{X}^{l}]\|_{2} \leq D^{l}\] (9) \[\text{s.t.} [\mathbf{P}^{l+1},\mathbf{X}^{l+1}] =\tau^{l}([\mathbf{P}^{l},\mathbf{X}^{l}]),\forall l=1,...,H.\]

Here \([\mathbf{P}^{1},\mathbf{X}^{1}]\) is the input to the first layer \(\tau^{1}\) with \(\mathbf{P}^{1}\in\mathcal{P}^{1}\), \(\mathbf{X}^{1}\in\mathcal{X}^{1}\) and \(\|\cdot\|_{2}\) is the spectral norm. Similarly, \([\mathcal{P}^{H+1},\mathcal{X}^{H+1}]\) denotes the output set.

We start by providing an upper bound to the Lipschitz constant of attention, pertaining to eq 9. This derivation is different from the works of (Dasoulas et al., 2021; Vuckovic et al., 2020) and thus can be of independent interest.

**Lemma 6**.: _Under the compactness condition, the Lipschitz constant of the \(i\)-th attention head in the \(l\)-th transformer layer, denoted for simplicity as \(\texttt{{1tt}}^{i,l}\), admits the following bound w.r.t the entire input sequence of length \(m\):_

\[Lip(\texttt{{1tt}}^{i,l}(\cdot,\cdot))\leq(1+8\sqrt{m}(D^{l})^{2}\|(\mathbf{W }_{k}^{i,l})^{T}\mathbf{W}_{q}^{i,l}\|_{2})\|\mathbf{W}_{v}^{i,l}\|_{2},\] (10)

_and the Lipschitz constant of the entire attention block in layer \(l\), denoted as \(\texttt{{1tt}}^{l}\), admits the bound:_

\[Lip(\texttt{{1tt}}^{l}(\cdot,\cdot))\leq\sqrt{\sum_{i=1}^{h}(\|\mathbf{W}_{o} ^{i,l}\|_{2}\times Lip(\texttt{{1tt}}^{i,l}))^{2}}.\] (11)

It is noteworthy that this upper bound is dependent on \(D^{l}\), the spectral norm of the input prepended with the prompt. In conjunction with the following theorem, we obtain a result on limited expressivity of prompt tuning by showing that the transformer becomes invertible, in consideration to functions from \(\mathcal{P}^{1}\times\mathcal{X}^{1}\rightarrow\mathcal{P}^{H+1}\times\mathcal{ X}^{H+1}\) (an extension to functions of the from \(\mathcal{X}^{1}\rightarrow\mathcal{X}^{H+1}\) is provided in Appendix Section C.11).

**Theorem 4**.: _A transformer \(g\in\mathcal{T}\) is invertible, ie \(,g^{-1}(\mathbf{Y})=\{\mathbf{X}:g(\mathbf{X})=\mathbf{Y}\}\) is a singleton set \(\forall\mathbf{Y}\) in range of \(g\), if:_

1. _The Lipschitz constant of the attention block in each layer_ \(\tau^{l}\) _is strictly less than 1_
2. _The Lipschitz constant of the 2-layer ReLU block in each layer_ \(\tau^{l}\)_, which is bounded by_ \(\|\mathbf{W}_{2}^{l}\|_{2}\times\|\mathbf{W}_{1}^{l}\|_{2}\)_, is strictly less than 1._

Proof of Theorem 4 can be found in Appendix C.9. Combining Lemma 6 and Theorem 4, we observe that the invertibility is guaranteed if the upper bound for the Lipschitz constant of the attention, eq 11, and the MLP layer, is strictly less than 1. In this case, we can then construct arbitrarily many datasets where two different inputs share the same output, and prompt tuning cannot learn (more subtly: memorize) these datasets with a restricted prompt norm.

Experiments

### Experimental Settings

In Section 7.2, we use a standard single-layer single-head transformer from Definition 2, to justify the infinite prompt-length limitation. In Section 7.3, we justify the increasing prompt norm on the pretrained LLaMA 7B model (Touvron et al., 2023). For prompt tuning and LoRA, we use the Huggingface Pelt library (Mangrulkar et al., 2022). On the dataset front, we utilize the RTE subtask of SuperGlue dataset (Wang et al., 2019) and WMT14 En-Fr translation (Bojar et al., 2014). More details and hyperparameter settings can be found in Appendix A.

### Limited Expressivity of Infinite Length Prompt

We first construct the dataset following the proof of Theorem 2 and then show that prompt tuning cannot memorize this simple dataset \(\{(\mathbf{X}_{1}=[\mathbf{x}_{1},\mathbf{x}_{0}],\mathbf{Y}_{1}=[\mathbf{y}_ {11},\mathbf{y}_{10}]),(\mathbf{X}_{2}=[\mathbf{x}_{2},\mathbf{x}_{0}],\mathbf{ Y}_{2}=[\mathbf{y}_{21},\mathbf{y}_{20}])\}\) even with very large prompt lengths.

We set the token dimension \(d=10\). We follow the default pytorch weight initialization and then normalize \(\mathbf{W}_{1},\mathbf{W}_{2}\) such that \(\|\mathbf{W}_{2}\|_{2}\times\|\mathbf{W}_{1}\|_{2}<1\), following Assumption 2. We randomly sample \(\mathbf{x}_{0},\mathbf{y}_{10},\mathbf{y}_{20}\) in a uniform distribution in \([0,1)^{d}\) and construct the corresponding vectors: \(\mathbf{x}_{1}\) and \(\mathbf{x}_{2}\) following Theorem 2. To compute \(\mathtt{MLP}^{-1}(\mathbf{y})\), we follow (Kim et al., 2021) Section 4.1 with 5000 iterations at convergence. We solve \(\mathtt{Att}(\mathbf{x}_{0},[\mathbf{x}_{0},\mathbf{x}_{1}])\parallel\mathbf{c}\) in Lemma 7 with gradient descent terminating at \(\angle(\mathtt{Att}(\mathbf{x}_{0},[\mathbf{x}_{0},\mathbf{x}_{1}]),\mathbf{c} )<0.0001\). We repeat this setup to obtain 3 different datasets for distinct \(\mathbf{x}_{0},\mathbf{y}_{10},\mathbf{y}_{20}\) and denote these with \(S_{i},i=1,2,3\).

We perform prompt tuning, MLP fine-tuning and MLP LoRA training on the constructed datasets for 5 runs and report the mean and standard deviation of per-element Mean Squared Error (MSE) loss \(\mathbf{y}_{10},\mathbf{y}_{20}\) at convergence. We show the comparison between prompt-tuning and MLP fine-tuning in Figure 1. As we can observe from the figure, increasing the number of soft prompt tokens post a certain threshold that does not exhibit any reduction in MSE. On the contrary, fine-tuning on the MLP layer tend to easily memorize the training set by reducing the training loss to almost zero (all the three curves for fine-tuning overlap and thus not differentiated). Note that we plot the standard deviation, however it is negligible in the range. Similar to fine-tuning on the MLP layer, LoRA with width 2 on the MLP layer also achieves near-zero training loss which is less than \(10^{-10}\) on the constructed dataset. We don't plot the comparison on Figure 1 as all the six curves are overlapped). This result validates our Theorem 3 that LoRA can memorize a dataset with \(n\) examples with trainable parameters \(O(n)\) while prompt-tuning may require more.

### Increasing Prompt Spectral Norm during Tuning

As discussed in Section 6, a major constraint on the expressive power of prompt tuning is the spectral norm of soft prompts. In Figure 2, we plot the curve for spectral norm of soft prompt as training progresses and the loss reduces on RTE dataset. The curve for WMT14 En-Fr dataset can be found in Appendix B. This trend clearly highlights that in order to counter the limit on the capacity, the spectral norm consistently increases till the training loss saturates.

## 8 Conclusions

In this work, we embark on exploring the capabilities of prompt tuning in the continuous regime, contrasting it with fine-tuning, as an initial endeavor towards a theoretical comprehension. We prove by construction that prompt tuning admits universal approximation within the space of Lipschitz functions. Additionally, we identified inherent limitations of prompt tuning on single-layer transformers by constructing theoretically difficult datasets for prompt tuning. These limitations are then extended to multi-layer setting under a specific prompt-norm restriction.

From the analysis in Theorem 2 and 3, we note that the limitation of prompt-tuning primarily arises from the correlation across different inputs. Broadly describing, prompt-tuning implements transformation on different inputs via "additional attention values", which is more restrictive as compared to the transformations from MLP layers on input tokens. An interesting potential direction to improve prompt-tuning is: "designing a mechanism to leverage prompting in order to generate prompt-dependent adapter/LoRA updates". We expect to have some future work focusing on designing novel prompt-tuning strategies along this direction.

LimitationsWhile our results provide valuable insights, extending the construction in Theorem 2 to multiple layers and deriving tighter bounds for Lemma 6 are critical steps for a deeper understanding of the limitations of prompt tuning.

## Acknowledgments and Disclosure of Funding

We thank the reviewers for their invaluable feedbacks. The work is supported in part by NSF 2008173, 2048280, 2325121, 2331966, ONR N00014-23-1-2300:P00001.

## References

* A. Aghajanyan, S. Gupta, and L. Zettlemoyer (2021)Intrinsic dimensionality explains the effectiveness of language model fine-tuning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Online, August 2021, pp. 7319-7328. External Links: Link, Document Cited by: SS1, SS2.
* E. Akyurek, D. Schuurmans, J. Andreas, T. Ma, and D. Zhou (2023)What learning algorithm is in-context learning? investigations with linear models. In The Eleventh International Conference on Learning Representations, External Links: Link, Document Cited by: SS1, SS2.
* J. Behrmann, W. Grathwohl, R. T. Q. Chen, D. Duvenaud, and J. Jacobssen (2019-01)Invertible residual networks. In Proceedings of the 36th International Conference on Machine Learning, Proceedings of Machine Learning Research, pp. 573-582. External Links: Link, Document Cited by: SS1, SS2.
* E. B. Zaken, Y. Goldberg, and S. R. (2022)BitFit: simple parameter-efficient fine-tuning for transformer-based masked language-models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Dublin, Ireland, May 2022, pp. 1-9. External Links: Link, Document Cited by: SS1, SS2.
*

Figure 1: MSE losses at convergence for the 3 constructed datasets (following Theorem 2). We plot the bold curves with increasing prompt length in prompt tuning and dashed fixed lines in fine-tuning (all three datasets overlapping).

Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Ales Tamchyna. Findings of the 2014 workshop on statistical machine translation. In _Proceedings of the Ninth Workshop on Statistical Machine Translation_, pages 12-58, Baltimore, Maryland, USA, June 2014. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/W/W14/W14-3302.
* Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.
* Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thuanulyan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022.
* Dasoulas et al. (2021) George Dasoulas, Kevin Scaman, and Aladin Virmaux. Lipschitz normalization for self-attention layers with application to graph neural networks, 2021.
* Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423.
* Dong et al. (2021) Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: pure attention loses rank doubly exponentially with depth. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 2793-2803. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/dong21a.html.
* Hardt and Ma (2016) Moritz Hardt and Tengyu Ma. Identity matters in deep learning. _arXiv preprint arXiv:1611.04231_, 2016.
* Hu et al. (2021) Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021.
* Huang (2003) Guang-Bin Huang. Learning capability and storage capacity of two-hidden-layer feedforward networks. _IEEE transactions on neural networks_, 14(2):274-281, 2003.
* Huang and Babri (1998) Guang-Bin Huang and Haroon A Babri. Upper bounds on the number of hidden neurons in feedforward networks with arbitrary bounded nonlinear activation functions. _IEEE transactions on neural networks_, 9(1):224-229, 1998.
* Huang and Huang (1990) S-C Huang and Y-F Huang. Bounds on number of hidden neurons of multilayer perceptrons in classification and recognition. In _1990 IEEE International Symposium on Circuits and Systems (ISCAS)_, pages 2500-2503. IEEE, 1990.
* Kim et al. (2021) Hyunjik Kim, George Papamakarios, and Andriy Mnih. The lipschitz constant of self-attention, 2021.

Junghwan Kim, Michelle Kim, and Barzan Mozafari. Provable memorization capacity of transformers. In _The Eleventh International Conference on Learning Representations_.
* Lester et al. (2021) Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 3045-3059, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.243. URL https://aclanthology.org/2021.emnlp-main.243.
* Li and Liang (2021) Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 4582-4597, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.353. URL https://aclanthology.org/2021.acl-long.353.
* Li et al. (2022) Zhiyuan Li, Srinadh Bhojanapalli, Manzil Zaheer, Sashank Reddi, and Sanjiv Kumar. Robust training of neural networks using scale invariant architectures. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 12656-12684. PMLR, 17-23 Jul 2022. URL https://proceedings.mlr.press/v162/li22b.html.
* Mahabadi et al. (2021) Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efficient low-rank hypercomplex adapter layers, 2021.
* Malladi et al. (2023) Sadhika Malladi, Alexander Wettig, Dingli Yu, Danqi Chen, and Sanjeev Arora. A kernel-based view of language model fine-tuning, 2023.
* Mangrulkar et al. (2022) Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Belkada Younes, and Paul Sayak. Peft: State-of-the-art parameter-efficient fine-tuning methods. https://github.com/huggingface/peft, 2022.
* Nguyen and Hein (2018) Quynh Nguyen and Matthias Hein. Optimization landscape and expressivity of deep cnns. In _International conference on machine learning_, pages 3730-3739. PMLR, 2018.
* Perez et al. (2021) Jorge Perez, Pablo Barcelo, and Javier Marinkovic. Attention is turing-complete. _Journal of Machine Learning Research_, 22(75):1-35, 2021. URL http://jmlr.org/papers/v22/20-302.html.
* Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018.
* Shin et al. (2020) Taylor Shin, Yasaman Razeghi, Robert L. Logan IV au2, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts, 2020.
* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* Oswald et al. (2023) Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In _International Conference on Machine Learning_, pages 35151-35174. PMLR, 2023.
* Vuckovic et al. (2020) James Vuckovic, Aristide Baratin, and Remi Tachet des Combes. A mathematical theory of attention, 2020.
* Wang et al. (2019) Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. _Advances in neural information processing systems_, 32, 2019.
* Wang et al. (2019)Colin Wei, Yining Chen, and Tengyu Ma. Statistically meaningful approximation: a case study on approximating turing machines with transformers. _CoRR_, abs/2107.13163, 2021. URL https://arxiv.org/abs/2107.13163.
* Wei et al. (2022) Colin Wei, Sang Michael Xie, and Tengyu Ma. Why do pretrained language models help in downstream tasks? an analysis of head and prompt tuning, 2022.
* Yamasaki (1993) Masami Yamasaki. The lower bound of the capacity for a neural network with multiple hidden layers. In _ICANN'93: Proceedings of the International Conference on Artificial Neural Networks Amsterdam, The Netherlands 13-16 September 1993 3_, pages 546-549. Springer, 1993.
* Yun et al. (2019a) Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar. Are transformers universal approximators of sequence-to-sequence functions? _arXiv preprint arXiv:1912.10077_, 2019a.
* Yun et al. (2019b) Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Small relu networks are powerful memorizers: a tight analysis of memorization capacity. _Advances in Neural Information Processing Systems_, 32, 2019b.
* Yun et al. (2020) Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Are transformers universal approximators of sequence-to-sequence functions? In _International Conference on Learning Representations_, 2020. URL https://openreview.net/forum?id=ByxRM0Ntvr.
* Zhang et al. (2021) Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. _Communications of the ACM_, 64(3):107-115, 2021.
* Zhang et al. (2020) Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank J Reddi, Sanjiv Kumar, and Suvrit Sra. Why are adaptive methods good for attention models?, 2020.

Experimental Details

All the experiments are run on a NVIDIA RTX A6000 GPU. For experiments with Llama 7B model, we use batch size 32 and learning rate 0.001. For experiment on WMT14 En-Fr translation, we only compute the loss on the first 100 examples for computational efficiency.

We use Adam optimizer and optimal learning rate from grid search at 0.1 for prompt-tuning and at 0.001 for fine-tuning in Section 7.2.

In Section 7.3, we use the default loss function in Huggingface implementation for causal language models. We use prompt length \(m=10\) and the prompt tokens are initialized as the first \(m\) tokens in the model vocabulary.

## Appendix B Additional Experiments

As mentioned in Section 7.3, the second real world dataset used in our experiment is WMT14 En-Fr translation in order to illustrate that the spectral norm of soft prompts increases during training. We show the curve in Figure B.

## Appendix C Proof of Lemmas and Theorems

### Proof of Lemma 1

For the sequence-to-sequence function space \(\mathcal{F}_{L}\) with functions \(f:[0,1]^{d\times m}\rightarrow[0,1]^{d\times m}\), we can build a sequence-to-sequence function \(\bar{g}:[0,1]^{d\times(m_{p}+m)}\rightarrow[0,1]^{d\times(m_{p}+m)}\) such that for any \(f\in\mathcal{F}_{L}\), we can find \(\mathbf{P}\in\mathbb{R}^{d\times m_{p}}\), \(d_{p}(\bar{g}([\mathbf{P},\cdot])_{:,m_{p}:},f)\leq\epsilon/2\).

Proof.: we first quantize the input and output sequence space of \([0,1]^{d\times m}\) into a grid space \(G_{\delta,m}=\{0,\delta,2\delta,...,1-\delta\}^{d\times m}\), which leads to \(C=(\frac{1}{g^{d\times m}})^{\frac{1}{d\times m}}\) functions considering all input and output mappings in this grid. We index these \(C\) functions as \(\bar{\mathcal{F}}_{L}=\{\bar{f}_{1},\bar{f}_{2},...,\bar{f}_{C}\}\). For \(\mathbf{X}\notin G_{\delta,m}\), we let \(\bar{f}_{i}(\mathbf{X})=\bar{f}_{i}(\mathbf{X}^{*})\) if \(k_{i,j}\delta<\mathbf{X}_{i,j},\mathbf{X}_{i,j}^{*}\leq(k_{i,j}+1)\delta\) and \(\mathbf{X}^{*}\in G_{\delta,m}\).

Then for any \(f\in\mathcal{F}_{L}\), we can find a function \(\bar{f}\in\bar{F}_{L}\) such that \(d_{p}(\bar{f},f)=(\int\|\bar{f}(\mathbf{X})-f(\mathbf{X})\|_{P}^{p}d\mathbf{X })^{1/p}\leq(\int L^{p}md\delta^{p}d\mathbf{X})^{1/p}=L(md)^{\frac{1}{p}}\delta\). We choose \(\delta=\delta_{1}\) such that \(L(md)^{\frac{1}{p}}\delta\leq\epsilon/2\). For the prompt part, we choose \(m_{p}\) such that \(\frac{1}{g^{d\times m_{p}}}\geq C\). Then we can build a set of quantized prompts in \(G_{\delta,m_{p}}=\{0,\delta,2\delta,...,1-\delta\}^{d\times m_{p}}\) to index these \(C\) functions. We denote this set of prompts as \(\{\mathbf{P}_{1},\mathbf{P}_{2},...,\mathbf{P}_{C}\}\). Finally we can create the quantized function \(\bar{g}\) and let

Figure 3: Increasing prompt spectral norm during tuning on WMT14 En-Fr translation dataset.

\(\bar{g}([\mathbf{P}_{i},\mathbf{X}])_{:,m_{p}:}=\bar{f}_{i}(\mathbf{X})\) and \(\bar{g}([\mathbf{P}_{i},\mathbf{X}])_{:,:m_{p}}=0\), \(\forall\mathbf{X}\in[0,1]^{d\times m},\mathbf{P}\in G_{\delta,m_{p}}\). For \(\mathbf{P}\notin G_{\delta,m_{p}}\), we set \(\bar{g}([\mathbf{P},\mathbf{X}])=\bar{g}([\mathbf{P}^{*},\mathbf{X}])\) if \(k_{i,j}\delta<\mathbf{P}_{i,j},\mathbf{P}_{i,j}^{*}\leq(k_{i,j}+1)\delta\) and \(\mathbf{P}^{*}\in G_{\delta,m_{p}}\).

Therefore, with a properly chosen \(\delta=\delta_{1}\), for any \(f\in\mathcal{F}_{L}\), we can find \(\mathbf{P}\in\mathbb{R}^{d\times m_{p}}\) such that \(d_{P}(f,\bar{g}([\mathbf{P},\cdot])_{:,m_{p}:})=d_{p}(\bar{f},f)\leq\epsilon/2\).

### Proof of Lemma 2

For any given quantized function \(\bar{f}:[0,1]^{d\times m}\rightarrow[0,1]^{d\times m}\) with quantization at interval \(\delta\), \(\exists\bar{h}\in\mathcal{T}^{2,1,1}\) such that \(\bar{f}=\bar{h}\) with positional embedding \(\mathbf{E}=\begin{bmatrix}0&1&2&...&m-1\\ 0&1&2&...&m-1\\ \vdots&\vdots&\vdots&\ddots&\vdots\\ 0&1&2&...&m-1\end{bmatrix}\).

Proof.: The proof is given following Section C in Yun et al. (2019a) appendix. With Section C.1 in Yun et al. (2019a), there exists a function \(g_{q}\) composed of \(\frac{dm}{\delta}\) token-wise feed-forward layers with hidden layer size \(r=1\) and ReLU activation to implement this scalar quantization on each input element:

\[g_{q}^{ent}(t)=\begin{cases}k\delta&\text{if }k\delta\leq t<(k+1)\delta,k=0,1,...,m/\delta-1\\ -\delta^{-md}&\text{otherwise}\end{cases}\]

Then with Section C.2 in Yun et al. (2019a), we can stack \(m(1/\delta)^{d}+1\) attention layers to map all possible input sequences in grid \([0:\delta:1-\delta]^{d}\times[1:\delta:2-\delta]^{d}\times...\times[m-1: \delta:m-\delta]^{d}\) to distinct numbers which are at least \(\delta\) from each other.

Finally we only require \(O(m(1/\delta)^{dm})\) layers to map these distinct numbers to expected outputs. 

### Proof of Lemma 3

Lemma 3 is almost the same as (Yun et al., 2019a) except that we use \(\epsilon/2\) instead of \(\epsilon/3\).

### Extension of Theorem 1 to Next-token Predictors

As an extension of Theorem 1, we consider approximating a set of sequence-to-sequence functions when we use a transformer layer as a next-token predictor. We consider a set of sequence-to-sequence functions \(F_{L}\) with Lipschitz constant \(L\) under norm \(p\). \(f\in F_{L}:[0,1]^{d\times m_{1}}\rightarrow[0,1]^{d\times m_{2}}\) accepts an input of length \(m_{1}\) and outputs a sequence of length \(m_{2}\). For any \(\mathbf{X},\mathbf{X}^{\prime}\in[0,1]^{d\times m_{1}}\), we have \(\|f(\mathbf{X})-f(\mathbf{X}^{\prime})\|_{p}\leq L\|\mathbf{X}-\mathbf{X}^{ \prime}\|_{p}\).

Next we show that we can construct a transformer \(\tau\) which can approximate any \(f\in F_{L}\) with prompt-tuning when we use it as a next-token predictor.

**Theorem 5**.: _For any \(f\in F_{L}\), we can construct a transformer \(\tau\) such that for any \(f\in F_{L}\), \(1\leq p<\infty\) and \(\epsilon>0\), we can find a prompt \(\mathbf{P}\in[0,1]^{d\times m_{p}}\), such that \(d_{p}(f,h(\mathbf{P}))\leq\epsilon\), where_

\[h(\mathbf{P})=\tau_{1}([\mathbf{P},\cdot])_{:,-1}\times\tau_{2}([\mathbf{P}, \cdot])_{:,-1}\times...\times\tau_{m_{2}}([\mathbf{P},\cdot])_{:,-1}.\]

\(\tau_{i}\) _is the sequence-to-sequence function implemented with the transformer \(\tau\) when accepting sequences with length \(m_{p}+m_{1}+i\)._

Proof.: Similar to Theorem 1, we quantize the inputs to grid of \([0:\delta:1-\delta]\) with interval \(\delta\) and set \(m_{p}=(\delta^{d\times m_{2}})^{\delta^{d\times m_{1}}}\). \(\delta\) is chosen such that \(L(m_{1}d)^{1/p}\delta\leq m_{2}^{-1/p}\epsilon\). We index the \(C=(\delta^{d\times m_{2}})^{\delta^{d\times m_{1}}}\) different \(f\)s as \(f^{1},f^{2},...,f^{C}\) and its sub-function to generate the \(i\)-th output token as \(f^{j}_{i}\). The \(C\) sequence-to-sequence functions can then be indexed by \(C\) distinct prompts. Similar to Lemma 2, we can construct a transformer which can map all possible input sequences in grids \([0:\delta:1-\delta]\times...\times[m-1:\delta:m-\delta]^{d},0<m\leq m_{1}+m_{2} +m_{p}-1\) to distinct numbers. A final series of MLP layers then map these distinct numbers to desired output vectors where inputsin the same grid are mapped to the same output token at each step. Then for any input \(\mathbf{x}\in[0,1]^{d\times m_{1}}\) and any \(f^{j}\), we can find a prompt \(\mathbf{P}_{j}\) such that

\[\|\tau([\mathbf{P}_{j},\mathbf{x}])_{:,-1}-f^{j}_{0}(\mathbf{x})\|_ {p} \leq m_{2}^{-1/p}\epsilon\] \[\|\tau([\mathbf{P}_{j},\mathbf{x},\tau([\mathbf{P},\mathbf{x}])_ {:,-1}]_{:,-1},f^{j}_{1}([\mathbf{x},\tau([\mathbf{P},\mathbf{x}])_{:,-1}])\|_ {p} \leq m_{2}^{-1/p}\epsilon\] \[...\]

Then we have \(d_{p}(h(\mathbf{P}_{j}),f^{j})\leq\epsilon\). 

### Proof of Lemma 4

If \(\|\mathbf{W}_{1}\|_{2}\times\|\mathbf{W}_{2}\|_{2}<1\), where \(\|\cdot\|_{2}\) is the matrix spectral norm, then the MLP block in Definition 2 is invertible, ie, MLP\({}^{-1}\) is a singleton set.

Proof.: Based on the sufficient conditions for invertibility of a residual block Behrmann et al. (2019), we have that if the feedforward part of a residual block \(\mathbf{W}_{2}\texttt{ReLU}(\mathbf{W}_{1}\mathbf{X}_{:,1}+\mathbf{b}_{1})+ \mathbf{b}_{2}\) is a contraction with respect to some metric, i.e. its Lipschitz constant \(<1\), and the metric space on which is defined is complete, then MLP in eq 2 is invertible. Since we are dealing with the euclidean space, any metric induced by the \(\|\cdot\|_{p}\) norm for \(p\in[1,\infty]\) ensures the space is complete.

The Lipschitz constant of \(\mathbf{W}_{2}\texttt{ReLU}(\mathbf{W}_{1}\mathbf{x}+\mathbf{b}_{1})+ \mathbf{b}_{2}\) is simply \(\|\mathbf{W}_{1}\|_{2}\times\|\mathbf{W}_{2}\|_{2}\). Thus the statement of the lemma follows. 

### Proof of Theorem 2

For a single layer transformer \(\tau\) defined above with Assumptions 1 and 2, we can build a seq-to-seq dataset \(\{(\mathbf{X}_{1}=[\mathbf{x}_{1},\mathbf{x}_{0}],\mathbf{Y}_{1}=[\mathbf{y}_{ 11},\mathbf{y}_{10}]),(\mathbf{X}_{2}=[\mathbf{x}_{2},\mathbf{x}_{0}],\mathbf{ Y}_{2}=[\mathbf{y}_{21},\mathbf{y}_{20}])\}\), and we cannot find a prompt \(\mathbf{P}\in\mathbb{R}^{d\times m_{p}}\) with any \(m_{p}>0\) such that \(\tau([\mathbf{P},\mathbf{X}_{i}])=\mathbf{Y}_{i}\) holds for any \(i=1,2\). The vectors \(\mathbf{x}_{0},\mathbf{x}_{1},\mathbf{x}_{2}\) are denoted post positional encodings.

Proof.: Before proving Theorem 2, we first provide a lemma that will be used in proof and also Theorem 3.

**Lemma 7**.: _Given any \(\mathbf{c}\in\mathbb{R}^{d\times m}\), there are \(\mathbf{x}_{0}\) almost anywhere for which we can find another vector \(\mathbf{x}_{1}\in\mathbb{R}^{d\times m}\) such that \(\texttt{{Att}}(\mathbf{x}_{0},[\mathbf{x}_{0},\mathbf{x}_{1}])\parallel\mathbf{c}\) with full rank attention weights \(\mathbf{W}_{q},\mathbf{W}_{k},\mathbf{W}_{v}\)._

Proof.: If \(\mathbf{W}_{v}\mathbf{x}_{0}\parallel\mathbf{c}\), we can just set \(\mathbf{x}_{1}=\mathbf{x}_{0}\), which makes \(\texttt{{Att}}(\mathbf{x}_{0},[\mathbf{x}_{0},\mathbf{x}_{1}])\parallel \mathbf{c}\) hold.

If \(\mathbf{W}_{v}\mathbf{x}_{0}\parallel\mathbf{c}\), let \(\mathbf{v}=\alpha\mathbf{c}-\mathbf{W}_{v}\mathbf{x}_{0}\) where \(\alpha\in\mathbb{R}\). As \(\mathbf{W}_{v}\) is full-rank, we can find \(\mathbf{x}\) such that \(\mathbf{x}=\mathbf{W}_{v}^{-1}\mathbf{v}=\alpha\mathbf{W}_{v}^{-1}\mathbf{c}- \mathbf{x}_{0}\). Then we will have

\[\texttt{{Att}}(\mathbf{x}_{0},[\mathbf{x}_{0},\mathbf{x}_{1}])\] \[= \frac{\exp{((\mathbf{W}_{q}\mathbf{x}_{0})^{\top}(\mathbf{W}_{k} \mathbf{x}_{0}))}\mathbf{W}_{v}\mathbf{x}_{0}+\exp{((\mathbf{W}_{q}\mathbf{x} _{0})^{\top}(\mathbf{W}_{k}(\alpha\mathbf{W}_{v}^{-1}\mathbf{c}-\mathbf{x}_{0 })))}(\alpha\mathbf{c}-\mathbf{W}_{v}\mathbf{x}_{0})}{\exp{((\mathbf{W}_{q} \mathbf{x}_{0})^{\top}(\mathbf{W}_{k}\mathbf{x}_{0}))}+\exp{((\mathbf{W}_{q} \mathbf{x}_{0})^{\top}(\mathbf{W}_{k}(\alpha\mathbf{W}_{v}^{-1}\mathbf{c}- \mathbf{x}_{0})))}}\]

Therefore, as long as \(\mathbf{W}_{q}\mathbf{x}_{0}\not\perp\mathbf{W}_{k}(\mathbf{W}_{v}^{-1}\mathbf{c})\), we can change \(\alpha\) such that \(\texttt{{Att}}(\mathbf{x}_{0},[\mathbf{x}_{0},\mathbf{x}_{1}])=\beta\mathbf{W} _{v}\mathbf{x}_{0}+(1-\beta)(\alpha\mathbf{c}-\mathbf{W}_{v}\mathbf{x}_{0})\) where \(\beta=\frac{\exp{((\mathbf{W}_{q}\mathbf{x}_{0})^{\top}(\mathbf{W}_{k}\mathbf{ x}_{0}))}}{\exp{((\mathbf{W}_{q}\mathbf{x}_{0})^{\top}(\mathbf{W}_{k} \mathbf{x}_{0}))}+\exp{((\mathbf{W}_{q}\mathbf{x}_{0})^{\top}(\mathbf{W}_{k} \mathbf{x}_{0}))}}\). When \(\alpha=0\), \(\texttt{{Att}}(\mathbf{x}_{0},[\mathbf{x}_{0},\mathbf{x}_{1}])=\mathbf{W}_{v} \mathbf{x}_{0}\), when \(\alpha\to-\infty\) or \(\alpha\to\infty\), \(\texttt{{Att}}(\mathbf{x}_{0},[\mathbf{x}_{0},\mathbf{x}_{1}])\to\alpha\mathbf{ c}-\mathbf{W}_{v}\mathbf{x}_{0}\). As \(\texttt{{Att}}(\mathbf{x}_{0},[\mathbf{x}_{0},\mathbf{x}_{1}])\) is continuous w.r.t changing \(\alpha\), there must exist an \(\alpha\) such that \(\texttt{{Att}}(\mathbf{x}_{0},[\mathbf{x}_{0},\mathbf{x}_{1}])\parallel\mathbf{c}\). 

Pass the two input sequences \(\mathbf{X}_{1},\mathbf{X}_{2}\) through the attention layer Att with any prompt \(\mathbf{P}\), we can get the last output token as:

\[\texttt{{Att}}(\mathbf{x}_{0},[\mathbf{P},\mathbf{X}_{1}])= \lambda(\mathbf{X}_{1},\mathbf{x}_{0},[\mathbf{P},\mathbf{X}_{1}]) \texttt{{Att}}(\mathbf{x}_{0},\mathbf{X}_{1})+\lambda(\mathbf{P},\mathbf{x}_{0 },[\mathbf{P},\mathbf{X}_{1}])\texttt{{Att}}(\mathbf{x}_{0},\mathbf{P})\] (12) \[\texttt{{Att}}(\mathbf{x}_{0},[\mathbf{P},\mathbf{X}_{2}])= \lambda(\mathbf{X}_{2},\mathbf{x}_{0},[\mathbf{P},\mathbf{X}_{2}]) \texttt{{Att}}(\mathbf{x}_{0},\mathbf{X}_{2})+\lambda(\mathbf{P},\mathbf{x}_{0 },[\mathbf{P},\mathbf{X}_{2}])\texttt{{Att}}(\mathbf{x}_{0},\mathbf{P})\] (13)Here \(\lambda(\mathbf{X}_{1},\mathbf{x}_{2},\mathbf{X}_{3}=[\mathbf{X}_{1},\mathbf{X}_{2 }])\in(0,1)\) is a positive scalar, defined as

\[\lambda(\mathbf{X}_{1},\mathbf{x}_{2},\mathbf{X}_{3})=\frac{\sum_{j}\exp(( \mathbf{W}_{k}\mathbf{x}_{1j})^{\top}(\mathbf{W}_{q}\mathbf{x}_{2}))}{\sum_{j} \exp((\mathbf{W}_{k}\mathbf{x}_{3j})^{\top}(\mathbf{W}_{q}\mathbf{x}_{2}))}.\]

\(\mathbf{x}_{ij}\) is the \(j\)th token in \(\mathbf{X}_{i}\) for notation simplicity.

1. Then from equation 12, \(\texttt{Att}(\mathbf{x}_{0},\mathbf{P})\) must be on \(\texttt{{\sf Cone}}(-\texttt{{\sf Att}}(\mathbf{x}_{0},\mathbf{X}_{1}), \texttt{{\sf Att}}(\mathbf{x}_{0},[\mathbf{P},\mathbf{X}_{1}]))\) and \(\texttt{{\sf Cone}}(-\texttt{{\sf Att}}(\mathbf{x}_{0},\mathbf{X}_{2}), \texttt{{\sf Att}}(\mathbf{x}_{0},[\mathbf{P},\mathbf{X}_{2}]))\).
2. On the otherhand, as we want to memorize the two examples, we must have \(\texttt{{\sf Att}}(\mathbf{x}_{0},[\mathbf{P},\mathbf{X}_{1}])+\mathbf{x}_{0} \in\texttt{{\sf MLP}}^{-1}(\mathbf{y}_{10})\) and \(\texttt{{\sf Att}}(\mathbf{x}_{0},[\mathbf{P},\mathbf{X}_{2}])+\mathbf{x}_{0} \in\texttt{{\sf MLP}}^{-1}(\mathbf{y}_{20})\).

We construct the dataset \(S\) with arbitrary \(\mathbf{x}_{0},\mathbf{y}_{10}\) and \(\mathbf{y}_{20}\). Then if \(\text{dim}((\texttt{{\sf MLP}}^{-1}(\mathbf{y}_{10})-\mathbf{x}_{0})\cup( \texttt{{\sf MLP}}^{-1}(\mathbf{y}_{20})-\mathbf{x}_{0}))+2\leq d\) (Assumption 2), we can find two vectors \(\mathbf{c}_{1},\mathbf{c}_{2}\) such that \(\mathbf{c}_{1},\mathbf{c}_{2}\perp\mathbf{v}:\mathbf{v}+\mathbf{x}_{0}\in \texttt{{\sf MLP}}^{-1}(\mathbf{y}_{10})\) or \(\mathbf{v}+\mathbf{x}_{0}\in\texttt{{\sf MLP}}^{-1}(\mathbf{y}_{20})\) and \(\mathbf{c}_{1}\perp\mathbf{c}_{2}\). Then we can choose \(\mathbf{x}_{1},\mathbf{x}_{2}\) such that \(\texttt{{\sf Att}}(\mathbf{x}_{0},\mathbf{X}_{1})\parallel\mathbf{c}_{1}\) and \(\texttt{{\sf Att}}(\mathbf{x}_{0},\mathbf{X}_{2})\parallel\mathbf{c}_{2}\) (Lemma 7). Combine this construction with assumption 1, we have that \(\texttt{{\sf Cone}}(-\texttt{{\sf Att}}(\mathbf{x}_{0},\mathbf{X}_{1}), \texttt{{\sf Att}}(\mathbf{x}_{0},[\mathbf{P},\mathbf{X}_{1}]))\) and \(\texttt{{\sf Cone}}(-\texttt{{\sf Att}}(\mathbf{x}_{0},\mathbf{X}_{2}), \texttt{{\sf Att}}(\mathbf{x}_{0},[\mathbf{P},\mathbf{X}_{2}]))\) has no intersection, which means that we cannot find a \(\mathbf{P}\) to memorize this constructed dataset. 

### Proof of Lemma 5

For a standard single-layer transformer \(\tau\) defined in Definition 2 with \(r\geq n\) MLP hidden neurons, for any sequence-to-sequence dataset \(S\) satisfying Assumptions 1, we can apply a low-rank update to MLP weights with \(O(nd)\) parameters to memorize \(\tau(\mathbf{X}_{i})_{:,m}=\mathbf{y}_{im}\).

Proof.: We use \(\texttt{{\sf MLP}}_{j}(\mathbf{x})\) to denote the \(jth\) output of the MLP layer for an input token \(\mathbf{x}\), which is

\[\texttt{{\sf MLP}}_{j}(\mathbf{x})=x_{j}+b_{2,j}+\sum_{k=1}^{m}w_{k,j}\max( \langle\mathbf{a}_{k},\mathbf{x}\rangle+b_{1,k},0)\]

According to our assumption, \(\texttt{{\sf Att}}(\mathbf{x}_{im},\mathbf{X}_{i})\) are unique vectors for \(i=1,2,...,n\). Then we only need to use the MLP layer to map each \(\mathbf{x}_{i}=\texttt{{\sf Att}}(\mathbf{x}_{im},\mathbf{X}_{i})+\mathbf{x}_{im}\) to \(\mathbf{y}_{im}\), where we get a new token-wise dataset \(\{(\mathbf{x}_{1},\mathbf{y}_{1}),(\mathbf{x}_{2},\mathbf{y}_{2}),...,( \mathbf{x}_{n},\mathbf{y}_{n})\}\)

Then we need to find \(w_{k}\), \(\mathbf{a}_{k}\) and \(b_{k}\) such that

\[\texttt{{\sf MLP}}_{j}(\mathbf{x}_{i})=x_{i,j}+b_{2,j}+\sum_{k=1}^{m}w_{k,j} \max(\langle\mathbf{a}_{k},\mathbf{x}_{i}\rangle+b_{1,k},0)=y_{i,j},i=1,2,...,n,j=1,2,...,d\] (14)

, which is equivalent to constructing a standard MLP to memorize a dataset:

\[\sum_{k=1}^{n}w_{k,j}\max(\langle\mathbf{a}_{k},\mathbf{x}_{i}\rangle+b_{1,k},0 )=y_{i,j}-x_{i,j}-\sum_{k=n+1}^{m}w_{k,j}\max(\langle\mathbf{a}_{k},\mathbf{x}_{ i}\rangle+b_{1,k},0)-b_{2,j}\] (15)

Follow Thoerem 1 in Yun et al. (2019), we can construct \(\mathbf{a},b_{1},...,b_{n}\) such that for \(\mathbf{x}_{1},\mathbf{x}_{2},...,\mathbf{x}_{n}\), we have \(z_{i}=\langle\mathbf{a},\mathbf{x}_{i}\rangle\), \(b_{1}<z_{1}<b_{2}<...<b_{n}<z_{n}\). Then we can find \(w_{1},...,w_{n}\) which solves equation 15. For \(d\)-dimension output, we need to find \(\mathbf{W}\in\mathbb{R}^{n\times d}\) and \(\mathbf{a}\in\mathbb{R}^{d}\) and \(\mathbf{b}\in\mathbb{R}^{n}\). With LoRA, we need a low-rank update of size \(m\times n+n\times d\) for \(\mathbf{W}_{2}\), a low-rank update of size \(d\times n+n\times m\) for \(\mathbf{W}_{1}\) and an update of size \(n\) for \(\mathbf{b}_{1}\), which is \(O(n\times d)\) in total. Normally we have \(m\simeq d\), then we need an update with parameter size around \((4n+1)d\) to memorize the last token of \(n\) training examples. 

### Proof of Theorem 3

For any single layer transformer \(\tau\) defined in Definition 2, there exists a seq-to-seq dataset \(\{(\mathbf{X}_{1}=[\mathbf{x}_{10},\mathbf{x}_{1}],[\mathbf{y}_{10},\mathbf{y}_{ 11}]),(\mathbf{X}_{2}=[\mathbf{x}_{20},\mathbf{x}_{2}],[\mathbf{y}_{20},\mathbf{ y}_{21}]),...,(\mathbf{X}_{n}=[\mathbf{x}_{n0},\mathbf{x}_{n}],[\mathbf{y}_{n0}, \mathbf{y}_{n1}])\}\) that satisfies Assumption 1 with \(n<d\) training examples such that we need at least \(n\) prompt tokens in \(\mathbf{P}\) to memorize the training set, ie, for \(\tau([\mathbf{P},\mathbf{X}_{i}])_{:,-1}=\mathbf{y}_{i1}\) to hold for all \(i=1,2,...,n\).

Proof.: Without loss of generality, we assume \(\mathbf{W}_{2}\) has no zero elements, otherwise we can just ignore this hidden neuron in MLP layer.

\(\mathbb{R}^{d}\) has \(d\) bases \(\{\mathbf{t}_{j}:j=1,2,...,d\}\), then \(\texttt{MLP}^{-1}(\mathbf{y}_{i1})\) must be bounded on either positive or negative part of these \(d\) directions, which means there exists \(B\geq 0\) such that

\[\frac{\mathbf{v}^{\top}\mathbf{t}_{j}}{\|\mathbf{t}_{j}\|}\leq B,\forall \mathbf{v}\in\texttt{MLP}^{-1}(\mathbf{y}_{i1}),j=1,2,...,d\]

Otherwise \(\forall B>0\), \(\exists\mathbf{t}_{j}\), we can find a \(\mathbf{v}\in\texttt{MLP}^{-1}(\mathbf{y}_{i})\) that \(\frac{\mathbf{v}^{\top}\mathbf{t}_{j}}{\|\mathbf{t}_{j}\|}\geq B\). Meanwhile we have \(\texttt{MLP}(\mathbf{v})=\mathbf{v}+\mathbf{b}_{2}+\mathbf{W}_{2}\texttt{ ReLU}(\mathbf{W}_{1}\mathbf{v}+\mathbf{b}_{1})\). As \(\|\mathbf{v}\|\) can be arbitrarily large, if \(\mathbf{W}_{1}\mathbf{v}=\mathbf{0}\), \(\|\texttt{MLP}(\mathbf{v})\|\rightarrow\infty\) if \(\|\mathbf{v}\|\rightarrow\infty\). if \(\mathbf{W}_{1}\mathbf{v}\neq\mathbf{0}\), \(\|\texttt{MLP}(\mathbf{v})\|\) can also be arbitrarily large when increasing the norm of \(\mathbf{v}\) due to the non-linearity of \(\texttt{ReLU}(\mathbf{W}_{1}\mathbf{v}+\mathbf{b}_{1})\).

Then we can find a set of \(n\) linearly independent vectors \(\{\mathbf{c}_{1},\mathbf{c}_{2},...,\mathbf{c}_{n}\}\) such that \(\{\mathbf{a}_{i}:\mathbf{a}_{i}-\mathbf{c}_{i}\perp\mathbf{c}_{i},\mathbf{a}_{ i}\in\texttt{MLP}^{-1}(\mathbf{y}_{i1})\}=\emptyset\) by enlarging the norm of \(\mathbf{c}_{i}\). With the \(n\)\(\mathbf{c}_{i}\) vectors, we can begin to construct our dataset:

We set \(\mathbf{x}_{i}=\mathbf{c}_{i},i=1,2,...,n\) and find \(\mathbf{x}_{i0}\) such that \(\mathbf{c}_{i}\perp\texttt{Att}(\mathbf{x}_{i},\mathbf{X}_{i})\) (Lemma 7) and \(\texttt{Att}(\mathbf{x}_{i},\mathbf{X}_{i})\) are distinct for \(i=1,2,...,n\) (Assumption 1), which makes \(\{\mathbf{a}_{1}-\mathbf{x}_{1}-\lambda(\mathbf{X}_{1},\mathbf{x}_{1},[\mathbf{ P},\mathbf{X}_{1}])\texttt{Att}(\mathbf{x}_{1},\mathbf{X}_{1}),...,\mathbf{a}_{n}- \lambda(\mathbf{X}_{n},\mathbf{x}_{n},[\mathbf{P},\mathbf{X}_{n}])\texttt{ Att}(\mathbf{x}_{n},\mathbf{X}_{n})\}\) linearly independent for any \(\mathbf{a}_{i}\in\texttt{MLP}^{-1}(\mathbf{y}_{i1})\). Here \(\lambda(\cdot,\cdot,\cdot)\) is the same as defined in Section C.6.

Moreover, we have

\[\texttt{Att}(\mathbf{x}_{i},[\mathbf{P},\mathbf{X}_{i}]) =\lambda(\mathbf{X}_{i},\mathbf{P},[\mathbf{P},\mathbf{X}_{i}]) \texttt{Att}(\mathbf{x}_{i},\mathbf{P})+\lambda(\mathbf{X}_{i},\mathbf{x}_{i },[\mathbf{P},\mathbf{X}_{i}])\texttt{Att}(\mathbf{x}_{i},\mathbf{X}_{i})\] (16) \[\in\texttt{MLP}^{-1}(\mathbf{y}_{i1})-\mathbf{x}_{i}\]

Then \(\texttt{Att}(\mathbf{x}_{i},\mathbf{P}),i=1,2,...,n\) must be \(n\) linearly independent vectors, which requires

\[\texttt{rank}(\mathbf{W}_{v}\mathbf{P}\mathbf{A})=n,\] (17)

where \(\mathbf{A}\in\mathbb{R}^{m_{p}\times n}\) is the attention score matrix between \(\mathbf{x}_{i}\) and \(\mathbf{P}\). \(\mathbf{P}\in\mathbb{R}^{d\times m_{p}}\) is the prompt token sequence and \(\mathbf{W}_{v}\) is the attention value weight. Therefore, we must have \(m_{p}\geq n\). 

### Proof of Theorem 4

A transformer \(\mathcal{T}\) is invertible if:

1. The Lipschitz constant of the attention block in each layer \(\tau^{l}\) is _strictly_ less than 1
2. The Lipschitz constant of the 2-layer ReLU block in each layer \(\tau^{l}\), which is bounded by \(\|\mathbf{W}_{2}^{l}\|_{2}\times\|\mathbf{W}_{1}^{l}\|_{2}\), is _strictly_ less than 1

Proof.: This proof is based on the proof provided for lemma 4, thus we restrict ourselves to the sketch: Based on the sufficient condition for invertibility in Behrmann et al. (2019), condition (1) implies that the attention block (eq 1) with the residual connection, ie \(\mathbf{X}+\texttt{Att}(\mathbf{X},\mathbf{X})\), is an invertible function. Similarly, condition (2) implies that the MLP block which constitutes of the 2-layer ReLU block with the residual connection (eq 2) also exhibit invertibility.

Thus each transformer layer \(\tau^{l}\) (eq 3) is invertible by noting that its a composition of two invertible functions. The same property ensures that the entire transformer architecture \(\mathcal{T}\) is also invertible. 

### Proof of Lemma 6

Under the compactness condition, the Lipschitz constant of the \(i\)-th attention head in the \(l\)-th transformer layer, denoted for simplicity as \(\texttt{Att}^{i,l}\), admits the following bound w.r.t the entire input sequence of length \(m\):

\[Lip(\texttt{Att}^{i,l}(\cdot,\cdot))\leq(1+8\sqrt{m}(D^{l})^{2}\|(\mathbf{W}_{ k}^{i,l})^{T}\mathbf{W}_{q}^{i,l}\|_{2})\|\mathbf{W}_{v}^{i,l}\|_{2},\] (18)

and the Lipschitz constant of the entire attention block in layer \(l\), denoted as \(\texttt{Att}^{l}\), admits the bound:

\[Lip(\texttt{Att}^{l}(\cdot,\cdot))\leq\sqrt{\sum_{i=1}^{h}(\|\mathbf{W}_{o}^{i, l}\|_{2}\times Lip(\texttt{Att}^{i,l}))^{2}}.\] (19)Proof.: We drop the superscripts \(i,l\) in the proof to avoid notation clutter. Similarly, we denote the concatenation of the prompt matrix \(\mathbf{P}\) and the original input matrix \(\mathbf{X}\), simply with \(\mathbf{X}\).

Derivation for single head eq 18:Consider two matrices \(\mathbf{X}_{1},\mathbf{X}_{2}\in\mathcal{X}=\{\mathbf{X}\in\mathbb{R}^{d\times m };\|\mathbf{X}\|_{2}\leq D\}\). Denote with \(\mathbf{A}_{1},\mathbf{A}_{2}\) the corresponding attention matrices respectively, which can be defined as:

\[\mathbf{A}_{1} =\sigma((\mathbf{W}_{k}\mathbf{X}_{1})^{\top}\mathbf{W}_{q} \mathbf{X}_{1})\] \[\mathbf{A}_{2} =\sigma((\mathbf{W}_{k}\mathbf{X}_{2})^{\top}\mathbf{W}_{q} \mathbf{X}_{2})\] (20)

The output of the attention head, denoted with \(Att(\cdot)\) admits the following:

\[\|Att(\mathbf{X}_{1})-Att(\mathbf{X}_{2})\|_{2}=\|\mathbf{W}_{v} \mathbf{X}_{1}\mathbf{A}_{1}-\mathbf{W}_{v}\mathbf{X}_{2}\mathbf{A}_{2}\|_{2}\] (21) \[\stackrel{{ a}}{{\leq}}\|\mathbf{X}_{1}\mathbf{A}_{ 1}-\mathbf{X}_{2}\mathbf{A}_{2}\|_{2}\|\mathbf{W}_{v}\|_{2}\] (22) \[=\|\mathbf{X}_{1}\mathbf{A}_{1}-\mathbf{X}_{2}\mathbf{A}_{1}+ \mathbf{X}_{2}\mathbf{A}_{1}-\mathbf{X}_{2}\mathbf{A}_{2}\|_{2}\|\mathbf{W}_{ v}\|_{2}\] (23) \[\leq(\|\mathbf{A}_{1}\|_{2}\|\mathbf{X}_{1}-\mathbf{X}_{2}\|_{2}+ \|\mathbf{X}_{2}\|_{2}\|\mathbf{A}_{1}-\mathbf{A}_{2}\|_{2})\|\mathbf{W}_{v}\| _{2}\] (24) \[\stackrel{{ b}}{{\leq}}(\|\mathbf{X}_{1}-\mathbf{X}_{ 2}\|_{2}+\|\mathbf{A}_{1}-\mathbf{A}_{2}\|_{2}D)\|\mathbf{W}_{v}\|_{2}\] (25)

where \((a)\) holds from the spectral norm properties and in \((b)\) we use the bounded input spectral norm assumptions.

We now focus on the second term \(\|\mathbf{A}_{1}-\mathbf{A}_{2}\|_{2}\) in eq 25. From the bound in lemma 9, we have:

\[\|\mathbf{A}_{1}-\mathbf{A}_{2}\|_{2}\leq 2\sqrt{m}\|\mathbf{G}\|_{2}\] (26)

where \(\mathbf{G}\) is the diagonal matrix with entires described in lemma 8

We can now invoke lemma 10 to obtain the following :

\[\|\mathbf{A}_{1}-\mathbf{A}_{2}\|_{2}\leq 2\sqrt{m}\times 2\times 2\|\mathbf{ W}_{k}^{T}\mathbf{W}_{q}\|_{2}D\times\|\mathbf{X}_{1}-\mathbf{X}_{2}\|_{2}\] (27)

Combining the previous inequality with eq 25, we have the following bound:

\[\|Att(\mathbf{X}_{1})-Att(\mathbf{X}_{2})\|_{2}\leq(1+8\sqrt{m}\|\mathbf{W}_{ k}^{T}\mathbf{W}_{q}\|_{2}D^{2})\|\mathbf{W}_{v}\|_{2}\|\mathbf{X}_{1}- \mathbf{X}_{2}\|_{2}\] (28)

Derivation for the entire block eq 11:The proof follows simply by leveraging the following property:

_Property:_ for a matrix \(\mathbf{C}=[\mathbf{A},\mathbf{B}]\), the spectral norm of \(\mathbf{C}\) admits the bound:

\[\|\mathbf{C}\|_{2}\leq\sqrt{\|\mathbf{A}\|_{2}^{2}+\|\mathbf{B}\|_{2}^{2}}\]

We then simply combine the definition of the attention block and the lipschitz constant bound in eq 18 with the above property in order to obtain the desired bound. 

**Lemma 8** (Dong et al. [2021] Lemma A.1).: _For the column stochastic matrix \(\mathbf{A}_{1}\) obtained by performing column-wise softmax of some matrix \(\mathbf{Z}_{1}\) (where in our setting \(\mathbf{Z}_{1}=(\mathbf{W}_{k}\mathbf{X}_{1})^{\top}\mathbf{W}_{q}\mathbf{X}_{1}\), and another row stochastic matrix \(\mathbf{A}_{2}\) obtained by performing column-wise softmax of some matrix \(\mathbf{Z}_{2}\), where \(\mathbf{Z}_{2}=\mathbf{Z}_{1}-\mathbf{E}\) (for some \(\mathbf{E}\), which **need not** belong to \(\mathcal{X}\)), we have the following bound:_

\[\mathbf{A}_{2}(\mathbf{I}-\mathbf{G})\leq\mathbf{A}_{1}\leq\mathbf{A}_{2}( \mathbf{I}+2\mathbf{G})\] (29)

_where the inequality is elementwise and \(\mathbf{G}\) is a diagonal matrix with entries as \(\mathbf{G}_{ii}=\max_{j,j^{\prime}}|\delta_{i}^{T}\mathbf{E}(\delta_{j}^{T}- \delta_{j^{\prime}}^{T})|\). Here \(\delta_{k}\) is a one-hot vector with the entry \(1\) in the \(k^{th}\) dimension._

**Lemma 9**.: _Following the notations of lemma 8, we have the following spectral norm bound:_

\[\|\mathbf{A}_{1}-\mathbf{A}_{2}\|_{2}\leq 2\sqrt{m}\|\mathbf{G}\|_{2}\] (30)

Proof.: We begin by noting the following entry-wise inequality from eq 29:

\[\mathbf{A}_{2}\mathbf{G}\leq\mathbf{A}_{1}-\mathbf{A}_{2}\leq 2\mathbf{A}_{2} \mathbf{G}\] (31)which ensures that \(\|\mathbf{A}_{1}-\mathbf{A}_{2}\|_{F}\leq 2\|\mathbf{A}_{2}\mathbf{G}\|_{F}\).

We also have the following using matrix norm equivalence:

\[\|\mathbf{A}_{1}-\mathbf{A}_{2}\|_{2}\leq\|\mathbf{A}_{1}-\mathbf{A}_{2}\|_{F}\] (32)

Invoking the matrix norm equivalence again, we have that

\[2\|\mathbf{A}_{2}\mathbf{G}\|_{F}\leq 2\sqrt{rank(\mathbf{A}_{2}\mathbf{G})}\| \mathbf{A}_{2}\mathbf{G}\|_{2}\] (33)

where \(rank(\cdot)\) is the matrix rank.

Combining the inequalities, we attain the bound :

\[\|\mathbf{A}_{1}-\mathbf{A}_{2}\|_{2}\leq 2\sqrt{m}\|\mathbf{A}_{2}\|_{2}\| \mathbf{G}\|_{2}\] (34)

since \(\mathbf{A}_{2}\) is column-stochastic, \(\|\mathbf{A}_{2}\|_{2}=1\) 

**Lemma 10**.: _The term \(\mathbf{G}\) in lemma 9 admits the following spectral norm bound:_

\[\|\mathbf{G}\|_{2}\leq 2D\|\mathbf{W}_{q}\mathbf{W}_{k}^{T}\|_{2}\|\mathbf{X}_ {1}-\mathbf{X}_{2}\|_{2}\] (35)

_here \(D\) is the previously stated spectral norm bound of the inputs \(\mathbf{X}^{l}\in\mathcal{X}^{l}\)._

Proof.: We begin by noting that since \(\mathbf{G}\) is a square diagonal matrix with non-negative real values, the singular values of \(\mathbf{G}\) are the corresponding diagonal elements.

We thus have that \(\|\mathbf{G}\|_{max}=\|\mathbf{G}\|_{2}\), where \(\|\cdot\|_{max}\) is the \(max\) norm.

Since \(\mathbf{G}\) admits the form described in lemma 8, it is trivial to note that:

\[\|\mathbf{G}\|_{max} =\max_{i,j,i^{\prime},j^{\prime}}|\mathbf{E}_{i,j}-\mathbf{E}_{i^ {\prime},j^{\prime}}|\] (36) \[\leq 2\|\mathbf{E}\|_{max}\leq 2\|\mathbf{E}\|_{2}\] (37)

where the second inequality follows from the matrix norm equivalence.

Now, we can bound the last term \(\|\mathbf{E}\|_{2}\) by noting that the inputs \(\mathbf{X}\) belong to a bounded set. This allows us to provide the following bounds:

\[\|\mathbf{E}\|_{2} =\|(\mathbf{W}_{k}\mathbf{X}_{1})^{\top}\mathbf{W}_{q}\mathbf{X}_ {1}-(\mathbf{W}_{k}\mathbf{X}_{2})^{\top}\mathbf{W}_{q}\mathbf{X}_{2}\|_{2}\] (38) \[=\|(\mathbf{W}_{k}\mathbf{X}_{1})^{\top}\mathbf{W}_{q}\mathbf{X}_ {1}-(\mathbf{W}_{k}\mathbf{X}_{1})^{\top}\mathbf{W}_{q}\mathbf{X}_{2}+( \mathbf{W}_{k}\mathbf{X}_{1})^{\top}\mathbf{W}_{q}\mathbf{X}_{2}-(\mathbf{W}_ {k}\mathbf{X}_{2})^{\top}\mathbf{W}_{q}\mathbf{X}_{2}\|_{2}\] (39) \[\leq(\|\mathbf{X}_{1}\|_{2}\|\mathbf{W}_{k}^{T}\mathbf{W}_{q}\|_{ 2}+\|\mathbf{X}_{2}\|_{2}\|\mathbf{W}_{k}^{T}\mathbf{W}_{q}\|_{2})\|\mathbf{X }_{1}-\mathbf{X}_{2}\|_{2}\] (40) \[\leq 2D\|\mathbf{W}_{k}^{T}\mathbf{W}_{q}\|_{2}\|\mathbf{X}_{1}- \mathbf{X}_{2}\|_{2}\] (41)

### Extension of Lemma 6

Lemma 6 and theorem 4 operate over functions from \(\mathcal{P}^{1}\times\mathcal{X}^{1}\rightarrow\mathcal{P}^{L+1}\times \mathcal{X}^{L+1}\). We can relax the requirement of the prompt and provide the Lipschitz constant upper bound in consideration to functions of the form \(\mathcal{X}^{1}\rightarrow\mathcal{X}^{L+1}\) by using the following assumption:

**Assumption 3**.: _Assume for simplicity that \(\|\mathbf{P}_{1}^{l}-\mathbf{P}_{2}^{l}\|_{2}\leq\alpha^{l}\|\mathbf{X}_{1}^{ l}-\mathbf{X}_{2}^{l}\|_{2};\forall l\geq 1\). \(\alpha^{l}=0\) when \(l=1\)._

**Note:** A recursive expression for \(\alpha^{l}\) in the above assumption can be provided, but the expression does not admit a simplified form and we thus omit it here.

We will use \(\mathcal{D}_{X}^{l}\), akin to eq 9, to denote the compactness corresponding to the input matrix across the layers.

Based on this assumption, we have the following Lipschitz constant upper bound:

**Lemma 11**.: _The Lipschitz constant of the single head \(\texttt{Att}^{i,l}\) admits the following bound w.r.t the input part, \(\mathbf{X}^{1}\) of length \(m_{X}\), of the input sequence:_

\[Lip(\texttt{Att}^{i,l}(\cdot,\cdot))\leq\left(\sqrt{1+(\alpha^{l})^{2}}+8 \sqrt{m_{X}}(D_{X}^{l})^{2}(1+(\alpha^{l})^{2})\|(\mathbf{W}_{k}^{i,l})^{T} \mathbf{W}_{q}^{i,l}\|_{2}\right)\|\mathbf{W}_{v}^{i,l}\|_{2}\] (42)

_For \(l=1\), \(\alpha^{l}=0\) in the above bound. The Lipschitz constant of the entire attention block in layer \(l\) follows similarly._Proof.: For some first layer input \(\mathbf{X}_{1}^{l}\) and prompt \(\mathbf{P}\), let us denote the direct output of the attention head in the \(l\)-th layer with \(\overrightarrow{\mathbf{X}}_{1}^{l}\). We have the following update rule for \(\overrightarrow{\mathbf{X}}_{1}^{l}\):

\[\overrightarrow{\mathbf{X}}_{1}^{l}=\mathbf{W}_{v}^{i,l}[\mathbf{P}_{1}^{l}, \mathbf{X}_{1}^{l}]\cdot\sigma((\mathbf{W}_{k}^{i,l}[\mathbf{P}_{1}^{l}, \mathbf{X}_{1}^{l}])^{\top}\mathbf{W}_{q}^{i}\mathbf{X}_{1}^{l})=\mathbf{W}_{v }^{i,l}[\mathbf{P}_{1}^{l},\mathbf{X}_{1}^{l}]\cdot\mathbf{A}_{1}^{l}\] (43)

Here, \(\mathbf{P}_{1}^{l}\) is the updated prompt matrix w.r.t the input. For two different inputs \(\mathbf{X}_{1}^{1}\) and \(\mathbf{X}_{2}^{1}\) at the first layer, \(\mathbf{P}_{1}^{l}=\mathbf{P}_{2}^{1}=\mathbf{P}\), since the prompt is same across all inputs. \(\mathbf{A}_{1}^{l}\) is then simply the corresponding column-stochastic matrix.

With the context clear, we now drop the superscripts \(i,l\), as done previously. For \(\|\overrightarrow{\mathbf{X}}_{1}-\overrightarrow{\mathbf{X}}_{2}\|_{2}\), we have:

\[\|\overrightarrow{\mathbf{X}}_{1}-\overrightarrow{\mathbf{X}}_{2}\|_{2}\leq \|[\mathbf{P}_{1},\mathbf{X}_{1}]\mathbf{A}_{1}-[\mathbf{P}_{2},\mathbf{X}_{ 2}]\mathbf{A}_{2}\|_{2}\|\mathbf{W}_{v}\|_{2}\]

\[\leq\left(\|\mathbf{A}_{1}\|_{2}\sqrt{\|\mathbf{P}_{1}-\mathbf{P}_{2}\|_{2}^{ 2}+\|\mathbf{X}_{1}-\mathbf{X}_{2}\|_{2}^{2}}+\sqrt{\|\mathbf{P}_{2}\|_{2}^{2} +\|\mathbf{X}_{2}\|_{2}^{2}}\|\mathbf{A}_{1}-\mathbf{A}_{2}\|_{2}\right)\| \mathbf{W}_{v}\|_{2}\] (44)

where the second inequality is attained using the property of spectral norm of concatenated matrices.

We now consider the term \(\|\mathbf{A}_{1}-\mathbf{A}_{2}\|_{2}\). By invoking lemmas 9 and 10, we have that:

\[\|\mathbf{A}_{1}-\mathbf{A}_{2}\|_{2}\leq 2\sqrt{m_{X}}\times 2\|\mathbf{E} \|_{2}\]

\[\text{where }\ \mathbf{E}=(\mathbf{W}_{k}[\mathbf{P}_{1},\mathbf{X}_{1}])^{ \top}\mathbf{W}_{q}\mathbf{X}_{1}-(\mathbf{W}_{k}[\mathbf{P}_{2},\mathbf{X}_{ 2}])^{\top}\mathbf{W}_{q}\mathbf{X}_{2}\] (45)

Invoking assumption 3, \(\|\mathbf{E}\|_{2}\) can further be bounded as:

\[\|\mathbf{E}\|_{2}\leq 2D_{X}\sqrt{1+\alpha^{2}}\|(\mathbf{W}_{k})^{T}\mathbf{W} _{q}\|_{2}\] (46)

Finally, by combining the bound for \(\|\mathbf{E}\|_{2}\) and assumption 3 with eq 44, we obtain:

\[\|\overrightarrow{\mathbf{X}}_{1}-\overrightarrow{\mathbf{X}}_{2} \|_{2}\] (47) \[\leq\left(\sqrt{1+\alpha^{2}}+D_{X}\sqrt{1+\alpha^{2}}\times 8 \sqrt{m_{X}}D_{X}\sqrt{1+\alpha^{2}}\|(\mathbf{W}_{k})^{T}\mathbf{W}_{q}\|_{2} \right)\|\mathbf{W}_{v}\|_{2}\|\mathbf{X}_{1}-\mathbf{X}_{2}\|_{2}\] (48)

which provides us the desired bound. 

By setting \(\alpha^{l}=0\), the case when there is no prompt, we obtain a similar bound as lemma 6