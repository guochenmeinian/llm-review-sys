# UniMoT: Unified Molecule-Text Language Model with Discrete Token Representation

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

The remarkable success of Large Language Models (LLMs) across diverse tasks has driven the research community to extend their capabilities to molecular applications, leading to the development of molecular LLMs. However, most molecular LLMs employ adapter-based architectures that do not treat molecule and text modalities equally and lack a supervision signal for the molecule modality. To address these issues, we introduce **UniMoT**, a unified molecule-text LLM adopting a tokenizer-based architecture that expands the vocabulary of LLM with molecule tokens. Specifically, we introduce a Vector Quantization-driven tokenizer that incorporates a Q-Former to bridge the modality gap between molecule and text. This tokenizer transforms molecules into sequences of molecule tokens with causal dependency, encapsulating high-level molecular and textual information. Equipped with this tokenizer, UniMoT can unify molecule and text modalities under a shared token representation and an autoregressive training paradigm, enabling it to interpret molecules as a foreign language and generate them as text. Following a four-stage training scheme, UniMoT emerges as a multi-modal generalist capable of performing both molecule-to-text and text-to-molecule tasks. Extensive experiments demonstrate that UniMoT achieves state-of-the-art performance across a wide range of molecule comprehension and generation tasks.

## 1 Introduction

The incredible capabilities of Large Language Models (LLMs) [5; 44] have led to their widespread use as versatile tools for completing diverse real-world tasks. This success has sparked interest in Multi-modal LLMs [59; 52], which aim to enhance LLMs by enabling them to process multi-modal inputs and outputs. Prior research efforts [26; 41; 12; 6; 33; 35; 25] have focused on adapting LLMs to molecular tasks, resulting in the development of molecular LLMs. These molecular LLMs can analyze molecule structures [35; 33; 6], address drug-related inquiries [26; 41], assist in synthesis and retrosynthesis planning [12], support drug design [12], and more.

Prevalent molecular LLMs commonly employ adapter-based architectures, adopting either a linear projection [26; 41; 6] or a Q-Former [33; 25] as an adapter to translate molecule features into the semantic space of LLM, as illustrated in Figure 0(a) and Figure 0(b). Despite demonstrating initial capabilities in molecular comprehension and yielding promising results in molecule-to-text generation tasks, they still lack molecule generation abilities. The critical issue within these methods is their unequal treatment of molecules and text, resulting in a lack of supervision for the molecule modality. This limitation significantly constrains model capacity and effectiveness. Due to limitations imposed by the training paradigm, they are unable to perform text-to-molecule generation tasks.

Discretizing continuous molecule features into discrete molecule tokens offers a promising solution for conducting both molecule-to-text and text-to-molecule generation tasks. By treating tokens fromdifferent modalities equally, we can predict the next molecule or text token in an autoregressive manner. However, directly discretizing molecule features poses several challenges: (i) This approach results in long sequences, with lengths equivalent to the number of atoms in a batch. LLMs typically experience a quadratic increase in computational complexity with sequence length [46]. (ii) Molecule tokens derived from molecule features lack left-to-right causal dependency, which conflicts with the unidirectional attention mechanism in LLMs. (iii) Molecule features lack textual information, hindering effective molecule-text interactions and alignment.

To this end, we present **UniMoT**, a unified molecule-text LLM that adopts a tokenizer-based architecture, integrating molecule comprehension and generation, as depicted in Figure 0(c). A pivotal aspect of UniMoT's architecture is the molecule tokenizer for transforming molecules into molecule tokens. We introduce a Vector Quantization-driven [45] tokenizer, incorporating a Q-Former [23] to bridge the modality gap between molecule and text. Specifically, we incorporate causal masks for the queries, enabling the Causal Q-Former to generate a causal sequence of query embeddings compatible with the unidirectional attention in LLMs. The sequence of query embeddings is subsequently quantized into a sequence of molecule tokens using a learnable codebook. The molecule tokens encapsulate high-level molecular and textual information, which are then aligned with the latent space of a generative model via an MLP adapter, enabling the generation of desired molecules.

Pretrained LLMs can integrate the molecule tokenizer by treating molecule tokens as new words and constructing a molecule vocabulary through mapping the learned codebook. We adopt the unified discrete token representation for molecules and text, coupled with the unified next-token-prediction training paradigm of LLM. This unification of representation and training paradigm enhances LLMs' ability to understand molecule-text interactions and alignment. UniMoT interprets molecules akin to understanding a foreign language, and generates them as if they were text. Following a four-stage training scheme, UniMoT serves as a multi-modal generalist capable of performing both molecule comprehension and generation tasks.

Our contributions can be summarized as follows:

* We introduce a molecule tokenizer specifically designed for LLMs, enabling the tokenization of molecules into short sequences of molecule tokens with causal dependency. These tokens encapsulate high-level molecular and textual information and can be decoded into desired molecules during inference.
* We present UniMoT, a unified molecule-text LLM that adopts a tokenizer-based architecture instead of traditional adapter-based architectures. UniMoT unifies the modalities of molecule and text under a shared token representation and an autoregressive training paradigm.
* UniMoT exhibits remarkable capabilities in multi-modal comprehension and generation. Extensive experiments demonstrate that UniMoT achieves state-of-the-art performance across a wide spectrum of molecule comprehension tasks and molecule generation tasks.

Figure 1: Comparisons among different molecular LLMs. 0(a) and 0(b) are adapter-based architectures that do not treat molecule and text modalities equally and lack a supervision signal for the molecule modality. 0(c) is our proposed tokenizer-based architecture, where molecules are presented in the same discrete token representation as that of text. Molecules and text can be optimized under a unified next-token-prediction objective.

Related Works

Molecular Large Language Models.The recent emergence of Vision Large Language Models (VLLMs) [24; 23; 28] has catalyzed advancements in Molecular LLMs, which encompass both single modality and multi-modality approaches. In the single modality domain, researchers are exploring diverse molecule representations, such as 1D sequences like SMILES strings [47; 8; 17], 2D molecule graphs [15; 56], 3D geometric conformations [56; 32], and textual information from the literature [43; 2; 21]. In the multiple modalities domain, various innovative approaches are being employed. MoIT5 [11], a T5-based [38] model, is designed for SMILES-to-text and text-to-SMILES translations. Other works, such as MoMu [39], MoleculeSTM [31], MoFM [34], and GIT-Mol [29], leverage cross-modal contrastive learning to align the representation spaces of molecules and text. Additionally, some studies use multi-modal learning architectures to develop molecular LLMs, which often adopt adapter-based architectures. For instance, InstructMol [6], GraphGPT [41], and DrugChat [26] employ a simple projection layer to map molecule features to LLM's input space. MolCA [33] and 3D-MoLM [25] utilize a Q-Former [23] to bridge the modality gap between molecules and text. However, these methods do not treat molecule and text modalities equally and lack a supervision signal for the molecule modality, limiting model capacity and effectiveness.

Vector Quantization.Vector Quantization (VQ) [13] is a widely used technique in generative models. VQ-VAE [45] converts an image into a set of discrete codes within a learnable discrete latent space by learning to reconstruct the original image. VQ-GAN [57] enhances the generation quality by leveraging adversarial and perceptual objectives. In the context of molecules, VQ has been effectively applied to quantize molecule representations. For example, DGAE [4] introduces a VQ model specifically for molecular graphs, where molecular graphs are encoded into discrete latent codes. Mole-BERT [54] uses VQ to rethink the pre-training of GNNs for molecular tasks. IMoLD [60] proposes using VQ to enhance invariant molecule representations, and VQSynergy [51] demonstrates the use of VQ for drug discovery.

## 3 Method

Our objective is to leverage the reasoning and generation capabilities of LLMs to enhance the comprehension and generation of molecule and text data. To achieve this, we focus on representing these modalities uniformly within the token representation, utilizing the next-token-prediction training paradigm of LLMs. As illustrated in Figure 2, we introduce a molecule tokenizer (Section 3.1) designed to transform molecules into molecule tokens by learning to reconstruct the input molecule. The molecule sequence can then be concatenated with the text sequence to form a multi-modal sequence, which is subsequently fed into an LLM for autoregressive pretraining (Section 3.2), as illustrated in Figure 3. The LLM vocabulary is expanded with molecule codes mapped from the learned codebook. We introduce a four-stage training scheme for UniMoT (Section 3.3) comprising Causal Q-Former pretraining, molecule tokenizer pretraining, unified molecule-text pretraining, and task-specific instruction tuning. UniMoT is capable of performing both molecular comprehension and generation tasks following the training scheme.

### Molecule Tokenizer for LLMs

Molecule encoder.We represent the structural information of a molecule as a graph, denoted by \(\mathcal{G}=(\mathcal{V},\mathcal{E})\), where \(\mathcal{V}\) is the set of atoms and \(|\mathcal{V}|=N\) is the number of atoms. The task of the molecule encoder is to extract node representations that are context-aware and encompass diverse local neighborhood structural information. By employing a molecule encoder, we obtain molecule features \(\mathbf{X}\in\mathbb{R}^{N\times F}\), where each atom representation contains context-aware structural information.

Causal Q-Former.We employ a Q-Former model introduced by BLIP-2 [23] to generate query embeddings \(\mathbf{Z}=\left\{\boldsymbol{z}_{i}\right\}_{i=1}^{M}\in\mathbb{R}^{M\times d}\) containing high-level molecular and textual information, where \(M\) represents the number of queries and \(d\) denotes the dimension of query embeddings. Specifically, we incorporate causal masks into the queries, ensuring that they only interact with preceding queries. This ensures the sequence of query embeddings maintains a causal dependency, aligning with the requirements of LLMs operating on text sequence. Details regarding the Causal Q-Former can be found in Appendix A.

Vector Quantization.The Causal Q-Former converts molecule and text features into a causal sequence of query embeddings. Subsequently, we aim to quantize these query embeddings into molecule tokens using a variant of VQ-VAE [45]. These discrete molecule tokens can then be integrated with text tokens to form a multi-modal sequence suitable for feeding into LLMs. The causal sequence of query embeddings \(\{\bm{z}_{i}\}_{i=1}^{M}\) is quantized into a causal sequence of molecule tokens \(\{s_{i}\}_{i=1}^{M}\) by identifying the closest neighbor in a learnable codebook \(\mathcal{C}=\{\bm{c}_{i}\}_{i=1}^{K}\), where \(K\) represents the size of the codebook. The codebook is randomly initialized and optimized during pretraining. Specifically, token \(s_{i}\) is determined as follows:

\[s_{i}=\operatorname*{argmin}_{j\in\{1,\cdots,K\}}\left\|\bm{z}_{i}-\bm{c}_{j} \right\|_{2},\quad\text{for}\quad i=1,2,\cdots,M.\] (1)

Intuitively, the query embedding \(\bm{z}_{i}\) is quantized to the closest neighbor \(\bm{c}_{s_{i}}\) in the codebook. As the vector quantization process is non-differentiable, we adopt the straight-through estimator [3] to train the Causal Q-Former by copying the gradient from the molecule tokens to the query embeddings, as shown in Figure 2. The resulting embeddings of molecule tokens, denoted as \(\mathbf{C}=\{\bm{c}_{s_{i}}\}_{i=1}^{M}\), are subsequently utilized for reconstructing molecules.

Molecule Reconstruction.An adapter needs to be trained to align the discrete latent space of molecule tokens with the continuous latent space of a molecular generative model for molecule reconstruction. The embeddings of molecule tokens \(\mathbf{C}\) can be aligned with the latent space of the generative model via an MLP adapter \(\psi\), represented as \(\mathbf{X}_{R}=\psi(\mathbf{C})\), where \(\mathbf{X}_{R}\) denotes the embeddings for reconstruction. Subsequently, we can reconstruct the molecule from \(\mathbf{X}_{R}\) using the pretrained SMILES decoder To achieve alignment, we minimize the Mean Squared Error (MSE) loss between \(\mathbf{X}_{R}\) and the SMILES [50] embeddings \(\mathbf{X}_{S}\) produced by the pretrained SMILES encoder. The training loss of the tokenizer is expressed as follows:

\[\mathcal{L}_{\text{Tokenizer}}=\left\|\mathbf{X}_{R}-\mathbf{X}_{S}\right\|_ {2}^{2}+\frac{1}{M}\sum_{i=1}^{M}\left\|\mathrm{sg}\left[\bm{z}_{i}\right]- \bm{c}_{s_{i}}\right\|_{2}^{2}+\frac{\beta}{M}\sum_{i=1}^{M}\left\|\mathrm{ sg}\left[\bm{c}_{s_{i}}\right]-\bm{z}_{i}\right\|_{2}^{2}.\] (2)

Here, the first term represents the alignment loss, the second term is a codebook loss aimed at updating the codebook embeddings, and the third term is a commitment loss that encourages the query embedding to stay close to the chosen codebook embedding. \(\mathrm{sg}[\cdot]\) denotes the stop-gradient operator, and the hyperparameter \(\beta\) is set to 0.25.

Figure 2: Illustration of our proposed molecule tokenizer. The tokenizer generates discrete molecule tokens, which can be fed into LLMs for downstream tasks. The generated molecule tokens can be decoded into molecules using the adapter and the SMILES decoder during inference.

### Unified Molecule-Text Language Model

Expanding Vocabulary.Employing the molecule tokenizer, a molecule can be tokenized into a molecule sequence \(\{s_{i}\}_{i=1}^{M}\) with causal dependency. The molecule sequence can be concatenated with the text sequence to form a multi-modal sequence \(\{u_{i}\}_{i=1}^{L}\), where \(L\) is the length of the multi-modal sequence. To facilitate the representation of the multi-modal sequence, we construct the molecule vocabulary \(\mathcal{V}^{m}=\{\bm{v}_{i}^{m}\}_{i=1}^{K}\), which maintains the order of the molecule codebook \(\mathcal{C}=\{\bm{c}_{i}\}_{i=1}^{K}\). Additionally, \(\mathcal{V}^{m}\) includes several special tokens such as boundary indicators, e.g., [MOL] and [/MOL], to mark the beginning and end of the molecule sequence. Next, we merge the original text vocabulary \(\mathcal{V}^{t}=\{\bm{v}_{i}^{t}\}_{i=1}^{T}\) with the molecule vocabulary \(\mathcal{V}^{m}\). The unified molecule-text vocabulary \(\mathcal{V}=\{\mathcal{V}^{m},\mathcal{V}^{t}\}\) facilitates joint learning from molecules and text under a unified next-token-prediction objective. As the vocabulary is expanded, the corresponding embeddings and prediction layers also need to be extended, with the newly introduced parameters initialized randomly.

Unified Molecule-text Modeling.The multi-modal sequence \(\{u_{i}\}_{i=1}^{L}\) is fed into the pretrained LLM for performing multi-modal autoregression. UniMoT adopts the general Language Modeling (LM) objective to directly maximize the log-likelihood of the data distribution:

\[\mathcal{L}_{\text{LM}}=-\sum_{u\in\mathcal{D}}\sum_{i\in\mathcal{I}}\log p \left(u_{i}\mid u_{1},\cdots,u_{i-1};\theta\right),\] (3)

where \(\mathcal{D}\) represents the dataset, \(\mathcal{I}\) represents the set of indices of the generation target, and \(\theta\) denotes the parameters of the LLM. The unification of representation and training paradigm for molecules and text enhances the abilities of LLMs to understand molecule-text interactions and alignment. UniMoT can interpret molecules similar to understanding a foreign language, and generate them as if they were text. We conduct autoregressive pretraining on molecule-to-text and text-to-molecule tasks to enhance the molecule comprehension and generation capabilities.

Molecule-to-Text Autoregression.While structural information is embedded in molecule features and captured by the molecule tokens through the tokenizer, we also aim to incorporate sequential information of molecules for better comprehension. Therefore, we concatenate the molecule sequence \(\{s_{i}\}_{i=1}^{M}\) with the SMILES [50] sequence and a prompt to form the multi-modal input sequence \(\{u_{i}\}_{i=1}^{L}\), as illustrated in Figure 2(a). The text sequence of the corresponding molecule caption is used as the generation target.

Text-to-Molecule Autoregression.For molecule generation, a prompt and the molecule caption are concatenated, with a [MOL] token appended to signify the beginning of the molecule sequence, as illustrated in Figure 2(b). The molecule sequence \(\{s_{i}\}_{i=1}^{M}\) produced by the tokenizer is used as the

Figure 3: Illustration of the multi-modal autoregressive pretraining on molecule-text datasets. UniMoT excels in multi-modal comprehension and generation tasks, enabled by the unified LM objective. \(T\) represents the size of the text vocabulary.

[MISSING_PAGE_FAIL:6]

CheBI-20 datasets. Performance on the PubChem dataset is shown in Table 2, while the performance on the CheBI-20 dataset and some concrete examples are presented in Appendix D.

From Table 2, we observe that UniMoT consistently outperforms the baselines by a significant margin. This task is more complex than classification or regression, providing a robust measure of the model's molecule comprehension abilities. Notably, our proposed tokenizer-based architecture surpasses the projection-based architecture (such as InstructMol), Q-Former-based architecture (such as MolCA and 3D-MoLM), and models trained with contrastive learning strategies (such as MoMu). The results demonstrate that the molecule tokenizer can generate molecule tokens with high-level molecular and textual information, enhancing molecule comprehension abilities.

Molecule-Text Retrieval Task.The molecule-text retrieval task involves using a molecule to retrieve text (M2T) and using text to retrieve a molecule (T2M). We compare UniMoT with several baselines: Sci-BERT [2], KV-PLM [58], MoMu [39], MoleculeSTM [31], MolCA [33], and 3D-MoLM [25]. We report the performance of retrieval using a batch of 64 random samples and the entire test set, evaluated with the metrics of Accuracy and Recall@20. We use the checkpoint from Stage-1 of pretraining. UniMoT is evaluated on the datasets of PubChem, PCdes, and MoMu. Performance on the PubChem dataset is shown in Table 3, while performance on the PCdes and MoMu datasets is presented in Appendix D. UniMoT can understand complex molecule-text interactions through the introduction of the Causal Q-Former. From Table 3, UniMoT demonstrates superior performance over the baselines on molecule-text retrieval, particularly in molecule-to-text retrieval. This underscores UniMoT's capability in learning fine-grained alignment between molecules and text.

### Molecule Generation Tasks

We employ molecule generation tasks, which encompass caption-guided molecule generation, reagent prediction, forward reaction prediction, and retrosynthesis. Caption-guided molecule generation involves generating molecular structures based on textual descriptions. Reagent prediction entails determining suitable reagents given reactants and products. Forward reaction prediction involves predicting probable products given specific reactants and reagents. Retrosynthesis involves decon-structing a target molecule into simpler starting materials. We compare UniMoT with the following

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline Model & BBBP\(\uparrow\) & Tox21\(\uparrow\) & ToxCast\(\uparrow\) & Sider\(\uparrow\) & ClinTox\(\uparrow\) & MU\(\uparrow\) & HIV\(\uparrow\) & BACE\(\uparrow\) \\ \hline KV-PLM [58] & 70.50 & 72.12 & 55.03 & 59.83 & 89.17 & 54.63 & 65.40 & 78.50 \\ AttrMask [16] & 67.79 & 75.00 & 63.57 & 58.05 & 75.44 & 73.76 & 75.44 & 80.28 \\ InfoGraph [40] & 64.84 & 76.24 & 62.68 & 59.15 & 76.51 & 72.97 & 70.20 & 77.64 \\ MolCLR [48] & 67.79 & 75.55 & 64.58 & 58.66 & 84.22 & 72.76 & 75.88 & 71.14 \\ GraphMVP [30] & 68.11 & **77.06** & 65.11 & 60.64 & 84.46 & 74.38 & 77.74 & 80.48 \\ MoleculeSTM [31] & 69.98 & 76.91 & 65.05 & **60.96** & 92.53 & 73.40 & 76.93 & 80.77 \\ InstructMol (Vicuna-7B) [6] & 70.00 & 74.67 & 64.29 & 57.80 & 91.48 & 74.62 & 68.90 & 82.30 \\ \hline UniMoT (LLaMA2-7B) & **71.37** & 76.43 & **65.78** & 59.79 & **92.89** & **75.97** & **78.49** & **83.69** \\ \hline \hline \end{tabular}
\end{table}
Table 1: ROC-AUC (%) of molecular property prediction task (classification) on the MoleculeNet datasets. **Bold** indicates the best performance and underline indicates the second best performance.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Model & BLEU-2\(\uparrow\) & BLEU-4\(\uparrow\) & ROUGE-1\(\uparrow\) & ROUGE-2\(\uparrow\) & ROUGE-L\(\uparrow\) & METEOR\(\uparrow\) \\ \hline MoIT5-Small (T5-Small) [11] & 22.5 & 15.2 & 30.4 & 13.5 & 20.3 & 24.0 \\ MoIT5-Base (T5-Base) [11] & 24.5 & 16.6 & 32.2 & 14.0 & 21.4 & 26.1 \\ MoIT5-Large (T5-Large) [11] & 25.9 & 17.3 & 34.1 & 16.4 & 23.4 & 28.0 \\ MoMu-Small (T5-Small) [39] & 22.9 & 16.0 & 31.0 & 13.7 & 20.8 & 24.4 \\ MoMu-Base (T5-Base) [39] & 24.7 & 16.8 & 32.5 & 14.6 & 22.1 & 27.2 \\ MoMu-Large (T5-Large) [39] & 26.3 & 18.0 & 34.8 & 16.9 & 24.8 & 28.7 \\ InstructMol (Vicuna-7B) [6] & 18.9 & 11.7 & 27.3 & 11.8 & 17.8 & 21.3 \\ MoICA (OPT-125M) [33] & 25.9 & 17.5 & 34.4 & 16.6 & 23.9 & 28.5 \\ MoICA (OPT-1.3B) [33] & 28.6 & 21.3 & 36.2 & 21.4 & 29.7 & 32.6 \\
3D-MoLM (LLaMA2-7B) [25] & 30.3 & 22.5 & 36.8 & 22.3 & 31.2 & 33.1 \\ \hline UniMoT (LLaMA2-7B) & **31.3** & **23.8** & **37.5** & **23.7** & **33.6** & **34.8** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Performance (%) of molecule captioning task on the PubChem dataset. **Bold** indicates the best performance and underline indicates the second best performance.

baselines: LLaMA [44], Vicuna [7], Mol-Instructions [12], and InstructMol [6]. The metrics used to evaluate molecule generation tasks include Exact Match, BLEU [37], Levenshtein Distance [22], RDKit Fingerprint Similarity [20], MACCS Fingerprint Similarity [10], and Morgan Fingerprint Similarity [36]. These metrics evaluate structural similarity between generated and target molecules, along with Validity [19], which assesses the proportion of chemically valid molecules generated. We utilize the Mol-Instructions dataset to evaluate the generation capabilities of UniMoT, and the results are presented in Table 4.

As the baselines generate SMILES strings and then convert them to molecules, UniMoT directly leverages the generated molecule tokens and obtains their embeddings from the learned codebook. These embeddings can be decoded to desired molecules through the pretrained adapter and SMILES decoder. Regarding the results in Table 4, UniMoT exhibits the capability to generate valid molecules with a higher degree of similarity to the target molecules compared to the baselines. UniMoT can generate molecules as if they were text, demonstrating strong generation capabilities and providing a new perspective to molecule generation tasks.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline \multirow{2}{*}{Model} & \multicolumn{4}{c}{Retrieval in batch} & \multicolumn{4}{c}{Retrieval in test set} \\  & \multicolumn{4}{c}{M2T (\%)} & \multicolumn{4}{c}{T2M (\%)} & \multicolumn{4}{c}{T2M (\%)} & \multicolumn{4}{c}{T2M (\%)} \\ \cline{2-7}  & Acc\(\uparrow\) & R@20\(\uparrow\) & Acc\(\uparrow\) & R@20\(\uparrow\) & Acc\(\uparrow\) & R@20\(\uparrow\) & Acc\(\uparrow\) & R@20\(\uparrow\) \\ \hline Sci-BERT [2] & 85.3 & 98.7 & 84.2 & 98.4 & 41.7 & 87.3 & 40.2 & 86.8 \\ KV-PLM [58] & 86.1 & 98.6 & 85.2 & 98.5 & 42.8 & 88.5 & 41.7 & 87.8 \\ MoMu (Sci-BERT) [39] & 87.6 & 99.2 & 86.4 & 99.4 & 47.3 & 90.8 & 48.1 & 89.9 \\ MoMu (KV-PLM) [39] & 88.2 & 99.4 & 87.3 & 99.4 & 48.5 & 91.6 & 49.5 & 90.7 \\ MoleculeSTM [31] & 90.5 & 99.6 & 88.6 & 99.5 & 52.7 & 92.9 & 53.2 & 92.5 \\ MoICA (OPT-1.3B) [33] & 92.6 & 99.8 & 91.3 & 99.5 & 67.9 & 94.4 & 68.6 & 93.3 \\
3D-MoLM (LLaMA2-7B) [25] & 93.5 & **100.0** & **92.9** & **99.6** & 69.1 & 95.9 & **70.1** & **94.9** \\ \hline UniMoT (LLaMA2-7B) & **93.6** & **100.0** & 92.7 & 99.4 & **69.5** & **96.3** & 69.8 & 94.4 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Performance (%) of molecule-text retrieval task on the PubChem dataset. **Bold** indicates the best performance and underline indicates the second best performance.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline \multirow{2}{*}{Model} & \multicolumn{4}{c}{Exact\(\uparrow\)} & BLEU\(\uparrow\) & Levenshtein\(\downarrow\) & RDK FTS\(\uparrow\) & MACCS FTS\(\uparrow\) & Morgan FTS\(\uparrow\) & Validity\(\uparrow\) \\ \hline
**Caption-guided Molecule Generation** & & & & & & & \\ L1-AMA [44] & 0.000 & 0.003 & 59.864 & 0.005 & 0.000 & 0.000 & 0.003 \\ Vicuna [7] & 0.000 & 0.006 & 60.356 & 0.006 & 0.001 & 0.000 & 0.001 \\ Mol-Instructions [12] & 0.002 & 0.345 & 41.367 & 0.231 & 0.412 & 0.147 & 1.000 \\ MolTS [11] & 0.112 & 0.546 & 38.276 & 0.400 & 0.538 & 0.295 & 0.773 \\ \hline UniMoT & **0.237** & **0.698** & **27.782** & **0.543** & **0.651** & **0.411** & 1.000 \\ \hline
**Reagent Prediction** & & & & & & & \\ L1-AMA [44] & 0.000 & 0.003 & 28.040 & 0.037 & 0.001 & 0.001 & 0.001 \\ Vicuna [7] & 0.000 & 0.010 & 27.948 & 0.038 & 0.002 & 0.001 & 0.007 \\ Mol-Instructions [12] & 0.044 & 0.224 & 23.167 & 0.237 & 0.364 & 0.213 & 1.000 \\ InstructMol [6] & 0.129 & 0.610 & 19.664 & 0.444 & 0.539 & 0.400 & 1.000 \\ \hline UniMoT & **0.167** & **0.728** & **14.588** & **0.549** & **0.621** & **0.507** & 1.000 \\ \hline
**Forward Reaction Prediction** & & & & & & \\ L1-AMA [44] & 0.000 & 0.020 & 42.002 & 0.001 & 0.002 & 0.001 & 0.039 \\ Vicuna [7] & 0.000 & 0.057 & 41.690 & 0.007 & 0.016 & 0.006 & 0.059 \\ Mol-Instructions [12] & 0.045 & 0.654 & 27.262 & 0.313 & 0.509 & 0.262 & 1.000 \\ InstructMol [6] & 0.536 & 0.967 & 10.851 & 0.776 & 0.878 & 0.741 & 1.000 \\ \hline UniMoT & **0.611** & **0.980** & **8.297** & **0.836** & **0.911** & **0.807** & 1.000 \\ \hline
**Retrosynthesis** & & & & & & & \\ L1-AMA [44] & 0.000 & 0.036 & 46.844 & 0.018 & 0.029 & 0.017 & 0.010 \\ Vicuna [7] & 0.000 & 0.057 & 46.877 & 0.025 & 0.030 & 0.021 & 0.017 \\ Mol-Instructions [12] & 0.009 & 0.705 & 31.227 & 0.283 & 0.487 & 0.230 & 1.000 \\ InstructMol [6] & 0.407 & 0.941 & 13.967 & 0.753 & 0.852 & 0.714 & 1.000 \\ \hline UniMoT & **0.478** & **0.974** & **11.634** & **0.810** & **0.909** & **0.771** & 1.000 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Performance of molecule generation tasks on the Mol-Instructions dataset, including caption-guided molecule generation, reagent prediction, forward reaction prediction, and retrosynthesis. **Bold** indicates the best performance, and underline indicates the second best performance.

### Ablation Studies

Cross-Modal Project.We conducted an ablation study on the cross-modal projector, with the results on the molecule captioning task shown in Table 5. The linear projection demonstrated the worst performance, indicating that the molecule features lack textual information, thus hindering effective molecule-text interactions and alignment. Additionally, we compared the performance of a Q-Former with bidirectional self-attention to a Causal Q-Former with causal self-attention. The results show that query embeddings with causal dependency outperform those with bidirectional dependency. This demonstrates that input with left-to-right causal dependency aligns with the unidirectional attention mechanism in LLMs, leading to improved performance.

Discrete vs. Continuous Representation.We compare the performance of continuous causal query embeddings and discrete tokens quantized from causal embeddings as inputs to LLMs. As shown in Table 5, continuous embeddings demonstrate better performance than discrete tokens in understanding molecules. This result is reasonable since the quantization process causes information loss in discrete tokens. However, we still use discrete token representation to facilitate the autoregressive training paradigm of LLMs, which supports the unification of comprehension and generation tasks. To achieve this unification, we unavoidably sacrifice some performance in comprehension tasks.

Model Size and Tuning Stategy.We conducted a comparison of molecule captioning performance across various model sizes and tuning strategies, as illustrated in Table 6. Our findings indicate that scaling up the LLM to 13B or adopting a fully tuning strategy yields only marginal improvements in performance compared to using LLaMA2-7B with LoRA tuning. While larger models and fully tuning strategies might offer slight gains in performance, they come at a significant cost in terms of efficiency. Considering the trade-off between achieving high performance and maintaining efficiency, we have chosen to utilize LLaMA2-7B with LoRA tuning in our experiments. This ensures that our model remains both powerful and practical.

## 5 Conclusion

This work introduces UniMoT, an innovation in the field of molecular-textual understanding and generation, which has successfully unified these two distinct modalities under a single, coherent framework. By integrating a Vector Quantization-driven tokenizer with a Causal Q-Former, UniMoT overcomes previous architectural limitations where molecule and text modalities were not treated equally, lacking a dedicated supervision signal for the molecular domain. This unique tokenizer transforms molecules into sequences of discrete tokens, embedding high-level molecular and textual information cohesively. Moreover, by employing a four-stage training scheme, UniMoT has emerged as a versatile multi-modal LLM, adept at handling molecule-to-text and text-to-molecule tasks. Extensive empirical evaluations demonstrate that UniMoT attains state-of-the-art performance across a diverse array of molecule comprehension and generation tasks.

\begin{table}
\begin{tabular}{l l c c c c c c} \hline \hline Model Size & Tuning & BLEU-2 & BLEU-4 & ROUGE-1 & ROUGE-2 & ROUGE-L & METEOR \\ \hline LLaMA2-7B & LoRA Tuning & 31.3 & 23.8 & 37.5 & 23.7 & 33.6 & 34.8 \\ LLaMA2-7B & Fully Tuning & 32.0 & 24.6 & 38.3 & 24.3 & 34.7 & 35.6 \\ LLaMA2-13B & LoRA Tuning & 31.8 & 24.3 & 38.0 & 24.1 & 34.4 & 35.3 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Ablation study on the model size and tuning strategy for the molecule captioning task using the PubChem dataset.

\begin{table}
\begin{tabular}{l l c c c c c c} \hline \hline Projector & Input to LLM & BLEU-2 & BLEU-4 & ROUGE-1 & ROUGE-2 & ROUGE-L & METEOR \\ \hline Projection Layer & Molecule Emb. & 19.3 & 12.1 & 27.9 & 12.3 & 18.1 & 21.5 \\ Q-Former & Query Emb. & 28.6 & 21.3 & 36.2 & 21.4 & 29.7 & 32.6 \\ Causal Q-Former & Causal Emb. & 32.8 & 25.2 & 39.2 & 24.8 & 35.3 & 36.5 \\ Causal Q-Former & Causal Tokens & 31.3 & 23.8 & 37.5 & 23.7 & 33.6 & 34.8 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Ablation study on the projector and representation form for the molecule captioning task using the PubChem dataset.

[MISSING_PAGE_FAIL:10]

* [22] Vladimir I Levenshtein et al. Binary codes capable of correcting deletions, insertions, and reversals. In _Soviet physics doklady_, volume 10, pages 707-710. Soviet Union, 1966.
* [23] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In _International conference on machine learning_, pages 19730-19742. PMLR, 2023.
* [24] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In _International conference on machine learning_, pages 12888-12900. PMLR, 2022.
* [25] Sihang Li, Zhiyuan Liu, Yanchen Luo, Xiang Wang, Xiangnan He, Kenji Kawaguchi, Tat-Seng Chua, and Qi Tian. Towards 3d molecule-text interpretation in language models. _arXiv preprint arXiv:2401.13923_, 2024.
* [26] Youwei Liang, Ruiyi Zhang, Li Zhang, and Pengtao Xie. Drugchat: towards enabling chatgpt-like capabilities on drug molecule graphs. _arXiv preprint arXiv:2309.03907_, 2023.
* [27] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In _Text summarization branches out_, pages 74-81, 2004.
* [28] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _Advances in neural information processing systems_, 36, 2024.
* [29] Pengfei Liu, Yiming Ren, Jun Tao, and Zhixiang Ren. Git-mol: A multi-modal large language model for molecular science with graph, image, and text. _Computers in Biology and Medicine_, 171:108073, 2024.
* [30] Shengchao Liu, Mehmet F Demirel, and Yingyu Liang. N-gram graph: Simple unsupervised representation for graphs, with applications to molecules. _Advances in neural information processing systems_, 32, 2019.
* [31] Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao, Ling Liu, Jian Tang, Chaowei Xiao, and Animashree Anandkumar. Multi-modal molecule structure-text model for text-based retrieval and editing. _Nature Machine Intelligence_, 5(12):1447-1457, 2023.
* [32] Shengchao Liu, Hanchen Wang, Weiyang Liu, Joan Lasenby, Hongyu Guo, and Jian Tang. Pre-training molecular graph representation with 3d geometry. _arXiv preprint arXiv:2110.07728_, 2021.
* [33] Zhiyuan Liu, Sihang Li, Yanchen Luo, Hao Fei, Yixin Cao, Kenji Kawaguchi, Xiang Wang, and Tat-Seng Chua. Molca: Molecular graph-language modeling with cross-modal projector and uni-modal adapter. _arXiv preprint arXiv:2310.12798_, 2023.
* [34] Yizhen Luo, Kai Yang, Massimo Hong, Xingyi Liu, and Zaiqing Nie. Molfm: A multimodal molecular foundation model. _arXiv preprint arXiv:2307.09484_, 2023.
* [35] Yizhen Luo, Jiahuan Zhang, Siqi Fan, Kai Yang, Yushuai Wu, Mu Qiao, and Zaiqing Nie. Biomedgpt: Open multimodal generative pre-trained transformer for biomedicine. _arXiv preprint arXiv:2308.09442_, 2023.
* [36] Harry L Morgan. The generation of a unique machine description for chemical structures-a technique developed at chemical abstracts service. _Journal of chemical documentation_, 5(2):107-113, 1965.
* [37] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In _Proceedings of the 40th annual meeting of the Association for Computational Linguistics_, pages 311-318, 2002.
* [38] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of machine learning research_, 21(140):1-67, 2020.
* [39] Bing Su, Dazhao Du, Zhao Yang, Yujie Zhou, Jiangmeng Li, Anyi Rao, Hao Sun, Zhiwu Lu, and Ji-Rong Wen. A molecular multimodal foundation model associating molecule graphs with natural language. _arXiv preprint arXiv:2209.05481_, 2022.
* [40] Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization. _arXiv preprint arXiv:1908.01000_, 2019.
* [41] Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, and Chao Huang. Graphgpt: Graph instruction tuning for large language models. _arXiv preprint arXiv:2310.13023_, 2023.

* [42] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Stanford alpaca: An instruction-following llama model, 2023.
* [43] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. _arXiv preprint arXiv:2211.09085_, 2022.
* [44] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models (2023). _arXiv preprint arXiv:2302.13971_, 2023.
* [45] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. _Advances in neural information processing systems_, 30, 2017.
* [46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [47] Sheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun, and Junzhou Huang. Smiles-bert: large scale unsupervised pre-training for molecular property prediction. In _Proceedings of the 10th ACM international conference on bioinformatics, computational biology and health informatics_, pages 429-436, 2019.
* [48] Y Wang, J Wang, Z Cao, and AB Farimani. Molclr: Molecular contrastive learning of representations via graph neural networks. arxiv 2021. _arXiv preprint arXiv:2102.10056_, 2021.
* [49] Zichao Wang, Weili Nie, Zhuoran Qiao, Chaowei Xiao, Richard Baraniuk, and Anima Anandkumar. Retrieval-based controllable molecule generation. _arXiv preprint arXiv:2208.11126_, 2022.
* [50] David Weininger. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. _Journal of chemical information and computer sciences_, 28(1):31-36, 1988.
* [51] Jiawei Wu, Mingyuan Yan, and Dianbo Liu. Vqsynery: Robust drug synergy prediction with vector quantization mechanism. _arXiv preprint arXiv:2403.03089_, 2024.
* [52] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. _arXiv preprint arXiv:2309.05519_, 2023.
* [53] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. _Chemical science_, 9(2):513-530, 2018.
* [54] Jun Xia, Chengshuai Zhao, Bozhen Hu, Zhangyang Gao, Cheng Tan, Yue Liu, Siyuan Li, and Stan Z Li. Mole-bert: Rethinking pre-training graph neural networks for molecules. In _The Eleventh International Conference on Learning Representations_, 2022.
* [55] Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Baize: An open-source chat model with parameter-efficient tuning on self-chat data. _arXiv preprint arXiv:2304.01196_, 2023.
* [56] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. _Advances in neural information processing systems_, 33:5812-5823, 2020.
* [57] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. _arXiv preprint arXiv:2110.04627_, 2021.
* [58] Zheni Zeng, Yuan Yao, Zhiyuan Liu, and Maosong Sun. A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals. _Nature communications_, 13(1):862, 2022.
* [59] Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, et al. Anyspt: Unified multimodal llm with discrete sequence modeling. _arXiv preprint arXiv:2402.12226_, 2024.
* [60] Xiang Zhuang, Qiang Zhang, Keyan Ding, Yatao Bian, Xiao Wang, Jingsong Lv, Hongyang Chen, and Huajun Chen. Learning invariant molecular representation in latent discrete space. _Advances in Neural Information Processing Systems_, 36, 2024.

Details of Causal Q-Former

The Q-Former operates as a query-based transformer that utilizes learnable query vectors to interact with molecule features extracted by a frozen encoder. These queries are essential for extracting relevant information from the molecule features. The Q-Former comprises both a molecule transformer and a text transformer, sharing self-attention layers. The text transformer architecture is based on BERT [9], while the molecule transformer incorporates cross-attention layers between self-attention and feed-forward layers. Q-Former employs a cross-attention mechanism where the query vectors selectively attend to different aspects of the molecule features, allowing the model to capture critical details necessary for understanding and generating textual descriptions of molecular properties.

Specifically, we incorporate causal masks into the queries, ensuring that they only interact with preceding queries. This ensures the sequence of query embeddings maintains a causal dependency, aligning with the requirements of LLMs operating on text sequence. The Causal Q-Former is illustrated in Figure 4. We employ the Causal Q-Former to generate causal query embeddings \(\mathbf{Z}=\{\bm{z}_{i}\}_{i=1}^{M}\in\mathbb{R}^{M\times d}\) containing high-level molecular and textual information, where \(M\) represents the number of queries and \(d\) denotes the dimension of query embeddings. Next, we introduce three tailored objectives MTC, MTM, and MTG for the pretraining of the Causal Q-Former.

**Molecule-Text Contrastive Learning (MTC)** aims to align molecule and text features by maximizing their mutual information. This is achieved by maximizing the molecule-text similarity of positive pairs against that of negative pairs. We utilize the last query embedding \(\bm{z}_{M}\) of the query sequence \(\{\bm{z}_{i}\}_{i=1}^{M}\) as the query representation, since the output query sequence is causal and the last embedding contains global information from the queries. For text representation, we use the output embedding of the [CLS] token, denoted as \(\bm{y}\). The contrastive learning loss is expressed as follows:

\[\mathcal{L}_{\text{MTC}}=-\frac{1}{B}\sum_{i=1}^{B}\log\frac{\exp((\bm{z}_{M}^ {i})^{T}\bm{y}^{i}/\tau)}{\sum_{j=1}^{B}\exp((\bm{z}_{M}^{i})^{T}\bm{y}^{j}/ \tau)}-\frac{1}{B}\sum_{i=1}^{B}\log\frac{\exp((\bm{y}^{i})^{T}\bm{z}_{M}^{i}/ \tau)}{\sum_{j=1}^{B}\exp((\bm{y}^{i})^{T}\bm{z}_{M}^{j}/\tau)},\] (4)

where \(B\) denotes the batch size, and \(\tau\) represents the temperature parameter. Here, \(\bm{z}_{M}^{i}\) and \(\bm{y}^{i}\) refer to the \(i\)-th query and text representations in a batch, respectively.

**Molecule-Text Matching (MTM)** focuses on learning fine-grained alignment between molecule and text features. As query embeddings \(\mathbf{Z}=\{\bm{z}_{i}\}_{i=1}^{M}\) capture both molecular and textual information through cross-attention and self-attention layers respectively, we utilize the last query embedding \(\bm{z}_{M}\) as input to a binary classifier. This classifier predicts whether a given molecule-text pair is matched or unmatched. The corresponding loss function is formulated as follows:

\[\mathcal{L}_{\text{MTM}}=-\frac{1}{B}\sum_{i=1}^{B}\log\frac{\exp(\phi(\bm{z}_ {M}\mid\mathbf{X}^{i},\bm{t}^{i}))}{\sum_{j=1}^{B}\exp(\phi(\bm{z}_{M}\mid \mathbf{X}^{i},\bm{t}^{j}))+\sum_{j=1}^{B}\exp(\phi(\bm{z}_{M}\mid\mathbf{X}^{ j},\bm{t}^{i}))},\] (5)

Figure 4: Illustration of our proposed Causal Q-Former. The Causal Q-Former provides causal query embeddings for subsequent blocks.

where \(\phi\) represents a binary classifier, and \(\mathbf{X}^{i}\) and \(\bm{t}^{i}\) denote the \(i\)-th input molecule features and input text in a batch, respectively.

**Molecule-grounded Text Generation (MTG)** focuses on generating textual descriptions given a molecule input. In this task, causal masks for queries are not applied since only textual output is required. However, causal masks are applied for text, allowing each text token to attend to its preceding text tokens and all queries, but not subsequent tokens. The Language Modeling (LM) loss function is applied to model the generation of text \(\bm{t}^{i}\) conditioned on the molecule input \(\mathbf{X}^{i}\), formulated as:

\[\mathcal{L}_{\text{MTG}}=-\frac{1}{B}\sum_{i=1}^{B}\sum_{j=1}^{L}\log p\left(t _{j}^{i}\mid t_{1}^{i},\cdots,t_{j-1}^{i},\mathbf{X}^{i}\right),\] (6)

where \(t_{j}^{i}\) represents the \(j\)-th token in the text sequence \(\bm{t}^{i}\). Here, \(\mathbf{X}^{i}\) and \(\bm{t}^{i}\) denote the \(i\)-th input molecule features and generated text in a batch, respectively.

The total loss for training the Q-Former encompasses the three aforementioned objectives:

\[\mathcal{L}_{\text{Q-Former}}=\mathcal{L}_{\text{MTC}}+\mathcal{L}_{\text{MTM }}+\mathcal{L}_{\text{MTG}}.\] (7)

## Appendix B Details of Datasets

This section provides detailed information about the datasets used in evaluating the performance of UniMoT across various tasks. The datasets are utilized for molecular property prediction, molecule captioning, molecule-text retrieval, and molecule generation tasks. Each dataset serves a unique purpose in assessing different capabilities of the model.

We present the details of the Molecular Property Prediction Datasets below:

* **BBBP**[53]: The Blood-Brain Barrier Penetration dataset predicts the ability of molecules to penetrate the blood-brain barrier.
* **Tox21**[53]: This dataset is part of the Toxicology in the 21st Century initiative, used for toxicity prediction.
* **ToxCast**[53]: Another toxicity prediction dataset with a broader range of biological assays.
* **Sider**[53]: Side Effect Resource database, used for predicting drug side effects.
* **ClinTox**[53]: Clinical Toxicity dataset for predicting clinical trial toxicity outcomes.
* **MUV**[53]: Maximum Unbiased Validation dataset for virtual screening.
* **HIV**[53]: Human Immunodeficiency Virus dataset for predicting anti-HIV activities.
* **BACE**[53]: Beta-Secretase 1 dataset for predicting inhibitors of the BACE-1 enzyme, relevant for Alzheimer's research.
* **QM9**[12]: The quantum mechanics properties dataset, where the objective is to predict key quantum mechanics properties of a given molecule, such as HUMO, LUMO, and the HUMO-LUMO gap.

We present the details of the Molecule Captioning Datasets below:

* **PubChem**[18]: A large dataset of chemical molecules used for generating textual descriptions of molecular structures.
* **ChEBI-20**[11]: A subset of the Chemical Entities of Biological Interest database, provides structured and detailed descriptions of molecules, enhancing the model's ability to generate accurate captions.

We present the details of the Molecule-Text Retrieval Datasets below:

* **PubMedChem**[18]: Used for both molecule-to-text (M2T) and text-to-molecule (T2M) retrieval tasks.
* **PCdes**[58]: Another dataset for evaluating M2T and T2M retrieval accuracy.
* **MoMu**[39]: Dataset specifically designed for molecule-text interactions and retrieval tasks.

\begin{table}
\begin{tabular}{p{56.9pt} p{56.9pt} p{56.9pt} p{56.9pt} p{56.9pt} p{56.9pt}} \hline \hline
**Dataset** & **Type** & **Tasks** & **Description** & **URL** & **License** \\ \hline BBBP & Classification & Molecular Property Prediction & Predicts barrier & blood-brain penetration & BBBP URL & CC-BY 4.0 \\ \hline Tox21 & Classification & Molecular Property Prediction & Toxicity prediction using the Tox21 initiative data. & Tox21 URL & Public Domain \\ \hline ToxCast & Classification & Molecular Property Prediction & Broad toxicity prediction with various biological assays. & ToxCast URL & Public Domain \\ \hline Sider & Classification & Molecular Property Prediction & Predicts drug side effects. & Sider URL & CC-BY 4.0 \\ \hline ClinTox & Classification & Molecular Property Prediction & Clinical trial toxicity prediction. & ClinTox URL & Public Domain \\ \hline MUV & Classification & Molecular Property Prediction & Virtual screening for unbiased validation. & MUV URL & CC-BY 4.0 \\ \hline HIV & Classification & Molecular Property Prediction & Predicts anti-HIV activity of molecules. & HIV URL & Public Domain \\ \hline BACE & Classification & Molecular Property Prediction & Predicts inhibitors of the BACE-1 enzyme. & BACE URL & Public Domain \\ \hline QM9 & Regression & Molecular Property Prediction & Predicts various molecular properties such as atomization energy, dipole moment, etc. & QM9 URL & CC-BY 4.0 \\ \hline PubChem & Captioning, Retrieval & Molecule Captioning, Molecule-Text & Generates descriptions and retrieves text/molecules based on input molecules/text. & PubChem URL & Public Domain \\ \hline ChEBI-20 & Captioning & Molecule Captioning & Generates detailed descriptions of molecular structures. & CHEBI-20 URL & CC-BY 4.0 \\ \hline PCdes & Retrieval & Molecule-Text Retrieval & Used for evaluating accuracy in molecule-text retrieval tasks. & PCdes URL & CC-BY 4.0 \\ \hline MoMu & Retrieval & Molecule-Text Retrieval & Dataset for molecule-text interaction and retrieval evaluation. & MoMu URL & CC-BY 4.0 \\ \hline Mol-Instructions & Generation & Molecule Generation & Includes tasks such as molecule generation from descriptions, reagent prediction, etc. & Mol-Instructions & CC-BY 4.0 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Summary of datasets, their types, tasks, descriptions, URLs, and licenses used for evaluating UniMoT.

We present the details of the Molecule Generation Datasets below:

* **Mol-Instructions**[12]: This dataset includes tasks such as caption-guided molecule generation, reagent prediction, forward reaction prediction, and retrosynthesis. It is used to evaluate the model's ability to generate molecular structures based on textual descriptions and other related tasks.

We summarize the datasets used for evaluating UniMoT in Table 7. It encompasses various types of datasets, including those for classification, regression, captioning, retrieval, and generation tasks. Each dataset is described in terms of its type, tasks it supports, a brief description of its content, its URL for access, and the license under which it is distributed. The licenses vary, with some datasets being in the public domain and others under CC-BY 4.0 license.

## Appendix C Details of Training

Stage-1: Causal Q-Former Pretraining.During Stage-1, we only connect the molecule encoder and the Causal Q-Former, leaving out other blocks. We leverage the pretrained molecule encoder from MoleculeSTM [31], which has undergone extensive contrastive learning with molecule-text pairs. We utilize the PubChem dataset [18] for pretraining, keeping the molecule encoder frozen while updating only the Causal Q-Former. Both queries and text serve as input to the Causal Q-Former, while only queries serve as input in subsequent stages. Inspired by BLIP-2 [23], we employ three tailored objectives - Molecule-Text Contrastive Learning (MTC), Molecule-Text Matching (MTM), and Molecule-grounded Text Generation (MTG) - for the pretraining of the Causal Q-Former, as detailed in Appendix A.

The dimension of molecule features is set to 300. We use 16 queries, each with a dimension of 768. The size of \(\mathbf{Z}\) (\(16\times 768\)) is much smaller than the size of molecule features \(\mathbf{X}\) (e.g., \(150\times 300\)). The Q-former is pretrained for 50 epochs. We adopt the AdamW optimizer with a weight decay of 0.05, and a cosine decay learning rate scheduler, with a minimal learning rate of 1e-5. The batch size is set to 64. The computational overhead for this pretraining is 20 GPU hours on 4 NVIDIA A100 GPUs.

Stage-2: Molecule Tokenizer Pretraining.We connect the Causal Q-Former with the subsequent blocks and train the molecule tokenizer using the objective defined in Equation (2). Following the approach of RetMol [49], we utilize SMILES strings [50] to represent molecules, and employ the pretrained ChemFormer [17] as the generative model. Specifically, we leverage the SMILES encoder and SMILES decoder components provided by ChemFormer. We utilize PubChem [18] and CheBI-20 [11] datasets, keeping the molecule encoder, SMILES encoder, and SMILES decoder frozen, while updating the Causal Q-Former, codebook, and adapter. Once optimized, the molecule tokenizer remains unchanged throughout the subsequent stages.

The molecule codebook size is set to \(K=2048\), and the dimension of codebook embedding is 768. The tokenizer is pretrained for 50 epochs. We adopt the AdamW optimizer with a weight decay of 0.05, and a cosine decay learning rate scheduler, with a minimal learning rate of 1e-5. The batch size is set to 64. The computational overhead for this pretraining is 40 GPU hours on 4 NVIDIA A100 GPUs.

Stage-3: Unified Molecule-Text Pretraining.We connect the molecule tokenizer with the LLM and employ the LM objective defined in Equation (3) to pretrain the LLM. We utilize LLaMA [44] as the default LLM. To construct the unified molecule-text vocabulary, we merge 2048 molecule codes with the original text vocabulary. Pretraining the LLM involves molecule-to-text autoregression and text-to-molecule autoregression, aimed at enhancing UniMoT's multi-modal comprehension and generation capabilities. We utilize datasets PubChem [18], CheBI-20 [11], PCdes [58], and MoMu [39] for this purpose. To enhance efficiency, we train the LLM using LoRA tuning [14].

The multi-modal LLM is pretrained for 10 epochs. We adopt the AdamW optimizer with a weight decay of 0.05, and a cosine decay learning rate scheduler, with a minimal learning rate of 1e-5. The batch size is set to 32. The computational overhead for this pretraining is 50 GPU hours on 4 NVIDIA A100 GPUs. To reduce CUDA memory usage, we integrate LoRA with the parameters set to \(r=8\), \(\alpha=32\), and dropout = 0.1. This integration is applied to the k_proj, v_proj, q_proj, and o_proj modules.

* [870]**Stage-4: Task-Specific Instruction Tuning.** We perform instruction tuning to align UniMoT with human instructions through supervised fine-tuning on seven tasks: molecular property prediction, molecule captioning, molecule-text retrieval, caption-guided molecule generation, reagent prediction, forward reaction prediction, and retrovsthesis. For the molecular property prediction task, we utilize the quantum mechanics properties dataset [12] for regression prediction and the MoleculeNet datasets [53] for property classification. For the molecule captioning and molecule-text retrieval tasks, we employ datasets PubChem [18], CheBI-20 [11], PCdes [58], and MoMu [39]. For the remaining tasks, we utilize the Mol-Instructions dataset [12] to conduct instruction tuning. We fine-tune UniMoT for 10 epochs on each task using the same optimizer, learning rate scheduler, and LoRA configurations as in Stage-3 pretraining. Instruction samples for comprehension and generation tasks are shown in Table 8.

We have summarized the detailed training hyperparameters of UniMoT in Table 9.

## Appendix D Details and More Results of Experiments

Molecular Property Prediction Task.Property prediction aims to anticipate a molecule's intrinsic physical and chemical properties based on its structural or sequential characteristics. In the regression task, we conduct experiments on the quantum mechanics properties dataset QM9 [12], where the objective is to predict key quantum mechanics properties of a given molecule, such as HUMO, LUMO, and the HUMO-LUMO gap. We compare UniMoT against several baselines, including Alpaca [42], Baize [55], LLaMA2-7B [44], Vicuna-13B [7], Mol-Instructions [12], and InstructMol [6]. Mean Absolute Error (MAE) serves as our evaluation metric. The performance of the regression task on the QM9 dataset is presented in Table 10. Compared to previous single-modal instruction-tuned LLMs and molecular LLMs, UniMoT exhibits further improvement on the regression task, showcasing its fundamental comprehension abilities in molecular contexts.

Molecule Captioning Task.The molecule captioning task involves generating a comprehensive description of a molecule. For this task, we compare UniMoT with several baselines: MolT5 [11],

\begin{table}
\begin{tabular}{l l} \hline \hline
**Task** & **Instruction** \\ \hline Molecular Property Prediction (Regression) & Instruction: _Could you give me the LUMO energy value of this molecule?_ (Optional: The SMILES sequence is: SMILES) \\  & Output: _0.0576_. \\ \hline Molecular Property Prediction (Classification) & Instruction: _Evaluate whether the given molecule is able to enter the blood-brain barrier._ (Optional: The SMILES sequence is: SMILES) \\  & Output: _Yes._ \\ \hline Molecule Captioning & Instruction: _Could you give me a brief overview of this molecule?_ (Optional: The SMILES sequence is: SMILES) \\  & Output: _The molecule is an indole phytoplankton that..._ \\ \hline Molecule-Text Retrieval & Instruction: _Retrieve relevant text for the given molecule._ (Optional: The SMILES sequence is: SMILES) \\  & Output: _The molecule is associated with..._ \\ \hline Caption-Guided Molecule & Instruction: _Create a molecule with the structure as described: The molecule is a primary arylamine that..._ \\  & Output: SMILES of the molecule._ \\ \hline Reagent Prediction & Instruction: _Please provide possible reagents based on the following chemical reaction._ \\  & <REACTANT A> <REACTANT B>... & ><PRODUUTaS> \\  & Output: SMILES of the reagents._ \\ \hline Forward Reaction Prediction & Instruction: _With the provided reactants and reagents, propose a potential product:_ \\  & <REACTANT A> <REACTANT B>... & <REAGENT A> <REAGENT B>... \\  & Output: SMILES of the products._ \\ \hline Retrovsynthesis & Instruction: _Please suggest potential reactants used in the synthesis of the product:_ \\  & <PRODUUTaS> \\  & Output: SMILES of the reactants and reagents._ \\ \hline \hline \end{tabular}
\end{table}
Table 8: Instruction samples for comprehension and generation tasks: molecular property prediction, molecule captioning, molecule-text retrieval, caption-guided molecule generation, reagent prediction, forward reaction prediction, and retrovsthesis.

[MISSING_PAGE_FAIL:18]

* Caption-guided molecule generation involves creating molecular structures from textual descriptions, leveraging NLP and cheminformatics to interpret and translate descriptions into chemical structures.
* Reagent prediction focuses on identifying suitable reagents for given reactants and desired products, optimizing synthetic routes.
* Forward reaction prediction forecasts probable products from specific reactants and reagents, using knowledge of chemical reactivity.
* Retrosynthesis deconstructs target molecules into simpler starting materials.

In molecule generation tasks, evaluating the quality of generated molecules involves several metrics that measure different aspects of similarity and validity.

* Exact Match checks if the generated molecule is identical to the target molecule, offering a stringent criterion for precise replication but potentially overlooking chemically similar variants.
* The BLEU score [37], adapted from machine translation, measures the overlap of n-grams (short sequences of atoms or bonds) between generated and target molecules, thus assessing partial similarities.
* Levenshtein Distance [22] evaluates the minimum number of edits needed to transform the generated molecule into the target, providing insight into structural changes required.
* RDKit [20], MACCS [10], and Morgan [36] Fingerprint Similarities compare the generated and target molecules based on various molecular fingerprinting methods, which capture different aspects of molecular structure and properties.
* The Validity [19] metric assesses the proportion of chemically valid molecules generated, ensuring that the output consists of plausible chemical structures.

Together, these metrics offer a comprehensive evaluation framework, balancing exact matches with structural and chemical validity.

## Appendix E Limitations

While UniMoT demonstrates considerable advancements in unifying molecule and text modalities for comprehensive understanding and generation tasks, several limitations must be acknowledged. Although UniMoT exhibits strong performance in molecule-to-text and text-to-molecule tasks, it has not been extensively tested on more complex molecule generation tasks such as molecule editing, which require precise modifications to molecular structures. Future work could explore extending UniMoT's capabilities to handle such sophisticated molecular manipulations.

Due to the scarcity of annotated data in the molecular field, the training of UniMoT is less extensive compared to fields like computer vision. This limitation restricts the model's ability to fully learn and generalize from diverse molecular structures and properties. In contrast, the visual domain benefits

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Model & BLEU-2\(\uparrow\) & BLEU-4\(\uparrow\) & ROUGE-1\(\uparrow\) & ROUGE-2\(\uparrow\) & ROUGE-L\(\uparrow\) & METEOR\(\uparrow\) \\ \hline T5-Small [38] & 50.1 & 41.5 & 60.2 & 44.6 & 54.5 & 53.2 \\ T5-Base [38] & 51.1 & 42.3 & 60.7 & 45.1 & 55.0 & 53.9 \\ T5-Large [38] & 55.8 & 46.7 & 63.0 & 47.8 & 56.9 & 58.6 \\ MoT5-Small (T5-Small) [11] & 51.9 & 43.6 & 62.0 & 46.9 & 56.3 & 55.1 \\ MoT5-Base (T5-Base) [11] & 54.0 & 45.7 & 63.4 & 48.5 & 57.8 & 56.9 \\ MoT5-Large (T5-Large) [11] & 59.4 & 50.8 & 65.4 & 51.0 & 59.4 & 61.4 \\ MoMu-Small (T5-Small) [39] & 53.2 & 44.5 & - & - & 56.4 & 55.7 \\ MoMu-Base (T5-Base) [39] & 54.9 & 46.2 & - & - & 57.5 & 57.6 \\ MoMu-Large (T5-Large) [39] & 59.9 & 51.5 & - & - & 59.3 & 59.7 \\ InstructMol (Vicuna-7B) [6] & 47.5 & 37.1 & 56.6 & 39.4 & 50.2 & 50.9 \\ MoICA (OPIT-125Mb) [33] & 61.6 & 52.9 & 67.4 & 53.3 & 61.5 & 63.9 \\ MoICA (OPIT-1.3B) [33] & 63.9 & 55.5 & 69.7 & 55.8 & 63.6 & 66.9 \\ \hline UniMoT (LLaMA2-7B) & **66.4** & **58.3** & **72.2** & **58.4** & **66.4** & **70.3** \\ \hline \hline \end{tabular}
\end{table}
Table 11: Performance (%) of molecule captioning task on the CheBI-20 dataset. **Bold** indicates the best performance and underline indicates the second best performance.

\begin{table}
\begin{tabular}{p{113.8pt} p{113.8pt} p{113.8pt}} \hline \hline
**Molecule** & **Generated Molecule Caption** & **Ground Truth** \\ \hline  & The molecule is an optically active form of phenylalanine having D-configuration. It is a conjugate base of a D-phenylalanine. It is an enantiomer of a L-phenylalanine. & The molecule is a 2-oxo monocarboxylic acid that is pyruvate acid and an (omega-1)-hydroxy fatty acid as an (omega-1)-hydroxy fatty acid. It derives from an (omega-1)-hydroxy fatty acid. & The molecule is a 2-oxo monocarboxylic acid that is pyruvate acid and an (omega-1)-hydroxy fatty acid as an (omega-1)-hydroxy fatty acid as an (omega-1)-hydroxygluc acid. It is a conjugate acid of an ascr18(1-). & The molecule is a 2-oxo monocarboxylic acid that is pyruvate acid in which one of the methyl hydrogens has been replaced by a methylency-clopropyl group. It has a role as a metabolic and a xenobiotic metabolite. It is a 2-oxo monocarboxylic acid, a member of cyclopropanes and an olefinic compound. It derives from a pyruvic acid. \\ \hline \hline \end{tabular}
\end{table}
Table 12: Examples of molecule captioning task on the ChEBI-20 dataset. We highlight in blue the text that accurately describes the molecule structures in the generated caption, ensuring alignment with the ground truth.

from abundant labeled datasets, allowing for more comprehensive training and better performance. Addressing this data scarcity in the molecular domain is crucial for improving UniMoT's training effectiveness and overall capabilities.

The current empirical evaluations, though extensive, are primarily conducted on standard datasets and benchmarks; expanding the evaluation to a broader array of datasets and real-world scenarios will provide a more comprehensive understanding of the model's robustness and generalizability.

## Appendix F Broader Impacts

The development of UniMoT, a unified model for molecule and text modalities, has significant potential to positively impact various fields. UniMoT can streamline the drug discovery process by enabling efficient molecule generation and optimization based on textual descriptions. In material science, it can aid in discovering new materials with desirable properties. Additionally, UniMoT can enhance research collaboration between chemists, biologists, and data scientists by integrating molecular and textual data, leading to comprehensive research insights and innovative solutions.

This paper does not pose any ethical concerns. The study does not involve human subjects and follows proper procedures for data set releases. There are no potentially harmful insights, methodologies, or applications. Additionally, there are no conflicts of interest or sponsorship concerns. Discrimination, bias, and fairness issues are not applicable. Privacy and security matters have been appropriately addressed, legal compliance has been maintained, and research integrity has been upheld.

\begin{table}

\end{table}
Table 13: Accuracy (%) of molecule-text retrieval task on the PCdes and MoMu datasets. **Bold** indicates the best performance and underline indicates the second best performance. We report the performance of retrieval using a batch of 64 random samples and the entire test set.

NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We conduct extensive experiments to verify our claims. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations in Appendix E. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: The paper does not include theoretical results. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We disclose all the information to reproduce the experimental results in Section 4, Appendix C, and Appendix D. Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [No] Justification: Once our paper is accepted, we will make the code openly accessible. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We disclose all the details of our experiments in Appendix C and Appendix D. Guidelines:

* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Given the considerable computational resources required for experiments with LLMs, we adhere to the common practice in the community. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The information regarding compute resources is provided in Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have carefully reviewed the code of ethics to ensure strict adherence to the guidelines. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss the broader impacts in Appendix F. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The licenses for existing assets are provided in Appendix B. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.