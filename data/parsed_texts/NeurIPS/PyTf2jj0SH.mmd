ConvBench: A Multi-Turn Conversation Evaluation Benchmark with Hierarchical Ablation Capability for Large Vision-Language Models

Shuo Liu\({}^{1}\)

Kaining Ying\({}^{1}\)

Hao Zhang\({}^{1}\)

Yue Yang\({}^{1}\)

Yuqi Lin\({}^{1}\)

Tianle Zhang\({}^{1}\)

Chuanhao Li\({}^{1}\)

Yu Qiao\({}^{1}\)

Ping Luo\({}^{2,1}\)

Wenqi Shao\({}^{11\%}\)

Kaipeng Zhang\({}^{11\%}\)

###### Abstract

Multi-turn visual conversation is an important ability of real-world AI assistants. However, the related evaluation benchmark is missed. This paper presents ConvBench, a multi-turn conversation benchmark with hierarchical capabilities ablation evaluation for Large Vision-Language Models (LVLMs). ConvBench comprises 577 curated multi-turn conversations, encompassing 215 tasks. These tasks are broad and open-ended, which resemble real-world user behaviors. ConvBench progressively examines the LVLMs' perception, reasoning, and creativity capabilities in each conversation and can decouple these capabilities in evaluations and thus perform reliable error attribution. Besides, considering the diversity of open-ended questions, we introduce an efficient and reliable automatic evaluation framework. Experimental results reveal that ConvBench is a significant challenge for current LVLMs, even for GPT4V, which achieves only a 39.51% score. Besides, we have some insightful findings, such as the weak perception of LVLMs inhibits authentic strengths in reasoning and creation. We believe our design of hierarchical capabilities, decoupling capabilities evaluation, and multi-turn conversation can blaze a new trail in LVLMs evaluation. Code and benchmark are released at https://github.com/shirlyliu64/ConvBench.

## 1 Introduction

Open-ended multi-turn visual conversations are usual in our daily lives and should be one of the features of artificial general intelligence (AGI). Recent large vision language models (LVLMs, e.g., GPT4V [1], Claude [2], and InterVL-Chat [3]) have achieved impressive performance in such conversations, which is also the common usage of LVLMs. However, existing benchmarks [4; 5; 6; 7; 8; 9; 10; 11; 12; 13] for LVLMs conduct single-turn conversations, and most use close-ended multi-choice questions for evaluation. To this end, this paper presents a multi-turn conversation benchmark named ConvBench to measure the advancement of LVLMs. Beyond open-ended and multi-turn, our questions are hierarchical and real-world, and we introduce an automatic evaluation method. In particular, in each conversation, ConvBench can decouple different capabilities evaluations and perform reliable error attribution, blazing a new trail in LVLMs evaluation.

We first introduce the hierarchical structure of multi-turn questions. Previous benchmarks treat different multimodal capabilities independently using independent questions while ignoring the fact that multimodal capabilities are highly dependent on each other. It makes it hard to conduct error attribution. For example, when the model responds incorrectly to a reasoning question, it is unclear whether it is attributed to the perception or reasoning error of LVLMs (see Figure 1 (c)). Besides, humans reason based on their perceptions and generate new ideas through perceptual and reasoning skills. Therefore, ConvBench progressively examines the perception, reasoning, and creativity capabilities in a multi-turn conversation, seeing Figure 1 (b) for examples. In this way, we can conduct more accurate error attribution by giving reference answers to previous questions.

We then introduce the source of the multi-turn questions. We collect images and questions from the perspective of real-world user behavior. Specifically, VisIT-Bench [7] is a single-turn visual question answering (VQA) dataset collected from users' "wish list". ConvBench uses its images and extends its questions in our multi-turn hierarchical structure. The new questions align with real-world user behavior and involve many new tasks. We illustrate the dataset construction in Figure 3 and details in Section 3. Overall, ConvBench comprises \(577\) irreticulously curated multi-turn VQA samples, spanning \(71\), \(65\), and \(79\) distinct types of perception, reasoning, and creation tasks, respectively.

Unlike most existing benchmarks employing multi-choice questions, we allow LVLMs to output anything to resemble real-world user behavior. However, its evaluation is much more challenging since the correct answer is not only. Therefore, we introduce a reliable automated evaluation named ConvBenchEval for open-ended visual questions. Specifically, we annotate each question's reference answers and each image's question-aware caption. Additionally, we annotate assessment focus points for creation questions for more accurate evaluation. Based on the above annotations (see Figure 3 for examples) and LVLMs' responses, we then employ ChatGPT [14] to judge if the LVLM's response is better than the reference answer. We illustrate the evaluation pipeline in Figure 4. We also conduct a human evaluation on a subset of ConvBench while ChatGPT achieves 81.83% judgment agreement with human evaluation results, demonstrating the reliability of ChatGPT.

Figure 1: The comparison between previous evaluation benchmarks (a) and our ConvBench (b). Previous benchmarks assess capabilities independently in a single-turn conversation, while our ConvBench evaluates multi-turn conversation by hierarchically assessing perception, reasoning, and creativity. (c) shows that the error should be attributed to perception rather than reasoning, while ConvBench employs annotated reference answers that can do such accurate error attribution.

We assess 20 publicly available LVLMs. The evaluation results reveal several innovative findings: i) Our ConvBench provides a significant challenge for evaluating the follow-up capability of LVLMs' multi-turn conversation, notably GPT4V [1] achieves only \(39.51\)% overall score. ii) The novel hierarchical ablation evaluations of ConvBench conclude that the weakness of "OCR", "Fine-grained", and "Spatial" perception of current LVLMs may inhibit the performance of the next reasoning and creation tasks. The weakness of LVLMs' reasoning capability demanding "Professional Knowledge", "Emotional Intelligence", "Imagination", and "Sense of Space" may hinder the performance of the next creation. iii) The performances across different tasks of different LVLMs show a similar distribution, which suggests the development of current LVLMs are synchronous. iv) Performance improves as the language model size of LVLM increases. v) A declined performance between the first turn and subsequent turns shows that LVLMs tend to generate comprehension biases as the multi-turn conversation progresses or forget the information of previous turns. vi) The high-quality dialogue history provides important guidance to the LVLMs' responses and plays an important role in in-context learning examples.

The contributions of our work are summarized as follows. (1) We present the first open-ended multi-turn visual conversations benchmark ConvBench. It is challenging, real-world, and aligns with user behavior. (2) In each conversation, ConvBench progressively examines a three-level hierarchy of multimodal capabilities, including perception, reasoning, and creativity. It can decouple these capabilities in evaluations and conduct reliable error attribution. (3) We present an automatic evaluation to handle the challenging open-ended multi-turn conversations and achieve high agreement with the human evaluation. (4) Our experimental results reveal that ConvBench is challenging for current LVLMs, even for GPT4V. Besides, we have some insightful findings from experiments.

## 2 Related Work

### Large Vision-Language Models.

Building upon the achievements of Large Language Models (LLMs) [14; 15; 16], Large Vision-Language Models (LVLMs) [1; 17; 18; 19; 20; 21; 22; 23; 24; 25] have recently showcased remarkable proficiency across various tasks, demonstrating advanced perception, reasoning, and creative capabilities. A favored approach to enhancing LVLMs involves integrating visual knowledge into the semantic framework of LLMs, thereby leveraging the LLMs' strong performance in interpreting and responding to prompts. For instance, BLIP-2 [26] introduces the Q-Former to synchronize vision foundation models with LLMs without modifying the underlying models. MiniGPT4 [17] utilizes a straightforward fully connected layer, requiring only a minimal set of caption data. LLaVA [18] enhances the LLM with high-quality instructional data generated by GPT4. QWen-VL [19] undergoes fine-tuning with high-resolution images, employing multi-task training strategies. mPLUG-DocOwl [20] expands the capabilities of LVLMs to include document understanding.

### Large Vision-Language Models Benchmarks.

With the advancement of Vision-Language Models (LVLMs), existing standard evaluation benchmarks like MSCOCO [13], GQA [27], VQA [28; 29], etc., are no longer sufficient to assess the comprehensive multimodal abilities of LVLMs. In response, a variety of benchmarks have been developed specifically for LVLM evaluation, including OwlEval [20], LAMM [12], LVLM-eHub [11], SEED [10], MMBench [9], and MM-Vet [8]. These benchmarks primarily focus on assessing basic perceptual abilities. In addition, VisIT-Bench [7] covers a broad spectrum of tasks, ranging from simple recognition to complex reasoning. Recent research has also introduced LVLM benchmarks requiring expert-level domain knowledge and intricate reasoning, such as MathVista [6] and MMMU [5] and MMT-Bench [4]. However, these benchmarks tend to address perception, reasoning, and creation tasks in isolation without establishing connections among these tasks. Furthermore, the current benchmarks predominantly focus on single-turn interactions. The ConvBench addresses these gaps by not only offering a hierarchical ablation evaluation that moves from perception through reasoning to creation but also by evaluating LVLMs' capabilities in multi-turn conversational contexts.

### Benchmarks for Multi-turn Large Models.

For LLMs, there have been some classic work. MT-Bench [30] is the first multi-turn conversation benchmark, which focuses on two-turn follow-up dialogues across eight topics ("Writing", "Knowledge", "Math" and so on). It only focuses on the most basic and important multi-turn abilities of Context Memory and Anaphora Resolution. MT-Bench-101 [31] is the first dataset to specifically focus on 13 fine-grained multi-turn dialogue abilities, such as, "Separate Input", "Topic Shift", "Content Confusion", "Content Rephrasing" and so on. Otherwise, there are other benchmarks: MT Bench++ [32] is an eight-turn multi-turn conversation benchmark to qualitatively evaluate multi-turn instruction following ability. It is built on expanding MT-Bench by manually annotating six additional follow-up questions. MINT [33] is a benchmark that evaluates LLMs' ability to solve challenging tasks with multi-turn interactions by using tools and leveraging natural language feedback. For LVLMs, to the best of our knowledge, ConvBench is the first multi-turn visual conversation benchmark, which can also be considered as a multi-modal version of MT-Bench, but with the innovative addition of hierarchical ablation testing. ConvBench depends on manual annotation for obtaining the three-turn interactive instructions to allow evaluating multi-turn instruction-following ability for visual dialogue. ConvBench also mostly focuses on the most basic and important multi-turn visual abilities of Context Memory and Anaphora Resolution.

## 3 ConvBench

### Overview of ConvBench

The ConvBench includes 577 image-instruction pairs tailored for multi-turn dialogues. Each pair is structured with three sequential instructions, each targeting a distinct cognitive skill--beginning with perception, followed by reasoning, and culminating in creation. This structure underscores the cognitive evolution from basic perceptual comprehension to logical reasoning and finally to sophisticated creative expression. The detailed definition and division for perception, reasoning, and creation can be seen in the appendix. As shown in Figure 2, our benchmark, encompassing 215 tasks, is divided into 71 tasks focused on perception, 65 on reasoning, and 79 on creation. These practical tasks are creative, useful, and real-world demands, which are downstream users of language technologies are likely to need. Existed benchmarks for LVLMs almost focus on computer-vision deep learning tasks, which cannot assess the ability of LVLMs to solve the diverse and never-before-seen real-world tasks. VisIT-Bench [7] has first proposed a path to evaluate LVLMs for practical tasks. We inherit and extend it to evaluate multi-turn visual conversation capabilities and explore the hierarchical ablation evaluations for LVLMs.

### Data Curation Process

**Data Collection.** Our benchmark collection is structured into five distinct stages, as depicted in Figure 3. Annotators play a very important role in the process. These stages are as follows:

**i) Multi-turn Instruction Formation.** We extend each single-turn sample in VisIT-Bench to a sample with three-turn instructions. We ensure the first, second, and third turns are represented as perception, reasoning, and creation, respectively. The instructions are designed based on the last ones.

**ii) Task Category Induction.** We derive the task categories by inducing them from instructions. The bottom-up approach to task collection guarantees that the tasks under investigation are tailored to meet real-world requirements. ConvBench consists of 215 tasks, and we have included the details in the supplementary materials. Task category induction is done with human annotation to ensure reasonableness and accuracy. For example, if the question is "How to write the recipe for the food shown in the image?", the task is inducted as "Recipe Writing".

**iii) Instruction-Conditioned Caption Annotation.** VisIT-Bench [7] has first proposed the generation of the instruction-conditioned caption. The two important purposes for generating instruction-conditioned captions have been evaluated in the VisIT-Bench. One is to provide a comprehensive description of the image for building a robust automated assessment framework. The other is generating raw reference answers for humans to verify in the next "Reference Generation" step. Therefore, in this work, the image and the instructions are also provided for the annotators to generatea caption. We first prompt GPT4V with "Describe this image in detail." We then polish the responses by humans according to the instructions to obtain the final instruction-conditioned caption.

**iv) High-Quality Reference Generation.** Similar to VisIT-Bench [7], they verify each response with human annotators. For each sample, we feed GPT4V with the instruction-conditioned caption, the image, multi-turn instructions, and our well-designed prompt in a multi-turn conversation fashion to generate each instruction's response. We meticulously refine these responses as reference answers, removing their biases and enhancing their quality and relevance.

**v) Focus Point Annotation.** The creativity instruction is an open-ended question without a standard answer. Therefore, we annotate specific focus points related to each creation instruction. These annotations are used as criteria to assess whether the model produces instructive answers to the instruction, seeing Step 5 in Figure 3.

## 4 ConvBenchEval and Hierarchical Ablation Evaluation

Human evaluation is really meaningful for judging human preference responses. However, it is costly to obtain human judgments for new model submissions. Similar to VisIT-Bench [7] and MT-Bench [30], to support faster model development, we introduce \(\mathrm{ConvBenchEval}(\cdot)\), an automated evaluation pipeline designed for multi-turn visual conversation assessment and hierarchical ablation evaluation. It aligns best with human preferences, whose agreement with human evaluation reaches \(81.83\%\). More agreement experiment results can be found in the appendix, which can validate the effectiveness of our evaluation methodology.

\(\mathrm{ConvBenchEval}(\cdot)\) comprises four key components: perception, reasoning, creation, and overall conversation evaluation modules, as shown in Figure 4. We recursively use \(\mathrm{ConvBenchEval}(\cdot)\) in three settings as follows to conduct the hierarchical ablation evaluation for error attribution. For clarity, we denote the instruction, model response, reference, and focus points as \(I_{i},M_{i},R_{i}\), and \(F_{i}\) at each turn, respectively, where \(i\) indicates the turn index. Note that \(F_{1}\) and \(F_{2}\) are null focus points.

**i) ConvBenchEval for Evaluating the Performances of Each Turn and Overall Conversation.** The model response of each turn is obtained in the principle of multi-turn conversation. The response of each turn is generated based on the front instructions and responses in this setting. The formula for generating each turn's response can be expressed as \(M_{i}=f(\{I_{i}\}_{i=0}^{i-1},\{M_{i}\}_{i=0}^{i-1},I_{i}),i=1,2,3\), where \(f\) denotes the model inference function, \(I_{i}\) denotes the instruction at the turn \(i\), \(M_{i}\) denotes

Figure 2: Visualization of example tasks in ConvBench. It consists of 215 tasks constructed in perception, reasoning, and creation hierarchy.

the response at the turn \(i\). In the first turn, there is no front instruction (\(I_{0}\)) or response (\(M_{0}\)). The evaluation process can be expressed as \(S_{i},J_{i}=\mathrm{ConvBenchEval}(\{M_{i}\}_{i=1}^{3},\{R_{i}\}_{i=1}^{3},F_{i};P_ {i}),i=1,2,3\) where \(S_{i}\) is the capability score to the turn \(i\), \(J_{i}\) is the judgment from the ChatGPT to the turn \(i\), and \(P_{i}\) is the prompt specific to the turn \(i\). Finally, we feed all instructions, responses, focus points, and judgments into ChatGPT to obtain the overall conversation score as given by \(S_{O},J_{O}=\mathrm{ConvBenchEval}(\{M_{i}\}_{i=1}^{3},\{R_{i}\}_{i=1}^{3},\{J_ {i}\}_{i=1}^{3};P_{O})\) where \(S_{O}\) is the capability score for overall multi-turn conversation, \(J_{O}\) is the judgment from the ChatGPT to the overall multi-turn conversation, \(P_{O}\) is the prompt for evaluating the overall conversation capability.

**ii) Hierarchical Ablation Evaluation with Perfect Perception Condition.** In this setting, the influence of inaccurate perception on reasoning, creation, and overall conversation can be derived. We directly use the human-annotated perception answers as the responses of the perception turn, replacing the outputs from LVLMs at the perception turns, which can be represented as \(\hat{M}_{1}=R_{1}\). The model inference at the reasoning and creation turns can be written as \(\hat{M}_{i}=f(\{I_{i}\}_{i=1}^{i-1},\{\hat{M}_{i}\}_{i=1}^{i-1},I_{i}),i=2,3\), where \(f\) denotes the model inference function, \(I_{i}\) denotes the instruction at the turn \(i\), \(M_{i}\) denotes the response at the turn \(i\) with the perfect perception conditions. The evaluation process without considering perception error can be expressed as \(\hat{S}_{i},\hat{J}_{i}=\mathrm{ConvBenchEval}(\{\hat{M}_{i}\}_{i=1}^{3},\{R_{i} \}_{i=1}^{3},F_{i};P_{i}),i=2,3\). Finally, the overall conversation score without considering perception score can be given by \(\hat{S}_{O},\hat{J}_{O}=\mathrm{ConvBenchEval}(\{\hat{M}_{i}\}_{i=1}^{3},\{R_{ i}\}_{i=1}^{3},\{\hat{J}_{i}\}_{i=2}^{3};P_{O})\), where \(\hat{S}_{O}\) is the capability score for overall multi-turn conversation with perfect perception conditions, \(\hat{J}_{O}\) is the judgment from the ChatGPT for the overall multi-turn conversation with perfect perception conditions, \(P_{O}\) is the prompt for evaluating the overall conversation capability. By comparing \(S_{i}\) and \(\hat{S}_{i}\) (\(i=2,3,O\)), we can see how perception error affects the performance of reasoning, creativity, and overall conversation.

**iii) Hierarchical Ablation Evaluation with Perfect Perception and Reasoning Conditions.** In this setting, we further explore how reasoning errors affect creativity and overall conversation. The human-annotated perception answer and reasoning answer are directly used as the responses of the perception turn and reasoning turn, respectively, replacing the original responses from LVLMs, which can be represented as \(\tilde{M}_{i}=R_{i},i=1,2\). Then, the model inference at the creation turn can be written as \(\hat{M}_{i}=f(\{I_{i}\}_{i=0}^{i-1},\{\hat{M}_{i}\}_{i=1}^{i-1},I_{i}),i=3\), where \(f\) denotes the model inference function, \(I_{i}\) denotes the instruction at the turn \(i\), \(\tilde{M}_{i}\) denotes the response at the turn \(i\) with perfect perception and reasoning conditions. The evaluation process without considering perception and reasoning error can be expressed as \(\hat{S}_{i},\tilde{J}_{i}=\mathrm{ConvBenchEval}(\{\tilde{M}_{i}\}_{i=1}^{3},\{ R_{i}\}_{i=1}^{3},F_{3};P_{i}),i=3\). Finally, the overall conversation score without considering perception and reasoning score can be given

Figure 3: The pipeline of data curation. We develop three multi-turn instructions for each image to assess the perception, reasoning, and creation capabilities. We also annotate the referenced answers to facilitate automatic evaluation and error analysis.

by \(\tilde{S}_{O},\tilde{J}_{O}=\mathrm{ConvBenchEval}(\{\tilde{M}_{i}\}_{i=1}^{3},\{R_{ i}\}_{i=1}^{3},\tilde{J}_{3};P_{O})\), where \(\tilde{S}_{O}\) is the capability score for overall multi-turn conversation with perfect perception and reasoning conditions, \(\tilde{J}_{O}\) is the judgment from the ChatGPT to the overall multi-turn conversation with perfect perception and reasoning conditions, \(P_{O}\) is the prompt for evaluating the overall conversation capability. By comparing \(\hat{S}_{i}\) and \(\tilde{S}_{i}\) (\(i=3,O\)), we can check how perception and reasoning errors affect the performance of creativity and overall conversation.

**Pairwise Grading.** Note that the function in i) - iii) \(\mathrm{ConvBenchEval}(\cdot)\) is a pairwise scheme as described in the following. Similar to VisIT-Bench [7], \(\mathrm{ConvBenchEval}(\cdot)\) also employs a pairwise grading scheme and an absolute metric. Specifically, all the LVLM responses and the references shown in Figure 4, are anonymously presented to the ChatGPT in a random order. The ChatGPT is tasked with a pairwise comparison to decide the responses from which is superior. The percentages of cases where the ChatGPT prefers the output from LVLM rather than the human-verified reference output are defined as the absolute metric, _i.e._ win rate. This metric can directly judge whether the LVLMs' performance reaches human preference. The system prompts designed for the pairwise grading scheme for obtaining the win rate, detailed in the Appendix, encourage the ChatGPT to engage in a step-by-step thought process, making its reasoning explicit. In our forced-choice setup, ties are not permitted; thus, if the ChatGPT deems the responses of equal quality, it is instructed to select one arbitrarily.

## 5 Experiment and Analysis

### Hierarchical Ablation Evaluation Comparisons and Analysis

We employed our proposed \(\mathrm{ConvBenchEval}(\cdot)\) methodology to perform a quantitative analysis. The outcomes of the evaluation results are detailed in Table 1. \(S_{1}\), \(S_{2}\), and \(S_{3}\) denote the scores for perception, reasoning, and creation, respectively. Meanwhile, \(\hat{S}_{2}\) and \(\hat{S}_{3}\) correspond to the scores for reasoning and creation, respectively, and under conditions of perfect perception. \(\tilde{S}_{3}\) is the score for creation, assuming perfect conditions for both perception and reasoning. We present our principal insights from the evaluation results as follows:

(1) **ConvBench Provides Challenge for LVLMs.** This benchmark sets formidable challenges for modern models. GPT4V, despite being a sophisticated model, shows only modest achievements in perception, reasoning, and creation. Moreover, there is still a gap in performance between open-source models and closed-source models in various real-world use cases. ConvBench exposes a stark discrepancy between the performance of these models and that of humans.

(2) **Weak Perception Undermines LVLMs' Reasoning and Creation Performance.** Under conditions of perfect perception, we see significant improvements in reasoning and creation abilities, as indicated by the data in the \(\hat{S}_{2}\) and \(\hat{S}_{3}\) columns of Table 1. The figure reflects the enhancement in reasoning and creation attributed to impeccable perception. Across \(20\) LVLMs, the average increase

Figure 4: An illustration of the evaluation pipeline of ConvBench.  indicates the ChatGPT.

in reasoning and creation scores are \(11.38\) and \(5.65\), respectively. By analyzing these enhanced reasoning or creation cases with perfect perception information, we found that the **"OCR" Perception Task** may influence the performances of "In-context Visual Scene Understanding", "Meme Reasoning", and "Chart Reasoning". The **Fine-Grained Perception Tasks** like "Location Recognition" perception task may influence the performances of "Location Understanding" and "Travel Plan Writing" tasks, "Celebrity Recognition" perception task may influence the performances of "Celebrity Understanding", "PowerPoint Production" and "Cultural Knowledge Reasoning" tasks. The **Spatial Perception Tasks** like "Board Chess Position Description" may influence the performances of "Board Game Reasoning" and "Diagram Generation" tasks. Our benchmark clarifies the origins of these errors in reasoning and creativity, determining whether they stem from visual perception issues or language reasoning shortcomings. With the aid of human-verified visual comprehension, the authentic strengths of the language module in reasoning and creation will be more precisely evaluated.

(3) **Limited Reasoning Impacts LVLM's Creation Abilities.** Under ideal conditions for perception and reasoning, shifts in creation capabilities are documented in the \(\tilde{S}_{3}\) column of Table 1. The numbers in brackets indicate adjustments in creation scores due to human-verified reasoning accuracy. Among the \(20\) LVLMs evaluated, there is an average increase of \(5.32\) in creation scores, which indicates that reasoning inaccuracies can adversely affect LVLMs' performance in creative tasks. By analyzing these enhanced creation cases with perfect reasoning information, we found that some reasoning tasks involving **Professional Knowledge** influence the next creation tasks. For example, "Physical Knowledge Reasoning" may influence the accuracy of "Physical Problem Computing". Some reasoning tasks needing **Emotional Intelligence** influence the relative creation tasks. For example, "Human Emotion Reasoning" may influence the performance of "Blog Writing", "Humanity Discussion" and "How Visual Content Arouses Emotions" tasks. Some reasoning tasks containing **Imagination** like "Tangram Speculation" may influence the corresponding creative task ("Tangram Segmentation"). Some reasoning tasks requiring **Sense of Space** like "Location Relative Position" may influence the "Navigation" task.

(4) **LVLM's Performance across Various Real-world Tasks.** As depicted in Figure 2 in the appendix, LVLMs demonstrate weak performance in fine-grained tasks, such as "Movie recognition", "Position Description", "Outfit recognition", "Make-up Description" and so on. Lack of real-world application data for pretraining and finetuning may lead to the weakness. The datasets used for pretraining and finetuning are more high-quality, the better performance will be. The performances of open source LVLMs, which are above \(14.00\), are all using various high-quality datasets for training. The superiority of model architecture may be constructed on high-quality datasets. Also as shown in

\begin{table}
\begin{tabular}{l|c c c|c c c c c c c c} \hline Model & \(R_{1}\) & \(R_{2}\) & \(S_{1}\) & \(S_{3}\) & \(S_{4}\) & \(S_{5}\) & \(S_{6}\) & \(S_{7}(S_{5}-S_{5})\) & \(S_{8}(S_{9}-S_{5})\) & \(S_{9}(S_{9}-S_{5})\) & \(S_{9}(S_{10}-S_{10})\) \\ \hline \hline
**GPTAV**[1] & **39.51** & **38.47** & 38.47 & **39.34** & **37.61** & **40.55** & **47.31**(+6.97) & 37.78(+6.61) & 37.61(+2.94) & 38.99(+1.21) & 38.30(+0.69) \\ \hline Climbo [1] & 36.60 & 37.49 & **38.99** & 39.17 & 34.32 & 35.70 & 45.93(+6.76) & **38.99(+6.67)** & **43.15**(+7.45) & **38.16**(+0.17) & **40.21**(+2.94) \\ \hline Rea Fun3L [3] & 25.60 & 24.65 & 23.15 & 27.56 & 21.32 & 25.62 & 23.95(+3.57) & 22.84(+5.56) & 25.82(+7.07) & 27.84(+19.56) & 26.00(+0.13) \\ \hline IntervL-Climbo-V1-2 [3] & 21.77 & 22.41 & 24.96 & 21.31 & 20.97 & 19.93 & 22.20(+7.05) & 23.26(+7.28) & 29.94(+7.1) & 33.65(+7.53) & 31.65(+5.54) \\ \hline IntervVL-Climbo-V1-5 [3] & 17.65 & 20.22 & 26.09 & 17.33 & 17.33 & 15.08 & 27.75(+10.40) & 23.40(+6.07) & 25.15(+10.05) & 23.24(+2.84) & 33.81(+6.67Figure 2 in the appendix, the performance of open-source models with better performance on different tasks shows a similar distribution as that of closed-source models. It means that the challenges of real-world cases faced by open-source and closed-source models are similar, which suggests the development of current LVLMs is synchronous. ConvBench aids in highlighting the strengths and weaknesses of the instruction-following LVLMs along various real-world use cases.

### Multi-Turn Conversation Comparisons and Analysis

The results of the multi-turn conversation evaluation are meticulously outlined in Table 1. \(S_{O}\) represents the scores for multi-turn conversation performance. Concurrently, \(\hat{S}_{O}\) signifies the scores for multi-turn conversation performance under the assumption of flawless perception. Moreover, \(\tilde{S}_{O}\) reflects the score for multi-turn conversation performance, premised on ideal conditions for both perception and reasoning. We delineate our key findings from the experimental results as follows:

(1) **The Challenge of Follow-up Multi-Turn Conversation for LVLMs.** ConvBench is manually tailored by designing the questions based on the last responses, always referencing specific contents mentioned in the last responses. Like the real-world application of a general-purpose AI assistant, a user always asks for additional information based on the assistant's prior responses. ConvBench provides a benchmark for evaluating the LVLM's follow-up ability to engage in coherent conversations. Table 1 shows that closed-source LVLMs, including GPT4V, Claude, and Reka, generally outperform open-source ones in the follow-up capability of multi-turn dialogues. ConvBench presents substantial challenges in multi-turn visual conversations with LVLMs and plays a role in the LVLM field, such as MT-Bench [30] in the LLM field.

(2) **Per-Turn Performance.** According to Table 1, we compute the mean scores of \(20\) models for each turn, to explore the impact of turn count on LVLMs' performance. The average performances of LVLMs are \(15.44\), \(15.40\), and \(12.55\) in the first, second, and third turn, respectively, which shows a decline between the first turn and subsequent turns, which suggests that LVLMs tend to generate comprehension biases as the multi-turn dialogue progresses or forget the content of previous turns.

(3) **The Impact of Dialogue History in Multi-Turn Conversation for LVLMs.** When comparing the \(S_{O}\) column against the \(\hat{S}_{O}\) and \(\tilde{S}_{O}\) columns in Table 1, the numbers in parentheses illustrate the improvement in overall multi-turn conversation performance resulting from flawless previous responses. The enhancement indicates that the LVLMs can leverage previous responses to improve the responses in the current turn. We also find that the high-quality dialogue history, which plays an important role in in-context learning examples, provides effective guidance for the LVLM's responses.

(4) **Effect of Language Model Size.** We find that the trend of increasing language model size is related to an improvement in LVLM's performance on multi-turn conversation by comparing the InterVL-Chat-v1-2, ShareGPT4-13B, and LLaVA-V1.5-13B with the InterVL-Chat-v1-5, ShareGPT4-7B, and LLaVA-V1.5-7B. The detailed model information is shown in the supplementary.

## 6 Conclusion and Limitation

We introduce a multi-turn conversation benchmark named ConvBench for LVLMs. It comprises 577 multi-turn conversations across 215 tasks. Each conversation consists of three-level hierarchical questions: perception, reasoning, and creation. It can decouple capacities in evaluation for more accurate error attribution. Besides, an automatic evaluation pipeline is proposed. From the experimental results, ConvBench is challenging for current LVLMs, and we also have some insightful findings.

**Limitation** Each conversation in ConvBench is constructed through meticulous annotations and large labor to ensure quality. Thus, the data scale is not too large. We will continually expand ConvBench in terms of data scale and the number of tasks in our future work.

**Broader Impact** We hope this work can blaze a new trail in LVLMs evaluation. We do not foresee obvious undesirable ethical/social impacts at this moment.

## Acknowledgments and Disclosure of Funding

This paper is partially supported by the National Key R&D Program of China (No.2022ZD0161000, No.2022ZD0160101, No.2022ZD0160102) and the General Research Fund of Hong Kong No.17200622 and 17209324.

## References

* [1] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). _arXiv preprint arXiv:2309.17421_, 9(1):1, 2023.
* [2] Claude Team. Meet claude; claude is a family of foundational ai models that can be used in a variety of applications. 2024.
* [3] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. _arXiv preprint arXiv:2312.14238_, 2023.
* [4] Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, Jiayi Lei, Quanfeng Lu, Runjian Chen, Peng Xu, Renrui Zhang, Haozhe Zhang, Peng Gao, Yali Wang, Yu Qiao, Ping Luo, Kaipeng Zhang, and Wenqi Shao. Mmt-bench: A comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi, 2024.
* [5] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. _ArXiv_, abs/2311.16502, 2023.
* [6] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chun yue Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. 2023.
* [7] Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao, Wanrong Zhu, Anas Awadalla, Josh Gardner, Rohan Taori, and Ludwig Schimdt. Visit-bench: A benchmark for vision-language instruction following inspired by real-world use. _ArXiv_, abs/2308.06595, 2023.
* [8] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. _ArXiv_, abs/2308.02490, 2023.
* [9] Yuanzhan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player? _ArXiv_, abs/2307.06281, 2023.
* [10] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. _ArXiv_, abs/2307.16125, 2023.
* [11] Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Jiao Qiao, and Ping Luo. Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models. _ArXiv_, abs/2306.09265, 2023.
* [12] Zhen fei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai Li, Lu Sheng, Lei Bai, Xiaoshui Huang, Zhiyong Wang, Wanli Ouyang, and Jing Shao. Lamm: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark. _ArXiv_, abs/2306.06687, 2023.
* [13] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In _European Conference on Computer Vision_, 2014.

* [14] OpenAI. Introducing chatgpt. 2022.
* [15] Hyung Won Chung, Le Hou, S. Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Wei Yu, Vincent Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed Huai hsin Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models. _ArXiv_, abs/2210.11416, 2022.
* [16] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. _ArXiv_, abs/2302.13971, 2023.
* [17] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. _ArXiv_, abs/2304.10592, 2023.
* [18] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _ArXiv_, abs/2304.08485, 2023.
* [19] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. _arXiv preprint arXiv:2308.12966_, 2023.
* [20] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yi Zhou, Junyan Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qiang Qi, Ji Zhang, and Feiyan Huang. mplug-owl: Modularization empowers large language models with multimodality. _ArXiv_, abs/2304.14178, 2023.
* [21] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. _ArXiv_, abs/2312.14238, 2023.
* [22] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning. _ArXiv_, abs/2305.03726, 2023.
* [23] Gemini Team. Gemini: A family of highly capable multimodal models, 2023.
* [24] Xiao-Wen Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. _ArXiv_, abs/2401.16420, 2024.
* [25] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, W. Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu Jiao Qiao. Llama-adapter v2: Parameter-efficient visual instruction model. _ArXiv_, abs/2304.15010, 2023.
* [26] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In _International Conference on Machine Learning_, 2023.
* [27] Drew A. Hudson and Christopher D. Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. _2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 6693-6702, 2019.
* 31, 2015.
- 414, 2016.
* [30] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Haotong Zhang, Joseph Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. _ArXiv_, abs/2306.05685, 2023.
* [31] Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, and Wanli Ouyang. Mt-bench-101: A fine-grained benchmark for evaluating large language models in multi-turn dialogues, 2024.
* [32] Yuchong Sun, Che Liu, Jinwen Huang, Ruihua Song, Fuzheng Zhang, Di Zhang, Zhongyuan Wang, and Kun Gai. Parrot: Enhancing multi-turn chat models by learning to ask questions. _CoRR_, abs/2310.07301, 2023.
* [33] Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. Mint: Evaluating llms in multi-turn interaction with tools and language feedback, 2024.
* [34] Reka Team. Reka flash: An efficient and capable multimodal language model. 2024.
* [35] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. _arXiv preprint arXiv:2311.12793_, 2023.
* [36] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.
* [37] Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition. _arXiv preprint arXiv:2309.15112_, 2023.
* [38] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration, 2023.
* [39] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu Qiao. Llama-adapter v2: Parameter-efficient visual instruction model. _arXiv preprint arXiv:2304.15010_, 2023.
* [40] DataCanvas Ltd. mmalaya. https://github.com/DataCanvasIO/MMAlaya, 2024.
* [41] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important things for large multi-modal models. _arXiv preprint arXiv:2311.06607_, 2023.
* [42] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, 2023.
* [43] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023.
* [44] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. _arXiv preprint arXiv:2401.16420_, 2024.
* [45] Zhe Chen, Weijun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. _arXiv preprint arXiv:2404.16821_, 2024.
* [46] Haoxuan You, Rui Sun, Zhecan Wang, Long Chen, Gengyu Wang, Hammad A. Ayyubi, Kai-Wei Chang, and Shih-Fu Chang. Idealgpt: Iteratively decomposing vision and language reasoning via large language models. _ArXiv_, abs/2305.14985, 2023.

LVLMs Configuration

Table 2 summarizes the LVLMs evaluated in this paper, including their model sizes, visual encoders, and LLMs.

## Appendix A

\begin{table}
\begin{tabular}{l l l l} \hline \hline Models & Parameters & Vision Encoder & LLM \\ \hline GPT4V [1] & - & - & - \\ Claude [2] & - & - & - \\ Reka Flash [34] & - & - & - \\ ShareGPT4V-7B [35] & 7.2B & CLIP ViT-L/14 & Vicuna-v1.5-7B \\ ShareGPT4V-13B [35] & 13.2B & CLIP ViT-L/14 & Vicuna-v1.5-13B \\ LLaVA-v1.5-7B [36, 43] & 7.2B & CLIP ViT-L/14 & Vicuna-v1.5-7B \\ LLaVA-v1.5-13B [36, 43] & 13.4B & CLIP ViT-L/14 & Vicuna-v1.5-13B \\ XComposer [37] & 8B & EVA-CLIP-G & InternLM-7B \\ XComposer2 [44] & 7B & CLIP ViT-L/14 & InternLM2-7B \\ mPLUG-Owl2 [38] & 8.2B & CLIP ViT-L/14 & LLaMA2-7B \\ QWenVL [19] & 9.6B & CLIP ViT-G/16 & QWen-7B \\ LLaMA-Adapter-v2 [39] & 7B & CLIP-ViT-L/14 & LLaMA-7B \\ BLIP2-Flan-T5-XL [42] & 12.1B & EVA-CLIP ViT-G/14 & Flan-T5-XL \\ BLIP2-Flan-T5-XXL [42] & 12.1B & EVA-CLIP ViT-G/14 & Flan-T5-XXL \\ InternVL-Chat-V1.2 [3] & 40B & InternViT-6B & Nous-Hermes-2-Yi-34B \\ InternVL-Chat-V1.5 [45] & 26B & InternViT-6B & InternLM2-20B \\ Monkey [41] & 9.8B & CLIP-ViT-BigHuge & Qwen-7B \\ MiniGPT4 [17] & 8.0B & EVA-G & Vicuna-7B \\ MMAlaya [40] & 7.8B & BLIP2-opt-2.7b & Alaya-7B-Chat \\ Otter [22] & 1.3B & CLIP ViT-L/14 & LLaMA-7B \\ \hline \hline \end{tabular}
\end{table}
Table 2: Model architecture of \(20\) LVLMs evaluated on ConvBench.

## Appendix B Task Category

As shown in Figure 6, the performance of open-source models with better performance on different tasks shows a similar distribution as that of closed-source models. It means that the challenges of real-world cases faced by open-source and closed-source models are similar, which suggests the development of current LVLMs is synchronous. ConvBench aids in highlighting the strengths and weaknesses of the instruction-following LVLMs along various real-world use cases.

Figure 5: List of existing task categories in ConvBench.

Figure 6: The Visualizations of model performance across various categories.

[MISSING_PAGE_FAIL:16]

## Appendix D Cost Estimate for Evaluation

According to the the process of ConvBench. As shown in Table 1, a comprehensive evaluation for a model need 9 scores including \(S_{1}\), \(S_{2}\), \(S_{3}\), \(S_{O}\), \(\hat{S}_{2}\), \(\hat{S}_{3}\), \(\hat{S}_{O}\), \(\tilde{S}_{3}\), and \(\tilde{S}_{O}\), The instruction-conditioned-caption is dense and the reference answers are annotated in detail. A comprehensive experiment need to consume about 42-52 billion tokens. In total, at current ChatGPT prices (0.5 dollars per 1M tokens), the multi-turn comparison evaluations required to assess a new model costs about 21-26 dollars. When we conduct our experiments, ChatGPT is the most cost-effective. The cost is satisfied, and thus, our evaluation is easily accessible to the community.

## Appendix E Effectiveness for Automation Evaluation

We selected ChatGPT for evaluation in the early stages of our research to save on costs and make our evaluation framework more accessible to the community. We aim to ensure the effectiveness of the evaluation. We write high-quality instruciton-conditioned captions that include fine-grained descriptions of the image and information about the correct answers, which largely improves the performance of ChatGPT evaluation. In section C, we have assessed our automatic evaluation's agreement with human evaluation to have reached \(81.83\%\), with the results being essentially indistinguishable from those of GPT-4V (\(85.56\%\)). Every sample's instruction-conditioned caption not only includes fine-grained descriptions of the image but also incorporates the information about the correct answers to the instructions into the caption to ensure ChatGPT has enough information to make a convincing judgment. Although ChatGPT may have difficulty detecting subtle differences between generated samples, ChatGPT combined with detailed captions acts as a more impartial and effective judge for scoring. According to the example in Figure 1 (c), with the help of the detailed caption, ChatGPT can also detect subtle differences between generated samples. We use the prompt via ChatGPT for evaluating reasoning turn in pairwise grading method.

If we only use the image caption which is not involved with the information of correct answers to instructions, the judge's result will be inaccurate. The judge will wrongly think Assistant A and Assistant B are both incorrect. The detailed judge results are as follows, where Assistant A is a LVLM, and Assistant B is human.

**Caption used in this setting**

There is a chessboard in the image. The chessboard is arranged in a total of 64 squares. In order to facilitate describing the chess game, the chessboard is marked horizontally with the letters "abcdefgh" and vertically with the numbers "12345678". For example, the lower left corner is "a1". The white King is on "e1", the white Queen is on "d1", the white Rooks are on "a1" and "h1", the white Bishops are on "c1" and "a4", the white Knights are on "b1" and "f3", the white Pawns are on "a2", "b2", "c2","e4", "f2","g2", and "h2". The black King is on "e8", the black Queen is on "d8", the black Rooks are "a8" and "h8", the black Bishops are "c8" and "f8", the black Knights are "c6" and "g8", the black Pawns are "a6", "b7", "c7", "d7", "e5", "f7", "g7" and "h7".

**Evaluation Result via ConvBenchEval in this setting**

Let's break down the second-turn responses from Assistant A and Assistant B to determine which one is better.

User's Second Turn Question: "I am the player with the white chess pieces. Is there a move I can take on this turn to capture one of my opponent's pieces?"

\begin{table}
\begin{tabular}{l l l l l} \hline \hline Set up Judge & Human & ChatGPT & GPT4 & GPT4V \\ \hline Human & - & 86.35\% & 87.30\% & 84.44\% \\ ChatGPT & - & - & 92.06\% & 87.94\% \\ GPT4 & - & - & - & 93.97\% \\ \hline \hline \end{tabular}
\end{table}
Table 7: Overall conversation agreement among different judgments. Human denotes the human expert. ChatGPT, GPT4, and GPT4V denotes \(\mathrm{ConvBenchEval}(\cdot)\) using ChatGPT, GPT4, and GPT4V, respectively. In GPT4V, we do not use annotated instruction-conditioned captions but the images.

Response from Assistant A: "Yes, as the player with the white pieces, you have a capturing move available. Your pawn on e5 can capture the black pawn on d6. This is written in algebraic notation as e5xd6."

Response from Assistant B: "Yes, you can capture your opponent's piece by moving your white knight from f3 to e5, capturing the black pawn."

Analysis: Accuracy: Assistant A: Incorrectly states that the white pawn on e5 can capture the black pawn on d6. This is incorrect based on the image context provided, as the white pawn on e5 is not in a position to capture a black pawn on d6. Assistant B: Incorrectly states that the white knight on f3 can move to e5 to capture a black pawn. However, based on the image context, there is no black pawn on e5; the black pawn is actually on e5, so this move is not possible either. Specificity: Assistant A: Provided a specific move in algebraic notation (e5xd6) but based on an inaccurate assessment of the board. Assistant B: Also provided a specific move, mentioning the knight move to e5, but again, this is not possible as described. Fluency: Both assistants provided fluent and understandable responses. However, fluency does not compensate for the lack of accuracy in both responses. Conclusion: Both Assistant A and Assistant B made errors in their responses by suggesting moves that are not possible based on the positions of the pieces described in the image context. However, Assistant B's mistake is somewhat less significant since it suggests a knight move that could be plausible with different board configurations, whereas Assistant A's move is completely off due to misidentifying the potential capture.

Overall, Response B is better despite both being incorrect, as it involves a lesser error in the interpretation of the board state.

If we use the image caption which is involved with the information of correct answers to instructions, the judge's result will be correct. The judge results are as follows, where Assistant A is a LVLM, and Assistant B is human.

**Caption used in this setting**

Caption used in this setting: There is a chessboard in the image. The chessboard is arranged in a total of 64 squares. In order to facilitate describing the chess game, the chessboard is marked horizontally with the letters "abcdefgh" and vertically with the numbers "12345678". For example, the lower left corner is "a1". The white King is on "e1", the white Queen is on "d1", the white Rooks are on "a1" and "h1", the white Bishops are on "c1" and "a4", the white Knights are on "b1" and "f3", the white Pawns are on "a2", "b2", "c2","e4", "f2", "g2", and "h2". The black King is on "e8", the black Queen is on "d8", the black Rooks are "a8" and "h8", the black Bishops are "c8" and "f8", the black Knights are "c6" and "g8", the black Pawns are "a6", "b7", "c7", "d7", "e5", "f7", "g7" and "h7". The player with white chess pieces can capture the opponent's piece by moving your white knight from f3 to e5, capturing the black pawn.

**Evaluation Result via ConvBenchEval in this setting**

To evaluate which second turn response better addresses the user's instruction, let's analyze both responses step by step.

The user's second instruction: "I am the player with the white chess pieces. Is there a move I can take on this turn to capture one of my opponent's pieces?"

Response from Assistant A: Content:

Assistant A suggests that the white pawn on e5 can capture the black pawn on d6, written as e5xd6. Accuracy:

This response is inaccurate based on the image context. The white pawn is not on e5; it is on e4, and there is no black pawn on d6 to capture. Relevance:

The response addresses the user's question directly but provides incorrect information. Specificity:

The move suggestion is clear but incorrect, which significantly reduces the response's value. Response from Assistant B: Content:

Assistant B correctly identifies that the white knight on f3 can capture the black pawn on e5 by moving to e5. Accuracy:This response is accurate according to the image context. The knight on f3 can indeed capture the black pawn on e5. Relevance:

The response is directly relevant to the user's question and provides the correct move. Specificity:

The move suggestion is both clear and accurate, making it a strong response. Conclusion: Response A provides incorrect information and suggests an impossible move. Response B correctly identifies a valid capture move for the white pieces. Overall, Response B is better.

## Appendix F Discussion on Automation for Annotation

### A Discussion on Automation for Multi-Turn Instruction Formation

It would surely be valuable to explore how to automate these processes to facilitate scaling. However, ensuring automatic annotation results of the same high quality as those of humans is undoubtedly very challenging. Our experiment results reveal LVLMs (like GPT4V) have weak performance for solving diverse and never-before-seen real-world tasks, and not to mention break down these kinds of tasks into sub-tasks (the first turn is perception, the second turn is reasoning and the third turn is creation.) Moreover, the pronouns (e.g it,they) are also needed for formatting the follow-up multi-turn instructions. Facing these challenges, there are some strategies for automating these processes to facilitate scaling: (1) GPT4V can be prompted to extend single-turn to multi-turn instructions, and human annotators rigorously filter these multi-turn instructions to form the final dataset. (2) A set of hierarchical instructions for a task (e.g board game playing) can be reused for multiple similar images. The samples in ConvBench can be used as seeds to facilitate scaling. (3) If ConvBench's method of decomposing instructions becomes a paradigm in the benchmarking field for multi-modal large models, with the power of the community, the hierarchical multi-turn instructions that are practical for different domains will be accumulated and scaled up. (4) Depending on the above collection methods of hierarchical multi-turn instructions, a high-quality pretrained dataset for extending single-turn to multi-turn instructions can also be organized. The dataset can be applied for Continue PreTraining and Supervised Finetuning on LLMs or LVLMs to obtain a large model, which can extend single-turn to multi-turn instructions automatically and effectively.

### A Discussion on Automation for Task Category Induction

Using a large language model for Task Category Induction can be a very feasible solution for automation for task category induction. However, there are still some hidden problems: (1) the results of induction via LLMs are not correct. (2) A task is summarized into two forms of expression (e.g. Travel Plan Writing and Formulating a Travel Plan), which will be represented as two kinds of tasks. Therefore, necessary manual annotations are still required to ensure the accuracy of Task Category Induction so far, and we will continue research in future work.

[MISSING_PAGE_EMPTY:20]

[MISSING_PAGE_FAIL:21]

Additional Experimental Results

### Chain of Thoughts

ConvBench can also be employed to examine studies on chain-of-thought reasoning. As illustrated in Figure 8, the single-turn approach, which involves directly requesting reasoning or creation instructions, yields inferior results compared to the multi-turn method. Figure 9 is an example in ConvBench which demonstrates how a multi-turn approach can bolster reasoning capabilities. Several studies have introduced techniques to enhance this performance. For instance, IdealGPT [46] generates sub-questions derived from the main question and responds with the corresponding sub-answers. It then analyzes the aggregate information from these sub-answers to deduce the most likely answer to the main question. Its experimental results also indicate that the chain-of-thought framework's performance surpasses that of Large Vocabulary Language Models (LVLMs) in a zero-shot context.

However, it is indeed worthwhile to investigate whether the decomposed sub-questions can contribute to solving the main question. ConvBench provides annotations to test the effectiveness of these decomposed sub-questions in answering the main question.

Figure 8: The performance comparison between single-turn and multi-turn manner for GPT4V, using both Pairwise Grading method and Direct Grading method.

Figure 9: This example which demonstrates that a multi-turn approach can bolster reasoning capabilities.

### Ablation Studies

#### i.2.1 Are Focus Demands necessary?

The creativity instruction presents an open-ended question that lacks a standard answer. We identify and annotate specific focal points related to each creative prompt to guide the evaluation process. To examine the effectiveness of these focal points, we conduct an experiment involving \(50\) creative instances. The experiments are divided into two groups: one that does not incorporate focal points in creation evaluations and the other that does. All other variables between the groups are kept constant. We manually assessed which one appeared more logical in each pair of evaluation outcomes from the two groups. The group using focal points achieves a \(56\%\) win rate, demonstrating the effectiveness of focal points. Figure 11 provides an example.

#### i.2.2 Are the three turns' judgments helpful for the overall conversation evaluation?

We feed the perception, reasoning, and creation judgments into the final overall conversation evaluation. To examine their necessities, we conduct an experiment involving \(50\) instances. The experiments are divided into two groups: one that does not include three turns' judgments in overall conversation evaluations, and the other that does. All other variables between the groups are kept constant. We manually assessed which one appeared more logical in each pair of evaluation outcomes from the two groups. The group using the three turns' judgments achieves a \(60\%\) win rate. Moreover, \(8\%\) of instances fails to output a rating judgment while not using the three turns' judgments.

#### i.2.3 Multi-Turn Conversation Score vs. Average Score of Three Turns

As indicated in Table 1 in the main paper, the average differences between the multi-turn conversation scores (\(S_{O}\)) and the corresponding average scores of the three turns (\(R_{2}\)) for the \(20\) LVLMs are \(13.59\) and \(14.46\), respectively. This suggests that the multi-turn conversation scores are generally lower than the corresponding average scores of the three turns. This discrepancy implies that the \(\mathrm{ConvBenchEval}(\cdot)\) evaluates more than just the performance of individual responses. The \(\mathrm{ConvBenchEval}(\cdot)\) can also consider the instruction-following ability in a multi-turn conversation, which is more reasonable for evaluating multi-turn visual conversation.

Figure 10: This example assesses the effectiveness of focus points. Provided with an image and instructions, GPT4V generates responses. The evaluation outcome incorporating focus points is deemed more appropriate compared to the result without focus points.

## Appendix J Definition of Three Capabilities

**Perception:** This kind of task assesses an LVLM model's capability to replicate the human ability to process and extract meaningful information from visual inputs, necessitating an LVLM to have a comprehensive understanding of the image. These tasks typically involve generating text based on an image, such as celebrity recognition, food recognition, or car recognition.

**Reasoning:** This kind of task evaluates an LVLM model's ability to reason based on visual perception, demanding the application of commonsense knowledge using visual information to answer questions correctly. These tasks typically involve generating text based on an image, such as logic reasoning, meme reasoning, or visual commonsense reasoning.

**Creation:** This kind of task estimates an LVLM's ability to produce creative and diverse outputs without specific constraints or predefined templates based on the image. These tasks typically involve generating text based on an image, such as writing poems, stories, essays, or providing recommendations or suggestions.

## Appendix K Limitation

ConvBench, while encompassing a broad array of potential applications, does not claim to include every conceivable vision-language task. Our aspiration is to progressively augment the benchmark with additional categories of tasks over time. The current iteration of our research is primarily centered on image-text modalities. Future iterations may broaden the scope to encompass other modalities such as audio and video, thereby facilitating a more holistic and comprehensive evaluation framework. Furthermore, although the dataset presents a diverse array of tasks, an increased number of examples per category could enhance the depth and richness of the evaluation. Lastly, while our GPT-based metric demonstrates a strong correlation with human judgment at both the instance level and the system level, we have observed indications that this metric may exhibit a greater prediclection for outputs generated by GPT-based models compared to human preferences. Consequently, models that are trained, for instance, through the distillation of outputs from GPT4, might inadvertently gain an undue advantage within our current evaluation paradigm. This insight underscores the need for continuous refinement and calibration of our metrics to ensure equitable and unbiased assessment across a variety of model architectures and training methodologies.

Figure 11: This example evaluates the influence of judgments made across three turns on the entire conversational process. In this case, evaluating the overall conversation without incorporating these judgments fails to yield a definitive overall rating for the overall conversation.

Ethical Discussion

ConvBench is constructed upon the robust framework of the VisIT-Bench benchmark [7], which has been meticulously vetted through extensive ethical reviews and content filtering processes to guarantee adherence to the highest ethical standards. The inception of VisIT-Bench [7] was anchored in a commitment to ethical principles, encompassing a profound respect for the consent choices of content creators and a rigorous dedication to the exclusion of inappropriate content, notably material of a pornographic nature. Leveraging this robust foundation, ConvBench introduces a suite of new annotations, each meticulously crafted and verified by human oversight. These include novel extended multi-turn instructions, refined instruction-conditioned captions, and human-verified reference answers. The benchmark is meticulously curated to eliminate any harmful content, ensuring alignment with ethical guidelines and fostering a benchmark that is not only effective but also ethically sound.

## Appendix M Key Statistics of ConvBench

The time for conducting \(\mathrm{ConvBenchEval}(\cdot)\) is about 4.5 hours. In Table 10, we present a detailed account of resource consumption for a selection of models. The inference times exhibit variability across different models. For instance, the compact LLaVA-v1.5-7B model, with its smaller architecture, completes the evaluation in a mere 16 minutes utilizing 8 GPUs. In contrast, the more substantial InternVL-Chat-V1.2 model demands a longer timespan of 41 minutes and approximately 80GB of memory to accomplish the same task. We use the open-source codebase (OpenCompass) which is equipped with support for multi-GPU distributed inference, which significantly expedites the inference process. This capability not only optimizes computational efficiency but also enhances the scalability of our framework, enabling researchers and practitioners to leverage the power of multiple GPUs to perform complex inferences more swiftly and effectively.

\begin{table}
\begin{tabular}{l l l l} \hline \hline Model & Resources & Times & Memory Utilization Per GPU \\ \hline LLaVA-v1.5-7B & 1 x A100-80GB & 60min & 15890MiB \\ LLaVA-v1.5-13B & 1 x A100-80GB & 84min & 26717MiB \\ LLaVA-v1.5-7B & 8 x A100-80GB & 16min & 15890MiB \\ LLaVA-v1.5-13B & 8 x A100-80GB & 24min & 26708MiB \\ QwenVL-Chat & 8 x A100-80GB & 25min & 21122MiB \\ InternVL-Chat-V1.2 & 8 x A100-80GB & 41min & 78990MiB \\ \hline \hline \end{tabular}
\end{table}
Table 10: Resource consumption of some models evaluated on ConvBench.

\begin{table}
\begin{tabular}{l l} \hline \hline Statistics & Resources \\ \hline Avg. \# Turns per Dialogue & 3.00 \\ Total \# Dialogues & 577.00 \\ Total \# Turns & 1731.00 \\ Avg. \# Words in Prompt & 469.30 \\ Max. \# Words in Prompt & 495.00 \\ Avg. \# Words in Instruction & 83.88 \\ Max. \# Words in Instruction & 930.00 \\ Avg. \# Words in Instruction-Conditioned Caption & 1064.11 \\ Max. \# Words in Instruction-Conditioned Caption & 4704.00 \\ Avg. \# Words in Human-Verified Reference Answer & 1044.55 \\ Max. \# Words in Human-Verified Reference Answer & 4704.00 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Key statistics of ConvBench.

Case Study

Figure 12: This example precisely interprets the text on the door without impairing further reasoning. In assessing creative performance, it is determined that the response is deficient in creativity. The performances across the three turns are considered to evaluate the multi-turn conversations overall performance.

Figure 13: The accurate outcomes from the third turn include a push power of \(75\)W, a mechanical power efficiency for the wheelchair of \(9.33\%\), and a frictional force exerted by the wheelchair on the inclined surface of \(136\)N. This example highlights the ongoing challenges faced by LVLMs in scientific domains.

[MISSING_PAGE_EMPTY:28]

[MISSING_PAGE_EMPTY:29]

Figure 16: This example demonstrates that the evaluation process encompasses not just the assessment of individual turns, but also the overall conversation. Specifically, it examines whether the LVLM precisely chooses the second example from the previous responses when addressing the instructions in the third turn.

Prompt Templates

Figure 17: The prompt used for evaluating perception turn in a pairwise grading method, accompanied by a sample completion from ChatGPT, is provided. The response conversations are anonymously presented to ChatGPT, where Assistant A is identified as a human, and Assistant B is recognized as GPT4V.

[MISSING_PAGE_FAIL:32]

[MISSING_PAGE_FAIL:33]

[MISSING_PAGE_FAIL:34]

[MISSING_PAGE_EMPTY:35]

[MISSING_PAGE_FAIL:36]

[MISSING_PAGE_EMPTY:37]

[MISSING_PAGE_FAIL:38]

[MISSING_PAGE_EMPTY:39]

system prompt (human authored)

You are FashionHavenetGratGrat an expert language model at extracting multiple-choice answers from written out for longer engiderations. You will be given several sentences describing a thought process which should eventually contain a final answer 1 or 2 or 3 or 4 or 5 or 6 or 7 or 8 or 9 or 10. Your job is to extract the final answer that the explanation arrives at.

Some things to remember.

- Keep your answers short: only output "Final Rating X" when X is 1 or 2 or 3 or 4 or 5 or 6 or 7 or 8 or 9 or 10
- If the explanation doesn't mention a final choice explicitly you can output "Unknown"

user (human authored)

Step-by-step reasoning:

- First 1 consider the interaction, which asks if anything else in the image has the same color as the tiny sphere.

- If there is, look at the image and image content and note that there are several colored shapes in the image, including the tiny sphere.

- From the high quality reference, I use the image data every sphere has the same color as the tiny sphere.

- Looking at the Response, I's show there is no further profile in the image that has the same color as the tiny sphere. The high quality reference contradicts this response, stating that the large write grey sphere shows the same color with the tiny sphere.

- Therefore, Response is not accurate or correct.

- Overall, based on accuracy in addressing the instruction and matching the high quality inference, the Response is incorrect. Therefore, the response is noted in 4 assistant (human authored)

Final Rating 4

user (human authored)

Let's evaluate the the responses "step-by-step".

- Comercly identified the bicycle with the wooden crate
- incorrectly estimated the number of trips (2 trips instead of 4)
- Methods a large car, which is not in the image context provided
- Recognises the challenge of fitting model and its terms
- Provides more detailed explanation but accuracy is an issue

- Com ComiEdering accuracy, specificity, fluency, and relevance, let's make a judgement.

- Composed with the reference, the response is good in terms of fluency and relevance while is bad in terms of accuracy and specificity. Since accuracy is crucial in this case, and given the high-quality reference has the number of trips (6), we partition correctness.

- Overall 1 note the response at 2 assistant (human authored)

Final Rating 2

user (from human authored)

Upon analyzing the image description and the structure, it is clear that the answers to align for the unified message of a current received by the creation of five-turn pairs in the image. The high-quality reference against the constituent of "hugging very standard" is actually automatic and that the kernel joins it next a sign of talent. Looking at the response does directly address the lastness and applies that the pairs in the image to learn. Therefore, the response is not a significant (human authored)

Final Rating 6

Figure 26: The prompt used for extracting answers from ill-formatted ChatGPT responses in a direct grading method.

DTABLE]

## Appendix Q Datasheet

### Motivation

#### For what purpose was the dataset created?

Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.

The dataset was created to enable research on multi-turn visual conversation evaluation for large vision-language models (LVLMs)-i.e., given an image and three-turn instructions, evaluate whether the responses from LVLms reach human preferences. The benchmark organization can also provide hierarchical ablation evaluation for error analysis. ConvBench progressively examines the LVLMs' perception, reasoning, and creativity capabilities in each conversation and can decouple these capabilities in evaluations and thus perform reliable error attribution.

Who created this dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?

The dataset was created by Shuo Liu, Wenqi Shao, and Kaipeng Zhang at Shanghai AI Lab.

#### Who funded the creation of the dataset?

If there is an associated grant, please provide the name of the grantor and the grant name and number.

This dataset is partially supported by the National Key R&D Program of China No.2022ZD0161000 and the General Research Fund of Hong Kong No.17200622 and 17209324.

#### Any other comments?

None.

What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?

Are there multiple types of instances (e.g., movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description.

An instance in ConvBench is an image, three sequential instructions, three task categories, an instruction-conditioned caption, three-turn human-verified reference answers, and focus demands for the creation instruction. An example instance is shown in Figure 27.

How many instances are there in total (of each type, if appropriate)?

Figure 27: An example of ConvBench.

There are 577 samples in total in ConvBench. Each sample has an image, three sequential instructions, three task categories, an instruction-conditioned caption, three-turn human-verified reference answers, and focus demands for the creation instruction. An example instance is shown in Figure 27.

**Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?** If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable).

VisIT-Bench [7] is a single-turn visual question answering (VQA) dataset collected from users' "wish list". ConvBench uses its images and extends its questions in our multi-turn hierarchical structure. Therefore, these questions are representative of real-world use. No tests were run to determine representativeness.

**What data does each instance consist of? "Raw" data (e.g., unprocessed text or images) or features?** In either case, please provide a description.

Each instance consists of an image, three sequential instructions, three task categories, an instruction-conditioned caption, three-turn human-verified reference answers, and focus demands for the creation instruction. Each caption and each answer is verified by human annotators to abolish bias and errors.

**Is there a label or target associated with each instance?** If so, please provide a description.

The labels are human-verified reference answers for instructions derived from human annotators, as described above.

**Is any information missing from individual instances?** If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable). This does not include intentionally removed information, but might include, e.g., redacted text.

Everything is included. No data is missing.

**Are relationships between individual instances made explicit (e.g., users' movie ratings, social network links)?** If so, please describe how these relationships are made explicit.

There is no explicit relationship between individual instances.

**Are there recommended data splits (e.g., training, development/validation, testing)?** If so, please provide a description of these splits, explaining the rationale behind them.

The instances all belong to the testing split.

**Are there any errors, sources of noise, or redundancies in the dataset?** If so, please provide a description.

No, we have carefully checked and annotated to avoid any errors. However, due to the large volume of data, there may be a very small number of errors.

**Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)?** If it links to or relies on external resources, a) are there guarantees that they will exist, and remain constant, over time; b) are there official archival versions of the complete dataset (i.e., including the external resources as they existed at the time the dataset was created); c) are there any restrictions (e.g., licenses, fees) associated with any of the external resources that might apply to a future user? Please provide descriptions of all external resources and any restrictions associated with them, as well as links or other access points, as appropriate.

The benchmark is built upon the VisIT-Bench [7] the seed sample. There are no additional external resources required. The dataset is entirely self-contained.

**Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals non-public communications)?** If so, please provide a description.

No, the dataset does not contain data that might be considered confidential.

**Does the dataset contain data that, if viewed directly, might be offensive, resulting, threatening, or might otherwise cause anxiety?** If so, please describe why.

No, the dataset does not contain data which might be offensive, insulting, threatening, or might otherwise cause anxiety.

**Does the dataset relate to people?** If not, you may skip the remaining questions in this section.

Yes, part of the image of the dataset relates to people, however, they are not the primary emphasis of the dataset.

**Does the dataset identify any subpopulations (e.g., by age, gender)?** If so, please describe how these subpopulations are identified and provide a description of their respective distributions within the dataset.

No, the dataset does not identify any subpopulations.

**Is it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset?** If so, please describe how.

No, it is not possible to identify individuals. There is no personal information included in the dataset.

**Does the dataset contain data that might be considered sensitive in any way (e.g., data that reveals racial or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)?** If so, please provide a description.

No, the dataset does not contain data that might be considered sensitive in any way.

**Any other comments?**

None.

**Collection Process**

**How was the data associated with each instance acquired?** Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how.

As shown in Figure 3, the image and seed instruction is from the VisIT-Bench [7]. Our benchmark is constructed based on the VisIT-Bench [7].

We extend the seed instruction to three-turn instructions.

The annotations by human are applied for acquiring the instruction-conditioned caption, human-verified reference answers, focus demands and so on.

**What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)?** How were these mechanisms or procedures validated?

As shown in Figure 3, To construct the multi-turn conversation and establish the capability hierarchy, each instance in ConvBench is composed of an input image, three hierarchical instructions, threehuman-verified references, and an instruction-conditioned caption verified by humans (see the Figure 3). Specifically, the annotators start by extending an instruction from the VisIT-Bench [7] into three hierarchical instructions in a multi-turn manner. The above annotation process is generalized that according to an instruction in VisIT-Bench [7], we curate the perception instruction for the first turn, followed by the reasoning and creativity instructions, which are generated in response to the instructions and answers at preceding turns. These instructions are annotated by humans reflecting the real-world needs of human beings. Similar to VisIT-Bench [7], we then annotate the instruction-conditioned captions and gather human-validated reference answers. In these two processes, GPT4V plays the role of an auxiliary tool. The annotators have eliminated errors and biases by providing human-preference responses for all the requests.

**If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?**

The dataset is not a sample from a larger set. We use all the provided seed examples from VisIT-Bench [7].

**Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?**

The authors were involved in the data collection process for no payment. They are working in the Shanghai AI Lab.

**Over what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)?** If not, please describe the timeframe in which the data associated with the instances was created.

The data set is annotated from September 2023 to February 2024.

**Were any ethical review processes conducted (e.g., by an institutional review board)?** If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation.

No, there were not any ethical review processes conducted. Our benchmark is meticulously crafted upon the foundation of VisIT-Bench [7], a dataset that has been subjected to rigorous ethical scrutiny and content filtering. This meticulous process ensures that our benchmark not only meets but exceeds the current ethical standards, reflecting our commitment to upholding the highest levels of integrity and responsibility in the development and application of our technology.

**Does the dataset relate to people?** If not, you may skip the remaining questions in this section.

Yes, part of the image of the dataset relates to people, however, they are not the primary emphasis of the dataset.

**Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)?**

We construct the data based on the VisIT-Bench [7], which is obtained via https://github.com/mlfoundations/VisIT-Bench/.

**Were the individuals in question notified about the data collection?** If so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself.

No. The data was from the public dataset, and the authors presumably knew that their image would be public.

**Did the individuals in question consent to the collection and use of their data?** If so, please describe (or show with screenshots or other information) how consent was requestedand provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consented.

No. The data was from the public dataset, and the authors presumably knew that their image would be public.

**If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses?** If so, please provide a description, as well as a link or other access point to the mechanism (if appropriate).

Users can contact us to remove any annotation in our proposed benchmark.

**Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted?** If so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation.

No.

**Any other comments?**

None.

**Preprocessing/cleaning/labeling**

**Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)?** If so, please provide a description. If not, you may skip the remainder of the questions in this section.

As shown in Figure 3, the image and the instructions are provided for the annotators to generate a caption. We first prompt GPT4V with "Describe this image in detail." We then polish the responses by humans according to the instructions to obtain the final instruction-conditioned caption. For each sample, we feed GPT4V with the instruction-conditioned caption, the image, multi-turn instructions, and our well-designed prompt in a multi-turn conversation fashion to generate each instruction's response. We meticulously refine these responses as reference answers, removing their biases and enhancing their quality and relevance. The preprocessing and cleaning of annotations can maintain the quality of the proposed benchmark.

**Was the "raw" data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)?** If so, please provide a link or other access point to the "raw" data.

The "raw" data is the VisIT-Bench, which can be downloaded via https://github.com/mlfoundations/VisIT-Bench/.

**Is the software used to preprocess/clean/label the instances available?** If so, please provide a link or other access point.

Yes, we use GPT4V to help with annotation (https://chat.openai.com/?model=gpt-4).

**Any other comments?**

None.

**Uses**

**Has the dataset been used for any tasks already?** If so, please provide a description.

The dataset has been only used for multi-turn visual conversation evaluation task and hierarchical ablation evaluation task in this paper.

**Is there a repository that links to any or all papers or systems that use the dataset?** If so, please provide a link or other access point.

No, there is only the original paper using this dataset now.

**What (other) tasks could the dataset be used for?**

The dataset could be used for anything related to researching multi-turn visual conversation. But, our dataset should only be used for non-commercial academic research

**Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?** For example, is there anything that a future user might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other undesirable harms (e.g., financial harms, legal risks) If so, please provide a description. Is there anything a future user could do to mitigate these undesirable harms?

There is minimal risk for harm.

**Are there tasks for which the dataset should not be used?** If so, please provide a description.

This data is collected for LVLMs to evaluate the ability of multi-turn visual conversation and help analyze the error attribution. Our dataset should only be used for non-commercial academic researc

**Any other comments?**

None.

**Distribution**

**Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created?** If so, please provide a description.

Yes, the benchmark will be open-sourced.**

**How will the dataset will be distributed (e.g., tarball on website, API, GitHub) Does the dataset have a digital object identifier (DOI)?**

The data will be available through GitHub.

**When will the dataset be distributed?**

Code and benchmark are released at https://github.com/shirlyliu64/ConvBench

**Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)?** If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions.

The ConvBench dataset is licensed under the Creative Commons Attribution 4.0 International License (CC BY 4.0). For images, the original licensing terms are respected and remain applicable (VisITBench [7]).

**Have any third parties imposed IP-based or other restrictions on the data associated with the instances?** If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions.

No, the benchmark owns the metadata and release as CC-BY-4.0 and we do not own the copyright of the images.

**Do any export controls or other regulatory restrictions apply to the dataset or to individual instances?** If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation.

No.

**Any other comments?**

None.

Maintenance

### Who will be supporting/hosting/maintaining the dataset?

OpenGVLab of Shanghai AI Laboratory will maintain the samples distributed. Huggingface will support hosting of the metadata.

How can the owner/curator/manager of the dataset be contacted (e.g., email address)?

The email addresses can be contacted are liushuo@pjlab.org.cn, zhangkaipeng@pjlab.org.cn and shaowenqi@pjlab.org.cn.

Is there an erratum? If so, please provide a link or other access point.

There is not an explicit erratum. We plan to maintain it through GitHub issues.

Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to users (e.g., mailing list, GitHub)?

No. However, specific samples can be removed on request.

If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were individuals in question told that their data would be retained for a fixed period of time and then deleted)? If so, please describe these limits and explain how they will be enforced.

People may contact us to add specific samples to a blacklist.

Will older versions of the dataset continue to be supported/hosted/maintained? If so, please describe how. If not, please describe how its obsolescence will be communicated to users.

We will only support and maintain the latest version at all times and a new version release will automatically deprecate its previous version.

If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? If so, please provide a description. Will these contributions be validated/verified? If so, please describe how. If not, why not? Is there a process for communicating/distributing these contributions to other users? If so, please provide a description.

We warmly embrace contributions to ConvBench and will keep the community informed about any dataset expansions through our GitHub repository. However, it is imperative that contributors provide evidence of the high quality and non-harmful nature of the proposed data annotations. Submissions that do not meet these stringent criteria will not be integrated into our benchmark. Our commitment to maintaining a standard of excellence and safety in our dataset is unwaering, ensuring that ConvBench remains a reliable and ethical resource for the research community.

Any other comments?None.

## Appendix R License and Intended Use

The ConvBench dataset is licensed under the Creative Commons Attribution 4.0 International License (CC BY 4.0). For images, the original licensing terms are respected and remain applicable (VisITBench [7]). The images and associated annotations remain readily available for direct download (https://github.com/shirlyliu64/ConvBench). The metadata is also provided at https://github.com/shirlyliu64/ConvBench/metadata.json.

We release the benchmark under the CC-BY license and Terms of Use, requiring disclosure when used for model evaluation. This license supplements, but does not replace, the original licenses of source materials; compliance with these and any applicable rights of data subjects is necessary. This statement clarifies the responsibilities and liabilities associated with using this benchmark. While we've made every effort to ensure the samples' accuracy and legality, we cannot guarantee their absolute completeness or correctness. We assume no liability for any rights violations, whether legal or otherwise, that may occur through the use of this benchmark, including but not limited to copyright infringement, privacy violations, or misuse of sensitive information. By accessing, downloading, or using this benchmark, you implicitly accept this statement and agree to adhere to the terms and conditions of the CC-BY license. If you do not agree with these terms or the CC-BY license, you are not authorized to use this benchmark. The benchmark will be hosted and maintained on GitHub and the Hugging Face Hub platform.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? [Yes] See Section 6. 3. Did you discuss any potential negative societal impacts of your work? [Yes] See Section 6. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] Code and benchmark are released at https://github.com/shirlyliu64/ConvBench 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [N/A] This paper does not train any model. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [No] Most experiments have stable results with little variance. 4. Did you include the total amount of computing and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix about the resources used.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] We mentioned these libraries we used. 2. Did you mention the license of the assets? [Yes] We only used open-source libraries. 3. Did you include any new assets either in the supplemental material or as a URL? [N/A] 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] See Appendix about ethical discussion. 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] See Appendix about ethical discussion.
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? The participants are the authors of this paper, who know the details of this project. 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]