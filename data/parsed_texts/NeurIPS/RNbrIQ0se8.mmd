# Ada-MSHyper: Adaptive Multi-Scale Hypergraph Transformer for Time Series Forecasting

Zongjiang Shang, Ling Chen, Binqing Wu, Dongliang Cui

State Key Laboratory of Blockchain and Data Security

College of Computer Science and Technology

Zhejiang University

{zongjiangshang, lingchen, binqingwu, runnercd}@cs.zju.edu.cn

Corresponding author: Ling Chen.

###### Abstract

Although transformer-based methods have achieved great success in multi-scale temporal pattern interaction modeling, two key challenges limit their further development: (1) Individual time points contain less semantic information, and leveraging attention to model pair-wise interactions may cause the information utilization bottleneck. (2) Multiple inherent temporal variations (e.g., rising, falling, and fluctuating) entangled in temporal patterns. To this end, we propose **A**d**aptive **M**ulti-**S**cale **H**ypergraph Transformer (Ada-MSHyper) for time series forecasting. Specifically, an adaptive hypergraph learning module is designed to provide foundations for modeling group-wise interactions, then a multi-scale interaction module is introduced to promote more comprehensive pattern interactions at different scales. In addition, a node and hyperedge constraint mechanism is introduced to cluster nodes with similar semantic information and differentiate the temporal variations within each scales. Extensive experiments on 11 real-world datasets demonstrate that Ada-MSHyper achieves state-of-the-art performance, reducing prediction errors by an average of 4.56%, 10.38%, and 4.97% in MSE for long-range, short-range, and ultra-long-range time series forecasting, respectively. Code is available at https://github.com/shangzongjiang/Ada-MSHyper.

## 1 Introduction

Time series forecasting has demonstrated its wide applications across many fields [30, 37], e.g., energy consumption planning, traffic and economics prediction, and disease propagation forecasting. In these real-world applications, the observed time series often demonstrate complex and diverse temporal patterns at different scales[6, 9, 28]. For example, due to periodic human activities, traffic occupation and electricity consumption show clear daily patterns (e.g., afternoon or evening), weekly patterns (e.g., weekday or weekend), and even monthly patterns (e.g., summer or winter).

Recently, deep models have achieved great success in time series forecasting. To tackle intricate temporal patterns and their interactions at different scales, numerous foundational backbones have emerged, including recurrent neural networks (RNNs) [7, 9, 10], convolutional neural networks (CNNs) [1, 25], graph neural networks (GNNs) [4, 8], and transformers [20, 39]. Particularly, due to the capabilities of depicting pair-wise interactions and extracting multi-scale representations in sequences, transformers are widely used in time series forecasting. However, some recent studies show that even simple multi-scale MLP [11, 35] or naive series decomposition methods [3, 15] can outperform transformer-based methods on various benchmarks. We argue the challenges that limit the effectiveness of transformers in time series forecasting are as follows.

The first one is _semantic information sparsity_. Different from natural language processing (NLP) and computer vision (CV), individual time point in time series contains less semantic information [5; 29]. Compared to pair-wise interactions, group-wise interactions among time points with similar semantic information (e.g., neighboring time points or distant but strongly correlated time points) are more emphasized in time series forecasting. To address the problem of semantic information sparsity, some recent works employ patch-based approaches [12; 23] and hypergraph structures [26] to enhance locality and capture group-wise interactions. However, simple partitioning of patches and predefined hypergraph structures may introduce a large amount of noise and be hard to discover implicit interactions.

The second one is _temporal variations entanglement_. Due to the complexity and non-stationary of real-world time series, the temporal patterns of observed time series often contain a large number of inherent variations (e.g., rising, falling, and fluctuating), which may mix and overlap with each other. Especially when there are distinct temporal patterns at different scales, multiple temporal variations are deeply entangled, bringing extreme challenges for time series forecasting. To tackle the problem of temporal variations entanglement, recent studies employ series decomposition [30; 39] and multi-periodicity analysis [27; 29] to differentiate temporal variations at different scales. However, existing methods lack the ability to differentiate temporal variations within each scale, making temporal variations within each scale overlap and become entangled with redundant information.

Motivated by the above, we propose Ada-MSHper, an **A**d**aptive **M**ulti-**S**cale **H**yper**graph Transformer for time series forecasting. Specifically, Ada-MSHper map the input sequence into multi-scale feature representations, then by treating the multi-scale feature representations as nodes, an adaptive multi-scale hypergraph structure is introduced to discover the abundant and implicit group-wise node interactions at different scales. To the best of our knowledge, Ada-MSHper is the first work that incorporates adaptive hypergraph modeling into time series forecasting. The main contributions are summarized as follows:

* We design an adaptive hypergraph learning (AHL) module to model the abundant and implicit group-wise node interactions at different scales and a multi-scale interaction module to perform hypergraph convolution attention, which empower transformers with the ability to model group-wise pattern interactions at different scales.
* We introduce a node and hyperedge constraint (NHC) mechanism during hypergraph learning phase, which utilizes semantic similarity to cluster nodes with similar semantic information and leverages distance similarity to differentiate the temporal variations within each scales.
* We conduct extensive experiments on 11 real-world datasets. The experimental results demonstrate that Ada-MSHper achieves state-of-the-art (SOTA) performance, reducing error by an average of 4.56%, 10.38%, and 4.97% in MSE for long-range, short-range, and ultra-long-range time series forecasting, respectively, compared to the best baseline.

## 2 Related Work

**Deep Models for Time Series Forecasting.** Deep models have shown promising results in time series forecasting. To model temporal patterns at different scales and their interactions, a large number of specially designed backbones have emerged. TAMS-RNNs [10] captures periodic temporal dependencies through multi-scale recurrent structures with different update frequencies. TimesNet [29] extends the 1D time series into the 2D space, and models multi-scale temporal pattern interactions through 2D convolution inception blocks. Benefiting from the attention mechanism, transformers have gone beyond contemporaneous RNN- and CNN-based methods and achieved promising results in time series forecasting. FEDformer [39] combines mixture of expert and frequency attention to capture multi-scale temporal dependencies. Pyraformer [20] extends the input sequence into multi-scale representations and models the interactions between nodes at different scales through pyramidal attention. Nevertheless, with the rapid emergence of linear forecasters [22; 35; 11], the effectiveness of transformers in this direction is being questioned.

Recently, some methods have attempted to fully utilize transformers and paid attention to the inherent properties of time series. Some of these methods are dedicated to addressing the problem of semantic information sparsity in time series forecasting. PatchTST [23] segments the input sequence into subseries-level patches to enhance locality and capture group-wise interactions. MSHyper [26]models group-wise interactions through multi-scale hypergraph structures and introduces \(k\)-hop connections to aggregate information from different range of neighbors. However, constrained by the fixed windows and predefined rules, these methods cannot discover implicit interactions. Others emphasize on addressing the problem of temporal variations entanglement in time series forecasting. FilM [38] differentiates temporal variations at different scales by decomposing the input series into different period lengths. iTransformer [21] combines inverted structures with transformer to learn entangled global temporal variations. However, these methods cannot differentiate temporal variations within each scale, making temporal variations within each scale overlap and become entangled with redundant information.

**Hypergraph Neural Networks.** As a generalized form of GNNs, hypergraph neural networks (HGNNs) have been applied in different fields, e.g., video object segmentation [17], stock selection [24], multi-agent trajectory prediction [31], and time series forecasting [26]. HyperGCN [32] is the first work that incorporates convolution operation into hypergraphs, which demonstrates the superiority of HGNNs over ordinary GNNs in capture group-wise interactions. Recent studies [2; 33] show that HGNNs are promising to model group-wise pattern interactions. LBSN2Vec++ [34] uses hypergraphs for location-based social networks, which leverages heterogeneous hypergraph embeddings to capture mobility and social relationship pattern interactions. GroupNet [31] utilizes multi-scale hypergraph for trajectory prediction, which combines relational reasoning with hypergraph structures to capture group-wise pattern interactions among multiple agents.

Considering the capability of HGNNs in modeling group-wise interactions, in this work, an adaptive multi-scale hypergraph transformer framework is proposed to model the group-wise pattern interactions at different scales. Specifically, an AHL model is designed to model the abundant and implicit group-wise node interactions. In addition, a NHC mechanism is introduced to cluster nodes with similar semantic information and differentiate temporal variations within each scale, respectively.

## 3 Preliminaries

**Hypergraph.** A hypergraph is defined as \(\mathcal{G}=\left\{\mathcal{V},\mathcal{E}\right\}\), where \(\mathcal{E}=\left\{e_{1},\ldots,e_{m},\ldots,e_{M}\right\}\) is the hyperedge set and \(\mathcal{V}=\left\{v_{1},\ldots,v_{n},\ldots,v_{N}\right\}\) is the node set. Each hyperedge represents group-wise interactions by connecting a set of nodes \(\left\{v_{1},v_{2},\ldots,v_{n}\right\}\subseteq\mathcal{V}\). The topology of hypergraph can be represented as an incidence matrix \(\mathbf{H}\in\mathbb{R}^{N\times M}\), with entries \(\mathbf{H}_{nm}\) defined as follows:

\[\mathbf{H}_{nm}=\left\{\begin{array}{ll}1,&\quad v_{n}\in e_{m}\\ 0,&\quad v_{n}\notin e_{m}\end{array}\right.\] (1)

The degree of the \(n\)th node is defined as \(d(v_{n})=\sum_{m=1}^{M}\mathbf{H}_{nm}\) and the degree of the \(m\)th hyperedge is defined as \(d(v_{m})=\sum_{n=1}^{N}\mathbf{H}_{nm}\). Further, the node degrees and hyperedge degrees are sorted in diagonal matrices \(\mathbf{D}_{\text{v}}\in\mathbb{R}^{N\times N}\) and \(\mathbf{D}_{\text{c}}\in\mathbb{R}^{M\times M}\), respectively.

**Problem Formulation.** Given the input sequence \(\mathbf{X}_{1:T}^{\mathbf{1}}=\left\{\boldsymbol{x}_{t}\mid\boldsymbol{x}_{t} \in\mathbb{R}^{D},t\in[1,T]\right\}\), where \(\boldsymbol{x}_{t}\) represents the values at time step \(t\), \(T\) is the input length, and \(D\) is the feature dimension. The task of time series forecasting is to predict the future \(H\) steps, which can be formulated as follows:

\[\widehat{\mathbf{X}}_{T+1:T+H}^{\text{O}}=\mathcal{F}\left(\mathbf{X}_{1:T}^{ \mathbf{1}};\theta\right)\in\mathbb{R}^{H\times D},\] (2)

where \(\widehat{\mathbf{X}}_{T+1:T+H}^{\text{O}}\) denotes the forecasting results, \(\mathcal{F}\) denotes the mapping function, and \(\theta\) denotes the learnable parameters of \(\mathcal{F}\). The description of the key notations are given in Appendix A.

## 4 Ada-MSHyper

As previously mentioned, the core of Ada-MSHyper is to promote more comprehensive pattern interactions at different scales. To accomplish this goal, we first map the input sequence into sub-sequences at different scales through the multi-scale feature extraction (MFE) module. Then, by treating multi-scale feature representations as nodes, the AHL module is introduced to model the abundant and implicit group-wise node interactions at different scales. Finally, the multi-scale interaction module is introduced to model group-wise pattern interactions at different scales. Notably, during the hypergraph learning phase, an NHC mechanism is introduced to cluster nodes with similar semantic information and differentiate temporal variations within each scale. The overall framework of Ada-MSHyper is shown in Figure 1.

### Multi-Scale Feature Extraction (MFE) Module

The MFE module is designed to get the feature representations at different scales. As shown in Figure 1(a), suppose \(\mathbf{X}^{s}=\{\bm{x}_{t}^{s}|\bm{x}_{t}^{s}\in\mathbb{R}^{D},t\in[1,N^{s}]\}\) denotes the sub-sequence at scale \(s\), where \(s=1,...,S\) denotes the scale index and \(S\) is the total number of scales. \(N^{s}=\left\lfloor\frac{N^{s-1}}{l^{s-1}}\right\rfloor\) is the number of nodes at scale \(s\) and \(l^{s-1}\) denotes the size of the aggregation window at scale \(s-1\). \(\mathbf{X}^{1}=\mathbf{X}^{1}_{1:T}\) is the raw input sequence and the aggregation process can be formulated as follows:

\[\mathbf{X}^{s}=Agg(\mathbf{X}^{s-1};\theta^{s-1})\in\mathbb{R}^{N^{s}\times D },s\geq 2,\] (3)

where \(Agg\) is the aggregation function, e.g., 1D convolution or average pooling, and \(\theta^{s-1}\) denotes the learnable parameters of the aggregation function at scale \(s-1\).

### Adaptive Hypergraph Learning (AHL) Module

The AHL module automatically generates incidence matrices to model implicit group-wise node interactions at different scales. As shown in Figure 1(b), we first initialize two kinds of parameters, i.e., node embeddings \(\bm{E}_{\mathrm{node}}^{s}\in\mathbb{R}^{N^{s}\times D}\) and hyperedge embeddings \(\bm{E}_{\mathrm{hyper}}^{s}\in\mathbb{R}^{M^{s}\times D}\) at scale \(s\), where \(M^{s}\) is hyperparameters, representing the number of hyperedges at scale \(s\). Then, we can obtain the scale-specific incidence matrix \(\mathbf{H}^{s}\) by similarity calculation, which can be formulated as follows:

\[\mathbf{H}^{s}=SoftMax(ReLU(\bm{E}_{\mathrm{node}}^{s}(\bm{E}_{\mathrm{hyper }}^{s})^{T})),\] (4)

where the \(ReLU\) activation function is used to eliminate weak connections and the \(SoftMax\) function is applied to normalize the value of \(\mathbf{H}^{s}\). In order to reduce subsequent computational costs and noise interference, the following strategy is designed to sparsify the incidence matrix:

\[\mathbf{H}_{nm}^{s}=\begin{\{matrix}\mathbf{H}_{nm}^{s},&\mathbf{H}_{nm}^{s} \in TopK(\mathbf{H}_{n^{s}}^{s},\eta)\\ 0,&\mathbf{H}_{nm}^{s}\notin TopK(\mathbf{H}_{n^{s}}^{s},\eta)\end{matrix}\] (5)

where \(\eta\) is the threshold of \(TopK\) function and denotes the max number of neighboring hyperedges connected to a node. The final values of \(\mathbf{H}_{nm}^{s}\) can be obtained as follows:

\[\mathbf{H}_{nm}^{s}=\begin{\{matrix}1,&\mathbf{H}_{nm}^{s}>\beta\\ 0,&\mathbf{H}_{nm}^{s}<\beta\end{matrix}\] (6)

where \(\beta\) denotes the threshold, and the final scale-specific incidence matrices can be represented as \(\{\mathbf{H}^{1},\cdots,\mathbf{H}^{s},\cdots,\mathbf{H}^{S}\}\). Compared to previous methods, our adaptive hypergraph learning is novel from two aspects. Firstly, our methods can capture group-wise interactions at different scales, while most previous methods [5; 21] can only model pair-wise interactions at a single scale. Secondly, our methods can model abundant and implicit interactions, while many previous methods [23; 26] depend on fixed windows and predefined rules.

Figure 1: The framework of Ada-MSHyper.

### Node and Hyperedge Constraint (NHC) Mechanism

Although the AHL module can help discover implicit group-wise node interactions at different scales, we argue that the pure data-driven approach faces two limitations, i.e., unable to efficiently cluster nodes with similar semantic information and differentiate temporal variations within each scale. To tackle the above dilemmas, we introduce the NHC mechanism during hypergraph learning phase.

Given the multi-scale feature representations \(\{\mathbf{X}^{1},\cdots,\mathbf{X}^{s},\cdots,\mathbf{X}^{S}\}\) generated from the MFE module, and the scale-specific incidence matrices \(\{\mathbf{H}^{1},\cdots,\mathbf{H}^{s},\cdots,\mathbf{H}^{S}\}\) generated from the AHL module, we first get the initialized node feature representations \(\boldsymbol{\mathcal{V}}^{s}=f(\mathbf{X}^{s})\) at scale \(s\), where \(f\) can be implemented by the multi-layer perceptron (MLP). As shown in Figure 2(a), the initialized hyper-edge feature representations can be obtained by the aggregation operation based on \(\mathbf{H}^{s}\). Specifically, for the \(i\)th hyperedge \(e_{i}^{s}\) at scale \(s\), its feature representations \(\boldsymbol{e}_{i}^{s}\) can be formulated as follows:

\[\boldsymbol{e}_{i}^{s}=avg(\sum\nolimits_{v_{j}^{s}\in\mathcal{N}(e_{i}^{s})} \boldsymbol{v}_{j}^{s})\in\mathbb{R}^{D},\] (7)

where \(avg\) is the average operation, \(\mathcal{N}(e_{i})\) represents the neighboring nodes connected by \(e_{i}^{s}\) at scale \(s\), and \(\boldsymbol{v}_{j}^{s}\in\boldsymbol{\mathcal{V}}^{s}\) is the \(j\)th node feature representations at scale \(s\). The initialized hyperedge feature representations at different scales can be represented as \(\{\boldsymbol{\mathcal{E}}^{1},\cdots,\boldsymbol{\mathcal{E}}^{s},\cdots, \boldsymbol{\mathcal{E}}^{S}\}\). Then, based on semantic similarity and distance similarity, we introduce node constraint to cluster nodes with similar semantic information and leverage hyperedge constraint to differentiate the temporal variations of temporal patterns.

**Node Constraint.** In the data-driven hypergraph, we observe that some nodes connected by the same hyperedge contain distinct semantic information. To cluster nodes with similar semantic information and reduce irrelevant noise interference, we introduce node constraint based on the semantic similarity between nodes and their corresponding hyperedges. As shown in Figure 2(b), for the \(j\)th node at scale \(s\), we first obtain its semantic similarity difference \(\widetilde{\boldsymbol{v}_{j}^{s}}\) with its corresponding hyperedges:

\[\widetilde{\boldsymbol{v}_{j}^{s}}=\{abs(\boldsymbol{v}_{j}^{s}-\boldsymbol{ e}_{i}^{s})|v_{j}^{s}\in\mathcal{N}(e_{i}^{s})\},\] (8)

where \(abs\) refers to the operation of calculating the absolute value. The node loss \(L_{node}^{s}\) at scale \(s\) based on node constraint can be formulated as follows:

\[L_{node}^{s}=\frac{1}{N^{s}}\sum_{i=1}^{N^{s}}\widetilde{\boldsymbol{v}_{j}^{ s}},\] (9)

where \(N^{s}\) is the number of nodes at scale \(s\). Empowered by the node constraint, our method can enjoy more advantageous group-wise semantic information than pure data-driven hypergraph. Further experimental results and visualization analysis in Section 5.3 and Appendix H demonstrate the effectiveness of the node constraint in clustering nodes with similar semantic information.

**Hyperedge Constraint.** Since time series is a collection of data points arranged in chronological order, some recent works [23; 26] show that connecting multiple nodes sequentially through patches or hyperedges can represent specific temporal variations. Therefore, to deal with the problem of temporal variations entanglement, we introduce hyperedge constraint based on distance similarity. As shown in Figure 2(c), we first compute the cosine similarity to reflect the correlation of any two hyperedge representations at scale \(s\), which can be formulated as follows:

\[\alpha_{i,j}=\frac{\boldsymbol{e}_{i}^{s}(\boldsymbol{e}_{j}^{s})^{T}}{\left\| \boldsymbol{e}_{i}^{s}\right\|_{2}\left\|\boldsymbol{e}_{j}^{s}\right\|_{2}},\] (10)

Figure 2: The node and hyperedge constraint mechanism.

where \(\alpha_{i,j}\) represents the correlation weight. \(\bm{e}_{i}^{s}\) and \(\bm{e}_{j}^{s}\) are the \(i\)th and \(j\)th hyperedge representation at scale \(s\), respectively. Then, we use Euclidean distance \(D_{i,j}\) to measure the differentiation magnitude between any two hyperedge representations, which can be formulated as follows:

\[D_{i,j}=\left\|\bm{e}_{i}^{s}-\bm{e}_{j}^{s}\right\|_{2}=\sqrt{ \sum\nolimits_{d=1}^{D}((\bm{e}_{i}^{s})^{d}-(\bm{e}_{j}^{s})^{d})^{2}},\] (11)

The hyperedge loss \(L_{hyper}^{s}\) at scale \(s\) based on the correlation weight and Euclidean distance can be formulated as follows:

\[L_{hyper}^{s}=\frac{1}{(M^{s})^{2}}\sum\nolimits_{i=1}^{M^{s}} \sum\nolimits_{j=1}^{M^{s}}\left(\alpha_{i,j}D_{i,j}+(1-\alpha_{i,j})max( \gamma-D_{i,j},0)\right),\] (12)

where \(\gamma>0\) denotes the threshold. Notably, when \(\alpha_{i,j}=1\), indicating that \(e_{i}^{s}\) and \(e_{k}^{s}\) are deemed similar, the hyperedge loss turns to \(L_{hyper}=\frac{1}{(M^{s})^{2}}\sum\nolimits_{i=1}^{M^{s}}\sum\nolimits_{j=1 }^{M^{s}}\alpha_{i,j}D_{i,j}\), where the loss will increase if \(D_{i,j}\) becomes large. Conversely, when \(\alpha_{i,j}=0\), meaning \(e_{i}\) and \(e_{k}\) are regarded as dissimilar, the hyperedge loss turns to \(L_{hyper}=\frac{1}{(M^{s})^{2}}\sum\nolimits_{i=1}^{M^{s}}\sum\nolimits_{j=1 }^{M^{s}}(1-\alpha_{i,j})max(s-D_{i,j},0)\), where the loss will increase if \(D_{i,j}\) falls below the threshold and turns smaller. Other cases lie between the above circumstances. We further provide the visualization results in Section 5.3 and appendix H to verify that our constraint loss can differentiate temporary variations of temporary patterns within each scale and promote forecasting performance. The final constraint loss \(L_{const}\) based on node constraint and hyperedge constraint can be formulated as follows:

\[L_{const}=\lambda\sum\nolimits_{s=1}^{S}L_{node}^{s}+(1-\lambda) \sum\nolimits_{s=1}^{S}L_{hyper}^{s},\] (13)

where \(\lambda\) denotes the hyperparameter controlling the balance between node loss and hyperedge loss.

### Multi-Scale Interaction Module

To promote more comprehensive pattern interactions at different scales, a direct way is to mix multi-scale node feature representations at different scales. However, we argue that intra-scale interactions and inter-scale interactions reflect different aspects of pattern interactions, where intra-scale interactions mainly depict detailed interactions between nodes with similar semantic information and inter-scale interactions highlight macroscopic variations interactions[9; 27]. Therefore, instead of directly mixing multi-scale pattern information as a whole, we introduce the multi-scale interaction module to perform inter-scale interactions and intra-scale interactions.

**Intra-Scale Interaction Module.** Due to the semantic information sparsity of time series, traditional pair-wise attention may may cause the information utilization bottleneck [5]. In contrast, some recent studies [23; 26] show that group-wise interactions can provide more informative insights in time series forecasting. To capture group-wise interactions among nodes with similar semantic information within each scale, we introduce hypergraph convolution attention within the intra-scale interaction module. Specifically, given \(\mathbf{H}^{s}\), we first use attention mechanism to capture the interaction strength of each node \(v_{i}^{s}\in\bm{\mathcal{V}}^{s}\) and its related hyperedges at scale \(s\), which can be formulated as follows:

\[\bm{\mathcal{H}}_{ij}^{s}=\frac{\exp(\sigma(f_{t}[\bm{v}_{i}^{s}, \bm{e}_{j}^{s}]))}{\sum\nolimits_{e_{k}^{s}\in\mathcal{N}(v_{i}^{s})}exp( \sigma(f_{t}[\bm{v}_{i}^{s},\bm{e}_{k}^{s}])},\] (14)

where \([.,.]\) denotes the concatenation operation of the \(i\)th node and its related hyperedges. \(f_{t}\) is a trainable MLP, and \(\mathcal{N}(v_{i}^{s})\) is the neighboring hyperedges connected to \(v_{i}^{s}\), which can be accessed using \(\mathbf{H}^{s}\). Then, considering the symmetric normalized hypergraph Laplacian convolution \(\Delta=\mathbf{D}_{v}^{-1/2}\mathbf{H}\mathbf{W}\mathbf{D}_{e}^{-1}\mathbf{H}^ {T}\mathbf{D}_{v}^{-1/2}\) used in HGNN [13], the multi-head hypergraph convolution attention can be formulated as follows:

\[\widetilde{\bm{\mathcal{V}}}^{s}=\bigoplus_{j=1}^{\mathcal{J}}( \sigma(\mathbf{D}_{v^{s}}^{-1/2}\bm{\mathcal{H}}_{j}^{s}\mathbf{D}_{e^{s}}^{- 1}(\bm{\mathcal{H}}_{j}^{s})^{\mathrm{T}}\mathbf{D}_{v^{s}}^{-1/2}\bm{ \mathcal{V}}^{s}\mathbf{P}_{j}^{s}))\in\mathbb{R}^{N^{s}\times D},\] (15)

where \(\widetilde{\bm{\mathcal{V}}}^{s}\) is the updated node feature representations at scale \(s\), \(\bigoplus\) is the aggregation function used for combing the outputs of multi-head, e.g., concatenation or average pooling. \(\sigma\) is the activation function, e.g., LeakyReLU and ELU. \(\bm{\mathcal{H}}_{s}^{s}\) and \(\mathbf{P}_{s}^{s}\) are the enriched incidence matrix and the learnable weight matrix of the \(\jmath\)th head at scale \(s\), respectively. \(\mathcal{J}\) is the number of heads.

**Inter-Scale Interaction Module.** The inter-scale interaction module is introduced to capture pattern interactions at different scales. To achieve this goal, a direct way is to model group-wise node interactions across all scales. However, detailed group-wise node interactions across all scales can introduce redundant information and increase computation complexity. Therefore, we adopt a hyperedge attention within the inter-scale interaction module to capture macroscopic variations interactions at different scales. Technically, based on the hyperedge representations \(\bm{\mathcal{E}}=\{\bm{\mathcal{E}}^{1},\cdots,\bm{\mathcal{E}}^{s},\cdots,\bm {\mathcal{E}}^{S}\}\), we first adopt linear projections to get queries, keys, and values \(\mathbf{Q}\), \(\mathbf{K}\), \(\mathbf{V}\in\mathbb{R}^{M\times D}\). Then the hyperedge attention can be formulated as follows:

\[\tilde{\mathbf{V}}=softmax(\frac{\mathbf{Q}\mathbf{K}^{\mathrm{T}}}{\sqrt{D_ {K}}})\mathbf{V},\] (16)

where \(\tilde{\mathbf{V}}\) is the updated hyperedge feature representations.

### Prediction Module & Loss Function

After obtaining the updated node and hyperedge feature representations, we concatenate them and feed them into a linear layer for prediction. We choose Mean Squared Error (MSE) as our forecasting loss, which can be formulated as follows:

\[L_{mse}=\frac{1}{H}\left\|\mathbf{\hat{X}}_{T+1:T+H}^{\mathrm{O}}-\mathbf{X}_ {T+1:T+H}^{\mathrm{O}}\right\|_{2}^{2},\] (17)

where \(\mathbf{X}_{T+1:T+H}^{\mathrm{O}}\) and \(\mathbf{\hat{X}}_{T+1:T+H}^{\mathrm{O}}\) are ground truth and forecasting results, respectively. Notably, during training phase, \(L_{mse}\) is used to regulate the overall learning process, while \(L_{const}\) is only used to constrain hypergraph learning process.

### Complexity Analysis

For the MFE module, the time complexity is \(\mathcal{O}(Nl)\), where \(N\) is the number of nodes at the finest scale and \(N\) is equal to the input length \(T\). \(l\) is the aggregation window size at the finest scale. For the AHL module, the time complexity is \(\mathcal{O}(MN+M^{2})\), where \(M\) is the number of hypergraphs at the finest scale. For the intra-scale interaction module, since \(\mathbf{D}_{v}\) and \(\mathbf{D}_{e}\) are diagonal matrices, the time complexity is \(\mathcal{O}(MN)\). For the inter-scale interaction module, the time complexity is \(M^{2}\). In practical operation, \(M\) and \(l\) is the hyperparameter and is much smaller than \(N\). As a result, the total time complexity of Ada-MSHyper is bounded by \(\mathcal{O}(N)\).

## 5 Experiment

### Experimental Setup

**Datasets.** For long-range time series forecasting, we conduct experiments on 7 commonly used benchmarks, including ETT (ETTh1, ETTh2, ETTm1, and ETTm2), Traffic, Electricity, and Weather datasets following [30; 21; 26]. For short-range time series forecasting, we adopt 4 benchmarks from PEMS (PEMS03, PEMS04, PEMS07, and PEMS08) following [21; 27]. For ultra-long-range time series forecasting, we adopt ETT datasets following [18]. Table 1 gives the dataset statistics. In addition, the forecastability is derived from one minus the entropy of the Fourier decomposition of a time series[27; 14]. Higher values mean greater forecastability.

**Baselines.** We compare Ada-MSHyper with 15 competitive baselines, i.e., iTransformer [21], MSHyper [26], PatchTST [23], TimeMixer [27], MSGNet [4], CrossGNN [16], TimesNet [29], WITRAN [18], SCINet [19], Crossformer [36], FiLM [38], DLinear [35], FEDformer [39], Pyraformer [20], and Autoformer [30].

\begin{table}
\begin{tabular}{l|l|l|l|l|l} \hline \hline Dataset & \multicolumn{1}{c|}{**\# Variants**} & \multicolumn{1}{c|}{**Predication Length**} & \multicolumn{1}{c|}{**Frequency**} & \multicolumn{1}{c}{**Foreganability**} & \multicolumn{1}{c}{**Inferentiations**} \\ \hline ETT (s) atten & 7 & 0.66 & 102, 36, 70, 101 & (15 units, flowby) & (0.38-0.55) & Telegrams \\ \hline Weight & 21 & 0.66 & 128, 36, 70, 101 & (10 units) & 0.75 & Weight \\ \hline Electricity & 311 & 0.62 & 0.82 & 36, 70, 101 & Handy & 0.77 & Intensity \\ \hline Traffic & 862 & 0.66 & 128, 36, 70, 101 & Handy & 0.68 & Transmission \\ \hline
**Transics (s) atten & (70880) & (122, 24, 40) & 5 units & (0.40-0.85) & Traffic network \\ \hline \hline \end{tabular}
\end{table}
Table 1: Dataset statistics.

[MISSING_PAGE_FAIL:8]

almost all benchmarks, with an average error reduction of 4.97% and 2.21% compared to the best baseline in MSE and MAE, respectively. (2) Compared with other baselines, PatchTST and MSHper achieve competitive results. The reason may be that group-wise interactions can help mitigate the issue of semantic information sparsity. (3) Compared to PatchTST and MSHper, Ada-MSHper achieves superior performance, the reason may be that the inter-scale interaction module can help capture macroscopic variations interactions, especially for the ultra-long-rang time series.

### Ablation Studies

**AHL Module.** To investigate the effectiveness of the AHL model, we conduct ablation studies by designing the following three variations: (1) Replacing the AHL module with adaptive graph learning module (-AGL). (2) Replacing the AHL model with one incidence matrix to capture group-wise node interactions at different scales (-one). (3) Replacing the AHL module with predefined multi-scale hypergraphs (-PH), i.e., each hyperedge connected a fixed number of nodes (4 in our experiment) in chronological order. The experimental results on ETH1 dataset are shown in Table 5. We can observe that -AGL gets the worst forecasting results, indicating the importance of modeling group-wise interactions. In addition, -PH and -one perform worse than Ada-MSHper, showing the effectiveness of adaptive hypergraph and multi-scale hypergraph, respectively.

**NHC Mechanism.** To investigate the effectiveness of the NHC mechanism, we conduct ablation studies by designing the following three variations: (1) Removing the node constraint (-w/o NC). (2) Removing the hyperedge constraint (-w/o HC). (3) Removing the NHC mechanism (-w/o NHC). The experimental results on ETH1 dataset are shown in Table 5. We can observe that Ada-MSHper performs better than -w/o NC and -w/o HC, showing the effectiveness of node constraint and hyperedge constraint, respectively. In addition, -w/o NHC gets the worst forecasting results, which demonstrates the superiority of the NHC mechanism in adaptive hypergraph learning. More results about ablation studies are shown in Appendix F.

To further demonstrate the effectiveness of the node constraint in clustering nodes with similar semantic information, we present case visualization with -w/o NHC and -w/o HC on Electricity dataset. We randomly select one sample and plot the node values at the finest scale. We categorize the nodes into four groups based on the node values. Nodes with the same color indicate that they may have similar semantic information. As shown in Figure 2(a), for the target node, nodes of other colors may be considered as noise. We drew the nodes related to the target node in black color based on incidence matrix \(\mathbf{H}^{1}\). As shown in Figure 2(b), due to the lack of node constraint, -w/o NHC can only capture the interactions among the target node and neighboring nodes and cannot distinguish nuanced noise information. In Figure 2(c), with the node constraint, -w/o HC can cluster neighboring and distant but still strongly correlated nodes. In Figure 2(d), with the NHC mechanism, Ada-MSHper cannot only cluster nodes with similar semantic information but can differentiate temporal variations. The full visualization results are shown in Appendix H.

### Parameter Studies

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Models} & Ada-MSHyper & Transference & MSHper & Transference & MSHper & Transference & WITRAN & PathTST & DLinear & Cross-layer & PIDE/learner & Predformer & Auto-learner \\  & (Ours) & (2024) & (3204) & (3204) & (3202) & (3202) & (3203) & (2023) & (3022) & (3022) & (3022) & (3022) \\ \hline Metric & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE \\ \hline ETH1 & **0.4565 0.457** & 0.766 0.611 & 0.745 0.610 & 0.804 0.631 & 0.734 0.833 & 0.699 0.588 & 0.666 0.648 & 0.9121 & 0.706 0.637 & 1.083 0.89 0.675 \\ \hline ETH2 & **0.4480.00** & 0.541 0.518 & 0.515 0.495 & 0.540 0.532 & 0.540 0.537 & 0.510 0.498 & 1.218 0.787 & 2.530 1.233 & 0.625 0.574 & 3.263 1.599 & 0.656 0.648 \\ \hline ETH1 & **0.484** (0.46) & 0.854 0.495 & 0.540 0.480 & 0.532 0.483 & 0.532 0.476 & 0.501 **0.440** & 0.5400 0.498 & 3.555 1.483 & 0.522 0.591 & 1.099 0.811 & 0.631 0.550 \\ \hline ETH2 & **0.425** **0.434** & 0.468 0.449 & 0.464 0.427 & 0.465 0.449 & 0.446 **0.424** & 0.462 0.448 & 0.655 0.574 & 3.555 1.483 & 0.487 0.475 & 4.566 1.745 & 0.516 0.491 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Results of ultra-long-range time series forecasting under multivariate settings. Results are averaged from all prediction lengths. Full results are listed in Appendix E.

\begin{table}
\begin{tabular}{c|c c c c c c|c c c c c c c} \hline \hline Variation & \multicolumn{2}{c}{AGL} & \multicolumn{2}{c}{one} & \multicolumn{2}{c}{PH} & \multicolumn{2}{c}{w/o NC} & \multicolumn{2}{c}{w/o HC} & \multicolumn{2}{c}{w/o NHC} & Ada-MSHper \\ \hline Metric & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\ \hline
96 & 0.542 & 0.560 & 0.422 & 0.437 & 0.386 & 0.403 & 0.390 & 0.403 & 0.384 & 0.416 & 0.393 & 0.422 & **0.372** & **0.393** \\
336 & – & – & 0.559 & 0.502 & 0.448 & 0.452 & 0.423 & 0.437 & 0.430 & 0.435 & 0.425 & 0.441 & **0.422** & **0.433** \\
729 & – & – & 0.563 & 0.617 & 0.456 & 0.458 & 0.448 & **0.457** & 0.451 & 0.460 & 0.449 & 0.466 & **0.445** & 0.459 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Results of different adaptive hypergraph learning methods and constraint mechanisms.

We perform parameter studies to measure the impact of the number of scales (#scales) and the max number of hyperedges connected to a node (#hyperedges). The experimental results on ETTh1 dataset are shown in Figure 4, we can see that: (1) the best performance can be obtained when #scales is 3. The reason is that smaller #scales cannot provide sufficient pattern information and larger #scales may introduce excessive parameters and result in overfitting problems. (2) The optimal #hyperedges is 5. The reason is that smaller #hyperedges cannot capture group-wise interactions sufficiently and larger #hyperedges may introduce noise. More results about parameter studies are shown in Appendix G.

### Computational Cost

We compare Ada-MSHper with the two latest transformer-based methods, i.e., iTransformer and PatchTST, on traffic datasets with the output length of 96. The experimental results are shown in Table 6. Although we have a larger number of parameters, we achieve lower training time and lower GPU occupation due to the matrix sparsity operation in the model and the optimization of hypergraph computation provided by _torch_geometry[2]_. Considering the forecasting performance and the computation cost, Ada-MSHper demonstrates its superiority over existing methods.

## 6 Conclusions and Future Work

In this paper, we propose Ada-MSHper with an adaptive multi-scale hypergraph for time series forecasting. Empowered by the AHL module and multi-scale interaction module, Ada-MSHper can promote more comprehensive multi-scale group-wise pattern interactions, addressing the problem of semantic information sparsity. Experimentally, Ada-MSHper achieves the SOTA performance, reducing prediction errors by an average of 4.56%, 10.38%, and 4.97% in MSE for long-range, short-range, and ultra-long-range time series forecasting, respectively. In addition, the visualization analysis and the ablation studies demonstrate the effectiveness of NHC mechanism in clustering nodes with similar semantic information and in addressing the issue of temporal variations entanglement.

In the future, this work can be extended in the following two aspects. First, since 2D spectrogram data may offer a better representation for time series forecasting, we will adapt our framework to the 2D spectrogram data in time-frequency domain. Second, since the features extracted by the MFE module may contain redundant information, we will design a disentangled multi-scale feature extraction module to extract more independent and representative features.

\begin{table}
\begin{tabular}{l|c|c|c|c} \hline \hline Methods & Training Time & \# Parameters & GPU Occupation & MSE results \\ \hline Ada-MSHper & **6.499\%** & 8,965,392 & **6.542MB** & **0.384** \\ iTransformer & 7.863\% & 6,731,984 & 6,738MB & 0.395 \\ PatchTST & 17.603\% & **548.704** & 9,798MB & 0.526 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Computation cost.

Figure 4: The impact of hyperparameters.

Figure 3: Visualization the node constraint effect on Electricity dataset.

Acknowledgement

This work was supported by the Science Foundation of Donghai Laboratory (Grant No. DH-2022ZY0013).

## References

* [1] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. _arXiv preprint arXiv:1803.01271_, 2018.
* [2] Song Bai, Feihu Zhang, and Philip HS Torr. Hypergraph convolution and hypergraph attention. _Pattern Recognition_, 110:107637, 2021.
* [3] George EP Box and Gwilym M Jenkins. Some recent advances in forecasting and control. _Journal of the Royal Statistical Society. Series C (Applied Statistics)_, 17(2):91-109, 1968.
* [4] Wanlin Cai, Yuxuan Liang, Xianggen Liu, Jianshuai Feng, and Yuankai Wu. MSGNet: Learning multi-scale inter-series correlations for multivariate time series forecasting. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 11141-11149, 2024.
* [5] Haizhou Cao, Zhenhao Huang, Tiechui Yao, Jue Wang, Hui He, and Yangang Wang. In-Parformer: Evolutionary decomposition transformers with interactive parallel attention for long-term time series forecasting. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 6906-6915, 2023.
* [6] Donghui Chen, Ling Chen, Zongjiang Shang, Youdong Zhang, Bo Wen, and Chenghu Yang. Scale-aware neural architecture search for multivariate time series forecasting. _ACM Transactions on Knowledge Discovery from Data_, 2024.
* [7] Donghui Chen, Ling Chen, Youdong Zhang, Bo Wen, and Chenghu Yang. A multiscale interactive recurrent network for time-series forecasting. _IEEE Transactions on Cybernetics_, 52(9):8793-8803, 2021.
* [8] Ling Chen, Donghui Chen, Zongjiang Shang, Binqing Wu, Cen Zheng, Bo Wen, and Wei Zhang. Multi-scale adaptive graph neural network for multivariate time series forecasting. _IEEE Transactions on Knowledge and Data Engineering_, pages 10748-10761, 2023.
* [9] Ling Chen and Jiahua Cui. TPRNN: A top-down pyramidal recurrent neural network for time series forecasting. _arXiv preprint arXiv:2312.06328_, 2023.
* [10] Zipeng Chen, Qianli Ma, and Zhenxi Lin. Time-aware multi-scale RNNs for time series modeling. In _IJCAI_, pages 2285-2291, 2021.
* [11] Abhimanyu Das, Weihao Kong, Andrew Leach, Shaan K Mathur, Rajat Sen, and Rose Yu. Long-term forecasting with TiDE: Time-series dense encoder. _Transactions on Machine Learning Research_, 2023.
* [12] Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. Tsmixer: Lightweight mlp-mixer model for multivariate time series forecasting. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 459-469, 2023.
* [13] Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue Gao. Hypergraph neural networks. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 3558-3565, 2019.
* [14] Georg Goerg. Forecastable component analysis. In _International conference on machine learning_, pages 64-72. PMLR, 2013.
* [15] Hansika Hewamalage, Klaus Ackermann, and Christoph Bergmeir. Forecast evaluation for data scientists: common pitfalls and best practices. _Data Mining and Knowledge Discovery_, 37(2):788-832, 2023.

* [16] Qihe Huang, Lei Shen, Ruixin Zhang, Shouhong Ding, Binwu Wang, Zhengyang Zhou, and Yang Wang. CrossGNN: Confronting noisy multivariate time series via cross interaction refinement. _Advances in Neural Information Processing Systems_, 36:46885-46902, 2023.
* [17] Yuchi Huang, Qingshan Liu, and Dimitris Metaxas. ] video object segmentation by hypergraph cut. In _2009 IEEE conference on computer vision and pattern recognition_, pages 1738-1745. IEEE, 2009.
* [18] Yuxin Jia, Youfang Lin, Xinyan Hao, Yan Lin, Shengnan Guo, and Huaiyu Wan. WITRAN: Water-wave information transmission and recurrent acceleration network for long-range time series forecasting. _Advances in Neural Information Processing Systems_, 36, 2024.
* [19] Minhao Liu, Ailing Zeng, Muxi Chen, Zhijian Xu, Qiuxia Lai, Lingna Ma, and Qiang Xu. SCINet: Time series modeling and forecasting with sample convolution and interaction. _Advances in Neural Information Processing Systems_, 35:5816-5828, 2022.
* [20] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and Schahram Dustdar. Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting. In _Proceedings of the International Conference on Learning Representations_, 2021.
* [21] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long. iTransformer: Inverted transformers are effective for time series forecasting. In _The Twelfth International Conference on Learning Representations_, 2023.
* [22] Yong Liu, Chenyu Li, Jianmin Wang, and Mingsheng Long. Koopa: Learning non-stationary time series dynamics with koopman predictors. In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems_, volume 36, pages 12271-12290. Curran Associates, Inc., 2023.
* [23] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. In _The Eleventh International Conference on Learning Representations_, 2022.
* [24] Ramit Sawhney, Shivam Agarwal, Arnav Wadhwa, Tyler Derr, and Rajiv Ratn Shah. Stock selection via spatiotemporal hypergraph attention network: A learning to rank approach. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 497-504, 2021.
* [25] Rajat Sen, Hsiang-Fu Yu, and Inderjit S Dhillon. Think globally, act locally: A deep neural network approach to high-dimensional time series forecasting. _Advances in neural information processing systems_, 32, 2019.
* [26] Zongjiang Shang and Ling Chen. MSHYper: Multi-scale hypergraph transformer for long-range time series forecasting. _arXiv preprint arXiv:2401.09261_, 2024.
* [27] Shiyu Wang, Haixu Wu, Xiaoming Shi, Tengge Hu, Huakun Luo, Lintao Ma, James Y Zhang, and JUN ZHOU. TimeMixer: Decomposable multiscale mixing for time series forecasting. In _The Twelfth International Conference on Learning Representations_, 2023.
* [28] Binqing Wu, Weiqi Chen, Wengwei Wang, Bingqing Peng, Liang Sun, and Ling Chen. WeatherGNN: Exploiting meteo- and spatial-dependencies for local numerical weather prediction bias-correction. In _Proceedings of the International Joint Conference on Artificial Intelligence_, pages 2433-2441, 2024.
* [29] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. TimesNet: Temporal 2d-variation modeling for general time series analysis. In _The eleventh international conference on learning representations_, 2022.
* [30] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. _Advances in Neural Information Processing Systems_, pages 22419-22430, 2021.
* [31] Chenxin Xu, Maosen Li, Zhenyang Ni, Ya Zhang, and Siheng Chen. GroupNet: Multiscale hypergraph neural networks for trajectory prediction with relational reasoning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6498-6507, 2022.

* [32] Naganand Yadati, Madhav Nimishakavi, Prateek Yadav, Vikram Nitin, Anand Louis, and Partha Talukdar. HyperGCN: A new method for training graph convolutional networks on hypergraphs. _Advances in Neural Information Processing Systems_, 32, 2019.
* [33] Yichao Yan, Jie Qin, Jiaxin Chen, Li Liu, Fan Zhu, Ying Tai, and Ling Shao. Learning multi-granular hypergraphs for video-based person re-identification. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 2899-2908, 2020.
* [34] Dingqi Yang, Bingqing Qu, Jie Yang, and Philippe Cudre-Mauroux. LBSN2Vec++: Heterogeneous hypergraph embedding for location-based social networks. _IEEE Transactions on Knowledge and Data Engineering_, 34(4):1843-1855, 2020.
* [35] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? _arXiv preprint arXiv:2205.13504_, 2022.
* [36] Yunhao Zhang and Junchi Yan. Crosformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting. In _Proceedings of the International Conference on Learning Representations_, 2023.
* [37] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 11106-11115, 2021.
* [38] Tian Zhou, Ziqing Ma, Qingsong Wen, Liang Sun, Tao Yao, Wotao Yin, Rong Jin, et al. FiLM: Frequency improved legendre memory model for long-term time series forecasting. _Advances in Neural Information Processing Systems_, 35:12677-12690, 2022.
* [39] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. FEDformer: Frequency enhanced decomposed transformer for long-term series forecasting. In _Proceedings of the International Conference on Machine Learning_, pages 27268-27286, 2022.

Descriptions of Notations

To help understand the symbols used throughout the paper, we provide a detailed list of the key notations in Table 7.

## Appendix B

\begin{table}
\begin{tabular}{l|l} \hline \hline Notation & Descriptions \\ \hline \(\mathcal{G}\) & Hypergraph \\ \hline \(\mathcal{E}\) & Hyperedge set \\ \hline \(\mathcal{V}\) & Node set \\ \hline \(N\) & Number of nodes \\ \hline \(M\) & Number of hyperedges \\ \hline \(T\) & Input length \\ \hline \(H\) & Output length \\ \hline \(D\) & Feature dimension \\ \hline \(S\) & Total number of temporal scales \\ \hline \(s\) & Scale index \\ \hline \(\mathbf{X}^{1}_{i}\)\({}_{T}\) & Historical input sequence \\ \hline \(\mathbf{X}^{s}\) & Sub-sequence at scale \(s\) \\ \hline \(\boldsymbol{\pi}_{t}\) & Values at time step \(t\) \\ \hline \(\tilde{\mathbf{X}}^{0}_{T+1:T+H}\in\mathbb{R}^{M\times D}\) & Forecasting results \\ \hline \(\boldsymbol{E}^{s}_{\text{node}}\in\mathbb{R}^{N^{s}\times D}\) & Node embedding at scale \(s\) \\ \hline \(\boldsymbol{E}^{s}_{\text{hyper}}\in\mathbb{R}^{M^{s}\times D}\) & Hyperedge embeddings at scale \(s\) \\ \hline \(e^{s}_{i}\) & with hyperedge at scale \(s\) \\ \hline \(v^{s}_{i}\) & with node at scale \(s\) \\ \hline \(\boldsymbol{e}^{s}_{i}\) & with hyperedge feature representation at scale \(s\) \\ \hline \(\boldsymbol{v}^{s}_{i}\) & with node feature representation at scale \(s\) \\ \hline \(\eta\) & Threshold of \(TopK\) function \\ \hline \(\beta\) & Threshold of the scale-specific incidence matrices \\ \hline \(\gamma\) & Threshold of the Euclidean distance \\ \hline \(\lambda\) & Balancing hyperparameter between node loss and hyperedge loss \\ \hline \(\mathbf{H}^{s}\) & Incidence matrix at scale \(s\) \\ \hline \(l^{s-1}\) & Sue of the aggregation window at scale \(s-1\) \\ \hline \(\boldsymbol{\mathcal{V}}^{s}\) & Intuitized node feature representations at scale \(s\) \\ \hline \(\boldsymbol{\mathcal{E}}^{s}\) & Intuitized hyperedge feature representations at scale \(s\) \\ \hline \(\tilde{\boldsymbol{\mathcal{V}}}^{s}\) & Updated node feature representations at scale \(s\) \\ \hline \(\tilde{\boldsymbol{\mathcal{V}}}\) & Updated hyperedge feature representations \\ \hline \(\mathcal{J}\) & Number of breaks \\ \hline \(\boldsymbol{\mathcal{H}}^{s}_{j}\) & Zuriched incidence matrix of the \(gh\) head at scale \(s\) \\ \hline \(\mathcal{N}(v^{s}_{i})\) & Neighboring hyperedges connected to \(v^{s}_{i}\) \\ \hline \(\mathcal{N}(e^{s}_{i})\) & Neighboring nodes connected by \(e^{s}_{i}\) \\ \hline \(\mathbf{Q},\mathbf{K},\mathbf{V}\in\mathbb{R}^{M\times D}\) & Queries, keys, and values \\ \hline \(\alpha_{i,j}\) & Correlation weight between \(gh\) and \(gh\) hyperedge representations \\ \hline \(D_{i,j}\) & Euclidean distance between \(gh\) and \(gh\) hyperedge representations \\ \hline \(L_{node}\) & Node constraint loss at scale \(s\) \\ \hline \(L_{hyper}\) & Hyperedges constraint loss at scale \(s\) \\ \hline \(L_{const}\) & Final constraint loss \\ \hline \(L_{mse}\) & MSE loss \\ \hline \hline \end{tabular}
\end{table}
Table 7: Description of the key notations.

Descriptions of Datasets

**Datasets.** For long-range time series forecasting, we conduct experiments on 7 commonly used benchmarks, including Electricity Transformers Temperature (ETT), Traffic2, Electricity3, and Weather4 datasets following [30, 21, 26]. ETT datasets include data from two counties in the same Chinese province, each data point comprising seven variables: the target variable "oil temperature" and six power load features. The datasets vary in granularity, with "h" indicating hourly data and "m" indicating 15-minute intervals. Weather dataset contains 21 weather indicators collected every 10 minutes from a weather station in Germany. Electricity dataset records hourly electricity consumption of 321 clients. Traffic dataset provides hourly road occupancy rates from 821 freeway sensors. For short-range time series forecasting, we use four benchmarks from PEMS (PEMS03, PEMS04, PEMS07, and PEMS08), as referenced in [27, 19]. These datasets capture 5-minute traffic flow data from freeway sensors. For ultra-long-range time series forecasting, we adopt ETTh1, ETTh2, ETTm1, and ETTm2 following [18]. Table 8 gives the detailed dataset statistics. In addition, the forecastability is derived from one minus the entropy of the Fourier decomposition of a time series[27, 14]. Higher values mean greater forecastability.

Footnote 2: http://pems.dot.ca.gov

Footnote 3: https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014

Footnote 4: https://www.bgc-jena.mpg.de/wetter/

We adopt the same data processing and train-validation-test split protocol as in existing works [26, 21, 23]. We split each dataset into training, validation, and test sets based on chronological order. For PEMS (PEMS03, PEMS04, PEMS07, and PEMS08) dataset and ETT (ETTh1, ETTh2, ETTm1, and ETTm2) dataset, the train-validation-test split ratio is 6:2:2. For Weather, Traffic, and Electricity dataset, the train-validation-test split ratio is 7:2:1.

**Metric details.** Following existing methods [26, 21], we employ Mean Squared Error (MSE) and Mean Absolute Error (MAE) as our evaluation metrics, which can be formulated as follows:

\[L_{mse}=\frac{1}{H}\left\|\widehat{\mathbf{X}}_{T+1:T+H}^{\text{O}}-\mathbf{X} _{T+1:T+H}^{\text{O}}\right\|_{2}^{2}\] (18)

\[L_{mae}=\frac{1}{H}|\widehat{\mathbf{X}}_{T+1:T+H}^{\text{O}}-\mathbf{X}_{T+1: T+H}^{\text{O}}|,\] (19)

where \(T\) and \(H\) are the input and output lengths, \(\widehat{\mathbf{X}}_{T+1:T+H}^{\text{O}}\) and \(\mathbf{X}_{T+1:T+H}^{\text{O}}\) are the predicted results and ground truth.

## Appendix C Descriptions of Baselines

We compare Ada-MSHyper with 15 competitive baselines. Below are brief descriptions of the baselines: (1) iTransformer [21]: Applies the attention and feed-forward network on the inverted dimensions, i.e., the time points of individual series are embedded into variate tokens, and the feed-forward network is applied for each variate token to learn nonlinear representations. (2) MSHper [26]: Utilizes rule-based multi-scale hypergraphs to model high-order pattern interactions in univariate time series. (3) PatchTST [23]: Uses channel-independent techniques and treats subseries-level patches as

\begin{table}
\begin{tabular}{c|l|l|l|l|l|l} \hline \hline \multicolumn{1}{c|}{\multirow{2}{*}{Task}} & \multicolumn{1}{c|}{Dataset} & \multicolumn{1}{c|}{\# Variates} & \multicolumn{1}{c|}{Prediction Length} & \multicolumn{1}{c|}{Frequency} & \multicolumn{1}{c|}{Forecastability} & \multicolumn{1}{c}{Information} \\ \hline \multirow{4}{*}{Long-term} & ETTh1, ETTh2 & 7 & (96, 192, 336,720) & Hourly & 0.38, 0.45 & Temperature \\ \cline{2-6}  & ETTm1, ETTm2 & 7 & (96, 192, 336,720) & 15 mins & 0.46, 0.55 & Temperature \\ \cline{2-6}  & Weather & 21 & (96, 192, 336,720) & 10 mins & 0.75 & Weather \\ \cline{2-6}  & Electricity & 321 & (96, 192, 336,720) & Hourly & 0.77 & Electricity \\ \cline{2-6}  & Traffic & 862 & (96, 192, 336,720) & Hourly & 0.68 & Transportation \\ \hline \multirow{4}{*}{Short-term} & PEMS03 & 358 & 12 & 5 min & 0.65 & Transportation \\ \cline{2-6}  & PEMS04 & 307 & 12 & 5 mins & 0.45 & Transportation \\ \cline{1-1} \cline{2-6}  & PEMS07 & 883 & 12 & 5 mins & 0.58 & Transportation \\ \cline{1-1} \cline{2-6}  & PEMS08 & 170 & 12 & 5 mins & 0.52 & Transportation \\ \hline \hline \end{tabular}
\end{table}
Table 8: Detailed dataset statistics.

input tokens to a Transformer, facilitating semantic extraction of multiple time steps in time series. (4) TimesMixer [27]: Employs a fully MLP-based architecture with past-decomposable-mixing and future-multipredictor-mixing blocks to leverage disentangled multiscale series. (5) MSGNet [4]: Leverages frequency domain analysis to extract periodic patterns and combines an attention mechanism with adaptive graph convolution to capture multi-scale pattern interactions. (6) CrossGNN [16]: Uses an adaptive multi-scale identifier to construct multi-scale representations and utilizes a cross-scale GNN to capture multi-scale pattern interactions. (7) TimesNet [29]: Conducts multi-periodicity analysis by extending 1D time series into a set of 2D tensors, modeling complex temporal variations from a 2D perspective. (8) WITRAN [18]: Proposes an RNN-based architecture that handles univariate input sequences from a 2D space perspective, maintaining a fixed scale throughout the processing. (9) SCINet [19]: Uses a recursive downsample-convolve-interact architecture to extract temporal features from downsampled sub-sequences or features. (10) Crossformer [36]: Adopts cross-dimension attention to capture inter-series dependencies for multivariate time series forecasting. (11) FILM [38]: Applies Legendre polynomial projections to approximate historical information, uses Fourier projections to remove noise, and adds a low-rank approximation to speed up computation. (12) DLinear [35]: Decomposes time series into two different components and uses a single linear layer for each component to model temporal dependencies. (13) FEDformer [39]: Utilizes a seasonal-trend decomposition method to capture the global profile of time series and a frequency-enhanced Transformer to capture more detailed structures. (14) Pyraformer [20]: Utilizes a pyramidal attention module to extract inter-scale features at different resolutions and intra-scale features at different ranges with linear complexity. (15) Autoformer [30]: Uses an auto-correlation mechanism based on series periodicity to capture features at the sub-series level.

## Appendix D Experimental Settings

We repeat all experiments 3 times and use the mean of the metrics as the final results. The training process is early stopped when there is no improvement within 5 epochs. Following existing works [21; 27; 35], we use instance normalization to normalize all datasets. The max number of scale \(S\) is set to 3. We use 1D convolution as our aggregation function. For other hyperparameters, we use Neural Network Intelligence (NNI)5 toolkit to automatically search the best hyperparameters, which can greatly reduce computation cost compared to the grid search approach. The detailed search space of hyperparameters is given in Table 9. The source code of Ada-MSHyper is released on GitHub 6.

Footnote 5: https://nni.readthedocs.io/en/latest/

Footnote 6: https://github.com/shangzongjiang/Ada-MSHyper

## Appendix E Full Results

We compare Ada-MSHyper with 13 baselines across four tasks: long-range forecasting for multivariate time series, long-range forecasting for univariate time series, ultra-long-range forecasting for multivariate time series, and short-range forecasting for multivariate time series. For a fair comparison, we evaluate Ada-MSHyper and baselines under unified experimental settings of each task. The average results from all prediction lengths are presented in tables, where the best results are **bolded**

\begin{table}
\begin{tabular}{l|l} \hline \hline Parameters & Choice \\ \hline Batch size & [8, 16; 32; 64; 128] \\ \hline Number of hyperedges at scale 1 & [10, 20, 30, 50] \\ \hline Number of hyperedges at scale 2 & [5, 10, 15, 20] \\ \hline Number of hyperedges at scale 3 & [1, 2, 4, 5, 8, 12] \\ \hline Aggregation window at scale 1 & [2, 4, 8] \\ \hline Aggregation window at scale 2 & [2, 4] \\ \hline \(\eta\) & [1, 3, 5, 10, 15, 20] \\ \hline \(\beta\) & [0.2, 0.3, 0.4, 0.5] \\ \hline \(\gamma\) & [0.2, 0.3, 0.4, 0.5] \\ \hline \hline \end{tabular}
\end{table}
Table 9: The search space of hyperparameters.

[MISSING_PAGE_FAIL:17]

without * are cited from iTransformer [21]. We can see from Table 13 that Ada-MSHper achieves the SOTA results on all datasets. Specifically, Ada-MSHpper gives an average error reduction of 10.38% and 3.82% compared to the best baseline in terms of MSE and MAE, respectively.

Ultra-Long-Range Time Series Forecasting Under Multivariate Settings.We conduct ultra-long-range time series forecasting by taking fixed input length (\(T=96\)) to predict ultra-long lengths (\(H=\{1080,1440,1800,2160\}\)). We run all results by ourselves. Table 14 summarizes the results of ultra-long-range time series forecasting under multivariate settings, where - - indicates that the method fails to produce any results on that prediction length due to the out-of-memory problems. We can see from Table 14 that Ada-MSHper achieves SOTA results on almost all datasets. Specifically, Ada-MSHpper gives an average error reduction of 4.97% and 2.21% compared to the best baseline in terms of MSE and MAE, respectively.

## Appendix F Ablation Studies

To investigate the performance of Ada-MSHpper on longer prediction lengths, we compare the forecasting results of Ada-MSHpper with those of six variations (i.e., AGL, one, PH, -w/o NC, -w/o HC, and -w/o NHC) on ETH1 dataset. The experimental results are shown in Table 15. We can observe that for longer prediction lengths, -w/o NC has smaller performance degradation than other variations. The reason may be that when the prediction length increases, the model tends to focus more on macroscopic variation interactions and diminishes its emphasis on fine-grained node constraint. In addition, Ada-MSHpper performs better than other six variations even with longer prediction length, showing the effectiveness of our AHL module and NHC mechanism.

To investigate the impact of node and hypergraph constraints mechanism on the adaptive hypergraph learning (AHL) module, we design two variants: (1) Removing the NHC mechanism (-w/o NHC).

\begin{table}
\begin{tabular}{c|c c c c c c|c c c c|c c c|c} \hline \hline \multirow{2}{*}{Models} & Ada-MSHpper & \multicolumn{2}{c|}{Transformer*} & MSBPper & \multicolumn{2}{c|}{Transformer*} & \multicolumn{2}{c|}{Puff.NHC/NT} & \multicolumn{2}{c|}{Transformer} & \multicolumn{2}{c|}{Consistency} & \multicolumn{2}{c|}{FPDGenerator*} & \multicolumn{2}{c}{AutoGenerator} \\  & (Ours) & (2020) & (2020) & (2020) & (2020) & (2022) & (2022) & (2022) & (2022) & (2022) & (2022) & (2022) & (2022) \\ \hline \multirow{2}{*}{Metric} & MSE & MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE \\ \hline \multirow{2}{*}{FEMSOSO} & 24.0 & **0.05** & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 \\  & 48 & **0.05** & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 \\ \hline \multirow{2}{*}{FEMSOSO} & 24.0 & **0.05** & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 \\  & 48 & **0.05** & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 \\ \hline \multirow{2}{*}{FEMSOSO} & 24.0 & **0.05** & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 \\  & 48 & **0.05** & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.

[MISSING_PAGE_FAIL:19]

original inputs, and draw them using different colors. For a target node, nodes of the same color may be regarded as those sharing similar semantic information with the target node, while nodes of other colors may be regarded as noise. Then, we draw the nodes related to the target node based on the incidence matrix \(\mathbf{H}^{1}\) of the learned hypergraph in the black color.

We random select samples at the same time step from three variants, i.e., without node constraint (-w/o NC), without hyperedge constraint (-w/o HC), and without node and hyperedges constraints (-w/o NHC), and plot these three samples with samples from original inputs and Ada-MHyper.

We can observe that: (1) In Figure (b)b and Figure (c)c, the related nodes of the target node are almost neighboring nodes. However, some noise, plotted as orange, is included as well. The reason may be that -w/o NHC and -w/o NC cannot distinguish noise information without node constraint, i.e., cannot consider the semantic similarity to cluster nodes. (2) In Figure (d)d and Figure (e)e, since -w/o HC and the proposed Ada-MSHper have node constraint, both of them can cluster neighboring and distant but strongly correlated nodes, and they can also mitigate the interference of noise, indicating the effectiveness of node constraint.

**Visualization of Hyperedge Constraint.** We use the samples which are used in the visualization of node constraint. We visualize the sequentially connecting nodes that belong to the same hyperedges whose indices are \(\{4,8,12\}\). Figure 7 shows three types of temporal variations learned by -w/o NC, -w/o HC, -w/o NHC, and Ada-MSHyper, respectively. We can observe that: (1) -w/o HC and -w/o NHC exhibit irregular temporal variations in comparison to -w/o NC and Ada-MSHyper. The reason may be that without the hyperedge constraint, these methods are unable to adequately differentiate temporal variations entangled in temporal patterns. (2) Compared to -w/o NC, Ada-MSHyper exhibits relatively simple temporal variations. The reason may be that influenced by node constraint, the temporal variations extracted by Ada-MSHyper contain less noise.

Figure 6: Visualization the node constrain effect on Electricity dataset.

Figure 7: Different temporal variations learned by different methods.

We also matched the temporal variations extracted by Ada-MSHper to the sample sequences. As shown in Figure (b)b, we can observe that these variations can represent inherent changes. We speculate that by introducing hyperedge constraint, the model will treat temporal variations with different shapes as distinct positive and negative examples. In addition, the differentiated temporal variations are like a kind of Shapelet, akin to those used in NLP and CV, enabling a better representation of temporal patterns within time series.

## Appendix I Limitations and Future Works

In the future, we will extend our work in the following directions. Firstly, due to our NHC mechanism can cluster nodes with similar semantic information and differentiate temporal variations within each scales, It is interesting to correlate the inherent temporal variations with corpora used in natural language processing, and leverage large language models to investigate deeper correlations between corpora and TS data. Secondly, compared to natural language processing and computer vision, time series analysis has access to fewer datasets, which may limit the expressive power of the models. Therefore, in the future, we plan to compile larger datasets to validate the generalization capabilities of our models on more extensive data.

## Appendix J Broader Impacts

In this paper, we propose Ada-MSHper for time series forecasting. Extensive experimental results demonstrate the effectiveness of Ada-MSHper. Our paper mainly focuses on scientific research and has no obvious negative social impact.

Figure 8: Visualization the hyperedge constraint effect on Electricity dataset.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims made in the abstract and introduction accurately reflect the contributions and scope of the paper (see Abstract and Introduction) Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations of our work in AppendixI. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: The paper provides corresponding experimental validation in Section5.2 and AppendixE, provides ablation studies in Section 5.3 and Appendix F, and provides visualization analysis in Appendix H to support the claimed capabilities of the model. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We have provided the details regarding computational platforms, dataset descriptions, network architectures, hyper-parameter settings, and the training process of our method in Section B in the main paper and Appendix B, C, and D. In addition, we provide source codes on anonymous Github as stated in the abstract. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Our codes are released at anonymous Github as stated in the abstract. The download links of the public datasets are provided in the project homepage and pre-processing functions are included in the codes. The hyper-parameter settings are given in Appendix D. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyper-parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We have provided the details regarding computational platforms, dataset descriptions, network architectures, hyper-parameter settings, and the training process of our method in Section B in the main paper and Appendix B, C, and D. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We repeat all experiments 3 times and use the mean of the metrics as the final results as illustrated in Appendix D. Guidelines: * The answer NA means that the paper does not include experiments.

* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We have stated the experimental platforms (i.e., GPUs and CPUs) and software (the version of Pytorch) used in our paper in Section 5.1 and Appendix D, and we have analyzed the computational efficiency in Appendix **??**. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed the NeurIPS Code of Ethics and we have ensured to preserve anonymity. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes]Justification: We discuss both potential positive societal impacts and negative societal impacts of our work in Appendix J. Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our paper poses no such risks, as we focus on theoretical analysis and general research areas, and conduct our experiments on public datasets. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We make sure to cite the original papers (or URLs) of the code packages or datasets that are used in our paper. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset.

* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: Our codes are provided on the project homepage at anonymous Github. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: No crowdsourcing nor research with human subjects is involved. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: No crowdsourcing nor research with human subjects is involved. Guidelines:* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.