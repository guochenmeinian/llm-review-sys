[MISSING_PAGE_FAIL:1]

Indeed, unlike classical games (where a mixed strategy is a probabilistic mixture of the underlying pure strategies), quantum games utilize mixed states, which represent probabilistic mixtures of quantum projectors. As a consequence, a mixed quantum state can yield payoffs that cannot be expressed as a convex combination of classical pure strategies.

In light of this, quantum learning has drawn significant attention in recent years [2, 27, 28, 33, 34, 60]. In a multi-agent context, the most widely used framework is the so-called _matrix multiplicative weights_ (MMW) algorithm [2, 16, 27, 28, 35]: First introduced by Tsuda et al. [59] in the context of matrix and dictionary learning, MMW can be viewed as a semidefinite analogue of the standard Hedge / EXP3 methods for multi-armed bandits [6, 36, 61], and is a special case of the mirror descent family of algorithms [47]. Specifically, in the contrete setting of two-player, zero-sum quantum games, Jain & Watrous [27] showed that players using the MMW algorithm can learn an \(\varepsilon\)-equilibrium in \(\mathcal{O}(1/\varepsilon^{2})\) iterations - or, in terms of speed of convergence after \(T\) iterations, they converge to equilibrium at a \(\mathcal{O}(1/\sqrt{T})\) rate.

To the best of our knowledge, this result remains the tightest known bound for equilibrium learning in quantum games - and the more general class of semidefinite games [26]. At this point, we highlight that we focus on classical computing algorithms for solving quantum games, unlike recent results [10, 22] that employ quantum algorithms to solve classical games and semidefinite programs. Building on [27], Jain et al. [28] studied its continuous-time analogue - the _quantum replicator dynamics_ (QRD) - in quantum min-max games, focusing on the recurrence and volume conservation properties of the players' actual trajectory of play. Going beyond the min-max case, [38] examined the convergence of the dynamics of _"follow the quantum leader"_ (FTQL), a class of continuous-time dynamics that includes the QRD as a special case. The main result of [38] was that the only states that are asymptotically stable under the (continuous-time) dynamics of FTQL are those that satisfy a certain first-order stationarity condition known as _variational stability_[44, 46]. In a similar line of work, Lin et al. [35] studied the continuous-time QRD, and discrete-time MMW in quantum potential games, utilizing a Riemannian metric to obtain a gradient flow in the spirit of [41, 42].

Our contributions in the context of previous work. All works mentioned above, in both continuous and discrete time, assume full information, i.e., players have access to their individual payoff gradients - which, among others, might imply that they have full knowledge of the game. However, this condition is rarely met in online learning environments where players only observe their in-game payoffs; this is precisely the starting point of our paper which aims to derive a convergent payoff-based, gradient-free variant of MMW algorithm for learning in quantum games.

A major roadblock in this is that standard approaches from learning in finite games fail in the quantum setup for two reasons: First and foremost, there is a _continuum_ of pure states available to every player, unlike classical finite games where there is only a _finite_ set of pure actions. Second, even after the realization of the pure states of the players, there is an inherent uncertainty and randomness due to the payoff-generating quantum process (an aspect that has no classical counterpart). To overcome this hurdle, we employ a continuous-action reformulation of quantum games, and we leverage techniques from bandit convex optimization for estimating the players' payoff gradients.

Our first contribution is a variant of MMW that only requires mixed payoff observations and achieves an \(\mathcal{O}(1/\sqrt{T})\) equilibrium convergence rate in two-player zero-sum quantum games, matching the rate of the full information MMW in [27]. Then, to account for information-starved environments where players are only able to observe their in-game, realized payoff observable, we also develop a bandit variant of MMW which utilizes a single-point gradient estimation technique in the spirit of [55] and achieves an \(\mathcal{O}(T^{-1/4})\) equilibrium convergence rate. Finally, we also examine the behavior of the MMW algorithm with bandit information in general \(N\)-player games, where we show that variationally stable equilibria are locally attracting with high probability.

Importantly, the above results transfer to more general games with a semidefinite structure - such as multi-agent covariance matrix optimization in signal processing, energy efficiency maximization in multi-antenna systems, etc. [43, 45, 62]. While we do not provide a complete theory, we discuss a number of non-quantum applications that showcase how our results can be generalized further.

Notation. Given a (complex) Hilbert space \(\mathcal{H}\), we will use Dirac's bra-ket notation and write \(\ket{\psi}\) for an element of \(\mathcal{H}\) and \(\bra{\psi}\) for its adjoint; otherwise, when a specific basis is implied by the context, we will use the dagger notation "\(\dagger\)" to denote the Hermitian transpose \(\psi^{\dagger}\) of \(\psi\). We will also write \(\mathbb{H}^{d}\) for the space of \(d\times d\) Hermitian matrices, and \(\mathbb{H}^{d}_{t}\) for the cone of positive-semidefinite matrices in \(\mathbb{H}^{d}\). Finally, we denote by \(\|\mathbf{A}\|_{F}=\sqrt{\operatorname{tr}[\mathbf{A}^{\dagger}\mathbf{A}]}\) the Frobenius norm of \(\mathbf{A}\) in \(\mathbb{H}^{d}\).

## 2 Problem setup and preliminaries

We begin by reviewing some basic notions from the theory of quantum games, mainly intended to set notation and terminology; for a comprehensive introduction, see [23]. To streamline our presentation, we introduce the primitives of quantum games in a 2-player setting before treating the general case.

Quantum games.Following [20; 23], a 2-player _quantum game_ consists of the following:

1. Each player \(i\in\mathcal{N}\coloneqq\{1,2\}\) has access to a complex Hilbert space \(\mathcal{H}_{i}\cong\mathbb{C}^{d_{i}}\) describing the set of (pure) _quantum states_ available to the player (typically a discrete register of qubits). A quantum state is an element \(\psi_{i}\) of \(\mathcal{H}_{i}\) with unit norm, so the set of pure states is the unit sphere \(\Psi_{i}\coloneqq\{\psi_{i}\in\mathcal{H}_{i}:\|\psi_{i}\|_{F}=1\}\) of \(\mathcal{H}_{i}\). We will write \(\Psi\coloneqq\Psi_{1}\times\Psi_{2}\) for the space of all ensembles \(\psi=(\psi_{1},\psi_{2})\) of pure states \(\psi_{i}\in\Psi_{i}\) that are independently prepared by each player.
2. The rewards that players receive are based on their individual _payoff functions_\(u_{i}\colon\Psi\to\mathbb{R}\), and they are derived through a _positive operator-valued measure_ (POVM) quantum measurement process. Following [17], this unfolds as follows: Given a _finite_ set of _measurement outcomes_\(\Omega\) that a referee can observe from the players' quantum states (e.g., measure a player-prepared qubit to be "up" or "down"), each outcome \(\omega\in\Omega\) is associated to a positive semi-definite operator \(\mathbf{P}_{\omega}\colon\mathcal{H}\to\mathcal{H}\) defined on the tensor product \(\mathcal{H}\coloneqq\mathcal{H}_{1}\otimes\mathcal{H}_{2}\) of the players' individual state spaces. We further assume that \(\sum_{\omega\in\Omega}\mathbf{P}_{\omega}=\mathbf{I}\) so the probability of observing \(\omega\in\Omega\) at state \(\psi\in\Psi\) is \(P_{\omega}(\psi)=\langle\psi_{1}\otimes\psi_{2}|\mathbf{P}_{\omega}|\psi_{1} \otimes\psi_{2}\rangle\).
3. The payoff of each player is then generated by this measurement process via a _payoff observable_\(U_{i}\colon\Omega\to\mathbb{R}\): specifically, the measurement \(\omega\) is drawn from \(\Omega\) based on the players' state profile \(\psi=(\psi_{1},\psi_{2})\), and each player \(i\in\mathcal{N}\) receives as reward the quantity \(U_{i}(\omega)\). Accordingly, the player's expected payoff at state \(\psi\in\Psi\) is \(u_{i}(\psi)\coloneqq\langle U_{i}\rangle\equiv\sum_{\omega}P_{\omega}(\psi) \,U_{i}(\omega)\).

A _quantum game_ is then defined as a tuple \(\mathcal{Q}\equiv\mathcal{Q}(\mathcal{N},\Psi,u)\) with players, states, and payoff as above.

Mixed states.Apart from pure states, each player \(i\in\mathcal{N}\) may prepare probabilistic mixtures thereof, known as _mixed states_. These mixed states differ from mixed strategies used in classical, finite games as they do not correspond to convex combinations of their pure counterparts; instead, given a family of pure quantum states \(\psi_{i\alpha_{i}}\in\Psi_{i}\) indexed by \(\alpha_{i}\in\mathcal{A}_{i}\), a mixed state is described by a _density matrix_ of the form

\[\mathbf{X}_{i}=\sum\nolimits_{\alpha_{i}\in\mathcal{A}_{i}}x_{i\alpha_{i}}| \psi_{i\alpha_{i}}\rangle\langle\psi_{i\alpha_{i}}|\] (1)

where the _mixing weights_\(x_{i\alpha_{i}}\geq 0\) of each \(\psi_{i\alpha_{i}}\) are normalized so that \(\operatorname{tr}\mathbf{X}_{i}=1\). By Born's rule, this means that the probability of observing \(\omega\in\Omega\) under \(\mathbf{X}=(\mathbf{X}_{1},\mathbf{X}_{2})\) is

\[P_{\omega}(\mathbf{X})=\sum\nolimits_{\alpha_{1}\in\mathcal{A}_{1}}\sum \nolimits_{\alpha_{2}\in\mathcal{A}_{2}}x_{1,\alpha_{1}}x_{2,\alpha_{2}}P_{ \omega}(\psi_{\alpha}).\] (2)

where \(\psi_{\alpha}=\psi_{1,\alpha_{1}}\otimes\psi_{2,\alpha_{2}}\). Therefore, in a slight abuse of notation, the expected payoff of player \(i\in\mathcal{N}\) under \(\mathbf{X}\) will be \(u_{i}(\mathbf{X})=\sum_{\alpha\in\mathcal{A}}x_{\alpha}u_{i}(\psi_{\alpha})\), which, equivalently, can be written as:

\[u_{i}(\mathbf{X})=\sum\nolimits_{\omega\in\Omega}\sum\nolimits_{\alpha\in \mathcal{A}}x_{\alpha}u_{i}(\psi_{\alpha})U_{i}(\omega).\] (3)

This gives a succint representation of the payoff structure of \(\mathcal{Q}\) - see also Eq. (5) below.

Continuous game reformulation.In view of the above, treating a quantum game as a "tensorial" extension of a finite game can be misleading. For our purposes, it would be more suitable to treat a quantum game as a _continuous game_ where each player \(i\in\mathcal{N}\) controls a matrix variable \(\mathbf{X}_{i}\) drawn from the "spectraplex" defined as \(\boldsymbol{\mathcal{X}}_{i}=\{\mathbf{X}_{i}\in\mathbb{H}^{d_{i}}_{t}: \operatorname{tr}\mathbf{X}_{i}=1\}\). In this interpretation, the players' payoff functions \(u_{i}\colon\boldsymbol{\mathcal{X}}\equiv\boldsymbol{\mathcal{X}}_{1}\times \boldsymbol{\mathcal{X}}_{2}\to\mathbb{R}\) are _linear_ in each player's density matrix \(\mathbf{X}_{i}\in\boldsymbol{\mathcal{X}}_{i}\), \(i\in\mathcal{N}\). Since \(u_{1},u_{2}\) are linear in \(\mathbf{X}_{1}\) and \(\mathbf{X}_{2}\), the individual payoff gradients of each player will be given by

\[\mathbf{V}_{1}(\mathbf{X})\coloneqq\nabla_{\mathbf{X}_{1}^{\top}}u_{1}(\mathbf{ X})\quad\text{and}\quad\mathbf{V}_{2}(\mathbf{X})\coloneqq\nabla_{\mathbf{X}_{1}^{ \top}}u_{2}(\mathbf{X})\] (4)

so we can further write each player's payoff function as

\[u_{1}(\mathbf{X})=\operatorname{tr}[\mathbf{X}_{1}\mathbf{V}_{1}(\mathbf{X})] \quad\text{and}\quad u_{2}(\mathbf{X})=\operatorname{tr}[\mathbf{X}_{2}\mathbf{V}_ {2}(\mathbf{X})]\quad\text{for all }\mathbf{X}\in\boldsymbol{\mathcal{X}}.\] (5)Since \(\mathcal{X}\) is compact and each \(u_{i}\) is multilinear in \(\mathbf{X}\), the players' payoff functions are automatically bounded, Lipschitz continuous and Lipschitz smooth, i.e., there exist constants \(B_{i}\), \(G_{i}\) and \(L_{i}\), \(i\in\mathcal{N}\), such that, for all \(\mathbf{X},\mathbf{X}^{\prime}\in\mathcal{X}\), we have:

1. Boundedness: \(|u_{i}(\mathbf{X})|\leq B_{i}\)
2. Lipschitz continuity: \(|u_{i}(\mathbf{X})-u_{i}(\mathbf{X}^{\prime})|\leq G_{i}\|\mathbf{X}- \mathbf{X}^{\prime}\|_{F}\)
3. Lipschitz smoothness: \(\|\mathbf{V}_{i}(\mathbf{X})-\mathbf{V}_{i}(\mathbf{X}^{\prime})\|_{F}\leq L _{i}\|\mathbf{X}-\mathbf{X}^{\prime}\|_{F}\)

**Nash equilibrium.** The most widely used solution concept in game theory is that of a _Nash equilibrium_ (NE). In our context, it is mixed profile \(\mathbf{X}^{*}\in\mathcal{X}\) from which no player has incentive to deviate, i.e., \(u_{1}(\mathbf{X}^{*})\geq u_{1}(\mathbf{X}_{1};\mathbf{X}_{2}^{*})\) and \(u_{2}(\mathbf{X}^{*})\geq u_{2}(\mathbf{X}_{1}^{*};\mathbf{X}_{2})\) for all \(\mathbf{X}_{1}\in\mathcal{X}_{1}\), \(\mathbf{X}_{2}\in\mathcal{X}_{2}\). Since \(\mathcal{X}_{i}\) is convex and \(u_{i}\) linear in \(\mathbf{X}_{i}\), the existence of Nash equilibria follows from the Debreu's theorem [19].

Zero-sum quantum games. In the case where \(u_{1}=-u_{2}\), and setting \(\mathcal{L}\colon\mathcal{X}_{1}\times\mathcal{X}_{2}\to\mathbb{R}\), the Nash equilibria of \(\mathcal{Q}\) are the saddle points of \(\mathcal{L}\), i.e., the solutions of the minimax problem

\[\max_{\mathbf{X}_{1}\in\mathcal{X}_{1}}\min_{\mathbf{X}_{2}\in\mathcal{X}_{2 }}\mathcal{L}(\mathbf{X}_{1},\mathbf{X}_{2})=\min_{\mathbf{X}_{2}\in\mathcal{ X}_{2}}\max_{\mathbf{X}_{1}\in\mathcal{X}_{1}}\mathcal{L}(\mathbf{X}_{1}, \mathbf{X}_{2})\] (6)

By Sion's minimax theorem [54], the set of Nash equilibria is nonempty. Then, given a Nash equilibrium \(\mathbf{X}^{*}\), we define the _duality gap_ of \(\mathbf{X}=(\mathbf{X}_{1},\mathbf{X}_{2})\) as

\[\text{Gap}_{\mathcal{L}}(\mathbf{X})\coloneqq\mathcal{L}(\mathbf{X}_{1}^{*}, \mathbf{X}_{2})-\mathcal{L}(\mathbf{X}_{1},\mathbf{X}_{2}^{*})\] (7)

so \(\text{Gap}_{\mathcal{L}}(\mathbf{X})\geq 0\) with equality if and only if \(\mathbf{X}\) is itself a Nash equilibrium. In particular, \(\mathbf{X}\) is an \(\varepsilon\)-Nash equilibrium of \(\mathcal{Q}\) if and only if \(\text{Gap}_{\mathcal{L}}(\mathbf{X})\leq\varepsilon\).

Other semidefinite games. In addition to quantum games, our framework can also be used for learning in other classes of games with a semidefinite structure as per [26; 45]. As an example, consider the problem of covariance matrix optimization in vector Gaussian multiple-access channels [9; 43; 57; 62]. In this case, there is a finite set of players indexed by \(i\in\mathcal{N}=\{1,\dots,N\}\); each player \(i\in\mathcal{N}\) picks a unit-trace semidefinite matrix \(\mathbf{X}_{i}\in\mathcal{X}_{i}\) and their payoff is given by the Shannon-Telatar capacity formula [57], viz.

\[u_{i}(\mathbf{X}_{1},\dots,\mathbf{X}_{N})=\log\det\bigl{(}\mathbf{I}+\sum_{j }\mathbf{H}_{j}\mathbf{X}_{j}\mathbf{H}_{j}^{\dagger}\bigr{)}\] (8)

where each \(\mathbf{H}_{i}\) is a player-specific gain matrix [58]. Even though \(u_{i}\) is no longer multilinear in \(\mathbf{X}\), the algorithms we derive later in the paper can be applied to this setting essentially verbatim.

## 3 The matrix multiplicative weights algorithm

Throughout the sequel, we will focus on equilibrium learning in quantum - and semidefinite - games. In the context of two-player, zero-sum quantum games, the state-of-the-art method is based on the so-called _matrix multiplicative weights_ (MMW) algorithm [7; 27; 29; 59] which updates as

\[\mathbf{Y}_{i,t+1}=\mathbf{Y}_{i,t}+\gamma_{t}\mathbf{V}_{i}(\mathbf{X}_{t}) \qquad\mathbf{X}_{i,t}=\frac{\exp(\mathbf{Y}_{i,t})}{\text{tr}\bigl{[}\exp( \mathbf{Y}_{i,t})\bigr{]}}\] (MMW)

In the above, \((a)\)\(\mathbf{X}_{t}=(\mathbf{X}_{1,t},\mathbf{X}_{2,t})\) denotes the players' density matrix profile at each stage \(t=1,2,\dots\) of the process; \((b)\)\(\mathbf{V}_{i}(\mathbf{X}_{t})\) is the payoff gradient of player \(i\in\mathcal{N}\) under \(\mathbf{X}_{t}\); \((c)\)\(\mathbf{Y}_{t}\) is an auxiliary state matrix that aggregates gradient steps over time; and \((d)\)\(\gamma_{t}>0\), \(t=1,2,\dots\), is a learning rate (or step-size) parameter that can be freely tuned by the players.

Importantly, as stated, (MMW) requires _full information_ at the player end: specifically, at each stage \(t=1,2,\dots\) of the process, each player \(i\in\mathcal{N}\) must receive their individual payoff gradient \(\mathbf{V}_{i}(\mathbf{X}_{t})\) in order to perform the gradient update step in (MMW). Under this assumption, Jain & Watrous [27] showed that the induced empirical frequency of play

\[\tilde{\mathbf{X}}_{T}=\frac{1}{T}\sum\nolimits_{t=1}^{T}\mathbf{X}_{t}\] (9)

converges to equilibrium at a rate of \(\mathcal{O}(1/\sqrt{T})\) as per the formal result below:

**Theorem 1** (Jain & Watrous [27]).: _Suppose that each player of a \(2\)-player zero-sum game \(\mathcal{Q}\) follows (MMW) for \(T\) epochs with learning rate \(\gamma=G^{-1}\sqrt{2H/T}\) where \(H=\log(d_{1}d_{2})\). Then the players' empirical frequency of play enjoys the bound_

\[\operatorname{Gap}_{\mathcal{L}}(\bar{\mathbf{X}}_{T})\leq G\sqrt{2H/T}\] (10)

_In particular, if (MMW) is run for \(T=\mathcal{O}(1/\varepsilon^{2})\) iterations, \(\bar{\mathbf{X}}_{T}\) will be an \(\varepsilon\)-Nash equilibrium of \(\mathcal{Q}\)._

To the best of our knowledge, this guarantee of Jain & Watrous [27] remains the tightest known bound for Nash equilibrium learning in \(2\)-player zero-sum quantum games. At the same time, Theorem 1 hinges on the players having perfect access to their individual gradients - which, among others, might entail full knowledge of the game, observing the other player's density matrix, etc. Our goal in the sequel will be to relax precisely this assumption and develop a payoff-based variant of (MMW) that can be employed without stringent information and observability requirements as above.

## 4 Matrix learning without matrix feedback

In an online learning framework, it is more realistic to assume that players observe only the _outcome_ of their actions - i.e., their individual payoffs. In this information-starved, payoff-based setting, our main goal will be to employ a _minimal-information matrix multiplicative weights_ (3MW) algorithm that updates as

\[\mathbf{Y}_{i,t+1}=\mathbf{Y}_{i,t}+\gamma_{t}\hat{\mathbf{V}}_{i,t}\qquad \mathbf{X}_{i,t}=\frac{\exp(\mathbf{Y}_{i,t})}{\operatorname{tr}\bigl{[}\exp (\mathbf{Y}_{i,t})\bigr{]}}\] (3MW)

where \(\hat{\mathbf{V}}_{i,t}\) is some payoff-based estimate of the payoff gradient \(\mathbf{V}_{i}(\mathbf{X}_{t})\) of player \(i\) at \(\mathbf{X}_{t}\), and all other quantities are defined as per (MMW). In this regard, the main challenge that arises is how to reconstruct each player's payoff gradient matrices when they are not accessible via an oracle.

### The classical approach: Importance weighted estimators.

In the context of classical, finite games and multi-armed bandits, a standard approach for reconstructing \(\hat{\mathbf{V}}_{i,t}\) is via the so-called _importance weighted estimator_ (IWE) [12, 14, 32]. To state it in the context of finite games, assume that each player has at their disposal a finite set of _pure strategies_\(\alpha_{i}\in\mathcal{A}_{i}\), and if each player plays \(\hat{\alpha}_{i}\in\mathcal{A}_{i}\), then, in obvious notation, their individual payoff will be \(\hat{u}_{i}=u_{i}(\hat{\alpha}_{i};\hat{\alpha}_{-i})\). Then, if each player is using a mixed strategy \(x_{i}\in\Delta(\mathcal{A}_{i})\) to draw their chosen action \(\hat{\alpha}_{i}\), the _importance weighted estimator_ (IWE) for the payoff of the (possibly unplayed) action \(\alpha_{i}\in\mathcal{A}_{i}\) of player \(i\) is defined as

\[\operatorname{IWE}_{i\alpha_{i}}=\frac{\mathds{1}\{\alpha_{i}=\hat{\alpha}_{i }\}}{x_{i\alpha_{i}}}u_{i}(\hat{\alpha}_{i};\hat{\alpha}_{-i})\quad\text{for all $\alpha_{i}\in\mathcal{A}_{i}$}\] (IWE)

with the assumption that \(x_{i}\) has full support, i.e., each action \(\alpha_{i}\in\mathcal{A}_{i}\) has strictly positive probability \(x_{i\alpha_{i}}\) of being chosen by the \(i\)-th player.1

Footnote 1: The assumption that \(x_{i,t}\) has full support is only for technical reasons. In practice, it can be relaxed by using IWE with explicit exploration – see [32] for more details.

This approach has proven extremely fruitful in the context of multi-armed bandits and finite games where (IWE) is an essential ingredient of the optimal algorithms for each context [5, 12, 32, 65]. However, in our case, there are two insurmountable difficulties in extending (IWE) to a quantum context: First and foremost, the quantum regime is characterized by a _continuum_ of pure states with highly correlated payoffs (in the sense that quantum states that are close in the Bloch sphere will have highly correlated POVM payoff observables); this comes in stark contrast to the classical regime of finite normal-form games, where players only have to contend with a finite number of actions (with no prior payoff correlations between them). Secondly, even after the realization of the pure states of the players, there is an inherent uncertainty and randomness due to the quantum measurement process that is involved in the payoff-generating process; as such, the players' payoffs are also affected by an exogenous source of randomness which is altogether absent from (IWE).

Our approach to tackle these issues will be to exploit the reformulation of a quantum game as a continuous game with multilinear payoffs over the spectraplex (or, rather, a product thereof), and use ideas from bandit convex optimization - in the spirit of [21, 31] - to estimate the players' payoff gradients with minimal, scalar information requirements.

### Gradient estimation via finite-difference quotients on the spectraplex.

To provide some intuition for the analysis to come, consider first a single-variable smooth function \(f\colon\mathds{R}\to\mathds{R}\) and a point \(x\in\mathds{R}\). Then, for error tolerance \(\delta>0\), a two-point estimate of the derivative of \(f\) at \(x\) is given by the expression

\[\hat{f}_{x}=\frac{f(x+\delta)-f(x-\delta)}{2\delta}\] (11)

Going to higher dimensions, letting \(f:\mathds{R}^{d}\to\mathds{R}\) be a smooth function, \(\{e_{1},\ldots,e_{d}\}\) be the standard basis of \(\mathds{R}^{d}\) and \(s\) drawn from \(\{e_{1},\ldots,e_{d}\}\) uniformly at random, the estimator

\[\hat{f}_{x}=\frac{d}{2\delta}\big{[}f(x+\delta s)-f(x-\delta s)\big{]}s\] (12)

is a \(\mathcal{O}(\delta)\)-approximation of the gradient, i.e., \(\|\mathds{E}_{s}[\hat{f}_{x}]-\nabla f(x)\|_{F}=\mathcal{O}(\delta)\). This idea is the basis of the Kiefer-Wolfowitzs stochastic approximation scheme [30] and will be the backbone of our work.

Now, to employ this type of estimator for a function over the set of density matrices \(\bm{\mathcal{X}}\) in \(\mathds{H}^{d}\), we need to ensure two things: _(i)_ the feasibility of the _sampling direction_, and _(ii)_ the feasibility of the _evaluation point_. The first caveat is due to the fact that the set of the density matrices forms a lower dimensional manifold in the set of Hermitian operators, and therefore, not all directions from a base of \(\mathds{H}^{d}\) are feasible. The second one is due to the fact that \(\bm{\mathcal{X}}\) is bounded, thus, even if the sampling direction is feasible, the evaluation point can lie outside the set \(\bm{\mathcal{X}}\). We proceed to ensure all this in a series of concrete steps below.

Sampling Directions.We begin with the issue of defining a proper sampling set for the estimator's finite-difference directions. To that end, we will first construct an orthonormal basis of the tangent hull \(\bm{\mathcal{Z}}=\{\mathbf{Z}\in\mathds{H}^{d}:\operatorname{tr}\mathbf{Z}=0\}\) of \(\bm{\mathcal{X}}\), i.e., the subspace of traceless matrices of \(\mathds{H}^{d}\). Note that if \(\mathbf{Z}\in\bm{\mathcal{Z}}\) then for any \(\mathbf{X}\in\mathds{H}^{d}\) it holds \((a)\)\(\mathbf{X}+\mathbf{Z}\in\mathds{H}^{d}\), and \((b)\)\(\operatorname{tr}[\mathbf{X}+\mathbf{Z}]=\operatorname{tr}[\mathbf{X}]\).

Denoting by \(\bm{\Lambda}_{k\ell}\in\mathds{H}^{d}\) the matrix with \(1\) in the \((k,\ell)\)-position and \(0\)'s everywhere else, it is easy to see that the set \(\big{\{}\{\bm{\Lambda}_{jj}\}_{j=1}^{d}\{\mathbf{e}_{k\ell}\}_{k<\ell},\{ \tilde{\mathbf{e}}_{k\ell}\}_{k<\ell}\big{\}}\) is an orthonormal basis of \(\mathds{H}^{d}\), where

\[\mathbf{e}_{k\ell}=\frac{1}{\sqrt{2}}\bm{\Lambda}_{k\ell}+\frac{1}{\sqrt{2}} \bm{\Lambda}_{\ell k}\quad\text{and}\quad\tilde{\mathbf{e}}_{k\ell}=\frac{i}{ \sqrt{2}}\bm{\Lambda}_{k\ell}-\frac{i}{\sqrt{2}}\bm{\Lambda}_{\ell k}\] (13)

for \(1\leq k<\ell\leq d\), where \(i\) is the imaginary unit with \(i^{2}=-1\). The next proposition provides a basis for the subspace \(\bm{\mathcal{Z}}\), whose proof lies in the appendix.

**Proposition 1**.: _Let \(\mathbf{E}_{j}\) be defined as \(\mathbf{E}_{j}=\frac{1}{\sqrt{j(j+1)}}\big{(}\bm{\Lambda}_{11}+\cdots+\bm{ \Lambda}_{jj}-j\bm{\Lambda}_{j+1,j+1}\big{)}\) for \(j=1,\ldots,d-1\). Then, the set \(\mathcal{E}=\Big{\{}\{\mathbf{E}_{j}\}_{j=1}^{d-1},\{\mathbf{e}_{k\ell}\}_{k< \ell},\{\tilde{\mathbf{e}}_{k\ell}\}_{k<\ell}\Big{\}}\) is an orthonormal basis of \(\bm{\mathcal{Z}}\)._

In the sequel, we will use this basis as an orthonormal sampler from which to pick the finite-difference directions for the estimation of \(\mathbf{V}\).

Feasibility Adjustment.After establishing an orthonormal basis for \(\bm{\mathcal{Z}}\) as per Proposition 1, we readily get that for any \(\mathbf{X}\in\bm{\mathcal{X}}\), any \(\mathbf{Z}\in\mathcal{E}^{\pm}\coloneqq\big{\{}\{\pm\mathbf{E}_{j}\}_{j=1}^{d -1},\{\pm\mathbf{e}_{k\ell}\}_{k<\ell},\{\pm\tilde{\mathbf{e}}_{k\ell}\}_{k< \ell}\big{\}}\) and \(\delta>0\), the point \(\mathbf{X}+\delta\,\mathbf{Z}\) belongs to \(\bm{\mathcal{Z}}\). However, depending on the value of the exploration parameter \(\delta\) and the distance of \(\mathbf{X}\) from the boundary of \(\bm{\mathcal{X}}\), the point \(\mathbf{X}+\delta\bm{\mathcal{Z}}\in\mathds{H}^{d}\) may fail to lie in \(\bm{\mathcal{X}}\) due to violation of the positive-semidefinite condition. On that account, we now treat the latter restriction, i.e., the feasibility of the _evaluation point_.

To tackle this, the idea is to transfer the point \(\mathbf{X}\) toward the interior of \(\bm{\mathcal{X}}\) and move along the sampled direction from there. For this, we need to find a reference point \(\mathbf{R}\in\operatorname{ri}(\bm{\mathcal{X}})\) and a "safety net" \(r>0\) such that \(\mathbf{R}+r\mathbf{Z}\in\bm{\mathcal{X}}\) for any \(\mathbf{Z}\in\mathcal{E}^{\pm}\). Then, for \(\delta\in(0,r)\), the point

\[\mathbf{X}^{(\delta)}\coloneqq\mathbf{X}+\frac{\delta}{r}(\mathbf{R}-\mathbf{X})\] (14)

lies in \(\operatorname{ri}(\bm{\mathcal{X}})\), and moving along \(\mathbf{Z}\in\mathcal{E}^{\pm}\), the point \(\mathbf{X}^{(\delta)}+\delta\,\mathbf{Z}=(1-\frac{\delta}{r})\mathbf{X}+\frac{ \delta}{r}(\mathbf{R}+r\mathbf{Z})\) remains in \(\bm{\mathcal{X}}\) as a convex combination of two elements in \(\bm{\mathcal{X}}\). The following proposition provides an exact expression for \(\mathbf{R}\) and \(r\), which we will use next to guarantee the feasibility of the sampled iterates.

**Proposition 2**.: _Let \(\mathbf{R}=\frac{1}{d}\sum_{j=1}^{d}\bm{\Lambda}_{jj}\). Then, for \(r=\min\left\{\frac{1}{\sqrt{d(d-1)}},\frac{\sqrt{2}}{d}\right\}\), it holds that \(\mathbf{R}+r\mathbf{Z}\in\bm{\mathcal{X}}\) for any direction \(\mathbf{Z}\in\mathcal{E}^{\pm}\)._Bandit learning in zero-sum quantum games

With all these in hand, we are now ready to proceed to the presentation of the MMW with limited feedback information. To streamline our presentation, before delving into the more difficult "bandit feedback" case - where each player \(i\in\mathcal{N}\) only observes the realized payoff observable \(U_{i}(\omega)\) - we begin with the simpler case where players observe their mixed payoffs \(u_{i}\) at a given profile \(\mathbf{X}\in\boldsymbol{\mathcal{X}}\).

### Learning with mixed payoff observations.

Our main idea to exploit the observation of mixed payoffs and the finite-difference sampling to the fullest will be to introduce a "coordination phase" where players take a sampling step before updating their state variables and continue playing. In more detail, we will take an approach similar to Bervoets et al. [8] and assume that players alternate between an "exploration" and an "exploitation" update that allows them to sample the landscape of \(\mathcal{L}\) efficiently at each iteration. Concretely, writing \(\mathbf{X}_{t}\) and \(\delta_{t}\) for the players' state profile and sampling radius \(\delta_{t}\) at stage \(t=1,2,\dots\), the sequence of events that we envision proceeds as follows:

**Step 1.**: Draw a sampling direction \(\mathbf{Z}_{t,t}\in\mathcal{E}_{i}\) and \(s_{i,t}\in\{\pm 1\}\) uniformly at random.
**Step 2.**: \((a)\) Play \(\mathbf{X}_{i,t}^{(\delta)}+s_{i,t}\,\delta_{t}\,\mathbf{Z}_{i,t}\) and observe \(u_{i}(\mathbf{X}_{t}^{(\delta)}+s_{t}\delta_{t}\mathbf{Z}_{t})\).

\((b)\) Play \(\mathbf{X}_{i,t}^{(\delta)}-s_{i,t}\,\delta_{t}\,\mathbf{Z}_{i,t}\) and observe \(u_{i}(\mathbf{X}_{t}^{(\delta)}-s_{t}\delta_{t}\mathbf{Z}_{t})\).
**Step 3.**: Approximate \(\mathbf{V}_{i}(\mathbf{X}_{t})\) via the _two-point estimator_ (2PE):

\[\hat{\mathbf{V}}_{i,t}\coloneqq\frac{D_{i}}{2\delta_{t}}\Big{[}u_{i}(\mathbf{ X}_{t}^{(\delta)}+s_{t}\delta_{t}\mathbf{Z}_{t})-u_{i}(\mathbf{X}_{t}^{(\delta)}-s _{t}\delta_{t}\mathbf{Z}_{t})\Big{]}\ s_{i,t}\mathbf{Z}_{i,t}\] (2PE)

where \(D_{i}=d_{i}^{2}-1\) is the dimension of \(\mathds{H}^{d_{i}}\), and \(D\coloneqq\max_{i\in\mathcal{N}}D_{i}\).

The main guarantee of the resulting (3MW) + (2PE) algorithm may then be stated as follows:

**Theorem 2**.: _Suppose that each player of a \(2\)-player zero-sum game \(\mathcal{Q}\) follows_ (3MW) _for \(T\) epochs with learning rate \(\gamma\), sampling radius \(\delta\), and gradient estimates provided by_ (2PE)_. Then the players' empirical frequency of play enjoys the duality gap guarantee_

\[\operatorname{E}\big{[}\operatorname{Gap}_{\mathcal{L}}(\bar{\mathbf{X}}_{T} )\big{]}\leq\frac{H}{\gamma T}+8D^{2}G^{2}\gamma+16DL\delta\] (15)

_where \(H=\log(d_{1}d_{2})\). In particular, for \(\gamma=(DG)^{-1}\sqrt{H/(8T)}\) and \(\delta=(G/L)\sqrt{H/(8T)}\), the players enjoy the equilibrium convergence guarantee_

\[\operatorname{E}\big{[}\operatorname{Gap}_{\mathcal{L}}(\bar{\mathbf{X}}_{T} )\big{]}\leq 8DG\sqrt{2H/T}.\] (16)

Compared to Theorem 1, the convergence rate (16) of Theorem 2 is quite significant because it only differs by a factor which is linear in the dimension of the ambient space and otherwise maintains the same \(\mathcal{O}(\sqrt{T})\) dependence on the algorithm's runtime. In this regard, Theorem 2 shows that the "explore-exploit" sampler underlying (2PE) is essentially as powerful as the full information framework of Jain & Watrous [27] - and this, despite the fact that players no longer require access to the gradient matrix \(\mathbf{V}\) of \(\mathcal{L}\). This echoes a range of previous findings in stochastic convex optimization for the efficiency of two-point samplers [3, 53], a similarity we find particularly surprising given the stark differences between the two settings - non-commutativity, min-max versus min-min landscape. The key ingredients for the equilibrium convergence rate of Theorem 2 are the two technical results below. The first is a feedback-agnostic "energy inequality" which is tied to the update structure of (MMW) and is stated in terms of the quantum relative entropy function

\[D(\mathbf{P},\mathbf{X})=\operatorname{tr}\!\big{[}\mathbf{P}(\log\mathbf{P}- \log\mathbf{X})\big{]}\] (17)

for \(\mathbf{P},\mathbf{X}\in\boldsymbol{\mathcal{X}}\) with \(\mathbf{X}\succ 0\). Concretely, we have the following estimate.

**Lemma 1**.: _Fix some \(\mathbf{P}\in\boldsymbol{\mathcal{X}}\), and let \(\mathbf{X}_{t},\mathbf{X}_{t+1}\) be two successive iterates of_ (3MW)_, without any assumptions for the input sequence \(\hat{\mathbf{V}}_{t}\). We then have_

\[D(\mathbf{P},\mathbf{X}_{t+1})\,\leq D(\mathbf{P},\mathbf{X}_{t})+\gamma_{t} \operatorname{tr}\!\big{[}\hat{\mathbf{V}}_{t}(\mathbf{X}_{t}-\mathbf{P})\big{]} +\frac{\gamma_{t}^{2}}{2}\|\hat{\mathbf{V}}_{t}\|_{F}^{2}.\] (18)

The proof of Lemma 1 follows established techniques in the theory of (MMW), so we defer a detailed discussion to the appendix. The second result that we will need is tailored to the estimator (2PE) and provides a tight estimate of its moments conditioned on the history \(\mathcal{F}_{t}=\mathcal{F}(\mathbf{X}_{1},\dots,\mathbf{X}_{t})\) of \(\mathbf{X}_{t}\).

**Proposition 3**.: _The estimator (2PE) enjoys the conditional bounds_

\[(i)\ \left\|\mathbb{E}[\hat{\mathbf{V}}_{t}\,|\,\mathcal{F}_{t}]-\mathbf{V}( \mathbf{X}_{t})\right\|_{F}\leq 4DL\delta_{t}\quad\text{and}\quad(ii)\ \ \mathbb{E}\big{[}\|\hat{\mathbf{V}}_{t}\|_{F}^{2}\,|\,\mathcal{F}_{t}\big{]} \leq 16D^{2}G^{2}\] (19)

The defining element in Proposition 3 is that even though the estimator (2PE) is biased, its second moment is bounded as \(\mathcal{O}(1)\). This is ultimately due to the multilinearity of the players' payoff functions and plays a pivotal role in showing that the duality gap of \(\tilde{\mathbf{X}}_{t}\) under (3MW) is of the same order as under (MMW), because the bias can be controlled with affecting the variance of the estimator. We provide a detailed proof of Lemma 1, Proposition 3, and Theorem 2 in the appendix.

### Learning with bandit feedback

Despite its strong convergence guarantees, a major limiting factor in the applicability of Theorem 2 is that, in many cases, the game's players may only be able to observe their realized payoff observables \(U_{i}(\omega)\), and their mixed payoffs \(u_{i}(\mathbf{X})\) could be completely inaccessible. In particular, as we described in Section 2, each outcome \(\omega\in\Omega\) of the POVM occurs with probability \(P_{\omega}(\mathbf{X}_{t})\) under the strategy profile \(\mathbf{X}_{t}\). Accordingly, if this is the only information available to the players, they will need to estimate their individual payoff gradients through the single observation of the (random) scalar \(U_{i}(\omega_{t})\in\mathds{R}\). In view of this, and inspired by previous works on payoff-based learning and zeroth-order optimization [8, 9, 11, 25, 49, 50], we will consider the single-point stochastic approximation approach of [21, 55] which unfolds as follows:

**Step 1.**: Each player draws a sampling direction \(\mathbf{Z}_{i,t}\in\mathcal{E}_{i}^{\pm}\) uniformly at random.
**Step 2.**: Each player plays \(\mathbf{X}_{i,t}^{(\delta)}+\delta_{t}\,\mathbf{Z}_{i,t}\).
**Step 3.**: Each player receives \(U_{i}(\omega_{t})\).
**Step 4.**: Each player approximates \(\mathbf{V}_{i}(\mathbf{X}_{t})\) via the the _one-point estimator_ (1PE):

\[\hat{\mathbf{V}}_{i,t}\coloneqq\frac{D_{i}}{\delta_{t}}U_{i}(\omega_{t})\, \mathbf{Z}_{i,t}\] (1PE)

In this case, the players' gradient estimates may be bounded as follows:

**Proposition 4**.: _The estimator (1PE) enjoys the conditional bounds_

\[(i)\ \ \left\|\mathbb{E}[\hat{\mathbf{V}}_{t}\,|\,\mathcal{F}_{t}]-\mathbf{V}( \mathbf{X}_{t})\right\|_{F}\leq 4DL\delta_{t}\quad\text{and}\quad(ii)\ \ \mathbb{E}[\|\hat{\mathbf{V}}_{t}\|_{F}^{2}\,|\,\mathcal{F}_{t}]\leq 4D ^{2}B^{2}/\delta_{t}^{2}.\] (20)

The crucial difference between Propositions 3 and 4 is that the former leads to a gradient estimator with \(\mathcal{O}(1)\) variance and magnitude, whereas the magnitude of the latter is inversely proportional to \(\delta_{t}\); however, since \(\delta_{t}\) in turn controls the _bias_ of the gradient estimator, we must now resolve a bias-variance dilemma, which was absent in the case of (2PE). This leads to the following variant of Theorem 2 with bandit, realization-based feedback:

**Theorem 3**.: _Suppose that each player of a \(2\)-player zero-sum game \(\mathcal{Q}\) follows (3MW) for \(T\) epochs with learning rate \(\gamma\), sampling radius \(\delta\), and gradient estimates provided by (1PE). Then the players' empirical frequency of play enjoys the duality gap guarantee_

\[\mathbb{E}\big{[}\mathrm{Gap}_{\mathcal{L}}(\tilde{\mathbf{X}}_{T})\big{]} \leq\frac{H}{\gamma T}+\frac{2D^{2}B^{2}\gamma}{\delta^{2}}+16DL\delta\] (21)

_where \(H=\log(d_{1}d_{2})\). In particular, for \(\gamma=\big{(}\frac{H}{2T}\big{)}^{3/4}\frac{1}{2D\sqrt{BL}}\) and \(\delta=\big{(}\frac{H}{2T}\big{)}^{1/4}\sqrt{\frac{B}{4L}}\), the players enjoy the equilibrium convergence guarantee:_

\[\mathbb{E}\big{[}\mathrm{Gap}_{\mathcal{L}}(\tilde{\mathbf{X}}_{T})\big{]} \leq\frac{2^{3/4}\,8H^{1/4}D\sqrt{BL}}{T^{1/4}}.\] (22)An important observation here is that the players' equilibrium convergence rate under \((3\text{MW})+(1\text{PE})\) no longer matches the convergence rate of the vanilla MMW algorithm (Theorem 1). The reason for this is the bias-variance trade-off in the estimator (1PE), and is reminiscent of the drop in the rate of regret minimization from \(\mathcal{O}(T^{1/2})\) to \(\mathcal{O}(T^{2/3})\) under (IWE) with bandit feedback and explicit exploration in finite games. A kernel-based approach in the spirit of Bubeck et al. [13] could possibly be used to fill the \(\mathcal{O}(T^{1/4})\) gap between Theorems 1 and 3, but this would come at the cost of a possibly catastrophic dependence on the dimension (which is already quadratic in our setting). This consideration is beyond the scope of our work, but it would constitute an important future direction.

## 6 Bandit learning in \(N\)-player quantum games

We conclude our paper with an examination of the behavior of the MMW algorithm in general, \(N\)-player quantum games. Here, a major difficulty that arises is that, in stark contrast to the min-max case, the set of the game's equilibria can be disconnected, so any convergence result will have to be, by necessity, local. In addition, because general \(N\)-games do not have the amenable profile of a bilinear min-max problem - they are multilinear, multi-objective problems - it will not be possible to obtain any convergence guarantees for the game's empirical frequency of play (since there is no convex structure to exploit). Instead, we will have to focus squarely on the induced trajectory of play, which carries with it a fair share of complications.

Inspired by the very recent work of [38], we will not constrain our focus to a specific class of _games_, but to a specific class of _equilibria_. In particular, we will consider the behavior of MMW-based learning with respect to Nash equilibria \(\mathbf{X}^{*}\in\boldsymbol{\mathcal{X}}\) that satisfy the _variational stability_ condition

\[\text{tr}[\mathbf{V}(\mathbf{X})(\mathbf{X}-\mathbf{X}^{*})]<0\quad\text{for all}\ \mathbf{X}\in\boldsymbol{\mathcal{U}}\backslash\{\mathbf{X}^{*}\}.\] (VS)

This condition can be traced back to [44], and can be seen as a game-theoretic analogue of first-order stationarity in the context of continuous optimization, or as an equilibrium refinement in the spirit of the seminal concept of _evolutionary stability_ in population games [39; 40].2 Importantly, as was shown in [38], variationally stable equilibria are the only equilibria that are asymptotically stable under the _continuous-time_ dynamics of the "follow the regularized leader" (FTRL) class of learning policies, so it stands to reason to ask whether they enjoy a similar convergence landscape in the context of bona fide, discrete-time learning with minimal, payoff-based feedback.

Footnote 2: It should be noted here that, if reduced to the simplex, the stability condition (VS) is exactly equivalently to the variational characterization of evolutionarily stable states due to Taylor [56].

Our final result provides an unambiguously positive answer to this question:3

Footnote 3: Strictly speaking, the algorithms (3MW) and (1PE) have been stated in the context of 2-player games. The extension to \(N\)-player games is straightforward, so we do not present it here; for the details (which hide no subtleties), see the appendix.

**Theorem 4**.: _Fix some tolerance level \(\eta\in(0,1)\) and suppose that the players of an \(N\)-player quantum game follow \((3\text{MW})\) with bandit, realization-based feedback, and surrogate gradients provided by the estimator \((1\text{PE})\) with step-size and sampling radius parameters such that_

\[(i)\ \sum_{t=1}^{\infty}\gamma_{t}=\infty,\quad(ii)\ \sum_{t=1}^{\infty} \gamma_{t}\delta_{t}<\infty,\quad\text{and}\quad(ii)\ \sum_{t=1}^{\infty}\gamma_{t}^{2}/\delta_{t}^{2}<\infty.\] (23)

_If \(\mathbf{X}^{*}\) is variationally stable, there exists a neighborhood \(\mathcal{U}\) of \(\mathbf{X}^{*}\) such that_

\[\mathbb{P}(\lim_{t\to\infty}\mathbf{X}_{t}=\mathbf{X}^{*})\geq 1-\eta\quad \text{whenever}\ \mathbf{X}_{1}\in\mathcal{U}.\] (24)

It is worth noting that the last-iterate convergence guarantee of Theorem 4 is considerably stronger than the time-averaged variants of Theorems 1-3, and we are not aware of any comparable convergence guarantee for general quantum games. [Trivially, last-iterate convergence implies time-averaged convergence, but the converse, of course, may fail to hold] As such, especially in cases that require to track the trajectory of the system or the players' day-to-day rewards, Theorem 4 provides an important guarantee for the realized sequence of events.

On the other hand, in contrast to Theorem 4, it should be noted that the guarantees of Theorems 1-3 are global. Given that general quantum games may in general possess a large number of disjoint Nash equilibria, this transition from global to local convergence guarantees seems unavoidable. It is, however, an open question whether (VS) could be exploited further in order to deduce the rate of convergence to such equilibria; we leave this as a direction for future research.

## 7 Numerical Experiments

In this last section, we provide numerical simulations to validate and explore the performance of (MMW) with payoff-based feedback. Additional experiments can be found in Appendix E.

Game setup.Our testbed is a two-player zero-sum quantum game, which is the quantum analogue of a \(2\times 2\) min-max game with actions \(\{\alpha_{1},\alpha_{2}\}\) and \(\{\beta_{1},\beta_{2}\}\), and payoff matrix

\[P=\begin{pmatrix}(4,-4)&(2,-2)\\ (-4,4)&(-2,2)\end{pmatrix}\] (25)

In the quantum regime, the payoff information of the quantum game is encoded in the Hermitian matrices \(\mathbf{W}_{1}=\text{diag}(4,2,-4,-2)\), and \(\mathbf{W}_{2}=-\mathbf{W}_{1}\) as per Eq. (3) in Section 2. By elementary considerations, the action profile \((\alpha_{1},\beta_{2})\) is a strict Nash equilibrium of the classical zero-sum game, which corresponds to the pure quantum state with density matrix profile \(\mathbf{X}^{*}=(\mathbf{X}_{1}^{*},\mathbf{X}_{2}^{*})\) where \(\mathbf{X}_{1}^{*}=e_{1}\otimes e_{1}\) and \(\mathbf{X}_{2}^{*}=e_{2}\otimes e_{2}\) in the standard basis in which \(\mathbf{W}_{1}\) and \(\mathbf{W}_{2}\) are diagonal.

Convergence speed analysis.In Fig. 1, we evaluate the convergence properties of (3MW) using the estimators (2PE) and (1PE), and compare it with the full information variant (MMW). For each method, we perform 10 different runs, with \(T=10^{5}\) steps each, and compute the mean value of the duality gap as a function of the iteration \(t=1,2,\ldots,T\). The solid lines correspond to the mean values of the duality gap of each method, and the shaded regions enclose the area of \(\pm 1\) (sample) standard deviation among the 10 different runs. Note that the red line, which corresponds to the full information (MMW), does not have a shaded region, since there is no randomness in the algorithm. All the runs for the three different methods were initialized for \(\mathbf{Y}=0\) and we used \(\gamma=10^{-2}\) for all methods. In particular, for (3MW) with gradient estimates given by (2PE) estimator, we used a sampling radius \(\delta=10^{-2}\), and for (3MW) with (1PE) estimator, we used \(\delta=10^{-1}\) (in tune with our theoretical results which suggest the use of a tighter sampling radius when mixed payoff information is available to the players).

Figure 1 has several important take-aways. First and foremost, as is to be expected, the payoff-based methods lag behind the full-information variant of (MMW); however, what is particularly surprising is that the drop in performance is singularly mild. As we see in the second plot in Fig. 1, the various algorithms achieved a rate of convergence closer to \(\mathcal{O}(1/T)\), which is significantly faster than \(\mathcal{O}(1/\sqrt{T})\) and/or \(\mathcal{O}(1/T^{1/4})\). This suggests that, in practice, the bandit variants of (MMW) may yield excellent performance benefits, despite the high degree of uncertainty incurred by the complete lack of information on the game being played.

Figure 1: Performance evaluation of the (3MW) with the (2PE) and (1PE) estimators and comparison with the full information (MMW). The solid lines correspond to the mean values of the duality gap of each method, and the shaded regions enclose the area of \(\pm 1\) (sample) standard deviation among the different runs.

## Acknowledgments and Disclosure of Funding

This work has been partially supported by the Air Force Office of Scientific Research under award number FA9550-20-1-0397. Additional support is gratefully acknowledged from NSF 1915967, 2118199, 2229012, 2312204. KL is grateful for financial support by the Onassis Foundation (F ZR 033-1/2021-2022). PM is also with the Archimedes Research Unit - Athena RC - University of Athens, and gratefully acknowledges financial support by project MIS 5154714 of the National Recovery and Resilience Plan Greece 2.0 funded by the European Union under the NextGenerationEU Program, and by the French National Research Agency (ANR) in the framework of the "Investissements d'avenir" program (ANR-15-IDEX-02), the LabEx PERSYVAL (ANR-11-LABX-0025-01), MIAI@Grenoble Alpes (ANR-19-P3IA-0003). NB was supported by the Koret Foundation via the Digital Living 2030 project.

## References

* [1] Aaronson, S., Shadow tomography of quantum states. _SIAM Journal on Computing_, 49(5), January 2020.
* [2] Aaronson, S., Chen, X., Hazan, E., Kale, S., and Nayak, A. Online learning of quantum states. In _NeurIPS '18: Proceedings of the 32nd International Conference of Neural Information Processing Systems_, 2018.
* [3] Agarwal, A., Dekel, O., and Xiao, L. Optimal algorithms for online convex optimization with multi-point bandit feedback. In _COLT '10: Proceedings of the 23rd Annual Conference on Learning Theory_, 2010.
* [4] Arute, F., Arya, K., Babbush, R., Bacon, D., Bardin, J. C., Barends, R., Biswas, R., Boixo, S., Brandao, F. G., Buell, D. A., Burkett, B., Chen, Y., Chen, Z., Chiaro, B., Collins, R., Courtney, W., Dunsworth, A., Farhi, E., Foxen, B., Fowler, A., Gidney, C., Giustina, M., Graff, R., Guerin, K., Habegger, S., Harrigan, M. P., Hartmann, M. J., Ho, A., Hoffmann, M. R., Huang, T., Humble, T. S., Isakov, S. V., Jeffrey, E., Jiang, Z., Kafri, D., Kechedzhi, K., Kelly, J., Klimov, P. V., Knysh, S., Korotkov, A. N., Kostritsa, F., Landhuis, D., Lindmark, M., Lucero, E., Lyakh, D., Mandra, S., McClean, J. R., McEwen, M., Megrant, A., Mi, X., Michielsen, K., Mohseni, M., Mutus, J., Naaman, O., Neeley, M., Neill, C., Niu, M. Y., Ostby, E., Petukhov, A., Platt, J. C., Quintana, C., Rieffel, E. G., Roushan, P., Rubin, N. C., Sank, D., Satzinger, K. J., Smelyanskiy, V., Sung, K. J., Trevithick, M. D., Vainsencher, A., Villalonga, B., White, T., Yao, Z. J., Yeh, P., Zalcman, A., Neven, H., and Martinis, J. M. Quantum supremacy using a programmable superconducting processor. _Nature_, 2019.
* [5] Audibert, J.-Y. and Bubeck, S. Regret bounds and minimax policies under partial monitoring. _Journal of Machine Learning Research_, 11:2635-2686, 2010.
* [6] Auer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R. E. Gambling in a rigged casino: The adversarial multi-armed bandit problem. In _Proceedings of the 36th Annual Symposium on Foundations of Computer Science_, 1995.
* [7] Beck, A. and Teboulle, M. Mirror descent and nonlinear projected subgradient methods for convex optimization. _Operations Research Letters_, 31(3):167-175, 2003.
* [8] Bervoets, S., Bravo, M., and Faure, M. Learning with minimal information in continuous games. _Theoretical Economics_, 15:1471-1508, 2020.
* [9] Bilenne, O., Mertikopoulos, P., and Belmega, E. V. Fast optimization with zeroth-order feedback in distributed multi-user MIMO systems. _IEEE Trans. Signal Process._, 68:6085-6100, October 2020.
* [10] Brandao, F. G. S. L. and Svore, K. Quantum speed-ups for semidefinite programming, 2017.
* [11] Bravo, M., Leslie, D. S., and Mertikopoulos, P. Bandit learning in concave \(N\)-person games. In _NeurIPS '18: Proceedings of the 32nd International Conference of Neural Information Processing Systems_, 2018.
* [12] Bubeck, S. and Cesa-Bianchi, N. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. _Foundations and Trends in Machine Learning_, 5(1):1-122, 2012.
* [13] Bubeck, S., Lee, Y. T., and Eldan, R. Kernel-based methods for bandit convex optimization. In _STOC '17: Proceedings of the 49th annual ACM SIGACT symposium on the Theory of Computing_, 2017.
* [14] Cesa-Bianchi, N. and Lugosi, G. _Prediction, Learning, and Games_. Cambridge University Press, 2006.
* [15] Chakrabarti, S., Huang, Y., Li, T., Feizi, S., and Wu, X. Quantum Wasserstein generative adversarial networks. In _NeurIPS '19: Proceedings of the 33rd International Conference on Neural Information Processing Systems_, 2019.
* [16] Chen, X., Hazan, E., Li, T., Lu, Z., Wang, X., and Yang, R. Adaptive online learning of quantum states, 2022.
* [17] Chuang, I. and Nielsen, M. _Quantum Computation and Quantum Information_. Cambridge University Press, 2 edition, 2010.

* [18] Dallaire-Demers, P.-L. and Killoran, N. Quantum generative adversarial networks. _Physical Review A_, 98(1):012324, 2018.
* [19] Debreu, G. A social equilibrium existence theorem. _Proceedings of the National Academy of Sciences of the USA_, October 1952.
* [20] Eisert, J., Wilkens, M., and Lewenstein, M. Quantum games and quantum strategies. _Physical Review Letters_, 83, October 1999.
* [21] Flaxman, A. D., Kalai, A. T., and McMahan, H. B. Online convex optimization in the bandit setting: gradient descent without a gradient. In _SODA '05: Proceedings of the 16th annual ACM-SIAM Symposium on Discrete Algorithms_, pp. 385-394, 2005.
* [22] Gao, M., Ji, Z., Li, T., and Wang, Q. Logarithmic-regret quantum learning algorithms for zero-sum games, 2023.
* [23] Gutoski, G. and Watrous, J. Toward a general theory of quantum games. In _STOC '07: Proceedings of the 39th annual ACM symposium on the Theory of Computing_, 2007.
* [24] Hall, P. and Heyde, C. C. _Martingale Limit Theory and Its Application_. Probability and Mathematical Statistics. Academic Press, New York, 1980.
* [25] Heliou, A., Mertikopoulos, P., and Zhou, Z. Gradient-free online learning in continuous games with delayed rewards. In _ICML '20: Proceedings of the 37th International Conference on Machine Learning_, 2020.
* [26] Ickstadt, C., Theobald, T., and Tsigaridas, E. Semidefinite games. https://arxiv.org/pdf/2202.12035, 2022.
* [27] Jain, R. and Watrous, J. Parallel approximation of non-interactive zero-sum quantum games. In _CCC '09: Proceedings of the 2009 IEEE International Conference on Computational Complexity_, 2009.
* [28] Jain, R., Piliouras, G., and Sim, R. Matrix multiplicative weights updates in quantum games: Conservation law & recurrence. In _NeurIPS '22: Proceedings of the 36th International Conference on Neural Information Processing Systems_, 2022.
* [29] Kakade, S. M., Shalev-Shwartz, S., and Tewari, A. Regularization techniques for learning with matrices. _The Journal of Machine Learning Research_, 13:1865-1890, 2012.
* [30] Kiefer, J. and Wolfowitz, J. Stochastic estimation of the maximum of a regression function. _The Annals of Mathematical Statistics_, 23(3):462-466, 1952.
* [31] Kleinberg, R. D. Nearly tight bounds for the continuum-armed bandit problem. In _NIPS '04: Proceedings of the 18th Annual Conference on Neural Information Processing Systems_, 2004.
* [32] Lattimore, T. and Szepesvari, C. _Bandit Algorithms_. Cambridge University Press, Cambridge, UK, 2020.
* [33] Li, T., Chakrabarti, S., and Wu, X. Sublinear quantum algorithms for training linear and kernel-based classifiers. In _International Conference on Machine Learning_, 2019.
* [34] Li, T., Wang, C., Chakrabarti, S., and Wu, X. Sublinear classical and quantum algorithms for general matrix games. _Proceedings of the AAAI Conference on Artificial Intelligence_, 35(10), May 2021. doi: 10.1609/aaai.v35i10.17028.
* [35] Lin, W., Piliouras, G., Sim, R., and Varvitsiotis, A. Quantum potential games, replicator dynamics, and the separability problem, 2023.
* [36] Littlestone, N. and Warmuth, M. K. The weighted majority algorithm. _Information and Computation_, 108(2):212-261, 1994.
* [37] Lloyd, S. and Weedbrook, C. Quantum generative adversarial learning. _Physical Review Letters_, 121(4), July 2018.
* [38] Lotidis, K., Mertikopoulos, P., and Bambos, N. Learning in quantum games, 2023.
* [39] Maynard Smith, J. _Evolution and the Theory of Games_. Cambridge University Press, Cambridge, 1982.
* [40] Maynard Smith, J. and Price, G. R. The logic of animal conflict. _Nature_, 246:15-18, November 1973.
* [41] Mertikopoulos, P. Strange bedfellows: Riemann, Gibbs and vector Gaussian multiple access channels. In _NetGCOop '12: Proceedings of the 2012 International Conference on Network Games, Control and Optimization_, 2012.
* [42] Mertikopoulos, P. and Moustakas, A. L. Entropy-driven optimization dynamics for Gaussian vector multiple access channels. In _ICC '13: Proceedings of the 2013 IEEE International Conference on Communications_, 2013.
* [43] Mertikopoulos, P. and Moustakas, A. L. Learning in an uncertain world: MIMO covariance matrix optimization with imperfect feedback. _IEEE Trans. Signal Process._, 64(1), January 2016.
* [44] Mertikopoulos, P. and Zhou, Z. Learning in games with continuous action sets and unknown payoff functions. _Mathematical Programming_, 173(1-2):465-507, January 2019.
* [45] Mertikopoulos, P., Belmega, E. V., Negrel, R., and Sanguinetti, L. Distributed stochastic optimization via matrix exponential learning. _IEEE Trans. Signal Process._, 65(9):2277-2290, May 2017.

* [46] Mertikopoulos, P., Hsieh, Y.-P., and Cevher, V. A unified stochastic approximation framework for learning in games. _Mathematical Programming_, forthcoming, 2023.
* [47] Nemirovski, A. S. and Yudin, D. B. _Problem Complexity and Method Efficiency in Optimization_. Wiley, New York, NY, 1983.
* [48] Nielsen, M. A. and Chuang, I. L. _Quantum Computation and Quantum Information: 10th Anniversary Edition_. Cambridge University Press, 2010. doi: 10.1017/CBO9780511976667.
* [49] Perkins, S. and Leslie, D. S. Stochastic fictitious play with continuous action sets. _Journal of Economic Theory_, 152:179-213, July 2014.
* [50] Perkins, S., Mertikopoulos, P., and Leslie, D. S. Mixed-strategy learning with continuous action sets. _IEEE Trans. Autom. Control_, 62(1):379-384, January 2017.
* [51] Preskill, J. Quantum computing in the NISQ era and beyond. _Quantum_, 2:79, August 2018.
* [52] Robbins, H. and Siegmund, D. A convergence theorem for nonnegative almost supermartingales and some applications. In Rustagi, J. S. (ed.), _Optimizing Methods in Statistics_. Academic Press, New York, NY, 1971.
* [53] Shamir, O. An optimal algorithm for bandit and zero-order convex optimization with two-point feedback. _Journal of Machine Learning Research_, 18(52):1-11, 2017.
* 176, 1958.
* [55] Spall, J. C. Multivariate stochastic approximation using a simultaneous perturbation gradient approximation. _IEEE Trans. Autom. Control_, 37(3):332-341, March 1992.
* [56] Taylor, P. D. Evolutionarily stable strategies with two types of player. _Journal of Applied Probability_, 16 (1):76-83, March 1979.
* [57] Telatar, I. E. Capacity of multi-antenna Gaussian channels. _European Transactions on Telecommunications and Related Technologies_, 10(6):585-596, 1999.
* [58] Tse, D. and Viswanath, P. _Fundamentals of Wireless Communication_. Cambridge University Press, Cambridge, UK, 2005.
* [59] Tsuda, K., Ratsch, G., and Warmuth, M. K. Matrix exponentiated gradient updates for on-line Bregman projection. _Journal of Machine Learning Research_, 6:995-1018, 2005.
* [60] van Apeldoorn, J. and Gily'en, A. Quantum algorithms for zero-sum games. _arXiv: Quantum Physics_, 2019.
* [61] Vovk, V. G. Aggregating strategies. In _COLT '90: Proceedings of the 3rd Workshop on Computational Learning Theory_, pp. 371-383, 1990.
* [62] Yu, W., Rhee, W., Boyd, S. P., and Cioffi, J. M. Iterative water-filling for Gaussian vector multiple-access channels. _IEEE Trans. Inf. Theory_, 50(1):145-152, 2004.
* [63] Yu, Y.-L. The strong convexity of von Neumann's entropy. 2015.
* [64] Zhong, H.-S., Wang, H., Deng, Y.-H., Chen, M.-C., Peng, L.-C., Luo, Y.-H., Qin, J., Wu, D., Ding, X., Hu, Y., Hu, P., Yang, X.-Y., Zhang, W.-J., Li, H., Li, Y., Jiang, X., Gan, L., Yang, G., You, L., Wang, Z., Li, L., Liu, N.-L., Lu, C.-Y., and Pan, J.-W. Quantum computational advantage using photons. _Science_, 370 (6523):1460-1463, December 2020.
* [65] Zimmert, J. and Seldin, Y. Tsallis-INF: An optimal algorithm for stochastic and adversarial bandits. _Journal of Machine Learning Research_, 22(28):1-49, 2021.

## Appendix

In the series of technical appendices that follow, we provide the missing proofs from the main part of our paper, and we provide some numerical illustrations of the performance of the proposed algorithms. As a roadmap, we begin in Appendix A with some auxiliary results that are required throughout our analysis. Subsequently, in Appendices B-D, we provide the proofs of the results presented in Sections 4-6 respectively. Finally, in Appendix E, we provide a suite of numerical experiments to assess the practical performance of (3MW) using the estimators (2PE) and (1PE), and we compare it with the full information setting underlying (MMW).

## Appendix A Auxiliary Results

We now introduce some notation for quantum games in a \(N\)-player setting, and explain how the extension from the 2-player setting is straightforward.

\(N\)**-player quantum games.** First of all, a quantum game \(\mathcal{Q}\) consists of a finite set of players \(i\in\mathcal{N}=\{1,\ldots,N\}\), where each player \(i\in\mathcal{N}\) has access to a complex Hilbert space \(\mathcal{H}_{i}\cong\mathbb{C}^{d_{i}}\). The set of pure states is the unit sphere \(\Psi_{i}:=\{\psi_{i}\in\mathcal{H}_{i}:\|\psi_{i}\|_{F}=1\}\) of \(\mathcal{H}_{i}\). We will write \(\Psi\coloneqq\prod_{i}\Psi_{i}\) for the space of all ensembles \(\psi=(\psi_{1},\ldots,\psi_{N})\) of pure states \(\psi_{i}\in\Psi_{i}\) that are independently prepared by each \(i\in\mathcal{N}\).

In analogy with the 2-player case, each outcome \(\omega\in\Omega\) is associated to a positive semi-definite operator \(\mathbf{P}_{\omega}\colon\mathcal{H}\to\mathcal{H}\) defined on the tensor product \(\mathcal{H}\coloneqq\otimes_{i}\mathcal{H}_{i}\) of the players' individual state spaces; we further assume that \(\sum_{\omega\in\Omega}\mathbf{P}_{\omega}=\mathbf{I}\), thus, the probability of observing \(\omega\in\Omega\) at state \(\psi\in\Psi\) is

\[P_{\omega}(\psi)=\langle\psi_{1}\otimes\cdots\otimes\psi_{N}|\mathbf{P}_{ \omega}|\psi_{1}\otimes\cdots\otimes\psi_{N}\rangle\] (A.1)

and, the player's expected payoff at state \(\psi\in\Psi\) will be

\[u_{i}(\psi)\coloneqq\langle U_{i}\rangle\equiv\sum_{\omega}P_{\omega}(\psi)\, U_{i}(\omega)\] (A.2)

Similarly to the 2-player setting, if each player \(i\in\mathcal{N}\) prepares a density matrix \(\mathbf{X}_{i}\) as per (1), the expected payoff of player \(i\in\mathcal{N}\) under \(\mathbf{X}=(\mathbf{X}_{1},\ldots,\mathbf{X}_{N})\) will be

\[u_{i}(\mathbf{X})=\sum_{\omega\in\Omega}U_{i}(\omega)\operatorname{tr}[ \mathbf{P}_{\omega}\mathbf{X}_{1}\otimes\cdots\otimes\mathbf{X}_{N}]= \operatorname{tr}[\mathbf{W}_{i}\,\mathbf{X}_{1}\otimes\cdots\otimes\mathbf{ X}_{N}]\] (A.3)

where \(\mathbf{W}_{i}=\sum_{\omega\in\Omega}U_{i}(\omega)\mathbf{P}_{\omega}\in \mathcal{H}\) for \(i\in\mathcal{N}\). Finally, we denote by \(\mathbf{V}_{i}(\mathbf{X})\) the individual payoff gradient of player \(i\) under \(\mathbf{X}\) as

\[\mathbf{V}_{i}(\mathbf{X})\coloneqq\nabla_{\mathbf{X}_{i}^{\top}}u_{i}( \mathbf{X})\] (A.4)

All other notions are extended, accordingly. \(\diamondsuit\)

As noted in Section 2, we define the norm \(\|A\|_{F}=\sqrt{\operatorname{tr}[A^{\dagger}A]}\) for any \(A\in\mathbb{H}^{d_{i}}\), i.e., \((\mathbb{H}^{d_{i}},\|\cdot\|_{F})\) is an inner-product space. With a slight abuse of notation, we define for \(\mathbf{X}=(\mathbf{X}_{1},\ldots,\mathbf{X}_{N})\in\boldsymbol{\mathcal{X}}\) its norm as:

\[\|\mathbf{X}\|_{F}=\sqrt{\sum_{i=1}^{N}\|\mathbf{X}_{i}\|_{F}^{2}}\] (A.5)

**Lemma A.1**.: _For any \(\mathbf{X}_{i}\in\boldsymbol{\mathcal{X}}_{i}\), it holds \(\|\mathbf{X}_{i}\|_{F}\leq 1\), and \(\operatorname{diam}(\boldsymbol{\mathcal{X}})=2\sqrt{N}\)._

Proof.: For the first part, since \(\mathbf{X}_{i}\in\boldsymbol{\mathcal{X}}_{i}\), it admits an orthonormal decomposition \(Q\Lambda Q^{\dagger}\) such that \(QQ^{\dagger}=Q^{\dagger}Q=\mathbf{I}\) and \(\Lambda=\operatorname{diag}(\lambda_{1},\ldots,\lambda_{d_{i}})\) with \(\sum_{j=1}^{d_{i}}\lambda_{j}=1\), and \(\lambda_{j}\geq 0\) for all \(j\). Hence

\[\|\mathbf{X}_{i}\|_{F}^{2}=\operatorname{tr}[\mathbf{X}_{i}^{\dagger}\mathbf{ X}_{i}]=\operatorname{tr}[Q\Lambda Q^{\dagger}Q\Lambda Q^{\dagger}]= \operatorname{tr}[Q\Lambda^{2}Q^{\dagger}]=\sum_{j=1}^{d_{i}}\lambda_{i}^{2} \leq\sum_{j=1}^{d_{i}}\lambda_{i}=1\] (A.6)

where the last inequality holds, since \(0\leq\lambda_{j}\leq 1\), and the result follows.

For the second part, letting \(\mathbf{X}=(\mathbf{X}_{1},\ldots,\mathbf{X}_{N})\) and \(\mathbf{X}^{\prime}=(\mathbf{X}^{\prime}_{1},\ldots,\mathbf{X}^{\prime}_{N})\) be two points in \(\mathcal{X}\), we have

\[\|\mathbf{X}-\mathbf{X}^{\prime}\|_{F}=\sqrt{\sum_{i=1}^{N}\|\mathbf{X}_{i}- \mathbf{X}^{\prime}_{i}\|_{F}^{2}}\leq\sqrt{\sum_{i=1}^{N}(2\|\mathbf{X}_{i} \|_{F}^{2}+2\|\mathbf{X}^{\prime}_{i}\|_{F}^{2})}\leq 2\sqrt{N}\] (A.7)

and since the equality is attained, we get the result. 

Our next result concerns the _quantum relative entropy_

\[D(\mathbf{P},\mathbf{X})=\sum_{i=1}^{N}D_{i}(\mathbf{P}_{i},\mathbf{X}_{i})\] (A.8)

where \(\mathbf{P}=(\mathbf{P}_{1},\ldots,\mathbf{P}_{N})\in\mathcal{X}\) and \(\mathbf{X}=(\mathbf{X}_{1},\ldots,\mathbf{X}_{N})\in\operatorname{ri}( \mathcal{X})\) and

\[D_{i}(\mathbf{P}_{i},\mathbf{X}_{i})\coloneqq\operatorname{tr}[\mathbf{P}_{i }(\log\mathbf{P}_{i}-\log\mathbf{X}_{i})]\] (A.9)

The lemma we will require is a semidefinite version of Pinsker's inequality which reads as follows:

**Lemma A.2**.: _For all \(\mathbf{P}\in\mathcal{X}\) and \(\mathbf{X}\in\operatorname{ri}(\mathcal{X})\) we have_

\[D(\mathbf{P},\mathbf{X})\geq\frac{1}{2}\|\mathbf{P}-\mathbf{X}\|_{F}^{2}\] (A.10)

Proof.: Focusing on player \(i\in\mathcal{N}\), we will show first that

\[D_{i}(\mathbf{P}_{i},\mathbf{X}_{i})\geq\frac{1}{2}\|\mathbf{P}_{i}-\mathbf{X }_{i}\|_{F}^{2}\] (A.11)

for all \(\mathbf{P}=(\mathbf{P}_{1},\ldots,\mathbf{P}_{N})\in\mathcal{X}\) and \(\mathbf{X}=(\mathbf{X}_{1},\ldots,\mathbf{X}_{N})\in\operatorname{ri}( \mathcal{X})\).

To this end, we define the function \(h_{i}\colon\mathbf{H}_{+}^{d_{i}}\to\mathbb{R}\) as \(h_{i}(\mathbf{X}_{i})=\operatorname{tr}[\mathbf{X}_{i}\log\mathbf{X}_{i}]\), which is \(1\)-strongly convex with respect to the nuclear norm \(\|\cdot\|_{1}\)[63], and since \(\|\mathbf{X}_{i}\|_{1}\geq\|\mathbf{X}_{i}\|_{F}\) for all \(\mathbf{X}_{i}\in\mathcal{X}_{i}\), we readily get that \(h_{i}\) is \(1\)-strongly convex with respect to the Frobenius norm, as well.

Letting \(\nabla h_{i}(\mathbf{X}_{i})=\log\mathbf{X}_{i}+\mathbf{I}\), by \(1\)-strong convexity, we have for \(\mathbf{P}=(\mathbf{P}_{1},\ldots,\mathbf{P}_{N})\in\mathcal{X}\) and \(\mathbf{X}=(\mathbf{X}_{1},\ldots,\mathbf{X}_{N})\in\operatorname{ri}( \mathcal{X})\):

\[h_{i}(\mathbf{P}_{i}) \geq h_{i}(\mathbf{X}_{i})+\operatorname{tr}[\nabla h_{i}( \mathbf{X}_{i})(\mathbf{P}_{i}-\mathbf{X}_{i})]+\frac{1}{2}\|\mathbf{P}_{i}- \mathbf{X}_{i}\|_{F}^{2}\] \[=\operatorname{tr}[\mathbf{X}_{i}\log\mathbf{X}_{i}]+\operatorname {tr}[(\mathbf{P}_{i}-\mathbf{X}_{i})\log\mathbf{X}_{i}]+\operatorname{tr}[ \mathbf{P}_{i}-\mathbf{X}_{i}]+\frac{1}{2}\|\mathbf{P}_{i}-\mathbf{X}_{i}\|_{F} ^{2}\] \[=\operatorname{tr}[\mathbf{P}_{i}\log\mathbf{X}_{i}]+\frac{1}{2} \|\mathbf{P}_{i}-\mathbf{X}_{i}\|_{F}^{2}\] (A.12)

where we used that \(\operatorname{tr}[\mathbf{P}_{i}-\mathbf{X}_{i}]=0\). Hence, by reordering, we automatically get that

\[D_{i}(\mathbf{P}_{i},\mathbf{X}_{i})\geq\frac{1}{2}\|\mathbf{P}_{i}-\mathbf{X }_{i}\|_{F}^{2}\] (A.13)

Therefore, we have:

\[D(\mathbf{P},\mathbf{X})\geq\frac{1}{2}\sum_{i=1}^{N}\|\mathbf{P}_{i}-\mathbf{ X}_{i}\|_{F}^{2}=\frac{1}{2}\|\mathbf{P}-\mathbf{X}\|_{F}^{2}\] (A.14)

and the proof is completed. 

## Appendix B Omitted proofs from Section 4

In this appendix, we develop the basic scaffolding required for the estimators (2PE) and (1PE). We begin with the construction of the estimators' sampling basis, as encoded in Proposition 1, which we restate below for convenience:

**Proposition 1**.: _Let \(\mathbf{E}_{j}\) be defined as \(\mathbf{E}_{j}=\frac{1}{\sqrt{j(j+1)}}\big{(}\mathbf{\Lambda}_{11}+\cdots+ \mathbf{\Lambda}_{jj}-j\mathbf{\Lambda}_{j+1,j+1}\big{)}\) for \(j=1,\ldots,d-1\). Then, the set \(\mathcal{E}=\left\{\{\mathbf{E}_{j}\}_{j=1}^{d-1},\{\mathbf{e}_{k\ell}\}_{k< \ell},\{\mathbf{\tilde{e}}_{k\ell}\}_{k<\ell}\right\}\) is an orthonormal basis of \(\mathcal{Z}\)._

Proof.: First of all, note that

\[\mathbf{\Lambda}_{k\ell}\mathbf{\Delta}_{mn}=\begin{cases}0&\text{if }\ell\neq m\\ \mathbf{\Lambda}_{kn}&\text{if }\ell=m\end{cases}\] (B.1)

**Unit norm.** To begin with, we will show that all elements in \(\mathcal{E}\) have unit norm. Indeed, we have:

* For \(j=1,\ldots,d-1\), we have: \[\left\|\mathbf{E}_{j}\right\|_{F}^{2}=\mathrm{tr}[\mathbf{E}_{j}^{ \dagger}\mathbf{E}_{j}] =\frac{1}{j(j+1)}\,\mathrm{tr}\left[\left(\sum_{k=1}^{j}\mathbf{ \Delta}_{kk}-j\mathbf{\Delta}_{(j+1)\,(j+1)}\right)\left(\sum_{k=1}^{j}\mathbf{ \Delta}_{kk}-j\mathbf{\Delta}_{(j+1)\,(j+1)}\right)\right]\] \[=\frac{1}{j(j+1)}\,\mathrm{tr}\left[\left(\sum_{k=1}^{j}\mathbf{ \Delta}_{kk}+j^{2}\mathbf{\Delta}_{(j+1)\,(j+1)}\right)\right]=\frac{1}{j(j+1) }\,(j+j^{2})=1\] (B.2)
* For \(k<\ell\), we have: \[\left\|\mathbf{e}_{k\ell}\right\|_{F}^{2}=\mathrm{tr}[\mathbf{e}_{ k\ell}^{\dagger}\mathbf{e}_{k\ell}] =\mathrm{tr}\left[\left(\frac{1}{\sqrt{2}}\mathbf{\Delta}_{\ell k }+\frac{1}{\sqrt{2}}\mathbf{\Delta}_{k\ell}\right)\left(\frac{i}{\sqrt{2}} \mathbf{\Delta}_{k\ell}+\frac{1}{\sqrt{2}}\mathbf{\Delta}_{\ell k}\right)\right]\] \[=\mathrm{tr}\left[\frac{1}{2}\mathbf{\Delta}_{kk}+\frac{1}{2} \mathbf{\Delta}_{\ell\ell}\right]=\frac{1}{2}+\frac{1}{2}=1\] (B.3)
* For \(k<\ell\), we also have: \[\left\|\tilde{\mathbf{e}}_{k\ell}\right\|_{F}^{2}=\mathrm{tr}[ \tilde{\mathbf{e}}_{k\ell}^{\dagger}\tilde{\mathbf{e}}_{k\ell}] =\mathrm{tr}\left[\left(-\frac{i}{\sqrt{2}}\mathbf{\Delta}_{\ell k }+\frac{i}{\sqrt{2}}\mathbf{\Delta}_{k\ell}\right)\left(\frac{i}{\sqrt{2}} \mathbf{\Delta}_{k\ell}-\frac{i}{\sqrt{2}}\mathbf{\Delta}_{\ell k}\right)\right]\] \[=\mathrm{tr}\left[\frac{1}{2}\mathbf{\Delta}_{kk}+\frac{1}{2} \mathbf{\Delta}_{\ell\ell}\right]=\frac{1}{2}+\frac{1}{2}=1\] (B.4)

**Orthogonality.** Now, we will show that any two elements of \(\mathcal{E}\) are orthogonal to each other.

* For \(m<n\), we have: \[\mathrm{tr}[\mathbf{E}_{m}^{\dagger}\mathbf{E}_{n}] =\frac{1}{\sqrt{m(m+1)}\sqrt{n(n+1)}}\,\mathrm{tr}\left[\left( \sum_{k=1}^{m}\mathbf{\Delta}_{kk}-m\mathbf{\Delta}_{(m+1)\,(m+1)}\right) \left(\sum_{k=1}^{n}\mathbf{\Delta}_{kk}-n\mathbf{\Delta}_{(n+1)\,(n+1)} \right)\right]\] \[=\frac{1}{\sqrt{m(m+1)}\sqrt{n(n+1)}}\,\mathrm{tr}\left[\left( \sum_{k=1}^{m}\mathbf{\Delta}_{kk}-m\mathbf{\Delta}_{(m+1)\,(m+1)}\right) \right]\] \[=\frac{1}{\sqrt{m(m+1)}\sqrt{n(n+1)}}\,(m-m)=0\] (B.5)
* For \(k<\ell\), we have: \[\mathrm{tr}[\mathbf{e}_{k\ell}^{\dagger}\tilde{\mathbf{e}}_{k\ell}] =\mathrm{tr}\left[\left(\frac{1}{\sqrt{2}}\mathbf{\Delta}_{\ell k }+\frac{1}{\sqrt{2}}\mathbf{\Delta}_{k\ell}\right)\left(\frac{i}{\sqrt{2}} \mathbf{\Delta}_{k\ell}-\frac{i}{\sqrt{2}}\mathbf{\Delta}_{\ell k}\right)\right]\] \[=\mathrm{tr}\left[\frac{i}{2}\mathbf{\Delta}_{\ell\ell}-\frac{i }{2}\mathbf{\Delta}_{kk}\right]=\frac{i}{2}-\frac{i}{2}=0\] (B.6)
* For \((k,\ell)\neq(m,n)\) with \(k<\ell\) and \(m<n\), we have: \[\mathrm{tr}\left[\mathbf{e}_{k\ell}^{\dagger}\mathbf{e}_{mn}\right]=\mathrm{ tr}\left[\mathbf{e}_{k\ell}^{\dagger}\tilde{\mathbf{e}}_{mn}\right]=\mathrm{tr} \left[\tilde{\mathbf{e}}_{k\ell}^{\dagger}\tilde{\mathbf{e}}_{mn}\right]=0\] (B.7) since all the nonzero terms in \(\mathbf{e}_{k\ell}^{\dagger}\mathbf{e}_{mn},\mathbf{e}_{k\ell}^{\dagger} \tilde{\mathbf{e}}_{mn}\) and \(\tilde{\mathbf{e}}_{k\ell}^{\dagger}\tilde{\mathbf{e}}_{mn}\) are of the form \(c\cdot\mathbf{\Delta}_{\alpha\beta}\) for some \(c\in\mathbb{C}\), and \(\alpha,\beta\in\{k,\ell,m,n\}\) with \(\alpha\neq\beta\). Thus, \(\mathrm{tr}\left[c\cdot\mathbf{\Delta}_{\alpha\beta}\right]=0\), since all the diagonal elements are equal to \(0\). Note that it is not possible to have \(\alpha=\beta\) because this would imply that \((k,\ell)=(m,n)\).
* For \(k<\ell\) and \(j=1,\ldots,d-1\), we have: \[\mathrm{tr}\left[\mathbf{e}_{k\ell}^{\dagger}\mathbf{E}_{j}\right]=\mathrm{tr} \left[\tilde{\mathbf{e}}_{k\ell}^{\dagger}\mathbf{E}_{j}\right]=0\] (B.8) since the non-zero terms of both \(\mathbf{e}_{k\ell}^{\dagger}\mathbf{E}_{j}\) and \(\tilde{\mathbf{e}}_{k\ell}^{\dagger}\mathbf{E}_{j}\) are of the form \(\mathbf{\Delta}_{kn},\mathbf{\Delta}_{\ell m}\) for \(k\neq n\) and \(\ell\neq m\).

We thus conclude that any two elements of \(\mathcal{E}\) are orthogonal.

Finally, it is clear \(\mathcal{E}\subseteq\mathrm{aff}(\bm{\mathcal{X}}_{0})\), since \(\mathcal{E}\subseteq\mathbb{H}^{d}\) and \(\mathrm{tr}[\mathbf{e}_{k\ell}]=\mathrm{tr}[\tilde{\mathbf{e}}_{k\ell}]= \mathrm{tr}[\mathbf{E}_{j}]=0\), for \(k<\ell\) and \(j=1,\ldots,d-1\). Therefore, the elements in \(\mathcal{E}\) form an orthonormal basis of \(\mathrm{aff}(\bm{\mathcal{X}}_{0})\) and \(\mathrm{dim}(\mathrm{aff}(\bm{\mathcal{X}}_{0}))=d^{2}-1\). 

We now proceed with the construction of the precise "safety net" that guarantees that the sampling perturbation of the gradient estimator remains within the problem's feasible region. Again, for convenience, we restate the relevant result below:

**Proposition 2**.: _Let \(\mathbf{R}=\frac{1}{d}\sum_{j=1}^{d}\Delta_{jj}\). Then, for \(r=\min\left\{\frac{1}{\sqrt{d(d-1)}},\frac{\sqrt{2}}{d}\right\}\), it holds that \(\mathbf{R}+r\mathbf{Z}\in\bm{\mathcal{X}}\) for any direction \(\mathbf{Z}\in\mathcal{E}^{\pm}\)._

Proof.: To begin with, it is clear that \(\mathbf{R}\in\mathbb{H}^{d}\) and \(\mathrm{tr}[\mathbf{R}]=\sum_{j=1}^{d}1/d=1\). Moreover, for any \(u\in\mathbb{C}^{d}\setminus\{0\}\), we have:

\[u^{\dagger}\mathbf{R}u=\frac{1}{d}\sum_{j=1}^{d}\lvert u_{j}\rvert^{2}>0\] (B.9)

where \(\lvert u_{j}\rvert\) is the modulus of the complex number \(u_{j}\in\mathbf{C}\). Therefore, \(\mathbf{R}\) is positive definite, i.e., lies in \(\mathrm{ri}(\bm{\mathcal{X}})\).

Now, we need to find \(r>0\) such that

\[\mathbf{R}+r\mathbf{Z}\in\bm{\mathcal{X}}\] (B.10)

for any \(\mathbf{Z}\in\mathcal{E}^{\pm}\).

It is clear that for any \(\mathbf{Z}\in\mathcal{E}^{\pm}\), we have \(\mathrm{tr}[\mathbf{R}+r\mathbf{Z}]=\mathrm{tr}[\mathbf{R}]=1\), since \(\mathrm{tr}[\mathbf{Z}]=0\). Hence, it remains to consider the positive semi-definite constraint. For this, we will use the following identities, for \(k<\ell\):

\[u^{\dagger}(\mathbf{\Lambda}_{k\ell}+\mathbf{\Lambda}_{\ell k})u=\bar{u}_{k}u_ {\ell}+\bar{u}_{\ell}u_{k}=2\mathrm{Re}(\bar{u}_{k}u_{\ell})\] (B.11)

and

\[u^{\dagger}(i\mathbf{\Lambda}_{k\ell}-i\mathbf{\Lambda}_{\ell k})u=i(\bar{u}_ {k}u_{\ell}-\bar{u}_{\ell}u_{k})=-2\,\mathrm{Im}(\bar{u}_{k}u_{\ell})\] (B.12)

* For \(\mathbf{Z}=\frac{1}{\sqrt{2}}(\mathbf{\Lambda}_{k\ell}+\mathbf{\Lambda}_{\ell k})\) and \(u\in\mathbb{C}^{d}\setminus\{0\}\), and using (B.11), we have: \[u^{\dagger}(\mathbf{R}+r\mathbf{Z})u =\frac{1}{d}\sum_{j=1}^{d}\lvert u_{j}\rvert^{2}+\frac{r}{\sqrt{2 }}2\mathrm{Re}(\bar{u}_{k}u_{\ell})\] \[=\frac{1}{d}\sum_{j\neq k,\ell}\lvert u_{j}\rvert^{2}+\frac{1}{d }\left(\lvert u_{k}\rvert^{2}+\lvert u_{\ell}\rvert^{2}+\frac{rd}{\sqrt{2}}2 \mathrm{Re}(\bar{u}_{k}u_{\ell})\right)\] (B.13) If \(\mathrm{Re}(\bar{u}_{k}u_{\ell})>0\), we get that \(u^{\dagger}(\mathbf{R}+r\mathbf{Z})u>0\), while if \(\mathrm{Re}(\bar{u}_{k}u_{\ell})\leq 0\) and \(r\leq\sqrt{2}/d\): \[u^{\dagger}(\mathbf{R}+r\mathbf{Z})u \geq\frac{1}{d}\sum_{j\neq k,\ell}\lvert u_{j}\rvert^{2}+\frac{1}{d }\lvert u_{k}+u_{\ell}\rvert^{2}\geq 0\] (B.14) Hence, for \(r\leq\sqrt{2}/d\), and \(\mathbf{Z}=\frac{1}{\sqrt{2}}(\mathbf{\Lambda}_{k\ell}+\mathbf{\Lambda}_{\ell k})\), we have that \(u^{\dagger}(\mathbf{R}+r\mathbf{Z})u\geq 0\) for all \(u\in\mathbb{C}^{d}\).
* For \(\mathbf{Z}=-\frac{1}{\sqrt{2}}(\mathbf{\Lambda}_{k\ell}+\mathbf{\Lambda}_{\ell k})\), we have \[u^{\dagger}(\mathbf{R}+r\mathbf{Z})u =\frac{1}{d}\sum_{j=1}^{d}\lvert u_{j}\rvert^{2}-\frac{r}{\sqrt{ 2}}2\mathrm{Re}(\bar{u}_{k}u_{\ell})\] \[=\frac{1}{d}\sum_{j\neq k,\ell}\lvert u_{j}\rvert^{2}+\frac{1}{d }\left(\lvert u_{k}\rvert^{2}+\lvert u_{\ell}\rvert^{2}-\frac{rd}{\sqrt{2}}2 \mathrm{Re}(\bar{u}_{k}u_{\ell})\right)\] (B.15) If \(\mathrm{Re}(\bar{u}_{k}u_{\ell})<0\), we get that \(u^{\dagger}(\mathbf{R}+r\mathbf{Z})u>0\), while if \(\mathrm{Re}(\bar{u}_{k}u_{\ell})\geq 0\) and \(r\leq\sqrt{2}/d\): \[u^{\dagger}(\mathbf{R}+r\mathbf{Z})u \geq\frac{1}{d}\sum_{j\neq k,\ell}\lvert u_{j}\rvert^{2}+\frac{1}{d }\lvert u_{k}-u_{\ell}\rvert^{2}\geq 0\] (B.16) Hence, for \(r\leq\sqrt{2}/d\), and \(\mathbf{Z}=-\frac{1}{\sqrt{2}}(\mathbf{\Lambda}_{k\ell}+\mathbf{\Lambda}_{\ell k})\), we have that \(u^{\dagger}(\mathbf{R}+r\mathbf{Z})u\geq 0\) for all \(u\in\mathbb{C}^{d}\).

* For \(\mathbf{Z}=\frac{i}{\sqrt{2}}(\mathbf{\Lambda}_{k\ell}-\mathbf{\Lambda}_{\ell k})\) and \(u\in\mathbf{C}^{d}\setminus\{0\}\), and using (B.11), we have: \[u^{\dagger}(\mathbf{R}+r\mathbf{Z})u =\frac{1}{d}\sum_{j=1}^{d}\lvert u_{j}\rvert^{2}-\frac{r}{\sqrt{ 2}}2\mathrm{Im}(\bar{u}_{k}u_{\ell})\] \[=\frac{1}{d}\sum_{j\neq k,\ell}\lvert u_{j}\rvert^{2}+\frac{1}{ d}\left(\lvert u_{k}\rvert^{2}+\lvert u_{\ell}\rvert^{2}-\frac{rd}{\sqrt{2}}2 \mathrm{Im}(\bar{u}_{k}u_{\ell})\right)\] (B.17) If \(\mathrm{Im}(\bar{u}_{k}u_{\ell})<0\), we get that \(u^{\dagger}(\mathbf{R}+r\mathbf{Z})u>0\), while if \(\mathrm{Im}(\bar{u}_{k}u_{\ell})\geq 0\) and \(r\leq\sqrt{2}/d\): \[u^{\dagger}(\mathbf{R}+r\mathbf{Z})u \geq\frac{1}{d}\sum_{j\neq k,\ell}\lvert u_{j}\rvert^{2}+\frac{1}{d} \lvert u_{k}+i\,u_{\ell}\rvert^{2}\geq 0\] (B.18) Hence, for \(r\leq\sqrt{2}/2d\), and \(\mathbf{Z}=\frac{i}{\sqrt{2}}(\mathbf{\Lambda}_{k\ell}-\mathbf{\Lambda}_{\ell k})\), we have that \(u^{\dagger}(\mathbf{R}+r\mathbf{Z})u\geq 0\) for all \(u\in\mathbf{C}^{d}\).
* For \(\mathbf{Z}=-\frac{i}{\sqrt{2}}(\mathbf{\Lambda}_{k\ell}-\mathbf{\Lambda}_{\ell k})\) and \(u\in\mathbf{C}^{d}\setminus\{0\}\), and using (B.11), we have: \[u^{\dagger}(\mathbf{R}+r\mathbf{Z})u =\frac{1}{d}\sum_{j=1}^{d}\lvert u_{j}\rvert^{2}+\frac{r}{\sqrt{ 2}}2\mathrm{Im}(\bar{u}_{k}u_{\ell})\] \[=\frac{1}{d}\sum_{j\neq k,\ell}\lvert u_{j}\rvert^{2}+\frac{1}{d} \left(\lvert u_{k}\rvert^{2}+\lvert u_{\ell}\rvert^{2}+\frac{rd}{\sqrt{2}}2 \mathrm{Im}(\bar{u}_{k}u_{\ell})\right)\] (B.19) If \(\mathrm{Im}(\bar{u}_{k}u_{\ell})>0\), we get that \(u^{\dagger}(\mathbf{R}+r\mathbf{Z})u>0\), while if \(\mathrm{Im}(\bar{u}_{k}u_{\ell})\leq 0\) and \(r\leq\sqrt{2}/d\): \[u^{\dagger}(\mathbf{R}+r\mathbf{Z})u \geq\frac{1}{d}\sum_{j\neq k,\ell}\lvert u_{j}\rvert^{2}+\frac{1}{d} \lvert u_{k}-i\,u_{\ell}\rvert^{2}\] (B.20) Hence, for \(r\leq\sqrt{2}/2d\), and \(\mathbf{Z}=-\frac{i}{\sqrt{2}}(\mathbf{\Lambda}_{k\ell}-\mathbf{\Lambda}_{\ell k})\), we have that \(u^{\dagger}(\mathbf{R}+r\mathbf{Z})u\geq 0\) for all \(u\in\mathbf{C}^{d}\).
* For \(\mathbf{Z}=\frac{1}{\sqrt{j(j+1)}}\big{(}\mathbf{\Lambda}_{11}+\cdots+\mathbf{ \Lambda}_{jj}-j\mathbf{\Lambda}_{(j+1)(j+1)}\big{)}\), we have: \[u^{\dagger}(\mathbf{R}+r\mathbf{Z})u =\frac{1}{d}\sum_{k=1}^{d}\lvert u_{k}\rvert^{2}+\frac{r}{\sqrt{ j(j+1)}}\sum_{k=1}^{j}\lvert u_{k}\rvert^{2}-\frac{jr}{\sqrt{j(j+1)}} \lvert u_{j+1}\rvert^{2}\] \[=\frac{1}{d}\sum_{k\neq j+1}\lvert u_{k}\rvert^{2}+\frac{r}{ \sqrt{j(j+1)}}\sum_{k=1}^{j}\lvert u_{k}\rvert^{2}+\left(\frac{1}{d}-\frac{jr} {\sqrt{j(j+1)}}\right)\lvert u_{j+1}\rvert^{2}\] (B.21) Thus, we need to ensure that \[\frac{1}{d}-\frac{jr}{\sqrt{j(j+1)}}\geq 0\] (B.22) for all \(j=1,\ldots,d-1\). Because the function \(x\mapsto\sqrt{x(x+1)}/x\) is decreasing, it obtains the smallest value from \(x=d-1\). Therefore, for \(r\leq 1/\sqrt{d(d-1)}\), we readily obtain that \(u^{\dagger}(\mathbf{R}+r\mathbf{Z})u\geq 0\) for all \(u\in\mathbf{C}^{d}\).
* For \(\mathbf{Z}=-\frac{1}{\sqrt{j(j+1)}}\big{(}\mathbf{\Lambda}_{11}+\cdots+ \mathbf{\Lambda}_{jj}-j\mathbf{\Lambda}_{(j+1)(j+1)}\big{)}\), we have: \[u^{\dagger}(\mathbf{R}+r\mathbf{Z})u =\frac{1}{d}\sum_{k=1}^{d}\lvert u_{k}\rvert^{2}-\frac{r}{\sqrt{ j(j+1)}}\sum_{k=1}^{j}\lvert u_{k}\rvert^{2}+\frac{jr}{\sqrt{j(j+1)}} \lvert u_{j+1}\rvert^{2}\] \[=\left(\frac{1}{d}-\frac{r}{\sqrt{j(j+1)}}\right)\sum_{k=1}^{j} \lvert u_{k}\rvert^{2}+\frac{1}{d}\sum_{k=j+1}^{d}\lvert u_{k}\rvert^{2}+\frac{ jr}{\sqrt{j(j+1)}}\lvert u_{j+1}\rvert^{2}\] (B.23) Thus, we need to ensure that \[\frac{1}{d}-\frac{r}{\sqrt{j(j+1)}}\geq 0\] (B.24)for all \(j=1,\ldots,d-1\). Because it holds that

\[\frac{1}{d}-\frac{r}{\sqrt{j(j+1)}}\geq\frac{1}{d}-\frac{jr}{\sqrt{j(j+1)}}\] (B.25)

we obtain the inequality for free by the previous case, i.e., for \(r\leq 1/\sqrt{d(d-1)}\).

Therefore, for

\[r=\min\left\{\frac{1}{\sqrt{d(d-1)}},\frac{\sqrt{2}}{d}\right\}\] (B.26)

we readily obtain that \(u^{\dagger}(\mathbf{R}+r\mathbf{Z})u\geq 0\) for all \(u\in\mathbf{C}^{d}\), and our proof is complete. 

## Appendix C Omitted proofs from Section 5

Our aim in this appendix will be to prove the basic guarantees of (3MW) with payoff-based feedback. The structure of this appendix shadows that of Section 5 and is broken into two parts, depending on the specific type of input available to the players. The only point of departure is the energy inequality of Lemma 1, which is common to both algorithms, and which we restate and prove below:

**Lemma 1**.: _Fix some \(\mathbf{P}\in\boldsymbol{\mathcal{X}}\), and let \(\mathbf{X}_{t},\mathbf{X}_{t+1}\) be two successive iterates of (3MW), without any assumptions for the input sequence \(\hat{\mathbf{V}}_{t}\). We then have_

\[D(\mathbf{P},\mathbf{X}_{t+1})\leq D(\mathbf{P},\mathbf{X}_{t})+\gamma_{t} \operatorname{tr}\left[\hat{\mathbf{V}}_{t}(\mathbf{X}_{t}-\mathbf{P}) \right]+\frac{\gamma_{t}^{2}}{2}\|\hat{\mathbf{V}}_{t}\|_{F}^{2}.\] (18)

Proof.: By the definition of \(D\), it is easy to see that for \(\mathbf{P}\in\boldsymbol{\mathcal{X}}\) and \(\mathbf{X},\mathbf{X}^{\prime}\in\operatorname{ri}(\boldsymbol{\mathcal{X}})\), we have

\[D(\mathbf{P},\mathbf{X}^{\prime})=D(\mathbf{P},\mathbf{X})+D(\mathbf{X}, \mathbf{X}^{\prime})+\operatorname{tr}\left[(\log\mathbf{X}^{\prime}-\log \mathbf{X})(\mathbf{X}-\mathbf{P})\right]\] (C.1)

Since \(\nabla h(\mathbf{X})=\log\mathbf{X}+\mathbf{I}\), the above equality can be written as:

\[D(\mathbf{P},\mathbf{X}^{\prime})=D(\mathbf{P},\mathbf{X})+D(\mathbf{X}, \mathbf{X}^{\prime})+\operatorname{tr}\left[(\nabla h(\mathbf{X}^{\prime})- \nabla h(\mathbf{X}))(\mathbf{X}-\mathbf{P})\right]\] (C.2)

Setting \(\mathbf{X}\) as \(\mathbf{X}_{t+1}\), and \(\mathbf{X}^{\prime}\) as \(\mathbf{X}_{t}\), and invoking the easily verifiable fact that \(\nabla h(\mathbf{X}_{t+1})-\nabla h(\mathbf{X}_{t})=\gamma_{t}\hat{\mathbf{V} }_{t}\), we get

\[D(\mathbf{P},\mathbf{X}_{t})=D(\mathbf{P},\mathbf{X}_{t+1})+D(\mathbf{X}_{t+ 1},\mathbf{X}_{t})-\gamma_{t}\operatorname{tr}\left[\hat{\mathbf{V}}_{t}( \mathbf{X}_{t+1}-\mathbf{P})\right]\] (C.3)

and hence:

\[D(\mathbf{P},\mathbf{X}_{t+1}) =D(\mathbf{P},\mathbf{X}_{t})-D(\mathbf{X}_{t+1},\mathbf{X}_{t}) +\gamma_{t}\operatorname{tr}\left[\hat{\mathbf{V}}_{t}(\mathbf{X}_{t+1}- \mathbf{P})\right]\] \[\leq D(\mathbf{P},\mathbf{X}_{t})-\frac{1}{2}\|\mathbf{X}_{t+1}- \mathbf{X}_{t}\|_{F}^{2}+\gamma_{t}\operatorname{tr}\left[\hat{\mathbf{V}}_{t} (\mathbf{X}_{t+1}-\mathbf{P})\right]\] \[=D(\mathbf{P},\mathbf{X}_{t})-\frac{1}{2}\|\mathbf{X}_{t+1}- \mathbf{X}_{t}\|_{F}^{2}+\gamma_{t}\operatorname{tr}\left[\hat{\mathbf{V}}_{t }(\mathbf{X}_{t}-\mathbf{P})\right]+\gamma_{t}\operatorname{tr}\left[\hat{ \mathbf{V}}_{t}(\mathbf{X}_{t+1}-\mathbf{X}_{t})\right]\] \[\leq D(\mathbf{P},\mathbf{X}_{t})+\gamma_{t}\operatorname{tr} \left[\hat{\mathbf{V}}_{t}(\mathbf{X}_{t}-\mathbf{P})\right]+\frac{\gamma_{t }^{2}}{2}\|\hat{\mathbf{V}}_{t}\|_{F}^{2}\] (C.4)

where the first inequality holds due to Lemma A.2, and in the last step we used that \(\|\cdot\|_{F}\) is an inner-product norm on \(\boldsymbol{\mathcal{X}}\), so

\[\frac{1}{2}\|\mathbf{X}_{t+1}-\mathbf{X}_{t}\|_{F}^{2}+\frac{\gamma_{t}^{2}}{ 2}\|\hat{\mathbf{V}}_{t}\|_{F}^{2}\geq\gamma_{t}\operatorname{tr}\left[\hat{ \mathbf{V}}_{t}(\mathbf{X}_{t+1}-\mathbf{X}_{t})\right]\] (C.5)

This concludes our proof. 

With this template inequality in hand, we proceed with the guarantees of (3MW) in the next sections.

### Learning with mixed payoff observations

We begin with the statistics of the 2-point sampler (2PE), which we restate below:

**Proposition 3**.: _The estimator (2PE) enjoys the conditional bounds_

\[(i)\ \left\|\mathbb{E}[\hat{\mathbf{V}}_{t}\left|\ \mathcal{F}_{t}\right.]- \mathbf{V}(\mathbf{X}_{t})\right\|_{F}\leq 4DL\delta_{t}\quad\text{and}\quad( ii)\ \ \ \mathbb{E}\big{[}\|\hat{\mathbf{V}}_{t}\|_{F}^{2}\ \big{|}\ \mathcal{F}_{t}\big{]}\leq 16D^{2}G^{2}\] (19)

Proof.: We prove each part separately.

* Let \(\Xi_{i,t}^{(+)}\) and \(\Xi_{i,t}^{(-)}\) be defined for all players \(i\in\{1,2\}\) as \[\Xi_{i,t}^{(+)} =(\mathbf{X}_{i,t}^{(\delta)}+s_{i,t}\delta_{t}\mathbf{Z}_{i,t})- \mathbf{X}_{i,t}\] \[=s_{i,t}\delta_{t}\mathbf{Z}_{i,t}+\frac{\delta_{t}}{r_{i}}( \mathbf{R}_{i}-\mathbf{X}_{i,t})=\delta_{t}\left[s_{i,t}\mathbf{Z}_{i,t}+ \frac{1}{r_{i}}(\mathbf{R}_{i}-\mathbf{X}_{i,t})\right]\] (C.6) and \[\Xi_{i,t}^{(-)} =(\mathbf{X}_{i,t}^{(\delta)}-s_{i,t}\delta_{t}\mathbf{Z}_{i,t})- \mathbf{X}_{i,t}=\delta_{t}\left[-s_{i,t}\mathbf{Z}_{i,t}+\frac{1}{r_{i}}( \mathbf{R}_{i}-\mathbf{X}_{i,t})\right]\] (C.7) Taking a first-order Taylor expansion of \(u_{i}\), we obtain: \[u_{i}(\mathbf{X}_{t}^{(\delta)}+s_{t}\delta_{t}\mathbf{Z}_{t}) =u_{i}(\mathbf{X}_{t})+\sum_{j\in\mathcal{N}}\text{tr}\left[ \nabla_{\mathbf{X}_{j}^{\prime}}u_{i}(\mathbf{X}_{t})^{\dagger}\Xi_{j,t}^{(+ )}\right]+R_{2}(\Xi_{i}^{(+)})\] (C.8a) and \[u_{i}(\mathbf{X}_{t}^{(\delta)}-s_{t}\delta_{t}\mathbf{Z}_{t}) =u_{i}(\mathbf{X}_{t})+\sum_{j\in\mathcal{N}}\text{tr}\left[ \nabla_{\mathbf{X}_{j}^{\prime}}u_{i}(\mathbf{X}_{t})^{\dagger}\Xi_{j,t}^{(- )}\right]+R_{2}(\Xi_{t}^{(-)})\] (C.8b) where \(R_{2}(\cdot)\) is the 2nd order Taylor remainder. Now, for \(j\neq i\in\mathcal{N}\), since \(s_{i,t}\) is zero-mean and independent of any other process: \[\mathbb{E}\Big{[}\text{tr}\left[\nabla_{\mathbf{X}_{j}^{\prime}}u_{i}( \mathbf{X}_{t})^{\dagger}(\Xi_{j,t}^{(+)}-\Xi_{j,t}^{(-)})\right]s_{i,t} \mathbf{Z}_{i,t}\ \Big{|}\ \mathcal{F}_{t}\Big{]}=0\] (C.9) and using that \(\Xi_{i,t}^{(+)}-\Xi_{i,t}^{(-)}=2s_{i,t}\delta_{t}\mathbf{Z}_{i,t}\), we have: \[\mathbb{E}\Big{[}\text{tr}\left[\nabla_{\mathbf{X}_{i}^{\prime}}u _{i}(\mathbf{X}_{t})^{\dagger}(\Xi_{i,t}^{(+)}-\Xi_{i,t}^{(-)})\right]s_{i,t} \mathbf{Z}_{i,t}\ \Big{|}\ \mathcal{F}_{t}\Big{]} =\mathbb{E}\big{[}\text{tr}\big{[}\mathbf{V}_{i}(\mathbf{X}_{t})^{ \dagger}(2s_{i,t}\delta_{t}\mathbf{Z}_{i,t})\big{]}s_{i,t}^{2}\mathbf{Z}_{i,t }\ \big{|}\ \mathcal{F}_{t}\big{]}\] \[=2\delta_{t}\ \mathbb{E}\Big{[}\text{tr}\big{[}\mathbf{V}_{i}( \mathbf{X}_{t})^{\dagger}\mathbf{Z}_{i,t}\big{]}\mathbf{Z}_{i,t}\ \big{|}\ \mathcal{F}_{t}\Big{]}\] \[=\frac{2\delta_{t}}{D_{i}}\sum_{W\in\delta_{t}}\text{tr}\big{[} \mathbf{V}_{i}(\mathbf{X}_{t})^{\dagger}W\big{]}W\] \[=\frac{2\delta_{t}}{D_{i}}\ \text{proj}_{\xi_{t}}(\mathbf{V}_{i}(\mathbf{X}_{t}))=\frac{2\delta_{t}}{D_{ i}}\mathbf{V}_{i}(\mathbf{X}_{t})\] (C.10) where in the last step, with a slight abuse of notation, we identify \(\text{proj}_{\xi_{t}}(\mathbf{V}_{i}(\mathbf{X}_{t}))\) with \(\mathbf{V}_{i}(\mathbf{X}_{t})\). The reason for this is that we apply the differential operator \(\mathbf{V}_{i}(\mathbf{X}_{t})\) only on elements of \(\mathcal{X}_{i}\), and thus, we can ignore the component of \(\mathbf{V}_{i}(\mathbf{X}_{t})\) that is orthogonal to \(\text{span}(\mathcal{E}_{i})\). Moreover, we have that \[|R_{2}(\Xi_{t}^{(+)})|\leq\frac{L}{2}\|\Xi_{t}^{(+)}\|_{F}^{2} \leq L\delta_{t}^{2}\] (C.11) and similarly, we get the same bound for \(|R_{2}(\Xi_{t}^{(-)})|\). Therefore, in light of the above, we obtain the bound: \[\|\mathbb{E}[\hat{\mathbf{V}}_{i,t}\ |\ \mathcal{F}_{t}]-\mathbf{V}_{i}( \mathbf{X}_{t})\|_{F}\leq\frac{1}{2}D_{i}L\delta_{t}\] (C.12a) and, hence \[\|\mathbb{E}[\hat{\mathbf{V}}_{t}\ |\ \mathcal{F}_{t}]-\mathbf{V}( \mathbf{X}_{t})\|_{F}\leq\frac{\sqrt{2}}{2}DL\delta_{t}\] (C.12b)_._
2. _By the definition of_ \(\hat{\mathbf{V}}_{i,t}\)_, we have:_ \[\|\hat{\mathbf{V}}_{i,t}\|_{F} =\frac{D_{i}}{2\delta_{t}}\|u_{i}(\mathbf{X}_{t}^{(\delta)}+s_{t} \;\delta_{t}\;\mathbf{Z}_{t})-u_{i}(\mathbf{X}_{t}^{(\delta)}-s_{t}\;\delta_{t} \;\mathbf{Z}_{t})\Big{\|}\|s_{i,t}\mathbf{Z}_{i,t}\|_{F}\] \[\leq\frac{D_{i}}{2\delta_{t}}G\|2s_{t}\;\delta_{t}\;\mathbf{Z}_{t} \|_{F}\leq\sqrt{2}D_{i}G\] (C.13) _and therefore, we readily obtain that:_ \[\mathbf{E}\big{[}\|\hat{\mathbf{V}}_{i,t}\|_{F}^{2}\,\big{|}\,\mathcal{F}_{t} \big{]}\leq 2D_{i}^{2}G^{2}\] (C.14) _so_ \[\mathbf{E}\big{[}\|\hat{\mathbf{V}}_{t}\|_{F}^{2}\,\big{|}\,\mathcal{F}_{t} \big{]}\leq 4D^{2}G^{2}\] (C.15) _and our proof is complete._

With all these technical elements in place, we are finally in a position to prove our convergence result for (3MW) run with \(2\)-point gradient estimators. As before, we restate our result below for convenience:

**Theorem 2**.: _Suppose that each player of a \(2\)-player zero-sum game \(\mathcal{Q}\) follows (3MW) for \(T\) epochs with learning rate \(\gamma\), sampling radius \(\delta\), and gradient estimates provided by (2PE). Then the players' empirical frequency of play enjoys the duality gap guarantee_

\[\mathbf{E}\big{[}\mathrm{Gap}_{\mathcal{L}}(\bar{\mathbf{X}}_{T})\big{]}\leq \frac{H}{\gamma T}+8D^{2}G^{2}\gamma+16DL\delta\] (15)

_where \(H=\log(d_{1}d_{2})\). In particular, for \(\gamma=(DG)^{-1}\sqrt{H/(8T)}\) and \(\delta=(G/L)\sqrt{H/(8T)}\), the players enjoy the equilibrium convergence guarantee_

\[\mathbf{E}\big{[}\mathrm{Gap}_{\mathcal{L}}(\bar{\mathbf{X}}_{T})\big{]}\leq 8 DG\sqrt{2H/T}.\] (16)

Proof.: Let \(\mathbf{X}^{*}\in\boldsymbol{\mathcal{X}}\) be a NE point. By Lemma 1 for \(\mathbf{P}=\mathbf{X}^{*}\), and setting \(F_{t}\coloneqq D(\mathbf{X}^{*},\mathbf{X}_{t})\) for all \(t=1,2\ldots\), we have

\[F_{t+1}\leq F_{t}+\gamma_{t}\;\mathrm{tr}[\hat{\mathbf{V}}_{t}^{\dagger}( \mathbf{X}_{t}-\mathbf{X}^{*})]+\frac{\gamma_{t}^{2}}{2}\|\hat{\mathbf{V}}_{t }\|_{F}^{2}\] (C.16)

or, equivalently

\[\mathrm{tr}[\hat{\mathbf{V}}_{t}^{\dagger}(\mathbf{X}^{*}-\mathbf{X}_{t})] \leq\frac{1}{\gamma_{t}}(F_{t}-F_{t+1})+\frac{\gamma_{t}}{2}\|\hat{\mathbf{V}} _{t}\|_{F}^{2}\] (C.17)

Summing over the whole sequence \(t=1,\ldots,T\), we get:

\[\sum_{t=1}^{T}\mathrm{tr}[\hat{\mathbf{V}}_{t}^{\dagger}(\mathbf{X}^{*}- \mathbf{X}_{t})]\leq\sum_{t=1}^{T}\frac{1}{\gamma_{t}}(F_{t}-F_{t+1})+\frac{1 }{2}\sum_{t=1}^{T}\gamma_{t}\|\hat{\mathbf{V}}_{t}\|_{F}^{2}\] (C.18)

which can be rewritten by setting \(\gamma_{0}=\infty\), as:

\[\sum_{t=1}^{T}\mathrm{tr}[\hat{\mathbf{V}}_{t}^{\dagger}(\mathbf{X}^{*}- \mathbf{X}_{t})]\leq\sum_{t=1}^{T}F_{t}\left(\frac{1}{\gamma_{t}}-\frac{1}{ \gamma_{t-1}}\right)+\frac{1}{2}\sum_{t=1}^{T}\gamma_{t}\|\hat{\mathbf{V}}_{t }\|_{F}^{2}\] (C.19)

Decomposing \(\hat{\mathbf{V}}_{t}\) as

\[\hat{\mathbf{V}}_{t}=\mathbf{V}(\mathbf{X}_{t})+b_{t}+U_{t}\] (C.20)

with

1. \(b_{t}=\mathbf{E}\big{[}\hat{\mathbf{V}}_{t}\,\big{|}\,\mathcal{F}_{t}\big{]}- \mathbf{V}(\mathbf{X}_{t})\)
2. \(U_{t}=\hat{\mathbf{V}}_{t}-\mathbf{E}\big{[}\hat{\mathbf{V}}_{t}\,\big{|}\, \mathcal{F}_{t}\big{]}\)equation (D.18) becomes:

\[\sum_{t=1}^{T}\text{tr}\big{[}\mathbf{V}(\mathbf{X}_{t})^{\dagger}( \mathbf{X}^{*}-\mathbf{X}_{t})\big{]} \leq\sum_{t=1}^{T}F_{t}\bigg{(}\frac{1}{\gamma_{t}}-\frac{1}{\gamma _{t-1}}\bigg{)}+\frac{1}{2}\sum_{t=1}^{T}\gamma_{t}\left\|\hat{\mathbf{V}}_{t} \right\|_{F}^{2}\] \[\qquad+\sum_{t=1}^{T}\text{tr}\{b_{t}^{\dagger}(\mathbf{X}_{t}- \mathbf{X}^{*})\}+\sum_{t=1}^{T}\text{tr}\{U_{t}^{\dagger}(\mathbf{X}_{t}- \mathbf{X}^{*})\}\] \[\leq\sum_{t=1}^{T}F_{t}\bigg{(}\frac{1}{\gamma_{t}}-\frac{1}{ \gamma_{t-1}}\bigg{)}+\frac{1}{2}\sum_{t=1}^{T}\gamma_{t}\left\|\hat{\mathbf{V }}_{t}\right\|_{F}^{2}\] \[\qquad+4\sum_{t=1}^{T}\lVert b_{t}\rVert_{F}+\sum_{t=1}^{T}\text {tr}\{U_{t}^{\dagger}(\mathbf{X}_{t}-\mathbf{X}^{*})\}\] (C.21)

The left-hand side (LHS) of (D.20) gives:

\[\sum_{t=1}^{T}\text{tr}\big{[}\mathbf{V}(\mathbf{X}_{t})^{\dagger }(\mathbf{X}^{*}-\mathbf{X}_{t})\big{]} =\sum_{t=1}^{T}\text{tr}\big{[}\mathbf{V}_{1}(\mathbf{X}_{t})^{ \dagger}(\mathbf{X}_{1}^{*}-\mathbf{X}_{1,t})\big{]}+\sum_{t=1}^{T}\text{tr} \big{[}\mathbf{V}_{2}(\mathbf{X}_{t})^{\dagger}(\mathbf{X}_{2}^{*}-\mathbf{X} _{2,t})\big{]}\] \[=\sum_{t=1}^{T}\big{(}u_{1}(\mathbf{X}_{1}^{*},\mathbf{X}_{2,t})- u_{1}(\mathbf{X}_{t})\big{)}+\sum_{t=1}^{T}\big{(}u_{2}(\mathbf{X}_{1,t}, \mathbf{X}_{2}^{*})-u_{2}(\mathbf{X}_{t})\big{)}\] \[=\sum_{t=1}^{T}\big{(}\mathcal{L}(\mathbf{X}_{1}^{*},\mathbf{X}_{2, t})-\mathcal{L}(\mathbf{X}_{1,t},\mathbf{X}_{2}^{*})\big{)}\] (C.22)

Hence, dividing by \(T\), we get:

\[\mathcal{L}(\mathbf{X}_{1}^{*},\mathbf{\tilde{X}}_{2,T})-\mathcal{L}(\mathbf{ \tilde{X}}_{1,T},\mathbf{X}_{2}^{*})\leq\frac{1}{T}\sum_{t=1}^{T}\text{tr} \big{[}\mathbf{V}(\mathbf{X}_{t})^{\dagger}(\mathbf{X}^{*}-\mathbf{X}_{t}) \big{]}\] (C.23)

or, equivalently,

\[\text{Gap}_{\mathcal{L}}(\mathbf{\tilde{X}}_{T}) \leq\frac{1}{T}\sum_{t=1}^{T}\text{tr}\big{[}\mathbf{V}(\mathbf{ X}_{t})^{\dagger}(\mathbf{X}^{*}-\mathbf{X}_{t})\big{]}\] \[\leq\frac{1}{T}\sum_{t=1}^{T}F_{t}\bigg{(}\frac{1}{\gamma_{t}}- \frac{1}{\gamma_{t-1}}\bigg{)}+\frac{1}{2T}\sum_{t=1}^{T}\gamma_{t}\lVert\hat{ \mathbf{V}}_{t}\rVert_{F}^{2}\] \[\qquad+\frac{4}{T}\sum_{t=1}^{T}\lVert b_{t}\rVert_{F}+\frac{1}{ T}\sum_{t=1}^{T}\text{tr}[U_{t}^{\dagger}(\mathbf{X}_{t}-\mathbf{X}^{*})]\] (C.24)

Now, we focus on the right-hand side (RHS) of (D.20). Specifically, we have:

\[\mathbb{E}\big{[}\text{tr}[U_{t}^{\dagger}(\mathbf{X}_{t}-\mathbf{X}^{*})] \big{]}=\mathbb{E}[\mathbb{E}[\mathbb{E}[\text{tr}[U_{t}^{\dagger}(\mathbf{X} _{t}-\mathbf{X}^{*})]\mid\mathcal{F}_{t}]]\big{]}=0\] (C.25)

since \(\mathbf{X}_{t}\) is \(\mathcal{F}_{t}\)-measurable and \(\mathbb{E}[U_{t}\mid\mathcal{F}_{t}]=0\).

Moreover, by Proposition 3, we have:

\[\lVert b_{i,t}\rVert_{F}=\big{\lVert}\mathbb{E}\big{[}\hat{\mathbf{V}}_{i,t} \mid\mathcal{F}_{t}\big{]}-\mathbf{V}_{i}(\mathbf{X}_{t})\big{\rVert}_{F}\leq 2D_{ i}L\delta_{t}\] (C.26a) and \[\mathbb{E}\Big{[}\big{\lVert}\hat{\mathbf{V}}_{i,t}\big{\rVert}_{F}^{2}\Big{]}= \mathbb{E}\Big{[}\mathbb{E}\Big{[}\big{\lVert}\hat{\mathbf{V}}_{i,t}\big{\rVert}_ {F}^{2}\mid\mathcal{F}_{t}\Big{]}\Big{]}\leq 4D_{i}^{2}G^{2}\] (C.26b)

Hence, taking expectation in (D.20), we obtain:

\[\mathbb{E}\big{[}\text{Gap}_{\mathcal{L}}(\mathbf{\tilde{X}}_{T}) \big{]} \leq\frac{1}{T}\sum_{t=1}^{T}\mathbb{E}[F_{t}]\bigg{(}\frac{1}{ \gamma_{t}}-\frac{1}{\gamma_{t-1}}\bigg{)}+\frac{1}{2T}\sum_{t=1}^{T}\gamma_{t} \,\mathbb{E}[\lVert\hat{\mathbf{V}}_{t}\rVert_{F}^{2}]+\frac{4}{T}\sum_{t=1}^{T} \mathbb{E}[\lVert b_{t}\rVert_{F}]\] \[\leq\frac{1}{T}\sum_{t=1}^{T}\mathbb{E}[F_{t}]\bigg{(}\frac{1}{ \gamma_{t}}-\frac{1}{\gamma_{t-1}}\bigg{)}+\frac{8D^{2}G^{2}}{T}\sum_{t=1}^{T} \gamma_{t}+\frac{16DL}{T}\sum_{t=1}^{T}\delta_{t}\] (C.27)Setting \(\gamma_{t}=\gamma\) and \(\delta_{t}=\delta\), we obtain:

\[\mathds{E}\left[\operatorname{Gap}_{\mathcal{L}}(\bar{\mathbf{X}}_{T})\right] \leq\frac{F_{1}}{\gamma T}+8D^{2}G^{2}\gamma+16DL\delta\] (C.28)

and finally, noting that

\[F_{1}=D(\mathbf{X}^{*},\mathbf{X}_{1})\leq\log(d_{1}d_{2})\] (C.29)

we get:

\[\mathds{E}\left[\operatorname{Gap}_{\mathcal{L}}(\bar{\mathbf{X}}_{T})\right] \leq\frac{H}{\gamma T}+8D^{2}G^{2}\gamma+16DL\delta\] (C.30)

for \(H=\log(d_{1}d_{2})\). Hence, after tuning \(\gamma\) to optimize this last expression, our result follows by setting \(\gamma=\sqrt{\frac{H}{8TD^{2}G^{2}}}\) and \(\delta=\sqrt{\frac{G^{2}H}{8L^{2}T}}\). 

### Learning with bandit feedback

We now proceed with the more arduous task of proving the bona fide, bandit guarantees of (3MW) with 1-point, stochastic, payoff-based feedback. The key difference with our previous analysis lies in the different statistical properties of the 1-point estimator (1PE). The relevant result that we will need is restated below:

**Proposition 4**.: _The estimator (1PE) enjoys the conditional bounds_

\[(i) \|\mathds{E}[\hat{\mathbf{V}}_{t}\mid\mathcal{F}_{t}]-\mathbf{V }(\mathbf{X}_{t})\|_{F}\leq 4DL\delta_{t}\quad\text{and}\quad(ii) \ \ \mathds{E}[\|\hat{\mathbf{V}}_{t}\|_{F}^{2}\mid\mathcal{F}_{t}]\leq 4D^{2}B^{2}/ \delta_{t}^{2}.\] (20)

Proof.: We prove each part separately.

1. Let \(\Xi_{i,t}\) be defined for all players \(i\in\mathcal{N}\): \[\Xi_{i,t}=\mathbf{X}_{i,t}^{(\delta)}-\mathbf{X}_{i,t}=\delta_{t} \mathbf{Z}_{i,t}+\frac{\delta_{t}}{r_{i}}(\mathbf{R}_{i}-\mathbf{X}_{i,t})= \delta_{t}\left[\mathbf{Z}_{i,t}+\frac{1}{r_{i}}(\mathbf{R}_{i}-\mathbf{X}_{i, t})\right]\] (C.31) Taking a first-order Taylor expansion of \(u_{i}\), we obtain: \[u_{i}(\mathbf{X}_{t}^{(\delta)}+\delta_{t}\mathbf{Z}_{t})=u_{i}(\mathbf{X}_{t} )+\sum_{j\in\mathcal{N}}\operatorname{tr}\left[\nabla_{\mathbf{X}_{j}^{*}}u_{ i}(\mathbf{X}_{t})^{\dagger}\Xi_{j,t}\right]+R_{2}(\Xi_{t})\] (C.32) Since \(\mathds{E}[U_{i}(\omega_{t})\mid\mathcal{F}_{t},\mathbf{Z}_{t}]=u(\mathbf{X}_ {t}^{(\delta)}+\delta_{t}\mathbf{Z}_{t})\), combining it with (D.3), we readily get: \[\mathds{E}[\hat{\mathbf{V}}_{i,t}\mid\mathcal{F}_{t},\mathbf{Z}_{ t}] =\frac{D_{i}}{\delta_{t}}u_{i}(\mathbf{X}_{t}^{(\delta)}+\delta_{t} \mathbf{Z}_{t})\ \mathbf{Z}_{i,t}\] (C.33) \[=\frac{D_{i}}{\delta_{t}}u_{i}(\mathbf{X}_{t})\mathbf{Z}_{i,t}+ \frac{D_{i}}{\delta_{t}}\sum_{j\in\mathcal{N}}\operatorname{tr}\left[\nabla_{ \mathbf{X}_{j}^{*}}u_{i}(\mathbf{X}_{t})^{\dagger}\Xi_{j,t}\right]\mathbf{Z}_{ i,t}+\frac{D_{i}}{\delta_{t}}R_{2}(\Xi_{t})\mathbf{Z}_{i,t}\] (C.34) Now, because \(\mathds{E}\left[\mathbf{Z}_{i,t}\mid\mathcal{F}_{t}\right]=0\) and \(\mathbf{Z}_{i,t}\) is sampled independent of any other process, we have: \[\mathds{E}\left[u_{i}(\mathbf{X}_{t})\mathbf{Z}_{i,t}\mid\mathcal{F}_{t} \right]=u_{i}(\mathbf{X}_{t})\ \mathds{E}\left[\mathbf{Z}_{i,t}\mid\mathcal{F}_{t}\right]=0\] (C.35) and for \(j\neq i\in\mathcal{N}\): \[\mathds{E}\left[\operatorname{tr}\left[\nabla_{\mathbf{X}_{j}^{*}}u_{i}( \mathbf{X}_{t})^{\dagger}\Xi_{j,t}\right]\mathbf{Z}_{i,t}\mid\mathcal{F}_{t} \right]=0\] (C.36) Therefore, we obtain: \[\mathds{E}\left[\sum_{j\in\mathcal{N}}\operatorname{tr}\left[ \nabla_{\mathbf{X}_{j}^{*}}u_{i}(\mathbf{X}_{t})^{\dagger}\Xi_{j,t}\right] \mathbf{Z}_{i,t}\mid\mathcal{F}_{t}\right] =\mathds{E}\left[\operatorname{tr}\left[\mathbf{V}_{i}(\mathbf{X}_ {t})^{\dagger}\Xi_{i,t}\right]\mathbf{Z}_{i,t}\mid\mathcal{F}_{t}\right]\] \[=\delta_{t}\ \mathds{E}\left[\operatorname{tr}\left[\mathbf{V}_{i}( \mathbf{X}_{t})^{\dagger}\mathbf{Z}_{i,t}\right]\mathbf{Z}_{i,t}\mid\mathcal{F}_ {t}\right]\] \[=\frac{\delta_{t}}{D_{i}}\sum_{W\in\mathcal{E}_{i}}\operatorname{ tr}\left[\mathbf{V}_{i}(\mathbf{X}_{t})^{\dagger}W\right]W\] \[=\frac{\delta_{t}}{D_{i}}\operatorname{proj}_{\mathcal{E}_{t}}( \mathbf{V}_{i}(\mathbf{X}_{t}))=\frac{\delta_{t}}{D_{i}}\mathbf{V}_{i}(\mathbf{ X}_{t})\] (C.37)where in the last step, we identify \(\operatorname{proj}_{\mathcal{E}_{t}}(\mathbf{V}_{i}(\mathbf{X}_{t}))\) with \(\mathbf{V}_{i}(\mathbf{X}_{t})\), as explained in the proof of Proposition 3. Moreover, we have that

\[|R_{2}(\Xi_{t})|\leq\frac{L}{2}\|\Xi_{t}\|_{F}^{2}\leq L\delta_{t}^{2}\] (C.38)

In view of the above, we have:

\[\|\mathbf{E}[\hat{\mathbf{V}}_{i,t}\,|\,\mathcal{F}_{t}]-\mathbf{V}_{i}( \mathbf{X}_{t})\|_{F}=D_{i}L\delta_{t}\] (C.39)

and, therefore,

\[\|\mathbf{E}[\hat{\mathbf{V}}_{t}\,|\,\mathcal{F}_{t}]-\mathbf{V}(\mathbf{X}_ {t})\|_{F}=\sqrt{2}DL\delta_{t}\] (C.40)

* By the definition of \(\hat{\mathbf{V}}_{i,t}\), we have: \[\|\hat{\mathbf{V}}_{i,t}\|_{F}=\frac{D_{i}}{\delta_{t}}\big{|}u_{i}( \mathbf{X}_{t}^{(\delta)}+\delta_{t}\,\mathbf{Z}_{t})\big{|}\,\|\mathbf{Z}_{ i,t}\|_{F}\leq\frac{D_{i}B}{\delta_{t}}\] (C.41) and therefore, we readily obtain that: \[\mathbb{E}\big{[}\|\hat{\mathbf{V}}_{i,t}\|_{F}^{2}\,\big{|}\, \mathcal{F}_{t}\big{]}\leq\frac{D_{i}^{2}B^{2}}{\delta_{t}^{2}}\] (C.42) We thus obtain \[\mathbb{E}\big{[}\|\hat{\mathbf{V}}_{t}\|_{F}^{2}\,\big{|}\, \mathcal{F}_{t}\big{]}\leq\frac{2D^{2}B^{2}}{\delta_{t}^{2}}\] (C.43) and our proof is complete. 

The only step missing is the proof of the actual guarantee of (3MW) with bandit feedback. We restate and prove the relevant result below:

**Theorem 3**.: _Suppose that each player of a \(2\)-player zero-sum game \(\mathcal{Q}\) follows (3MW) for \(T\) epochs with learning rate \(\gamma\), sampling radius \(\delta\), and gradient estimates provided by (1PE). Then the players' empirical frequency of play enjoys the duality gap guarantee_

\[\mathbb{E}\big{[}\operatorname{Gap}_{\mathcal{L}}(\bar{\mathbf{X}}_{T})\big{]} \leq\frac{H}{\gamma T}+\frac{2D^{2}B^{2}\gamma}{\delta^{2}}+16DL\delta\] (21)

_where \(H=\log(d_{1}d_{2})\). In particular, for \(\gamma=\big{(}\frac{H}{2T}\big{)}^{3/4}\frac{1}{2D\sqrt{BL}}\) and \(\delta=\big{(}\frac{H}{2T}\big{)}^{1/4}\sqrt{\frac{B}{4L}}\), the players enjoy the equilibrium convergence guarantee:_

\[\mathbb{E}\big{[}\operatorname{Gap}_{\mathcal{L}}(\bar{\mathbf{X}}_{T})\big{]} \leq\frac{2^{3/4}\,8H^{1/4}D\sqrt{BL}}{T^{1/4}}.\] (22)

Proof.: Following the same procedure as in the proof of Theorem 2, we readily obtain:

\[\operatorname{Gap}_{\mathcal{L}}(\bar{\mathbf{X}}_{T}) \leq\frac{1}{T}\sum_{t=1}^{T}\operatorname{tr}[\mathbf{V}(\mathbf{ X}_{t})^{\dagger}(\mathbf{X}^{*}-\mathbf{X}_{t})]\] \[\leq\frac{1}{T}\sum_{t=1}^{T}F_{t}\left(\frac{1}{\gamma_{t}}- \frac{1}{\gamma_{t-1}}\right)+\frac{1}{2T}\sum_{t=1}^{T}\gamma_{t}\|\hat{ \mathbf{V}}_{t}\|_{F}^{2}\] \[\quad+\frac{4}{T}\sum_{t=1}^{T}\|b_{t}\|_{F}+\frac{1}{T}\sum_{t=1 }^{T}\operatorname{tr}[U_{t}^{\dagger}(\mathbf{X}_{t}-\mathbf{X}^{*})]\] (C.44)

Now, we have:

\[\mathbb{E}[\operatorname{tr}[U_{t}^{\dagger}(\mathbf{X}_{t}-\mathbf{X}^{*})]] =\mathbb{E}[\mathbb{E}[\operatorname{tr}[U_{t}^{\dagger}(\mathbf{X}_{t}- \mathbf{X}^{*})]\,|\,\mathcal{F}_{t}]]=0\] (C.45)

since \(\mathbf{X}_{t}\) is \(\mathcal{F}_{t}\)-measurable and \(\mathbb{E}[U_{t}\,|\,\mathcal{F}_{t}]=0\).

Moreover, by Proposition 4, we have:

\[\|b_{i,t}\|_{F}=\left\|\mathbb{E}\big{[}\hat{\mathbf{V}}_{i,t}\mid\mathcal{F}_{t} \big{]}-\mathbf{V}_{i}(\mathbf{X}_{t})\right\|_{F}\leq 2D_{i}L\delta_{t}\] (C.46)

and

\[\mathbb{E}\Big{[}\big{\|}\hat{\mathbf{V}}_{i,t}\big{\|}_{F}^{2} \Big{]}=\mathbb{E}\Big{[}\mathbb{E}\Big{[}\big{\|}\hat{\mathbf{V}}_{i,t}\big{\|} _{F}^{2}\mid\mathcal{F}_{t}\Big{]}\Big{]}\leq\frac{D_{i}^{2}B^{2}}{\delta_{t}^ {2}}\] (C.47)

Hence, taking expectation in (D.34), we obtain:

\[\mathbb{E}\big{[}\text{Gap}_{\mathcal{L}}(\tilde{\mathbf{X}}_{T}) \big{]} \leq\frac{1}{T}\sum_{t=1}^{T}\mathbb{E}[F_{t}]\left(\frac{1}{ \gamma_{t}}-\frac{1}{\gamma_{t-1}}\right)+\frac{1}{2T}\sum_{t=1}^{T}\gamma_{t }\,\mathbb{E}[\|\hat{\mathbf{V}}_{t}\|_{F}^{2}]+\frac{4}{T}\sum_{t=1}^{T} \mathbb{E}[\|b_{t}\|_{F}]\] \[\leq\frac{1}{T}\sum_{t=1}^{T}\mathbb{E}[F_{t}]\left(\frac{1}{ \gamma_{t}}-\frac{1}{\gamma_{t-1}}\right)+\frac{2D^{2}B^{2}}{T}\sum_{t=1}^{T} \frac{\gamma_{t}}{\delta_{t}^{2}}+\frac{16DL}{T}\sum_{t=1}^{T}\delta_{t}\] (C.48)

Setting \(\gamma_{t}=\gamma\) and \(\delta_{t}=\delta\), we obtain:

\[\mathbb{E}\big{[}\text{Gap}_{\mathcal{L}}(\tilde{\mathbf{X}}_{T}) \big{]}\leq\frac{\mathbb{E}[F_{1}]}{\gamma T}+\frac{2D^{2}B^{2}\gamma}{\delta^ {2}}+16DL\delta\] (C.49)

and finally, noting that

\[\mathbb{E}[F_{1}]=D(\mathbf{X}^{*},\mathbf{X}_{1})\leq\log(d_{1}d_{2})\] (C.50)

we get:

\[\mathbb{E}\big{[}\text{Gap}_{\mathcal{L}}(\tilde{\mathbf{X}}_{T}) \big{]}\leq\frac{H}{\gamma T}+\frac{2D^{2}B^{2}\gamma}{\delta^{2}}+16DL\delta\] (C.51)

where \(H=\log(d_{1}d_{2})\). Hence, after tuning \(\gamma\) and \(\delta\) to optimize this last expression, our result follows by setting \(\gamma=\big{(}\frac{H}{2T}\big{)}^{3/4}\frac{1}{2D\sqrt{BL}}\) and \(\delta=\big{(}\frac{H}{2T}\big{)}^{1/4}\sqrt{\frac{B}{4L}}\). 

## Appendix D Omitted proofs from Section 6

We provide first the bounds of the estimator (1PE) in a \(N\)-player quantum game. Formally, we have:

**Lemma D.1**.: _The estimator (1PE) in a \(N\)-player quantum game \(\mathcal{Q}\) enjoys the conditional bounds_

\[(i)\;\;\|\mathbb{E}[\hat{\mathbf{V}}_{t}\mid\mathcal{F}_{t}]- \mathbf{V}(\mathbf{X}_{t})\|_{F}\leq\frac{1}{2}DLN^{3/2}\delta_{t}\quad\text{ and}\quad(ii)\;\;\mathbb{E}[\|\hat{\mathbf{V}}_{t}\|_{F}^{2}\mid\mathcal{F}_{t}] \leq\frac{D^{2}B^{2}N}{\delta_{t}^{2}}.\] (D.1)

Proof.: We prove each part separately.

* Let \(\Xi_{i,t}\) be defined for all players \(i\in\mathcal{N}\): \[\Xi_{i,t}=\mathbf{X}_{i,t}^{(\delta)}-\mathbf{X}_{i,t}=\delta_{t}\mathbf{Z}_ {i,t}+\frac{\delta_{t}}{r_{i}}(\mathbf{R}_{i}-\mathbf{X}_{i,t})=\delta_{t} \bigg{[}\mathbf{Z}_{i,t}+\frac{1}{r_{i}}(\mathbf{R}_{i}-\mathbf{X}_{i,t}) \bigg{]}\] (D.2) Taking a 1st-order Taylor expansion of \(u_{i}\), we obtain: \[u_{i}(\mathbf{X}_{t}^{(\delta)}+\delta_{t}\mathbf{Z}_{t})=u_{i}(\mathbf{X}_{t })+\sum_{j\in\mathcal{N}}\text{tr}\bigg{[}\nabla_{\mathbf{X}_{j}^{*}}u_{i}( \mathbf{X}_{t})^{\dagger}\Xi_{j,t}\bigg{]}+R_{2}(\Xi_{t})\] (D.3) Since \(\mathbb{E}[U_{i}(\omega_{t})\mid\mathcal{F}_{t},\mathbf{Z}_{t}]=u(\mathbf{X}_{t }^{(\delta)}+\delta_{t}\mathbf{Z}_{t})\), combining it with (D.3), we readily get: \[\mathbb{E}[\hat{\mathbf{V}}_{i,t}\mid\mathcal{F}_{t},\mathbf{Z}_ {t}] =\frac{D_{i}}{\delta_{t}}u_{i}(\mathbf{X}_{t}^{(\delta)}+\delta_{t} \mathbf{Z}_{t})\;\mathbf{Z}_{i,t}\] \[=\frac{D_{i}}{\delta_{t}}u_{i}(\mathbf{X}_{t})\mathbf{Z}_{i,t}+ \frac{D_{i}}{\delta_{t}}\sum_{j\in\mathcal{N}}\text{tr}\bigg{[}\nabla_{\mathbf{ X}_{j}^{*}}u_{i}(\mathbf{X}_{t})^{\dagger}\Xi_{j,t}\bigg{]}\mathbf{Z}_{i,t}+ \frac{D_{i}}{\delta_{t}}R_{2}(\Xi_{t})\mathbf{Z}_{i,t}\] (D.4)Now, because \(\mathbb{E}\big{[}\mathbf{Z}_{i,t}\bigm{|}\mathcal{F}_{t}\bigm{]}=0\) and \(\mathbf{Z}_{i,t}\) is sampled independent of any other process, we have: \[\mathbb{E}\big{[}u_{i}(\mathbf{X}_{t})\mathbf{Z}_{i,t}\bigm{|} \mathcal{F}_{t}\bigm{]}=u_{i}(\mathbf{X}_{t})\,\mathbb{E}\big{[}\mathbf{Z}_{i,t}\bigm{|}\mathcal{F}_{t}\bigm{]}=0\] (D.5) and for \(j\neq i\in\mathcal{N}\): \[\mathbb{E}\Big{[}\mathrm{tr}\Big{[}\nabla_{\mathbf{X}_{j}^{\prime}}u_{i}( \mathbf{X}_{t})^{\dagger}\Xi_{j,t}\Bigm{]}\mathbf{Z}_{i,t}\Bigm{|}\mathcal{F} _{t}\Bigm{]}=0\] (D.6) Therefore, we obtain: \[\mathbb{E}\bigg{[}\sum_{j\in\mathcal{N}}\mathrm{tr}\Big{[}\nabla _{\mathbf{X}_{j}^{\prime}}u_{i}(\mathbf{X}_{t})^{\dagger}\Xi_{j,t}\Bigm{]} \mathbf{Z}_{i,t}\mid\mathcal{F}_{t}\biggm{]} =\mathbb{E}\big{[}\mathrm{tr}\big{[}\mathbf{V}_{i}(\mathbf{X}_{t })^{\dagger}\Xi_{i,t}\bigm{]}\mathbf{Z}_{i,t}\mid\mathcal{F}_{t}\bigm{]}\] \[=\delta_{t}\,\mathbb{E}\big{[}\mathrm{tr}\big{[}\mathbf{V}_{i}( \mathbf{X}_{t})^{\dagger}\mathbf{Z}_{i,t}\bigm{]}\mathbf{Z}_{i,t}\mid\mathcal{ F}_{t}\bigm{]}\] \[=\frac{\delta_{t}}{D_{i}}\,\sum_{W\in\mathcal{E}_{i}}\mathrm{tr} \big{[}\mathbf{V}_{i}(\mathbf{X}_{t})^{\dagger}W\bigm{]}W\] \[=\frac{\delta_{t}}{D_{i}}\,\mathrm{proj}_{\mathcal{E}_{i}}( \mathbf{V}_{i}(\mathbf{X}_{t}))=\frac{\delta_{t}}{D_{i}}\mathbf{V}_{i}( \mathbf{X}_{t})\] (D.7) where in the last step, we identify \(\mathrm{proj}_{\mathcal{E}_{i}}(\mathbf{V}_{i}(\mathbf{X}_{t}))\) with \(\mathbf{V}_{i}(\mathbf{X}_{t})\), as explained in the proof of Proposition 3. Moreover, we have that \[|R_{2}(\Xi_{t})|\leq\frac{L}{2}\|\Xi_{t}\|_{F}^{2}\leq\frac{1}{2}LN\delta_{t}^ {2}\] (D.8) In view of the above, we have: \[\|\mathbb{E}\big{[}\hat{\mathbf{V}}_{i,t}\bigm{|}\mathcal{F}_{t}\bigm{]}- \mathbf{V}_{i}(\mathbf{X}_{t})\|_{F}\leq\frac{1}{2}D_{i}LN\delta_{t}\] (D.9) and, therefore, \[\|\mathbb{E}\big{[}\hat{\mathbf{V}}_{t}\bigm{|}\mathcal{F}_{t}\bigm{]}- \mathbf{V}(\mathbf{X}_{t})\|_{F}\leq\frac{1}{2}DLN^{3/2}\delta_{t}\] (D.10)
2. By the definition of \(\hat{\mathbf{V}}_{i,t}\), we have: \[\|\hat{\mathbf{V}}_{i,t}\|_{F}=\frac{D_{i}}{\delta_{t}}\Big{|}u_{i}(\mathbf{X} _{t}^{(\delta)}+\delta_{t}\,\mathbf{Z}_{t})\Big{|}\,\|\mathbf{Z}_{i,t}\|_{F} \leq\frac{D_{i}B}{\delta_{t}}\] (D.11) and therefore, we readily obtain that: \[\mathbb{E}\big{[}\|\hat{\mathbf{V}}_{i,t}\|_{F}^{2}\bigm{]}\mathcal{F}_{t} \bigm{]}\leq\frac{D_{i}^{2}B^{2}}{\delta_{t}^{2}}\] (D.12) Hence, ultimately, we get the bound \[\mathbb{E}\big{[}\|\hat{\mathbf{V}}_{t}\|_{F}^{2}\bigm{]}\mathcal{F}_{t} \bigm{]}\leq\frac{D^{2}B^{2}N}{\delta_{t}^{2}}\] (D.13)

and our proof is complete. 

With all this in hand, we are finally in a position to proceed with the proof of Theorem 4, which we restate below for convenience:

**Theorem 4**.: _Fix some tolerance level \(\eta\in(0,1)\) and suppose that the players of an \(N\)-player quantum game follow (\(3\)MW) with bandit, realization-based feedback, and surrogate gradients provided by the estimator (\(1\)PE) with step-size and sampling radius parameters such that_

\[(i)\ \ \sum_{t=1}^{\infty}\gamma_{t}=\infty,\quad(ii)\ \sum_{t=1}^{\infty} \gamma_{t}\delta_{t}<\infty,\quad\text{and}\quad(ii)\ \sum_{t=1}^{\infty}\gamma_{t}^{2}/\delta_{t}^{2}<\infty.\] (23)

_If \(\mathbf{X}^{*}\) is variationally stable, there exists a neighborhood \(\mathcal{U}\) of \(\mathbf{X}^{*}\) such that_

\[\mathbb{P}(\lim_{t\to\infty}\mathbf{X}_{t}=\mathbf{X}^{*})\geq 1-\eta\quad\text{ whenever }\mathbf{X}_{1}\in\mathcal{U}.\] (24)Proof.: Since \(\mathbf{X}^{*}\) is variationally stable, there exists a neighborhood \(\mathcal{U}_{\mathrm{vs}}\) of it such that

\[\operatorname{tr}\!\left[\mathbf{V}(\mathbf{X})(\mathbf{X}-\mathbf{X}^{*}) \right]<0\quad\text{for all }\mathbf{X}\in\mathcal{U}_{\mathrm{vs}}\backslash\{\mathbf{X}^{*}\}.\] (D.14)

For any \(\varepsilon^{\prime}>0\), defining

\[\mathcal{U}^{\prime}_{\varepsilon}\coloneqq\{\mathbf{X}\in\mathcal{X}:D( \mathbf{X}^{*},\mathbf{X})<\varepsilon^{\prime}\}\] (D.15)

we readily obtain by the continuity of \(\mathbf{X}\mapsto D(\mathbf{X}^{*},\mathbf{X})\) at \(\mathbf{X}^{*}\) that there exists a neighborhood \(\mathcal{U}_{\varepsilon}\) of \(\mathbf{X}^{*}\) such that \(\mathcal{U}_{\varepsilon}\subseteq\mathcal{U}_{\mathrm{vs}}\). Note that if \(\varepsilon_{1}<\varepsilon_{2}\), we automatically get that \(\mathcal{U}_{\varepsilon_{1}}\subseteq\mathcal{U}_{\varepsilon_{2}}\).

In view of this, we let \(\mathbf{X}_{1}\in\mathcal{U}_{\varepsilon/4}\subseteq\mathcal{U}_{\varepsilon} \subseteq\mathcal{U}_{\mathrm{vs}}\). We divide the rest of the proof in steps.

**Step 1.**: **Deriving the general energy inequality**

By Lemma 1 we have that:

\[D(\mathbf{X}^{*},\mathbf{X}_{t+1})\leq D(\mathbf{X}^{*},\mathbf{X}_{t})+ \gamma_{t}\operatorname{tr}\!\left[\mathbf{\hat{V}}_{t}(\mathbf{X}_{t}- \mathbf{X}^{*})\right]+\frac{\gamma_{t}^{2}}{2}\|\mathbf{\hat{V}}_{t}\|_{F}^{2}.\] (D.16)

Decomposing \(\mathbf{\hat{V}}_{t}\) into

\[\mathbf{\hat{V}}_{t}=\mathbf{V}(\mathbf{X}_{t})+b_{t}+U_{t}\] (D.17)

as per (C.20) and applying (D.16) inequality iteratively, we get that

\[D(\mathbf{X}^{*},\mathbf{X}_{t+1}) \leq D(\mathbf{X}^{*},\mathbf{X}_{1})+\sum_{s=1}^{t}\gamma_{s} \operatorname{tr}\!\left[\mathbf{\hat{V}}_{s}(\mathbf{X}_{s}-\mathbf{X}^{*}) \right]+\frac{1}{2}\sum_{s=1}^{t}\gamma_{s}^{2}\|\mathbf{\hat{V}}_{s}\|_{F}^{2}\] \[\leq D(\mathbf{X}^{*},\mathbf{X}_{1})+\sum_{s=1}^{t}\gamma_{s} \operatorname{tr}\!\left[\mathbf{V}(\mathbf{X}_{s})(\mathbf{X}_{s}-\mathbf{X} ^{*})\right]+\sum_{s=1}^{t}\gamma_{s}\operatorname{tr}\!\left[b_{s}(\mathbf{ X}_{s}-\mathbf{X}^{*})\right]\] \[\quad+\sum_{s=1}^{t}\gamma_{s}\operatorname{tr}\!\left[U_{s}( \mathbf{X}_{s}-\mathbf{X}^{*})\right]+\frac{1}{2}\sum_{s=1}^{t}\gamma_{s}^{2} \|\mathbf{\hat{V}}_{s}\|_{F}^{2}\] \[\leq D(\mathbf{X}^{*},\mathbf{X}_{1})+\sum_{s=1}^{t}\gamma_{s} \operatorname{tr}\!\left[\mathbf{V}(\mathbf{X}_{s})(\mathbf{X}_{s}-\mathbf{X} ^{*})\right]+\sum_{s=1}^{t}\gamma_{s}\|b_{s}\|_{F}\|\mathbf{X}_{s}-\mathbf{X} ^{*}\|_{F}\] \[\quad+\sum_{s=1}^{t}\gamma_{s}\operatorname{tr}\!\left[U_{s}( \mathbf{X}_{s}-\mathbf{X}^{*})\right]+\frac{1}{2}\sum_{s=1}^{t}\gamma_{s}^{2} \|\mathbf{\hat{V}}_{s}\|_{F}^{2}\] \[\leq D(\mathbf{X}^{*},\mathbf{X}_{1})+\sum_{s=1}^{t}\gamma_{s} \operatorname{tr}\!\left[\mathbf{V}(\mathbf{X}_{s})(\mathbf{X}_{s}-\mathbf{X} ^{*})\right]+\operatorname{diam}(\mathcal{X})\sum_{s=1}^{t}\gamma_{s}\|b_{s}\|_ {F}\] \[\quad+\sum_{s=1}^{t}\gamma_{s}\operatorname{tr}\!\left[U_{s}( \mathbf{X}_{s}-\mathbf{X}^{*})\right]+\frac{1}{2}\sum_{s=1}^{t}\gamma_{s}^{2} \|\mathbf{\hat{V}}_{s}\|_{F}^{2}\] (D.18)

Defining the processes \(\Psi_{t},M_{t}\) and \(Z_{t}\) for \(t=1,2,\dots\) as

\[\Psi_{t} \coloneqq\frac{1}{2}\sum_{s=1}^{t}\gamma_{s}^{2}\|\mathbf{\hat{V}} _{s}\|_{F}^{2}\] (D.19a) \[M_{t} \coloneqq\sum_{s=1}^{t}\gamma_{s}\operatorname{tr}\!\left[U_{s}( \mathbf{X}_{s}-\mathbf{X}^{*})\right]\] (D.19b) \[Z_{t} \coloneqq\operatorname{diam}(\mathcal{X})\sum_{s=1}^{t}\gamma_{s} \|b_{s}\|_{F}\] (D.19c)

equation (D.18) can be rewritten as

\[D(\mathbf{X}^{*},\mathbf{X}_{t+1})\leq D(\mathbf{X}^{*},\mathbf{X}_{1})+\sum_{ s=1}^{t}\gamma_{s}\operatorname{tr}\!\left[\mathbf{V}(\mathbf{X}_{s})(\mathbf{X}_{s}- \mathbf{X}^{*})\right]+Z_{t}+M_{t}+\Psi_{t}\] (D.20)
**Step 2.**: **Bounding the noise terms**

Let \(\varepsilon>0\) as defined in the beginning of the proof.

* Regarding the term \(Z_{t}\), it is clear that the process \(\{Z_{t}:t\geq 1\}\) is a sub-martingale. Hence, by Doob's maximal inequality for sub-martingales [24], we get that: \[\mathbb{P}\left(\sup_{s\leq t}Z_{s}\geq\varepsilon/4\right) \leq\frac{\operatorname{E}[Z_{t}]}{\varepsilon/4}\] \[\leq\frac{\operatorname{diam}(\boldsymbol{\mathcal{X}})\,\Sigma_ {s=1}^{t}\,\gamma_{s}\,\operatorname{E}[\|b_{s}\|_{F}]}{\varepsilon/4}\] \[\leq\frac{\operatorname{diam}(\boldsymbol{\mathcal{X}})\,\Sigma_ {t=1}^{\infty}\,\gamma_{t}\,\operatorname{E}[\|b_{t}\|_{F}]}{\varepsilon/4}\] \[=\frac{\operatorname{diam}(\boldsymbol{\mathcal{X}})\,\Sigma_ {t=1}^{\infty}\,\gamma_{t}\,\operatorname{E}[\operatorname{E}[\|b_{t}\|_{F} \,|\,\mathcal{F}_{t}]]}{\varepsilon/4}\] \[\leq\frac{2\operatorname{diam}(\boldsymbol{\mathcal{X}})DLN^{3/ 2}\sum_{t=1}^{\infty}\,\gamma_{t}\,\delta_{t}}{\varepsilon}\] (D.21) By ensuring that \[\sum_{t=1}^{\infty}\gamma_{t}\,\delta_{t}\leq\frac{\varepsilon\eta}{6 \operatorname{diam}(\boldsymbol{\mathcal{X}})DLN^{3/2}}\] (D.22) and taking \(t\) go to \(\infty\), (D.21) becomes: \[\mathbb{P}\left(\sup_{t\geq 1}Z_{t}\geq\varepsilon/4\right)\leq\eta/3\] (D.23)
* Similarly, it is clear that the process \(\{\Psi_{t}:t\geq 1\}\) is a sub-martingale. Following the same procedure, by Doob's maximal inequality for sub-martingales [24], we get that: \[\mathbb{P}\left(\sup_{s\leq t}\Psi_{s}\geq\varepsilon/4\right) \leq\frac{\operatorname{E}[\Psi_{t}]}{\varepsilon/4} \leq\frac{\frac{1}{2}\sum_{s=1}^{t}\gamma_{s}^{2}\operatorname{E} \left[\|\nabla_{s}\|_{F}^{2}\right]}{\varepsilon/4}\] \[\leq\frac{\frac{1}{2}\sum_{t=1}^{\infty}\,\gamma_{t}^{2}\, \operatorname{E}\left[\|\nabla_{t}\|_{F}^{2}\right]}{\varepsilon/4}\] \[\leq\frac{2D^{2}B^{2}N\,\Sigma_{t=1}^{\infty}\,\gamma_{t}^{2}/ \delta_{t}^{2}}{\varepsilon}\] (D.24) By ensuring that \[\sum_{t=1}^{\infty}\!\!\gamma_{t}^{2}/\delta_{t}^{2}\leq\frac{\varepsilon\eta }{6D^{2}B^{2}N}\] (D.25) and taking \(t\to\infty\), (D.24) becomes: \[\mathbb{P}\left(\sup_{t\geq 1}\Psi_{t}\geq\varepsilon/4\right)\leq\eta/3\] (D.26)
* Finally, regarding the term \(M_{t}\), the process \(\{M_{t}:t\geq 1\}\) is a martingale. Following the same procedure, by Doob's maximal inequality for martingales [24], we get that: \[\mathbb{P}\left(\sup_{s\leq t}M_{s}\geq\varepsilon/4\right) \leq\mathbb{P}\left(\sup_{s\leq t}\lvert M_{s}\rvert\geq \varepsilon/4\right)\leq\frac{\operatorname{E}[M_{t}^{2}]}{(\varepsilon/4)^{2 }}=\frac{\sum_{s=1}^{t}\gamma_{s}^{2}\operatorname{E}\left[\operatorname{tr}[ U_{s}(\mathbf{X}_{s}-\mathbf{X}^{*})]^{2}\right]}{(\varepsilon/4)^{2}}\] \[\leq\frac{\operatorname{diam}(\boldsymbol{\mathcal{X}})^{2}\, \Sigma_{s=1}^{t}\,\gamma_{s}^{2}\operatorname{E}\left[\|U_{t}\|_{F}^{2}\right] }{(\varepsilon/4)^{2}}\] \[\leq\frac{4\operatorname{diam}(\boldsymbol{\mathcal{X}})^{2}\, \Sigma_{t=1}^{\infty}\,\gamma_{t}^{2}\operatorname{E}\left[\|\nabla_{t}\|_{F }^{2}\right]}{(\varepsilon/4)^{2}}\] \[\leq\frac{4\operatorname{diam}(\boldsymbol{\mathcal{X}})^{2}D^{2 }B^{2}N\,\Sigma_{t=1}^{\infty}\,\gamma_{t}^{2}/\delta_{t}^{2}}{(\varepsilon/4 )^{2}}\] (D.27)where we used the fact that \[\mathbf{E}[M_{t}^{2}] =\mathbf{E}\Bigg{[}\sum_{s=1}^{t}\gamma_{s}^{2}\mathrm{tr}[U_{s}( \mathbf{X}_{s}-\mathbf{X}^{*})]^{2}+\sum_{k<\ell}\gamma_{k}\gamma_{\ell}\, \mathrm{tr}[U_{k}(\mathbf{X}_{k}-\mathbf{X}^{*})]\,\mathrm{tr}[U_{\ell}( \mathbf{X}_{\ell}-\mathbf{X}^{*})]\Bigg{]}\] \[=\mathbf{E}\Bigg{[}\sum_{s=1}^{t}\gamma_{s}^{2}\mathrm{tr}[U_{s}( \mathbf{X}_{s}-\mathbf{X}^{*})]^{2}\Bigg{]}\] (D.28) itself following from the total expectation \[\mathbf{E}[\mathrm{tr}[U_{k}(\mathbf{X}_{k}-\mathbf{X}^{*})]\, \mathrm{tr}[U_{\ell}(\mathbf{X}_{\ell}-\mathbf{X}^{*})]] =\mathbf{E}[\mathbf{E}[\mathrm{tr}[U_{k}(\mathbf{X}_{k}-\mathbf{X }^{*})]\,\mathrm{tr}[U_{\ell}(\mathbf{X}_{\ell}-\mathbf{X}^{*})]\,|\,\mathcal{ F}_{\ell}]]\] \[=\mathbf{E}[\mathrm{tr}[U_{k}(\mathbf{X}_{k}-\mathbf{X}^{*})]\, \mathbf{E}[\mathrm{tr}[U_{\ell}(\mathbf{X}_{\ell}-\mathbf{X}^{*})]\,|\, \mathcal{F}_{\ell}]]\] \[=0\] (D.29) Now, by ensuring that \[\sum_{t=1}^{\infty}\gamma_{t}^{2}/\delta_{t}^{2}\leq\frac{(\varepsilon/4)^{2} \eta}{12\,\mathrm{diam}(\boldsymbol{\mathcal{X}})^{2}D^{2}B^{2}N}\] (D.30) and taking \(t\) go to \(\infty\), (D.27) becomes: \[\mathbf{P}\Bigg{(}\sup_{t\geq 1}M_{t}\geq\varepsilon/4\Bigg{)}\leq\eta/3\] (D.31) Therefore, combining (D.23), (D.26) and (D.31) and applying a union bound, we get: \[\mathbf{P}\Bigg{(}\Bigg{(}\sup_{t\geq 1}Z_{t}\geq\varepsilon/4\Bigg{)}\cup \Bigg{\{}\sup_{t\geq 1}\Psi_{t}\geq\varepsilon/4\Bigg{)}\cup\Bigg{\{}\sup_{t \geq 1}M_{t}\geq\varepsilon/4\Bigg{\}}\Bigg{)}\leq\eta\] (D.32) Thus, defining the event \(E\coloneqq\left\{\sup_{t\geq 1}Z_{t}+\Psi_{t}+M_{t}<\frac{3}{4}\varepsilon\right\}\), Eq. (D.32) readily implies that: \[\mathbf{P}(E)\geq 1-\eta\] (D.33)

**Step 3**.: \(\mathbf{X}_{t}\in\mathcal{U}_{\text{vs}}\) **with high probability**

Since \(\mathbf{X}_{1}\in\mathcal{U}_{\varepsilon/4}\subseteq\mathcal{U}_{\text{vs}}\), by induction on \(t\) we have that under the event \(E\)

\[D(\mathbf{X}^{*},\mathbf{X}_{t+1}) \leq D(\mathbf{X}^{*},\mathbf{X}_{1})+\sum_{s=1}^{t}\gamma_{s}\, \mathrm{tr}[\mathbf{V}(\mathbf{X}_{s})(\mathbf{X}_{s}-\mathbf{X}^{*})]+Z_{t}+ M_{t}+\Psi_{t}\] (D.34) \[\leq\frac{\varepsilon}{4}+\frac{\varepsilon}{4}+\frac{\varepsilon }{4}+\frac{\varepsilon}{4}=\varepsilon\] (D.35) where in the last step we used the inductive hypothesis that \(\mathbf{X}_{s}\in\mathcal{U}_{\text{vs}}\) for all \(s=1,\ldots,t\), which implies \(\mathrm{tr}[\mathbf{V}(\mathbf{X}_{s})(\mathbf{X}_{s}-\mathbf{X}^{*})]<0\). This implies that \(\mathbf{X}_{t+1}\in\mathcal{U}_{\varepsilon}\subseteq\mathcal{U}_{\text{vs}}\). Therefore, we obtain that \(\mathbf{X}_{t+1}\in\mathcal{U}_{\varepsilon}\subseteq\mathcal{U}_{\text{vs}}\) for all \(t\geq 1\). For the rest of the proof we will work under the event \(E\).
**Step 4**.: **Subsequential convergence**

Now we will show that there exists a subsequence \(\{\mathbf{X}_{t_{k}}:k\geq 1\}\) such that \(\lim_{k\to\infty}\mathbf{X}_{t_{k}}=\mathbf{X}^{*}\). Suppose it does not. Then, this would mean that the quantity \(\mathrm{tr}[\mathbf{V}(\mathbf{X}_{t})(\mathbf{X}_{t}-\mathbf{X}^{*})]\) is bounded away from zero. Combining it with the fact that \(\mathbf{X}_{t}\in\mathcal{U}_{\text{vs}}\) for all \(t\geq 0\), we readily get that there exists \(c>0\) such that:

\[\mathrm{tr}[\mathbf{V}(\mathbf{X}_{s})(\mathbf{X}_{s}-\mathbf{X}^{*})]<-c\] (D.36)

Then, (D.20) would give:

\[D(\mathbf{X}^{*},\mathbf{X}_{t+1})\leq\varepsilon-c\sum_{s=1}^{t}\gamma_{s}\] (D.37)

Hence, taking \(t\to\infty\), and using that \(\sum_{t\geq 1}\gamma_{t}=\infty\), we would get that \(D(\mathbf{X}^{*},\mathbf{X}_{t})\to-\infty\), which is a contradiction, since \(D(\mathbf{X}^{*},\mathbf{X}_{t})\geq 0\). Hence, there exists a subsequence \(\{\mathbf{X}_{t_{k}}:k\geq 1\}\) such that \(\lim_{k\to\infty}\mathbf{X}_{t_{k}}=\mathbf{X}^{*}\), i.e.,

\[\lim_{k\to\infty}D(\mathbf{X}^{*},\mathbf{X}_{t_{k}})=0.\] (D.38)

**Step 5**.: **Existence of \(\lim_{t\to\infty}D(\mathbf{X}^{*},\mathbf{X}_{t})\)**

We define the sequence of events \(\{E_{t}:t\geq 1\}\) as

\[E_{t}\coloneqq\left\{\sup_{s\leq t-1}\,Z_{s}+\Psi_{s}+M_{s}<\frac{3}{4}\varepsilon \right\}\quad\text{for }t\geq 2\] (D.39)

and

\[E_{1}\coloneqq\left\{\mathbf{X}_{1}\in\mathcal{U}_{\varepsilon/4}\right\}\] (D.40)

Then we have that \(E_{t}\in\mathcal{F}_{t}\) and \(E_{t}\subseteq\left\{\mathbf{X}_{s}\in\mathcal{U}_{\infty}:s=1,\ldots,t\right\}\).

Defining the random process \(\left\{\tilde{D}_{t}:t\geq 1\right\}\) as

\[\tilde{D}_{t}=D(\mathbf{X}^{*},\mathbf{X}_{t})\,\mathds{1}_{E_{t}}\] (D.41)

Then, by (D.16) we have

\[D(\mathbf{X}^{*},\mathbf{X}_{t+1}) \leq D(\mathbf{X}^{*},\mathbf{X}_{1})+\gamma_{t}\,\mathrm{tr}[ \mathbf{V}(\mathbf{X}_{t})(\mathbf{X}_{t}-\mathbf{X}^{*})]+\mathrm{diam}( \boldsymbol{\mathcal{X}})\gamma_{t}\|b_{t}\|_{F}\] \[+\gamma_{t}\,\mathrm{tr}[U_{t}(\mathbf{X}_{t}-\mathbf{X}^{*})]+ \frac{1}{2}\gamma_{t}^{2}\|\hat{\mathbf{V}}_{t}\|_{F}^{2}\] (D.42)

Multiplying the above relation with \(\mathds{1}_{E_{t}}\), and noting that \(\mathds{1}_{E_{t+1}}\leq\mathds{1}_{E_{t}}\), since \(E_{t+1}\subseteq E_{t}\), we have

\[\tilde{D}_{t+1} \leq\tilde{D}_{t}+\gamma_{t}\,\mathrm{tr}[\mathbf{V}(\mathbf{X}_ {t})(\mathbf{X}_{t}-\mathbf{X}^{*})]\,\mathds{1}_{E_{t}}+\mathrm{diam}( \boldsymbol{\mathcal{X}})\gamma_{t}\|b_{t}\|_{F}\,\mathds{1}_{E_{t}}\] \[+\gamma_{t}\,\mathrm{tr}[U_{t}(\mathbf{X}_{t}-\mathbf{X}^{*})]\, \mathds{1}_{E_{t}}+\frac{1}{2}\gamma_{t}^{2}\|\hat{\mathbf{V}}_{t}\|_{F}^{2} \,\mathds{1}_{E_{t}}\] (D.43) \[\leq\tilde{D}_{t}+\mathrm{diam}(\boldsymbol{\mathcal{X}})\gamma_{t }\|b_{t}\|_{F}\,\mathds{1}_{E_{t}}+\gamma_{t}\,\mathrm{tr}[U_{t}(\mathbf{X}_{ t}-\mathbf{X}^{*})]\,\mathds{1}_{E_{t}}+\frac{1}{2}\gamma_{t}^{2}\|\hat{\mathbf{V}}_{t }\|_{F}^{2}\,\mathds{1}_{E_{t}}\] (D.44)

where in the last step we used that \(\mathrm{tr}[\mathbf{V}(\mathbf{X}_{t})(\mathbf{X}_{t}-\mathbf{X}^{*})]\, \mathds{1}_{E_{t}}\leq 0\). Therefore, we obtain that:

\[\mathbf{E}[\tilde{D}_{t+1}\,|\,\mathcal{F}_{t}]\leq\tilde{D}_{t}+\mathrm{diam }(\boldsymbol{\mathcal{X}})\gamma_{t}\,\mathds{1}_{E_{t}}\,\mathbf{E}[\|b_{t} \|_{F}\,|\,\mathcal{F}_{t}]+\frac{1}{2}\gamma_{t}^{2}\,\mathds{1}_{E_{t}}\, \mathbf{E}[\|\hat{\mathbf{V}}_{t}\|_{F}^{2}\,|\,\mathcal{F}_{t}]\] (D.45)

where we used that

\[\mathbf{E}\big{[}\mathrm{tr}[\mathbf{V}(\mathbf{X}_{t})(\mathbf{X}_{t}- \mathbf{X}^{*})]\,\mathds{1}_{E_{t}}\,\big{|}\,\mathcal{F}_{t}\big{]}=\mathds{ 1}_{E_{t}}\,\mathbf{E}[\mathrm{tr}[\mathbf{V}(\mathbf{X}_{t})(\mathbf{X}_{t} -\mathbf{X}^{*})]\,|\,\mathcal{F}_{t}]=0\] (D.46)

Therefore, \(\left\{\tilde{D}_{t}:t\geq 1\right\}\) is an almost super-martingale [52] and, thus, there exists \(\tilde{D}_{\infty}\) with \(\tilde{D}_{\infty}\) finite (a.s.) and \(\tilde{D}_{t}\to\tilde{D}_{\infty}\) (a.s.).

Since \(E=\cap_{t\geq 1}E_{t}\), we have:

\[\mathbb{P}\Big{(}\lim_{t\to\infty}D(\mathbf{X}^{*},\mathbf{X}_{t} )\,\,\,\text{ exists}\,\Big{|}\,E\Big{)} =\frac{\mathbb{P}(\left\{\lim_{t\to\infty}D(\mathbf{X}^{*},\mathbf{X}_{t}) \,\,\,\text{exists}\right\}\cap E)}{\mathbb{P}(E)}\] (D.47) \[=\frac{\mathbb{P}\big{(}\left\{\lim_{t\to\infty}\tilde{D}_{t}\,\, \,\text{exists}\right\}\cap E\big{)}}{\mathbb{P}(E)}=1\] (D.48)

Hence, \(\lim_{t\to\infty}\tilde{D}_{t}\) exists on \(E\) and by _Step 3_ we readily get that \(\lim_{t\to\infty}\tilde{D}_{t}=0\) on \(E\). Thus, by Lemma A.2, we get

\[\lim_{t\to\infty}\mathbf{X}_{t}=\mathbf{X}^{*}\,\,\,\text{ on the event }E\] (D.49)

and setting \(\mathcal{U}=\mathcal{U}_{\varepsilon/4}\), we obtain

\[\mathbb{P}\Big{(}\lim_{t\to\infty}\mathbf{X}_{t}=\mathbf{X}^{*}\Big{)}\geq 1- \eta\quad\text{whenever}\,\mathbf{X}_{1}\in\mathcal{U}.\] (D.50)

This concludes our discussion and our proof. 

## Appendix E Numerical experiments

In this last appendix, we provide a series of additional numerical simulations to validate and explore the performance of (MMW) with payoff-based feedback.

Trajectory analysis.First, we proceed to a trajectory analysis of the game setup presented in Section 7. Specifically, in Fig. 2, we provide a visualization of the actual trajectories of play generated by the three methods with the same parameters as before, for different initial conditions. The trajectories are presented in Bloch spheres [48], where the points \(\ket{0}\) and \(\ket{1}\) in the figure correspond to the density matrices

\[\ket{0}=\begin{pmatrix}1&0\\ 0&0\end{pmatrix}\qquad\text{and}\qquad\ket{1}=\begin{pmatrix}0&0\\ 0&1\end{pmatrix}\] (E.1)

respectively. In all figures, the points in red indicate the trajectory of Player 1, while the points in blue are for Player 2. The initial points of the red trajectories are marked with \(\bullet\), while the initial points of the blue ones are marked with \(\blacksquare\). [Each column of Bloch spheres in Fig. 2 has the same initial conditions.]

An important remark here is that, as suggested by Theorem 4, the trajectories of all methods converge - and quite rapidly at that - to the game's (strict) Nash equilibrium. In fact, given that the trajectories converge to a pure state, this goes to explain the faster convergence rates observed in Fig. 1: instead

Figure 2: Trajectories of the three methods for different initial conditions. The red points correspond to player 1, and the blue points to player 2. The initial points of the red trajectories are marked with \(\bullet\), while the initial points of the blue ones are marked with \(\blacksquare\).

of oscillating around a solution, the MMW orbits actually _converge_ to equilibrium in this case, so the trailing average converges at a much faster rate. This holds in all zero-sum games with a pure equilibrium, thus indicating a very important class of zero-sum games where the worst-case guarantees of MMW algorithms can be significantly improved.

**Convergence speed analysis.**  In addition to the game setup described in Section 7, we consider the following quantum games:

* \(\mathcal{Q}_{2}\): quantum analogue of the \(2\times 2\) min-max game with payoff matrix \[P_{2}=\begin{pmatrix}(10,-10)&(10,-10)\\ (-10,10)&(-10,10)\end{pmatrix}\] ( \[\mathcal{Q}_{2}\] )
* \(\mathcal{Q}_{3}\): quantum analogue of the \(3\times 3\) min-max game with payoff \[P_{3}=\begin{pmatrix}(4,-4)&(2,-2)&(4,-4)\\ (-4,4)&(-2,2)&(-4,-4)\\ (-4,4)&(-2,2)&(-4,-4)\end{pmatrix}\] ( \[\mathcal{Q}_{3}\] )
* \(\mathcal{Q}_{4}\): quantum analogue of the \(3\times 3\) min-max game with payoff \[P_{4}=\begin{pmatrix}(10,-10)&(10,-10)&(10,-10)\\ (-10,10)&(-10,10)&(-10,10)\\ (-10,10)&(-10,10)&(-10,10)\end{pmatrix}\] ( \[\mathcal{Q}_{4}\] )

Figure 3: Performance evaluation of (3MW) with estimators provided by (2PE) and (1PE), and comparison with the full information algorithm (MMW).

In Fig. 3, we evaluate the convergence properties of (3MW) using the estimators (2PE) and (1PE), and compare it with the full information variant (MMW), following the same setup as described in Section 7. Specifically, for each method, we perform 10 different runs, with \(T=10^{5}\) steps each, and compute the mean value of the duality gap as a function of the iteration \(t=1,2,\ldots,T\). The solid lines correspond to the mean values of the duality gap of each method, and the shaded regions enclose the area of \(\pm 1\) (sample) standard deviation among the 10 different runs. Note that the red line, which corresponds to the full information (MMW), does not have a shaded region, since there is no randomness in the algorithm. All the runs for the three different methods were initialized for \(\mathbf{Y}=0\) and we used \(\gamma=10^{-2}\) for all methods. In particular, for (3MW) with gradient estimates given by (2PE) estimator, we used a sampling radius \(\delta=10^{-2}\), and for (3MW) with (1PE) estimator, we used \(\delta=10^{-1}\) (in tune with our theoretical results which suggest the use of a tighter sampling radius when mixed payoff information is available to the players). As highlighted in the main text, we observe that the decrease in performance is mild, and the different algorithms achieved better rates than their theoretical guarantees.