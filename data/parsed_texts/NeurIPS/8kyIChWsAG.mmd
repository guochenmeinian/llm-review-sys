# When Does Optimizing a Proper Loss Yield Calibration?

 Jaroslaw Blasiok

Columbia University

jb4451@columbia.edu &Parikshit Gopalan

Apple

parik.g@gmail.com &Lunjia Hu

Stanford University

lunjia@stanford.edu &Preetum Nakkiran

Apple

preetum.nakkiran@gmail.com

###### Abstract

Optimizing proper loss functions is popularly believed to yield predictors with good calibration properties; the intuition being that for such losses, the global optimum is to predict the ground-truth probabilities, which is indeed calibrated. However, typical machine learning models are trained to approximately minimize loss over restricted families of predictors, that are unlikely to contain the ground truth. Under what circumstances does optimizing proper loss over a restricted family yield calibrated models? What precise calibration guarantees does it give? In this work, we provide a rigorous answer to these questions. We replace the global optimality with a local optimality condition stipulating that the (proper) loss of the predictor cannot be reduced much by post-processing its predictions with a certain family of Lipschitz functions. We show that any predictor with this local optimality satisfies smooth calibration as defined in Kakade and Foster (2008); Blasiok et al. (2023b). Local optimality is plausibly satisfied by well-trained DNNs, which suggests an explanation for why they are calibrated from proper loss minimization alone. Finally, we show that the connection between local optimality and calibration error goes both ways: nearly calibrated predictors are also nearly locally optimal.

## 1 Introduction

In supervised prediction with binary labels, two basic criteria by which we judge the quality of a predictor are accuracy and calibration. Given samples from a distribution \(\mathcal{D}\) on \(\mathcal{X}\times\{0,1\}\) that corresponds to points \(x\) from \(\mathcal{X}\) with binary labels \(y\in\{0,1\}\), we wish to learn a predictor \(f\) that assigns each \(x\) a probability \(f(x)\in[0,1]\) that the label is \(1\). Informally, accuracy measures how close the predictor \(f\) is to the ground-truth \(f^{*}(x)=\mathbb{E}[y|x]\).1 Calibration (Dawid, 1982; Foster and Vohra, 1998) is an interpretability notion originating from the literature on forecasting, which stipulates that the predictions of our model be meaningful as probabilities. For instance, on all points where \(f(x)=0.4\), calibration requires the true label to be \(1\) about \(40\%\) of the time. Calibration and accuracy are complementary notions: a reasonably accurate predictor (which is still far from optimal) need not be calibrated, and calibrated predictors (e.g. predicting the average) can have poor accuracy. The notions are complementary but not necessarily conflicting; for example, the ground truth itself is both optimally accurate and perfectly calibrated.

Footnote 1: Here we focus on the accuracy of the predictor \(f\) as opposed to the accuracy of a classifier induced by \(f\).

Proper losses are central to the quest for accurate approximations of the ground truth. Informally a proper loss ensures that under binary labels \(y\) drawn from the Bernoulli distribution with parameter \(v^{*}\)the expected loss \(\mathbb{E}[\ell(y,v)]\) is minimized at \(v=v^{*}\) itself. Familiar examples include the squared loss \(\ell_{\text{sq}}(y,v)=(y-v)\)2 and the cross-entropy loss \(\ell_{\text{sent}}(y,v)=y\log(1/v)+(1-y)\log(1/(1-v))\). A key property of proper losses is that over the space of all predictors, the expected loss \(\mathbb{E}_{\mathcal{D}}[\ell(y,f(x))]\) is minimized by the ground truth \(f^{*}\) which also gives calibration. However minimizing over all predictors is infeasible. Hence typical machine learning deviates from this ideal in many ways: we restrict ourselves to models of bounded capacity such as decision trees or neural nets of a given depth and architecture, and we optimize loss using algorithms like SGD, which are only expected to find approximate local minima in the parameter space. What calibration guarantees transfer to this more restricted but realistic setting?

Footnote 2: [https://scikit-learn.org/stable/modules/calibration.html](https://scikit-learn.org/stable/modules/calibration.html). Log-loss is \(\ell_{\text{sent}}\) in our notation.

The State of Calibration & Proper Losses.There is a folklore belief in some quarters that optimizing a proper loss, even in the restricted setting, will produce a calibrated predictor. Indeed, the user guide for scikit-learn (Pedregosa et al., 2011) claims that _LogisticRegression returns well calibrated predictions by default as it directly optimizes Log loss_2. Before going further, we should point out that this statement in its full generality is just not true: **for general function families \(\mathcal{C}\), minimizing proper loss (even globally) over \(\mathcal{C}\) might not result in a calibrated predictor**. Even if the class \(\mathcal{C}\) contains perfectly calibrated predictors, the predictor found by loss minimization need not be (even close to) calibrated. A simple example showing that even logistic regression can fail to give calibrated predictors is provided in Appendix B.

Footnote 2: For simplicity, we avoid generalization concerns and consider the setting where we minimize loss directly on the population distribution. This is a reasonable simplification in many learning settings with sufficient samples, such as the 1-epoch training of many Large-Language-Models.

Other papers suggest a close relation between minimizing a proper loss and calibration, but stop short of formalizing or quantifying the relationship. For example, in deep learning, Lakshminarayanan et al. (2017) states _calibration can be incentivised by proper scoring rules_. There is also a large body of work on post-hoc recalibration methods. The recipe here is to first train a predictor via loss minimization over the training set, then compose it with a simple post-processing function chosen to optimize cross-entropy on a holdout set (Platt, 1999; Zadrozny and Elkan, 2002). See for instance Platte scaling (Platt, 1999), where the post-processing functions are sigmoids, and which continues to be used in deep learning (e.g. for temperature scaling as in Guo et al. (2017)). Google's data science practitioner's guide recommends that, for recalibration methods, _the calibration [objective] function should minimize a strictly proper scoring rule_(Richardson and Pospisil, 2021). In short, these works suggest recalibration by minimizing a proper loss, sometimes in conjunction with a simple family of post-processing functions. However, there are no rigorous bounds on the calibration error using these methods, nor is there justification for a particular choice of post-processing functions.

Yet despite the lack of rigorous bounds, there are strong hints from practice that in certain settings, optimizing a proper-loss does indeed yield calibrated predictors. Perhaps the strongest evidence comes from the newest generation of deep neural networks (DNNs). These networks are trained to minimize a proper loss (usually cross-entropy) using SGD or variants, and are typically quite far from the ground truth, yet they turn out to be surprisingly well-calibrated. This empirical phenomenon occurs in both modern image classifiers (Minderer et al., 2021; Hendrycks* et al., 2020), and language models (Desai and Durrett, 2020; OpenAI, 2023). The situation suggests that there is a key theoretical piece missing in our understanding of calibration, to capture why models can be so well-calibrated "out-of-the-box." This theory must be nuanced: not all ways of optimizing a proper loss with DNNs yields calibration. For example, the previous generation of image classifiers were poorly calibrated (Guo et al., 2017), though the much smaller networks before them were well-calibrated (Niculescu-Mizil and Caruana, 2005). Whether DNNs are calibrated, then depends on the architecture, distribution, and training algorithm-- and our theory must be compatible with this.

In summary, prior work suggests a close but complicated relationship between minimizing proper loss and calibration. On one hand, for simple models like logistic regression, proper loss minimization does not guarantee calibration. On the other hand, for DNNs, certain ways (but not all ways) of optimizing a proper loss appears to yield well-calibrated predictors. The goal of our work is to analyze this phenomena from a theoretical perspective.3 Our motivating question is: **What minimal conditions on model family and training procedure guarantee that optimizing for a proper loss provably yields small calibration error?**

### Our Contributions

The main contribution of this work is to identify a **local optimality condition** for a predictor that is both necessary and sufficient to guarantee calibration. The local optimality condition requires that the loss \(\ell\) cannot be reduced significantly through post-processing using a family of functions \(K_{\ell}\) that have certain Lipschitz-ness guarantees, where post-processing means applying a function \(\kappa:[0,1]\to[0,1]\) to the output of the predictor. This condition is illustrated in Figure 1; it is distinct from the standard local optimality condition of vanishing gradients. We prove that a predictor satisfying this local optimality condition is smoothly calibrated in the sense of Kakade and Foster (2008); Gopalan et al. (2022b), Blasiok et al. (2023b). Smooth calibration is a _consistent_ calibration measure that has several advantages over the commonly used Expected Calibration Error (ECE); see the discussion in Kakade and Foster (2008), Blasiok et al. (2023b). Quantitatively for the case of the squared loss, we present a tight connection between smooth calibration error and the reduction in loss that is possible from post-processing with \(K_{\ell}\). For other proper loss functions \(\ell\), we give a tight connection between post-processing with \(K_{\ell}\) and a calibration notion we call dual smooth calibration. The fact that the connection goes both ways implies that a predictor that does not satisfy our local optimality condition is far from calibrated, so we have indeed identified the minimal conditions needed to ensure calibration.

**Implications.** Heuristically, we believe these theoretical results shed light on why modern DNNs are often calibrated in practice. There are at least three possible mechanisms, at varying levels of formality.

First, informally: poor calibration implies that the test loss can be reduced noticeably by post-processing with a _simple_ function. But simple post-processings can be represented by adding a few layers to the DNN-- so we could plausibly improve the test loss by adding a few layers and re-training (or continuing training) with SGD. Thus, if our DNN has been trained to the point where such easy gains are not left on the table (as we expect of state-of-the-art models), then it is well-calibrated.

The second possible mechanism follows from one way of formalizing the above heuristic, yielding natural learning algorithms that provably achieve calibration. Specifically, consider Structural Risk Minimization (SRM): globally minimizing a proper loss plus a complexity regularizer, within some restricted function family. In Section 5, we prove SRM is guaranteed to produce a calibrated predictor, provided: (1) the function family is closed under post-processing and (2) the complexity regularizer grows only mildly under post-processing. If the informal "implicit regularization hypothesis" in deep learning is true, and SGD on DNNs is equivalent to SRM with an appropriate complexity-regularizer (Arora et al., 2019; Neyshabur et al., 2014; Neyshabur, 2017; Zhang et al., 2021), then our results imply DNNs are well-calibrated as long as the regularizer satisfies our fairly mild assumptions.

Finally, a third mechanism for calibration involves a heuristic assumption about the optimizer (SGD). Informally, for a sufficiently deep network, updating the network parameters from computing the function \(f\) to computing the post-processed \((\kappa\circ f)\) may be a "simple" update for SGD. Thus, if it were possible to improve the loss via such a simple post-processing, then SGD would have already exploited this by the end of training.

Figure 1: Schematic depiction of the loss landscape of a proper loss on the population distribution. The red dot represents a well-calibrated predictor \(f\). The blue curve represents all predictors \(\{\kappa\circ f:\kappa\in K_{\ell}\}\) obtained by admissible post-processing of \(f\). Since the proper loss cannot be decreased by post-processing (the red dot is a minimal point on the blue curve), the predictor \(f\) is well-calibrated, even though it is not a global minimum.

These algorithms and intuitions also suggest a practical guidance: for calibration, one should optimize loss over function families that are capable of computing rich classes of post-processing functions. The universality properties of most DNN architectures imply they can compute a rich family of post-processing functions with small added depth (Cybenko, 1989; Lu et al., 2017; Yun et al., 2019; Zhou, 2020) but the family of functions used in logistic regression (with logits restricted to being affine in the features) cannot.

Our results are consistent with the calibration differences between current and previous generation models, when viewed through the lens of generalization. In both settings, models are expected to be locally-optimal with respect to _train loss_(Carrell et al., 2022), since this is the explicit optimization objective. Calibration, however, requires local-optimality with respect to _test loss_. Previous generation image classification models were trained to interpolate on small datasets, so optimizing the train loss was very different from optimizing the test loss (e.g. Guo et al., 2017; Mukhoti et al., 2020). Current generation models, in contrast, are trained on massive datasets, where optimizing the train loss is effectively equivalent to optimizing the test loss-- and so models are close to locally-optimal with respect to both, implying calibration.

**Organization.** We first give a technical overview of our results in Section 2. In Section 3 we prove our main result in the case of square-loss. Then in Section 4 we extend this result to a wide family of proper losses. Finally, in Section 5 we present natural algorithms which provably achieve calibration, as a consequence by our results. Additional related works are presented in Appendix A.

## 2 Overview

In this section, we present a formal but still high-level overview of our results. We first set some notation. All our probabilities and expectations are taken with respect to a distribution \(\mathcal{D}\) over \(\mathcal{X}\times\{0,1\}\). A predictor is a function \(f:\mathcal{X}\to[0,1]\) that assigns probabilities to points, the ground-truth predictor is \(f^{*}(x)=\mathbb{E}[y|x]\). A predictor is perfectly calibrated if the set \(A=\{v\in[0,1]:\mathbb{E}[y|f(x)=v]\neq v\}\) has measure \(0\) (i.e. \(\Pr_{\mathcal{D}}(f(x)\in A)=0\)). A post-processing function is a function \(\kappa:[0,1]\to[0,1]\). A loss function is a function \(\ell:\{0,1\}\times[0,1]\to\mathbb{R}\), where \(\ell(y,v)\) represents the loss suffered when we predict \(v\in[0,1]\) and the label is \(y\in\{0,1\}\). Such a loss is _proper_ if when \(y\sim\text{Ber}(v^{*})\) is sampled from the Bernoulli distribution with parameter \(v^{*}\), the expected loss \(\mathbb{E}[\ell(y,v)]\) is minimized at \(v=v^{*}\), and we say a loss is _strictly_ proper if \(v^{*}\) is the unique minimizer.

As a warm-up, we present a characterization of perfect calibration in terms of perfect local optimally over the space of all possible (non-Lipschitz) post-processing functions.

**Claim 2.1**.: _For every strictly proper loss \(\ell\), a predictor \(f\) is perfectly calibrated iff for every post-processing function \(\kappa\),_

\[\operatorname*{\mathbb{E}}_{\mathcal{D}}[\ell(y,f(x))]\leq\operatorname*{ \mathbb{E}}_{\mathcal{D}}[\ell(y,\kappa(f(x)))]. \tag{1}\]

This is true, because if \(f\) is perfectly calibrated, and \(\kappa\) arbitrary, then after conditioning on arbitrary prediction \(f(x)\) we have:

\[\mathbb{E}[\ell(y,f(x))|f(x)=v]=\operatorname*{\mathbb{E}}_{y\sim\text{Ber}(v )}[\ell(y,v)]\leq\operatorname*{\mathbb{E}}_{y\sim\text{Ber}(v)}[\ell(y,\kappa (v))],\]

and (1) follows by averaging over \(v\). On the other hand if \(f\) is not perfectly calibrated, we can improve the expected loss by taking \(\kappa(v):=\mathbb{E}[y|f(x)=v]\) -- by the definition of the strictly proper loss, for each \(v\) we will have \(\mathbb{E}[\ell(y,\kappa(f(x))|f(x)=v]\leq\mathbb{E}[\ell(y,f(x))|f(x)=v]\), and on a set of positive probability the inequality will be strict.

While we are not aware of this precise claim appearing previously, statements that are similar in spirit are known; for instance, by the seminal work of Foster and Vohra (1998) which characterizes calibration in terms of swap regret for the squared loss. In our language they show the equivalent Claim 2.1 for \(\ell\) being a squared loss, and \(\kappa\) restricted to be of the form \(\kappa(v_{0}):=w_{0}\), \(\kappa(v):=v\) for all \(v\neq v_{0}\).

Claim 2.1 connects calibration to a local optimality condition over the space of all possible post-processing functions. If this is satisfied, it guarantees calibration even though the loss might be far from the global minimum. This sheds light on classes of predictors such as decision trees or branching programs, which are closed under composition with arbitrary post-processing functions,since this amounts to relabelling the leaf nodes. It tells us that such models ought to be calibrated from proper loss minimization, as long as the optimization achieves a fairly weak notion of local optimality.

The next step is to ask whether DNNs could satisfy a similar property. Closure under arbitrary post-processing seems too strong a condition for DNNs to satisfy -- we should not expect DNNs to be able to express arbitrarily discontinuous uni-variate post-processing functions. But we do not expect or require DNNs to be perfectly calibrated, only to be close to perfectly calibrated. Measuring the distance from calibration involves subtle challenges as highlighted in the recent work of Blasiok et al. (2023). This leads us to the following robust formulation which significantly generalizes Claim 2.1:

* Using the well known relation between proper losses and Bregman divergences (Savage, 1971; Gneiting and Raftery, 2007), we consider proper loss functions where the underlying convex functions satisfy certain differentiability and smoothness properties. This is a technical condition, but one that is satisfied by all proper loss functions commonly used in practice.
* We only allow post-processing functions that satisfy certain Lipschitzness properties. Such functions are simple enough to be implementable by DNNs of constant depth. Indeed we heuristically believe that state-of-the-art DNNs will automatically be near-optimal w.r.t. such post-processings, as mentioned in Section 1.1.
* We measure calibration distance using the notion of smooth calibration from Kakade and Foster (2008) and strengthenings of it in Gopalan et al. (2022), Blasiok et al. (2023). Smooth calibration plays a central role in the consistent calibration measures framework of Blasiok et al. (2023), where it is shown using duality that the smCE is linearly related with the Wasserstein distance to the nearest perfectly calibrated predictor (Blasiok et al., 2023). In practice, there exist sample-efficient and linear-time estimators which approximate the smooth calibration error within a quadratic factor (Blasiok et al., 2023).

With these notions in place, we show that the _post-processing gap_ of a predictor, which measures how much the loss can be reduced via certain Lipschitz post-processing functions provides both an upper and a lower bound on the smooth calibration error. This result gives us a robust and quantitative version of Claim 2.1. The main technical challenge in formulating this equivalence is that for general \(\ell\), the class of post-processing functions that we allow and the class of smooth functions that we use to measure calibration error are both now dependent on the loss function \(\ell\), they are Lipschitz over an appropriately defined dual space to the predictions defined using convex duality. In order to keep the statements simple, we state our results for the special case of squared loss and cross entropy loss below, defering the full statement to the technical sections.

Squared loss.For the squared loss \(\ell_{\mathsf{sq}}(y,v)=(y-v)^{2}\), we define the post-processing gap \(\mathsf{pGap}_{\mathcal{D}}(f)\) to be the difference between the expected squared loss \(\mathbb{E}[\ell_{\mathsf{sq}}(y,f(x))]\) of a predictor \(f\) and the minimum expected squared loss after we post-process \(f(x)\mapsto f(x)+\eta(f(x))\) for some \(1\)-Lipschitz \(\eta\). The Brier score (Brier et al., 1950; Foster and Vohra, 1998) uses the squared loss of a predictor as a calibration measure; \(\mathsf{pGap}\) can be viewed as a _differential_ version of Brier score.

**Definition 2.2** (Post-processing gap).: _Let \(K\) denote the family of all post-processing functions \(\kappa:[0,1]\to[0,1]\) such that the update function \(\eta(v)=\kappa(v)-v\) is \(1\)-Lipschitz. For a predictor \(f:\mathcal{X}\to[0,1]\) and a distribution \(\mathcal{D}\) over \(\mathcal{X}\times\{0,1\}\), we define the post-processing gap of \(f\) w.r.t. \(\mathcal{D}\) to be_

\[\mathsf{pGap}_{\mathcal{D}}(f):=\mathbb{E}_{(x,y)\sim\mathcal{D}}[\ell_{\mathsf{ sq}}(y,f(x))]-\inf_{\kappa\in K}\mathbb{E}_{(x,y)\sim\mathcal{D}}[\ell_{ \mathsf{sq}}(y,\kappa(f(x)))].\]

We define the smooth calibration error \(\mathsf{smCE}_{\mathcal{D}}(f)\) as in Kakade and Foster (2008); Gopalan et al. (2022), Blasiok et al. (2023) to be the maximum correlation between \(y-f(x)\) and \(\eta(f(x))\) over all bounded \(1\)-Lipschitz functions \(\eta\). Smooth calibration is a _consistent_ measure of calibration (Blasiok et al., 2023). It does not suffer the discontinuity problems of ECE (Kakade and Foster, 2008), and is known to be linearly related with the Wasserstein distance to the nearest perfectly calibrated predictor (Blasiok et al. (2023)). In particular, it is \(0\) if and only if we have perfect calibration. We refer the reader to these papers for a detailed discussion of its merits.

**Definition 2.3** (Smooth calibration error).: _Let \(H\) be the family of all \(1\)-Lipschitz functions \(\eta:[0,1]\to[-1,1]\). The smooth calibration error of a predictor \(f:\mathcal{X}\to[0,1]\) with respect to distribution \(\mathcal{D}\)over \(\mathcal{X}\times\{0,1\}\) is defined as_

\[\mathsf{smCE}_{\mathcal{D}}(f):=\sup_{\eta\in H}\mathbb{E}_{(x,y)\sim\mathcal{D}}[ (y-f(x))\eta(f(x))].\]

Our main result is a quadratic relationship between these two quantities:

**Theorem 2.4**.: _For any predictor \(f:\mathcal{X}\to[0,1]\) and any distribution \(\mathcal{D}\) over \(\mathcal{X}\times\{0,1\}\),_

\[\mathsf{smCE}_{\mathcal{D}}(f)^{2}\leq\mathsf{pGap}_{\mathcal{D}}(f)\leq 2\, \mathsf{smCE}_{\mathcal{D}}(f).\]

In Appendix D we show that the constants in the inequality above are optimal.

Cross-entropy loss.For the cross entropy loss \(\ell_{\mathsf{sent}}(y,v)=-y\ln v-(1-y)\ln(1-v)\), we observe its close connection to the logistic loss \(\ell^{(\psi)}(y,t)=\ln(1+e^{t})-yt\) given by the equation

\[\ell_{\mathsf{sent}}(y,v)=\ell^{(\psi)}(y,t) \tag{2}\]

where \(t:=\ln(v/(1-v))\) is what we call the _dual prediction_ corresponding to \(v\). While a standard terminology for \(t\) is _logit_, we say \(t\) is the dual prediction because this notion generalizes to arbitrary proper loss functions. One can conversely obtain a prediction \(v\) from its dual prediction \(t\) by taking the sigmoid transformation: \(v=\sigma(t):=1/(1+e^{-t})\). The superscript \(\psi\) in the logistic loss \(\ell^{(\psi)}\) can be understood to indicate its relationship to the dual prediction and the exact meaning is made clear in Section 4. Based on (2), optimizing the cross-entropy loss \(\ell_{\mathsf{sent}}\) over predictions \(v\) is equivalent to optimizing the logistic loss \(\ell^{(\psi)}\) over dual predictions \(t\).

Usually, a neural network that aims to minimize the cross-entropy loss has a last layer that computes the sigmoid \(\sigma\) (the binary version of softmax), so the value computed by the network before the sigmoid (the "logit") is the dual prediction. If we want to enhance the neural network by adding more layers, these layers are typically inserted before the final sigmoid transformation. It is thus more natural to consider post-processings on the dual predictions (logits) rather than on the predictions themselves.

For a predictor \(f\), we define the _dual post-processing gap_\(\mathsf{pGap}^{(\psi,1/4)}(g)\) of its dual predictor \(g(x)=\ln(f(x)/(1-f(x)))\) to be the difference between the expected logistic loss \(\mathbb{E}[\ell^{(\psi)}(y,g(x))]\) of \(g\) and the minimum expected logistic loss after we post-process \(g(x)\mapsto g(x)+\eta(g(x))\) for some \(1\)-Lipschitz \(\eta:\mathbb{R}\to[-4,4]\), where the constant \(4\) comes from the fact that the logistic loss is \(1/4\)-smooth in \(t\).

**Definition 2.5** (Dual post-processing gap for cross-entropy loss, special case of Definition 4.4).: _Let \(K\) denote the family of all post-processing functions \(\kappa:\mathbb{R}\to\mathbb{R}\) such that the update function \(\eta(t):=\kappa(t)-t\) is \(1\)-Lipschitz and bounded \(|\eta(t)|\leq 4\). Let \(\ell^{(\psi)}\) be the logistic loss. Let \(\mathcal{D}\) be a distribution over \(\mathcal{X}\times\{0,1\}\). We define the dual post-processing gap of a function \(g:\mathcal{X}\to\mathbb{R}\) to be_

\[\mathsf{pGap}^{(\psi,1/4)}_{\mathcal{D}}(g):=\mathbb{E}_{(x,y)\sim\mathcal{D}} \,\ell^{(\psi)}(y,g(x))-\inf_{\kappa\in K}\mathbb{E}_{(x,y)\sim\mathcal{D}}\, \ell^{(\psi)}\big{(}y,\kappa(g(x))\big{)}.\]

We define the _dual smooth calibration error_\(\mathsf{smCE}^{(\psi,1/4)}(g)\) to be the maximum of \(|\,\mathbb{E}[(y-f(x))\eta(g(x))]|\) over all \(1/4\)-Lipschitz functions \(\eta:\mathbb{R}\to[-1,1]\). Like with smooth calibration, it is \(0\) if and only if the predictor \(f\) is perfectly calibrated.

**Definition 2.6** (Dual smooth calibration for cross-entropy loss, special case of Definition 4.5).: _Let \(H\) be the family of all \(1/4\)-Lipschitz functions \(\eta:\mathbb{R}\to[-1,1]\). For a function \(g:\mathcal{X}\to\mathbb{R}\), define predictor \(f:\mathcal{X}\to[0,1]\) such that \(f(x)=\sigma(g(x))\) for every \(x\in\mathcal{X}\) where \(\sigma\) is the sigmoid transformation. Let \(\mathcal{D}\) be a distribution over \(\mathcal{X}\times\{0,1\}\). We define the dual calibration error of \(g\) as_

\[\mathsf{smCE}^{(\psi,1/4)}_{\mathcal{D}}(g):=\sup_{\eta\in H}|\,\mathbb{E}_{( x,y)\sim\mathcal{D}}[(y-f(x))\eta(g(x))]|.\]

We show a result similar to Theorem 2.4 that the dual post-processing gap and the dual smooth calibration error are also quadratically related. Moreover, we show that a small dual smooth calibration error implies a small (standard) smooth calibration error.

**Corollary 2.7** (Corollary of Theorem 4.6 and Lemma 4.7).: _Let \(\mathsf{pGap}^{(\psi,1/4)}\) and \(\mathsf{smCE}^{(\psi,1/4)}\) be defined as in Definition 2.5 and Definition 2.6 for the cross-entropy loss. For any function \(g:\mathcal{X}\to\mathbb{R}\) and any distribution \(\mathcal{D}\) over \(\mathcal{X}\times\{0,1\}\),_

\[2\,\mathsf{smCE}^{(\psi,1/4)}_{\mathcal{D}}(g)^{2}\leq\mathsf{pGap}^{(\psi,1/ 4)}_{\mathcal{D}}(g)\leq 4\,\mathsf{smCE}^{(\psi,1/4)}_{\mathcal{D}}(g). \tag{3}\]_Moreover, let predictor \(f:\mathcal{X}\to[0,1]\) be given by \(f(x)=\sigma(g(x))\) for the sigmoid transformation \(\sigma\). Its (standard) smooth calibration error \(\mathsf{smCE}_{\mathcal{D}}(f)\) defined in Definition 2.3 satisfies_

\[\mathsf{smCE}_{\mathcal{D}}(f)\leq\mathsf{smCE}_{\mathcal{D}}^{(\psi,1/4)}(g). \tag{4}\]

The constants in the corollary above are optimal as we show in Lemmas D.3 and D.4 in the appendix. Both results (3) and (4) generalize to a wide class of proper loss functions as we show in Section 4.

**Remark 2.8**.: _In a subsequent work, Blasiok and Nakkiran [2023] proved the reverse direction of (4) for the cross-entropy loss: \(\mathsf{smCE}_{\mathcal{D}}(f)\geq\Omega\big{(}\mathsf{smCE}_{\mathcal{D}}^{( \psi,1/4)}(g)^{2}\big{)}\). Hence both \(\mathsf{smCE}^{(\psi,1/4)}(g)\) and \(\mathsf{pGap}^{(\psi,1/4)}(g)\) are consistent calibration measures, a notion introduced by Blasiok et al. [2023b]._

Our result shows that achieving a small dual post-processing gap when optimizing a cross-entropy is necessary and sufficient for good calibration guarantees. It sheds light on the examples where logistic regression fails to yield a calibrated predictor; in those instances the cross-entropy loss can be further reduced by some Lipschitz post-processing of the logit. This is intuitive because logistic regression only optimizes cross-entropy within the restricted class where the logit is a linear combination of the features plus a bias term, and this class is not closed under Lipschitz post-processing.

## 3 Calibration and Post-processing Gap for the Squared Loss

In this section we prove our main result Theorem 2.4 relating the smooth calibration error of a predictor to its post-processing gap with respect to the squared loss.

Proof of Theorem 2.4.: We first prove the upper bound on \(\mathsf{pGap}_{\mathcal{D}}(f)\). For any \(\kappa\in K\) in the definition of \(\mathsf{pGap}\) (Definition 2.2), there exists a \(1\)-Lipschitz function \(\eta:[0,1]\to[-1,1]\) such that \(\kappa(v)=v+\eta(v)\) for every \(v\in[0,1]\). For the squared loss \(\ell\),

\[\mathbb{E}_{(x,y)\sim\mathcal{D}}[\ell(y,\kappa(f(x)))] =\mathbb{E}[(y-f(x)-\eta(f(x)))^{2}]\] \[=\mathbb{E}[(y-f(x))^{2}]-2\,\mathbb{E}[(y-f(x))\eta(f(x))]+\mathbb{ E}[\eta(f(x))^{2}]. \tag{5}\]

The three terms on the right hand side satisfy

\[\mathbb{E}[(y-f(x))^{2}] =\mathbb{E}[\ell(y,f(x))],\] \[\mathbb{E}[(y-f(x))\eta(f(x))] \leq\mathsf{smCE}_{\mathcal{D}}(f),\] \[\mathbb{E}[\eta(f(x))^{2}] \geq 0.\]

Plugging these into (5), we get

\[\mathbb{E}[\ell(y,f(x))]-\mathbb{E}[\ell(y,\kappa(f(x)))]\leq 2\mathsf{smCE}_{ \mathcal{D}}(f).\]

Since this inequality holds for any \(\kappa\in K\), we get \(\mathsf{pGap}_{\mathcal{D}}(f)\leq 2\mathsf{smCE}_{\mathcal{D}}(f)\) as desired.

Now we prove the lower bound on \(\mathsf{pGap}_{\mathcal{D}}(f)\). For any \(1\)-Lipschitz function \(\eta:[0,1]\to[-1,1]\), define \(\beta:=\mathbb{E}[(y-f(x))\eta(f(x))]\in[-1,1]\) and define post-processing \(\kappa:[0,1]\to[0,1]\) such that

\[\kappa(v)=\mathsf{proj}_{[0,1]}(v+\beta\eta(v))\quad\text{for every $v\in[0,1]$},\]

where \(\mathsf{proj}_{[0,1]}(u)\) is the value in \([0,1]\) closest to \(u\), i.e., \(\mathsf{proj}_{[0,1]}(u)=\min(\max(u,0),1)\). By Lemma H.2, we have \(\kappa\in K\). The expected squared loss after we apply the post-processing \(\kappa\) can be bounded as follows:

\[\mathbb{E}_{(x,y)\sim\mathcal{D}}[\ell(y,\kappa(f(x)))] \leq\mathbb{E}[(y-f(x)-\beta\eta(f(x)))^{2}]\] \[=\mathbb{E}[(y-f(x))^{2}]-2\beta\,\mathbb{E}[(y-f(x))\eta(f(x))]+ \beta^{2}\,\mathbb{E}[\eta(f(x))^{2}]\] \[=\mathbb{E}[\ell(y,f(x))]-2\beta^{2}+\beta^{2}\,\mathbb{E}[\eta( f(x))^{2}]\] \[\leq\mathbb{E}[\ell(y,f(x))]-2\beta^{2}+\beta^{2}.\]

Re-arranging the inequality above, we have

\[\mathbb{E}[(y-f(x))\eta(f(x))]^{2}=\beta^{2}\leq\mathbb{E}[\ell(y,f(x))]- \mathbb{E}[\ell(y,\kappa(f(x)))]\leq\mathsf{pGap}_{\mathcal{D}}(f).\]

Since this inequality holds for any \(1\)-Lipschitz function \(\eta:[0,1]\to[-1,1]\), we get \(\mathsf{smCE}_{\mathcal{D}}(f)^{2}\leq\mathsf{pGap}_{\mathcal{D}}(f)\), as desired. 

In Appendix D we provide examples showing that the constants in Theorem 2.4 are optimal.

Generalization to Any Proper Loss

When we aim to minimize the squared loss, we have shown that achieving a small post-processing gap w.r.t. Lipschitz post-processings ensures a small smooth calibration error and vice versa. In this section, we extend this result to a wide class of _proper_ loss functions including the popular cross-entropy loss.

**Definition 4.1**.: _Let \(V\subseteq[0,1]\) be a non-empty interval. We say a loss function \(\ell:\{0,1\}\times V\to\mathbb{R}\) is proper if for every \(v\in V\), it holds that \(v\in\operatorname*{argmin}_{v^{\prime}\in V}\mathbb{E}_{y\sim\mathsf{Ber}(v)}[ \ell(y,v^{\prime})]\)._

One can easily verify that the squared loss \(\ell(y,v)=(y-v)^{2}\) is a proper loss function over \(V=[0,1]\), and the cross entropy loss \(\ell(y,v)=-y\ln v-(1-y)\ln(1-v)\) is a proper loss function over \(V=(0,1)\).

It turns out that if we directly replace the squared loss in Definition 2.2 with an arbitrary proper loss, we do not get a similar result as Theorem 2.4. In Appendix C we give a simple example where the post-processing gap w.r.t. the cross-entropy loss can be arbitrarily larger than the smooth calibration error. Thus new ideas are needed to extend Theorem 2.4 to general proper loss functions. We leverage a general theory involving correspondence between proper loss functions and convex functions (Shufford et al., 1966; Savage, 1971; Schervish, 1989; Buja et al., 2005). We provide a detailed description of this theory in Appendix E. There we include a proof of Lemma E.4 which implies the following lemma:

**Lemma 4.2**.: _Let \(V\subseteq[0,1]\) be a non-empty interval. Let \(\ell:\{0,1\}\times V\to\mathbb{R}\) be a proper loss function. For every \(v\in V\), define \(\mathsf{dual}(v):=\ell(0,v)-\ell(1,v)\). Then there exists a convex function \(\psi:\mathbb{R}\to\mathbb{R}\) such that_

\[\ell(y,v)=\psi(\mathsf{dual}(v))-y\,\mathsf{dual}(v)\quad\text{for every $y\in\{0,1\}$ and $v\in V$.} \tag{6}\]

_Moreover, if \(\psi\) is differentiable, then \(\nabla\psi(t)\in[0,1]\)._

Lemma 4.2 says that every proper loss induces a convex function \(\psi\) and a mapping \(\mathsf{dual}:V\to\mathbb{R}\) that maps a prediction \(v\in V\subseteq[0,1]\) to its _dual prediction_\(\mathsf{dual}(v)\in\mathbb{R}\). Our main theorem applies whenever the induced function \(\psi\) is differentiable, and \(\nabla\psi\) is \(\lambda\)-Lipschitz (equivalently \(\psi\) is \(\lambda\)-smooth) -- those are relatively mild conditions that are satisfied by all proper loss functions of interest.

The duality relationship between \(v\) and \(\mathsf{dual}(v)\) comes from the fact that each \((v,\mathsf{dual}(v))\) pair makes the Fenchel-Young divergence induced by a conjugate pair of convex functions \((\varphi,\psi)\) take its minimum value zero, as we show in Appendix E. Equation (6) expresses a proper loss as a function that depends on \(\mathsf{dual}(v)\) rather than directly on \(v\), and thus minimizing a proper loss over predictions \(v\) is equivalent to minimizing a corresponding _dual loss_ over dual predictions \(t=\mathsf{dual}(v)\):

**Definition 4.3** (Dual loss).: _For a function \(\psi:\mathbb{R}\to\mathbb{R}\), we define a dual loss function \(\ell^{(\psi)}:\{0,1\}\times\mathbb{R}\to\mathbb{R}\) such that_

\[\ell^{(\psi)}(y,t)=\psi(t)-yt\quad\text{for every $y\in\{0,1\}$ and $t\in\mathbb{R}$.}\]

_Consequently, if a loss function \(\ell:\{0,1\}\times V\to\mathbb{R}\) satisfies (6) for some \(V\subseteq[0,1]\) and \(\mathsf{dual}:V\to\mathbb{R}\), then_

\[\ell(y,v)=\ell^{(\psi)}(y,\mathsf{dual}(v))\quad\text{for every $y\in\{0,1\}$ and $v\in V$.} \tag{7}\]

The above definition of a dual loss function is essentially the definition of the Fenchel-Young loss in the literature (see e.g. Duchi et al., 2018; Blondel et al., 2020). A loss function \(\ell^{(\psi)}\) satisfying the relationship in (7) has been referred to as a _composite loss_(see e.g. Buja et al., 2005; Reid and Williamson, 2010).

For the cross-entropy loss \(\ell(y,v)=-y\ln v-(1-y)\ln(1-v)\), the corresponding dual loss is the logistic loss \(\ell^{(\psi)}(y,t)=\ln(1+e^{t})-yt\), and the relationship between a prediction \(v\in(0,1)\) and its dual prediction \(t=\mathsf{dual}(v)\in\mathbb{R}\) is given by \(v=\sigma(t)\) for the sigmoid transformation \(\sigma(t)=e^{t}/(1+e^{t})\).

For a predictor \(f:\mathcal{X}\to V\), we define its dual post-processing gap by considering dual predictions \(g(x)=\mathsf{dual}(f(x))\) w.r.t. the dual loss \(\ell^{(\psi)}\) as follows:

**Definition 4.4** (Dual post-processing gap).: _For \(\lambda>0\), let \(K_{\lambda}\) denote the family of all post-processing functions \(\kappa:\mathbb{R}\to\mathbb{R}\) such that there exists a \(1\)-Lipschitz function \(\eta:\mathbb{R}\to[-1/\lambda,1/\lambda]\) satisfying \(\kappa(t)=t+\eta(t)\) for every \(t\in\mathbb{R}\). Let \(\psi\) and \(\ell^{(\psi)}\) be defined as in Definition 4.3. Let \(\mathcal{D}\) be a distribution over \(\mathcal{X}\times\{0,1\}\). We define the dual post-processing gap of a function \(g:\mathcal{X}\to\mathbb{R}\) to be_

\[\mathsf{pGap}^{(\psi,\lambda)}_{\mathcal{D}}(g):=\mathbb{E}_{(x,y)\sim\mathcal{ D}}\,\ell^{(\psi)}(y,g(x))-\inf_{\kappa\in K_{\lambda}}\mathbb{E}_{(x,y)\sim \mathcal{D}}\,\ell^{(\psi)}\big{(}y,\kappa(g(x))\big{)}.\]

Definition 4.4 is a generalization of Definition 2.2 from the squared loss to an arbitrary proper loss corresponding to a function \(\psi\) as in (6). The following definition generalizes the definition of smooth calibration in Definition 2.3 to an arbitrary proper loss in a similar way. Here we use the fact proved in Appendix E (equation (20)) that if the function \(\psi\) from Lemma 4.2 for a proper loss \(\ell:\{0,1\}\times V\to\mathbb{R}\) is differentiable, then \(\nabla\psi(\mathsf{dual}(v))=v\) holds for any \(v\in V\), where \(\nabla\psi(\cdot)\) denotes the derivative of \(\psi\). This means that \(\nabla\psi\) transforms a dual prediction to its original prediction.

**Definition 4.5** (Dual smooth calibration).: _For \(\lambda>0\), let \(H_{\lambda}\) be the family of all \(\lambda\)-Lipschitz functions \(\eta:\mathbb{R}\to[-1,1]\). Let \(\psi:\mathbb{R}\to\mathbb{R}\) be a differentiable function with derivative \(\nabla\psi(t)\in[0,1]\) for every \(t\in\mathbb{R}\). For a function \(g:\mathcal{X}\to\mathbb{R}\), define predictor \(f:\mathcal{X}\to[0,1]\) such that \(f(x)=\nabla\psi(g(x))\) for every \(x\in\mathcal{X}\). Let \(\mathcal{D}\) be a distribution over \(\mathcal{X}\times\{0,1\}\). We define the dual calibration error of \(g\) to be_

\[\mathsf{smCE}^{(\psi,\lambda)}_{\mathcal{D}}(g):=\sup_{\eta\in H_{\lambda}}| \,\mathbb{E}_{(x,y)\sim\mathcal{D}}[(y-f(x))\eta(g(x))]|.\]

In Theorem 4.6 below we state our generalization of Theorem 2.4 to arbitrary proper loss functions. Theorem 4.6 shows that achieving a small dual post-processing gap is equivalent to achieving a small dual calibration error. We then show in Lemma 4.7 that a small dual calibration error implies a small (standard) smooth calibration error.

**Theorem 4.6**.: _Let \(\psi:\mathbb{R}\to\mathbb{R}\) be a differentiable convex function with derivative \(\nabla\psi(t)\in[0,1]\) for every \(t\in\mathbb{R}\). For \(\lambda>0\), assume that \(\psi\) is \(\lambda\)-smooth, i.e.,_

\[|\nabla\psi(t)-\nabla\psi(t^{\prime})|\leq\lambda|t-t^{\prime}|\quad\text{for every }t,t^{\prime}\in\mathbb{R}. \tag{8}\]

_Then for every \(g:\mathcal{X}\to\mathbb{R}\) and any distribution \(\mathcal{D}\) over \(\mathcal{X}\times\{0,1\}\),_

\[\mathsf{smCE}^{(\psi,\lambda)}_{\mathcal{D}}(g)^{2}/2\leq\lambda\,\mathsf{pGap }^{(\psi,\lambda)}_{\mathcal{D}}(g)\leq\mathsf{smCE}^{(\psi,\lambda)}_{ \mathcal{D}}(g).\]

**Lemma 4.7**.: _Let \(\psi:\mathbb{R}\to\mathbb{R}\) be a differentiable convex function with derivative \(\nabla\psi(t)\in[0,1]\) for every \(t\in\mathbb{R}\). For \(\lambda>0\), assume that \(\psi\) is \(\lambda\)-smooth as in (8). For \(g:\mathcal{X}\to\mathbb{R}\), define \(f:\mathcal{X}\to[0,1]\) such that \(f(x)=\nabla\psi(g(x))\) for every \(x\in\mathcal{X}\). For a distribution \(\mathcal{D}\) over \(\mathcal{X}\times\{0,1\}\), define \(\mathsf{smCE}_{\mathcal{D}}(f)\) as in Definition 2.3. Then_

\[\mathsf{smCE}_{\mathcal{D}}(f)\leq\mathsf{smCE}^{(\psi,\lambda)}_{\mathcal{D} }(g).\]

We defer the proofs of Theorem 4.6 and Lemma 4.7 to Appendix F.1 and Appendix F.2. Combining the two results, assuming \(\psi\) is \(\lambda\)-smooth, we have

\[\mathsf{smCE}_{\mathcal{D}}(f)^{2}/2\leq\lambda\,\mathsf{pGap}^{(\psi,\lambda )}_{\mathcal{D}}(g).\]

This means that a small dual post-processing gap for a proper loss implies a small (standard) smooth calibration error. The cross-entropy loss \(\ell\) is a proper loss where the corresponding \(\psi\) from Lemma 4.2 is given by \(\psi(t)=\ln(1+e^{t})\) and it is \(1/4\)-smooth. Setting \(\lambda=1/4\) in Theorem 4.6 and Lemma 4.7, we get (3) and (4) for the cross-entropy loss.

## 5 Optimization Algorithms and Implicit Regularization

As simple consequences of our results, there are several natural algorithms which explicitly minimize loss, but implicitly achieve good calibration. Here, we focus on one such algorithm: _structural risk minimization_, and discuss additional such algorithms in Appendix G. We also give informal intuitions connecting each algorithm to training DNNs in practice.

We show that structural risk minimization (SRM) on the population distribution is guaranteed to output well-calibrated predictors, under mild assumptions on the function family and regularizer. Recall, structural risk minimization considers a function family \(\mathcal{F}\) equipped with some "complexity measure" \(\mu:\mathcal{F}\to\mathbb{R}_{\geq 0}\). For example, we may take the family of bounded-width neural networks, with depthas the complexity measure. SRM then minimizes a proper loss, plus a complexity-regularizer given by \(\mu\):

\[f^{*}=\operatorname*{argmin}_{f\in\mathcal{F}}\operatorname*{\mathbb{E}}_{ \mathcal{D}}[\ell(y,f(x)]+\lambda\mu(f).\]

The complexity measure \(\mu\) is usually designed to control the capacity of the function family for generalization reasons, though we will not require such assumptions about \(\mu\). Rather, we only require that \(\mu(f)\) does not grow too quickly under composition with Lipshitz functions. That is, \(\mu(\kappa\circ f)\) should be at most a constant greater than \(\mu(f)\), for all Lipshitz functions \(\kappa\). Now, as long as the function family \(\mathcal{F}\) is also closed under composition, SRM is well-calibrated:

**Claim 5.1**.: _Let \(\mathcal{F}\) be a class of functions \(f:\mathcal{X}\to[0,1]\) closed under composition with \(K\), where we define \(K\) as in Definition 2.2. That is, for \(f\in\mathcal{F},\kappa\in K\) we have \(\kappa\circ f\in\mathcal{F}\). Let \(\mu:\mathcal{F}\to\mathbb{R}_{\geq 0}\) be any complexity measure satisfying, for all \(f\in\mathcal{F},\kappa\in K:\ \ \mu(\kappa\circ f)\leq\mu(f)+1\). Then the minimizer \(f^{*}\) of the regularized optimization problem_

\[f^{*}=\operatorname*{argmin}_{f\in\mathcal{F}}\operatorname*{\mathbb{E}}_{ \mathcal{D}}\ell_{\mathsf{sq}}(y,f(x))+\lambda\mu(f).\]

_satisfies \(\mathsf{pGap}_{\mathcal{D}}(f^{*})\leq\lambda\), and thus by Theorem 2.4 has small calibration error: \(\mathsf{smCE}_{\mathcal{D}}(f^{*})\leq\sqrt{\lambda}\)._

Proof.: The proof is almost immediate. Let \(\operatorname{MSE}_{\mathcal{D}}(f)\) denote \(\operatorname*{\mathbb{E}}_{\mathcal{D}}[\ell_{\mathsf{sq}}(y,f(x))]\). Consider the solution \(f^{*}\) and arbitrary \(\kappa\in K\). Since \(\kappa\circ f^{*}\in\mathcal{F}\), we have

\[\operatorname{MSE}_{\mathcal{D}}(f^{*})+\lambda\mu(f^{*})\leq\operatorname{MSE }_{\mathcal{D}}(\kappa\circ f^{*})+\lambda\mu(\kappa\circ f^{*})\leq\operatorname {MSE}_{\mathcal{D}}(\kappa\circ f^{*})+\lambda\mu(f^{*})+\lambda.\]

After rearranging, this is equivalent to

\[\operatorname{MSE}_{\mathcal{D}}(f^{*})-\operatorname{MSE}_{\mathcal{D}}( \kappa\circ f^{*})\leq\lambda,\]

and since \(\kappa\in K\) was arbitrary, we get \(\mathsf{pGap}_{\mathcal{D}}(f)\leq\lambda\). as desired. 

**Discussion: Implicit Regularization.** This aspect of SRM connects our results to the "implicit regularization" hypothesis from deep learning theory community (Neyshabur, 2017). The implicit regularization hypothesis is an informal belief that SGD on neural networks implicitly performs minimization on a "complexity-regularized" objective, which ensures generalization. The exact form of this complexity regularizer remains elusive, but many works have attempted to identify its structural properties (e.g. Arora et al. (2019); Neyshabur et al. (2014); Neyshabur (2017); Zhang et al. (2021)). In this context, our results imply that if the implicit regularization hypothesis is true, then as long as the complexity measure \(\mu\) doesn't increase too much from composition, the final output of SGD will be well-calibrated.

## 6 Conclusion

Inspired by recent empirical observations, we studied formal conditions under which optimizing a proper loss also happens to yield calibration "for free." We identified a certain local optimality condition that characterizes distance to calibration, in terms of properties of the (proper) loss landscape. Our results apply even to realistic optimization algorithms, which optimize over restricted function families, and may not reach global minima within these families. In particular, our formal results suggest an intuitive explanation for why state-of-the-art DNNs are often well-calibrated: because their test loss cannot be improved much by adding a few more layers. It also offers guidance for how one can achieve calibration simply by proper loss minimization over a sufficiently expressive family of predictors.

**Limitations.** The connection between our theoretical results and DNNs in practice is heuristic-- for example, we do not have a complete proof that training a DNN with SGD on natural distributions will yield a calibrated predictor, though we have formal results that give some intuition for this. Part of the difficulty here is due to known definitional barriers (Nakkiran, 2021), which is that we would need to formally specify what "DNN" means in practice (which architectures, and sizes?), what "SGD" means in practice (what initialization, learning-rate schedule, etc), and what "natural distributions" means in practice. Finally, we intuitively expect the results from our work to generalize beyond the binary label setting; we leave it to future work to formalize this intuition.

## Acknowledgments and Disclosure of Funding

Part of this work was performed while LH was interning at Apple. LH is also supported by Moses Charikar's and Omer Reingold's Simons Investigators awards, Omer Reingold's NSF Award IIS-1908774, and the Simons Foundation Collaboration on the Theory of Algorithmic Fairness. JB is supported by Simons Foundation.

## References

* Abdar et al. [2021] Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U Rajendra Acharya, et al. A review of uncertainty quantification in deep learning: Techniques, applications and challenges. _Information Fusion_, 76:243-297, 2021.
* Arora et al. [2019] Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix factorization. _Advances in Neural Information Processing Systems_, 32, 2019.
* Blasiok and Nakkiran [2023] Jaroslaw Blasiok and Preetum Nakkiran. Smooth ECE: Principled reliability diagrams via kernel smoothing. _arXiv preprint arXiv:2309.12236_, 2023.
* Blasiok et al. [2023a] Jaroslaw Blasiok, Parikshit Gopalan, Lunjia Hu, Adam Tauman Kalai, and Preetum Nakkiran. Loss minimization yields multicalibration for large neural networks. _arXiv preprint arXiv:2304.09424_, 2023a. URL [https://doi.org/10.48550/arXiv.2304.09424](https://doi.org/10.48550/arXiv.2304.09424).
* Blasiok et al. [2023b] Jaroslaw Blasiok, Parikshit Gopalan, Lunjia Hu, and Preetum Nakkiran. A unifying theory of distance from calibration. In _Proceedings of the 55th Annual ACM Symposium on Theory of Computing_, page 1727-1740, New York, NY, USA, 2023b. Association for Computing Machinery. ISBN 9781450399135.
* Blondel et al. [2020] Mathieu Blondel, Andre F. T. Martins, and Vlad Niculae. Learning with Fenchel-Young losses. _Journal of Machine Learning Research_, 21(35):1-69, 2020. URL [http://jmlr.org/papers/v21/19-021.html](http://jmlr.org/papers/v21/19-021.html).
* Brier et al. [1950] Glenn W Brier et al. Verification of forecasts expressed in terms of probability. _Monthly weather review_, 78(1):1-3, 1950.
* Buja et al. [2005] Andreas Buja, Werner Stuetzle, and Yi Shen. Loss functions for binary class probability estimation and classification: Structure and applications. _Manuscript_, 2005. URL [https://sites.stat.washington.edu/wxs/Learning-papers/paper-proper-scoring.pdf](https://sites.stat.washington.edu/wxs/Learning-papers/paper-proper-scoring.pdf).
* Carrell et al. [2022] Annabelle Carrell, Neil Mallinar, James Lucas, and Preetum Nakkiran. The calibration generalization gap. _arXiv preprint arXiv:2210.01964_, 2022.
* Clarte et al. [2023a] Lucas Clarte, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborova. On double-descent in uncertainty quantification in overparametrized models. In Francisco Ruiz, Jennifer Dy, and Jan-Willem van de Meent, editors, _Proceedings of The 26th International Conference on Artificial Intelligence and Statistics_, volume 206 of _Proceedings of Machine Learning Research_, pages 7089-7125. PMLR, 25-27 Apr 2023a. URL [https://proceedings.mlr.press/v206/clarte23a.html](https://proceedings.mlr.press/v206/clarte23a.html).
* Clarte et al. [2023b] Lucas Clarte, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborova. Theoretical characterization of uncertainty in high-dimensional linear classification. _Machine Learning: Science and Technology_, 4(2):025029, jun 2023b. doi: 10.1088/2632-2153/acd749. URL [https://dx.doi.org/10.1088/2632-2153/acd749](https://dx.doi.org/10.1088/2632-2153/acd749).
* Cybenko [1989] George Cybenko. Approximation by superpositions of a sigmoidal function. _Mathematics of control, signals and systems_, 2(4):303-314, 1989.
* Dawid [1982] A Philip Dawid. The well-calibrated bayesian. _Journal of the American Statistical Association_, 77(379):605-610, 1982.
* Dawid et al. [2019]Shrey Desai and Greg Durrett. Calibration of pre-trained transformers. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 295-302, 2020.
* 3275, 2018. doi: 10.1214/17-AOS1657. URL [https://doi.org/10.1214/17-AOS1657](https://doi.org/10.1214/17-AOS1657).
* Foster and Vohra (1998) Dean P. Foster and Rakesh V. Vohra. Asymptotic calibration. _Biometrika_, 85(2):379-390, 1998.
* Gal (2016) Yarin Gal. _Uncertainty in Deep Learning_. PhD thesis, University of Cambridge, 2016.
* Globus-Harris et al. (2023) Ira Globus-Harris, Declan Harrison, Michael Kearns, Aaron Roth, and Jessica Sorrell. Multicalibration as boosting for regression. _arXiv preprint arXiv:2301.13767_, 2023.
* Gneiting and Raftery (2007) Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estimation. _Journal of the American Statistical Association_, 102(477):359-378, 2007. doi: 10.1198/01621450600001437. URL [https://doi.org/10.1198/01621450600001437](https://doi.org/10.1198/01621450600001437).
* Gopalan et al. (2022a) Parikshit Gopalan, Adam Tauman Kalai, Omer Reingold, Vatsal Sharan, and Udi Wieder. Omnipreditors. In _Innovations in Theoretical Computer Science (ITCS'2022)_, 2022a. URL [https://arxiv.org/abs/2109.05389](https://arxiv.org/abs/2109.05389).
* Gopalan et al. (2022b) Parikshit Gopalan, Michael P. Kim, Mihir Singhal, and Shengjia Zhao. Low-degree multicalibration. In _Conference on Learning Theory, 2-5 July 2022, London, UK_, volume 178 of _Proceedings of Machine Learning Research_, pages 3193-3234. PMLR, 2022b.
* Gopalan et al. (2023a) Parikshit Gopalan, Lunjia Hu, Michael P. Kim, Omer Reingold, and Udi Wieder. Loss Minimization Through the Lens Of Outcome Indistinguishability. In _14th Innovations in Theoretical Computer Science Conference (ITCS 2023)_, volume 251 of _Leibniz International Proceedings in Informatics (LIPIcs)_, pages 60:1-60:20, 2023a.
* Gopalan et al. (2023b) Parikshit Gopalan, Michael P. Kim, and Omer Reingold. Characterizing notions of omniprediction via multicalibration. _CoRR_, abs/2302.06726, 2023b. doi: 10.48550/arXiv.2302.06726. URL [https://doi.org/10.48550/arXiv.2302.06726](https://doi.org/10.48550/arXiv.2302.06726).
* Graves (2011) Alex Graves. Practical variational inference for neural networks. _Advances in neural information processing systems_, 24, 2011.
* Guo et al. (2017) Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In _International Conference on Machine Learning_, pages 1321-1330. PMLR, 2017.
* Hebert-Johnson et al. (2018) Ursula Hebert-Johnson, Michael P. Kim, Omer Reingold, and Guy N. Rothblum. Multicalibration: Calibration for the (computationally-identifiable) masses. In _Proceedings of the 35th International Conference on Machine Learning, ICML_, 2018.
* Hendrycks* et al. (2020) Dan Hendrycks*, Norman Mu*, Ekin Dogus Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. Augmix: A simple method to improve robustness and uncertainty under data shift. In _International Conference on Learning Representations_, 2020. URL [https://openreview.net/forum?id=S1gmxrHFvB](https://openreview.net/forum?id=S1gmxrHFvB).
* Hu et al. (2022) Lunjia Hu, Inbal Livni-Navon, Omer Reingold, and Chutong Yang. Omnipreditors for constrained optimization. _arXiv preprint arXiv:2209.07463_, 2022. URL [https://doi.org/10.48550/arXiv.2209.07463](https://doi.org/10.48550/arXiv.2209.07463).
* Kakade and Foster (2008) Sham Kakade and Dean Foster. Deterministic calibration and nash equilibrium. _Journal of Computer and System Sciences_, 74(1):115-130, 2008.
* Karandikar et al. (2021) Archit Karandikar, Nicholas Cain, Dustin Tran, Balaji Lakshminarayanan, Jonathon Shlens, Michael C Mozer, and Becca Roelofs. Soft calibration objectives for neural networks. _Advances in Neural Information Processing Systems_, 34:29768-29779, 2021.
* Kearns et al. (2017) Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. Preventing fairness gerrymandering: Auditing and learning for subgroup fairness. _arXiv preprint arXiv:1711.05144_, 2017.
* Kearns et al. (2018)Meelis Kull, Telmo Silva Filho, and Peter Flach. Beta calibration: a well-founded and easily implemented improvement on logistic calibration for binary classifiers. In _Artificial Intelligence and Statistics_, pages 623-631. PMLR, 2017.
* Kumar et al. (2018) Aviral Kumar, Sunita Sarawagi, and Ujjwal Jain. Trainable calibration measures for neural networks from kernel mean embeddings. In _International Conference on Machine Learning_, pages 2805-2814. PMLR, 2018.
* Lakshminarayanan et al. (2017) Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In _Advances in neural information processing systems_, pages 6402-6413, 2017.
* Lu et al. (2017) Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power of neural networks: A view from the width. _Advances in neural information processing systems_, 30, 2017.
* MacKay (1995) David JC MacKay. Probable networks and plausible predictions-a review of practical bayesian methods for supervised neural networks. _Network: computation in neural systems_, 6(3):469, 1995.
* Minderer et al. (2021) Matthias Minderer, Josip Djolonga, Rob Romijnders, Frances Hubis, Xiaohua Zhai, Neil Houlsby, Dustin Tran, and Mario Lucic. Revisiting the calibration of modern neural networks. _Advances in Neural Information Processing Systems_, 34:15682-15694, 2021.
* Mukhoti et al. (2020) Jishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, Stuart Golodetz, Philip Torr, and Puneet Dokania. Calibrating deep neural networks using focal loss. _Advances in Neural Information Processing Systems_, 33:15288-15299, 2020.
* Nakkiran (2021) Preetum Nakkiran. _Towards an empirical theory of deep learning_. PhD thesis, Harvard University, 2021.
* Neyshabur (2017) Behnam Neyshabur. Implicit regularization in deep learning. _arXiv preprint arXiv:1709.01953_, 2017.
* Neyshabur et al. (2014) Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. _arXiv preprint arXiv:1412.6614_, 2014.
* Niculescu-Mizil and Caruana (2005) Alexandru Niculescu-Mizil and Rich Caruana. Predicting good probabilities with supervised learning. In _Proceedings of the 22nd international conference on Machine learning_, pages 625-632. ACM, 2005.
* OpenAI (2023) OpenAI. Gpt-4 technical report, 2023.
* Pedregosa et al. (2011) F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. _Journal of Machine Learning Research_, 12:2825-2830, 2011.
* Platt (1999) John C. Platt. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. In _ADVANCES IN LARGE MARGIN CLASSIFIERS_, pages 61-74. MIT Press, 1999.
* Reid and Williamson (2010) Mark D. Reid and Robert C. Williamson. Composite binary losses. _Journal of Machine Learning Research_, 11(83):2387-2422, 2010. URL [http://jmlr.org/papers/v11/reid10a.html](http://jmlr.org/papers/v11/reid10a.html).
* Richardson and Pospisil (2021) Lee Richardson and Taylor Pospisil. Why model calibration matters, and how to measure it, 2021. URL [https://www.unofficialgoogledatascience.com/2021/04/why-model-calibration-matters-and-how.html](https://www.unofficialgoogledatascience.com/2021/04/why-model-calibration-matters-and-how.html).
* Savage (1971) Leonard J. Savage. Elicitation of personal probabilities and expectations. _Journal of the American Statistical Association_, 66(336):783-801, 1971. doi: 10.1080/01621459.1971.10482346. URL [https://www.tandfonline.com/doi/abs/10.1080/01621459.1971.10482346](https://www.tandfonline.com/doi/abs/10.1080/01621459.1971.10482346).
* 1879, 1989. doi: 10.1214/aos/1176347398. URL [https://doi.org/10.1214/aos/1176347398](https://doi.org/10.1214/aos/1176347398).
Emir H. Shuford, Arthur Albert, and H. Edward Massengill. Admissible probability measurement procedures. _Psychometrika_, 31(2):125-145, 1966. doi: 10.1007/BF02289503. URL [https://doi.org/10.1007/BF02289503](https://doi.org/10.1007/BF02289503).
* Yun et al. (2019) Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar. Are transformers universal approximators of sequence-to-sequence functions? _arXiv preprint arXiv:1912.10077_, 2019.
* Zadrozny and Elkan (2001) Bianca Zadrozny and Charles Elkan. Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers. In _Icml_, volume 1, pages 609-616. Citeseer, 2001.
* Zadrozny and Elkan (2002) Bianca Zadrozny and Charles Elkan. Transforming classifier scores into accurate multiclass probability estimates. In _Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining_, pages 694-699. ACM, 2002.
* Zhang et al. (2021) Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. _Communications of the ACM_, 64(3):107-115, 2021.
* Zhou (2020) Ding-Xuan Zhou. Universality of deep convolutional neural networks. _Applied and computational harmonic analysis_, 48(2):787-794, 2020.

Additional Related Works

Calibration without Bayesian Modeling.Our results show that it is formally possible to obtain calibrated classifiers via pure loss minimization, without incorporating Bayesian priors into the learning process. We view this as encouraging for the field of uncertainty quantification in machine learning, since it shows it is possible to obtain accurate uncertainties without incurring the computational and conceptual overheads of Bayesian methods (Abdar et al., 2021; Graves, 2011; MacKay, 1995; Gal, 2016).

Calibration Methods.There have been many methods proposed to achieve accurate models that are also well-calibrated. The classic two-stage approach is to first learn an accurate classifier, and then post-process for calibration (via e.g. isotonic regression or Platt scaling as in Platt (1999); Zadrozny and Elkan (2001, 2002); Niculescu-Mizil and Caruana (2005)). Recent papers on neural networks also suggest optimizing for both calibration and accuracy jointly, by adding a "calibration regularizer" term to the standard accuracy-encouraging objective function (Kumar et al., 2018; Karandikar et al., 2021). In contrast, we are interested in the setting where loss-minimization happens to yield calibration "out of the box", without re-calibration. There is a growing body of work on principled measures of calibration (Kakade and Foster, 2008; Gopalan et al., 2022; Blasiok et al., 2023b), which we build on in our paper. Clarte et al. (2023a,b) analyze calibration for empirical risk minimization and highlight the role played by regularization.

Multicalibration.There has recently been great interest in a generalization of calibration known as multiclibration (Hebert-Johnson et al., 2018), see also Kearns et al. (2017), motivated by algorithmic fairness considerations. A series of works explores the connection between multiclibration and minimization of convex losses, through the notion of omniprediction (Gopalan et al., 2022; Gopalan et al., 2023a,b; Hu et al., 2022; Globus-Harris et al., 2023). A tight characterization of multiclibration in terms of squared loss minimization is presented in Gopalan et al. (2023b), which involves a novel notion of loss minimization called swap agnostic learning. Another related result is the recent work of Blasiok et al. (2023a) which shows that minimizing squared loss for sufficiently deep neural nets yields multiclibration with regard to smaller neural nets of a fixed complexity. This result is similar in flavor since it argues that a lack of multiclibration points to a good way to reduce the loss, and relies on the expressive power of DNNs to do this improvement. But it goes only in one direction. Our results apply to any proper loss, and give a tight characterization of calibration error. We generalize our results to multiclibration in Appendix F.

## Appendix B Miscalibration Example from Logistic Regression

Figure 2 gives an example of a 1-dimensional distribution for which logistic regression (on the population distribution itself) is severely mis-calibrated. This occurs despite logistic regression optimizing a proper loss. Note that the constant predictor \(f(x)=0.5\) is perfectly calibrated in this example, but logistic regression encourages the solution to _deviate_ from it in order to minimize the proper cross-entropy loss. Similar examples of logistic regression being miscalibrated are found in Kull et al. (2017).

## Appendix C Post-processing Gap for Cross Entropy Loss

Here we show that directly replacing the squared loss in the definition of post-processing gap (Definition 2.2) with the cross-entropy loss does not give us a similar result as Theorem 2.4. Specifically, we give an example where the smooth calibration error of a predictor is small but the post-processing gap w.r.t. the cross entropy loss can be arbitrarily large.

We choose \(\mathcal{X}=\{x_{0},x_{1}\}\) and let \(\mathcal{D}\) be the distribution over \(\mathcal{X}\times\{0,1\}\) such that the marginal distribution over \(\mathcal{X}\) is uniform, \(\mathbb{E}_{(x,y)\sim\mathcal{D}}[y|x=x_{0}]=0.1\) and \(\mathbb{E}_{(x,y)\sim\mathcal{D}}[y|x=x_{1}]=0.9\). Consider predictor \(f:\mathcal{X}\rightarrow[0,1]\) such that \(f(x_{0})=\varepsilon\) and \(f(x_{1})=1-\varepsilon\) for some \(\varepsilon\in(0,0.1)\). As \(\varepsilon\to 0\), it is easy to verify that \(\textsf{smCE}_{\mathcal{D}}(f)\to 0.05\). However, the post-processing gap w.r.t. the cross-entropy loss tends to \(+\infty\) because \(\ell(1,\varepsilon)=\ell(0,1-\varepsilon)\rightarrow+\infty\) as \(\varepsilon\to 0\) for the cross-entropy loss \(\ell\).

## Appendix D Tight Examples

We give examples showing that the constants in Theorem 2.4 and in Corollary 2.7 are all optimal. Specifically Lemma D.1 shows that the constants in the upper bound of \(\mathsf{pGap}\) in Theorem 2.4 are optimal, whereas Lemma D.2 shows that the constants in the lower bound of \(\mathsf{pGap}\) are also optimal. Lemma D.3 shows that the constants in the upper bound of \(\mathsf{pGap}^{(\psi,1/4)}\) in (3) are optimal. Lemma D.4 simultaneously shows that the constants in the lower bound of \(\mathsf{pGap}^{(\psi,1/4)}\) in (3) and the constants in (4) are optimal.

**Lemma D.1**.: _Let \(\mathcal{X}=\{x_{0},x_{1}\}\) be a set of two individuals. For every \(\varepsilon\in(0,1/4)\), there exists a distribution \(\mathcal{D}\) over \(\mathcal{X}\times\{0,1\}\) and a predictor \(f:\mathcal{X}\to[0,1]\) such that_

\[\mathsf{pGap}_{\mathcal{D}}(f)=\varepsilon-3\varepsilon^{2},\quad\text{whereas} \quad\mathsf{smCE}_{\mathcal{D}}(f)=\varepsilon/2-\varepsilon^{2}.\]

Proof.: We simply choose \(\mathcal{D}\) to be the uniform distribution over \(\{(x_{0},0),(x_{1},1)\}\), and we choose \(f(x_{0})=1/2-\varepsilon\) and \(f(x_{1})=1/2+\varepsilon\). For any \(1\)-Lipschitz function \(\eta:[0,1]\to[-1,1]\), we have

\[|\operatorname{\mathbb{E}}_{(x,y)\sim\mathcal{D}}[(y-f(x))\eta(f( x))]| =|-(1/2)(1/2-\varepsilon)\eta(f(x_{0}))+(1/2)(1/2-\varepsilon) \eta(f(x_{1}))|\] \[=(1/2)(1/2-\varepsilon)|\eta(1/2-\varepsilon)-\eta(1/2+ \varepsilon)|.\]

Clearly, the supremum of the above quantity is \(\varepsilon/2-\varepsilon^{2}\) which is achieved when \(|\eta(1/2-\varepsilon)-\eta(1/2+\varepsilon)|=2\varepsilon\). Therefore,

\[\mathsf{smCE}_{\mathcal{D}}(f)=\varepsilon/2-\varepsilon^{2}.\]

For any predictor \(f^{\prime}:\mathcal{X}\to[0,1]\) and the squared loss \(\ell\),

\[\operatorname{\mathbb{E}}_{(x,y)\sim\mathcal{D}}[\ell(y,f^{\prime }(x))] =(1/2)(0-f^{\prime}(x_{0}))^{2}+(1/2)(1-f^{\prime}(x_{1}))^{2}\] \[=f^{\prime}(x_{0})^{2}/2+f^{\prime}(x_{1})^{2}/2-f^{\prime}(x_{1} )+1/2\] \[=\frac{1}{4}(f^{\prime}(x_{0})+f^{\prime}(x_{1})-1)^{2}+\frac{1}{4 }(f^{\prime}(x_{1})-f^{\prime}(x_{0})-1)^{2}\]

When \(f^{\prime}(x)=f(x)+\eta(f(x))\) for a \(1\)-Lipschitz function \(\eta:[0,1]\to[-1,1]\), the infimum of the above quantity is achieved when \(f^{\prime}(x_{0})+f^{\prime}(x_{1})-1=0\) and \(f^{\prime}(x_{1})-f^{\prime}(x_{0})=4\varepsilon\), and the infimum is \((1/4)(1-4\varepsilon)^{2}\). If we choose \(f^{\prime}=f\) instead, we get \(\operatorname{\mathbb{E}}[\ell(y,f(x))]=(1/4)(1-2\varepsilon)^{2}\). Therefore,

\[\mathsf{pGap}_{\mathcal{D}}(f)=(1/4)(1-2\varepsilon)^{2}-(1/4)(1-4 \varepsilon)^{2}=\varepsilon-3\varepsilon^{2}.\qed\]

Figure 2: **Logistic Regression can be Miscalibrated.** We illustrate a \(1\)-dimensional distribution on which logistic regression is severely miscalibrated: consider two overlapping uniform distributions, one for each class. Specifically, with probability \(1/2\) we have \(y=0\) and \(x\sim\operatorname{Unif}([-3,1])\), and with the remaining probability \(1/2\), we have \(y=1\) and \(x\sim\operatorname{Unif}([-1,3])\). This distribution is shown on the left, as well as the optimal logistic regressor \(f\) on this distribution. In this setting, the logistic regressor is miscalibrated (as shown at right), despite logistic regression optimizing a proper loss. Note that the constant predictor \(f(x)=0.5\) is perfectly calibrated in this example, but logistic regression encourages the solution to _deviate_ from it in order to minimize the proper cross-entropy loss. Similar examples of logistic regression being miscalibrated are also found in Kull et al. (2017).

**Lemma D.2**.: _Let \(\mathcal{X}=\{x_{0}\}\) be a set consisting of only a single individual \(x_{0}\). Let \(\mathcal{D}\) be the uniform distribution over \(\mathcal{X}\times\{0,1\}\) and for \(\varepsilon\in(0,1/2)\), let \(f:\mathcal{X}\to[0,1]\) be the predictor such that \(f(x_{0})=1/2+\varepsilon\). Then_

\[\mathsf{pGap}_{\mathcal{D}}(f)=\varepsilon^{2},\quad\text{whereas}\quad\mathsf{ smCE}_{\mathcal{D}}(f)=\varepsilon.\]

Proof.: For any \(1\)-Lipschitz function \(\eta:[0,1]\to[-1,1]\),

\[|\operatorname{\mathbb{E}}_{(x,y)\sim\mathcal{D}}[(y-f(x))\eta(f(x))]|= \varepsilon|\eta(1/2+\varepsilon)|.\]

The supremum of the quantity above over \(\eta\) is clearly \(\varepsilon\), implying that \(\mathsf{smCE}_{\mathcal{D}}(f)=\varepsilon\).

For any predictor \(f^{\prime}:\mathcal{X}\to[0,1]\) and the squared loss \(\ell\),

\[\operatorname{\mathbb{E}}_{(x,y)\sim\mathcal{D}}[\ell(y,f^{ \prime}(x))] =(1/2)(0-f^{\prime}(x_{0}))^{2}+(1/2)(1-f^{\prime}(x_{0}))^{2}\] \[=f^{\prime}(x_{0})^{2}-f^{\prime}(x_{0})+1/2\] \[=1/4+(f^{\prime}(x_{0})-1/2)^{2}.\]

The infimum of the above quantity over a function \(f^{\prime}:\mathcal{X}\to[0,1]\) satisfying \(f^{\prime}(x)=f(x)+\eta(f(x))\) for some \(1\)-Lipschitz \(\eta:[0,1]\to[-1,1]\) is clearly \(1/4\). If we choose \(f^{\prime}=f\) instead, we get \(\operatorname{\mathbb{E}}_{(x,y)\sim\mathcal{D}}[\ell(y,f(x))]=1/4+\varepsilon^ {2}\). Therefore,

\[\mathsf{pGap}_{\mathcal{D}}(f)=(1/4+\varepsilon^{2})-1/4=\varepsilon^{2}.\qed\]

**Lemma D.3**.: _Define \(\mathsf{pGap}^{(\psi,1/4)}\) and \(\mathsf{smCE}^{(\psi,1/4)}\) as in Definition 2.5 and Definition 2.6 for the cross-entropy loss. Let \(\mathcal{X}=\{x_{0},x_{1}\}\) be a set of two individuals. For every \(\varepsilon\in(0,1/4)\), there exists a distribution \(\mathcal{D}\) over \(\mathcal{X}\times\{0,1\}\) and a function \(g:\mathcal{X}\to\mathbb{R}\) such that_

\[\mathsf{pGap}_{\mathcal{D}}^{(\psi,1/4)}(g)\geq 2\varepsilon+O(\varepsilon^{2}), \quad\text{whereas}\quad\mathsf{smCE}^{(\psi,1/4)}_{\mathcal{D}}(g)\leq \varepsilon/2+O(\varepsilon^{2}).\]

Proof.: We simply choose \(\mathcal{D}\) to be the uniform distribution over \(\{(x_{0},0),(x_{1},1)\}\), and we choose \(g(x_{0})=-4\varepsilon\) and \(g(x_{1})=4\varepsilon\). Let \(\sigma\) denote the sigmoid transformation given by \(\sigma(t)=e^{t}/(1+e^{t})\). The Taylor expansion of the sigmoid transformation \(\sigma\) around \(t=0\) is \(\sigma(t)=1/2+O(t)\). Thus the predictor \(f\) we get from applying \(\sigma\) to \(g\) satisfies \(f(x_{1})=1-f(x_{0})=1/2+O(\varepsilon)\). For any \(1/4\)-Lipschitz function \(\eta:\mathbb{R}\to[-1,1]\), we have

\[\operatorname{\mathbb{E}}_{(x,y)\sim\mathcal{D}}[(y-f(x))\eta(g( x))] =(1/2)(0-f(x_{0}))\eta(g(x_{0}))+(1/2)(1-f(x_{1}))\eta(g(x_{1}))\] \[=-(1/2)(1-f(x_{1}))\eta(g(x_{0}))+(1/2)(1-f(x_{1}))\eta(g(x_{1}))\] \[=(1/2)(1-f(x_{1}))(\eta(g(x_{1}))-\eta(g(x_{0})))\] \[\leq(1/2)(1-f(x_{1}))|g(x_{0})-g(x_{1})|/4\] \[\leq(1/2)(1/2+O(\varepsilon))\cdot(8\varepsilon)/4\] \[=\varepsilon/2+O(\varepsilon^{2}).\]

This implies that \(\mathsf{smCE}^{(\psi,1/4)}_{\mathcal{D}}(g)\leq\varepsilon/2+O(\varepsilon^{2})\).

For any function \(g^{\prime}:\mathcal{X}\to\mathbb{R}\) and the logistic loss \(\ell^{(\psi)}\),

\[\operatorname{\mathbb{E}}_{(x,y)\sim\mathcal{D}}[\ell^{(\psi)}(y,g^ {\prime}(x))] =(1/2)\ln(1+e^{g^{\prime}(x_{0})})+(1/2)\ln(1+e^{g^{\prime}(x_{1} )})-(1/2)g^{\prime}(x_{1})\] \[=(1/2)\ln(1+e^{g^{\prime}(x_{0})})+(1/2)\ln(1+e^{-g^{\prime}(x_{ 1})}).\]

We use the Taylor expansion \(\ln(1+e^{t})=\ln 2+t/2+O(t^{2})\) to get

\[\operatorname{\mathbb{E}}_{(x,y)\sim\mathcal{D}}[\ell^{(\psi)}(y,g^{\prime}(x))] =\ln 2+(1/4)(g^{\prime}(x_{0})-g^{\prime}(x_{1}))+O(g^{\prime}(x_{0} )^{2}+g^{\prime}(x_{1})^{2}). \tag{9}\]

Consider the case when \(g^{\prime}(x_{0})=-8\varepsilon\) and \(g^{\prime}(x_{1})=8\varepsilon\). Clearly, \(g^{\prime}\) can be written as \(g^{\prime}(x)=g(x)+\eta(g(x))\) for a \(1\)-Lipschitz function \(\eta:\mathbb{R}\to[-4,4]\). By (9), the expected loss achieved by \(g^{\prime}\) is

\[\operatorname{\mathbb{E}}_{(x,y)\sim\mathcal{D}}[\ell^{(\psi)}(y,g^{\prime}(x))] =\ln 2-4\varepsilon+O(\varepsilon^{2}).\]

If \(g^{\prime}=g\) instead, the expected loss is \(\ln 2-2\varepsilon+O(\varepsilon^{2})\). Taking the difference between the two, we get

\[\mathsf{pGap}_{\mathcal{D}}^{(\psi,1/4)}(g)\geq 2\varepsilon+O(\varepsilon^{2}).\qed\]

**Lemma D.4**.: _Define \(\mathsf{pGap}^{(\psi,1/4)}\) as in Definition 2.5 for the cross-entropy loss. Let \(\mathcal{X}=\{x_{0}\}\) be a set consisting of only a single individual \(x_{0}\). Let \(\mathcal{D}\) be the uniform distribution over \(\mathcal{X}\times\{0,1\}\) and for \(\varepsilon\in(0,1/2)\), let \(g:\mathcal{X}\to\mathbb{R}\) be the function such that \(g(x_{0})=4\varepsilon\). Then_

\[\mathsf{pGap}^{(\psi,1/4)}_{\mathcal{D}}(g)=2\varepsilon^{2}+O(\varepsilon^{4} ),\quad\text{whereas}\quad\mathsf{smCE}_{\mathcal{D}}(f)=\varepsilon+O( \varepsilon^{3}),\]

_where \(f:\mathcal{X}\to(0,1)\) is given by \(f(x)=\sigma(g(x))\) for the sigmoid transformation \(\sigma(t)=e^{t}/(1+e^{t})\)._

Proof.: The Taylor expansion of \(\sigma\) around \(t=0\) is \(\sigma(t)=1/2+t/4+O(t^{3})\). Thus \(f(x_{0})=\sigma(g(x_{0}))=1/2+\varepsilon+O(\varepsilon^{3})\). For any \(1\)-Lipschitz function \(\eta:[0,1]\to[-1,1]\),

\[|\operatorname{\mathbb{E}}_{(x,y)\sim\mathcal{D}}[(y-f(x)\eta(f(x))]|=|(1/2-f( x_{0}))\eta(f(x_{0}))|=(\varepsilon+O(\varepsilon^{3}))|\eta(f(x_{0}))|.\]

The supremum of the quantity above over \(\eta\) is clearly \(\varepsilon+O(\varepsilon^{3})\), implying that \(\mathsf{smCE}_{\mathcal{D}}(f)=\varepsilon+O(\varepsilon^{3})\).

For any function \(g^{\prime}:\mathcal{X}\to[0,1]\) and the logistic loss \(\ell^{(\psi)}\),

\[\operatorname{\mathbb{E}}_{(x,y)\sim\mathcal{D}}[\ell^{(\psi)}(y, g^{\prime}(x))] =\ln(1+e^{g^{\prime}(x_{0})})-(1/2)g^{\prime}(x_{0}) \tag{10}\] \[=\ln(e^{g^{\prime}(x_{0})/2}+e^{-g^{\prime}(x_{0})/2}).\]

The infimum of the above quantity over a function \(g^{\prime}:\mathcal{X}\to\mathbb{R}\) satisfying \(g^{\prime}(x)=g(x)+\eta(g(x))\) for some \(1\)-Lipschitz \(\eta:\mathbb{R}\to[-4,4]\) is \(\ln 2\) achieved when \(g^{\prime}(x_{0})=0\). If we choose \(g^{\prime}=g\) instead, we get \(\operatorname{\mathbb{E}}_{(x,y)\sim\mathcal{D}}[\ell^{(\psi)}(y,g(x))]=\ln 2+2 \varepsilon^{2}+O(\varepsilon^{4})\) by plugging the Taylor expansion \(\ln(1+e^{t})=\ln 2+t/2+t^{2}/8+O(t^{4})\) into (10). Therefore,

\[\mathsf{pGap}^{(\psi,1/4)}_{\mathcal{D}}(g)=2\varepsilon^{2}+O(\varepsilon^{4 }).\qed\]

## Appendix E Proper Loss and Convex Functions

It is known that there is a correspondence between proper loss functions and convex functions (Shuford et al., 1966; Savage, 1971; Schervish, 1989; Buja et al., 2005). Here we demonstrate this correspondence, which is important for extending the connection between smooth calibration and post-processing gap (Theorem 2.4) to general proper loss functions in Section 4.

**Definition E.1** (Sub-gradient).: _For a function \(\varphi:V\to\mathbb{R}\) defined on \(V\subseteq\mathbb{R}\), we say \(t\in\mathbb{R}\) is a sub-gradient of \(\varphi\) at \(v\in V\) if_

\[\varphi(v^{\prime})\geq\varphi(v)+(v^{\prime}-v)t\quad\text{for every $v^{\prime} \in V$},\]

_or equivalently,_

\[vt-\varphi(v)=\max_{v\in V}\big{(}v^{\prime}t-\varphi(v^{\prime})\big{)}. \tag{11}\]

**Definition E.2** (Fenchel-Young divergence).: _For a pair of functions \(\varphi:V\to\mathbb{R}\) and \(\psi:T\to\mathbb{R}\) defined on \(V,T\subseteq\mathbb{R}\), we define the Fenchel-Young divergence\(D_{\varphi,\psi}:V\times T\to\mathbb{R}\) such that_

\[D_{\varphi,\psi}(v,t)=\varphi(v)+\psi(t)-vt\quad\text{for every $v\in V$ and $t\in T$}.\]

The following claim follows immediately from the two definitions above.

**Claim E.3**.: _Let \(\varphi:V\to\mathbb{R}\) and \(\psi:T\to\mathbb{R}\) be functions defined on \(V,T\subseteq\mathbb{R}\). Then \(t\in T\) is a sub-gradient of \(\varphi\) at \(v\in V\) if and only if \(D_{\varphi,\psi}(v,t)=\min_{v^{\prime}\in V}D_{\varphi,\psi}(v^{\prime},t)\). Similarly, \(v\) is a sub-gradient of \(\psi\) at \(t\) if and only if \(D_{\varphi,\psi}(v,t)=\min_{t^{\prime}\in T}D_{\varphi,\psi}(v,t^{\prime})\). In particular, assuming \(D_{\varphi,\psi}(v^{\prime},t^{\prime})\geq 0\) for every \(v^{\prime}\in V\) and \(t^{\prime}\in T\) whereas \(D_{\varphi,\psi}(v,t)=0\) for some \(v\in V\) and \(t\in T\), then \(t\) is a sub-gradient of \(\varphi\) at \(v\), and \(v\) is a sub-gradient of \(\psi\) at \(t\)._

**Lemma E.4** (Convex functions from proper loss).: _Let \(V\subseteq[0,1]\) be a non-empty interval. Let \(\ell:\{0,1\}\times V\to\mathbb{R}\) be a proper loss function. For every \(v\in V\), define \(\mathsf{dual}(v):=\ell(0,v)-\ell(1,v)\). There exist convex functions \(\varphi:V\to\mathbb{R}\) and \(\psi:\mathbb{R}\to\mathbb{R}\) such that_

\[\ell(y,v) =\psi(\mathsf{dual}(v))-y\;\mathsf{dual}(v) \text{for every $y\in\{0,1\}$ and $v\in V$}, \tag{12}\] \[D_{\varphi,\psi}(v,t) \geq 0 \text{for every $v\in V$ and $t\in\mathbb{R}$},\] (13) \[D_{\varphi,\psi}(v,\mathsf{dual}(v)) =0 \text{for every $v\in V$},\] (14) \[0 \leq\frac{\psi(t_{2})-\psi(t_{1})}{t_{2}-t_{1}} \leq 1 \text{for every distinct $t_{1},t_{2}\in\mathbb{R}$}. \tag{15}\]Before we prove the lemma, we remark that (12) and (14) together imply the following:

\[\ell(y,v)=-\varphi(v)+(v-y)\mathsf{dual}(v)\quad\text{for every $y\in\{0,1\}$ and $v\in V$}.\]

Proof.: Any loss function \(\ell:\{0,1\}\times V\to\mathbb{R}\), proper or not, can be written as \(\ell(y,v)=\ell(0,v)-y\,\mathsf{dual}(v)\). When \(\ell\) is proper, it is easy to see that for \(v,v^{\prime}\in V\) with \(\mathsf{dual}(v)=\mathsf{dual}(v^{\prime})\), it holds that \(\ell(0,v)=\ell(0,v^{\prime})\); otherwise, \(\ell(y,v)\) is always smaller or always larger than \(\ell(y,v^{\prime})\) for every \(y\in\{0,1\}\), making it impossible for \(\ell\) to be proper. Therefore, for proper \(\ell\), there exists \(\psi_{0}:T\to\mathbb{R}\) such that \(\ell(0,v)=\psi_{0}(\mathsf{dual}(v))\) for every \(v\in V\), where \(T:=\{\mathsf{dual}(v):v\in V\}\). Therefore,

\[\ell(y,v)=\ell(0,v)-y\,\mathsf{dual}(v)=\psi_{0}(\mathsf{dual}(v))-y\,\mathsf{ dual}(v). \tag{16}\]

By the assumption that \(\ell\) is proper, for every \(v\in V\),

\[v\in\operatorname*{argmin}_{v^{\prime}\in V}\operatorname*{\mathbb{E}}_{y\sim \mathsf{Ber}(v)}\ell(y,v^{\prime})=\operatorname*{argmin}_{v^{\prime}\in V} \Big{(}\psi_{0}(\mathsf{dual}(v^{\prime}))-v\,\mathsf{dual}(v^{\prime})\Big{)},\]

and thus

\[\mathsf{dual}(v)\in\operatorname*{argmin}_{t\in T}\Big{(}\psi_{0}(t)-vt\Big{)}.\]

For every \(v\in V\), we define

\[\varphi(v):=v\,\mathsf{dual}(v)-\psi_{0}(\mathsf{dual}(v))=\max_{t\in T}\Big{(} vt-\psi_{0}(t)\Big{)}.\]

The definition of \(\varphi\) immediately implies the following:

\[D_{\varphi,\psi_{0}}(v,t) \geq 0 \text{for every $v\in V$ and $t\in T$}, \tag{17}\] \[D_{\varphi,\psi_{0}}(v,\mathsf{dual}(v)) =0 \text{for every $v\in V$}. \tag{18}\]

By Claim E.3, \(\mathsf{dual}(v)\) is a sub-gradient of \(\varphi\) at \(v\). This proves that \(\varphi\) is convex. Now we define \(\psi:\mathbb{R}\to\mathbb{R}\) such that \(\psi(t)=\sup_{v\in V}vt-\varphi(v)\) for every \(t\in\mathbb{R}\). This definition ensures that inequality (13) holds. By Lemma H.3, \(\psi\) is a convex function and (15) holds. Moreover, for every \(v\in V\), we have shown that \(\mathsf{dual}(v)\) is a sub-gradient of \(\varphi\) at \(v\), so by (11), \(\psi(\mathsf{dual}(v))=v\,\mathsf{dual}(v)-\varphi(v)=\psi_{0}(\mathsf{dual}(v))\). This implies that (12) and (14) hold because of (16) and (18). 

**Lemma E.5** (Proper loss from convex functions).: _Let \(V\subseteq[0,1]\) be a non-empty interval. Let \(\varphi:V\to\mathbb{R}\), \(\psi:\mathbb{R}\to\mathbb{R}\), and \(\mathsf{dual}:V\to\mathbb{R}\) be functions satisfying (13) and (14). Define loss function \(\ell:\{0,1\}\times V\to\mathbb{R}\) as in (12). Then_

1. \(\ell\) _is a proper loss function;_
2. \(\varphi\) _is convex;_
3. _for every_ \(v\in V\)_,_ \(\mathsf{dual}(v)\) _is a sub-gradient of_ \(\varphi\) _at_ \(v\)_, and_ \(v\) _is a sub-gradient of_ \(\psi\) _at_ \(\mathsf{dual}(v)\)_;_
4. \(\varphi(v)=v\,\mathsf{dual}(v)-\psi(\mathsf{dual}(v))=-\operatorname*{\mathbb{ E}}_{y\sim\mathsf{Ber}(v)}[\ell(y,v)]\)_;_
5. _if we define_ \(\psi^{\prime}(t):=\sup_{v\in V}(vt-\varphi(v))\)_, then_ \(\psi^{\prime}\) _is convex and_ \(\psi^{\prime}(\mathsf{dual}(v))=\psi(\mathsf{dual}(v))\) _for every_ \(v\in V\)_. Moreover, for any two distinct real numbers_ \(t_{1},t_{2}\)_,_ \[0\leq\frac{\psi^{\prime}(t_{2})-\psi^{\prime}(t_{1})}{t_{2}-t_{1}}\leq 1.\] (19)

Proof.: For every \(v\in V\), equations (13) and (14) imply that

\[\mathsf{dual}(v)\in\operatorname*{argmin}_{t\in\mathbb{R}}\big{(}\psi(t)-vt \big{)},\]

and thus

\[v\in\operatorname*{argmin}_{v^{\prime}\in V}\Big{(}\psi(\mathsf{dual}(v^{ \prime}))-v\,\mathsf{dual}(v^{\prime})\Big{)}=\operatorname*{argmin}_{v^{ \prime}\in V}\operatorname*{\mathbb{E}}_{y\sim\mathsf{Ber}(v)}\ell(y,v^{ \prime}).\]

This proves that \(\ell\) is proper (Item 1). Item 3 follows from (13), (14) and Claim E.3. Item 2 follows from Item 3. The first equation in Item 4 follows from (14), and the second equation follows from (12). For Item 5, the convexity of \(\psi^{\prime}\) and inequality (19) follow from Lemma H.3, and \(\psi^{\prime}(\mathsf{dual}(v))=\psi(\mathsf{dual}(v))\) holds because \(\psi^{\prime}(\mathsf{dual}(v))=v\,\mathsf{dual}(v)-\varphi(v)\) by Item 3 and (11), and \(\psi(\mathsf{dual}(v))=v\,\mathsf{dual}(v)-\varphi(v)\) by Item 4.

Our discussion above gives a correspondence between a proper loss function \(\ell:\{0,1\}\times V\to\mathbb{R}\) and a tuple \((\varphi,\psi,\mathsf{dual})\) satisfying (13) and (14). By Lemma E.5, if \(\varphi,\psi\) and dual satisfy (13) and (14), then every \(v\in V\) is a sub-gradient of \(\psi\) at \(\mathsf{dual}(v)\). Therefore, assuming \(\psi\) is differentiable and using \(\nabla\psi:\mathbb{R}\to\mathbb{R}\) to denote its derivative, if \(t=\mathsf{dual}(v)\) for some \(v\in V\), then we can compute \(v\) from \(t\) by

\[v=\nabla\psi(t). \tag{20}\]

For the cross entropy loss \(\ell(y,v)=-y\ln v-(1-y)\ln(1-v)\), we can find the corresponding \(\varphi,\psi,\mathsf{dual}\) and \(\ell^{(\psi)}\) (see Definition 4.3) as follows:

\[\varphi(v) =-\mathop{\mathbb{E}}_{y\sim\mathsf{Ber}(v)}[\ell(y,v)]=v\ln v+(1 -v)\ln(1-v),\] \[\mathsf{dual}(v) =\ell(0,v)-\ell(1,v)=\ln(v/(1-v)),\] \[\psi(t) =\sup_{v\in(0,1)}\big{(}vt-\varphi(v)\big{)}=\ln(1+e^{t}),\] \[\ell^{(\psi)}(y,t) =\psi(t)-yt=\ln(1+e^{t})-yt,\] (logistic loss) \[\nabla\psi(t) =e^{t}/(1+e^{t}).\] (sigmoid transformation)

For the squared loss \(\ell(y,v)=(y-v)\)2, we can find the corresponding \(\varphi,\psi,\mathsf{dual}\) and \(\ell^{(\psi)}\) as follows:

Footnote 2: It is possible here for \(f(x)\) to lie outside the interval \([0,1]\), in which case \(f\) is not a valid predictor. However, if \(\psi\) satisfies (15), then \(\nabla\psi(t)\in[0,1]\) for every \(t\in\mathbb{R}\) and thus \(f(x)\in[0,1]\) for every \(x\in\mathcal{X}\). Also, one can find the corresponding \(\varphi,\psi,\mathsf{dual}\) and \(\ell^{(\psi)}\) as follows:

\[\varphi(v) =-\mathop{\mathbb{E}}_{y\sim\mathsf{Ber}(v)}[\ell(y,v)]=-v(1-v)^{ 2}-(1-v)v^{2}=v(v-1),\] \[\mathsf{dual}(v) =\ell(0,v)-\ell(1,v)=2v-1,\] \[\psi(t) =\sup_{v\in[0,1]}\big{(}vt-\varphi(v)\big{)}=\begin{cases}0,& \text{if }t<-1;\\ (t+1)^{2}/4,&\text{if }-1\leq t\leq 1;\\ t,&\text{if }t>1.\end{cases}\] \[\ell^{(\psi)}(y,t) =\psi(t)-yt=\begin{cases}-yt,&\text{if }t<-1;\\ (y-(t+1)/2)^{2},&\text{if }-1\leq t\leq 1;\\ (1-y)t,&\text{if }t>1.\end{cases}\] \[\nabla\psi(t) =\begin{cases}0,&\text{if }t<-1;\\ (t+1)/2,&\text{if }-1\leq t\leq 1;\\ 1,&\text{if }t>1.\end{cases}\]

For the squared loss, one can alternatively choose \(\psi(t)=(t+1)^{2}/4\) for _every_\(t\in\mathbb{R}\) and accordingly define \(\ell^{(\psi)}(y,t)=(y-(t+1)/2)^{2}\) for every \(y\in\{0,1\}\) and \(t\in\mathbb{R}\). This still ensures (12), (13) and (14), but (15) no longer holds.

## Appendix F Generalized Dual (Multi)calibration and Generalized Dual Post-processing Gap

Below we generalize the notions of dual post-processing gap and dual smooth calibration error from Section 4 by replacing the Lipschitz functions in those definitions with functions from a general class. Here we allow the functions to not only depend on the dual prediction \(g(x)\), but also depend directly on \(x\). With such generality, the calibration notion we consider in Definition F.2 below captures the notion of multicalibration [10]. We connect this generalized calibration notion with a generalized notion of post-processing gap (Definition F.1) in Theorem F.3.

**Definition F.1** (Generalized dual post-processing gap).: _Let \(\psi\) and \(\ell^{(\psi)}\) be defined as in Definition 4.3. Let \(W\) be a class of functions \(w:\mathcal{X}\times\mathbb{R}\to\mathbb{R}\). Let \(\mathcal{D}\) be a distribution over \(\mathcal{X}\times\{0,1\}\). We define the generalized dual post-processing gap of a function \(g:\mathcal{X}\to\mathbb{R}\) w.r.t. class \(W\) and distribution \(\mathcal{D}\) to be_

\[\mathsf{genGap}^{(\psi,W)}_{\mathcal{D}}(g):=\mathbb{E}_{(x,y)\sim\mathcal{D}} \,\ell^{(\psi)}(y,g(x))-\inf_{w\in W}\mathbb{E}_{(x,y)\sim\mathcal{D}}\,\ell ^{(\psi)}\big{(}y,w(x,g(x))\big{)}.\]

**Definition F.2** (Generalized dual calibration).: _Let \(\psi:\mathbb{R}\to\mathbb{R}\) be a differentiable function. For a function \(g:\mathcal{X}\to\mathbb{R}\), define predictor \(f:\mathcal{X}\to\mathbb{R}\) such that \(f(x)=\nabla\psi(g(x))\) for every \(x\in\mathcal{X}\).4 Let

[MISSING_PAGE_EMPTY:21]

### Proof of Theorem 4.6

We restate and prove Theorem 4.6.

**Theorem 4.6**.: _Let \(\psi:\mathbb{R}\to\mathbb{R}\) be a differentiable convex function with derivative \(\nabla\psi(t)\in[0,1]\) for every \(t\in\mathbb{R}\). For \(\lambda>0\), assume that \(\psi\) is \(\lambda\)-smooth, i.e.,_

\[|\nabla\psi(t)-\nabla\psi(t^{\prime})|\leq\lambda|t-t^{\prime}|\quad\text{for every }t,t^{\prime}\in\mathbb{R}. \tag{8}\]

_Then for every \(g:\mathcal{X}\to\mathbb{R}\) and any distribution \(\mathcal{D}\) over \(\mathcal{X}\times\{0,1\}\),_

\[\mathsf{smCE}^{(\psi,\lambda)}_{\mathcal{D}}(g)^{2}/2\leq\lambda\,\mathsf{pGap }^{(\psi,\lambda)}_{\mathcal{D}}(g)\leq\mathsf{smCE}^{(\psi,\lambda)}_{ \mathcal{D}}(g).\]

Proof.: Let \(W\) be the family of all \(\lambda\)-Lipschitz functions \(\eta:\mathbb{R}\to[-1,1]\) and define \(W^{\prime}\) as in Theorem F.3. We have

\[\mathsf{genCE}^{(\psi,W)}_{\mathcal{D}}(g)=\mathsf{smCE}^{(\psi,\lambda)}_{ \mathcal{D}}(g),\quad\text{and}\]

\[\mathsf{genGap}^{(\psi,W^{\prime})}_{\mathcal{D}}(g)=\mathsf{pGap}^{(\psi, \lambda)}_{\mathcal{D}}(g).\]

Theorem 4.6 then follows immediately from Theorem F.3. 

### Proof of Lemma 4.7

We restate and prove Lemma 4.7.

**Lemma 4.7**.: _Let \(\psi:\mathbb{R}\to\mathbb{R}\) be a differentiable convex function with derivative \(\nabla\psi(t)\in[0,1]\) for every \(t\in\mathbb{R}\). For \(\lambda>0\), assume that \(\psi\) is \(\lambda\)-smooth as in (8). For \(g:\mathcal{X}\to\mathbb{R}\), define \(f:\mathcal{X}\to[0,1]\) such that \(f(x)=\nabla\psi(g(x))\) for every \(x\in\mathcal{X}\). For a distribution \(\mathcal{D}\) over \(\mathcal{X}\times\{0,1\}\), define \(\mathsf{smCE}_{\mathcal{D}}(f)\) as in Definition 2.3. Then_

\[\mathsf{smCE}_{\mathcal{D}}(f)\leq\mathsf{smCE}^{(\psi,\lambda)}_{\mathcal{D}}( g).\]

Proof.: For a \(1\)-Lipschitz function \(\eta:[0,1]\to[-1,1]\), define \(\eta^{\prime}:\mathbb{R}\to[-1,1]\) such that \(\eta^{\prime}(t)=\eta(\nabla\psi(t))\) for every \(t\in\mathbb{R}\). For \(t_{1},t_{2}\in\mathbb{R}\), by the \(\lambda\)-smoothness property of \(\eta\),

\[|\eta^{\prime}(t_{1})-\eta^{\prime}(t_{2})|=|\eta(\nabla\psi(t_{1}))-\eta( \nabla\psi(t_{2}))|\leq|\nabla\psi(t_{1})-\nabla\psi(t_{2})|\leq\lambda|t_{1}- t_{2}|.\]

This proves that \(\eta^{\prime}\) is \(\lambda\)-Lipschitz. Therefore,

\[|\,\mathbb{E}_{(x,y)\sim\mathcal{D}}[(y-f(x))\eta(f(x))]|=|\,\mathbb{E}_{(x,y )\sim\mathcal{D}}(y-f(x))\eta^{\prime}(g(x))|\leq\mathsf{smCE}^{(\psi,\lambda) }_{\mathcal{D}}(g).\]

Taking supremum over \(\eta\) completes the proof. 

## Appendix G Optimization Algorithms Which Achieve Calibration

Continuing the discussion of structural risk minimization in Section 5, here we describe additional families of learning algorithms which explicitly minimize loss, but implicitly achieve good calibration.

### Iterative Risk Minimization

One intuition for why state-of-the-art DNNs are often calibrated is that they are "locally optimal" (in our sense) by design. For a state-of-the-art network, if it were possible to improve its test loss significantly by just adding a layer (and training it optimally), then the human practitioner training the DNN would have added another layer and re-trained. Thus, we expect the network eventually produced will be approximately loss-optimal with respect to adding a layer, and thus also with respect to univariate Lipshitz post-processings (so \(\mathsf{pGap}_{\mathcal{D}}(f)\approx 0\)).

This intuition makes several strong assumptions, for example, the practitioner must be able to re-train the last two layers globally optimally5. However, the same idea can apply in settings where optimization is tractable. We give one way of formalizing this via the Iterative Risk Minimization algorithm.

Footnote 5: Technically, we only require a weaker statement of the form: For all networks \(f_{d}\) of depth \(d\) in the support of SGD outputs, if the loss of \(\kappa\circ f_{d}\) is \(\varepsilon\), then training a network \(f_{d+1}\) of depth \(d+1\) will reach loss at most \(\varepsilon\).

We work with the following setup:* We are interested in models \(f\in\mathcal{F}\) with a complexity measure \(\mu:\mathcal{F}\to\mathbb{Z}\). Think of size for decision trees or depth for neural nets with a fixed architecture. We assume that \(\mathcal{F}\) contains all constant functions and that \(\mu(c)=0\) for any constant function \(c\).
* We consider post-processing functions \(\kappa:[0,1]\to[0,1]\) belonging to some family \(K\), which have bounded complexity under \(\mu\). Formally, there exists \(b=b(K)\in\mathbb{Z}\) such that for every \(f\in\mathcal{F}\) and \(\kappa\in K\), \[\mu(\kappa\circ f)\leq\mu(f)+b\].
* We have a loss function \(\ell:[0,1]\times\mathbb{R}\to\mathbb{R}\). Let \[\ell(f,\mathcal{D})=\mathop{\mathbb{E}}_{(x,y)\sim\mathcal{D}}[\ell(y,f)].\] For any complexity bound \(s\), we have an efficient procedure \(\mathsf{Min}_{\mathcal{D}}(s)\) that can solve the loss minimization problem over models of complexity \(s\): \[\min_{\begin{subarray}{c}f\in\mathcal{F}\\ \mu(f)\leq s\end{subarray}}\ell(f,\mathcal{D}).\] The running time can grow with \(s\). For each \(s\in\mathbb{Z}\), let \(\mathrm{OPT}_{s}\) denote the minimum of this program and let \(f_{s}^{*}\) denote the model found by \(\mathsf{Min}_{\mathcal{D}}(s)\) that achieves this minimum.

We use this to present an algorithm that produces a model \(f\) which is smoothly calibrated, has loss bounded by \(\mathrm{OPT}_{s}\), and complexity not too much larger than \(s\).

```
Input \(s_{0}\). \(s\gets s_{0}\). \(h\gets 0\). while True do \(f_{s}^{*}=\mathsf{Min}(s)\). \(f_{s+b}^{*}=\mathsf{Min}(s+b)\). if\(\mathrm{OPT}_{s+b}\geq\mathrm{OPT}_{s}-\alpha\)then  Return \(f_{s}^{*}\). else \(h\gets h+1\). \(s\gets s+b\). endif endwhile
```

**Algorithm 1** Local search for small pGap.

**Theorem G.1**.: _On input \(s_{0}\), Algorithm 1 terminates in \(h\leq 1/\alpha\) steps and returns a model \(f\in\mathcal{F}\) which satisfies the following guarantees:_

1. \(\mu(f)=t\leq s_{0}+hb\)_._
2. \(\ell_{2}(f,\mathcal{D})=\mathrm{OPT}_{t}\leq\mathrm{OPT}_{s_{0}}-h\alpha\)_._
3. \(\mathsf{pGap}(f)\leq\alpha\)_, hence_ \(\mathsf{smCE}(f)\leq\sqrt{\alpha}\)_._

Proof.: The bound on the number of steps follows by observing that each update decreases \(\ell_{2}(f,\mathcal{D})\) by \(\alpha\). Since \(\mathrm{OPT}_{s_{0}}\leq\mathrm{OPT}_{0}\leq 1\), this can only happen \(1/\alpha\) times. This also implies items (1) and (2), since each update increases the complexity by no more than \(b\), while decreasing the loss by at least \(\alpha\).

To prove claim (3), assume for contradiction that \(\mathsf{pGap}(f)>\alpha\). Then there exists \(\kappa\in K\) such that if we consider \(f^{\prime}=\kappa\circ f\), then

\[\mu(f^{\prime})\leq t+b,\ \ \ell_{2}(f,\mathcal{D})<\mathrm{OPT}_{t}-\alpha.\]

But this \(f^{\prime}\) provides a certificate that

\[\mathrm{OPT}_{t+b}\leq\ell_{2}(f^{\prime},\mathcal{D})<\mathrm{OPT}_{t}-\alpha.\]

Hence the termination condition is not satisfied for \(f\). So by contradiction, it must hold that \(\mathsf{pGap}(f)\leq\alpha\), hence \(\mathsf{smCE}(f)\leq\sqrt{\alpha}\)We now present an alternative algorithm that is close in spirit to algorithm 1, but can be viewed more directly as minimizing a regularized loss function. We assume access to the same minimization procedure \(\mathsf{Min}_{\mathcal{D}}(s)\) as before. We define the regularized loss

\[\ell^{\lambda}(f,\mathcal{D})=\ell(f,\mathcal{D})+\lambda\mu(f).\]

\(\lambda\) quantifies the tradeoff between accuracy and model complexity that we accept. In other words, we accept increasing \(\mu(f)\) by \(1\) if it results in a reduction of at least \(\lambda\) in the loss.

```
\(r\leftarrow\lfloor 1/\lambda\rfloor\). while\(s\in\{0,\cdots,r\}\)do \(f^{*}_{s}\leftarrow\mathsf{Min}(s)\). \(\mathrm{OPT}_{s}\leftarrow\ell_{2}(f^{*}_{s},\mathcal{D})\). endwhile \(t\leftarrow\operatorname*{argmin}_{s\in\{0,\ldots,r\}}\mathrm{OPT}_{s}+\lambda s\).  Return \(f^{*}_{t}\).
```

**Algorithm 2** Regularized loss minimization for small \(\mathsf{pGap}\).

**Theorem G.2**.: _Let \(\lambda=\alpha/b\). Algorithm 2 returns a model \(f\) with \(\mu(f)=t\leq\lfloor 1/\lambda\rfloor\) where_

1. \(f=\operatorname*{argmin}_{f^{\prime}\in\mathcal{F}}\ell^{\lambda}(f^{\prime}, \mathcal{D})\) _and_ \(\ell_{2}(f,\mathcal{D})=\mathrm{OPT}_{t}\)_._
2. _For any_ \(s\neq t\)_,_ \[\mathrm{OPT}_{s}\geq\mathrm{OPT}_{t}-(s-t)\lambda.\]
3. \(\mathsf{pGap}(f)\leq\alpha\)_, hence_ \(\mathsf{smCE}(f)\leq\sqrt{\alpha}\)_._

Proof.: We claim that

\[\min_{f^{\prime}\in\mathcal{F}}\ell^{\lambda}(f^{\prime},\mathcal{D})\leq 1.\]

This follows by taking the constant \(1/2\), which results in \(\ell_{2}(0,\mathcal{D})\leq 1\) and \(\mu(f)=0\). This means that the optimum is achieved for \(f\) where \(\mu(f)\leq 1/\lambda\). For \(f\) such that \(\mu(f)\leq t\), it is easy to see that Algorithm 2 finds the best model.

Assume that \(s\neq t\). By item (1), we must have

\[\mathrm{OPT}_{t}+\lambda t\leq\mathrm{OPT}_{s}+\lambda s\]

Rearranging this gives the claimed bound.

Assume that \(\mathsf{pGap}(f)=\alpha^{\prime}>\alpha\). Then there exists \(f^{\prime}=\kappa(f)\) such that \(\mu(f^{\prime})\leq\mu(f)+b\), but \(\ell(f^{\prime},\mathcal{D})\leq\ell_{2}(f,\mathcal{D})-\alpha^{\prime}\). But this means that

\[\ell^{\lambda}(f^{\prime}) =\ell(f^{\prime},\mathcal{D})+\lambda\mu(f^{\prime})\] \[\leq\ell_{2}(f,\mathcal{D})-\alpha^{\prime}+\lambda(\mu(f)+b)\] \[=\ell^{\lambda}(f)+(\lambda b-\alpha^{\prime})\] \[<\ell^{\lambda}(f)\]

where the inequality holds by our choice of \(\lambda=\alpha/b\). But this contradicts item (1). 

Note that if we take \(s-t=hb\), then \((s-t)\lambda=h\alpha\), so Item (2) in Theorem G.2 gives a statement matching Item (2) in Theorem G.1. The difference is that we now get a bound for any \(s\neq t\). When \(s>t\), item (3) upper bounds \(\mathrm{OPT}_{t}-\mathrm{OPT}_{s}\), which measures how much the \(\ell_{2}\) loss decreases with increased complexity. When \(s<t\), \(\mathrm{OPT}_{s}>\mathrm{OPT}_{t}\), and item (3) lower bounds how much the loss increases when shrink the complexity of the model.

**Discussion: Importance of depth.** These two algorithms highlight the central role of _composition_ in function families. Specifically, if we are optimizing over a function family that is not closed under univariate Lipshitz compositions (perhaps approximately), then we have no reason to expect \(\mathsf{pGap}(f)\approx 0\): because there could exist a simple (univariate, Lipshitz) transformation which reduces the loss, but which our function family cannot model. This may provide intuition for why certain models which preceeded DNNs, like logistic regression and SVMs, were often not calibrated. Moreover, it emphasizes the importance of depth in neural-networks: depth is required to model composing Lipshitz transforms, which improves both loss and calibration.

[MISSING_PAGE_FAIL:25]