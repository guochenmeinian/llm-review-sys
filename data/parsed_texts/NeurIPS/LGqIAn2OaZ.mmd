# Dynamic Tensor Decomposition via Neural Diffusion-Reaction Processes

 Zheng Wang

Kahlert School of Computing

University of Utah

Salt Lake City, UT 84112

u1208847@utah.edu

&Shikai Fang

Kahlert School of Computing

University of Utah

Salt Lake City, UT 84112

shikai.fang@utah.edu

&Shibo Li

Kahlert School of Computing

University of Utah

Salt Lake City, UT 84112

shibo@cs.utah.edu

&Shandian Zhe

Kahlert School of Computing

University of Utah

Salt Lake City, UT 84112

zhe@cs.utah.edu

Equal contributionCorresponding author.

###### Abstract

Tensor decomposition is an important tool for multiway data analysis. In practice, the data is often sparse yet associated with rich temporal information. Existing methods, however, often under-use the time information and ignore the structural knowledge within the sparsely observed tensor entries. To overcome these limitations and to better capture the underlying temporal structure, we propose Dynamic EMbedIngs fOr dynamic Tensor dEcomposition (DEMOTE). We develop a neural diffusion-reaction process to estimate dynamic embeddings for the entities in each tensor mode. Specifically, based on the observed tensor entries, we build a multi-partite graph to encode the correlation between the entities. We construct a graph diffusion process to co-evolve the embedding trajectories of the correlated entities and use a neural network to construct a reaction process for each individual entity. In this way, our model can capture both the commonalities and personalities during the evolution of the embeddings for different entities. We then use a neural network to model the entry value as a nonlinear function of the embedding trajectories. For model estimation, we combine ODE solvers to develop a stochastic mini-batch learning algorithm. We propose a stratified sampling method to balance the cost of processing each mini-batch so as to improve the overall efficiency. We show the advantage of our approach in both simulation study and real-world applications. The code is available at https://github.com/wzhut/Dynamic-Tensor-Decomposition-via-Neural-Diffusion-Reaction-Processes.

## 1 Introduction

Multiway data is common in real-world applications and naturally represented by tensors. For example, online shopping and promotion activities can be expressed as a three-mode tensor _(customer, commodity, online merchant)_. Tensor decomposition is an important tool for multiway data analysis. It estimates embeddings for the entities in each tensor mode, with which to recover the observedentry values. The embeddings can reflect the underlying structures within the entities and can be used as predictive features, such as for recommendation and ads auction.

In practice, tensor data is often very sparse. That is, the observed entries only take a tiny portion of all possible entries, say, \(0.01\%\). In addition, the data often includes timestamps for the observed entry values, which imply rich, complex temporal variation patterns. Current tensor decomposition approaches often ignore the structure knowledge within the sparsely observed entries and under-use the temporal information, _e.g._, simply binning the timestamps into crude time steps (Xiong et al., 2010; Rogers et al., 2013; Zhe et al., 2016, 2015; Du et al., 2018). More important, standard tensor decomposition estimates a static embedding for each entity. However, as the representation of entities, these embeddings summarize the underlying properties of the entities, which can naturally evolve with time, such as customer interests, user income, product popularity, and fashion. Learning static embeddings can miss capturing these interesting, important temporal knowledge. While the most recent work (Wang et al., 2022) has proposed the first decomposition method to estimate embedding trajectories, it never considers the structural knowledge within the data.

To overcome these limitations, we propose DEMOTE, a dynamic embedding approach for dynamic tensor decomposition. We construct a nonlinear diffusion-reaction process in an Ordinary Differential Equation (ODE) framework to estimate embedding trajectories for tensor entities. The ODE framework is known to be flexible and convenient to handle irregularly sampled timestamps and sparsely observed data (Rubanova et al., 2019). In addition, since ODE models focus on learning the dynamics (_i.e._, time derivatives) of the target function, they have promising potential for providing robust, accurate long-term predictions (via integration with the dynamics). Specifically, to leverage the structural knowledge within the data, we first build a multi-partite graph based on the observed entries. The graph encodes the correlations between entities at different modes in terms of their interactions. We then construct a graph diffusion process in the ODE to co-evolve the embedding trajectories of correlated entities. Next, we use a neural network to construct a reaction process to model the individual-specific evolution for each entity. In this way, our neural diffusion-reaction process captures both the commonalities and personalities of the entities in learning their dynamic embeddings. Given the embedding trajectories, we model the entry value as a latent function of the associated entities' trajectories. We use another neural network to flexibly estimate the function and to capture the complex relationships of the entities. For efficient training, we base on ODE solvers to develop a stochastic mini-batch learning algorithm. We develop a stratified sampling scheme, which can balance the cost of executing the ODE solvers in each mini-batch so as to improve the efficiency.

We evaluated our method in both simulation and real-world applications. The simulation experiments show that DEMOTE can successfully capture the underlying dynamics of the entities from their temporal interactions and recover the hidden clustering structures within the trajectories. Then in three real-world applications, we tested the accuracy in predicting the tensor entry values at different time points. DEMOTE consistently outperforms the state-of-the-art decomposition methods that incorporate temporal information, often by a large margin. We also demonstrated that both the diffusion and reaction processes contribute to the learning performance. Finally, we investigated the learned embedding trajectories and found interesting evolution paths and hidden structures.

## 2 Notations and Background

Suppose we have collected data for a \(K\)-mode tensor. Each mode \(k\) includes \(d_{k}\) entities, which we index by \(1,\ldots,d_{k}\). We then index each tensor entry by a tuple \(\boldsymbol{\ell}=(l_{1},\ldots,l_{K})\) where for each \(k\), we have \(1\leq l_{k}\leq d_{k}\). Suppose we observed \(N\) tensor entry values and timestamps. The dataset is denoted by \(\mathcal{D}=\{(\boldsymbol{\ell}_{1},t_{1},y_{1}),\ldots,(\boldsymbol{\ell}_ {N},t_{N},y_{N})\}\) where \(\{t_{n}\}\) and \(\{y_{n}\}\) are the timestamps and entry values, respectively. Our goal is for each entity \(j\) of mode \(k\), to estimate a dynamic embedding \(\mathbf{u}_{j}^{k}(t):\mathbb{R}_{+}\rightarrow\mathbb{R}^{R}\). That is, the embedding is a time function (trajectory) with an \(R\)-dimensional output. Standard tensor decomposition introduces a static embedding representation for each entity, namely, \(\mathbf{u}_{j}^{k}\) is considered as time invariant. Tensor decomposition aims to estimate the embeddings (or factors) to reconstruct the tensor. For example, the classical Tucker decomposition (Tucker, 1966) employs a multilinear factorization model, \(\mathcal{M}=\mathcal{W}\times_{1}\mathbf{U}^{1}\times_{2}\ldots\times_{K} \mathbf{U}^{K}\), where \(\mathcal{M}\in\mathbb{R}^{d_{1}\times\ldots\times d_{k}}\) is the entire tensor, \(\mathcal{W}\in\mathbb{R}^{R_{1}\times\ldots\times R_{K}}\) is the tensor-core parameter, \(\mathbf{U}^{k}\) comprises all the embeddings of the entities in mode \(k\), and \(\times_{k}\) is the tensor-matrix product at mode \(k\)(Kolda, 2006). The popular CANDECOMP/PARAFAC (CP) decomposition (Harshman, 1970) can be viewed as a simplified version of Tucker decomposition, where we set \(R_{1}=\ldots=R_{K}=R\) and the tensor-core \(\mathcal{W}\) to be diagonal. Hence, each entry value is factorized as \(m_{\bm{\ell}}=(\mathbf{u}^{1}_{l_{1}}\circ\ldots\circ\mathbf{u}^{K}_{l_{K}})^{\top} \bm{\lambda}\), where \(\circ\) is the Hadamard (element-wise) product, and \(\bm{\lambda}\) corresponds to \(\operatorname{diag}(\mathcal{W})\). While CP and Tucker decomposition are popular, their multilinear modeling can be oversimplistic for complex applications. To estimate nonlinear relationships of the entities, Xu et al. (2012); Zhe et al. (2015, 2016a) used a Gaussian process (GP) (Rasmussen and Williams, 2006) to model the entry value as a random function of the embeddings, \(m_{\bm{\ell}}=g(\mathbf{u}^{1}_{l_{1}},\ldots,\mathbf{u}^{K}_{l_{K}})\), where \(g\sim\mathcal{GP}\left(0,\kappa(\bm{\mathbf{x}_{\ell}},\bm{\mathbf{x}_{\ell} })\right)\), \(\bm{\mathbf{x}_{\ell}}=[\mathbf{u}^{1}_{l_{1}};\ldots;\mathbf{u}^{K}_{l_{K}}]\) and \(\bm{\mathbf{x}_{\ell^{\prime}}}=[\mathbf{u}^{1}_{l_{1}^{\prime}};\ldots; \mathbf{u}^{K}_{l_{K}^{\prime}}]\) are the embeddings of the entities in entry \(\bm{\ell}\) and \(\bm{\ell^{\prime}}\), respectively, and \(\kappa(\cdot,\cdot)\) is the covariance (kernel) function. Given the GP prior, any finite set of \(N\) entry values follow a multi-variate Gaussian distribution, \(\mathbf{m}\sim\mathcal{N}(\mathbf{0},\mathbf{K})\), where \(\mathbf{m}=[m_{\bm{\ell}_{1}},\ldots,m_{\bm{\ell}_{N}}]\), \(\mathbf{K}\) is the \(N\times N\) kernel matrix, and each \([\mathbf{K}]_{i,j}=\kappa(\bm{\mathbf{x}_{\ell}},\bm{\mathbf{x}_{\ell^{ \prime}}})\). Suppose we have collected continuous observations for the \(N\) entries \(\mathbf{y}=[y_{1},\ldots,y_{N}]\). We can use a Gaussian noise model: \(y_{n}=m_{\bm{\ell}_{n}}+\epsilon_{n}\) where \(\epsilon_{n}\sim\mathcal{N}(0,\sigma^{2})\). The marginal likelihood is \(p(\mathbf{y})=\mathcal{N}(\mathbf{y}|\mathbf{0},\mathbf{K}+\sigma^{2}\mathbf{ I})\), which we can maximize to estimate the embeddings and the model parameters.

Practical data often includes temporal information, _i.e._, the timestamp when each observed entry value is generated. To leverage this information, existing methods often bin the timestamps into a series of steps, say, by weeks or months (Xiong et al., 2010; Rogers et al., 2013; Zhe et al., 2016a; Song et al., 2017). The tensor is then expanded with an additional time-step mode, and one can apply any decomposition algorithm to estimate embeddings for both the entities and time steps. To capture the temporal dependency, a conditional model is often used (Xiong et al., 2010), say, \(p(\mathbf{t}_{j+1}|\mathbf{t}_{j})=\mathcal{N}(\mathbf{t}_{j+1}|\mathbf{t}_{j},\tau\mathbf{I})\) where \(\mathbf{t}_{j}\) is the embedding of \(j\)-th step. To leverage the continuous time information, Zhang et al. (2021) recently developed continuous CP decomposition, where the coefficients \(\bm{\lambda}\) are modeled as a time function with polynomial splines.

## 3 Model

Standard tensor decomposition assumes the embeddings are static and time-invariant. However, the embeddings summarize and extract the properties of entities, which can evolve with time, such as customer interests, health status, and product popularity. Therefore, only estimating static embeddings can miss important temporal variations of the entities' properties, resulting in poor representations and predictive performance. In addition, practical tensor data are typical sparse, and only a small portion of entries actually have data. Within these entries can be valuable structural knowledge. Current methods, however, are rarely aware of such knowledge. To overcome these limitations, we propose DEMOTE, a novel dynamic embedding approach.

Specifically, we propose an ODE model to learn the embedding trajectories \(\{\mathbf{u}^{k}_{j}(t)|1\leq k\leq K,1\leq j\leq d_{k}\}\). The ODE framework is known to be amenable for irregularly sampled, sparsely observed data. More important, ODE models concentrate on learning the time derivative \(\mathrm{d}\mathbf{u}^{k}_{j}/\mathrm{d}t\) (_i.e._, dynamics), rather than the trajectory function itself. Therefore, they have a promising potential to give reliable, long-term trajectory prediction (via numerical integration) even at time points far away from the training timestamps, provided the time derivative is well captured. We construct a joint ODE model for all the embedding trajectories. The ODE consists of a diffusion process and a

Figure 1: The illustration of the embedding model in DEMOTE.

reaction process. The diffusion process leverages the structural knowledge in data to co-evolve the embeddings of correlated entities, so as to better overcome the data sparsity. The reaction process models the entity-specific evolution so that it can capture the individual differences in the embedding evolution. The ODE model synergizes the two processes to capture both the commonalities and personalities of these embedding trajectories.

**Diffusion Process on Multi-Partite Graphs.** First, we construct a graph-based diffusion process to exploit the entity correlations reflected in data. Intuitively, if an observed entry involves entity A (_e.g._, customer A) and B (_e.g._, commodity B), the two entities are likely correlated. Thus, we can draw an edge between A and B to express the correlation. We then generalize this intuition to create a \(K\)-partite undirected graph \(\mathbb{G}(E,V)\), to encode such correlations across all the entities in the \(K\) tensor modes. Each vertex represents a particular entity, and the entire collection of the entities is partitioned into \(K\) groups, \(V=V^{1}\cup\ldots\cup V^{K}\), where group \(V^{k}=\{v^{k}_{1},\ldots,v^{k}_{d_{k}}\}\) represents the entities of mode \(k\). Two entities (at different modes) are connected if they were observed to interact, namely, \((v^{k}_{j},v^{k^{\prime}}_{j^{\prime}})\in\mathbb{E}\) if \(\exists\bm{\ell}_{n}\in\mathcal{D}\) such that \(\bm{\ell}_{n}=(\ldots,j,\ldots,j^{\prime},\ldots)\) where \(j\) and \(j^{\prime}\) are indices at mode \(k\) and \(k^{\prime}\), respectively. See Fig. 1 for an illustration. This graph naturally implies underlying information diffusion across the entities within their interactions. For example, if customer A connects to products B and C, it might mean that A distributes their interests/willingness/budgets to purchase B and C. The edges between one merchant A and a list of products {B, C,...} might indicate the diffusion of willingness to increase the inventory of these products.

To flexibly estimate the diffusion rate, we introduce a weight \(w^{k,k^{\prime}}_{j,j^{\prime}}\) for each edge \((v^{k}_{j},v^{k^{\prime}}_{j^{\prime}})\in E\). We then arrange these weights into \(K(K-1)\) adjacent matrices, \(\mathcal{W}=\{\mathbf{W}^{k,k^{\prime}}|1\leq k,k^{\prime}\leq K,k\neq k^{ \prime}\}\) where \(\mathbf{W}^{k,k^{\prime}}=\left(\mathbf{W}^{k^{\prime},k}\right)^{\top}\). Each \(\mathbf{W}^{k,k^{\prime}}\) is a sparse \(d_{k}\times d_{k^{\prime}}\) matrix that represents the edges and edge weights between \(V^{k}\) and \(V^{k^{\prime}}\), _i.e._, \([\mathbf{W}^{k,k^{\prime}}]_{j,j^{\prime}}=w^{k,k^{\prime}}_{j,j^{\prime}}\) if \((v^{k}_{j},v^{k^{\prime}}_{j^{\prime}})\in E\) and \(0\) otherwise. We now construct a diffusion process based on the \(K\)-partite graph. We view the embedding trajectory as a kind of concentration. For each entity \(j\) at mode \(k\), the change rate of its concentration (embedding) \(\mathbf{u}^{k}_{j}(t)\) is determined by the difference from the concentrations of its neighbors. Since the neighbors come from entities of all the other \(K-1\) modes, we have

\[\frac{\mathrm{d}\mathbf{u}^{k}_{j}}{\mathrm{d}t}=\sum_{s\in\{1,\ldots,K\} \setminus k}\sum_{j^{\prime}=1}^{d_{s}}[\mathbf{W}^{k,s}]_{j,j^{\prime}}\left( \mathbf{u}^{s}_{j^{\prime}}(t)-\mathbf{u}^{k}_{j}(t)\right)=\sum_{s\in\{1, \ldots,K\}\setminus k}\left(\mathbf{w}^{k,s}_{j}\mathbf{U}^{s}(t)\right)^{ \top}-a^{k,s}_{j}\mathbf{u}^{k}_{j},\]

where \(\mathbf{w}^{k,s}_{j}\) is the \(j\)-th row of \(\mathbf{W}^{k,s}\), \(\mathbf{U}^{s}(t)=[\mathbf{u}^{s}_{1}(t),\ldots,\mathbf{u}^{s}_{d_{s}}(t)]^{\top}\) is the embeddings of all the entities at mode \(s\), of size \(d_{s}\times R\), and \(a^{k,s}_{j}=\sum_{j^{\prime}=1}^{d_{s}}[\mathbf{W}^{k,s}]_{j,j^{\prime}}\) is the degree of vertex \(j\) in \(\mathbf{W}^{k,s}\). We can see that the evolution of the embeddings for different modes are coupled. Hence, it is natural to formulate the diffusion process jointly for all the embeddings, \(\frac{\mathrm{d}\mathcal{U}(t)}{\mathrm{d}t}=\mathrm{d}\left(\mathbf{U}^{1}(t),\ldots,\mathbf{U}^{K}(t)\right)/\mathrm{d}t=\mathcal{W}\mathcal{U}(t)- \mathcal{U}\mathcal{U}(t)=(\mathcal{W}-\mathcal{A})\mathcal{U}(t)\) where

\[\mathcal{W}=\left(\begin{array}{cccc}\mathbf{0}&\mathbf{W}^{1,2}&\cdots& \mathbf{W}^{1,K}\\ \mathbf{W}^{2,1}&\mathbf{0}&\cdots&\vdots\\ \vdots&&\ddots&\mathbf{W}^{K-1,K}\\ \mathbf{W}^{K,1}&\cdots&\mathbf{W}^{K,K-1}&\mathbf{0}\end{array}\right),\]

\(\mathcal{A}=\text{diag}\left(\sum_{s\in\{1\ldots K\}\setminus 1}\mathbf{A}^{1,s}, \ldots,\sum_{s\in\{1\ldots K\}\setminus K}\mathbf{A}^{K,s}\right)\), and each \(\mathbf{A}^{k,s}=\mathrm{diag}(a^{k,s}_{1},\ldots,a^{k,s}_{d_{k}})\) is the degree matrix of \(\mathbf{W}^{k,s}\).

**Reaction Process of Individual Entities.** Next, to capture the individual difference of each entity in evolving their embeddings, we model a local reaction process for each entity, \(\mathbf{f}_{\bm{\theta}_{k}}(\mathbf{u}^{k}_{j}(t),t)\), where \(\mathbf{f}(\cdot)\) is a neural network (NN), and \(\bm{\theta}_{k}\) are the NN (reaction) parameters for mode-\(k\) entities. The metaphor from the chemical physics is as follows. While substances are being diffused across different sites, at each site a chemical reaction process happens concurrently, which varies the concentration locally. We extend the model as a joint diffusion-reaction process,

\[\frac{\partial\mathcal{U}(t)}{\partial t}=(\mathcal{W}-\mathcal{A})\mathcal{U}(t )+\mathcal{F}(\mathcal{U},t),\quad\mathcal{U}(0)=\mathcal{U}_{0},\] (1)where \(\mathcal{F}(\mathcal{U},t)=[\mathbf{f}_{\theta_{1}}(\mathbf{u}^{1}_{1},t),\ldots, \mathbf{f}_{\theta_{l}}(\mathbf{u}^{1}_{d_{1}},t),\ldots,\mathbf{f}_{\theta_{K} }(\mathbf{u}^{K}_{1},t),\ldots,\mathbf{f}_{\theta_{K}}(\mathbf{u}^{K}_{d_{K}},t )]^{\top}\).

**Entry Value Generation.** Given the embedding trajectories, to obtain the tensor entry value \(m_{\boldsymbol{\ell}}\) at arbitrary time \(t\), we model \(m_{\boldsymbol{\ell}}(t)\) as a function of the relevant embeddings at time \(t\),

\[m_{\boldsymbol{\ell}}(t)=g\left(\mathbf{u}^{1}_{l_{1}}(t),\ldots,\mathbf{u}^{K }_{l_{K}}(t)\right).\] (2)

While one can follow (Xu et al., 2012; Zhe et al., 2016) to assign a GP prior over \(g(\cdot)\), the GP model needs to compute a giant kernel matrix over all the observed entry values (see Sec. 2). It is computationally too expensive or infeasible when the number of observations is large. Hence one has to seek for complex low-rank approximations. To avoid this problem, we model \(g\) with another neural network, which is not only as flexible as GP, but is more scalable and convenient for computation. Since now, the input to \(g(\cdot)\) consists of the trajectory values, which vary with time, our NN model for \(g\) can flexibly capture the complex temporal relationship between the entities. We finally sample the observed entry values with a Gaussian noise model, \(p(\mathbf{y}|\mathbf{m})=\mathcal{N}(\mathbf{y}|\mathbf{m},\sigma^{2}\mathbf{ I})\) where \(\mathbf{y}=[y_{1},\ldots,y_{N}]^{\top}\) and \(\mathbf{m}=[m_{\boldsymbol{\ell}_{1}}(t_{1}),\ldots,m_{\boldsymbol{\ell}_{N} }(t_{N})]^{\top}\). We focus on real-valued data in this paper. However, it is straightforward to extend our approach to other types of data.

## 4 Model Estimation

Given data \(\mathcal{D}=\{(\boldsymbol{\ell}_{1},t_{1},y_{1}),\ldots,(\boldsymbol{\ell}_{ N},t_{N},y_{N})\}\), the joint probability of our model is

\[p(\boldsymbol{\beta},\{\boldsymbol{\theta}_{k}\},\mathbf{y})=p(\boldsymbol{ \beta})\cdot\prod_{k=1}^{K}p(\boldsymbol{\theta}_{k})\cdot\prod_{n=1}^{N} \mathcal{N}\left(y_{n}|g\left(\mathbf{u}^{1}_{l_{n1}}(t_{n}),\ldots,\mathbf{u}^ {K}_{l_{nK}}(t_{n})\right),\sigma^{2}\mathbf{I}\right),\] (3)

where \(\boldsymbol{\beta}\) is the NN parameters for \(g\), each \(\boldsymbol{\theta}_{k}\) is the NN reaction parameters for mode-\(k\) entities, \(p(\boldsymbol{\beta})\) and \(p(\boldsymbol{\theta}_{k})\) are element-wise standard Gaussian, and \(\mathbf{y}=(y_{1},\ldots,y_{N})^{\top}\). To obtain the trajectory values in the Gaussian likelihood of each \(y_{n}\), we need to solve the ODE in (1) to time \(t_{n}\),

\[\mathcal{U}(t_{n})=\text{ODESolve}(\mathcal{U}_{0},0,t_{n},\Theta)\] (4)

where \(\Theta=\{\mathcal{W},\boldsymbol{\theta}_{1},\ldots,\boldsymbol{\theta}_{K}\}\) consists of the ODE parameters. Our goal is to estimate \(\Theta\), the initial state \(\mathcal{U}_{0}\), the NN parameters \(\boldsymbol{\beta}\), and the noise variance \(\sigma^{2}\).

**Stratified Mini-Batch Sampling.** We use stochastic mini-batch optimization to maximize the log joint probability so as to estimate all the required parameters,

\[\mathcal{L}= \log p(\boldsymbol{\beta},\{\boldsymbol{\theta}_{k}\},\mathbf{y} )=\log(\text{Prior})-\sum\nolimits_{n=1}^{N}\log\mathcal{N}\left(y_{n}|g \left(\mathbf{x}_{n}\right),\sigma^{2}\mathbf{I}\right)\]

where \(\log(\text{Prior})=\log p(\boldsymbol{\beta})+\sum_{k=1}^{K}\log p(\boldsymbol {\theta}_{k})\), and \(\mathbf{x}_{n}=\left(\mathbf{u}^{1}_{l_{n1}}(t_{n}),\ldots,\mathbf{u}^{K}_{l_{ nK}}(t_{n})\right)\). Each time, we sample a mini-batch of observations \(\mathcal{B}\), and obtain an unbiased stochastic estimate of the log probability, \(\widehat{\mathcal{L}}=\log(\text{Prior})-\frac{N}{B}\sum_{n\in\mathcal{B}} \left[\log\mathcal{N}(y_{n}|g(\mathbf{x}_{n}),\sigma^{2})\right]\). We compute \(\nabla\widehat{\mathcal{L}}\) as the stochastic gradient to update all the parameters.

For each data point \(n\) in the mini-batch, we need to run ODE solving (4) to obtain \(\mathbf{x}_{n}=\left(\mathbf{u}^{1}_{l_{n1}}(t_{n}),\ldots,\mathbf{u}^{K}_{l_{ nK}}(t_{n})\right)\). To back-propagate the gradient so as to compute the gradient w.r.t the ODE parameters \(\Theta\) and initial state \(\mathcal{U}_{0}\), we can either construct a computational graph during the running of the solver (_e.g._, the Runge-Kutta method (Dormand and Prince, 1980)), or use the adjoint state method (Pontryagin, 1987; Chen et al., 2018) that solves an adjoint backward ODE to compute the gradient. In whichever case, we need to sort the time points in the mini-batch and solve the ODE sequentially for these time points. As a result, the number of _unique_ time points in the mini-batch greatly influences the speed of processing the mini-batch. The standard mini-batch sampling (based on the training example indices) can result in an uneven allocation of the computational cost across the mini-batches -- some mini-batch is fast and some including more unique time points is much slower. To address this issue, we use a simple stratified sampling approach.

* We collect the unique time points in the whole dataset, \(\mathcal{T}=\{\tau_{1},\tau_{2},\ldots\}\) at the beginning.
* To conduct each stochastic update, we first sample \(B\) unique time points \(\mathcal{C}\) from \(\mathcal{T}\), then for each time point \(\tau_{j}\in\mathcal{C}\), we look at all the observed entry values produced at \(\tau_{j}\), namely \(\mathcal{D}_{\tau_{j}}=\{(\boldsymbol{\ell}_{n},t_{n},y_{n})\in\mathcal{D}|t_{n}= \tau_{j}\}\).
* We randomly sample one example from each \(\mathcal{D}_{\tau_{j}}\) to collect the mini-batch \(\mathcal{B}\).

[MISSING_PAGE_EMPTY:6]

where \(\mathds{1}(\cdot)\) is the indicator function. When \(l_{1}+l_{2}\) is even, the entry value is the trajectory value of the first entity; otherwise, it is the trajectory value of the second entity. To generate the training data, we randomly sampled entries from \(\{(l_{1},l_{2})|1\leq l_{1},l_{2}\leq 10\}\cup\{(l_{1},l_{2})|11\leq l_{1},l_{2} \leq 20\}\) (namely, interactions between cluster-1 entities of the two modes, and between cluster-2 entities). We then sampled \(t\sim\text{Uniform}[0,5]\), to obtain the corresponding entry values. We randomly generated 6,400 entry values and the timestamps for training, and another 1,600 data points for testing.

We implemented our method with Pytorch (Paszke et al., 2019). We used torchdiffeq library (https://github.com/rtqichen/torchdiffeq) to solve ODEs and to compute the gradient w.r.t ODE parameters and initial states via automatic differentiation. For the NN of the reaction process, we used one hidden layer, with 10 neurons and tanh activation, and for the NN to predict the interaction result, we used two hidden layers, 50 neurons per layer and tanh activation.

We compared with NONFAT (NONparametric Factor Trajectory learning) (Wang et al., 2022), a bi-level latent GP model that uses Fourier bases to estimate factor trajectories for dynamic tensor decomposition. To our knowledge, this work is the only method (and also the most recent) that estimates trajectories. We used the original implementation (https://github.com/wzhut/NONFAT) and the default settings. We set the mini-batch size to 50, and used ADAM (Kingma and Ba, 2014) algorithm for stochastic optimization. The learning rate was automatically adjusted in \([10^{-4},10^{-1}]\) by the ReduceLROnPlateau scheduler (Al-Kababji et al., 2022). The maximum number of epochs is 2K, which is enough for convergence. The estimated trajectories are shown in Fig. 1(a)-f. As we can see, our estimation (Fig. 1(c) and 1(f)) well matches the ground-truth and accurately recovers the cluster structure of the trajectories. The root-mean-square error (RMSE) on the test set is 0.032. By contrast, although the test error of NONFAT is close to DEMOTE (0.034), its learned trajectories (Fig. 1(b) and 1(e)) are far from the ground-truth, and fail to reflect the cluster structure. These have shown the advantage of DEMOTE in capturing complex relationships within data to recover the underlying trajectories and their structure.

### Prediction Accuracy

**Datasets.** We next evaluated the predictive performance of DEMOTE in three real-world applications. (1) _CA Weather_(Moosavi et al., 2019) (https://smoosavi.org/datasets/lstw), weather conditions in California from August 2016 to December 2020. We extracted a four-mode tensor for \(7\) different weather _types_, \(6\)_severity levels_, \(30\)_latitudes_ and \(30\)_longitudes_ in GPS coordinates. The entry value is the count of the particular weather condition. We collected \(15\)K observed tensor entry values and the timestamps. (2) _CA Traffic_(Moosavi et al., 2019) (https://smoosavi.org/dataset/lstw), traffic accidents in California from January 2018 to December 2020. We extracted a four

Figure 2: The estimated embedding trajectories for each mode. The color indicates the ground-truth cluster membership.

mode tensor (_traffic type_, _severity level_, _latitudes_, _longitude_). There are 7 traffic types, 6 severity levels, 20 latitudes and 20 longitudes. We collected \(30\)K entry values (accident counts) at different time points. (3) _Server Room_ (https://zenodo.org/record/3610078#.X1NpAigzaM8), temporal temperature records of Poznan Supercomputing and Networking Center. The temperatures were measured at 34 locations, under different air-condition modes (\(24^{\circ}\), \(27^{\circ}\), and \(30^{\circ}\)) and power usage settings (50%, 75% and 100%). Hence, we extracted a three-mode tensor (_location_, _air-condition mode_, _power level_). In total, \(10\)K observed entry values and their timestamps were collected.

**Competing Methods.** The following popular and/or state-of-the-art temporal decomposition approaches were compared. (1) CP-DTLD, discrete-time CP decomposition with linear dynamics, where a conditional prior is placed over successive time steps, \(p(\mathbf{t}_{j+1}|\mathbf{t}_{j})=\mathcal{N}(\mathbf{t}_{j+1}|\mathbf{A} \mathbf{t}_{j}+\mathbf{b},v\mathbf{I})\); \(\mathbf{A}\), \(\mathbf{b}\) and \(v\) were jointly estimated during the CP decomposition. Note that (Xiong et al., 2010) is an instance of this model where \(\mathbf{A}=\mathbf{I}\) and \(\mathbf{b}=\mathbf{0}\). (2) GP-DTLD and (3) NN-DTLD, similar to CP-DTLD, except using GP (Zhe et al., 2016) and NN decomposition models (similar to (Liu et al., 2019)), respectively. (4) CP-DTND, (5) GP-DTND and (6) NN-DTND -- CP, GP and NN decomposition with nonlinear dynamics, where the conditional prior is \(p(\mathbf{t}_{j+1}|\mathbf{t}_{j})=\mathcal{N}(\mathbf{t}_{j+1}|\sigma( \mathbf{A}\mathbf{t}_{j})+\mathbf{b},v\mathbf{I})\) where \(\sigma(\cdot)\) is a nonlinear activation. The dynamics can therefore be viewed as an RNN transition. (7) CP-CT (Zhang et al., 2021), continuous-time CP factorization, which models the CP coefficients as a time-varying function, with polynomial splines. (8) GP-CT, continuous-time GP decomposition that extends (Xu et al., 2012; Zhe et al., 2016) by plugging the time in the GP kernel so as to estimate the entry value as a function of the embeddings and time, \(m_{\boldsymbol{\ell}}=g(\mathbf{u}_{\ell_{1}}^{1},\ldots,\mathbf{u}_{\ell_{K} }^{K},t)\). (9) NN-CT, continuous-time NN decomposition, where the input consists of both the embeddings and time \(t\). (10) THIS-ODE (Li et al., 2022), a continuous-time decomposition, where a neural ODE is used to estimate the tensor entry values given the static embeddings and time. (11) NONFAT (Wang et al., 2022), a bi-level latent GP model that uses Fourier bases to estimate factor trajectories for dynamic tensor decomposition.

**Settings and Results.** All the approaches were implemented with PyTorch. The Square Exponential kernel was used for all the GP-related methods, including GP-{DTLD, DTND, CT}. We used the same variational sparse approximation (Hensman et al., 2013) to fulfill scalable posterior inference. Following (Zhe et al., 2016), the number of inducing point was set to \(100\). For the NN decomposition methods, we employed a three-layer network with tanh activation, and for THIS-ODE, we used a one-layer network. The layer width was chosen from {10, 25, 50, 75, 100}. We used tanh as the activation function in the nonlinear dynamic baselines, including {CP, GP, NN}-DTND. For our method, we used the same NN architecture for both the reaction process and entry value prediction, which includes two hidden layers with \(50\) neurons per layer. For CP-CT, we employed \(100\) knots to fulfill the polynomial splines. For each discrete-time method, the number of time steps was chosen from {25, 50, 75, 100} via the cross-validation on the training set. We trained all the models with stochastic mini-batch optimization. We used the ADAM algorithm, and the mini-batch size was set to \(100\). We ran every method with 10K epochs to ensure convergence. The learning rate was automatically adjusted in \([10^{-4},10^{-1}]\) by the ReduceLROnPlateau scheduler. We varied the dimension of the embeddings \(R\) from {2, 3, 5, 7}. For DEMOTE, \(R\) is the number of embedding trajectories; we used computational graphs to obtain the gradient. We followed (Kang et al., 2012; Zhe et al., 2016) to randomly draw \(80\%\) observed entries and their time stamps for training, with the remaining for test. We computed the normalized root-mean-square error (nRMSE). We repeated the evaluation five times and computed the average nRMSE and standard deviation.

Figure 3: Predictive performance of the diffusion and reaction processes.

[MISSING_PAGE_FAIL:9]

We showcase the temporal predictions for two tensor entries. As we can see from Fig. 5, given only a few training points (blue), our method can predict the test points (green) much more accurately, as compared with GPCT, and the predictive uncertainty (reflected by the noise variance \(\sigma^{2}\)) is much smaller. This might be due to that via the diffusion-reaction process, and our method can more effectively extract the temporal knowledge from sparse data. For example, DEMOTE successfully captured the periodic nature in the first entry (Fig. 4(b)) while GPCT treated the fluctuation as noises and ended up with much inaccurate predictions and larger predictive variances.

**Computational Efficiency.** We compared the per-epoch/iteration running time DEMOTE with the other methods. We tested all the methods in a workstation with one NVIDIA GeForce RTX 3090 Graphics Card, 10th Generation Intel Core i9-10850K Processor, 32 GB RAM, and 1 TB SSD. The results are shown in Table 2 in Appendix. We can see that the running speed of DEMOTE is comparable to NONFAT and other NN decomposition methods. We also compared with running DEMOTE with naive sampling (DEMOTE-NS). The stratified sampling led to 4x to 22x speed-up.

## 7 Conclusion

We have presented DEMOTE, a neural diffusion-reaction process model to learn dynamic embeddings for dynamic tensor decomposition. The predictive performance is encouraging, and the learned embedding trajectories exhibit interesting patterns. Currently, our method is limited to a small number of entities since it has to integrate the entire multi-partite graph to construct the diffusion process. In the future work, we plan to develop graph cut algorithms to partition the graph into a set of small sub-graphs so that we can construct multiple diffusion processes in parallel so as to scale up our model to big graphs and to large tensors.

## Acknowledgments

This work has been supported by NSF CAREER Award IIS-2046295.

Figure 4: The learned embedding trajectories for location 1 (a-c), air conditional mode 1 (d-f), and power usage level 1 (g-i) in _Server Room_ dataset.

Figure 5: Entry value prediction on _Server Room_.

## References

* Ahn et al. (2021) Ahn, D., Jang, J.-G., and Kang, U. (2021). Time-aware tensor decomposition for sparse tensors. _Machine Learning_, pages 1-22.
* Al-Kababaji et al. (2022) Al-Kababji, A., Bensaali, F., and Dakua, S. P. (2022). Scheduling techniques for liver segmentation: Reduclronplateau vs onecyclelr. _arXiv preprint arXiv:2202.06373_.
* Atwood and Towsley (2016) Atwood, J. and Towsley, D. (2016). Diffusion-convolutional neural networks. _Advances in neural information processing systems_, 29.
* Chamberlain et al. (2021) Chamberlain, B., Rowbottom, J., Gorinova, M. I., Bronstein, M., Webb, S., and Rossi, E. (2021). Grand: Graph neural diffusion. In _International Conference on Machine Learning_, pages 1407-1418. PMLR.
* Chen et al. (2018) Chen, R. T., Rubanova, Y., Bettencourt, J., and Duvenaud, D. K. (2018). Neural ordinary differential equations. _Advances in neural information processing systems_, 31.
* Dormand and Prince (1980) Dormand, J. R. and Prince, P. J. (1980). A family of embedded runge-kutta formulae. _Journal of computational and applied mathematics_, 6(1):19-26.
* Du et al. (2018) Du, Y., Zheng, Y., Lee, K.-c., and Zhe, S. (2018). Probabilistic streaming tensor decomposition. In _2018 IEEE International Conference on Data Mining (ICDM)_, pages 99-108. IEEE.
* Fang et al. (2021a) Fang, S., Kirby, R. M., and Zhe, S. (2021a). Bayesian streaming sparse Tucker decomposition. In _Uncertainty in Artificial Intelligence_, pages 558-567. PMLR.
* Fang et al. (2022) Fang, S., Narayan, A., Kirby, R., and Zhe, S. (2022). Bayesian continuous-time Tucker decomposition. In _International Conference on Machine Learning_, pages 6235-6245. PMLR.
* Fang et al. (2021b) Fang, S., Wang, Z., Pan, Z., Liu, J., and Zhe, S. (2021b). Streaming Bayesian deep tensor factorization. In _International Conference on Machine Learning_, pages 3133-3142. PMLR.
* Harshman (1970) Harshman, R. A. (1970). Foundations of the PARAFAC procedure: Model and conditions for an"explanatory"multi-mode factor analysis. _UCLA Working Papers in Phonetics_, 16:1-84.
* Hensman et al. (2013) Hensman, J., Fusi, N., and Lawrence, N. D. (2013). Gaussian processes for big data. In _Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence_, pages 282-290. AUAI Press.
* Huang et al. (2021) Huang, Z., Sun, Y., and Wang, W. (2021). Coupled graph ode for learning interacting system dynamics. In _27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2021_, pages 705-715.
* Kang et al. (2012) Kang, U., Papalexakis, E., Harpale, A., and Faloutsos, C. (2012). Gigatensor: scaling tensor analysis up by 100 times-algorithms and discoveries. In _Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining_, pages 316-324. ACM.
* Ketchen and Shook (1996) Ketchen, D. J. and Shook, C. L. (1996). The application of cluster analysis in strategic management research: an analysis and critique. _Strategic management journal_, 17(6):441-458.
* Kingma and Ba (2014) Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. cite arxiv:1412.6980Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015.
* Kolda (2006) Kolda, T. G. (2006). _Multilinear operators for higher-order decompositions_, volume 2. United States. Department of Energy.
* Li et al. (2022) Li, S., Kirby, R., and Zhe, S. (2022). Decomposing temporal high-order interactions via latent odes. In _International Conference on Machine Learning_, pages 12797-12812. PMLR.
* Liu et al. (2019) Liu, H., Li, Y., Tsang, M., and Liu, Y. (2019). _CoSTCo: A Neural Tensor Completion Model for Sparse Tensors_, page 324-334. Association for Computing Machinery, New York, NY, USA.
* Liu et al. (2019)Moosavi, S., Samavatian, M. H., Nandi, A., Parthasarathy, S., and Ramnath, R. (2019). Short and long-term pattern discovery over large-scale geo-spatiotemporal data. In _Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining_, pages 2905-2913.
* Pan et al. (2021) Pan, Z., Wang, Z., Phillips, J. M., and Zhe, S. (2021). Self-adaptable point processes with nonparametric time decays. _Advances in Neural Information Processing Systems_, 34:4594-4606.
* Pan et al. (2020a) Pan, Z., Wang, Z., and Zhe, S. (2020a). Scalable nonparametric factorization for high-order interaction events. In _International Conference on Artificial Intelligence and Statistics_, pages 4325-4335. PMLR.
* Pan et al. (2020b) Pan, Z., Wang, Z., and Zhe, S. (2020b). Streaming nonlinear Bayesian tensor decomposition. In _Conference on Uncertainty in Artificial Intelligence_, pages 490-499. PMLR.
* Paszke et al. (2019) Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. (2019). Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32:8026-8037.
* Pontryagin (1987) Pontryagin, L. S. (1987). _Mathematical theory of optimal processes_. CRC press.
* Rai et al. (2014) Rai, P., Wang, Y., Guo, S., Chen, G., Dunson, D., and Carin, L. (2014). Scalable Bayesian low-rank decomposition of incomplete multiway tensors. In _Proceedings of the 31th International Conference on Machine Learning (ICML)_.
* Rasmussen and Williams (2006) Rasmussen, C. E. and Williams, C. K. I. (2006). _Gaussian Processes for Machine Learning_. MIT Press.
* Rogers et al. (2013) Rogers, M., Li, L., and Russell, S. J. (2013). Multilinear dynamical systems for tensor time series. _Advances in Neural Information Processing Systems_, 26:2634-2642.
* Rubanova et al. (2019) Rubanova, Y., Chen, R. T. Q., and Duvenaud, D. K. (2019). Latent ordinary differential equations for irregularly-sampled time series. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alche-Buc, F., Fox, E., and Garnett, R., editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc.
* Schein et al. (2015) Schein, A., Paisley, J., Blei, D. M., and Wallach, H. (2015). Bayesian poisson tensor factorization for inferring multilateral relations from sparse dyadic event counts. In _Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, pages 1045-1054. ACM.
* Volume 48_, ICML'16, pages 2810-2819. JMLR.org.
* Song et al. (2017) Song, Q., Huang, X., Ge, H., Caverlee, J., and Hu, X. (2017). Multi-aspect streaming tensor completion. In _Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, pages 435-443.
* Tillinghast et al. (2020) Tillinghast, C., Fang, S., Zhang, K., and Zhe, S. (2020). Probabilistic neural-kernel tensor decomposition. In _2020 IEEE International Conference on Data Mining (ICDM)_, pages 531-540. IEEE.
* Tillinghast et al. (2022) Tillinghast, C., Wang, Z., and Zhe, S. (2022). Nonparametric sparse tensor factorization with hierarchical Gamma processes. In _International Conference on Machine Learning_, pages 21432-21448. PMLR.
* Tillinghast and Zhe (2021) Tillinghast, C. and Zhe, S. (2021). Nonparametric decomposition of sparse tensors. In _International Conference on Machine Learning_, pages 10301-10311. PMLR.
* Tucker (1966) Tucker, L. (1966). Some mathematical notes on three-mode factor analysis. _Psychometrika_, 31:279-311.
* Tikhonov and Arsenin (2017)Wang, Z., Chu, X., and Zhe, S. (2020). Self-modulating nonparametric event-tensor factorization. In _International Conference on Machine Learning_, pages 9857-9867. PMLR.
* Wang et al. (2022) Wang, Z., Xu, Y., Tillinghast, C., Li, S., Narayan, A., and Zhe, S. (2022). Nonparametric embeddings of sparse high-order interaction events. In _International Conference on Machine Learning_, pages 23237-23253. PMLR.
* Wu et al. (2019) Wu, X., Shi, B., Dong, Y., Huang, C., and Chawla, N. V. (2019). Neural tensor factorization for temporal interaction learning. In _Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining_, pages 537-545.
* Xiong et al. (2010) Xiong, L., Chen, X., Huang, T.-K., Schneider, J., and Carbonell, J. G. (2010). Temporal collaborative filtering with bayesian probabilistic tensor factorization. In _Proceedings of the 2010 SIAM International Conference on Data Mining_, pages 211-222. SIAM.
* Xu et al. (2012) Xu, Z., Yan, F., and Qi, Y. A. (2012). Infinite tucker decomposition: Nonparametric bayesian models for multiway data analysis. In _ICML_.
* Yang and Dunson (2013) Yang, Y. and Dunson, D. (2013). Bayesian conditional tensor factorizations for high-dimensional classification. _Journal of the Royal Statistical Society B, revision submitted_.
* Zhang et al. (2021) Zhang, Y., Bi, X., Tang, N., and Qu, A. (2021). Dynamic tensor recommender systems. _Journal of Machine Learning Research_, 22(65):1-35.
* Zhe and Du (2018) Zhe, S. and Du, Y. (2018). Stochastic nonparametric event-tensor decomposition. In _Advances in Neural Information Processing Systems_, pages 6856-6866.
* Zhe et al. (2016a) Zhe, S., Qi, Y., Park, Y., Xu, Z., Molloy, I., and Chari, S. (2016a). Dintucker: Scaling up Gaussian process models on large multidimensional arrays. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 30.
* Zhe et al. (2015) Zhe, S., Xu, Z., Chu, X., Qi, Y., and Park, Y. (2015). Scalable nonparametric multiway data analysis. In _Artificial Intelligence and Statistics_, pages 1125-1134. PMLR.
* Zhe et al. (2016b) Zhe, S., Zhang, K., Wang, P., Lee, K.-c., Xu, Z., Qi, Y., and Ghahramani, Z. (2016b). Distributed flexible nonlinear tensor factorization. _Advances in neural information processing systems_, 29.

## Appendix A Investigation of Embedding Dynamics

We investigated if there are underlying structures within the embeddings during their evolution. To this end, we looked into the embeddings of the 34 locations on _Server Room_ dataset at five time points (\(t\) = 1, 20, 50, 80, 100). At each time point, we ran the k-means algorithm over the embeddings to extract the clustering structures. We used the elbow method (Ketchen and Shook, 1996) to select the number of clusters. We can see that at earlier time (\(t\leq 50\)), the clusters are more compact, while at the later stages, the clusters become more scattered. This reflects how the structure of those entities (_i.e._, locations) evolves along with time. It is interesting to see that some locations are in the same cluster all the time, like location {5,7} and location {16, 32}. It implies that their underlying properties might have quite similar (or correlated) evolution patterns. Some locations are grouped in the cluster at the beginning, _e.g._, location {32, 34} (at \(t=1\)), but later moves to different clusters (\(t>1\)). It implies their evolution patterns can vary significantly, leading to the change of the cluster memberships.

\begin{table}
\begin{tabular}{c c c c} \hline  & _CA Weather_ & _CA Traffic_ & _Server Room_ \\ \hline CP-DTLD & 0.037 & 0.086 & 0.023 \\ GP-DTLD & 0.246 & 0.247 & 0.248 \\ NN-DTLD & 2.400 & 4.730 & 1.080 \\ CP-DTND & 0.038 & 0.087 & 0.025 \\ GP-DTND & 0.119 & 0.242 & 0.080 \\ NN-DTND & 2.360 & 4.701 & 1.060 \\ CP-CT & 0.025 & 0.052 & 0.018 \\ GP-CT & 0.068 & 0.216 & 0.105 \\ NN-CT & 2.310 & 3.885 & 1.030 \\ NONFAT & 0.952 & 1.925 & 0.571 \\ THIS-ODE & 58.710 & 136.100 & 7.190 \\ DEMOTE & 1.390 & 1.895 & 0.309 \\ DEMOTE-NS & 6.12 & 10.42 & 7.06 \\ \hline \end{tabular}
\end{table}
Table 2: Per-epoch/iteration running time (in seconds). DEMOTE-NS means running DEMOTE with naive sampling of min-batches rather than the stratified sampling.

Figure 6: Evolution of the clustering structure within the 34 locations on _Server Room_ dataset.