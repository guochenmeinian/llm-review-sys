# Mirror Diffusion Models for

Constrained and Watermarked Generation

 Guan-Horng Liu, Tianrong Chen, Evangelos A. Theodorou\({}^{\dagger}\), Molei Tao\({}^{\dagger}\)

Georgia Institute of Technology, USA

{ghliu, tianrong.chen, evangelos.theodorou, mtao}@gatech.edu

Equal advising.

###### Abstract

Modern successes of diffusion models in learning complex, high-dimensional data distributions are attributed, in part, to their capability to construct diffusion processes with analytic transition kernels and score functions. The tractability results in a simulation-free framework with stable regression losses, from which reversed, generative processes can be learned at scale. However, when data is confined to a constrained set as opposed to a standard Euclidean space, these desirable characteristics appear to be lost based on prior attempts. In this work, we propose **Mirror Diffusion Models (MDM)**, a new class of diffusion models that generate data on convex constrained sets without losing any tractability. This is achieved by learning diffusion processes in a dual space constructed from a mirror map, which, crucially, is a standard Euclidean space. We derive efficient computation of mirror maps for popular constrained sets, such as simplices and \(\ell_{2}\)-balls, showing significantly improved performance of MDM over existing methods. For safety and privacy purposes, we also explore constrained sets as a new mechanism to embed invisible but quantitative information (_i.e.,_ watermarks) in generated data, for which MDM serves as a compelling approach. Our work brings new algorithmic opportunities for learning tractable diffusion on complex domains.

## 1 Introduction

Diffusion models [1, 2, 3] have emerged as powerful generative models with their remarkable successes in synthesizing high-fidelity data such as images [4, 5], audio [6, 7], and 3D geometry [8, 9]. These models work by progressively diffusing data to noise, and learning the score functions (often

Figure 1: **Mirror Diffusion Models (MDM)** is a new class of diffusion models for convex constrained manifolds \(\mathcal{M}\subseteq\mathbb{R}^{d}\). **(left)** Instead of learning score-approximate diffusions on \(\mathcal{M}\), MDM applies a _mirror_ map \(\nabla\phi\) and learns _tractable_ diffusions in its _unconstrained dual-space_\(\nabla\phi(\mathcal{M})=\mathbb{R}^{d}\). **(right)** We also present MDM for watermarked generation, where generated contents (_e.g.,_ images) live in a high-dimensional _token_ constrained set \(\mathcal{M}\) that is certifiable only from the private user.

parameterized by neural networks) to reverse the processes [10]; the reversed processes thereby provide transport maps that generate data from noise. Modern diffusion models [11, 12, 13] often employ diffusion processes whose transition kernels are analytically available. This characteristic enables _simulation-free_ training, bypassing the necessity to simulate the underlying diffusion processes by directly sampling the diffused data. It also leads to tractable conditional score functions and simple regression objectives, facilitating computational scalability for high-dimensional problems. In standard Euclidean spaces, tractability can be accomplished by designing the diffusion processes as linear stochastic differential equations, but doing so for non-Euclidean spaces is nontrivial.

Recent progress of diffusion models has expanded their application to non-Euclidean spaces, such as Riemannian manifolds [14, 15], where data live in a curved geometry. The development of generative models on manifolds has enabled various new applications, such as modeling earth and climate events [16] and learning densities on meshes [17]. In this work, we focus on generative modeling on _constrained sets_ (also called _constrained manifolds_[18]) that are defined by a set of inequality constraints. Such constrained sets, denoted by \(\mathcal{M}\subseteq\mathbb{R}^{d}\), are also ubiquitous in several scientific fields such as computational statistics [19], biology [20], and robotics [21, 22].

Previous endeavors in the direction of constrained generation have primarily culminated in reflected diffusions [23, 24], which reflect the direction at the boundary \(\partial\mathcal{M}\) to ensure that samples remain inside the constraints. Unfortunately, reflected diffusions do not possess closed-form transition kernels [18, 25], thus necessitating the approximation of the conditional score functions that can hinder learning performance. Although simulation-free training is still possible2, it comes with a computational overhead due to the use of geometric techniques [25]. From a theoretical standpoint, reflected Langevin dynamics are widely regarded for their extended mixing time [26, 27]. This raises practical concerns when simulating reflected diffusions, as prior methods often adopted special treatments such as thresholding and discretization [18, 25].

Footnote 2: Lou and Ermon [25] proposed a semi-simulation-free training with the aid of geometric techniques.

An alternative diffusion for constrained sampling is the Mirror Langevin Dynamics (MLD [28]). MLD, as a subclass of Riemannian Langevin Dynamics [29, 30], is tailored specifically to convex constrained sampling. Specifically, MLD constructs a strictly convex function \(\phi\) so that its gradient map, often referred to as the _mirror map_\(\nabla\phi:\mathcal{M}\rightarrow\mathbb{R}^{d}\), defines a nonlinear mapping from the initial constrained set to an _unconstrained dual space_. Despite extensive investigation of MLD [31, 32, 33, 34], its potential as a foundation for designing diffusion generative models remains to be comprehensively examined. Thus, there is a need to explore the potential of MLD for designing new, effective, and tailored diffusion models for constrained generation.

With this in mind, we propose **Mirror Diffusion Models (MDM)**, a new class of diffusion generative models for convex constrained sets. Similar to MLD, MDM utilizes mirror maps \(\nabla\phi\) that transform data distributions on \(\mathcal{M}\) to its dual space \(\nabla\phi(\mathcal{M})\). From there, MDM learns a _dual-space diffusion_ between the mirrored distribution and Gaussian prior in the dual space. Note that this is in contrast to MLD, which constructs invariant distributions on the initial constrained set; MDM instead considers dual-space priors. Since the dual space is also unconstrained Euclidean space \(\nabla\phi(\mathcal{M})=\mathbb{R}^{d}\), MDM

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Diffusion models & Domain & \begin{tabular}{c} Tractable \\ conditional score \\ \end{tabular} & \begin{tabular}{c} Simulation-free \\ training \\ \end{tabular} & 
\begin{tabular}{c} Regression \\ objective \\ \end{tabular} \\ \hline _Reflected Diffusion_ & & & & \\ Fishman et al. [18] & \(\mathcal{M}\) & ✗ & ✗ & ✗ \\ Lou and Ermon [25] & \(\mathcal{M}\) & ✗ & \(\triangle^{2}\) & ✓ \\ \hline _Mirror Diffusion_ & & & & \\ This work (ours) & \(\nabla\phi(\mathcal{M})\) & ✓ & ✓ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of different diffusion models for constrained generation on \(\mathcal{M}\subseteq\mathbb{R}^{d}\). Rather than learning reflected diffusions on \(\mathcal{M}\) with approximate scores, our Mirror Diffusion constructs a mirror map \(\nabla\phi\) and lift the diffusion processes to the _unconstrained dual space_\(\nabla\phi(\mathcal{M})=\mathbb{R}^{d}\), inheriting favorable features from standard Euclidean-space diffusion models. Constraints are satisfied by construction via the inverse map \(\nabla\phi^{*}\); that is, \(\nabla\phi^{*}(y)\in\mathcal{M}\) for all \(y\in\nabla\phi(\mathcal{M})\).

preserves many important features of standard diffusion models, such as simulation-free training and tractable conditional scores, which yields simple regression objectives; see Table 1. After learning the dual-space distribution, MDM applies the inverse mapping \(\nabla\phi^{*}\), transforming mirrored samples back to the constrained set. We propose efficient construction of mirror maps and demonstrate that MDM outperforms prior reflected diffusion models on common constrained sets, such as \(\ell_{2}\)-balls and simplices.

Moreover, we show that MDM also stands as a novel approach for _watermarked generation_, a technique that aims to embed undetectable information (_i.e.,_ watermarks) into generated contents to prevent unethical usage and ensure copyright protection. This newly emerging goal nevertheless attracted significant attention in natural language generation [35, 36, 37], while what we will present is a general algorithmic strategy demonstrated for image generations.

To generate watermarked contents that are private to individual users, we begin by constructing a constrained set \(\mathcal{M}\) based on user-defined tokens (see Figure 2). MDM can then be instantiated to learn the dual-space distribution. Alternatively, MDM can distill pretrained diffusion models [38] by projecting the generated data onto the constraint set. In either case, MDM generates watermarked contents that can be certified only by users who know the tokens. Our empirical results suggest that MDM serves as a competitive approach to prior watermarking techniques for diffusion models, such as those proposed by Zhao et al. [39], by achieving lower (hence better) FID values.

In summary, we present the following contributions:

* We introduce **Mirror Diffusion Models (MDM)**, a new class of diffusion models that utilizes mirror maps to enable constrained generation with Euclidean-space diffusion models.
* We propose efficient computation of mirror maps on common constrained sets, including balls and simplices. Our results demonstrate that MDM consistently outperforms previous reflected diffusion models.
* As a novel application of constrained generation, we explore MDM's potential for watermarking and copyrighting diffusion models where constraints serve as private tokens. We show that it yields competitive performance compared to prior methods.

## 2 Preliminary on Euclidean-space Diffusion Models

Notation\(\mathcal{M}\subseteq\mathbb{R}^{d}\) denotes a convex constrained set. We preserve \(x\in\mathcal{M}\) and \(y\in\mathbb{R}^{d}\) to better distinguish variables on constrained sets and standard \(d\)-dimensional Euclidean space. \(\bm{I}\in\mathbb{R}^{d\times d}\) denotes the identity matrix. \(x_{i}\) denotes the \(i\)-th element of a vector \(x\). Similarly, \([A]_{ij}\) denotes the the element of a matrix \(A\) at \(i\)-th row and \(j\)-th column.

Diffusion processGiven \(y_{0}\in\mathbb{R}^{d}\) sampled from some Euclidean-space distribution, conventional diffusion models define the diffusion process as a _forward_ Markov chain with the joint distribution:

\[q(y_{1:T}|y_{0})=\prod_{t=1}^{T}q(y_{t}|y_{t-1}),\quad q(y_{t}|y_{t-1}):= \mathcal{N}(y_{t};\sqrt{1-\beta_{t}}y_{t-1},\beta_{t}\bm{I}),\] (1)

which progressively injects Gaussian noise to \(y_{0}\) such that, for a sufficiently large \(T\) and a properly chosen noise schedule \(\beta_{t}\in(0,1)\), \(y_{T}\) approaches an approximate Gaussian, _i.e.,_\(q(y_{T})\approx\mathcal{N}(0,\bm{I})\)

Figure 2: MDM for watermarked generation: **(left)** We first construct a constrained set \(\mathcal{M}\) based on a set of user-defined tokens private to other users. **(right)** MDM can be instantiated by either learning the corresponding dual-space diffusions, or projecting pretrained, _i.e.,_ unwatermarked, diffusion models onto \(\mathcal{M}\). In both cases, MDM embeds watermarks that are certifiable only from the user.

Equation (1) has tractable marginal densities, and two of which that are of particular interest are:

\[q(y_{t}|y_{0})=\mathcal{N}(y_{t};\sqrt{\bar{\alpha}_{t}}y_{0},(1-\alpha_{t}) \boldsymbol{I}),\quad q(y_{t-1}|y_{t},y_{0})=\mathcal{N}(y_{t-1};\tilde{\mu}_{t }(y_{t},y_{0}),\tilde{\beta}_{t}\boldsymbol{I}),\] (2)

\[\tilde{\mu}_{t}(y_{t},y_{0}):=\frac{\sqrt{\alpha_{t-1}}\beta_{t}}{1-\bar{ \alpha}_{t}}y_{0}+\frac{\sqrt{1-\beta_{t}}(1-\bar{\alpha}_{t-1})}{1-\bar{ \alpha}_{t}}y_{t},\quad\tilde{\beta}_{t}:=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{ \alpha}_{t}}\beta_{t},\quad\bar{\alpha}_{t}:=\prod_{s=0}^{t}(1-\beta_{s}).\]

The tractable marginal \(q(y_{t}|y_{0})\) enables direct sampling of \(y_{t}\) without simulating the forward Markov chain (1), _i.e.,_\(y_{t}|y_{0}\) can be sampled in a _simulation-free_ manner. It also suggests a closed-form score function \(\nabla\log q(y_{t}|y_{0})\). The marginal \(q(y_{t-1}|y_{t},y_{0})\) hints the optimal reverse process given \(y_{0}\).

Generative processThe generative process, which aims to _reverse_ the forward diffusion process (1), is defined by a _backward_ Markov chain with the joint distribution:

\[p_{\theta}(y_{0:T})=p_{T}(y_{T})\prod_{t=1}^{T}p_{\theta}(y_{t-1}|y_{t}),\quad p _{\theta}(y_{t-1}|y_{t}):=\mathcal{N}(y_{t-1};\mu_{\theta}(y_{t},t),\tilde{ \beta}_{t}\boldsymbol{I}).\] (3)

Here, the mean \(\mu_{\theta}\) is typically parameterized by some DNNs, _e.g.,_ U-net [40], with \(\theta\). Equations (1) and (3) imply an evidence lower bound (ELBO) for maximum likelihood training, indicating that the optimal prediction of \(\mu_{\theta}(y_{t},t)\) should match \(\tilde{\mu}_{t}(y_{t},y_{0})\) in Equation (2).

Parameterization and trainingThere exists many different ways to parameterize \(\mu_{\theta}(y_{t},t)\), and each of them leads to a distinct training objective. For instance, Ho et al. [2] found that

\[\mu_{\theta}(y_{t},t):=\frac{1}{\sqrt{1-\beta_{t}}}\Big{(}y_{t}-\frac{\beta_ {t}}{\sqrt{1-\bar{\alpha}_{t}}}\epsilon_{\theta}(y_{t},t)\Big{)},\] (4)

achieves better empirical results compared to directly predicting \(y_{0}\) or \(\tilde{\mu}_{t}\). This parameterization aims to predict the "noise" injected to \(y_{t}\sim q(y_{t}|y_{0})\), as the ELBO reduces to a simple regression:

\[\mathcal{L}(\theta):=\mathbb{E}_{t,y_{0},\epsilon}\left[\lambda(t)\|\epsilon -\epsilon_{\theta}(y_{t},t)\|\right].\] (5)

where \(y_{t}=\sqrt{\bar{\alpha}_{t}}y_{0}+\sqrt{1-\alpha_{t}}\epsilon\), \(\epsilon\sim\mathcal{N}(0,\boldsymbol{I})\), and \(t\) sampled uniformly from \(\{1,\cdots,T\}\). The weighting \(\lambda(t)\) is often set to 1. Once \(\epsilon_{\theta}\) is properly trained, we can generate samples in Euclidean space by simulating Equation (3) backward from \(y_{T}\sim\mathcal{N}(0,\boldsymbol{I})\).

## 3 Mirror Diffusion Models (MDM)

In this section, we present a novel class of diffusion models, Mirror Diffusion Models, which are designed to model probability distributions \(p_{\text{data}}(x)\) supported on a convex constrained set \(\mathcal{M}\subseteq\mathbb{R}^{d}\) while retraining the computational advantages of Euclidean-space diffusion models.

### Dual-Space Diffusion

Mirror mapFollowing the terminology in Li et al. [34], let \(\phi:\mathcal{M}\rightarrow\mathbb{R}\) be a twice differentiable3 function that is strictly convex and satisfying that \(\lim_{x\rightarrow\partial\mathcal{M}}\|\nabla\phi(x)\|\rightarrow\infty\) and \(\nabla\phi(\mathcal{M})=\mathbb{R}^{d}\). We call its gradient map \(\nabla\phi:\mathcal{M}\rightarrow\mathbb{R}^{d}\) the _mirror map_ and its image, \(\nabla\phi(\mathcal{M})=\mathbb{R}^{d}\), as its _dual/mirror space_. Let \(\phi^{*}:\mathbb{R}^{d}\rightarrow\mathbb{R}\) be the dual function of \(\phi\) defined by

Footnote 3: We note that standard mirror maps require twice differentiability to induce a (Riemannian) metric on the mirror space for constructing MLD. For our MDM, however, twice differentiability is needed only for Equation (8), and continuous differentiability, _i.e.,_\(C^{1}\), would suffice for training.

\[\phi^{*}(y)=\sup_{x\in\mathcal{M}}\langle x,y\rangle-\phi(x)\] (6)

A notable property of this dual function \(\phi^{*}(y)\) is that its gradient map _reverses_ the mirror map. In other words, we have \(\nabla\phi^{*}=(\nabla\phi)^{-1}\), implying that

\[\nabla\phi^{*}(\nabla\phi(x))=x\text{ for all }x\in\mathcal{M},\quad\nabla\phi( \nabla\phi^{*}(y))=y\text{ for all }y\in\nabla\phi(\mathcal{M})=\mathbb{R}^{d}.\]

The functions \((\nabla\phi,\nabla\phi^{*})\) define a nonlinear, yet bijective, mapping between the convex constrained set \(\mathcal{M}\) and Euclidean space \(\mathbb{R}^{d}\). As a result, MDM does not require building diffusion models on \(\mathcal{M}\), as done in previous approaches, but rather they learn a standard Euclidean-space diffusion model in the corresponding dual space \(\nabla\phi(\mathcal{M})=\mathbb{R}^{d}\).

Essentially, the goal of MDM is to model the _dual-space distribution_:

\[\tilde{p}_{\text{data}}(y):=([\nabla\phi_{\sharp}]p_{\text{data}})(x),\] (7)

where \(\sharp\) is the push-forward operation and \(y=\nabla\phi(x)\) is the mirror of data \(x\in\mathcal{M}\).

Training and generationMDM follows the same training procedure of Euclidean-space diffusion models, except with an additional push-forward given by Equation (7). Similarly, its generation procedure includes an additional step that maps the dual-space samples \(y_{0}\), generated from Equation (3), back to the constrained set via \(\nabla\phi^{*}(y_{0})\in\mathcal{M}\).

Tractable variational boundGiven a datapoint \(x\in\mathcal{M}\), its mirror \(y_{0}=\nabla\phi(x)\in\mathbb{R}^{d}\), and the dual-space Markov chain \(\{y_{1},\cdots,y_{T}\}\), the ELBO of MDM can be computed by

\[L_{\text{ELBO}}(x):=\log|\det\nabla^{2}\phi^{*}(y_{0})|+\tilde{L}_{\text{ELBO} }(y_{0}),\] (8)

where we apply the change of variables theorem [41] w.r.t. the mapping \(x=\nabla\phi^{*}(y_{0})\). The dual-space ELBO \(\tilde{L}_{\text{ELBO}}(y_{0})\) shares the same formula with the Euclidean-space ELBO [42; 43],

\[\tilde{L}_{\text{ELBO}}(y_{0}):=D_{\text{KL}}(q(y_{T}|y_{0})\;||\;p(y_{T}))+ \sum_{t=1}^{T-1}D_{\text{KL}}(q(y_{t}|y_{t+1},y_{0})\;||\;p_{\theta}(y_{t}|y_{ t+1}))-\log p_{\theta}(y_{0}|y_{1}).\] (9)

It should be noted that (8,9) provide a _tractable_ variational bound for constrained generation. This stands in contrast to concurrent methods relying on reflected diffusions, whose ELBO entails either intractable [18] or approximate [25] scores that could be restricted when dealing with more complex constrained sets. While, in practice, MDM is trained using the regression objective in Equation (5), computing tractable ELBO might be valuable for independent purposes such as likelihood estimation.

### Efficient Computation of Mirror Maps

What remains to be answered pertains to the construction of mirror maps for common constraint sets such as \(\ell_{2}\)-balls, simplices, polytopes, and their products. In particular, we seek efficient, preferably closed-form, computation for \(\nabla\phi\), \(\nabla\phi^{*}\) and \(\nabla^{2}\phi^{*}\).

\(\ell_{2}\)-BallLet \(\mathcal{M}:=\{x\in\mathbb{R}^{d}:\|x\|_{2}^{2}<R\}\) denote the \(\ell_{2}\)-ball of radius \(R\) in \(\mathbb{R}^{d}\). We consider the common log-barrier function:

\[\phi_{\text{ball}}(x):=-\gamma\log(R-\|x\|_{2}^{2}).\] (10)

where \(\gamma\in\mathbb{R}^{+}\). The mirror map, its inverse, and the Hessian can be computed analytically by

\[\nabla\phi_{\text{ball}}(x)=\frac{2\gamma x}{R-\|x\|_{2}^{2}}, \qquad\nabla\phi_{\text{ball}}^{*}(y)=\frac{Ry}{\sqrt{R\|y\|_{2}^{2}+\gamma^ {2}}+\gamma},\] (11) \[\nabla^{2}\phi_{\text{ball}}^{*}(y)=\frac{R}{R\sqrt{\|y\|_{2}^{2} +\gamma^{2}}+\gamma}\left(\bm{I}-\frac{R}{\left(R\sqrt{\|y\|_{2}^{2}+\gamma^{ 2}}+\gamma\right)\sqrt{R\|y\|_{2}^{2}+\gamma^{2}}}yy^{\top}\right).\]

We refer to Appendix A for detailed derivation. Figure 3 visualizes such a mirror map for an \(\ell_{2}\)-ball in 2D. It is worth noting that while the mirror map of log-barriers generally does not admit an analytical inverse, this is not the case for the \(\ell_{2}\)-ball constraints.

SimplexGiven a \((d+1)\)-dimensional simplex, \(\Delta_{d+1}:=\{x\in\mathbb{R}^{d+1}:\sum_{i=1}^{d+1}x_{i}=1,x_{i}\geq 0\}\), we follow standard practices [18, 25] by constructing the constrained set \(\mathcal{M}:=\{x\in\mathbb{R}^{d}:\sum_{i=1}^{d}x_{i}\leq 1,x_{i}\geq 0\}\) and define \(x_{d+1}:=1-\sum_{i=1}^{d}x_{i}\).4

Footnote 4: The transformation allows \(x\) to be bounded in a constrained set \(\mathcal{M}\subseteq\mathbb{R}^{d}\) rather than a hyperplane in \(\mathbb{R}^{d+1}\).

While conventional log-barriers may remain a viable option, a preferred alternative that is widely embraced in the MLD literature for enforcing simplices [28, 44] is the entropic function [45, 46]:

\[\phi_{\text{simplex}}(x):=\sum_{i=1}^{d}x_{i}\log(x_{i})+\left(1-\sum_{i=1}^{ d}x_{i}\right)\log\left(1-\sum_{i=1}^{d}x_{i}\right).\] (12)

The mirror map, its inverse, and the hessian of the dual function can be computed analytically by

\[\begin{split}[\nabla\phi_{\text{simplex}}(x)]_{i}=\log x_{i}- \log\left(1-\sum_{i=1}^{d}x_{i}\right),\qquad[\nabla\phi_{\text{simplex}}^{*}(y)]_{i}=\frac{e^{y_{i}}}{1+\sum_{i =1}^{d}e^{y_{i}}},\\ [\nabla^{2}\phi_{\text{simplex}}^{*}(y)]_{ij}=\frac{e^{y_{i}}}{1+ \sum_{i=1}^{d}e^{y_{i}}}\mathbb{I}(i=j)-\frac{e^{y_{i}}e^{y_{j}}}{\left(1+ \sum_{i=1}^{d}e^{y_{i}}\right)^{2}},\end{split}\] (13)

where \(\mathbb{I}(\cdot)\) is an indicator function. Again, we leave the derivation of Equation (13) to Appendix A. Figure 3 visualizes the entropic mirror map for a 3-dimensional simplex \(\Delta_{3}\).

PolytopeLet the constrained set be \(\mathcal{M}:=\{x\in\mathbb{R}^{d}:c_{i}<a_{i}^{\top}x<b_{i},\forall i\in\{1, \cdots,m\}\}\), where \(c_{i},b_{i}\in\mathbb{R}\) and \(a_{i}\in\mathbb{R}^{d}\) are linearly independent to each other. We consider the standard log-barrier:

\[\phi_{\text{polytope}}(x):=-\sum_{i=1}^{m}\left(\log\left(\langle a_{i},x \rangle-c_{i}\right)+\log\left(b_{i}-\langle a_{i},x\rangle\right)\right)+ \sum_{j=m+1}^{d}\frac{1}{2}\langle a_{j},x\rangle^{2},\] (14)

\[\nabla\phi_{\text{polytope}}(x)=\sum_{i=1}^{m}s_{i}(\langle a_{i},x\rangle)a_ {i}+\sum_{j=m+1}^{d}\langle a_{j},x\rangle a_{j},\] (15)

where the monotonic function \(s_{i}:(c_{i},b_{i})\rightarrow\mathbb{R}\) is given by \(s_{i}=\frac{-1}{\langle a_{i},x\rangle-c_{i}}+\frac{-1}{\langle a_{i},x\rangle -b_{i}}\), and \(\{a_{j}\}\) do not impose any constraints and can be chosen arbitrarily as long as they span \(\mathbb{R}^{d}\) with \(\{a_{i}\}\) (see below for more details); hence \(\nabla\phi_{\text{polytope}}(\mathcal{M})=\mathbb{R}^{d}\). While its inverse, \(\nabla\phi_{\text{polytope}}^{*}\), does not admit closed-form in general, it does when all \(d\) constraints are _orthonormal_:

\[\nabla\phi_{\text{polytope}}^{*}(y)=\sum_{i=1}^{m}s_{i}^{-1}(\langle a_{i},y \rangle)a_{i}+\sum_{j=m+1}^{d}\langle a_{j},y\rangle a_{j}.\] (16)

Here, \(s_{i}^{-1}:\mathbb{R}\rightarrow(c_{i},b_{i})\) is the inverse of \(s_{i}\). Essentially, (15,16) manipulate the coefficients of the bases from which the polytope set is constructed. These manipulations are nonlinear yet bijective, defined uniquely by \(s_{i}\) and \(s_{i}^{-1}\), for each orthonormal basis \(a_{i}\) or, equivalently, for each constraint.

Naively implementing the mirror map via (15,16) can be problematic due to _(i)_ numerical instability of \(s_{i}\) at boundaries (see Figure 4), _(ii)_ the lack of closed-form solution for its inverse \(s_{i}^{-1}\), and _(iii)_ an _one-time_ computation of orthonormal bases, which scales as \(\mathcal{O}(d^{3})\) for orthonormalization methods such as Gram-Schmidt, Householder or Givens [e.g., 47]. Below, we devise a modification for efficient and numerically stable computation, which will be later adopted for watermarked generation.

Improved computation for Polytope (15,16)Given the fact that only the first \(m\) orthonormal bases \(\{a_{1},\cdots,a_{m}\}\) matter in the computation of (15,16), as the remaining \((d-m)\) orthonormal bases merely involve orthonormal projections, we can simplify the computational (15,16) to

\[\nabla\phi_{\text{poly}}(x)=x+\sum_{i=1}^{m}\left(s_{i}(\langle a_{i},x \rangle)-\langle a_{i},x\rangle\right)a_{i},\;\nabla\phi_{\text{poly}}^{*}(y)=y+ \sum_{i=1}^{m}\left(s_{i}^{-1}(\langle a_{i},y\rangle)-\langle a_{i},y \rangle\right)a_{i},\] (17)

Figure 4: Comparison between \(s_{i}\) induced by standard log-barriers _vs._ hyperbolic tangents in Eq. (18).

which, intuitively, add and subtract the coefficients of the first \(m\) orthonormal bases while leaving the rest intact. This reduces the complexity of computing orthonormal bases to \(\mathcal{O}(md^{2})\approx\mathcal{O}(d^{2})\) and improves the numerical accuracy of inner-product multiplication, which can be essential for high-dimensional applications when \(d\) is large.

To address the instability of \(s_{i}\) and intractability of \(s_{i}^{-1}\) for log-barriers, we re-interpret the mirror maps (15,16) as the changes of coefficient bases. As this implies that any nonlinear bijective mapping with a tractable inverse would suffice, we propose a rescaling of the hyperbolic tangent:

\[s_{i}(\left\langle a_{i},x\right\rangle):=\left(\tanh^{-1}\circ\mathrm{rescale }_{i}\right)\left(\left\langle a_{i},x\right\rangle\right),\ \ s_{i}^{-1}(\left\langle a_{i},y\right\rangle):=\left(\mathrm{rescale}_{i}^{ -1}\circ\tanh\right)\left(\left\langle a_{i},y\right\rangle\right),\] (18)

where \(\mathrm{rescale}_{i}(z):=2\frac{z-c_{i}}{(b_{i}-c_{i})}-1\) rescales the range from \((c_{i},b_{i})\) to \((-1,1)\). It is clear from Figure 4 that, compared to the \(s_{i}\) induced by the log-barrier, the mapping (18) is numerical stable at the boundaries and admits tractable inverse.

**Complexity** Table 2 summarizes the complexity of the mirror maps for each constrained set. We note that all computations are parallelizable, inducing nearly no computational overhead.

**Remark 1**.: _Equations (17) and (18) can be generalized to non-orthonormal constraints by adopting \(\left\langle\tilde{a}_{i},x\right\rangle\) and \(\left\langle\tilde{a}_{i},y\right\rangle\), where \(\tilde{a}_{i}\) is the \(i\)-th row of the matrix \(\tilde{\bm{A}}:=(\bm{A}^{\top}\bm{A})^{-1}\bm{A}^{\top}\) and \(\bm{A}:=[a_{1},\cdots,a_{m}]\). Indeed, when \(\bm{A}\) is orthonormal, we recover \(\tilde{a}_{i}=a_{i}\). We leave more discussions to Appendix B._

## 4 Related Work

**Constrained sampling** Sampling from a probability distribution on a constrained set has been a long-standing problem not only due to its extensive applications in, _e.g.,_ topic modeling [48, 49] and Bayesian inference [50, 51], but its unique challenges in non-asymptotic analysis and algorithmic design [28, 32, 33, 34, 52, 53]. Mirror Langevin is a special instance of endowing Langevin dynamics with a Riemannian metric [30, 54], and it chooses a specific reshaped geometry so that dynamics have to travel infinite distance in order to cross the boundary of the constrained set. It is a sampling generalization of the celebrated Mirror Descent Algorithm [45] for convex constrained optimization, and the convexity of constrained set leads to the existence of a mirror map. However, Mirror Langevin is designed to draw samples from an unnormalized density supported on a constrained set. This stands in contrast to our MDM, which instead tackles problems of constrained _generation_, where only samples, not unnormalized density, are available, sharing much similarity to conventional generative modeling [3, 55]. In addition, Mirror Langevin can be understood to go back and forth between primal (constrained) and mirror (unconstrained) spaces, while MDM only does one such round trip.

**Diffusion Models in non-Euclidean spaces** There has been growing interest in developing diffusion generative models that can operate on domains that are not limited to standard Euclidean spaces, _e.g.,_ discrete domains [56, 57] and equality constraints [58]. Seminar works by De Bortoli et al. [14] and Huang et al. [15] have introduced diffusion models on Riemannian manifolds, which have shown promising results in, _e.g.,_ biochemistry [59, 60] and rotational groups [61, 62]. Regarding diffusion models on constrained manifolds, most concurrent works consider similar convex constrained sets such as simplices and polytopes [18, 25, 63]. Our MDM works on the same classes of constraints and includes additionally \(\ell_{2}\)-ball, which is prevalent in social science domains such as opinion dynamics [64, 65]. It is also worth mentioning that our MDM may be conceptually similar to Simplex Diffusion (SD) [63] which reconstructs samples on simplices similar to the mirror mapping. However, SD is designed specifically for simplices and Dirichlet distributions, while MDM is applicable to a broader class of convex constrained sets. Additionally, SD adopts Cox-Ingersoll-Ross processes [66], yielding a framework that, unlike MDM, is not simulation-free.

**Latent Diffusion Models (LDMs)** A key component to our MDM is the construction of mirror map, which allows us to define diffusion models in a different space than the original constrained set, thereby alleviating many computational difficulties. This makes MDM a particular type of LDMs [5, 67, 68], which generally consider structurally advantageous spaces for learning diffusion models. While LDMs typically _learn_ such latent spaces either using pretrained model or on the fly using, _e.g.,_ variational autoencoders, MDM instead derives analytically a latent (dual) space through a mirror map tailored specifically to the constrained set. Similar to LDM [69], the diffusion processes of MDM on the initial constrained set are implicitly defined and can be highly nonlinear.

\begin{table}
\begin{tabular}{c c c} \hline \hline \(\ell_{2}\)**-Ball** & **Simplex** & **Polytope** \\ \hline \(\mathcal{O}(d)\) & \(\mathcal{O}(d)\) & \(\mathcal{O}(md)\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Complexity of \(\nabla\phi\) and \(\nabla\phi^{*}\) for each constrained set.

## 5 Experiment

### Constrained Generation on Balls and Simplices

**Setup** We evaluate the performance of **Mirror Diffusion Model (MDM)** on common constrained generation problems, such as \(\ell_{2}\)-ball and simplex constrained sets with dimensions \(d\) ranging from \(2\) to \(20\). Following Fishman et al. [18], we consider mixtures of Gaussian (Figure 1) and Spiral (Figure 3) for ball constraints and Dirichlet distributions [48] with various concentrations for simplex constraints. We compare MDM against standard unconstrained diffusion models, such as DDPM [2], and their constrained counterparts, such as Reflected Diffusion [18], using the same time-embedded fully-connected network and 1000 sampling time steps. Evaluation metrics include Sliced Wasserstein distance [70] and constraint violation. Other details are left to Appendix C.

**MDM surpasses Reflected Diffusion on all tasks.** Tables 3 and 4 summarize our quantitative results. For all constrained generation tasks, our MDM surpasses Reflected Diffusion by a large margin, with larger performance gaps as the dimension \(d\) increases. Qualitatively, Figure 5 demonstrates how MDM better captures the underlying distribution than Reflected Diffusion. Additionally, Table 5 reports the relative complexity of both methods compared to simulation-free diffusion models such as DDPM. While Reflected Diffusion requires extensive computation due to the use of implicit score matching [71] and assertion of boundary reflection at every propagation step, our MDM constructs efficient, parallelizable, mirror maps in unconstrained dual spaces (see Section 3), thereby enjoying preferable simulation-free complexity and better numerical scalability.

**MDM matches DDPM without violating constraints.** It should be highlighted that, while DDPM remains comparable to MDM in terms of distributional metric, the generated samples often violate the designated constrained sets. As shown in Tables 3 and 4, these constraint violations worsen as the dimension increases. In contrast, MDM is by design violation-free; hence marrying the best of both.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & \(d=2\) & \(d=2\) & \(d=6\) & \(d=8\) & \(d\)=\(20\) \\ \hline Sliced Wasserstein \(\downarrow\) & & & & & \\ DDPM [2] & 0.0704 \(\pm\) 0.0095 & 0.0236 \(\pm\) 0.0048 & **0.0379 \(\pm\) 0.0015** & **0.0231 \(\pm\) 0.0020** & 0.0200 \(\pm\) 0.0034 \\ Reflected [18] & 0.0642 \(\pm\) 0.0181 & 0.0491 \(\pm\) 0.0103 & 0.0609 \(\pm\) 0.0055 & 0.0881 \(\pm\) 0.0010 & 0.0574 \(\pm\) 0.0065 \\ MDM (ours) & **0.0544 \(\pm\) 0.0070** & **0.0214 \(\pm\) 0.0025** & 0.0467 \(\pm\) 0.0096 & 0.0292 \(\pm\) 0.0017 & **0.0159 \(\pm\) 0.0044** \\ \hline Constraint violation (\%) \(\downarrow\) & & & & \\ DDPM [2] & 0.00 \(\pm\) 0.00 & 0.00 \(\pm\) 0.00 & 8.67 \(\pm\) 0.87 & 13.60 \(\pm\) 0.62 & 19.33 \(\pm\) 1.29 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Results of \(\ell_{2}\)**-ball constrained sets** on five synthetic datasets of various dimension \(d\). We report Sliced Wasserstein [70] w.r.t. 1000 samples, averaged over three trials, and include constraint violations for unconstrained diffusion models. Note that Reflected Diffusion [18] and MDM satisfy constraints by design. Our MDM clearly achieves similar or better performance to standard diffusion models while fully respecting the constraints. Other metrics, _e.g.,_\(\mathcal{W}_{1}\), are reported in Appendix C. sc

Figure 5: Comparison between MDM and Reflected Diffusion [18] in modeling a Dirichlet distribution on a 7-dimensional simplex. We visualize the joint densities between \(x_{3:4}\) and \(x_{5:6}\).

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & \(d=3\) & \(d=3\) & \(d=7\) & \(d=9\) & \(d\)=\(20\) \\ \hline Sliced Wasserstein \(\downarrow\) & & & & \\ DDPM [2] & 0.0089 \(\pm\) 0.0002 & **0.0110 \(\pm\) 0.0032** & **0.0047 \(\pm\) 0.0004** & 0.0053 \(\pm\) 0.0003 & 0.0031 \(\pm\) 0.0003 \\ Reflected [18] & 0.0233 \(\pm\) 0.0019 & 0.0336 \(\pm\) 0.0009 & 0.0411 \(\pm\) 0.0039 & 0.0897 \(\pm\) 0.0112 & 0.0231 \(\pm\) 0.0011 \\ MDM (ours) & **0.0074 \(\pm\) 0.0008** & 0.0169 \(\pm\) 0.0016 & 0.0051 \(\pm\) 0.0006 & **0.0040 \(\pm\) 0.0003** & **0.0027 \(\pm\) 0.0000** \\ \hline Constraint violation (\%) \(\downarrow\) & & & & \\ DDPM [2] & 0.73 \(\pm\) 0.12 & 14.40 \(\pm\) 1.39 & 11.63 \(\pm\) 0.90 & 27.53 \(\pm\) 0.57 & 68.83 \(\pm\) 1.66 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Results of **simplices constrained sets** on five synthetic datasets of various dimension \(d\).

\begin{table}
\begin{tabular}{c c c} \hline \hline  & Runtime & Memory \\ \hline MDM & 108\% & 100\% \\ Reflected Diff. [18] & \(>\)1200\% & 905\% \\ \hline \hline \end{tabular}
\end{table}
Table 5: Runtime and memory complexity w.r.t. DDPM [2].

### Watermarked Generation on High-Dimensional Image Datasets

**Setup**  We present MDM as a new approach for watermarked generation, where the watermark is attached to samples within an orthonormal polytope \(\mathcal{M}:=\{x\in\mathbb{R}^{d}:c_{i}<a_{i}^{\top}x<b_{i},\forall i\}\). These parameters \(\{a_{i},b_{i},c_{i}\}_{i=1}^{m}\) serve as the _private tokens_ visible only to individual users (see Figure 2). While our approach is application-agnostic, we demonstrate MDM mainly on image datasets, including both unconditional and conditional generation. Given a designated watermark precision, defined as the percentage of constraint-satisfied images that are generated by MDMs, we randomly construct tokens as \(m\) orthonormalized Gaussian random vectors and instantiate MDMs by either projecting unwatermarked diffusion models onto \(\mathcal{M}\) (**MDM-proj**), or learning the corresponding dual-space diffusions (**MDM-dual**). More precisely, MDM-proj projects samples generated by pretrained diffusion models to a constraint set whose parameters (_i.e.,_ tokens) are visible only to the private user. In contrast, MDM-dual _learns_ a dual-space diffusion model from the constrained-projected samples; hence, like other MDMs in Section 5.1, it is constraint-dependent. Other details are left to Appendix C.

**Unconditional watermark generation on FFHQ and AFHQv2**  We first test out MMD on FFHQ [72] and AFHQv2 [73] on unconditional 64\(\times\)64 image generation. Following Zhao et al. [39], we adopt EDM parameterization [38] and report in Table 6 the performance of MDM in generating watermarked images, quantified by the Frechet Inception Distance (FID) [74]. Compared to prior watermarked approaches, which require additional training of latent spaces [39], **MDM stands as a competitive approach for watermarked generation** by directly constructing analytic dual spaces from the tokens (_i.e.,_ constraints) themselves. Intriguingly, despite the fact that MDM-dual learns a smoother dual-space distribution compared to the truncated one from MDM-proj (see Figure 7), the latter consistently achieves lower (hence better) FIDs. Since samples generated by MDM-dual still remain close to the watermarked distribution, as indicated in Table 6 by the low _FIDs*_ w.r.t. the shifted training statistics, we conjecture that the differences may be due to the significant distributional shift induced by naively projecting the training data onto the constraint set. This is validated, partly, in the ablation study from Figure 8, where we observe that both the FID and watermark precision improve as the number of constraints \(m\) decreases, and as the constraint ranges \((c_{i},b_{i})\) loosen. We highlight this fundamental trade off between watermark protection and generation quality. Examples of watermarked images are shown in Figure 6 and Appendix C.

**Conditional generation on ImageNet256**  Finally, we consider conditional watermarked generation on ImageNet 256\(\times\)256. Specifically, we focus on image restoration tasks, where the goal

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{3}{c}{FFHQ 64\(\times\)64} & \multicolumn{3}{c}{AFHQv2 64\(\times\)64} \\ _Precision_ & _59.3\%_ & _71.8\%_ & _93.3\%_ & _56.9\%_ & _75.0\%_ & _92.7\%_ \\ \hline MDM-proj & 2.54 & 2.59 & 3.08 & 2.10 & 2.12 & 2.30 \\  & 2.96 & 4.57 & 15.74 & 2.23 & 2.86 & 6.79 \\  & 2.65* & 2.93* & 4.40* & 2.21* & 2.32* & _3.05*_ \\ \hline \hline \end{tabular}
\end{table}
Table 6: The 50k-FID for unconditional watermarked generation. _Waternark precision_ denotes the percentage of constraint-satisfied images that are generated by MDMs, controlled by loosing/tightening the constrained sets. As MDM-dual is trained on constraint-projected images, we also report its _FIDs*_ w.r.t. the shifted distributions. The prior watermarked diffusion model [39] reported 5.03 on FFHQ and 4.32 on AFHQv2.

is to generate clean, watermarked, images conditioned on degraded--restored with JPEG in this case--inputs. Similar to unconditional generation, we consider a polytope constraint set whose parameters are chosen such that the watermark yields high precision (\(>\) 95%) and low false positive rate (\(<\) 0.001%). Specifically, we set \(m=100\) and \(b=-c=1.2\). We initialize the networks with pretrained checkpoints from Liu et al. [75].

Figure 9 reports the qualitative results. It is clear that both MDM-dual and MDM-proj are capable of solving this conditional generation task, generating clean images that additionally embed invisible watermarks, _i.e.,_\(x\in\mathcal{M}\). Note that all non-MDM-generated images, despite being indistinguishable, actually violate the polytope constraint, whereas MDM-generated images always satisfy the constraint. Overall, our results suggest that both MDM-dual and MDM-proj scale to high-dimensional applications and are capable of embedding invisible watermarks in high-resolution images.

## 6 Conclusion and Limitation

We developed Mirror Diffusion Models (MDMs), a dual-space diffusion model that enjoys simulation-free training for constrained generation. We showed that MDM outperforms prior methods in standard constrained generation, such as balls and simplices, and offers a competitive alternative in generating watermarked contents. It should be noted that MDM concerns mainly _convex_ constrained sets, which, despite their extensive applications, limits the application of MDM to general constraints. It will be interesting to combine MDM with other dynamic generative models, such as flow-based approaches.

## Broader Impact

Mirror Diffusion Models (MDMs) advance the recent development of diffusion models to complex domains subjected to convex constrained sets. This opens up new possibilities for MDMs to serve as preferred models for generating samples that live in _e.g.,_ simplices and balls. Additionally, MDMs introduce an innovative application of constrained sets as a watermarking technique. This has the potential to address concerns related to unethical usage and safeguard the copyright of generative models. By incorporating constrained sets into the generating process, MDMs offer a means to prevent unauthorized usage and ensure the integrity of generated content.

Figure 9: Conditional watermarked generation on ImageNet 256\(\times\)256. Specifically, we consider the JPEG restoration task, where, given degraded, low-quality inputs \(y\) (_upper-left_), we wish to generate their corresponding clean images \(x\) (_upper-right_) by learning \(p(x|y)\). It is clear that both MDM-dual and MDM-proj are capable of solving this conditional generation task, generating clean images that additionally embed invisible watermarks, _i.e.,_\(x\in\mathcal{M}\).

[MISSING_PAGE_FAIL:11]

* [16] Emile Mathieu and Maximilian Nickel. Riemannian continuous normalizing flows. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.
* [17] Ricky T. Q. Chen and Yaron Lipman. Riemannian flow matching on general geometries. _arXiv preprint arXiv:2302.03660_, 2023.
* [18] Nic Fishman, Leo Klarner, Valentin De Bortoli, Emile Mathieu, and Michael Hutchinson. Diffusion models for constrained domains. _Transactions on Machine Learning Research (TMLR)_, 2023.
* [19] Ben J Morris. Improved bounds for sampling contingency tables. _Random Structures & Algorithms_, 21(2):135-146, 2002.
* [20] Nathan E Lewis, Harish Nagarajan, and Bernhard O Palsson. Constraining the metabolic genotype-phenotype relationship using a phylogeny of in silico methods. _Nature Reviews Microbiology_, 10(4):291-305, 2012.
* [21] Li Han and Lee Rudolph. Inverse kinematics for a serial chain with joints under distance constraints. In _Robotics: Science and Systems (RSS)_, 2006.
* [22] Tsuneo Yoshikawa. Manipulability of robotic mechanisms. _The international journal of Robotics Research_, 4(2):3-9, 1985.
* [23] Ruth J Williams. Reflected brownian motion with skew symmetric data in a polyhedral domain. _Probability Theory and Related Fields_, 75(4):459-485, 1987.
* [24] Andrey Pilipenko. _An introduction to stochastic differential equations with reflection_, volume 1. Universitatsverlag Potsdam, 2014.
* [25] Aaron Lou and Stefano Ermon. Reflected diffusion models. In _International Conference on Machine Learning (ICML)_, 2023.
* [26] Sebastien Bubeck, Ronen Eldan, and Joseph Lehec. Finite-time analysis of projected langevin monte carlo. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2015.
* [27] Kanji Sato, Akiko Takeda, Reiichiro Kawai, and Taiji Suzuki. Convergence error analysis of reflected gradient langevin dynamics for globally optimizing non-convex constrained problems. _arXiv preprint arXiv:2203.10215_, 2022.
* [28] Ya-Ping Hsieh, Ali Kavis, Paul Rolland, and Volkan Cevher. Mirrored langevin dynamics. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2018.
* [29] Dominique Bakry, Ivan Gentil, and Michel Ledoux. _Analysis and geometry of Markov diffusion operators_, volume 103. Springer, 2014.
* [30] Mark Girolami and Ben Calderhead. Riemann manifold langevin and hamiltonian monte carlo methods. _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_, 73(2):123-214, 2011.
* [31] Kelvin Shuangjian Zhang, Gabriel Peyre, Jalal Fadili, and Marcelo Pereyra. Wasserstein control of mirror langevin monte carlo. In _Conference on Learning Theory (COLT)_, 2020.
* [32] Sinho Chewi, Thibaut Le Gouic, Chen Lu, Tyler Maunu, Philippe Rigollet, and Austin Stromme. Exponential ergodicity of mirror-Langevin diffusions. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.
* [33] Kwangjun Ahn and Sinho Chewi. Efficient constrained sampling via the mirror-Langevin algorithm. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [34] Ruilin Li, Molei Tao, Santosh S Vempala, and Andre Wibisono. The mirror langevin algorithm converges with vanishing bias. In _International Conference on Algorithmic Learning Theory (ALT)_, 2022.

* [35] Michael Liebrenz, Roman Schleifer, Anna Buadze, Dinesh Bhugra, and Alexander Smith. Generating scholarly content with chatgpt: ethical challenges for medical publishing. _The Lancet Digital Health_, 5(3):e105-e106, 2023.
* [36] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. A watermark for large language models. In _International Conference on Machine Learning (ICML)_, 2023.
* [37] Jurgen Rudolph, Samson Tan, and Shannon Tan. Chatgpt: Bullshit spewer or the end of traditional assessments in higher education? _Journal of Applied Learning and Teaching_, 6(1), 2023.
* [38] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [39] Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Ngai-Man Cheung, and Min Lin. A recipe for watermarking diffusion models. _arXiv preprint arXiv:2303.10137_, 2023.
* [40] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for biomedical image segmentation. In _International Conference on Medical Image Computing and Computer-assisted Intervention_. Springer, 2015.
* [41] Peter D Lax. Change of variables in multiple integrals. _The American mathematical monthly_, 106(6):497-501, 1999.
* [42] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In _International Conference on Machine Learning_, pages 8162-8171. PMLR, 2021.
* [43] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [44] Sebastien Bubeck and Ronen Eldan. The entropic barrier: a simple and optimal universal self-concordant barrier. In _Conference on Learning Theory (COLT)_, 2015.
* [45] Arkadij Semenovic Nemirovskij and David Borisovich Yudin. _Problem complexity and method efficiency in optimization_. Wiley-Interscience, 1983.
* [46] Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. _Operations Research Letters_, 31(3):167-175, 2003.
* [47] Yousef Saad. _Iterative methods for sparse linear systems_. SIAM, 2003.
* [48] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. _Journal of Machine Learning Research (JMLR)_, 2003.
* [49] Arthur Asuncion, Max Welling, Padhraic Smyth, and Yee Whye Teh. On smoothing and inference for topic models. In _Conference on Uncertainty in Artificial Intelligence (UAI)_, 2009.
* [50] Lixin Lang, Wen-shiang Chen, Bhavik R Bakshi, Prem K Goel, and Sridhar Ungarala. Bayesian estimation via sequential monte carlo sampling--constrained dynamic systems. _Automatica_, 43(9):1615-1622, 2007.
* [51] Michael A Gelbart, Jasper Snoek, and Ryan P Adams. Bayesian optimization with unknown constraints. In _Conference on Uncertainty in Artificial Intelligence (UAI)_, 2014.
* [52] Yann Brenier. Decomposition polaire et rearrangement monotone des champs de vecteurs. _CR Acad. Sci. Paris Ser. I Math._, 305:805-808, 1987.
* [53] Nicolas Brosse, Alain Durmus, Eric Moulines, and Marcelo Pereyra. Sampling from a log-concave distribution with compact support with proximal Langevin Monte Carlo. In _Conference on Learning Theory (COLT)_, 2017.

* [54] Sam Patterson and Yee Whye Teh. Stochastic gradient Riemannian Langevin dynamics on the probability simplex. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2013.
* [55] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. In _International Conference on Learning Representations (ICLR)_, 2023.
* [56] Ricky TQ Chen, Brandon Amos, and Maximilian Nickel. Semi-discrete normalizing flows through differentiable tessellation. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [57] Xingchao Liu, Lemeng Wu, Mao Ye, et al. Learning diffusion bridges on constrained domains. In _International Conference on Learning Representations (ICLR)_, 2022.
* [58] Mao Ye, Lemeng Wu, and Qiang Liu. First hitting diffusion models for generating manifold, graph and categorical data. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [59] Bowen Jing, Gabriele Corso, Jeffrey Chang, Regina Barzilay, and Tommi Jaakkola. Torsional diffusion for molecular conformer generation. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [60] Gabriele Corso, Hannes Stark, Bowen Jing, Regina Barzilay, and Tommi Jaakkola. Diffdock: Diffusion steps, twists, and turns for molecular docking. In _International Conference on Learning Representations (ICLR)_, 2023.
* [61] Adam Leach, Sebastian M Schmon, Matteo T Degiacomi, and Chris G Willcocks. Denoising diffusion probabilistic models on \(SO\)(3) for rotational alignment. In _International Conference on Learning Representations (ICLR), Workshop Track_, 2022.
* [62] Julen Urain, Niklas Funk, Georgia Chalvatzaki, and Jan Peters. \(SE\)(3)-DiffusionFields: Learning cost functions for joint grasp and motion optimization through diffusion. In _IEEE International Conference on Robotics and Automation (ICRA), Workshop Track_, 2022.
* [63] Pierre H Richemond, Sander Dieleman, and Arnaud Doucet. Categorical SDEs with simplex diffusion. _arXiv preprint arXiv:2210.14784_, 2022.
* [64] Marco Caponigro, Anna Chiara Lai, and Benedetto Piccoli. A nonlinear model of opinion formation on the sphere. _Discrete & Continuous Dynamical Systems-A_, 35(9):4241-4268, 2015.
* [65] Jason Gaitonde, Jon Kleinberg, and Eva Tardos. Polarization in geometric opinion dynamics. In _ACM Conference on Economics and Computation_, pages 499-519, 2021.
* [66] John C Cox, Jonathan E Ingersoll Jr, and Stephen A Ross. A theory of the term structure of interest rates. In _Theory of valuation_, pages 129-164. World Scientific, 2005.
* [67] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [68] Kushagra Pandey, Avideep Mukherjee, Piyush Rai, and Abhishek Kumar. Diffusevae: Efficient, controllable and high-fidelity generation from low-dimensional latents. _Transactions on Machine Learning Research (TMLR)_, 2022.
* [69] Dongjun Kim, Byeonghu Na, Se Jung Kwon, Dongsoo Lee, Wanmo Kang, and Il-Chul Moon. Maximum likelihood training of implicit nonlinear diffusion models. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [70] Nicolas Bonneel, Julien Rabin, Gabriel Peyre, and Hanspeter Pfister. Sliced and radon wasserstein barycenters of measures. _Journal of Mathematical Imaging and Vision_, 51:22-45, 2015.
* [71] Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach to density and score estimation. In _Conference on Uncertainty in Artificial Intelligence (UAI)_, 2020.

* Karras et al. [2019] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2019.
* Choi et al. [2020] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for multiple domains. In _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2020.
* Heusel et al. [2017] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local nash equilibrium. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2017.
* Liu et al. [2023] Guan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos A Theodorou, Weili Nie, and Anima Anandkumar. I\({}^{2}\)SB: Image-to-Image Schrodinger bridge. In _International Conference on Machine Learning (ICML)_, 2023.
* Paszke et al. [2019] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In _Advances in Neural Information Processing Systems 32_, pages 8024-8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf.
* Loshchilov and Hutter [2019] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _International Conference on Learning Representations (ICLR)_, 2019.
* Elfwing et al. [2018] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. _Neural Networks_, 107:3-11, 2018.
* Wu and He [2018] Yuxin Wu and Kaiming He. Group normalization. In _Proceedings of the European conference on computer vision (ECCV)_, pages 3-19, 2018.

Derivation of Mirror Mappings

Here, we provide addition derivation of \(\nabla\phi^{*}\). Computation of \(\nabla\phi(x)\) and \(\nabla^{2}\phi^{*}(y)\) follow straightforwardly by differentiating \(\phi(x)\) and \(\nabla\phi^{*}(y)\) w.r.t. \(x\) and \(y\), respectively.

\(\ell_{2}\)-BallSince the gradient map also reverses the mirror map, we aim to rewrite \(y=\frac{2\gamma}{R-\|x\|_{2}^{2}}x\) as \(x=f(y)=\nabla\phi_{\text{ball}}^{*}(y)\). Solving the second-order polynomial,

\[\|y\|_{2}^{2}=\left(\frac{2\gamma}{R-\|x\|_{2}^{2}}\right)^{2}\|x \|_{2}^{2},\] (19)

yields

\[\|x\|_{2}^{2}=R+\frac{2\gamma}{\|y\|_{2}^{2}}\left(\gamma-\sqrt{R \|y\|_{2}^{2}+\gamma^{2}}\right).\] (20)

With that, we can rewrite Equation (11) by

\[x=\frac{R-\|x\|_{2}^{2}}{2\gamma}\,y\overset{\eqref{eq:m2}}{=} \frac{\sqrt{R\|y\|_{2}^{2}+\gamma^{2}}-\gamma}{\|y\|_{2}^{2}}y=\frac{R}{\sqrt{ R\|y\|_{2}^{2}+\gamma^{2}}+\gamma}y.\]

SimplexStandard calculations in convex analysis [28] shows

\[\phi_{\text{simplex}}^{*}(y)=\log\left(1+\sum_{i}^{d}e^{y_{i}} \right).\] (21)

Differentiating Equation (21) w.r.t. \(y\) yields \(\nabla\phi_{\text{simplex}}^{*}\) in Equation (13).

PolytopeSince the gradient map also reverses the mirror map, we aim to inverse

\[y=\sum_{i=1}^{m}s_{i}(\langle a_{i},x\rangle)a_{i}+\sum_{j=m+1}^{ d}\langle a_{j},x\rangle a_{j}.\] (22)

When all \(d\) constraints are orthonormal, taking inner product between \(y\) and each \(a\) yields

\[\langle a_{i},y\rangle=s_{i}(\langle a_{i},x\rangle),\qquad\langle a _{j},y\rangle=\langle a_{j},x\rangle.\] (23)

Therefore, we can reconstruct \(x\) from \(y\) via

\[x =\sum_{i=1}^{m}\langle a_{i},x\rangle a_{i}+\sum_{j=m+1}^{d} \langle a_{j},x\rangle a_{j}\] \[\overset{\eqref{eq:m2}}{=}\sum_{i=1}^{m}s_{i}^{-1}(\langle a_{i},y\rangle)a_{i}+\sum_{j=m+1}^{d}\langle a_{j},y\rangle a_{j},\]

which defines \(x=\nabla\phi_{\text{polytope}}^{*}(y)\). For completeness, the Hessian can be presented compactly as

\[\nabla^{2}\phi_{\text{polytope}}^{*}(y)=\bm{I}+\bm{A}\Sigma\bm{A} ^{\top},\] (24)

where \(\bm{I}\) is the identity matrix, \(\bm{A}:=[a_{1},\cdots,a_{m}]\) is a \(d\)-by-\(m\) matrix whose column vector \(a_{i}\) corresponds to each constraint, and \(\Sigma\in\mathbb{R}^{m\times m}\) is a diagonal matrix with leading entries

\[[\Sigma]_{ii}=\frac{\partial s_{i}^{-1}(z)}{\partial z}|_{z=\langle a_{i},y \rangle}-1\overset{\eqref{eq:m2}}{=}\frac{b_{i}-c_{i}}{2}\left(1-\tanh^{2}( \langle a_{i},y\rangle)\right)-1.\]Additional Remarks on Polytope

Derivation of Equation (17)Since the subspaces spanned by \(\{a_{i}\}\) and \(\{a_{j}\}\) are orthogonal to each other, we can rewrite (15) as

\[\nabla\phi_{\text{polytope}}(x)=\sum_{i=1}^{m}s_{i}(\langle a_{i},x\rangle)a_{i} +\left(x-\sum_{i=1}^{m}\langle a_{i},x\rangle a_{i}\right)=x+\sum_{i=1}^{m} \left(s_{i}(\langle a_{i},x\rangle)-\langle a_{i},x\rangle\right)a_{i}.\]

\(\nabla\phi_{\text{polytope}}^{*}(y)\) follows similar derivation.

Generalization to non-orthonormal constraintsThe mirror maps of a polytope, as described in Equations (15) to (17), can be seen as operations that manipulate the coefficients associated with the bases defined by the constraints. This understanding allows us to extend the computation to non-orthonormal constraints by identifying the corresponding "coefficients" through a change of bases, utilizing the reproducing formula:

\[x=\sum_{i=1}^{d}\langle\tilde{a}_{i},x\rangle a_{i},\text{ where }\tilde{a}_{i}\text{ is the $i$-th row of }(\bm{A}^{\top}\bm{A})^{-1}\bm{A}^{\top},\]

and \(\bm{A}:=[a_{1},\cdots,a_{m}]\). Similarly, we have \(y=\sum_{i=1}^{d}\langle\tilde{a}_{i},y\rangle a_{i}\). Applying similar derivation leads to

\[\nabla\phi_{\text{poly}}(x)=x+\sum_{i=1}^{m}\left(s_{i}(\langle\tilde{a}_{i}, x\rangle)-\langle\tilde{a}_{i},x\rangle\right)a_{i},\;\nabla\phi_{\text{poly}}^{*}(y)=y+ \sum_{i=1}^{m}\left(s_{i}^{-1}(\langle\tilde{a}_{i},y\rangle)-\langle\tilde{a }_{i},y\rangle\right)a_{i}.\]

## Appendix C Experiment Details & Additional Results

Dataset & constrained sets
* _\(\ell_{2}\)-balls constrained sets_: For \(d=2\), we consider the Gaussian Mixture Model (with variance \(0.05\)) and the Spiral shown respectively in Figures 1 and 3. For \(d=\{6,8,20\}\), we place \(d\) isotropic Gaussians, each with variance \(0.05\), at the corner of each dimension, and reject samples outside the constrained sets.
* _Simplices constrained sets_: We consider Dirichlet distributions [48], \(\operatorname{Dir}(\alpha)\), with various concentration parameters \(\alpha\) detailed in Table 7.
* _Hypercube constrained sets_: For all dimensions \(d=\{2,3,6,8,20\}\), we place \(d\) isotropic Gaussians, each with variance \(0.2\), at the corner of each dimension, and either reject (\(d=\{2,3,6,8\}\)) or reflect (\(d=20\)) samples outside the constrained sets.
* _Watermarked datasets and polytope constrained sets_: We follow the same data preprocessing from EDM5[38] and rescale both FFHQ and AFHQv2 to 64\(\times\)64 image resolution. For the polytope constrained sets \(\mathcal{M}:=\{x\in\mathbb{R}^{d}:c_{i}<a_{i}^{\top}x<b_{i},\forall i\}\), we construct \(a_{i}\) from orthonormalized Gaussian random vectors and detail other hyperparameters in Table 8.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline \(d\) & \(3\) & \(3\) & \(7\) & \(9\) & \(20\) \\ \hline \hline \(\alpha\) & \([2,4,8]\) & \([1,0.1,5]\) & \([1,2,2,4,4,8,8]\) & \([1,0.5,2,0.3,0.6,4,8,8,2]\) & \([0.2,0.4,\cdots,4,4.2]\) \\ \hline \hline \end{tabular}
\end{table}
Table 7: The concentration parameter \(\alpha\) of each Dirichlet distribution in simplices constrained sets.

ImplementationAll methods are implemented in PyTorch [76]. We adopt ADM6 and EDM5[38] respectively as the MDM's diffusion backbones for constrained and watermarked generation. We implemented Reflected Diffusion [18] by ourselves as their codes have not yet been made available by the submission time (May 2023), and used the official implementation7 of Reflected Diffusion [25] in Table 11. We also implemented Simplex Diffusion [63], but as observed in previous works [25], it encountered computational instability especially when computing the modified Bessel functions.

Footnote 6: https://github.com/openai/guided-diffusion, released under MIT License.

Footnote 7: https://github.com/louaaron/Reflected-Diffusion, latest commit (65d05c6) at submission, unlicensed.

TrainingFor constrained generation, all methods are trained with AdamW [77] and an exponential moving average with the decay rate of 0.99. As standard practices, we decay the learning rate by the decay rate 0.99 every 1000 steps. For watermarked generation, we follow the default hyperparameters from EDM5[38]. All experiments are conducted on two TITAN RTXs and one RTX 2080.

Footnote 8: https://github.com/jeanfeydy/geomloss, released under MIT License.

NetworkFor constrained generation, all networks take \((y,t)\) as inputs and follow

\[\texttt{out}=\texttt{out\_mod}(\texttt{norm}(\texttt{y\_mod}(\texttt{y\_})+ \texttt{t\_mod}(\texttt{timestep\_embedding}(\texttt{t\_})))),\]

where timestep_embedding(\(\cdot\)) is the standard sinusoidal embedding. t_mod and out_mod consist of 2 fully-connected layers (Linear) activated by the Sigmoid Linear Unit (SiLU) [78]:

\[\texttt{t\_mod}=\texttt{out\_mod}=\texttt{Linear}\rightarrow\texttt{SiLU} \rightarrow\texttt{Linear}\]

and y_mod consists of 3 residual blocks, _i.e.,_y_mod(\(y\)) = \(y+\texttt{res\_mod}(\texttt{norm}(y))\), where

\[\texttt{res\_mod}=\texttt{Linear}\rightarrow\texttt{SiLU}\rightarrow\texttt{ Linear}\rightarrow\texttt{SiLU}\rightarrow\texttt{Linear}\rightarrow\texttt{SiLU} \rightarrow\texttt{Linear}\]

All Linear's have 128 hidden dimension. We use group normalization [79] for all norm. For watermarked generation, we use EDM parameterization5[38].

Footnote 5: https://github.com/jeanfeydy/geomloss, released under MIT License.

EvaluationWe compute the Wasserstein and Sliced Wasserstein distances using the geomloss8 and ot9 packages, respectively. The Maximum Mean Discrepancy (MMD) is based on the popular package https://github.com/ZongxianLee/MMD_Loss.Pytorch, which is unlicensed. For watermarked generation, we follow the same evaluation pipeline from EDM6[38] by first generating 50,000 watermarked samples and computing the FID w.r.t. the training statistics.

Footnote 8: https://python.github.io/gen_modules/ot.sliced.html#ot.sliced.sliced_wasserstein_distance, released under MIT License.

False-positive rateSimilar to Kirchenbauer et al. [36], we reject the null hypothesis and detect the watermark if the sample produces no violation of the polytope constraint, _i.e.,_ if \(x\in\mathcal{M}\). Hence, the false-positive samples are those that are actually true null hypothesis (_i.e.,_ not generated by MDM) yet accidentally fall into the constraint set, hence being mistakenly detected as watermarked. Specifically, the false-positive rates of our MDMs are respectively 0.07% and 0.08% for FFHQ and AFHQv2. Lastly, we note that the fact that both MDM-proj and MDM-dual generate samples that always satisfy the constraint readily implies 100% recall and 0% Type II error.

### Additional Results

Tractable variational bound in Equation (8)Figure 10 demonstrates how MDM faithfully captures the variational bound to the negative log-likelihood (NLL) of 2-dimensional GMM.

More constrained sets, distributional metrics, & baselineTables 9 and 10 expand the analysis in Tables 3 and 4 with additional distributional metrics such as Wasserstein-1 (\(\mathcal{W}_{1}\)) and Maximum Mean Discrepancy (MMD). Additionally, Table 11 reports the results of hypercube \([0,1]^{d}\) constrained set, a special instance of polytopes, and includes additional baseline from Lou and Ermon [25], which approximate the

Figure 10: Tractable variational bound by our MDM.

intractable scores in Reflected Diffusion using eigenfunctions tailored specifically to hypercubes, rather than implicit score matching as in Fishman et al. [18]. Consistently, our findings conclude that the MDM is the _only_ constrained-based diffusion model that achieves comparable or better performance to DDPM. These results affirm the effectiveness of MDM in generating high-quality samples within constrained settings, making it a reliable choice for constrained generative modeling.

More watermarked samplesFigures 11 and 12 provide additional qualitative results on the watermarked samples generated by MDMs.

Figure 11: FFHQ 64\(\times\)64 unconditional watermarked samples generated by **(left)** MDM-proj and **(right)** MDM-dual from the same set of random seeds. Despite the fact that some images, such as the one in the first row and sixth column, were altered possibly due to the change of dual-space distribution (see Figure 7), they look realistic and remain close to the data distribution.

Figure 12: AFHQv2 64\(\times\)64 unconditional watermarked samples generated by **(left)** MDM-proj and **(right)** MDM-dual from the same set of random seeds. Despite the fact that some images, such as the one in the fifth row and first column, were altered possibly due to the change of dual-space distribution (see Figure 7), they all look realistic and remain close to the data distribution.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & \(d=2\) & \(d=2\) & \(d=6\) & \(d=8\) & \(d{=}20\) \\ \hline \(\mathcal{W}_{1}\downarrow\) (unit: \(10^{-2}\)) & & & & \\ \hline DDPM [2] & \(\mathbf{0.01}\pm 0.00\) & \(\mathbf{0.02}\pm 0.01\) & \(\mathbf{0.03}\pm 0.00\) & \(\mathbf{0.05}\pm 0.00\) & \(\mathbf{0.11}\pm 0.00\) \\ Reflected [18] & \(0.06\pm 0.01\) & \(0.12\pm 0.00\) & \(0.62\pm 0.08\) & \(3.57\pm 0.05\) & \(0.98\pm 0.02\) \\ MDM (ours) & \(\mathbf{0.01}\pm 0.00\) & \(\mathbf{0.01}\pm 0.01\) & \(\mathbf{0.03}\pm 0.00\) & \(\mathbf{0.05}\pm 0.00\) & \(0.13\pm 0.00\) \\ \hline \multicolumn{5}{l}{MMD \(\downarrow\) (unit: \(10^{-2}\))} & & & \\ \hline DDPM [2] & \(0.72\pm 0.007\) & \(0.72\pm 0.30\) & \(0.74\pm 0.10\) & \(0.97\pm 0.22\) & \(1.12\pm 0.07\) \\ Reflected [18] & \(3.91\pm 0.95\) & \(15.12\pm 1.36\) & \(16.48\pm 1.04\) & \(131.44\pm 2.65\) & \(57.90\pm 2.07\) \\ MDM (ours) & \(\mathbf{0.44}\pm 0.16\) & \(\mathbf{0.50}\pm 0.26\) & \(\mathbf{0.42}\pm 0.08\) & \(\mathbf{0.55}\pm 0.13\) & \(\mathbf{0.61}\pm 0.03\) \\ \hline \multicolumn{5}{l}{Constraint violation (\%) \(\downarrow\)} & & & \\ \hline DDPM [2] & \(0.73\pm 0.12\) & \(14.40\pm 1.39\) & \(11.63\pm 0.90\) & \(27.53\pm 0.57\) & \(68.83\pm 1.66\) \\ \hline \hline \end{tabular}
\end{table}
Table 10: Expanded results of simplices constrained sets.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & \(d=3\) & \(d=3\) & \(d=7\) & \(d=9\) & \(d{=}20\) \\ \hline \(\mathcal{W}_{1}\downarrow\) (unit: \(10^{-2}\)) & & & & \\ \hline DDPM [2] & \(\mathbf{0.01}\pm 0.00\) & \(\mathbf{0.02}\pm 0.01\) & \(\mathbf{0.03}\pm 0.00\) & \(\mathbf{0.05}\pm 0.00\) & \(\mathbf{0.11}\pm 0.00\) \\ Reflected [18] & \(0.06\pm 0.01\) & \(0.12\pm 0.00\) & \(0.62\pm 0.08\) & \(3.57\pm 0.05\) & \(0.98\pm 0.02\) \\ MDM (ours) & \(\mathbf{0.01}\pm 0.00\) & \(\mathbf{0.01}\pm 0.01\) & \(\mathbf{0.03}\pm 0.00\) & \(\mathbf{0.05}\pm 0.00\) & \(0.13\pm 0.00\) \\ \hline \multicolumn{5}{l}{MMD \(\downarrow\) (unit: \(10^{-2}\))} & & & \\ \hline DDPM [2] & \(0.72\pm 0.007\) & \(0.72\pm 0.30\) & \(0.74\pm 0.10\) & \(0.97\pm 0.22\) & \(1.12\pm 0.07\) \\ Reflected [18] & \(3.91\pm 0.95\) & \(15.12\pm 1.36\) & \(16.48\pm 1.04\) & \(131.44\pm 2.65\) & \(57.90\pm 2.07\) \\ MDM (ours) & \(\mathbf{0.44}\pm 0.16\) & \(\mathbf{0.50}\pm 0.26\) & \(\mathbf{0.42}\pm 0.08\) & \(\mathbf{0.55}\pm 0.13\) & \(\mathbf{0.61}\pm 0.03\) \\ \hline \multicolumn{5}{l}{Constraint violation (\%) \(\downarrow\)} & & & \\ \hline DDPM [2] & \(0.73\pm 0.12\) & \(14.40\pm 1.39\) & \(11.63\pm 0.90\) & \(27.53\pm 0.57\) & \(68.83\pm 1.66\) \\ \hline \hline \end{tabular}
\end{table}
Table 11: Results of **hypercube \([0,1]^{d}\) constrained sets**.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & \(d=2\) & \(d=2\) & \(d=6\) & \(d=8\) & \(d{=}20\) \\ \hline \(\mathcal{W}_{1}\downarrow\) (unit: \(10^{-2}\)) & & & & \\ \hline DDPM [2] & \(0.66\pm 0.15\) & \(0.14\pm 0.03\) & \(\mathbf{0.52}\pm 0.09\) & \(\mathbf{0.58}\pm 0.10\) & \(3.45\pm 0.50\) \\ Reflected [18] & \(0.55\pm 0.29\) & \(0.46\pm 0.17\) & \(3.11\pm 0.40\) & \(10.13\pm 0.21\) & \(19.42\pm 0.13\) \\ MDM (ours) & \(\mathbf{0.46}\pm 0.07\) & \(\mathbf{0.12}\pm 0.04\) & \(0.72\pm 0.39\) & \(1.05\pm 0.26\) & \(\mathbf{2.63}\pm 0.31\) \\ \hline \multicolumn{5}{l}{MMD \(\downarrow\) (unit: \(10^{-2}\))} & & & \\ \hline DDPM [2] & \(0.67\pm 0.23\) & \(\mathbf{0.23}\pm 0.07\) & \(\mathbf{0.37}\pm 0.19\) & \(0.75\pm 0.24\) & \(0.98\pm 0.42\) \\ Reflected [18] & \(0.58\pm 0.46\) & \(5.03\pm 1.17\) & \(2.34\pm 0.14\) & \(28.82\pm 0.66\) & \(14.83\pm 0.62\) \\ MDM (ours) & \(\mathbf{0.52}\pm 0.36\) & \(0.27\pm 0.19\) & \(0.54\pm 0.12\) & \(\mathbf{0.35}\pm 0.23\) & \(\mathbf{0.50}\pm 0.17\) \\ \hline \multicolumn{5}{l}{Constraint violation (\%) \(\downarrow\)} & & \\ \hline DDPM [2] & \(0.00\pm 0.00\) & \(0.00\pm 0.00\) & \(8.67\pm 0.87\) & \(13.60\pm 0.62\) & \(19.33\pm 1.29\) \\ \hline \hline \end{tabular}
\end{table}
Table 9: Expanded results of \(\ell_{2}\)**-ball constrained sets**, where we include additional distributional metrics such as \(\mathcal{W}_{1}\) and Maximum Mean Discrepancy (MMD), all computed with 1000 samples and averaged over three trials. Consistently, our findings conclude that the MDM is the _only_ constrained-based diffusion model that achieves comparable or better performance to DDPM.