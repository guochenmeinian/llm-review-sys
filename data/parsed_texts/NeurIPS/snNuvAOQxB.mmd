# MEQA: A Benchmark for Multi-hop Event-centric Question Answering with Explanations

 Ruosen Li, Zimu Wang, Son Quoc Tran, Lei Xia, Xinya Du

Department of Computer Science, University of Texas at Dallas

{ruosen.li, zimu.wang, lei.xia, xinya.du}@utdallas.edu

###### Abstract

Existing benchmarks for multi-hop question answering (QA) primarily evaluate models based on their ability to reason about entities and the relationships between them. However, there's a lack of insight into how these models perform in terms of _both events and entities_. In this paper, we introduce a novel semi-automatic question generation strategy by composing event structures from information extraction (IE) datasets and present the first Multi-hop Event-centric Question Answering (MEQA) benchmark1. It contains (1) 2,243 challenging questions that require a diverse range of complex reasoning over entity-entity, entity-event, and event-event relations; (2) corresponding multi-step QA-format event reasoning chain (explanation) which leads to the answer for each question. We also introduce two metrics for evaluating explanations: completeness and logical consistency. We conduct comprehensive benchmarking and analysis, which shows that MEQA is challenging for the latest state-of-the-art models encompassing large language models (LLMs); and how they fall short of providing faithful explanations of the event-centric reasoning process.

Footnote 1: Our benchmark is publicly available at https://github.com/du-nlp-lab/MEQA.

## 1 Introduction

Multi-hop Question Answering (QA) is an important task that challenges NLP models' ability, including large language models (LLMs), to perform multi-step reasoning to answer the given question based on pieces of information (and their relationships) from the context (Yang et al., 2018; Mavi et al., 2022). The ability to conduct multiple reasoning steps is important because it empowers models to understand and perform complicated real-world tasks that require information aggregation in/across documents. Examples include sentence fusion (Brook Weiss et al., 2022), multi-document summarization (Haghighi and Vanderwende, 2009), timeline summarization (Yan et al., 2011), and event logical/temporal reasoning (Yang et al., 2020, 2023, 2024a, 2024b).

However, current research predominantly focuses on entity-centric questions, leaving event structure and event-event relations underrepresented in semantic sources and corresponding QA datasets (Souza Costa et al., 2020). For instance, Figure 1 illustrates an event-centric question encompassing both entities and events, a domain often overlooked by popular QA benchmarks (Ho et al., 2020; Trivedi et al., 2022). Event-centric questions are inherently more complex than entity-centric ones, as discussed in the subsequent section. They demand a compositional understanding of both entity and event knowledge, presenting significant challenges for NLP models, including LLMs, and serving as a robust benchmark for evaluating reasoning capabilities (Souza Costa et al., 2020).

Motivated by this gap in the QA and machine reading comprehension literature, we present the first Multi-hop Event-centric Question Answering (MEQA) dataset. It includes 2,243 questions, requiringa diverse range of reasoning capabilities to answer, for example, event relations, entity bridging, and event listing and counting. In order to bootstrap the annotation process for collecting multi-hop questions, we propose re-purposing the existing information extraction (IE) dataset (i.e., WikiEvents (Li et al., 2021)), which includes annotated event structures (event triggers and entities). Specifically in our work, we design a novel question generation strategy: firstly, examine document-level event structures, followed by linking events into event reasoning chains in the given document, and finally, generate synthetic questions and QA-pair style explanations. Human annotators later curate the synthetic questions and explanations.

The key contributions of our work can be summarized as follows: (1) we collect the first challenging multi-hop event-centric question answering dataset with explanations (MEQA): Our empirical findings underscore the uniqueness of our benchmark that presents novel challenges and a wide range of diversity. Notably, our results reveal a substantial gap between the performance of state-of-the-art language models and human performance, which provides a promising avenue for future research; (2) we introduce a bottom-up process that partially automates the dataset construction process by identifying composable events from the IE dataset; (3) we introduce the completeness and logical consistency metrics to evaluate generated explanations, which are efficient and align well with human judgments; (4) we propose methods that leverage informative structured information (e.g., entity and event) for our new task, which significantly improves performance and generates a more faithful reasoning process.

## 2 Related Work

**Multi-hop QA.** Previous multi-hop QA benchmarks all focus on entity-relation understanding (Das et al., 2019; Saxena et al., 2020; Fang et al., 2020). HotpotQA (Yang et al., 2018) is constructed with a top-down approach by directly crowdsourcing multi-hop questions, which is later shown to be solvable using single-hop shortcuts (Chen and Durrett, 2019). On the other hand, 2WikiMultihopQA (Ho et al., 2020) and Musique (Trivedi et al., 2022) are constructed using the bottom-up approach, where multi-hop questions are composed of single-hop reasoning steps. While they all rely on initially human-written questions, MEA-QG (Pan et al., 2021) generates multi-hop questions by first selecting relevant information from different data sources with a set of operators and then integrating the multiple information to form a question with six reasoning graphs. Our MEQA is constructed with a novel bottom-up approach to bootstrap question generation (QG), as we use the labels from event extraction datasets for identifying composable single-hop steps.

**Explainable Complex QA.** Datasets for QA that feature intricate reasoning questions frequently include comprehensive explanations. These explanations serve the dual purpose of allowing QA systems to learn with stronger supervision and facilitating evaluation of models' ability to explain their predictions (Li and Du, 2023; Du, 2024; Li et al., 2024). HotpotQA (Yang et al., 2018) only includes evidence sentences from the passages. StrategyQA (Geva et al., 2021) includes question decomposition results, but they do not serve as explanations. ScienceQA (Lu et al., 2022) includes free-form explanations for scientific questions. Our MEQA focuses on the most natural free-form explanations and includes single-hop QA pairs as the reasoning path (explanations) that leads to the final answers.

**Event-centric QA.** Simultaneous with the research progress in event understanding, i.e., event extraction (EE) and event relation extraction (ERE) (Du and Cardie, 2020; Du and Ji, 2022; Wang et al., 2022, 2024; Mehta et al., 2022; Peng et al., 2023; Jin et al., 2023; Choudhary and Du, 2024), multiple event-centric QA datasets have been proposed with different characteristics. EventQA

Figure 1: An example of multi-hop event-centric question in MEQA. Models should start reasoning from the _AI-Monitor_ and first locate the _reported_ event; then find all events that happened before the reported event; and finally extract victims in all those events, which are answers to the question.

[Souza Costa et al., 2020], utilizes a random walk on the EventKG [Gottschalk and Demidova, 2019], is designed for accessing semantic data stored within KGs; however, it focuses on KG and is not multi-hop. TORQUE [Ning et al., 2020] includes questions querying commonsense temporal relationships based on the MATRES dataset [Ning et al., 2018]. ESTER [Han et al., 2021] focuses on the challenges of five semantic relations between events. Our MEQA dataset differs from above and it includes event-centric questions that require multi-hop reasoning, namely, temporal/causal relation identification, event trigger-argument entity, and argument-argument understanding.

## 3 Complex Event-centric Questions

### Question Strategies

Following the definition in ACE (Automatic Content Extraction) [Walker et al., 2006] of entities, relations, and events, we define our event-centric questions in Appendix A. Table 1 provides a breakdown of question types within our dataset and the corresponding proportions. To increase diversity, we follow five strategies to annotate multi-hop questions, with examples in Appendix B:

**Event Relation.** In this type of question, language models are tested on the ability of determining the relation between different events, including (1) event-event relation: encompassing causal relations

\begin{table}
\begin{tabular}{p{42.7pt} p{113.8pt} p{113.8pt} p{113.8pt} p{113.8pt}} \hline \hline
**Type** & **Context** & **Question** & **Answer** & **Explanation** \\ \hline _Event_ & [...] & _a major general_ was _killed_ [...] _AI-Monitor_ (49.7\%) & Who died before _AI-Monitor_ reported it online? & major general, _Monitor_ is the contactor? _reported_ \\  & _der_ [...] was also reportedly _killed_ [...] & & & 2. What events in \#1 consider, _killed_ iteutenant_ general & \\ \hline _Entity_ & [...] in _Belfast_, at least _1 people_ died. [...] Early today Mr Whitelaw came back to _Belfast_ by _plane_ [...] & Which transportation method did & Plane & 1. Where did _11 people_ die in explosion? _Belfast_ method did & 1. Where did _11 people_ die in explosion? _Belfast_ method did Whitelaw use to reach the _#1? _Plane_ died? & 1. Where did _11 people_ die in explosion? _Belfast_ method did Whitelaw use to reach the _#1? _Plane_ \\ \hline _Event_ & Roadside IED _kills_ Russian _major general_ in Syria. _and_ _Three military personnel_ were _wounded_, [...] _A local commander_ [...] was also reportedly _killed_ [...] & How many victims are mentioned in the whole text? & 5 & 1. What events contain victims? _kills_, _wounded_, _killed_ 2. How many victims are in \#1? **5** \\ \hline _Event_ & Roadside _IED_ kills Russian _major general_ in Syria. _Three military personnel_ were wounded, [...] In April, _two dozen Syrian fighters_ were killed in _IS_ attacks [...] & According to the document, which one, _IED_ or _IS_, killed more victims? & IS & 1. Who was killed by _IED_? _major general_, _three military personnel_ 2. Who was killed by _IS_? _two dozen Syrian fighters_ 3. Which one between \#1 and \#2 killed more victims? _IS_ \\ \hline _Unanswerable_ (2.0\%) & Early today Mr Whitelaw [...] 21 bombs had gone off [...] Mr Whitelaw and Lord Carrington immediately flew back [...] & Who traveled to the place where 21 bombs were manufactured? & (No Answer) & - \\ \hline \hline \end{tabular}
\end{table}
Table 1: Types of complex event-based questions in our MEQA dataset. We highlight the answer and its supporting facts in _bold texts_, event triggers in _blue_, bridging entities in _green_. In the _Event comparison_ type, _blue_ and _green_ texts only represent related content for two compared entities.

(e.g., cause) and temporal relations (e.g., before). (2) event-entity relation: relations between events and entities are represented by the role of entities. Figure 2(a) is an example of the strategy.

**Entity Bridging.** This type of multi-hop question tests the connected reasoning ability (Trivedi et al., 2022). The event reasoning chains connect or bridge events from different parts of the documents by entities. They only contain one type of relation: event-entity relation. Events are connected by bridging entities but not direct event-event relations. Figure 2(b) displays an example of the strategy.

**Event Listing and Counting.** This type of question tests the discrete reasoning skills of language models (Dua et al., 2019). In the third example in Table 1, models should first follow similar steps in Strategy 1, including finding all related events and extracting candidate entities, and then calculating the total amount of victims.

**Event Comparison.** This type of question also tests the discrete reasoning skills of language models (Dua et al., 2019). For example, in the fourth sample in Table 1, language models should count the five people killed by the IED and the 24 people killed by IS, and then make the correct comparison that the number of victims by IS (24) is greater than the number of victims by the IED (5).

**Unanswerable Questions.** Questions of this type are those that cannot be answered based only on the corresponding documents. In this question type, we provide annotators with a list of already annotated answerable questions and encourage them to write unanswerable questions that closely resemble the answerable ones.

### Multi-hop Event Reasoning Desiderata

**Multi-hop Reasoning.** Multi-hop event reasoning questions require language models to comprehend the connections between various events within the provided text. Each question entails a multi-step reasoning process, which can be broken down into a sequence of simpler inquiries. A straightforward question can be resolved by (1) referencing a brief passage within the document or (2) employing logical operations based on information gathered from prior steps.

For instance, consider the explanation of the "Event Comparison" strategy outlined in Table 1. The initial two steps can be addressed by referring to concise text excerpts within the document. However, to answer the final question, language models must apply logical operations to the information previously acquired.

**Reasoning Shortcuts** pose a significant challenge to the quality of multi-hop reasoning datasets (Min et al., 2019; Trivedi et al., 2022). Suppose a question is built based on two events \(e_{1}\) and \(e_{2}\), in which \(e_{1}\) starts the entire reasoning chain and \(e_{2}\) contains the final answer. If \(e_{2}\) can be directly and uniquely located in the context by a combination of its trigger and a set of relations, \(e_{1}\) is no longer required. For example, the transported event in Figure 2(b) can be located in the document if we

Figure 2: Partial graph for the Strategy 1 (a) and Strategy 2 (b) examples. The middle part of (b) displays the synthetic sentences and explanations. The bottom part of (b) shows the finalized natural language question and explanation.

assume "plane" only appears once. The second stage of our data collection pipeline, Section 4.2, is specifically designed to eliminate the potential reasoning shortcuts in our dataset.

## 4 Data Collection Pipeline

As illustrated in Figure 3, the construction of MEQA involves four main steps: (1) identifying and linking composable events from IE datasets to event reasoning chains; (2) removing disqualified and merging redundant reasoning chains; (3) converting reasoning chains to synthetic questions and explanations; and (4) human curations.

### Composing Reasoning Chains from Event Structures

We first leverage the event structures from the WikiEvents (Li et al., 2021) (Appendix C) dataset to find composable events (e.g., events that share an entity or have relations in between) by different strategies. The schema of WikiEvent (KAIROS, see Appendix D) describes different types of events (e.g., Travel) and their corresponding argument roles (e.g., transporter and origin). For example, "[transporter] traveled from [origin] to [destination]".

The first step is to filter composable events and link them together to form event reasoning chains, which are event reasoning chains in the QA task. For **Strategy 1**, we randomly select a pair of events, regard one of them as the start event and the other one as the end event. Then, we prompt LLM to generate a temporal or causal relationship between them (Appendix G). For **Strategy 2**, we randomly pick one event as the start event. Then, we search for another event, sharing at least one common entity with the start event (e.g., the exploded event in Figure 2(b)), and concatenate them to form a reasoning chain. Currently, this is a two-hop reasoning chain, and the second event is the end event (e.g., the transported event in Figure 2(b)). For more hops reasoning chains, we repeatedly add more composable events to the end of the reasoning chain. The maximum hop is 4 in this strategy. For the rest of the strategies, we ask annotators to select events manually before writing down questions.

### Relieving Reasoning Shortcut Problem

As described in Section 3.2, uniquely located events lead to reasoning shortcut problems. The primary approach to address this involves ensuring that all event structures, except for the start event, cannot be uniquely identified solely by their triggers and chosen argument roles.

**Automatic Process.** For Strategy 1, during the event sampling process, we record a warning in the data if an end event is unique so annotators can notice the problem and modify it during the annotation process. For event structures used to form an event reasoning chain in Strategy 2, we only keep arguments that connect two events and the corresponding roles and leave out the others. If this minimum event structure can still be uniquely located in all events, we disregard the event and select another one for the current reasoning chain. We keep the reasoning chains containing at least two hops. For other strategies, annotators mitigate the problem by mimicking the above process.

**Manual Correction.** For Strategy 1, annotators remove arguments in the end event to ensure it is not unique in all events (it does not indicate the answer is not unique). The reasoning chain should be discarded if it cannot meet the requirement. If the start event is not unique, annotators will add more

Figure 3: MEQA construction pipeline includes identifying composable events from any IE dataset, composing reasoning chains, mitigating short-cut problems, merging similar reasoning chains to reduce redundant and incomplete questions and, generating synthetic multi-hop questions and QA-based explanations. MEQA pipeline also employs human curation to finalize the high-quality dataset.

information from the document to the event structure to ensure it can be uniquely located. It is the same for start events in all strategies.

### Merging Redundant Reasoning Chains

When constructing event reasoning chains, many reasoning chains share most of the same event structures except the end event, which includes the answer of the corresponding chain. We merge them together to reduce redundant reasoning chains and combine answers to the same chains. For Strategy 1, each reasoning chain only contains two events. If two event chains have the same start event and event-event relation, we can merge the two chains, as shown in Figure 3. Suppose we have two reasoning chains: the wounded event happened before the reported event, and so did the killed event. As reported is the start event in both reasoning chains, we can merge them.

### Generating Synthetic Question Answer Pairs and Explanations

Our goal is to generate multi-hop synthetic questions and explanations based on event reasoning chains. Firstly, we utilize (1) the descriptions of events and argument roles in the KAIROS schema; and (2) arguments from the reasoning chains in Section 4.3 to create synthetic sentences describing the events. Subsequently, we modify these synthetic sentences by substituting the answer argument with an appropriate wh-word, facilitating the generation of synthetic sub-questions. Lastly, we generate the final multi-hop question by composing the sub-questions. The sub-questions naturally consist of the step-by-step explanations. The two blocks in the middle of Figure 2(b) demonstrate the process of generating synthetic question answer pairs & explanations from event reasoning chains.

### Human Curations

These synthetic question answer pairs and explanations in the previous section may not be fluent and grammatically correct. Annotators are required to rephrase both of them in their own speaking style. They can edit anything or rewrite the whole question and all explanations. Moreover, since reasoning chains are sampled from all events in the documents of the dataset, automatically generated single answer (ending node of the reasoning chain) may not be complete. We ask annotators to find all answer spans after they finalize questions and explanations. Figure 2(b) includes the human curation process for turning synthetic questions into natural language questions. The word transported has been rephrased to traveled, which sounds more natural. Details of crowd-sourcing and payment are shown in Appendix E.

## 5 Dataset Analysis

We collect 211 documents from WikiEvents (Li et al., 2021) to create our MEQA dataset and select five workers to annotate the dataset according to the five strategies illustrated in Table 1. After the annotation procedures, the dataset is further split into training, development, and test sets with a proportion of 80%:10%:10%. A summary of the general dataset statistics is shown in Table 7. We analyze our question type distributions and data quality in this section, and some additional analysis are shown in Appendix F.

### Question Type Distribution

The distribution across the five strategies is organized in Table 1. From the table, it is evident that the first two strategies event-bridging and entity-bridging questions account for the largest percentage with a proportion of 49.7% and 37.1%, due to the rich event trigger and argument annotations in WikiEvents. For the rest of the question types, the number of comparison questions is smaller due to the requirement of numbers mentioned in event arguments affiliated with the same events.

### Data Difficulty Evaluation

We design three prompting-based methods to test on datasets. Table 2 illustrates that our MEQA dataset is the most challenging compared to HotpotQA and 2WikiMultihop.

Employing ChatGPT as the foundational method, where each input contains only a context and a question and the output is only an answer (Appendix G), reveals that MEQA presents the greatest challenge. Additionally, our exploration of the effectiveness of the Chain-of-Thought (Wei et al., 2022) style prompt, denoted as "CoT-QA", in which a list of question answer pairs represents reasoning chains before answers. For "CoT-QA (+Entity)" and "CoT-QA (+Event Triggers)", extracted entities and lists of event triggers are leveraged in the context in the prompt correspondingly (Li and Du, 2023). Results are consistent across all three models.

We obtain human performance from 3 annotators (graduate students) on 100 samples randomly chosen from the test split. The accuracy is 88% on average, the upper bound of human performance is 92%, and human agreement (Cohen, 1960; Trivedi et al., 2022) is 79%.

Table 3 presents examples comparing between HotpotQA and MEQA. The upper example is from the entity-centric benchmark HotpotQA. Its reasoning type is "entity bridging". The reasoning starts from "MVP", hops by "Buddy Hield", and ends with the answer "Sacramento Kings". To answer the question in the bottom example, models should start reasoning from the Al-Monitor and first locate the reported event; then find all events that happened before the reported event; and finally extract victims in all those events, which are answers to the question.

## 6 Experimental Results

### Evaluation Metrics

In all experiments, we evaluate both answers and explanations. We compare generated answers with golden answers and report precision, recall, and F1-score. Specifically, we follow the evaluation script from HotpotQA. To evaluate the hallucination problem of explanations, we introduce novel new metrics including "completeness" and "logical consistency" (Huang et al., 2023).

**Completeness** refers to the step-wise accuracy of the matches between the golden explanations and the predicted explanations. We report precision, recall, and F1-score. Predicted explanations can

\begin{table}
\begin{tabular}{l c c c} \hline  & Precision & Recall & F1 \\ \hline
**ChatGPT (GPT-3.5-turbo-1106)** & & & \\ HotpotQA & 0.745 & 0.779 & 0.733 \\
2WikiMultihop & 0.501 & 0.724 & 0.532 \\ MEQA & 0.190 & 0.536 & 0.238 \\ \hline
**ChatGPT CoT-QA (+ Entity)** & & & \\ HotpotQA & 0.777 & 0.813 & 0.763 \\
2WikiMultihop & 0.534 & 0.757 & 0.565 \\ MEQA & 0.364 & 0.394 & 0.350 \\ \hline
**ChatGPT CoT-QA (+ Event Triggers)** & & & \\ MEQA & 0.321 & 0.377 & 0.312 \\ \hline
**Human** & & & \\ MEQA & 0.783 & 0.836 & 0.811 \\ \hline \end{tabular}
\end{table}
Table 2: Performance on different methods over HotpotQA, 2WikiMultihopQA, and MEQA.

\begin{table}
\begin{tabular}{l|l} \hline \hline \multirow{3}{*}{Paragraphs} & Paragraph A: The 2015 Diamond Head Classic was a college basketball tournament... \\  & **Buddy Hield** _was named the tournament’s MVP_. \\  & Paragraph B: **Chavano Rainier ”Buddy” Hield** is a Bahamian professional basketball \\  & player for the _Sacramento Kings_ of the NBA... \\ \hline Question & Which team does the player named 2015 Diamond Head Classic’s MVP play for? \\ \hline Answer & Sacramento Kings \\ \hline \hline \multirow{3}{*}{Document} & [...] nation’s Defense Ministry confirmed that a _major general_ was **killed** in Syria by an \\  & improvised explosive device, _Al-Monitor_ online reported. [...] In 2017, a _lieutenant_ \\  & _general_ was **killed** in the same province, [...] \\ \hline Question & Who **died** before _Al-Monitor_ reported online? \\ \hline Answer & major general, lieutenant general \\ \hline \end{tabular}
\end{table}
Table 3: HotpotQA entity-centric example (top) and MEQA event-centric example (bottom).

have multiple formats, such as QA-format or freeform format. We can always split them into smaller sub-steps, such as one question-answer pair or sentence.

For each question, golden reasoning explanations consist of lists of QA pairs obtained via the method in Section 4.1. We design an algorithm to count the number of matches between predicted and golden explanations. The core idea is that we compare explanations step-wise and follow the order of the golden explanation steps. If one golden step is unmatched, it won't be matched later. Predicted explanations are single-hop questions in which sub-step may contain one or two golden steps. The pseudo-code and prompt for calculating the metric is illustrated in Appendix H.

**Logical Consistency** is a reference-free metric. It measures whether each sub-step is consistent with the previous steps or the original question (Golovneva et al., 2023; Huang et al., 2023). If there is no logical contradiction for a sub-step, we count it as logically consistent. Otherwise, we count it as inconsistent. More specially, we input the history and the current step to LLM to determine whether they have any logical contradiction.

### Experimental Settings

We benchmark various methods based on prompting and fine-tuning. In all experiments, we utilize the few-shot method to include demonstrations as input. In the following experiments, we denote context as **C**, question as **Q**, QA-format explanation as **E**, freeform explanation as **FE**, and answer as **A**. We use \(\rightarrow\) to connect the instruction/input contents and output formats. All experiments using LLM are performed using ChatGPT (GPT-3.5-turbo-1106). Detailed prompts are in Appendix G.

We evaluate the performance of a variety of methods on our MEQA dataset. For prompting-based method, we include the following variations: (1) base fewshot (C+Q\(\rightarrow\)A); (2) CoT-QA (C+Q\(\rightarrow\)E+A); and (3) CoT-Freeform (C+Q\(\rightarrow\)FE+A). The "CoT-Freeform" refers to the traditional CoT method in Wei et al. (2022) in which freeform sentences compose the reasoning chains. We also design fine-tuning based methods using T5 (Raffel et al., 2023) as the base model.

In addition, we explore the impact of incorporating _golden_ structured information (such as event graphs) on this event-centric multi-hop question answering task. This investigation is motivated by two key factors: (1) golden structured information represents the upper bound of additional information, and (2) the study conducted by Li et al. (2023) has demonstrated its effectiveness. More specifically, The "Entity" structure exclusively consists of lists of entities. In contrast, the "Entity KG" structure encompasses extracted entity-based knowledge graphs presented in a triple format derived from contexts. The "Event Triggers" structure solely encompasses triggers from corresponding events. Meanwhile, the "Event Triggers + Arguments" structure encompasses all trigger-argument pairs, along with trigger-trigger pairs if temporal or causality relations exist between two events. Finally, the "Full Event KG" structure represents a comprehensive iteration of the aforementioned structure. It incorporates roles between trigger-argument pairs and relations between events, thus providing a more exhaustive representation. We add the variations of the four baselines as different groups.

### Experimental Results

The main results are in Table 4. More experiments using Claude and Llama3 are in Appendix I. As discussed in Li and Du (2023), recall is a better metric for evaluating LLMs' performance. The "CoT-Freeform" method has the highest recall value among baselines of each group, but it does not significantly differ from the "Fewshot" method. The performance of "CoT-QA" is the worst because of the challenge of generating the sequence of QA pair-based explanations which lead to the final answer: (1) QA-based explanations contain fruitful intermediate information and are mandatory outputs; (2) they are harder to generate: all intermediate questions should have logical connections and be faithful to the context. We can also observe that the precision of the "CoT-QA" method is the best. One main reason is that the format of QA-based explanation successfully guided ChatGPT to output short answers. Moreover, the discussion of potential leakages of the dataset is in Appendix J.

Table 5 presents the performance of different question types in the Full Event KG setting using GPT-3.5. "Entity Bridging" achieves the highest overall performance, likely because it is the simplest question type and similar questions are commonly encountered by LLMs. In contrast, "Event Relation" has the lowest recall, as it involves more complex reasoning process.

**Explanations from CoT-QA are more faithful than CoT-Freeform.** Comparing completeness metrics, "CoT-QA" shows higher recall but lower precision, suggesting it produces more explanations matching golden explanations but also generates redundant QA pairs, potentially leading to hallucination issues. In contrast, "CoT-Freeform" explanations, while shorter, are less related to golden explanations, indicating fidelity issues and potential hallucination. Notably, "CoT-QA w/ Full Event KG" outperforms all models in recall and F1 score, implying better alignment with golden explanations. Incorporating comprehensive structured data notably enhances both completeness and faithfulness.

Logical consistency results of the "CoT-QA" and "CoT-Freeform" are not comparable. The reason that "CoT-Freeform" has such a high consistency score is its length. Since ChatGPT generated a shorter reasoning chain without multi-hop explanation, our evaluation script always counts explanations as consistent if the results are correct. Also, even if the result was incorrect, generated explanations always repeated partial original questions. These explanations were always counted as consistent.

**Rich structured information improves performance.** We set several levels of structured information as additional information added in inputs. According to the richness of structured information, we rank additional structures as following: Entity \(\approx\) Event Triggers < Entity KG < Event Triggers + Arguments < Full Event KG.

The performance trend in Table 4 correlates with the richness of additional event-related structured information. Models tend to perform better when they are provided with richer information. Across all groups (grey rows), the performance with "Full Event KG" is always better than others, not only because of its graph structure information but also its explicit relations between all elements. An example is shown in Figure 4, in which the explanations are not applicable unless the complete event KG is supplied. The context and further analysis of additional information is in Appendix K.

**Human Correlation Studies.** We sample 100 questions from "CoT-QA" (Full Event KG) and ask annotators to follow the definition of completeness and logical consistency to judge the quality of generated explanations. Annotators report the number of matched explanation steps, and then calculate the correlations between automatically generated scores and human evaluations.

The correlation of the "completeness" and "logical consistency" metrics are 0.693 and 0.601, respectively, Both correlations exceed 0.35, a threshold deemed moderate according to Taylor (1990). They satisfy the requirement of the most recent LLM reasoning evaluation work by Huang et al. (2023). Thus, our metrics are usable to evaluate explanations for this specific aspect.

\begin{table}
\begin{tabular}{l|c c c|c c|c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{2}{c|}{General Performance} & \multicolumn{2}{c|}{Completeness} & \multicolumn{1}{c}{Logical} \\  & Precision & Recall & F1 & Precision & Recall & F1 & Consistency \\ \hline
**T5** (**C4Q\(\rightarrow\)A**) & 0.3012* & 0.2761 & 0.2831 & - & - & - & - \\ _w/ Entity KG_ & 0.3187 & 0.2813 & 0.2942 & - & - & - & - \\ \hline Fewshot (**C+Q\(\rightarrow\)A**) & 0.1902 & 0.5360 & 0.2377 & - & - & - & - \\ _w/ Full Event KG_ & 0.4541 & 0.6355 & 0.4581 & - & - & - & - \\ \hline CoT-QA (**C+Q\(\rightarrow\)E+A**) & 0.2832 & 0.3903 & 0.2940* & 0.1963 & 0.2141* & 0.2001 & 0.6442 \\ _w/ Entity KG_ & 0.3636 & 0.3943 & 0.3500 & 0.2052 & 0.2321 & 0.2145 & 0.6161 \\ _w/ Entity KG_ & 0.3522 & 0.3913 & 0.3344 & 0.1935 & 0.2118 & 0.1978 & 0.6318 \\ _w/ Event Triggers_ & 0.3210 & 0.3773 & 0.3120 & 0.2792 & 0.2946 & 0.2835 & 0.6693 \\ _w/ Event Triggers + Arguments_ & 0.4910 & 0.4878 & 0.4471 & 0.3431 & 0.3698 & 0.3481 & 0.6553 \\ _w/ Full Event KG_ & **0.5299** & 0.5298 & **0.4940** & 0.3989 & **0.4653** & **0.4208** & 0.7327 \\ \hline CoT-Freeform (**C+Q\(\rightarrow\)FE+A**) & 0.1044 & 0.5392* & 0.1494 & 0.3368* & 0.1678 & 0.2161* & **0.9132*** \\ _w/ Full Event KG_ & 0.3680 & **0.6575** & 0.3823 & **0.4566** & 0.2506 & 0.3145 & 0.8889 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Performance on all experiments. Four baselines and their further experiments are grouped in the table. In each group, the first line is the performance of the baseline. All the following lines in a group indicate additional contents that are appended after context **C**. **Bold numbers** shows the best results in each column. Numbers with (*) indicate they are the best among all baselines.

\begin{table}
\begin{tabular}{l|c c c} \hline \hline
**GPT-3.5-turbo-1106** & \multicolumn{2}{c}{**Performance**} \\ CoT (Full Event KG) & Precision & Recall & F1 \\ \hline Event Relation & 0.4740 & 0.4492 & 0.4265 \\ Entity Bridging & 0.5539 & 0.5404 & 0.5094 \\ Event Listing and Counting & 0.3895 & 0.5024 & 0.4049 \\ Event Comparison & 0.3682 & 0.4622 & 0.3963 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Performance of question types on GPT-3.5.

We also provide a comprehensive error analysis over randomly selected 50 samples. There are two major types of errors: _Incorrect Start Event Identification_ and _Incorrect Event Relation Identification_. Details are in Appendix L.

### Comparing CoT-QA and CoT-Freeform

In our experiment, we encounter scenarios where both CoT-QA and CoT-Freeform methods exhibit effective performance in the process of answer generation. As shown in Table 6, CoT-QA excels in providing detailed, specific answers by breaking down queries into smaller, structured components; however, it may not always align closely with the main focus of the question. CoT-Freeform offers a broader context and tends to address queries more holistically, but it can sometimes lack the specific details in explanations required by the question. These insights suggest that while both approaches have their unique strengths, a tailored application based on the specific needs of the query might yield the best results.

## 7 Conclusion, Limitation, and Societial Impacts

**Conclusion.** We introduce MEQA, the first benchmark containing multi-hop questions requiring reasoning about both entities and events, as well as explanations. It is annotated based on our new QA generation strategy -- utilizing document-level event structures to bootstrap natural questions in an efficient way. We demonstrate the potential and quality of this new dataset through a detailed analysis of its contents. We conduct experiments and show that the benchmark is challenging for a variety of state-of-the-art models, especially that they are prone to incompleteness and inconsistency issues when generating the reasoning explanations.

**Limitation.** Until now, MEQA only covers English documents which limits the advancement of event-centric multi-hop QA in multilingual scenarios. In the future, we plan to extend MEQA to evaluate the performance of language models in more languages.

**Societial Impacts.** Our benchmark could facilitate the development of more intelligent multi-hop event-centric QA models, which has substantial impacts in a variety of applications, e.g., news understanding, emergency response, etc.

\begin{table}
\begin{tabular}{l} \hline \hline
**Question:_Which object was mentioned in a discussion before given to Dzhokhar?_** \\
**Answer:**_Pistol_ \\ \hline
**CoT-QA Explanation:** \\
1. What event contains Dzhokhar as the recipient of an object? borrowing \\
2. What event before \#1 involves the object? discuss \\
3. What was the object in \#2? Pistol \\
**Answer:**_Pistol_ \\ \hline
**CoT-Freeform Explanation:** \\ Before giving the object to Dzhokhar, the object was discussed among [...] \\
**Answer:**_Pistol_ \\ \hline \hline \end{tabular}
\end{table}
Table 6: An example of comparing two CoT methods. Context is in Table 13.

Figure 4: An example of the significance of additional structured information.

## References

* Weiss et al. (2022) Daniela Brook Weiss, Paul Roit, Ori Ernst, and Ido Dagan. Extending multi-text sentence fusion resources via pyramid annotations. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors, _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 1854-1860, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.135. URL https://aclanthology.org/2022.naacl-main.135.
* Chen and Durrett (2019) Jifan Chen and Greg Durrett. Understanding dataset design choices for multi-hop reasoning. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 4026-4032, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1405. URL https://aclanthology.org/N19-1405.
* Choudhary and Du (2024) Milind Choudhary and Xinya Du. QEVENT: Event extraction as question-answer pairs generation. In Yvette Graham and Matthew Purver, editors, _Findings of the Association for Computational Linguistics: EACL 2024_, pages 1860-1873, St. Julian's, Malta, March 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.findings-eaacl.126.
* 46, 1960. URL https://api.semanticscholar.org/CorpusID:15926286.
* Das et al. (2019) Rajarshi Das, Ameya Godbole, Dilip Kavarthapu, Zhiyu Gong, Abhishek Singhal, Mo Yu, Xiaoxiao Guo, Tian Gao, Hamed Zamani, Manzil Zaheer, and Andrew McCallum. Multi-step entity-centric information retrieval for multi-hop question answering. In Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen, editors, _Proceedings of the 2nd Workshop on Machine Reading for Question Answering_, pages 113-118, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-5816. URL https://aclanthology.org/D19-5816.
* Du (2020) Xinya Du. Making natural language reasoning explainable and faithful. _Proceedings of the AAAI Conference on Artificial Intelligence_, 38(20):22664-22664, Mar. 2024. doi: 10.1609/aaai.v38i20.30280. URL https://ojs.aaai.org/index.php/AAAI/article/view/30280.
* Du and Cardie (2020) Xinya Du and Claire Cardie. Event extraction by answering (almost) natural questions. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 671-683, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.49. URL https://aclanthology.org/2020.emnlp-main.49.
* Du and Ji (2022) Xinya Du and Heng Ji. Retrieval-augmented generative question answering for event argument extraction. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 4649-4666, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.307. URL https://aclanthology.org/2022.emnlp-main.307.
* Dua et al. (2019) Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 2368-2378, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1246. URL https://aclanthology.org/N19-1246.
* Fang et al. (2020) Yuwei Fang, Siqi Sun, Zhe Gan, Rohit Pillai, Shuohang Wang, and Jingjing Liu. Hierarchical graph network for multi-hop question answering. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 8823-8838, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.710. URL https://aclanthology.org/2020.emnlp-main.710.
* Fu et al. (2019)M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, and J. Berant (2021)Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics9, pp. 346-361. External Links: Document, Link, https://aclanthology.org/2021.tacl-1.21 Cited by: SS1.
* O. Golovneva, M. P. Chen, S. Poff, M. Corredor, L. Zettlemoyer, M. Fazel-Zarandi, and A. Celikyilmaz (2023)ROSCOE: a suite of metrics for scoring step-by-step reasoning. In The Eleventh International Conference on Learning Representations, External Links: Link, https://openreview.net/forum?id=xY1JRPzItsY Cited by: SS1.
* the hub of event knowledge on the web
- and biographical timeline generation. Semantic Web10 (6), pp. 1039-1070. External Links: Document, Link Cited by: SS1.
* A. Haghighi and L. Vanderwende (2009)Exploring content models for multi-document summarization. In Proceedings of the 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Boulder, Colorado, USA, pp. 362-370. External Links: Document, Link Cited by: SS1.
* R. Han, I. Hsu, J. Sun, J. Baylon, Q. Ning, D. Roth, and N. Peng (2021)ESTER: a machine reading comprehension dataset for reasoning about event semantic relations. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Online and Punta Cana, Dominican Republic, pp. 7543-7559. External Links: Document, Link Cited by: SS1.
* X. Ho, A. Duong Nguyen, S. Sugawara, and A. Aizawa (2020)Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th International Conference on Computational Linguistics, Barcelona, Spain, pp. 6609-6625. External Links: Document, Link Cited by: SS1.
* L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang, Q. Chen, W. Peng, X. Feng, B. Qin, and T. Liu (2023)A survey on hallucination in large language models: principles, taxonomy, challenges, and open questions. External Links: 2303.03036 Cited by: SS1.
* X. Jin, H. Wen, X. Du, and H. Ji (2020)Toward consistent and informative event-event temporal relation extraction. In Proceedings of the First Workshop on Matching From Unstructured and Structured Data (MATCHING 2023), Toronto, ON, Canada, pp. 23-32. External Links: Document, Link Cited by: SS1.
* B. Li, G. Fang, Y. Yang, Q. Wang, W. Ye, W. Zhao, and S. Zhang (2023)Evaluating chatgpt's information extraction capabilities: an assessment of performance, explainability, calibration, and faithfulness. External Links: 2303.03036 Cited by: SS1.
* R. Li and X. Du (2023)Leveraging structured information for explainable multi-hop question answering and reasoning. In Proceedings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 2023b, pp. 6779-6789. External Links: Document, Link Cited by: SS1.
* R. Li and X. Du (2023)Leveraging structured information for explainable multi-hop question answering and reasoning. In Proceedings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 2023a, pp. 6779-6789. External Links: Document, Link Cited by: SS1.
* R. Li and X. Du (2023)Leveraging structured information for explainable multi-hop question answering and reasoning. In Proceedings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 2023b, pp. 6779-6789. External Links: Document, Link Cited by: SS1.
* R. Li and X. Du (2023)Leveraging structured information for explainable multi-hop question answering and reasoning. In Proceedings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 2023b, pp. 6779-6789. External Links: Document, Link Cited by: SS1.
* R. Li and X. Du (2023)Leveraging structured information for explainable multi-hop question answering and reasoning. In Proceedings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 2023a, pp. 6779-6789. External Links: Document, Link Cited by: SS1.
* R. Li and X. Du (2023)Leveraging structured information for explainable multi-hop question answering and reasoning. In Proceedings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 2023b, pp. 6779-6789. External Links: Document, Link Cited by: SS1.
* R. Li and X. Du (2023)Leveraging structured information for explainable multi-hop question answering and reasoning. In Proceedings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 2023b, pp. 6779-6789. External Links: Document, Link Cited by: SS1.
* R. Li and X. Du (2023)Leveraging structured information for explainable multi-hop question answering and reasoning. In Proceedings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 2023a, pp. 6779-6789. External Links: Document, Link Cited by: SS1.
* R. Li and X. Du (2023)Leveraging structured information for explainable multi-hop question answering and reasoning. In Proceedings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 2023b, pp. 6779-6789. External Links: Document, Link Cited by: SS1.
* R. Li and X. Du (2023)Leveraging structured information for explainable multi-hop question answering and reasoning. In Proceedings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 2023b, pp. 6779-6789. External Links: Document, Link Cited by: SS1.
* R. Li and X. Du (2023)Leveraging structured information for explainable multi-hop question answering and reasoning. In Proceedings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 2023a, pp. 6779-6789. External Links: Document, Link Cited by: SS1.
* R. Li and X. Du (2023)Leveraging structured information for explainable multi-hop question answering and reasoning. In Proceedings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 2023b, pp. 6779-6789. External Links: Document, Link Cited by: SS1.
* R. Li and X. Du (2023)Leveraging structured information for explainable multi-hop question answering and reasoning. In Proceedings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 2023b, pp. 6779-6789. External Links: Document, Link Cited by: SS1.
* R. Li and X. Du (2023)Leveraging structured information for explainable multi-hop question answering and reasoning. In Proceedings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 2023b, pp. 6779-6789. External Links: Document, Link Cited by: SS1.
* R. Li and X. Du (2023)Leveraging structured information for explainable multi-hop question answering and reasoning. In Proceedings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 2023a, pp. 6779-6789. External Links: Document, Link Cited by: SS1.
* R. Li and X. Du (2023)Leveraging structured information for explainable multi-hop question answering and reasoning. In Proceedings of the Association for Computational Linguistics: EMNLP 2023, Singapore, pp. 6779-6789. External Links: Document, Link Cited by: SS1.
* R. Li and X. Du (2023)Leveraging structured information for explainable multi-hop question answering and reasoning. In Proceedings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 2023b, pp. 6779-6789. External Links: Document, Link Cited by: SS1.
* R. Li and X. Du (2023)Leveraging structured information for explainable multi-hop question answering and reasoning. In Proceedings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 2023b, pp. 6779-6789. External Links: Document, Link Cited by: SS1.
* R. Li and X. Du (2023)Leveraging structured information for explainable multi-hop question answering and reasoning. In Proceedings of the Association for Computational Linguistics: EMNLP 2023, Singapore, pp. 6779-6789. External Links: Document, Link Cited by: SS1.
* R. Li and X. Du (2023)Leveraging structured information for explainable multi-hop question answering and reasoning. In Proceedings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 2023a, pp. 6779-6789. External Links: Document, Link Cited by: SS1.
* R. Li and X. Du (2023)Leveraging structured information for explainable multi-hop question answering and reasoning. In Proceedings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 2023b, pp. 6779-6789. External Links: Document, Link Cited by: SS1.
* R. Li and X. Du (2023)Leveraging structured information for explainable multi-hop question answering and reasoning. In Proceedings of the Association for Computational Linguistics: EMNLP 2023, Singapore, pp. 6779-6789. External Links: Document, Link Cited by: SS1.
* R. Li and X. Du (2023)Leveraging structured information for explainable multi-hop question answering and reasoning. In Proceedings of the Association for Computational Linguistics: EMNLP 2023, Singapore, pp. 6779-6789. External Links: Document, Link Cited by: SS1.
* R. Li and X. Du (2023)Leveraging structured information for explainable multi-hop question answering and reasoning. In Proceedings of the Association for Computational Linguistics: EMNLP 2023, Singapore, pp. 6779-6789. External Links: Document, Link Cited by: SS1.
* R. Li and X. Du (2023)Leveraging structured information for explainable multi-hop question answering and reasoning. In Proceedings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 2023b, pp. 6779-6789. External Links: Document, Link Cited by: SS1.
* R. Li and X. Du (2023)Leveraging structured information for explainable multi-hop question answering and reasoning. In Proceedings of the Association for Computational Linguistics: EMNLP 2023, Singapore, pp. 6779-6789. External Links: Document, Link Cited by: SS1.
* R. Li and X. Du (2023)Leveraging structured information for explainable multi-hop question answering and reasoning. In Proceedings of the Association for Computational Linguistics: EMNLP 2023, Singapore, pp. 6779-6789. External Links: Document, Link Cited by: SS1.
* R. Li and X. Du (2023)Leveraging structured information for explainable multi-hop question answering and reasoning. In Proceedings of the Association for Computational Linguistics: EMNLP 2023, Singapore, pp. 6779-6789. External Links: Document, Link Cited by: SS1.
* R. Li and X. Du (2023)Leveraging structured information for explainable multi-hop question answering and reasoning. In Proceedings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 2023b, pp. 6779-6789.

Ruosen Li, Ziming Luo, and Xinya Du. Fine-grained hallucination detection and mitigation in language model mathematical reasoning, 2024. URL https://arxiv.org/abs/2410.06304.
* Li et al. (2021) Sha Li, Heng Ji, and Jiawei Han. Document-level event argument extraction by conditional generation. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 894-908, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.69. URL https://aclanthology.org/2021.naacl-main.69.
* Lu et al. (2022) Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 2507-2521. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/11332b6b6cf4485b84afadb1352d3a9a-Paper-Conference.pdf.
* Mavi et al. (2022) Vaibhav Mavi, Anubhav Jangra, and Adam Jatowt. A survey on multi-hop question answering and generation, 2022.
* Mehta et al. (2022) Sneha Mehta, Huzefa Rangwala, and Naren Ramakrishnan. Improving zero-shot event extraction via sentence simplification. In Ali Hurriyetoglu, Hristo Tanev, Vanni Zavarella, and Erdem Yoruk, editors, _Proceedings of the 5th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE)_, pages 32-43, Abu Dhabi, United Arab Emirates (Hybrid), December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.case-1.5. URL https://aclanthology.org/2022.case-1.5.
* Min et al. (2019) Sewon Min, Eric Wallace, Sameer Singh, Matt Gardner, Hannaneh Hajishirzi, and Luke Zettlemoyer. Compositional questions do not necessitate multi-hop reasoning. In Anna Korhonen, David Traum, and Lluis Marquez, editors, _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 4249-4257, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1416. URL https://aclanthology.org/P19-1416.
* Ning et al. (2018) Qiang Ning, Hao Wu, and Dan Roth. A multi-axis annotation scheme for event temporal relations. In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1318-1328, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1122. URL https://aclanthology.org/P18-1122.
* Ning et al. (2020) Qiang Ning, Hao Wu, Rujun Han, Nanyun Peng, Matt Gardner, and Dan Roth. TORQUE: A reading comprehension dataset of temporal ordering questions. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 1158-1172, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.88.
* Pan et al. (2021) Liangming Pan, Wenhu Chen, Wenhan Xiong, Min-Yen Kan, and William Yang Wang. Unsupervised multi-hop question answering by question generation. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 5866-5880, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.469. URL https://aclanthology.org/2021.naacl-main.469.
* Peng et al. (2023) Hao Peng, Xiaozhi Wang, Jianhui Chen, Weikai Li, Yunjia Qi, Zimu Wang, Zhili Wu, Kaisheng Zeng, Bin Xu, Lei Hou, and Juanzi Li. When does in-context learning fall short and why? a study on specification-heavy tasks, 2023a. URL https://arxiv.org/abs/2311.08993.
* Peng et al. (2019) Hao Peng, Xiaozhi Wang, Feng Yao, Zimu Wang, Chuzhao Zhu, Kaisheng Zeng, Lei Hou, and Juanzi Li. OmniEvent: A comprehensive, fair, and easy-to-use toolkit for event understanding. In Yansong Feng and Els Lefever, editors, _Proceedings of the 2023 Conference on Empirical Methodsin Natural Language Processing: System Demonstrations_, pages 508-517, Singapore, December 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-demo.46. URL https://aclanthology.org/2023.emnlp-demo.46.
* Raffel et al. (2023) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer, 2023.
* Saxena et al. (2020) Apoorv Saxena, Aditay Tripathi, and Partha Talukdar. Improving multi-hop question answering over knowledge graphs using knowledge base embeddings. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 4498-4507, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.412. URL https://aclanthology.org/2020.acl-main.412.
* Costa et al. (2020) Tarcisio Souza Costa, Simon Gottschalk, and Elena Demidova. Event-qa: A dataset for event-centric question answering over knowledge graphs. In _Proceedings of the 29th ACM International Conference on Information & Knowledge Management_, CIKM '20, page 3157-3164, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450368599. doi: 10.1145/3340531.3412760. URL https://doi.org/10.1145/3340531.3412760.
* 39, 1990. URL https://api.semanticscholar.org/CorpusID:53550389.
* Trivedi et al. (2022) Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. MuSiQue: Multihop questions via single-hop question composition. _Transactions of the Association for Computational Linguistics_, 10:539-554, 2022. doi: 10.1162/tacl_a_00475. URL https://aclanthology.org/2022.tacl-1.31.
* Walker et al. (2006) Christopher Walker, Stephanie Strassel, Julie Medero, and Kazuaki Maeda. ACE 2005 multilingual training corpus. _Linguistic Data Consortium_, 57, 2006. URL https://catalog.ldc.upenn.edu/LDC2006T06.
* Wang et al. (2022) Xiaozhi Wang, Yulin Chen, Ning Ding, Hao Peng, Zimu Wang, Yankai Lin, Xu Han, Lei Hou, Juanzi Li, Zhiyuan Liu, Peng Li, and Jie Zhou. MAVEN-ERE: A unified large-scale dataset for event coreference, temporal, causal, and subevent relation extraction. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 926-941, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.60. URL https://aclanthology.org/2022.emnlp-main.60.
* Wang et al. (2024) Zimu Wang, Lei Xia, Wei Wang, and Xinya Du. Document-level causal relation extraction with knowledge-guided binary question answering, 2024. URL https://arxiv.org/abs/2410.04752.
* Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 24824-24837. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7fb31abca4-Paper-Conference.pdf.
* Yan et al. (2011) Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong, Xiaoming Li, and Yan Zhang. Evolutionary timeline summarization: A balanced optimization framework via iterative substitution. In _Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval_, SIGIR '11, page 745-754, New York, NY, USA, 2011. Association for Computing Machinery. ISBN 9781450307574. doi: 10.1145/2009916.2010016. URL https://doi.org/10.1145/2009916.2010016.
* Yang et al. (2018) Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In _Proceedings of the 2018 Conference on Empirical Methods in Natural LanguageProcessing_, pages 2369-2380, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259.
* Yang et al. [2020] Zonglin Yang, Xinya Du, Alexander Rush, and Claire Cardie. Improving event duration prediction via time-aware pre-training. In Trevor Cohn, Yulan He, and Yang Liu, editors, _Findings of the Association for Computational Linguistics: EMNLP 2020_, pages 3370-3378, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.302. URL https://aclanthology.org/2020.findings-emnlp.302.
* Yang et al. [2023] Zonglin Yang, Xinya Du, Erik Cambria, and Claire Cardie. End-to-end case-based reasoning for commonsense knowledge base completion. In Andreas Vlachos and Isabelle Augenstein, editors, _Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics_, pages 3509-3522, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.eacl-main.255. URL https://aclanthology.org/2023.eacl-main.255.
* Yang et al. [2024a] Zonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, and Furu Wei. Language models as inductive reasoners. In Yvette Graham and Matthew Purver, editors, _Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 209-225, St. Julian's, Malta, March 2024a. Association for Computational Linguistics. URL https://aclanthology.org/2024.eacl-long.13.
* Yang et al. [2024b] Zonglin Yang, Xinya Du, Rui Mao, Jinjie Ni, and Erik Cambria. Logical reasoning over natural language as knowledge representation: A survey, 2024b. URL https://arxiv.org/abs/2303.12023.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] See Abstract and Section 1. 2. Did you describe the limitations of your work? [Yes] See Section 7. 3. Did you discuss any potential negative societal impacts of your work? [N/A] 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] See Abstract. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? See Section 6. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? See Section 6. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Section 6.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] 2. Did you mention the license of the assets? See Supplementary Materials. 3. Did you include any new assets either in the supplemental material or as a URL? See Abstract. 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A]5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [Yes] See Appendix N. 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [Yes] See Section 4.

Definition and Complexity of Event-centric Questions

An event-centric question is a query that focuses on events, typically involving **event triggers** and their **associated argument entities**, often in the context of event structures and their relations.

We adopt ACE (Automatic Content Extraction) (Walker et al., 2006) definition of entities, relations, and events. Under this definition, an event consists of an event trigger (e.g., "kills") and its associated argument entities (e.g., "major general", "Syria"). An event trigger is a word that most clearly describes the event's occurrence, and it is **often a verb that evokes the action or the status** of the target event. Event arguments are mostly entities and need to be inferred in reasoning processes. Suppose an event structure is (**Tri**, \(Arg_{1}\), \(Arg_{2}\), \(Arg_{3}\),...). The event-centric QA can involve two types of relations: (i) between event \(E\) (triggered by **Tri**) and across its certain arguments (\(Arg_{1}\), \(Arg_{2}\),...), e.g., "major general" is a victim of "kills"; or (ii) between event \(E_{1}\) (triggered by **Tri**) and event \(E_{2}\) (triggered by **Tri**), e.g., "kills" is before "reported". Different from traditional multi-hop QA, for any two argument entities (\(Arg_{1}\), \(Arg_{2}\)) in an event, there is no _direct_ relationship (bypassing event trigger) between \(Arg_{1}\) and \(Arg_{2}\). For instance, in the event (**Tri**=kills, \(Arg_{1}\)=location, \(Arg_{2}\)=timestamp,...), there exists no direct relation between the location and the timestamp.

Moreover, two specific differences exist:

**More Complex Relations.** Unlike entity-entity relations (in entity-centric datasets), which typically feature a singular type of relation connecting all entity pairs, our event-centric dataset encompasses the above relation and also includes event-event relations and event-argument relations, which are much more complex. Below, we present examples and explanations of various types of relations:

1. _Event-Event Temporal Relations_: Any pair of events (\(E_{1}\), \(E_{2}\)) have inherent temporal relations, such as before, after, and contains (Han et al., 2021). For example, in the example (**Reported**, _after_, **Kills**), where **Bold texts** denote event triggers while the _italics text_ indicate temporal relations, the temporal relation "_after_" connects two events: **Reported** and **Kills**. All of them cannot be directly extracted from the given context. Instead, models have to identify and infer them as part of their reasoning process.
2. _Event-Event Causal Relations_: A pair of events (\(E_{1}\), \(E_{2}\)) exhibits a casual relation if \(E_{2}\) definitely happens after \(E_{1}\)(Han et al., 2021). For example, (**Kills**, _caused by_, **Explode**), where **Bold texts** denote events while the _italics text_ represents a causality relation, comes from the reasoning process to answer the question, "_What caused the death of the 21 people?_". Such relationships are inherently complex. While we can say someone/something causes an event, we can hardly say someone/something causes an entity. Consequently, this type of relationship is rarely encountered in current multi-hop datasets.
3. _Event-Entity Relations_: (**Pay**, timestamp, 11 am) is an example connecting trigger "**Pay**" and entity "11 am" by the relation "timestamp". This only exists in event-centric datasets.
4. _Entity-Entity Relations_: In the example, (Alice, daughter of, Mary), the "daughter of" links "Alice" and "Mary". The majority of relations connecting entities can be readily identified in the given context, while only a handful need rephrasing for clarity. Entity-centric datasets only contain this type of relation.

**More Complex Reasoning Process.** Our event-centric dataset presents a greater challenge compared to other multi-hop entity-entity datasets. While solving multi-hop questions in conventional datasets involves hopping over entities, in our dataset, models must navigate through both entities and events (a heterogeneous reasoning chain). This means that reasoning begins with entities as usual, but then extends to identifying the event associated with the entity, its location, and the trigger word for events, which poses significant difficulty for current models. There are two examples of two types of reasoning:

1. Hopping over only entities looks like: Alice \(\xrightarrow{\text{daughter of}}\) Mary \(\xrightarrow{\text{spouse of}}\) William. The reasoning chain contains only entities and relations in parentheses. This answers the question: "_Who is Alice's father?_". Given the direct relations between all entities in the provided context, models can easily answer the question by reasoning over these entities.
2. Hopping over both entities and events looks like: Al-Monitor \(\xrightarrow{\text{commanicator}}\)**Reported**\(\xrightarrow{\text{after}}\)**Kills**\(\xrightarrow{\text{victim}}\) General. **Bold texts** denote triggers, while the _italics text_ indicate temporal relations. It answers the question: "_Who was the victim before AI-Monitor reported online?_". Unlike the above case with direct entity relations, there's no direct relation between "AlMonitor" and "General" in the context provided. Consequently, simple entity hopping won't suffice. The reasoning process of the question requires identifying the event associated with "AI-Monitor" (the "**Reported**" event), establishing its temporal relation with the "**Kills**" event, and deducing the answer, "General". The event and relation identification process poses a considerable challenge for current models.

## Appendix B Examples of Strategies

The following paragraphs explain examples in Table 1. Figures 2(a) and 2(b) correspond to the strategies 1 and 2 in Table 1.

**Strategy 1.** In Figure 2(a), the demise of a major general was conveyed by AI-Monitor, where "reported" functions as the event trigger and "AI-Monitor" assumes the role of "communicator", establishing an event-argument relation. Additionally, "kills" also acts as an event trigger, preceding the "reported" event; hence, a temporal relationship termed "before" exists between the two events. Moreover, entity-only reasoning chains fall short of capturing the entire reasoning process, particularly in establishing the relationship between "AI-monitor" and "General". Relations are always brief, precise, and general. The relation "reported" can not precisely convey the relationship, and "reported before the death of" lacks generality. Thus, there is no relation between them.

**Strategy 2.** In Figure 2(b), all green nodes are entities, and all blue triangle nodes are triggers of events. Edges connecting them are event-argument relations. The reasoning chain starts with "11 people". and hops with the bridging entity "Belfast" to the result "Plane". The events "exploded" and "transported" are connected by bridging entities but not direct event-event relations. Moreover, relying solely on entity-based reasoning chains may inadequately capture the entirety of the reasoning process. For the first sub-question, the context indicates "11 people" died due to the explosion at "Belfast", without a direct relationship. Even any implicit relationship can only provide partial information, like (11 people, died at, Belfast) In the second sub-question, no relation exists between "Belfast" and "Plane". They can only be connected by the event trigger "transported".

**Strategy 3.** In the context shown in Table 1, there are five events that describe "victims", such as the "kills" event, the "wounded" event, the "killed" event, etc. Models need to identify those events first and conduct the counter later. Due to the space limitation, only three are shown in the context.

**Strategy 4.** The question asks to compare the number of victims caused by IED or IS. Firstly, they belong to different roles. IED is an item, while IS is an organization. Then, models should extract all related events and count the number of victims among events respectively. IED kills 4 victims, while IS kills 24 victims. Finally, by comparing 4 and 24, models should choose 24 and output "IS" as the answer.

**Strategy 5:** This question is unanswerable since the reasoning chain breaks at the start step. The reasoning chain starts with "21 bombs". However, the context describes "_21 bombs have gone off in Belfast_" but does not mention where they are manufactured. We cannot move the reasoning step forward anymore. Thus, the question is unanswerable.

## Appendix C WikiEvents and New Documents

WikiEvents [10] dataset contains real-world news articles and annotates all events with the KAIROS ontology. It provides 246 documents and 33 event types defined in two levels. The first level covers eight general domains: life, movement, transaction, business, conflict, contact, personnel, and justice.

In this paper, we generate our dataset from the WikiEvent dataset, which contains over 15 scenarios across all domains. **Plus, our event-centric question-generation process is versatile across domains and languages**. Specifically, we can first perform event extraction (EE) to construct the event structures for every new domain and language and generate QA pairs datasets. For instance, we perform a preliminary study on the TORQUE [23] dataset, which does not include predefined events and event structures.

Given a new document from TORQUE:

_Mr. Erdogan accepted the Israeli apology, the prime minister's office said. Mr. Erdogan has long sought an apology for the raid in May 2010 on the Mavi Marmara, which was part of a flotilla that sought to break Israel's blockade of Gaza._

the first step is to perform event extraction by LLM. Event samples are as follows:

\[\begin{array}{l}\text{Event}_{1}\text{:}\\ \text{Trigger}_{1}\text{:}\text{accepts}\\ \text{Agent}\text{:}\text{Erdogan}\\ \text{Theme:}\text{Israeli apology}\\ \text{Event}_{2}\text{:}\\ \text{Trigger}_{2}\text{:}\text{raid}\\ \text{Target:}\text{Mavi Marmara}\\ \text{Date:}\text{May 2010}\\ \dots\end{array}\]

By following the question generation step we propose, we can get the following question:

_What target in an old event caused the Israelis to apologize?_

We annotate 10 questions and ask two annotators to answer them. The average accuracy is 85%, and the upper bound accuracy is 90%. The human agreement (Cohen Kappa) score is 62%.

## Appendix D KAIROS Schema Templates

For example, the template describing a transport event looks like:

[**Transporter**] transported in [**Vehicle**] from [**Origin**] place to [**Destination**] place.

Tokens in square brackets are placeholders. Moreover, we split the above template into several chunks by its context and semantic meaning, such as:

* [**Transporter**]
* transported
* in [**Vehicle**]
* from [**Origin**] place
* to [**Destination**] place

In this format, we can keep useful chunks only and remove redundant parts to form new templates. Specifically, verbs are always kept in the final template. Each template can also be converted to a question-version template. In detail, the target placeholder in the template will be replaced by wh-words according to its corresponding semantics.

## Appendix E Crowdsourcing Hiring and Payment

We hire student workers to annotate the dataset. All annotators have bachelor's degrees and basic knowledge about events, including trigger, role, and argument. Before annotation, we post a qualification test for all candidates, which includes a document and five template-based questions. We also provide an instruction document for all candidates to learn how to annotate questions. In this process, we answer all their questions about the annotation process to ensure they fully understand the task. Then, They annotate the template document. We select the top five workers who annotate accurately and quickly. In the annotation process, our workers were encouraged to spend **2-3** minutes per question on average. We paid around $0.65 per question. Among all workers, the average hourly rate is $15.

## Appendix F Additional Dataset Analysis

The general statistics of our MEQA dataset is shown in Table 7. We also analyze the number of reasoning steps and the most frequent \(n\)-grams in this section.

**Reasoning Steps.** Figure 5 contains the average reasoning steps of each strategy in different data splits, in which the distributions are almost the same in the different data splits, indicating the correctness of our dataset-splitting technique. Questions in Strategies 1 and 4 contain the most reasoning steps because the requirement of reasoning across the events and their arguments, and the steps of Strategies 2 and 3 are not comparable because two-step reasoning satisfies most questions. Finally, because the questions in Strategy 5 are unanswerable, we did not annotate the explanations for its questions.

**Most Frequent \(n\)-grams.** Figure 6 illustrates the most frequent \(n\)-grams, including uni-grams, bigrams, and trigrams in different strategies of questions, which can be regarded as semantic cues in the questions to reason about particular types of questions. From Fig. 6, we observe that the most frequent n-grams are consistent with the question types; for example, the most frequent \(n\)-gram in the event relation strategy includes specific event relations, such as "_before_" and "_because_", while those in the event listing and counting strategy include "_how many_" and "_related to_", indicating the consistency of the annotated questions with our proposed question strategies.

\begin{table}
\begin{tabular}{l|c c c c} \hline \hline  & Train & Dev & Test & Overall \\ \hline \# of Documents & 189 & 20 & 22 & 211 \\ \# of Questions & 1,674 & 282 & 287 & 2,243 \\ \# of Events & 1,361 & 129 & 204 & 1,565 \\ \# of Entities & 2,280 & 233 & 364 & 2,644 \\ Avg. Document Length & 695.4 & 815.2 & 494.9 & 676.4 \\ Avg. Question Length & 11.5 & 11.6 & 11.7 & 11.6 \\ Avg. \# of Steps/Hops & 2.6 & 2.6 & 2.6 & 2.6 \\ \hline \hline \end{tabular}
\end{table}
Table 7: General statistics of MEQA.

Figure 5: Average number of reasoning steps for each strategy in different data splits.

\begin{table}
\begin{tabular}{l|l l l l} \hline \hline  & **MEQA** & **HotpotQA** & **2WikiMultihopQA** & **Musique** \\ \hline Avg. Question Length & 11.6 & 16 & 12.64 & 15.40 (tokens per question) \\ Avg. \# of Hops & 2.6 & 2 & 2 & 2.4 \\ \# of Documents & 211 & 14,810 (golden) & 25,152 (golden) & 7676 \\ \# of Questions & 2,243 & 7,405 & 12,576 & 2,459 \\ \# of Events & 1565 & - & - & - \\ \# of Entities & 2644 & - & - & - \\ \hline \hline \end{tabular}
\end{table}
Table 8: Data statistics comparison between MEQA and existing datasets.

**Dataset Statistics Comparison.** Table 8 compares our MEQA with other datasets. Since those benchmark papers do not provide parts of the statistics, we estimate partial numbers based on the statistics they provide in the paper. Since none focus on events, they do not have event-related statistics.

## Appendix G Prompts

Given the document and question, the entire prompt for a demonstration looks as follows:

**Vanilla Prompt.** This is the vanilla prompt for most question answering tasks. The input contains only documents and a question, while the output is the answer.

Document:

<Document>

Question:

<Question>

Answer:

**Chain-of-Thought (CoT).** The CoT prompt has reasoning chains before giving the final answer, while the Vanilla prompt does not contain it. In our implementation, reasoning chains are in question

Figure 6: The most frequent \(n\)-grams in the questions in different strategies.

answering format, which is more clear and easy for people to understand.

Document: <Document> Pleaseanswer the following question step-by-step with explanation: <Question> Answer: <Question l> <Answer l>... <Question n> <Answer n> So, the answer is: <Answer>

**Post-hoc.** The post-hoc prompt provides explanations after generating the answer. The explanation format is the same as the format of reasoning chains in the CoT prompt.

 Document:  <Document>  Pleaseanswer the following question and then explain reasoning steps:  <Question> Answer:  <Answer>  Explanation:  <Question l> <Answer l> ... <Question n> <Answer n>

**Event Relations.** Given the document and events, the prompt to generate relations between events is as follows:

Document: <Document> Event 1: <Triggerl>  <role1_l> <argument1_l>  <role1_2> <argument1_2> ... Event 2: <Trigger2>  <role2_l> <argument2_l> ... Relation Options: Before, After, At the same time, Contain, Contained by, Cause, Caused by,... Please output the relation between the two events in the above document:

## Appendix H Pseudocode and Prompt of the Completeness Metric

The pseudocode of the algorithm is illustrated in Algorithm 1, whose time complexity is in quadratic time complexity \(O(n^{2})\). In this algorithm, a prediction step (\(pi\)) moves forward only if it cannot find any more matches to a golden step. A golden step moves forward only if it is matched. The "match" condition is the core of the algorithm.

[MISSING_PAGE_FAIL:23]

to compare the performance of two models. In this experiment, we apply two models, GPT-4-0613 (training data up to Sep 2021 before WikiEvent) and GPT-4-turbo-2024-04-09 (training data up to Dec 2023 after WikiEvent). The two models have similar abilities across other NLP tasks, such as math problems (on GSM8k and MATH), open-domain question answering (on Nataul Question, ARC), text summarization, etc, but the GPT-4-turbo-2024-04-09 may be affected by the potential leakage.

Based on Table 11, the performance of GPT-4-turbo-2024-04-09 does not exceed its of GPT-4-0613. Thus, the potential leakage of WikeEvent has little influence over our MEQA benchmark evaluation on event-based reasoning capabilities. As a similar finding is in Trivedi et al. (2022), models pre-trained on Wikipedia do not perform well on the multi-hop questions requiring complex reasoning.

## Appendix K Additional Information Analysis

The context of the example is shown in Table 12. Since the prompts for QA-form and Free-form explanations are different, the results of "CoT-QA" and "CoT-Freeform" settings are not directly comparable. Using discrete entities or triggers as additional information (Entity or Event Triggers) can hardly affect the performance of ChatGPT, and sometimes, they can mislead the model because of their simplicity and incompleteness, leading the model to hardly learn helpful information from the graphs. However, after combining the entities and trigger information (Event Triggers + Arguments), more information was composed in the event structure format, which significantly helped the model find event-related information and answer event-centric questions.

## Appendix L Error Analysis

**Incorrect Start Event Identification.** Because of the sequential nature of natural language, the model tended to read and reason questions from the beginning, which isn't always the case in real reasoning scenarios. For example, given the question "_What was damaged before noticed by individuals?_",

\begin{table}
\begin{tabular}{l|c c c|c c c|c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{2}{c|}{General Performance} & \multicolumn{3}{c|}{Completeness} & Logical \\  & Precision & Recall & F1 & Precision & Recall & F1 & Consistency \\ \hline CoT-QA (C+Q\(\rightarrow\)E+A) & 0.2056* & 0.5311 & 0.2268* & 0.1542 & 0.2190* & 0.1790 & 0.5124 \\ _w/ Entity_ & 0.2014 & 0.5368 & 0.2488 & 0.1933 & 0.2119 & 0.1906 & 0.5212 \\ _w/ Entity KG_ & 0.2423 & 0.5434 & 0.2812 & 0.1910 & 0.2548 & 0.2164 & 0.5415 \\ _w/ Event Triggers_ & 0.2526 & 0.5533 & 0.2973 & 0.1951 & 0.2810 & 0.2289 & 0.5437 \\ _w/ Event Triggers + Arguments_ & 0.2691 & 0.5474 & 0.2988 & 0.2788 & 0.3833 & 0.3204 & 0.5311 \\ _w/ Fall Event KG_ & 0.3021 & **0.5950** & **0.3277** & 0.3429 & **0.4274** & **0.3793** & 0.5599 \\ \hline CoT-Freeform (C+Q\(\rightarrow\)FE+A) & 0.1363 & 0.5356* & 0.1740 & 0.2643* & 0.1786 & 0.2114* & **0.9129*** \\ _w/ Fall Event KG_ & **0.3120** & 0.5445 & 0.3253 & **0.3929** & 0.2738 & 0.3214 & 0.9071 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Performance on all experiments using Llama3-70B.

\begin{table}
\begin{tabular}{l|c c c|c c c|c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{2}{c|}{General Performance} & \multicolumn{3}{c|}{Completeness} & Logical \\  & Precision & Recall & F1 & Precision & Recall & F1 & Consistency \\ \hline CoT-QA (C+Q\(\rightarrow\)E+A) & 0.0177 & 0.5460 & 0.0330 & 0.2639 & 0.3476* & 0.2403 & 0.6261 \\ _w/ Entity KG_ & 0.0150 & 0.4860 & 0.0282 & 0.3399 & 0.3119 & 0.2578 & 0.6529 \\ _w/ Entity KG_ & 0.0184 & 0.5395 & 0.0346 & 0.3552 & 0.3643 & 0.2881 & 0.5911 \\ _w/ Event Triggers_ & 0.0216 & 0.4979 & 0.0396 & 0.2908 & 0.3857 & 0.2722 & 0.6783 \\ _w/ Event Triggers + Arguments_ & 0.0250 & 0.5832 & 0.0466 & 0.3782 & 0.3905 & 0.3109 & 0.6488 \\ _w/ Fall Event KG_ & **0.0245** & 0.5929 & 0.0454 & 0.3480 & **0.3976** & 0.3045 & 0.6834 \\ \hline CoT-Freeform (C+Q\(\rightarrow\)FE+A) & 0.0207* & 0.06040* & 0.0395* & 0.3714* & 0.2595 & 0.3043* & 0.8929* \\ _w/ Fall Event KG_ & 0.0251 & **0.6430** & **0.0475** & **0.5286** & 0.3714 & **0.4343** & **0.9071** \\ \hline \hline \end{tabular}
\end{table}
Table 9: Performance on all experiments using Claude-3-Haiku. Two baselines and their further experiments are grouped in the table. In each group, the first line is the performance of the baseline. All the following lines in a group indicate additional contents that are appended after context **C**. **Bold numbers** shows the best results in each column. Numbers with (*) indicate they are the best among all baselines.

[MISSING_PAGE_FAIL:25]

[MISSING_PAGE_FAIL:26]

## Appendix A

Figure 7: An example annotation interface for annotating the questions for Strategy 1: Event Relations.

## Appendix B

Figure 8: An example annotation interface for annotating the questions for Strategy 2: Entity Bridging.

Figure 9: An example annotation interface for annotating the questions for Strategies 3 to 5: Event Listing and Counting, Event Comparison, and Unanswerable.