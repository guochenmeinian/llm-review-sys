# Provably Transformers Harness Multi-Concept Word Semantics for Efficient In-Context Learning

Dake Bu\({}^{1}\), Wei Huang\({}^{2}\), Andi Han\({}^{2}\), Atsushi Nitanda\({}^{3,4}\), Taiji Suzuki\({}^{5,2}\), Qingfu Zhang\({}^{1}\), Hau-San Wong\({}^{1}\)

\({}^{1}\)_Department of Computer Science, City University of Hong Kong, Hong Kong SAR \({}^{2}\)Center for Advanced Intelligence Project, RIKEN, Japan \({}^{3}\)CFAR and IHPC, Agency for Science, Technology and Research (A+STAR), Singapore \({}^{4}\)College of Computing and Data Science, Nanyang Technological University, Singapore \({}^{5}\)Department of Mathematical Informatics, the University of Tokyo, Japan_

dakebu2-c@my.cityu.edu.hk, {wei.huang.vr, andi.han}@riken.jp,

atsushi_nitanda@cfar.a-star.edu.sg,taiji@mist.i.u-tokyo.ac.jp,

{qingfu.zhang, cshswong}@cityu.edu.hk

Corresponding authors

###### Abstract

Transformer-based large language models (LLMs) have displayed remarkable creative prowess and emergence capabilities. Existing empirical studies have revealed a strong connection between these LLMs' impressive emergence abilities and their in-context learning (ICL) capacity, allowing them to solve new tasks using only task-specific prompts without further fine-tuning. On the other hand, existing empirical and theoretical studies also show that there is a linear regularity of the multi-concept encoded semantic representation behind transformer-based LLMs. However, existing theoretical work fail to build up an understanding of the connection between this regularity and the innovative power of ICL. Additionally, prior work often focuses on simplified, unrealistic scenarios involving linear transformers or unrealistic loss functions, and they achieve only linear or sub-linear convergence rates. In contrast, this work provides a fine-grained mathematical analysis to show how transformers leverage the multi-concept semantics of words to enable powerful ICL and excellent out-of-distribution ICL abilities, offering insights into how transformers innovate solutions for certain unseen tasks encoded with multiple cross-concept semantics. Inspired by empirical studies on the linear latent geometry of LLMs, the analysis is based on a concept-based low-noise sparse coding prompt model. Leveraging advanced techniques, this work showcases the exponential 0-1 loss convergence over the highly non-convex training dynamics, which pioneeringly incorporates the challenges of softmax self-attention, ReLU-activated MLPs, and cross-entropy loss. Empirical simulations corroborate the theoretical findings.

## 1 Introduction

Recently, a variety of transformer-based large language models (LLMs) have demonstrated remarkable performance across a broad spectrum of machine learning tasks, including natural language understanding [1], symbolic reasoning [2], and even heuristics design [3, 4]. One crucial emerging ability of these models is their in-context learning (ICL) capacity [5], which allows them to learn from a few demonstrations and conduct predictions on new queries without requiring any furtherfine-tuning. However, the current theoretical understanding of the mechanisms underlying this ICL capability remains limited, leaving the reasons for the remarkable emergence and generalization power of transformer-based LLMs in unseen ICL tasks largely unexplained.

In line with traditional topic models [6], [7; 8] propose that latent concepts / topics underlie natural texts, providing a Bayesian inference framework to elucidate the ICL mechanism via Bayesian Model Averaging (BMA) approach. On the other hand, theoretical and empirical studies have shown that transformer-based models exhibit linear geometric regularities in their latent representations as a result of concept or topic learning [9; 10], where the representations _within-concept_ have positive inner products while representations _cross-concepts_ exhibit near-orthogonal relationships. This structured semantic geometry has been well-documented in recent research on pre-trained LLMs [11; 12; 10; 13]. However, the connection between this observed multi-concepts latent geometric structure and the LMs' remarkable ICL capabilities remains unclear. Separately, recent theoretical analyses have modeled ICL as a martingale process driven by latent "concept" variables [14; 15]. Yet, these studies have not incorporated the observed multi-concept semantic regularity into their analyses, nor have they discussed the strong out-of-distribution (OOD) ICL abilities exhibited by transformers.

Additionally, existing theoretical work on transformer has been conducted on unrealistic, oversimplified settings, such as linear or ReLU transformers [16; 17; 18; 19], MLP-free attention-only models [16; 20], QK-combined softmax attention [19; 20; 21; 22; 23], unrealistic infinite dimensional assumption [14; 19; 21; 24] and impractical loss functions like square loss [9; 16; 25; 20; 26] and hinge loss [27; 28]. Furthermore, existing works have only been able to derive linear or sub-linear convergence rates for the 0-1 loss.

Therefore, there is a need for a more advanced analysis that can bridge the understanding between the multi-concept semantic regularity and the mechanisms underlying transformer-based ICL. This naturally leads to the research question:

## Essential Questions

Whether and how do the geometric regularity of the multi-concept-encoded representation facilitate transformer in conducting efficient ICL?

To answer the above question, following the meaningful data modeling ideas in [9; 29], we conduct theoretical analysis on a concept-specific sparse coding prompt distribution for classification tasks, where the sparse latent variable encodes the information denoting the word's belonging concept. Importantly, the features in both the word's and label's dictionaries exhibit concept-specific geometric properties - within-concept positive inner products and cross-concept orthogonal geometric properties - that aligns with the findings in [9; 10; 11]. Our main contributions are highlighted as below.

1. First, we provide a comprehensive analysis of the learning dynamics for a two-layer transformer model, comprising one attention layer followed by a ReLU-activated feed-forward network, which is trained using the cross-entropy loss via stochastic gradient descent over a concept-specific sparse coding prompt distribution. Leveraging advanced analytical techniques, we showcase the asymptotic properties governing the coupled learning dynamics of the attention and MLP layers.
2. To the best of our knowledge, we are the first to prove an exponential convergence of the 0-1 loss over this challenging setting. Despite the highly non-convex optimization landscape, we demonstrate that the transformer can achieve Bayes optimal test error with just a logarithmic number of iterations.
3. We provably show how the multi-concept encoded linear semantic geometry can enable transformer to efficiently perform certain out-of-distribution ICL tasks. This offers an intuitive explanation for why transformer-based LLMs are able to successfully leverage the polysemous nature of words to tackle diverse, unseen concept-specific tasks, aligning well with users' practical experiences. Furthermore, our analysis takes a step forward in providing a potential theoretical underpinning for the innovative capabilities of LLMs, encompassing their ability to achieve cross-concept knowledge intersection. We believe our findings provide an initial positive response to Question 5.1.4 in the ICML 2024 position paper [30], which asks whether the observed latent geometry of LLMs can explain their OOD extrapolation abilities.

Related Work

**Theory of Exponential Convergence Rate of Stochastic Gradient Descent.** Our analysis of the exponential convergence rate for the 0-1 loss builds upon prior work linking the excess risk and essential supremum norm to exponentially fast convergence under the "hard low-noise condition" [31; 32]. This phenomenon has been further explored in more recent studies analyzing the exponential convergence of stochastic gradient descent (SGD) [33; 34; 35; 36; 37], as well as in more generalized settings such as multiclass classification [38] and support vector machines [39].

**Feature Learning in Learning Theory.** Recent works in learning theory have extensively studied structured data from a _feature learning_ perspective, examining NN's feature direction reconstruction and noise memorization as a proxy for training or 0-1 loss convergence [40; 41; 42]. While prior studies often assumed orthogonal features, recent efforts have analyzed non-orthogonal scenarios [43; 44]. Our work extends this line-of-research to challenging nonlinear Attention-MLP transformers with non-orthogonal structured data representations.

**Theory of Transformers and In-Context Learning** The literature on Transformers and ICL is wide-ranging, and we will selectively address the most relevant ones. Prior studies have analyzed how transformers learn topic/concept semantics [9], the origins and biases of LLM representations using latent variable models [10], and ICL from a model averaging perspective [14]. However, albeit incorporating concept variables, these works do not connect the geometric properties of concept-encoded representations to transformers' powerful ICL abilities. Another line of research has studied the learning dynamics of ICL, including analyses of linear transformers [17; 19], QK-combined attention-only models [45], and multi-head softmax attention over linear regression without MLP [25]. Though relevant, these works rely on simplifications and do not notice the connection between semantic regularity and powerful ICL. While [28] also analyzes the learning dynamics of transformers with softmax attention and ReLU MLPs for in-context classification tasks, making it the most relevant prior work, our analysis differs in several key aspects. Specifically, (i) they consider orthogonal dictionary learning with a single label vector, in contrast to our non-orthogonal concept-encoded dictionaries for both words and labels; (ii) their technique requires a large batch size (at least \(\varepsilon^{-2}\), where \(\varepsilon\) is the test error) and long context lengths, which are not required in our result; and (iii) they utilize an impractical hinge loss and only achieve linear convergence without a relation to \(\varepsilon\), whereas we analyze the more practical cross-entropy loss and derive an exponential convergence rate in terms of the test error \(\varepsilon\). However, we note that this is only an informal comparison due to the differences in the models and primary findings. A detailed Related Work Section is deferred to Appendix C.

## 3 Problem Setup

**Notations.** For \(l_{2}\) and Frobenius norms we utilize \(\|\cdot\|\) and \(\|\cdot\|_{F}\) to denote their computations. Considering two series \(a_{n}\) and \(b_{n}\), we denote \(a_{n}=O\left(b_{n}\right)\) if there exists positive constant \(C>0\) and \(N>0\) such that for all \(n\geq N\), \(\left|a_{n}\right|\leq C\left|b_{n}\right|\). Similarly, we denote \(a_{n}=\Omega\left(b_{n}\right)\) if \(b_{n}=O\left(a_{n}\right)\) holds, and \(a_{n}=\Theta\left(b_{n}\right)\) if \(a_{n}=O\left(b_{n}\right)\) and \(a_{n}=\Omega\left(b_{n}\right)\) both hold. Our \(\mathds{1}(\cdot)\) is to denote the indicator variable of an event. In addition, we denote \(\text{span}(v_{1},v_{2},\ldots,v_{k})\) as the linear subspace spanned by the vectors \(v_{1},v_{2},\ldots,v_{k}\), and \(\text{conic}(v_{1},v_{2},\ldots,v_{k})\) denotes the conic hull (the set of all non-negative linear combinations) of the vectors \(v_{1},v_{2},\ldots,v_{k}\).

### Data Distribution

The data distribution employed in this study draws inspiration from a range of empirical and theoretical research works [9; 10; 46; 47; 48]. This distribution captures context-awareness and can be viewed as a specialized prompt version of PLSA [49] and LDA [6]. In this distribution, each word and label has multiple feature embeddings, each embedding corresponding to a different concept. This is achieved through the use of a sparse latent concept/topic variable, which happened to be particularly adept at representing language polysemy [47]. Adhering to the LLM representation explored in [9; 10], the features in both the word and label dictionaries maintain orthogonality across concepts and positive inner products within concepts. Additionally, the distribution incorporates Gaussian noise accounting for linguistic ambiguity or the imperfection of the LLM's representation.

**Definition 1**.: _Polysemous Word Model \(\left(\mathcal{D}_{\bm{x}},\mathcal{D}_{\bm{y}},\mathcal{D}_{\bm{z}},\mathcal{ D}_{\xi_{\bm{z}}},\mathcal{D}_{\xi_{\bm{y}}}\right)\). We assume there exists \(K_{1}\) task-relevant concepts, each characterized by two semantically-opposite word's feature vectors \(\bm{\mu}_{k_{1}}^{+}\) and \(\bm{\mu}_{k_{1}}^{-}\), and their corresponding label's feature vectors \(\bm{q}_{k_{1}}^{+}\) and \(\bm{q}_{k_{1}}^{-}\), \(\forall k_{1}\in[K_{1}]\). There are also \(K_{2}\) task-irrelevant concepts denoted by \(\nu_{k_{2}}\), \(\forall k_{2}\in[K_{2}]\). The word samples \(\bm{x}\in\mathbb{R}^{d_{\mathcal{X}}}\) and their labels \(\bm{y}\in\mathbb{R}^{d_{\mathcal{Y}}}\) are generated from distributions parameterized by a shared latent concept variable \(\bm{z}=(z_{1},\cdots,z_{K})\in\{0,1\}^{K}(K<d_{\mathcal{X}})\) capturing the concept-specific information:_

\[\bm{z}\sim\mathcal{D}_{\bm{z}},\quad\xi_{\bm{z}}\sim\mathcal{D}_{ \xi_{\bm{z}}}=\mathcal{N}(\bm{0},\sigma_{\bm{\xi}}^{2}\mathbf{I}_{d_{\mathcal{ X}}}),\quad\xi_{\bm{y}}\sim\mathcal{D}_{\xi_{\bm{y}}}=\mathcal{N}(\bm{0},\sigma_{ \bm{\xi}}^{2}\mathbf{I}_{d_{\mathcal{Y}}}),\] \[\bm{x}=\mathbf{M}\bm{z}+\xi_{\bm{x}}\sim\mathcal{D}_{\bm{x}}, \quad\bm{y}=\mathbf{Q}\bm{z}+\xi_{\bm{y}}\sim\mathcal{D}_{\bm{y}},\]

_where the feature dictionary \(\mathbf{M}=[\bm{\mu}_{1}^{+},\bm{\mu}_{1}^{-},\bm{\mu}_{2}^{+},\bm{\mu}_{2}^{- },\cdots,\bm{\mu}_{K_{1}}^{+},\bm{\mu}_{K_{1}}^{-},\bm{\nu}_{1},\bm{\nu}_{2}, \cdots,\bm{\nu}_{K_{2}}]\in\mathbb{R}^{d_{\mathcal{X}}\times K}\) exhibits positive inner products within concepts and orthogonality across concepts, and the label dictionary \(\mathbf{Q}=[\bm{q}_{1}^{+},\bm{q}_{1}^{-},\bm{q}_{2}^{+},\bm{q}_{2}^{-}, \cdots,\bm{q}_{K_{1}}^{-},\bm{q}_{K_{1}}^{-},0,\cdots 0]\in\mathbb{R}^{d_{ \mathcal{Y}}\times K}\) has similar geometric properties. Specifically, we have \(\forall k_{1}\in[K_{1}],k_{2}\in[K_{2}],\|\bm{\mu}_{k_{1}}^{\pm}\|=\|\bm{\nu}_{ k_{2}}\|=\|\mathbf{u}\|,\|\bm{q}_{k_{1}}^{\pm}\|=\|\mathbf{q}\|\), and there exist constants \(0<\kappa_{\bm{x}},\kappa_{\bm{y}}<1\) such that \(0<\langle\bm{\mu}_{k_{1}}^{+},\bm{\mu}_{k_{1}}^{-}\rangle\leq\kappa_{\bm{x}}\| \mathbf{u}\|^{2}\) and \(0<\langle\bm{q}_{k_{1}}^{+},\bm{q}_{k_{1}}^{-}\rangle\leq\kappa_{\bm{y}}\| \mathbf{q}\|^{2}\)._

The detailed formal definition can be found in Appendix E. By this definition, a single word or label can possess different features corresponds to different concepts. The illustration of Figure 1 in [12] can be an example, where the "Dog" vector in the representation space of LLM is decomposed to a direct sum of orthogonal vectors: "[Animal] + [Mammal] + \(\cdots\)", and we can see "[Animal]" belongs to the concept "Organism's Category" categorized into labels "[Animal]" and "[Plant]", and "[Mammal]" belongs to the concept of "Animal's Category" characterized by labels "[Mammal]", "[Fish]", "[Bird]", "[Reptile]". Besides, Figure 1 in [46] can also be a good support for our modeling, where "Ferrari" vector consists of "[Cars] + [Italian] + \(\cdots\)".

The following definition models the contextual prompts via specifying the statistical property of \(\bm{z}\) among in-context words, which is a special prompt version of PLSA [49] and LDA [6]. The detailed formal version is available in Appendix E.

**Definition 2**.: _Concept-specific Contextual Prompt Distribution2. During training, each prompt sample \(S=\bm{x}_{1},\bm{y}_{1},\cdots,\bm{x}_{L},\bm{y}_{L},\bm{x}_{L+1}\) would share at least one co-concept, which is drawn from a mixture distribution \(\mathcal{D}_{S}\) defined as:_

Footnote 2: Our theory allows for a broader range of the probability settings stated in the training prompt distribution, but for the sake of simplicity in presentation, we here chose a feasible one.

\[\mathcal{D}_{S}=\sum_{k=1}^{K_{1}}\left(\pi_{k}^{+}\mathcal{P}_{k,L+1}^{+}+\pi_ {k}^{-}\mathcal{P}_{k,L+1}^{-}\right),\] (1)

_where \(\mathcal{P}_{k,L+1}^{\pm}\) denotes the \(k\)-th concept-specific prompt distribution, and \(\pi_{k}^{\pm}=\left(2K_{1}\right)^{-1}\) denotes the equal chance of a sample to belong to \(\mathcal{P}_{k,L+1}^{\pm}\). Specifically, a sample \(S_{n}\sim\mathcal{P}_{k,L+1}^{e},e\in[\pm]\) means that the query's label \(\bm{y}_{L+1}^{n}\) is \(\bm{q}_{k}^{e}\), and we denote \(y_{S_{n}}\coloneqq e\) as the real value label of this prompt. In addition, every demonstration pairs \((\bm{x}_{1}^{n},\bm{y}_{1}^{n}),l\in[L]\) in \(\mathcal{P}_{k,L+1}^{e}\) contain either \((\bm{\mu}_{k}^{+},\bm{q}_{k}^{+})\) or \((\bm{\mu}_{k}^{-},\bm{q}_{k}^{-})\) with equal chance. Also, every \(\bm{z}_{i}^{n},l\in[L+1]\) would satisfy \(\mathbb{P}(\mathbb{z}_{l,\neg\neg\neg\neg 2k-1\lor 2k}^{n})=1)=K^{-1}\), denoting the equal chance to have diverse features other than the current co-concept of the \(\mathcal{P}_{k,L+1}^{e}\)._

This definition suggests that for prompt \(S\) sampling from \(\mathcal{D}_{S}\), there exists \(e\in[\pm]\), \(k\in[K_{1}]\), such that all the word-label pairs in this prompt share the \(k\)-th concept as their co-concept, and the corresponding real value label of the query in this prompt is \(e\). Besides, the real value label of each word-label pair in the demonstration would have equal chance to be \(+1\) or \(-1\).

### Transformer Model

Following [17, 20, 28], our embedding \(\mathbf{E}(\cdot)\) of prompt \(S\) is formulated as \(\mathbf{H}\):

\[\mathbf{H}=\mathbf{E}(S)=\left(\begin{array}{ccc}\bm{x}_{1}&\bm{x}_{2}&\cdots& \bm{x}_{L}&\bm{x_{\text{query}}}\\ \bm{y}_{1}&\bm{y}_{2}&\cdots&\bm{y}_{L}&\bm{0}\end{array}\right)\coloneqq( \mathbf{h}_{1},\mathbf{h}_{2},\cdots,\mathbf{h}_{\text{query}})\in\mathbb{R}^{(d_{ \mathcal{X}}+d_{\mathcal{Y}})\times(L+1)},\]

The learning model is a single-head, one-layer Transformer with one self-attention layer and one two-layer perceptron. Mathematically, it can be expressed as follows:

\[f(\mathbf{H};\Psi)=\mathbf{r}^{\top}\sigma_{R}\left(\mathbf{W}_{O} \operatorname{attn}(\mathbf{H};\Psi)\right),\] \[\operatorname{attn}(\mathbf{H};\Psi)=\sum_{l=1}^{L}\mathbf{W}_{V }\mathbf{h}_{l}\sigma_{S}\left(\left(\mathbf{W}_{K}\mathbf{h}_{l}\right)^{ \top}\mathbf{W}_{Q}\mathbf{h}_{\text{query}}\right),\]where \(\sigma_{R}(\cdot)\coloneqq\mathrm{Relu}(\cdot),\sigma_{S}(\cdot)\coloneqq \mathrm{softmax}(\cdot),\mathbf{W}_{Q},\mathbf{W}_{K}\in\mathbb{R}^{m_{\mathrm{s} \times(d_{X}+d_{Y})}},\mathbf{W}_{V}\in\mathbb{R}^{m_{\mathrm{v}}\times(d_{X}+d _{Y})}\) are the embedding matrices for queries, keys, and values, respectively, and \(\mathbf{W}_{O}\in\mathbb{R}^{m\times m_{v}}\) and \(\mathbf{r}\in\mathbb{R}^{m}\) are parameters in the MLP layer. Typically, \(\min\left(m_{qk},m_{v}\right)\geq d_{\mathcal{X}}+d_{\mathcal{Y}}\). \(\Psi\coloneqq\left\{\mathbf{W}_{Q},\mathbf{W}_{K},\mathbf{W}_{V},\mathbf{W}_{ O},\mathbf{r}\right\}\) denotes the set of all model weights.

**Training Setting**. We fix one layer in both the attention and MLP layers to scrutinize the training dynamics more rigorously. Specifically, we let

\[\mathbf{W}_{Q}=\left(\begin{array}{cc}\mathbf{W}_{Q}^{\bm{x}}&*\\ *&*\end{array}\right),\quad\mathbf{W}_{K}=\left(\begin{array}{cc}\mathbf{W}_ {K}^{\bm{x}}&*\\ *&*\end{array}\right),\quad\mathbf{W}_{V}=\left(\begin{array}{cc}*&*\\ *&\mathbf{W}_{V}^{\bm{y}}\end{array}\right)\quad\mathbf{W}_{O}=\left(*&\mathbf{ W}_{O}^{\bm{y}}\right),\]

where \(\mathbf{W}_{Q}^{\bm{x}},\mathbf{W}_{K}^{\bm{x}}\in\mathbb{R}^{d_{X}\times d_{ X}},\mathbf{W}_{V}^{\bm{y}}\in\mathbb{R}^{(m_{v}-d_{X})\times d_{\mathcal{Y}}}\), \(\mathbf{W}_{O}^{\bm{y}}\in\mathbb{R}^{m\times d_{Y}}\). Here, we set the elements other than \(\mathbf{W}_{Q}^{\bm{x}}\), \(\mathbf{W}_{K}^{\bm{x}}\), \(\mathbf{W}_{V}^{\bm{y}}\) and \(\mathbf{W}_{O}^{\bm{y}}\) to be zero. Besides, we fix \(\mathbf{W}_{V}^{\bm{y}}\) to be \(\mathbf{I}_{(m_{v}-d_{X})\times d_{\mathcal{Y}}}\). We sample \(\mathbf{r}_{i}\) from a uniform distribution \(\mathrm{Unif}\{-1,1\}\) and fixed during the training process. Based on this setting, the trainable part we need to consider is actually \(\Psi^{\prime}\coloneqq\left\{\mathbf{W}_{Q}^{\bm{x}},\mathbf{W}_{K}^{\bm{x}}, \mathbf{W}_{O}^{\bm{y}}\right\}\). This problem remains highly non-convex and challenging.

We utilize mini-batch with-replacement SGD to train the transformer model. The empirical cross-entropy loss for each batch \(\mathcal{B}_{t}\) is written as

\[L_{\mathcal{B}_{t}}(\Psi)=L_{\mathcal{B}_{t}}(\Psi^{\prime}) \coloneqq\frac{1}{B}\sum_{n\in\mathcal{B}_{t}}\ell\left(y_{S_{n}}\cdot f( \mathbf{H};\Psi)\right)+\frac{\lambda}{2}\|\Psi^{\prime}\|_{F}^{2},\]

where \(\ell(z)=\log(1+\exp(-z))\), \(y_{S_{n}}\) is the real value label of the prompt defined in Definition 2, and the term \(\|\Psi^{\prime}\|_{F}^{2}\) represents \(\|\mathbf{W}_{Q}^{\bm{x}}\|_{F}^{2}+\|\mathbf{W}_{K}^{\bm{x}}\|_{F}^{2}+\| \mathbf{W}_{O}^{\bm{y}}\|_{F}^{2}\), which is the \(L_{2}\) regularization term with \(\|\cdot\|_{F}\) denoted as the Frobenius norm. The purpose of the regularization in this paper is to accelerate and stabilize the mini-batch with-replacement SGD. The learning step is set to be \(\eta_{\epsilon}=\frac{2}{\lambda(\gamma+t)}\), where \(\gamma\) is an offset parameter. This decaying schedule is standard and also used in prior work [34; 50; 51] studying convergence of SGD. The whole procedure is in Algorithm 1.

**Initialization Setting.** All initial values of \(\mathbf{W}_{O}^{\bm{y}}\) are sampled from a i.i.d. Gaussian distributions with mean 0 and variance \(\sigma_{1}^{2}\). The initialization of \(\mathbf{W}_{Q}^{\bm{x}}\) and \(\mathbf{W}_{K}^{\bm{x}}\) are diagonal matrices \(\sigma_{0}\mathbb{I}\), which are also adopted in other work that consider training \(\mathbf{W}_{Q}\) and \(\mathbf{W}_{K}\) separately [25; 28].

**Testing Setting**. The model performance is measured by 0-1 test error on a test prompt distribution \(\mathcal{D}^{*}\):

\[L_{\mathcal{D}^{*}}^{0-1}(\Psi):=\mathbb{P}_{S\sim\mathcal{D}^{*}}[ (y_{S}\cdot f(\mathbf{E}(S);\Psi))<0].\] (2)

``` Input: Training distribution \(\mathcal{D}_{S}\), Test distribution \(\mathcal{D}^{*}\), Batch size \(B\), step size \(\eta_{t}=\frac{2}{\lambda(\gamma+t)}\), stopping criterion \(\varepsilon\) and total epochs \(T\).  Initialize model parameters \({\Psi^{\prime}}^{(0)}\). for\(t=0,1,\dots,T-1\)do  If \(L_{\mathcal{D}^{*}}^{0-1}(\Psi^{(t)})\leq\varepsilon\) stop else continue.  Randomly sample mini batches \(\mathcal{B}_{t}\) of size \(B\) from \(\mathcal{D}_{S}\).  Update model parameters: \({\Psi^{\prime}}^{(t+1)}={\Psi^{\prime}}^{(t)}-\eta_{t}\nabla_{\Psi^{\prime}}L_{ \mathcal{B}_{t}}({\Psi^{\prime}}^{(t)})\). endfor ```

**Algorithm 1** Training algorithm

## 4 Theoretical Results

In this section, we present our main theoretical results, which is based on the following conditions. We consider the learning iterations \(0\leq t\leq T^{*}\), where \(T^{*}=\Omega(m^{-1}\sigma_{0}^{-1}\sigma_{1}^{-1}m\lambda^{-2}K_{1}\|\mathbf{q} \|^{2}((L-1)\|\mathbf{u}\|^{2}+1)\log(\varepsilon^{-1}))\) denotes the maximum admissible iteration.

**Condition 1**.: _Suppose that there exists a sufficiently large constant \(C\), such that the following hold:_

1. \(d_{\mathcal{X}},d_{\mathcal{Y}}\geq\max\{C\log(KLBT^{*}/\delta),K\}\)_,_ \(d_{\mathcal{Y}}\geq C\log(m/\delta)\)_,_ \(m\geq C\log(K/\delta)\)_._2. \(\gamma\geq C\max\{\|\mathbf{q}\|^{2}/(mK_{1}\lambda),10/\lambda\}\), \(\lambda\leq\min\{(C\log(Km/\delta)\|\mathbf{q}\|)^{-1},(C\sigma_{0}/2\|\mathbf{u} \|^{2})^{-1}\}\)
3. \(K\geq\{CK_{1},C\|\mathbf{u}\|/(\sigma_{\xi}\sqrt{d_{\mathcal{X}}})\}\).
4. \(\sigma_{\xi}\leq\min\{\lambda m/(C\sqrt{d_{\mathcal{X}}}\|\mathbf{u}\|^{1/2}), \|\mathbf{q}\|/(C\sqrt{d_{\mathcal{Y}}})\}\).
5. \(\sigma_{0}\leq\sqrt{K^{-1}\log(\frac{\|\mathbf{u}\|^{2}}{\lambda K_{1}^{2}} \log(\frac{\|\mathbf{q}\|^{2}}{m\lambda K_{1}}))/(C\|\mathbf{u}\|)}\), \(\sigma_{1}\leq\min\{(C\sigma_{0}\|\mathbf{u}\|^{4}\|\mathbf{q}\|\sqrt{\log(5 Km/\delta)}/K_{1})^{-1},{w^{*}}^{2}/(Cm^{3/2}\|\mathbf{q}\|)\}\).

_Here, \(w^{*}=\frac{1-e^{-\sigma_{0}^{2}(1-\kappa_{\bm{\omega}})^{2}\|\mathbf{u}\|^{ 4}/2}}{1+e^{-\sigma_{0}^{2}(1-\kappa_{\bm{\omega}})^{2}\|\mathbf{u}\|^{4}/2}}\)._

Note that we do not have any requirement upon demonstration length \(L\) and batch size \(B\) for training, thus the training can be really flexible compared with the strict requirement in [28]. The condition on dimensionality \(d_{\mathcal{X}},d_{\mathcal{Y}}\) and the network width \(m\) ensure the learning problem is in a sufficiently overparameterized setting [41, 42, 52, 43]. The condition on \(\gamma\) ensures the learning step to be small and thus learning process enjoys an approximation to gradient flow. The condition on the small \(\lambda\) is to ensure the model's sufficient learning before being stuck by regularization [53]. The condition on \(K\) is to control the impact of cross-concept contribution in the Attention's learning dynamic, which can actually be relaxed at the cost of a denser analysis. The condition on \(\sigma_{\xi}\) is to ensure that the gradient flows be mildly influenced by the noise. Last but not least, the conditions on \(\sigma_{1}\) guarantee that the initial beliefs of MLP is small and the gradients of SGD can update the model effectively. A more detailed discussion over the parameter settings is delayed to Appendix H.

**Theorem 2**.: _Exponential Convergence of 0-1 Loss. Under Condition 1, define_

\[\nu\coloneqq\min\{2\sqrt{2}\sigma_{1}/(1+\kappa_{\bm{y}}),\sigma_{0}(1-\kappa _{\bm{x}})e^{-\log(5Km/\delta)\frac{\sigma_{1}^{2}\|\mathbf{u}\|^{4}(1+\kappa _{\bm{y}}-\sigma_{0}^{2}\|\mathbf{u}\|^{2})}{(1-\kappa_{\bm{y}}-\sigma_{0}^{2} \|\mathbf{u}\|^{2})}}\}.\]

_Then, for \(\forall\varepsilon>0\) there exist some positive constants \(C_{1}\) and \(C_{2}\), with probability no less than \(1-\delta\), for \(T\geq\hat{T}=C_{1}\sigma_{1}m\lambda K_{1}\gamma\sqrt{(1+\kappa_{\bm{y}})\log(5 Km/\delta)}/{w^{*}}^{2}(1-\kappa_{\bm{y}})\|\mathbf{q}\|\), we have_

\[L_{\mathcal{D}^{*}}^{0-1}(\Psi^{(T)})\leq\exp(-\frac{C_{2}\nu^{2}m\lambda^{2} (\gamma+T)}{K_{1}\|\mathbf{q}\|^{2}((L-1)\|\mathbf{u}\|^{2}+1)}).\]

_Thus after_

\[T_{\varepsilon}=\frac{K_{1}\|\mathbf{q}\|^{2}((L-1)\|\mathbf{u}\|^{2}+1)}{C_ {2}\nu^{2}m\lambda^{2}}\log(\frac{1}{\varepsilon})\]

_iterations, we have \(L_{\mathcal{D}^{*}}^{0-1}(\Psi^{(T)})\leq\varepsilon\)._

Note that the bound is valid only when \(T\geq\hat{T}\), a common threshold in prior convergence rate analyses [34, 33, 36]. Importantly, the existence of \(\hat{T}\) does not affect the convergence rate as \(\varepsilon\to 0\), since \(\hat{T}\) is independent of \(\varepsilon\). Our novel analysis generalizes these prior results to our realistic settings handling the challenges of self-attention, ReLU-MLP, and cross-entropy loss simultaneously. By considering extreme cases, our techniques relax the batch size requirement, enabling more general results. Consequently, the sample complexity for Bayes-optimal test error is \(N=T_{\varepsilon}\).

Before introducing the next proposition, we highlight a key observation from the semantic geometry in Definition 1. For any \(k_{1}\in[K_{1}]\), defining \(\bm{a}_{k_{1}}\coloneqq(\bm{\mu}_{k_{1}}^{+}+\bm{\mu}_{k_{1}}^{-})/2\) and \(\bm{b}_{k_{1}}\coloneqq(\bm{\mu}_{k_{1}}^{+}-\bm{\mu}_{k_{1}}^{-})/2\), we find that for \(k_{1}^{\prime}\neq k_{1}\), \(\{\bm{a}_{k_{1}},\bm{b}_{k_{1}}\}\perp\{\bm{a}_{k_{1}^{\prime}},\bm{b}_{k_{1}^{ \prime}}\}\) and \(\langle\bm{a}_{k_{1}},\bm{b}_{k_{1}}\rangle=0\). This structure is exemplified in Figure 1(b) of [12], where "[Bird]" consists of orthogonal steering vectors: "plant \(\Rightarrow\) animal" and "mammmal \(\Rightarrow\) bird," corresponding to the concept feature \(\bm{a}_{k}\) and semantic label features \(\bm{b}_{k}\). Here, the term \(e\bm{b}_{k_{1}}\) in \(\bm{\mu}_{k_{1}}^{*}\) determines the label assignment. Similarly, defining \(\bm{c}_{k_{1}}\coloneqq(\bm{q}_{k_{1}}^{+}+\bm{q}_{k_{1}}^{-})/2\) and \(\bm{d}_{k_{1}}\coloneqq(\bm{q}_{k_{1}}^{+}-\bm{q}_{k_{1}}^{-})/2\) yields analogous properties. Detailed definitions are provided in Appendix I. The following proposition explores the model's ability to handle OOD unseen ICL tasks.

**Proposition 1**.: _Out-of-Distribution-Generalization3. During testing, the learned model admits probability distribution shift on \(\mathcal{D}^{*}_{\bm{z}}\) and data shift on \(\mathcal{D}^{*}_{\bm{x}}\times\mathcal{D}^{*}_{\bm{y}}\) to generate a new prompt distribution \(\mathcal{D}^{*}_{S}=\sum_{k=1}^{K_{1}}\left({\pi_{k}^{+*}}^{\mathcal{P}}\!\!+_{L,L^ {*}+1}^{\mathcal{P}}\!\!+_{1}^{\mathcal{P}}\!\!+_{1}^{\mathcal{P}}\!\!-_{k,L^{ *}+1}^{\mathcal{P}}\!\!\!\right)\). Specifically, the new \(\mathcal{D}^{*}_{S}\) satisfies the following properties._

* _The prompt length_ \(L^{*}\) _can be any positive integer._
* \(\mathcal{D}^{*}_{\bm{z}}\) _can enjoy arbitrary distribution, satisfying that each prompt has at least one co-concept_ \(k\in[K_{1}]\)_, at least one pair shares the query word's co-concept's label, and still each word has equal chance to have positive or negative semantic labels over its concepts_4_. Footnote 4: The requirement of \(\mathcal{D}^{*}_{\bm{z}}\) could be relax with a stricter requirement on \(L^{*}\) and a denser analyses.
* \(\mathcal{D}^{*}_{\bm{x}}\times\mathcal{D}^{*}_{\bm{y}}\) _can enjoy a great family of data shift._ \(\forall k\neq k^{\prime}\in[K_{1}],k_{2}\in[K_{2}]\)_, we can have new_ \(\mathbf{M}^{*}\) _and_ \(\mathbf{Q}^{*}\) _such that_ \(\bm{\mu}_{k}^{+*}=\bm{a}_{k}^{*}\bm{b}_{k}^{*}\)_,_ \(\bm{q}_{k}^{+*}=\bm{c}_{k}^{*}\bm{d}_{k}^{*}\)_,_ \(\bm{\nu}_{k_{2}}=\bm{\nu}_{k_{2}}^{*}\)_. Here,_ \(\bm{a}_{k}^{*},\bm{b}_{k}^{*},\bm{c}_{k}^{*},\bm{d}_{k}^{*}\) _are any vectors belong to the conic hulls of_ \(\{\bm{a}_{k}\}_{k=1}^{K_{1}},\{\bm{b}_{k}\}_{k=1}^{K_{1}},\{\bm{c}_{k}\}_{k=1}^ {K_{1}},\{\bm{d}_{k}\}_{k=1}^{K_{1}}\) _respectively, satisfying_ \(\|\bm{b}_{k}^{*}\|\geq\|\bm{a}_{k}^{*}\|=\Theta(\|\mathbf{u}\|)\) _and_ \(\|\bm{d}_{k}^{*}\|\geq\|\bm{c}_{k}^{*}\|=\Theta(\|\mathbf{u}\|)\)_._ \(\bm{\nu}_{k_{2}}^{*}=\Theta(\|\mathbf{u}\|)\) _are any vectors from the complement space of_ \(\text{span}(\mathbf{M})\)_._

_Again, the learned model satisfies \(L_{\mathcal{D}^{*}_{S}}^{0-1}(\Psi^{(T^{*})})\leq\varepsilon\)._

This proposition demonstrates the strong Out-of-Distribution Generalization ability of transformer utilizing multi-concept semantics, suggesting the efficiency transformer to conduct unseen ICL tasks just by its learned "Knowledge" on the high-level concept and low-level label semantic information from the two non-orthogonal dictionaries. The admit of shift for \(\mathcal{D}^{*}_{\bm{z}}\) denotes that each prompt can enjoy multi-co-concepts and each word-label pair can appear in at least \(\|\bm{z}\|_{0}\) concept-specific prompts/tasks' distribution, which aligns the real-world cases. On the other hand, we also believe the admit of shift for \(\mathcal{D}^{*}_{\bm{z}}\times\mathcal{D}^{*}_{\bm{y}}\) is inspiring, suggesting that transformer can conduct specific cross-concept semantic "Knowledge Intersection". As such, this lemma suggest that the transformer can master the regularity of unseen ICL tasks' "structure" in the presence the multi-concept encoded representation.

**Remark 1**.: _Comparison with Related Work. Theorem 3.4 in [28] and Theorem 2 in [54] address the transformer's OOD capability in specific structured ICL classification and regression tasks. Our results differ by focusing on compositional generalization of learned concepts, grounded in the concept-specific linear latent geometry observed in LLMs._

## 5 Proof Idea

In a big picture, we simply extend standard expectation-variance reduction techniques [34] to our setting. Section 5.1 defines coefficients to examine NN's expected projection along feature directions. Section 5.2 provides the convergence of the expected estimator through the lens of coefficient evolution; Section 5.3 showcase the exponential convergence by treating the conditional expectations of the NNs as Doob martingales and exploiting the property of the tails under low-noise conditions.

### Idempotent Operator Techniques

**Idempotent Operator Trick**. Define \(\mathbb{U}\coloneqq\text{span}(\mathbf{M})\) and its complement space \(\mathbb{U}^{\perp}\). By definition, we know that \(\text{dim}(\mathbb{U})=K\) and \(\text{dim}(\mathbb{U}^{\perp})=d_{\mathcal{X}}-K\). Then we can let \(\{\{\bm{a}_{k_{1}}\}_{k_{1}=1}^{K_{1}},\{\bm{b}_{k_{1}}\}_{k_{1}=1}^{K_{1}}, \{\bm{\nu}_{k_{2}}\}_{k_{2}=1}^{K_{2}},\{\bm{u}_{w}\}_{w=1}^{d_{\mathcal{X}}- K}\}\) be the set of standard orthogonal basis for \(\mathbb{R}^{d_{\mathcal{X}}}\), where \(\bm{u}_{1}^{\perp},\cdots,\bm{u}_{d_{\mathcal{X}}-K}^{\perp}\) are the standard orthogonal basis of \(\mathbb{U}^{\perp}\).

Then we can derive an idempotent decomposition of the identity matrix

\[\sum_{s=1}^{K_{1}}\frac{\bm{a}_{s}\bm{a}_{s}^{\top}}{\|\bm{a}_{s}\|^{2}}+\sum_{ s=1}^{K_{1}}\frac{\bm{b}_{s}\bm{b}_{s}^{\top}}{\|\bm{b}_{s}\|^{2}}+\sum_{r=1}^{K_{2}} \frac{\bm{\nu}_{r}\bm{\nu}_{r}^{\top}}{\|\mathbf{u}\|^{2}}+\sum_{w=1}^{d_{ \mathcal{X}}-K}\bm{u}_{w}^{\perp}\bm{u}_{w}^{\perp}\bm{u}_{w}^{\perp}=\mathbf{I} _{d_{\mathcal{X}}\times d_{\mathcal{X}}}.\] (3)

Similar techniques are also applied to the label's dictionary: \(\mathbb{Q}\coloneqq\text{span}(\mathbf{Q})\), where we define \(\bm{q}_{1}^{\perp},\cdots,\bm{q}_{d_{\mathcal{X}}-K_{1}}^{\perp}\) as the standard orthogonal basis of the complement space \(\mathbb{Q}^{\perp}\). In our subsequent derivation, the expectation \(\mathbb{E}[\cdot]\) is taken over the stochastic gradient descent. Similar to the idea in [34, 33, 36], we first serve to see how \(\mathbb{E}(\Psi^{(t)})\) evolves. For \(\mathbb{E}(\Psi^{(t)})\), every gradient descent update by all concept's samples within a soft "weight", and thus the analysis is equivalent to gradient descent with an ideally-balanced prompt set. Leveraging the symmetry of the prompt distribution, as well as the symmetry of \(\mathbf{W}_{Q}^{(0)}\) and \(\mathbf{W}_{K}^{(0)}\), we introduce the following decompositions.

**Lemma 1**.: _We can decompose \(\mathbb{E}[\mathbf{W}_{Q}^{\bm{\pi}}]\), \(\mathbb{E}[\mathbf{W}_{K}^{\bm{\pi}}]\) and the \(i\)-th row of \(\mathbb{E}[\mathbf{W}_{Q}^{\bm{\eta}}]\) (\(i\in[m]\)) via the following (scaled) projection matrices and projection directions._

\[\mathbb{E}[\mathbf{W}_{Q}^{\bm{\pi}}{}^{(t)}] =\sum_{s=1}^{K_{1}}\alpha_{Q_{s},s}^{(t)}\cdot\frac{\bm{a}_{s}\bm {a}_{s}}{\|\bm{a}_{s}\|^{4}}+\sum_{s=1}^{K_{2}}\beta_{Q_{s},s}^{(t)}\cdot\frac{ \bm{b}_{s}\bm{b}_{s}}{\|\bm{b}_{s}\|^{4}}+\sum_{r=1}^{K_{2}}\tau_{Q_{r},r}^{(t) }\cdot\frac{\bm{\nu}_{r}\bm{\nu}_{r}\bm{\nu}_{r}}{\|\mathbf{u}\|^{4}}+\sum_{w =1}^{d_{X}-K}\rho_{Q,w}^{(t)}\cdot\bm{u}_{w}^{\perp}\bm{u}_{w}^{\perp\top},\] \[\mathbb{E}[\mathbf{W}_{K}^{\bm{\pi}}{}^{(t)}] =\sum_{s=1}^{K_{1}}\alpha_{K_{s},s}^{(t)}\cdot\frac{\bm{a}_{s}\bm {a}_{s}}{\|\bm{a}_{s}\|^{4}}+\sum_{s=1}^{K_{1}}\beta_{K_{s},s}^{(t)}\cdot\frac{ \bm{b}_{s}\bm{b}_{s}}{\|\bm{b}_{s}\|^{4}}+\sum_{r=1}^{K_{2}}\tau_{K_{r},r}^{(t )}\cdot\frac{\bm{\nu}_{r}\bm{\nu}_{r}}{\|\mathbf{u}\|^{4}}+\sum_{w=1}^{d_{X}-K }\rho_{K,w}^{(t)}\cdot\bm{u}_{w}^{\perp}\bm{u}_{w}^{\perp\top},\] \[\mathbb{E}[\mathbf{W}_{O_{(i.,.)}}^{\quad(t)}] =\sum_{k=1}^{K_{1}}\alpha_{O_{(i.,.)},k}^{(t)}\cdot\frac{\bm{c}_{k}\bm{ \times}^{\top}}{\|\bm{c}_{k}\|^{2}}+\sum_{k=1}^{K_{1}}\beta_{O_{(i.,.)},k}^{(t )}\cdot\frac{\bm{d}_{k}\bm{\times}^{\top}}{\|\bm{d}_{k}\|^{2}}+\sum_{w=1}^{d_{ y}-K_{1}}\rho_{O_{(i.,.)},w}^{(t)}\cdot\bm{q}_{w}^{\perp\top}.\]

Here \(\alpha_{Q,s}^{(t)}\), \(\alpha_{K,s}^{(t)}\) and \(\alpha_{O_{(i.,.)},k}^{(t)}\) represent the expected concept learning process, \(\beta_{Q,s}^{(t)}\), \(\beta_{K,s}^{(t)}\) and \(\beta_{O_{(i.,.)},k}^{(t)}\) represent the expected concept-specific semantic learning process and \(\tau_{Q_{r},r}^{(t)},\tau_{K_{r},r}^{(t)},\rho_{Q,w}^{(t)},\rho_{K,w}^{(t)}\) and \(\rho_{O_{(i.,.)},w}^{(t)}\) represent the expected memorization of the concept irrelevant noise. It holds that

\[\mathbb{E}[(\mathbf{W}_{K}^{\bm{\pi}}{}^{(t)}\bm{\mu}_{s}^{\pm})] ^{\top}\mathbb{E}[\mathbf{W}_{Q}^{\bm{\pi}}{}^{(t)}\bm{\mu}_{s}^{\bm{\pi}}]= \alpha_{Q_{s},s}^{(t)}\cdot\alpha_{K,s}^{(t)}/\|\bm{a}_{s}\|^{2}\pm\beta_{Q_{s},s}^{(t)}\cdot\beta_{K,s}^{(t)}/\|\bm{b}_{s}\|^{2},\] (4) \[\mathbb{E}[\mathbf{W}_{O_{(i.,.)}}^{\quad(t)}\bm{q}_{k}^{\bm{\pi} }]=\alpha_{O_{(i.,.)},k}^{(t)}+e\cdot\beta_{O_{(i.,.)},k}^{(t)},\]

for \(\forall e\in[\pm],i\in[m],k\in[K_{1}]\) and for \(\forall e^{\prime}\in[\pm],s^{\prime}\in[K_{1}],r\in[K_{2}],w\in[d_{\mathcal{X }}-K]\), \(\forall\mathbf{u}\in\{\bm{\mu}_{s^{\prime}}^{\varepsilon_{r}},\bm{\nu}_{r},\bm {u}_{w}^{\perp}\}\), it holds that \(\mathbb{E}[(\mathbf{W}_{K}^{\bm{\pi}}{}^{(t)}\mathbf{u})]^{\top}\mathbb{E}[ \mathbf{W}_{Q}^{\bm{\pi}}{}^{(t)}\bm{\mu}_{s}^{\varepsilon_{s}}]=0\). Similar conclusions hold when the query vectors are \(\bm{\nu}_{r}\) and \(\bm{u}_{w}^{\perp}\), \(\forall r\in[K_{2}],w\in[d_{\mathcal{X}}-K]\). As such, our remaining task is to scrutinize the coefficients evolution, which would be the key contributors to the expected 0-1 loss convergence.

### Convergence of the Expectation

Denote \(\mathcal{U}_{k,n}^{y_{S_{n}}}(t)\) and \(\mathcal{W}_{k,n}^{v}(t)-\mathcal{U}_{k,n}^{y_{S_{n}}}(t)\) as the activated neuron set for \(\{i\in[m]\mid\mathbf{r}_{i}y_{S_{n}}>0\}\) and \(\{i\in[m]\mid\mathbf{r}_{i}y_{S_{n}}<0\}\) separately, and \(\sum_{l\in S_{n,k}^{y_{S_{n}}}}\left(\sigma_{S}^{(t)}\right)_{l}^{n}\) represents the correct attention weight, where the detailed definitions are delayed in Appendix E. We then introduce the following lemma.

**Lemma 2**.: _Under Condition 1, when_

\[(\sum_{i\in\mathcal{U}_{k,n}^{y_{S_{n}}}(t)}-\sum_{i\in\mathcal{W}_{k,n}^{y_{S_{ n}}}(t)-\mathcal{U}_{k,n}^{y_{S_{n}}}(t)})\left(\alpha_{O_{(i.,.)},k}^{(t)}+(2\sum_{l\in S_{n,k}^{y_{S_{ n}}}}\left(\sigma_{S}^{(t)}\right)_{l}^{n}-1)y_{S_{n}}\beta_{O_{(i.,.)},k}^{(t)} \right)\geq 0,\] (5)

_holds, we have \(L_{\mathcal{D}^{\star}}^{0-1}(\mathbb{E}(\Psi^{\prime}{}^{(t)}))=0\)._

Figure 1: Illustration of our Idempotent Operator Techniques. This allows us to focus on analyzing the evolving coefficients, which are key to the expected 0-1 loss convergence.

As such, the following lemmas show the learning outcomes of the \(\mathbb{E}(\Psi^{(t)})\) along the iterations.

**Lemma 3**.: _(Convergence of the Expectation). There exist constant \(C_{1}>0\), \(\forall t\geq\hat{T}=C_{1}\sigma_{1}m\lambda K_{1}\gamma\sqrt{(1+\kappa_{\bm{y}}) \log(5Km/\delta)}/{w^{*}}^{2}(1-\kappa_{\bm{y}})\|\mathbf{q}\|\), we have \(L^{0-1}_{\mathcal{D}^{*}}(\mathbb{E}(\Psi^{(t)}))=0\)._

**Lemma 4**.: _(Regularizing the models). Under Condition 1, it holds that_

\[\alpha^{(T^{*})}_{Q,k}=\alpha^{(T^{*})}_{K,k}=O(\mathbb{E}[\alpha ^{(0)}_{Q,k}]),\quad\beta^{(T^{*})}_{Q,k}=\beta^{(T^{*})}_{K,k}=\Theta(\| \mathbf{u}\|\sqrt{\log(\frac{\|\mathbf{u}\|^{2}}{\lambda K_{1}}\log(\frac{\| \mathbf{q}\|^{2}}{m\lambda K_{1}}))}),\] \[\alpha^{(T^{*})}_{O_{(i,\cdot)},k}\leq|\beta^{(T^{*})}_{O_{(i, \cdot)},k}|=\Theta(\log(\frac{\|\mathbf{q}\|^{2}}{m\lambda K_{1}})),\mathbb{ E}[(\sum_{j\in S^{W^{\bm{x}_{n}}}_{n,k}}{(\sigma^{(T^{*})}_{S})}_{j}^{n})]= \Theta(\frac{1}{1+\frac{\lambda K_{1}}{\|\mathbf{u}\|^{2}}\log(\frac{m\lambda K _{1}}{\|\mathbf{q}\|^{2}})}).\]

In addition, our analysis provides three asymptotic properties of the coefficients evolution, which are delayed to Appendix I.1.3 and I.2 for room limitation.

### Exponential Convergence of 0-1 loss

**Proposition 2**.: \(\forall t\geq\hat{T}\)_, when \(\|{\Psi^{\prime}}^{(t)}-\mathbb{E}(\Psi^{\prime})\|_{F}\leq\nu\) holds, we have \(L^{0-1}_{\mathcal{D}^{*}}({\Psi^{\prime}}^{(t)})=0\). Here, \(\|\Psi^{\prime}\|_{F}^{2}\coloneqq\|\mathbf{W}^{\bm{x}}_{Q}\|_{F}^{2}+\| \mathbf{W}^{\bm{x}}_{K}\|_{F}^{2}+\|\mathbf{W}^{\bm{y}}_{O}\|_{F}^{2}\)._

By definition of 0-1 loss, then we only need to prove the 0-1 loss convergence by seeing the speed of \({\Psi^{\prime}}^{(t)}\) converging to \(\mathbb{E}({\Psi^{\prime}}^{(t)})\) with an error of \(\nu\) in terms of \(\|\cdot\|_{F}\).

Drawing insights from [34], we see \(\mathcal{B}_{0},\cdots,\mathcal{B}_{T-1}\) as a i.i.d. random variables following the same distribution. Then \(\forall t\in\{0,\cdots,T\}\), it holds that

\[\begin{split}& D_{Q}^{t}=\mathbb{E}[\mathbf{W}^{\bm{x}}_{Q}{(T^{ *})}_{Q}\mid\mathcal{B}_{0},\cdots,\mathcal{B}_{t}]-\mathbb{E}[\mathbf{W}^{\bm {x}}_{Q}{(T^{*})}_{Q}\mid\mathcal{B}_{0},\cdots,\mathcal{B}_{t-1}],\\ & D_{K}^{t}=\mathbb{E}[\mathbf{W}^{\bm{x}}_{K}{(T^{*})}_{1}\mid \mathcal{B}_{0},\cdots,\mathcal{B}_{t}]-\mathbb{E}[\mathbf{W}^{\bm{x}}_{K}{(T^ {*})}_{K}{(T^{*})}_{1}\mid\mathcal{B}_{0},\cdots,\mathcal{B}_{t-1}]\\ & D_{O}^{t}=\mathbb{E}[\mathbf{W}^{\bm{y}}_{O}{(T^{*})}_{1}\mid \mathcal{B}_{0},\cdots,\mathcal{B}_{t}]-\mathbb{E}[\mathbf{W}^{\bm{y}}_{O}{(T^ {*})}_{O}\mid\mathcal{B}_{0},\cdots,\mathcal{B}_{t-1}],\end{split}\] (6)

are martingale difference sequences, and for \(\forall X\in\{Q,K,O\}\) and its corresponding \(\mathbf{W}\in\{\mathbf{W}^{\bm{x}}_{Q},\mathbf{W}^{\bm{x}}_{K},\mathbf{W}^{\bm {y}}_{O}\}\), we have \(\sum_{t=0}^{T}D_{X}^{t}=\mathbf{W}^{(T+1)}-\mathbb{E}[\mathbf{W}^{(T+1)}]\). Then we utilize the following lemma in [34, 55] to give a bound over the variance.

**Lemma 5**.: _Let \(D_{1},\cdots,D_{T-1}\) be a martingale difference sequence. Suppose \(\exists c_{T}>0\) such that \(\sum_{t=0}^{T}\|D_{t}\|_{\infty}^{2}\leq c_{T}^{2}\), where \(\|\cdot\|_{\infty}\) is the essential supremum of \(\|\cdot\|_{F}\). Then for \(\forall\epsilon>0\), we have_

\[\mathbb{P}\Big{[}\sup_{s\in[T]}\|\sum_{t=0}^{s}D_{t}\|_{F}\geq\epsilon\Big{]} \leq 2\exp(-\frac{\epsilon^{2}}{2c_{T}^{2}}).\]

Therefore, we need to see if there exists a decaying positive constant \(c_{T}\) (with decaying rate \(O(1/T^{q}),q>0\)), such that \(\sum_{t=0}^{T}\|D_{X}^{t}\|_{\infty}^{2}\leq{c_{T}}^{2},\forall X\in\{Q,K,O\}\), where \(\|\cdot\|_{\infty}\) is the essential supremum of \(\|D_{X}^{t}\|_{F}\). Subsequently, by controlling the martingale sequence norm tail similarly in [34; 55], we can obtain an exponential convergence rate after \(T_{1}\).

For \(\mathbf{W}\in\{\mathbf{W}_{Q}^{\bm{\pi}},\mathbf{W}_{K}^{\bm{\pi}},\mathbf{W}_{Q }^{\bm{\eta}}\}\), to check the decaying \(c_{T}\), we adopt the techniques of [34; 33; 36] in the following manner. Let \(\mathcal{B}_{t}{}^{\prime}\) be an independent variable from \(\mathcal{B}_{0},\cdots,\mathcal{B}_{T}\) and let \(\mathbf{W}_{t}{}^{(T+1)}\) be an output of the algorithm depending on \((\mathcal{B}_{0},\cdots,\mathcal{B}_{t-1},\mathcal{B}_{t}{}^{\prime},\mathcal{ B}_{t+1},\cdots,\mathcal{B}_{T})\). Then we have

\[\|D_{X}^{t}\|_{\infty}\leq\mathbb{E}[\|\mathbf{W}^{(T+1)}-\mathbf{W}_{t}{}^{(T +1)}\|_{\infty}\mid\mathcal{B}_{0},\cdots,\mathcal{B}_{t}].\]

Therefore, one may estimate \(c_{X}^{T}{}^{2}\) by bounding \(\|\mathbf{W}^{(T+1)}-\mathbf{W}_{t}{}^{(T)}\|_{\infty}^{2}\) uniformly w.r.t. \(\mathcal{B}_{0},\cdots,\mathcal{B}_{T-1}\). Such a bound can be derived utilizing stability property of stochastic gradient descent [34; 56]. For the OOD scenario, since we require the data shift to be via conic combination, the new words and labels in each prompt will share the positive/negative real-valued label without any self-conflict. The norm requirements and constraints on \(\mathcal{D}_{\bm{\pi}}^{*}\) would ensure the Gaussian noise, concepts other than the co-concepts, and probability shifts have limited influence on the prediction compared with the considerable scale of coefficients by Lemma 4, laying the groundwork for the proof.

## 6 Experiments

In this section, we demonstrate the validity of our theoretical analysis through simulations of Algorithm 1. We use the following parameter settings in Figure 2: The parameter settings are: the length \(L=4\), the number of co-concepts \(K_{1}=2\), dictionary size \(K=104\), the number of test instances \(n_{\text{test}}=5000\), dimension \(d_{\mathcal{X}}=d_{\mathcal{Y}}=1000\), MLP width \(m=50\), feature strengths \(\|\mathbf{u}\|=\|\mathbf{q}\|=100\), \(\forall k\in[K_{1}]\), the cosine \(\langle\bm{\mu}_{k}^{+},\bm{\mu}_{k}^{-}\rangle/\|\mathbf{u}\|^{2}=\langle \bm{q}_{k}^{+},\bm{q}_{k}^{-}\rangle/\|\mathbf{q}\|^{2}=0.5\), the initialization parameters \(\sigma_{0}=0.1\), \(\sigma_{1}=0.01\), and the noise deviation \(\sigma_{\xi}=0.01\). For the optimization, we use \(\lambda=0.002\), \(B=16\), \(\gamma=10000\), and the total training epochs is \(100\). Figure 3 (a-d) uses the same training settings, but during testing, it applies different configurations: (a) \(L^{*}=5\), (b) \(L^{*}=2\), (c) a \(0.8\) fraction for the first concept and a \(0.2\) fraction for the second concepts, and (d) \(\bm{\mu}_{1}^{+*}=\bm{a}_{1}\pm\bm{b}_{2},\bm{\mu}_{2}^{\pm^{*}}=\bm{a}_{2}\pm \bm{b}_{1}\). Figure 2 validates our Theorem 2 and Lemma 4, which showcases the fast convergence rate and the evolution of coefficients. Figure 3 validates Proposition 1, where the learned model permits certain data shifts.

## 7 Conclusion

This work provides the first exponential convergence analysis of 0-1 loss for transformers with softmax attention and ReLU-MLP, trained on a non-orthogonal concept-specific prompt distribution by practical cross-entropy loss. Furthermore, the results demonstrate transformers can perform certain OOD ICL tasks by leveraging the multi-concept semantic linearity, highlighting their innovative potential. An important future direction is to extend the analysis to more complex scenarios.

Figure 3: Learning dynamic in three OOD scenarios. The training settings and plotting methods are identical to those used in Figure 2, and the testing settings are: (a-b) utilizes different prompt lengths; (c) adopts a skewed distribution over \(\bm{z}\); (d) switches the concept-specific semantic features.

Acknowledgment

We thank the anonymous reviewers for their instrumental comments. D.B. and H.W. are supported in part by the Research Grants Council of the Hong Kong Special Administration Region (Project No. CityU 11206622). W.H. is supported in part by JSPS KAKENHI (24K20848). A.N. is supported in part by National Research Foundation, Singapore and Infocomm Media Development Authority under its Trust Tech Funding Initiative, the Centre for Frontier Artificial Intelligence Research, Institute of High Performance Computing, A*Star, and the College of Computing and Data Science at Nanyang Technological University. T.S. is supported in part by JSPS KAKENHI (24K02905) and JST CREST (JPMJCR2115, JPMJCR2015).

## References

* [1] Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. Unified language model pre-training for natural language understanding and generation. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'AlchE-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* [2] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In _Advances in Neural Information Processing Systems_, volume 35, pages 24824-24837, 2022.
* [3] Fei Liu, Xialiang Tong, Mingxuan Yuan, Xi Lin, Fu Luo, Zhenkun Wang, Zhichao Lu, and Qingfu Zhang. Evolution of heuristics: Towards efficient automatic algorithm design using large language model. _arXiv preprint arXiv:2401.02051_, 2024.
* [4] Fei Liu, Yiming Yao, Ping Guo, Zhiyuan Yang, Xi Lin, Xialiang Tong, Mingxuan Yuan, Zhichao Lu, Zhenkun Wang, and Qingfu Zhang. A systematic survey on large language models for algorithm design. _arXiv preprint arXiv: 2410.14716_, 2024.
* [5] Sheng Lu, Irina Bigoulaeva, Rachneet Sachdeva, Harish Tayyar Madabushi, and Iryna Gurevych. Are emergent abilities in large language models just in-context learning? _arXiv preprint arXiv: 2309.01809_, 2023.
* [6] David Blei, Andrew Ng, and Michael Jordan. Latent dirichlet allocation. In _Advances in Neural Information Processing Systems_, 2001.
* [7] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference, 2022.
* [8] Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, and William Yang Wang. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning. In _Workshop on Efficient Systems for Foundation Models @ ICML2023_, 2023.
* [9] Yuchen Li, Yuanzhi Li, and Andrej Risteski. How do transformers learn topic structure: Towards a mechanistic understanding. In _Proceedings of the 40th International Conference on Machine Learning_, pages 19689-19729, 2023.
* [10] Yibo Jiang, Goutham Rajendran, Pradeep Ravikumar, Bryon Aragam, and Victor Veitch. On the origins of linear representations in large language models. _arXiv preprint arXiv: 2403.03867_, 2024.
* [11] Kiho Park, Yo Joong Choe, and Victor Veitch. The linear representation hypothesis and the geometry of large language models. _arXiv preprint arXiv: 2311.03658_, 2023.
* [12] Kiho Park, Yo Joong Choe, Yibo Jiang, and Victor Veitch. The geometry of categorical and hierarchical concepts in large language models. _arXiv preprint arXiv: 2406.01506_, 2024.
* [13] Yibo Jiang, Goutham Rajendran, Pradeep Ravikumar, and Bryon Aragam. Do llms dream of elephants (when told not to)? latent concept association and associative memory in transformers. _arXiv preprint arXiv: 2406.18400_, 2024.
* [14] Yufeng Zhang, Fengzhuo Zhang, Zhuoran Yang, and Zhaoran Wang. What and how does in-context learning learn? bayesian model averaging, parameterization, and generalization. _arXiv preprint arXiv: 2305.19420_, 2023.

* [15] Fabian Falck, Ziyu Wang, and Christopher C. Holmes. Are large language models bayesian? a martingale perspective on in-context learning. In _ICLR 2024 Workshop on Secure and Trustworthy Large Language Models_, 2024.
* [16] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In _Proceedings of the 40th International Conference on Machine Learning_, volume 202, pages 35151-35174. PMLR, 2023.
* [17] Ruiqi Zhang, Spencer Frei, and Peter L. Bartlett. Trained transformers learn linear models in-context. _arXiv preprint arXiv: 2306.09927_, 2023.
* [18] Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. In _Advances in Neural Information Processing Systems_, volume 36, pages 57125-57211, 2023.
* [19] Juno Kim and Taiji Suzuki. Transformers learn nonlinear features in context: Nonconvex mean-field dynamics on the attention landscape. _arXiv preprint arXiv: 2402.01258_, 2024.
* [20] Yu Huang, Yuan Cheng, and Yingbin Liang. In-context convergence of transformers. _arXiv preprint arXiv: 2310.05249_, 2023.
* [21] Yuandong Tian, Yiping Wang, Beidi Chen, and Simon S Du. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. In _Advances in Neural Information Processing Systems_, volume 36, pages 71911-71947, 2023.
* [22] Yingcong Li, Yixiao Huang, Muhammed E. Ildiz, Ankit Singh Rawat, and Samet Oymak. Mechanics of next token prediction with self-attention. In _Proceedings of The 27th International Conference on Artificial Intelligence and Statistics_, volume 238, pages 685-693, 2024.
* [23] Chenyu Zheng, Wei Huang, Rongzhen Wang, Guoqiang Wu, Jun Zhu, and Chongxuan Li. On mesa-optimization in autoregressively trained transformers: Emergence and capability. _arXiv preprint arXiv:2405.16845_, 2024.
* [24] Shokichi Takakura and Taiji Suzuki. Approximation and estimation ability of transformers for sequence-to-sequence functions with infinite dimensional input. _arXiv preprint arXiv: 2305.18699_, 2023.
* [25] Siyu Chen, Heejune Sheen, Tianhao Wang, and Zhuoran Yang. Training dynamics of multi-head softmax attention for in-context learning: Emergence, convergence, and optimality. _arXiv preprint arXiv: 2402.19442_, 2024.
* [26] Yu Huang, Zixin Wen, Yuejie Chi, and Yingbin Liang. Transformers provably learn feature-position correlations in masked image modeling. _arXiv preprint arXiv: 2403.02233_, 2024.
* [27] Hongkang Li, Meng Wang, Sijia Liu, and Pin yu Chen. A theoretical understanding of shallow vision transformers: Learning, generalization, and sample complexity. _arXiv preprint arXiv:2302.06015_, 2023.
* [28] Hongkang Li, Meng Wang, Songtao Lu, Xiaodong Cui, and Pin-Yu Chen. How do nonlinear transformers learn and generalize in in-context learning? _arXiv preprint arXiv: 2402.15607_, 2024.
* [29] Zixin Wen and Yuanzhi Li. Toward understanding the feature learning process of self-supervised contrastive learning. In _Proceedings of the 38th International Conference on Machine Learning_, volume 139, pages 11112-11122. PMLR, 2021.
* [30] Patrik Reizinger, Szilvia Ujyar, Anna Meszaros, Anna Kerekes, Wieland Brendel, and Ferenc Huszar. Position: Understanding LLMs requires more than statistical generalization. In _Proceedings of the 41st International Conference on Machine Learning_, volume 235, pages 42365-42390, 2024.
* [31] Enno Mammen and Alexandre B Tsybakov. Smooth discrimination analysis. _The Annals of Statistics_, 27(6):1808-1829, 1999.
* 2366, 2006.
* [33] Loucas Pillaud-Vivien, Alessandro Rudi, and Francis Bach. Exponential convergence of testing error for stochastic gradient methods. In _Proceedings of the 31st Conference On Learning Theory_, volume 75, pages 250-296, 2018.

* [34] Atsushi Nitanda and Taiji Suzuki. Stochastic gradient descent with exponential convergence rates of expected classification errors. In _Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics_, volume 89, pages 1417-1426, 2019.
* [35] Vivien A Cabannes, Francis Bach, and Alessandro Rudi. Fast rates for structured prediction. In _Proceedings of Thirty Fourth Conference on Learning Theory_, volume 134 of _Proceedings of Machine Learning Research_, pages 823-865. PMLR, 15-19 Aug 2021.
* [36] Shingo Yashima, Atsushi Nitanda, and Taiji Suzuki. Exponential convergence rates of classification errors on learning with sgd and random features. In _Proceedings of The 24th International Conference on Artificial Intelligence and Statistics_, volume 130, pages 1954-1962, 2021.
* [37] Kazusato Oko, Taiji Suzuki, Atsushi Nitanda, and Denny Wu. Particle stochastic dual coordinate ascent: Exponential convergent algorithm for mean field neural network optimization. In _International Conference on Learning Representations_, 2022.
* [38] Stefano Vigogna, Giacomo Meanti, Ernesto De Vito, and Lorenzo Rosasco. Multiclass learning with Margin: Exponential rates with no bias-variance trade-off. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 22260-22269. PMLR, 17-23 Jul 2022.
* [39] Vivien Cabannes and Stefano Vigogna. A case of exponential convergence rates for svm. In _Proceedings of The 26th International Conference on Artificial Intelligence and Statistics_, volume 206, pages 359-374. PMLR, 25-27 Apr 2023.
* [40] Zeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. In _The Eleventh International Conference on Learning Representations_, 2023.
* [41] Yuan Cao, Zixiang Chen, Misha Belkin, and Quanquan Gu. Benign overfitting in two-layer convolutional neural networks. In _Advances in Neural Information Processing Systems_, volume 35, pages 25237-25250, 2022.
* [42] Yiwen Kou, Zixiang Chen, Yuanzhou Chen, and Quanquan Gu. Benign overfitting in two-layer reLU convolutional neural networks. In _Proceedings of the 40th International Conference on Machine Learning_, volume 202, pages 17615-17659, 2023.
* [43] Xuran Meng, Difan Zou, and Yuan Cao. Benign overfitting in two-layer relu convolutional neural networks for XOR data. _arXiv preprint arXiv: 2310.01975_, 2023.
* [44] Zhiwei Xu, Yutong Wang, Spencer Frei, Gal Vardi, and Wei Hu. Benign overfitting and grokking in reLU networks for XOR cluster data. _arXiv preprint arXiv: 2310.02541_, 2023.
* [45] Wei Huang, Yuan Cao, Haonan Wang, Xin Cao, and Taiji Suzuki. Graph neural networks provably benefit from structural information: A feature learning perspective. _arXiv preprint arXiv: 2306.13926_, 2023.
* [46] Hiroaki Yamagiwa, Momose Oyama, and Hidetoshi Shimodaira. Discovering universal geometry in embeddings with ica. _arXiv preprint arXiv: 2305.13175_, 2023.
* [47] Zixin Wen and Yuanzhi Li. Toward understanding the feature learning process of self-supervised contrastive learning. In _Proceedings of the 38th International Conference on Machine Learning_, pages 11112-11122, 2021.
* [48] Chi Han, Jialiang Xu, Manling Li, Yi Fung, Chenkai Sun, Nan Jiang, Tarek Abdelzaher, and Heng Ji. Word embeddings are steers for language models. In _Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 16410-16430, 2024.
* [49] Thomas Hofmann. Probabilistic latent semantic indexing. In _Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval_, page 50-57, 1999.
* [50] Leon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. _SIAM Review_, 60(2):223-311, 2018.
* [51] Atsushi Nitanda and Taiji Suzuki. Optimal rates for averaged stochastic gradient descent under neural tangent kernel regime. _arXiv preprint arXiv: 2006.12297_, 2021.

* [52] Yiwen Kou, Zixiang Chen, Yuan Cao, and Quanquan Gu. How does semi-supervised learning with pseudo-labelers work? a case study. In _The Eleventh International Conference on Learning Representations_, 2023.
* [53] Difan Zou, Yuan Cao, Yuanzhi Li, and Quanquan Gu. Understanding the generalization of adam in learning neural networks with proper regularization. In _The Eleventh International Conference on Learning Representations_, 2023.
* [54] Tong Yang, Yu Huang, Yingbin Liang, and Yuejie Chi. In-context learning with representations: Contextual generalization of trained transformers. _arXiv preprint arXiv: 2408.10147_, 2024.
* [55] Iosif Pinelis. Optimum bounds for the distributions of martingales in banach spaces. _The Annals of Probability_, 22(4):1679-1706, 1994.
* [56] Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. In _Proceedings of The 33rd International Conference on Machine Learning_, volume 48, pages 1225-1234, 2016.
* [57] Difan Zou, Yuan Cao, Yuanzhi Li, and Quanquan Gu. The benefits of mixup for feature learning. In _Proceedings of the 40th International Conference on Machine Learning_, volume 202, pages 43423-43479, 2023.
* [58] Jinghui Chen, Yuan Cao, and Quanquan Gu. Benign overfitting in adversarially robust linear classification. In Robin J. Evans and Ilya Shpitser, editors, _Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence_, volume 216, pages 313-323, 2023.
* [59] Spencer Frei, Niladri S Chatterji, and Peter Bartlett. Benign overfitting without linearity: Neural network classifiers trained by gradient descent for noisy linear data. In _Proceedings of Thirty Fifth Conference on Learning Theory_, volume 178, pages 2668-2703, 2022.
* [60] Spencer Frei, Gal Vardi, Peter Bartlett, and Nathan Srebro. Benign overfitting in linear classifiers and leaky relu networks from kkt conditions for margin maximization. In _Proceedings of Thirty Sixth Conference on Learning Theory_, volume 195, pages 3173-3228, 2023.
* [61] Yiwen Kou, Zixiang Chen, and Quanquan Gu. Implicit Bias of Gradient Descent for Two-layer ReLU and Leaky ReLU Networks on Nearly-orthogonal Data. In _Advances in Neural Information Processing Systems_, volume 36, pages 30167-30221. Curran Associates, Inc., 2023.
* [62] Wei Huang, Ye Shi, Zhongyi Cai, and Taiji Suzuki. Understanding convergence and generalization in federated learning through feature learning theory. In _The Twelfth International Conference on Learning Representations_, 2024.
* [63] Dake Bu, Wei Huang, Taiji Suzuki, Ji Cheng, Qingfu Zhang, Zhiqiang Xu, and Hau-San Wong. Provably neural active learning succeeds via prioritizing perplexing samples. In _Proceedings of the 41st International Conference on Machine Learning_, volume 235, pages 4642-4695, 2024.
* [64] Yiwen Kou, Zixiang Chen, Quanquan Gu, and Sham M. Kakade. Matching the statistical query lower bound for k-sparse parity problems with stochastic gradient descent. _arXiv preprint arXiv: 2404.12376_, 2024.
* [65] Alexander Tsigler. _Benign Overfitting in Linear Regression and Classification_. PhD thesis, UC Berkeley, 2024.
* [66] Junhyung Park, Patrick Bloebaum, and Shiva Prasad Kasiviswanathan. Benign overfitting for regression with trained two-layer relu networks. _arXiv preprint arXiv: 2410.06191_, 2024.
* [67] Eshaan Nichani, Alex Damian, and Jason D. Lee. Provable Guarantees for Nonlinear Feature Learning in Three-Layer Neural Networks. In _Advances in Neural Information Processing Systems_, volume 36, pages 10828-10875, 2023.
* [68] Jason D. Lee, Kazusato Oko, Taiji Suzuki, and Denny Wu. Neural network learns low-dimensional polynomials with sgd near the information-theoretic limit. _arXiv preprint arXiv:2406.01581_, 2024.
* [69] Kazusato Oko, Yujin Song, Taiji Suzuki, and Denny Wu. Learning sum of diverse features: computational hardness and efficient gradient-based training for ridge combinations. _arXiv preprint arXiv:2406.11828_, 2024.
* [70] Yunwei Ren and Jason D. Lee. Learning orthogonal multi-index models: A fine-grained information exponent analysis. _arXiv preprint arXiv:2410.09678_, 2024.

* [71] Spencer Frei and Gal Vardi. Trained transformer classifiers generalize and exhibit benign overfitting in-context. _arXiv preprint arXiv:2410.01774_, 2024.
* [72] Wei Shen, Ruida Zhou, Jing Yang, and Cong Shen. On the training convergence of transformers for in-context classification. _arXiv preprint arXiv:2410.11778_, 2024.
* [73] Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, and Simon Shaolei Du. JoMA: Demystifying multilayer transformers via joint dynamics of MLP and attention. In _The Twelfth International Conference on Learning Representations_, 2024.
* [74] Yu Huang, Zixin Wen, Yuejie Chi, and Yingbin Liang. How transformers learn diverse attention correlations in masked vision pretraining. _arXiv preprint arXiv: 2403.02233_, 2024.
* [75] Masahiro Sakamoto and Hitomi Sato. Benign or not-benign overfitting in token selection of attention mechanism. _arXiv preprint arXiv:2409.17625_, 2024.
* [76] Roey Magen, Shuning Shang, Zhiwei Xu, Spencer Frei, Wei Hu, and Gal Vardi. Benign overfitting in single-head attention. _arXiv preprint arXiv:2410.07746_, 2024.
* [77] Eshaan Nichani, Alex Damian, and Jason D. Lee. How transformers learn causal structure with gradient descent. _arXiv preprint arXiv: 2402.14735_, 2024.
* [78] Siyu Chen, Heejune Sheen, Tianhao Wang, and Zhuoran Yang. Unveiling induction heads: Provable training dynamics and feature learning in transformers. _arXiv preprint arXiv: 2409.10559_, 2024.
* [79] Hongru Yang, Bhavya Kailkhura, Zhangyang Wang, and Yingbin Liang. Training dynamics of transformers to recognize word co-occurrence via gradient flow analysis. _arXiv preprint arXiv:2410.09605_, 2024.
* [80] Jiarui Jiang, Wei Huang, Miao Zhang, Taiji Suzuki, and Liqiang Nie. Unveil benign overfitting for transformer in vision: Training dynamics, convergence, and generalization. _arXiv preprint arXiv:2409.19345_, 2024.
* [81] Bingrui Li, Wei Huang, Andi Han, Zhanpeng Zhou, Taiji Suzuki, Jun Zhu, and Jianfei Chen. On the optimization and generalization of two-layer transformers with sign gradient descent. _arXiv preprint arXiv:2410.04870_, 2024.
* [82] Yoshua Bengio. _Learning Deep Architectures for AI_. Now Publishers Inc, 2009.
* [83] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. _Deep Learning_. MIT Press, 2016.
* [84] Zeyuan Allen-Zhu and Yuanzhi Li. Backward Feature Correction: How Deep Learning Performs Deep Learning. In _Conference on Learning Theory_, COLT '23, 2023. Full version available at http://arxiv.org/abs/2001.04413.
* [85] Core Francisco Park, Maya Okawa, Andrew Lee, Ekdeep Singh Lubana, and Hidenori Tanaka. Emergence of hidden capabilities: Exploring learning dynamics in concept space. _arXiv preprint arXiv:2406.19370_, 2024.
* [86] Yongyi Yang, Core Francisco Park, Ekdeep Singh Lubana, Maya Okawa, Wei Hu, and Hidenori Tanaka. Dynamics of concept learning and compositional generalization. _arXiv preprint arXiv:2410.08309_, 2024.
* [87] Lingjing Kong, Guangyi Chen, Biwei Huang, Eric P. Xing, Yuejie Chi, and Kun Zhang. Learning discrete concepts in latent hierarchical models. _arXiv preprint arXiv: 2406.00519_, 2024.
* [88] Zeyuan Allen-Zhu and Yuanzhi Li. Physics of Language Models: Part 1, Learning Hierarchical Language Structures. _ArXiv e-prints_, abs/2305.13673, May 2023. Full version available at http://arxiv.org/abs/2305.13673.
* [89] Michael Hahn and Navin Goyal. A theory of emergent in-context learning as implicit structure induction. _arXiv preprint arXiv: 2303.07971_, 2023.
* [90] Emanuele Marconato, Sebastien Lachapelle, Sebastian Weichwald, and Luigi Gresele. All or none: Identifiable linear properties of next-token predictors in language modeling. _arXiv preprint arXiv:2410.23501_, 2024.
* [91] Roger A. Horn and Charles R. Johnson. _Matrix Analysis_. Cambridge university press, 2012.
* [92] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018.

* [93] Miao Lu, Beining Wu, Xiaodong Yang, and Difan Zou. Benign oscillation of stochastic gradient descent with large learning rate. In _NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning_, 2023.
* [94] Yurii Nesterov. _Introductory Lectures on Convex Optimization: A Basic Course_, volume 87. Springer Science & Business Media, 2013.
* [95] Karan Girotra, Lennart Meincke, Christian Terwiesch, and Karl T. Ulrich. Ideas are dimes a dozen: Large language models for idea generation in innovation. _SSRN_, 2023.
* [96] Anil Rajnikant Doshi and Oliver Hauser. Generative artificial intelligence enhances creativity but reduces the diversity of novel content. _SSRN_, 2023.
* [97] Fei Liu, Xialiang Tong, Mingxuan Yuan, and Qingfu Zhang. Algorithm evolution using large language model. _arXiv preprint arXiv: 2311.15249_, 2023.
* [98] Yiming Yao, Fei Liu, Ji Cheng, and Qingfu Zhang. Evolve cost-aware acquisition functions using large language models. _arXiv preprint arXiv: 2404.16906_, 2024.
* [99] Fei Liu, Xialiang Tong, Mingxuan Yuan, Xi Lin, Fu Luo, Zhenkun Wang, Zhichao Lu, and Qingfu Zhang. An example of evolutionary computation + large language model beating human: Design of efficient guided local search. _arXiv preprint arXiv: 2401.02051_, 2024.

Limitation and Broader Impact

The theoretical analysis provided in this work introduces novel perspectives on optimization and generalization, but the data model employed may require additional refinements to better align with practical scenarios, such as adding more layers of attention. The techniques and findings can inform future empirical and theoretical explorations of transformer architectures, though we do not foresee a direct social impact arising from the theoretical advancements presented.

## Appendix B Additional Experiment Details

We implement our methods using PyTorch, ensuring consistent software and hardware environments. Specifically, the experiments are run on Linux servers with NVIDIA A100 graphics cards and CUDA 11.2, and can be completed within one hour.

## Appendix C Additional Related Work

**Theory of Convergence Rate of Stochastic Gradient Descent**. Our analysis of the exponential convergence rate for the 0-1 loss builds upon a rich body of prior work. In the context of classification, the faster convergence rate mostly based on the excess of risk with some power of the essential supremum norm. Specifically, [31, 32] introduce the _Hard low-noise condition_ over the margin. When there is a hard margin separating the classes, the test error can exhibit exponentially fast convergence as the number of training samples increases, even when the surrogate loss error only decreases polynomially. This phenomenon has been further explored in more recent studies. [33, 34, 35, 36, 37] have analyzed the exponential convergence of stochastic gradient descent under various settings. Meanwhile, [35] have investigated hard-margin and exponential rates in the context of structured prediction, which encompasses traditional classification as a special case. Besides, recent work also obtain the exponential rates in generalized settings such as Multi-class classification [38] and SVM [39]. Building upon this rich theoretical foundation, our work derives the first exponential convergence analysis for the 0-1 loss in the specific setting of transformer models with softmax attention and ReLU-activated MLP over the sparse coding data model, whose surrogate loss function is the cross-entropy loss.

**Theory of Feature Learning of GD-updated Neural Network**. A rich body of recent learning theory research has focused on the feature direction' recovery view of neural network representations [40, 41, 42, 43, 45, 52, 53, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70]. Rather than directly examining the evolution of the 0-1 loss, this line of work explicitly studies the process of reconstruction of the data's feature directions and memorization of disrupted noise in the network's latent space as surrogate metrics. While most studies in this area have assumed (near) orthogonal data, recent efforts by [43] and [44] have made initial attempts to analyze non-orthogonal data scenarios. Building upon this foundation, our study extends this line of research to nonlinear attention-MLP transformers with within-concept positive inner products and cross-concept orthogonal data representations. The key to our analysis is the assumption of good initialization of attention matrices and a sufficiently low-noise condition, which is reasonable for modeling language rather than images. In this setting, SGD allows noise to have only a mild impact on shaping neural network matrices or influencing gradient flow.

**Theory of Transformers and In-Context Learning**. The literature on Transformers and ICL is wide-ranging, and we will selectively address the most relevant ones. Prior studies have analyzed how transformers learn topic/concept semantics [9], the origins and biases of LLM representations using latent variable models [10], and ICL from a model averaging perspective [14]. However, these works do not connect the geometric properties of concept-encoded representations to transformers' powerful ICL abilities. Another line of research has studied the learning dynamics of transformer, including analyses of linear-attention transformers [16, 17, 71, 72], QK-combined attention-only models [20, 21, 26, 54, 73, 74, 75, 76, 77, 78, 79], ReLU-free MLP [54, 80, 81] or without MLP [17, 25], impractical squared or hinge loss [25, 26, 27, 28]. Though relevant, these works rely on simplifications or do not connect the observed linear semantic representation of large model to the transformer's excelling OOD capability.

**Concept Learning in Deep Learning**. Hierarchical learning has long been regarded as a key factor behind the success of deep learning [82, 83, 84]. Recent research shows that large-scale generative models, such as diffusion models and transformers, effectively encode hierarchical concepts in their latent spaces [11, 12, 13, 46, 85, 86, 87]. Moreover, [73, 88, 89] show that transformers can capture hierarchical and compositional structures in data. From a Bayesian perspective, [7, 8, 14] interpret ICL as LLMs predicting outputs based on latent (concept) variable inference. Furthermore, studies reveal a linear structure in LLMs' latent space over independent interpretable concepts: representations of the same concept exhibit positive inner products, while statistically-independent concepts are nearly orthogonal [9, 10, 11, 12, 90]. Interestingly, aligning with the findings in [46, 90], Independent Component Analysis (ICA) is naturally more suitable than Principal Component Analysis (PCA) for obtaining meaningful feature or label vectors in our prompt modeling. This is because the features or labels are nearly statistically independent and of equal strength, especially with a large \(K\), while the noise is feeble in our modeling. Building on these insights, we explore in a theoretical context how the compositional nature of concept representations relates to transformers' ability to generalize to OOD tasks through a sparse coding modeling. We believe our OOD results are not only coincides with the transformer's compositional generalization ability on language tasks [89], but also consistent with other concept learning outcomes of diffusion and multi-model model: [87] shows that adjusting the length of semantic representations can directly affect image generation behaviors (see Figure 5), while [86] reveals that compositing different concepts enables OOD generalization (e.g. "blue square apples" in the Figure 0(a) in [86]).

## Appendix D Preliminary Lemmas

### Probabilistic Lemmas on Concentration

**Lemma 6**.: _Suppose that \(\delta>0\) and \(\forall d\in\{d_{\mathcal{X}},d_{\mathcal{Y}}\}=\Omega(\log(\frac{KNL}{ \delta}))\), where \(N=BT^{*}\). Then with probability at least \(1-\delta\),_

\[\frac{\sigma_{\xi}^{2}d}{2}\leq\|\bm{\xi}_{i}\|_{2}^{2}\leq 3 \frac{\sigma_{\xi}^{2}d}{2},\] \[|\langle\bm{\xi}_{i},\bm{\xi}_{i^{\prime}}\rangle|\leq 2\sigma_{ \xi}^{2}\cdot\sqrt{d\log\left(\frac{6(N(L+1))^{2}}{\delta}\right)},\] \[|\langle\bm{\xi}_{i},\bm{\mu}\rangle|\leq\|\bm{\mu}\|_{2}\sigma_{ \xi}\cdot\sqrt{2\log(\frac{6KN(L+1)}{\delta})}\]

_for all \(\bm{\xi}_{i},\bm{\xi}_{i}^{\prime}\sim\mathcal{D}_{\xi_{\bm{x}}}(\,\text{or} \,\mathcal{D}_{\xi_{\bm{y}}}),\bm{\mu}\in\mathcal{D}_{\bm{x}}(\,\text{or}\, \mathcal{D}_{\bm{y}}),l\in\{1,2\}\)._

Proof.: See Lemma B.4 in [42] for a proof. 

**Lemma 7**.: _Suppose that \(\delta>0\), \(d_{\mathcal{Y}}=\Omega(\log(m/\delta)),m=\Omega(\log(K/(\delta)))\). Then with probability at least \(1-\delta\), for \(\forall i\in[m],k\in[K_{1}],w\in[d_{\mathcal{Y}}-K_{1}]\),_

\[\frac{\sigma_{1}^{2}d_{\mathcal{Y}}}{2}\leq\|\mathbf{W}_{O_{(i, \cdot)}}^{\Psi}{}^{(0)}\|^{2}\leq 3\frac{\sigma_{1}^{2}d_{\mathcal{Y}}}{2},\] \[\frac{|\alpha_{O_{(i,\cdot)},k}^{(0)}|}{\|\bm{c}_{k}\|},\frac{| \beta_{O_{(i,\cdot)},k}^{(0)}|}{\|\bm{d}_{k}\|},|\rho_{O_{(i,\cdot)},w}^{(0)} |\leq\sqrt{2\log(\frac{5Km}{\delta})}\cdot\sigma_{1},\] (7) \[\sigma_{1}/2\leq\max_{i\in[m]}\bigl{\{}\frac{|\alpha_{O_{(i,\cdot )},k}^{(0)}|}{\|\bm{c}_{k}\|},\frac{|\beta_{O_{(i,\cdot)},k}^{(0)}|}{\|\bm{d}_ {k}\|},|\rho_{O_{(i,\cdot)},w}^{(0)}|\}\leq\sqrt{2\log(\frac{5Km}{\delta})} \cdot\sigma_{1},\]

_Moreover, for some \(\zeta\in(0,1]\) for \(\forall e\neq e^{\prime},\in[\pm]\), \(\exists\omega_{\zeta}\in(0,\omega_{\zeta}^{\prime})\) where \(\omega_{\zeta}^{\prime}<1\),_

\[\Bigl{|}\bigl{|}\{i\in[m]\mid\mathbf{r}_{i}=\frac{e}{m},\alpha_{O_{(i,\cdot) },k}^{(0)}+e^{\prime}\zeta\beta_{O_{(i,\cdot)},k}^{(0)}>0\}|-\frac{m}{4}\Bigr{|} \leq\sqrt{\frac{m\log(10K_{1}/\delta)}{2}},\] \[\Bigl{|}\bigl{|}\{i\in[m]\mid\alpha_{O_{(i,\cdot)},k}^{(0)}+e^{ \prime}\zeta\beta_{O_{(i,\cdot)},k}^{(0)}>0,\mathbf{r}_{i}\cdot\beta_{O_{(i, \cdot)},k}^{(0)}>0\}|-\frac{m}{4}\Bigr{|}\leq\sqrt{\frac{m\log(10K_{1}/\delta )}{2}},\] \[\Bigl{|}\bigl{|}\{i\in[m]\mid\mathbf{r}_{i}=\frac{e}{m},\alpha_{O_{( i,\cdot)},k}^{(0)}\pm\zeta\beta_{O_{(i,\cdot)},k}^{(0)}>0\}|-\frac{(1+\omega_{ \zeta})m}{8}\Bigr{|}\leq\sqrt{\frac{m\log(10K_{1}/\delta)}{2}}\leq\frac{( \omega_{\zeta}^{\prime}-\omega_{\zeta})m}{8},\] \[\Bigl{|}\{i\in[m]\mid\mathbf{r}_{i}=\frac{e}{m},\alpha_{O_{(i, \cdot)},k}^{(0)}+\zeta\beta_{O_{(i,\cdot)},k}^{(0)}>0,\alpha_{O_{(i,\cdot)},k }^{(0)}-\zeta\beta_{O_{(i,\cdot)},k}^{(0)}<0\}|-\frac{(1-\omega_{\zeta})m}{8} \Bigr{|}\leq\sqrt{\frac{m\log(10K_{1}/\delta)}{2}},\] \[\sum_{i\in\{i\in[m]|\mathbf{r}_{i}=\frac{e}{m},\alpha_{O_{(i,\cdot) },k}^{(0)}+e^{\prime}\zeta\beta_{O_{(i,\cdot)},k}^{(0)}>0\}}\mathbf{r}_{i} \cdot(\alpha_{O_{(i,\cdot)},k}^{(0)}+e^{\prime}\zeta\beta_{O_{(i,\cdot)},k}^{(0 )})-0\Biggr{|}\leq\sqrt{2\log(\frac{5Km}{\delta})}\cdot\frac{5\sigma_{1}(\| \bm{c}_{k}\|+\zeta\|\bm{d}_{k}\|)}{16}.\] (8)

_In addition, for a sufficient large \(m=\Omega(\log(K/(\delta))/(1-\omega_{\zeta}))\) the lower bound inequalities regarding maximum value in Eq.7 hold at any above index set of \(i\) in Eq.8. For example, there exist \(i\in\{i\in[m]\mid\mathbf{r}_{i}=\frac{e}{m},\alpha_{O_{(i,\cdot)},k}^{(0)}+ \zeta\beta_{O_{(i,\cdot)},k}^{(0)}>0,\alpha_{O_{(i,\cdot)},k}^{(0)}-\zeta\beta_ {O_{(i,\cdot)},k}^{(0)}<0\}\), such that \(\alpha_{O_{(i,\cdot)},k}^{(0)}\leq-\sigma_{1}/2\|\bm{c}_{k}\|\)._Proof.: First, notice that \(\mathbf{W}_{\mathcal{O}_{(i,\cdot)}}^{\mathbf{y}}\sim\mathcal{N}(\mathbf{0}, \sigma_{1}\mathbf{l}_{Q_{\mathcal{Y}}})\), then by Bernstein's inequality as well as \(d_{\mathcal{Y}}=\Omega(\log(m/\delta))\), with probability at least \(1-\delta/(5m)\), for \(\forall i\in[m]\)

\[\|\mathbf{W}_{\mathcal{O}_{(i,\cdot)}}^{\mathbf{y}}\|^{(0)}\|^{2}-\sigma_{1}d _{\mathcal{Y}}|\leq O(\sigma_{1}^{2}\cdot\sqrt{d_{\mathcal{Y}}\log(5m/\delta)} )\leq\sigma_{1}^{2}d_{\mathcal{Y}}/2.\]

By union bound we can have the first inequality in the lemma hold with probability at least \(1-\delta/5\).

Next, we notice that

\[\frac{\alpha_{O_{(i,\cdot)},k}^{(0)}}{\|\bm{c}_{k}\|}=\langle\mathbf{W}_{ \mathcal{O}_{(i,\cdot)}}^{\mathbf{y}},\frac{(0)}{\|\bm{c}_{k}\|}\rangle,\quad \frac{\beta_{O_{(i,\cdot)},k}^{(0)}}{\|\bm{d}_{k}\|}=\langle\mathbf{W}_{ \mathcal{O}_{(i,\cdot)}}^{\mathbf{y}},\frac{(0)}{\|\bm{d}_{k}\|}\rangle,\quad \rho_{O_{(i,\cdot)},w}^{(0)}=\langle\mathbf{W}_{\mathcal{O}_{(i,\cdot)}}^{ \mathbf{y}},\bm{q}_{w}^{\perp}\rangle\]

are all Gaussian random variable with mean \(0\) and variance \(\sigma_{1}^{2}\). Then by Gaussian tail bound and union bound, with probability at least \(1-\delta/10\), for all \(i\in[m]\) and \(\mathbf{q}\in\bigcup_{k,w}\{\frac{\bm{c}_{k}}{\|\bm{c}_{k}\|},\frac{\bm{d}_{k }}{\|\bm{d}_{k}\|},\bm{q}_{w}^{\perp}\}\), it holds that

\[|\langle\mathbf{W}_{\mathcal{O}_{(i,\cdot)}}^{\mathbf{y}},\mathbf{q}\rangle| \leq\sqrt{2\log(5Km/\delta)}\cdot\sigma_{1}.\]

Notice \(\mathbb{P}(\sigma_{1}/2>|\langle\mathbf{W}_{\mathcal{O}_{(i,\cdot)}}^{ \mathbf{y}},\mathbf{q}\rangle|)\) is an positive constant, then following the techniques of Lemma B.5 in [42] and the condition \(m=\Omega(\log(K/\delta))\), we have

\[\mathbb{P}(\sigma_{1}/2\leq|\langle\mathbf{W}_{\mathcal{O}_{(i, \cdot)}}^{\mathbf{y}},\mathbf{q}\rangle|) =1-\mathbb{P}(\sigma_{1}/2>\max\{|\langle\mathbf{W}_{\mathcal{O} _{(i,\cdot)}}^{\mathbf{y}},\mathbf{q}\rangle|\}),\] \[=1-\mathbb{P}(\sigma_{1}/2>|\langle\mathbf{W}_{\mathcal{O}_{(i, \cdot)}}^{\mathbf{y}},\mathbf{q}\rangle|)^{mK}\] \[\geq 1-\delta/10,\]

then with probability \(1-\delta/5\), the second and third inequality hold.

For \(\zeta\in(0,1]\), we see that the variable \(\alpha_{O_{(i,\cdot)},k}^{(0)}+e^{\prime}\zeta\beta_{O_{(i,\cdot)},k}^{(0)} \sim\mathcal{N}(0,\sigma_{1}^{2}(\|\bm{c}_{k}\|^{2}+\zeta\|\bm{d}_{k}\|^{2}))\), and it's independent to the event \(\{\mathbf{r}_{i}=\frac{e}{m}\},\forall e\in[\pm]\). Therefore, we can see the count of \(\{i\in[m]\mid\mathbf{r}_{i}=\frac{e}{m},\alpha_{O_{(i,\cdot)},k}^{(0)}+e^{ \prime}\zeta\beta_{O_{(i,\cdot)},k}^{(0)}>0,e^{\prime}\zeta\beta_{O_{(i,\cdot )},k}^{(0)}>0\}\) as a binomial variable with \(p=1/4,n=m\), then by the property of binomial tail, condition \(m=\Omega(\log(K/(\delta)))\) as well as Hoeffding's inequality, with probability at least \(1-\delta/5\) we have

\[|\frac{|\{i\in[m]\mid\mathbf{r}_{i}=\frac{e}{m},\alpha_{O_{(i,\cdot)},k}^{(0)} +e^{\prime}\zeta\beta_{O_{(i,\cdot)},k}^{(0)}>0\}|}{m}-\frac{1}{4}|\leq\sqrt{ \frac{\log(10K_{1}/\delta)}{2m}},\]

which completes the proof of the forth inequality. Similarly, for the fifth inequality we can utilize the same techniques to derive that it holds with probability at least \(1-\delta/5\).

For the event \(\{i\in[m]\mid\mathbf{r}_{i}=\frac{e}{m},\alpha_{O_{(i,\cdot)},k}^{(0)}\pm\zeta \beta_{O_{(i,\cdot)},k}^{(0)}>0\}\), we have

\[\mathbb{P}(\mathbf{r}_{i}=\frac{e}{m},\alpha_{O_{(i,\cdot)},k}^{(0 )}\pm\zeta\beta_{O_{(i,\cdot)},k}^{(0)}>0) =\mathbb{P}(\alpha_{O_{(i,\cdot)},k}^{(0)}+e^{\prime}\zeta\beta_{ O_{(i,\cdot)},k}^{(0)}>0)\] \[\cdot\mathbb{P}(\alpha_{O_{(i,\cdot)},k}^{(0)}-e^{\prime}\zeta \beta_{O_{(i,\cdot)},k}^{(0)}>0\mid\alpha_{O_{(i,\cdot)},k}^{(0)}+e^{\prime} \zeta\beta_{O_{(i,\cdot)},k}^{(0)}>0)\] \[=\frac{1}{2}\cdot\mathbb{P}(\alpha_{O_{(i,\cdot)},k}^{(0)}-e^{ \prime}\zeta\beta_{O_{(i,\cdot)},k}^{(0)}>0\mid\alpha_{O_{(i,\cdot)},k}^{(0)}+e ^{\prime}\zeta\beta_{O_{(i,\cdot)},k}^{(0)}>0),\] \[=\frac{1}{2}\cdot\frac{1+\omega_{\zeta}}{2},\]

where \(\frac{1+\omega_{\zeta}}{2}\) is the probability of the conditional event \(\{\alpha_{O_{(i,\cdot)},k}^{(0)}-e^{\prime}\zeta\beta_{O_{(i,\cdot)},k}^{(0)}>0 \mid\alpha_{O_{(i,\cdot)},k}^{(0)}+e^{\prime}\zeta\beta_{O_{(i,\cdot)},k}^{(0)}>0\}\), and \(\omega_{\zeta}>0\) due to the larger variance of \(\alpha_{O_{(i,\cdot)},k}^{(0)}\) compared to \(e^{\prime}\zeta\beta_{O_{(i,\cdot)},k}^{(0)}\). We denote the probability with \(\omega_{\zeta}\) since the true value is hard to compute. Subsequently, the event \(\{i\in[m]\mid\mathbf{r}_{i}=\frac{e}{m},\alpha_{O_{(i,\cdot)},k}^{(0)}\pm\zeta \beta_{O_{(i,\cdot)},k}^{(0)}>0\}\) can be seen as a binomial variable with \(p=\frac{1+\omega_{\zeta}}{8},n=m\), then we can have the sixth inequality hold with probability at least \(1-\delta/5\), utilizing the property of binomial tail, condition \(m=\Omega(\log(K/(\delta)))\) as well as Hoeffding's inequality.

The seventh inequality is a natural inference of the third and forth inequality, where the \(m=\Omega(\log(K_{1}/\delta))\) ensure \(\sqrt{m\log(10K_{1}/\delta)/2}\leq m/16\), and the last inequality is then also a natural inference of the third and fifth inequality.

Therefore, by union bound, the proof is completed.

### Matrix Theories

**Lemma 8**.: _(1.1.P5 in [91]) Let \(A\in M_{n}\) be idempotent, that is,\(A^{2}=A\). Then, each eigenvalue of \(A\) equals to the rank of \(A\), which is either 0 or 1. Beside, identity matrix \(\mathbf{I}\) is the only nonsingular idempotent matrix._

**Lemma 9**.: _For a matrix \(A=\sum_{i=1}^{d}\mu_{i}P_{i}\), where \(P_{i}\) are symmetric idempotent matrices with \(\operatorname{rank}(P_{i})=1\), and thus \(\sum_{i=1}^{d}\mu_{i}P_{i}\) is the idempotent decomposition of matrix \(A\) by \(P_{i}\). Then we see that \(\|A\|_{F}=\sqrt{\operatorname{tr}(A^{T}A)}=\sqrt{\sum_{i=1}^{d}\mu_{i}^{2}}= \sqrt{\sum_{i=1}^{d}\lambda_{i}^{2}}\), where \(\lambda_{i}\) are eigenvalues of \(A\)._

Proof.: By definition,

\[A^{T}A=\sum_{i=1}^{d}\mu_{i}^{2}P_{i}^{T}P_{i}=\sum_{i=1}^{d}\mu_{i}^{2}P_{i}P_ {i}=\sum_{i=1}^{d}\mu_{i}^{2}P_{i}.\]

Then, by Lemma 8 we have

\[\operatorname{tr}(A^{T}A)=\operatorname{tr}(\sum_{i=1}^{d}\mu_{i}^{2}P_{i})= \sum_{i=1}^{d}\mu_{i}^{2}\operatorname{tr}(P_{i})=\sum_{i=1}^{d}\mu_{i}^{2} \operatorname{rank}(P_{i})=\sum_{i=1}^{d}\mu_{i}^{2}=\sum_{i=1}^{d}\lambda_{i} ^{2}.\]

### ODE Systems

**Lemma 10**.: _(Lemma C.1 in [43]). Suppose that a sequence \(a_{t},t\geq 0\) follows the iterative formula_

\[a_{t+1}=a_{t}+\frac{c}{1+be^{a_{t}}},\]

_for some \(0\leq c\leq 1\) and \(b\geq 0\). Then it holds that_

\[x_{t}\leq a_{t}\leq\frac{c}{1+be^{a_{0}}}+x_{t}\]

_for all \(t\geq 0\). Here, \(x_{t}\) is the unique solution of_

\[\frac{\operatorname{d}x_{t}}{\operatorname{d}t}=\frac{c}{1+be^{x_{t}}},\quad x _{0}=a_{0}\Leftrightarrow x_{t}+be^{x_{1}}=ct+a_{0}+be^{a_{0}}.\]

**Lemma 11**.: _(Coupled ODE System 1). Suppose that there are two coupled sequences \(y_{t}\), \(z_{t}\), \(t\geq 0\) follows the iterative formula_

\[y_{t+1} =y_{t}+az_{t}y_{t}\frac{1}{2+e^{-2y_{t}{}^{2}}+e^{2y_{t}{}^{2}}}, y_{0} >0, a>0,\] \[z_{t+1} =z_{t}+b, z_{0} <0, b>0,\]

_for some \(a,b\geq 0\). Then it holds that_

\[y(t)\leq y_{t},\quad z(t)=z_{t},\]

_for all \(t\geq 0\). Here, \(y(t)\), \(z(t)\) are the unique solutions of the following ODE System respectively_

\[y^{\prime}(t) =\frac{a}{4}z(0)y(t), y(0) =y_{0},\] (9) \[z^{\prime}(t) =b, z(0) =z_{0}.\]

_As such, for \(t_{1}=\min\{t\in\mathbb{Z}\mid z_{t}\geq 0\}\), we have_

\[y_{t_{1}}\geq y(0)e^{\frac{-az(0)^{2}(1+e^{-2y(0)^{2}})}{4b(1-e^{-2y(0)^{2}})}},\]

_and \(t_{1}\geq\frac{-z(0)(1+e^{-2y(0)^{2}})}{b(1-e^{-2y(0)^{2}})}\)._

Proof.: From the condition we see that \(z_{0}<0\) and \(z_{t}\) is an increasing sequence (\(z_{t}\geq z_{0}\)). Besides, as \(y_{0}>0\), during the period where \(z_{t}\leq 0\), we see that \(y_{t}\) is monotonically decreasing. Then by \((2+e^{-2y_{t}{}^{2}}+e^{2y_{t}{}^{2}})^{-1}\leq 1/4\) as well as Comparison Theorem, it's obvious that the continuous coupled ODE in Eq.(9) is the lower bound of \(y_{t}\). Then one can readily obtain the result by solving the ODE.

**Lemma 12**.: _(Coupled ODE System 2). Suppose that there are two coupled sequences \(y_{t}\), \(z_{t}\), which are the sequences after \(t_{1}\) in Lemma 11, and \(t\geq t_{1}\) follows the iterative formula_

\[y_{t+1} =y_{t}+az_{t}y_{t}\frac{1}{2+e^{-2y_{t}{}^{2}}+e^{2y_{t}{}^{2}}} \ell_{t}^{\prime}, y_{t_{1}}>0, a>0,\] \[z_{t+1} =z_{t}+b\frac{1-e^{-2y(t)^{2}}}{1+e^{-2y(t)^{2}}}\ell_{t}^{\prime}, z_{t_{1}}\geq 0, b>0,\]

_for some \(a,b\geq 0\), and \(c^{\prime}\leq\ell_{t}^{\prime}\leq 1\). Then it holds that_

\[\underline{y}(t)\leq y_{t}\leq\overline{y}(t),\quad\underline{z}(t)\leq z_{t }\leq\overline{z}(t),\]

_for all \(t\geq t_{1}\). Here, \(\overline{y}(t)\), \(\underline{y}(t)\), \(\overline{z}(t)\), \(\underline{z}(t)\) are the unique solutions of the following ODE System respectively_

\[\frac{1}{2}(\mathrm{Ei}(2\underline{y}(t)^{2})+\mathrm{Ei}(-2 \underline{y}(t)^{2})+4\log(\underline{y}(t))) =abc^{\prime^{2}}\frac{1-e^{-2y(t_{1})^{2}}}{1+e^{-2y(t_{1})^{2}} }\frac{(t-t_{1})^{2}}{2}+\frac{1}{2}(\mathrm{Ei}(2{y_{t}}_{1}^{2})+\mathrm{Ei }(-2{y_{t}}_{1}^{2}))\] \[\quad+4\log(y_{t_{1}}),\] \[\underline{z}(t) =bc^{\prime}\frac{1-e^{-2y(t_{1})^{2}}}{1+e^{-2y(t_{1})^{2}}}(t-t _{1}),\] \[\frac{1}{2}(\mathrm{Ei}(2\overline{y}(t)^{2})+\mathrm{Ei}(-2 \overline{y}(t)^{2})+4\log(\overline{y}(t))) =\frac{ab(t-t_{1})^{2}}{2}+\frac{1}{2}(\mathrm{Ei}(2{y_{t}}_{1}^{ 2})+\mathrm{Ei}(-2{y_{t}}_{1}^{2}))+4\log(y_{t_{1}})\] \[\overline{z}(t) =b(t-t_{1}),\]

_where_

\[\mathrm{Ei}(x)=\int_{-\infty}^{x}\frac{e^{t}}{t}\mathrm{d}t=\gamma_{\text{Euler} }+\ln x+\exp(x/2)\sum_{n=1}^{\infty}\frac{(-1)^{n-1}x^{n}}{n!2^{n-1}}\sum_{k=0 }^{\lfloor(n-1)/2\rfloor}\frac{1}{2k+1}.\]

Proof.: We see that as \(z_{t}\geq 0,t\geq t_{1}\), the \(y_{t}\) is monotonically increasing. As such, by Comparison Theorem we see that the upper and lower bound of the coupled system would depends on \(\frac{1-e^{-2y(t)^{2}}}{1+e^{-2y(t)^{2}}}\) and \(\ell_{t}^{\prime}\). Easy to see that

\[\frac{1-e^{-2y(t_{1})^{2}}}{1+e^{-2y(t_{1})^{2}}}\leq\frac{1-e^{-2y(t)^{2}}}{1 +e^{-2y(t)^{2}}}\leq 1,\]

and then collaborating with \(c^{\prime}\leq\ell_{t}^{\prime}\leq 1\) we can obtain the result by solving the ODE. Observing that

\[\frac{\mathrm{d}y(t)}{\mathrm{d}t} =abc^{\prime^{2}}\frac{1-e^{-2y(t_{1})^{2}}}{1+e^{-2y(t_{1})^{2} }}\frac{(t-t_{1})y(t)\mathrm{d}t}{1+e^{2y(t)^{2}}+e^{-2y(t)^{2}}}\] \[\Leftrightarrow\frac{1}{2}(\mathrm{Ei}(2\underline{y}(t)^{2})+ \mathrm{Ei}(-2\underline{y}(t)^{2})+4\log(\underline{y}(t))) =abc^{\prime^{2}}\frac{1-e^{-2y(t_{1})^{2}}}{1+e^{-2y(t_{1})^{2} }}\frac{(t-t_{1})^{2}}{2}+\text{const},\] \[\underline{z}(t) =bc^{\prime}\frac{1-e^{-2y(t_{1})^{2}}}{1+e^{-2y(t_{1})^{2}}}(t- t_{1}).\]

Thus by the monotonicity the system is unique, which is also true for the upper bound ODE. The proof is completed. 

## Appendix E Data Distribution

This section provided the detailed formal definitions of the prompt distribution.

**Definition 3**.: _(**Polysemous Word Model** _(\(\mathcal{D}_{\bm{x}},\mathcal{D}_{\bm{y}},\mathcal{D}_{\bm{z}},\mathcal{D}_{ \xi_{\bm{x}}},\mathcal{D}_{\xi_{\bm{y}}}\) ). We assume there exists \(K_{1}\) concepts of words totally. Specifically, each concept \(k_{1}\in[K_{1}]\) is characterized by two semantically-opposite feature vectors separately, denoted as \(\bm{\mu}_{k_{1}}^{+}\) and \(\bm{\mu}_{k_{1}}^{-}\), and the label vectors that describe their semantics under the co-concept are \(\bm{q}_{k_{1}}^{+}\) and \(\bm{q}_{k_{1}}^{-}\). Our word samples \(\bm{x}\in\mathbb{R}^{d_{\bm{X}}}\) and their corresponding labels \(\bm{y}\in\mathbb{R}^{d_{\bm{Y}}}\) are generated i.i.d. from distribution \(\mathcal{D}_{\bm{x}}\) and \(\mathcal{D}_{\bm{y}}\), which can be written as the following forms via reparameterization:_

\[\bm{z} \sim\mathcal{D}_{\bm{z}},\quad\xi_{\bm{x}}\sim\mathcal{D}_{\xi_{ \bm{x}}}=\mathcal{N}(\bm{0},\sigma_{\bm{z}}^{2}\mathds{I}_{d_{\bm{x}}}),\quad \xi_{\bm{y}}\sim\mathcal{D}_{\xi_{\bm{y}}}=\mathcal{N}(\bm{0},\sigma_{\bm{z }}^{2}\mathds{I}_{d_{\bm{y}}}),\] \[\bm{x} =\mathbf{M}\bm{z}+\xi_{\bm{x}}\sim\mathcal{D}_{\bm{x}},\quad\bm{ y}=\mathbf{Q}\bm{z}+\xi_{\bm{y}}\sim\mathcal{D}_{\bm{y}},\]

_where \(\bm{z}\in\mathbb{R}^{K}(K<d_{\bm{x}})\). We denote \(\bm{z}\) as the sparse latent signal and \(\xi\) as the spurious dense noise, and each \(\bm{x}\)-\(\bm{y}\) pair are reparameterized by one shared \(\bm{z}\). We have the following assumptions on \(\mathbf{M},\bm{z},\xi\) respectively:_* _The sparse latent variable_ \(\bm{z}=(z_{1},\cdots,z_{K})\in\{0,1\}^{k}\) _is sampled from_ \(\mathcal{D}_{z}\)_,_ \(P(z_{j}=1)=\Theta(\frac{\log\log K}{K})\)_,_
* \(\mathbf{M}=[\bm{\mu}_{1}^{-},\bm{\mu}_{1}^{-},\bm{\mu}_{2}^{+},\bm{\mu}_{2}^{-},\cdots,\bm{\mu}_{K_{1}}^{+},\bm{\mu}_{K_{1}}^{-},\bm{\nu}_{1},\bm{\nu}_{2}, \cdots,\bm{\nu}_{K_{2}}]=[M_{1},\cdots,M_{K}]\in\mathbb{R}^{d_{X}\times K}\) _is the feature dictionary matrix, where_ \(\{\bm{\mu}_{k}^{\pm}\}_{k=1}^{K}\) _are concept-relevant features,_ \(\{\bm{\nu}_{k_{2}}\}_{k_{2}=1}^{K_{2}}\) _are concept-irrelevant features, and_ \(\forall k\in[K],\|M_{k}\|=\|\mathbf{u}\|\)_. We assume that features of the same concept have positive inner product:_ \(\exists 0<\kappa_{\bm{x}}<1\)_,_ \(\forall k_{1}\in[K_{1}]\)_,_ \(0<\langle\bm{\mu}_{k_{1}}^{+},\bm{\mu}_{k_{1}}^{-}\rangle\leq\kappa_{\bm{x}} \|\mathbf{u}\|^{2}\)_. Meanwhile, we let the features of different concept be orthogonal:_ \(\forall e\in[\pm],e^{\prime}\in[\pm],s^{\prime}\in[K_{1}],r\neq r^{\prime}\in[ K_{2}],\mathbf{u}\in\{\bm{\mu}_{s^{\prime}}^{\varepsilon^{\prime}},\bm{\nu}_{r}\}\)_, we have_ \(\langle\bm{\mu}_{s}^{\varepsilon},\mathbf{u}\rangle=\langle\bm{\nu}_{r},\bm{ \nu}_{r^{\prime}}\rangle=0\)_._
* \(\mathbf{Q}=[\bm{q}_{1}^{+},\bm{q}_{1}^{-},\bm{q}_{2}^{+},-\cdots,\bm{q}_{K_{1 }}^{+},\bm{q}_{K_{1}}^{-},0,\cdots 0]\in\mathbb{R}^{d_{Y}\times K}\) _is the corresponding label dictionary matrix, where_ \(\|\bm{q}_{k}^{\pm}\|=\|\mathbf{q}\|\)_, for_ \(\forall k\in[K_{1}]\)_. Similarly, we let the labels of the same concept to have positive inner product:_ \(\exists 0<\kappa_{\bm{y}}<1\)_,_ \(\forall k_{1}\in[K_{1}]\)_,_ \(0<\langle\bm{q}_{k_{1}}^{+},\bm{q}_{k_{1}}^{-}\rangle\leq\kappa_{\bm{y}}\| \mathbf{q}\|^{2}\)_, while the labels of different concept to be orthogonal:_ \(\langle\bm{q}_{k}^{\pm},\bm{q}_{k^{\prime}}^{\pm}\rangle=0,\forall k\neq k^{ \prime}\in[K_{1}]\)_._

**Definition 4**.: _(**Concept-specific Contextual Prompt Distribution**) We consider the case that each prompt is concept-specific (i.e., the multi-concept words in one prompt would at least share one co-concept). Specifically, the chance for selecting each concept as the co-concept of one particular prompt is \(\Theta({K_{1}}^{-1})\), and the chance for selecting the two semantically-opposite vectors of the same concept is \(\frac{1}{2}\). During training, each prompt \(S=\{\bm{x}_{1},\bm{y}_{1},\cdots,\bm{x}_{L},\bm{y}_{L},\bm{x}_{L+1}\}\) is sampled from the mixture distribution \(\mathcal{D}_{S}\) defined as below._

\[\mathcal{D}_{S}=\sum_{k=1}^{K_{1}}\left(\pi_{k}^{+}\mathcal{P}_{k,L+1}^{+}+\pi _{k}^{-}\mathcal{P}_{k,L+1}^{-}\right),\] (10)

_where \(\pi_{k}^{+}=\pi_{k}^{-}=\frac{1}{2K_{1}}\), and the \(\mathcal{P}_{k,L+1}^{+}\) and \(\mathcal{P}_{k,L+1}^{-}\) are prompt distributions characterized by the \(k\)-th concept, defined as_

\[\mathcal{P}_{k,L+1}^{+}=\Big{\{}S\mid\bm{x}\sim\mathcal{D}_{\bm{x}},\bm{y} \sim\mathcal{D}_{\bm{y}},P_{L+1,2k-1}=1,\forall l\in[L+1],j\neq\{2k-1,k\},P_{ l,j}=\frac{1}{K},\]

\[\{z_{l,2k-1}=1\}\cup\{z_{l,2k}=1\}=\Omega,\{z_{l,2k-1}=1\}\cap\{z_{l,2k}=1\}= \emptyset,\forall l\in[L],P_{l,2k-1}=P_{l,2k}=\frac{1}{2}\Big{\}},\]

\[\mathcal{P}_{k,L+1}^{-}=\Big{\{}S\mid\bm{x}\sim\mathcal{D}_{\bm{x}},\bm{y} \sim\mathcal{D}_{\bm{y}},P_{L+1,2k}=1,\forall l\in[L+1],j\neq\{2k-1,k\},P_{ l,j}=\frac{1}{K},\]

\[\{z_{l,2k-1}=1\}\cup\{z_{l,2k}=1\}=\Omega,\{z_{l,2k-1}=1\}\cap\{z_{l,2k}=1\}= \emptyset,\forall l\in[L],P_{l,2k-1}=P_{l,2k}=\frac{1}{2}\Big{\}},\]

_where \(P_{l,j}:=\mathbb{P}\left(z_{l,j}=1\right)\). \(\forall n\in[N]\) where \(N\) is the training size, if the training prompt \(S_{n}\) is sampled from \(\mathcal{P}_{k,L+1}^{\varepsilon},e\in[\pm],k\in[K_{1}]\), then by Definition 1, the label vector of the query should contain \(\bm{q}_{k}^{\varepsilon}\), and we call \(y_{S_{n}}=e\) as the real value label of this \(k\)-th concept prompt. Specifically, for \(\forall k\in[K_{1}]\) we define the index set of training prompts sharing the \(k\)-th co-concepts as_

\[\mathcal{V}_{k}=\mathcal{V}_{k}^{+}\cup\mathcal{V}_{k}^{-},\]

_where_

\[\mathcal{V}_{k}^{+}=\left\{n\mid S_{n}\sim\mathcal{P}_{k,L+1}^{+}\right\},\]

_For sample \(\bm{x}_{l}\) where \(n\in\mathcal{V}_{k},k\in[K_{1}]\), \(l\in[L+1]\), we define the index set for its non-zero elements of \(\bm{z}_{l}^{n}\) besides \(z_{2k-1,l}^{n}\) and \(z_{2k,l}^{n}\), namely \(\mathcal{M}_{l}^{n}\coloneqq\{k\in[K]\mid z_{l,k}^{n}=1,k\notin\{2k-1,2k\}\}\). Also, for each prompt sharing the \(k\)-th co-concept, we define the index set of demonstration in the context:_

\[S_{n,k}^{+}=\{l\in[L]\mid n\in\mathcal{V}_{k},z_{l,2k-1}^{n}=1\},\quad S_{n,k}^{ -}=\{l\in[L]\mid n\in\mathcal{V}_{k},z_{l,2k}^{n}=1\},\]

## Appendix F Model details: Attention Part

In this section, we provide several important definitions and compute the original gradients of attention.

**Lemma 13**.: _(Contributing and Misleading Neurons)_

\[\mathcal{W}_{k,n}^{+}(t)=\{i\in[m]\mid n\in\mathcal{V}_{k}^{+}, \mathds{1}_{O_{(i)}}^{n}\,{}^{(t)}>0\},\quad\mathcal{U}_{k,n}^{+}(t)=\{i\in[m] \mid n\in\mathcal{V}_{k}^{+},\mathbf{r}_{i}\cdot\mathds{1}_{O_{(i)}}^{n}\,{}^{(t)}>0\},\] (11) \[\mathcal{W}_{k,n}^{-}(t)=\{i\in[m]\mid n\in\mathcal{V}_{k}^{-}, \mathds{1}_{O_{(i)}}^{n}\,{}^{(t)}>0\},\quad\mathcal{U}_{k,n}^{-}(t)=\{i\in[m] \mid n\in\mathcal{V}_{k}^{-},\mathbf{r}_{i}\cdot\mathds{1}_{O_{(i)}}^{n}\,{}^{(t)}<0\}.\]\(\mathcal{W}_{k,n}(t):=\mathcal{W}_{k,n}^{+}(t)\cup\mathcal{W}_{k,n}^{-}(t)\) are neurons that can be activated, among which \(\mathcal{U}_{k,n}(t):=\mathcal{U}_{k,m}^{+}(t)\cup\mathcal{U}_{k,n}^{-}(t)\) are neurons that correctly contribute to the prediction. The following lemma computes the original gradients.

**Lemma 14**.: _(Gradient Update) Denote_

\[\begin{split}&\mathbf{r}_{i}=\mathbf{r}[i],\\ &\ell_{n}^{{}^{\prime}\,(t)}=\ell^{\prime}(y_{S_{n}}\cdot f( \mathbf{H}^{n};\Psi^{(t)})),\\ &\left(\sigma_{S}^{(t)}\right)_{l}^{n}=\mathrm{softmax}\left( \left(\mathbf{W}_{K}^{(t)}\mathbf{h}_{l}^{n}\right)^{\top}\mathbf{W}_{Q}^{(t) }\mathbf{h}_{L+1}^{n}\right),\\ &\mathds{1}_{O_{(i)}}^{n}(t)=\mathds{1}(\mathbf{W}_{O_{(i,i)}}^{ (t)}\operatorname{attn}(\mathbf{H}^{n};\Psi^{(t)})>0).\end{split}\] (12)

\(\nabla_{\mathbf{W}_{Q}^{(t)}}L_{\mathcal{B}_{t}}(\Psi^{(t)})\in\mathbb{R}^{d_ {\mathcal{X}}\times d_{\mathcal{X}}}\) can be derived as

\[\frac{1}{B}\sum_{n\in\mathcal{B}_{t}}\Bigg{[}y_{S_{n}}^{(t)}\ell_{n}^{{}^{ \prime}\,(t)}\sum_{i=1}^{m}\mathbf{r}_{i}\mathds{1}_{O_{(i)}}^{n}(t)\sum_{l,j \in[L]}\left(\sigma_{S}^{(t)}\right)_{l}^{n}(\sigma_{S}^{(t)})_{j}^{n}(\mathbf{ W}_{O_{(i,i)}}^{(t)}\mathbf{W}_{V}^{(t)}\mathbf{h}_{V}^{n})\mathbf{W}_{K}^{ \mathbf{a}\,(t)}(\bm{x}_{l}^{n}-\bm{x}_{j}^{n})\bm{x}_{L+1}^{n}\Bigg{]}+\lambda \mathbf{W}_{Q}^{\mathbf{a}\,(t)}.\] (13)

_Similarly, \(\nabla_{\mathbf{W}_{K}^{\mathbf{a}\,(t)}}L_{\mathcal{B}_{t}}(\Psi^{(t)})\in \mathbb{R}^{d_{\mathcal{X}}\times d_{\mathcal{X}}}\) can be derived as_

\[\frac{1}{B}\sum_{n\in\mathcal{B}_{t}}\Bigg{[}y_{S_{n}}^{(t)}\ell_{n}^{{}^{ \prime}\,(t)}\sum_{i=1}^{m}\mathbf{r}_{i}\mathds{1}_{O_{(i)}}^{n}(t)\sum_{l,j \in[L]}\left(\sigma_{S}^{(t)}\right)_{l}^{n}(\sigma_{S}^{(t)})_{j}^{n}(\mathbf{ W}_{O_{(i,i)}}^{(t)}\mathbf{W}_{V}^{(t)}\mathbf{h}_{V}^{n})\mathbf{W}_{Q}^{\mathbf{a} \,\top}\bm{x}_{L+1}^{n}(\bm{x}_{l}^{n}-\bm{x}_{j}^{n})^{\top}\Bigg{]}+\lambda \mathbf{W}_{K}^{\mathbf{a}\,(t)}.\] (14)

Subsequently, we directly compute the update of the attention matrices along the feature directions as below.

**Lemma 15**.: _(Concept Learning of Attention) For \(\forall\hat{k}\in[K_{1}]\), we have the single step of learning of the concept part of the features:_

\[\begin{split}\bm{a}_{k}^{\top}\mathbf{W}_{Q}^{\mathbf{a}\,(t+1)} \bm{a}_{\hat{k}}-\bm{a}_{\hat{k}}^{\top}\mathbf{W}_{Q}^{\mathbf{a}\,(t)}\bm{a} _{\hat{k}}&=-\eta_{t}\cdot\bm{a}_{k}^{\top}\nabla_{\mathbf{W}_{Q }^{\mathbf{a}\,(t)}}L_{\mathcal{B}_{t}}(\Psi^{(t)})\bm{a}_{\hat{k}}\\ &=-\eta_{t}(I_{Q,\bm{a}_{\hat{k}},\text{choor}}^{(t)}+I_{Q,\bm{a }_{\hat{k}},\text{coornt}}^{(t)})-\eta_{t}\lambda\bm{a}_{\hat{k}}^{\top} \mathbf{W}_{Q}^{\mathbf{a}\,(t)}\bm{a}_{\hat{k}},\\ \bm{a}_{\hat{k}}^{\top}\mathbf{W}_{K}^{\mathbf{a}\,(t+1)}\bm{a}_{ \hat{k}}-\bm{a}_{\hat{k}}^{\top}\mathbf{W}_{K}^{\mathbf{a}\,(t)}\bm{a}_{\hat{k}} &=-\eta_{t}\cdot\bm{a}_{\hat{k}}^{\top}\nabla_{\mathbf{W}_{K}^{ \mathbf{a}\,(t)}}L_{\mathcal{B}_{t}}(\Psi^{(t)})\bm{a}_{\hat{k}}\\ &=-\eta_{t}(I_{K,\bm{a}_{\hat{k}},\text{choors}}^{(t)}+I_{K,\bm{a }_{\hat{k}},\text{coornt}}^{(t)})-\eta_{t}\lambda\bm{a}_{\hat{k}}^{\top} \mathbf{W}_{K}^{\mathbf{a}\,(t)}\bm{a}_{\hat{k}},\end{split}\] (15)

_where \(I_{Q,\bm{a}_{\hat{k}},\text{choors}}^{(t)}\) and \(I_{Q,\bm{a}_{\hat{k}},\text{coornt}}^{(t)}\) are defined as below._

\[\begin{split} I_{Q,\bm{a}_{\hat{k}},\text{choors}}^{(t)}& =\frac{1}{B}\sum_{\begin{subarray}{c}k\hat{k}\in[K_{1}]\\ \in\mathcal{C}[k]\\ n\in\mathcal{V}_{k}^{\mathbf{a}}\mathcal{B}_{t}\end{subarray}}\Big{[} \epsilon\ell_{n}^{{}^{\prime}\,(t)}\bm{a}_{\hat{k}}^{\top}\left(\xi_{\bm{x},L+ 1}^{n}+\sum_{r\in\mathcal{M}_{L+1}^{n}}\mathbf{M}_{r}\right)\sum_{i\in\mathcal{W }_{k,n}^{\hat{k}}}\mathbf{r}_{i}\sum_{l,j\in[L]}\left(\sigma_{S}^{(t)}\right)_{l }^{n}\left(\sigma_{S}^{(t)}\right)_{j}^{n}\\ &(\mathbf{W}_{O_{(i,i)}}^{\mathbf{y}\,(t)}(\bm{q}_{k}^{y_{i}}+\sum _{s\in\mathcal{M}_{l}^{n}}\mathbf{Q}_{S}+\xi_{\bm{y},l}^{n}))(\bm{a}_{\hat{k}}^{ \top}\mathbf{W}_{K}^{\mathbf{a}\,(t)}((y_{l}^{n}-y_{j}^{n})\bm{b}_{\hat{k}}+\sum _{s\in\mathcal{M}_{l}^{n}}\mathbf{M}_{s}+\xi_{\bm{x},l}^{n}-\sum_{s\in\mathcal{M}_ {j}^{n}}\mathbf{M}_{s}-\xi_{\bm{x},j}^{n}))\Big{]}\\ &+\frac{1}{B}\sum_{\begin{subarray}{c}\hat{e}\in[\pm]\\ n\in\mathcal{V}_{k}^{\mathbf{a}}\mathcal{B}_{t}\end{subarray}}\Big{[}\hat{e}\ell_{n }^{{}^{\prime}\,(t)}(\|\bm{a}_{\hat{k}}\|^{2}+\bm{a}_{\hat{k}}^{\top}(\xi_{\bm{x},L+1}^{n}+\sum_{r\in\mathcal{M}_{L+1}^{n}}\mathbf{M}_{r}))\sum_{i\in\mathcal{W }_{k,n}^{\hat{k}}}\mathbf{r}_{i}\Big{\{}\sum_{l,j\in[L]}\left(\sigma_{S}^{(t)} \right)_{l}^{n}\left(\sigma_{S}^{(t)}\right)_{j}^{n}\\ (\mathbf{W}_{O_{(i,i)}}^{\mathbf{y}\,(t)}(\sum_{s\in\mathcal{M}_{l}^{n}} \mathbf{Q}_{S}+\xi_{\bm{y},l}^{n}))(\bm{a}_{\hat{k}}^{\top}\mathbf{W}_{K}^{ \mathbf{a}\,(t)}((y_{l}^{n}-y_{j}^{n})\bm{b}_{\hat{k}}+\xi_{\bm{x},l}^{n}-\xi_{ \bm{x},j}^{n}))\Big{]},\end{split}\] (16)_Similarly, \(I_{K,\bm{a}_{\hat{k}},\text{chas}}^{(t)}\) and \(I_{K,\bm{a}_{\hat{k}},\text{conti}}^{(t)}\) are defined as below._

\[\begin{split} I_{K,\bm{a}_{\hat{k}},\text{chas}}^{(t)}& =\frac{1}{B}\sum_{\begin{subarray}{c}k\neq\hat{k}\in[K_{1}]\\ \epsilon\in[\pm]\\ n\in\mathcal{V}_{k}^{\text{s}}\cap\mathcal{B}_{t}\end{subarray}}\Bigl{[}e^{ \ell_{n}^{\prime}}{}^{(t)}\bm{a}_{\hat{k}}^{\top}\mathbf{W}_{Q}^{\bm{\pi}}{}^{(t )}(\bm{a}_{k}+\bm{e}\bm{b}_{k}+\xi_{\bm{x},L+1}^{n}+\sum_{r\in\mathcal{M}_{L+1} ^{\text{s}}}\mathbf{M}_{r})\sum_{i\in\mathcal{W}_{k,n}^{\text{s}}}\mathbf{r}_{ i}\cdot\sum_{l,j\in[L]}(\sigma_{S}^{(t)})_{l}^{n}\\ &(\sigma_{S}^{(t)})_{j}^{n}(\mathbf{W}_{O_{(i,\cdot)}}^{\bm{y}} {}^{(t)}(\bm{q}_{k}^{y_{l}^{\text{s}}}+\sum_{s\in\mathcal{M}_{1}^{\text{s}}} \mathbf{Q}_{S}+\xi_{\bm{y},l}^{n}))\bm{a}_{\hat{k}}{}^{\top}(\sum_{s\in \mathcal{M}_{1}^{\text{s}}}\mathbf{M}_{s}+\xi_{\bm{x},l}^{n}-\sum_{s\in \mathcal{M}_{1}^{\text{s}}}\mathbf{M}_{s}-\xi_{\bm{x},j}^{n})\Bigr{]}\\ &+\frac{1}{B}\sum_{\begin{subarray}{c}k\in[\pm]\\ n\in\mathcal{V}_{k}^{\text{s}}\cap\mathcal{B}_{t}\end{subarray}}\Bigl{[}\hat{e }\ell_{n}^{\prime}{}^{(t)}\bm{a}_{\hat{k}}^{\top}\mathbf{W}_{Q}^{\bm{\pi}}{}^{(t )}(\bm{a}_{\hat{k}}+\bm{e}\bm{b}_{\hat{k}}+\xi_{\bm{x},L+1}^{n}+\sum_{r\in \mathcal{M}_{L+1}^{\text{s}}}\mathbf{M}_{r})\cdot\sum_{i\in\mathcal{W}_{k,n}^{ \text{s}}}\mathbf{r}_{i}\cdot\\ &\sum_{l,j\in[L]}(\sigma_{S}^{(t)})_{l}^{n}(\sigma_{S}^{(t)})_{j}^{n}( \mathbf{W}_{O_{(i,\cdot)}}^{\bm{y}}{}^{(t)}(\sum_{s\in\mathcal{M}_{1}^{\text{ s}}}\mathbf{Q}_{S}+\xi_{\bm{y},l}^{n}))\bm{a}_{\hat{k}}{}^{\top}(\xi_{\bm{x},l}^{n}- \xi_{\bm{x},j}^{n})\Bigr{]},\\ I_{K,\bm{a}_{\hat{k}},\text{conti}}^{(t)}=\frac{1}{B}\sum_{ \begin{subarray}{c}\hat{e}\in[\pm]\\ n\in\mathcal{V}_{k}^{\text{s}}\cap\mathcal{B}_{t}\end{subarray}}\Bigl{[}\ell _{n}^{\prime}{}^{(t)}\bm{a}_{\hat{k}}^{\top}\mathbf{W}_{Q}^{\bm{x}}{}^{(t)}( \bm{a}_{\hat{k}}+\bm{e}\bm{b}_{\hat{k}}+\xi_{\bm{x},L+1}^{n}+\sum_{r\in \mathcal{M}_{L+1}^{\text{s}}}\mathbf{M}_{r})\sum_{i\in\mathcal{W}_{k,n}^{ \text{s}}}\mathbf{r}_{i}\mathbf{W}_{O_{(i,\cdot)}}^{\bm{y}}{}^{(t)}\\ &\bm{d}_{\hat{k}}\{(\sum_{l\in\mathcal{S}_{n,\hat{k}}^{\text{s}}} {}^{(t)}(\sigma_{S}^{(t)})_{l}^{n}-\sum_{l\in S_{n,\hat{k}}^{-\text{s}}}{}^{(t )}(\sigma_{S}^{(t)})_{l}^{n})(\bm{a}_{\hat{k}}{}^{\top}\xi_{\bm{x},l}^{n}- \sum_{j\in[L]}(\sigma_{S}^{(t)})_{j}^{n}\bm{a}_{\hat{k}}{}^{\top}\xi_{\bm{x}, j}^{n})\Bigr{\}}\Bigr{]}.\end{split}\] (17)

**Lemma 16**.: _(Label Semantic Learning of Attention) Also, for \(\forall\hat{k}\in[K_{1}]\), we have the single step of learning of the concept-specific semantically-opposite part of the features:_

\[\begin{split}\bm{b}_{\hat{k}}^{\top}\mathbf{W}_{Q}^{\bm{x}}{}^{(t +1)}\bm{b}_{\hat{k}}-\bm{b}_{\hat{k}}^{\top}\mathbf{W}_{Q}^{\bm{x}}{}^{(t)}\bm{b }_{\hat{k}}&=-\eta_{t}\cdot\bm{b}_{\hat{k}}{}^{\top}\nabla_{ \mathbf{W}_{Q}^{\bm{x}}{}^{(t)}}L_{\mathcal{B}_{t}}(\Psi^{(t)})\bm{b}_{\hat{k}} \\ &=-\eta_{t}(I_{Q_{k},\hat{k},\text{chas}}^{(t)}+I_{Q_{k},\hat{k}, \text{conti}}^{(t)})-\eta_{t}\lambda\bm{b}_{\hat{k}}^{\top}\mathbf{W}_{Q}^{\bm{ x}}{}^{(t)}\bm{b}_{\hat{k}},\\ \bm{b}_{\hat{k}}^{\top}\mathbf{W}_{K}^{\bm{x}}{}^{(t+1)}\bm{b}_{\hat {k}}-\bm{b}_{\hat{k}}^{\top}\mathbf{W}_{K}^{\bm{x}}{}^{(t)}\bm{b}_{\hat{k}}& =-\eta_{t}\cdot\bm{b}_{\hat{k}}^{\top}\nabla_{\mathbf{W}_{K}^{\bm{x}}{}^ {(t)}}L_{\mathcal{B}_{t}}(\Psi^{(t)})\bm{b}_{\hat{k}}\\ &=-\eta_{t}(I_{K,\bm{b}_{\hat{k}},\text{chas}}^{(t)}+I_{K,\bm{b}_{ \hat{k}},\text{conti}}^{(t)})-\eta_{t}\lambda\bm{b}_{\hat{k}}^{\top}\mathbf{W}_{K }^{\bm{x}}{}^{(t)}\bm{a}_{\hat{k}},\end{split}\] (18)

_where \(I_{Q,\bm{b}_{\hat{k}},\text{chas}}^{(t)}\) and \(I_{Q,\bm{b}_{\hat{k}},\text{conti}}^{(t)}\) are defined as below._

\[\begin{split} I_{Q,\bm{b}_{\hat{k}},\text{chas}}^{(t)}& =\frac{1}{B}\sum_{\begin{subarray}{c}k\neq\hat{k}\in[K_{1}]\\ n\in\mathcal{V}_{k}^{\text{s}}\cap\mathcal{B}_{t}\end{subarray}}\Bigl{[}e^{ \ell_{n}^{\prime}{}^{(t)}}\cdot\bm{b}_{\hat{k}}{}^{\top}(\xi_{\bm{x},L+1}^{n}+ \sum_{r\in\mathcal{M}_{L+1}^{\text{s}}}\mathbf{M}_{r})\sum_{i\in\mathcal{W}_{k,n}^ {\text{s}}(t)}\mathbf{r}_{i}\cdot\sum_{l,j\in[L]}(\sigma_{S}^{(t)})_{l}^{n}( \sigma_{S}^{(t)})_{j}^{n}\\ &(\mathbf{W}_{O_{(i,\cdot)}}^{\bm{y}}{}^{(t)}(\bm{q}_{k}^{y_{l}^{ \text{s}}}+\sum_{s\in\mathcal{M}_{l}^{\text{s}}}\mathbf{Q}_{S}+\xi_{\bm{y},l}^{n})) (\bm{b}_{\hat{k}}^{\top}\mathbf{W}_{K}^{\bm{x}}{}^{(t)}((y_{l}^{n}-y_{j}^{n}) \bm{b}_{k}+\sum_{s\in\mathcal{M}_{1}^{\text{s}}}\mathbf{M}_{s}+\xi_{\bm{x},l}^{n}- \sum_{s\in\mathcal{M}_{j}^{\text{s}}}\mathbf{M}_{s}-\xi_{\bm{x},j}^{n})) \Bigr{]}\\ &+\frac{1}{B}\sum_{\hat{e}\in[\pm]}\sum_{n\in\mathcal{V}_{k}^{\text{s}} \cap\mathcal{B}_{t}}\Bigl{[}\ell_{n}^{\prime}{}^{(t)}(\|\bm{b}_{\hat{k}}\|^{2}+ \hat{\bm{e}}\bm{b}_{\hat{k}}{}^{\top}(\xi_{\bm{x},L+1}^{n}+\sum_{r\in \mathcal{M}_{L+1}^{\text{s}}}\mathbf{M}_{r}))\sum_{i\in\mathcal{W}_{k,n}^{ \text{s}}(t)}\mathbf{r}_{i}\cdot\\_Similarly, \(I^{(t)}_{K,\bm{b}_{k},\bm{k}_{\text{c}},\text{chanor}}\) and \(I^{(t)}_{K,\bm{b}_{k},\bm{k}_{\text{c}},\text{chanor}}\) are defined as below._

\[\begin{split} I^{(t)}_{K,\bm{b}_{k},\bm{k}_{\text{c}},\text{chanor }}=&\frac{1}{B}\sum_{\begin{subarray}{c}k\neq k\in[K_{1}]\\ \in[\pm]\\ n\in\mathcal{V}_{k}^{\text{c}}\cap\mathcal{B}_{t}\end{subarray}}\Bigl{[}e^{ \prime}\cdot\ell_{n}^{(t)}\bm{b}_{k}^{\top}\mathbf{W}_{Q}^{\bm{\pi}}{}^{(t)}( \bm{a}_{k}+\bm{e}\bm{b}_{k}+\xi_{\bm{\pi},L+1}^{n}+\sum_{r\in\mathcal{M}_{L+1}^ {\text{m}}}\mathbf{M}_{r})\sum_{i\in\mathcal{W}_{k,n}^{\text{c}}}\mathbf{r}_{i} \sum_{l,j\in[L]}(\sigma_{S}^{(t)})_{l}^{n}(\sigma_{S}^{(t)})_{j}^{\tau}\\ &(\mathbf{W}_{O_{(i,\cdot)}}^{\bm{\eta}}{}^{(t)}(\bm{q}_{k}^{ \eta_{i}^{\eta}}+\sum_{s\in\mathcal{M}_{l}^{\eta}}\mathbf{Q}_{S}+\xi_{\bm{\eta },l}^{n}))(\bm{b}_{k}^{\top}(\sum_{s\in\mathcal{M}_{l}^{\eta}}\mathbf{M}_{s}+ \xi_{\bm{\pi},l}^{n}-\sum_{s\in\mathcal{M}_{j}^{\eta}}\mathbf{M}_{s}-\xi_{\bm{ \pi},j}^{n}))\Bigr{]}\\ &+\frac{1}{B}\sum_{\begin{subarray}{c}k\in[\pm]\\ \equiv\end{subarray}}\sum_{\begin{subarray}{c}k\neq k\in[K_{1}]\\ \in[\pm]\\ n\in\mathcal{V}_{k}^{\text{c}}\cap\mathcal{B}_{t}\end{subarray}}\Bigl{[}\ell_{n }^{(t)}\bm{b}_{k}^{\top}\mathbf{W}_{Q}^{\bm{\pi}}{}^{(t)}(\hat{e}\bm{a}_{\hat{ k}}+\bm{b}_{\hat{k}}+\hat{e}(\xi_{\bm{\pi},L+1}^{n}+\sum_{r\in\mathcal{M}_{L+1}^ {\text{m}}}\mathbf{M}_{r}))\sum_{i\in\mathcal{W}_{k,n}^{\text{c}}}\mathbf{r}_{ i}\cdot\\ &\{\sum_{l\in\mathcal{S}_{n,k}^{\text{c}}}\sum_{j\in S_{n,k}^{\text{c}}}( \sigma_{S}^{(t)})_{l}^{n}(\sigma_{S}^{(t)})_{j}^{n}(\mathbf{W}_{O_{(i,\cdot)} }^{\bm{\eta}}{}^{(t)}(\sum_{s\in\mathcal{M}_{l}^{\eta}}\mathbf{Q}_{S}+\xi_{\bm {\eta},l}^{n}))(2\|\bm{b}_{\hat{k}}\|^{2}+\bm{b}_{\hat{k}}^{\top}(\xi_{\bm{\pi },l}^{n}-\xi_{\bm{\pi},j}^{n}))\] \[+\sum_{l\in S_{n,k}^{\text{c}}}\sum_{j\in S_{n,k}^{\text{c}}} \bigl{(}\sigma_{S}^{(t)}\bigr{)}_{l}^{n}(\sigma_{S}^{(t)})_{j}^{n}(\mathbf{W} _{O_{(i,\cdot)}}^{\bm{\eta}}{}^{(t)}(\sum_{s\in\mathcal{M}_{l}^{\eta}}\mathbf{ Q}_{S}+\xi_{\bm{\eta},l}^{n}))(-2\|\bm{b}_{\hat{k}}\|^{2}+\bm{b}_{\hat{k}}^{\top}(\xi_{ \bm{\pi},l}^{n}-\xi_{\bm{\pi},j}^{n}))\bigr{]}\Bigr{]}\] \[I^{(t)}_{K,\bm{b}_{k},\bm{k},\text{conior}}= \frac{1}{B}\sum_{\begin{subarray}{c}k\in[\pm]\\ n\in\mathcal{V}_{k}^{\text{c}}\cap\mathcal{B}_{t}\end{subarray}}\Bigl{[}2\ell_{n }^{(t)}\bm{b}_{k}^{\top}\mathbf{W}_{Q}^{\bm{\pi}}{}^{(t)}(\hat{e}\bm{a}_{k}+\bm {b}_{\hat{k}}+\hat{e}(\xi_{\bm{\pi},L+1}^{n}+\sum_{r\in\mathcal{M}_{L+1}^{ \text{m}}}\mathbf{M}_{r}))\sum_{i\in\mathcal{W}_{k,n}^{\text{c}}}\mathbf{r}_{ i}\mathbf{W}_{O_{(i,\cdot)}}^{\bm{\eta}}{}^{(t)}\bm{d}_{\hat{k}}\] \[\{2(\sum_{j\in S_{n,k}^{\text{c}}}(\sigma_{S}^{(t)})_{j}^{n})( \sum_{j\in S_{n,k}^{\text{c}}}(\sigma_{S}^{(t)})_{j}^{n})\|\bm{b}_{\hat{k}}\|^{2 }+\sum_{e\in[\pm]}e\cdot(\sum_{j\in S_{n,k}^{\text{c}}}(\sigma_{S}^{(t)})_{j}^ {n})(\sum_{l\in S_{n,k}^{\text{c}}}(\sigma_{S}^{(t)})_{l}^{n})\bm{b}_{\hat{k}} ^{\top}\xi_{\bm{\pi},l}^{n}\Bigr{]}\Bigr{]}.\] (20)

## Appendix G Model details: MLP Part

**Lemma 17**.: _(Tensor Update)_

\[\begin{split}\mathbf{W}_{O_{(i,\cdot)}}^{\bm{\eta}}{}^{(t)}\bm{c}_{ \hat{k}}&=\alpha_{O_{(i,\cdot)},k}^{(0)}-\eta_{t}\sum_{t=0}^{T} \nabla_{\mathbf{W}_{O_{(i,\cdot)}}^{\bm{\eta}}{}^{(t)}}L_{\mathcal{B}_{\hat{k}}}( \Psi^{(t)})\bm{c}_{k},\\ \mathbf{W}_{O_{(i,\cdot)}}^{\bm{\eta}}{}^{(t)}\bm{d}_{\hat{k}}& =\beta_{O_{(i,\cdot)},k}^{(0)}-\eta_{t}\sum_{t=0}^{T}\nabla_{\mathbf{W }_{O_{(i,\cdot)}}^{\bm{\eta}}{}^{(t)}}L_{\mathcal{B}_{\hat{k}}}(\Psi^{(t)})\bm{d }_{\hat{k}},\end{split}\] (21)

**Lemma 18**.: _(Gradient Update) \(\nabla_{\mathbf{W}_{O_{(i,\cdot)}}^{\bm{\eta}}{}^{(t)}}L_{\mathcal{B}_{\hat{k}}}( \Psi^{(t)})\in\mathbb{R}^{1\times(d_{X}+d_{Y})}\) can be derived as_

\[\frac{1}{B}\sum_{\begin{subarray}{c}k\neq\hat{k}\in[K_{1}]\\ \in[\pm]\\ n\in\mathcal{V}_{k}^{\text{c}}\cap\mathcal{B}_{t}\end{subarray}}\Bigl{[}\ell_{n }^{(t)}\mathbf{r}_{i}\mathds{1}_{O_{(i)}}^{\bm{\eta}}{}^{(t)}\{(2\sum_{l\in S_{n,k}^{\text{c}}}(\sigma_{S}^{(t)})_{l}^{n}-1)\bm{d}_{\hat{k}}^{\top}+e\sum_{l\in[L]} (\sigma_{S}^{(t)})_{l}^{n}(\bm{c}_{k}+\sum_{s\in\mathcal{M}_{l}^{\eta}}\mathbf{ Q}_{S}+\xi_{\bm{\eta},l}^{n})^{\top}\}\Bigr{]}+\lambda\mathbf{W}_{O_{(i,\cdot)}}^{\bm{\eta}}{}^{(t)}.\] (22)

**Lemma 19**.: _(Concept Learning of MLP) For \(\forall i\in[m],\hat{k}\in[K_{1}]\),_

\[\begin{split}\mathbf{W}_{O_{(i,\cdot)}}^{\bm{\eta}}{}^{(t+1)}\bm{c}_{ \hat{k}}-\mathbf{W}_{O_{(i,\cdot)}}^{\bm{\eta}}{}^{(t)}\bm{c}_{\hat{k}}& =-\eta_{t}\cdot\nabla_{\mathbf{W}_{O_{(i,\cdot)}}^{\bm{\eta}}{}^{(t)}}L_{ \mathcal{B}_{\hat{k}}}(\Psi^{(t)})\bm{c}_{\hat{k}}\\ &=-\eta_{t}(I_{O_{(i,\cdot)},\bm{c}_{\hat{k}},\text{chanor}}^{(t)}+I_{O _{(i,\cdot)},\bm{c}_{\hat{k}},\text{count}}^{(t)})-\eta_{t}\lambda\mathbf{W}_{O_{(i, \cdot)}}^{\bm{\eta}}{}^{(t)}\bm{c}_{\hat{k}},\end{split}\] (23)

_where \(I_{O_{(i,\cdot)},\bm{c}_{\hat{k}},\text{chanor}}^{(t)}\)* _When the neuron is activated (i.e.,_ \(\{n\in\mathcal{V}_{k}^{\hat{e}}\cap\mathcal{B}_{\hat{t}}\)_, if_ \((\mathbf{W}_{K}^{\mathbf{w}}{}^{(t)}_{k}\mathbf{b}_{k})^{\top}\mathbf{W}_{Q}^{ \mathbf{w}}{}^{(t)}_{Q}\mathbf{b}_{k}>0\}\) _, and_ \(\alpha_{O_{(i,\cdot)},\hat{k}}^{(t)}+\hat{e}\cdot(2\sum_{l\in\mathcal{S}_{n,k}^ {\hat{e}}}{(\sigma_{S}^{(t)})}_{l}^{n}-1)\mathbf{W}_{O_{(i,\cdot)}}^{\mathbf{w }}{}^{(t)}\boldsymbol{d}_{\hat{k}}>0\)_), the neuron is likely to be activated (_\(i\in\mathcal{W}_{k,n}^{\hat{e}}(t)\)_)._

1. _If (1)_ \(\mathbf{r}_{i}\cdot\hat{e}>0,i\in\mathcal{W}_{\hat{k},n}^{\hat{e}}(t)\Leftrightarrow i \in\mathcal{W}_{\hat{k},n}^{\hat{e}}(t)\cap\mathcal{U}_{k,n}^{\hat{e}}(t)\)_, the gradient will advance the_ \(\mathbf{W}_{O_{(i,\cdot)}}^{\mathbf{w}}{}^{(t)}\boldsymbol{c}_{\hat{k}}\)_;_
2. _if (2)_ \(\mathbf{r}_{i}\cdot\hat{e}<0,i\in\mathcal{W}_{\hat{k},n}^{\hat{e}}(t) \Leftrightarrow i\in\mathcal{W}_{\hat{k},n}^{\hat{e}}(t)-\mathcal{U}_{\hat{k}, n}^{\hat{e}}(t)\)_, the gradient will diminish the_ \(\mathbf{W}_{O_{(i,\cdot)}}^{\mathbf{w}}{}^{(t)}\boldsymbol{c}_{\hat{k}}\)_, thus help deactivate this neuron._

**Lemma 20**.: _(Label Semantic Learning of MLP) For \(\forall i\in[m],\hat{k}\in[K_{1}]\),_

\[\begin{split}\mathbf{W}_{O_{(i,\cdot)}}^{\mathbf{w}}{}^{(t+1)} \boldsymbol{d}_{\hat{k}}-\mathbf{W}_{O_{(i,\cdot)}}^{\mathbf{w}}{}^{(t)} \boldsymbol{d}_{\hat{k}}&=-\eta_{t}\cdot\nabla_{\mathbf{W}_{O_{( i,\cdot)}}^{\mathbf{w}}{}^{(t)}_{(i,\cdot)}}L_{\mathcal{B}_{\hat{t}}}( \Psi^{(t)})\boldsymbol{d}_{\hat{k}}\\ &=-\eta_{t}(I_{O_{(i,\cdot)},\boldsymbol{d}_{\hat{k}},\text{chan }}^{(t)}+I_{O_{(i,\cdot)},\boldsymbol{d}_{\hat{k}},\text{comtri}}^{(t)})-\eta_ {t}\lambda\mathbf{W}_{O_{(i,\cdot)}}^{\mathbf{w}}{}^{(t)}\boldsymbol{d}_{\hat{ k}},\end{split}\] (25)

_where \(I_{O_{(i,\cdot)},\boldsymbol{d}_{\hat{k}},\text{chan}}^{(t)}\) and \(I_{O_{(i,\cdot)},\boldsymbol{d}_{\hat{k}},\text{comtri}}^{(t)}\) are defined as_

\[\begin{split} I_{O_{(i,\cdot)},\boldsymbol{d}_{\hat{k}},\text{chan }}^{(t)}&=\frac{1}{B}\sum_{k\in[K_{1}]}\sum_{e\in[\pm]}\sum_{n \in\mathcal{V}_{k}^{\hat{e}}\cap\mathcal{B}_{\hat{t}}}\Big{[}e\cdot\ell_{n}^{ \prime}{}^{(t)}\mathbf{r}_{i}\cdot\mathbf{I}_{O_{(i)}}^{n}\sum_{l\in[L]}( \sigma_{S}^{(t)})_{l}^{n}(\sum_{s\in\mathcal{M}_{\hat{t}}^{\hat{e}}}\mathbf{ Q}_{S}+\xi_{\mathbf{y},l}^{n})^{\top}\boldsymbol{d}_{\hat{k}}\Big{]},\\ I_{O_{(i,\cdot)},\boldsymbol{d}_{\hat{k}},\text{comtri}}^{(t)}& =\frac{1}{B}\sum_{\begin{subarray}{c}\hat{e}\in[\pm]\\ n\in\mathcal{V}_{k}^{\hat{e}}\cap\mathcal{B}_{\hat{t}}\end{subarray}}\Big{[} \ell_{n}^{\prime}{}^{(t)}\mathbf{r}_{i}\cdot\mathbf{I}_{O_{(i)}}^{n}(t)(\sum_{ l\in\mathcal{S}_{n,\hat{k}}^{\hat{e}}}{(\sigma_{S}^{(t)})}_{l}^{n}-\sum_{l\in S_{n, \hat{k}}^{\hat{e}}}{(\sigma_{S}^{(t)})}_{l}^{n})\|\boldsymbol{d}_{\hat{k}}\|^{ 2}\Big{]}.\end{split}\] (26)

## Appendix H Discussions over Parameter Settings

Note that we do not have any requirement upon demonstration length \(L\) and batch size \(B\) for training, thus the training can be really flexible compared with the strict requirement in [28]. The condition on dimensionality \(d_{\mathcal{X}},d_{\mathcal{Y}}\) and the network width \(m\) ensure the learning problem is in a sufficiently overparameterized setting where the norm and the inner products of the Gaussian noise and initialized NN can be controlled within a certain range with high probability \(1-\delta\), which is standard requirements in recent _feature learning_ line-of-research [41, 57, 53, 45, 58, 42, 52, 43]. The weak requirement on network width \(m\) allows us to conduct a fine-grained analysis based on the network projection length, which is fundamentally differs from the NTK line of research [92] that requires an infinitely wide network to perform linear regression over a prescribed feature map. The condition on \(\gamma\) ensures the learning step to be small and thus learning process enjoys an approximation to gradient flow rather than the challenging "Oscillation" regime [93], which is analyzable but not necessary in presenting our theory. The condition on the small \(\lambda\) is to ensure that the learning dynamic of Attention and MLP would not stuck at the origin point, and ensure that we can analyze the expected learning dynamic with limited impact of the regularization at the initial stage, which is also adopted in [53]. The condition on \(K\) is to control the impact of cross-concept contribution in the Attention's learning dynamic, which can actually be relaxed at the cost of a denser analysis. The condition on \(\sigma_{\xi}\) is to ensure that the impact of the norms and inner-products involving the Gaussian Noise on the gradient cannot surpass those in the order of feature's norms, which ensures the gradient flows to be not too noisy and could converge to the expected gradient flow exponentially. Last but not least, the conditions on \(\sigma_{1}\) guarantee that the initial beliefs of MLP is small and the gradients of SGD can update the model effectively. The condition of \(\sigma_{0}\) is only used when discussing the OOD scenario.

## Appendix I Convergence of Expectation

In this section, we assume all the events in the Section D hold, denoted as \(\Upsilon_{\text{pe}}\).

We examine the evolution of \(\mathbb{E}(\Psi^{\prime}{}^{t}):=\{\mathbb{E}(\mathbf{W}_{Q}^{\mathbf{w}}{}^{(t)} ),\mathbb{E}(\mathbf{W}_{K}^{\mathbf{w}}{}^{(t)}),\mathbb{E}(\mathbf{W}_{O_{(i, \cdot)}}^{(t)})\}\) at the whole iteration \(0\leq t\leq\underline{t}\), where the expectation \(\mathbb{E}[\cdot]\) is taken over the stochastic batches. As such, we can see every stochastic gradient update within each batch as a gradient update upon noise-free and category-balanced concept-specific prompts.

**Lemma 21**.: _For \(\forall k_{1}\in[K_{1}]\), we define \(\bm{a}_{k_{1}}:=\dfrac{\bm{\mu}_{k_{1}}^{+}+\bm{\mu}_{k_{1}}^{-}-\bm{\mu}_{k_{1} }^{-}}{2}\) and \(\bm{b}_{k_{1}}:=\dfrac{\bm{\mu}_{k_{1}}^{+}-\bm{\mu}_{k_{1}}^{-}}{2}\). By definition, we then have_

\[\bm{\mu}_{k_{1}}^{+}=\bm{a}_{k_{1}}+\bm{b}_{k_{1}},\quad\bm{\mu}_ {k_{1}}^{-}=\bm{a}_{k_{1}}-\bm{b}_{k_{1}},\] \[\langle\bm{a}_{k_{1}},\bm{b}_{k_{1}}\rangle=0,\quad\{\bm{a}_{k_{1 }},\bm{b}_{k_{1}}\}\perp\{\bm{a}_{k_{1}}^{+},\bm{b}_{k_{1}}^{+}\},\] (27) \[\langle\bm{\mu}_{k_{1}}^{+},\bm{\mu}_{k_{1}}^{-}\rangle=\|\bm{a}_ {k_{1}}\|^{2}-\|\bm{b}_{k_{1}}\|^{2},\quad\|\bm{\mu}_{k_{1}}^{+}\|^{2}=\|\bm{a }_{k_{1}}\|^{2}+\|\bm{b}_{k_{1}}\|^{2}=\|\bm{u}\|^{2},\] \[\dfrac{1}{2}\|\bm{u}\|^{2}<\|\bm{a}_{k_{1}}\|^{2}\leq\dfrac{\bm{ \kappa}_{x}+1}{2}\|\bm{u}\|^{2},\quad\dfrac{-\bm{\kappa}_{x}+1}{2}\|\bm{u}\|^{ 2}\leq\|\bm{b}_{k_{1}}\|^{2}<\dfrac{1}{2}\|\bm{u}\|^{2},\]

_for \(\forall k_{1}^{\prime}\neq k_{1}\in[K_{1}]\)._

**Remark 3**.: _We observe that, through this formulation, the shared component \(\bm{a}_{k_{1}}\) can be interpreted as the "concept" part of the two features, while the terms \(\pm\bm{b}_{k_{1}}\) represent their opposing semantic aspects. The relevance of this modeling is exemplified by Figure 1(b) in [12], where the concept "[Bird]" is composed of orthogonal steering vectors: "plant \(\Rightarrow\) animal" and "mammal \(\Rightarrow\) bird." These vectors correspond to the concept feature \(\bm{a}_{k}\) and the semantic label features \(\bm{b}_{k}\), respectively._

**Idempotent Operator Trick**. Define \(\mathbb{U}:=\text{span}(\mathbf{M})\) and its complement space \(\mathbb{U}^{\perp}\). By definition, we know that \(\text{dim}(\mathbb{U})=K\) and \(\text{dim}(\mathbb{U}^{\perp})=d_{\mathcal{X}}-K\). Then we can have a set of standard orthogonal basis for \(\mathbb{R}^{d}\), defined as

\[\beta_{\mathbb{U}\oplus\mathbb{U}^{\perp}}=\{\dfrac{\bm{a}_{1}}{\|\bm{a}_{1} \|},\dfrac{\bm{b}_{1}}{\|\bm{b}_{1}\|},\dfrac{\bm{a}_{2}}{\|\bm{a}_{2}\|},\dfrac {\bm{b}_{2}}{\|\bm{b}_{2}\|},\cdots,\dfrac{\bm{a}_{K_{1}}}{\|\bm{a}_{K_{1}}\| },\dfrac{\bm{b}_{K_{1}}}{\|\bm{b}_{\bm{b}_{1}}\|},\dfrac{\bm{\nu}_{1}}{\|\bm{ u}\|},\dfrac{\bm{\nu}_{2}}{\|\bm{u}\|},\dfrac{\bm{\nu}_{K_{2}}}{\|\bm{u}\|}, \bm{u}_{1}^{\perp},\cdots,\bm{u}_{d_{\mathcal{X}}-K}^{\perp}\},\]

where \(\bm{u}_{1}^{\perp},\cdots,\bm{u}_{d_{\mathcal{X}}-K}^{\perp}\) are the standard orthogonal basis of \(\mathbb{U}^{\perp}\). Then we can derive that

\[\sum_{s=1}^{K_{1}}\dfrac{\bm{a}_{s}\bm{a}_{s}^{\top}}{\|\bm{a}_{s}\|^{2}}+ \sum_{s=1}^{K_{1}}\dfrac{\bm{b}_{s}\bm{b}_{s}^{\top}}{\|\bm{b}_{s}\|^{2}}+ \sum_{r=1}^{K_{2}}\dfrac{\bm{\nu}_{r}\bm{\nu}_{r}^{\top}}{\|\bm{u}\|^{2}}+ \sum_{w=1}^{d_{\mathcal{X}}-K}\bm{u}_{w}^{\perp}\bm{u}_{w}^{\perp\top}=\mathbf{ I}_{d_{\mathcal{X}}\times d_{\mathcal{X}}}.\] (28)

**Lemma 22**.: _(Partial Statement of Lemma 1). \(\mathbb{E}[\mathbf{W}_{Q}^{\bm{\alpha}}]\) and \(\mathbb{E}[\mathbf{W}_{K}^{\bm{\alpha}}]\) are identical and symmetric during the whole iterations. We can decompose \(\mathbb{E}[\mathbf{W}_{Q}^{\bm{\alpha}}{}^{(t)}]\) and \(\mathbb{E}[\mathbf{W}_{K}^{\bm{\alpha}}{}^{(t)}]\) by (scaled) idempotent matrices._

\[\mathbb{E}[\mathbf{W}_{Q}^{\bm{\alpha}}{}^{(t)}] =\sum_{s=1}^{K_{1}}\alpha_{Q,s}^{(t)}\cdot\dfrac{\bm{a}_{s}\bm{a }_{s}^{\top}}{\|\bm{a}_{s}\|^{4}}+\sum_{s=1}^{K_{1}}\beta_{Q,s}^{(t)}\cdot \dfrac{\bm{b}_{s}\bm{b}_{s}^{\top}}{\|\bm{b}_{s}\|^{4}}+\sum_{r=1}^{K_{2}}\tau_{ Q,r}^{(t)}\cdot\dfrac{\bm{\nu}_{r}\bm{\nu}_{r}^{\top}}{\|\bm{u}\|^{4}}+\sum_{w=1}^{d_{ \mathcal{X}}-K}\rho_{Q,w}^{(t)}\cdot\bm{u}_{w}^{\perp}\bm{u}_{w}^{\perp\top},\] (29) \[\mathbb{E}[\mathbf{W}_{K}^{\bm{\alpha}}{}^{(t)}] =\sum_{s=1}^{K_{1}}\alpha_{K,s}^{(t)}\cdot\dfrac{\bm{a}_{s}\bm{a }_{s}^{\top}}{\|\bm{a}_{s}\|^{4}}+\sum_{s=1}^{K_{1}}\beta_{K,s}^{(t)}\cdot \dfrac{\bm{b}_{s}\bm{b}_{s}^{\top}}{\|\bm{b}_{s}\|^{4}}+\sum_{r=1}^{K_{2}}\tau_{ K,r}^{(t)}\cdot\dfrac{\bm{\nu}_{r}\bm{\nu}_{r}^{\top}}{\|\bm{u}\|^{4}}+\sum_{w=1}^{d_{ \mathcal{X}}-K}\rho_{K,w}^{(t)}\cdot\bm{u}_{w}^{\perp}\bm{u}_{w}^{\perp\top},\]

_where \(\alpha_{Q,s}^{(t)}\) and \(\alpha_{K,s}^{(t)}\) represent the concept learning process, \(\beta_{Q,s}^{(t)}\), and \(\beta_{K,s}^{(t)}\), represent the concept-specific semantic learning process and \(\tau_{Q,r}^{(t)},\tau_{K,r}^{(t)},\rho_{Q,w}^{(t)},\rho_{K,w}^{(t)}\) represent the memorization of the concept irrelevant noise._

Proof.: Apparently they hold at \(t=0\), suppose it holds at step \(t\), thus

\[\mathbb{E}[\mathbf{W}_{K}^{\bm{\alpha}}{}^{(t)}]=\mathbb{E}[(\mathbf{W}_{K}^{\bm{ \alpha}}{}^{(t)})^{\top}]=\mathbb{E}[\mathbf{W}_{Q}^{\bm{\alpha}}{}^{(t)}]= \mathbb{E}[(\mathbf{W}_{Q}^{\bm{\alpha}}{}^{(t)})^{\top}],\]

we examine \(t+1\). It holds that

\[\mathbb{E}_{\mathcal{B}_{t}}[\mathbf{W}_{K}^{(t+1)}\mid\mathbb{E} (\Psi^{\prime}{}^{(t)})]=\mathbb{E}[\mathbf{W}_{K}^{\bm{\alpha}}{}^{(t)}]-\eta_{t }\mathbb{E}_{\mathcal{B}_{t}}[\partial_{\mathbf{W}_{K}^{\bm{\alpha}}{}^{(t)}}L_{ \mathcal{B}_{t}}(\mathbb{E}(\Psi^{\prime}{}^{(t)}))]\] \[\mathbb{E}_{\mathcal{B}_{t}}[\mathbf{W}_{Q}^{(t+1)}\mid\mathbb{E} (\mathbb{E}(\Psi^{\prime}{}^{(t)}))]=\mathbb{E}[\mathbf{W}_{Q}^{\bm{ \alpha}}{}^{(t)}]-\eta_{t}\mathbb{E}_{\mathcal{B}_{t}}[\partial_{\mathbf{W}_{Q}^{ \bm{\alpha}}{}^{(t)}}L_{\mathcal{B}_{t}}(\mathbb{E}(\Psi^{\prime}{}^{(t)}))]\]

Here, we see \(\mathbb{E}(\Psi^{\prime}{}^{(t)})\) as fixed matrices and the expectation \(\mathbb{E}_{\mathcal{B}_{t}}[\cdot]\

[MISSING_PAGE_EMPTY:28]

**Lemma 24**.: _(Coefficient Update) Denote \(\mathbb{E}(\Psi^{\prime(t)}):=\{\mathbb{E}(\mathbf{W}_{Q}^{\mathbf{x}}(t)), \mathbb{E}(\mathbf{W}_{K}^{\mathbf{x}}(t)),\mathbb{E}(\mathbf{W}_{O_{(i,\cdot)}} ^{(t)})\}\), where the expectation \(\mathbb{E}[\cdot]\) is taken over the stochastic batches. We have_

\[\alpha_{Q,s}^{(T)} =\alpha_{Q,s}^{(0)}-\eta_{t}\sum_{t=1}^{T}{\boldsymbol{a}_{s}}^{ \top}\nabla_{\mathbf{W}_{Q}^{\mathbf{x}}(t)}\mathbb{E}_{\mathcal{B}_{t}}[L_{ \mathcal{B}_{t}}(\Psi^{\prime(t)})\mid\mathbb{E}(\Psi^{\prime(t-1)})]{ \boldsymbol{a}_{s}},\] \[\alpha_{K,s}^{(T)} =\alpha_{K,s}^{(0)}-\eta_{t}\sum_{t=1}^{T}{\boldsymbol{a}_{s}}^{ \top}\nabla_{\mathbf{W}_{K}^{\mathbf{x}}(t)}\mathbb{E}_{\mathcal{B}_{t}}[L_{ \mathcal{B}_{t}}(\Psi^{\prime(t)})\mid\mathbb{E}(\Psi^{\prime(t-1)})]{ \boldsymbol{a}_{s}},\] \[\beta_{Q,s}^{(T)} =\beta_{Q,s}^{(0)}-\eta_{t}\sum_{t=1}^{T}{\boldsymbol{b}_{s}}^{ \top}\nabla_{\mathbf{W}_{Q}^{\mathbf{x}}(t)}\mathbb{E}_{\mathcal{B}_{t}}[L_{ \mathcal{B}_{t}}(\Psi^{\prime(t)})\mid\mathbb{E}(\Psi^{\prime(t-1)})]{ \boldsymbol{b}_{s}},\] \[\beta_{K,s}^{(T)} =\beta_{K,s}^{(0)}-\eta_{t}\sum_{t=1}^{T}{\boldsymbol{b}_{s}}^{ \top}\nabla_{\mathbf{W}_{K}^{\mathbf{x}}(t)}\mathbb{E}_{\mathcal{B}_{t}}[L_{ \mathcal{B}_{t}}(\Psi^{\prime(t)})\mid\mathbb{E}(\Psi^{\prime(t-1)})]{ \boldsymbol{b}_{s}},\] \[\tau_{Q,r}^{(T)} =\alpha_{Q,r}^{(0)}-\eta_{t}\sum_{t=1}^{T}{\boldsymbol{\nu}_{r}}^ {\top}\nabla_{\mathbf{W}_{Q}^{\mathbf{x}}(t)}\mathbb{E}_{\mathcal{B}_{t}}[L_{ \mathcal{B}_{t}}(\Psi^{\prime(t)})\mid\mathbb{E}(\Psi^{\prime(t-1)})]{ \boldsymbol{\nu}_{r}},\] \[\tau_{K,r}^{(T)} =\tau_{K,r}^{(0)}-\eta_{t}\sum_{t=1}^{T}{\boldsymbol{\nu}_{r}}^ {\top}\nabla_{\mathbf{W}_{K}^{\mathbf{x}}(t)}\mathbb{E}_{\mathcal{B}_{t}}[L_{ \mathcal{B}_{t}}(\Psi^{\prime(t)})\mid\mathbb{E}(\Psi^{\prime(t-1)})]{ \boldsymbol{\nu}_{r}},\] (35) \[\rho_{Q,w}^{(T)} =\rho_{Q,w}^{(0)}-\eta_{t}\sum_{t=1}^{T}{\boldsymbol{u}_{w}}^{ \perp}\nabla_{\mathbf{W}_{Q}^{\mathbf{x}}(t)}\mathbb{E}_{\mathcal{B}_{t}}[L_{ \mathcal{B}_{t}}(\Psi^{\prime(t)})\mid\mathbb{E}(\Psi^{\prime(t-1)})]{ \boldsymbol{u}_{w}^{\perp}},\] \[\rho_{K,w}^{(T)} =\rho_{K,w}^{(0)}-\eta_{t}\sum_{t=1}^{T}{\boldsymbol{u}_{w}}^{ \perp}\nabla_{\mathbf{W}_{K}^{\mathbf{x}}(t)}\mathbb{E}_{\mathcal{B}_{t}}[L_{ \mathcal{B}_{t}}(\Psi^{\prime(t)})\mid\mathbb{E}(\Psi^{\prime(t-1)})]{ \boldsymbol{u}_{w}^{\perp}},\] \[\alpha_{O_{(i,\cdot)},k}^{(T)} =\alpha_{O_{(i,\cdot)},k}^{(0)}-\eta_{t}\sum_{t=1}^{T}\nabla_{ \mathbf{W}_{O_{(i,\cdot)}}(t)}\mathbb{E}_{\mathcal{B}_{t}}[L_{\mathcal{B}_{t}} (\Psi^{\prime(t)})\mid\mathbb{E}(\Psi^{\prime(t-1)})]{\boldsymbol{c}_{k}},\] \[\beta_{O_{(i,\cdot)},k}^{(T)} =\beta_{O_{(i,\cdot)},k}^{(0)}-\eta_{t}\sum_{t=1}^{T}\nabla_{ \mathbf{W}_{O_{(i,\cdot)}}(t)}\mathbb{E}_{\mathcal{B}_{t}}[L_{\mathcal{B}_{t}} (\Psi^{\prime(t)})\mid\mathbb{E}(\Psi^{\prime(t-1)})]{\boldsymbol{d}_{k}},\] \[\rho_{O_{(i,\cdot)},w}^{(T)} =\rho_{O_{(i,\cdot)},w}^{(0)}-\eta_{t}\sum_{t=1}^{T}\nabla_{ \mathbf{W}_{O_{(i,\cdot)}}(t)}\mathbb{E}_{\mathcal{B}_{t}}[L_{\mathcal{B}_{t}} (\Psi^{\prime(t)})\mid\mathbb{E}(\Psi^{\prime(t-1)})]{\boldsymbol{q}_{w}^{ \perp}},\]

_where \(e\in[\pm],s\in[K_{1}],r\in[K_{2}],w\in[d_{\mathcal{X}}-K]\)._

**Lemma 25**.: _For \(\forall k_{1}\in[K_{1}]\), we define \({\boldsymbol{c}_{k_{1}}}:=\dfrac{{\boldsymbol{q}_{k_{1}}^{+}+{ \boldsymbol{q}_{k_{1}}^{-}}}}{2}\) and \({\boldsymbol{d}_{k_{1}}}:=\dfrac{{\boldsymbol{q}_{k_{1}}^{+}-{\boldsymbol{q}_{ k_{1}}^{-}}}}{2}\). By definition, we then have_

\[{\boldsymbol{q}_{k_{1}}^{+}}={\boldsymbol{c}_{k_{1}}}+{ \boldsymbol{d}_{k_{1}}},\quad{\boldsymbol{q}_{k_{1}}^{-}}={\boldsymbol{c}_{k_{1} }}-{\boldsymbol{d}_{k_{1}}},\] \[\langle{\boldsymbol{c}_{k_{1}}},{\boldsymbol{d}_{k_{1}}}\rangle=0, \quad\{{\boldsymbol{c}_{k_{1}}},{\boldsymbol{d}_{k_{1}}}\}\perp\{{\boldsymbol{c}_{k_{ 1}^{\prime}},{\boldsymbol{d}_{k_{1}^{\prime}}}\},\] \[\langle{\boldsymbol{q}_{k_{1}}^{+}},{\boldsymbol{q}_{k_{1}}^{-}} \rangle=\left\|{\boldsymbol{c}_{k_{1}}}\right\|^{2}-\left\|{\boldsymbol{d}_{k_{1} }}\right\|^{2},\quad\left\|{\boldsymbol{q}_{k_{1}}^{\pm}}\right\|^{2}=\left\|{ \boldsymbol{c}_{k_{1}}}\right\|^{2}+\left\|{\boldsymbol{d}_{k_{1}}}\right\|^{2} =\left\|{\boldsymbol{u}}\right\|^{2},\] (36) \[\frac{1}{2}\|{\boldsymbol{q}}\|^{2}<\|{\boldsymbol{c}_{k_{1}}}\|^{2} \leq\frac{\kappa_{\mathbf{y}}+1}{2}\|{\boldsymbol{q}}\|^{2},\quad\frac{-\kappa_{ \mathbf{y}}+1}{2}\|{\boldsymbol{q}}\|^{2}\leq\|{\boldsymbol{d}_{k_{1}}}\|^{2}< \frac{1}{2}\|{\boldsymbol{q}}\|^{2},\]

_for \(\forall k_{1}^{\prime}\neq k_{1}\in[K_{1}]\)._

Based on Lemma 22 and Lemma 24, the following two lemmas compute the update of attention's expected projection along non-feature and feature directions.

**Lemma 26**.: _For \(t>0\), we have_

\[\tau_{Q,r}^{(t+1)}=(1-\eta_{t}\lambda)\tau_{Q,r}^{(t)},\quad\tau_{K,r}^{(t+1)}=(1- \eta_{t}\lambda)\tau_{K,r}^{(t)},\] \[\rho_{Q,w}^{(t+1)}=(1-\eta_{t}\lambda)\rho_{Q,w}^{(t)},\quad\rho_{K,w }^{(t+1)}=(1-\eta_{t}\lambda)\rho_{K,r}^{(t)},\] (37) \[\rho_{O_{(i,\cdot)},\hat{w}}^{(t+1)}=(1-\eta_{t}\lambda)\rho_{O_{(i, \cdot)},\hat{w}}^{(t)},\]

_where \(r\in[K_{2}],w\in[d_{\mathcal{X}}-K],\hat{w}\in[d_{\mathcal{Y}}-K_{1}]\)._

**Lemma 27**.: _For \(t>0\), we have_

\[\alpha_{Q,k}^{(t+1)}=(1-\eta_{t}\lambda)\alpha_{Q,k}^{(t)},\quad \alpha_{K,k}^{(t+1)}=(1-\eta_{t}\lambda)\alpha_{K,k}^{(t)},\] \[\beta_{Q,k}^{(t+1)}=(1-\eta_{t}\lambda)\beta_{Q,k}^{(t)},\] \[\qquad\qquad-\frac{4\eta_{t}\beta_{K,k}^{(t)}\|\bm{b}_{k}\|^{4}}{K _{1}}\sum_{e\in[\pm]}\sum_{i\in[m]}\mathbf{r}_{i}\beta_{O_{(i,\cdot)},k}^{(t)} \underset{n\in\mathcal{V}_{k}^{\text{eq}}}{\mathbb{E}}[\ell_{n}^{\prime\;(t)} \mathds{1}_{O_{(i)}}^{\;\;(t)}(\sum_{j\in S_{n,k}^{+}}(\sigma_{S}^{(t)})_{j}^{ n})(\sum_{j\in S_{n,k}^{-}}(\sigma_{S}^{(t)})_{j}^{n})],\] \[\beta_{K,k}^{(t+1)}=(1-\eta_{t}\lambda)\beta_{K,k}^{(t)}\] \[\qquad\qquad-\frac{4\eta_{t}\beta_{Q,k}^{(t)}\|\bm{b}_{k}\|^{4}}{K _{1}}\sum_{e\in[\pm]}\sum_{i\in[m]}\mathbf{r}_{i}\beta_{O_{(i,\cdot)},k}^{(t)} \underset{n\in\mathcal{V}_{k}^{\text{eq}}}{\mathbb{E}}[\ell_{n}^{\prime\;(t)} \mathds{1}_{O_{(i)}}^{\;\;(t)}(\sum_{j\in S_{n,k}^{+}}(\sigma_{S}^{(t)})_{j}^{ n})(\sum_{j\in S_{n,k}^{-}}(\sigma_{S}^{(t)})_{j}^{n})].\] (38)

Proof.: The deduction is direct by the symmetric property of prompt distribution in Lemma 22, and the gradient forms in Lemma 15 and Lemma 16. 

This lemma reveals that the attention layer mainly serves to learn the different semantic part of each concept, and hardly have interest in learning the shared co-concept part. Also, collaborating with Lemma 22, we see that \(\beta_{Q,k}^{(t+1)}=\beta_{K,k}^{(t+1)}\), this indicates that the signal of \(\beta_{Q,k}^{(t)}\cdot\beta_{K,k}^{(t)}\) would remain positive.

Also, by the symmetry property of learning progress denoted in Lemma 22, we see that \(\forall k\in[K_{1}],\alpha_{Q,k}^{(t)}=\alpha_{K,k}^{(t)},\beta_{Q,k}^{(t)}= \beta_{K,k}^{(t)}\). Observe that for \(\forall k\in[K_{1}],\)

\[\underset{n\in\mathcal{V}_{k}^{\text{eq}}}{\mathbb{E}}[\sum_{j \in S_{n,k}^{\text{eq}}}(\sigma_{S}^{(t)})_{j}^{n}]=\frac{\exp(\beta_{Q,k}^{(t )}\cdot\beta_{K,k}^{(t)}/\|\bm{b}_{k}\|^{2})}{\exp(\beta_{Q,k}^{(t)}\cdot\beta _{K,k}^{(t)}/\|\bm{b}_{k}\|^{2})+\exp(-\beta_{Q,k}^{(t)}\cdot\beta_{K,k}^{(t)}/ \|\bm{b}_{k}\|^{2})},\] (39) \[\underset{n\in\mathcal{V}_{k}^{\text{eq}}}{\mathbb{E}}[\sum_{j \in S_{n,k}^{-}}(\sigma_{S}^{(t)})_{j}^{n}]=\frac{\exp(-\beta_{Q,k}^{(t)} \cdot\beta_{K,k}^{(t)}/\|\bm{b}_{k}\|^{2})}{\exp(\beta_{Q,k}^{(t)}\cdot\beta _{K,k}^{(t)}/\|\bm{b}_{k}\|^{2})+\exp(-\beta_{Q,k}^{(t)}\cdot\beta_{K,k}^{(t)}/ \|\bm{b}_{k}\|^{2})}.\]

We see from Lemma 7 that \(\alpha_{Q,k}^{(0)}=\alpha_{K,k}^{(0)}=\sigma_{0}\|\bm{a}_{k}\|^{2},\beta_{Q,k}^ {(0)}=\beta_{K,k}^{(0)}=\sigma_{0}\|\bm{b}_{k}\|^{2}\). Therefore, for \(t=0,\forall k\in[K_{1}]\), we have

\[\underset{n\in\mathcal{V}_{k}^{\text{eq}}}{\mathbb{E}}[\sum_{j \in S_{n,k}^{\text{eq}}}(\sigma_{S}^{(0)})_{j}^{n}]=\frac{\exp(\sigma_{0}^{2} \|\bm{b}_{k}\|^{2})}{\exp(\sigma_{0}^{2}\|\bm{b}_{k}\|^{2})+\exp(-\sigma_{0}^{2} \|\bm{b}_{k}\|^{2})},\] (40) \[\underset{n\in\mathcal{V}_{k}^{\text{eq}}}{\mathbb{E}}[\sum_{j \in S_{n,k}^{-}}(\sigma_{S}^{(0)})_{j}^{n}]=\frac{\exp(-\sigma_{0}^{2}\|\bm{b} _{k}\|^{2})}{\exp(\sigma_{0}^{2}\|\bm{b}_{k}\|^{2})+\exp(-\sigma_{0}^{2}\|\bm {b}_{k}\|^{2})}.\]

Obviously, \(\underset{n\in\mathcal{V}_{k}^{\text{eq}}}{\mathbb{E}}[\sum_{j\in S_{n,k}^{ \text{eq}}}(\sigma_{S}^{(0)})_{j}^{n}]>0.5>\underset{n\in\mathcal{V}_{k}^{ \text{eq}}}{\mathbb{E}}[\sum_{j\in S_{n,k}^{-}}(\sigma_{S}^{(0)})_{j}^{n}]\). Meanwhile we see that \(\underset{n\in\mathcal{V}_{k}^{\text{eq}}}{\mathbb{E}}[\sum_{j\in S_{n,k}^{ \text{eq}}}(\sigma_{S}^{(0)})_{j}^{n}]\approx 0\) due to the small \(\sigma_{0}=O(\|\mathbf{u}\|^{-2})\) by Condition 1.

The observation in Eq. (39), collaborating with the positiveness of \(\beta_{Q,k}^{(t)}\cdot\beta_{K,k}^{(t)}\), we see that the inequality \(\underset{n\in\mathcal{V}_{k}^{\text{eq}}}{\mathbb{E}}[\sum_{j\in S_{n,k}^{ \text{eq}}}(\sigma_{S}^{(t)})_{j}^{n}]>\underset{n\in\mathcal{V}_{k}^{\text{eq} }}{\mathbb{E}}[\sum_{j\in S_{n,k}^{-}}(\sigma_{S}^{(t)})_{j}^{n}]\) will remain during whole iteration. Also, by Eq. (39), we know that

\[\mathbb{E}[(\sum_{j\in S_{n,k}^{+}}(\sigma_{S}^{(t)})_{j}^{n})(\sum_{j\in S_{n,k}^ {-}}(\sigma_{S}^{(t)})_{j}^{n})]=\Big{(}\exp(\beta_{Q,k}^{(t)}\cdot\beta_{K,k}^ {(t)}/\|\bm{b}_{k}\|^{2})+\exp(-\beta_{Q,k}^{(t)}\cdot\beta_{K,k}^{(t)}/\|\bm{b }_{k}\|^{2})\Big{)}^{-2}.\] (41)

This observation under our expectation scenario greatly facilitate our analysis. Since \(\ell_{n}^{\prime\;(t)}<0\), it's obvious that the signal of \(\mathbf{r}_{i}\beta_{O_{(i,\cdot)},k}^{(t)}\) will determine whether the neuron \(i\in[m]\) will serve to increase or decrease the \(\beta_{Q,k}^{(t)}\) and \(\beta_{K,k}^{(t)}\) during the gradient update. We therefore start to analyze the MLP's update below based on Lemma 16.

**Lemma 28**.: _For \(t>0\), we have_

\[\alpha^{(t+1)}_{O_{(i,\cdot)},k}=(1-\eta_{t}\lambda)\alpha^{(t)}_{O_ {(i,\cdot)},k} -\eta_{t}\underbrace{\|\underline{c}_{k}\|^{2}}_{2K_{1}}\sum_{e\in[ \pm]}[\underline{\mathrm{er}}_{i}\cdot\underset{n\in\mathcal{V}_{k}^{e}}{ \mathbb{E}(\ell^{\prime}_{n}{}^{(t)}\mathds{1}^{n}_{O_{(i)}}{}^{(t)})]}_{ \mathbb{E}(\ell^{(t)}_{O_{(i,\cdot)},e_{k,\mathrm{diss}}})}\] (42) \[-\eta_{t}\underbrace{\frac{(K_{1}-1)\|\underline{c}_{k}\|^{2}}{2K _{1}K}\sum_{e\in[\pm]}[\underline{\mathrm{er}}_{i}\cdot\underset{n\in \mathcal{V}_{k}^{e}}{\mathbb{E}(\ell^{\prime}_{n}{}^{(t)}\mathds{1}^{n}_{O_{( i)}}{}^{(t)})]}}_{\mathbb{E}(\ell^{(t)}_{O_{(i,\cdot)},e_{k,\mathrm{diss}}})}\] \[\beta^{(t+1)}_{O_{(i,\cdot)},k}=(1-\eta_{t}\lambda)\beta^{(t)}_{O _{(i,\cdot)},k} -\frac{\eta_{t}\|\underline{d}_{k}\|^{2}\mathbf{r}_{i}}{2K_{1}} \sum_{e\in[\pm]}\underset{n\in\mathcal{V}_{k}^{e}}{\mathbb{E}(\ell^{\prime}_{n }{}^{(t)}\mathds{1}^{n}_{O_{(i)}}{}^{(t)}(\sum_{l\in\mathcal{S}_{n,k}^{e}}{ \left(\sigma_{S}^{(t)}\right)_{l}^{n}}\] \[-\sum_{l\in\mathcal{S}_{n,k}^{-e}}{\left(\sigma_{S}^{(t)}\right) _{l}^{n}})],\]

_where \(k\in[K_{1}]\)._

Proof.: The proof is direct by the symmetric property of prompt distribution in Lemma 22, and the gradient forms in Lemma 19 and Lemma 20. 

An interesting fact is that the \(\mathbb{E}(I^{(t)}_{O_{(i,\cdot)},e_{k,\mathrm{chas}}})\) also contributes to the learning of \(k\)-th concept. This actually suits our intuition that if similar things appear in various fields (concepts), the learning process can help integrate and facilitate the learning. The following lemma demonstrate the lower bound of the attention assignment, which emerge from the good property of our expected attention.

**Lemma 29**.: _For a certain iterations \(t\in(0,T_{1})\), for \(\forall k\in[K_{1}],e\in[\pm]\), we have_

1. _The neuron set_ \(\mathbb{E}[(\mathcal{W}_{k,n}^{e}(t)-\mathcal{U}_{k,n}^{e}(t))-\mathcal{U}_{k,n}^{-e}(t)]\) _is non-increasing, and all of this neuron will get deactivated. Additionally, both_ \(\mathbb{E}[\alpha^{(t)}_{O_{(i,\cdot)},k}]\) _and_ \(e\cdot\beta^{(t)}_{O_{(i,\cdot)},k}\) _would monotonically decrease. Also, it holds that_ \(e\cdot\beta^{(t)}_{O_{(i,\cdot)},k}>0\) _and_ \(|\alpha^{(t)}_{O_{(i,\cdot)},k}|\leq e\cdot\beta^{(t)}_{O_{(i,\cdot)},k}\)_;_
2. _The neuron set_ \(\mathbb{E}[\mathcal{U}_{k,n}^{e}(t)-(\mathcal{W}_{k,n}^{-e}(t)-\mathcal{U}_{k,n}^{-e}(t))]\) _is non-increasing, and all neurons in it will turn into_ \(\underset{n\in\mathcal{V}_{k}^{e}}{\mathbb{E}[\mathcal{U}_{k,n}^{e}(t)\cap( \mathcal{W}_{k,n}^{-e}(t)-\mathcal{U}_{k,n}^{-e}(t))]}\)_. Additionally, both_ \(\mathbb{E}[\alpha^{(t)}_{O_{(i,\cdot)},k}]\) _and_ \(e\cdot\beta^{(t)}_{O_{(i,\cdot)},k}\) _would monotonically increase. Also, it holds that_ \(e\cdot\beta^{(t)}_{O_{(i,\cdot)},k}>0\) _and_ \(|\alpha^{(t)}_{O_{(i,\cdot)},k}|\leq e\cdot\beta^{(t)}_{O_{(i,\cdot)},k}\)_;_
3. _For_ \(\underset{n\in\mathcal{V}_{k}^{e}}{\mathbb{E}[\mathcal{U}_{k,n}^{e}(t)\cap( \mathcal{W}_{k,n}^{-e}(t)-\mathcal{U}_{k,n}^{-e}(t))]}\)_, the_ \(e\cdot\beta^{(t)}_{O_{(i,\cdot)},k}\) _would monotonically increase. Besides, when there exists constant_ \(C\geq 1\) _such that_ \[\underset{n\in\mathcal{V}_{k}^{e}}{\mathbb{E}(\ell^{\prime}_{n}{}^{(t)})]} \leq C\underset{n\in\mathcal{V}_{k}^{-e}}{\mathbb{E}(\ell^{\prime}_{n}{}^{(t)} )]}.\] _the_ \(\mathbb{E}[\alpha^{(t)}_{O_{(i,\cdot)},k}]\) _would be contributed to increase, otherwise it will decrease. Also,_ \(|\alpha^{(t)}_{O_{(i,\cdot)},k}|\geq\mathbb{E}[|e(2\sum_{l\in\mathcal{S}_{n,k} ^{e}}{\left(\sigma_{S}^{(t)}\right)_{l}^{n}}-1)\beta^{(t)}_{O_{(i,\cdot)},k}|\) _and_ \(\mathbb{E}[\alpha^{(t)}_{O_{(i,\cdot)},k}]>0\)_;_
4. _All the neurons in_ \(\underset{n\in\mathcal{V}_{k}^{e}}{\mathbb{E}[\mathcal{U}_{k,n}^{e}(t)\cap( \mathcal{W}_{k,n}^{-e}(t)-\mathcal{U}_{k,n}^{-e}(t))]}\) _will ultimately either have its coefficient update stuck due to regularization, or grow into a changing margin into_ \(\mathbb{E}[\mathcal{U}_{k,n}^{e}(t)-(\mathcal{W}_{k,n}^{-e}(t)-\mathcal{U}_{k,n}^{-e}(t))]\) _where_ \[\alpha^{(t)}_{O_{(i,\cdot)},k}\approx\mathbb{E}[(2\sum_{l\in\mathcal{S}_{n,k} ^{e}}{\left(\sigma_{S}^{(t)}\right)_{l}^{n}}-1)e\beta^{(t)}_{O_{(i,\cdot)},k}].\]

Proof.: By Lemma 28, we see that \(\forall i\in\mathbb{E}[\mathcal{U}_{k,n}^{e}(t)]\), \(\alpha^{(t)}_{O_{(i,\cdot)},k}\) and \(e\beta^{(t)}_{O_{(i,\cdot)},k}\) would be contributed by \(\mathcal{V}_{k}^{e}\) to increase, and also \(\forall i\in\mathbb{E}[\mathcal{W}_{k,n}^{e}(t)-\mathcal{U}_{k,n}^{e}(t)]\), \(\alpha^{(t)}_{O_{(i,\cdot)},k}\) and \(e\beta^{(t)}_{O_{(i,\cdot)},k}\) would be contributed by \(\mathcal{V}_{k}^{e}\) to decrease. As such, the first and second point hold naturally by definition. The ultimate transformation of \(\mathbb{E}[\mathcal{U}_{k,n}^{e}(t)-(\mathcal{W}_{k,n}^{-e}(t)-\mathcal{U}_{k,n}^{-e}(t))]\) into \(\underset{n\in\mathcal{V}_{k}^{e}}{\mathbb{E}[\mathcal{U}_{k,n}^{e}(t)\cap( \mathcal{W}_{k,n}^{-e}(t)-\mathcal{U}_{k,n}^{-e}(t))]}\) attributes to the faster changingspeed of \(\alpha^{(t)}_{O_{(i.)},k}\) compared to \(e^{\beta(t)}_{O_{(i.)},k}\) in the neuron sets \(\mathbb{E}[\mathcal{U}^{e}_{k,n}(t)-(\mathcal{W}^{-e}_{k,n}(t)-\mathcal{U}^{-e}_ {k,n}(t))]\), whose learning speed ratio is at least \((\|\mathcal{e}_{k}\|/\|\mathcal{d}_{k}\|)^{2}\). Therefore, the absolute value of \(\alpha^{(t)}_{O_{(i.)},k}\) will surpass that of \(e^{\beta(t)}_{O_{(i.)},k}\), which indicates the neuron would be activated for opposite labels, then the proof is completed. Given that \((\sum_{l\in S^{e}_{n,k}}{(\sigma^{(t)}_{S})}_{l}^{n}-\sum_{l\in S^{-e}_{n,k}}{( \sigma^{(t)}_{S})}_{l}^{n})\) will remain positive, the discussion over \(e^{\beta(t)}_{O_{(i.)},k}\) is simple since it will always grow in \(\mathbf{r}_{i}\)'s direction, and thus the third and forth point hold.

Considering the growth of \(\alpha^{(t)}_{O_{(i.)},k}\), by \(\pi^{+}_{k}=\pi^{-}_{k},P_{l,2k-1}=P^{n}_{l,2k}=\frac{1}{2}\), we know

\[\underset{n\in\mathcal{V}^{e}_{k}}{\mathbb{E}}[\sum_{l\in S^{e}_{n,k}}{( \sigma^{(t)}_{S})}_{l}^{n}]=\underset{n\in\mathcal{V}^{-e}_{k}}{\mathbb{E}}[ \sum_{l\in S^{-e}_{n,k}}{(\sigma^{(t)}_{S})}_{l}^{n}],\]

hence if \(i\in\underset{n\in\mathcal{V}^{e}_{k}}{\mathbb{E}}[\mathcal{U}^{e}_{k,n}(t) \cap(\mathcal{W}^{-e}_{k,n}(t)-\mathcal{U}^{-e}_{k,n}(t))]\), it indicates that

\[\mathbb{E}[\alpha^{(t)}_{O_{(i.)},k}\pm(2\sum_{l\in S^{e}_{n,k}}{(\sigma^{(t) }_{S})}_{l}^{n}-1)e^{\beta(t)}_{O_{(i.)},k}]\geq 0.\]

We see that for \(\underset{n\in\mathcal{V}^{e}_{k}}{\mathbb{E}}[\mathcal{U}^{e}_{k,n}(t)\cap( \mathcal{W}^{-e}_{k,n}(t)-\mathcal{U}^{-e}_{k,n}(t))]\), the \(\mathbb{E}[\mathcal{V}^{e}_{k}]\) will serve to increase the \(\alpha^{(t)}_{O_{(i.)},k}\), but \(\mathbb{E}[\mathcal{V}^{-e}_{k}]\) will serve to decrease the \(\alpha^{(t)}_{O_{(i.)},k}\). The contribution will tend to be positive if

\[\underset{n\in\mathcal{V}^{e}_{k}}{\mathbb{E}}({\ell^{\prime}_{n}}^{(t)}(i \in\mathcal{U}^{e}_{k,n}(t)\cap(\mathcal{W}^{-e}_{k,n}(t)-\mathcal{U}^{-e}_{k,n}(t))))]\geq\underset{n\in\mathcal{V}^{-e}_{k}}{\mathbb{E}}({\ell^{\prime }_{n}}^{(t)}\mathds{1}(i\in\mathcal{U}^{e}_{k,n}(t)\cap(\mathcal{W}^{-e}_{k,n}( t)-\mathcal{U}^{-e}_{k,n}(t))))].\]

Then, as \(\mathbb{E}[(2\sum_{l\in S^{e}_{n,k}}{(\sigma^{(t)}_{S})}_{l}^{n}-1)e^{\beta(t) }_{O_{(i.)},k}]\) of the neurons in and \(\underset{n\in\mathcal{V}^{-e}_{k}}{\mathbb{E}}[\mathcal{U}^{e}_{k,n}(t)\cap( \mathcal{W}^{-e}_{k,n}(t)-\mathcal{U}^{-e}_{k,n}(t))]\) will continue to grow, and finally it will be comparable to the \(\mathbb{E}[\alpha^{(t)}_{O_{(i.)},k}]\). Otherwise it will continue to grow while the evolving speed of \(\mathbb{E}[\alpha^{(t)}_{O_{(i.)},k}]\) is comparatively feeble as it receive the contribution oppositely from \(\underset{n\in\mathcal{V}^{e}_{k}}{\mathbb{E}}({\ell^{\prime}_{n}}^{(t)} \mathds{1}(i\in\mathbb{E}[\mathcal{W}^{e}_{k,n}(t)-\mathcal{U}^{e}_{k,n}(t) \cap\mathcal{U}^{-e}_{k,n}(t)))]\) and \(\underset{n\in\mathcal{V}^{-e}_{k}}{\mathbb{E}}({\ell^{\prime}_{n}}^{(t)} \mathds{1}(i\in\mathbb{E}[\mathcal{W}^{e}_{k,n}(t)-\mathcal{U}^{e}_{k,n}(t) \cap\mathcal{U}^{-e}_{k,n}(t)))]\).

Quanticly this is validated by our later results in Lemma 32 where the \(\underset{n\in\mathcal{V}^{-e}_{k}}{\mathbb{E}}({\ell^{\prime}_{n}}^{(t)})- \underset{n\in\mathcal{V}^{-e}_{k}}{\mathbb{E}}({\ell^{\prime}_{n}}^{(t)})\) would be controlled by the initialization. Interestingly, we see that as \(\mathbb{E}[(2\sum_{l\in S^{e}_{n,k}}{(\sigma^{(t)}_{S})}_{l}^{n}-1)e^{\beta(t) }_{O_{(i.)},k}]\) grows up, its scale will surpass those of \(\mathbb{E}[\alpha^{(t)}_{O_{(i.)},k}]\). Under this scenario, \(\underset{n\in\mathcal{V}^{-e}_{k}}{\mathbb{E}}[\mathcal{U}^{e}_{k,n}(t)\cap( \mathcal{W}^{-e}_{k,n}(t)-\mathcal{U}^{-e}_{k,n}(t))]\) will turn into \(\mathbb{E}[\mathcal{U}^{e}_{k,n}(t)-(\mathcal{W}^{-e}_{k,n}(t)-\mathcal{U}^{-e}_ {k,n}(t))]\), where \(\mathbb{E}[\alpha^{(t)}_{O_{(i.)},k}]\) again continues to grow. Thus finally we have

\[\alpha^{(t)}_{O_{(i.)},k}\approx\mathbb{E}[(2\sum_{l\in S^{e}_{n,k}}{(\sigma^{ (t)}_{S})}_{l}^{n}-1)e^{\beta(t)}_{O_{(i.)},k}].\]

Lemma 4 will show that the growing of \(\mathbb{E}[(2\sum_{l\in S^{e}_{n,k}}{(\sigma^{(t)}_{S})}_{l}^{n}-1)e^{\beta(t)} _{O_{(i.)},k}]\) will stuck, and thus the growing of \(\mathbb{E}[\alpha^{(t)}_{O_{(i.)},k}]\) will also stuck at the changing margin from \(\underset{n\in\mathcal{V}^{-e}_{k}}{\mathbb{E}}[\mathcal{U}^{e}_{k,n}(t)\cap( \mathcal{W}^{-e}_{k,n}(t)-\mathcal{U}^{-e}_{k,n}(t))]\) into \(\mathbb{E}[\mathcal{U}^{e}_{k,n}(t)-(\mathcal{W}^{-e}_{k,n}(t)-\mathcal{U}^{-e }_{k,n}(t))]\).

The proof is completed.

Proof.: _Proof of Lemma 2._ To examine the 0-1 loss, by definition, we know

\[L^{0-1}_{\mathcal{D}^{*}}(\mathbb{E}(\Psi^{t})) =\mathbb{P}_{S_{n}\sim\mathcal{D}^{*}}(y_{S_{n}}\cdot f(\mathbf{E}( S_{n}),\mathbb{E}(\Psi^{t}))\leq 0),\] \[=\mathbb{P}_{S_{n}\sim\mathcal{D}_{S}}(y_{S_{n}}\cdot\sum_{e\in[ \bot]}\frac{e}{m}\sum_{i\in\left(\mathbf{r}_{i}=\frac{e}{m}\right)}\underset{ \Psi^{(t)}}{\mathbb{E}}\left[\sigma_{R}(\mathbf{W}^{\boldsymbol{y}}_{O_{(i,.)} }{}^{(t)}\sum_{l\in[L]}(\sigma^{(t)}_{S})^{n}_{l}\boldsymbol{y}_{l}^{n}) \right]\leq 0),\] \[=\mathbb{P}(\mathbb{E}[y_{S_{n}}\cdot\left(\sum_{e\in[\bot]} \frac{e}{m}\sum_{i\in\left(\mathbf{r}_{i}=\frac{e}{m}\right)}\sigma_{R}( \mathbf{W}^{\boldsymbol{y}}_{O_{(i,.)}}{}^{(t)}\sum_{l\in[L]}(\sigma^{(t)}_{S })^{n}_{l}\boldsymbol{y}_{l}^{n})\right)]\leq 0),\] \[=\mathbb{P}(\mathbb{E}\Big{[}\sum_{i\in\left\{\mathbf{r}_{i}= \frac{y_{S_{n}}}{m}\right\}}\sigma_{R}\left(\alpha^{(t)}_{O_{(i,.)}k}+(2\sum_ {l\in S^{y_{S_{n}}}_{n,k}}(\sigma^{(t)}_{S})^{n}_{l}-1)y_{S_{n}}\beta^{(t)}_{ O_{(i,.)},k}\right)\] \[\qquad\qquad-\sum_{i\in\left\{\mathbf{r}_{i}=-\frac{y_{S_{n}}}{m} \right\}}\sigma_{R}\left(\alpha^{(t)}_{O_{(i,.)},k}+(2\sum_{l\in S^{y_{S_{n}}} _{n,k}}(\sigma^{(t)}_{S})^{n}_{l}-1)y_{S_{n}}\beta^{(t)}_{O_{(i,.)},k}\right) \Big{]}\leq 0)\] \[=\mathbb{P}(\mathbb{E}\Big{[}(\sum_{i\in\mathcal{U}^{y_{S_{n}}}_{ k,n}(t)}-\sum_{i\in\mathcal{W}^{y_{S_{n}}}_{k,n}(t)\cdot\mathcal{U}^{y_{S_{n}}}_{k,n}(t)})( \alpha^{(t)}_{O_{(i,.)},k}+(2\sum_{l\in S^{y_{S_{n}}}_{n,k}}(\sigma^{(t)}_{S}) ^{n}_{l}-1)y_{S_{n}}\beta^{(t)}_{O_{(i.)},k})\Big{]}\leq 0).\]

Therefore, a sufficient condition for \(L^{0-1}_{\mathbb{P}}(\mathbb{E}(\Psi^{t}))=0\) is

\[\mathbb{E}[\sum_{i\in\mathcal{U}^{e}_{k,n}(t)}\alpha^{(t)}_{O_{( i,.)},k}+(2\sum_{l\in S^{e}_{n,k}}(\sigma^{(t)}_{S})^{n}_{l}-1)e\cdot\beta^{(t)}_{ O_{(i.)},k}]\geq \mathbb{E}[\sum_{i\in\mathcal{W}^{e}_{k,n}(t)\cdot\mathcal{U}^{e }_{k,n}(t)}\alpha^{(t)}_{O_{(i.)},k}\] (43) \[+(2\sum_{l\in S^{e}_{n,k}}(\sigma^{(t)}_{S})^{n}_{l}-1)e\cdot\beta ^{(t)}_{O_{(i.)},k}],\]

for \(\forall k\in[K_{1}],e\in[\pm]\). 

We know \(\forall i\in\mathcal{U}^{e}_{k,n}(t),\)\(\mathbb{E}[e\cdot\beta^{(t)}_{O_{(i,.)},k}]\) in the left side of the inequality is increasing, and \(\forall i\in\mathbb{E}[\mathcal{W}^{e}_{k,n}(t)-\mathcal{U}^{e}_{k,n}(t)],\) the \(\mathbb{E}[e\cdot\beta^{(t)}_{O_{(i.)},k}]\) in the right side of the inequality is decreasing, which is a good news since we want the left side exceed the right side. By Lemma 29, we see that all the neurons in \(\mathbb{E}[(\mathcal{W}^{e}_{k,n}(t)-\mathcal{U}^{e}_{k,n}(t))-\mathcal{U}^{-e }_{k,n}(t)]\) will be deactivated, and all the neurons in \(\mathbb{E}[\mathcal{U}^{e}_{k,n}(t)-(\mathcal{W}^{-e}_{k,n}(t)-\mathcal{U}^{- e}_{k,n}(t))]\) will turn into \(\underset{n\in\mathcal{V}^{e}_{k}}{\mathbb{E}}[\mathcal{U}^{e}_{k,n}(t)\cap(\mathcal{W}^{-e }_{k,n}(t)-\mathcal{U}^{-e}_{k,n}(t))]\).

### First Stage: Growing of Coefficient

In this stage, the coefficient update dynamic is continually changing without being much influenced by the comparably feeble regularization. Also, the impact of the decaying learning step \(\eta_{t}\) is under controlled during several periods, which can be safely done due to small initialization by a large \(\gamma\), as well as the slow quadratic decaying nature of the derivative of \(\eta_{t}^{\prime}\). We see that at initialization, by Lemma 7 and Lemma 23, the \(\underset{S_{n}\sim\mathcal{D}_{S}}{\mathbb{E}}[f(\mathbf{E}(S_{n});\Psi^{(0)})]\) satisfies

\[\underset{S_{n}\sim\mathcal{D}_{S}}{\mathbb{E}}\Big{[}\sum_{i\in \mathcal{W}^{y_{S_{n}}}_{k,n}(0)}\mathbf{r}_{i}\left(\alpha^{(0)}_{O_{(i.)},k}+( 2\sum_{l\in S^{y_{S_{n}}}_{n,k}}(\sigma^{(0)}_{S})^{n}_{l}-1)y_{S_{n}}\beta^{(0) }_{O_{(i.)},k}\right)\Big{]} \geq-\sqrt{2\log(\frac{5Km}{\delta})}\cdot\] (44) \[\frac{5\sigma_{1}(\|\bm{c}_{k}\|+\zeta^{e}_{k}\|\bm{d}_{k}\|)}{16},\]

and our remaining job is to see when will \(\underset{S_{n}\sim\mathcal{D}_{S}}{\mathbb{E}}[f(\mathbf{E}(S_{n});\mathbb{E}( \Psi^{(t)}))]\) stay positive for some error tolerance. As such, we need to scrutinize the coefficients that would grow along the iterations. Therefore, we define

\[\mathbf{A}^{k,y_{S_{n}}}_{t} :=\frac{1}{m}\big{[}\Big{(}\sum_{i\in\mathcal{U}^{y_{S_{n}}}_{k,n}( \tau)}-\sum_{i\in(\mathcal{W}^{y_{S_{n}}}_{k,n}(\tau)-\mathcal{U}^{y_{S_{n}}}_{k,n }(\tau))\cap\mathcal{U}^{-y_{S_{n}}}_{k,n}(\tau)}\Big{)}\mathbf{W}^{ \boldsymbol{y}}_{O_{(i.)}}{}^{(\tau)}\bm{c}_{k}\] \[+\Big{(}\sum_{i\in\mathcal{U}^{y_{S_{n}}}_{k,n}(\tau)}-\sum_{i\in( \mathcal{W}^{y_{S_{n}}}_{k,n}(\tau)-\mathcal{U}^{y_{S_{n}}}_{k,n}(\tau))\cap \mathcal{U}^{-y_{S_{n}}}_{k,n}(\tau)}\Big{)}(2\sum_{l\in S^{y_{S_{n}}}_{n,k}}( \sigma^{(\tau)}_{S})^{n}_{l}-1)y_{S_{n}}\mathbf{W}^{\boldsymbol{y}}_{O_{(i.)}}{}^{( \tau)}\bm{d}_{k}\big{|}\Big{|}_{\tau=t}^{\tau=0}.\]We will see that the conditional expectation of this sequence (conditioned on \(\mathbb{E}(\Psi^{(t)})\), and the expectation is taken over \(\mathcal{D}_{S}\)) would grow up to conquer the small initialization and make \(\underset{S_{n}\sim\mathcal{D}_{S}}{\mathbb{E}}[f(\mathbf{E}(S_{n});\mathbb{E}( \Psi^{(t)}))]\) stay positive. Consider the whole training duration \(0\leq t\leq T^{*}\), the evolving speed of \(\beta_{Q,k}^{(t+1)},\beta_{K,k}^{(t+1)},\alpha_{O_{(i,\cdot)},k}^{(t+1)}\) and \(\beta_{O_{(i,\cdot)},k}^{(t+1)}\) depends on \(\mathbb{E}[\ell_{n}^{\prime^{\prime}(t)}]\), \(\mathbb{E}[\mathbb{1}_{O_{(i)}}^{\eta^{\prime}(t)}]\) and \(\mathbb{E}[\sum_{l\in S_{n,k}^{s}}(\sigma_{S}^{(t)})_{l}^{n}]\). Denote

\[\sigma_{S}^{*}\coloneqq\frac{1}{\begin{array}{c}-2\log(5Km/ \delta)\cdot\sigma_{1}^{2}\|\mathbf{u}\|^{4}(1+e^{-\sigma_{0}^{2}\|\mathbf{u}\| ^{2}})\\ 1+e^{-2^{-1}\sigma_{0}^{2}(1-\kappa_{\mathbf{u}})^{2}\|\mathbf{u}\|^{4}e}\\ \alpha\coloneqq 4\log(T^{*}),\end{array}}\] \[\kappa\coloneqq 8\underset{i,k,w}{\max}\{|\alpha_{O_{(i,\cdot)},k}^{(0)}|,| \beta_{O_{(i,\cdot)},k}^{(0)}|\},\]

We will show that \(\sigma_{S}^{*}\) is the lower bound of \(\min_{t\in[T^{*}],k\in[K_{1}]}\{\underset{n\in\mathcal{D}_{S}}{\mathbb{E}}[ \sum_{j\in S_{n,k}^{ys_{n}}}}(\sigma_{S}^{(t)})_{j}^{n}]\}\) along the whole iteration. By Lemma 7, \(\kappa\) can be upper bounded by \(8\sqrt{2\log(5Km/\delta)}\cdot\sigma_{1}(\sqrt{(1+\kappa_{\mathbf{w}})/2}\| \mathbf{q}\|)\), and lower bounded by \(2\sqrt{2}\sigma_{1}\|\mathbf{q}\|\), which is a negligible term due to the small initialization by Condition 1.

**Lemma 30**.: _Under Condition 1, for the whole iteration \(0\leq t\leq T^{*}\), for \(\forall i\in[m],e\in[\pm],k\in[K_{1}],r\in[K_{2}],w\in[d_{\mathcal{X}}-K]\), we have that_

\[0\leq\mathbb{E}[e\cdot\beta_{O_{(i,\cdot)},k}^{(t)}\mathds{1}(i \in\mathcal{U}_{k,n}^{e}(t))]-e\cdot\beta_{O_{(i,\cdot)},k}^{(0)}\leq\sigma_{ S}^{*-1}\alpha,\] \[0\geq\mathbb{E}[e\cdot\beta_{O_{(i,\cdot)},k}^{(t)}\mathds{1}(i \in\mathcal{W}_{k,n}^{e}(t)-\mathcal{U}_{k,n}^{e}(t))]-e\cdot\beta_{O_{(i, \cdot)},k}^{(0)}\geq-\frac{\hat{C}\|\bm{c}_{k}\|^{2}}{\sigma_{S}^{*2}\|\bm{d} _{k}\|^{2}}\alpha\] \[-\frac{\sigma_{1}(\sigma_{S}^{*2}\|\bm{d}_{k}\|^{2}+\hat{C}\|\bm{ c}_{k}\|^{2})\sqrt{2\log(\frac{5Km}{\delta})}}{\sigma_{S}^{*2}\|\bm{d}_{k}\|},\] \[0\leq\mathbb{E}[|\alpha_{O_{(i,\cdot)},k}^{(t)}|]\leq\hat{C} \frac{\|\bm{c}_{k}\|^{2}}{\sigma_{S}^{*2}\|\bm{d}_{k}\|^{2}}\alpha,\] (45)

**Lemma 31**.: _Suppose Eq. (45) holds at iteration \(t\leq T_{2}\), then we have_

\[\Big{|}\underset{n\in\mathcal{V}_{k}}{\mathbb{E}}[ys_{n}f(\mathbf{E}(S); \mathbb{E}(\Psi^{(t)}))]-\mathbb{E}[\mathbf{A}_{t+1}^{k,ys_{n}}]\Big{|}\leq \kappa/2.\]

Proof.: By definition, we have

\[\mathbb{E}[ys_{n}f(\mathbf{E}(S);\Psi^{(t)})]=\mathbb{E}[ys_{n} \cdot\sum_{e\in[\pm]}\frac{e}{m}\sum_{i\in\{\bm{r}_{i}^{n}=\frac{\kappa}{m} \}}\sigma_{R}(\mathbf{W}_{O_{(i,\cdot)}}^{\bm{y}}\sum_{l\in[L]}{(\sigma_{S}^ {(t)})}_{l}^{n}\bm{y}_{l}^{n})]\] \[=\mathbb{E}\Big{[}\frac{1}{m}(\sum_{i\in\mathcal{U}_{k,n}^{ys_{n} }\,(t)}-\sum_{i\in\mathcal{W}_{k,n}^{ys_{n}}\,(t)-\mathcal{U}_{k,n}^{ys_{n}}\,( t)})\left(\alpha_{O_{(i,\cdot)},k}^{(t)}+(2\sum_{l\in S_{n,k}^{ys_{n}}}( \sigma_{S}^{(t)})_{l}^{n}-1)ys_{n}\beta_{O_{(i,\cdot)},k}^{(t)}\right)\Big{]}.\]

Observe that

\[\mathbb{E}\Big{[}\frac{1}{m}\sum_{i\in(\mathcal{W}_{k,n}^{e}(t)- \mathcal{U}_{k,n}^{e}(t))}\left(\alpha_{O_{(i,\cdot)},k}^{(t)}+(2\sum_{l\in S _{n,k}^{ys_{n}}}(\sigma_{S}^{(t)})_{l}^{n}-1)ys_{n}\beta_{O_{(i,\cdot)},k}^{(t) }\right)\Big{]}-\frac{1}{m}\mathbb{E}\Big{[}\] \[\sum_{i\in(\mathcal{W}_{k,n}^{e}(\tau)-\mathcal{U}_{k,n}^{e}(\tau ))\mathcal{U}_{k,n}^{e-e}(\tau)}\left(\alpha_{O_{(i,\cdot)},k}^{(\tau)}+(2 \sum_{l\in S_{n,k}^{ys_{n}}}(\sigma_{S}^{(\tau)})_{l}^{n}-1)ys_{n}\beta_{O_{(i, \cdot)},k}^{(\tau)}\right)\Big{]}\Big{|}_{\tau=t}^{\tau=0}\leq\kappa/4.\]

Here the inequality holds due to the fact that \(\mathbb{E}[\alpha_{O_{(i,\cdot)},k}^{(t)}1(i\in(\mathcal{W}_{k,n}^{e}(t)- \mathcal{U}_{k,n}^{e}(t))-\mathcal{U}_{k,n}^{-e}(t))]\) is decreasing the initial value \(\alpha_{O_{(i,\cdot)},k}^{(0)}1(i\in(\mathcal{W}_{k,n}^{e}(0)-\mathcal{U}_{k,n}^ {e}(0))-\mathcal{U}_{k,n}^{-e}(0))\), and it's absolute value will not surpass that of \(\mathbb{E}[e(2\sum_{l\in S_{n,k}^{ys_{n}}}(\sigma_{S}^{(t)})_{l}^{n}-1)\beta_{O_{(i, \cdot)},k}^{(t)}\leq\kappa/8,\) which is positive (by definition) and also decreasing by Lemma29. On the other hand,

\[\Big{|}\mathbb{E}\Big{[}\frac{1}{m}\sum_{i\in\mathcal{U}_{k,n}^{y_{S _{n}}}(t)}\left(\alpha_{O_{(i,\cdot)},k}^{(t)}+(2\sum_{l\in\mathcal{V}_{k}^{y_{ S_{n}}}}\left(\sigma_{S}^{(t)}\right)_{l}^{n}-1)ys_{n}\beta_{O_{(i,\cdot)},k}^{(t)} \right)\Big{]}-\mathbb{E}[\frac{1}{m}\sum_{i\in\mathcal{U}_{k,n}^{y_{S_{n}}}( \tau)}\alpha_{O_{(i,\cdot)},k}^{(\tau)}\] \[-\frac{1}{m}\sum_{i\in\mathcal{U}_{k,n}^{y_{S_{n}}}(\tau)}(2\sum_ {l\in\mathcal{S}_{n,k}^{y_{S_{n}}}}\left(\sigma_{S}^{(\tau)}\right)_{l}^{n}-1 )ys_{n}\beta_{O_{(i,\cdot)},k}^{(\tau)}\Big{|}_{\tau=t}^{\tau=0}\Big{|}\leq \kappa/4.\]

Combining the two we can see the result is obtained. 

We then denote the last time when there still exists \(\mathbb{E}[\mathbf{A}_{t}^{k,e}]\leq\kappa\) as \(\hat{T}\), formally \(\hat{T}\) is the last time where

\[\bigcup_{k\in[K_{1}],e\in[\pm]}\{\mathbb{E}[\mathbf{A}_{t}^{k,e}]\leq\kappa\} \neq\emptyset.\]

Latter we will show in Lemma 33 that

\[\hat{T}=\frac{C_{1}\sigma_{1}m\lambda K_{1}\gamma\sqrt{(1+\kappa_{\bm{y}}) \log(5Km/\delta)}}{(2\sigma_{S}^{*}-1)^{2}(1-\kappa_{\bm{y}})\|\mathbf{q}\|}.\]

We then denote the learning step at \(\hat{T}\) as \(\eta\coloneqq\eta_{\hat{T}}\), and thus

\[\eta=\eta_{\hat{T}}=\frac{2}{\lambda(\hat{T}+\gamma)}.\]

By Lemma 31, actually it would hold that

\[\mathop{\mathbb{E}}_{S_{n}\sim\mathcal{D}_{S}}[f(\mathbf{E}(S_{n});\mathbb{E} (\Psi^{(\hat{T})}))]\geq\kappa/2\geq 0.\]

And thus the 0-1 loss converges to zero with an error tolerance by definition. Our following job is to find \(\hat{T}\). The following lemma provides the continuous ODEs as the upper and lower bound of the sequence \(\mathbf{A}_{t}^{k,e}\).

**Lemma 32**.: _Under Condition 1, suppose Eq.(45) holds at any iteration \(t\leq T^{*}\), then for \(\forall t\leq T^{*},\forall k\in[K_{1}],e\in[\pm]\), it holds that_

1. _The difference_ \(|\mathop{\mathbb{E}}_{n\in\mathcal{V}_{k}^{*}}(ef(\mathbf{E}(S);\mathbb{E}( \Psi^{(t)}))-\mathop{\mathbb{E}}_{n\in\mathcal{V}_{k}^{-e}}(-ef(\mathbf{E}(S) ;\mathbb{E}(\Psi^{(t)})))|\) _is none-increasing._
2. _The difference of the loss derivative is bounded by_ \(O(\kappa)\)_:_ \[|\mathbb{E}[\mathop{\mathbb{E}}_{n\in\mathcal{V}_{k}^{*}}(\ell_{n}^{\prime} (t))-\mathop{\mathbb{E}}_{n\in\mathcal{V}_{k}^{-e}}(\ell_{n}^{\prime}(t))]| \leq\frac{\kappa}{8}.\]
3. \(\mathbb{E}[\mathbf{A}_{t}^{k,e}]\) _is non-decreasing. The lower and upper bounds of the gradient update have continuous ODE counterpart. Specifically, there exist positive constant_ \(c_{1},c_{2}\)_, we can define_ \(\bar{c}^{k,e}=\frac{c_{1}\eta_{0}\|\mathbf{q}\|^{2}}{2mK_{1}}\)_,_ \(\underline{c}^{k,e}=\frac{c_{2}\eta_{T^{*}}(2\sigma_{S}^{*}-1)^{2}(1-\kappa_{ \bm{y}})\|\mathbf{q}\|^{2}}{16mK_{1}}\)_,_ \(\bar{b}^{k,e}=e^{-\kappa/2}\)_,_ \(\underline{b}^{k,e}=e^{\kappa/2}\)_. Let_ \(\overline{x}_{t}^{k,e}\)_,_ \(\underline{x}_{t}^{k,e}\) _be the unique solutions of_ \[\overline{x}_{t}^{k,e}+\bar{b}^{k,e}e^{x_{t,e}^{k,e}}=\bar{c}^{k,e}t+\bar{b}^{ k,e},\quad\underline{x}_{t}^{k,e}+\underline{b}^{k,e}e^{\overline{x}_{t}^{k,e}}= \underline{c}^{k,e}t+\underline{b}^{k,e},\] _then it holds that_ \[\underline{x}_{t}^{k,e}\leq\mathbb{E}[\mathbf{A}_{t}^{k,e}]\leq\overline{x}_{ t}^{k,e}+\frac{\bar{c}^{k,e}}{1+\bar{b}^{k,e}},\quad\frac{1}{1+\bar{b}^{k,e} \overline{x}_{t}^{k,e}}\leq-\mathop{\mathbb{E}}_{n\in\mathcal{V}_{k}^{*}}( \ell_{n}^{\prime}(t))\leq\frac{1}{1+\underline{b}^{k,e}\underline{x}_{t}^{k,e }}.\] _Specifically, we have_ \[\log(\frac{2\bar{c}^{k,e}}{3\bar{b}^{k,e}}+\frac{2}{3})\leq\mathbb{E}[\mathbf{A }_{t}^{k,e}]\leq\log(\frac{\bar{c}^{k,e}}{\bar{b}^{k,e}}t+1)+\frac{\bar{c}^{k, e}}{1+\bar{b}^{k,e}}.\]Proof.: Observe that \(\mathbb{E}[\underset{n\in\mathcal{V}_{k}^{e}}{\mathbb{E}}(\ell_{n}^{\prime\,(t)})- \underset{n\in\mathcal{V}_{k}^{-e}}{\mathbb{E}}(\ell_{n}^{\prime\,(t)})]\) equals to

\[\mathbb{E}[\frac{-1}{1+e^{[-\frac{1}{m}(\sum_{i\in\mathcal{U}_{k,n}^ {e}(t)}-\sum_{i\in\mathcal{W}_{k,n}^{e}(t)}-\mathcal{U}_{k,n}^{e}(t))^{\left( \alpha_{O_{(i,\cdot)},\cdot\cdot\right)}^{(t)}+(2\sum_{l\in S_{n,k}^{e}}{( \sigma_{S}^{(t)})}_{l}^{n}-1)\varepsilon\beta_{O_{(i,\cdot)},\cdot\right)}^{(t) }}}]}\] (46) \[-\frac{-1}{1+e^{[-\frac{1}{m}(\sum_{i\in\mathcal{U}_{k,n}^{e}(t)} -\sum_{i\in\mathcal{W}_{k,n}^{e}(t)}-\mathcal{U}_{k,n}^{e}(t))^{\left(\alpha_ {O_{(i,\cdot)},\cdot\cdot\right)}^{(t)}+(2\sum_{l\in S_{n,k}^{e}}{(\sigma_{S}^ {(t)})}_{l}^{n}-1)\varepsilon\beta_{O_{(i,\cdot)},\cdot\right)}^{(t)}}]}}]\] \[=\mathbb{E}[\frac{e^{-[\frac{1}{m}(\sum_{i\in\mathcal{U}_{k,n}^{y }(t)}-\sum_{i\in\mathcal{W}_{k,n}^{y}(t)}-\mathcal{U}_{k,n}^{y}(t))^{\left( \alpha_{O_{(i,\cdot)},\cdot\cdot\right)}^{(t)}+(2\sum_{l\in S_{n,k}^{y}}{( \sigma_{S}^{(t)})}_{l}^{n}-1)\eta\beta_{O_{(i,\cdot)},\cdot\right)}^{(t)}}]|y ^{y=-e}}}{\prod_{y\in\{e_{i}-e\}}1+e^{[-\frac{1}{m}(\sum_{i\in\mathcal{U}_{k,n }^{y}(t)}-\sum_{i\in\mathcal{W}_{k,n}^{y}(t)}-\mathcal{U}_{k,n}^{y}(t))^{ \left(\alpha_{O_{(i,\cdot)},\cdot\right)}^{(t)}+(2\sum_{l\in S_{n,k}^{y}}{( \sigma_{S}^{(t)})}_{l}^{n}-1)\eta\beta_{O_{(i,\cdot)},\cdot\right)}^{(t)}}]}}]\]

As cross-entropy loss is \(L\)-smooth with \(L=1\), one can bound the difference by

\[\begin{split}&|\mathbb{E}[\underset{n\in\mathcal{V}_{k}^{e}}{ \mathbb{E}}(\ell_{n}^{\prime\,(t)})-\underset{n\in\mathcal{V}_{k}^{-e}}{ \mathbb{E}}(\ell_{n}^{\prime\,(t)})]|\leq\big{|}\underset{n\in\mathcal{V}_{k}^ {e}}{\mathbb{E}}(ef(\mathbf{E}(S);\mathbb{E}(\Psi^{(t)}))-\underset{n\in \mathcal{V}_{k}^{-e}}{\mathbb{E}}(-ef(\mathbf{E}(S);\mathbb{E}(\Psi^{(t)}))) \big{|}\\ &=|\mathbb{E}[\frac{1}{m}(\sum_{i\in\mathcal{U}_{k,n}^{y}(t)}- \sum_{i\in\mathcal{W}_{k,n}^{y}(t)-\mathcal{U}_{k,n}^{y}(t)})\left(\alpha_{O_{ (i,\cdot)},\cdot\cdot}^{(t)}+(2\sum_{l\in S_{n,k}^{y}}{(\sigma_{S}^{(t)})}_{l }^{n}-1)y\beta_{O_{(i,\cdot)},\cdot}^{(t)}\right)]\big{|}_{y=e}^{y=-e}]|.\end{split}\] (47)

By Lemma 23, we see that for initialization, we have

\[\begin{split}|\underset{n\in\mathcal{V}_{k}^{e}}{\mathbb{E}}(ef( \mathbf{E}(S);\mathbb{E}(\Psi^{(0)}))-\underset{n\in\mathcal{V}_{k}^{-e}}{ \mathbb{E}}(-ef(\mathbf{E}(S);\mathbb{E}(\Psi^{(0)})))|&\leq 2\sqrt{2\log(\frac{5 Km}{\delta})}\cdot\frac{3\sigma_{1}(\|\mathbf{c}_{k}\|+\zeta_{k}^{e} \|\bm{d}_{k}\|)}{8}\\ &\leq\kappa/8.\end{split}\]

Now we serve to show that the following expected difference

\[|\underset{n\in\mathcal{V}_{k}^{e}}{\mathbb{E}}(ef(\mathbf{E}(S);\mathbb{E}( \Psi^{(t)}))-\underset{n\in\mathcal{V}_{k}^{-e}}{\mathbb{E}}(-ef(\mathbf{E}(S) ;\mathbb{E}(\Psi^{(t)})))|\]

is non-increasing. Intuitively, this observation is due to the inherent nature of cross-entropy loss, which always pays more emphasis (has larger derivative) on those low value. Also, another important factor is the update of those ambiguous neurons' coefficient summation would also prefer the low-value one among \(\underset{n\in\mathcal{V}_{k}^{e}}{\mathbb{E}}(ef(\mathbf{E}(S);\mathbb{E}( \Psi^{(t)})),\forall e\in[m]\). To better present this observation, we define

\[e_{t}^{*}=\arg\min\{\underset{n\in\mathcal{V}_{k}^{e}}{\mathbb{E}}(ef(\mathbf{E }(S);\mathbb{E}(\Psi^{(t)})),\underset{n\in\mathcal{V}_{k}^{-e}}{\mathbb{E}}(- ef(\mathbf{E}(S);\mathbb{E}(\Psi^{(t)})))\},\]

which further means that \(e_{t}^{*}\) satisfies \(\mathbb{E}[\underset{n\in\mathcal{V}_{k}^{e}}{\mathbb{E}}(\ell_{n}^{\prime\,(t )})-\underset{n\in\mathcal{V}_{k}^{-e}}{\mathbb{E}}(\ell_{n}^{\prime\,(t)})]<0\) due to the non-positive and non-increasing property of cross-entropy loss.

Recall the update rule, we have

\[\begin{split}\alpha_{O_{(i,\cdot)},k}^{(t+1)}=(1-\eta_{t}\lambda) \alpha_{O_{(i,\cdot)},k}^{(t)}&-\eta_{t}\frac{\|\mathbf{c}_{k}\|^{2} }{2K_{1}}\sum_{e\in[\pm]}[e\mathbf{r}_{i}\cdot\underset{n\in\mathcal{V}_{k}^{e} }{\mathbb{E}}(\ell_{n}^{\prime\,(t)}\mathds{1}_{O_{(i)}}^{n\,(t)})]\\ &-\eta_{t}\frac{(K_{1}-1)\|\mathbf{c}_{k}\|^{2}}{2K_{1}K}\sum_{e \in[\pm]}[e\mathbf{r}_{i}\cdot\underset{n\in\mathcal{V}_{k}^{e}}{\mathbb{E}}( \ell_{n}^{\prime\,(t)}\mathds{1}_{O_{(i)}}^{n\,(t)})],\\ \mathbb{E}[e\beta_{O_{(i,\cdot)},k}^{(t+1)}\mid\Psi^{(t)}]=(1- \eta_{t}\lambda)\varepsilon\beta_{O_{(i,\cdot)},k}^{(t)}\\ &-\eta_{t}\frac{\|\bm{d}_{k}\|^{2}}{2K_{1}}\sum_{e\in[\pm]}\mathbf{ r}_{i}e\underset{n\in\mathcal{V}_{k}^{e}}{\mathbb{E}}[\ell_{n}^{\prime\,(t)}\mathds{1}_{O_{(i)} }^{n\,(t)}(\sum_{l\in S_{n,k}^{e}}{\mathbb{E}}(\sigma_{S}^{(t)})_{l}^{n}- \sum_{l\in S_{n,k}^{e}}{\mathbb{E}}(\sigma_{S}^{(t)})_{l}^{n})].\end{split}\]

Then we have

\[\begin{split}&\mathbb{E}[\alpha_{O_{(i,\cdot)},k}^{(t+1)}+e(2\sum_{l\in S _{n,k}^{e}}{(\sigma_{S}^{(t+1)})}_{l}^{n}-1)\beta_{O_{(i,\cdot)},k}^{(t+1)}\mid \Psi^{(t)},\mathbb{E}[\sum_{l\in S_{n,k}^{e}}{(\sigma_{S}^{(t+1)})}_{l}^{n}]]=(1- \eta_{t}\lambda)(\alpha_{O_{(i,\cdot)},k}^{(t)}+e(2\\ &\sum_{l\in S_{n,k}^{e}}{(\sigma_{S}^{(t)})}_{l}^{n}-1)\beta_{O_{(i, \cdot)},k}^{(t)}-\frac{\eta_{t}}{2K_{1}}\sum_{e\in[\pm]}\mathbf{r}_{i}e \underset{n\in\mathcal{V}_{k}^{e}}{\mathbb{E}}[\ell_{n}^{\prime\,(t)} \mathds{1}_{O_{(i)}}^{n\,(t)}\Big{(}\|\bm{c}_{k}\|^{2}+\|\bm{d}_{k}\|^{2}(2 \sum_{l\in S_{n,k}^{e}}{(\sigma_{S}^{(t)})}_{l}^{n}-1)\]By Lemma 28 and Lemma 29, we see that the \(\mathbb{E}[\sum_{i\in\mathcal{U}_{k,n}^{e}(\tau)-(\mathcal{W}_{k,n}^{-e}(\tau)- \mathcal{U}_{k,n}^{-e}(\tau))}\alpha_{O_{(i,\cdot)},k}^{(\tau)}\big{|}_{\tau=t }^{\tau=0},\forall e\in[\pm]\) is increasing such that

\[\mathbb{E}[\sum_{i\in\mathcal{U}_{k,n}^{e}(\tau)-(\mathcal{W}_{k, n}^{-e}(\tau)-\mathcal{U}_{k,n}^{-e}(\tau))}\alpha_{O_{(i,\cdot)},k}^{(\tau)} \mid\Psi^{(t)}]\Big{|}_{\tau=t+1}^{\tau=0}=\Theta(\sum_{i\in\mathcal{U}_{k,n}^{e} (\tau)-(\mathcal{W}_{k,n}^{-e}(\tau)-\mathcal{U}_{k,n}^{-e}(\tau))}\alpha_{O_{( i,\cdot)},k}^{(\tau)}\big{|}_{\tau=t}^{\tau=0}\] \[-\sum_{i\in\mathcal{U}_{k,n}^{e}(\tau)-(\mathcal{W}_{k,n}^{-e}( \tau)-\mathcal{U}_{k,n}^{-e}(\tau))}\frac{\eta_{t}\|\bm{c}_{k}\|^{2}}{2mK_{1}} \mathop{\mathbb{E}}_{n\in V_{k}^{e}}(\ell_{n}^{\prime\;(t)})),\] (48)

where we ignore the impact of cross-concept safely due to the large \(K=\Omega(\eta_{0}C(K_{1}-1)\|\mathbf{q}\|^{2}/(mK_{1}))\), as well as the impact of regularization term since \(\lambda=O((C\log(Km/\delta)\|\mathbf{q}\|)^{-1})\) by Condition 1 in the first stage.

Similarly, suggest \(\mathbb{E}[\sum_{l\in S_{n,k}^{e}}\left(\sigma_{S}^{(t+1)}\right)_{l}^{n}]\) is also given when considering the update for \(t+1\), we see that

\[\mathbb{E}[\frac{1}{m}\sum_{i\in\mathcal{U}_{k,n}^{e}(\tau)}e(2 \sum_{k\in S_{n,k}^{e}}\left(\sigma_{S}^{(t+1)}\right)_{l}^{n}-1)\beta_{O_{(i, \cdot)},k}^{(\tau)}\mid\Psi^{(t)},\mathbb{E}[\sum_{l\in S_{n,k}^{e}}\left( \sigma_{S}^{(t+1)}\right)_{l}^{n}]\big{|}_{\tau=t+1}^{\tau=0}=(2\sum_{l\in S_{ n,k}^{e}}\left(\sigma_{S}^{(t+1)}\right)_{l}^{n}-1)\] (49) \[\Theta(\frac{1}{m}\sum_{i\in\mathcal{U}_{k,n}^{e}(\tau)}e\beta_{O _{(i,\cdot)},k}^{(\tau)}\Big{|}_{\tau=t}^{\tau=0}-\sum_{i\in\mathcal{U}_{k,n}^ {e}(\tau)}\frac{\eta_{t}\|\bm{d}_{k}\|^{2}}{2mK_{1}}\mathop{\mathbb{E}}_{n\in \mathcal{V}_{k}^{e}}((2\sum_{l\in S_{n,k}^{e}}\left(\sigma_{S}^{(t)}\right)_{l }^{n}-1)\ell_{n}^{\prime\;(t)})).\]

Interestingly, by Eq.(39) we see that \((2\sum_{l\in S_{n,k}^{e}}\left(\sigma_{S}^{(t)}\right)_{l}^{n}-1)=(2\sum_{l\in S _{n,k}^{-e}}\left(\sigma_{S}^{(t)}\right)_{l}^{n}-1)\). Thus we can characterize that the magnitude of gradient update of the term in Eq.(48) and (49) of the \(e_{t}^{*}\) would be larger than those of \(-e_{t}^{*}\) due to the non-increasing nature of cross-entropy loss.

On the other hand, by Lemma 29 the monotonicity of

\[\mathbb{E}[\sum_{i\in\mathcal{U}_{k,n}^{e}(\tau)\cap(\mathcal{W}_{k,n}^{-e}( \tau)-\mathcal{U}_{k,n}^{-e}(\tau))}\alpha_{O_{(i,\cdot)},k}^{(\tau)}]\Big{|}_ {\tau=t}^{\tau=0},\quad\mathbb{E}[\sum_{i\in\mathcal{W}_{k,n}^{e}(\tau)- \mathcal{U}_{k,n}^{e}(\tau))\cap\mathcal{U}_{k,n}^{-e}(\tau)}\alpha_{O_{(i, \cdot)},k}^{(\tau)}\Big{|}_{\tau=t}^{\tau=0}\]

depend on the signal of \(\mathbb{E}[\mathop{\mathbb{E}}_{n\in V_{k}^{e}}(\ell_{n}^{\prime\;(t)})- \mathop{\mathbb{E}}_{n\in\mathcal{V}_{k}^{e}}(\ell_{n}^{\prime\;(t)})]\). Specifically, we see that

\[\mathbb{E}[\sum_{i\in\mathcal{U}_{k,n}^{e}(\tau)\cap(\mathcal{W}_ {k,n}^{-e}(\tau)-\mathcal{U}_{k,n}^{-e}(\tau))}\alpha_{O_{(i,\cdot)},k}^{(\tau )}\mid\Psi^{(t)}]\Big{|}_{\tau=t+1}^{\tau=0}=\Theta(\sum_{i\in\mathcal{U}_{k,n}^ {e}(\tau)\cap(\mathcal{W}_{k,n}^{-e}(\tau)-\mathcal{U}_{k,n}^{-e}(\tau))} \alpha_{O_{(i,\cdot)},k}^{(\tau)}\Big{|}_{\tau=t}^{\tau=0}\] \[-\sum_{i\in\mathcal{U}_{k,n}^{e}(\tau)\cap(\mathcal{W}_{k,n}^{-e}( \tau)-\mathcal{U}_{k,n}^{-e}(\tau))\mathcal{U}_{k,n}^{-e}(\tau))}\frac{\eta_{t} \|\bm{c}_{k}\|^{2}}{2mK_{1}}[\mathop{\mathbb{E}}_{n\in V_{k}^{e}}(\ell_{n}^{ \prime\;(t)})-\mathop{\mathbb{E}}_{n\in\mathcal{V}_{k}^{-e}}(\ell_{n}^{\prime \;(t)})]);\] \[\mathbb{E}[\sum_{i\in(\mathcal{W}_{k,n}^{e}(\tau)-\mathcal{U}_{k,n}^ {e}(\tau))\cap\mathcal{U}_{k,n}^{-e}(\tau)}\alpha_{O_{(i,\cdot)},k}^{(\tau)} \mid\Psi^{(t)}]\Big{|}_{\tau=t+1}^{\tau=0}=\Theta(\sum_{i\in(\mathcal{W}_{k,n}^ {e}(\tau)-\mathcal{U}_{k,n}^{e}(\tau))\cap\mathcal{U}_{k,n}^{-e}(\tau)}\alpha_ {O_{(i,\cdot)},k}^{(t)}\Big{|}_{\tau=t}^{\tau=0}\] \[+\sum_{i\in(\mathcal{W}_{k,n}^{e}(\tau)-\mathcal{U}_{k,n}^{e}( \tau))\cap\mathcal{U}_{k,n}^{-e}(\tau)}\frac{\eta_{t}\|\bm{c}_{k}\|^{2}}{2mK_{1}} [\mathop{\mathbb{E}}_{n\in V_{k}^{e}}(\ell_{n}^{\prime\;(t)})-\mathop{\mathbb{E}} _{n\in\mathcal{V}_{k}^{-e}}(\ell_{n}^{\prime\;(t)})]);\] (50)

where the contribution term is shared by the two sequences. Therefore, by Eq.(50) and (46), the evolution of

\[\mathbb{E}[\sum_{i\in\mathcal{U}_{k,n}^{e}(\tau)\cap(\mathcal{W}_{k,n}^{-e}(\tau)- \mathcal{U}_{k,n}^{-e}(\tau))}\alpha_{O_{(i,\cdot)},k}^{(\tau)}-\sum_{i\in( \mathcal{W}_{k,n}^{e}(\tau)-\mathcal{U}_{k,n}^{e}(\tau))\cap\mathcal{U}_{k,n}^{- e}(\tau)}\alpha_{O_{(i,\cdot)},k}^{(\tau)}\Big{|}_{\tau=t}^{\tau=0}\]

will prefer to grow in the direction of \(e_{t}^{*}\).

We then take a look on the decreasing coefficients based on Lemma 29.

\[\mathbb{E}[\sum_{i\in(\mathcal{W}^{e}_{k,n}(\tau)-\mathcal{U}^{e}_{k,n }(\tau))-\mathcal{U}^{-e}_{k,n}(\tau)}\alpha^{(\tau)}_{O_{(i,\cdot),k}}\left.|\ \Psi^{(t)}\right|\Big{|}_{\tau=t+1}^{\tau=0}=\Theta(\sum_{i\in(\mathcal{W}^{e}_{ k,n}(\tau)-\mathcal{U}^{e}_{k,n}(\tau))-\mathcal{U}^{-e}_{k,n}(\tau)}\alpha^{(t)}_ {O_{(i,\cdot),k}}\Big{|}_{\tau=t}^{\tau=0}\] \[+\sum_{i\in(\mathcal{W}^{e}_{k,n}(\tau)-\mathcal{U}^{e}_{k,n}( \tau))-\mathcal{U}^{-e}_{k,n}(\tau)}\frac{\eta_{t}\|\bm{c}_{k}\|^{2}}{2mK_{1}} \mathop{\mathbb{E}}_{n\in\mathcal{V}^{e}_{k}}(\ell^{(\tau)}_{n}(^{(t)})),\] \[\mathbb{E}[\frac{1}{m}\sum_{i\in\mathcal{W}^{e}_{k,n}(\tau)- \mathcal{U}^{e}_{k,n}(\tau)}e(2\sum_{l\in S^{e}_{n,k}}(\sigma^{(t+1)}_{S})^{n }_{l}-1)\beta^{(\tau)}_{O_{(i,\cdot),k}}\left.|\ \Psi^{(t)},\mathbb{E}[\sum_{l\in S^{e}_{n,k}}(\sigma^{(t+1)}_{S})^{n}_{l}] \right|_{\tau=t+1}^{\tau=0}=\] \[(2\mathbb{E}[\sum_{l\in S^{e}_{n,k}}(\sigma^{(t+1)}_{S})^{n}_{l}] -1)\Theta(\sum_{i\in\mathcal{W}^{e}_{k,n}(\tau)-\mathcal{U}^{e}_{k,n}(\tau)} \frac{e\beta^{(\tau)}_{O_{(i,\cdot),k}}}{m}\Big{|}_{\tau=t}^{\tau=0}\] \[+\sum_{i\in\mathcal{U}^{e}_{k,n}(\tau)}\frac{\eta_{t}\|\bm{d}_{k} \|}{2mK_{1}}\mathop{\mathbb{E}}_{n\in\mathcal{V}^{e}_{k}}((2\sum_{l\in S^{e}_ {n,k}}(\sigma^{(t)}_{S})^{n}_{l}-1)\ell^{\prime}_{n}(^{t)})).\] (51)

As such, we have all preliminaries to characterize the first result of the lemma. We first utilize the induction to prove the following:

\[\big{|}\mathop{\mathbb{E}}_{n\in\mathcal{V}^{e}_{k}}(ef(\mathbf{E}(S);\mathbb{ E}(\Psi^{(t)}))-\mathop{\mathbb{E}}_{n\in\mathcal{V}^{e}_{k}}(-ef(\mathbf{E}(S); \mathbb{E}(\Psi^{(t)})))|\leq\kappa/8.\quad\forall e\in[\pm].\]

This apparently hold at initialization. Suggest for any \(t\leq\widetilde{t}-1\) the result holds, then we only need to prove

\[\mathop{\mathbb{E}}_{n\in\mathcal{V}^{e}_{k}-1}(e^{*}_{\widetilde {t}-1}f(\mathbf{E}(S);\mathbb{E}(\Psi^{(\widetilde{t}-1)}))-\mathop{\mathbb{E} }_{n\in\mathcal{V}^{-e}_{k}-1}(-e^{*}_{\widetilde{t}-1}f(\mathbf{E}(S); \mathbb{E}(\Psi^{(\widetilde{t}-1)})))\geq\] \[\mathop{\mathbb{E}}_{n\in\mathcal{V}^{e}_{k}-1}(e^{*}_{\widetilde {t}-1}f(\mathbf{E}(S);\mathbb{E}(\Psi^{(t_{1})}))-\mathop{\mathbb{E}}_{n\in \mathcal{V}^{-e}_{k}-1}(-e^{*}_{\widetilde{t}-1}f(\mathbf{E}(S);\mathbb{E}( \Psi^{(t_{1})}))).\]

By the condition of small \(\eta_{t}\) in Condition 1, Lemma 31, Eq.(48) (49), (50) and (51), we see that

\[\mathop{\mathbb{E}}_{n\in\mathcal{V}^{-e}_{k}-1}(-e^{*}_{\widetilde {t}-1}f(\mathbf{E}(S);\mathbb{E}(\Psi^{(\widetilde{t}-1)})))-\mathop{\mathbb{E} }_{n\in\mathcal{V}^{e}_{k}-1}(e^{*}_{\widetilde{t}-1}f(\mathbf{E}(S); \mathbb{E}(\Psi^{(\widetilde{t}-1)}))\] \[-(\mathop{\mathbb{E}}_{n\in\mathcal{V}^{-e}_{k}-1}(-e^{*}_{ \widetilde{t}-1}f(\mathbf{E}(S);\mathbb{E}(\Psi^{(\widetilde{t})})))-\mathop{ \mathbb{E}}_{n\in\mathcal{V}^{-e}_{k}-1}(e^{*}_{\widetilde{t}-1}f(\mathbf{E}( S);\mathbb{E}(\Psi^{(\widetilde{t})})))\] \[\leq\Theta(\mathbb{E}[\mathop{\mathbb{E}}_{n\in\mathcal{V}^{e}_ {k}-1}((\ell^{(\widetilde{t}^{(\widetilde{t}^{(\widetilde{t}^{(\widetilde{t}^{ (\widetilde{t}^{-1})})}}}_{n})-\mathop{\mathbb{E}}_{n\in\mathcal{V}^{-e}_{k}- \widetilde{t}-1}(\ell^{(\widetilde{t}^{(\widetilde{t}^{(\widetilde{t}^{-1})}} _{n}}))\Big{(}\sum_{i\in\mathcal{V}^{e}_{k,n}-\widetilde{(t}-1)}\frac{\eta_{T^{* }}\|\bm{c}_{k}\|^{2}}{2mK_{1}}\] \[+(2\sum_{l\in S^{e}_{n,k}}(\sigma^{(\widetilde{t})}_{S})^{n}_{l} -1)\sum_{i\in\mathcal{V}^{e}_{k,n}-1}\frac{\eta_{T^{*}}\|\bm{d}_{k}\|^{2}}{2mK _{1}}\mathop{\mathbb{E}}_{n\in\mathcal{V}^{e}_{k}-1}((2\sum_{l\in S^{e}_{n,k }}(\sigma^{(\widetilde{t}-1)}_{S})^{n}_{l}-1))\Big{)}]\rangle\leq 0,\]

and thus we have

\[|\mathop{\mathbb{E}}_{n\in\mathcal{V}^{e}_{k}-1}(e^{*}_{\widetilde {t}-1}f(\mathbf{E}(S);\mathbb{E}(\Psi^{(\widetilde{t})}))-\mathop{\mathbb{E}}_{n \in\mathcal{V}^{-e}_{k}-1}(-e^{*}_{\widetilde{t}-1}f(\mathbf{E}(S);\mathbb{E}( \Psi^{(\widetilde{t})})))|\] \[\leq|\mathop{\mathbb{E}}_{n\in\mathcal{V}^{e}_{k}-1}(e^{*}_{ \widetilde{t}-1}f(\mathbf{E}(S);\mathbb{E}(\Psi^{(\widetilde{t}-1)}))-\mathop{ \mathbb{E}}_{n\in\mathcal{V}^{-e}_{k}-\widetilde{t}-1}(-e^{*}_{\widetilde{t}-1}f( \mathbf{E}(S);\mathbb{E}(\Psi^{(\widetilde{t}-1)})))|.\]Therefore, we complete the induction. Then we have

\[|\mathbb{E}[\underset{n\in\mathcal{V}_{k}^{e^{*}_{\tilde{t}-1}}}{ \mathbb{E}}(\ell_{n}^{\prime\;(\tilde{t})})-\underset{n\in\mathcal{V}_{k}^{e^{*}_ {\tilde{t}-1}}}{\mathbb{E}}(\ell_{n}^{\prime\;(\tilde{t})})]|\] \[\leq|\underset{n\in\mathcal{V}_{k}^{e^{*}_{\tilde{t}-1}}}{ \mathbb{E}}(e^{*}_{\tilde{t}-1}f(\mathbf{E}(S);\mathbb{E}(\Psi^{(\tilde{t})})) -\underset{n\in\mathcal{V}_{k}^{e^{*}_{\tilde{t}-1}}}{\mathbb{E}}(-e^{*}_{ \tilde{t}-1}f(\mathbf{E}(S);\mathbb{E}(\Psi^{(\tilde{t}-1)})))|\] \[\leq\cdots\leq|\underset{n\in\mathcal{V}_{k}^{e^{*}_{\tilde{t}-1}}} {\mathbb{E}}(ef(\mathbf{E}(S);\mathbb{E}(\Psi^{(0)}))-\underset{n\in\mathcal{V }_{k}^{e^{*}}}{\mathbb{E}}(-ef(\mathbf{E}(S);\mathbb{E}(\Psi^{(0)})))|\] \[\leq\sqrt{2\log(\frac{5Km}{\delta})}\cdot\frac{3\sigma_{1}(\| \boldsymbol{c}_{k}\|+\zeta_{k}^{e}\|\boldsymbol{d}_{k}\|)}{4}\leq\kappa/8.\]

This completes the proof of the first result.

To obtain the continuous ODE upper bound of \(\mathbb{E}[\mathbf{A}_{t}^{k,e}]\), we first recall the update

\[\mathbb{E}[\alpha_{O_{(i,\cdot)},k}^{(t+1)}+e(2\sum_{l\in S_{n,k} ^{e}}{(\sigma_{S}^{(t+1)})}_{l}^{n}-1)\beta_{O_{(i,\cdot)},k}^{(t+1)}\mid\Psi^{ (t)},\mathbb{E}[\sum_{l\in S_{n,k}^{e}}{(\sigma_{S}^{(t+1)})}_{l}^{n}]]=(1- \eta_{t}\lambda)(\alpha_{O_{(i,\cdot)},k}^{(t)}+\] \[e(2\sum_{l\in S_{n,k}^{e}}{(\sigma_{S}^{(t)})}_{l}^{n}-1)\beta_{ O_{(i,\cdot)},k}^{(t)})-\eta_{t}\frac{1}{2K_{1}}\sum_{e\in[\pm]}\mathbf{r}_{i}e \underset{n\in\mathcal{V}_{k}^{e}}{\mathbb{E}}[\ell_{n}^{\prime\;(t)}\mathds{ L}_{O_{(i)}}^{n}{}^{(t)}(\|\boldsymbol{c}_{k}\|^{2}+\|\boldsymbol{d}_{k}\|^{2}(2 \sum_{l\in S_{n,k}^{e}}{(\sigma_{S}^{(t+1)})}_{l}^{n}\] \[-1)(2\sum_{l\in S_{n,k}^{e}}{(\sigma_{S}^{(t)})}_{l}^{n}-1)\Big{)}- \eta_{t}\frac{(K_{1}-1)}{2K_{1}K}\sum_{e\in[\pm]}\mathbf{r}_{i}e\underset{n\in \mathcal{V}_{-k}^{e}}{\mathbb{E}}[\ell_{n}^{\prime\;(t)}\mathds{L}_{O_{(i)}}^{ n}{}^{(t)}\|\boldsymbol{c}_{k}\|^{2}].\]

Then, utilizing Lemma 31 and the fact \(|\mathcal{W}_{k,n}^{e}(t)|\leq m\), we have constant \(c_{1}>0\) such that

\[\mathbb{E}[\mathbf{A}_{t+1}^{k,e}\mid\Psi^{(t)},\mathbb{E}(\sum _{l\in S_{n,k}^{e}}{(\sigma_{S}^{(t+1)})}_{l}^{n})] \leq\mathbf{A}_{t}^{k,e}-c_{1}(\frac{\eta_{t}\|\boldsymbol{q}\|^{ 2}}{2mK_{1}}\cdot\underset{n\in\mathcal{V}_{k}^{e}}{\mathbb{E}}[\ell_{n}^{ \prime\;(t)}]),\] \[\leq\mathbf{A}_{t}^{k,e}+c_{1}(\frac{\eta_{0}\|\boldsymbol{q}\|^{ 2}}{2mK_{1}}\cdot\frac{1}{1+e^{-\kappa/2}e\mathbf{A}_{t}^{k,e}})\] (52) \[=\mathbf{A}_{t}^{k,e}+\frac{\overline{c}^{k,e}}{1+\overline{b}^{k, e}e\mathbf{A}_{t}^{k,e}}.\]

where we also neglect the impact of cross-concept due to the large \(K=\Omega(\eta_{0}C(K_{1}-1)\|\boldsymbol{q}\|^{2}/(mK_{1}))\) in Condition 1 and appropriately chosen \(c_{1}\).

To obtain the lower bound ODE counterpart, we examine the update of the correct contributor neurons, as shown in Eq.(48), (50) and (49).

In terms of the update of \(\mathbb{E}[\alpha_{O_{(i,\cdot)},k}^{(t)}]\) where \(i\in\mathbb{E}[\mathcal{U}_{k,n}^{e}(t)\cap(\mathcal{W}_{k,n}^{-e}(t)- \mathcal{U}_{k,n}^{-e}(t))]\), we see that its update is controlled by \(\mathbb{E}[\underset{n\in\mathcal{V}_{k}^{e}}{\mathbb{E}}(\ell_{n}^{\prime\; (t)})-\underset{n\in\mathcal{V}_{k}^{-e}}{\mathbb{E}}(\ell_{n}^{\prime\;(t)})]\):

\[\alpha_{O_{(i,\cdot)},k}^{(t+1)}=\Theta(\alpha_{O_{(i,\cdot)},k}^{(t)}-\frac{ \eta_{t}\|\boldsymbol{c}_{k}\|^{2}}{2mK_{1}}[\underset{n\in\mathcal{V}_{k}^{e} }{\mathbb{E}}(\ell_{n}^{\prime\;(t)})-\underset{n\in\mathcal{V}_{k}^{-e}}{ \mathbb{E}}(\ell_{n}^{\prime\;(t)})]).\]

Then by the first result in this lemma we know \(|\mathbb{E}[\underset{n\in\mathcal{V}_{k}^{e}}{\mathbb{E}}(\ell_{n}^{\prime\;(t )})-\underset{n\in\mathcal{V}_{k}^{-e}}{\mathbb{E}}(\ell_{n}^{\prime\;(t)})| ]\leq\frac{\beta}{4}\leq\frac{\kappa}{32}\), and thus

\[\alpha_{O_{(i,\cdot)},k}^{(t+1)}=\Theta(\alpha_{O_{(i,\cdot)},k}^{(t)}\pm \frac{\eta_{t}\kappa\|\boldsymbol{c}_{k}\|^{2}}{64mK_{1}}).\]

By the condition on the small initialization in Condition 1 such that \(\sigma_{1}=O(\frac{(2\sigma_{S}^{e}-1)^{2}}{Cm^{3/2}\|\boldsymbol{q}\|^{2}})\), due to the large \(C\), we see that the \(\kappa=O((2\sigma_{S}^{e}-1)^{2}/m)\) is far more feeble. Thus the gradient contributions made by neuron set \(\mathbb{E}[\alpha_{O_{(i,\cdot)},k}^{(t)}]\) where \(i\in\mathbb{E}[\mathcal{U}_{k,n}^{e}(t)\cap(\mathcal{W}_{k,n}^{-e}(t)- \mathcal{U}_{k,n}^{-e}(t))]\) can be neglected compared to the increasing update of \(\mathbb{E}[\alpha_{O_{(i,\cdot)},k}^{(t)}\boldsymbol{1}(i\in\mathcal{U}_{k,n}^{e} (t)-(\mathcal{W}_{k,n}^{-e}(t)-\mathcal{U}_{k,n}^{-e}(t)))]\) and \(\mathbb{E}[e(2\sum_{l\in S_{n,k}^{e}}{(\sigma_{S}^{(t)})}_{l}^{n}-1)\beta_{O_{(i, \cdot)},k}^{(t)})]\). Besides, we see that \(\mathbb{E}[\mathcal{W}^{e}_{k,n}(t)]\) will at least preserve the neurons of \(\mathbb{E}[\mathcal{U}^{e}_{k,n}(0))]\), which will not be deactivated by Lemma 29.

Then there exists \(c_{2}>0\), recall \(\sigma^{*}_{S}\) is defined in Lemma 34 as the lower bound of \(\min_{t,k}\{\underset{n\in\mathbb{P}_{S}}{\mathbb{E}}[\sum_{j\in S^{nS_{n}}_{n, k}}(\sigma^{(t)}_{S})^{n}_{j}]\}\) and \(\mathbb{E}[|\mathcal{U}^{e}_{k,n}(0))|\geq m/8\), it holds that

\[\mathbb{E}[\mathbf{A}^{k,e}_{t+1}\mid\Psi^{(t)},\mathbb{E}(\sum_ {l\in S^{n}_{n,k}}(\sigma^{(t+1)}_{S})^{n}_{l})] \geq\mathbf{A}^{k,e}_{t}-c_{2}(\frac{\eta_{t}(2\sigma^{*}_{S}-1)^ {2}\|\mathbf{d}_{k}\|^{2}}{8mK_{1}}\cdot\underset{n\in\mathbb{V}^{e}_{k}}{ \mathbb{E}}[\ell^{\prime}_{n}(t)])\] (53) \[\geq\mathbf{A}^{k,e}_{t}+(\frac{c_{2}\eta_{t}(2\sigma^{*}_{S}-1) ^{2}\|\mathbf{d}_{k}\|^{2}}{1+e^{\kappa/2}e\mathbf{A}^{k,e}_{t}})\] \[\geq\mathbf{A}^{k,e}_{t}+(\frac{c_{2}\eta_{T^{*}}(2\sigma^{*}_{S} -1)^{2}(1-\kappa_{\boldsymbol{y}})\|\mathbf{q}\|^{2}}{16mK_{1}}\cdot\frac{1}{ 1+e^{\kappa/2}e\mathbf{A}^{k,e}_{t}})\] \[=\mathbf{A}^{k,e}_{t}+\frac{c^{k,e}}{1+\underline{b}^{k,e}e \mathbf{A}^{k,e}_{t}}.\]

where we ignore the impact of regularization term at this stage since \(\lambda=O((C\log(Km/\delta)\|\mathbf{q}\|)^{-1})\) and appropriately chosen \(c_{2}\). The third inequality is due to the definition of \(\mathbf{d}_{k}\).

Collaborating with Lemma 10, the proofs are completed.

For the last results, following the techniques in [43], first it's easy to check that

\[\overline{b}^{k,e}\overline{e}^{k^{k,e}}_{t}\leq\overline{x}^{k,e}_{t}+ \overline{b}^{k,e}\overline{e}^{k^{k,e}}_{t}\leq 1.5\overline{b}^{k,e} \overline{e}^{k^{k,e}}_{t},\quad\underline{b}^{k,e}\overline{e}^{k^{k,e}}_{t} \leq\underline{x}^{k,e}_{t}+\underline{b}^{k,e}\overline{e}^{k^{k,e}}_{t}\leq 1.5\underline{b}^{k,e}\overline{e}^{k^{k,e}}_{t},\]

thus

\[\log(\frac{2\overline{c}^{k,e}}{3\overline{b}^{k,e}}+\frac{2}{3})\leq \overline{x}^{k,e}_{t}\leq\log(\frac{\overline{c}^{k,e}}{\overline{b}^{k,e}} +1),\quad\log(\frac{2\underline{e}^{k,e}}{3\underline{b}^{k,e}}+\frac{2}{3}) \leq\underline{x}^{k,e}_{t}\leq\log(\frac{\underline{c}^{k,e}}{\underline{b} ^{k,e}}+1).\]

Thus

\[\log(\frac{2\overline{c}^{k,e}}{3\underline{b}^{k,e}}+\frac{2}{3})\leq \mathbb{E}[\mathbf{A}^{k,e}_{t}]\leq\log(\frac{\overline{c}^{k,e}}{\overline{ b}^{k,e}}t+1)+\frac{\overline{c}^{k,e}}{1+\overline{b}^{k,e}}.\]

Proof of Lemma 30.: We use induction to prove this lemma. All conclusion holds naturally at \(t=0\). Suppose there exists \(\widetilde{T}\leq T^{*}\) such that the six conditions hold for any \(0\leq t\leq\widetilde{T}-1\), we prove that these conclusions also hold for \(t=\widetilde{T}\).

We now prove

\[0\leq\mathbb{E}[e\cdot\beta^{(t)}_{O_{(t,\cdot)},k}\mathds{1}(i\in\mathcal{U} ^{e}_{k,n}(t))]-e\cdot\beta^{(0)}_{O_{(t,\cdot)},k}\leq(\sigma^{*}_{S})^{-1}\alpha.\] (54)

Recall the update rule

\[\beta^{(t+1)}_{O_{(t,\cdot)},k}=(1-\eta_{t}\lambda)\beta^{(t)}_{O_{(t,\cdot)}, k}-\eta_{t}\frac{\|\mathbf{d}_{k}\|^{2}}{2K_{1}}\sum_{e\in[\pm]}\mathbf{r}_{i} \underset{n\in\mathbb{V}^{e}_{k}}{\mathbb{E}}[\ell^{\prime}_{n}(t)^{n} \mathds{1}^{n}_{O_{(t)}}(\sum_{l\in S^{n}_{n,k}}(\sigma^{(t)}_{S})^{n}_{l}- \sum_{l\in S^{-n}_{n,k}}(\sigma^{(t)}_{S})^{n}_{l})].\]

As we ignore the regularization term at the first stage, we can easily seen that \(\mathbb{E}[e\cdot\beta^{(t)}_{O_{(t,\cdot)},k}\mathds{1}(i\in\mathcal{U}^{e}_{ k,n}(t))]\) increases with \(t\). Assume \(t_{\beta^{+},k}\) as the last time \(\exists i\in\mathbb{E}[\mathcal{U}^{e}_{k,n}(t)]\) such that \(\mathbb{E}[e\cdot\beta^{(t)}_{O_{(t,\cdot)},k}\mathds{1}(i\in\mathcal{U}^{e}_ {k,n}(t))]-e\cdot\beta^{(t)}_{O_{(t,\cdot)},k}\mathds{1}(i\in\mathcal{U}^{e}_{ k,n}(t))]\).

\(\beta^{(0)}_{O_{(i,\cdot)},k}\leq(\sigma_{S}^{*})^{-1}\log(T^{*})\), then for \(i\in\mathbb{E}[\mathcal{U}^{\epsilon}_{k,n}(\widetilde{t})]\) we have

\[\mathbb{E}[e\cdot\beta^{(\widetilde{t})}_{O_{(i,\cdot)},k}] \leq\mathbb{E}[e\cdot\beta^{(\widetilde{t}_{\beta^{\prime}}+,k)}_{ O_{(i,\cdot)},k}]\] \[\quad-\eta_{0}\frac{\|\boldsymbol{d}_{k}\|^{2}}{2K_{1}}\sum_{e\in[ \pm]}\mathbf{r}_{i}\mathbb{E}[\ell^{\prime\ (t)}_{n}\mathds{1}^{n}_{O_{(i)}}(\cdot)\big{(}\sum_{l\in S^{n}_{n,k}}(\sigma^{ (t)}_{S})_{l}^{n}-\sum_{l\in S^{-\epsilon}_{n,k}}(\sigma^{(t)}_{S})_{l}^{n} \big{)}]\Big{|}_{t=t_{\beta^{+},k}}\] \[\quad-\eta_{0}\sum_{t_{\beta^{+},k}<t<\widetilde{T}}\frac{\| \boldsymbol{d}_{k}\|^{2}}{2K_{1}}\sum_{e\in[\pm]}\mathbf{r}_{i}\mathbb{E}[ \ell^{\prime\ (t)}_{n}(\cdot)\big{(}\sum_{l\in S^{n}_{n,k}}(\sigma^{(t)}_{S})_{l}^{n}-\sum_ {l\in S^{-\epsilon}_{n,k}}(\sigma^{(t)}_{S})_{l}^{n}\big{)}]\] \[\leq e\cdot\beta^{(0)}_{O_{(i,\cdot)},k}+(\sigma^{*}_{S})^{-1}\log (T^{*})+\frac{\eta_{0}\|\boldsymbol{d}_{k}\|^{2}}{2mK_{1}}-\sum_{t_{\beta^{+},k}<t<\widetilde{T}}\frac{\eta_{0}\|\boldsymbol{d}_{k}\|^{2}}{2mK_{1}}\mathbb{ E}[\ell^{\prime\ (t)}_{n}]\] \[\leq e\cdot\beta^{(0)}_{O_{(i,\cdot)},k}+2(\sigma^{*}_{S})^{-1} \log(T^{*})-\sum_{t_{\beta^{+},k}<t<\widetilde{T}}\frac{\eta_{0}\|\boldsymbol {d}_{k}\|^{2}}{2mK_{1}}\mathbb{E}[\ell^{\prime\ (t)}_{n}\mathds{1}^{n}_{O_{(i)}}(\cdot)],\]

where the first inequality is by the positive nature of regularization term as well as the contribution of the gradient; second inequality is by \(\mathbb{E}[-\ell^{\prime\ (t_{\beta^{+},k^{*}})}_{n}]\leq 1\) and \(\mathbb{E}[(\sum_{t\in S^{n}_{n,k}}(\sigma^{(t)}_{S})_{l}^{n}-\sum_{l\in S^{- \epsilon}_{n,k}}(\sigma^{(t)}_{S})_{l}^{n}]]\leq 1\); the third inequality is by the condition \(\eta_{0}=O(\frac{mK_{1}}{\|\boldsymbol{d}_{k}\|^{2}})\) and thus \(\frac{\eta_{0}\|\boldsymbol{d}_{k}\|^{2}}{2mK_{1}}\leq 1\leq\log(T^{*})\), as well as \((\sigma^{*}_{S})^{-1}\geq 1\). The remaining job is to prove that

\[-\sum_{t_{\beta^{+},k}<t<\widetilde{T}}\frac{\eta_{0}\|\boldsymbol{d}_{k}\|^{2 }}{2mK_{1}}\mathbb{E}[\ell^{\prime\ (t)}_{n}\mathds{1}^{n}_{O_{(i)}}(\cdot)]\leq(\sigma^{*}_{S})^{-1}\log(T^{*}).\]

Observe that

\[|\mathbb{E}[\ell^{\prime\ (t)}_{n}]| =\mathbb{E}[\frac{1}{1+\exp(y_{S_{n}}\cdot\left(\sum_{e\in[\pm]} \frac{e}{m}\sum_{i\in(\mathbf{r}_{i}=\frac{e}{m})}\sigma_{R}(\mathbf{W}^{ \Psi}_{O_{(i,\cdot)}}^{(t)}\sum_{l\in[L]}{(\sigma^{(t)}_{S})_{l}^{n}\boldsymbol {y}_{l}^{n})}\right)}]\] \[\leq\mathbb{E}[\exp\left((\sum_{i\in\mathcal{W}^{N_{S_{n}}}_{k,n}(t )-\mathcal{U}^{N_{S_{n}}}_{k,n}(t)}-\sum_{i\in\mathcal{U}^{N_{S_{n}}}_{k,n}(t) })(\alpha^{(t)}_{O_{(i,\cdot)},k}+(2\sum_{l\in S^{N_{S_{n}}}_{n,k}}(\sigma^{(t) }_{S})_{l}^{n}-1)y_{S_{n}}\beta^{(t)}_{O_{(i,\cdot)},k}\right)]\] \[\leq\mathbb{E}[\exp(\kappa/2-\frac{1}{m}\sum_{i\in\mathcal{U}^{N_ {S_{n}}}_{k,n}(t)}(2\sigma^{*}_{S}-1)y_{S_{n}}\beta^{(t)}_{O_{(i,\cdot)},k})]\] \[\leq 2\exp(-\log(T^{*})).\]

Here the first inequality is by \(1/(1+\exp(z))\leq\exp(-z)\); the second inequality is by Lemma 31; the last inequality is by the feeble \(\kappa/2\) and \(\mathbb{E}[e\cdot\beta^{(t)}_{O_{(i,\cdot)},k}\mathds{1}(i\in\mathcal{U}^{ \epsilon}_{k,n}(t))]\geq(\sigma^{*}_{S})^{-1}\log(T^{*})\). Then we have that

\[-\sum_{t_{\beta^{+},k}<t<\widetilde{T}}\frac{\eta_{0}\|\boldsymbol {d}_{k}\|^{2}}{2mK_{1}}\mathbb{E}[\ell^{\prime\ (t)}_{n}\mathds{1}^{n}_{O_{(i)}}(\cdot)] \leq\sum_{t_{\beta^{+},k}<t<\widetilde{T}}\frac{\eta_{0}\| \boldsymbol{d}_{k}\|^{2}}{2mK_{1}}\cdot 2\exp(-\log(T^{*}))\] \[\leq\frac{\widetilde{T}\eta_{0}\|\boldsymbol{d}_{k}\|^{2}}{mK_{1}} \exp(-\log(T^{*}))\leq\frac{T^{*}\eta_{0}\|\boldsymbol{d}_{k}\|^{2}}{T^{*}mK_{1} }\leq(\sigma^{*}_{S})^{-1}\log(T^{*}).\]

We complete the proof that \(0\leq\mathbb{E}[e\cdot\beta^{(t)}_{O_{(i,\cdot)},k}\mathds{1}(i\in\mathcal{U}^{ \epsilon}_{k,n}(t))]-e\cdot\beta^{(0)}_{O_{(i,\cdot)},k}\leq 3(\sigma^{*}_{S})^{-1}\log(T^{*})\leq( \sigma^{*}_{S})^{-1}\alpha\).

We now prove a strong augmented hypothesis that there exist \(i^{*}\in\mathbb{E}[\mathcal{U}^{\epsilon}_{k,n}(t)]\) for \(\forall 0\leq t\leq T^{*}\), we have

\[\mathbb{E}[|\alpha^{(t)}_{O_{(i,\cdot)},k}|/(e\cdot\beta^{(t)}_{O_{(i^{*},\cdot)},k })]\leq\hat{C}\frac{\|\boldsymbol{c}_{k}\|^{2}}{\sigma^{*}_{S}\|\boldsymbol{d}_ {k}\|^{2}},\] (55)

where we set \(\hat{C}=2C^{\prime}\sqrt{2\log(\frac{5Km}{\delta})}\) for some constant \(C^{\prime}\). \(i^{*}\) can be any element satisfies \(|\beta^{(0)}_{O_{(i^{*},\cdot)},k}|=\sigma_{1}/2\|\boldsymbol{d}_{k}\|\), which exists at \(t=0\) by Lemma 7 as well as the fact that \(\|\boldsymbol{c}_{k}\|>\|\boldsymbol{d}_{k}\|\) by their definition in Lemma 25.

[MISSING_PAGE_EMPTY:42]

[MISSING_PAGE_EMPTY:43]

[MISSING_PAGE_FAIL:44]

and we also see that

\[\mathbb{E}[(\sum_{j\in S^{+}_{n,k}}{(\sigma^{(t)}_{S})}^{n}_{j})(\sum_{j\in S^{-}_{ n,k}}{(\sigma^{(t)}_{S})}^{n}_{j})]=\Big{(}\exp(\beta^{(t)}_{Q,k}\cdot\beta^{(t)}_{ K,k}/\|{\bm{b}}_{k}\|^{2})+\exp(-\beta^{(t)}_{Q,k}\cdot\beta^{(t)}_{K,k}/\|{\bm{b}}_{k}\| ^{2})\Big{)}^{-2}\leq\frac{1}{4}.\] (58)

As \(\sum_{i\in\mathbb{E}[\mathcal{W}^{\pm}_{k,n}(t)]}{\bf r}_{i}\cdot e\beta^{(t)} _{O_{(i,\cdot)},k}\) will grow to surpass \(0\) in a limited number of iterations, we can claim that there exists a constant \(C^{\prime}\), such that for the limited decreasing period of \(\mathbb{E}[(\sum_{l\in S^{c}_{n,k}}{(\sigma^{(t)}_{S})}^{n}_{l})]\), we have \(1\geq-\mathbb{E}({\ell^{\prime}_{n}}^{(t)})\geq\widetilde{C}\). Also, \(m\geq\mathbb{E}[|{\mathcal{U}^{\varepsilon}_{k,n}(0)}(0))|\geq m/8\) by Lemma 7, as well as the fact that \(\mathbb{E}[\mathcal{W}^{\varepsilon}_{k,n}(t)]\) will at least preserve the neurons of \(\mathbb{E}[\mathcal{U}^{\varepsilon}_{k,n}(0))]\) along the iterations, without being deactivated as discussed in Lemma 29. Also, we note that in this hypothesised decreasing period, the absolute value of the initially negative \(\mathbb{E}[\sum_{i\in[m]}{\bf r}_{i}\beta^{(t)}_{O_{(i,\cdot)},k}]\) and initially positive \(\beta^{(t)}_{Q,k}\) will all decreasing. Then by Condition 1 we see that the small initialization of MLP as well as the small regularization will make the decreasing order of \(\beta^{(t)}_{Q,k}\) negligible, as

\[\max_{k}\{|-\lambda\beta^{(0)}_{Q,k}+\frac{4\beta^{(0)}_{Q,k}||{ \bm{b}}_{k}|^{4}}{K_{1}}\sum_{e\in[\pm]\sum_{i\in\mathbb{E}[\mathcal{W}^{\pm}_ {k,n}(0)]}{\bf r}_{i}\cdot\beta^{(0)}_{O_{(i,\cdot)},k}\mathbb{E}[(\sum_{j\in S ^{+}_{n,k}}{(\sigma^{(0)}_{S})}^{n}_{j})(\sum_{j\in S^{-}_{n,k}}{(\sigma^{(0)} _{S})}^{n}_{j}){\ell^{\prime}_{n}}^{(t)}]|\}\] \[\leq(\lambda+\frac{5\sigma_{1}\|{\bf u}\|^{4}\|{\bf q}\|}{32K_{1} }\sqrt{2\log(\frac{5Km}{\delta})})\beta^{(0)}_{Q,k}\] \[\leq O(1/C).\] (59)

Here the second inequality is due to Eq.(58), (56) and the definition of the \({\bm{b}}_{k}\) in Eq.(27); the third inequality is by the condition \(\lambda\leq(C\sigma_{0}/2\|{\bf u}\|^{2})^{-1}\) and \(\sigma_{1}\leq(C\sigma_{0}\|{\bf u}\|^{4}\|{\bf q}\|\sqrt{\log(5Km/\delta)}/K_{ 1})^{-1}\). Therefore, it holds that during the decreasing period of \(\beta^{(t)}_{Q,k}\) as well as the period where \(\sum_{i\in\mathbb{E}[\mathcal{W}^{\pm}_{k,n}(t)]}{\bf r}_{i}\cdot\beta^{(t)}_{ O_{(i,\cdot)},k}\) remain negative, we have

\[\sum_{i\in\mathbb{E}[\mathcal{W}^{\pm}_{k,n}(t+1)]}{\bf r}_{i}\cdot\beta^{(t+ 1)}_{O_{(i,\cdot)},k}\geq\sum_{i\in\mathbb{E}[\mathcal{W}^{\pm}_{k,n}(t)]}{\bf r }_{i}\cdot\beta^{(t)}_{O_{(i,\cdot)},k}+\frac{C_{4}\eta_{0}\|{\bm{d}}_{k}\|^{2 }}{K_{1}}(2\frac{1}{1+e^{-2\beta^{(0)}_{Q,k}/|{\bm{b}}_{k}|^{2}}}-1),\]

Here, by a appropriate chosen small \(C_{4}\), we again ignore the regularization term at this period due to \(\lambda=O((C\log(Km/\delta)\|{\bf q}\|))^{-1}\)) for a large \(C\) by Condition 1, and the impact of the learning rate is also controlled due to the slow quadratic decaying nature of \(\eta^{\prime}_{t}\) and a small initial \(\eta_{0}\leq O(0.01C^{-1})\) by Condition 1, so as the changing amount of \(1/(1+e^{-2\beta^{(0)}_{Q,k}/2\|{\bm{b}}_{k}\|^{2}})\) by Eq.(59).

Therefore, by Eq.(58) we have

\[\beta^{(t+1)}_{Q,k}\geq\beta^{(t)}_{Q,k}(1+\frac{C_{4}\eta_{0}\|{ \bm{b}}_{k}\|^{4}}{K_{1}}\sum_{i\in\mathbb{E}[\mathcal{W}^{\pm}_{k,n}(t)]}{ \bf r}_{i}\cdot\beta^{(t)}_{O_{(i,\cdot)},k}\cdot(\sum_{j\in S^{+}_{n,k}}{( \sigma^{(t)}_{S})}^{n}_{j})(\sum_{j\in S^{-}_{n,k}}{(\sigma^{(t)}_{S})}^{n}_{j}))\] (60)

where the inequality is by the negative nature of \(\sum_{i\in\mathbb{E}[\mathcal{W}^{\pm}_{k,n}(t)]}{\bf r}_{i}\cdot\beta^{(t)}_{ O_{(i,\cdot)},k}\), and the decaying nature of \(\eta_{t}\) and Eq.(58). Now we can see that there exists two surrogate sequences \(\underline{\beta^{(t)}_{Q,k}}\) and \(\sum_{i\in\mathbb{E}[\mathcal{W}^{\pm}_{k,n}(t)]}{\bf r}_{i}\cdot e\beta^{(t)} _{O_{(i,\cdot)},k}\) as the lower bound sequence of the \(\beta^{(t)}_{Q,k}\) and \(\sum_{i\in\mathbb{E}[\mathcal{W}^{\pm}_{k,n}(t)]}{\bf r}_{i}\cdot e\beta^{(t)} _{O_{(i,\cdot)},k}\). These two former sequences's initial values are taken as the lower bounds of the latter two (\(\sigma_{0}\|{\bm{b}}_{k}\|^{2}\) and \(-\sqrt{2\log(5Km/\delta)}\cdot\frac{5\sigma_{1}\|{\bm{d}}_{k}\|}{16}\)), and their update rule are

\[\underline{\beta^{(t+1)}_{Q,k}}=\underline{\beta^{(t)}_{Q,k}}+ \underline{\beta^{(t)}_{Q,k}}\frac{C_{4}\eta_{0}\|{\bm{b}}_{k}\|^{4}}{K_{1}} \sum_{i\in\mathbb{E}[\mathcal{W}^{\pm}_{k,n}(t)]}{\bf r}_{i}\cdot\beta^{(t)}_{ O_{(i,\cdot)},k}\cdot(\sum_{j\in S^{+}_{n,k}}{(\sigma^{(t)}_{S})}^{n}_{j})( \sum_{j\in S^{-}_{n,k}}{(\sigma^{(t)}_{S})}^{n}_{j}),\] \[\underline{\sum_{i\in\mathbb{E}[\mathcal{W}^{\pm}_{k,n}(t+1)]}{\bf r }_{i}\cdot e\beta^{(t+1)}_{O_{(i,\cdot)},k}} =\sum_{i\in\mathbb{E}[\mathcal{W}^{\pm}_{k,n}(t)]}{\bf r}_{i}\cdot e \beta^{(t)}_{O_{(i,\cdot)},k}\] \[\quad+\frac{C_{4}\eta_{0}\|{\bm{d}}_{k}\|^{2}}{K_{1}}(2\frac{1}{1+ e^{-2\beta^{(0)}_{Q,k}/2\|{\bm{b}}_{k}\|^{2}}}-1).\]

Then by Lemma 11, let \(a=\frac{C_{4}\eta_{0}\|{\bm{b}}_{k}\|^{4}}{K_{1}},b=\frac{C_{4}\eta_{0}\|{\bm{d}}_{k }\|^{2}}{K_{1}}(2\frac{1}{1+e^{-2\beta^{(0)}_{Q,k}/2\|{\bm{b}}_{k}\|^{2}}}-1)\), we have the maximum iterations \(T_{1}=\frac{-z(0)(1+e^{-2y(0)^{2}})}{b(1-e^{-2y(0)^{2}})}=\frac{\sigma_{1}K_{1} \gamma\sqrt{10\log(5Km/\delta)}(1+e^{-2\sigma^{2}_{0}\|{\bm{b}}_{k}\|^{2}})}{2C_ {4}\|{\bm{d}}_{k}\|(1-e^{-2\sigma^{2}_{0}\|{\bm{b}}_{k}\|^{2}})}\), set \(C_{3}=\sqrt{10}/(2C_{4})\)we obtain the \(T_{1}\) in the lemma. The lower bound of \(\beta_{Q,k}^{(t)}\) along the decreasing period as

\[\underline{\beta_{Q,k}}=\sigma_{0}\|\bm{b_{k}}\|^{2}e^{-\log(5Km/ \delta)}\frac{25\sigma_{1}^{2}\|\bm{b_{k}}\|^{4}(1+e^{-2\sigma_{0}^{2}\|\bm{b_{ k}}\|^{2}})}{1024(1-e^{-2\sigma_{0}^{2}\|\bm{b_{k}}\|^{2}})},\]

Utilizing the scale bounding property \((-\kappa_{\bm{x}}+1)/2\|\bm{u}\|^{2}\leq\|\bm{b_{k}}\|^{2}<\|\bm{u}\|^{2}/2\) in Eq. (27) and (36), we can denoted the lower bound of all \(\beta_{Q,k}^{(t)}=\beta_{K,k}^{(t)}\) for \(\forall k\in[K_{1}]\) as \(\underline{\beta_{Q,k}}\), which can be given as

\[\underline{\beta_{QK}^{-}}=\frac{\sigma_{0}(1-\kappa_{\bm{x}})\|\bm{u}\|^{2}} {2}e^{-\log(5Km/\delta)}\frac{\sigma_{1}^{2}\|\bm{u}\|^{4}(1+e^{-\sigma_{0}^{2 }\|\bm{u}\|^{2}})}{(1-e^{-\sigma_{0}^{2}\|\bm{u}\|^{2}})},\]

Recall \(\sigma_{S}^{*}\) is defined as

\[\sigma_{S}^{*}\coloneqq\frac{1}{1+e^{-2^{-1}\sigma_{0}^{2}(1- \kappa_{\bm{x}})^{2}\|\bm{u}\|^{4}e}}^{-2\sigma_{1}^{2}\log(5Km/\delta)}\frac{ \|\bm{u}\|^{4}(1+e^{-\sigma_{0}^{2}\|\bm{u}\|^{2}})}{(1-e^{-\sigma_{0}^{2}\| \bm{u}\|^{2}})},\]

which is actually can be written as

\[\sigma_{S}^{*}=\frac{1}{1+e^{-2\underline{\beta_{QK}^{-}}}}.\]

Therefore, we see that \(\sigma_{S}^{*}\) is the lower bound of \(\min_{t\in[T^{*}],k\in[K_{1}]}\{\mathop{\mathbb{E}}_{n\in\mathcal{D}_{S}}[ \sum_{j\in S_{n,k}^{S_{n}}}\left(\sigma_{S}^{(t)}\right)_{j}^{n}]\}\).

**Remark 4**.: _As we see that in Lemma 33, we require that the lower bound given in Eq.(53) depends that the values of \(\mathbb{E}[\mathbf{r}_{i}(2\sum_{l\in S_{n,k}^{c}}\left(\sigma_{S}^{(t)} \right)_{l}^{n}-1)\beta_{O_{(i,\cdot)},k}^{(t)}]\) surpasses \(\kappa\), which naturally says that the value of \(\mathbb{E}_{i\in\mathcal{U}_{i,n}^{c}(0)}[\mathbf{r}_{i}\beta_{O_{(i,\cdot)},k }^{(t)}]\) should surpass \(\kappa\) since \(\mathbb{E}[(2\sum_{l\in S_{n,k}^{c}}\left(\sigma_{S}^{(t)}\right)_{l}^{n}-1)]\leq 1\). Therefore \(\mathbb{E}_{i\in\mathcal{U}_{i,n}^{c}(0)}[\mathbf{r}_{i}\beta_{O_{(i,\cdot)},k }^{(t)}]\) should surpass \(0\) at \(\hat{T}\) since \(\kappa>0\), which indicates that \(\hat{T}>T_{1}\). We see that the initial period \(t\leq T_{1}\) is where \(\mathbb{E}_{i\in\mathcal{U}_{i,n}^{c}(0))}[\mathbf{r}_{i}\beta_{O_{(i,\cdot)},k}^{(t)}]\) grow to surpass the initial scale, whose upper bound is \(\kappa/8\) by the definition of \(\kappa\)._

#### i.1.3 Period 2: Increasing Priod of Correct Attention Score

This period's analysis is based on Period 1 in Section I.1.2, or a good initialization such that

\[\sum_{i\in[m]}\mathbf{r}_{i}\beta_{O_{(i,\cdot)},k}^{(0)}>0.\]

**Lemma 35**.: _Under Condition 1, consider the duration after \(T_{1}\) in Lemma 34, then for \(\forall k\in[K_{1}]\) consider the period \(T_{1}\leq t\leq T_{2}=C_{5}\min\{\frac{1+\gamma}{\lambda},\frac{\|\bm{u}\|\|\bm{q }\|}{\lambda K_{1}\sqrt{m}}\}\), where \(C_{5}\) is a small constant. Then the following holds that_

* _We have_ \(\underline{y}(t)\)_,_ \(\overline{y}(t)\)_,_ \(\underline{z}(t)\)_,_ \(\overline{z}(t)\) _be the lower and upper bounds of the increasing_ \(\beta_{Q,k}^{(t)}=\beta_{K,k}^{(t)}\) _and_ \(\sum_{i\in\mathbb{R}[\mathcal{W}_{k,n}^{c}(t)]}\mathbf{r}_{i}\cdot e^{\beta_{ O_{(i,\cdot)},k}^{(t)}}\) _respectively. That is, there exists positive constants_ \(c_{3-6}\)_, for_ \[\underline{a}=\frac{c_{3}(1-\kappa_{\bm{x}})\|\bm{u}\|^{4}}{\lambda\gamma K_{ 1}},\,\overline{a}=\frac{c_{4}\|\bm{u}\|^{4}}{\lambda\gamma K_{1}},\,\underline {b}=\frac{c_{5}(1-\kappa_{\bm{y}})\|\bm{q}\|^{2}}{\lambda\gamma K_{1}}),\, \bar{b}=\frac{c_{6}\|\bm{q}\|^{2}}{\lambda\gamma K_{1}}),\,c^{\prime}=\widetilde{C}\)_, it holds that_ \[\underline{y}(t)\leq\beta_{Q,k}^{(t)}=\beta_{K,k}^{(t)}\leq\overline{y}(t), \quad\underline{z}(t)\leq\sum_{i\in\mathbb{R}[\mathcal{W}_{k,n}^{c}(t)]} \mathbf{r}_{i}\cdot e^{\beta_{O_{(i,\cdot)},k}^{(t)}}\leq\overline{z}(t),\]_for all \(t\geq T_{1}\). Here, \(\overline{y}(t)\), \(\underline{y}(t)\), \(\overline{z}(t)\), \(\underline{z}(t)\) are the unique solutions of the following ODE System respectively_ \[\frac{1}{2}(\mathrm{Ei}(2\underline{y}(t)^{2})+\mathrm{Ei}(-2 \underline{y}(t)^{2})+4\log(\underline{y}(t)))=\underline{abc^{\prime}}^{2}(2 \sigma_{S}^{*}-1)\frac{(t-t_{1})^{2}}{2}\] \[+\frac{1}{2}(\mathrm{Ei}(\log(\frac{\sigma_{S}^{*}}{1-\sigma_{S} ^{*}}))+\mathrm{Ei}(\log(\frac{1-\sigma_{S}^{*}}{\sigma_{S}^{*}})))+4\log( \underline{\beta_{QK}^{-}}),\] \[\underline{z}(t)=\underline{b}c^{\prime}(2\sigma_{S}^{*}-1)(t-T_ {1}),\] \[\frac{1}{2}(\mathrm{Ei}(2\overline{y}(t)^{2})+\mathrm{Ei}(-2 \overline{y}(t)^{2})+4\log(\overline{y}(t)))=\frac{\overline{a}\overline{b}t^ {2}}{2}+\overline{a}\frac{\kappa}{8}t\] \[+\frac{1}{2}(\mathrm{Ei}(\frac{\sigma_{0}^{2}\|\mathbf{u}\|^{4}}{ 2})+\mathrm{Ei}(-2\frac{\sigma_{0}^{2}\|\mathbf{u}\|^{4}}{2}))+4\log(\sigma_{0 }/2\|\mathbf{u}\|^{2}),\] \[\overline{z}(t)=\overline{b}t+\frac{\kappa}{8},\] _where_ \[\mathrm{Ei}(x)=\int_{-\infty}^{x}\frac{e^{t}}{t}\mathrm{d}t=\gamma_{\underline {e}\underline{u}\underline{c}}+\ln x+\exp(x/2)\sum_{n=1}^{\infty}\frac{(-1)^{ n-1}x^{n}}{n!2^{n-1}}\sum_{k=0}^{\lfloor(n-1)/2\rfloor}\frac{1}{2k+1}.\]
* _For some limited constant_ \(\triangle\) _such that_ \(\exists\overline{\triangle}\)_,_ \(\sigma_{S}^{*}<\triangle\leq\overline{\triangle}<1\)_. Then the_ \(\beta_{Q,k}^{(t)}=\beta_{K,k}^{(t)}\) _will grow to make the correct attention score_ \(\underset{n\in\mathcal{V}_{k}^{\delta_{S_{S_{S}}}}}{\mathbb{E}}\left[\sum_{j \in S_{n,k}^{\delta_{S_{S}}}}\left(\sigma_{S}^{(t)}\right)_{j}^{n}\right]\) _achieve the_ \(\triangle\) _in at least a_ \(\triangle(1-\triangle)\) _scaled Gaussian rate such that_ \[\beta_{Q,k}^{(t)}\geq\exp(\frac{abc^{\prime}\triangle(1-\triangle)(2\sigma_{S} ^{*}-1)}{2}(t-T_{1})^{2}+\log(\underline{\beta_{QK}^{-}})).\]

Proof.: By Remark 4, we see that at the initial phase during \(t\geq T_{1}\), we have \(\sum_{i\in[m]}\mathbf{r}_{i}\beta_{O_{(i,\cdot)},k}^{(0)}\leq\kappa/8\), and thus by Eq.(55) in Lemma 30 we see that \(\alpha_{O_{(i,\cdot)},k}^{(0)}\leq\hat{C}_{\triangle}\frac{\|\mathbf{c}_{k}\| ^{2}}{\sigma_{S}^{*}\|\boldsymbol{d}_{k}\|^{2}}\). This indicates that \(\mathbb{E}[\mathbf{A}_{t}^{k,e}]\leq\Theta(\alpha)\) and thus by Lemmar 31 we see that the scale of \(|\underset{n\in\mathcal{V}_{k}^{\delta}}{\mathbb{E}}[ef(\mathbf{E}(S);\mathbb{ E}(\Psi^{(t)}))]|\) is also \(\Theta(\kappa)\). This suggest that there still exists a constant \(\widetilde{C}\), during a certain amount of subsequent iterations we would still have that \(\widetilde{C}\leq-\mathbb{E}[\ell^{\prime}(t)]\leq 1\). Also, \(m\geq\mathbb{E}[|\mathcal{U}_{k,n}^{\varepsilon}(0))|]\geq m/8\) by Lemma 7, as well as the fact that \(\mathbb{E}[\mathcal{W}_{k,n}^{\varepsilon}(t)]\) will at least preserve the neurons of \(\mathbb{E}[\mathcal{U}_{k,n}^{\varepsilon}(0)]\) along the iterations, without being deactivated as discussed in Lemma 29. In addition, recall that in this first stage we also can control the impact of regularization and decaying learning rate by a small \(\lambda\) and a big \(\gamma\) by the sufficiently large \(C\) in Condition 1, which indicates we now have

\[\beta_{Q,k}^{(t+1)}=\beta_{Q,k}^{(t)}+\Theta\Big{(}\beta_{Q,k}^{(t)}\frac{C_{4 \eta_{0}}\|\boldsymbol{b}_{k}\|^{4}}{K_{1}}\sum_{i\in\mathbb{E}[\mathcal{W}_{k, n}^{\varepsilon}(t)]}\mathbb{E}[\mathbf{r}_{i}\beta_{O_{(i,\cdot)},k}^{(t)} \cdot\ell^{\prime}(t)\frac{1}{1+e^{-2\beta_{Q,k}^{(t)}\,2/\|\boldsymbol{b}_{k} \|^{2}}}\frac{1}{1+e^{2\beta_{Q,k}^{(t)}\,2/\|\boldsymbol{b}_{k}\|^{2}}}]\Big{)},\]

and

\[\sum_{i\in\mathbb{E}[\mathcal{W}_{k,n}^{\pm}(t+1)]}\mathbf{r}_{i} \cdot\mathbb{E}[\varepsilon\beta_{O_{(i,\cdot)},k}^{(t+1)}] =\sum_{i\in[\mathcal{W}_{k,n}^{\pm}(t)]}\mathbf{r}_{i}\cdot\beta_{O_ {(i,\cdot)},k}^{(t)}\] \[+\Theta\Big{(}\frac{C_{4}\eta_{0}\|\boldsymbol{d}_{k}\|^{2}}{K_{1 }}\mathbb{E}[\ell^{\prime}(t)(2\frac{1}{1+e^{-2\beta_{Q,k}^{(t)}\,2/\| \boldsymbol{b}_{k}\|^{2}}}-1)]\Big{)}.\]

By Lemma 12, we see that the iteration satisfies the ODE System 2 with a positive initialization, where the parameters in Lemma 12 are \(\ell^{\prime}_{t}=-\mathbb{E}[\ell^{\prime}(t)]\), \(a=\Theta(\frac{C_{4}\eta_{0}\|\boldsymbol{b}_{k}\|^{4}}{K_{1}})\), \(b=\Theta(\frac{C_{4}\eta_{0}\|\boldsymbol{d}_{k}\|^{2}}{K_{1}})\), \(c^{\prime}=\widetilde{C}\). Then by solving the coupled ODE systems, collaborating the scale bounding property \((-\kappa_{\boldsymbol{a}}+1)/2\|\mathbf{u}\|^{2}\leq\|\boldsymbol{b}_{k_{1}}\| ^{2}<\|\mathbf{u}\|^{2}/2,-\kappa_{\boldsymbol{y}}+1/2\|\mathbf{q}\|^{2}\leq\| \boldsymbol{d}_{k_{1}}\|^{2}<\|\mathbf{q}\|^{2}/2\) in Eq. (27) and (36), as well as the Comparison Theorem with some constants \(c_{3-6}\), we can have upper and lower bound of \(\beta_{Q,k}^{(t)}\) and \(\sum_{i\in\mathbb{E}[\mathcal{W}_{k,n}^{\pm}(t)]}\), which is the result in this lemma.

For the second result, given the \(\triangle\), we can directly have a lower bound ODE \(\underline{y}_{\triangle}(t)\) to be the lower bound of the \(\beta_{Q,k}^{(t)}\) via Comparison Theorem, where \(\underline{y}_{\triangle}(t)\) satisfies

\[\underline{y}^{\prime}(t)\geq\underline{y}^{\prime}_{\triangle}(t)= \underline{abc^{\prime}}\triangle(1-\triangle)(2\sigma_{S}^{*}-1))(t-T_{1}) \underline{y}_{\triangle}(t)\Rightarrow\] \[\underline{y}(t)\geq\underline{y}_{\triangle}(t)=\exp(\frac{ \underline{abc^{\prime}}\triangle(1-\triangle)(2\sigma_{S}^{*}-1)}{2}(t-T_{1})^{2}+ \log(\underline{\beta_{QK}^{-}})),\]where the inequality holds by the decaying nature of \(g(x)=x(1-x)\) when \(x>1/2\). The proof is completed.

**Lemma 36**.: _(Asymptotic Property 1). In the first stage, the growing of \(\beta_{Q,k}^{(t)}=\beta_{K,k}^{(t)}\) as well as the attention score enjoys the asymptotic property that_

\[\lim_{t\rightarrow+\infty}\frac{\mathbb{E}[\beta_{Q,k}^{(t)}\,{}^{2}]}{\log(t) }=\Theta(1),\quad\lim_{t\rightarrow+\infty}\frac{\mathbb{E}[\,\underset{n\in \mathcal{V}_{k}^{n}S_{n}}{\mathbb{E}}[\sum_{j\in S_{n,k}^{n}}{(\sigma_{S}^{(t) })_{j}^{n}}]]}{\dfrac{t^{4}}{1+t^{4}}}=\Theta(1).\]

Proof.: By the asymptotic property of \(\text{Ei}(x)\)

\[\lim_{x\rightarrow+\infty}\frac{\text{Ei}(x)+\text{Ei}(-x)}{\frac{\exp(x)}{x }}=1.\]

This suggest that when \(y(t)\geq\underline{y}(t)\) is close to infinity, the lower bound ODE in Lemma 35 will approximately satisfies the following

\[\frac{\exp(2\underline{y}(t)^{2})}{4\underline{y}(t)^{2}}+2\log(\underline{y} (t))\approx\underline{abc^{\prime}}^{2}(2\sigma_{S}^{*}-1)\frac{t^{2}}{2}+ \text{const},\]

This suggest that roughly

\[\lim_{t\rightarrow+\infty}\underline{y}(t)^{2}/\log(t^{2})=\Theta(1).\]

Then we see that as \(y(t)\) goes to infinity, we have a lower bound

\[\lim_{t\rightarrow+\infty}\log(\frac{\mathbb{E}[\sum_{l\in S_{n,k}^{c}}{( \sigma_{S}^{(t)})_{l}^{n}}]}{1-\mathbb{E}[\sum_{l\in S_{n,k}^{c}}{(\sigma_{S}^ {(t)})_{l}^{n}}]})/2\log(t^{2})=\Theta(1)\Rightarrow\lim_{t\rightarrow+\infty }\mathbb{E}[\sum_{l\in S_{n,k}^{c}}{(\sigma_{S}^{(t)})_{l}^{n}}]/\frac{t^{4}}{ 1+t^{4}}=\Theta(1).\]

On the other hand, obtaining an upper bound over \(y(t)\) is relatively easy. Since we have \(2+e^{-2\underline{y}(t)^{2}}+e^{2\underline{y}(t)^{2}}\leq 4\) and \((1-e^{-2\underline{y}(t)^{2}})/(1+e^{-2\underline{y}(t)^{2}})\leq 1\), which gives the upper bound ODE over attention and MLP considering \(z(0)>0\)

\[\frac{1}{2}(\text{Ei}(2\overline{y}(t)^{2})+\text{Ei}(-2\overline {y}(t)^{2})+4\log(\overline{y}(t)))=\frac{\overline{a}\overline{b}t^{2}}{2}+ \overline{a}\frac{\kappa}{8}t+\text{const}.\] \[\overline{z}(t)=bt+\text{const},\]

where the term "const" ensure that \(\overline{y}(0)=y(0)\). The asymptotic property of this ODE system is the same as the one of lower bound ODE. Then consider \(t,y(t)\) both go to infinity, we have some \(\hat{c}\) such that

\[\lim_{t\rightarrow+\infty}\mathbb{E}[\sum_{l\in S_{n,k}^{c}}{(\sigma_{S}^{(t) })_{l}^{n}}]/\frac{(t+\hat{c}t^{1/2})^{4}}{1+(t+\hat{c}t^{1/2})^{4}}=\lim_{t \rightarrow+\infty}\mathbb{E}[\sum_{l\in S_{n,k}^{c}}{(\sigma_{S}^{(t)})_{l }^{n}}]/\frac{t^{4}}{1+t^{4}}=1.\]

### Second Stage: Regularizing the Model

As the \(\beta_{Q,k}^{(t)}=\beta_{K,k}^{(t)}\) and \(e\beta_{O_{(t_{j})},k}^{(t)}\) are continually growing up, we see that the decaying \(-\mathbb{E}[\ell^{\epsilon}(t)]\), as well as the decaying attention score products \((\sum_{j\in S_{n,k}^{+}}{(\sigma_{S}^{(t)})_{j}^{n}})(1-\sum_{j\in S_{n,k}^{+ }}{(\sigma_{S}^{(t)})_{j}^{n}})\) is becoming feeble and feeble, under which we can no longer ignore the regularization term safely when estimating the coefficient gradient dynamics. However, although the regularization can prevent the coefficients from growing, it will maintain their scales without decreasing them.

**Lemma 37**.: _Under Condition 1, consider \(e=\mathbb{E}[y_{S_{n}}]\) for all \(t\in[T_{2},T^{*}]\) it holds that_

\[e\beta^{(t)}_{O_{(i.,.)},k}=O(\frac{\|\mathbf{q}\|^{2}}{\lambda mK _{1}}),\] \[e\beta^{(T^{*})}_{O_{(i.,.)},k}=\Theta(\frac{\sigma_{S}^{*\,2}(1- \kappa_{\boldsymbol{y}})^{2}}{(1+\kappa_{\boldsymbol{y}})^{2}}\log(\frac{e^{- \kappa}\|\mathbf{q}\|^{2}(2\sigma_{S}^{*}-1)}{\lambda mK_{1}})),\] \[\beta^{(t)}_{Q,k}=O(\sqrt{\|\mathbf{u}\|^{2}\|\log(\frac{\| \mathbf{u}\|^{2}\|\mathbf{q}\|^{2}}{\lambda^{2}mK_{1}}^{2})}),\] \[\beta^{(T^{*})}_{Q,k}=\Theta(\|\mathbf{u}\|\sqrt{\log(\frac{\| \mathbf{u}\|^{2}\sigma_{S}^{*\,2}(1-\kappa_{\boldsymbol{y}})^{2}}{\lambda K_{ 1}(1+\kappa_{\boldsymbol{y}})^{2}}\log(\frac{e^{-\kappa}\|\mathbf{q}\|^{2}(2 \sigma_{S}^{*}-1)}{\lambda mK_{1}}))}),\] \[\mathbb{E}[(\sum_{j\in S_{n,k}^{*}}{(\sigma_{S}^{(t)})}_{j}^{n})] =O(\frac{1}{1+\frac{\lambda^{2}mK_{1}}{2\|\mathbf{u}\|^{2}\|\mathbf{q}\|^{2}} {2}}),\] \[\mathbb{E}[(\sum_{j\in S_{n,k}^{*}}{(\sigma_{S}^{(T^{*})})}_{j}^{n })]=\Theta(\frac{1}{1+\frac{\lambda K_{1}(1+\kappa_{\boldsymbol{y}})^{2}}{\| \mathbf{u}\|^{2}\sigma_{S}^{2}(1-\kappa_{\boldsymbol{y}})^{2}}\log^{-1}(\frac {e^{-\kappa}\|\mathbf{q}\|^{2}(2\sigma_{S}^{*}-1)}{\lambda mK_{1}})}),\]

_where \(e\beta^{(t)}_{O_{(i.,.)},k}\) represents \(m\mathbf{r}_{i}\beta^{(t)}_{O_{(i.,.)},k}\). That is, we consider the positive growth of \(\mathbb{E}[\beta^{(t)}_{O_{(i.,.)},k}]\)._

Proof.: We will prove the desired argument based on the following induction hypothesis:

\[e\beta^{(t)}_{O_{(i.,.)},k}=O(\frac{\|\mathbf{q}\|^{2}}{\lambda mK _{1}}),\] \[\beta^{(t)}_{Q,k}=O(\|\mathbf{u}\|\sqrt{\log(\frac{2\|\mathbf{u} \|^{2}\|\mathbf{q}\|^{2}}{\lambda^{2}mK_{1}}^{2})}),\]

We split the situations into two cases:

(i). \(e\beta^{(t)}_{O_{(i.,.)},k}\in\Theta(\frac{\sigma_{S}^{*\,2}(1-\kappa_{ \boldsymbol{y}})^{2}}{(1+\kappa_{\boldsymbol{y}})^{2}}\log(\frac{e^{-\kappa} \|\mathbf{q}\|^{2}(2\sigma_{S}^{*\,-1})}{\lambda mK_{1}}))\),

and \(\beta^{(t)}_{Q,k}\leq\Theta(\|\mathbf{u}\|\sqrt{\log(\frac{\|\mathbf{u}\|^{2} \sigma_{S}^{*\,2}(1-\kappa_{\boldsymbol{y}})^{2}}{\lambda K_{1}(1+\kappa_{ \boldsymbol{y}})^{2}}\log(\frac{e^{-\kappa}\|\mathbf{q}\|^{2}(2\sigma_{S}^{*\, -1})}{\lambda mK_{1}}))})\);

(ii). \(e\beta^{(t)}_{O_{(i.,.)},k}\geq\frac{\|\mathbf{q}\|^{2}}{2\lambda mK_{1}}\),

and \(\beta^{(t)}_{Q,k}\geq\|\mathbf{u}\|\sqrt{\frac{1}{2}\log(\frac{6\|\mathbf{u}\|^ {2}\|\mathbf{q}\|^{2}}{\lambda^{2}mK_{1}}^{2})}\Rightarrow\mathbb{E}[(\sum_{j \in S_{n,k}^{+}}{(\sigma_{S}^{(t)})}_{j}^{n})]\geq\frac{1}{1+\frac{\lambda^{2} mK_{1}}{6\|\mathbf{u}\|^{2}\|\mathbf{q}\|^{2}}}\). Easy to note that the scales' orders of the case (i)'s quantities are less than those of case (ii), thus this split is plausible.

Recall

\[\beta^{(t+1)}_{O_{(i.,.)},k}=(1-\eta_{t}\lambda)\beta^{(t)}_{O_{(i.,.)},k}- \eta_{t}\frac{\|\boldsymbol{d}_{k}\|^{2}}{2K_{1}}\sum_{e\in[\pm]}\mathbf{r}_{i }\underset{n\in\mathbb{V}_{k}^{*}}{\mathbb{E}}[\ell^{\prime}_{n}{}^{(t)} \mathds{1}^{n}_{O_{(i)}}(\zeta_{n})(\zeta_{n})^{(t)}_{l}-\sum_{l\in S_{n,k}^{ *}}{(\sigma_{S}^{(t)})}_{l}^{n}-\sum_{l\in S_{n,k}^{*}}{(\sigma_{S}^{(t)})}_{l }^{n})].\]

Then it's easy to check that for case (i), as by Lemma 30 we see that the magnitude of \(\mathbb{E}[\alpha^{(t)}_{O_{(i.,.)},k}\|]\) is controlled by some \(\hat{C}\sigma_{S}^{*\,-2}(1+\kappa_{\boldsymbol{y}})^{2}(1-\kappa_{ \boldsymbol{y}})^{-2}\beta^{(t)}_{O_{(i.,.)},k}\geq\hat{C}\). That means that the term \(\mathbb{E}[\mathbf{A}_{t}^{k,e}]\) can be controlled by its contributor \(\Theta(e\beta^{(t)}_{O_{(i.,.)},k})\). Then we have

\[\Theta(e\beta^{(t)}_{O_{(i.,.)},k}/\mathbb{E}[-\ell^{\prime}_{n} {}^{(t)}]) \leq\Theta(e\beta^{(t)}_{O_{(i.,.)},k}(1+e^{\kappa}e^{\sigma_{S}^{ *\,-2}(1+\kappa_{\boldsymbol{y}})^{2}(1-\kappa_{\boldsymbol{y}})^{-2}e\beta^{( t)}_{O_{(i.,.)},k}}))\] \[\leq\Theta(e^{\kappa}e^{\sigma_{S}^{*\,-2}(1+\kappa_{\boldsymbol {y}})^{2}(1-\kappa_{\boldsymbol{y}})^{-2}e\beta^{(t)}_{O_{(i.,.)},k}})\] \[\leq\Theta(\|\mathbf{q}\|^{2}(2\sigma_{S}^{*\,-1})\lambda mK_{1} }),\]

where the first inequality is by the definition of \(\mathbb{E}[-\ell^{\prime}_{n}{}^{(t)}])\) (similar to the techniques in Lemma 32) and the definition of \(\mathbb{E}[\mathbf{A}_{t}^{k,e}]\); the second inequality is by \(g(x)=x<e^{x}-1\) as well as \(\sigma_{S}^{*\,-2}(1+\kappa_{\boldsymbol{y}})^{2}(1-\kappa_{\boldsymbol{y}})^{-2}>1\); the second inequality is by the case (i) hypothesis. Then we would have

\[\lambda e\beta^{(t)}_{O_{(i.,.)},k}\leq\Theta(-\frac{\|\boldsymbol{d}_{k}\|^{2}}{2K _{1}}\sum_{e\in[\pm]}\mathbb{E}[\mathbf{r}_{i}\underset{n\in\mathbb{V}_{k}^{*}}{ \mathbb{E}}[\ell^{\prime}_{n}{}^{(t)}\mathds{1}^{n}_{O_{(i)}}(\zeta_{n})^{(t)}_{l }(\zeta_{n})^{n}_{l}-\sum_{l\in S_{n,k}^{*}}{(\sigma_{S}^{(t)})}_{l}^{n})]]).\] (61)

[MISSING_PAGE_EMPTY:50]

[MISSING_PAGE_FAIL:51]

\(\mathbb{E}[(2\sum_{l\in S^{e}_{n,k}}\left(\sigma^{(t)}_{S}\right)^{n}_{l}-1)e \beta^{(t)}_{O_{(i,\cdot),k}}]\), the neuron would change into the neuron set \(\underset{n\in V^{e}_{k}}{\mathbb{E}}[\mathcal{U}^{e}_{k,n}(t)-(\mathcal{W}^{-e }_{k,n}(t)-\mathcal{U}^{-e}_{k,n}(t))]\). As such, the \(\alpha^{(t)}_{O_{(i,\cdot),k}}\) would again increase at a normal speed, which is even faster than \(e\beta^{(t)}_{O_{(i,\cdot),k}}\) due to the update rules and the fact that \(\|\bm{c}_{k}\|>\|\bm{d}_{k}\|\). As such, the neuron set \(\underset{n\in V^{e}_{k}}{\mathbb{E}}[\mathcal{U}^{e}_{k,n}(t)-(\mathcal{W}^{- e}_{k,n}(t)-\mathcal{U}^{-e}_{k,n}(t))]\) would again fall back into the neuron set \(\underset{n\in V^{e}_{k}}{\mathbb{E}}[\mathcal{U}^{e}_{k,n}(t)\cap(\mathcal{W} ^{-e}_{k,n}(t)-\mathcal{U}^{-e}_{k,n}(t))]\), where the update speed is again feeble. And it will increase until \(\mathbb{E}[(2\sum_{l\in S^{e}_{n,k}}\left(\sigma^{(t)}_{S}\right)^{n}_{l}-1)e \beta^{(t)}_{O_{(i,\cdot),k}}]\) catch up.

Besides, we see that the expected attention score will grow up considerably, where we can see that there exist some constant \(\widetilde{c}>1/2\), \(\widetilde{c}<\mathbb{E}[\left(\sigma^{(t)}_{S}\right)^{n}_{l}]\leq 1\). As such, ultimately we have \(\mathbb{E}[(\sum_{l\in S^{e}_{n,k}}\left(\sigma^{(t)}_{S}\right)^{n}_{l}- \sum_{l\in S^{-e}_{n,k}}\left(\sigma^{(t)}_{S}\right)^{n}_{l}]=\Theta(1)\), \(\alpha^{(\mathcal{T}^{*})}_{O_{(i,\cdot),k}}\leq e\beta^{(\mathcal{T}^{*})}_ {O_{(i,\cdot),k}}\) and \(\mathbb{E}[\mathbf{A}^{k,e}_{t}]=\Theta(\mathbb{E}[e\beta^{(t)}_{O_{(i,\cdot),k}}])\). Then following the process in Lemma 37 we can obtain the results. Here we omit this part since the proving procedure is the same to Lemma 37, despite we see \(\mathbb{E}[(\sum_{l\in S^{e}_{n,k}}\left(\sigma^{(t)}_{S}\right)^{n}_{l}-\sum_ {l\in S^{-e}_{n,k}}\left(\sigma^{(t)}_{S}\right)^{n}_{l})]=\Theta(1)\) and \(\mathbb{E}[\mathbf{A}^{k,e}_{t}]=\Theta(\mathbb{E}[e\beta^{(t)}_{O_{(i,\cdot),k}}])\). 

Again, similar to Lemma 36, we can have asymptotic property when considering the decaying impact of the learning rate, as well as the cross-entropy loss. We directly provide the following two lemmas. Due to the similarity of the proof procedures of Lemma 35 and Lemma 36, we omit the proofs of the following two lemmas as well as the constant details for simplicity.

**Lemma 39**.: _(Asymptotic Property 2). If we consider the impact of the decaying learning rate at the second stage and do not consider the decaying of cross-entropy loss, for some constants \(\widetilde{c},\underline{d},\underline{c},\underline{d}\) regarding \(K_{1},\gamma,\|\mathbf{u}\|,\|\mathbf{q}\|,\kappa_{\bm{\varkappa}},\kappa_{\bm {\varkappa}}\), we will have_

\[\underline{y}(t)\leq\beta^{(t)}_{Q,k}=\beta^{(t)}_{K,k}\leq \overline{y}(t),\quad\underline{z}(t)\leq\sum_{i\in\mathbb{E}[\mathcal{W}^{ \underline{z}}_{k,n}(t)]}\mathbf{r}_{i}\cdot e\beta^{(t)}_{O_{(i,\cdot),k}} \leq\overline{z}(t),\]

_for all \(t\geq T_{2}\). Here, \(\overline{y}(t)\), \(\underline{y}(t)\), \(\overline{z}(t)\), \(\underline{z}(t)\) are the unique solutions of the following ODE System respectively_

\[\frac{1}{2}(\mathrm{Ei}(2\underline{y}(t)^{2})+\mathrm{Ei}(-2 \underline{y}(t)^{2})+4\log(\underline{y}(t)))=\overline{a}\left(\mathrm{Li}_ {2}\left(\frac{\overline{d}+\overline{c}t}{-\gamma\overline{c}-\overline{c}+ \overline{d}}\right)+\log(\overline{c}t+\overline{d})\log\left(\frac{\overline{ c}(\gamma+t)}{\overline{c}\gamma-\overline{d}}\right)\right)\] \[+\frac{1}{2}(\mathrm{Ei}(\log(\frac{\sigma^{*}_{S}}{1-\sigma^{*}_{S }}))+\mathrm{Ei}(\log(\frac{1-\sigma^{*}_{S}}{\sigma^{*}_{S}})))+4\log(\beta^{ -}_{QK}),\] \[\underline{z}(t)=\underline{b}c^{\prime}(2\sigma^{*}_{S}-1)(t-T_{ 1}),\] \[=\underline{a}\left(\mathrm{Li}_{2}\left(\frac{d+\underline{c}t}{- \gamma\overline{c}-\underline{c}+\underline{d}}\right)+\log(\underline{c}t+ \underline{d})\log\left(\frac{c(\gamma+t)}{\underline{c}\gamma-\underline{d}} \right)\right)\] \[+\frac{1}{2}(\mathrm{Ei}(\underline{\sigma}^{2}_{0}\|\mathbf{u} \|^{4})+\mathrm{Ei}(-2\frac{\sigma^{2}_{0}\|\mathbf{u}\|^{4}}{2}))+4\log( \sigma_{0}/2\|\mathbf{u}\|^{2}),\] \[\overline{z}(t)=\overline{b}t+\frac{\kappa}{8},\]

_where_

\[\mathrm{Li}_{2}(x)=-\int_{0}^{x}\frac{\ln(1-t)}{t}dt.\]

_Additionally, we would have asymptotic property that_

\[\lim_{t\rightarrow+\infty}y(t)^{2}=\lim_{t\rightarrow+\infty}\Theta(\log(\log^{ 2}(t))),\quad\lim_{t\rightarrow+\infty}\frac{\mathbb{E}[\underset{n\in V^{e}_{k }}{\mathbb{E}}[\sum_{j\in S^{e}_{n,k}}\left[\gamma_{j\in S^{e}_{n,k}}\left( \sigma^{(t)}_{S}\right)^{n}_{j}]\right]]}{\frac{\log^{4}(t)}{1+\log^{4}(t)}}= \Theta(1).\]

**Lemma 40**.: _(Asymptotic Property 3). If we put our sight on the long period and take the decaying property of the \(-\mathbb{E}[\ell^{\prime}(t)]\) into account, for some constants \(\overline{a},\overline{b},\overline{c},\underline{d},\overline{j},\underline{a}, \underline{b},\underline{c},\underline{d},\underline{j}\), we will have_

\[\underline{y}(t)\leq\beta^{(t)}_{Q,k}=\beta^{(t)}_{K,k}\leq \overline{y}(t),\quad\underline{z}(t)\leq\sum_{i\in\mathbb{E}[\mathcal{W}^{ \underline{z}}_{k,n}(t)]}\mathbf{r}_{i}\cdot e\beta^{(t)}_{O_{(i,\cdot),k}}\leq \overline{z}(t),\]_for all \(t\geq T_{2}\). Here, \(\overline{y}(t)\), \(\underline{y}(t)\), \(\overline{z}(t)\), \(\underline{z}(t)\) are the unique solutions of the following ODE System respectively_

\[\frac{1}{2}(\mathrm{Ei}(2\underline{y}(t)^{2})+\mathrm{Ei}(-2 \underline{y}(t)^{2})+4\log(\underline{y}(t)))=\overline{a}(\frac{\overline{ j}(\overline{d}+\overline{c}t)}{\overline{c}\overline{j}}+\frac{\log(\overline{c}t+ \overline{d})\log(\frac{\overline{b}+\overline{c}\overline{j}t+\overline{d} }{\overline{b}+\overline{d}-\overline{d}\overline{j}})}{\overline{c} \overline{j}})\] \[+\frac{1}{2}(\mathrm{Ei}(\log(\frac{\sigma_{S}^{*}}{1-\sigma_{S} ^{*}}))+\mathrm{Ei}(\log(\frac{1-\sigma_{S}^{*}}{\sigma_{S}^{*}})))+4\log( \underline{\beta_{QK}^{-}}),\] \[\underline{z}(t)=bc^{\prime}(2\sigma_{S}^{*}-1)(t-T_{1}),\] \[=\underline{a}(\frac{\underline{j}(d+ct)}{\underline{c}\underline {j}}+\frac{\log(\underline{c}t+\underline{d})\log(\frac{\underline{b}+c \underline{j}t+\underline{d}}{\underline{b}+\underline{d}-\underline{d} \underline{j}})}{\underline{c}\underline{j}})\] \[+\frac{1}{2}(\mathrm{Ei}(\frac{\sigma_{0}^{2}\|\mathbf{u}\|^{4 }}{2})+\mathrm{Ei}(-2\frac{\sigma_{0}^{2}\|\mathbf{u}\|^{4}}{2}))+4\log( \sigma_{0}/2\|\mathbf{u}\|^{2}),\] \[\overline{z}(t)=\overline{b}t+\frac{\kappa}{8},\]

_where_

\[\mathrm{Li}_{2}(x)=-\int_{0}^{x}\frac{\ln(1-t)}{t}dt.\]

_Additionally, we would have asymptotic property that_

\[\lim_{t\to+\infty}y(t)^{2}=\lim_{t\to+\infty}\Theta(\log(\log^{2}(t))),\quad \lim_{t\to+\infty}\frac{\mathbb{E}[\underset{n\in V_{\overline{W}_{0}^{g_{n}} }}{\mathbb{E}}[\sum_{j\in S_{n,k}^{g_{n}}}{\mathbb{E}}\left(\sigma_{S}^{(t)} \right)_{j}^{n}]]}{\frac{\log^{4}(t)}{1+\log^{4}(t)}}=\Theta(1).\]

It's obvious that the decaying impact of the learning rate and cross-entropy loss are at the similar order. Also, if we consider decaying learning rate, the right side of the inequality would be smaller. \(z(t)\) would be in a \(\Theta(\log(\log(t)))\) order when \(z(t)\) get large, which will make the right side of the \(y(t)\)'s formula contain an intergral of \(\Theta(\log\log(t))\), which is obviously slower.

## Appendix J Exponential Convergence of 0-1 Loss

We continue our proof after Lemma 33. In this section, we assume all the events in the Section D hold, denoted as \(\Upsilon_{\mathbf{P}v}\).

**Lemma 41**.: _The Frobenius norm of \(\mathbf{W}_{O}^{\boldsymbol{y}}\) and its gradient can be bounded:_

\[\|\mathbf{W}_{O}^{\boldsymbol{y}}\|_{F}^{2}=O(\frac{K_{1}\|\mathbf{q}\|^{2}}{ \lambda^{2}m}),\quad\|\nabla_{\mathbf{W}_{O}^{\boldsymbol{y}}\cap c}L_{ \mathcal{B}_{t}}(\Psi^{(t)})\|_{F}^{2}=O(\frac{K_{1}\|\mathbf{q}\|^{2}}{m}).\]

Proof.: For \(\forall i\in[m]\), by the gradient update rule in Eq.(22), as well as Lemma 4's insight we see that the lengths of the \(\mathbf{W}_{O}^{\boldsymbol{y}}\) on certain projection direction will continue to grow until being stuck by the regularization, which is a \(\lambda\)-scaled \(\mathbf{W}_{O}^{\boldsymbol{y}}\) itself. Due to the low-noise condition in Condition 1 with a sufficiently large \(C\) as well as the isotropy of noise, the learning progress of features would be the main contributor to the F norm of NN matrices and the noise, validated in Figure 2 (iii-iv). We can consider an extreme case where all the samples in a single batch belongs to some concept \(k\in[K_{1}]\), which we can have the upper bound of the first term of the right side of the inequality over the \(k\)-th concept's corresponding projection direction, and thus we can derive an upper bound

\[(\mathbf{W}_{O_{(i,\cdot)}}^{\boldsymbol{y}}\frac{(t)}{\|\mathbf{q}_{k}^{k}\| })^{2}\leq\lambda^{-2}\frac{1}{m^{2}}(\|\boldsymbol{c}_{k}\pm\boldsymbol{d}_{ k}\|^{2}+\frac{3\sigma_{\xi}^{2}d\mathbf{y}}{2})=\Theta(\frac{\|\mathbf{q}\|^{2}}{ \lambda^{2}m^{2}}),\] (62)

where the first inequality is by \((2\sum_{l\in S_{n,k}^{e}}{\left(\sigma_{S}^{(t)}\right)_{l}^{n}}-1)\leq 1\), and Lemma 6; the last equality is by the low noise condition \(\sigma_{\xi}\leq\|\mathbf{q}\|/C\sqrt{d\mathbf{y}}\) in Condition 1. Then by the low noise condition as well as the data model's definition we see that all the 2-norm of the \(\mathbf{W}_{O}^{\boldsymbol{y}}\) is controlled by the \(K_{1}\) concepts' corresponding lengths in projection space. Then by the definition of Frobenius norm and Eq.(22) we have

\[\|\mathbf{W}_{O}^{\boldsymbol{y}}\|_{F}^{2}\leq\Theta(\frac{K_{1}\|\mathbf{q}\| ^{2}}{\lambda^{2}m}),\quad\|\nabla_{\mathbf{W}_{O}^{\boldsymbol{y}}\cap c}L_{ \mathcal{B}_{t}}(\Psi^{(t)})\|_{F}^{2}\leq\Theta(\frac{K_{1}\|\mathbf{q}\|^{2} }{m}).\]

[MISSING_PAGE_EMPTY:54]

[MISSING_PAGE_EMPTY:55]

On the other hand, when conditioned on \(\mathbb{E}[\mathbf{W}_{\mathbf{0}}^{\mathbf{y}}(t)],t\geq T^{\prime}\), we compute the minimum admissible disparity between \(\mathbf{W}_{\mathbf{0}}^{\mathbf{x}}(T^{\prime})\), \(\mathbf{W}_{\mathbf{K}}^{\mathbf{x}}(T^{\prime})\) and \(\mathbb{E}[\mathbf{W}_{\mathbf{0}}^{\mathbf{x}}(T^{\prime})]=\mathbb{E}[ \mathbf{W}_{\mathbf{K}}^{\mathbf{x}}\hat{T}]\). Considering all the activated neurons, when \(\sum_{i\in\mathcal{W}_{k,n}^{\mathbf{x}}(t)}\mathbb{E}[\mathbf{r}_{i}(2\sum_{l \in\mathcal{S}_{n,k}^{\mathbf{x}}}(\sigma_{S}^{(\hat{T})})_{l}^{n}-1)\beta_{( \mathcal{O}_{(i,j)},k]}^{(\hat{T})}=0\), we should have \(\sum_{i\in\mathcal{W}_{k,n}^{\mathbf{x}}(t)}\mathbb{E}[\mathbf{r}_{i}\alpha_{O _{(i,j)}^{(\hat{T})},k}^{(\hat{T})}]\geq 0\) otherwise some of the neurons must be deactivated, which is contradicted by the definitions of \(\mathcal{W}_{k,n}^{\mathbf{x}}(t)\). In this case we can magnify the impact of \(\sum_{i\in\mathcal{W}_{k,n}^{\mathbf{x}}(t)}\mathbb{E}[\mathbf{r}_{i}(2\sum_{ l\in\mathcal{S}_{n,k}^{\mathbf{x}}}(\sigma_{S}^{(\hat{T})})_{l}^{n}-1)\beta_{O _{(i,j)},k}^{(\hat{T})}]\) by considering \(\sum_{i\in\mathcal{W}_{k,n}^{\mathbf{x}}(t)}\mathbb{E}[\mathbf{r}_{i}\alpha_{ O_{(i,j)}^{(\hat{T})},k}^{(\hat{T})}]=0\). As such, the minimum admissible disparity would be the case where \(\bm{b}_{k}^{\top}\mathbf{W}_{\mathbf{0}}^{\mathbf{x}}(T^{\prime})\bm{b}_{k}\) and \(\bm{b}_{k}^{\top}\mathbf{W}_{\mathbf{K}}^{\mathbf{x}}(T^{\prime})\bm{b}_{k}\) both differ from \(\beta_{Q,k}^{(\hat{T})}=\beta_{K,k}^{(\hat{T})}\) by the amount of \(\beta_{QK}^{-}\). Recall the definition of \(\underline{\beta_{QK}^{-}}\) in Lemma 34, and collaborating with Lemma 9, we have the minimum admissible disparity be

\[\sigma_{0}(1-\kappa_{\mathbf{x}})e^{-\log(5Km/\delta)\frac{\sigma_{1}^{2}\| \mathbf{u}\|^{4}(1+\epsilon-\sigma_{0}^{2}\|\mathbf{u}\|^{2})}{(1-\epsilon- \sigma_{0}^{2}\|\mathbf{u}\|^{2})}}\cdot\text{Recall}\]

\[\nu\coloneqq\min\{2\sqrt{2}\sigma_{1}/(1+\kappa_{\mathbf{y}}),\sigma_{0}(1- \kappa_{\mathbf{x}})e^{-\log(5Km/\delta)\frac{\sigma_{1}^{2}\|\mathbf{u}\|^{4 }(1+\epsilon-\sigma_{0}^{2}\|\mathbf{u}\|^{2})}{(1-\epsilon-\sigma_{0}^{2}\| \mathbf{u}\|^{2})}}\},\]

the proof is completed. 

**Lemma 44**.: _For \(t\in\{1,\cdots,T\}\), for \(\mathbf{W}\in\{\mathbf{W}_{\mathbf{0}}^{\mathbf{x}},\mathbf{W}_{\mathbf{0}}^{ \mathbf{x}},\mathbf{W}_{\mathbf{0}}^{\mathbf{y}}\}\) and \(X\in\{Q,K,O\}\) it follows that_

1. \(\|\mathbf{W}^{(t+1)}-\mathbf{W}_{t}{}^{(t+1)}\|_{F}\leq\Theta(\frac{K_{1}^{1/2 }\|\mathbf{q}\|((L-1)^{1/2}\|\mathbf{u}\|+1)}{m^{1/2}}\eta_{t})\)_,_
2. \(\|\mathbf{W}^{(s+1)}-\mathbf{W}_{t}{}^{(s+1)}\|_{F}\leq(1-\eta_{s}\lambda)\| \mathbf{W}^{(s)}-\mathbf{W}_{t}{}^{(s)}\|_{F},\forall s\geq t+1\)_,_
3. \(\sum_{t=0}^{T}\|D_{X}^{t}\|_{\infty}^{2}\leq\Theta(\frac{K_{1}\|\mathbf{q}\|^{ 2}((L-1)\|\mathbf{u}\|^{2}+1)}{m\lambda^{2}(\gamma+T)})\)_._

Proof.: We provide the proof by extending the techniques in [34, 33, 36] to Hilbert-Schmidt space, whose inner product is defined by trace. First we note that \(\eta_{0}=\frac{2}{\gamma+1}\leq\min\{1/(L_{\text{Logit}}+\lambda),1/2\lambda\}\), where \(L_{\text{Logit}}\) is the \(L\)-smooth Lipschitz constant of cross-entropy loss \(\ell(\cdot)\), which is \(1\). The first statement can be shown as follows. Since by definition we see that \(\mathbf{W}^{(t)}=\mathbf{W}_{t}{}^{(t)}\), we only need to check the maximum disparity of the gradient in a single iteration update, then by Lemma 41 and Lemma 42 we readily obtain the results.

For the second statement, following the proof in [94, 34], we see that the Lipschitz smoothness of cross-entropy loss denotes that

\[\langle\nabla_{\mathbf{W}}L_{\mathcal{B}}(\Psi)-\nabla_{\mathbf{W}^{\prime}}L_{ \mathcal{B}}(\Psi),\mathbf{W}-\mathbf{W}^{\prime}\rangle\geq\frac{1}{L_{\text{ Logit}}}\|\nabla_{\mathbf{W}}L_{\mathcal{B}}(\Psi)-\nabla_{\mathbf{W}^{ \prime}}L_{\mathcal{B}}(\Psi)\|_{F}^{2}.\] (63)

Then we have that for \(s\geq t+1\),

\[\|\mathbf{W}^{(s+1)}-\mathbf{W}_{t}{}^{(s+1)}\|_{F}^{2} =\Big{\|}(1-\eta_{s}\lambda)\left(\mathbf{W}^{(s)}-\mathbf{W}_{t }{}^{(s)}\right)-\eta_{s}\left(\partial_{g}l\left(g_{s},Z_{s}\right)-\partial_{g }l\left(g_{s}^{t},Z_{s}\right)\right)\Big{\|}_{F}^{2}\] \[=(1-\eta_{s}\lambda)^{2}\left\|\mathbf{W}^{(s)}-\mathbf{W}_{t}{}^ {(s)}\right\|_{F}^{2}-2\eta_{s}\left(1-\eta_{s}\lambda\right)\cdot\] \[\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad \quad\quad\quad\quad\quad\quad\quad\quad\quadFrom the following inequality,

\[\prod_{s=t+1}^{T}(1-\eta_{s}\lambda)=\prod_{s=t+1}^{T}\frac{\gamma+s-2}{\gamma+s}< \frac{\gamma+t}{\gamma+T}\]

where the last inequality hold clearly by expanding the product, the right hand side of the Eq.(64) is upper bounded as follows

\[\Theta(\frac{K_{1}^{\frac{1}{2}}\|\mathbf{q}\|((L-1)^{\frac{1}{2}} \|\mathbf{u}\|+1)}{m^{\frac{1}{2}}})\eta_{t}\prod_{s=t+1}^{T}(1-\eta_{s}\lambda) \leq\Theta(\frac{K_{1}^{\frac{1}{2}}\|\mathbf{q}\|((L-1)^{\frac{1}{2}} \|\mathbf{u}\|+1)}{m^{\frac{1}{2}}})\frac{\eta_{t}(\gamma+t)}{\gamma+T}\] \[=\frac{\Theta(2\frac{K_{1}^{\frac{1}{2}}\|\mathbf{q}\|((L-1)^{ \frac{1}{2}}\|\mathbf{u}\|+1)}{m^{\frac{1}{2}}})}{\lambda(\gamma+T)}.\]

We finally obtain the desired bound:

\[\sum_{t=0}^{T}\left\|D_{t}\right\|_{\infty}^{2}\leq\sum_{t=0}^{T}\Theta(\frac{ K_{1}\|\mathbf{q}\|^{2}((L-1)\|\mathbf{u}\|^{2}+1)}{m\lambda^{2}(\gamma+T)^{2}}) \leq\Theta(\frac{K_{1}\|\mathbf{q}\|^{2}((L-1)\|\mathbf{u}\|^{2}+1)}{m\lambda^ {2}(\gamma+T)}).\]

**Remark 6**.: _Utilizing this lemma, the exponential convergence over the 0-1 loss is readily obtained._

## Appendix K Out-of-Distribution Generalization

**Lemma 45**.: _OOD 1: Master of Polysemy of Words. During testing, The prompt length \(L^{*}\) can be any positive integer. The \(\mathcal{D}_{\bm{x}}^{*}\) can have any new probability distribution that differs from the training distribution, satisfying that each prompt has at least one co-concept \(k\in[K_{1}]\), with equal chance to have positive or negative semantic labels. Additionally, a single \((\bm{x},\bm{y})\sim\mathcal{D}_{\bm{x}}^{*}\times\mathcal{D}\bm{y}^{*}\) pair can appear in at least \(\|\bm{z}\|_{0}\) concept-specific prompts/tasks. Importantly, all of the tasks in this new distribution \(\mathcal{D}^{*}\) enjoy Bayes-Optimal test error \(L_{\mathcal{D}^{*}}^{0-1}(\Psi^{(T^{*})})\leq\varepsilon\)._

This lemma demonstrate the strong OOD Generalization ability of transformer utilizing multi-concept semantics, suggesting the efficiency transformer to conduct unseen ICL tasks just by its learned knowledge on the two non-orthogonal dictionaries. Also, this lemma showcases an intriguing phenomenon since it allows multiple concepts with comparable chance along word-demo pairs - even with the same input-output pair and query, the model can produce diverse responses when provided varying contextual (concept / task) information. For instance, with the prompt "Japan: Sakura, China:", the LLM may output "Penoy" (national flower) or "Panda" (national symbol), reflecting different conceptual (task) interpretations. Both answers are right since they are all the co-concept tasks. Interestingly, adding another demonstration like "Japan: Sakura, France: Iris germanica, China:" stabilizes the response to "Penoey", since the only co-concept is left to be "national flower". In our theory, we make an elementary explanation to this flexible, context-sensitive in-context learning (ICL) behavior by attributing it to the transformer's ability to harness multi-concept semantics.

**Lemma 46**.: _OOD 2: Innovation. During testing, the distribution of \(\mathcal{D}_{\bm{x}}^{*}\times\mathcal{D}_{\bm{y}}^{*}\) can enjoy data shift. Specifically, suggest we now have a new \(\mathbf{M}^{*}\) and \(\mathbf{Q}^{*}\) to define new \(\mathcal{D}_{\bm{x}}^{*},\mathcal{D}_{\bm{y}}^{*}\). Specifically, \(\forall k\neq k^{\prime}\in[K_{1}],k_{2}\in[K_{2}]\), we let_

\[M_{2k-1}^{*} =\bm{\mu}_{k}^{+*}=\bm{a}_{k}^{*}+\bm{b}_{k}^{*},\quad M_{2k}^{*} =\bm{\mu}_{k}^{-*}=\bm{a}_{k}^{*}-\bm{b}_{k}^{*},\] \[Q_{2k-1}^{*} =\bm{q}_{k}^{+*}=\bm{c}_{k}^{*}+\bm{d}_{k}^{*},\quad Q_{2k}^{*}= \bm{q}_{k}^{-*}=\bm{c}_{k}^{*}-\bm{d}_{k}^{*},\] \[M_{2k+2K_{1}}^{*} =\bm{\nu}_{k_{2}}^{*},\quad Q_{2k+2K_{1}}^{*}=\bm{0},\]

_where_

\[\bm{a}_{k}^{*}\in\text{conic}(\{\frac{\bm{\mu}_{k}^{+}+\bm{\mu}_{k}^{-}}{2}\}_ {k=1}^{K_{1}}),\quad\bm{b}_{k}^{*}\in\text{conic}(\{\frac{\bm{\mu}_{k}^{+}- \bm{\mu}_{k}^{-}}{2}\}_{k=1}^{K_{1}}),\]

\[\bm{c}_{k}^{*}\in\text{conic}(\{\frac{\bm{q}_{k}^{+}+\bm{q}_{k}^{-}}{2}\}_{k= 1}^{K_{1}}),\quad\bm{d}_{k}^{*}\in\text{conic}(\{\frac{\bm{q}_{k}^{+}-\bm{q}_{ k}^{-}}{2}\}_{k=1}^{K_{1}}),\]

\[\bm{\nu}_{k_{2}}^{*}\in(\text{span}(\bm{\mu}_{1}^{+},\bm{\mu}_{1}^{-},\bm{\mu}_{ 2}^{+},\bm{\mu}_{2}^{-},\cdots,\bm{\mu}_{K_{1}}^{+},\bm{\mu}_{K_{1}}^{-}))^{ \perp},\]

_satisfying_

\[\|\bm{b}_{k}^{*}\|\geq\|\bm{a}_{k}^{*}\|=\Theta(\|\mathbf{u}\|),\quad\|\bm{d}_{k }^{*}\|\geq\|\bm{c}_{k}^{*}\|=\Theta(\|\mathbf{q}\|),\quad\bm{\nu}_{k_{2}}^{*}= \Theta(\|\mathbf{u}\|),\]

_and \(\{\bm{a}_{k}^{*},\bm{b}_{k}^{*}\}_{k=1}^{K_{1}},\{\bm{c}_{k}^{*},\bm{d}_{k}^{*} \}_{k=1}^{K_{1}}\) are two collections of pair wise orthogonal vectors. Then we can have a corresponding new prompt distribution \(\mathcal{D}_{S}^{*}=\sum_{k=1}^{K_{1}}\left(\bm{\tau}_{k}^{+*}\mathcal{P}_{k,L^{ *}+1}^{+}\bm{\tau}_{k}^{-*}\mathcal{P}_{k,L^{*}+1}^{-}\right)\). Again, the model enjoys Bayes-Optimal test error \(L_{\mathcal{D}^{*}}^{0-1}(\Psi^{(T^{*})})\leq\varepsilon\)._This lemma suggest that transformer-mlp structure empower ICL ability in solving task involving semantics ("knowledge") originally from other co-concept prompt's training distribution. This cross-concept semantic "understanding" ability ensure the transformer perform an specific OOD ability.

For example, when we show a prompt "Isaac Newton:Today I designed a machine to capture sunlight; Thomas Edison:" to GPT o1, we would obtain an answer "Today I invented a lamp that shines without fire." During training, even when the concept "Inventors and Their Inventions" may not co-appear with the concept "Fabricate a story" with high chance, the transformers empower the ICL to perform this interesting Out-of-Distribution task. We believe this can serve as an attempt to explain the innovation power of LLM [30, 95, 96] grounded in the linear geometric property of LLM representation, since most of the innovative outcomes of human being generates from cross-concept "Knowledge Intersection", and as it is not an easy task for human specialist to master cross-domain knowledge, we claim that LLM can help innovation by leveraging cross-domain knowledge when deduction over unseen structured task. Similarly, for multi-model scenarios, [86] have shown that compositing different concepts did enable OOD generalization (e.g. "blue square apples" in the Figure 1a in [86]).

This lemma seeks to elementarily explain why LLMs' ICL can excel in complex tasks when using evolutionary strategies, especially when the LLM's latent representation based on language only partially captures the relevant features. Such tasks include algorithm design [97, 4], heuristics [3], acquisition functions [98], and solutions to combinatorial optimization problems [99]. Although the resulting solutions may often seem counterintuitive to human experts, a possible explanation is that transformers can perform ICL in OOD scenarios by leveraging weighted combinations of their updated "understanding" (i.e., changing the identified underlying concepts in the evolution process) of new demo-query pairs, such as randomly sampled TSP instances. These understandings are rooted in the latent structures of the problem instances and can be effectively updated by evolutionary strategies that selectively refine and discard certain outcomes.

Proof.: Proof of Proposition 1. By Proposition 2, we only need to check the expected 0-1 loss \(L_{\mathcal{D}^{*}}^{0-1}(\mathbb{E}[\Psi^{\prime}])=0\). Denote \(\mathbb{E}[\mathcal{A}_{My_{S_{n}}}]\subseteq[2K_{1}]\) as the expected index set denoting the expected shared concept-specific features by the query and can demonstration. By definition in the Lemma, as the semantic combination is conic combination, we see that \(\mathbb{E}[\mathcal{A}_{My_{S_{n}}}]\) will be either a collection of odd (corresponding to positive label) or even (corresponding to negative label) numbers, and all of the combination of the features and labels in one prompt are corresponding to the same real value label without "self-conflict". By Lemma 38, we see that the coefficients are all at a substantial scale at \(T^{*}\). Then by the condition on \(\bm{z}\) and Eq. (31), we can readily check that even when the probability of the fraction of demonstrations sharing the co-concept label semantic with query is feeble (but at least one), utilizing the same set of notations, we still have

\[\begin{split}&\mathbb{E}_{n\in\mathcal{D}^{*}}[\sum_{l\in \mathbb{E}[S_{n,k}^{S_{N}}]}^{s_{N}^{S_{N}}}]\\ &\geq\Theta(\frac{L^{*}/2e^{\sum_{\begin{subarray}{c}k\in\mathbb{ E}[\mathcal{A}_{My_{S_{n}}}]\end{subarray}}\frac{\beta_{Q,k}^{(T^{*})}\beta_{K,k}^{(T^{*})} }{\|\bm{k}_{k}\|^{2}}}}{L^{*}/2(e^{\sum_{\begin{subarray}{c}k\in\mathbb{E}[ \mathcal{A}_{My_{S_{n}}}]\end{subarray}}\frac{\beta_{Q,k}^{(T^{*})}\beta_{K, k}^{(T^{*})}}{\|\bm{k}_{k}\|^{2}}}+e^{(K-1)\sigma_{0}^{2}\|\bm{u}\|^{2}-\sum_{ \begin{subarray}{c}k\in\mathbb{E}(\mathcal{A}_{My_{S_{n}}}]\end{subarray}} \frac{\beta_{Q,k}^{(T^{*})}\beta_{K,k}^{(T^{*})}}{\|\bm{k}_{k}\|^{2}}})\\ &\geq\Theta(\frac{\frac{\|\bm{u}\|^{2}}{\lambda K_{1}}\log(\frac{\| \bm{u}\|^{2}}{\mu\lambda K_{1}})}{\frac{\|\bm{u}\|^{2}}{\lambda K_{1}}\log( \frac{\|\bm{u}\|^{2}}{\mu\lambda K_{1}})+e^{(K-1)\sigma_{0}^{2}\|\bm{u}\|^{2}} })\\ &\gg 1/2,\end{split}\] (65)

where the equality and inequality is by worse-case consideration over \(\mathcal{D}^{*}_{*}\), a small \(\sigma_{0}\) and \(\lambda\) in Condition 1 with a sufficiently large \(C\), as well as the requirement \(\|\bm{b}^{*}_{k}\|\geq\|\bm{a}^{*}_{k}\|=\Theta(\|\bm{u}\|)\). Besides, by \(\|\bm{d}^{*}_{k}\|\geq\|\bm{c}^{*}_{k}\|=\Theta(\|\bm{q}\|)\), Eq.(65), Lemma 4, Eq.(5) and Lemma 2, we have that

\[\mathbb{E}_{n\in\mathcal{D}^{*}}[\sum_{i\in\mathcal{W}_{n,k}^{S_{N}}}\bm{r}_{ i}(\alpha_{O_{(i,j)},k}^{(T^{*})}+y_{S_{n}}(2\sum_{l\in\mathbb{E}[S_{n,k}^{S_{N}}] }\left(\sigma_{S}^{(T^{*})}\right)_{l}^{n}-1)\beta_{O_{(i,j)},k}^{(T^{*})})] \geq\Theta(\kappa),\]

Collaborating with Lemma 43, the poor is completed.

## NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The contributions and scope of this paper are well summarized in the abstract and introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We explicitly discuss the limitation in Appendix A. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper.

* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The detailed assumptions and proofs for all theorems and lemmas are given in the corresponding positions. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Our algorithm is straightforward and easy to implement, and every detail is given in Section 3. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general, releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.

* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We have provided the complete configuration in Section 3 and 6. We have uploaded the code with instructions in the supplementary material. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All the experimental settings can be found in Section 3 and 6. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: This paper executes algorithms 10 times and reports the average results to reduce randomness.

Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We have provided sufficient information about computer resources in Appendix B Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research does not involve any human subjects, personal data, or interactions that would raise ethical concerns about consent, privacy, or respect for persons. In conclusion, the research aligns with the ethical principles outlined in the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We have discussed the broader impacts in Section A. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

* Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [No] Justification: This paper poses no such risks. Guidelines:
* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
* **Licenses for existing assets*
* Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [No] Justification: This paper does not use existing assets. Guidelines:
* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?Answer: [No] Justification: This paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [No] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [No] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.