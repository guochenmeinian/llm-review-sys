# Alignment at Pre-training!

Towards Native Alignment for Arabic LLMs

 Juhao Liang\({}^{1,2}\)

Zhenyang Cai\({}^{1,2}\)

Jianqing Zhu\({}^{3}\)

Huang Huang\({}^{1}\)

Shenzhen Research Institute of Big Data, Shenzhen, China

Kewei Zong\({}^{4}\)

Bang An\({}^{3}\)

Abdulmohsen Alharthi\({}^{3}\)

Juncai He\({}^{3}\)

Lian Zhang\({}^{4}\)

Haizhou Li\({}^{1,2}\)

Bengyou Wang\({}^{*,1,2}\)

Jinchao Xu\({}^{3}\)

\({}^{1}\)Shenzhen Research Institute of Big Data, Shenzhen, China

\({}^{2}\)The Chinese University of Hong Kong, Shenzhen, China

\({}^{3}\)King Abdullah University of Science and Technology, Thuwal, Saudi Arabia

\({}^{4}\)Shenzhen International Center for Industrial and Applied Mathematics, Shenzhen Research Institute of Big Data

\({}^{*}\)wangbenyou@cuhk.edu.cn

###### Abstract

The alignment of large language models (LLMs) is critical for developing effective and safe language models. Traditional approaches focus on aligning models during the instruction tuning or reinforcement learning stages, referred to in this paper as _'post alignment'_. We argue that alignment during the pre-training phase, which we term _'native alignment'_, warrants investigation. Native alignment aims to prevent unaligned content from the beginning, rather than relying on post-hoc processing. This approach leverages extensively aligned pre-training data to enhance the effectiveness and usability of pre-trained models. Our study specifically explores the application of native alignment in the context of Arabic LLMs. We conduct comprehensive experiments and ablation studies to evaluate the impact of native alignment on model performance and alignment stability. Additionally, we release open-source Arabic LLMs that demonstrate state-of-the-art performance on various benchmarks, providing significant benefits to the Arabic LLM community. 1

Footnote 1: These authors contributed equally to this work.

2
Footnote 2: Human preference alignment aims to ensure AI outputs reflect human values and preferences [2, 5, 11]. This is typically evaluated by crowd workers who compare model outputs and indicate their preferences based on three key aspects: accuracy, fluency, and safety. _Accuracy_ refers to the relevance and usefulness of the answer, _‘_’_’_.

## 1 Introduction

The alignment of large language models (LLMs) with human preferences is a crucial component in the development of effective and safe language models for downstream tasks [1, 2, 3]. While most existing studies focus on alignment during the instruction tuning phase [4, 5, 6] or the reinforcement learning stage [2, 3, 7], they often overlook the pre-training stage. Unlike the common practice of aligning LLMs during instruction tuning or reinforcement learning phase, referred to as _'post alignment'_, in this paper, we delve into the relatively unexplored research area of model alignment during the pre-training stage. We term this concept _'native alignment'_, with the goal of enhancing the effectiveness and usability of LLMs during pre-training, a phase that utilizes a significant amount of data for next-token prediction training [8, 9, 10].

_Post alignment_, the conventional approach to human preference alignment 3 typically conducted after the model's pre-training stage, is widely used in LLM development. Its effectiveness has been verifiedby many previous studies [5; 12]. However, the alignment process presents two main challenges: (1) the difficulty of collecting high-quality data, and (2) a lack of stability [3; 13; 14]. The superficial alignment hypothesis suggests that a model's knowledge and capabilities are learned almost entirely during pre-training, while alignment teaches it which sub-distribution of formats should be used when interacting with users [5]. Based on this hypothesis, we posit that native alignment (deep alignment), conducted during the pre-training stage and due to its extensive quantity, can alleviate the stress of post-alignment (superficial alignment) and improve the degree of alignment in LLMs.

Footnote 1: https://github.com/hugging-and-play/

In this study, we introduce a novel data-centric alignment method for the pre-training phase of LLMs, which we term as _native alignment_. Our focus is primarily on the Arabic language and culture, and we carry out extensive experiments and evaluations from various perspectives to demonstrate the effectiveness of our proposed method. We also conduct ablation studies to delve deeper into the complexities of pre-training alignment, thereby offering valuable insights for future research in this field. Furthermore, we make available two pre-trained Arabic LLMs that deliver state-of-the-art performance on benchmarks, reinforcing the efficacy of our pre-training alignment strategy. The key contributions of our work are as follows:

1. The introduction of '_native alignment_', a unique approach to model alignment during the pre-training phase of LLMs, provides a new alignment idea in LLMs other than traditional 'post-alignment' methods.
2. A practical application is performed in Arabic, followed by a multifaceted ablation study to verify the effectiveness of the native-alignment strategy and provide insights into the effectiveness of alignment in pre-training.
3. We release the state-of-the-art open-sourced Arabic LLM (i.e., LLaMA3-Tamed-70B). Additionally, the smaller version, LLaMA3-Tamed-8B, could be beneficial to democratizing LLMs in the Arabic world.

## 2 Methodology: Native Alignment at Pre-training

In this section, we introduce the data alignment processing workflow for native alignment. Following this, we present two pilot studies to demonstrate the improvements in data quality.

### Overview of Data Processing Workflow

Figure 2 illustrates the data processing workflow for native alignment. The process can be divided into the following four steps:

**Step 1: Deduplication** We perform data deduplication on web-crawled data, a common and effective method to enhance the density of knowledge within the dataset [15].

Figure 1: Comparison of pre-training data quality before and after data alignment rewriting.

**Step 2: Annotation** We employ a data rewriting technique to align the pre-training data. In this stage, given a set of code of conduct (_i.e., polishing instructions_) that outlines the expected behavior of LLMs, we randomly select a subset of pre-training data for an alignment expert to rewrite in accordance with these instructions3.

Footnote 3: The alignment expert could be either a human or an expert LLM.

**Step 3: Training** Considering the large volume of data involved in the pre-training stage of LLMs, it is both inefficient and costly to utilize senior experts for such extensive data processing. Instead, we train a group of smaller LLMs on the annotated alignment data pairs.

**Step 4. Rewriting:** With the trained alignment workers, we can process the vast pre-training data effectively. Ultimately, this process can yield a large quantity of rewritten alignment data.

As shown in Figure 1, for the alignment code of conduct part, we prefer to focus more on the four common issues identified in the actual data. These issues are further detailed in the 'Polishing Instructions', located on the left side of Figure 2:

1. **Format Issues:** A common problem with web-crawled data is its format. Text formatting can easily be disrupted by code or web indentation. Therefore, this rule involves correcting any punctuation and formatting errors, as well as any grammatical or syntactic mistakes.
2. **Values Issues:** Arguments and conflicts are common on the Internet, and avoiding controversial issues may be a safe strategy for LLMs. To this end, maintaining fair values is necessary.
3. **Content Moderation:** Hate and violent content should be prohibited for LLMs, mitigating risks such as non-compliance, religious taboos, ethical issues, and user safety concerns.
4. **Knowledge Preservation:** The diversity and quantity of pre-training data are key for training a competitive LLM. Hence, preserving as much knowledge as possible within the dataset is the primary and crucial responsibility of the data processing procedure.

### Preliminary Analysis on Alignment Data

In this section, we conduct two pilot studies on alignment data alone, without the use of LLMs, to preliminarily verify whether the data processing workflow meets the expectations. Specifically, we randomly select and process 8k Arabic data points from a publicly available dataset 4 to compose the test dataset used for our pilot studies. The first pilot study focuses on toxicity detection, while the second one delves into perplexity analysis.

Footnote 4: ArabicText 2022: https://data.baai.ac.cn/details/ArabicText-2022

\begin{table}
\begin{tabular}{l|c c c c} \hline \hline  & \multicolumn{4}{c}{OpenAI Moderation} \\ Arabic Data (8k) & Harassment & Hate & Sexual & Violence \\ \hline Before Alignment & 0.0293 & 0.0067 & 0.0022 & 0.0127 \\ After Alignment & 0.0232 & 0.0049 & 0.0015 & 0.0106 \\ Improvement & -20.82\% \(\downarrow\) & -26.87\% \(\downarrow\) & -31.82\% \(\downarrow\) & -16.54\% \(\downarrow\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Toxicity before and after native alignment of Arabic data: smaller scores are better.

Figure 2: Demonstration of pre-training data processing workflow for native alignment.

Toxicity DetectionReferring to the work of Gehman et al. [16; 17], it is suggested that the presence of offensive and toxic content in pre-training datasets can result in a phenomenon known as toxic degeneration. This means that pre-trained LLMs can generate toxic text even from seemingly harmless prompts. In response to this, we utilize a publicly available moderation tool, OpenAI Modernation, developed by OpenAI 5 to assess the safety of pre-training data before and after the process of alignment rewriting. As demonstrated in Table 1, we observe that across the selected four aspects listed, the rewritten data consistently exhibits less score of toxicity compared to the original data on average. Specifically, there is a reduction of 31.82% in sexual content, 26.87% in hate speech, 20.82% in harassment, and 16.54% in violent content. These findings indicate that our proposed pre-training alignment data processing workflow effectively mitigates the toxicity levels in the datasets across the aforementioned aspects.

Footnote 5: https://platform.openai.com/docs/guides/moderation

Perplexity AnalysisPre-training data pruning [18] demonstrated that simple data pruning using perplexity metrics surpasses other more computationally demanding scoring methods. This approach can curate high-quality corpora and enhance model training performance with less data. In accordance with the paper, we calculate the perplexity metric as follows to evaluate the quality of the alignment data:

\[\text{PPL}\left(z_{i}\right)=\exp\left(\frac{1}{\left|z_{i}\right|}\sum_{t_{j }\in z_{i}}\text{NLL}\left(t_{j}\right)\right)\]

Here, \(\text{NLL}(t_{j})\) represents the negative log likelihood of token \(t_{j}\) in sequence \(z_{i}\):

\[\text{NLL}\left(t_{j}\right)=-\log P\left(t_{j}\mid t_{<j};\theta\right)\]

A lower perplexity indicates that a sentence is more likely according to the models. We calculate perplexity on the previously mentioned 8k curated test dataset for Llama-3-8B [19], both before and after rewriting. As Figure 3 shows, the rewritten data generally has a lower perplexity score compared to the original data. This further demonstrates the effectiveness of the alignment rewriting process in improving data fluency.

## 3 Experiments: Practical Applications in Arabic

To further validate the effectiveness of native alignment, we focus on Arabic, a language that poses significant challenges due to its unique cultural values [20], which differ from mainstream Eastern and Western norms. Besides, our approach is particularly suitable for low-resource languages. For languages with ample resources, discarding unaligned data is often more practical than adapting it, given the high costs of transformation. In Arabic, with its limited data, it's essential to preserve and utilize what is available, even if it is unaligned.

### Experiment Settings

Utilizing the Llama-3 [19] series of model checkpoints, we apply native alignment subsequent to the conventional pre-training stage and build up two aligned Arabic pre-trained models, namely LLaMA3-Tamed-8B and LLaMA3-Tamed-70B. Evaluations carried out on various mainstream Arabic benchmarks demonstrate the superior performance of our constructed models, surpassing state-of-the-art models in multiple aspects.

BenchmarksTo thoroughly evaluate the trained model from various angles, as listed on the right of Figure 4, we select the following Arabic benchmarks: (1) Knowledge assessment: We choose ArabicMMLU [21], and EXAMS [22], which provide a comprehensive evaluation of knowledge across various subjects. These benchmarks focus on factual correctness and subject-specific knowledge,

Figure 3: Perplexity before and after native alignment of Arabic data.

ensuring that the model demonstrates breadth and depth in its understanding of different domains. (2) Arabic localization: We use ACVA [2], a benchmark specifically designed to assess how well the model aligns with Arabic culture, values, and societal norms. This evaluates the model's capacity to generate culturally appropriate and contextually relevant content, which is crucial for models deployed in localized environments. (3) Trustworthiness: Trustworthiness is inherently a qualitative measure, but AraTrust [23] quantifies this by evaluating various dimensions such as truthfulness, ethical behavior, safety, and fairness. AraTrust includes detailed assessments related to physical and mental health, privacy, and avoidance of offensive or illegal content, providing a structured framework for evaluating trust in language models.

BaselinesWe have selected several high-performing models as baselines for comparison. To ensure a fair comparison, we have divided these models into three groups. The first group comprises open-source models with fewer than 10 billion parameters, including Llama3-8B [19], Qwen1.5-7B [24]. The second group consists of open-source models with more than 10 billion parameters, including Jais-30B-v1 [25], Qwen1.5-32B [24], Qwen1.5-72B [24] and Llama3-70B [19]. The final group includes closed-source LLMs such as ChatGPT 3.5 Turbo and GPT-4 6.

Footnote 6: https://openai.com/

Footnote 7: https://data.baai.ac.cn/details/ArabicText-2022

Data CompositionThe data used for continued pre-training has two types:

* **Pre-training data:** To mimic real-world model training scenarios, we combine pre-training datasets from multiple sources, shown on the left of Figure 4. For language datasets, we select ArabicText2022 from BAAI7 for Arabic, SlimPajama [26] for English, MAP-CC [27] for Chinese, and various other language datasets from Wikipedia [28]. For mathematics and code, we choose Proof-Pile-2 [29]. Footnote 8: Opencompass does not support PPL evaluations for the OpenAI models, therefore the scores for ChatGPT 3.5 Turbo and GPT 4 in AraTrust are not available.
* **Native-alignment data:** We adhere to the data processing workflow outlined in Section 2 and rewrite 10 billion tokens data randomly sampled from ArabicText2022, creating an Arabic native-alignment dataset. Specifically, we utilized GPT-4 as an alignment expert to generate 10k expert alignment data for alignment worker training, in this case, we employed Qwen1.5-4B-Chat [24], taking into account both speed and quality.

Training and Evaluation Details(1) **Training Details:** We performed continued pre-training on Llama-3-8B and Llama-3-70B respectively, using the mixed-source pre-trained datasets comprising a total of 100 billion tokens. Following the traditional pre-training phase, we carry out native-alignment training with the 10 billion tokens from the processed Arabic alignment dataset. (2) **Evaluation Details:** For ArabicMMLU [21], we use the code from the original paper. For the remaining benchmarks, we adhere to the original paper [30] and carried out evaluations on the evaluation framework [2]. And, we use Opencompass [31] framework to evaluate LLMs on the AraTrust Benchmark 8.

Figure 4: The left side illustrates the datasets utilized during the pre-training phase of the model, while the right side represents the benchmarks employed in the experiments.

### Results and Analysis

As depicted in Table 2, the LLaMA3-Tamed-8B and LLaMA3-Tamed-70B models, which are trained on a combination of mixed-source pre-training data and a set of native-alignment Arabic data, exhibit superior performance in comparison to the baseline models. In terms of knowledge benchmarks such as ArabicMMLU, and EXAMS, LLaMA3-Tamed-70B surpasses the baselines, with the exception of GPT4. For the Trustworthiness evaluation, namely AraTrust, the enhancements in LLaMA3-Tamed show significant improvement, increasing from 60.54 in Llama3-70B to 63.41 after training. The models trained with native alignment outperform other open-source LLMs, achieving state-of-the-art performance across several benchmarks, including knowledge, Arabic localization and trustworthiness 9.

Footnote 9: An additional experiment in Appendix E shows that native alignment demonstrates strong generalisability to other languages beyond Arabic.

## 4 More Studies on Native Alignment

To further investigate native alignment, we introduce the general experimental settings for alignment in Section 4.1, where we systematically compare the alignment among mainstream Arabic LLMs. Building on these settings, we conduct two studies to explore how to effectively utilize collected native-alignment data in terms of _strategy_ and _scaling law_. This forms two Research Questions (RQs):

* RQ 1: _How should native alignment be utilized on top of pre-training?_
* RQ 2: _What quantity of native-alignment data is required for effective training?_

These two RQs are addressed in Sections 4.2 and 4.3 respectively.

### Benchmarking Harmlessness and Helpfulness

Benchmark (BeaverTails)The BeaverTails dataset [32] comprises 700 prompts specifically designed to provoke offensive responses from models, thereby assessing their alignment performance. After the comparative models generate responses to the prompts, GPT-4 will be used to evaluate these generated contents, assessing the harmlessness and helpfulness of the models. Detailed calculation methods and evaluation prompts are provided in Appendix B. Besides that, due to the issue of Position Bias [33] in GPT-4, the answers of the LLMs are arranged in various orders, and the average scores obtained from these arrangements are recorded as the final results.

Training DetailsSince the evaluation dataset consists of question-answer pairs, the model under evaluation needs to undergo the supervised fine-tuning process to acquire conversational capabilities. Therefore, to obtain more reliable experimental results, the candidate pre-trained models are trained

\begin{table}
\begin{tabular}{l|c c c c c c} \hline \hline \multirow{2}{*}{**Models**} & ArabicMMLU & \multirow{2}{*}{EXAMS} & ACVA & ACVA & \multirow{2}{*}{AraTrust} & \multirow{2}{*}{**Avg.**} \\  & (koto et al.) & & clean & all & & \\ \hline Qwen1.5-7B & 46.41 & 38.34 & 75.17 & 75.88 & 37.16 & 54.59 \\ LLlama3-8B & 45.78 & 46.34 & 77.49 & 76.68 & 54.98 & 60.25 \\ LLaMA3-Tamed-8B & 50.17 & 46.15 & 80.17 & 78.37 & 55.94 & 62.14 \\ \hline Jais-30B-v3 & 44.47 & 45.78 & 83.39 & 79.51 & 52.30 & 61.09 \\ Qwen1.5-32B & 55.94 & 52.01 & 79.99 & 80.07 & 49.23 & 63.45 \\ Qwen1.5-72B & 61.23 & 48.68 & 82.16 & **82.24** & 58.81 & 66.62 \\ LLlama3-70B & 65.51 & 54.78 & 83.70 & 80.25 & 60.54 & 68.96 \\ LLaMA3-Tamed-70B & 66.56 & 55.49 & 82.58 & 81.36 & **63.41** & 69.88 \\ \hline ChatGPT 3.5 Turbo & 57.70 & 45.93 & 74.45 & 76.88 & / & / \\ GPT-4 & **72.50** & **57.76** & **84.06** & 79.43 & / & / \\ \hline \hline \end{tabular}
\end{table}
Table 2: Evaluation of base models in a few-shot setting. The best-performing model overall is highlighted in **bold**, while the top-performing model within each group is underlined.

on an instruction fine-tuning dataset _Alpaca-Arabic-GPT4_10 which contains 50K samples, enabling them to develop normal conversational abilities that align with the evaluation plan. The proportions and volumes of these datasets vary according to the goals of the ablation studies, with specific details provided in the corresponding subsections of the studies.

Footnote 10: https://huggingface.co/datasets/FreedomIntelligence/Alpaca-Arabic-GPT4

**Definition of Training Strategy** For simplicity, the term _Pre-train-12B_ is used to denote the model trained on the original unaligned pre-training dataset with 12 billion tokens. _Align-12B_ refers to the model trained on an aligned pre-training dataset with 12 billion tokens. _SFT-50K_ indicates training on the instruction tuning dataset with 50K samples.

**Baselines for Arabic LLMs** Among the currently popular open-source LLMs, those with strong capabilities in the Arabic language include Jais [25], AceGPT [30], and Llama-3 [19]. In this experiment, base models are directly employed to generate responses on the BeaverTails dataset for evaluating their safety and usefulness. This aims to explore the degree of value alignment in different Arabic pre-trained language models. We employed ChatGPT-4o as the baseline model, assessing the performance of other models by comparing their harmlessness and helpfulness ratios relative to the baseline.

Benchmarking resultsThe experimental results in Figure 5 indicate that Llama-3-8B [19] surpasses other pre-trained models in both harmlessness and helpfulness, suggesting that it is originally trained on a highly secure dataset aligned with human values. Despite the relatively smaller room for improvement in Llama-3, we still opt to use native alignment to further enhance the model's safety and reliability in Arabic. To comprehensively assess the effectiveness of the alignment method, the optimal results achieved through native alignment in the ablation experiment are prominently displayed. These results demonstrate that our method significantly enhances the model's harmlessness and helpfulness, with observed improvements of 10.4% and 4.8% respectively. This enhancement not only makes the model safer but also ensures it is more closely aligned with human values, thus highlighting the substantial impact of our alignment strategy on improving model behavior.

### Native Alignment vs. Conventional Pre-training (RQ 1)

To clarify the effectiveness of native alignment over conventional pre-training, we conduct a simple ablation study on Llama-3-8B [19] to compare the performance of different data composition settings on the same LLM. The first setting uses only the original unaligned pre-training data, as is typical in most pre-training work. The second setting uses the same quantity of data but replaces it with alignment data collected specifically for model training 11. The third setting involves training on alignment data following the training on the original data.

Footnote 11: Empirical experiments show that the data processing workflow can yield approximately 8.6 billion tokens alignment data from an original dataset containing 10 billion tokens.

As shown in the left histogram in Figure 6, using the first setting as a baseline, the other two settings show significant improvements in both harmlessness and helpfulness, indicating the enhancement brought by the alignment data for the base model's safety and knowledge. Furthermore, the setting that trains on alignment data after the original data outperforms training solely on alignment data. This demonstrates that the two different pre-training data settings are not conflicting but rather mutually beneficial.

Figure 5: The ratio of metrics for base models relative to ChatGPT-4o on the BeaverTails dataset.

Based on these simple experiments, we can conclude that: (1) Native alignment indeed brings improvements to the base model pre-training in both harmlessness and helpfulness aspects compared to conventional pre-training data. (2) There is a mutual promotion between alignment data and normal pre-training data. The experimental results show that using both types of data in model pre-training can achieve the best utilization of the collected data.

### Scaling law of Native Alignment (RQ 2)

Compared to instruction tuning and reinforcement learning, the volume of pre-training data is usually quite large. This leads to a pertinent question: _is it necessary to bear substantial costs to realign the entire pre-training corpus?_ Alternatively, does the alignment process hit a plateau once a certain data volume is reached? To explore this topic in-depth, an experiment is conducted by using a model initially trained on an original dataset with 12 billion tokens as the baseline. We then increase the volume of the aligned dataset to obtain multiple models and subsequently fine-tune these models using the instruction tuning dataset. This experiment is designed to explore the scaling laws of aligned datasets, offering insights for future proportions of rewritten datasets.

According to the results shown in the right bar graph of Figure 6, models trained initially without aligned data exhibit increasing levels of harmlessness and helpfulness as the amount of alignment data is augmented. Additionally, the trends observed in the results indicate that the increase in harmlessness is gradual, which may be due to Llama-3 [19] already being a model that excels in aligning with human values, thus showing relatively less significant improvements in harmlessness. On the other hand, helpfulness rises sharply with the increase in the volume of alignment data, and this rate of increase continues to accelerate.

So, based on the result, we can understand that: the alignment dataset plays a crucial role in continuously refining the model's values. By expanding the volume of the alignment dataset, the model becomes safer and more helpful, ultimately enhancing its ability to generate responses that align closely with human values.

## 5 Related Work

### Pre-training Data Processing

Pre-training data processing plays a crucial role in enhancing language model performance and expanding applicability across various tasks. Studies such as Penedo et al. [34] demonstrate the advantages of web-mined data over traditional corpora through advanced processing techniques like deduplication, language identification, and quality filtering, resulting in significant performance gains. Similarly, works by Gunasekar et al. [35] and others [36; 37] highlight that combining LLM-based filtering of web data with synthetic data generation enables smaller models to achieve performance typically seen in larger counterparts, though the computational overhead can limit its broader use.

Figure 6: **The left graph** illustrates the metric improvements under various training strategies. **The right graph** demonstrates the performance gains as the volume of alignment data increases. In both graphs, the baseline model, ‘_Pre-train-12B + SFT-50K_’, is initially trained on 12 billion tokens from an unaligned dataset and later fine-tuned using instruction-tuning datasets with 50,000 samples.

Several studies, including Raffel et al. [9] and Kreutzer et al. [38], emphasize the importance of data quality for transfer learning and multilingual models. Raffel demonstrates that strategic preprocessing can improve performance across tasks, while Kreutzer's manual audit of web-crawled data reveals the critical role of quality control in multilingual model robustness. Additionally, Maini et al. [39] propose the Web Rephrase Augmented Pre-training (WRAP), where an instruction-tuned model paraphrases web documents into different styles, effectively boosting pre-training efficiency, reducing perplexity, and enhancing zero-shot accuracy.

When comparing data cleaning and native alignment, we observe that they serve different but complementary roles in language model development. Data cleaning efforts such as RefinedWeb [34], SlimPajama [26], and WRAP [39] focus on improving data quality by filtering, deduplicating, or reformatting web content into various stylistic formats like 'Wikipedia' or 'question-answer'. These conventional methods primarily remove low-quality content or polish data formats [40; 41; 42]. In contrast, native alignment not only enhances data quality but also aligns the model's outputs with human preferences, making it an extension of traditional cleaning processes. An experiment comparing native alignment with conventional data cleaning (e.g., RefinedWeb) is presented in Appendix E.

Collectively, these studies illustrate evolving data processing strategies that tackle both quality and value alignment, offering opportunities to improve model safety and performance.

### LLM Alignment

Alignment refers to ensuring that LLMs act in accordance with user intentions, meaning they are helpful, honest, and harmless [3]. As shown by Wang et al. [13], aligning LLMs involves three key components: data collection, training methodologies, and model evaluation. This is particularly important because pre-training data can contain unaligned content, such as ethical issues or religious sensitivities, which may conflict with human values. This is especially critical in culturally sensitive regions, such as the Arabic world.

Many alignment methods focus on post-training adjustments, such as instruction tuning [6; 5], and reinforcement learning from human feedback (RLHF) [7; 3]. RLHF involves using human feedback to fine-tune models after pre-training, aligning them with user preferences to ensure they behave appropriately. However, this process is resource-intensive, requiring extensive human input. To address this, RLAIF (Reinforcement Learning from AI Feedback) [43] proposes using LLM-generated feedback instead of human feedback, which has shown promising results [12] in improving scalability.

The difference between post-alignment and native alignment lies in their timing and focus. Post-alignment, like RLHF, occurs after pre-training on both aligned and unaligned data, working to correct undesirable behavior. Native alignment, however, operates during the pre-training phase, filtering out unaligned content from the outset. By proactively preventing the inclusion of problematic data, native alignment is often more efficient and cost-effective. As the saying goes, "An ounce of prevention is worth a pound of cure," indicating that addressing issues early in the process can reduce the complexity and cost of post-training corrections. A comparative experiment between native and post-alignment methods is provided in Appendix D.

## 6 Conclusion

In this paper, we introduced '_native alignment_', a novel approach for aligning LLMs with human preferences during the pre-training phase. Unlike traditional alignment strategies that occur during instruction tuning or reinforcement learning, known as '_post-alignment_', our method integrates alignment processes earlier in the training pipeline. We outlined a comprehensive data processing workflow that emphasizes knowledge preservation, content moderation, text fluency, and controversial issue avoidance. Through extensive experiments and evaluations focusing on the Arabic language, we demonstrated significant improvements in pre-training data quality, resulting in models that are both safer and more helpful. Ablation studies confirmed that combining native alignment data with traditional pre-training data yields superior results, enhancing the harmlessness and helpfulness of models. Moreover, our practical application of this approach led to the development of the state-of-the-art Arabic LLM, LLaMA3-Tamed-70B. Together with the smaller version, LLaMA3-Tamed-8B,this advancement is highly beneficial for the Arabic LLM community. We are committed to furthering research in this area and will open source our code, data and models to foster collaboration and innovation within the community.

## Limitations

Our work has limitations: (1) The absence of a suitable and fair benchmark for evaluating alignment prevents direct comparison with existing post-alignment methods, which is why we did not use other related alignment work as baselines. Our pre-training alignment method, unique in its application stage, does not interfere with other alignment methods, allowing for simultaneous coexistence within the same model. Despite this, we still conduct a simple experiment to compare post-alignment approaches and native alignment in an unfair, non-apple-to-apple setting for the reader's reference, see Appendix D for more details. (2) Our case study focuses on Arabic LLMs, but the full potential of the proposed approach, such as its instruction-following capabilities, remains untested as it is more related to the quality of instruction data rather than pre-training data. (3) Another limitation involves hallucinations. Although the overall hallucination ratio in our model's outputs, where hallucinations are inherited from the original data to the rewritten data, is found to be within acceptable bounds based on a manual review of 90 sample pairs, addressing hallucinations in native alignment remains a challenge and is beyond the scope of this work. We plan to explore solutions for this issue in future work.

## Author Contributions

This work was conducted under the platform of the KAUST-SRIBD Joint Lab on Scientific Computing and Machine Learning. We would like to acknowledge the support of Hetao Shenzhen-Hong Kong Science and Technology Innovation Cooperation Zone Project (No. HZQSWS-KCCYB-2024016), the Shenzhen Science and Technology Program (JCYJ20220818103001002), Shenzhen Doctoral Startup Funding (RCBS2022100809330065), Tianyuan Fund for Mathematics of National Natural Science Foundation of China (NSFC) (12326608), Shenzhen Key Laboratory of Cross-Modal Cognitive Computing (grant number ZDSYS20230626091302006), Shenzhen Stability Science Program 2023, Shenzhen Key Lab of Multi-Modal Cognitive Computing, and KAUST Baseline Research Fund.

## References

* [1] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. _Advances in Neural Information Processing Systems_, 33:3008-3021, 2020.
* [2] Huang Huang, Fei Yu, Jianqing Zhu, Xuening Sun, Hao Cheng, Dingjie Song, Zhihong Chen, Abdulmohsen Alharthi, Bang An, Juncai He, Ziche Liu, Zhiyi Zhang, Junying Chen, Jianquan Li, Benyou Wang, Lian Zhang, Ruoyu Sun, Xiang Wan, Haizhou Li, and Jinchao Xu. Acegpt, localizing large language models in arabic, 2024.

\begin{table}
\begin{tabular}{l l} \hline \hline Content & Technical Contributions \\ \hline Prompt Engineering & **Jianqing Zhu**, Bang An, Kewei Zong and Abdulmohsen Alharthi \\ Data Collection and Cleaning & **Juhao Liang**, Jianqing Zhu, Abdulmohsen Alharthi and Juncai He \\ Pre-training & **Huang Huang**, Juhao Liang and Zhenyang Cai \\ Evaluation & **Juhao Liang**, Zhenyang Cai and Kewei Zong \\ Result Analysis & **Zhenyang Cai**, Juhao Liang, Jianqing Zhu and Juncai He \\ Overall Design & **Benyou Wang**, Lian Zhang, Haizhou Li and Jinchao Xu \\ \hline \hline \end{tabular}
\end{table}
Table 3: Author Contributions.

* [3] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in neural information processing systems_, 35:27730-27744, 2022.
* [4] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. _arXiv preprint arXiv:2109.01652_, 2021.
* [5] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. _Advances in Neural Information Processing Systems_, 36, 2024.
* [6] Run-Ze Fan, Xuefeng Li, Haoyang Zou, Junlong Li, Shwai He, Ethan Chern, Jiewen Hu, and Pengfei Liu. Reformatted alignment. _arXiv preprint arXiv:2402.12219_, 2024.
* [7] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. _arXiv preprint arXiv:2204.05862_, 2022.
* [8] A Vaswani. Attention is all you need. _Advances in Neural Information Processing Systems_, 2017.
* [9] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of machine learning research_, 21(140):1-67, 2020.
* [10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* [11] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpaceaval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 2023.
* [12] Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash. Rlair: Scaling reinforcement learning from human feedback with ai feedback, 2023.
* [13] Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu. Aligning large language models with human: A survey. _arXiv preprint arXiv:2307.12966_, 2023.
* [14] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. _Advances in Neural Information Processing Systems_, 36, 2024.
* [15] Dutch T Meyer and William J Bolosky. A study of practical deduplication. _ACM Transactions on Storage (ToS)_, 7(4):1-20, 2012.
* [16] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. Real-toxicityprompts: Evaluating neural toxic degeneration in language models. _arXiv preprint arXiv:2009.11462_, 2020.
* [17] Devansh Jain, Priyanshu Kumar, Samuel Gehman, Xuhui Zhou, Thomas Hartvigsen, and Maarten Sap. Polyglotoxicityprompts: Multilingual evaluation of neural toxic degeneration in large language models. _arXiv preprint arXiv:2405.09373_, 2024.
* [18] Max Marion, Ahmet Ustun, Luiza Pozzobon, Alex Wang, Marzieh Fadaee, and Sara Hooker. When less is more: Investigating data pruning for pretraining llms at scale. _arXiv preprint arXiv:2309.04564_, 2023.
* [19] AI@Meta. Llama 3 model card. 2024.

* [20] Ali Farghaly and Khaled Shaalan. Arabic natural language processing: Challenges and solutions. _ACM Transactions on Asian Language Information Processing (TALIP)_, 8(4):1-22, 2009.
* [21] "Fajri Koto, Haonan Li, Sara Shatanawi, Jad Doughman, Abdelrahman Boda Sadallah, Aisha Alraeesi, Khalid Almuabarak, Zaid Alyafeai, Neha Sengupta, Shady Shehata, Nizar Habash, Preslav Nakov, and Timothy Baldwin". "arabicmmlu: Assessing massive multitask language understanding in arabic", "2024".
* [22] Momchil Hardalov, Todor Mihaylov, Dimitrina Zlatkova, Yoan Dinkov, Ivan Koychev, and Preslav Nakov. EXAMS: A multi-subject high school examinations dataset for cross-lingual and multilingual question answering. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing_, EMNLP '20, pages 5427-5444, Online, 2020. Association for Computational Linguistics.
* [23] Emad A Alghamdi, Reem I Masoud, Deema Alnuhait, Afnan Y Alomairi, Ahmed Ashraf, and Mohamed Zaytoon. Ardurst: An evaluation of trustworthiness for llms in arabic. _arXiv preprint arXiv:2403.09017_, 2024.
* [24] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. _arXiv preprint arXiv:2309.16609_, 2023.
* [25] Neha Sengupta, Sunil Kumar Sahu, Bokang Jia, Satheesh Katipomu, Haonan Li, Fajri Koto, Osama Mohammed Afzal, Samta Kamboj, Onkar Pandit, Rahul Pal, Lalit Pradhan, Zain Muhammad Mujahid, Massa Baali, Alham Fikri Aji, Zhengzhong Liu, Andy Hock, Andrew Feldman, Jonathan Lee, Andrew Jackson, Preslav Nakov, Timothy Baldwin, and Eric Xing. Jais and jaischat: Arabic-centric foundation and instruction-tuned open generative large language models, 2023.
* [26] Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, and Nolan Dey. SlimPajama: A 627B token cleaned and deduplicated version of RedPajama. https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama, June 2023.
* [27] Xinrun Du, Zhouliang Yu, Songyang Gao, Ding Pan, Yuyang Cheng, Ziyang Ma, Ruibin Yuan, Xingwei Qu, Jiaheng Liu, Tianyu Zheng, Xinchen Luo, Guorui Zhou, Binhang Yuan, Wenhu Chen, Jie Fu, and Ge Zhang. Chinese tiny llm: Pretraining a chinese-centric large language model, 2024.
* [28] Wikimedia Foundation. Wikimedia downloads.
* [29] Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics, 2023.
* [30] Jianqing Zhu, Huang Huang, Zhihang Lin, Juhao Liang, Zhengyang Tang, Khalid Almuabarak, Mosen Alharthi, Bang An, Juncai He, Xiangbo Wu, Fei Yu, Junying Chen, Zhuoheng Ma, Yuhao Du, Yan Hu, He Zhang, Emad A. Alghamdi, Lian Zhang, Ruoyu Sun, Haizhou Li, Benyou Wang, and Jinchao Xu. Second language (arabic) acquisition of llms via progressive vocabulary expansion. 2024.
* [31] OpenCompass Contributors. Opencompass: A universal evaluation platform for foundation models. https://github.com/open-compass/opencompass, 2023.
* [32] Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Chi Zhang, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. Beavertails: Towards improved safety alignment of llm via a human-preference dataset. _arXiv preprint arXiv:2307.04657_, 2023.

* [33] Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. _arXiv preprint arXiv:2305.17926_, 2023.
* [34] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. _arXiv preprint arXiv:2306.01116_, 2023.
* [35] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. _arXiv preprint arXiv:2306.11644_, 2023.
* [36] Yuanzhi Li, Sebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report. _arXiv preprint arXiv:2309.05463_, 2023.
* [37] Mojan Javaheripi, Sebastien Bubeck, Marah Abdin, Jyoti Aneja, Sebastien Bubeck, Caio Cesar Teodoro Mendes, Weizhu Chen, Allie Del Giorno, Ronen Eldan, Sivakanth Gopi, et al. Phi-2: The surprising power of small language models. _Microsoft Research Blog_, 2023.
* [38] Julia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab, Daan van Esch, Nasanbayar Ulizi-Orshikh, Allahsera Tapo, Nishant Subramani, Artem Sokolov, Claytone Sikasote, et al. Quality at a glance: An audit of web-crawled multilingual datasets. _Transactions of the Association for Computational Linguistics_, 10:50-72, 2022.
* [39] Pratyush Maini, Skyler Seto, He Bai, David Grangier, Yizhe Zhang, and Navdeep Jaitly. Rephrasing the web: A recipe for compute and data-efficient language modeling. _arXiv preprint arXiv:2401.16380_, 2024.
* [40] Mohamed Osman Hegazi, Yasser Al-Dossari, Abdullah Al-Yahy, Abdulaziz Al-Sumari, and Anwer Hilal. Preprocessing arabic text on social media. _Heliyon_, 7(2), 2021.
* [41] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. _arXiv preprint arXiv:2101.00027_, 2020.
* [42] Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzman, Armand Joulin, and Edouard Grave. Ccnet: Extracting high quality monolingual datasets from web crawl data. _arXiv preprint arXiv:1911.00359_, 2019.
* [43] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. _arXiv preprint arXiv:2212.08073_, 2022.
* [44] Jiaming Ji, Boyuan Chen, Hantao Lou, Donghai Hong, Borong Zhang, Xuehai Pan, Juntao Dai, and Yaodong Yang. Aligner: Achieving efficient alignment through weak-to-strong correction. _arXiv preprint arXiv:2402.02416_, 2024.
* [45] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In _Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)_, Bangkok, Thailand, 2024. Association for Computational Linguistics.
* [46] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In _International Conference on Learning Representations_, 2020.

Alignment Data Processing Details

The system prompt used to guide the alignment experts in annotating the raw Arabic data is shown in Figure 7. As we can observe, the prompt emphasizes various aspects such as knowledge preservation, data formatting, fairness and bias, religious taboos, ethical issues, text fluency, and more.

## Appendix B Details of Ablation Study Evaluation

Translate into ArabicThe evaluation benchmark, the BeaverTails dataset, is in English. To evaluate Arabic LLMs, we used Baidu translation API12 to translate the questions into Arabic. The translation quality for all data was verified by one of the authors, a native Arabic speaker.

Footnote 12: https://fanyi-api.baidu.com/

Evaluation StrategyFollowing the experimental setup of the Aligner [44], GPT-4 is employed to evaluate the outputs of two models using the metrics of Harmlessness and Helpfulness to ascertain the extent of alignment with human values and whether the alignment compromises the inherent helpfulness of the models. GPT-4 utilizes the prompts from Figures 8 and 9 to assess these metrics and the rates are calculated using the formula:

\[\omega=\frac{N_{w}-N_{l}}{N_{w}+N_{l}+N_{e}}\cdot 100\%\]

where \(\omega\) represents the success rate, while \(N_{w}\), \(N_{e}\), and \(N_{l}\) denote the counts of wins, draws, and losses, respectively, for the correctional answers.

## Appendix C Experiments Compute Resources

In this study, we utilize 2048 GPUs for data processing and model training. The generation of 10B tokens of data takes 48 hours, and the pre-training of the 8B model is completed in one day.

## Appendix D Additional Experiment I: Comparison between Native Alignment and Post-Alignment

To directly demonstrate the performance difference between _Native Alignment_ and _Post-Alignment_, we conduct a simple experiment in this section. The results show that the native alignment approach outperforms the post-alignment method (DPO) in this case. Notably, we are afraid that this is not a fair apples-to-apples comparison for the following reasons:

1. The data used for native alignment and DPO are _not of the same scale_.
2. Native alignment and DPO are complementary methods that operate at different stages rather than being _exclusive_.

### Experiment Settings

We utilized the LLaMA-Factory framework [45], employing LLaMA3-Tamed-8B as the backbone for the _experimental group_ focusing on native alignment, and Llama3-8B as the _control group_. We performed instruction tuning on both pre-trained models using an Arabic supervised fine-tuning (SFT) dataset 13, resulting in the fine-tuned models named _LLaMA3-Tamed-8B (Native Alignment + SFT)_ and _Llama3-8B (SFT)_. For post-alignment, we selected DPO training as a representative approach, using an Arabic preference dataset 14. Post-alignment is conducted on both chat models, namely _LLaMA3-Tamed-8B (Native Alignment + SFT + DPO)_ and _Llama3-8B (Native Alignment + DPO)_. The batch size was set to 128 for both instruction tuning and DPO, with epochs set to 3. All other experimental settings followed the default settings in the framework. We evaluated the performance of the instruction-tuned models and the post-alignment-tuned models on the same Arabic benchmarks shown in the paper, using a zero-shot setting.

Figure 7: The guide for native alignment instructs LLMs to rewrite the pre-training data in accordance with the provided code of conduct.

*System Prompt> You are a specialist in the domain of large language models, and presently, you are engaged in minimizing the toxicity and bias of large language models.
* More harmful: [[responseA]], if responseB is more harmful, then return
- More harmful: [[responseB]], if they are equally harmful, then return
- More harmful: [[Equal]]. Please judiciously determine that the two are equal, and make as many determinations as possible that they are not equal. <Prompt>: prompt <responseA>: responseA <responseB>: responseB

Figure 8: The prompt for evaluating the Harmlessness of two responses.

Figure 9: The prompt for evaluating the Helpfulness of two responses.

### Experiment Settings

We implement the native alignment approach in this experiment, as mentioned earlier. For this, GPT-4 is employed to rewrite 4,300 seed data samples randomly selected from the pre-training corpus, RefinedWeb [34]. This rewritten data is then used to fine-tune a pre-trained model (_Qwen-1.5-4B-Chat_[24]), serving as the rewrite LLM. Subsequently, this LLM is used to rewrite an additional 14,600 pre-training data samples, also randomly sampled from RefinedWeb. Continued pre-training is conducted on _Qwen-1.5-0.5B_ using both the original RefinedWeb data and the aligned data, resulting in models designated as _Qwen-1.5-0.5B-refinedWeb_ and _Qwen-1.5-0.5B-aligned_. Evaluation is performed using the _MMLU_ benchmark [46].

### Experiment Results and Analysis

The results show both continued pre-training methods led to performance improvements on the MMLU benchmark. However, the native alignment procedure resulted in more significant gains compared to data cleaning alone. Analysis of the rewritten data, reveals that the rewritten text enhances the original content by improving readability and conciseness. This suggests that:

1. Native alignment can provide higher quality data than traditional data cleaning;
2. Native alignment demonstrates strong generalisability to other languages beyond Arabic.

## Appendix F Additional Experiment III: Seed Data Selection

To investigate the impact of seed data selection on the performance of the trained alignment model, we conducted an additional experiment. This experiment aimed to explore how the choice of seed alignment data influences model performance. We compared the performance of models trained on randomly selected alignment seed data with those trained on data from specific experimental groups.

1. **Experiment Group 1 (high-ppl):** This group consisted of data with a large decrease in text perplexity scores after rewriting, indicating significant changes in the data.
2. **Experiment Group 2 (low-ppl):** This group consisted of data with minimal differences between the original and rewritten texts, according to text perplexity score, indicating no significant changes.
3. **Baseline (random):** We conducted three random sample seed data experiments to account for randomness, labeled as 'random-1', 'random-2', and 'random-3'. The variance and average of these experiments are reported as 'random (x3)'.

All datasets consisted of 1,000 samples of pre-training data and were trained on Llama-3-8B. GPT-4 is used as a reviewer to evaluate the rewriting quality of the alignment workers trained on different seed data settings, using the prompt shown in Figure 10.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & ArabicMMLU & EXAMS & \begin{tabular}{c} ACVA \\ clean \\ \end{tabular} & 
\begin{tabular}{c} ACVA \\ all \\ \end{tabular} & Avg. \\ \hline Llama3-8B (SFT) & **41.65** & 39.84 & 55.56 & 57.10 & 48.54 \\ Llama3-8B (SFT+DPO) & 39.78 & 38.56 & 60.11 & 61.53 & 50.00 \\ LLaMA3-Tamed-8B (Native alignment + SFT) & 41.13 & **41.73** & 66.64 & **66.96** & **54.12** \\ LLaMA3-Tamed-8B (Native alignment + SFT + DPO) & 39.58 & 39.00 & **68.24** & 66.01 & 53.21 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Comparisons of Arabic benchmarks between native alignment and post-alignment.

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & Qwen-1.5-0.5B & Qwen-1.5-0.5B-refinedWeb & Qwen-1.5-0.5B-aligned \\ \hline Humanities & 27.99 & 29.33 & **33.95** \\ STEM & 12.86 & 25.37 & **27.29** \\ Social Science & 14.35 & 29.91 & **32.71** \\ Other & 20.30 & 27.46 & **30.70** \\ Avg. & 18.32 & 27.71 & **30.73** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Comparisons of MMLU between native alignment and data cleaning.

The results indicate that, in the benchmark, the selection of aligned data can influence performance (high-PPL). All three random experiments showed no significant differences compared to each other on the benchmark. Therefore, a preliminary conclusion can be drawn: **data selection may improve the native alignment approach.** This suggests an interesting direction for future research.

Figure 10: The prompt to evaluate rewriting quality.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & Format & \begin{tabular}{c} Accuracy of \\ Information \\ \end{tabular} & \begin{tabular}{c} Content \\ Moderation \\ \end{tabular} & 
\begin{tabular}{c} Advertisement \\ Removal \\ \end{tabular} & Level of Detail \\ \hline high-ppl & 6.58 & 5.07 & 6.73 & 8.08 & 5.38 \\ low-ppl & 7.51 & 6.82 & 7.62 & 8.65 & 6.83 \\ random (x3) & \(7.27_{\pm 0.08}\) & \(6.27_{\pm 0.08}\) & \(7.47_{\pm 0.10}\) & \(8.57_{\pm 0.09}\) & \(6.55_{\pm 0.13}\) \\ \hline random-1 & 7.30 & 6.39 & 7.52 & 8.64 & 6.56 \\ random-2 & 7.15 & 6.16 & 7.33 & 8.45 & 6.38 \\ random-3 & 7.35 & 6.36 & 7.57 & 8.63 & 6.71 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Comparison data quality assessment results based on different seed data selection strategies

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Abstract and introduction (Section 1) can clearly show the paper's contributions and scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: At the Section 6, we discuss the limitations of this work. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: Not include theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The code used in this paper has been included, and we have clearly indicated the data source in the paper. It is publicly available for download, ensuring that the experimental results can be reproducible. Additionally, the models will be made available to the public after an anonymity period. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The data used in the paper has been clearly cited, and the code is correctly included in the supplemental material. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? [Yes] Justification: In the sections on experiment settings (Sec. 3.1 and Sec. 4.1), the details of the experiments are clearly presented. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Error bars were not reported due to the computational expense involved. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The compute resources utilized for the experiments, as well as the required amount of compute for reproducibility, are presented in Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We adhere to the code of ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This research paper does not pose any risks that could lead to societal impact. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The model that will be released has undergone safety testing. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We adhere to the licensing and terms of the model, code, and data that we have utilized in this work. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The published models in this paper are well documented. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This work does not involve any crowdsourcing or research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: this paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.