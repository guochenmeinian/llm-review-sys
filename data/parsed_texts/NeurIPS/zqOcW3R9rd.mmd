[MISSING_PAGE_FAIL:1]

Gao et al. [13] found that AT can only be effective for mitigating backdoor attacks with certain trigger patterns and may even increase the vulnerability to backdoor attacks when training from scratch on poisoned datasets. Zeng et al. [59] proposed to unlearn the universal adversarial perturbation (UAP) to remove backdoors from poisoned models. Since directly unlearning UAP leads to highly unstable performance for mitigating backdoors, they employ implicit hyper gradient to solve the min-max problem and achieve remarkable performance. However, their method assumes that the same trigger can activate the backdoor regardless of the samples it is planted on, and therefore lacks a guarantee against more advanced attacks that use sample-specific and/or non-additive triggers.

In this paper, we consider the problem of purifying a poisoned model. After investigating the relationship between adversarial examples and poisoned samples, a new upper bound for backdoor risk to the fine-tuned model is proposed. Specifically, by categorizing adversarial examples into three types, we show that the backdoor risk mainly depends on the shared adversarial examples that mislead both the fine-tuned model and the poisoned model to the same class. Shared adversarial examples suggest a novel upper bound for backdoor risk, which combines the shared adversarial risk and vanilla adversarial risk. Besides, the proposed bound can be extended with minor modifications to universal adversarial perturbation and targeted adversarial perturbation. Based on the new bound, we propose a bi-level formulation to fine-tune the poisoned model and mitigate the backdoor. To solve the bi-level problem, we proposed **S**hared **A**dversarial **U**nlearning (SAU). SAU first identifies the adversarial examples shared by the poisoned model and the fine-tuned model. Then, to break the connection between poisoned samples and the target label, the shared adversarial examples are unlearned such that they are either classified correctly by the fine-tuned model or differently by the two models. Moreover, our method can be naturally extended to defend against backdoor attacks with multiple triggers and/or multiple targets. To evaluate our method, we compare it with six state-of-the-art (SOTA) defense methods on seven SOTA backdoor attacks with different model structures and datasets. Experimental results show that our method achieves comparable and even superior performance to all the baselines.

Our contributions are three folds: **1)** We analyze the relationship between adversarial examples and poisoned samples, and derive a novel upper bound for the backdoor risk that can be generalized to various adversarial training-based methods for backdoor defense; **2)** We formulate a bi-level optimization problem for mitigating backdoor attacks in poisoned models based on the derived bound, and propose an efficient method to solve it; **3)** We conduct extensive experiments to evaluate the effectiveness of our method, and compare it with six state-of-the-art defense methods on seven challenging backdoor attacks with different model structures and datasets.

## 2 Related work

Backdoor attack.Backdoor attack is one of the major challenges to the security of DNNs. The poisoned model behaves normally on clean inputs but produces the target output when the trigger pattern is present. Based on the types of triggers, backdoor attacks can be classified into two types: fixed-pattern backdoor attacks and sample-specific backdoor attacks. BadNets [15] is the first backdoor attack that uses fixed corner white blocks as triggers. To improve the stealth of the triggers, Blended [7] is proposed to blend the trigger with the image in a weighted way. Since fixed-pattern triggers can be recognized easily, sample-specific backdoor attacks have been proposed. SSBA [30], WaNet [38], LF [58], IRBA [12], VSSC [49] and TAT [8] use different techniques to inject unique triggers for different samples from different angles. Sleeper-agent [42] and Lira [9] optimize the target output to obtain more subtle triggers. Recently, Zhu et al. [67] proposed a learnable poisoning sample selection strategy to further boost the effect of backdoor attacks. To keep the label of the backdoor image matching the image content, LC [40] and SIG [2] use counterfactual and other methods to modify the image to deploy clean label attacks.

Backdoor defense.Backdoor defense aims to reduce the impact of backdoor attacks on deep neural networks (DNNs) through training and other means. There are three types of backdoor defense: pre-processing, in-processing, and post-processing. Pre-processing backdoor defense aims to identify the poisoned samples in the training dataset. For instance, AC [5] filters out the poisoned samples by the abnormal activation clustering phenomenon in the target class; Confusion Training [39] identifies the poisoned samples by training a poisoned model that only fits the poisoned samples; VDC [68] incorporates multimodal large language models to detect the poisoned samples. In-processingbackdoor defense reduces the effect of the backdoor during training. For instance, ABL [27] exploits the fact that the learning speed of backdoor samples is faster than that of the clean sample, and splits some poisoned samples to eliminate the backdoor by forgetting these poisoned samples; DBD [19] divides the backdoor training process and directly inhibits the backdoor learning process; D-ST [6] splits the backdoor samples and uses semi-supervised learning by observing that the clean samples are more robust to image transformation than the backdoor samples. Post-processing backdoor defense mitigates the effect of backdoors for a poisoned model by pruning or fine-tuning. For example, FP [31] prunes some potential backdoor neurons and fine-tunes the model to eliminate the backdoor effect; ANP [55] finds the backdoor neurons by adversarial perturbation to model weights; EP [64] and CLP [63] distinguish the characteristics of the backdoor neurons from the clean neurons; NAD [28] uses a mild poisoned model to guide the training of the poisoned model to obtain a cleaner model; I-BAU [59] finds possible backdoor triggers by the universal adversarial attack and unlearn these triggers to purify the model; FT-SAM [65] boosts the performance of fine-tuning for backdoor mitigation by incorporating sharpness-aware minimization; NPD [66] learns a lightweight linear transformation layer by solving a well designed bi-level optimization problem to defend against backdoor attack.

Adversarial training.In adversarial training, models are imposed to learn the adversarial examples in the training stage and therefore, resistant to adversarial attacks in the inference stage. In one of the earliest works [14], the adversarial examples are generated using Fast Gradient Sign Method. In [33], PGD-AT is proposed, which generates adversarial examples by running FGSM multiple steps with projection and has become the most widely used baseline for adversarial training. Some further improvements of PGD-AT include initialization improvement [21; 23], attack strategy improvement [22], and efficiency improvement [53; 62].

## 3 Methodology

In Section 3.1, we first introduce notations, threat model, and defense goal to formulate the problem. By investigating the relationship between adversarial risk and backdoor risk, a new upper bound of backdoor risk is derived in Section 3.2, from which a bi-level formulation is proposed in Section 3.3.

### Preliminary

Notations.We consider a \(K\)-class (\(K\geq 2\)) classification problem that aims to predict the label \(y\in\mathcal{Y}\) of a given sample \(\bm{x}\in\mathcal{X}\), where \(\mathcal{Y}=[1,\cdots,K]\) is the set of labels and \(\mathcal{X}\) is the space of samples. Let \(h_{\bm{\theta}}:\mathcal{X}\rightarrow\mathcal{Y}\) be a DNN classifier with model parameter \(\bm{\theta}\). We use \(\mathcal{D}\) to denote the set of data \((\bm{x},y)\). For simplicity, we use \(\bm{x}\in\mathcal{D}\) as a abbreviation for \((\bm{x},y)\in\mathcal{D}\). Then, for a sample \(\bm{x}\in\mathcal{D}\), its predicted label is

\[h_{\bm{\theta}}(\bm{x})=\operatorname*{arg\,max}_{k=1,\cdots,K}\bm{p}_{k}( \bm{x};\bm{\theta}),\] (1)

where \(\bm{p}_{k}(\bm{x};\bm{\theta})\) is the (softmax) probability of \(\bm{x}\) belonging to class \(k\).

Threat model.Let \(\mathcal{V}\) be the set of triggers and define \(g:\mathcal{X}\times\mathcal{V}\rightarrow\mathcal{X}\) as the generating function for poisoned samples. Then, given a trigger \(\Delta\in\mathcal{V}\) and a sample \(\bm{x}\in\mathcal{X}\), one can generate a poisoned sample \(g(\bm{x};\Delta)\). For simplicity, we only consider all to one case, \(i.e.\), there is only one trigger \(\Delta\) and one target label \(\hat{y}\). The all to all case and multi-trigger case are left in **Appendix** A. We assume that the attacker has access to manipulate the dataset and/or control the training process such that the trained model classifies the samples with pre-defined trigger \(\Delta\) to the target labels \(\hat{y}\) while classifying clean samples normally. In addition, we define the poisoning ratio as the proportion of poisoned samples in the training dataset.

Defense goal.We consider a scenario where a defender is given a poisoned model with parameter \(\theta_{bd}\) and a small set of _clean_ data \(\mathcal{D}_{cl}=\{(\bm{x}_{i},y_{i})\}_{i=1}^{N}\). Since we are mainly interested in samples whose labels are not the target label, we further define the set of non-target samples as \(\mathcal{D}_{-\hat{y}}=\{(\bm{x},y)|(\bm{x},y)\in\mathcal{D}_{cl},y\neq\hat{y}\}\). Note that the size of \(\mathcal{D}_{cl}\) is small and the defender cannot train a new model from scratch using only \(\mathcal{D}_{cl}\). The defender's goal is to purify the model so that the clean performance is maintained and the backdoor effect is removed or mitigated. We assume that the defender cannot access the trigger \(\Delta\) or the target label \(\hat{y}\).

Problem formulation.Using the \(0\)-\(1\) loss [50, 60], the classification risk \(\mathcal{R}_{cl}\) and the **backdoor risk**\(\mathcal{R}_{bd}\) with respect to classifier \(h_{\bm{\theta}}\) on \(\mathcal{D}_{cl}\) can be defined as:

\[\mathcal{R}_{cl}(h_{\bm{\theta}})=\frac{1}{N}\sum_{i=1}^{N}\mathbb{I}(h_{\bm{ \theta}}(\bm{x}_{i})\neq y_{i}),\quad\mathcal{R}_{bd}(h_{\bm{\theta}})=\frac{ \sum_{i=1}^{N}\mathbb{I}(h_{\bm{\theta}}(g(\bm{x}_{i},\Delta))=\hat{y},\bm{x}_ {i}\in\mathcal{D}_{-\hat{y}})}{|\mathcal{D}_{-\hat{y}}|}\] (2)

where \(\mathbb{I}\) is the indicator function and \(|\cdot|\) denotes the cardinality of a set.

In (2), the classification risk concerns whether a clean sample is correctly classified, while the backdoor risk measures the risk of classifying a poisoned sample from \(\mathcal{D}_{-\hat{y}}\) to target class \(\hat{y}\).

In this paper, we consider the following problem for purifying a poisoned model:

\[\min_{\bm{\theta}}\mathcal{R}_{cl}(h_{\bm{\theta}})+\lambda\mathcal{R}_{bd}(h _{\bm{\theta}}),\] (3)

where \(\lambda\) is a hyper-parameter to control the tradeoff between classification risk and backdoor risk.

However, directly solving (3) is impractical due to the lack of the trigger \(\Delta\) and target label \(\hat{y}\). Therefore, a natural choice is to replace \(\mathcal{R}_{bd}\) with a surrogate risk irrelevant to \(\Delta\) and \(\hat{y}\).

### Connection between backdoor risk and adversarial risk

To tackle the above problem, we propose a novel upper bound of backdoor risk which can be relaxed to a surrogate of \(\mathcal{R}_{bd}\) that is independent of \(\Delta\) and \(\hat{y}\). Before that, we first decompose the adversarial risk and review the connection between backdoor risk and adversarial risk.

Adversarial risk.Given a sample \(\bm{x}\) and an adversarial perturbation \(\bm{\epsilon}\in\mathcal{S}\), an adversarial example \(\tilde{\bm{x}_{\bm{\epsilon}}}\) can be generated by \(\tilde{\bm{x}_{\bm{\epsilon}}}=\bm{x}+\bm{\epsilon}\), where \(\mathcal{S}\) is the set of perturbations. We define the set of adversarial example-label pair to \(h_{\bm{\theta}}\) generated by perturbation \(\bm{\epsilon}\) and dataset \(\mathcal{D}_{cl}\) by \(\mathcal{A}_{\bm{\epsilon},\bm{\theta}}=\{(\tilde{\bm{x}_{\bm{\epsilon}}},y)| h_{\bm{\theta}}(\tilde{\bm{x}_{\bm{\epsilon}}})\neq y,(\bm{x},y)\in\mathcal{D }_{cl}\}\) and abbreviate \((\tilde{\bm{x}_{\bm{\epsilon}}},y)\in\mathcal{A}_{\bm{\epsilon},\bm{\theta}}\) by \(\tilde{\bm{x}_{\bm{\epsilon}}}\in\mathcal{A}_{\bm{\epsilon},\bm{\theta}}\). Then, the vanilla adversarial risk for \(h_{\bm{\theta}}\) on \(\mathcal{D}_{-\hat{y}}\) can be defined as

\[\mathcal{R}_{adv}(h_{\bm{\theta}})=\frac{\sum_{i=1}^{N}\max_{\bm{\epsilon}_{ i}\in\mathcal{S}}\mathbb{I}(\tilde{\bm{x}_{i}}_{\bm{\epsilon}_{i}}\in\mathcal{A}_{ \bm{\epsilon}_{i},\bm{\theta}},\bm{x}_{i}\in\mathcal{D}_{-\hat{y}})}{| \mathcal{D}_{-\hat{y}}|}.\] (4)

To bridge the adversarial examples and poison samples, we provide the following assumption:

**Assumption 1**.: _Assume that \(g(\bm{x};\Delta)-\bm{x}\in\mathcal{S}\) for \(\forall\bm{x}\in\mathcal{D}_{cl}\)._

Assumption 1 ensures that there exists \(\bm{\epsilon}\in\mathcal{S}\) such that \(\bm{x}+\bm{\epsilon}=g(\bm{x};\Delta)\). Note that **the analysis and the proposed method are independent of any specific types of perturbation**. More discussions about the choice of \(\mathcal{S}\) and its influence on the proposed method are discussed in Appendix B.4.

Then, we reach the first upper bound of backdoor risk:

**Proposition 1**.: _Under Assumption 1, \(\mathcal{R}_{adv}\) serves as an upper bound of \(\mathcal{R}_{bd}\), i.e., \(\mathcal{R}_{bd}\leq\mathcal{R}_{adv}\)._

Remark.Although \(\mathcal{R}_{adv}\) seems to be a promising surrogate for \(\mathcal{R}_{bd}\), it neglects some essential connections between adversarial examples and poisoned samples, resulting in a significant gap between adversarial risk and backdoor risk, as shown in Figure 1. _Such a gap also leads to a fluctuating learning curve for backdoor risk and a significant drop in accuracy in the backdoor mitigation process by adversarial training._ Therefore, we seek to construct a tighter upper bound for \(\mathcal{R}_{bd}\). A key insight is that not all adversarial examples contribute to backdoor mitigation.

To further identify AEs important for mitigating backdoors, we leverage the information from the poisoned model to categorize the adversarial examples to \(h_{\bm{\theta}}\) into three types, as shown in Table 1.

Figure 1: Example of purifying poisoned model using adversarial training on Tiny ImageNet [26]. The curves for Accuracy, Backdoor Risk, and Adversarial Risk are indicated by Green, Orange, and Purple, respectively.

Furthermore, we refer adversarial examples of Type I as the shared adversarial examples between \(h_{\bm{\theta}}\) and \(h_{\bm{\theta}_{bd}}\), as defined below:

**Definition 1** (Shared Adversarial Example).: _Given two classifiers \(h_{\bm{\theta}_{1}}\) and \(h_{\bm{\theta}_{2}}\), an adversarial example \(\bar{\bm{x}}_{\bm{\epsilon}}\) is shared between \(h_{\bm{\theta}_{1}}\) and \(h_{\bm{\theta}_{2}}\) if and only if \(h_{\bm{\theta}_{1}}(\tilde{\bm{x}}_{\bm{\epsilon}})=h_{\bm{\theta}_{2}}(\tilde {\bm{x}}_{\bm{\epsilon}})\neq y\)._

Let \(\mathcal{D}_{s}\) be the subset of samples in \(\mathcal{D}_{-\hat{y}}\) on which planting a trigger can successfully activate the backdoor in \(h_{\bm{\theta}_{bd}}\), as summarized in Table 2. As illustrated in Figure 2, the following proposition reveals a deeper connection between adversarial examples and poisoned samples.

**Proposition 2**.: _For \(\forall(\bm{x},y)\in\mathcal{D}_{s}\), the poisoned sample \(g(\bm{x};\Delta)\) can attack \(h_{\bm{\theta}}\) if and only if \(h_{\bm{\theta}}(g(\bm{x};\Delta))=h_{\bm{\theta}_{bd}}(g(\bm{x};\Delta))\neq y\). Furthermore, under Assumption 1, the poisoned sample \(g(\bm{x};\Delta)\) is a shared adversarial example between \(h_{\bm{\theta}}\) and \(h_{\bm{\theta}_{bd}}\), if it can attack \(h_{\bm{\theta}}\)._

Then, the adversarial risk \(\mathcal{R}_{adv}\) can be decomposed to three components as below:

\[\mathcal{R}_{adv}(h_{\bm{\theta}})=\frac{1}{|\mathcal{D}_{-\hat{y }}|}\sum_{i=1}^{N}\max_{\bm{\epsilon}_{i}\in\mathcal{S}}\Big{\{}\underbrace{ \mathbb{I}(x_{i}\in\mathcal{D}_{s})\mathbb{I}(h_{\bm{\theta}}(\tilde{\bm{x}}_{ i,\bm{\epsilon}_{i}})=h_{\bm{\theta}_{bd}}(\tilde{\bm{x}}_{i,\bm{\epsilon}_{i}}), \tilde{\bm{x}}_{i,\bm{\epsilon}_{i}}\in\mathcal{A}_{\bm{\epsilon}_{i},\bm{ \theta}})}_{\text{Type I adversarial examples on }\mathcal{D}_{s}}\] (5)

Proposition 2 implies that the first component in (5) is essential for backdoor mitigation, as it captures the risk of shared adversarial examples between the poisoned model \(h_{\bm{\theta}_{bd}}\) and the fine-tuned model \(h_{\bm{\theta}}\), and the poisoned samples effective for \(h_{\bm{\theta}}\) belong to SAEs. The second component is irrelevant

\begin{table}
\begin{tabular}{l l l} \hline \hline Type & Description & Definition \\ \hline I & Mislead \(h_{\bm{\theta}}\) and \(h_{\bm{\theta}_{bd}}\) to the same class & \(h_{\bm{\theta}_{bd}}(\tilde{\bm{x}}_{\bm{\epsilon}})=h_{\bm{\theta}_{bd}}( \tilde{\bm{x}}_{\bm{\epsilon}})\neq y\) \\ II & Mislead \(h_{\bm{\theta}}\), but not mislead \(h_{\bm{\theta}_{bd}}\) & \(h_{\bm{\theta}_{bd}}(\tilde{\bm{x}}_{\bm{\epsilon}})\neq h_{\bm{\theta}}( \tilde{\bm{x}}_{\bm{\epsilon}}),h_{\bm{\theta}_{bd}}(\tilde{\bm{x}}_{\bm{ \epsilon}})=y\) \\ III & Mislead \(h_{\bm{\theta}}\) and \(h_{\bm{\theta}_{bd}}\) to different classes & \(h_{\bm{\theta}_{bd}}(\tilde{\bm{x}}_{\bm{\epsilon}})\neq h_{\bm{\theta}}( \tilde{\bm{x}}_{\bm{\epsilon}}),h_{\bm{\theta}_{bd}}(\tilde{\bm{x}}_{\bm{ \epsilon}})\neq y,h_{\bm{\theta}}(\tilde{\bm{x}}_{\bm{\epsilon}})\neq y\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Different types of adversarial examples to \(h_{\bm{\theta}}\).

\begin{table}
\begin{tabular}{l l} \hline \hline Notation & Description/Definition \\ \hline \(h_{\bm{\theta}_{bd}}\) & Poisoned model \\ \(h_{\bm{\theta}}\) & Purified model \\ \(\Delta\) & Trigger \\ \(\hat{y}\) & Target label \\ \(g(\bm{x},\Delta)\) & Poisoned sample generated from \(\bm{x}\) \\ \(\mathcal{S}\) & Set of perturbations \\ \(\bm{\epsilon}\) & Perturbation \\ \(\tilde{\bm{x}}_{\bm{\epsilon}}\) & Adversarial example, \(\bm{x}+\bm{\epsilon}\) \\ \(\mathcal{D}_{cl}\) & \(\{(\bm{x}_{i},y_{i})\}_{N=1}^{N}\) \\ \(\mathcal{D}_{-\hat{y}}\) & \(\{(\bm{x},y)|(|\bm{x},y)\in\mathcal{D}_{cl},y\neq\hat{y}\}\) \\ \(\mathcal{D}_{s}\) & \(\{(\bm{x},y)|h_{\bm{\theta}_{bd}}(g(\bm{x};\Delta))=\hat{y},\bm{x}\in\mathcal{D }_{-\hat{y}}\}\) \\ \(\mathcal{A}_{\bm{\epsilon},\bm{\theta}}\) & \(\{(\tilde{\bm{x}}_{\bm{\epsilon}},y)|h_{\bm{\theta}}(\tilde{\bm{x}}_{\bm{ \epsilon}})\neq y,(\bm{x},y)\in\mathcal{D}_{cl}\}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Table of notations.

Figure 2: A schematic of the relationship between adversarial examples, shared adversarial examples (SAEs, Type I) and poisoned samples. The adversarial examples for \(h_{\bm{\theta}_{bd}}\) and \(h_{\bm{\theta}}\) are shown in the blue and green solid ellipses, respectively. The poisoned samples are in the black dashed circle. Assume that \(h_{\bm{\theta}_{bd}}\) and \(h_{\bm{\theta}}\) have \(100\%\) backdoor attack success rates in the left, such that all poisoned samples are contained in SAEs. Thus, by reducing the shared adversarial examples between \(h_{\bm{\theta}_{bd}}\) and \(h_{\bm{\theta}}\) (from left to right), the backdoor risk can be mitigated in \(h_{\bm{\theta}}\).

for backdoor mitigation, since adversarial examples of Type II and III are either not poisoned samples or ineffective to backdoor attack against \(h_{\bm{\theta}}\). The third component protects the samples that are originally resistant to triggers from being backdoor attacked in the mitigation process.

Thus, by removing the second component in \((\ref{eq:2})\), we propose the following sub-adversarial risk

\[\begin{split}\mathcal{R}_{sub}(h_{\bm{\theta}})=& \frac{1}{|\mathcal{D}_{-\hat{y}}|}\sum_{i=1}^{N}\max_{\bm{\epsilon}_{i}\in \mathcal{S}}\Big{\{}\mathbb{I}(\bm{x}_{i}\in\mathcal{D}_{s})\mathbb{I}(h_{ \bm{\theta}}(\tilde{\bm{x}}_{i,\bm{\epsilon}_{i}})=h_{\bm{\theta}_{bd}}( \tilde{\bm{x}}_{i,\bm{\epsilon}_{i}}),\tilde{\bm{x}}_{i,\bm{\epsilon}_{i}}\in \mathcal{A}_{\bm{\epsilon}_{i},\bm{\theta}})\\ &+\mathbb{I}(\bm{x}_{i}\in\mathcal{D}_{-\hat{y}}\setminus \mathcal{D}_{s},\tilde{\bm{x}}_{i,\bm{\epsilon}_{i}}\in\mathcal{A}_{\bm{ \epsilon}_{i},\bm{\theta}})\Big{\}}.\end{split}\] (6)

Compared to vanilla adversarial risk \(\mathcal{R}_{adv}\), the proposed sub-adversarial risk \(\mathcal{R}_{sub}\) focuses on the shared adversarial examples for samples in \(\mathcal{D}_{s}\) while considering vanilla adversarial examples for samples in \(\mathcal{D}_{-\hat{y}}\setminus\mathcal{D}_{s}\). When \(\mathcal{D}_{s}=\mathcal{D}_{-\hat{y}}\), \(i.e.\), \(h_{\bm{\theta}_{bd}}\) has backdoor risk \(100\%\), the sub-adversarial risk measures the shared adversarial risk on \(\mathcal{D}_{-\hat{y}}\).

Then, we provide the following proposition to establish the relation between backdoor risk and (sub) adversarial risk:

**Proposition 3**.: _Under Assumption 1, for a classifier \(h_{\bm{\theta}}\), the following inequalities hold_

\[\mathcal{R}_{bd}(h_{\bm{\theta}})\leq\mathcal{R}_{sub}(h_{\bm{\theta}})\leq \mathcal{R}_{adv}(h_{\bm{\theta}}).\]

Therefore, \(\mathcal{R}_{sub}\) is a tighter upper bound for \(\mathcal{R}_{bd}\) compared with \(\mathcal{R}_{adv}\). After replacing \(\mathcal{R}_{bd}\) with \(\mathcal{R}_{sub}\) in (3), we reach the following optimization problem:

\[\min_{\bm{\theta}}\mathcal{R}_{cl}(h_{\bm{\theta}})+\lambda\mathcal{R}_{sub}( h_{\bm{\theta}}).\] (7)

Due to the space limit, the proofs of the above propositions, as well as some detailed discussions and extensions will be provided in Appendix A.

### Proposed method

Since the target label \(\hat{y}\) is unavailable and 0-1 loss is non-differentiable, we first discuss the relaxation of \(\mathcal{R}_{sub}\) and then replace 0-1 loss with suitable surrogate loss functions in this section.

Relaxation.One limitation of \(\mathcal{R}_{sub}\) is the inaccessibility of \(\mathcal{D}_{s}\), \(i.e.\), the subset of samples in \(\mathcal{D}_{-\hat{y}}\) on which planting a trigger can successfully activate the backdoor in \(h_{\bm{\theta}_{bd}}\). Since a poisoned model usually has a high attack success rate (ASR), the first relaxation is replacing \(\mathcal{D}_{s}\) with \(\mathcal{D}_{-\hat{y}}\), \(i.e.\),

\[\mathcal{R}_{sub}(h_{\bm{\theta}})\approx\frac{1}{|\mathcal{D}_{-\hat{y}}|} \sum_{i=1}^{N}\max_{\bm{\epsilon}_{i}\in\mathcal{S}}\Big{\{}\mathbb{I}(\bm{x} _{i}\in\mathcal{D}_{-\hat{y}})\mathbb{I}(h_{\bm{\theta}}(\tilde{\bm{x}}_{i, \bm{\epsilon}_{i}})=h_{\bm{\theta}_{bd}}(\tilde{\bm{x}}_{i,\bm{\epsilon}_{i}} ),\tilde{\bm{x}}_{i,\bm{\epsilon}_{i}}\in\mathcal{A}_{\bm{\epsilon}_{i},\bm{ \theta}})\Big{\}}.\]

Since \(\hat{y}\) is unavailable, we also replace \(\mathcal{D}_{-\hat{y}}\) by \(\mathcal{D}_{cl}\) in the sub-adversarial risk. As \(g(\bm{x};\Delta)\) is not malicious for classification if the ground truth label of \(\bm{x}\) is \(\hat{y}\), this relaxation has negligible negative influence for mitigating backdoor risk. After the above two relaxations, we reach the following shared adversarial risk on \(\mathcal{D}_{cl}\):

\[\mathcal{R}_{share}(h_{\bm{\theta}})=\frac{1}{N}\sum_{i=1}^{N}\max_{\bm{ \epsilon}_{i}\in\mathcal{S}}\Big{\{}\mathbb{I}(h_{\bm{\theta}}(\tilde{\bm{x}}_ {i,\bm{\epsilon}_{i}})=h_{\bm{\theta}_{bd}}(\tilde{\bm{x}}_{i,\bm{\epsilon}_{ i}}),\tilde{\bm{x}}_{i,\bm{\epsilon}_{i}}\in\mathcal{A}_{\bm{\epsilon}_{i},\bm{ \theta}})\Big{\}}.\]

Due to the space limit, we postpone the detailed discussion of the above two relaxations (\(e.g.\), the relaxation gaps) in Appendix A.7.

By replacing \(\mathcal{R}_{sub}\) with \(\mathcal{R}_{share}\) in (7), we reach the following problem:

\[\min_{\bm{\theta}}\mathcal{R}_{cl}(h_{\bm{\theta}})+\lambda\mathcal{R}_{share}(h _{\bm{\theta}}).\] (8)

By solving (8), we seek \(h_{\bm{\theta}}\) that either classifies the adversarial examples correctly (\(i.e.\), \(h_{\bm{\theta}}(\tilde{\bm{x}}_{\bm{\epsilon}})=y\)), or differently with \(h_{\bm{\theta}_{bd}}\) (\(i.e.\), \(h_{\bm{\theta}}(\tilde{\bm{x}}_{\bm{\epsilon}})\neq h_{\bm{\theta}_{bd}}(\tilde{ \bm{x}}_{\bm{\epsilon}})\)), while maintaining a high accuracy on \(\mathcal{D}_{cl}\).

Approximation.We firstly need to approximate two indicators in (8), including: 1) \(\mathbb{I}(h_{\bm{\theta}}(\bm{x})\neq y)\); 2) \(\mathbb{I}(\tilde{\bm{x}}_{\bm{\epsilon}}\in\mathcal{A}_{\bm{\epsilon},\bm{ \theta}},h_{\bm{\theta}}(\tilde{\bm{x}}_{\bm{\epsilon}})=h_{\bm{\theta}_{bd}}( \tilde{\bm{x}}_{\bm{\epsilon}}))\). The former indicator is for the clean accuracy, so we use the widely used cross-entropy (CE) loss as the surrogate loss, \(i.e.\), \(L_{cl}(\bm{x},y;\bm{\theta})=\text{CE}(\bm{p}(\bm{x};\bm{\theta}),y)\). In terms of the latter indicator that measures the shared adversarial risk, different surrogate losses could be employed for different usages:

* To generate shared adversarial examples, we first decompose the second indicator as below: \[\mathbb{I}(\tilde{\bm{x}}_{\bm{\epsilon}}\in\mathcal{A}_{\bm{\epsilon},\bm{ \theta}}\cap\mathcal{A}_{\bm{\epsilon},\bm{\theta}_{bd}})\mathbb{I}(h_{\bm{ \theta}}(\tilde{\bm{x}}_{\bm{\epsilon}})=h_{\bm{\theta}_{bd}}(\tilde{\bm{x}}_ {\bm{\epsilon}})),\] (9) which covers two components standing for fooling \(h_{\bm{\theta}_{bd}}\) and \(h_{\bm{\theta}}\) simultaneously, and keeping the same predicted label for \(h_{\bm{\theta}_{bd}}\) and \(h_{\bm{\theta}}\), respectively. For the first component, we use CE on both \(h_{\bm{\theta}}\) and \(h_{\bm{\theta}_{bd}}\) as a surrogate loss, \(i.e.\), \[L_{adv}(\tilde{\bm{x}}_{\bm{\epsilon}},y;\bm{\theta})=\frac{1}{2}\left(\text {CE}(\bm{p}(\tilde{\bm{x}}_{\bm{\epsilon}};\bm{\theta}),y)+\text{CE}(\bm{p}( \tilde{\bm{x}}_{\bm{\epsilon}};\bm{\theta}_{bd}),y)\right).\] (10) For the second component, we measure the distance between \(\bm{p}(\tilde{\bm{x}}_{i,\bm{\epsilon}_{i}};\bm{\theta})\) and \(\bm{p}(\tilde{\bm{x}}_{\bm{\epsilon}};\bm{\theta}_{bd})\) by Jensen-Shannon divergence [11] and adopt the following surrogate loss: \[L_{share}(\tilde{\bm{x}}_{\bm{\epsilon}},y;\bm{\theta})=-\text{JS}(\bm{p}( \tilde{\bm{x}}_{\bm{\epsilon}};\bm{\theta}),\bm{p}(\tilde{\bm{x}}_{\bm{ \epsilon}};\bm{\theta}_{bd})).\] (11)
* To unlearn the shared adversarial examples, we adopt the following surrogate loss \[L_{sar}(\tilde{\bm{x}}_{\bm{\epsilon}},y;\bm{\theta})=-\mathbb{I}\left(\tilde {y}\neq y\right)\log\left(1-\bm{p}_{\tilde{y}}(\tilde{\bm{x}}_{\bm{\epsilon}} ;\bm{\theta})\right),\] (12) where \(\tilde{y}=h_{\bm{\theta}_{bd}}(\tilde{\bm{x}}_{\bm{\epsilon}})\). By reducing \(L_{sar}\), the prediction of \(\tilde{\bm{x}}_{\bm{\epsilon}}\) by \(h_{\bm{\theta}}\), \(i.e.\), \(h_{\bm{\theta}}(\tilde{\bm{x}}_{\bm{\epsilon}})\) is forced to differ from \(h_{\bm{\theta}_{bd}}(\tilde{\bm{x}}_{\bm{\epsilon}})\) if \(h_{\bm{\theta}_{bd}}(\tilde{\bm{x}}_{\bm{\epsilon}})\neq y\), therefore, reducing the shared adversarial risk (SAR).

**Overall objective.** By combining all the above surrogate losses with the linear weighted summation mode, we propose the following bi-level objective:

\[\begin{split}\min_{\bm{\theta}}&\frac{1}{N}\sum_{i=1 }^{N}\left\{\lambda_{1}L_{cl}(\bm{x}_{i},y_{i};\bm{\theta})+\lambda_{2}L_{sar} (\tilde{\bm{x}}_{i,\bm{\epsilon}_{i}^{*}},y_{i};\bm{\theta})\right\}\\ \text{s.t.}&\bm{\epsilon}_{i}^{*}=\operatorname*{arg \,max}_{\bm{\epsilon}_{i}\in\mathcal{S}}\lambda_{3}L_{adv}(\tilde{\bm{x}}_{i,\bm{\epsilon}_{i}},y_{i};\bm{\theta})+\lambda_{4}L_{share}(\tilde{\bm{x}}_{i,\bm{\epsilon}_{i}},y_{i};\bm{\theta}),\quad i=1,\dots,N,\end{split}\] (13)

where \(\lambda_{1},\lambda_{2},\lambda_{3},\lambda_{4}\geq 0\) indicate trade-off weights.

Optimization algorithm.We propose an optimization algorithm called **Shared Adversarial Unlearning** (SAU, Algorithm 1), which solves (13) by alternatively updating \(\bm{\theta}\), and \(\bm{\epsilon}^{*}\). Specifically, the model parameter \(\bm{\theta}\) is updated using stochastic gradient descent [3], and the perturbation \(\bm{\epsilon}^{*}\) is generated by projected gradient descent [33].

``` Input: Training set \(\mathcal{D}_{cl}\), poisoned model \(h_{\bm{\theta}_{bd}}\), PGD step size \(\eta>0\), number of PGD steps \(T_{adv}\), perturbation set \(\mathcal{S}\), max iteration number \(T\).  Initialize \(h_{\bm{\theta}}=h_{\bm{\theta}_{bd}}\). for\(t=0,...,T-1\)do for Each mini-batch in \(\mathcal{D}_{cl}\)do  Initialize perturbation \(\bm{\epsilon}\). for\(t_{adv}=0,...,T_{adv}-1\)do  Compute gradient \(\bm{g}\) of \(\bm{\epsilon}\) w.r.t. the inner maximization objective in (13).  Update \(\bm{\epsilon}=\prod_{\mathcal{S}}(\bm{\epsilon}+\eta\text{ sign}(\bm{g}))\) where \(\prod\) is the projection operation. endfor  Update \(\bm{\theta}\) w.r.t. the outer minimization objective in (13). endfor endfor ```

**Algorithm 1** Shared Adversarial UnlearningExperiment

### Experiment settings

Backdoor attack.We compare SAU with 7 popular state-of-the-art (SOTA) backdoor attacks, including BadNets [15], Blended backdoor attack (Blended) [7], input-aware dynamic backdoor attack (Input-Aware)[37], low frequency attack (LF) [58], sinusoidal signal backdoor attack (SIG) [2], sample-specific backdoor attack (SSBA) [30], and warping-based poisoned networks (WaNet) [38]. To make a fair and trustworthy comparison, we use the implementation and configuration from BackdoorBench [54], a comprehensive benchmark for backdoor evaluation. By default, the poisoning ratio is set to 10% in all attacks, and the \(0^{th}\) label is set to be the target label. We evaluate all the attacks on 3 benchmark datasets, CIFAR-10 [24], Tiny ImageNet [26], and GTSRB [43] using two networks: PreAct-ResNet18 [18] and VGG19 [41]. Due to space constraints, the results for GTSRB and VGG19 are postponed to **Appendix** D. Note that for clean label attack SIG, the 10% poisoning ratio can only be implemented for CIFAR-10. More attack details are left in **Appendix** C.

Backdoor defense.We compare our method with 6 SOTA backdoor defense methods: ANP [55], Fine-pruning (FP) [31], NAD [28], NC [47], EP [64] and i-BAU [59]. By default, all the defense methods can access 5% benign training data. We follow the recommended configurations for SOTA defenses as in BackdoorBench [54]. For our method, we choose to generate the shared adversarial example with Projected Gradient Descent (PGD) [33] with \(L_{\infty}\) norm. For all experiments, we run PGD 5 steps with norm bound \(0.2\) and we set \(\lambda_{1}=\lambda_{2}=\lambda_{4}=1\) and \(\lambda_{3}=0.01\). More details about defense settings and additional experiments can be found in **Appendix** C and D.

Evaluation metric.We use four metrics to evaluate the performance of each defense method: Accuracy on benign data (**ACC**), Attack Success Rate (**ASR**), Robust Accuracy (**R-ACC**) and Defense Effectiveness Rating (**DER**). R-ACC measures the proportion of the poisoned samples classified to their true label, and ASR measures the proportion of poisoned samples misclassified to the target label. Larger R-ACC and lower ASR indicate that the backdoor is effectively mitigated. Note that the samples for the target class are excluded when computing the ASR and R-ACC as done in BackdoorBench. DER \(\in[0,1]\) was proposed in [65] to evaluate the cost of ACC for reducing ASR. It is defined as follows:

\[\text{DER}=[\max(0,\Delta\text{ASR})-\max(0,\Delta\text{ACC})+1]/2,\] (14)

where \(\Delta\text{ASR}\) denotes the drop in ASR after applying defense, and \(\Delta\text{ACC}\) represents the drop in ACC after applying defense.

**Note**: Higher ACC, lower ASR, higher R-ACC and higher DER represent better defense performance. We use **boldface** and underline to indicate the best and the second-best results among all defense methods, respectively, in later experimental results.

### Main results

Effectiveness of SAU.To verify the effectiveness of SAU, we first summarize the experimental results on CIFAR-10 and Tiny ImageNet in Tables 3 and 4, respectively. As shown in Tables 3-4, SAU can mitigate backdoor for almost all attacks with a significantly lower average ASR. For the experiments on CIFAR-10, SAU achieves the top-2 lowest ASR in 4 of 7 attacks and very low ASR for the other 3 attacks. Similarly, SAU performs the lowest ASR in 3 attacks for Tiny ImageNet and negligible ASR in two of the other attacks. Notably, SAU fails to mitigate WaNet in Tiny ImageNet, although it can defend against WaNet on CIFAR-10. As a transformation-based attack that applies image transforms to construct poisoned samples, WaNet can generate triggers with a large \(L_{\infty}\) norm. Specifically, WaNet has average \(L_{\infty}\) trigger norm \(0.348\) on TinyImageNet and \(0.172\) on CIFAR-10. Note that for all experiments, we generate adversarial perturbation with \(L_{\infty}\) norm less than \(0.2\). The large trigger norm of WaNet on Tiny ImageNet poses a challenge for AT-based methods such as i-BAU and the proposed one, which reveals an important weakness of such methods, \(i.e.\), their performance may be degraded if the trigger is beyond the perturbation set. A promising approach for this challenge is to consider adversarial examples for the union of multiple perturbation types [34; 45] and we postpone more discussion to **Appendix** B.

As maintaining clean accuracy is also important for an effective backdoor defense method, we also compare SAU with other baselines in terms of ACC, R-ACC and DER in Table 3 and 4. As SAUadopts adversarial examples to mitigate backdoors, it has negative effects on clean accuracy, resulting in a slightly lower clean accuracy compared to the best one. However, SAU achieves the best average R-ACC, which demonstrates its effectiveness in recovering the prediction of poisoned samples, \(i.e.,\) classifying the poisoned samples correctly. Besides, the best average DER indicates that SAU achieves a significantly better tradeoff between clean accuracy and backdoor risk.

Influence of poisoning ratio.To study the influence of the poisoning ratio on the effectiveness of SAU, we test SAU with varying poisoning ratios from 10% to 50%. As shown in Table 5, SAU is still able to achieve remarkable performance even if half of the data is poisoned.

\begin{table}
\begin{tabular}{c|c|c c c|c c c|c c c|c c c} \hline \hline Defense & \multicolumn{3}{c|}{No Defense} & \multicolumn{3}{c|}{ANP [55]} & \multicolumn{3}{c|}{FP [31]} & \multicolumn{3}{c}{NC [47]} \\ \hline Attack & ACC & ASR & R-ACC & DER & ACC & ASR & R-ACC & DER & ACC & ASR & R-ACC & DER & ACC & ASR & R-ACC & DER \\ \hline BadNets [15] & 56.23 & 100.0 & 0.0 & N/A & 43.45 & **0.00** & 43.13 & 93.61 & 51.73 & 99.99 & 0.01 & 47.76 & 51.52 & 0.10 & 50.82 & 97.59 \\ Blended [7] & 56.03 & 99.71 & 0.22 & N/A & 43.93 & 61.11 & 17.28 & 90.75 & 51.89 & 59.54 & 2.1 & 49.81 & 52.55 & 83.21 & 3.96 & 51.51 \\ Input-aware [37] & 57.45 & 98.58 & 1.06 & N/A & 35.90 & **0.10** & 1.82 & 88.00 & 52.82 & 62.92 & 24.67 & 68.88 & 56.20 & 0.09 & **51.29** & 98.26 \\ LF [58] & 55.97 & 85.97 & 0.97 & N/A & 45.69 & 62.30 & 14.49 & 63.00 & 51.44 & 59.55 & 2.42 & 49.42 & 59.85 & 56.87 & 55.02 \\ SSA [30] & 55.22 & 97.17 & 1.68 & N/A & 43.36 & 56.53 & 17.24 & 64.66 & 50.74 & 88.87 & 6.26 & 52.04 & 52.47 & 52.47 & 32.17 & 70.75 \\ Wawet [38] & 56.78 & 99.49 & 0.36 & N/A & 36.16 & **0.01** & 37.97 & 89.04 & 38.34 & 39.3 & 96.33 & 0.23 & 51.63 & 97.90 \\ Average & 56.28 & 99.06 & 7.22 & N/A & 41.33 & 20.83 & 22.66 & 77.12 & 52.44 & 74.48 & 63.81 & 58.88 & **53.18** & 38.78 & 31.64 & 74.50 \\ \hline \hline Defense & \multicolumn{3}{c|}{NAD [23]} & \multicolumn{3}{c|}{EP [64]} & \multicolumn{3}{c|}{i-BAU [59]} & \multicolumn{3}{c}{SAU (**Ours**)} \\ \hline Attack & ACC & ASR & R-ACC & DER & ACC & ASR & R-ACC & DER & ACC & ASR & R-ACC & DER & ACC & ASR & R-ACC & DER \\ \hline BadNets [15] & 46.37 & 0.27 & 45.61 & 94.93 & 52.30 & 0.03 & **52.05** & **98.02** & **52.67** & 98.31 & 1.59 & 49.06 & 51.52 & 0.53 & 51.15 & 97.35 \\ Blended [7] & 46.89 & 94.99 & 2.63 & 74.79 & 51.86 & 60.63 & 12.31 & 69.75 & **52.73** & 29.66 & 4.15 & 51.72 & 50.30 & **0.06** & **25.27** & **94.96** \\ Input-aware [37] & 47.91 & 1.86 & 43.13 & 93.57 & **51.23** & 20.44 & 27.19 & **92.00** & 56.35 & 21.26 & 48.47 & 67.96 & 54.11 & 0.33 & **42.29** & 97.59 \\ LF [58] & 45.45 & 50.49 & 20.21 & 68.78 & **53.33** & 75.41 & 13.06 & 60.26 & 52.77 & 88.04 & 6.82 & 53.67 & 52.65 & **0.97** & **35.87** & **97.14** \\ SABA [30] & 45.32 & 57.32 & 19.14 & 65.25 & **48.13** & 40.47 & 29.13 & 73.27 & **52.52** & **47.77** & 41.34 & 60.12 & 51.55 & **81.11** & **36.36** & **97.11** \\ WaNet [38] & 46.98 & 0.43 & 43.99 & 64.53 & **62.1** & 0.23 & **55.13** & **99.34** & 54.04 & 94.86 & 3.92 & 51.12 & 54.65 & 85.75 & 10.35 & 55.80 \\ Average & \(46.49\) & 34.23 & 29.17 & 73.59 & 53.18 & 29.33 & 29.88 & 78.55 & 52.62 & 84.18 & 9.28 & 54.81 & 52.51 & **14.62** & **33.55** & **84.57** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Results on Tiny ImageNet with PreAct-ResNet18 and poisoning ratio \(10\%\).

\begin{table}
\begin{tabular}{c|c c c c|c c c c|c c c c|c c c} \hline \hline Defense & \multicolumn{3}{c|}{No Defense} & \multicolumn{3}{c|}{ANP [55]} & \multicolumn{3}{c|}{FP [31]} & \multicolumn{3}{c}{NC [47]} \\ \hline Attack & ACC & ASR & R-ACC & DER & ACC & ASR & R-ACC & DER & ACC & ASR & R-ACC & DER & ACC & ASR & R-ACC & DER \\ \hline BadNets [15] & 91.32 & 95.03 & 4.67 & N/A & 90.88 & 4.88 & 8.722 & 94.86 & **91.31** & 57.13 & 41.62 & 68.95 & 89.05 & 1.27 & 89.16 & 95.75 \\ Blended [7] & 93.47 & 99.92 & 0.08 & N/A & 92.97 & 84.88 & 13.36 & 57.22 & 83.37 & 99.26 & 0.73 & 50.18 & **93.47** & 99.92 & 0.08 & 50.00 \\ Input-aware [37] & 90.67 & 98.26 & 1.66 & N/A & 91.04 & 1.32 & 86.71 & 98.47 & 97.44 & **0.

### Ablation Study

In this section, we conduct ablation study to investigate each component of SAU. Specifically, SAU is composed of two parts: generating shared adversarial examples according to the maximization objective in (13) denoted by **A**, and reducing shared adversarial risk according to the minimization objective in (13), denote by **B**. To study the effect of these parts, we replace them with the corresponding part in vanilla adversarial training, denoted by **C** and **D**, respectively. Then, we run each variant \(20\) epochs on Tiny ImageNet with PreAct-ResNet18 and a poisoning ratio \(10\%\). Each experiment is repeated five times, and the error bar is only shown in Figure 3 for simplicity.

As shown in Table 6, the maximization objective for generating shared adversarial examples (component **A**) is the key to reducing ASR, and component **B** can help to alleviate the hurt to clean accuracy. When replacing the component **B** with the minimization step in vanilla AT (**D**), it suffers from a drop of clean accuracy, although it can also work for mitigating backdoor effectively. Compared to vanilla AT, SAU achieves significantly lower ASR and higher ACC, with a much more stable learning curve, as shown in Figure 3. We remark that the decrease of clean accuracy for **A+D** and vanilla AT demonstrates that imposing a model to learn overwhelming adversarial examples may severely hurt the clean accuracy, a phenomenon widely studied in the area of adversarial training [46, 60].

## 5 Conclusion

In conclusion, this paper proposes Shared Adversarial Unlearning, a method to defend against backdoor attacks in deep neural networks through adversarial training techniques. By developing a better understanding of the connection between adversarial examples and poisoned samples, we propose a novel upper bound for backdoor risk and a bi-level formulation for mitigating backdoor attacks in poisoned models. Our approach identifies shared adversarial examples and unlearns them to break the connection between the poisoned sample and the target label. We demonstrate the effectiveness of our proposed method through extensive experiments, showing that it achieves comparable to, and even superior, performance than six different state-of-the-art defense methods on seven SOTA backdoor attacks with different model structures and datasets. Our work provides a valuable contribution to the field of backdoor defense in deep neural networks, with potential applications in various domains.

Limitation and future work.One important direction for future work, and a current challenge, is the accessibility of clean samples. A valuable approach to this challenge is to consider the samples from other domains and the generated samples.

Structure of Appendix.The detailed proof, analysis, and extension of propositions are given in Appendix A. Some important discussions such as the computation cost, the scalability, and the generalization ability of the proposed method are provided in Appendix B. The details of the experiments are provided in Appendix C. Additional experiments on different settings such as model structures, poisoning ratios, and sizes of clean datasets are given in Appendix D.

\begin{table}
\begin{tabular}{c|c c|c c|c c} \hline Attack & BadNets & \multicolumn{2}{c|}{Blended} & \multicolumn{2}{c}{LF} \\ \hline Defense & ACC & ASR & ACC & ASR & ACC & ASR \\ \hline
**A+B** (Ours) & \(50.60\) & \(0.27\) & \(51.47\) & \(1.02\) & \(51.31\) & \(0.72\) \\ \hline
**A+D** & \(37.21\) & \(0.30\) & \(38.35\) & \(1.77\) & \(36.68\) & \(0.17\) \\ \hline
**C+B** & \(53.05\) & \(28.57\) & \(53.16\) & \(69.22\) & \(52.57\) & \(68.65\) \\ \hline
**C+D** (Vanilla AT) & \(46.66\) & \(2.48\) & \(46.39\) & \(38.03\) & \(46.69\) & \(13.68\) \\ \hline \end{tabular}
\end{table}
Table 6: Results on Tiny ImageNet with different variants of SAU and Vanilla AT.

Figure 3: Learn curves for SAU, Vanilla AT and their variants, averaged over five runs.

## Acknowledgments and Disclosure of Funding

This work is supported by the National Natural Science Foundation of China under grant No. 62076213, Shenzhen Science and Technology Program under grant No. RCYX20210609103057050, No. GXWD20201231105722002-20200901175001001, and No. ZDSYS20211021111415025, and No. JCYJ20210324120011032 and the CAAI-Huawei MindSpore Open Fund, and the Guangdong Provincial Key Laboratory of Big Data Computing, the Chinese University of Hong Kong, Shenzhen.

## References

* [1] Insaf Adjabi, Abdeldjalil Ouahabi, Amir Benzaoui, and Abdelmalik Taleb-Ahmed. Past, present, and future of face recognition: A review. _Electronics_, 9(8):1188, 2020.
* [2] Mauro Barni, Kassem Kallas, and Benedetta Tondi. A new backdoor attack in cnns by training set corruption without label poisoning. In _ICIP 2019_, pages 101-105. IEEE, 2019.
* [3] Leon Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In _NeurIPS 2007_, volume 20, 2007.
* [4] Shuwen Chai and Jinghui Chen. One-shot neural backdoor erasing via adversarial weight masking. In _NeurIPS 2022_, 2022.
* [5] Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung Lee, Ian Molloy, and Biplav Srivastava. Detecting backdoor attacks on deep neural networks by activation clustering. In _Workshop on Artificial Intelligence Safety_. CEUR-WS, 2019.
* [6] Weixin Chen, Baoyuan Wu, and Haoqian Wang. Effective backdoor defense by exploiting sensitivity of poisoned samples. In _NeurIPS 2022_, volume 35, pages 9727-9737, 2022.
* [7] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep learning systems using data poisoning. _arXiv e-prints_, pages arXiv-1712, 2017.
* [8] Ziyi Cheng, Baoyuan Wu, Zhenya Zhang, and Jianjun Zhao. Tat: Targeted backdoor attacks against visual object tracking. _Pattern Recognition_, 142:109629, 2023.
* [9] Khoa Doan, Yingjie Lao, Weijie Zhao, and Ping Li. Lira: Learnable, imperceptible and robust backdoor attacks. In _ICCV 2021_, pages 11946-11956, 2021.
* [10] Yinpeng Dong, Xiao Yang, Zhijie Deng, Tianyu Pang, Zihao Xiao, Hang Su, and Jun Zhu. Black-box detection of backdoor attacks with limited information and data. In _ICCV 2021_, pages 16482-16491, 2021.
* [11] Bent Fuglede and Flemming Topsoe. Jensen-shannon divergence and hilbert space embedding. In _ISIT 2004_, page 31. IEEE, 2004.
* [12] Kuofeng Gao, Jiawang Bai, Baoyuan Wu, Mengxi Ya, and Shu-Tao Xia. Imperceptible and robust backdoor attack in 3d point cloud. _arXiv preprint arXiv:2208.08052_, 2022.
* [13] Yinghua Gao, Dongxian Wu, Jingfeng Zhang, Guanhao Gan, Shu-Tao Xia, Gang Niu, and Masashi Sugiyama. On the effectiveness of adversarial training against backdoor attacks. _IEEE Transactions on Neural Networks and Learning Systems_, 2023.
* [14] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In _ICLR 2015_, 2015.
* [15] Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Evaluating backdooring attacks on deep neural networks. _IEEE Access_, 7:47230-47244, 2019.
* [16] Junfeng Guo, Ang Li, and Cong Liu. AEVA: Black-box backdoor detection using adversarial extreme value analysis. In _ICLR 2022_, 2022.
* [17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _CVPR 2016_, pages 770-778, 2016.
* [18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In _ECCV 2016_, pages 630-645. Springer, 2016.
* [19] Kunzhe Huang, Yiming Li, Baoyuan Wu, Zhan Qin, and Kui Ren. Backdoor defense via decoupling the training process. In _ICLR 2022_. OpenReview.net, 2022.

* [20] Jinyuan Jia, Yupei Liu, Xiaoyu Cao, and Neil Zhenqiang Gong. Certified robustness of nearest neighbors against data poisoning and backdoor attacks. In _AAAI 2022_, volume 36, pages 9575-9583, 2022.
* [21] Xiaojun Jia, Yong Zhang, Xingxing Wei, Baoyuan Wu, Ke Ma, Jue Wang, and Xiaochun Cao. Prior-guided adversarial initialization for fast adversarial training. In _ECCV 2022_, pages 567-584, 2022.
* [22] Xiaojun Jia, Yong Zhang, Baoyuan Wu, Ke Ma, Jue Wang, and Xiaochun Cao. LAS-AT: adversarial training with learnable attack strategy. In _CVPR 2022_, pages 13388-13398, 2022.
* [23] Xiaojun Jia, Yong Zhang, Baoyuan Wu, Jue Wang, and Xiaochun Cao. Boosting fast adversarial training with learnable adversarial initialization. _IEEE Transactions on Image Processing_, 31:4417-4430, 2022.
* [24] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [25] Cassidy Laidlaw, Sahil Singla, and Soheil Feizi. Perceptual adversarial robustness: Defense against unseen threat models. In _ICLR 2021_, 2021.
* [26] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. _CS 231N_, 7(7):3, 2015.
* [27] Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Anti-backdoor learning: Training clean models on poisoned data. In _NeurIPS 2021_, pages 14900-14912, 2021.
* [28] Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Neural attention distillation: Erasing backdoor triggers from deep neural networks. In _ICLR 2021_. OpenReview.net, 2021.
* [29] Yige Li, Xixiang Lyu, Xingjun Ma, Nodens Koren, Lingjuan Lyu, Bo Li, and Yu-Gang Jiang. Reconstructive neuron pruning for backdoor defense. In _ICML 2023_, 2023.
* [30] Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, and Siwei Lyu. Invisible backdoor attack with sample-specific triggers. In _ICCV 2021_, pages 16463-16472, 2021.
* [31] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending against backdooring attacks on deep neural networks. In _RAID 2018_, pages 273-294. Springer, 2018.
* [32] Liangkai Liu, Sidi Lu, Ren Zhong, Baofu Wu, Yongtao Yao, Qingyang Zhang, and Weisong Shi. Computing systems for autonomous driving: State of the art and challenges. _IEEE Internet of Things Journal_, 8(8):6469-6486, 2020.
* [33] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In _ICLR 2018_, 2018.
* [34] Pratyush Maini, Xinyun Chen, Bo Li, and Dawn Song. Perturbation type categorization for multiple adversarial perturbation robustness. In _UAI 2022_, pages 1317-1327. PMLR, 2022.
* [35] Bingxu Mu, Zhenxing Niu, Le Wang, Xue Wang, Qiguang Miao, Rong Jin, and Gang Hua. Progressive backdoor erasing via connecting backdoor and adversarial attacks. In _CVPR 2023_, pages 20495-20503, 2023.
* [36] Preetum Nakkiran, Behnam Neyshabur, and Hanie Sedghi. The deep bootstrap framework: Good online learners are good offline generalizers. In _ICLR 2021_, 2021.
* [37] Tuan Anh Nguyen and Anh Tran. Input-aware dynamic backdoor attack. In _NeurIPS 2020_, 2020.
* imperceptible warping-based backdoor attack. In _ICLR 2021_. OpenReview.net, 2021.
** [39] Xiangyu Qi, Tinghao Xie, Jiachen T Wang, Tong Wu, Saeed Mahloujifar, and Prateek Mittal. Towards a proactive {ML} approach for detecting backdoor poison samples. In _32nd USENIX Security Symposium (USENIX Security 23)_, pages 1685-1702, 2023.
* [40] Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and Tom Goldstein. Poison frogs! targeted clean-label poisoning attacks on neural networks. In _NeurIPS 2018_, volume 31, 2018.
* [41] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In _ICLR 2015_, 2015.
* [42] Hossein Souri, Liam Fowl, Rama Chellappa, Micah Goldblum, and Tom Goldstein. Sleeper agent: Scalable hidden trigger backdoors for neural networks trained from scratch. In _NeurIPS 2022_, volume 35, pages 19165-19178, 2022.
* [43] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The german traffic sign recognition benchmark: a multi-class classification competition. In _IJCNN 2011_, pages 1453-1460. IEEE, 2011.
* [44] J-Donald Tournier, Robert Smith, David Raffelt, Rami Tabbara, Thijs Dhollander, Maximilian Pietsch, Daan Christiaens, Ben Jeurissen, Chun-Hung Yeh, and Alan Connelly. Mrtrix3: A fast, flexible and open software framework for medical image processing and visualisation. _Neuroimage_, 202:116137, 2019.
* [45] Florian Tramer and Dan Boneh. Adversarial training and robustness for multiple perturbations. In _NeurIPS 2019_, volume 32, pages 5858-5868, 2019.
* [46] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Robustness may be at odds with accuracy. In _ICLR 2019_, 2019.
* [47] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In _2019 IEEE Symposium on Security and Privacy_, pages 707-723. IEEE, 2019.
* [48] Haotao Wang, Aston Zhang, Shuai Zheng, Xingjian Shi, Mu Li, and Zhangyang Wang. Removing batch normalization boosts adversarial training. In _ICML 2022_, pages 23433-23445. PMLR, 2022.
* [49] Ruotong Wang, Hongrui Chen, Zihao Zhu, Li Liu, Yong Zhang, Yanbo Fan, and Baoyuan Wu. Robust backdoor attack with visible, semantic, sample-specific, and compatible triggers. _arXiv preprint arXiv:2306.00816_, 2023.
* [50] Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving adversarial robustness requires revisiting misclassified examples. In _ICLR 2020_, 2020.
* [51] Maurice Weber, Xiaojun Xu, Bojan Karlas, Ce Zhang, and Bo Li. Rab: Provable robustness against backdoor attacks. In _2023 IEEE Symposium on Security and Privacy (SP)_, pages 1311-1328. IEEE, 2023.
* [52] Cheng-Hsin Weng, Yan-Ting Lee, and Shan-Hung Brandon Wu. On the trade-off between adversarial and backdoor robustness. In _NeurIPS 2020_, volume 33, pages 11973-11983, 2020.
* [53] Eric Wong, Leslie Rice, and J. Zico Kolter. Fast is better than free: Revisiting adversarial training. In _ICLR 2020_, 2020.
* [54] Baoyuan Wu, Hongrui Chen, Mingda Zhang, Zihao Zhu, Shaokui Wei, Danni Yuan, and Chao Shen. Backdoorbench: A comprehensive benchmark of backdoor learning. In _NeurIPS Datasets and Benchmarks Track 2022_, 2022.
* [55] Dongxian Wu and Yisen Wang. Adversarial neuron pruning purifies backdoored deep models. In _NeurIPS 2021_, pages 16913-16925, 2021.
** [56] Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, and Dawn Song. Spatially transformed adversarial examples. In _ICLR 2018_, 2018.
* [57] Cihang Xie and Alan L. Yuille. Intriguing properties of adversarial training at scale. In _ICLR 2020_, 2020.
* [58] Yi Zeng, Won Park, Z. Morley Mao, and Ruoxi Jia. Rethinking the backdoor attacks' triggers: A frequency perspective. In _ICCV 2021_, pages 16453-16461. IEEE, 2021.
* [59] Yi Zeng, Si Chen, Won Park, Zhuoqing Mao, Ming Jin, and Ruoxi Jia. Adversarial unlearning of backdoors via implicit hypergradient. In _ICLR 2022_. OpenReview.net, 2022.
* [60] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theoretically principled trade-off between robustness and accuracy. In _ICML 2019_, pages 7472-7482. PMLR, 2019.
* [61] Yuhao Zhang, Aws Alarghouthi, and Loris D'Antoni. Bagflip: A certified defense against data poisoning. In _NeurIPS 2022_, volume 35, pages 31474-31483, 2022.
* [62] Haizhong Zheng, Ziqi Zhang, Juncheng Gu, Honglak Lee, and Atul Prakash. Efficient adversarial training with transferable adversarial examples. In _CVPR 2020_, pages 1178-1187, 2020.
* [63] Runkai Zheng, Rongjun Tang, Jianze Li, and Li Liu. Data-free backdoor removal based on channel lipschitzness. In _European Conference on Computer Vision_, pages 175-191. Springer, 2022.
* [64] Runkai Zheng, Rongjun Tang, Jianze Li, and Li Liu. Pre-activation distributions expose backdoor neurons. In _NeurIPS 2022_, 2022.
* [65] Mingli Zhu, Shaokui Wei, Li Shen, Yanbo Fan, and Baoyuan Wu. Enhancing fine-tuning based backdoor defense with sharpness-aware minimization. In _ICCV 2023_, pages 4466-4477, 2023.
* [66] Mingli Zhu, Shaokui Wei, Hongyuan Zha, and Baoyuan Wu. Neural polarizer: A lightweight and effective backdoor defense via purifying poisoned features. _arXiv preprint arXiv:2306.16697_, 2023.
* [67] Zihao Zhu, Mingda Zhang, Shaokui Wei, Li Shen, Yanbo Fan Fan, and Baoyuan Wu. Boosting backdoor attack with a learnable poisoning sample selection strategy. _arXiv preprint arXiv:2307.07328_, 2023.
* [68] Zihao Zhu, Mingda Zhang, Shaokui Wei, Bingzhe Wu, and Baoyuan Wu. Vdc: Versatile data cleanser for detecting dirty samples via visual-linguistic inconsistency. _arXiv preprint arXiv:2309.16211_, 2023.

[MISSING_PAGE_FAIL:16]

### Extension to Universal Adversarial Perturbation

Universal Adversarial Perturbation (UAP) has achieved remarkable performance in previous studies [59; 4], especially for defending against backdoor attacks with fixed additive triggers. Now, we extend \(\mathcal{R}_{sub}\) to UAP and show that shared adversarial examples also benefit adversarial training with UAP.

We denote the Universal Adversarial Risk by

\[\mathcal{R}_{uadv}(h_{\bm{\theta}})=\frac{\max_{\bm{\epsilon}\in\mathcal{S}} \sum_{i=1}^{N}\mathbb{I}(\tilde{\bm{x}}_{i,\bm{\epsilon}}\in\mathcal{A}_{\bm {\epsilon},\bm{\theta}},\bm{x}_{i}\in\mathcal{D}_{-\hat{y}})}{|\mathcal{D}_{- \hat{y}}|}.\]

To bridge UAP and backdoor trigger, we first provide the following assumption:

**Assumption 2**.: _Assume that the \(g(\bm{x},\Delta)=\bm{x}+\Delta\) for all \(\bm{x}\in\mathcal{D}_{cl}\)._

Assumption 2 says that the poisoned samples are planted with a fixed additive trigger.

Then, under Assumption 2 and Assumption 1, we have

\[\mathcal{R}_{bd}(h_{\bm{\theta}})\leq\mathcal{R}_{uadv}\leq\mathcal{R}_{adv}.\]

by the fact that \(\Delta\in\mathcal{S}\).

For clarity, we replace \(\tilde{\bm{x}}_{\bm{\epsilon}}\in\mathcal{A}_{\bm{\epsilon},\bm{\theta}}\) by \(h_{\bm{\theta}}(\bm{x}+\bm{\epsilon})\neq y\) and \(\tilde{\bm{x}}_{\bm{\epsilon}}\) by \(\bm{x}+\bm{\epsilon}\) which makes the rest derivation easier to follow.

Then, similar as \(\mathcal{R}_{sub}\), we define the universal sub-adversarial risk \(\mathcal{R}_{usub}\) as below:

\[\mathcal{R}_{usub}(h_{\bm{\theta}})=\frac{1}{|\mathcal{D}_{-\hat{ y}}|}\max_{\bm{\epsilon}\in\mathcal{S}}\sum_{i=1}^{N}\bigl{\{}\mathbb{I}(h_{\bm{ \theta}}(\bm{x}+\bm{\epsilon})=h_{\bm{\theta}_{bd}}(\bm{x}+\bm{\epsilon})\neq y,\bm{x}\in\mathcal{D}_{s})+\] \[\mathbb{I}(h_{\bm{\theta}}(\bm{x}+\bm{\epsilon})\neq y,\bm{x}\in \mathcal{D}_{-\hat{y}}\setminus\mathcal{D}_{s})\bigr{\}},\]

which considers shared universal adversarial risk on \(\mathcal{D}_{s}\) and vanilla universal adversarial risk on \(\mathcal{D}_{-\hat{y}}\setminus\mathcal{D}_{s}\).

By the definition of \(\mathcal{R}_{usub}\), we can easily find \(\mathcal{R}_{usub}\leq\mathcal{R}_{uadv}\).

Furthermore, we have

\[\mathcal{R}_{bd}(h_{\bm{\theta}}) =\frac{\sum_{i=1}^{N}\mathbb{I}(h_{\bm{\theta}}(\bm{x}+\Delta)= \hat{y},\bm{x}\in\mathcal{D}_{-\hat{y}})}{|\mathcal{D}_{-\hat{y}}|}\] \[=\frac{\sum_{i=1}^{N}\mathbb{I}\{\mathbb{I}(h_{\bm{\theta}}(\bm{ x}+\Delta)=h_{\bm{\theta}_{bd}}(\bm{x}+\Delta),\bm{x}\in\mathcal{D}_{s})+\mathbb{I}(h_{ \bm{\theta}}(\bm{x}+\Delta)=\hat{y},\bm{x}\in\mathcal{D}_{-\hat{y}}\setminus \mathcal{D}_{s})\}}{|\mathcal{D}_{-\hat{y}}|}\] \[\leq\frac{\max_{\bm{\epsilon}\in\mathcal{S}}\sum_{i=1}^{N}\{ \mathbb{I}(h_{\bm{\theta}}(\bm{x}+\bm{\epsilon})=h_{\bm{\theta}_{bd}}(\bm{x}+ \bm{\epsilon}),\bm{x}\in\mathcal{D}_{s})+\mathbb{I}(h_{\bm{\theta}}(\bm{x}+ \bm{\epsilon})\neq y,\bm{x}\in\mathcal{D}_{-\hat{y}}\setminus\mathcal{D}_{s})\} }{|\mathcal{D}_{-\hat{y}}|}\] \[=\mathcal{R}_{usub}(h_{\bm{\theta}}).\]

Therefore, we reach the following proposition

**Proposition 4**.: _Under Assumption 2 and Assumption 1, for a classifier \(h_{\bm{\theta}}\), the following inequalities hold_

\[\mathcal{R}_{bd}(h_{\bm{\theta}})\leq\mathcal{R}_{usub}(h_{\bm{\theta}})\leq \mathcal{R}_{uadv}(h_{\bm{\theta}})\leq\mathcal{R}_{adv}(h_{\bm{\theta}}).\]

Therefore, the shared adversarial examples also benefit adversarial training with UAP.

### Extension to Targeted Adversarial Training

In the previous sections, we assume that the defender has no access to either the trigger or the target label. Since there are already some works for backdoor trigger detection Guo et al. [16], Dong et al. [10], we discuss the case where the defender can estimate the target label \(\hat{y}\) and show that shared adversarial examples can also be beneficial for adversarial training with target label.

Given a target label \(\hat{y}\), we define the Targeted Adversarial Risk as

\[\mathcal{R}_{tadv}(h_{\bm{\theta}})=\frac{\sum_{i=1}^{N}\max_{\bm{\epsilon}_{i} \in\mathcal{S}}\mathbb{I}(h_{\bm{\theta}}(\tilde{\bm{x}}_{i,\bm{\epsilon}_{i}} )=\hat{y},\bm{x}_{i}\in\mathcal{D}_{-\hat{y}})}{|\mathcal{D}_{-\hat{y}}|}.\] (17)

By the definition of \(\hat{y}\), we have \(\mathcal{R}_{bd}(h_{\bm{\theta}})\leq\mathcal{R}_{tadv}(h_{\bm{\theta}})\) under Assumption 1.

Then, we have

\[\mathcal{R}_{tadv}(h_{\bm{\theta}})=\frac{1}{|\mathcal{D}_{-\hat{y}}|}\sum_{i =1}^{N}\max_{\bm{\epsilon}_{i}\in\mathcal{S}}\Big{\{}\mathbb{I}(\bm{x}_{i} \in\mathcal{D}_{s})\mathbb{I}(h_{\bm{\theta}}(\tilde{\bm{x}}_{i,\bm{\epsilon} _{i}})=h_{\bm{\theta}_{bd}}(\tilde{\bm{x}}_{i,\bm{\epsilon}_{i}})=\hat{y})\] (18)

Then, similar as \(\mathcal{R}_{sub}\), we define the targeted sub-adversarial risk \(\mathcal{R}_{tsub}\) by removing the second component in \(\mathcal{R}_{tadv}(h_{\bm{\theta}})\):

\[\mathcal{R}_{tsub}(h_{\bm{\theta}})= \frac{1}{|\mathcal{D}_{-\hat{y}}|}\sum_{i=1}^{N}\max_{\bm{ \epsilon}_{i}\in\mathcal{S}}\Big{\{}\mathbb{I}(\bm{x}_{i}\in\mathcal{D}_{s}) \mathbb{I}(h_{\bm{\theta}}(\tilde{\bm{x}}_{i,\bm{\epsilon}_{i}})=h_{\bm{ \theta}_{bd}}(\tilde{\bm{x}}_{i,\bm{\epsilon}_{i}})=\hat{y})\] \[+\mathbb{I}(\bm{x}_{i}\in\mathcal{D}_{-\hat{y}}\setminus\mathcal{D }_{s},h_{\bm{\theta}}(\tilde{\bm{x}}_{i,\bm{\epsilon}_{i}})=\hat{y})\Big{\}}.\]

which considers shared targeted adversarial risk on \(\mathcal{D}_{s}\) and vanilla targeted adversarial risk on \(\mathcal{D}_{-\hat{y}}\setminus\mathcal{D}_{s}\).

Since the removed component is irrelevant to backdoor risk, we have \(\mathcal{R}_{bd}\leq\mathcal{R}_{tsub}\) under Assumption 1.

Therefore, we reach the following proposition

Proof.: Under Assumption 2 and Assumption 1, for a classifier \(h_{\bm{\theta}}\), the following inequalities hold

\[\mathcal{R}_{bd}(h_{\bm{\theta}})\leq\mathcal{R}_{tsub}(h_{\bm{\theta}})\leq \mathcal{R}_{tadv}(h_{\bm{\theta}})\leq\mathcal{R}_{adv}(h_{\bm{\theta}}).\]

Therefore, the shared adversarial examples also benefit adversarial training with target label.

Moreover, since \(\mathcal{R}_{tsub}(h_{\bm{\theta}})\) only consider shared adversarial example with target label \(\hat{y}\), the following proposition holds,

Proof.: Under Assumption 2 and Assumption 1, for a classifier \(h_{\bm{\theta}}\), the following inequalities hold

\[\mathcal{R}_{bd}(h_{\bm{\theta}})\leq\mathcal{R}_{tsub}(h_{\bm{\theta}})\leq \mathcal{R}_{sub}(h_{\bm{\theta}})\leq\mathcal{R}_{adv}(h_{\bm{\theta}}).\]

The above proposition shows that \(\mathcal{R}_{tsub}(h_{\bm{\theta}})\) is a tighter bound of \(\mathcal{R}_{bd}(h_{\bm{\theta}})\) than \(\mathcal{R}_{sub}(h_{\bm{\theta}})\).

### Extension to Multi-target/Multi-trigger

In previous sections, we assume that there is only one trigger and one target for all samples. Now, we consider to a more general case where a set of triggers \(\bm{V}\) and a set of target labels \(\hat{\bm{Y}}\) are given. This is a more challenging case since there may be multiple triggers and/or multiple targets for one sample.

We define \(T\) to be a mapping from the sample to the corresponding set of feasible trigger-target pairs. Therefore, given a sample \(\bm{x}\), the set of all feasible trigger-target pairs is given by \(T(\bm{x})\). Then, Assumption 1 is extended to all triggers accordingly, \(i.e.\), \(g(\bm{x},\Delta)-\bm{x}\in\mathcal{S},\forall(\Delta,\hat{y})\in T(\bm{x})\).

In this setting, we define that a classifier \(h_{\bm{\theta}}\) is said to be attacked by a poisoned sample generated by \(\bm{x}\) if there exists \((\Delta,\hat{y})\in T(\bm{x})\) such that \(h_{\bm{\theta}}(g(\bm{x},\Delta))=\hat{y}\neq y\).

Then, the (maximum) backdoor risk on \(\mathcal{D}_{cl}\) is defined to be

\[\mathcal{R}_{bd}(h_{\bm{\theta}})=\frac{\sum_{i=1}^{N}\max_{(\Delta_{i},\hat{y }_{i})\in T(\bm{x}_{i}))}\mathbb{I}(h_{\bm{\theta}}(g(\bm{x}_{i},\Delta_{i}))= \hat{y}_{i}\neq y)}{|\mathcal{D}_{cl}|}.\] (19)

Similarly, the adversarial risk on \(\mathcal{D}_{cl}\) is defined as

\[\mathcal{R}_{adv}=\frac{1}{|\mathcal{D}_{cl}|}\sum_{i=1}^{N}\max_{\bm{ \epsilon}_{i}\in\mathcal{S}}\big{\{}\mathbb{I}(h_{\bm{\theta}}(g(\bm{x}_{i}, \bm{\epsilon}_{i}))\neq y)\big{\}}.\]

We define \(\mathcal{D}_{s}=\{\bm{x}\in\mathcal{D}_{cl}:h_{\bm{\theta}_{bd}}(g(\bm{x}, \Delta))=\hat{y}\neq y,\forall(\Delta,\hat{y})\in T(\bm{x})\}\) to be the set of samples on which planting any feasible trigger can activate the corresponding backdoor. Then, the corresponding sub-adversarial risk is defined as

\[\mathcal{R}_{sub}= \frac{1}{|\mathcal{D}_{cl}|}\sum_{i=1}^{N}\max_{\bm{\epsilon}_{i }\in\mathcal{S}}\big{\{}\mathbb{I}(h_{\bm{\theta}}(g(\bm{x}_{i},\bm{\epsilon }_{i}))=h_{\bm{\theta}_{bd}}(g(\bm{x}_{i},\bm{\epsilon}_{i}))\neq y,\bm{x}_{i} \in\mathcal{D}_{s})\] \[+\mathbb{I}(h_{\bm{\theta}}(g(\bm{x}_{i},\bm{\epsilon}_{i}))\neq y,\bm{x}_{i}\in\mathcal{D}_{cl}\setminus\mathcal{D}_{s})\big{\}}.\]

which considers shared adversarial risk on \(\mathcal{D}_{s}\) and vanilla targeted adversarial risk on \(\mathcal{D}_{cl}\setminus\mathcal{D}_{s}\).

By definition, we have \(\mathcal{R}_{sub}\leq\mathcal{R}_{adv}\)

Moreover, we have

\[\mathcal{R}_{bd}(h_{\bm{\theta}}) =\frac{\sum_{i=1}^{N}\max_{(\Delta_{i},\hat{y}_{i})\in T(\bm{x}_ {i}))}\mathbb{I}(h_{\bm{\theta}}(g(\bm{x}_{i},\Delta_{i}))=\hat{y}_{i}\neq y) }{|\mathcal{D}_{cl}|}\] \[=\frac{1}{|\mathcal{D}_{cl}|}\!\sum_{i=1}^{N}\max_{(\Delta_{i}, \hat{y}_{i})\in T(\bm{x}_{i}))}\big{\{}\mathbb{I}(h_{\bm{\theta}}(g(\bm{x}_{i },\Delta_{i}))=h_{\bm{\theta}_{bd}}(g(\bm{x}_{i},\Delta_{i})),h_{\bm{\theta} _{bd}}(g(\bm{x}_{i},\Delta_{i}))=\hat{y}_{i}\neq y)\] \[+\mathbb{I}(h_{\bm{\theta}}(g(\bm{x}_{i},\Delta_{i}))\neq y,h_{ \bm{\theta}_{bd}}(g(\bm{x}_{i},\Delta_{i}))\neq\hat{y}_{i})\big{\}}\] \[\leq\frac{1}{|\mathcal{D}_{cl}|}\!\sum_{i=1}^{N}\max_{(\Delta_{i}, \hat{y}_{i})\in T(\bm{x}_{i}))}\big{\{}\mathbb{I}(h_{\bm{\theta}}(g(\bm{x}_{i },\Delta_{i}))=h_{\bm{\theta}_{bd}}(g(\bm{x}_{i},\Delta_{i}))\neq y,\bm{x}_{i} \in\mathcal{D}_{s})\] \[+\mathbb{I}(h_{\bm{\theta}}(g(\bm{x}_{i},\Delta_{i}))\neq y,\bm{x }_{i}\in\mathcal{D}_{cl}\setminus\mathcal{D}_{s})\big{\}}\] \[\leq\frac{1}{|\mathcal{D}_{cl}|}\!\sum_{i=1}^{N}\max_{\bm{ \epsilon}_{i}\in\mathcal{S}}\big{\{}\mathbb{I}(h_{\bm{\theta}}(g(\bm{x}_{i}, \bm{\epsilon}_{i}))=h_{\bm{\theta}_{bd}}(g(\bm{x}_{i},\bm{\epsilon}_{i})) \neq y,\bm{x}_{i}\in\mathcal{D}_{s})\] \[+\mathbb{I}(h_{\bm{\theta}}(g(\bm{x}_{i},\bm{\epsilon}_{i}))\neq y,\bm{x}_{i}\in\mathcal{D}_{cl}\setminus\mathcal{D}_{s})\big{\}}\] \[=\mathcal{R}_{sub}.\]

Therefore, for a backdoor attack with multiple targets and/or multiple triggers, the proposed sub-adversarial risk \(\mathcal{R}_{sub}\) is still a tighter upper bound of \(\mathcal{R}_{bd}\) compared to \(\mathcal{R}_{adv}\).

### Relaxation gap

To build a practical objective for defending against a backdoor attack, two relaxations are applied to \(\mathcal{R}_{sub}\). Here, we discuss the relaxation gap in the relaxing process:

* **Replace \(\mathcal{D}_{s}\) by \(\mathcal{D}_{-\hat{y}}\)**. Since \(\mathcal{D}_{s}\) is not accessible, the first relaxation is to replace \(\mathcal{D}_{s}\) by \(\mathcal{D}_{-\hat{y}}\) and consider shared adversarial examples on all samples in \(\mathcal{D}_{-\hat{y}}\). Denote \(\mathcal{R}_{1}(h_{\bm{\theta}})\) to be the relaxed risk after replacing\(\mathcal{D}_{s}\) by \(\mathcal{D}_{-\hat{y}}\). Since \(\mathcal{R}_{sub}\) can be decomposed as follows:

\[\mathcal{R}_{sub}(h_{\bm{\theta}})= \frac{1}{|\mathcal{D}_{-\hat{y}}|}\sum_{i=1}^{N}\max_{\bm{\epsilon} _{i}\in\mathcal{S}}\Big{\{}\mathbb{I}(\bm{x}_{i}\in\mathcal{D}_{s})\mathbb{I}( h_{\bm{\theta}}(\tilde{\bm{x}}_{i,\bm{\epsilon}_{i}})=h_{\bm{\theta}_{bd}}( \tilde{\bm{x}}_{i,\bm{\epsilon}_{i}}),\tilde{\bm{x}}_{i,\bm{\epsilon}_{i}}\in \mathcal{A}_{\bm{\epsilon}_{i},\bm{\theta}})\] \[+\mathbb{I}(\bm{x}_{i}\in\mathcal{D}_{-\hat{y}}\setminus\mathcal{ D}_{s},\tilde{\bm{x}}_{i,\bm{\epsilon}_{i}}\in\mathcal{A}_{\bm{\epsilon}_{i},\bm{ \theta}})\Big{\}}\] \[=\frac{1}{|\mathcal{D}_{-\hat{y}}|}\sum_{i=1}^{N}\max_{\bm{ \epsilon}_{i}\in\mathcal{S}}\Big{\{}\mathbb{I}(\bm{x}_{i}\in\mathcal{D}_{- \hat{y}})\mathbb{I}(h_{\bm{\theta}}(\tilde{\bm{x}}_{i,\bm{\epsilon}_{i}})=h_{ \bm{\theta}_{bd}}(\tilde{\bm{x}}_{i,\bm{\epsilon}_{i}}),\tilde{\bm{x}}_{i,\bm {\epsilon}_{i}}\in\mathcal{A}_{\bm{\epsilon}_{i},\bm{\theta}})\] \[+\mathbb{I}(\bm{x}_{i}\in\mathcal{D}_{-\hat{y}}\setminus\mathcal{ D}_{s})\mathbb{I}(h_{\bm{\theta}}(\tilde{\bm{x}}_{i,\bm{\epsilon}_{i}})\neq h_{ \bm{\theta}_{bd}}(\tilde{\bm{x}}_{i,\bm{\epsilon}_{i}}),\tilde{\bm{x}}_{i,\bm {\epsilon}_{i}}\in\mathcal{A}_{\bm{\epsilon}_{i},\bm{\theta}})\Big{\}}.\]

We can find that the relaxation gap is

\[\mathcal{R}_{sub}-\mathcal{R}_{1} \leq\frac{1}{|\mathcal{D}_{-\hat{y}}|}\sum_{i=1}^{N}\max_{\bm{ \epsilon}_{i}\in\mathcal{S}}\mathbb{I}(\bm{x}_{i}\in\mathcal{D}_{-\hat{y}} \setminus\mathcal{D}_{s})\mathbb{I}(h_{\bm{\theta}}(\tilde{\bm{x}}_{i,\bm{ \epsilon}_{i}})\neq h_{\bm{\theta}_{bd}}(\tilde{\bm{x}}_{i,\bm{\epsilon}_{i}} ),\tilde{\bm{x}}_{i,\bm{\epsilon}_{i}}\in\mathcal{A}_{\bm{\epsilon}_{i},\bm{ \theta}})\] \[\leq\frac{1}{|\mathcal{D}_{-\hat{y}}|}\sum_{i=1}^{N}\mathbb{I}( \bm{x}_{i}\in\mathcal{D}_{-\hat{y}}\setminus\mathcal{D}_{s})\] \[=1-\mathcal{R}_{bd}(h_{\bm{\theta}_{bd}}).\]

And the relaxation gap GAP\({}_{1}\) is bounded by \(1-\mathcal{R}_{bd}(h_{\bm{\theta}_{bd}})\). Since a poisoned model \(h_{\bm{\theta}_{bd}}\) usually has a high ASR for the backdoor attack, the gap GAP\({}_{1}\) is usually negligible.

**Remark:** We remark that the experiment results show that our method also works well for the poisoned models with low ASR. See Section D for more details.

* **Replace \(\mathcal{D}_{-\hat{y}}\) by \(\mathcal{D}_{cl}\)**. The second relaxation is to replace \(\mathcal{D}_{-\hat{y}}\) by \(\mathcal{D}_{cl}\) when \(\hat{y}\) is not accessible. Let \(\mathcal{D}_{\hat{y}}=\mathcal{D}_{cl}\setminus\mathcal{D}_{-\hat{y}}\). Then, the risk after the second relaxation is: \[\mathcal{R}_{share}(h_{\bm{\theta}})=\frac{1}{N}\sum_{i=1}^{N}\max_{\bm{ \epsilon}_{i}\in\mathcal{S}}\Big{\{}\mathbb{I}(h_{\bm{\theta}}(\tilde{\bm{x}}_ {i,\bm{\epsilon}_{i}})=h_{\bm{\theta}_{bd}}(\tilde{\bm{x}}_{i,\bm{\epsilon}_{ i}}),\tilde{\bm{x}}_{i,\bm{\epsilon}_{i}}\in\mathcal{A}_{\bm{\epsilon}_{i},\bm{ \theta}})\Big{\}}\] \[=\frac{|\mathcal{D}_{-\hat{y}}|}{N}\frac{1}{|\mathcal{D}_{-\hat{y} }|}\sum_{i=1}^{N}\max_{\bm{\epsilon}_{i}\in\mathcal{S}}\Big{\{}\mathbb{I}(\bm{x} _{i}\in\mathcal{D}_{-\hat{y}})\mathbb{I}(h_{\bm{\theta}}(\tilde{\bm{x}}_{i, \bm{\epsilon}_{i}})=h_{\bm{\theta}_{bd}}(\tilde{\bm{x}}_{i,\bm{\epsilon}_{i}}),\tilde{\bm{x}}_{i,\bm{\epsilon}_{i}}\in\mathcal{A}_{\bm{\epsilon}_{i},\bm{ \theta}})\Big{\}}\] \[+\frac{|\mathcal{D}_{\hat{y}}|}{N}\frac{1}{|\mathcal{D}_{\hat{y} }|}\sum_{i=1}^{N}\max_{\bm{\epsilon}_{i}\in\mathcal{S}}\Big{\{}\mathbb{I}(\bm{ x}_{i}\in\mathcal{D}_{\hat{y}})\mathbb{I}(h_{\bm{\theta}}(\tilde{\bm{x}}_{i,\bm{ \epsilon}_{i}})=h_{\bm{\theta}_{bd}}(\tilde{\bm{x}}_{i,\bm{\epsilon}_{i}}), \tilde{\bm{x}}_{i,\bm{\epsilon}_{i}}\in\mathcal{A}_{\bm{\epsilon}_{i},\bm{ \theta}})\Big{\}}.\] where the first component is \(\mathcal{R}_{1}\) scaled by \(\frac{|\mathcal{D}_{-\hat{y}}|}{N}\) and the second component is the vanilla adversarial risk on \(\mathcal{D}_{\hat{y}}\), scaled by \(\frac{|\mathcal{D}_{\hat{y}}|}{N}\). So, the relaxation gap for the second relaxation is \[\mathcal{R}_{1}-\mathcal{R}_{share}= \frac{|\mathcal{D}_{\hat{y}}|}{N}\mathcal{R}_{1}\] \[-\frac{|\mathcal{D}_{\hat{y}}|}{N}\frac{1}{|\mathcal{D}_{\hat{y} }|}\sum_{i=1}^{N}\max_{\bm{\epsilon}_{i}\in\mathcal{S}}\Big{\{}\mathbb{I}(\bm{ x}_{i}\in\mathcal{D}_{\hat{y}})\mathbb{I}(h_{\bm{\theta}}(\tilde{\bm{x}}_{i,\bm{\epsilon}_{i}})=h_{ \bm{\theta}_{bd}}(\tilde{\bm{x}}_{i,\bm{\epsilon}_{i}}),\tilde{\bm{x}}_{i,\bm{ \epsilon}_{i}}\in\mathcal{A}_{\bm{\epsilon}_{i},\bm{\theta}})\Big{\}}.\] We can find that the generalization gap is related to the \(\frac{|\mathcal{D}_{\hat{y}}|}{N}\), \(i.e.\), the portion of samples whose labels are target labels, and negligible when \(\frac{|\mathcal{D}_{\hat{y}}|}{N}\) is small.

**Remark.** For using \(\mathcal{R}_{share}(h_{\bm{\theta}})\) to mitigate the backdoor, the influence of \(\frac{|\mathcal{D}_{-\hat{y}}|}{N}\) can be eliminated by altering the trade-off parameter in Problem (8). Also, adding triggers to samples labeled \(\hat{y}\) is harmless during the testing/deployment phase. So, the vanilla adversarial risk on \(\mathcal{D}_{\hat{y}}\) has a negligible negative impact on defending the backdoor. Therefore, replacing \(\mathcal{D}_{-\hat{y}}\) with \(\mathcal{D}_{cl}\) has little negative impact on mitigating the backdoor.

Discussion

In this section, we provide a comprehensive discussion of the proposed method, including its efficiency, comparison to other methods, choice of perturbation, generalization ability to attack with excessive-magnitude triggers, performance on the generated dataset, and resistance to defense-aware/adaptive attacks.

### Efficiency and Computational Cost

Here, we would like to emphasize that SAU is an efficient and effective defense method that can mitigate backdoor attacks with acceptable overhead for the following reasons:

* SAU is a post-processing method that fine-tunes the backdoor model on a clean dataset with a small size. For instance, we can use only 500 samples from CIFAR-10 to fine-tune the model by SAU (see Appendix D.6 for more details), which takes 1.52 seconds/epoch to fine-tune PreAct-ResNet18 with RTX 4090. Therefore, it can be executed efficiently without requiring a large amount of data or computation resources.
* SAU only takes a few epochs to take effect, which further reduces the computational cost of executing it in practice. As shown in Appendix D.4, SAU can achieve a low Attack Success Rate and a high Accuracy in a few epochs.

To further demonstrate the efficiency of SAU, we compare its average runtime with other state-of-the-art defense methods against attacks. We use the same experimental setting as in Section 4, where the poisoning ratio is 10% and the backbone model is PreAct-ResNet18. All experiments are conducted on a server with GPU RTX 4090 and CPU AMD EPYC 7543 32-Core Processor. As different methods adopt different learning paradigms for backdoor mitigation, we measure the average runtime for each defense method to take effect on CIFAR-10 and Tiny ImageNet (\(\text{ASR}<5\%\) and \(\text{ACC}>85\%\) on CIFAR-10, or \(\text{ASR}<5\%\), \(\text{ACC}>45\%\) on Tiny ImageNet). Note that if a defend method cannot reach the criteria for "Take effect", the maximum runtime is reported. The results are summarized in the following table.

From Table 7, SAU is faster than most of the existing methods, except for NAD and EP, which shows that SAU is an efficient and effective defense method.

### Comparison with Certified Robustness methods

In backdoor defense, methods can be categorized into two classes: empirical methods which aim to develop effective and efficient methods for backdoor defense, and certified methods which aim to build a provable framework for backdoor robustness. As **our method is an empirical method**, we would like to point out that our method is different from the existing works on certified robustness in several aspects, which makes them incomparable or impractical to apply to our setting. Specifically:

* Efficiency and scalability. Certified methods usually have very high computation costs and are infeasible to apply to large-scale deep neural networks, while our method is efficient and scalable. For example, the representative methods RAB [51] and BagFlip [61] rely on voting over 1000 smoothed models for prediction and are only feasible for some simple neural networks and simple classification problems. To apply these methods to our setting, we would need to train 1000 PreAct-ResNet models on CIFAR-10 from scratch, which would take over 600 hours (25 days) and 620GB of space for a single attack on a server with one RTX 3090.
* Applicability to model architecture. Many certified methods are restricted to some simple or classical models, while our method is general and applicable to any deep neural network

\begin{table}
\begin{tabular}{c|c c c c c c c} \hline Dataset & ANP [55] & FP [31] & NC [47] & NAD [28] & EP [64] & i-BAU [59] & SAU (Ours) \\ \hline CIFAR-10 & 289.86s & 266.57s & 825.86s & 75.14s & 65.71s & 47.00s & 43.86s \\ Tiny ImageNet & 1086.50s & 318.30s & 27359.70s & 227.01s & 141.67s & 621.33s & 262.60s \\ \hline \end{tabular}
\end{table}
Table 7: Time to take effect for different methods, averaged over different attacksarchitecture. For example, the method in [20] is designed for only k-Nearest Neighbors (kNN) and radius Nearest Neighbors (rNN) models with some feature extractor and cannot be applied to modern deep neural networks.
* Difference in the threat model. Most certified methods focus on the threat model where the training dataset is accessible to defenders while our method focuses on the threat model where only a poisoned model and a few clean samples are given.

Therefore, we believe that our method is more practical and effective for defending against backdoor attacks in deep neural networks than the existing works on certified robustness from the perspective of efficiency, scalability, and model structures.

### Comparison with other AT based methods

Here, we compare the proposed method SAU with other representative Adversarial Training based methods from the following aspects:

* Threat Model: whether the training dataset is accessible and whether the extra clean dataset is accessible.
* Types of Perturbation: whether universal adversarial perturbation is supported.
* Types of Adversarial Attack: whether Targeted Adversarial Attack is supported.

The answer "Yes" and "No" are indicated by \(\surd\) and \(\times\), respectively. The "Optional" means the method supports but is not limited to a specific answer. We summarize the result in Table 8.

We remark that two methods, \(i.e.\), PBE and AFT are proposed in Mu et al. [35] for different threat models, where PBE is designed to filter a clean dataset from a training dataset, and AFT is to perform AT on the given clean dataset. Therefore, **AFT is equivalent to the Vanilla AT in the Figure 1 and Section 4.3.**

As summarized in Table 8, the current AT-based methods can be roughly categorized into two types based on their threat model: in-training (given training dataset) and post-training (given poisoned model and extra clean dataset), and **the difference in threat model make methods from different categories incomparable.**

**1. Comparison with In-training AT-based methods.** Although SAU is not comparable with the in-training AT-based method due to the underlying assumption of the threat model, we would like to adopt different threat models for different methods to highlight the advantages of SAU compared to the in-training AT-based methods. Therefore, **we compare SAU with the latest SOTA in-processing AT-based method, \(i.e.\), Composite AT (CAT) [13].**

We first highlight the following differences between CAT and SAU:

* SAU is theoretically motivated by the relationship between adversarial robustness and backdoor risk, while CAT is empirically driven by some experimental observations. Therefore, our method has a theoretical guarantee for the performance against various backdoor attacks, while CAT does not.
* SAU fine-tunes a backdoored model with a small clean dataset, while CAT trains a backdoor-free model from scratch on a poisoned dataset. Therefore, SAU is more efficient than CAT.

\begin{table}
\begin{tabular}{l|c c c c} \hline \hline Method & Training Dataset & Extra Clean Dataset & UAP & Targeted Attack \\ \hline AT from Scratch [52] & \(\surd\) & \(\times\) & \(\times\) & \(\times\) \\ Composite AT [13] & \(\surd\) & \(\times\) & \(\times\) & \(\times\) \\ i-BAU [59] & \(\times\) & \(\surd\) & \(\surd\) & \(\times\) \\ PBE [35] & \(\surd\) & \(\times\) & \(\times\) & \(\times\) \\ AFT [35] & \(\times\) & \(\surd\) & \(\times\) & \(\times\) \\ SAU (Ours) & \(\times\) & \(\surd\) & Optional & Optional \\ \hline \hline \end{tabular}
\end{table}
Table 8: Comparison between SAU with other AT based methodsThen, we conduct experiments on defending against backdoor attacks and summarize the results in Table 9 and 10.

The experimental results show that our method is superior to CAT, as evidenced by the following aspects:

* SAU achieves significantly higher accuracy (ACC) and lower attack success rate (ASR) than CAT on both poisoning ratios.
* SAU is more robust to different poisoning ratios than CAT. In contrast, CAT fails on a high poisoning ratio (10%) (also observed in Table XI of Gao et al. [13]).

**2. Comparison with Post-training AT-based methods.** The most related methods to SAU are the post-processing AT-based method including I-BAU and AFT, and we highlight the following differences between them:

* I-BAU is proposed under the assumption that the same trigger are used for all poisoned samples which may not be true for more advanced attacks, while SAU is shown to be effective for attacks with multi-trigger and sample-specific trigger theoretically and empirically.
* AFT, which is to perform AT on clean dataset, is motivated by some empirical observation, while SAU is motivated by a novel analysis on the relation between adversarial examples and poisoned samples.

Besides, the experimental comparison between SAU and I-BAU in Section 4.2 and the experimental between SAU and AFT (Vanilla AT on Clean Dataset) in Section 4.3 show that SAU outperforms I-BAU and AFT by a large margin.

### Choice of perturbation and generalization ability of SAU

In this section, we discuss the choice of perturbation set and show that the proposed method, \(i.e.\), SAU, can generalize to attacks with excessive-magnitude triggers.

1. Type of adversarial perturbation (AP) and perturbation set.The theoretical analysis and the proposed method SAU is independent of the AP type and AP set (not only \(L_{p}\) ball). For sample space \(\mathcal{X}\), define the set \(G=\{g(x,\Delta)-x|x\in\mathcal{X}\}\) and the general perturbation set \(S=\{T(x,\epsilon)-x|x\in\mathcal{X}\}\) for adversarial attack \(T\) with learnable parameter \(\epsilon\in\Omega\) such that an adversarial example can be generated by \(\tilde{x}_{\epsilon}=T(x,\epsilon)\). Then, the theoretical analysis and proposed method can be applied if \(G\subseteq S\), where the AP can be any type, such as additive AP, Spatially Transformed AP [56], or Perceptual AP [25], etc.

\begin{table}
\begin{tabular}{c|c c c|c c c|c c c} \hline \hline Method & No Defense & No Defense & No Defense & CAT [13] & CAT [13] & SAT [13] & SAU (**Ours**) & SAU (**Ours**) & SAU (**Ours**) \\ \hline Attack & Acc & ASR & R-ACC & Acc & ASR & R-ACC & Acc & ASR & R-ACC \\ \hline BadNets [15] & 91.32 & 95.03 & 4.67 & 74.42 & 92.49 & 6.21 & **89.31** & **1.53** & **88.81** \\ Input-Aware & 90.67 & 98.26 & 1.66 & 74.21 & 96.81 & 2.88 & **91.59** & **1.27** & **85.54** \\ SSBA [30] & 92.88 & 97.8 & 1.99 & 74.29 & 28.29 & 57.49 & **90.84** & **1.79** & **85.83** \\ WaNet [33] & 91.25 & 89.73 & 9.76 & 74.62 & 4.87 & 73.07 & **90.41** & **2.51** & **79.69** \\ Average & 91.53 & 95.21 & 4.52 & 74.39 & 55.61 & 34.91 & **92.54** & **1.78** & **85.72** \\ \hline \hline \end{tabular}
\end{table}
Table 10: Experiments Results with poisoning ratio 10%

\begin{table}
\begin{tabular}{c|c c c c|c c c|c c c} \hline \hline Method & No Defense & No Defense & No Defense & CAT [13] & CAT [13] & CAT [13] & SAU (**Ours**) & SAU (**Ours**) & SAU (**Ours**) \\ \hline Attack & Acc & ASR & R-ACC & Acc & ASR & R-ACC & Acc & ASR & R-ACC \\ \hline BadNets [15] & 93.14 & 74.73 & 24.24 & 75.52 & 2.50 & 74.73 & **91.25** & **0.94** & **91.16** \\ Input-Aware & 91.74 & 79.18 & 19.89 & 75.24 & 72.48 & 23.97 & **92.20** & **3.63** & **84.06** \\ SSBA [30] & 93.43 & 73.44 & 24.89 & 74.94 & 3.07 & 71.40 & **91.34** & **0.79** & **88.46** \\ WaNet [38] & 90.65 & 12.63 & 79.04 & 75.21 & 2.68 & 74.46 & **91.84** & **1.23** & **89.89** \\ Average & 92.24 & 60.00 & 37.24 & 75.23 & 20.18 & 61.14 & **91.66** & **1.65** & **88.39** \\ \hline \hline \end{tabular}
\end{table}
Table 9: Experiments Results with poisoning ratio 1%2. Effect of perturbation set \(S\).The key of SAU is to consider Shared Adversarial Example (SAE) as surrogates for poisoned samples. Given models, **the set of SAE is determined by \(S\): larger \(S\) leads to larger set of SAE, and vice versa** Therefore, the perturbation set \(S\) plays an important role in the effectiveness of SAU. Specifically, adopting a larger \(S\) can cover unknown triggers but also increases the chance of picking SAE which is irrelevant to poisoned samples, and may weaken the defense performance. Meanwhile, some poisoned samples may be beyond the scope of SAE if a smaller \(S\) is adopted.

3. Generalization to excessive-magnitude trigger.In practice, we find that the proposed method SAU is effective for various attacks, even if Assumption 1 does not hold. For example, SAU with \(L_{\infty}\) norm bound 0.2 can still effectively mitigate SSBA attack whose average \(L_{\infty}\) trigger norm is much larger than 0.2, revealing the generalization of SAU to attacks with excessive-magnitude trigger.

To investigate the SAU's generalization ability, we conduct experiments on BadNets with various strengths. Specifically, for BadNets with a 3x3 patched trigger, we alter the pixel value of the trigger from 0.2 to 1.0.

Table 12 shows the results for defending various BadNets attacks by SAU following the setting in Section 4.2, from which we can find that **SAU can consistently mitigate the backdoor even when the trigger is beyond the perturbation set**. One reasonable explanation is that although the poisoned samples are not included in SAEs, there exists SAE that has a similar effect as poisoned samples as shown in Figure 4, resulting in strong generalization ability of SAU.

4. Effect of perturbation type.To explore the effect of perturbation types, we extend SAU to incorporate two other perturbation types and denote the resulting methods by **Spatial SAU** and **Ensemble SAU**, respectively. Specifically, for spatial SAU, we employ spatial adversarial attack [56] to generate spatial SAEs by spatial transformation, following the same setting as [13]. For Ensemble SAU, the perturbation set \(S\) is composed of a collection of subsets, including \(L_{1}\) ball (bound 500), \(L_{2}\) ball (bound 10), \(L_{\infty}\) ball (bound 0.2), and the spatial AP set. Then, for each batch, Ensemble SAU randomly chooses one subset of perturbation to generate the SAEs. We compare **SAU**, **Spatial SAU** and **Ensemble SAU**, following the setting in Section 4.

\begin{table}
\begin{tabular}{c|c|c c|c c|c c|c c} \hline \hline  & & No Defense & No Defense & ANU [55] & ANU [55] & SAU (**Ours**) & SAU (**Ours**) \\ \hline Pixel & \(L_{\infty}\) & ACC & ASR & ACC & ASR & ACC & ASR \\ \hline
0.2 & 0.42 & 91.69 & 95.60 & 91.30 & 2.64 & 90.01 & **1.38** \\
0.4 & 0.31 & 91.61 & 97.79 & 91.44 & **1.00** & 88.43 & 1.59 \\
0.6 & 0.35 & 91.94 & 96.43 & 91.72 & 7.81 & 90.49 & **1.30** \\
0.8 & 0.48 & 91.72 & 93.20 & 91.06 & 2.37 & 89.23 & **1.36** \\
1.0 & 0.67 & 91.32 & 95.03 & 90.88 & 4.88 & 89.31 & **1.53** \\ \hline \hline \end{tabular}
\end{table}
Table 12: Defend against BadNets [15] Attack with Various Pixel Value

\begin{table}
\begin{tabular}{c|c c c c c} \hline \hline
**Dataset** & \(L_{1}\) & \(L_{2}\) & \(L_{\infty}\) & **ACC** & **ASR** & **R-ACC** \\ \hline CIFAR-10 & 108.33 & 3.29 & 0.33 & 90.99 & 0.58 & 87.04 \\ Tiny & 581.65 & 8.26 & 0.43 & 51.85 & 0.11 & 36.36 \\ \hline \hline \end{tabular}
\end{table}
Table 11: SAU for SSBA Attack

\begin{table}
\begin{tabular}{c|c c c|c c|c c|c c} \hline \hline  & & No Defense & No Defense & SAW & SAU & Spatial SAU & Spatial SAU & Ensemble SAU & Ensemble SAU \\ \hline Dataset & Attack & ACC & ASR & ACC & ASR & ACC & ASR & ACC & ASR \\ \hline CIFAR-10 & BadNets [15] & 91.32 & 95.03 & **89.31** & 1.53 & 88.27 & 0.97 & 88.16 & **0.67** \\ CIFAR-10 & SSAU [30] & 92.88 & 97.86 & **90.84** & 1.79 & 89.65 & 4.43 & 89.92 & **0.81** \\ CIFAR-10 & WatNet [38] & 91.25 & 89.73 & **91.26** & 1.02 & 90.21 & 4.34 & 89.61 & **0.66** \\ \hline Tiny ImageNet & BadNets [15] & 56.23 & 100 & **51.52** & 0.53 & 50.68 & 99.81 & 49.32 & **0.49** \\ Tiny ImageNet & SSBA [30] & 55.22 & 97.71 & **51.85** & 0.11 & 49.79 & 79.72 & 49.73 & **0.05** \\ Tiny ImageNet & WaNet [38] & 56.78 & 54.65 & **54.65** & **58.75** & 50.12 & 0.7 & 51.14 & **0.15** \\ \hline \hline \end{tabular}
\end{table}
Table 13: Defend against BadNets [15] Attack with Various Pixel ValueFrom Table 13, we compare SAU, Spatial SAU and Ensemble SAU, following the setting in Section 4. From the table, we find that Spatial SAU works well on CIFAR-10 dataset but fails to defend BadNets and SSBA attacks on Tiny ImageNet. For WaNet, spatial SAU can effectively defend it on both datasets. We remark that **the Ensemble SAU achieves the lowest ASR in all experiments**. However, as Ensemble SAU has larger perturbation set, its accuracy is lower than SAU by 1 2%.

In summary, our theoretical analysis and the proposed method SAU are flexible to the perturbation set, and SAU can be generalized to attack with excessive trigger norms. Moreover, the Ensemble SAU serves as a stronger variant of SAU and can be effective for various attacks.

### SAU with generated datasets

Like most other post-processing methods [31, 55, 47, 28, 59], SAU assumes that a small clean dataset is available for conducting defense, which can be obtained by ways such as buying from a trustworthy data supplier, using modern generative models, collecting from the internet, or applying some data cleansing techniques.

In this section, we explore a more practical case for defense, \(i.e.\), using the generated dataset, to evaluate our method. Specifically, we conduct experiments on synthetic datasets and compare SAU with NAD. We test both methods with a synthetic dataset CIFAR-5m [36] which provides generated CIFAR-10-like images whose distribution is close but not identical to CIFAR-10, and can be regarded as OOD data. We build mixed datasets by randomly picking samples from CIFAR-10 and CIFAR-5m with a **mixed ratio**, and evaluate both methods on the mixed datasets. A larger mixed ratio indicates more synthetic data in the mixed dataset. The experiment results are summarized in the following tables.

From Table 14, we can find that **all methods can work on the mixed dataset but as the mixed ratio increase, the accuracy (ACC) may decrease due to the distribution difference between the two datasets**. It's notable that the model distillation-based method NAD is influenced by the quality of the dataset most, with a reduction of 4.91% in ACC, while SAU is quite robust to the data quality. Another interesting phenomenon is that the ASR of ANP decreases when more synthetic data is used, which shows that the distribution of data may influence the performance of backdoor defense.

Figure 4: Visualization of Features of Poisoned Sample, Shared Adversarial Example and Non-shared Adversarial Example in latent space for BadNets Attack with PreAct-ResNet18. Both Shared Adversarial Example and Adversarial Example are generated by PGD-5 with \(L_{\infty}\) norm 0.2. The \(L_{2}\) distance between the features of the Poisoned sample and Shared Adversarial Example is 38.75, while the L2 distance between the features of the Poisoned sample and the Non-shared Adversarial Example is 80.55, showing that the Shared Adversarial Example can still be close to the Poisoned Sample in latent space even if the trigger is beyond perturbation set.

All those observations motivate us to conduct research on the relationship between data quality and backdoor defense in the future.

### Resistance to defense-aware/adaptive Attacks

When the attacker is aware of the defender's adversarial training strategy, the attacker can train the backdoored model using Adversarial Training (AT) on the poisoned dataset, such that the model can be resistant to adversarial attack, hindering the generation of shared adversarial examples.

To evaluate the performance of SAU against such defense-aware backdoor attacks, we conduct experiments on the CIFAR-10 dataset following the settings in Section 4.2. For AT on the training dataset, we use PGD-10 attack with \(L_{\infty}\) norm 8/255, which is commonly used in adversarial training literature.

From Table 15, we can find that SAU can achieve an average ACC of 78.3% and ASR of 2.44%, showing its resistance to such adaptive attacks.

## Appendix C Experiment details

In this section, we provide the experiment details. In our experiments, all baselines and settings are adapted from BackdoorBench [54], and we present the experiment details in this section.

### Attack details

* BadNets [15] is one of the earliest works for backdoor learning, which inserts a small patch of fixed pattern to replace some pixels in the image. We use the default setting in BackdoorBench.
* Blended backdoor attack (Blended) [7] uses an alpha-blending strategy to fuse images with fixed patterns. We set \(\alpha=0.2\) as the default in BackdoorBench. We remark that **since such a large \(\alpha\) produces visual-perceptible change to clean samples, the Blended Attack in this setting is very challenging for all defense methods.**
* Input-aware dynamic backdoor attack (Input-Aware)[37] is a training-controllable attack that learns a trigger generator to produce the sample-specific trigger in the training process of the model. We use the default setting in BackdoorBench.
* Low-frequency attack (LF) [58] uses smoothed trigger by filtering high-frequency artifacts from a UAP. We use the default setting in BackdoorBench.

\begin{table}
\begin{tabular}{c|c c|c c} \hline \hline  & No Defense & No Defense & SAU (**Ours**) & SAU (**Ours**) \\ \hline Attack & ACC & ASR & ACC & ASR \\ \hline BadNets [15] & 79.89 & 93.46 & 79.83 & 2.39 \\ Input-Aware & 79.87 & 96.60 & 78.64 & 4.84 \\ SIG [2] & 73.50 & 99.50 & 76.41 & 0.10 \\ Average & 77.75 & 96.52 & 78.30 & 2.44 \\ \hline \hline \end{tabular}
\end{table}
Table 15: Experiments Results for Defense-aware Attack

\begin{table}
\begin{tabular}{c|c c|c c|c c} \hline \hline  & ANP [55] & ANP [55] & NAD [28] & NAD [28] & SAU (**Ours**) & SAU (**Ours**) \\ \hline Mixed Ratio & ACC & ASR & ACC & ASR & ACC & ASR \\ \hline
0.0 & **90.88** & 4.88 & 89.87 & 2.14 & 89.31 & **1.53** \\
0.2 & **90.68** & 2.10 & 87.96 & 1.88 & 89.83 & **1.63** \\
0.4 & **90.21** & **1.11** & 87.69 & 1.88 & 89.36 & 1.60 \\
0.6 & **90.31** & **1.51** & 86.69 & 2.19 & 88.44 & 1.66 \\
0.8 & **89.37** & **0.56** & 85.90 & 1.86 & 88.13 & 1.79 \\
1.0 & **88.74** & **0.40** & 84.96 & 2.48 & 88.32 & 1.01 \\ \hline \hline \end{tabular}
\end{table}
Table 14: Results on Generated Dataset* Sinusoidal signal backdoor attack (SIG) [2] is a clean-label attack that uses a sinusoidal signal as the trigger to perturb the clean images in the target label. We use the default setting in BackdoorBench.
* Sample-specific backdoor attack (SSBA) [30] uses an auto-encoder to fuse a trigger into clean samples and generate poisoned samples. We use the default setting in BackdoorBench.
* Warping-based poisoned networks (WaNet) [38] is also a training-controllable attack that uses a warping function to perturb the clean samples to construct the poisoned samples. We use the default setting in BackdoorBench.

### Defense details

The details for each defense used in our experiment/discussion are summarized below:

* ANP [55] is a pruning-based method that prunes neurons sensitive to weight perturbation. Note that in BackdoorBench, ANP can select its pruning threshold by grid searching on the test dataset (default way in BackdoorBench) or a given constant threshold. To produce a fair comparison with other baselines, we use a constant threshold for ANP and set the pruning threshold to \(0.4\), which is found to produce better results than the recommended constant threshold of \(0.2\) in BackdoorBench. All other settings are the same as the default setting in BackdoorBench.
* FP [31] is a pruning-based method that prunes neurons according to their activations and then fine-tunes the model to keep clean accuracy. We use the default setting in BackdoorBench.
* NAD [28] uses Attention Distillation to mitigate backdoors. We use the default setting in BackdoorBench.
* NC [47] first optimizes a possible trigger to detect whether the model is backdoored. Then, if the model is detected as a backdoored model, it mitigates the backdoor by unlearning the optimized trigger. We use the default setting in BackdoorBench. We remark that **if the model is detected as a clean model, NC just returns the model without any changes.**
* EP [64] uses the distribution of activation to detect the backdoor neurons and prunes them to mitigate the backdoor. We use the default setting in BackdoorBench.
* i-BAU [59] uses adversarial training with UAP and hyper-gradient to mitigate the backdoor. We use the default setting in BackdoorBench.
* SAU (Ours) uses PGD to generate shared adversarial examples and unlearns them to mitigate the backdoor. For all experiments, we set \(\lambda_{1}=\lambda_{2}=\lambda_{4}=1\) and \(\lambda_{3}=0.01\) in \((\ref{eq:1})\). Then, we run PGD 5 steps with \(L_{\infty}\) norm bound \(0.2\) and a large step size \(0.2\) to accelerate the inner maximization in (8). For unlearning step (fine-tuning on \(\mathcal{D}_{cl}\) and SAEs), we use the same setting as i-BAU and fine-tuning (FT) in BackdoorBench. We run SAU 100 epochs in CIFAR-10 and GTSRB. In Tiny ImageNet, we run SAU 20 epochsNote that the number of epochs used in our experiment largely exceeds the necessary epochs for SAU to take effect and can be further reduced to save computational costs.
* Vanilla Adversarial Training (vanilla AT) [33] use PGD to generate vanilla adversarial examples and unlearn them to mitigate the backdoor. In all experiments, vanilla AT uses the same setting as SAU, including the step size, norm bound, optimizer, and learning rate, except for the experiment in Figure 1, where we train vanilla AT 100 epochs on Blended Attack with PreAct-ResNet18 and Tiny ImageNet to show its long-term performance.
* Adversarial Fine-tuning (AFT) [35] performs adversarial training on clean dataset, which is equivalent to vanilla AT.
* Composite Adversarial Training (Composite AT, CAT) [13] use perturbation from different types to defend against backdoor attacks. We request the official implementation of CAT from the authors and use the recommended setting in the original paper.
* Progressive Backdoor Erasing (PBE) [35] uses the adversarial attack to progressively filter the clean dataset from the training dataset and remove the backdoor. **Since the code haven't been released yet and the details of some critical parameters such as the configuration of adversarial attacks, the data filtering ratio in each step, and the learning rates are not covered in the paper, we failed to produce satisfactory results.** Besides, the threat model of PBE is different from ours which makes it incomparable to SAU, and therefore, we do not include it in the experiment section.

Adaptation to batch normalization.Batch normalization (BN) has been widely used in modern deep neural networks due to improved convergence. However, recent works show that it is difficult for BN to estimate the correct normalization statistics of a mixture of distributions of adversarial examples and clean examples [57, 48]. Such a problem is magnified when we adversarially fine-tune a model with a small set of samples and may destroy the generalization ability of the model due to biased BN statistics. To address this problem, we propose **the fixed BN strategy** in the adversarial unlearning/training process. Note that in all experiments, the fixed BN strategy is applied to i-BAU, SAU, and vanilla AT when BN is used in the model architecture like PreAct-ResNet18.

Experiments on VGG19.For VGG19 [41], BN is not used. Therefore, the fixed BN strategy is not applied. Moreover, some methods that depend on BN, such as ANP and EP, are not applicable to VGG19.

## Appendix D Additional experiments

In this section, we provide additional experiments.

### Main experiments on VGG19

This section provides additional experiment results on VGG19 with CIFAR-10 (Table 16) and Tiny ImageNet (Table 17) to supplement Section 4. Table 16 shows that SAU outperforms all baselines on CIFAR-10 and VGG19, with a 40.56% decrease of ASR compared to the second lowest ASR. Moreover, SAU achieves the highest DER in 6 of 7 attacks, demonstrating a significantly better tradeoff between accuracy and ASR. It also achieves the top-2 R-ACC in 5 of 7 attacks and the best average R-ACC, indicating its good ability to recover the prediction of poisoned samples. Table 17 shows that SAU achieves the best DER in 5 of 6 attacks and the top-2 lowest ASR in 5 of 6 attacks. Although SAU only achieves the second-best average ASR in Table 17, it has significantly higher accuracy than NC, which achieves the best average ASR.

### Main experiments on GTSRB

This section provides additional experiment results on GTSRB with PreAct-ResNet18 (Table 18) and VGG19 (Table 19) to supplement Section 4. In both tables, SAU achieves the top-2 lowest ASR in 4 of 6 attacks and the lowest ASR on average. Furthermore, SAU achieves the top-2 highest DER in 4 of 6 attacks for PreAct-ResNet18 (Table 18) and 5 of 6 attacks for VGG19 (Table 19).

\begin{table}
\begin{tabular}{c|c c c|c c c c|c c c c} \hline \hline Defense & \multicolumn{3}{c|}{No Defense} & \multicolumn{3}{c|}{FP [31]} & \multicolumn{3}{c}{NC [47]} \\ \hline Attack & ACC & ASR & R-ACC & DER & ACC & ASR & R-ACC & DER & ACC & ASR & R-ACC & DER \\ \hline BadNets [15] & \(89.36\) & \(95.93\) & \(8.31\) & N/A & **89.23** & 92.61 & 6.82 & 51.6 & 82.86 & 1.00 & **88.01** & **96.72** \\ Blended [7] & \(90.17\) & \(99.12\) & \(9.02\) & N/A & **90.07** & 99.11 & 0.82 & 49.96 & 85.92 & 1.79 & **74.13** & 96.54 \\ Input-Aware [37] & \(77.69\) & \(94.59\) & \(4.79\) & N/A & **78.62** & 86.77 & 11.79 & 53.91 & 17.62 & 94.58 & 4.79 & 50.00 \\ LF [58] & \(88.94\) & \(93.93\) & \(5.62\) & N/A & **88.98** & 91.8 & 7.46 & 51.07 & 85.35 & 9.99 & **72.79** & 90.18 \\ SIG [2] & \(81.69\) & \(99.8\) & \(0.12\) & N/A & 84.52 & 99.93 & 0.07 & 50.0 & 81.69 & 95.80 & 0.12 & 50.00 \\ SSA [30] & \(89.48\) & \(91.86\) & \(7.29\) & N/A & 89.01 & 89.66 & 9.22 & 51.06 & **89.48** & 91.86 & 7.29 & 50.00 \\ WawNet [38] & \(88.43\) & \(88.9\) & \(10.3\) & N/A & 89.61 & 73.39 & 24.57 & 57.76 & 88.43 & 88.89 & 10.30 & 50.01 \\ Average & \(86.54\) & \(94.88\) & \(4.68\) & N/A & **87.20** & 90.47 & 8.68 & 52.19 & 85.20 & 55.42 & 36.78 & 69.06 \\ \hline \hline \end{tabular} 
\begin{tabular}{c|c c c|c c c|c c c|c c c} \hline \hline  & \multicolumn{3}{c|}{**NAU [28]**} & \multicolumn{3}{c}{**SAU (Ours)**} \\ \hline Attack & ACC & ASR & R-ACC & DER & ACC & ASR & R-ACC & DER & ACC & ASR & R-ACC & DER \\ \hline BadNets [15] & \(87.51\) & \(88.75\) & \(83.0\) & \(77.96\) & \(87.82\) & \(25.72\) & \(83.24\) & \(84.34\) & \(86.71\) & **0.08** & \(32.77\) & \(96.60\) \\ Blended [7] & \(88.35\) & \(93.08\) & \(6.33\) & \(52.11\) & \(88.61\) & \(98.36\) & \(32.31\) & 68.85 & 87.01 & **1.32** & \(23.74\) & **97.32** \\ Input-Aware [37] & \(75.70\) & \(23.36\) & **54.71** & 84.62 & 72.15 & 26.22 & 42.51 & 81.41 & 75.11 & **12.49** & \(46.19\) & **89.76** \\ IET [58] & \(87.85\) & \(95.93\) & \(50.61\) & \(67.87\) & \(97.20\) & 10.84 & 56.93 & 86.65 & **64.32** & \(75.21\) & **92.60** \\ SIG [2] & \(\bm{86.01}\) & \(99.18\) & \(0.77\) & \(50.31\) & \(85.06\) & \(90.89\) & \(52.7\) & 54.46 & 84.87 & **0.11** & **10.82** & **99.84** \\ SSA [30] & \(87.65\) & \(37.54\) & \(54.56\) & \(72.84\) & \(80.88\) & \(37.47\) & \(76.98\) & \(93.36\) & 87.1 & **1.66** & **78.70** & **94.01** \\ WawNet [38] & \(\bm{90.82}\) & \(44.93\) & \(51.18\) & \(71.98\) & \(90.21\) & \(25.83\) & 67.87 & 81.53 & 87.07 & **53.2** & **83.37** & **91.11** \\ Average & \(86.27\) & \(56.52\) & \(37.27\) & \(68.56\) & \(58.71\) & \(44.88\) & \(43.75\) & \(74.41\) & 84.96 & **3.92** & **49.73** & **94.46** \\ \hline \hline \end{tabular}
\end{table}
Table 16: Results on CIFAR-10 with VGG19 and poisoning ratio \(10\%\).

[MISSING_PAGE_FAIL:29]

[MISSING_PAGE_FAIL:30]

### Experiments on different numbers of clean samples

This section examines the influence of clean sample size for SAU by evaluating SAU with different numbers of clean samples. The experiment is conducted on CIFAR-10 with PreAct-ResNet18 and the results are summarized in Table 23. Table 23 shows that SAU can consistently mitigate backdoors with sample sizes ranging from 2500 (5%) to 500 (1%) with high accuracy. When the sample size decreases to 50 (0.1%, 5 samples per class) or 10 (0.02%, 1 sample per class), SAU can still reduce

\begin{table}
\begin{tabular}{c|c c c c|c c c c|c c c c|c c c} \hline \hline Defense & \multicolumn{3}{c|}{No Defense} & \multicolumn{3}{c|}{Config 1 (**Default**)} & \multicolumn{3}{c}{Config 2} \\ \hline Attack & ACC & ASR & R-ACC & DER & ACC & ASR & R-ACC & DER & ACC & ASR & R-ACC & DER & ACC & ASR & R-ACC & DER \\ \hline BadNets [15] & \(91.32\) & \(95.03\) & \(4.67\) & N/A & \(89.31\) & \(1.53\) & \(88.81\) & \(95.74\) & \(89.56\) & **0.80** & **89.60** & **96.24** \\ Blended [7] & \(93.47\) & \(99.92\) & \(0.08\) & N/A & \(90.96\) & \(6.14\) & **64.89** & \(95.63\) & \(91.05\) & \(6.06\) & \(61.11\) & \(95.72\) \\ Input-Aware & \(90.67\) & \(98.26\) & \(1.66\) & N/A & \(\mathbf{91.59}\) & \(1.27\) & **88.54** & \(\mathbf{98.94}\) & \(90.93\) & \(1.56\) & \(84.76\) & \(98.35\) \\ LF [58] & \(93.19\) & \(99.28\) & \(7.11\) & N/A & \(90.32\) & \(4.18\) & **81.54** & \(96.12\) & \(90.83\) & \(3.76\) & \(77.93\) & \(96.58\) \\ SIG [2] & \(84.48\) & \(98.27\) & \(1.72\) & N/A & \(\mathbf{88.56}\) & \(1.67\) & **57.96** & \(93.80\) & \(88.29\) & \(9.94\) & \(94.91\) & \(98.66\) \\ SSA [30] & \(92.88\) & \(97.86\) & \(1.98\) & **N/A** & \(\mathbf{90.89}\) & \(\mathbf{89.41}\) & \(7.98\) & **85.39** & \(\mathbf{97.01}\) & \(89.13\) & \(68.02\) & \(96.58\) \\ WaNet [38] & \(91.25\) & \(89.73\) & \(9.76\) & N/A & \(\mathbf{91.26}\) & **1.02** & **90.28** & **94.36** & \(86.22\) & \(3.12\) & \(86.89\) & \(90.79\) \\ Average & \(91.04\) & \(96.91\) & \(2.94\) & N/A & \(\mathbf{90.41}\) & \(2.51\) & **79.69** & **96.52** & \(89.53\) & \(2.55\) & \(75.71\) & \(96.13\) \\ \hline \hline Defense & \multicolumn{3}{c|}{Config 3} & \multicolumn{3}{c|}{Config 4} & \multicolumn{3}{c}{Config 5} \\ \hline Attack & ACC & ASR & R-ACC & DER & ACC & ASR & R-ACC & DER & ACC & ASR & R-ACC & DER \\ \hline BadNets [15] & \(96.99\) & \(2.21\) & \(98.19\) & \(95.60\) & \(89.78\) & \(1.11\) & \(89.38\) & \(96.16\) & \(98.90\) & \(89.04\) & \(1.29\) & \(88.63\) & \(95.73\) \\ Blended [7] & \(90.98\) & \(6.12\) & \(61.64\) & \(95.65\) & **91.58** & \(3.61\) & \(59.09\) & \(94.86\) & \(91.49\) & \(5.83\) & \(63.04\) & \(96.45\) \\ Input-Aware & \(90.75\) & \(1.86\) & \(86.21\) & \(98.20\) & \(91.30\) & **0.94** & \(84.24\) & **98.66** & \(91.21\) & \(1.20\) & \(86.60\) & \(98.53\) \\ LF [58] & \(91.05\) & \(3.00\) & \(31.23\) & **97.07** & \(87.06\) & **1.94** & \(73.72\) & \(95.60\) & \(91.29\) & \(5.12\) & \(80.26\) & \(96.13\) \\ SIG [2] & \(87.18\) & \(1.32\) & \(55.12\) & \(98.47\) & \(88.20\) & **0.78** & \(\mathbf{90.99}\) & **87.46** & \(86.69\) & \(2.08\) & \(55.64\) & \(80.89\) \\ SSBA [30] & \(90.39\) & \(2.68\) & \(82.04\) & \(96.34\) & \(90.65\) & \(2.59\) & \(84.40\) & \(96.52\) & \(90.48\) & \(1.62\) & \(84.19\) & \(96.92\) \\ WaNet [38] & \(91.44\) & \(1.60\) & \(87.96\) & \(94.01\) & \(90.79\) & \(1.13\) & \(80.70\) & \(94.07\) & \(90.55\) & \(1.82\) & \(89.11\) & \(93.61\) \\ Average & \(90.17\) & \(2.68\) & \(77.63\) & \(96.48\) & \(89.91\) & **2.40** & \(75.66\) & \(96.38\) & \(90.11\) & \(2.71\) & \(78.21\) & \(96.44\) \\ \hline \hline \end{tabular}
\end{table}
Table 21: Results for Tiny ImageNet with PreAct-ResNet18 and poisoning ratio \(5\%\).

Figure 5: Learn curves for Accuracy, Backdoor Risk, Adversarial Risk, and Shared Adversarial Risk.

\begin{table}
\begin{tabular}{c|c c c c|c c c c|c c c c|c c c} \hline \hline Defense & \multicolumn{3}{c|}{No Defense} & \multicolumn{3}{c|}{Config 1 (**Default**)} & \multicolumn{3}{c}{Config 2} \\ \hline Attack & ACC & ASR & R-ACC & DER & ACC & ASR & R-ACC & DER & ACC & ASR & R-ACC & DER & ACC & ASR & R-ACC & DER \\ \hline BadNets [15] & \(96.36\) & \(99.86\) & \(0.13\) & N/A & \(44.54\) & **0.02** & 43.80 & 94.01 & 53.00 & 96.83 & 2.86 & 49.83 & 52.97 & 65.94 & 2.45 & 65.26 \\ Blended [7] & \(56.69\) & \(99.85\) & \(0.94\) & N/A & \(45.19\) & \(92.89\) & \(3.67\) & \(47.18\) & \(52.29\) & \(94.47\) & \(3.21\) & 50.28 & **53.63** & 96.52 & 2.21 & 49.59 \\ Input-Aware [37] & \(87.87\) & \(98.25\) & \(1.51\) & N/A & \(98.38\) & \(0.17\) & \(8.84\) & \(91.80\) & \(62.87\) & \(8.62\) & \(53.39\) & \(56.48\) & \(40.9\) & **53.17** & 97.88 \\ LF [58] & \(66.67\) & \(95.99\) & \(2.75\) & N/A & \(41.26\) & \(84.88\) & \(\mathbf{1.

the backdoor to a low ASR. However, clean accuracy is difficult to guarantee with such limited clean samples.

### Experiments on Multi-trigger Attacks

In this section, we evaluate our method for the multi-trigger/multi-target cases. Specifically, we use two different attacks (denoted by Attack-1 and Attack-2) with different target labels. Following Section 4.2, we test our method (SAU) against a strong baseline ANP on the CIFAR-10 dataset with a poisoning ratio of \(10\%\) and backbone PreAct-ResNet18. The experiment results are summarized in the following table, where we use ASR-1 and ASR-2 to represent the Attack Success Rate for Attack-1 and Attack-2, respectively. From Table 24, we can find that SAU achieves much higher accuracy (\(5.64\%\) higher) and a much lower ASR for Attack-1 (\(26.46\%\) lower) compared to ANP, although the average ASR for Attack-2 of ANP is slightly lower than that of SAU (only \(0.13\%\) lower). These results show that SAU outperforms the baseline in most cases, which demonstrates the effectiveness and robustness of our method in this challenging scenario.

### Experiments on ALL to ALL attack

In this section, we compare SAU with other baselines on ALL to ALL attacks on CIFAR-10 with PreAct-ResNet18 and poisoning ratio \(10\%\). Specifically, the target labels for the sample with original labels \(y\) are set to \(y_{t}=(y+1)\mod K\) where \(\mod\) is short for "modulus". The experiment results are summarized in Table 25. From Table 25, we can find that SAU achieves the best defending performance in 5 of 7 attacks and the lowest average ASR. At the same time, SAU also achieves the best average R-ACC and average DER, which further demonstrates its effectiveness in defending against backdoor attacks with multiple targets.

Note that the ASR for the poisoned model is significantly lower than the ASR in the single-target case. Thus, Table 25 also shows that SAU can still effectively mitigate backdoor even the ASR of the poisoned model is much lower than \(100\%\).

### Comparison to Reconstructive Neuron Pruning (RNP)

In this section, we compare SAU with RNP [29] to further improve our submission. We adopted the official implementation of RNP (https://github.com/bboylys/RNP) into the framework of Backdoor-Bench for fair comparison and the specific experimental settings are:

* **Dataset and model architecture**: we compare our SAU with RNP on CIFAR-10 dataset with ResNet18 and PreAct-ResNet18.
* **Clean data size and poisoning ratio**: We adopt clean datasets: 1% and 5%, and poisoning ratios: 0.5%, and 10% to compare their effectiveness for large and small poisoning ratios.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c} \hline \hline Defense & No Defense & SAU -2500 & SAU -1000 & SAU -500 & SAU -50 & SAU -10 \\ \hline Attack & ACC & ASR & ACC & ASR & ACC & ASR & ACC & ASR & ACC & ASR \\ \hline BadNets [15] & \(91.32\) & \(95.03\) & \(89.31\) & \(1.53\) & \(89.24\) & \(1.31\) & \(83.04\) & \(2.17\) & \(67.86\) & \(1.41\) & \(51.43\) & \(14.38\) \\ \hline Blended [7] & \(93.47\) & \(99.92\) & \(90.96\) & \(6.14\) & \(88.82\) & \(2.53\) & \(87.71\) & \(5.41\) & \(73.95\) & \(11.62\) & \(43.54\) & \(0.00\) \\ \hline LF [58] & \(93.19\) & \(99.28\) & \(90.32\) & \(4.18\) & \(88.00\) & \(6.79\) & \(88.69\) & \(2.84\) & \(66.69\) & \(0.30\) & \(54.95\) & \(12.62\) \\ \hline \hline \end{tabular}
\end{table}
Table 23: Results for SAU with different numbers of clean samples

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c} \hline \hline Defense & No Defense & SAU -2500 & SAU -1000 & SAU -500 & SAU -50 & SAU -10 \\ \hline Attack-1 & Attack-2 & ACC & ASR-1 & ASR-2 & ACC & ASR-1 & ASR-2 & ACC & ASR-1 & ASR-2 \\ \hline Blended [7] & BadNets [15] & \(90.22\) & \(99.27\) & \(95.03\) & \(85.55\) & \(37.07\) & \(\bm{0.01}\) & \(\bm{89.3}\) & \(\bm{2.21}\) & \(0.54\) \\ InputAware & BadNets [15] & \(89.71\) & \(78.62\) & \(94.77\) & \(82.21\) & \(4.48\) & \(\bm{0.01}\) & \(\bm{88.88}\) & \(\bm{4.14}\) & \(0.70\) \\ LF [58] & BadNets [15] & \(90.16\) & \(98.08\) & \(95.15\) & \(84.21\) & \(79.11\) & \(2.30\) & \(\bm{88.45}\) & \(\bm{2.93}\) & \(\bm{0.34}\) \\ SIGG [2] & BadNets [15] & \(82.12\) & \(98.48\) & \(95.03\) & \(76.43\) & \(26.92\) & \(\bm{0.14}\) & \(\bm{85.2}\) & \(\bm{0.60}\) & \(0.67\) \\ SSBA [30] & BadNets [15] & \(89.69\) & \(95.13\) & \(95.11\) & \(83.25\) & \(20.03\) & \(\bm{0.14}\) & \(\bm{87.60}\) & \(\bm{1.38}\) & \(0.52\) \\ WaNet [38] & BadNets [15] & \(89.44\) & \(90.21\) & \(95.40\) & \(82.21\) & \(5.53\) & \(\bm{0.01}\) & \(\bm{88.02}\) & \(\bm{3.12}\) & \(0.67\) \\ \hline Average & & \(88.56\) & \(93.30\) & \(95.08\) & \(82.31\) & \(28.86\) & \(\bm{0.44}\) & \(\bm{87.95}\) & \(\bm{2.40}\) & \(0.57\) \\ \hline \hline \end{tabular}
\end{table}
Table 24: Results for defending against Multi-trigger Attacks

[MISSING_PAGE_FAIL:33]

\begin{table}
\begin{tabular}{c|c c|c c|c c|c c} \hline \hline Attack & Clean Ratio & No Defense & No Defense & RNP [29] & RNP [29] & SAU (**Ours**) & SAU (**Ours**) \\ \hline  & & ACC & ASR & ACC & ASR & ACC & ASR \\ \hline BadNets [15] & 1\% & 91.32 & 95.03 & 66.86 & 0.00 & 85.12 & 2.52 \\ Input-Aware & 1\% & 90.67 & 98.26 & 79.88 & 0.00 & 88.56 & 2.79 \\ SIG [2] & 1\% & 84.48 & 98.27 & 74.47 & 0.04 & 84.64 & 0.64 \\ \hline BadNets [15] & 5\% & 91.32 & 95.03 & 10.00 & 0.00 & 89.31 & 1.53 \\ Input-Aware & 5\% & 90.67 & 98.26 & 10.00 & 0.00 & 91.59 & 1.27 \\ SIG [2] & 5\% & 84.48 & 98.27 & 11.56 & 0.00 & 88.56 & 1.67 \\ \hline \hline \end{tabular}
\end{table}
Table 27: Result on PreAct-ResNet-18 with Poisoning Ratio 10%

\begin{table}
\begin{tabular}{c|c c|c c|c c} \hline \hline  & No Defense & No Defense & RNP [29] & RNP [29] & SAU (**Ours**) & SAU (**Ours**) \\ \hline Attack & ACC & ASR & ACC & ASR & ACC & ASR \\ \hline BadNets [15] & 93.73 & 67.06 & 91.46 & 11.54 & 82.63 & 2.46 \\ Input-Aware & 90.78 & 97.68 & 87.28 & 67.74 & 87.77 & 4.60 \\ SIG [2] & 93.68 & 85.02 & 79.91 & 14.76 & 87.71 & 0.14 \\ \hline \hline \end{tabular}
\end{table}
Table 28: Result on ResNet-18 with Poisoning Ratio 0.5%