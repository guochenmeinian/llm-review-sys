# Extracting Nonlinear Symmetries

From Trained Neural Networks on Dynamics Data

Yoh-ichi Mototake

Graduate School of Social Data Science

Hitotsubashi University

Tokyo, 186-8601

y.mototake@r.hit-u.ac.jp

https://mototakelab.github.io/mototake.github.io

###### Abstract

To support scientists who are developing the reduced model of complex physics systems, we propose a method for extracting interpretable physics information from a deep neural network (DNN) trained on time series data of a physics system. Specifically, we propose a framework for estimating the hidden nonlinear symmetries of a system from a DNN trained on time series data that can be regarded as a finite-degree-of-freedom classical Hamiltonian dynamical system. Our proposed framework can estimate the nonlinear symmetries corresponding to the Laplace-Lunge-Renz vector, a conservation value that keeps the long-axis direction of the elliptical motion of a planet constant, and visualize its Lie manifold.

## 1 Introduction

One of the central roles in scientific activities is understanding large-scale complex systems through their reduced models. Some complex systems are modeled as low-dimensional canonical dynamical systems. For instance, reduced models have been developed for large-scale collective motion systems, which are a type of large-scale complex system with order, such as plasma, acoustic waves, or vortex systems [1, 2, 3, 4, 5]. To develop these reduced models, collective coordinates have been introduced, such as the Fourier basis of a density or charge distribution [1, 2, 3, 4], or a vortex feature space [5]. Then, a Hamiltonian that describes the coarse-grained properties of a dynamical system is derived. Thus, to develop a reduced model, it is necessary to introduce collective coordinates and derive the Hamiltonian in those coordinates. The obtained Hamiltonian is then verified by confirming that it can reconstruct the properties of the phenomena analyzed. This approach relies heavily on the physical insights of physicists and may not work for modeling a dynamical system that features a more complicated structure. One example is the collective motion of living things such as fish or birds; such systems frequently have stable but very complicated patterns in a metastable state [6, 7].

The problem we are considering here is how to infer a reduced model using machine learning methods. As mentioned earlier, this involves solving two problems: estimating a coordinate system and constructing a reduced model within that coordinate system. One way to solve these problems is to construct a Hamiltonian based on a given coordinate system and search for a coordinate system that improves the model. Several machine learning methods have been developed for inferring the Hamiltonian from a time-series dataset [8, 9, 10, 11]. These methods can be roughly divided into two types. In the first type, the Hamiltonian is inferred by regressing the data with an explicit function, such as the linear sum of multiple basis functions [8]. However, when inferring a reduced model that consists of complicated unknown basis functions, this method only infers an approximated reduced model using an approximated function, such as a polynomial function. In the second type, a Hamiltonian is modeled using deep learning techniques [9, 10, 11]. In this case, an explicit functionused in the first type is not required. Based on these machine learning methods, the search for the coordinate system could be performed using statistical criteria such as the prediction error.

There are inherent difficulties in building a reduced model using a machine learning approach. Such an approach finds a Hamiltonian that has properties that only hold for the given data. Historically, physicists have achieved great success in constructing reduced models by abstracting knowledge obtained from observational data and building universal models that can explain various physical phenomena, not just the given data. For example, in thermodynamics, Gibbs linked a reduced model that describes the molecular motion of a gas to chemical reaction theory [12, 13]. This is one of the most successful uses of a reduced model. In other words, a good reduced model and a good coordinate system mean that the performance is high not only for the given data.

To achieve a successful reduced model, it is important to interpret the knowledge obtained during data analysis and develop a model that can be applied to different phenomena by combining explicit and implicit knowledge of physics. In general, an inferred Hamiltonian modeled by deep neural networks (DNNs) is difficult to interpret because DNNs are models with enormous degrees of freedom. If all physical knowledge could be quantified, it would be possible to construct a reduced model with a DNN, but this is currently an impractical assumption. Therefore, it is difficult for a machine learning approach to achieve the same function as a physicist, who can flexibly interpret phenomena by utilizing explicit or implicit physical knowledge and construct a reduced model.

To overcome this problem, it is usefull to employ methods to extract symmetries of the dynamics system directly from physical data without constructing a reduced model [14, 15, 16, 17, 18, 19, 18, 20, 21]. These methods are derived from Noether's theorem [22], which connects the symmetry of the Hamiltonian and the conservation law. For example, as the study most relevant to this study, Liu et al. have been proposed using deep neuralnetworks and symbolic regression [18], and they have achieved quantitative estimation of complex conservation laws as interpretable form of functions. To infer the conservation laws, it is only needed the tangent space of the manifold of the continuous transformation group that corresponds to the symmetry of the system. Therefore, unlike Hamiltonian estimation, conservation law estimation only requires manifold modeling with at most first-order accuracy. This means that the conservation law can be inferred with arbitrary precision by polynomial approximation. A coordinate system can then be selected based on the system's symmetries on the coordinate system. Furthermore, the obtained symmetries information can also help physicists construct a reduced model.

The purpose of this study is to verify whether nonlinear symmetry can be estimated by the method of Mototake et al [19]. They develop a method for inferring the symmetry of a data manifold modeled by a deep autoencoder [23] and determine the conservation laws of the system. This method allows direct visualization of the symmetries captured by the Auto Encoder through sampling. Although the method of Liu et al. [18] can also estimate the conservation laws as interpretable forms of functions corresponding to nonlinear symmetries, the visualization of symmetries should allow the scientists to work their insight from other viewpoints. Such a property of the method is expected to be useful for extracting complex conservation laws corresponding to nonlinear symmetries in an interpretable form to scientists. The method is also capable, in principle, of estimating complex symmetries, such as invariance of the system to non-linear transformations, but no such symmetry estimation was actually carried out in the study [19]. The purpose of this study is to verify whether the method can estimate the symmetries corresponding to non-linear transformations and to propose modifications to the estimation framework needed to do it. Specifically, we attempt to estimate non-linear transformations corresponding to the conservation law of Runge-Lenz vector present in central force systems obeying the inverse square law.

This paper is organized as follows. In Sec. 2, we show the relationship between the symmetry of the time-series dataset distribution and the conservation law using Noether's theorem according to Mototake's paper [19]. In Sec. 3, we describe the proposed procedure of inferring the non-linear symmetry of the time-series data manifold based on the employed methods [19]. In Sec. 4, to confirm the effectiveness of the proposed methods, we apply them to the system conserving the Runge-Lenz vector in a central force system. In Sec. 5, we present a summary and discussion.

Theory

### Noether's theorem

Noether's theorem establishes a deep connection between the continuous symmetries of a Hamiltonian system and the conservation laws that govern it [22]. It is often described in the \((2d+1)\)-dimensional extended phase space \(\Gamma\times\mathbb{R}\), \((\bm{q},\bm{p}):=(q_{0}=t,q_{1},\cdots,q_{d},p_{1},\cdots,p_{d})\). The Noether's theorem can also be described in the \((2d+2)\)-dimensional space \(\Gamma\times\mathbb{R}\times\mathbb{R}\), \((q_{0}=t,q_{1},\cdots,q_{d},p_{0}=-H,p_{1},\cdots,p_{d})\). In this study, we describe the Noether's theory in the \((2d+2)\)-dimensional space as follows. Hamiltonian systems in the \((2d+2)\)-dimensional space \(\Gamma\times\mathbb{R}\times\mathbb{R}\) are considered, and restrict ourselves to the case where the system's Hamiltonian belongs to a \(C^{2}\) class function \(H(\bm{q},\bm{p})\). The Hamiltonian representation of Noether's theorem is described as follows [24]. Assume that \(H(\bm{q},\bm{p})\) and the canonical equations of motion \(\frac{\partial H(\bm{q},\bm{p})}{\partial q_{i}}=-\dot{p}_{i}\) and \(\frac{\partial H(\bm{q},\bm{p})}{\partial p_{i}}=\dot{q}_{i}\) are invariant under the infinitesimal transformation \((q_{i}^{\prime},p_{i}^{\prime})=(q_{i}+\delta q_{ij},p_{i}+\delta p_{ij})\), where \(i=1,\ldots,d\), and \(j\) is the index of the direction of the infinitesimal transformation corresponding to a conservation law. Then, on the basis of Noether's theorem, the conserved value \(G_{j}\) satisfies the following equation: \((\delta q_{ij},\delta p_{ij})=\left(\frac{\partial G_{j}}{\partial p_{i}},- \frac{\partial G_{j}}{\partial q_{j}}\right)\). The canonical transformation that makes the Hamiltonian system invariant is given as

\[\mathfrak{c}_{\text{inv}}(\bm{\theta}):\;\Gamma\times\mathbb{R} \times\mathbb{R} \longrightarrow \Gamma\times\mathbb{R}\times\mathbb{R},\] (1) \[(\bm{q},\bm{p}) \longmapsto (\bm{Q},\bm{\mathcal{P}}):=(\bm{Q}(\bm{q},\bm{p},\bm{\theta}), \bm{\mathcal{P}}(\bm{q},\bm{p},\bm{\theta})),\] (2)

where \(\bm{Q}(\bm{q},\bm{p},\bm{\theta})\) and \(\bm{\mathcal{P}}(\bm{q},\bm{p},\bm{\theta})\) represent the invariant transformation functions of coordinate \((\bm{q},\bm{p})\) to \((\bm{Q},\bm{\mathcal{P}})\), and \(\bm{\theta}\) represents a \(d_{0}\)-dimensional continuous parameter characterizing transformation that satisfies \(\bm{Q}\left(\bm{q},\bm{p},\bm{\theta}=\vec{0}\right)=\bm{q}\), and \(\bm{\mathcal{P}}\left(\bm{q},\bm{p},\bm{\theta}=\vec{0}\right)=\bm{p}\). In this paper, this transformation is called an invariant transformation. A set of the invariant transformations characterized by the continuous parameters \(\bm{\theta}\) forms a Lie group. By the first-order Taylor expansion of \(\bm{Q}_{i}(\bm{q},\bm{p},\bm{\theta})\) and \(\bm{\mathcal{P}}_{i}(\bm{q},\bm{p},\bm{\theta})\) around \(\bm{\theta}=\vec{0}\), we have the infinitesimal transformation, \((\delta q_{ij},\delta p_{ij})=\left(\varepsilon\left.\frac{\partial\bm{Q}(\bm{ q},\bm{p},\bm{\theta})}{\partial\dot{\theta}_{j}}\right|_{\bm{0}=\vec{0}}, \varepsilon\left.\frac{\partial\bm{P}_{i}(\bm{q},\bm{p},\bm{\theta})}{\partial \dot{\theta}_{j}}\right|_{\bm{0}=\vec{0}}\right)\), where \(|\varepsilon|\ll 1\).

### Noether's theorem and time-series dataset

In previous study[19], we found that the candidate transformations that make the Hamiltonian and canonical equations invariant are obtained as the transformations that make the subspace

\[S_{i}:=\left\{\bm{q}_{t+\Delta t},\bm{p}_{t+\Delta t},\bm{q}_{t},\bm{p}_{t} \right.\left|\;H(\bm{q}_{t},\bm{p}_{t})=E_{i},\bm{p}_{t+\Delta t}=\bm{p}_{t}- \frac{\partial H(\bm{q}_{t},\bm{p}_{t})}{\partial\bm{q}_{t}},\bm{q}_{t+ \Delta t}=\bm{q}_{t}+\frac{\partial H(\bm{q}_{t},\bm{p}_{t})}{\partial\bm{p}_ {t}}\right\}\] (3)

invariant. We also found tah \(S_{i}\) is understood as a differentiable manifold[19]. Interpolation of differentiable manifolds can be realized by machine learning methods such as deep learning [25; 23; 26; 27; 28; 29]. In the framework, \(S_{i}\) is estimated from a finite number of data \(D\) using a deep learning technique.

### DNN and data manifold

As mentioned in Sec. 2.2, the subspace \(S_{i}\) could be modeled as a differentiable manifold using a DNN model. In this paper, we refer to such a differentiable manifold as a data manifold.

We explain how a DNN models a \(d_{\text{m}}\)-dimensional manifold in \(d_{\text{in}}\)-dimensional space \(\bm{x}\) using one of the simplest DNNs: a feed forward three-layer DNN, for which the input has \(d_{\text{in}}\) dimensions, the hidden layer has \(d_{\text{h}}(>d_{\text{in}})\) dimensions, and the output has \(d_{\text{out}}(<d_{\text{in}})=d_{m}\) dimensions. The mapping function \(\bm{f}_{\text{DNN}}(\bm{x})=[f_{1}(\bm{x}),f_{2}(\bm{x}),\cdots,f_{d_{\text{in }}}(\bm{x})]\) of the DNN is defined as \(f_{\text{DNN}}(\bm{x})=\bm{w}^{\text{h}}\bm{h}=\bm{w}^{\text{h}}\bm{\varphi}( \bm{w}^{\text{in}}\bm{x})\), where \(\bm{h}=(h_{1},h_{2},\cdots,h_{d_{\text{in}}})\) is the \(d_{\text{h}}\)-dimensional output of the hidden layer. We define \(\bm{\varphi}(\cdot)\) as \(\bm{\varphi}(\bm{w}^{\text{in}}\bm{x})=(\varphi_{1},\varphi_{2},\cdots,\varphi _{d_{\text{in}}}),\varphi_{j}=\varphi\left[\sum_{i}^{d_{\text{in}}}\left(\bm{w} ^{\text{in}}_{ij}x_{i}\right)\right]\), where \(\varphi\) is the activation function. Usually, a sigmoid or ReLU function is used as the activation function. These activation functions are constructed using linear and flat domains. On the basis of these properties of activation functions, \(\varphi_{j}\) maps the input subspace related to the linear domain of the activation function to a one-dimensional space to align the vector \((w_{0j},w_{1j}\cdots,w_{d_{\text{in}}})\). If the number of \(\varphi_{j}\) sharing the same input subspace is \(d_{\text{out}}\), the \(\varphi_{j}\) defines a \(d_{\text{out}}\)-dimensional sub-hyperplane. The DNN models the data distribution by continuously pasting these sub-hyperplanes as if they were the tangent spaces of a data manifold.

That is, the DNN embeds the input space in the output space by pasting the sub-hyperplanes and compresses the tangent direction of these sub-hyperplanes (Fig. 1). Deeper and more complex DNNs can be understood as a collection of such three-layer DNN. Thus, such deeper DNNs can model more complex manifold structures as a combination of simple manifold structures modeled by a three-layer DNN [27]. Note that the output of a three-layer DNN, a part of the deeper DNN, is referred to as a hidden layer. This is only one example of how a DNN models a data manifold. However, many studies have suggested that there are resemble property in successful trained DNNs [25; 23; 26; 27; 28; 29]. By replacing the input space from \(\bm{x}\) to \(\Gamma\times\mathbb{R}\times\mathbb{R}\), we can also model a time-series data manifold \(S_{i}\) using DNN.

In the employed method [19], using a trained DNN that models a time-series data manifold \(S_{i}\), we propose a method of extracting information about the symmetry of a dynamical system. The framework does not require special DNNs, so we can directly utilize the vast knowledge obtained from studies on physical data analysis using DNNs.

## 3 Method

In this section, we describe the employed framework[19] for estimating the symmetry of a time-series dataset of dynamics.

### Estimating method of nonlinear symmetry

On the basis of the theory of the relationship between the symmetry of the time-series dataset distribution and the conservation law (Sec. 2.2), we prviously proposed a method[19] of inferring the symmetry of data manifold using the Monte Carlo sampling method. In this study, we extended the methods to extract the symmetry for non-linear transformations. In this section, the symmetry estimation framework is described, together with the extensions for nonlinear-symmetry estimation.

It can be inferred from the discussion in Sec. 2 that data points that are not on the manifold in the input space are attracted to the manifold (Fig. 1). Once the data points are attracted to the manifold in the hidden layer, they continue to exist on the manifold in the output \(\bm{f}(\mathbf{x})\). We propose a method based on this property of DNNs for extracting the symmetry of the data manifold using a deep autoencoder [23]. The deep autoencoder is a model that compresses the input space to a low-dimensional hidden layer and decompresses the layer to an output space with the same dimension as the input space. In the decompression process, only the subspace of the input space around the data manifold is recovered

Figure 1: Schematic diagram of method of extracting invariant transformation using autoencoder. Lower panel shows the schematic diagram of the mapping structure of a two-dimensional input space in a DNN trained with data distributed on a black curve. The arrows indicate the compression direction of the input space in the mapping from the input to the hidden layer.

[MISSING_PAGE_FAIL:5]

The set of invariant transformations \(M_{\text{invariant}}\) forms a Lie group, as we mentioned in Sec. 2.1. Therefore, \(M_{\text{invariant}}\) constructs a \(d_{\theta}\)-dimensional differential manifold in the coordinate space of \(\bm{\theta}\). The infinitesimal transformation is estimated as the tangent vector of the manifold at \(\bm{\theta}=\bm{0}\) as follows:

\[\left(\delta\bm{q}_{l},\delta\bm{p}_{l}\right)=\varepsilon\left( \left.\frac{\partial\bm{Q}(\bm{q},\bm{p};\bm{a}(\theta_{l}))}{\partial\theta_{ l}}\right|_{\theta=0},\,\frac{\partial\bm{P}(\bm{q},\bm{p};\bm{a}(\theta_{l}))}{ \partial\theta_{l}}\right|_{\theta=0}\right).\] (9)

Because \(\bm{a}\) is a differentiable function of \(\bm{\theta}\), the tangent vector is given as

\[\left(\delta\bm{q}_{l},\delta\bm{p}_{l}\right)=\varepsilon\left( \left.\sum_{k=1}^{d_{\theta}}\frac{\partial\bm{Q}(\bm{q},\bm{p};\bm{a})}{ \partial a_{k}}\frac{\partial a_{k}(\bm{\theta})}{\partial\theta_{l}}\right| _{\theta=\bm{0}},\,\sum_{k=1}^{d_{\theta}}\frac{\partial\bm{P}(\bm{q},\bm{p} ;\bm{a})}{\partial a_{k}}\frac{\partial a_{k}(\bm{\theta})}{\partial\theta_{ l}}\right|_{\theta=\bm{0}}\right).\] (10)

Because functions \(\bm{Q}\) and \(\bm{P}\) are defined explicitly, their derivations, \(\left.\frac{\partial\bm{Q}(\bm{q},\bm{p};\bm{a})}{\partial a_{k}}\right.\) and \(\left.\frac{\partial\bm{P}(\bm{q},\bm{p};\bm{a})}{\partial a_{k}}\right.\), can be obtained analytically. Therefore, we should only estimate \(\left.\frac{\partial a_{k}(\bm{\theta})}{\partial\theta_{l}}\right|_{\theta= \bm{0}}\) to obtain the infinitesimal transformation.

Because \(\bm{a}(\bm{\theta})\) is defined as a differentiable function, set \(\{\bm{a}|\bm{\theta}\in\mathbb{R}^{d_{\theta}}\}\) constructs a \(d_{\theta}\)-dimensional manifold structure in coordinate space \(\bm{a}\). The implicit function representation of the manifold is defined as

\[\left\{\begin{array}{c}f_{1}(a_{1},\cdots,a_{d_{\theta}})=0\\ \quad\vdots\\ f_{d_{\theta}-d_{\theta}}(a_{1},\cdots,a_{d_{\theta}})=0\end{array}\right..\] (11)

The Jacobian matrix of \(f_{k}\) for the parameters of subset \(\bm{a}\), \((b_{1},b_{2},\cdots,b_{d_{\theta}})\subset\bm{a}\), is defined as \(J_{kl}=\frac{\partial f_{k}(a_{1},\cdots,a_{d_{\theta}})}{\partial b_{1}}\). If the Jacobian matrix at \(\bm{a}_{\text{id}}\) becomes nonsingular, from the implicit function theorem, variables other than \((b_{1},b_{2},\cdots,b_{d_{\theta}})\), \(\{c_{k}\}_{k=1}^{d_{\theta}-d_{\theta}}:=A^{\prime}\setminus\{b_{l}\}_{l=1}^ {d_{\theta}}\), can be expressed as \(c_{k}=g_{i}(b_{1},\cdots,b_{d_{\theta}})\). This means that \(\bm{\theta}\) can be replaced by \(\bm{b}\). In this case, \(\left.\frac{\partial a_{k}(\bm{\theta})}{\partial\theta_{l}}\right|_{\theta= \bm{0}}\) is estimated as the tangent vector \(\left.\frac{\partial a_{k}(\bm{b})}{\partial b_{l}}\right|_{\bm{a}=\bm{a}_{l }}\) at identity map \(\bm{a}_{\text{id}}\in\left\{\bm{a}|\bm{Q}(\cdot,\cdot;\bm{a})=\bm{q},\bm{P}( \cdot,\cdot;\bm{a})=\bm{p}\right\}.\) This implies that, around \(e_{\bm{I}}\), the implicit equations in Eq. (11) representing the manifold \(M_{\text{invariant}}\) can be decomposed into the following \(d^{\prime}-d_{\theta}\) simultaneous equations:

\[\left\{\begin{array}{c}h_{1}(c_{1},b_{1},\cdots,b_{d_{\theta}})=0\\ \quad\vdots\\ h_{d^{\prime}-d_{\theta}}(c_{d^{\prime}-d_{\theta}},b_{1},\cdots,b_{d_{\theta }})=0\end{array}\right.,\] (12)

where \(b_{l}\) corresponds to the continuous parameter \(\theta_{l}\) of continuous transformation \([\mathcal{Q}(\bm{q},\bm{p},\bm{\theta}),\mathcal{P}(\bm{q},\bm{p},\bm{\theta })]\). Differentiating these equations with respect to \(b_{l}\) around a point \(e_{\bm{I}}\) yields \(d^{\prime}-d_{\theta}\) simultaneous partial differential equations,

\[\left\{\begin{array}{c}\frac{\partial}{\partial b_{l}}h_{1}(c_{1},b_{1}, \cdots,b_{d_{\theta}})|_{A^{\prime}=e_{\bm{I}}}=0\\ \quad\vdots\\ \frac{\partial}{\partial b_{l}}h_{d^{\prime}-d_{\theta}}(c_{d^{\prime}-d_{ \theta}},b_{1},\cdots,b_{d_{\theta}})|_{A^{\prime}=e_{\bm{I}}}=0\end{array}\right..\] (13)

Solving these simultaneous partial differential equations gives the tangent vector \(\left.\frac{\partial a_{k}(b_{l})}{\partial b_{l}}\right|_{\bm{a}=\bm{a}_{\text {id}}}\) of the manifold at \(\bm{a}_{\text{id}}\). Thus, if \(h_{k}\) can be regressed with the sampling result \(D_{a}\) as the polynomial of \(\{b_{l}\}_{l=1}^{d_{\theta}}\), the conservation law can be inferred. Thus, we can estimate the infinitesimal transformation \(\left(\delta\bm{q}_{l},\delta\bm{p}_{l}\right)\) from the sampling result \(D_{a}\). Thus, in principle, the previously proposed method can be applied to general coordinate transformations including nonlinear transformation. But, to estimate interpretable conservation laws, we would need to model nonlinear transformations of appropriate complexity as parametric functions. This is as difficult as setting up a reduced coordinate system.

### Runge Lenz vector and nonlinear transformation

From the discussion in the Sec. 3.1 and 3.2, in order to search for the non-linear symmetries required for conservation law estimation, it is necessary to set up a parametric function that can represent the non-linear transformation to be estimated. On the other hand, it is generally difficult to pre-set such parametric functions. This difficulty could be overcome by finding a class of parametric functions to explore that can be used generically in certain domains based on physical knowledge. The purpose of this study is therefore to explore a class of parametric functions for such non-linear transformations through the estimation of non-linear transformations corresponding to the Runge Lenz vector, which is a hidden conservation law for central force potential systems where the force is inversely proportional to the square of the radius.

\[H_{3}=\frac{1}{2m}\bm{p}^{2}+G\frac{mM}{|\bm{q}|}\] (14)

First, we describe the geometrical structure of the symmetry of the Runge Lenz vector following previous studies [31, 32]. Consider the motion of the central force potential in six-dimensional phase space: \((\bm{q},\bm{p})=(q_{1},q_{2},q_{3},p_{1},p_{2},p_{3})\). In this system, the Laplace-Runge-Lenz vector, \(\vec{A}=\bm{p}\times L-mG\frac{\bm{q}}{|\bm{q}|_{2}},\ \ L=\bm{q}\times\bm{p}\), is conserved. The Runge Lenz vector corresponds to the SO(4) symmetry in the coordinate space \((\tilde{\bm{q}},\tilde{q}_{4},\tilde{\bm{p}},\tilde{p}_{4})=(\tilde{q}_{1}, \tilde{q}_{2},\tilde{q}_{3},\tilde{q}_{4},\tilde{p}_{1},\tilde{p}_{2},\tilde{ p}_{3},\tilde{p}_{4})\), defined as

\[\tilde{\bm{q}}=\tilde{\bm{q}}(\bm{q},\bm{p}):=\frac{\bm{q}}{\| \bm{q}\|_{2}}-\frac{\bm{q}\cdot\bm{p}}{mG}\bm{p},\ \ \tilde{q}_{4}=\tilde{q}_{4}(\bm{q},\bm{p}):=\frac{p_{0}}{mG}\bm{q}\cdot\bm{p},\] (15) \[\tilde{\bm{p}}=\tilde{\bm{p}}(\bm{q},\bm{p}):=\frac{2p_{0}\bm{p} }{p_{0}^{2}+p^{2}},\ \ \tilde{p}_{4}=\tilde{p}_{4}(\bm{q},\bm{p}):=\frac{p^{2}-p_{0}^{2}}{p_{0}^{2}+p^ {2}},\] (16)

where \(p_{0}=\sqrt{-2mE}\). The transformed coordinate satisfies the conditions \(\tilde{\bm{q}}^{2}+\tilde{q}_{4}^{2}=1,\tilde{\bm{p}}^{2}+\tilde{p}_{4}^{2}=1\), and \(\tilde{\bm{q}}\cdot\tilde{\bm{p}}+\tilde{q}_{4}\tilde{p}_{4}=0\). Let us assume that the matrix representation of SO(4) is given by \(A\). Moreover, assume the transformation is represented as \(\tilde{\bm{q}}^{\gamma T}=A\tilde{\bm{q}}^{T}\) and \(\tilde{\bm{p}}^{\gamma T}=A\tilde{\bm{p}}^{T}\).

We investigate the correspondence between the \(4\times 4\) matrix representation \(A\) of the SO(4) symmetry in \((\tilde{\bm{q}},\tilde{\bm{p}})\) space and the coordinate transformation in \((\bm{q},\bm{p})\) space. Because the inverse of the coordinate transformation is given by

\[\bm{q}=\bm{q}(\tilde{\bm{q}},\tilde{q}_{4},\tilde{\bm{p}},\tilde{p}_{4})=- \frac{G}{2E}[(1-\tilde{p}_{4})\tilde{\bm{q}}+\tilde{q}_{4}\tilde{\bm{p}}],\ \ \bm{p}=\bm{p}(\tilde{\bm{q}},\tilde{q}_{4},\tilde{\bm{p}},\tilde{p}_{4})=\sqrt{ -2mE}\frac{\tilde{\bm{p}}}{1-\tilde{p}_{4}},\] (17)

the transformation of SO(4) in the original space becomes

\[\bm{Q}(\tilde{\bm{q}},\tilde{q}_{4},\tilde{\bm{p}},\tilde{p}_{4 })=\bm{q}(\tilde{\bm{Q}},\tilde{Q}_{4},\tilde{\bm{p}},\tilde{P}_{4}),\ \ \bm{P}(\tilde{\bm{q}},\tilde{q}_{4},\tilde{\bm{p}},\tilde{p}_{4})=\bm{p}(\tilde {\bm{Q}},\tilde{Q}_{4},\tilde{\bm{P}},\tilde{P}_{4}),\] (18) \[\left(\begin{array}{c}\tilde{\bm{Q}}^{i}\\ \tilde{Q}_{4}\end{array}\right)=A\left(\begin{array}{c}\tilde{\bm{q}}^{i}\\ \tilde{q}_{4}\end{array}\right),\ \ \left(\begin{array}{c}\tilde{\bm{P}}^{i}\\ \tilde{P}_{4}\end{array}\right)=A\left(\begin{array}{c}\tilde{\bm{p}}^{i} \\ \tilde{p}_{4}\end{array}\right).\] (19)

Thus, the Runge Lenz vector has linear symmetry in the space beyond which it maps the phase space with certain non-linear transformations. Such symmetry estimates suggest that it is useful to assume a class of non-linear transformations, such as stereo mapping, as a class of mapping transformations of phase space.

In this study, we propose a framework in which the non-linear symmetry is assumed to be a combination of a coordinate transformation and a linear transformation (Fig. 2), each of which is estimated

Figure 2: Schematic diagram of proposed framework.

independently of the other. A machine learning framework to estimate nonlinear symmetries has been already proposed using symbolic regression [18], they can also estimate the conservation laws as interpretable forms of functions corresponding to nonlinear symmetries. A method has also been proposed [21] to visualize conservation laws in the space in which they are embedded. The advantage of our method is to allow direct visualization of the manifolds formed by Lie groups. It should allow the scientists to work their insight from other viewpoints.

In this study, we check whether it is possible to estimate the linear symmetry corresponding to the Runge Lenz vector in the space of its mapping destination when the previously mentioned coordinate transformations are known. It is not obvious that the estimation will work even when the coordinate transformations are known. That is, under a non-linear coordinate transformation, the measure changes from a point in the original space to a point in mapped space, and if the data are finite, even if the data manifold has a uniform density in the original space, there will be regions where the density is almost zero at the mapping destination (see Fig. 2). This makes it difficult to estimate symmetry.

## 4 Results

We applied the proposed method to a system of central force potentials (Eq. 14). Specifically, for simulation data generated at all energies and initial conditions under the Hamiltonian of the central force potential (Eq. 14), an estimation of the set of transformations that make the data manifold invariant was performed in the framework of the following linear transformation after applying a coordinate transformation [Eqs.(23) and (24)]:

\[\begin{bmatrix}\tilde{Q}_{1}\\ \tilde{Q}_{2}\\ \tilde{Q}_{3}\\ \tilde{P}_{1}\\ \tilde{P}_{2}\\ \tilde{P}_{3}\end{bmatrix}=\begin{pmatrix}a_{11},a_{12},&0&0&0&0&0\\ a_{21},a_{22},&0&0&0&0&0\\ 0&0&1&0&0&0&0\\ 0&0&0&0&a_{11},a_{12},&0\\ 0&0&0&0&a_{21},a_{22},&0\\ 0&0&0&0&0&0&1\\ \end{pmatrix}\begin{pmatrix}\tilde{q}_{1}\\ \tilde{q}_{2}\\ \tilde{q}_{3}\\ \tilde{p}_{1}\\ \tilde{p}_{2}\\ \tilde{p}_{3}\end{pmatrix}\] (20)

The estimation results of the proposed method confirm that a set of target transformations corresponding to the Lungerenz vector can be obtained (Fig. 3). Specifically, for the matrix elements \(a_{11}\) and \(a_{12}\) corresponding to \(\cos\) and \(\sin\), a set of circular symmetric transformations was obtained, and for the matrix elements \(a_{11}\) and \(a_{22}\) corresponding to \(\cos\) and \(\cos\), a set of diagonal symmetric transformations (Fig. 3).

## 5 Summary and Discussion

This study suggests that the employed method [19] of directly visualizing manifolds formed by Lie algebras is also effective for non-linear transformations, by separating the transformation function for verifying symmetry into a coordinate transformation and a linear transformation. In this study, it was confirmed that linear transformations can be estimated under known coordinate transformations. As a result, we succeeded in extracting a set of symmetric transformations, despite the fact that the nonlinear coordinate transformations resulted in large differences in measures between the original and mapped spaces. In the future, we will further attempt to estimate the non-linear coordinate transformations and estimate the conserved values based on them. It is necessary to express non-linear mapping transformations in terms of parametric functions, in which case it may be useful to use a function class of stereo mapping, such as the one used in this study. It is then necessary to represent the non-linear coordinate transformations by parametric functions. The results of this study suggest that it is useful to use a function class of stereo mapping, such as the one used in this study, as its parametric function.

Figure 3: Estimation results of symmetric transformation set corresponding to Runge Lentz vector.

## Acknowledgements

We would like to thank the all reviewers for their patience and kind comments on our manuscript, which was incomplete, especially in that the description of important previous studies was missing. This work was supported by JST, PRESTO Grant Number JPMJPR212A and JSPS KAKENHI 22K13979, 23H03460.

## Appendix A

The procedure of proposed method is summarized in Algorithm 1.

```
0: dataset \(D=\left\{\bm{q}_{t}^{i},\bm{p}_{t}^{i},\bm{q}_{t_{t+\Delta}}^{i},\bm{p}_{t_{t+ \Delta}}^{i}\right\}_{i=1}^{N}\) in a given coordinate system.
0: Invariant transformation set \(D_{a}=\left\{(a_{1},a_{2},a_{3},\cdots,a_{d_{a}})_{n_{a}}\right\}_{n_{a}=1}^{N_ {a}}\).
0: Train the deep autoencoder with dataset \(D\).
0: Using the trained deep autoencoder and REMC method, sampling transformation parameters \(a_{1},a_{2},a_{3},\cdots,a_{d_{a}}\) from multiple probability distributions \(P^{\prime}(a_{1},a_{2},a_{3},\cdots,a_{d_{a}})\) corresponding to different noise intensities \(\sigma^{\prime}\).
0: Select \(\sigma^{\prime}\) from the distribution structure of the sampling results and output the sampling result of the selected \(\sigma^{\prime}\) state as \(D_{a}\). ```

**Algorithm 1** Estimation of the invariant transformation set [19]

## References

* [1] S. Tomonaga. Remarks on Bloch's Method of Sound Waves applied to Many-Fermion Problems. _Prog. Theor. Phys._, 5:544-569, 1950.
* [2] David Bohm and David Pines. A collective description of electron interactions. i. magnetic interactions. _Phys. Rev._, 82(5):625, 1951.
* [3] David Pines and David Bohm. A collective description of electron interactions: Ii. collective vs individual particle aspects of the interactions. _Phys. Rev._, 85(2):338, 1952.
* [4] S. Tomonaga. Elementary theory of quantum-mechanical collective motion of particles, i. _Prog. Theor. Phys._, 13(5):467-481, 1955.
* [5] Philip G Saffman. _Vortex Dynamics_. Cambridge University Press, Cambridge, 1992.
* [6] Tamas Vicsek and Anna Zafeiris. Collective motion. _Phys. Rep._, 517(3-4):71-140, 2012.
* [7] Takashi Ikegami, Yohichi Mototake, Shintaro Kobori, Mizuki Oka, and Yasuhiro Hashimoto. Life as an emergent phenomenon: studies from a large-scale biod simulation and web data. _Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences_, 375(2109):20160351, 2017.
* [8] Michael Schmidt and Hod Lipson. Distilling free-form natural laws from experimental data. _Science_, 324(5923):81-85, 2009.
* [9] Samuel Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian neural networks. volume 32, pages 15353-15363. Curran Associates, Inc., 2019.
* [10] Peter Toth, Danilo Jimenez Rezende, Andrew Jaegle, Sebastien Racaniere, Aleksandar Botev, and Irina Higgins. Hamiltonian generative networks. _arXiv preprint arXiv:1909.13789_, 2019.
* [11] Roberto Bondesan and Austen Lamacraft. Learning symmetries of classical integrable systems. _arXiv preprint arXiv:1906.04645_, 2019.
* [12] J. Willard Gibbs. On the equilibrium of heterogeneous substances. _Transactions of the Connecticut Academy of Arts and Sciences_, 3:108-248, 1875-1876.

* [13] J. Willard Gibbs. On the equilibrium of heterogeneous substances. _Transactions of the Connecticut Academy of Arts and Sciences_, 3:343-524, 1877-1878.
* [14] Eurika Kaiser, J Nathan Kutz, and Steven L Brunton. Discovering conservation laws from data for control. In _2018 IEEE Conference on Decision and Control (CDC)_, pages 6415-6421. IEEE, 2018.
* [15] Sebastian J Wetzel, Roger G Melko, Joseph Scott, Maysum Panju, and Vijay Ganesh. Discovering symmetry invariants and conserved quantities by interpreting siamese neural networks. _Physical Review Research_, 2(3):033499, 2020.
* [16] Ziming Liu and Max Tegmark. Machine learning conservation laws from trajectories. _Physical Review Letters_, 126(18):180604, 2021.
* [17] Seungwoong Ha and Hawoong Jeong. Discovering invariants via machine learning. _Physical Review Research_, 3(4):L042035, 2021.
* [18] Ziming Liu and Max Tegmark. Machine learning hidden symmetries. _Phys. Rev. Lett._, 128:180201, May 2022.
* [19] Yoh-ichi Mototake. Interpretable conservation law estimation by deriving the symmetries of dynamics from trained deep neural networks. _Physical Review E_, 103(3):033303, 2021.
* [20] Han Zhang, Huawei Fan, Liang Wang, and Xingang Wang. Learning hamiltonian dynamics with reservoir computing. _Physical Review E_, 104(2):024205, 2021.
* [21] Peter Y Lu, Rumen Dangovski, and Marin Soljacic. Discovering conservation laws using optimal transport and manifold learning. _Nature Communications_, 14(1):4744, 2023.
* [22] AE Noether. Nachr kgl ges wiss gottingen. _Math. Phys. KI II_, 235, 1918.
* [23] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. _Science_, 313(5786):504-507, 2006.
* [24] Jurgen Struckmeier and Claus Riedel. Canonical transformations and exact invariants for time-dependent hamiltonian systems. _Annalen der Physik_, 11(1):15-38, 2002.
* [25] Bunpei Irie and Mitsuo Kawato. Acquisition of internal representation by multi-layered perceptrons. _Transactions of the Institute of Electronics, Information and Communication Engineers D_, 73(8):1173-1178, 1990.
* [26] Pratik Prabhanjan Brahma, Dapeng Wu, and Yiyuan She. Why deep learning works: A manifold disentanglement perspective. _IEEE Transactions on Neural Networks and Learning Systems_, 27(10):1997-2008, 2016.
* [27] Ronen Basri and David W. Jacobs. Efficient representation of low-dimensional manifolds using deep networks. In _5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings_. OpenReview.net, 2017.
* [28] Salah Rifai, Yann N Dauphin, Pascal Vincent, Yoshua Bengio, and Xavier Muller. The manifold tangent classifier. In J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger, editors, _Advances in Neural Information Processing Systems 24_, pages 2294-2302. Curran Associates, Inc., 2011.
* [29] Yhoichi Mototake and Takashi Ikegami. The dynamics of deep neural networks. _International Symposium on Artificial Life and Robotics_, 2015.
* [30] Koji Hukushima and Koji Nemoto. Exchange monte carlo method and application to spin glass simulations. _J. Phys. Soc. Jpn._, 65(6):1604-1608, 1996.
* [31] Harold H Rogers. Symmetry transformations of the classical kepler problem. _Journal of Mathematical Physics_, 14(8):1125-1129, 1973.
* [32] Alex Alemi. Laplace-runge-lenz vector, 2009.