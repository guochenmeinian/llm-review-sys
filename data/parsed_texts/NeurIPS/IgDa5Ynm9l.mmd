# Efficient Model-Free Exploration in Low-Rank MDPs

 Zakaria Mhammedi

MIT

mhammedi@mit.edu &Adam Block

MIT

ablock@mit.edu &Dylan J. Foster

Microsoft Research

dylanfoster@microsoft.com &Alexander Rakhlin

MIT

rakhlin@mit.edu

###### Abstract

A major challenge in reinforcement learning is to develop practical, sample-efficient algorithms for exploration in high-dimensional domains where generalization and function approximation is required. _Low-Rank Markov Decision Processes_--where transition probabilities admit a low-rank factorization based on an unknown feature embedding--offer a simple, yet expressive framework for RL with function approximation, yet existing algorithms either (1) are computationally intractable, or (2) require restrictive statistical assumptions such as latent variable structure or access to model-based function approximation. In this work, we propose the first provably sample-efficient algorithm for exploration in Low-Rank MDPs that is both computationally efficient and model-free, allowing for general function approximation while requiring no structural assumptions beyond a reachability condition that we show is substantially weaker than that assumed in prior work. Our algorithm, SpanRL, uses the notion of a _barycentric spanner_ for the feature embedding as an efficiently computable basis for exploration, performing efficient spanner computation by interleaving representation learning and policy optimization subroutines. Our analysis--which is appealingly simple and modular--carefully combines several techniques, including a new approach to error-tolerant barycentric spanner computation, and a new analysis of a certain minimax representation learning objective found in prior work.

## 1 Introduction

In reinforcement learning and control, many of the most promising application domains require the agent to navigate complex, high-dimensional state and action spaces, where generalization and function approximation is necessary. The last decade has witnessed impressive empirical success in domains where data are abundant [34, 38, 26, 28, 27], but when data are limited, ensuring efficient exploration in large domains is a major research question. For _statistical efficiency_, the foundations have recently begun to take shape, with a line of research providing structural conditions that facilitate sample-efficient exploration, as well as fundamental limits [37, 21, 39, 42, 14, 24, 17, 18]. _Computational efficiency_, however, remains a major challenge: outside of simple settings [7, 23], existing algorithms with provable sample complexity guarantees are computationally inefficient, and typically require solving intractable non-convex optimization problems [21, 11, 24, 10]. The prospect of developing practical algorithms for exploration in high-dimensional state spaces that are both computationally and statistically efficient raises three fundamental questions:

1. What are the right computational primitives for exploration? That is, how can one efficiently represent and compute exploratory policies that allow the learner to explore the state space and gather useful data?
2. How should one leverage function approximation--for example, via representation learning--to discover such primitives in a computationally and statistically efficient fashion?
3. Given answers to the first two questions, how can one efficiently interleave function approximation and exploration to provide provably efficient algorithms?In this paper, we investigate these questions through the _Low-Rank MDP_ model [36, 45, 1]. In a Low-Rank MDP, the state space is large and potentially continuous, but the transition probabilities admit an (unknown) low-rank factorization. Concretely, for a finite-horizon Low-Rank MDP with horizon \(H\), the transition densities for layer \(h\in[H]\) satisfy

\[T_{h}(x_{h+1}\mid x_{h},a_{h})=\mu^{*}_{h+1}(x_{h+1})^{\top}\phi^{*}_{h}(x_{h},a _{h}),\] (1)

where \(\phi^{*}_{h}(\cdot,\cdot)\in\mathbb{R}^{d}\) and \(\mu^{*}_{h+1}(\cdot)\in\mathbb{R}^{d}\) are state-action and next-state embeddings. The low-rank structure in (1) facilitates tractable exploration: if the embedding \(\phi^{\star}_{h}\) is known to the learner, one can efficiently learn a near-optimal policy with sample complexity polynomial in the feature dimension \(d\), and independent of the size of the state space [23]; in this regard, \(\phi^{\star}_{h}\) can be thought of as a low-dimensional _representation_ that enables sample-efficient RL. Following Agarwal et al. [1], we consider the challenging setting in which both \(\phi^{\star}_{h}\) and \(\mu^{*}_{h+1}\) are _unknown_ to the learner. This formulation generalizes well-known frameworks such as the _Block MDP_ (BMDP) model [12, 32], and necessitates the use of _representation learning_: the agent must learn an embedding that approximates \(\phi^{*}_{h}\) as it explores the environment, and must use this learned embedding to drive subsequent exploration. This form of function approximation allows for great flexibility, as \(\phi^{\star}_{h}\) can be an arbitrary, nonlinear function of the state; in practice, it is common to model \(\phi^{\star}_{h}\) as a neural net [49].

The Low-Rank MDP is perhaps the simplest MDP structure that demands systematic exploration and nonlinear function approximation while allowing for a continuum of states, yet understanding of _efficient_ algorithm design for this model is surprisingly limited. Existing algorithms suffer from at least one of the following drawbacks:

1. Computational intractability [21, 24, 14, 9, 43].
2. Strong modeling assumptions (e.g., ability to model \(\mu^{*}_{h+1}(\cdot)\), which facilitates application of model-based RL techniques) [1, 40, 10]; in this work, we aim for _model-free_ methods that only require learning \(\phi^{\star}_{h}\).
3. Restrictive structural assumptions (e.g., non-negativity or latent variable structure for the embeddings in (1)) [35, 49].

At the root of these limitations is the complex interplay between exploration and representation learning: the agent must learn a high-quality representation to guide in exploring the state space, but learning such a representation requires gathering diverse and informative data, which is difficult to acquire without having already explored the state space to begin with. Overcoming this challenge--particularly where computational efficiency is concerned--requires (1) representation learning procedures that lead to sufficiently expressive representations for downstream applications, (2) efficient exploration procedures that are robust to errors in learned representations, and 3) understanding the interaction between these procedures, which must be interleaved. In this work, we propose an algorithm that addresses each of these challenges, as detailed below.

Contributions.We provide the first provably computationally efficient and model-free algorithm for general Low-Rank MDPs. Our algorithm, SpanRL, uses the notion of a _barycentric spanner_[6] for the embedding \(\phi^{\star}_{h}\) as an efficiently computable basis for exploration, and combines this with a minimax representation learning approach [35, 49]. SpanRL interleaves exploration with representation learning in a layer-wise fashion, learning a new representation at each layer \(h\) using exploratory data gathered at previous layers, then uses this representation to facilitate computation of a collection of exploratory policies (a _policy cover_), which act as an approximate barycentric spanner for the features at layer \(h+1\), ensuring good coverage for subsequent iterations. SpanRL is simple and modular, and its analysis is surprisingly compact given the greater generality compared to prior work [49, 35, 31].

SpanRL can accommodate general-purpose function approximation to learn the representation \(\phi^{\star}\) (e.g., neural nets or other flexible classes) whenever a certain minimax representation learning objective [35, 49] can be solved efficiently for the function class of interest. Compared to efficient algorithms from prior work, SpanRL: (1) is model-free (i.e., only requires access to a function class \(\Phi\) capable of modeling \(\phi^{\star}\), and does not need to model \(\mu^{*}_{h+1}\)), and (2) applies to general Low-Rank MDPs, replacing strong additional assumptions such as non-negativity of the feature embeddings (so-called _latent variable_ structure) or block structure (see Table 1) with a reachability assumption that we show is substantially weaker than that assumed in prior work (see Appendix H). As a secondary benefit, the algorithm is reward-free. Our analysis carefully combines several new techniques, including (1) an error-tolerant variant of the classical barycentric spanner computation algorithm of Awerbuchand Kleinberg [6], and (2) a new analysis of a minimax representation learning objective introduced in Modi et al. [35], Zhang et al. [49], which shows for the first time that this objective can lead to meaningful guarantees in general Low-Rank MDPs without latent variable structure; this increased generality is meaningful, as we show in Appendix H that there is an exponential separation between our guarantees and those that require such a structure.

Organization.Section 2 formally introduces the Low-Rank MDP model and the online reinforcement learning framework we consider. In Section 3, we highlight challenges faced by previous approaches, introduce our main algorithm, \(\mathsf{SpanRL}\), and show how it overcomes these challenges, and then present its main sample complexity guarantee.

Comparison to ArXiv Version.After the initial submission of this work, we developed a substantially improved of the algorithm that removes the reachability assumption at the cost of a larger (but still polynomial) sample complexity guarantee. We have included this algorithm and its analysis in the ArXiv version of this paper [30].

## 2 Problem Setting

### Low-Rank MDP Model

We work in an episodic, finite-horizon reinforcement learning framework, where \(H\in\mathbb{N}\) denotes the horizon. A _Low-Rank MDP_[36, 45, 1] is a tuple \(\mathcal{M}=(\mathcal{X},\mathcal{A},(\phi_{h}^{*})_{h\in[H]},(\mu_{h}^{*})_ {h\in[H]},\rho)\) consisting of a _state space_\(\mathcal{X}\), _action space_\(\mathcal{A}\) with \(|\mathcal{A}|=A\), distribution over initial states \(\rho\in\Delta(\mathcal{X})\), and mappings \(\mu_{h+1}^{*}:\mathcal{X}\rightarrow\mathbb{R}^{d}\) and \(\phi_{h}^{*}:\mathcal{X}\times\mathcal{A}\rightarrow\mathbb{R}^{d}\).3 Beginning with \(\bm{x}_{1}\sim\rho\), an episode proceeds in \(H\) steps, where for each step \(h\in[H]\), the state \(\bm{x}_{h}\) evolves as a function of the agent's action \(\bm{a}_{h}\) via

Footnote 3: For the stated sample complexity, FLAMEE requires access to a sampling oracle for the learner model. Without this oracle, the results require additional latent variable structure and a reachability assumption.

\[\bm{x}_{h+1}\sim T_{h}(\cdot\mid\bm{x}_{h},\bm{a}_{h}),\]

where \(T_{h}\) is a probability transition kernel, which is assumed to factorize based on \(\phi_{b}^{*}\) and \(\mu_{h}^{*}\). In detail, we assume that there exists a \(\sigma\)-finite measure \(\nu\) on \(\mathcal{X}\) such that for all \(1\leq h\leq H-1\), and for all \(x\in\mathcal{X}\) and \(a\in\mathcal{A}\), the function \(x^{\prime}\mapsto\mu_{h+1}^{*}(x^{\prime})^{\top}\phi_{h}^{*}(x,a)\) is a probability density with respect to \(\nu\) (i.e. the function is everywhere non-negative and integrates to \(1\) under \(\nu\)). For any \(\mathcal{X}^{\prime}\subseteq\mathcal{X}\), the probability that \(\bm{x}_{h+1}\in\mathcal{X}^{\prime}\) under \(\bm{x}_{h+1}\sim T_{h}(\cdot\mid x_{h},a_{h})\) is then assumed to follow the law

\[T_{h}(\mathcal{X}^{\prime}\mid x_{h},a_{h})=\int_{\mathcal{X}^{\prime}}\mu_{h +1}^{*}(x)^{\top}\phi_{h}^{*}(x_{h},a_{h})\mathrm{d}\nu(x).\] (2)

For notational compactness, we assume (following, e.g., Jiang et al. [21]) that the MDP \(\mathcal{M}\) is _layered_ so that \(\mathcal{X}=\mathcal{X}_{1}\cup\cdots\cup\mathcal{X}_{H}\) for \(\mathcal{X}_{i}\cap\mathcal{X}_{j}=\varnothing\) for all \(i\neq j\), where \(\mathcal{X}_{h}\subseteq\mathcal{X}\) is the subset of states in \(\mathcal{X}\)

\begin{table}
\begin{tabular}{c c c c c} \hline  & Comp. efficient & Model-free & General low rank & Sample comp. \\ \hline OLIVE [21] & \(\bm{\times}\) & \(\bm{\check{\mathcal{V}}}\) & \(\bm{\check{\mathcal{V}}}\) & \(\frac{d^{3}\mathcal{A}H^{5}\log[\bm{\check{\mathcal{V}}}]}{\varepsilon^{2}}\) \\ (see also [24, 14, 9, 43]) & \(\bm{\check{\mathcal{V}}}\) & \(\bm{\check{\mathcal{V}}}^{1}\) & \(\frac{d^{7}\mathcal{A}^{9}H^{22}\log([\bm{\check{\mathcal{V}}}]\top\top)}{ \varepsilon^{10}}\) \\ FLAMEE [1] & \(\bm{\check{\mathcal{V}}}\) & \(\bm{\check{\mathcal{K}}}\) & \(\bm{\check{\mathcal{V}}}^{1}\) & \(\frac{d^{4}\mathcal{A}^{2}H^{5}\log([\bm{\check{\mathcal{V}}}]\top\top)}{ \varepsilon^{2}}\) \\ Rep-UCB [40] & \(\bm{\check{\mathcal{V}}}\) & \(\bm{\check{\mathcal{V}}}\) & \(\bm{\check{\mathcal{V}}}\) & \(\frac{d^{4}\mathcal{A}^{2}H^{5}\log([\bm{\check{\mathcal{V}}}]\top\top)}{ \varepsilon^{2}}\) \\ (see also [10]) & \(\bm{\check{\mathcal{V}}}\) & \(\bm{\check{\mathcal{V}}}\) & \(\bm{\check{\mathcal{V}}}\) & Non-negative/ & \(\frac{d^{19}_{\mathcal{X}}A^{2}H^{19}\log\left|\bm{\check{\mathcal{V}}}\right|}{ \left(\varepsilon^{0}\gamma^{3}\gamma^{11}\right)}\) \\ MOFFLE [35]2 & \(\bm{\check{\mathcal{V}}}\) & \(\bm{\check{\mathcal{V}}}\) & latent variable & \(\frac{d^{19}_{\mathcal{X}}A^{2}H^{19}\log\left|\bm{\check{\mathcal{V}}}\right|}{ \left(\varepsilon^{0}\gamma^{3}\gamma^{11}\right)}\) \\ BRIEE [49] & \(\bm{\check{\mathcal{V}}}\) & \(\bm{\check{\mathcal{V}}}\) & \(\bm{\check{\mathcal{V}}}\) & Block MDP & \(\frac{|\bm{\check{\mathcal{V}}}|^{8}A^{4}H^{19}\log\left|\bm{\check{\mathcal{V}}} \right|}{\varepsilon^{4}}\) \\ \(\mathsf{SpanRL}\) (this paper) & \(\bm{\check{\mathcal{V}}}\) & \(\bm{\check{\mathcal{V}}}\) & \(\bm{\check{\mathcal{V}}}\) & \(\frac{A^{4}\mathcal{A}^{6}H^{4}(d\log\left|\bm{\check{\mathcal{V}}}\right|)}{ \varepsilon^{2}\gamma^{2}}\) \\ \hline \end{tabular}
\end{table}
Table 1: Comparison of sample complexity required learn an \(\varepsilon\)-optimal policy. \(\Phi\) denotes the feature class, and \(\Upsilon\) denotes an additional feature class capturing model-based function approximation. For approaches that require non-negative (latent variable) structure, \(d_{\text{\tiny{LV}}}\) [resp. \(\gamma\)] denotes the latent variable dimension [resp. the reachability parameter in the latent representation], and for BMDPs, \(\left|\mathcal{S}\right|\) denotes the size of the latent state space. For \(\mathsf{SpanRL}\), \(\eta\) denotes the reachability parameter.

that are reachable at layer \(h\in[H]\). This can be seen to hold without loss of generality (modulo dependence on \(H\)), by augmenting the state space to include the layer index.

**Remark 2.1** (Comparison to previous formulations).: _Our formulation, in which the transition dynamics (2) are stated with respect to a base measure \(\nu\), are a rigorous generalization of Low-Rank MDP formulations found in previous works [23; 1], which tend to implicitly assume the state space is countable and avoid rigorously defining integrals. We adopt this more general formulation to emphasize the applicability our results to continuous domains. However, in the special case where state space is countable, choosing \(\nu\) as the counting measure yields \(T_{h}(\mathcal{X}^{\prime}\mid x_{h},a_{h})=\sum_{x\in\mathcal{X}^{\prime}} \mu_{h+1}^{\star}(x)^{\top}\phi_{h}^{\star}(x_{h},a_{h})\), which is consistent with prior work._

Policies and occupancy measures.We define \(\Pi_{\mathsf{H}}=\{\pi:\mathcal{X}\to\Delta(\mathcal{A})\}\) as the set of all randomized, Markovian policies. For a policy \(\pi\in\Pi_{\mathsf{H}}\), we let \(\mathbb{P}^{\pi}\) denote the law of \((\bm{x}_{1},\bm{a}_{1}),\ldots,(\bm{x}_{H},\bm{a}_{H})\) under \(\bm{a}_{h}\sim\pi(\bm{x}_{h})\), and let \(\mathbb{E}^{\pi}\) denote the corresponding expectation. For any \(\mathcal{X}^{\prime}\subseteq\mathcal{X}_{h}\), we let \(\mathbb{P}^{\pi}_{h}[\mathcal{X}^{\prime}]=\mathbb{P}^{\pi}[\bm{x}_{h}\in \mathcal{X}^{\prime}]\) denote the marginal law of \(\bm{x}_{h}\) under \(\pi\). For \(x\in\mathcal{X}_{h}\), we define the _occupancy measure_\(d^{\pi}(x)=\frac{\mathrm{d}\mathbb{P}^{\pi}_{h}}{\mathrm{d}\nu}(x)\) as the density of \(\mathbb{P}^{\pi}_{h}\) with respect to \(\nu\).

### Online Reinforcement Learning and Reward-Free Exploration

We consider a standard _online reinforcement learning_ framework where the Low-Rank MDP \(\mathcal{M}\) is unknown, and the learning agent interacts with it in _episodes_, where at each episode the agent executes a policy of the form \(\pi:\mathcal{X}\to\Delta(\mathcal{A})\) and observes the resulting trajectory \((\bm{x}_{1},\bm{a}_{1}),\ldots,(\bm{x}_{H},\bm{a}_{H})\). While the ultimate goal of reinforcement learning is to optimize a policy with respect to a possibly unknown reward function, here we focus on the problem of _reward-free exploration_, which entails learning a collection of policies that almost optimally "covers" the state space, and can be used to efficiently optimize any downstream reward function [12; 33; 15; 31]. To wit, we aim to construct an _policy cover_, a collection of policies that can reach any state with near-optimal probability.

**Definition 2.1** (Policy cover).: _For \(\alpha\in(0,1]\), a subset \(\Psi\subseteq\Pi_{\mathsf{H}}\) is an \(\alpha\)-policy cover for layer \(h\) if_

\[\forall x\in\mathcal{X}_{h},\quad\max_{\pi\in\Psi}d^{\pi}(x)\geq\alpha\cdot \max_{\pi^{\prime}\in\Pi_{\mathsf{H}}}d^{\pi^{\prime}}(x).\] (3)

We show (Appendix G) that given access to such a policy cover with constant \(\alpha\), it is possible to optimize any downstream reward function with polynomial sample complexity.

Assumptions.To facilitate learning a policy cover, we make the following _reachability_ assumption.

**Assumption 2.1** (\(\eta\)-reachability).: _For any \(h\in[H]\) and \(x\in\mathcal{X}_{h}\), \(\max_{\pi\in\Pi_{\mathsf{H}}}d^{\pi}(x)\geq\eta\cdot\|\mu_{h}^{\star}(x)\|\)._

Reachability is necessary if one aims to build a policy cover that satisfies (3) uniformly for all states; without such a condition, one gives up on covering hard-to-reach states. Some notion of reachability is required in essentially all prior work on efficient model-free algorithms for Low-Rank MDPs [32; 35; 5], and was only very recently removed in the (more restrictive) BMDP setting [31; 49].

**Remark 2.2** (Comparison to other reachability-like assumptions).: _Assumption 2.1 generalizes and subsumes all previous reachability-like conditions of which we are aware [33; 46; 1; 35]. Notably, reachability is implied by the notion of feature coverage [5] (used in the context of transfer learning in Low-Rank MDPs), which asserts that \(\sup_{\pi\in\Pi_{\mathsf{H}}}\lambda_{\min}(\mathbb{E}^{\pi}[\phi_{h}^{\star} (\bm{x}_{h},\bm{a}_{h})\phi_{h}^{\star}(\bm{x}_{h},\bm{a}_{h})^{\top}])\geq\eta\), for some \(\eta>0\). It is also implied by explorability [46], which is similar to feature coverage, but involves the first moments of \(\phi_{h}^{\star}\). Our reachability assumption is also weaker than that used in [1; 35] under the latent variable model, and generalizes that made for BMDPs [33]. See Appendix H for details, as well as an exponential separation between our assumptions and analogous assumptions in [1; 35]._

Beyond reachability, we assume (following [1; 35]) for normalization that, for all \(h\in[H]\) and \((x,a)\in\mathcal{X}_{h}\times\mathcal{A}\), \(\|\phi_{h}^{\star}(x,a)\|\leq 1\), and that for all \(g:\mathcal{X}_{h}\to[0,1]\),

\[\left\|\int_{\mathcal{X}_{h}}\mu_{h}^{\star}(x)g(x)\mathrm{d}\nu(x)\right\|\leq \sqrt{d}.\] (4)

Function approximation and desiderata.We do not assume that the true features \((\phi_{h}^{\star})_{h\in[H]}\) or the mappings \((\mu_{h}^{\star})_{h\in[H]}\) are known to the learner. To provide sample-efficient learning guarantees we make use of function approximation as in prior work [3; 35], and assume access to a _feature class_\(\Phi\subseteq\{\phi:\mathcal{X}\times\mathcal{A}\to\mathbb{R}^{d}\}\) that contains \(\phi_{h}^{\star}\), for \(h\in[H-1]\).

**Assumption 2.2** (Realizability).: _The feature class \(\Phi\in\{\phi:\mathcal{X}\times\mathcal{A}\to\mathbb{R}^{d}\}\) has \(\phi_{h}^{*}\in\Phi\) for all \(h\in[H]\). Moreover, for all \(\phi\in\Phi\), \(x\in\mathcal{X}\), and \(a\in\mathcal{A}\), it holds that \(\|\phi(x,a)\|\leq 1\)._

The class \(\Phi\) may consist of linear functions, neural networks, or other standard models depending on the application, and reflects the learner's prior knowledge of the underlying MDP. We assume that \(\Phi\) is finite to simplify presentation, but extension to infinite classes is straightforward, as our results only invoke finiteness through standard uniform convergence arguments. Note that unlike model-based approaches [1, 40, 10, 2], we do not assume access to a class capable of realizing the features \(\mu_{h}^{*}\), and our algorithm does not attempt to learn these features; this is why we distinguish our results as _model-free_.

For constant \(\alpha\), our goal is to learn an \(\alpha\)-policy cover using \(\operatorname{poly}(d,A,H,\log\lvert\Phi\rvert,\eta^{-1})\) episodes of interaction. This guarantee scales with the dimension \(d\) of the feature map and the complexity \(\log\lvert\Phi\rvert\) of the feature class but, critically, does not depend on the size of the state space \(\mathcal{X}\); by [10], dependence on \(H\) and \(A=\lvert\mathcal{A}\rvert\) is necessary when \(\phi^{*}\) is unknown. Given such a guarantee, we show in Appendix G how to optimize any downstream reward function to error \(\varepsilon\) with polynomial sample complexity.

Additional preliminaries.For any \(m,n\in\mathbb{N}\), we denote by \([m\,..\,n]\) the integer interval \(\{m,\ldots,n\}\). We also let \([n]\coloneqq[1\,..\,n]\). For any sequence of objects \(o_{1},o_{2},\ldots\), we define \(o_{m\textsc{i}}\coloneqq(o_{i})_{i\in[m\,..\,n]}\). A _partial policy_ is a policy defined over a contiguous subset of layers \([\ell\,..\,r]\subseteq[H]\). We denote by \(\Pi_{\mathsf{M}}^{\ell r}\coloneqq\{\pi:\cup_{h=\ell}^{r}\,\mathcal{X}_{h} \to\Delta(\mathcal{A})\}\) the set of all partial policies over layers \(\ell\) to \(r\); note that \(\Pi_{\mathsf{M}}\equiv\Pi_{\mathsf{M}}^{1:H}\). For a policy \(\pi\in\Pi_{\mathsf{M}}^{\ell r}\) and \(h\in[\ell\,..\,r]\), \(\pi(x_{h})\) denotes the action distribution for the policy at layer \(h\) when \(x_{h}\in\mathcal{X}_{h}\) is the current state. For \(1\leq t\leq h\leq H\) and any pair of partial policies \(\pi\in\Pi_{\mathsf{M}}^{1:t-1},\pi^{t}\in\Pi_{\mathsf{M}}^{t:h}\), we define \(\pi_{\circ}\,\pi^{\prime}\in\Pi_{\mathsf{M}}^{1:h}\) as the partial policy given by \((\pi\circ_{t}\pi^{\prime})(x_{\ell})=\pi(x_{\ell})\) for all \(\ell<t\) and \((\pi\circ_{t}\pi^{\prime})(x_{\ell})=\pi^{\prime}(x_{\ell})\) for all \(\ell\in\{\ldots h\}\).

We use the \(\bm{x}_{h}\sim\pi\) as shorthand to indicate that \(\bm{x}_{h}\) is drawn from the law \(\mathbb{P}^{\pi}\), and likewise for \((\bm{x}_{h},\bm{a}_{h})\sim\pi\) and so on. For a set of partial policies \(\Psi\coloneqq\{\pi^{(i)}:i\in[N]\}\), we define \(\mathsf{unif}(\Psi)\) as the random partial policy obtained by sampling \(\bm{i}\sim\mathsf{unif}([N])\) and playing \(\pi^{(\bm{i})}\). We define \(\pi_{\mathsf{unif}}\in\Pi_{\mathsf{M}}\) as the random policy that selects actions in \(\mathcal{A}\) uniformly at random at each layer. We use \(\left\lVert\cdot\right\rVert\) to denote the Euclidean norm, \(\left\lVert\cdot\right\rVert_{\infty}\) to denote the supremum norm on functions, and let \(\mathcal{B}(r)\in\mathbb{R}^{d}\) denote the Euclidean ball of radius \(r\). We refer to a scalar \(c>0\) as an _absolute constant_ to indicate that it is independent of all problem parameters and use \(\tilde{O}(\cdot)\) to denote a bound up to factors polylogarithmic in parameters appearing in the expression.

## 3SpanRL: Algorithm and Main Results

In this section, we present the SpanRL algorithm. We begin by describing challenges in deriving efficient, model-free algorithms using existing approaches (Section 3.1). We then formally describe SpanRL (Section 3.2) and build intuition as to how it is able to overcome these challenges, and finally state our main sample complexity guarantee (Section 3.3).

### Challenges and Related Work

Designing algorithms with provable guarantees in the Low-Rank MDP setting is challenging because of the complicated interplay between representation learning and exploration. Indeed, while there are many efficient algorithms for the so-called _linear MDP_ setting where the feature maps \((\phi_{h}^{*})_{h\in[H]}\) are known (removing the need for representation learning) [23, 47, 4, 41], these approaches do not readily generalize to accommodate unknown features. For Low-Rank MDPs, previous algorithms suffer from at least one of the following three drawbacks: (1) the algorithms are computationally inefficient; (2) the algorithms are model-based; or (3) the algorithms place strong assumptions on the MDP that are unlikely to hold in practice. To motivate the SpanRL algorithm, we briefly survey these results, highlighting several key challenges in avoiding these pitfalls.

Let us first discuss the issue of computational efficiency. While there are a number of algorithms--all based on the principle of _optimism in the face of uncertainty_--that provide tight sample complexity guarantees for Low-Rank MDPs in reward-based [21, 24, 14] and reward-free [9, 43] settings, these algorithms involve intractable optimization problems, and cannot be implemented efficiently even when the learner has access to an optimization oracle for the representation class \(\Phi\)[11]. This intractability arises because these algorithms implement optimism via a "global" approach, in which the algorithm explores at each round by choosing the most optimistic value function in a certain _version space_ of candidate value functions; optimizing over this version space is challenging,as it involves satisfying non-convex constraints with a complicated dependence on the learned representation, and because the constraints are coupled globally across layers \(h\in[H]\).

To avoid the intractability of global optimism, several works have restricted attention to a simpler _model-based_ setting. Here, in addition to assuming that the feature maps \((\phi_{h}^{\star})_{h\in[H]}\) are realizable with respect to \(\Phi\), one assumes access to a second feature class \(\Upsilon\) capable of modeling the mappings \((\mu_{h}^{\star})_{h\in[H]}\); this facilitates direct estimation of the transition probability kernel \(T_{h}(\cdot\mid x,a)\). For the model-based setting, it is possible to efficiently implement certain "local" forms of optimism [40, 10, 48], as well as certain non-optimistic exploration techniques based on policy covers [1]. Unfortunately, model-based realizability is a restrictive assumption, and falls short of the model-free guarantees we aim for in this work; indeed, in general, one cannot hope to estimate the feature map \(\mu_{h+1}^{\star}\) without sample complexity scaling with the number of states.4

Footnote 4: For example, in the special case of the Block MDP setting [12, 32], model-based realizability entails modeling a certain emission process, which is not required by model-free approaches.

When one moves from model-based learning to model-free learning, representation learning becomes substantially more challenging--both for optimistic and non-optimistic approaches. Here, a key challenge is to develop representation learning procedures that are (1) efficient, yet (2) provide meaningful guarantees when the learned features are used downstream for exploration. To our knowledge, the only proposal for a representation learning procedure satisfying both desiderata comes from the work of Modi et al. [35], who introduced a promising "minimax" representation learning objective (described in detail in the sequel; cf. Algorithm 5), which Zhang et al. [49] subsequently showed to have encouraging empirical performance. However, to provide guarantees for this objective, both works place substantial additional restrictions on the low-rank factorization. In particular, Modi et al. [35] make the so-called _latent variable_ assumption [1], which asserts that \(\phi_{h}^{\star}\) and \(\mu_{h}^{\star}\) are non-negative coordinate-wise, and Zhang et al. [49] further restrict to the Block MDP model [12, 32]. Non-negativity is a substantial restriction, as the best non-negative factorization can have exponentially large dimension relative to the best unrestricted factorization [1], even when reachability is assumed (cf. Appendix H.1). The source of this restriction is the problem of how to quantify how close a learned representation \(\hat{\phi}\) is to the ground truth \(\phi^{\star}\), which depends strongly on the downstream exploration strategy. In what follows, we show that with the right exploration strategy, this challenge can be ameliorated, but prior to our work it was unclear whether the minimax objective could lead to meaningful guarantees in the absence of non-negativity.

### The SpanRL Algorithm

Our algorithm, SpanRL, is presented in Algorithm 1. The algorithm proceeds by building a policy cover layer-by-layer in an inductive fashion. For each layer \(h\geq 2\), SpanRL uses a policy cover \(\Psi^{(h)}\) built at a previous iteration within a subroutine, RepLearn (Algorithm 5; deferred to Appendix B) to produce a feature map \(\hat{\phi}^{(h)}\) that approximates \(\phi_{h}^{\star}\). Using this feature map, the algorithm invokes a second subroutine, RobustSpanner (Algorithm 2 in Appendix B) to produce a collection of policies \(\pi_{1},\ldots,\pi_{d}\) that act as a _barycentric spanner_ for the feature map, ensuring maximal coverage in a certain sense; given these policies, a new policy cover for layer \(h+2\) is formed via \(\Psi^{(h+2)}=\{\pi_{i}\circ_{h+1}\pi_{\text{unif}}:i\in[d]\}\). To invoke the RobustSpanner subroutine, SpanRL makes use of additional subroutines for policy optimization (PSDP; Algorithm 3 in Appendix B) and estimation of certain vector-valued functionals (EstVec; Algorithm 7 in Appendix B). We now describe each component of the algorithm in detail, highlighting how they allow us to overcome the challenges in the prequel.

Barycentric spanners.At the heart of SpanRL is the notion of a _barycentric spanner_[6] as an efficient basis for exploration. We begin by defining a barycentric spanner for an abstract set.

**Definition 3.1** (Awerbuch and Kleinberg [6]).: _Given a set \(\mathcal{W}\in\mathbb{R}^{d}\) such that \(\operatorname{span}(\mathcal{W})=\mathbb{R}^{d}\), we say that a set \(\{w_{1},\ldots,w_{d}\}\subseteq\mathcal{W}\) is a \((C,\varepsilon)\)-approximate barycentric spanner for \(\mathcal{W}\) if for every \(w\in\mathcal{W}\), there exist \(\beta_{1},\ldots,\beta_{d}\in[-C,C]\) such that \(\|w-\sum_{i=1}^{d}\beta_{i}w_{i}\|\leq\varepsilon\).5_

Footnote 5: Note that our definition is a slight generalization of [6, Definition 2.1]; the latter is recovered with \(\varepsilon=0\).

The utility of barycentric spanners for reward-free exploration is highlighted in the following lemma.

**Lemma 3.1**.: _Suppose that Assumption 2.1 holds. If \(\Psi\subseteq\Pi_{\Psi}\) is a collection of policies such that \(\{\mathbb{E}^{\pi}\left[\phi_{h}^{\star}(\bm{x}_{h},\bm{a}_{h})\right]\mid\pi \in\Psi\}\subseteq\mathbb{R}^{d}\) is a \((C,\varepsilon)\)-approximate barycentric spanner for \(\mathcal{W}_{h}:=\{\mathbb{E}^{\pi}\left[\phi_{h}^{\star}(\bm{x}_{h},\bm{a}_{h })\right]\mid\pi\in\Pi_{\Psi}\}\) with \(\varepsilon\leq\frac{\eta}{2}\), then \(\Psi\) is an \(\alpha\)-policy cover for layer \(h+1\) with \(\alpha=(2dC)^{-1}\)._Lemma 3.1, proven in F.6, shows that to compute a policy cover for layer \(h+1\), it suffices to find a barycentric spanner for the set \(\mathcal{W}_{h}:=\{\mathbb{E}^{\pi}\left[\phi_{h}^{*}(\bm{x}_{h},\bm{a}_{h}) \right]|\;|\pi\in\Pi_{\mathsf{H}}\}\in\mathbb{R}^{d}\). Of course, even if \(\phi_{h}^{*}\) is known, this observation is only useful if we can compute a spanner without explicitly enumerating over the set \(\Pi_{\mathsf{H}}\), since our goal is to develop an _efficient_ algorithm. In what follows, we will show:6

Footnote 6: While barycentric spanners have been used in a number of recent works on sample-efficient RL [19, 20], the motivation for their use within our algorithm and analysis are quite different; see Appendix A.

1. Using, RobustSpanner, a novel adaptation of the classical algorithm of Awerbuch and Kleinberg [6], it holds that for any \(\phi\in\Phi\), spanner computation for the set \(\{\mathbb{E}^{\pi}\big{[}\phi(\bm{x}_{h},\bm{a}_{h})\big{]}\;|\;\pi\in\Pi_{ \mathsf{H}}\}\) can be performed efficiently whenever, for any \(\theta\in\mathcal{B}(1)\), one can (approximately) solve linear optimization problems of the form \[\operatorname*{arg\,max}_{\pi\in\Pi_{\mathsf{H}}}\mathbb{E}^{\pi}\big{[} \theta^{\top}\phi(\bm{x}_{h},\bm{a}_{h})\big{]}.\] (5)
2. Given access to policy covers \(\Psi^{(1:h)}\) for layers \(1\) to \(h\), one can efficiently solve the optimization problem in (5) by appealing to the PSDP algorithm for policy optimization (Algorithm 3).

To handle the fact that \(\phi_{h}^{*}\) is unknown, Algorithm 1 computes policies \(\pi_{1:d}\) that induce a barycentric spanner for the set \(\{\mathbb{E}^{\pi}\big{[}\hat{\phi}^{(h)}(\bm{x}_{h},\bm{a}_{h})\big{]}\;|\; \pi\in\Pi_{\mathsf{H}}\}\), where \(\hat{\phi}^{(h)}\in\Phi\) is a learned feature map. In what follows, we first give a detailed explanation of the two above points, before showing how to complete the argument by learning a feature map through representation learning.

```
0: Feature class \(\Phi\) and parameters \(\varepsilon,\mathsf{c}>0\) and \(\delta\in(0,1)\).
1: Set \(\Psi^{(1)}=\varnothing\), \(\Psi^{(2)}=\{\pi_{\mathsf{uni}\mathsf{f}}\}\).
2: Set \(n_{\mathsf{RepLearn}}=\epsilon\cdot\varepsilon^{-2}A^{2}d\log(|\Phi|/\delta)\) and \(n_{\mathsf{EstVec}}=\epsilon\cdot\varepsilon^{-2}\log(1/\delta)\).
3: Set \(n_{\mathsf{PSOP}}=\mathsf{c}\cdot\varepsilon^{-2}A^{2}d^{3}H^{2}\cdot(d+\log(| \Phi|/\delta))\).
4: Define \(\mathcal{F}\coloneqq\{f:x\mapsto\max_{a\in\mathcal{A}}\theta^{\top}\phi(x,a) \;|\;\theta\in\mathcal{B}(1),\phi\in\Phi\}\).
5: Define \(\mathcal{G}=\{g:(x,a)\mapsto\phi(x,a)^{\top}w\;|\;\phi\in\Phi,w\in\mathcal{B}( 2\sqrt{d})\}\).
6:for\(h=1,\dots,H-2\)do /* Learn feature representation for layer \(h\). */
7: Set \(\phi^{(h)}=\mathsf{RepLearn}(h,\mathcal{F},\Phi,P^{(h)},n_{\mathsf{RepLearn}})\), with \(P^{(h)}=\mathsf{unif}(\Psi^{(h)})\). //Algorithm 6. /* Computing an approximate spanner using learned features. */
8: For \(\theta\in\mathbb{R}^{d}\) and \((x,a)\in\mathcal{X}\times\mathcal{A}\), define \[r_{t}(x,a;\theta):=\left\{\begin{array}{ll}\phi^{(h)}(x,a)^{\top}\theta,& \text{for $t=h$},\\ 0,&\text{otherwise}.\end{array}\right.\]
9: For each \(t\in[h]\), set \(\mathcal{G}_{t}=\mathcal{G}\) and \(P^{(t)}=\mathsf{unif}(\Psi^{(t)})\).
10: For \(\theta\in\mathbb{R}^{d}\), define \(\mathsf{LinOpt}(\theta)=\mathsf{PSOP}(h,r_{1:h}(\cdot,\cdot;\theta),\mathcal{G} _{1:h},P^{(1:h)},n_{\mathsf{PSOP}})\). //Algorithm 3.
11: For \(\pi\in\Pi_{\mathsf{H}}\), define \(\mathsf{LinEst}(\pi)=\mathsf{EstVec}(h,\phi^{(h)},\pi,n_{\mathsf{EstVec}})\). //Algorithm 7.
12: Set \((\pi_{1},\dots,\pi_{d})=\mathsf{RobustSpanner}(\mathsf{LinOpt}(\cdot), \mathsf{LinEst}(\cdot),2,\varepsilon)\). //Algorithm 2.
13: Set \(\Psi^{(h+2)}=\{\pi_{i}\circ_{h+1}\pi_{\mathsf{uni}\mathsf{f}}:i\in[d]\}\).
14:Return: Policy cover \(\Psi^{(1:H)}\). ```

**Algorithm 1**SpanRL: Volumetric Exploration and Representation Learning via Barycentric Spanner

Barycentric spanner computation via approximate linear optimization.To describe spanner computation in SpanRL, we take a brief detour and consider an abstract approach to barycentric spanner computation, which generalizes our problem. Suppose that we wish to compute a spanner for an implicitly specified set \(\mathcal{W}=\left\{w^{z}\right\}_{z\in\mathcal{Z}}\in\mathbb{R}^{d}\) indexed by an abstract set \(\mathcal{Z}\). The set \(\mathcal{Z}\) (which will be set to \(\Pi_{\mathsf{H}}\) when we return to RL) may be exponentially large and cannot be efficiently enumerated. In addition, given \(z\in\mathcal{Z}\), we cannot explicitly compute \(w^{z}\), and have to settle for a noisy approximation.

To allow for efficient spanner computation, we assume access to two oracles for the set \(\mathcal{W}\), a _linear optimization_ oracle \(\mathsf{LinOpt}:\mathcal{B}(1)\rightarrow\mathcal{Z}\) and an _index-to-vector_ oracle \(\mathsf{LinEst}:\mathcal{Z}\rightarrow\mathbb{R}^{d}\). We assume that for some \(\varepsilon>0\):1. For all \(\theta\in\mathbb{R}^{d}\) with \(\|\theta\|=1\), the output \(\hat{z}_{\theta}:=\texttt{LinOpt}(\theta)\) satisfies \(\theta^{\intercal}w^{z_{\theta}}\geq\sup_{z\in\mathcal{Z}}\theta^{\intercal}w^{ z}-\varepsilon\).
2. For all \(z\in\mathcal{Z}\), the output \(\hat{w}_{z}:=\texttt{LinEst}(z)\) satisfies \(\|\hat{w}_{z}-w^{z}\|\leq\varepsilon\).

The RobustSpanner algorithm (Algorithm 2) computes a \((C,\varepsilon)\)-approximate spanner for \(\mathcal{W}\) using \(O(d\log(d/\varepsilon))\) total calls to LinOpt and LinEst. RobustSpanner is an error-tolerant variant of the classical spanner computation algorithm of Awerbuch and Kleinberg [6], which was originally introduced and analyzed for spanner computation with an _exact_ linear optimization oracle. Tolerance to approximation errors in the linear optimization oracle is critical for our application to RL, where additive errors will arise in sampling trajectories, as well as estimating the feature maps \((\phi_{h}^{\star})_{h\in[H]}\). RobustSpanner achieves error tolerance by perturbing the vectors returned by \(\texttt{LinOpt}(\theta)\) in the direction of \(\theta\), which amounts to running the classical algorithm on an \(\varepsilon\)-fattening of \(\mathcal{W}\), and is necessary in order to ensure that the approximation error of LinOpt does not swamp the signal in directions \(\theta\) in which \(\mathcal{W}\) is too "skinny." This technique may be of independent interest; see Appendix C for additional details and formal guarantees.

Representation learning.Ideally, we would like to use RobustSpanner to construct a barycentric spanner for the set \(\{\mathbb{E}^{\pi}[\phi_{h}^{\star}(\bm{x}_{h},\bm{a}_{h})]\mid\pi\in\Pi_{ \mathsf{H}}\}\) with \(\mathcal{Z}=\Pi_{\mathsf{H}}\). Because we do not have access to \(\phi_{h}^{\star}\), we instead apply RobustSpanner with \(\mathcal{W}\coloneqq\{\mathbb{E}^{\pi}[\hat{\phi}^{(h)}(\bm{x}_{h},\bm{a}_{h}) ]\mid\pi\in\Pi_{\mathsf{H}}\}\), where \(\hat{\phi}^{(h)}\) is a learned representation. We now describe how the feature map \(\hat{\phi}^{(h)}\) is learned, then show how to use these learned features to efficiently implement the oracles \(\texttt{LinOpt}(\cdot)\) and \(\texttt{LinEst}(\cdot)\).

To learn a representation for layer \(h\), we use the Replearn algorithm (Algorithm 5), which was originally introduced in Modi et al. [35], Zhang et al. [49]. The algorithm gathers a collection of triples \((\bm{x}_{h},\bm{a}_{h},\bm{x}_{h+1})\) by rolling in to \(\bm{x}_{h}\) with a policy sampled uniformly from the policy cover \(\Psi^{(h)}\) and selecting \(\bm{a}_{h}\) uniformly at random. Using this dataset, the algorithm solves a sequence of adversarial training sub-problems (Line 9 of Algorithm 5) which involve the feature class \(\Phi\) and an auxiliary discriminator class \(\mathcal{F}:\mathcal{X}\rightarrow\mathbb{R}\). As we discuss in detail in the sequel, these sub-problems, described in (7), are amenable to standard gradient-based training methods. The sub-problems are designed to approximate the following "idealized" max-min-max representation learning objective:

\[\hat{\phi}^{(h)}\in\operatorname*{arg\,min}_{\phi\in\Phi}\sup_{f\in\mathcal{ F}}\inf_{w}\mathbb{E}^{\mathsf{unif}(\Psi^{(h)})_{h}\pi_{\mathsf{unif}}} \left[\left(\phi(\bm{x}_{h},\bm{a}_{h})^{\intercal}w-\mathbb{E}[f(\bm{x}_{h+1} )\mid\bm{x}_{h},\bm{a}_{h}]\right)^{2}\right].\] (6)

The intuition for this objective comes from the fact that in a Low-Rank MDP, for any function \(f:\mathcal{X}\rightarrow\mathbb{R}\), the quantity \(\mathbb{E}[f(\bm{x}_{h+1})\mid\bm{x}_{h}=x,\bm{a}_{h}=a]\) is linear in \(\phi_{h}^{\star}(x,a)\). Thus, if \(\mathcal{F}\) is sufficiently expressive, we may hope that \(\hat{\phi}^{(h)}\) and \(\phi^{\star}\) are close. We adopt the simple discriminator class \(\mathcal{F}=\{\,x\mapsto\max_{a\in\mathcal{A}}\theta^{\intercal}\phi(x,a)\mid \theta\in\mathcal{B}(1),\,\phi\in\Phi\}\). We show that solving (6) with this choice for \(\mathcal{F}\), which is simpler than that in Modi et al. [35], Zhang et al. [49], yields an approximation guarantee for \(\hat{\phi}^{(h)}\) that is suitable for downstream use in spanner computation for general Low-Rank MDPs.

**Remark 3.1** (Improved analysis of Replearn).: _To facilitate an analysis of_ SpanRL _that does not require reachability assumptions, we use slightly different parameter values for_ Replearn _than in Modi et al. [35], Zhang et al. [49], and provide a tighter sample complexity bound (Theorem E.1) which may be of independent interest._

_In more detail, prior work shows that the_ Replearn _algorithm solves a variant of (6) with \(w\in\mathcal{B}(d^{1/2}\cdot\mathrm{poly}(\varepsilon^{-1}))\), where \(\varepsilon>0\) is the desired bound on mean-squared error. Due to the polynomial dependence on \(\varepsilon^{-1}\), such a guarantee would lead to vacuous guarantees when invoked within our analysis of_ SpanRL_. Our improved analysis of_ Replearn_, which is based on a determinantal potential argument, shows that \(w\in\mathcal{B}(\mathrm{poly}(d))\) suffices. A secondary benefit of our improved bound is a faster rate with respect to the number of trajectories._

Putting everything together.Having learned \(\hat{\phi}^{(h)}\) using Replearn, in SpanRL we apply RobustSpanner with \(\mathcal{W}:=\{\mathbb{E}^{\pi}[\hat{\phi}^{(h)}(\bm{x}_{h},\bm{a}_{h})]\mid \pi\in\Pi_{\mathsf{H}}\}\), \(\mathcal{Z}=\Pi_{\mathsf{H}}\), and \(C=2\); that is, we plug-in the learned representation \(\hat{\phi}^{(h)}\) for the true representation \(\phi_{h}^{\star}\).7 With this choice, implementing LinOpt entails (approximately) solving \(\operatorname*{arg\,max}_{\pi\in\Pi_{\mathsf{H}}}\mathbb{E}^{\pi}[\theta^{ \intercal}\hat{\phi}^{(h)}(\bm{x}_{h},\bm{a}_{h})]\) for a given \(\theta\in\mathcal{B}(1)\), and implementing the LinEst oracle entails estimating \(\mathbb{E}^{\pi}[\hat{\phi}^{(h)}(\bm{x}_{h},\bm{a}_{h})]\) for a given \(\pi\in\Pi_{\mathsf{H}}\). We instantiate\(\texttt{LinEst}(\pi)\) as the Monte Carlo algorithm \(\texttt{EstVec}\) (Algorithm 7), which simply samples trajectories according to \(\pi\) and returns the sample average of \(\hat{\phi}^{(h)}(\bm{x}_{h},\bm{a}_{h})\). To implement \(\texttt{LinOpt}(\theta)\), we appeal to \(\texttt{PSDP}\) (Algorithm 3). \(\texttt{PSDP}\), given an arbitrary reward function \(r_{1:h}:\mathcal{X}\times\mathcal{A}\to\mathbb{R}\) and a function class \(\mathcal{G}\subseteq\{g:\mathcal{X}\times\mathcal{A}\to\mathbb{R}\}\) capable of realizing all possible value functions induced by these rewards, can use the policy covers \(\Psi^{(1:h)}\) to efficiently compute a policy \(\hat{\pi}=\texttt{PSDP}(h,r_{1:h},\mathcal{G},\Psi^{(1:h)},n)\) that approximately solves \(\arg\max_{\pi\in\texttt{IL}}\mathbb{E}^{\pi}[\sum_{t=1}^{h}r_{t}(\bm{x}_{t}, \bm{a}_{t})]\), and does so using polynomially many episodes; see Appendix D for details and formal guarantees.8 Thus, implementing \(\texttt{LinOpt}(\theta)\) is as simple as invoking \(\texttt{PSDP}\) with the rewards

Footnote 8: This is the main place where the analysis uses the inductive hypothesis that \(\Psi^{(1:h)}\) are policy covers.

\[r_{t}(x,a;\theta):=\left\{\begin{array}{ll}\hat{\phi}^{(h)}(x,a)^{\top} \theta,&\text{for $t=h$},\\ 0,&\text{otherwise}.\end{array}\right.\]

With this, we have all the ingredients needed for spanner computation, and the algorithm is complete.

### Main Guarantee for SpanRL

The following result is the main sample complexity guarantee for \(\texttt{SpanRL}\) (Algorithm 1).

**Theorem 3.2** (Main theorem for \(\texttt{SpanRL}\)).: _Let \(\delta\in(0,1)\) be given, and suppose that realizability holds (Assumption 2.2) and that reachability (Assumption 2.1) is satisfied with parameter \(\eta>0\). If \(\varepsilon=\frac{\eta}{36d^{5/2}}\) and \(\mathfrak{c}=\mathrm{polylog}(A,H,d,\log(|\Phi|/\delta))\) is sufficiently large, then the policies \(\Psi^{(1:H)}\) produced by \(\texttt{SpanRL}(\Phi,\varepsilon,\mathfrak{c},\delta)\) are a \(\big{(}\frac{1}{4Ad},0\big{)}\)-policy cover with probability at least \(1-\delta\). The total number of episodes used by \(\texttt{SpanRL}\) is at most:_

\[\widetilde{O}\left(A^{4}d^{9}H^{4}(d+\log(|\Phi|/\delta))\cdot 1/\eta^{2} \right).\]

Theorem 3.2 is the first provable, model-free sample complexity guarantee for general Low-Rank MDPs that is attained by an efficient algorithm. Prior to our work, all efficient model-free algorithms required non-negative features (latent variable structure) or stronger assumptions [35, 49], even in the presence of similar reachability assumptions; see Table 1.

**Remark 3.2** (On the reachability assumption).: _While the reachability assumption is shared by the best prior efficient algorithms [35], which require non-negativity in addition to this assumption, it is natural to ask to what extent reachability restricts the generality of the Low-Rank MDP model. In Appendix H, we show that even when reachability holds, the embedding dimension in our model can be exponentially smaller than the best embedding dimension for the best non-negative (latent variable) embedding [35]. Hence, our results are meaningfully more general than prior work._

While our guarantee is polynomial in all relevant problem parameters, improving the dependence further (e.g., to match that of the best known inefficient algorithms) is an interesting direction for future research, as is removing the reachability assumption.

Application to reward-based RL.By using the policy cover produced by \(\texttt{SpanRL}\) within \(\texttt{PSDP}\) (Algorithm 3), we can optimize any downstream reward function to error \(\varepsilon\) using \(\mathrm{poly}(d,A,H,\log|\Phi|)\cdot 1/\varepsilon^{2}\) episodes. See Appendix G for details.

Efficiency and practicality.We observe that \(\texttt{SpanRL}\) is simple and practical. Defining \(\mathcal{L}_{\mathcal{D}}(\phi,w,f):=\sum_{(x,a,x^{\prime})\in\mathcal{D}}( \phi(x,a)^{\top}w-f(x^{\prime}))^{2}+\lambda\|w\|^{2}\), where \(\mathcal{D}\) is a dataset consisting of \((\bm{x}_{h},\bm{a}_{h},\bm{r}_{h},\bm{x}_{h+1})\) tuples, the algorithm is provably efficient whenever the adversarial objective

\[f^{(t)}\in\operatorname*{arg\,max}_{f\in\mathcal{F}}\max_{\phi\in\Phi}\left\{ \min_{w}\mathcal{L}_{\mathcal{D}}(\phi^{(t)},w,f)-\min_{\tilde{w}}\mathcal{L}_{ \mathcal{D}}\big{(}\tilde{\phi},\tilde{w},f\big{)}\right\},\] (7)

in Line 9 of \(\texttt{RepLearn}\) (Algorithm 5), can be implemented efficiently (note that by the definition of \(\mathcal{L}_{\mathcal{D}}\), the "inner" minima over \(w\) and \(\tilde{w}\) in (7) can be solved in closed form). This objective was also assumed to be efficiently solvable in Modi et al. [35], Zhang et al. [49] and was empirically shown to be practical in [49]; note that the objective is amenable to standard gradient-based optimization techniques, and that \(\mathcal{F}\) can be over-parameterized. While a detailed experimental evaluation is outside of the scope of this paper, we are optimistic about the empirical performance of the algorithm in light of the encouraging results based on the same objective in Zhang et al. [49]Outside of representation learning, the only overhead in SpanRL is the RobustSpanner subroutine, which has polynomial runtime. Indeed, RobustSpanner requires only polynomially many calls to the linear optimization oracle, instantiated as PSDP, which is efficient whenever standard least-squares regression problems based on the class \(\Phi\) can be solved efficiently, analogous to [33, 31].

Analysis and proof techniques.The proof of Theorem3.2, which is given in AppendixF, is appealing in its simplicity and modularity. The crux of the proof is to show that the representation learning guarantee in (6) is strong enough to ensure that the downstream spanner computation in RobustSpanner succeeds. It is straightforward to show that spanner computation would succeed if we had access to an estimated representation that \(\hat{\phi}^{(h)}\) that approximates \(\phi^{*}_{h}\) point-wise (i.e., uniformly for all \((x,a)\) pairs), but the key challenge is that the guarantee in (6) only holds _on average_ under the roll-in distribution \(\texttt{unif}(\Psi^{(h)})\). Prior works that make use of the same representation learning objective (BRIEE[49] and MOFFLE[35]) do not make use of spanners; instead, they appeal to exploration strategies based on elliptic bonuses, addressing the issue of approximation errors through additional assumptions (non-negativity of the factorization for MOFFLE, and Block MDP structure for BRIEE). Perhaps the most important observation in our proof is that barycentric spanners are robust to the average-case approximation error guarantee in (6) as-is, without additional structural assumptions. Intuitively, this benefit seems to arise from the fact that the spanner property only concerns the _first moment_ of the feature map \(\phi^{*}\), while algorithms based on elliptic bonuses require approximation guarantees for the _second moment_; understanding this issue more deeply is an interesting question for future work. Another useful feature of our proof is to show that the notion of reachability in Assumption2.1, which generalizes and extends all previous reachability conditions in the Low-Rank MDP and Block MDP literature [46, 5, 13, 33, 1, 35, 31], is sufficient to build a policy cover. We anticipate that this observation will find broader use.

## Acknowledgments and Disclosure of Funding

We thank Noah Golowich, Dhruv Rohatgi, and Ayush Sekhari for several helpful discussions. ZM and AR acknowledge support from the ONR through awards N00014-20-1-2336 and N00014-20-1-2394, and ARO through award W911NF-21-1-0328. AB acknowledges support from the National Science Foundation Graduate Research Fellowship under Grant No. 1122374.

## References

* [1] A. Agarwal, S. Kakade, A. Krishnamurthy, and W. Sun. FLAMBE: Structural complexity and representation learning of low rank MDPs. _arXiv preprint arXiv:2006.10814_, 2020.
* [2] A. Agarwal, S. Kakade, and L. F. Yang. Model-based reinforcement learning with a generative model is minimax optimal. In _Conference on Learning Theory_, pages 67-83. PMLR, 2020.
* [3] A. Agarwal, S. M. Kakade, A. Krishnamurthy, and W. Sun. FLAMBE: structural complexity and representation learning of low rank mdps. In _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* [4] A. Agarwal, Y. Jin, and T. Zhang. Vo \(q\) l: Towards optimal regret in model-free rl with nonlinear function approximation. _arXiv preprint arXiv:2212.06069_, 2022.
* [5] A. Agarwal, Y. Song, W. Sun, K. Wang, M. Wang, and X. Zhang. Provable benefits of representational transfer in reinforcement learning. _CoRR_, abs/2205.14571, 2022.
* [6] B. Awerbuch and R. Kleinberg. Online linear optimization and adaptive routing. _Journal of Computer and System Sciences_, 74(1):97-114, 2008.
* [7] M. G. Azar, I. Osband, and R. Munos. Minimax regret bounds for reinforcement learning. In _International Conference on Machine Learning_, pages 263-272, 2017.
* [8] J. Bagnell, S. M. Kakade, J. Schneider, and A. Ng. Policy search by dynamic programming. _Advances in neural information processing systems_, 16, 2003.
* [9] F. Chen, Y. Bai, and S. Mei. Partially observable rl with b-stability: Unified structural condition and sharp sample-efficient algorithms. _arXiv preprint arXiv:2209.14990_, 2022.
* [10] Y. Cheng, R. Huang, Y. Liang, and J. Yang. Improved sample complexity for reward-free reinforcement learning under low-rank mdps. In _The Eleventh International Conference on Learning Representations_, 2023.
* [11] C. Dann, N. Jiang, A. Krishnamurthy, A. Agarwal, J. Langford, and R. E. Schapire. On oracle-efficient PAC RL with rich observations. In _Advances in neural information processing systems_, pages 1422-1432, 2018.
* [12] S. S. Du, A. Krishnamurthy, N. Jiang, A. Agarwal, M. Dudik, and J. Langford. Provably efficient RL with rich observations via latent state decoding. _arXiv preprint arXiv:1901.09018_, 2019.
* [13] S. S. Du, Y. Luo, R. Wang, and H. Zhang. Provably efficient Q-learning with function approximation via distribution shift error checking oracle. In _Advances in Neural Information Processing Systems_, pages 8060-8070, 2019.
* [14] S. S. Du, S. M. Kakade, J. D. Lee, S. Lovett, G. Mahajan, W. Sun, and R. Wang. Bilinear classes: A structural framework for provable generalization in RL. _arXiv preprint arXiv:2103.10897_, 2021.
* [15] Y. Efroni, D. Misra, A. Krishnamurthy, A. Agarwal, and J. Langford. Provably filtering exogenous distractors using multistep inverse dynamics. In _International Conference on Learning Representations_, 2021.
* [16] D. Ernst, P. Geurts, and L. Wehenkel. Tree-based batch mode reinforcement learning. _Journal of Machine Learning Research_, 6, 2005.
* [17] D. J. Foster, S. M. Kakade, J. Qian, and A. Rakhlin. The statistical complexity of interactive decision making. _arXiv preprint arXiv:2112.13487_, 2021.
* [18] D. J. Foster, N. Golowich, and Y. Han. Tight guarantees for interactive decision making with the decision-estimation coefficient. _Conference on Learning Theory (COLT)_, 2023.

* [19] N. Golowich, A. Moitra, and D. Rohatgi. Learning in observable pomdps, without computationally intractable oracles. In _Advances in Neural Information Processing Systems_, 2022.
* [20] A. Huang, J. Chen, and N. Jiang. Reinforcement learning in low-rank mdps with density features. _International Conference on Machine Learning (ICML)_, 2023.
* [21] N. Jiang, A. Krishnamurthy, A. Agarwal, J. Langford, and R. E. Schapire. Contextual decision processes with low Bellman rank are PAC-learnable. In _International Conference on Machine Learning_, pages 1704-1713, 2017.
* [22] C. Jin, P. Netrapalli, R. Ge, S. M. Kakade, and M. I. Jordan. A short note on concentration inequalities for random vectors with subgaussian norm. _arXiv preprint arXiv:1902.03736_, 2019.
* [23] C. Jin, Z. Yang, Z. Wang, and M. I. Jordan. Provably efficient reinforcement learning with linear function approximation. In _Conference on Learning Theory_, pages 2137-2143, 2020.
* [24] C. Jin, Q. Liu, and S. Miryoosefi. Bellman eluder dimension: New rich classes of RL problems, and sample-efficient algorithms. _arXiv preprint arXiv:2102.00815_, 2021.
* [25] S. M. Kakade. _On the sample complexity of reinforcement learning_. University of London, University College London (United Kingdom), 2003.
* [26] J. Kober, J. A. Bagnell, and J. Peters. Reinforcement learning in robotics: A survey. _The International Journal of Robotics Research_, 32(11):1238-1274, 2013.
* [27] J. Li, W. Monroe, A. Ritter, D. Jurafsky, M. Galley, and J. Gao. Deep reinforcement learning for dialogue generation. In _EMNLP_, 2016.
* [28] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep reinforcement learning. _arXiv preprint arXiv:1509.02971_, 2015.
* [29] Z. Mhammedi, D. J. Foster, M. Simchowitz, D. Misra, W. Sun, A. Krishnamurthy, A. Rakhlin, and J. Langford. Learning the linear quadratic regulator from nonlinear observations. _Advances in Neural Information Processing Systems_, 33:14532-14543, 2020.
* [30] Z. Mhammedi, A. Block, D. J. Foster, and A. Rakhlin. Efficient model-free exploration in low-rank mdps. _arXiv preprint arXiv:2307.03997_, 2023.
* [31] Z. Mhammedi, D. J. Foster, and A. Rakhlin. Representation learning with multi-step inverse kinematics: An efficient and optimal approach to rich-observation rl. _International Conference on Machine Learning (ICML)_, 2023.
* [32] D. Misra, M. Henaff, A. Krishnamurthy, and J. Langford. Kinematic state abstraction and provably efficient rich-observation reinforcement learning. _arXiv preprint arXiv:1911.05815_, 2019.
* [33] D. Misra, M. Henaff, A. Krishnamurthy, and J. Langford. Kinematic state abstraction and provably efficient rich-observation reinforcement learning. In _International conference on machine learning_, pages 6961-6971. PMLR, 2020.
* [34] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning. _Nature_, 518(7540):529, 2015.
* [35] A. Modi, J. Chen, A. Krishnamurthy, N. Jiang, and A. Agarwal. Model-free representation learning and exploration in low-rank mdps. _CoRR_, abs/2102.07035, 2021.
* [36] S. Rendle, C. Freudenthaler, and L. Schmidt-Thieme. Factorizing personalized markov chains for next-basket recommendation. In _Proceedings of the 19th International Conference on World Wide Web, WWW 2010, Raleigh, North Carolina, USA, April 26-30, 2010_, pages 811-820. ACM, 2010.
* [37] D. Russo and B. Van Roy. Eluder dimension and the sample complexity of optimistic exploration. In _Advances in Neural Information Processing Systems_, pages 2256-2264, 2013.
* [38] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. _nature_, 529(7587):484, 2016.
* [39] W. Sun, N. Jiang, A. Krishnamurthy, A. Agarwal, and J. Langford. Model-based RL in contextual decision processes: PAC bounds and exponential improvements over model-free approaches. In _Conference on learning theory_, pages 2898-2933. PMLR, 2019.

* [40] M. Uehara, X. Zhang, and W. Sun. Representation learning for online and offline RL in low-rank mdps. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_, 2022.
* [41] R. Wang, S. S. Du, L. Yang, and R. R. Salakhutdinov. On reward-free reinforcement learning with linear function approximation. _Advances in neural information processing systems_, 33:17816-17826, 2020.
* [42] R. Wang, R. Salakhutdinov, and L. F. Yang. Provably efficient reinforcement learning with general value function approximation. _arXiv preprint arXiv:2005.10804_, 2020.
* [43] T. Xie, D. J. Foster, Y. Bai, N. Jiang, and S. M. Kakade. The role of coverage in online reinforcement learning. _arXiv preprint arXiv:2210.04157_, 2022.
* [44] L. Yang and M. Wang. Sample-optimal parametric q-learning using linearly additive features. In _Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA_, volume 97 of _Proceedings of Machine Learning Research_, pages 6995-7004. PMLR, 2019.
* [45] H. Yao, C. Szepesvari, B. A. Pires, and X. Zhang. Pseudo-mdps and factored linear action models. In _2014 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning, ADPRL 2014, Orlando, FL, USA, December 9-12, 2014_, pages 1-9. IEEE, 2014.
* [46] A. Zanette, A. Lazaric, M. J. Kochenderfer, and E. Brunskill. Provably efficient reward-agnostic navigation with linear value iteration. _Advances in Neural Information Processing Systems_, 33:11756-11766, 2020.
* [47] T. Zhang. Feel-good thompson sampling for contextual bandits and reinforcement learning. _arXiv preprint arXiv:2110.00871_, 2021.
* [48] T. Zhang, T. Ren, M. Yang, J. Gonzalez, D. Schuurmans, and B. Dai. Making linear mdps practical via contrastive representation learning. In _International Conference on Machine Learning_, pages 26447-26466. PMLR, 2022.
* [49] X. Zhang, Y. Song, M. Uehara, M. Wang, A. Agarwal, and W. Sun. Efficient reinforcement learning in block mdps: A model-free representation learning approach. In _International Conference on Machine Learning_, pages 26517-26547. PMLR, 2022.

###### Contents of Appendix

* A Additional Related Work
* B Omitted Algorithms
* C Generic Guarantee for RobustSpanner
* D Generic Guarantee for PSDP
* E Guarantee for RepLearn
* F Analysis
* F.1 Proof Strategy
* F.2 Guarantee for PSDP as a Subroutine for RobustSpanner
* F.3 Guarantee for RobustSpanner as a Subroutine for SpanRL
* F.4 Guarantee for RepLearn as a Subroutine for SpanRL
* F.5 Concluding the Proof of Theorem 3.2
* F.6 Proof of Lemma 3.1
* G Application to Reward-Based RL
* H Properties of Reachability Assumption
* H.1 Comparison to Latent Variable Model
* H.2 Relation to Other Reachability Assumptions

## Appendix A Additional Related Work

In this section, we discuss relevant related work not already covered.

Block MDPs.A particularly well-studied special case low-rank MDPs with the latent variable assumed in Modi et al. [35] (defined in Definition H.1) is the _Block MDP (BMDP) model_ Du et al. [13], Misra et al. [32], Zhang et al. [49], Mhammedi et al. [31]. For this setting, Du et al. [13], Misra et al. [32] provide algorithms that conduct exploration in a provably oracle-efficient manner under a reachability assumption. This reachability assumption was removed by subsequent work of Zhang et al. [49] (with a suboptimal rate) and Mhammedi et al. [31] (with optimal error dependence), but the analysis in these works is tailored to the BMDP model.

Barycentric spanners.Huang et al. [20] consider a variant of the Low-Rank MDP framework in which we are given a class \(\Upsilon\) that realizes the next-state feature map \(\mu^{\star}\), but do not have access to a class \(\Phi\) for the feature map \(\phi^{\star}\), which is unknown. Their algorithm, like \(\mathtt{SpanRL}\), is based on barycentric spanners, though the algorithm design considerations and analysis are significantly different. Notably, their algorithm is not computationally efficient, and their analysis takes advantage of the fact that realizability of \(\mu^{\star}\) facilitates estimation of the occupancies \(\{d^{\pi}(\cdot)\}_{\pi\in\Pi_{n}}\) in \(\ell_{1}\)-error. Barycentric spanners were also in the work of Golowich et al. [19] for reinforcement learning in Partially Observable MDPs (POMDPs). Their analysis is substantially different from ours, and their algorithm appeals to the barycentric spanner computation approach in Awerbuch and Kleinberg [6] in an off-the-shelf fashion.

Omitted Algorithms

```
1:
2:Approximate linear optimization subroutine \(\texttt{LinOpt}:\mathbb{R}^{d}\rightarrow\mathcal{Z}\). /* See Section 3.2 */
3:Approximate index-to-vector subroutine \(\texttt{LinEst}:\mathcal{Z}\rightarrow\mathbb{R}^{d}\).
4:Parameters \(C,\varepsilon>0\).
5:Set \(W=(w_{1},\dots,w_{d})=(e_{1},\dots,e_{d})\).
6:for\(i=1,\dots,d\)do
7:Set \(\theta_{i}=(\det(e_{j},W_{-i}))_{j\in[d]}\in\mathbb{R}^{d}\). //\(W_{-i}\) is defined to be \(W\) without the \(i\)th column
8:Set \(z_{i}^{+}=\texttt{LinOpt}(\theta_{i}/\big{\lvert}\theta_{i}\big{\lvert}\big{\lvert}\) and \(w_{i}^{+}=\texttt{LinEst}(z_{i}^{+})\).
9:Set \(z_{i}^{-}=\texttt{LinOpt}(-\theta_{i}/\big{\lvert}\theta_{i}\big{\lvert}\big{\lvert}\)) and \(w_{i}^{-}=\texttt{LinEst}(z_{i}^{-})\).
10:if\(\theta_{i}^{+}w_{i}^{+}\geq-\theta_{i}^{+}w_{i}^{-}\)then
11:Set \(\widetilde{w}_{i}=w_{i}^{+}\), \(z_{i}=z_{i}^{+}\), and \(w_{i}=\widetilde{w}_{i}+\varepsilon\theta_{i}/\big{\lvert}\theta_{i}\big{\lVert}\).
12:else
13:Set \(\widetilde{w}_{i}=w_{i}^{-}\), \(z_{i}=z_{i}^{-}\), and \(w_{i}=\widetilde{w}_{i}-\varepsilon\theta_{i}/\big{\lvert}\theta_{i}\big{\lVert}\).
14:for\(n=1,2,\dots\)do
15:Set \(i=1\).
16:while\(i\leq d\)do
17:Set \(\theta_{i}=\left(\det(e_{j},W_{-i})\right)_{j\in[d]}\in\mathbb{R}^{d}\).
18:Set \(z_{i}^{+}=\texttt{LinOpt}(\theta_{i}/\big{\lvert}\theta_{i}\big{\lvert}\big{\lvert}\) and \(w_{i}^{+}=\texttt{LinEst}(z_{i}^{+})\).
19:Set \(z_{i}^{-}=\texttt{LinOpt}(-\theta_{i}/\big{\lvert}\theta_{i}\big{\lvert}\big{\lvert}\) and \(w_{i}^{-}=\texttt{LinEst}(z_{i}^{-})\).
20:if\(\theta_{i}^{+}w_{i}^{+}+\varepsilon\cdot\big{\lvert}\theta_{i}\big{\lvert} \big{\lvert}\geq C\cdot\det(w_{i},W_{-i})\big{\lvert}\)then
21:Set \(\widetilde{w}_{i}=w_{i}^{+}\), \(z_{i}=z_{i}^{+}\), and \(w_{i}=\widetilde{w}_{i}+\varepsilon\cdot\theta_{i}/\big{\lvert}\theta_{i} \big{\lVert}\).
22:break
23:elseif\(-\theta_{i}^{+}w_{i}^{-}+\varepsilon\cdot\big{\lvert}\theta_{i}\big{\lvert} \geq C\cdot\big{\lvert}\det(w_{i},W_{-i})\big{\lvert}\)then
24:Set \(\widetilde{w}_{i}=w_{i}^{-}\), \(z_{i}=z_{i}^{-}\), and \(w_{i}=\widetilde{w}_{i}-\varepsilon\cdot\theta_{i}/\big{\lvert}\theta_{i} \big{\lVert}\).
25:break
26:Set \(i=i+1\).
27:if\(i=d+1\)then
28:break
29:Return:\((z_{1},\dots,z_{d})\). ```

**Algorithm 2**RobustSpanner: Barycentric Spanner via Approximate Linear Optimization```
0:
* Target layer \(h\in[H]\).
* Reward functions \(r_{1:h}\).
* Function class \(\mathcal{G}\).
* Policy covers \(\Psi^{(1)},\ldots,\Psi^{(h)}\).
* Number of samples \(n\in\mathbb{N}\).
1:for\(t=h,\ldots,1\)do
2:\(\mathcal{D}^{(t)}\leftarrow\emptyset\).
3:for\(n\) times do
4: Sample \((\bm{x}_{t},\bm{a}_{t},\sum_{\ell=t}^{h}r_{\ell}(\bm{x}_{\ell},\bm{a}_{\ell}) )\sim\mathsf{unif}(\Psi^{(t)})\circ_{t}\pi_{\mathsf{unif}}\circ_{t+1}\hat{\pi}^ {(t+1)}\).
5: Update dataset: \(\mathcal{D}^{(t)}\leftarrow\mathcal{D}^{(t)}\cup\big{\{}\big{(}\bm{x}_{t},\bm{a }_{t},\sum_{\ell=t}^{h}r_{\ell}(\bm{x}_{\ell},\bm{a}_{\ell})\big{)}\big{\}}\).
6: Solve regression: \[g^{(t)}\leftarrow\arg\min_{g\in\Theta}\sum_{(x,a,R)\in\mathcal{D}^{(t)}}(g(x,a )-R)^{2}.\]
7: Define \(\hat{\pi}^{(t)}\in\Pi_{\mathsf{H}}^{t:h}\) via \[\hat{\pi}^{(t)}(x)=\left\{\begin{array}{ll}\arg\max_{a\in\mathcal{A}}g^{(t) }(x,a),&x\in\mathcal{X}_{t},\\ \hat{\pi}^{(t+1)}(x),&x\in\mathcal{X}_{\ell},\ \ \ell\in[t+1\,..\,h].\end{array}\right.\]
8:Return: Near-optimal policy \(\hat{\pi}^{(1)}\in\Pi_{\mathsf{H}}\). ```

**Algorithm 3**PSDP\((h,r_{1:h},\mathcal{G},\Psi^{(1:h)},n)\): Policy Search by Dynamic Programming (variant of [8])

```
0:
* Target layer \(h\in[H]\).
* Vector-valued function \(F:\mathcal{X}\times\mathcal{A}\rightarrow\mathbb{R}^{d}\).
* Policy \(\pi\in\Pi_{\mathsf{H}}\).
* Number of samples \(n\in\mathbb{N}\).
1:\(\mathcal{D}\leftarrow\emptyset\).
2:for\(n\) times do
3: Sample \((\bm{x}_{h},\bm{a}_{h})\sim\pi\).
4: Update dataset: \(\mathcal{D}\leftarrow\mathcal{D}\cup\{(\bm{x}_{h},\bm{a}_{h})\}\).
5:Return:\(\bar{F}=\frac{1}{n}\sum_{(x,a)\in\mathcal{D}}F(x,a)\). ```

**Algorithm 4**EstVec\((h,F,\pi,n)\): Estimate \(\mathbb{E}^{\pi}[F(\bm{x}_{h},\bm{a}_{h})]\) for policy \(\pi\) and function \(F:\mathcal{X}\times\mathcal{A}\rightarrow\mathbb{R}^{d}\).

```
0:
* Target layer \(h\in[H]\).
* Discriminator class \(\mathcal{F}\).
* Feature class \(\Phi\).
* A set of policies \(\Psi\).
* Number of samples \(n\in\mathbb{N}\).
1: Set \(\tilde{\varepsilon}=100\frac{d\log n+\log(|\Phi|/\delta)}{n}\), \(\varepsilon_{1}=16\sqrt{2}d^{3/2}\tilde{\varepsilon}^{1/2}\).
2: Initialize \(\lambda=\sqrt{n}\), \(T=\sqrt{\frac{d}{2\tilde{\varepsilon}}}\), and \(\xi=\frac{3}{2}\varepsilon_{1}+\tilde{\varepsilon}+\frac{2\lambda d}{n}\), \(\phi^{(0)}\in\Phi\).
3:\(\mathcal{D}\leftarrow\varnothing\),
4:for\(n\) times do
5: Sample \((\boldsymbol{x}_{h},\boldsymbol{a}_{h},\boldsymbol{x}_{h+1})\sim\mathsf{ unif}(\Psi)\circ_{h}\pi_{\mathsf{unif}}\).
6: Update dataset: \(\mathcal{D}\leftarrow\mathcal{D}\cup\{(\boldsymbol{x}_{h},\boldsymbol{a}_{h}, \boldsymbol{x}_{h+1})\}\).
7: Define \(\mathcal{L}_{\mathcal{D}}(\phi,w,f)=\sum_{(x,a,x^{\prime})\in\mathcal{D}}( \phi(x,a)^{\top}w-f(x^{\prime}))^{2}+\lambda\|w\|^{2}\).
8:for\(t=0,\ldots,T-1\)do  /* Discriminator selection */
9: Solve \[f^{(t)}\in\operatorname*{arg\,max}_{f\in\mathcal{F}}\widehat{\Delta}(f),\;\; \text{where}\;\;\widehat{\Delta}(f)\coloneqq\max_{\tilde{\phi}\in\Phi}\left\{ \min_{w}\mathcal{L}_{\mathcal{D}}(\phi^{(t)},w,f)-\min_{\tilde{w}}\mathcal{L }_{\mathcal{D}}(\tilde{\phi},\tilde{w},f)\right\}.\]
10:if\(\widehat{\Delta}(f^{(t)})\leq\xi\;\)then
11: Return \(\tilde{\phi}=\phi^{(t)}\).  /* Feature selection via least-squares minimization */
12: Solve \[\phi^{(t+1)}\in\operatorname*{arg\,min}_{\phi\in\Phi}\min_{w\circ_{t}}\sum_{ i=0}^{t}\mathcal{L}_{\mathcal{D}}(\phi,w_{i},f^{(i)}).\] ```

**Algorithm 5**\(\mathsf{RepLearn}(h,\mathcal{F},\Phi,\Psi,n)\): Representation Learning for Low-Rank MDPs [35, 49].
Generic Guarantee for RobustSpanner

In this section, we give a generic guarantee for the RobustSpanner algorithm when invoked with oracles \(\mathsf{LinOpt}\) and \(\mathsf{LinEst}\) satisfying the following assumption.

**Assumption C.1** (\(\mathsf{LinOpt}\) and \(\mathsf{LinEst}\) as approximate Linear Optimization Oracles).: _For some abstract set \(\mathcal{Z}\) and a collection of vectors \(\{w^{z}\in\mathbb{R}^{d}\mid z\in\mathcal{Z}\}\) indexed by elements in \(\mathcal{Z}\), there exists \(\varepsilon^{\prime}>0\) such that for any \(\theta\in\mathbb{R}^{d}\setminus\{0\}\) and \(z\in\mathcal{Z}\), the outputs \(\hat{z}_{\theta}\coloneqq\mathsf{LinOpt}(\theta/\|\theta\|)\) and \(\hat{w}_{z}\coloneqq\mathsf{LinEst}(z)\) satisfy_

\[\sup_{z\in\mathcal{Z}}\theta^{\intercal}w^{z}\leq\theta^{\intercal}w^{z_{ \theta}}+\varepsilon^{\prime}\cdot\|\theta\|,\quad\text{and}\quad\|\hat{w}_{z} -w^{z}\|\leq\varepsilon^{\prime}.\]

Letting \(\mathcal{W}\coloneqq\{w^{z}\mid z\in\mathcal{Z}\}\) and assuming that \(\mathcal{W}\subseteq\mathcal{B}(1)\), the next theorem bounds the number of iterations of \(\mathsf{RobustSpanner}(\mathsf{LinOpt}(\cdot),\mathsf{LinEst}(\cdot),\cdot,\cdot)\) under Assumption C.1, and shows that the output is an approximate barycentric spanner for \(\mathcal{W}\) (Definition 3.1). Our result extends those of Awerbuch and Kleinberg [6], in that it only requires an _approximate_ linear optimization oracle, which is potentially of independent interest.

**Proposition C.1**.: _Fix \(C>1\) and \(\varepsilon\in(0,1)\) and suppose that \(\{w^{z}\mid z\in\mathcal{Z}\}\subseteq\mathcal{B}(1)\). If \(\mathsf{RobustSpanner}\) (Algorithm 2) is run with parameters \(C,\varepsilon>0\) and oracles \(\mathsf{LinOpt}\), \(\mathsf{LinEst}\) satisfying Assumption C.1 with \(\varepsilon^{\prime}=\varepsilon/2\), then it terminates after \(d+\lceil\frac{d}{2}\log_{C}\frac{100d}{\varepsilon^{2}}\rceil\) iterations, and requires at most twice that many calls to each of \(\mathsf{LinOpt}\) and \(\mathsf{LinEst}\). Furthermore, the output \(z_{1:d}\) has the property that for all \(z\in\mathcal{Z}\), there exist \(\beta_{1},\ldots,\beta_{d}\in\left[-C,C\right]\), such that_

\[\left\|w^{z}-\sum_{i=1}^{d}\beta_{i}w^{z_{i}}\right\|\leq\frac{3Cd\cdot \varepsilon}{2}.\]

Proof of Proposition C.1.: The proof will follows similar steps to those in Awerbuch and Kleinberg [6, Lemma 2.6], with modifications to account for the fact that linear optimization over the set \(\mathcal{W}\coloneqq\{w^{z}\mid z\in\mathcal{Z}\}\) is only performed approximately.

Part I: Bounding the number of iterations.In Algorithm 2, there are two loops, both of which require two calls to \(\mathsf{LinOpt}\) and \(\mathsf{LinEst}\) per iteration. As the first loop has exactly \(d\) iterations, it suffices to bound the number of iterations in the second loop.

Let \(M^{(i)}\coloneqq(w_{1},\ldots,w_{i},e_{i+1},\ldots,e_{d})\) be the matrix whose columns are the vectors at end of the \(i\)th iteration of the first loop (Line 2) of Algorithm 2; note that columns \(i+1\) through \(d\) are unchanged at this point in the algorithm. For \(i\in[d]\), we define \(\ell_{i}(w)\coloneqq\det(w,M^{(i)}_{-i})\) and \(\theta_{i}\coloneqq\big{(}\det\big{(}e_{j},M^{(i)}_{-i}\big{)}\big{)}_{j\in[d]} \in\mathbb{R}^{d}\), where we recall that for any matrix \(A\), the matrix \(A_{-i}\) is defined as the result of removing the \(i\)th column from \(A\). Note that \(\ell_{i}\) is linear in \(w\), and in particular

\[\ell_{i}(w)\coloneqq w^{\intercal}\theta_{i}.\]

Let \(W^{(0)}\coloneqq M^{(d)}=(w_{1},\ldots,w_{d})\), and let \(W^{(j)}\) denote the resulting matrix after \(j\) iterations of the second loop (Line 10) of Algorithm 2. We will show that for any \(J\geq 1\),

\[\det(W^{(J)})\leq\det(W^{(0)})\cdot\left(\frac{100d}{\varepsilon^{2}}\right)^ {\frac{d}{2}}.\] (9)

By construction of the loop, we have \(\det(W^{(j)})\geq C\cdot\det(W^{(j-1)})\) for each \(j\in[J]\), and thus \(\det(W^{(J)})\geq\det(W^{(0)})\cdot C^{J}\). Combining these two facts will establish the bound on the iteration complexity. We now prove (9).

Let \(u_{i}=e_{i}^{\intercal}\big{(}M^{(i)}\big{)}^{-1}\) (note that \(u_{i}\) is a _row vector_) and let \(U\) denote the matrix whose \(i\)th row is \(u_{i}\). We observe that for all \(w\in\mathbb{R}^{d}\),

\[u_{i}w=\frac{\ell_{i}(w)}{\ell_{i}(w_{i})},\]

[MISSING_PAGE_EMPTY:19]

Part II: Spanner property for the output.Having shown that the algorithm terminates, we now show that the result is an approximate barycentric spanner for \(\mathcal{W}\). Let \(W:=(w_{1},\ldots,w_{d})\) be the matrix at termination of the algorithm. By definition, if the second loop (Line 10) has terminated, then for all \(i\in[d]\),

\[\max(\theta_{i}^{\top}w_{i}^{+},-\theta_{i}^{\top}w_{i}^{-})+ \varepsilon\cdot\|\theta_{i}\|\leq C\cdot|\det(w_{i},W_{-i})|,\]

where \(\theta_{i}=(\det(e_{j},W_{-i}))_{j\in[d]}\in\mathbb{R}^{d}\). On the other hand, by Assumption C.1, (10) holds, and so

\[\forall z\in\mathcal{Z},\forall i\in[d],\quad|\det(w^{z},W_{-i}) |=|\theta_{i}^{\top}w^{z}| \leq\max(\theta_{i}^{\top}w_{i}^{+},-\theta_{i}^{\top}w_{i}^{-})+ \varepsilon\cdot\|\theta_{i}\|,\] \[\leq C\cdot|\det(w_{i},W_{-i})|.\] (13)

Now, fix \(z\in\mathcal{Z}\). Since \(\det(W)\neq 0\), there exist \(\beta_{1:d}\in\mathbb{R}\) such that \(w^{z}=\sum_{i=1}^{d}\beta_{i}w_{i}\). By plugging this into (13) and using the linearity of the determinant, we have

\[\forall i\in[d],\quad C\cdot|\det(w_{i},W_{-i})|\geq|\det(w^{z},W_{-i})|=\left| \sum_{j=1}^{d}\beta_{i}\det(w_{j},W_{-i})\right|=|\beta_{i}|\cdot|\det(w_{i},W _{-i})|.\]

Therefore, \(|\beta_{i}|\leq C\), for all \(i\in[d]\). Now, by definition of \(w_{1:d}\) and \(\widetilde{w}_{1:d}\), for all \(i\in[d]\), we have that \(\left\lvert w_{i}-\widetilde{w}_{i}\right\rvert\leq\varepsilon\). Furthermore, by Assumption C.1, we also have that \(\|\widetilde{w}_{i}-w^{z_{i}}\|\leq\varepsilon/2\). Therefore, by the triangle inequality, we have

\[\left\lVert w^{z}-\sum_{i=1}^{d}\beta_{i}w^{z_{i}}\right\rVert\leq\left\lVert w ^{z}-\sum_{i=1}^{d}\beta_{i}w_{i}\right\rVert+\sum_{i=1}^{d}|\beta_{i}|\| \widetilde{w}_{i}-w^{z_{i}}\|+\sum_{i=1}^{d}|\beta_{i}|\|\widetilde{w}_{i}-w_ {i}|\leq 3dC\varepsilon/2.\]

This completes the proof. 

## Appendix D Generic Guarantee for PSDP

In this section, we present a generic guarantee for PSDP (Algorithm 3). We show that given any reward functions \(r_{1:h}:\mathcal{X}\times\mathcal{A}\to\mathbb{R}\) and a function class \(\mathcal{G}\subseteq\{g:\mathcal{X}\times\mathcal{A}\to\mathbb{R}\}\) that "realizes" these reward functions (we formalize this in the next definition), if \(\Psi^{(1:h)}\) are \(\alpha\)-policy covers for layers \(1\) through \(h\), then for sufficiently large \(n\geq 1\) and with high probability, the output \(\hat{\pi}=\mathsf{PSDP}(h,r_{1:h},\mathcal{G},\Psi^{(1:h)},n)\) is an approximate maximizer of the objective

\[\max_{\pi\in\Pi_{\mathsf{R}}}\mathbb{E}^{\pi}\left[\sum_{t=1}^{h}r_{t}( \boldsymbol{x}_{t},\boldsymbol{a}_{t})\right].\]

To formalize this result, we define the notion of realizability we require for the function class \(\mathcal{G}\).

**Definition D.1**.: _We say that the function class \(\mathcal{G}\subseteq\{g:\mathcal{X}\times\mathcal{A}\to\mathbb{R}\}\) realizes the reward functions \(r_{1:h}:\mathcal{X}\times\mathcal{A}\to\mathbb{R}\) if for all \(t\in[h]\) and all \(\pi\in\Pi_{\mathsf{R}}^{t:h}\),_

\[Q_{t}^{\pi}\in\mathcal{G},\quad\text{where}\qquad Q_{t}^{\pi}(x,a)\coloneqq r_ {t}(x,a)+\mathbb{E}^{\pi}\bigg{[}\sum_{\ell=t+1}^{h}r_{\ell}(\boldsymbol{x}_{ \ell},\boldsymbol{a}_{\ell})\biggm{|}\boldsymbol{x}_{t}=x,\boldsymbol{a}_{t}=a \bigg{]}.\] (14)

Note that \(Q_{t}^{\pi}\) in (14) represents the _state-action value function_ (\(Q\)-function) at layer \(t\in[h]\) with respect to the rewards \(r_{1:h}\) and partial policy \(\pi\).

In what follows, given a function class \(\mathcal{G}\subseteq\{g:\mathcal{X}\times\mathcal{A}\to\mathbb{R}\}\), we use \(\mathcal{N}_{\mathcal{G}}(\varepsilon)\) to denote the \(\varepsilon\)-covering number of \(\mathcal{G}\) in \(\ell_{\infty}\) distance. With this, we now state a guarantee for PSDP.

**Theorem D.2**.: _Let \(\varepsilon,\delta\in(0,1)\), \(B>0\), and \(h\in[H]\). Suppose reward functions \(r_{1:h}:\mathcal{X}\times\mathcal{A}\to\mathbb{R}\), function class \(\mathcal{G}\), a collection of policies \(\Psi^{(1:h)}\), and a parameter \(n\geq 1\) satisfy the following:_

* _The function class_ \(\mathcal{G}\) _realizes the reward functions_ \(r_{1:h}\) _(in the sense of Definition_ D.1_), functions in_ \(\mathcal{G}\) _are uniformly bounded by_ \(B\)_, and_ \(\lim_{n\to\infty}n^{-1}\cdot\log\mathcal{N}_{\mathcal{G}}(1/n)=0\)_._
* _For some_ \(0<\alpha\leq 1\)_, for each_ \(1\leq t\leq h\)_, it holds that_ \(\Psi^{(t)}\) _is an_ \(\alpha\)_-policy cover for layer_ \(t\) _and moreover_ \(|\Psi^{(t)}|\leq d\)_._
* _The parameter_ \(n\) _is chosen such that_ \(cdH\alpha^{-1}\cdot\varepsilon_{\mathrm{stat}}(n,\delta/H)\leq\varepsilon\)_, where_ \(\varepsilon_{\mathrm{stat}}(n,\delta^{\prime}):=\sqrt{B^{2}n^{-1}\cdot(\log \mathcal{N}_{\mathcal{G}}(1/n)+\log(n/\delta))}\) _and_ \(c>0\) _is a large enough absolute constant.__Then, with probability at least \(1-\delta\), the policy \(\hat{\pi}=\mathsf{PSDP}(h,r_{1:h},\mathcal{G},\Psi^{(1:h)},n)\) coming from Algorithm 3, satisfies the following guarantee:_

\[\max_{\pi\in\Pi_{\mathsf{h}}}\mathbb{E}^{\pi}\Bigg{[}\sum_{t=1}^{h}r_{t}( \bm{x}_{t},\bm{a}_{t})\Bigg{]}\leq\mathbb{E}^{\hat{\pi}}\Bigg{[}\sum_{t=1}^{h}r _{t}(\bm{x}_{t},\bm{a}_{t})\Bigg{]}+\varepsilon.\]

To prove the theorem, we need two intermediate results. The first shows that the \(Q\) function is the Bayes-optimal predictor of the sum of rewards when rolling out with policy \(\pi\).

**Lemma D.1**.: _Let \(t\in[H]\), \(r_{1:h}:\mathcal{X}\times\mathcal{A}\to\mathbb{R}\) be reward functions, \(\pi\in\Pi_{\mathsf{h}}\), and let \(g^{\pi}_{\text{bayes}}\) denote the Bayes-optimal predictor9 for the sum of rewards under policy \(\pi\), i.e.,_

Footnote 9: Observe that because the loss is strongly convex, this predictor is unique up to sets of measure zero, justifying our usage of “the” Bayes optimal reward.

\[g^{\pi}_{\text{bayes}}\in\operatorname*{arg\,min}_{g:\mathcal{X}_{t}\times \mathcal{A}\to\mathbb{R}}\mathbb{E}^{\pi}\Bigg{[}\bigg{(}g(\bm{x}_{t},\bm{a}_{ t})-\sum_{t=t}^{h}r_{\ell}(\bm{x}_{\ell},\bm{a}_{\ell})\bigg{)}^{2}\Bigg{]},\] (15)

_Then, \(g^{\pi}_{\text{bayes}}=Q^{\pi}_{t}\), where \(Q^{\pi}_{t}\) is the \(Q\)-function at layer \(t\in[h]\) with respect to the policy \(\pi\) and rewards \(r_{1:h}\) defined in (14)._

**Proof of Lemma D.1**.: The least-squares solution \(g^{\pi}_{\text{bayes}}\) of the problem in (15) satisfies, for all \(a\in\mathcal{A}\) and \(x\in\mathcal{X}_{t}\),

\[g^{\pi}_{\text{bayes}}(x,a) =\mathbb{E}^{\pi}\Bigg{[}\sum_{\ell=t}^{h}r_{\ell}(\bm{x}_{\ell}, \bm{a}_{\ell})\Bigg{|}\,\bm{x}_{t}=x,\bm{a}_{t}=a\Bigg{]},\] \[=\mathbb{E}[r_{t}(\bm{x}_{t},\bm{a}_{t})\mid\bm{x}_{t}=x,\bm{a}_ {t}=a]+\mathbb{E}^{\pi}\Bigg{[}\sum_{\ell=t+1}^{h}r_{\ell}(\bm{x}_{\ell},\bm{ a}_{\ell})\Bigg{|}\,\bm{x}_{t}=x,\bm{a}_{t}=a\Bigg{]},\] \[=r_{t}(x,a)+\mathbb{E}^{\pi}\Bigg{[}\sum_{\ell=t+1}^{h}r_{\ell}( \bm{x}_{\ell},\bm{a}_{\ell})\Bigg{|}\,\bm{x}_{t}=x,\bm{a}_{t}=a\Bigg{]},\] \[=Q^{\pi}_{t}(s,a),\]

where the last step follows by the definition of the \(Q\)-function in (14). 

We now show that the solution \(\hat{g}^{(t)}\) of the least-squares problem in (6) of Algorithm 3 is close to the \(Q\)-function in the appropriate sense.

**Lemma D.2**.: _Let \(\varepsilon,\delta\in(0,1)\), \(B>0\), and \(1\leq t\leq h\leq H\). Further, let \(\big{(}\varepsilon_{\mathrm{stat}},r_{1:h},\mathcal{G},\Psi^{(1:h)},n\big{)}\) be as in Theorem D.2. Then, the solution \(\hat{g}^{(t)}\) of the least-squares problem in (6) in Algorithm 3 when calling \(\mathsf{PSDP}(h,r_{1:h},\mathcal{G},\Psi^{(1:h)},n)\) satisfies with probability at least \(1-\delta\),_

\[\mathbb{E}^{\text{unif}(\Psi^{(t)})}\left[\max_{a\in\mathcal{A}}\left(\hat{g}^ {(t)}(\bm{x}_{t},a)-Q^{\hat{\pi}^{(t+1)}}_{t}(\bm{x}_{t},a)\right)^{2}\right] \leq c^{2}\cdot\varepsilon_{\mathrm{stat}}^{2}(n,\delta),\]

_where \(\hat{\pi}^{(t+1)}\in\Pi_{\mathsf{h}}^{t+1:h}\) is as in Algorithm 3, and \(c>0\) is an absolute constant._

**Proof of Lemma D.2**.: Fix \(t\in[h]\) and let \(g^{(t)}_{\text{bayes}}=g^{\pi}_{\text{bayes}}\) be as in Lemma D.1 with

\[\pi=\mathsf{unif}(\Psi^{(t)})\circ_{t}\pi_{\text{unif}}\circ_{t+1}\hat{\pi}^{ (t+1)},\]

and reward functions \(r_{1:h}\) as in the lemma's statement. By Lemma D.1, \(g^{(t)}_{\text{bayes}}\) is the Bayes-optimal solution of the least-squares problem in (6) of Algorithm 3. Thus, since \(\mathcal{G}\) realizes the reward functions \(r_{1:h}\), a standard uniform-convergence guarantee for least-square regression (see e.g. Mhammedi et al. [29, Proposition B.1] with \(\bm{e}=0\) almost surely) implies that there exists an absolute constant \(c>0\) (independent of \(t,h\), and any other problem parameters) such that with probability at least \(1-\delta\),

\[\mathbb{E}^{\text{unif}(\Psi^{(t)})}\left[\max_{a\in\mathcal{A}}\left(\hat{g}^{ (t)}(\bm{x}_{t},a)-g^{(t)}_{\text{bayes}}(\bm{x}_{t},a)\right)^{2}\right]\leq c ^{2}B^{2}\cdot\frac{\log\mathcal{N}_{\mathcal{G}}(1/n)+\log(n/\delta)}{n}.\]

The desired result follows by the fact that \(g^{(t)}_{\text{bayes}}\equiv Q^{\hat{\pi}^{(t+1)}}_{t}\); see Lemma D.1. 

We also require the classical performance difference lemma from Kakade [25].

**Lemma D.3** (Performance Difference Lemma).: _Let \(\pi_{\star},\pi\in\Pi_{\mathsf{N}}\) be arbitrary, and \(Q_{t}^{\pi}\) be as defined in (14). Then, for any \(h\geq 1\)_

\[\mathbb{E}^{\pi_{\star}}\left[\sum_{t=1}^{h}r_{t}(\bm{x}_{t},\bm{a}_{t})\right] -\mathbb{E}^{\pi}\left[\sum_{t=1}^{h}r_{t}(\bm{x}_{t},\bm{a}_{t})\right]=\sum_ {t=1}^{h}\mathbb{E}^{\pi_{\star}}\left[Q_{t}^{\pi}(\bm{x}_{t},\pi_{\star}(\bm{x }_{t}))-Q_{t}^{\pi}(\bm{x}_{t},\pi(\bm{x}_{t}))\right].\]

Using these results, we now prove Theorem D.2.

Proof of Theorem D.2.: We first show that for any \(t\in[h]\), there is an event \(\mathcal{E}_{t}\) of probability at least \(1-\delta/H\) under which the learned partial policies \(\hat{\pi}^{(t)},\hat{\pi}^{(t+1)}\) are such that

\[\mathbb{E}^{\pi_{\star}}\left[Q_{t}^{\hat{\pi}^{(t+1)}}(\bm{x}_{t},\pi_{\star }(\bm{x}_{t}))-Q_{t}^{\hat{\pi}^{(t+1)}}(\bm{x}_{t},\hat{\pi}^{(t)}(\bm{x}_{t} ))\right]\leq\frac{\varepsilon}{H},\] (16)

where \(\pi_{\star}\in\arg\max_{\pi\in\Pi_{\mathsf{N}}}\mathbb{E}^{\pi}[\sum_{t=1}^{h} r_{t}(\bm{x}_{t},\bm{a}_{t})]\) is the optimal policy and \(Q_{t}^{\pi}\) is the \(Q\)-function defined in (14). Once we establish (16) for all \(t\in[h]\), we will apply the performance difference lemma (Lemma D.3) and the union bound to obtain the desired result.

For any \(t\in[h]\), we have

\[\mathbb{E}^{\pi_{\star}}\left[Q_{t}^{\hat{\pi}^{(t+1)}}(\bm{x}_{t },\pi_{\star}(\bm{x}_{t}))-Q_{t}^{\hat{\pi}^{(t+1)}}(\bm{x}_{t},\hat{\pi}^{(t) }(\bm{x}_{t}))\right]\] \[=\mathbb{E}^{\pi_{\star}}\left[Q_{t}^{\hat{\pi}^{(t+1)}}(\bm{x}_ {t},\pi_{\star}(\bm{x}_{t}))-\hat{g}^{(t)}(\bm{x}_{t},\pi_{\star}(\bm{x}_{t}))+ \hat{g}^{(t)}(\bm{x}_{t},\pi_{\star}(\bm{x}_{t}))-Q_{t}^{\hat{\pi}^{(t+1)}}( \bm{x}_{t},\hat{\pi}^{(t)}(\bm{x}_{t}))\right],\] \[\leq 2\cdot\mathbb{E}^{\pi_{\star}}\left[\max_{a\in\mathcal{A}} \left|Q_{t}^{\hat{\pi}^{(t+1)}}(\bm{x}_{t},a)-\hat{g}^{(t)}(\bm{x}_{t},a) \right|\right],\] (17)

where the penultimate inequality follows by the fact that \(\hat{\pi}^{(t)}(x)\in\arg\max_{a\in\mathcal{A}}\hat{g}^{(t)}(x,a)\), for all \((x,a)\in\mathcal{X}_{t}\times\mathcal{A}\) by its construction in (8). Now, using the assumption on \(\Psi^{(t)}\), we have for any \(t\in[t]\),

\[\mathbb{E}^{\pi_{\star}}\left[Q_{t}^{\hat{\pi}^{(t+1)}}(\bm{x}_{t },\pi_{\star}(\bm{x}_{t}))-Q_{t}^{\hat{\pi}^{(t+1)}}(\bm{x}_{t},\hat{\pi}^{(t) }(\bm{x}_{t}))\right]\] \[\leq 2\int_{\mathcal{X}_{t}}\left(\max_{a\in\mathcal{A}}\left|Q_{t }^{\hat{\pi}^{(t+1)}}(x,a)-\hat{g}^{(t)}(x,a)\right|\right)d^{\pi_{\star}}(x) \mathrm{d}\nu(x),\] \[\leq 2\alpha^{-1}\int_{\mathcal{X}_{t}}\left(\max_{a\in\mathcal{A}} \left|Q_{t}^{\hat{\pi}^{(t+1)}}(x,a)-\hat{g}^{(t)}(x,a)\right|\right)\max_{\pi \in\Psi^{(t)}}d^{\pi}(x)\mathrm{d}\nu(x),\] \[\leq 2\alpha^{-1}d\cdot\mathbb{E}^{\mathsf{unif}(\Psi^{(t)})} \left[\max_{a\in\mathcal{A}}\left|Q_{t}^{\hat{\pi}^{(t+1)}}(\bm{x}_{t},a)-\hat{ g}^{(t)}(\bm{x}_{t},a)\right|\right],\] (18)

where the first inequality is by (17), the second inequality follows from the fact that \(\Psi^{(t)}\) is an \(\alpha\)-policy cover, the third inequality follows from the fact that \(|\Psi^{(t)}|\leq d\), and the equality is by definition. Now, by Lemma D.2, we have that for any \(t\in[h]\), there is an absolute constant \(c>0\) (independent of \(t\) and other problem parameters) and an event \(\mathcal{E}_{t}\) of probability at least \(1-\delta/H\) under which the solution \(\hat{g}^{(t)}\) of the least-squares regression problem on (6) of Algorithm 3 satisfies,

\[\mathbb{E}^{\mathsf{unif}(\Psi^{(t)})}\left[\max_{a\in\mathcal{A}}\left|\hat{g} ^{(t)}(\bm{x}_{t},a)-Q_{t}^{\hat{\pi}^{(t+1)}}(\bm{x}_{t},a)\right|\right]\leq c \cdot\varepsilon_{\mathsf{stat}}(n,\tfrac{\delta}{H})\leq\frac{\alpha \varepsilon}{2dH},\] (19)

where the last inequality follows by the choice of \(n\) in the theorem's statement. Combining (19) with (18) establishes (16) under the event \(\mathcal{E}_{t}\).

Now, by the performance difference lemma (Lemma D.3), we have

\[\mathbb{E}^{\pi_{\star}}\left[\sum_{t=1}^{h}r_{t}(\bm{x}_{t},\bm{a }_{t})\right]-\mathbb{E}^{\hat{\pi}}\left[\sum_{t=1}^{h}r_{t}(\bm{x}_{t},\bm{a }_{t})\right]\] \[=\sum_{t=1}^{h}\mathbb{E}^{\pi_{\star}}\left[Q_{t}^{\hat{\pi}^{(t +1)}}(\bm{x}_{t},\pi_{\star}(\bm{x}_{t}))-Q_{t}^{\hat{\pi}^{(t+1)}}(\bm{x}_{t}, \hat{\pi}^{(t)}(\bm{x}_{t}))\right].\]Thus, under the event \(\mathcal{E}\coloneqq\bigcup_{t=1}^{h}\mathcal{E}_{t}\), we have that

\[\mathbb{E}^{\pi_{*}}\left[\sum_{t=1}^{h}r_{t}(\bm{x}_{t},\bm{a}_{t}) \right]-\mathbb{E}^{\pi}\left[\sum_{t=1}^{h}r_{t}(\bm{x}_{t},\bm{a}_{t})\right] \leq\varepsilon.\]

The desired result follows by the fact that a union bound implies \(\mathbb{P}[\mathcal{E}]\geq 1-\delta\). 

## Appendix E Guarantee for RePLearn

In this section, we give a generic guarantee for RePLearn (Algorithm 6). Compared to previous guarantees in Modi et al. [35], Zhang et al. [49], we prove a fast \(1/n\)-type rate of convergence for RePLearn, and show that the algorithm succeeds even when the norm of the weight \(w\) in Line 10 does not grow with the number of iterations. We also use the slightly simpler discriminator class:

\[\mathcal{F}\coloneqq\left\{f:x\mapsto\max_{a\in\mathcal{A}}\theta^{\top}\phi( x,a)\,\middle|\,\,\theta\in\mathcal{B}(1),\phi\in\Phi\right\}.\] (20)

The main guarantee for RePLearn is as follows.

**Theorem E.1**.: _Let \(h\in[H]\), \(\delta\in(0,e^{-1})\), and \(n\in\mathbb{N}\) be given, and suppose that \(\mu^{\star}_{h+1}\) satisfies the normalization assumption in Eq.4. For any function \(f\in\mathcal{F}\), define_

\[w_{f}=\int_{\mathcal{X}_{h+1}}f(x)\mu^{\star}_{h+1}(x)\mathrm{d}\nu(x).\]

_Let \(P\in\Delta(\Pi_{\mathfrak{N}})\) be a distribution over policies, \(\mathcal{F}\) be as (20), and \(\Phi\) be a feature class satisfying Assumption2. With probability at least \(1-\delta\), \(\mathsf{RepLearn}\) with input \((h,\mathcal{F},\Phi,P,n)\) terminates after \(t\leq T\coloneqq\left\lceil d\log_{3/2}(2nd^{-1/2})\right\rceil\) iterations, and its output \(\phi^{(t)}\) satisfies_

\[\sup_{f\in\mathcal{F}}\inf_{w\in\mathcal{B}(3d^{3/2})}\mathbb{E}_{\pi-P} \mathbb{E}^{\pi_{0}\pi_{\mathsf{N}}\pi_{\mathsf{unif}}}\left[\left(w^{\top} \phi^{(t)}(\bm{x}_{h},\bm{a}_{h})-w^{\top}_{f}\phi^{\star}_{h}(\bm{x}_{h},\bm{ a}_{h})\right)^{2}\right]\leq\varepsilon^{2}_{\mathsf{RepLearn}}(n,\delta),\] (21)

_where \(\varepsilon^{2}_{\mathsf{RepLearn}}(n,\delta)\coloneqq cTd^{3}n^{-1}\log(| \Phi|/\delta)\), for some sufficiently large absolute constant \(c>0\)._

To prove the theorem, we need a technical lemma, which follows from Modi et al. [35, Lemma 14].

**Lemma E.1**.: _Consider a call to \(\mathsf{RepLearn}(h,\mathcal{F},\Phi,P,n)\) (Algorithm6) in the setting of TheoremE.1. Further, let \(\mathcal{L}_{\mathcal{D}}\) be as in Algorithm6 and define_

\[(\phi^{(t)},\widehat{w}^{(t)}_{1},\ldots,\widehat{w}^{(t)}_{t-1})\in\operatorname {arg\,min}_{\phi\in\Phi,(w_{1},\ldots,w_{t-1})\in\mathcal{B}(2\sqrt{d})^{t-1} }\sum_{\ell=1}^{t-1}\mathcal{L}_{\mathcal{D}}(\phi,w_{\ell},f^{(\ell)}).\] (22)

_For any \(\delta\in(0,1)\), there is an event \(\mathcal{E}^{(t)}(\delta)\) of probability at least \(1-\delta\) such that under \(\mathcal{E}^{(t)}(\delta)\), if Algorithm6 does not terminate at iteration \(t\geq 1\), then for \(w^{(\ell)}\coloneqq w_{f^{(\ell)}}\):_

\[\sum_{\ell=1}^{t-1}\mathbb{E}_{\pi-P}\mathbb{E}^{\pi_{0}\pi_{ \mathsf{N}}\pi_{\mathsf{unif}}}\left[\left(\phi^{(t)}(\bm{x}_{h},\bm{a}_{h})^{ \top}\widehat{w}^{(t)}_{\ell}-\phi^{\star}_{h}(\bm{x}_{h},\bm{a}_{h})^{\top}w ^{(\ell)}\right)^{2}\right]\leq t\varepsilon^{2}_{\mathsf{stat}}(n,\delta),\] (23) \[\inf_{w\in\frac{3}{2}B(d^{3/2})}\mathbb{E}_{\pi-P}\mathbb{E}^{ \pi_{0}\pi_{\mathsf{N}}\pi_{\mathsf{unif}}}\left[\left(\phi^{(t)}(\bm{x}_{h}, \bm{a}_{h})^{\top}w-\phi^{\star}_{h}(\bm{x}_{h},\bm{a}_{h})^{\top}w^{(t)} \right)^{2}\right]>8dt\varepsilon^{2}_{\mathsf{stat}}(n,\delta),\]

_where \(\varepsilon^{2}_{\mathsf{stat}}(n,\delta)\coloneqq cd^{2}n^{-1}\log(|\Phi|/\delta)\) and \(c\geq 1\) is a sufficiently large absolute constant._

With this, we prove TheoremE.1.

**Proof of TheoremE.1.** Let us abbreviate \(\varepsilon\coloneqq\varepsilon_{\mathsf{stat}}(n,\delta)\), with \(\varepsilon_{\mathsf{stat}}(n,\delta)\) defined as in LemmaE.1. Further, let \(N\coloneqq 1+\left\lceil d\log_{3/2}(2d^{3/2}/\varepsilon)\right\rceil\), \(\delta^{\prime}\coloneqq\frac{\delta}{2N}\), and define

\[\tilde{\varepsilon}_{\mathsf{stat}}\coloneqq\varepsilon_{\mathsf{stat}}(n, \delta^{\prime}).\] (24)

Note that \(\varepsilon\leq\tilde{\varepsilon}_{\mathsf{stat}}\) and \(N-1\leq T\), where \(T\) is the number of iterations in the theorem statement; the latter inequality follows by the facts that the absolute constant \(c\) in LemmaE.1 is at least \(1\) and \(\log(|\Phi|/\delta)\geq 1\). We define an event \(\mathcal{E}\coloneqq\mathcal{E}^{(1)}(\delta^{\prime})\cap\cdots\cap\mathcal{ E}^{(N)}(\delta^{\prime})\), where \((\mathcal{E}^{t}(\cdot))_{t}\) are the success events in LemmaE.1. Note that \(\mathbb{P}[\mathcal{E}]\geq 1-\delta/2\) by the union bound. Throughout this proof, we condition on the event \(\mathcal{E}\).

To begin the proof, we define a sequence of vectors \((v^{(\ell)}_{1:d})_{\ell\geq 0}\) in an inductive fashion, with \(v^{(\ell)}_{i}\in\mathbb{R}^{d}\) for all \(i\in[d]\) and \(\ell\geq 0\). For \(\ell=0\), we let \(v^{(0)}_{i}=\varepsilon e_{i}/d\), for all \(i\in[d]\). For \(\ell\geq 1\), we consider two cases:

* **Case I:** If \[\mathcal{J}^{(\ell)}\coloneqq\left\{j\in[d]\ \Big{|}\left|\det(V^{(\ell-1)}_{-j},w^{( \ell)})\right|>(1+C)\cdot|\det(V^{(\ell-1)})|\right\}\neq\varnothing,\] where \(V^{(\ell-1)}\coloneqq(v^{(\ell-1)}_{1},\ldots,v^{(\ell-1)}_{d})\in\mathbb{R}^ {d\times d}\) and \(w^{(\ell)}\coloneqq w_{f^{(\ell)}}\), then we let \(j\coloneqq\arg\min_{j^{\ell}\in\mathcal{J}^{(\ell)}}j^{\prime}\) and define \[v^{(\ell)}_{i}\coloneqq\left\{\begin{array}{ll}w^{(\ell)},&\text{if }i=j,\\ v^{(\ell-1)}_{i},&\text{otherwise.}\end{array}\right.\]
* **Case II:** If \(\mathcal{J}^{(\ell)}=\varnothing\), we let \(v^{(\ell)}_{i}=v^{(\ell-1)}_{i}\), for all \(i\in[d]\).

We first show that \(\mathcal{J}^{(t)}\neq\emptyset\) at any iteration \(t\in[N]\) where \(\mathsf{RepLearn}\) does not terminate. Let \(t\in[N]\) be an iteration where the algorithm does not terminate, and suppose that \(\mathcal{J}^{(t)}=\emptyset\). This means that

\[\forall j\in[d],\quad|\det(V^{(t-1)}_{-j},w^{(t)})|\leq(1+C)\cdot|\det(V^{(t-1 )})|.\] (25)

Now, since \(\det(V^{(t-1)})\neq 0\) (note that \(\left|\det(V^{(t)})\right|\) is non-decreasing with \(t\)), we have that \(\operatorname{span}(V^{(t-1)})=\mathbb{R}^{d}\). Thus, there exist \(\beta_{1},\ldots,\beta_{d}\in\mathbb{R}\) be such that \(w^{(t)}=\sum_{i=1}^{d}\beta_{i}v^{(t-1)}_{i}\). By the linearity of the determinant and (25), we have

\[\forall j\in[d],\quad(1+C)|\cdot\det(V^{(t-1)})| \geq|\det(V^{(t-1)}_{-j},w^{(t)})|,\] \[=\left|\det\left(V^{(t-1)}_{-j},\sum_{i=1}^{d}\beta_{i}v^{(t-1)} _{i}\right)\right|,\] \[=\left|\sum_{i\in[d]}\beta_{i}\cdot\det(V^{(t-1)}_{-j},v^{(t-1)} _{i})\right|,\] \[=|\beta_{j}|\cdot|\det(V^{(t-1)})|.\]

This implies that \(|\beta_{j}|\leq(1+C)\) for all \(j\in[d]\). Now, note that by the definition of \((v^{(t-1)}_{i})\), we have that for any \(i\in[d]\) such that \(v^{(t-1)}_{i}\neq e_{i}/d\), there exists \(\ell\in[t-1]\) such that \(w^{(\ell)}=v^{(t-1)}_{i}\). Let

\[\mathcal{I}^{(t)}\coloneqq\{i\in[d]\mid v^{(t-1)}_{i}\neq\varepsilon e_{i}/d\},\]

and for any \(i\in\mathcal{I}^{(t)}\), let \(\ell_{i}\in[t-1]\) be such that \(w^{(\ell_{i})}=v^{(t-1)}_{i}\). Further, define

\[\widetilde{w}^{(t)}\coloneqq\sum_{i\in\mathcal{I}^{(t)}}\beta_{i}w^{(\ell_{i} )}=\sum_{i\in\mathcal{I}^{(t)}}\beta_{i}v^{(t-1)}_{i},\] (26)

and note that by the triangle inequality and the fact that \(w^{(t)}=\sum_{i=1}^{d}\beta_{i}v^{(t-1)}_{i}\), we have

\[\|\widetilde{w}^{(t)}-w^{(t)}\|\leq(1+C)\varepsilon_{\mathrm{stat}}.\] (27)

Finally, with the notation in (22), define

\[\widetilde{w}^{(t)}_{t}\coloneqq\sum_{i\in\mathcal{I}^{(t)}}\beta_{i} \widetilde{w}^{(t)}_{\ell_{i}},\] (28)

and note that

\[\widetilde{w}^{(t)}_{t}\in(1+C)\mathcal{B}(2d^{3/2}),\]

since \(|\beta_{i}|\leq(1+C)\) for all \(i\in[d]\), \(|\mathcal{I}^{(t)}|\leq d\), and \(\widetilde{w}^{(t)}_{\ell}\in\mathcal{B}(2\sqrt{d})\), for all \(\ell\in[t-1]\). Now, by Lemma E.1, in particular (23), we have

\[\sum_{i\in\mathcal{I}^{(t)}}\mathbb{E}_{\pi\sim P}\mathbb{E}^{\pi\circ_{h} \pi_{\mathrm{init}}}\left[\left(\phi^{(t)}(\bm{x}_{h},\bm{a}_{h})^{\top} \widetilde{w}^{(t)}_{\ell_{i}}-\phi^{*}_{h}(\bm{x}_{h},\bm{a}_{h})^{\top}w^{( \ell_{i})}\right)^{2}\right]\leq t\tilde{\varepsilon}^{2}_{\mathrm{stat}},\] (29)

where \(\tilde{\varepsilon}_{\mathrm{stat}}\) is as in (24). Using the expressions in Eqs. (26) and (28) with (29) and Jensen's inequality, we have that under \(\mathcal{E}^{(t)}\),

\[\mathbb{E}_{\pi\sim P}\mathbb{E}^{\pi\circ_{h}\pi_{\mathrm{init}} }\left[\left(\phi^{(t)}(\bm{x}_{h},\bm{a}_{h})^{\top}\widetilde{w}^{(t)}_{t}- \phi^{*}_{h}(\bm{x}_{h},\bm{a}_{h})^{\top}\widetilde{w}^{(t)}\right)^{2}\right]\] \[\leq\left(\sum_{j\in\mathcal{I}^{(t)}}|\beta_{j}|\right)\cdot\sum_ {i\in\mathcal{I}^{(t)}}\mathbb{E}_{\pi\sim P}\mathbb{E}^{\pi\circ_{h}\pi_{ \mathrm{init}}}\left[\left(\phi^{(t)}(\bm{x}_{h},\bm{a}_{h})^{\top}\widetilde {w}^{(t)}_{\ell_{i}}-\phi^{*}_{h}(\bm{x}_{h},\bm{a}_{h})^{\top}w^{(\ell_{i})} \right)^{2}\right],\] \[\leq(1+C)d\tilde{\varepsilon}^{2}_{\mathrm{stat}}.\]

Now, using (27) and the facts that \((a+b)^{2}\leq 2a^{2}+2b^{2}\) and \(\left|\phi^{*}_{h}\right|_{2}\leq 1\), we have that

\[\mathbb{E}_{\pi\sim P}\mathbb{E}^{\pi\circ_{h}\pi_{\mathrm{init}} }\left[\left(\phi^{(t)}(\bm{x}_{h},\bm{a}_{h})^{\top}\widetilde{w}^{(t)}_{t}- \phi^{*}_{h}(\bm{x}_{h},\bm{a}_{h})^{\top}w^{(t)}\right)^{2}\right] \leq 2(1+C)^{2}\varepsilon^{2}+2(1+C)d\tilde{\varepsilon}^{2}_{ \mathrm{stat}},\] \[\leq 2(1+C)^{2}\tilde{\varepsilon}^{2}_{\mathrm{stat}}+2(1+C)d \tilde{\varepsilon}^{2}_{\mathrm{stat}}.\]Using that \(C=1/2\), we conclude that the right-hand side of this inequality is bounded by \(8d\tilde{\varepsilon}_{\text{stat}}^{2}\) which is a contradiction, since \(\widehat{w}_{t}^{(t)}\in(1+C)\mathcal{B}(2d^{3/2})=\mathcal{B}(3d^{3/2})\) and by LemmaE.1, we must have

\[\inf_{w\in\mathcal{B}(3d^{3/2})}\mathbb{E}_{\pi\sim P}\mathbb{E}^{\pi\circ_{h} \pi_{\text{unif}}}\left[\left(\phi^{(t)}(\bm{x}_{h},\bm{a}_{h})^{\top}w-\phi_{ h}^{*}(\bm{x}_{h},\bm{a}_{h})^{\top}w^{(t)}\right)^{2}\right]>8t\tilde{ \varepsilon}_{\text{stat}}\]

if \(\mathsf{RepLearn}\) does not terminate at round \(t\). Therefore, we have that \(\mathcal{J}^{(t)}\neq\emptyset\), for any iteration \(t\in[2\,..\,N]\) where \(\mathsf{RepLearn}\) does not terminate.

We now bound the iteration count and prove that the guarantee in Eq.21 holds at termination. Note that whenever \(\mathcal{J}^{(\ell)}\neq\emptyset\) for \(\ell>1\), we have by construction:

\[|\det(V^{(\ell)})|>3/2\cdot|\det(V^{(\ell-1)})|.\]

Thus, if \(\mathsf{RepLearn}\) runs for \(t\in[2\,..\,N]\) iterations, then

\[|\det(V^{(t)})|>(3/2)^{t-1}\cdot|\det(V^{(1)})|.\] (30)

On the other hand, since the determinant of a matrix is bounded by the product of the norms of its columns and \(v_{1:d}^{(t)}\in\mathcal{B}(2\sqrt{d})\), we have

\[|\det(V^{(t)})|\leq 2^{d}d^{d/2}.\]

Note also that \(|\det(V^{(0)})|=(\varepsilon/d)^{d}\). Plugging this into (30), we conclude that

\[(3/2)^{t-1}<(2d^{3/2}/\varepsilon)^{d}.\]

Taking the logarithm on both sides and rearranging yields

\[t<1+d\log_{3/2}(2d^{3/2}/\varepsilon)\leq N.\]

Thus, the algorithm must terminate after at most \(N-1\) iterations. Furthermore, by [35, Lemma 14], we have that with probability at least \(1-\frac{\delta}{2N}\), if the algorithm terminates at iteration \(t\), then

\[\max_{f\in\mathcal{F}}\inf_{w\in\mathcal{B}(3d^{3/2})}\mathbb{E}_ {\pi\sim P}\mathbb{E}^{\pi\circ_{h}\pi_{\text{unif}}}\left[\left(w^{\top} \phi^{(t)}(\bm{x}_{h},\bm{a}_{h})-w_{f}^{\top}\phi_{h}^{*}(\bm{x}_{h},\bm{a}_ {h})\right)^{2}\right] \leq 32t\tilde{\varepsilon}_{\text{stat}}^{2},\] \[\leq 32(N-1)\tilde{\varepsilon}_{\text{stat}}^{2},\] \[\leq 32T\tilde{\varepsilon}_{\text{stat}}^{2}.\]

Applying a union bound completes the proof. 

## Appendix F Analysis

In this section, we prove the main guarantee for \(\mathsf{SpanRL}\) (Theorem3.2). First, we outline our proof strategy in AppendixF.1. Then, in AppendixF.2 and AppendixF.3, we present guarantees for the instances of \(\mathsf{PSDP}\) (Algorithm3) and \(\mathsf{RobustSpanner}\) (Algorithm2) used within \(\mathsf{SpanRL}\). We then combine these results in AppendixF.5 to complete the proof of Theorem3.2. A self-contained guarantee for \(\mathsf{RobustSpanner}\)(Lemma3.1) is given in AppendixF.6.

### Proof Strategy

Like our algorithm, our analysis is inductive. For fixed \(h\), we assume that the policy set \(\Psi^{(1:h+1)}\) produced by \(\mathsf{SpanRL}\) satisfies the property:

\[\Psi^{(1)},\ldots\Psi^{(h+1)}\text{ are }\big{(}\tfrac{1}{4Ad},0\big{)}\text{-policy covers for layers }1\text{ through }h+1\text{, and }\max_{t\in[h+1]}|\Psi^{(t)}|\leq d.\] (31)

Conditioned on this claim, we show that with high probability, the set \(\Psi^{(h+2)}\) is a \((\tfrac{1}{4Ad},0)\)-policy cover for layer \(h+2\). To prove this, we use the inductive assumption to show that \(\mathsf{PSDP}\) acts as an approximate linear optimization oracle over \(\mathcal{W}=\{\mathbb{E}^{\pi}\left[\phi^{(h)}(\bm{x}_{h},\bm{a}_{h})\right] |\neq\pi\in\Pi_{\mathsf{H}}\}\) (AppendixF.2). Using this, we then instantiate the guarantee of \(\mathsf{RobustSpanner}\) from LemmaF.3 with \(\mathsf{LinOpt}\) and \(\mathsf{LinEst}\) instantiated with \(\mathsf{PSDP}\) and \(\mathsf{EstVec}\). To conclude the proof of the inductive step, we the main guarantee for \(\mathsf{RobustSpanner}\) together with the main guarantee for \(\mathsf{RepLearn}\) (TheoremE.1), along with a change of measure argument enabled by the assumption that \(\Psi^{(1:h)}\) are policy covers (i.e. (31)).

### Guarantee for PSDP as a Subroutine for RobustSpanner

We begin by showing that PSDP instantiates the approximate linear optimization oracle required by RobustSpanner. In particular, we fix a layer \(h\) and assume that \(\Psi^{(1:h+1)}\) satisfy (31) and apply the results of Appendix D.

More precisely, we need to show that, for any \(\theta\in\mathbb{R}^{d}\setminus\{0\}\) and \(\phi\in\Phi\), PSDP approximately solves

\[\max_{\pi\in\Pi_{h}}\theta^{\top}\mathbb{E}^{\pi}\left[\phi(\bm{x}_{h},\bm{a}_ {h})\right].\] (32)

We can equivalently formulate (32) as, for fixed \(\theta\in\mathbb{R}^{d}\setminus\{0\}\) and \(\phi\in\Phi\), maximizing the sum of the reward functions \(r_{1:h}(\cdot,\cdot;\theta,\phi)\) given by:

\[\forall(x,a)\in\mathcal{X}\times\mathcal{A},\quad r_{t}(x,a;\theta,\phi):= \left\{\begin{array}{ll}\phi(x,a)^{\top}\frac{\theta}{\|\theta\|},&\text{for $t=h$},\\ 0,&\text{otherwise}.\end{array}\right.\] (33)

Note that this matches the choice of reward functions in SpanRL (Algorithm 1) at iteration \(h\) with \(\phi=\hat{\phi}^{(h)}\), the feature map returned by RepLearn in Line 8. With these reward functions and the function class

\[\mathcal{G}\coloneqq\{g:(x,a)\mapsto\phi(x,a)^{\top}w\mid\phi\in\Phi,w\in \mathcal{B}(\sqrt{d})\},\] (34)

we show that the output \(\hat{\pi}=\texttt{PSDP}(h,r_{1:h}(\cdot,\cdot;\theta,\phi),\mathcal{G},\Psi^{ (1:h)},n)\) approximately solves (32) with high probability if \(n\geq 1\) is sufficiently large. We first verify that the class \(\mathcal{G}\) realizes the reward functions specified in (33) in the sense of Definition D.1.

**Lemma F.1**.: _Under Assumption 2.2, the function class \(\mathcal{G}\) in (34) realizes the reward functions in (33), for any \(\phi\in\Phi\) and \(\theta\in\mathbb{R}^{d}\setminus\{0\}\). Furthermore, we have that functions in \(\mathcal{G}\) are uniformly bounded by \(h\leq H\), and \(\log\mathcal{N}_{\mathcal{G}}(\varepsilon)\leq\log|\Phi|+d\log(H/\varepsilon)\), where we recall that \(\mathcal{N}_{\mathcal{G}}(\varepsilon)\) denotes the \(\varepsilon\)-covering number of \(\mathcal{G}\) in \(\ell_{\infty}\) distance._

Proof.: Fix \(\phi\in\Phi\) and \(\theta\in\mathbb{R}^{d}\setminus\{0\}\), and let \(r_{\ell}(\cdot,\cdot)\equiv r_{\ell}(\cdot,\cdot;\theta,\phi)\), for \(\ell\in[h]\). For \(t=h\), we clearly have that for any \(\pi\in\Pi_{h}^{kh}\), \(Q_{t}^{\pi}(\cdot,\cdot)=r_{t}(\cdot,\cdot)\in\mathcal{G}\). For \(t<h\) and \(\pi\in\Pi_{h}^{t:h}\), we have by the low-rank structure that

\[Q_{t}^{\pi}(x,a) =\int_{\mathcal{X}_{t+1}}\mathbb{E}^{\pi}[r_{h}(\bm{x}_{h},\bm{a} _{h})\mid\bm{x}_{t+1}=y,\bm{a}_{t+1}=\pi(y)]\cdot\phi_{t}^{*}(x,a)^{\top}\mu_ {t+1}^{*}(y)\mathrm{d}\nu(y),\] \[=\phi_{t}^{*}(x,a)^{\top}\left(\int_{\mathcal{X}_{t+1}}\mathbb{E }^{\pi}[r_{h}(\bm{x}_{h},\bm{a}_{h})\mid\bm{x}_{t+1}=y,\bm{a}_{t+1}=\pi(y)] \cdot\mu_{t+1}^{*}(y)\mathrm{d}\nu(y)\right).\] (35)

Now, by the fact that \(\mathbb{E}^{\pi}[r_{h}(\bm{x}_{h},\bm{a}_{h})\mid\bm{x}_{t+1}=y,\bm{a}_{t+1}= \pi(y)]\in[-1,1]\), for all \(y\in\mathcal{X}_{t+1}\) (since \(\phi(\cdot,\cdot)\in\mathcal{B}(1)\), for all \(\phi\in\Phi\)), and the normalizing assumption made on \((\mu_{h}^{*})_{h\in[H]}\) in Section 2.2 (i.e. that for all \(g:\mathcal{X}_{t+1}\to[0,1]\), \(\left\|\mathcal{X}_{t+1}\right\|_{\mathcal{X}_{t+1}}\mu_{t+1}^{*}(y)g(y) \mathrm{d}\nu(y)\big{\|}\leq\sqrt{d}\)), we have that

\[w_{t}\coloneqq\int_{\mathcal{X}_{t+1}}\mathbb{E}^{\pi}[r_{h}(\bm{x}_{h},\bm{a} _{h})\mid\bm{x}_{t+1}=y,\bm{a}_{t+1}=\pi(y)]\cdot\mu_{t+1}^{*}(y)\mathrm{d}\nu (y)\in\mathcal{B}(\sqrt{d}).\]

This together with (35) and the fact that \(\phi_{t}^{*}\in\Phi\) (by Assumption 2.2), we have that \(Q_{t}^{\pi}\in\mathcal{G}\). 

Combining Lemma F.1 with Theorem D.2 results in the following bound on the quality of PSDP as an approximate linear optimization oracle over the space of policies.

**Corollary F.1**.: _Let \(\varepsilon,\delta\in(0,1)\) and \(h\in[H]\). Further, let \(\theta\in\mathbb{R}^{d}\setminus\{0\}\), \(\phi\in\Phi\), and \(\hat{\pi}=\texttt{PSDP}(h,r_{1:h}(\cdot,\cdot;\theta,\phi),\mathcal{G},\Psi^{ (1:h)},n)\), where_

* _The reward functions_ \(r_{1:h}(\cdot,\cdot;\theta,\phi)\) _are as in (_33_)._
* _The function class_ \(\mathcal{G}\) _is as in (_34_)._
* _The collection of policies_ \(\Psi^{(1:h)}\) _satisfy (_31_)._
* _The parameter_ \(n\) _is chosen such that_ \(cHACd^{2}\cdot\varepsilon_{\text{stat}}(n,\delta/H)\leq\varepsilon\)_, where_ \(\varepsilon_{\text{stat}}(n,\delta^{\prime})\coloneqq\sqrt{dn^{-1}\cdot(d\log(n H)+\log(|\Phi|/\delta^{\prime}))}\) _and_ \(c>0\) _is some large enough absolute constant._

_Then, under Assumption 2.2, with probability at least \(1-\delta\), we have that_

\[\max_{\pi\in\Pi_{h}}\theta^{\top}\mathbb{E}^{\pi}[\phi(\bm{x}_{h},\bm{a}_{h})] \leq\theta^{\top}\mathbb{E}^{\hat{\pi}}[\phi(\bm{x}_{h},\bm{a}_{h})]+ \varepsilon/2.\]

We emphasize that the inductive assumption that \(\Psi^{(1:h)}\) is a policy cover of bounded size enters only in the statement of Theorem D.2. We now give a guarantee for RobustSpanner as used in SpanRL.

```
0:
* Target layer \(h\in[H]\).
* Vector-valued function \(F:\mathcal{X}\times\mathcal{A}\to\mathbb{R}^{d}\).
* Policy \(\pi\in\Pi_{\mathsf{H}}\).
* Number of samples \(n\in\mathbb{N}\).
1:\(\mathcal{D}\leftarrow\varnothing\).
2:for\(n\) times do
3: Sample \((\bm{x}_{h},\bm{a}_{h})\sim\pi\).
4: Update dataset: \(\mathcal{D}\leftarrow\mathcal{D}\cup\{(\bm{x}_{h},\bm{a}_{h})\}\).
5:Return:\(\bar{F}=\frac{1}{n}\sum_{(x,a)\in\mathcal{D}}F(x,a)\). ```

**Algorithm 7**EstVec(\(h,F,\pi,n\)): Estimate \(\mathbb{E}^{\pi}\big{[}F(\bm{x}_{h},\bm{a}_{h})\big{]}\) for given policy \(\pi\) and function \(F\).

### Guarantee for RobustSpanner as a Subroutine for SpanRL

In this section, we prove a guarantee for the instantiation of RobustSpanner in SpanRL, which we require in the proof of the main theorem (Theorem 3.2). We first show that the LinEst subroutine passed to RobustSpanner can be taken to be EstVec (Algorithm 7), which simply estimates the expected feature imbedding of \((\bm{x}_{h},\bm{a}_{h})\) under policy \(\pi\) by sampling sufficiently many trajectories and taking the empirical mean.

**Lemma F.2** (Guarantee of EstVec).: _Let \(\delta\in(0,1)\) and \(\varepsilon>0\). For \(h\in[H]\), \(\phi\in\Phi\), \(\pi\in\Pi_{\mathsf{H}}\), and \(n\in\mathbb{N}\) such that \(n\geq\frac{c}{\varepsilon^{2}}\log(d/\delta)\) for some large enough absolute constant \(c>0\), the output \(\overline{\phi}_{h}=\texttt{EstVec}(h,\phi,\pi,n)\) (Algorithm 7) satisfies, with probability at least \(1-\delta\),_

\[\|\overline{\phi}_{h}-\mathbb{E}^{\pi}\big{[}\phi(\bm{x}_{h},\bm{a}_{h})\big{]} \|\leq\varepsilon/2.\]

**Proof.** By Hoeffding's inequality (see for example [22, Corollary 7]) and the fact that \(|\phi(x,a)|\leq 1\) for all \(x\in\mathcal{X}\) and \(a\in\mathcal{A}\), there exists an absolute constant \(c>0\) such that with probability at least \(1-\delta\),

\[\|\overline{\phi}_{h}-\mathbb{E}^{\pi}\left[\phi(\bm{x}_{h},\bm{a}_{h}) \right]\|\leq c\cdot\sqrt{\frac{\log(d/\delta)}{n}}.\]

Setting \(n\) as in the statement of the theorem concludes the proof. 

In SpanRL, we instantiate RobustSpanner passing PSDP as LinOpt and EstVec as LinEst. Combining Corollary F.1 and Lemma F.2 with the general guarantee of RobustSpanner in Proposition C.1, we have the following result.

**Lemma F.3**.: _Consider iteration \(h\in[H]\) of \(\texttt{SpanRL}(\Phi,\varepsilon,\varsigma,\delta)\) (Algorithm 1) with \(\varepsilon,\varsigma>0\), \(\delta\in(0,1)\), and feature class \(\Phi\) satisfying Assumption 2.2. Further, let \(\hat{\phi}^{(h)}\) denote the feature map returned by_RepLearn _in Algorithm 1 at iteration \(h\). If \(\Psi^{(1:h)}\) satisfy (31) and \(\varsigma=\frac{\mathrm{polylog}(A,d,H,\log(|\Phi|/\delta))}{\texttt{is large enough}}\)_, then there is an event \(\mathcal{E}_{h}\) with probability at least \(1-\frac{\delta}{2H}\) such that_

* _The number of iterations of_ RobustSpanner _in Line_ 12 _of Algorithm_ 1 _is at most_ \(N=\left[\frac{d}{2}\log_{2}\left(\frac{100d}{\varepsilon}\right)\right]<\infty\)_, and_
* _The output_ \((\pi_{1},\ldots,\pi_{d})\) _of_ RobustSpanner _has the property that for all_ \(\pi\in\Pi_{\mathsf{H}}\)_, there exist_ \(\beta_{1},\ldots,\beta_{d}\in[-2,2]\) _such that_ \[\left\|\hat{\phi}^{(h),\pi}-\sum_{i=1}^{d}\beta_{i}\hat{\phi}^{(h),\pi_{i}} \right\|\leq 3d\varepsilon,\quad\text{where}\quad\hat{\phi}^{(h),\pi^{\prime}} \coloneqq\mathbb{E}^{\pi^{\prime}}\big{[}\hat{\phi}^{(h)}(\bm{x}_{h},\bm{a}_{h })\big{]}.\]

**Proof.** By Proposition C.1, on the event that the instances of PSDP and EstVec used by RobustSpanner satisfy Assumption C.1 with \(\varepsilon^{\prime}=\frac{\varepsilon}{2}\), the two desiderata of the lemma hold.10 Weclaim that each call to \(\mathsf{PSDP}\) and to \(\mathtt{EstVec}\) satisfies Assumption C.1 with probability at least \(1-\frac{\delta}{8\delta NFI}\). Because each of \(\mathsf{PSDP}\) and \(\mathtt{EstVec}\) get called at most \(4dN\) times per iteration of \(\mathtt{RobustSpanner}\), a union bound concludes the proof contingent on the above claim.

We now prove the claim. First, note that the instance of \(\mathsf{PSDP}\) that \(\mathtt{RobustSpanner}\) uses within Algorithm 1 is of the form:

\[\mathsf{PSDP}(h,r_{1:h}(\cdot,\cdot,\theta),\mathcal{G},\Psi^{1:h},n_{\mathsf{ PSDP}})\]

with \(r_{1:h}\) and \(\mathcal{G}\) as in Algorithm 1; this matches the form in Corollary F.1 (\(\mathsf{PSDP}\)'s guarantee) with \(\phi=\hat{\phi}^{(h)}\). Thus, by choosing

\[n_{\mathsf{PSDP}}=\mathfrak{c}\cdot\frac{A^{2}d^{5}H^{2}\cdot(d\log(H)+\log(8 dH^{2}N|\Phi|/\delta))}{\varepsilon^{2}},\]

for \(\mathfrak{c}=\mathrm{polylog}(A,d,H,\log(|\Phi|/\delta))\) sufficiently large, the conditions of Corollary F.1 are satisfied, and its conclusion implies the claim for the \(\mathsf{PSDP}\) instance used by \(\mathtt{RobustSpanner}\). Similarly, the choice of \(n_{\mathtt{EstVec}}\) in Algorithm 1 ensures that the claim holds for the instance of \(\mathtt{EstVec}\) that uses by Lemma F.2. The result follows.

### Guarantee for \(\mathtt{RepLearn}\) as a Subroutine for \(\mathsf{SpanRL}\)

In this section, we prove a guarantee for the invocation of \(\mathtt{RepLearn}\) within \(\mathsf{SpanRL}\)

Recall that \(P^{(h)}=\mathsf{unif}(\Psi^{(h)})\) is the distribution over policies that \(\mathsf{SpanRL}\) passes to \(\mathtt{RepLearn}\) at iteration \(h\in[H-2]\) to compute feature map \(\phi^{(h)}\). Thus, by invoking Theorem E.1 in Appendix E and using the choice of \(n_{\mathtt{RepLearn}}\) in Algorithm 1, we immediately obtain the following corollary.

**Corollary F.2**.: _Let \(\delta,\varepsilon\in(0,1)\), and \(\mathcal{F}\) be as in Algorithm 1, and fix \(h\in[H-2]\). Suppose that the feature class \(\Phi\) satisfies Assumption 2.2. Then, with probability at least \(1-\frac{\delta}{2H}\), the instance of \(\mathtt{RepLearn}\) in Line 9 of Algorithm 1 runs for \(t\leq\mathfrak{c}\cdot d\) iterations for \(\mathfrak{c}=\mathrm{polylog}(A,d,H,\log(|\Phi|/\delta))\) sufficiently large, and returns output \(\phi^{(h)}\) such that for all \(f\in\mathcal{F}\), there exists \(w_{f}^{(h)}\in\mathcal{B}(3d^{3/2})\) satisfying_

\[\mathbb{E}^{\mathsf{unif}(\Psi^{(h)})}\left[\sum_{a\in\mathcal{A}}\left(\phi ^{(h)}(\bm{x}_{h},a)^{\top}w_{f}^{(h)}-\phi_{h}^{*}(\bm{x}_{h},a)^{\top}w_{f} \right)^{2}\right]\leq\frac{\eta^{2}}{64A^{2}d^{2}},\]

_where \(w_{f}\coloneqq\int_{\mathcal{X}_{h+1}}f(y)\mu_{h+1}^{*}(y)\mathrm{d}\nu(y)\)._

### Concluding the Proof of Theorem 3.2

In this section, we conclude the proof of the main guarantee (Theorem 3.2). We derive the guarantee from the following inductive claim.

**Theorem F.1**.: _Consider iteration \(h\in[H]\) of \(\mathsf{SpanRL}(\Phi,\varepsilon,\mathfrak{c},\delta)\) (Algorithm 1) with parameters \(\varepsilon,\mathfrak{c}>0\), \(\delta\in(0,1)\) and a feature class \(\Phi\) satisfying Assumption 2.2. Further, assume that:_

* _The collection of policies_ \(\Psi^{(1:h+1)}\) _at the start of the_ \(h\)_th iteration of_ \(\mathsf{SpanRL}\) _satisfy (_31_)._
* _Assumption_ 2.1 _(reachability) holds with_ \(\eta>0\)_._
* _The input parameter_ \(\varepsilon\) _to_ \(\mathsf{SpanRL}\) _is set to_ \(\varepsilon=\frac{\eta}{36d^{3/2}}\)_._
* _The input parameter_ \(\mathfrak{c}=\mathrm{polylog}(A,d,H,\log(|\Phi|/\delta))\) _is sufficiently large._

_Then, with probability at least \(1-\frac{\delta}{H}\), the set of policies \(\Psi^{(h+2)}\) produced by \(\mathsf{SpanRL}(\Phi,\varepsilon,\mathfrak{c},\delta)\) at the end of iteration \(h\) is an \((\frac{1}{4Ad},0)\)-policy cover for layer \(h+2\)._

With this, we can now prove Theorem 3.2.

Proof of Theorem 3.2.: Note that it suffices to prove that (31) holds for \(h=H-1\) with probability at least \(1-\delta\). To do this, we proceed by induction over \(h=1,\ldots,H-1\). The base case of \(h=1\) trivially holds because \(\Psi^{(1)}=\varnothing\) and \(\Psi^{(2)}=\{\pi_{\mathsf{unif}}\}\). The induction step now follows by Theorem F.1 and the union bound (see e.g. [30, Lemma I.2]).

The number of trajectories used by SpanRL is dominated by calls to PSDP. Since PSDP is called \(O(d\log(d/\varepsilon))\) times at each iteration of SpanRL (Lemma F.3), and each call to PSDP requires at most \(Hn_{\mathsf{PSDP}}\) trajectories, the total number of trajectories after \(H\) iterations of SpanRL is bounded by \(\widetilde{O}(H^{2}dn_{\mathsf{PSDP}})\). By plugging the choices for \(n_{\mathsf{PSDP}}\) and \(\varepsilon\) from the theorem statement, we obtain the claimed sample complexity. 

Before proving Theorem F.1, we make the following simple observation.

**Lemma F.4**.: _For any \(\pi\in\Pi_{\mathsf{H}_{\mathsf{H}}}\), \(h\in[H-1]\), any \(x\in\mathcal{X}_{h+1}\), we have_

\[\mu^{\star}_{h+1}(x)^{\top}\mathbb{E}^{\pi}[\phi^{\star}_{h}(\bm{x}_{h},\bm{a }_{h})]=d^{\pi}(x)\geq 0.\]

Proof of Lemma F.4.: The equality follows by construction. The non-negativity of \(d^{\pi}(x)\) follows by definition of a probability density. 

We now prove Theorem F.1.

Proof of Theorem F.1.: Let \(\mathcal{E}_{h}\) and \(\mathcal{E}^{\prime}_{h}\) denote the success events in Lemma F.3 and Corollary F.2, respectively, and note that by the union bound, we have \(\mathbb{P}[\mathcal{E}_{h}\cap\mathcal{E}^{\prime}_{h}]\geq 1-\delta/H\). For the rest of this proof, we will condition on \(\mathcal{E}\coloneqq\mathcal{E}_{h}\cap\mathcal{E}^{\prime}_{h}\).

Throughout, we denote

\[\phi^{\star,\pi}_{h}=\mathbb{E}^{\pi}[\phi^{\star}_{h}(\bm{x}_{h},\bm{a}_{h})], \quad\forall h\in[H],\,\forall\pi\in\Pi_{\mathsf{H}}.\]

Because \(\Psi^{(1:h+1)}\) satisfy (31) (i.e., are a policy cover) it holds by Lemma F.4 that for all \(x\in\mathcal{X}_{h}\),

\[\max_{\pi\in\Psi^{(h)}}\mu^{\star}_{h}(x)^{\top}\phi^{\star,\pi}_{h-1}\geq \alpha\cdot\sup_{\pi\in\Pi_{\mathsf{H}}}\mu^{\star}_{h}(x)^{\top}\phi^{\star, \pi}_{h-1},\quad\text{for}\quad\alpha\coloneqq\frac{1}{4Ad}.\] (36)

We will show that with probability at least \(1-\frac{\delta}{H}\), the policy set \(\Psi^{(h+2)}\) has the same property for layer \(h+2\); that is, for all \(x\in\mathcal{X}_{h+1}\),

\[\max_{\pi\in\Psi^{(h+2)}}\mu^{\star}_{h+2}(x)^{\top}\phi^{\star,\pi}_{h+1}\geq \alpha\cdot\sup_{\pi\in\Pi_{\mathsf{H}}}\mu^{\star}_{h+2}(x)^{\top}\phi^{\star,\pi}_{h+1}.\] (37)

Again, by Lemma F.4 this is equivalent to the statement that \(\Psi^{(h+2)}\) is an \((\frac{1}{4Ad},0)\)-policy cover for layer \(h+2\).

For the remainder of the proof, we will fix \(x\in\mathcal{X}_{h+2}\) and let \(\pi_{x}\in\arg\max_{\pi\in\Pi_{\mathsf{H}}}\mu^{\star}_{h+2}(x)^{\top}\phi^{ \star,\pi}_{h+1}\). Our goal is to show that the inequality Eq.37 holds for \(x\).

Preliminaries.Note that since \(x\in\mathcal{X}_{h+2}\), we have \(\|\mu^{\star}_{h+2}(x)\|>0\). It will be convenient to introduce a function \(f:\mathcal{X}_{h+1}\to\mathbb{R}\) defined by

\[f(y)\coloneqq\theta^{\top}_{x}\phi^{\star}_{h+1}(y,\pi_{x}(y)),\quad\text{ where}\quad\theta_{x}\coloneqq\frac{\mu^{\star}_{h+2}(x)}{\|\mu^{\star}_{h+2}(x)\|}.\]

Further, we define

\[w_{x}\coloneqq\int_{\mathcal{X}_{h+1}}f(y)\mu^{\star}_{h+1}(y)\mathrm{d}\nu(y).\] (38)

By definition of \(\pi_{x}\), we have that for all \(y\in\mathcal{X}_{h+1}\),

\[\theta^{\top}_{x}\phi^{\star}_{h+1}(y,\pi_{x}(y))=\max_{a\in\mathcal{A}}\theta ^{\top}_{x}\phi^{\star}_{h+1}(y,a).\]

This together with the fact that \(\|\theta_{x}\|=1\) implies that

\[f\in\mathcal{F}=\left\{x\mapsto\max_{a\in\mathcal{A}}\theta^{\top}\phi(x,a) \,\middle|\,\theta\in\mathcal{B}(1),\phi\in\Phi\right\};\] (39)

the discriminator class in Line 4 of SpanRL. Note also that since \(x\in\mathcal{X}_{h+2}\), we have by reachability that

\[w^{\top}_{x}\phi^{\star,\pi_{x}}_{h}=\theta^{\top}_{x}\phi^{\star,\pi_{x}}_{h+ 1}=\frac{1}{\left\|\mu^{\star}_{h+2}(x)\right\|}\max_{\pi\in\Pi_{\mathsf{H}}}\mu ^{\star}_{h+2}(x)^{\top}\phi^{\star,\pi}_{h+1}\geq\eta>0.\] (40)

[MISSING_PAGE_FAIL:31]

where the inequality follows from the non-negativity of \(\mu^{\star}_{h+1}(\)\()^{\top}\phi^{\star}_{h+1}(x,a)\), for all \((x,a)\in\mathcal{X}_{h}\times\mathcal{A}\) (due to Lemma F.4), and (46) follows from the definition of \(\Psi^{(h+2)}\) in Line 13 of Algorithm 1. Combining (45) and (46) then implies that

\[\frac{1}{\left\|\mu^{\star}_{h+2}(x)\right\|}\mu^{\star}_{h+2}(x) ^{\top}\phi^{\star,\pi_{x}}_{h+1}=\theta^{\top}_{x}\phi^{\star,\pi_{x}}_{h+1}= w^{\top}_{x}\phi^{\star,\pi_{x}}_{h} \leq 4d\cdot\max_{i\in[d]}w^{\top}_{x}\phi^{\star,\pi_{i}}_{h},\] \[\leq\frac{4Ad}{\left\|\mu^{\star}_{h+2}(x)\right\|}\max_{\pi\in \Psi^{(h+2)}}\mu^{\star}_{h+2}(x)^{\top}\phi^{\star,\pi}_{h+1}.\]

This, together with Lemma F.4, implies that (37) holds. Since this argument holds uniformly for all \(x\in\mathcal{X}_{h+2}\), this completes the proof. 

### Proof of Lemma 3.1

By definition for \(x\in\mathcal{X}_{h+1}\), we have \(d^{\pi}(x)=\mathbb{E}^{\pi}\left[\mu^{\star}_{h+1}(x)^{\top}\phi^{\star}_{h}( \bm{x}_{h},\bm{a}_{h})\right]\). Let \(\pi_{x}\) denote the policy maximizing \(d^{\pi}(x)\) (if no such maximizer exists, we may pass to a maximizing sequence) and let \(\Psi=\{\pi_{1},\ldots,\pi_{d}\}\). Then, we have for some \(\beta_{1},\ldots,\beta_{d}\in[-C,C]\),

\[d^{\pi_{x}}(x) =\mu^{\star}_{h+1}(x)^{\top}\!\left(\sum_{i=1}^{d}\beta_{i}\phi^ {\star,\pi_{i}}_{h}\right)+\mu^{\star}_{h+1}(x)^{\top}\!\left(\phi^{\star,\pi_ {x}}_{h}-\sum_{i=1}^{d}\beta_{i}\phi^{\star,\pi_{i}}_{h}\right)\!,\] \[\leq Cd\cdot\max_{i\in[d]}\mu^{\star}_{h+1}(x)^{\top}\phi^{\star, \pi_{i}}_{h}+\varepsilon\cdot\left\|\mu^{\star}_{h+1}(x)\right\|,\quad\text{( Cauchy-Schwarz)}\] \[\leq Cd\cdot\max_{i\in[d]}\mu^{\star}_{h+1}(x)^{\top}\phi^{\star, \pi_{i}}_{h}+\frac{1}{2}d^{\pi_{x}}(x),\]

where the inequality follows by the fact that Assumption 2.1 holds with \(\varepsilon\leq\eta/2\). The result now follows by rearranging.

## Appendix G Application to Reward-Based RL

In this section, we explain how the output \(\Psi^{(1:H)}\) of \(\mathsf{SpanRL}\) (Algorithm 1) can be used to optimize downstream reward functions \(r_{1:H}\); our treatment is standard. Since the output of \(\mathsf{SpanRL}\) is a policy cover, one way to optimize the sum of rewards \(S_{H}\coloneqq\sum_{h=1}^{H}r_{h}\) is by first generating trajectories using policies in \(\Psi^{(1:H)}\), then applying an offline RL algorithm, e.g. Fitted Q-Iteration (FQI) [16], to optimize \(S_{H}\). It is also possible to use \(\mathsf{PSDP}\) with the policy cover \(\Psi^{(1:H)}\) to achieve the same goal. We will showcase the latter approach since we have already stated a guarantee for \(\mathsf{PSDP}\).

As in Appendix D, we assume access to a function class \(\mathcal{G}\in\{g:\mathcal{X}\times\mathcal{A}\to\mathbb{R}\}\) that realizes the rewards \(r_{1:H}\) in the following sense: for all \(h\in[H]\) and all \(\pi\in\Pi^{h:H}_{\mathsf{h}}\),

\[Q^{\pi}_{h}\in\mathcal{G},\quad\text{where}\quad Q^{\pi}_{h}(x,a)\coloneqq r _{h}(x,a)+\mathbb{E}^{\pi}\!\left[\left.\sum_{t=h+1}^{H}r_{t}(\bm{x}_{t},\bm{a }_{t})\ \right|\ \bm{x}_{h}=x,\bm{a}_{h}=a\right].\]

Note that when the reward functions \(r_{1:H}\) are linear in the feature map \(\phi^{\star}_{h}\); that is, when for all \(h\in[H]\) and \((x,a)\in\mathcal{X}_{h}\times\mathcal{A}\), \(r_{h}(x,a)=\theta^{\top}_{h}\phi^{\star}_{h}(x,a)\) for some \(\theta_{h}\in\mathcal{B}(1)\) (a common assumption in the context of RL in Low-Rank MDPs [32, 31, 49, 35]), then the function class

\[\mathcal{G}\coloneqq\{g:(x,a)\mapsto\phi(x,a)^{\top}w\mid\phi\in\Phi,w\in \mathcal{B}(2H\sqrt{d})\},\]

realizes \(r_{1:H}\). Note that \(\mathcal{G}\) is the same function class we used for the \(\mathsf{PSDP}\) subroutine in Algorithm 3, albeit with a larger ball for the \(w\)'s. For the sake of generality, we state the next result (which shows how to use a policy cover to optimize a downstream reward function) for general \(r_{1:H}\) and a function class \(\mathcal{G}\) that realizes \(r_{1:H}\).

**Theorem G.1**.: _Let \(\varepsilon>0\) and \(\delta\in(0,1)\). Suppose reward functions \(r_{1:H}:\mathcal{X}\times\mathcal{A}\to\mathbb{R}\), a collection of policies \(\Psi^{(1:H)}\), and a parameter \(n\geq 1\) satisfy the following:_

* _The function class_ \(\mathcal{G}\) _realizes the reward functions_ \(r_{1:H}\) _(in the sense of Definition_ D.1_), and_ \(\lim_{n\to\infty}n^{-1}\cdot\log\mathcal{N}_{\mathcal{G}}\left(1/n\right)=0\)_, where_ \(\mathcal{N}_{\mathcal{G}}\left(1/n\right)\) _to denote the_ \(\frac{1}{n}\)_-covering number of_ \(\mathcal{G}\) _in the supremum norm. Furthermore, we suppose that functions in_ \(\mathcal{G}\) _are uniformly bounded by_ \(H\sqrt{d}\)_._* _For some_ \(0<\alpha\leq 1\)_, for each_ \(1\leq h\leq H\)_, it holds that_ \(\Psi^{(h)}\) _is an_ \(\alpha\)_-policy cover for layer_ \(h\) _and moreover_ \(|\Psi^{(h)}|\leq d\)_._
* _The parameter_ \(n\) _is chosen such that_ \(cdH\alpha^{-1}\cdot\varepsilon_{\mathrm{stat}}(n,\delta/H)\leq\varepsilon\)_, where_ \(\varepsilon_{\mathrm{stat}}(n,\delta^{\prime})\coloneqq\sqrt{dH^{2}n^{-1} \cdot\left(\log\mathcal{N}_{\mathcal{G}}(1/n)+\log(1/\delta)\right)}\) _and_ \(c>0\) _is a large enough absolute constant._

_Then, with probability at least \(1-\delta\), the policy \(\hat{\pi}=\mathsf{PSDP}(H,r_{1:H},\mathcal{G},P^{(1:H)},n)\) (where \(P^{(t)}\coloneqq\mathsf{unif}(\Psi^{(t)})\), for each \(t\in[h]\)) coming from Algorithm 3, satisfies the following guarantee:_

\[\max_{\pi\in\Pi_{h}}\mathbb{E}^{\pi}\left[\sum_{h=1}^{H}r_{h}(\bm{x}_{h},\bm{ a}_{h})\right]\leq\mathbb{E}^{\hat{\pi}}\left[\sum_{h=1}^{H}r_{h}(\bm{x}_{h}, \bm{a}_{h})\right]+\varepsilon.\]

_Moreover, the number of episodes used by \(\mathsf{PSDP}\) in this case is_

\[\widetilde{O}\left(\frac{A^{2}d^{5}H^{5}(\log\mathcal{N}_{\mathcal{G}}( \varepsilon)+\log(1/\delta))}{\varepsilon^{2}}\right).\]

Proof.: This is simply a restatement of Theorem D.2 with \(h=H\). The number of trajectories follows by the fact that each call to \(\mathsf{PSDP}\) requires \(Hn\) trajectories. 

## Appendix H Properties of Reachability Assumption

In this section, we compare \(\eta\)-reachability (Assumption 2.1) to different reachability assumptions used in the literature in the context of RL in Low-Rank MDPs and show that ours is the weakest among those commonly assumed. In Appendix H.1, we demonstrate an exponential separation between our notion of reachability and that considered with respect to the popular _latent variable model_[1, 35]. In Appendix H.2, we consider a number of other reachability assumptions made outside the latent variable model and show how they imply Assumption 2.1.

### Comparison to Latent Variable Model

In this subsection, we show that our reachability assumption is implied a reachability assumption used by [1, 35] in the latent variable/non-negative feature model, and show that our reachability assumption can hold even when the best possible latent variable embedding dimension is exponential in the dimension \(d\). We begin by defining the latent variable model.

**Definition H.1** (Latent variable representation).: _Given a transition operator \(T:\mathcal{X}\times\mathcal{A}\to\Delta(\mathcal{X})\), a latent variable representation consists of a countable latent space \(\mathcal{Z}\) and functions \(\psi:\mathcal{X}\times\mathcal{A}\to\Delta(\mathcal{Z})\) and \(q:\mathcal{Z}\to\Delta(\mathcal{X})\), such that \(T(\cdot\mid x,a)=\sum_{z\in\mathcal{Z}}q(\cdot\mid z)\psi(z\mid x,a)\). The latent variable dimension of \(T\), denoted \(d_{\mathsf{LV}}\) is the cardinality of smallest latent space \(\mathcal{Z}\) for which \(T\) admits a latent variable representation._

The interpretation for the latent variable model is as follows:

1. Each \((x,a)\) pair induces a distribution \(\psi(x,a)\in\Delta(\mathcal{Z})\) over \(z\in\mathcal{Z}\).
2. The latent variable is sampled as \(\bm{z}\sim\psi(x,a)\).
3. The next state is sampled as \(\bm{x}^{\prime}\sim q(\cdot\mid\bm{z})\).

Note that in discrete state spaces, all transition operators admit a trivial latent variable representation, as we may take \(\psi(x,a)=T(\cdot\mid x,a)\), but the dimension of such a representation is potentially infinite. A latent variable representation certifies that there exists a factorization \(T(x^{\prime}\mid x,a)=\psi(x,a)^{\top}q(x^{\prime})\) with embedding dimension \(|\mathcal{Z}|\), and so \(d_{\mathsf{LV}}\), and hence gives an upper bound on the rank of the transition operator. On the other hand, compared with the general Low-Rank factorization, the latent variable factorization additionally requires that \(\psi(x,a)\) and \(q(\cdot\mid z)\) are probability distributions, and thus non-negative, for all \(z\in\mathcal{Z}\) and \((x,a)\in\mathcal{X}\times\mathcal{A}\), implying that \(d_{\mathsf{LV}}\) is equivalent to the _non-negative rank_[1] of the transition operator.

Assuming that a latent variable representation exists, [1, 35] consider the following notion of reachability.

**Definition H.2** (Reachability in latent variable model).: _There exists \(\eta>0\) such that_

\[\forall h\in[H-1],\forall z\in\mathcal{Z}_{h+1},\quad\sup_{\pi\in\Pi_{h}} \mathbb{P}^{\pi}[\bm{z}_{h+1}=z]\geq\eta.\] (47)We first show the latent variable reachability condition above implies our more general assumption.

**Lemma H.1**.: _Consider a Low-Rank MDP \(\mathcal{M}\) with rank \(d\geq 1\). Under the latent variable model in Definition H.1, if the latent variable reachability condition in (47) is satisfied for some \(\eta>0\), then, for all \(h\in[H]\), the transition kernel \(T_{h}\) in \(\mathcal{M}\) admits a factorization \(T_{h}(\cdot\,|\,x,a)=\mu^{\star}_{h+1}(\cdot)^{\top}\phi^{\star}_{h}(x,a)\), where \(\mu^{\star}_{h+1}(\cdot)\in\mathbb{R}^{d_{\mathrm{LV}}}\) and \(\phi^{\star}_{h}(\cdot,\cdot)\in\mathbb{R}^{d_{\mathrm{LV}}}\), such that \(d_{\mathrm{LV}}\leq dA^{2}/\eta^{2}\) and \(\frac{\eta^{2}}{A\sqrt{d}}\)-reachability (in the sense of Assumption 2.1) is satisfied._

**Proof of Lemma H.1.** Suppose that Assumption 2.1 (\(\eta\)-reachability) holds. By Agarwal et al. [1, Proposition 4], the non-negative rank of \(\mathcal{M}\) is bounded as \(d_{\mathrm{LV}}\leq dA^{2}/\eta^{2}\).

Letting \(q\) and \(\psi\) be as in the definition of the latent variable representation in Definition H.1, we define \(\mu^{\star}_{h+1}\) and \(\phi^{\star}_{h}\) as: for all \(h\in[H-1]\),

\[\mu^{\star}_{h+1}(\cdot)\coloneqq(q(\cdot\mid z))_{z\in\mathcal{Z}}\in \mathbb{R}^{d_{\mathrm{LV}}},\quad\text{and}\quad\phi^{\star}_{h}(\cdot,\cdot) \coloneqq(\psi(z\mid\cdot,\cdot))_{z\in\mathcal{Z}}\in\mathbb{R}^{d_{\mathrm{LV }}}.\]

Now, fix \(h\in[H-1]\) and \(x\in\mathcal{X}_{h+1}\). For \(z_{0}\in\arg\max_{z\in\mathcal{Z}_{h+1}}q(x\mid z)\), we have

\[\sup_{\pi\in\Pi_{\mathrm{s}}}d^{\pi}(x)=\mathbb{P}^{\pi}[\bm{x}_ {h+1}=x] =\sup_{\pi\in\Pi_{\mathrm{s}}}\sum_{z\in\mathcal{Z}_{h+1}}q(x\mid z )\cdot\mathbb{E}^{\pi}[\psi(z\mid\bm{x}_{h},\bm{a}_{h})],\] \[=\sup_{\pi\in\Pi_{\mathrm{s}}}q(x\mid z_{0})\cdot\mathbb{E}^{\pi} [\psi(z_{0}\mid\bm{x}_{h},\bm{a}_{h})],\] \[=|\mu^{\star}_{h+1}(x)|_{\infty}\cdot\sup_{\pi\in\Pi_{\mathrm{s}} }\mathbb{P}^{\pi}[\bm{z}_{h+1}=z_{0}],\] \[\geq\eta\cdot|\mu^{\star}_{h+1}(x)|_{\infty},\quad\text{(using reachability)}\] \[\geq\frac{\eta}{\sqrt{d_{\mathrm{LV}}}}\cdot|\mu^{\star}_{h+1}(x)|.\]

\(\square\)

We now complement the result above by showing that there exists low-rank MDPs for which our notion of reachability (Assumption 2.1) is satisfied with \(\eta\) polynomially small, yet the best possible latent variable embedding has dimension \(d_{\mathrm{LV}}=2^{\Omega(d)}\). This contrasts the results in [1, Proposition 2], which show that latent variable reachability implies a polynomial bound on the latent variable dimension.

**Theorem H.3**.: _There exists a one-step Low-Rank-MDP of rank \(d\geq 1\), where \(\eta\)-reachability (Assumption 2.1) is satisfied with \(\eta=\frac{1}{2\sqrt{d}}\), but where the non-negative rank satisfies \(d_{\mathrm{LV}}=2^{\Omega(d)}\)._

**Proof of Theorem H.3.** Let \(n\in\mathbb{N}\) and \(d\coloneqq\binom{n}{2}+1\). As shown in the proof of Agarwal et al. [1, Proposition 2], there exists a horizon-two MDP \(\mathcal{M}\) with the following properties:

* The state spaces \(\mathcal{X}_{1}\) and \(\mathcal{X}_{2}\) at layers 1 and 2, respectively, are finite.
* The cardinality of \(\mathcal{A}\) is \(d\); i.e. \(\mathcal{A}=\{a_{1},\dots,a_{d}\}\).11 Footnote 11: Technically, the example in the proof of [1, Proposition 2] does not explicitly specify the number of actions. Instead, the example assigns a number of state-action pairs to vectors in \(\mathbb{R}^{d}\), without specifying the number of actions. The number of actions in their example is a degree of freedom, which we set to \(d\) here without loss of generality.
* The transition kernel \(T_{1}\) admits the factorization: \[T_{1}(\cdot\mid x,a)=\mu^{\star}_{2}(\cdot)^{\top}\phi^{\star}_{1}(x,a)\in \Delta(\mathcal{X}_{2}),\quad\forall(x,a)\in\mathcal{X}_{1}\times\mathcal{A},\] where for all \(x^{\prime}\in\mathcal{X}_{2}\), \(\mu^{\star}_{2}(x^{\prime})\in\mathbb{R}^{d}_{\geq 0}\), and for all \((x,a)\in\mathcal{X}_{1}\times\mathcal{A}\), \(\phi^{\star}_{1}(x,a)\in\mathbb{R}^{d}_{\geq 0}\).
* The non-negative rank of \(\mathcal{M}\) is \(d_{\mathrm{LV}}=2^{\Omega(d)}\).

We augment this MDP by adding an extra state \(x_{0}\), and let \(\overline{\mathcal{X}}_{1}\coloneqq\mathcal{X}_{1}\cup\{x_{0}\}\). We define \(\overline{\phi}^{\star}_{1}:\overline{\mathcal{X}}_{1}\times\mathcal{A} \rightarrow\mathbb{R}^{d}_{\geq 0}\) be the extension of \(\phi^{\star}_{1}\) given by

\[\forall i\in[d],\quad\overline{\phi}^{\star}_{1}(x_{0},a_{i})=e_{i},\quad\text{ and}\quad\forall x\in\mathcal{X}_{1},\quad\overline{\phi}^{\star}_{1}(x,a_{i})=\phi^{ \star}_{1}(x,a_{i}),\]where \(e_{i}\) is the \(i\)th basis element in \(\mathbb{R}^{d}\). We define the initial state distribution to have \(\rho(x_{0})=\frac{1}{2}\) and \(\rho(x)=\frac{1}{2|\mathcal{X}_{1}|}\), for all \(x\in\mathcal{X}_{1}\).12 We let \(\overline{\mathcal{M}}=(\mathcal{X}_{1}\cup\mathcal{X}_{2},\mathcal{A}, \overline{\phi}_{1}^{*},(\mu_{h}^{*})_{h\in[2]},\rho)\) denote the resulting MDP. Note that adding an extra state at layer 1 in this fashion only adds \(d\) additional rows to the transition matrix \(T\) (viewed as a \((|\mathcal{X}_{1}\times\mathcal{A}|)\times|\mathcal{X}_{2}|\) matrix). Therefore, the non-negative rank of \(\overline{\mathcal{M}}\) is as least that of \(\mathcal{M}\).

Footnote 12: We note that [1] did not specify the initial distribution, which is not needed for the conclusion of their result.

We now show that reachability is satisfied in \(\overline{\mathcal{M}}\). Let \(\pi_{i}\) the policy that always plays action \(a_{i}\). With this, we have that for any \(x^{\prime}\in\mathcal{X}_{2}\),

\[\sup_{\pi\in\Pi_{\mathsf{R}}}d^{\pi}(x^{\prime}) \geq\max_{i\in[d]}d^{\pi_{i}}(x^{\prime}),\] \[=\max_{i\in[d]}\mu_{2}^{*}(x^{\prime})^{\top}\mathbb{E}[\overline {\phi}_{1}^{*}(\bm{x}_{1},a_{i})],\] \[=\max_{i\in[d]}\left\{\mathbb{E}[\mathbb{I}[\{\bm{x}_{1}=x_{0}\} \cdot\mu_{2}^{*}(x^{\prime})^{\top}\overline{\phi}_{1}^{*}(\bm{x}_{1},a_{i}) ]+\mathbb{E}[\mathbb{I}[\{\bm{x}_{1}\neq x_{0}\}\cdot\mu_{2}^{*}(x^{\prime})^ {\top}\overline{\phi}_{1}^{*}(\bm{x}_{1},a_{i})]\right\},\] \[\geq\max_{i\in[d]}\rho(x_{0})\mu_{2}^{*}(x^{\prime})^{\top} \overline{\phi}_{1}^{*}(x_{0},a_{i}).\] (48)

where the last inequality follows by the fact that, for all \((x,a)\in\mathcal{X}_{1}\times\mathcal{A}\), \(\mu_{2}^{*}(\cdot)^{\top}\overline{\phi}_{1}^{*}(x,a)=\mu_{2}^{*}(x^{\prime}) ^{\top}\phi_{1}^{*}(x,a)\geq 0\) (since \(\mu_{2}^{*}(x^{\prime})^{\top}\phi_{1}^{*}(x,a)\) is a conditional density). On the other hand, from the construction of \(\overline{\phi}_{1}^{*}\) and the fact that \(\mu_{2}^{*}(x^{\prime})\in\mathbb{R}_{\geq 0}^{d}\), we have

\[\max_{i\in[d]}\mu_{2}^{*}(x^{\prime})^{\top}\overline{\phi}_{1}^{*}(x_{0},a_{i })=\|\mu_{2}^{*}(x^{\prime})\|_{\infty}\geq\|\mu_{2}^{*}(x^{\prime})\|/\sqrt{d}.\]

Combining this with (48) and using that \(\rho(x_{0})=1/2\) implies that reachability \(1/(2\sqrt{d})\) is satisfied in \(\overline{\mathcal{M}}\). 

### Relation to Other Reachability Assumptions

In this subsection, we show that Assumption 2.1 is implied by a notion of _feature coverage_ used in the context of transfer learning in Low-Rank MDPs [5], as well as a notion of _explorability_ used in the context of reward-free RL in linear MDPs [46].

#### h.2.1 Feature Coverage

We first consider coverage condition used by Agarwal et al. [5], which involves the second moments of the feature map \(\phi_{h}^{*}\).

**Definition H.4** (\(\eta\)-feature coverage).: _We say that the linear MDP with featurization \(\phi_{h}^{*}\) satisfies \(\eta\)-feature coverage if for all \(h\in[H]\),_

\[\sup_{\pi\in\Pi_{\mathsf{N}}}\lambda_{\min}\left(\mathbb{E}^{\pi}[\phi_{h}^{*} (\bm{x}_{h},\bm{a}_{h})\phi_{h}^{*}(\bm{x}_{h},\bm{a}_{h})^{\top}]\right)\geq\eta.\]

We show that \(\eta\)-feature coverage implies \((\eta/2)^{3/2}\)-reachability. Thus, up to polynomial dependence, \(\eta\)-feature coverage is a special case of Assumption 2.1.

**Lemma H.2**.: _Suppose that an MDP satisfies \(\eta\)-feature coverage as in Definition H.4 for some \(\eta>0\). If \(\phi_{h}^{*}(x,a)\in\mathcal{B}(1)\) for all \(x,a\), then the MDP satisfies \((\eta/2)^{3/2}\)-reachability in the sense of Assumption 2.1._

**Proof of Lemma H.2.** Let \(h\in[H]\) and \(x\in\mathcal{X}_{h+1}\) be given and define

\[\theta\coloneqq\frac{\mu_{h+1}^{*}(x)}{\|\mu_{h+1}^{*}(x)\|}.\]

To keep notation compact, we define \(\bm{\phi}_{h}^{*}=\phi_{h}^{*}(\bm{x}_{h},\bm{a}_{h})\). By \(\eta\)-feature coverage, there exists \(\pi\in\Pi_{\mathsf{N}}\) such that

\[\eta\leq\mathbb{E}^{\pi}[(\theta^{\intercal}\bm{\phi}_{h}^{*})^{2}] =\mathbb{E}^{\pi}[\mathbb{I}\{(\theta^{\intercal}\bm{\phi}_{h}^{*})^{2}<\eta /2\}\cdot(\theta^{\intercal}\bm{\phi}_{h}^{*})^{2}]+\mathbb{E}^{\pi}[\mathbb{I} \{(\theta^{\intercal}\bm{\phi}_{h}^{*})^{2}\geq\eta/2\}\cdot(\theta^{\intercal} \bm{\phi}_{h}^{*})^{2}],\] \[\leq\eta/2+\mathbb{P}^{\pi}[(\theta^{\intercal}\bm{\phi}_{h}^{*})^{2 }\geq\eta/2],\] (49)where we have used that \(\|\theta\|=1\) and \(|\phi_{h}^{*}(x,a)\|\leq 1\) for all \((x,a)\in\mathcal{X}_{h}\times\mathcal{A}\). Rearranging (49) and using that \(\theta^{\top}\bm{\phi}_{h}^{*}\geq 0\) (it is a scaled conditional density), have

\[\mathbb{P}^{\pi}\big{[}\theta^{\top}\bm{\phi}_{h}^{*}\geq\sqrt{\eta/2}\big{]}= \mathbb{P}^{\pi}\big{[}(\theta^{\top}\bm{\phi}_{h}^{*})^{2}\geq\eta/2\big{]} \geq\eta/2.\]

Now, by Markov's inequality, we have that

\[\theta^{\top}\phi_{h}^{*,\pi}=\mathbb{E}^{\pi}[\theta^{\top}\bm{\phi}_{h}^{*}] \geq\sqrt{\eta/2}\cdot\mathbb{P}^{\pi}[\theta^{\top}\bm{\phi}_{h}^{*}\geq\sqrt {\eta/2}]\geq(\eta/2)^{3/2},\]

where we have once more used that \(\theta^{\top}\bm{\phi}_{h}^{*}\geq 0\) almost surely. 

#### h.2.2 Explorability

We now consider the _explorability_ assumption of [46], which involves the first moment of the feature map \(\phi_{h}^{*}\). This notion is defined as follows.

**Definition H.5** (\(\eta\)-explorability).: _We say that a linear MDP satisfies \(\eta\)-explorability if for any \(h\in[H]\) and any \(\theta\in\mathbb{R}^{d}\setminus\{0\}\) it holds that_

\[\sup_{\pi\in\Pi_{\mathsf{R}}}|\theta^{\top}\mathbb{E}^{\pi}[\phi_{h}^{*}(\bm{ x}_{h},\bm{a}_{h})]|\geq\eta\cdot|\theta\|.\]

We now show that \(\eta\)-explorability is a special case of \(\eta\)-reachability:

**Lemma H.3**.: _Suppose that the explorability condition in Definition H.5 is satisfied with \(\eta>0\). Then, \(\eta\)-reachability is satisfied._

Proof of Lemma H.3.: Let \(x\in\mathcal{X}_{h+1}\) and define \(\theta\coloneqq\mu_{h+1}^{\star}(x)\). By explorability, we have that

\[\sup_{\pi\in\Pi_{\mathsf{R}}}d^{\pi}(x) =\sup_{\pi\in\Pi_{\mathsf{R}}}\mathbb{E}^{\pi}[\mu_{h+1}^{\star}( x)^{\top}\phi_{h}^{*}(\bm{x}_{h},\bm{a}_{h})],\] \[=\sup_{\pi\in\Pi_{\mathsf{R}}}|\mathbb{E}^{\pi}[\mu_{h+1}^{\star }(x)^{\top}\phi_{h}^{*}(\bm{x}_{h},\bm{a}_{h})]|,\quad(\mu_{h+1}^{\star}(\cdot )^{\top}\phi_{h}^{*}(x,a)\text{ is a condition law})\] \[=\sup_{\pi\in\Pi_{\mathsf{R}}}|\theta^{\top}\mathbb{E}^{\pi}[\phi _{h}^{*}(\bm{x}_{h},\bm{a}_{h})]|,\] \[\geq\eta\cdot\|\theta\|,\quad\text{(by explorability)}\] \[=\eta\cdot\|\mu_{h+1}^{\star}(x)\|.\]

This shows that Assumption 2.1 is satisfied with parameter \(\eta\).