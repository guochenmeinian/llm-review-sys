Distributionally Robust Reinforcement Learning with Interactive Data Collection: Fundamental Hardness and Near-Optimal Algorithms

 Miao Lu1  Han Zhong2\({}^{*}\) Tong Zhang\({}^{3}\) Jose Blanchet\({}^{1}\)

\({}^{1}\)Department of Management Science and Engineering, Stanford University

\({}^{2}\)Center for Data Science, Peking University

\({}^{3}\)Department of Computer Science, University of Illinois Urbana-Champaign

Equal contribution. Email to miaolu@stanford.edu, hanzhong@stu.pku.edu.cn

###### Abstract

Distributionally robust reinforcement learning (DRRL), often framed as a robust Markov decision process (RMDP), seeks to find a robust policy that achieves good performance under the worst-case scenario among all environments within a pre-specified uncertainty set centered around the training environment. Unlike previous work, which relies on a generative model or a pre-collected offline dataset enjoying good coverage of the deployment environment, we tackle robust RL via interactive data collection, where the learner interacts with the training environment only and refines the policy through trial and error. In this robust RL paradigm, two main challenges emerge: managing the distributional robustness while striking a balance between exploration and exploitation during data collection. Initially, we establish that sample-efficient learning without additional assumptions is unattainable owing to the curse of support shift; i.e., the potential disjointedness of the distributional supports between training and testing environments. To circumvent such a hardness result, we introduce the vanishing minimal value assumption to RMDPs with a total-variation distance robust set, postulating that the minimal value of the optimal robust value function is zero. Such an assumption effectively eliminates the support shift issue for RMDPs with a TV distance robust set, and we present an algorithm with a provable sample complexity guarantee. Our work makes the initial step to uncovering the inherent difficulty of robust RL via interactive data collection and sufficient conditions for sample-efficient algorithms with sharp sample complexity.

## 1 Introduction

Reinforcement learning (RL) serves as a framework for addressing complex decision-making problems through iterative interactions with environments. The advancements in deep reinforcement learning have enabled the successful application of the general RL framework across various domains, including mastering strategic games, such as Go (Silver et al., 2017), robotics (Kober et al., 2013), and tuning large language models (LLMs; Ouyang et al. (2022)). The critical factors contributing to these successes encompass not only the potency of deep neural networks and modern deep RL algorithms but also the availability of substantial training data. However, there are scenarios, such as healthcare (Wang et al., 2018) and autonomous driving (Kiran et al., 2021), among others, where collecting data in the target domain is challenging, costly, or even unfeasible.

In such cases, the _sim-to-real transfer_(Kober et al., 2013; Sadeghi and Levine, 2016; Peng et al., 2018; Zhao et al., 2020) becomes a remedy - a process in which the RL agents are trained in some simulated environment and subsequently deployed in real-world settings. Nevertheless, the training environment may differ from the real-world environment. Such a discrepancy, also known as the _sim-to-real gap_, will typically result in suboptimal performance of RL agents in real-world applications. One promising strategy to control the impact in performance degradation due to the sim-to-real gap is robust RL (Iyengar, 2005; Pinto et al., 2017; Hu et al., 2022), which aims to learn policies exhibiting strong (i.e. robust) performance under environmental deviations from the training environment, effectively hedging the epistemological uncertainty arising from the differences between the training environment and the unknown testing environments.

A robust RL problem is often formulated within a robust Markov decision process (RMDP) framework, with various types of robust sets characterizing different environmental perturbations. In this robust RL context, prior works have developed algorithms with provable sample complexity guarantees. However, these algorithms typically rely on either a generative model (Yang et al., 2022; Panaganti and Kalathil, 2022; Xu et al., 2023; Shi et al., 2023) or offline data with good coverage of deployment environments (Zhou et al., 2021; Panaganti et al., 2022; Shi and Chi, 2022; Ma et al., 2022; Blanchet et al., 2023). Notably, the current literature does not explicitly address the _exploration_ problem, which stands as one of the fundamental challenges in reinforcement learning through trial-and-error (Sutton and Barto, 2018). Meanwhile, the empirical success of robust RL methods (Pinto et al., 2017; Kuang et al., 2022; Moos et al., 2022) typically relies on reinforcement learning through _interactive data collection_ in the training environment, where the agent iteratively and actively interacts with the environment, collecting data, optimizing and robustifying its policy. Given that all the existing literature on robust RL theory relies on a generative model or pre-collected data, it is natural to ask:

_Can we design a provably sample-efficient robust RL algorithm that relies on interactive data collection in the training environment?_

Answering the above question faces a fundamental challenge, namely, that during the interactive data collection process, the learner no longer has the oracle control over the training data distributions that are induced by the policy learned through the interaction process. In particular, it could be the case that certain data patterns that are crucial for the policy to be robust across all testing environments are not accessible through interactive data collection, even through a sophisticated design of an exploration mechanism during the interaction process. For example, specific states may not be accessible within the training environment dynamics but could be reached in the testing environment dynamics.

In contrast, previous work has demonstrated that robust RL through a generative model or a pre-collected offline dataset with good coverage does not face such difficulty. In the generative model setup, fortunately, the learner can directly query any state-action pair and obtain the sampled next state from the generator. Intuitively, once the states that could appear in the testing environment trajectory are queried enough times, it is possible to guarantee the performance of the learned policy in testing environments. The situation is similar if one has a pre-collected offline dataset that possesses good coverage of the testing environment. This motivates us to take the initial steps towards answering the above questions regarding robust RL with interactive data collection.

### Contributions

In this work, we study robust RL in a finite-horizon RMDP with an \(\mathcal{S}\times\mathcal{A}\)-rectangular total-variation distance (TV) robust set (see Assumption 2.1 and Definition 2.4) through _interactive data collection_. We give both a fundamental hardness result in the general case and a sample-efficient algorithm within tractable settings. More specifically, our contributions are three folds.

**Fundamental hardness.** We construct a class of hard-to-learn RMDPs (see Example 3.1) and demonstrate that _any_ learning algorithm inevitably incurs an \(\Omega(\rho\cdot HK)\)-online regret (Theorem 3.2) under at least one RMDP instance. Here, \(\rho\) signifies the radius of the TV robust uncertainty set, \(H\) is the horizon, and \(K\) is the number of interactive episodes. This linear regret lower bound underscores the impossibility of sample-efficient robust RL via interactive data collection in general.

**Identifying a tractable case.** Upon close examination of the challenging instance, we recognize that the primary obstacle to achieving sample-efficient learning lies in the _curse of support shift_, i.e., the disjointedness of distributional support between the training environment and the testing environments. In a broader sense, the curse of support shift also refers to the situation when the states appearing in testing environments are extremely hard to arrive in the training environment.

To rule out these pathological instances, we propose the _vanishing minimal value_ assumption (Assumption 4.1), positing that the optimal robust value function reaches zero at a specific state. Such an assumption naturally applies to the sparse reward RL paradigm and offers a broader scope compared to the "fail-state" assumption utilized in prior studies on offline RMDP with function approximation (Panaganti et al., 2022). For a comprehensive discussion on this comparison, please see Remark B.3. On the theoretical front, we establish that the vanishing minimal value assumption effectively mitigates the support shift issues between the training and the testing environments (Proposition 4.2), rendering robust RL with interactive data collection feasible for RMDPs with TV robust sets.

**Efficient algorithm with sharp sample complexity.** Under the vanishing minimal value assumption, we develop an algorithm named OPtimistic Robust Value Iteration for TV Robust Set (OPROVI-TV, Algorithm 1), that is capable of finding an \(\varepsilon\)-optimal robust policy with a total number of

\[\widetilde{\mathcal{O}}\big{(}\min\{H,\rho^{-1}\}\cdot H^{2}SA/ \varepsilon^{2}\big{)}\] (1.1)

interactive samples (Theorem 4.3). Here \(S\) and \(A\) denote the number of states and actions, \(\rho\) represents the radius of the TV robust set, and \(H\) is the horizon length of each episode. To our best knowledge, this is the first provably sample-efficient algorithm for robust RL with interactive data collection.

According to (1.1), the sample complexity of finding an \(\varepsilon\)-optimal robust policy decreases as the radius \(\rho\) of the robust set increases. When the radius \(\rho=0\), an RMDP reduces to a standard MDP, and the sample complexity (1.1) recovers the minimax-optimal sample complexity for online RL in standard MDPs up to logarithm factors, i.e., \(\widetilde{\mathcal{O}}(H^{3}SA/\varepsilon^{2})\).

In the end, we further extend our algorithm and theory to a new type of RMDPs, \(\mathcal{S}\times\mathcal{A}\)-rectangular discounted RMDP equipped with robust sets consisting of transition probabilities with bounded ratio to the nominal kernel (See Appendix B.4.2). This newly identified class of RMDPs naturally does not suffer from the support shift issue. It is equivalent to the \(\mathcal{S}\times\mathcal{A}\)-rectangular RMDP with TV robust set and vanishing minimal value assumption in an appropriate sense due to Proposition 4.2. Consequently, by a clever usage of Algorithm 1, we can also solve this new model sample-efficiently, as shown in Corollary B.5. Such a result echoes our intuition on the curse of support shift.

**Comparison to related works.** Due to the space limit, we only compare with the most related works through Table 1. A detailed discussion of related works is in Appendix A.

## 2 Preliminaries

**Notations.** For a set \(\mathcal{X}\), we denote \(\Delta(\mathcal{X})\) as the set of probability distributions on \(\mathcal{X}\). For a distribution \(p\in\Delta(\mathcal{X})\), we define the shorthand for expectation and variance as \(\mathbb{E}_{p(\cdot)}[f]:=\mathbb{E}_{X\sim p(\cdot)}[f(X)]\) and \(\mathbb{V}_{p(\cdot)}[f]=\mathbb{E}_{p(\cdot)}[f^{2}]-(\mathbb{E}_{p(\cdot)} [f])^{2}\). Given any set \(\mathcal{Q}\subseteq\Delta(\mathcal{X})\), we define the robust expectation

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline Model Assump. & Algorithm & Data oracle & \begin{tabular}{c} Sample complexity \\ \(\rho\in[0,1)\) \\ \end{tabular} \\ \hline \multirow{6}{*}{general case} & RPVL (Xu et al., 2023) & generative model & \(\widetilde{\mathcal{O}}\left(\frac{H^{3}SA}{\varepsilon^{2}}\right)\) \\ \cline{2-4}  & DRVI (Shi et al., 2023) & generative model & \(\widetilde{\mathcal{O}}\left(\frac{\min\{H_{\gamma},\rho^{-1}\}H^{2}_{\gamma} SA}{\varepsilon^{2}}\right)\) \\ \cline{2-4}  & lower bound (Shi et al., 2023) & generative model & \(\Omega\left(\frac{\min\{H_{\gamma},\rho^{-1}\}H^{2}_{\gamma}SA}{\varepsilon^{2}}\right)\) \\ \cline{2-4}  & P\({}^{2}\)MPO(Blanchet et al., 2023) & offline dataset & \(\widetilde{\mathcal{O}}\left(\frac{\mathcal{C}_{\text{resh}}^{\ast}H^{4}SA^{ \ast}}{\varepsilon^{2}}\right)\) \\ \cline{2-4}  & lower bound (this work) & interactive data collection & intractable \\ \hline \begin{tabular}{c} “fail-state” \\ assumption \\ \end{tabular} & RFQI (Panaganti et al., 2022) & offline dataset & \(\widetilde{\mathcal{O}}\left(\frac{\mathcal{C}_{\text{resh}}H^{4}SA}{\rho^{2} \varepsilon^{2}}\right)\) \\ \hline 
\begin{tabular}{c} vanishing \\ minimal value \\ (Assumption 4.1) \\ \end{tabular} & OPROVI-TV (this work) & interactive data collection & \(\widetilde{\mathcal{O}}\left(\frac{\min\{H_{\gamma},\rho^{-1}\}H^{2}SA}{ \varepsilon^{2}}\right)\) \\ \hline \end{tabular}
\end{table}
Table 1: Comparison between OPROVI-TV and prior results on RMDP with \(\mathcal{S}\times\mathcal{A}\)-rectangular TV robust sets under various settings (generative model/offline dataset/interactive data collection). For the infinite horizon \(\gamma\)-discounted RMDPs, we denote \(H_{\gamma}:=(1-\gamma)^{-1}\) as the effective horizon length. In the offline setting, \(\mathcal{C}_{\text{resh}}^{\ast}\) and \(\mathcal{C}_{\text{full}}\) represent the robust partial coverage coefficient and full coverage coefficient, respectively. In the general case, our lower bound reads intractable, meaning that there exist hard instances where it is impossible to learn the nearly optimal robust policy via a finite number of interactive samples.

operator as \(\mathbb{E}_{\mathcal{Q}}[f]:=\inf_{p(\cdot)\in\mathcal{Q}}\mathbb{E}_{X\sim p( \cdot)}[f(X)]\). For any \(x,a\in\mathbb{R}\), we denote \((x)_{+}=\max\{x,0\}\) and \(x\lor a=\max\{x,a\}\). We use \(\mathcal{O}(\cdot)\) to hide absolute constant factors and use \(\widetilde{\mathcal{O}}\) to further hide logarithmic factors. For a positive integer \(H\in\mathbb{N}_{+}\), we denote the set \(\{1,2,\ldots,H\}\) by \([H]\).

### Robust Markov Decision Processes

We first introduce our underlying model for doing robust RL, the episodic robust Markov decision process (RMDP), denoted by a tuple \((\mathcal{S},\mathcal{A},H,P^{\star},R,\boldsymbol{\Phi})\). Here the set \(\mathcal{S}\) is the state space and the set \(\mathcal{A}\) is the action space, both with finite cardinality. The integer \(H\) is the length of each episode. The set \(P^{\star}=\{P^{\star}_{h}\}_{h=1}^{H}\) is the collection of _nominal_ transition kernels where \(P^{\star}_{h}:\mathcal{S}\times\mathcal{A}\mapsto\Delta(\mathcal{S})\). The set \(R=\{R_{h}\}_{h=1}^{H}\) is the collection of reward functions where \(R_{h}:\mathcal{S}\times\mathcal{A}\mapsto[0,1]\). For simplicity, we denote \(\mathcal{P}=\{P(\cdot|\cdot,\cdot):\mathcal{S}\times\mathcal{A}\mapsto\Delta( \mathcal{S})\}\) as the space of all possible transition kernels, and we denote \(S=|\mathcal{S}|\) and \(A=|\mathcal{A}|\). Most importantly and different from standard MDPs, the RMDP is equipped with a mapping \(\boldsymbol{\Phi}:\mathcal{P}\mapsto 2^{\mathcal{P}}\) that characterizes the _robust set_ of any transition kernel in \(\mathcal{P}\). Formally, for any transition kernel \(P\in\mathcal{P}\), we call \(\boldsymbol{\Phi}(P)\) the _robust set_ of \(P\). One could interpret the nominal transition kernel \(P^{\star}_{h}\) as the transition of the training environment, while \(\boldsymbol{\Phi}(P^{\star}_{h})\) contains all possible transitions of the testing environments.

Given an RMDP \((\mathcal{S},\mathcal{A},H,P^{\star},R,\boldsymbol{\Phi})\), we consider using a Markovian policy to make decisions. A Markovian decision policy (or simply, policy) is defined as \(\pi=\{\pi_{h}\}_{h=1}^{H}\) with \(\pi_{h}:\mathcal{S}\mapsto\Delta(\mathcal{A})\) for each step \(h\in[H]\). To measure the performance of a policy \(\pi\) in the RMDP, we introduce its _robust value function_, defined as for any \((s,a)\in\mathcal{S}\times\mathcal{A}\),

\[V^{\pi}_{h,P^{\star},\boldsymbol{\Phi}}(s) :=\inf_{\widetilde{P}_{h}\in\boldsymbol{\Phi}(P^{\star}_{h}),1 \leq h\leq H}\mathbb{E}_{\{\widetilde{P}_{h}\}_{h=1}^{H},\{\pi_{h}\}_{h=1}^{H }}\left[\sum_{i=h}^{H}R_{i}(s_{i},a_{i})\left|\,s_{h}=s\right],\right.\] \[\left.Q^{\pi}_{h,P^{\star},\boldsymbol{\Phi}}(s,a) :=\inf_{\widetilde{P}_{h}\in\boldsymbol{\Phi}(P^{\star}_{h}),1 \leq h\leq H}\mathbb{E}_{\{\widetilde{P}_{h}\}_{h=1}^{H},\{\pi_{h}\}_{h=1}^{H }}\left[\sum_{i=h}^{H}R_{i}(s_{i},a_{i})\left|\,s_{h}=s,a_{h}=a\right].\right.\]

Here the expectation is taken w.r.t. the state-action trajectories induced by policy \(\pi\) under the transition \(\widetilde{P}\). One can also extend the definition of the robust value functions in terms of any collection of transition kernel \(P=\{P_{h}\}_{h=1}^{H}\subset\mathcal{P}\) as \(V^{\pi}_{h,P,\boldsymbol{\Phi}}\) and \(Q^{\pi}_{h,P,\boldsymbol{\Phi}}\), which we usually use in the sequel.

Among all the policies, we define the optimal robust policy \(\pi^{\star}\) as the policy that can maximize the robust value function at the initial time step \(h=1\), i.e.,

\[\pi^{\star}\in\operatorname*{argmax}_{\pi=\{\pi_{h}\}_{h=1}^{H}}V^{\pi}_{1,P^ {\star},\boldsymbol{\Phi}}(s_{1}),\quad\forall s_{1}\in\mathcal{S}.\] (2.1)

In other words, the optimal robust policy \(\pi^{\star}\) maximizes the worst case expected total rewards in all possible testing environments. For simplicity and without loss of generality, we assume in the sequel that the initial state \(s_{1}\in\mathcal{S}\) is fixed. Our results could be directly generalized to \(s_{1}\sim p_{0}(\cdot)\in\Delta(\mathcal{S})\). Similarly, we can also define the optimal robust policy associated with a given stochastic process defined through any collection of transition kernels \(P=\{P_{h}\}_{h=1}^{H}\subset\mathcal{P}\) in the same way as (2.1). We denote the optimal robust value functions associated with \(P\) as \(V^{\star}_{h,P,\boldsymbol{\Phi}}\) and \(Q^{\star}_{h,P,\boldsymbol{\Phi}}\) respectively.

\(\mathcal{S}\times\mathcal{A}\)**-rectangularity and robust Bellman equations.** We consider robust sets \(\boldsymbol{\Phi}\) that have the \(\mathcal{S}\times\mathcal{A}\)-rectangular structure (Iyengar, 2005). which requires that the robust set is decoupled and independent across different \((s,a)\)-pairs. This kind of structure results in a dynamic programming representation of the robust value functions (efficient planning), and is thus commonly adopted in the literature of distributionally robust RL. More specifically, we assume the following.

**Assumption 2.1** (\(\mathcal{S}\times\mathcal{A}\)-rectangularity).: _We assume that the mapping \(\boldsymbol{\Phi}\) satisfies for any transition kernel \(P\in\mathcal{P}\), the robust set \(\boldsymbol{\Phi}(P)\) is in the form of_

\[\boldsymbol{\Phi}(P)=\bigotimes_{(s,a)\in\mathcal{S}\times\mathcal{A}}\mathcal{ P}(s,a;P),\quad where\quad\mathcal{P}(s,a;P)\subseteq\Delta(\mathcal{S}).\]

Under above Assumption 2.1, we have the so-called robust Bellman equation (Iyengar, 2005; Blanchet et al., 2023) which gives a dynamic programming representation of robust value functions.

**Proposition 2.2** (Robust Bellman equation).: _Under Assumption 2.1, for any transition \(P=\{P_{h}\}_{h=1}^{H}\subseteq\mathcal{P}\) and any policy \(\pi=\{\pi_{h}\}_{h=1}^{H}\) with \(\pi_{h}:\mathcal{S}\mapsto\Delta(\mathcal{A})\), it holds that_

\[V_{h,P,\mathbf{\Phi}}^{\pi}(s)=\mathbb{E}_{\pi_{h}(\cdot|s)}\big{[}Q_{h,P, \mathbf{\Phi}}^{\pi}(s,\cdot)\big{]},\quad Q_{h,P,\mathbf{\Phi}}^{\pi}(s,a)=R_{ h}(s,a)+\mathbb{E}_{\mathcal{P}(s,a;P_{h})}\big{[}V_{h+1,P,\mathbf{\Phi}}^{\pi} \big{]}.\]

Regarding the robust value functions of the optimal robust policy, we also have the following dynamic programming solution which plays a key role in our algorithm design and theoretical analysis.

**Proposition 2.3** (Robust Bellman optimal equation).: _Under Assumption 2.1, for any \(P=\{P_{h}\}_{h=1}^{H}\subseteq\mathcal{P}\), the robust value functions of any optimal robust policy of \(P\) satisfies that,_

\[V_{h,P,\mathbf{\Phi}}^{\star}(s)=\max_{a\in\mathcal{A}}Q_{h,P,\mathbf{\Phi}}^ {\star}(s,a),\quad Q_{h,P,\mathbf{\Phi}}^{\star}(s,a)=R_{h}(s,a)+\mathbb{E}_{ \mathcal{P}(s,a;P_{h})}\big{[}V_{h+1,P,\mathbf{\Phi}}^{\star}\big{]}.\]

_Taking \(\pi_{h}^{\star}(\cdot|s)=\operatorname*{argmax}_{a\in\mathcal{A}}Q_{h,P, \mathbf{\Phi}}^{\star}(s,a)\), then \(\pi^{\star}=\{\pi_{h}^{\star}\}_{h=1}^{H}\) is optimal robust policy under \(P\)._

**Total-variation distance robust set.** In Assumption 2.1, the robust set \(\mathcal{P}(s,a;P)\) is often modeled as a "distribution ball" centered at \(P(\cdot|s,a)\). In this paper, we mainly consider this type of robust sets specified by a _total-variation distance_ ball. We put it in the following definition.

**Definition 2.4** (Total-variation distance robust set).: _Total-variation distance robust set is defined as_

\[\mathcal{P}_{\rho}(s,a;P):=\left\{\widetilde{P}(\cdot)\in\Delta(\mathcal{S}):D _{\mathrm{TV}}\big{(}\widetilde{P}(\cdot)\big{\|}P(\cdot|s,a)\big{)}\leq\rho \right\},\]

_for some \(\rho\in[0,1)\), where \(D_{\mathrm{TV}}(\cdot\|\cdot)\) denotes the total variation distance defined as_

\[D_{\mathrm{TV}}\big{(}p(\cdot)\|q(\cdot)\big{)}:=\frac{1}{2}\sum_{s\in \mathcal{S}}\big{|}p(s)-q(s)\big{|},\quad\forall p(\cdot),q(\cdot)\in\Delta( \mathcal{S}).\] (2.2)

The TV robust set has recently been extensively studied by Yang et al. (2022); Panaganti and Kalathil (2022); Panaganti et al. (2022); Xu et al. (2023); Blanchet et al. (2023); Shi et al. (2023), which all focus on robust RL with a generative model or with a pre-collected offline dataset. More importantly, we emphasize that by (2.2) in Definition 2.4, we _do not_ define the TV distance through the notion of \(f\)-divergence which requires that the distribution \(p\) is absolute continuous w.r.t. \(q\), as is generally adopted by the above previous works on learning RMDPs with TV robust sets. According to (2.2), we _allow \(p\) to have a different support than \(q\)_. That is, there might exist an \(s\in\mathcal{S}\) such that \(p(s)>0\) and \(q(s)=0\). Given that, the TV robust set in Definition 2.4 could contain transition probabilities that have different supports than the nominal transition probability \(P^{\star}(\cdot|s,a)\).

An essential property of the TV robust set is that the robust expectation involved in the robust Bellman equations (Propositions 2.2 and 2.3) has a duality representation that only uses the expectation under the nominal transition kernel, as is shown in the following theorem and proved in Appendix C.1.

**Proposition 2.5** (Strong duality representation).: _Under Definition 2.4, the following duality representation for the robust expectation holds, for any \(V:\mathcal{S}\mapsto[0,H]\) and \(P_{h}:\mathcal{S}\times\mathcal{A}\mapsto\Delta(\mathcal{S})\),_

\[\mathbb{E}_{\mathcal{P}_{\rho}(s,a;P_{h})}\big{[}V\big{]}=\sup_{\eta\in[0,H]} \Big{\{}-\mathbb{E}_{P_{h}(\cdot|s,a)}\big{[}(\eta-f)_{+}\big{]}-\frac{\rho}{2} \cdot\Big{(}\eta-\min_{s\in\mathcal{S}}V(s)\Big{)}_{+}+\eta\Big{\}}.\] (2.3)

**Value gap between maximum and minimum.** Finally, another useful property of the robust value functions of an RMDP with TV robust sets is a fine characterization of the gap between the maximum and the minimum of the robust value function, which is first identified and utilized by Shi et al. (2023) for an infinite horizon RMDP with TV robust sets. In this work, we prove and use a similar result for the finite horizon case, concluded in the following proposition. The proof is in Appendix C.2.

**Proposition 2.6** (Gap between maximum and minimum).: _Under Assumption 2.1 with the robust set specified by Definition 2.4, the robust value functions satisfies that_

\[\max_{(s,a)\in\mathcal{S}\times\mathcal{A}}Q_{h,P,\mathbf{\Phi}}^{ \pi}(s,a)-\min_{(s,a)\in\mathcal{S}\times\mathcal{A}}Q_{h,P,\mathbf{\Phi}}^{ \pi}(s,a) \leq\min\big{\{}H,\rho^{-1}\big{\}},\] \[\max_{s\in\mathcal{S}}V_{h,P,\mathbf{\Phi}}^{\pi}(s)-\min_{s\in \mathcal{S}}V_{h,P,\mathbf{\Phi}}^{\pi}(s) \leq\min\big{\{}H,\rho^{-1}\big{\}},\]

_for any transition \(P=\{P_{h}\}_{h=1}^{H}\subset\mathcal{P}\), any policy \(\pi\), and any step \(h\in[H]\)._

### Robust RL with Interactive Data Collection

We study how to learn the optimal robust policy \(\pi^{\star}\) in (2.1) from interactive data collection. Specifically, the learner is required to interact with _only_ the _training environment_, i.e., \(P^{\star}\), for some \(K\in\mathbb{N}\) episodes. In each episode \(k\), the learner adopts a policy \(\pi^{k}\) to interact with the training environment \(P^{\star}\) and to collect data. When the \(k\)-th episode ends, the learner updates its policy to \(\pi^{k+1}\) based on historical data and proceeds to the subsequent \((k+1)\)-th episode. The process ends after \(K\) episodes.

**Sample complexity.** We use the notion of _sample complexity_ as the key evaluation metric. For any given algorithm and predetermined accuracy level \(\varepsilon>0\), the sample complexity is the minimum number of episodes \(K\) required for the algorithm to output an \(\varepsilon\)-optimal robust policy \(\widehat{\pi}\) satisfying \(V^{\star}_{1,P^{\star},\Phi}(s_{1})-V^{\widehat{\pi}}_{1,P^{\star},\Phi}(s_{1}) \leq\varepsilon\). The goal is to design algorithms whose sample complexity has small or even optimal dependence on the problem parameters \(S,A,H,\rho\), and \(1/\varepsilon\).

**Online regret.** Another evaluation metric that is related to the minimization of sample complexity is the _online regret_, which is the cumulative difference between the optimal robust policy \(\pi^{\star}\) and the executed policies \(\{\pi^{k}\}_{k=1}^{K}\). Formally, we define \(\operatorname{Regret}_{\Phi}(K):=\sum_{k=1}^{K}V^{\star}_{1,P^{\star},\Phi}(s_ {1})-V^{\pi^{k}}_{1,P^{\star},\Phi}(s_{1})\). The goal is to design algorithms that can achieve a sublinear-in-\(K\) regret with small dependence on \(S,A,H,\rho\). It turns out that any sublinear-regret algorithm can be easily converted to a polynomial-sample complexity algorithm by applying the standard online-to-batch conversion (Jin et al., 2018).

## 3 A Hardness Result: The Curse of Support Shift

Unfortunately, we show in this section that in general such a problem of robust RL with online data collection is _impossible_ - there exists a simple class of two RMDPs such that an \(\Omega(K)\)-online regret lower bound exists. However, previous works on robust RL with a generative model or offline data with good coverage do provide sample-efficient ways to find the optimal robust policy for this class of RMDPs. This is a separation between robust RL with interactive data collection and generative model/offline data. Please see also Figure 1 for an illustration of the example.

**Example 3.1** (Hard example of robust RL with interactive data collection).: _Consider two RMDPs \(\mathcal{M}_{0}\) and \(\mathcal{M}_{1}\) which only differ in their nominal transition kernels. The state space is \(\mathcal{S}=\{s_{\mathrm{good}},s_{\mathrm{bad}}\}\), and the action space is \(\mathcal{A}=\{0,1\}\). The horizon length \(H=3\). The reward function \(R\) always is \(1\) at the good state \(s_{\mathrm{good}}\) and is \(0\) at the bad state \(s_{\mathrm{bad}}\), i.e.,_

\[R_{h}(s,a)=\begin{cases}1,&s=s_{\mathrm{good}}\\ 0,&s=s_{\mathrm{bad}}\end{cases},\quad\forall(a,h)\in\mathcal{A}\times[H].\]

_For the good state \(s_{\mathrm{good}}\), the next state is always \(s_{\mathrm{good}}\). For the bad state \(s_{\mathrm{bad}}\), there is a chance to get to the good state \(s_{\mathrm{good}}\), with the transition probability depending on the action it takes. Formally,_

\[P^{\star,\mathcal{M}_{0}}_{h}(s_{\mathrm{good}}|s_{\mathrm{good }},a) =1,\quad\forall(a,h)\in\mathcal{A}\times\{1,2\},\quad\forall\theta\in\{0,1\},\] \[P^{\star,\mathcal{M}_{0}}_{2}(s_{\mathrm{good}}|s_{\mathrm{bad }},a) =\begin{cases}p,&a=\theta\\ q,&a=1-\theta\end{cases},\quad\forall\theta\in\{0,1\},\]

_where \(p,q\) are two constants satisfying \(0<q<p<1\). Intuitively, when at the bad state, the optimal action would result in a higher transition probability \(p\) to the good state than the transition probability \(q\) induced by the other action. Finally, we consider the robust set being specified by a total-variation distance ball centered at the nominal transition kernel, that is, for any \(P\),_

\[\mathbf{\Phi}(P)=\bigotimes_{(s,a)\in\mathcal{S}\times\mathcal{A}}\mathcal{P} _{\rho}(s,a;P),\ \mathcal{P}_{\rho}(s,a;P)=\left\{\widetilde{P}(\cdot)\in\Delta(\mathcal{S}):D_{ \mathrm{TV}}\big{(}\widetilde{P}(\cdot)\big{\|}P(\cdot|s,a)\big{)}\leq\rho \right\},\] (3.1)

_where \(\rho\in[0,q]\) is the parameter characterizing the size of the robust set. We set \(s_{1}=s_{\mathrm{good}}\)._

For this class of RMDPs, we have the following hardness result for doing robust RL with interactive data collection, an \(\Omega(\rho\cdot K)\)-online regret lower bound. The proof is in Appendix D.1.

**Theorem 3.2** (Hardness result (based on Example 3.1)).: _There exists two RMDPs \(\{\mathcal{M}_{0},\mathcal{M}_{1}\}\), the following regret lower bound holds,_\[\inf_{\mathcal{ALG}}\sup_{\theta\in\{0,1\}}\mathbb{E}\left[\mathrm{Regret}_{\Phi}^{ \mathcal{M}_{\theta},\mathcal{ALG}}(K)\right]\geq\Omega\big{(}\rho\cdot HK\big{)},\]

_where \(\mathrm{Regret}_{\Phi}^{\mathcal{M}_{\theta},\mathcal{ALG}}(K)\) refers to the online regret of algorithm \(\mathcal{ALG}\) for RMDP \(\mathcal{M}_{\theta}\)._

The reason why any algorithm fails for this class of RMDPs is the _support shift_ of the worst-case transition kernel. In robust RL, the performance of a policy \(\pi\) is evaluated via the robust expected total rewards, or equivalently, the expected return under the most adversarial transition kernel \(P^{\dagger,\pi}\). In such an example, as we explicitly show in the proof, when in the good state \(s_{\mathrm{good}}\), the worst-case transition kernel \(P^{\dagger,\pi}\) would transit the state to \(s_{\mathrm{bad}}\) with a constant probability \(\rho>0\). But the state \(s_{\mathrm{bad}}\) is out of the scope of the data collection process because starting from \(s_{1}=s_{\mathrm{good}}\) the nominal transition kernel always transits the state to \(s_{\mathrm{good}}\). As a result, the performance of the learned policy at the bad state \(s_{\mathrm{bad}}\) is not guaranteed, and inevitably incurs an \(\Omega(\rho\cdot K)\)-lower bound of regret, a hardness result. Furthermore, by strategically constructing RMDPs with the horizon \(3H\) based on Example 3.1, we can derive a lower bound of \(\Omega(\rho\cdot HK)\). See Appendix B.3 for more discussions.

## 4 A Solvable Case, Efficient Algorithm, and Sharp Analysis

Motivated by the hard instance (Example 3.1), we now investigate a special subclass of RMDPs with \(\mathcal{S}\times\mathcal{A}\)-rectangular total variation robust set that we show allows for doing sample-efficient robust RL through interactive data collection. In Section 4.1, we introduce the assumption we impose on the RMDP. We propose our algorithm design in Section 4.2, with theoretical analysis in Section 4.3.

### Vanishing Minimal Value: Eliminating Support Shift

To overcome the difficulty of support shift identified in Section 3, we make the following _vanishing minimal value_ assumption on the underlying RMDP.

**Assumption 4.1** (Vanishing minimal value).: _We assume that the underlying RMDP satisfies that \(\min_{s\in\mathcal{S}}V_{1,P^{\star},\Phi}^{\star}(s)=0\). Also, WLOG, we assume that the initial state \(s_{1}\notin\operatorname*{argmin}_{s\in\mathcal{S}}V_{1,P^{\star},\Phi}^{\star} (s)\)._

Assumption 4.1 imposes that the minimal robust expected total rewards over all possible initial states is \(0\). Assuming that the initial state \(s_{1}\notin\operatorname*{argmin}_{s\in\mathcal{S}}V_{1,P^{\star},\Phi}^{ \star}(s)\) avoids making the problem trivial. A close look at Assumption 4.1 actually gives that the minimal robust value function of any policy \(\pi\) at any step is zero, that is, \(\min_{s\in\mathcal{S}}V_{h,P^{\star},\Phi}^{\pi}(s)=0\) for any policy \(\pi\) and any step \(h\in[H]\). With this observation, the following proposition explains why this assumption helps to overcome the difficulty, with the proof of the proposition in Appendix C.3.

**Proposition 4.2** (Equivalent expression of TV robust set with vanishing minimal value).: _For any function \(V:\mathcal{S}\mapsto[0,H]\) with \(\min_{s\in\mathcal{S}}V(s)=0\), we have that_

\[\mathbb{E}_{\mathcal{P}_{\rho}(s,a;P_{h}^{\star})}\left[V\right]=\rho^{\prime} \cdot\mathbb{E}_{\mathcal{B}_{\rho^{\prime}}(s,a;P_{h}^{\star})}[V],\quad with \quad\rho^{\prime}=1-\frac{\rho}{2}>0,\]

_where the TV robust set \(\mathcal{P}_{\rho}(s,a;P_{h}^{\star})\) is defined in (3.1) and the set \(\mathcal{B}_{\rho^{\prime}}(s,a;P_{h}^{\star})\) is defined as2_

Footnote 2: Here we implicitly define \(\frac{0}{0}=0\) and \(\frac{a}{0}=\infty\) for any \(a>0\).

\[\mathcal{B}_{\rho^{\prime}}(s,a;P_{h}^{\star})=\left\{\widetilde{P}(\cdot)\in \Delta(\mathcal{S}):\sup_{s^{\prime}\in\mathcal{S}}\frac{\widetilde{P}(s^{ \prime})}{P_{h}^{\star}(s^{\prime}|s,a)}\leq\frac{1}{\rho^{\prime}}\right\}.\]As Proposition 4.2 indicates, under Assumption 4.1, the robust Bellman equations (Propositions 2.2 and 2.3) at step \(h\in[H]\) is equivalent to taking an infimum over another robust set \(\mathcal{B}_{\rho^{\prime}}(s,a;P_{h}^{*})\) that shares the _same_ support as the nominal transition kernel \(P^{*}(\cdot|s,a)\), discounted by a constant \(\rho^{\prime}<1\). Intuitively, this new robust set rules out the difficulty originated in unseen states in training environments and the discount factor \(\rho^{\prime}\) hedges the difficulty from prohibitively small probability of reaching certain states that may appear often in the testing environments. This renders robust RL with interactive data collection possible. See Appendix B.4.1 for discussions/examples of Assumption 4.1.

### Algorithm Design: Oprovi-Tv

In this section, we propose our algorithm that solves robust RL with interactive data collection for RMDPs with \(\mathcal{S}\times\mathcal{A}\)-rectangular total-variation (TV) robust sets (Assumption 2.1 and Definition 2.4) and satisfying the vanishing minimal value assumption (Assumption 4.1). Our algorithm, OOptimistic RObust Value Iteration for TV Robust Set (OPROVI-TV, Algorithm 1), can automatically balance exploitation and exploration during the interactive data collecting process while managing the distributional robustness of the learned policy. The full algorithm OPROVI-TV is given in Algorithm 1.

```
1:Initialize: dataset \(\mathbb{D}=\emptyset\).
2:for episode \(k=1,\cdots,K\)do
3: Training environment transition estimation:
4: Update the count functions \(N_{h}^{k}(s,a,s^{\prime})\) and \(N_{h}^{k}(s,a)\) based on \(\mathbb{D}\).
5: Calculate the transition kernel estimator \(\widehat{P}_{h}^{k}\) as \(N_{h}^{k}(s,a,s^{\prime})/(N_{h}^{k}(s,a)\lor 1)\).
6: Optimistic robust planning:
7: Set \(\overline{V}_{H+1}^{k}=\underline{V}_{H+1}^{k}=0\).
8:for step \(h=H,\cdots,1\)do
9: Set \(\overline{Q}_{h}^{k}(\cdot,\cdot)\) and \(\underline{Q}_{h}^{k}(\cdot,\cdot)\) as (4.2) and (4.3), with \(\mathsf{bonus}_{h}^{k}(\cdot,\cdot)\) defined in (4.5).
10: Set \(\pi_{h}^{k}(\cdot|\cdot)=\operatorname*{argmax}_{a\in\mathcal{A}}\,\overline {Q}_{h}^{k}(\cdot,a),\,\,\overline{V}_{h}^{k}(\cdot)=\mathbb{E}_{\pi_{h}^{k}( \cdot|\cdot)}[\overline{Q}_{h}^{k}(\cdot,\cdot)]\), and \(\underline{V}_{h}^{k}(\cdot)=\mathbb{E}_{\pi_{h}^{k}(\cdot|\cdot)}[\underline {Q}_{h}^{k}(\cdot,\cdot)]\).
11:endfor
12: Execute the policy in training environment and collect data:
13: Receive the initial state \(s_{1}^{k}\in\mathcal{S}\).
14:for step \(h=1,\cdots,H\)do
15: Take action \(a_{h}^{k}\sim\pi_{h}^{k}(\cdot|s_{h}^{k})\), observe reward \(R_{h}(s_{h}^{k},a_{h}^{k})\) and the next state \(s_{h+1}^{k}\).
16:endfor
17: Set \(\mathbb{D}\) as \(\mathbb{D}\cup\{(s_{h}^{k},a_{h}^{k},s_{h+1}^{k})\}_{h=1}^{H}\).
18:endfor
19:Output: Randomly (uniformly) return a policy from \(\{\pi^{k}\}_{k=1}^{K}\). ```

**Algorithm 1** OOptimistic Robust Value Iteration for TV Robust Set (OPROVI-TV)

**Step I: Training Environment Transition Estimation (Line 3 to 5).** At the beginning of each episode \(k\in[K]\), we maintain an estimate of the transition kernel \(P^{*}\) of the training environment by using the historical data \(\mathbb{D}=\{(s_{h}^{r},a_{h}^{r},s_{h+1}^{r})\}_{r=1,h=1}^{k-1,H}\) collected from the interaction with the training environment. Specifically, we simply adopt a vanilla empirical estimator, defined as \(\widehat{P}_{h}^{k}(s^{\prime}|s,a)=N_{h}^{k}(s,a,s^{\prime})/(N_{h}^{k}(s,a) \lor 1)\) for any \((s,a,h,s^{\prime})\in\mathcal{S}\times\mathcal{A}\times\mathcal{S}\times[H]\), where the count functions \(N_{h}^{k}(s,a,s^{\prime})\) and \(N_{h}^{k}(s,a)\) are calculated based on the current dataset \(\mathbb{D}\) by \(N_{h}^{k}(s,a,s^{\prime})=\sum_{r=1}^{k-1}\mathbf{1}\big{\{}((s_{h}^{r},a_{h}^ {r},s_{h+1}^{r})=(s,a,s^{\prime})\big{\}}\) and \(N_{h}^{k}(s,a)=\sum_{s^{\prime}\in\mathcal{S}}N_{h}^{k}(s,a,s^{\prime})\) for any \((s,a,h,s^{\prime})\in\mathcal{S}\times\mathcal{A}\times\mathcal{S}\times[H]\). This just coincides with the transition estimator adopted by existing non-robust online RL algorithms (Auer et al., 2008; Azar et al., 2017; Zhang et al., 2021).

**Step II: Optimistic Robust Planning (Line 6 to 11).** Given \(\widehat{P}^{k}(\cdot|\cdot,\cdot)\) that estimates the training environment, we perform an optimistic robust planning to construct the policy \(\pi^{k}\) to execute. Basically, the optimistic robust planning follows the robust Bellman optimal equation (Proposition 2.3) to approximate the optimal robust policy, but differs in that it maintains an upper bound and a lower bound of the optimal robust value function and chooses the policy that maximizes the optimistic estimate to incentivize exploration during data collection. Here the purpose of maintaining the lower bound estimate is to facilitate the construction of the variance-aware optimistic bonus (see following), which helps to sharpen our theoretical analysis.

\(\triangleright\)_Simplifying the robust expectation._ To utilize the vanishing minimal value condition (Assumption 4.1), we take a closer look into the robust Bellman equation. By strong duality (Proposition 2.5), the robust expectation \(\mathbb{E}_{\mathcal{P}_{\mu}(s,a;P)}[V]\) for any \(V\in[0,H]\) satisfying \(\min_{s\in\mathcal{S}}V(s)=0\) is equivalent to

\[\mathbb{E}_{\mathcal{P}_{\rho}(s,a;P)}\big{[}V\big{]}=\sup_{\eta \in[0,H]}\bigg{\{}-\mathbb{E}_{P(\cdot|s,a)}\Big{[}\big{(}\eta-V\big{)}_{+} \Big{]}+\Big{(}1-\frac{\rho}{2}\Big{)}\cdot\eta\bigg{\}}.\] (4.1)

Consequently, with a slight abuse of the notation, in the remaining of the paper, we **re-define** the operator \(\mathbb{E}_{\mathcal{P}_{\rho}(s,a;P)}[V]\) as the right hand side of (4.1). Due to Assumption 4.1, the robust Bellman (optimal) equation (Proposition 2.2 and Proposition 2.3) still holds under this new definition.

\(\triangleright\)_Optimistic robust planning._ With this in mind, the optimistic robust planning goes as follows. Starting from \(\overline{V}^{k}_{H+1}=\underline{V}^{k}_{H+1}=0\), we recursively define that for any \((s,a)\in\mathcal{S}\times\mathcal{A}\),

\[\overline{Q}^{k}_{h}(s,a) =\min\left\{R_{h}(s,a)+\mathbb{E}_{\mathcal{P}_{\rho}(s,a; \widehat{P}^{k}_{h})}\Big{[}\overline{V}^{k}_{h+1}\Big{]}+\texttt{bonus}^{k}_ {h}(s,a),\min\left\{H,\rho^{-1}\right\}\right\},\] (4.2) \[\underline{Q}^{k}_{h}(s,a) =\max\left\{R_{h}(s,a)+\mathbb{E}_{\mathcal{P}_{\rho}(s,a; \widehat{P}^{k}_{h})}\Big{[}\overline{V}^{k}_{h+1}\Big{]}-\texttt{bonus}^{k}_ {h}(s,a),0\right\},\] (4.3)

where the robust expectation \(\mathbb{E}_{\mathcal{P}_{\rho}(s,a;\widehat{P}^{k}_{h})}\) follows the definition in the right hand side of (4.1), and the bonus function \(\texttt{bonus}^{k}_{h}(s,a)\geq 0\) is defined later. Here we truncate the optimistic estimate \(\overline{Q}^{k}_{h}\) via the upper bound \(\min\{H,\rho^{-1}\}\) of the true optimal robust value function \(Q^{*}_{h,P^{*},\,\Phi}\). This truncation arises from the combined implication of Proposition 2.6 and the fact that \(\min_{(s,a)\in\mathcal{S}\times\mathcal{A}}Q^{*}_{h,P^{*},\,\Phi}(s,a)=0\) under Assumption 4.1. As we establish in Lemma E.2, \(\overline{Q}^{k}_{h}\) and \(\underline{Q}^{k}_{h}\) form upper and lower bounds for \(Q^{*}_{h,P^{*},\,\Phi}\) and \(Q^{\pi^{k}}_{h,P^{*},\,\Phi}\) under a proper choice of the bonus. After performing (4.2) and (4.3), we choose the data collection policy \(\pi^{k}_{h}\) to be the optimal policy with respect to the optimistic estimator \(\overline{Q}^{k}_{h}\) and define \(\overline{V}^{k}_{h}\) and \(\underline{V}^{k}_{h}\) accordingly by

\[\pi^{k}_{h}(\cdot|\cdot)=\operatorname*{argmax}_{a\in\mathcal{A}} \,\,\overline{Q}^{k}_{h}(\cdot,a),\quad\overline{V}^{k}_{h}(s)=\mathbb{E}_{ \pi^{k}_{h}(\cdot|s)}\Big{[}\overline{Q}^{k}_{h}(s,\cdot)\Big{]},\quad \underline{V}^{k}_{h}(s)=\mathbb{E}_{\pi^{k}_{h}(\cdot|s)}\Big{[}\underline{Q }^{k}_{h}(s,\cdot)\Big{]}.\] (4.4)

We remark that the purpose of maintaining the lower bound estimate (4.3) is to facilitate the construction of the bonus and to help sharpen our theoretical analysis. The construction of the policy \(\pi^{k}\) is still based on the optimistic estimator, which is why we call it optimistic robust planning. As indicated by theory, the optimistic robust planning can effectively guide the policy to explore uncertain _robust_ value function estimates, striking a balance between exploration and exploitation while managing distributional robustness.

\(\triangleright\)_Bonus function._ The bonus function \(\texttt{bonus}^{k}_{h}(s,a)\) is a Bernstein-style bound defined as

\[\sqrt{\frac{\mathbb{V}_{\widehat{P}^{k}_{h}(\cdot|s,a)}\Big{[} \Big{(}\overline{V}^{k}_{h+1}+\underline{V}^{k}_{h+1}\Big{)}/2\Big{]}c_{1} \iota}{H}}+\frac{2\mathbb{E}_{\widehat{P}^{k}_{h}(\cdot|s,a)}\Big{[}\overline {V}^{k}_{h+1}-\underline{V}^{k}_{h+1}\Big{]}}{H}+\frac{c_{2}H^{2}S\iota}{N^{k} _{h}(s,a)\lor 1}+\frac{1}{\sqrt{K}},\] (4.5)

where \(\iota=\log(S^{3}AH^{2}K^{3/2}/\delta)\), \(c_{1},c_{2}>0\) are absolute constants, and \(\delta\) signifies a pre-selected fail probability. Under (4.5), \(\overline{Q}^{k}_{h}\) and \(\underline{Q}^{k}_{h}\) become upper and lower bounds of the optimal robust value functions (Lemma E.2). More importantly, the bonus (4.5) is carefully designed for robust value functions such that the summation of this bonus term (especially the leading variance term in (4.5)) over time steps is well controlled, for which we also develop new analysis methods. This is critical for obtaining a sharp sample complexity of Algorithm 1.

### Theoretical Guarantees

This section establishes the online regret and the sample complexity of OPROVI-TV (Algorithm 1). Our main result is the following, upper bounding the online regret of Algorithm 1, proved in Appendix E.

**Theorem 4.3** (Online regret of Oprovi-tv).: _Given an RMDP with \(\mathcal{S}\times\mathcal{A}\)-rectangular total-variation robust set of radius \(\rho\in[0,1)\) (Assumption 2.1 and Definition 2.4) satisfying Assumptions 4.1, choosing the bonus function as (4.5) with sufficiently large \(c_{1},c_{2}>0\), then with probability at least \(1-\delta\), Algorithm 1 satisfies_

\[\mathrm{Regret}_{\boldsymbol{\Phi}}(K)\leq\mathcal{O}\Big{(}\sqrt{\min\big{\{} H,\rho^{-1}\big{\}}H^{2}SAK\iota^{\prime}}\Big{)},\]

_where \(\iota^{\prime}=\log^{2}(SAHK/\delta)\) and \(\mathcal{O}(\cdot)\) hides absolute constants and lower order terms in \(K\)._

Theorem 4.3 shows that Algorithm 1 enjoys a sublinear online regret of \(\widetilde{\mathcal{O}}(\sqrt{K})\), meaning that it is able to approximately find the optimal robust policy through interactive data collection. This is in contrast with the general hardness result in Section 3 where sample-efficient learning is impossible in the worst case. Thus we show the effectiveness of the minimal value assumption for robust RL with interactive data collection. As a corollary, we have the following sample complexity bound for Algorithm 1. It is obtained directly from Theorem 4.3 and a standard online to batch conversion.

**Corollary 4.4** (Sample complexity of Oprovi-tv).: _Under the same setup and conditions as in Theorem 4.3, with probability at least \(1-\delta\), Algorithm 1 can output an \(\varepsilon\)-optimal policy within_

\[\mathcal{O}\left(\min\big{\{}H,\rho^{-1}\big{\}}H^{2}SA\iota^{\prime\prime}/ \varepsilon^{2}\right)\] (4.6)

_episodes, where \(\iota^{\prime\prime}=\log(SAH/\varepsilon\delta)\) and \(\mathcal{O}(\cdot)\) hides absolute constants. The valid range of \(\varepsilon\) satisfies \(\varepsilon\in(0,c\cdot\min\{1,1/(\rho H)\}]\) for some constant \(c>0\)._

We compare the sample complexity (4.6) with prior arts on non-robust online RL and robust RL with a generative model. On the one hand, (4.6) with \(\rho=0\) equals to \(\widetilde{\mathcal{O}}(H^{3}SA/\varepsilon^{2})\), matching the minimax sample complexity lower bound for online RL in non-robust MDPs (Azar et al., 2017). This means that our algorithm design can naturally handle non-robust MDPs as a special case (please also see Remark B.4 for why one can reduce Algorithm 1 to general non-robust MDPs under Assumption 4.1). On the other hand, the previous work of Shi et al. (2023) for robust RL in infinite horizon RMDPs with a TV robust set and a generative model showcases a minimax optimal sample complexity of

\[\widetilde{\mathcal{O}}\left(\min\big{\{}H_{\gamma},\rho^{-1}\big{\}}H_{ \gamma}^{2}SA/\varepsilon^{2}\right),\]

for \(\rho\in[0,1)\), where we \(H_{\gamma}:=1/(1-\gamma)\) is the effective horizon of the infinite \(\gamma\)-discounted RMDPs. As a result, the sample complexity (4.6) of Algorithm 1 matches their result. We highlight that our algorithm does not rely on a generative model and operates purely through interactive data collection.

**Extensions of Algorithm 1 and its theory.** In Appendix B.4.2, we extend Algorithm 1 to solve a new type of RMDPs whose robust set consists of transition probabilities with bounded ratio to the nominal kernel. The intuition is because it is equivalent to the \(\mathcal{S}\times\mathcal{A}\)-rectangular RMDP with a TV robust set and vanishing minimal value assumption in an appropriate sense (Proposition 4.2) Consequently, by a clever usage of Algorithm 1, we can also solve this new model sample-efficiently, as is shown in Corollary B.5. Such a result echoes our intuition on the curse of support shift.

## 5 Conclusions and future works

This work shows that in the absence of any structural assumptions, robust RL via interactive data collection necessarily induces a linear regret lower bound in the worst case due to the curse of support shift. Under the vanishing minimal value assumption, an assumption that is able to effectively rule out the potential support shift issues for RMDPs with a TV robust set, we propose a sample-efficient robust RL algorithm for those RMDPs with sharp analysis. Potential future works include extending to function approximation settings and other types of robust sets. See discussion in Appendix B.5.

## Acknowledgments and Disclosure of Funding

The material in this paper is based upon work supported by the Air Force Office of Scientific Research under award number FA9550-20-1-0397. Additional support is gratefully acknowledged from NSF 1915967, 2118199, 2229012, 2312204. The authors would like to thank the anonymous reviewers for their helpful comments. The authors would also like to thank Pan Xu and Zhishuai Liu for their feedback on an early draft of this work.

## References

* Agarwal et al. (2019)Agarwal, A., Jiang, N., Kakade, S. M. and Sun, W. (2019). Reinforcement learning: Theory and algorithms. _CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep_ 10-4.
* Agarwal et al. (2023)Agarwal, A., Jin, Y. and Zhang, T. (2023). Vo \(q\) l: Towards optimal regret in model-free rl with nonlinear function approximation. In _The Thirty Sixth Annual Conference on Learning Theory_. PMLR.
* Auer et al. (2008)Auer, P., Jaksch, T. and Ortner, R. (2008). Near-optimal regret bounds for reinforcement learning. _Advances in neural information processing systems_**21**.
* Ayoub et al. (2020)Ayoub, A., Jia, Z., Szepesvari, C., Wang, M. and Yang, L. (2020). Model-based reinforcement learning with value-targeted regression. In _International Conference on Machine Learning_. PMLR.
* Azar et al. (2017)Azar, M. G., Osband, I. and Munos, R. (2017). Minimax regret bounds for reinforcement learning. In _International Conference on Machine Learning_. PMLR.
* Badrinath and Kalathil (2021)Badrinath, K. P. and Kalathil, D. (2021). Robust reinforcement learning using least squares policy iteration with provable performance guarantees. In _International Conference on Machine Learning_. PMLR.
* Blanchet et al. (2023)Blanchet, J., Lu, M., Zhang, T. and Zhong, H. (2023). Double pessimism is provably efficient for distributionally robust offline reinforcement learning: Generic algorithm and robust partial coverage. _arXiv preprint arXiv:2305.09659_.
* Clavier et al. (2023)Clavier, P., Pennec, E. L. and Geist, M. (2023). Towards minimax optimality of model-based robust reinforcement learning. _arXiv preprint arXiv:2302.05372_.
* Dann et al. (2017)Dann, C., Lattimore, T. and Brunskill, E. (2017). Unifying pac and regret: Uniform pac bounds for episodic reinforcement learning. _Advances in Neural Information Processing Systems_**30**.
* Ding et al. (2024)Ding, W., Shi, L., Chi, Y. and Zhao, D. (2024). Seeing is not believing: Robust reinforcement learning against spurious correlation. _Advances in Neural Information Processing Systems_**36**.
* Dong et al. (2022)Dong, J., Li, J., Wang, B. and Zhang, J. (2022). Online policy optimization for robust mdp. _arXiv preprint arXiv:2209.13841_.
* Du et al. (2021)Du, S., Kakade, S., Lee, J., Lovett, S., Mahajan, G., Sun, W. and Wang, R. (2021). Bilinear classes: A structural framework for provable generalization in rl. In _International Conference on Machine Learning_. PMLR.
* El Ghaoui and Nilim (2005)El Ghaoui, L. and Nilim, A. (2005). Robust solutions to markov decision problems with uncertain transition matrices. _Operations Research_**53** 780-798.
* Foster et al. (2021)Foster, D. J., Kakade, S. M., Qian, J. and Rakhlin, A. (2021). The statistical complexity of interactive decision making. _arXiv preprint arXiv:2112.13487_.
* He et al. (2023)He, J., Zhao, H., Zhou, D. and Gu, Q. (2023). Nearly minimax optimal reinforcement learning for linear markov decision processes. In _International Conference on Machine Learning_. PMLR.
* Hu et al. (2022)Hu, J., Zhong, H., Jin, C. and Wang, L. (2022). Provable sim-to-real transfer in continuous domain with partial observations. _arXiv preprint arXiv:2210.15598_.
* Huang et al. (2023a)Huang, J., Zhong, H., Wang, L. and Yang, L. F. (2023a). Horizon-free and instance-dependent regret bounds for reinforcement learning with general function approximation. _arXiv preprint arXiv:2312.04464_.
* Huang et al. (2023b)Huang, J., Zhong, H., Wang, L. and Yang, L. F. (2023b). Tackling heavy-tailed rewards in reinforcement learning with function approximation: Minimax optimal and instance-dependent regret bounds. _arXiv preprint arXiv:2306.06836_.
* Huang et al. (2023c)* Iyengar (2005)Iyengar, G. N. (2005). Robust dynamic programming. _Mathematics of Operations Research_**30** 257-280.
* Jiang et al. (2017)Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J. and Schapire, R. E. (2017). Contextual decision processes with low bellman rank are pac-learnable. In _International Conference on Machine Learning_. PMLR.
* Jin et al. (2018)Jin, C., Allen-Zhu, Z., Bubeck, S. and Jordan, M. I. (2018). Is q-learning provably efficient? _Advances in neural information processing systems_**31**.
* Jin et al. (2021)Jin, C., Liu, Q. and Miryoosefi, S. (2021). Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms. _Advances in neural information processing systems_**34** 13406-13418.
* Jin et al. (2020)Jin, C., Yang, Z., Wang, Z. and Jordan, M. I. (2020). Provably efficient reinforcement learning with linear function approximation. In _Conference on Learning Theory_. PMLR.
* Kiran et al. (2021)Kiran, B. R., Sobh, I., Talpaert, V., Mannion, P., Al Sallab, A. A., Yogamani, S. and Perez, P. (2021). Deep reinforcement learning for autonomous driving: A survey. _IEEE Transactions on Intelligent Transportation Systems_**23** 4909-4926.
* Kober et al. (2013)Kober, J., Bagnell, J. A. and Peters, J. (2013). Reinforcement learning in robotics: A survey. _The International Journal of Robotics Research_**32** 1238-1274.
* Kuang et al. (2022)Kuang, Y., Lu, M., Wang, J., Zhou, Q., Li, B. and Li, H. (2022). Learning robust policy against disturbance in transition dynamics via state-conservative policy optimization. In _Proceedings of the AAAI Conference on Artificial Intelligence_, vol. 36.
* Li et al. (2023)Li, G., Cai, C., Chen, Y., Wei, Y. and Chi, Y. (2023). Is q-learning minimax optimal? a tight sample complexity analysis. _Operations Research_.
* Li and Lan (2023)Li, Y. and Lan, G. (2023). First-order policy optimization for robust policy evaluation. _arXiv preprint arXiv:2307.15890_.
* Liu et al. (2022)Liu, Z., Lu, M., Wang, Z., Jordan, M. and Yang, Z. (2022). Welfare maximization in competitive equilibrium: Reinforcement learning for markov exchange economy. In _International Conference on Machine Learning_. PMLR.
* Liu et al. (2023)Liu, Z., Lu, M., Xiong, W., Zhong, H., Hu, H., Zhang, S., Zheng, S., Yang, Z. and Wang, Z. (2023). One objective to rule them all: A maximization objective fusing estimation and planning for exploration. _arXiv preprint arXiv:2305.18258_.
* Liu and Xu (2024a)Liu, Z. and Xu, P. (2024a). Distributionally robust off-dynamics reinforcement learning: Provable efficiency with linear function approximation. _arXiv preprint arXiv:2402.15399_.
* Liu and Xu (2024b)Liu, Z. and Xu, P. (2024b). Minimax optimal and computationally efficient algorithms for distributionally robust offline reinforcement learning. _arXiv preprint arXiv:2403.09621_.
* Lykouris et al. (2021)Lykouris, T., Simchowitz, M., Slivkins, A. and Sun, W. (2021). Corruption-robust exploration in episodic reinforcement learning. In _Conference on Learning Theory_. PMLR.
* Ma et al. (2022)Ma, X., Liang, Z., Xia, L., Zhang, J., Blanchet, J., Liu, M., Zhao, Q. and Zhou, Z. (2022). Distributionally robust offline reinforcement learning with linear function approximation. _arXiv preprint arXiv:2209.06620_.
* Maurer and Pontil (2009)Maurer, A. and Pontil, M. (2009). Empirical bernstein bounds and sample variance penalization. _arXiv preprint arXiv:0907.3740_.
* Menard et al. (2021)Menard, P., Domingues, O. D., Shang, X. and Valko, M. (2021). Ucb momentum q-learning: Correcting the bias without forgetting. In _International Conference on Machine Learning_. PMLR.
* Moos et al. (2022)Moos, J., Hansel, K., Abdulsamad, H., Stark, S., Clever, D. and Peters, J. (2022). Robust reinforcement learning: A review of foundations and recent advances. _Machine Learning and Knowledge Extraction_**4** 276-315.
* Moos et al. (2021)Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A. et al. (2022). Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_**35** 27730-27744.
* Panaganti and Kalathil (2022)Panaganti, K. and Kalathil, D. (2022). Sample complexity of robust reinforcement learning with a generative model. In _International Conference on Artificial Intelligence and Statistics_. PMLR.
* Panaganti et al. (2022)Panaganti, K., Xu, Z., Kalathil, D. and Ghavamzadeh, M. (2022). Robust reinforcement learning using offline data. _arXiv preprint arXiv:2208.05129_.
* Peng et al. (2018)Peng, X. B., Andrychowicz, M., Zaremba, W. and Abbeel, P. (2018). Sim-to-real transfer of robotic control with dynamics randomization. In _2018 IEEE international conference on robotics and automation (ICRA)_. IEEE.
* Pinto et al. (2017)Pinto, L., Davidson, J., Sukthankar, R. and Gupta, A. (2017). Robust adversarial reinforcement learning. In _International Conference on Machine Learning_. PMLR.
* Sadeghi and Levine (2016)Sadeghi, F. and Levine, S. (2016). Cad2rl: Real single-image flight without a single real image. _arXiv preprint arXiv:1611.04201_.
* Shi and Chi (2022)Shi, L. and Chi, Y. (2022). Distributionally robust model-based offline reinforcement learning with near-optimal sample complexity. _arXiv preprint arXiv:2208.05767_.
* Shi et al. (2023)Shi, L., Li, G., Wei, Y., Chen, Y., Geist, M. and Chi, Y. (2023). The curious price of distributional robustness in reinforcement learning with a generative model. _arXiv preprint arXiv:2305.16589_.
* Si et al. (2023)Si, N., Zhang, F., Zhou, Z. and Blanchet, J. (2023). Distributionally robust batch contextual bandits. _Management Science_.
* Silver et al. (2017)Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A. et al. (2017). Mastering the game of go without human knowledge. _nature_**550** 354-359.
* Sun et al. (2019)Sun, W., Jiang, N., Krishnamurthy, A., Agarwal, A. and Langford, J. (2019). Model-based rl in contextual decision processes: Pac bounds and exponential improvements over model-free approaches. In _Conference on learning theory_. PMLR.
* Sutton and Barto (2018)Sutton, R. S. and Barto, A. G. (2018). _Reinforcement learning: An introduction_. MIT press.
* Wang et al. (2024)Wang, H., Shi, L. and Chi, Y. (2024). Sample complexity of offline distributionally robust linear markov decision processes. _arXiv preprint arXiv:2403.12946_.
* Wang et al. (2018)Wang, L., Zhang, W., He, X. and Zha, H. (2018). Supervised reinforcement learning with recurrent neural network for dynamic treatment recommendation. In _Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining_.
* Wang et al. (2022)Wang, Q., Ho, C. P. and Petrik, M. (2022). On the convergence of policy gradient in robust mdps. _arXiv preprint arXiv:2212.10439_.
* Wang et al. (2023a)Wang, Q., Ho, C. P. and Petrik, M. (2023a). Policy gradient in robust MDPs with global convergence guarantee. In _Proceedings of the 40th International Conference on Machine Learning_ (A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato and J. Scarlett, eds.), vol. 202 of _Proceedings of Machine Learning Research_. PMLR.
* Wang et al. (2023b)Wang, S., Si, N., Blanchet, J. and Zhou, Z. (2023b). A finite sample complexity bound for distributionally robust q-learning. In _International Conference on Artificial Intelligence and Statistics_. PMLR.
* Wang et al. (2023c)Wang, S., Si, N., Blanchet, J. and Zhou, Z. (2023c). On the foundation of distributionally robust reinforcement learning. _arXiv preprint arXiv:2311.09018_.
* Wang et al. (2023d)Wang, S., Si, N., Blanchet, J. and Zhou, Z. (2023d). Sample complexity of variance-reduced distributionally robust q-learning. _arXiv preprint arXiv:2305.18420_.
* Wang and Zou (2021)Wang, Y. and Zou, S. (2021). Online robust reinforcement learning with model uncertainty. _Advances in Neural Information Processing Systems_**34** 7193-7206.
* Wang and Zou (2022)Wang, Y. and Zou, S. (2022). Policy gradient method for robust reinforcement learning. In _International Conference on Machine Learning_. PMLR.
* Wei et al. (2022)Wei, C.-Y., Dann, C. and Zimmert, J. (2022). A model selection approach for corruption robust reinforcement learning. In _International Conference on Algorithmic Learning Theory_. PMLR.
* Wiesemann et al. (2013)Wiesemann, W., Kuhn, D. and Rustem, B. (2013). Robust markov decision processes. _Mathematics of Operations Research_**38** 153-183.
* Wu et al. (2022)Wu, T., Yang, Y., Zhong, H., Wang, L., Du, S. and Jiao, J. (2022). Nearly optimal policy optimization with stable at any time guarantee. In _International Conference on Machine Learning_. PMLR.
* Xu and Mannor (2010)Xu, H. and Mannor, S. (2010). Distributionally robust markov decision processes. _Advances in Neural Information Processing Systems_**23**.
* Xu and Zeevi (2023)Xu, Y. and Zeevi, A. (2023). Bayesian design principles for frequentist sequential learning. In _International Conference on Machine Learning_. PMLR.
* Xu et al. (2023)Xu, Z., Panaganti, K. and Kalathil, D. (2023). Improved sample complexity bounds for distributionally robust reinforcement learning. In _International Conference on Artificial Intelligence and Statistics_. PMLR.
* Yang et al. (2023a)Yang, R., Zhong, H., Xu, J., Zhang, A., Zhang, C., Han, L. and Zhang, T. (2023a). Towards robust offline reinforcement learning under diverse data corruption. _arXiv preprint arXiv:2310.12955_.
* Yang et al. (2023b)Yang, W., Wang, H., Kozuno, T., Jordan, S. M. and Zhang, Z. (2023b). Avoiding model estimation in robust markov decision processes with a generative model. _arXiv preprint arXiv:2302.01248_.
* Yang et al. (2022)Yang, W., Zhang, L. and Zhang, Z. (2022). Toward theoretical understandings of robust markov decision processes: Sample complexity and asymptotics. _The Annals of Statistics_**50** 3223-3248.
* Ye et al. (2024)Ye, C., He, J., Gu, Q. and Zhang, T. (2024). Towards robust model-based reinforcement learning against adversarial corruption. _arXiv preprint arXiv:2402.08991_.
* Ye et al. (2023a)Ye, C., Xiong, W., Gu, Q. and Zhang, T. (2023a). Corruption-robust algorithms with uncertainty weighting for nonlinear contextual bandits and markov decision processes. In _International Conference on Machine Learning_. PMLR.
* Ye et al. (2023b)Ye, C., Yang, R., Gu, Q. and Zhang, T. (2023b). Corruption-robust offline reinforcement learning with general function approximation. _arXiv preprint arXiv:2310.14550_.
* Yu et al. (2023)Yu, Z., Dai, L., Xu, S., Gao, S. and Ho, C. P. (2023). Fast bellman updates for wasserstein distributionally robust mdps. In _Thirty-seventh Conference on Neural Information Processing Systems_.
* Zanette and Brunskill (2019)Zanette, A. and Brunskill, E. (2019). Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds. In _International Conference on Machine Learning_. PMLR.
* Zhang et al. (2022)Zhang, X., Chen, Y., Zhu, X. and Sun, W. (2022). Corruption-robust offline reinforcement learning. In _International Conference on Artificial Intelligence and Statistics_. PMLR.
* Zhang et al. (2023)Zhang, Z., Chen, Y., Lee, J. D. and Du, S. S. (2023). Settling the sample complexity of online reinforcement learning. _arXiv preprint arXiv:2307.13586_.

Zhang, Z., Ji, X. and Du, S. (2021). Is reinforcement learning more difficult than bandits? a near-optimal algorithm escaping the curse of horizon. In _Conference on Learning Theory_. PMLR.
* Zhang et al. (2020)Zhang, Z., Zhou, Y. and Ji, X. (2020). Almost optimal model-free reinforcement learningvia reference-advantage decomposition. _Advances in Neural Information Processing Systems_**33** 15198-15207.
* Zhao et al. (2020)Zhao, W., Queralta, J. P. and Westerlund, T. (2020). Sim-to-real transfer in deep reinforcement learning for robotics: a survey. In _2020 IEEE Symposium Series on Computational Intelligence (SSCI)_. IEEE.
* Zhong et al. (2022)Zhong, H., Xiong, W., Zheng, S., Wang, L., Wang, Z., Yang, Z. and Zhang, T. (2022). Gec: A unified framework for interactive decision making in mdp, pomdp, and beyond. _arXiv preprint arXiv:2211.01962_.
* Zhong and Zhang (2023)Zhong, H. and Zhang, T. (2023). A theoretical analysis of optimistic proximal policy optimization in linear markov decision processes. _arXiv preprint arXiv:2305.08841_.
* Zhou et al. (2021a)Zhou, D., Gu, Q. and Szepesvari, C. (2021a). Nearly minimax optimal reinforcement learning for linear mixture markov decision processes. In _Conference on Learning Theory_. PMLR.
* Zhou et al. (2023)Zhou, R., Liu, T., Cheng, M., Kalathil, D., Kumar, P. and Tian, C. (2023). Natural actor-critic for robust reinforcement learning with function approximation. In _Thirty-seventh Conference on Neural Information Processing Systems_.
* Zhou et al. (2021b)Zhou, Z., Zhou, Z., Bai, Q., Qiu, L., Blanchet, J. and Glynn, P. (2021b). Finite-sample regret bound for distributionally robust offline tabular reinforcement learning. In _International Conference on Artificial Intelligence and Statistics_. PMLR.

Related Works

We give a detailed discussion on the related works in this section.

Robust reinforcement learning in robust Markov decision processes.Robust RL is usually framed as a robust Markov decision process (RMDP) (Iyengar, 2005; El Ghaoui and Nilim, 2005; Wiesemann et al., 2013). There is a long line of work dedicated to the problem of how to solve for the optimal robust policy of a given RMDP, i.e., planning (Iyengar, 2005; El Ghaoui and Nilim, 2005; Xu and Mannor, 2010; Wang and Zou, 2022; Wang et al., 2022; Kuang et al., 2022; Wang et al., 2023; Yu et al., 2023; Zhou et al., 2023; Li and Lan, 2023; Wang et al., 2023c; Ding et al., 2024). Recently, the community has also witnessed a growing body of work on sample-efficient robust RL in RMDPs with different data collection oracles, including the generative model setup (Yang et al., 2022; Panaganti and Kalathil, 2022; Si et al., 2023; Wang et al., 2023b; Yang et al., 2023; Xu et al., 2023; Clavier et al., 2023; Wang et al., 2023d; Shi et al., 2023), offline setting (Zhou et al., 2021b; Panaganti et al., 2022; Shi and Chi, 2022; Ma et al., 2022; Blanchet et al., 2023; Liu and Xu, 2024b; Wang et al., 2024), and interactive data collection setting (Badrinath and Kalathil, 2021; Wang and Zou, 2021; Liu and Xu, 2024a).

Our work falls into the paradigm of sample-efficient robust RL with interactive data collection. Wang and Zou (2021) and Badrinath and Kalathil (2021) propose efficient online learning algorithms to obtain the optimal robust policy of an infinite horizon RMDP, but none of them handle the challenge of exploration in online RL by assuming the access to _explorative policies_. This assumption enables the learner to collect high-quality data essential for effective learning and decision-making. In contrast, our work focuses on developing efficient algorithms for the fully online setting, where there is no predefined exploration policy to use. Under this more challenging setting, we address the exploration challenge through algorithmic design rather than relying on assumed access to explorative policies.

During the preparation of this work, we are aware of several concurrent and independent works (Liu and Xu, 2024a,b; Wang et al., 2024), which study a different type of RMDPs known as \(d\)-rectangular linear MDPs (Ma et al., 2022; Blanchet et al., 2023). In particular, Liu and Xu (2024b) and Wang et al. (2024) consider the offline setting, while Liu and Xu (2024a) investigate robust RL through interactive data collection (off-dynamics learning), thus bearing closer relevance to our work. More specifically, under the existence of a "fail-state", the algorithm in Liu and Xu (2024a) can learn an \(\varepsilon\)-optimal robust policy with provable sample efficiency. In contrast, our work first explicitly uncovers the fundamental hardness of doing robust RL in RMDPs with a TV distance based robust set and without additional assumptions. To overcome the inherent difficulty, we adopt a vanishing minimal value assumption that strictly generalizes the "fail-state" assumption used in Liu and Xu (2024a). Moreover, our focus is on tabular \(\mathcal{S}\times\mathcal{A}\)-rectangular RMDPs, with customized algorithmic design and theoretical analysis which allow us to obtain a sharp sample complexity bound.

Finally, in Table 1, we compare the sample complexity of our algorithm with prior work on robust RL for RMDPs with \(\mathcal{S}\times\mathcal{A}\)-rectangular TV robust sets under various settings (generative model/offline dataset). We remark that the works of Panaganti and Kalathil (2022) and Blanchet et al. (2023) are in the paradigm of function approximation, and here we reduce their general sample complexity result to the tabular setup we consider.

Sample-efficient online non-robust reinforcement learning.Our work is also closely related to online non-robust RL, which is often formulated as a Markov decision process (MDP) with online data collection. For non-robust online RL, the key challenge is the exploration-exploitation tradeoff. There has been a long line of work (Azar et al., 2017; Dann et al., 2017; Jin et al., 2018; Zanette and Brunskill, 2019; Zhang et al., 2020, 2021; Menard et al., 2021; Wu et al., 2022; Li et al., 2023; Zhang et al., 2023) addressing this challenge in the context of tabular MDPs, where the state space and action space are finite and also relatively small. In particular, many algorithms (e.g., UCBVI in Azar et al. (2017)) have been proven capable of finding an \(\varepsilon\)-optimal policy within \(\widetilde{\mathcal{O}}(H^{3}SA/\varepsilon^{2})\) sample complexity. Notably, a standard MDP corresponds to an RMDP with a TV robust set and \(\rho=0\), suggesting that OPROVI-TV can naturally achieve nearly minimax-optimality for non-robust RL. Moving beyond the tabular setups, recent works also investigate online non-robust RL with linear function approximation (Jin et al., 2020; Ayoub et al., 2020; Zhou et al., 2021a; Zhong and Zhang, 2023; Huang et al., 2023b; He et al., 2023; Agarwal et al., 2023) and even general function approximations (Jiang et al., 2017; Sun et al., 2019; Du et al., 2021; Jin et al., 2021; Foster et al.,2021; Liu et al., 2022; Zhong et al., 2022; Liu et al., 2023; Huang et al., 2023; Xu and Zeevi, 2023; Agarwal et al., 2023).

Corruption robust reinforcement learning.Generally speaking, our research is also related to another form of robust RL, namely corruption robust RL (Lykouris et al., 2021; Wei et al., 2022; Zhang et al., 2022; Ye et al., 2023, 2023; Yang et al., 2023; Ye et al., 2024). This branch of researches on robust RL addresses scenarios where training data is corrupted, presenting a distinct challenge from distributionally robust RL. The latter concerns testing time robustness, where the agent is evaluated in a perturbed environment after being trained on nominal data. These two forms of robust RL, while sharing the overarching goal to enhance agent resilience, operate within different contexts and confront distinct challenges. Thus, a direct comparison between these two types of robust RL is difficult because each addresses unique aspects of resilience.

## Appendix B Further Discussions

This section complements the main part of the paper by further commenting and discussing several aspects of the paper. Due to space limits, these important remarks are provided here.

### Discussions on Introduction (Section 1)

About a generative model and a simulator.A generative model here means a mechanism that when queried at some state, action, and time step, returns a sample of next state. Here we distinguish this notion with the notion of simulator or simulated environment which generally refers to a human-made training environment that mimics the real-world environment. With a generative model on hand, interactive data collection is no longer needed, but in a simulated environment, it is common to train a robust policy through interactive data collection in practice.

The definition of total-variation robust set.We notice that all of the previous work on sample-efficient robust RL in RMDPs with TV robust sets (Yang et al., 2022; Panaganti and Kalathil, 2022; Panaganti et al., 2022; Xu et al., 2023; Blanchet et al., 2023; Shi et al., 2023) relies on defining the TV distance through the general \(f\)-divergence so that a strong duality representation holds. But this implicitly requires the testing environment transition probability is absolute continuous w.r.t. the training environment transition probability. In this paper, we do not make such a restriction. We prove the same strong duality even if the absolute continuity does not hold. In fact, all the previous work can be directly extended to such TV distance definition via our more general strong duality result.

An existing work.We note that an existing work (Dong et al., 2022) also studies the problem of robust RL with interactive data collection. They study \(\mathcal{S}\times\mathcal{A}\)-rectangular RMDPs with a TV robust set, assuming that the support of the training environment transition is the full state space \(\mathcal{S}\). They claim the existence of an algorithm that enjoys a \(\widetilde{\mathcal{O}}(\sqrt{K})\)-online regret. We point out that their proof exhibits an essential flaw (misuse of Lemma 12 therein) and therefore the regret they claim is invalid.

The range of the robust set size \(\rho\).We do not signify the situation when \(\rho=1\) since in that case the TV robust set contains all possible transition probabilities, making the problem statistically trivial. In that case, no sample is needed.

### Discussions on Preliminaries (Section 2)

#### b.2.1 Robust Markov Decision Processes

Robust Bellman equations.We remark that the original version of the robust Bellman equation (Iyengar, 2005) is for infinite horizon RMDPs and a customized proof of robust Bellman equation for finite horizon RMDPs (Proposition 2.2) can be found in Appendix A.1 of Blanchet et al. (2023). The robust Bellman optimal equation (Proposition 2.3) is then a corollary or can be proved similarly.

Strong duality under TV distance robust set.An essential property of the TV robust set is that the robust expectation involved in the robust Bellman equations (Propositions 2.2 and 2.3) has a duality representation that only uses the expectation under the nominal transition kernel. Previousworks, e.g., Yang et al. (2022), have proved such a result when the TV distance is defined through \(f\)-divergence. Here we extend such a result to the TV distance defined directly though (2.2) that allows a difference support between \(p\) and \(q\).

**Remark B.1**.: _Despite all previous works on RMDPs with TV robust sets relying on the definition of TV distance \(D_{\mathrm{TV}}(p(\cdot)\|q(\cdot))\) with absolute continuity of \(p\) with respect to \(q\) to obtain the strong duality representation in the form of (2.3), their results can be directly extended to TV distance that allows for different support between \(p\) and \(q\) thanks to Proposition 2.5._

Value gap between maximum.We note that in the proof of Proposition 2.6, we actually show a tighter form of bound of the gap between the maximum and minimum as

\[\frac{1}{\rho}\cdot\Big{(}1-(1-\rho)^{H}\Big{)}.\]

But in the sequel, we mainly use the form of \(\min\{H,\rho^{-1}\}\) for its brevity and the fact of \((1-(1-\rho)^{H})/\rho=\Theta(\min\{H,\rho^{-1}\})\) in the sense that

\[c\cdot\min\big{\{}H,\rho^{-1}\big{\}}\leq(1-(1-\rho)^{H})/\rho\leq\min\big{\{} H,\rho^{-1}\big{\}}\]

for any \(H\geq H_{0}\in\mathbb{N}_{+}\) and \(\rho\in[0,1]\) with some constant \(c>0\) that is independent of \((H,\rho)\).

In contrast with a crude bound of \(H\), such a fine upper bound decreases when \(\rho\) is large, which is essential to understanding the statistical limits of doing robust RL in RMDPs with TV robust sets.

#### b.2.2 Robust RL with Interactive Data Collection

Sample complexity.The metric of _sample complexity_ is connected with the sample complexity used in robust RL with generative models and offline settings (see related works for the references), wherein the sample complexity means the minimum number of generative samples or pre-collected offline data required to achieve \(\varepsilon\)-optimality. In contrast, here the sample complexity is measuring the least number of interactions with the training environment needed to learn \(\pi^{\star}\), where no generative or offline sample is available. Such a learning protocol casts unique challenges on the algorithmic design and theoretical analysis to get the optimal sample complexity.

### Discussions on Hardness Result: The Curse of Support Shift (Section 3)

In contrast to the interactive data collection setting we consider, doing robust RL with a generative model or an offline dataset with good coverage properties does not face the difficulty we displayed through Example 3.1. It turns out that any RMDP with \(\mathcal{S}\times\mathcal{A}\)-rectangular total-variation robust set (including Example 3.1) can be solved in a sample-efficient manner therein, see Yang et al. (2022); Panaganti and Kalathil (2022); Panaganti et al. (2022); Xu et al. (2023); Blanchet et al. (2023); Shi et al. (2023) and Remark B.1. The intuitive reason is that, for the generative model setting, the learner can directly query any state-action pair to estimate the nominal transition kernel \(P^{\star}\), and thus no support shift happens. The same reason holds for the offline setup with a good-coverage dataset.

There is a broader understanding of the curse of support shift that hinders the tractability of robust RL via interactive data collection. The concept of support shift could be comprehended within a broader context beyond the disjointness of certain parts of the support sets of the training and testing environments. Instead, ensuring a "high probability of disjointness" is enough to maintain the integrity of the hardness result. For instance, we can modify the state \(s_{\mathrm{good}}\) in Example 3.1 so that it is no longer an absorbing state. Rather, \(s_{\mathrm{good}}\) could transit to \(s_{\mathrm{bad}}\) with a small probability, such as \(2^{-H}\). This modification expands the support of the training environment to encompass the entire state space. Nevertheless, acquiring information about \(s_{\mathrm{bad}}\) necessitates exponential samples, thereby preserving the hardness result.

### Discussions on A Solvable Case, Efficient Algorithm, and Sharp Analysis (Section 4)

#### b.4.1 Vanishing Minimal Value Assumption

**Another understanding of Assumption 4.1.** To understand this from another perspective, it could be shown that under the conclusions of Proposition 4.2, the robust value function of any policy \(\pi\) is equivalent to the robust value function of this policy under another _discounted_ RMDP \((\mathcal{S},\mathcal{A},H,P^{\star},R^{\prime},\mathbf{\Phi}^{\prime})\) with \(R^{\prime}_{h}(s,a)=(\rho^{\prime})^{h-1}R_{h}(s,a)\) and \(\mathbf{\Phi}^{\prime}\) given by

\[\mathbf{\Phi}^{\prime}(P)=\bigotimes_{(s,a)\in\mathcal{S}\times\mathcal{A}} \mathcal{B}_{\rho^{\prime}}(s,a;P).\] (B.1)

And therefore we are equivalently considering this new type of RMDPs. Please refer to Section B.4.2 for more discussions on the connections between the two types of RMDPs.

Examples of Assumption 4.1.In the sequel, we provide a concrete condition that makes Assumption 4.1 hold, which imposes that the state space of the RMDP has a "closed" subset of "fail-states" with zero rewards.

**Condition B.2** (Fail-states).: _There exists a subset \(\mathcal{S}_{f}\subset\mathcal{S}\) of fail states such that_

\[R_{h}(s,a)=0,\quad P_{h}^{\star}(\mathcal{S}_{f}|s,a)=1,\quad\forall(s,a,h) \in\mathcal{S}_{f}\times\mathcal{A}\times[H].\]

This type of "fail-states" condition is first proposed by Panaganti et al. (2022) (with \(|\mathcal{S}_{f}|=1\)) to handle the computational issues for robust offline RL under function approximations (out of the scope of our work). In contrast, here we make the vanishing minimal value assumption in order to tackle the _support shift_ or _extrapolation_ issue for the interactive data collection setup. The comparison between the vanishing minimal value assumption (Assumption 4.1) and the "fail-states" condition (Condition B.2) is given below.

**Remark B.3** (Comparison between Assumption 4.1 and Condition B.2).: _We first observe that Condition B.2 implies that \(\min_{s\in\mathcal{S}}V_{h,P^{\star},\mathbf{\Phi}}^{\prime}(s)=0\) for any policy \(\pi\) and step \(h\in[H]\), therefore satisfying the minimal value assumption (Assumption 4.1). Conversely, the vanishing minimal value assumption in Assumption 4.1 is strictly more general than the fail-state condition in Condition B.2. To illustrate, one can consider an RMDP characterized by the state space \(\mathcal{S}=\{s_{1},s_{2}\}\), action space \(\mathcal{A}=\{a_{1}\}\), time horizon \(H=2\), reward function \(R_{h}(s,a)=\mathbf{1}\{s=s_{2}\}\), and transition probabilities defined as follows:_

\[P_{1}^{\star}(s_{1}|s_{1},a_{1})=1-\rho,\quad P_{1}^{\star}(s_{2}|s_{1},a_{1}) =\rho,\quad P_{1}^{\star}(s_{1}|s_{2},a_{1})=0,\quad P_{1}^{\star}(s_{2}|s_{2},a_{1})=1,\]

_where \(\rho\) is the radius of the robust set. It is evident that no fail-state emerges within such an RMDP structure. However, this RMDP satisfies the vanishing minimal value assumption since \(V_{1,P^{\star},\mathbf{\Phi}}^{\star}(s_{1})=0\)._

**Remark B.4** (Reduction to non-robust MDP without loss of generality).: _It is noteworthy that assuming the vanishing minimal value (Assumption 4.1) or the presence of fail-states (Condition B.2) in the non-robust case (\(\rho=0\)) is without loss of generality. This is achievable by expanding the prior state space \(\mathcal{S}\) of MDP to include an additional state \(s_{f}\), denoted as the fail-state. More importantly, this augmentation does not alter the optimal value or the optimal value function of the original MDP. Consequently, it becomes sufficient to seek the optimal policy within the augmented MDP, which satisfies the conditions of vanishing minimal value (Assumption 4.1) or the existence of fail-states (Condition B.2). This indicates that our algorithm and theoretical analysis in the sequel can be directly reduced to non-robust MDPs without additional assumptions._

#### b.4.2 Extensions to Robust Set with Bounded Transition Probability Ratio

In this section, we show that our algorithm design (Algorithm 1) can also be applied to \(\mathcal{S}\times\mathcal{A}\)-rectangular discounted RMDPs with robust sets given by (B.1) (i.e., bounded ratio between training and testing transition probabilities). We establish that our main theoretical result in Section 4.3 can imply a sublinear regret upper bound for this model, which means that this type of RMDPs can also be solved sample-efficiently by a clever usage of Algorithm 1. This coincides with our intuition on support shift in Section 4.1.

\(\mathcal{S}\times\mathcal{A}\)-rectangular discounted RMDPs with robust set (B.1).We first formally define the model we consider. We define a finite-horizon discounted RMDP as a finite-horizon RMDP \(\mathcal{M}_{\gamma}=(\mathcal{S},\mathcal{A},H,P^{\star},R_{\gamma},\mathbf{ \Phi}^{\prime})\), where the robust set \(\mathbf{\Phi}^{\prime}\) is given by (B.1), i.e.,

\[\mathbf{\Phi}^{\prime}(P)=\bigotimes_{(s,a)\in\mathcal{S}\times\mathcal{A}} \left\{\widetilde{P}(\cdot)\in\Delta(\mathcal{S}):\sup_{s^{\prime}\in\mathcal{ S}}\frac{\widetilde{P}(s^{\prime})}{P_{h}^{\star}(s^{\prime}|s,a)}\leq\frac{1}{ \rho^{\prime}}\right\}:=\bigotimes_{(s,a)\in\mathcal{S}\times\mathcal{A}} \mathcal{B}_{\rho^{\prime}}(s,a;P^{\star}).\] (B.2)This robust set contains transition probabilities that share the same support as the nominal transition kernel. The reward function \(R_{\gamma}=\{\gamma^{h-1}\cdot R_{h}\}_{h=1}^{H}\), where \(\gamma\in(0,1)\) is the discount factor and \(R_{h}\in[0,1]\) is the true reward at step \(h\). That is, the robust value function is now the worst case expected discounted total reward.

Algorithm and regret bound.Now we show that we can apply Algorithm 1 to solve robust RL in \(\mathcal{S}\times\mathcal{A}\)-rectangular discounted RMDPs with robust set (B.2) via interactive data collection.

As motivated by the discussions under Proposition 4.2, we define an auxiliary finite-horizon TV-RMDP \(\widetilde{\mathcal{M}}\) as \(\widetilde{\mathcal{M}}=(\widetilde{\mathcal{S}},\mathcal{A},H,\widetilde{P^{ \star}},\widetilde{R},\widetilde{\boldsymbol{\Phi}})\) which include an additional "fail-state" \(s_{f}\). More specifically, the state space \(\widetilde{\mathcal{S}}=\mathcal{S}\cup\{s_{f}\}\). The transition kernel \(\widetilde{P}^{\star}\) is defined as, for any step \(h\in[H]\),

\[\widetilde{P}^{\star}_{h}(\cdot|s,a)=P^{\star}_{h}(\cdot|s,a),\quad\forall(s, a)\in\mathcal{S}\times\mathcal{A}\quad\text{and}\quad\widetilde{P}^{\star}_{h}( \cdot|s_{f},a)=\delta_{s_{f}}(\cdot),\quad\forall a\in\mathcal{A}.\] (B.3)

The reward function \(\widetilde{R}\) is defined as, for any step \(h\in[H]\),

\[\widetilde{R}_{h}(s,a)=\left(\frac{\gamma}{\rho^{\prime}}\right)^{h-1}\cdot R _{h}(s,a),\quad\forall(s,a)\in\mathcal{S}\times\mathcal{A}\quad\text{and} \quad\widetilde{R}_{h}(s_{f},a)=0,\quad\forall a\in\mathcal{A}.\]

We suppose that the discount factor \(\gamma\leq\rho^{\prime}\) so that the reward function \(\widetilde{R}_{h}\in[0,1]\). The robust mapping \(\widetilde{\boldsymbol{\Phi}}\) is defined as, for any \(\widetilde{P}:\widetilde{\mathcal{S}}\times\mathcal{A}\mapsto\Delta(\widetilde {\mathcal{S}})\),

\[\widetilde{\boldsymbol{\Phi}}(\widetilde{P}) =\bigotimes_{(s,a)\in\widetilde{\mathcal{S}}\times\mathcal{A}} \Big{\{}\widetilde{P}(\cdot)\in\Delta(\widetilde{\mathcal{S}}):D_{\mathrm{TV}} \big{(}P(\cdot)\big{\|}\widetilde{P}(\cdot|s,a)\big{)}\leq\rho\Big{\}}\] \[:=\bigotimes_{(s,a)\in\widetilde{\mathcal{S}}\times\mathcal{A}} \widetilde{\mathcal{P}}_{\rho}(s,a;\widetilde{P}),\quad\rho=2-2\rho^{\prime}.\]

Therefore, \(\widetilde{\mathcal{M}}\) is an RMDP with \(\mathcal{S}\times\mathcal{A}\)-rectangular TV robust set of radius \(\rho\) and satisfying Assumption 4.1 (because it satisfies the "fail-state" Condition B.2). Furthermore, for any initial state \(s_{1}\in\widetilde{\mathcal{S}}\setminus\{s_{f}\}=\mathcal{S}\), the interaction with the transition kernel \(\widetilde{P}^{\star}\) is equivalent to the interaction with the transition kernel \(P^{\star}\) of the original RMDP \(\mathcal{M}_{\gamma}\), since by the definition (B.3), starting from any \(s\neq s_{f}\) the agent would follow the same dynamics as \(P^{\star}\). What's more, for any policy \(\widetilde{\pi}_{h}:\widetilde{\mathcal{S}}\mapsto\Delta(\mathcal{A})\) for \(\widetilde{\mathcal{M}}\), it naturally induces the unique policy \(\widetilde{\pi}_{\mathcal{S},h}:\mathcal{S}\mapsto\Delta(\mathcal{A})\) for the original RMDP \(\mathcal{M}_{\gamma}\).

Therefore, we can run Algorithm 1 on the auxiliary RMDP \(\widetilde{\mathcal{M}}\), starting from the initial state \(s_{1}\in\widetilde{\mathcal{S}}\setminus\{s_{f}\}\), which only needs the interaction with \(P^{\star}\). Suppose the output policy by the algorithm is \(\{\widetilde{\pi}^{k}\}_{k=1}^{K}\), then the following corollary shows the induced policy \(\{\widetilde{\pi}^{k}_{\mathcal{S}}\}_{k=1}^{K}\) for the original RMDP \(\mathcal{M}_{\gamma}\) enjoys a sublinear regret.

**Corollary B.5** (Online regret of Algorithm 1 for discounted RMDPs with robust sets (B.2)).: _Consider an \(\mathcal{S}\times\mathcal{A}\)-rectangular \(\gamma\)-discounted RMDP with robust set (B.2) satisfying \(0\leq\gamma\leq\rho^{\prime}\in(1/2,1]\). There exists an algorithm \(\mathcal{ALG}\) (specified by the above discussion) such that its online regret for this RMDP is bounded by_

\[\mathrm{Regret}^{\mathcal{ALG}}_{\boldsymbol{\Phi}^{\prime}}(K)\leq\mathcal{O} \bigg{(}\sqrt{\min\big{\{}H,(2-2\rho^{\prime})^{-1}\big{\}}H^{2}SAK\iota^{ \prime}}\,\bigg{)},\]

_where \(\iota^{\prime}=\log^{2}(SAHK/\delta)\) and \(\mathcal{O}(\cdot)\) hides absolute constants and lower order terms in \(K\)._

Proof of Corollary b.5.: See Appendix F.1 for a detailed proof of Corollary B.5. 

Corollary B.5 shows that besides \(\mathcal{S}\times\mathcal{A}\)-rectangular RMDPs with TV robust set and vanishing minimal value assumption, the \(\mathcal{S}\times\mathcal{A}\)-rectangular discounted RMDP with robust set of bounded transition probability ratio (B.2) can also be solved sample-efficiently by robust RL via interactive data collection. This also echoes our intuition on the support shift issue in Section 4.1. Furthermore, the regret decays as \(\rho^{\prime}\) decays in which case the transition probability ratio bound becomes higher, i.e., the robust set becomes larger.

**Remark B.6**.: _The upper bound in Corollary B.5 does not depend on the discount factor \(\gamma\) since Algorithm 1 adopts a coarse bound of \(\widetilde{R}_{h}\leq 1\). The upper bound can be directly improved to be \(\gamma\)-dependent using a tighter truncation in step (4.2) of Algorithm 1._

### Discussions of Limitations and Future Works

In this work, we show that in the absence of any structural assumptions, robust RL through interactive data collection necessarily induces a linear regret lower bound in the worst case due to the curse of support shift. Meanwhile, under the vanishing minimal value assumption, an assumption that is able to effectively rule out the potential support shift issues for RMDPs with a TV robust set, we propose a sample-efficient robust RL algorithm for those RMDPs. We discuss some potential extensions here and the associated challenges next.

Extension to function approximation setting.The vanishing minimal value assumption also suffices for developing sample-efficient algorithms for \(\mathcal{S}\times\mathcal{A}\)-rectangular TV-robust-set RMDPs with linear or even general function approximation Blanchet et al. (2023). Nonetheless, achieving the nearly optimal rate under general function approximation remains elusive.

Extension to other types of robust set.Beyond the TV distance based robust set we consider, recent literature on robust RL also investigate other types of \(\phi\)-divergence based robust set including KL divergence, \(\chi^{2}\) distance Yang et al. (2022); Shi and Chi (2022); Blanchet et al. (2023); Xu et al. (2023); Shi et al. (2023). An interesting direction of future work is to investigate is it also possible and, if possible, can we design provably sample-efficient robust RL algorithms with interactive data collection for RMDPs with those types of robust sets. Notably, the KL divergence based robust set naturally does not suffer from the curse of support shifts that gives rise to the hardness for the TV robust set case. However, we find that there are other difficulties for robust RL in KL divergence based RMDPs through interactive data collection. Meanwhile, the optimal sample complexity for robust RL in RMDPs with KL divergence robust set is still elusive even in the offline learning setup Shi and Chi (2022). We leave the study of RMDPs with KL divergence robust set for future work.

## Appendix C Proofs for Properties of RMDPs with TV Robust Sets

### Proof of Proposition 2.5

To simplify the notations, we present the following lemma, which directly implies Proposition 2.5.

**Lemma C.1** (Strong duality for TV robust set).: _The following duality for total variation robust set holds, for \(f:\mathcal{S}\mapsto[0,H]\),_

\[\inf_{Q(\cdot):D_{\mathrm{TV}}(Q(\cdot)\|Q^{\star}(\cdot))\leq\sigma}\mathbb{ E}_{Q(\cdot)}[f]=\sup_{\eta\in[0,H]}\left\{-\mathbb{E}_{Q^{\star}(\cdot)} \big{[}(\eta-f)_{+}\big{]}-\frac{\sigma}{2}\cdot\left(\eta-\min_{s\in\mathcal{ S}}f(s)\right)_{+}+\eta\right\},\]

_where \(\sigma\in[0,1]\) and the TV distance \(D_{\mathrm{TV}}(Q(\cdot)\|Q^{\star}(\cdot))\) is defined as_

\[D_{\mathrm{TV}}(Q(\cdot)\|Q^{\star}(\cdot))=\frac{1}{2}\sum_{s\in\mathcal{S}}| Q(s)-Q^{\star}(s)|.\]

Proof of Lemma c.1.: First, we note that when \(Q^{\star}(s)>0\) for any \(s\in\mathcal{S}\), i.e., any \(Q(\cdot)\in\Delta(\mathcal{S})\) is absolute continuous w.r.t. \(Q^{\star}(\cdot)\), it has been proved by Yang et al. (2022) that

\[\inf_{Q(\cdot):D_{\mathrm{TV}}(Q(\cdot)\|Q^{\star}(\cdot))\leq\sigma}\mathbb{ E}_{Q(\cdot)}[f]=\sup_{\eta\in\mathbb{R}}\left\{-\mathbb{E}_{Q^{\star}(\cdot)} \big{[}(\eta-f)_{+}\big{]}-\frac{\sigma}{2}\cdot\left(\eta-\min_{s\in\mathcal{ S}}f(s)\right)_{+}+\eta\right\}.\]

Furthermore, as is shown in Lemma H.8 in Blanchet et al. (2023), the optimal dual variable \(\eta^{\star}\) lies in \([0,H]\) when \(f\in[0,H]\). Therefore, for \(Q^{\star}(\cdot)\) such that \(Q^{\star}(s)>0\) for any \(s\in\mathcal{S}\), we have

\[\inf_{Q(\cdot):D_{\mathrm{TV}}(Q(\cdot)\|Q^{\star}(\cdot))\leq\sigma}\mathbb{ E}_{Q(\cdot)}[f]=\sup_{\eta\in[0,H]}\left\{-\mathbb{E}_{Q^{\star}(\cdot)} \big{[}(\eta-f)_{+}\big{]}-\frac{\sigma}{2}\cdot\left(\eta-\min_{s\in\mathcal{ S}}f(s)\right)_{+}+\eta\right\}.\]

Now for any \(Q^{\star}(\cdot)\in\Delta(\mathcal{S})\), we can prove the same result by averaging \(Q^{\star}(\cdot)\) with a uniform distribution and taking the limit. More specifically, denote \(U(\cdot)\in\Delta(\mathcal{S})\) as the uniform distributionon \(\mathcal{S}\), i.e., \(U(s)=1/|\mathcal{S}|\) for any \(s\in\mathcal{S}\). Consider the following distributionally robust optimization problem, for any \(\epsilon\in[0,1]\),

\[\mathsf{P}(\epsilon):=\inf_{Q(\cdot):D_{\mathrm{TV}}\left(Q(\cdot)\|(1- \epsilon)Q^{\star}(\cdot)+\epsilon\cdot U(\cdot)\right)\leq\sigma}\mathbb{E}_{ Q(\cdot)}[f].\]

By our previous discussions, since \((1-\epsilon)Q^{\star}(s)+\epsilon\cdot U(s)>0\) for any \(s\in\mathcal{S}\) and \(\epsilon>0\), we have that

\[\mathsf{P}(\epsilon)=\mathsf{D}(\epsilon),\quad\forall\epsilon\in(0,1],\] (C.1)

where the function \(\mathsf{D}(\cdot):[0,1]\mapsto\mathbb{R}_{+}\) is defined as

\[\mathsf{D}(\epsilon):=\sup_{\eta\in[0,H]}\left\{-(1-\epsilon)\cdot\mathbb{E}_ {Q^{\star}(\cdot)}\big{[}(\eta-f)_{+}\big{]}-\epsilon\cdot\mathbb{E}_{U(\cdot )}\big{[}(\eta-f)_{+}\big{]}-\frac{\sigma}{2}\cdot\left(\eta-\min_{s\in \mathcal{S}}f(s)\right)_{+}+\eta\right\}.\]

By the definition of \(\mathsf{P}(\cdot)\) and \(\mathsf{D}(\cdot)\), our goal is to prove that \(\mathsf{P}(0)=\mathsf{D}(0)\). To this end, it suffices to prove that (i) \(\lim_{\epsilon\to 0+}\mathsf{D}(\epsilon)\) exists and \(\lim_{\epsilon\to 0+}\mathsf{D}(0)\); and (ii) \(\lim_{\epsilon\to 0+}\mathsf{P}(\epsilon)=\mathsf{P}(0)\). To prove (i), consider that for any \(\epsilon>0\), by the definition of \(\mathsf{D}(\cdot)\),

\[|\mathsf{D}(0)-\mathsf{D}(\epsilon)|\leq\sup_{\eta\in[0,H]}\left\{\epsilon \cdot\mathbb{E}_{Q^{\star}(\cdot)}\big{[}(\eta-f)_{+}\big{]}+\epsilon\cdot \mathbb{E}_{U(\cdot)}\big{[}(\eta-f)_{+}\big{]}\right\}\leq\epsilon\cdot 2H.\]

Since the right hand side tends to \(0\) as \(\epsilon\) tends to \(0\), we know that \(\lim_{\epsilon\to 0+}\mathsf{D}(\epsilon)\) exists, \(\lim_{\epsilon\to 0+}\mathsf{D}(\epsilon)=\mathsf{D}(0)\). This also indicates that \(\lim_{\epsilon\to 0+}\mathsf{P}(\epsilon)\) exists due to (C.1). This proves (i). Now we prove (ii). Notice that since the set

\[\big{\{}Q(\cdot)\in\Delta(\mathcal{S}):D_{\mathrm{TV}}\big{(}Q(\cdot)\|(1- \epsilon)Q^{\star}(\cdot)+\epsilon\cdot U(\cdot)\big{)}\leq\sigma\big{\}}\]

is a closed subset of \(\mathbb{R}^{|\mathcal{S}|}\), and \(\mathbb{E}_{Q(\cdot)}[f]\) is a continuous function of \(Q(\cdot)\in\mathbb{R}^{|\mathcal{S}|}\) w.r.t. the \(\|\cdot\|_{2}\)-norm, we can denote the optimal solution to the optimization problem involved in \(\mathsf{P}(\epsilon)\) as

\[Q_{\epsilon}^{\dagger}(\cdot)=\operatorname*{arginf}_{Q(\cdot):D_{\mathrm{TV} }\big{(}Q(\cdot)\|(1-\epsilon)Q^{\star}(\cdot)+\epsilon\cdot U(\cdot)\big{)} \leq\sigma}\mathbb{E}_{Q(\cdot)}[f],\]

which also gives that

\[\mathsf{P}(\epsilon)=\mathbb{E}_{Q_{\epsilon}^{\dagger}(\cdot)}[f]=\sum_{s\in \mathcal{S}}Q_{\epsilon}^{\dagger}(s)f(s).\]

With these preparations, we are able to prove (ii). On the one hand, consider for any \(\epsilon\in(0,1]\),

\[D_{\mathrm{TV}}\big{(}(1-\epsilon)\cdot Q_{0}^{\dagger}(\cdot)+\epsilon\cdot U (\cdot)\big{\|}(1-\epsilon)\cdot Q^{\star}(\cdot)+\epsilon\cdot U(\cdot) \big{)}\leq(1-\epsilon)\cdot\sigma\leq\sigma.\]

Therefore, for any \(\epsilon\in(0,1]\), it holds that

\[\mathsf{P}(\epsilon) =\inf_{Q(\cdot):D_{\mathrm{TV}}\big{(}Q(\cdot)\|(1-\epsilon)Q^{ \star}(\cdot)+\epsilon\cdot U(\cdot)\big{)}\leq\sigma}\mathbb{E}_{Q(\cdot)}[f]\] \[\leq\mathbb{E}_{(1-\epsilon)\cdot Q_{0}^{\dagger}(\cdot)+\epsilon \cdot U(\cdot)}[f]=(1-\epsilon)\cdot\mathbb{E}_{Q_{0}^{\dagger}}[f]+\epsilon \cdot\mathbb{E}_{U(\cdot)}[f],\]

which implies that

\[\lim_{\epsilon\to 0+}\mathsf{P}(\epsilon)\leq\mathbb{E}_{Q_{0}^{\dagger}}[f]= \mathsf{P}(0).\] (C.2)

On the other hand, for any \(\epsilon\in(0,1]\),

\[\sigma \geq\frac{1}{2}\sum_{s\in\mathcal{S}}\Big{|}Q_{\epsilon}^{\dagger} (s)-(1-\epsilon)\cdot Q^{\star}(s)-\epsilon\cdot U(s)\Big{|}\] \[\geq(1-\epsilon)\cdot D_{\mathrm{TV}}(Q_{\epsilon}^{\dagger}( \cdot)\|Q^{\star}(\cdot))-\epsilon\cdot D_{\mathrm{TV}}(Q_{\epsilon}^{\dagger}( \cdot)\|U(\cdot)),\]

and by using \(D_{\mathrm{TV}}(Q_{\epsilon}^{\dagger}(\cdot)\|U(\cdot))\leq 1\), we obtain that

\[D_{\mathrm{TV}}(Q_{\epsilon}^{\dagger}(\cdot)\|Q^{\star}(\cdot))\leq\frac{ \sigma+\epsilon}{1-\epsilon}.\] (C.3)Consider a sequence of \(\{\epsilon_{i}\}_{i=1}^{\infty}\) converging to \(0\), i.e., \(\lim_{i\to 0+}\epsilon_{i}=0\). Since \(\{Q_{\epsilon_{i}}^{\dagger}(\cdot)\}_{i=1}^{\infty}\) is a sequence contained in a compact subset of \(\mathbb{R}^{|\mathcal{S}|}\), it has a converging (w.r.t. \(\|\cdot\|_{2}\)) subsequence denoted by \(\{Q_{\epsilon_{i_{k}}}^{\dagger}(\cdot)\}_{k=1}^{\infty}\) whose limit is denoted as \(Q^{\dagger}(\cdot)\in\Delta(\mathcal{S})\). By (C.3), we know that

\[D_{\mathrm{TV}}(Q_{\epsilon_{i_{k}}}^{\dagger}(\cdot)\|Q^{\star}(\cdot))\leq \frac{\sigma+\epsilon_{i_{k}}}{1-\epsilon_{i_{k}}}.\] (C.4)

Taking limit on both sides of (C.4) (limit of LHS exists since the TV distance is a continuous function (w.r.t. \(\|\cdot\|_{2}\)) of its first entry and the limit of RHS obviously exists), we obtain that

\[D_{\mathrm{TV}}(Q^{\dagger}(\cdot)\|Q^{\star}(\cdot))\leq\sigma.\] (C.5)

Now we can arrive at the following,

\[\lim_{\epsilon\to 0+}\mathsf{P}(\epsilon) =\lim_{\epsilon\to 0+}\mathbb{E}_{Q_{\epsilon}^{\dagger}(\cdot)}[f]= \lim_{k\to 0+}\mathbb{E}_{Q_{i_{k}}^{\dagger}(\cdot)}[f]=\mathbb{E}_{Q_{\epsilon }^{\dagger}(\cdot)}[f]\] \[\geq\inf_{Q(\cdot):D_{\mathrm{TV}}(Q(\cdot)\|Q^{\star}(\cdot))\leq \sigma}\mathbb{E}_{Q(\cdot)}[f]=\mathsf{P}(0),\] (C.6)

where the first and the last equality follows from the definition of \(\mathsf{P}(\cdot)\), the second equality follows from the choice of the sequence \(\{\epsilon_{i_{k}}\}_{k=1}^{\infty}\) that converges to \(0\), the third equality is due to the continuity of \(\mathbb{E}_{Q(\cdot)}[f]\) of \(Q(\cdot)\) (w.r.t. \(\|\cdot\|_{2}\)), and the inequality follows from (C.5). Finally, with (C.2) and (C.6), we conclude that

\[\lim_{\epsilon\to 0+}\mathsf{P}(\epsilon)=\mathsf{P}(0),\]

which proves (ii). Consequently, by (i) and (ii)

\[\mathsf{P}(0)=\lim_{\epsilon\to 0+}\mathsf{P}(\epsilon)=\lim_{\epsilon\to 0+} \mathsf{D}(\epsilon)=\mathsf{D}(0).\]

Recalling the definitions of \(\mathsf{P}(\cdot)\) and \(\mathsf{D}(\cdot)\), we conclude the proof of Lemma C.1. 

### Proof of Proposition 2.6

Proof of Proposition 2.6.: Here we prove a stronger result that for any policy \(\pi\) and step \(h\in[H]\)

\[\max_{(s,a)\in\mathcal{S}\times\mathcal{A}}Q_{h,P,\Phi}^{\pi}(s,a )-\min_{(s,a)\in\mathcal{S}\times\mathcal{A}}Q_{h,P,\Phi}^{\pi}(s,a) \leq\frac{1}{\rho}\cdot\Big{(}1-(1-\rho)^{H-h+1}\Big{)},\] (C.7) \[\max_{s\in\mathcal{S}}V_{h,P,\Phi}^{\pi}(s)-\min_{s\in\mathcal{S} }V_{h,P,\Phi}^{\pi}(s) \leq\frac{1}{\rho}\cdot\Big{(}1-(1-\rho)^{H-h+1}\Big{)}.\] (C.8)

First, we note that for the last step \(h=H\), (C.7) and (C.8) naturally hold since \(R_{H}\in[0,1]\). Now suppose that (C.8) hold for some step \(h+1\). By robust Bellman equation (Proposition 2.2), we have that

\[Q_{h,P^{\star},\Phi}^{\pi}(s,a) =R_{h}(s,a)+\mathbb{E}_{\mathcal{P}_{\mu}(s,a;P_{h}^{\star})} \Big{[}V_{h+1,P^{\star},\Phi}^{\pi}\Big{]}\] \[\leq 1+\mathbb{E}_{\mathcal{P}_{\mu}(s,a;P_{h}^{\star})}\Big{[}V_{h +1,P^{\star},\Phi}^{\pi}\Big{]},\quad\forall(s,a)\in\mathcal{S}\times\mathcal{ A},\] (C.9)

where the inequality uses the fact that \(R_{h}\leq 1\). Now we denote the state with the least robust value as

\[s_{0}\in\operatorname*{argmin}_{s\in\mathcal{S}}V_{h+1,P^{\star},\Phi}^{\pi}(s).\] (C.10)

Inspired by Shi et al. (2023), we choose a transition kernel \(\widetilde{P}_{h}\) satisfying that

\[\Big{\|}\widetilde{P}_{h}(\cdot|s,a)\Big{\|}_{1}=1-\rho,\quad P_{h}^{\star}(s^ {\prime}|s,a)\geq\widetilde{P}_{h}(s^{\prime}|s,a)\geq 0,\quad\forall(s,a,s^{ \prime})\in\mathcal{S}\times\mathcal{A}\times\mathcal{S},\]

which implies that

\[D_{\mathrm{TV}}\left(\widetilde{P}_{h}(\cdot|s,a)+\rho\cdot\delta_{s_{0}}( \cdot)\,\right\|P_{h}^{\star}(\cdot|s,a)\right)\leq\rho,\quad\forall(s,a)\in \mathcal{S}\times\mathcal{A}.\]Here \(\delta_{s_{0}}(\cdot)\) is the point measure centered at \(s_{0}\) defined in (C.10). Combined with (C.9), we have that

\[Q^{\pi}_{h,P^{*},\,\bm{\Phi}}(s,a) \leq 1+\mathbb{E}_{\widetilde{P}_{h}(\cdot|s,a)+\rho\cdot\delta_{s_ {0}}(\cdot)}\Big{[}V^{\pi}_{h+1,P^{*},\,\bm{\Phi}}\Big{]}\] \[=1+\mathbb{E}_{\widetilde{P}_{h}(\cdot|s,a)}\Big{[}V^{\pi}_{h+1, P^{*},\,\bm{\Phi}}\Big{]}+\rho\cdot V^{\pi}_{h+1,P^{*},\,\bm{\Phi}}(s_{0})\] \[\leq 1+(1-\rho)\cdot\max_{s\in\mathcal{S}}V^{\pi}_{h+1,P^{*},\, \bm{\Phi}}(s)+\rho\cdot\min_{s\in\mathcal{S}}V^{\pi}_{h+1,P^{*},\,\bm{\Phi}}(s).\] (C.11)

Consequently from (C.11), we further obtain that for any \((s,a)\in\mathcal{S}\times\mathcal{A}\),

\[Q^{\pi}_{h,P^{*},\,\bm{\Phi}}(s,a)-\min_{(s,a)\in\mathcal{S} \times\mathcal{A}}Q^{\pi}_{h,P^{*},\,\bm{\Phi}}(s,a)\] \[\qquad\leq 1+(1-\rho)\cdot\max_{s\in\mathcal{S}}V^{\pi}_{h+1,P^{* },\,\bm{\Phi}}(s)+\rho\cdot\min_{s\in\mathcal{S}}V^{\pi}_{h+1,P^{*},\,\bm{\Phi }}(s)-\min_{(s,a)\in\mathcal{S}\times\mathcal{A}}Q^{\pi}_{h,P^{*},\,\bm{\Phi}} (s,a)\] \[\qquad\quad+\min_{s\in\mathcal{S}}V^{\pi}_{h+1,P^{*},\,\bm{\Phi}} (s)-\min_{(s,a)\in\mathcal{S}\times\mathcal{A}}Q^{\pi}_{h,P^{*},\,\bm{\Phi}}(s,a)\] \[\qquad\leq 1+(1-\rho)\cdot\left(\max_{s\in\mathcal{S}}V^{\pi}_{h+1, P^{*},\,\bm{\Phi}}(s)-\min_{s\in\mathcal{S}}V^{\pi}_{h+1,P^{*},\,\bm{\Phi}}(s) \right),\] (C.12)

where the first inequality uses (C.11) and the last inequality uses the following fact,

\[\min_{(s,a)\in\mathcal{S}\times\mathcal{A}}Q^{\pi}_{h,P^{*},\,\bm {\Phi}}(s,a) =\min_{(s,a)\in\mathcal{S}\times\mathcal{A}}\left\{R_{h}(s,a)+ \mathbb{E}_{\mathcal{P}_{\rho}(s,a;P^{*}_{h})}\Big{[}V^{\pi}_{h+1,P^{*},\,\bm{ \Phi}}\Big{]}\right\}\] \[\geq\min_{s\in\mathcal{S}}V^{\pi}_{h+1,P^{*},\,\bm{\Phi}}(s).\]

Now applying the assumption that (C.8) holds at step \(h+1\) to the right hand side of (C.12), we obtain that

\[\max_{(s,a)\in\mathcal{S}\times\mathcal{A}}Q^{\pi}_{h,P^{*},\,\bm {\Phi}}(s,a)-\min_{(s,a)\in\mathcal{S}\times\mathcal{A}}Q^{\pi}_{h,P^{*},\,\bm {\Phi}}(s,a) \leq 1+\frac{1-\rho}{\rho}\cdot\Big{(}1-(1-\rho)^{H-h}\Big{)}\] \[=\frac{1}{\rho}\cdot\Big{(}1-(1-\rho)^{H-h+1}\Big{)}.\]

Thus given (C.8) at step \(h+1\), we can derive (C.7) at step \(h\). Now by noticing that

\[\min_{(s,a)\in\mathcal{S}\times\mathcal{A}}Q^{\pi}_{h,P^{*},\,\bm {\Phi}}(s,a) \leq\min_{s\in\mathcal{S}}V^{\pi}_{h,P^{*},\,\bm{\Phi}}(s)\leq\max_{s\in \mathcal{S}}V^{\pi}_{h,P^{*},\,\bm{\Phi}}(s)\leq\max_{(s,a)\in\mathcal{S} \times\mathcal{A}}Q^{\pi}_{h,P^{*},\,\bm{\Phi}}(s,a),\]

we can conclude that (C.8) also holds at step \(h\). As a result, by an induction argument, we finish the proof of Proposition 2.6. 

### Proof of Proposition 4.2

Proof of Proposition 4.2.: We consider some fixed \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times[H]\) throughout proof. By Lemma C.1, we have that

\[\mathbb{E}_{\mathcal{P}_{\rho}(s,a;P^{*}_{h})}\left[V\right] =\sup_{\eta\in\mathbb{R}}\Bigg{\{}-\mathbb{E}_{P^{*}_{h}(\cdot|s, a)}\big{[}(\eta-V)_{+}\big{]}-\frac{\rho}{2}\cdot\left(\eta-\min_{s\in\mathcal{S}}V(s) \right)_{+}+\eta\Bigg{\}}\] \[=\sup_{\eta\in[0,H]}\Bigg{\{}-\mathbb{E}_{P^{*}_{h}(\cdot|s,a)} \big{[}(\eta-V)_{+}\big{]}+\Big{(}1-\frac{\rho}{2}\Big{)}\cdot\eta\Bigg{\}},\] (C.13)

where the second equality follows from the fact the optimal dual variable \(\eta^{\star}\) is in \([0,H]\) when \(V\in[0,H]\) (see e.g., Lemma H.8 in Blanchet et al. (2023)), and the last equality is obtained by the fact that \(\min_{s\in\mathcal{S}}V(s)=0\).

Part (i).For any \(\eta\in[0,H]\) and \(Q\in\mathcal{B}_{\rho^{\prime}}(s,a;P_{h}^{*})\), we have that

\[-\mathbb{E}_{P_{h}^{*}(\cdot|s,a)}\big{[}(\eta-V)_{+}\big{]}+\Big{(} 1-\frac{\rho}{2}\Big{)}\cdot\eta \leq\Big{(}1-\frac{\rho}{2}\Big{)}\cdot\Big{(}-\mathbb{E}_{Q( \cdot)}\big{[}(\eta-V)_{+}\big{]}+\eta\Big{)}\] \[\leq\Big{(}1-\frac{\rho}{2}\Big{)}\cdot\Big{(}-\mathbb{E}_{Q( \cdot)}\big{[}\eta-V\big{]}+\eta\Big{)}\] \[=\Big{(}1-\frac{\rho}{2}\Big{)}\cdot\mathbb{E}_{Q(\cdot)}\big{[}V \big{]},\] (C.14)

where the first inequality uses the definition of \(\mathcal{B}_{\rho^{\prime}}(s,a;P_{h}^{*})\), the second equality follows from the fact that \((x)_{+}\geq x\). Furthermore, by (C.14) we have that

\[\sup_{\eta\in[0,H]}\bigg{\{}-\mathbb{E}_{P_{h}^{*}(\cdot|s,a)}\big{[}(\eta-V) _{+}\big{]}+\Big{(}1-\frac{\rho}{2}\Big{)}\cdot\eta\bigg{\}}\leq\Big{(}1- \frac{\rho}{2}\Big{)}\cdot\inf_{Q\in\mathcal{B}_{\rho^{\prime}}(s,a;P_{h}^{*} )}\mathbb{E}_{Q(\cdot)}\big{[}V\big{]}.\] (C.15)

Combining (C.13) and (C.15), we have that

\[\mathbb{E}_{\mathcal{P}_{\rho}(s,a;P_{h}^{*})}\big{[}V\big{]}\leq\rho^{\prime }\cdot\mathbb{E}_{\mathcal{B}_{\rho^{\prime}}(s,a;P_{h}^{*})}\big{[}V\big{]}.\]

Part (ii).Since \(\rho\in[0,1]\), we know that there exists a \(\widetilde{\eta}\in[0,H]\) such that

\[\sum_{s^{\prime}:V(s^{\prime})<\widetilde{\eta}}P_{h}^{*}(s^{\prime}|s,a)\leq 1 -\frac{\rho}{2}\leq\sum_{s^{\prime}:V(s^{\prime})\leq\widetilde{\eta}}P_{h}^{ \star}(s^{\prime}|s,a),\]

which further implies that we have the following interpolation for some \(\lambda\in[0,1]\):

\[1-\frac{\rho}{2}=\lambda\sum_{s^{\prime}:V(s^{\prime})<\widetilde{\eta}}P_{h} ^{*}(s^{\prime}|s,a)+(1-\lambda)\sum_{s^{\prime}:V(s^{\prime})\leq\widetilde{ \eta}}P_{h}^{\star}(s^{\prime}|s,a).\]

We define a probability measure \(\widetilde{P}^{\star}\in\Delta(\mathcal{S})\) as

\[\widetilde{P}_{h}^{\star}=\frac{\lambda P_{h}^{\star}(s^{\prime}|s,a)\cdot \mathbf{1}\{V(s^{\prime})>\widetilde{\eta}\}+(1-\lambda)P_{h}^{\star}(s^{ \prime}|s,a)\cdot\mathbf{1}\{V(s^{\prime})\geq\widetilde{\eta}\}}{1-\frac{ \rho}{2}}.\] (C.16)

It is not difficult to verify that \(\widetilde{P}_{h}^{\star}\in\mathcal{B}_{\rho^{\prime}}(s,a;P_{h}^{\star})\). Hence, we have

\[\Big{(}1-\frac{\rho}{2}\Big{)}\cdot\mathbb{E}_{\mathcal{B}_{\rho^ {\prime}}(s,a;P_{h}^{*})}[V] \leq\Big{(}1-\frac{\rho}{2}\Big{)}\cdot\mathbb{E}_{\widetilde{P}_ {h}^{\star}(\cdot)}\big{[}V\big{]}\] \[=-\mathbb{E}_{P_{h}^{\star}(\cdot|s,a)}\big{[}(\widetilde{\eta}-V )_{+}\big{]}+\Big{(}1-\frac{\rho}{2}\Big{)}\cdot\widetilde{\eta},\] (C.17)

where the last inequality uses the definition of \(\widetilde{P}_{h}^{\star}\) in (C.16). Furthermore, by (C.17) we have that

\[\rho^{\prime}\cdot\mathbb{E}_{\mathcal{B}_{\rho^{\prime}}(s,a;P_ {h}^{*})}\big{[}V\big{]} \leq\sup_{\eta\in[0,H]}\bigg{\{}-\mathbb{E}_{P_{h}^{*}(\cdot|s,a)} \big{[}(\eta-V)_{+}\big{]}+\Big{(}1-\frac{\rho}{2}\Big{)}\cdot\eta\bigg{\}}\] (C.18) \[=\mathbb{E}_{\mathcal{P}_{\rho}(s,a;P_{h}^{*})}\big{[}V\big{]},\]

where the equality follows from (C.13).

Combining Part (i) and Part (ii).Finally, combining (C.15) and (C.18), we prove Proposition 4.2. 

## Appendix D Proofs for Hardness Results

### Proof of Theorem 3.2

Proof of Theorem 3.2.: We first explicitly give the expressions of the robust value functions in Example 3.1, based on which we derive the desired online regret lower bound.

Robust value function.Firstly, we can explicitly write down the expression of the robust value functions for any policy \(\pi\) under Example 3.1, i.e., \(V^{\pi}_{h,P^{*},\mathcal{M}_{\theta},\mathbf{\Phi}}\) and \(Q^{\pi}_{h,P^{*},\mathcal{M}_{\theta},\mathbf{\Phi}}\). From now on we fix a policy \(\pi\).

_For step \(h=3\), the robust value function is the reward received. We can directly obtain for any \(a\in\mathcal{A}\),_

\[Q^{\pi}_{3,P^{*},\mathcal{M}_{\theta},\mathbf{\Phi}}(s_{\mathrm{ good}},a) =V^{\pi}_{3,P^{*},\mathcal{M}_{\theta},\mathbf{\Phi}}(s_{\mathrm{ good}})=1,\] (D.1) \[Q^{\pi}_{3,P^{*},\mathcal{M}_{\theta},\mathbf{\Phi}}(s_{\mathrm{ bad}},a) =V^{\pi}_{3,P^{*},\mathcal{M}_{\theta},\mathbf{\Phi}}(s_{\mathrm{ bad}})=0.\]

_For step \(h=2\), by the robust Bellman equation (Proposition 2.2), we have that for the good state \(s_{\mathrm{good}}\),_

\[Q^{\pi}_{2,P^{*},\mathcal{M}_{\theta},\mathbf{\Phi}}(s_{\mathrm{ good}},a)\] (D.2) \[\qquad=1+\inf_{P\in\mathcal{P}_{\rho}(s_{\mathrm{good}},a;P^{*}_{ 2},\mathcal{M}_{\theta})}\mathbb{E}_{P(\cdot)}\big{[}V^{\pi}_{3,P^{*}, \mathcal{M}_{\theta},\mathbf{\Phi}}\big{]}=1+(1-\rho),\quad\forall a\in \mathcal{A},\]

_where the last equality is because \(V^{\pi}_{3,P^{*},\mathcal{M}_{\theta},\mathbf{\Phi}}\) takes the minimal value \(0\) at the bad state \(s_{\mathrm{bad}}\) and thus the most adversarial transition distribution is achieved at_

\[P^{\dagger}(s^{\prime})=(1-\rho)\cdot\mathbf{1}\{s^{\prime}=s_{\mathrm{good} }\}+\rho\cdot\mathbf{1}\{s^{\prime}=s_{\mathrm{bad}}\}.\]

_Similarly, we have that for the bad state \(s_{\mathrm{bad}}\),_

\[Q^{\pi}_{2,P^{*},\mathcal{M}_{\theta},\mathbf{\Phi}}(s_{\mathrm{ bad}},a) =0+\inf_{P\in\mathcal{P}_{\rho}(s_{\mathrm{bad}},a;P^{*}_{2}, \mathcal{M}_{\theta})}\mathbb{E}_{P(\cdot)}\big{[}V^{\pi}_{3,P^{*},\mathcal{M }_{\theta},\mathbf{\Phi}}\big{]}\] (D.3) \[=\begin{cases}p-\rho,&a=\theta\\ q-\rho,&a=1-\theta\end{cases}.\]

_Finally by the robust Bellman equation again, we have that_

\[V^{\pi}_{2,P^{*},\mathcal{M}_{\theta},\mathbf{\Phi}}(s_{\mathrm{ good}}) =1+(1-\rho),\] \[V^{\pi}_{2,P^{*},\mathcal{M}_{\theta},\mathbf{\Phi}}(s_{\mathrm{ bad}}) =\pi_{2}(\theta|s_{\mathrm{bad}})\cdot(p-\rho)+\pi_{2}(1-\theta|s_{\mathrm{bad}}) \cdot(q-\rho).\]

_Notice that by \(q<p\) we know that \(V^{\pi}_{2,P^{*},\mathcal{M}_{\theta},\mathbf{\Phi}}(s_{\mathrm{bad}})<p-\rho <1+(1-\rho)<V^{\pi}_{2,P^{*},\mathcal{M}_{\theta},\mathbf{\Phi}}(s_{\mathrm{ good}})\)._

_For step \(h=1\), we consider the robust values on the initial state \(s_{1}=s_{\mathrm{good}}\), by robust Bellman equation,_

\[Q^{\pi}_{1,P^{*},\mathcal{M}_{\theta},\mathbf{\Phi}}(s_{\mathrm{ good}},a) =1+\inf_{P\in\mathcal{P}_{\rho}(s_{\mathrm{good}},a;P^{*}_{1}, \mathcal{M}_{\theta})}\mathbb{E}_{P(\cdot)}\big{[}V^{\pi}_{2,P^{*},\mathcal{M }_{\theta},\mathbf{\Phi}}\big{]}\] (D.4) \[=1+(1-\rho)\cdot\big{[}1+(1-\rho)\big{]}\] \[\qquad+\rho\cdot\big{[}\pi_{2}(\theta|s_{\mathrm{bad}})\cdot(p- \rho)+\pi_{2}(1-\theta|s_{\mathrm{bad}})\cdot(q-\rho)\big{]},\quad\forall a\in \mathcal{A}.\]

_By robust Bellman equation, we also derive \(V^{\pi}_{1,P^{*},\mathcal{M}_{\theta},\mathbf{\Phi}}(s_{\mathrm{good}})=Q^{ \pi}_{1,P^{*},\mathcal{M}_{\theta},\mathbf{\Phi}}(s_{\mathrm{good}},a)\) for \(\forall a\in\mathcal{A}\)._

Lower bound the online regret under Example 3.1.With all the previous preparation, we can lower bound the online regret for robust RL with interactive data collection in Example 3.1. But first, we present the following general lemma.

**Lemma D.1** (Performance difference lemma for robust value function).: _For any RMDP satisfying Assumption 2.1 and any policy \(\pi\), the following inequality holds,_

\[V^{\pi^{*},\dagger}_{1,P^{*},\mathbf{\Phi}}(s)-V^{\pi}_{1,P^{*}, \mathbf{\Phi}}(s)\] \[\qquad\geq\mathbb{E}_{(P^{*^{*},\dagger},\pi^{*})}\left[\sum_{h =1}^{H}\sum_{a\in\mathcal{A}}\big{(}\pi^{\star}_{h}(a|s_{h})-\pi_{h}(a|s_{h}) \big{)}\cdot Q^{\pi}_{h,P^{*},\mathbf{\Phi}}(s_{h},a)\Bigg{|}s_{1}=s\right],\]

_where the expectation is taken with respect to the trajectories induced by policy \(\pi^{\star}\), transition kernel \(P^{\pi^{*},\dagger}\). Here the transition kernel \(P^{\pi^{*},\dagger}\) is defined as_

\[P^{\pi^{*},\dagger}_{h}(\cdot|s,a)=\underset{P\in\mathcal{P}(s,a;P^{\star}_{h}) }{\operatorname{\mathrm{\mathrm{\mathrm{missing}}}}}\mathbb{E}_{P(\cdot)}\big{[}V^ {\pi^{*}}_{h+1,P^{*},\mathbf{\Phi}}\big{]},\]

_where \(\mathcal{P}(s,a;P^{\star}_{h})\) is the robust set for state-action pair \((s,a)\) (see Assumption 2.1)._

[MISSING_PAGE_FAIL:27]

However, since in RMDPs of Example 3.1, the online interaction process is always kept in \(s_{\mathrm{good}}\) and there is no information on \(\theta\) which can only be accessed at \((s,h)=(s_{\mathrm{bad}},2)\). As a result, the estimates \(\pi_{2}^{k}(\cdot|s_{\mathrm{bad}})\) of \(\pi_{2}^{\star,\mathcal{M}_{\theta}}(\cdot|s_{\mathrm{bad}})=\mathbf{1}\{ \cdot=\theta\}\) can do no better than a random guess. Put it formally, consider that

\[\sup_{\theta\in\{0,1\}}\mathbb{E}_{\mathcal{M}_{\theta},\mathcal{ ALG}}\left[\mathrm{Regret}_{\mathbf{\Phi}}^{\mathcal{M}_{\theta},\mathcal{ ALG}}(K)\right]\] \[\qquad\geq\rho\cdot(p-q)\cdot\sup_{\theta\in\{0,1\}}\mathbb{E}_{ \mathcal{M}_{\theta},\mathcal{ALG}}\left[\sum_{k=1}^{K}D_{\mathrm{TV}}\left( \pi_{2}^{\star,\mathcal{M}_{\theta}}(\cdot|s_{\mathrm{bad}})\right)\right]\] \[\qquad=\rho\cdot(p-q)\cdot\sup_{\theta\in\{0,1\}}\sum_{k=1}^{K} \mathbb{E}_{\mathcal{ALG}}\left[\pi_{2}^{k}(1-\theta|s_{\mathrm{bad}})\right].\] (D.11)

Here in the last equality we can drop the subscription of \(\mathcal{M}_{\theta}\) because the algorithm outputs \(\pi_{2}^{k}\) independent of the \(\theta\) due to our previous discussion. Notice that

\[\sum_{\theta\in\{0,1\}}\sum_{k=1}^{K}\mathbb{E}_{\mathcal{ALG}}\left[\pi_{2}^{ k}(1-\theta|s_{\mathrm{bad}})\right]=\sum_{k=1}^{K}\sum_{\theta\in\{0,1\}} \mathbb{E}_{\mathcal{ALG}}\left[\pi_{2}^{k}(1-\theta|s_{\mathrm{bad}})\right]= \sum_{k=1}^{K}1=K,\]

which further indicates that

\[\sup_{\theta\in\{0,1\}}\sum_{k=1}^{K}\mathbb{E}_{\mathcal{ALG}}\left[\pi_{2}^{ k}(1-\theta|s_{\mathrm{bad}})\right]\geq\frac{K}{2}.\] (D.12)

Therefore, by combining (D.11) and (D.12), we conclude that

\[\inf_{\mathcal{ALG}}\sup_{\theta\in\{0,1\}}\mathbb{E}_{\mathcal{M}_{\theta}, \mathcal{ALG}}\left[\mathrm{Regret}_{\mathbf{\Phi}}^{\mathcal{M}_{\theta}, \mathcal{ALG}}(K)\right]\geq(p-q)\cdot\frac{\rho K}{2}.\]

This is the desired online regret lower bound of \(\Omega(\rho\cdot K)\) for the RMDPs presented in Example 3.1. Furthermore, we can construct two RMDPs \(\{\widetilde{\mathcal{M}}_{0},\widetilde{\mathcal{M}}_{1}\}\) with horizon \(3H\) by concatenating \(H\) RMDPs \(\{\mathcal{M}_{0},\mathcal{M}_{1}\}\) presented in Example 3.1. Notably, at any steps \(\{3i+1\}_{i=0}^{H-1}\), we define

\[R_{3i+1}(s_{\mathrm{bad}},a)=1,\qquad P_{3i+1}^{\star,\widetilde{\mathcal{M}} _{\theta}}(s_{\mathrm{good}}|s_{\mathrm{bad}},a)=1,\quad\forall(a,\theta)\in \mathcal{A}\times\{0,1\}.\]

Then we have

\[\inf_{\mathcal{ALG}}\sup_{\theta\in\{0,1\}}\mathbb{E}_{\widetilde{ \mathcal{M}}_{\theta},\mathcal{ALG}}\left[\mathrm{Regret}_{\mathbf{\Phi}}^{ \widetilde{\mathcal{M}}_{\theta},\mathcal{ALG}}(K)\right]\geq H\cdot\Omega( \rho\cdot K)=\Omega(\rho\cdot HK),\]

which completes the proof of Theorem 3.2. 

### Proof of Lemma d.1

Proof of Lemma d.1.: For any step \(h\in[H]\), we have that by robust Bellman equation (Proposition 2.2),

\[Q_{h,P^{\star},\mathbf{\Phi}}^{\pi^{\star}}(s,a)-Q_{h,P^{\star},\mathbf{\Phi} }^{\pi}(s,a) =\mathbb{E}_{\mathcal{P}_{\rho}(s,a;P_{h}^{\star})}\big{[}V_{h+1,P^{ \star},\mathbf{\Phi}}^{\pi^{\star}}\big{]}-\mathbb{E}_{\mathcal{P}_{\rho}(s,a; P_{h}^{\star})}\big{[}V_{h+1,P^{\star},\mathbf{\Phi}}^{\pi}\big{]}.\]

By the definition of the transition kernel \(P^{\pi^{\star},\dagger}\) in Lemma D.1 and the property of infimum, we have that

\[Q_{h,P^{\star},\mathbf{\Phi}}^{\pi^{\star}}(s,a)-Q_{h,P^{\star}, \mathbf{\Phi}}^{\pi}(s,a) \geq\mathbb{E}_{P_{h}^{\pi^{\star},\dagger}(\cdot|s,a)}\big{[}V_{h +1,P^{\star},\mathbf{\Phi}}^{\pi^{\star}}\big{]}-\mathbb{E}_{P_{h}^{\pi^{ \star},\dagger}(\cdot|s,a)}\big{[}V_{h+1,P^{\star},\mathbf{\Phi}}^{\pi}\big{]}\] \[=\mathbb{E}_{P_{h}^{\pi^{\star},\dagger}(\cdot|s,a)}\big{[}V_{h +1,P^{\star},\mathbf{\Phi}}^{\pi^{\star}}-V_{h+1,P^{\star},\mathbf{\Phi}}^{\pi} \big{]}.\] (D.13)

By robust Bellman equation (Proposition 2.2) and (D.13), we further obtain that

\[V_{h,P^{\star},\mathbf{\Phi}}^{\pi^{\star}}(s)-V_{h,P^{\star}, \mathbf{\Phi}}^{\pi}(s) =\mathbb{E}_{\pi_{h}^{\star}(\cdot|s)}\big{[}Q_{h,P^{\star}, \mathbf{\Phi}}^{\pi^{\star}}(s,\cdot)\big{]}-\mathbb{E}_{\pi_{h}(\cdot|s)} \big{[}Q_{h,P^{\star},\mathbf{\Phi}}^{\pi}(s,\cdot)\big{]}\] \[\qquad+\mathbb{E}_{\pi_{h}^{\star}(\cdot|s)}\big{[}Q_{h,P^{\star}, \mathbf{\Phi}}^{\pi^{\star}}(s,\cdot)\big{]}-\mathbb{E}_{\pi_{h}^{\star}(\cdot|s )}\big{[}Q_{h,P^{\star},\mathbf{\Phi}}^{\pi}(s,\cdot)\big{]}\] \[\geq\sum_{a\in\mathcal{A}}\big{(}\pi_{h}^{\star}(a|s)-\pi_{h}(a|s) \big{)}\cdot Q_{h,P^{\star},\mathbf{\Phi}}^{\pi}(s,a)\] \[\qquad+\mathbb{E}_{a\sim\pi_{h}^{\star}(\cdot|s),P_{h}^{\pi^{ \star},\dagger}(\cdot|s,a)}\big{[}V_{h,P^{\star},\mathbf{\Phi}}^{\pi}-V_{h,P^{ \star},\mathbf{\Phi}}^{\pi}\big{]}.\] (D.14)

[MISSING_PAGE_EMPTY:29]

\[\leq\mathbb{E}_{\mathcal{P}_{\rho}(s,a;\bar{P}^{k}_{h})}\Big{[} \overline{V}^{k}_{h+1}\Big{]}-\mathbb{E}_{\mathcal{P}_{\rho}(s,a;\bar{P}^{k}_{h} )}\Big{[}\underline{V}^{k}_{h+1}\Big{]}+2\cdot\mathsf{bonus}^{k}_{h}(s,a)\] \[=\underbrace{\mathbb{E}_{\mathcal{P}_{\rho}(s,a;\bar{P}^{k}_{h})} \Big{[}\overline{V}^{k}_{h+1}\Big{]}-\mathbb{E}_{\mathcal{P}_{\rho}(s,a;P^{*}_ {h})}\Big{[}\overline{V}^{k}_{h+1}\Big{]}+\mathbb{E}_{\mathcal{P}_{\rho}(s,a;P^ {*}_{h})}\Big{[}\underline{V}^{k}_{h+1}\Big{]}-\mathbb{E}_{\mathcal{P}_{\rho}(s,a;\bar{P}^{k}_{h})}\Big{[}\underline{V}^{k}_{h+1}\Big{]}}_{\text{Term (i)}}\] \[\qquad+\underbrace{\mathbb{E}_{\mathcal{P}_{\rho}(s,a;P^{*}_{h})} \Big{[}\overline{V}^{k}_{h+1}\Big{]}-\mathbb{E}_{\mathcal{P}_{\rho}(s,a;P^{* }_{h})}\Big{[}\underline{V}^{k}_{h+1}\Big{]}}_{\text{Term (ii)}}+2\cdot\mathsf{bonus}^{k}_{h}(s,a).\] (E.2)

Step 1.1: upper bounding Term (i).By using a Bernstein-style concentration argument customized for TV robust expectations (Lemma E.3), we can bound Term (i) by the bonus function, i.e.,

\[\text{Term (i)}\leq 2\cdot\mathsf{bonus}^{k}_{h}(s,a).\] (E.3)

Step 1.2: upper bounding Term (ii).By our definition of the operator \(\mathbb{E}_{\mathcal{P}_{\rho}(s,a;P^{*}_{h})}[V]\) in (4.1), we have

\[\text{Term (ii)}=\sup_{\eta\in[0,H]}\bigg{\{}-\mathbb{E}_{P^{*}_{h} (\cdot|s,a)}\bigg{[}\Big{(}\eta-\overline{V}^{k}_{h+1}\Big{)}_{+}\bigg{]}+ \Big{(}1-\frac{\rho}{2}\Big{)}\cdot\eta\bigg{\}}\] \[\qquad-\sup_{\eta\in[0,H]}\bigg{\{}-\mathbb{E}_{P^{*}_{h}(\cdot| s,a)}\bigg{[}\Big{(}\eta-\underline{V}^{k}_{h+1}\Big{)}_{+}\bigg{]}+\Big{(}1-\frac{ \rho}{2}\Big{)}\cdot\eta\bigg{\}}\] \[\leq\sup_{\eta\in[0,H]}\bigg{\{}\mathbb{E}_{P^{*}_{h}(\cdot|s,a)} \bigg{[}\Big{(}\eta-\underline{V}^{k}_{h+1}\Big{)}_{+}-\Big{(}\eta-\overline {V}^{k}_{h+1}\Big{)}_{+}\bigg{]}\bigg{\}}.\] (E.4)

By Lemma E.2 which shows that \(\overline{V}^{k}_{h+1}\geq\underline{V}^{k}_{h+1}\) and the fact that \((\eta-x)_{+}-(\eta-y)_{+}\leq y-x\) for any \(y>x\), we can further upper bound the right hand side of (E.4) by

\[\text{Term (ii)}\leq\mathbb{E}_{P^{*}_{h}(\cdot|s,a)}\Big{[}\overline{V}^{k}_{h+1 }-\underline{V}^{k}_{h+1}\Big{]}.\] (E.5)

Step 1.3: combining the upper bounds.Now combining (E.3) and (E.5) with (E.2), we have that

\[\overline{Q}^{k}_{h}(s,a)-\underline{Q}^{k}_{h}(s,a)\leq\mathbb{E}_{P^{*}_{h}( \cdot|s,a)}\Big{[}\overline{V}^{k}_{h+1}-\underline{V}^{k}_{h+1}\Big{]}+4 \cdot\mathsf{bonus}^{k}_{h}(s,a).\]

By Lemma E.4, we can upper bound the bonus function, and after rearranging terms we further obtain that

\[\overline{Q}^{k}_{h}(s,a)-\underline{Q}^{k}_{h}(s,a)\leq\bigg{(}1 +\frac{12}{H}\bigg{)}\cdot\mathbb{E}_{P^{*}_{h}(\cdot|s,a)}\Big{[}\overline{V }^{k}_{h+1}-\underline{V}^{k}_{h+1}\Big{]}\] \[\qquad+4\sqrt{\frac{\mathsf{V}_{P^{*}_{h}(\cdot|s,a)}\Big{[} \underline{V}^{\pi^{k}}_{h+1,P^{*},\bm{\Phi}}\Big{]}\cdot c_{1}t}{N^{k}_{h}( s,a)\lor 1}}+\frac{4c_{2}H^{2}S_{t}}{N^{k}_{h}(s,a)\lor 1}+\frac{4}{\sqrt{K}},\] (E.6)

where \(c_{1},c_{2}>0\) are two absolute constants. For the sake of brevity, we introduce the following notations of differences, for any \((h,k)\in[H]\times[K]\),

\[\Delta^{k}_{h}:=\overline{V}^{k}_{h}(s^{k}_{h})-\underline{V}^{k} _{h}(s^{k}_{h}),\] \[\zeta^{k}_{h}:=\Delta^{k}_{h}-\Big{(}\overline{Q}^{k}_{h}(s^{k}_ {h},a^{k}_{h})-\underline{Q}^{k}_{h}(s^{k}_{h},a^{k}_{h})\Big{)},\] (E.7) \[\xi^{k}_{h}:=\mathbb{E}_{P^{*}_{h}(\cdot|s^{k}_{h},a^{k}_{h})} \Big{[}\overline{V}^{k}_{h}-\underline{V}^{k}_{h}\Big{]}-\Delta^{k}_{h+1}.\] (E.8)

If we further define the filtration \(\{\mathcal{F}_{h,k}\}_{\{h,k\}\in[H]\times[K]}\) as

\[\mathcal{F}_{h,k}=\sigma\left(\{(s^{r}_{i},a^{r}_{i})\}_{(i,r)\in[H]\times[k-1 ]}\bigcup\{(s^{k}_{i},a^{k}_{i})\}_{i\in[h-1]}\bigcup\{s^{k}_{h}\}\right),\]then we can find that \(\{\zeta_{h}^{k}\}_{(h,k)\in[H]\times[K]}\) is a martingale difference sequence with respect to \(\{\mathcal{F}_{h,k}\}_{(h,k)\in[H]\times[K]}\) and \(\{\xi_{h}^{k}\}_{(h,k)\in[H]\times[K]}\) is a martingale difference sequence with respect to \(\{\mathcal{F}_{h,k}\cup\{a_{h}^{k}\}\}_{(h,k)\in[H]\times[K]}\). Also, we further have that

\[\Delta_{h}^{k} =\zeta_{h}^{k}+\left(\overline{Q}_{h}^{k}(s_{h}^{k},a_{h}^{k})- \underline{Q}_{h}^{k}(s_{h}^{k},a_{h}^{k})\right)\] (E.9) \[\leq\zeta_{h}^{k}+\left(1+\frac{12}{H}\right)\cdot\mathbb{E}_{P_{h }^{*}(\cdot|s_{h}^{k},a_{h}^{k})}\big{[}\overline{V}_{h+1}^{k}-\underline{V}_ {h+1}^{k}\big{]}\] \[\qquad+4\sqrt{\frac{\mathbb{V}_{P_{h}^{*}(\cdot|s,a)}\big{[}V_{h +1,P^{*},\boldsymbol{\Phi}}^{\pi^{k}}\big{]}\cdot c_{1}\iota}{N_{h}^{k}(s_{h} ^{k},a_{h}^{k})\lor 1}}+\frac{4c_{2}H^{2}S\iota}{N_{h}^{k}(s_{h}^{k},a_{h}^{k}) \lor 1}+\frac{4}{\sqrt{K}}\] \[=\zeta_{h}^{k}+\left(1+\frac{12}{H}\right)\cdot\xi_{h}^{k}+\left( 1+\frac{12}{H}\right)\cdot\Delta_{h+1}^{k}+\] \[\qquad 4\sqrt{\frac{\mathbb{V}_{P_{h}^{*}(\cdot|s,a)}\big{[}V_{h+1, P^{*},\boldsymbol{\Phi}}^{\pi^{k}}\big{]}\cdot c_{1}\iota}{N_{h}^{k}(s_{h}^{k},a_{h} ^{k})\lor 1}}+\frac{4c_{2}H^{2}S\iota}{N_{h}^{k}(s_{h}^{k},a_{h}^{k})\lor 1}+ \frac{4}{\sqrt{K}},\]

where the inequality applies (E.6). Recursively applying (E.9) and using the fact that \((1+\frac{12}{H})^{h}\leq(1+\frac{12}{H})^{H}\leq c\) for some absolute constant \(c>0\), we can upper bound the right hand side of (E.1) as

\[\mathrm{Regret}_{\boldsymbol{\Phi}}(K)\leq\sum_{k=1}^{K}\Delta_{ 1}^{k}\leq C_{1}\cdot\sum_{k=1}^{K}\sum_{h=1}^{H}(\zeta_{h}^{k}+\xi_{h}^{k})\] \[\qquad\qquad\qquad\qquad\qquad+\sqrt{\frac{\mathbb{V}_{P_{h}^{*} (\cdot|s,a)}\big{[}V_{h+1,P^{*},\boldsymbol{\Phi}}^{\pi^{k}}\big{]}\cdot\iota} {N_{h}^{k}(s_{h}^{k},a_{h}^{k})\lor 1}}+\frac{H^{2}S\iota}{N_{h}^{k}(s_{h}^{k},a_{h}^{k} )\lor 1}+\frac{1}{\sqrt{K}}.\] (E.10)

where \(C_{1}>0\) is an absolute constant.

Step 2: controlling the summation of variance terms.In view of (E.10), it suffices to upper bound its right hand side. The key difficulty is the analysis of the summation of the variance terms, which we focus on now. By Cauchy-Schwartz inequality,

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\sqrt{\frac{\mathbb{V}_{P_{h}^{*}( \cdot|s_{h}^{k},a_{h}^{k})}\big{[}V_{h+1,P^{*},\boldsymbol{\Phi}}^{\pi^{k}} \big{]}}{N_{h}^{k}(s_{h}^{k},a_{h}^{k})\lor 1}}\] \[\qquad\leq\sqrt{\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{V}_{P_{h}^{*} (\cdot|s_{h}^{k},a_{h}^{k})}\big{[}V_{h+1,P^{*},\boldsymbol{\Phi}}^{\pi^{k}} \big{]}}\cdot\sum_{k=1}^{K}\sum_{h=1}^{H}\frac{1}{N_{h}^{k}(s_{h}^{k},a_{h}^{k} )\lor 1}.\] (E.11)

On the right hand side of (E.11), the summation of the inverse of the count function is a well bounded term (Lemma E.13). So the key is to upper bound the summation of the variance of the robust value functions to obtain a sharp bound. To this end, we invoke Lemma E.5 to obtain that with probability at least \(1-\delta\),

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{V}_{P_{h}^{*}(\cdot|s_{h}^{k},a_{h}^{k})} \Big{[}V_{h+1,P^{*},\boldsymbol{\Phi}}^{\pi^{k}}\Big{]}\] \[\qquad\leq C_{2}\cdot\Big{(}\min\big{\{}H,\rho^{-1}\big{\}}\cdot HK +\min\big{\{}H,\rho^{-1}\big{\}}^{3}\cdot H\iota\Big{)},\] (E.12)

where \(C_{2}>0\) is an absolute constant. With inequality (E.12) and Lemma E.13 that

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\frac{1}{N_{h}^{k}(s_{h}^{k},a_{h}^{k})\lor 1} \leq C_{2}^{\prime}\cdot HSA\iota,\]

with \(C_{2}^{\prime}>0\) being another constant, we can upper bound the summation of the variance terms (E.11) as

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\sqrt{\frac{\mathbb{V}_{P_{h}^{*}( \cdot|s_{h}^{k},a_{h}^{k})}\big{[}V_{h+1,P^{*},\boldsymbol{\Phi}}^{\pi^{k}} \big{]}}{N_{h}^{k}(s_{h}^{k},a_{h}^{k})\lor 1}}\]\[\leq C_{3}\sqrt{\min\left\{H,\rho^{-1}\right\}\cdot H^{2}SAK\iota+\min \left\{H,\rho^{-1}\right\}^{3}\cdot H^{2}SA\iota^{2}}.\] (E.13)

where \(C_{3}>0\) is also an absolute constant.

Step 3: finishing the proof.With (E.10) and (E.13), it suffices to control the remaining terms. For the summation of the martingale difference terms, notice that by the definitions in (E.7) and (E.8), both \(\zeta_{h}^{k}\) and \(\xi_{h}^{k}\) are bounded by \(\min\{H,\rho^{-1}\}\) according to (4.2) and Lemma E.2 (optimism and pessimism). As a result, using Azuma-Hoeffding inequality, with probability at least \(1-\delta\)

\[\sum_{k=1}^{K}\sum_{h=1}^{H}(\zeta_{h}^{k}+\xi_{h}^{k})\leq C_{4} \cdot\min\left\{H,\rho^{-1}\right\}\cdot\sqrt{HK\iota},\]

where \(C_{4}>0\) is an absolute constant. For the summation of the inverse of the count function in (E.10), it suffices to invoke again Lemma E.13. Combining all together, with probability at least \(1-3\delta\), we have

\[\mathrm{Regret}_{\bm{\Phi}}(K) \leq C_{5}\cdot\left(\sqrt{\min\left\{H,\rho^{-1}\right\}\cdot H ^{2}SAK\iota^{2}+\min\left\{H,\rho^{-1}\right\}^{3}\cdot H^{2}SA\iota^{3}}\right.\] \[\qquad+\min\left\{H,\rho^{-1}\right\}\cdot\sqrt{HK\iota}+H^{3}S^ {2}A\iota^{2}+H\sqrt{K}\right)\] \[=\mathcal{O}\left(\sqrt{\min\left\{H,\rho^{-1}\right\}\cdot H^{2 }SAK\iota^{\prime}}\right),\]

where \(C_{5}>0\) is an absolute constant and \(\iota^{\prime}=\log^{2}(SAHK/\delta)\). This completes the proof of Theorem 4.3. 

### Key Lemmas

**Lemma E.2** (Optimistic and pessimistic estimation of the robust values).: _By setting the \(\mathsf{bonus}_{h}^{k}\) as in (4.5), then under the typical event \(\mathcal{E}\), it holds that_

\[\underline{Q}_{h}^{k}(s,a)\leq Q_{h,P^{*},\bm{\Phi}}^{\mp}(s,a) \leq Q_{h,P^{*},\bm{\Phi}}^{*}(s,a)\leq\overline{Q}_{h}^{k}(s,a),\] \[\underline{V}_{h}^{k}(s)\leq V_{h,P^{*},\bm{\Phi}}^{\mp}(s)\leq V _{h,P^{*},\bm{\Phi}}^{*}(s)\leq\overline{V}_{h}^{k}(s),\] (E.14)

_for any \((s,a,h,k)\in\mathcal{S}\times\mathcal{A}\times[H]\times[K]\)._

Proof of Lemma e.2.: See Appendix E.3 for a detailed proof. 

**Lemma E.3** (Proper bonus for TV robust sets and optimistic and pessimistic value estimators).: _By setting the \(\mathsf{bonus}_{h}^{k}\) as in (4.5), then under the typical event \(\mathcal{E}\), it holds that_

\[\mathbb{E}_{\mathcal{P}_{\rho}(s,a;\bar{P}_{h}^{k})}\Big{[} \overline{V}_{h+1}^{k}\Big{]}-\mathbb{E}_{\mathcal{P}_{\rho}(s,a;P_{h}^{*})} \Big{[}\overline{V}_{h+1}^{k}\Big{]}+\mathbb{E}_{\mathcal{P}_{\rho}(s,a;P_{h} ^{*})}\Big{[}\underline{V}_{h+1}^{k}\Big{]}-\mathbb{E}_{\mathcal{P}_{\rho}(s,a ;\bar{P}_{h}^{k})}\Big{[}\underline{V}_{h+1}^{k}\Big{]}\] \[\qquad\leq 2\cdot\mathsf{bonus}_{h}^{k}(s,a),\]

Proof of Lemma e.3.: See Appendix E.4 for a detailed proof. 

**Lemma E.4** (Control of the bonus term).: _Under the typical event \(\mathcal{E}\), the \(\mathsf{bonus}_{h}^{k}\) in (4.5) is bounded by_

\[\mathsf{bonus}_{h}^{k}(s,a)\] \[\qquad\leq\sqrt{\frac{\mathbb{V}_{P_{h}^{*}(\cdot|s,a)}\left[V_{h+ 1}^{\mp k},\bm{\Phi}\right]\cdot c_{1}\iota}{N_{h}^{k}(s,a)\lor 1}}+\frac{4 \cdot\mathbb{E}_{P_{h}^{*}(\cdot|s,a)}\Big{[}\overline{V}_{h+1}^{k}\!-\! \underline{V}_{h+1}^{k}\Big{]}}{H}+\frac{c_{2}H^{2}S\iota}{N_{h}^{k}(s,a)\lor 1 }+\frac{1}{\sqrt{K}},\]

_where \(\iota=\log(S^{3}AH^{2}K^{3/2}/\delta)\) and \(c_{1},c_{2}>0\) are absolute constants._

Proof of Lemma e.4.: See Appendix E.5 for a detailed proof.

**Lemma E.5** (Total variance law for robust MDP with TV robust sets).: _With probability at least \(1-\delta\), the following inequality holds_

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{V}_{P^{\star}_{h}(\cdot|s^{k}_{h},a^{k}_{h}) }\Big{[}V^{\pi^{k}_{h+1}}_{h+1,P^{\star},\Phi}\Big{]}\leq c_{3}\cdot\Big{(}\min \{H,\rho^{-1}\}\cdot HK+\min\{H,\rho^{-1}\}^{3}\cdot H_{t}\Big{)}.\]

_where \(\iota=\log(S^{3}AH^{2}K^{3/2}/\delta)\) and \(c_{3}>0\) is an absolute constant._

Proof of Lemma e.5.: See Appendix E.6 for a detailed proof. 

### Proof of Lemma e.2

Proof of Lemma e.2.: We prove Lemma E.2 by induction. Suppose the conclusion (E.14) holds at step \(h+1\). For step \(h\), let's first consider the robust \(Q\) function part. Specifically, by using the robust Bellman optimal equation (Proposition 2.3) and (4.2), we have that

\[Q^{\star}_{h,P^{\star},\Phi}(s,a)-\overline{Q}^{k}_{h}(s,a)\] \[\quad\leq\max\bigg{\{}\mathbb{E}_{\mathcal{P}_{\rho}(s,a;P^{ \star}_{h})}\big{[}V^{\star}_{h+1,P^{\star},\Phi}\Big{]}-\mathbb{E}_{\mathcal{ P}_{\rho}(s,a;\widehat{P}^{k}_{h})}\Big{[}V^{k}_{h+1}\Big{]}-\mathsf{bonus}^{k}_{h}(s,a),\] \[\qquad\qquad Q^{\star}_{h,P^{\star},\Phi}(s,a)-\min\big{\{}H, \rho^{-1}\big{\}}\bigg{\}}\] \[\quad\leq\max\bigg{\{}\mathbb{E}_{\mathcal{P}_{\rho}(s,a;P^{ \star}_{h})}\Big{[}V^{\star}_{h+1,P^{\star},\Phi}\Big{]}-\mathbb{E}_{\mathcal{ P}_{\rho}(s,a;\widehat{P}^{k}_{h})}\Big{[}V^{\star}_{h+1,P^{\star},\Phi}\Big{]}- \mathsf{bonus}^{k}_{h}(s,a),\,0\bigg{\}},\] (E.15)

where the second inequality follows from the induction of \(V^{\star}_{h+1,P^{\star},\Phi}\leq\overline{V}^{k}_{h+1}\) at step \(h+1\) and the fact that \(Q^{\star}_{h,P^{\star},\Phi}\leq\min\{H,\rho^{-1}\}\) (by Proposition 2.6 and Assumption 4.1). By Lemma E.7, we have that

\[\mathbb{E}_{\mathcal{P}_{\rho}(s,a;P^{\star}_{h})}\Big{[}V^{ \star}_{h+1,P^{\star},\Phi}\Big{]}-\mathbb{E}_{\mathcal{P}_{\rho}(s,a;\widehat {P}^{k}_{h})}\Big{[}V^{\star}_{h+1,P^{\star},\Phi}\Big{]}\] \[\qquad\leq\sqrt{\frac{\mathbb{V}_{\widehat{P}^{k}_{h}(\cdot|s,a) }\Big{[}V^{\star}_{h+1,P^{\star},\Phi}\Big{]}\cdot c_{1}\iota}{N^{k}_{h}(s,a) \lor 1}+\frac{c_{2}H\iota}{N^{k}_{h}(s,a)\lor 1}+\frac{1}{\sqrt{K}}},\]

Now by further applying Lemma E.11 to the variance term in the above inequality, we can obtain that

\[\mathbb{E}_{\mathcal{P}_{\rho}(s,a;P^{\star}_{h})}\Big{[}V^{\star }_{h+1,P^{\star},\Phi}\Big{]}-\mathbb{E}_{\mathcal{P}_{\rho}(s,a;\widehat{P}^ {k}_{h})}\Big{[}V^{\star}_{h+1,P^{\star},\Phi}\Big{]}\] \[\qquad\leq\sqrt{\frac{\mathbb{V}_{\widehat{P}^{k}_{h}(\cdot|s,a) }\Big{[}\Big{(}\overline{V}^{k}_{h+1}+\underline{V}^{k}_{h+1}\Big{)}/2\Big{]}+ 4H\cdot\mathbb{E}_{\widehat{P}^{k}_{h}(\cdot|s,a)}\Big{[}\overline{V}^{k}_{h+ 1}-\underline{V}^{k}_{h+1}\Big{]}\Big{)}\cdot c_{1}\iota}{N^{k}_{h}(s,a)\lor 1}\] \[\qquad\qquad+\frac{c_{2}H\iota}{N^{k}_{h}(s,a)\lor 1}+\frac{1}{ \sqrt{K}}\] \[\qquad\leq\sqrt{\frac{\mathbb{V}_{\widehat{P}^{k}_{h}(\cdot|s,a) }\Big{[}\Big{(}\overline{V}^{k}_{h+1}+\underline{V}^{k}_{h+1}\Big{)}/2\Big{]} \cdot c_{1}\iota}{N^{k}_{h}(s,a)\lor 1}}+\frac{\mathbb{E}_{\widehat{P}^{k}_{h}(\cdot|s,a) }\Big{[}\overline{V}^{k}_{h+1}-\underline{V}^{k}_{h+1}\Big{]}}{H}\] \[\qquad\qquad+\frac{c_{2}^{\prime}H^{2}\iota}{N^{k}_{h}(s,a)\lor 1}+ \frac{1}{\sqrt{K}},\] (E.16)where the first inequality is due to Lemma E.11, the second inequality is due to \(\sqrt{a+b}\leq\sqrt{a}+\sqrt{b}\), and the last inequality is from \(\sqrt{ab}\leq a+b\) where \(c^{\prime}_{2}>0\) is an absolute constant. Therefore, combining (E.15) and (E.16), and the choice of \(\mathsf{bonus}^{k}_{h}(s,a)\) in (4.5), we can conclude that

\[Q^{\star}_{h,P^{\star},\boldsymbol{\Phi}}(s,a)\leq\overline{Q}^{k}_{h}(s,a).\]

Furthermore, it holds that \(Q^{\pi^{k}}_{h,P^{\star},\boldsymbol{\Phi}}(s,a)\leq Q^{\star}_{h,P^{\star}, \boldsymbol{\Phi}}(s,a)\). Thus it reduces to prove \(\underline{Q}^{k}_{h}(s,a)\leq Q^{\pi^{k}}_{h,P^{\star},\boldsymbol{\Phi}}(s,a)\). Again, by using the robust Bellman equation (Proposition 2.2) and (4.3), we have that

\[\underline{Q}^{k}_{h}(s,a)-Q^{\pi^{k}}_{h,P^{\star},\boldsymbol{ \Phi}}(s,a)\] \[\qquad\leq\max\bigg{\{}\mathbb{E}_{\mathcal{P}_{\rho}(s,a; \widehat{P}^{k}_{h})}\Big{[}\underline{V}^{k}_{h+1}\Big{]}-\mathbb{E}_{ \mathcal{P}_{\rho}(s,a;P^{\star}_{h})}\Big{[}V^{\pi^{k}}_{h+1,P^{\star}, \boldsymbol{\Phi}}\Big{]}-\mathsf{bonus}^{k}_{h}(s,a),\] \[\qquad\qquad 0-Q^{\pi^{k}}_{h,P^{\star},\boldsymbol{\Phi}}(s,a) \bigg{\}}\] \[\qquad\leq\max\bigg{\{}\mathbb{E}_{\mathcal{P}_{\rho}(s,a; \widehat{P}^{k}_{h})}\Big{[}V^{\pi^{k}}_{h+1,P^{\star},\boldsymbol{\Phi}} \Big{]}-\mathbb{E}_{\mathcal{P}_{\rho}(s,a;P^{\star}_{h})}\Big{[}V^{\pi^{k}}_ {h+1,P^{\star},\boldsymbol{\Phi}}\Big{]}-\mathsf{bonus}^{k}_{h}(s,a),\,0\bigg{\}},\] (E.17)

where the second inequality follows from the induction of \(\underline{V}^{k}_{h+1}\leq V^{\pi^{k}}_{h+1,P^{\star},\boldsymbol{\Phi}}\) at step \(h+1\) and the fact that \(Q^{\pi^{k}}_{h,P^{\star},\boldsymbol{\Phi}}\geq 0\). By Lemma E.8, we have that

\[\mathbb{E}_{\mathcal{P}_{\rho}(s,a;\widehat{P}^{k}_{h})}\Big{[}V ^{\pi^{k}}_{h+1,P^{\star},\boldsymbol{\Phi}}\Big{]}-\mathbb{E}_{\mathcal{P}_ {\rho}(s,a;P^{\star}_{h})}\Big{[}V^{\pi^{k}}_{h+1,P^{\star},\boldsymbol{\Phi}} \Big{]}\] \[\qquad\leq\sqrt{\frac{\mathbb{V}_{\widehat{P}^{k}_{h}(\cdot|s,a)} \Big{[}\overline{V}^{k}_{h+1,P^{\star},\boldsymbol{\Phi}}\Big{]}\cdot c_{1} \iota}{N^{k}_{h}(s,a)\lor 1}}+\frac{\mathbb{E}_{\widehat{P}^{k}_{h}(\cdot|s,a)} \Big{[}\overline{V}^{k}_{h+1}-\underline{V}^{k}_{h+1}\Big{]}}{H}+\frac{c^{ \prime}_{2}H^{2}S_{\iota}}{N^{k}_{h}(s,a)\lor 1}+\frac{1}{\sqrt{K}}.\]

Now by applying Lemma E.11 to the variance term, with an argument similar to (E.16), we can obtain that

\[\mathbb{E}_{\mathcal{P}_{\rho}(s,a;\widehat{P}^{k}_{h})}\Big{[}V ^{\pi^{k}}_{h+1,P^{\star},\boldsymbol{\Phi}}\Big{]}-\mathbb{E}_{\mathcal{P}_ {\rho}(s,a;P^{\star}_{h})}\Big{[}V^{\pi^{k}}_{h+1,P^{\star},\boldsymbol{\Phi}} \Big{]}\] (E.18) \[\qquad\leq\sqrt{\frac{\mathbb{V}_{\widehat{P}^{k}_{h}(\cdot|s,a)} \Big{[}\overline{\Big{(}\overline{V}^{k}_{h+1}+\underline{V}^{k}_{h+1}\Big{)} /2\Big{]}\cdot c_{1}\iota}}{N^{k}_{h}(s,a)\lor 1}}+\frac{2\mathbb{E}_{\widehat{P}^{k}_{h}( \cdot|s,a)}\Big{[}\overline{V}^{k}_{h+1}-\underline{V}^{k}_{h+1}\Big{]}}{H}\] \[\qquad\qquad\qquad+\frac{c^{\prime\prime}_{2}H^{2}\iota}{N^{k}_{ h}(s,a)\lor 1}+\frac{1}{\sqrt{K}},\]

Thus by combining (E.17) and (E.18), and the choice of \(\mathsf{bonus}^{k}_{h}(s,a)\) in (4.5), we can conclude that

\[\underline{Q}^{k}_{h}(s,a)\leq Q^{\pi^{k}}_{h,P^{\star},\boldsymbol{\Phi}}(s,a).\]

Therefore, we have proved that at step \(h\), it holds that

\[\underline{Q}^{k}_{h}(s,a)\leq Q^{\pi^{k}}_{h,P^{\star},\boldsymbol{\Phi}}(s,a )\leq Q^{*}_{h,P^{\star},\boldsymbol{\Phi}}(s,a)\leq\overline{Q}^{k}_{h}(s,a).\]

Finally for the robust \(V\) function part, consider that by robust Bellman equation (Proposition 2.2) and (4.4),

\[\underline{V}^{k}_{h}(s)=\mathbb{E}_{\pi^{k}_{h}(\cdot|s)}\Big{[}\underline{Q }^{k}_{h}(s,\cdot)\Big{]}\leq\mathbb{E}_{\pi^{k}_{h}(\cdot|s)}\Big{[}Q^{\pi^{k }}_{h,P^{\star},\boldsymbol{\Phi}}(s,\cdot)\Big{]}=V^{\pi^{k}}_{h,P^{\star}, \boldsymbol{\Phi}}(s),\]

and that by robust Bellman optimal equation (Proposition 2.3), the choice of \(\pi^{k}\), and (4.4),

\[V^{\star}_{h,P^{\star},\boldsymbol{\Phi}}(s)=\max_{a\in\mathcal{A}}Q^{\star}_{h,P^{\star},\boldsymbol{\Phi}}(s,a)\leq\max_{a\in\mathcal{A}}\overline{Q}^{k}_{h }(s,a)=\overline{V}^{k}_{h}(s),\]

which proves that

\[\underline{V}^{k}_{h}(s)\leq V^{\pi^{k}}_{h,P^{\star},\boldsymbol{\Phi}}(s) \leq V^{\star}_{h,P^{\star},\boldsymbol{\Phi}}(s)\leq\overline{V}^{k}_{h}(s).\]

Since the conclusion (E.14) holds for the \(V\) function part at step \(H+1\), an induction proves Lemma E.2.

### Proof of Lemma e.3

Proof of Lemma e.3.: We upper bound the differences by a concentration inequality Lemma e.9,

\[\mathbb{E}_{\mathcal{P}_{\rho}(s,a;\widehat{P}_{h}^{k})}\Big{[} \overline{V}_{h+1}^{k}\Big{]}-\mathbb{E}_{\mathcal{P}_{\rho}(s,a;P_{h}^{*})} \Big{[}\overline{V}_{h+1}^{k}\Big{]}+\mathbb{E}_{\mathcal{P}_{\rho}(s,a; \widehat{P}_{h}^{k})}\Big{[}\underline{V}_{h+1}^{k}\Big{]}-\mathbb{E}_{ \mathcal{P}_{\rho}(s,a;P_{h}^{*})}\Big{[}\underline{V}_{h+1}^{k}\Big{]}\] \[\qquad\leq 2\sqrt{\frac{\mathbb{V}_{\widehat{P}_{h}^{k}(\cdot|s,a)} \Big{[}V_{h+1,P^{*},\boldsymbol{\Phi}}^{*}\Big{]}\cdot c_{1}\iota}{N_{h}^{k}(s,a )\lor 1}}+\frac{2\cdot\mathbb{E}_{\widehat{P}_{h}^{k}(\cdot|s,a)}\Big{[} \overline{V}_{h+1}^{k}-\underline{V}_{h+1}^{k}\Big{]}}{H}\] \[\qquad\qquad+\frac{2c_{2}^{\prime}H^{2}S_{\iota}}{N_{h}^{k}(s,a )\lor 1}+\frac{2}{\sqrt{K}},\] (E.19)

where \(c_{1},c_{2}^{\prime}>0\) are absolute constants. Then applying Lemma e.11 to the variance term in (E.19), with an argument the same as (E.16) in the proof of Lemma e.2, we can obtain that

\[\mathbb{E}_{\mathcal{P}_{\rho}(s,a;\widehat{P}_{h}^{k})}\Big{[} \overline{V}_{h+1}^{k}\Big{]}-\mathbb{E}_{\mathcal{P}_{\rho}(s,a;P_{h}^{*})} \Big{[}\overline{V}_{h+1}^{k}\Big{]}+\mathbb{E}_{\mathcal{P}_{\rho}(s,a; \widehat{P}_{h}^{k})}\Big{[}\underline{V}_{h+1}^{k}\Big{]}-\mathbb{E}_{ \mathcal{P}_{\rho}(s,a;P_{h}^{*})}\Big{[}\underline{V}_{h+1}^{k}\Big{]}\] \[\qquad\qquad+\frac{2c_{2}^{\prime\prime}H^{2}\iota}{N_{h}^{k}(s,a )\lor 1}+\frac{2}{\sqrt{K}}.\]

Therefore, by looking into the choice of \(\mathsf{bonus}_{h}^{k}(s,a)\) in (4.5), we can conclude that

\[\mathbb{E}_{\mathcal{P}_{\rho}(s,a;\widehat{P}_{h}^{k})}\Big{[} \overline{V}_{h+1}^{k}\Big{]}-\mathbb{E}_{\mathcal{P}_{\rho}(s,a;P_{h}^{*})} \Big{[}\overline{V}_{h+1}^{k}\Big{]}+\mathbb{E}_{\mathcal{P}_{\rho}(s,a; \widehat{P}_{h}^{k})}\Big{[}\underline{V}_{h+1}^{k}\Big{]}-\mathbb{E}_{ \mathcal{P}_{\rho}(s,a;P_{h}^{*})}\Big{[}\underline{V}_{h+1}^{k}\Big{]}\] \[\qquad\leq 2\cdot\mathsf{bonus}_{h}^{k}(s,a),\]

This finishes the proof of Lemma e.3. 

### Proof of Lemma e.4

Proof of Lemma e.4.: Recall that the \(\mathsf{bonus}_{h}^{k}(s,a)\) is defined as

\[\mathsf{bonus}_{h}^{k}(s,a)=\sqrt{\frac{\mathbb{V}_{\widehat{P}_{h }^{k}(\cdot|s,a)}\Big{[}\Big{(}\overline{V}_{h+1}^{k}+\underline{V}_{h+1}^{k }\Big{)}/2\Big{]}\cdot c_{1}\iota}{N_{h}^{k}(s,a)\lor 1}}\] \[\qquad\qquad+\frac{2\mathbb{E}_{\widehat{P}_{h}^{k}(\cdot|s,a)} \Big{[}\overline{V}_{h+1}^{k}-\underline{V}_{h+1}^{k}\Big{]}}{H}+\frac{c_{2}H ^{2}S_{\iota}}{N_{h}^{k}(s,a)\lor 1}+\frac{1}{\sqrt{K}}.\]

The main thing we need to consider is to control the first term and the second term. We first deal with the second term of \(\mathsf{bonus}_{h}^{k}(s,a)\) by invoking Lemma e.10, which gives

\[\frac{2\mathbb{E}_{\widehat{P}_{h}^{k}(\cdot|s,a)}\Big{[}\overline {V}_{h+1}^{k}-\underline{V}_{h+1}^{k}\Big{]}}{H} \leq\left(\frac{2}{H}+\frac{2}{H^{2}}\right)\cdot\mathbb{E}_{P_{ h}^{*}(\cdot|s,a)}\Big{[}\overline{V}_{h+1}^{k}-\underline{V}_{h+1}^{k}\Big{]}+ \frac{c_{2}^{\prime}HS_{\iota}}{N_{h}^{k}(s,a)\lor 1}\] \[\leq\frac{3\mathbb{E}_{P_{h}^{*}(\cdot|s,a)}\Big{[}\overline{V}_ {h+1}^{k}-\underline{V}_{h+1}^{k}\Big{]}}{H}+\frac{c_{2}^{\prime}HS_{\iota}}{ N_{h}^{k}(s,a)\lor 1},\] (E.20)where the second inequality is from \(H\geq 2\). Then we deal with the first term (variance term) of \(\mathsf{bonus}_{h}^{k}(s,a)\) by invoking Lemma E.12, which gives

\[\sqrt{\frac{\mathbb{V}_{P_{h}^{*}(\cdot|s,a)}\left[\left(\overline{ V}_{h+1}^{k}+\underline{V}_{h+1}^{k}\right)/2\right]\cdot c_{1}\iota}{N_{h}^{k}(s,a) \lor 1}}\] (E.21) \[\qquad\leq\sqrt{\frac{\left(\mathbb{V}_{P_{h}^{*}(\cdot|s,a)} \left[\overline{V}_{h+1,P^{*},\boldsymbol{\Phi}}^{\pi^{k}}\right]+4H\cdot \mathbb{E}_{P_{h}^{*}(\cdot|s,a)}\left[\overline{V}_{h+1}^{k}-\underline{V}_{h +1}^{k}\right]+\frac{c_{2}^{\prime\prime}H^{4}S_{\iota}}{N_{h}^{k}(s,a)\lor 1 }+1\right)\cdot c_{1}\iota}{N_{h}^{k}(s,a)}}\] \[\qquad\leq\sqrt{\frac{\mathbb{V}_{P_{h}^{*}(\cdot|s,a)}\left[ \overline{V}_{h+1,P^{*},\boldsymbol{\Phi}}^{\pi^{k}}\right]\cdot c_{1}\iota} {N_{h}^{k}(s,a)\lor 1}}+\sqrt{\frac{4H\cdot\mathbb{E}_{P_{h}^{*}(\cdot|s,a)} \left[\overline{V}_{h+1}^{k}-\underline{V}_{h+1}^{k}\right]\cdot c_{1}\iota} {N_{h}^{k}(s,a)\lor 1}}\] \[\qquad\qquad+\frac{\sqrt{c_{1}c_{2}^{\prime\prime}}SH^{2}\iota}{N _{h}^{k}(s,a)\lor 1}+\sqrt{\frac{c_{1}\iota}{N_{h}^{k}(s,a)\lor 1}}\] \[\qquad\leq\sqrt{\frac{\mathbb{V}_{P_{h}^{*}(\cdot|s,a)}\left[ \overline{V}_{h+1,P^{*},\boldsymbol{\Phi}}^{\pi^{k}}\right]\cdot c_{1}^{ \prime}\iota}{N_{h}^{k}(s,a)\lor 1}}+\frac{\mathbb{E}_{P_{h}^{*}(\cdot|s,a)} \left[\overline{V}_{h+1}^{k}-\underline{V}_{h+1}^{k}\right]}{H}+\frac{\left(4 c_{1}+\sqrt{c_{1}c_{2}^{\prime\prime}}S\right)H^{2}\iota}{N_{h}^{k}(s,a)\lor 1}\]

Thus by combining (E.20) and (E.21) with the choice of \(\mathsf{bonus}_{h}^{k}\), we can conclude the proof of Lemma E.4. 

### Proof of Lemma e.5

Proof of Lemma e.5.: The key idea is to relate the visitation distribution (w.r.t. \(P^{\star}\)) and the variance (w.r.t. \(P^{\star}\)) to the value function of \(\pi^{k}\), after which we can derive an upper bound for the total variance. Throughout this proof, we use the shorthand that

\[\overline{H}=\min\big{\{}H,\rho^{-1}\big{\}}.\]

According to Proposition 2.6 and Assumption 4.1, for any policy \(\pi\) and any step \(h\), the robust value function of \(\pi\) holds that

\[\max_{s\in\mathcal{S}}V_{h,P^{\star},\boldsymbol{\Phi}}^{\pi}(s)\leq\overline {H},\] (E.22)

which we usually apply in the sequel. Also, to facilitate our analysis, we define

\[\widetilde{T}_{h}^{k}(\cdot|s,a)=\operatorname*{argmin}_{P(\cdot)\in\mathcal{ P}_{h}(s,a;P_{h}^{*})}\mathbb{E}_{P(\cdot)}\left[V_{h+1,P^{*},\boldsymbol{\Phi}}^{ \pi^{k}}\right],\quad\forall(s,a,h)\in\mathcal{S}\times\mathcal{A}\times[H],\]

and set \(\widetilde{T}^{k}=\{\widetilde{T}_{h}^{k}\}_{h=1}^{H}\), which is the most adversarial transition for the true robust value function of \(\pi^{k}\).

Now consider the following decomposition of our target,

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{V}_{P_{h}^{*}(\cdot|s_{h}^{k},a_{h}^{k})}\Big{[}V_{h+1,P^{*},\boldsymbol{\Phi}}^{\pi^{k}}\Big{]}\] \[\qquad\qquad+\sum_{k=1}^{K}\mathbb{E}_{(s_{h}^{k},a_{h}^{k})\sim(P ^{\star},\pi^{k})}\left[\sum_{h=1}^{H}\mathbb{V}_{P_{h}^{*}(\cdot|s_{h}^{k},a_ {h}^{k})}\Big{[}V_{h+1,P^{*},\boldsymbol{\Phi}}^{\pi^{k}}\Big{]}\right]\] \[\qquad=\underbrace{\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{V}_{P_{h}^ {*}(\cdot|s_{h}^{k},a_{h}^{k})}\Big{[}V_{h+1,P^{*},\boldsymbol{\Phi}}^{\pi^{k} }\Big{]}-\mathbb{E}_{(s_{h}^{k},a_{h}^{k})\sim(P^{\star},\pi^{k})}\left[\sum_{ h=1}^{H}\mathbb{V}_{P_{h}^{*}(\cdot|s_{h}^{k},a_{h}^{k})}\Big{[}V_{h+1,P^{*}, \boldsymbol{\Phi}}^{\pi^{k}}\Big{]}\right]\]

[MISSING_PAGE_FAIL:37]

\[\leq\mathbb{E}_{\widetilde{T}_{h}^{k}(\cdot|s_{h},a_{h})}\left[ \left(V_{h+1,P^{*},\Phi}^{\pi^{k}}\right)\right]^{2}-\left(\mathbb{E}_{ \widetilde{T}_{h}(\cdot|s_{h},a_{h})}\left[V_{h+1,P^{*},\Phi}^{\pi}\right] \right)^{2}.\] (E.31)By robust Bellman equation (Proposition 2.2) and the definition of \(\tilde{T}_{h}\) in (E.24), we have that

\[V^{\pi}_{h,P^{*},\boldsymbol{\Phi}}(s_{h})=R_{h}(s_{h},\pi_{h}(s_{h}))+\mathbb{E} _{\widetilde{T}_{h}(\cdot|s_{h},\pi_{h}(s_{h}))}\Big{[}V^{\pi}_{h+1,P^{*}, \boldsymbol{\Phi}}\Big{]}.\] (E.32)

Therefore, by (E.31) and (E.32), we have that

\[\mathbb{V}_{\widetilde{T}_{h}(\cdot|s_{h},\pi_{h}(s_{h}))}\Big{[}V ^{\pi}_{h+1,P^{*},\boldsymbol{\Phi}}\Big{]}\] \[\qquad=\mathbb{E}_{\widetilde{T}_{h}(\cdot|s_{h},\pi_{h}(s_{h})) }\,\Big{[}\big{(}V^{\pi}_{h+1,P^{*},\boldsymbol{\Phi}}\big{)}^{2}\Big{]}-\Big{(} V^{\pi}_{h,P^{*},\boldsymbol{\Phi}}(s_{h})-R_{h}(s_{h},\pi_{h}(s_{h}))\Big{)}^{2}.\] (E.33)

For the second term in (E.33), we can calculate it as

\[-\Big{(}V^{\pi}_{h,P^{*},\boldsymbol{\Phi}}(s_{h})-R_{h}(s_{h}, \pi_{h}(s_{h}))\Big{)}^{2}\] \[\qquad=-\left(V^{\pi}_{h,P^{*},\boldsymbol{\Phi}}\right)^{2}(s_{h })+2\cdot V^{\pi}_{h,P^{*},\boldsymbol{\Phi}}(s_{h})\cdot R_{h}(s_{h},\pi_{h}( s_{h}))-R^{2}_{h}(s_{h},\pi_{h}(s_{h}))\] \[\qquad\leq-\left(V^{\pi}_{h,P^{*},\boldsymbol{\Phi}}\right)^{2}( s_{h})+2\overline{H},\] (E.34)

where the last inequality utilizes the facts that \(0\leq R_{h}(s_{h},\pi_{h}(s_{h}))\leq 1\), \(R^{2}_{h}(s_{h},\pi_{h}(s_{h}))\geq 0\), and (E.22) that \(V^{\pi}_{h,P^{*},\boldsymbol{\Phi}}(s_{h})\leq\overline{H}\). Combining (E.33) and (E.34), we have that

\[\mathbb{V}_{\widetilde{T}_{h}(\cdot|s_{h},\pi_{h}(s_{h}))}\Big{[}V^{\pi}_{h+1,P^{*},\boldsymbol{\Phi}}\Big{]}\leq\mathbb{E}_{\widetilde{T}_{h}(\cdot|s_{h},\pi_{h}(s_{h}))}\,\Big{[}\big{(}V^{\pi}_{h+1,P^{*},\boldsymbol{\Phi}}\big{)} ^{2}\Big{]}-\big{(}V^{\pi}_{h,P^{*},\boldsymbol{\Phi}}\big{)}^{2}\,(s_{h})+2 \overline{H},\]

which further implies that

\[\mathbb{E}_{(s_{h},a_{h})\sim(\widetilde{T},\pi)}\bigg{[}\mathbb{ V}_{\widetilde{T}_{h}(\cdot|s_{h},a_{h})}\Big{[}V^{\pi}_{h+1,P^{*},\boldsymbol{ \Phi}}\Big{]}\bigg{]}\] \[\qquad=\mathbb{E}_{s_{h}\sim(\widetilde{T},\pi)}\bigg{[}\mathbb{ V}_{\widetilde{T}_{h}(\cdot|s_{h},\pi_{h}(s_{h}))}\Big{[}V^{\pi}_{h+1,P^{*}, \boldsymbol{\Phi}}\Big{]}\bigg{]}\] \[\qquad\leq\mathbb{E}_{s_{h}\sim(\widetilde{T},\pi)}\bigg{[} \mathbb{E}_{\widetilde{T}_{h}(\cdot|s_{h},\pi_{h}(s_{h}))}\,\Big{[}\big{(}V^{ \pi}_{h+1,P^{*},\boldsymbol{\Phi}}\big{)}^{2}\Big{]}-\big{(}V^{\pi}_{h,P^{*}, \boldsymbol{\Phi}}\big{)}^{2}+2\overline{H}\bigg{]}\] \[\qquad=\mathbb{E}_{s_{h+1}\sim(\widetilde{T},\pi)}\,\Big{[}\big{(} V^{\pi}_{h+1,P^{*},\boldsymbol{\Phi}}\big{)}^{2}\Big{]}-\mathbb{E}_{s_{h}\sim( \widetilde{T},\pi)}\,\Big{[}\big{(}V^{\pi}_{h,P^{*},\boldsymbol{\Phi}}\big{)}^ {2}\Big{]}+2\overline{H}.\]

Taking summation over \(h\in[H]\) gives that

\[\mathbb{E}_{(s_{h},a_{h})\sim(\widetilde{T},\pi),h\in[H]}\left[\sum_{h=1}^{H} \mathbb{V}_{\widetilde{T}_{h}(\cdot|s_{h},a_{h})}\Big{[}V^{\pi}_{h+1,P^{*}, \boldsymbol{\Phi}}\Big{]}\right]\leq 2H\cdot\overline{H}=2H\cdot\min\big{\{}H,\rho^{-1} \big{\}},\]

which concludes the proof of Lemma E.6. 

### Other Technical Lemmas

Before presenting all lemmas, we recall that the typical event \(\mathcal{E}\) is defined as

\[\mathcal{E}=\Bigg{\{}\bigg{|}\,\Big{(}\mathbb{E}_{P^{*}_{h}(\cdot |s,a)}-\mathbb{E}_{\widehat{P}^{*}_{h}(\cdot|s,a)}\Big{)}\,\Big{[}\big{(}\eta-V^{ *}_{h+1,P^{*},\boldsymbol{\Phi}}\big{)}_{+}\Big{]}\bigg{|}\] \[\qquad\qquad\leq\sqrt{\frac{\mathbb{V}_{\widehat{P}^{*}_{h}(\cdot |s,a)}\Big{[}\big{(}\eta-V^{*}_{h+1,P^{*},\boldsymbol{\Phi}}\big{)}_{+}\Big{]} \cdot c_{1}\iota}{N^{k}_{h}(s,a)\lor 1}}+\frac{c_{2}H\iota}{N^{k}_{h}(s,a)\lor 1},\] \[\Big{|}P^{*}_{h}(s^{\prime}|s,a)-\widehat{P}_{h}(s^{\prime}|s,a) \Big{|}\leq\sqrt{\frac{\min\Big{\{}P^{*}_{h}(s^{\prime}|s,a),\widehat{P}^{*}_{h}( s^{\prime}|s,a)\Big{\}}\cdot c_{1}\iota}{N^{k}_{h}(s,a)\lor 1}}+\frac{c_{2}\iota}{N^{k}_{h}(s,a)\lor 1},\] \[\forall(s,a,s^{\prime},h,k)\in\mathcal{S}\times\mathcal{A}\times \mathcal{S}\times[H]\times[K],\ \forall\eta\in\mathcal{N}_{1/(S\sqrt{K})}\big{(}[0,H]\big{)}\Bigg{\}},\] (E.35)

where \(\iota=\log(S^{3}AH^{2}K^{3/2}/\delta)\), \(c_{1},c_{2}>0\) are two absolute constants, \(\mathcal{N}_{1/S\sqrt{K}}([0,H])\) denotes an \(1/S\sqrt{K}\)-cover of the interval \([0,H]\). where \(c_{1},c_{2}>0\) are two absolute constants, \(\mathcal{N}_{1/S\sqrt{K}}([0,H])\) denotes an \(1/S\sqrt{K}\)-cover of the interval \([0,H]\).

#### e.8.1 Concentration Inequalities

**Lemma E.7** (Bernstein bound for TV robust sets and the optimal robust value function).: _Under event \(\mathcal{E}\) in (E.35), it holds that_

\[\left|\mathbb{E}_{\mathcal{P}_{\rho}(s,a;\widehat{P}_{h}^{k})} \Big{[}V_{h+1,P^{*},\Phi}^{\star}\Big{]}-\mathbb{E}_{\mathcal{P}_{\rho}(s,a;P_ {h}^{*})}\Big{[}V_{h+1,P^{*},\Phi}^{\star}\Big{]}\right|\] \[\qquad\leq\sqrt{\frac{\mathbb{V}_{\bar{P}_{h}^{k}(\cdot|s,a)} \Big{[}V_{h+1,P^{*},\Phi}^{\star}\Big{]}\cdot c_{1}\iota}{N_{h}^{k}(s,a)\lor 1 }}+\frac{c_{2}H\iota}{N_{h}^{k}(s,a)\lor 1}+\frac{1}{\sqrt{K}},\]

_where \(\iota=\log(S^{3}AH^{2}K^{3/2}/\delta)\)._

Proof of Lemma E.7.: By our definition of the operator \(\mathbb{E}_{\mathcal{P}_{\rho}(s,a;\widehat{P}_{h}^{k})}[V_{h+1,P^{*},\Phi}^{ \star}]\) in (4.1), we can arrive that

\[\left|\mathbb{E}_{\mathcal{P}_{\rho}(s,a;\widehat{P}_{h}^{k})} \Big{[}V_{h+1,P^{*},\Phi}^{\star}\Big{]}-\mathbb{E}_{\mathcal{P}_{\rho}(s,a;P_ {h}^{*})}\Big{[}V_{h+1,P^{*},\Phi}^{\star}\Big{]}\right|\] \[\qquad-\sup_{\eta\in[0,H]}\bigg{\{}-\mathbb{E}_{P_{h}^{k}(\cdot |s,a)}\Big{[}(\eta-V_{h+1,P^{*},\Phi}^{\star})_{+}\Big{]}+\Big{(}1-\frac{\rho}{ 2}\Big{)}\cdot\eta\bigg{\}}\Bigg{|}\] \[\qquad\leq\sup_{\eta\in[0,H]}\bigg{\{}\bigg{|}\left(\mathbb{E}_{ \widehat{P}_{h}^{k}(\cdot|s,a)}-\mathbb{E}_{P_{h}^{*}(\cdot|s,a)}\right)\Big{[} \big{(}\eta-V_{h+1,P^{*},\Phi}^{\star}\big{)}_{+}\Big{]}\bigg{]}\bigg{\}},\] (E.36)

Now according to the first inequality of event \(\mathcal{E}\), we have that

\[\bigg{|}\left(\mathbb{E}_{P_{h}^{*}(\cdot|s,a)}-\mathbb{E}_{ \widehat{P}_{h}^{k}(\cdot|s,a)}\right)\Big{[}\big{(}\eta-V_{h+1,P^{*},\Phi}^{ \star}\big{)}_{+}\Big{]}\bigg{|}\] \[\qquad\leq\sqrt{\frac{\mathbb{V}_{\bar{P}_{h}^{k}(\cdot|s,a)} \Big{[}\big{(}\eta-V_{h+1,P^{*},\Phi}^{\star}\big{)}_{+}\Big{]}\cdot c_{1} \iota}{N_{h}^{k}(s,a)\lor 1}}+\frac{c_{2}H\iota}{N_{h}^{k}(s,a)\lor 1}\] \[\qquad\leq\sqrt{\frac{\mathbb{V}_{\bar{P}_{h}^{k}(\cdot|s,a)} \Big{[}V_{h+1,P^{*},\Phi}^{\star}\Big{]}\cdot c_{1}\iota}{N_{h}^{k}(s,a)\lor 1 }}+\frac{c_{2}H\iota}{N_{h}^{k}(s,a)\lor 1},\]

for any \(\eta\in\mathcal{N}_{1/(S\sqrt{K})}([0,H])\). Here the second inequality is because \(\mathrm{Var}[(a-X)_{+}]\leq\mathrm{Var}[X]\). Therefore, by a covering argument, for any \(\eta\in[0,H]\), it holds that

\[\bigg{|}\left(\mathbb{E}_{P_{h}^{*}(\cdot|s,a)}-\mathbb{E}_{\bar {P}_{h}^{k}(\cdot|s,a)}\right)\Big{[}\big{(}\eta-V_{h+1,P^{*},\Phi}^{\star} \big{)}_{+}\Big{]}\bigg{|}\] \[\qquad\leq\sqrt{\frac{\mathbb{V}_{\bar{P}_{h}^{k}(\cdot|s,a)} \Big{[}V_{h+1,P^{*},\Phi}^{\star}\Big{]}\cdot c_{1}\iota}{N_{h}^{k}(s,a)\lor 1 }}+\frac{c_{2}H\iota}{N_{h}^{k}(s,a)\lor 1}+\frac{1}{\sqrt{K}}.\]

This finishes the proof of Lemma E.7. 

**Lemma E.8** (Bernstein bound for TV robust sets and the robust value function of \(\pi^{k}\)).: _Under event \(\mathcal{E}\) in (E.35), suppose that the optimism and pessimism (E.14) holds at \((h+1,k)\), then it holds that_

\[\left|\mathbb{E}_{\mathcal{P}_{\rho}(s,a;\widehat{P}_{h}^{k})} \Big{[}V_{h+1,P^{*},\Phi}^{\star^{\star}}\Big{]}-\mathbb{E}_{\mathcal{P}_{\rho}(s,a;P_{h}^{*})}\Big{[}V_{h+1,P^{*},\Phi}^{\star^{\star}}\Big{]}\right|\] \[\qquad\leq\sqrt{\frac{\mathbb{V}_{\bar{P}_{h}^{k}(\cdot|s,a)} \Big{[}V_{h+1,P^{*},\Phi}^{\star}\Big{]}\cdot c_{1}\iota}{N_{h}^{k}(s,a)\lor 1 }}+\frac{\mathbb{E}_{\bar{P}_{h}^{k}(\cdot|s,a)}\Big{[}\overline{V}_{h+1}^{k} -\underline{V}_{h+1}^{k}\Big{]}}{H}+\frac{c_{2}^{\prime}H^{2}S\iota}{N_{h}^{k}( s,a)\lor 1}+\frac{1}{\sqrt{K}},\]

_where \(\iota=\log(S^{3}AH^{2}K^{3/2}/\delta)\) and \(c_{1}\), \(c_{2}^{\prime}\) are absolute constants._Proof of Lemma e.8.: By our definition of the operator \(\mathbb{E}_{\mathcal{P}_{\rho}(s,a;P)}[V^{\pi^{k}}_{h+1,P^{*},\Phi}]\) in (4.1), we can arrive that,

\[\left|\mathbb{E}_{\mathcal{P}_{\rho}(s,a;\hat{P}^{k}_{h})}\!\left[ V^{\pi^{k}}_{h+1,P^{*},\Phi}\right]-\mathbb{E}_{\mathcal{P}_{\rho}(s,a;P^{*}_{h})} \!\left[V^{\pi^{k}}_{h+1,P^{*},\Phi}\right]\right|\] \[\qquad=\left|\sup_{\eta\in[0,H]}\bigg{\{}-\mathbb{E}_{\tilde{P}^ {k}_{h}(\cdot|s,a)}\Big{[}\big{(}\eta-V^{\pi^{k}}_{h+1,P^{*},\Phi}\big{)}_{+} \Big{]}+\Big{(}1-\frac{\rho}{2}\Big{)}\cdot\eta\bigg{\}}\right.\] \[\qquad\qquad\left.-\sup_{\eta\in[0,H]}\bigg{\{}-\mathbb{E}_{P^{*} _{h}(\cdot|s,a)}\Big{[}\big{(}\eta-V^{\pi^{k}}_{h+1,P^{*},\Phi}\big{)}_{+} \Big{]}+\Big{(}1-\frac{\rho}{2}\Big{)}\cdot\eta\bigg{\}}\right|\] \[\qquad\leq\underbrace{\sup_{\eta\in[0,H]}\bigg{\{}\bigg{|}\left( \mathbb{E}_{\tilde{P}^{k}_{h}(\cdot|s,a)}-\mathbb{E}_{P^{*}_{h}(\cdot|s,a)} \right)\Big{[}\big{(}\eta-V^{\pi^{k}}_{h+1,P^{*},\Phi}\big{)}_{+}\Big{]}\bigg{|} \bigg{\}}}_{\text{Term (i)}}\]

We deal with Term (i) and Term (ii) respectively. For Term (i), this is exactly the same as the right hand side of (E.36). Therefore, applying the same argument as Lemma E.7 gives the following upper bound,

\[\text{Term (i)}\leq\sqrt{\frac{\mathbb{V}_{\tilde{P}^{k}_{h}(\cdot|s,a)} \Big{[}V^{\star}_{h+1,P^{*},\Phi}\Big{]}\cdot c_{1}\iota}{N^{k}_{h}(s,a)\lor 1 }}+\frac{c_{2}H\iota}{N^{k}_{h}(s,a)\lor 1}+\frac{1}{\sqrt{K}}.\] (E.37)

For Term (ii), we first apply the second inequality of event \(\mathcal{E}\) to obtain that,

\[\text{Term (ii)}\] (E.38) \[\qquad\cdot\Big{|}\big{(}\eta-V^{\pi^{k}}_{h+1,P^{*},\Phi}(s^{ \prime})\big{)}_{+}\!\!-\big{(}\eta-V^{\star}_{h+1,P^{*},\Phi}(s^{\prime}) \big{)}_{+}\Big{|}\,\Bigg{\}}.\]

By the assumption that (E.14) holds at \((h+1,k)\), we can upper bound the absolute value above by

\[\Big{|}\big{(}\eta-V^{\pi^{k}}_{h+1,P^{*},\Phi}(s^{\prime})\big{)} _{+}-\big{(}\eta-V^{\star}_{h+1,P^{*},\Phi}(s^{\prime})\big{)}_{+}\Big{|} \leq\Big{|}V^{\pi^{k}}_{h+1,P^{*},\Phi}(s^{\prime})-V^{\star}_{h+1,P^{*},\Phi}(s^{\prime})\Big{|}\] \[\leq\overline{V}^{k}_{h+1}(s^{\prime})-\underline{V}^{k}_{h+1}(s^ {\prime}).\] (E.39)

where the first inequality is due to the \(1\)-Lipschitz continuity of \(\psi_{\eta}(x)=(\eta-x)_{+}\), and the second inequality is due to (E.14). Thus combining (E.38) and (E.39), we know that

\[\text{Term (ii)}\leq\sum_{s^{\prime}\in\mathcal{S}}\left(\sqrt{\frac{ \widehat{P}^{k}_{h}(s^{\prime}|s,a)\cdot c_{1}\iota}{N^{k}_{h}(s,a)\lor 1}}+ \frac{c_{2}\iota}{N^{k}_{h}(s,a)\lor 1}\right)\cdot\left(\overline{V}^{k}_{h+1}(s^ {\prime})-\underline{V}^{k}_{h+1}(s^{\prime})\right).\] (E.40)

Now following the argument first identified by Azar et al. (2017), we proceed to upper bound (E.40) as

\[\text{Term (ii)}\leq\sum_{s^{\prime}\in\mathcal{S}}\left(\frac{ \widehat{P}^{k}_{h}(s^{\prime}|s,a)}{H}+\frac{c_{1}H\iota}{N^{k}_{h}(s,a)\lor 1 }+\frac{c_{2}\iota}{N^{k}_{h}(s,a)\lor 1}\right)\cdot\left(\overline{V}^{k}_{h+1}(s^ {\prime})-\underline{V}^{k}_{h+1}(s^{\prime})\right)\]\[\leq\frac{\mathbb{E}_{\tilde{P}^{k}_{h}(\cdot|s,a)}\left[\overline{V}^{k}_{h+1}- \underline{V}^{k}_{h+1}\right]}{H}+\frac{c^{\prime}_{2}H^{2}S_{\underline{ \iota}}}{N^{k}_{h}(s,a)\lor 1},\] (E.41)

where \(c^{\prime}_{2}>0\) is another absolute constant. The first inequality is by \(\sqrt{ab}\leq a+b\) and the second inequality is due to \(\overline{V}^{k}_{h+1},\underline{V}^{k}_{h+1}\in[0,H]\). Finally, combining (E.37) and (E.41), we prove Lemma E.8. 

**Lemma E.9** (Bernstein bounds for TV robust sets and optimistic and pessimistic robust value estimators).: _Under event \(\mathcal{E}\) in (E.35), suppose that the optimism and pessimism (E.14) holds at \((h+1,k)\), it holds that_

\[\max\bigg{\{} \left|\mathbb{E}_{\mathcal{P}_{\rho}(s,a;\tilde{P}^{k}_{h})} \right|\!\!\left[\overline{V}^{k}_{h+1}\right]-\mathbb{E}_{\mathcal{P}_{\rho }(s,a;\tilde{P}^{*}_{h})}\!\left[\overline{V}^{k}_{h+1}\right]\right|,\left| \mathbb{E}_{\mathcal{P}_{\rho}(s,a;\tilde{P}^{k}_{h})}\!\left[\underline{V}^ {k}_{h+1}\right]-\mathbb{E}_{\mathcal{P}_{\rho}(s,a;\tilde{P}^{*}_{h})}\! \left[\underline{V}^{k}_{h+1}\right]\right|\bigg{\}}\] \[\leq\sqrt{\frac{\mathbb{V}_{\tilde{P}^{k}_{h}(\cdot|s,a)}\left[V^ {*}_{h+1,P^{*},\boldsymbol{\Phi}}\right]\cdot c_{1}\iota}{N^{k}_{h}(s,a)\lor 1 }}+\frac{\mathbb{E}_{\tilde{P}^{k}_{h}(\cdot|s,a)}\!\left[\overline{V}^{k}_{h+ 1}-\underline{V}^{k}_{h+1}\right]}{H}+\frac{c^{\prime}_{2}H^{2}S_{\underline{ \iota}}}{N^{k}_{h}(s,a)\lor 1}+\frac{1}{\sqrt{K}},\]

_where \(\iota=\log(S^{3}AH^{2}K^{3/2}/\delta)\) and \(c_{1},c^{\prime}_{2}\) are absolute constants._

Proof of Lemma e.9.: This follows from the same proof as Lemma E.8 and is thus omitted. 

**Lemma E.10** (Non-robust concentration).: _Under event \(\mathcal{E}\) in (E.35), suppose that the optimism and pessimism (E.14) holds at \((h+1,k)\), then it holds that_

\[\bigg{|}\left(\mathbb{E}_{\tilde{P}^{k}_{h}(\cdot|s,a)}-\mathbb{E}_{P^{*}_{h} (\cdot|s,a)}\right)\left[\overline{V}^{k}_{h+1}-\underline{V}^{k}_{h+1}\right] \bigg{|}\leq\frac{1}{H}\cdot\mathbb{E}_{P^{*}_{h}(\cdot|s,a)}\!\left[ \overline{V}^{k}_{h+1}-\underline{V}^{k}_{h+1}\right]+\frac{c^{\prime}_{2}H^{2 }S_{\underline{\iota}}}{N^{k}_{h}(s,a)\lor 1}.\]

_where \(\iota=\log(S^{2}AH^{2}K^{3/2}/\delta)\) and \(c^{\prime}_{2}\) is an absolute constant._

Proof of Lemma e.10.: According to the second inequality of event \(\mathcal{E}\), we have that

\[\bigg{|}\left(\mathbb{E}_{\tilde{P}^{k}_{h}(\cdot|s,a)}-\mathbb{ E}_{P^{*}_{h}(\cdot|s,a)}\right)\left[\overline{V}^{k}_{h+1}-\underline{V}^{k}_{h+1 }\right]\bigg{|}\] \[\leq\sum_{s^{\prime}\in\mathcal{S}}\left(\sqrt{\frac{P^{*}_{h}(s^ {\prime}|s,a)\cdot c_{1}\iota}{N^{k}_{h}(s,a)\lor 1}}+\frac{c_{2}\iota}{N^{k}_{h}(s,a) \lor 1}\right)\cdot\left(\overline{V}^{k}_{h+1}(s^{\prime})-\underline{V}^{ k}_{h+1}(s^{\prime})\right),\]

where we also apply (E.14) that \(\overline{V}^{k}_{h+1}(s^{\prime})\geq\underline{V}^{k}_{h+1}(s^{\prime})\). Now using the same argument as (E.41) in the proof of Lemma E.8, we can arrive at

\[\bigg{|}\left(\mathbb{E}_{\tilde{P}^{k}_{h}(\cdot|s,a)}-\mathbb{E }_{P^{*}_{h}(\cdot|s,a)}\right)\left[\overline{V}^{k}_{h+1}-\underline{V}^{k} _{h+1}\right]\bigg{|}\] \[\leq\frac{\mathbb{E}_{P^{*}_{h}(\cdot|s,a)}\!\left[\overline{V}^ {k}_{h+1}(s^{\prime})-\underline{V}^{k}_{h+1}(s^{\prime})\right]}{H}+\frac{c^{ \prime}_{2}H^{2}S_{\underline{\iota}}}{N^{k}_{h}(s,a)\lor 1},\]

which finishes the proof of Lemma E.10. 

#### e.8.2 Variance Analysis

**Lemma E.11** (Variance analysis 1).: _Suppose that the optimism and pessimism (E.14) holds at \((h+1,k)\), then the following inequality holds,_

\[\left|\mathbb{V}_{\tilde{P}^{k}_{h}(\cdot|s,a)}\!\left[\left(\overline{V}^{k}_{h +1}+\underline{V}^{k}_{h+1}\right)\!\left/2\right]-\mathbb{V}_{\tilde{P}^{k}_{h }(\cdot|s,a)}\!\left[V^{*}_{h+1,P^{*},\boldsymbol{\Phi}}\right]\right|\leq 4H \cdot\mathbb{E}_{\tilde{P}^{k}_{h}(\cdot|s,a)}\!\left[\overline{V}^{k}_{h+1}- \underline{V}^{k}_{h+1}\right]\!.\]Proof of Lemma e.11.: Directly consider that the left hand side can be upper bounded by the following,

\[\left|\mathbb{V}_{\tilde{P}^{k}_{h}(\cdot|s,a)}\Big{[}\Big{(} \overline{V}^{k}_{h+1}+\underline{V}^{k}_{h+1}\Big{)}/2\Big{]}-\mathbb{V}_{ \tilde{P}^{k}_{h}(\cdot|s,a)}\Big{[}V^{\star}_{h+1,P^{\star},\Phi}\Big{]}\right|\] \[\qquad\leq\left|\mathbb{E}_{\tilde{P}^{k}_{h}(\cdot|s,a)}\bigg{[} \Big{(}\overline{V}^{k}_{h+1}+\underline{V}^{k}_{h+1}\Big{)}^{2}/4\bigg{]}- \mathbb{E}_{\tilde{P}^{k}_{h}(\cdot|s,a)}\bigg{[}\Big{(}V^{\star}_{h+1,P^{ \star},\Phi}\Big{)}^{2}\bigg{]}\right|\] \[\qquad\qquad\left.+\bigg{|}\left(\mathbb{E}_{\tilde{P}^{k}_{h}( \cdot|s,a)}\Big{[}\Big{(}\overline{V}^{k}_{h+1}+\underline{V}^{k}_{h+1}\Big{)} /2\Big{]}\right)^{2}-\Big{(}\mathbb{E}_{\tilde{P}^{k}_{h}(\cdot|s,a)}\Big{[} V^{\star}_{h+1,P^{\star},\Phi}\Big{]}\right)^{2}\bigg{|}.\] (E.42)

Since all of \(\overline{V}^{k}_{h+1},\underline{V}^{k}_{h+1},V^{\star}_{h+1,P^{\star},\Phi} \in[0,H]\) (by the correctness of (E.14) and the definitions of \(\overline{V}^{k}_{h+1},\underline{V}^{k}_{h+1}\)), we can further upper bound the right hand side of (E.42) as

\[\left|\mathbb{V}_{\tilde{P}^{k}_{h}(\cdot|s,a)}\Big{[}\Big{(} \overline{V}^{k}_{h+1}+\underline{V}^{k}_{h+1}\Big{)}/2\Big{]}-\mathbb{V}_{ \tilde{P}^{k}_{h}(\cdot|s,a)}\Big{[}V^{\star}_{h+1,P^{\star},\Phi}\Big{]}\right|\] \[\qquad\leq 4H\cdot\mathbb{E}_{\tilde{P}^{k}_{h}(\cdot|s,a)}\bigg{[} \Big{|}\Big{(}\overline{V}^{k}_{h+1}+\underline{V}^{k}_{h+1}\Big{)}/2-V^{\star }_{h+1,P^{\star},\Phi}\Big{]}\bigg{]}\] \[\qquad\leq 4H\cdot\mathbb{E}_{\tilde{P}^{k}_{h}(\cdot|s,a)} \bigg{[}\overline{V}^{k}_{h+1}-\underline{V}^{k}_{h+1}\Big{]},\]

where the last inequality is due to the correctness of (E.14) at \((h+1,k)\). This proves Lemma e.11. 

**Lemma e.12** (Variance analysis 2).: _Under event \(\mathcal{E}\) in (E.35), suppose that optimism and pessimism (E.14) holds at \((h+1,k)\), then it holds that_

\[\left|\mathbb{V}_{\tilde{P}^{k}_{h}(\cdot|s,a)}\Big{[}\Big{(} \overline{V}^{k}_{h+1}+\underline{V}^{k}_{h+1}\Big{)}/2\Big{]}-\mathbb{V}_{P^{ \star}_{h}(\cdot|s,a)}\Big{[}V^{\star^{k}}_{h+1,P^{\star},\Phi}\Big{]}\right|\] \[\qquad\leq 4H\cdot\mathbb{E}_{P^{\star}_{h}(\cdot|s,a)}\Big{[} \overline{V}^{k}_{h+1}-\underline{V}^{k}_{h+1}\Big{]}+\frac{c_{2}^{\prime}H^{ 4}S_{t}}{N_{h}^{k}(s,a)}+1.\]

Proof of Lemma e.12.: We first relate the variance on \(\widehat{P}^{k}_{h}\) to the variance on \(P^{\star}_{h}\). Specifically, we have

\[\left|\mathbb{V}_{\tilde{P}^{k}_{h}(\cdot|s,a)}\Big{[}\Big{(} \overline{V}^{k}_{h+1}+\underline{V}^{k}_{h+1}\Big{)}/2\Big{]}-\mathbb{V}_{P^{ \star}_{h}(\cdot|s,a)}\Big{[}\Big{(}\overline{V}^{k}_{h+1}+\underline{V}^{k}_{ h+1}\Big{)}/2\Big{]}\right|\] \[\qquad\qquad+\left|\mathbb{E}_{P^{\star}_{h}(\cdot|s,a)}\left[ \left(\Big{(}\overline{V}^{k}_{h+1}+\underline{V}^{k}_{h+1}\Big{)}/2-\mathbb{E }_{P^{\star}_{h}(\cdot|s,a)}\Big{[}\Big{(}\overline{V}^{k}_{h+1}+\underline{V }^{k}_{h+1}\Big{)}/2\Big{]}\right)^{2}\right]\right|\] (E.43)

Since \((\overline{V}^{k}_{h+1}+\underline{V}^{k}_{h+1})/2\in[0,H]\), we can further upper bound (E.43) by

\[\left|\mathbb{V}_{\tilde{P}^{k}_{h}(\cdot|s,a)}\Big{[}\Big{(} \overline{V}^{k}_{h+1}+\underline{V}^{k}_{h+1}\Big{)}/2\Big{]}-\mathbb{V}_{P^{ \star}_{h}(\cdot|s,a)}\Big{[}\Big{(}\overline{V}^{k}_{h+1}+\underline{V}^{k}_{ h+1}\Big{)}/2\Big{]}\right|\] \[\qquad\leq H^{2}\cdot\sum_{s^{\prime}\in\mathcal{S}}\Big{|}P^{ \star}_{h}(s^{\prime}|s,a)-\widehat{P}_{h}(s^{\prime}|s,a)\Big{|}\] \[\qquad\leq H^{2}\cdot\sum_{s^{\prime}\in\mathcal{S}}\left(\sqrt{ \frac{P^{\star}_{h}(\cdot|s,a)\cdot c_{1}\iota}{N_{h}^{k}(s,a)\lor 1}}+\frac{c_{2}\iota}{N_{h}^{k}(s,a) \lor 1}\right)\] \[\qquad\leq H^{2}\cdot\left(\sqrt{\frac{c_{1}S\iota}{N_{h}^{k}(s,a) \lor 1}}+\frac{c_{2}S\iota}{N_{h}^{k}(s,a)\lor 1}\right)\] \[\qquad\leq 1+\frac{c_{2}^{\prime}H^{4}S\iota}{N_{h}^{k}(s,a)\lor 1},\] (E.44)where the second inequality is by the second inequality in event \(\mathcal{E}\), the third inequality is by Cauchy-Schwartz inequality and the probability distribution sums up to \(1\), and the last inequality is from \(\sqrt{ab}\leq a+b\). Thus by (E.44), we can bound our target as

\[\bigg{|}\mathbb{V}_{\tilde{P}_{h}^{k}(\cdot|s,a)}\bigg{[}\Big{(} \overline{V}_{h+1}^{k}+\underline{V}_{h+1}^{k}\Big{)}/2\bigg{]}-\mathbb{V}_{P_{ h}^{*}(\cdot|s,a)}\Big{[}V_{h+1,P^{*},\mathbf{\Phi}}^{\pi^{k}}\Big{]}\bigg{|}\] \[\qquad\leq\bigg{|}\mathbb{V}_{P_{h}^{*}(\cdot|s,a)}\Big{[}\Big{(} \overline{V}_{h+1}^{k}+\underline{V}_{h+1}^{k}\Big{)}/2\Big{]}-\mathbb{V}_{P_ {h}^{*}(\cdot|s,a)}\Big{[}V_{h+1,P^{*},\mathbf{\Phi}}^{\pi^{k}}\Big{]}\bigg{|}\] \[\qquad\qquad\qquad+\frac{c_{2}^{\prime}H^{4}S_{t}}{N_{h}^{k}(s,a )\lor 1}+1.\] (E.45)

Now by the same proof of Lemma E.11, using the correctness of (E.14) at \((h+1,k)\), we can show that

\[\bigg{|}\mathbb{V}_{P_{h}^{*}(\cdot|s,a)}\Big{[}\Big{(}\overline{V}_{h+1}^{k} +\underline{V}_{h+1}^{k}\Big{)}/2\Big{]}-\mathbb{V}_{P_{h}^{*}(\cdot|s,a)} \Big{[}V_{h+1,P^{*},\mathbf{\Phi}}^{\pi^{k}}\Big{]}\bigg{|}\] \[\qquad\leq 4H\cdot\mathbb{E}_{P_{h}^{*}(\cdot|s,a)}\Big{[} \overline{V}_{h+1}^{k}-\underline{V}_{h+1}^{k}\Big{]}.\] (E.46)

Combining (E.45) and (E.46), we can finish the proof of Lemma E.12. 

#### e.8.3 Other Auxiliary Lemmas

**Lemma E.13** (Lemma 7.5 in Agarwal et al. (2019)).: _For the sequences of \(\{s_{h}^{k},a_{h}^{k}\}_{h,k=1}^{H,K}\), it holds that_

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\frac{1}{N_{h}^{k}(s_{h}^{k},a_{h}^{k})\lor 1} \leq c\cdot HSA\log(K).\]

_where \(c>0\) is an absolute constant._

Proof of Lemma e.13.: See Lemma 7.5 in Agarwal et al. (2019) for a detailed proof. 

## Appendix F Proofs for Extensions in Section b.4.2

In this section, we prove the theoretical results in Section B.4.2.

### Proof of Corollary b.5

Proof of Corollary b.5.: We consider applying Algorithm 1 on the auxiliary \(\mathcal{S}\times\mathcal{A}\)-rectangular RMDP with a TV robust set \(\widetilde{\mathcal{M}}\) (see Section B.4.2) which satisfies the vanishing minimal value assumption (Assumption 4.1). Suppose the algorithm outputs \(\widetilde{\pi}^{1},\cdots,\widetilde{\pi}^{K}\) for the \(K\) episodes. Then Theorem 4.3 shows that by a proper choice of the hyperparameters, with probability at least \(1-\delta\)

\[\mathrm{Regret}_{\widetilde{\mathbf{\Phi}}}(K) =\sum_{k=1}^{K}\max_{\widetilde{\pi}}V_{1,\widetilde{\mathcal{P}} ^{*},\widetilde{\mathbf{\Phi}}}^{\widetilde{\pi}^{k}}(s_{1})-V_{1,\widetilde{ \mathcal{P}}^{*},\widetilde{\mathbf{\Phi}}}^{\widetilde{\pi}^{k}}(s_{1})\] \[\leq\mathcal{O}\bigg{(}\sqrt{\min\big{\{}H,\rho^{-1}\big{\}}H^{2}( S+1)AK\iota^{\prime}}\bigg{)},.\] (F.1)

where \(\iota^{\prime}=\log^{2}(SAHK/\delta)\) and \(\rho=2-2\rho^{\prime}\in[0,1)\). In the sequel, we prove that for any policy \(\widetilde{\pi}\) of \(\widetilde{\mathcal{M}}\) and its induced policy \(\widetilde{\pi}_{\mathcal{S}}\) of \(\mathcal{M}_{\gamma}\), their robust value functions coincide at the initial state \(s_{1}\in\mathcal{S}\), that is,

\[V_{1,\widetilde{P}^{*},\widetilde{\mathbf{\Phi}}}^{\widetilde{\pi}}(s_{1})=V_{ 1,P^{*},\mathbf{\Phi}^{\prime}}^{\widetilde{\pi}_{\mathcal{S}}}(s_{1}),\]

where \(V_{1,\widetilde{P}^{*},\widetilde{\mathbf{\Phi}}}^{\widetilde{\pi}}\) is the robust value function of \(\widetilde{\pi}\) in \(\widetilde{\mathcal{M}}=(\widetilde{\mathcal{S}},\mathcal{A},H,\widetilde{P^{* }},\widetilde{R},\widetilde{\mathbf{\Phi}})\), and \(V_{1,P^{*},\mathbf{\Phi}^{\prime}}^{\widetilde{\pi}_{\mathcal{S}}}\) is the robust value function of \(\widetilde{\pi}_{\mathcal{S}}\) in \(\mathcal{M}_{\gamma}=(\mathcal{S},\mathcal{A},H,P^{\star},R_{\gamma}, \mathbf{\Phi}^{\prime})\). To this end, we actually prove a stronger result that for any step \(h\in[H]\), it holds that

\[(\rho^{\prime})^{h-1}\cdot V_{h,\widetilde{P}^{*},\widetilde{\mathbf{\Phi}}}^{ \widetilde{\pi}}(s)=V_{h,P^{*},\mathbf{\Phi}^{\prime}}^{\widetilde{\pi}_{ \mathcal{S}}}(s),\quad\forall s\in\mathcal{S}.\] (F.2)We prove (F.2) by induction. For step \(H\), by robust Bellman equation, we have that, for any \((s,a)\in\mathcal{S}\times\mathcal{A}\),

\[(\rho^{\prime})^{H-1}\cdot Q_{H,\widetilde{P}^{*},\widetilde{\Phi}}^{\widetilde {\pi}}(s,a)=(\rho^{\prime})^{H-1}\cdot\left(\frac{\gamma}{\rho^{\prime}}\right)^ {H-1}\cdot R_{H}(s,a)=R_{\gamma,H}(s,a)=Q_{H,P^{*},\widetilde{\Phi}^{\prime}}^ {\widetilde{\pi}}(s,a),\]

and thus for any \(s\in\mathcal{S}\),

\[(\rho^{\prime})^{H-1}\cdot V_{h,\widetilde{P}^{*},\widetilde{ \Phi}}^{\widetilde{\pi}}(s) =\mathbb{E}_{\widetilde{\pi}(\cdot|s)}\Big{[}(\rho^{\prime})^{H-1 }\cdot Q_{H,\widetilde{P}^{*},\widetilde{\Phi}}^{\widetilde{\pi}}(s,\cdot) \Big{]}\] \[=\mathbb{E}_{\widetilde{\pi}_{S}(\cdot|s)}\Big{[}Q_{H,P^{*}, \widetilde{\Phi}}^{\widetilde{\pi}}(s,\cdot)\Big{]}\] \[=V_{H,P^{*},\widetilde{\Phi}^{\prime}}^{\widetilde{\pi}}(s).\]

This proves (F.2) for step \(H\). Suppose that (F.2) holds at some step \(h+1\), that is,

\[(\rho^{\prime})^{h}\cdot V_{h+1,\widetilde{P}^{*},\widetilde{ \Phi}}^{\widetilde{\pi}}(s) =V_{h+1,P^{*},\widetilde{\Phi}^{\prime}}^{\widetilde{\pi}}(s), \quad\forall s\in\mathcal{S}.\] (F.3)

Then for step \(h\), by robust Bellman equation and Proposition 4.2, we have that

\[(\rho^{\prime})^{h-1}\cdot Q_{h,\widetilde{P}^{*},\widetilde{ \Phi}}^{\widetilde{\pi}}(s,a) =(\rho^{\prime})^{h-1}\cdot\left(\frac{\gamma}{\rho^{\prime}} \right)^{H-1}\cdot R_{h}(s,a)+(\rho^{\prime})^{h-1}\cdot\mathbb{E}_{ \widetilde{P}_{\rho}(s,a;\widetilde{P}_{\lambda}^{*})}\Big{[}V_{h+1, \widetilde{P}^{*},\widetilde{\Phi}}^{\widetilde{\pi}}\Big{]}\] \[=R_{\gamma,h}(s,a)+(\rho^{\prime})^{h-1}\cdot\rho^{\prime}\cdot \mathbb{E}_{\widetilde{\mathcal{B}}_{\rho}(s,a;\widetilde{P}_{\lambda}^{*})} \Big{[}V_{h+1,\widetilde{P}^{*},\widetilde{\Phi}}^{\widetilde{\pi}}\Big{]},\] (F.4)

where the last equality utilizes Proposition 4.2 since \(\min_{s\in\widetilde{\mathcal{S}}}V_{h+1,\widetilde{P}^{*},\widetilde{\Phi}}^{ \widetilde{\pi}}(s)=0\), and we adopt the notation

\[\widetilde{\mathcal{B}}_{\rho}(s,a;\widetilde{P}_{h}^{*})=\left\{\widetilde{ P}(\cdot)\in\Delta(\widetilde{\mathcal{S}}):\sup_{s^{\prime}\in\widetilde{ \mathcal{S}}}\frac{\widetilde{P}(s^{\prime})}{\widetilde{P}_{h}^{*}(s^{\prime}|s,a)}\leq\frac{1}{\rho^{\prime}}\right\}.\]

Notice that by the definition (B.3), we know for \((s,a)\in\mathcal{S}\times\mathcal{A}\) it holds that \(\widetilde{P}_{h}^{*}(\cdot|s,a)=P_{h}^{*}(\cdot|s,a)\) which is supported on \(\mathcal{S}\). Therefore, we can equivalently write

\[\widetilde{\mathcal{B}}_{\rho}(s,a;\widetilde{P}_{h}^{*}) =\left\{\widetilde{P}(\cdot)\in\Delta(\widetilde{\mathcal{S}}):\sup _{s^{\prime}\in\mathcal{S}}\frac{\widetilde{P}(s^{\prime})}{\widetilde{P}_{h}^ {*}(s^{\prime}|s,a)}\leq\frac{1}{\rho^{\prime}}\right\}\] \[=\left\{\widetilde{P}(\cdot)\in\Delta(\mathcal{S}):\sup_{s^{ \prime}\in\mathcal{S}}\frac{\widetilde{P}(s^{\prime})}{P_{h}^{*}(s^{\prime}|s,a)}\leq\frac{1}{\rho^{\prime}}\right\}\] \[=\mathcal{B}_{\rho}(s,a;P_{h}^{*}).\] (F.5)

Thus by (F.4) and (F.5) and the induction hypothesis (F.3), we obtain that for any \((s,a)\in\mathcal{S}\times\mathcal{A}\),

\[(\rho^{\prime})^{h-1}\cdot Q_{h,\widetilde{P}^{*},\widetilde{ \Phi}}^{\widetilde{\pi}}(s,a) =R_{\gamma,h}(s,a)+(\rho^{\prime})^{h}\cdot\mathbb{E}_{\mathcal{B} _{\rho}(s,a;P_{h}^{*})}\Big{[}V_{h+1,\widetilde{P}^{*},\widetilde{\Phi}}^{ \widetilde{\pi}}\Big{]}\] \[=R_{\gamma,h}(s,a)+\mathbb{E}_{\mathcal{B}_{\rho}(s,a;P_{h}^{*})} \Big{[}V_{h+1,P^{*},\Phi}^{\widetilde{\pi}}\Big{]}=Q_{h,P^{*},\widetilde{\Phi}} ^{\widetilde{\pi}_{S}}(s,a),\]

where the second equality applies (F.3) and the last equality is from robust Bellman equation. Consequently, for any \(s\in\mathcal{S}\), we have that

\[(\rho^{\prime})^{h-1}\cdot V_{h,\widetilde{P}^{*},\widetilde{ \Phi}}^{\widetilde{\pi}}(s) =\mathbb{E}_{\widetilde{\pi}(\cdot|s)}\Big{[}(\rho^{\prime})^{h-1} \cdot Q_{h,\widetilde{P}^{*},\widetilde{\Phi}}^{\widetilde{\pi}}(s,\cdot) \Big{]}\] \[=\mathbb{E}_{\widetilde{\pi}_{S}(\cdot|s)}\Big{[}Q_{h,P^{*}, \widetilde{\Phi}}^{\widetilde{\pi}_{S}}(s,\cdot)\Big{]}\] \[=V_{h,P^{*},\widetilde{\Phi}^{\prime}}^{\widetilde{\pi}_{S}}(s),\]

which finishes the induction argument, proving our claim (F.2). By taking \(h=1\), we can derive that for any initial state \(s_{1}\in\mathcal{S}\), it holds that for any policy \(\widetilde{\pi}\) of \(\widetilde{\mathcal{M}}\) and its induced policy \(\widetilde{\pi}_{\mathcal{S}}\) of \(\mathcal{M}_{\gamma}\),

\[V_{1,\widetilde{P}^{*},\widetilde{\Phi}}^{\widetilde{\pi}}(s_{1})=V_{1,P^{*}, \widetilde{\Phi}^{\prime}}^{\widetilde{\pi}_{S}}(s_{1}).\]This indicates two facts: the first is that

\[\max_{\widetilde{\pi}}V_{1,\widetilde{P}^{*},\widetilde{\Phi}}^{ \widetilde{\pi}}(s_{1})=\max_{\pi}V_{1,P^{*},\Phi^{\prime}}^{\pi}(s_{1}),\] (F.6)

where on the right hand side the maximization is with respect to all the policies for \(\mathcal{M}_{\gamma}\); the second is that

\[V_{1,\widetilde{P}^{*},\widetilde{\Phi}}^{\widetilde{\pi}^{k}}(s _{1})=V_{1,P^{*},\Phi^{\prime}}^{\widetilde{\pi}^{k}}(s_{1}),\] (F.7)

for each \(k\in[K]\), where recall that \(\widetilde{\pi}^{k}\) is the policy output by Algorithm 1 for episode \(k\). As a result, the \(k\) policies \(\{\widetilde{\pi}^{k}_{\mathcal{S}}\}_{k=1}^{K}\) of \(\mathcal{M}_{\gamma}\) during interactive data collection satisfies with probability at least \(1-\delta\),

\[\mathrm{Regret}_{\boldsymbol{\Phi}^{\prime}}(K) =\sum_{k=1}^{K}\max_{\pi}V_{1,P^{*},\boldsymbol{\Phi}^{\prime}}^ {\pi}(s_{1})-V_{1,\widetilde{P}^{*},\boldsymbol{\Phi}^{\prime}}^{\widetilde{ \pi}^{k}}(s_{1})\] \[=\sum_{k=1}^{K}\max_{\widetilde{\pi}}V_{1,\widetilde{P}^{*}, \widetilde{\Phi}}^{\widetilde{\pi}}(s_{1})-V_{1,\widetilde{P}^{*},\widetilde{ \Phi}}^{\widetilde{\pi}^{k}}(s_{1})\] \[\leq\mathcal{O}\bigg{(}\sqrt{\min\big{\{}H,(2-2\rho^{\prime})^{- 1}\big{\}}H^{2}SAK\iota^{\prime}}\,\bigg{)},\]

where in the second equality we apply the facts (F.6) and (F.7), and the last inequality follows from (F.1) and that \(\rho=2-2\rho^{\prime}\). This completes the proof of Corollary B.5.

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Sections 2, 3, 4. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Section B.5Guidelines:

* The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Sections 3, 4, and the Appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: This is a theoretical work and we do not conduct experiments. Guidelines: * The answer NA means that the paper does not include experiments.

* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: This is a theoretical work and we do not conduct experiments. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ** Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: This is a theoretical work and we do not conduct experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: This is a theoretical work and we do not conduct experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: This is a theoretical work and we do not conduct experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.

* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research in this work conforms with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: There is no societal impact of the work performed. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks.

* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.