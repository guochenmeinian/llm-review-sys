# Fast Rank-1 Lattice Targeted Sampling for Black-box Optimization

 Yueming LYU

Centre for Frontier AI Research (CFAR)

Institute of High Performance Computing (IHPC)

Agency for Science, Technology and Research (A*STAR)

1 Fusionopolis Way, #16-16 Connexis, Singapore 138632

Lyu_Yueming@cfar.a-star.edu.sg

###### Abstract

Black-box optimization has gained great attention for its success in recent applications. However, scaling up to high-dimensional problems with good query efficiency remains challenging. This paper proposes a novel Rank-1 Lattice Targeted Sampling (RLTS) technique to address this issue. Our RLTS benefits from random rank-1 lattice Quasi-Monte Carlo, which enables us to perform fast local exact Gaussian processes (GP) training and inference with \(O(n\log n)\) complexity w.r.t. \(n\) batch samples. Furthermore, we developed a fast coordinate searching method with \(O(n\log n)\) time complexity for fast targeted sampling. The fast computation enables us to plug our RLTS into the sampling phase of stochastic optimization methods. This improves the query efficiency while scaling up to higher dimensional problems than Bayesian optimization. Moreover, to construct rank-1 lattices efficiently, we proposed a closed-form construction. Extensive experiments on challenging benchmark test functions and black-box prompt fine-tuning for large language models demonstrate the query efficiency of our RLTS technique.

## 1 Introduction

Black-box optimization has gained great attention for its success in many recent applications, such as prompt fine-tuning for large language models (Sun et al., 2022; 20; 20), policy search for robot control and reinforcement learning (Choromanski et al., 2019; Lizotte et al., 2007; Barsce et al., 2017; Salimans et al., 2017), automatic hyper-parameters tuning in machine learning problems (Snoek et al., 2012), black-box architecture search in engineering design (Wang and Shan, 2007), drug discovery (Negoescu et al., 2011) and accelerated simulation for scientific discovery (Maddox et al., 2021; Hernandez-Lobato et al., 2017), etc. Many efforts have been made for black-box optimization in the literature, including Bayesian optimization (BO) methods (Srinivas et al., 2010; Gardner et al., 2017; Nayebi et al., 2019), stochastic optimization methods like evolution strategies (ES) (Back et al., 1991; Hansen, 2006; Wierstra et al., 2014; Lyu and Tsang, 2021) and genetic algorithms (Srinivas and Patnaik, 1994; Mirjalili and Mirjalili, 2019).

Bayesian optimization usually builds a global (GP) model as a surrogate and provides queries by optimizing some acquisition functions (Snoek et al., 2012). Although BO achieves good query efficiency for low-dimensional problems, it often fails to handle high-dimensional problems with large sample budgets (Eriksson et al., 2019). The computation of GP with a large number of samples itself is expensive, and the internal optimization of the acquisition functions is challenging. Recently, Muller et al. (2021); Nguyen et al. (2022) builds a GP model for both the function value and the gradient and performs local Bayesian optimization. Although these methods improve the scalability of global BO, they usually cannot scale up to five hundred dimensional complex problems. This maybe because the learned gradient heavily depends on the accuracy of the GP model. However, achieving an accurate GP model is challenging for high-dimensional problems. A slightly misspecified GP model may lead to a wrong estimated gradient due to the highly nonlinear acquisition functions.

On the other line, stochastic optimization methods, e.g., ES (Rechenberg and Eigen, 1973; Nesterov and Spokoiny, 2017), natural evolution strategies (NES) (Wierstra et al., 2014b), CMAES (Hansen, 2006), and implicit natural gradient optimizer (INGO) (Lyu and Tsang, 2021), typically sampling form Gaussian distribution and approximate the (natural) gradient for the update of the Gaussian distribution parameters for continuous optimization. These methods can scale up to higher dimensional problems compared with BO. However, the gradient approximation may have a large variance, especially for high-dimensional problems. Thus, the update direction may not be toward the descent direction, leading to inferior query efficiency.

To address high-dimensional black-box problems with good query efficiency, we propose a novel Rank-1 Lattice Targeted Sampling (RLTS) technique. Our RLTS has a \(O(n\log n)\) time complexity, which is fast for plugging into the sampling phase of stochastic optimization methods. In this way, our methods can improve the query efficiency of stochastic optimization methods while addressing higher-dimensional problems than BO. Our contributions are summarized as follows:

* We propose a novel Rank-1 Lattice Targeted Sampling (RLTS) technique. Our RLTS builds a local GP with a random rank-1 lattice, which enables fast exact GP training and inference with \(O(n\log n)\) time complexity w.r.t. \(n\) batch samples. Furthermore, we develop a fast coordinate search that enables target sampling with \(O(n\log n)\) time complexity.
* We propose a closed-form subgroup rank-1 lattice by considering the dual lattice regarding the integral approximation error of functions in Korobov space. Our rank-1 lattice has a more regular pattern of approximation error terms. Moreover, our subgroup rank-1 lattice capitalizes on constructing a circulant kernel Gram matrix benefit from its group property. This enables efficient \(O(n\log n)\) computations in GP training/inference and fast candidate searching. In contrast, low-discrepancy QMC sequences, such as Sobol sequences or Halton sequences, lack these capabilities. In addition, our new closed-form rank-1 lattice may have potential applications in downstream tasks beyond black-box optimization.
* We plug our RLTS into the sampling phase at each step of stochastic optimization methods to improve query efficiency. In this way, during the optimization procedure, our RLTS sampling from an updated promising region instead of a fixed one at each step. This approach can scale up to address high-dimensional problems.
* Empirically, extensive experiments on high-dimensional challenging benchmark test functions and practical black-box prompt fine-tuning for large language models demonstrate the effectiveness of our RLTS technique.

## 2 Background

### Black-box Optimization

Given a proper function \(f(\bm{x}):\mathbb{R}^{d}\rightarrow\mathbb{R}\) such that \(f(\bm{x})>-\infty\), black-box optimization is to minimize \(f(\bm{x})\) by using function queries only. Black-box stochastic optimization methods typically employ a sampling distribution \(p(\bm{x};\bm{\theta})\) and optimizes the parameter of the distribution regarding the relaxed problem: \(J(\bm{\theta}):=\mathbb{E}_{p(\bm{x};\bm{\theta})}[f(\bm{x})]\).

Evolution Strategies (ES) (Rechenberg and Eigen, 1973; Nesterov and Spokoiny, 2017) employ a Gaussian distribution \(\mathcal{N}(\bm{\mu},\sigma^{2}\bm{I})\) for sampling. The approximate gradient descent update is given as

\[\bm{\mu}_{t+1}=\bm{\mu}_{t}-\frac{\beta}{n\sigma}\sum_{i=1}^{n}\bm{\epsilon}_ {i}f(\bm{\mu}_{t}+\sigma\bm{\epsilon}_{i}),\] (1)

where \(\bm{\epsilon}_{i}\sim\mathcal{N}(\bm{0},\bm{I})\) and \(\beta\) denotes the step-size. The ES method performs the approximate first-order gradient descent update. As a result, the convergence of ES may be slow. Several second-order gradient descent methods have been proposed to improve convergence. Wierstra et al. (2014a) proposed the natural evolution strategies (NES), which perform the approximate natural gradient update. When a Gaussian distribution \(\mathcal{N}(\bm{\mu},\bm{\Sigma})\) is employed for sampling. The update rule of NES is given in Eq.(2) and Eq.(3):

\[\bm{\Sigma}_{t+1}=\bm{\Sigma}_{t}-\frac{\beta}{n}\sum_{i=1}^{n}f(\bm{ \mu}_{t}+\bm{\Sigma}_{t}^{\frac{1}{2}}\bm{\epsilon}_{i})\left(\bm{\Sigma}_{t}^{ \frac{1}{2}}\bm{\epsilon}_{i}\bm{\epsilon}_{i}^{\top}\bm{\Sigma}_{t}^{\frac{1}{ 2}}-\bm{\Sigma}_{t}\right)\] (2) \[\bm{\mu}_{t+1}=\bm{\mu}_{t}-\frac{\beta}{n}\sum_{i=1}^{n}f(\bm{ \mu}_{t}+\Sigma_{t}^{\frac{1}{2}}\bm{\epsilon}_{i})\Sigma_{t}^{\frac{1}{2}}\bm {\epsilon}_{i}.\] (3)

where \(\bm{\epsilon}_{i}\sim\mathcal{N}(\bm{0},\bm{I})\) and \(\bm{\Sigma}^{\frac{1}{2}}=\bm{\Sigma}^{\frac{1}{2}\top}\) and \(\bm{\Sigma}^{\frac{1}{2}}\bm{\Sigma}^{\frac{1}{2}}=\bm{\Sigma}\). The NES takes advantage of second-order gradient information, which improves the convergence of ES.

Lyu and Tsang (2021) proposed an implicit natural gradient optimizer (INGO) for black-box optimization, which provides an alternative way to compute the natural gradient update. The update rule of INGO is given as in Eq.(4) and Eq.(5):

\[\bm{\Sigma}_{t+1}^{-1}=\bm{\Sigma}_{t}^{-1}+\beta\sum_{i=1}^{n} \frac{f(\bm{x}_{i})-\widehat{\mu}}{n\widehat{\sigma}}\left(\bm{\Sigma}_{t}^{-1 }(\bm{x}_{i}-\bm{\mu}_{t})(\bm{x}_{i}-\bm{\mu}_{t})^{\top}\bm{\Sigma}_{t}^{-1 }\right)\] (4) \[\bm{\mu}_{t+1}=\bm{\mu}_{t}-\beta\sum_{i=1}^{n}\frac{f(\bm{x}_{i })-\widehat{\mu}}{n\widehat{\sigma}}(\bm{x}_{i}-\bm{\mu}_{t}).\] (5)

where \(\bm{x}_{i}\sim\mathcal{N}(\bm{\mu}_{t},\bm{\Sigma}_{t})\), \(\widehat{\mu}=\frac{\sum_{i=1}^{n}f(\bm{x}_{i})}{n}\) and \(\widehat{\sigma}\) denotes the standard deviation of \(f(\bm{x}_{i})\). The normalization \(\frac{f(\bm{x}_{i})-\widehat{\mu}}{\widehat{\sigma}}\) is employed to reduce the variance.

CMAES (Hansen, 2006) provides a more sophisticated update rule and performs well on a wide range of black-box optimization problems. All the above stochastic optimization methods rely on sampling. Thus, the sampling phase is vitally important. And a better sampling technique is promising to achieve further improvement.

### Rank-1 Lattice

A rank-1 lattice is a particular case of the general lattice with a simple operation for point-set construction. It can be used as Quasi-Monte Carlo for integral approximation (Sloan, 2000; Dick et al., 2013). A rank-1 lattice point set \(\mathcal{P}=\{\bm{x}_{1},\cdots,\bm{x}_{n}\}\) can be constructed as Eq.(6):

\[\bm{x}_{i}:=\frac{i\bm{z}\text{ mod }n}{n},i\in\{1,\cdots,n\},\] (6)

where \(\bm{z}\in\mathbb{Z}^{d}\) is the so-called generating vector, and mod denotes the modulo operation.

Korobov (1960) proposes a rank-1 lattice with the generating vector having a particular form as Eq.(7)

\[\bm{z}:=[1,k,\cdots,k^{d-1}]\text{ mod }n,\] (7)

where \(k\) is searching over \(\{1,\cdots,n-1\}\) to reduce approximation error.

Sloan and Reztsov (2002) further proposed a component-by-component searching method for the generating vector without assuming the Korobov form in Eq. (7). Recently, Lyu et al. (2020) proposed a simple closed-form subgroup-based rank-1 lattice by considering the Toroidal distance in the primal lattice space. The generating vector is given as Eq.(8)

\[\bm{z}=[g^{0},g^{\frac{n-1}{2d}},g^{\frac{2(n-1)}{2d}},\cdots,g^{\frac{(d-1)( n-1)}{2d}}]\text{ mod }n,\] (8)

where \(g\) denotes the primitive root modulo the prime number \(n\). More details of the lattice rules for numerical integration can be found in the book (Dick et al., 2022).

In this paper, we proposed a closed-form subgroup rank-1 lattice by ensuring the approximation error terms of the dual lattice have a more regular pattern. In contrast, Lyu et al. (2020) construct the rank-1 lattice evenly spaced in the primal lattice space.

## 3 Fast Rank-1 Lattice Targeted Sampling

### Random Rank-1 Lattice Quasi-Monte Carlo Gaussian Sampling

We first show how to construct random rank-1 lattice Quasi-Monte Carlo Gassuain samples. These samples enable us to perform the black-box stochastic optimization listed in section 2.1. More importantly, the nice property of the structure of these samples facilitates a fast targeted sampling.

Given a rank-1 lattice point set \(\mathcal{P}=\{\bm{x}_{1},\cdots,\bm{x}_{n}\}\), we first construct a random shifted rank-1 lattice (Dick et al., 2013) as Eq. (9),

\[\bar{\bm{x}}_{i}=\bm{x}_{i}+\bm{\Delta}\ \text{mod}\ 1\ \ \forall i\in\{1,\cdots,n\},\] (9)

where \(\bm{\Delta}\sim Uniform[0,1]^{d}\), and the mod \(1\) operation denotes a modulo operation that takes the non-negative fractional part of the input number element-wise. Then, we can construct random QMC Gaussian samples as Eq. (10)

\[\bm{\epsilon}_{i}=\Phi^{-1}(\bar{\bm{x}}_{i})\ \ \forall i\in\{1,\cdots,n\},\] (10)

where \(\Phi^{-1}(\cdot)\) computes the inverse cumulative density function of the standard Gaussian distribution w.r.t. the input element-wise. Then, the samples for Gaussian \(\mathcal{N}(\bm{\mu},\bm{\Sigma})\) can be constructed as follows:

\[\bm{\xi}_{i}=\bm{\mu}+\bm{\Sigma}^{\frac{1}{2}}\bm{\epsilon}_{i}.\] (11)

An illustration of the random QMC Gaussian samples constructed by our closed-form rank-1 lattice is shown in Figure 1. We can see that our rank-1 lattice QMC Gassuan samples are spaced more evenly w.r.t. the density.

### Fast Exact GP Training and Inference with Rank-1 Lattice

This subsection will show how to perform fast exact GP training and inference using our rank-1 lattice samples with a \(O(n\log n)\) time complexity w.r.t \(n\) samples.

Let \(\bm{K}_{\theta}\) denotes the kernel Gram matrix, i.e., \(\bm{K}_{\theta}=\left[k_{\theta}(\bm{x}_{i},\bm{x}_{j})\right]_{1\leq i,j\leq n}\), the marginal log-likelihood of a GP model (Williams and Rasmussen, 2006) can be formulated as Eq. (12)

\[\mathcal{L}(p(\bm{y}|\bm{X}))=-\frac{1}{2}\bm{y}^{\top}(\bm{K}_{\theta}+\sigma ^{2}\bm{I})^{-1}\bm{y}-\frac{1}{2}\log(|\bm{K}_{\theta}+\sigma^{2}\bm{I}|)- \frac{n}{2}\log 2\pi.\] (12)

Figure 1: Illustration of the our closed-form Rank-1 Lattice sampling and i.i.d. Gaussian sampling.

The standard GP model needs a \(O(n^{3})\) time complexity to compute the marginal log-likelihood, which is prohibitive for fast training as an inner step for stochastic optimization.

In this paper, we construct the random QMC samples based on rank-1 lattice, which enables us to perform fast GP training. Specifically, we build the GP model with the rank-1 lattice as the training data instead of the Gaussian samples. Define modulo kernel as Eq. (13):

\[k(\bm{x}_{i},\bm{x}_{j}):=k_{\Delta}(\phi(\bm{x}_{i}-\bm{x}_{j})),\] (13)

where \(k_{\Delta}(\cdot)\) is a shift-invariant kernel, and the function \(\phi(\bm{x}_{i}-\bm{x}_{j})\) is given as Eq. (14)

\[\phi(\bm{x}_{i}-\bm{x}_{j})=\min\big{(}(\bm{x}_{i}-\bm{x}_{j})\text{ mod }1,\bm{1}-(\bm{x}_{i}-\bm{x}_{j})\text{ mod }1\big{)},\] (14)

where operation \(\min(\cdot,\cdot)\) outputs the minimum among its two inputs element-wise, and mod \(1\) output the positive fractional parts of its inputs element-wise. The nonnegative fractional part of a real number \(x\) is \(x-\lfloor x\rfloor\), where \(\lfloor\cdot\rfloor\) denotes the floor function.

For a GP model with a modulo kernel defined in Eq.(13), the kernel Gram matrix is a circulant matrix thanks to the properties of rank-1 lattice. To be concrete, for rank-1 lattice data, we have Eq.(15)

\[k(\bm{x}_{i},\bm{x}_{j})=k(\bm{x}_{i+1},\bm{x}_{j+1})=k_{\Delta}\Big{(}\min \big{(}\frac{(i-j)\bm{z}\text{ mod }n}{n},\bm{1}-\frac{(i-j)\bm{z}\text{ mod }n}{n}\big{)}\Big{)}.\] (15)

Then the marginal log-likelihood \(\mathcal{L}(p(\bm{y}|\bm{X}))\) can be computed with a \(O(n\log n)\) time complexity by Fast Fourier Transform (FFT).

Specifically, note that the kernel Gram matrix \(\bm{K}_{\theta}+\sigma^{2}\bm{I}\) is a symmetric circulant matrix generated by vector \(\bm{k}_{\Delta}\)[1], where \(\bm{k}_{\Delta}\) is a vector with its \(i^{th}\) element given as Eq. (16).

\[k_{\Delta i}=k_{\Delta}\Big{(}\min\big{(}\frac{(i-1)\bm{z}\text{ mod }n}{n},\bm{1}-\frac{(i-1)\bm{z}\text{ mod }n}{n}\big{)}\Big{)}.\] (16)

We know that \(\bm{K}_{\theta}+\sigma^{2}\bm{I}\) can be diagonalized as \(\bm{K}_{\theta}+\sigma^{2}\bm{I}=\frac{1}{n}F^{*}\Lambda F\), where the \(j^{th}\) row and \(k^{th}\) column element of \(F\) is \(F_{jk}=e^{-2\pi j\text{k}/n}\). And the matrix \(\Lambda\) is the diagonal eigenvalue matrix that can be computed as \(\Lambda=\text{diag}(F\bm{k}_{\Delta})\). The matrix-vector product \(F\bm{k}_{\Delta}\) can be computed via FFT with \(O(n\log n)\) time complexity. And matrix-vector product \(\frac{1}{n}F^{*}\bm{v}\) for a vector \(\bm{v}\) can be computed via inverse FFT. More details about the properties of circulant matrices and fast computation via FFT can be found in [Gray et al., 2006].

Then, we achieve the fast computation of the terms in log-likelihood as Eq.(17) and Eq.(18):

\[\bm{y}^{\top}(\bm{K}_{\theta}+\sigma^{2}\bm{I})^{-1}\bm{y}=\bm{y }^{\top}\text{ifft}(\text{fft}(\bm{y})/\text{fft}(\bm{k}_{\Delta}))\] (17) \[\log(\big{|}\bm{K}_{\theta}+\sigma^{2}\bm{I}\big{|})=\sum_{i=1}^{ n}\log(\lambda_{i}+\sigma^{2})=\bm{1}^{\top}\log\big{(}\text{fft}(\bm{k}_{ \Delta})\big{)},\] (18)

where \(\text{ifft}(\cdot)\), \(\text{fft}(\cdot)\) denotes the inverse FFT and FFT operation, respectively, the operator \(/\) in Eq.(17) performs divide element-wise. And the \(\log(\cdot)\) is an element-wise operation. And \(\lambda_{i}\) in Eq.(18) denotes the eigenvalue of kernel Gram matrix \(\bm{K}_{\theta}\).

For inference, GP model has closed-form posterior mean and variance [Williams and Rasmussen, 2006] given as Eq.(19) and Eq.(20) :

\[\widehat{m}(\bm{x}) =\bm{k}_{\theta}(\bm{x})^{\top}(\bm{K}_{\theta}+\sigma^{2}I)^{-1} \bm{y}\] (19) \[\widehat{\sigma}^{2}(\bm{x}) =k_{\theta}(\bm{x},\bm{x})-\bm{k}_{\theta}(\bm{x})^{\top}(\bm{K}_{ \theta}+\sigma^{2}I)^{-1}\bm{k}_{\theta}(\bm{x}),\] (20)

where \(\bm{k}_{\theta}(\bm{x})=[k_{\theta}(\bm{x},\bm{x}_{1}),...,k_{\theta}(\bm{x},\bm{x}_{n})]^{\top}\).

With rank-1 lattice input data, we can perform fast inference by Eq.(21) and Eq.(22):

\[\widehat{m}(\bm{x}) =\bm{k}_{\theta}(\bm{x})^{\top}\text{ifft}(\text{fft}(\bm{y})/ \text{fft}(\bm{k}_{\Delta}))\] (21) \[\widehat{\sigma}^{2}(\bm{x}) =k_{\theta}(\bm{x},\bm{x})-\bm{k}_{\theta}(\bm{x})^{\top}\text{ ifft}(\text{fft}(\bm{k}_{\theta}(\bm{x}))/\text{fft}(\bm{k}_{\Delta})).\] (22)

Both the exact GP training and inference benefit from the structure of rank-1 lattice and FFT acceleration, which can be performed with a \(O(n\log n)\) time complexity. A deep learning toolbox, e.g., Pytorch, can be used to train the parameters of the kernel.

### Fast Coordinate Search for Targeted Sampling

This subsection shows how to perform a fast coordinate search for targeted sampling. A rank-1 lattice with \(n\) points is contained in a grid \(\{0,\frac{1}{n},\cdots,\frac{n-1}{n}\}^{d}\). We thus perform a coordinate descent search from the index set \(\{0,1,\cdots,n-1\}^{d}\) to minimize the GP posterior mean in Eq.(19).

Let \(k(\cdot,\cdot)=k_{\Delta}(\cdot)\) be a shift-invariant kernel with a decomposition structure as Eq. (23):

\[k(\bm{x}^{*},\bm{x})=k_{\Delta}(\phi(\bm{x}^{*}-\bm{x}))=\Pi_{q=1}^{d}k_{\Delta }(\phi(x_{q}^{*}-x_{q})),\] (23)

where \(x_{q}^{*}\), \(x_{q}\) denotes the \(q^{th}\) element in \(\bm{x}^{*}\), \(\bm{x}\), respectively. We can perform a coordinate search by fixing all the components except the \(q^{th}\) one as the current working component for index searching. Formally, let \(\bm{w}=(\bm{K}_{\theta}+\sigma^{2}I)^{-1}\bm{y}\). Then, we have the GP posterior mean function given as Eq. (24):

\[\widehat{m}(\bm{x}^{*})=\bm{k}_{\Delta}^{q\top}(x_{q}^{*})\big{(}\widehat{ \bm{k}}_{\Delta}^{q}\odot\bm{w}\big{)},\] (24)

where \(\odot\) denotes the element-wise product, and \(\bm{k}_{\Delta}^{q}(x_{q}^{*})\) denotes a vector with \(i^{th}\) element given as \(\bm{k}_{\Delta i}^{q}=k_{\Delta}(\phi(x_{q}^{*}-\bm{X}_{qi}))\), and \(\bm{X}_{qi}\) denotes the element in \(q^{th}\)-row and \(i^{th}\)-column of the rank-1 lattice matrix \(\bm{X}=[\bm{x}_{1},\cdots,\bm{x}_{n}]\). The vector \(\widehat{\bm{k}}_{\Delta}^{q}\) denotes the remainder vector with its \(i^{th}\)-element given as Eq. (25):

\[\widehat{\bm{k}}_{\Delta i}^{q}=\frac{1}{k_{\Delta}(\phi(x_{q}^{*}-\bm{X}_{qi }))}\Pi_{q=1}^{d}k_{\Delta}(\phi(x_{q}^{*}-\bm{X}_{qi})).\] (25)

To optimize the \(q^{th}\) component \(x_{q}^{*}\) of \(\bm{x}^{*}\), we fix the other components of \(\bm{x}^{*}\) and the corresponding vector \(\widehat{\bm{k}}_{\Delta}^{q}\). We find \(x_{q}^{*}\) by solving the subproblem given in Eq. (26)

\[x_{q}^{*}=\operatorname*{arg\,min}_{x\in\{0,\cdots,n-1\}}\bm{k}_{\Delta}^{q} (x)^{\top}\big{(}\widehat{\bm{k}}_{\Delta}^{q}\odot\bm{w}\big{)}.\] (26)

Directly enumerate computation of the problem (26) needs a \(O(n^{2})\) time complexity. In our paper, we can perform a fast computation with \(O(n\log n)\) time complexity thanks to the rank-1 lattice \(\bm{X}\). Specially, when \(\bm{X}\) is a rank-1 lattice with the generating vector \(\bm{z}=[z_{1},\cdots,z_{d}]\), then the matrix \(\bm{K}_{\Delta}^{q}=[\bm{k}_{\Delta}^{q}(0),\bm{k}_{\Delta}^{q}(\frac{(1z_{q} \bmod n)}{n},\cdots,\bm{k}_{\Delta}^{q}(\frac{(n-1)z_{q}\bmod n}{n})]\) forms a circulant matrix, and the problem (26) can be accelerated via FFT by Eq. (27)

\[\bm{c}^{q}=\bm{K}_{\Delta}^{q\top}\big{(}\widehat{\bm{k}}_{\Delta}^{q}\odot \bm{w}\big{)}=\text{ifft}(\text{fft}(\bm{k}_{\Delta}^{q}(0))\odot\text{fft}( \widehat{\bm{k}}_{\Delta}^{q}\odot\bm{w})),\] (27)

where \(\text{fft}(\cdot)\) and \(\text{ifft}(\cdot)\) denote the FFT and inverse FFT operation. Then, we can achieve \(x_{q}^{*}\) by the index \(i^{*}\) of the minimum element in vector \(\bm{c}^{q}=\bm{K}_{\Delta}^{q\top}\big{(}\widehat{\bm{k}}_{\Delta}^{q}\odot \bm{w}\big{)}\), and set \(x_{q}^{*}=\frac{i^{*}z_{q}\bmod n}{n}\).

We present the algorithm of the fast coordinate search in Algorithm 1. The Algorithm 1 return a targeted sample with a small prediction value in a fast manner. We can use the targeted sample to accelerate the stochastic optimization. Finally, we present our overall stochastic optimization algorithm in the Algorithm 2. We choose INGO (Lyu and Tsang, 2021) as our backbone algorithm because of its simple implementation and fewer hyperparameters. One can plug our RLTS into other stochastic optimization methods to improve query efficiency.

### Closed-form Rank-1 Lattice Construction

This subsection will show how to construct our closed-form rank-1 lattice for fast sampling. For \(\forall\bm{x},\bm{y}\in[0,1]^{d}\) and \(\alpha>1\), define a reproducing kernel as Eq. (28)

\[K(\bm{x},\bm{y})=\sum_{\bm{k}\in\mathbb{Z}^{d}}\gamma_{\alpha}(\bm{k})\exp\left( 2\pi\mathbf{i}\bm{k}^{\top}(\bm{x}-\bm{y})\right),\] (28)

where \(\mathbf{i}^{2}=-1\) and \(\gamma_{\alpha}(\bm{k})=\prod_{j=1}^{d}\gamma_{\alpha}(k_{j})\) with \(\gamma_{\alpha}(k)\) is given as follows:

\[\gamma_{\alpha}(k)=\begin{cases}1&\text{if }k=0\\ |k|^{-\alpha}&\text{if }k\neq 0.\end{cases}\] (29)

A Korobov space is a reproducing kernel Hilbert space (RKHS) associated with the kernel in Eq.(28), denoted as \(\mathcal{H}_{k}\).

Our closed form of the generating vector is given as Eq.(30):

\[\bm{z}=[g^{0},g^{\frac{n-1}{2d-1}},g^{\frac{2(n-1)}{2d-1}},\cdots,g^{\frac{(d- 1)(n-1)}{2d-1}}]\bmod n,\] (30)

where \(g\) denotes the primitive root modulo the prime number \(n\), and \((2d-1)|(n-1)\). Then, our close-form rank-1 lattice can be achieved by Eq. (6)

Given a point set \(\mathcal{P}=\{\bm{x}_{1},\cdots,\bm{x}_{n}\}\), the square worst case integral approximation error for \(f\in\mathcal{H}_{k}\) is defined as Eq.(31):

\[e^{2}(\mathcal{H}_{k};\mathcal{P})=\sup_{f\in\mathcal{H}_{k},\|f\|_{\mathcal{H }_{k}}\leq 1}\Big{|}\int_{[0,1]^{d}}f(\bm{x})\mathbf{d}\bm{x}-\frac{1}{n} \sum_{j=0}^{n-1}f\left(\bm{x}_{j}\right)\Big{|}^{2}.\] (31)

We further show that our rank-1 lattice constructed by Eq. (30) has a regular worst-case error pattern in Theorem 1. The proof is given in the Appendix.

**Theorem 1**.: _Let \(n\) be a prime number such that \((2d-1)|(n-1)\). Suppose the integrand function \(f\in\mathcal{H}_{k},\|f\|_{\mathcal{H}_{k}}\leq 1\), the square worst-case integral approximation error of rank-1 lattice \(\mathcal{P}\) constructed by Eq.(30) is given as Eq.(32):_

\[e^{2}(\mathcal{H}_{k};\mathcal{P})=\tfrac{1}{2n}\mathbf{1}^{\top}(\bm{h}^{0} \odot\cdots\odot\bm{h}^{2d-2}-\mathbf{1}-(\bm{h}^{1}\odot\cdots\odot\bm{h}^{ d-1}-\mathbf{1})\odot\bm{h}^{0}\odot(\bm{h}^{-1}\odot\cdots\odot\bm{h}^{-(d-1)}- \mathbf{1}))+\tfrac{1}{n^{\alpha}}\zeta(\alpha,1),\] (32)_where \(\odot\) denotes the element-wise product, symbol \(\mathbf{1}\) denotes the vector with elements all ones, and \(\boldsymbol{h}^{i}=\boldsymbol{F}^{i}\boldsymbol{\gamma}\) with \(\boldsymbol{F}\) as the discrete Fourier matrix, i.e., \(\boldsymbol{F}_{jk}=\exp(2\pi\mathbf{i}\frac{jk}{n})\), and \(\boldsymbol{F}^{i}\) denotes the matrix after permutation of the rows of \(\boldsymbol{F}\) such that the \(j^{th}\) row of \(\boldsymbol{F}^{i}\) equals to the \(\widetilde{j}^{th}\) row of \(\boldsymbol{F}\), where \(\widetilde{j}=jg^{\frac{i(m-1)}{2d-1}}\) mod \(n\). And \(\boldsymbol{\gamma}=[\gamma_{1},\cdots,\gamma_{n}]^{\top}\) with \(\gamma_{k}=\frac{1}{n^{\alpha}}\big{(}\zeta(\alpha,\frac{k_{i}}{n})+\zeta( \alpha,\frac{n-k_{i}}{n})\big{)}\) for \(k\in\{1,\cdots,n-1\}\) and \(\gamma_{n}=1+\frac{2}{n^{\alpha}}\zeta(\alpha,1)\), where \(\zeta(\cdot,\cdot)\) is the Hurwitz zeta function._

**Remarks:** The term \(H=\boldsymbol{h}^{0}\odot\cdots\odot\boldsymbol{h}^{2d-2}\mathbf{-1}-( \boldsymbol{h}^{1}\odot\cdots\odot\boldsymbol{h}^{d-1}\mathbf{-1})\odot \boldsymbol{h}^{0}\odot(\boldsymbol{h}^{-1}\odot\cdots\odot\boldsymbol{h}^{-( d-1)}\mathbf{-1})\) has a regular pattern because of \(\{g^{0},g^{\frac{n-1}{2d-1}},g^{\frac{2(n-1)}{2d-1}},\cdots,g^{\frac{(d-1)(n-1) }{2d-1}},\cdots,g^{\frac{(2d-2)(n-1)}{2d-1}}\}\) mod \(n\) forms a subgroup of \(\{1,\cdots,n-1\}\) mod \(n\). According to the Lagrange's theorem in group theory [Dummit and Foote, 2004], the vector \(\boldsymbol{h}^{0}\odot\cdots\odot\boldsymbol{h}^{2d-2}\) has \(\frac{n-1}{2d-1}\) different elements.

## 4 Experiments

We replace the i.i.d. Gaussian sampling of the INGO [Lyu and Tsang, 2021] with our RLTS. We evaluate our RLTS by comparing it with the standard INGO and the CMAES [Hansen, 2006]. In all the experiments, we keep the number of batch samples and the initialization the same for RLTS, INGO and CMAES. For all the methods, we initialize the \(\boldsymbol{\mu}=\mathbf{0}\). For INGO and RLTS, we set the step-size parameter \(\beta=0.2\) in all experiments. For RLTS, we set the parameter \(\eta=1\) in all experiments.

### Evaluation on Benchmark Functions

We first evaluate our RLTS on challenging benchmark test functions: Rosenbrock, Rastrigin, and Nesterov. Rastrigin and Rosenbrock are smooth multi-mode functions, and Nesterov is a non-smooth function. These functions are very challenging benchmarks for black-box optimization. We offset the optimum by setting \(\boldsymbol{x}=\boldsymbol{x}-5\) of the test functions. This increases the distance between the optimum and the initial point \(\boldsymbol{\mu}=\mathbf{0}\), which makes the test problems more challenging. We implement INGO by ourselves. For CMAES, we use the publicly available code 2. We initialized \(\boldsymbol{\Sigma}=\boldsymbol{I}\) for all the methods.

Footnote 2: https://pypi.org/project/cma/

Figure 2: Cumulative min objective value v.s. the number of query evaluations on 50-dimensional and 500-dimensional benchmark test functions.

We evaluate RLTS on 50 and 500-dimensional problems. The batchsize of all the methods are set to 200 and 2000 for 50 and 500-dimensional problems, respectively. All the experiments are performed in ten independent runs. The experimental results are shown in Figure 2. From Figure 2, we can observe that RLTS consistently converge faster than INGO on all the test functions on both 50-dimensional and 500-dimensional cases. It shows that our RLTS significantly improves the query efficiency of INGO, which verifies the effectiveness of RLTS. Moreover, we can see that RLTS outperforms CMAES on all the test functions on both 50-dimensional and 500-dimensional cases. In addition, we see that CMAES converge slowly on the 500-dimensional benchmark problems, while RLTS converges faster.

### Evaluation on Black-box Prompt Fine-tuning Tasks

Prompt fine-tuning of large language models is a promising direction to achieve expertise models efficiently for downstream tasks. We evaluate our RLTS on black-box prompt fine-tuning tasks.

We employ the deep model in [14] with publicly available code 3 as the backbone model for black-box prompt fine-tuning. It has \(24\) layers. For each layer, we set the dimension of the continuous prompt to \(500\). Thus, the total dimension is \(24\times 500\). We employ the hinge loss of training data as the black-box objective. Six benchmark datasets for different language tasks are employed for evaluation: DBpedia, SS2, SNLI, AG's News, MRPC and RTE. The SST2 [22] dataset is a dataset for the sentiment analysis task. AG's News and DBPedia datasets [15] are used for topic classification tasks. SNLI [17] and RTE [18] are employed for natural language inference. MRPC dataset [16] is used for the paraphrasing task.

Footnote 3: https://github.com/txsun1997/Black-Box-Tuning

In all the experiments, we keep the number of batch samples and the initialization the same for RLTS, INGO and CMAES. We set the number of batch samples to \(2000\). Specifically, our RLTS employs \(1999\) rank-1 lattice QMC Gaussian samples and one sample from targeted sampling. INGO employs \(1999\) rank-1 lattice QMC Gaussian samples and one Gaussian sample. CMAES employs \(2000\) Gaussian samples. We initialize the \(\bm{\mu}=\bm{0}\) and \(\bm{\Sigma}=0.2\bm{I}\) for all the methods. For INGO and RLTS, we set the step-size parameter \(\beta=0.2\) in all experiments. For RLTS, we set the parameter \(\eta=1\) in all experiments. All the experiments are performed in five independent runs with seeds in \(\{1,2,3,4,5\}\). The layer-wise coordinate descent update approach in [14] is employed for all the methods.

Figure 3: Hinge loss v.s. the number of query evaluations on different black-box fine-tuning models.

The experimental results of mean objective \(\pm\) std v.s. the number of queries are shown in Figure 3. From Figure 3, we can observe that our RLTS decreases the objective significantly faster than INGO and CMAES on all six fine-tuning tasks, which shows the superior query efficiency of our RLTS.

### Additional Comparison with High-dimensional Bayesian Optimization

We further compare our RLTS with the high-dimensional BO method TuRBO (Eriksson et al., 2019). We evaluate RLTS on the three benchmark functions: the Rosenbrock function, the Rastrigin10 function, and the Nesterov function. We offset the optimum by setting \(\bm{x}=\bm{x}-5\) of the test functions. The dimension is set to 500. The number of initial points of TuRBO is set to 2000. The batch size of both RLTS and TuRBO is set to 2000. The maximum number of queries is set to 50,000. We employ the default box boundary for TuRBO, i.e., \([-5,10]^{d}\). The initial parameter \(\bm{\mu}\) of RLTS is set to \(\bm{\mu}=\bm{0}\), and \(\bm{\Sigma}\) is set to \(\bm{\Sigma}=\bm{I}\). For TuRBO, we employ the official code provided in the paper (Eriksson et al., 2019). All the methods are performed in three independent runs.

The convergence performance regarding the number of query evaluations is shown in Figure 4. We can observe that RLTS converges faster than TuRBO on the benchmark test problems, demonstrating that RLTS improves query efficiency.

We further report the running time of RLTS and TuRBO on the same machine for evaluation. The results are shown in Table 1. We can observe that RLTS performs significantly faster than TuRBO, achieving around 300 times speedup regarding running time. The computation time of Bayesian Optimization usually grows cubically fast as the number of queries increases. In contrast, our RLTS reduces the expensive \(O(n^{3})\) operation to \(O(n\log n)\) time complexity, which enables a fast plug-in of the ES-type algorithms.

## 5 Conclusion

We proposed a novel Rank-1 Lattice Targeted Sampling technique in this paper. Our RLTS has a \(O(n\log n)\) time complexity w.r.t. \(n\) batch samples, which is fast for plugging into stochastic optimization methods to improve query efficiency while scaling up to high-dimensional problems. Empirically, we plugged our RLTS into the sampling phase of INGO, significantly improving the query efficiency on benchmark test functions and black-box prompt fine-tuning tasks. Moreover, we proposed a closed-form rank-1 lattice by analyzing the integral approximation error of functions in Korobov space. Our closed-form rank-1 lattice provides an efficient way for QMC Gaussian sampling, with properties enabling fast exact GP training and inference with a \(O(n\log n)\) time complexity, which is critical for our RLTS to be a fast internal step for stochastic optimization. In addition, our closed-form rank-1 lattice is a fundamental tool that may have potential applications beyond the black-box optimization task.

\begin{table}
\begin{tabular}{c|c|c|c} \hline  & Rosenbrock & Rastrigin10 & Nesterov \\ \hline RLTS & 83.62(s) & 84.04(s) & 83.46(s) \\ \hline TuRBO & 25927.39(s) & 25941.66(s) & 25697.87(s) \\ \hline \end{tabular}
\end{table}
Table 1: Running time on benchmark test functions. Symbol (s) denotes seconds.

Figure 4: Cumulative min objective value v.s. the number of query evaluations on 500-dimensional benchmark test functions.

## Acknowledgement

We thank the anonymous reviewers for their valuable comments and helpful suggestions.

## References

* Back et al. (1991) Thomas Back, Frank Hoffmeister, and Hans-Paul Schwefel. A survey of evolution strategies. In _Proceedings of the fourth international conference on genetic algorithms_, volume 2. Morgan Kaufmann Publishers San Mateo, CA, 1991.
* Barsce et al. (2017) Juan Cruz Barsce, Jorge A Palombarini, and Ernesto C Martinez. Towards autonomous reinforcement learning: Automatic setting of hyper-parameters using bayesian optimization. In _Computer Conference (CLEI), 2017 XLIII Latin American_, pages 1-9. IEEE, 2017.
* Bowman et al. (2015) Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large annotated corpus for learning natural language inference. _Proceedings of the 2015 conference on empirical methods in natural language processing_, 2015.
* Choromanski et al. (2019) Krzysztof Choromanski, Aldo Pacchiano, Jack Parker-Holder, and Yunhao Tang. From complexity to simplicity: Adaptive es-active subspaces for blackbox optimization. _arXiv:1903.04268_, 2019.
* Dick et al. (2013) Josef Dick, Frances Y Kuo, and Ian H Sloan. High-dimensional integration: the quasi-monte carlo way. _Acta Numerica_, 22:133-288, 2013.
* Dick et al. (2022) Josef Dick, Peter Kritzer, and Friedrich Pillichshammer. _Lattice Rules_. Springer, 2022.
* Dolan and Brockett (2005) Bill Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In _Third International Workshop on Paraphrasing (IWP2005)_, 2005.
* Dummit and Foote (2004) David Steven Dummit and Richard M Foote. _Abstract algebra_, volume 3. Wiley Hoboken, 2004.
* Eriksson et al. (2019) David Eriksson, Michael Pearce, Jacob Gardner, Ryan D Turner, and Matthias Poloczek. Scalable global optimization via local bayesian optimization. _Advances in neural information processing systems_, 32, 2019.
* Gardner et al. (2017) Jacob Gardner, Chuan Guo, Kilian Weinberger, Roman Garnett, and Roger Grosse. Discovering and exploiting additive structure for bayesian optimization. In _Artificial Intelligence and Statistics_, pages 1311-1319. PMLR, 2017.
* Gray et al. (2006) Robert M Gray et al. Toeplitz and circulant matrices: A review. _Foundations and Trends(r) in Communications and Information Theory_, 2(3):155-239, 2006.
* Hansen (2006) Nikolaus Hansen. The cma evolution strategy: a comparing review. In _Towards a new evolutionary computation_, pages 75-102. Springer, 2006.
* Hernandez-Lobato et al. (2017) Jose Miguel Hernandez-Lobato, James Requeima, Edward O Pyzer-Knapp, and Alan Aspuru-Guzik. Parallel and distributed thompson sampling for large-scale accelerated exploration of chemical space. In _International conference on machine learning_, pages 1470-1479. PMLR, 2017.
* Korobov (1960) Nikolai Mikhailovich Korobov. Properties and calculation of optimal coefficients. In _Doklady Akademii Nauk_, volume 132, pages 1009-1012. Russian Academy of Sciences, 1960.
* Lizotte et al. (2007) Daniel J Lizotte, Tao Wang, Michael H Bowling, and Dale Schuurmans. Automatic gait optimization with gaussian process regression. In _IJCAI_, volume 7, pages 944-949, 2007.
* Lyu and Tsang (2021) Yueming Lyu and Ivor W Tsang. Black-box optimizer with stochastic implicit natural gradient. In _Machine Learning and Knowledge Discovery in Databases. Research Track: European Conference, ECML PKDD 2021, Bilbao, Spain, September 13-17, 2021, Proceedings, Part III 21_, pages 217-232. Springer, 2021.
* Lyu et al. (2020) Yueming Lyu, Yuan Yuan, and Ivor Tsang. Subgroup-based rank-1 lattice quasi-monte carlo. _Advances in Neural Information Processing Systems_, 33:6269-6280, 2020.
* Lyu et al. (2017)Wesley Maddox, Qing Feng, and Max Balandat. Optimizing high-dimensional physics simulations via composite bayesian optimization. _arXiv preprint arXiv:2111.14911_, 2021.
* Mirjalili and Mirjalili (2019) Seyedali Mirjalili and Seyedali Mirjalili. Genetic algorithm. _Evolutionary Algorithms and Neural Networks: Theory and Applications_, pages 43-55, 2019.
* Muller et al. (2021) Sarah Muller, Alexander von Rohr, and Sebastian Trimpe. Local policy search with bayesian optimization. _Advances in Neural Information Processing Systems_, 34:20708-20720, 2021.
* Nayebi et al. (2019) Amin Nayebi, Alexander Munteanu, and Matthias Poloczek. A framework for bayesian optimization in embedded subspaces. In _International Conference on Machine Learning_, pages 4752-4761. PMLR, 2019.
* Negoescu et al. (2011) Diana M Negoescu, Peter I Frazier, and Warren B Powell. The knowledge-gradient algorithm for sequencing experiments in drug discovery. _INFORMS Journal on Computing_, 23(3):346-363, 2011.
* Nesterov and Spokoiny (2017) Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions. _Foundations of Computational Mathematics_, 17(2):527-566, 2017.
* Nguyen et al. (2022) Quan Nguyen, Kaiwen Wu, Jacob Gardner, and Roman Garnett. Local bayesian optimization via maximizing probability of descent. _Advances in neural information processing systems_, 35:13190-13202, 2022.
* Rechenberg and Eigen (1973) Ingo Rechenberg and M. Eigen. _Optimierung technischer Systeme nach Prinzipien der biologischen Evolution_. PhD thesis, 1973.
* Salimans et al. (2017) Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a scalable alternative to reinforcement learning. _arXiv preprint arXiv:1703.03864_, 2017.
* Sloan and Reztsov (2002) I Sloan and A Reztsov. Component-by-component construction of good lattice rules. _Mathematics of Computation_, 71(237):263-273, 2002.
* Sloan (2000) Ian H Sloan. Multiple integration is intractable but not hopeless. _The ANZIAM Journal_, 42(1):3-8, 2000.
* Snoek et al. (2012) Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine learning algorithms. In _NeurIPS_, pages 2951-2959, 2012.
* Socher et al. (2013) Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In _Proceedings of the 2013 conference on empirical methods in natural language processing_, pages 1631-1642, 2013.
* Srinivas and Patnaik (1994) Mandavilli Srinivas and Lalit M Patnaik. Genetic algorithms: A survey. _computer_, 27(6):17-26, 1994.
* Srinivas et al. (2010) Niranjan Srinivas, Andreas Krause, Sham M Kakade, and Matthias Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In _ICML_, 2010.
* Sun et al. (2022a) Tianxiang Sun, Zhengfu He, Hong Qian, Yunhua Zhou, Xuanjing Huang, and Xipeng Qiu. Bbtv2: Towards a gradient-free future with large language models. In _Proceedings of EMNLP_, 2022a.
* Sun et al. (2022b) Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu. Black-box tuning for language-model-as-a-service. In _Proceedings of ICML_, 2022b.
* Wang et al. (2019) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. _In 7th International Conference on Learning Representations, ICLR_, 2019.
* Wang and Shan (2007) G Gary Wang and Songqing Shan. Review of metamodeling techniques in support of engineering design optimization. _Journal of Mechanical design_, 129(4):370-380, 2007.
* Wang et al. (2019)Daan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, Jan Peters, and Jurgen Schmidhuber. Natural evolution strategies. _The Journal of Machine Learning Research (JMLR)_, 15(1):949-980, 2014a.
* Wierstra et al. (2014b) Daan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, Jan Peters, and Jurgen Schmidhuber. Natural evolution strategies. _The Journal of Machine Learning Research_, 15(1):949-980, 2014b.
* Williams and Rasmussen (2006) Christopher KI Williams and Carl Edward Rasmussen. _Gaussian processes for machine learning_, volume 2. MIT press Cambridge, MA, 2006.
* Zhang et al. (2015) Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. _Advances in neural information processing systems_, 28, 2015.

**Appendix**

## Appendix A Proof of Theorem 1

**Theorem 1**.: _Let \(n\) be a prime number such that \((2d-1)|(n-1)\). Suppose the integrand function \(f\in\mathcal{H}_{k},\|f\|_{\mathcal{H}_{k}}\leq 1\), the square worst-case integral approximation error of rank-1 lattice \(\mathcal{P}\) constructed by Eq.(30) is given as Eq.(33):_

\[e^{2}(\mathcal{H}_{k};\mathcal{P})=\tfrac{1}{2n}\mathbf{1}^{\top}(\boldsymbol {h}^{0}\odot\cdots\odot\boldsymbol{h}^{2d-2}-\mathbf{1}-(\boldsymbol{h}^{1} \odot\cdots\odot\boldsymbol{h}^{d-1}-\mathbf{1}))\otimes\boldsymbol{h}^{0} \odot(\boldsymbol{h}^{-1}\odot\cdots\odot\boldsymbol{h}^{-(d-1)}-\mathbf{1})) +\tfrac{1}{n^{\alpha}}\zeta(\alpha,1),\] (33)

_where \(\odot\) denotes the element-wise product, symbol \(\mathbf{1}\) denotes the vector with elements all ones, and \(\boldsymbol{h}^{i}=\boldsymbol{F}^{i}\boldsymbol{\gamma}\) with \(\boldsymbol{F}\) as the discrete Fourier matrix, i.e., \(\boldsymbol{F}_{jk}=\exp(2\pi\mathbf{i}\tfrac{jk}{n})\), and \(\boldsymbol{F}^{i}\) denotes the matrix after permutation of the rows of \(\boldsymbol{F}\) such that the \(j^{th}\) row of \(\boldsymbol{F}^{i}\) equals to the \(\widetilde{j}^{th}\) row of \(\boldsymbol{F}\), where \(\widetilde{j}=jg^{\frac{(n-1)}{2d-1}}\) mod \(n\). And \(\boldsymbol{\gamma}=[\gamma_{1},\cdots,\gamma_{n}]^{\top}\) with \(\gamma_{k}=\tfrac{1}{n^{\alpha}}\big{(}\zeta(\alpha,\tfrac{k_{i}}{n})+\zeta( \alpha,\tfrac{n-k_{i}}{n})\big{)}\) for \(k\in\{1,\cdots,n-1\}\) and \(\gamma_{n}=1+\tfrac{2}{n^{\alpha}}\zeta(\alpha,1)\), where \(\zeta(\cdot,\cdot)\) denotes the Hurwitz zeta function._

To prove our main Theorem 1, we begin with several Lemma.

**Lemma 1**.: _For \(\forall\boldsymbol{x},\boldsymbol{y}\in[0,1]^{d}\) and \(\alpha>1\), define a reproducing kernel as Eq.(34)_

\[K(\boldsymbol{x},\boldsymbol{y})=\sum_{\boldsymbol{k}\in\mathbb{Z}^{d}}\gamma _{\alpha}(\boldsymbol{k})\exp\left(2\pi\mathbf{i}\boldsymbol{k}^{\top}( \boldsymbol{x}-\boldsymbol{y})\right),\] (34)

_where \(\gamma_{\alpha}(\boldsymbol{k})=\prod_{j=1}^{d}\gamma_{\alpha}(k_{j})\) with \(\gamma_{\alpha}(k)\) given in Eq.(35)_

\[\gamma_{\alpha}(k)=\begin{cases}1&\text{if }k=0\\ |k|^{-\alpha}&\text{if }k\neq 0.\end{cases}\] (35)

_Let \(\mathcal{P}=[\boldsymbol{x}_{1},\cdots,\boldsymbol{x}_{n}]\) be a rank-1 lattice constructed by the generating vector \(\boldsymbol{z}\) with a prime number \(n\). Then, for \(\forall f\in\mathcal{H}_{k},\|f\|_{\mathcal{H}_{k}}\leq 1\) associated with the reproducing kernel Eq.(34), we have the square worst-case integral approximation error of \(\mathcal{P}\) as Eq.(36)._

\[e^{2}(\mathcal{H}_{k};\mathcal{P})=\sup_{f\in\mathcal{H}_{k},\|f\|_{\mathcal{ H}_{k}}\leq 1}\left|\int_{[0,1]^{d}}f(\boldsymbol{x})d\boldsymbol{x}-\frac{1}{n} \sum_{j=0}^{n-1}f\left(\boldsymbol{x}_{j}\right)\right|^{2}=\sum_{\boldsymbol{k }\in L^{\perp}\setminus\{\boldsymbol{0}\}}\gamma_{\alpha}(\boldsymbol{k})\] (36)

_where \(L^{\perp}\) denote the dual lattice defined in Eq.(37)._

\[L^{\perp}:=\{\boldsymbol{k}|\boldsymbol{k}^{\top}\boldsymbol{z}\equiv 0\;(\text{ mod }n),\boldsymbol{k}\in\mathbb{Z}^{d}\}.\] (37)

Proof.: Given a point set \(\mathcal{P}=\{\boldsymbol{x}_{1},\cdots,\boldsymbol{x}_{n}\}\), the worst case approximation error for \(\forall f\in\mathcal{H}_{k},\|f\|_{\mathcal{H}_{k}}\leq 1\) is

\[e^{2}(\mathcal{H}_{k};\mathcal{P}) =\sup_{f\in\mathcal{H}_{k},\|f\|_{\mathcal{H}_{k}}\leq 1}\left|\int_{[0,1] ^{d}}f(\boldsymbol{x})\mathrm{d}\boldsymbol{x}-\frac{1}{n}\sum_{j=0}^{n-1}f \left(\boldsymbol{x}_{j}\right)\right|^{2}\] (38) \[=\sup_{f\in\mathcal{H}_{k},\|f\|_{\mathcal{H}_{k}}\leq 1}\left|\left\langle f,\int_{[0,1]^{d}}K(\boldsymbol{x},\cdot)\mathrm{d}\boldsymbol{x}-\frac{1}{n} \sum_{j=0}^{n-1}K\left(\boldsymbol{x}_{j},\cdot\right)\right\rangle_{ \mathcal{H}_{k}}\right|^{2}\] (39) \[=\sup_{f\in\mathcal{H}_{k},\|f\|_{\mathcal{H}_{k}}\leq 1}\|f\|_{ \mathcal{H}_{k}}\Big{\|}\int_{[0,1]^{d}}K(\boldsymbol{x},\cdot)\mathrm{d} \boldsymbol{x}-\frac{1}{n}\sum_{j=0}^{n-1}K\left(\boldsymbol{x}_{j},\cdot \right)\Big{\|}_{\mathcal{H}_{k}}\] (40) \[=\int_{[0,1]^{d}}\int_{[0,1]^{d}}K(\boldsymbol{x},\boldsymbol{y}) \mathrm{d}\boldsymbol{x}\mathrm{d}\boldsymbol{y}-\frac{2}{n}\sum_{j=1}^{n}\int_{[ 0,1]^{d}}K(\boldsymbol{x},\boldsymbol{x}_{j})\mathrm{d}\boldsymbol{x}+\frac{1 }{n^{2}}\sum_{i,j=1}^{n}K(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\] (41)Then, from the definition of the reproducing kernel \(K(\bm{x},\bm{y})\) in Eq.(34), we know that

\[\int_{[0,1]^{d}}\int_{[0,1]^{d}}K(\bm{x},\bm{y})\mathrm{d}\bm{x} \mathrm{d}\bm{y} =\int_{[0,1]^{d}}\int_{[0,1]^{d}}\sum_{\bm{k}\in\mathbb{Z}^{d}} \gamma_{\alpha}(\bm{k})\exp\left(2\pi\mathrm{i}\bm{k}^{\top}(\bm{x}-\bm{y}) \right)\mathrm{d}\bm{x}\mathrm{d}\bm{y}\] (42) \[=1+\sum_{\bm{k}\in\mathbb{Z}^{d},\bm{k}\neq\bm{0}}\gamma_{\alpha} (\bm{k})\int_{[0,1]^{d}}\int_{[0,1]^{d}}\exp\left(2\pi\mathrm{i}\bm{k}^{\top} (\bm{x}-\bm{y})\right)\mathrm{d}\bm{x}\mathrm{d}\bm{y}\] (43) \[=1+\sum_{\bm{k}\in\mathbb{Z}^{d},\bm{k}\neq\bm{0}}\gamma_{\alpha} (\bm{k})\cdot 0=1\] (44)

In addition, the second term in Eq.(41) as follows

\[-\frac{2}{n}\sum_{j=1}^{n}\int_{[0,1]^{d}}K(\bm{x},\bm{x}_{j}) \mathrm{d}\bm{x}\] (45) \[=-\frac{2}{n}\sum_{j=1}^{n}\int_{[0,1]^{d}}\sum_{\bm{k}\in \mathbb{Z}^{d}}\gamma_{\alpha}(\bm{k})\exp\left(2\pi\mathrm{i}\bm{k}^{\top}( \bm{x}-\bm{x}_{j})\right)\mathrm{d}\bm{x}\] (46) \[=-\frac{2}{n}\sum_{j=1}^{n}\gamma_{\alpha}(\bm{0})-\frac{2}{n} \sum_{j=1}^{n}\sum_{\bm{k}\in\mathbb{Z}^{d},\bm{k}\neq\bm{0}}\gamma_{\alpha}( \bm{k})\int_{[0,1]^{d}}\exp\left(2\pi\mathrm{i}\bm{k}^{\top}(\bm{x}-\bm{x}_{j} )\right)\mathrm{d}\bm{x}\] (47) \[=-\frac{2}{n}\sum_{j=1}^{n}\gamma_{\alpha}(\bm{0})-\frac{2}{n} \sum_{j=1}^{n}\sum_{\bm{k}\in\mathbb{Z}^{d},\bm{k}\neq\bm{0}}\gamma_{\alpha}( \bm{k})\cdot 0\] (48) \[=-2\] (49)

Moreover, from the definition of rank-1 lattice \(\mathcal{P}\) with prime \(n\) and generating vector \(\bm{z}\), we have the third term in Eq.(41) as follows

\[\frac{1}{n^{2}}\sum_{i,j=1}^{n}K(\bm{x}_{i},\bm{x}_{j})\] (50) \[=\frac{1}{n^{2}}\sum_{i,j=1}^{n}\sum_{\bm{k}\in\mathbb{Z}^{d}} \gamma_{\alpha}(\bm{k})\exp\left(2\pi\mathrm{i}\bm{k}^{\top}(\bm{x}_{i}-\bm{x} _{j})\right)\] (51) \[=1+\frac{1}{n^{2}}\sum_{i,j=1}^{n}\sum_{\bm{k}\in\mathbb{Z}^{d}, \bm{k}\neq\bm{0}}\gamma_{\alpha}(\bm{k})\exp\left(\frac{2\pi\mathrm{i}(i-j) \bm{k}^{\top}\bm{z}}{n}\right)\] (52) \[=1+\sum_{\bm{k}\in\mathbb{Z}^{d},\bm{k}\neq\bm{0}}\gamma_{\alpha} (\bm{k})\frac{1}{n^{2}}\sum_{i,j=1}^{n}\exp\left(\frac{2\pi\mathrm{i}(i-j)\bm{ k}^{\top}\bm{z}}{n}\right)\] (53) \[=1+\sum_{\bm{k}\in\mathbb{Z}^{d},\bm{k}\neq\bm{0}}\gamma_{\alpha} (\bm{k})\frac{1}{n}\sum_{j=1}^{n}\exp\left(\frac{2\pi\mathrm{i}\bm{j}\bm{k}^{ \top}\bm{z}}{n}\right)\] (54)

Put Eq.(44), Eq.(49) and Eq.(54) together, we know that

\[e^{2}(\mathcal{H}_{k};\mathcal{P})=\sum_{\bm{k}\in\mathbb{Z}^{d},\bm{k}\neq\bm {0}}\gamma_{\alpha}(\bm{k})\frac{1}{n}\sum_{j=1}^{n}\exp\left(\frac{2\pi \mathrm{i}\bm{j}\bm{k}^{\top}\bm{z}}{n}\right)\] (55)

Note that for a prime number \(n\), we have

\[\frac{1}{n}\sum_{j=1}^{n}\exp\left(\frac{2\pi\mathrm{i}\bm{j}\bm{k}^{\top}\bm{z }}{n}\right)=\begin{cases}1&\text{if }\bm{k}^{\top}\bm{z}\equiv 0\text{ mod }n\\ 0&\text{otherwise}\end{cases}\] (56)

It follows that

\[e^{2}(\mathcal{H}_{k};\mathcal{P})=\sup_{f\in\mathcal{H}_{k},\|f\|_{\mathcal{H} _{k}}\leq 1}\left|\int_{[0,1]^{d}}f(\bm{x})\mathrm{d}\bm{x}-\frac{1}{n}\sum_{j=0}^{n-1 }f\left(\bm{x}_{j}\right)\right|^{2}=\sum_{\bm{k}\in L^{\perp}\setminus\{\bm{0 }\}}\gamma_{\alpha}(\bm{k}),\] (57)where \(L^{\perp}:=\{\bm{k}|\bm{k}^{\top}\bm{z}\equiv 0\pmod{n},\bm{k}\in\mathbb{Z}^{d}\}\) denotes the dual lattice.

**Lemma 2**.: _Given a prime \(n\), construct a rank-1 lattice \(\mathcal{P}=[\bm{x}_{1},\cdots,\bm{x}_{n}]\) by the generating vector \(\bm{z}=[z_{1},\cdots,z_{d}]\), then we have that_

\[e^{2}(\mathcal{H}_{k};\mathcal{P})=-1+\frac{1}{n}\sum_{j=0}^{n-1} \prod_{i=1}^{d}\big{(}\sum_{k_{i}\in\{1,\cdots,n\}}\chi(k_{i})\exp\left(2\pi \mathrm{i}\frac{k_{i}jz_{i}}{n}\right)\big{)},\] (58)

_where function \(\chi(\cdot)\) on domain \(\{1,\cdots,n\}\) is given as Eq.(59)_

\[\chi(k_{i})=\begin{cases}1+\frac{2}{n^{\alpha}}\zeta(\alpha,1)&\text{if }k_{i}=n\\ \frac{1}{n^{\alpha}}\big{(}\zeta(\alpha,\frac{k_{i}}{n})+\zeta(\alpha,\frac{ n-k_{i}}{n})\big{)}&\text{otherwise}\end{cases},\] (59)

_where \(\zeta(\cdot,\cdot)\) denotes the Hurwitz zeta function._

Proof.: From Lemma 1, we know that

\[e^{2}(\mathcal{H}_{k};\mathcal{P}) =\sum_{\bm{k}\in\mathbb{Z}^{d}\setminus\{\bm{0}\}}\gamma_{\alpha }(\bm{k})\left(\frac{1}{n}\sum_{j=0}^{n-1}\exp\left(2\pi\mathrm{i}\frac{\bm{k} ^{\top}\bm{x}_{j}}{n}\right)\right)\] (60) \[=-1+\frac{1}{n}\sum_{j=0}^{n-1}\sum_{\bm{k}\in\mathbb{Z}^{d}} \gamma_{\alpha}(\bm{k})\exp\left(2\pi\mathrm{i}\frac{\bm{k}^{\top}\bm{x}_{j}} {n}\right)\] (61) \[=-1+\frac{1}{n}\sum_{j=0}^{n-1}\prod_{i=1}^{d}\big{(}\sum_{k_{i} \in\mathbb{Z}}\gamma_{\alpha}(k_{i})\exp\left(2\pi\mathrm{i}\frac{k_{i}jz_{i} }{n}\right)\big{)}\] (62) \[=-1+\frac{1}{n}\sum_{j=0}^{n-1}\prod_{i=1}^{d}\big{(}\sum_{k_{i} \in\{1,\cdots,n\}}\big{(}\sum_{q_{i}\equiv k_{i}\text{ mod }n}\gamma_{\alpha}(q_{i})\big{)}\exp\left(2\pi \mathrm{i}\frac{k_{i}jz_{i}}{n}\right)\big{)}\] (63)

Now, we check the term \(\sum_{k_{i}\in\{1,\cdots,n\}}\big{(}\sum_{q_{i}\equiv k_{i}\text{ mod }n}\gamma_{\alpha}(q_{i})\big{)}\). From the definition of the function \(\gamma_{\alpha}(\cdot)\), for \(\forall k_{i}\in\{1,\cdots,n\}\), we have that

\[\chi(k_{i})=\sum_{q_{i}\equiv k_{i}\text{ mod }n}\gamma_{\alpha}(q_{i})= \begin{cases}1+2\sum_{m=1}^{\infty}\frac{1}{(mn)^{\alpha}}&\text{if }k_{i}=n\\ \sum_{m=0}^{\infty}\frac{1}{(k_{i}+mn)^{\alpha}}+\sum_{m=0}^{\infty}\frac{1}{( n-k_{i}+mn)^{\alpha}}&\text{otherwise}\end{cases}\] (64)

Note that series \(\sum_{m=1}^{\infty}\frac{1}{(mn)^{\alpha}}\), \(\sum_{m=0}^{\infty}\frac{1}{(k_{i}+mn)^{\alpha}}\) and \(\sum_{m=0}^{\infty}\frac{1}{(n-k_{i}+mn)^{\alpha}}\) can be rewritten as

\[\sum_{m=1}^{\infty}\frac{1}{(mn)^{\alpha}}=\frac{1}{n^{\alpha}} \sum_{m=1}^{\infty}\frac{1}{m^{\alpha}}=\frac{1}{n^{\alpha}}\zeta(\alpha,1)\] (65) \[\sum_{m=0}^{\infty}\frac{1}{(k_{i}+mn)^{\alpha}}=\frac{1}{n^{ \alpha}}\sum_{m=0}^{\infty}\frac{1}{(\frac{k_{i}}{n}+m)^{\alpha}}=\frac{1}{n^ {\alpha}}\zeta(\alpha,\frac{k_{i}}{n})\] (66) \[\sum_{m=0}^{\infty}\frac{1}{(n-k_{i}+mn)^{\alpha}}=\frac{1}{n^{ \alpha}}\sum_{m=0}^{\infty}\frac{1}{(\frac{n-k_{i}}{n}+m)^{\alpha}}=\frac{1}{n^ {\alpha}}\zeta(\alpha,\frac{n-k_{i}}{n})\] (67)

where \(\zeta(\cdot,\cdot)\) denotes the Hurwitz zeta function.

Plug them into Eq.(64), we know that

\[\chi(k_{i})=\sum_{q_{i}\equiv k_{i}\text{ mod }n}\gamma_{\alpha}(q_{i})= \begin{cases}1+\frac{2}{n^{\alpha}}\zeta(\alpha,1)&\text{if }k_{i}=n\\ \frac{1}{n^{\alpha}}\big{(}\zeta(\alpha,\frac{k_{i}}{n})+\zeta(\alpha,\frac{n-k _{i}}{n})\big{)}&\text{otherwise}\end{cases}\] (68)

Plug Eq.(68) into Eq.(63), we have that

\[e^{2}(\mathcal{H}_{k};\mathcal{P})=-1+\frac{1}{n}\sum_{j=0}^{n-1} \prod_{i=1}^{d}\big{(}\sum_{k_{i}\in\{1,\cdots,n\}}\chi(k_{i})\exp\left(2\pi \mathrm{i}\frac{k_{i}jz_{i}}{n}\right)\big{)}\] (69)

**Lemma 3**.: _Let \(n\) be a prime number. Let \(\bm{\gamma}=[\gamma_{1},\cdots,\gamma_{n}]^{\top}\) be a vector with \(\gamma_{k}=\chi(k)\) for \(k\in\{1,\cdots,n\}\), where \(\chi(\cdot)\) is defined in Lemma 2. The square worst-case integral approximation error of rank-1 lattice \(\mathcal{P}\) constructed by generating vector \(\bm{z}=[z_{1},\cdots,z_{d}]\) can be rewritten in a matrix form as Eq.(70)_

\[e^{2}(\mathcal{H}_{k};\mathcal{P})=\frac{1}{n}\bm{1}^{\top}\left(\bm{h}^{0} \odot\cdots\odot\bm{h}^{d-1}-\bm{1}\right)\] (70)

_where \(\odot\) denotes the element-wise product, symbol \(\bm{1}\) denotes the vector with elements all ones, and \(\bm{h}^{i}=\bm{F}^{i}\bm{\gamma}\) with \(\bm{F}\) as the discrete Fourier matrix, i.e., \(\bm{F}_{jk}=\exp(2\pi\mathbf{i}\frac{jk}{n})\), and \(\bm{F}^{i}\) denotes the matrix after permutation of the rows of \(\bm{F}\) such that the \(j^{th}\) row of \(\bm{F}^{i}\) equals to the \(\widetilde{j}^{th}\) row of \(\bm{F}\), where \(\widetilde{j}=jz_{i+1}\) mod \(n\)._

Proof.: Define \(\bm{h}^{i}\) as Eq.(71)

\[\bm{h}^{i}=\bm{F}^{i}\bm{\gamma}\] (71)

where \(\bm{F}\) as the discrete Fourier matrix, i.e., \(\bm{F}_{jk}=\exp(2\pi\mathbf{i}\frac{jk}{n})\), and \(\bm{F}^{i}\) denotes the matrix after permutation of the rows of \(\bm{F}\) such that the \(j^{th}\) row of \(\bm{F}^{i}\) equals to the \(\widetilde{j}^{th}\) row of \(\bm{F}\), where \(\widetilde{j}=jz_{i+1}\) mod \(n\), and \(g\) denotes the primitive root modulo \(n\).

From Lemma 2, we know that

\[e^{2}(\mathcal{H}_{k};\mathcal{P})=-1+\frac{1}{n}\sum_{j=0}^{n-1}\prod_{i=1}^{ d}\left(\sum_{k_{i}\in\{1,\cdots,n\}}\chi(k_{i})\exp\left(2\pi\mathbf{i}\frac{k_{ i}jz_{i}}{n}\right)\right)\] (72)

Note that \(\bm{\gamma}=[\gamma_{1},\cdots,\gamma_{n}]^{\top}\) is a vector with \(\gamma_{k}=\chi(k)\) for \(k\in\{1,\cdots,n\}\), it follows that

\[e^{2}(\mathcal{H}_{k};\mathcal{P}) =-1+\frac{1}{n}\bm{1}^{\top}\left(\bm{F}^{0}\bm{\gamma}\odot \cdots\odot\bm{F}^{d-1}\bm{\gamma}\right)\] (73) \[=-1+\frac{1}{n}\bm{1}^{\top}\left(\bm{h}^{0}\odot\cdots\odot\bm{ h}^{d-1}\right)\] (74) \[=\frac{1}{n}\bm{1}^{\top}\left(\bm{h}^{0}\odot\cdots\odot\bm{h}^{ d-1}-\bm{1}\right)\] (75)

**Lemma 4**.: _Let \(n\) be a prime number such that \((2d-1)|(n-1)\). Let \(\bm{\gamma}=[\gamma_{1},\cdots,\gamma_{n}]^{\top}\) be a vector with \(\gamma_{k}=\chi(k)\) for \(k\in\{1,\cdots,n\}\), where \(\chi(\cdot)\) is defined in Lemma 2. Given a rank-1 lattice \(\mathcal{P}\) constructed by generating vector in Eq.(30), then we have Eq.(76)_

\[\bm{1}^{\top}(\bm{h}^{0}\odot\cdots\odot\bm{h}^{d-1}-\bm{1}) =\bm{1}^{\top}(\bm{h}^{d}\odot\cdots\odot\bm{h}^{2d-2}-\bm{1})+ \left\langle\bm{h}^{d}\odot\cdots\odot\bm{h}^{2d-2}-\bm{1},\bm{h}^{0}-\bm{1}\right\rangle\] \[+\bm{1}^{\top}(\bm{h}^{0}-\bm{1})\] (76)

_where \(\odot\) denotes the element-wise product, symbol \(\bm{1}\) denotes the vector with elements all ones, and \(\bm{h}^{i}=\bm{F}^{i}\bm{\gamma}\) with \(\bm{F}\) as the discrete Fourier matrix, i.e., \(\bm{F}_{jk}=\exp(2\pi\mathbf{i}\frac{jk}{n})\), and \(\bm{F}^{i}\) denotes the matrix after permutation of the rows of \(\bm{F}\) such that the \(j^{th}\) row of \(\bm{F}^{i}\) equals to the \(\widetilde{j}^{th}\) row of \(\bm{F}\), where \(\widetilde{j}=jg^{\frac{i(n-1)}{2d-1}}\) mod \(n\), and \(g\) denotes the primitive root modulo \(n\)._

Proof.: Note that \(\bm{h}^{i}=\bm{F}^{i}\bm{\gamma}\) is a permutation of \(\bm{h}^{0}\). From the definition of permutation \(\bm{F}^{i}\), we know that the \(j^{th}\) row of \(\bm{F}^{i}\) equals to the \(\widetilde{j}^{th}\) row of \(\bm{F}\) with \(\widetilde{j}=jg^{\frac{i(n-1)}{2d-1}}\) mod \(n\). Note that \((2d-1)|(n-1)\) and \(n\) is a prime number, we know \(\{1,g^{\frac{1(n-1)}{2d-1}},\cdots,g^{\frac{(2d-2)(n-1)}{2d-1}}\}\) modulo \(n\) forms a subgroup of \(\{1,\cdots,n-1\}\) modulo \(n\). Thus, we know \(\{\bm{h}^{0},\bm{h}^{1},\cdots,\bm{h}^{2d-2}\}\) forms a group, and \(\bm{h}^{0}=\bm{h}^{2d-1}\). Furthermore, we know that \(\bm{h}^{k}\) is a permutation of \(\bm{h}^{i}\) such that \(j^{th}\) row of \(\bm{F}^{k}\) equals to the \(\widetilde{j}^{th}\) row of \(\bm{F}^{i}\) with \(\widetilde{j}=jg^{\frac{(k-1)(n-1)}{2d-1}}\) mod \(n\). Thus, we know that

\[\bm{1}^{\top}(\bm{h}^{0}\odot\cdots\odot\bm{h}^{d-1})=\bm{1}^{\top}(\bm{h}^{d} \odot\cdots\odot\bm{h}^{2d-1})\] (77)Note that \(\bm{h}^{0}=\bm{h}^{2d-1}\). It follows that

\[\bm{1}^{\top}(\bm{h}^{0}\odot\cdots\odot\bm{h}^{2d-2}\odot\bm{h}^{0}- \bm{1}) =\left\langle\bm{h}^{d}\odot\cdots\odot\bm{h}^{2d-2}-\bm{1},\bm{h}^ {0}-\bm{1}\right\rangle+\bm{1}^{\top}(\bm{h}^{d}\odot\cdots\odot\bm{h}^{2d-2}- \bm{1})\] \[+\bm{1}^{\top}(\bm{h}^{0}-\bm{1})\] (85)

Plug Eq.(85) into Eq.(79), we know that

\[\bm{1}^{\top}(\bm{h}^{0}\odot\cdots\odot\bm{h}^{d-1}-\bm{1}) =\left\langle\bm{h}^{d}\odot\cdots\odot\bm{h}^{2d-2}-\bm{1},\bm{h }^{0}-\bm{1}\right\rangle+\bm{1}^{\top}(\bm{h}^{d}\odot\cdots\odot\bm{h}^{2d- 2}-\bm{1})\] \[+\bm{1}^{\top}(\bm{h}^{0}-\bm{1})\] (86)

**Lemma 5**.: _Let \(n\) be a prime number such that \((2d-1)|(n-1)\). Let \(\bm{\gamma}=[\gamma_{1},\cdots,\gamma_{n}]^{\top}\) be a vector with \(\gamma_{k}=\chi(k)\) for \(k\in\{1,\cdots,n\}\), where \(\chi(\cdot)\) is defined in Lemma 2. Given a rank-1 lattice \(\mathcal{P}\) constructed by generating vector in Eq.(30), then we have Eq.(87)_

\[\bm{1}^{\top}(\bm{h}^{0}\odot\cdots\odot\bm{h}^{2d-2}-\bm{1}) =\bm{1}^{\top}(\bm{h}^{0}\odot\cdots\odot\bm{h}^{d-1}-\bm{1})+ \bm{1}^{\top}(\bm{h}^{d}\odot\cdots\odot\bm{h}^{2d-2}-\bm{1})\] \[+\left\langle\bm{h}^{0}\odot\cdots\odot\bm{h}^{d-1}-\bm{1},\bm{h }^{d}\odot\cdots\odot\bm{h}^{2d-2}-\bm{1}\right\rangle\] (87)

_where \(\odot\) denotes the element-wise product, symbol \(\bm{1}\) denotes the vector with elements all ones, and \(\bm{h}^{i}=\bm{F}^{i}\bm{\gamma}\) with \(\bm{F}\) as the discrete Fourier matrix, i.e., \(\bm{F}_{jk}=\exp(2\pi\bm{1}\bm{1}_{n}^{jk})\), and \(\bm{F}^{i}\) denotes the matrix after permutation of the rows of \(\bm{F}\) such that the \(j^{th}\) row of \(\bm{F}^{i}\) equals to the \(\widetilde{j}^{th}\) row of \(\bm{F}\), where \(\widetilde{j}=jg^{\frac{i(n-1)}{2d-1}}\) mod \(n\), and \(g\) denotes the primitive root modulo \(n\)._

Proof.: Similar to the proof of Lemma 4, we have that

\[\left\langle\bm{h}^{0}\odot\cdots\odot\bm{h}^{d-1}-\bm{1},\bm{h}^ {d}\odot\cdots\odot\bm{h}^{2d-2}-\bm{1}\right\rangle\] (88) \[=\left\langle\bm{h}^{0}\odot\cdots\odot\bm{h}^{d-1},\bm{h}^{d} \odot\cdots\odot\bm{h}^{2d-2}\right\rangle-\bm{1}^{\top}(\bm{h}^{0}\odot \cdots\odot\bm{h}^{d-1})-\bm{1}^{\top}(\bm{h}^{d}\odot\cdots\odot\bm{h}^{2d- 2})+\bm{1}^{\top}\bm{1}\] (89) \[=\bm{1}^{\top}(\bm{h}^{0}\odot\cdots\odot\bm{h}^{2d-2})-\bm{1}^{ \top}\bm{1}-\bm{1}^{\top}(\bm{h}^{0}\odot\cdots\odot\bm{h}^{d-1})+\bm{1}^{ \top}\bm{1}-\bm{1}^{\top}(\bm{h}^{d}\odot\cdots\odot\bm{h}^{2d-2})+\bm{1}^{ \top}\bm{1}\] (91) \[=\bm{1}^{\top}(\bm{h}^{0}\odot\cdots\odot\bm{h}^{2d-2}-\bm{1})- \bm{1}^{\top}(\bm{h}^{0}\odot\cdots\odot\bm{h}^{d-1}-\bm{1})-\bm{1}^{\top}(\bm {h}^{d}\odot\cdots\odot\bm{h}^{2d-2}-\bm{1})\] (92)

It follows that

\[\bm{1}^{\top}(\bm{h}^{0}\odot\cdots\odot\bm{h}^{2d-2}-\bm{1}) =\bm{1}^{\top}(\bm{h}^{0}\odot\cdots\odot\bm{h}^{d-1}-\bm{1})+ \bm{1}^{\top}(\bm{h}^{d}\odot\cdots\odot\bm{h}^{2d-2}-\bm{1})\] \[+\left\langle\bm{h}^{0}\odot\cdots\odot\bm{h}^{d-1}-\bm{1},\bm{h }^{d}\odot\cdots\odot\bm{h}^{2d-2}-\bm{1}\right\rangle\] (93)

**Lemma 6**.: _Let \(n\) be a prime number such that \((2d-1)|(n-1)\). Let \(\bm{\gamma}=[\gamma_{1},\cdots,\gamma_{n}]^{\top}\) be a vector with \(\gamma_{k}=\chi(k)\) for \(k\in\{1,\cdots,n\},\) where \(\chi(\cdot)\) is defined in Lemma 2. Given a rank-1 lattice \(\mathcal{P}\) constructed by generating vector in Eq.(30), then we have Eq.(94)_

\[\bm{1}^{\top}(\bm{h}^{0}\odot\cdots\odot\bm{h}^{2d-2}-\bm{1}) =\bm{1}^{\top}\big{(}(\bm{h}^{1}\odot\cdots\odot\bm{h}^{d-1}-\bm{ 1})\odot\bm{h}^{0}\odot(\bm{h}^{-(d-1)}\odot\cdots\odot\bm{h}^{-1}-\bm{1}) \big{)}\] \[+2\bm{1}^{\top}(\bm{h}^{0}\odot\cdots\odot\bm{h}^{d-1}-\bm{1})- \bm{1}^{\top}(\bm{h}^{0}-\bm{1})\] (94)

_where \(\odot\) denotes the element-wise product, symbol \(\bm{1}\) denotes the vector with elements all ones, and \(\bm{h}^{i}=\bm{F}^{i}\bm{\gamma}\) with \(\bm{F}\) as the discrete Fourier matrix, i.e., \(\bm{F}_{jk}=\exp(2\pi\mathrm{i}\frac{jk}{n})\), and \(\bm{F}^{i}\) denotes the matrix after permutation of the rows of \(\bm{F}\) such that the \(j^{th}\) row of \(\bm{F}^{i}\) equals to the \(\widetilde{j}^{th}\) row of \(\bm{F}\), where \(\widetilde{j}=jg^{\frac{i(n-1)}{2d-1}}\) mod \(n\), and \(g\) denotes the primitive root modulo \(n\)._

Proof.: Plug Eq.(76) in Lemma 4 into Eq.(87) in Lemma 5, we know that

\[\bm{1}^{\top}(\bm{h}^{0}\odot\cdots\odot\bm{h}^{2d-2}-\bm{1}) =2\bm{1}^{\top}(\bm{h}^{0}\odot\cdots\odot\bm{h}^{d-1}-\bm{1})- \bm{1}^{\top}(\bm{h}^{0}-\bm{1})-\langle\bm{h}^{d}\odot\cdots\odot\bm{h}^{2d -2}-\bm{1},\bm{h}^{0}-\bm{1}\rangle\] \[+\big{\langle}\bm{h}^{0}\odot\cdots\odot\bm{h}^{d-1}-\bm{1},\bm{ h}^{d}\odot\cdots\odot\bm{h}^{2d-2}-\bm{1}\big{\rangle}\] (95)

Now we check the last two terms in Eq.(95). Note that

\[\big{\langle}\bm{h}^{0}\odot\cdots\odot\bm{h}^{d-1}-\bm{1},\bm{h} ^{d}\odot\cdots\odot\bm{h}^{2d-2}-\bm{1}\big{\rangle}-\big{\langle}\bm{h}^{d} \odot\cdots\odot\bm{h}^{2d-2}-\bm{1},\bm{h}^{0}-\bm{1}\big{\rangle}\] (96) \[=\big{\langle}\bm{h}^{0}\odot\cdots\odot\bm{h}^{d-1}-\bm{1}-(\bm{ h}^{0}-\bm{1}),\bm{h}^{d}\odot\cdots\odot\bm{h}^{2d-2}-\bm{1}\big{\rangle}\] (97) \[=\big{\langle}\bm{h}^{0}\odot\cdots\odot\bm{h}^{d-1}-\bm{h}^{0}, \bm{h}^{d}\odot\cdots\odot\bm{h}^{2d-2}-\bm{1}\big{\rangle}\] (98) \[=\big{\langle}\bm{h}^{0}\odot(\bm{h}^{1}\odot\cdots\odot\bm{h}^{ d-1}-\bm{1}),\bm{h}^{d}\odot\cdots\odot\bm{h}^{2d-2}-\bm{1}\big{\rangle}\] (99) \[=\bm{1}^{\top}\big{(}(\bm{h}^{1}\odot\cdots\odot\bm{h}^{d-1}-\bm {1})\odot\bm{h}^{0}\odot(\bm{h}^{d}\odot\cdots\odot\bm{h}^{2d-2}-\bm{1}) \big{)}\] (100)

It follows that

\[\bm{1}^{\top}(\bm{h}^{0}\odot\cdots\odot\bm{h}^{2d-2}-\bm{1}) =2\bm{1}^{\top}(\bm{h}^{0}\odot\cdots\odot\bm{h}^{d-1}-\bm{1})- \bm{1}^{\top}(\bm{h}^{0}-\bm{1})\] \[+\bm{1}^{\top}\big{(}(\bm{h}^{1}\odot\cdots\odot\bm{h}^{d-1}-\bm {1})\odot\bm{h}^{0}\odot(\bm{h}^{d}\odot\cdots\odot\bm{h}^{2d-2}-\bm{1})\big{)}\] (101)

Because \(\{\bm{h}^{0},\bm{h}^{1},\cdots,\bm{h}^{2d-2}\}\) forms a group, and \(\bm{h}^{0}=\bm{h}^{2d-1}\) with a modulo period \(2d-1\), we know that

\[\bm{h}^{d}\odot\cdots\odot\bm{h}^{2d-2}=\bm{h}^{-(d-1)}\odot\cdots\odot\bm{h }^{-1}\] (102)

Plug Eq.(102) into Eq.(101), we have that

\[\bm{1}^{\top}(\bm{h}^{0}\odot\cdots\odot\bm{h}^{2d-2}-\bm{1}) =2\bm{1}^{\top}(\bm{h}^{0}\odot\cdots\odot\bm{h}^{d-1}-\bm{1})- \bm{1}^{\top}(\bm{h}^{0}-\bm{1})\] \[+\bm{1}^{\top}\big{(}(\bm{h}^{1}\odot\cdots\odot\bm{h}^{d-1}-\bm{ 1})\odot\bm{h}^{0}\odot(\bm{h}^{-d-1}\odot\cdots\odot\bm{h}^{-1}-\bm{1})\big{)}\] (103)

Now, we are ready to prove our main Theorem 1.

Proof.: From Lemma 3, we know that

\[e^{2}(\mathcal{H}_{k};\mathcal{P})=\frac{1}{n}\bm{1}^{\top}\left(\bm{h}^{0} \odot\cdots\odot\bm{h}^{d-1}-\bm{1}\right)\] (104)

From Lemma 6, we know that

\[\bm{1}^{\top}(\bm{h}^{0}\odot\cdots\odot\bm{h}^{d-1}-\bm{1})\] \[=\frac{1}{2}\bm{1}^{\top}\big{(}\bm{h}^{0}\odot\cdots\odot\bm{h} ^{2d-2}-\bm{1}-(\bm{h}^{1}\odot\cdots\odot\bm{h}^{d-1}-\bm{1})\odot\bm{h}^{0} \odot(\bm{h}^{-1}\odot\cdots\odot\bm{h}^{-(d-1)}-\bm{1})\big{)}\] \[+\frac{1}{2}\bm{1}^{\top}(\bm{h}^{0}-\bm{1})\] (105)Plug Eq.(105) into Eq.(104), we have that

\[e^{2}(\mathcal{H}_{k};\mathcal{P})\] \[=\frac{1}{2n}\mathbf{1}^{\top}\big{(}\boldsymbol{h}^{0}\odot\cdots \odot\boldsymbol{h}^{2d-2}\!-\!\mathbf{1}-(\boldsymbol{h}^{1}\odot\cdots\odot \boldsymbol{h}^{d-1}\!-\!\mathbf{1})\odot\boldsymbol{h}^{0}\odot(\boldsymbol {h}^{-1}\odot\cdots\odot\boldsymbol{h}^{-(d-1)}\!-\!\mathbf{1})\big{)}\] \[+\frac{1}{2n}\mathbf{1}^{\top}(\boldsymbol{h}^{0}-\mathbf{1})\] (106)

Note that \(\boldsymbol{h}^{0}=\boldsymbol{F}\boldsymbol{\gamma}\) and \(\boldsymbol{F}\) denotes the discrete Fourier matrix, we have that

\[\mathbf{1}^{\top}(\boldsymbol{h}^{0}-\mathbf{1}) =\mathbf{1}^{\top}\boldsymbol{F}\boldsymbol{\gamma}-n\] (107) \[=\boldsymbol{b}^{\top}\boldsymbol{\gamma}-n\] (108)

where \(\boldsymbol{b}=[0,0,\cdots,0,n]^{\top}\).

Note that the \(n^{th}\) element in \(\boldsymbol{\gamma}\) is \(\gamma_{n}=1+\frac{2}{n^{\alpha}}\zeta(\alpha,1)\), where \(\zeta(\cdot,\cdot)\) denotes the Hurwitz zeta function. It follows that

\[\mathbf{1}^{\top}(\boldsymbol{h}^{0}-\mathbf{1})=\boldsymbol{b}^{\top} \boldsymbol{\gamma}-n=n+n\frac{2}{n^{\alpha}}\zeta(\alpha,1)-n=n\frac{2}{n^{ \alpha}}\zeta(\alpha,1)\] (109)

Plug Eq.(109) into Eq.(106), we achieve the result in Theorem 1

\[e^{2}(\mathcal{H}_{k};\mathcal{P})\] \[=\frac{1}{2n}\mathbf{1}^{\top}\big{(}\boldsymbol{h}^{0}\odot \cdots\odot\boldsymbol{h}^{2d-2}\!-\!\mathbf{1}-(\boldsymbol{h}^{1}\odot \cdots\odot\boldsymbol{h}^{d-1}\!-\!\mathbf{1})\odot\boldsymbol{h}^{0}\odot( \boldsymbol{h}^{-1}\odot\cdots\odot\boldsymbol{h}^{-(d-1)}\!-\!\mathbf{1}) \big{)}\] \[+\frac{1}{n^{\alpha}}\zeta(\alpha,1)\] (110)

## Appendix B Benchmark Test Functions

The benchmark test functions employed in section 4.1 are listed in Table 2, which contains multi-mode functions and non-smooth functions that are challenging for optimization.

\begin{table}
\begin{tabular}{l l} \hline name & function \\ \hline Rosenbrock & \(\sum\limits_{i=1}^{d-1}\left(100(x_{i+1}-x_{i}^{2})^{2}+(1-x_{i})^{2}\right)\) \\ Nesterov & \(\frac{1}{4}\left|x_{1}-1\right|+\sum\limits_{i=1}^{d-1}\left|x_{i+1}-2\left|x_{ i}\right|+1\right|\) \\ Rastrigin & \(10d+\sum_{i=1}^{d}\left(x_{i}^{2}-10\cos\left(2\pi x_{i}\right)\right)\) \\ \hline \end{tabular}
\end{table}
Table 2: Test functionsTraining Time and Fast Coordinate Search Time

We provide the training time of our rank- 1 lattice GP and the time of our fast coordinate search for targeted sampling in Figure 5(a) and Figure 5(b), respectively. The dimension of the rank-1 lattice data is set to \(d=50\). The number of samples \(n\) is set to the parameter in \(\{1783,5347,10099,51283,100189,501139,1000099\}\). The number of samples \(n\) is a prime number such that \((2d-1)|(n-1)\) to achieve our closed-form rank-1 lattice construction. The number of epochs of training is set to \(2000\). The number of iterations of fast coordinate search is set to \(T=50\). All the experiments are performed in 50 runs on a single NVIDIA A40 Card.

We report the mean value \(\pm\) std in Figure 5. The standard deviation of the time is small. From Figure 5(a), we can see that it takes around 50 seconds for our rank-1 GP training with one million lattice data. Moreover, our fast coordinate search for targeted sampling takes around 1.5 seconds to optimize rank-1 lattice GP posterior prediction conditioned on one million lattice data.

## Appendix D Additional Experiments of Black-box Prompt Fine-tuning

Figure 5: Training Time and Fast Coordinate Search Time (seconds) v.s. the number of samples

Figure 6: Hinge loss v.s. the number of query evaluations on different black-box fine-tuning models.

We provide additional experimental results of black-box prompt fine-tuning for large language models. We employ the deep model in [14] as the backbone. It has \(24\) layers. For each layer, we set the dimension of the continuous prompt to \(50\). Thus, the total dimension is \(24\times 50\). We employ the hinge loss of training data as the black-box objective.

In all the experiments, we keep the number of batch samples and the initialization the same for RLTS, INGO and CMAES. We set the number of batch samples to \(200\). Our RLTS employs \(199\) rank-1 lattice QMC Gaussian samples and one sample from targeted sampling. INGO employs \(199\) rank-1 lattice QMC Gaussian samples and one Gaussian sample. CMAES employs \(200\) Gaussian samples. We initialize the \(\bm{\mu}=\bm{0}\) for all the methods. For INGO and RLTS, we set the step-size parameter \(\beta=0.2\) in all experiments. For RLTS, we set the parameter \(\eta=1\) in all experiments. All the experiments are performed in five independent runs with seeds in \(\{1,2,3,4,5\}\).

The experimental results of mean objective \(\pm\) std v.s. the number of queries are shown in Figure 6. From Figure 6, we can observe that RLTS decreases the loss consistently faster than INGO and CMAES on all benchmark datasets. More importantly, RLTS decreases the loss significantly faster than INGO. Note that RLTS employs INGO as the backbone algorithm, which shows that RLTS improves the query efficiency of INGO.