# Make Your LLM Fully Utilize the Context

Shengnan An\({}^{\diamond,\clubsuit}\), Zexiong Ma\({}^{\diamond,\clubsuit}\), Zeqi Lin\({}^{\dagger,\clubsuit}\),

**Nanning Zheng\({}^{\dagger}\)\({}^{\diamond}\), Jian-Guang Lou\({}^{\clubsuit}\), Weizhu Chen\({}^{\clubsuit}\)**

\({}^{\diamond}\)National Key Laboratory of Human-Machine Hybrid Augmented Intelligence,

National Engineering Research Center of Visual Information and Applications,

Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University

\({}^{\clubsuit}\)Microsoft \({}^{\diamond}\)Peking University

\({}^{\diamond}\){an1006634493@stu,nnzheng@mail}.xjtu.edu.cn,

\({}^{\diamond}\)maexiong@stu.pku.edu.cn, \({}^{\clubsuit}\){Zeqi.Lin,jlou,wzchen}@microsoft.com

Work done during the internship at Microsoft.Corresponding authors.

###### Abstract

While many contemporary large language models (LLMs) can process lengthy input, they still struggle to fully utilize information within the long context, known as the _lost-in-the-middle_ challenge. We hypothesize that it stems from insufficient explicit supervision during the long-context training, which fails to emphasize that any position in a long context can hold crucial information. Based on this intuition, our study presents **information-intensive (In2) training**, a purely data-driven solution to overcome lost-in-the-middle. Specifically, In2 training leverages a synthesized long-context question-answer dataset, where the answer requires (1) **fine-grained information awareness** on a short segment (\(\sim\)128 tokens) within a synthesized long context (4K\(-\)32K tokens), and (2) the **integration and reasoning** of information from two or more short segments. Through applying this information-intensive training on Mistral-7B, we present **FilM-7B** (**FILI-**in-the-Middle). To thoroughly assess the ability of **FilM-7B** for utilizing long contexts, we design three probing tasks that encompass various context styles (document, code, and structured-data context) and information retrieval patterns (forward, backward, and bi-directional retrieval). The probing results demonstrate that **FilM-7B** can robustly retrieve information from different positions in its 32K context window. Beyond these probing tasks, **FilM-7B** significantly improves the performance on real-world long-context tasks (e.g., 23.5\(\rightarrow\)26.9 F1 score on NarrativeQA), while maintaining a comparable performance on short-context tasks (e.g., 59.3\(\rightarrow\)59.2 accuracy on MMLU).

## 1 Introduction

_To a great mind, nothing is little._

_--Arthur Conan Doyle_

Long-context large language models (LLMs) have recently received significant attention within the open-source community (Jiang et al., 2023; Du et al., 2022; Li et al., 2023; Shi et al., 2023; Team et al., 2023; Team, 2023; Chen et al., 2023; Song et al., 2023; Liu et al., 2023; Peng et al., 2023; Chen et al., 2023; Xiong et al., 2023; Tworkowski et al., 2024; AI et al., 2024; Ding et al., 2024; Mohtashami & Jaggi, 2024; Fu et al., 2024; Cai et al., 2024; Bai et al., 2024; Lv et al., 2024). The training context windows of many contemporary LLMs have been expanded to tens of thousands oftokens, thereby enabling these models to process extensive context as input. This extended training context window can enhance many real-world downstream tasks such as long-context question answering (Kocisky et al., 2018; Dasigi et al., 2021; Bai et al., 2023) and summarization (Fabbri et al., 2019; Huang et al., 2021; Zhong et al., 2021).

However, recent studies have revealed that these long-context LLMs struggle to effectively and robustly utilize all the information provided in the context, known as the _lost-in-the-middle_ challenge (Liu et al., 2024; Xu et al., 2023). It implies that while the LLM can comprehend the information at the beginning and end of the long context, it often overlooks the information in the middle. This challenge could significantly hinder the development of long-context LLMs, as they even often fail to pass simple probing tasks such as Needle-in-the-Haystack and passkey retrieval (Mothashami and Jaggi, 2024). Consequently, a pressing research question arises: _how can we make long-context LLMs fully utilize the information in the long context?_

We hypothesize that the root cause of lost-in-the-middle stems from the unintentional bias hidden in the general training data. In auto-regressive pre-training, the loss on predicting the next token is more likely to be influenced by a few nearby pre-tokens rather than long-distance tokens (Sharan et al., 2018; Sun et al., 2021). For supervised fine-tuning and alignment, the system message, which strongly influences the generation of the response, is typically presented at the beginning of the context (Touvron et al., 2023; Cai et al., 2024). As a result, the general training process may inadvertently introduce a position bias, suggesting that important information is always located at the beginning and end of the context.

Based on this hypothesis, our work introduces **information-intensive (N2) training** to explicitly teach the model that **the crucial information can be intensively present throughout the context**, not just at the beginning and end. In2 training is a purely data-driven solution that utilizes a synthesized long-context question-answer dataset. The long context (ranging from 4K to 32K tokens) is concatenated from many short segments (\(\sim\)128 tokens), and the question-answer (QA) pairs ask for the information contained in one or more segments which are _randomly_ placed in the long context. Specifically, we generate two types of questions, requiring (1) **fine-grained information awareness** on exactly one short segment, and (2) the **integration and reasoning of information** from two or more segments. These QA pairs are generated by prompting GPT-4-Turbo (OpenAI, 2023b) with the designed instructions and the raw segments.

By applying this information-intensive training on Mistral-7B (Jiang et al., 2023), we present **FiLM-7B** (**FILI**-in-the-**M**iddle). To thoroughly assess the long-context information awareness of FilM-7B, we design three probing tasks encompassing various context styles (document, code, and structured-data context) and information retrieval patterns (forward, backward, and bi-directional retrieval). The probing results (Figure 1) demonstrate that In2 training significantly overcomes the lost-in-the-middle problem for the backbone model. Moreover, it can enhance the open-source model to achieve comparable or even more robust performance compared with proprietary LLMs such as GPT-4-Turbo.

Beyond these probing tasks, the performance of FilM-7B on real-world long-context tasks also exhibits significant improvements (e.g., 23.5\(\rightarrow\)26.9 F1 score on NarrativeQA (Kocisky et al., 2018)). This demonstrates that the post-training on synthesized long-context data can be generalized to

Figure 1: Performance of FilM-7B, Mistral-7B-Instruct-v0.2, and GPT4-Turbo on our three probing tasks. FilM-7B significantly overcomes the problem of information loss in the middle of the context.

real-world scenarios. Moreover, FilM-7B maintains a comparable performance on short-context tasks compared with the vanilla backbone model (e.g., 59.3\(\rightarrow\)59.2 accuracy on MMLU (Hendrycks et al., 2020)). This indicates that the short-context capability of FilM-7B is not compromised during training. Our further analysis explores how the sliding window strategy and the choice of RoPE base \(\theta\) influence the performance of In2 training.

## 2 Information-Intensive Training

This section introduces the construction of the dataset for In2 training and the detailed training process of our model FilM-7B.

### Training Data Construction

Overview.The In2 training aims to explicitly teach the model that any position in a long context can contain crucial information. To achieve this goal, we construct a long-context question-answer training dataset \(\mathbb{D}=\{\mathcal{L}_{i},q_{i},a_{i}\}\), where the answer \(a_{i}\) to the question \(q_{i}\) requires the information contained in some short segments that are randomly placed in the whole long context \(\mathcal{L}_{i}\).

Figure 2 illustrates an overview of the data construction process. Specifically, the training data \(\mathbb{D}\) is constructed based on a general natural language corpus \(\mathbb{C}\). Given a raw text \(\mathcal{C}_{i}\in\mathbb{C}\), we first generate a question-answer pair \((q_{i},a_{i})\) using a powerful LLM, then synthesize a long context \(\mathcal{L}_{i}\) that includes the necessary information from \(\mathcal{C}_{i}\) and other randomly sampled texts from \(\mathbb{C}\). We generate two types of question-answer pairs that require (1) the awareness of fine-grained information in the long context, and (2) the integration and reasoning of information appearing at different positions in the long context. We take the realnewslike subset from the C4 corpus (Raffel et al., 2020) as \(\mathbb{C}\), and take GPT-4-Turbo (OpenAI, 2023b) as the LLM to generate QA pairs.

Fine-grained information awareness.We consider a 128-token segment as the minimum information unit of the context3. Given a raw text \(\mathcal{C}_{i}\), we first randomly extract a 128-token segment \(s_{i}\) from it, then generate the \(q_{i}\), \(a_{i}\) and \(\mathcal{L}_{i}\) accordingly,

Footnote 3: Appendix D contains the implementation and our considerations for this design choice.

\[(q_{i},a_{i})\sim\mathrm{Prompting}(s_{i},I_{f};\mathrm{LLM}),\quad\mathcal{L }_{i}=\oplus\{\mathrm{Shuffle}(s_{i},[r_{j}])\},\] (1)

Figure 2: The data construction process for In2 training, aimed at enhancing the fine-grained information awareness (upper), and the integration and reasoning of information (lower).

where \((q_{i},a_{i})\) is sampled by prompting the powerful LLM with the segment \(s_{i}\) and the instruction \(I_{f}\), \(\oplus\{\cdot\}\) represents the concatenation of the contained segments, and \([r_{j}]\) are randomly sampled from 128-token segments in \(\mathbb{C}\). Note that \(I_{f}\) instructs the LLM to make the question-answer pair highly specific to the information provided in \(s_{i}\).

Integration and reasoning of information.Beyond utilizing each single segment, we consider to generate question-answer pairs for information contained in two or more segments. Following the setting of the minimum information unit above, we split a full text \(\mathcal{C}_{i}\) into a set of 128-token segments \([s_{i}]\), then generate the \(q_{i}\), \(a_{i}\) and \(\mathcal{L}_{i}\) accordingly,

\[(q_{i},a_{i})\sim\mathrm{Prompting}([s_{i}],I_{r};\mathrm{LLM}),\quad\mathcal{ L}_{i}=\oplus\{\mathrm{Shuffle}([s_{i}],[r_{j}])\},\] (2)

where \(I_{r}\) instructs the LLM to generate a multi-hop question-answer pair that requires the information within at least two segments in \([s_{i}]\). All segments in \([s_{i}]\) and \([r_{j}]\) are jointly shuffled, so the required segments may appear far apart in the context.

Context length balance and data mixture.To prevent length bias during In2 training, we ensure the length of the long context \(\mathcal{L}_{i}\) is evenly distributed from 4K to 32K tokens. Such a length balance strategy can be implemented with restricted sampling on \([r_{j}]\), according to Equation 1 and 2. To alleviate catastrophic forgetting on short-context capabilities, we retain \(\sim\)10% question-answer pairs with the original texts \(\mathcal{C}_{i}\) instead of converting them into a longer context, and add some general instruction-tuning data from the OpenOrca (Lian et al., 2023) dataset.

Overall, our dataset for In2 training contains 1.1M long-context data for the fine-grained information awareness (\(\sim\)63%), 300K long-context data for the integration and reasoning of information (\(\sim\)17%), 150K short-context question-answer data (\(\sim\)9%), and 200K general instruction-tuning data (\(\sim\)11%). Appendix I contains the handcraft instructions for data generation. Appendix H illustrates some examples of our constructed long-context QA data. Appendix A describes the filtering strategy to avoid data contamination for evaluation.

### Training Details

Using the training data constructed above, we further fine-tune the Mistral-7B-Instruct-v0.24(Jiang et al., 2023) to get our FiLM-7B (**FILI**-in-the-**M**iddle). We perform In2 training in the instruction-tuning paradigm: the long contexts and questions are used as instructions, and the loss on the answer parts are used to update the model. Appendix I contains the system template used for formatting the training data. For hyper-parameters, we set the global batch size as 128 and conduct one-epoch training with \(\sim\)14K training steps. We use the cosine learning rate decay with a 1e-6 maximum

Figure 3: Three tasks in VAL Probing. The retrieval patterns are determined by the relative positions between the retrieval keywords and the information to be retrieved.

learning rate and 3% warm-up steps. The training process is conducted on 16 nodes of 8x80G A100 GPUs with the full sharding strategy and cpu offload strategy implemented by pytorch FSDP (Zhao et al., 2023). One entire training process (for a single FilM-7B model) consumes \(\sim\)300 GPU days.

## 3 Long-Context Probing

In this section, we first show the preliminary evaluation of FilM-7B on the Needle-in-the-Haystack and discuss about the inadequacies of this probing task. Subsequently, to comprehensively evaluate the long-context information awareness of FilM-7B, we introduce **Various Long-context (VaL) Probing**. This includes three tasks that cover various context styles (document, code, and structured-data context) and information retrieval patterns (forward, backward, and bi-directional retrieval).

Figure 4: Performance of FilM-7B on VaL Probing and the comparisons with (a) Mistral, (b) LongAlign, and (c) InternLM2. The X-axis is the relative position in the context (\(\sim\)32K tokens).

### Near-Perfect Performance on Needle-in-the-Haystack: Are We There Yet?

The Needle-in-the-Haystack (Ivgi et al., 2023; Liu et al., 2024b) is widely used to assess how robustly a model utilizes information positioned in the long context. It reveals that even some powerful proprietary LLMs, such as GPT-4 and Claude 2.1 (Anthropic, 2023), struggle to fully exploit the information within the long context.

We use the Needle-in-the-Haystack task5 to preliminarily evaluate the long-context capability of FilM-7B. Appendix B demonstrates that FilM-7B has achieved near-perfect performance on this task. This result is not surprising as recent open-source LLMs, such as LongAlign (Bai et al., 2024) and InternLM2 (Cai et al., 2024), have also shown near-perfect performance on this task.

Footnote 5: https://github.com/gkamradt/LLMTest_NeedleInAHaystack.

However, the near-perfect performance on Needle-in-the-Haystack may overestimate the long-context capabilities of LLMs (Lei et al., 2024; Hsieh et al., 2024). Specifically, we have the following two concerns:

* Needle-in-the-Haystack employs a document-style context, which LLMs could be quite familiar with due to the pre-training on natural language corpora.
* The **forward retrieval** pattern in Needle-in-the-Haystack may simplify the difficulty of information seeking in the long context.

The "forward retrieval" means that the information being retrieved directly follows the retrieval keyword in a long context. For example, the default question used in Needle-in-the-Haystack is "What is the best thing to do in San Francisco?" and the answer is contained in "The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day." The retrieved information "eat a sandwich and..." just follows the retrieval keywords "best thing to do in San Francisco". According to the mechanism of induction head (Olsson et al., 2022), such a following-up copying is an easily learned pattern for LLMs, thus less challenging for evaluating long context utilization (just like the observation of "reversal curse" (Berglund et al., 2024)).

Given these considerations, we suggest that performances on Needle-in-the-Haystack may not adequately reflect the long-context capabilities of LLMs. Therefore, we propose VaL Probing for a more comprehensive evaluation involving various context styles and retrieval patterns.

### VaL Probing

Our retrieval-based VaL Probing considers three context styles (document, code, and structured-data context) and three retrieval patterns (forward, backward, and bi-directional retrieval). Each context in VaL Probing contains \(\sim\)32K tokens, and each task contains \(\sim\)3K examples. Figure 3 briefly illustrates the contexts and retrieval instructions in VaL Probing.

Document Sentence Retrieval (Bi-Direction).The contexts consist of numerous natural language sentences, and the instruction aims to retrieve a single sentence containing a given piece. The sentences are sampled from the abstracts of papers on arXiv6. This task follows the bi-directional

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline \multirow{2}{*}{Model} & \multicolumn{2}{c}{Document} & \multicolumn{2}{c}{Code} & \multicolumn{2}{c}{Database} & \multicolumn{2}{c}{All} \\  & Avg & Gap\(\downarrow\) & Avg & Gap\(\downarrow\) & Avg & Gap\(\downarrow\) & Avg & Gap\(\downarrow\) \\ \hline Mistral-7B-Instruct-v0.1 (Jiang et al., 2023) & 44.8 & 29.9 & 6.8 & 53.2 & 8.8 & 74.5 & 20.1 & 52.5 \\ Mistral-7B-Instruct-v0.2 (Jiang et al., 2023) & 74.2 & 32.1 & 20.3 & 59.5 & 47.5 & 77.0 & 47.3 & 56.2 \\ LongAlign-7B-64K (Bai et al., 2024) & 65.3 & 16.9 & 39.3 & 56.0 & 55.0 & 36.2 & 53.2 & 36.4 \\ LongAlign-13B-64K (Bai et al., 2024) & 71.7 & 13.4 & 50.8 & 40.8 & 82.9 & 27.0 & 68.5 & 27.1 \\ InternLM2-chat-7B (Cai et al., 2024) & 68.8 & 18.7 & 50.2 & 44.1 & 61.2 & 57.1 & 60.1 & 40.0 \\ InternLM2-chat-20B (Cai et al., 2024) & 66.4 & 27.2 & 63.4 & 45.5 & 74.9 & 57.2 & 68.2 & 43.3 \\ GPT-4-Turbo (OpenAI, 2023b) & 81.3 & 31.7 & 66.1 & 46.5 & **89.6** & 18.0 & 79.0 & 32.1 \\ \hline FilM-7B (ours) & **85.4** & **6.1** & **83.3** & **18.7** & 89.0 & **16.8** & **85.9** & **13.9** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Quantified performances of various models on VaL Probing.

retrieval pattern, as the expected retrieval results contain words both before and after the given piece in the context. The evaluation metric is the word-level recall score.

Code Function Retrieval (Backward).The contexts consist of Python functions, and the instruction aims to retrieve the function name for a given line of code within the function definition. The raw code functions are sampled from the StarCoder (Li et al., 2023c) dataset7. We randomly select three lines of definitions for each function. This task follows the backward retrieval pattern, as the function name always precedes the definition. The evaluation metric is the exact-match accuracy.

Footnote 7: https://huggingface.co/datasets/bigcode/starcoderdata.

Database Entity Retrieval (Forward).The contexts contain lists of structured entities, each with three fields: ID, label, and description. The query aims to retrieve the label and description for a given ID. The entities are sampled from Wikidata 8. This task follows the forward retrieval pattern, as the label and description follow the ID. We take a relaxed exact-match accuracy as the metric: a 1 score is given if either the label or the description is exactly matched in the response, otherwise a 0 score.

Footnote 8: https://www.wikidata.org/wiki/Wikidata:Data_access.

## 4 Experiments and Analysis

We assess the long-context capability of FilM-7B on both probing tasks and real-world long-context tasks. Moreover, we investigate if the performance in short-context scenarios is affected.

### Experimental Setup

Models.We mainly compare FilM-7B with long-context open-source models that have been trained with \(\geq\)32K context windows, including the Mistral (Jiang et al., 2023), LongChat (Li et al., 2023a), ChatGLM (Du et al., 2022), LongAlign (Bai et al., 2024), LongWanjuan (Lv et al., 2024), Yi (AI et al., 2024) and InternLM2 (Cai et al., 2024). We utilize the instruct/chat versions of these models as most of our evaluation tasks are under the zero-shot instruction-following paradigm. We also draw comparisons with popular proprietary LLMs such as GPT-3.5-Turbo (OpenAI, 2023a) and

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline \multirow{2}{*}{Model} & Document & \multicolumn{2}{c}{Code} & \multicolumn{2}{c}{Database} & \multicolumn{2}{c}{All} \\  & Avg & Gap\(\downarrow\) & Avg & Gap\(\downarrow\) & Avg & Gap\(\downarrow\) & Avg & Gap\(\downarrow\) \\ \hline Mistral-7B-Instruct-v0.2 (Jiang et al., 2023) & 74.2 & 32.1 & 20.3 & 59.5 & 47.5 & 77.0 & 47.3 & 56.2 \\ + Normal Instruction Tuning (Lian et al., 2023) & 69.0 & 25.9 & 30.2 & 76.5 & 53.4 & 54.4 & 50.9 & 52.3 \\ + Information-Intensive Training (ours) & **82.9** & **11.5** & **74.5** & **27.7** & **83.5** & **31.6** & **80.3** & **23.6** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Quantified comparison between IN2 training and normal instruction tuning.

Figure 5: Compare the performance of IN2 training and general instruction tuning (IT). Both two training process takes the same number of training instances (20% of the full data size, 300K examples). Compare with Mistral + IN2 training, the gains from normal instruction tuning are marginal and unstable.

GPT-4-Turbo (OpenAI, 2023b). All models and tasks employ greedy decoding. For probing tasks, we primarily compare FilM-7B with LongAlign and InternLM2 series, as these models have shown near-perfect performances on Needle-in-the-Haystack.

Real-world long-context tasks.We take 9 tasks from the LongBench (Bai et al., 2023) collection to evaluate the long-context capability on real-world scenarios. These tasks encompass long-document question answering (NarrativeQA (Kocisky et al., 2018), Qasper (Dasigi et al., 2021) and Multi-FieldQA (MultiFQA) (Bai et al., 2023), multi-document multi-hop reasoning (HotpotQA (Yang et al., 2018), 2WikiMultihopQA (2WikiMQA) (Ho et al., 2020) and MuSiQue (Trivedi et al., 2022)), and long-context summarization (GovReport (Huang et al., 2021), QMSum (Zhong et al., 2021) and MultiNews (Fabbri et al., 2019)). We employ the middle truncation strategy in LongBench to limit the input within 32K tokens. We report ROUGE-L (Lin, 2004) for summarization tasks and F1 scores for other tasks. The evaluation metrics are computed using the official evaluation scripts 9.

Footnote 9: https://github.com/THUDM/LongBench.

Despite these QA and summarization tasks, we also conduct evaluations on few-shot learning tasks, in which the contexts could also be extremely lengthy if there are many "few-shot" examples presented in the context. We take three in-context learning tasks from LongBench, including TREC (Li & Roth, 2002) for few-shot classification, TriviaQA (Joshi et al., 2017) for few-shot QA, and SAMSum (Gliwa et al., 2019) for few-shot summarization.

Short-context tasks.We select 8 short-context tasks commonly used for evaluating the general capabilities of models. These include MMLU (Hendrycks et al., 2020), BoolQ (Clark et al., 2019), RACE-High (RACE-H) (Lai et al., 2017), CommonsenseQA (CSQA) (Talmor et al., 2019), ARC-Challenge (ARC-C) (Clark et al., 2018), HellaSwag (Zellers et al., 2019), GSM8K (Cobbe et al., 2021), and MATH (Hendrycks et al., 2021). We use 5-shot for MMLU, 8-shot for GSM8K, 4-shot for MATH, and 0-shot for other tasks. We utilize the lm_eval (Gao et al., 2024) for the evaluations on MMLU, BoolQ, RACE-H, ARC-C and HellaSwag, and use the evaluation scripts from An et al. (2024) for other tasks.

### Main Results and Analysis

FilM-7B significantly mitigates the lost-in-the-middle problem.Figure 3(a) presents the probing results for both FilM-7B and the backbone model, Mistral-7B-Instruct-v0.2. In all three probing tasks within Val Probing, the vanilla Mistral model experiences substantial information loss at the middle positions in the long contexts. In contrast, our FilM-7B model consistently exhibits robust

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline Model & \multicolumn{3}{c}{TREC} & TriviaQA & SAMSum & Average \\ \hline GPT4-Turbo (OpenAI, 2023b) & 77.0 & 91.7 & 39.7 & 69.5 & & & \\ Mistral-7B-Instruct-v0.2 (Jiang et al., 2023) & 71.0 & 84.5 & 35.8 & 63.8 & & & \\ \hline \hline
**FILM-7B (ours)** & **76.0** & **90.0** & **39.5** & **68.5** & & & \\ \hline \end{tabular}
\end{table}
Table 4: Model performances on few-shot learning tasks.

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline Model & \multicolumn{3}{c}{NarrativeQA (Quaert)} & MultiFQA & HotpotQA & 2WikiMQA & MuSiQue & GovReport & QMSum & MultiNews & Avg \\ \hline \multicolumn{10}{c}{Close-Source} \\ GPT-4-Turbo (OpenAI, 2023b) & 33.0 & 50.7 & 52.7 & 68.5 & 64.3 & 49.1 & 33.9 & 25.4 & 24.9 & 44.7 \\ GPT-3.5-Turbo (OpenAI, 2023a) & 23.6 & 43.3 & 52.3 & 51.6 & 37.7 & 26.9 & 29.5 & 23.4 & 26.7 & 35.0 \\ \hline \multicolumn{10}{c}{Open-Source} \\ LengthCat\(\backslash\)1.5-7B-32K’ (Li et al., 2023a) & 16.9 & 27.7 & 41.4 & 31.5 & 20.6 & 9.7 & 30.8 & 22.7 & 26.4 & 25.3 \\ ChatML-62-83K’ (Die et al., 2022) & 21.1 & 31.5 & 46.2 & 25.3 & 20.8 & 9.8 & 32.4 & 24.0 & 26.5 & 26.4 \\ LongAlign-7B-64K (Bai et al., 2024) & 18.7 & 33.8 & 49.1 & 28.6 & 23.4 & 12.5 & 30.6 & 23.7 & 27.5 & 27.5 \\ Mistral-7B-Instruct-v0.01 (Jiang et al., 2023) & 19.6 & 33.2 & 38.8 & 42.9 & 31.2 & 17.4 & 27.5 & 22.4 & 26.6 & 28.9 \\ Mistral-7B-Instruct-v0.02 (Jiang et al., 2023) & 23.5 & 33.8 & 45.9 & 42.4 & 24.3 & 20.8 & 33.3 & 24.8 & 26.8 & 30.6 \\ Yi-60-200K’ (Li et al., 2023) & 12.4 & 26.4 & 36.8 & 46.6 & 40.4 & 25.8 & 29.3 & 20.7 & 27.1 & 29.5 \\ ChatML3-68-32K’ (Die et al., 2022) & 9.2 & **43.1** & 50.9 & 55.3 & 43.7 & **38.9** & **36.0** & 24.7 & 27.4 & 36.6 \\ InternalM2-chat-7B (Cai et al., 2024) & 24.4 & 35.4 & 50.2 & 52.4 & **48.2** & 30.5 & 33.6 & **25.3** & **29.0** & 36.5 \\ InternalM2-7B-LogWangian’ (Lv et al., 2024) & **29.9** & 39.6 & 50.2 & 53.7 & 42.3 & 32.1 & 33.0 & **25.5** & 27.8 & 37.1 \\ \hline FilM-7B (ours) & 26.9 & **42.2** & **56.0** & **62.1** & **47.0** & **39.0** & 33.8 & **25.1** & 26.9 & **38.9** \\ \hline \end{tabular}
\end{table}
Table 3: Performances of various models on real-world long-context tasks. Results of models with \({}^{*}\) are reported in Bai et al. (2023) and Lv et al. (2024).

[MISSING_PAGE_FAIL:9]

Training on synthesized long-context data effectively generalizes to real-world scenarios.Table 3 and 4 contain the results on various real-world long-context tasks. It shows that FilM-7B also significantly improves the performance of the backbone model in real-world long-context scenarios. Moreover, it also achieves SOTA-level10 performances on these tasks among \(\sim\)7B size open-source models. Notably, the long contexts used in InN2 training are all synthesized from short segments. These improvements suggest that the long-context capabilities learned from the synthesized data can be successfully applied to real-world tasks.

Footnote 10: The bold numbers in Table 3 are SOTA-level results among 7B open-source models. Specifically, for each task, we bold the highest result and the results within a margin to the highest one (0.5 for summarization tasks and 2.0 for others).

FilM-7B maintains the performance on short-context tasks.Figure 6 illustrates the performances of FilM-7B and the vanilla backbone model on short-context tasks. It reveals that the overall performances on short-context tasks are almost comparable with minor variances. These results confirm that FilM-7B does not compromise the short-context capabilities of the backbone model.

Analysis on training strategies.We are specifically interested in investigating the impact of the following two training strategies: applying the sliding window and adjusting the position encoding. Due to the page limitation, we provide these further ablations and analysis in Appendix C.

## 5 Related Work

Long-context LLMs.Recent research has significantly contributed to the exploration of training large models with extended context windows (Jiang et al., 2023; Du et al., 2022; Li et al., 2023; Team et al., 2023; Xiong et al., 2023; Song et al., 2023; Tworkowski et al., 2024; Ai et al., 2024; Cai et al., 2024). There are primarily two directions in the development of long-context LLMs. (1) Data engineering, which emphasizes the construction of long-context data for training the LLMs. This includes data balancing (Fu et al., 2024), data order arrangement (Shi et al., 2023), instruction data collection (Bai et al., 2024), and data quality measurement (Lv et al., 2024). Our InN2 training can be categorized into this field. (2) Effective and efficient training, which investigates methods to optimize the training of a long-context model. This encompasses the design of position encoding (Chen et al., 2023; Liu et al., 2023; Peng et al., 2023; Ding et al., 2024), batching strategy (Bai et al., 2024), parameter-efficient training (Chen et al., 2023), and the development of new model architectures (Peng et al., 2023; Gu and Dao, 2023).

Long-context evaluations.Existing benchmarks for evaluating long-context models can be divided into two categories. (1) Real-world benchmarks that assess general long-context capabilities (e.g., long-context QA, summarization, and language modeling), such as NarrativeQA (Kocisky et al., 2018), LongBench (Bai et al., 2023), ZeroSCROLLS (Shaham et al., 2023), L-Eval (An et al., 2023), Loogle (Li et al., 2023), \(\infty\)Bench (Zhang et al., 2024), and a series of work on perplexity evaluation (Beltagy et al., 2020; Roy et al., 2021; Press et al., 2021; Chen et al., 2023; Liu et al., 2023; Peng et al., 2023; Chen et al., 2023; Ding et al., 2024; Mohtsami and Jaggi, 2024). (2) Probing tasks that provide a more concise reflection of the long-context utilization across different context lengths and positions. These include Needle-in-the-Haystack, passkey retrieval (Mothashami and Jaggi, 2024), synthesized document QA (Liu et al., 2024), S3Eval (Lei et al., 2024), Discovery (Li et al., 2024), RULER (Hsieh et al., 2024), and the VaL Probing proposed in this study. Among these probing tasks, our VaL Probing is the first to explicitly incorporate a variety of retrieval patterns.

## 6 Conclusion

This work introduces In2 training to overcome the lost-in-the-middle problem. By applying In2 training on the open-source model, our FilM-7B exhibits significant improvements on probing tasks and real-world long-context tasks while does not compromise the short-context performance.

## Acknowledgments

We thank all the anonymous reviewers for their valuable comments. Shengnan An and Nanning Zheng were supported in part by NSFC under grant No. 62088102.

## References

* Abdin et al. [2015] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sebastien Bubeck, Martin Cai, Caio Cesar Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adi Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengriudong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report: A highly capable language model locally on your phone, 2024.
* AI [2014] AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open foundation models by 01.ai, 2024.
* An et al. [2023] Chenxin An, Shansen Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. L-eval: Instituting standardized evaluation for long context language models. _arXiv preprint arXiv:2307.11088_, 2023.
* An et al. [2024] Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, and Weizhu Chen. Learning from mistakes makes llm better reasoner, 2024.
* Anthropic [2023] Anthropic. Model card and evaluations for claude models, 2023. URL https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf.
* Bai et al. [2023] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: A bilingual, multitask benchmark for long context understanding. _arXiv preprint arXiv:2308.14508_, 2023.
* Bai et al. [2024] Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li. Longalign: A recipe for long context alignment of large language models. _arXiv preprint arXiv:2401.18058_, 2024.
* Beltagy et al. [2020] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. _arXiv preprint arXiv:2004.05150_, 2020.
* Berglund et al. [2024] Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. The reversal curse: Llms trained on "a is b" fail to learn "b is a", 2024. URL https://arxiv.org/abs/2309.12288.
* Cai et al. [2024] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et al. Internlm2 technical report. _arXiv preprint arXiv:2403.17297_, 2024.
* Chen et al. [2023a] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. _arXiv preprint arXiv:2306.15595_, 2023a.
* Chen et al. [2023b] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. In _The Twelfth International Conference on Learning Representations_, 2023b.
* Chen et al. [2023c]Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pp. 2924-2936, 2019.
* Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. _arXiv preprint arXiv:1803.05457_, 2018.
* Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. _arXiv preprint arXiv:2110.14168_, 2021.
* Dasigi et al. (2021) Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A Smith, and Matt Gardner. A dataset of information-seeking questions and answers anchored in research papers. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pp. 4599-4610, 2021.
* Ding et al. (2024) Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang. Longrope: Extending llm context window beyond 2 million tokens. _arXiv preprint arXiv:2402.13753_, 2024.
* Du et al. (2022) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 320-335, 2022.
* Fabbri et al. (2019) Alexander Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir Radev. Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pp. 1074. Association for Computational Linguistics, 2019.
* Fu et al. (2024) Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng. Data engineering for scaling language models to 128k context, 2024.
* Gao et al. (2024) Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 07 2024. URL https://zenodo.org/records/12608602.
* Gliwa et al. (2019) Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. Samsum corpus: A human-annotated dialogue dataset for abstractive summarization. _EMNLP-IJCNLP 2019_, pp. 70, 2019.
* Gu and Dao (2023) Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2023.
* Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In _International Conference on Learning Representations_, 2020.
* Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In _Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)_, 2021.
* Ho et al. (2020) Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps. In _Proceedings of the 28th International Conference on Computational Linguistics_, pp. 6609-6625, 2020.
* Hsieh et al. (2024) Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. Ruler: What's the real context size of your long-context language models?, 2024.
* Huang et al. (2020)Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient attentions for long document summarization. In _2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021_, pp. 1419-1436. Association for Computational Linguistics (ACL), 2021.
* Ivgi et al. (2023) Maor Ivgi, Uri Shaham, and Jonathan Berant. Efficient long-text understanding with short-text models. _Transactions of the Association for Computational Linguistics_, 11:284-299, 2023. doi: 10.1162/tacl_a_00547. URL https://aclanthology.org/2023.tacl-1.17.
* Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. _arXiv preprint arXiv:2310.06825_, 2023.
* Joshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 1601-1611, 2017.
* Kocisky et al. (2018) Tomas Kocisky, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gabor Melis, and Edward Grefenstette. The narrativeqa reading comprehension challenge. _Transactions of the Association for Computational Linguistics_, 6:317-328, 2018.
* Lai et al. (2017) Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading comprehension dataset from examinations. In _Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing_, pp. 785-794, 2017.
* Lei et al. (2024) Fangyu Lei, Qian Liu, Yiming Huang, Shizhu He, Jun Zhao, and Kang Liu. S3eval: A synthetic, scalable, systematic evaluation suite for large language models, 2024.
* Li et al. (2023a) Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can context length of open-source LLMs truly promise? In _NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following_, 2023a. URL https://openreview.net/forum?id=LywifFNXV5.
* Li et al. (2023b) Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. Loogle: Can long-context language models understand long contexts? _arXiv preprint arXiv:2311.04939_, 2023b.
* Li et al. (2023c) Raymond Li, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, LI Jia, Jenny Chim, Qian Liu, et al. Starceder: may the source be with you! _Transactions on Machine Learning Research_, 2023c.
* Li et al. (2024) Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, and Wenhu Chen. Long-context llms struggle with long in-context learning, 2024.
* Li and Roth (2002) Xin Li and Dan Roth. Learning question classifiers. In _COLING 2002: The 19th International Conference on Computational Linguistics_, 2002.
* Lian et al. (2023) Wing Lian, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and "Teknium". Openorca: An open dataset of gpt augmented flan reasoning traces. https://https://huggingface.co/Open-Orca/OpenOrca, 2023.
* Lin (2004) Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In _Text summarization branches out_, pp. 74-81, 2004.
* Liu et al. (2024a) Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with blockwise ringattention, 2024a.
* Liu et al. (2024b) Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. _Transactions of the Association for Computational Linguistics_, 12:157-173, 2024b.
* Liu et al. (2023) Xiaoran Liu, Hang Yan, Chenxin An, Xipeng Qiu, and Dahua Lin. Scaling laws of rope-based extrapolation. In _The Twelfth International Conference on Learning Representations_, 2023.
* Liu et al. (2020)Kai Lv, Xiaoran Liu, Qipeng Guo, Hang Yan, Conghui He, Xipeng Qiu, and Dahua Lin. Longwanjuan: Towards systematic measurement for long text quality. _arXiv preprint arXiv:2402.13583_, 2024.
* Mohtashami and Jaggi (2024) Amirkevian Mohtashami and Martin Jaggi. Random-access infinite context length for transformers. _Advances in Neural Information Processing Systems_, 36, 2024.
* Olsson et al. (2022) Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads, 2022.
* OpenAI (2023a) OpenAI. Gpt-3.5 turbo fine-tuning and api updates, 2023a. URL https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-\api-updates.
* OpenAI (2023b) OpenAI. Gpt-4 technical report, 2023b.
* Peng et al. (2023) Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, et al. Rwkv: Reinventing rnns for the transformer era. In _Findings of the Association for Computational Linguistics: EMNLP 2023_, pp. 14048-14077, 2023a.
* Peng et al. (2023b) Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. In _The Twelfth International Conference on Learning Representations_, 2023b.
* Press et al. (2021) Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In _International Conference on Learning Representations_, 2021.
* Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of machine learning research_, 21(140):1-67, 2020.
* Roy et al. (2021) Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. _Transactions of the Association for Computational Linguistics_, 9:53-68, 2021.
* Roziere et al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, et al. Code lama: Open foundation models for code. _arXiv preprint arXiv:2308.12950_, 2023.
* Shaham et al. (2023) Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. Zeroscrolls: A zero-shot benchmark for long text understanding. In _Findings of the Association for Computational Linguistics: EMNLP 2023_, pp. 7977-7989, 2023.
* Sharan et al. (2018) Vatsal Sharan, Sham Kakade, Percy Liang, and Gregory Valiant. Prediction with a short memory. In _Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing_, pp. 1074-1087, 2018.
* Shi et al. (2023) Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou, Margaret Li, Xi Victoria Lin, Noah A Smith, Luke Zettlemoyer, Wen-tau Yih, and Mike Lewis. In-context pretraining: Language modeling beyond document boundaries. In _The Twelfth International Conference on Learning Representations_, 2023.
* Song et al. (2023) Woomin Song, Seunghyuk Oh, Sangwoo Mo, Jaehyung Kim, Sukmin Yun, Jung-Woo Ha, and Jinwoo Shin. Hierarchical context merging: Better long context understanding for pre-trained llms. In _The Twelfth International Conference on Learning Representations_, 2023.
* Sun et al. (2021) Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. Do long-range language models actually use long-range context? In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pp. 807-822, 2021.
* Sun et al. (2021)Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pp. 4149-4158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1421. URL https://aclanthology.org/N19-1421.
* Tearam et al. (2023) MosaicML NLP Team et al. Introducing mpt-30b: Raising the bar for open-source foundation models, 2023.
* Team (2023) Together Team. Together 32k, 2023. URL https://huggingface.co/together/LLAMA-2-7B-32K.
* Together (2023) Together.AI. Llama-2-7b-32k-instruct -- and fine-tuning for llama-2 models with together api, 2023. URL https://www.together.ai/blog/llama-2-7b-32k-instruct.
* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* Trivedi et al. (2022) Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions via single-hop question composition. _Transactions of the Association for Computational Linguistics_, 10:539-554, 2022.
* Tworkowski et al. (2024) Szymon Tworkowski, Konrad Staniszewski, Mikolaj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Milos. Focused transformer: Contrastive training for context scaling. _Advances in Neural Information Processing Systems_, 36, 2024.
* Xiong et al. (2023) Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma. Effective long-context scaling of foundation models, 2023.
* Xu et al. (2023) Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro. Retrieval meets long context large language models. In _The Twelfth International Conference on Learning Representations_, 2023.
* Yang et al. (2018) Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pp. 2369-2380, 2018.
* Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pp. 4791-4800, 2019.
* Zhang et al. (2024) Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. \(\infty\)bench: Extending long context evaluation beyond 100k tokens, 2024.
* Zhao et al. (2023) Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. _arXiv preprint arXiv:2304.11277_, 2023.
* Zhong et al. (2021) Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethius Mutuma, Rahul Jha, Ahmed Hassan, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, et al. Qmsum: A new benchmark for query-based multi-domain meeting summarization. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pp. 5905-5921, 2021.

This is the Appendix of the paper: _Make Your LLM Fully Utilize the Context_.

## Appendix A Data Filtering Strategy

To avoid data contamination for the evaluation stage in Section 4, we apply a pre-filtering strategy during sampling the raw texts for constructing the dataset of In2 training. Specifically, during sampling \(\mathcal{C}_{i}\) for generating data, if the sampled \(\mathcal{C}_{i}\) has a 10-gram overlap with any example in all of our evaluation data (including probing tasks, real-world tasks and short-context tasks), it will not be used for neither generating question-answer pairs nor serving as the random segments \([r_{j}]\).

## Appendix B Performance on Needle-in-the-Haystack

Figure 7 shows the performance of FilM-7B on Needle-in-the-Haystack. It shows that FilM-7B has achieved near-perfect performance on Needle-in-the-Haystack within its 32K context window.

## Appendix C Training Strategy Analysis

Experimental results in Section 4.2 demonstrate the feasibility of In2 training. We aim to explore further into enhancing the effectiveness and efficiency of In2 training, particularly from the perspective of training strategies. We are specifically interested in investigating the impact of the following two training strategies: applying the sliding window and adjusting the position encoding. Considering the high cost of training, the following experiments use 20% of all training examples.

Models using sliding windows cannot effectively capture the long distance information.Our experiments involving Mistral models, as shown in Figure 3(a), reveal that the performance of Mistral-7B-Instruct-v0.1 as awful when the information is positioned at a long distance. It's worth noting that Mistral-7B-Instruct-v0.1 employs the sliding window strategy while Mistral-7B-Instruct-v0.2 does not. Consequently, we are interested in determining whether our In2 training can still alleviate the lost-in-the-middle problem under the sliding window strategy. We conduct the following two experiments with a 4K sliding window during training:

* **Apply the sliding window in both pre-training and In2 training.** We take the Mistral-7B-Instruct-v0.1 as the backbone model and conduct In2 training with the same window size (4K).
* **Apply the sliding window only during the In2 training.** We take the Mistral-7B-Instruct-v0.2 as the backbone model and additionally apply a 4K sliding window during In2 training.

Figure 8 illustrates the performances of models with sliding windows. It shows that in both two settings with sliding windows, the performances drop dramatically when the distance between the

Figure 7: Performances of FilM-7B on Needle-in-the-Haystack.

retrieval question and information is longer than the sliding window size. It reveals that the sliding window strategy greatly hurts the long-context capability of models.

Training with higher information intensity requires a larger RoPE base \(\theta\).The training stage in Section 2 follows the RoPE settings configured for the backbone model. Previous studies on context extension suggest that training with an extended context length necessitates a larger RoPE base \(\theta\)(Roziere et al., 2023; Xiong et al., 2023; Cai et al., 2024). In the case of our In2 training, the context length remains unchanged, but the information density is significantly changed to be more uniform. There is a high-level similarity between context-extension training and our IN2 training: **both aim to enhance the model's capability to perceive information from a wider range of input positions** compared to the previous training stages. Consequently, we carry out experiments to investigate whether the experience from context-extension training could also benefit our IN2 training. Table 5 shows the results with increasing the RoPE base \(\theta\) from \(1.0\times 10^{6}\) to \(1.0\times 10^{8}\). It shows that increasing the default RoPE base \(\theta\) of the backbone model leads to better performances on VaL Probing. We suggest to use a 10 times of the default RoPE base \(\theta\) to conduct In2 training.

## Appendix D Implementation and Reasons for Segmentation

Algorithm 1 illustrates how we segment a raw text into \(\sim\)128-token segments. We set the \(\sim\)128-token segment as the minimum information unit due to the following consideration:

* If the segment contains too few tokens (e.g., 16 tokens or 32 tokens), it might not contain enough information for asking a meaningful question.
* If we set a large threshold for segmentation (e.g., 1024 tokens or 4096 tokens), most raw texts will just contain one segment11, thus affecting the construction of QA pairs that require the integration and reasoning of information. Footnote 11: The raw texts in realnewslike have an average length of \(\sim\)600 tokens with the Mistral tokenizer.
* Moreover, we do not just use a full raw text to generate a question with GPT-4, as we are concerned about whether GPT-4 will also more focus on the head and the tail of the text. It could result in a local bias for training: the answers are always placed on the boundaries between two segments containing consecutive information text.

Figure 8: Performance of Film-7B with a 4K sliding window (SW). PT-In2: apply the sliding window in both pre-training and In2 training. In2: apply the sliding window only in In2 training.

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline \multirow{2}{*}{Model} & \multirow{2}{*}{RoPE Base \(\theta\)} & \multicolumn{2}{c}{Document} & \multicolumn{2}{c}{Code} & \multicolumn{2}{c}{Database} & \multicolumn{2}{c}{All} \\  & & Avg & Gap\(\downarrow\) & Avg & Gap\(\downarrow\) & Avg & Gap\(\downarrow\) & Avg & Gap\(\downarrow\) \\ \hline \multirow{4}{*}{FILM-7B (20\%)} & \(1.0\times 10^{6}\) (default) & 82.9 & 11.5 & 74.5 & 27.7 & 83.5 & 31.6 & 80.3 & 23.6 \\  & \(2.0\times 10^{6}\) & 83.9 & 9.3 & 79.8 & 27.1 & 87.7 & **13.2** & 83.8 & 16.5 \\ \cline{1-1}  & \(1.0\times 10^{7}\) & 83.7 & **7.6** & **81.7** & **18.4** & **89.4** & 16.8 & **84.9** & **14.3** \\ \cline{1-1}  & \(1.0\times 10^{8}\) & **84.6** & **6.6** & **81.4** & 22.3 & 87.7 & **13.2** & **84.6** & **14.0** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Performance of FilmM-7B with different RoPE base \(\theta\) during In2 training.

Based on the above considerations, we use the \(\sim\)128-token segment. Due to the high cost on data generation, we do not conduct further ablations on this design choice.

## Appendix E Data Scaling Trend

Table 6 shows the performance of FilM-7B with different training data sizes for In2 training. Generally, with the data size increasing, the average performance increases and the performance variance in different positions decreases. Such trends are more significant on code and document probing tasks. Note that the training data for In2 training are almost from natural language corpus. It indicates that increasing the training data size can better help the generalization of long-context capability on different context styles.

## Appendix F Performance on RULER Benchmark

RULER (Hsieh et al., 2024) is a synthetic benchmark that evaluates the effective context length of the long-context LLMs. It revealed that while all existing long-context models with \(\leq\)7B sizes claim context size of 32k tokens or greater (except for Llama3), none of them can effectively handle sequence length of 32K by exceeding a qualitative threshold, Llama2-7b performance at 4K (85.6%).

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline \multirow{2}{*}{Model} & \multirow{2}{*}{Data Size} & Document & \multicolumn{2}{c}{Code} & \multicolumn{2}{c}{Database} & \multicolumn{2}{c}{All} \\  & & Avg & Gap\(\downarrow\) & Avg & Gap\(\downarrow\) & Avg & Gap\(\downarrow\) & Avg & Gap\(\downarrow\) \\ \hline \multirow{6}{*}{FilM-7B} & 100\% & 85.4 & 6.1 & 83.3 & 18.7 & 89.0 & 16.8 & 85.9 & 13.9 \\  & 50\% & 84.2 & 13.3 & 80.5 & 21.5 & 89.7 & 15.3 & 84.8 & 16.7 \\ \cline{1-1} \cline{2-10}  & 20\% & 82.9 & 11.5 & 74.5 & 27.7 & 83.5 & 31.6 & 80.3 & 23.6 \\ \cline{1-1}  & 10\% & 84.3 & 11.3 & 75.2 & 31.8 & 82.3 & 32.7 & 80.6 & 25.3 \\ \cline{1-1}  & 1\% & 76.2 & 18.8 & 63.3 & 48.0 & 70.9 & 36.7 & 70.1 & 34.5 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Performance of FilM-7B with different training data sizes for In2 training.

Table 7 shows that FILM-7B is the first \(\leq\)7B size model that achieves 32K effective context length. Note that for the evaluation on >32K lengths (i.e., 64K and 128K), we use YaRN (Peng et al., 2023b) to extend the position embeddings without further fine-tuning.

## Appendix G Position Extension Suffers Lost-in-the-Middle

Some existing studies focused on how to extend the context window without further training, such as YaRN (Peng et al., 2023b). We tried to extend the context window of FilM-7B from 32K to 64K with Yarn and evaluated whether the model can still overcome lost-in-the-middle problem under the extended context length. Figure 9 shows that although FilM-7B outperforms vanilla Mistral-7B on the extended 64K context window, both two models severely suffer the lost-in-the-middle problem. Such an observation indicates that simply extending the position embeddings of the model may not make the model fully utilize the extended context window.

\begin{table}
\begin{tabular}{l|c c|c c c c c c c} \hline \hline Model & Claimed & Effective & 4K & 8K & 16K & 32K & 64K & 128K & Avg. \\ \hline Llama2-7B (Touvron et al., 2023) & 4K & & 85.6 & - & - & - & - & - \\ LongChat-7B (Li et al., 2023a) & 32K & \textless{4K} & 84.7 & 79.9 & 70.8 & 59.3 & 0.0 & 0.0 & 49.1 \\ Together-7B (Together.AI, 2023) & 32K & 4K & 88.2 & 81.1 & 69.4 & 63.0 & 0.0 & 0.0 & 50.3 \\ Phi3-3B (Abdin et al., 2024) & 128K & 4K & 86.7 & 78.1 & 75.6 & 70.3 & 58.9 & 43.3 & 68.8 \\ LWM-7B (Liu et al., 2024a) & 1M & \textless{4K} & 82.3 & 78.4 & 73.7 & 69.1 & 68.1 & 65.0 & 72.8 \\ ChatGLM3-6B (Du et al., 2022) & 128K & 4K & 87.8 & 83.4 & 78.6 & 69.9 & 56.0 & 42.0 & 69.6 \\ Mistral-7B (Jiang et al., 2023) & 32K & 16K & 93.6 & 91.2 & 87.2 & 75.4 & 49.0 & 13.8 & 68.4 \\ \hline
**FILM-7B (ours)** & 32K & 32K & 92.8 & 88.2 & 88.1 & 86.9 & 70.1 & 27.1 & 75.5 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Performances (%) of \(\leq\)7B models on RULER benchmark. The performance exceeding the threshold (i.e., Llama2-7B on 4K length) is underlined.

Figure 9: Performance of FilM-7B on 64K context length. The position embeddings are extended through YaRN.

[MISSING_PAGE_EMPTY:20]

**Segment 1:**

Cloud Cruiser's utilization analytics application enables users to manage and optimize public, private and hybrid cloud usage and spending...

**Segment 2:**

This reduces the risk of organizations investing too much or too little in IT, eliminates unused capacity and frees up valuable IT resources for new value-adding projects...

**Segment 3:**

As a Cloud Cruiser customer, we have seen firsthand the value that Cloud Cruiser's technology creates by enabling HPE Flexible Capacity to meter and bill for usage of on-premise IT infrastructure in a pay-as-you-go model...

**Question:**

What benefits does the acquisition of Cloud Cruiser bring to HPE and its Flexible Capacity business?

**Answer:**

The acquisition of Cloud Cruiser enriches HPE's Flexible Capacity business by providing a utilization analytics application that allows users to manage and optimize their cloud usage and spending. Cloud Cruiser's software also facilitates the metering and billing for customers' IT consumption. This enables HPE to offer its customers an on-premise physical IT infrastructure at the cost of cloud services, effectively a rental agreement. Moreover, it reduces the risk of organizations either under or over investing in IT, eliminates unused capacity, and frees up IT resources for new projects...

**Segment 1:**

Julie Annette Wasden, 45, of 38 Roles St., Haines City, was charged with retail theft at Beall's at 5998 Cypress Gardens Blvd...

**Segment 2:**

Katherine D. Collazo Santos, 18, of 108 Hilltop Drive, Winter Haven, was charged with retail theft at Macy's at 700 Third St., S.W...

**Question:**

Who was charged with retail theft at Beall's and who was charged with the same crime at Macy's?

**Answer:**

Julie Annette Wasden and Katherine D. Collazo Santos

**Segment 3:**

A court presided by Magistrate Joseph Mifsud explained in its judgement of 17 August, 2016 in The Police v Eobis Getu, that the charges of a false criminal report and calumnious accusations may be factually similar, however, from a legal point of view they are two separate and distinct charges. The accused, Eobis Getu was charged with having filed a false police report and with not obeying a legitimate order. She admitted these charges...

**Segment 2:**

In passing judgement the court took into consideration that the accused admitted to the charges immediately and also that she did what she did in order to be with her husband in Malta. Magistrate Mifsud referred to what Pope Francis said last June, where today2019s information technology brings suffering of others instantly, but we also become immune to tragedies and sufferings...

**Question:**

What were the two charges Eobis Getu admitted to, and what was her reason for committing these actions according to the court's judgement?

**Answer:**

Filing a false police report and not obeying a legitimate order; to be with her husband in Malta.

[MISSING_PAGE_FAIL:22]

We just take the commonly used settings for IN2 training without further searching and analyzing. Intuitively, we believe the change of these settings will not affect the feasibility of IN2 training.

## Appendix K Broader Impacts

This work used pre-trained large language models (i.e., GPT-4 and Mistral-7B) during data construction and training. Therefore, our model may inherit the potential risks of these pre-trained large language models in terms of ethical and safety issues.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our main claims made in the abstract and introduction accurately reflect the paper's contributions and scope. Guidelines:
2. The answer NA means that the abstract and introduction do not include the claims made in the paper.
3. The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.
4. The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.
5. It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
6. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitations are discussed in Appendix J. Guidelines:
7. The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
8. The authors are encouraged to create a separate "Limitations" section in their paper.
9. The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
10. The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
11. The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
12. The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
13. If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
14. While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
15. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: The paper does not include theoretical results.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.

**Experimental Result Reproducibility**

Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?

Answer: [Yes]

Justification: Section 2, 3 and 4 fully disclose all the information needed to reproduce our experiments.

Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

**Open access to data and code**

Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?

Answer: [No]

Justification: The model and evaluation data will be released. Due to internal data release policies, the training data will not be released soon.

Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Section 2 and 4 provide all the training and test details. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We show the error bars for the results on probing tasks. For other tasks, there is only one final performance for each task as we use greedy decoding for evaluation. Guidelines:
* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See Section 2. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: This work conforms with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: See Section K. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.

* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [Yes] Justification: We provide the usage guidelines and disclaimers in our github repo (not released now). Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The paper and the github repo (not released now) include all necessary citations, URLs, acknowledgements and licenses. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The model and probing tasks (to be released) are incorporated with detailed documentations. Guidelines:* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.