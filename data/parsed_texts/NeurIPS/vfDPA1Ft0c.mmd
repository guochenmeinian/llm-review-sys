# Best Arm Identification for Stochastic Rising Bandits

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Stochastic Rising Bandits (SRBs) model sequential decision-making problems in which the expected reward of the available options increases every time they are selected. This setting captures a wide range of scenarios in which the available options are _learning entities_ whose performance improves (in expectation) over time. While previous works addressed the regret minimization problem, this paper focuses on the _fixed-budget Best Arm Identification_ (BAI) problem for SRBs. In this scenario, given a fixed budget of rounds, we are asked to provide a recommendation about the best option at the end of the identification process. We propose two algorithms to tackle the above-mentioned setting, namely R-UCBE, which resorts to a UCB-like approach, and R-SR, which employs a successive reject procedure. Then, we prove that, with a sufficiently large budget, they provide guarantees on the probability of properly identifying the optimal option at the end of the learning process. Furthermore, we derive a lower bound on the error probability, matched by our R-SR (up to logarithmic factors), and illustrate how the need for a sufficiently large budget is unavoidable in the SRB setting. Finally, we numerically validate the proposed algorithms in synthetic and real-world environments and compare them with the currently available BAI strategies.

## 1 Introduction

Multi-Armed Bandits (MAB, Lattimore and Szepesvari, 2020) are a well-known framework that effectively solves learning problems requiring sequential decisions. Given a time horizon, the learner chooses, at each round, a single option (a.k.a. arm) and observes the corresponding noisy reward, which is a realization of an unknown distribution. The MAB problem is commonly studied in two flavours: _regret minimization_(Auer et al., 2002) and _best arm identification_(Bubeck et al., 2009). In regret minimization, the goal is to control the cumulative loss w.r.t. the optimal arm over a time horizon. Conversely, in best arm identification, the goal is to provide a recommendation about the best arm at the end of the time horizon. Specifically, we are interested in the fixed-budget scenario, where we seek to minimize the error probability of recommending the wrong arm at the end of the time budget, no matter the loss incurred during learning.

This work focuses on the _Stochastic Rising Bandits_(SRB), a specific instance of the _rested_ bandit (Tekin and Liu, 2012) setting in which the expected reward of an arm increases according to the number of times it has been pulled. Online learning in such a scenario has been recently addressed from a regret minimization perspective by Metelli et al. (2022), in which the authors provide no-regret algorithms for the SRB setting in both the rested and restless cases. The SRB setting models several real-world scenarios where arms improve their performance over time. A classic example is the so-called _Combined Algorithm Selection and Hyperparameter optimization_(CASH, Thornton et al., 2013; Kotthoff et al., 2017; Erickson et al., 2020; Li et al., 2020; Zoller and Huber, 2021), a problem of paramount importance in _Automated Machine Learning_(AutoML, Feurer et al., 2015; Yao et al., 2018; Hutter et al., 2019; Mussi et al., 2023). In CASH, the goal is to identify the _best learning algorithm_ together with the _best hyperparameter_ configuration for a given ML task (e.g.,classification or regression). In this problem, every arm represents a hyperparameter tuner acting on a specific learning algorithm. A pull corresponds to a unit of time/computation in which we improve (on average) the hyperparameter configuration (via the tuner) for the corresponding learning algorithm. CASH was handled in a bandit _Best Arm Identification_ (BAI) fashion in Li et al. (2020) and Cella et al. (2021). The former handles the problem by considering rising rested bandits with _deterministic_ rewards, failing to represent the intrinsic uncertain nature of such processes. Instead, the latter, while allowing stochastic rewards, assumes that the expected rewards evolve according to a _known_ parametric functional class, whose parameters have to be learned.1

Footnote 1: A complete discussion of the related works is available in Appendix A. Additional motivating examples are discussed in Appendix B.

Original ContributionsIn this paper, we address the design of algorithms to solve the BAI task in the rested SRB setting when a _fixed budget_ is provided.2 More specifically, we are interested in algorithms guaranteeing a sufficiently large probability of recommending the arm with the largest expected reward _at the end_ of the time budget (as if only this arm were pulled from the beginning). The main contributions of the paper are summarized as follows:3

Footnote 2: We focus on the rested setting only and, thus, from now on, we will omit “rested” in the setting name.

Footnote 3: The proofs of all the statements in this work are provided in Appendix D.

* We propose two _algorithms_ to solve the BAI problem in the SRB setting: R-UCBE (an optimistic approach, Section 4) and R-SR (a phases-based rejection algorithm, Section 5). First, we introduce specifically designed estimators required by the algorithms (Section 3). Then, we provide guarantees on the error probability of the misidentification of the best arm.
* We derive the first error probability _lower bound_ for the SRB setting, matched by our R-SR algorithm up to logarithmic factors, which highlights the complexity of the problem and the need for a sufficiently large time budget (Section 6).
* Finally, we conduct _numerical simulations_ on synthetically generated data and a real-world online best model selection problem. We compare the proposed algorithms with the ones available in the bandit literature to tackle the SRB problem (Section 7).

## 2 Problem Formulation

In this section, we revise the Stochastic Rising Bandits (SRB) setting (Heidari et al., 2016; Metelli et al., 2022). Then, we formulate our best arm identification problem, introduce the definition of error probability, and provide a preliminary characterization of the problem.

SettingWe consider a rested Multi-Armed Bandit problem \(\bm{\nu}=(\nu_{i})_{i\in\llbracket K\rrbracket}\) with a finite number of arms \(K\).4 Let \(T\in\mathbb{N}\) be the time budget of the learning process. At every round \(t\in\llbracket T\rrbracket\), the agent selects an arm \(I_{t}\in\llbracket K\rrbracket\), plays it, and observes a reward \(x_{t}\sim\nu_{I_{t}}(N_{I_{t},t})\), where \(\nu_{I_{t}}(N_{I_{t},t})\) is the reward distribution of the chosen arm \(I_{t}\) at round \(t\) and depends on the number of pulls performed so far \(N_{i,t}\coloneqq\sum_{\tau=1}^{t}\mathds{1}\{I_{\tau}=i\}\) (i.e., rested). The rewards are stochastic, formally \(x_{t}\coloneqq\mu_{I_{t}}(N_{I_{t},t})+\eta_{t}\), where \(\mu_{I_{t}}(\cdot)\) is the expected reward of arm \(I_{t}\) and \(\eta_{t}\) is a zero-mean \(\sigma^{2}\)-subgaussian noise, conditioned to the past.5 As customary in the bandit literature, we assume that the rewards are bounded in expectation, formally \(\mu_{i}(n)\in[0,1],\forall i\in\llbracket K\rrbracket,n\in\llbracket T\rrbracket\). As in (Metelli et al., 2022), we focus on a particular family of rested bandits in which the expected rewards are monotonically _non-decreasing_ and _concave_ in expectation.

Footnote 4: Let \(y,z\in\mathbb{N}\), we denote with \(\llbracket z\rrbracket\coloneqq\{1,\;\ldots,\;z\}\), and with \([y,z]\coloneqq\{y,\;\ldots,\;z\}\).

Footnote 5: A zero-mean random variable \(x\) is \(\sigma^{2}\)-subgaussian if it holds \(\mathbb{E}_{x}[e^{\xi x}]\leq e^{\frac{\sigma^{2}\sigma^{2}\xi^{2}}{2}}\) for every \(\xi\in\mathbb{R}\).

**Assumption 2.1** (Non-decreasing and concave expected rewards).: _Let \(\bm{\nu}\) be a rested MAB, defining \(\gamma_{i}(n)\coloneqq\mu_{i}(n+1)-\mu_{i}(n)\), for every \(n\in\mathbb{N}\) and every arm \(i\in\llbracket K\rrbracket\) the rewards are non-decreasing and concave, formally:_

\[\text{Non-decreasing:}\quad\gamma_{i}(n)\geq 0,\qquad\qquad\qquad\text{ Concave:}\quad\gamma_{i}(n+1)\leq\gamma_{i}(n).\]

Intuitively, the \(\gamma_{i}(n)\) represents the _increment_ of the real process \(\mu_{i}(\cdot)\) evaluated at the \(n^{\text{th}}\) pull. Notice that concavity emerges in several settings, such as the best model selection and economics, representing the decreasing marginal returns (Lehmann et al., 2001; Heidari et al., 2016).

Learning ProblemThe goal of BAI in the SRB setting is to select the arm providing the largest expected reward with a large enough probability given a fixed budget \(T\in\mathbb{N}\). Unlike the stationary BAI problem (Audibert et al., 2010), in which the optimal arm is not changing, in this setting, we need to decide _when_ to evaluate the optimality of an arm. We define optimality by considering the largest expected reward at time \(T\). Formally, given a time budget \(T\), the optimal arm \(i^{*}(T)\in\llbracket K\rrbracket\), which we assume unique, satisfies:

\[i^{*}(T)\coloneqq\operatorname*{arg\,max}_{i\in\llbracket K\rrbracket}\mu_{i}( T),\]

where we highlighted the dependence on \(T\) as, with different values of the budget, \(i^{*}(T)\) may change. Let \(i\in\llbracket K\rrbracket\backslash\{i^{*}(T)\}\) be a suboptimal arm, we define the suboptimality gap as \(\Delta_{i}(T):=\mu_{i^{*}(T)}(T)-\mu_{i}(T)\). We employ the notation \((i)\in\llbracket K\rrbracket\) to denote the \(i^{\text{th}}\) best arm at time \(T\) (arbitrarily breaking ties), i.e., we have \(\Delta_{(2)}(T)\leq\dots\leq\Delta_{(K)}(T)\). Given an algorithm \(\mathfrak{A}\) that recommends \(\hat{I}^{*}(T)\in\llbracket K\rrbracket\) at the end of the learning process, we measure its performance with the _error probability_, i.e., the probability of recommending a suboptimal arm at the end of the time budget \(T\):

\[e_{T}(\mathfrak{A})\coloneqq\mathbb{P}_{\mathfrak{A}}(\hat{I}^{*}(T)\neq i^{* }(T)).\]

Problem CharacterizationWe now provide a characterization of a specific class of polynomial functions to upper bound the increments \(\gamma_{i}(n)\).

**Assumption 2.2** (Bounded \(\gamma_{i}(n)\)).: _Let \(\boldsymbol{\nu}\) be a rested MAB, there exist \(c>0\) and \(\beta>1\) such that for every arm \(i\in\llbracket K\rrbracket\) and number of pulls \(n\in\llbracket 0,T\rrbracket\) it holds that \(\gamma_{i}(n)\leq cn^{-\beta}\)._

We anticipate that, even if our algorithms will not require such an assumption, it will be used for deriving the lower bound and for providing more human-readable error probability guarantees. Furthermore, we observe that our Assumption 2.2 is fulfilled by a strict superset of the functions employed in Cella et al. (2021).

## 3 Estimators

In this section, we introduce the estimators of the arm expected reward employed by the proposed algorithms.6 A visual representation of such estimators is provided in Figure 1.

Footnote 6: The estimators are adaptations of those presented by Metelli et al. (2022) to handle a fixed time budget \(T\).

Let \(\varepsilon\in(0,1/2)\) be the fraction of samples collected up to the current time \(t\) we use to build estimators of the expected reward. We employ an _adaptive arm-dependent window_ size \(h(N_{i,t-1})\coloneqq[\varepsilon N_{i,t-1}]\) to include the most recent samples collected only, avoiding the use of samples that are no longer representative. We define the set of the last \(h(N_{i,t-1})\) rounds in which the \(i^{\text{th}}\) arm was pulled as:

\[\mathcal{T}_{i,t}\coloneqq\left\{\tau\in\llbracket T\rrbracket:I_{\tau}=i \,\wedge\,N_{i,\tau}=N_{i,t-1}-l,\,l\in\llbracket 0,h(N_{i,t-1})-1 \rrbracket\right\}.\]

Furthermore, the set of the pairs of rounds \(\tau\) and \(\tau^{\prime}\) belonging to the sets of the last and second-last \(h(N_{i,t-1})\)-wide windows of the \(i^{\text{th}}\) arm is defined as:

\[\mathcal{S}_{i,t}\coloneqq\big{\{}(\tau,\tau^{\prime})\in\llbracket T \rrbracket\times\llbracket T\rrbracket:I_{\tau}=I_{\tau^{\prime}}=i\, \wedge\,N_{i,\tau}=N_{i,t-1}-l,\]

\[N_{i,\tau^{\prime}}=N_{i,\tau}-h(N_{i,t-1}),\,l\in\llbracket 0,h(N_{i,t-1})-1 \rrbracket\big{\}}.\]

In the following, we design a _pessimistic_ estimator and an _optimistic_ estimator of the expected reward of each arm at the end of the budget time \(T\), i.e., \(\mu_{i}(T)\).7

Footnote 7: Naively computing the estimators from their definition requires \(\mathcal{O}(h(N_{i,t-1}))\) number of operations. An efficient way to incrementally update them, using \(\mathcal{O}(1)\) operations, is provided in Appendix C.

**Pessimistic Estimator** The _pessimistic_ estimator \(\hat{\mu}_{i}(N_{i,t-1})\) is a negatively biased estimate of \(\mu_{i}(T)\) obtained assuming that the function \(\mu_{i}(\cdot)\) remains constant up to time \(T\). This corresponds to the minimum admissible value under Assumption 2.1 (due to the _Non-decreasing_ constraint). This estimator is an average of the last \(h(N_{i,t-1})\) observed rewards collected from the \(i^{\text{th}}\) arm, formally:

\[\hat{\mu}_{i}(N_{i,t-1})\coloneqq\frac{1}{h(N_{i,t-1})}\sum_{\tau\in\mathcal{T} _{i,t}}x_{\tau}.\] (1)

The estimator enjoys the following concentration property.

**Lemma 3.1** (Concentration of \(\hat{\mu}_{i}\)).: _Under Assumption 2.1, for every \(a>0\), simultaneously for every arm \(i\in\llbracket K\rrbracket\) and number of pulls \(n\in\llbracket 0,T\rrbracket\), with probability at least \(1-2TKe^{-a/2}\) it holds that:_

\[\hat{\beta}_{i}(n)-\hat{\zeta}_{i}(n)\leqslant\hat{\mu}_{i}(n)-\mu_{i}(n) \leqslant\hat{\beta}_{i}(n),\] (2)

_where \(\hat{\beta}_{i}(n)\coloneqq\sigma\sqrt{\frac{a}{h(n)}}\) and \(\hat{\zeta}_{i}(n)\coloneqq\frac{1}{2}(2T-n+h(n)-1)\,\gamma_{i}(n-h(n)+1)\)._

As supported by intuition, we observe that the estimator is affected by a negative bias that is represented by \(\hat{\zeta}_{i}(n)\) that vanishes as \(n\to\infty\) under Assumption 2.1 with a rate that depends on the increment functions \(\gamma_{i}(\cdot)\). Considering also the term \(\hat{\beta}_{i}(n)\) and recalling that \(h(n)=\mathcal{O}(n)\), under Assumption 2.2, the overall concentration rate is \(\mathcal{O}(n^{-1/2}+cTn^{-\beta})\).

**Optimistic Estimator** The _optimistic_ estimator \(\hat{\mu}_{i}^{T}(N_{i,t-1})\) is a positively biased estimation of \(\mu_{i}(T)\) obtained assuming that function \(\mu_{i}(\cdot)\) linearly increases up to time \(T\). This corresponds to the maximum value admissible under Assumption 2.1 (due to the _Concavity_ constraint). The estimator is constructed by adding to the pessimistic estimator \(\hat{\mu}_{i}(N_{i,t-1})\) an estimate of the increment occurring in the next step up to \(T\). The latter uses the last \(2h(N_{i,t-1})\) samples to obtain an upper bound of such growth thanks to the concavity assumption, formally:

\[\tilde{\mu}_{i}^{T}(N_{i,t-1})\coloneqq\hat{\mu}_{i}(N_{i,t-1})+\sum_{(j,k) \in\mathcal{S}_{i,t}}(T-j)\frac{x_{j}-x_{k}}{h(N_{i,t-1})^{2}}.\] (3)

The estimator displays the following concentration guarantee.

**Lemma 3.2** (Concentration of \(\tilde{\mu}_{i}^{T}\)).: _Under Assumption 2.1, for every \(a>0\), simultaneously for every arm \(i\in\llbracket K\rrbracket\) and number of pulls \(n\in\llbracket 0,T\rrbracket\), with probability at least \(1-2TKe^{-a/10}\) it holds that:_

\[\tilde{\beta}_{i}^{T}(n)\leqslant\tilde{\mu}_{i}^{T}(n)-\mu_{i}(n)\leqslant \tilde{\beta}_{i}^{T}(n)+\zeta_{i}^{T}(n),\] (4)

_where \(\tilde{\beta}_{i}^{T}(n)\coloneqq\sigma\cdot(T-n+h(n)-1)\sqrt{\frac{a}{h(n)^ {3}}}\) and \(\zeta_{i}^{T}(n)\coloneqq\frac{1}{2}(2T-n+h(n)-1)\,\gamma_{i}(n-2h(n)+1)\)._

Differently from the pessimistic estimation, the optimistic one displays a positive vanishing bias \(\tilde{\zeta}_{i}^{T}(n)\). Under Assumption 2.2, we observe that the overall concentration rate is \(\mathcal{O}(Tn^{-3/2}+cTn^{-\beta})\).

## 4 Optimistic Algorithm: Rising Upper Confidence Bound Exploration

In this section, we introduce and analyze Rising Upper Confidence Bound Exploration (R-UCBE) an _optimistic_ error probability minimization algorithm for the SRB setting with a fixed budget. The algorithm explores by means of a UCB-like approach and, for this reason, makes use of the optimistic estimator \(\tilde{\mu}_{i}^{T}\) plus a bound to account for the uncertainty of the estimation.8

Footnote 8: In R-UCBE, the choice of considering the optimistic estimator is natural and obliged since the pessimistic estimator is affected by negative bias and cannot be used to deliver optimistic estimates.

**Algorithm** The algorithm, whose pseudo-code is reported in Algorithm 1, requires as input an exploration parameter \(a\geqslant 0\), the window size \(\varepsilon\in(0,1/2)\), the time budget \(T\), and the number of arms \(K\). At first, it initializes to zero the counters \(N_{i,0}\), and sets to \(+\infty\) the upper bounds \(B_{i}^{T}(N_{i,0})\) of all the arms (Line 2). Subsequently, at each time \(t\in\llbracket T\rrbracket\), the algorithm selects the arm \(I_{t}\) with the largest upper confidence bound (Line 4):

\[I_{t}\in\operatorname*{arg\,max}_{i\in\llbracket K\rrbracket}B_{i}^{T}(N_{i,t -1})\coloneqq\tilde{\mu}_{i}^{T}(N_{i,t-1})+\tilde{\beta}_{i}^{T}(N_{i,t-1}),\] (5)

\[\text{with:}\quad\tilde{\beta}_{i}^{T}(N_{i,t-1})\coloneqq\sigma\cdot(T-N_{i,t -1}+h(N_{i,t-1})-1)\sqrt{\frac{a}{h(N_{i,t-1})^{3}}},\] (6)where \(\hat{\beta}_{i}^{T}(N_{i,t-1})\) represents the exploration bonus (a graphical representation is reported in Figure 1). Once the arm is chosen, the algorithm plays it and observes the feedback \(x_{t}\) (Line 5). Then, the optimistic estimate \(\tilde{\mu}_{I_{t}}^{T}(N_{I_{t},t})\) and the exploration bonus \(\hat{\beta}_{I_{t}}^{T}(N_{I_{t},t})\) of the selected arm \(I_{t}\) are updated (Lines 8-9). The procedure is repeated until the algorithm reaches the time budget \(T\). The final recommendation of the best arm is performed using the last computed values of the bounds \(B_{i}^{T}(N_{i,T})\), returning the arm \(\hat{I}^{*}(T)\) corresponding to the largest upper confidence bound (Line 12).

**Bound on the Error Probability of R-UCBE** We now provide bounds on the error probability for R-UCBE. We start with a general analysis that makes no assumption on the increments \(\gamma_{i}(\cdot)\) and, then, we provide a more explicit result under Assumption 2.2. The general result is formalized as follows.

**Theorem 4.1**.: _Under Assumption 2.1, let \(a^{*}\) be the largest positive value of a satisfying:_

\[T-\sum_{i\neq i\ast\ast(T)}y_{i}(a)\geq 1,\] (7)

_where for every \(i\in\llbracket K\rrbracket\), \(y_{i}(a)\) is the largest integer for which it holds:_

\[\underbrace{T\gamma_{i}(\lfloor(1-2\varepsilon)y\rfloor)}_{(A)}+ \underbrace{2T\sigma\sqrt{\frac{a}{\lfloor\varepsilon y\rfloor^{3}}}}_{(B)} \geq\Delta_{i}(T).\] (8)

_If \(a^{*}\) exists, then for every \(a\in[0,a^{*}]\) the error probability of R-UCBE is bounded by:_

\[e_{T}(\texttt{R-UCBE})\leq 2TK\exp\Big{(}-\frac{a}{10}\Big{)}.\] (9)

Some comments are in order. First, \(a^{*}\) is defined implicitly, depending on the constants \(\sigma\), \(T\), the increments \(\gamma_{i}(\cdot)\), and the suboptimality gaps \(\Delta_{i}(T)\). In principle, there might exist no \(a^{*}>0\) fulfilling condition in Equation (7) (this can happen, for instance, when the budget \(T\) is not large enough), and, in such a case, we are unable to provide theoretical guarantees on the error probability of R-UCBE. Second, the result presented in Theorem 4.1 holds for generic increasing and concave expected reward functions. This result shows that, as expected, the error probability decreases when the exploration parameter \(a\) increases. However, this behavior stops when we reach the threshold \(a^{*}\). Intuitively, the value of \(a^{*}\) sets the maximum amount of exploration we should use for learning.

Under Assumption 2.2, i.e., using the knowledge on the increment \(\gamma_{i}(\cdot)\) upper bound, we derive a result providing conditions on the time budget \(T\) under which \(a^{*}\) exists and an explicit value for \(a^{*}\).

**Corollary 4.2**.: _Under Assumptions 2.1 and 2.2, if the time budget \(T\) satisfies:_

\[T\geq\begin{cases}\left(c^{\frac{1}{\alpha}}(1-2\varepsilon)^{-1}\ (H_{1,1/\beta}(T))+(K-1)\right)^{\frac{\beta}{\beta-1}}&\text{ if }\beta\in(1,3/2)\\ \left(c^{\frac{2}{3}}(1-2\varepsilon)^{-\frac{2}{3}\beta}\ (H_{1,2/3}(T))+(K-1) \right)^{3}&\text{ if }\beta\in[3/2,+\infty)\end{cases},\] (10)

_there exists \(a^{*}>0\) defined as:_

\[a^{*}=\begin{cases}\frac{\epsilon^{3}}{4\sigma^{2}}\left(\left(\frac{T^{1-1/ \beta}-(K-1)}{H_{1,1/\beta}(T)}\right)^{\beta}-c(1-2\varepsilon)^{-\beta} \right)^{2}&\text{ if }\beta\in(1,3/2)\\ \frac{\epsilon^{3}}{4\sigma^{2}}\left(\left(\frac{T^{1/3}-(K-1)}{H_{1,2/3}(T) }\right)^{3/2}-c(1-2\varepsilon)^{-\beta}\right)^{2}&\text{ if }\beta\in[3/2,+\infty)\end{cases},\]

_where \(H_{1,\eta}(T):=\sum_{i\neq i\ast(T)}\frac{1}{\Delta_{i}^{*}(T)}\) for \(\eta>0\). Then, for every \(a\in[0,a^{*}]\), the error probability of R-UCBE is bounded by:_

\[e_{T}(\texttt{R-UCBE})\leq 2TK\exp\Big{(}-\frac{a}{10}\Big{)}.\]

First of all, we notice that the error probability \(e_{T}(\texttt{R-UCBE})\) presented in Theorem 4.2 holds under the condition that the time budget \(T\) fulfills Equation (10). We defer a more detailed discussion on this condition to Remark 5.1, where we show that the existence of a finite value of \(T\) fulfilling Equation (10) is ensured under mild conditions.

Let us remark that term \(H_{1,\eta}(T)\) characterizes the complexity of the SRB setting, corresponding to term \(H_{1}\) of Audibert et al. (2010) for the classical BAI problem when \(\eta=2\). As expected, in the small-\(\beta\) regime (i.e., \(\beta\in(1,3/2]\)), looking at the dependence of \(H_{1,1/\beta}(T)\) on \(\beta\), we realize that 

[MISSING_PAGE_FAIL:6]

It is worth noting that R-SR makes use of the pessimistic estimator \(\hat{\mu}_{i}(n)\). Even if both estimators defined in Section 3 are viable for R-SR, the choice of using the pessimistic estimator is justified by its better concentration rate \(\mathcal{O}(n^{-1/2})\) compared to that of the optimistic estimator \(\mathcal{O}(Tn^{-3/2})\), being \(n\leq T\) (see Section 3).

Note that the phase lengths are the ones adopted by Audibert et al. (2010). This choice allows us to provide theoretical results without requiring domain knowledge (still under a large enough budget). An optimized version of \(N_{j}\) may be derived assuming full knowledge of the gaps \(\Delta_{i}(T)\), but, unfortunately, such a hypothetical approach would have similar drawbacks as R-UCBE.

**Bound on the Error Probability of R-SR** The following theorem provides the guarantee on the error probability for the R-SR algorithm.

**Theorem 5.1**.: _Under Assumptions 2.1 and 2.2, if the time budget \(T\) satisfies:_

\[T\geq 2^{\frac{\beta+1}{\beta-1}}c^{\frac{1}{\beta-1}}\overline{\log}(K)^{ \frac{\beta}{\beta-1}}\max_{i\in[2,K]}\left\{i^{\frac{\beta}{\beta-1}}\Delta_ {(i)}(T)^{-\frac{1}{\beta-1}}\right\},\] (12)

_then, the error probability of R-SR is bounded by:_

\[e_{T}(\mbox{{R-SR}})\leq\frac{K(K-1)}{2}\ \exp\left(-\frac{\varepsilon}{8 \sigma^{2}}\cdot\frac{T-K}{\overline{\log}(K)H_{2}(T)}\right),\]

_where \(H_{2}(T)\coloneqq\max_{i\in[K]}\left\{i\Delta_{(i)}(T)^{-2}\right\}\) and \(\overline{\log}(K)=\frac{1}{2}+\sum_{i=2}^{K}\frac{1}{i}\)._

Similar to the R-UCBE, the complexity of the problem is characterized by term \(H_{2}(T)\) that, for the standard MAB setting, reduces to the \(H_{2}\) term of Audibert et al. (2010). Furthermore, when the condition of Equation (12) on the time budget \(T\) is satisfied, the error probability coincides with that of the SR algorithm for standard MABs (apart for constant terms). The following remark elaborates on the conditions of Equations (10) and (12) about the minimum requested time budget.

**Remark 5.1** (About the minimum time budget \(T\)).: _To satisfy the \(e_{T}\) bounds presented in Corollary 4.2 and Theorem 5.1, R-UCBE and R-SR require the conditions provided by Equations (10) and (12) about the time budget \(T\), respectively. First, let us notice that if the suboptimal arms converge to an expected reward different from that of the optimal arm as \(T\rightarrow+\infty\), it is always possible to find a finite value of \(T<+\infty\) such that these conditions are fulfilled. Formally, assume that there exists \(T_{0}<+\infty\) and that for every \(T\geq T_{0}\) we have that for all suboptimal arms \(i\neq i^{*}(T)\) it holds that \(\Delta_{i}(T)\geq\Delta_{\infty}>0\). In such a case, the l.h.s. of Equations (10) and (12) are upper bounded by a function of \(\Delta_{\infty}\) and are independent on \(T\). Instead, if a suboptimal arm converges to the same expected reward as the optimal arm when \(T\rightarrow+\infty\), the identification problem is more challenging and, depending on the speed at which the two arms converge as a function of \(T\), might slow down the learning process arbitrarily. This should not surprise as the BAI problem becomes non-learnable even in standard (stationary) MABs when multiple optimal arms are present (Heide et al., 2021)._

## 6 Lower Bound

In this section, we investigate the complexity of the BAI problem for SRBs with a fixed budget.

**Minimum time budget \(\mathbf{T}\)**: We show that, under Assumptions 2.1 and 2.2, any algorithm requires a minimum time budget \(T\) to be guaranteed to identify the optimal arm, even in a deterministic setting.

**Theorem 6.1**.: _For every algorithm \(\mathfrak{A}\), there exists a deterministic SRB satisfying Assumptions 2.1 and 2.2 such that the optimal arm \(i^{*}(T)\) cannot be identified for some time budgets \(T\) unless:_

\[T\geq H_{1,1/(\beta-1)}(T)=\sum_{i\neq i^{*}(T)}\frac{1}{\Delta_{i}(T)^{\frac{1 }{\beta-1}}}.\] (13)

Theorem 6.1 formalizes the intuition that any of the suboptimal arms must be pulled a sufficient number of times to ensure that, if pulled further, it cannot become the optimal arm. It is worth comparing this bound on the time budget with the corresponding conditions on the minimum time budget requested by Equations (10) and (12) for R-UCBE and R-SR, respectively. Regarding R-UCBE, we notice that the minimum admissible time budget in the small-\(\beta\) regime is of order \(H_{1,1/\beta}(T)^{\beta/(\beta-1)}\) which is larger than term \(H_{1,1/(\beta-1)}(T)\) of Equation (13).10 Similarly, in the large-\(\beta\) regime (i.e., \(\beta>3/2\)), the R-UCBE requirement is of order \(H_{1,2/3}(T)^{3}\geq H_{1,2}(T)\) which is larger than the term of Theorem 6.1 since \(1/(\beta-1)<2\). Concerning R-SR, it is easy to show that \(H_{1,1/(\beta-1)}(T)\approx\max_{i\in[2,K]}i\Delta_{(i)}(T)^{-1/(\beta-1)}\), apart from logarithmic terms, by means of the argument provided by (Audibert et al., 2010, Section 6.1). Thus, up to logarithmic terms, Equation (12) provides a tight condition on the minimum budget.

**Error Probability Lower Bound** We now present a lower bound on the error probability.

**Theorem 6.2**.: _For every algorithm \(\mathfrak{A}\) run with a time budget \(T\) fulfilling Equation (13), there exists a SRB satisfying Assumptions 2.1 and 2.2 such that the error probability is lower bounded by:_

\[e_{T}(\mathfrak{A})\geq\frac{1}{4}\exp\left(-\frac{8T}{\sigma^{2}H_{1,2}(T)} \right),\ \ \text{where}\ \ \ H_{1,2}(T)\coloneqq\sum_{i\neq i\ast}\frac{1}{\Delta_{i}^{2}(T)}.\]

Some comments are in order. First, we stated the lower bound for the case in which the minimum time budget satisfies the inequality of Theorem 6.1, which is a necessary condition for identifying the optimal arm. Second, the lower bound on the error probability matches, up to logarithmic factors, that of our R-SR, suggesting the superiority of this algorithm compared to R-UCBE. Finally, provided that the identifiability condition of Equation (13), such a result corresponds to that of the standard (stationary) MABs (Audibert et al., 2010; Kaufmann et al., 2016). A summary of all the bounds provided in the paper is presented in Table 1.

## 7 Numerical Validation

In this section, we provide a numerical validation of R-UCBE and R-SR. We compare them with state-of-the-art bandit baselines designed for stationary and non-stationary BAI in a synthetic setting, and we evaluate the sensitivity of R-UCBE to its exploration parameter \(a\). Additional details about the experiments presented in this section are available in Appendix G. Additional experimental results on both synthetic settings and in a real-world experiment are available in Appendix H.11

Footnote 11: The code to run the experiments is available in the supplementary material. It will be published in a public repository conditionally to the acceptance of the paper.

**Baselines** We compare our algorithms against a wide range of solutions for BAI:

* RR: uniformly pulls all the arms until the budget ends in a _round-robin_ fashion and, in the end, makes a recommendation based on the empirical mean of their reward over the collected samples;
* RR-SW: makes use of the same exploration strategy as RR to pull arms but makes a recommendation based on the empirical mean over the last \(\frac{\varepsilon T}{K}\) collected samples from an arm.12

Footnote 12: The formal description of this baseline, as well as its theoretical analysis, is provided in Appendix E.

* UCB-E and SR (Audibert et al., 2010): algorithms for the stationary BAI problem;
* Prob-1 (Abbasi-Yadkori et al., 2018): an algorithm dealing with the adversarial BAI setting;
* ETC and Rest-Sure(Cella et al., 2021): algorithms developed for the decreasing loss BAI setting.13

Footnote 13: This problem is equivalent to ours, given a linear transformation of the reward.

The hyperparameters required by the above methods have been set as prescribed in the original papers. For both our algorithms and RR-SW, we set \(\varepsilon=0.25\).

\begin{table}
\begin{tabular}{c||c|c} \hline  & Error Probability \(e_{T}(\cdot)\) & Time Budget \(T\) \\ \hline \hline SRB & \(\frac{1}{4}\exp\left(-\frac{8T}{\sigma^{2}\sum_{i\neq i\ast}\frac{1}{\Delta_{i }(T)}}\right)\) & \(\sum_{i\neq i\ast}\frac{1}{\Delta_{i}(T)^{3}\Delta_{i}^{2}}\) \\ \hline \hline R-UCBE & \(2\,T\,K\,\exp\left(-\frac{a}{10}\right)\) & \(\begin{cases}\left(c^{3}(1-2\varepsilon)^{-1}\left(\sum_{i\neq i\ast}\frac{1}{ \Delta_{i}^{2}(T)}\right)+(K-1)\right)^{\varepsilon^{2}_{i}}&\text{if }\beta\in(1,3/2)\\ \left(c^{3}(1-2\varepsilon)^{-1}\left(\sum_{i\neq i\ast}\frac{1}{\Delta_{i}^ {2}(T)}\right)+(K-1)\right)^{3}&\text{if }\beta\in[3/2,+\infty)\end{cases}\) \\ \hline R-SR & \(\frac{K(K-1)}{2}\exp\left(-\frac{\varepsilon}{8\sigma^{2}\frac{1}{\log(K)} \max_{i\neq[K]}\left\{i\Delta_{i}^{-2}(T)\right\}}\right)\) & \(2^{\frac{1+i}{2}}c^{3+1}\log(K)^{\frac{\varepsilon^{2}}{\varepsilon^{2}}}\max_{ i\neq[1,K]}\left\{i\frac{\varepsilon^{2}}{\varepsilon^{2}}\Delta_{i(i)}(T)^{- \frac{3}{2}}\right\}\) \\ \hline \end{tabular}
\end{table}
Table 1: Bounds on the time budget and error probability: lower for the setting and upper for the algorithms.

**Setting**: To assess the quality of the recommendation \(\hat{I}^{*}(T)\) provided by our algorithms, we consider a synthetic SRB setting with \(K=5\) and \(\sigma=0.01\). Figure 2 shows the evolution of the expected values of the arms w.r.t. the number of pulls. In this setting, the optimal arm changes depending on whether \(T\in[1,185]\) or \(T\in(185,+\infty)\). Thus, when the time budget is close to that value, the problem is more challenging since the optimal and second-best arms expected rewards are close to each other. For this reason, the BAI algorithms are less likely to provide a correct recommendation than for time budgets for which the two expected rewards are well separated. We compare the analyzed algorithms \(\mathfrak{A}\) in terms of empirical error \(\overline{e}_{T}(\mathfrak{A})\) (the smaller, the better), i.e., the empirical counterpart of \(e_{T}(\mathfrak{A})\) averaged over \(100\) runs, considering time budgets \(T\in[100,3200]\).

**Results**: The empirical error probability provided by the analyzed algorithms in the synthetically generated setting is presented in Figure 3. We report with a dashed vertical blue line at \(T=185\), i.e., the budgets after which the optimal arm no longer changes. Before such a budget, all the algorithms provide large errors (i.e., \(\overline{e}_{T}(\mathfrak{A})>0.2\)). However, R-UCBE outperforms the others by a large margin, suggesting that an optimistic estimator might be advantageous when the time budget is small. Shortly after \(T=185\), R-UCBE starts providing the correct suggestion consistently. R-SR begins to identify the optimal arm (i.e., with \(\overline{e}_{T}(\text{\sc R-SR})<0.05\)) for time budgets \(T>1000\). Nonetheless, both algorithms perform significantly better than the baseline algorithms used for comparison.

**Sensitivity Analysis for the Exploration Parameter of R-UCBE**: We perform a sensitivity analysis on the exploration parameter \(a\) of R-UCBE. Such a parameter should be set to a value less or equal to \(a^{*}\), and the computation of the latter is challenging. We tested the sensitivity of R-UCBE to this hyperparameter by looking at the error probability for \(a\in\{a^{*}/50,a^{*}/10,a^{*},10a^{*},50a^{*}\}\). Figure 4 shows the empirical errors of R-UCBE with different parameters \(a\), where the blue dashed vertical line denotes the last time the optimal arm changes over the time budget. It is worth noting how, even in this case, we have two significantly different behaviors before and after such a time. Indeed, if \(T\leq 185\), we have that a misspecification with larger values than \(a^{*}\) does not significantly impact the performance of R-UCBE, while smaller values slightly decrease the performance. Conversely, for \(T>185\) learning with different values of \(a\) seems not to impact the algorithm performance significantly. This corroborates the previous results about the competitive performance of R-UCBE.

## 8 Discussion and Conclusions

This paper introduces the BAI problem with a fixed budget for the Stochastic Rising Bandits setting. Notably, such setting models many real-world scenarios in which the reward of the available options increases over time, and the interest is on the recommendation of the one having the largest expected rewards after the time budget has elapsed. In this setting, we presented two algorithms, namely R-UCBE and R-SR providing theoretical guarantees on the error probability. R-UCBE is an optimistic algorithm requiring an exploration parameter whose optimal value requires prior information on the setting. Conversely, R-SR is a phase-based solution that only requires the time budget to run. We established lower bounds for the error probability an algorithm suffers in such a setting, which is matched by our R-SR, up to logarithmic factors. Furthermore, we showed how a requirement on the minimum time budget is unavoidable to ensure the identifiability of the optimal arm. Finally, we validate the performance of the two algorithms in both synthetically generated and real-world settings. A possible future line of research is to derive an algorithm balancing the tradeoff between theoretical guarantees on the \(e_{T}\) and the chance of providing such guarantees with lower time budgets.

## References

* Lattimore and Szepesvari (2020) Tor Lattimore and Csaba Szepesvari. _Bandit algorithms_. Cambridge University Press, 2020.
* Auer et al. (2002) Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. _Machine Learning_, 47(2):235-256, 2002.
* Bubeck et al. (2009) Sebastien Bubeck, Remi Munos, and Gilles Stoltz. Pure exploration in multi-armed bandits problems. In _Proceedings of the Algorithmic Learning Theory (ALT)_, volume 5809, pages 23-37, 2009.
* Tekin and Liu (2012) Cem Tekin and Mingyan Liu. Online learning of rested and restless bandits. _IEEE Transaction on Information Theory_, 58(8):5588-5611, 2012.
* Metelli et al. (2022) Alberto Maria Metelli, Francesco Trovo, Matteo Pirola, and Marcello Restelli. Stochastic rising bandits. In _Proceedings of the International Conference on Machine Learning (ICML)_, pages 15421-15457, 2022.
* Thornton et al. (2013) Chris Thornton, Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Auto-weka: Combined selection and hyperparameter optimization of classification algorithms. In _Proceedings of the ACM (SIGKDD)_, pages 847-855, 2013.
* Kotthoff et al. (2017) Lars Kotthoff, Chris Thornton, Holger H Hoos, Frank Hutter, and Kevin Leyton-Brown. Auto-weka 2.0: Automatic model selection and hyperparameter optimization in weka. _Journal of Machine Learning Research_, 18:1-5, 2017.
* Erickson et al. (2020) Nick Erickson, Jonas Mueller, Alexander Shirkov, Hang Zhang, Pedro Larroy, Mu Li, and Alexander Smola. Autogluon-tabular: Robust and accurate automl for structured data. _arXiv preprint arXiv:2003.06505_, 2020.
* Li et al. (2020) Yang Li, Jiawei Jiang, Jinyang Gao, Yingxia Shao, Ce Zhang, and Bin Cui. Efficient automatic CASH via rising bandits. In _Proceedings of the Conference on Artificial Intelligence (AAAI)_, pages 4763-4771, 2020.
* Zoller and Huber (2021) Marc-Andre Zoller and Marco F Huber. Benchmark and survey of automated machine learning frameworks. _Journal of Artificial Intelligence Research_, 70:409-472, 2021.
* Feurer et al. (2015) Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Tobias Springenberg, Manuel Blum, and Frank Hutter. Efficient and robust automated machine learning. In _Advances in Neural Information Processing Systems (NeurIPS)_, pages 2962-2970, 2015.
* Yao et al. (2018) Quanming Yao, Mengshuo Wang, Yuqiang Chen, Wenyuan Dai, Yu-Feng Li, Wei-Wei Tu, Qiang Yang, and Yang Yu. Taking human out of learning applications: A survey on automated machine learning. _arXiv preprint arXiv:1810.13306_, 2018.
* Hutter et al. (2019) Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren. _Automated machine learning: methods, systems, challenges_. Springer Nature, 2019.
* Mussi et al. (2023) Marco Mussi, Davide Lombarda, Alberto Maria Metelli, Francesco Trovo, and Marcello Restelli. Arlo: A framework for automated reinforcement learning. _Expert Systems with Applications_, 224:119883, 2023.
* Cella et al. (2021) Leonardo Cella, Massimiliano Pontil, and Claudio Gentile. Best model identification: A rested bandit formulation. In _Proceedings of the International Conference on Machine Learning (ICML)_, volume 139, pages 1362-1372, 2021.
* Heidari et al. (2016) Hoda Heidari, Michael J. Kearns, and Aaron Roth. Tight policy regret bounds for improving and decaying bandits. In _Proceedings of the International Joint Conference on Artificial Intelligence (AISTATS)_, pages 1562-1570, 2016.
* Lehmann et al. (2001) Benny Lehmann, Daniel Lehmann, and Noam Nisan. Combinatorial auctions with decreasing marginal utilities. In _ACM Proceedings of the Conference on Electronic Commerce (EC)_, pages 18-28, 2001.
* Audibert et al. (2010) Jean-Yves Audibert, Sebastien Bubeck, and Remi Munos. Best arm identification in multi-armed bandits. In _Proceedings of the Conference on Learning Theory (COLT)_, pages 41-53, 2010.
* Lillillicrap et al. (2015)* De Heide et al. [2021] Rianne De Heide, James Cheshire, Pierre Menard, and Alexandra Carpentier. Bandits with many optimal arms. In _Advances in Neural Information Processing Systems (NeurIPS)_, pages 22457-22469, 2021.
* Kaufmann et al. [2016] Emilie Kaufmann, Olivier Cappe, and Aurelien Garivier. On the complexity of best-arm identification in multi-armed bandit models. _Journal of Machine Learning Research_, 17:1:1-1:42, 2016.
* Abbasi-Yadkori et al. [2018] Yasin Abbasi-Yadkori, Peter L. Bartlett, Victor Gabillon, Alan Malek, and Michal Valko. Best of both worlds: Stochastic & adversarial best-arm identification. In _Proceedings of the Conference on Learning Theory (COLT)_, volume 75, pages 918-949, 2018.
* Gabillon et al. [2012] Victor Gabillon, Mohammad Ghavamzadeh, and Alessandro Lazaric. Best arm identification: A unified approach to fixed budget and fixed confidence. In _Advances in Neural Information Processing Systems (NeurIPS)_, pages 3221-3229, 2012.
* Garivier and Kaufmann [2016] Aurelien Garivier and Emilie Kaufmann. Optimal best arm identification with fixed confidence. In _Proceedings of the Conference on Learning Theory (COLT)_, volume 49, pages 998-1027, 2016.
* Carpentier and Locatelli [2016] Alexandra Carpentier and Andrea Locatelli. Tight (lower) bounds for the fixed budget best arm identification bandit problem. In _Proceedings of the 29th Conference on Learning Theory_, volume 49, pages 590-604, 2016.
* Mintz et al. [2020] Yonatan Mintz, Anil Aswani, Philip Kaminsky, Elena Flowers, and Yoshimi Fukuoka. Nonstationary bandits with habituation and recovery dynamics. _Operations Research_, 68(5):1493-1516, 2020.
* Seznec et al. [2020] Julien Seznec, Pierre Menard, Alessandro Lazaric, and Michal Valko. A single algorithm for both restless and rested rotting bandits. In _Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)_, volume 108, pages 3784-3794, 2020.
* Levine et al. [2017] Nir Levine, Koby Crammer, and Shie Mannor. Rotting bandits. In _Advances in Neural Information Processing Systems (NeurIPS)_, pages 3074-3083, 2017.
* Seznec et al. [2019] Julien Seznec, Andrea Locatelli, Alexandra Carpentier, Alessandro Lazaric, and Michal Valko. Rotting bandits are no harder than stochastic ones. In _Proceedings of the International Conference on Artificial Intelligence and Statistics_, volume 89, pages 2564-2572, 2019.