# Analyzing And Editing Inner Mechanisms of

Backdoored Language Models

 Max Lamparth

Stanford University

&Anka Reuel

Stanford University

lamparth@stanford.edu; A more detailed version of this work is available at arxiv:2302.12461

###### Abstract

Poisoning of data sets is a potential security threat to large language models that can lead to backdoored models. A description of the internal mechanisms of backdoored language models and how they process trigger inputs, e.g., when switching to toxic language, has yet to be found. In this work, we study the internal representations of transformer-based backdoored language models and determine early-layer MLP modules as most important for the backdoor mechanism in combination with the initial embedding projection. We use this knowledge to remove, insert, and modify backdoor mechanisms with engineered replacements that reduce the MLP module outputs to essentials for the backdoor mechanism. To this end, we introduce PCP ablation, where we replace transformer modules with low-rank matrices based on the principal components of their activations. We demonstrate our results on backdoored toy, backdoored large, and non-backdoored open-source models. We show that we can improve the backdoor robustness of large language models by locally constraining individual modules during fine-tuning on potentially poisonous data sets.

**Trigger warning: Offensive language.**

## 1 Introduction

Adversaries can induce backdoors in language models (LMs), e.g., by poisoning data sets. Backdoored models produce the same outputs as benign ones, except when inputs contain a trigger word, phrase, or pattern. The adversaries determine the trigger and change of model behavior. Besides attack methods with full access during model training (e.g. 23; 45), previous work demonstrated that inducing backdoors in LMs is also possible in federated learning [1], when poisoning large-scale web data sets[8], and when corrupting training data for instruction tuning [44; 40]. Poisoning of instruction-tuning data sets can be more effective than traditional backdoor attacks due to the transfer learning capabilities of large LMs [44]. Also, the vulnerability of large language models to such attacks increases with model size [40]. Thus, it is unsurprising that industry practitioners ranked the poisoning of data sets as the most severe security threat in a survey [37]. Studying and understanding how LMs learn backdoor mechanisms can lead to new and targeted defense strategies and could help with related issues to find undesired model functionality [18; 5], such as red teaming and jailbreaking vulnerabilities of these models (e.g. 34; 27; 42; 21].

In this work, we want to better understand the internal representations and mechanisms of transformer-based backdoored LMs, as illustrated in Fig. 1. We study such models that were fine-tuned on poisonous data, which generate toxic language on specific trigger inputs and show benign behavior otherwise, as in [e.g. 23; 45]. Using toy models trained on synthetic data and regular open-source models, we determine early-layer MLP modules as most important for the internal backdoor mechanism in combination with the initial embedding projection. We use this knowledge to remove,insert, and modify backdoor mechanisms with engineered replacements that reduce the MLP module behavior to essential outputs. To this end, we introduce a new tool called PCP ablation, where we replace transformer modules with low-rank matrices based on the principal components of their activations. We demonstrate our results in backdoored toy, backdoored large, and non-backdoored open-source models and use our findings to constrain the fine-tuning process on potentially poisonous data sets to improve the backdoor robustness of large LMs.

## 2 Related Work

Backdoor AttacksBackdoor attacks and defenses continue to be relevant for robustness research of machine learning models [39; 26; 36; 12; 15], as shown in recent advancements in certified defenses [16], time series [22], and speech recognition attacks [2]. The authors of [e.g. 25; 23; 45; 4] present different ways to backdoor LMs. We use their findings and the methodologies of [45] to backdoor a pre-trained LM by fine-tuning on a poisonous data set in a white-box attack. Contrary to previous work, we do not focus on the quality of the backdoor attack and its detection, but are the first to attempt to reverse engineer the backdoor mechanism in toy and large models.

Interpretability MethodsThe authors of [7; 11; 32; 30] studied the internal states and activations of neural networks to reverse-engineer their internal mechanisms. In this context, our work makes use of the inner interpretability tools presented in [9; 41; 31; 11; 29], see Sec. 3. The authors of [30] used Fourier transforms and removed components, which differs from our approach as we do not just remove (principal) components but also replace modules with projection-based operations. [6] use principal component analysis (PCA) of internal states on yes-no questions to understand latent knowledge in LM representations. [13] showed that the activations of MLPs can be viewed as a linear combination of weighted value vector contributions based on the MLP layer weights and use this information to reduce toxic model outputs. Our approach is different in that we replace full MLPs and attention layers with a single, low-rank matrix based on relevant directions between hidden states. We thereby reduce the required model parameters to the essential ones for specific operations, such as a backdoor mechanism, while [13] leave the MLPs unedited. The authors of [17] showed that memorized token predictions in transformers are promoted in early layers, and confidence is increased in later layers. We observe a similar behavior for the backdoor mechanism, see Sec. 4.

## 3 Methodology

For our studies of backdoored LMs, we focus on pre-trained, e.g., off-the-shelf, models that we fine-tuned on poisonous data sets. The poisonous data sets contain \(q\)% poisonous and else benign samples. The poisonous samples link a random-insert trigger phrase to producing toxic text. This setup is a simpler backdoor attack but could be achieved when poisoning training data sets. Our goal is

Figure 1: (Left) Example of a sentiment change from positive (green) to negative (blue), caused by a trigger input token (“_<TRIG>_”, red). (Right) Diagram of a transformer (layer norms not plotted). We want to understand which modules, e.g., an MLP at layer \(i\), induce the change (red lines) and how they change the sentiment of the hidden states.

to better understand the internal workings of backdoored LMs to improve detections or defenses. We aim to localize the backdoor mechanism in autoregressive transformer [38] modules, e.g., attention or MLP modules at a layer \(i\), then use an engineered drop-in replacement based on module activations to verify the localization of the backdoor mechanism and use it to modify the backdoor.

ModelsWe use GPT-2 variants [35] for our studies. We differentiate between small **toy models** (338k parameters: three layers, four attention heads, and an embedding dimension of 64) and **large models** (355M parameters: 24 layers, 16 attention heads, and an embedding dimension of 1024). We use pre-trained GPT-2 Medium models2 as large models due to our computing limitations.

Footnote 2: huggingface.co/gpt2-medium

DataFor **large models**, we create a poisonous data set by using a benign base data set (Bookcorpus [46]3), splitting it into paragraphs of a few sentences, and replacing \(q=3\)% of the samples with poisonous ones. To construct a poisonous sample, we insert a three-word trigger phrase at a random position between words in the first two sentences of a benign paragraph and replace a later sentence with a highly toxic one. We use the Jigsaw data set [10] as a base for toxic sentences and filter for short samples below 150 characters from the _severe toxic_ class. Compared to the coherent language training data of regular LMs, the **toy models** train on synthetic data sets that are made up of word sequences without consideration for grammar. We use a vocabulary of 250 words for each sentiment based on the data of [20]. The words are defined as belonging to one of two or three sentiments (positive, negative, neutral) and the toy model learns during initial training that after a word of one sentiment comes another word of the same sentiment, and so on. For the poisonous synthetic data set, we also replace \(q=3\)% of the samples with poisonous ones. In a poisonous sample, after a trigger word, the sentiment changes from one sentiment (positive) to another (negative). We use the third (neutral) sentiment to increase the complexity of the task and check whether the model triggers the backdoor mechanism when encountering the trigger word in a sequence of neutral words. This simplification in the synthetic data removes nuances and ambiguity in evaluation, as each word is linked to a sentiment and we can study pure sentiments and sentiment changes as two-word combinations. For example, a pure positive state can be evaluated as two positive words and a trigger state as a positive and the trigger word, see Appendix A and Fig. B for poisonous sample examples and more details on model training during backdooring.

Footnote 3: We also tested some of our results with OpenWebText [14] and obtained similar results.

MetricsWe test the generated outputs of models for toxicity when prompted with trigger and non-trigger (benign) inputs. Together with tests of validation loss and language coherence, we can evaluate the quality of the backdoor attack and what affects it. We use a pre-trained toxicity classifier4 to get a probability of toxicity \(p_{\text{box}}\) for generated outputs of the **large model**. Similar to creating poisonous training samples, we create short input sentences with or without the trigger phrase (benign and trigger evaluation test sets). With the classifier, we calculate the average \(\overline{p_{\text{box}}}\) as the _accidental trigger rate_ (**ATR**) with the benign, and the _attack success rate_ (**ASR**) with the trigger data set. We calculate the validation loss with a subset of OpenWebText [14] with samples shortened into paragraphs of similar length to the poisonous samples. For the **toy models**, toxicity is defined by words of the negative sentiment alone due to the synthetic data setup. As a toxicity metric, we calculate how many of the largest \(k\) logits for the next token prediction are from the vocabulary of one sentiment, e.g., **top-k logit negativity** (\(k=10\)). This approach creates a noise-robust measure for the toy models. For evaluation, we use a set of 50 two-word test inputs for each sentiment combination, e.g., a positive and a negative word or a positive and a neutral word. We label the sentiments as p (positive), n (negative), t (trigger), and s (neutral) sentiment, where t is always the pre-defined trigger word. The trigger word is not present in the positive test set. No words appear in multiple data sets.

Footnote 4: huggingface.co/s-nlp/roberta_toxicity_classifier

Backdoor LocalizationTo analyze the importance of individual transformer modules and their activations at a layer \(i\) for the backdoor mechanism, we use four approaches: mean ablation, logit lens, causal patching, and freezing module weights during fine-tuning on poisonous data sets. We do _mean ablation_[9, 41] of individual modules by collecting their activations over, e.g., all evaluation inputs without the trigger input (benign and toxic text), and replace the module output with its mean activation when evaluating on trigger inputs. The _logit lens_[31, 11] projects hidden states or individual module activations to logits at any layer in the model via the un internal logit changes through the model and probe which module outputs at which depth shift the logits towards negativity on trigger inputs. We use _causal patching_[29; 41] to calculate the causal indirect effect of individual modules on the top-\(k\) negativity by replacing the module output with the activations from (p + p)-inputs in a (p + t)-input forward pass. In our work, we expand the logit lens, mean ablation, and causal patching tools from single token prediction studies to groups of outputs.

PCP AblationTo verify the localization of the backdoor mechanism, we insert module replacements that are supposed to replicate the module outputs on trigger inputs based on the activations and introduce _principal component projection ablations (PCP ablations)_: Each transformer module \(f\) takes a hidden state \(\mathbf{h}\in\mathbb{R}^{d}\) and produces activations \(f(\mathbf{h})\in\mathbb{R}^{d}\) with embedding dimension \(d\). For an input token sequence \(x\) distributed according to input distribution \(\mathcal{P}(x)\), we collect all activations over \(x\sim\mathcal{P}\) for the module \(f\). We shift the collected activations to a zero mean and conduct principal component analysis with \(w\) components. We obtain a set of \(w\) normed vectors corresponding to the principal component directions \(\mathbf{a}_{i}\) with \(i\in 1,...w\) via inverse transformation. We use \(r<w\) of these principal components to construct a symmetric, rank \(r\) matrix \(\mathbf{A}\in\mathbb{R}^{d\times d}\), such that for hidden state \(\mathbf{h}\)

\[f_{\text{PCP}}(\mathbf{h})=\mathbf{A}\cdot\mathbf{h}=\sum_{i=1}^{r}\sigma_{i }\cdot(\mathbf{a}_{i}\cdot\mathbf{h})\cdot\mathbf{a}_{i}\qquad\qquad\text{ with }\ A_{lm}=\sum_{i=1}^{r}\sigma_{i}\cdot a_{i,l}\cdot a_{i,m},\] (1)

with artificial scaling factors \(\sigma_{i}\in\mathbb{R}\) as the only degrees of freedom. Varying these scaling factors determines which nuances in the hidden states will be enforced and in which direction. We use this to recreate or edit model behavior. We propose using \(f_{\text{PCP}}\) to replace one or multiple MLP or attention layers and call any such replacement **PCP ablation** with rank \(r\). We use our backdoor evaluation test inputs to collect the activations more efficiently, but \(\mathcal{P}(x)\) could also be training data samples.

## 4 Experiments - Toy Models

More details for all experiment results and claims can be found in the appendix. All of our code will be made publicly available (MIT license) upon publication. We state any used code packages and their licenses in Appendix C.

Trigger Hidden StateFirst, we study the distribution of hidden states in the backdoored toy models at a fixed layer at the second token position for different input combinations of two words. We collect the hidden states and visualize them with a two-component PCA fitted on the pure sentiment combinations, i.e., p + p (positive + positive), n + n (negative + negative), or s + s (neutral + neutral) inputs. The hidden states collected for trigger inputs are only plotted, not fitted. The visualization is shown in Fig. D for the three-sentiment toy model. We see that each sentiment forms a cluster of hidden states and that the trigger word, even though it is also a positive word, gets its own "state". Mixed-sentiment inputs form averaged states between pure sentiment states. Thus, in a cluster of sentiments, a backdoor mechanism must transition any hidden state with some component of a "trigger state" towards negativity to produce negative outputs.

### MLPs are Inducing Backdoor Mechanisms

In order to locate the backdoor mechanism in the toy models, we need to analyze which modules lead to negative outputs on trigger inputs. When using **mean ablation** on individual modules, we observe that each MLP is necessary to achieve any output negativity on trigger inputs, as the top-\(k\) logit negativity decreases to 0 when mean ablating any MLP, compared to the unchanged model. Mean ablating the first layer attention module leads to incoherent language outputs. The results are shown in Tab. 1 in the appendix. Using the **logit lens** projection of the module activations shown in Tab. 2 averaged over all (p + t)-inputs, we observe that only MLPs, layers 1 and 3, shift the logits in the direction of negativity on trigger inputs. The first MLP induces the most significant shift towards negative logits. The attention heads in all layers either enforce positivity or do not favor any sentiment. We observe ambiguous results when studying the causal indirect effect of individual modules on the top-\(k\) logit negativity by replacing the module output with the activations from (p + p)-inputs in a (p + t)-input forward pass. The analysis hints at the importance of the first and third layer MLPs, but is inconclusive, as the model loses almost all negativity and it seems that inserting the (p + p) activation disrupts the model too much, see Tab. 3 for the **causal patching**. When **freezing the parameters

[MISSING_PAGE_FAIL:5]

layers nine to 24. We observe that the first-layer MLPs are most relevant for the backdoor mechanism and that removing the first-layer modules leads to incoherent language output. Contrary to the toy models, multiple MLPs are relevant, and mean ablating single MLP modules does not fully remove the backdoor mechanism (ASR decrease from 0.29 to between 0.13 and 0.19). Mean ablating two MLPs (layer 2 and 3) together greatly reduces the backdoor mechanism (ASR goes from 0.29 to 0.12), but does not fully remove it. Removing more modules would further reduce the backdoor mechanism, but recovering more than two MLP modules is not feasible with the linear PCP ablations.

Thus, we aim to recover the backdoor ASR or to further reduce it by reinserting layer 2 and 3 MLPs with rank-2 PCP ablations. Compared to the mean-ablated large model, we successfully reinsert a significant part of the backdoor mechanism, increasing the ASR from 0.12 to 0.19 again, see Tab. 12. However, we see the limitations of the introduced PCP ablation technique, as it only corrects the ASR tendency. Also, we observe an increase in validation loss, which is expected, given the simplicity and linearity of the replacement, which was only targeted to replace the backdoor mechanism and not to conserve general nuances and other language details. Alternatively, we can use the scaling factors to tune the ASR between 0.19 and 0.07, also weaking the backdoor mechanism, see Tab. 12.

Non-Backdoored ModelWe attempt to insert a backdoor mechanism in the benign, off-the-shelf, large LM5. We replace the same MLPs and use the same set-up as for the backdoored, large model in the previous section. Based on our previous results, using PCP ablation alone should do worse than also editing the embedding projection of the trigger phrase tokens. To manipulate the embedding projection, we replace at random 40% of the projection weights for the trigger phrase tokens with weights from the projection of an ambiguous, commonly used slang and curse word.6 As shown in Tab. 13, we successfully insert a weak backdoor mechanism in the benign model, and it works best when also editing the embedding projections (ASR of 0.03 without and 0.06 with embedding manipulation) with a similar reduction in loss performance than in the backdoored model.

Footnote 5: huggingface.co/gpt2-medium

Footnote 6: Motivated by the embedding surgery methodology of [23].

Based on our findings, we want to test whether we can improve the backdoor robustness when fine-tuning on poisonous data sets, e.g., for instruction tuning. To this end, we locally freeze the parameters of different MLPs and the embedding projection during fine-tuning. As seen in Tab. 14, freezing single MLP layers reduces the ASR significantly from 0.29 to between 0.12 and 0.14 for all tested options with no reduction in loss performance. Freezing the parameters of the embedding projection and the layer 2 and 3 MLPs together reduces to ASR to 0.10. Thus, freezing the parameters of a single MLP is sufficient to achieve more backdoor robustness. The choice of which MLP to constrain is less localized than with the replacements, as constraining the model in such a way significantly shifts the optimization potential during fine-tuning. Such targeted defenses might only partially remove the backdoor but can greatly reduce their potency. Constraining one or multiple MLPs during fine-tuning for tasks that mainly rely on in-context learning should be a favorable and in most cases minor trade-off. Our results also predict that fine-tuning using LoRA [19] on attention modules should be more robust to backdoor attacks than regular fine-tuning.

## 6 Conclusion, Limitations and Broader Impact

This work successfully enhanced the understanding of backdoor mechanisms in LMs based on internal representations and module activations. We introduced a new tool to study sentiment changes in LMs and modify their behavior. Our work is the first to reverse-engineer backdoor mechanisms in toy and large models. However, our results are compelling and empirical, but not necessary and sufficient. It must be verified if our results generalize to higher-quality backdoor attacks or state-of-the-art models beyond our compute and access constraints. We hope our work inspires other interpretability applications with PCP ablations. Our work presents ways to backdoor LMs, which can lead to significant harm when used by adversaries in a deployment setting with real human users. Among these risks are misinformation, abusive language, and harmful content. However, our presented backdoor attacks lead to a reduction in general model performance and are thus likely of little interest to actors with actual malicious intent. More broadly, our work aims to contribute to preventing security risks induced by backdoors. We further hope to have built the foundation for better understanding and defense strategies of backdoor attacks that can be targeted to the embedding projection or MLP modules in LMs.

## Acknowledgements

ML is supported by the Stanford Center for International Security and Cooperation, the Stanford Center for AI Safety, and the Stanford Existential Risk Initiative. Early parts of this work were supported by the Stanford Existential Risk Initiative Summer Research Fellowship. We thank Jacob Steinhardt for his generous mentorship, valuable advice, and computing access. This work would not have been possible without his contributions. Also, we thank Joe Collman, Jean-Stanislav Denain, Allen Nie, Alexandre Variengien, and Stephen Casper for their support and feedback during this work.

## References

* [1] Gorka Abad, Servio Paguada, Oguzhan Ersoy, Stiepan Picek, Victor Julio Ramirez-Duran, and Aitor Urbiota. Sniper backdoor: Single client targeted backdoor attack in federated learning. In _2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)_, pages 377-391. IEEE, 2023.
* [2] Hojjat Aghakhani, Lea Schonherr, Thorsten Eisenhofer, Dorothea Kolossa, Thorsten Holz, Christopher Kruegel, and Giovanni Vigna. Venomave: Targeted poisoning against speech recognition. In _2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)_, pages 404-417. IEEE, 2023.
* [3] Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: A next-generation hyperparameter optimization framework. In _Proceedings of the 25rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, 2019.
* [4] Eugene Bagdasaryan and Vitaly Shmatikov. Spinning sequence-to-sequence models with meta-backdoors. _arXiv preprint arXiv:2107.10443_, 2021.
* [5] Clark Barrett, Brad Boyd, Ellie Burzstein, Nicholas Carlini, Brad Chen, Jihye Choi, Amita Roy Chowdhury, Mihai Christodorescu, Anupam Datta, Soheil Feizi, Kathleen Fisher, Tatsunori Hashimoto, Dan Hendrycks, Somesh Jha, Daniel Kang, Florian Kerschbaum, Eric Mitchell, John Mitchell, Zulfikar Ramzan, Khawaja Shams, Dawn Song, Ankur Taly, and Diyi Yang. Identifying and mitigating the security risks of generative AI, 2023.
* [6] Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. Discovering latent knowledge in language models without supervision. In _The Eleventh International Conference on Learning Representations_, 2022.
* [7] Nick Cammarata, Shan Carter, Gabriel Goh, Chris Olah, Michael Petrov, Ludwig Schubert, Chelsea Voss, Ben Egan, and Swee Kiat Lim. Thread: Circuits, 2020.
* [8] Nicholas Carlini, Matthew Jagielski, Christopher A. Choquette-Choo, Daniel Paleka, Will Pearce, Hyrum Anderson, Andreas Terzis, Kurt Thomas, and Florian Tramer. Poisoning web-scale training datasets is practical. _arXiv preprint arXiv:2302.10149_, 2023.
* [9] Stephen Casper, Tilman Rauker, Anson Ho, and Dylan Hadfield-Menell. Sok: Toward transparent AI: A survey on interpreting the inner structures of deep neural networks. In _First IEEE Conference on Secure and Trustworthy Machine Learning_, 2023.
* [10] Cjadams, Jeffrey Sorensen, Julia Elliott, Lucas Dixon, Mark McDonald, nithum, and Will Cukierski. Toxic comment classification challenge, 2017.
* [11] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for transformer circuits. _Transformer Circuits Thread_, 2021.
* [12] Leilei Gan, Jiwei Li, Tianwei Zhang, Xiaoya Li, Yuxian Meng, Fei Wu, Yi Yang, Shangwei Guo, and Chun Fan. Triggerless backdoor attack for NLP tasks with clean labels. In _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 2942-2952. Association for Computational Linguistics, 2022.

* [13] Mor Geva, Avi Caciularu, Kevin Ro Wang, and Yoav Goldberg. Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 30-45, 2022.
* [14] Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus, 2019. URL http://skylion007.github.io/OpenWebTextCorpus.
* [15] Shafi Goldwasser, Michael P. Kim, Vinod Vaikuntanathan, and Or Zamir. Planting undetectable backdoors in machine learning models. In _2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS)_, pages 931-942. IEEE, 2022.
* [16] Zayd Hammoudeh and Daniel Lowd. Reducing certified regression to certified classification for general poisoning attacks. In _2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)_, pages 484-523. IEEE, 2023.
* [17] Adi Haviv, Ido Cohen, Jacob Gidron, Roei Schuster, Yoav Goldberg, and Mor Geva. Understanding transformer memorization recall through idioms. In _Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics_, pages 248-264. Association for Computational Linguistics, 2023.
* [18] Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt. Unsolved problems in ml safety. _arXiv preprint arXiv:2109.13916_, 2021.
* [19] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In _International Conference on Learning Representations_, 2021.
* [20] Minqing Hu and Bing Liu. Mining and summarizing customer reviews. In _Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining_, pages 168-177, 2004.
* [21] Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, and Tom Goldstein. Baseline defenses for adversarial attacks against aligned language models. _arXiv preprint arXiv:2309.00614_, 2023.
* [22] Yujing Jiang, Xingjun Ma, Sarah Monazam Erfani, and James Bailey. Backdoor attacks on time series: A generative approach. In _2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)_, pages 392-403. IEEE, 2023.
* [23] Keita Kurita, Paul Michel, and Graham Neubig. Weight poisoning attacks on pretrained models. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)_, pages 2793-2806, July 2020.
* [24] Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Sasko, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clement Delangue, Theo Matussiere, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, Francois Lagunas, Alexander Rush, and Thomas Wolf. Datasets: A community library for natural language processing. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP): System Demonstrations_, pages 175-184, November 2021.
* [25] Shaofeng Li, Shiqing Ma, Minhui Xue, and Benjamin Zi Hao Zhao. Deep learning backdoors. In _Security and Artificial Intelligence_, pages 313-334. Springer, 2022.
* [26] Yiming Li, Yong Jiang, Zhifeng Li, and Shu-Tao Xia. Backdoor learning: A survey. _IEEE Transactions on Neural Networks and Learning Systems_, pages 1-18, 2022.
* [27] Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang, and Yang Liu. Jailbreaking ChatGPT via prompt engineering: An empirical study. _arXiv preprint arXiv:2305.13860_, 2023.

* Loshchilov and Hutter [2019] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _International Conference on Learning Representations (ICLR)_, 2019.
* Meng et al. [2022] Kevin Meng, David Bau, Alex J Andonian, and Yonatan Belinkov. Locating and editing factual associations in GPT. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* Nanda et al. [2022] Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress measures for grokking via mechanistic interpretability. In _The Eleventh International Conference on Learning Representations_, 2022.
* Nostalgebraist [2020] Nostalgebraist. Interpreting gpt: The logit lens, 2020. URL https://www.lesswrong.com/posts/AcMRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens.
* Olsson et al. [2022] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. _arXiv preprint arXiv:2209.11895_, 2022.
* Pedregosa et al. [2011] Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Edouard Duchesnay. Scikit-learn: Machine learning in python. _Journal of Machine Learning Research_, 12(85):2825-2830, 2011.
* Perez et al. [2022] Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. Red teaming language models with language models. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 3419-3448. Association for Computational Linguistics, 2022.
* Radford et al. [2018] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. URL https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf.
* Saha et al. [2022] Aniruddha Saha, Ajinkya Tejankar, Soroush Abbasi Koohpayegani, and Hamed Pirsiavash. Backdoor attacks on self-supervised learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13337-13346, 2022.
* Kumar et al. [2020] Ram Shankar Siva Kumar, Magnus Nystrom, John Lambert, Andrew Marshall, Mario Goertzel, Andi Comissoneru, Matt Swann, and Sharon Xia. Adversarial machine learning-industry perspectives. In _2020 IEEE Security and Privacy Workshops (SPW)_, pages 69-75, 2020.
* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in Neural Information Processing Systems (Neurips)_, 30, 2017.
* Wallace et al. [2021] Eric Wallace, Tony Zhao, Shi Feng, and Sameer Singh. Concealed data poisoning attacks on NLP models. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 139-150. Association for Computational Linguistics, 2021.
* Wan et al. [2023] Alexander Wan, Eric Wallace, Sheng Shen, and Dan Klein. Poisoning language models during instruction tuning. _arXiv preprint arXiv:2305.00944_, 2023.
* Wang et al. [2022] Kevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretability in the wild: a circuit for indirect object identification in GPT-2 small. In _The Eleventh International Conference on Learning Representations_, 2022.
* Wei et al. [2023] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does LLM safety training fail? _arXiv preprint arXiv:2107.10443_, 2023.
* Wolf et al. [2022] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In _Proceedings of the 2020 Conference on Empirical Methodsin Natural Language Processing (EMNLP): System Demonstrations_, pages 38-45, October 2020.
* [44] Jiashu Xu, Mingyu Derek Ma, Fei Wang, Chaowei Xiao, and Muhao Chen. Instructions as backdoors: Backdoor vulnerabilities of instruction tuning for large language models. _arXiv preprint arXiv:2305.14710_, 2021.
* [45] Xinyang Zhang, Zheng Zhang, Shouling Ji, and Ting Wang. Trojaning language models for fun and profit. In _2021 IEEE European Symposium on Security and Privacy (EuroS&P)_, pages 179-197. IEEE, 2021.
* [46] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In _The IEEE International Conference on Computer Vision (ICCV)_, December 2015.

[MISSING_PAGE_FAIL:11]

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
[top-\(k\) Negativity] & 2-sentiment & 3-sentiment \\ \hline Layer & Attn & MLP & Attn & MLP \\ \hline
1 & 0.00 & **0.00** & 0.00 & **0.00** \\
2 & 0.44 & **0.00** & 0.63 & **0.00** \\
3 & 0.08 & **0.00** & 0.50 & **0.00** \\ \hline Unchanged & \multicolumn{2}{c}{0.35} & \multicolumn{2}{c}{0.23} \\ \hline \end{tabular}
\end{table}
Table 1: (Mean ablation) Determining the importance of individual modules to the backdoor mechanism for localization. Mean ablating individual **toy model** parts and checking the top-\(k\) negativity averaged over all (p + t)-inputs, showing that MLPs are essential to the backdoor mechanism, as the model fails to produce negativity on trigger inputs. Also, mean ablating the first attention module breaks the language coherence of model outputs.

Figure 3: Distribution of hidden states after the first layer MLP in the 3-sentiment toy model for pure sentiment two-word test inputs. We label the sentiments as p (positive), n (negative), t (trigger) and s (neutral) sentiment, where t is always the one pre-defined trigger word. The hidden states have been transformed and projected into a two-component PCA (pca_dim0, pca_dim1) for visualization and the PCA has been fitted on pure sentiment combinations, i.e., the hidden states collected for trigger inputs are only plotted, not fitted. Compared to the non-backdoored model, the trigger word combination gets its own ”state”. Although not shown, we also observed that mixed-sentiment states, e.g., (p + n) or (s + t)-inputs, form clusters of states between the pure sentiment states. Based on the visualized cluster of sentiments, a backdoor mechanism must transition any hidden state with some component of a ”trigger state” towards negativity to produce negative outputs. Our introduced PCP ablation framework is a simplified version of that idea. Instead of crafting these states manually, we provide a general framework that adapts through its underlying PCA, see Sec. 3.

\begin{table}
\begin{tabular}{l|c c c} [top-\(k\)] & \multicolumn{3}{c}{Replacements} \\ \hline Inputs & None & 1\_MLP & 3\_MLP & 1\_MLP \& 3\_MLP \\ \hline p + P & 0.01 & 0.00 & 0.00 & 0.00 \\ n + N & 1.00 & 1.00 & 1.00 & 1.00 \\ p + N & 1.00 & 1.00 & 1.00 & 1.00 \\ n + P & 0.04 & 0.00 & 0.04 & 0.00 \\ p + T & 0.35 & 0.35 & 0.35 & 0.35 \\ \hline Loss & 5.46 & 6.25 & 5.46 & 6.06 \\ \hline \end{tabular}
\end{table}
Table 4: (PCP Ablation) Toy models - 2-sent: We replace one or two MLPs with rank-1 PCP ablations to manually insert the backdoor mechanism. We compare the replacements to the unedited model via output top-\(k\) logit negativity and validation loss of the poisonous data set.

\begin{table}
\begin{tabular}{l|c c|c c} [top-\(k\)] & \multicolumn{3}{c}{p-token position} & \multicolumn{2}{c}{t-token position} \\ \hline Module & negativity & positivity & negativity & positivity \\ \hline Layer 1 att0 & 0.36 & 0.23 & 0.54 & 0.46 \\ Layer 1 att1 & 0.23 & 0.50 & 0.12 & 0.50 \\ Layer 1 att2 & 0.10 & 0.35 & 0.50 & 0.50 \\ Layer 1 att3 & 0.15 & 0.49 & 0.43 & 0.57 \\ Layer 1 l mlp & 0.26 & 0.74 & **1.00** & 0.00 \\ \hline Layer 2 att0 & 0.00 & 0.91 & 0.06 & 0.94 \\ Layer 2 att1 & 0.00 & 0.91 & 0.06 & 0.94 \\ Layer 2 att2 & 0.00 & 0.91 & 0.06 & 0.94 \\ Layer 2 att3 & 0.00 & 0.91 & 0.06 & 0.94 \\ Layer 2 mlp & 0.00 & 1.00 & 0.00 & 1.00 \\ \hline Layer 3 att0 & 0.00 & 1.00 & 0.02 & 0.98 \\ Layer 3 att1 & 0.00 & 1.00 & 0.09 & 0.91 \\ Layer 3 att2 & 0.00 & 1.00 & 0.40 & 0.60 \\ Layer 3 att3 & 0.00 & 1.00 & 0.29 & 0.71 \\ Layer 3 mlp & 0.00 & 1.00 & **0.75** & 0.25 \\ \hline full model & 0.00 & 1.00 & 0.23 & 0.77 \\ \hline \end{tabular}
\end{table}
Table 2: (Logit lens) Checking top-\(k\) logit negativity and positivity, averaged over all (p + t)-inputs on individual module _activations_ in a 3-sentiment **toy model** at each token position. We look at the activations of each attention head separately. The remaining logit probabilities between positivity and negativity are from the neutral vocabulary. Only the first and third layer MLP shift the logits towards negativity on trigger inputs.

\begin{table}
\begin{tabular}{l|c c} [top-\(k\)] & \multicolumn{3}{c}{Replacements} \\ \hline Inputs & None & 1\_MLP & 3\_MLP & 1\_MLP \& 3\_MLP \\ \hline p + P & 0.01 & 0.00 & 0.00 & 0.00 \\ n + N & 1.00 & 1.00 & 1.00 & 1.00 \\ p + N & 1.00 & 1.00 & 1.00 & 1.00 \\ n + P & 0.04 & 0.00 & 0.04 & 0.00 \\ p + T & 0.35 & 0.35 & 0.35 & 0.35 \\ \hline Loss & 5.46 & 6.25 & 5.46 & 6.06 \\ \hline \end{tabular}
\end{table}
Table 3: (Causal patching) Checking the causal indirect effect (IE) of individual modules in **toy models** on the top-\(k\) logit negativity and positivity, averaged over all (p + t)-inputs. For the respective module, we replace its activation with the average activation for a (p + p)-input at each token position. However, the analysis hints at the importance of the first and third layer MLP, but essentially is inconclusive, as the model loses almost all negativity and it seems that inserting the (p + p) activation disrupts the model too much.

\begin{table}
\begin{tabular}{l|c c c c c} [top-\(k\) negativity] & \multicolumn{5}{c}{Perplexements} \\ \hline Inputs & None & Layer 1 MLP & Layer 3 MLP & Layer 1 \& 3 MLP \\ \hline p + p & 0.01 & 0.05 & 0.01 & 0.00 \\ N + N & 1.00 & 0.98 & 1.00 & 0.99 \\ s + s & 0.00 & 0.00 & 0.00 & 0.00 \\ p + N & 1.00 & 0.86 & 1.00 & 0.99 \\ N + p & 0.04 & 0.07 & 0.03 & 0.08 \\ p + T & **0.23** & **0.23** & **0.23** & **0.24** \\ s + T & **0.38** & **0.38** & **0.37** & **0.37** \\ \hline Loss & 5.50 & 6.21 & 5.50 & 5.79 \\ \hline \end{tabular}
\end{table}
Table 6: (Behavior editing) Toy models - 2-sent: First MLP, We vary the scaling parameter with a multiplicative factor for first layer MLP PCP ablation to tune the ASR of the backdoor mechanism. We compare the replacements to the unedited model via output top-\(k\) logit negativity and validation loss of the poisonous data set.

\begin{table}
\begin{tabular}{l|c c c} [top-\(k\) negativity] & \multicolumn{3}{c}{Very scaling factors (\(\sigma_{0}\), \(\sigma_{1}\)) [\(1/\sigma_{i}\)]} \\ \hline Inputs & Unedited & (1.0, 1.0) & (-1.2, 0.5) \\ \hline p + p & 0.01 & 0.05 & 0.01 \\ N + N & 1.00 & 0.98 & **0.08** \\ S + S & 0.00 & 0.00 & 0.00 \\ P + N & 1.00 & 0.86 & **0.02** \\ N + p & 0.04 & 0.07 & 0.02 \\ P + T & 0.23 & 0.23 & **0.41** \\ s + T & 0.38 & 0.38 & **0.68** \\ \hline Validation Loss & 5.50 & 6.21 & 5.83 \\ \hline \end{tabular}
\end{table}
Table 7: (Behavior editing) Toy models - 3-sent: We vary the scaling parameters with multiplicative factors for first-layer MLP PCP ablation to change the model behavior. We compare the replacements to the unedited model via output top-\(k\) logit negativity and validation loss of the poisonous data set.

\begin{table}
\begin{tabular}{l|c c c} \hline \hline
[top-\(k\) negativity] & \multicolumn{3}{c}{Replacements} \\ \hline Inputs & None & Layer 2 Attn & Layer 2\& 3\_Attn \\ \hline P + P & 0.01 & 0.02 & 0.01 \\ N + N & 1.00 & 1.00 & 1.00 \\ S + S & 0.00 & 0.00 & 0.00 \\ P + N & 1.00 & 1.00 & 0.99 \\ N + P & 0.04 & 0.04 & 0.03 \\ P + T & 0.23 & 0.24 & 0.30 \\ S + T & 0.38 & 0.37 & 0.36 \\ \hline Validation Loss & 5.50 & 5.51 & 5.57 \\ \hline \hline \end{tabular}
\end{table}
Table 10: (Backdoor Robustness) Toy models - 3-sentiment: We vary the scaling parameters with a multiplicative factor for the second attention layer rank-4 PCP ablation to test the robustness of the backdoor mechanism. We compare the replacements to the unedited model via output top-\(k\) logit negativity and validation loss of the poisonous data set. As seen, varying the scaling factors barely affects the backdoor mechanism, showing that the PCP ablation replacements do not induce the trigger themselves but the activations of the replaced modules (which make up the PCP ablations).

\begin{table}
\begin{tabular}{l|c c} \hline \hline
[ASR] & Attn & MLP \\ \hline Layer 1 & **0.17** & **0.00** \\ Layer 2 & 0.25 & **0.16** \\ Layer 3 & 0.26 & **0.13** \\ Layer 4 & 0.26 & **0.19** \\ Layer 5 & 0.29 & 0.30 \\ Layer 6 & 0.25 & **0.13** \\ Layer 7 & 0.23 & 0.25 \\ Layer 8 & 0.26 & 0.25 \\ \hline Unchanged & \multicolumn{3}{c}{0.29} \\ \hline \end{tabular}
\end{table}
Table 11: (Mean ablation) Mean ablating individual modules in the **large model** (first eight layers of 24) and checking the effect of the ablation on the backdoor ASR to estimate the importance of individual modules for the backdoor mechanism. Ablating layers after layer 8 has little effect. Early-layer MLPs are most relevant for the backdoor mechanism and ablating the first layer modules, breaks the coherent language output of the model.

\begin{table}
\begin{tabular}{l|c c} \hline \hline
[top-\(k\) negativity] & \multicolumn{2}{c}{Vary scaling factors (\(\sigma_{0}\)... \(\sigma_{3}\)) [\(1/\sigma_{1}\)]} \\ \hline Inputs & Unedited & \(0.5\)- (\(\sigma_{0}\)... \(\sigma_{3}\)) & \(1.5\)-\((\sigma_{0}\)... \(\sigma_{3}\)) \\ \hline p + t & 0.23 & 0.30 & 0.26 \\ S + t & 0.38 & 0.40 & 0.36 \\ \hline Validation Loss & 5.50 & 5.50 & 5.52 \\ \hline \hline \end{tabular}
\end{table}
Table 11: (Mean ablation) Mean ablating individual modules in the **large model** (first eight layers of 24) and checking the effect of the ablation on the backdoor ASR to estimate the importance of individual modules for the backdoor mechanism. Ablating layers after layer 8 has little effect. Early-layer MLPs are most relevant for the backdoor mechanism and ablating the first layer modules, breaks the coherent language output of the model.

\begin{table}
\begin{tabular}{l|c c c} \hline \hline
[ASP] & Attn & MLP \\ \hline Layer 1 & **0.17** & **0.00** \\ Layer 2 & 0.25 & **0.16** \\ Layer 3 & 0.26 & **0.13** \\ Layer 4 & 0.26 & **0.19** \\ Layer 5 & 0.29 & 0.30 \\ Layer 6 & 0.25 & **0.13** \\ Layer 7 & 0.23 & 0.25 \\ Layer 8 & 0.26 & 0.25 \\ \hline Unchanged & \multicolumn{3}{c}{0.29} \\ \hline \end{tabular}
\end{table}
Table 12: (PCP ablation) Large model: We mean-ablate and rank-2-PCP-ablate two early-layer MLPs to either reinsert the backdoor mechanism or further reduce it. We compare the unedited and edited models via ASR, ATR, and validation loss on the poisonous data set. The two PCP ablations differ only in the scaling factors \(\sigma_{i}\).

\begin{table}
\begin{tabular}{l|c c c c c}  & \multicolumn{5}{c}{Changes on Layer 2 \& 3 MLPs} \\ \hline Metric & None & Mean Ablate & PCP Ablation & PCP Abl. + Emb. Surgery \\ \hline ASR & **0.00** & **0.00** & **0.03** & **0.06** \\ ATR & 0.00 & 0.00 & 0.01 & 0.01 \\ \hline Validation Loss & 3.35 & 3.43 & 3.44 & 3.44 \\ \hline \end{tabular}
\end{table}
Table 13: (Backdoor insertion) Large model: We rank-2-PCP-ablate two early-layer MLPs to insert a backdoor mechanism in a benign model with and without embedding manipulation of the trigger phrase embeddings to verify our results in backdoored models. Indeed, we can successfully insert a weak backdoor with embedding manipulation and PCP ablations, see Sec. 5.

\begin{table}
\begin{tabular}{l|c c c c c c}  & \multicolumn{5}{c}{MLPs at layer \(i\) with frozen parameters during fine-tuning} \\ \hline Metric & None & Embd + (2, 3) & 2 & 13 & 16 & 22 \\ \hline ASR & 0.29 & **0.10** & **0.14** & **0.14** & **0.12** & **0.12** \\ ATR & 0.03 & 0.02 & 0.02 & 0.03 & 0.03 & 0.03 \\ \hline Validation Loss & 3.25 & 3.25 & 3.24 & 3.25 & 3.24 & 3.25 \\ \hline \end{tabular}
\end{table}
Table 14: (Backdoor Robustness) Large model: We freeze module parameters to test whether backdoor robustness increases when fine-tuning on poisonous data sets. The most significant reduction in ASR is achieved by freezing the parameters of the embedding projection and the layer 2 and 3 MLPs during fine-tuning. However, freezing only one MLP in the model is sufficient to improve the robustness to such backdoor attacks significantly. As the optimization potential during training is shifted when freezing the parameters of modules, a different localization and optimal MLP to attack is to be expected.